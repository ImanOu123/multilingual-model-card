{"title": "Mining: New Paradigms for New Challenges C\u00e8sar Ferri Ram\u00edrez, Jos\u00e9 Hern\u00e1ndez Orallo and Mar\u00eda Jos\u00e9 Ram\u00edrez Quintana 13. Semantic and Linguistic Visual Information: Applications", "authors": "Randy Goebel; Wolfgang Wahlster; Francisco Herrera; Jos\u00e9 Manuel Ben\u00edtez; Colin Fyfe; Moonis Ali; Enrique Herrera-Viedma; Antonio G L\u00f3pez-Herrera; Eduardo Peis; Carlos Porcel; Jose A G\u00e1mez; Jos\u00e9 M Puerta; Jason J Jung; Dariusz Kr\u00f3l; Juan M Corchado; Emilio S Corchado; Dante I Tapia; Edurne Barrenechea; Humberto Bustince; Pedro Couto; Pedro Melo-Pinto; Juan Jos\u00e9 Rodr\u00edguez; C\u00e9sar Garc\u00eda-Osorio; Cognitive Interactive; Cecilio Environments; Juan Angulo;  Antonio-Ortega; Jos\u00e9 Manuel Molina L\u00f3pez; Miguel\u00e1ngel Patricio; Fazel Famili; Jos\u00e9 M Pe\u00f1a; V\u00edctor Robles; And\u00e1ngel Merch\u00e1n; Jes\u00fas Chamorro-Mart\u00ednez; Daniel S\u00e1nchez; Jos\u00e9 Luis Verdegay; Pierre Rouz\u00e8; Acosta S\u00e1nchez; Spain Aguilar; Spain Ajith; Norway Alba; Spain Bae; Y South; Korea Bahamonde; Spain Becerra-Alonso; Spain Barbakh; Palestine Belli; Germany Bello; Cuba Benavides Cu\u00e9llar; Spain Bernad\u00f3-Mansilla; Spain Borzemski; D Charles; Sh.-M Chen; Taiwan Chien; Taiwan Chou; Taiwan Chung; U K Coelho; Brazil Corchado; Spain Corchado; Spain Cord\u00f3n; Spain Cornelis; Belgium Cotta; Spain Da Costa; Portugal Dapoigny; France De Baets; Belgium De Carvalho; Brazil De Melo; Del Jes\u00fas; Marichal Plasencia; Spain Mart\u00ednez; Spain Matthews; Usa Mehrotra; Usa Mel\u00e9ndez; Spain Mizoguchi; Japan Molina; Spain Monostori; Hungary Murphey; Usa Nedjah; Brazil Nguyen; Poland Ohsawa; Japan Okuno; Japan Olivas; Spain Pan; Taiwan Pedrycz; Canada Pelta; Spain Pe\u00f1a; Spain Peregr\u00edn; Spain Pereira De Souto; Brazil Prade; France R-Moreno; Del Valle; C Dorronsoro; J Duro; R Escalera; S Escot; D Esteva; M Euzenat; J Fang; C Fauteux; F Fern\u00e1ndez Caballero; A Fern\u00e1ndez-Luna; J Fern\u00e1ndez-Olivares; J Fern\u00e1ndez; J Flach; P Flores; J Fr\u00edas-Mart\u00ednez; E Garc\u00eda Varea; I Garc\u00eda; J Gasca; R Godoy; D G\u00f3mez-Verdejo; V G\u00f3mez-Vilda; P Gonz\u00e1lez-Abril; L Gonz\u00e1lez; S Gra\u00f1a; M Guadarrama; S Guerra; L Han; Y Herrero; A Herrero; P Hou; W I\u00f1esta; J M Jatowt; A Juli\u00e1n; V Jurado; A Kang; S Kokol; P Ku; W Kuncheva; S Appice; A Aziz; A Barrenechea; E Carmona-Poyato; \u00c1 Ceci; M Chamorro-Mart\u00ednez; J Chen; N Chen; W Coheur; L Couto; ' Amato; C Di Mauro; N Fanizzi; N Fern\u00e1ndez; A Galar; M Garc\u00eda; \u00d3 Gaspar-Cunha; A Xii Conference; Organization Hajiany; A Hern\u00e1ndez-Orallo; J Hurtado Mart\u00edn; G Hwang; G Iglesias Rodr\u00edguez; R Jaffry; S Jiang; X Jung; E Jur\u00edo; Y Klein; M Komatani; K Leng; J Lo; D L\u00f3pez-Molina; C M\u00e1rquez; F Mart\u00ednez-\u00c1lvarez; F Mckenzie; A Medina-Carnicer; R Montero; J Mucientes; M Nakadai; Xi Wang; Oresti Ba\u00f1os; H\u00e9ctor Pomares; Ignacio Rojas; Dan E Tamir; Clara Novoa; Daniel Lowell; Wladimiro D\u00edaz-Villanueva; Francesc J Ferri; Vicente Cerver\u00f3n; Hazra Imran; Aditi Sharan; Juan Garc\u00eda; Francisco Garc\u00eda; Roberto Ther\u00f3n; Francesco Carbone; Jes\u00fas Contreras; Josefa Z Hern\u00e1ndez; Bora \u0130 Kumova; H\u00fcseyin \u00c7ak\u0131r; Rimantas Kybartas; Nurdan Akhan Baykan; Nihat Yilmaz; Sarunas Raudys; Koen W De Bock; Dirk Van Den Poel; Elena Mihaela;  Breaban; Mehmed Kantardzic; Joung Woo; Chamila Walgampaya; Jes\u00fas Maudes; Carlos Pardo; Carlos J Alonso-Gonz\u00e1lez; Q Isaac; Iv\u00e1n Ramos-Mu\u00f1oz; M Ar\u00e1nzazu; \u00d3scar J Prieto; Belarmino Pulido; Ngoc Hoang; Hai Luong; Thi Thanh; Chang Wook Nguyen;  Ahn; Abdesslem Layeb; Abdel Hakim Deneche; Souham Meshoul; Marcus Vin\u00edcius Carvalho Da Silva; Nadia Nedjah; Luiza De Macedo Mourelle; Anabela Moreira Bernardino; Eug\u00e9nia Moreira Bernardino; Juan Manuel S\u00e1nchez-P\u00e9rez; Juan Antonio G\u00f3mez-Pulido; Miguel Angel Vega-Rodr\u00edguez; Rubem Euz\u00e9bio Ferreira; Urszula Markowska-Kaczmar; Filip Krygowski; Cristina Alcalde; Ana Burusco; Ram\u00f3n Fuentes-Gonz\u00e1lez; Manuel-\u00c1ngel Gadeo-Martos; Jose-\u00c1ngel Fern\u00e1ndez-Prieto; Joaqu\u00edn Canada Bago; Juan-Ram\u00f3n Velasco; Ana Bel\u00e9n Cara; Piotr M Marusak; Mar\u00eda Jos\u00e9 Gacto; Rafael Alcal\u00e1; Jos\u00e9 Luis; Luzia Vidal De Souza; Paulo Henrique Siqueira; Ying Wu; Mark Sh Levin; Maxim V Petukhov; Roberto Candela; Giulio Cottone; Giuseppe Fileccia Scimemi; Eleonora Riva Sanseverino; Ali Hmer; Malek Mouhoub; F\u00e1bio Roberto Chavarette; Jos\u00e9 Manoel Balthazar; Ivan Rizzo Guilherme; Orlando Saraiva; Nascimento Junior; Maximiliano Baruto; Frank Hoonakker; Nicolas Lachiche; Alexandre Varnek; Alain Wagner; Ute Schmid; Martin Hofmann; Florian Bader; Tilmann H\u00e4berle; Thomas Schneider; Oscar Luaces; Henrique A Luiz;  Rodrigues; Carlos Alberto Alves; Jos\u00e9 R Quevedo; Antonio Bahamonde; Maciej Drwal; Leszek Borzemski; Paulo J L Adeodato; Petr\u00f4nio L Braga; Adrian L Arnaud; Germano C Vasconcelos; Frederico Guedes; H\u00e9lio B Menezes; Giorgio O Limeira; Ludivine Cr\u00e9pin; Yves Demazeau; Olivier Boissier; Fran\u00e7ois Jacquenet; Elaine Lawrence; John Debenham; C\u00e9lia Da; Costa Pereira; Andrea G B Tettamanzi; Tibor Bosse; Ghazanfar F Siddiqui; Jan Treur; Wei Wan; Jamal Bentahar; Abdessamad Ben Hamza; Alexei Sharpanskykh; Usef Faghihi; Philippe Fouriner-Viger; Roger Nkambou; Pierre Poirier; Lorenzo Ciardelli; Andrea Beoldo; Francesco Pasini; Carlo Regazzoni; Jaime Mart\u00edn; Mario Iba\u00f1ez; Natividad Mart\u00ednez Madrid; Ralf Seepold; Prabhat K Mahanti; Mohammad Al-Fayoumi; Soumya Banerjee; Feras Al-Obeidat; Tony White; Amirali Salehi-Abari; Braden Box; Victoria Nebot; Rafael Berlanga; Chien-Liang Wu; Jia-Ling Koh; Norifumi Hirata; Shun Shiramatsu; Tadachika Ozono; Toramatsu Shintani; Ernesto Ra\u00fal;  Men\u00e9ndez-Mora; Ryutaro Ichise; Ahmed Badreddine; Nahla Ben Amor; Arun Subramanian; Kishan G Mehrotra; Chilukuri K Mohan; Pramod K Varshney; Thyagaraju Damarla; Aida De Haro-Garc\u00eda; Nicol\u00e1s Garc\u00eda-Pedrajas", "pub_date": "", "abstract": "Automatic Query expansion is a well-known method to improve the performance of information retrieval systems. In this paper we have suggested information theoretic measures to improve efficiency of co-occurrence based automatic query expansion. We have used pseudo relevance feedback based local approach. The expansion terms were selected from the top N documents using co-occurrence based approach. They were then ranked using two different information theoretic approaches. First one is standard Kullback-Leibler divergence (KLD). As a second measure we have suggested use of a variant KLD. Experiments were performed on TREC-1 dataset. The result suggests that there is a scope of improving co-occurrence based query expansion by using information theoretic measures. Extensive experiments were done to select two important parameters: number of top N documents to be used and number of terms to be used for expansion.", "sections": [{"heading": "Preface", "text": "The need for intelligent systems technology in solving real-life problems has been consistently growing. In order to address this need, researchers in the field have been developing methodologies and tools to develop intelligent systems for solving complex problems. The International Society of Applied Intelligence (ISAI) through its annual IEA/AIE conferences provides a forum for international scientific and industrial community in the field of Applied Artificial Intelligence to interactively participate in developing intelligent systems, which are needed to solve twenty first century's ever growing problems in almost every field.\nThe 23rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligence Systems (IEA/AIE-2010) held in C\u00f3rdoba, Spain, followed IEA/AIE tradition of providing an international scientific forum for researchers in the field of applied artificial intelligence. The presentations of the invited speakers and authors mainly focused on developing and studying new methods to cope with the problems posed by real-life applications of artificial intelligence. Papers presented in the twenty third conference in the series covered theories as well as applications of intelligent systems in solving complex real-life problems.\nWe received 297 papers for the main track, selecting 119 of them with the highest quality standards. Each paper was revised by at least three members of the Program Committee. The papers in the proceedings cover a wide number of topics including: applications to robotics, business and financial markets, bioinformatics and biomedicine, applications of agent-based systems, computer vision, control, simulation and modeling, data mining, decision support systems, evolutionary computation and its applications, fuzzy systems and their applications, heuristic optimization methods and swarm intelligence, intelligent agent-based systems, internet applications, knowledge management and knowledge based systems, machine learning, neural network applications, optimization and heuristic search, and other real-life applications.\nThe main track was complemented with 13 special sessions whose topics included soft computing in information access systems on the web, data preprocessing in data mining, engineering knowledge and semantic systems, applied intelligent systems for future classrooms, soft computing methods for environmental and industrial applications, soft computing in computer vision and image processing, distributed problem solving with artificial intelligence techniques, ensemble learning, interactive and cognitive environments, context information in intelligent systems, data analysis, optimization and visualization for bioinformatics and neuroscience, industrial applications of data mining and semantic and linguistic visual information.\nTogether, these papers highlight new trends and frontiers of applied artificial intelligence and show how new research could lead to new and innovative applications. They also show that new trends are appearing to cope with the increasingly difficult new challenges that are faced by artificial intelligence. We hope you will find them interesting and useful for your own research.\nThe conference also invited five outstanding scholars to give plenary keynotes speeches. They were Nitesh Chawla, from the University of Notre Dame, USA, Oscar Cord\u00f3n from the European Center for Soft Computing, Spain, Ludmila Kuncheva, from the University of Bangor, UK, Jos\u00e9 Luis Verdegay, from the University of Granada, Spain, and Pierre Rouz\u00e8, from Ghent University, Belgium.\nWe would like to express our thanks to the members of the Program Committee and all the reviewers of the special sessions for their hard work. This work is central to the success of any conference.\nThe conference was organized by the Research Group on Computational Intelligence and Bioinformatics of the University of C\u00f3rdoba jointly with the Soft Computing and Intelligent Information Systems Research Group of the University of Granada in cooperation with the International Society of Applied Intelligence (ISAI).\nWe would like to thank all members of the organization for their unselfish efforts to make the conference a success. We also would like to thank the University of C\u00f3rdoba and its Polytechnic School for their support. We would like to thank Springer for their help in publishing the proceedings. We would like to thank our main sponsors, ISAI, as well as our other sponsors: Association for the Advancement of Artificial Intelligence (AAAI), Association for Computing Machinery (ACM/SIGART), Canadian Artificial Intelligence Association (CA-IAC), European Neural Network Society (ENNS), International Neural Network Society (INNS), Japanese Society for Artificial Intelligence (JSAI), Taiwanese Association for Artificial Intelligence (TAAI), Taiwanese Association for Consumer Electronics (TACE), and Texas State University-San Marcos.\nWe would like to thank the invited speakers for their interesting and informative talks of a world-class standard. We cordially thank all authors for their valuable contributions as well as the other participants in this conference. The conference would not have been possible without their support.\nThanks are also due to the many experts who contributed to making the event a success.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Current information retrieval systems are limited by many factors reflecting the difficulty to satisfy user requirements expressed by short queries. Reformulation of the user queries is a common technique in information retrieval to cover the gap between the original user query and his need of information. The most widely used technique for query reformulation is query expansion, where the original user query is expanded with new terms extracted from different sources. Queries submitted by users are usually very short. Efthimiadis [7] has done a complete review on the classical techniques of query expansion. The main problem of query expansion is that in some cases the expansion process worsens the query performance. Improving the robustness of query expansion has been the goal of many researchers in the last years and most proposed approaches use external collections [8,9,10] to extract candidate terms for the expansion. In our previous work, [12] we have focused on how a thesaurus can be used for query expansion. Query Expansion can be: Manual, semiautomatic and automatic. In corpus-based automatic query expansion the terms to be added to the query can either be selected globally (from the entire document collection) or locally (from top N retrieved documents). Methods based on global analysis are computationally very expensive and its effectiveness is not better than that of methods based on local analysis [32,15,16]. Xu and Croft [17] have suggested the use of local context analysis (LCA) to achieve tradeoff between local and global query expansion. Our work relates to automatic query expansion done locally.\nMost of the automatic query expansion methods use co-occurrence based approach to select the terms for query expansion. However, this is very broad and general approach and all the co-occurring terms don't have equal probability of improving query performance. Therefore, some other measures must be used in order to filter out nonuseful terms and select suitable terms. Selecting suitable query terms is only one step toward improving query performance. In order to optimize query performance some parameters are to be set: number of terms to be added to query, number of top ranked documents used for selecting query terms. In absence of any theoretical justifications these parameters have to be set empirically.\nIn this paper we have suggested some measures to improve efficiency of cooccurrence based query expansion. We have suggested use of information theoretic approaches to rank the co-occurring terms. One of the approaches used is Kullback-Liebler Divergence (KLD) and other is the variant of KLD. Extensive experiments have been done to adjust the parameters (number of terms to be added to query, number of top ranked documents). The results have been compared and analyzed for all the three methods.\nIn the rest of this paper, we first make a review on related work in Section 2. Sections 3 and 4 describe the co-occurrence and information-theoretic approaches, respectively; Section 5 describes our methodology. The experimental results are presented in Section 6 and Section 7 summarizes the main conclusions of this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Early work of Maron [21] demonstrated the potential of term co-occurrence data for the identification of query term variants. Lesk [18] expanded a query by the inclusion of terms that had a similarity with a query term greater than some threshold value of the cosine coefficient. Lesk noted that query expansion led to the greatest improvement in performance, when the original query gave reasonable retrieval results, whereas, expansion was less effective when the original query had performed badly. Sparck Jones [30] has conducted the extended series of experiments on the ZOOdocument subset of the Cranfield test collection. The terms in this collection were clustered using a range of different techniques and the resulting classifications were then used for query expansion. Sparck Jones results suggested that the expansion could improve the effectiveness of a best match searching, if only, the less frequent terms in the collection were clustered with the frequent terms being unclustered and if only, very similar terms were clustered together. This improvement in performance was challenged by Minker et al. [22]. Some work on query expansion has been based on probabilistic models of the retrieval process. Researchers have tried to relax some of the strong assumptions of a term statistical independence that normally needs to be invoked, if probabilistic retrieval models are to be used [4,26]. In a series of papers, Van Rijsbergen had advocated the use of query expansion techniques based on a minimal spanning tree (MST), which contains the most important of the inter-term similarities calculated using the term co-occurrence data and which is used for expansion by adding in those terms that are directly linked to query terms in the MST [13,29,2,31]. Later work compared relevance feedback using both expanded and nonexpanding queries and using both MST and non-MST methods for query expansion on the Vaswani test collection [28,29]. Voorhees [6] expanded queries using a combination of synonyms, hypernyms and hyponyms manually selected from WordNet, and achieved limited improvement on short queries. Stairmand [19] used WordNet for query expansion, but they concluded that the improvement was restricted by the coverage of the WordNet and no empirical results were reported. More recent studies focused on combining the information from both co-occurrence-based and handcrafted thesauri [24,25]. Liu et al. [27] used WordNet for both sense disambiguation and query expansion and achieved reasonable performance improvement. However, the computational cost is high and the benefit of query expansion using only WordNet is unclear. Carmel [5] measures the overlap of retrieved documents between using the individual term and the full query. Previous work [1] attempt to sort query terms according to the effectiveness based on a greedy local optimum solution. Ruch et al. [23] studied the problem in the domain of biology literature and proposed an argumentative feedback approach, where expanded terms are selected from only sentences classified into one of four disjunct argumentative categories. Cao [11] uses a supervised learning method for selecting good expansion terms from a number of candidate terms.", "publication_ref": ["b108"], "figure_ref": [], "table_ref": []}, {"heading": "Co-occurrence Approach", "text": "The methods based on the term co-occurrence which have been used since the 70's to identify the semantic relationships that exist among terms. Van Rijsbergen [2] has given the idea of using co-occurrence statistics to detect the semantic similarity between terms and exploiting it to expand the user's queries. In fact, the idea is based on the Association Hypothesis:\n\"If an index term is good at discriminating relevant from non-relevant documents then any closely associated index term is likely to be good at this.\"\nThe main problem with the co-occurrence approach was mentioned by Peat and Willet [14] who claim that similar terms identified by co-occurrence tend to occur also very frequently in the collection and therefore, these terms are not good elements to be discriminate between relevant and non-relevant documents. This is true when the co-occurrence analysis is done generally on the whole collection but if we, apply it only on the top ranked documents discrimination does occur to a certain extent. We have used the pseudo relevance feedback method where we select top N documents using cosine similarity measures and terms are selected from this set.\nIn order to select co-occurring terms we have used two well-know coefficients:jaccard and frequency, which are as follows.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "_ ( , )", "text": "ij i j i j i j d jaccard co t t d d d = + \u2212 (1)\nWhere d i and d j are the number of documents in which terms t i and t j occur, respectively , and d ij is the number of documents in which t i and t j co-occur.\n, , _ ( , ) ( )\ni j i j d t d t d D freq co t t f f \u2208 = \u00d7 \u2211(2)\nt i = all terms of top N docs terms t j = query terms f d,ti = frequency of term t i in doc f d,tj= frequency of term t j in doc d= top N doc\nWe apply these coefficients to measure the similarity between terms represented by the vectors. However, there is a risk in applying these measures directly, since the candidate term could co-occur with the original query terms in the top documents by chance. The higher its degree is in whole corpus, the more likely it is that candidate term co-occurs with query terms by chance. The larger the number of co-occurrences, the less likely that term co-occur with query terms by chance. In order to reduce probability of adding the term by chance, we use the following equation to measure the degree of co-occurrence of a candidate term with query. N = number of documents in the corpus D = number of top ranked documents used c = candidate term listed for query expansion n c = number of documents in the corpus that contain c co(c,t j ) = number of co-occurrences between c and t j in the top ranked documents i. e jaccard_co(t i ,t j ) or freq_co(t i ,t j )\nTo obtain a value measuring how good c is for whole query Q, we need to combine its degrees of co-occurrence with all individual original query terms 1 2 , ... n t t t . For this suitabilityforQ is computed. To expand a query Q, we rank the terms in the top ranked documents according to their suitability for Q and choose the top ranked terms for query expansion.\nIn general the co-occurrence based approach selects highly frequent co-occurring terms with respect to the query terms. However, good query expansion terms are those terms that are closely related to the original query and are expected to be more frequent in the top ranked set of documents retrieved with the original query than in other subsets of the collection. Information theoretic approaches have been found useful to incorporate above-mentioned idea. Next section deals with use of information theoretic approach for query expansion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Information-Theoretic Approach", "text": "Information theoretic approaches used in query expansion are based on studying the difference between the term distribution in the whole collection and in the subsets of documents that are relevant to the query, in order to, discriminate between good expansion terms and poor expansion term. One of the most interesting approaches based on term distribution analysis has been proposed by Claudio et al. [3], who uses the concept the Kullback-Liebler Divergence to compute the divergence between the probability distributions of terms in the whole collection and in the top ranked documents obtained using the original user query. The most likely terms to expand the query are those with a high probability in the top ranked set and low probability in the whole collection. For the term t this divergence is: \n[ ]( ) ( ) ( ) ( )\nHere P R (t) is the probability of t estimated from the corpus R. P C (t) is the probability of t \u2208V estimated using the whole collection. To estimate P C (t) , we used the ratio between the frequency of t in C and the number of terms in C, analogously to P R (t);\nR C f t ift V R P t N R p t o th erw ise \u03b3 \u03b4 \u23a7 \u2208 \u23aa = \u23a8 \u23aa \u23a9 (( ) ( ) ( ) ( )\n)7\nWhere c is the set of all documents in the collection R is the set of top retrieved documents relative to a query. V(R) is the vocabulary of all the terms in R. NR is the number of terms in R. f(t) is the frequency of t in R\nWe have done our experiments with one more variation in which we have used a function other than f(t)/NR, taking also into account the likely degree of relevance of the documents retrieved in the initial run:\n[ ] \u2211 \u2211\u2211 (8) In order to see the effect of information theoretic measures, we first selected the expansion terms using suitability value (equation 5) then equation (6 and 8) was used to rank the selected terms. For calculating the value of P R (t)(equation 7) we set \u03b3=1, which restricts the candidate set to the terms contained in R. and then the top ranked terms for query expansion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Description of Our Methodology", "text": "We have performed local query expansion based on pseudo relevance feedback. Following are the steps in our methodology.\n1. Indexing -Our system first identified the individual terms occurring in the document collection. 2. Word stemming. To extract word-stem forms, we used porter-stemming algorithm [20]. 3. Stop wording. We used a stop list to delete the common occurring words from the documents. 4. Document weighting. We assigned weights to the terms in each document by the classical tf.idf scheme. 5. Weighting of unexpanded query: To weigh terms in unexpanded query, we used the tf scheme. 6. Document ranking with unexpanded query: We computed a document ranking using common coefficients jaccard between the document vectors and the unexpanded query vector. 7. Listing of candidate terms: We use jacc_coefficient or freq_coefficient using equation (1) or (2) to list out the candidate terms which could be used for expansion. 8. Expansion term ranking: The candidates were ranked by using equation (5) or (6) and top terms were chosen for expansion. 9. Construction of expanded query: We simply added the top terms to the original query. 10. Document ranking with expanded query: The final document ranking was computed by using jaccard coefficient between the document vectors and the expanded query vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "For our experiments, we used volume 1 of the TIPSTER document collection, a standard test collection in the IR community. Volume 1 is a 1.2 Gbyte collection of full-text articles and abstracts. The documents came from the following sources.\nWSJ --Wall Street Journal (1986, 1987, 1988, 1989,1990,1991 and 1992) AP --AP Newswire (1988,1989 and 1990) ZIFF --Information from Computer Select disks (Ziff-Davis Publishing) FR --Federal Register (1988) DOE --Short abstracts from Department of Energy\nWe have used WSJ corpus, and TREC topic set, with 50 topics, of which we only used the title (of 2.3 average word length). In our first approach, equation ( 5) was used for selecting the expansion terms in ranked order. In the second approach, we selected all the terms based on suitability (equation ( 5)) (jaccard_coefficient is used to select the similar terms). These terms were then ranked using KLD measure (equation ( 6)). In a similar way, for the third approach we used a variant of KLD in order to select the subset of terms from the terms selected by suitability value. We have compared the result of all these approaches with that of unexpanded query.\nWe have used different measures to evaluate each method. The measures considered are MAP (Mean Average Precision), Precision@5, Precision@10, and R-Precision. Precision and Recall are general measures to quantify overall efficiency of a retrieval system. However, when a large number of relevant documents are retrieved overall precision and recall values do not judge quality of the result. A retrieval method is considered to be efficient if it has high precision at low recalls. In order to quantify this precision can be calculated at different recall levels. We have calculated Precision@5, Precision@10 recall level.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameter Study", "text": "We have studied two parameters that are fundamental in query expansion: number of candidate terms to expand the query and number of documents from the top ranked set used to extract the candidate terms. The optimal value of these parameters can be different for each method, and thus we have studied them for each case. Following graphs shows the result for different parameter values for each of the methods.  We can observe that in all cases the best value for number of document selected for query expansion is around 10 documents and for the number of query expansion terms is 30. This implies that there is a certain threshold on number of documents and number of query expansion terms to be added in order to improve efficiency of query expansion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparative Analysis of Result", "text": "Table 1 shows overall comparative result for all query expansion methods considered in our work. The parameter values for number of top documents is 10 and number of query terms to be added are 30. From the table we can observe that in general terms selected with suitability ranking are better candidates for query expansion in comparison to standard jaccard and frequency coefficients. We also observed that with the KLD we are able to improve the overall precision (MAP) and recall. In some cases, KLD_variant is able to improve precision@5. By changing various parameters, we may be able to visualize the effect of KLD_variant. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions and Future Works", "text": "In this paper we have suggested the use of information theoretic measures in order to improve efficiency of co-occurrence based automatic query expansion. The experiments were performed on TREC dataset. We have used standard KLD as one of the information theoretic measures and suggested a variant of KLD. We observe that there is a considerable scope of improving co-occurrence based query expansion by using information theoretic measures. More experiments can be done in order to visualize the effect of suggested KLD variant. Further, the other information theoretic measures can be proposed to improve efficiency of automatic query expansion. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In general, an ontology describes formally a domain of discourse. Typically, an ontology consists of a finite list of terms and the relationships between these terms. The terms denote important concepts (classes of objects) of the domain. Relationships typically include hierarchies of classes. A hierarchy specifies a class C to be a subclass of another class C' if every object in C is also included in C'. Apart from subclass relationships, ontologies may include information such as properties, value restrictions, disjoint statements, specifications of logical relationships between objects. We focus on OWL ontologies that distinguish between two main categories of properties: Object properties and Datatype properties. Object properties relate classes in the domain with classes in the range while Datatype properties relate classes in the domain with simple data values in the range. Relationships among classes are given by the Object properties so in order to measure them these properties need to be analysed. In software engineering, Coupling Between Objects (CBO) has been defined in [2] with two variants. CBO-in that measures the number of classes that depend on the class under consideration and CBO-out that measures the number of classes on which the class under consideration depends. This approach has been taken into account to define our metrics but focused on ontologies.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Our Contribution", "text": "In this paper we formally define the metrics that help to analyse and evaluate coupling between classes. These metrics are focused on providing an insight about the importance of the classes according to their relationships and the way they are related. Moreover they are also useful to analyse large ontologies in order to simplify the identification of most coupled classes that represent the main classes.\nWe start this article with a brief introduction of ontologies and coupling, then we discuss some related work in the second part. In the third section we formally define our metrics proposal. The fourth section is used to analyse a case study with a real ontology and finally in the last section we conclude and discuss the future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "There is no so much work about metrics on ontologies. The paper [3] reviews the current state-of-the-art and basically proposes normalization as a pre-process to apply structural metrics. This normalization process consists of five steps: name anonymous classes, name anonymous individuals, classify hierarchically and unify the names, propagate the individuals to the deepest possible classes and finally normalize the object properties. This proposal is focused on content metrics based on OntoMetric framework and basically they have been proposed to improve ontology behaviour or to fix some mistakes. The paper [4] proposes some mechanisms to rank metrics to diverse ontologies. Basically this proposal consists of a Java Servlet to process as inputs some keywords introduced by the user. Then the framework searches using Swoogle 1 engine and retrieves all the URI's representing the ontologies related with these keywords. Then the framework searches on its internal database if these ontologies have been previously analysed and retrieves their information.\nWithout any doubt OntoQA [13][14] represents the main proposal about metrics on ontologies. It proposes some Schema Metrics to measure the richness of schema relationships, attributes and schema inheritance. These metrics are focused on evaluating the ontology in general. Another proposed categories are class richness, average population, cohesion, the importance of a class, fullness of a class, class inheritance and class relationship richness, connectivity and readability. This work describes two similar but not equal metrics. Class Relationship Richness is defined as the number of relationships that are being used by instances that belong to the class. By other hand, the Connectivity of a class is defined as the number of instances of other classes that are connected to instances of the selected class. The main differences are that these metrics take into account the instances belonging to the class instead of relations declared in the class. Some papers related with coupling metrics on software engineering have been published [8][9] [10]. One of the first papers to provide with definitions for coupling in object-oriented systems is [5]. Moreover it also analyses the effects of coupling and defines a coupling metric on an ordinal scale within the framework proposed. Authors take the previous definition of Wand and Weber [7] of CBO (Coupling Between Objects); that was previously defined as a proportional value to the number of non-inheritance related couples with other classes. Based on this definition [6] proposes a new software complexity metrics based on five steps. The first one defines the calculation of modules coupling by counting the number of instances from one class inside other one and vice versa. The second one refers to the calculation of logical coupling while the third step refers to making clusters or sets of classes. The fourth step defines the intersection of two clusters and the final step calculates the average of the intersection rates. Furthermore a visualisation tool has been proposed to visualise the metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Our Proposal", "text": "As we said above relations between classes in an ontology provide us with an insight of the ontology. The direction of a property can be logically considered going from the domain to the range. This direction provides us important information about the role that classes play in the ontology. For instance a class that belongs to the domain of many properties would represent one of the main subjects of the ontology. This is obvious because it implies that this class is being qualified, evaluated or described. In contrast, a class that belongs to the range of one or many properties would represent a qualifier, characteristic or even more important may be used to infer a certain type, and sometimes these classes represent an enumeration of values. It is important to differentiate between both types of classes in an ontology. Furthermore it is also important to distinguish between a property that has the same class belonging to both domain and range.\nOur metrics have been defined to be the analogous counterpart from Object-Oriented Systems to ontologies. We define CBE (Coupling Between Entities) for ontologies with two possibilities. CBE-out representing the coupling where the class belongs to the domain of the property and CBE-in representing the coupling where the class belongs to the range of the property. Furthermore we also define SC (Self Coupling) for those properties that have the same class belonging to both domain and range of the property. Figure 1 shows a diagram where CBE is illustrated over class A. The same class may play different roles in an ontology, nevertheless almost always there are some of them that play specific roles being either subjects or qualifiers, depending on the side they belong to. As consecuence we have formally defined 5 metrics that are listed below: Definitions:\n1. Let \u0398 be an OW L ontology 2. Let \u03a6 be the set of properties \u2208 \u0398 3. Let C be the set of classes \u2208 \u0398 4. \u2203 c \u2208 C 5. \u2203 \u03c1 \u2286 \u03a6 1. We define CBE-out metric as:\nif c \u2208 domain(p) \u2200 p \u2208 \u03c1 then CBE-out = | \u03c1 | 2. We define CBE-in metric as:\nif c \u2208 range(p) \u2200 p \u2208 \u03c1 then CBE-in = | \u03c1 | 3. We define CBE-io metric as:\nif \u2203 p \u2208 \u03c1 and q \u2208 \u03c1 | p is inverse of q then CBE-io = | \u03c1 | /2 4. We define SC (Self Coupling) as:\nif \u2203 c \u2208 domain(p) and c \u2208 range(p) \u2200 p \u2208 \u03c1 then SC = | \u03c1 | 5. Finally we define the Total Coupling value as:", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "TC = CBE-out + CBE-in + CBE-io + SC", "text": "The main advantages of our metrics include: the capacity to detect classes that represent the subject of the ontology and classes that qualify, to be able to discern between both. Moreover metrics tell us information about how coupled classes in the ontology are, resulting in a low or high coupled ontology. We consider that high coupled ontologies would be desirable because low coupling would imply that classes are not related each other except for ISA relationships.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Case Study", "text": "According to our definition we implemented these metrics using Java Programming Language and Jena API 2 to manage OWL ontologies. We selected Semantic Web Technology Evaluation Ontology (SWETO) version 1.4 3 to be analysed [12]. This general-purpose ontology was developed in 2004 by the Computer Science Department at the University of Georgia. This SWETO version consists of 114 classes and 13 object properties. Table 1 shows the result of applying our metrics to this ontology and all the classes that have at least 1 coupling value. \nP lace 0 0 3 0 0 3 AcademicDepartment 0 0 2 0 0 2 Event 1 1 1 0 0 2 Researcher 1 1 0 0 0 1 University 1 1 0 0 0 1 P rof essor 1 1 0 0 0 1 Scientif icP ublication 1 1 0 0 0 1 T hing 1 1 0 0 0 1 Country 0 0 1 0 0 1 Classif ication 0 0 1 0 0 1\nThe simple analysis of SWETO coupling metrics has shown interesting results. Even having no background information about the ontology, we can clearly deduce the most coupled class also represents the most important one, in this case 'Person'. This is an obvious result because if this class contains most of the object properties then it represents the main ontology's subject. Furthermore this fact can be supported by the fact that all the relationships are in the CBEout metric which means that this class belongs to the domain in all the object properties. This fact implies these object properties were defined having as main purpose to satisfy some needs for this specific class. On the other hand, classes that only have CBE-in values such as Place, Academic Department, Country or Classification belong to the range in the properties. It means that individuals of these classes represent all the possible values the properties can take in the range. Moreover these individuals represent qualifiers for other entities. Analysing CBE-io metric we realise that there are no values greater than 0. It means there are no inverse functions declared in the ontology. Moreover there is only one class with self coupling value (Publication); meaning that exists one property which has the same class in the domain and range. This analysis let us to have an insight into the behaviour of the ontology even without previous knowledge background of it. The main purpose is to relate persons with places, organizations or academic departments. Furthermore more specialized classes such as Professor or Researcher are related with Scientific Publication. We can deduce from this ontology that it shows low coupling between classes. Just 13 out of 114 classes are related, which represents less than 10 percent of the total.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions and Future Work", "text": "We have provided a formal definition of some helpful metrics to analyse the coupling between classes in an ontology. These metrics are based in the coupling metrics for software engineering field but modified to satisfy an ontology needs. We have analysed a case study using a public ontology focusing on the advantages of using our metrics. Our metrics let us discover the most significant classes in the ontology according to the way they interact with others as well as to have an insight of the role played by the individuals. Finally the future work will include the definition of more metrics as well as a visualisation to represent object and datatype properties, classes and coupling between them. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In recent years computer science has faced more and more complex problems related to information creation and fruition. Applications in which small groups of users publish static information or perform complex tasks in a closed system are not scalable and nowadays are out of date. In the last years a new paradigm changed the way Web applications are designed and used: Web 2.0 [1] represents 'THE' Web collaborative paradigm (according to O'Reilly and Battelle [2]) and can be seen as architecture of participation where users can contribute to the website content creation with network effects. The collaborative paradigm leads to the generation of large amounts of content and when a critical mass of documents is reached, information becomes unavailable. Knowledge and information management are not scalable unless formalisms are adopted. Semantic Web's aim is to transform human readable content into machine readable [3]. With this goal data interchange formats (e.g. RDF/XML, N3, Turtle, N-Triples), and languages such as RDF Schema (RDFS) and the Web Ontology Language (OWL) have been defined.\nComputer supported collaborative work research analyzed the introduction of Web 2.0 in corporations: McAfee [4] called \"Enterprise 2.0\" a paradigm shift in corporations towards the 2.0 philosophy: collaborative work should not be based in the hierarchical structure of the organization but should follow the Web 2.0 principles of open collaboration. This is especially true for innovation processes when the open innovation paradigm is adopted (Chesbrough [5]). In a world of widely distributed knowledge, companies do not have to rely entirely on their own research, but should open the innovation to all the employees of the organization, to providers and customers.\nIn this paper we discuss how open innovation can be supported by Web 2.0 and semantic technologies demonstrating how these technologies increase efficiency and effectiveness. Three real systems have been implemented with this approach for three international corporations, in financial 1, energy 2 and telecommunication 3 fields with an average of three thousands employees involved in the innovation processes of each firm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Innovation Meets Enterprise 2.0 and Semantic Technologies", "text": "In a scenario in which collaborative work is not supported and members of the community can barely interact with others, solutions to everyday problems and organizational issues rely on individual initiative. Innovation and R&D management are complex processes for which collaboration and communication are fundamental. They imply creation, recognition and articulation of opportunities, which need to be evolved into a business proposition in a second stage. The duration of these tasks can be drastically shortened if ideas come not just from the R&D department. This is the basis of the open innovation paradigm which opens up the classical funnel to encompass flows of technology and ideas within and outside the organization. Ideas are pushed in and out the funnel until just a few reach the stage of commercialization. Technologies are needed to support the opening of the innovation funnel, to foster interaction for the creation of ideas (or patents) and to push them through and inside/outside the funnel. Platt [6] states that innovation, together with marketing and training, is one of the processes which could most benefit from the introduction of Enterprise 2.0 tools in a company. Gloor [7] defines the \"Collaborative Innovation Networks\" as \"a cyberteam of self-motivated people with a collective vision, enabled by the Web to collaborate in achieving a common goal by sharing ideas, information, and work\". The technology framework identified by Gloor has to grant a high degree of interactivity, connectivity and sharing. All these characteristics can be identified in an Enterprise 2.0 environment where editing and creating documents is easier and interaction and collaboration are key.\nOn the other hand, as we stated, performing analysis and structuring the information easily becomes unaffordable and data unavailable. Web 2.0 tools do not have formal models that allow the creation of complex systems managing large amounts of data. For this reason it is possible to improve any of the six components of Enterprise 2.0 technologies described by McAfee [4] by the introduction of semantic analysis of the content: \u2212 Search: search should not be limited to documents. It should also consider people, their interests and their knowledge. \u2212 Links: links between documents are important but links between contents could be more powerful. Identifying semantically related content helps users in the creation and discovery of contents. Semantic links between people sharing the same knowledge should also be considered. \u2212 Authoring: any Enterprise 2.0 platform should rely on solid mechanisms for an organic growth of the contents and tools for an easy and user-friendly content creation have to be provided. Semantic tools for automatic information discovery during the process of creation help in this task. \u2212 Tags: solutions like folksonomies (folk's taxonomies), collaborative tagging and social tagging are adopted for collaborative categorization of contents. In this scenario we have to face the problem of scalability and interoperability [8]: making users free to use any keyword is very powerful but this approach does not consider the natural semantic relations between the tags. \u2212 Extensions: Recommendations systems are key when the user faces a large amount of contents. Automatic recommendations (\"Users like you also like\") could benefit from semantic analysis of contents. Through the semantic analysis of documents created by a specific user it is possible to recommend semantically similar documents. \u2212 Signals: Users can easily feel overwhelmed by the large amount of information. It is necessary to introduce technology to signal users when new content of interest appears. RSS allow users to subscribe to sources. Semantic technology allows the creation of automatic alerts for new interesting content based on semantic analysis.\nSemantic Web can contribute introducing computer-readable representations for simple fragments of meaning. As we will see in the following paragraphs, an ontologybased analysis of a plain text provides a semantic contextualization of the contents, supports tasks such as finding semantic distance between contents and helps in creating relations between people with shared knowledge and interests. Both Web 2.0 principles and semantic technologies can be naturally applied to the innovation process for the following reasons:\n\u2212 Communication is essential for knowledge creation: semantic technologies facilitate information exchange; \u2212 Knowledge is created and shared within complex processes; \u2212 Knowledge can still be unavailable to some members of the community due to the dimension of the organization; \u2212 Innovation needs communities composed of heterogeneous groups of members with different skills and interests: Web 2.0 is based on cooperative work; \u2212 Information flows are associated with knowledge creation. This flow contains information about expertise, relationships among members etc. If a specific tool does not handle this data, it is easily lost: semantic technologies support access to unstructured data.\nThanks to the adoption of these technologies an innovation culture is created in the firm; while the support of collaborative work, talent management and proposal evaluation provides the enhancement of efficacy and efficiency of the innovation process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Actors and Tools in the Corporate Scenario", "text": "Enterprise 2.0 technologies have the potential to usher in a new era by making both the practices of knowledge work and its output more visible. Lee [9] states that to increase the participation in online communities is necessary to create a common space for collaboration, to have a common ground, to make members of the community aware of the presence and work of other actors, and introduce interaction enablers and mechanisms. Therefore, a Web 2.0 shared platform to perform interaction has to be introduced: blogs, forums and wikis are the best environment to support the exchange of ideas, create new knowledge and suggest solutions and improvements for the organization. Here the \"content\" is stored. In addition, a repository of knowledge, such as an ontology, is required. The ontology provides a formal description of the domain in which community members interact. Terms in the ontology represent the bridge between people and contents: the detection of semantic relationships between contents and people through the identification of concepts is the core of collaboration support. Moreover, the adoption of a reward system is key to involve people in the innovation process. Money is not the sole motivating factor. There may be other factors such as prestige and ego. A company could collaborate in another firm's innovation process as a marketing strategy, in order to gain public recognition as an \"innovative partner\". Technology has to support the innovation process in this aspect as well, helping decision makers in the enterprise to evaluate the ideas and to reward the members of the community.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Innovation Process Stages and Idea Lifecycle", "text": "The innovation process could be divided in six stages: creation, filtering, decision making, acknowledgment, implementation and exploitation.\nDuring the creation phase a member of the community creates a new idea. The filtering is carried out by the collaborative work of the community (editing, voting and commenting on existing ideas). A group of experts should then evaluate the idea before the innovation managers decide whether to implement the idea or to hibernate it. For the ideas which are going to be implemented the author(s) receive social acknowledgment and, depending on the organization, financial reward. Our analysis focuses on the three first stages in which a technological framework is needed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 2. Innovation stages, idea lifecycle and the corresponding interaction layers", "text": "The \"idea lifecycle\" is the process the ideas go through during the innovation stages: a draft is uploaded into a collaboration platform (such as a wiki, blog or a forum could be) where it can be related and compared to other ideas (contextualization), modified, voted and commented on (refinement) and finally evaluated (hibernation or implementation). For each of these stages it is possible to identify different types of interaction we can organize in layers: \u2212 Individual Layer: a user or a user group sketches a draft not integrated or related to other proposals. There is no interaction with the rest of the community. \u2212 Indirect Interaction Layer: the idea is contextualized and introduced into the interaction platform. The platform can support the authors to discover similar proposals and integrate their idea with others, so that they interact with the community in an indirect way through the ideas previously proposed. \u2212 Direct Interaction Layer: collaboration between the members of the community is actively supported. The authors may consult other members to refine their proposal, and community members are invited to comment and modify the idea. People with common interests are put in touch for creating new proposals. If the idea matures enough it is evaluated. Evaluation is managed from the platform and performed by a group of experts selected from the community to assess specific aspects, such as technical or commercial viability. The outcome of this process will help decision makers in the R&D department to select the ideas to be finally implemented.\nEach layer requires a technological framework in order to support and improve collaboration between the members of the community, the evaluation and the decision.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Individual Layer", "text": "Adopting a market metaphor, people are creators and consumers of contents which can be stored in a semi-structured workspace, such as a wiki. Wiki pages can be easily created and updated and foster collaboration in document creation. Contents are characterized by the concepts they contain. The first two basic functionalities we identify for collaboration support consist in finding (i) \"what\" a specific content is about and (ii) \"what\" a member of the community consumes and generates. In this way, concepts are associated with contents and concepts with people. Creators know what they write; consumers are interested about what they read.\nIn order to perform this task, manual tagging could be used (defining this way a folksonomy) but semantic tools for automatic concept recognition grant a more formal identification of the content of the document: the user receives suggestions about the identified concepts in the text and is free to choose which ones better describe the content. Concept detection in contents and about people characterizes the individual layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Indirect Interaction Layer", "text": "By \"indirect interaction\" we mean the creation and consumption of content, which does not require direct collaboration. In this context, members of the community proactively decide to participate in the community life and the system only supports their tasks. The functionalities we identify for this layer are: (i) search and detection of concept-related contents, (ii) discovery of personal interest and (iii) discovery of personal knowledge.\nThe information about the \"meaning\" of contents leads directly to the discovery of \"concept-related\" contents. Once again, the knowledge stored in the ontology allows a semantic comparison between contents. A simple keywords comparison between two texts would provide a measure given by the number of common terms. Adopting a semantic approach, not just the appearance of the same terms is considered, but also synonymy and semantic distance in the ontology is taken into account in order to establish the degree of similarity. For example, a text about \"financial products\" and another about \"hedge funds\" are somehow related. The distance given by the hierarchy and the relations between terms of the ontology reveals how similar the two concepts are. The higher the distance, the lower the degree of similarity is. We will describe in the next paragraph how this distance is calculated.\nDetection of related content is necessary whether new content is created or consumed. The process of generating new contents takes into account ideas already introduced by other members of the community in order to avoid duplicated information and to help in the edition of new contents.\nConcepts are grouped in \"knowledge areas\" in the ontology and we can say that people and contents are related not just to single terms but have a more wide relation to knowledge areas of the corporation. Knowledge areas are sets of concepts of the ontology, and can have intersections and be subsets of each other. Knowing which concepts a person handles and knowing which concept belongs to which knowledge area it is possible to calculate how a person (or content) is related to an area. Information retrieval algorithms can be applied for determining which knowledge areas are involved in a text.\nConcepts related to a person reveal important information about the skills, knowledge and interests of people and help in talent management. A content consumer is making explicit her interest about specific concepts or specific knowledge areas. A content creator demonstrates her knowledge and skills whenever she writes a new proposal. The knowledge is given by the set of terms this person writes, while interests are the set of words a person reads. The definition of knowledge areas is part of the modeling of the domain and is included in the ontology.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Direct Interaction Layer", "text": "It is not always possible to rely on the proactive impulse of the members of the community to collaborate without an external influence. Semantic technologies can be used to implement functionalities to involve passive members in the cooperative community life. Content creators are asked to rate recently generated content covering topics similar to the ones they are semantically related to.\nFurthermore, the creation of content should be a collaborative act: while the user introduces text in the interaction platform, not just related ideas are shown in real time but also people with related knowledge. Handling knowledge (and interests) and sets of words allow the comparison of people and ideas as we can compare plain texts: tools to calculate semantic distance used for discovering similar ideas are still valid. By tracking user content consumption we obtain the set of words, which represents her interests. This data is used to inform a person about new interesting content.\nThe number of co-authorships, revisions, comments to each proposal provides an impact index of an idea and a map of the relations between members of the community.\nA feedback system is included in the interaction platform allowing members of the community to rate ideas. This tool becomes very important if the discovery of new experts of specific knowledge areas is to be achieved: a person who produces valuable content about a topic (a knowledge area) is a potential new expert in the area. In conclusion, we identified two main functionalities in this layer: (i) content rating and broadcasting and (ii) updating of the community members' interactions map (detection of people with related interests or knowledge, discovery of new potential experts, etc).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Issues", "text": "In this section we give some details about the implementation of the three main components of the model: the interaction platform, the ontology and the semantic engine for text comparison.\nThe interaction platform is a web application designed following the Web 2.0 principles of participation and usability. Every employee of the three different enterprises where the system has been adopted has access to the company innovation platform. Challenges for innovation about a specific topic are proposed by the R&D department. Employees can participate in the challenge and compete for a prize for the best idea or proactively propose an idea not related to any active challenge. An innovation committee is in charge of taking the final decision about which ideas to implement (and reward) or hibernate at a given deadline. An idea is similar to a wiki entry which any member of the community is free to edit. If the blog philosophy is adopted, only the authors can edit the idea. Versions, changes and information about the users involved in the definition of the idea are kept for the evaluation of the contribution of every author in case a reward is given. Users can also comment or vote for ideas. Interactions between community members (co-authoring, voting, or writing a comment) are stored in order to draw an interaction map for each user in her personal space where other information such as related people or new interesting ideas identified by the system is presented.\nThe semantic functionalities are implemented in a three layered architecture as shown in Figure 3: ontology and ontology access is the first layer, keyword to ontology entity mapping is the second and the last layer is semantic indexing and search.", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Fig. 3. The semantic architecture", "text": "For the three different companies, three ontologies, each one modeling the specific business field, have been implemented in RDF/RDFS. Knowledge engineers and domain experts worked together to define concepts and relations in the ontologies. General aspects, such as \"product\", \"service\", \"process\", \"customer\" are common terms for the three ontologies, while specific terms have been added for each case. Ontologies are accessed through the Sesame RDF framework 4 .\nAn engine to map keywords to ontology entities has been implemented in order to detect which terms (if any) in the text of an idea are present in the ontology. For this task we consider: morphological variations, orthographical errors and synonyms (for the terms defined in the ontology). Synonyms are manually defined by knowledge engineer and domain experts as well.\nThe indexes and the search engine are based on Lucene 5 . Three indexes have been created: ideas index, user interests index (using the text of the ideas the user reads) and user knowledge interests index (using the text of the ideas the user writes). Each index contains terms tokenized using blank space for word delimitation and ontology terms as single tokens (e.g. if the text contains \"credit card\" and this is a term of the ontology, \"credit\", \"card\" and \"credit card\" are added as tokens to the index). When we look for related ideas to a given one the following tasks are executed: \u2212 extraction of the text of the idea for using it as a query string; \u2212 morphological analysis; \u2212 ontology terms identification (considering synonyms); \u2212 query expansion exploiting ontological relations. If a synonym of an ontology term is detected, the ontology term is added to the query. If a term corresponding to an ontology class is found, subclasses and instances labels are used to expand the query. If an instance label is identified, the corresponding class name and sibling instance labels are added to the query. Different boosts are given to the terms used for each different query expansion.\nThe same tasks are performed for searches of related people. For expert detection, semantic search results are filtered with statistical results about successful ideas.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Issues", "text": "In order to evaluate the impact of the introduction of this new paradigm and technologies new metrics should be considered. The number of new products and the number of registered patents gives the measure of the success of the R&D department in a closed model. In an open innovation scenario, time to market and the number of successful proposals (or patents in use) has to be evaluated. Our experience in the three cases of studies has not yet allowed a deep analysis of the financial outcome and detailed statistics cannot be given. Nevertheless the adoption of the model did cause an increase in the number of proposals generated from many different departments of the firms. It is also important to highlight the high collaboration rate in the proposals definition: the Web 2.0 environment helped in establishing an innovation culture in the firms, while the semantic technologies helped not just in fostering interaction for the creation of new ideas, but also in supporting the decision process. The higher number of proposals is balanced by the collaborative filtering of the community (through the rating system and experts evaluation) and only valuable ideas reach the last stage of decision making. The impact index pointed out the most debated proposals offering a pulse of the interests of the community. Talent management profited from all the information given about created and consumed contents by every member of the community. R&D experts in the firms perceived a more efficient and effective innovation process and some of the information obtained thanks to the use of the system was shared with Knowledge Management and Human Resources units.\nIn two cases (in the bank and in the energy firm), departments other than R&D are studying the introduction of this model for improving collaboration and knowledge exchange in their business unit.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "This paper introduces real experiences of the combination of collaborative technology (Web 2.0 like) and knowledge technology (Semantic Web like) to develop successful solutions for supporting knowledge intensive business processes. This paper described the actors and tools involved in a three-layered interaction model for open innovation in which Web 2.0 and semantic technologies support knowledge management, cooperation and evaluation of the proposals. We have been able to measure the impact of these technologies in the innovation process at its first stages: collaboration increased and the evaluation process is perceived as more effective (according to the theory of the wisdom of crowds). Our future work includes the refinement of the model, a deeper analysis of the outcome after the adoption of this model and the definition of more exact metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The first studies on syllogisms were pursued in the field of right thinking by the philosopher Aristotle [1]. His syllogisms provide patterns for argument structures that always yield conclusions, for given premises. Some syllogisms are always valid for given valid premises, in certain environments. Most of the syllogisms however, are always invalid, even for valid premises and whatever environment is given. This suggests that structurally valid syllogisms may yield invalid conclusions in different environments.\nGiven two relationships between the quantified objects P, M and S, a syllogism allows deducing a quantified transitive object relationship between S and P. Depending on alternative placements of the objects within the premises, 4 basic types of syllogistic figures are possible. Aristotle had specified the first three figures. The 4. figure was discovered in the middle age. In the middle of the 19 th century, experimental studies about validating invalid syllogisms were pursued. For instance, Reduction of a syllogism, by changing an imperfect mood into a perfect one [13]. Conversion of a mood, by transposing the terms, and thus drawing another proposition from it of the same quality [11], [10].\nAlthough shortly thereafter syllogism were superseded by propositional logic [7], they are still matter of research. For instance philosophical studies have confirmed that syllogistic reasoning does model human reasoning with quantified object relationships [2]. For instance in psychology, studies have compared five experimental studies that used the full set of 256 syllogisms [4], [12] about different subjects. Two settings about choosing from a list of possible conclusions for given two premisses [5], [6], two settings about specifying possible conclusions for given premisses [8], and one setting about decide whether a given argument was valid or not [9]. It has been found that the results of these experiments were very similar and that differences in design appear to have had little effect on how human evaluate syllogisms [4]. These empirically obtained truth values for the 256 moods are mostly close to their mathematical truth ratios that we calculate with our algorithmic approach.\nAlthough the truth values of all 256 moods have been analysed empirically, mostly only logically correct syllogisms are used for reasoning or modus ponens and modus tollens, which are generalisations of syllogisms [14]. Uncertain application environments, such as human-machine interaction, require adaptation capabilities and approximate reasoning [16] to be able to reason with various sorts of uncertainties. For instance, we know that human may reason purposefully fallacious, aiming at deception or trickery. Doing so, a speaker may intent to encourage a listener to agree or disagree with the speaker's opinions. For instance, an argument may appeal to patriotism, family or may exploit an intellectual weakness of the listener. We are motivated by the idea for constructing a fuzzy syllogistic system of possibilistic arguments for calculating the truth ratios of illogical arguments and approximately reason with them.\nThis paper presents an algorithm for deciding syllogistic cases, for algorithmically calculating syllogistic reasoning and an application to automated reasoning. Firstly, categorical syllogisms are discussed briefly. Thereafter an arithmetic representation for syllogistic cases is presented, followed by an approach for algorithmically deciding syllogisms and a possible application for recognising fallacies and reasoning with them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Categorical Syllogisms", "text": "A categorical syllogism can be defined as a logical argument that is composed of two logical propositions for deducing a logical conclusion, where the propositions and the conclusion each consist of a quantified relationship between two objects.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Syllogistic Propositions", "text": "A syllogistic proposition or synonymously categorical proposition specifies a quantified relationship between two objects. We shall denote such relationships with the operator . Four different types are distinguished {A, E, I, O} (Table 1):\n\u2022 A is universal affirmative: All S are P \u2022 E is universal negative: All S are not P \u2022 I is particular affirmative: Some S are P \u2022 O is particular negative: Some S are not P One can observe that the proposition I has three cases (a), (b), (c) and O has (a), (b), (c). The cases I (c) and O (c) are controversial in the literature. Some do not consider them as valid [3] and some do [15]. Since case I (c) is equivalent to proposition A, A becomes a special case of I. Similarly, since case O (c) is equivalent to proposition E, E becomes a special case of O. At this point we need to note however that exactly these cases complement the homomorphic mapping between syllogistic cases and the set-theoretic relationships of three sets. This is discussed below.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Syllogistic Figures", "text": "A syllogism consists of the three propositions major premise, minor premise and conclusion. The first proposition consist of a quantified relationship between the objects M and P, the second proposition of S and M, the conclusion of S and P (Table 2). Since the proposition operator may have 4 values, 64 syllogistic moods are possible for every figure and 256 moods for all 4 figures in total. For instance, AAA-1 constitutes the mood MAP, SAM -SAP in figure 1. The mnemonic name of this mood is Barbara, which comes from syllogistic studies in medieval schools. Mnemonic names were given to each of the in total 24 valid moods, out of the 256, for easier memorising them [3].  We shall denote a propositional statement with i, in order to distinguish between possibly equal propositional operators of the three statements of a particular mood, where i {1, 2, 3}.\nA further consequence of including the above mentioned cases I (c) and O (c) in our algorithmic approach is that the number of valid moods increases with AAO-4 from 24 to 25. Since no mnemonic name was given to this mood in the literature by now, name it herewith \"anasoy\".", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_5"]}, {"heading": "Algorithmic Representation", "text": "In the following our approach for algorithmically deciding any given syllogistic mood is presented. Algorithmically analysing all 2624 truth cases of the 256 moods enables us to calculate all mathematical truth values of all moods, sort the moods according their truth values and define a fuzzy syllogistic system of possibilistic arguments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Set-Theoretical Analysis", "text": "For three symmetrically intersecting sets there are in total 7 possible sub-sets in a Venn diagram (Fig 1). If symmetric set relationships are relaxed and the three sets are named, for instance with the syllogistic terms P, M and S, then 41 set relationships are possible. These 41 relationships are distinct, but re-occur in the 256 moods as basic syllogistic cases. The 7 sub-sets in case of symmetric relationships and the 41 distinct set relationships in case of relaxed symmetry are fundamental for the design of an algorithmic decision of syllogistic moods. We have pointed out earlier that, including the cases I (c) and O (c) of the syllogistic propositions I and O, is required by the algorithm to calculate correctly. Without these cases, the algorithm presented below, cannot decide some cases of some moods or cannot find valid moods at all. For instance, as valid moods in figure I, only AAA, AAI, AII and EAE can be found by the algorithm, although EAO and EIO are also true. If the algorithm considers the cases I (c) and O (c), then all 6 valid moods of figure I are found. The reason for that is that the syllogistic propositions are basically a symmetric sub-set of the in total 12 distinct set relationships between two named sets. Therefore the cases I (c) and O (c) are required to complement the symmetric relationships between the syllogistic propositions.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Arithmetic Representation", "text": "Based on theses 7 sub-sets, we define 9 distinct relationships between the three sets P, M and S (Table 3). These 9 relationships are mapped homomorphically onto the 9 arithmetic relations, denoted with 1, ..., 9. For instance P\u2229M is mapped onto 1=a+e and P-M is mapped onto 4=f+b. These relationships can be verified visually in the Venn diagram (Fig 1).\nOne can observe that the symmetric relationship between the three sets (Fig 1) is preserved in the homomorphically mapped arithmetic relations (Table 3). The above homomorphism represents the essential data structure of the algorithm for deciding syllogistic moods.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_20", "tab_20"]}, {"heading": "Table 3. Homomorphism between the 9 basic syllogistic cases and 9 arithmetic relations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithmic Decision", "text": "The pseudo code of the algorithm for determining the true and false cases of a given moods is based on selecting the possible set relationships for that mood, out of all 41 possible set relationships.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DETERMINE mood", "text": "READ figure number {1,2,3,4} READ with 3 proposition ids {A,E,I,O} GENERATE 41 possible set combinations with 9 relationships into an array setCombi [41,9]={{1,1,1,1,1,1,1,1,1}, ..., {0,1,0,0,1,1,1,1,1}} VALIDATE every proposition with either validateAllAre, validateAllAreNot, validateSomeAreNot or validateSomeAre DISPLAY valid and invalid cases of the mood VALIDATE mood validateAllAre(x,y) //all M are P if(x=='M' && y=='P') CHECK the sets suitable for this mood in setCombi if 1=1 and 2=0 then add this situation as valid if(setCombi [i][0]==1 && setCombi[i][1]==0) //similar for validateAllAreNot(), validateSomeAre(), validateSomeAreNot()", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Statistics about the Syllogistic System", "text": "The introduced algorithm enables revealing various interesting statistics about the structural properties of the syllogistic system. Some of them are presented now. Since our objective is to utilise the full set of all 256 moods as a fuzzy syllogistic system of possibilistic arguments, we have first calculated the truth values for every mood in form of a truth ration between its true and false cases, so that the truth ratio becomes a real number, normalised within [0, 1]. Thereafter we have sorted all moods in ascending order of their truth ratio (Fig 2). Note the symmetric distribution of the moods according their truth values. 25 moods have a ratio of 0 (false) and 25 have ratio 1 (true). 100 moods have a ratio between 0 and 0.5 and 100 have between 05 and 1. 6 moods have a ratio of exactly 0.5.\nEvery mood has 0 to 21 true and 0 to 21 false cases, which is a real sub-set of the 41 distinct cases. The total number of true or false cases varies from one mood to another, from 1 to 24 cases. For instance, mood AAA-1 has only 1 true and 0 false cases, whereas mood OIA-1 has 3 true and 21 false cases. Hence the truth ratio of AAA-1 is 1 and that of OIA-is 3/21=1/7. The algorithm calculates 2624 syllogistic cases in total, since all cases of the 256 moods map the 41 distinct cases multiple times. Interesting is also that for any given figure the total number of all true cases is equal to all false cases, ie 328 true and 328 false cases. Thus we get for all 4 syllogistic figures the total number of 4 x 2 x 328 = 2624 cases. More statistical details will be discussed in a separate work.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Fuzzy Syllogistic System", "text": "Based on the structural properties of the syllogistic system, we elaborate now a fuzzified syllogistic system.\nOne can see (Fig 2) that every syllogistic case is now associated with an exact truth ration. We utilise the symmetric distribution of the truth ratios, for defining the membership function FuzzySyllogisticMood(x) with a possibility distribution that is similarly symmetric (Fig 2). The linguistic variables were adopted from a meta membership function for a possibilistic distribution of the concept likelihood [17]. The complete list with the names of all 256 moods is appended (Table A1).\nAs we have mentioned earlier, the algorithmically calculated truth ratios of the 256 moods (Fig 2) mostly comply with those empirically obtained truth ratios in psychological studies [4]. Hence the suggested possibilistic interpretation should reflect an approximately correct model of the syllogistic system.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15", "fig_15"], "table_ref": ["tab_2"]}, {"heading": "Unlikely (true<false)", "text": "(100) Likely (false<true) (100) Uncertain (6) Certainly ( 25) CertainlyNot ( 25) Fig. 2. 256 syllogistic moods sorted in ascending order of their truth ratio true/false, if number of truth cases of a mood is true<false and false/true ratio, if false<true. Definition of the possibility distribution FuzzySyllogisticMood(x) with the linguistic variables CertainlyNot, Unlikely, Uncertain, Likely, Certainly and their cardinalities 25, 100, 6, 100, 25, respectively.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Recognising Fallacies and Fuzzy Syllogistic Reasoning", "text": "In logic a fallacy is a misconception resulting from incorrect reasoning in argumentation. 7 syllogistic fallacies are known in the literature:\n\u2022 Equivocation fallacy or fallacy of necessity: Unwarranted necessity is placed in the conclusion, by ignoring other possible solutions. \u2022 Fallacy of undistributed middle: Middle term must be distributed in at least one premiss. \u2022 Illicit major/minor: No term can be distributed in the conclusion, which is not distributed in the premisses. \u2022 Fallacy of exclusive premisses: Two negative premisses.\n\u2022 Affirmative conclusion from negative premiss: Positive conclusion, but at least one negative premiss. \u2022 E xistential fallacy: Two universal premisses, but particular conclusion.\nThese fallacies comply exactly with the 7 rules for eliminating invalid moods, which were discovered already by Aristotle [1].\nOur objective is to use the whole set of 256 syllogistic moods as one system of possibilistic arguments for recognising fallacies and reasoning with them. For that purpose, we specify the following steps:\n\u2022 Calculate all truth cases and truth ratio of a given mood.\n\u2022 Try to recognise fallacies by + identifying false or true possibilities: reduction of A to I or E to O, respectively + generalising true or false possibilities: generalisation of I to A or O to E.\n\u2022 Try to map the initial mood to a mood with a truth ratio closer to 1.\n\u2022 Approximately reason with the truth ratios.\nWe will now discuss these steps experimentally on the following example (Fig 3). Firstly, we calculate the 3 true (Fig 4) and 3 false (Fig 5) cases of mood AIA-1 and its truth ratio of 0.5. Secondly, we identify following fallacies:\n\u2022 1 (A):\nSimply not all stories in The Child's Magic Horn are sad. The truth is that only some stories in The Child's Magic Horn are sad 1 (I). \u2022 3 (A): Not all stories I cry at are stories in The Child's Magic Horn, because I will possibly cry at some other stories as well. The truth is that only some of all the stories I cry at are stories in The Child's Magic Horn 3 (I).\nThirdly, based on the identified fallacies and reductions to 1 (I) and 3 (I), we can easily calculate the mood III-1 to be \"more true\" for the given sample propositions. In dead, mood III-1 has with 4 false/19 true cases = 0.73, a better truth ratio.\nIn the last step, we may use the truth ration of the mood for fuzzy syllogistic reasoning as a model for approximate reasoning with quantified propositions.", "publication_ref": [], "figure_ref": ["fig_28", "fig_29", "fig_116"], "table_ref": []}, {"heading": "P:", "text": "Sad A: all M are P M:\nStories in The Child's Magic Horn I: some S are M S:\nTales I cry at A: all S are P 1 (A): All \"Stories in The Child's Magic Horn\" are \"Sad\" 2 (I): Some \"Tales I cry at\" are \"Stories in The Child's Magic Horn\" 3 (A): All \"Tales I cry at\" are \"Stories in The Child's Magic Horn\" ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented an algorithmic approach for analysing the syllogistic system. The algorithm facilitates structurally analysing the syllogistic moods and reveals interesting statistics about the truth cases of the moods. First experimental results show that the syllogistic system is inherently symmetric.\nWith the membership function FuzzySyllogisticMood(x) we have proposed a fuzzy syllogistic system of possibilistic arguments and its possible application for recognising fallacies and fuzzy syllogistic reasoning.\nFuture work shall aim at systematically revealing all significant statistical properties of the syllogistic system, by using the algorithm. We believe that this approach may prove a practical approach for reasoning with inductively learned knowledge, where P, M, S object relationships can be learned inductively and the \"most true\" mood can be calculated automatically for those relationships. That shall be our future work, alon with examples including recognising intentional or unintentional fallacies, with the objective to facilitate automated human-machine interaction.\nThe table (Table A1) shows the 256 moods in 5 categories with truth ratio normalised in [0,1]. False, undecided and true moods are not sorted. Unlikely and Likely moods are sorted in ascending order of their truth ratio. The table also shows the possibility distribution of the membership function FuzzySyllogisticMood(x), with x {CertainlyNot, Unlikely, Uncertain, Likely, Certainly}, defined over the truth ratios of the moods. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Introduction", "text": "Tags are collected from end users in various information spaces, e.g., blogs and wikis. They are called folksonomies (in other words, collaborative or social taggings), and regarded as an important evidence to implement a collective intelligence (CI) [1]. Moreover, a number of interesting applications and services have been designed and developed in a form of open APIs. They can be combined with each other for better services, so-called mashup [2,3]. However, more crucial challenge on these folksonomies is to discover hidden patterns between knowledge provided from users underlying the online information space. It means that individual intelligence of each user has been reflexed into the set of tags, and it is necessary to integrate the individual intelligence with others for solving more complex problems [4,5].\nParticularly, we focus on taggings written in different languages in a number of information spaces. In this work, we focus on multilingual folksonomy to show what kind of (and how) the CI can be emerged i) among users who speak different languages and ii) among multilingual users. For example, in Del.icio.us, a bookmark may be tagged with multilingual tags at the same time, as shown in Fig. 1. Given certain bookmarks (e.g., http://www.korea.net/), in the first case, both a tag \"Korea\" by Jason and a tag \"Cor\u00e9e\" by J\u00e9r\u00f4me can be attached together. As second case, if Jason is a bilingual person (e.g., English and Spanish), he might use two tags \"Korea\" and \"Corea\", simulataneously.\nThere have been many studies to discover meaningful information (e.g., tag relatedness [6]) from a given folksonomy. However, most of them are limited on an identical language. Thus, we can note that in this work there are two main issues to dealt with as follows;\nhow to reveal relationships between multilingual tags (i.e., between t 1 and t 2 and between t 1 and t 3 ) and between users (i.e., u a and u b ), and how to exploit the discovered information for better services (i.e., searching for r 1 to u b ).\nThereby, we want to identify individual intelligence (i.e., ability to speak languages and to translate from one to another) by social affinity between users participating in the folksonomies. Finally, community of lingual practice can be organized for providing users with better services (e.g., query transformation [7] and mashup [2]). More importantly, social reputation has been exploited to identify individual intelligence from multiple folksonomies. We may regard the social reputation as an indirect measurement of each user's expertise on multilingual skills and practice. Furthermore, as another important contribution of this study, we have to mention that background knowledge with external sources (e.g., dictionaries and thesauri) is not needed to refer to. Also, some words newly generated from online can be efficiently matched.\nThe outline of this paper is as follows. In Sect. 2, we explain several definitions with notations, and how to conduct semantic grounding of the tags by using such notations. Sect. 3 address social consensus-based identification of the lingual practice of online users. Then, in Sect. 4, we want to show how to use the matching results between multilingual tags on tag-based information retrieval systems. Sect. 5 demonstrates experimental results obtained from the tag-based information retrieval systems. In Sect. 6 and Sect. 7, we will discuss and compare several important issues from the proposed method with some existing ones, and finally, draw a conclusion of this work.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Semantic Grounding of Tags", "text": "In this section, we want to discuss formalization as well as to semantic grounding of the tagging information from users.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 1 (Tag).", "text": "A certain user u can annotate a set of tags t r u for describing his context about a certain resource r. A tag set by u can be represented as\nT u = { t \u00d7 r \u00d7 l |l \u2208 L u } (1)\nwhere t indicates a keyword written in a language l \u2208 L u .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 2 (Folkosonomy).", "text": "A folkosonomy F can be simply written by\nF = ua\u2208U F T ua (2)\nwhere U F is a set of users contributing their intelligence to the folkosonomy. It is a simple integration of their sets of tags. In addition, we can easily computer the number unique resources (denoted as |R F |).\nFor instance, in Fig. 1, the tag sets of two users u a and u b can be given by\nT ua = { Korea, r 1 , EN , Korea, r 2 , EN , Corea, r 1 , SP } (3) T u b = { Cor\u00e9e, r 2 , F R } (4\n)\nwhere the tags are written in three different languages.\nHere, we want to discover the co-occurrence patterns inside the folkosonomy F. Two kinds of functions \u03c4 U and \u03c4 R are formulated to measure i) lingual practices of an individual user (\u0394 U ), and ii) the co-occurrence patterns between users (\u0394 R ), respectively. Definition 3 (\u0394 U -Co-occurrence pattern). Given a certain user u a , languages used for multilingual tags can be found out. Thus, for all his tags, his lingual practice can be represented by a function\n\u03c4 U (l, l ) = |{t| t \u00d7 r \u00d7 l , t \u00d7 r \u00d7 l \u2208 T ua , l = l }| max(|{t|t \u00d7 l}|, |{t |t \u00d7 l }|) (5)\nwhere l, l \u2208 L ua and t, t \u2208 T ua .\nThis pattern means how frequently he has been using multilingual tags with a certain resource at the same time.\nDefinition 4 (\u0394 R -Co-occurrence pattern). Given two sets of resources R, co-occurrence patterns between multilingual tags can be found by a function\n\u03c4 R (t, t ) = |{ t, t | t \u00d7 r \u00d7 l \u2208 T ua , t \u00d7 r \u00d7 l \u2208 T u b }| max(|R Fa |, |R F b |) (6)\nwhere |R Fa | and |R F b | are the numbers of the resources from folksonomies the corresponding users are participating in.\nRegarding the folksonomies for \u0394 U -and \u0394 R -Co-occurrence patterns, it is not necessary to be same information sources of such folksonomies. It means that in this work, we want to merge partial intelligence extracted from each folksonomy into one.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Identifying Individual Intelligence", "text": "Here, we want to address that online collaborative systems (e.g., folksonomies) need to be more robust. The system can not guarantee that all of anonymous users provide reliable information and contribute rational efforts on online systems. Thus, we simply assume that the lingual practice of each user will be more expertise when more people have the same opinions with his.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 5 (Social consensus).", "text": "Given a user u a , a social consensus about his lingual practice between two languages l and l is represented by \u03c6 l,l ua = \u03c4 R (t, t ) \u00d7 (exp\n|U + | |U| \u2212 1) (7\n)\nwhere |U + | means the number of online users who has same opinion with u a out of the whole participants U.\nOnce we have this social consensus of all users, we can build community of lingual practice C. With respect to two languages in Equ. 7, a group of users whose \u03c6 l,l is over a pre-defined threshold can be selected as a community member u a \u2208 C l,l \u2286 C. In fact, the number of all possible communities is depending on the number of languages used by people (i.e., |C| = |L| C 2 ). More importantly, the value of \u03c6 l,l ua \u00d7 \u03c4 U (l, l ) can be regarded as a centrality of user u a within the corresponding community.\nAs a result, we can discover matching between multilingual tags, which are called correspondences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 6 (Correspondence). A set of correspondences discovered matching process between two multilingual tags is given by", "text": "C { t, t | t \u00d7 r \u00d7 l , t \u00d7 r \u00d7 l \u2208 T ua , u a = max u\u2208C l,l u} (8\n)\nwhere C indicate any possible communities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tag-Based Information Retrieval", "text": "The goal of the proposed multilingual tag analysis is basically to match two multilingual tags. Here, we want to demonstrate one possible application, tag-based information retrieval, within multiple folksonomies, differently from Tagster [8]. Similar to query translation [9], a tag-based query can be transformed (translated) into another language, by referring to the correspondences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 7 (Tag-based query).", "text": "A tag-based query q can be assumed to be travelled from u src to u dest . The query grammar is simply given by q ::= t src |\u00acq|q \u2227 q|q \u2228 q (9) where t src \u2208 T src .\nIn this study, we are interested in queries consisting of a set of tags written by the language of source user, so that the queries can be transformed by tag replacement strategy based on correspondences discovered by multilingual tag matching. The basic idea of this strategy is as follows.\n1. Making member list by sorting out the community member with respect to the centrality value 2. Finding the correspondences from the member list until replacing all of tags in the query Unless we can find all the matches, the query transformation process returns failure by nature. In addition, we want to note that the community organized by the folksonomy can be overlapped. It means that the users can be included in more than one community.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimentation", "text": "To evaluate the proposed scheme, we have collected a large amount of tag information from the following information sources;\n-Social bookmarking systems, Del.icio.us 1 -Photo sharing systems, Flickr 2\nMore importantly, as a language identifier, we have exploited a Google AJAX Language API 3 . This API can identify more than 20 languages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of Multilingual Tag Matching", "text": "Due to the lack of tags, we have chosen only 12 languages to test the proposed scheme. We did not implement any preprocessing procedure for stop word removal and stemming. By comparing with normal dictionaries, a precision factor (P (l, l )) has been measured, but a recall factor has not. As shown in Table 1. The average precision of tag matching between all possible combination is 59.78%. While the maximum precision is about 87.2%, which is the tag matching between French and Japanese, the minimum precision is about 23.3% between Czech and Polish.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Evaluation of Multilingual Resource Retrieval", "text": "We have selected a number of users to organize a simplified community of lingual practice. Hence, by using community identification equation, we have organized three communities with highly cohesive community, as follows. -Community 1 : U 1 , U 8 , U 10 , U 12 , U 14 (i.e., Q (Community 1 ) = 0.792) -Community 2 : U 2 , U 3 , U 6 , U 7 , U 15 , U 16 , U 17 (i.e., Q (Community 2 ) = 0.817) -Community 3 : U 4 , U 5 , U 9 , U 11 , U 13 , U 18 (i.e., Q (Community 3 ) = 0.792)\nWe found out that the proposed community identification method is working very well, because the modularity value is relatively high. In particularly, although Community 2 have more members, the cohesiveness is higher than other communities.\nAs second experimentation, we have conducted human evaluation for RSSbased information recommendation. During 10 days, we kept tracking of bloggers' rating patterns, as shown Fig. 2. The precision (i.e., user satisfaction) of context-based RSS feeds in three communities has been increased over time in common. This is caused by the dynamic community identification process. The bloggers were able to be automatically involved into more relevant communities.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Discussion", "text": "We want to mention several important issues that we have realized from this work. Firstly, Comparison with many studies on folksonomy analysis is needed. In terms of using co-occurrence, which can be regarded as social consensus by simple statistics, the proposed approach is similar to them. However, we have tackled analyzing multilingual tagging correspondences. Also, we have tried to identify single intelligence of each user from his lingual practice. Therefore, the precision measures in both of matching multilingual tags and providing better services have shown significantly high level.\nEven though there have been a number of research investigations on information retrieval and natural language processing communities, very few studies on resources with multilingual annotations (e.g., tags) have been done (e.g., elearning resources [10]). As additional implication, we found out the users who speak quite different language families (e.g., French and Japanese) tend to contribute more multilingual tags, and be better performance.\nFurthermore, some works on semantic web and knowledge engineering communities have been proposing some standardization in a form of ontologies and XML-based metadata [11]. In our study, we have designed automated data portability between multiple folksonomies without any metadata.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Concluding Remarks and Future Work", "text": "In this paper, we have proposed two main contributions; i) representation of tagging contexts (i.e., why the users are using the tags), and ii) co-occurrence measurement between multilingual tags. As a conclusion, this paper have shown higher performance of multilingual query transformation and information retrieval, without exploit description about tag context. It was about 60% precision.\nHowever, we have realized that the Google language API we have exploited have quite high rate of error result for language identification. We can expect that we will have a far higher performance when the API provide more robust identification. Also, as a future work, we are planning to employ natural language processing tools to preprocess the user tags, (because there are too many typo errors and mistakes from end users) -to combine more folksonomies which are available on the web, and to consider user identification in different folksonomies by using a certain identity indicators (e.g., email address, Open ID, and so on).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Color is a fundamental physical property of image processing. It is widely used in physical analysis [1], [2], [3]. In optical mineralogy, color is used for the recognition of minerals in order to identify the rock names. Color is useful for the recognition of minerals under microscopes with polarized light. Hence, microscopes with polarized light capabilities are used for optical mineralogy [4], [5], [6], [7]. Microscopes are commonly used for manual mineral identification in thin sections. But there are some problems about color which depend on a variety of factors including illumination, mineral type, thickness of thin section etc. Thus, automated mineral identification systems [5], [6] are based on scanned or plane and polarized images and use the natural color of the mineral.\nToday, many vision systems appear for the quality control of products in all areas [8]. They have been applied for boundary detection, segmentation, feature extraction and identification of objects. Because of these varieties of applications, image vision is getting popular and also is used in different fields [9], [10], [11]. In this study, thin section images were analyzed by using image processing in order to identify minerals.\nAfter getting color parameters of minerals, they have to be passed to some classification system in order to know which class of minerals they represent. A lot of research has been done on mineral exploration, e.g. [5], [6] [7]. Most of researchers paid principal attention to obtaining of data, its preprocessing and proper feature selection. But they missed a thorough analysis of experiment performance. That could lead to inaccurate results. The most frequently used method for mineral classification is artificial neural network (ANN) with one hidden layer [5], [7]. But its shortcoming is that selection of proper architecture (i.e. selecting best amount of neurons in each layer) is time consuming.\nThe objective of this study is to find simple, reliable method suitable for mineral data classification. In this study we consider two types of classification methodsstandard multiple class classifiers and two stage classifiers based on simple pair-wise classifiers. The last ones were selected due to their lightweight, fixed architecture and proven ability to perform not worse or even better than standard ANNs solutions.\nThe mineral data distributions we analyze in this study are rather complicatedclasses have highly diverse sizes and covariance matrixes. Besides, classes are overlapping and located near each other. Thus we employed similarity features in order to separate data and to get higher classification performance.\nThe paper is organized in the following way. In sections 2 and 3 we present the classification methods we used. In section 4 we present obtaining and manipulation of mineral data, while obtained results are listed in section 5. Conclusions and remarks are in section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "One Stage Classifiers", "text": "One stage classification methods are methods where only one algorithm is employed and used for decision.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-class single layer perceptron (K-SLP).", "text": "Multi-class single layer perceptron [12], [13] consists of one layer containing K single layer perceptrons f (w\nT x + b)\nwhere x is data vector, w is a p-dimensional weight vector, b is a bias weight and f is the activation function. In our study we used nonlinear sigmoid function\n) 1 /( 1 ) ( x e x f \u2212 + = (1)\nas the activation function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Radial basis function (RBF) neural networks.", "text": "The RBF-based classifier consists of three layers: input layer, a hidden radial basis function layer and a linear output layer [12], [13], [14], [15]. Radial basis layer is composed of g radial basis neurons that calculate y i =rad(||C i1 -x||/H i ), i = 1, \u2026 , g. We used the model of multivariate Gaussian distribution as a transfer function for radial basis neuron rad. C i1 is the i-th \"center\" of the radial basis neuron, and H i is the smoothing parameter. Output layer is linear:\n, 2 2 i T i b i o + = y w\nwhere y = (y 1 , \u2026, y g ) T , 2 i w is the weight vector and 2 i b is the bias term. The newly classified vector x is classified according to the maximum of outputs. We used the Matlab neural network (NN) toolbox to form the RBF networks. Artificial pseudo-validation sets were used to select parameters g and H i (see the end of section 3.1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Kernel discriminant analysis (KDA).", "text": "In KDA approach, conditional probability density functions of input vectors are used. Then the kernel-based local estimates are applied to their results. In classification phase, independent decisions are performed at each point of the feature space [13], [15]. We used the Gaussian kernel and classified according to the maximum of products\n\u2211 \u2212 \u2212 \u2212 = \u2212 i N j ij T ij h i N i q 1 1 )) ( ) ( exp( x x x x (2)\nwhere q i is prior probability of the class i = 1, 2, \u2026, K; j = 1, 2, \u2026, Ni, K -number of classes, Ni -number of training vectors in each class, and h is a smoothing parameter (we used value 1.0 due to data normalization).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Two Stage Classifiers", "text": "In two stage classification methods, the first stage uses some classifiers for primary classification and then, in the second stage, the results of this primary classification are fused in a particular manner.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pair-Wise Classifiers", "text": "In this study we used pair-wise classifiers classifying only two classes as classifiers in the first stage. Thus in the case of classifying K data classes, K(K-1)/2 pair-wise classifiers are obtained. In our study we used single layer perceptrons (SLPs) [12], [13] and support vector classifiers (SVCs) [16] as pair-wise classifiers.\nSingle layer perceptron. Single layer perceptron (SLP) is an artificial model of biological neuron which may be represented as ) ( b x w f T + , where w and b are weight vector and bias which are obtained during process of perceptron training. Vector x is a p-dimensional data vector and f is the output activation function (1).\nWe used gradient descent algorithm [12] to train SLP. The attractive feature of SLP is that during its training it may evolve through seven different statistical classifiers [17] which may be optimal classifiers for data with particular properties. Usually these properties of the data are not known. Thus it is very important to stop SLP training at the proper time. One of the best ways to do that is to use pseudo-validation data [13] (see the last subsection of this section).\nSupport vector classifier. Support vector classifier (SVC) [16] solves the following primal problem:\n\u2211 + = l i T b w i C w w 1 , , 2 1 min \u03be \u03be subject to i i T i b x w y \u03be \u03c6 \u2212 \u2265 + 1 ) ) ( ( , 0 \u2265 i \u03be , i=1, \u2026., l. (3) Where } 1 , 1 { \u2212 \u2208 i y\nis the label of one of l training vectors x i , C >0 is the upper bound and ) ( i x \u03c6 is a kernel function. Since SLP in the classification task is a linear function, i.e. result depends on the value of the argument of function (1), in order to make fair comparison we chose kernel function of SVC to be\ni i x x = ) (\n\u03c6 . Thus we used weighted linear SVC realized in LIBSVM library [18]. We used pseudo-validation data (see next subsection) to select proper upper bound C from value set [2 -7 , 2 -6 , \u2026 , 2 10 ] [19]. We also employed different weighting of parameter C for each class of the pair. Weights were selected inversely proportional to the number of training vectors in that class [20].\nPseudo-validation data. The number of iterations for SLP training, parameter C for SVC, parameters of RBF neural network and parameters of similarity features (see section 4) were selected according to classification error on pseudo-validation data. It was formed from training data by means of k nearest neighbors colored noise injection [21]. To generate such noise, for each single training vector x i , we find its k nearest neighbors of the same pattern class and add an artificially generated noise only in a subspace formed by the vector x i and k neighboring training vectors x i1, x i2, \u2026, x ik. . Random Gaussian N(0, \u03c3 2 ) variables are added ni nn times along the k lines connecting vector x i and x i1, x i2, \u2026, x ik. . The noise injection parameters were chosen empirically: k=2, \u03c3=1.0 and ni nn =2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fusion Methods", "text": "After obtaining decisions of pair-wise classifiers, one has to properly fuse their results. There are plenty of methods dedicated to this (e.g. [22 -26]). In this study we chose popular, simple methods.\nVoting. This rule performs allocation of K(K-1)/2-dimensional vector formed by the first stage pair-wise classifiers according to the majority of class labels in this vector. This method is also known as \"Max Wins\" method.\nThe directed acyclic graph method (DAG). This algorithm (DAGSVM [27]) organizes pair-wise SVMs in rooted binary direct acyclic graph to make the final decision. When a vector is submitted for classification, it is first evaluated by the root classifier (root DAG node). Subsequently, decision making is passed to the left or right node depending on the current node decision until one of K nodes with no children is reached. This node labels a new vector. In our experiments, we also used pair-wise SLPs instead of SVMs, used in original paper.\nHastie-Tibshiranie (H-T) method. Let p 1 , p 2 , \u2026,p K be the probabilities of the vectors to be classified. An assumption is made that for each class pair i, j, j i \u2260 there are n ij data samples from which conditional probabilities r ij =Prob(i|i or j) could be estimated. The proposed model in [28] is \u03bc ij =p i /(p i +p j ) in which they try to find such \nand finding p to minimize this function. The score (gradient) equations are\nK i r n n ij i j ij ij i j ij ,..., 2 , 1 : = \u2211 = \u2211 \u2260 \u2260 \u03bc (5) subject to \u2211 = 1 i p .\nIn order to compute i p authors of H-T method propose iterative procedure [28].", "publication_ref": ["b108", "b108"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "In this study thin sections were observed using the James Swift microscope. Images were taken by a digital camera in a rotating experimental stage instead of a fixed stage. The experimental stage can be rotated from 0 to 180 degrees by 1 degree increments, while polarizer and analyzer remain crossed to each other in a vertical direction during the analysis. Illumination source was a 12V/100W halogen light. Thin section images were taken through a Videolab camera mounted on the microscope. Images were transmitted to a computer by Inca software.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 1. Maximum intensity image of minerals", "text": "Images obtained both under plane-polarized and cross-polarized light contain the maximum intensity values (Fig. 1). For maximum intensity the images at every 10 increment were first captured and then compared to the previous images. All images were stored in RGB format with the dimension of 450x370 pixels with the resolution of 150 dpi. Twenty-two digital images were taken from nine thin sections. Thin sections were taken from the department of geological engineering in Sel\u00e7uk University, Turkey. In our study, a total of 5 common minerals -quartz (110 samples), muscovite (110 samples), biotite (60 samples), chlorite (60 samples) and opaque (60 samples) -were used. For image quantization, first a median filter was applied to images for noise reduction and then the histogram was equalized. Thus we got 6 features of each mineral image pixel. The first three color parameters were extracted from cross-polarized light, and the other three from plane-polarized light.\nPrior to training the classifiers, the data was normalized by standard deviations of each single feature. Then principal components and eigenvalues of pooled sample covariance matrix were used to transform the data prior to training the SLPs and SVCs. Moreover, prior to training the pair-wise classifiers, each time the two-class mean vectors were moved to the zero point.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Similarity features.", "text": "In this study similarity features were employed. If x i is an original data vector, then its similarity feature vector consists of N tr (number of training vectors) components:\n) ) ( * exp( 1 2 \u2211 \u2212 \u2212 = = p k k j k i j i x x s \u03b1 , (6\n)\nwhere p is the dimensionality of data, j is the index of jth similarity feature, vector superscript k denotes k-th element of original data vector and \u03b1 is the normalization coefficient. This way the new dimension equal to N tr is obtained. In our study half of the 400 data vectors were used for training. Thus for each data vector x i we obtained a 200-dimensional similarity feature vector s i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "In order to get a rather high reliability of results, we performed 250 2-fold cross validation generalization estimation procedures for all the data and all the methods. We permuted the data 250 times, divided it into two equal pieces for training and testing, and the same permutations of data were used in all kinds of experiments (different data and classification methods), i.e. all the experiments were performed with exactly the same data sets.\nFirstly we employed all the classification algorithms with original data. The classification error rates were rather high. For the best pair-wise method (DAG + SVC as pair-wise classifier) it was 0.211 (see Table1). The multi-class RBF method, with error rate of 0.189, showed the best results among overall methods.\nAs in the studies done by other researchers, e.g. [5], [7], we also used ANN with one hidden layer for original mineral data. Neurons in hidden layer were selected (by employment of pseudo-validation data) from an empirically predefined set. The error rate obtained with such ANN was 0.25 -the worst out of the used methods. Besides, as it was already mentioned before, the selection of neurons in hidden layer was highly time consuming. Thus we refused to use this method for further study.\nThe effect of similarity feature employment may be seen in Fig. 2. The classes became \"C\" shaped and more separable.  The results presented on the second line of Table 1 show that despite the increase of dimensionality, multi-class one stage methods and pair-wise methods based on SVC showed better results than using original data.\nThe best method in our experiments was a two stage method based on SLP as pairwise classifiers and using Hastie-Tibshirani fusion method (SLP+HT). For the estimation of inaccuracy the expression [13]  . Thus we may see that many other methods also performed well since their error rate fits within this accuracy interval (i.e. up to 0.173+0.019=0.192). After employing similarity features, some multi-class methods performed better as well as pair-wise ones, but the latter ones are recommended for practical use due to their vast ability for further improvement.\nThe results with employment of pair-wise SLP in voting-based fusion methods (Voting and DAG) are not as good due to their sensitivity of sample size and dimensionality ratio. This behavior is due to their similarity to statistical methods [17] which work well if sample size is much greater than dimensionality because of problems arising with covariance matrix estimation (for more see e.g. [29]). From the results it may be also observed that probability estimation based H-T fusion method overcomes shortcomings of SLP classifier and exploits its results best.\nIn order to overcome high dimensionality problems for SLP as pair-wise classifier in voting based fusion methods we used simple linear dimension reduction using eigenvalues of covariance matrixes -principal component analysis. We reselected dimensionalities from vast set of different values from 2 to 195 (the total dimensionality of similarity features was 200). The experiments showed that with dimensionality reduced to proper size, better results may be obtained. E.g., when using dimensionality equal to 2, the results may become 1.5 times worse because of the loss of some information. While using dimensionality approximately between 40 and 50, SLP as pair-wise and Voting as fusion method (SLP+Voting) may perform with a generalization error rate of 0.176 -much better than without dimensionality modification (error rate of 0.227). If dimensionality is increased further, then classification error increases again due to additional redundant information. While reducing dimensionality the generalization errors of other methods also decreases, but within the above mentioned accuracy.\nThe results of this study also showed that parameter \u03b1 used for obtaining similarity features highly depends both on classification method (see the best values in line 3 of Table 1) and data permutation (see the percentage of use of best \u03b1 value in line 4 of Table 1).", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": ["tab_2", "tab_2", "tab_2"]}, {"heading": "Reliability of results.", "text": "The number of experiments with different data permutations plays a great role in the reliability of results. If considering only one experiment, then out of 250 2-fold experiments using similarity features (without dimensionality reduction) we may select the best and the worst ones which are highly different from the means of all experiments listed in Table 1. E.g., in one experiment with SLP+Voting method we obtained an error rate of even 0.125, while with worst experiment for the best averaged methods (e.g. SLP+HT or K-SLP) we obtained an error rate of up to 0.25. So in order to obtain reliable results it is necessary to perform a rather large amount experiments.\nAlthough the average results of our experiments do not seem perfect, they are satisfactory for the mineral classification, since an error rate of even 0.25-0.3 is acceptable for this task. Besides, the accuracy of classification may be increased by obtaining more pixels from the same mineral to be classified.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions and Remarks", "text": "In order to simplify complexity and obtain non-linear decision boundaries in the complex shaped input feature space we used similarity features. This data transformation makes data more separable. On the other hand it enlarges dimension and makes it hard for SLP pair-wise classifiers to learn due to their similarity to classical statistical classifiers. Thus for mineral classification we recommend using similarity features and two stage classification methods with pair-wise classifiers or fusion rules which are less sensitive to dimensionality increase or K-SLP multiple class classification method.\nThe main issue in classification of such complicated data is proper selection of parameters (both the classifier and the data). In order to get precise estimation of methods a lot of experimental results should be applied on as much data as possible.\nProposed two stage classification methods are much better in calculation speed than multilayer artificial neural networks, since they do not require special training procedure to obtain proper architecture (input values, number of hidden neurons).\nThe novelties of our study are: 1) using two-stage classification methods to classify multi-class mineral data, 2) using similarity features to make mineral data nonlinearly more separable, and 3) using a pseudo-validation data set to select parameters of the decision making algorithms.\nIn this paper we used the straightforward way of using similarity features and dimension reduction by principal component analysis. The decision making strategy developed in present paper could be improved by using advanced similarity feature selection techniques. It also opens a free space for a straightforward introducing of pricing of incorrect classification, which is an important issue in industrial application. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In today's business environment, an effective Customer Relationship Management strategy is of the foremost importance [1]. One of the most important aspects of CRM is customer retention, i.e. the prevention of customers from ceasing to buy products or services and leaving the company. One often-used strategy in this context is to identify the potential churners in an early stage, and to treat these customers accordingly by offering adapted incentives in order to re-establish their relationship with the company. This practice is generally pursued in churn prediction.\nIn churn prediction, information from customers that is available in the company database is used to determine their proneness to attrite. Relevant predictive features typically include historical transactions from the customer with the company, demographic information of the customer and so on. Data mining techniques, and more specifically, classification algorithms, are then deployed to generalize the relationship that exists between a customer's characteristics, and his or her probability to churn [2]. Once built, these models can be used to predict the future behavior of customers and to deliver targeting information for churn-preventing marketing campaigns.\nIn a churn-prediction model, predictive performance is extremely important [3]. In this study, the use of ensemble learning for churn prediction is considered. Applications of ensemble classifiers to churn prediction include Random Forests [4], AdaBoost [5], AdaCost [6], Bagging [7], Stochastic Gradient Boosting [8] and ensembles of Artificial Neural Networks [3]. These studies all demonstrate the beneficial impact of using ensemble classifiers over single classifiers for classification performance in the context of churn prediction.\nAn often-used performance criterion in churn prediction is lift (e.g. [7,8]). Lift measures how many times the classification model improves the identification of potential churners in the selection, over random guessing. A popular criterion in research on churn is top-decile lift, where the top 10 percent of customers with the highest probabilities to churn are considered. In this study, two strategies to improve lift performance of four well-known ensemble classifiers are presented. A first strategy involves using C4.4 probability estimation trees (PETs) [9] as base classifier in the ensemble classifiers instead of regular C4.5 decision trees [10]. Probability estimation trees are designed to generate better posterior probabilities than regular decision trees. They have been shown to provide better ranking capabilities than regular decision trees, and can hence be expected to improve lift performance when used in ensembles. A second strategy involves altering the fusion rules that are used to combine the predictions of individual ensemble member classifiers into aggregate predictions. A comparison is made between average aggregation, which is considered to be the most basic fusion rule when rankable predictions are desired, to weighted approaches based on lift performance measures of the individual classifiers in the ensemble. In an experimental validation, the effect of both strategies will be investigated for Bagging [11], the Random Subspace Method (RSM; [12]), the combination of Bagging and RSM (SubBag; [13]) and AdaBoost [14]. Experiments are conducted for five real-life churn data sets from various European companies, belonging to different sectors. Also, in addition to top-decile lift, a range of alternative selection percentages is considered for the calculation of lift measures.\nThe paper is organized as follows. In Section 2, an overview is presented of probability estimation trees, ensemble classification and the ensemble classifiers considered in this study, and lift. Section 3 presents an overview of the used churn data sets, the conditions and the results of an experimental comparison. In a final section, a conclusion is formulated, and limitations to the study and directions for future research are provided.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probability Estimation Trees", "text": "Tree induction algorithms, such as C4.5, CART and CHAID are primarily designed to generate 'crisp' classifications: they map an instance, based on its values on a set of features, to precisely one class. However, many applications, such as churn prediction benefit from the availability of an estimation of confidence in a class prediction, such as a class membership probability. An alternative to standard classification trees in this aspect are probability estimation trees (PETs) that estimate class membership probabilities. Many PETs have been introduced in literature (e.g. [9,15]). In its most basic form, maximum likelihood probability estimates are generated as follows. Denote a decision tree that represents a C-class classification problem. Assume that at the end leaf for class c i \u2208 {c 1 , c 2 , ..., c C }, there are N instances belonging to C l \u2264 C classes, and that k instances belong to class c i . The maximum likelihood posterior class membership probability for class c i is then equal to k N [16]. It has been shown that PETs based on this rule often perform poorly at estimating class membership probabilities [9,17]. In [9], Provost and Domingos identify a number of reasons for this. They argue that maximum likelihood probabilities are potentially highly inaccurate, especially if the number of training instances at an end leaf is small. Further, they argue that pruning, aimed at constructing small but accurate trees, results in lower quality of estimated class probabilities. In order to generate better PETs, they suggest C4.4, an adapted version of the C4.5 tree-building algorithm. In C4.4, maximum-likelihood estimates are smoothed by using the Laplace correction, which adjusts probability estimates in order to make them less extreme. The Laplace estimate used for C4.4 is given by k+1 N +C l . Further, in C4.4, no pruning is applied, and 'collapsing', a secondary pruning strategy inherent to C4.5, is no longer performed.\nIn [9], Provost and Domingos applied Bagging to C4.4 PETs and demonstrated in their experiments that an ensemble of PETs substantially improved AUC performance for a majority of the examined data sets. However, they did not consider alternative ensemble strategies. Moreover, the lift performance of PETs and particularly ensembles of PETs has, to the best of our knowledge, never been analyzed in the context of customer churn prediction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ensemble Classification", "text": "Ensemble classification has been a popular field of research in recent years. Multiple studies have demonstrated the beneficial effect of combining many classification models into aggregated ensemble classifiers on classification accuracy (e.g., [18,19]). While several algorithms have been proposed in literature, many are inspired by two classical ensemble strategies: Bagging [11] and Boosting [14]. In Bagging (bootstrap aggregating), each member classifier in the ensemble is trained on a bootstrap sample (i.e., a random sample taken with replacement and with the same size) of the original training data. Member outputs are aggregated using majority voting: instances are assigned the class that is most frequently assigned by the ensemble members [11]. Bagging can introduce a significance improvement in accuracy as a result of a reduction of variance versus individual decision trees. The most well-known boosting algorithm is AdaBoost [14]. In AdaBoost, instances that are mislabeled receive higher weight during consecutive training iterations and hence, the classifier is forced to concentrate on hard-to-predict instances. A related method to Bagging is the Random Subspace Method (RSM, [12]), also known as attribute bagging [20]. In RSM, a random feature subset is sampled for the training of an ensemble member. Finally, Bagging and RSM are combined in the SubBag algorithm, proposed in [13].\nAn important element of any ensemble classifier algorithm is the fusion rule, used to aggregate ensemble member's outputs to ensemble predictions. A categorization is often made between fusion rules for label outputs and fusion rules for continuous outputs [16]. A well-studied and simple combiner is plurality voting, as mentioned earlier in the case of Bagging. Other algorithms, such as RSM implement average aggregation, which takes the average of the ensemble members' outputs. For both methods, weights can be assigned to ensemble members which score higher on a performance metric of choice to obtain weighted majority voting or weighted average aggregation [16]. Training error rates are often used as weights. In this study, top p-th percentile lift and derivations are introduced as weights for weighted average aggregation fusion rules.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lift", "text": "In the context of churn prediction, lift focuses on the segment of customers with the highest risk to the company, i.e. customers with the highest probability to churn. The definition of lift depends upon the percentage of riskiest customers one is considering for a retention campaign. Suppose that a company is interested in the top p-th percentile of most likely churners, based on predicted churn probabilities. The top p-th percentile lift then equals the ratio of the proportion of churners in the top p-th percentile of ordered posterior churn probabilities, \u03c0 p% , to the churn rate in the total customer population, \u03c0; top pth percentile lif t = \u03c0 p% \u03c0 . As the proportion of customers that a company is able and willing to target depends on the specific content, the experiments will calculate lift performance for different percentiles. The concept of directly optimizing the lift measure is similar to the one proposed in [21]. The authors maximize the number of purchases at a given mailing depth (used as a constraint) for database marketing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Validation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "To investigate the suitability of probability estimation trees as base classifiers in ensemble classifiers for increasing lift in churn prediction, this study considers five real-life churn data sets from different business contexts and for different products or services. For reasons of confidentiality, company names are not disclosed. The characteristics of these data sets are provided in Table 1.\nAs shown in Table 1, churn data sets are typically characterized by relative high numbers of features and instances. An exception is the DIY chain data set. A second issue is the class imbalance of the data. Churn is usually a rare event [30]. This is particularly a problem for the Bank1, Bank2 and Bank3 data sets. Many techniques have been proposed to deal with class imbalance in churn prediction [7,8]. In [22], the effect of class imbalance on a number of performance metrics is analyzed for probability estimation trees. This study indicates the importance of an appropriate treatment for the problem of class imbalance for the quality of probability estimates of PETs. While the authors suggest a wrapper method to determine an optimal sampling level of undersampling, in this study, majority class instances in the training data sets are undersampled in order to obtain balanced class distributions. In particular, the training data set for a classifier is composed of all instances belonging to the minority class and a random sample of majority class instances with a size equal to the number of minority class instances.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Experimental Settings", "text": "In this study, we consider four ensemble classifiers: Bagging, RSM, SubBag and AdaBoost. C4.4 and C4.5 base classifiers are implemented using the J48 classifier, which is a C4.5 implementation available in WEKA [23]. C4.4 is implemented as in [9]. Bagging, RSM, SubBag, AdaBoost and the proposed variations are implemented in MATLAB. Ensemble sizes are set to 100 constituent ensemble members. One final parameter is the random feature subset size for RSM and SubBag. This parameter is set equal to 75 % of the number of features in the respective data sets, as suggested in [13]. The default combination rule of Bagging and SubBag (i.e., majority voting) is replaced by average aggregation, as class predictions are not suited for an evaluation in terms of a rank-based measure such as lift.\nA first comparison involves the use of C4.4 PETs versus standard C4.5 classification trees as base classifiers in ensemble classifiers. In a second comparison, the influence of the introduction of alternative fusion rules based on top p-th percentile lift is investigated. Throughout the comparisons, different lift definitions are considered, with p ranging from 50 to 5. More specifically, p \u2208 {50, 40, 30, 20, 10, 5, p r } where p r is the (rounded) actual churn rate as observed in the training data ,which is provided in Table 1. This additional percentile represents the often-used strategy of companies to target as many potential churners as they expect to emerge based on past experience.\nWeighted average aggregation is applied using as weights: (i) top p-th percentile lift, further referred to as lif t p , (ii) lif t p \u2212 1, and (iii) rescaled lif t p , defined as lif tp\u2212min lif tp min lif tp where min lif t p is the minimum over the lift values as calculated for the set of base classifiers. All weights are based on lift performance of the individual base classifiers on the training data set. Alternative (ii) is inspired by fact that a lift of 1 implies a model not outperforming random guessing. Only if the classifier outperforms random guessing is (lif t p \u2212 1) a positive weight for the respective classifier. In alternative (iii), the minimal observed lift is set as a minimum threshold for classifiers to receive a positive weight.\nReported results are averaged over a 5x2-fold cross-validation. Within a 2-fold cross-validation, the training set is randomly split into two parts; the first part is used for model training, while the second part is used for model validation and vice versa.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results", "text": "Results are reported as counts of wins, losses and ties based on paired t-tests with significance level \u03b1 = 0.05. When a reference algorithm performs significantly better (worse) in terms of lift performance, a win (loss) is registered, while equal lift performance between a reference and a benchmark algorithm results in a tie. Tables 2 to 5 provide wins-losses-ties counts for variations based on Bagging, RSM, SubBag and AdaBoost. Tables 6 to 9 provide average performance ranks for the ensemble variations.  [12]. This is also confirmed in table 6, where there is a clear dominance of all C4.4-based variations. The influence of the weighted combinations rules based on lift is less clear. It appears that the proposed weighing schemes marginally improve lift performance for C4.5-based Bagging. However, for Bagging with C4.4 member classifiers, the alternative combination rules do not result in an additional increase of lift performance. Tables 3 and 7 present results for the Random Subspace Method. Here different results are found. The introduction of C4.4 as base classifier in RSM only very slightly improves performance over RSM with C4.5 base classifiers. However, fusion rules based on top p-th percentile lift invoke substantial improvements of lift performance over average aggregation for RSM, regardless of the nature of its base classifiers. The best results are observed for rescaled lift and (lif t \u2212 1) weights.  For SubBag, the implementation of C4.4 member classifiers and alternative fusion rules has no clear impact upon lift. While the average rankings in table 8 indicate an advantage for C4.5-based SubBag with lift-weighed fusion rules, the wins, losses and ties counts suggest that the differences are small.\nFinally, consider the experimental results for AdaBoost in tables 5 and 9. Here, both modified base learners and adapted fusion rules contribute to an improvement of lift performance. The introduction of C4.4 base learners generates a marked improvement over using C4.5 base classifiers. The use of lift-based fusion rules invokes a further increase. The best performance is observed for regular lift weights.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_13", "tab_20"]}, {"heading": "Conclusion", "text": "Customer retention and churn prediction are important elements of Customer Relationship Management (CRM) strategies. An often-used evaluation metric for churn-prediction models is lift, which measures the degree to which the model is better in identifying the customers most likely to churn, over random guessing. This study investigates strategies to improve lift performance of four well-known ensemble algorithms. The first is the adoption of C4.4 probability estimation trees (PETs) as base classifiers, instead of regular C4.5 classification trees. The second strategy involves replacing standard fusion rules for ensemble members' outputs by weighted average aggregation, using three alternative lift-based weight sets. Both strategies are applied to four well-known ensemble algorithms: Bagging, the Random Subspace Method (RSM), SubBag and Ada-Boost. Experiments on five real-life churn prediction data sets are conducted to compare C4.5 and C4.4 trees as base classifiers, and original versus the proposed fusion rules. The results indicate variation in the effect of the proposed strategies on lift performance depending on the nature of the ensemble algorithm: (i) Bagging greatly benefits from adopting C4.4 base classifiers using average aggregation as a fusion rule, while lift-based weighted averaging does not substantially improve lift performance; (ii) weighted average aggregation is a viable strategy to increase lift performance of RSM, while it does not benefit from adopting C4.4 trees as base classifiers; (iii) SubBag does not notably benefit from either of both strategies, and (iv) the lift performance of AdaBoost can be substantially improved by implementing both C4.4 base classifiers and weighted average aggregation based on lift.\nCertain limitations to this study can be identified. While the results clearly reveal a number of trends, the authors acknowledge that further experiments are needed to allow for statistically proven generalizations. Further, no comparisons are made between the proposed methods and classification algorithms that are designed to optimize ranking performance such as AUC. Future work includes the adoption of different PET algorithms and alternative ensemble strategies, such as the recently proposed Rotation Forests [26] and application of error decomposition and other techniques to gain insight in the accuracy-diversity trade-off to gain more insight in the specific conditions that need to be fulfilled to successfully ensemble PETs for increasing lift performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Classification and clustering are two problems intensively studied in machine learning. Classification aims at assigning new data items to existing groupings. Clustering is the problem of identifying natural or interesting groupings in data. Although similar at first sight, they belong to two distinct paradigms: supervised versus unsupervised learning.\nFeature selection (FS) is a problem of great interest for both scenarios -classification and clustering -with the aim of improving the performance of the corresponding machine learning techniques. Feature ranking is a relaxation of FS: the features are ranked based on their relevance to the problem under investigation. With regard to clustering, fewer approaches exist in literature due to the difficulties raised by the unsupervised nature of the problem; most of them offer feature rankings because the optimal number of features to be selected is hard to be determined in the unsupervised scenario.\nThe current work investigates an extension of a feature ranking technique we have recently proposed in the context of unsupervised clustering [1]. The method is based on evolving an ensemble of feature subsets which serve further for feature ranking. The algorithm is extended here to return the optimal subset of features for clustering, overcoming the initial drawback of fixed cardinality imposed over the feature subspace. As a result, the extended method is able to return the optimum number of features in a completely unsupervised scenario. Additionally, an extension is proposed to deal with the semi-supervised version of clustering, namely supervised information is incorporated in form of similarity/dissimilarity pairwise constraints.\nThe paper is structured as follows. Section 2 presents succinctly existing approaches to feature selection and feature ranking for unsupervised and semisupervised clustering. Section 3 revisits the method we proposed previously for feature ranking [1] and describes an extension for the semi-supervised case. Section 4 extends the method towards a complete feature selection procedure and Section 5 evaluates the performance of the method on complex synthetic data sets. The paper concludes with a short discussion in Section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work on Feature Selection and Feature Ranking", "text": "Feature selection (FS) generally aims at reducing the representation of the data in order to lower the computational cost of further analysis. To this goal, filter methods were designed which try to remove redundant features in a preprocessing step.\nWith regard to the two intensively-studied problems in machine learningclassification and clustering -FS plays different roles. In classification FS aims to identify the features which predict with the highest accuracy the class labels. In clustering FS aims to identify the features which lead to well-defined groupings.\nFeature ranking is a generalization of FS: the features are ordered in accordance with their relative contribution to the goals expressed previously.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Unsupervised Scenario", "text": "In the unsupervised scenario for clustering no information is available with regard to the number of clusters nor with regard to the assignment of some particular data instances.\nFilter approaches to FS aim at quantifying the merit of each feature ignoring the subsequent method of analysis. Two strategies are used to compute the merit of each feature: one that aims at removing redundant features and one that scores the relevance of features. Redundancy-based approaches hold that mutually-dependent features should be discarded. In this regard, clustering on features is proposed selecting further one representative feature per class [4]. On the contrary, there exist approaches in the second category [11,14] that compute relevance accepting that relevant features are highly dependent on the clusters structure and therefore, they are pairwise dependent; pairwise dependence scores are computed using mutual information and mutual prediction [14]. Other approaches rank the features accordingly to their variances or accordingly to their contribution to the entropy calculated on a leave-one-out basis [15,3].\nUnlike most of the filter approaches, wrapper methods evaluate feature subsets and not simple features. These approaches perform better since the evaluation is based on the exploratory analysis method employed for data analysis. However, wrapper approaches have two drawbacks: high computational time, and bias. The high computational time is due to the evaluation procedure which consists in running a full clustering algorithm. The bias is due to the objective function used to evaluate different partitions. In an unsupervised framework, the objective function which guides the search for good partitions induces some biases on the number of clusters and the size of the feature subspace. Regarding the number of clusters, several unsupervised clustering criteria were proposed to deal with the unsupervised clustering problem (i.e. Silhouette Width and Davis-Bouldin Index). However, all objective functions are based on computing some distance function for every pair of data items; the dimensionality influences the distribution of the distances between data items and thus induces a bias on the size of the feature space.\nIn order to reduce the bias with regard to the number of features, a few strategies were proposed. Dy and Brodley [5] introduce the cross-projection normalization: given two feature subsets, the best partition is determined for each feature subspace and the resulting partitions are each evaluated in the other subspace. The cross-projection normalization is used in a greedy scenario (sequential forward search); because it is not transitive, its use in global search techniques is inappropriate. However, this drawback is alleviated in [1] by using a steady-state genetic algorithm which implements a crowding scheme at replacement encouraging the competition among pairs of relatively similar feature subsets. The cross-projection normalization is thus studied in a larger context and its performance proved to be highly dependent on the unsupervised clustering criteria used: i.e. Silhouette Width [13] which is one of the best performers in unsupervised clustering behaved worst under this normalization. In our opinion, the cross-projection normalization does not offer a feasible solution to the feature cardinality bias: the results regarding feature selection show that part of the relevant features are eliminated and, furthermore, irrelevant features are selected as relevant. This suggests that the bias towards small features subspaces is not completely removed and that the cross-projection misleads the search algorithm.\nMulti-objective optimization algorithms are a more straightforward way to deal with biases: the bias introduced in the primary objective function is counterbalanced by other objective functions. A more extensive study on the use of multi-objective optimization for unsupervised feature selection is carried out in [7]: some drawbacks of the existing methods are outlined and several objective functions are thoroughly tested on a complex synthetic benchmark.\nMore recently, ensemble unsupervised feature ranking and selection were proposed. In [9] clustering is performed on random subsets of features and each feature is ranked through analyzing the correlations between the features and the clustering solution. Based on the ensemble of feature rankings, one consensus ranking is constructed. Even if this ensemble feature ranking method is considered to work unsupervised, some degree of supervision is introduced in the study: in order to deliver partitions in feature subspaces, k-Means is run with the known number of clusters. In [6] an ensemble of feature rankings is obtained using the Laplacian score for random feature subsets and a subset of the best features with respect to the different rankings is obtained by assuming that the distribution of the irrelevant features at each of the remaining kth rank is uniform.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Semi-supervised Scenario", "text": "In the semi-supervised scenario for clustering, external information is introduced in the form of a reduced number of pairwise constraints: similarity constraints indicate pairs of data items which must share the same cluster and dissimilarity constraints indicate pairs of data items which must be put in different clusters. The number of clusters is still unknown; however, some information with regard to the minimum number of clusters allowed can be inferred from the constraints.\nThe semi-supervised problem stands as a junction for supervised learning and unsupervised learning. Therefore, two wrapper scenarios are proposed in literature: 1) classifiers are extended to incorporate unlabeled data and 2) clustering methods are modified to benefit from guidance provided by the labeled data.\nIn the first category, Ren et.al. [12] learn a classifier on the reduced labeled data and extend it at each iteration introducing randomly-selected unlabeled data; implicitly, new features are added iteratively in a forward-search manner.\nIn the second category, Handl and Knowles extend their multi-objective algorithm proposed for unsupervised feature selection [8]. The Adjusted Rand Index (ARI) [10] is introduced to measure the consistency with the given constraints or class labels as a third objective or in a linear and non-linear combination with the unsupervised clustering criterion. The solution recording the highest consistency reflected by the ARI score is reported from the final Pareto front.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constructing an Ensemble of Near-Optimal Feature Subsets", "text": "In [1] a multi-modal genetic algorithm is used to derive an ensemble of nearoptimal feature subsets for unsupervised clustering. The wrapper paradigm is employed. The bias with regard to the cardinality of the feature subsets introduced by the unsupervised clustering criteria (as explained in Section 2.1) impedes us to select the optimal feature subset. For this reason a fixed cardinality is imposed for all feature subsets and the ensemble of solutions in the final generation of the genetic algorithm is used to derive feature weights which lead further to an optimal ranking. Experimental results showed that the method is feasible in the context of data sets with a large number of noisy features. However, its performance is dependant on the cardinality of the candidate feature subsets imposed within population. The current paper alleviates this drawback and proposes a complete feature selection algorithm. Subsection 3.1 revisits the genetic algorithm in [1] and Subsection 3.2 proposes an extension for the semi-supervised case.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Unsupervised Scenario", "text": "The use of a multi-modal algorithm to search for optimal feature subsets is highly justified by the multi-modal nature of the problem: different feature subspaces may lead to different meaningful partitions of the original data. However, the reason for using such an approach in the current paper is different: the diversity in population maintained by such an algorithm will guarantee an uniform distribution of the irrelevant features. This premise of uniform distribution of the irrelevant features has been already used in the ensemble FS method presented in [6] but the framework we propose here is totally different.\nThe algorithm we use is the Multi-Niche Crowding genetic algorithm (MNC-GA) [16]. It is employed for several reasons: it maintains diversity throughout the search and maintains stable sub-populations within different niches of the search space. Also, by implementing a steady-state scheme it allowed us to study in previous work the cross-projection normalization [1]. Fig. 1. One iteration in the MNC-GA algorithm for FS Figure 1 illustrates one iteration of the MNC-GA algorithm for feature selection. A chromosome in the algorithm encodes a feature subspace and the number of clusters of the partition to be derived in that feature subspace. The evaluation of such a chromosome necessitates a clustering algorithm: the k-Means algorithm is used to return the near-optimal partition in the encoded feature subspace with the number of clusters specified in the encoding. The evaluation of a feature subset is thus reduced to the evaluation of the derived partition. Unsupervised clustering criteria able to evaluate partitions with various numbers of features and built over feature subspaces of different cardinalities are required. Widely used unsupervised clustering criteria like Silhouette [13] and Davis-Bouldin [2] would favor feature subsets of lower cardinality and would direct the search towards the feature subset of minimum allowed cardinality. We proposed a new unsupervised clustering criterion which reduces part of this bias by penalizing small numbers of features m:\nCrit = 1 1 + W B \u2022 m m + 0.5 log 2 (k+1)+1 (1)\nwhere W = k i=1 d\u2208Ci s(c i , d) is the within-cluster inertia computed as the sum of the distances between all data items d and their cluster centers c i ;\nB = k i=1 |C i | \u2022 s(c i , g\n) is the between-cluster inertia computed as the sum of the distances between the cluster centers c i and the center of the entire data set g weighted with the size of each cluster |C i |.\nExperiments on synthetic data sets with the criterion in equation 1) suggested that it is a brave competitor to the Silhouette Width criterion, working at reduced computational costs; it outperformed the Davis-Bouldin Index in our experiments [1]. It still records a bias towards lower cardinalities of the feature space but not as pronounced as the bias introduced by the other unsupervised clustering criteria.\nTo deal with the bias introduced by the unsupervised clustering criteria, limits are imposed to the number of features selected within a chromosome.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "The Semi-supervised Scenario", "text": "We investigate two scenarios to introduce external information in the previous unsupervised approach. In a first scenario, the fitness function is modified to reflect the consistency of the partition with the labeled data: the product between the unsupervised clustering criterion in equation 1) and the ARI [10] score involving the labeled data is used. In the second scenario we force all partitions to satisfy the given constraints by employing constrained KMeans [17] as clustering procedure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Selection", "text": "In [1] feature rankings are derived based on the distribution of the features encoded in the final population of the algorithm described previously. The success of this ranking scheme is based on two premises: 1) uniform distribution of the irrelevant features and 2) high frequency of the relevant features in the final generation of the genetic algorithm. With regard to the roles of the genetic operators, the mutation is responsible for diversity which supports the first premise while the crossover operator propagates in population the best characteristics, supporting the second premise.\nBecause of the bias with regard to the number of features introduced by the fitness function, the chromosomes encode feature subspaces of fixed cardinality which is a parameter of the algorithm. This is an important drawback of the algorithm: if the number of relevant features is much smaller compared to the cardinality imposed, the relevant features get suffocated and the partition resulted does not reflect the distribution of values across the relevant features. From this point of view we anticipate that the ensemble method proposed in [9] which is based on measuring the correlation between the variables and the clustering solution suffers form the same drawback.\nTo overcome this drawback and to develop a method able to go further and perform feature selection we propose to vary the cardinality of the feature subspaces along the run of the genetic algorithm. The main decision factors involved are the variance of the fitness in population and the distribution of features in population reported to the cardinality of the encoded feature subspaces.\nRegarding the dynamic of the fitness variance in population along the run, a behavior typical to genetic algorithms is recorded. A small variance in the first iteration is due to sub-optimal solutions. Once good schemata are retrieved, the variance increases due to the presence of a small number of high-fitness chromosomes. Then, the variance in fitness decreases as the population tends to converge. When the variance in fitness is smaller than the variance recorded in the first iteration and it remains unmodified for several iterations, the multimodal genetic algorithm reaches convergence. It is worth noticing that we do not condition convergence to null variance, for several reasons. First of all, the multi-modal genetic algorithm is supposed to converge to multiple optima in the search space which signify different fitness values in the final iteration. Secondly, the fitness of the chromosomes is computed based on the partitions generated with k-Means; therefore, two identical chromosomes encoding the same feature subspace and equal numbers of clusters, could have been assigned different fitness values because of slightly different partitions generated as result of different initialization of the clustering algorithm.\nWhen the conditions required for convergence are fulfilled, the distribution of selected features in population is computed. A heuristic step is employed at this stage: a feature is considered relevant if its frequency in population exceeds 50% (more than half of the chromosomes in population selects it). On this basis, the number of relevant features is computed; if it is smaller than the cardinality of the feature subspace imposed to chromosomes, the cardinality is decremented by 1 and the algorithm is restarted. To benefit from the information gathered throughout the search one new chromosome is constructed encoding the features marked as relevant and adding random chosen features to reach the cardinality imposed. To avoid the hitchhiking phenomenon which was shown to cause premature convergence in GAs, only the 25% best chromosomes are kept and the rest of the population is randomly generated, encouraging diversity.\nWhen the conditions required for convergence are fulfilled, and the cardinality imposed to the feature subsets encoded by chromosomes does not exceed the number of features computed as relevant, the algorithm returns the features marked as relevant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In order to compare the results obtained by the new method with related work, the experiments are carried out on the artificial data sets created by Handl and Knowles [7]. A number of 40 data sets are used in our experiments, clustered into 4 groups (10 data sets per group) named dd-kk; d expresses the number of relevant features and takes the values {2,10} and k expresses the number of clusters and takes the values {4, 10}. For all data sets 100 Gaussian features were introduced as noise. Before applying the algorithm all data sets are standardized to have mean 0 and variance 1 for each feature. For the semi-supervised scenario, five data items are extracted randomly from each class and are used further as labeled data.\nThe parameters of the algorithm are set as follows. The population size was set to 50. In order to ensure diversity throughout the run in MNC-GA the size of the group at selection, the size of the group at replacement and the number of groups at replacement are all set to 10% of the population size. The number of features selected in each chromosome was set to 20 and then decreased along the run as explained in section 4; the choice of this specific value was made in order to be consistent with the experiments presented in [7] where the cardinality of the candidate feature subsets varies in the range 1-20.\nThe performance of our method is evaluated with regard to the quality of the partition obtained and with regard to the consistency between the feature subset returned and the relevant feature subset.\nThe partition reported in our experiments is obtained running k-Means with different numbers of clusters on the feature subset returned by our method. In the unsupervised case the best partition is extracted using the clustering criterion in equation 1). In the semi-supervised case, in the first scenario the product between ARI and the clustering criterion is used, while in the second scenario the constrained k-Means is used in conjunction with the clustering criterion. The Adjusted Rand Index (ARI) [10] is used to evaluate the partitions delivered by our method against the known true partition of the data set.\nIn order to judge the consistency between the returned feature subset and the known relevant feature subset, two measures from information retrieval are employed. Precision is defined as the number of relevant features identified divided by the total number of features returned and stands for Specificity. Recall is defined as the number of relevant features retrieved divided by the total number of existing relevant features and stands for Sensitivity. Also, their combination under the harmonic mean, known as F-measure, is reported.\nAs measure of time-complexity, the number of fitness evaluations required for a complete run of the algorithm is computed.\nTable 1 presents the results for the unsupervised scenario as averages over 10 runs for each data set, 10 data sets per problem class.\nFigure 2 (left) includes for comparison purposes the results presented in [7] obtained with the multi-objective genetic algorithm in a wrapper context and also in a filter scenario based on entropy. These results were obtained in a supervised manner from the Pareto front; a small decrease in performance is recorded if an automatic extraction procedure is involved, as shown in [7]. Figure 2 (right) presents the results obtained for semi-supervised feature selection.  [7] within a wrapper scenario with several clustering criteria used as the primary objective: Silhouette Width, Davies Bouldin and Davies-Bouldin normalized with respect to the number of features; Entropy corresponds to the multi-objective algorithm investigated in [7] within a filter scenario which is based on an entropy measure; MNC-GA corresponds to the method investigated in the current paper. Right: the unsupervised scenario and the two semi-supervised approaches.\nThe results in Table 1 show that the method is able to identify the relevant features and delivers high-quality partitions. The comparisons with the multiobjective algorithm, which is one of the few feasible solutions to unsupervised FS, show that the multi-modal approach behaves comparable.\nRegarding the semi-supervised scenario, the gain in performance is evident compared to the unsupervised case, especially for the data sets with high numbers of clusters. The experiments show that constraining the partitions to satisfy the labeled data employing constrained k-Means generally hastens the retrieval of the relevant features and provides better results compared to the alternative approach. Additional experiments we have performed with higher numbers of labeled data items revealed that no significant improvements are obtained for the method which incorporates the supervised information in the fitness function. However, the method which makes use of Constrained k-Means continues to record performance improvements when increasing the number of labeled samples because the partitions are guaranteed to satisfy the provided labels.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Conclusion", "text": "Feature selection for unsupervised clustering is a problem of great interest in the current context of data mining. However, few feasible solutions exist in literature due to the difficulties raised by the unsupervised nature of the problem. Ensemble methods are gaining ground in this context. The current paper proposes a wrapper feature selection algorithm for data sets with large numbers of noisy features. The method makes use of an ensemble of near-optimal feature subsets evolved with a multi-modal genetic algorithm. Such an optimization algorithm is necessary to retrieve the relevant features in a large search space. At the same time, it is capable of maintaining high diversity in population, ensuring uniform distribution for the irrelevant features.  Classification 2, 193-198 (1985) 11. Talavera, L.: Feature selection as a preprocessing step for hierarchical clustering.\nIn ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Streaming data continuously flows in and out of a computer system until it shuts down. This means that streaming data is also a potentially infinite source. Today, streaming data is ubiquitous. Streaming-data mining (storing, analyzing, and visualizing such a continuous, infinite sequence of data) is a challenging task. In particular, classification techniques in the streaming-data mining can be applied to real-time decision support in business and industrial applications [1].\nIn order to generate a classification model, it is essential to label samples [7]. Most classification techniques for classifying streaming data assume availability of labels for all previous samples. The incremental learning methodology immediately requires the correct class of a new sample after classifying the new sample [4]. The ensemble methodology that periodically builds a new classifier for an ensemble can call for correct classes of accumulated samples when building a new classifier [3,6,7,[9][10][11][12]. However, in online applications, such as intrusion detection and click fraud, labeling all previous streaming data may be costly and time-consuming.\nIn terms of the classification accuracy, Cozman et al. [2] and Kuncheva et al [5] analyzed the effect of unlabeled training data using Na\u00efve Bayes rule. They demonstrated that if labeled training data correctly represents the underlying distribution of a problem, then unlabeled data is expected to improve upon classification error. However, if labeled training data is biased, using unlabeled data may do more harm than good. Therefore, it is another issue of streaming data classification to use unlabeled samples for classifying streaming data with change in distributions.\nIn order to build a new classifier or modify a previous classifier on streaming unlabeled data, it is an intuitive method to periodically request a human expert to label a small amount of samples. To be used as a more practical method, the manual labeling of samples should be requested as little as possible.\nWe propose the method of forming an ensemble classifier on streaming unlabeled data. In order to build a new classifier for an ensemble, the proposed ensemble method selects unlabeled samples according to the change in distribution of streaming unlabeled data. If an ensemble \"guesses\" that some selected unlabeled samples belong to the same distribution, which is different from previous distributions, it requests a human expert to label them to build a new classifier.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ensemble Approach for Streaming Unlabeled Data", "text": "An ensemble classifier consists of several models (classifiers). It predicts the class of a new sample by combining the predictions made by each classifier. Using a combining method to determine the final output, the weighted output of each classifier is combined.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 1. Ensemble approach for classifying streaming unlabeled data", "text": "An ensemble classifier for classifying steaming data must determine when a new classifier should be built to maintain its classification accuracy. Also, to build a new classifier, an ensemble classifier must determine the samples which should be manually labeled. The typical ensemble approach partitions streaming data into a finite data set (chunk) using a predefined time interval to build a new classifier, as shown in Fig. 1. The time interval is usually defined as the number of streaming samples [3,6,7,[9][10][11][12]. In Fig. 1, it is defined as 5. Such an approach can request a human expert to label all samples of a chunk periodically.\nIn order to select samples that could be labeled to build a new classifier for an ensemble, Zhu et al. [12] introduced three simple methods, and proposed the method called MV (Minimal Variance) using ensemble variance. All of these methods select the predefined number of samples from a chunk: (1) The random sampling (RS) method randomly selects those unlabeled samples; (2) The local uncertainty sampling (LU) method selects those unlabeled samples using an uncertainty measure of only the current chunk, without considering any other previous chunks; (3) The global uncertainty sampling (GU) method first labels a tiny set of samples from the current chunk and then builds a new classifier from them. Samples, that could be labeled, are selected from the unlabeled samples remaining in the current chunk using classifiers in an ensemble and the new classifier; (4) The minimal variance (MV) method only uses the ensemble variance as an uncertainty measure in the global uncertainty sampling (GU) method.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Suspicious Streaming Data", "text": "Suspicious streaming data is defined as \"streaming unlabeled data that should be labeled to improve accuracy of the current classifier\". Suspicious data from streaming unlabeled data might be samples that belong to a distribution different from previous distributions, which are distributions of train data of each previous classifier in an ensemble. If a streaming sample belongs to a previous distribution, the sample may have a high probability of being correctly classified by the current classifier. We assume that a previous distribution is a normal distribution.\nSuppose that streaming samples on d-dimensional space have belonged to k previous distributions until now. These streaming samples are separated into k data sets according to previous distributions, } ,..., { },..., ,..., { }, ,..., {\n1 2 2 21 2 1 1 11 1 knk k k n n x x S x x S x x S = = =\n.\nx ij represents the j th sample in the i th data set. Each previous distribution N i is summarized within the mean m i and the standard deviation \u03c3 i of the corresponding data set S i . If a new streaming sample x t arrives, we can evaluate if the new sample x t belongs to one of the previous distributions using equation (1) \n\u23a9 \u23a8 \u23a7 > = otherwise , 0 ) , ( if , 1 ) , ( i i t i t m x dist N x f \u03c3 (1)\nwhere, dist(x t ,m i ) represents the distance between the new sample x t and the mean m i of\nN i (0 \u2264 i \u2264 k).\nIf the new sample does not belong to any of the previous distributions, it becomes a suspicious sample.\nEquation ( 2) is a distance function used in the proposed methodology. The distance function combines each normalized output of Euclidean, Euclidean(), and the cosine distance, Cosine(), functions according to each indicator, I 1 , I 2 . Each indicator has a value of 0 or 1, and the output of this distance function is between 0 and 1. The Euclidean is used for numerical attributes, and the cosine distance is used for nominal (categorical) attributes. For example, if there is no nominal attribute in a streaming data, the indicator I 2 of the cosine distance function becomes 0.\n\u2211 = \u2212 = = + + + = d k c j c i c j c i n j n i n j n i c j c i n j n i j i x x Cosine d x x Co Max Euclidean x x Euclidean x x Eu x x Co I I I x x Eu I I I x x dist 1 2 1 2 2 1 1 )) , ( 1 ( 1 ) ,( _ ) , ( ) , ( ) , ( ) , ( ) , ( (2)\nx i n denotes a sample that consists of only numerical attributes of a sample x i . x i c denotes a sample that consists of only nominal attributes of a sample x i . In particular, each nominal attribute of x i c represents a frequency vector in which each element represents the count of its values. Suppose that \"Color\" is the nominal attribute and it has three values: {red, green, blue}. If \"Color\" value of x i c is red, then its frequency vector becomes (1,0,0). In a single sample, all frequency vectors become a unit vector. The Euclidean_Max represents the longest distance in training samples including the training samples for the initial classifier. The d denotes the number of nominal attributes.\nWhen a mean vector of a previous distribution is calculated, the meaning of each nominal attribute becomes a frequency rate vector of the corresponding data set. For example, if a data set has 1000 samples, and the frequency vector of the \"Color\" attribute is (700,200,100), then the mean of the \"Color\" attribute becomes (0.7,0.2,0.1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ensemble Method Using Suspicious Streaming Samples", "text": "We propose an ensemble method of dynamically maintaining an ensemble classifier according to changes in distribution of streaming unlabeled data. The proposed ensemble consists of several classifiers, and the mean vectors and the standard deviations of train data for each classifier. The final classification output of an ensemble is obtained by weighting each classifier's prediction. The proposed ensemble determines streaming unlabeled data that could be labeled from the suspicious streaming samples. Also, whenever an ensemble classifies a new sample, it evaluates weights of each classifier according to the relation of the new sample to previous distributions in the current ensemble; for further details see Ryu et al. [8]. This section describes how to build a new classifier for an ensemble using suspicious samples.\nAn initial ensemble consists of a single classifier that is built on a collected train data set in the offline mode. Constructing a new classifier for an ensemble including an initial classifier, an ensemble also summarizes the previous distribution (dotted circle) with the mean vector, m, and the standard deviation, \u03c3, of its train data set (circles) as shown in Fig. 2. By the equation (1), a streaming sample (triangle) that is outside a previous distribution becomes a suspicious sample as depicted in the figure.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Fig. 2. Previous distribution and suspicious streaming sample in our ensemble method", "text": "The proposed ensemble forms a train set on suspicious streaming samples (triangle) that belong to the same distribution using the cluster concept as shown in Fig. 3. Numbers inside the triangles denote the incoming order of stream samples. When the first suspicious sample occurs, it becomes the seed of a new cluster region. The radius of the new cluster region is settled as a predefined cluster radius, \u03b8 r . The seed does not move until the number of samples within the 1 st cluster region reaches the predefined minimum number of samples, \u03b8 s . If the next suspicious streaming sample (the 2 nd suspicious sample) is located within this cluster region, it is assigned to the first cluster. If a new suspicious sample is located outside previous cluster regions as the 3 rd suspicious sample in Fig. 3, an ensemble generates another new cluster region. Within a cluster region, when the number of assigned samples reaches the predefined minimum number of samples, \u03b8 s , an ensemble requests a human expert to label them. After they are labeled, an ensemble builds a new classifier on them.\nThe proposed ensemble method does not periodically build a new classifier for an ensemble. It builds a new classifier according to changes in distribution of streaming data as shown in Fig. 4. Suppose that there is one previous distribution, P1~(m 1 , \u03c3 1 ), and nine suspicious streaming samples continuously occur. If \u03b8 r and \u03b8 s are 0.3 and 5, respectively, a new classifier is built on samples within the second cluster region after the 9 th sample. Also, another previous distribution, P2 is defined with the mean vector, m 2 , and the standard deviation, \u03c3 2 of these samples.\nA value of \u03b8 r is in the range of [0, 1] as a result of the equation (2). If \u03b8 s is defined as a large value, an ensemble has to wait for a new classifier until enough suspicious samples are collected as its train data.", "publication_ref": [], "figure_ref": ["fig_28", "fig_28", "fig_29"], "table_ref": []}, {"heading": "Experiments", "text": "We evaluated the proposed ensemble method using synthetic streaming data and the intrusion detection data of KDD'99 Cup. Its results on the synthetic data sets are compared with the results shown in Zhu et al. [12]. In the intrusion detection data, our method is compared with the ensemble method that requests correct classes of all samples gathered during a time interval. In all experiments, correct classes of samples were used whenever an ensemble builds a new classifier. The J48 decision tree (C4.5) from Weka (www.cs.waikato.ac.nz/ml/weka/) was used to build a new classifier of an ensemble in our experiments as well as Zhu et al. [12]. Synthetic data. We utilize the stream-data generation method used in Zhu et al. [12]. To generate streaming data, each parameter is settled with values used in Zhu et al. [12]. Each generated streaming data set has 50,000 samples with 10 dimensions. The concept drifting involves 5 attributes, and attribute weights change with a magnitude of 0.1 in every 1000 samples and weight adjustment inverts the direction with 20% of change. We generated 500 initial train samples with the first concept-drifting condition of streaming data before generating streaming data.\nIntrusion detection data. This data set has been widely used in the stream data research [3]. It contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment (www.sigkdd.org). We used 10% subset of this data set, kddcup.data_10_percent.zip because Geo et al. [3] showed the changes in the data distribution in this subset. The subset has 494,020 samples with 41 attributes and 22 attack types. We transformed this subset into a data set with two classes (\"normal\" and \"intrusion\") as in [3].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results on Synthetic Streaming Data", "text": "In order to compare with results in Zhu et al. [12], we applied the proposed method to three types of synthetic streaming data: two-class, three-class, and four-class. Results are shown in Table 1. We also kept the most recent 10 classifiers in an ensemble. Each value in the table corresponds to average (\u00b1 standard deviation) of (1) classification accuracies and (2) the total number of generated classifiers for an ensemble (classifier counts) of 10 experiments that used different data sets with same input parameters. Existing methods (random sampling (RS), local uncertainly (LU), global uncertainty (GU), and minimal variance (MV)) [12] periodically build a new classifier according to the predefined chunk size. Their classifier counts, therefore, have the same values. For example, when a chunk size is predefined as 250, the total number of generated classifiers becomes 200.\nTo build a new classifier, existing methods selected only samples of 10% from a chunk, and then used their correct classes. Therefore, we predefined the threshold, \u03b8 s , as 25, 50, 75, 100, and 200. The value, 0.5, for \u03b8 r was decided through changes in measurements. At this value, both the classification accuracy and the classifier count have relatively small variances.\nThe proposed method shows higher classification accuracy and less classifier count than existing methods in many cases. In particular, when the number of samples that will be labeled (size of train data set) is 25, our ensemble produces average 26.9% higher accuracy and average 36.7% less classifier count. In the results of our ensemble, the larger the size of train data set is, the smaller the accuracy is. We will discuss this issue in Section 5. The proposed method also produced 0.46 of standard deviation on accuracies (84.15%, 85.28%, and 84.90%) that are averaged per synthetic streaming data type. This standard deviation value is smaller than that of existing methods: (SV, 10.70), (LU,10.86), (GU,11.90), (MV,11.27). This shows that our method is much more robust than existing methods in a multi-class problem.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results on Intrusion Detection Data", "text": "In Table 2, we report performances of ensemble methods including existing methods (simple voting (SV) [3], weighted ensemble (WE) [11]) on the intrusion detection data. When building a new classifier, existing methods use the correct classes of \"all\" samples within a chunk. To determine the final output of an ensemble, the SV selects the most frequent class from a set of classes that is predicted by each classifier. The WE combines predictions of each classifier per a class as the weighted average, and then the class with the greatest average value is selected. In the WE, weights of each classifier in an ensemble are evaluated as their classification accuracy on the most recent chunk.\nFor two parameter values: (1) the number of samples that will be labeled, and (2) the maximum number of classifiers in ensemble, we selected the values used in the synthetic streaming data. In an ensemble, when there are over 10 classifiers, our method and the SV keep 10 classifiers built at the most recent time. The WE deletes one with the lowest weight after evaluating weights of each classifier. In Table 2, a value of \u03b8 r , is selected as 0.3. At this value, our method shows a balanced trade-off between classification accuracy and labeling efficiency. With average 98.9% (8483.4) lesser new classifiers for the intrusion detection data, the proposed method produced similar accuracy to existing methods. If the chunk size is predefined as larger value, the existing methods can produce smaller value of the classifier count. However, the more number of samples should be labeled.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_5"]}, {"heading": "Fig. 5. Classifier count and error rate on streaming data", "text": "The first 8,653 samples (i.e 1.7%) of the intrusion detection data (kddcup.data_ 10_percent.zip) are used to build the initial classifier for an ensemble. This initial train data set includes about 10% of samples with \"intrusion\" class. Figure 5 shows the distribution in time of an average classifier count and an average error rate per 40K streaming data. The error rate represents the proportion of the number of misclassified samples to 40K samples. Both values of the average classifier count and the average error rate increase in sections where the ratio of class is changed such as 0K~40K, 120K~160K, 320K~360K, and 440K~480K. In the section between 40K and 80K, the average error rate increases, while the average classifier count reduces because an ensemble delays building a new classifier until the number of suspicious samples reaches the threshold, \u03b8 s . In the section between 160K and 320K, any requesting correct classes of samples did not occur because most samples within this section might belong to previous distributions in the current ensemble.", "publication_ref": [], "figure_ref": ["fig_116"], "table_ref": []}, {"heading": "Discussion", "text": "Table 1 is the experimental results on synthetic streaming data. When the number of samples that will be labeled has a large value, the classification accuracy of the proposed method decreases, while that of the existing methods increase. Our method builds a new classifier according to changes in a distribution of streaming data, not at a time interval. To compare with results in Zhu et al. [12], we deleted the oldest classifier in the current ensemble when the number of its classifiers is over the predefined maximum number. Accordingly, the oldest classifier is removed from the current ensemble only if a new classifier is added to the current ensemble. In our method, if the size of a training data set is large, the current ensemble may contain previous distributions, where the change in a class distribution occurs, and the corresponding classifiers. They may negatively affect classification accuracy of the current ensemble. The class distribution means the areas with high density of each class inside a previous distribution region. Also, that delete mechanism may remove a previous distribution with an unchanged class distribution and the corresponding classifier.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions", "text": "We proposed an ensemble method of dynamically forming an ensemble on streaming unlabeled data. To build a new classifier, the proposed ensemble method selects samples that will be labeled according to changes in streaming data distribution. Those unlabeled samples are defined as suspicious streaming samples that do not belong to any previous distribution in the current ensemble.\nOur method constructed an ensemble more efficiently than existing ensemble methods that periodically build a new classifier according to a time interval. From synthetic streaming data sets, our method produced average 14.1% higher classification accuracy than existing ensemble methods [12], and the number of new classifiers for an ensemble reduced by average 12.6%. In particular, we showed that our method is much more robust than existing methods in a multi-class problem. Also, for the real-world problem (intrusion detection data set), our method produced similar average 96.29% accuracy to existing methods [3,11] using correct labels of only 0.007%(3410) samples of 485,377 streaming unlabeled data. While, existing methods [3,11] use correct labels of all streaming unlabeled data. We believe that in order to apply an ensemble method to real-world streaming data, an ensemble should demand correct classes of samples as little as possible.\nIn this paper, our method deleted the oldest classifier within the current ensemble. As mentioned in the discussion section, the delete mechanism considering only time feature might negatively affect the performance of an ensemble. Therefore, we are planning to extend the proposed method to delete a useless classifier in the current ensemble using other information, such as density, as well as time information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Projections techniques are broadly used to reduce input dimensionality in classification problems. Projection methods are designed to preserve in someway data original structure in the projected space, so projected data can be used to speed up classifiers training and sometimes help to avoid noise and over-fitting. PCA is probably the most popular projection method. It is used to reduce data dimensionality capturing a percentage of the variance in the original data. The main drawback of PCA is also its computational complexity. Random Projections (RP) [1], [2] have a lower computational cost. Some RPs can maintain pairwise distances in the projected space within an arbitrary small factor. Support Vector Machines (SVM) [3] are very accurate and stable classifiers. Small changes in the training dataset does not make very different SVMs. Therefore, it is difficult to get an ensemble of SVMs that performs better than a single SVM using state of art ensemble methods. One question to answer in this paper is if randomness inherent to RPs can be considered as a source of diversity that aims at getting accurate SVM ensembles. So, we are not interested in using projection for reducing input dimensionality but to increase SVM ensembles performance.\nRotation Forests [4] is an ensemble method for decision trees. It uses PCA to project different groups of attributes in each base classifier. In [5] is shown that PCA is better than RP for such ensemble method of decision trees. However, the essential ingredient of Rotation Forests is to project using those groups of attributes, which also makes base classifiers in the ensemble are trained different each other. That difference in training process does not produce very diverse base classifiers, but it seems to keep their individual accuracy. In [5] diversity is analyzed for RPs with and without splitting into groups of attributes. Projecting without splitting the input space turn into more diverse classifiers but also less accurate. Hence, the additional diversity obtained through the \"full\" random projection is not useful for the ensemble. That work was made for decision trees which are very sensitive to small changes. In our work we also test the effect of split projections using SVM as base classifiers.\nThe rest of the paper is organized as follows: Random Projections considered in this work are described in Section 2. An experimental study is presented in Section 3. Analysis of diversity is in Section 4. Finally, conclusions are summarized in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Random Projections", "text": "For projecting data a transformation matrix is needed. When an instance x is projected the vector containing its values is multiplied by this matrix obtaining a new vector (i.e. x projection). In random projections the matrix entries are random numbers. In this work three types of random projections have been used.\n1. Each entry in transformation matrix comes from a Gaussian random generator. This RP is denoted as Gaussian in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The entries values are", "text": "\u221a 3 \u00d7 x,\nwhere x is a random number taking the following values: \u22121 with probability 1/6, 0 with probability 2/3 and +1 with probability 1/6. This RP is denoted as Sparse in this work. 3. The entries are \u22121 with probability 1/2 and +1 with probability 1/2. This RP is denoted as Binary in this work.\nThe two latter are described in [1]. They are based on Johnson and Lindenstrauss theorem [6]. This theorem states that given > 0, an integer n and k a positive integer such that\nk \u2265 k 0 = O( \u22122 log(n)). For every set P of n points in R d there exists f : R d \u2192 R k such that for all u, v \u2208 P (1 \u2212 ) u \u2212 v 2 \u2264 f (u) \u2212 f (v) 2 \u2264 (1 + ) u \u2212 v 2 (1)\nHence, these two methods are fast to compute and aim at preserving pairwise euclidean distances in the projected space. It is expected that Sparse projection can contribute to diversity because the amount of zeros. The zeros would reject some original dimensions in the computation of some of the new dimensions. There are successful ensemble methods that also trains their base classifiers by excluding some existing features. In the Random Subspaces method [7] each base classifier only takes into account a subset of the attributes from the original space. The size of this subset of attributes is specified as a percentage.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Experimental validation has been made using WEKA [8] for the ensembles and projections. In order to limit the analysis scope to a manageable number of combinations, linear kernel was the only kernel considered for the SVMs in the experiment. LIBLINEAR [9] was used because it provides a fast linear kernel SVM implementation. Default parameters were used in all methods where not indicated.\nThe 62 datasets from UCI repository [10] used in the experiments are shown in Table 1. Nominal attributes are computed using Nominal to Binary transformation in all methods. Random Projections were used in the following ensembles:\n1. An ensemble of SVMs trained with projected data. Three sizes of projected space dimension have been tested (i.e. 75%, 100% and 125% of attributes).\nThe ensemble computes its prediction as the straight average of the probabilities predicted by its members. These configurations are denoted as RP-Ensemble n%, where n is the percentage indicating the dimension of the projected space. Hence 125% configutation makes input dimensionality grow. 2. A Rotation Forests [4] variant that replaces PCA projection by RPs and the Decision Trees by SVMs. Rotation Forests divides input space into different partitions for each base classifier. In this experiment the size of all partitions has been set to 5. For each partition an RP is computed. The sizes of the projections tested has been set again to 75%, 100% and 125% of the 5 attributes. Again we want to test projections that augment the original problem dimension. These configurations has been denoted as Rot-RP n%, where n is the percentage indicating the dimension of the projected partitions. For Rot-RP all nominal attributes have been previously converted into binary to increase the number or partitions. Column #I in Table 1 shows the resulting dimensionality after such conversion.\nThese 6 configurations have been tested using the 3 random projections described in previous section (i.e. Gaussian, Sparse, and Binary, denoted as G, S and B respectively), resulting into 18 RP based ensembles. These RP based ensembles have been tested against the base method on its own, (i.e. LIBLINEAR SVM) and the following state of art ensemble methods:\n-Bagging [11] of SVM.\n-Boosting of SVM. AdaBoost [12] and MultiBoost [13] versions were considered. Resampling version of both methods was used (i.e. training instances for each base classifier are obtained according its weights distribution). The number of subcommittees in MultiBoost was set to 10. -Random Subspaces [7] using 50% and 75% of original features (i.e. each nominal feature is considered as one feature). -Rotation Forests [4] replacing Decision Trees by SVM. Two configurations was tested changing the parameter that controls the proportion of variance retained by PCA projection, which was set to 75% and 100%. These configurations are denoted as Rot-PCA in tables. In order to compare Rotation PCAs to Rotation-RPs the size of attribute partitions was also set to 5, and nominal to binary conversion is applied beforehand as well.\nAll ensemble configurations in the experiment use 50 base SVM classifiers. The results were obtained using 5 \u00d7 2 stratified cross validations.\nTable 2 shows all the methods ordered by their average ranks [14], \"average ranks by themselves provide a fair comparison of the algorithms\". Average ranks are computed sorting all the methods by their accuracy for each dataset. Then the average position of each method through all datasets is assigned as average rank for that method. The six best ranked methods in the table are Rot-RPs configurations followed by Rot-PCA 100% and Bagging. Moreover, configurations projecting 125% of attributes from these six configurations can get better results than some configurations projecting 100%. There is a tie between SVM and RP-Ensemble 125% S, and the rest of methods are placed behind SVM. Projections reducing the input space get the bottom places. Table 3 shows wins, ties and losses of best Rot-RPs against SVM and the rest of ensembles ranked better or equal than SVM . Every Rot-RP configuration wins the RP-Ensemble configuration in the last row. Regarding the rest of rows (i.e. SVM, Bagging, Rot-PCA 100%) Sparse and Binary projections for Rot-RP 100% show the worse results. Differences of these Rot-RP configurations with the other three methods range from one to two victories. However, there are bigger differences when using Rot-RP 100% with Gaussian projections or with Rot-RP 125% with any type of random projection. For these four Rot-RP configurations there are always more wins than losses when compared with the other methods.\nAccording to [14] using a sign test one method is significantly better than other, with a confidence level of 0.05, if the number of wins plus half the ties is at least N/2 + 1.96 \u221a N/2. For N = 62 datasets, the number of necessary wins is 39. Table 3 shows:\n1. RP-Ensemble 125% S loses significantly against all configurations of Rot-RP 100% and 125%. This result agrees with [4] where experiments states that splitting input space is essential. 2. Stand alone SVM significantly loses against the 125% Rot-RP configurations. 3. Rot-PCA 100% is also very near to lose significantly against Rot-RP 125% S. This result is apparently in contradiction with [4] where ingredients of Rotation Forests method are analyzed. In that work PCA is better than RP for decision trees as base classifiers. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_5", "tab_20", "tab_20"]}, {"heading": "Kappa-Error Analysis", "text": "Ensembles success requires both accuracy and diversity from its base members. Kappa-error diagrams [15] are used to show how much accuracy and diversity is in base classifiers of an ensemble for a given dataset. In these diagrams a cloud is plotted. Each point of the cloud represents a pair of base classifiers. The coordinate x of a point is a measure of diversity between these base classifiers (i.e. \u03ba), and the y coordinate is the average error from both classifiers.\n\u03ba takes their values from \u22121 to +1. +1 means both classifiers agree always, 0 means that there is the same level of agreement as if outputs were random, and \u22121 means higher disagreements, which are not usual. So a cloud of points at bottom left corner means that base classifiers of an ensemble are accurate and diverse.\nEach kappa-error diagram can only contain a few clouds. Usually two clouds are plotted in each diagram representing the behavior of two ensembles for an only dataset. To show the behavior of 62 datasets kappa-error relative movement diagrams [16] are more suitable. Figure 1 shows these diagrams for our experiment. In these diagrams each arrow represents a dataset. Each diagram compares overall accuracy and diversity between two ensembles.\nComputation of kappa-error relative movement diagrams is as follows:\n1. For each dataset and ensemble in the study centers of clouds from kappa error diagrams are computed. For diagrams in this work, clouds come from computing 5 \u00d7 2 cross validation. 2. An arrow is drawn for each dataset connecting the centers of each pair of ensembles to compare. 3. Finally, all arrows are taken to the origin of coordinates.\nTherefore, if arrows that come from an ensemble A clouds to an ensemble B clouds are taking bottom left direction, it means that base classifiers in B are more accurate and diverse than in A. In Figure 1 all diagrams compare one of the most successful ensembles in the study against the winner of the average rank (i.e. Rot-RP 125% B). Arrows are prone to point bottom right corner in all diagrams, so it means that base classifiers in the winner ensemble are less diverse than in its competitors, but slightly more accurate. ", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusions", "text": "In this work RPs are used to build ensembles of SVMs. Three types of RPs were tested (i.e. Gaussian, Sparse, Binary) against traditional PCA technique and other state of art ensemble methods. Projections were applied in two ways: (i) transforming the whole instance with the same projection and (ii) splitting the instance into groups of attributes and projecting each group with a different projection. The latter strategy is taken from the Rotation Forest method.\nIn the experiments, configurations that reduce input dimensionality perform worse than the ones that keep or augment it. Projecting the instances without splitting also leads to poor results. Rot-RFs 125% and 100% are the best ranked configurations. However, a further analisys has uncovered that 125% configurations seems to perform slightly better than 100% configurations. Kappaerror relative movement diagrams show that best configuration does not lead to more diverse base classifiers, but more accurate. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The application of machine learning methods to DNA microarray gene expression data allows for systematic and high throughput analysis procedures, compared to histological and other methods [24]. Microarray classification is used to discover discriminating genes that allow identifying tissue types and it has been intensively applied to cancer diagnosis. However, microarray data classification is a challenging issue because of its high dimensionality and the small sample sizes. Typical values are around 10.000 gene expressions and a hundred or less tissue samples. To tackle this problem, the need to reduce dimensionality was soon recognized and the use of feature selection techniques has become customary [22]. Since the pioneering work of [7], a plethora of methods have been proposed combining different feature selection methods with different classification techniques.\nRecently, interest has aroused on the application of ensemble methods to microarray classification problems. An up to date review is provided in [19]. Ensemble methods combine the output of several individual classifiers to obtain predictions that are usually more precise and robust than a standalone classifier in several domains [12]. Among the newly proposed ensemble methods, Rotation Forest [21,11] seems to be an effective method for microarray classification, particularly when few genes are retained [23].\nRotation Forest generates precise ensembles because it finds a good tradeoff between accuracy and diversity of the classifiers it combines. The basic idea of Rotation Forest consists of projecting the input space on a random partition of the attribute set, generating axes rotations on each partition that are aggregated to create a new input space of the same dimension as the original, but where axes have been rotated. Originally, Principal Components Analysis, PCA, [9], keeping all components, was used to generate the axes rotation. However, recent works suggest that, for microarray classification problems, Independent Component Analysis (ICA) [10], may be a better option. To the best of our knowledge, there are two works [19,17] that found that Rotation Forest using ICA to transform the axes is preferable to Rotation Forest using PCA. However, these experimental studies are limited because they focus on one or two microarray data sets.\nIn this work, we extend this comparison to ten data sets to experimentally test if Rotation Forest with ICA is to be preferred to Rotation Forest with PCA on microarray classification problems. We focus on classification when a small number of genes is selected, because there is evidence that a small number of genes are sufficient [7,15,27].\nThe rest of the paper is organized as follows. Section 2 briefly describes Rotation Forest, provides some insight on the motivation of ICA for microarray gene expression problems and discusses the attribute selection methods employed. Section 3 describes the data sets and the experimental setting, while section 4 presents and discusses the experimental results. Section 5 summarizes the conclusions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rotation Forest", "text": "Rotation Forest is an ensemble method introduced in [21] that builds homogeneous ensembles by feature axes rotation. Hence, base classifiers must be sensitive to axes rotations, which makes decision trees a common choice as base classifiers.\nThe key idea of Rotation Forest is building accurate base classifiers using all features to construct each classifier, introducing diversity by generating different axes rotations. If instances of the learning problem are described by n features, Rotation Forest randomly selects K disjoint subsets of M features. In the original formulation, PCA is used to create an axes rotation on each K subset. No feature selection is made, because all components are preserved. A rotation matrix R i is built aggregating the rotation found for each K subset and rearranging it according to the original order of the features in the instance space. The original training set is processed by matrix R i to generate the training set T i , which is used to induce the base classifier C i . Due to the fact that probability of having different rotation matrix R i for a large number of classifier may be small, two additional heuristics are employed to favor base classifiers diversity. Instead of performing PCA projecting the whole training data on each K subset of features, a nonempty subset of classes is randomly selected for each K subset of features, and PCA is performed on K bootstrap samples of 75% of the training data.\nAs indicated in [11], splitting the feature space in K subsets is essential to the method, being responsible for providing diversity. The other important element is the rotation method used, because it influences the accuracy of the classifiers. In its original formulation, PCA was proposed to create the axes rotation on each subset of features, keeping all components in order to avoid losing discriminatory information. However, different transformations may be applied. In [11] PCA is compared to Nonparametric Discriminant Analysis [5], Sparse Random Projections and Random Projections [3]. PCA was found to be superior to the aforementioned alternatives. Recently, Independent Component Analysis [10] has been advocated as a preferred method for microarray gene expression problems. ICA looks for a set of features that are maximally independent for each other. In its linear version, it can be interpreted as finding the latent variables of the problem, whose linear combination models the observed data. Hence, it is sensible to apply ICA transformation to microarray gene expression, considering each extracted independent component as a potential biological process, like in [14], where ICA is found superior to PCA. Other authors also report on the advantages of using ICA as feature extraction method in microarray domain, for instance [16,28]. Hence, we consider both PCA and ICA, notating each corresponding version as RF-PCA and RF-ICA.\nTo compare Rotation Forest behavior with classic ensemble methods, Bagging [1] and Boosting [4] are also considered.\nAn important issue that strongly influences classifiers behavior on microarray data sets is the feature selection method. We have opted by SVM Recursive Feature Elimination (SVM-RFE), and ReliefF. SVM-RFE was introduced to gene selection in bioinformatics by Guyon et al. [8] and it is recognized as a very effective algorithm for microarray gene expression classification task [25,22]. ReliefF has been chosen because it is claimed [23] to behave well when few attributes are selected. Having two different attribute selection methods also allows checking whether the choice of RF-PCA or RF-ICA may be influenced by the attribute selection method.", "publication_ref": ["b108"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setting", "text": "Experiments were performed on 10 genomic and proteomic data sets, whose basic properties are summarized in Table 1. All data sets are publicly available at Kent Ridge Biomedical Data Set Repository [20].\nExcept for MLL and ALL, the data sets are related with binary classification problems. MLL is a three class problem, and ALL comprises seven different classes of pediatric acute lymphoblastic leukemia.\nThe combination of 2 attribute selection algorithms and 4 classification ensemble techniques produces 8 basic methods that are named joining their acronym by '+'. Hence, ReliefF+Bagging indicates that Bagging is applied to data sets filtered with ReliefF. Feature selection algorithms are invoked to obtain a specific number of features, given by an additional parameter. Since we are interested on the behavior of the methods when few attributes are selected, we have covered the range [4,128] in powers of 2. Consequently, we had a total of 48 (2 \u00d7 4 \u00d7 6) different configurations.\nFor each data set, error rates have been obtained with Nadeau and Bengio methodology that proposes the corrected resampled t-test for hypothesis testing [18]. This method requires 15 repetitions of training and test, where 90% randomly selected instances are used for training and the remaining 10% for testing. Feature selection techniques were applied internally to each training set to provide an honest evaluation.\nAll experiments were performed on the data mining framework Weka [26], with default values for all parameters, except in some cases: in SVM-RFE we set to 20% the number of features eliminated by the algorithm in each cycle; for all the classification ensemble techniques we used J48 pruned as the base classifiers and we set to 100 the number of base classifiers; in Rotation Forests we consider groups of 3 features (default value).\nWe have integrated a Java module of FastICA [13] as a filter in Weka. Thus, we can choose the axes projection method in Rotation Forest. The data are previously normalized.\nRankings and various statistical tests were performed with the publicly available software facilitated in [6].", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Experimental Results", "text": "Table 2 summarizes the results obtained in our experiments, showing for each [method, number of selected attributes, data set] the average accuracy. We mark in bold the best results for each data set and selected number of attributes, and also we ordered the different methods always under the same schema. We can observe the following behavior. As a general rule, the accuracy increases as the number of attributes increases. With a very low number of attributes, there is no specific method that clearly performs better than the rest. And there is a clear trend in favor of Rotation Forests (either PCA or FastICA) when the number of selected attributes increases. Table 3. Ranking of the methods using the significant differences from all pairwise comparisons. \"W\" stands for \"wins\" and \"L\" for \"losses\". ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_20"]}, {"heading": "Method", "text": "W-L W-L W-L W-L W-L W-L Total #\nV M -R F E + R F -P C A 0 4 3 8 5 3 2 3 S V M -R F E + R F -F a s t I C A 0 3 1 2 2 5 1 3 ReliefF+RF-PCA 0 -2 2 0 2 3 5 ReliefF+RF-FastICA 1 -4 2 0 3 2 4 SVM-RFE+Boosting -1 2 -3 1 0 1 0 ReliefF+Boosting 0 -1 -2 -1 -1 -3 -8 ReliefF+Bagging 2 -2 0 -4 -6 -4 -14 SVM-RFE+Bagging -2 0 -3 -6 -5 -7 -23\nTable 3 shows a dominance ranking of the methods according to the difference between the number of times each method has been significantly better and significantly worse than another method, considering all pairwise comparisons for a given number of genes. Last column shows the total dominance, obtained adding the results for each considered number of attributes.\nWe can see that selecting four attributes, only Bagging and Rotation Forest with FastICA, both using ReliefF, have a difference (Wins-Losses)> 0; selecting eight attributes, both Rotation Forest (with PCA and FastICA) and Boosting have that difference > 0. Increasing the number of attributes selected, Rotation Forest with PCA and FastICA achieve to improve those differences whereas in the other methods they get worse.\nAccording to Table 3, the best results are obtained using Rotation Forest with PCA or FastICA and SVM-RFE, being PCA better than FastICA. The following methods in the ranking are both Rotation Forest with ReliefF.  When several data sets are involved, using only the average accuracy sometimes blurs the effect of the different algorithms. Hence, we decided to use rankings to establish which algorithms are the most successful in a global view [2]. Using the freely available software [6], we obtain some statistics and various rankings that are shown in next figures.\nFigure 1 shows clearly two different branches: one containing the classic ensemble methods (Bagging and Boosting) on the upper part of the figure, and other branch containing those algorithms using Rotation Forest with PCA and FastICA as projection techniques. Notice that the lowest ranking positions are related with better performing algorithms.\nIman-Davenport's test rejects the null hypothesis (all methods are equivalent on their ranks) for all the algorithms working with more than 4 selected attributes.\nWe perform a study for each number of selected attributes using Bonferroni-Dum's procedure, which compares the best algorithm against the rest. The results show that: a) For more than 8 selected attributes, the null hypothesis can be rejected for Bagging in all the attribute selection methods used. b) For Boosting similar conclusions can be drawn but limited to ReliefF algorithm, and c) For very low number of selected attributes (less than 16), none of the methods can be rejected under the null hypothesis, except Bagging with SVM-RFE and 8 attributes.\nWe studied also the behavior of the different classifying techniques in relation with the attribute selection method used.\nThe rankings showed in figure 2 can be interpreted in the same way as we have done earlier. Subfigures (a) and (b) show clearly that Rotation Forests outperforms the classic ensemble methods, and also that only for a low number of selected attributes and ReliefF, the null hypothesis cannot be rejected.\nIn figure 2(c) can be seen a study of the behavior of Rotation Forest, comparing only the performance of the two projection techniques used in this work. In this case, there are no strong conclusions to be drawn, and it can be said that statistically speaking, all the algorithms involving Rotation Forests are equivalent, that is, the null hypothesis about equal rank position cannot be rejected. But by mere inspection of the graphic, it can be seen that SVM-RFE as attribute selection technique and Rotation Forest with PCA produces usually better results when the number of attributes is greater than 16; whereas ReliefF with Rotation Forests and FastICA are preferred for low attribute number.\nTo consider the effect of pruning, experiments have also been performed combining unpruned trees. Experimental results are quite similar, allowing to extract the same conclusions.", "publication_ref": [], "figure_ref": ["fig_2", "fig_15", "fig_15"], "table_ref": ["tab_20", "tab_20"]}, {"heading": "Conclusions", "text": "Experimental work has shown that using Independent Component Analysis as a projection method in Rotation Forest does not generally improve on Rotation Forest with Principal Component Analysis. Moreover, according to dominance ranks, Rotation Forest PCA is the preferred method.\nAccording to Friedman rankings, for 16 or more attributes, Rotation Forest, both PCA and ICA, outperforms Bagging and Boosting, but differences blur for 8 and especially 4 attributes using ReliefF as attribute selection method.\nA direct comparison of Rotation Forest methods does not find statistically significant differences on their rankings. This conclusion differs from previously found results for some specific data sets. Several issues may influence the behavior of the algorithms, such as data nature, attribute selection technique, and Rotation Forest parameters and projection method. Further research is needed to properly understand the influence of the projection method on Rotation Forest behavior.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Ensembles [1] are combinations of models. In many situations, an ensemble gives better results than any of its members. Although they have been studied mainly for classification, there are also ensemble methods for regression.\nThe models to be combined have to be different, otherwise the ensemble is unnecessary. One way to have different models is to construct them with different methods. Nevertheless, there are ensemble methods that combine models obtained from the same method. Most of these ensemble methods change the dataset in some way.\nIn Bagging [2] each member is trained with a sample of the training data. Normally, the size of the sample is the same than the size of the original training data, but the sample is with replacement. Hence, some training examples will appear several times in the sample while others will not appear. The prediction of the ensemble is the average of its members predictions.\nIn Random Subspaces [3] each member is trained with all the training examples, but with a subset of the attributes. The dimension of the subspaces is a parameter of the method. The prediction of the ensemble is also the average of the predictions.\nBagging and Random Subspaces can be used for classification and for regression. AdaBoost [4] initially was a method for classification, but there are some variants for regression, such as AdaBoost.R2 [5]. In these methods, each training example has a weight. Initially, all the examples have the same weight. The construction of the ensemble members must take into account the examples weights. After an ensemble member is constructed, the examples weights are adjusted. The idea is to give more weight to the examples with greater errors in the previous iterations. Hence, in the construction of the next member, these examples will be more important. The ensemble members also have weights, they depend on their error. In AdaBoost.R2, the predicted value of the ensemble is a weighted median. In [6], this method was the one with the best results among several ensemble methods for regression.\nIterated Bagging [7] is a method for regression based on Bagging. It combines several Bagging ensembles. The first Bagging ensemble is constructed as usual. Based on the predictions of the previous Bagging ensemble, the values of the predicted variable are altered. The next Bagging ensemble is trained with these altered values. These values are the residuals: the difference between the real and the predicted values. Nevertheless, these predictions are not obtained using all the members in the Bagging ensemble. The error of the predictions for a training example would be too optimistic, the majority of the ensemble methods have been trained with that example. These predictions are obtained using the out-of-bag estimation: the prediction for an example is obtained using only those ensemble members that were not trained with that example. The prediction of an Iterated Bagging ensemble is the sum of the predictions of its Bagging ensembles. According to [8], Iterated Bagging is in general the most effective method.\nIn Negative Correlation Learning [9], the ensemble members are constructed in parallel. The networks are trained using a penalty term, they penalize the similarity of the current network with the ensemble. This method has not been considered in this work because it requires the use of a modified base method.\nThe rest of the paper is organised as follows. Next section details the experimental settings. The results are discussed in section 3. Section 4 is dedicated to diversity error diagrams. Finally, section 5 presents some concluding remarks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Settings", "text": "The experiments were conducted using 5\u00d72 fold cross validation [10]. The performance of the different methods over different datasets was measured using root mean squared error (RMSE). The base models were Multilayer Perceptrons. Ensemble size was 50.\nSeveral ensemble methods were considered:\n-Randomization. When the base method has a random element, different models can be obtained from the same training data. In the case of Multilayer Perceptrons, the initial weights are initialized randomly. Randomization is an ensemble of such randomizable models, which prediction is the average of the members predictions. -Bagging [2]. -Random Subspaces [3]. For the dimension of the subspaces, two values were considered: 50% and 75% of the number of attributes. -AdaBoost.R2 [5]. This method can be used with different loss functions. Three are proposed in [5] and used in this work: linear, square and exponential. The suffixes \"-Li\", \"-Sq\" and \"-Ex\" are used to denote the used function. Moreover, methods based on AdaBoost can be used in two ways [11]. In the reweighting version, the base model is trained with all the training data, it must take into account the weight distribution. In the resampling version, the base model is trained with a sample from the training data. This sample is constructed taken into account the weights. These versions are denoted with \"-W\" and \"-S\". -Iterated Bagging [7]. The used configuration was 5\u00d710: Bagging is iterated 5 times, the ensemble size of each Bagging is 10.\nMoreover, other methods were included in the study, as a baseline for the comparisons:\n-A single MLP.\n-Linear regression. Two versions were considered: using all the features and using only the selected features with the method described in [12]. -Nearest neighbors. There are two versions, in the first one the number of neighbors is 1. In the other, the number of neighbors is selected using \"leave one out\".\nWeka [13] was used for the experiments. It includes the base method (Multilayer Perceptron), Bagging and Random Subspaces. The rest of the methods (i.e., Iterated Bagging and AdaBoost.R2), were implemented in this library. For Multilayer Perceptrons the default settings in Weka were used. There is a hidden layer, the number of neurons is half the number of attributes (including the objective attribute). The learning rate is 0.3, the momentum is 0.2 and the number of epochs is 500.\nTable 1 shows the characteristics of the 61 considered datasets. They are available in the format used by Weka 1 . 30 of them were collected by Luis Torgo 2 .", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results", "text": "In order to compare all the configurations considered, average ranks [14] were used. For each dataset, the methods are sorted according to their performance. The best method has rank 1, the second rank 2 and so on. If there are ties, these methods have the same rank, the average value. For each method, its average rank is obtained as the average value over all the considered datasets. According to [14], \"average ranks by themselves provide a fair comparison of the algorithms\". Table 2 shows the methods sorted according to their average ranks.\nA single MLP has worse average rank than k-Nearest Neighbors and Linear Regression. Moreover, an ensemble of MLP based only in the random initialization of weights is also worse than these baseline methods. Nevertheless, several ensembles of MLP have better average ranks than the baseline methods.\nThe best average rank is for Bagging. The second method is Random Subspaces, using 75% of the attributes. The next positions are for AdaBoost.R2, the version that uses resampling. On the other hand, the results for AdaBoost.R2 are among the worst when training the base models directly with the weighted instances.\nTable 3 shows a direct comparison of a single MLP and Bagging with the other methods. When comparing two methods, the number of datasets where one method has better, equal, or worse results than the other is calculated. According to [14], using a sign test, one method is significantly better than other, with a confidence level of 0.05, if the number of wins plus half the ties is at least N/2 +1.96 \u221a N /2. For N = 61 datasets, this number is 39. The number of wins, ties and losses and the average ranks are calculated using a direct comparison of the results for the different methods. Nevertheless, they do not take into account the size of the differences. For this purpose, we use the quantitative scoring [15,6]. Given the results for two methods i and j in one dataset, this score is defined as\nS i,j = RM SE j \u2212 RM SE i max(RM SE i , RMSE j )\nWhere RM SE i is the root mean squared error for the method i. Unless both methods have zero error, this measure will be between \u22121 and 1, although it can be expressed as a percentage. The sign indicates which method is better.\nFigure 1 shows these scores (as percentages) for the considered methods, compared with Bagging. The score is calculated for each dataset and the datasets are sorted according to its score. The number of values above and below zero corresponds with the number of wins and losses in Table 3. When comparing two methods with these graphs, it is desired to have more positive values than negative, but it is also desirable that the absolute values were greater for positive scores than for negative scores. In this case there are more positive values, and the greater absolute scores are also for positive values.\nThe results for Iterated Bagging are clearly worse than the results for Bagging. This can be caused by the selected configuration, 5 iterations of Bagging with ensembles. That configuration was selected because it was desired to compare ensembles with the same number of base models, 50. That is, the Bagging configuration (10 base models) that is iterated, is not the same that the Bagging configuration (50 base models) that is not iterated. If more base models were allowed, Iterated Bagging could be used with Bagging of 50 models and it could improve the results of Bagging. Successful ensembles are formed by models with low errors, but that are diverse. These two objectives are contradictory, because if the errors of two models are small, they cannot be very different. Several diversity measures had been proposed in order to analyze the behaviour of ensemble methods [16].\nOne of the techniques used is diversity-error diagrams [17]. They are scatter plots, there is a point for each pair of models. The horizontal axis represents the diversity between the two models, for classification, usually \u03ba (kappa) is used. The vertical axis represents the average error of the two models.\nIn regression, several error measures can be considered, in this work RMSE was used:\nRM SE = n i=1 (a i \u2212 p i ) 2 n\nWhere a i are the actual values and p i are the predicted values.\nFor measuring the diversity, the RMSE of one of the models with respect to the other was used:\nRM SE = n i=1 (q i \u2212 p i ) 2 n\nWhere p i and q i are the predictions of the two models. Note that with this measure, bigger values indicate more diversity, while for kappa, bigger values indicated less diversity.\nFigure 2 shows these diagrams for the best three methods according to the average ranks and the datasets with more instances. In general, the ensemble members for Bagging have the smallest errors, but they are the less diverse. In this case, it seems that the additional diversity in Random Subspaces and AdaBoost.R2 does not compensate the increased errors of the ensemble members.", "publication_ref": [], "figure_ref": ["fig_2", "fig_15"], "table_ref": ["tab_5", "tab_20", "tab_20"]}, {"heading": "Conclusions", "text": "The performance of ensemble methods for regression have been studied, using Multilayer Perceptrons as base method. The considered ensemble methods have been Randomization, Random Subspaces, Bagging, Iterated Bagging and AdaBoost.R2.\nThe best method, according to the average ranks, is Bagging, followed by Random Subspaces (being the subspace size 75% of the original space). These results disagree with previous studies [8,6,18], where Iterated Bagging or AdaBoost.R2 had better results. One source of the differences can be the different datasets used, but in [18] the considered datasets were the same than for this work. The difference in the results can be caused by the different base models considered, Multilayer Perceptrons instead or regression or model trees.\nThese results were obtained with predefined settings: the ensemble size (50), the default parameters for the Multilayer Perceptron, the subspace sizes (50% or 75%) for the Random Subspaces method, . . . Different settings could give different conclusions.\nThe values of some parameters could be adjusted, although increasing significantly the computation time. Nevertheless, the comparison is fair because the same base classifier method was used and the ensembles were formed by the same number of base classifier.\nDiversity-error diagrams have been used to study the behavior of the ensemble members. In this case, it seems to be more important to have member with low error than high diversity, because this is the situation for the best method, Bagging.\nThe best results are obtained with one of the more simple ensemble methods, Bagging, hence an open question is if there are other ensemble methods more suited for combining Multilayer Perceptrons in regression tasks.\nGiven the disparity of the results for the ensemble methods when using trees and Multilayer Perceptrons as base models, it seems interesting to study the behavior of ensemble methods using other base models, specially other types of Neural Networks, such as RBF (Radial Basis Function) Networks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In the Artificial Intelligence field, the DX community has developed Consistency Based Diagnosis, CBD, as the major paradigm for model based diagnosis [3]. CBD performs fault detection and isolation with just models for correct behavior, but the absence of fault models knowledge is partly responsible of the low discriminative power that CBD may exhibit [6]. Usually, to solve this drawback, knowledge about fault modes is introduced. In this work, we have considered the predictive approach, which use models of fault modes to estimate faulty behavior, as in Sherlock [5] or TRANSCEND [10]. However, adding fault modes increase complexity. For N components in a system, fault isolation must discriminate among 2 N modes (ok or fault). Fault identification must discriminate among K N modes (for an average K behavioural modes).\nTo avoid this problem, we propose to use CBD without fault models, and include fault identification knowledge obtained with machine learning techniques, including ensemble methods.\nThere are several systems that couple model based diagnosis with machine learning. In [19], from the neural network perspective, a typology of approaches is presented. In [2] a review of the compilation approach is included. Currently, three basic approaches can be found in literature, with some proposals in between: compilation, residual classification and model learning. The compilation method uses machine learning to induce classifiers that model the relations from symptoms to faults. The main objective of the compilation approach is to improve the efficiency of model-based diagnosis usually to perform on-line diagnosis [2,11]. The residual classification method uses machine learning to induce classifiers that model the relation from residuals to faults, with the objective of improving the robustness of the isolation from residuals [14,19]. The model learning method uses machine learning to induce models of the system. The objective of this approach is obtaining models of complex systems whose behavior is not well known [13,19,8].\nIn this work, the compilation approach is used to combine CBD with machine learning techniques, maintaining the soundness of the CBD approach. CBD is in charge of fault detection and isolation, while machine learning is used for refining CBD isolation providing a first step towards fault identification: discrimination among fault modes. The identification problem is approached as a multivariate time series classification task and time series classifiers are induced off line from simulated data. This approach has been previously tested in [1].\nThe aforementioned method has the inconvenient that no information is used regarding which observations are relevant to each mode fault. Actually, a single classifier was built, having as inputs all available observations and as outputs all fault modes considered. This is a naive methodology that is not appropriate for systems with a large number of observations and fault modes. A better approach is to exploit the structural knowledge used by the CBD method to define the input-ouput structure of the classifiers.\nIn this work we propose to use Possible Conflicts, PCs, [15] for both Fault Detection and Isolation, and system decomposition. In the rest of the paper, we briefly describe the structural decomposition induced by PCs and the machine learning techniques used. Then we explain how PCs decomposition is used to define the structure of an ensemble of classifiers. This ensemble can be easily integrated in the CBD cycle without affecting isolation soundness. The approach is tested in a simulated scenario and systematically evaluated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Possible Conflicts for On Line CBD and System Decomposition", "text": "The computation of possible conflicts is a compilation technique which, under certain assumptions, is equivalent to on-line conflict calculation in the General Diagnostic Engine [4], GDE, or Fault Detection and Isolation using ARRs obtained through structural analysis. A detailed description of consistency based diagnosis with possible conflicts can be found in [15].\nThe main idea behind the possible conflict concept is that the set of subsystems capable to generate a conflict can be identified off-line, in a three steps process:\nThe first one represents the system as an hyper-graph, H SD = {V, R}, where V is the set of variables of the system and R = {r 1 , r 2 , . . . , r m } is a family of sub-sets in V , where each r k represents a constraint in the model.\nThe second step looks for minimal over-constrained subsystems, called Minimal Evaluation Chains (MEC), H ec = {V ec , R ec }, where V ec \u2286 V , R ec \u2286 R. Evaluation chains are necessary conditions for a possible conflict to exist. Additionally, each M EC identifies, by definition, a subsystem of H SD .\nIn the third step, extra knowledge is added to assure that a M EC, H ec , can be solved using local propagation criterion. If it is possible, a Minimal Evaluation Model (MEM) is defined, H mem = {V mem , R mem }, with V mem = V ec and R mem = {r 1 k1 , r 2 k2 , . . . , r m km }. r i ki is a causal constraint obtained assigning a causality to r i \u2208 R ec . The set of relations of a M EC with at least one M EM is called a possible conflict (PC).\nActually a M EM is a computational model with discrepancy detection capability. If there is a discrepancy between predictions from those models and current observations, the possible conflict is confirmed as a real conflict. Afterwards, diagnosis candidates are obtained from conflicts following Reiter's theory.\nAdditionally, M EM s provide a mean to decompose the original system, H SD = {V, R}. Each M EM is uniquely related to one M EC. And each M EC identifies, by definition, a subsystem of H SD . Then, the set of M EM induces a decomposition on H SD . The decomposition is not exhaustive (some variables and relations may not be included in any subsystem) neither exclusive (some relations and variables may belong to various subsystems), but it is systematic because the algorithms that compute possible conflicts find every M EC and M EM . An important property of this decomposition is that every found subsystem is minimal in the sense that no proper subsystem has discrepancy detection capability. This assures that the decomposition induced by M EM s is unique.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Time Series Classifiers for Fault Identification", "text": "In this work, fault identification is approached as a problem of multivariate time series classification. This is adequate for those dynamic systems where the set of available observations (measurements and systems parameters and settings) is fixed and no new measurement is available to refine diagnosis candidates. We restrict ourselves to discrete and finite time series of real numbers. Several systems, like industrial continuous processes, embedded or autonomous systems, satisfy these requirements. The historical values of each observable variable may be considered as a univariate time series and the set of historical values of all the variables of interest as a multivariate time series. Therefore diagnosis of past and current faults may be achieved analyzing the past multivariate time series, particularly inducing multivariate time series classifiers.\nTo test the effect of system decomposition, we have selected six machine learning techniques: Decision Trees (DT), Naive Bayes (NB), Support Vector Machines (SVM) (with linear or perceptron kernel), Nearest Neighbor (k-NN) (with Dynamic Time Warping, DTW, as a dissimilarity measure), and Stacking Nearest Neighbor (Stack-k-NN).\nDT, NB, SVM and k-NN are standard machine learning techniques. Stack-k-NN is a variant of the Stacking ensemble method [18], adapted to generate a multivariate time series classifier from univariate classifiers. k-NN is used to classify each univariate time series that form the multivariate one. An additional classifier, Naive Bayes in this work, is used to generalize the output of the univariate classifiers. This configuration has been tested in [1]. Experimental results have shown that Stack-k-NN outperforms the five standard machine learning algorithms considered on various data sets.\nWhen necessary, the machine learning algorithms have been adapted to provide class confidences instead of assigning a single class. This feature will be used to combine the classifiers outputs when decomposition selects more than one classifier.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Attribute and Class Selection via Possible Conflicts", "text": "In previous works, knowledge about the system to be diagnosed has not been used to decide the input-output structure of the classifiers. A single global classifier, Classif ier, was constructed. Let n be the number of available observations. Then, the input space of the classifiers is the multivariate time series space I = T S 1 \u00d7 T S 2 \u00d7 ... \u00d7 T S n , with T S i the time series space associated to observation i. Respecting fault modes, we assume system detectability, and only consider fault modes that may be detected by some possible conflict, according to the fault signature matrix of the system. Let's C i be the set of fault models associated to P C i . Then, the set of classes, C, is defined as C = \u222a i C i , i = 1, 2...m, with m the number of P Cs. Therefore, the classifier Classif ier, applies I into C:\nClassif ier : I \u2212\u2192 C\nAlthough Stack-k-NN behaves satisfactory with this simple setting for a small size problem [1], it is not to be expected that the method may scale up to systems with hundreds of observations and fault modes.\nOn practical applications, a great effort is usually done to analyze the characteristics of the problem. It is well known [7,9,16] that the induction of classifiers is increasingly difficult with the number of classes to consider and with the number of attributes; particularly harmful is the presence of irrelevant and/or redundant attributes. A lot of work has been done to automate this analysis, specially in the field of attribute selection. However, it is still claimed that a thorough knowledge of the problem and the help of problem experts is necessary in all but simple applications [17].\nOne obvious way of simplifying the problem is decomposing the physical system, looking for subsystems where the space I has lower dimension and the set C has lower cardinality. The Possible Conflict approach provides a systematic method to decompose a physical system.\nPossible Conflict Decomposition: Class Decomposition. We have shown in section 2 that the set of M EM s induce a unique decomposition on the system. Because each M EM i is only sensible to the mode faults of C i , we have that |C i |\u2264 |C|. This provides a first decomposition step that reduces the number of classes of a classifier. If we have m Possible Conflicts, then we can replace the single global classifier, Classif ier, by m local classifiers, Classif ier-C i , where the suffix C indicates that decomposition has been applied to class selection and subindex i identifies the P C used. The structure of these classifiers is:\nClassif ier-C i : I \u2212\u2192 C i\nThese local classifiers, Classif ier-C i , still work on the same input space, but have the potential to work with a smaller number of classes.\nPossible Conflict Decomposition: Attribute and Class Decomposition. The decomposition may be carried out a step further, to the level of the M EM subsystem. Now, the M EM s project the input space I into m subspaces I i , with m the number of M EM s and I i the space of time series associated to the observations of M EM i , that is, the observations of V memi . The projections are not exclusive and several subspaces may share several dimensions, that is, observations. Note also that some dimensions of I may not be projected in any subspace.\nWith this last level of decomposition, the original global classifier, Classif ier, is replaced by m local classifiers, Classif ier-AC i , where the suffix AC indicates that decomposition has been applied to classes and observations (attributes). The structure of these classifiers is:\nClassif ier-AC i : I i \u2212\u2192 C i\nPossible Conflict Decomposition: Ensemble of Classifiers. During the identification stage, several classifiers might be invoked for the same fault. Actually, the m local classifiers may be considered as an ensemble of classifiers. We define Ensemble-C (resp. Ensemble-AC) as the ensemble of m local classifiers Classif ier-C i (resp.\nClassif ier-AC i ). Then, a mechanism is needed to combine the class prediction of the local classifiers. We use a simple average scheme. The local classifiers assign a probability to each class of their output domain and a null probability to the remainder classes. These probabilities are averaged and provided as the output of the ensemble. By definition, the family of C i is a coverage of C, although not a mutually exclusive one, because it is possible to have C i \u2229 C j = for some i = j. Hence some degree of overlapping among the set of classes is to be expected. There is also some overlapping among the dimensions of the input space of the classifiers when attribute selection is applied. From the machine learning point of view, this is a desirable property: we may have several classifiers providing different confidence to the same classes. This has the potential to generate diversity, which is a much looked after property on the ensemble methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Case Study", "text": "For this work, we have used the laboratory scale plant shown in figure 1. It is made up of four tanks {T 1 , . . . , T 4 }, five pumps {P 1 , . . . , P 5 }, and two PID controllers acting on pumps P 1 , P 5 to keep the level of {T 1 , T 4 } close to the specified set point. To control temperature on tanks {T 2 , T 3 } we use two resistors {R 2 , R 3 }, respectively.\nIn this plant we have eleven different measurements: levels of tanks T 1 and T 4 -{LT 01, LT 04}-, the control action of PID controllers on pumps {P 1 , P 5 } -{LC01, LC04}-, in-flow on tank T 1 -{F T 01}-, outflow on tanks {T 2 , T 3 , T 4 } -{F T 02, F T 03, F T 04}-, and temperatures on tanks {T 2 , T 3 , T 4 } -{T T 02, T T 03, T T 04}-. Actions on pumps {P 2 , P 3 , P 4 }, and resistors -{R 2 , R 3 }are also known.\nThe plant may work with different operation modes. In this work a simple setting without recirculation -pumps {P 3 , P 4 } and resistor R 2 are switch off-has been chosen.   \nP C1 t1 dm , t1 f b1 , t1 f b2 T1, P1, P2 LT 01 P C2 t1 f b1 , t2 dm , t2 f T1, T2, P1 F T 02 P C3 t1 f b1 , t1 dE , t2 dE , t2 dm T1, P1, T2 T T 02 P C4 t1 f b2 , t3 dm , t3 f T1, P2, T3 F T 03 P C5 t1 f b2 , t1 dE , t3 dE , t3 dm , r3p T1, P2, T3, R3 T T 03 P C6 t4 dm T4 LT 04 P C7 t4 f b T4, P5 F T 04\nWe have used common equations in simulation for this kind of process:  \nP C1 1 1 1 1 1 1 1 1 1 P C2 1 1 1 1 1 1 1 P C3 1 1 1 1 1 1 1 1 P C4 1 1 1 1 1 1 P C5 1 1 1 1 1 1 1 P C6 1 1 1 1 1 P C7 1 1 1 1 1\nThe fault signature matrix provides the class decomposition. For each P C i , C i includes the fault modes marked 1 on the corresponding row. For instance, C 3 = {f 4 , f 7 , f 11 }.\nAttribute and Class Decomposition. The input an output observations of each possible conflict, obtained from each M EM , are shown in table 2. Now, each subspace I i includes the observations marked 1 on the corresponding column. For instance,\nI 3 = T S F T 01 \u00d7 T S F T 02 \u00d7 T S LT 01 \u00d7 T S LC01 \u00d7 T S T T 02 .", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "We have resorted to a detailed, non linear quantitative simulation of the plant. We have run twenty simulations for each class, adding 2.5% noise in the sensors readings. We have modeled each fault class with a parameter in the [0, 1] range. Each simulation lasted 900 seconds. We randomly generate the fault magnitude, and its origin, in the interval [180,300]. The sample rate is 3 seconds. Since we just have eleven observations, then each simulation will provide eleven series of three hundred numeric elements. For complexity reasons, we only consider single faults. We also have assumed that the system is in stationary state before the fault appears.\nTo evaluate the viability of the Possible Conflict decomposition, a simplified scenario has been developed. The simulation data has been used to tune the possible conflicts thresholds. Hence, each instance of a fault mode simulation confirms the possible conflicts as indicated in the fault signature matrix. This has the inconvenient of introducing some bias on the evaluation method, because in a real scenario a possible conflict may not be activated when a fault mode is present. Consequently, the empirical results only compares the behavior of the different classifiers in the ideal case when fault detection and isolation is error free. Time series classifiers are invoked once real conflicts have been confirmed with a fragment of series from t to the min(current time, t + maximum series length), with t a time instant previous to fault detection. When decomposition is applied, information on confirmed conflicts is also provided to the ensemble method.\nAll the experiments have been performed on the WEKA tool [17]. Error estimations and hypotheses testing have been obtained with the corrected resampled t-test. Instead of standard cross validation, we have made fifteen training-test iterations. In each iteration, we have randomly selected 90% of available data for the training set and 10% for the test set. This experimental setting is proposed in [12]. We have tested the system for 30, 40, 50, 70, and 100% of the total time series lenght. With 30% of the series the system is beginning the transition to a new stationary state, that is nearly reached with a 70%. Table 3 shows the result obtained. The column Class (resp. Attribute) indicates if Class decomposition has been applied (resp. Attribute decomposition).\nWithout decomposition, first row of each learning method, DT and Stack-1-NN provide the best results. Both methods are less sensitive to irrelevant attributes, specially Stack-1-NN.\nEnsembles based on class decomposition systematically increase the accuracy of the classifiers. Improvement is particularly important for series length of 30% for all base classifiers, when fault effects are starting to manifest. Enhancement is also notable for SVM with kernel perceptron as base classifier, which achieves the highest accuracy except for lengths of 30% and 100%, where Stack-1-NN and SVM with liner kernel, respectively, obtains slightly better accuracies.\nThe behavior of ensembles based on class and attribute decomposition depends on the base classifiers and the considered length of the series. Accuracies always increase for 30% series length. This is an interesting result, because it facilitates precise early fault isolation. For other lengths, DT and NB ensembles may perform slightly worse. Accuracy of SVM with kernel perceptron slightly decreases, while SVM with linear kernel clearly worsen. Stack-1-NN still improves. The base classifier that benefits most of attribute decomposition is 1-NN. Somehow this is something to be expected, because nearest neighbor algorithms degrade severely with irrelevant attributes.\nDiscussion. Several ensemble methods have been tested in this work. Experimental results indicate that in the absence of structural knowledge, Stacking-1-NN is a good choice. As it was expected, ensembles based on structural knowledge improve on global base classifiers. Ensembles based on class decomposition with SVM kernel perceptron as base classifier provides excellent results, only exceed by ensembles of Staking 1-NN for 30% series length.\nEnsembles based on class and attribute decomposition provides the maximum accuracy, except for 100% series length, where SVM with linear kernel wins by a small margin. Table 3 shows in bold the methods with highest accuracy for each series length. Significance tests with 0.05 level have been performed against all the other methods in the corresponding column. For 30, 40%, 1-NN is the preferred method: accurate and simple. For other series length, ensembles of Stacking 1-NN seems slightly better.\nAnd additional benefit of attribute decomposition, not considered in this evaluation, is that it may reduce by an important factor the training time (classification time if 1-NN) respect to class decomposition, because all the local classifiers works on a smaller input space if attribute decomposition is applied.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_20", "tab_20"]}, {"heading": "Conclusions and Further Work", "text": "This work has shown how Possible Conflicts technique may be used to decompose a system. The decomposition defines the structure of an ensemble of classifiers that may be integrated on line in the Consistency Based Diagnosis cycle, providing fault identification information. The integration relies on the compilation approach of Machine Learning and Model Based Diagnosis.\nThe proposal has been evaluated on a simplified scenario on various machine learning methods. Experimental results show that Class decomposition improves the accuracy of the classifiers. The effect of Attribute decomposition is not so consistent, although improves the accuracy of the classifiers at the first stages of the diagnosis processes.\nFuture work requires a thorough evaluation of the proposal on the same pilot plant used in this paper. Data from real faults have been collected. Also, additional simulation data must be generated, to evaluate the behavior of the classifiers independently of the possible conflicts detection and isolation accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Being an outgrowth from classical genetic algorithms, Estimation of Distribution Algorithms (EDAs) [1] have emerged as a robust optimization methodology. Instead of obtaining new solutions by traditional recombination and mutation operators, EDAs gradually develop and maintain a probability distribution of promising candidates. New offspring can be generated by sampling this underlying distribution. Bayesian Optimization Algorithm (BOA) [2] belongs to the class of multivariate EDAs. BOA is capable of tackling multivariate interaction problems decomposable into subproblems of bounded difficulty. Extended Compact Genetic Algorithm (ECGA) [3], Factorized Distribution Algorithm (FDA) [1], and Estimation of Bayesian Network Algorithm (EBNA) [4] are other examples of state-of-the-art EDAs. The strength of BOA is brought into the continuous world by Real-coded Bayesian Optimization Algorithm (rBOA) [5,6].\nA standard BOA is composed of the following steps.\nStep 1: Set i \u2190 0. Randomly generate first population P(0). Evaluate P(0).\nStep 2: Select a parent set S(i) of promising solutions from P(i).\nStep 3: Learn a Bayesian network B(i) from S(i).\nStep 4: Generate new solutions population O(i) by sampling from B(i).\nStep 5: Evaluate entire O(i).\nStep 6: Create P(i + 1) by replacing some solutions of P(i) with O(i).\nStep 7: Set i \u2190 i + 1. If the termination criteria are not met, go to Step 2.\nBOA uses Bayesian networks [7] to model the regularities of the promising solutions. The process of learning a Bayesian network from a seleted population is concisely presented in Pelikan et al. [2]. A Bayesian network encodes the following joint probability distribution\np(X 0 , X 1 , ..., X n\u22121 ) = n\u22121 i=0 p(X i |\u03a0 i ),(1)\nwhere each node X i is the i th random variable, \u03a0 i is the set of parent nodes of X i in the network (from each node of the set \u03a0 i , there exists an edge directing into X i ), and p(X i |\u03a0 i ) is the conditional probability of X i given its parents \u03a0 i . Each variable X i has a corresponding conditional probability table (CPT) storing its conditional probabilities concerning all possible values of \u03a0 i . Despite being a robust global black box optimizer, BOA has two potential bottlenecks involved in its process, the Bayesian networks construction and the fitness evaluations. In real-world application, the latter one costs a considerable amount of time and resources. Thus, various efficiency enhancement techniques for EDAs have been developed to reduce the number of (fitness) evaluations required for discovering the global optimum [8,9,10]. In this paper, a novel fitness evaluation relaxation strategy for BOA is proposed. To this end, the concept of the entropy measurement of (sub)populations is utilized.\nThe paper is organized as follows. Section 2 briefly describes related works covering various evaluation relaxation approaches for BOA. The concept of the entropy of a certain population and its inherent characteristics are discussed in Sect. 3. Our evaluation relaxation strategy is presented in Sect. 4. Experiments and results are shown in Sect. 5. Section 6 concludes the paper and highlights the future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "In BOA, the conditional dependencies between random variables are encoded in a directed acyclic graph. Taking advantage of such high level of abstraction, various techniques have been proposed to achieve evaluation relaxation for BOA.\nFitness inheritance in BOA [9] builds surrogate fitness models based on Bayesian networks. At each iteration, only a certain proportion of newly generated solutions are determined using the actual evaluation function. Fitness values of the remaining offspring are estimated by the partial contributions of each variable X i = x i with respect to its parents \u03a0 i = \u03c0 i as follows\nf estimation (X 0 , X 1 , ..., X n\u22121 ) =f + n\u22121 i=0 (f (X i |\u03a0 i ) \u2212f (\u03a0 i )),(2)\nwheref denotes the average value of all individuals used to construct the fitness model,f (X i |\u03a0 i ) is the average fitness of solutions having a certain configuration of X i = x i and \u03a0 i = \u03c0 i , andf (\u03a0 i ) is the average fitness of solutions with a particular instance of \u03a0 i = \u03c0 i . While being proved to be a good interpolation for test problems, the construction of the above surrogate fitness model results in a greater populationsizing requirement. Furthermore, the algorithm's performance when enlarging the problem size has not been rigorously tested in the research [9].\nBOA with substructural hillclimbing [10] utilizes the above-mentioned surrogate fitness model to estimate the fitness of a mutated individual at each step of the local search until it reaches a local optimum. While a reduction in the number of actual evaluations is achieved, this hillclimbing also requires larger population sizes. Besides, when the problem size is increased (when l > 80 as reported in [10]), the speed-up of the algorithm slows down.\nIn this paper, we measure the entropy value of the (sub)population at each iteration of BOA. Based on this measurement, a novel efficiency enhancement strategy is proposed to accelerate the BOA in terms of the number of evaluations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Entropy Measurement of Populations", "text": "Based on the dependencies encoded in a Bayesian network, the entropy H(X) of a certain population of BOA is derived as follows:\nH(X 0 , X 1 , ..., X n\u22121 ) = n\u22121 i=0 H(X i |\u03a0 i ) = n\u22121 i=0 \u03c0i\u2208Qi p(\u03c0 i )H(X i |\u03a0 i = \u03c0 i ) = \u2212 n\u22121 i=0 \u03c0i\u2208Qi p(\u03c0 i ) xi\u2208Xi p(x i |\u03c0 i ) log 2 p(x i |\u03c0 i ) = \u2212 n\u22121 i=0 \u03c0i\u2208Qi xi\u2208Xi p(x i , \u03c0 i ) log 2 p(x i |\u03c0 i ) = \u2212 n\u22121 i=0 \u03c0i\u2208Qi xi\u2208Xi m(x i , \u03c0 i ) N log 2 m(x i , \u03c0 i ) m(\u03c0 i ) ,(3)\nwhere Q i is the set of all possible instances of \u03a0 i (parent nodes of X i ), X i denotes the set of all possible values of X i , N is the population size, m(x i , \u03c0 i ) is the number of individuals having (X i , \u03a0 i ) set to (x i , \u03c0 i ), and m(\u03c0 i ) is the number of individuals having \u03a0 i equal to \u03c0 i . Research shows that such an entropy measurement can be used to determine convergence criteria for BOA [11]. Using (3), we can compute the entropy of some particular portions of the population with respect to the current network. Figure 1 shows that the entropy of the better half of the population (parents set) decreases after every iteration. When the BOA converges, this entropy value reaches its minimum value 0. This result is naturally logical as the BOA continuously narrows its sampling towards a certain promising region of the search space. The closer the algorithm moves to a specific convergence point, the more predictable it becomes. Thus, the inherent randomness of the probability distribution in the population gradually decreases.\nWe define an elite set as a set of the most promising individuals selected from the population. In Fig. 1, the top \u03c4 = 5% of the population (in terms of fitness value) are selected as the elite set. This set also exhibits a tendency to decrease its entropy measurement after every iteration. Obviously, this regional entropy is always smaller than the overall entropy of the entire population, and it reaches the minimum value 0 sooner. From such observation, we conjecture that a promising candidate solution is an individual whose appearance in the elite set causes a reduction in the entropy measurement of that set. The next section describes how this conjecture is used in our evaluation relaxation strategy.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Entropy-Based Evaluation Relaxation Strategy for", "text": "Bayesian Optimization Algorithm (eBOA)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Algorithm", "text": "The proposed eBOA is described as follows:\nStep 1: Initialization: Set i \u2190 0, t \u2190 0. Randomly generate the initial population P(0). Evaluate P(0).\nStep 2: Selection: Select a set S(i) of promising solutions from P(i).\nCreate an elite set E(i) of solutions from \u03c4 % of P(i) having the highest actual fitness values.\nStep 3: Model construction: Construct a Bayesian network B(i) fit for S(i).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Based on B(i), compute the entropy H(i) of E(i) by (3).", "text": "Step 4: Create a set of offspring O(i) by sampling B(i).\nStep 5: Perform on-demand evaluation for O(i).\nStep 6: Replace some solutions of P(i) by O(i) to create P(i + 1).\nStep 7: Set i \u2190 i + 1. If the termination criteria are not met, go to Step 2.\nThe on-demand evaluation strategy used in Step 5 is defined as follows. Let t be a counter variable, and \u03ba be the interval of sporadic evaluations.\nCase 1: If H(i) \u2264 H(0) 2\nand t < \u03ba, Consider each newly generated offspring X of O(i),\n1. Put X into E(i) to create E (i). Compute the entropy H (i) for E (i). 2. If H (i) \u2264 H(i), estimate f estimation (X) = f (Y ), Y \u2208 E(i), \u2200Z \u2208 E(i), f(Y ) \u2264 f (Z) . (4\n)\nOtherwise, evaluate f (X).\n3. t \u2190 t + 1. Case 2: If H(i) > H(0) 2 or t = \u03ba, 1. Evaluate all offspring of O(i). 2. t \u2190 0.\nNext, we explain the two main ideas of our entropy-based evaluation relaxation, the on-demand evaluation strategy and the sporadic evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "On-Demand Evaluation Strategy (ODES)", "text": "Evaluating the fitness of a candidate solution is an expensive operation, thus we should only do so when necessary. As stated in Sect. 3, an individual is a promising candidate solution if its appearance achieves a reduction in the entropy value of the elite set. If an individual causes such an entropy reduction, it has the same characteristics (in terms of building blocks) with other candidate solutions in the elite set. Thus, it can be selected without being evaluated by the actual fitness function. Otherwise, the individual does not belong to the elite set, and it should be evaluated to obtain the correct fitness value.\nIf an individual is deemed to belong to the elite set, it should be assigned a high fitness value. We have performed various fitness assignment methods for a selected individual, such as assigning the fitness value of the closest individual (in Hamming distance) or using the median value of the elite set. However, their empirical results (which are not shown here) exhibit no significant difference. In this work, we choose to assign the fitness value of the worst candidate solution in the elite set to that individual. The justification for doing so is to minimize the severity of incorrect estimation errors. For the same reason, the elite set should only be selected from the candidates evaluated by the actual fitness function.\nThe condition H(i) \u2264 H(0) 2 denotes that we wait until the entropy of the elite set decreases by half of its original value before applying ODES. Before this juncture, the randomness (or unpredictability) of the elite set still remains high. If ODES is applied earlier, the estimation power is not strong enough to give good approximations. This would result in slower convergence and more actual evaluations. On the other hand, if we delay the application of ODES until later iterations, the algorithm almost converges and we cannot achieve the maximal reduction in the number of fitness evaluations. Thus, the iteration i when H(i) \u2264 H(0) 2 is a rational choice. Experiments supporting this choice are given in Sect. 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sporadic Evaluation", "text": "The more our on-demand evaluation strategy is continuously used, the more estimation errors are accumulated. Even though our model improves its accuracy in later iterations, the appearances of incorrectly estimated individuals prevent the algorithm from convergence. One simple method to eliminate such accumulated errors is to sporadically perform complete evaluations of entire offspring populations. Instead of evaluating the whole population at each iteration as in the standard BOA, this should be performed only after every some generations have passed.\nMethods with more complicated calculations can be developed to determine appropriate iterations to apply the sporadic evaluation. In this paper, however, we simply choose the criterion that after every \u03ba iterations, the actual fitness function should be used to evaluate all the newly generated offspring. The condition t < \u03ba in the above-mentioned algorithm denotes this criterion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments and Discussion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Test Problems", "text": "In this work, the OneMax function and the Concatenated 5-bit trap function are taken as the test problems. OneMax defines the fitness value of an individual as simply its sum of all the bits:\nf onemax (X 0 , X 1 , ..., X n\u22121 ) = n\u22121 i=0 X i ,(5)\nwhere (X 0 , X 1 , ..., X n\u22121 ) is an input binary string of n bits. The optimal solution for an n-bit OneMax problem is a binary string containing all 1s. Since OneMax is an easy problem for evolutionary algorithms, both BOA and eBOA must be able to work well on this problem. A concatenated 5-bit trap is composed of several trap functions of order 5. Each trap-5 function is defined as follows:\nf trap5 (u) = 5 i f u = 5 4 \u2212 u if u < 5 , (6\n)\nwhere u is the number of bits having value 1 in a trap-5 function. The values of all the traps are added together to form the overall fitness value. An n-bit trap-5 function has one global optimum (a string of all 1s) and (2 n/5 \u2212 1) local optima.\nThe difficulty in optimizing this function is that in each 5-bit trap function, all 5 bits have to be considered together. A robust EDA has to be able to discover the structure of the problem because all statistics of any lower orders lead the algorithm away from the global optimum.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "Both the OneMax and the Concatenated 5-bit trap problems are employed to test the standard BOA and eBOA. The problem sizes are enlarged from 30, to 60, 90, 120, and 150 bits. For each test problem and each problem size above, 30 independent experiments are performed. For each experiment, the bisection method is used to determine the minimal population size for the algorithm to obtain the optimal solution (with 100% correct bits). The convergence criterion is when the proportion of a certain value on each position reaches 99%. The truncation selection with \u03c4 = 50% is used to select the better half of population as the parents set. The offspring replace the worse half of the old population. In all runs of eBOA, we select the elite set with the top \u03c4 = 5% from the old population, and we perform sporadic evaluation after every \u03ba = 5 iterations. Figure 2 and Table 1 compare the performance of BOA and eBOA on the OneMax problem. On average, eBOA requires 85% of the number of fitness evaluations of BOA until convergence. Although in some test cases, the average number of evaluations of both algorithms are not significantly different, eBOA is still proved to be competitive with BOA in solving the OneMax problem.\nFigure 3 and Table 2 compare the performance of standard BOA and eBOA on the trap-5 problem. The results prove that our eBOA achieves a significant reduction in the number of evaluations until convergence. On average, eBOA only needs 76% of the number of fitness evaluations of BOA. Furthermore, eBOA does    not compromise on scalability as the problem size increases. This experiment clearly supports the claim that eBOA outperforms the standard BOA.\nTable 3 shows another interesting result obtained by eBOA in solving the trap-5 problem. While being able to reduce the number of evaluations, eBOA does not introduce any larger population-sizing requirements. Note that statistical tests demonstrate no significant difference between the population sizes of BOA and eBOA.    4 provides experimental results to support our choice of the starting point to apply ODES as stated in Sect. 4.2. We perform experiments for eBOA solving the 30-bit trap-5 problem with different starting points. In terms of entropy reduction, 0% means to start ODES from the beginning of the optimization process, 100% indicates the standard BOA, and 50% comes under eBOA. When the entropy of the elite set decreases as a half of its original value, starting ODES achieves the minimal number of evaluations.", "publication_ref": [], "figure_ref": ["fig_15", "fig_28", "fig_29"], "table_ref": ["tab_2", "tab_5", "tab_20"]}, {"heading": "Conclusion", "text": "In this paper, we have presented eBOA with an entropy-based evaluation relaxation strategy. As we have observed in Sect. 3, the entropy value of the elite set gradually decreases as the algorithm moves towards the convergence point. Taking advantage of this inherent characteristic of the entropy value, we have proposed a recognition method to decide whether an individual needs to be evaluated by the actual fitness function or not. Experimental results have proved that eBOA achieves a significant reduction in the number of fitness evaluations in comparison with the standard BOA. Moreover, it does not impose any larger population-sizing requirements. With a similar population size, the on-demand evaluation strategy can considerably accelerate the optimization process\nIn future work, we will investigate the effects of incorporating other estimation models into eBOA. Besides, we would like to bring the strength of on-demand evaluation strategy to other evolutionary algorithms. Such an entropy-based efficiency enhancement technique can contribute to a new line of research for EDAs in terms of evaluation relaxation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "\"SAT\" problem is the shorthand of Boolean satisfiability problem. It is defined as the task of determining the satisfiability of a given Boolean formula by looking for the variable assignment that makes this formula evaluating to true. The Maximum Satisfiability (Max-SAT) problem is a variant of SAT problem that aims to find the variable assignment maximizing the number of satisfied clauses. In 1971, Stephen Cook [1] had demonstrated that the Max-SAT problem is NP-complete. This complex problem has several applications in different areas such as model checking, graph colouring and task planning to cite just few.\nModern Max-SAT solvers have deeply improved the techniques and algorithms to find optimal solutions. In practice there are two broad classes of algorithms for solving instances of SAT: Complete and Incomplete methods. Complete algorithms are able to verify the satisfiability or unsatisfiablilty of the SAT problem. They usually have an exponential complexity [2]. The most popular algorithms of this class are based on the Davis-Putnam-Loveland algorithm (DPLL) [3]; for example, a branch and bound algorithm based on DPLL is one of the most competitive exact algorithms for Max-SAT [4]. On the other hand, incomplete methods are principally based on local search and evolutionary algorithms. Incomplete methods find good quality solutions in reasonable time. Therefore, they don't guarantee optimality. This class of methods encompasses Evolutionary Algorithms (EA) [5], Stochastic Local Search (SLS) methods [6] and hybrid methods [7].\nEvolutionary computation has been proven to be an effective way to solve complex engineering problems. It presents many interesting features such as adaptation, emergence and learning. Artificial neural networks, genetic algorithms and artificial immune systems are examples of bio-inspired systems used to this end. An Artificial Immune System (AIS) [8] is a type of optimization algorithm inspired from the principles and processes of the vertebrate immune system. In this paper, we propose a new approach to deal with the maximum satisfiability problem. It is a population based method where every individual represents a potential solution to the problem at hand. The features of the proposed method consist in applying different immune system principles like clonal selection and mutation to govern the dynamics of the population in a way to optimize a defined objective function. To foster the convergence to optimality, a local search has been embedded within the optimization process.\nThe remainder of the paper is organized as follows. In section 2, a formulation of the tackled problem is given. Section 3 presents some basic concepts of Artificial Immune Computing. In section 4, the proposed method is described. Experimental results are discussed in section 5. Finally, conclusions and future work are drawn.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Given a Boolean formula F expressed in CNF (Conjunctive Normal Form) and having n Boolean variables x 1 , x 2 ... x n , and m clauses. The k-SAT problem can be formulated as follows:\n\u2022 An assignment to those variables is a vector\nv = (v 1 , v 2 \u2026 v n ) \u2208 {0,1} n \u2022 A clause C i of length k is a disjunction of k literals, C i =( x 1 OR x 2 OR\u2026 OR x k )\n\u2022 Each literal is a variable or a negation of a variable \u2022 Each variable can appear multiple times in the expression.\nFor some constant k, the k-SAT problem requests a variable assignment that makes a formula F = C 1 AND C 2 AND \u2026 AND C m evaluate to true.\nMax-SAT is the problem of finding the assignment that satisfies the highest possible number of clauses. Therefore, it is categorized as an optimization problem. The Max-SAT problem can be defined by specifying implicitly a pair ( )\nSC , \u03a9\n, where \u03a9 is the set of all potentials solution ({0, 1} n ) and SC is a mapping \u039d \u2192 \u03a9\n, called score of the assignment, equal to the number of true clauses. Consequently, the problem consists of defining the best binary assignment that maximizes the number of true clauses in the Boolean formula. Clearly, there are 2 n potential satisfying assignments for this problem, and it has been proven that the k-SAT problem is NP-complete for any k \u2265 3. There are other variances of the Max-SAT problem such as Weighted Max-SAT [9] and Partial Max-SAT [10].\nIn this paper we deal with the Max-3-SAT problem. It is a combinatorial optimization problem. Therefore, it is impossible to obtain exact solutions in polynomial time as the required computation grows exponentially with the size of the problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Artificial Immune Systems", "text": "A Natural Immune System (NIS) is a complex system that can be analyzed at different levels: molecules, cells and organs. Complex interactions between entities within each level enable the resulting immune system to protect the body from any harmful entity, or exogenous agent, called antigen. A specific kind of cells, known as B-cells, is responsible for the destruction of the antigen. The B-cell produces antibodies that binds with the antigens and marks them for destruction. The strength of the antibody/antigen binding is termed antigenic affinity [8].\nOne particular feature of the natural immune system is its ability to construct specific antibodies against new antigens. The clonal selection principle [8] handles this as follows: when a new antigen is detected, the available B-cells start producing antibodies. Those B-cells that best recognize the antigen proliferate by cloning. The clones then undergo hyper-mutation mechanisms to promote their genetic variation. B-cells with high affinity differentiate into plasma cells and memory cells, and B-cells with low affinity are either destroyed or mutated. Plasma cells produce a high number of antibodies against the invading antigen. Memory cells, on the other hand, are long living cells that confers the system a memory of the encountered antigen.\nArtificial immune systems can be viewed as a composition of intelligent methodologies inspired from natural immune systems for solving real world problems [8]. Many models have been inspired from natural immune systems, such as negative selection and danger theory. Those models have been applied to a wide range of applications: multiple sequence alignment [11], network security, optimization and image alignment [12].\nArtificial Clonal selection is an artificial immune system particularly adapted for optimization. Its basic algorithm is as follows [8]:\n1. Generate a set of (P) candidate solutions, composed of the subset of memory cells (Mc) added to the remaining (Pr) population (P = Mc + Pr); 2. Determine (Select) the n best individuals of the population (Pn) based on an affinity measure; 3. Reproduce (Clone) these best individuals of the population, giving rise to a temporary population of clones (C). The clone size is an increasing function of the affinity with the antigen; 4. Submit the population of clones to a hyper-mutation scheme, where the hypermutation is inversely proportional to the antigenic affinity of the antibody. A maturated antibody population is generated (C*); 5. Re-select the improved individuals from C* to compose the memory set Mc.\nSome members of P can be replaced by other improved members of C*; 6. Replace d antibodies by novel ones (diversity introduction). The lower affinity cells have higher probabilities of being replaced.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Proposed Approach", "text": "To solve the Maximum Satisfiability problem, we propose a new algorithm, called ClonSAT, based on the clonal selection principles and enhanced by a local search procedure. ClonSAT starts from an initial random population of B-Cells; each B-cell is an assignment and encodes a potential solution. Thus At each iteration, we begin by assessing the affinity of each B-cell. The affinity value is the number of satisfied clauses in the Boolean formula. Next, only the N best B-cells are allowed to clone themselves. The better the Bcell, the more clones it will produce. The number of clones for each B-cell is calculated as follows:\n\u03b2 \u00d7 = \u2211 j j affinity i affinity i nbClones (1)\nWhere affinity i is the affinity of the i th selected B-cell and \u03b2 is a parameter that indicates the desired clone population's size. Afterwards, the clone population starts a mutation process that transforms the clones to a degree inversely proportional to their affinity. The mutation process is as follows: a. For each clone clone i : compute its normalized affinity:\nAff Aff Aff aff affN i i min max min \u2212 \u2212 = (2)\nminAff and maxAff are the minimal and maximal affinities found in the clone population. b. Calculate the number of mutations to apply to clone i :\nmax ) 1 ( min \u00d7 \u2212 + \u00d7 = i affN i affN i s nbMutation (3)\nThe min and max parameters indicate how many mutations to apply to the worst and best individual respectively. c. For each mutation a random bit is flipped.\nAfter the mutation step, we evaluate the affinity of the mature population and select the best B-cell as a candidate cell. If it is better than the memory cell, the candidate cell becomes the new memory cell. Finally, we replace all the B-cells of the current population with the best ones from both the current population and the mature population and we replace the worst elements of the current population with new randomly generated ones. The whole process is repeated until the stopping criterion is met.\nIn order to increase intensification capabilities of search, we apply a local search algorithm when the memory cell has not been improved for more than a given number of generations; for this purpose we propose the use of the well known WalkSat algorithm [13] as follows:\nStart with the actual memory cell. Repeat (predefined number of flips) -Pick a random unsatisfied clause.\n-Select and flip a variable from that clause: i. With probability p, pick a random variable ii. With probability 1-p, pick greedily a variable that minimizes the number of unsatisfied clauses", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "ClonSat has been implemented using Java language. To asses the experimental performance of ClonSat, we have performed several tests taken from the AIM benchmark instances. The AIM instances are all generated with a particular Random-3-SAT instance generator [14]. In all experiments, we have run the ClonSat program using the parameters' settings shown in Table 1. Furthermore, we have compared ClonSAT with the GSAT [6] and QSAT [15] algorithms. QSAT is a quantum evolutionary algorithm with a simple flipping procedure. Freidman tests were carried out to test the significance of the difference in the accuracy of each method in this experiment. The results, reported in table 2, are encouraging and prove the feasibility of using an artificial immune system to deal with the Max 3-Sat problem. In most cases, ClonSAT performs as well as GSAT and even better than QSAT, as it's shown by the Freidman test (figure 1). This is interesting because, apart from the affinity function, the algorithm that we used is a standard artificial immune system. The addition of walkSAT allows the program to perform even better with the most difficult cases.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2"]}, {"heading": "Conclusion", "text": "In this paper we proposed a new approach, called ClonSAT, to solve the Max-3-SAT problem. ClonSAT is based on hybridizing an Artificial Immune System with a local search algorithm. Although more experiments and comparisons are required, the results so far are promising and demonstrate the feasibility of AISs to deal with the Max-SAT problem; in most cases, our program gives comparable or better solutions than GSAT and QSAT programs. In addition, the proposed framework provides an extensible platform for evaluating different variant of satisfiability problem. Our future work consists on investigating the effect of different local search methods on the performance of our approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "As the integration rate of semiconductors increases, more complex system-on-chips (SoCs) are launched. A simple SoC is formed by homogeneous or heterogeneous independent components while a complex SoC is formed by interconnected heterogeneous components. The interconnection and communication of these components by a communication architecture form a network-on-chip (NoC). A NoC is similar to a general network but with limited resources such as bandwidth, area and power. Each component of a NoC is designed as an intellectual property (IP) block. Normally, a NoC is designed to run a specific application. This application, usually, consists of a limited number of tasks that are implemented by a set of IP blocks. An IP block can be assigned for more than a single task of the application or it can be dedicated to execute a single task. For instance, a processor IP block can execute different tasks as a general processor does but the NoC designer, due performance, can assign just one task for that specific processor. In the other hand, a multiplier IP block for floating point numbers can only multiply floating point numbers and the NoC designer can reuse that IP if the application has more than one task of floating point multiplication. The number of IP blocks designers, as well as the number of available IP blocks, is growing up fast.\nA NoC consists of sets of resources and switches. Resources and switches are connected by resource network interfaces. Switches are connected by communication channels. The pair of switch/resource forms a tile. The simplest way to connect the available resources and switches is arranging them as a mesh so these are able to communicate with each other by sending messages via an available communication path. A switch is able to buffer and route messages between resources. On a mesh-based NoC each switch is connected to up to four other neighboring switches through input and output channels. While a switch is sending data through a channel, it can can buffer incoming data through another channel.\nUsually, an application is described as a graph of tasks called task graph (TG), which is a high level description. The IP blocks features can be obtained from their manufacturer documentation. The IP assignment and IP mapping are key research problems for efficient NoC-based designs [7]. Electronic Design Automation (EDA) tools must deal with these two problems. On a low level description, every IP mapping must be synthesized, leading to a very slow but precise evaluation. On a high level description, evaluation is first driven by models of the NoC-based platform, leading to a fast evaluation and the precision depends of the modeling. Along the design process the description abstraction level is decreased until reach an RTL description.\nIP assignment and IP mapping are combinatorial optimization problems classified as NP-hard problems [5]. We use multi-objective evolutionary algorithms (MOEAs) with specific operators and objective functions to yield an optimal IP assignment and IP mapping. As normally multi-objective problems present a set of solutions, we consider the preferences of a decision maker (DM) to find a single solution or at most a small subset of solutions. In this paper, we propose a power-aware multi-objective evolutionary decision support system to help NoC designers on a high level stage of a platformbased NoC design. For this purpose, we use two MOEAs: NSGA-II [2] and microGA [1]. Both of these algorithms were modified according to some prescribed NoC design constraints and to accept preferences defined by the DM.\nThe rest of the paper is organized as follows: In Section 2, we introduce the IP assignment and IP mapping problems in platform-based designs. Then, in Section 3, we describe a structured TG and IP repository model based on the E3S data. After that, in Section 4, we sketch the multi-objective evolutionary approach and present the objective functions. Later, in Section 5, we show some experimental result. Last but not least, in Section 6, we draw some conclusions and outline some future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IP Assignment and Mapping Problems", "text": "The platform-based design methodology for SoC encourages the reuse of components to reduce costs and to reduce the time-to-market of new designs. The designer of NoCbased systems faces two main problems: selecting the adequate set of IPs and finding the best physical mapping of these IPs into the NoC structure. On a platform-based design, the selection of IPs is called IP assignment stage and the physical mapping is called IP mapping stage.\nThe main objective of the IP assignment stage is to select, from a IP repository, a set of IPs that exploit re-usability and optimize the execution of a given application. At this stage, no information about physical location of IPs is available so optimization must be done based on application's description (as a TG) and IP features only. So, the result of this stage is the set of IPs that maximizes NoC performance due IPs features. The TG is then annotated and an application characterization graph (ACG) is produced, wherein each node (task) has an IP assigned to it. The TG and ACG are defined in Section 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The number of possible assignments is defined by", "text": "A = n 0 \u00d7 n 1 \u00d7 . . . n m\u22122 \u00d7 n m\u22121 ,\nwherein m represents the number of tasks in the application, t 0 , t 1 , . . . , t m\u22121 and n i is the number of IPs that can be assigned to task t i .\nGiven an application, described by its ACG, the problem that we are concerned with now is to determine how to topologically map the selected IPs onto the network platform, such that the objectives of interest are optimized. At this stage, a more accurate evaluation can be done taking into account the distance between resources and the number of switches and channels crossed by a data package along a path. The result of this process should be an optimal allocation of one of the prescribed IP assignments, to execute a desired application on a NoC platform.\nThe mapping stage uses the result obtained from the assignment, which consists of many non-dominated solutions. Let s be the number of distinct assignments evolved and p i be the number of processors used in assignment i and n i be the minimal number of resources in the NoC to be utilized in the implementation of the application with assignment solution i. In this case, the total number of possible mappings is defined as in (1).\nM s = s i=1 n i ! (n i \u2212 p i )!(1)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Graph and IP Repository Models", "text": "In order to formulate the IP mapping problem, it is necessary to introduce a formal definition of an application description first. An application can be described as a set of tasks that can be executed sequentially or in parallel. It can be represented by a directed acyclic graph of tasks, called task graph. A Task Graph (TG) G = G(T, D) is a directed acyclic graph where each node represents a computational module in the application referred to as task t i \u2208 T . Each directed arc d i,j \u2208 D, between tasks t i and t j , characterizes either data or control dependencies. Each task t i is annotated with relevant information, such as a unique identifier and type of task in the network. Each d i,j is associated with a value V (d i,j ), which represents the volume of bits exchanged during the communication between tasks t i and t j . Once the IP assignment has been completed, each task is associated with an IP identifier. The result of the assignment is a graph of IPs representing the processor elements (PEs) responsible for executing the application. This graph is called application characterization graph. An ACG G = G(C, A) is a directed graph, where each vertex c i \u2208 C represents a IP assigned to one or more tasks, forming a core, and each directed arc a i,j characterizes the communication process from core c i to core c j . Each a i,j can be tagged with IP/application specific information, such as communication rate, communication bandwidth or volume of bits exchanged between cores c i and c j . A TG is based on application features only while an ACG is based on application and IP features, providing us with a much more realistic representation of an application in runtime on a NoC platform. The abstraction level decreases from the TG to the ACG representation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Power-Aware Multi-objective Evolutionary Approach", "text": "The search space of the IP assignment and mapping problems, for a given application, may exceed millions or even billions of possible combinations. Among the huge number of possible solutions, it is possible to find many equally optimal solutions, called nondominated solutions [1]. In huge non-continuous search space, deterministic approaches do not deal very well with MOPs. In order to deal with such a big search space in a reasonable time, a power-aware multi-objective evolutionary decision support system to aid platform-based NoC design is proposed.\nInstead of obtain a single solution after IP assignment and IP mapping like in a typical platform-based NoC design, the proposed system exploits different solutions of assignment and mapping to equalizes the trade-off among the objectives of interest and introduces the DM's preferences to refine final result.\nThe kernel of the proposed aid system is driven by two well-known MOEAs: NSGA-II [2] and microGA [1]. Both adopt the domination concept with a ranking method of classification. In order to deal with the IP assignment and IP mapping problems both algorithms, i.e NSGA-II and microGA, were adapted to recognize two individuals representations: a assignment representation and a mapping representation. Originally, those algorithms do not consider the preferences of a DM and a major modifications were introduced to turn them into power-aware multi-objective genetic algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Representation", "text": "The chromosome is formed by a set of genes and each one represents a node id from the TG. Each gene g has a IP id field that corresponds to a IP from the repository capable to execute the associated task type. If a IP is dedicated to execute a single task of th TG, the dedi field value is 1, otherwise it is 0. Initially, a random IP id is assigned to each gene, with the constraint of the IP type. Tournament selection, one-point crossover and simple mutation were used. The crossover operator, without any constraint, can only produce feasible individuals because the order of genes is not changed. The mutation is controlled by IP type constraint to avoid selecting a random IP, from IP repository, of different type.\nThe mapping individual representation is inherited from the assignment individual representation. It is augmented with the RES id field, which indicates the resource on which a gene is mapped on the NoC platform, so representing a physical information. On a N \u00d7 N regular mesh, assume that the tiles are numbered successively from topleft to bottom-right, row by row. The row of the i th tile is given by i/N , and the corresponding column by i mod N . Note that the first resource id as the first row and the first column are numbered 0.\nThree objectives of interest where identified for the platform-based NoC design optimization. In this paper the DM's preferences will constrain the power consumption of the NoC while the other two objectives, area occupied and time of execution, will be ranked as usually done.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Assignment Evaluation", "text": "The fitness of an assignment solution S is measured in terms of the silicon area that would be used to implement the NoC-based application using S, the approximate execution time of the so implemented application and the power consumption required when the application is executed. Note that in this case, only computation time and power due to computation are considered. Those introduced by the communication can only be considered when the actual location where the IPs are mapped within the NoC resource nodes are known, i.e. in the mapping stage. In the following, we explain in details how each of these characteristics is quantified.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Area.", "text": "In order to compute the area required it is necessary to add up the area of each processor used in a given solution S. The identifier of each processor (procID) is retrieved visiting each gene of S. Grouping the nodes of same processor and identifying the nodes of dedicates processors, is a method to identify the processors of solution S. Equation 2 shows how to compute the area of solution S, wherein function PE(S) provides the set of non-dedicated processors used in S. The notation S[t] ip indicates the IP assigned to task t in S and S[t] dedi the value of field dedi for task t in S.\nArea(S) = t\u2208T G area S[t]ip * S[t] dedi + p\u2208PE(S) area p (2)\nExecution time. In order to compute the execution time required by a solution S, it is necessary to find the critical path of the ACG. The critical path can be found visiting all nodes of all paths and recording the execution time of the slowest path. When tasks that should be executed in parallel are allocated in the same processor, these tasks must be scheduled sequentially. If at least one of this tasks is from the critical path, the execution time will be increased. Assume that the scheduling order is dictated by the increasing order of the task identifier. In this context, consider the case where t 1 , t 2 , . . . , t k , are k tasks that can be implemented in parallel, but are allocated to the same processor. The execution time associated with a path that goes through a task t i is increased by the sum of execution times of all tasks that are scheduled before t i . These tasks are those whose identifier is smaller than the identifier of the task t i . Equation 3 shows the details of this computation. In this context, the function C(g) returns all possible paths of the task graph g, function P(t) returns the set of all tasks in the ACG that may be executed in parallel with task t and are associated with the same processor in the solution S, function D(t) informs all the tasks that depends on the execution of t and that are also allocated to the same processor in S. Note that the attribute level of the nodes of a task graph can be used to determine the members of the set returned by function P(t).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T ime(S) = max", "text": "M\u2208C(GT ) t\u2208M time S[t]ip + T (S) T (S) = \u23a7 \u23aa \u23a8 \u23aa \u23a9 0 if S[t] dedi = 1or P (t) = D (t) = \u2205 t \u2208 P (t) \u222a D (t) t < t time S[t ]ip otherwise(3)\nPower consumption. To evaluate the power consumption of a application represented by a TG, the power consumption of each IP assigned must be added. In (4), power S[t]ip represents the power consumption when a task t is executed by its assigned IP in a solution S, and \u03be a and \u03be a are the power constraints imposed for the assignment.\n\u03be a \u2264 P ower(S) = t\u2208GT power S[t]ip \u2264 \u03be a (4)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mapping Evaluation", "text": "The fitness of mapping solution S is measured in terms of the silicon area that would be used to implement the NoC-based application using S, the execution time of the so implemented application and the power consumption required when the application is executed. In the following, we explain in details how each of these characteristics is quantified.\nArea. To compute the area required by a given mapping it is necessary to know the area needed for the selected processors and that occupied by the used channels and switches. As a processor can be responsible for more than one task, each ACG node must be visited in order to check the processor identifier for each node. It is necessary to identify those cases where a processor is dedicated for a task before grouping the nodes with same procID attribute. Nodes with same procID marked as non-dedicated are executed by the same processor. Nodes marked as dedicated are executed by a dedicated processor. The total number of channels and switches can be obtained through the consideration of all communication paths between exploited tiles. Note that a given IP mapping may not use all the available tiles, links and switches that are available in the NoC structure. Also, observe that a portion of a path may be re-used in several communication paths.\nIn this paper, we adopted XY deterministic route strategy [4]. The data emanating from tile i to j is sent first horizontally to the left or right side of the corresponding switch until it reaches the column of tile j, then, it is sent up or down, also depending on the position of tile j with respect to tile i until it reaches the row of tile j. The number of channels in the aforementioned route can be computed by the function CH(i, j) as described in (5). This also called the Manhattan distance between tiles i and j.\nCH(i, j) = | i/N \u2212 j/N | + | i\\N \u2212 j\\N | (5)\nThe number of hops between tiles along a given path leads to the number of channels between those tiles, and incrementing that number by 1 yields the number of traversed switches, as shown in (6). The total area required is computed summing up the areas required by the implementation of all distinct processors, switches and channels. The area required by switches and channels depends of the NoC platform. A general decision support tool must allow the designer to configure these parameters for different platforms.\nSW(i, j) = CH(i, j) + 1(6)\nEquation 7 describes the computation involved to obtain the total area of a given mapping solution S. For a given allocation, function Area A (.) gives the area of the allocation exactly like (2) does. The allocation that originated mapping S is given by A S . Function E(g) returns all the edges of the task graph g, while attributes src and tgt returns the source and target tasks, respectively. \nExecution time. To compute the execution time of a given mapping, we consider the execution time of each task of the critical path, their schedule and the additional time due to data transportation through channels and switches along the communication path.\nThe execution time of each task is defined by the taskTime attribute in TG. Channels and switches can be counted using ( 5) and ( 6), respectively. Analyzing the assignment problem we identified a situation that increases the execution time of an application, which occurs when parallel tasks are allocated to the same processor. The mapping problem analysis revealed other two situations that can increase the execution time of the application: (i) Parallel tasks with common source sharing communication channels and (ii) Parallel tasks with common target sharing communication channels. Equation 8gives the execution time considering computation and communication for a given mapping solution S. Function C(g) returns all possibles paths Q for a given task graph g, while T ime p (Q) returns the time necessary to processes the tasks of a path Q and T ime c (Q) the time spent due communication among tasks in path Q, assuming there is no contention. Considering contention, its necessary to add the delay concerning the two aforementioned situations. Delay caused by situation (i) is computed by function f 1 and delay caused by situation (ii) is computed by function f 2 in (8).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T ime(S) = max", "text": "Q\u2208C(GT ) (T ime p (Q) + T ime c (Q) + T (Q)) T (Q) = t L \u00d7 (f 1 (Q) + f 2 (Q))](8)\nThe time spent due computation for a path Q of the task graph is computed as shown in (9). Function P(t) returns all the tasks at the same level of task t and associated with the same processor, for a given mapping solution S. Function D(t) returns all the task dependents of t and executed by the same processor, while A S [t] ip returns information about the IP assigned to a task t in S.\nT ime p (Q) = t\u2208Q time AS [t]ip + T (S) T (S) = \u23a7 \u23aa \u23a8 \u23aa \u23a9 0 if A S [t] dedi = 1 or P (t) = D (t) = \u2205 t \u2208 P (t) \u222a D (t) t < t time AS[t ]ip otherwise(9)\nThe time spent due communication among tasks along a path Q is computed as shown in (10). Function E(T G) returns all the edges d(src, tgt) of the task graph, vol d(t,t ) is the volume of bits transmitted from task t to task t , t R is the switch processing time and t L is the channel transmission time.\nT ime c (Q) = d(t, t ) \u2208 E(GT )| t \u2208 Q, t \u2208 Q vol d(t,t ) phit t L SW(S[d t ] rec , S[d t ] rec )(t R + t L ) (10)\nFunction f 1 computes delays on path Q regarding situation (i). Algorithm 1 describes this function. For each parallel task that must be achieved through the same communication channel, the overall execution time is increased due to data pipelining. Function T argets(t) returns all the tasks of the task graph that depends of task t, iCH(t, t ) the initial communication channel index of task t to task t and penalty is the number of flits that would be transmitted when situation (i) occurs. A flit represents the flow unit, multiple of the phit that represents the physical unit given by the channel width.\nFunction f 2 computes delays on path Q regarding to situation (ii). If a switch receives packages from two different channels at the same time and needs to route them through the same output channel, there will be a package pipelining. Algorithm 2 computes how many times this situation occurs. Function CHs(t, t ) returns an ordered list of the required channels during communication of tasks t and t and penalty is the number of flits that would be transmitted when situation (ii) occurs.\nPower consumption. To compute the power consumption of a mapping, it is necessary to consider the power consumed due processing and communication. The total power consumed is given as in (11), wherein P ower p and P ower c represent processing and communication consumption, respectively and \u03be m and \u03be m the power constraints.  The power consumption due processing is given summarizing the power consumption of each executed task of a mapping solution S. In (12), P ower (t,p) represents the power consumed when a task t is executed by a processor p.\nP ower p (S) = t\u2208GT power S[t]ip (12) The power consumed due communication is a important feature to be considered on a NoC power model in order to get a accurate evaluation. This feature depends on the application communication pattern and the NoC platform. The communication pattern is given by the assignment and mapping, while the NoC platform is defined by the network topology, switching strategy and routing algorithm. The power consumed by sending one bit from tile i to tile j is computed as shown in (13). Parameters E S bit and E C bit represents power consumed by switches and channels, respectively. These parameter are platform dependent and must be setted by the NoC designer.\nE i,j bit = SW \u00d7 E S bit + CH \u00d7 E C bit (13\n)\nThe task graph gives the volume of bits from task t to t through oriented edge d t,t .\nAssuming that tasks t and t are mapped on tiles i and j respectively, the amount of bits transmitted from tile i to j is denoted as vol d (t,t ) . Communication between tiles i and j can be established with a unique channel C i,j or with a sequence of m > 1 channels \n[c i,x0 , c x0,x1 , c x1,x2 , . . . , c", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "The E3S (0.9) Benchmark Suite [3] was used to carry on the simulations. The suite contains the characteristics of 17 embedded processors. These processors are characterized by the measured execution times of 46 different types of tasks, power consumption derived from processor datasheets, die size required, price and clock frequency. In addition, E3S contains common applications executed by embedded systems in environments. We show the results obtained for 7 different applications, as described in the first 4 columns of Table 1, wherein N is the number of tasks and M is the number of data dependencies of the application. The minimal and average values of power consumption obtained were used in this paper to set the preferred minimal and maximal bounds, respectively, of power consumption for each application. However, any other value of power consumption can be set by the DM as suited. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions", "text": "The problem of assigning and mapping IPs are NP-hard problems and key research problems in NoC design field. In this paper we propose a innovative power-aware multi-objective evolutionary decision support system to aid NoC designers assigning and mapping a prescribed set of IPs into a NoC physical structure. A power-aware optimization was performed and the performance of the NSGA-II and microGA compared. The latter performed better for all aplications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Resilient Packet Ring (RPR), also known as IEEE 802.17, is a standard designed for optimising the transport of data traffic over optical fibre ring networks [1][2][3]. The load balancing model for RPR differs from the Synchronous Optical Network/ Synchronous Digital Hierarchy (SONET/SDH) ring loading. Namely, in SONET/SDH rings the demands assigned to go clockwise compete for common span capacity with the demands assigned to go counter-clockwise. In RPR two distinct rings occur and the demands do not compete for the common capacity. The Weighted Ring Arc Loading Problem (WRALP) arises in the engineering and planning of the RPR systems, while the Weighted Ring Edge Loading Problem (WRELP) arises in the SONET/SDH rings. The load of an arc is defined to be the total weight of those requests that are routed through the Arc in its direction (WRALP) and the load of an edge is the number of routes traversing the Edge in either direction (WRELP). WRALP/WRELP ask for such a routing scheme that the maximum load on arcs/edges will be minimum.\nThe load balancing problems can be classified into two formulations: with demand splitting (split) or without demand splitting (non-split). The split loading allows the splitting of a demand into two portions to be carried out in both directions, while in a non-split loading each demand must be entirely carried out in either clockwise or counter-clockwise direction. In this paper we study the non-split WRALP.\nResearchs on the no-split WRELP performed by Cosares and Saniee [4] and Dell'Amico et al. [5] studied the problem on SONET rings. Cosares and Saniee [4] proved that the formulation without demand splitting is a NP-complete problem. Recent studies on the non-split WRELP use Evolutionary Algorithms (EAs) [6][7]. For the split WRELP, Schrijver et al. [8] summarise various approaches and their algorithms are compared in Myung and Kim [9] and Wang [10].\nThe WRALP considered in the present paper is identical to the one described by Kubat and Smith [11] (non-split), Cho et al. [12] (split and non-split) and Yuan and Zhou [13] (split). They try to find approximate solutions in a reduced amount of time. Our purpose is different, we want to compare the performance of our algorithm with others in the achievement of the best-known solution. Using the same principle Bernardino et al. [14] proposed four hybrid Particle Swarm Optimisation (PSO) algorithms to solve the non-plit WRALP.\nIn this paper we propose a Discrete Differential Evolution (DDE) algorithm to solve the non-split WRALP. Our algorithm is based on the DDE algorithm proposed by Pan et al. [15] for solving the permutation flowshop scheduling problem. The DDE algorithm first mutates a target population to produce the mutant population. Then the target population is recombined with the mutant population in order to generate a trial population. Finally, a selection operator is applied to both target and trial populations to determine who will survive for the next generation [15]. We use a Local Search (LS) embedded in the DDE algorithm to improve the solution quality.\nWe compare the performance of DDE with four algorithms: Genetic Algorithm (GA), Differential Evolution (DE), Tabu Search (TS) and LS-Probability Binary PSO (LS-PBPSO), used in literature.\nThe paper is structured as follows. In Section 2 we describe the WRALP; in Section 3 we present the DDE algorithm; in Section 4 we discuss the computational results obtained and, finally, in Section 5 we report about the conclusions. . A communication request on R n is an ordered pair (s,t) of distinct nodes, where s is the source and t is the destination. We assume that data can be transmitted clockwise or counter-clockwise on the ring, without splitting. We use P + (s,t) to indicate the directed (s,t) path clockwise around R n, and P -(s,t) the directed (s,t) path counter-clockwise around R n .\nA request (s,t) is often associated with an integer weight w>=0; we denote this weighted request by (s,t; w). Let D={(s 1 ,t 1 ; w 1 ),(s 2 ,t 2 ; w 2 ),..., (s m ,t m ; w m )} be a set of integrally weighted requests on R n . For each request/pair (s i ,t i ) we need to design a directed path P i of R n from s i to t i . A set P={P i : i=1, 2, ..., m} of such directed paths is called a routing for D.\nIn this work, the solutions are represented using binary vectors (Table 1). For some integer V i =1, 1\u2264 i \u2264 m, the total amount of data is transmitted along P + (s i ,t i ); V i =0, the total amount of data is transmitted along P -(s i ,t i ). The vector V=(V 1 , V 2 , \u2026, V m ) determines a routing scheme for D.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "The Proposed Discrete Differential Evolution Algorithm", "text": "The DE was introduced by Storn and Price in 1995 [16]. The DE algorithm is a population-based algorithm using crossover, mutation and selection operators [17]. It resembles the structure of an EA, but differs in the generation of new candidate solutions and by using a 'greedy' selection scheme. The DE algorithm uses the mutation operation as a search mechanism and the selection operation to direct the search toward the prospective regions in the search space. The algorithm also uses a nonuniform crossover. By using the components of the existing population members to build trial vectors, the crossover operator efficiently shuffles information about successful combinations, enabling the search for a better solution space [17].\nThe application of the DE on combinatorial optimisation problems with binary decision variables is not usual. One of the possible reasons is the encoding scheme. Most of the discrete problems have solutions obtainable through permutation vectors, while the DE maintains and evolves floating-point vectors. In order to apply the DE to solve binary problems, the most important is to find a suitable encoding scheme, which can perform a transformation between floating-point and permutation vectors.\nBernardino et al. [7] proposed a DE (HDE) algorithm to solve the WRELP. The HDE algorithm applies a separate LS process to improve individuals. It combines global and local search by using an EA to perform exploration, while the LS method performs exploitation. The HDE algorithm uses the binary representation. After applying the standard equations, the algorithm verifies if the trial solutions contain values outside the allowed range. If a gene (pair) is outside of the allowed range, it is necessary to apply the following transformation:\nIF pair >= 0.5 pair = 1 ELSE IF pair <0.5 pair = 0\nThis transformation significantly increases the algorithm execution time. To solve this problem we use a DDE algorithm [15]. The solutions are based on discrete values and can be applied to all types of combinatorial optimisation problems [18].\nThe main steps of the DDE algorithm are given below: ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Initialization of Parameters", "text": "The following parameters must be defined by the user: (1) ni -number of individuals;\n(2) mi -maximum number of iterations; (3) pp -perturbation probability; (4) np -number of perturbations and ( 5) pc -crossover probability.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Target Population", "text": "The initial solutions can be created randomly or in a deterministic form. The deterministic form is based in a Shortest-Path Algorithm (SPA). The SPA is a simple traffic demand assignment rule in which the demand will traverse the smallest number of segments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of Solutions", "text": "To evaluate how good a potential solution is in relation to other potential solutions, we use the following fitness function:\nWi,\u2026,wm demands of the pairs (si,ti),\u2026,(sm,tm)\nVi, \u2026, Vm = 0 P -(si,ti); 1 P + (si,ti) (\nLoad on arcs:\nL(V, + k a )= \u2211 + + \u2208 ) t , (s P a : i i i k wi L(V, \u2212 k a )= \u2211 \u2212 \u2212 \u2208 ) t , (s P a : i i i k wi (2a) \u2200k=1,\u2026,n; \u2200i=1,\u2026,m(2b)\nFitness function:\nmax{max L(V, + k a ),max L(V, \u2212 k a )} (3)\nThe fitness function is responsible for performing the evaluation and returning a positive number (fitness value) that reflects how optimal the solution is. It is based on the following constraints: (1) between each node pair (s i ,t i ) there is a demand value >=0. Each positive demand value is routed in either clockwise (C) or counterclockwise (CC) direction; (2) for an arc the load is the sum of w k for clockwise or counter-clockwise direction between nodes e k and e k+1 . The objective is to minimise the maximum load on the arcs of a ring (3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mutant Population", "text": "A mutant individual is obtained by perturbing the best solution of previous generation in the target population. To obtain the mutant individual, the following equation is used:\n{ ) ( ) ( ) ( 1 1 pp r if P DC P ntrator osestConce MutationCl t i t g np t g Pm < \u2212 \u2212 = . 1 \u2212 t g P\nis the best solution from the previous generation in the target population and DC np is the destruction and construction procedure with the destruction size of np as a perturbation operator; and MutationDirection is a simple mutation operator. With this operator, one gene (pair) is randomly selected and its direction is exchanged. A uniform random number r is generated between 0 and 1. If r is less than pp then the perturbation operator (DC) is applied to generate the mutant individual:\n) ( 1 \u2212 = t g np t i P DC Pm\n. Otherwise, the best solution from the previous population is perturbed using a simple mutation operator:\n) ( 1 \u2212 = t g t i P rection MutationDi Pm .\nThe general mechanism of the DC procedure is represented bellow:\nFOR i=1 TO np DO i1= random (m/2) i2= random (m-m/2) for c=i1 to c=i2+i1 if random(2)=0 if (sc-tc) = n/2 if random(2)=1 testSolution[c] = CC else testSolution[c] = C else if (sc-tc) > n/2 testSolution[c] = C else testSolution[c] = CC else testSolution[c]= bestSolution[c]; if fitnessTest < fitnessOld break; if fitnessTest < fitnessNew newSolution=testSolution\nA new solution is obtained by performing multiple perturbations (np). Some of the pairs of the solution are selected and in 50% of the cases their direction is changed to the shortest path; otherwise it changes its direction to the direction of the best solution of the population. The algorithm repeats this process until the np is reached.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Trial Population", "text": "Following the perturbation phase, a trial individual is obtained using the following expression:\n{ ) ( ) , ( 1 pc r if P Pm CR Pm t i t i t i t i Pt < \u2212 = .\nA uniform random number r is generated between 0 and 1. If r is lower than pc then the crossover operator is applied to generate the trial individual:\n) , ( 1 \u2212 = t i t m t i P P CR Pt\n. The crossover operator CR adopted was \"Uniform\" [6]. The crossover operator produces two children. In this study, we selected one of the children randomly. If r is higher or equal to pc then the trial individual is chosen as:\n1 \u2212 = t i t i P Pt\n. The trial individual is made up either from the perturbation operator or from the crossover operator.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Selection", "text": "The selection is based on the survival of the fitness among the trial and target individuals using the following expression: .\n\u23aa \u23a9 \u23aa \u23a8 \u23a7 = \u2212 < \u2212 )) 1 ( ) ( (", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Local Search", "text": "The LS algorithm applies a partial neighbourhood examination. We create two different LS methods that can be chosen by the user. In the LS \"Exchange Direction\" (LS-ED) some pairs of the solution are selected and their directions are exchanged (partial search). This method can be summarised in the following pseudo-code steps:\nFor t=0 to numberNodesRing/4 P1 = random (number of pairs) P2 = random (number of pairs) N = neighborhoods of ACTUAL-SOLUTION (one neighborhood results of interchange the direction of P1 and/or P2) SOLUTION = FindBest (N) If ACTUAL-SOLUTION is worst than SOLUTION ACTUAL-SOLUTION = SOLUTION\nIn the LS \"Exchange Max Arc\" (LS-EMA), first it is necessary to establish the arc with the highest fitness. A set of neighbours is obtained by interchanging the direction of some pairs that flow by the arc with the highest fitness (partial search). This method can be summarised in the following pseudo-code steps: \nDefine", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Termination Criterion", "text": "The algorithm stops when a maximum number of iterations (mi) is reached. Further information on DDE can be found in [19].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We evaluate the utility of the algorithms using the same examples produced by Bernardino et al. [14]. They consider six different ring sizes: 5, 10, 15, 20, 25 and 30 and four demand cases: (1) complete set of demands between 5 and 100 with uniform distribution;\n(2) half of the demands in (1) set to zero; (3) 75% of the demands in (1) set to zero and (4) complete set of demand between 1 and 500 with uniform distribution. The last case was only used for the 30 nodes ring. For convenience, the instances used are labeled C ij , where 1<i<6 represents the ring size and 1<j<4 represents the demand case.\nWe perform comparisons between all parameters of the DDE using the instance C41 with 50 iterations and creating the initial solutions randomly, to obtain the best combination of parameters.\nThe best results obtained with DDE use np between 5 and 10, pp>0.6 and pc<0.3 (Fig. 1) and ni=[50,100]. These parameters were experimentally found to be good and robust for the problems tested.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Fig. 1. Influence of parameters", "text": "We studied the influence of the two different LS methods developed on the execution time, the average fitness and the number of best solutions found. The LS-ED obtains a better average fitness, however the LS-EMA is less time consuming. We verify that at the same time, no matter the number of iterations, the two LS methods produce an identical number of best solutions (Fig. 2 and Fig. 3).\nIn our experiments we use different population sizes. The number of individuals was set to {10,20,30,\u2026,160}. We studied the impact on the execution time, the average fitness and the number of best solutions found (Fig. 2). The best values are between 50 and 100. With these values the algorithm can reach a reasonable number of good solutions in a reasonable amount of time (Fig. 2c). With a higher number of initial solutions, the algorithm can reach a better average fitness, but it is more time consuming (Fig. 2b). For parameter np, the number of perturbations, np between 5 and 10 has been shown experimentally to be more efficient (Fig. 3). In our experiments np was set to {1,2,3,\u2026,30}. A small np did not allow the system to escape from local minima, because the resulting solution was in most cases the same as the starting permutation. A high np has a significant impact on the execution time (Fig. 3b).\nIn general, experiments have shown that the proposed parameter setting for the DDE is very robust to small modifications.\nTo compare our results we consider the results produced with the GA proposed by Bernardino et al. [6], the DE and TS algorithms proposed by Bernardino et al. [7], and the LS-PBPSO proposed by Bernardino et al. [14]. The suggestions from literature helped to guide our choice of parameter values for the GA [6], DE and TS algorithms [7] and the LS-PBPSO algorithm [14]. The GA was applied to populations of 200 individuals, it uses the \"Uniform\" method for recombination, the \"Change Direction\" method for mutation and the \"Tournament with Elitism\" for selection. For the GA, we consider crossover probability in the range [0.6,0.9] and a mutation probability in the range [0.5,0.7]. The DE was applied to populations of 50 individuals, it uses the strategy \"Best1Bin\", CR in the range [0.3,0.5] and factor F in the range [0.5,0.7]. The TS considers a number of elements in the tabu list between 4 and 10. The LS-PBPSO was applied to populations of 40 particles and we consider the value 1.49 for the parameters C1 and C2, and for the inertia velocity (W) values in the range [0.6,0.8]. For the DDE we consider populations of 50 individuals, 5 perturbations, pc in the range [0.1,0.2], pp in the range [0.6,0.8] and the LS method \"Exchange Direction\".\nThe algorithms have been executed using a processor Intel Quad Core Q9450 and the initial solutions of all algorithms were created using random solutions. For the problem C64 we used the SPA for creating the initial populations.\nTable 2 presents the best obtained results by Bernardino et al. [14]. The first column represents the number of instance (Instance), the second and the third columns show the number of nodes (Nodes) and the number of pairs (Pairs) and finally the fourth column shows the minimum fitness values obtained. Table 3 presents the best results obtained with GA, HDE, TS, LS-PBPSO and DDE. The first column represents the number of the problem (Prob), the second column demonstrates the number of iterations used to test each instance and the remaining columns show the results obtained (Time -Run Times, IT -Iterations) by the five algorithms. The results have been computed based on 100 different executions for each test instance, using the best combination of parameters found and different seeds. Table 3 considers only the 30 best executions. All the algorithms reach the best solutions before the run times and number of iterations presented. The DDE, HDE and GA obtain better solutions for larger instances. The TS is the slowest algorithm and it obtains a higher average fitness (Table 4). DDE is the faster algorithm for larger instances.\nWhen using the SPA for creating the initial solutions, the times and number of iterations decreases -problem C64. This problem is computationally harder than the C61, however the best solution is obtained faster. To improve the solutions we consider more efficient to apply initially a SPA and then the meta-heuristic to improve the solutions.\nTable 4 presents the average fitness and the average time obtained with GA, HDE, TS, LS-PBPSO and DDE using a limited number of iterations for the problems C41, C51 and C61 (harder problems). The first column represents the number of the problem (Prob), the second column demonstrates the number of iterations used to test each instance and the remaining columns show the results obtained (AvgF -Average Fitness, AvgT -Average Time) by the five algorithms. The results have been computed based on 100 different executions for each test instance using the best combination of parameters found and different seeds. The DDE is the algorithm that presents the best average fitness in the best execution time.", "publication_ref": [], "figure_ref": ["fig_15", "fig_28", "fig_15", "fig_15", "fig_15", "fig_28", "fig_28"], "table_ref": ["tab_5", "tab_20", "tab_20", "tab_48", "tab_48"]}, {"heading": "Conclusions", "text": "In this paper we present a DDE algorithm to solve the WRALP. The performance of our algorithm is compared with the algorithms: GA, HDE, TS and LS-PBPSO.\nRelatively to the problem studied, the DDE algorithm presents better results. The DDE provides good solutions in a smaller execution time.\nExperimental results demonstrate that the proposed DDE algorithm is an effective and competitive approach in composing fairly satisfactory results regarding the solution quality and execution time for the WRALP. When using the SPA for creating the initial solutions, the best solution is obtained faster.\nIn literature the application of the DDE for this problem is nonexistent, for that reason this article shows its enforceability in the resolution of this problem.\nThe continuation of this work will be the search and implementation of new methods for speeding up the optimisation process. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The increasing demand of electronic systems, that require more and more processing power, low energy consumption, reduced area and low cost, has lead to the development of more complex embedded systems, also known as System-on-Chip (SoC), in order to run multimedia, Internet and wireless communication applications [9]. These systems can be built of several independent subsystems, that work in parallel and interchange data. When these systems have more than one processor, they are called Multi-Processor System-on-Chip (MPSoC). Currently, several products, such as cell phones, portable computers, digital televisions and video games, are built using embedded systems. While in embedded systems the communication between Intellectual Property (IP) blocks is basically done through a shared bus, in multiprocessor embedded systems this kind of interconnection compromises the expected performance [2]. In this case, the communication is best developed using an intrachip network, implemented by a Network-on-Chip (NoC) [6] [5] [1] platform. Some multimedia and Internet applications for wireless communications are using genetic algorithms and can benefit from the advantages provided by parallel processing on MPSoCs. In this paper, we present a parallel genetic algorithm that runs on Hermes Multi-Processor System (HMPS) architecture and discuss the impact of migration strategies on performance. In Section 2, we describe the HMPS architecture. The parallel genetic algorithm, used in this paper, is presented in Section 3 and some simulation results are introduced in Section 4. Finally, we draw some conclusions and future work in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Processor System-on-Chip Platform", "text": "Figure 1 shows the Multi-Processor System-on-Chip (MPSoC), called Hermes Multiprocessor System (HMPS) [3]. MPSoC architectures may be represented as a set of processing nodes that communicate via a communication network. Switches compose the network and RISC processors the processing nodes (Plasma). Information exchanged between resources are transfered as messages, which can be split into smaller parts called packages [7]. The switch allows for retransmission of messages from one module to another and decides which path these messages should take. Each switch has a set of bidirectional ports for the interconnection with a resource and the neighboring switches. As the total number of tasks composing the target application may exceed the MPSoC memory resources, one processor is dedicated to the management of the system resources (MP -Manager Processor). The MP has access to the task repository, from where tasks are allocated to some processors of the system.\nThe interconnection network is based on HERMES [4], that implements wormhole packet switching with a 2D-mesh topology. The HERMES switch employs input buffers, centralized control logic, an internal crossbar and five bi-directional ports. The Local port establishes the communication between the switch and its local IP core. The other ports of the switch are connected to neighboring switches. A centralized round-robin arbitration grants access to incoming packets and a deterministic XY routing algorithm is used to select the output port.\nThe processor is based on the PLASMA processor [10], a RISC microprocessor. It has a compact instruction set comparable to a MIPS-1, 3 pipeline stages, no cache, no Memory Management Unit (MMU) and no memory protection support in order to keep it as small as possible. A dedicated Direct Memory Access (DMA) unit is also used for speeding up task mapping, but not for data communications. The processor local memory (1024 Kbytes) is divided into four independent pages. Page 0 receives the microkernel and pages 1 to 3 the tasks. Each task can hold 256 Kbytes (0x40000).\nThe HMPS communication primitives, WritePipe() and ReadPipe(), essentially abstract communications, so that tasks can communicate with each other without knowing their position on the system, either on the same processor or a remote one. When HMPS starts, only the microkernel is loaded into the local memory. All tasks are stored in the task repository. The manager processor is responsible for reading the object codes from the task repository and transmit them to the other processors. The DMA module is responsible for transferring the object code from the network interfaces to the local memory.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Parallel Genetic Algorithm", "text": "The Parallel Genetic Algorithm (PGA) is based on the island model, in which serial isolated subpopulations evolve in parallel and each one is controlled by a single processor. This processor periodically sends its best individuals to neighboring subpopulations and receives their best individuals. These individuals are used to substitute the local worst ones. It is obvious that the GA time processing increases with population size. Therefore, small subpopulations tend to converge quickly when isolated.\nThe PGA is executed by the HMPS platform. Each processor corresponds to an island and its initial subpopulation is randomly generated, evolving independently from the other subpopulations, until the migration operator is activated, as described in Algorithm 1. Premature convergence occurs less in a multi-population GA and can be ignored, when other islands produce better results. Each island can use a different set of GA operators, i.e. crossover and mutation rates, which causes different convergence. Migration of the chromosomes among the islands prevents mono-race populations, which converge Algorithm 1. PGA 1: Initialize the evolutionary parameters 2: t \u2190 0 3: Initialize a random population p(t) 4: Evaluate p(t) in order to find th best solution 5: while (t < NumGenerations) do 6:\nt \u2190 t + 1 7:\nSelect p(t) from p(t \u2212 1) 8: Crossover 9: Mutation 10:\nEvaluate p(t) in order to find th best solution 11:\nif (t mod MigrationRate = 0 ) then 12:\nMigrate local best[p(t)] to the next processor 13:\nReceive remote best[p(t)] from the previous processor 14:\nReplace worst[p(t)] by best[p(t)] 15:\nend if 16: end while prematurely. Periodic migration, which occurs after some generations, prevents a common convergence among the islands.\nThe PGA requires the definition of some parameters: number of processors, how often the migration will take place, which individuals will migrate and which individuals will be replaced due to migration. The island model introduces a migration operator in order to migrate the best individuals from one subpopulation to another.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Topology Strategies", "text": "In this work, we investigate two topology strategies to migrate individuals from one subpopulation to another: ring and neighborhood. In the ring topology, the best individuals from one subpopulation can only migrate to an adjacent one. As seen in Figure 2, the best individuals from subpopulation 6 can only migrate to subpopulation 1 and the best individuals from subpopulation 1 can only migrate to subpopulation 2. In Algorithm 2, migration is implemented using this kind of strategy. In the neighborhood topology, the best individuals from one subpopulation can migrate to a left and to a right neighbor, as seen in Figure 3. For this kind of strategy, migration is implemented as in Algorithm 3.\nChoosing the right time of migration and which individuals should migrate are two critical decisions. Migrations should occur after a time long enough for allowing the development of good characteristics in each subpopulation. Migration is a trigger for evolutionary changes and should occur after a fixed number of generations in each subpopulation. The migrant individuals are usually selected from the best individuals in the origin subpopulation and they replace the worst ones in the destination subpopulation. Since there are no fixed rules that would give good results, intuition is still strongly recommended to fix the migration rate [11]. next := 0; previous := local \u2212 1; 10: end if 11: Send the best individuals to the task, whose identifier is next; 12: Receive the best individuals from the task, whose identifier is previous. next := local + 1; previous := local \u2212 1; 7: end if 8: if local = number of tasks \u22121 then 9:\nnext := 0; previous := local \u2212 1; 10: end if 11: Send the best individuals to the task, whose identifier is previous; 12: Send the best individuals to the task, whose identifier is next; 13: Receive the best individuals from the task, whose identifier is previous; 14: Receive the best individuals from the task, whose identifier is next. Sending an individual from one subpopulation to another increases the fitness of the destination subpopulation and maintains the population diversity of the other subpopulation. As in the sequential GA, issues of selection pressure and diversity arise. If a subpopulation receives frequently and consistently highly fit individuals, these become predominant in the subpopulation and the GA will focus its search on them at the expense of diversity loose. On the other hand, if random individuals are received, the diversity may be maintained, but the fitness of the subpopulation may not be improved as desired. As migration policy, the best individual is chosen as the migrant, replacing the worst one in the receiving subpopulations. For the migration frequency, an empirical value was adopted based on the number of generations.", "publication_ref": [], "figure_ref": ["fig_15", "fig_28"], "table_ref": []}, {"heading": "Simulation Results", "text": "The two non-linear functions defined by Equation 1 were used by the PGA for optimization. Function f 1 (x) has 14 local maximum e one global maximum in the interval [-1, 2], with an approximate global maximum of 2.83917, at x = 1.84705. Function f 2 (x, y) has various local minimum and one global minimum in the interval \u22123 \u2264 x \u2264 3 and \u22123 \u2264 y \u2264 3, and an approximate global minimum of \u221212.92393, at x = 2, 36470 and y = 2.48235.\nmax x f 1 (x) = sen(10\u03c0x) + 1 min x,y f 2 (x, y) = cos(4x) + 3sen(2y) + (y \u2212 2) 2 \u2212 (y + 1) (1)\nThe performance of the PGA can be evaluated based on its speedup and efficiency. Speedup S p [8] is defined according to Equation 2, where T 1 is the execution time of the sequential version of the genetic algorithm and T p is the execution time of its parallel version.\nS p = T 1 T p (2)\nEfficiency E p [8] is defined according to Equation 3, where 1 p < E p \u2264 1 and p is the number of processors employed.\nE p = S p p (3)\nBased on simulation results for the optimization of f 1 (x) and f 2 (x, y) using the ring and neighborhood topologies, we obtained the graphics for speedup and efficiency shown in Figure 4 and Figure 5 respectively. The data are presented as triples consisting of the number of slave processors used (NumProc), the migration rate (MigRate) and the migration interval (MigInt). ", "publication_ref": [], "figure_ref": ["fig_29", "fig_116"], "table_ref": []}, {"heading": "Conclusions", "text": "For the ring topology, the behavior of the two functions shows that, keeping the migration interval constant and varying the migration rate, if the increase in the migration rate resulted in an increase in speedup and efficiency, the fitness of the individuals, received by one or more populations during the migration phase, accelerated the evolutionary process, decreasing the convergence time. On the other hand, if the increase in the migration rate resulted in the decrease of speedup and efficiency, then we can say that the fitness of these individuals did not influence enough the evolutionary process of the populations that received them. In this case, the convergence time increases.\nIn the future, we intend to investigate the impact of other migration strategies on the performance of the parallel Network-on-chip based implementation of genetic algorithms. One of the these topologies is broadcasting, which allows each processor to send the best solution found so far to all the other processors in the network. We will assess the impact of heavy message send/receive workload on the overall system performance. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Genetic algorithms are a method of searching solutions in optimisation problems.\nAs the range of problems that can be potentially solved using genetic algorithms varies greatly, so do the algorithms themselves. Furthermore, many parameters (like the number of individuals in a population for instance) affect the algorithm's behaviour. The abundance of variations to choose from and parameters to set can make the process of implementing a genetic algorithm quite lengthy. The final, most efficient configuration can be found by trial and error -different variants are implemented and tested, different values of parameters are tried out. Design patterns have been used in software development for many years. They provide well-tested solutions to problems commonly encountered during software design. Using patterns improves code robustness and readability. For those and other reasons patterns are applied in various kinds of systems, including systems based on genetic algorithms. But so far research focused mostly on using design patterns in genetic algorithms to create an alterable framework. The research focused on applying the patterns to gain certain benefits, not on analysing the benefits themselves. Furthermore, only the final effects were described, not the implementation process or the role of patterns.\nThe goal of this research was to examine how the process of modifying a genetic algorithm-based system is affected by the use of design patterns. Two tests have been performed: one based on a case study, the other based on comparing two systems. Both tests used a genetic algorithm system, named GATATest, implemented for this research. It solves a non-trivial, real-life problem.\nThe content of this paper is organized as follows: genetic algorithms, design and implementation issues are described first, then design patterns as well as detailed information on the patterns applicable to genetic algorithms are presented shortly. Next, the evaluation methods of the design patterns influence is described. The summary finishes the paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Genetic Algorithms", "text": "The idea behind this technique is quite simple: the algorithm starts with a population of random solutions to a given problem. The solutions are tested and evaluated. The ones that prove to be the best reproduce with each other, possibly breeding new, better solutions. Occasionally a random modification -mutation, is introduced in a solution to search through areas not covered by the current population. The algorithm starts from a population of individuals, each representing a solution to the given problem, encoded in a chromosome. The algorithm consists of several steps, as shown in Fig. 1. Its first step is evaluating the fitness of the individuals. In this process each solution is tested and assigned a value representing how well it performed with regard to the rest of the population. This is the individual's fitness value and the better the solution, the greater it is. Next the algorithm's termination criteria are tested. If the criteria are met, the algorithm stops and the best individual from the current population is selected as the result. In the opposite case, a new population of individuals is generated. The process of creating a new population starts with selecting individuals for reproduction. The selection is performed based on the fitness calculated. Better solutions have a higher chance of being selected and can be selected multiple times while inferior solutions might not be selected at all. After proper individuals have been selected, they are joined in pairs. This is most commonly done randomly. The pairs then produce offspring -each pair produces two new individuals. The chromosomes of the offspring are determined by means of crossing over. In the simplest algorithm -one-point crossover -the chromosomes of the parents are cut in two parts (of possibly different length) at the same, randomly selected point, the parts are exchanged and form two new chromosomes.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Fig. 1. The basic genetic algorithm", "text": "Mutation means that each gene on a chromosome has a (usually small) probability of being changed to a different value. After all mutations have been performed, the new population completely replaces the old one and the algorithm starts from the beginning.\nDespite the basic algorithm being constructed using simple ideas and mechanisms, its implementation can be quite complex. There is a great abundance of possible modifications to the basic genetic algorithm. This covers not only the various versions of selection and genetic operations but also the advanced structures of the chromosome and additional extensions. The need of modeling the solution and choosing an appropriate encoding only adds to the complexity of choosing the most efficient options for a single implementation. Researchers conducted many studies on which options to choose and how to best represent the solution for specific kinds of problems. Such research can be very useful in designing and implementing a genetic algorithm. However, in order to make use of such information, the type of problem being solved has first to be identified. Searching for the most suitable selection strategy, changing the representation of the solution or introducing more complicated concepts might require a lot of modifications. A practical approach to this kind of situation would be improving the algorithm's design. If the system itself allowed for easier modifications, the cost of implementing changes to the algorithm would be much lower.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Design Patterns", "text": "Design patterns in software engineering are general solutions to well-known object design problems. They were thoroughly tested and have proven themselves to be effective. Yet despite the power that lies in software design patterns the solutions they provide are quite simple and are considered to be elegant among software designers.\nThe literature study performed as part of this research has indicated some patterns that might be useful in such a system. We can mention here the abstract factory design pattern, which is responsible for creating objects. It provides the user with an interface for instantiating objects of a family of classes. Whenever an object of such a class is to be created, a method of the appropriate factory class is called to create the object. The abstract factory can be used in genetic algorithms to ensure compatibility between different parts of the algorithm. An example would be a factory responsible for creating chromosomes and encapsulated algorithms for crossover and mutation [5].\nThe goal of the bridge design pattern is to separate the implementation from its abstraction. Normally, when the implementation and abstraction are defined by an interface or abstract class and a concrete implementation, they are tied together. The separation provided by the bridge pattern allows either the implementation or the abstraction to be modified without affecting the other.\nSimilarly to the abstract factory pattern, the builder pattern is responsible for creating objects. The goal, however, is quite different -builders are used for creating objects of a single class that has a complex structure. The builder's interface reflects the parts of the object or steps of the creation process. Concrete builders correspond to different representations of the object.\nThe decorator pattern allows to add functionality to existing objects without changing their structure. Furthermore, the functionality is added for a single object, not entire classes. This is done by wrapping an object with a decorator objects that contains the added functionality. The decorators operations are used to access its functions.\nThe strategy design pattern is used to express algorithms as interchangeable objects. Algorithms are encapsulated in classes and instantiated as objects. This way the behaviour of the context utilising a strategy can be changed by using a different strategy object.\nThe template method pattern defines an outline of an algorithm which is implemented in a method. Its main steps are defined as separate methods. Subclasses of the class containing the algorithm may change the implementation of particular steps, but the overall structure remains the same.\nThe visitor pattern defines operations that are to be performed on objects belonging to an object structure. The implementation of a new operation would require to modify each object class to add the new behaviour. As these classes might be quite numerous, this operation might require great effort. Instead, the visitor pattern encapsulates the operation in a visitor class that visits the object structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Methods", "text": "Rationale suggests that using design patterns in the design of a system based on a genetic algorithm should improve the systems flexibility and allow for easier modifications. Applications of patterns to such systems have been described in literature [5], [6], [9], but the actual influence of patterns on the development of the system has not been evaluated. This section describes the methods of verifying the usefulness of design patterns. As the ease of modifying a system is not directly measurable, the tests are based on subjective software developers opinions and measuring software complexity on the basis of Halstead's metrics.\nIn order to observe the effects of using design patterns in practice a system named GATATest, was designed and constructed. The most of the patterns described in previous section was applied. After the system was implemented, various changes to the algorithm were introduced to test the ease of modification. The range of changes included:\nimplementing different selection and crossover methods, -changing the termination criteria, -introducing different chromosome representations, -modifying the evaluation method.\nIn order to more objectively evaluate the program in this research, Halstead's metrics are used to compare genetic algorithm designs with and without the use of patterns. They treat the program as a collection of tokens that can either be an operand or an operator [1]. They are measures of a program's complexity. They are calculated using the JHawk tool; following the method described in [8]. These metrics indicate which design is more complex. In general, the more complex a design is, the more difficult it is to modify the code. Therefore, based on the values of the metrics, it can be deduced whether the patterns used improve the process of modifying the algorithm.\nOnly the length, volume and effort measures are used for further computations. The length is defined as the total number of tokens used; it can be calculated by the formula:\nN = N 1 + N 2 , (1\n)\nwhere N 1 is the total number of operators, N 2 is the total number of operands.\nThe metrics volume measures the size of the implementation and resembles \"the number of mental comparisons required to generate a program\" [4].\nV = N log 2 n, (2\n)\nwhere n is the sum of n 1 (the number of distinct operators) and n 2 (the number of distinct operands).\nV * is the volume of the minimal implementation of the algorithm and it is calculated as:\nV * = (2 + n * 2 )log 2 (2 + n * 2 ).\nHere n * 2 is the number of potential operands (it represents a minimal number of operands that can represent a computer program and are specific for every programming language).\nThe last metrics the effort measures the amount of mental activity required to implement the algorithm as the measured program:\nE = V L (3\n)\nwhere L is level metrics which measures how well a program is written and formally can be expressed as L = V * /V . The measures for a class are sums of the values calculated for individual methods. Similarly, calculating the metrics for a package requires summing the values computed for individual classes. This method can be extended for the whole system by summing the metrics of all packages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The GATATest System", "text": "This section describes the GATATest system used in both evaluation methods. It is based on the genetic algorithm which goal is to evaluate sets of market indicators in order to find the one set that allows for the most precise predictions.\nFor the purpose of the system 3 market indicators from technical analysis were implemented: simple moving average, weighted moving average and exponential moving average [2]. These indicators can be calculated taking into account various numbers of past values. In the system each of the averages uses 15, 30 and 45 last values, which results in 9 different indicators. The indicators are kept in a list structure in the system. The chromosome defines a combination of the indicators in the list. It has the form of a boolean array. A boolean value on the i-th position in the chromosome codes whether the i-th indicator is part of this solution. Evaluation is based on testing the predictions generated by a set of indicators on real data. The data consists of the values of WIG20 futures contracts on the Warsaw Stock Exchange between 17-Nov-2000 and 28-Apr-2008. The evaluation of a specific chromosome is performed in 2 stages: calculating the objective function and transforming the result to a fitness value which is expressed in number of points reflecting the precision of prediction.\nOther important elements of the algorithm include mechanisms of creating the initial population, generating a new population (selection, crossover, mutation) and the termination criteria. In the initial algorithm, these elements were based on the fundamental version of a genetic algorithm [3]. Details can be presented as follows: initialisation -the initial population is created randomly and consists of 50 individuals; selection -the roulette wheel algorithm was used. This algorithm selects parents randomly. The probability of selecting an individual is equal to the individual's fitness value divided by the sum of the fitness of all individuals. The crossover operation was implemented as one-point crossover. Bitwise mutation [3] is applied (every bit on the chromosome has the probability of 0.1% to be changed to the opposite value). The initial algorithm has only one termination criterion -it will stop after 100 generations have elapsed.\nFig. 2 presents the UML model of the system. It contains only the classes forming the structure of the algorithm. Classes responsible for the problem domain technical analysis -are not included. Furthermore, only the abstract classes are presented for clarity. The Algorithm class is the interface for the whole algorithm. It has only one method that runs the algorithm. It is also responsible for presenting the result. The main engine of the system is located in the Population class. It coordinates all processes regarding the creation of the initial and subsequent generations, verifying termination criteria etc. The Individual class represents an individual in the algorithm. It contains a chromosome, which Fig. 2. The UML model of the GATATest system can be retrieved for processing. It can also be assigned a value -the effect of its evaluation. The function of the Pair class is to bind two Individual objects together. It is used in the selection and crossover processes to create pairs of individuals for reproduction. The classes TAIndicator and DataElement are used in several methods, though they were not placed on the model. They represent an abstract technical analysis indicator and a portion of data prepared for analysis, respectively. Other classes in the model are parts of design patterns. The system's design makes extensive use of the strategy pattern. The Population class forms the general structure of the algorithms and defines its steps (initialisation, evaluation, selection, crossover, mutation, termination criteria test). All these specific steps are implemented as strategies. The Population class aggregates the strategies and uses them to perform the stages of the algorithm.\nThe bridge pattern is used to separate the structure of the chromosome and the form of the gene. The ChromosomeStructure class defines the implementation of the chromosome (array, list, tree). The GeneImplementation class specifies the implementation of the gene (boolean, integer, object) and how many genes code a single parameter. It is also responsible for creating and mutating genes. The crossover operation is performed on the structure created by the abstraction, thus the ChromosomeStructure and CrossoverStrategy implementations need to be compatible. The MutationStrategy also needs to be compatible with the abstraction, as it iterates through the genes. The GeneImplementation class implements the method of mutating a gene, so it is independent of the mutation operation.\nThe builder pattern creates objects representing chromosomes. The builder is responsible for matching a ChromosomeStructure with a GeneImplementation and creating the required genes. The extendChromosome() method extends the chromosome by adding genes encoding the use of a specific indicator. The factory is responsible for instantiating strategy and chromosome objects. Concrete factories also ensure that the created objects are compatible with each other.\nTemplate methods were used in several classes of the system. Good illustrations are the EvaluationStrategy and SelectionStrategy classes. In the first class, the evaluate() method is a template method. The EvaluationStrategy class relies on its subclasses to perform the actual evaluation in the calculateObjectiveFunction()method. The result is then transformed to a fitness value.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15"], "table_ref": []}, {"heading": "Evaluation Study", "text": "The first evaluation is based on our subjective observation of easiness level in introducing changes in GATATest code which aim was to improve the performance of the application in terms of achieved fitness function values. During the case study, a total of 9 changes were introduced. Most of the vital elements of the algorithm were modified; only the operation of mutation and the initialisation of the population were unchanged. The description of changes and results obtained by introducing them are presented in Table 1. The initial system generated individuals with a maximum value of 1761 points. The results of the final version of GATATest reached the level of 3130 points (fitness function).\nTable 1. The effects of the changes on the results and performance of the algorithm: Ravg is the average result (in points), Rmax is the maximum result from all the runs (in points), Tmax is the average time of a single run (measured in seconds)", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "System version", "text": "Change Most of the changes included creating new elements instantiated by the Ab-stractFactory. For each of these changes a concrete factory was created that instantiated the new elements. The final system has 9 concrete factories that resemble the evolution of the system from its initial form to the final version. Change 1 introduces elitism. Change 2 causes creation an alternative termination criterion. Change 3 introduces trust levels. It is expressed as a real value in the range from 0 to 1 (both inclusive) and is encoded in the chromosome using binary code. It is taken into account in the voting process. Change 4 represents change of selection strategy in genetic algorithm. It is calculated in relation to the weakest individual in the population -the fitness reflects the difference in the value between the given individual and the weakest one. Change 5 -extends the Change 3. The new gene implementation encodes two trust levels for each indicator: one for its rise predictions, the other for fall predictions. Change 6 introduces the possibility to generate a trust level of negative value which means that when an indicator predicts one type of trend, it is treated as the opposite prediction with a positive trust level of the same value. Change 7 -it changes the encoding of the trust levels from binary coded values to real values. Change 8 relies on introducing intermediate crossover and flat crossover. One of the important differences between these operations and one-point crossover used previously is that they create only one offspring from a pair of parents. Change 9 introduces the structure of the chromosome. The structure was changed from an array to a constant-sized list.\nThe conducted case study allowed to examine all the applied patterns. Abstract factory was the most often engaged pattern. Whenever a new strategy or builder was created and was to be used, a concrete implementation of the AbstractFactory class needed to be created or modified. The advantage of using an abstract factory was visible. All of the algorithm's configuration was held in one place. The effects of this feature could be clearly seen in changes 8 and 9, where the strategy classes had to be instantiated to match the chromosome.\nEvaluating the bridge pattern presents some difficulties, as the abstraction and implementation parts can be defined in various ways. In general, the bridge pattern can be viewed as useful for separating the structure and representation of the chromosome. In sum, this pattern would be better suited for genetic algorithm frameworks, where the exact form of the chromosome varies depending on the problem.\nThe builder pattern was used to configure the bridges representing the chromosome. In the presented design the builder pattern was applicable because the building process could be divided into separate phases. Each phase resembled the extension of the chromosome to encode another indicator. Yet not all chromosome representations might allow such a division, so the pattern might not be generally appropriate for every genetic algorithm system. Its usefulness is evaluated as rather low.\nEffects of the strategy pattern application was lowering the effort of changing the algorithm's operations. The strategies used in the system have simple interfaces and new strategy classes can be added easily. Another effect was reducing the complexity of the system.\nThanks to the template methods only the abstract classes needed to be modified to gain additional functionality. The added code affected all subclasses automatically. Moreover, the concrete classes were simpler and contained less methods. Two of the design patterns described in section 3 were not used in the GATATest system at all -the decorator pattern and the visitor pattern. However, further research would allow to examine these patterns.\nThe second evaluation of the design patterns was based on metrics calculated with the use of the JHawk tool to compare two systems: one designed using design patterns, and one designed without them. As the first system, the GATATest system was used. The other system was created for the purpose of this comparison and is called Patternless. Much of the code in the Patternless system was reused from GATATest. Only three of Halstead's metrics were calculated: length, volume and effort. Table 2 presents the values of the metrics and information about the number of classes and methods in each system.\nThe comparison of the two systems shows that GATATest has more classes and methods than Patternless. This is mostly caused by the use of patternsthey enforce the creation of additional classes and levels of abstraction. The code is more scattered across the many classes of the system. Furthermore, additional code is needed to provide communication between the classes. Therefore the length and volume metrics are higher in GATATest than in Patternless but the effort value is higher. The code in Patternless is accumulated in a lesser number of methods so the number of operators and operands in a single method is higher. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Summary", "text": "The aim of the presented research was to check whether the software design patterns allow for a much more flexible design. The pattern that prove to have the greatest influence on the systems flexibility were the abstract factory and strategy. The latter was most useful when applied to the selection and crossover operations of the algorithm. The template method had a positive effect, although not as important as the previously mentioned patterns. The bridge and builder patterns also improved the flexibility of the system but were redundant -the same features could be provided by the abstract factory and simple inheritance, respectively. The drawback of the case study is that it was performed by the authors only that is why to obtain more objective opinion these tests will be conducted by more developers. The computed Halstead's software metrics show that GATATest has greater length and volume values as compared to the other system. Such a result is not surprising -the use of design pattern required the creation of additional classes and code to provide communication between them. Despite this fact, the effort metrics bore higher values for the Patternless system.\nIt can be argued that a system that both has more source code and requires less effort to implement, is also less complex.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In some previous works ( [5], [6]), we defined an L-Fuzzy context (L, X, Y, R), with L a complete lattice, X and Y the sets of objects and attributes respectively and R \u2208 L X\u00d7Y an L-Fuzzy relation between the objects and the attributes, as an extension to the fuzzy case of the Formal contexts of Wille ([13]) when the relation between the objects and the attributes that we want to study takes values in a complete lattice L. In order to work with these L-Fuzzy contexts, we use the derivation operators 1 and 2 defined by:\n\u2200A \u2208 L X , B \u2208 L Y A 1 (y) = inf x\u2208X {I(A(x), R(x, y))} , B 2 (x) = inf y\u2208Y {I(B(y), R(x, y))} .\nwhere I is a fuzzy implication operator defined in (L, \u2264), which is decreasing in its first argument, and where A 1 represents, in a fuzzy way, the attributes related to the objects of A and B 2 the attributes related to the objects of B.\nThe information of the context is visualized by means of the L-Fuzzy concepts which are pairs (A, A 1 ) \u2208 (L X , L Y ) with A \u2208 f ix(\u03d5) the set of the fixed points of the operator \u03d5, being this one defined by the derivation operators 1 and 2 Work partially supported by the Research Group \"Intelligent Systems and Energy\" of the University of the Basque Country, under Grant GIU 07/45 and by the Research Project of the Government of Navarra (Resolution 2031 of 2008). mentioned above as \u03d5(A) = (A 1 ) 2 = A 12 . These pairs represent, in a fuzzy way, a set of objects that share some attributes and can be interpreted as follows: We focus on those objects and attributes whose membership degrees stand out from the rest.\nThe set L = {(A, A 1 )/A \u2208 f ix(\u03d5)} with the order relation \u2264 defined as:\n\u2200(A, A 1 ), (C, C 1 ) \u2208 L, (A, A 1 ) \u2264 (C, C 1 ) if A \u2264 C (or eq. C 1 \u2264 A 1\n) is a complete lattice that is said to be an L-Fuzzy concept lattice ( [5], [6]).\nOther extensions of the Formal concept analysis to the Fuzzy area are in [12], [4], [10] and [11] .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Use of Linguistic Labels in L-Fuzzy Contexts", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linguistic Variables", "text": "We begin by summarizing some well-known definitions of fuzzy logic.\nA fuzzy number [14] is a normal and convex fuzzy set. There are many kinds of fuzzy numbers, e.g. triangle, trapezoid, S-shaped, bell etc. These fuzzy numbers characterize the linguistic variables that will appear next.\nTaking the definition of Zadeh [14]: By a linguistic variable we mean a variable whose values are words or sentences instead of numbers and that is characterized by a tuple (V, T (V ), U, G, M) where V is the name of the variable, T (V ) is the set of linguistic labels or values, U is the Universe of discourse, G is a syntactic rule which generates the values of T (V ) and M is the semantic rule which assigns to each linguistic value t \u2208 T (V ) its meaning M (t).\nThis meaning of a linguistic label t is defined by a compatibility function c t : U \u2192 [0, 1] which assigns its compatibility with U to every t.\nWe will now consider linguistic variables defined in the Universal set [0, 1] where the meaning of the label M (t) is represented by a symmetrical trapezoidal fuzzy number. Specifically, we will use those represented in Fig. 1  Observe that these trapezoidal numbers are the restriction to the interval [0,1] of the original ones defined in R.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Notation.", "text": "We denote the compatibility of the value x \u2208 [0, 1] with the label t by x t .\nThen, \u2200x \u2208 [0, 1]:\nc t (x) = x t = \u23a7 \u23aa \u23a8 \u23aa \u23a9 1 + m(x \u2212 a) if x \u2264 a 1 i f a \u2264 x \u2264 b 1 + m(b \u2212 x) if x \u2265 b Where m = min 1 a , 1 1 \u2212 b .\nThese two values, a, b \u2208 [0, 1], are those assigned to label t \u2208 T (V ) in its definition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Obtaining L-Fuzzy Contexts with Significant Relations", "text": "The process of obtaining the relation R of an L-Fuzzy context (with values in a lattice L) that represents the relationship between the set of objects and the other of attributes is not standardized (both where the context takes values in the same rank as in different ranks). In particular, the methods used do not work well when we have a line (row or column) of the relation with very low values since their objects and attributes do not appear in the L-Fuzzy concepts as outstanding elements (and then, as we mentioned in the introduction, will not appear in their interpretation).\nMany times, we have concluded that it is better to eliminate those lines (of object or attribute) and to reduce the context of work. Nevertheless, if we eliminate the objects and the attributes, then we will lose information. For example, some of the existing relationships between the objects and the attributes will disappear.\nTo solve this problem, given a certain group of objects that we want to analyze, we will try to obtain relevant values in each one of the columns of the L-Fuzzy context by means of a linguistic variable that clarifies the different attributes but that, in addition, indicates to us how the original values have been transformed. To do this, for every attribute of the context we will choose its best linguistic label. (Analogous, we can interchange the role of the objects and the attributes.)\nWe are going to use a linguistic variable V whose set of terms or labels allows us to classify the values of the relation according to its proximity to 0 or 1. It is important that \u2200x \u2208 L, \u2203|t \u2208 T (V ) such that x t = 1. For this reason, we will use trapezoidal fuzzy labels.\nIn this way, if we take as a departure point an L-Fuzzy context (L, X, Y, R), we are going to transform some elements of Y (those with low values) by the linguistic variable V that will change the context.\nThen, for every attribute y j , we are going to see how we can assign the label: We take the values of the relation corresponding to the attribute y j , we obtain their maximum M yj and we choose the only label t verifying that M yj t = 1. That is, a \u2264 M yj \u2264 b, where a and b are the values associated with label t in its definition. We will denote this label by t yj . Definition 1. For every y j \u2208 Y , the obtained label t yj is said to be the best linguistic label associated with attribute y j .\nIn the same way, we can assign its best linguistic label t xi to every object x i .\nThen, let Z \u2286 Y be the set of attributes y j with low values of R(x i , y j ), \u2200x i \u2208 X. We can take the best linguistic label t yj \u2208 T (V ) for every attribute y j \u2208 Z in order to transform these attributes into others more relevant.\nLet (L, X, Y, R) be an L-Fuzzy context and C = {(y j , t yj ), y j \u2208 Z \u2286 Y, t yj \u2208 T (V )} that associates linguistic labels to the elements of Z. We are going to give the following definition:\nDefinition 2. The L-Fuzzy context (L, X, Y C , R C ), where Y C = (Y \\Z) {y j ty j , \u2200y j \u2208 Z} and R C (x i , y j ) = R(x i , y j ) if y j \u2208 Y R(x i , y j ) ty j in other case is said to be a labeled L-Fuzzy context.\nIn the same way, we can define the new L-Fuzzy context taking as a departure point the object set.\nNext, we will see how this change influences the calculation of the L-Fuzzy concepts associated with the basic points.\nProposition 1. If A \u2208 L X is a basic point, A(x) = 1 if x = x i 0 in other case then, A 1 (y) = R C (x i , y) \u2208 L, \u2200y \u2208 Y C\n, is the L-Fuzzy concept intension obtained taking A as a departure point. Moreover, the extension verifies that A 12 (x i ) = 1.\nProof. If A \u2208 L X is a basic point, to calculate the L-Fuzzy concept derived from A, we apply the derivation operator and, using a residuated implication operator (for example, the Lukasiewicz one), we obtain the intension of the concept:\nA 1 (y) = inf x\u2208X {I(A(x), R C (x, y))} = R C (x i , y), \u2200y \u2208 Y C .\nTherefore, if y j ty j \u2208 Y C is one of the attributes modified by label t j , the modified values R C (x i , y j ty j ) can be found in the intension of the corresponding L-Fuzzy concept:\nA 1 (y j ty j ) = R C (x i , y j ty j ) \u2200x i \u2208 X, y j ty j \u2208 Y C , t yj \u2208 T (V ) .\nOn the other hand, with respect to the intension of the L-Fuzzy concept:\nA 12 (x) = inf y\u2208Y C {I(A 1 (y), R C (x, y))} = inf y\u2208Y C {I(R C (x i , y), R C (x, y))} .\nThen, we can say that A 12 (x i ) = 1.\nThat is, we have some L-Fuzzy concepts where the modified attributes appear outstanding in the new labeled L-Fuzzy context. This is going to allow us to analyze the behavior of these attributes with respect to the different objects of the context.\nExample 1. Let (L, X, Y, R) be the L-Fuzzy context represented in Table 1 where the values of column y 2 are quite low.\nTable 1. L-Fuzzy context R y1 y2 y3 x1 0.7 0.2 1 x2 0 0.1 0.8 x3 1 0.4 0.6 x4 0.3 0 0.9\nThese are the L-Fuzzy concepts derived from the basic points x 1 , x 2 , x 3 and x 4 using the Lukasiewicz implication operator:\nx 1 \u2192 {(x 1 /1, x 2 /0. 3, x 3 /0. 6, x 4 /0. 6), (y 1 /0. 7, y 2 /0. 2, y 3 /1)} x 2 \u2192 {(x 1 /1, x 2 /1, x 3 /0. 8, x 4 /0. 9), (y 1 /0, y 2 /0. 1, y 3 /0. 8)} x 3 \u2192 {(x 1 /0. 7, x 2 /0, x 3 /1, x 4 /0. 3), (y 1 /1, y 2 /0. 4, y 3 /0. 6)} x 4 \u2192 {(x 1 /1, x 2 /0. 7, x 3 /0. 7, x 4 /1), (y 1 /0. 3, y 2 /0, y 3 /0. 9)}\nAs can be seen, the membership degree of y 2 is not one of the highest in any of the L-Fuzzy concepts. Thus, as we mentioned in the introduction, this attribute will not appear in the interpretation of the L-Fuzzy concepts.\nWe are going to transform the column y 2 into another one with higher values. To do this, we take label t y2 with value 1 for the maximum of the column. In this case, label t y2 = medium is the result (with values a = 0.4 and b = 0.6 in its definition) obtaining the relation of Table 2.\nTable 2. New relation R C R C y1 y 2medium y3 x1 0.7 0.5 1 x2 0 0 . 2 0 . 8 x3 1 1 0 . 6 x4 0.3 0 0.9\nThe L-Fuzzy concepts calculated from the basic points in this new L-Fuzzy context (L, X, Y C , R C ) are:\nx 1 \u2192 {(x 1 /1, x 2 /0. 3, x 3 /0. 6, x 4 /0. 5), (y 1 /0. 7, y 2medium /0. 5, y 3 /1)} x 2 \u2192 {(x 1 /1, x 2 /1, x 3 /0. 8, x 4 /0. 8), (y 1 /0, y 2medium /0. 2, y 3 /0. 8)} x 3 \u2192 {(x 1 /0. 5, x 2 /0, x 3 /1, x 4 /0), (y 1 /1, y 2medium /1, y 3 /0. 6)} x 4 \u2192 {(x 1 /1, x 2 /0. 7, x 3 /0. 7, x 4 /1), (y 1 /0. 3, y 2medium /0, y 3 /0. 9)}\nIn this case, from the L-Fuzzy concept assigned to x 3 , we can say that x 3 is mainly associated with a high value of attribute y 1 and a medium value of y 2 .\nThis best label association process arises initially for those objects (or attributes) with low values due to the problem that has been explained. Nevertheless, it is possible to assign labels to all the objects (or attributes) as has been done in the case of Many-valued contexts [13,9] in the Formal concept theory by means of scales. This will allow us to have a general study of the L-Fuzzy context using the same tool in all cases.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_5"]}, {"heading": "L-Fuzzy Contexts with Anomalous Values", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Use of the Labels Associated with Objects and Attributes in L-Fuzzy Contexts with Anomalous Values", "text": "The described process of assigning its best label to every object or attribute of the L-Fuzzy context can not be very suitable when we have anomalous values in the context. We will understand by an anomalous value of an L-Fuzzy context, that which is not similar to the rest of the values of its row, nor of its column. If these anomalous values are the maximum of the corresponding row or column, they are those that determine the label that we associate, even though the rest of the values of the row or column do not fit well with them. On the other hand, only some of those anomalous values will be erroneous and, in the cases which we can verify this, it will be interesting to replace them.\nWe will follow these steps: -For every object x i and attribute y j , we obtain the maximum M xi and M yj of its row and column in order to associate their best labels (t xi and t yj ) by the process explained in the previous section. -We substitute every maximum by 0 and then, we calculate the new labels in the resulting relation. In the case of the new labels t * xi and t * yj change with respect to the old ones, R(x i , y j ) will be an anomalous value.\n-We will analyze (if it is possible) if the anomalous value is erroneous and, in that case, we will replace it.\nExample 2. Let (L, X, Y, R) be the L-Fuzzy context represented in Table 3.\nAnd let V be the linguistic variable which labels {very \u2212 high, high, medium, low, very \u2212 low} are associated to the fuzzy numbers defined by the intervals  We substitute now these maximum values by 0 and calculate the new best labels obtaining t *\nx2 = medium and t * y2 = medium. As the two labels have changed, we can conclude that the value R(x 2 , y 2 ) is anomalous and we will replace it in the cases where it has been proven to be erroneous.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_20"]}, {"heading": "Replacement of the Erroneous Values", "text": "Once the erroneous values have been detected, following the proposed idea for the case of absent values in the interval-valued L-Fuzzy contexts [1,3], we will develop the next process to replace these erroneous values: Let (L, X, Y, R) be the L-Fuzzy context where we have detected that R(x i , y j ) is erroneous. We will try to replace this erroneous value using linguistic variables and implication between attributes [2] of this type:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "If y k is high (medium, low) then y j is low (medium, high).", "text": "To do this, we take a linguistic variable V whose set of labels T (V ) contains the labels used in the implications. Then, for every attribute y j \u2208 Y and for every label t \u2208 T (V ) we obtain a new attribute y j t . And we get a new L-Fuzzy context (L, X, Y Y , R) where Y = {y j t /y j \u2208 Y, t \u2208 T (V )} and the relation R is extended \u2200x i \u2208 X to the new attributes in the following way:\nR(x i , y j ) = R(x i , y j ) if y j \u2208 Y R(x i , y j ) t in", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "other case", "text": "In this new context, we analyze if, excluding the object x i for which we have the erroneous value, it is possible to find any implication of the type y kt 1 \u21d2 y j t2 , that is verified with high values of support and confidence [2]. In this case, if an object has the attribute y kt 1 to a certain degree, it will also have the attribute y j t2 at the same level. Extending this result to the object x i the erroneous value can be estimated.\nExample 3. We are going to return to the L-Fuzzy context (L, X, Y, R) that we have considered in Example 2, given by Table 3 where we know that the value R(x 2 , y 2 ) is anomalous and we are going to suppose that we have proven that it is also erroneous.\nThe linguistic variable V that we are using is given by {very \u2212 high, high, medium, low, very \u2212 low} labels, associated to the fuzzy numbers defined by the intervals\n[1, 1], [0.7, 0.9], [0.4, 0.6], [0.1, 0.3] and [0, 0] respectively.\nWe analyze the implications between attributes in the set of objects X\\{x 2 }, in order to obtain those that are fulfilled with a high degree of support and confidence. Thus, if we analyze the implication y 1 low \u21d2 y 2 low we have to add to the context these new attributes and extend the relation with two new columns obtained from the compatibility of the initial attributes with the corresponding labels. The relation of the new context is given in Table 4. Considering only the objects of the set X\\{x 2 } in the new relation R, we obtain high values of support and confidence for the implication between attributes y 1low \u21d2 y 2low :\nsupp(y 1low \u21d2 y 2low ) = 2 3 = 0.67 , conf(y 1low \u21d2 y 2low ) = 2 2. 2 = 0.91 .\nWe can conclude that in a high percentage of cases (67%), we have the attributes y 1 low and y 2 low . Furthermore, in a 91% of cases, where the attribute y 1low appears, the membership degree of attribute y 2low is at least the same. As this percentage is high, we can expand the result and suppose that it is verified also in the case of object x 2 , and then:\nR(x 2 , y 2low ) \u2265 R(x 2 , y 1low ) = 1 .\nThus, we have:\nR(x 2 , y 2 ) low = R(x 2 , y 2low ) = 1 .\nThen, taking into account the definition of label low, we can conclude that the value R(x 2 , y 2 ) \u2208 [0. 1, 0. 3] and we will choose the medium point of this interval (rounding the values when it is necessary in order to obtain elements of the lattice L), to replace the erroneous value. That is, we will replace the erroneous value by R(x 2 , y 2 ) = 0. 2.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_20", "tab_48"]}, {"heading": "Conclusions", "text": "We have shown in this work how the linguistic variables can be very useful to obtain significant relations in the L-Fuzzy contexts and to locate anomalous values. This is not the only possible application. We will see in future works how they can also be used to represent those initial situations that we intend to analyze by means of the study of the derived L-Fuzzy concepts, or to analyze more thoroughly the anomalous values of an L-Fuzzy context. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Knowledge Extraction Based on Fuzzy Unsupervised Decision Tree: Application to an Emergency Call Center", "text": "Francisco Barrientos 1, and Gregorio Sainz Abstract. This paper describes the application of a fuzzy version of Unsupervised Decision Tree (UDT) to the problem of an emergency call center. The goal is to obtain a decision support system that helps in the resource planning, reaching a trade-off between efficiency and quality of service. To reach this objective, the different types of days have been characterized based on variables that permits available resources assignment in an easy and understandable way. In order to deal with availability of expert knowledge on the problem, an unsupervised methodology had to be used, so fuzzy UDT is a solution merging decision trees and clustering, providing the performance of both viewpoints. Quality indexes give criteria for the selection of a reasonable solution to the complexity, as well as interpretability of the trees and the quality of generated clusters, and also the type of days and the performance from the resources point of view.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The main problem in the design and management of any call center is to achieve an adequate trade-off between Quality of Service (QoS) and efficiency. A mediumlarge sized center can manage thousands of calls per day, and each one must be answered in very few seconds [1]. The management of a call center needs to know the approximate workload to be resolved in a given period, and only then is it possible to estimate the availability of resources to respond to this demand in accordance with the security and quality protocols established in each case. In this way, the center can reach the desired quality standards as well as the user's satisfaction [2]. This work has been partially supported by the Regional Government of Castilla y Le\u00f3n through the Agencia de Protecci\u00f3n Civil y Consumo.\nThe scope of this study is focused on emergency calls, taking into account the maximum availability of the service in order to avoid missing calls. Here, economic factors are not considered.\nIn this context, the availability of some decision taking system to support the call center managers, giving them help on the out of range service operation, which means the service is operating outside the expected parameters/standards, so as to mobilize the resources that may be needed in time. This works proposes an approach based on a fuzzy version of Unsupervised Decision Trees to reach a linguistic description of day categories involving the workload and resources required for each day category in the call center.\nThis paper is organized as follows: Section 2 gives a short description of the problem of resource management for an emergency call center and surveys the theoretical basis that supports the model. Section 3 explains the proposal of this work; then the methodology and the experimentation carried out and the analysis of results are given. Finally, the main conclusions and further research are outlined.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Emergency Call Center Problem", "text": "Call centers have been modeled by queuing theory usually, using the paradigm of producer/consumer. Another possible approach is to model the received calls as a time series and use time series forecasting to estimate the required resources to render the service [2].\nIn this case, another approach is suggested using a fuzzy version of the Unsupervised Decision Trees (UDT) [3] to characterize the days based on several variables, some intrinsic and others extrinsic to the system (Table 1), and which may be relevant for the call center workload. Based on historical data, estimations and forecasts of these variables, the system will characterize the next few days, and the associated workload, which will give support on making decisions concerning the availability and planning of resources to reach the service standards. This solution appears to be a good alternative due to the difficulty of getting expert knowledge on this focus of the problem. The UDT, unlike other unsupervised learning methods, does not hide the information on the characterization of each class and the involved attributes on the class assigned decision making, so this performance permits to obtain a linguistic description of the solution in accordance with the domain.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Decision Trees", "text": "The fact that two of the \"Top Ten Algorithms in Data Mining\" [4] are tree-based algorithms demonstrates the wide popularity of these methods in the field of data mining. Decision trees are perhaps one of the most widely used paradigms in the world of machine learning, because of their characteristics [5]. A key factor that has influenced its spread is the fact that there are different free implementations available. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Unsupervised Decision Trees", "text": "Recently, research on Unsupervised versions of Decision Trees have been done [3], with hybrid solutions between trees and clustering [5] techniques. This approach expects to combine the advantages of both methods: the classification of the data without prior information, based only on the values of the considered attributes, and the easy interpretation of the results, since each leaf node represents a cluster, and the path from the root to each leaf node represents a classification rule in the if-then-else form based on linguistic premises. Several criteria must be considered for fuzzy UDT: Inhomogeneity Threshold (IT) is related to the content of information in a data set. It is minimal when data are entirely homogeneous, and information content increases when data inhomogeneity increases. This threshold determines whether the node must be segmented. Segment Size Threshold (SST) is the minimum number of samples in a leaf node. If a node exceeds the inhomogeneity threshold it must be divided into segments of at least SST size, and each segment will be a new leaf node.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fuzzy Clustering", "text": "The goal of clustering is the classification of objects according to similarities between them and the organization of data into groups. The word \"similar\" must be understood as mathematical similarity, measured in a well-defined meaning, e.g. the Euclidean distance between two elements.\nSince there are some similarities between the techniques of clustering and decision trees, some quality indexes originally defined for fuzzy clustering domain, could be used to check the quality of the results obtained with unsupervised fuzzy trees, complementing those of the trees themselves.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quality Indexes", "text": "In UDT we cannot use data sets to estimate the classification error. Therefore, other quality indexes must be used to determine the goodness of the results. Here, tree performance (T-measure, IQN-T, IC ) [5] [6] and partition/clustering quality measures (SC, S, XB ) [8] [9] have been considered, as well as the combination of both measure types to reach a solution with a good trade-off.\nT-measure. T \u2208 [0, 1) evaluates the efficiency of a decision tree [5], where a value of 0 is undesirable and a value close to 1 signifies a good decision tree.\nT = 2n \u2212 N lnodes i=1 w i d i 2n \u2212 1 ,(1)\nwhere\nw i = Ni N for a resolved leaf node 2Ni N otherwise .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IQN-T.", "text": "The quality of a node \u03a9 (called IQN for Impurity Quality Node) is defined [6] as a combination between its purity and its depth.\nIQN T (\u03a9) = (1 \u2212 \u03d5(\u03a9))f (depth T (\u03a9)) , (2\n)\nwhere \u03d5(\u03a9) is an impurity measure normalized between [0, 1]. In this case, impurity has been defined as the average distance of all the data belonging to a leaf node to its centre of mass and f (x) = x. This gives us an idea of how compact the classes defined by the leaf nodes are. Unlike previous measure, this one takes precedence over more expanded trees, since nodes are better suited to the available training examples, even with the risk of falling into overfiting [7].\nIC. This index is based on the combination of ( 1) and ( 2), since each one promotes a contradictory performance, then a new tree quality index, called IC, is obtained.\nIC = (1 \u2212 d(T, IQN T )) , (3\n)\nwhere d(T, IQN T ) is the distance between the previous indexes, seeking a tradeoff between them in order to have a tree neither too compact nor too expanded.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SC.", "text": "Partition Index is the ratio of the sum of compactness and separation of the clusters [8]. It is useful when comparing different partitions having an equal number of clusters. A lower value of SC indicates a better partition.\nSC(c) = c i=1 N j=1 (\u03bc ij ) m x j \u2212 v i 2 N i c k=1 v k \u2212 v i 2 .\n(4)\nS. Separation Index, as opposed to the partition index (SC ), uses a minimumdistance separation for partition validity [8].\nS(c) = c i=1 N j=1 (\u03bc ij ) 2 x j \u2212 v i 2 N min i,k v k \u2212 v i 2 . (5\n)\nXB. Xie and Beni's Index aims to quantify the ratio of the total variation within clusters and the separation of clusters [9]. The optimal number of clusters should minimize the value of the index.\nXB(c) = c i=1 N j=1 (\u03bc ij ) m x j \u2212 v i 2 N min i,j x j \u2212 v i 2 . (6\n)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "The first step is the acquisition of raw data, whatever its origin or format. Then, it is necessary to do a data pre-processing, to standardize formats, normalize value ranges, missing data recovery, and so on. Once data are ready, an initial feature selection is performed, simplifying the problem by eliminating those variables that do not provide enough information or are redundant, etc. Next, the UDT is carried out using the previously selected variables. The tree performs a second selection, determining what attributes are most relevant to the problem and in what order. Finally, we analyze the results according to previously defined indexes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Gathering and Pre-processing", "text": "The initially available data to build the tree are: the daily number of calls and their classification from January 2005 to April 2007, the calendar with the events that happen each day, and data about the weather in this time period. The relevance of the historical data is not known \"a priori\", so in this case a heuristic limit was set: 28 days (4 weeks prior to the actual day). According to the variables shown in Table 1, in the form \"name D-i\" for data corresponding to i days before today, the input attribute set contains 553 items.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "initial vars. + (19 historical vars./day \u00d7 28 days) = 553 variables", "text": "Calendar data are not considered between historical variables, this is because the number of initial variables differs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Initial Features Selection and Fuzzification", "text": "In order to reduce the initial variable input space, PCA (Principal Components Analysis) [10] [11] has been used. The initial 553 characteristics were reduced to 295, considering 90% of explained variance and the values over the average of the eigenvectors.\nThe optimal fuzzy partition for each feature was calculated using KBCT [12]. This tool generates 3 different types of partitions for each variable: HFP [13] [14], Regular and Kmeans [15], with a [2,9] range for the number of different linguistic labels. The partition that got the best ratings was selected, according to the following indexes and criteria: minimizes Partition Entropy (PE) [16] and maximizes Partition Coefficient (PC) [16] and Chen index [17]. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FUDT Construction", "text": "The limits of inhomogeneity and segment size threshold are established experimentally for each problem. The lower limit is set at 4 elements, for a lower number segment could be considered irrelevant and subject to the effects of any possible outlier or noise. On the other hand, due to the total number of samples, over 30 items by segment means that the tree would collapse, do not creating any node other than root.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results of FUDT", "text": "Table shows the main results obtained with the FUDT algorithm using the selected fuzzy variables described in Subsection 3.2 above and the thresholds of Table 3. Columns correspond with the number of Leaf Nodes or terminals (LN) and Depth (D) of the tree in each experiment, while IT is for Inhomogeneity Threshold and SST for Segment Size Threshold. They are grouped into two blocks, showing quality measures depending on the structure of the trees: T, IQN-T and CI, and the quality indexes of the clusters: SC, S and XB.\nIn order to compare values from SC, S and XB, these values have been divided by the number of leaf nodes, due to the fact that each tree has a different number of nodes.\nIt has been found that beyond a certain inhomogeneity threshold level, the tree collapses, thus not creating any node other than the root node. For this reason, these values have not been considered and the study focuses on the really relevant cases. Therefore, the tree has been rebuilt by fixing a value for the inhomogeneity threshold and giving values to the segment size threshold. This avoids the influence of extreme values on the index normalization. Table 4 contains the renewed indexes and Figure 1 shows the table data graphically.\nRecall that the measure T gives a higher value on those trees that are more compact, reaching the maximum (T 1) when all the leaf nodes derivate directly  from the root. On the other hand, the value of IQN-T is maximum when the tree is fully expanded, when it reaches the minimum value of impurity. Since a too expanded tree can lose efficiency due to the effect of overfitting and too simple ones can have a very high classification error, the IC was suggested as a way to find a trade-off between both values. Since a low value for SC indicates a better partition, and the optimal number of clusters should minimize the value of the Xie and Beni index (XB ), from Figure 1(b), it is possible to deduce that the best trees are the most widespread since the first experiments have a smaller segment size threshold. As previously, these trees can suffer from weak generalization capability.\nThe maximum value for the IC (Figure 1(a)) is achieved by an inhomogeneity threshold of 0.40 and a segment size of 16. For this experiment, the value of SC and XB is reasonably low, so this seems a good candidate for further study. The schema below shows the internal structure of the selected tree.\nThe tree has 37 nodes, 22 of them are leaf or terminal nodes and the tree depth is 5. This means that at most there are 22 different classes that are described by rules generated from the root node to each of the leaves, which at most have 5 antecedents. These rules are compatible with readability-interpretability criteria, because 7 \u00b1 2 is considered in scientific literature as a human limit in order to achieve an efficient handling of the rules [18].\nThe most populated nodes are 26 and 30. Their descriptions, according to linguistic attributes, point out that the first one corresponds with soft temperature days, a low number of total calls distributed in a high number of calls related to Incidents but a low number of calls related to No Incidents. The second node however describes days characterized by high temperatures, a low number of calls, but equally distributed between Incidents and No Incidents.\nFuzzy UDT (IT: 0.40, SST: 16). ", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": ["tab_20", "tab_48"]}, {"heading": "Conclusions", "text": "A fuzzy version of the unsupervised decision tree algorithm has been considered in order to deal with the assignments of resources for a call center. The proposal is based on the characterization of day categories in order to determine the workload and resources needed for each category. Due to the weak expert knowledge available on this way of facing the problem, an unsupervised approach have been considered. These categories have to be described in linguistic terms on the domain concerned. A selection of variables has been carried out in two steps: using PCA to obtain a pre-selection and Fuzzy UDT, which decides the final variables to be taken into account. This two step methodology is mandatory due to the high number of initial variables considered and the unsupervised tree approach.\nIn order to obtain a reasonable tree, a hybrid solution based on indexes of tree and cluster quality have been used. The results obtained can be considered reasonable, and in future versions, they can be improved by the refinement of the criteria considered. An optimized version of the knowledge base could be developed, but the level of interpretability must be kept.\nThe developed system intends to help the managers of the emergency call center, establishing the base line for the normal operation of the system. Later, with some estimations and forecasts of certain variables, the system will characterize the desired day, which will help in the decision making of planning the resources assignment. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Wireless Sensor Networks (WSNs) [1] have become an enabling technology for a wide range of applications. The traditional WSN architecture consists of a large number of sensor nodes which are densely deployed over an area of interest. These nodes can be conceived as small computers, extremely basic in terms of their interfaces and their components [2], having limited battery, reduced memory and processing capabilities.\nOn the other hand, there are some tendencies to include artificial intelligent technologies in WSNs [3][4] [5], such as artificial neural network and fuzzy logic. However, few attentions have been paid to integrate Fuzzy Rule-Based Systems (FRBSs) into WSNs (embedded FRBSs). In [7] and [6,8], two schemes have been proposed for embedded FRBSs, applied to model fire detection and an agriculture plague respectively. These schemes do not describe the use of a human-machine interface neither a FRBS specifically adapted to sensor limitations. In order to make easy and efficient the integration of embedded FRBSs in WSNs, the structure proposed in this work includes an interface to make possible the knowledge base (KB) edition and obtaining operation results, and a FRBS designed to increase sensor performance. On the other hand, the use of an interface makes easy this work but it does not guarantee an optima KB generation. In addition, this paper introduces two additional modules proposed to optimize these KBs by means of rule redundancy reductions and rule adaptations to specific modeling problems.\nThe remainder of the paper is organized as follows. Section 2 presents a brief description of the distributed architecture proposed. Section 3 describes the functionality of each module included in this architecture. Results are reported in Section 4 and finally some conclusions are drawn in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distributed Architecture for Fuzzy Rule-Based System Embedded in Wireless Sensor Network", "text": "Basically, the factors that determinate the embedded FRBS performance are three: 1) the limitations associated with physical resources; 2) the ones related with an appropriated FBRS structuring and programming; 3) the associated with a KB right design to be used. An appropriate FBRS performance will decrease the consumption of energy system, what will increase the time of battery discharge. To optimize this performance, this work proposes the use of a FBRS with distributed functions placed in: 1) central computer, the tasks of KB edition and optimization; and 2) nodes into WSN, the modules of input scaling, fuzzyfication, inference engine which use the optimized KB, defuzzyfication, output scaling, and communication module (figure 2). The process of KB optimization proposed in this paper does not deal with the generation of an optima KB, rather it is concerned with the transformation of a given KB, in order to decrease the inference algorithm run time and the energy consumption, with a reasonable decrease in their accuracy. The KB optimization goal is to obtain a KB that allows the increase in the sensor global performance.\nThe proposed design, specially the inference engine, has been adapted to physical limitations of the used sensor: Sun SPOT [9] (180 MHz 32-bit ARM920T processor, 512K RAM and 802.15.4 radio).\nFigure 1 shows the general structure of the system, which is composed of: personal computer, base station gateway, communication protocol, and sensors in WSN.", "publication_ref": [], "figure_ref": ["fig_15", "fig_2"], "table_ref": []}, {"heading": "Fuzzy Rule-Based System Embedded in Sensor Modular Structure Description", "text": "The FRBS used in this paper is based on the model of Mamdani [10]. Two variants of Mamdani FRBSs have been proposed [11]: 1) descriptive and 2) approximate [12] [13] [14] [15]. The structures of these systems are similar, but each type of this FRBS has different properties and presents complementary advantages and drawbacks. In order to improve embedded FRBS behavior, it is possible to use these advantages.\nIn descriptive FBRS (or linguistic Mamdani FRBS), rules carry a linguistic label that points to a particular fuzzy set of a linguistic partition of the underlying linguistic variable. In approximate FBRS rules, the input variables and the output one are fuzzy variables instead of linguistic variables.\nApproximate FRBSs demonstrate some specific advantages over linguistic FRBSs making them particularly useful for certain types of applications [14]: 1) each rule employs its own distinct fuzzy sets, resulting in additional degrees of freedom and increase in expressiveness and 2) the number of rules can be adapted to the complexity of the problem. These properties enable approximate FRBSs to achieve a better degree of accuracy than linguistic FRBS in complex problem domains. In descriptive FRBSs, the main advantage is the large degree of interpretability of linguistic rules [16] [17] [18] [19].\nIn this paper, we propose the use of a visual interface as a method of KB composition. Using this interface, the human expert can specify the linguistic labels associated to each linguistic variable, the structure of the rules in the rule base (RB), and the meaning of each label. This method is the simplest one to be applied when the expert is able to express his knowledge in the form of linguistic rules. To improve the accuracy of this linguistic approach, this interface enables the specification of exact membership functions as well.\nIn the process of KB optimization it is necessary to avoid redundant rules. These If-Then rules have the property that a system state is covered by more than one rule as the fuzzy sets in the antecedents overlap. The existence of redundant rules may cause degradation in the performance of the FBRS. Therefore, it is important to evaluate the utility of a rule, by analyzing its impact on the global system behavior, and then decide whether a rule should be discarded from the rule set.\nIn the approach proposed in this paper, after the KB is composed, a module is used to reduce rules redundancy. Firstly, its algorithm searches rules with overlap in theirs antecedent and consequent; secondly, it makes groups of rules with only one difference in the same proposition of the antecedent; and thirdly, for each one of these groups, it selects the rules with adjacent fuzzy sets in this proposition. These redundant rules can be simplified in only one rule. The new rule will be the same as the old rules except in their different proposition, and now, their new fuzzy set will be built by the composition of the involved adjacent fuzzy sets. A new KB is made with reduction of redundant rules.\nA reduction of rules can produce lack of accuracy in modeling. In order to evaluate the accuracy of a new KB, this paper proposes the use of an algorithm that contains the following steps: first, for a wide set of system states, it obtains the output of FRBS, using two KB: 1) the original (output 1) and 2) the reduced one (output 2); second, for each FRBS output it determines the absolute value of the subtraction (output 1 -output 2); and third it sums these absolute errors. In the interface, the last module enables the analysis of this error, to decide if the reduced KB achieves the target of accuracy.\nIn order to make the most of approximate and descriptive FRBS advantages, this approach enables the use of a visual interface to make easy the composition of the descriptive and approximate KBs, while the inference engine, of the embedded FRBS, works as the approximate Mamdani-type. Therefore, previously at the KB transmission, it is necessary the transformation of the descriptive KB into an approximate one.\nIn order to reduce the computational burden this paper proposes the use of FRBS in mode B-FITA (First Infer, Then Aggregate) and the operator centre of gravity. In the Sun SPOT sensor, the embedded inference engine has been programmed in Java using the J2ME platform.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "In order to illustrate the proposed methodology, three experiments (Ex) have been performed to model, with simple KBs, the behavior of two plagues of the olive tree: the Prays (Prays oleae bern) and Repilo (Spilocaea oleagina). The life cycles of these plagues depend on humidity and temperature, but they are not the same. The common steps of each experiment consist of: KB generation in visual interface, rule transformations, KB transmission, modeling a wide set of system states (4000) into FRBS embedded, and transmission of results. Furthermore, each experiment has a specific part. In Ex 1, it has been used an approximate KB which is composed of a RB with two groups of rules (GR) (table 1 and 2) and theirs associated specific fuzzy set (FS) definitions (Fig 3 and 4).\nIn Ex 2, it has been used an approximate KB which is composed of a RB with two GR (table 3 and 4) and theirs associated specific FS definitions (Fig 6 and 7). In this case, the rules and FS definitions have been obtained using the redundancy reduction module. As can be observed this algorithm decreases six rules (table 3 and 4) and generates, by means of fusion, three new FSs (figure 6 and 7).\nWith the purpose of illustrating the redundancy reduction process, an example of redundant rule reduction is shown. In the group of rules used to model the Prays (table 1), it can be noted a set of three rules with the same consequent and the same second proposition in their antecedent: Applying the proposed algorithm these original rules can be simplified in only one rule (New R1). New R1: If Humidity is S&M&L and Temperature is VS Then Prays Alert is VS Except for the first proposition in the antecedent, the rule \"New R1\" is equal to the three originals. Now, the FS associated with the \"Humidity\" variable is \"S&M&L\" (figure 6), which has been obtained by means of composition of the three original FS (figure 3). In Ex 3, it has been used a descriptive KB which is composed of a RB with two GRs (table 1 and 2) and the definition of linguistic labels associated to each linguistic variable (Fig 5). In this case, the use of linguistic labels prevents specific FS definitions for each GR. Once the three experiments have been made, the following parameters have been calculated: accuracy of plague modeling, run time (inference rate) and battery consumption (charge of battery) performances (table 5).\nIn table 5, the accuracy parameter has been obtained by means of division of absolute error addition (in Prays and Repilo output) by the sum of descriptive FRBS Prays and Repilo output. As can be observed in table 5: a) the use of specific approximate FRBS to model the Prays and Repilo olive plagues provides an improvement in accuracy,  compared with the use of descriptive FRBS; b) the use of specific approximate FRBS, with reduced KB, provides an improvement in the run time (or inference rate) and the battery consumption (or time of battery discharge), compared with the use of specific approximate FRBS. From the analysis of the experimental results obtained, we notice that, it has been possible to compose an optimized KB, adapted to model olive plagues, which generates a decrease in FRBS run time (increase the inference rate) and a decrease in battery consumption (decrease the time of discharge in battery sensor), without decreasing the accuracy.", "publication_ref": [], "figure_ref": ["fig_28", "fig_53", "fig_53", "fig_117", "fig_28", "fig_116"], "table_ref": ["tab_2", "tab_20", "tab_20", "tab_2", "tab_2", "tab_56"]}, {"heading": "Conclusions", "text": "In this paper, we have presented a distributed structure of FRBS embedded in WSN which incorporates: a) a visual interface to make easy the composition of approximate and descriptive KBs; b) a module to reduce rule redundancy; c) a module to transform descriptive into approximate KBs; d) a communication protocol; and e) an approximate FRBS adapted to be executed in a sensor. Results have shown the effectiveness of the proposed structure to optimize the execution of FRBS embedded into a sensor.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The popularity of fuzzy controllers over the last years is highly due to its main capabilities: They avoid the need of accurate mathematical models of the systems under control and they make it possible to apply human expert knowledge about the operation of such systems in order to design a proper controller [1]; besides, it has been proved that they are universal approximators (i.e. they are able to approximate any continuous function in a compact set to any desired accuracy) [2]. The design of a fuzzy controller is not always an easy task [3]. Adhoc fuzzy controllers can be designed for experts when the plant under control is well known [4]. On the other hand, intelligent and automatic methods are needed when the knowledge about the plant is more reduced. The literature shows several methods to adapt the controller's parameters online (i.e. consequents of the rules and/or membership functions) [5] or even their topology [3,1]. However, the aforementioned methods rely on assumptions made about the plant's equations (e.g. certain bounds are supposed to be known), so their application is not always possible. Therefore, the most challenging case is being unable to make such assumptions. This problem has been solved offline, with algorithms based on pretraining with I/O data [6]. The online adaptation of the controller's parameters for fixed topologies has been addressed as well [7]. However, little has been written about the online self-organization of the fuzzy controller when no prior knowledge about the plant is available [8].\nIn this work we present a method for the online self-organization of the topology of a fuzzy controller, together with the adaptation of the rule consequents. Both types of adaptation are performed while the controller is working, providing better tolerance to noise and robustness under changes in the plant's dynamics. The algorithm is based in the property of universal approximation of fuzzy controllers, which states that the proper addition of membership functions makes possible to reach a desired accuracy level for a functional approximation. To illustrate its capabilities, simulation results are provided.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online Self-organization of Fuzzy Controllers", "text": "The mathematical representation of the plant to be controlled may be expressed in terms of its differential equations or by its difference equations, provided that these are obtained from the former using a short enough sampling period:\ny(k + 1) = f (y(k), ..., y(k \u2212 p), u(k), ..., u(k \u2212 q)) (1)\nwhere y(k) is the plant output at time k, f is an unknown continuous and differentiable function, u is the control input and p and q are constants which determine the order of the plant. The restriction usually imposed to these plants is that they must be controllable [7], i.e. \u2202f \u2202u = 0 for every input within the operation range. Hence, the mentioned derivative must have a constant sign [3,7]. The aim of the controller is to guarantee that the plant's output tracks a given reference signal, r(k). Thus, in the absence of actuator bounds, we can assume that there exists a function F such that the control input given by\nu(k) = F (x(k)) (2)\nwith x(k) = (r(k), y(k), ..., y(k \u2212 p), u(k \u2212 1), ..., u(k \u2212 q), is capable of reaching the set point in the following instant: y(k + 1) = r(k). In order to approximate such function we employ a 0-order TSK fuzzy system with a complete set of rules that are defined as\nIF x 1 is X i1 1 AND x 2 is X i2 2 AND...x N is X iN N THEN u = R i1i2...iN (3)\nwhere\nX iv v \u2208 {X 1 v , X 2 v , ..., X nv v } are the membership functions of input X v , n v\nis the number of membership functions for that variable and R i1i2...iN is a numerical value representing the rule consequent. The characteristics of the fuzzy system are: triangular membership functions (MF), product as T-norm for the inference method and weighted average for defuzzification. Thus, the fuzzy controller's output is given by:\nu(k) =F (x(k); \u0398) = n1 i1=1 n2 i2=1 ... nN iN =1 R i1i2...iN \u2022 N m=1 \u03bc X im m (x m (k)) n1 i1=1 n2 i2=1 ... nN iN =1 N m=1 \u03bc X im m (x m (k))(4)\nwhere \u03bc X im m is the activation degree of membership function i m of the input X m and \u0398 is the set of parameters of the fuzzy system. These parameters include the consequents of the rules (R i1i2...iN ), the number of membership functions for each input (n v with v = 1...N ) and the centers of the membership functions (\u03b8 j v , where v denotes the input and j is the order of this MF within the set of membership functions, supposing that they are sorted in increasing center order).\nThe proposed algorithm works in two stages: First, the consequents of the existing rules are adapted with the aim of reducing the plant's output error. When this does not provide any further improvement in the control performance, the topology of the controller is changed by adding a new membership function and creating the corresponding rules (second stage). The consequents of the new rules also need to be adapted, so the algorithm switches back to the first stage. The algorithm does not need any information about the plant's equations or their bounds. Furthermor, it can start working from very simple topologies, even empty ones, which avoids the need of prunning rules. Although its execution never finishes, the second phase is stopped when the desired accuracy level is achieved; this avoids an excessive growth of the number of rules. To measure the control performance and decide when to switch to the second phase, the mean square error (MSE) between the reference signal and the plant output is used [9]. The following sections describe in detail both stages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stage One: Adaptation of the Rule Consequents", "text": "In this stage, the controller learns which actions lead to the stabilization of the plant by modifying the consequents of the fuzzy rules. Applying a gradientdescent technique is not feasible, as it requires to compute the partial derivative \u2202y/\u2202u and our initial hypothesis is that the differential equations that govern the plant are unknown. Nevertheless, as the plant must be controllable, its derivative has a definite constant sign. Hence we can use the information regarding the plant monotonicity with respect to the control input to obtain the right direction in which to move the rule consequents [7,9]. Therefore, our adaptation process consists on the proposal of a correction to the consequents based on the evaluation of the current state of the plant. If the aforementioned monotonicity is positive (i.e. the plant's output grows as the control input grows) and at time k + 1 the plant's output is larger than desired (y(k + 1) > r(k)), it means that the control signal applied in time k should have been lower; likewise, if y(k + 1) < r(k), the control signal should have been larger. In the case of negative monotonicity, the only difference is that the direction of the changes has to be swapped. Since each rule has a different degree of responsibility on the plant's current state, the penalty applied to each of them is proportional to their degree of activation when obtaining the control input u(k). Therefore, the modification applied to the consequent of the i-th rule at time k is given by:\n\u0394R i (k) = C \u2022 \u03bc i (k \u2212 1) \u2022 e y (k) = C \u2022 \u03bc i (k \u2212 1) \u2022 (r(k \u2212 1) \u2212 y(k)) (5)\nwhere \u03bc i (k \u2212 1) is the activation degree of the i-th rule at instant k \u2212 1, r(k \u2212 1) is the set point at that time, y(k) is the current system's output and C is a normalization constant with the same sign as the monotonicity of the plant with respect to u and whose absolute value can be set offline as |C| = \u0394u/\u0394r, where \u0394u is the range of the controller's actuator and \u0394r is the range in which the reference signal varies (they are both a priori known). Note that the expression (5) uses the reference value at instant k \u2212 1 instead of its current value. The reason for this is that the rules activated at instant k \u2212 1 served to reach the desired value r(k \u2212 1) and not r(k).\nOn the other hand, most real life controllers have limitations on their operation and this affects the control process. For instance, if the actuator is only able to operate within the range [u min , u max ] and at a given moment the optimal control input is u(k) > u max , the input finally applied to the plant will be u max , so it will not be possible to reach the desired set point at the next time step. However, we cannot penalize the rules, as they are already giving the best possible answer. To solve this inconvenience, we ensure that no penalty is applied to rules because of the actuator's limitations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stage Two: Modification of the Topology", "text": "Most of the adaptive fuzzy controllers proposed at present use fixed structures that are defined beforehand [5]. However, when the plant's dynamics are unknown, choosing the proper topology is not a trivial task [7,3]. In this case, controllers capable of designing their own structure are needed. To tackle this problem online, information about all the operating regions of the plant needs to be gathered during the controller's normal operation. Most authors [8,10] modify the controller's structure with every new incoming training data, which makes the topology dependable on the sequence of control actions performed. To overcome this, we consider the full operating region; this way, the robustness of the method is increased.\nThe method proposed here exploits the fact that the very operation of the system provides input/output (I/O) data about the true inverse function of the plant to be controlled. Hence, if the control input u(k) produces the plant's output y(k +1), the output error is not the only information we have. Regardless whether y(k + 1) is the desired output or not, we know that if in the same state we find the reference value r(k ) = y(k + 1) again, the optimal control input will be precisely u(k). With the aim of using this information towards the selforganization of the fuzzy controller, we store the I/O data provided by the plant in a memory M . This information is later used to decide which input needs most a new membership function.\nTwo steps are performed to modify the fuzzy controller's structure: First, the controller's most relevant input is chosen based on the degree of responsibility that each input has on the approximation error. Then, the new membership function is added and the parameters of the resulting fuzzy controller are initialized with the aim of minimizing the degradation of the performance right after the change. This two steps are explained next:\nStep 1: Selection of the Controller's Most Relevant Input. The simplest way to modify the controller's topology is by adding a new MF to every input variable [11]. However, the number of rules depends exponentially on the number of membership functions, so this option is not feasible from a practical point of view [3]. To overcome this problem, it is recommended to choose carefully which variable is going to receive new membership functions. In our case, we base this election on the analysis of the complete error surface reached by the current configuration [12]. Although other methods consider only the point of maximum error, this makes the method more sensible to noise [3].\nThe main idea behind the election of the \"most important\" input is to analyze every one of them separately to check their degree of responsibility on the current approximation error. Let us assume we are analyzing the input x v ; in this case, we assign a large number of membership functions (N \u221e ) to all the other inputs. This is equivalent to having a perfect approximation in those dimensions and, therefore, only x v is responsible for the approximation error.\nLet F \u221e be the approximation of the data stored in memory M produced by the fuzzy system with the mentioned topology. In this case, we can compute the responsibility index for the input x v (RI v ) as:\nRI v = K i=1 (u M (x M i ) \u2212 F v \u221e (x M i )) 2 (6\n)\nwhere K is the number of data stored in memory M , x M i is the i-th input vector stored in memory M , and u M (x i ) is the output produced by that input.\nFinally, the input variable with the largest responsibility index is the one selected to receive a new membership function: its number of membership functions is increased by one and the functions are equidistributed through all the variable's range of operation. The addition of this new MF implies the creation of new rules. The second step of this phase handles the initialization of the consequents for the resulting fuzzy controller.\nStep 2: Initialization of the Rules of the New Fuzzy Controller. Since we are using a complete set of rules, every time a new MF is added to an input\nx v , N i=1 i =v n i new fuzzy rules are created.\nAlthough it is possible to initialize the new consequents to random values and let them be adapted by the algorithm's first phase, this would cause a sudden decrease on the control quality in the first moments after the topology modification. To avoid this situation, we initialize the new rules to values that guarantee minimum quality degradation.\nLet\u0398 be the set of parameters before the new MF was added and \u0398 the new set of parameters. The idea is that the outline of the global function represented by the fuzzy system is kept the same as before, i.e.F (x; \u0398) =F (x;\u0398)\u2200x.\nTo achieve this, we impose that at the point of maximum activation of the rule, the consequent equals the output produced by the system under its previous configuration for the same input. As the maximum activation degree is reached when all the inputs are located at the centres of the membership functions of the antecedent, we have that: R i1,...,iv,...,iN =F (c;\u0398) ( 7) where c = (\u03b8 i1 1 , ..., \u03b8 iv v , ..., \u03b8 iN N ), with i 1 = 1, ..., n 1 , ..., i v = 1, ..., n v , ... i N = 1, ..., n N (i.e. all the rules have to update their consequents).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simulation Results", "text": "To provide a better insight of the algorithm simulation results are provided in this section. Let us consider the plant commonly known as water tank (see Figure 1). This plant represents a tank that has a valve allowing the introduction of liquid, and a vent in its lower side that lets the liquid out. The control objective is to adjust the power of the water entrance so as to achieve a determined height H in the water level. The amount of water that gets in the tank is proportional to the voltage applied to the entrance valve, whilst the water that flows out is proportional to the square root of the level reached by the liquid inside. It is obvious that the amount of water in the tank is equal to the difference between the amount of water getting in and the amount of water getting out. Thus, the differential equation that defines the water level stored in the tank at any time is the following [13]:\ndV ol dt = A dH dt = bV \u2212 a \u221a H (8)\nwhere V ol is the water volume inside the tank, A is the area of the transversal section of the tank, b is a constant associated to the water entrance rate, a is a constant related to the water exit rate and H is the water level inside the tank at a specific moment in time. Despite the formula's simplicity, the presence of a square root makes the differential equation to be non-linear, thus making its analysis difficult. This system also has the peculiarity of giving the actuator a limited range of operation, because negative signals cannot be applied. If the water level has to decrease, the only possible action is to stop letting water in the tank and wait for the liquid to flow out of the tank.\nFor this example we have used a fuzzy controller with two inputs (the reference signal r(k) and the plant's output y(k)). Initially, the controller is empty, which means that every input variable has only one membership function and the only rule is initialized to zero. For the simulation, a reference signal composed by several random step functions within the range [0.5, 4] is used, forming a 1000iterations long pattern. The sampling rate is 10 samples per second. Parameters for the used plant are: A = 10, a = 1, b = 2.5. The actuator operates within the range [0,5].\nTable 1 shows the evolution of the self-organization process, with each row representing a topology change. For each of them we show the new topology obtained, the values of the indexes SSE i used to decide the next change and the mean square error reached for the given topology after the adaptation of the new consequents. Note that the values of the MSE have been multiplied by 10 3 in order to make them more readable. The first two changes set the topology 2x2, which means that both variables are relevant for the control process. After that, the importance of both inputs is kept similar, as we reach an even topology (4x4). However, in the end, r(k) becomes more important, as it is observed from the fact that it receives six MFs, compared to the four assigned to y(k). This means that, in order to achieve a better accuracy, the control policy has to be finer for the values of r(k).\nOn the other hand, Figure 2 compares the plant's output with the desired reference signal along different moments of the execution. Figure 2(a) shows how the empty controller at the beginning of the execution is incapable of controlling the system properly. This is because it does not have any information about the plant and we are starting working with an empty controller. However, after a  few minutes the control starts improving, as the algorithm learns, until reaching a very good performance before one hour elapses (Figure 2(b)). Note that the biggest errors in the tracking process are due to the limitation of the actuator: in Figure 2(b) it can be observed that the decrease of the water level is slower than the increase. As mentioned before, this happens because the only possible action for lowering the liquid level is closing the valve.\nFinally, we analyze the robustness of the algorithm against unexpected changes in the plant being controlled. The proposed method does not use any specific information or makes any hypotheses about the system it controls, except for the sign of the monotonicity with respect to the control signal u. However it uses the I/O data collected from the system operation itself, and therefore if an internal change in the plant happens, the information supplied will allow the adaptation of the approximation to the new characteristics.\nIn order to visualize this property, the value of parameter b in (8) has been reduced during execution. This induces the applied voltage V to produce a lower water entrance rate, which is equivalent to a decay on the actuator's performance due to its use. Initially, plant parameters are A = 10, a = 1, b = 10. At instant 5000, the value of b is reduced to half its initial value. In Figure 3(a) the effect of this change over the tracking of the reference signal can be observed: a clear deterioration in the performance of the control takes place. Nevertheless, the algorithm starts counteracting this deterioration from the beginning to such an extent that after some minutes the control is as good as it was before the change in the plant dynamics (Figure 3(b)).", "publication_ref": [], "figure_ref": ["fig_2", "fig_15", "fig_15", "fig_15", "fig_15", "fig_28", "fig_28"], "table_ref": ["tab_2"]}, {"heading": "Conclusions", "text": "In this work, we have proposed an online adaptive self-organizing fuzzy controller. Without any offline pretraining this system is capable to adapt both the rule consequents and the system topology online, based on I/O data obtained from the plant during the system's normal operation. It is also able to determine which variables need more membership functions and where to locate such functions in order to improve the control performance. The simulations with a mechanic suspension system have shown its capability to perform a high-quality control even if starting from an empty configuration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Output constraints often decide on safety and/or economic efficiency of the process. In the case when the control action must be generated frequently and, therefore, relatively simple analytical controllers (with explicit control law) are used, the constraint handling mechanism must be as simple as possible, though efficient. The method proposed in the paper has these features.\nIn the proposed method the prediction generation known from the MPC algorithms is used; see e.g. [1,3,7,9]. Then the control action which was generated by a controller is modified in such a way that the predicted output does not violate the constraints. It should be stressed, that, unlike in other methods designed for analytical controllers [5], in the proposed approach the predicted output many sampling instants ahead is taken into consideration during constraint handling. Moreover, the modeling inaccuracy can be easily taken into consideration in the proposed method. Very good performance offered by the method is demonstrated in the example control system of a nonlinear control plant with delay.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mechanism of Output Constraint Handling", "text": "It is assumed that the control plant is described by the Takagi-Sugeno model with local models in the form of difference equations:\nRule f : (1)\nif y k is B f 1 and . . . and y k\u2212n+1 is B f n and u k is C f 1 and . . . and\nu k\u2212m+1 is C f m then y f k+1 = b 1,f \u2022 y k + . . . + b n,f \u2022 y k\u2212n+1 + c 1,f \u2022 u k + . . . + c m,f \u2022 u k\u2212m+1 ,\nwhere b 1,f , . . . , b n,f , c 1,f , . . . , c m,f are coefficients of the f th local (linear) model, y k is the value of the output variable at the k th sampling instant, u k is the manipulated variable value at the k th sampling instant,\nB f 1 , . . . , B f n , C f 1 , . . . , C f m\nare fuzzy sets, f = 1, . . . , l, l is the number of fuzzy rules.\nIn general, output of the fuzzy model is described by the following formula:\ny k+1 = n j=1 b j k \u2022 y k\u2212j+1 + m j=1 c j k \u2022 u k\u2212j+1 , (2\n)\nwhere b j k = l f =1 w f k \u2022 b j,f , c j k = l f =1 w f k \u2022 c j,f\n, w f k are normalized weights obtained using the fuzzy reasoning (see e.g. [6,8]); they depend, in general, on past output and control values, however only their dependence on time is denoted for brevity and clarity of description.\nAssuming that the control action will not change (u k = u k+1 = ...), behavior of the control plant can be predicted many sampling instants ahead using iterative method, i.e. iteratively using the fuzzy model (2). Thus, the predicted output values are described by the following formula:\ny k+i = i\u22121 j=1 b j k+i \u2022 y k\u2212j+i + n j=i b j k+i \u2022y k\u2212j+i + i j=1 c j k+i \u2022u k + m j=i+1 c j k+i \u2022u k\u2212j+i , (3)\nwhere y k+i is the output of the fuzzy model for the (k + i) th sampling instant.\nMoreover, it is advisable to assess and take into consideration the modeling inaccuracy and influence of unmeasured disturbances. If one does not have the disturbance and modeling error estimates, one can adapt a mechanism used in the MPC algorithms. It consists in calculating difference between measured output and output of the model\nd k = y k \u2212 y k . (4\n)\nThen it is assumed that d k will be the same in the next sampling instants. Therefore, finally, one obtains the following prediction:\ny k+i|k = i j=1 c j k+i \u2022u k + m j=i+1 c j k+i \u2022u k\u2212j+i + i\u22121 j=1 b j k+i \u2022 y k\u2212j+i + n j=i b j k+i \u2022y k\u2212j+i +d k .\n(5) The prediction (5) can be written in more compact form as:\ny k+i|k = C k+i \u2022 u k + D k+i , (6\n)\nwhere\nD k+i = m j=i+1 c j k+i \u2022u k\u2212j+i + i\u22121 j=1 b j k+i \u2022 y k\u2212j+i + n j=i b j k+i \u2022y k\u2212j+i +d k , C k+i = i j=1 c j\nk+i . Usually, one demands that output value does not violate the limits which do not change in time. Therefore, the output constraints which should be fulfilled are as follows:\ny min \u2264 y k+i|k \u2264 y max , (7\n)\nwhere y min and y max are lower and upper output limits, respectively. Thanks to using the prediction ( 6) the output constraints can be applied many samplings instants ahead:\ny min \u2264 C k+i \u2022 u k + D k+i \u2264 y max . (8\n)\nThe output constraints ( 8) can be transformed into sets of constraints put on the currently derived control value for lower constraints:\nC k+i \u2022 u k \u2265 y min \u2212 D k+i (9)\nand for upper constraints:\nC k+i \u2022 u k \u2264 y max \u2212 D k+i . (10\n)\nNext, the following rules of control value modification should be applied:\nfor lower constraints:\n-if C k+i \u2022 u k \u2265 y min \u2212 D k+i then u k = ymin\u2212D k+i C k+i and -for upper constraints: -if C k+i \u2022 u k \u2264 y max \u2212 D k+i then u k = ymax\u2212D k+i C k+i . Remark 1.\nThe key issue is to use the modified control values (actually applied to the plant) in the next iterations during control signal calculation by the controller. Otherwise, control performance may be degraded.\nRemark 2. The parameters b j k+i and c j k+i may depend, in general, on u k and future output values. Therefore, the method above is an approximate one. However, if a problem needs improvement of constraint satisfaction, the modification of control value u k may be repeated a few times, iteratively, in order to correct the result.\nRemark 3. The calculations in the proposed mechanism are not too complicated. Moreover, one can decide how many sampling instants ahead output of the control plant will be predicted and constrained. Thus, the mechanism of constraint handling may be easily tailored to the particular problem.\nRemark 4. Analytical controllers equipped with the proposed mechanism can be applied in constrained control systems for which other types of controllers (e.g. numerical MPC algorithms based on optimization problem solved at each iteration of the algorithm, often used in constrained control systems) are too complex and too time consuming.\nIn the proposed method a basic mechanism of modeling inaccuracy assessment known from MPC algorithms may be used, as discussed earlier. However, if one can assess the values of modeling errors, this assessment can be used to introduce a safety margin to the output constraints. The better the assessment of the modeling error, the better results will be obtained.\nThe output prediction with uncertainty can be described by:\ny k+i|k = y k+i|k + r k+i|k = C k+i \u2022 u k + D k+i + r k+i|k , (11\n)\nwhere components r k+i|k represent influence of the modeling error on the prediction; they are usually unknown. If the following assessment of the minimum and maximum values of the r k+i|k was done:\nr min k+i|k \u2264 r k+i|k \u2264 r max k+i|k , (12\n)\nwhere r min k+i|k \u2264 0 and r max k+i|k \u2265 0, then the following rules of control value modification should be applied:\nfor lower constraints:\n-\nif C k+i \u2022 u k \u2265 y min \u2212 D k+i \u2212 r min k+i|k then u k = ymin\u2212D k+i \u2212r min k+i|k C k+i and -for upper constraints: -if C k+i \u2022 u k \u2264 y max \u2212 D k+i \u2212 r max k+i|k then u k = ymax\u2212D k+i \u2212r max k+i|k C k+i .\nRemark 5. In practice, the further in the future a predicted value of the output variable is, the more difficult (and more conservative) the assessment of the modeling error usually is. Therefore, in practice, one can apply the control value modification rules with the same modeling error assessments for all future sampling instants (r min k+i|k = r min and r max k+i|k = r max ) where r min and r max may be equal, e.g. to the values obtained for the (k + 1) st sampling instant, i.e. r min = r min k+1|k and r max = r max k+1|k . The other solution is to resign from taking the modeling uncertainty into consideration in the further, than assumed, sampling instants. The prediction and constraint handling mechanism can be applied in the next iteration of the controller, with updated modeling inaccuracy assessment, anyway.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simulation Experiments", "text": "The proposed mechanism is tested in the control system of a distillation columna nonlinear plant with delay. The control plant is described by the fuzzy Takagi-Sugeno model which consists of three following rules (the sampling time T s = 40 min was assumed):\nRule\nf : if u k\u22122 is M f , then y f k+1 = b f \u2022 y k + c f \u2022 u k\u22122 + d f , (13\n)\nwhere The output variable y is the impurity of the product (counted in ppm). It is assumed that the output is constrained y \u2264 400 ppm due to quality demand. The allowable impurity cannot be exceeded, otherwise the product will be wasted. The manipulated variable u is the reflux to product ratio (the higher it is the purer product is obtained).\nf = 1,\nThe fuzzy model was used to design an analytical fuzzy DMC (FDMC) controller (see e.g. [4,9]) and in the constraint handling mechanism. In order to test the proposed approach in presence of modeling inaccuracy, the second model, of Hammerstein structure [2] with polynomial model of the statics, served as the control plant during simulation experiments. Only the basic mechanism of modeling inaccuracy assessment was used.\nThe example responses are shown in Fig. 2. The set-point value is set to y = 395 ppm. If the mechanism of output constraint handling is not applied then the constraint is violated (dash-dotted lines in Fig. 2). Application of the approach in which only one (next) predicted output value is constrained brings fulfillment of the constraint but the output is oscillating and the response is not smooth (dotted lines in Fig. 2). Application of the proposed approach for next three predicted output values gives the best result (solid lines in Fig. 2).\nThe constraint is fulfilled and the output achieves the set-point value fast and practically without overshoot.\nUsage of the proposed mechanism improved operation of the controller and made the output signal smoother than in the case when only one predicted output value (for the next sampling instant) was taken into consideration. The reason of that can be observed in the control signal. The controller began to take the constraint into consideration (and change the control signal comparing to the case without any constraint handling mechanism) earlier than in the case when only one predicted output value was constrained.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15", "fig_15", "fig_15"], "table_ref": []}, {"heading": "Summary", "text": "The effective and easy to use mechanism of output constraint handling is proposed in the paper. Thanks to the usage of process behavior prediction, known from the MPC approach, the constraints many sampling instants ahead can be taken into consideration. Therefore, the control signal can be modified in advance what can ensure fulfillment of constraints and improve performance of the control system.\nThe proposed method, though based on the MPC approach, can be applied to any analytical controller. Moreover, it can be easily adapted to the case when Takagi-Sugeno fuzzy models other than considered in the paper are used, e.g. with state-space equations or step responses utilized as the local models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Fuzzy modeling usually tries to improve the accuracy of the system without inclusion of any interpretability measure, an essential aspect of Fuzzy Rule-Based Systems (FRBSs). However, the problem of finding the right trade-off between accuracy and interpretability has achieved a growing interest [1].\nMany authors improve the trade-off between accuracy and interpretability of FRBSs, obtaining linguistic models not only accurate also interpretable. We can distinguish two kinds of approaches for managing the interpretability:\n1. Measuring the complexity of the model [2,3,4] (usually measured as number of rules, variables, labels per rule, etc.). 2. Measuring the interpretability of the fuzzy partitions [3,5,6,7] by means of a semantic interpretability measure. Focusing on the second type, a semantic interpretability index has been proposed in [7] for preserving the interpretability while a tuning of the Membership Functions (MFs) is performed. The tuning of MFs enhances the performance of FRBSs and consists of refining the parameters that identify the MFs associated to the labels comprising the Data Base (DB) [2,8,9]. The proposed index is used as an additional measure that quantify the interpretability of the tuned DB. While in the previous works constraints [6] or absolute measures [5] are used, in [7] a relative interpretability index allows to maintain the interpretability of the original MFs that could be given by an expert, by means of the aggregation of several metrics while a tuning is performed.\nIn this work, we analyze the performance of the combination of the tuning based on the semantic interpretability index together with a rule selection method to reduce the model complexity. The application of Multi-Objective Evolutionary Algorithms (MOEAs) [10,11] allows to obtain a set of solutions with different degrees of accuracy and interpretability [2,4,5,7]. We use an extension of the MOEA proposed in [7] in order to perform a rule selection together with the tuning of MFs with the following three objectives:\nmaximization of the semantic interpretability index minimization of the number of rules minimization of the system error This algorithm is based on SP EA2 [12] and is called T S SP 2\u2212SI (Tuning and Selection by SPEA2 for Semantic Interpretability). The combination of the tuning and the rule selection in the same process allows to obtain precise models with an important reduction in the number of rules [13]. In order to analyze this method, it is compared to a single objective accuracy-guided algorithm [13] for tuning and rule selection and to the tuning based on the semantic index in [7]. Two real-world problems have been considered showing that the solutions of the previous approaches are dominated by those obtained by T S SP 2\u2212SI .\nIn order to do that, section 2 presents the index to measure the semantic interpretability. Section 3 presents the T S SP 2\u2212SI algorithm for the tuning of MFs and rule selection. Section 4 analyzes the combined action of rule selection and tuning in terms of the accuracy, complexity and semantic interpretability of the obtained models. Finally, section 5 points out some conclusions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Semantic Interpretability Index", "text": "In this section, we describe several metrics that are combined in a index to measure the interpretability when a tuning is performed on the DB. These metrics were proposed in [7], except one of them that will be generalized here. In order to ensure the semantics integrity through the MFs optimization process [1,6], some researchers have proposed several properties. Considering one or more of these properties several constraints can be applied in the design process in order to obtain a DB maintaining the linguistic model comprehensibility to the higher possible level [14,15].  In order to maintain the semantic integrity we consider also these constraints by defining the variation intervals for each MF parameter. Even though we could consider other types of fuzzy partitions, in this work, we use strong fuzzy partitions with triangular MFs defined by means of three parameters (See Figure 1). For each M F j = (a j , b j , c j ) where j=(1,...m) and m is the number of MFs in a given DB, the variation intervals are calculated in the following way:\nThe metrics in [7] are based on the existence of these variation intervals (integrity constraints). These metrics allow to measure certain characteristics of tuned MFs regarding the original ones. The index and metrics have been proposed for triangular MFs, but they can be easily extended with some small changes in the formulation to Gaussian or trapezoidal. In this work, the metric \u03b3 is a generalization that transforms it into a relative metric. These metrics are: )/2 represents the maximum variation for each central parameter. Thus \u03b4 * is defined as \u03b4 * = max j {\u03b4 j } . The \u03b4 * metric takes values between 0 and 1, therefore values near to 1 show that the MFs present a great displacement. The following transformation is made so that this metric represents proximity (maximization):\nMaximize \u03b4 = 1 \u2212 \u03b4 * .\n(1)", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "MFs Lateral Amplitude Rate Measure (\u03b3)", "text": "This metric can be used to control the MF shapes. It is based on relating the left and right parts of the support of the original and the tuned MFs. Let us define leftS j = |a j \u2212 b j | as the amplitude of the left part of the original MF support and rightS j = |b j \u2212 c j | as the right part amplitude. Let us define leftS j = |a j \u2212 b j | and rightS j = |b j \u2212 c j | as the corresponding parts in the tuned MFs. \u03b3 j is calculated using the following equation for each M F : \u03b3j = min{lef tSj /rightSj , leftS j /rightS j } max{lef tSj/rightSj , leftS j /rightS j } .\n(\nValues near to 1 mean that the left and right rate in the original MFs are highly maintained in the tuned MFs. Finally \u03b3 is calculated by obtaining the minimum value of \u03b3 j :\nMaximize \u03b3 = minj {\u03b3j } .\n(3)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MFs Area Similarity Measure (\u03c1)", "text": "This metric can be used to control the area of the MF shapes. It is based on relating the areas of the original and the tuned MFs. Let us define A j as the area of the triangle representing the original M F j , and A j as the new area. \u03c1 j is calculated using the following equation for each M F :\n\u03c1j = min{Aj , A j } max{Aj, A j } . (4\n)\nValues near to 1 mean that the original area and the tuned area of the MFs are more similar (less changes). The \u03c1 metric is calculated by obtaining the minimum value of \u03c1 j :\nMaximize \u03c1 = minj {\u03c1j} .\n(5)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantics Based Interpretability Index: Gm3m", "text": "The semantic interpretability index, namely Gm3m proposed in [7], is defined as the geometric mean of the three metrics. The geometric mean is used because in case that only one of the metrics has very low values (causing low interpretability) it is also obtained small values of GM3M. The index is defined as:\nMaximize GM 3M = 3 \u03b4 \u2022 \u03b3 \u2022 \u03c1 (6)\nThe value of Gm3m ranges between 0 (the lowest level of interpretability) and 1 (the highest level of interpretability).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MOEA for Rule Selection and Tuning of FRBSs", "text": "The presented algorithm is an extension of the MOEA proposed in [7], to perform a tuning based on the semantic interpretability index GM 3M , while it is combined with a rule selection. It is called Tuning and Selection by SPEA2 for Semantic Interpretability (T S SP 2\u2212SI ) and is based on the well-known SP EA2 [12] algorithm. TS SP 2\u2212SI implements such concepts as incest prevention and restarting [16], and incorporates the main ideas of the algorithm proposed in [2] for guiding the search towards the desired Pareto zone. Thus, the presented algorithm is aimed at generating a complete set of Pareto-optimum solutions with different trade-offs between accuracy and interpretability. We have chosen as base of our method the SPEA2 algorithm since in [2], approaches based on SPEA2 were shown to be more effective when performing a tuning of the MFs. In the next subsections the main components of this algorithm are described and the specific characteristics are presented.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Coding Scheme and Initial Gene Pool", "text": "A double coding scheme for both rule selection (C S ) and tuning (C T ) is used:\nC p = C p S C p T .\nIn the C p S = (c S1 , . . . , c Sm ) part, the coding scheme consists of binary-coded strings with m being the number of initial rules. Depending on whether a rule is selected or not, values '1' or '0' are respectively assigned to the corresponding gene. In the C T part a real coding is used, being m i the number of labels of each of the n variables in the DB,\nC p T = C 1 C 2 . . . C n ; C i = (a i 1 , b i 1 , c i 1 , . . . , a i m i , b i m i , c i m i ), i = 1, . . . , n .\nThe initial population is obtained with all individuals having all genes with value '1' in C S . In the C T part, the initial DB is included as a first individual and the remaining individuals are generated at random within the corresponding variation intervals defined in previous section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Objectives", "text": "The objectives considered in this algorithm are:\n1. Semantic interpretability maximization: Semantic based index (Gm3m).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Complexity minimization: Number of Rules (NR). 3. Error minimization: Mean Squared Error (M SE).", "text": "MSE = 1 2\u2022|E| |E| l=1 (F (x l ) \u2212 y l ) 2 ,\nwhere |E| is the dataset size, F (x l ) is the output of the FRBS when the l-th example is an input and y l is the known desired output. The fuzzy inference system uses the center of gravity weighted by the matching strategy as a defuzzification operator and the minimum t-norm as implication and conjunctive operators.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Crossover and Mutation", "text": "This method uses an intelligent crossover and a mutation operator that it allows to take advantage of the information contained in the parents, improving the balance between exploration and exploitation. Due to the method uses different codes in each one of the parts of the chromosome, it is necessary to apply different operators in each area. The steps to obtain each offspring are as follows:\n-BLX-0.5 [17] crossover is applied to obtain the C T part of the offspring.\n-Once the offspring C T part has been obtained, the binary part C S is obtained based on the C T parts (MFs) of parents and offspring. For each gene in the C S part which represents a concrete rule: 1. The MFs involved in such rule are extracted from the corresponding C T parts for each individual involved in the crossover (offspring and parents 1 and 2). Thus, we can obtain the specific rules that each of the three individuals are representing. 2. Euclidean normalized distances are computed between the offspring rule and each parent rule by considering the center points (vertex) of the MFs comprising such rules. 3. The parent with the closer rule to the one obtained by the offspring is the one that determines if this rule is selected or not for the offspring by directly copying its value in C S for the corresponding gene.\nThis process is repeated until all the C S values are assigned for the offspring. By applying this operator, exploration is performed in the C T part, and C S is directly obtained based on the previous knowledge of each parent. The mutation operator does not need to add rules, directly sets to zero a gene selected at random in the C S part. In the C T part changes a gene value at random. Four offspring are obtained repeating this process four times, only the two most accurate are taken as descendants.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Characteristics of TS SP 2\u2212SI", "text": "This algorithm makes use of the SP EA2 selection mechanism. However in order to improve its search ability the following changes are considered:\n-It includes a mechanism for incest prevention based on the concepts of CHC [16], for maintaining population diversity. This mechanism avoids premature convergence. Only those parents whose hamming distance divided by 4 is higher than a threshold are crossed. Since we consider a real coding scheme in C T , we have to transform each gene using a Gray Code with a fixed number of bits per gene (BGene). In this way, the threshold value is initialized as L = (#C T * BGene)/4, where #C T is the number of genes in the C T part of the chromosome. At each generation of TS SP 2\u2212SI , the threshold value is decremented by one which allows crossing over closer solutions. -A restarting operator is applied by maintaining the most accurate individual, and the most interpretable individual as a part of the new population. The remaining individuals in the new population take the values of the most accurate individual in the C S part and values generated at random in the C T part. We apply the first restart if 50 percent of crossovers are detected at any generation (this percentage is updated each time restarting is performed as % r = (1+% r )/2). Moreover, the most accurate solution should be improved before each restarting. To preserve a well formed Pareto front, the restarting is not applied in the last evaluations of the algorithm (i.e., when the number of evaluations is equal to the number of evaluations consumed until the first restart multiplied by 10)\n-In each stage of the algorithm (between restarting points), the number of solutions in the external population (P t+1 ) considered to form the mating pool is progressively reduced, by focusing only on those with the best accuracy. To do that, the solutions are sorted from the best to the worst (considering accuracy as sorting criterion) and the number of solutions considered for selection is reduced progressively from 100% at the beginning to 50% at the end of each stage by taking into account the value of L. In the last evaluations this mechanism, whose main objective is focusing on the most accurate solutions, is also disabled in order to obtain a wide well formed Pareto front.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of the Performance of the Combined Action of Both, Rule Selection and Tuning", "text": "To analyze the performance of the tuning based on the semantic interpretability index together with a rule selection, two real-world regression problems with different complexities (different number of variables and available data) are considered to be solved (these data sets are available at, http://www.keel.es/): The Wang and Mendel method (WM) [18] is used to obtain the initial Rule Bases (RBs) that will be tuned by the said methods. We consider a 5-fold crossvalidation model, i.e., 5 random partitions of data each with 20%, and the combination of 4 of them (80%) as training and the remaining one as test. For each one of the 5 data partitions, the considered methods have been run 6 times, showing for each problem the averaged results of a total of 30 runs. In the case of T S SP 2\u2212SI and T SP 2\u2212SI the averaged values are calculated considering the most accurate solution from each Pareto front obtained. The values of the input parameters considered by TS are: population size of 61, 100000 evaluations, 0.6 as crossover probability and 0.2 as mutation probability per chromosome. The values of the input parameters considered by the MOEAs are: population size of 200, external population size of 61, 100000 evaluations, 0.2 as mutation probability and 30 bits per gene for the Gray codification.\nTable 1 shows the results obtained with WM, where N R stands for the number of rules, MSE tra/tst for the averaged error obtained over the training/test data, \u03c3 for their respective standard deviations and GM 3M for the semantic interpretability index. In the WM method, GM 3M takes value 1 that is the highest level of interpretability since the MFs are not modified. The results obtained by the three analyzed methods are shown in Table 2. In addition, we also show \u03b4, \u03b3 and \u03c1 that represent the values of the metrics, and t represents the results of applying a test t-student (with 95 percent confidence) in order to ascertain whether differences in the performance of the best results are significant when compared with that of the other algorithm in the table. The interpretation of the t column is: [ ] represents the best averaged result and [+] means that the best result has better performance than that of the related row.\nAnalysing the results showed in Table 2, we can highlight the following facts:\n-The studied method obtains the best results in training and test with respect to the other methods in both problems. -The most accurate solutions from T S SP 2\u2212SI improve the accuracy and obtain more interpretable models, with 29%(WIZ) and 41%(MOR) of improvement in GM3M with respect to the T S method and with 47%(WIZ) and 66%     3 presents a DB obtained with T S and some DBs obtained with the presented method in WIZ. For T S SP 2\u2212SI it includes three DBs, one with the most accurate solution, other with a solution not only accurate also interpretable and another highly interpretable DB, that obtains 40% of improvement with respect to the WM method with a value of GM3M near to 1.", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": ["tab_2", "tab_5", "tab_5"]}, {"heading": "Concluding Remarks", "text": "This work analyzes the performance of the tuning based on semantic interpretability while it is combined with rule selection. The interaction between rule selection and tuning approach with the Gm3m index, allows an important reduction of the system complexity and obtain more interpretable and, at the same time, more accurate models, improving the MSE-GM3M trade-off.\nThe presented method obtains wide well formed Pareto fronts that provide a large variety of solutions to select from more accurate solutions to more interpretable ones. The solutions obtained by the MOEA dominate in general the ones obtained by the mono-objective method and by the previous approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In general, once a model is built and estimated, it has to be evaluated. This is true in the Soft Computing framework as well as in the classical Statistics approach. By evaluating a model we understand to find out if the model satisfies a set of quality criteria that allow us to say if the interesting characteristics of the system under study are actually being captured by it or not.\nNotwithstanding, this set of evaluation criteria is heavily dependent on several considerations: the final use that the model is built for, the inner characteristics of the system that are to be captured and whether the emphasis is put on the empirical behaviour of the model or if there are theoretical considerations that are considered to be more important. This is evident when we consider the evaluation means used in the Soft Computing field as opposed to those used in the statistical approach to time series analysis.\nIn the usually engineering-oriented Soft Computing framework, there has been an overwhelming preeminence of just one evaluation criterion, and this has been the goodness of fit. Generally, evaluation of a model consists on computing the prediction (or classification) error produced when it is faced with a previously unseen problem of the same type of the one used to estimate it. This measure, in its different flavours (mean squared error, mean average error and so on) is affected by some inherent limitations: it is not very meaningful for a single model unless compared against other models, and is usually range-dependent, which makes it difficult to compare the same model applied to different problems represented by data sets with different characteristics.\nOn the other hand, the evaluation in the statistical approach to time series has usually more to do with obtaining an estimate of the probability that the model is effectively capturing the interesting characteristics of the data set, and this is achieved through developing hypothesis tests, also known as misspecification tests.\nThere is a basic assumption behind modelling: a part of the system under study behaves according to a model but there is another part which cannot be explained by it and is usually considered to be white noise. This is the main idea encoded in the expression of the general model\ny t = G(x t ; \u03c8) + \u03b5 t , (1\n)\nand it is also behind some of the diagnostic checking procedures.\nIt is interesting to obtain a precise knowledge about the series of the residuals, {\u03b5 t }, by for example determining if its values are independent and normally distributed. If the residuals were not independent, that would mean that the model is failing to capture an important part of the behaviour of the series, and hence it should be respecified. This can be done through the test presented in [5].\nAnother desirable property that the model should satisfy refers to the variance of the series {\u03b5 t }. If a model is properly capturing the inner behaviour of the series, the residuals should have the same variance at any point of the series. Failing to ensure this implies that the model's precission depends on time, and hence that there are parts of the state-space that are not properly modelled. This will affect very negatively to the performance of the model. Thus this situation should be properly detected so that convenient action for modelling is taken.\nThe current paper paper addresses the detection of this situation when fuzzy rule-based systems are used to model time series. The chosen procedure is throug the defition of a hypothesis test, which we describe and do a preliminary evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Heteroskedasticity in Time Series Modeling", "text": "Ethymologically, heteroskedasticity means differing dispersion or variance. In statistics, a time series is called heteroskedastic if it has different variances throughout the time, and homoskedastic if it shows constant variance in the observable period.\nSuppose we have a time series {y t } n t=1 and a vector of time series (explanatory variables) {x t } n t=1 . When considering conditional expectations of y t given x t , the time series {y t } n t=1 is said to be heteroscedastic if the conditional variance of y t given x t changes with t. This is also referred as conditional heteroscedasticity to emphasize the fact that it is the series of conditional variance that changes and not the unconditional variance.\nA graphical representation might help understand heteroskedasticity The left part of figure 1, (which is adapted from [7]), depicts a classic picture of a homoskedastic situation. We can see a regression line estimated via orthogonal least squares in a simple, bivariate model. The vertical spread of the data around the predicted line appears to be fairly constant as X changes. In contrast, the right part of the figure shows a similar model with heteroskedasticity. The vertical spread of the errors is large for small values of X and then gets smaller as X rises. If the spread of the errors is not constant across the X values, heteroskedasticity is present. In the case of fuzzy rule-based models for time series analysis, we might be interested in studying the heteroskedasticity of the residual series in the state-space regions defined by the antecedent of the rules. If our model's residual series show smoothly changing variance between the rules, it is likely that some rules are failing to capture the behaviour of the series in their state-space subset. This represents an important source of diagnostic information about the goodness of the model.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Fuzzy Rule-Based Models for Time Series Analysis", "text": "When dealing with time series problems (and, in general, when dealing with any problem for which precision is more important than interpretability), the Takagi-Sugeno-Kang paradigm is preferred over other variants of FRBMs. When applied to model or forecast a univariate time series {y t }, the rules of a TSK FRBM are expressed as:\nIf y t\u22121 is A 1 and y t\u22122 is A 2 and . . . and y t\u2212p is A p THEN y t = b 0 + b 1 y t\u22121 + b 2 y t\u22122 + . . . + b p y t\u2212p . (2)\nIn this rule, all the variables y t\u2212i are lagged values of the time series, {y t }.\nConcerning the fuzzy reasoning mechanism for TSK rules, the firing strength of the ith rule is obtained as the t-norm (usually, multiplication operator) of the membership values of the premise part terms of the linguistic variables:\n\u03c9 i (x) = d j=1 \u03bc A i j (x j ),(3)\nwhere the shape of the membership function of the linguistic terms \u03bc A i j can be chosen from a wide range of functions. One of the most common is the Gaussian bell, although it can also be a logistic function and even non-derivable functions as a triangular or trapezoidal function.\nThe overall output is computed as a weighted average or weighted sum of the rules output. In the case of the weighted sum, the output expression is:\ny t = G (x t ; \u03c8) + \u03b5 t = R i=1 \u03c9 i (x t ) \u2022 b i x t + \u03b5 t , (4\n)\nwhere G is the general nonlinear function with parameters \u03c8, R denotes the number of fuzzy rules included in the system and \u03b5 t is the series of the residuals as mentioned in the Introduction. While many TSK FRBMs perform a weighted average to compute the output, additive FRBMs are also a common choice. They have been used in a large number of applications, for example [8,9,10,16]. It has been proved [1] that this specification of the FRBM nests some models from the autoregressive regime switching family. More precisely, it is closely related with the Threshold Autoregressive model (TAR) [15], the Smooth Transition Autoregressive model (STAR) [14], the Linear Local-Global Neural Network (L 2 GNN) [13] and the Neuro-Coefficient STAR [12].\nThis relation has given place to an ongoing exchange of knowledge and methods from the statistical framework to the fuzzy rule-based modelling of time series. For instance, a linearity test against FRBM has been developed [6], and more contributions are yet to come.\nIn this paper we will consider two types of membership functions: sigmoid, \u03bc S , and Gaussian, \u03bc G . The sigmoid function is the one used in [12], and although it is not so common in the fuzzy literature, we will use it here as an immediate result derived from the equivalences stated in [2]. As we know, it is defined as\n\u03bc S (x t ; \u03c8)) = 1 1 + exp (\u2212\u03b3(\u03c9x t \u2212 c)) , (5\n)\nwhere \u03c8 = (\u03b3, \u03c9, c).\nOn the other hand, Gaussian function will also be used because it is the most common membership function in fuzzy models. It is usually expressed as\n\u03bc G (x t ; \u03c8)) = i exp \u2212 (x i \u2212 c i ) 2 2\u03c3 2(6)\nbut we will rewrite it as\n\u03bc G (x t ; \u03c8)) = i exp \u2212\u03b3(x i \u2212 c i ) 2 , (7\n)\nwhere \u03c8 = (\u03b3, c).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Test of Homoscedasticity of the Residuals of an FRBM", "text": "If an FRBM is properly identified and estimated, one might expect that the residuals have a normal distribution, \u03b5 t \u223c N(0, \u03c3 2 ). Moreover, it is expected that the residuals retain this distribution throughout time, that is, that the mean and the variance of \u03b5 t remain constant through the changes of regime resulting from the prevalence of the different rules in different parts of the state-space. It is hence interesting to develop a test which can determine if the variance \u03c3 2 of the residual series changes when the model switches from one regime to another or not. Assuming it does vary, we might note it as a time series \u03c3 2 t , whose specification would be:\n\u03c3 2 t = \u03c3 2 + r i=1 \u03c3 2 i \u03bc \u03c3,i x t ; \u03c8 \u03bc\u03c3,i(8)\nwhere \u03bc \u03c3,i are sigmoid or Gaussian function satisfying the identifiability restrictions defined in [4]. This formulation allows the variance to change smoothly between regimes. Following [11], in order to avoid complicated restrictions over the parameters to guarantee a positive variance, we rewrite equation ( 8) as\n\u03c3 2 t = exp G \u03c3 x t ; \u03c8 \u03c3 , \u03c8 \u03bc\u03c3,i = exp \u03c2 + r i=1 \u03c2 i \u03bc \u03c3,i x t ; \u03c8 \u03bc\u03c3,i , (9\n)\nwhere \u03c8 \u03c3 = [\u03c2, \u03c2 1 , ..., \u03c2 r ] is a vector of real parameters.\nTo derive the test, let us consider r = 1. This is not a restrictive assumption because the test statistic remains unchanged if r > 1. We rewrite model ( 9) as\n\u03c3 2 t = exp (\u03c2 + \u03c2 1 \u03bc \u03c3 (x t ; \u03c8 \u03bc\u03c3 )) , (10\n)\nwhere \u03bc \u03c3 is defined as (5) or as (7), depending on the membership function used by the model. In both cases, sigmoid or Gaussian, the null hypothesis of homoscedasticity of the residuals is H 0 : \u03b3 \u03c3 = 0. As usual, model ( 10) is only identified under the alternative \u03b3 \u03c3 = 0 and we expand the membership function into a firstorder Taylor expansion around \u03b3 \u03c3 = 0. Replacing the function by its Taylor approximation and ignoring the remainder, both the sigmoid and the Gaussian case result in\n\u03c3 2 t = exp \u03c1 + q i=1 \u03c1 i x i,t ,(11)\nso the null hypothesis becomes H 0 :\n\u03c1 1 = \u03c1 2 = ... = \u03c1 q = 0. Under H 0 , exp(\u03c1) = \u03c3 2 .\nThe local approximation to the normal log-likelihood function in a neighbourhood of H 0 for observation t is\nl t = \u2212 1 2 ln(2\u03c0) \u2212 1 2 \u03c1 + q i=1 \u03c1 i x i,t \u2212 \u03b5 2 t 2 exp(\u03c1 + q i=1 \u03c1 i x i,t ) . (12\n)\nIn order to derive a LM type test, we need the partial derivatives of the loglikelihood:\n\u2202l t \u2202\u03c1 = \u2212 1 2 + \u03b5 2 t 2 exp(\u03c1 + q i=1 \u03c1 i x i,t ) , (13\n)\n\u2202l t \u2202\u03c1 i = \u2212 x i 2 + \u03b5 2 t x i 2 exp(\u03c1 + q i=1 \u03c1 i x i,t ) , (14\n)\nand their consistent estimators under the null hypothesis:\n\u2202l t \u2202\u03c1 H0 = 1 2 \u03b5 2 t \u03c3 2 \u2212 1 , (15\n)\n\u2202l t \u2202\u03c1 i H0 = x i,t 2 \u03b5 2 t \u03c3 2 \u2212 1 , (16\n)\nwhere\u03c3 2 = 1/T T t=1\u03b5 2 t .\nThe LM statistic can then be written as\nLM = 1 2 T t=! \u03b5 2 t \u03c3 2 \u2212 1 x t T t=1x tx t \u22121 T t=! \u03b5 2 t \u03c3 2 \u2212 1 x t (17\n)\nwherex t = [1, x t ] .\nFor details, see [11]. The test can be carried out in stages as follows:\n1. Estimate model ( 4) assuming homoscedasticity and compute the residuals\u03b5 t .\nOrthogonalize the residuals by regressing them on \u2207G(x t ;\u03c8) and as before\ncompute the SSR 0 = 1 T T t=1 \u03b5 2 t \u03c3 2 \u03b5 t \u2212 1 2\n, where\u03c3 2 is the unconditional\nvariance of\u03b5 t . 2. Regress \u03b5 2 t \u03c3 2 \u03b5 t \u2212 1 onx t and compute the residual sum of squares SSR 1 = 1 T T t=1\u03bd 2 t . 3. Compute the \u03c7 2 statistic LM \u03c3 \u03c7 2 = T SSR 0 \u2212 SSR 1 SSR 0 or the F version of the test LM \u03c3 F = (SSR 0 \u2212 SSR 1 ) s SSR 1 (T \u2212 s \u2212 n) \u22121 .\nWhere T is the number of observations. Under H 0 , LM \u03c3 \u03c7 2 is asymptotically distributed as a \u03c7 2 with s degrees of freedom and LM \u03c3 F has approximately an F distribution with s and T \u2212 s \u2212 n degrees of freedom.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Evaluation", "text": "In this work we have performed a preliminary assessment of the properties of the test. In this line, we have considered three real-world time series, modeled them with FRMBs and then proceeded to their analysis.\nThe considered cases are fully described in [1], and are a well known ecology problem (the Lynx series), a planning/management problem and a botanic problem.\nThe first series, commonly referred to as the Lynx series, is composed of the annual records of lynx captures in a certain part of Canada during a period spanning 113 years. It is a common benchmarking series used to test and compare time series models, and here we have used its logarithmic transformation. An FRBM with two rules (model A) was identified following the iterative procedure proposed in [1], and it was later estimated using a Genetic Algorithm.\nThe second considered series comes from an emergency call center and is the record of the number of calls received daily throughout four years. As the series is non-stationary and shows a high variability, it was differenced after applying a log-transformation. The identified FRBM (model B) was also composed of just two fuzzy rules, which were also fine tuned through a Genetic Algorithm.\nFinally, the third series was a daily aerobiological log obtained over sixteen years in the city of Granada (Spain), containing daily counts of airborne olive tree pollen grains. This series was previously studied in [3].\nTable 1 shows some information about the application of the FRBM, both in its sigmoid and Gaussian versions, to the three time series mentioned above. More precisely, for each model, the table shows, the number of rules of the model, the values for the variance of the residuals (\u03c3 \u03b5t ) and the Akaike information criterion (AIC), together with the p-value obtained with the test for homoscedasticity of the residuals.\nBy studying the p-values shown in columns 5 and 8 we can see how the null hypothesis of the test was rejected in all the six cases, which leads us to conclude that the variance of the residuals remained constant through time in every application.\nAs mentioned above, this is a necessary condition for considering that a model is properly capturing the behaviour of a time series.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions and Final Remarks", "text": "In this paper, a new statistical tool to evaluate the residuals of a fuzzy rule-based model has been presented. It consists of a test against homoskedasticity of the residuals, that is, a test that allow the user to determine if the variance of the residual series remains constant through time. In other words, this test is able to tell if a model's errors are bigger in some parts of the state-space or not.\nThis represents a useful contribution and another step towards a statistically sound framework for the use of fuzzy rule-based models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Applications of Operations Research in the transportation field always produce great impact, because these techniques are able to improve the quality of pick-up and delivery service and they can reduce the operating costs of the systems considered. An important application is the School Transportation Problem, which is a special case of vehicle routing with time windows (VRPTW), heterogeneous fleet and considering simultaneous pick up and delivery. In these kinds of problems a set of vehicles make the pick up and / or delivery products or people to consumers dispersed in an area. The goal is to find a set of vehicle routes and schedules that satisfies a variety of constraints and minimizes the total fleet operating costs [1], [2].\nThe development of models that offer optimal solutions to this problem is complex, because it is a problem that considers many constraints and the computational cost to do this task is high, sometimes it becomes impossible to be performed. For this reason many efforts have been made by researchers in worldwide to find new approaches that can produce good solutions to such problems with low computational cost [3]. Constrains considering vehicle capacity, maximum distance of each route, time windows and minimum coverage of the breakpoints.\nIn this paper we applied some techniques of Operations Research and heuristic algorithms to solve the real problem of the School Transportation in the Brazilian state of Parana. Using this methodology the manager of the Bus School Transport will be able to improve the service by reducing the time of the students inside the vehicles while minimize the total distance travelled by all the vehicles. In this problem the stop points are mixed, in other words students from different schools and degrees can be picked up and dropped by the same route, since the objective of minimizing the total distance of the route and attend a larger number of students can be hold.\nThe VRP appears in many applications such as garbage collect, distributions of drinks, gasoline and other products. The School Transportation Problem can be faced like a VRP considering a heterogeneous fleet, consists of determining a set of routes and schedule each vehicle to a route respecting the vehicle capacity constrains. The demand points are the student homes, and constrains to determine the routes are the vehicles capacity, the maximum time spent for each student into the vehicle may be limited by maximum length of the route. This paper was carried out in three steps. The first step is to determine the breakpoints (Bus Stops -BS) of the vehicles, considering the maximum distance that students can walk from their homes until these points. In the second stage the distances between two Bus Stop and/or School Points (SP) are calculated. Finally, in the third step we applied the Location Based Heuristic (LBH) with some proposed adaptations to be used in the real situation, and it was called Adapted Location Based Heuristic (ALBH) to route and schedule the buses so as minimizing total operating costs respecting the constrains to find a feasible solution for the problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Literature Review", "text": "In the literature few records of practical applications of theories of VRP to the School Bus Transportation are found. In [4] students are assigned to an intersection of streets adjacent to streets from their homes, and a subset of these points is considered potential points to solve the traditional routing problem.\nIn [5] Tabu Search is used to solve the traditional routing problem, showing results by solving several problems of the literature and presenting comparisons of their results with other techniques, but in general the solved problems are small.\nA formulation of Integer Linear Programming to solve the School Bus Transportation is presented in [6], with appropriate restrictions in a Flemish region, where some comparisons and partial solutions to small problems are shown by testing feasible solutions and comparing the computational time by using this technique and similar others.\nSome techniques that can be used to solve the VRP is shown in [7], including traditional techniques such as Clark Wright savings, 2-stage methods, and even metaheuristics like Tabu Search. Results are presented comparing all the techniques shown in the article, using as parameters both the computational time as the quality of each solution.\nReference [8] show 3 techniques to solve the problem of School Bus Transportation in New Jersey City, they are: the Clarke and Wright Savings, the computer program called Router and a Sweep method. The results of these three techniques show that is possible to find a good solution for the city used in the tests.\nIn [9] a self-organizing Neural Network is used to solve both the problem of Multiple Salesman as the problem of vehicle routing with capacity constraints. The Neural Network is compared with similar techniques in the literature and the results are promising.\nIn their paper Braca et al [3] present a methodology to solve the School Bus Transportation and it was applied to New York City. The algorithm to solve this problem was proposed by Bramel and Simchi-Levi [10], [11] and converges asymptotically to the optimal solution to vehicle routing problem capacity. In this problem the authors used 838 breakpoints and 73 schools. The minimum number of vehicles determined by the algorithm was 59 to be used in the morning and 56 in the afternoon. In this work we choose the Braca's algorithm because the similarity between the problems, and this method was applied to solve the School Bus Transportation 399 cities of Parana, but we present here the results only for 20 of them, because the difficulty to compare the results. Some modifications were implemented to adapt this technique to the real problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The School Bus Problem", "text": "A set of students dispersed in an area must be picked up at your bus stop and dropped at your school every school day. After the bus stops have been determined the students must be assigned to the bus stop nearest from their homes the next step is to route and schedule the vehicles minimizing the total distances daily travelled and then reducing the time that the students spend inside the vehicle while the safety requirements are satisfied.\nThe real case that we considered here is about a Brazilian State of Parana, covering 399 cities, the customers are students in elementary and high school. They must be picked up in their bus stop and dropped off at their schools. The school bus transportation costs are calculated considering the total distance daily travelled and the amount and type of vehicles used to do the job. Usually, the fleet is heterogeneous, because there are regions where some vehicles are unable to traffic according to the road conditions, which generally are very steep and narrow, making impossible that certain kind of vehicles travel trough these roads. In this Province the School Bus Transportation is under responsibility of the Municipal Departments of Education, who is responsible to contract the vehicles that will be used to take the students at their schools.\nThe goal is to assign the students to the break points, find the better route and schedule the vehicle that will be used in this route, minimizing total distance traveled and the amount of used vehicles. The mathematical formulation for this problem is presented below, in accordance to the equations (1) to (12):\nMinimize \u2208 \u2208 = \u2211 \u2211 \u2211 1 k ij ijk i V j V k c x (1)\nSubject to:\n\u2208 = \u2264 \u2211 \u2211 1 E k jk j V k y K (2) ik V j jik V j ijk y x x = = \u2211 \u2211 \u2208 \u2208 , \u2200i\u2208V, k = 1, 2, ..., K (3\n)\nhk V V i ik S l hl y y w E \u2265 = \u2211 \u2211 \u2208 \u2208 \\ , h\u2208V E , k = 1, 2, ..., K(4)\n\u2211 = \u2264 K k ik y 1 1, \u2200i\u2208V\\V E (5\n)\nk V i S l ilk C z \u2264 \u2211\u2211 \u2208 \u2208 , k = 1, 2,..., K (6) ik ilk y z \u2264 , \u2200i, l, k (7) 1 1 = \u2211\u2211 \u2208 = V i K k ilk z , \u2200l\u2208S(8)\ny ik \u2208{0,1}, \u2200i\u2208V, k = 1, 2, ..., K(9)\nx ijk \u2208{0,1}, \u2200i,j\u2208V|i \u2260 j (10) z ilk \u2208{0,1}, \u2200i,j\u2208V|i \u2260 j (11)\nw hl \u2208{0,1}, \u2200h\u2208V E , \u2200l\u2208S(12)\nwhere, c ij = distance between i and j; K = amount of vehicles; C k = capacity of the vehicle k; V = set of customers to be visited (bus stop); V E = set of the depots (school points); S = set of customers (set of students); x ijk = 1 if arc(i, j) belongs to the route operated by vehicle k and 0 otherwise; y ik = 1 if the client i is visited by vehicle k and 0 otherwise; z ilk = 1 if the customer l is picked-up by the vehicle k at the breakpoint i and 0 otherwise; w hl = 1 if the client l goes to school h and 0 otherwise. The objective function (1) seeks to minimize the total distance traveled by all the vehicles. Constrains (2) guarantee that all the vehicles started its route at the depot (school). Constrains (3) ensure that if the customer i is visited by the vehicle k, so an arc must be traveled by the vehicle k by starting and departure from the same point i. Constrains (4) guarantee that all the customers will be in the route of this respective schools. Constrains (5) ensure that each client is visited by exactly one vehicle except the schools. Constrains (6) define that the capacity of the vehicles will not be exceeded. Constrains (7) guarantee that the customer l is not collected at the point i by the vehicle k if the vehicle k does not pass trough to the point i. Constrains (8) guarantee that each customer will be collected only once. Constrains from ( 9) to ( 12) define the decision variables.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The School Bus Problem", "text": "To solve this problem we considered three steps that are described below:\nStep 1: Determination of the buses stops -In this stage the stop points are defined in accordance with the geographical position of the student's home. Each student should be assigned to their nearest bus stop. In this stage some parameters must be considered and they are defined by Municipal Departments of Education of the State and they are:\n\u2022 d max : Maximum distance that the student must walk from this home until the bus stop -this parameter is limited by a minimum and maximum value it must be between 500 and 3,000 meters; \u2022 Distance between the buses stops: the distance between the buses stop must be between 1,000 and 3,000 meters; \u2022 Cover: Minimum distance between the customers (students) to their depot (school). This parameter should be between 1,000 and 3,000 meters, that means if customer lives less than this distance from the depot he should not use the school bus transportation.\nFig. 1 displays the buses stops proposed by the algorithm and the geographic positions of the customers and the depots in Castro city for the morning period, using d max = 1,500 meters.\nBefore building the routes, is necessary to calculate the real distances between the buses stops and the depots (schools). To calculate these distances it was necessary to build the roads, because there are no digital maps available for these cities. The dataset were provided by the State Ministry of Urban Development in Parana containing the geographical coordinate points of the pathways of 100 to 100 meters for each city and the information from crosses and end of the roads. Using these datasets it was possible to generate the road map that shows the available ways that might be explored in searching better solutions for problem. The datasets of each city is extremely large, which makes the process slow, for example the database for Castro City contains 43,793 points beside the bus stops and depots points. The pseudo code of the algorithm to determine the buses stop is presented below.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Algorithm to find out the buses stops", "text": "{Let: U set of the customers that were not assigned; P(.)the buses stops index; d max maximum distance between customers and their buses stops; dist(i,j) distance from the customer (student home) i until the bus stop j; cover: minimum distance between the customer until his school};\nSelect U* customers i such as dist(i,school(i))\u2264 cover); Let U\u2190U\\U*; {Choose randomly the geographic position of each customer i \u2208 U. This position is the bus stop j; Select customers k such as dist(k,j)\u2264dmax, \u2200k P(k)=j. }\nStep 2: Calculating the Real distances between the buses stops -Although the computational cost to calculate the real distances are often greater than the cost to calculate the Euclidean distances it is really necessary in real cases. Let A i the set of the adjacent points to the point i and T k the set of the other points in the same road k. The pseudo code to calculate the shortest distance between two points is presented below. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Calculate of the real distances between two points", "text": "{Let: i the initial point, j the final point; dist(.) function to calculate real distance between two points; R set of sequence of points between i and j into the route; n = 1}; While n\u22642 select the set of the adjacent points from the point i, Ai and calculate dist(m,j) \u2200m\u2208Ai;\n{Let k such as dist(k,j)=minm\u2208Ai,m\u2209R{dist(m,j)}, R\u2190R\u222a{k} {Select one of 4 possible alternatives to the point k:\n-If the point k is the end of road, find R* = {l,...,k}, such as R*\u2282R and l is the initial point of the cross-road between k and l. R\u2190R\\R*, and i=l. In next itera-tions, points of the R* must be prohibits for the route; -If point k make a cycle, find R*={k,...,k}, such as R*\u2282R determine the part of the route that contains the cycle. Let R\u2190R\\R* and i=k. In next iterations the points of set R* must be prohibits in the route; -If k is not a crossroad, find Tk = {k1,k2,...,k,...,kt}, where k1 and kt are crossroads. Let R\u2190R\u222aTk, and i=kt -If k is crossroad, let i=k} If k=j, R is the full route from the point i to j; If n=1, let i\u2190j and j\u2190i, R**\u2190R, R=\u2205 and n\u2190n+1; } Choose between R and R** the shortest route.\nStep 3: Building the routes -ALBH -Adapted Location Based Heuristic -After the assignment of the customers (students) has been made to their bus stops and the real distances have been calculated is time to construct the routes. The used technique in this work was based in Location Based Heuristic (LBH) and it was called Adapted Location Based Heuristic (ALBH), which converges asymptotically to the optimal solution in Vehicle Routing Problems Capabilities and heterogeneous fleets. The pseudo code to calculate the shortest distance between two points is presented below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Students Bus stops Schools", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Design of the routes -ALBH", "text": "Let v the biggest vehicle that was not assigned to a route; {Let m=0 and S={1,2,...,l} the set of the all customers in their bus stops that are not into a route. Select m\u2208S, such as dist(m,school(m))= maxi\u2208S{dist(i,school(i))}. While S\u2260\u2205: {Select the index of the farthest student from his school that is not in the route j\u2208S, such as dist(j,school(j))= maxi\u2208S{dist(i,school(i))}. Let S\u2190S\\{j}. Calculate the longer route allowed for the student j, where: The heuristic solution of vehicle v is {R1,R2,...,Rm}; Check if the used vehicle is completely full, if there are many places not used, select another vehicle that was not routed yet and that is more appropriate to the route. }\nThe function comp_route(.) is used to calculate the total distance by inserting the point i and their respective school in the route R. The position of the inserted point is exchanged until the shortest path in the route has been found. The function route(.) insert the customer k and his depot (school) in the best position defined by the function comp_route(.). The most important modifications proposed in ALBH algorithm are: vehicles have different capacities C v ; the routes start with the farthest point from its school; the vehicle assignment for this route is the biggest vehicle and after the construction of the route is checked if there is a smallest vehicle available that can be assigned for the route. The n 1 term is used to allow longer routes when the number of customers in the buses is less than the total demand of the route. The n 2 term is used to avoid very long routes and to prohibit that the maximum length of the route is limited by the distance between the farthest bus stop and its school. If this term is not used many routes can be created containing only one stop point because its forbidden to pick another students in this same route. The methodology is used separately for each period (morning, afternoon and night) and the daily total distance for each vehicle is calculated considering both taking the students from their homes to school and from the school to their homes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments and Results", "text": "The experiments were accomplished using the data base from ten cities in the state of Parana, they are. The results were compared to the real situation. The real distances to make the comparisons were calculated using obtained by using a GPS system (Global Positioning System) for mapping the School Bus routes that are actually performed in each city.\nTable 1 shows the dataset for each city about the amount of students using bus school transportation, vehicles and schools. The amount of available vehicles in each city is showed in the 3 rd column of the Table 1. Fig. 2 shows the design of the routes on the Castro maps for the morning period. In this simulation the longer distance used between the stop points is 1,500 meters and the cover (distance between the customer and depot) is 2,000 meters. The used technique was ALBH which provided an economy ranging between 8.8 and 58.8% in the total daily distance traveled as shown in Table 1.\nThe School Transportation Manager can set the parameters that better adapt to the reality of that city. The choice of these parameters have great influence in the optimization process, for example if the longer route is less than 20 km there will be a greater demand on the amount of vehicles while longer routes can raise the time of the students inside the vehicle.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": ["tab_2", "tab_2", "tab_2"]}, {"heading": "Fig. 2. Map of Castro city -routes in the morning", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "To evaluate the performance of our approach, we compared the obtained results with the real situation it means that the total mileage traveled by the fleet was compared to the mileage that is being held in each city by the manager of the school bus transportation. The results were not compared to another algorithm because this problem had not been solved before using any technique. In the first column of table 1 are listed the name of the cities for which the simulation was performed. In the other columns of the", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stop Points Schools", "text": "Table 1 are showed the dataset about each city and the results are in the 8 th column. The computational time (last column) depends on the how large is the city, the number of the students and schools. The main advantages of using this methodology are: decrease the time that the student spend into the bus by reducing the amount of the bus stops; decrease in the costs by reducing the number of the vehicles used and the total daily distance traveled in each period as is showed in the Table 1. The economy ranges between 1% and 49%. In this experiment the best solution was obtained when the biggest distance between the customers to his depot is of 2,000 meters. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Conclusions and Future Works", "text": "In this paper we proposed a new methodology for the School Bus Transportation and it was performed to ten cities of Brazilian state of Parana. The problem was solved in three phases, the first one is about to determine the better positions of the Buses Stop; the second is calculate the real distances between all the points and the last one is use the ALBH to build the routes that can better solve the problem.\nThe tests were performed using a Pentium IV computer, 2.8 GHz, 1Gb of ram memory. The computational time is not high considering the amount of the points in each data set. The computational time is calculated considering the all process for each period (morning, afternoon and night) in each city. This all system must be performed at most twice a year, in the beginning and in the middle of the year in order to consider changes the address students. During this time any insertion or exclusion of the bus stop should be made manually to avoid huge changes in the routes. The requirements to perform this process are: dataset of the cities containing the geographic positions of the students, the schools and the number of the available vehicles to be used, this allows that the optimization process can be easily applied to anywhere by making few adaptations. It is important to note that no digital map is required. To perform the process is only necessary to have the coordinates of the points and the proposed methodology is able to construct the roads. In the future work we intend to compare the results obtained by using the ALBH algorithm and the proposed methodology with another approach like Neural Network.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Particle Swarm Optimization (PSO) [11,12],based on the Swarm Intellegence algorithm [2], has been widely studied recently as a stochastic search technique for global optimization over continuous problem spaces. The PSO algorithm was motivated as simulating the behaviors of a population of simple agents (ants, birds or even people) interacting locally with each other and with their environment. Even when there is no central control of how an individual simple agent should behave, the local interactions between these agents can lead to a globally optimized behavior. During recent years, PSO algorithms have encompassed a broad variety of problems such as artificial intelligence [4], clustering [6], Markov Decision Processes [5], etc.\nExploratory data analysis is a set of methods with which we try to extract as much information as possible from a data set of high dimension and huge volume. In this paper, we will develop PSO-based algorithms for exploratory data analysis. We first derive a PSO-based method for projection problems, such as principal component analysis (PCA) and exploratory projection pursuit (EPP). The results show that our algorithm can identify the optimal solutions quickly with high accuracy, even when the size of data set is small, which often leads to poor generalization, and the number of iterations is low. Then we incorporate the PSO algorithm with Q-learning, a form of reinforcement learning [14], and apply the PSO-based Q-learning algorithm to solve the PCA problem. We compare the results with those we report in [17,1]. We demonstrate that the PSO-based algorithm has better performance in that not only is the accuracy of the final results improved, but also the number of iterations required to achieve the global optimum are reduced. Finally, we illustrate a topology preserving mapping with the PSO algorithm.\nWe illustrate our new methods on simple processes such as PCA and EPP but this is for didactic purposes only and is a continuation of our previous investigations into non-standard optimizations [17,1]. We envisage that the results of these investigations will feed into more complex optimizations in future.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Particle Swarm Optimization", "text": "Particle Swarm algorithms were developed on the basis of the astonishing selforganization exhibited by groups of very simple agents which may be insects, birds or animals. Such agents appear to exhibit group behavior which far transcends the individual thought processes of the agents and the algorithms attempt to emulate such emergent behaviors. Particle Swarm Optimization (PSO) [11,12] is a stochastic search algorithm, which aims to identify the global optimum of an objective function without requiring any gradient rule.\nIn PSO, a swarm of simple agents (particles) is initialized in the multidimensional problem space with random positions X i and velocities V i . These particles are considered to move in the problem space searching for the global optimal solution and the location of each particle in the problem space represents one possible solution. Thus when a particle moves to another location, a different solution to the problem is generated. In each iteration, the solutions represented by the particles are evaluated by a fitness function f , then the locations X i = (x i1 , x i2 , ..., x im ) and velocities V i = (v i1 , v i2 , ..., v im ) of these particles are adjusted using\nV i (t + 1) = \u03c9 \u2022 V i (t) + C 1 \u2022 \u03d5 1 \u2022 (P li \u2212 X i (t)) + C 2 \u2022 \u03d5 2 \u2022 (P g \u2212 X i (t)) (1) X i (t + 1) = X i (t) + V i (t + 1)(2)\nwhere the variables \u03d5 1 and \u03d5 2 are random positive numbers, drawn from a uniform distribution, C 1 and C 2 are called acceleration constants and \u03c9 is called the inertia weight. The variable P li is the local best solution found so far by the i th particle and the variable P g is the location of the particle with highest fitness during the previous iterations, called the global best solution. We can see that the movement of particles encompasses two impulses. The first is called the cognitive behavior where one particle follows its own cognitive experience via its own optimal personal local solution foregoing the group solution. The second is to consider the social behavior in which each particle gets attracted to the group's center. Therefore, at the end of the simulation, most of the particles will converge to a small ball surrounding the global optimum of the search space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PSO in Exploratory Data Analysis", "text": "Exploratory data analysis is a set of methods with which we try to extract as much information as possible often from a data set of high dimension and huge volume. In this section, we apply the PSO algorithm to a set of projection methods. We consider that each particle is deemed to be taking movements in an environment that consists of data to be explored, in order to maximize the fitness in this environment. We use stochastic units, W, drawn from a Gaussian distribution to sample the population of particles which means each unit represents one possible solution and the samples/units W are all drawn from N (m, \u03b2 2 I), the Gaussian distribution with mean m and variance \u03b2 2 . It is these parameters, the mean m and variance \u03b2 2 that the particle swarm optimization algorithm produces and this algorithm is directed by the efficiency of the individual samples. Although different projection methods have their own objective functions, they share the common property that the fitness function determines how well one particle fits its environment. We thus show our method is quite a general one that can be easily applied to different projection methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Principal Component Analysis", "text": "Principal Component Analysis (PCA) finds the linear filters W onto which projections of a data set have greatest variance. Thus each particle represents one possible filter and is evaluated in proportion to its variance. At iteration t, we create an initial M particles, w t 1 , ..., w t M from N (m, \u03b2 2 I), a D\u2212dimensional isotropic Gaussian distribution with centre, m. We calculate the fitness value for each of these particles and the particle with highest fitness value is identified as the local best solution w t * . At each iteration, the whole swarm keeps a memory of the best particle w * visited so far by all the particles, which is known as the global best solution. We move the distribution of the stochastic unit with the learning rule\nm \u2190 (1 \u2212 \u03b7)m + \u03b7(C 1 \u2022 \u03d5 1 \u2022 (w t * \u2212 m) + C 2 \u2022 \u03d5 2 \u2022 (w * \u2212 m)) (3\n)\n\u03b2 2 \u2190 (1 \u2212 \u03b7)\u03b2 2 + \u03b7 \u239b \u239d t i=1 (w i * \u2212\u0175 T * ) 2 t \u239e \u23a0 (4\n)\nwhere \u03b7 is the learning rate. The two variables, C 1 and C 2 are acceleration constants, which are used to effect the stochastic nature of the algorithm and scaled by constants 0 < C 1 , C 2 < 2. The value of C 1 \u03d5 1 controls the degree of local interactions in the current population of particles and the value of C 2 \u03d5 2 controls the degree of global interaction. The variable\u0175 T * , T = 1, 2, ..., t is the average value of all the local best solutions we have found during the past t iterations.\nWe summarize our PSO-based PCA algorithm as follows:\n1. Select one item of data from the data set randomly. 2. Generate a population of particles w t 1 , ..., w t M from the currently estimated distribution N (m t , \u03b2 2 t ) . 3. Evaluate each particle according to the fitness function and identify the local best solution w t * .\n4. Compare the fitness values of the local best solution, w t * and the current global best solution and identify the new global solution, w * . 5. Update the parameters of the distribution to make a new probability density function with (3) and (4). 6. If less than maximum number of iterations, go back to step 1.\nTo illustrate our algorithm, we create a 5-dimensional artificial data set of 10000 samples, whose elements are drawn independently from Gaussian distributions with x i \u223c N (0, i 2 ), so x 5 has the greatest variance and x 1 has the lowest variance. The fitness function is defined as f = 1 1+exp(\u2212\u03b3|w T x|) . To identify multiple components, we use the Gram-Schmidt method as the deflation method. Thus, from the second component onwards, right after generating a particle w t j from the currently estimated distribution N (m t , \u03b2 2 t I), we subtract (w T j m k )m k , k = 1, 2, . . . , j \u2212 1 from w j . We set \u03d5 1 = 1 and \u03d5 2 = 1.2 and the learning rate \u03b7 = 0.01, which is reduced linearly to zero. The number of iteration is 10000. We see in Table 1 that the five principal components have been identified with very high accuracy and our algorithm converges smoothly and quickly as shown in Figure 1.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2"]}, {"heading": "Exploratory Projection Pursuit", "text": "Exploratory Projection Pursuit (EPP) (see [7,8]) defines a group of techniques designed to investigate structure in high dimensional data sets by finding \"interesting\" directions in the data space. To illustrate our method for exploratory projection pursuit, we create 1000 samples of 5 dimensional data in which 4 elements of each vector are drawn from N (0, 1), while the fifth contains data with negative kurtosis: we draw this also from N (0, 1), but randomly add or subtract 5. Before performing the algorithm, we sphere the data set so that it has zero mean and unit variance in all directions. We use S(w) = | tanh(w T x)| as the fitness function. Note that g(s) = | tanh(s)| is an even function that can be used to measure kurtosis. Table 2 shows the outcome of the simulation. We can see that the distribution with negative kurtosis has been identified with high accuracy and extremely quickly by the defined fitness function. Convergence was as fast and stable as for the PCA experiment.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Principal Component Analysis with Q-Learning", "text": "Reinforcement learning [14] is a sub-area of machine learning which trains agents by reward and punishment without needing to specify how to achieve a task. Reinforcement learning algorithms attempt to find a policy that maps states to actions so as to maximize some notion of long-term reward.\nIn reinforcement learning models, an agent exists within its environment and at each time of interaction, t, the agent perceives the state s t \u2208 S of the environment and the set of possible actions A(s t ). Then the agent chooses an action a \u2208 A(s t ) that changes the state of the environment from s t to s t+1 and receives a reward r t+1 . The agent's behavior, B, should be based on a policy \u03c0, mapping states to actions, that tends to maximise the long-term sum of values of the rewards.\nThe Q-learning method has been introduced in [15,16]. This method directly approximates the optimal action-value function, Q * , by the learned action-value function, Q, and the best possible action selected in the subsequent state:\nQ(s t , a t ) \u2190 Q(s t , a t ) + \u03b1[r t+1 + \u03b3 max a Q(s t+1 , a) \u2212 Q(s t , a t )].\nIn [17,1], we derived a method to solve PCA problems with Q-learning. In this section, we incorporate the PSO algorithm with Q-learning and apply the PSObased Q-learning algorithm to solve PCA problem.\nThe state of the system at any time is the data sample presented to the system at that time, i.e. we equate s t with x t , s t = x t . We use a parametric (actually Gaussian) estimator for the statistic to be calculated and the action taken in response to the state is to sample the weight vector w from the distribution N (m, \u03b2 2 I) with the current estimate of the parameters, m and \u03b2 2 , having been optimized in the previous iteration. Then we may identify rewards and update both the estimated Q-value (which is what we wish to maximise) of the estimator and the parameters (mean and variance) of the estimator.\nWe use the same 5 dimensional data as before in which the first principal component is readily identified as the fifth input dimension. At each iteration, the particles, W, are drawn from the Gaussian distribution, N (m, \u03b2 2 I), which are used to represent the possible actions given the current state/data point. For the PCA problem, the reward of each possible action is defined by r = 1 1+exp(\u2212\u03b3|w T x|) . Then the algorithm identifies the locally best particle, w t * , with the highest reward r t * . Meanwhile, the algorithm also keeps a memory of the globally best particle, w * , with the highest Q-value Q * so far. The local best particle w t * with its reward r t * is used to calculate the new Q-value of the state/data point by the following\n\u0394Q t * i \u2190 \u03b1(r t * + \u03b3Q * \u2212 Q i ) ( 5 ) Q i \u2190 Q i + \u0394Q t * i (6)\nIn each iteration, the calculation is then followed by\nm = (1 \u2212 \u03b7)m + \u03b7 \u2022 \u0394Q i \u2022 (C 1 \u2022 \u03d5 1 \u2022 (w t * \u2212 m) + C 2 \u2022 \u03d5 2 \u2022 (w * \u2212 m))(7)\nwhere i is the index of current data point we have randomly selected, D is the dimensionality of the data and \u03b7 is the learning rate.\nThe number of iterations is 5000 and the learning rate is initialized to 0.01 and reduced linearly to zero. Again the algorithm identifies the optimal principal component direction very quickly with high accuracy. We compare the performance of this algorithm with the results by immediate reward reinforcement learning and the Q-learning method that we show in [17] in Figure 2. It is clear that with the same number of iterations, the PSO-based PCA algorithm converges to the optimal solution but the other two methods fail to do so.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Topology Preserving Manifolds", "text": "A topographic mapping captures some structure in the data set, so that points which are mapped close to one another have some common feature while points that are mapped far from one another do not share this feature. The most common topographic mapping is Kohonen's self-organizing map (SOM) [13]. The Generative Topographic Mapping (GTM) [3] is a mixture of experts model which treats the data as having been generated by a set of latent points where the mapping is non-linear. In [9], we have derived an alternative topology preserving model, called the Topographic Products of Experts (ToPoE), based on products of experts [10], which is closely related to the generative topographic mapping. In this section, we present a PSO-based method to perform the topology preserving mapping.\nGiven a set of data points t 1 , ..., t N , we follow [3,9] to create a latent space of points x 1 , . . . , x K which lie equidistantly on a line or at the corners of a grid. To allow non-linear modeling, we define a set of M basis functions, \u03c6 1 (), . . . , \u03c6 M (), with centres \u03bc j in latent space. Thus we have a matrix \u03a6 where \u03c6 kj = \u03c6 j (x k ), each row of which is the response of the basis functions to one latent point, or, alternatively each column of which is the response of one of the basis functions to the set of latent points. Typically, the basis function is a squared exponential. These latent points are then mapped to a set of points m 1 , . . . , m K in data space where m j = (\u03a6 j W) T , through a set of weights, W. The matrix W is M \u00d7 D and is the sole parameter which we change during training. We have\nm k = M j=1 w j \u03c6 j (x k ) = M j=1 w j exp(\u2212\u03b2||\u03bc j \u2212 x k || 2 ), \u2200k \u2208 {1, . . . , K}. (8)\nwhere \u03c6 j (), j = 1, . . . , M are the M basis functions, and w j is the weight from the j th basis function to the data space. The algorithm is summarized as:\n1. Randomly select a data point, t n . 2. Find the closest prototype, say m k * , to t n . 3. Generate T particles from the Gaussian distribution, N (m k * , \u03b2 2 k * I). Call the particles, y k * ,1 , ..., y k * ,T . We note that we are using m 1 , m 2 , . . . , m K to perform two conceptually separate functions, as prototypes or means to which the data will be quantized and as centres of Gaussian distributions from which samples will be drawn. 4. Evaluate the particles using S(y) = exp(\u2212\u03b3 y \u2212 t n 2 ) as the fitness function. 5. Identify the local best particle, y k * ,t * , which has the largest fitness value. 6. Update the parameters\nw \u2190 w + \u03b7[C 1 \u2022 \u03d5 1 \u2022 (y k * ,t * \u2212 m k * )\u03c6(x k * ) + C 2 \u2022 \u03d5 2 \u2022 (w * \u2212 w)] (9)\nwhere \u03b7 is the learning rates and y k * ,t * is the local best particle and w * is the global best solution we have found up to the current iteration. 7. Update the prototypes' positions using (8). 8. Evaluate the total distance between prototypes and data points and identify the global best solution, w * , which is defined in (10).\nw * \u2208 arg min w\u2208W K k=1 N n=1 m k \u2212 t n 2(10)\nIt is worth noting that for topology preserving mappings, the global optimal solution we look for is a set of weights through which the latent points in the latent space are projected to prototypes in the data space and the distance between these prototypes and all the data points is minimized. In our algorithm above, given one data point t n selected at one iteration, the particles are drawn around the prototype m k * that is closest to the selected data point t n , and the local best particle y k * ,t * in step 5 represents a new location of the prototype m k * that is possibly closer to the data point t n . Since the particles do not represent the weights, the particles do not represent a topology preserving mapping, but the local best particle can be regarded as a local best solution. We thus do not keep a memory of the global best particle as the standard PSO does. Instead we keep a memory of the global best solution w * in step 8, which is defined in formula ( 10) and in step 6, and we update the weights by considering the local best solution y k * ,t * and the global best solution w * . Figure 3 shows the result of a simulation in which there are 20 latent points lying equally spaced in a one dimensional latent space, passed through 5 basis functions and mapped to the data space by the linear mapping W . We generate 1000 2-dimensional data points, (x 1 , x 2 ), from the function x 2 = x 1 + 1.25 sin(x 1 ) + \u03c1, where \u03c1 is the noise from a uniform distribution in [0, 1]. Hence the data set, though 2 dimensional, has an implicit dimensionality of 1. The number of iterations is 10000. The latent points' projections are shown in the figure as black *s and are adjacent latent points' projections are joined with a line. We clearly see that the one dimensional nature of the data has been identified and that neighbouring latent points have responsibility for neighbouring data points. ", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have illustrated the use of PSO-based algorithm for exploratory data analysis. For the linear projection problems, we define the population of particles at one iteration by a set of parameters, W sampled by stochastic units drawn from N (m, \u03b2 2 I), the Gaussian distribution with mean m and variance \u03b2 2 . We demonstrated the PSO-based algorithm on principal component analysis and exploratory projection pursuit. The results have shown that the PSO-based algorithms can identify the optimal direction quickly, robustly and with high accuracy. Then we incorporate the PSO-based algorithm into the Q-learning method, where the new algorithm is used to solve the PCA problem. We demonstrated the new algorithm can converge to the global optimum more quickly with higher accuracy compared with the results we have in [17,1] by the immediate reward reinforcement learning and the Q-learning method. Finally, we have developed a PSO-based algorithm for topology preserving mappings. Instead of considering the local best particle and global best particle in the standard PSO algorithm, we use the local best solution and global best solution to update the weights through which the latent points in the latent space are projected to prototypes in the data space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In recent years we have witnessed a tremendous growth of communication networks, which resulted in a large variety of combinatorial optimization problems. One of these problems is the Terminal Assignment (TA) Problem. In centralized computer networks, a central computer services several terminals or workstations. In a large network, some concentrators are used to increase the network efficiency. A collection of terminals is connected to a concentrator and each concentrator is connected to the central computer. The TA problem involves determining which terminals will be serviced by each concentrator. The number of concentrators and terminals and their locations are known. Each concentrator is limited in the amount of traffic that it can accommodate. For that reason, each terminal must be assigned to one node of the set of concentrators, in a way that no concentrator oversteps its capacity [1][2] [3]. In this work, the problem is interpreted as a multi-objective task. The optimization goals are to simultaneously produce feasible solutions, to minimize the distances between concentrators and terminals assigned to them and to maintain a balanced distribution of terminals among concentrators. The TA problem is a NP-complete combinatorial optimization problem. It means that the TA problem cannot be solved to optimality within polynomially bounded computation times.\nThis paper presents an application of a population-based optimization algorithm called the Bees Algorithm. The Bees Algorithm is inspired by the food foraging behaviour of honey bees [4] and uses a neighbourhood search method and a local search method to be able to locate the global minimum. The Bees Algorithm has been successfully applied to different optimization problems [5] including the training of neural networks for control chart pattern recognition, finding multiple feasible solutions to preliminary design problems, identification of wood defects, overcoming the local optimum problem of the K-means clustering algorithm, job scheduling, manufacturing cell formation, multi-objective optimization, optimizing the design of mechanical components, tuning a fuzzy logic controller for a robot gymnast, computer vision, image analysis and others.\nOur algorithm is based on the Bees Algorithm proposed by Pham et al. in [4]. Embedded in the Bees Algorithm we use a Local Search (LS) algorithm proposed by Bernardino et al. [6], which is used to improve the quality of the solutions.\nWe compare the performance of the Bees Algorithm with four algorithms: Tabu Search (TS) Algorithm, Local Search Genetic Algorithm (LSGA), Hybrid Differential Evolution (HDE) Algorithm and Hybrid Ant Colony Optimization (HACO) Algorithm, used in literature.\nThis paper is structured as follows. In Section 2 we present the definition of the TA problem; in Section 3 we describe the Bees Algorithm implemented; in Section 4 we discuss the computational results obtained and, finally, in Section 5 we report the conclusions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Terminal Assignment Problem", "text": "The TA problem involves the determination of which terminals will be serviced by each concentrator [1]. In the TA problem a communication network will connect N terminals and each with T i demand via M concentrators and each with C j capacity. No terminal's demand exceeds the capacity of any concentrator. A terminal site has a fixed and known location CT i (x,y). A concentrator site has also a fixed and known location CP j (x,y).\nIn this work, the solutions are represented using integer vectors. We use the terminal-based representation (see Fig. 1). Each position in the vector corresponds to a terminal. The value carried by the position i of the vector specifies the concentrator to which the terminal i is to be assigned.   [4]. The ABC algorithm also simulates the behaviour of bees, but uses a different algorithm model.\nA SI algorithm is a population based algorithm. It starts with a population of individuals (i.e. potential solutions). These individuals are then manipulated over many iteration steps by mimicking the social behaviour of insects or animals, in an effort to find the optimal solution in the space of the problem solution. A potential solution \"flies\" through the search space by modifying itself according to its past experience and its relationship with other individuals in the population and the environment [7].\nIn Bees Algorithm [5], after creating the initial population of bees (ns), bees are ranked according to the fitnesses of their sites. The best nss of the ns sites are classified as \"selected sites\" and the best nbs of the nss sites are classified as \"best sites\". The nbss bees are sent to the \"best sites\" and the nbbs bees are sent to the remaining (nss-nbs) \"selected sites\". These bees (nbss and nbbs) produce new sites in the neighbourhood of the \"selected sites\". Searches in the neighbourhood of the best sites, which represent more promising solutions, are made by recruiting more bees to follow them than the other selected bees. The remaining bees (ns-nss) are classified as scout and assigned randomly.\nOur algorithm is based on the Bees Algorithm proposed by Pham et al. in [4]. The basic form of the algorithm uses a neighbourhood search to explore around the selected sites. To improve the performance of the algorithm, we incorporate a LS algorithm proposed by Bernardino et al. [6]. The LS is used to improve the quality of the solutions in the population. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Initialization of parameters", "text": "The following parameters must be defined by the user (1) mi -number of iterations;\n(2) ns -number of initial bees; (3) nss -number of sites selected out of ns visited sites; (4) nbs -number of best sites out of nss selected sites; (5) nbbs -number of bees recruited for the best nbs sites; (6) nbss -number of bees recruited for the remaining selected sites (nss-nbs) and ( 7) nm -number of modifications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Initial Population", "text": "The initial population (P 0 ) can be created randomly or in a deterministic form. The deterministic form is based in the Greedy Algorithm proposed by Abuali et al. [8].\nThe Greedy Algorithm assigns terminals to the closest feasible concentrator.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of solutions", "text": "To evaluate how good a potential solution is relative to other potential solutions we use a fitness function. The fitness function returns a number (fitness value) that reflects how optimal the solution is. The fitness function is based on: (1) the total number of terminals connected to each concentrator (the purpose is to guarantee a balanced distribution of terminals among the concentrators); (2) the distance between the concentrators and the terminals assigned to them (the purpose is to minimize the distances between the concentrators and respective assigned terminals); (3) the penalization if a solution is not feasible (the purpose is to penalize the solutions when the total capacity of one or more concentrators is overloaded). The purpose is to minimize the fitness function.\nThe fitness function is based on the fitness function used in [2]: \n( ) { [ ] [] ( ) [ ] [] ( ){\n= \u2212 + \u2212 = = \u23aa \u23a9 \u23aa \u23a8 \u23a7 = + + = \u2211 \u2211 \u2211 = = \u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b + \u239f \u23a0 \u239e \u239c \u239d \u239b = \u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b \u2212 + \u239f \u23a0 \u239e \u239c \u239d \u239b = =", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Local Search", "text": "At the beginning of each iteration, our algorithm applies the LS procedure to the solutions in the population. The LS algorithm consists on applying a partial neighbourhood examination. We generate a neighbour by swapping two terminals between two concentrators c1 and c2 (randomly chosen). The algorithm searches for a better solution in the initial set of neighbours. If the better neighbour improves the actual solution, then the LS algorithm replaces the actual solution with the better neighbour. Otherwise, the algorithm creates another set of neighbours. In this case, one neighbour results in assigning one terminal of c1 to c2 or c2 to c1. The neighbourhood size is\nN(c1)*N(c2) or N(c1)*N(c2) + N(c1)+N(c2).\nThe LS algorithm consists on the following steps: The evaluation process is the step that consumes more time, which is usually the case in many real-life problems. Our LS procedure has some important improvements compared to the LS proposed by Bernardino et al. [6]. After creating a neighbour, the algorithm does not perform a full examination to calculate the new fitness value; it only updates the fitness value based on the modifications that were made to create the neighbour.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Select best bees", "text": "The bees that have the smallest fitnesses are chosen as \"selected bees\" (PB t ) and the sites visited by them are selected for neighbourhood search. )\n( 1 \u2212 = = \u2211 + =", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compute number of bees", "text": "In our implementation the algorithm computes the number of bees, which will be sent to a site, according to previously determined probabilities:\nnb i = number of bees sent to site i.\nnbss p nb ELSE nbbs p nb THEN nbs i IF i i i i * * = = <=", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Neighbourhood Search", "text": "The algorithm conducts searches in the neighbourhood of the selected sites, assigning more bees to search near to the best nbs sites.\nThe general mechanism of the neighbourhood search is represented in the next pseudo-code: ( distance(t2,c1) <= distance(t1,c1) or distance(t1,c2) <= distance(t2,c2)) ) THEN Assign t1 to c2 and t2 to c1 cond = false WHILE cond=true A neighbour is obtained by performing multiple moves which length is specified as nm (number of modifications). The algorithm performs nm modifications to find a new solution. First the algorithm chooses a random terminal t and searches the closest concentrator. If the concentrator has enough capacity and maintains a balanced distribution of terminals then the terminal t is assigned to the closest concentrator, closestC. Otherwise, the algorithm generates two random terminals, t1 and t2. The algorithm verifies the two concentrators, c1 and c2, assigned to them. If the concentrators have enough capacities and at least one of the concentrators is closest to the terminal that will be assigned, then the algorithm exchanges the terminals, t1 and t2 between the two concentrators, c1 and c2. The algorithm repeats this process until at least one exchange is made.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Select best bee", "text": "Only the bee with the smallest fitness will be selected to form the next population.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Assign remaining bees", "text": "In standard Bees Algorithm, the remaining bees in the population (ns-nss) are assigned randomly. In our implementation the scouts can be created using the Greedy Algorithm proposed by Abuali et al. [8].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Termination criteria", "text": "The algorithm stops when a maximum number of iterations (mi) is reached.\nFurther information on Bees Algorithm can be found in [9].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "In order to test the performance of our approach, we use a collection of TA instances of different sizes. We take 9 problems from literature [10].\nThe better results obtained with the Bees Algorithm use ns between 10 and 50, nss between ns/2 and ns, nbs between nss/2 and nss, nbss between 30 and 100 and nbbs between 30 and 100. These parameters were experimentally considered good and robust for the problems tested.\nSmall populations are very desirable for reducing the required computational resources. The Bees Algorithm has a good performance using initially a small population (Fig. 3).\nFor parameter nm, the number of modifications nm between [N/20...N/5] has been shown experimentally to be more efficient (Fig. 4). In our experiments nm was set to {0, 1, 2, \u2026, N}. A high nm has a significant impact on the execution time (Fig. 4). A small nm did not allow the system to escape from local minima, because the resulting solution was in most cases the same as the initial permutation. In general, the experiments have shown that the proposed parameter setting is very robust to small modifications.\nTo compare our results we consider the results produced with the Local Search Genetic Algorithm, the Tabu Search Algorithm, the Hybrid Differential Evolution Algorithm and the Hybrid Ant Colony Optimization Algorithm. The GA was first applied to TA by Abuali et al. [8]. The GA is widely used in literature to make comparisons with other algorithms. TS was applied to this problem by Xu et al. [11] and Bernardino et al. [10]. We compare our algorithm with the TS, LSGA, HDE and HACO algorithms proposed by Bernardino et al. [10][12][6] [13], because they use the same test instances.\nTable 1 presents the best-obtained results with Bees Algorithm, TS, LSGA, HDE and HACO. The first column represents the number of the problem (Prob) and the remaining columns show the results obtained (BestF -Best Fitness, Time -Run Times) by the five algorithms. The algorithms have been executed using a processor Intel Core Duo T2300. The run time corresponds to the average time that the algorithms need to obtain the best feasible solution.\nTable 2 presents the average fitnesses and standard deviations. The first column represents the number of the problem (Prob) and the remaining columns show the results obtained (AvgF -Average Fitness, Std -Standard Deviation) by the five algorithms. To compute the results in table 2 we use 300 iterations/generations for instances 1-4, 500 for instance 5, 1000 for instance 6, 1500 for instance 7 and 2000 for instances 8-9. The parameters of the Bees Algorithm are set to ns=10, nss=10, nbs=5, nbss=10 and nbbs=30 and nm=6. The HDE and LSGA were applied to populations of 200 individuals. The HACO was applied to populations of 30 individuals. The initial solutions were created using the Greedy Algorithm.   The values presented have been computed based on 50 different executions (50 best executions out of 100 executions) for each test instance.\nThe five algorithms reach feasible solutions for all test instances. In comparison, the Bees Algorithm presents a better average fitness for larger instances. The Bees Algorithm can reach the best-known solutions for all instances. HDE, HACO and LSGA can also find the best-known solutions, but in a higher execution time.\nAs it can be seen, for larger instances, the standard deviations and the average fitnesses for the Bees Algorithm are smaller. It means that the Bees Algorithm is more robust than TS, LSGA, HDE and HACO.\nAll the statistics obtained show that the performance of Bees Algorithm is superior to TS, LSGA, HDE and HACO.", "publication_ref": [], "figure_ref": ["fig_28", "fig_29", "fig_29"], "table_ref": ["tab_2", "tab_5"]}, {"heading": "Conclusions", "text": "In this paper we present a Bees Algorithm to solve the Terminal Assignment Problem. The performance of' Bees Algorithm is compared with four algorithms from the literature, namely, LSGA, TS, HDE and HACO. The Bees Algorithm is a new evolutionary optimization technique, capable of performing simultaneous local and global search.\nRelatively to the problem studied, the Bees Algorithm presents better results. The computational results show that Bees Algorithm had a stronger performance, improving the results obtained by previous approaches. Moreover, in terms of standard deviation, the Bees Algorithm also proved to be more stable and robust than the other algorithms.\nExperimental results demonstrate that the proposed Bees Algorithm is an effective and competitive approach in composing satisfactory results with respect to solution quality and execution time for the Terminal Assignment Problem.\nIn literature the application of Bees Algorithm for this problem is nonexistent, for that reason this article shows its enforceability in the resolution of this problem.\nThe implementation of parallel algorithms will speed up the optimization process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In recent years the significance of connection between users (clients) and access points of wireless communication networks has been increased (e.g., [8], [9], [12]). This paper addresses assignment of users to wireless telecommunication network access points of a wireless network. Fig. 1 illustrates users and access points of a wireless telecommunication network.\nHere the considered problem is firstly based on multicriteria generalized assignment/allocation model. Generalized assignment problems have been intensively studied (e.g., [1], [7], [16], [20]). In this article multicriteria generalized assignment problem is firstly examined with application to wireless telecommunication networks. A set of examined criteria involves the following: (i) maximum of bandwidth, (ii) number of users which are under service at the same time, (iii) network reliability requirements, etc. Multicriteria assignment problem is formulated (NP-hard [5]) and heuristic is proposed. Two problem versions are examined: (i) each user is connected to the only one access points, (ii) a user can be connected to several access points. A numerical example illustrates the approach. Authors MatLab programs (http://www.mathworks.com/) were used for computing. \nUsers 1 0 ( ) v v v v $ $ $ $ $ $ X E z \u00cb\u00a8\u00a8\u00a8B E E z", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Problem Statement", "text": "The following is assumed: (1) there is a hilly terrain; (2) access points of the wireless telecommunication network are distributed over the terrain; (3) users can be distributed arbitrarily over there; (4) each user requires an access to a access point of the wireless telecommunication network (i.e., assignment) and each assignment \"user-access point\" is described by a set of parameters; and (5) access points and users have coordinates (x, y, z) at the terrain map.\nOur engineering problem consists in the following: Assign maximal number of users to access points of wireless telecommunication network to maximize a generalized reliability of connection, quality of signal propagation, quality of usage of frequency spectrum, QoS, quality of information transmission protection while taking into account requirements of users: (a) frequency spectrum, (b) level of information transmission protection, etc. Now let us consider a basic problem statement. Let \u03a8 = {1, ..., i, ..., n} be a set of users and \u0398 = {1, ..., j, ..., m} be a set of access points. Each user i is described by parameter vector (x i , y i , z i , f i , k i , p i , r i , d i ), where components are as follows: coordinates of user (x i , y i , x i ), parameter corresponding to required frequency bandwidth (1 Mbit/s ... 10 Mbit/s) f i , maximal possible number of access points for connection k i (this parameter is used for an extended 2nd problem version), required level of CoS (class, priority) p i , required reliability of information transmission r i , required level of information protection d i . Each access point is described as follows: (x j , y j , z j , f j , n j , r j , d j ), where coordinates of access point (x j , y j , x j ), parameter corresponding to maximal possible traffic (i.e., maximum of possible bandwidth) f j , maximal possible number of users under service n j , reliability of channel for data transmission r j , parameter of information protection d j . Table 1 contains description of scales for parameters above.\nAs a result, each pair \"user-access point\" can be described as well: (\u2200(i, j), i \u2208 \u03a8, j \u2208 \u0398) can be described as well by the following parameter: (1) level of reliability r ij , (2) parameter of distance and existence of a barrier (i.e, quality of signal propagation) \u03b2 ij , (3) parameter of using a bandwidth f ij , (4) level of QoS (class or priority) p ij , (5) parameter of data protection d ij . Thus, the following vector parameter is obtained  [1,3] p i = 1: all user requirements have to be satisfied, p i = 2: half of the required frequency bandwidth may be used, corresponding users have the second level of priority, reliability can be decreased; p i = 3: connection of user can be realized in the case of any bandwidth. r i (user) [1,10] r i = 1: information can be lost (up to 20 %, e.g., movings), r i = 10: information cannot be lost (maximal reliability required by user). r j (access point) [1,10] r j = 1: information can be lost (up to 20 %), r j = 10: information cannot be lost. d i (user) [1,10] d i = 1: information is not confidential, d i = 5: medium level of information protection d i = 10: information is confidential. d j (access point) [1,10] d j = 1: trivial tools for data protection, d j = 10: the highest level of data protection.\n\u2200(i, j), i \u2208 \u03a8 , j \u2208 \u0398: c ij = (r ij , \u03b2 ij , f ij , p ij , d ij ).\nThe assignment of user i to access point j is defined by Boolean variable x ij (x ij = 1 in the case of assignment i to j and x ij = 0 otherwise). Thus, the assignment solution (\u03a8 \u21d2 \u0398) is defined by Boolean matrix X = ||x ij ||, i = 1, n, j = 1, m. Now let us consider computing rules for assignment user i (\u2200i \u2208 \u03a8 ) to access point j (\u2200i \u2208 \u0398):\n(1) Reliability: r ij = min{r i , r j }. \n\u03b2 ij = \u23a7 \u23a8 \u23a9 0, (l ij > L max /2)&(e ij = 1), 5, (l ij < L max /2)&(e ij = 1) or (l ij \u2208 (L max /2, L max ])&(e ij = 0), 10, (l ij < L max /2)&(e ij = 0). Parameter of \"connectivity\" by \u03b2 ij is: \u03be \u03b2 ij = 0, if \u03b2 ij = 0, 1, otherwise. (5) QoS (priority): p ij = p i . (6) Required/possible bandwidth: f ij (at initial stage f ij = f i ). Three cases are examined: (a) p ij = 1: f ij = f i , (b) p ij = 2: f ij = f i , if (max j f j \u2212 1 m m j=1 f j ) \u2265 f i , max j f j \u2212 1 m m j=1 f j , if (max j f j \u2212 1 m m j=1 f j ) < f i , (c) p ij = 3: f ij = f i , if (max j f j \u2212 1 m m j=1 f j ) \u2265 f i , max j f j \u2212 1 m m j=1 f j , if (max j f j \u2212 1 m m j=1 f j ) < f i .\nHere it is assumed, that two kinds of traffic exist: (i) elastic (users service uses only an accessible bandwidth), (ii) non-elastic (user has his requirements to a certain bandwidth for users services).\n(7) Parameter of protection for data transmission: d ij . Three cases are under examination:\n(a) p ij = 1: d ij = d j , if d j \u2265 d i , 0, if d j < d i ; (b) p ij = 2: d ij = d j , if d j \u2265 d i /2, 0, if d j < d i /2; (c) p ij = 3: d ij = d j .\nParameter of \"connectivity\" by d ij is:\n\u03be d ij = 0, if d ij = 0, 1, otherwise. In addition, \u2200i \u2208 \u03a8 is defined \u0398 i \u2286 \u0398. Clearly, if |\u0398 i | = 0\n, user i can be deleted from the future examination. This situation corresponds to parameters \u03be \u03b2 ij , \u03be d ij . Fig. 2 depicts data processing.\n{j, x j , y j , z j , r j , f j , p j , d j , } c {i, x i , y i , z i , r i , f i , p i , d i , } c { l ij , r ij , e ij , \u03b2 ij , f ij , p ij , d ij , \u03be \u03b2 ij , \u03be d ij } c c ij = (r ij , \u03b2 ij , f ij , p ij , d ij ) c c ij c \u0398 i", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": ["tab_2"]}, {"heading": "Fig. 2. Data processing", "text": "The following set of generalized objective functions (criteria) is used (a simplified additive versions):\n(i) total reliability R(X) = n i=1 m j=1 r ij x ij , (ii) total parameter of quality for signal propagation B(X) = n i=1 m j=1 \u03b2 ij x ij , (iii) generalized quality of usage of frequency spectrum F (X) = n i=1 m j=1 f ij x ij , (iv) generalized parameter of QoS P (X) = n i=1 m j=1 p ij x ij , (v) gen- eralized parameter of protection for information transmission D(X) = n i=1 m j=1 d ij x ij .\nThus, vector-like quality of solution X is:\nC(X) = (R(X), B(X), F (X), P (X), D(X)).\nFurther, let us consider constraints:\n1. For bandwidth of access point j:\nn i=1 f ij x ij \u2264 f j \u2200j \u2208 \u0398,\nwhere f j is maximum of bandwidth for access point j.\n2. For number of users in each access point j: n i=1 x ij \u2264 n j \u2200j \u2208 \u0398, where n j is the maximum of users which are assigned to access point j.\n3. For assignment of users to access point: (i) version 1 (each user is assigned to the only one access point):\nj\u2208\u0398i x ij \u2264 1 \u2200i \u2208 \u03a8 or (ii) version 2 (a user can be assigned to several access points):\nj\u2208\u0398i x ij \u2264 k i \u2200i \u2208 \u03a8 .\nIt is reasonable to point out connection of a user to several access points can lead to the following: (a) transmission of data through several access points (i.e., different routes) can provide an increased reliability, (b) initial information can be divided into part which are transmitted through different access points (i.e., routes) with synthesis at a destination point and this approach can provide an increased transmission protection. Finally the problem (version 1) is:\nmax R(X) = n i=1 j\u2208\u0398i r ij x ij , max B(X) = n i=1 j\u2208\u0398i \u03b2 ij x ij , max F (X) = n i=1 j\u2208\u0398i f ij x ij , max P (X) = n i=1 j\u2208\u0398i p ij x ij , max D(X) = n i=1 j\u2208\u0398i d ij x ij s.t. n i=1 f ij x ij \u2264 f j \u2200j \u2208 \u0398, n i=1 x ij \u2264 n j \u2200j \u2208 \u0398, j\u2208\u0398i x ij \u2264 1 \u2200i \u2208 \u03a8, x ij = 0 \u222a 1, \u2200 i = 1, n, \u2200 j = 1, m, x ij = 0, \u2200 i = 1, n, j \u2208 {\u0398 \\ \u0398 i }.\nEvidently, in version 2 another constraint 3 is used:\nj\u2208\u0398i x ij \u2264 k i \u2200i \u2208 \u03a8 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Solving Scheme", "text": "The obtained combinatorial problem is NP-hard ( [5], [6]). In recent decades active research projects have been conducted in the field of multicriteria assignment/ allocation (e.g., [4], [11], [14], [15], [17], [18]). Usually the following approaches are used:\n(1) enumerative methods (e.g., branch-and-bound methods) (e.g., [1], [16], [17]);\n(2) interactive (man-machine) procedures [11];\n(3) reducing an initial optimization model to a simplified problem, for example, reducing a multicriteria problem to an one-criterion problem and usage of efficient (i.e., polynomial) algorithms, e.g., Hungarian method, etc. (e.g., [10]);\n(4) heuristics including the following: (a) simple greedy algorithms, (b) approximation algorithms, (c) random algorithms, (d) meta-heuristics (e.g., hybrid algorithms), (e) variable neighborhood search VNS, (f) genetic algorithms, evolutionary multiobjective optimization (e.g., [3], [15], [19]); etc.\nIn this work three solving schemes were under examination: Scheme 1. An enumerative algorithm. Scheme 2. Two-stage heuristic: (i) simplification of the problem that was based on mapping of parameter vector for connection \"user-access point\" to an ordinal scale, here multicriteria ranking is used as a modification of ELECTRE technique, (ii) solving the obtained one-criterion assignment problem (e.g., greedy algorithm). Scheme 3. Three-stage heuristic: (i) solving of an initial multicriteria problem for each criterion to generate a corresponding set of solutions, (ii) unification of the obtained solution sets and revelation of Pareto-efficient solutions, (iii) analysis of the obtained Pareto-efficient solutions and selection of the best one (or ones) (here additional rules and/or expert judgment can be used).\nFurther results of using Scheme 2 above are described. It is reasonable to note, the considered assignment problem may have many applications for dynamical modes of telecommunication networks (i.e., Ad Hoc networks, mobile networks, mesh networks) and usually there is a very limited time interval for the solving process. Thus, it is necessary to use simple and very efficient heuristics (e.g., greedy algorithms).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Numerical Example", "text": "The considered example consists of 22 users and 6 access points (Fig. 3, Fig. 4). Tables 2 and 3 contain initial information for users and access points.   Computing of integrated parameters of correspondence c ij (Table 5) is based on mapping of vector estimate c ij (Tables 5 and 6) into ordinal scale [1,3] (3 corresponds to the best level of correspondence, multicriteria ranking based on ELECTRE technique is used). In addition to Tables 5 and 6, sets {\u0398 i } ( c ij = 0 if j \u2208 {\u0398 \\ \u0398 i } ) are taken into account. Thus, a simplified one-criterion assignment problem is solved (version 1, i.e., k i = 1 \u2200i = 1, n):\nVectors (\u03be \u03b2 ij , \u03be d ij ) (Table 4) define sets {\u0398 i } (i = 1, n): \u0398 1 = {1, 3, 5}, \u0398 2 = {1, 2, 3, 5}, \u0398 3 = {1, 2, 3, 4, 5}, \u0398 4 = {1, 2, 3, 4}, \u0398 5 = {2, 4}, \u0398 6 = {2, 4, 6}, \u0398 7 = {1, 3, 4, 5}, \u0398 8 = {1, 2, 3, 4, 5, 6}, \u0398 9 = {1, 2, 3, 4, 5, 6}, \u0398 10 = {2, 3, 4, 5, 6}, \u0398 11 = {2, 3, 4, 5, 6}, \u0398 12 = {1, 3, 4, 5}, \u0398 13 = {1, 3, 4, 5}, \u0398 14 = {1, 3, 4, 5, 6}, \u0398 15 = {2, 3, 4, 6}, \u0398 16 = {1, 3, 4, 5}, \u0398 17 = {1, 5}, \u0398 18 = {3, 4, 5, 6}, \u0398 19 = {2, 3, 4, 6}, \u0398 20 = {1, 3, 4, 5, 6}, \u0398 21 = {1,\n\u00a4 \u00a5 u 17 \u00a7 \u00a6 \u00a4 \u00a5 u 13 \u00a1 \u00a1 e e r 3 f i r r \u00a1 \u00a1 \u00a1 e e r r \u00a7 \u00a6 \u00a4 \u00a5 u 12 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a7 \u00a6 \u00a4 \u00a5 u 1 \u00a7 \u00a6 \u00a4 \u00a5 u 2 \u00a1 \u00a1 e e r 1 f i d d d d \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 7 \u00a7 \u00a6 \u00a4 \u00a5 u 16 \u00a7 \u00a6 \u00a4 \u00a5 u 20 \u00a7 \u00a6 \u00a4 \u00a5 u 21 \u00a7 \u00a6 \u00a4 \u00a5 u 14 \u00a7 \u00a6 \u00a4 \u00a5 u 9 \u00a7 \u00a6 \u00a4 \u00a5 u 10 \u00a1 \u00a1 e e r 4 f i \u00a1 \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 11 \u00a7 \u00a6 \u00a4 \u00a5 u 15 \u00a7 \u00a6 \u00a4 \u00a5 u 19 \u00a7 \u00a6 \u00a4 \u00a5 u 18 \u00a7 \u00a6 \u00a4 \u00a5 u 22 \u00a1 \u00a1 e e r 6 f i d d \u00a1 \u00a1 \u00a1 \u00a1 \u00a1 r r r \u00a7 \u00a6 \u00a4 \u00a5 u 8 \u00a7 \u00a6 \u00a4 \u00a5 u 3 \u00a7 \u00a6 \u00a4 \u00a5 u 4 \u00a7 \u00a6 \u00a4 \u00a5 u 5 \u00a7 \u00a6 \u00a4 \u00a5 u 6 \u00a1 \u00a1\nmax n i=1 j\u2208\u0398i c ij x ij s.t. n i=1 f ij x ij \u2264 f j \u2200j \u2208 \u0398, n i=1 x ij \u2264 n j \u2200j \u2208 \u0398, j\u2208\u0398i x ij \u2264 1 \u2200i \u2208 \u03a8, x ij = 0 \u222a 1, \u2200 i = 1, n, \u2200j = 1, m, x ij = 0, \u2200 i = 1, n, j \u2208 {\u0398 \\ \u0398 i }.\nIn version 2 another constraint 3 is used:\nj\u2208\u0398i x ij \u2264 k i \u2200i \u2208 \u03a8 .\nResults of the solving process are: (i) Fig. 3 (version 1) and (ii) Fig. 4 (version 2).     \ni x i y i z i f i k i p i r i d i\n4. Matrix ||(\u03be \u03b2 ij , \u03be d ij )|| i Access points j 1 2 3 4 5 6 1 1, 1 0, 1 1, 1 0, 1 1, 1 0, 1 2 1, 1 1, 1 1, 1 0, 1 1, 1 0, 1 3 1, 1 1, 1 1, 1 1, 1 1, 1 0, 0 4 1, 1 1, 1 1, 1 1, 1 0, 1 0, 1 5 0, 1 1, 1 0, 1 1, 1 0, 1 0, 1 6 0, 1 1, 1 0, 1 1, 1 0, 1 1, 1 7 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 8 1, 1 1, 1 1, 1 1, 1 1, 1 1, 1 9 1, 1 1, 1 1, 1 1, 1 1, 1 1, 1 10 0, 1 1, 1 1, 1 1, 1 1, 1 1, 1 11 0, 1 1, 1 1, 1 1, 1 1, 1 1, 1 12 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 13 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 14 1, 1 0, 1 1, 1 1, 1 1, 1 1, 1 15 0, 1 1, 1 1, 1 1, 1 0, 1 1, 1 16 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 17 1, 1 0, 0 1, 0 1, 0 1, 1 0, 0 18 0, 1 0, 1 1, 1 1, 1 1, 1 1, 1 19 0, 1 1, 1 1, 1 1, 1 0, 1 1, 1 20 1, 1 0, 1 1, 1 1, 1 1, 1 1, 1 21 1, 1 0, 0 1, 1 1, 0 1, 1 1, 0 22 0, 1 0, 1 0, 1 1, 1 0, 1 1, 1\ni Access points j 1 2 3 4 5 6 1 3 0 3 0 3 0 2 2 1 1 0 2 0 3 1 1 1 1 1 0 4 1 1 2 1 0 0 5 0 1 0 1 0 0 6 0 1 0 1 0 1 7 2 0 2 1 2 0 8 1 1 1 1 1 1 9 2 1 1 1 1 1 10 0 1 2 1 3\n\u00a4 \u00a5 u 17 \u00a7 \u00a6 \u00a4 \u00a5 u 13 \u00a1 \u00a1 e e r 3 f i r r d d \u00a1 \u00a1 \u00a1 e e r r \u00a7 \u00a6 \u00a4 \u00a5 u 12 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a7 \u00a6 \u00a4 \u00a5 u 1 \u00a7 \u00a6 \u00a4 \u00a5 u 2 \u00a1 \u00a1 e e r 1 f i d d d d \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 7 \u00a7 \u00a6 \u00a4 \u00a5 u 16 \u00a7 \u00a6 \u00a4 \u00a5 u 20 \u00a7 \u00a6 \u00a4 \u00a5 u 21 \u00a7 \u00a6 \u00a4 \u00a5 u 14 \u00a7 \u00a6 \u00a4 \u00a5 u 9 \u00a7 \u00a6 \u00a4 \u00a5 u 10 \u00a1 \u00a1 e e r 4 f i \u00a8\u00a1 \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 11 \u00a7 \u00a6 \u00a4 \u00a5 u 15 \u00a7 \u00a6 \u00a4 \u00a5 u 19 \u00a7 \u00a6 \u00a4 \u00a5 u 18 \u00a7 \u00a6 \u00a4 \u00a5 u 22 \u00a1 \u00a1 e e r 6 f i d d \u00a1 \u00a1 \u00a1 \u00a1 \u00a1 r r r \u00a7 \u00a6 \u00a4 \u00a5 u 8 \u00a7 \u00a6 \u00a4 \u00a5 u 3 \u00a7 \u00a6 \u00a4 \u00a5 u 4 \u00a7 \u00a6 \u00a4 \u00a5 u 5 \u00a7 \u00a6 \u00a4 \u00a5 u 6 \u00a1 \u00a1 e e r 2 f i r r e e r r \u00a1 \u00a1 e e r 5 f i t t t \u00a1 \u00a1 \u00a1 d d \u00a1 \u00a1 \u00a1 e e", "publication_ref": [], "figure_ref": ["fig_28", "fig_29", "fig_28", "fig_29"], "table_ref": ["tab_5", "tab_56", "tab_56", "tab_56"]}, {"heading": "Conclusion", "text": "The suggested approach is the first step for using multicriteria combinatorial problems in assignment of users to access points of wireless telecommunication networks. Our main attention was targeted to a new problem formulation and a simple solving scheme. Clearly the considered approach may be applied in other domains for connection of users with service centers (e.g., electricity systems, maintenance in manufacturing, environmental monitoring). It is reasonable to point out the following perspective research directions:\n1. Problem statement: (1.1) problem parts: (a) procedures for computing of parameters, for example: integrated parameters (barrier-distance \u03b2 ij ), quality of signal propagation (f ij ); (b) criteria, (c) constraints; (1.2) possibility to use other management modes, for example, multi-hop schemes, P2P; (1.3) mobility of access points (and users); and (1.4) on-line extension of users set.\n2. Models: (2.1) more complicated optimization models (e.g., under uncertainty); (2.2) taking into account \"neighbor\" assignments (possible collisions, influence), here quadratic assignment problem [2] or an approach on the basis of hierarchical morphological design [14] can be used. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Recently, a lot of work has been carried out in bio-inspired computational optimization, especially in continuous domains. Among the methods set up, we can cite Evolutionary computation [1], [2] and nature inspired methods such as Ant Colony Optimization, ACO. The latter was initially developed for combinatorial optimization [3], [4] and has been recently adapted to continuous optimization [5]. Ant Colony Optimization is inspired by the ants foraging behavior and requires that the problem is partitioned into a finite set of components these being intermediate targets before reaching the ultimate goal. The solution is generally the minimum-cost strategy followed by the agent (ant) to reach the target. In Ant Colony Optimization for continuous optimization, ACOR, the partition of the problem into finite set is given by the intrinsic search space decomposition into the different dimensions.\nACOR is a population based algorithm, therefore, for each ant two main steps must be performed: the ant based solution construction and the pheromone update. The first step comprises a number of sub-operations such as: for each search space dimension: a) probabilistic choice of one base solution from an archive of best-so-far solutions; b) perturbation of the relevant parameter following a gaussian probability distribution. The second step simply consists in the archive update. Indeed, the Gaussian distribution used for the perturbation of each parameter is built using the information derived from the entire archive. The closer the solutions are in a given dimension, the smaller the standard deviation. With respect to this issue, the authors have noticed in some cases a limited ability to perform exploration. This is especially true when multimodal functions must be minimized. Other authors [6], [7] have already observed this behavior with evolutionary programming. In particular, Lee and Yao [6] propose an adaptive LEP, L\u00e9vy based Evolutionary Programming algorithm in which the Gaussian mutation is replaced by a L\u00e9vy distributed mutation. In this paper the authors propose to modify the ACOR algorithm by replacing the Gaussian mutation with a L\u00e9vy distributed mutation, called ACOR L . The effects are similar to those attained in [7], although different performances are observed in certain cases. The application section reports tests over a set of test functions taken from [6] as well as a real world application in the field of composite laminates buckling load maximization. Composite laminates are used in many fields of engineering due to their outstanding mechanical and structural characteristics: low weight, high stiffness and strength. Furthermore the design of a laminate could be easily accomplished changing stacking sequence, fiber orientation, ply tickness and the material used by means of standard industrial processes. Design optimization of composite structures usually leads to multimodal search spaces and different approaches were adopted to deal with this optimization problem, among others gradient based methods [8], genetic algorithms [9], simulated annealing [10], genetic algorithm and pattern search algorithm [11].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The L\u00e9vy Probability Distribution", "text": "In the field of global optimization, various phenomena have been already studied in the literature such as thermodynamics and evolution in the eighties. More recently, one of the laws governing enhanced diffusive processes called L\u00e9vy flights has been considered for modeling the perturbation mechanism in global optimization. In this paper, the L\u00e9vy distribution is considered for generating step size during the ACOR search. This distribution has the property of generating points that can be far from the starting ones. This can be better understood considering that Gaussian white noises random variables are symmetric about their mean and do not allow any skewness of the distribution or unilateral random input. The only distribution that allows such a great variability and obeys to a generalized central limit theorem is the so called \u03b1-stable L\u00e9vy distribution, introduced by the mathematician Paul L\u00e9vy about 1920 [12]. Stable distributions are characterized by heavy-tailed probability density function that causes infinite variance and are defined by four coefficients [13]. A random variable X is said to have a \u03b1-stable distribution if there are parameters 0 < \u03b1 \u2264 2, \u03c3>0 , 1 \u2264 \u03b2 \u2264 1 , \u03bc \u2208 R such that its characteristic function \u03c6 X (\u03b8) has the form:\n\u03c6 X (\u03b8) = exp{\u2212\u03c3 \u03b1 |\u03b8| \u03b1 1 \u2212 i\u03b2(sign(\u03b8))tan \u03c0\u03b1 2 + i\u03bc\u03b8}, if\u03b1 = 1 exp{\u2212\u03c3|\u03b8| 1 + i\u03b2 2 \u03c0 (sign(\u03b8))ln|\u03b8| + i\u03bc\u03b8}, if\u03b1 = 1 (1)\nThe four parameters affect the shape of the distribution in an essential way and it is common to introduce an appropriate notation to take them into account. We denote with the symbol X \u223c S \u03b1 (\u03c3, \u03b2, \u03bc) a stable random variable with assigned parameters characterizing (1). Some properties of the stable distribution, not proved but straightforward from the definition of the characteristic function, will help to better clarify their meaning.\nAddition of constant. Let X \u223c S \u03b1 (\u03c3, \u03b2, \u03bc) and let a be a real parameter. Then, adding a to X gives a random variable X+a with distribution X \u223c S \u03b1 (\u03c3, \u03b2, \u03bc+a).\nThe parameter \u03bc is thus a shift parameter. The parameter \u03bc cannot be identified in general as the mean of the distribution, because for 0< \u03b1 <1 , the mean of the variable X \u223c S \u03b1 (\u03c3, \u03b2, \u03bc) diverges. Only in the interval 1< \u03b1 \u22642, the two concepts actually coincide.\nMultiplication by a constant. Let be X \u223c S \u03b1 (\u03c3, \u03b2, \u03bc) and a real. Then, multiplying X by a gives a random variable aX with distribution\nX \u223c S \u03b1 (|a|\u03c3, Sign(a)\u03b2, a\u03bc) if \u03b1 = 1 and X \u223c S \u03b1 (|a|\u03c3, Sign(a)\u03b2, a\u03bc \u2212 (2/\u03c0)a(Log|a|)\u03c3\u03b2) if \u03b1 = 1. \u03c3 is called scale parameter. When \u03b1 = 2 , the characteristic function (1)\nbecomes the characteristic function of a random variable normal distributed with mean \u03bc and variance 2\u03c3 2 , indicated as X \u223c N (\u03bc, \u221a 2\u03c3). In general, the scale parameter does not coincide with the standard deviation, that, for 0 < \u03b1 < 2 is infinite.\nIn Figure 1, two trajectories following the normal (\u03b1=2) and the L\u00e9vy distribution (\u03b1=1.6) are reported. The trajectories are generated by adding a L\u00e9vy distributed quantity having zero mean and \u03c3=1 to x 1 and x 2 .\nThe normal path in panel (a) is sample continuous (similar, but not to be confused with the continuity of a function, and descending from the application of the Kolmogorov criterion [14] while in panel (b) the L\u00e9vy path is not sample continuous as long jumps and clustered small fluctuations are present alternate with clustered small fluctuation. This is a consequence of the heavy tails of the L\u00e9vy distribution and it is influenced by the stability index: indeed, if a goes to zero, jumps become bigger and fluctuations vanish; conversely, if \u03b1=2 the continuous (no jumps) normal behaviour is attained. Then, loosely speaking, we could say that L\u00e9vy paths tend to escape from a bounded region, while normal paths localize.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Function Optimization Using ACOR L", "text": "As said in the introduction, Ant Colony Optimization was first proposed for combinatorial optimization problems. Since its emergence many attempts have been made to use it for tackling continuous problems. More recently, M.Dorigo and K. Socha [5] have proposed the natural extension of the ACO algorithm to continuous domains, ACOR L . The idea that is central to the way ACOR works is the incremental construction of solutions based on the biased (by pheromone) probabilistic choice of solution components. At each construction step, the ant chooses a Probability Density Function. Details about the ACOR implementation are out of the scope of this paper, for further details please refer to [5]. In what follows, the main steps of the ACOR L algorithm are briefly outlined.\nCreate an archive T of k solutions, T = {x 1 , x 2 , ...x k }. Where\nx r = [x r 1 , x r 2 , .x r N ]\n. Order the solutions of the archive T according to their objective function value. Given a decision variable x i , i=1,.N, an ant constructs a solution by performing N construction steps. At construction step i, the ant chooses a value for the variable x i . At this construction step, only the information related to the i-th dimension is used. Select a base solution r from the archive T to be modified according to the following probability:\np r = \u03c9 r k j=1 \u03c9 j (2\n)\nwhere\n\u03c9 r = 1 qk \u221a 2\u03c0 e \u2212 (r\u22121) 2 2q 2 k 2(3)\nwhich essentially defines the weight \u03c9 r to be a value of the Gaussian function with argument r, mean 1 and standard deviation qk, where q is a parameter of the algorithm. When q is small, the best-ranked solutions are strongly preferred, and when it is large, the probability becomes less dependent on the rank of the solution.\nAll the components x r i for i:=1 to N of the chosen r-th solution in the following steps are perturbed following the L\u00e9vy distribution. As already pointed out, the L\u00e9vy distribution is characterised by four parameters: the scale parameter, \u03c3, the skewness parameter, \u03b2, the shift parameter \u03bc and the \u03b1 parameter.\nThe first is defined as:\n\u03c3 r i = \u03be k e=1 |x e i \u2212 x r i | k \u2212 1 real (4\n)\nwhere \u03be is a parameter user-defined in the algorithm ranging from 0 and 1. The higher the value of this parameter the slower the convergence speed. The third parameter, \u03bc, is the value of the i-th parameter of the base solution itself (x r i ). The second parameter is set to 0, namely no dissymmetry of the probability density function about the shift value \u03bc. The fourth parameter \u03b1 is a control parameter set by the user and its value ranges between 0 and 2. So the i-th parameter is newly determined. The same procedure is repeated for all the N parameters. At the end, once the solution is entirely constructed, it is evaluated and if better than any of the solutions in T, it is included into the archive set T. From what was said above, if the used defined parameter \u03b1 is set to 2, the ACOR L coincides with ACOR. The proposed algorithm, ACOR L , for function optimization works as follow.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1. ACOR L Pseudocode", "text": "Random creation of the solutions archive of size k Choice of \u03be,q,\u03b1,\u03bc while not(termination) do for z=1 to m do Choice of one solution from the archive using (2) for all parameter (Ant construction) do Calculate standard deviation \u03c3 i l using (4) Modify the i-th parameter in the following way: \nx i = x i + S\u03b1(\u03c3 i l ,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results and Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mathematical Test Functions", "text": "Some applications have been carried out on a test suite of 5 mathematical test functions taken from [6]. In all cases, the objective functions have to be minimized.\nExcept than f 1 that has no local minima, all the other functions have several local minima (f 5 , f 6 , f 7 ) or some local minima (f 11 ). In order to compare the attained results with those attained using classical ACOR, for both algorithms the following parameters values taken from [5] have been chosen: parameter \u03be=0.85; parameter q= 0.0001; archive size: 50; number of ants: 2. Parameter \u03b1 has been set to different values in different runs, in order to assess its influence on the efficiency of the algorithm. Thus the following values of \u03b1 have been taken 2.0, 1.8, 1.6, 1.4, 1.2, 1.0, 0.8. The termination condition is based on the maximum number of function evaluations, which has been set to 150000 for f 1 , f 5 , f 6 , f 7 , and 3000 for f 11 . Table 1 reports the functions f 1 , f 5 , f 6 , f 7 and f 11 [6]. Table 2 shows the experimental results of a comparison between the performance  It is clear from the table above that ACOR L performed no worse or better in a statistically meaningful sense than ACOR on all benchmark functions having local minima. This is clearly shown by the values of the t-test.\nf6 = \u221220exp[\u22120.2 1 N N i=1 x 2 i ] \u2212 exp[ 1 N N i=1 cos(2\u03c0xi)] 30 [-32, 32] N f7 = 1 4000 N i=1 x 2 i \u2212 N i=1 cos( x i \u221a i ) + 1 30 [-600, 600] N f11 = (1 + (x1 + x2 + 1) 2 (19 \u2212 14x1 + 3x 2 1 \u2212 14x2 + 6x1x2 + 3x 2 2 ))x 2 [-2, 2] N (30 + (2x1 \u2212 3x2) 2 )(18 \u2212 32x1 + 12x 2 1 + 48x2 \u2212 36x1x2 + 27x 2 2 )\nFig. 2 shows the optimization processes for ACOR L (ACOR is ACOR L with \u03b1=2) with different values of \u03b1 over function f 5 , each point represents the mean or the standard deviation of the best so far solution at each iteration for a sample of 100 independent runs. The acronym FES stays for function evaluations.\nAfter a fast descent of the mean, ACOR gets stuck into local minima and does not improve its performance compared to \u03b1=1.6 and \u03b1=1.8. These both improve their mean value till the very end of the run, reaching lower values of the objective function. The behaviour is confirmed looking at the standard deviation. In ACOR, the standard deviation first increases allowing a wide exploration of the search space, then decreases and after about 30000 evaluations it does not change anymore. At the same time, it can be observed that the mean value does not move anymore. For different values of \u03b1, and in particular for \u03b1=1.8, the standard deviation decreases gradually, while the search process approaches the minimum.\nThe behaviour of the standard deviation observed for \u03b1=2 shows a limited tendency to diversification of the attainable solutions. Higher diversification can be observed for \u03b1=1.8 and \u03b1=1.6 for a longer part of the process, leading to lower values of the mean and of the median at the end of the process. Lower median means that a high diversification has brought a large number of very good results and a limited number of bad results over the set of independent runs. A similar behaviour has been observed for the other functions showing many local minima (f 6 and f 7 ).", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": ["tab_2", "tab_5"]}, {"heading": "Composite Laminates Design Optimization", "text": "When a plate is subjected to in-plane compressive loads exists a value of the loads for which the originally flat equilibrium state is no longer stable. This load is called the buckling load. Before reaching the buckling load the plate has only in plane forces and deformations, membrane prebuckling state. Reaching the buckling load the plate suddendly leaves the flat state and large out of plane displacements arises usually leading to the structural collapse. The value of the buckling load depends on geometry, boundary conditions, material properties and the buckling mode shape. Considering a rectangular composite plate simply supported and subjected only to normal compressive loads the plate buckles into m and n half waves in the x and y direction, respectively, when the loads reach the values \u03bb b N x and \u03bb b N y .\nIn the general case of laminate with multiple anisotropic layers and without any stacking sequence symmetry the problem doesn't admit a simple solution. If we assume particular constraints on the stacking sequences, i.e. plates for which the bending twisting coefficients are zero are so small in respect to the other coefficients to be assumed zero, using the classical laminate theories [15] the buckling load factor \u03bb b could be found as:\n\u03bb b (m, n) = \u03c0 2 a 2 m 4 D 11 + 2(D 12 + 2D 66 )r 2 m 2 n 2 + r 4 n 4 D 22 m 2 N x + r 2 n 2 N y (5\n)\nwhere a and b are the lamina dimensions; r = a b the aspect ratio; N x and N y the applied loads; D ij the bending stiffness of the composite plate depending from the assumed stacking sequence of the laminate.\nThe smallest value of \u03bb b over all possibles values of m and n represents the lowest value of loads for which the buckling conditions are reached and hence the critical buckling load factor \u03bb cb . According to [9] limiting the values of m and n to 1,2 gives a good estimation of critical buckling load, so for an assigned plate geometry the optimization problem could be stated as:\nmax Dij min m,n \u03bb b (m, n); m, n \u2208 1, 2(6)\nAccording to the classical laminate theories [15] before the buckling condition is reached the plane stress condition is assumed valid for each ply of the laminate.\nIn the generic lamina k the constituive equations could be expressed as:\n\u23a1 \u23a3 \u03c3 xx \u03c3 yy \u03c4 xy \u23a4 \u23a6 = \u23a1 \u23a3 Q 11 Q 12 Q 13 Q 21 Q 22 Q 23 Q 31 Q 32 Q 33 \u23a4 \u23a6 k . \u23a1 \u23a3 xx yy \u03b3 xy \u23a4 \u23a6 (7\n)\nwhere Q ij are the lamina stiffness components expressed in the plate reference axis. The bending stiffness D ij of a plate made by n lamina could be now expressed as\nD ij = 1 3 n k=1 Q ij z 3 k \u2212 z 3 k\u22121 (8\n)\nwhere z k and z k\u22121 are the coordinate of the k lamina through the laminate thickness.\nThe terms Q ij could be expressed knowing the fiber orientations \u03b8 k and the elastic properties of the material along the principal directions\nE k 11 , E k 22 , G k 12 , \u03bd k\n12 of each lamina, [15]. For an assumed plate geometry the design variables are hence the elastic properties and the fiber orientations of each lamina.\nIn this paper a laminate made by graphite epoxy lamina of constant thickness t was considered, the elastic properties and thickness of the material are the following:\nE 11 = 127.6 GPa; E 22 = 13.0 GPa; G 12 = 6.4 GPa; \u03bd 12 = 0.3. The ply thickness is t = 0.127 mm.\nThe laminate has length a = 0.508 m, width b = 0.254 m, and is made by 64 plies. total thickness t = 8.128 mm , [9]. The only design variables are hence the angles \u03b8 k of each lamina. We applied ACOR with Gaussian perturbation and ACOR L with L\u00e9vy perturbation to a different set of allowed fiber orientations and of different constraints on the laminate stacking sequence able to reduce the number of independent variables. Table 3 shows the different set of possible fiber orientations, the constraint adopted on the stacking sequence and the number of independent variables for each case analyzed in the present paper.\nThe continuous relaxation approach is adopted in the optimization algorithm, i.e. the discrete variables are replaced by continuous ones and in the evaluation of the objective function are transformed in the allowed discrete values. This choice is suitable due to the natural order in the design variables space. The ACOR parameters values has been taken the same adopted for the function optimization problems, for \u03b1 the following values have been taken {2.0, 1.9, 1.8}. The maximum number of function evaluations has been set to 5000. Table 4 shows the results obtained with different values of \u03b1. Also in this case the behaviour is similar to that found for the mathematical functions with ACOR L performances better than classical ACOR ones. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_20", "tab_48"]}, {"heading": "Conclusions", "text": "In this paper, a new perturbation operator, based on L\u00e9vy distribution, is proposed for Ant Colony Optimization in continuous domains. The modified algorithm is here called ACOR L . The behaviour of the algorithm in some interesting real world problems has been observed and studied in the paper. In particular the ACOR L has been applied to a difficult multi-modal problem of composite laminates buckling load maximization. As it can be noted the wider exploration potential of the L\u00e9vy distribution allows the algorithm in case of multimodal functions showing many local minima to attain statistically significant better performance than standard ACOR. Further studies will be addressed towards the implementation of an adaptive version of ACOR L .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In this paper, we describe a constraint programming system, with a web site as front-end to demonstrate a suggested solution technique of the Teaching Assignment Problem. The timetabling problem in general is mostly, if not always, an over constrained combinatorial optimization problem and hence it is considered one of the most difficult problems to solve. The Teaching Assignment Problem in essence is a branch of the timetabling problem and it is the problem of assigning professors to time slots that is occupied by courses in a specific week.\nThe resulted weekly timetable is to be used to organize the teaching process at a university or any educational institute given that the courses have already been scheduled over the time slots and rooms. Each professor is assigned a total number of courses to be taught that should not be violated. Each professor is allowed to express interest or dislike in certain courses through weighed preferences that can or cannot be satisfied. Some professors can be assigned some courses in advance. The final solution should be constructed based on distributing the given courses over professors based on fairness principle as well as maximizing the total weight of the solution. The total weight of a solution is the sum of all satisfied preferences. The literature is very rich on the topic of university timetabling in general as there are different ways to solve the problem; most of them depend on specific needs considered by the institution that the timetabling is designed for. However, to the knowledge of the authors, no literature is dealing with the teaching assignment problem. In our case, we considered the problem as two-fold stages. The first is to assign courses to rooms and time slots and the second is to assign professors to the resulting time slots with courses. In this study, we only tackled the second one. Timetabling problems, in general, are usually over constrained as it is not always possible to satisfy all requirements.\nUser preferences can be used to relax these requirements. In our study case, we have used a more specific model with preferences which utilizes weight for each constraint and try to maximize the total weight of satisfied soft constraints. As a development approach, our work includes a development of a solver for soft constraints. The solver was implemented by the authors as an extension of a well-known CSP solver named \"Java Cream\" [1] to include soft constraints in the backtracking mechanism which the Java Cream Solver is lacking. The solver itself was re-coded entirely, by the author, using Microsoft C# language from Java language. Some of the optimized technologies introduced in C# and in .NET framework, such as LINQ, were used to enhance and optimize the local search.\nThe next section of this paper provides a related work for the problem. Section 3 provides a description to the teaching assignment problem. The added soft-constraint approach that was implemented within the solver along with the modified search algorithm developed for this problem is detailed in section 4. This includes a description of how the problem has been solved as well as the representation of soft and hard constraints. Furthermore, a discussion on how the search is done is provided at the end of this section. Section 5 provides a description for the web based system used to implement the solver. Computational results are discussed in Section 6. The final section reviews the results of our work and looks to future extensions of the problem solution and soft-constraint solver improvements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Over the last 30 years, the timetabling problem is considered to be one of the broadly studied scheduling problems in Artificial Intelligence and Operations Research literature [2]. Educational timetabling, to be specific, has been the main topic of quite few papers in various scientific journals and the topic of many theses in academia society. The course timetabling problem deals effectively with courses and time slots which have to be scheduled during the academic term. The problem basically is the scheduling of a known number of courses into a known number of time slots spread all over the week in such a way that constraints are satisfied.\nAs there are many versions of the Timetabling Problem, a variety of techniques have been used to solve it [3], [4]. Most of these techniques range from graph colouring to heuristic algorithms. Another focus of research in the timetabling problem was on the application of a single solution approach which in effect a large variety of such approaches have been tried out, such as an integer programming approach [4], Tabu search [3], and Simulated Annealing [5]. Recently, some researchers have attempted to combine several approaches, such as hybridization of exact algorithms and Meta-heuristics. One of the most primitive methods used to solve this problem is graph colouring in which vertices represent events where two vertices are connected if and only if there is a conflict. [5], [6], [7], [8] and [9] proposed a number of formulations by graph colouring for a set of class teacher timetabling problems and discussed the inherent complexity. In [10], graph colouring has been used to solve course and exam timetabling. Linear programming models were also used to formulate the course time-tabling problem usually with binary variables [11], [12], [13], and [14]. An Integer Programming approach [15] was also used to model the timetabling problem as assignment problem with numerous types of constraints and large number of binary or integer variables. Rudov and Murray introduced an extension of constraint logic programming [16] that allows for weighted partial satisfaction of soft constraints is implemented to the development of an automated timetabling system. In [17], an Evolution Strategy to generate the optimal or near optimal schedule of classes is used to determine the best, or near best timetable of lecture/courses for a university department. Case Based Reasoning is another approach that has recently been applied to university timetabling [18], [19], [20], and [21]. Case Based Reasoning is believed to be studied as early as 1977 with the study of Schank and Abelson [22]. Case Based Reasoning has also been successfully applied to scheduling and optimization problems. Burke et al. [23] also, in a published article, developed a graph-based hyper-heuristic (GHH) which has its own search space that operates in high level with the solution space of the problem generated by the so-called low level heuristics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Description", "text": "In general, the timetabling problem is the assignment of time slots to a set of events. These assignments usually include many considerable constraints of different types. At the department of Computer Science, University of Regina, in any term, the timetabling process currently consists of constructing a class schedule prior to student registration. The professors and classes timetabling problem [24] and [25] is NP-Complete. The teaching assignment problem is the problem of assigning courses, scattered over time slots, to professors. In our case, the teaching assignment problem is described as follows. Our approach is nothing to do with these assignments as these assignments are considered as input for the problem to solve. 2. There is a finite set of professors P = p 1 , p 2 , . . . , p |P | . 3. In this scheduling problem, courses represent variables while professors represent variables domain values. 4. The problem is to schedule P to C in a way such that no professor p i is in more than one place at a time t i . 5. The constraints for this problem are soft and hard. The soft constraints should not all be satisfied and on the contrary all hard constraints must be satisfied so a possible solution to the problem is one that satisfies all the hard constraints but not necessary soft constraints. 6. Soft Constraints are preferences that do not deal with time conflicts and have weight (or Cost) associated with them. Our goal is to maximize the total weight of a solution (or minimize the total cost). We have two types of soft constraints; the first is count, where a professor has a maximum number of courses assigned to him that should not be exceeded. The second is preferences that any professor can express as interest or dislike in certain courses which have weights (or costs). This type of constraints can or cannot be satisfied. 7. In our case, we have two types of soft constraints, both of them related to professors preferences. These preferences named equal and not equal, which is indicate if a professors provided an interest or dislike in a that course (variable). 8. Hard Constraints are typically constraints that physically cannot be violated.\nThis includes time slots that must not overlap in time, and in our problem time slots that overlap in time must not be taught by the same professor.\nThere is another type of hard constraints where time slots represent a course can be assigned to a professor in advance prior to starting the search for a solution. 9. There is a total weight function that measures the quality of the current solution. The object of this function is to return the sum of all weights/costs associated with the satisfied preferences. The aim of the optimization technique is to maximize the total weight function or minimize the total cost.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm Description", "text": "As mentioned above, the solver, used in solving the problem, is re-coded from a well-known solver named \"Java Cream\" using Microsoft C# language. The original solver can be used to model any constraint satisfaction or optimization on finite domains problems [26]. However, it lacks any proper handling of soft constraints. As known, any timetabling/scheduling problem would be mostly over constrained and therefore it cannot be solved unless constraints are relaxed. Hence, the necessity came to add soft constraints as part of the re-coded solver to solve timetabling problems. The backtracking method is the one that was modified to take into account soft constraints. Although we describe only the modified backtracking method, which is used by two of the five methods that the solver adopts: Branch and Bound; and Iterative Branch and Bound. However, because preliminary tests showed that performance is not significantly improved in our application when using the other three methods; Taboo Search, Random Walk and Simulated Annealing, we only consider the backtracking method to meet our requirements. Furthermore, all solver search methods use the same variable/value ordering methods and hence would use the same approach mentioned here. We think that because the application study case variables and values are relatively small, the tests performance using other methods has not improved but might be better if another application is implemented which might have more complex variables and values.\nBasically, the backtracking algorithm [27] has two phases. The first stage is what is called \"a forward phase\" in which the variables are selected sequentially and the current partial solution is extended by assigning a consistent value for the next variable if one exists. The second phase is known as \"a backward phase\" in which the algorithm returns to the previous assigned variable when no consistent solution exists for the current variable. For the forward phase, the adopted solver originally decides which variable to instantiate next by selecting the one that has minimum number of domain values (i.e. the one that its domain size is minimum). Then the solver decides which value to assign to the next variable by assigning the maximum value in the variable domain. By assigning a value to a variable, this value is eliminated from all other variables' domains. This is known as look-ahead backtracking.\nThe variables in the original solver are ordered according to their domain size and the variable with the highest domain size is first. In the modified solver, you still can use the same mechanism, but when \"soft constraints approach\" is used, variables are ordered by the highest weight on soft constraints and then on highest domain size.\nThe values in the original solver are ordered incrementally. However, in the modified solver that uses soft constraints approach, values are ordered by values associated to equal soft constraints that least have been assigned to any variable before first and then other values incrementally and last values that are associated with not equal soft constraints.\nBecause of the soft constraints that have been added to the solver, we have improved the two backtracking phases as follows if \"soft constraint approach\" method is selected in solving the problem:\n1. On deciding which variable (Course) to instantiate next, the solver tries primarily to select the variable with the highest weight on equal soft constraints that have not been assigned a value (Its domain size is greater than one); if not then it will return randomly one of the variables (courses). The idea behind this is to try to select a variable that has soft constraints associated with it first, if not found then it will act on the other types of variables. 2. If the previous hint is not implemented, it will give the chance to assign values to variables that do not have soft constraints with them where they should have been at least trying to be assigned to variables with soft constraints. In this case, variables with preferences will miss the chance to get their preferences assigned to them. 3. On deciding which value to assign to the variable selected in the previous step, a method, first, checks if there are equal soft constraints associated to that variable. If there are not any, then it will randomly select a value from its domain (i.e. domain values represent professors). 4. It is worth mentioning that even if there are no soft constraints associated with it, the method tries to not to choose a value that is associated with another variable that has an equal soft constraints as it might be needed in a later stage. 5. If there are indeed equal soft constraints associated to that variable, then it assigns the value that least has been assigned to any variable before. This is in compliance with the \"fairness\" principle. Furthermore, the value is selected randomly if there is more than one value.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Web Based Interface for the Teaching Assignment Problem Solver System (TAPS)", "text": "We have implemented a web-based application for solving the timetabling problem. The idea behind this approach is to get professors to enter their preferences through the web site. Web based applications generally are more convenient for users. For instance, every professor can enter his/her preferences from office/home and there is no need to provide this information to application operator to enter their data. The Teaching Assignment Problem Solver (TAPS) was The MVC model provides a rich graphic user interface using HTML and JQuery (Java script based library). There are four main parts for the web site. The first is devoted to courses management and its information can be entered by administrator. The information includes the course assigned week days, assigned time slots, and assigned professor (if needed). The later will be treated by the solver as hard constraint. The interface also displays the number of professors interested and not interested in that course. The whole courses are displayed as a table where there is an option for inserting, editing and deleting a course. The second is dedicated to professors' information management and their information can be entered by professors themselves. The information includes the number of courses that can be assigned to each professor (i.e. count constraint) and the professors' preferences. Both preferences will be dealt by the solver as soft constraints. The third is for searching for a solution using the information provided and using the C# cream as a background solver. The solution section provides an interface for searching for solutions using the information provided in the previous two sections. If solutions are found, they will be displayed in this section's web page. Among the information displayed, there is time spent for generating all solutions and the time spent for each solution along with the solution weight. There is also the option to display the next and previous solution. There are three tables; the first one is the main table where courses are displayed with professors. The second table displays professor's names and the number of assigned courses. The displays all constraints (hard and soft) used in finding the solutions. The last section is for Web site settings. This includes the number of hours per course, maximum break minutes per session, maximum number of courses per professor, number of preferences per professors, maximum number of generated solutions, the option to generate only better solutions in terms of solution weight and the maximum timeout that should be used in the solver to generate solutions. The screen shots below illustrate the professors and solutions sections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Tests and Results", "text": "The experimental tests compare between our proposed approach and the original backtracking method.\nIn order to do tests on the designed Web site and the proposed solver, we used data from the Computer Science department at the University of Regina for both courses and professors information including courses time slots. Then we assigned randomly some of the courses to some professors and assigned professors to show some interest and dislikes in some of the courses.\nOverall, we used 17 courses as solver variables and 10 professors as solver values. From these courses we entered interest in 4 courses for different professors and disinterest in just one course. Experimental computations were done with a number of objectives in mind. The main goal was to provide a table of courses assigned to professors where all hard constraints are satisfied and the weight of soft constraints is maximized.\nThe second experiment involved using the same solver to solve a problem with the same variables and domain values but using non preference approach (The original backtracking method).\nWe have also set the solver to generate the first 100 solutions considering the first solution is the most optimized one and to generate only same or better weighted solutions and the solver has only 100 seconds to generate any solution at any given time. We used a PC with the following capabilities: Core 2 Duo Quad processor (2.4 GHz) with 6 GB ram. We have asked the solver to search for solutions 10 times to get a bigger picture of the search time spent in finding solutions.\nThe results showed that the average time to find a solution was between 1.92 ms and 2.567 ms. When using non-preference approach (i.e. original solver's ordinary variable/value ordering), we were unable to find a feasible solution that can satisfy the maximum number of soft constraint in the first 100 solutions. On the contrary, when using the preference approach, the first 10 solutions were optimal for our problem that satisfied hard and soft constraints.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have successfully applied modified back tracking method to solve the teaching assignment problem. Feasible schedules were obtained for real data sets, including professors' preferences without the need for a huge computational effort. The original solver was meant to solve integer based variables problems but for problems with hard constraints. We have extended the solver to adopt soft constraints and we think that it has been a success. In conclusion, this application of Teaching Assignment Problem Solver appears to be quite successful and we are satisfied and ready to implement it to generate actual schedules for future terms. We also think that this approach can be implemented in similar problems like Exam Supervision scheduling. Based on the gathered experience of this test, we concluded that this approach is computationally feasible.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The field of micro machining is forcing a profound redefinition of the nature and attributes of electronic devices. The technology of micro electro mechanical systems (MEMS) has found numerous applications in recent years, for example the electromechanically filters, biological and chemical sensing, force sensing and scanning probe microscopes [1][2][3][4].\nThis technology allows motion to be incorporated into the function of micro scale devices. However, the design of such mechanical systems may be quite challenging due to nonlinear effects that may strongly affect the dynamics.\nMicroscopic gyroscopes [5,6] are helping enable an emerging technology called electronic stability control. The resulting system helps prevent accidents by automatically activating brakes on out-of-control vehicles. The technology may be particularly useful for vehicles with a higher center of gravity, which makes them prone to rolling. Electronic stability control is available in luxury vehicles, but sensors made from quartz were too expensive for widespread installation. Innovations in MEMS gyroscope technology make these systems more affordable.\nControl problems consist of attempts to stabilize an unstable system to equilibrium point, a periodic orbit, or more general, about a given reference trajectory. In the last years, a significant interest in control of the nonlinear systems, exhibiting unstable behavior, has been observed and many of the techniques discussed in the literature [7][8][9]. Among strategies of control with feedback the most popular is OGY (Ott-Grebogi-York) method [7]. This method uses the Poincar\u00e9 map of the system. Recently, a methodology, based on the application of the Lyapunov-Floquet transformation, was proposed by Sinha et al. [8] in order to solve this kind of problem. This method allows directing the chaotic motion to any desired periodic orbit or to a fixed point. It is based on linearization of the equations, which described the error between the actual and desired trajectories. Another one technique was proposed by Rafikov and Balthazar in [9], where the Dynamic Programming was used to solve the formulated optimal control problems. Several techniques that can be applied to a wide range of problems.\nDifferent from numerical approach, mentioned above, there are other algorithm based approach. In this sense, here we proposed the algorithm based approach, that used particle swarm optimization (PSO) algorithms. The PSO algorithms is a population based stochastic optimization technique developed by Kennedy and Eberhart in 1995 [10]. Using PSO algorithms allows directing the chaotic motion to any desired periodic orbit or to a fixed point. In this work, we proposed and develop a PSO based optimization algorithms for control the unstable movement of MEMS gyroscope.\nThe paper is outlined as follows. In Section are showed the concepts related with the nonlinear model to the MEMS gyroscope. In Section 3, is shown the application of the Particle Swarm Optimization algorithm in the MEM gyroscope. In Section 4, the proposed control swarm approach are presented. In Section 5, we do some concluding remarks of this work. In section 6, we list the main bibliographic references used.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MEMS Gyroscope Model", "text": "The technology of micro electro mechanical systems (MEMS) has found numerous applications in recent years, for example, the MEMS gyroscope ( Fig. 1).\nHere, we consider a mechanical model and the derivation of governing equations done by [5] for the MEMS gyroscope, commonly function on the coupling of two linear resonant modes via the Coriolis force.\nThe micro gyroscope device consists itself of a perforated proof mass constrained to move in the plane by a suspension of micro beams. It is forced along one axis, the so-called drive axis, by a set of non-interdigitated comb drives, and its motion along the other axis, is detected by a set of parallel plate capacitors.\nThe governing equations of motion of MEMS gyroscope were obtained by [5] and they are:  \n( )( ) [ ] ( ) ( ) [ ] 0 2\n= \u03a9 + + + + = \u03a9 \u2212 + + + + + + + x y k y k y c y m y x wt V r k x wt V r k x c x m a a (1)\n, 1 , k m 2 , , , , , , k L k k k k V L r k L k k V r mk c k m t L y q L x q a a s d \u03b5\u03be \u03b5\u03b4 \u03b5\u03b3 \u03b5\u03bb \u03b5\u03bd \u03b5\u03bb \u03b5\u03b6 \u03c4 (2)\nWe will obtain:\n( ) ( ) [ ] ( ) ( ) [ ] (\n)\n0 1 2 0 2 cos 1 v 2 cos 1 1 2 3 3 3 3 1 = \u2032 + + + + \u2032 + \u2032 \u2032 = \u2032 \u2212 + + + + + + \u2032 + \u2032 \u2032 d s s s s s d d d d q q q q q q q wt q wt q q \u03b5\u03b3 \u03b5\u03be \u03b5\u03b4 \u03b5\u03b6 \u03b5\u03b3 \u03b5\u03bb \u03b5 \u03b5\u03bb \u03b5\u03b6 (3)\nBy applying the method of averaging in (3) and rewriting the equations of the dynamical system, in state form, the governing equations may be written as being [5]:\n( ) [ ( ) [ ] ( )(\n)] 3 8 4 ( 4 [ 8 ))], 2 cos( ) ( 2 ) 3(v 8 4 ( 4 [ 8 2 cos 2 ))], (2 ) 2 ( 8 ( cos 4 8 2 2 2 4 3 1 2 4 3 2 1 3 1 2 1 3 3 1 1 4 3 2 1 3 2 4 3 1 2 1 2 1 3 1 1 4 3 2 1 x x x x sin x x x x x x x x x sin x x x x x x x x x sin x x x x x x \u03be \u03c3 \u03b4 \u03b3 \u03b5 \u03bb \u03bb \u03bb \u03c3 \u03bb \u03b3 \u03b5 \u03b6 \u03b3 \u03b5 \u03bb \u03bb \u03b6 \u03b3 \u03b5 + \u2212 + \u2212 \u2212 = + + + + \u2212 + \u2212 \u2212 = + \u2212 \u2212 = + + \u2212 + \u2212 = (4))\nHere the parameter x 1 is the amplitude of oscillation the drive axis, and x 2 is the amplitude of oscillation along the sensing axis. The variables x 3 and x 4 are the phases of oscillation for the two axes. In the Figure 2 is showed the dynamics behavior of time history for the x 1 and x 2.\nIn the Figure 3 shown the phase portrait for x 1 and x 2.\nIn the Figure 4 shows the diagram of the stability for x 1 (with the region's control applied it is illustrate). ", "publication_ref": [], "figure_ref": ["fig_2", "fig_15", "fig_28", "fig_29"], "table_ref": []}, {"heading": "Swarm Control Design", "text": "The particle swarm optimization (PSO), this technique is a population based stochastic optimization technique developed by Kennedy and Eberhart in 1995 [10]. A particle swarm optimization algorithm consists of a number of individuals refining their knowledge of the given search space. In each iteration, the particle swarm optimization algorithm refines its search by attracting the particles to positions with good solutions, considering the best solution until the moment and the best solution of the iteration. The particle swarm optimization technique has ever since turned out to be a competitor in the field of numerical optimization. The PSO approach to nonlinear and control has been observed and discussed in the literature [11][12][13].\nHere, we propose a method for control of unstable systems using the Particle Swarm Optimization with optimization techniques. The method, is used for control the unstable movement of MEMS Gyroscope to stabilize the system to period orbit. The proposed method formulates the nonlinear system identification as an optimization problem in parameter space and then particle swarm optimization are used in the optimization process to find the estimation values of the parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Particle Swam Optimization", "text": "The Particle Swarm Optimization (PSO) algorithm, introduced by Kennedy and Eberhart [10], is a computational simulation of social and biological inspired algorithm.\nPSO consists of a algorithm with low computational cost and information sharing innate to the social behavior of the composing individuals. These individuals, also called particles, flow through the multidimensional search space looking for feasible solutions of the problem. The position of each particle in this search space represents a possible solution whose feasibility is evaluated using an objective function.\nThe PSO algorithms refines in each iteration, its search by attracting the particles to positions with good solutions, using the best solution ( i p ) found by the particle in last iteration and the best solution found so far considering all the particles ( g p ).\nIn each iteration, a particle i having position i\nx have its velocity i v updated in the following way:\n( ) ( ) ( ) i g i i i i i i x p x p v w v \u2212 + \u2212 + \u03a7 = 2 1 \u03d5 \u03d5 (5)\nwhere X is know as the constriction coefficient described in [14], w is the inertia weight, i p is best solution found by the particle in last iteration and the g p best solution found so far considering all the particles, and 1 \u03d5 and 2 \u03d5 are random values different for each particle and for each dimension. The position of each particle is updated during the execution of iteration. This is done by adding the velocity vector to the 1 position vector, i.e.,\ni i i v x x + = (6)\nSetting for the velocity parameters determine the performance of the particle swarm optimization to a large extent. This process is repeated until the desired result is obtained or a certain number of iterations is reached or even if the solution possibility is discarded. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Control Swarm Approach", "text": "The proposed algorithm, showed above, formulates the nonlinear system identification as an optimization problem in parameter space, and then adaptive particle swarm optimizations are used in the optimization process to find the estimation values of the parameters.\nThe algorithm is used for control the behavior unstable of the nonlinear dynamics model (4). The goal of this control synthesis is find the estimation values of the parameters, to drive the orbit of the system to a periodic orbit.\nWe apply the Particle Swarm Optimization algorithm, presented in earlier section for the MEMS Gyroscope (4), to reduce the unstable behavior of this nonlinear system to a period orbit. The Fig. 5, 6 and 7 showed the behavior controlled and uncontrolled of the system (4). In comparing, non-controlled system (see Fig. 2 and 3) with of numerical results of PSO (Fig. 5-7) we can verify that control orbit generated by PSO approach has small diameter.", "publication_ref": [], "figure_ref": ["fig_116", "fig_15", "fig_53"], "table_ref": []}, {"heading": "Algorithm of the Particle Swarm Optimization", "text": "Create and initialize an nx-dimensional swarm, S, through the system of equations ( 4) and shown in the projection of the phase space (figure 3). S.xi i is used to denote the position of particle i in swarm S. S.yi i is used to denote the best position of particle i in swarm S. S. \u0177 , is used to denote the global best position of particle i in swarm S. ", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, a dynamics of the MEMS gyroscope, proposed by [5,6] it is investigated.\nWe applied the particle swarm optimization technique applied to control MEMS Gyroscope. This control allows reduction of the oscillatory movement of the system to a desired period orbit.\nIn comparing of numerical results of PSO with the non controlled system (Fig. 2  and 3) we can verify that control orbit generated by PSO approach (Fig. 5-7) has small diameter.\nThe particle swarm optimization technique presents a computational algorithm motivated by a social analogy. The algorithm control allowed reducing the oscillatory movement of the nonlinear systems to a period orbit. The Fig 5-7, illustrate the effectiveness of the control algorithm to these problem.", "publication_ref": [], "figure_ref": ["fig_15", "fig_53", "fig_53"], "table_ref": []}, {"heading": "Introduction", "text": "Quantitative Structure Activity Relationship (QSAR) consists in predicting some chemical property given the structure of the molecule. It is an important research area in chemistry, and a very challenging application domain for data mining. QSAR typically deals with a single molecule. Chemical reactions usually involve several molecules. As it is possible to predict properties of molecules, the same should be possible with reactions. The problem is to plug several molecules, reactants and products, in a data mining algorithm.\nThis article points out the use of a Condensed Graph of Reaction (CGR) to represent a reaction involving several molecules as if it was a single molecule, therefore allowing the use of existing techniques dealing with a single molecule. This is illustrated on a real chemical problem.\nChemistry, in particular QSAR, is a main application domain of machine learning and data mining. Inductive Logic Programming and Relational Data Mining can represent and learn from complex structures such as molecules. Moreover they can use background knowledge such as rings, generic atoms [1,2,3,4]. However to the best of our knowledge they have not been applied to chemical reactions.\nSome papers related to data mining methods predicting properties of reactions have been published, but they do not really model the reaction. For instance Brauer [5] and Katriski [6] have published papers dealing with Quantitative Structure Reactivity Relationship concerning only one reaction making some parameter (such as solvent) vary. Another attempt has been proposed by Halberstam [7] to model the rate constant of reaction involving two reactants and one product (A +B \u2192 C) where the second reactant (B) is always the same. The study was then reduced to a classical QSAR on one compound.\nThis paper is organised as follows. The condensed graphs of reactions are defined in section 2. ISIDA fragment descriptors are presented in section 3. The prediction of the rate constant of reaction is described in section 4. Section 5 concludes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Condensed Graphs of Reactions", "text": "A Condensed Graph of Reaction [8] represents a superposition of reactants and products graphs. A CGR is a complete connected and non oriented graph in which each node represents an atom and each edge a bond. CGR uses both conventional bonds (single, double, aromatic, etc.) which are not transformed in the course of reaction, and dynamical bonds corresponding to those created, broken or modified during the reaction (cf. Figure 1).\nActually a CGR is a pseudo molecule in which some new bond types have been added. An editor of CGR has been added to our software environment specialised in chemical data mining: ISIDA (In SIlico Design and Analysis) [9]. Any new type of dynamical bond could be easily added in the list of bond types.\nMoreover we developed an algorithm that generates a CGR from the file formats (RXN and RD) usual in chemoinformatics to model reactions. This programs requires the information about the atom mapping in reactants and products. This information is often available from existing software to edit and manage chemical data, otherwise it can be added thanks to our editor. Indeed the key point to produce a CGR is to map the reaction, that means that each atom on the left of the arrow corresponds to an atom on the right of the arrow. On Figure 1.(a) each atom is uniquely numbered in order to assign the same number to the same atom on both sides of the arrow, for instance the atom numbered 12 on Figure 1.(a). Moreover, some flags are added to describe the bonds that change, as described in the \"CTFile format\" document [10] from Elsevier MDL c . In the case of our example reaction a \"rxn\" flag is drawn beside the created bond between the atoms mapped 6 and 12 and for the broken bond between atoms 6 and 13.\nIn most of the database, the mapping is automatically done, with some errors due to mismatching of the atoms on each side of the arrow. For our dataset, the mapping was manually done and verified by a chemist to guarantee avoiding mismatch. Once the reactions are correctly mapped, the CGR are created. The algorithm consists in gathering the atoms of all the compounds of the reaction without duplication of the mapped atom. Then the connection table of reactants and products are examined to find the reactivity flag and write the dynamical bond in the CGR. This change of representation allows one to store the reaction database in the format (SD) usual in chemoinformatics to represent individual molecules.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "ISIDA Fragment Descriptors", "text": "For each compound, the ISIDA fragment descriptors [11,12,13] produce a vector of integers counting the occurrences of molecular fragments. The nature of each descriptor is a molecular fragment, as detailed below, and its value is the count of this fragment in a molecule.\nFragments are built by computing the shortest paths in the molecular graph between two atoms, in terms of the number of nodes passed through. The fragment is a representation of the Atoms and Bonds (AB) traversed by this path.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 2. Example of ISIDA fragment descriptors applied to a Condensed Reaction Graph", "text": "The ISIDA fragment descriptors apply to the CGR. For the reactions only fragments containing at least one dynamical bond are selected. Some example of fragments in their linear notation are shown in Figure 2. The first example (Cl \u2212S-C*C*C*C) represents the shortest path between the two atoms circled on the molecular graph (length = 6). If several shortest paths could be found, all of them are taken into account. Symmetric fragments, for example the C-C-N and N-C-C, are considered as a single descriptor. The fragmentation takes a minimum and a maximum length as parameters. In this paper, the fragments having from 2 to 6 atoms were considered I(AB,2-6).", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Prediction of the Rate Constant of Reaction", "text": "This section illustrates the use of the CGR and ISIDA fragment descriptors to predict the rate constant of reaction. First the data are described, then the model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "The data used for the computation of the rate constant comes from a compilation [14] of the rate and equilibrium constants of heterolytic organic reactions. The selected reactions concern Nucleophile Substitution 2 (SN 2 ) in water. The database 1 was manually built and contains 249 reactions at 25 Celsius degrees with their log(k) where k is the rate constant. The log(k) fluctuates from -6.38 to 4.29, its mean is -1.55 and its standard deviation is 1.84 (cf. Figure 3).\nData in this compilation were extracted from publications implementing various methods and experimental protocols to measure the rate constant. This is the source of some variability in the data set. For instance, different values are reported for the same reaction just by changing the reactant concentration. For instance, for the same reaction, the rate constant can vary in a range of 1 to 1.5 Fig. 3. The repartition of log(k) for the 249 reactions of the database log units. This might be due to experimental errors. In such situations the mean value has been used as the experimental rate constant. It's not really possible to estimate the experimental error of the data because too many sources are involved. But we estimate our data error around 1 log unit.", "publication_ref": [], "figure_ref": ["fig_28", "fig_28"], "table_ref": []}, {"heading": "Settings", "text": "Three methods were used to model our data : (i) M5P (model tree), (ii) SVMreg (an SVM method for regression problems) and (iii) linear regression (LR), from WEKA [15]. Usually SVM gives more accurate results, but M5P produces models easier to interpret. M5P and the linear regression were used with their default parameters. The SVMreg used a RBF kernel and the default values for its parameters (c = 1, \u03b3 = 0.01, \u03b5 = 0.001).\nThe RMSE (Root Mean Squared Error) and the correlation coefficient (R 2 ) were computed to estimate the error of the procedure. There is sometimes an ambiguity between the correlation coefficient and the determination coefficient. We used the correlation coefficient as defined by the equation 1).\nR 2 = 1 \u2212 n i=1 (a i \u2212 p i ) 2 n i=1 (a i \u2212 a) 2 (1)\nwhere p is the predicted value, a the actual value and n the total number of example in the test set. A ten fold cross validation was used for these computations to validate and compare our models. Actually the cross validation procedure was repeated ten times and the result of the calculations for each method are the average of the RMSE and of the correlation coefficients of all the runs. The corresponding standard deviation was evaluated in order to check over that the different splittings do not produce too large variations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "The performances of the three methods on the I(AB,2-6) fragmentation are reported below: The correlation coefficient is greater than 0.6. The methods produced models that correctly approximate the constant rate of reaction. Non-linear models (M5P and SVMreg) are better than linear models (RR), and more complex models (SVMreg) are better than the more understandable models produced by M5P. This can be checked on the REC curve, cf. Figure 4. A REC (Regression Error Characteristic) curve [16] plots the accuracy with respect to the error threshold. The accuracy, in the regression context, is defined as the number of instances such that the absolute error between their actual and predicted target values is below the threshold. Figure 5 plots the actual and predicted values for each of the 249 reactions. For each model, two lines are added: the identity Y=X in order to illustrate the determination coefficient, and the best linear model fitting the points in order to illustrate the correlation coefficient. RMSE and R 2 measures how much the points are spread around the identity. Points are less spread away with SVMreg than with M5P and RR. Such a plot also allows to identify outliers, points that are further away from the diagonal. Actually most of those points correspond to reactions involving unfrequent fragments, therefore those reactions are more difficult to learn from such a small dataset.\nFigure 6 contains the model built by M5P on the whole dataset. For the dynamical bonds, we defined new bond types in MOL files: \"5\" for the breaking of a single bond, and \"1\" for the formation of a single bond. The first condition \"C-P5F <= 0.5\" means that the fragment \"a carbon atom with a single bond to a phosphorus with a broken single bond to a fluorine atom\" has less than 0.5 occurrences in the CGR. The analyse of this model is focused on the structure of the tree, not on the linear models contained in each leaf, in order to identify whether some leaves correspond to meaningful sets of S N 2 reactions, and at least to check whether the conditions in the nodes make sense.\nFirst, the structure of a decision tree is not meaningful. For instance the first leaf (LM1) is defined by the absence of fragments C-P5F, P1O, S-S1C, and Cl5C. Obviously a reaction is defined by the fragment it contains rather than by the fragments it does not contain. So we had to find the corresponding reactions in the dataset, and observed that those reactions mainly involve iodine.\nReactions involving different halogens have different kinetics. So it is meaningful to distinguish iodine (LM1) from chlorine (LM2). The third leaf (LM3) involves a fragment (S-S1C) that is exceptional, but therefore it is meaningful to isolate the corresponding reactions in a leaf.\nWe notice that all other branches of the tree, actually 2/3 of the reactions, involve phosphorus. So there is a bias in the dataset. Some branches distinguish whether a bond to fluorine (C-P5F, LM12) or to a sulfur (P5S-C) is broken. Some conditions are refined,\nfor instance P1O is refined into C-C-N-P1O or C-C-O-P1O.\nBut some refinements such as P5S-C-C-N-C and P5S-C-C-S-C do not seem meaningful. However they occur at depths 6 and 8 respectively and cover few reactions, hence they might come from overfitting, and they could be avoided by a stronger pruning.\nFinally, despite the dataset being too small to finely model all S N 2 reactions, the fragments built from CGR and selected in the model tree make sense.", "publication_ref": [], "figure_ref": ["fig_29", "fig_116", "fig_117"], "table_ref": []}, {"heading": "Conclusion", "text": "Condensed Graphs of Reactions enable any existing QSAR technique to be applied to chemical reactions. This approach has been successfully experimented on a real chemical problem, using ISIDA fragment descriptors to generate an attribute-value representation of the chemical reactions, and out-of-the-box regression techniques from Weka.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In many business domains, especially for software companies, dealing with user requests for support and service has a growing demand on time and costs. Such user requests might be concerned with lack of information or understanding of installing or using the software, with the need specialised routines for non-standard problems, or with the report of errors and the need of trouble-shooting strategies. While general recommendations for very frequent requests can be collected on a FAQ site, typically many requests have to be dealt with on an individual basis. If support is distributed between many employees, possibly also distributed over different locations, it can be often the case that one support engineer has to deal with a problem which another support engineer has already solved on a previous occasion. A common data base of user requests and how they were handled (successfully) could reduce time and effort for service and support dramatically. Such a data base could even be the backbone for automated support answers for simple standard requests.\nGiven a data base with support requests and solution routines, the main problem is to provide a suitable similarity measure for retrieval of a suitable solution routine for a new request. In the context of a case-based reasoning approach (Aamodt & Plaza, 1994), similarity is determined between a new and an already known case. Alternatively, cases can be generalised into prototypes (Rosch, 1983;Zadeh, 1982;Wilson & Martinez, 1993). In this case, the most similar prototype is retrieved and -depending on the application domain -either the associated standard solution can be applied to the new case or a parametrised solution routine can be instantiated in accordance to the new case. Using prototypes can have an advantage over cases for large data bases because retrieval time can be reduced when new cases have only to be matched against the prototypes and not against all cases. Furthermore, prototype theory (Rosch, 1983) mimics a successful human cognitive strategy (Wiese, Konerding, & Schmid, 2008). A prototype represents the relevant aspects of a set of similar objects or situations while irrelevant details are ignored.\nThe most prominent approach to similarity in data mining, case-based reasoning and classifier learning is to use feature based measures such as Euclidean distances or other Minkowski metrics (Everitt, Landau, & Leese, 2001). If data are not given as sets of features but in form of a structured representatione.g., as records or as terms -there are two strategies to obtain a similarity rating available: An obvious approach is to transform the structures into feature sets (Yan, Zhu, Yu, & Han, 2006; Geibel, Sch\u00e4dler, & Wysotzki, 2003). This has the advantage that many standard approaches to data-mining and classification can be used. Another possibility is to use specialised approaches to structural similarity such as edit distance (Bunke & Messmer, 1994) or determining greatest common structures (Messmer & Bunke, 2000; Plaza, 1995; Estruch, Ferri, Hern\u00e1ndez-Orallo, & Ram\u00edrez-Quintana, 2009). Structure-based approaches have the advantage that information contained in the relations between objects is not lost by transformation into features.\nOur area of application is concerned with incident reports for the business information software SAP Business ByDesign. In this software, creation and visualisation of incident reports are based on an incident model. Incident reports are created by an incident wizard. The current system state together with all context data (current workspace, current object, UI) and a filled in report mask are saved into an XML document and sent first to a key user and -if he or she cannot solve the problem -sent further to a support engineer at SAP. The reports can be viewed by the support engineer in the SAP support studio software where he or she has different possibilities to analyse the report using a graphical presentation of context data.\nThe work presented here, is a first exploration of the utility of structure generalisation in this domain. Currently, we work on manually created incident clusters and focus on generalisation and retrieval. In the following, we first present how incidents are represented in form of trees. Afterwards, we introduce our approach to tree generalisation and retrieval. Within the general framework of structure matching and learning we propose different algorithmic realisations. An evaluation of these realisations and a comparison with the inductive logic programming algorithm Foil for a set of sample incidents is presented. We conclude with possible improvements, extensions, and suggestions for application.", "publication_ref": ["b394", "b405", "b405"], "figure_ref": [], "table_ref": []}, {"heading": "Incident Trees", "text": "Incidents occurring in the context of the SAP Business ByDesign System are represented in a unique form, given as an incident model which is specified as an XML-tree. It contains, for example, information about the software version, the workcenter (the role of the user) and the business object for which the incident occurred. The model is an abstraction of the system's class hierarchy. Incident objects are referred to by a name, their corresponding class is given as a type. The details of the incident model are reported in Bader (2009). The general structure of an incident follows the following form: Each element has a prescribed type which characterises the context information and the content of an incident. For a given incident, an element is instantiated with a name-string. Depending on its type, an element can have zero to a typically small number n of children. An extract of the incident model is given in Figure 1.\nAn example use case is, that an employee wants to order some office equipment. He or she works from the \"home\" workcenter (WC(home)) and entered the shopping basket user interface (UI(ShopBaskQAF)). When an incident is reported in this situation, the business objet \"purchase request\" (BO(PurReq)) becomes part of the incident context.\nThe incident model -also called model tree -represents the general structure which is underlying each possible incident report and thereby restricts the form of incident trees.", "publication_ref": ["b395"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Definition 1.", "text": "A model tree M is a tree of fixed size with typed elements e : \u03c4 . The type of the element determines the number and types of its child nodes.\nNote, that in Figure 1 we write an element e as \u03c4 (name) where name is a variable which can be instantiated by a constant name-string for a given incident.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Definition 2. An incident tree I is an instantiation of the model tree M :", "text": "Each element e \u2208 M is either mapped to (empty element) or a constant name string of type \u03c4 . At least one element in I must be unequal . \u03bb is the root position of T , -if T = e(T 1 , . . . T n ) and u is a position in T i , then i.u. is a position in T .\nT.p = e refers to a specific element in T . T.p = e(T 1 , . . . , T n ) refers to a specific element and its children.\nFor better readability we omit types in the definition. In the algorithms presented in the following section, it is guaranteed that mapped elements are of the same type since mapping is guided by a fixed model tree underlying all instance trees.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tree Generalisation and Retrieval", "text": "For learning of incident prototypes, sets of incidents -called cluster -are generalised with respect to their common structure. In consequence, each cluster is represented by a prototype and new incoming incidents are compared with all prototypes to retrieve the most similar one. In the following we propose three approaches to generalisation and retrieval: Anti-unification of trees as base-line approach and two variants for generating structure-dominance trees. Since we are not concerned with arbitrary trees but with trees based on a unique structure given as model tree, there is no need to rely on general approaches to tree matching (Wang, Zhang, Jeong, & Shasha, 1994).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Anti-unification of Trees", "text": "Syntactic first-order anti-unification is an approach to generate least generalisations over terms (Plotkin, 1969;Burghardt & Heinz, 1996). An anti-instance of a set of terms is calculated by traversing them simultaneously and keeping the common structure. If terms start with different symbols, these terms are represented as mappings to variables in the anti-instance. The mappings can be used to recreate the original terms from the anti-instance by transforming them into substitutions.\nSince trees and terms are corresponding data structures, this approach can be transferred to incident trees. That is, a cluster prototype is defined as antiinstance of a set of incident trees. Instead of calculating mappings, we introduce an -element in the anti-instance at the position of mismatched terms. ", "publication_ref": ["b403", "b397"], "figure_ref": [], "table_ref": []}, {"heading": ". , T Ni }). \u2022 Else P.p := : \u03c4 (an empty element).", "text": "An anti-instance of a set of trees corresponds to the intersection of all trees with respect to a model tree (see Figure 2 for an illustration).\nFor retrieval, the most similar prototype P i for an incoming incident I new must be determined. This can be realised by anti-unifying I new with each prototype and partially ordering the anti-instances A Pi,Inew with respect to their subsumption relation (Plaza, 1995). Definition 4. An incident tree T is said to subsume another incident tree T , that is, T is a generalisation of T (T > T ) if sub(T, T , \u03bb) = true with sub( , T , p) = true sub(T, T , p) = false if T.p = and T.p = T .p sub(T (t 1 , . . . , t n ), T (t 1 , . . . , t m ), p) = true if T.p = T .p and n = m and for all t i sub(t i , t i , p.i).", "publication_ref": ["b402"], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Structure Dominance Tree Generalisation", "text": "Syntactic anti-unification is not robust with respect to noise. Furthermore, using an identity criterium for element matching is very strict. If, for example, n \u2212 1 incidents have an identical element at position p and only one incident has a different or empty entry for this element, the prototype at this position is empty. Therefore, we introduce a new approach to prototype learning -structure dominance tree generalisation (SDTG). The basic idea is to collect the number of occurrences of different elements at a position. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": ". , T Ni }).", "text": "While syntactic anti-unification returns the intersection of a set of incident trees as prototype, SDTG returns the union of all incident trees (see Figure 2 for an illustration).\nA combination of both approaches can be realised as follows: -Initially the prototype is empty: P.\u03bb = .\n-We traverse the model tree top-down, starting with M.\u03bb.\n-Until I is empty, for the current position p in model tree M , M.p = e : \u03c4 \u2022 If for all incident tree in I holds I 1 .p = . . . = I N .p then P.p := e and if M.p = e(T 1 , . . . , T n ) then for all incidents I i do sdtgau(p.i, P, M, {T 1i , . . . , T Ni }). \u2022 Else P.p := [e i /f i ] : \u03c4 becomes a tuple of all occurring elements at position p together with their relative frequencies and we proceed for all children with sdtgau(p.i, P, M, {T 1i , . . . , T Ni }).\nAn illustrative example is given in Figure 2.\nRetrieval for SDTG and SDTGAU can be realised using some similarity measure over trees. We explored several measures and it showed, that Manhattan distance is the most robust and reliable measure:\nd(I, P ) = n i=1 |f P i \u2212 f Ii |\nwith\nf P i = 1.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15"], "table_ref": []}, {"heading": "Empirical Results", "text": "For empirical evaluation of our structural approaches to incident mining, we obtained 57 real example incidents from SAP support which were manually grouped in 11 clusters based on their root cause analysis. One cluster contained three incidents, four clusters four incidents each, one cluster five incidents, four clusters six incidents each and one cluster 9 incidents. The rather low number of instances per cluster is realistic for this application domain. That is, for this domain only approaches to prototype or classifier learning which produce reliable results for small numbers of cases are applicable. The size of incidents varied between 27 and 2646 nodes with an average size of 812 nodes.\nIn addition to the approaches described above, we used the inductive logic programming algorithm Foil (Quinlan & Cameron-Jones, 1995) which is a well established approach to learning from relational data. Since Foil needs positive and negative examples for its rule induction, for each cluster we used one incident from all other clusters as negative example. Furthermore, for using Foil the incident model was represented as set of Prolog clauses given as background knowledge.\nAll evaluations were run on an Intel Core 2 Duo with 2.4 GHz, 2 GB DDR3 working memory, 250 GB hard disk space and operation system Mac OS X 10.5.6. Algorithms were realised with Java Sun JDK 1.5.0 16-b06-284. Foil in version 6 was compiled with GCC 4.01. Since running times for all trials had a very low standard deviation for prototype generation as well as for retrieval, we only give average run times and omit giving standard deviations.\nFor a first evaluation, for each of the 11 clusters the prototypes were generated over all incidents (see Table 1). Afterwards, each instance was used in the retrieval phase. Times for generation of prototypes and for retrieval were averaged over all runs. In general, times for all approaches were reasonably fast. As was to be expected, anti-unification returned perfect results with the smallest prototypes (168 nodes in average). Foil produced a rather large number of erroneous and ambiguous classifications. This result is mostly due to our unsophisticated approach for presentation of negative examples. Since negative examples are used to specialise rules, typically some care in providing suitable negatives is needed. Both SDTG and SDTGAU returned no perfect, but acceptable results.\nA more precise evaluation was realised using a leave-one-out approach (see Table 2. Due to the small number of examples, we used three runs for prototype generation disregarding the first, the second and the third incident in each cluster respectively. Again, times are given as averages over all runs. Again, all times for prototype construction and for retrieval are acceptable. Now anti-unification returns the wrong prototype in 4 out of 33 cases which is better than SDTG but slightly worse than SDTGAU.\nFinally, we evaluated how our approaches can deal with noisy data. Out of the original 11 clusters, we created 110 new clusters, each containing n \u2212 1 incidents of an original cluster and one incident of one of the other clusters (see Table 3). As was to be expected, anti-unification breaks down for noisy data. SDTGAU outperformed all other approaches returning only 1 misclassification. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_5", "tab_20"]}, {"heading": "Conclusions and Further Work", "text": "For the given application domain -mining of incident reports characterised by an incident model -we could show first promising results for a set of simple structure-generalisation algorithms. Of course, the number of clusters and incidents used in the evaluation is rather small and a further evaluation using a larger data base should be realised. Using a small set of incidents had the advantage that we had full control over clustering which was done manually by a domain expert. For a large scale performance evaluation, we need to extend our approach to automated clustering as a first step.\nFor clustering we again propose to take into account the incident structures. That is, we plan to realise a clustering based on structural similarity between instances (Taskar, Segal, & Koller, 2001).\nThe proposed algorithms do not take into account the possibility that a tree element might have more than one child of the same type. A planned extension of the algorithms therefore is, to include tree matching to obtain best matches for arbitrary sets of type-identical nodes.\nAfter extension of our approach to automated clustering, we plan to realise a semi-automated assistance tool for support engineers with SDTGAU as generalisation algorithm: For an incoming incident, a ranked list of retrieved prototypes can be offered. If the engineer accepts one of these prototypes, the new incident is saved in the selected cluster. The support engineer furthermore can edit the prescribed support routines associated with the cluster prototypes. In repeated intervals -after substantial growth of the incident data base -automated clustering and prototype generalisation can be re-done to stratify the data-base. We assume that such a tool might relieve support engineers of repetitive work, considerably heighten efficiency of support and ultimately provide fast and reliable support for the users. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In this paper we discuss how to learn alarm functions in a real world problem. Starting from a faithful description of present circumstances, these functions must predict future situations of risk so that we may then act to prevent any foreseeable damage. In this context, the costs of prediction errors are not symmetrical: the consequences of false prediction of an alarm (false positives) are often not as serious as those of predicting false non-alarms (false negatives). We deal with continuous target variables whose values above a given threshold should be notified as soon as possible with the highest degree of accuracy. We try, at least, to minimize the percentage of false negatives. The straightforward approach is to learn a regressor: in addition to providing alarm alerts, the regressor produces a numeric assessment of how serious the situation may be.\nWe present an agriculture case study. The incidence of coffee rust epidemics is caused by a fungus called Hemileia vastatrix Berk. & Br., a devastating disease to coffee plantations. This incidence can be measured by the percentage of leaves infected by the fungus. It is well known that the factors that stimulate the growth of the fungi are weather conditions, the type of plantation and the current incidence. Thus, a regression learning task must include these features as predictors and the future incidence as the target value.\nWe trained a Regression Support Vector Machine (SVM) with quite good results. The correlation between predicted and actual incidences is about 0.94 in a cross-validation experiment. However, if we try to devise an alarm system for predicting values above a given threshold, we find that the number of false negatives is too high. In this case the threshold is \u03c4 = 4.5. This is not an academic parameter, it is the threshold used in Brazilian plantations; see [6,7,8] for a detailed discussion.\nTo overcome this weakness of regression, we try to learn models to predict approximations to incidence values instead of exact values. To implement this idea, there are a number of possible alternatives. In the approach presented here, we relax the specifications of regression, changing target points for intervals of a fixed width, say 2 . Following [1,5], these predictors may be called nondeterministic regressors.\nThe method employed to learn intervals of fixed width uses regression SVM. These learning algorithms search for predictors that minimize a loss function which ignores errors situated within a certain distance ( ) of the true value: -insensitive loss functions.\nTo transform interval predictions into alarms, we adopt a cautious policy. Only those intervals completely included below the threshold will be understood as non-alarms. On the other hand, if a predicted interval is above the threshold, that will mean an alarm. However, we have a third possibility situated somewhere in between: predicted intervals that include points above and below the threshold. We label these situations as warnings. The usefulness of warnings is that they capture classification errors of pure deterministic regressors. In fact, these errors arise for predictions near the threshold.\nIn other words, we can convert misclassifications into a type of situation that may require deeper analysis. However, when the alarm system predicts an alarm, and especially a non-alarm, the confidence in these predictions is very high. The radius of the intervals is proportional to the number of warnings and hence to the prudence of the whole alarm system. In this sense, our approach is closely related to that of classifiers with a reject option [2,4].\nOur conclusion is that a trade off between the number of false negatives and warnings would lead to a useful alarm system for coffee rust. The search for an optimal value for is beyond the scope of this paper, as we would have to consider the important economic and environmental aspects involved in coffee growing. Nonetheless, considering the results reported at the end of the paper (Section 6), the feasibility of implementing an alarm system is guaranteed. Moreover, the only requirement is a cheap weather station.\nIn the next section the coffee rust disease and the dataset used in the paper are presented in detail. Sections 3 and 4 are devoted to deterministic and nondeterministic alarms respectively. In Section 5 we discuss the temporal perspective of the alarms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Coffee Rust", "text": "The coffee rust caused by fungi Hemileia vastatrix Berk. & Br. is the main coffee crop disease in the world. In Brazil, damages lead to yield reduction of up to 35% in regions where climate conditions are propitious to the disease. The impact is thus considerable due to the economic importance of coffee crop.\nThe traditional way to prevent the disease is to apply agrochemical fungicides on fixed calendar dates. However, the fungicides contaminate the environment and reduce the quality of the coffee. Moreover, as the intensity of the disease between seasons suffers major variations, the use of agrochemicals is not always justified.\nThe aim of this paper is to discuss the viability of building alarm functions to alert on high incidences of coffee rust. The purpose would be to build economically viable control measures. Our proposal is a predictor, learned using data mining tools, that would allow applying agrochemicals only when necessary, leading to healthier products and reductions in costs and environmental impact.\nIt is important to emphasize here that fungicides must by applied in advance since they need several days to take effect in coffee plants. Having all this in mind, we used a dataset [6,7,8] whose temporal dimension is very important. The data was obtained on a monthly basis from an experimental farm (Funda\u00e7\u00e3o Procaf\u00e9, Varginha, MG, Brazil), from October 1998 to October 2006, with reports of coffee rust incidences. In September of each year (beginning of agricultural season), eight plots producing coffee were selected, four with thin spacing (approximately 4000 plants/ha) and four with dense spacing (approximately 8000 plants/ha). For each case, two plots were selected with high fruit load (above 1800 kg/ha) and two with low fruit load (below 600 kg/ha). There was no disease control in those plots. Meteorological data was automatically registered every 30 minutes by a weather station close to where the incidence of coffee was being evaluated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Learning Task", "text": "From a formal point of view, throughout this paper we deal with the dataset described as follows.\nLet X be a set of descriptions of current situations. Here we wanted to represent, using the data collected, the idea that an alarm system can be used at any time, not only from the first day of one month to guess the incidence in the first day of the next month. In the coffee rust problem, if we want to predict the incidence of the fungi in a target day, we consider predictions made with different days ahead. Thus X is a set of vectors whose components are: The weather scores are 13 variables per day, and they include: temperatures, solar radiation, number of hours with sun light, wind speeds, rain, relative humidity, number of hours with relative humidity above 95%, average temperature during these hours, and the same values but during the night. For more details, see [7,8].\nTherefore, the dimension of the vectors of the input space X is 590. On the other hand, the output space in this case is just the interval of real numbers, Y = [0, 100], to capture the percentage of coffee leaves inflected by the fungi.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Regression and Deterministic Alarms", "text": "We start presenting the baseline approach obtained from a standard regression tool. From a formal point of view, learning tasks can be presented in the following general framework. Let X be an input space, and let Y be an output space. A learning task is given by a training set S = {(x 1 , y 1 ), . . . , (x n , y n )} drawn from an unknown distribution P r(X, Y ) from the product X \u00d7 Y. The aim of such a task is to find a hypothesis h (of a space H of functions from X to Y) that optimizes the expected prediction performance (or risk) on samples independently and identically distributed (i.i.d.) according to the distribution P r(X, Y ):\nR \u0394 (h) = \u0394(h(x), y) d(P r(x, y)),(1)\nwhere \u0394(h(x), y) is a loss function that measures the penalty due to the prediction h(x) when the true value is y.\nIf Y is a metric space (usually the set of real numbers), the learning job is a regression task, as in the case of coffee rust. In this case, the aim of learners is to obtain a hypothesis whose predictions are as similar as possible to actual values in the output space. This can be accomplished, for instance, using least squares regression.\nOn the other hand, the goal of SVM regressors is to minimize the so-called -insensitive loss function. If is a positive value, this loss does not penalize predictions whose distance to true values is below ; in symbols,\n\u0394 (h(x), y) = max{0, |h(x) \u2212 y| \u2212 }.\n(2)\nIn any case, once we have learned a regressor h, if \u03c4 is a threshold in Y, we interpret the outputs of h as follows\nAlarm(h(x)) = non-alarm h(x) \u2208 (\u2212\u221e, \u03c4] alarm h(x) \u2208 (\u03c4, +\u221e).(3)\nNotice that the performance of what has been learned can be measured in two different but complementary ways: using the scores of regressors applied to h, and the scores of classifiers applied to Alarm \u2022 h.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Regression with Broad Insensitive Zone: Nondeterministic Alarms", "text": "Let us assume that we have a regressor whose accuracy to predict a continuous variable is not completely satisfactory. For instance, the performance of a regressor may fail when it is measured in terms of alarm classifications (Eq. 3). This is the case of regressors obtained from the coffee rust learning task. The scores will be discussed later in Section 6. To overcome this problem, as was explained in the introduction, we are going to use regressors allowed to predict intervals rather than single points. The idea is that the true class of an entry x may be somewhere into the predicted interval for x. A simple way to implement this idea is to look for regressors that predict intervals of a fixed width, say 2 . Notice that this is exactly the semantics of -insensitive zone (Eq. 2). For later reference, we recall the formulas of SVM regressors here.\nGiven a regression learning task S (Section 3) and a tube value > 0, a regression SVM learns a function\nh (x) = n i=1 (\u03b1 \u2212 i \u2212 \u03b1 + i )K(x i , x) + b * , (4\n)\nwhere K is the rbf kernel, K(\nx i , x j ) = e \u2212 x i \u2212x j 2 2\u03c3 2\n; b * , and \u03b1 + , \u03b1 \u2212 are respectively the solution and the Lagrange multipliers of the following convex optimization problem:\nmin w,b,\u03be 1 2 w, w + C n i=1 (\u03be + i + \u03be \u2212 i ), s.t. ( w, \u03c6(x i ) + b) \u2212 y i \u2264 + \u03be + i , y i \u2212 ( w, \u03c6(x i ) + b) \u2264 + \u03be \u2212 i , (5) \u03be + i , \u03be \u2212 i \u2265 0, i = 1, . . . , n.\nThe interval regressor associated to h is then defined by\nh ND( ) (x) = [h (x) \u2212 , h (x) + ].(6)\nNotice that we have one optimal interval regressor for each value of the tube, .\nThe regressor h , accordingly to (Eq. 5) is different for each value of . When the aim is to learn a deterministic regressor, typically is a small number; by default, we use = 0.1. However, for interval predictions, we may use wider tubes. However, the problem of interval regressors is that they are not as precise as regular regressors. There is some degree of vagueness in interval predictions. For this reason, following [1,5], we call them nondeterministic predictors.\nTo handle this type of predictions, we have to reformulate the alarms associated with a regressor. We need to interpret predictions that include, at the same time, alarms and non-alarms. Our proposal is to label these situations as warnings: something between alarms and non-alarms. The scores reported in Section 6 will illustrate the advantages of nondeterministic alarm functions.\nFormally, we propose the following extension of (Eq. 3). Given an interval regressor h ND , if \u03c4 is a threshold in Y, we shall interpret the outputs of the regressor as follows\nAlarm(h ND (x)) = \u23a7 \u23a8 \u23a9 non-alarm h ND (x) \u2282 (\u2212\u221e, \u03c4] alarm h ND (x) \u2282 (\u03c4, +\u221e) warning otherwise. (7\n)\nAdditionally, from the classification point of view, given a nondeterministic regressor, h ND , for a test set S = {(x 1 , y 1 ), . . . , (x m , y m )}, it is important to measure the proportion of test examples that fall outside the tube\nout tube(h ND , S ) = 1 m m i=1 1 \u2212 y i \u2208 h ND (x i ) . (8\n)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Time Series Alarms", "text": "As was explained in Section 2.1, an alarm system for coffee rust would be able to be used at any time. To simulate this capacity, given an incidence percentage y measured the first day of one month, we considered different values t \u2208 [30, 25,20,15,10] for the number of days ahead of predictions. For each t we have the corresponding weather records. Thus, in the learning task, for each y we have a time series\n{(x t , y) : t \u2208 [30, 25, 20, 15, 10]}. (9\n)\nTo evaluate the sequence of alarm alerts produced by an interval regressor h ND in (Eq. 9), the idea is that if an alarming prediction occurs for some t, then the reaction would be to use the agrochemical fungicides; any subsequent notice of non-alarm would not be heard. On other hand truly non-alarming predictions for y would need a sequence of non-alarms for all t values. Formally, this point of view is captured by the following definition\nAlarm(h ND (x t )) = \u23a7 \u23a8 \u23a9 non-alarm \u2200t, h ND (x t ) \u2282 (\u2212\u221e, \u03c4] alarm\n\u2203t, h ND (x t ) \u2282 (\u03c4, +\u221e) warning otherwise.\n(10) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "In this section we report a number of experiments carried out to illustrate the role played by the width of intervals involved in alarm predictions. With the dataset introduced in Section 2.1, we used a 10-fold cross validation to estimate the scores reported in the following figure and tables. As was mentioned in the introduction, in all the experiments, the threshold used to discriminate alarms was \u03c4 = 4.5.\nThe SVM regressors were learned using LibSVM [3], with an rbf kernel. The parameters C and \u03c3 were adjusted using an internal grid search in each training set. The ranges for this search were: C \u2208 [0.001, 0.01, 0.1, 1, 10, 100, 1000], and \u03c3 \u2208 [0.01, 0.1, 0.3, 0.5, 0.7]. The search employed an internal 2-fold cross validation repeated 3 times; the aim being to optimize the average \u0394 (Eq. 2).\nFirst we compared the scores achieved from the point of view of regression for different values of and different plantation types. The results are gathered in Table 1.\nWe observe that correlations are quite different from the data of plantations with low (l = 1) and high (l = 2) fruit loads. The quality of regressors is worse in the case of low fruit load. However, the correlation obtained for the whole dataset is quite high, around 0.94. The value of the radius of the predicted interval, , has no influence on these results. But, of course, has a dramatic impact in the proportion of points outside the tube. Here, the results range from almost all to 0.57. Obviously, it is easier to include examples inside wider tubes.\nIn Figure 1 we represent graphically the predictions and true values. To make the figure more clear, we show only a subset of examples: predictions made one month ahead. We used the predictions of the interval regressor learned with = 4, h ND(4) ; that is, a regressor whose predictions are intervals with a width of 8. We can appreciate that the errors are higher when predictions range from 30 to 50.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2"]}, {"heading": "Time series.", "text": "In Table 2 we report the results obtained by the alarm functions obtained for the time series described in Section 5. In this case, in crossvalidations we took care that time series (Eq. 9) were never separated into train and test splits.\nThe table shows the confusion matrices obtained in cross-validations. For deterministic regression, the default value of the insensitive zone used was = 0.1. In this case, of course, there are no doubtful classifications: no warnings appear in the corresponding columns of Table 2. Unfortunately, the consequence is that the number of errors is too high: 14 false non-alarms, and 16 false alarms.\nTable 2. Confusion matrices obtained (using cross-validation over time series) for different values of the radius of the insensitive zone or tube. Columns represent true classes: alarm (a), non-alarm (\u00aca). Rows report the occurrences of each possible prediction (Pre) (alarm, warning (w), non-alarm) for each combination of load (l) and spacing (s). The last rows shows the scores considering at the same time all types of plantations; that is, the sum of the corresponding confusion matrices. If we use wider predicted intervals ( \u2265 1), the number of errors decreases dramatically, but the price is that there is an increase in the number of warning predictions. Thus, for = 1 the number of false non-alarms is only 5 with 22 warnings (6.5% of all cases). Let us remark that all these false non-alarms are due to plantations with low fruit load (l = 1), which is coherent with the results obtained for regression scores, see Table 1. With \u2265 2, the number of false nonalarms is zero, but the warnings rise to 14.4%, 19.7% and 23.2% respectively for = 2, 3, 4.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_5", "tab_5", "tab_2"]}, {"heading": "Conclusion", "text": "We discussed the viability of an alarm system for coffee rust, the main coffee crop disease in the world. In this case, the aim is to apply the chemical prevention of the diseases only when necessary to achieve healthier products and reductions in cost and environmental impact. But we must be vigilant to avoid false nonalarms since they would conduct to not prevent an awful increase in the incidence of the disease.\nThe approach presented here proposes to handle predictions about continuous variables by regressors able to predict intervals rather than single points. They can be learned from regression learning tasks using the so-called -insensitive zone ( is the radius of the predicted intervals). An optimal solution can be obtained by Regression Support Vector Machines.\nThe use of interval predictors allow us to distinguish a third type of situations placed between alarms and non-alarms. We called them warnings. Roughly speaking, we found that the confidence in non-alarm predictions is higher as increases, while it is quite stable for alarm predictions. Somehow, the alarm predictor becomes more prudent, but requires more frequently deeper analysis to decide what to do in uncertain (warning) situations.\nA trade off between the number of non-alarms and warnings would lead to a useful alarm system for the coffee rust. If we want to search for an optimal value for , we must consider the important economic and environmental aspects involved in coffee growing.\nFinally, it is worth noting here that the cost of implementing the alarm systems presented in this paper is very low. The only requirement is a cheap weather station able to register the data described in Section 2.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Many network applications benefit greatly from the utilization of reliable load forecasting. Examples of such application include content distribution systems, grid computing schedulers and load balancing systems in specialized overlay networks.\nWe have developed a specialized measurement system for the purpose of web traffic measurements. The MWING (which stands for multi-agent web-ping, [3]) is a distributed measurement framework, designed as a platform for the continuous probing of the end-to-end HTTP transaction performance. Deploying a dedicated measurement system in the Internet, instead of using already existing platform (like PlanetLab, [15]), allowed us to obtain possibly reliable real-world traffic data. In addition, MWING system potentially gives the maximum flexibility and control over the examination schemes, and is capable of carrying out other experiments.\nIn our research, we are interested in the prediction of the transmission rate as observed in a typical web server communication. We consider the estimation of goodput -the number of useful bytes transmitted in a time unit. Our aim is to find a way to predict the level of goodput on a given path, with the help of measurement system, similar in capabilities to MWING. The expected goodput gives the most practical information about the network performance, perceived by the user. However, due to its nature, the forecasting of goodput is generally a difficult problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The predictability of the end-to-end throughput has been already studied from many different aspects. Various time series based and formula based approaches have been used for the performance forecasting (e.g. [16], [8]). For summary of the study of IP traffic nature (self-similarity, burstiness) see [1] and [14]. Recently, the methods from artificial intelligence and machine learning are more willingly applied to the prediction of Internet traffic characteristics (e.g. [10], [13]).\nHowever, many of these ideas are still pending for a rigorous verification in the live experiments, performed in evolving Internet.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Selection", "text": "The MWING system is designed to provide the estimates of throughput and goodput between a selected set of client machines and HTTP servers in the Internet. A single probe from a client gives the measured time periods of the following communication stages:\n-Dns -time to resolve address via DNS lookup -Con -time to establish TCP connection, -First -time elapsed between sending HTTP GET request, and receiving the first response packet -Last -remaining time of the HTTP transaction.\nAdditionally, in-between delays are measured, denoted as D2S (between the end of Dns and sending first TCP SYN packet), and A2G (between sending TCP ACK packet and sending GET request). The total measured time is\nT = Dns + D2S + Con + A2G + First + Last.\nThe goodput estimate is Gpt = S/T , where S denotes the transmitted resource size expressed in bytes. These S bytes include only the essential information we want to fetch from the server (e.g. hypertext file, video object, etc.), no matter how many additional bytes are needed to transmit it (for error correction, protocol stack information, etc.). The goodput can be thought of as an application level throughput.\nMWING performs the measurements continuously in 30 minutes intervals. Consequently, we consider only the effects discernible on at least 1 hour spans. All the measurements were performed simultaneously from four client machines (called measurement agents). Three of them were located in the university networks in Poland: Wroclaw (WRO), Gdansk (GDA), Gliwice (GLI). The fourth one was located in Las Vegas, USA (LAS). Since the performance of a path could be affected by many unknown factors [8], it is reasonable to make use of several different predictors. Some shortterm factors, which we do not measure directly, are caused, for example, by the cross-traffic on the parts of the path from client to server, the instantaneous server load, and changes in intermediate hop directions due to the routing algorithms. The long-term factors are, for example, major changes in routing tables or changes of the server's hardware or software.\nConsequently, the goodput level prediction procedure will make use of the historical measurements to grasp the long-term factors. Because the older the observations we take into account, the more we have to \"average\" over the relevant information, we reduce the dataset taken into account to a shifting-window of a fixed size. The decision needs to be made upon the short-term influences as well, thus we assume that we make a prediction for the instant of time directly after a performed measurement. This approach leads to the use of time series analysis. We propose the adaptation of autoregressive moving average models [4], as discussed in detail in Section 5.\nIn the first step of our analysis we considered correlation matrices estimated for various measurement periods. We found the most significant predictors for Gpt and LogGpt 1 random variables, as illustrated in Table 1. It is evident that in most cases the coefficients for LogGpt are prevailing, since the partial times show stronger linear dependencies, compared to Gpt. There is, as expected, strong negative correlation with Dns, Con and First values (the goodput is inversely proportional to the sum of these time periods). The Last value describes the major part of client-server transaction time, however in case of long transactions, the MWING-type measurement of this value cannot be used for on-line prediction. We also leave out the D2S and A2G, to reduce the number of features in the model.\nWe have performed tests for statistical significance of the periodic exogenous factors in [6], i.e. time of the day (Hour), and day of the week (DoW). As expected, in the majority of cases, the influence of Hour is evident, and this is supported by our results. Taking DoW is reasonable in some cases for longer training sets. We used both standard one-way F-statistic ANOVA test, as well as rank based Kruskal-Wallis test. These test may reveal both linear and nonlinear dependencies. Assuming the level of significance \u03b1 = 0.05, the influence of the day of the week was observed as follows: 86% (LAS), 85% (WRO), 64% (GDA), 46% (GLI) 2 . Similarly, the influence of the local hour is proven to be significant; our test results were: 56% (LAS), 87% (WRO), 62% (GDA), 50% (GLI).\nThe goodput time series are nonlinear, which is easy to show, even for short measurement periods, e.g. with the use of BDS test [5]. In essence, such time series should be considered as a composition of deterministic (chaotic) and stochastic processes, see Fig. 1.\nFig. 1. An example of 1000 subsequent measurements of LogGpt on a single path (lasting for nearly 2 months). There is a significant drop slightly after the half.\nIn order to justify the use of autoregressive part in our models, we have analyzed the autocorrelation of goodput time series. The estimated values were usually high only for very short lags, see Fig. 2. This explains the general low predictability of the goodput. Nevertheless, the time-local information on the trend can be very useful, as we cannot tell in advance how correlated the series will be.\nIn summary, the general model to learn from the MWING datasets is:\nLogGpt n \u223c f (LogGpt n\u22121 , LogGpt n\u22122 , . . . ; Dns, Con, First, Hour, DoW)(1)\nwhere Dns, Con and First are continuous variables, and Hour (0-23) and DoW (1-7) denote the local time instant of the measurement. The last two variables are considered as the exogenous inputs. Each input is normalized onto the [0, 1] interval, via the formula:\nz i = (x i \u2212 min x i )/(max x i \u2212 min x i ).\nFor class-based prediction, only a discrete set of values of LogGpt are allowed, each representing certain traffic level.\nThe function f can be any appropriate nonlinear model, as discussed further in Section 5. Such models may make use of higher order predictors, or nonlinear transformations of predictors. In the time series modelling terminology, these models can be classified as a nonlinear autoregressive exogenous models (NARX).", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_15"], "table_ref": ["tab_2"]}, {"heading": "Performance Level Differentiation via Clustering", "text": "We have analyzed both classification and regression problems, which arise in the web traffic engineering. The prediction of exact goodput value is a difficult task, however for most applications we are interested only in its average level. We utilize the two-stage data mining approach, in order to determine (and be able to update later) the appropriate traffic intensity levels.\nOn a fixed client-server path we usually observe cumulation of goodput around typical values. Because of the various slowing factors, appearing at random, the goodput distributions are generally not unimodal (see example on Fig. 3). In fact, the probability densities of Gpt (and LogGpt) for an end-to-end connection would be best modelled as a composition of a small number of shifted bell-shaped curves, as a mixture model:\nf (x) = K n=1 a n f n (x; \u0398 n ) ( 2)\nwhere K n=1 a n = 1. Simplifying the description, each component could count for one setup in the routing tables of the network core; the longer we observe the link, the more such components may appear.\nIf we assume the gaussian components f n (x; \u0398 n ), we can make use of maximum likelihood principle for estimating parameters of model \u0398 n (assuming the number of classes K). The expectation-maximization algorithm (EM) is a well known tool, useful for this kind of problems [12]. This approach has been already successfully used in the Internet traffic modelling, see [11].\nInstead of assuming the number of classes in advance, we can perform the EM based clustering, to find the best number of components K (in a desired range). This technique makes use of the Bayesian Information Criterion [9], to evaluate the quality of model for different K. The subsequent runs of EM algorithm result in models for which the maximized value of the likelihood function is taken for comparison. The BIC criterion is estimated as RSS/\u03c3 2 e +k ln(n), where RSS is the residual sum of squares, \u03c3 e is the error variance (we assume normal distribution of errors), k = 2 is the number of free parameters, and n is the sample size. We repeat the whole process for different K, and select the model with lowest BIC, minimizing the average log-likelihood.\nKnowing the components f n (x; \u0398 n ) we calculate the decision boundaries, i.e. the points x i separating classes:\n\u2200 i=1,...,K\u22121 x i : f i (x i ; \u0398 i ) = f i+1 (x i ; \u0398 (i+1) ).\nThe advantage of Gaussian components is that the boundary points can be found easily, in the same way as in the linear discriminant analysis. Taking the logarithms of two components, the problem simply reduces to the solution of quadric equation (if variances happen to be the same, this reduces to linear equation). Let (\u03bc 1 , \u03c3 1 ) and (\u03bc 2 , \u03c3 2 ) be two components. The boundary points are obtained by solving:\n(\u03c3 2 1 \u2212 \u03c3 2 2 )x 2 + 2(\u03bc 1 \u03c3 2 2 \u2212 \u03bc 2 \u03c3 2 1 )x + \u03c3 2 1 \u03bc 2 2 \u2212 \u03c3 2 2 \u03bc 2 1 \u2212 2(\u03c3 1 \u03c3 2 ) 2 ln(\u03c3 1 /\u03c3 2 ) = 0\nwith respect to x. Now with each of the K intervals:\n[0, x 1 ), [x 1 , x 2 ), . . . , [x K\u22121 , \u221e)\nwe associate a performance (average goodput level) class. Note that their sizes can vary significantly, see Fig. 3. Moreover, for a given server, one clustering stays valid only for a limited time.", "publication_ref": [], "figure_ref": ["fig_28", "fig_28"], "table_ref": []}, {"heading": "Predictive Analysis", "text": "In order to obtain a good predictive model, which is both accurate and flexible for the on-line learning, we have tested several state of the art machine learning algorithms. The full presentation of our research is beyond the scope of this text. Here we present two examples of NARX classifiers, which were among the top performers in the general settings: the autoregressive exogenous model with neural network classifier (NNET), and with k-nearest neighbor classifier (KNN).\nOur approach to the time series forecasting is based on the adapting off-theshelf statistical classifiers into the autoregressive models. This can be thought of as the on-line learning paradigm, since we would like to use the predictor continuously in time, and expect it to adapt to the changing conditions. Such classifier tries to reconstruct the dynamics of the underlying process.\nVirtually any classifier f can be adopted to the Model 1, possibly with some special tweaks. A NARX classifier is provided with a shifting window of a fixed size of w last observations of the feature vector. This vector includes both endogenous inputs (Dns, Con, First) and exogenous (Hour, DoW). Additionally, it includes the autoregressive part, i.e. the previous l values of LogGpt (l is the lag parameter). After training the classifier with a set of w observations, we make one prediction, the value of LogGpt w+1 . For all subsequent predictions, we drop the oldest measured LogGpt from the training set (along with the oldest training vector), and include the predicted one.\nThe autoregressive part contributes to the detection of local trends in the time series, while the remaining part allows for seeking the best probabilistic decision, given a feature vector.\nThe NNET model makes use of 2-layer feedforward backpropagation neural network for the classification. The hidden layer typically contains 7-12 sigmoid units. The decision is made by rounding the output to the nearest class.\nThe KNN model uses nonlinear transformation of the input space. Let r = Dns + Con + First. It uses the terms of degree 3 polynomial of r as endogenous inputs. Given an input vector, the decision is made by selecting the class, for which the majority of k nearest training vectors belong. Such classifier tries to approximate the optimal (Bayesian) decision, i * = arg max P (i|x), by relaxing the conditional probability within a small neighborhood of the feature vector instance x [7]. This classifier is considered nonlinear, as its resulting decision boundaries can adjust to any shape.\nFor the purpose of model performance comparison we use the lag parameter l = 1, and compare different shifting-window sizes w: 1 day, 3 days, 1 week, 2 weeks and 3 weeks. We have tested the models against a collection of goodput measurements, taken in between 2008.4.24 -2009.7.5 by MWING system. The datasets contained missing values, due to the temporary server or network malfunctions. Some of the observed servers were subject to higher traffic than others and, in result, some of them provided a more challenging prediction problem than others. We did not take into consideration situations with very low variance traffic, for which the number of observations in different clusters is highly unequal (for example over 90% of training cases belonged to one cluster). In such cases the best prediction degrades to a priori probability decision.\nFigure 4 shows the classification rate of the both models for 25 servers. For each server the EM clustering was run prior to the classification (there were 4-5 clusters at average). The clustering stage was repeated after every 5 full window shifts, updating the traffic levels' boundaries. Figure 5 presents an example single server case: the boundaries are updated 6 times, each update accounts for one week. This shows how the overall traffic shaping can vary in time.\nFor a small group of considered servers a very accurate prediction (over 90% correct) was achieved. For the majority of them, with correctly set up class boundaries, it was possible to achieve 75-80% classification rate.   For comparison, we also considered reverse approach, i.e. solving analogous regression problem (prediction of exact value of LogGpt), followed by discretization into previously clustered classes. Using exactly the same prediction scheme, the average regression error was around 5-7% (depending on client/server dataset, see Fig. 6).\nGenerally, the NNET model performed substantially better, however for some datasets the performance was comparable. A big advantage of the KNN model is that its implementations can be very fast. For the reasonable sizes of the shifting window the decision making is instant. This allows for the on-line realtime parameter tuning, holding simultaneously a group of similar classifiers. In both cases the automated selection of the best classifier can be applied, given enough computational resources. Even with delayed updates, the models stay fairly accurate for many weeks.", "publication_ref": [], "figure_ref": ["fig_29", "fig_116", "fig_117"], "table_ref": []}, {"heading": "Conclusions", "text": "Next generation networks, as we believe, would benefit from the use of performance prediction techniques. In particular, there is a strong interest among network operators and providers in obtaining the full control over the quality of Internet services. Predictive analysis can be also very useful for designing customized network solutions.\nThe nonlinear autoregressive models with exogenous inputs appear to be among the methods of choice for this purpose. These models combine the advantages of the time series based prediction techniques with the static machine learning inference for cumulated observations. Using neural network based predictors, adapted to our on-line learning scheme, we have achieved even over 90% prediction rate on several web servers. With the combination of gaussian mixture clustering, the presented two-stage methodology allows for the fully automatic recognition of the web traffic intensity in the Internet.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In the 1990s, most of the Brazilian power companies went private and started operating, under concession from the government, regulated by the National Agency of Electrical Energy (ANEEL = Ag\u00eancia Nacional de Energia El\u00e9trica) and inspected by the National System Operator (ONS = Operador Nacional do Sistema). The companies operating in this regulated market are paid for the service they provide and are penalized for system unavailability at the Operational Function (FUNOP = FUN\u00e7\u00e3o OPeracional) level [1]. Each unavailability penalty depends on the value of the FUNOP asset, its characteristics, the duration of the power interruption and, mainly, if the interruption had been planned or not; an unplanned unavailability costs roughly 20 times more than a planned one of the same duration [1].\nThe reliability of electrical power grids is already very high and under continuous improvement. Each FUNOP is composed of several equipments which implement an operational function in power generation, transmission or distribution. For preserving this high reliability profile, strict maintenance plans are periodically conducted on these equipments, with particular features for each family of equipments.\nIn general, the maintenance plan is made according mainly to the equipment manufacturer's recommendations. That takes into account the electrical load, the temperature and other aspects to define the periodicity, the procedures and parameter monitoring and adjustments. The equipment manufacturers have carried out series of trials within their plants and also collect data from their costumers' installations and apply statistical methods for defining their maintenance recommendations.\nHowever, there are many other factors interfering in the system reliability in different power grids such as the quality of the repairmen's labor, ways of loading the system etc. There is also a major aspect to be considered; as the system's quality improves, less data about risky conditions are produced. Therefore, the better the system becomes, the less data about faults will be available for statistical modeling of risky conditions. Fortunately, more data from monitoring operation in normal conditions are being collected and will be available for future modeling.\nInstead of using the traditional statistical modeling, this paper introduces an approach based on behavioral data. That may seem odd if one thinks of the system operating in a stable regime, under a constant fault rate. However, as the faults are very rare events, it is not possible to assure a constant fault rate and the adherence in the hypothesis test always gives at least a small difference; behavioral consolidation of data may capture variations which are important for risk estimation. The results presented here support this idea.\nThis paper is organized in six more sections. Section 2 characterizes the unavailability problem faced by CHESF (Companhia Hidro El\u00e9trica do S\u00e3o Franscisco) with the data structure available and the integration and transformation needed. Section 3 shows the modeling of the problem as a binary decision based on the maintenance plan, the creation of behavioral variables and the selection of the most relevant variables for modeling. Section 4 describes the knowledge extraction process via a bagged ensemble of logistic regression models. Section 5 presents and interprets the results achieved on a statistically independent data set. Section 6 summarizes the important contributions, the limitations of the approach and future work to be done to broaden the research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Characterization", "text": "CHESF (Companhia Hidro El\u00e9trica do S\u00e3o Franscisco) is one of the biggest Power generating company in Brazil producing 10,618 MW in 14 hydroelectric power plants and 1 thermoelectric. It also transmits this energy along an 18-thousand km long power grid [2]. Its annual revenue has reached R$ 5.64 billion (= US$ 3.15 billion) in 2008, with a net profit of R$ 1.43 billion. Unfortunately, the revenue losses caused by penalties for unavailabilities still remain undisclosed. CHESF's power grid has 462 FUNOPs of 7 different families with an average of 39 equipments in a total of 17.8 thousand equipments with an average age of approximately 19 years of operation.\nThe seven different FUNOP families are: transmission lines, power transformers, reactors, capacitor banks, synchronous compensators, static compensators and isolated cables.\nJust before being put in operation, the equipments and FUNOPs are registered in the Asset Management System (SIGA = Sistema de Gerenciamento de Ativos). After becoming operational, the equipments have all their maintenances, planned or not, recorded in the same system (SIGA).\nEach unavailability, no matter the cause, is recorded in the accountability system within the Asset Management System (SIGA). Unavailabilities that occurred before of January 1, 2008 were recorded in the system (DISPON) which had no direct link to the SIGA system.\nThese two data sources hosted in two different systems with relational databases needed to be integrated in a single data mart because they are the basis for the unavailability risk estimation system to be developed.\nThe difference in granularity between the DISPON and SIGA databases and the consequent lack of a unique key together with the legacy systems turned this database integration into a non-trivial task.\nAsset registration and their maintenance records have been integrated in the SIGA system for the last two years but there were several adjustments in data imported from legacy systems for previous periods in a much longer history.\nThe most important difficulty faced however was the integration with the DISPON system where each unavailability recorded had not been linked to an equipment maintenance action. Furthermore, DISPON had been abandoned without any data importation to the current SIGA installed only 2 years ago. So, the unavailability data were dumped from the legacy database (DISPON) and were joined to the current SIGA database to form the complete unavailability database. These integration steps alone took around 60% of the project duration, having required a lot of interactions with the IT management and electrical engineers at CHESF.\nThe purpose of this work is to estimate the risk of occurring unavailabilities in the FUNOPs which compose the power grid at each moment. At this point, it is important to emphasize a trick made to turn the risk assessment problem into a binary decision problem for data mining. Considering that unavailabilities caused by planned maintenances are negligible in cost compared to unplanned ones (only 1:20 ratio), and that maintenance actions reset the operational status of the system to optimal, the temporal sequence of planned maintenance actions defines a frame of time intervals where the presence or absence of unplanned unavailabilities characterize the binary target for the supervised training process. This binary target definition approach will be explained in the next section, along with the creation of behavioral information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Transformation", "text": "The variables present in the integrated database in a relational architecture needed to be transformed into more meaningful variables concerning the binary decision problem characterized for modeling the unavailability risk assessment problem. This section explains the proposed random variables that produce the most adequate mapping from the original input space to the data mart variables. It also presents how the binary decision target was defined.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variable Creation", "text": "Behavioral data are widely used in behavior scoring for credit risk assessment [3] and other business applications. In that domain, in general, it consists of the RFM (Recency, Frequency and Monetary value) variables' creation approach [4]. For systems' faults at CHESF, the approach was adapted to capture the relevant sequential features implicit in each event within the FUNOP related to recency, frequency and impact along time for faults and errors (RFI approach). In this approach, the impact is measured by the duration, cost and other features related to each system component / event. This is a very important basis for systematic and automatic creation of behavioral variables, considering several time spans. Other variables inherent to the FUN-OPs and related to their complexity were created, such as the amount of equipments, the families of equipments and the entropy of the equipment distribution within the FUNOP. This is a Domain Driven Data Mining approach [5] of embedding the expert's knowledge from the electrical engineering field into the decision support system. The RFI approach can be generalized to model rare events in several application domains where the impact is captured by several different metrics (to be published elsewhere).\nAnother important aspect is that, due to the very small amount of faults per equipment, their rate of faults is defined at the equipment family level. Several \"ratio\" variables were created for measuring differences from a FUNOP to the population. So, the ratio of the average rate of faults per family of equipments within a FUNOP and the average in the whole grid form an important set of variables. At this point, it is important to highlight that, in general, equipments are not replaced or swapped in the power grid; they are simply maintained.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proposed Model and Label Definition", "text": "Considering the scarcity of data about system faults and the consequent high imprecision of the estimated distributions and fault rates, the approach adopted here was to convert a classical statistical problem into a data mining problem with the advantage of reducing the amount of limiting assumptions in the modeling process.\nIn this approach, the temporal sequence of planned maintenance actions defines a frame of time intervals used for modeling and labeling the system condition. The label is defined as \"bad\" if there is at least one unplanned unavailability within that time interval and, \"good\" otherwise. This characterizes the binary target needed for the supervised training process of the decision support system [6].\nThe set of all planned maintenances defines a sequence of time intervals, each of which possess a binary label and takes into account all the past history of the FUNOP and its components (behavior), as illustrated in Fig. 1. Fig. 1. Planned maintenances define a sequence of time intervals for modeling the problem as a binary decision and labeling the target An approximation has been made in the approach depicted above, considering the negligible cost of the planned unavailabilities compared to the un-planned ones (1:20 ratio) and the fact that planned unavailabilities may be produced during a planned maintenance itself. Therefore, planned unavailabilities were discarded from the training data for the modeling process. No other constraint has been made concerning data distribution types or their parameters, different from the statistical approaches.\nThe goal of this modeling approach is to take preventive actions whenever a 'bad' prediction is made within a time interval. Despite not being in the long term maintenance plan, this short term planned maintenance action produces either negligible penalty (1:20 of the fault unavailabity penalty) or no penalty at all (several preventive maintenance actions do not cause unavailability).", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Variables Selection", "text": "As the process of systematic creation of behavioral variables makes it very easy to automatically produce new variables, variable selection is needed to preserve only the most meaningful and discriminative variables. The selection process was based on an approach for maximizing the information gain of the input variables in relation to the binary target and, simultaneously, minimizing the similarity (redundancy) among the input variables selected, measured by appropriate metrics, in a univariate fashion.\nAs all input variables were numerical and the target binary, the Max_KS (Kolmogorov-Smirnov) metric [7] was used for ranking the variables by their univariate discriminative importance. The redundancy among input variables was measured by linear correlation. The input variables with correlation higher than 0.9 with other variables of higher Max_KS were discarded from the model. Following this approach, only 30 among over 900 input variables were preserved. Table 1 lists the top five most relevant variables selected with their information gain measured in terms of Max_KS and AUC_ROC (Area under the ROC Curve) [8], to be explained in Subsection 5.1.\nIt is clear that unplanned unavailability along the last two years of operation is the most relevant aspect for estimating the risk of unavailability before the next planned maintenance. It is interesting that the equipments' age appear only in 22 nd place in the ranking with Max_KS=0.09 and AUC_ROC=0.48, suggesting that the system fault rate is indeed at the flat part of its curve. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Data Sampling", "text": "As the modeling strategy involves the creation of behavioral variables, there is statistical dependence among the examples, differently from typical classification problems. Therefore data division for modeling and testing the system should to be temporally disjoint in two blocks, as done in time series forecasting tasks [9] for more realistic performance assessment. The diagram in Fig. 2 below shows this division in time.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Fig. 2. Data partition along time for modeling and performance assessment of the system", "text": "This data partition took into account the change in the computational environment to represent the worst case in terms of performance assessment. In the modeling set, the target class (unavailability) represents 18.1% of the examples whereas, in the testing set, it represents only 10.5% of the examples. An additional difficulty is related to the differences in the way data were recorded before and after SIGA, which not even CHESF's personnel can precisely assess.\nThe modeling data refer to the whole period before the SIGA system was deployed while the testing data have their target defined after SIGA's deployment. The behavioral variables of the testing data, however, also capture historical information from the preceding period.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Logistic Regression and Model Ensemble", "text": "The modeling technique chosen was logistic regression for several interesting features it possesses being the quality and understandability of the solution produced and the small amount of data required the most relevant features for this work. Logistic regression has been successfully applied to binary classification problems, particularly to credit risk assessment [3], it does not require a validation set for over-fitting prevention and it presents explicitly the knowledge extracted from data in terms of statistically validated coefficients [10].\nAs preliminary experiments with different data samples showed a high variance in performance, it was clear that an ensemble of systems was necessary [11]. In this work, the ensemble consisting of 31 Logistic Regression models has reduced the system's variance and their median was taken as the response for each test example. This median approach had been adopted by the authors' teams since 2007 in PAKDD Data Mining Competition [12] and in NN3 Time Series Forecasting Competition [13]. As already stated, the modeling technique chosen was Logistic Regression due to its explicit coefficients and for not having the need of a validation set. For training the 31 models, 50% of the examples in the modeling data set were randomly sampled without replacement. These parameters were chosen by linear experimental project [14] with the ensemble size taking the values 31, 51 and 101 and the percentage taking the values 70% 60% and 50%.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Metrics, Results and Interpretation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Performance Metrics", "text": "As there was no criterion available yet for defining the decision threshold along the continuous output of the logistic regression ensemble, the performance assessment was carried out using two metrics for the whole decision domain (the score range): the maximum Kolmogorov-Smirnov distance (Max_KS) [7] and the Area Under the ROC Curve (AUC_ROC) [8]. The AUC_ROC metric is widely accepted for performance assessment of binary classification based on continuous output. Similar wide acceptance holds for the Max_KS within the business application domain.\nDifferently from its original purpose as a statistical non parametric tool for measuring the adherence of cumulative distribution functions (CDF) [7], in binary decision systems, the KS maximum distance is applied for assessing the lack of adherence between the data sets from the 2 classes, having the score as independent variable. The Kolmogorov-Smirnov Curves are the difference between the CDFs of the data sets of the two classes. The higher the curve, the better the system and the point of maximum value is particularly important in performance evaluation.\nAnother widely used tool is the Receiver Operating Characteristic Curve (ROC Curve) [8] whose plot represents the compromise between the true positive and the false positive example classifications based on a continuous output along all its possible decision threshold values (the score). The closer the ROC curve is to the upper left corner (optimum point), the better the decision system is. The focus is on assessing the performance throughout the whole X-axis range by calculating the area under the ROC curve (AUC) [8]. The bigger the area, the closer the system is to the optimum decision which happens with the AUC_ROC equal to one.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Interpretation", "text": "Performance was assessed on the testing set which consisted of the out-of-sample data with 4,059 examples reserved for this purpose only. Fig. 3 shows the Kolmogorov-Smirnov curve with its Max_KS=0.341. Fig. 4 shows the ROC curve with its AUC_ROC=0.699. The curves are quite noisy probably because of the small amount of data in the testing set. There are only around 400 examples from the target class in this data set whose CDF is a very noisy curve (top plot in Fig. 3) whereas the non-target class (\"good\") is a smooth curve. Even being noisy, the performance curves are consistent and present an improvement which will be useful, for CHESF, particularly considering that the testing set represents a worst case approximation.", "publication_ref": [], "figure_ref": ["fig_28", "fig_29", "fig_28"], "table_ref": []}, {"heading": "Concluding Remarks", "text": "This paper has presented a domain driven data mining approach to the problem of Operational Function unavailability in the electrical power grid of one of the biggest power companies in Brazil -CHESF.\nDifferent from statistical approaches, this innovative work has modeled the unavailability as a data mining binary decision problem with behavioral input variables. These variables were created by sliding windows of different sizes timed by the planned maintenance events which were labeled as \"bad\" when an unplanned unavailability occurred before its next planned maintenance.\nAn important advantage of this approach compared to the statistical ones is that it does not impose any constraint on the data distributions to be modeled. The only approximation made was to consider the planned unavailability's cost negligible compared to that of an unplanned one; around 5% of the value.\nIt should be emphasized here that there is a big difference between the concepts of approach and technique which becomes clear when the statistical technique logistic regression is used within a domain driven data mining approach for modeling the whole problem as a sequence of rare events consolidated in RFI variables which capture sequential information in terms of Recency, Frequency and Impact.\nThe median of an ensemble of bagged logistic regression models has provided the unavailability's risk estimating score and its coefficients made explicit the most relevant variables for each suggested decision.\nResults of the experiments carried out on an out-of-sample test set have shown that the approach is viable for risk estimation. It attained a Max_KS=0.341 and AUC_ROC=0.699, in a worst case scenario.\nAfter this approach's validation, the testing data set has been included in the modeling data set and the system has been re-trained with the same procedure. Now, the system has just been put in operation and its performance will be monitored for the next six months when CHESF will be making pro-active maintenance based on the system predictions. Both the quality the solution and the availability of the power grid can lead to redesigning maintenance periods.\nSeveral refinements still have to be made, particularly, those referring to the revenue losses caused by the penalties for power grid unavailability. This refinement can be made by considering the \"losses\" either in the modeling process or in the postprocessing stage along with the risk estimating score produced by the decision support system. Also, the variable selection process should include multivariate techniques such as the variance inflation factor (VIF) [15].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In user centered multi-agent systems, agents assist users by managing data that they delegate. To tackle the privacy preservation problems that are induced by automatic processing of sensitive data, we proposed the model of Hippocratic Multi-Agent System (HiMAS) [6]. HiMAS proposes a MAS framework integrating the management of private sphere with nine principles to preserve privacy.\nThis model requires to specify and implement regulation mechanisms for the detection of suspicious agents (i.e. agents that violate the private sphere). The private sphere and all relative information are personal and subjective. By this way, external regulation mechanisms like PONDER [10] or the electronic institutions like ISLANDER [8] for example are not appropriate because this kind of systems requires that an entity knows every communicated sensitive data, but this is in fact a privacy violation.\nIn this article, we integrate in HiMAS an internal regulation mechanism that protects the private sphere: a social order based on trust and reputation. We propose a generic framework for trust model in order to transform a social order from hippocratic point of view for privacy preservation.\nSection 2 presents the HiMAS model. Section 3 focuses on the trust model installing the social order in the system. In section 4 we present a hippocratic social order that regulates agents behavior with regards to privacy. To do so we integrate the HiMAS principles in the process and check their respect for all exchanged sensitive data. The experimental validation of our proposition is presented in the next section. We finally conclude and provide some perspectives.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hippocratic Multi-Agent Systems (HiMAS)", "text": "In this section we briefly recall the basic principles of the model of Hippocratic Multi-Agent Systems [6] that we have have proposed for privacy preservation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Private Sphere, Consumer and Provider", "text": "In a HiMAS, the private sphere is modeled by the set of sensitive data to be preserved with their management rules. A HiMAS agent can play two roles in the context of data communication (called sensitive data transaction): it may be a provider that sends sensitive data to another agent that is called the consumer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Nine Normative Principles to Preserve Privacy", "text": "In order to preserve privacy, a HiMAS imposes agents to respect nine normative principles inspired by the hippocratic databases ) are embedded in a sensitive data transaction protocol [5] that we briefly present in the next subsection. In this article, we focus on the compliance principle, that we propose to model with the social order based on trust and reputation (section 3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sensitive Data Transaction Protocol", "text": "The sensitive data transaction protocol [5] allows HiMAS agents to check the intention of the consumer in relation to the required data thanks to a content language. This content language defines all the possibilities for the sensitive data manipulations in terms of collection, use, disclosure, retention and openness regarding the respect of the private sphere after a transaction. The HiMAS agents use this content language to build their policy (resp. preference) when they endorse the consumer (resp. provider). Policy and preference specify the objectives of the transaction, the disclosure list, the future uses and the retention time of the sensitive data. This protocol allows the HiMAS agents to detect suspicious behaviors: if an agent does not respect the manipulations specified by the content language, it is considered as suspicious and the transaction is cancelled. The provider sends required sensitive data to the consumer when there is an agreement between their policy and preference, otherwise the transaction is also cancelled.\nAfter a sensitive data transaction, the agents attach the policy to the communicated sensitive data and the compliance principle can act: each agent should be able to check the respect of the policy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Trust and Reputation Model for Social Order", "text": "The social order proposed in [3] represents the collaboration between agents based on trust and more particularly on a recommendation process, using propagated reputation [2], an internal behavior regulation mechanism. The respect of the private sphere does not allow external regulation mechanism due to its personal and subjective aspect 1 [6]. In this direction, we need to choose a trust model that takes these two parameters into account and that also allows to build a hippocratic social order with regards to privacy preservation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-agent Trust Model", "text": "Our research context being the user centered multi-agent systems, the user is usually delegating her private sphere to an autonomous agent but also to all the regulation process. In order to adapt our model for every applications, we propose to extend the Castelfranchi and Falcone model [4] that is designed for a general context of multi-agent systems, to introduce a hippocratic social order into a HiMAS because it does not depend on the domain and this makes its adaptation easier to any context. Moreover, it is a cognitive model that corresponds to a user centered approach [9].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Trust Model for Hippocratic Social Order", "text": "Castelfranchi and Falcone defines in [4] the concept of trust as a task delegation between agents: an agent delegates a task to another one only if a trust relationship exists between these two agents.\nTrust is then studied as a set of mental states that are based on three kinds of information in order to establish a trust relationship according to a given context (or a given action) \u03a9: the direct experiences, the recommendations and the systemic trust (i.e. direct, propagated and stereotyped reputation). In a HiMAS, \u03a9 represents the objectives of the sensitive data transaction.\nIn order to establish a trust relationship with an agent j, an agent i compares its trust belief of a trust function to a threshold where 0 means no trust and 1 an absolute trust for j in the given context \u03a9.\nApplying the compliance principle requires to give HiMAS agents the ability to check the respect of the other principles. To do so, we make a first strong hypothesis: each HiMAS agent respects the compliance principle by denouncing every suspicious behavior it detects according to the sensitive data policy.\nThe trust model is grounded to a punishment model: only the privacy violations change the trust belief negatively. A trust relationship can be destroyed but cannot be rebuilt because no reward system is applied, so that the trust judgement can only decrease.\nThe social order we propose allows the HiMAS agents to pass a judgement on the reliability of the other agents according the four kinds of constraints, that represent the facet of trust belief, specified in a policy: use, disclosure, retention and openness. The three information sources must be linked to this four facets, noted f , the consumer and the sensitive data transaction context. We define the compilation of these information sources as: In order to get a trust value [4], the HiMAS agents should determine two beliefs that belong to the interval [0,1]:\n-DoA c,\u03a9,f (Degree of Ability) = F A (DoDR c,\u03a9,f , DoP R c,\u03a9,f ) where F A is the compilation of DoDR and DoP R.\n-DoW c,\u03a9,f (Degree of Willingness) = F W (DoP R c,\u03a9,f , DoST c,\u03a9,f ) where F W is the compilation of DoP R and DoST .\nThe trust belief a provider assigns to a consumer c according to the facet f and the given context \u03a9 is determined by the following function:\nDoT c,\u03a9,f (Degree of Trust) = F (F A c,\u03a9,f , F W c,\u03a9,f )\nThe last step to establish a trust relationship consists in combining of the trust belief for each facet:\nDoT c,\u03a9 = f (f A c,\u03a9,f1 ..f A c,\u03a9,fn , f W c,\u03a9,f1 ..f W c,\u03a9,fn )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hippocratic Social Order", "text": "The extension of the model proposed in [4] we propose allows us to introduce a hippocratic social order in HiMAS in order to model the compliance principle. This task requires first to establish a trust relationship with regards to the HiMAS model. Then we need to model the basis of the hippocratic social order after a sensitive data transaction between a consumer and a provider. These two steps allow us to implement our proposition and to determine the possible impacts on the HiMAS agents. We do not evaluate the trust model performance but its use in order to model the compliance principle.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hippocatric Trust Relationship", "text": "Establishing a trust relationship allows to preserve the private sphere by detecting suspicious agents. This relationship allows agents of a HiMAS to pass a judgement on consumers reliability about the respect of their policies. We consider that all the trust information, and more particularly the propagated reputations, are sensitive data and must be protected by the HiMAS principles, because these data are in direct relation to the users represented by the agent.\nTo guarantee the respect of the private sphere, we propose to integrate the process of trust building and management in a hippocratic model that will be independent of the trust model that is used and is generic for each model that manage the propagated reputations. This hippocratic model allows to preserve trust data thanks to the nine HiMAS normative principles.\nIn order to respect the protocol we have proposed in [5], we extend the content language with the specific case of the propagated reputations with a specific objective \"social order\". This objective defines that the received data are the propagated reputations and that these kinds of data must only be used in order to pass a judgement on another agent or to revise an old judgement. Moreover, propagated reputations must not be disclosed and must be accessible only during the step of the corresponding process.\nWith the hippocratic social order, HiMAS agents should exchange propagated reputations. With this protocol, we impose to agency to provide their own trust data and to not forward those of another agent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sensitive Data Transaction and Compliance Principle", "text": "HiMAS agents being now able to pass a judgment on the consumers reliability, we need to introduce trust in the sensitive data transaction protocol [5].\nTaking into account this aspect introduces a new step in the reasoning mechanism of the provider (see Figure 1 in red). The provider uses its trust beliefs to decide about the acceptation of the consumer policy. In case this level is correct, the provider decide to continue the transaction, else to cancel the transaction. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Scenario and Parameters", "text": "To evaluate our proposition, we chose a scenario related to a specific domain: the management of calendars [7]. Each user is represented by an agent that manages her calendar. All the agents share and disclose randomly the meetings using the sensitive data transaction protocol proposed in [5].\nAfter each sensitive data transaction, the consumer links the corresponding policy to the sensitive data it receives and includes it in its private sphere. When the consumer becomes a provider and when it sends this sensitive data, it also respects this policy and sends it as a preference that the new consumer must respect. By this way, the new consumer can check the reliability of the past consumer. If the policy has been violated, the new consumer sends a warning to the agency and the trust level of the suspicious decreases. During the initialization of an agent, its calendar is randomly created with a set of meetings. The DoST c,\u03a9,f , DoDR c,\u03a9,f and DoP R c,\u03a9,f for each agent and for each facet get the value of 1. The policies and preferences are chosen randomly in the ontology of the content language presented in [5]. A suspicious agent is an agent that violates its policy every time for every facet. This agent is banished of the agency when a minority of blocking (a third of the agency) considers it as suspicious. In order to not banish an agent too much quickly 2 (see Figure 2), we have fixed the decrease of reputation to 0,15. The threshold deciding when to destroy a trust relationship is fixed to 0,5.\nIn order to test our hippocratic social order, we experiment 20 times this scenario with different sets of agents with one suspicious agent and according to three kinds of networks: a social network (no dependance constraint between agents), a tree network (an agent can interact with its relative and its son) and a layered network (an agent can interact with its relatives and its sons).\nThe second kind of experiments focuses on the number of suspicious agents. In a set of 50 agents organized in social networks, we introduce 1, 5, 12, 18 and suspicious agents in the agency. By this way, we study the limitations of our proposition according to the percentage of suspicious agents.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Results", "text": "Network topology. In social networks, the detection of a suspicious agent is not related to the number of agents in the agency. Indeed, only about 2,5 sensitive data transactions per agent are required with the suspicious agent for its detection (see Figure 3). For tree networks, due to an important requirement of memory because of the number of communications, our experiments could not exceed a society of 50 agents. In fact, a simulation with 50 agents requires around 1940 transactions that is approach approximately the required number for 150 agents in the social network. However, even if the number of sensitive data transactions increases, the required number of transactions in order to detect suspicious agents remains at the same level that for the social networks (Figure 3).\nIn the layered networks, the number of sensitive data transactions is higher than in the social networks (see Figure 3), but the required number of sensitive data transactions with the suspicious agents is about 3,5 transactions per agent (see Figure 3) and so the performance is the same than in the social networks. To sum up, we can conjecture that the number of agents and the network topology do not really influence the detection of the suspicious agent: each agent needs about 3 sensitive data transactions for its detection.", "publication_ref": [], "figure_ref": ["fig_28", "fig_28", "fig_28", "fig_28"], "table_ref": []}, {"heading": "Number of suspicious agents.", "text": "We can notice that the increase of the number of suspicious agents in a agency can affect the suspicious behavior detection. Indeed, as shown at Figure 4, the results are in the same interval as the ones previously presented until a percentage of 30% of the agency: the average of sensitive transaction for the suspicious agent detection per agent and per suspicious agent is the same for only one agent. After this percentage, the hippocratic social order becomes less powerful because the average of the sensitive data transaction per agent become more important.", "publication_ref": [], "figure_ref": ["fig_29"], "table_ref": []}, {"heading": "Conclusions and Perspectives", "text": "In order to model and implement the compliance principle defined in the HiMAS model, we have used a trust model that allows us to enforce privacy preservation without any global view of policies for the agency. The trust model that we propose is an extension of Castelfranchi and Falcone model [4]. It includes the preferences of the users for the trust management using reputations.\nWe have also investigated privacy preservation in the trust process by considering trust information as sensitive data. By this way, we extend the content language proposed in [5] in order to take the objective of social order into account and introduce trust in the sensitive data protocol to check consumer reliability.\nOur perspectives first focus on the implementation of our proposition in other trust models in order to determine their influence on privacy preservation.\nIn another direction of work, we propose to extend a multi-agent system application dealing with decentralized calendar management [7] that already uses the sensitive data transaction protocol [5] by implementing the hippocratic social order.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Electronic market trading involves the complete process of: need identification, product brokering, supplier brokering, offer-exchange, contract negotiation, and contract execution [1], as well as the ability to model business relationships. Three core technologies are needed to support electronic markets:\ntrading agents -intelligent agents that are designed to operate in tandem with the real-time information flows received from data mining systems. -data mining -real-time data mining technology to tap information flows from the marketplace and the World Wide Web, and to deliver timely information at the right granularity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "virtual institutions -virtual places on the World Wide Web in which in-", "text": "formed trading agents can trade securely both with each other and with human agents in a natural way -not to be confused with the term \"virtual organisations\" as used in Grid computing.\nThis paper describes an e-trading system that integrates these three technologies. The e-Market Framework is available on the World Wide Web 1 . This project aims to make informed automated trading a reality, and develops further the \"Curious Negotiator\" framework [2]. This work does not address all of the issues in automated trading. For example, the work relies on developments in: XML and semantic web, secure data exchange, value chain management and financial services.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Mining", "text": "We have designed information discovery and delivery agents that utilise text and network data mining for supporting real-time negotiation. This work has addressed the central issues of extracting relevant information from different online repositories with different formats, with possible duplicative and erroneous data. That is, we have addressed the central issues in extracting information from the World Wide Web. Our mining agents understand the influence that extracted information has on the subject of negotiation and takes that in account.\nReal-time embedded data mining is an essential component of the proposed framework. In this framework the trading agents make their informed decisions, based on utilising two types of information: First, information extracted from the negotiation process (i.e. from the exchange of offers). Second, information from external sources, extracted and provided in condensed form.\nThe embedded data mining system provides the information extracted from the external sources. The system complements and services the informationbased architecture developed in [3] and [4]. The data mining system initially constructs data sets that are \"focused\" on requested information. From the vast amount of information available in electronic form, we need to filter the information that is relevant to the information request. In our example, this will be the news, opinions, comments, white papers related to five models of digital cameras. Technically, the automatic retrieval of the information pieces utilises the universal news bot architecture presented in [5]. Developed originally for news sites only, the approach is currently being extended to discussion boards and company white papers.\nThe \"focused\" data set is dynamically constructed in an iterative process. The data mining agent constructs the news data set according to the concepts in the query. Each concept is represented as a cluster of key terms (a term can include one or more words), defined by the proximity position of the frequent key terms. On each iteration the most frequent (terms) from the retrieved data set are extracted and considered to be related to the same concept. The extracted keywords are resubmitted to the search engine. The process of query submission, data retrieval and keyword extraction is repeated until the search results start to derail from the given topic.\nThe set of topics in the original request is used as a set of class labels. In our example we are interested in the evidence in support of each particular model camera model. A simple solution is for each model to introduce two labelspositive opinion and negative opinion, ending with ten labels. In the constructed \"focused\" data set, each news article is labelled with one of the values from this set of labels. An automated approach reported in [5] extends the tree-based approach proposed in [6].\nOnce the set is constructed, building the \"advising model\" is reduced to a classification data mining problem. As the model is communicated back to the information-based agent architecture, the classifier output should include all the possible class labels with an attached probability estimates for each class. Hence, we use probabilistic classifiers (e.g. Na\u00efve Bayes, Bayesian Network classifiers [7] without the min-max selection of the class output [e.g., in a classifier based on Na\u00efve Bayes algorithm, we calculate the posterior probability P p (i) of each class c(i) with respect to combinations of key terms and then return the tuples < c(i), P p (i) > for all classes, not just the one with maximum P p (i). In the case when we deal with range variables the data mining system returns the range within which is the estimated value. For example, the response to a request for an estimate of the rate of change between two currencies over specified period of time will be done in three steps: (i) the relative focused news data set will be updated for the specified period; (ii) the model that takes these news in account is updated, and; (iii) the output of the model is compared with requested ranges and the matching one is returned. The details of this part of the data mining system are presented in [8]. The currently used model is a modified linear model with an additional term that incorporates a news index Inews, which reflects the news effect on exchange rate. The current architecture of the data mining system in the e-market environment is shown in Figure 1. The {\u03b8 1 , . . . , \u03b8 t } denote the output of the system to the information-based agent architecture. In addition, the data mining system provides parameters that define the \"quality of the information\", including:\nthe time span of the \"focused\" data set, defined by the eldest and the latest information unit); -estimates of the characteristics of the information sources, including reliability, trust and cost, that then are used by the information-based agent architecture.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Trading Agents", "text": "We have designed a new agent architecture founded on information theory. These \"information-based\" agents operate in real-time in response to market information flows.\nWe have addressed the central issues of trust in the execution of contracts, and the reliability of information [4]. Our agents understand the value of building business relationships as a foundation for reliable trade. An inherent difficulty in automated trading -including e-procurement -is that it is generally multi-issue. Most of the work on multi-issue negotiation has focussed on one-to-one bargaining -for example [9]. There has been rather less interest in one-to-many, multi-issue auctions - [10] analyzes some possibilities.\nThe main focus of our agents is their information and their strength of belief in its integrity [11]. If their information is sufficiently certain then they may be able to estimate a utility function and to operate rationally in the accepted sense. However an agent may also be prepared to develop a semi-cooperative, non-utilitarian relationship with a trusted partner.\nAn agent called \u03a0 is the subject of this discussion. \u03a0 engages in multi-issue negotiation with a set of other agents:\n{\u03a9 1 , \u2022 \u2022 \u2022 , \u03a9 o }.\nThe foundation for \u03a0's operation is the information that is generated both by and because of its negotiation exchanges. Any message from one agent to another reveals information about the sender. \u03a0 also acquires information from the environment -including general information sources -to support its actions. \u03a0 uses ideas from information theory to process and summarize its information. \u03a0's aim may not be \"utility optimization\" -it may not be aware of a utility function. If \u03a0 does know its utility function and if it aims to optimize its utility then \u03a0 may apply the principles of game theory to achieve its aim. The information-based approach does not to reject utility optimization -in general, the selection of a goal and strategy is secondary to the processing and summarizing of the information.\nIn addition to the information derived from its opponents, \u03a0 has access to a set of information sources {\u0398 1 , \u2022 \u2022 \u2022 , \u0398 t } that may include the marketplace in which trading takes place, and general information sources such as news-feeds accessed via the Internet. Together, \u03a0, {\u03a9 1 , \u2022 \u2022 \u2022 , \u03a9 o } and {\u0398 1 , \u2022 \u2022 \u2022 , \u0398 t } make up a multiagent system. The integrity of \u03a0's information, including information extracted from the Internet, will decay in time. The way in which this decay occurs will depend on the type of information, and on the source from which it was drawn. Little appears to be known about how the integrity of real information, such as news-feeds, decays, although its validity can often be checked -\"Is company X taking over company Y?\" -by proactive action given a cooperative information source \u0398 j . So \u03a0 has to consider how and when to refresh its decaying information.\n\u03a0 triggers a goal, g \u2208 G, in two ways: first in response to a message received from an opponent {\u03a9 i } \"I offer you e1 in exchange for an apple\", and second in response to some need, \u03bd \u2208 N , \"goodness, we've run out of coffee\". In either case, \u03a0 is motivated by a need -either a need to strike a deal with a particular feature (such as acquiring coffee) or a general need to trade. \u03a0's goals could be short-term such as obtaining some information \"what is the time?\", mediumterm such as striking a deal with one of its opponents, or, rather longer-term such as building a (business) relationship with one of its opponents. So \u03a0 has a trigger mechanism T where: T : {X \u222a N } \u2192 G.\nFor each goal that \u03a0 commits to, it has a mechanism, G, for selecting a strategy to achieve it where G : G \u00d7 M \u2192 S where S is the strategy library. A strategy s maps an information base into an action, s(Y t ) = z \u2208 Z. Given a goal, g, and the current state of the social model m t , a strategy: s = G(g, m t ). Each strategy, s, consists of a plan, b s and a world model (construction and revision) function, J s , that constructs, and maintains the currency of, the strategy's world model W t s that consists of a set of probability distributions. A plan derives the agent's next action, z, on the basis of the agent's world model for that strategy and the current state of the social model: z = b s (W t s , m t ), and z = s(Y t ). J s employs two forms of entropy-based inference:\n-Maximum entropy inference, J + s , first constructs an information base I t s as a set of sentences expressed in L derived from Y t , and then from I t s constructs the world model, W t s , as a set of complete probability distributions. -Given a prior world model, W u s , where u < t, minimum relative entropy inference, J \u2212 s , first constructs the incremental information base I (u,t) s of sentences derived from those in Y t that were received between time u and time t, and then from W In the absence of new [info] the integrity of distributions decays.\nIf D = (q i ) n i=1\nthen we use a geometric model of decay:\nq t+1 i = (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 q t i , for i = 1, . . . , n(1)\nwhere \u03c1 D \u2208 (0, 1) is the decay rate. This raises the question of how to determine \u03c1 D . Just as an agent may know the decay limit distribution it may also know something about \u03c1 D . In the case of an information-overfed agent there is no harm in conservatively setting \u03c1 D \"a bit on the low side\" as the continually arriving [info] will sustain the estimate for D.\nWe now describe how new [info] is imported to the distributions. A single chunk of [info] may effect a number of distributions. Suppose that a chunk of [info] is received from \u03a9 and that \u03a0 attaches the epistemic belief probability R t (\u03a0, \u03a9, O([info])) to it. Each distribution models a facet of the world. Given a distribution D t = (q t i ) n i=1 , q t i is the probability that the possible world \u03c9 i for D is the true world for D. The effect that a chunk [info] has on distribution D is to enforce the set of linear constraints on D, J D s ([info]). If the constraints J D s ([info]) are taken by \u03a0 as valid then \u03a0 could update D to the posterior\ndistribution (p [info] i\n) n i=1 that is the distribution with least relative entropy with respect to (q t i ) n i=1 satisfying the constraint:\ni {p [info] i : J D s ([info]) are all in \u03c9 i } = 1. (2) But R t (\u03a0, \u03a9, O([info])) = r \u2208 [0, 1] and \u03a0 should only treat the J D s ([info]\n) as valid if r = 1. In general r determines the extent to which the effect of [info] on D is closer to (p\n[info] i\n) n i=1 or to the prior (q t i ) n i=1 distribution by:\np t i = r \u00d7 p [info] i + (1 \u2212 r) \u00d7 q t i (3)\nBut, we should only permit a new chunk of [info] to influence D if doing so gives us new information. For example, if 5 minutes ago a trusted agent advises \u03a0 that the interest rate will go up by 1%, and 1 minute ago a very unreliable agent advises \u03a0 that the interest rate may go up by 0.5%, then the second unreliable chunk should not be permitted to 'overwrite' the first.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Information reliability. We estimate R t (\u03a0, \u03a9, O([info]", "text": ")) by measuring the error in information. \u03a0's plans will have constructed a set of distributions. We measure the 'error' in information as the error in the effect that information has on each of \u03a0's distributions. Suppose that a chunk of [info] is received from agent \u03a9 at time s and is verified at some later time t. For example, a chunk of information could be \"the interest rate will rise by 0.5% next week\", and suppose that the interest rate actually rises by 0.25% -call that correct information [fact]. What does all this tell agent \u03a0 about agent \u03a9's reliability? Consider one of \u03a0's distributions D that is {q s i } at time s. Let (p\n[info] i\n) n i=1 be the minimum relative entropy distribution given that [info] has been received as calculated in Eqn. 2, and let (p The idea of Eqn. 3, is that the current value of r should be such that, on average, (p s i ) n i=1 will be seen to be \"close to\" (p\n[fact] i\n) n i=1 when we eventually discover [fact] -no matter whether or not [info] was used to update D. That is, given [info], [fact] and the prior (\nq s i ) n i=1 , calculate (p [info] i\n) n i=1 and (p\n[fact] i ) n i=1 using Eqn. 2.\nThen the observed reliability for distribution D, R\n([info]|[fact]) D\n, on the basis of the verification of [info] with [fact] is the value of r that minimises the Kullback-Leibler distance between (p s i ) n i=1 and (p\n[fact] i ) n i=1 : arg min r n i=1 (r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i ) log r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i p [fact] i\nIf E [info] is the set of distributions that [info] effects, then the overall observed reliability on the basis of the verification of [info] with [fact] is:\nR ([info]|[fact]) = 1 \u2212 ( max D\u2208E [info] |1 \u2212 R ([info]|[fact]) D |)\nThen for each ontological context o j , at time t when, perhaps, a chunk of [info], with O([info]) = o k , may have been verified with [fact]:\nR t+1 (\u03a0, \u03a9, o j ) = (1 \u2212 \u03c1) \u00d7 R t (\u03a0, \u03a9, o j )+ \u03c1 \u00d7 R ([info]|[fact]) \u00d7 Sem(o j , o k ) (4)\nwhere Sem(\u2022,\n\u2022) : O \u00d7 O \u2192 [0, 1]\nmeasures the semantic distance between two sections of the ontology, and \u03c1 is the learning rate. Over time, \u03a0 notes the ontological context of the various chunks of [info] received from \u03a9 and over the various ontological contexts calculates the relative frequency, P t (o j ), of these contexts, o j = O ([info]). This leads to a overall expectation of the reliability that agent \u03a0 has for agent \u03a9:\nR t (\u03a0, \u03a9) = j P t (o j ) \u00d7 R t (\u03a0, \u03a9, o j )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Negotiation", "text": "For illustration \u03a0's communication language is restricted to the illocutions: Offer(\u2022), Accept(\u2022), Reject(\u2022) and Withdraw(\u2022). The simple strategies that we will describe all use the same world model function, J s , that maintains the following two probability distributions as their world model:\n-P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) -the strength of belief that \u03a0 has in the proposition that she should accept the proposal \u03b4 = (a, b) from agent \u03a9 in satisfaction of need \u03bd at time t, where a is \u03a0's commitment and b is \u03a9's commitment. P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) is estimated from:\n1. P t (Satisfy(\u03a0, \u03a9, \u03bd, \u03b4)) a subjective evaluation (the strength of belief that \u03a0 has in the proposition that the expected outcome of accepting the proposal will satisfy some of her needs). 2. P t (Fair(\u03b4)) an objective evaluation (the strength of belief that \u03a0 has in the proposition that the proposal is a \"fair deal\" in the open market. 3. P t (\u03a0CanDo(a) an estimate of whether \u03a0 will be able to meet her commitment a at contract execution time.\nThese three arrays of probabilities are estimated by importing relevant information, [info].\n-P t (\u03a9Acc(\u03b2, \u03b1, \u03b4)) -the strength of belief that \u03a0 has in the proposition that \u03a9 would accept the proposal \u03b4 from agent \u03a0 at time t. Every time that \u03a9 submits a proposal she is revealing information about what she is prepared to accept, and every time she rejects a proposal she is revealing information about what she is not prepared to accept. Eg: having received the stamped illocution Offer(\u03a9, \u03a0, \u03b4) (\u03a9,\u03a0,u) , at time t > u, \u03a0 may believe that P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) = \u03ba this is used as a constraint on P t+1 (\u03a9Acc(\u2022)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Virtual Institutions", "text": "This work is done on collaboration with the Spanish Governments IIIA Laboratory in Barcelona. Electronic Institutions are software systems composed of autonomous agents, that interact according to predefined conventions on language and protocol and that guarantee that certain norms of behaviour are enforced.\nVirtual Institutions enable rich interaction, based on natural language and embodiment of humans and software agents in a \"liveable\" vibrant environment. This view permits agents to behave autonomously and take their decisions freely up to the limits imposed by the set of norms of the institution. An important consequence of embedding agents in a virtual institution is that the predefined conventions on language and protocol greatly simplify the design of the agents. A Virtual Institution is in a sense a natural extension of the social concept of institutions as regulatory systems that shape human interactions [12]. Virtual Institutions are electronic environments designed to meet the following requirements towards their inhabitants:\nenable institutional commitments including structured language and norms of behaviour which enable reliable interaction between autonomous agents and between human and autonomous agents; -enable rich interaction, based on natural language and embodiment of humans and software agents in a \"liveable\" vibrant environment.\nThe first requirement has been addressed to some extent by the Electronic Institutions (EI) methodology and technology for multi-agent systems, developed in the Spanish Government's IIIA Laboratory in Barcelona [12]. The EI environment is oriented towards the engineering of multiagent systems. The Electronic Institution is an environment populated by autonomous software agents that interact according to predefined conventions on language and protocol. Following the metaphor of social institutions, Electronic Institutions guarantee that certain norms of behaviour are enforced. This view permits that agents behave autonomously and make their decisions freely up to the limits imposed by the set of norms of the institution. The interaction in such environment is regulated for software agents. The human, however, is \"excluded\" from the electronic institution.\nThe second requirement is supported to some extent by the distributed 3D Virtual Worlds technology. Emulating and extending the physical world in which we live, Virtual Worlds offer rich environment for a variety of human activities and multi-mode interaction. Both humans and software agents are embedded and visualised in such 3D environments as avatars, through which they communicate. The inhabitants of virtual worlds are aware of where they are and who is there -elements of the presence that are excluded from the current paradigm of e-Commerce environments. Following the metaphor of the physical world, these environments do not impose any regulations (in terms of language) on the interactions and any restrictions (in terms of norms of behaviour). When this encourages the social aspect of interactions and establishment of networks, these environments do not provide means for enabling some behavioural norms, for example, fulfilling commitments, penalisation for misbehaviour and others.\nVirtual Institutions addressed both requirements, retaining the features and advantages of the above discussed approaches. They can be seen as the logical evolution and merger of the two streams of development of environments that can host electronic markets as mixed societies of humans and software agents.\nTechnologically, Virtual Institutions are implemented following a three-layered framework, which provides deep integration of Electronic Institution technology and Virtual Worlds technology. The Electronic Institution Layer hosts the environments that support the Electronic Institutions technological component: the graphical EI specification designer ISLANDER and the runtime component AMELI [13]. At runtime, the Electronic Institution layer loads the institution specification and mediates agents interactions while enforcing institutional rules and norms.\nThe Communication Layer connects causally the Electronic Institutions layer with the 3D representation of the institution, which resides in the Social layer. The causal connection is the integrator. It enables the Electronic Institution layer to respond to changes in the 3D representation (for example, to respond to the human activities there), and passes back the response of the Electronic Institution layer in order to modify the corresponding 3D environment and maintain the consistency of the Virtual Institution. Virtual Institution representation is a graph and its topology can structure the space of the virtual environment in different ways. This is the responsibility of the Social layer. In this implementation the layer is represented in terms of a 3D Virtual World technology, structured around rooms, avatars, doors (for transitions) and other graphical elements. Technically, the Social layer is currently utilising Adobe Atmosphere virtual world technology. The design of the 3D World of the Virtual Institution is developed with the Annotation Editor, which ideally should take as an input a specification of the Electronic Institution layer and produce an initial layout of the 3D space. Currently, part of the work is done manually by a designer.\nThe core technology -the Causal Connection Server, enables the Communication Layer to act in two directions. Technically, in direction from the Electronic Institution layer, messages uttered by an agent have immediate impact in the Social layer. Transition of the agent between scenes in the Electronic Institution layer, for example, must let the corresponding avatar move within the Virtual World space accordingly. In the other direction, events caused by the actions of the human avatar in the Virtual World are transferred to the Electronic Institution layer and passed to an agent. This implies that actions forbidden to the agent by the norms of the institution (encoded in the Electronic Institution layer), cannot be performed by the human. For example, if a human needs to register first before leaving for the auction space, the corresponding agent is not allowed to leave the registration scene. Consequently, the avatar is not permitted to open the corresponding door to the auction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "A demonstrable prototype e-Market system permits both human and software agents to trade with each other on the World Wide Web. The main contributions described are: the broadly-based and \"focussed\" data mining systems, the intelligent agent architecture founded on information theory, and the abstract synthesis of the virtual worlds and the electronic institutions paradigms to form \"virtual institutions\". These three technologies combine to present our vision of the World Wide Web marketplaces of tomorrow.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Despite the substantial advances in multiagent systems and automated negotiation [1], it is perhaps surprising that negotiation in electronic business [2] remains a substantially manual procedure. Multi-agent systems often make use of human-agent teams especially in complex areas that concern contract negotiations Electronic trading environments may be overwhelmed by information, including information drawn from general resources such as the World Wide Web using smart retrieval technology [3]. We propose that rather than strive to make strategic, economically rational decisions, intelligent agents in electronic markets should capitalise on the real-time information flows and should aim to make 'informed decisions' that take account of the integrity of all relevant information. Traditional agent architectures, such as the Belief-Desire-Intention (BDI) model, do not address directly the management of dynamic information flows of questionable integrity. This paper describes an agent architecture that has been designed specifically to operate in tandem with information discovery systems.\nThis is part of our e-Market Framework that is available on the World Wide Web 1 . This framework aims to make informed automated trading a reality, and aims to address the realities of electronic business, namely \"what you know is the most important matter\". This work does not address all of the issues in automated trading [2]. For example, the work relies on developments in: XML and semantic web, secure data exchange, value chain management and financial services. Further the design of electronic marketplaces is not described here.\nIntelligent agents and the information theory based architecture designed specifically to cope with real-time information flows are described in Sec. 2.\nThe management of dynamic information flows is described in Sec. 3. The interaction of more than one of these agents engaging in competitive negotiation is described in Sec. 4. Sec. 5 concludes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Information-Theoretic Foundation for Agents", "text": "We have designed a new agent architecture founded on information theory. These \"information-based\" agents operate in real-time in response to market information flows. The central issues of trust in the execution of contracts is discussed in [4] [5]. The \"information-based\" agent's reasoning is based on a first-order logic world model that manages multi-issue negotiation as easily as single-issue.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rationale", "text": "This section provides the rationale for the formal work that follows.\nPercepts, the content of messages, are all that an agent has to inform it about the world and other agents. The validity of percepts is constrained: by the agent's uncertainty in the reliability of the sender of the message, by the elapsed time since the message arrived, and by the agent's level of individual caution in its belief. The information-based agent's world model is deduced from the percepts using inference rules that transform percepts into statements in probabilistic logic.\nThe integrity of percepts decreases in time. The way in which it decreases will be determined by the type of the percept, as well as by the issues such as uncertainty about a senders reliability, elapsed time and caution. An agent may have background knowledge concerning the expected integrity of a percept as t \u2192 \u221e. Information-based agents represent this background knowledge as a decay limit distribution. If the background knowledge is incomplete then one possibility for an agent is to assume that the decay limit distribution has maximum entropy whilst being consistent with the data.\nAll messages are valueless unless their integrity can be verified to some degree at a later time, perhaps for a cost. To deal with this issue we employ an institution agent that always reports promptly and honestly on the execution of all commitments, forecasts, promises and obligations. The institution agent is a simple solution to the integrity verification issue as well as determination of ownership of data. This enables the agents to negotiate and to evaluate the execution of commitments by simple message passing.\nAn agent's percepts generally constitute a sparse data set whose elements have differing integrity. An agent may wish to induce tentative conclusions from this sparse and uncertain data of changing integrity. Percepts are transformed by inference rules into statements in probabilistic logic as described above. Information-based agents may employ entropy-based logic [6] to induce complete probability distributions from those statements. This logic is consistent with the laws of probability, but the results derived assume that the data is complete or, according to Watts Assumption, is 'all that there is to know'.\nAn agent acts in response to some need or needs. A need may be exogenous such as the agents 'owner' needs to buy some brandy for example, or a message from another agent offering to trade may trigger a latent need to trade profitably. A need may also be endogenous such as the agent deciding that its owner has more brandy than is required. An agent may be attempting to satisfy a number of needs at any time, simultaneously and may have expectations of its future needs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Agent Architecture", "text": "An agent observes events in its environment and represents some of those observations in its world model as beliefs. As time passes, an agent may not be prepared to accept such beliefs as being \"true\", and qualifies those representations with epistemic probabilities. Those qualified representations of prior observations are the agents information. Given this information, an agent may then choose to adopt goals and strategies. Those strategies may be based on game theory, for example. To enable the agent's strategies to make good use of its information, tools from information theory are applied to summarise and process that information. Such an agent is called information-based.\nAn agent called \u03a0 is the subject of this discussion. \u03a0 engages in multi-issue negotiation with a set of other agents: {\u03a9 1 , \u2022 \u2022 \u2022 , \u03a9 o }, and information providing agents {\u0398 1 , \u2022 \u2022 \u2022 , \u0398 k }. \u03a0 has two languages: C and L. C is an illocutionary-based language for communication. L is a first-order language for internal representation -precisely it is a first-order language with sentence probabilities optionally attached to each sentence representing \u03a0's epistemic belief in the truth of that sentence. Messages expressed in C from {\u0398 i } and {\u03a9 i } are received, time-stamped, source-stamped and placed in an in-box X . The messages in X are then translated using an import function I into sentences expressed in L that have integrity decay functions (usually of time) attached to each sentence, they are stored in a repository Y t . And that is all that happens until \u03a0 triggers a goal.\n\u03a0 triggers a goal, g \u2208 G, in two ways: first in response to a message received from an opponent {\u03a9 i } \"I offer you e100 in exchange for a pallet of paper\", and second in response to some need, \u03bd \u2208 N , \"we need to order some more paper\". In either case, \u03a0 is motivated by a need -either a need to strike a deal, or a general need to trade. \u03a0's goals could be short-term such as obtaining some information \"what is the euro / dollar exchange rate?\", medium-term such as striking a deal with one of its opponents, or, rather longer-term such as building a (business) relationship with one of its opponents. So \u03a0 has a trigger mechanism T where:\nT : {X \u222a N } \u2192 G.\nFor each goal that \u03a0 commits to, it has a mechanism, G, for selecting a strategy to achieve it where G : G \u00d7 M \u2192 S where S is the strategy library and M the world model. A strategy s maps an information base into an action, s(Y t ) = z \u2208 Z. Given a goal, g, and the current state of the social model m t , a strategy: s = G(g, m t ). Each strategy, s, consists of a plan, b s and a world model (construction and revision) function, J s , that constructs, and maintains the currency of, the strategy's world model W t s that consists of a set of probability ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u03a0's Reasoning", "text": "Once \u03a0 has selected a plan a \u2208 A it uses maximum entropy inference to derive the {D s i } n i=1 and minimum relative entropy inference to update those distributions as new data becomes available. Entropy, H, is a measure of uncertainty [7] in a probability distribution for a discrete random variable X:\nH(X) \u2212 i p(x i ) log p(x i )\nwhere p(x i ) = P(X = x i ). Maximum entropy inference is used to derive sentence probabilities for that which is not known by constructing the \"maximally noncommittal\" [6] probability distribution, and is chosen for its ability to generate complete distributions from sparse data.\nLet G be the set of all positive ground literals that can be constructed using \u03a0's language L. A possible world, v, is a valuation function: G \u2192 { , \u22a5}. V|K s = {v i } is the set of all possible worlds that are consistent with \u03a0's knowledge base K s that contains statements which \u03a0 believes are true. A random world for K s , W |K s = {p i } is a probability distribution over V|K s = {v i }, where p i expresses \u03a0's degree of belief that each of the possible worlds, v i , is the actual world. The derived sentence probability of any \u03c3 \u2208 L, with respect to a random world W |K s is:\n(\u2200\u03c3 \u2208 L)P {W |K s } (\u03c3) n { p n : \u03c3 is in v n } (1)\nThe agent's belief set B s t = {\u03a9 j } M j=1 contains statements to which \u03a0 attaches a given sentence probability B(.\n). A random world W |K s is consistent with B s t if: (\u2200\u03a9 \u2208 B s t )(B(\u03a9) = P {W |K s } (\u03a9)). Let {p i } = {W |K s , B s\nt } be the \"maximum entropy probability distribution over V|K s that is consistent with B s t \". Given an agent with K s and B s t , maximum entropy inference states that the derived sentence probability for any sentence, \u03c3 \u2208 L, is:\n(\u2200\u03c3 \u2208 L)P {W |K s ,B s t } (\u03c3) n { p n : \u03c3 is in v n } (2)\nFrom Eqn. 2, each belief imposes a linear constraint on the {p i }. The maximum entropy distribution: arg max p H(p), p = (p 1 , . . . , p N ), subject to M + 1 linear constraints: g j (p) = N i=1 c ji p i \u2212 B(\u03a9 j ) = 0, j = 1, . . . , M. g 0 (p) = N i=1 p i \u2212 1 = 0, where c ji = 1 if \u03a9 j is in v i and 0 otherwise, and p i \u2265 0, i = 1, . . . , N, is found by introducing Lagrange multipliers, and then obtaining a numerical solution using the multivariate Newton-Raphson method. In the subsequent subsections we'll see how an agent updates the sentence probabilities depending on the type of information used in the update.\nGiven a prior probability distribution q = (q i ) n i=1 and a set of constraints C, the principle of minimum relative entropy chooses the posterior probability distribution p = (p i ) n i=1 that has the least relative entropy 2 with respect to q:\n{W |q, C} arg min p n i=1 p i log p i q i\nand that satisfies the constraints. This may be found by introducing Lagrange multipliers as above. Given a prior distribution q over {v i } -the set of all possible worlds, and a set of constraints C (that could have been derived as above from a set of new beliefs) minimum relative entropy inference states that the derived sentence probability for any sentence, \u03c3 \u2208 L, is:\n(\u2200\u03c3 \u2208 L)P {W |q,C} (\u03c3) n { p n : \u03c3 is in v n } (3)\nwhere {p i } = {W |q, C}. The principle of minimum relative entropy is a generalisation of the principle of maximum entropy. If the prior distribution q is uniform, then the relative entropy of p with respect to q, p q, differs from \u2212H(p) only by a constant. So the principle of maximum entropy is equivalent to the principle of minimum relative entropy with a uniform prior distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Managing Dynamic Information Flows", "text": "The illocutions in the communication language C include information, [info].\nThe information received from general information sources will be expressed in terms defined by \u03a0's ontology. We define an ontology signature as a tuple S = (C, R, \u2264, \u03c3) where C is a finite set of concept symbols (including basic data types); R is a finite set of relation symbols; \u2264 is a reflexive, transitive and anti-symmetric relation on C (a partial order); and, \u03c3 : R \u2192 C + is the function assigning to each relation symbol its arity. Concepts play the role of type, and the is-a hierarchy is the notion of subtype. Thus, type inference mechanisms can be used to type all symbols appearing in expressions. We assume that \u03a0 makes at least part of that ontology public so that the other agents {\u03a9 1 , . . . , \u03a9 o } may communicate [info] that \u03a0 can understand. \u03a9's reliability is an estimate of the extent to which this [info] is correct. The only restriction on incoming [info] is that it is expressed in terms of the ontology -this is very general. However, the way in which [info] is used is completely specific -it will be represented as a set of linear constraints on one or more probability distributions in the world model. A chunk of [info] may not be directly related to one of \u03a0's chosen distributions or may not be expressed naturally as constraints, and so some inference machinery is required to derive these constraints -this inference is performed by model building functions, J s , that have been activated by a plan s chosen by \u03a0. J D s ([info]) denotes the set of constraints on distribution D derived by J s from [info].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Updating the World Model with [info]", "text": "The procedure for updating the world model as [info] is received follows. If at time u, \u03a0 receives a message containing [info] it is time-stamped and sourcestamped [info] (\u03a9,\u03a0,u) , and placed in a repository Y t . If \u03a0 has an active plan, s, with model building function, J s , then J s is applied to [info] (\u03a9,\u03a0,u) to derive constraints on some, or none, of \u03a0's distributions. The extent to which those constraints are permitted to effect the distributions is determined by a value for the reliability of\n\u03a9, R t (\u03a0, \u03a9, O([info])), where O([info]) is the ontological context of [info].\nAn agent may have models of integrity decay for some particular distributions, but general models of integrity decay for, say, a chunk of information taken at random from the World Wide Web are generally unknown. However the values to which decaying integrity should tend in time are often known. For example, a prior value for the truth of the proposition that a \"22 year-old male will default on credit card repayment\" is well known to banks. If \u03a0 attaches such prior values to a distribution D they are called the decay limit distribution for D, (d D i ) n i=1 . No matter how integrity of [info] decays, in the absence of any other relevant information it should decay to the decay limit distribution. If a distribution with n values has no decay limit distribution then integrity decays to the maximum entropy value 1 n . In other words, the maximum entropy distribution is the default decay limit distribution.\nIn the absence of new [info] the integrity of distributions decays.\nIf D = (q i ) n i=1\nthen we use a geometric model of decay:\nq t+1 i = (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 q t i , for i = 1, . . . , n(4)\nwhere \u03c1 D \u2208 (0, 1) is the decay rate. This raises the question of how to determine \u03c1 D . Just as an agent may know the decay limit distribution it may also know something about \u03c1 D . In the case of an information-overfed agent there is no harm in conservatively setting \u03c1 D \"a bit on the low side\" as the continually arriving [info] will sustain the estimate for D.\nWe now describe how new [info] is imported to the distributions. A single chunk of [info] may effect a number of distributions. Suppose that a chunk of [info] is received from \u03a9 and that \u03a0 attaches the epistemic belief probability R t (\u03a0, \u03a9, O([info])) to it. Each distribution models a facet of the world. Given a distribution D t = (q t i ) n i=1 , q t i is the probability that the possible world \u03c9 i for D is the true world for D. The effect that a chunk [info] has on distribution D is to enforce the set of linear constraints on D, J D s ([info]). If the constraints J D s ([info]) are taken by \u03a0 as valid then \u03a0 could update D to the posterior distribution (p\n[info] i\n) n i=1 that is the distribution with least relative entropy with respect to (q t i ) n i=1 satisfying the constraint:\ni {p [info] i : J D s ([info]) are all in \u03c9 i } = 1. (5\n)\nBut R t (\u03a0, \u03a9, O([info])) = r \u2208 [0, 1] and \u03a0 should only treat the J D s ([info]\n) as valid if r = 1. In general r determines the extent to which the effect of [info] on D is closer to (p\n[info] i\n) n i=1 or to the prior (q t i ) n i=1 distribution by:\np t i = r \u00d7 p [info] i + (1 \u2212 r) \u00d7 q t i (6)\nBut, we should only permit a new chunk of [info] to influence D if doing so gives us new information. For example, if 5 minutes ago a trusted agent advises \u03a0 that the interest rate will go up by 1%, and 1 minute ago a very unreliable agent advises \u03a0 that the interest rate may go up by 0.5%, then the second unreliable chunk should not be permitted to 'overwrite' the first. We capture this by only permitting a new chunk of [info] to be imported if the resulting distribution has more information relative to the decay limit distribution than the existing distribution has. Precisely, this is measured using the Kullback-Leibler distance measure -this is just one criterion for determining whether the [info] should be used -and [info] is only used if:\nn i=1 p t i log p t i d D i > n i=1 q t i log q t i d D i (7)\nIn addition, we have described in Eqn. 4 how the integrity of each distribution D will decay in time. Combining these two into one result, distribution D is revised to:\nq t+1 i = (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 p t i if [info] is usable (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 q t i otherwise for i = 1, \u2022 \u2022 \u2022 , n\n, and decay rate \u03c1 D as before.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Information Reliability", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sec. 3.1 relies on an estimate of R t (\u03a0, \u03a9, O([info])", "text": "). This estimate is constructed by measuring the 'error' in observed information as the error in the effect that information has on each of \u03a0's distributions. Suppose that a chunk of [info] is received from agent \u03a9 at time s and is verified at some later time t. For example, a chunk of information could be \"the interest rate will rise by 0.5% next week\", and suppose that the interest rate actually rises by 0.25% -call that correct information [fact]. What does all this tell agent \u03a0 about agent \u03a9's reliability?\nConsider one of \u03a0's distributions D that is {q s i } at time s. Let (p [info] i\n) n i=1 be the minimum relative entropy distribution given that [info] has been received as calculated in Eqn. 5, and let (p\n[fact] i\n) n i=1 be that distribution if [fact] had been received instead. Suppose that the reliability estimate for distribution D was R s D . This section is concerned with what R s D should have been in the light of knowing now, at time t, that [info] should have been [fact], and how that knowledge effects our current reliability estimate for D, R t (\u03a0, \u03a9, O([info])).\nThe idea of Eqn. 6, is that the current value of r should be such that, on average, (p s i ) n i=1 will be seen to be \"close to\" (p\n[fact] i\n) n i=1 when we eventually discover [fact] -no matter whether or not [info] was used to update D, as determined by the acceptability test in Eqn. 7 at time s. That is, given [info], [fact] and the prior (q s i ) n i=1 , calculate (p\n[info] i\n) n i=1 and (p\n[fact] i\n) n i=1 using Eqn. 5. Then the observed reliability for distribution D, R\n([info]|[fact]) D\n, on the basis of the verification of [info] with [fact] is the value of r that minimises the Kullback-Leibler distance between (p s i ) n i=1 and (p\n[fact] i ) n i=1 : arg min r n i=1 (r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i ) log r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i p [fact] i\nIf E [info] is the set of distributions that [info] affect, then the overall observed reliability on the basis of the verification of [info] with [fact] is: \nR ([info]|[fact]) = 1 \u2212 (max D\u2208E [info] |1 \u2212 R ([info]|[fact])D\nR t+1 (\u03a0, \u03a9, o j ) = (1 \u2212 \u03c1) \u00d7 R t (\u03a0, \u03a9, o j ) + \u03c1 \u00d7 R ([info]|[fact]) \u00d7 Sem(o j , o k )\nwhere Sem(\u2022, [8] between two sections of the ontology, and \u03c1 is the learning rate. Over time, \u03a0 notes the ontological context of the various chunks of [info] received from \u03a9 and over the various ontological contexts calculates the relative frequency, P t (o j ), of these contexts, o j = O ([info]). This leads to a overall expectation of the reliability that agent \u03a0 has for agent \u03a9: R t (\u03a0, \u03a9) = j P t (o j ) \u00d7 R t (\u03a0, \u03a9, o j ).\n\u2022) : O \u00d7 O \u2192 [0, 1] measures the semantic distance", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Negotiation", "text": "For illustration \u03a0's communication language [9] is restricted to the illocutions: Offer(\u2022), Accept(\u2022), Reject(\u2022) and Withdraw(\u2022). The simple strategies that we will describe all use the same world model function, J s , that maintains the following two probability distributions as their world model: -P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) -the strength of belief that \u03a0 has in the proposition that she should accept the proposal \u03b4 = (a, b) from agent \u03a9 in satisfaction of need \u03bd at time t, where a is \u03a0's commitment and b is \u03a9's commitment. P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) is estimated from:\n1. P t (Satisfy(\u03a0, \u03a9, \u03bd, \u03b4)) a subjective evaluation (the strength of belief that \u03a0 has in the proposition that the expected outcome of accepting the proposal will satisfy some of her needs). 2. P t (Fair(\u03b4)) an objective evaluation (the strength of belief that \u03a0 has in the proposition that the proposal is a \"fair deal\" in the open market. 3. P t (\u03a0CanDo(a) an estimate of whether \u03a0 will be able to meet her commitment a at contract execution time.\nThese three arrays of probabilities are estimated by importing relevant information, [info], as described in Sec. 3.\n-P t (\u03a9Acc(\u03b2, \u03b1, \u03b4)) -the strength of belief that \u03a0 has in the proposition that \u03a9 would accept the proposal \u03b4 from agent \u03a0 at time t. Every time that \u03a9 submits a proposal she is revealing information about what she is prepared to accept, and every time she rejects a proposal she is revealing information about what she is not prepared to accept. Eg: having received the stamped illocution Offer(\u03a9, \u03a0, \u03b4) (\u03a9,\u03a0,u) , at time t > u, \u03a0 may believe that P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) = \u03ba this is used as a constraint on P t+1 (\u03a9Acc(\u2022)) which is calculated using Eqn. 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Negotiation Strategies", "text": "An agent's strategy s is a function of the information Y t that it has at time t. Four simple strategies make offers only on the basis of P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)), \u03a0's acceptability threshold \u03b3, and P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)). The greedy strategy s + chooses:\narg max \u03b4 {P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) | P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) 0}\nit is appropriate when \u03a0 believes \u03a9 is desperate to trade.\nThe expected-acceptability-to-\u03a0-optimizing strategy s * chooses:\narg max \u03b4 {P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) \u00d7 P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) | P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) \u2265 \u03b3}\nwhen \u03a0 is confident and not desperate to trade. The strategy s \u2212 chooses:\narg max \u03b4 {P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) | P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) \u2265 \u03b3}\nit optimises the likelihood of trade -when \u03a0 is keen to trade without compromising its own standards of acceptability. An approach to issue-tradeoffs is described in [10]. The bargaining strategy described there attempts to make an acceptable offer by \"walking round\" the iso-curve of \u03a0's previous offer \u03b4 (that has, say, an acceptability of \u03b3 \u03b4 \u2265 \u03b3) towards \u03a9's subsequent counter offer. In terms of the machinery described here, an analogue is to use the strategy s \u2212 : arg max \u03b4 {P t (\u03a9Acc(\u03a9, \u03a0, \u03b4))|P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) \u2265 \u03b3 \u03b4 } with \u03b3 = \u03b3 \u03b4 . This is reasonable for an agent that is attempting to be accommodating without compromising its own interests. The complexity of the strategy in [10] is linear with the number of issues. The strategy described here does not have that property, but it benefits from using P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) that contains foot prints of the prior offer sequence -estimated by repeated use of Eqn. 3in that distribution more recent data gives estimates with greater certainty.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "This paper has described a new breed of \"information-based\" agents are founded on concepts from information theory We believe that to automate trading we must build intelligent agents that are 'informed', that can proactively acquire information to reduce uncertainty, that can estimate the integrity of real-time information flows, and can use uncertain information as a foundation for strategic decision-making [2]. An 'information-based' agent architecture has been described, that is founded on ideas from information theory, and has been developed specifically for this purpose.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction and Related Work", "text": "There is a consensus among researchers that the generation of the goals to be adopted by an agent depends on its mental state [4,5,7]. It can be the result of an explicit external request that is accepted by the agent, e.g., [14]; or a consequence of the agent's mental attitudes, e.g., [5]. Instead, the choice of the best set of goals to be adopted (pursued) depends also on the consistency (or feasibility) of such goals.\nThere are two directions followed by researchers to define desire/goal consistency. One considers goal generation and adoption as a whole, the other considers them as two separate steps. In the former case, the evaluation of the consistency of a desire set takes the cognitive components into account, but can lead to a consistent but sub-optimal goal set. In the latter case, the evaluation of consistency does not take the cognitive components of the agent into account (logical consistency). This can lead the agent to choose sets of desires which are logically consistent but inconsistent from the cognitive point of view.\nWe propose a possibilistic approach in which the generation and the adoption parts are considered separately and propose a new and possibilistic-based definition of desire/goal consistency which incorporates the two points of view. Using a possibilistic framework to represent beliefs and desires allows us to also represent partially sure beliefs and partiallly desirable world states.\nTo make justice to the complexity of real world, the agent's beliefs are represented by a possibility distribution and we adapt the belief conditioning operator proposed by Dubois and Prade [9] to update the beliefs.\nA consequence of representing beliefs as a matter of degree is that desires also have to be considered as such. Like beliefs, desires are represented by a possibility distribution that induces a complete preorder over the set of possible worlds. However, for the sake of simplicity, we make the assumption that an agent only generates positive desires [2].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Possibilistic Representation", "text": "The representation of beliefs and desires calls for a quick recall of possibility theory.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Possibility Theory", "text": "Possibility theory is a mathematical theory of uncertainty that relies upon fuzzy set theory [15], in that the (fuzzy) set of possible values for a variable of interest is used to describe the uncertainty as to its precise value. The membership function of such set, \u03c0, is called a possibility distribution.\nA possibility distribution for which there exists a completely possible value (\u2203v 0 ; \u03c0(v 0 ) = 1) is said to be normalized.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 1 (Possibility and Necessity Measures).", "text": "A possibility distribution \u03c0 induces a possibility measure and its dual necessity measure, denoted by \u03a0 and N respectively. Both measures apply to a crisp set A and are defined as follows:\n\u03a0(A) = max s\u2208A \u03c0(s);\n(1)\nN (A) = 1 \u2212 \u03a0(\u0100) = min s\u2208\u0100 {1 \u2212 \u03c0(s)}. (2\n)\nAnother interesting measure that can be defined based on a possibility distribution is guaranteed possibility [11].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 2 (Guaranteed Possibility Measure)", "text": "Given a possibility distribution \u03c0, a guaranteed possibility measure, noted \u0394, is defined as:\n\u0394(A) = min s\u2208A \u03c0(s); (3\n)\nA few properties of possibility, necessity, and guaranted possibility measures induced by a normalized possibility distribution on a finite universe of discourse \u03a9 are the following. For all subsets A, B \u2286 \u03a9:\n1. \u03a0(A \u222a B) = max{\u03a0(A), \u03a0(B)}, N (A \u2229 B) = min{N (A), N(B)}; 2. \u03a0(\u2205) = N (\u2205) = 0, \u03a0(\u03a9) = N (\u03a9) = 1, \u03a0(A) = 1 \u2212 N (\u0100); 3. N (A) \u2264 \u03a0(A), \u0394(A) \u2264 \u03a0(A); 4. N (A) > 0 implies \u03a0(A) = 1, \u03a0(A) < 1 implies N (A) = 0;\nA consequence of these properties is that max{\u03a0(A), \u03a0(\u0100)} = 1. In case of complete ignorance on A, \u03a0(A) = \u03a0(\u0100) = 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Language and Interpretations", "text": "Information manipulated by a cognitive agent must be represented symbolically. To develop our theoretical framework, we adopt perhaps the simplest symbolic representation, in the form of a classical propositional language.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 3 (Language).", "text": "Let A be a finite 1 set of atomic propositions and let L be the propositional language such that A\u222a{ , \u22a5} \u2286 L, and, \u2200\u03c6, \u03c8 \u2208 L, \u00ac\u03c6 \u2208 L, \u03c6\u2227\u03c8 \u2208 L, \u03c6 \u2228 \u03c8 \u2208 L.\nWe will denote by \u03a9 = {0, 1} A the set of all interpretations on A. An interpretation I \u2208 \u03a9 is a function I : A \u2192 {0, 1} assigning a truth value p I to every atomic proposition p \u2208 A and, by extension, a truth value \u03c6 I to all formulas \u03c6 \u2208 L. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Representing Beliefs and Desires", "text": "The beliefs and desires of a cognitive agent are represented thanks to two different possibility distributions \u03c0 and u respectively. Thus, a normalized possibility distribution \u03c0 means that there exists at least one possible situation which is consistent with the available knowledge. All these considerations are in line with what is proposed in [2,10] with the following differences: (i) the qualitative utilities associated to positive desires are the result of a deliberative process, that is, they depend on the mental state of the agent; (ii) desires may be inconsistent and a way to calculate the degree of (logical and cognitive) consistency of the agent's desires is proposed; (iii) there is an explicit distinction between goals and desires; (iv) for the sake of simplicity we do not consider negative desires.\nRepresenting Desires. The desires of an agent depend on its beliefs. While desires are represented by means of a possibility distribution, one must understand that such a distribution is just an epiphenomenon of an underlying, more primitive mechanism which determines how desires arise. A description of such mechanism is given in terms of desire-generation rules.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 5 (Desire-Generation Rule). A desire-generation rule R is an expression of the form", "text": "\u03b2 R , \u03c8 R \u21d2 + D \u03c6, where \u03b2 R , \u03c8 R , \u03c6 \u2208 L. The unconditional counterpart of this rule is \u03b1 \u21d2 + D \u03c6, with \u03b1 \u2208 (0, 1].\nThe intended meaning of a conditional desire-generation rule is: \"an agent desires every world in which \u03c6 is true at least as much as it believes \u03b2 R and desires \u03c8 R \", or, put in terms of qualitative utility, \"the qualitative utility attached by the agent to every world satisfying \u03c6 is greater than, or equal to, the degree to which it believes \u03b2 R and desires \u03c8 R \". The intended meaning of an unconditional rule is that the qualitative utility of every world I |= \u03c6 is at least \u03b1 for the agent. Given a desire-generation rule R, we shall denote rhs(R) the formula on the righthand side of R.\nRepresenting Graded Beliefs. A belief, which is a component of an agent's cognitive state, can be regarded as a necessity degree induced by a normalized possibility distribution \u03c0 : \u03a9 \u2192 [0, 1] on the possible worlds. The possibility degree \u03c0(I) represents the plausibility order of the possible world situation represented by interpretation I.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 6 (Graded Belief).", "text": "Let N be the necessity measure induced by \u03c0, and \u03c6 be a formula. The degree to which the agent believes \u03c6 is given by:\nB(\u03c6) = N ([\u03c6]) = 1 \u2212 max I |=\u03c6 {\u03c0(I)}. (4\n)\nStraightforward consequences of the properties of possibility and necessity measures are that B(\u03c6) > 0 \u21d2 B(\u00ac\u03c6) = 0, this means that if the agent somehow believes \u03c6 then it cannot believe \u00ac\u03c6 at all; B( ) = 1, B(\u22a5) = 0, and\nB(\u03c6 \u2227 \u03c8) = min{B(\u03c6), B(\u03c8)}, B(\u03c6 \u2228 \u03c8) \u2265 max{B(\u03c6), B(\u03c8)}. (5\n)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mental State", "text": "We now have the elements to define the mental state of an agent, which consists of its beliefs and the rules defining the deliberation mechanism whereby desires are generated based on beliefs. Dr. Gent knows that, if the paper is accepted, publishing it (which is his great desire), means to pay the conference registration (for his co-author or for himself) and then be ready to go to C\u00f3rdoba to present it, in case I. M. is unavailable. If the paper is accepted (a), Dr. Gent is willing to pay the registration (r); furthermore, if the paper is accepted and Dr. Flaky turns out to be unavailable (q), he is willing to go to C\u00f3rdoba to present it (p). Finally, if he knows the paper is accepted and wishes to present it, he will desire to have a hotel room (h) and a plane ticket reserved (t).\nDr. Gent has some a priori beliefs about this situation, namely that if the hotels are all booked out (b), he will not succeed in booking a hotel room; similarly, he believes that if the planes are full (f ), he will not succeed in reserving a flight, although this is not necessarily true, if he puts himself in the waiting list and a reservation is cancelled. Finally, he believes the organizers will enforce the rule whereby his paper will be presented only if it is accepted and a registration is paid.\nThe set of atomic propositions is then A = {a, b, f, h, p, r, t, q} and\nR J = \u23a7 \u23a8 \u23a9 R 1 : a, p \u21d2 + D t \u2227 h, R 2 : a \u2227 q, \u21d2 + D p, R 3 : a, \u21d2 + D r. \u23ab \u23ac \u23ad .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Beliefs", "text": "The belief change operator used here is an adaptation of Dubois and Prade's belief conditioning operator [9] and allows to update the possibility distribution \u03c0 in light of new trusted information.\nA source of information is considered trusted to a certain extent. This means that its membership degree to the fuzzy set of trusted sources is a value \u03c4 \u2208 [0, 1]. Let \u03c6 \u2208 L be incoming information from a source trusted to degree \u03c4 . The belief change operator is defined as follows:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 8 (Belief Change Operator). The possibility distribution \u03c0 which induces the new belief set B after receiving information \u03c6 is computed from possibility distribution \u03c0 relevant to the previous belief set B (B = B * \u03c4", "text": "\u03c6 , \u03c0 = \u03c0 * \u03c4 \u03c6 ) as follows: for all interpretation I,\n\u03c0 (I) = \u23a7 \u23a8 \u23a9 \u03c0(I) \u03a0([\u03c6]) , if I |= \u03c6 and B(\u00ac\u03c6) < 1; 1, if I |= \u03c6 and B(\u00ac\u03c6) = 1; min{\u03c0(I), (1 \u2212 \u03c4 )}, if I |= \u03c6. (6)\nThe second case in Equation 6 provides for the revision of beliefs that contradict \u03c6. In general, the operator treats new information \u03c6 in the negative sense: being told \u03c6 denies the possibility of world situations where \u03c6 is false (third case of Equation 6). The possibility of world situations where \u03c6 is true may only increase due to the first case in equation 6 or revision (second case of Equation 6). If information from a fully trusted source contradicts an existing proposition that is fully believed, then revising with the above operator leads the agent to believe the more recent information and give up the oldest to restore consistency. Finally, it can be shown that the belief change operator * obeys a possibilistic version of the AGM revision rationality postulates [12]. It is easy to verify that the * operator is a generalization of the possibilistic conditioning operator of Dubois and colleagues [9]. Example (continued). Dr. Gent's a priori beliefs may be modeled by assuming Dr. Gent at some point had no beliefs at all (\u03c0 is 1 everywhere), and then was \"told\":\n-f \u2283 \u00ach with certainty 1 (i.e., f \u2283 \u00ach by a fully trusted source), -b \u2283 \u00act with certainty 0.9 (i.e., b \u2283 \u00act by a source with trust \u03c4 = 0.9), -\u00ac(r \u2227 a) \u2283 \u00acp with certainty 1, which yields the possibility distribution shown in Figure 1.\nAt this point, the following happens:\n-1/a: Dr. Gent receives the notification of acceptance of his paper: the source is the program chair of IEAAIE, whom Dr. Gent trusts in full; -0.75/q: soon after learning that the paper has been accepted, Dr. Flaky rushes into Dr. Gent's office to inform him that he is no more available to go to C\u00f3rdoba; as always, Dr. Gent does not completely trust what Dr. Flaky tells him, as he is wellknown for changing his mind very often; Interpretations have been grouped together where possible, due to lack of space: when no literal appears for a given atom in a row or column heading, it is understood that the row or column applies for both truth assignments.\n\u00acp \u00acp p p p \u00acr r r \u00act t \u00act t \u00aca, \u00acb, \u00acf 1 1 0 0 0 \u00aca, \u00acb, f, \u00ach 1 1 0 0 0 \u00aca, b, \u00acf 1 0.1 0 0 0 \u00aca, b, f, \u00ach 1 0.1 0 0 0 a, \u00acb, \u00acf 1 1 0 1 1 a, \u00acb, f, \u00ach 1 1 0 1 1 a, b, \u00acf 1 0.1 0 1 0.1 a, b, f, \u00ach 1 0.1 0 1 0.1 f,\n-0.2/f : a few weeks later, Dr. Gent meets a colleague who tells him he has heard another colleague say someone on IEAAIE's organizing committee told her all the hotel rooms in C\u00f3rdoba are already booked out; Dr. Gent considers this news as yet unverified; nevertheless, he takes notice of it.\nDr. Gent's beliefs are represented by the \"final\" possibility distribution shown in Figure 1.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Desires", "text": "We suppose that the agent's subjective qualitative utilities are determined dynamically through a rule-based deliberation mechanism. Associating a qualitative utility first to worlds and not to formulas allows us to (i) directly construct the possibility distribution u; and (ii) makes it possible to also calculate the qualitative degree of formulas which do not appear explicitly on the right-hand side of any rule.\nLike in [3], the qualitative utility associated to each positive desire formula is computed on the basis of the guaranted possibility measure \u0394.\nThe set of the agent's justified positive desires, J , is induced by the assignment of a qualitative utility u, which, unlike \u03c0, needs not be normalized, since desires may very well be inconsistent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 9 (Justified Desire). Given a qualitative utility assignment u (formally a possibility distribution), the degree to which the agent desires \u03c6 \u2208 L is given by", "text": "J (\u03c6) = \u0394([\u03c6]) = min I|=\u03c6 u(I). (7\n)\nIn words, the degree of justification of a desire is given by the guaranteed qualitative utility of the set of all worlds in which the desire would be fulfilled. Intuitively, a desire is justified to the extent that all the worlds in which it is fulfilled are desirable.\nInterpreting J (\u03c6) as a degree of membership defines the fuzzy set J of the agent's justified positive desires.\nIn turn, a qualitative utility assignment u is univocally determined by the mental state of the agent as explained below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 10 (Rule Activation). Let R = \u03b2 R , \u03c8 R \u21d2 +", "text": "D \u03c6 be a desire-generation rule. The degree af activation of R, Deg(R), is given by (formally a possibility distribution) is defined, for all I \u2208 \u03a9, as\nDeg(R) = min{B(\u03b2 R ), J (\u03c8 R )}.\nu(I) = max R\u2208R I J Deg(R). (8\n)\nThe apparent circularity of Definitions 9, 10, and 11 is resolved by an algorithmic translation which reveals u is the limit distribution obtained by iteratively applying the definitions. Given a mental state S = \u03c0, R J , the corresponding qualitative utility assignment, u, is computed by the following algorithm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 (Deliberation)", "text": "1. i \u2190 0; for all I \u2208 \u03a9, u 0 (I) \u2190 0;\n2. i \u2190 i + 1; 3. For all I \u2208 \u03a9, u i (I) \u2190 max R\u2208R I J Deg i\u22121 (R), if R I J = \u2205, 0, otherwise,\nwhere Deg i\u22121 (R) is the degree of activation of rule R calculated using u i\u22121 as the qualitative utility assignment; 4. if max I |u i (I) \u2212 u i\u22121 (I)| > 0, i.e., if a fixpoint has not been reached yet, go back to Step 2; 5. For all I \u2208 \u03a9, u(I) \u2190 u i (I); u is the qualitative utility assignment corrisponding to mental state S.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proposition 1. Algorithm 1 always terminates.", "text": "Example (continued). Dr. Gent's desires may now be determined, based on the desiregeneration rules R 1 , R 2 , and R 3 , and Dr. Gent's beliefs, represented by the possibility distribution shown in Figure 1, by applying Algorithm 1, which stops at iteration i = 3 with the qualitative utility distribution shown in Figure 2.\nAs expected, J (r) = 1 and J (t \u2227 h) = J (p) = 0.75, but J (t) = J (h) = 0. However, these are not all of Dr. Gent's desires. Other desires are justified under these conditions, for instance J (\u00aca \u2227 p) = 0.75 and J (\u00aca \u2227 r) = 1, and even J (\u00acr \u2227 p) = 0, 75 and J (r \u2227 \u00acp) = 1. ", "publication_ref": [], "figure_ref": ["fig_2", "fig_15"], "table_ref": []}, {"heading": "Goals", "text": "Here, we make a clear distinction between desires and goals. Desires may be inconsistent. Goals, instead, are defined as a consistent subset of desires.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 12.", "text": "The overall possibility of a set S \u2286 L of formulas is\n\u03a0([S]) = max I\u2208[S] \u03c0(I). (9\n)\nThe following definition extends J , the degree of justification of a desire, to sets of desires.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 13. The overall justification of a set S \u2286 L of formulas is", "text": "J (S) = \u0394([S]) = min I\u2208[S] u(I). (10\n)\nTherefore, by the properties of the minumum guaranteed possibility:\nProposition 2.\nThe justified degree of a set of desires is greater than or equal to all the justisfied degree of each desire in the considered set, formally:\nJ (S)) \u2265 max \u03c6\u2208S {J (\u03c6)}.(11)\nProposition 3. The addition of a desire to a set of desires cannot lead to a decrease of the justification level of the resulting enlarged set of desires. Let S \u2286 L be a set of desires. For all desire \u03c6,\nJ (S \u222a {\u03c6}) \u2265 J (S); (12) J (S) \u2265 J (S \\ {\u03c6}). (13\n)\nA rational agent will select as goals the set of desires that, besides being logically \"consistent\", is also maximally desirable, i.e., maximally justified. The problem with logical \"consistency\", however, is that it does not capture \"implicit\" inconsistencies among desires, that is consistency due to the agent beliefs (I adopt as goals only desires which are not inconsistent with my beliefs). Therefore, a suitable definition of desire consistency in the possibilistic setting is required. Such definition must take the agent's cognitive state into account as pointed out, for example, in [6,13,1].\nFor example, an agent desires p and desires q, believing that p \u2283 \u00acq. Although {p, q}, as a set of formulas, i.e., syntactically, is logically consistent, it is not if one take the belief p \u2283 \u00acq into account.\nWe argue that a suitable definition of such \"cognitive\" consistency is one based on the possibility of the set of desires, as defined above. Indeed, Definition 14. a set of desires S is consistent, in the cognitive sense, if and only if \u03a0([S]) > 0.\nOf course, this definition of cognitive consistency implies logical consistency: if S is logically inconsistent, \u03a0([S]) = 0. We will take a step forward, by assuming a rational agent will select as goals the most desirable set of desires among the most possible such sets.\nLet D = {S \u2286 supp(J ) 2 }, i.e., the set of desire sets whose justification is greater than zero. Given \u03b3 \u2208 (0, 1], D \u03b3 = {S \u2208 D : \u03a0([S]) \u2265 \u03b3} is the subset of D containing only those sets whose overall possibility is at least \u03b3.\nFor every given level of possibility \u03b3, a rational agent will elect as its goal set the maximally desirable of the \u03b3-possible sets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 15 (Goal set). The \u03b3-possible goal set is", "text": "G \u03b3 = arg max S\u2208D\u03b3 J (S) if D \u03b3 = \u2205, \u2205 otherwise.\nWe denote by \u03b3 * the maximum possibility level such that G \u03b3 = \u2205. Then, the goal set elected by a rational agent will be\nG * = G \u03b3 * , \u03b3 * = max G\u03b3 =\u2205 \u03b3. (14\n)\nProposition 4. A rational agent chooses a goal set that is maximally possible (with a non zero possibility anyway) and maximally justifiable.\nExample (continued). To determine a consistent set of goals to commit to, Dr. Gent must perform a goal election, which, in this case, yields \u03b3 * = 1 and G * = {r}: Dr. Gent must pay the registration, that is for sure; planning his trip is less urgent, for Dr. Flaky, as far as Dr. Gent believes, might still change his mind.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "A theoretical framework for goal generation in BDI agent has been justified and developed. Beliefs and desires are represented by means of two possibility distributions. A deliberative process is responsible for generating the distribution of qualitative utility that underlies desire justification, and the election of goals considers their cognitive consistency, realized as possibility. Due to lack of space, all the proofs have been omitted.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Traditionally, macroeconomics addresses the behaviour of a world-wide, national or regional economy as a whole [3], whereas microeconomics investigates the economic behaviour and decision making of individual agents, for example, consumers, households or firms [11]. Since the latter aims to understand why and how agents make certain economic decisions, various social, cognitive, and emotional factors of human behaviour are studied. This has resulted in the emergence of the field of behavioural economics [14]. Although this may be very useful when one wants to analyse the behaviour of individual agents, there is some debate about the extent to which it is useful to incorporate these aspects when studying global processes in economics, e.g., [5]. Do personal factors such as risk avoidance, greed, and personal circumstances provide more insight in the global patterns, or can they simply be ignored or treated in a more abstract, aggregated manner? This paper provides some answers to these questions from a computational perspective.\nIn recent years, various authors have studied processes in economics by building computational models of them, and analysing the dynamics of these models using agent-based simulation techniques [15]. Ironically, also in the area of agent-based modelling, a debate exists about the pros and cons of two perspectives, namely agent-based and population-based modelling. Agent-based models are often assumed to produce more detailed, faithful behaviour, whereas population-based models abstract from such details to focus on global patterns (e.g., [2], [7], and [9]).\nGiven these similarities between the debate between macro-and microeconomics on the one hand, and the debate between population-based and agent-based modelling on the other hand, it makes sense to align the two debates. Hence, the goal of the current paper is to explore the differences and commonalities between populationbased and agent-based modelling in an economical context. This will be done via a case study on the interplay between individual greed and the global economy.\nThis paper is structured as follows. In Section 2, the existing debate between agentbased and population-based modelling is briefly explained. In Section 3, both an agent-based and a population-based model are introduced for the example domain. In Section 4, a number of simulation results of both models are shown, and the similarities are discussed. Next, Section 5 provides a mathematical analysis on the models. Section 6 concludes the paper with a discussion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Agent-Based versus Population-Based Modelling", "text": "The classical approaches to simulation of processes in which larger groups of agents are involved are population-based: a number of groups are distinguished (populations) and each of these populations is represented by a numerical variable indicating their number or density (within a given area) at a certain time point. The simulation model takes the form of a system of difference or differential equations expressing temporal relationships for the dynamics of these variables. Well-known classical examples of such population-based models address ecological processes, for example, predatorprey dynamics (e.g., [6], [12], [13] and [16]), and the dynamics of epidemics (e.g., [1], [6], and [8]). Such models can be studied by simulation and by using analysis techniques from mathematics and dynamical systems theory.\nFrom the more recently developed agent system area it is often taken as a presupposition that simulations based on individual agents are a more natural or faithful way of modelling, and thus will provide better results (e.g., [2] and [7]). Although for larger numbers of agents such agent-based approaches are more expensive computationally than population-based approaches, such a presupposition may provide a justification of preferring their use over population-based approaches, in spite of the computational disadvantages. In other words, they are justified because the results are expected to deviate from the results of population-based simulation, and are considered more realistic. However, in contrast there is another silent assumption sometimes made, namely that for larger numbers of agents (in the limit), agent-based simulations approximate population-based simulations. This would indicate that for larger numbers of agents agent-based simulation just can be replaced by population-based simulation, which would weaken the justification for agent-based simulation discussed above. In, e.g., ( [4; 9]), these considerations are explored for the domains of epidemics and crime displacement, respectively. The results put forward in these papers reveal several commonalities between both types of simulation, but also some differences. For example, for some specific parameter settings (concerning population size and rationality of the individual agents, among others), the results of population-based simulation seem to approximate those of agent-based simulation, whereas for other situations some differences can be observed. Furthermore, as could be expected, the computation time of the populations-based simulations is shown to be much lower than that of the agent-based simulation.\nIn the next sections, similar issues are explored, but this time for a domain within economics. Comparative simulation experiments have been conducted based on different simulation models, both agent-based and population-based.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Agent-Based and Population-Based Simulation Model", "text": "In this section, the two simulation models are introduced. First, an agent-based perspective is taken. The main idea behind this model is that the state of the global (world) economy influences the level of greed of the individual agents in the population, which is supposed to relate to the risk level of their investment decisions: in case the economic situation is positive, then people are tempted to take more risk. Moreover, the investment decisions of the individual agents in turn influence the global economy: in case agents become too greedy [10], this is assumed to have a negative impact on the economic situation, for example, due to higher numbers of bankruptcy. In addition, the state of the economy is assumed to be influenced by technological development which is driven by innovation. Inspired by these ideas, the interplay between agents' greed and the global economy is modelled as a dynamical system, in a way that has some similarity to predator-prey models in two variations: agent-based, where each agent has its own greed level, and population-based, where only an average greed level of the whole population is considered.\nThe agent-based model assumes n heterogeneous agents, which all interact within a certain economy. For each agent k, the individual greed is represented using a variable y k , and the global economic situation is represented using a variable x. The complete set of variables and parameters used in the model is shown in Table 1.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Table 1. Variables and parameters used in the agent-based model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variables", "text": "x World economy y (1) , ..., y (n)  Based on these concepts, a system of difference equations was designed that consists of n+3 formulae; here (2) specifies a collection of n equations for each of the n agents, where each agent has its individual values for y (k) , c k and e k :\n( The population-based dynamical model is similar to the agent-based model, but the difference is that it abstracts from the differences of the individual agents. This is done by replacing the average greed z over all y (k) in formula ( 1) by one single variable y indicating the greed of the population as a whole, and using a single formula (2), which is only applied at the population level, in contrast to the collection of formulae (2) in the agent-based model, which are applied for all agents separately. The resulting population-based model is shown in Table 2 and in the formulae below. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "TD new = TD old + inn* TD old *\u2206t", "text": "Note that in differential equation format the agent-based and population-based dynamical model can be expressed by n+2, respectively 3 differential equations as shown in Table 3. Moreover, as the innovation rate inn is assumed constant over time, for both cases the differential equation for TD can be solved analytically with solution TD(t) = TD(0) e inn t .", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_20"]}, {"heading": "Table 3. The two models expressed by n+2, respectively 3 differential equations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Agent-based model", "text": "Population-based model dx/dt = ax -bxz d y (k) /dt = (ckb xy (k) (2-y (k) ) / TD) -ek y (k) dTD/dt = inn TD z = ( \u03a3k y (k) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": ")/n dx/dt = ax -bxy dy/dt =( cb xy(2-y) / TD ) -ey dTD/dt = inn TD 4 Simulation Results", "text": "Based on the model introduced above, a number of simulation experiments have been performed under different parameter settings (with population size varying from 2 to 400 agents), both for the agent-based and for the population-based case. Below, a number of them are described. First an agent-based simulation experiment is described. In this first experiment, 25 agents were involved. The initial settings used for the variables and parameters involved in the experiment are shown in Table 4. The results of the simulations are shown in Figure 1a and 1b. In Figure 1a, time is on the horizontal axis and the value of the world economy is represented on the vertical axis. It is evident from the graph that the economy grows as time increases (but fluctuating continuously). Figure 1b shows the individual greed values of all 25 agents. As can be seen they fluctuate within a bandwidth of about 25% with lowest points between about 0.1 and 0.15, and highest points around 0.45. The pattern of the average greed over all 25 agents is shown in Figure 1c.\nFor the population-based simulation, all the parameter settings are the same as in Table 4, except parameters y, c and e. The values for parameters y, c and e used in the population-based simulation were determined on the basis of the settings for the agent-based simulations by taking the average y, c and e for all fifty agents:\ny = ( \u03a3 k y k )/n c = ( \u03a3 k c k )/n e = ( \u03a3 k e k )/n\nThe results of the population-based simulations are shown in Figure 2a (economy) and 2b (greed). As can be seen from these figures, the results approximate the results for the agent-based simulation. The difference of the world economy for the population-based and agent-based simulation (averaged over all time points) turns out to be 0.112, and the difference between the average greed of the 25 agents in the agentbased simulation and the greed for the population-based simulation is 0.005. In addition, a number of simulation runs have been performed for other population sizes. Figure 3a displays the (maximum and average) difference between the world economy in the agent-based model and the world economy in the population-based model for various population sizes. Similarly, Figure 3b displays the difference between the average greed in the agent-based model and the greed in the population-based model for various population sizes. The red line indicates the maximum value and the blue line the average value over all time points. As the figures indicate, all differences approximate a value that is close to 0 as the population size increases. Although the results of these particular simulation experiments should not be over-generalised, this is a first indication that for higher numbers of agents, the results of the agent-based model can be approximated by those of the population-based model.  ", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_15", "fig_28", "fig_28"], "table_ref": ["tab_48", "tab_48"]}, {"heading": "Mathematical Analysis", "text": "In this section a mathematical analysis is presented concerning the conditions under which partial or full equilibria occur; it is assumed that the parameters a, b, c and e are nonzero. For an overview of the equilibria results, see Table 5.\nDynamics of the economy. The economy grows when dx/dt > 0 and shrinks when dx/dt < 0; it is in equilibrium when dx/dt = 0. Assuming x nonzero, according to equation (1) for the population-based model, this can be related to the value of the greed as follows economy grows dx/dt > 0 \u21d4 ax -bxy > 0 \u21d4 a -by > 0 \u21d4 y < a/b economy shrinks dx/dt < 0 \u21d4 ax -bxy < 0 \u21d4 a -by < 0 \u21d4 y > a/b economy in equilibrium dx/dt = 0 \u21d4 ax -bxy = 0 \u21d4 a -by = 0 \u21d4 y = a/b So, as soon as the greed exceeds a/b the economy will shrink (for example, due to too many bankruptcies), until the greed has gone below this value. This indeed can be observed in the simulation traces. For the agent-based model similar criteria can be derived, but then relating to the average greed z instead of y.\nFull Equilibria for the Population-Based Model. The first issue to be analysed is whether (nonzero) equilibria exist for the whole population-based model, and if so, under which conditions. This can be analysed by considering that x, y and TD are constant and nonzero. For x constant above it was derived from (1) that the criterion is y = a/b. For TD constant the criterion is inn = 0 as immediately follows from (3). The criterion for dy/dt = 0 can be derived from It turns out that for any nonzero setting for the parameters a, b, c and e and for setting inn = 0 for the innovation parameter and for any value of TD a nontrivial equilibrium is (only) possible with values as indicated above. Note that this shows that for inn nonzero a nontrivial full equilibrium is not possible, as TD will change over time. However, partial equilibria for greed still may be possible. This will be analysed next Equilibria for greed in the population-based model. Suppose that the innovation inn is nonzero. In this case it cannot be expected that technological development TD and economy x stay at constant nonzero values. However still for the greed variable y an equilibrium may exist. Note that for inn = 0 this also includes the result for the full equilibrium obtained earlier. Moreover, as the equation for TD can be solved analytically, and x = \u03b1 TD, also an explicit solution for x can be obtained:\nTD(t) = TD(0) e inn t x(t) = \u03b1 TD(t) = \u03b1 TD(0) e inn t = x(0) e inn t\nHere \u03b1 can be expressed in the parameters as follows: This shows that according to the model greed can be in an equilibrium y = (a -inn)/b, in which case the economy shows a monotonic exponential growth.\nFull Equilibria for the agent-based model. Similar to the approach followed above:\n(1) dx/dt = (ax -bxz) = 0 (2) d y (k) /dt = (c k bx y (k) (2-y (k) ) / TD -e k y (k) ) = 0 (for all agents k) (3) dTD/dt = inn TD = 0 (4) z = ( \u03a3 k y (k) )/n\nA full equilibrium can be expressed by the following equilibria equations:\n(1) ax = bxz (2) c k bx y (k) (2-y (k) ) / TD = e k y (k) (3) inn TD = 0 (4) z = (\u03a3 k y (k) )/n\nIt is assumed that a, b, c k and e k are nonzero. One trivial solution is x = y (k) = 0.\nAssuming that x, y (k) and TD all are nonzero, the equations (1) to (3) are simplified: From this the values for the y (j) can be determined: \ny (j) = 2-", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_56"]}, {"heading": "TD(t) = TD(0) e inn t x(t) =(1/(2b-a +inn)) ( \u03a3k (ek /ck )/ n) TD(0) e inn t z = (a -inn)/b y (j) =2-(2-((a -inn ) /b)) n / \u03a3k (ek / ej )(cj / ck) TD(t) = TD(0) e inn t x(t) =(1 /(2b -a +inn)) (e / c) TD(0) e inn t y = (a -inn)/b 6 Discussion", "text": "This paper discusses similarities and dissimilarities between agent-based models and population-based models in behavioural economics. Inspired by variants of predator prey models (e.g., [6], [12], [13], and [16]), a dynamic behavioral economical model was developed for the relationship between individual agents' greed and the global economy. Simulation experiments for different population sizes were performed for both an agent-based and a population-based model. For both cases the results show that the world economy grows in a fluctuating manner over time and the average greed of the agents fluctuates between 0.1 and 0.45. A mathematical analysis was performed for both, showing the conditions under which equilibria occur.\nIt turned out that, in particular for large population sizes, the differences in the economy and average greed between agent-based and population based simulations are close to zero. In different domains, in [4] and [9], under certain conditions similar results were obtained. In literature on agent-based simulation such as in (e.g., [2] and [7]), it is argued that although agent-based modelling approaches are more expensive computationally than population-based modelling approaches, they are preferable due to more accuracy. In contrast to this, the results in the current paper indicate that for the considered domain the agent-based approaches can be closely approximated by population-based simulations. On the other hand, for cases with a rather small number n of agents the population-based approach may be inadequate. This may raise the question whether a more differentiated point of view in the debate can be considered, namely that for numbers of n agents exceeding a certain N, population-based models are as adequate as agent-based models, whereas for n < N agent-based models are more adequate. A challenge may be to determine this number N for different cases.\nFor future work, more differentiated personality aspects will be included in the agent model, concerning risk profile and emotions (e.g., feeling insecure) involved, depending upon which decisions are made for the investment (in banking products or stock market). A further aim is to develop a web-based business application incorporating a virtual agent that will interact with a client and regulate the emotions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "As the internet becomes more and more prevalent, communities of web services, which are large scale virtual networks of web services, attract more and more attention [1,2]. In the last two years, these communities started to move toward \"agent-like\" models that include largely independent agent-based web services. However, this merging results in more complex systems where functional and non-functional properties cannot be easily checked by simply inspecting the system model. Since it is very expensive to modify communities of web services that have been deployed, it is desirable to have methods available for the verification of communities' properties earlier in the design phases. Model checking [5] is a suitable solution in this case because it is a formal technique allowing the automatic verification of the systems design against specific properties that capture the requirements.\nIn nineties, Researchers have put forward in [7] an application of model checking within the context of the logic of knowledge. After that, several approaches have been proposed for model checking multi-agent systems. In [16], Wooldridge et al. have proposed an imperative programming language, MABLE to specify multi-agent systems along with a Belief-Desire-Intention (BDI) logic to express the properties. SPIN, an automata-based model checker has been used to verify if the specified MABLE model satisfies the expressed properties. Another method based on the SPIN model checker has been developed in [3] using AgentSpeak(F) Language, a BDI logic-based programming language [12]. As a model grows, automata-based model checking can face a serious state explosion problem. One technique to avoid this problem is symbolic model checking based on Ordered Binary Decision Diagrams (OBDDs). NuSMV [4], MCK [14] and MCMAS [9] are examples of model checkers using this approach. NuSMV supports both Linear Temporal Logic (LT L) and branching time logic (CT L). MCK works on a particular input model of synchronous interpreted systems of knowledge. The specification formulae in MCK can be either LT L or CT L augmented with knowledge. Similar to MCK, in MCMAS, models are described into a modular language called Interpreted Systems Programming Language (ISPL). Although the framework of interpreted systems is powerful and popular in multi-agent systems, it cannot be directly used by designers to describe business and industrial systems. For this type of applications, it deems appropriate to use suitable modeling languages such as UML (Unified Modeling Language).\nThe motivation of this paper is to build the connection among UML, agentbased communities, and symbolic model checking so that we can use existing model checkers, like MCMAS, to check these communities' models directly. We propose an approach based upon symbolic model checking to verify communities presented by UML activity diagram, which shows the activities and flow of control for the model [15]. We formalize agent-based communities of web services with the execution semantics of UML activity diagram. We adopt CT L * CA proposed in [2] for communicating agents as the logic for specifying the properties to be checked. We use the MCMAS model checker in our symbolic approach for the verification of communities of web services. There are two reasons behind choosing MCMAS: 1) unlike NuSMV and MCK, MCMAS supports directly agent specifications we need for agent-based communities of web services; and 2) in terms of the adopted specification language, MCMAS is the closest to CT L * CA . We experiment this approach with an implementation to verify the P NAW S protocol (Persuasion/Negotiation protocol for Agent-based Web Services) [2]. P NAW S is a communication protocol used by agent-based web services to negotiate joining a given community.\nThe structure of this paper is as follows: In Section 2, we present an overview of our model checking approach and explain how we formalize the activity diagram to represent communicating agent-based web services. The specification language CT L * CA logic for communicating agents will be also discussed. In Section 3, we define the rules for mapping and transforming formalized activity diagram model and properties specification into the Interpreted Systems Programming Language (ISPL), which is used as the input language of the MCMAS model checker. Section 4 presents the experimental results of verifying the P NAW S protocol with MCMAS. Finally, we summarize our work and discuss future work in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Modeling/Specifying Communities of Web Services", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approach Overview", "text": "Model checking is a three-step process [5]: modeling, specification, and verification. We use UML activity diagrams to model the system, CT L * CA as specification language to state the properties that the design must satisfy, and symbolic model checking with OBDDs for verification.\nFig. 1 illustrates our general approach. It starts with modeling communities of web services as activity diagrams and specifying the properties as formal requirements. The modeled system and formalized specifications are read as inputs by our automatic transformation engine, which uses transformation definitions (rules) to map the input model and specifications (properties) into the ISPL model and formulae. Finally, the MCMAS model checker verifies the ISPL model against the formulae. Witnesses are generated if the formulae are true (i.e. the properties are satisfied); otherwise, counterexamples are generated. Before using this approach, an issue should be resolved: the activity diagram modeling the community of web services can have an infinite state space, while symbolic model checking requires the state space to be finite. To convert activity diagram from infinite to finite state space, Eshuis and Wieringa have proposed in [6] some techniques to remove unbounded states, which do not have a maximum number of their active instances. We adopt this method in our approach.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "UML Activity Diagrams", "text": "A UML activity diagram shows the activities and flow of control for the model [15]. Activity states are represented with rounded rectangles, a black solid circle stands for an initial node and a black in-out circle is a final node. A diamond represents a decision or merging state. A bar shows an activity that splits a flow into several concurrent flows or an activity that synchronizes several concurrent flows and joins them into one single flow.\nFig. 2 shows an activity diagram for a concrete example of P NAW S protocol model [2]. According to this protocol, agent-based web services interact with each other in a negotiation setting. Our example diagram presents the Master web service (MWS) agent's behavior of inviting a Slave web service to join its community and the Slave web service (SWS) agent's behavior of negotiating the joining contract. The MWS, which represents the community, starts the session by sending the invitation. The SWS can either accept or refuse. If the SWS accepts, the session will end with successful invitation. Otherwise, the MWS will defend the invitation proposal with negotiation arguments. Then, if the defence is not definitely accepted or refused, the MWS and SWS will start a negotiation process consisting of a sequence of challenge/justification and attack until they achieve an agreement, refusal or timeout. The activity diagram used in this paper describes the behavior of agents that interact with each other. These agents perform certain actions according to the protocol they use, which is a set of rules describing the allowed communicative acts in different situations. An agent has beliefs, goals, and intentions that are stored in a database accessible to the agent but external to the activity diagram. Activity states represent activities preformed by certain agents, such as accepting or refusing a proposal. A transition from one state to another is trigged by a set of internal (by agent's own actions) or external (by other agents' actions) activities. We use Act \u2212\u2192 to represent the transition relation. In order to use activity diagrams in our symbolic model checking approach, we need to define their formal semantics. Because we associated them to agent communication protocols, a suitable solution would be to define a formal agent hypergraph from the notion of activity hypergraph used to model check activity diagrams [6]. The idea behind an agent hypergraph is to capture the execution structure of the communication protocol among agent-based web services. We use CT L * CA model to represent the communicative acts agents use when communicating. These communicative acts are defined as action performed on public commitments the agents make. For example, by inviting a Slave web service to join a community, the Master creates a new public commitment and accepting this invitation means accepting the content of this public commitment. An agent hypergraph is defined as a tuple: < S, s 0 , Ag, Act, Act \u2212\u2192, V P C >, where:\n-S is a set of all possible states in the system. There are three kinds of states in this set: one initial state, at least one final state, and none or several activity states which are not initial state or final states. -s 0 is the initial state.\n-Ag is a non-empty set of agents.\n-Act is a set of allowed actions agents can perform. -Act \u2212\u2192\u2286 S \u00d7 Ag \u00d7 S is the transition relation. We write s i , Ag n Act k \u2212\u2192 s j to express how the agent Ag n evolves from one state s i to another state s j by performing the action Act k . -V P C : S \u2192 2 C is a function associating to each state the set of public commitments made in this state, where C is the set of all public commitments.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Logic for Specification -CT L * CA", "text": "Syntax. We use CT L * CA [2] to specify the properties our agent-based communities of web services should satisfy. CT L * CA extends CT L * [5] by adding public commitments and action formulae. This logic supports two kinds of formulae: state formulae S evaluated over states and path formulae P evaluated over paths that are infinite sequences of states. We use p, p 1 , p 2 , . . . to range over the set of atomic propositions \u03a6 p and \u03c6 1 , \u03c6 2 , . . . to range over path formulae. The syntax of this logic is given in Table 1. The temporal operator X + \u03c6 1 means in the next state \u03c6 1 is true, X \u2212 \u03c6 1 means in the previous state \u03c6 1 is true, \u03c6 1 U + \u03c6 2 means \u03c6 1 is true utile \u03c6 2 becomes true and \u03c6 1 U \u2212 \u03c6 2 means \u03c6 1 is true since \u03c6 2 was true. A stands for the universal path quantifier and E stands for the existential path quantifier. The formula \u03c6 1 \u2234 \u03c6 2 means that \u03c6 1 is an argument for \u03c6 2 and is read as: \u03c6 1 so \u03c6 2 . This operator introduces argumentation as a logical relation between path formulae.\nThe formula P C(Ag 1 , Ag 2 , t, \u03c6 1 ) is the public commitment made by agent Ag 1 at the moment t towards agent Ag 2 saying that the path formula \u03c6 1 is true. Act k (Ag n , P C(Ag 1 , Ag 2 , t, \u03c6 1 )) means that agent Ag n (n \u2208 {1, 2}) performs an action Act k on the commitment made by Ag 1 towards Ag 2 . The set of actions performed on commitments may change to suit different systems. For communities of web services, we use Create, Accept, Ref use, Def end, Challenge, Justif y, and Attack.\nFormal Semantics. The formal model M associated to this logic corresponds exactly to our agent hypergraph defined above. Because of space limit and to focus more on the verification issue, which is the main contribution of this paper, here we only specify the semantics of the argument and commitment operators. The semantics of CT L * CA state formulae is as usual (semantics of CT L * ). A path satisfies a state formula if the initial state in the path does so. Along a path x i , which starts at state s i , \u03c6 1 \u2234 \u03c6 2 holds iff \u03c6 1 is true and in the next state through the same path if \u03c6 1 holds then \u03c6 2 holds too. Formally (\u21d2 stands for material implication):\nx i |= M \u03c6 1 \u2234 \u03c6 2 iff x i |= M \u03c6 1 and x i+1 |= M \u03c6 1 \u21d2 \u03c6 2\nA state s i satisfies P C(Ag 1 , Ag 2 , t, \u03c6 1 ) iff the commitment is in this state and there is a path along which the commitment content holds. Formally:\ns i |= M P C(Ag 1 , Ag 2 , t, \u03c6 1 ) iff P C(Ag 1 , Ag 2 , t, \u03c6 1 ) \u2208 V P C (s i ) and s i |= M E\u03c6 1 A path x i satisfies Act k (Ag n , P C(Ag 1 , Ag 2 , t, \u03c6 1 )\n) iff Act k is in the label of the first transition on this path and in the past 1 P C(Ag 1 , Ag 2 , t, \u03c6 1 ) holds along the same path. Formally:\nx i |= M Act k (Ag n , P C(Ag 1 , Ag 2 , t, \u03c6 1 )) iff s i , Ag n Act k \u2212\u2192 s i+1 and x i |= M F \u2212 P C(Ag 1 , Ag 2 , t, \u03c6 1 )", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Verification", "text": "In this section, we will use the P NAW S protocol presented in Section 2.2 to show how our verification approach works. As discussed earlier, we use the MCMAS model checker. In MCMAS, multi-agent systems are described by the Interpreted Systems Programming Language (ISPL), where the system is distinguished into two types of agents: environment agent, which is used to describe boundary conditions and infrastructures, and standard agents. ISPL can also be used to define atomic propositions, action formulae and the specification of properties to be checked. To automatically use this model checker to verify the communication protocol of community of web services, we define a mapping between our agent hypergraph and ISPL and encode our CT L * CA formulae in MCMAS.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mapping and Transforming Agent Hypergraph to ISPL", "text": "In MCMAS, Each agent is composed by: a set of local states, a set of actions, a rule (protocol) describing which action can be performed by an agent, and evolution functions that describe how the local states of the agents evolve based on their current local states and agents' actions [9]. The mapping from our agent hypergraph to ISPL is defined by he following rules:\n1. S \u2212 to \u2212 LocalState: Every state in the agent hypergraph is mapped to the ISPL environment agent, a local state with the same name. 2. Ag \u2212 to \u2212 Agent: Every agent in the agent hypergraph is mapped to an ISPL agent with the same name. A special agent environment should also be added to the ISPL agent list.\n3. V P C \u2212 to \u2212 LocalV alue: V P C is transformed to local values of the agent that creates the public commitments. The values can be bounded integers, Booleans or enumerations based on the types of these commitments. 4. Act \u2212 to \u2212 action/rule: Every action in the agent hypergraph is converted to an ISPL action list of an agent that can execute the action. 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Act", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2212\u2192 \u2212to \u2212 evolution:", "text": "Act \u2212\u2192 is translated into an ISPL agent evolution list. For example if we have s i , Ag n Act k \u2212\u2192 s j , the Ag n 's evolution in ISPL will be: state = s(j) if state = s(i) and Action = Act(k); 6. s 0 \u2212 to \u2212 Init: s 0 is mapped to an ISPL initial state.\nBased on these mapping rules, we use P NAW S as an example to transform the associated agent hypergraph into ISPL. In communities of web services, the system includes two types of agents: Master agent and Slave agent. We add an Environment agent to describe the system boundary conditions and infrastructures. Environment agent is a special agent in ISPL system that provides observable variables that can be accessed by other agents. Every agent starts with declaration of local variables. The first mapping rule is used to define the local variables of agent. We declare a state variable to list all possible states in the system:\nVars: State: {WaitingforCreate, RefuseReceive...}; end Vars Actions an agent can perform are constructed into Actions section of ISPL file and follow the 4th mapping rule. We also add \"null\" action to stand for no action. In ISPL Protocol section, we give the permitted actions in each state. Transitions are defined in ISPL Evolution section to show the states change based on the 5th mapping rule.\nProtocol: state = WaitingforCreate: {Create}; ... end Protocol Evolution: state = ChallengeReceived if state = DefenceSend and Slave1.Action=Challenge; ... end evolution Moreover, we declare a set of initial states. The system starts at state waiting for creating a protocol session with all the counter register reset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "InitStates", "text": "(Environment.state = WaitingforCreated) and (Master.commitments = toCreate) and (Environment.attackCount = 0) and (Environment.challengeCount = 0); end InitStates", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Encoding Specifications in ISPL", "text": "ISPL specifies both the model and properties. It supports CTL and ATL. Our specifications are expressed by CT L * CA , which extends CT L * . Therefore, the basic operators are similar to CTL and we can directly use them in ISPL. For the new operators in CT L * CA , we define rules to convert them into ISPL.\nTo translate \u03c6 1 \u2234 \u03c6 2 formula into ISPL, we need to declare two variables over the system: a1 and a2 to stand for \u03c6 1 and \u03c6 2 . We also need to define the equivalent formula according to the semantics given in Section 2.3, where X stands for next and -> stands for implication. Evaluation a1; a2; ... end Evaluation Formula a1 and X(a1 -> a2); end Formula For the formulae P C(Ag 1 , Ag 2 , t, \u03c6 1 ) and Act k (Ag n , P C(Ag 1 , Ag 2 , t, \u03c6 1 )), the semantics is already encoded in ISPL as V P C and Act \u2212\u2192 are already translated by the 3rd and 5th rules. We just need to define a local value a1 in agent Ag1's definition to present the commitment, then create the action Act k over this commitment. The moment t is declared in Environment agent because both agents Ag1 and Ag2 need to access it at that moment. The code below is an example of the Create action (i.e. sending an invitation).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evolution:", "text": "Lvar = Ag2Lvar if moment = t and Ag1.Action = Create and a1; ... end evolution\nIn order to verify the model, we first define some atomic propositions over the system. Thereby, the propositional formulae, which we need to check by MC-MAS, are defined based on these propositions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "We have implemented the mapping rules and the P NAW S case study scenario along with the specifications in ISPL and verified them with MCMAS. We formalize various properties of compliance for P NAW S protocol. Here are some examples, where G means globally, F means in the future and A and E are the universal and existential quantifiers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Termination: P NAW S always terminates.", "text": "AG termination Termination is an atomic proposition for termination state of the protocol. Intuitively, this property should hold because with finite states and restrained unbound states, a protocol will end. 2. Soundness: The protocol is correct. An example of soundness is: if there is a challenge, a justification will follow in the future. AG (challenge -> EF justify) 3. Reachability: certain states are reachable through any possible sequence of transitions, starting from the initial state. For example, if there is a refusal for an invitation, the protocol will reach defense state. AG (refusal -> EF defense)\n4. Liveness: Liveness means something good will eventually happen. An example of liveness is: if there is a negotiation, an acceptance will eventually follow in the future. AG (Attack or Chalenge -> EF Accept)\nOur system was running on Windows Vista Home Premium on Inter Core 2 Duo CPU T6400 2.00GHz with 3.0GB memory. We used different numbers of slave agents in the systems to monitor the changes as the models grow. Experimental results are presented in Table 2. The first column indicates the numbers of Slave Agents in the system. The number of actual reachable states in the corresponding model is shown in column two. The third column reports the total number of nodes that were requested and obtained during reordering and verification process. Memory usage and approximate execution time are listed in column four and column five. The rest of columns show that the properties are satisfied.\nThe results clearly show that the state space grows exponentially, but thanks to symbolic model checking the execution time is low. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Conclusion and Future Work", "text": "Many research proposals [6,10,13] have addressed the verification of behavior specifications in UML. Also, extensive research [2,9,11,14,16] has been done on the verification of multi-agent systems. However, few work focus on these two techniques together. This paper proposes a fully automated approach to verify agent-based communities of web services modeled in UML activity diagrams.\nWe formalized UML activity diagrams using agent hypergraphs and specified properties using a new logic for agent communication: CT L * CA . We defined and implemented the mapping rules for transforming the agent hypergraphs and CT L * CA specifications into the ISPL language. Finally, we used MCMAS to experiment with the P NAW S protocol. In this work, action formulae are only captured by the transition labels. Considering the full semantics of different actions in different situations is needed to check more complicated protocols. Our plan for future work is to extend CT L * CA by adding different action formulae and proposing a new OBDD-based algorithm for the model checking. We also plan to extend MCMAS to be fully compatible with this new logic. Besides model checking, we are planning to use Model Driven Architecture (MDA), launched by the Object Management Group (OMG) in 2001 as a promising software design method, to develop a flexible platform for agent-based communities of web services. We intent to use model transformation, which is a process that generates a refined model from a source model [8]. This process is based on a transformation definition, which is a set of transformation rules that describe how one or more constructs in the source language can be transformed into one or more constructs in the target language. This process can be achieve automatically, which helps in reducing programming errors and coding time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "One of the interesting areas in which cognitive models can be applied in a practically useful manner is the area of Ambient Intelligence, addressing technology to contribute to personal care for safety, health and wellbeing; e.g., [1]. Such applications make use of sensor devices to acquire sensor information about humans and their functioning, and of intelligent devices exploiting knowledge for analysis of such information. Based on this, appropriate actions can be undertaken that improve the human's safety, health, and behaviour. Commonly, decisions about such actions are made by these intelligent devices based only on observed behavioural features of the human and her context (cf. [3]). A risk of such an approach is that the human is guided only at the level of her behaviour and not at the level of the underlying cognitive states causing the behaviour. Such a situation might lead to suggesting the human to suppress behaviour that is entailed by her internal cognitive states, without taking into account these cognitive states (and their causes) themselves.\nAs an alternative route, the approach put forward in this paper incorporates a cognitive analysis of the internal cognitive states underlying certain behavioural aspects. To this end, a computational model is described, in which a given cognitive model of the human's functioning is exploited. A cognitive model is formalised using the Temporal Trace Language (TTL) [2]. In contrast to many existing cognitive modelling approaches based on some form of production rule systems, TTL allows explicit representation of time and complex temporal relations.\nBy performing cognitive analysis the computational model is able to determine automatically which cognitive states relate to considered behavioural (or performance) aspects of the human, which external events (e.g., stimuli) are required to be monitored to identify these cognitive states (monitoring foci), and how to derive conclusions about the occurrence of cognitive states from such acquired monitoring information. More specifically, monitoring foci are determined by deriving representation relations for the human's cognitive states that play a role in the cognitive model considered. Within Philosophy of Mind a representation relation relates the occurrence of an internal cognitive state property of a human at some time point to the occurrence of other (e.g., external) state properties at the same or at different time points [7]. For example, the desire to go outside may be related to an earlier good weather observation. As temporal relations play an important role here, in the computational model these representation relations are expressed as temporal predicate logical specifications. From these temporal expressions externally observable events are derived that are to be monitored. From the monitoring information on these events the computational model verifies the representation expressions, and thus concludes whether the human is in such a state. Furthermore, in case an internal state has been identified that may affect the behaviour or performance of the human in a certain way, appropriate actions may be proposed.\nThe paper is organised as follows. The modelling approach is introduced in Section 2. An example used throughout the paper is described in Section 3. In Section 4 the proposed cognitive analysis approach is described. Finally, the paper is concluded with a discussion and summary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Modelling Approach", "text": "To model the dynamics of cognitive processes with an indication of time, a suitable temporal language is required. In the current paper, to specify temporal relations the Temporal Trace Language (TTL) is used. This reified temporal predicate logical language supports formal specification and analysis of dynamic properties, covering both qualitative and quantitative aspects. Dynamics are represented in TTL as an evolution of states over time. A state is characterized by a set of state properties expressed over (state) ontology Ont that hold. In TTL state properties are used as terms (denoting objects). To this end the state language is imported in TTL. Sort STATPROP contains names for all state formulae. The set of function symbols of TTL includes \u2227, \u2228, \u2192, \u2194: STATPROP x STATPROP \u2192 STATPROP; not: STATPROP \u2192 STATPROP, and \u2200, \u2203: S VARS x STATPROP \u2192 STATPROP, of which the counterparts in the state language are Boolean propositional connectives and quantifiers. To represent dynamics of a system sort TIME (a set of time points) and the ordering relation > : TIME x TIME are introduced in TTL. To indicate that some state property holds at some time point the relation at: STATPROP x TIME is introduced. The terms of TTL are constructed by induction in a standard way from variables, constants and function symbols typed with all beforementioned sorts. The language TTL has the semantics of many-sorted predicate logic. A special software environment has been developed for TTL, featuring a Property Editor for building TTL properties and a Checking Tool that enables automated formal verification of such properties against a set of traces.\nThe modelling approach presented in this paper adopts a rather general specification format for cognitive models that comprises past-present relationships between cognitive states and between cognitive states and sensor and effector states, formalised by temporal statements expressible within TTL. In this format, for a cognitive state a temporal pattern of past states can be specified, which causes the generation of this state; see also [6]. A past-present statement (abbreviated as a pp-statement) is a statement \u03d5 of the form B \u21d4 H, where the formula H, called the head and denoted by head(\u03d5), is a statement of the form at(p, t) for some time point t and state property p, and B, called the body and denoted by body(\u03d5), is a past statement for t. A past statement for a time point t over state ontology Ont is a temporal statement in TTL, such that each time variable s different from t is restricted to the time interval before t: for every time quantifier for a time variable s a restriction of the form t > s is required within the statement. Sometimes B is called the definition of H.\nMany types of cognitive models can be expressed in such a past-present format, such as causal models, dynamical system and connectionist models, rule-based models, and models in which memory of past events is used, such as case-based models. In the next section an example of a cognitive model specified in past-present format is given.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Case Study", "text": "To illustrate the proposed model a simplified example to support an elderly person in food and medicine intake is used. The following setting is considered. In normal circumstances the interval between two subsequent food intakes by the human during the day is known to be between 2 and 5 hours. When the human is hungry, she goes to the refrigerator and gets the food. Sometimes the human feels internal discomfort, which can be soothed by taking medicine X. The box with the medicine lies in a cupboard. There should be no food consumption for 2 hours after taking medicine. To maintain a satisfactory health condition of the human, intelligent support is employed, which is described by the computational model presented throughout the paper.\nThe behaviour of the human for this example is considered as goal-directed and is modelled using the BDI (Belief-Desire-Intention) architecture [9]. The graphical representation of the cognitive model that produces the human behaviour is given in Fig. 1. In this model the beliefs are based on the observations. For example based on the observation that food is taken, the belief b1 that food is taken is created. The desire and intention to have food are denoted by d1 and i1 correspondingly. The desire and intention to take medicine are denoted by d2 and i2 correspondingly. The model from the example was formalised by the following properties in past-present format:", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "IP1(c): General belief generation property", "text": "At any point in time a (persistent) belief state b about c holds iff at some time point in the past the human observed c. Formally: \u2203t2 [ t1 > t2 & at(observed(c), t2) ] \u21d4 at(b, t1)\nIP2: Desire d1 generation At any point in time the internal state property d1 holds iff at some time point in the past b1 held. Formally: \u2203t4 [ t3 > t4 & at(b1, t4) ] \u21d4 at(d1, t3)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IP3: Intention i1 generation", "text": "At any point in time the internal state property i1 holds iff at some time point in the past b2 and d1 held. Formally: \u2203t6 [ t5 > t6 & at(d1, t6) & at(b2, t6)] \u21d4 at(i1, t5)\nIP4: Action eat food generation At any point in time the action eat food is performed iff at some time point in the past both b3 and i1 held. Formally: \u2203t8 [ t7 > t8 & at(i1, t8) & at(b3, t8)] \u21d4 at(performed(eat food), t7)\nIP5: Desire d2 generation At any point in time the internal state property d2 holds iff at some time point in the past b4 held. Formally: \u2203t10 [ t9 > t10 & at(b4, t10) ] \u21d4 at(d2, t9)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IP6: Intention i2 generation", "text": "At any point in time the internal state property i2 holds iff at some time point in the past b5 and d2 held. Formally: \u2203t12 [ t11 > t12 & at(d2, t12) & at(b5, t12)] \u21d4 at(i2, t11) IP7: Action medicine intake generation At any point in time the action medicine intake is performed iff at some time point in the past both b6 and i2 held. Formally:\n\u2203t14 [ t13 > t14 & at(i2, t14\n) & at(b6, t14)] \u21d4 at(performed(medicine intake), t13)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cognitive Analysis", "text": "First, a set of goals is defined on the human's states and behaviour. The goal for the case study is to maintain a satisfactory health condition of the human. Each goal is refined into more specific criteria that should hold for the human's functioning. In particular, for the case study the goal is refined into three criteria:\n(1) food is consumed every 5 hours (at latest) during the day;\n(2) after the medicine is taken, no food consumption during the following 2 hours occurs;\n(3) after 3 hours from the last food intake no medicine intake occurs.\nBased on the criteria expressions, a set of output states (called an output focus) and a set of internal (cognitive) states (called an internal focus) of the human are determined, which are used for establishing the satisfaction of the criteria. For the case study the output focus consists of the states performed(eat food) and performed(medicine intake).\nA cognitive model of the human defines relations between an output state and internal states which cause the generation of the output state. The latter provide a more in depth understanding of why certain behaviours (may) occur. In general, using a cognitive model one can determine a minimal specification that comprises temporal relations to internal states, which provides necessary and sufficient conditions on internal states to ensure the generation of an output state. An automated procedure to generate such specifications is considered in Section 4.1. Such a specification is a useful means for prediction of behaviour. That is, if an essential part of a specification becomes satisfied (e.g., when some important internal state(s) hold(s)), the possibility that the corresponding output state will be generated increases significantly. If such an output is (not) desired, actions can be proposed in a knowledgeable manner, based on an in depth understanding of the internal states causing the behaviour. Thus, the essential internal states (called predictors for an output) from specifications for the states in the output focus should be added to the internal focus.\nNormally states in an internal focus cannot be observed directly. Therefore, representation relations are to be established between these states and externally observable states of the human (i.e., the representational content should be defined for each internal state in focus). Representation relations are derived from the cognitive model representation as shown in Section 4.2 and usually have the form of more complex temporal expressions over externally observable states. To detect occurrence of an internal state, the corresponding representational content should be monitored constantly, which is considered in Section 4.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generating Predictors for Output States", "text": "A predictor(s) for a particular output can be identified based on a specification of human's internal dynamics that ensures the generation of the output. In general, more than one specification can be identified, which is minimal in terms of numbers of internal states and relations between them, however sufficient for the generation of a particular output. Below an automated procedure for the identification of all possible minimal specifications for an output state based on a cognitive model is given. The rough idea underlying the procedure is the following. Suppose for a certain output state property p the pp-statement B \u21d4 at(p, t) is given. Moreover, suppose that in B only two atoms of the form at(p1, t1) and at(p2, t2) with internal states p1 and p2 occur, whereas as part of the cognitive model specifications B1 \u21d4 at(p1, t1) and B2 \u21d4 at(p2, t2) are available. Then, within B the atoms can be replaced (by substitution) by the formula B1 and B2. Thus, at(p, t) may be related by equivalence to four specifications:\nB \u21d4 at(p, t) B[B2/at(p2, t2)] \u21d4 at(p, t) B[B1/at(p1, t1)] \u21d4 at(p, t) B[B1/at(p1, t1), B2/at(p2, t2)] \u21d4 at(p, t)\nHere for any formula C the expression C[x/y] denotes the formula C transformed by substituting x for y.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm. GENERATE-MINIMAL-SPECS-FOR-OUTPUT", "text": "Input: Cognitive model X; output state in focus specified by at(s, t) Output: All possible minimal specifications for at(s, t) in list L 1 Let L be a list containing at(s, t), and let \u03b4p, \u03b4 be empty substitution lists. 2 For each formula \u03d5I \u2208 L: at(ai, t) \u2194 \u03c8ip(at1,\u2026, atm) identify \u03b4i = {atk/body(\u03d5k) such that \u03d5k \u2208 X and head(\u03d5k)=atk}. Then \u03b4 is obtained as a union of \u03b4i for all formulae from L. 3 \u03b4 = \u03b4 \\ \u03b4p 4 if \u03b4 is empty, finish. 5 For each formula \u03d5I \u2208 L obtain a set of formulae by all possible combinations of substitution elements from \u03b4 applied to \u03d5I. Add all identified sets to L. 6 \u03b4p = \u03b4p \u222a \u03b4, proceed to step 2.\nFor each generated specification the following measures can be calculated:\n(1) The measure of desirability indicating how desirable is the human's state, described by the generated specification at a given time point. The measure ranges from -1 (a highly undesirable state) to 1 (a highly desirable state). (2) The minimum and maximum time before the generation of the output state(s). This measure is critical for timely intervention in human's activities.\nThese measures serve as heuristics for choosing one of the generated specifications.\nTo facilitate the choice, constrains on the measures may be defined, which ensure that an intervention occurs only when a considerable (un)desirability degree of the human's state is determined, but also the minimum time before the (un)desirable output(s) is above some acceptable threshold. To calculate the measure (1), the degree of desirability is associated with each output state of the cognitive model. Then, it is determined which output states from the cognitive specification can be potentially generated, given that the bodies of the formulae from the generated specification are evaluated to TRUE. This is done by executing the cognitive specification with body(\u03d5i) = TRUE for all \u03d5i from the generated specification. Then, the desirability of a candidate specification is calculated as the average over the degrees of desirability of the identified output states, which can be potentially generated. The measures (2) can be calculated when numerical timing relations are defined in the properties of a cognitive specification. After a specification is chosen, a set of predictor states from the specification for the output states in focus can be identified. When statistical information in the form of past traces of human behaviour is available, then the set of predictors is determined by identifying for each candidate two sets: a set of traces S in which the outputs in focus were generated and set T \u2286 S in which the candidate set of predictors was generated. The closer the ratio |T|/|S| to 1, the more reliable is the candidate set of predictors for the output(s) in focus.\nFor the case study from the automatically generated specifications that create the state performed(eat food) the one expressed by property IP4 is chosen. It has the desirability of the state performed(eat food)). Furthermore, it is assumed that the time interval t7-t8 in IP4 is sufficient for an intervention. The predictor state from the chosen specification is i1, as its predictive power depends on the occurrence of b3 only. Thus, i1 is included in the internal focus. By a similar line of reasoning, the specification expressed by property IP7 is chosen, in which i2 is the predictor state included into the internal focus. Thus, the internal focus for the cognitive model is the set {i1, i2}.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Representation Relations", "text": "A representation relation for an internal state property p relates the occurrence of p to a specification \u03a6 that comprises a set of state properties and temporal relations between them. In such a case it is said that p represents \u03a6, or \u03a6 describes representational content of p. In this section an automated approach to identify representation relations for cognitive states from a cognitive model is described.\nThe representational content considered backward in time is specified by a history (i.e., a specification that comprises temporal (or causal) relations on past states) that relates to the creation of some cognitive state. In the literature on Philosophy of Mind different approaches to defining representation relations have been put forward (cf. [7]). For example, according to the classical causal/correlation approach, the representational content of an internal state property is given by a one-to-one mapping to an external state property. The application of this approach is limited to simple types of behaviour. In cases when an internal property represents a more complex temporal combination of state properties, other approaches have to be used. For example, the temporal-interactivist approach (cf. [6]) allows defining representation relations by referring to multiple (partially) temporally ordered interaction state properties; i.e., input (sensor) and output (effector) state properties over time.\nTo automate the representation relation identification based on this idea, a procedure was developed. To apply this procedure, cognitive specification is required to be stratified. This means that there is a partition of the specification \u03a0 = \u03a01 \u222a \u2026 \u222a \u03a0n into disjoint subsets such that the following condition holds: for i > 1: if a subformula at(\u03d5, t) occurs in a body of a statement in \u03a0i, then it has a definition within \u222aj <i \u03a0j.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm. GENERATE-REPRESENTATION-RELATION", "text": "Input: Cognitive specification X; cognitive state specified by at(s, t), for which the representation relation is to be identified Output: Representation relation for at(s, t) 1 Stratify X:\n1.1 Define the set of formulae of the first stratum (h=1) as {\u03d5i: at(ai, t) \u2194 \u03c8ip(at1,\u2026, atm) \u2208 X | \u2200k m \u2265k \u22651 atk is expressed using InputOnt}; proceed with h=2. 1.2 The set of formulae for stratum h is identified as {\u03d5i: at(ai, t) \u2194 \u03c8ip(at1,\u2026, atm) \u2208 X | \u2200k m \u2265k \u22651 \u2203l l < h \u2203\u03c8 \u2208 STRATUM(X, l) AND head(\u03c8) = atk AND \u2203j m \u2265j \u22651 \u2203\u03be \u2208 STRATUM(X, h-1) AND head(\u03be)=atj }; proceed with h=h+1. 1.3 Until a formula of X exists not allocated to a stratum, perform 1.2. 2 Create the stratified specification X' by selecting from X only the formulae of the strata with the number i < k, where k is the number of the stratum, in which at(s, t) is defined. Add the definition of at(s, t) from X to X'. 3 Replace each formula of the highest stratum n of X' \u03d5i:\nat(ai, t) \u2194 \u03c8ip(at1,\u2026, atm) by \u03d5I \u03b4 with renaming of temporal variables if required, where \u03b4 = {atk\\ body(\u03d5k) such that \u03d5k \u2208 X' and head(\u03d5k)=atk}. Further, remove all formulae { \u03d5 \u2208 STRATUM(X', n-1) | \u2203\u03c8 \u2208 STRATUM(X', n) AND head(\u03d5) is a subformula of the body(\u03c8)}) 4 Append the formulae of stratum n to stratum n-1, which becomes the highest stratum (n=n-1). 5 Until n>1, perform steps 3 and 4. The obtained specification with one stratum (n=1) is the representation relation specification for at(s, t)\nIn Step 3 subformulae of each formula of the highest stratum n of X' are replaced by their definitions, provided in lower strata. Then, the formulae of n-1 stratum used for the replacement are eliminated from X'. As result of such a replacement and elimination, X' contains n-1 strata (Step 4). Steps 3 and 4 are performed until X' contains one stratum only. In this case X' consists of a formula \u03d5 defining the representational content for at(s, t), i.e., head(\u03d5) is at(s, t) and body(\u03d5) is a formula expressed over interaction states and (temporal) relations between them.\nIn the following it is shown how this algorithm is applied for identifying the representational content for state i1 from the internal focus from the case study. By performing Step 1 the specification of the cognitive model is automatically stratified: stratum 1: {IP1((own_ position_refrigerator), IP1(food_not_eaten_more_than_2h), IP1(own_position_cupboard), IP1(medicine_box_taken)}; stratum 2: {IP2, IP5}; stratum 3: {IP3, IP6}; stratum 4: {IP4, IP7}. By Step 2 the properties IP4, IP5, IP6, IP7 are eliminated as unnecessary for determining the representational content of i1.\nIn Step 3 we proceed with property IP3 of the highest stratum (3):\n\u2203t6 [ t5 > t6 & at(d1, t6) & at(b2, t6)] \u21d4 at(i1, t5)\nIn this step property IP8 is obtained by replacing d1 and b2 state properties in IP3 by their definitions with renaming of temporal variables:\n\u2203t6 [ t5 > t6 & \u2203t4 [ t6 > t4 & at(b1, t4) ] & \u2203t2 [ t6 > t2 & at(observed(own_position_refrigerator), t2) ] ] \u21d4 at(i1, t5)\nFurther, the properties IP3, IP2 and IP1(own_position_ refrigerator) are removed from the specification and the property IP8 is added to the stratum 2. Then, IP9 is obtained by replacing b1 in IP8 by its definition:\n\u2203t6 [ t5 > t6 & \u2203t4 [ t6 > t4 & \u2203t15 [ t4 > t15 & at(observed(food_not_eaten_more_than_2h), t15) ] ] & \u2203t2 [ t6 > t2 & at(observed(own_position_refrigerator), t2) ] ] \u21d4 at(i1, t5)\nAfter that properties IP8 and IP1(food_not_eaten_more_than_2h) are removed from the specification and IP9 becomes the only property of the stratum 1. Thus, IP9 defines the representational content for the state i1 that occurs at any time point t5.\nSimilarly, the representational content for the other state from the internal focus i2 is identified as:\n\u2203t12 [ t11 > t12 & \u2203t16 [ t12 > t16 & at(observed(own_position_cupboard), t16) ] ] \u21d4 at(i2, t11)\nThe algorithm has been implemented in Java with the overall time complexity for the worst case is O(|X| 2 ), where |X| is the length of a cognitive specification X.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Behavioural Monitoring", "text": "To support the monitoring process, it is useful to decompose a representational content expression into atomic subformulae that describe particular interaction and world events. The subformulae are determined in a top-down manner, following the nested structure of the overall formula:\nmonitor_focus(F) \u2192 in_focus(F) in_focus(E) \u2227 is_composed_of(E,C,E1,E2) \u2192 in_focus(E1) \u2227 in_focus(E2)\nHere is_composed_of(E,C,E1,E2) indicates that E is an expression obtained from subexpressions E1 and E2 by a logical operator C (i.e., and, or, implies, not, forall, exists). At each decomposition step subexpressions representing events are added to the list of foci that are used for monitoring. This list augmented by the foci on the states from the output focus is used for monitoring. For the case study from the representation content for i1 and i2 atomic monitoring foci: observed(food_not_eaten_more_than_2h), observed(own_position_refrigerator) and observed(own_position_cupboard) were derived.\nFurthermore, the information on the states in the output and internal foci, on the chosen predictors for the output states, and on the identified representation relations is used to monitor constantly. As soon as a an event from the atomic monitoring foci occurs, the component initiates automated verification of the corresponding representational content property on the history of the events in focus occurred so far. The automatic verification is performed using the TTL Checker tool (for the details on the verification algorithm see [2]).\nAnother task is to ensure that the goal criteria hold. The satisfaction of the criteria is checked using the TTL Checker tool. To prevent the violation of a criterion promptly, information related to the prediction of behaviour (i.e., predictors for outputs) can be used. More specifically, if the internal states-predictors for a set of output states O hold, and some behaviour or performance criterion is violated under O, then an intervention in human activities is required. The type of intervention may be defined separately for each criterion. For the case study as soon as the occurrence of the prediction states i1 and i2 is established, the violation of the criteria identified previously is determined under the condition that the predicted outputs hold. To prevent the violation of the criteria, the following intervention rules are specified:\n(1) If the human did not consume food during last 5 hours, then inform the human about the necessary food intake. (2) If the human took medicine X less than 2 hours ago (time point t2 in minutes) and the existence of the predictor i1 is established, then inform the human that she still needs to wait (120-t2) minutes for taking medicine. (3) If the human did not consume food during last 3 hours and the existence of the predictor i2 is established, inform the human that she better eats first.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Conclusions", "text": "In this paper a computational model was presented incorporating a more in depth analysis based on a cognitive model of a human's functioning. Having such a cognitive model allows relating certain behavioural or performance aspects that are considered, to underlying cognitive states causing these aspects. Often cognitive models are used either by performing simulation, or by temporal reasoning methods; e.g. [8]. In this paper a third way of using such models is introduced, namely by deriving more indirect relations from these models. Such an approach can be viewed as a form of knowledge compilation [4] in a pre-processing phase, so that the main processing phase is less intensive from the computational point of view. Such a form of automated knowledge compilation occurs in two ways: first, to derive the relationships between considered behaviour or performance aspects to the relevant internal cognitive states, and next to relate such cognitive states to observable events (monitoring foci). These monitoring foci are determined from the cognitive model by automatically deriving representation relations for cognitive states in the form of temporal specifications. From these temporal expressions the events are derived that are to be monitored, and from the monitoring information on these events the representation expressions are verified automatically.\nA wide range of existing ambient intelligence applications is formalised using production rules (cf. [5]) and if-then statements. Two important advantages of such rules are modelling simplicity and executability. However, such formalism is not suitable for expressing more sophisticated forms of temporal relations, which can be specified using the TTL language. In particular, references to multiple time points possible in TTL are necessary for modelling forms of behaviour more complex than stimulusresponse (e.g., to refer to memory states). Furthermore, TTL allows representing temporal intervals and to refer to histories of states, for example to express that a medicine improves the health condition of a patient.\nAnother popular approach to formalise recognition and prediction of human behaviour is by Hidden Markov Models (HMM) (e.g., [10]). In HMM-based approaches known to the authors, recognition of human activities is based on contextual information of the activity execution only; no cognitive or (gradual) preparation states that precede actual execution of activities are considered. As indicated in [10] a choice of relevant contextual variables for HMMs is not simple and every additional variable causes a significant increase in the complexity of the recognition algorithm. Knowledge of cognitive dynamics that causes particular behaviour would provide more justification and support for the choice of variables relevant for this behaviour. Furthermore, as pointed in [3], for high quality behaviour recognition a large corpus of training data is needed. The computational costs of the pre-processing (knowledge compilation) phase of our approach are much lower (polynomial in the size of the specification). Also, no model training is required. However, the proposed approach relies heavily on the validity of cognitive models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "CTS [1] is a cognitive agent designed to provide assistance during training in virtual learning environments. In this work, it is applied to a tutoring system in order to provide assistance to astronauts learning how to manipulate Canadarm2, the robotic telemanipulator attached to the International Space Station (ISS). CTS is partly based on the latest neurobiology and neuropsychology theories of human brain function (see Figure 1) and operates through cognitive cycles (five per second) which are based on LIDA [2]. The learners' manipulations of the virtual world simulator, simulating Canadarm2 [3], constitute the interactions between them and CTS. In particular, the virtual world simulator sends all manipulation data to CTS, which, in turn, sends learners various advices to improve their performance (Figure 2). One of CTS' most significant limitations, in its current implementation, is its incapacity to find out why an astronaut made a mistake, i.e, to find the causes of the mistakes. To address this issue, we propose to implement a Causal Learning Mechanism in CTS and combine it with its existing Emotional Learning Mechanism (see (Faghihi et al., 2008) for more details). In humans, the process of inductive reasoning stems in part from activity in the left prefrontal cortex and the amygdala; it is a multimodular process [4]. We base our proposed improvements to CTS' architecture on this same logic. CTS' modular and distributed organization is ideal for the use of distinct mathematical methods and algorithms that can be tailored to the specific requirements of the Emotional Learning mechanism and the newly integrated Causal Learning mechanism.\nCausal learning is the process through which we come to infer and memorize an event's reasons or causes based on previous beliefs and current experience that either confirm or invalidate previous beliefs [5]. In the context of CTS, we refer to Causal Learning as the use of inductive reasoning to generalize rules from sets of experiences (Purves et al., 2008). CTS observes astronaut behavior without complete information regarding the reasons for their behavior. Our prediction is that, through inductive reasoning, it can infer the proper set of causal relations from its observations of the astronaut's behavior.", "publication_ref": ["b547", "b537"], "figure_ref": ["fig_2", "fig_15"], "table_ref": []}, {"heading": "Fig. 1. CTS' Architecture", "text": "The goal of CTS' Causal Learning Mechanism (CLM) is two-fold: 1) to find causal relations between events during training sessions in order to better assist users; 2) to implement partial procedural learning in CTS' Behavior Network (BN) which is based on [6] 1 . To implement CTS' CLM, we draw inspiration from Maldonado's work [5], which defines three hierarchical levels of causal learning: 1) the lower level, responsible for the memorization of task execution; 2) the middle level, responsible for the computation of retrieved information; 3) the higher level, responsible for the integration of this evidence with previous causal knowledge.\nIn the present paper, we begin with a brief review of the existing work concerning the implementation of Causal Learning in cognitive agents. We then propose our new architecture combining elements of the Emotional Mechanism (EM) and Causal Learning. Finally, we present results from our experiments with this cognitive agent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Causal Learning Models and Their Implementation in Cognitive Agents", "text": "To our knowledge, two cognitive research groups have attempted to incorporate Causal Learning mechanisms in their cognitive architecture. The first is Schoppek with the ACT-R architecture [7], who hadn't included a role for emotions in this causal learning and retrieval processes. ACT-R constructs the majority of its information according to the I/O knowledge base method. It also uses a sub-symbolic form of knowledge to produce associations between events. As explained by Schoppek [8], in ACT-R, sub-symbolic knowledge applies its influence through activation processes.\nHowever, the causal model created by Schoppek in ACT-R \"overestimates discrimination between old and new states\". The second is Sun [9] who proposed the CLAR-ION architecture. In CLARION's current version, during bottom-up learning, the propositions (premises and actions) are already present in top level (explicit) modules before the learning process starts, and only the links between these nodes emerges from the implicit level (rules). Thus, there is no unsupervised causal learning for the new rules created in CLARION [10]. Various causal learning models have been proposed, such as Gopnik's model [11]. All proposed model use a Bayesian approach for the construction of knowledge. A problem arises, then, for Bayesian networks need experts to assign predefined values to variables. The second problem in Bayesian networks are the risk for combinatory explosion in the case of huge amount of data. In our case, according to the huge amount of data stores in CTS' modules, due to the interaction with learners, we argue that a combination of sequential pattern mining algorithms with association rules is more appropriate. The other advantage of causal learning using association rules is that the system learns in an incremental and real time manner-the system updates its information by interacting with various users. And finally, the aforementioned problem explained by Schoppek, and which occurs with ACT-R, cannot occur when using association rules for causal learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Causal Memory in CTS Architecture", "text": "CTS architecture relies on the functional \"consciousness\" 2 [2] mechanism for much of its operations. It also bears some functional similarities with the physiology of the nervous system. Its modules communicate with one another by contributing information to its Working Memory (WM) through information codelets 3 [12] (see [1] for more details). CTS is equipped with an Emotional Mechanism that is based on the OCC (Ortony, Clore, & and Collins, 1988) emotion model which extends from current models by defining learning in the emotional mechanism as one that helps different types of learning (e.g. Episodic Learning) and differentiating a variety of emotions (Faghihi et al., 2008). It is also equipped with an episodic mechanism that helps astronauts while they manipulate Canadarm2 in the virtual world. CTS' episodic mechanism collaborates with the emotional mechanism for the encoding and remembering of information [13]. In this work, we incorporate in CTS a form of selfsufficient learning -Causal Learning. This takes place through CTS' cognitive cycle (Figure 1) of which we give a brief overview below.\nThe cycle begins with a perception and ends with an action. The collaboration between CTS' Emotional mechanism and Causal Learning can be very briefly described in the following manner: CTS' WM is monitored by various types of codelets, one of which are expectation codelets (see (Faghihi et al., 2008) for more details). If expectation codelets observe information coming in WM confirming that the behavior's expected result failed, then the failure brings CTS' emotional and attention mechanisms back to that information. First, emotional codelets observing WM send a portion of emotional valences that is sufficient to get CTS' attention to select the failed information and bring it back to its consciousness. Emotional codelets' influence remains present throughout the following cognitive cycles, until CTS finds a solution or has no remedy for the failure.\nHereafter, we briefly summarize each step in CTS' cognitive cycle and in italics, describe the influence of emotions (here EM) and/or of CLM For a visual of the same, please refer to Figure 1.\nStep 1: The first stage of the cognitive cycle is to perceive the environment; that is, to recognize and interpret the stimulus (see [1] for more information).\nStep 2: The percept enters Working Memory (WM). The percept is brought into WM as a network of information codelets that covers the many aspects of the situation (see [1] for more information). In this step, if the received information is considered important or dangerous by EM, there will be a direct reaction from EM which primes an automatic behavior from BN [14].\nCLM: CLM also inspects and fetches WM relevant information. Relevant traces from different memories are automatically retrieved. These will be sequences of events in the form of a list relevant to the new information. The sequences include the current event, its relevant rules and the residual information from previous cognitive cycles in WM. The retrieved traces contain codelet links with other codelets. Each time new information codelets enter WM, the memory traces are updated depending on the new links created between these traces and the new information codelets. Once information enriched CLM sends it back to the WM.\nStep 3: Memories are probed and other unconscious resources contribute. All these resources react to the last few consciousness broadcasts (internal processing may take more than one single cognitive cycle).\nStep 4: Coalitions assemble. In the reasoning phase, coalitions of information are formed or enriched. Attention codelets join specific coalitions and help them compete with other coalitions toward entering \"consciousness\".\nEM: Emotional codelets observe the WM's content, trying to detect and instill energy to codelets believed to require it and attach a corresponding emotional tag. As a result, emotions influence which information comes to consciousness, and modulate what will be explicitly memorized.\nStep 5: The selected coalition is broadcast. The Attention mechanism spots the most energetic coalition in WM and submits it to the \"access consciousness,\" which broadcasts it to the whole system. With this broadcast, any subsystem (appropriate module or team of codelets) that recognizes the information may react to it. CLM: First, CLM retrieves the past frequently reappearing information, ignoring temporal part of them, best matching the current information resident in WM. This occurs by constantly extracting associated rules from the broadcasted information and the list of special events previously consolidated. Then, CLM eliminates the rules that do not meet the temporal ordering of events.\nSteps 6 and 7: Here unconscious behavioral resources (action selection) are recruited. Among the modules that react to broadcasts is the Behavior Network (BN). BN plans actions and, by an emergent selection process, decides upon the most appropriate act to adopt. The selected Behavior then sends away the behavior codelets linked to it. EM: 0020When CTS' BN starts a deliberation, for instance to build a plan, the plan is emotionally evaluated as it is built, the emotions playing a role in the selection of the steps. If the looping concerns the evaluation of a hypothesis, it gives it an emotional evaluation, perhaps from learned lessons from past experiences.", "publication_ref": ["b547", "b547"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "CLM: the extraction of the rules in step 5, may invoke a stream of behaviors related to the current event, with activation passing through the links between them Figure 2.D). At this point CLM wait for the CTS' decision making mechanism and CTS' episodic learning mechanism solution for the ongoing situation) [13]. Then, CLM puts its proposition as a solution to CTS' WM, if decision making and episodic learning mechanisms propositions are not energetic enough to be chosen by CTS attention.", "text": "Step 8: Action execution. Motor codelets stimulate the appropriate muscles or internal processes. EM: Emotions influence the execution, for instance in the speed and the amplitude of the movements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CLM:", "text": "The stream of behaviors activated in the CTS' BN (step 7) may receive inhibitory energies, from CLM, for some of their special behaviors. This means, according to CTS' experiences, CLM may use a shortcut (eliminates some intermediate nodes) between two nodes in behavior Network (BN) to achieve a goal (e.g.,in Figure 2.D two points v and z). In some cases, again according to CTS' experiences, CLM may prevent the execution of unnecessary behaviors in CTS' BN during the execution of a stream of behaviors.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "The Causal Learning Process", "text": "The next subsections give a detailed explanation of the three phases of the Causal Learning mechanism as it is implemented in CTS' architecture.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Memory Consolidation Process", "text": "The causal memory consolidation process, which occurs in the Step 2 of CTS' cognitive cycle, takes place during each of CTS' cognitive cycles. Like the human left prefrontal cortex, CTS' CLM extracts past common events from its past experience, as they were recorded in its different memories. Events are information that CTS receives from Canadarm2 during astronauts' training sessions for arm manipulation [3] (Figure 2.A). A trace of what occurred in the system is recorded in CTS' different memories during consciousness broadcasts [13]. For instance, each event X= (t i ,A i ) in CTS represents what happens during a cognitive cycle. While the timestamp t i of an event indicates the cognitive cycle number, the set of items A i of an event contains an item that represents the coalition of information codelets (see step 4 of CTS' cognitive cycle) that were broadcasted during the cognitive cycle. For example, one partial sequence recorded during our experimentations was < (t=1, c2), (t=2, c4)>. This sequence shows that during cognitive cycle 1, the coalition c2 (that the user forgot to adjust the camera in the simulator, Figure 2.A) was broadcasted, followed by the broadcast of c4 (that the user made an imminent collision in the simulator, Figure 2.A) during cognitive cycle 2.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15", "fig_15"], "table_ref": []}, {"heading": "Learning Extracted Rules", "text": "The second phase of CLM occurs in Step 5 of CTS' cognitive cycle. First, it mines rules from the sequences of events by removing the time for each recorded event during CTS' executions. To do so, the algorithm takes as input the sequence database (sequences of coalitions that were broadcasted for each execution of CTS). It then produces the set of all causal rules contained in the database as output. The algorithm starts by ignoring the temporal information from the sequence database to obtain a transaction database. Once this is done, the algorithm then applies an association rule mining algorithm to discover all the association rules from this transaction database with a minimum support and confidence threshold defined by domain expert. It performs one pass on the original sequence database to eliminate the rules that do not meet the minimum support and confidence according to the temporal ordering of events, within a given time interval. The algorithm thus eliminates the non-causal rules. The set of rules that is kept will become the set of all causal rules [15].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Using Mined Patterns to Improve CTS' Behavior", "text": "The third part of Causal Learning, as explained above, occurs in Step 7 and Step 8 of CTS' cognitive cycle. It consists of improving CTS' behavior by making it reuse previous rules to anticipate the reasons for users' mistakes and to determine how to best help them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Testing Causal Learning in the New CTS", "text": "To validate CTS' CLM, we integrated it into Canadarm2, our simulator designed to train astronauts to manipulate arm (Figure 2.A). Users were invited to perform arm manipulations using Canadarm2. In these experiments, users had to move the arm from one configuration to another in the simulator while avoiding collisions between the arm and the space station. This is a complex task, as the arm has seven joints and the user must (1) choose the best three cameras (from a set of about twelve cameras on the space station) for viewing the environment (since no camera offers a global view of the environment), ( 2) not move the arm too close to the ISS, (3) choose the right joint for the arm movements, and (4) adjust parameters of cameras properly. These experiments sought to validate CTS' ability to find the causes of mistakes made by users. During these experiments, we observed that CTS was able to find the causes and propose appropriate hints to help users. Some experiments are described next.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Users' Learning Situations", "text": "As previously mentioned, a user learns by practicing arm manipulations and receiving hints created initially by an expert and given to the user by CTS. The learner's success (defined as the extent of self-satisfaction in CTS) will be variable, depending on CTS' appropriate application of these hints.\nWe performed more than 250 CTS executions of arm in Canadarm2 including good moves and dangerous moves, such as collisions. During each execution, CTS chose a scenario depending on the situation. After each CTS execution, CLM extracted causal rules and the emotional valence attributed to the given scenario, and used these for future interactions. Our experiments showed that CTS is capable of finding the right causes of problems created by users in different situations. In what follows, two different experiments are detailed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Scenario 1: Approximate problem", "text": "When manipulating Canadaarm2 (arm), it is important for the users to know the exact distance between the arm and ISS at all times. This prevents future collisions or collision risks on ISS. Figure 2.D shows the scenario created by an expert in the CTS' Behavior Network (BN). This scenario is an intervention by CTS to help the user while manipulating the arm to avoid collisions between the arm and ISS. The user weakly estimated the distance between the arm and ISS because1) the user chose to move the wrong joint; 2) the user was tired; 3) the user did not remember his course; 4) the user has never passed through this zone.\nAs we can see in the Figure 2.D, this is a long scenario and each time, to find the cause of mistakes made by the user, CTS may be required to interact for a long period of time (e.g. asking questions, giving hints and demonstrating some examples ) to find the causes and provide appropriate feed-back. The scenario starts when CTS detects that a user has chosen the wrong joint and is moving the arm too close to the ISS. CTS first prompts the following message: Have you ever passed through this zone? 1) If the answer given by the user is yes, CTS asks the user to verify the name of the joint that he has selected. Then, if the user fails to answer correctly, CTS proposes a hint in the form of a demonstration or it stops arm manipulation. In this case, the user needs to revise the course before starting the arm manipulation again; 2) if the user's answer is no, CTS asks him to estimate the distance between the arm and ISS. If the user fails to answer correctly then the next hint from CTS asks the user if s/he is tired or forgot the course about this zone or if s/he needs some help; if the user answers correctly, it means that the user is an expert user and that the situation is not dangerous. Interacting with various users and according to the users' answers, CTS found the following rule 1) 60 % of the time \"user chose the wrong joints user makes the arm pass too close to the ISS\", Figure 2.D (V W); 2) in 35% of the time \"user has never passed through this zone user manipulate near to the ISS\", Figure 2.D (V Z); 3) in 5% of the time \"user is an expert user makes the arm pass too close to the ISS\", Figure 2.D (V X).", "publication_ref": [], "figure_ref": ["fig_15", "fig_15", "fig_15", "fig_15", "fig_15"], "table_ref": []}, {"heading": "Scenario 2", "text": "It is a fact that users must perform Camera adjustments before moving the arm in the virtual world. During our experiments, we noted that this step was frequently forgotten by users, and moreover, users frequently did not realize that they had neglected this step, when asked to reflect back on their potential mistakes during their last performance. This increase collision risk (as depicted in Figure 2.A) in the virtual world. Interacting with various users and according to the users' answers, for this scenario, CTS found the following rule 1) in 60 % of the time \"the user is tired the user performs a camera adjustment error\"; 2) in 30 % of the time \"the user forgot the course the user performs a camera adjustment error\"; 3) in 10 % of the time \"the user lacks motivation the user is inactive\".\nAs mentioned above, after some experience, CTS' CLM is capable of inducing (by jumping from one point to another point in the BN, Figure 2.D) the source of the mistakes made by the users and propose a solution for them in the virtual world. However, given that CTS is a tutor and must interact with the user, jumping from start point to the end of the scenario (Figure 2.D, V Z) causes the elimination of some important steps in the BN. To prevent this, we tagged the important nodes in the BN as not to be eliminated. Thus after some experiments CLM, to go from V Z , obligatory, passed through intermediate nodes such as node Y (Figure 2.D). We call it the CTS' partial procedural learning (Step 8 of CTS' cognitive cycle). ", "publication_ref": [], "figure_ref": ["fig_15", "fig_15", "fig_15", "fig_15"], "table_ref": []}, {"heading": "CTS' Performance after the Implementation of Causal Learning", "text": "We performed a second experiment with CTS' causal learning mechanism, but this time to observe how our association rule algorithm behaves when the number of recorded sequences increases. The experiment was done on a 3.6 GHz Pentium 4 computer running Windows XP, and consisted of performing more than 250 CTS executions for various situations (e.g., scenario 1 and scenario 2). In this situation, CTS conducts a dialogue with the user that includes from four to 20 messages or questions depending on what the user answers and the choices CTS makes. During each trial, we randomly answered the questions asked by CTS, and took various measures during CTS' learning phase. Each recorded sequence contained approximately 30 broadcasts. Figure 3 presents the results of the experiment. For all graphs, the X axis represents the executions from 1 to 250. The Y axis denotes execution times in graph A, and rule counts in graph B-D. The first graph (A) shows the time for mining rules which was generally short (less than 10 s) and after some executions remained low and stabilized at around 4 rules during the last executions. In our context, this performance is very satisfying. However, the performance of the rule mining algorithm could still be improved as we have not yet fully optimized all of its processes and data structures. In particular, in future works we will consider modifying the algorithm to perform incremental mining of rules. The second graph (B) shows number of causal rules found after each CTS execution. This would improve performance, at it would not be necessary to recompute from scratch the set of patterns for each new added sequence. The third graph (C) shows the average number of behaviors executed (nodes in the BN) for each CTS execution without causal learning. It ranges from 4 to 8 behavior broadcasts. The fourth graph (D) depicts, after the implementation of causal learning, the number of rules used by CTS at each execution. Each executed rule means that CTS skipped some unnecessary intermediate steps in the BN. The average number of executed rules for each interaction ranged from 0 to 4 rules. This means that CTS generally used fewer nodes to perform the same task after the implementation of causal learning. ", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Conclusion", "text": "Reasoning is central to cognition. Scientists believe that humans first use heuristics or trial-and-error to solve problems. However, this approach is not useful for much of abstract reasoning, given its reliance of causal knowledge, which also supports our capacity for planning, imagination, inference, etc [4,11]. In order to provide optimal tutoring assistance CTS must likewise be able to properly infer the causes of the users' mistakes in various situations. To our knowledge, researchers in artificial intelligence have up to now limited themselves to Bayesian methods to design causal reasoning and causal learning models for cognitive agents. However, the Bayesian approach is not suitable when agents, such as CTS, face large amounts of data. This study, for the first time, combines sequential pattern mining algorithms and association rules to devise a causal learning model for a cognitive agent based on the sequential and temporal nature of the data stored in the system. Causal knowledge is generated in CTS after 1) the information is broadcasted in the system, 2) a decision made for the ongoing problem, which 3) is reinforced by future experiences while CTS interacts with its environment. The Emotional Learning mechanism is applied through the activation it sends to the information situated in CTS' working memory. This causes specific pieces of information to be chosen by CTS' Attention mechanism. This information, if mined by the causal learning algorithm, will more likely be activated in the future when CTS encounters similar problematic situations. Causal learning also helps partial procedural learning in CTS' Behavior Network (BN). After a certain number of similar experiences, the causal learning algorithm eliminates unnecessary nodes in CTS' BN. Our mechanism could then be considered as an alternative to a Bayesian algorithm. After implementing our Causal Learning Mechanism, we observed that CTS can find the causes of the users' mistakes in the virtual world and thus provide better tutoring assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The design of automatic systems for preventing driving incidents is a current and active domain of research involving both automobile manufacturers and academia. For these applications the analysis of what happens inside and outside a car is one of the most interesting sources of information.\nAs a matter of fact, the joint analysis of on board/off board car context can be used to derive considerations on driver's behavior and then to detect possible dangerous situations (sleep, dangerous lane changes, etc.) or a driving style which does not respect traffic regulation (see [1] and [18]).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "On-board Analysis", "text": "The most significant data that can be extracted from a camera monitoring the driver are the gaze direction, the position of face, frequency of blinking eyes and mouth state. In the state of art several works can be found on face pose Corresponding author. estimation, which is the feature we will focus on for on-board analysis, mainly addressed to Human-Computer Interaction or automotive applications. In [2] a complete and up-to date survey of head pose estimation algorithms is presented. Various approaches are categorized according to the typology of the approaches by pointing out the condition of usage, the assumptions and the obtainable performances. In [3] lips and eyes are located in the image tracked over time exploiting their color characteristic with respect to the skin. Then a 3D model is constructed to determine the gaze angles using constant projection assumptions. In [4] gaze is also estimated by analyzing the motion of some relevant features in the eyes and mouth area. This last method however does not take into account possible illumination changes since it is designed for indoor Human-Computer Interaction applications. The paper by Tu et al. [5] instead relies only on nose tip localization to estimate face pose by using \"tensorposes\" models. Other works try to cope with the difficult environmental conditions of the automotive applications (i.e. frequent and relevant illumination changes) by detecting eyes using infrared cameras (e.g. see [1] and [6]). These approaches are usually more robust and they can operate also with very low illumination but the cost of an infrared camera is much higher than a traditional webcam.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Off-board Analysis", "text": "The focus of this section is to detect the position of the vehicle on the road and the lane changes. Moreover, the analysis of the road type (highway, urban road, etc.) and of traffic is performed to provide relevant information to evaluate the possible risks of the driving behavior. In the literature, several works can be found addressing the problem of lane detection and tracking, however we will concentrate on lane detection using video sensors since they perform well in several situations. In [7], a survey of lane detection algorithms is proposed where the key element of these algorithms are outlined. A 360 degrees single PAL camera-based system is presented in [8], where authors provide both the driver's face pose and eye status and the driver's viewing scene basing on a machine learning algorithm for object tracking. A widely used technique to post-process of the output of the road marking extraction is the Hough transform as shown for example in [9]. In [10], a generic method for a probabilistic identification of driving situations and maneuvers was introduced basing on a Bayesian network and fuzzy features as input parameters Such framework allows to identify emergency braking situations and lane changes with a good accuracy. In the paper by Wang et al. [11], a road detection and tracking method based on a condensation particle filter for real-time video-based navigation applications is presented. Worth of note is that most of these works are tested on highways where the background is usually less variable and complex with respect to urban roads.\nThe rest of the paper is organized as follows. In Section 2 the system architecture is briefly presented. In Section 3 on-board analysis approach is described as well as off-board analysis in Section 4 with some preliminary results. Finally in Section 5 conclusions are drawn and future developments are discussed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System Architecture", "text": "The proposed architecture is composed by two general purpose webcams connected to a laptop computer installed inside the car. The PC has an internet connection through a UMTS card in order to make available context information as well as movies registered on the vehicle on a remote server.\nA schematic representation of system physical and logical architectures is presented in Fig. 1 and 2. Modules 1 (TLC1) and 2 (TLC2) are the thread capture images from webcam using 2. The acquired images are sent to recorders that generate synchronized movies exploiting TIMER module which is a timer thread that sends to recorders signals to concerning start / end of the movie.\nTherefore, data acquisition and processing is carried out by equipping a car with a general purpose laptop linked with two webcams appropriately positioned to frame the exterior of car and the driver. Finally, a GUI has been developed for the acquisition and storage of various real-time synchronized data. The acquisition module allows to receive up to a maximum of 4 video streams at 25 frames/sec, also obtained by different cameras, and to timely synchronize data in order to consistently associate the information processed by each sensor.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "On-board Context Analysis", "text": "In this work, a tracking based approach for driver gaze detection is proposed to the aim of obtaining a better ratio between accuracy and speed. Such method relies on the evaluation of the relative movement of the head between consecutive frames of a video sequence [17]. In the proposed method three processing step have been considered: (a) face detection (face, eyes, mouth and nose), (b) face tracking and (c) face analysis and angle of view calculation. Firstly, an initialization step is performed for face detection. Secondly, the tracking algorithm enables localizing the position of the face in the video frame and evaluating the relative position of every facial trait like the nose, the mouth and the eyes. Lastly, when face position estimation has been performed, for each video frame the pose of the face can be evaluated in order to extract the angle of view and other relevant information. In the next subsections detection, a more detailed description of the previously presented steps is provided.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Face Detection", "text": "For each trait (face, eyes, mouth, nose) Viola-Jones detector [15] is applied. This algorithm was broadly used in the last years for a lot of detection applications, in particular to the aim of localizing faces. Three major phases are performed: feature extraction, classification using boosting and multi-scale detection, enabling a fast and highly accurate detection. In such a method five detectors need to be used for identifying each face component leading to a computationally expensive approach that can hardly comply with required fast processing time. To reduce the computational effort, direct bottom-up detection has been substituted by a top-down geometrically constrained initialization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Face Tracking", "text": "Face tracking cannot be consistently performed using a single detector because all detectors are sensible to rotations. According to this statement, for each face trait an instance of Kanade-Lucas-Tomasi (KLT) feature tracker algorithm [16] has been used. Feature selection has been specifically studied in order to maximize the quality of tracking, and it is therefore optimal by construction with respect to ad-hoc measures based on textures. Moreover, processing phase is computationally inexpensive and helps discriminating between good and bad features based on a measure of dissimilarity related to affine motion depending on the change of the underlying image model. This choice allows obtaining a good precision in terms of estimate of eyes position and, at the same time, ensures to keep eyes position information when great neck twists occur. Finally, KLT features allow extracting additional information on face rotation and shape changes that can be used to evaluate driver's pose.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Face Analysis", "text": "View angle is one of the most important information that is needed to assess to driver state. It can be disassembled in yaw (rotation with respect to horizontal plane), roll (longitudinal rotation related to movement) and pitch (vertical rotation) angles. In this paper, a rough but fast estimation of outlook direction is proposed. To this end, gaze angle has been subdivided in a set of n discrete possible values. To estimate yaw angle, KLT features position, the displacement of eyes during tracking phase and the triangle created by the two eyes and the nose are used. A first value of the yaw angle can be calculated form KLT feature as follows:\n= var(R x ) var(L x )(1)\nwhere var(R x ) is the variance of right eye on x axis and var(L x ) is the variance of left eye on x axis. It is important to point out that the relative movement of eyes' position during the tracking step together with the studies of the relative position between eyes and nose lead to an improvement of yaw angle estimation.\nIf information related to nose position is not available or cannot be properly acquired, the triangle between the nose and the eyes cannot be created so only the information provided by KLT feature and eyes moving are used. In the latter case, the yaw angle evaluation could be affected; however preliminary results show that a good estimation can be obtained. On the other hand, roll angle evaluation has been performed basing on two features: (a) the relative position of the centers of the eyes with respect to the centre of the face and (b) the difference between the y coordinate values of each eye. As a general rule, we have assumed (having been demonstrated in a large testing phase) that values of the yaw angle near to 0 correspond to the situation of driver looking straight ahead (i.e. driver is looking at the street and his/her level of attention is adequate) while values far from 0 correspond to the case of driver looking in other directions rather than street one (i.e. a possible dangerous situation can happen because the driver is absent-minded).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary Results", "text": "A lot of experiments have been performed using a simple webcam at 320x240 of resolution. The webcam has been installed on a car and it has been used to analyze a driver during a thirty minutes drive. In the proposed experiments (Fig. 3) the red points indicate the eyes, yellow and green points indicate respectively the nose and the mouth. The purple line on the top of the face indicates the roll angle while yellow/blue one shows the direction of the yaw angle. Table 1 shows the experimental result obtained by the on-board video analysis. The percentage of frame with errors is obtained comparing algorithm results with observations. A more significant percentage of errors occur in detection and tracking phases rather than in angle view calculation.  This output can be justified taking into account that the continuous change of luminosity, which usually affects driving situations, either forces the proposed method to re-initialize the detection algorithm or leads to unexpected errors in face tracking.", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": ["tab_2"]}, {"heading": "Off-board Context Analysis", "text": "The purpose of the algorithms for off-board context analysis is to extract information about what is happening outside the vehicle. In particular, in order to evaluate the level of safety related to driver's behavior, the main features which will be considered are:\nnumber and position of roadways; -position of the vehicle on the road with respect to traffic lines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Method Implementation", "text": "The following steps have been applied to extract road context information from a video sequence:\n1. edges extraction from each frame using the Canny operator [12] (Fig. 4(a)).\nThe Canny operator is a very well known method in image processing to identify in the image the connected areas characterized by significant luminosity intensity difference with respect to neighbor pixels by computing the gradient of the image intensity value; 2. after extracting the edges, the resulting binary image is used to detect the lines using Hough algorithm [13] [14] (Fig. ) detected lines must be post-processed in order to select only those that are candidate to identify the lanes. To this end, under the assumptions that the camera acquires a frontal view of the road, the following steps are performed:\n(\n3. the two lines that belong to the lane where the vehicle is driving on are located. The closest line on the left with respect to the center of the image and the one on the right are considered as the first reference lines and they are identified respectively as l 0 and l 1 ; 4. once extracted the two lines, attention is focused on an area within the triangle formed by them. A frame per frame statistical analysis of the pixels belonging to the road is performed to create a model of the road. In particular, the color model of the road is evaluated to select the areas where to look for the other candidate lines. A single Gaussian probabilistic model N (\u03bc, ) of the color of the road is learned computing the sample mean and sample variance on the three color channels red, blue and green, where \u03bc is the mean and is the covariance matrix; 5. all pixels in the image below the point of intersection between the two lines identified at step 3. are considered and each pixel is compared with the Gaussian model of the road looking for those that are more similar to the model. An example can be seen in Fig. 4(c)); 6. the next step is to evaluate whether the road has one or two lanes and which is the position of the vehicle with respect to them. Considering all the lines extracted in step 3., one on the left l 0L and one on the right l 1R that are sufficiently far respectively from l 0 and l 1 , are considered. The fraction of road pixels with respect to all the ones within the two candidate lanes identified by (l 0 ; l 0L ) and (l 1 ; l 1R ) is computed. If this fraction is greater than a predefined value (e.g. 70%) and if the intersection point between l 0 and l 0L (or l 1 and l 1R ) is close enough to the intersection point between l 0 and l 1 1, than the candidate lane is considered as a valid one. An example of the final output of the lane detection procedure is shown in Fig. 4(d). ", "publication_ref": [], "figure_ref": ["fig_29", "fig_29", "fig_29"], "table_ref": []}, {"heading": "Preliminary Results", "text": "In order to test the proposed approach a webcam positioned on the vehicle dashboard has been used to acquire video sequence of real-world guiding scenes both in highways and urban road scenarios. During the tests, different contextual information has been considered:\n1. Number of roadways; 2. Vehicle position.\nSince both the number of roadways and the position of the vehicle does not instantly change algorithm output obtained for each frame of the video sequences have been averaged over a longer temporal window. More in detail, the number of lanes has been evaluated over a window of 2 seconds while for the analysis of the position of the vehicle a 1 second window has been used. In Figure 5 the GUI used for receiving and showing contextual information is presented while in Table 2 statistical data concerning vehicle's behavior are shown.  ", "publication_ref": [], "figure_ref": ["fig_116"], "table_ref": ["tab_5"]}, {"heading": "Conclusions and Future Work", "text": "In this work, a framework for analyzing driver's attention, detecting lanes and individuating vehicle position is proposed. This information can be relevant to design Intelligent Vehicles able to understand driver behavior and intent for preventive safety. Proposed methods are able to cope with typical difficulties present in this scenario such as illumination changes and dynamic background. Experiments performed on real world video sequences taken on-board cameras demonstrate the robustness of the presented framework and the capability to operate in a real-time fashion. From the one hand, future work will be focused on calculation of other parameters for identifying driver state, like evaluation pitch component of angle of view, analysis of blinking frequency of the eyes and a value of mouth state (open, close, speaking). To the other hand, traffic analysis and evaluation of other vehicles state and behavior will be explored to empower off-board contextual analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The Prague Declaration [1], adopted during the eHealth European Ministerial Conference celebrated in Prague in February 2009, shows the relevance that eHealth has nowadays and in the future. This declaration presents the different stakeholders involved in the development of eHealth, and it highlights as well that \"the lack of interoperability has been identified as one of the main areas to address\". The main target group of telecare and telemedicine (in this approach) are people chronically ill, elderly or handicapped. Telecare utilizes information and communication technologies to transfer medical information for diagnosis and therapy of patients in their place of domicile while telemedicine is related to the delivery of clinical care at distance [2], for example a teletransmission of ECG (electrocardiograph).\nTelecare services can significantly increase the quality of life for this group of people. However, there is still a lack in standardization that would allow to connect and to maintain the equipment provided from different vendors in a compatible and reliable way. Beyond the pure technical aspect, the incorporation of persons forming part of daily life is a crucial point.\nFigure 1 shows a scenario that presents all stakeholders incorporated in our work. This includes the patient, relatives of the patient, people in charge of monitoring and, of course, the provider of the infrastructure (communication and devices) for the service. This service is composed of a health data transmission service and a videoconference service while the infrastructure at the patient's home is centered in the Residential Gateway (RGW). A RGW [3] is a small embedded computer running a software platform to manage several services at home, like entertainment applications, videosurveillance, etc. These services are integrated into an OSGi [4] platform that provides also support for remote control and maintenance.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Fig. 1. Overview of the eHealth scenario", "text": "In the test scenario implementation, a patient's health data monitoring service is controlled by the RGW and the eHealth equipment is connected via Bluetooth. Any medical information is forwarded to the eHealth Service Provider using HL7 messages. During an on-line medical citation between for example a nurse and the patient, a video call is established. This functionality is based on the UPnP AV standard (Universal Plug & Play for Audio-Video) because it provides a modular framework for multimedia communications and many end-user devices are supporting this standard.\nThe next sections are organized in the following way: In Section 2, the state of the art of telecare is reviewed. Our proposal is presented in Section 3, describing the platform architecture in more detail. Section 4 presents the application, experiments developed and results obtained. Finally, the last section concludes the paper and gives some future outlook.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "State of the Art", "text": "This section presents the work and research lines related to telemedicine and similar developments followed by a brief introduction to the technologies and standards used in the system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Research on Telecare and Telemedicine", "text": "Home telecare is foreseen as an important factor for future medical assistance [5], [6]. Some telemedicine and telecare approaches are based already on OSGi [7], [8], [9], [10] but they do not offer a complete integration of all services provided in our approach.\nFor example, the Seguitel system [11] is a social and telecare service platform based on OSGi. It is oriented to provide services designed under a methodology that ensures a SLA (Service Level Agreement) but this approach introduces several middleware layers and it is not covering healthcare standard interoperability. Other projects that work in similar environments like HEALTHMATE [12] (Personal intelligent health mobile systems for Telecare and Teleconsultation), TELECARE [13] (A multi-agent tele-supervision system for elderly care) or PIPS [14] (Personalized Information Platform for Life and Health Services) have similar lacks as Seguitel.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Technologies", "text": "This section provides an overview of the technologies used to achieve the objectives outlined in the introduction. These technologies are mainly the health informatics standards, the OSGi framework, the UPnP Audio Visual standard and the use of Bluetooth for communications with medical devices. They will be briefly described next.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Health Informatics Standards", "text": "Home telecare requires that patient data must be transmitted following messaging standard. Currently, HL7 [15], [16] is a widely applied protocol to exchange clinical data. Moreover, there are open source tools available to process and transmit HL7 messages [17], [18].\nFurthermore, there is a standard under development, the ISO/IEEE 11073 (also known as x73) standard [19], to transmit medical information among devices, but there are hardly no medical devices yet in the market supporting the standard. Many available devices follow proprietary protocols, so it is not possible to interact with other devices or platforms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "OSGi Framework", "text": "The OSGi [4] framework is a Java-based open architecture for network delivery of managed services. Services are added through software components (bundles). The platform carries out a complete management of bundles' life cycle: install, remove, start, stop and update. The bundles are Java applications running on the same JVM (Java Virtual Machine) that can share code.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "UPnP AV Standard", "text": "The videoconference system allows the communication between the patient and any other member of his group. For example, an assistant or medical personal as well as his relatives can be members of the group. The videoconference functionality needs a multimedia device infrastructure managed by the RGW. The UPnP AV [20] is a standardized UPnP architecture for multimedia systems in home networks. It is a widely spread standard used in multimedia home networks. It allows an automatic discovery of multimedia services with a low CPU usage for a streaming negotiation and management. Additionally, there are open source libraries of the standard available. Other approaches are based on SIP and IMS [21] but UPnP devices are more widely spread in the market.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bluetooth", "text": "The Bluetooth wireless protocol [22] is a short-range communications technology intended to replace wires connecting fixed or mobile devices. The Bluetooth specification supports secure and low power communication for a wide range of devices to connect and transmit information with each other. There are low-cost Bluetooth adapters available in the market as well as medical measurement devices like the UA-767PBT Blood Pressure Monitor from A&D Medical. Thanks to Bluecove, an open-source library that provides a JSR-82 Java interface for Bluetooth Profiles, it is possible to implement OSGi bundles that communicate with Bluetooth devices available for many operating systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "eHealth System Architecture", "text": "This section starts with the description of some scenarios covered by the developed system. Finally, an overview of the architecture is presented followed by the presentation of the main elements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Telecare Scenarios", "text": "Previous telecare proposals are often organized in a way not taking into account the communication with the relatives and friends of a patient. But according to several studies 23], elderly or dependent people are reluctant to use telecare services because they do not personally know the operator or like to contact a person in the telecare service centre. Usability can be increased when incorporating relatives and friends into the flow. In a possible scenario, a doctor initiates a video call with the patient to remotely check some data about the heart health, like the blood-pressure, heart rate or the weight. The RGW keeps an address list of relatives and friends; so, the patient can communicate with them if he needs to contact an emergency service in a serious situation. Moreover, relatives and friends can check medical remainders to help the patient during the treatment.\nIn summary, the some of the scenarios covered by the telecare service are: \u2022 An elderly man has a medical citation with the doctor to review his heart health.\n\u2022 An elderly woman that lives alone receives a video call from an assistant or a relative to take care about her. \u2022 The system warns the patient when he has to be prepared for a planned videoconference or when it is time to take a medicine.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System Overview", "text": "Telecare supports the integration of patient-oriented services, like medical data transmission, audio/video calls or healthcare appointment management. Our proposal tries to provide an interoperable and scalable solution. The system is divided in four basic subsystems: Medical, eHealth, Data and Multimedia. These elements are managed by a RGW running with Linux and an OSGi framework hosting different services which can be managed remotely by the telecare or access provider. An architecture schema of the four subsystems is shown in Figure 2. The Medical subsystem can include a wide variety of devices and protocols. In this approach we have integrated Bluetooth devices to take some measures from the patient. Moreover, it carries out the transmission in HL7 messages. The eHealth subsystem manages the patient's appointments and medical treatments and implements a graphical user interface (GUI). The Data subsystem includes an SQL Database to save the whole information about patient health data, home sensors data, etc. Finally, the multimedia subsystem establishes the communication between doctors, patients and relatives by means of monitors and webcams.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "eHealth Subsystem", "text": "The eHealth subsystem in the patient's RGW is composed of a set of bundles that includes a graphical user interface, an eHealth bundle and an appointment manager. The Patient GUI is a Swing-based application adapted to the patient and incorporates the eHealth bundle services. This bundle is included also in the doctor and system administrator because these applications share some features. Using the patient's GUI, is possible to access to a simple patient's Electronic Health Record (EHR), to look up treatments, medical appointments and remainders.\nThe patient can launch off-line or on-line medical appointments. In the first case, the patient follows a wizard, for example introducing weight and blood pressure, and waits for the results shown on the display. Health data are sent by the HL7 Driver, so the doctor or nurse can check it later. In an on-line medical appointment, the patient communicates with the doctor or nurse through a video call.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Medical Subsystem", "text": "The Medical subsystem includes a patient healthcare data access module, in our approach; this is a Bluetooth driver, as well as a Measure Notifier and a HL7 driver. The Bluetooth Driver parses the messages sent by the personal scale or blood pressure monitor and the Measure Notifier alerts the eHealth Bundle immediately to take a new measure. This procedure is needed because it could take two minutes for the patient to make the measurement and the Bluetooth Driver receives the data. Measures to be collected, the frequency and their relevance are predetermined by the doctor and the devices available and is reflected in the database of the system.\nAfter a data recovery, the HL7 Driver bundle transmits an ORU-RO1 observation message in HL7 format [24]. The eHealth Server Provider receives and processes the message by a HL7 engine. The HL7 Driver also takes care of sending an ADT-A05 (pre-admit a patient) from the eHealth Server Provider to the RGW when the patient starts to use the telecare system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multimedia Subsystem", "text": "In a home telecare scenario, a multimedia infrastructure is required to allow a seamless communication between healthcare actors. As presented in the next section, a multimedia real-time streaming is established between the assistant, the patient and relatives to provide an Audio-Video (AV) call. This infrastructure should be flexible enough to allow several multimedia devices to be connected.\nFigure 3 shows the internal design of the Multimedia Subsystem. The AV subsystem handles the AV communication according to the UPnP AV specification. The UPnP Control Point is implemented as an generic UPnP Control Point. The Event", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Fig. 3. Multimedia Subsystem Design", "text": "Manager handles the events coming from home devices. The Device/Service Inventory is a repository of services that the UPnP Control Point can add and remove. The Importer, imports UPnP services and exports OSGi Service while the Exporter performs the opposite operation: a module that imports OSGi Services and exports UPnP services. Finally, a Generic Device merges and publishes the platform functionality as UPnP services.\nOur approach is based in well known multimedia and network standards: UPnP, HTTP (HyperText Transfer Protocol), MPEG-2 and SIP (Session Initiation Protocol). The SIP protocol allows connecting remote devices in dynamic environments like home access networks because the IP address usually is received dynamically and the link status is variable.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Currently, we have developed and tested a prototype with the functionality described above. First experiments are being implemented in a laboratory with two local networks simulating the communication between two environments, like the patient's home and the ambulance office.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 4. Graphical interface of the patient application", "text": "The simulated RGW is running in an embedded computer with limited resources using an Intel Pentium Celeron CPU, 512 MB memory and Debian Linux. Apache Felix [25] a OSGi R4 Service Platform compliant implementation released under an open source license is chosen as the RGW software platform running over a Java Virtual Machine provided by Sun JDK 1.6. The AV UPnP software is implemented based on a branched version of Cybergarage [26] Java libraries. A simple USB webcam with a microphone incorporated is used to acquire multimedia data. The Medical Information System is simulated by a desktop computer running Mirth Engine application and Apache Felix with the same bundles than in the RGW except the Doctor GUI. HAPI open source libraries are used to implement the HL7 Driver following the HL7 version 2.6 standard. The MySQL database is chosen because its simplicity and robustness; it is managed by the Data subsystem in the RGW and the eHealth Server Provider.\nDifferent graphical applications for the patient and the health professionals have been implemented. They include a basic functionality like citation management, health data transmission and video calls. Figure 4 shows the interface of the application developed for the patient. The first image shows the health data measure wizard for an off-line appointment. The second one shows the patient application during an on-line appointment. To the left there is a simplified Electronic Health Record with a medication list. The video and a contact list are shown in the centre and right hand side. The latest medical appointments are shown at the bottom.", "publication_ref": [], "figure_ref": ["fig_29"], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "An eHealth system for a complete home assistance has been presented. Our approach is based on well-known standards, incorporating a personal health data transmission service supporting video calls. It supports health data interoperability also incorporating medical devices located on the patient's site and open to incorporate future devices.\nThe patient equipment includes a RGW based on open source software and some medical monitor devices. Graphical interface applications for the telecare actors have been implemented including basic functionality as well as a healthcare information system with a medical appointment management and videoconference system to support on-line medical appointments. The videoconference system establishes the communication between patient, doctor and his relatives using cheap end consumer devices.\nFuture work is directed in two ways. The first one works in the line of increasing the functionality incorporating home sensors and implementing an inference engine to generate medical alerts based on the patient health data. The second line of actuation has relation to the usability of the application. Although the main functionality is working, it is needed to test it with real patients in order to provide a nearest solution to its problems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Just 30 minutes of moderate activity five days a week, can improve your health according to the Centers for Disease Control and Prevention 1 . By enabling activity monitoring at individual-scale, over extended period of time in a ubiquitous way, physical and psychological health and fitness could be improved. Furthermore, communication among relatives, friends or professionals could be enriched, showing graphics of weekly activity (very interesting for sportsman or elderly's relatives).\nCurrent remote health monitoring applications are in an early commercial stage 2,3,4 where application programmers, along with medical experts, are trying to analyze diverse parameters for providing wireless automated health care. In such a way, these systems need to transmit data to the backend very often, either by doctors' analysis, or due to computational intensive diagnosis algorithms that can't be executed efficiently on an embedded processor in a wearable device.\nCommercial approaches use specific hardware, but we thought that modern mobile phones can achieve the same goals. However, high rates of physiological data have an adverse impact on the phone usability, not only due to expensive long-range communication, but also due to the costly data recovery and battery life.\nWith this paper, we hope to develop a dynamic, efficient and reliable system to control the user's monitored activities that have been practiced. Furthermore, the monitoring proposition must be as least intrusive as possible, since users, generally, are adverse to be controlled through traditional surveillance systems like surveillance cameras or sensors for all around their houses. A problem of these systems is the restricted sphere of action.\nThe system described above, tries to carry out the person's monitoring wherever he goes. This can be carried out thanks to that current mobile devices which incorporate accelerometry sensors, that means the user can carry the device at all times. This does that the range of our application will not be only the subject's house, work or training place, but it could be also controlled wherever he goes.\nOur aim is to register every movement practiced by the user and classify it in different activities such as, for example, walking, running, jumping, going up and down stairs or even falls. Once having done that, the result of the classification will be visible by means of a web portal to user's family, doctors and anybody that the system administrator thinks it is necessary.\nFurthermore, user's monitoring must carry out without its will be a too heavy load for owner subject. As we will see later, some existing devices allow make an user's monitoring, but the main problem is that only are available through proprietary hardware. Another problem happens as well, the user must wear an additional device; therefore, this can become uncomfortable and increase the risk of being forgotten by the user.\nThe opposite of above is that our system, on the user's side, expects to be integrated in the user's mobile device monitoring. The advantage of this decision it is just that mobiles are part of user's life style (every day more and more), and thus, the risk regarding loss or oversight is much lower than with an additional device. In addition to this, the boost and versatility of these devices make possible that the system possibilities can be increased.\nThe mobile devices connectivity is huge nowadays. In addition to the Wi-Fi technology that is more and more integrated in devices, the communication companies are done a strong vouch for 3G connectivity in these terminals. This technology allows us to connect users to Internet wherever they are. Thus, the access to Internet will be available any place and any time. Based on it, we can talk about developing an ubiquitous system that allows us to know subject's diary activity by means of information flowing through Internet.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "The estimated physical activity, according to data obtained which are based on accelerometer, is, at the present, a research topic. There are some systems that are able to carry out this task. Among all of them, we will detail the most important and we will examine it to observe the difference and resemblance in accordance with our proposal.\nThe first difference we can observe among the developed systems up to now is the situation of the device and the device used for collecting information. There are systems that use proprietary hardware ([1], [2] and [9]), while others use general purpose hardware ( [3], [4] and [10]), as the one we are describing. Obviously, to use generic hardware will result in a benefit, because the price and versatility of these devices are tricks to his advantage. However, these devices have a drawback: the limitations on the data collection quality. The accelerometers integrated on generic devices, such as the last generation of devices, have, overall, less quality in the data collection than other accelerometers that are integrated on specific devices. In this way, investigations that use specific hardware must focus on solving this trouble.\nOther difference that we can find between the several researches is its own aim. In [5] we can see that the accelerometry sensor is located in a glove, which must wear the user and it is able to recognize several activities based on the hand movement. However, in other researches like [3], the sensor is situated in the user's pocket. Having arrived at this point, we can wonder what the most efficient method is. If we based on results, [5] and [8] has more precision than [3] and, furthermore, it is able to recognize larger number of activities. However, this solution could be uncomfortable for the user, thus the chosen system must be selected depending on the needed required.\nIn addition to the previous researches, other proposal for people with surveillance exists [7] that is not only based on accelerometry, but it also adds other elements like, for example, surveillance video cameras.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System Design", "text": "The remote surveillance system will be developed in three different environments: user's terminal, sever and remote query terminal. In the user's terminal will take place the data collection from the accelerometry sensor, data filtered a priori, classification of the user's activity and filtered a posteriori. The server environment will provide query interface for activities that the user is carrying out, communication service with the user's terminal and services needed for the communication with terminal query.\nFinally, in query terminal will be executed the information query software. In design of system is contemplated that the software will be of two kinds: web interface or desktop interface. Web interface will be accessible to any user who desire look up the user's activity from any web browser by means of the application server. Desktop interface (both for Smartphone and for PC) will be communicated with the server through the web services offered by the own server.\nIn the next sections we will explain in more detail in what does consist the monitoring process and the activities control process. But before that, we will show a general view of the system. Figure [1] shows a flowchart of the system framework: ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Obtaining and Filtering Data", "text": "Following properties are desired for our activity recognizer installed on user device: manageable device, built-in GPS device, accelerometer sensor and its consumption should be enough for recharge the batteries each 24 hours at least.\nAdvances in microelectronics have reduced the cost of small and accurate sensors. As a result, accelerometers are embedded in modern mobile phones, the most ubiquitous device nowadays. Tri-axial accelerometers can measure G-force every axis and some features could be extracted to recognize the current user's activity.\nOther profit of the activity monitoring system based on accelerometry is the low energy consumption produced in the device. This is essential when we talk about developing applications for mobile devices, because the bottleneck of these devices is the batteries. Furthermore, the user does not need recharge it constantly. Thus, the use of the system will be more comfortable for the user.\nSpecifically, we use the Samsung Omnia for our data collection, which contains a three axis accelerometer with a sensitivity of +-2G and a resolution of 0.004g. Samsung device lasts more than 2 days with continuous use of this accelerometer, so the energy consumption will not be a problem and the user will not be constantly concerned by recharging his/her device.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Query", "text": "User terminal Server", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Q ery", "text": "The fact of having chosen the Omnia device for testing the system is because it integrates all the requirements described earlier: accelerometer, GPS device, operating system and all the facilities to work directly with all the data sensor of accelerometry. Furthermore, it is a current and, relatively, inexpensive device. This ensures us that the system is available for anyone without this meaning, a higher technology spending.\nAlthough the data obtained are completely valid, it will be necessary to carry out a data filtering before making the classification. The filtering aim is to remove all the signal noise. In most of the cases, the noise of the accelerometry sensor is negligible, but exists a kind of noise that can affect seriously to the activity classification. This noise is produced by vibrations that take place on device when it is carried by the user.\nWe should consider an important aspect of our system: there is no restriction when it comes to choose the place where we must wear the device. This restriction is demanded by any related works. However, in our system, the user can wear the device where he wants. This increases the complexity of the development, because the mobile device subjection couldn't be peak condition. A bad subjection will produce unintended vibrations when the user carries out the activity. This is the reason of executing a filtering, in order to obtain valid data.\nThroughout all the movement detection process, it has worked with the module of accelerometry. Usually, most accelerometer tends to measure accelerations that take place at three axis of device (triaxial accelerometer). Figure [2] shows a graphic witch the three axis in a generic mobile device. \n| | \u2022 \u2022\nwhere , are values of the acceleration in a certain time instant in the axis x, y, z respectively.\nTo develop recognition of user activity accelerometry readings have been divided into temporal windows [Figure 3] then, analysis of a time window will result in a possible activity that the user is taking place during the time period that covers the window. Cooley-Tukey algorithm was used to reduce the execution time ( \u2022 log . Then, Butterworth low pass filter was applied to eliminate noise caused by device vibrations. Furthermore, the frequency domain signal treatment allows us new readings, using interpolation, a very useful tool when the frequency is as low as our case. The latency of this transformation and filtering is around 2 seconds (in a separate thread) so our recognition activity system has a 5 seconds delay.", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Classification of the User's Activity", "text": "Since the entire process of classification (and training) will be made on the device itself, it is essential to minimize the computational cost. In addition, the design objective recognition of activity patterns is that all logic is done in the terminal. Since the learning until the classification, it should be based on stable pillars and the least heavy possible. This means that the recognition process must be, computationally, the least cost possible and as accurate as possible.\nAlthough there are various techniques that can produce a more accurate of classification, are computationally much more expensive and further learning is, in most cases, unthinkable carry out in a mobile terminal.\nThe learning and classification method chosen is based on a probability table, and a second level, governed by a dynamic Markov chain which gives more control to the system. Prior to the detection process five vectors formed by six values must be identified.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 2. The rank vectors of the statistical variable \u03b2 is defined as one that sets up between its positions a range of values which can group readings values taken from a time window.", "text": "These vectors pretend to define a range for each of the statistical measures presented below:\n-Arithmetic mean: -\u221e, 0.8, 0.9, 1.1, 1.2 and +\u221e -Range: -\u221e, 0.24, 0.6, 0.9, 1.2 and +\u221e -Variance: -\u221e, 0.09, 0.15, 0.25, 0.35 and +\u221e -Coefficient of Variation: -\u221e, 10, 25, 38, 45 and +\u221e -Minimum: -\u221e, 0.3, 0.5, 0.7, 0.9 and +\u221e That is, the vector corresponding to the arithmetic mean is composed of the above values that determine five ranges: [-\u221e, 0.8), [0.8, 0.9), [0.9, 1.1), [1.1, 1.2), [1.2, +\u221e).\nIn learning, the user must perform each of the activities that can recognize the system. Unlike other works, the number and type of activities are not determined a priori, but the system administrator or users determine the activities that can be recognized. When performing the learning of a particular activity, each vectors corresponding to each of the statistical measure stores the number of temporal windows whose statistical measure is included in that range as well as the activity that is being developed. Thus, after completing the learning process, we have a series of vectors that contain at each position the number of temporal windows of each activity that have been detected in a certain range.\nFor determinate the activity that the user is carry out, the statistical values described above have been calculated based on the | | content in a given time window. After the expiry of the time corresponding to the time window, we have five values we use to determine the statistical activity that has taken place by the user during that time period. The next step is to visit the array of frequencies which are generated during the learning process. There will be a matrix for each activity and shall consist of N tables (where N is the number of activities that the system can recognize) composed by five rows (one for each statistical measure) and five columns (one for each range defined by the ranks vectors). Definition 3. The matrix of frequencies associated with \u2202 activity is defined as that matrix which reflected in the position [i, j] the number of readings that have been collected in the learning process of the activity \u2202 of the statistical variable i in range j. J As the range defined in the vectors of range. After each process of learning, the values that made up the matrix are normalized to avoid dependence on the training time of each activity.\nHaving analyzed the current time window, we resort to the matrix learning \u2202 of each activity. We will make a summation of those positions in the matrix in which the range of values of the statistical variable of row i, coincides with the value obtained from the time window. With this we get a number denoted as \u03a9 (\u2202).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 4.", "text": "The Contest Sum, denoted by \u03a9 (\u2202), is defined as the contents sum of the matrix learning activity \u2202 positions where the value obtained in the analysis of the time window t for the statistical measurement i coincides with the range defined in the standard position j. Therefore, at this point, the probability of such activity is known based on reading conducted by accelerometry sensor and the study of the time window generated. It is also important bear in mind the reduced runtime of algorithm of detection. The algorithm used has linear complexity, so that its mobile terminal development is possible and would not entail excessive computational cost. To clarify the process of classification of the user's activity, an example will be made below. First, the system collects accelerometry data every 200 milliseconds, although this frequency can be adjusted on the system configuration to allow lower frequency reading if the device support it. Every time the system get a data, its magnitude is calculated as it was detailed in Definition 1.This module will be stored in an array created for that purpose. This will happen during the time set for the time window, so after this period the vector of modules will be filled by all the readings taken. Then, from the stored data, statistical values listed in Definition 2 are calculated, i.e., Arithmetic Mean, Range, Variance, Coefficient of Variation and Minimum. For the present example, suppose that the values for these fields are:\n-Arithmetic Mean: 0.87 -Range: 0.7 -Variance: 0.21 -Coefficient of Variation: 35 -Minimum: 0.2 Now it's time to compare these values with the matrix described in Definition 3. To continue the previous example, assume that the system is able to recognize three activities: running, walking and jumping. In this way, our cube matrix shall consist of three two-dimensional arrays (one for each activity can recognize) 5x5 size (number of intervals for each statistical variable calculated and number of statistical variables respectively). Three-dimensional matrices resulting from the completion of the learning process are shown below: Once presented the learning matrix, based on Definition 4, the sum of learning cases for each of the activities that the system is able to recognize based on data obtained after analyzing the current time window are obtained. In the previous matrix, values in bold correspond to those intervals in which the group of values of the time window analyzed are contained. Based on the values obtained, the sums \u03a9 (\u2202) of each of the following activities will be: \nRunning Interval [1] Interval [2] Interval [3] Interval [", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Classify Filter", "text": "Most probable activity was recognized using accelerometer data, but the result of this classification suffers from noise and transitions between activities generate abrupt oscillations. A Markov chain will be introduced to control in a higher level the final classification of most probable activity.\nThe selection of a Markov chain to model the user's activity is based on the fact that if a user is doing an activity, there will be other related activities that he/she will do with a higher probability. For example, when a user is sitting, still sitting will be more probable than change to jumping activity.\nMarkov chain states will be the different activities that system could recognize. Given state , probability of transition to state will determine transition probabilities of Markov chain ( ). That is to say, change probability from one activity to other or the same every 3 seconds due to duration time window.\nObviously assigned probabilities to transition matrix are static, that is to say, they are defined by the system before beginning the activity recognition. However, dynamic probability index is proposed in this work: Definition 6. Dynamic probability index for transition between and will be denoted by and will be defined as the hope that activity is done on current state (\u03a9 ( )) multiplied by the transition probability between and normalized:\n\u03a9 \u2022 \u2211 \u03a9 \u2022\nFrom this moment, when we talk about transition probability of Markov chain from to , we will refer to .\nDefinition 7. Markov chain's transition matrix then will be denoted as T:\n\u2026 \u2026 \u2026 \u2026\nWe will define the state vector in instant t and we will denote as to the vector of length j, where j is the number of activities that system can recognize and where each position of the vector corresponds to the probability that a transition to activity j ( in instant t is done from activity done in instant t-1:\n\u2022 Once defined the state vector, we can select the most probable activity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 8. We will define it as the current recognized activity and we denoted by to the activity for which component s larger: max", "text": "It will be noted that the recognized activity after all the classification process is , and it must not to match to , due to the defined Markov chain includes additional information to the model. To summarize, a very efficient Na\u00efve classifier made an initial classification to then pass the output to a Markov chain that eliminates noise based on temporal knowledge of previous activity and the likelihood of transitioning into next activity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Server", "text": "Once the classification of the activity has been completed and the system has recognized the activity, that the user is doing, it is time to send this information to the server. The aim is that the information was available to all users that have permission to look it. Thus, to control the activity of the user is possible.\nThis requires first to describe the architecture of the server in order to know the interactions available to it. The server will have a dual function. On one side will have a web services interface with which the system will be able to communicate with the user's mobile device monitoring, so that it is possible to send information about the activity which is being carried out and the time it began. On the other hand offer a web interface and a set of web services through which other users could view the information of activity that user is carrying out, either through a web browser or through special software that makes use of those services. Figure [4] shows the structure of the server.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 4. Server diagram", "text": "The updated server will contain a set of web services that will allow the application, for recognizing the user's activity, can send the activity information to the server and store it in the database. For them will be defined at least one function: Updating activity. The parameters of this function are the time and date on which the user began the activity and the identifier of the activity itself with the ID of the user who performed it. This feature adds to the database a new entry with the data provided.\nWe must make an explanation before continue. The continuous transmission of information about the activity carried out could make the battery of the user mobile user expires quickly. Therefore, only new user activity will be sent on the servers when it change. That is, if an user left the device on the bedside table, for example, the last activity would be sent \"without device\", which would occur by detecting transmission that has left the device on a surface. Subsequently, it does not send any information to the server for the rest of the night, until the user returns to pick up his/her device to start the daily activity. In case that the device loses the connection with the server for any reason, for example the failure of network access, the activity performed by the user will be stored in the mobile device next to the time at which such activity occurred. As soon as the connection to the server is refreshed, such activities will be uploaded to the server to update the record of user's activities. With this process we increase the reliability of the system for consultancy work, as no false readings assigned activities due to failed connections to the server. First, the use of a keep alive signal was issued, but the idea was rejected in order to avoid overloading the server. Also, keeping alive signal would be meaningless if the above technique was used, because the data would be updated automatically once the server connection is restored. This web service makes it possible to update the user's status, but not consulted. To this end, the web service provider and Web page server was developed. Both are connected to the database and obtain from it, the necessary information. Normally this information is the activity undertaken by a particular user during a certain time. In addition, it provides the ability to generate statistics about activities, duration and level of physical activity based on user activities detected.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "Results about evaluation of the system are presented below. On one side will discuss the effectiveness and efficiency of the proposed architecture and secondly, the reliability and accuracy of the recognition of physical activity system proposed.\nThe activity of recognition based on the Naive-Bayes classifier with the noise reduction system based on the Cooley-Tukey and Butterworth filter has produced excellent results even with the presence of high noise levels.\nTo check the accuracy of the method, the system has been trained to be able to recognize 5 activities of daily life of any person: walking, running, jumping, going up and down stairs. The learning undertaken by all users subject under study has consisted of 15 minutes per activity, excluding jumping activity for which training has only been 5 minutes.\nThe following table lists the results of classification based on the number of temporal windows correctly and incorrectly recognized in the detection process: The results show that the prototype for detection of physical activity has got some very good results despite a short training. However, opposite to what one might think, a long learning can negatively affect the outcome of the classification, due to overlearning. This effect usually occurs when the number of training data is very high for certain activities while others too low. This problem causes that the less trained activities are more concrete when Naive-Bayes method described above makes the classification, while more trained activities have a higher probability in all ranks of classification determined by the rank vectors. Figure 5 shows a comparison between our systems under two conditions: normal learning (with similar training times for each activity) and overlearning with walking activity much more trained. On the x-axis we can see the activities that the user has done during one hour. On the y-axis are shown the number of activities of each type recognized, this axis leads us to determinate the accuracy of recognition system. Finally, each color represents the activity recognized on an overlearned system, on a normal system and the true activity that user was doing. ", "publication_ref": [], "figure_ref": ["fig_116"], "table_ref": []}, {"heading": "Conclusions", "text": "We have achieved to perform a comprehensive system able to recognize, classify and share information about physical activity in a group of subjects. We have thus succeeded in controlling the activity of a user with a restriction of movement. This is very common in athletes who are in a period of restricted activity (e.g. rest) or elderly, since, thanks to the system, we can control anytime theirs activity. In other words, the activity that users are doing can be viewed by care persons without the need of being there, at the same place as user, allowing later the analysis of information of their activities.\nMoreover, the entire system has been adapted to a goal: to be installed on mobile devices. This requirement has led to all methods of detection, classification and recognition of activities which must have a reduced computational cost. Due to the above, not only the risk of saturation of the mobile device system due to excessive calculations is reduced, but also power consumption is reduced to a minimum. This makes the user must recharge the device less frequently and therefore the system will be more functional and comfortable.\nFinally, the proposed designed system has provided the capacity to access to the information of assisted subject through several ways. In this way, multitude of different devices (phones, PCs, PDAs, etc.) are covered by the system in order to access to the information anywhere and anytime.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Information retrieval is an interactive and iterative process and single query seldom satisfy users query on web. In that case, the user has to reformulate his/her initial query because it was over or under-specified, or did not use terminology matching relevant documents, or simply contained errors or typos. The query information can be analyzed from query log and better search experience can be constructed through search engine. It has been observed, whenever the user enters two queries in sequence; the connection or relation between the two repetitive queries can be defined as a query transition. If the user retains in the same search paradigm, then this connection is referred as a query reformulation. The goal of this work is to create a query reformulation model using evolutionary computational technique, and thus the first task is to define the target categories of the proposed model. Conceptually, if the two sequential queries of users demonstrate broader syntactic and semantic gap, then the proposed model performs a query slice clustering on query flow graph technique using Differential Evolution algorithm. Most of the existing clustering techniques, based on evolutionary algorithms, accept the number of classes K as an input instead of determining the same on the run. Nevertheless, in many practical situations, the appropriate number of groups in a previously unhandled data set may be unknown or impossible to determine. For example, it is found that, while clustering a set of retrieved documents arising from the query to a search engine, the number of classes K changes for each set of each document. In addition to, if the data set is described by high-dimensional feature vectors like as web centric data, it may be practically impossible to visualize the data for tracking its number of clusters. The context of the present query reformulation envisages into the query graph. The information extracted from query logs can be summarized and suitably represented through query graphs. The query graph is bipartite, with nodes representing queries and documents, and with an arc connecting a query q and a document d if and only if d was clicked by some user after submitting the query q. This describes an application of Differential Evolution to the automatic segmentation and clustering of large unlabeled web data sets to identify the pattern of completeness of query. In contrast to, most of the existing clustering or segmentation techniques, the proposed algorithm requires no prior knowledge of the data to be segmented. Rather, it determines the optimal number of partitions of the data on the \"fly\". We apply the proposed model to a large query logs to investigate the new patterns and similar segment relationship during the reformulation of query by the user. The validation of the algorithm is able to produce more pattern of query reformulation and thus improve the better experience of searching. The rest of the paper has been organized as follows: Section 2 elaborates the problem with examples. The related works have been discussed in section 2.1. Section 3 describes mathematical background of the proposed Differential Evolution technique. Section 4 presents proposed algorithm and section 4.1 validates the proposal with a public data set of query log. Finally, section 5 gives conclusion and further scope of research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Statement of Problem", "text": "The query reformulation can be represented as zero dissimilarity (same query with the induction of error, like the user switches to capital letter or spelling has been modified (example: \"indai\" and \"India\"). There is second possibility of query reformulation, where the wordings of query have been altered, but keeping exactly the same target in the sense of presentation of query. Example includes \"Cheap Hotel\" and \"Hotel with Minimum Cost\". Even there are unconditional changes in query presentation in sequence like\"Tourist Spots of India\", \"Mobil Connections providers in India\". Finally, the complete query could be modified by the user both in syntax and semantics. The basic stages of query reformulation are shown in Fig. 1.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Fig. 1. Query Reformulation Types", "text": "The variation in query string also may differ in the new query formation with added generalization level. That means, the modified query will be more general to understand the search pattern of user. For an example the query on \"Mountaineering\", can be made more generalized to query \"explore Mountains\". In reverse direction, the specialization also can augment query reformulation of user, like \"Catalogue of Furniture\", to \"iron grid furniture\". The generalization and specialization both can be expected from the user's generalization and ambition to recall and reformulate the query, whereas a specialization is the need to improve precision and reduces the search space. Practically they form a transitive relation and that must be antisymmetric in nature. To analyze all hidden and latent variation, a sound clustering and query segmentation technique is required prior to parse the final query through search engine. Therefore, the proposed model concentrates on query logs and infers the hidden semantics of user interactions with search engines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Baeza-Yates [1] identifies five different types of graphs. In all cases, the nodes are queries; a link is introduced between two nodes respectively if:\n\u2022 the queries contain the same word(s) (word graph),\n\u2022 the queries belong to the same session (session graph),\n\u2022 users clicked on the same URLs in the list of their results (URL 1 cover graph), \u2022 there is a link between the two clicked URLs (URL link graph) \u2022 There are l common terms in the content of the two (link graph).\nThese graphs can be effectively utilized for segmenting and clustering the session based on the user's search pattern. The study of query reformulation types started with the work of Lau and Horvitz [4], who sampled 4960 queries from a query log and manually labeled the transitions they found, proposing a classification of queryreformulation types. Rieh and Xie [5] manually labeled 313 search missions and suggested a more fine-grained classification. The information extracted from query logs can be summarized and suitably represented through query graphs, several examples of which are cited in [6][7]. Recently, Boldi et al elaborated the concept of query reformulation and suggested learning phenomena in the pattern itself [8]. To improve the analysis of accuracy and effective clustering of query reformulation, the present work introduces the evolutionary technique in the form of Differential Evolution.\nThe next section describes the essential backend components, which are deployed in the proposed model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Essential Components of Proposed Model", "text": "For the proposed model, we define following components mathematically: Query Log: A query log records information about the search actions of the users of a search engine. Such information includes the queries submitted by the users; documents are viewed as a result to each query, and documents clicked by the users. A typical query log L is a set of records <q i , u i , t i , V i ,C i >, where: qi is the submitted query, u i is an anonymized identifier for the user who submitted the query, t i is a timestamp, V i is the set of documents returned as results to the query, and C i is the set of documents clicked by the user [2].\n\u2022 Sessions: A user query session is defined as the sequence of queries of one particular user within a specific time limit. More formally, if t \u03b8 is a timeout threshold, a user query session S is a maximal ordered sequence: S = \u02d9<q i1 , u i1 , t i1 >, . . . ,< q ik , u ik , t ik >, where u i1 = ....= u ik = u \u2208 U , t i1 \u2264 \u2264 .... t ik , and t ij+1 -t ij \u2264 t \u03b8 , for all j = 1, 2, . . . , k -1.\n\u2022 Chains: A chain is a topically coherent sequence of queries of one user. Radlinski and Joachims [3] defined a chain as \"a sequence of queries with a similar information need\". Unlike the concept of session, chains involve relating queries based on the user information need, which is an extremely hard problem. \u2022 Pattern:\nA pattern is a physical or abstract structure of objects. It is distinguished from others by a collective set of attributes called features, which together represent a pattern [9]. Let P = {P 1 , P 2 . . ., P n } be a set of n patterns or data points, each having d features. These patterns can also be represented by a profile data matrix X n\u00d7d with nd-dimensional row vectors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2022 Query Recommendation after Reformulation", "text": "A simple recommendation scheme that uses the query flow graph is to pick, for an input query q, the node having the largest w 0 (q, q 0 ). The query-flow graph Gqf is a directed graph Gqf = (V, E, w) where: the set of nodes is V = Q \u222a (s, t}, i.e., the distinct set of queries Q submitted to the search engine and two special nodes s and t, representing a starting state and a terminal state which can be seen as the begin and the end of a chain.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Differential Evolution Technique: State of the Art", "text": "In this paper, the concept of Differential Evolution (DE) has been used to determine the automatic clustering in different query stages and finally the query graph is formulated with more in depth pattern behavior of search. The classical DE is a population-based global optimization algorithm that uses a floating-point (real-coded) representation. The i th individual vector of the population at time-step (generation) t has d components (dimensions), i.e., ) t ( Z i = [Z i,1 (t), Z i,2 (t), . . . , Z i,d (t)]. For each individual vec-  Cr \u2208 [0, 1] is a scalar parameter of the algorithm, called the crossover rate. If the new offspring yields a better value of the objective function, it replaces its parent in the next generation; otherwise, the parent is retained in the population. For n data points, each d dimensional, and for a user-specified maximum number of clusters set in query reformulation K max , a chromosome is a vector of real numbers of dimension K max + K max \u00d7 d. The first K max entries are positive floating point numbers in [0, 1], each of which controls, whether the corresponding cluster is to be activated (i.e., to be really used for classifying the query type for each session) or not. The remaining entries are reserved for K max cluster centers, each d dimensional. Thus, we select an optimal threshold in query cluster; in the form of Threshold ij forms the following prototype rule: \nIF", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Test Dataset and Validation of Results", "text": "The example data set of typical query classification using DE has been prepared. The categorization is done on the basis of the query in session 1 and reformulated query in session 2 (Refer Table 1). After parsing to the proposed DE classifier, the type of query could be intermediately predicted. It should be noted that in this data set, the reformation for typographic errors has not been included for classification. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Generalized", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Specialized", "text": "The parameter set for proposed DE is shown in Table 2. All the essential parameters like population size, cross over rate, K max and K min cluster number for query and reformulation of query has been fixed from the implementation aspects of DE. The final predicted clusters for reformulated cross over queries for two consecutive sessions can be calculated through Candidate Solution measure. Chou et al. have proposed the Candidate Solution (CS) measure [10] for evaluating the validity of a clustering scheme. Before applying the CS measure, the centroid of a cluster is computed by averaging the data vectors that belong to that cluster using: A distance metric between any two data points i X and j X is denoted by d (, j X , i X ). Then, the CS measure can be defined as:\n(2)\nAfter applying the Candidate Solution measure, we implement the proposed algorithm of DE on \"spring 2006 Data Asset\" distributed by Microsoft Research, we built the query-flow graph from this dataset (script shown in appendix to extract query log). A set of 120 input queries were selected. Out of which, some complex queries in both sessions are presented in Table 1. From practical point of view, it is suggested to set the number of parents to 10 times the number of parameters, select weighing factor: F=0.8 and cross over constant CR=0.9.\nFrom the plot given in Fig. 3, it is shown that different scale factor (F) associated with query and reformulation of query for session q and q\u2032 for entire generation of DE. The convergence of query seems varying for each different value of F starting from 0.2 to1.5. Here, we deliberately set CR =0.3 to demonstrate that the variation of the value of cross over rate from standard 0.9 to o.3 will affect the segmentation and classification of query cluster reformulation strategy. This has also been inferred from the empirical result of query reformulation and convergence that increase in number of population and simultaneously lower the scale factor; convergence is more likely in proposed DE (for each query cluster per session to occur) but generally it takes longer time. Thus, the algorithm shows a trade-off between robustness and speed of convergence in terms query reformulation strategy. The result could be broadly compared with other popular clustering techniques, which clearly shows that Differential Evolution technique has been applied on any query point without having no prior knowledge on the query patter, Therefore functionally it works more faster than the other on line crawling mechanism.", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": ["tab_5", "tab_2"]}, {"heading": "Conclusion", "text": "This paper presents a query reformulation and clustering technique using Differential Evolution. The proposed DE automatically determines the type of a query and new pattern of query reformulation. Thus, effective search and crawling technique could be devised about the predictability of different complex query reformulation of user.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The proliferation of content on the World Wide Web (WWW) in the past decade has been, in the most part, economically supported by web advertising. A web advertisement is a section of a web page that is dedicated not to the content of the page but to graphics and/or text promoting the product or service of a third party. A web advertisement usually offers a link to an outside web site that provides further details regarding the product. The advertising party and the webmaster who owns the content have a contract which states that the advertiser will pay the webmaster in exchange for this service. The precise amount offered is frequently conditional on the success of the ad in compelling the end user to follow its link for more information. This success is usually measured by clickthrough rate (or CTR). A CTR is calculated by dividing the number of users who clicked on a specific ad by the number of times the ad is delivered.\nIt is then in the webmaster's best financial interest to maximize the number of his users that will, in fact, click-through. One approach to doing this is to ensure that the ad's content is matched to the users' interests, if there is a choice amongst candidate ads for a web page. It is this problem of choice that motivates the research reported in this paper. The anonymity of the web creates several obstacles to effectively resolving this problem of choice. It is not possible to track who exactly is coming to your website without browser cookies or some other invasive technology, and even these are unreliable [1]. Even if it could be known which user is which, it is not easy to gauge their interests and demographics outside broad categories like geographic location or browser usage. A webmaster can only rely on one piece of information from the user when choosing which ad to match with a served page, and that is the content of the requested page.\nDespite the limited amount of information, the webmaster must determine the users' interests, or at least which ads they are most likely to click on. Our main contribution includes the introduction of an ant-based algorithm for the association of web pages and ads with each other. Utilizing stigmergic [2,3] principles, the proposed algorithm builds models that can be quickly updated and provide recommendations for ad-serving. Moreover, we introduce a simulation test bed for evaluation of the proposed algorithm. The experiments performed demonstrate the utility of the proposed algorithm.\nThe paper consists of 6 further sections. The paper continues by providing important background information on Ant Colony algorithms in Section 2. Section 3 briefly describes related work in the area of ad association using biologicallyinspired algorithms. Section 4 describes the main contributions of this paper: algorithms for ad association and a test bed that is used to evaluate them. Sections 5 and 6 describe the experimental setup and results respectively. Section 7 summarizes the key messages of the paper and briefly highlights potential future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "The heuristics that an ant colony uses to find food have inspired a computational metaheuristic that is known as Ant Colony Optimization (ACO) [4]. Starting with a simple, connected graph with start and destination nodes and every edge having a pheromone level, \u03c4 ij , each ant steps from the node it is on to another connected node. The probability of the selection of an edge, e ij , to follow at time t while the ant is located at node i can be calculated using Equation 1. Here, \u03c4 ij is the amount of pheromone on edge e ij and \u03b7 ij represents the desirability of a given direction. N (i) contains the neighbors of node i. The parameters \u03b1 and \u03b2 are system parameters.\nP ij (t) = \u03c4 \u03b1 ij (t)\u03b7 \u03b2 ij x\u2208N (i) \u03c4 \u03b1 ix (t)\u03b7 \u03b2 ix (1)\nThis process is repeated with each ant at each node until it reaches the destination. When the destination is reached an amount of pheromone is deposited on each edge that is inversely proportional to the total length of the path. To prevent premature convergence, pheromones are allowed to evaporate over time.\nAlgorithmically, this means at each iteration, reduce the pheromone level, \u03c4 ij , by multiplying it by (1 \u2212 \u03c1) where 0 < \u03c1 < 1 is the evaporation rate. With this addition, we get the simplest form of ACO [4]. An ACO variant will be used to recommend Web advertisements and will be detailed in Section 4.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The algorithms used for ad selection for Web pages are proprietary and remain largely unreported in the literature. Furthermore, real time ad selection is not based upon online click-though, which is the motivation for the research reported in this paper. However, the value of click-through data in this domain is well understood. See, for example, [5,6]. The problem of offline ad association can be viewed as a data mining problem if a large body of page requests can be used to induce classifiers. While the body of literature is large in this general space, Kim et al. [7] have used decision trees to guide the creation of advertisements for online storefronts and [6] has optimized search results using support vector machines and click-through data. We strongly believe that [5] could be used to analyze ads provided to our system to create the adKeywords vectors shown in the clickThrough algorithm (see Section 4) and facilitate the use of non-zero values of \u03b2 (see Algorithm 3 and Table 1). However, our interest in this paper is the incremental creation of classifiers online and, more specifically, through a use of biologically-inspired algorithms. With this latter qualification, prior research is sparse, with AdPalette [8] being noteworthy. AdPalette uses genetic algorithms to customize advertisements with usage, relying on crossover and mutation in order to combine promising ad components on pages.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Model", "text": "The research reported here was performed in a simulation environment. Figure 1 represents the actual Web environment being simulated. Simulated users with defined preferences create queries that are used to generate responses that contain a simulated advertisement. In the Web environment shown in Figure 1, users (e.g., Bob and Alice) interact with one or more web servers (e.g., Web Server A and B). When Bob asks for a page from Web Server A (indicated by 1 in the circle) content is returned that contains JavaScript that runs inside of Bob's browser. The JavaScript causes the Ad Server to be contacted with keywords extracted from the content delivered by Web Server A. The ad returned from the Ad Server is shown by the number 3 in a circle in Figure 1.\nThe simulation models the interaction that occurs between the web page users and the web server that processes page requests and matches the advertisements to the content. Two types of processes run simultaneously: one server script where most of the actual computing occurs, and a user script that contains randomly generated users that create and send off page requests to the server to be processed. Essentially, the user script models a user's interactions with various web servers that are connected, in turn, to an ad serving system. The functions of the user script are to (a) generate users and (b) generate queries and assess responses. The server script represents the functions of the ad serving system. The server script is responsible for analyzing the queries that it receives and making a decision as to which ad to serve. The first function of the user script is to generate up to l users. Each user entity is meant to simulate one actual user's preferences and choices. To represent the variability of users' interests, each user has a rating for each of a pool of m keywords that represents how interested the user is in that particular subject. Note, since we are merely simulating a user, actual keywords are not used but the preferences, userInterest, recorded as a vector of values [k 0 , k 1 , . . . , k m\u22121 ], with the value of each k i being between 0 -meaning no interest in the subject i -and 1 -meaning the highest possible interest in i -are instead. These values are randomly generated, but are weighted towards the extremes such that there is a one third chance of the interest level being between 0.75 and 1, and a one third chance of the interest level being between 0 and 0.25. This is meant to represent that is more likely that a person has a considerable amount of interest in a topic or very little interest at all, as opposed to being ambivalent about it.\nAfter the user is created, the user script synthesizes finding a page of interest, defined by keywords, to the user. Pages are represented by randomly generating a relevance vector [r 0 , r 1 , . . . , r m\u22121 ] to represent a possible page, again with values between 0 and 1 that represent how tightly tied to the subject matter the content is. In an actual implementation with real Web content, this could be determined using a vector space model [9] that analyzes the page content relevant to other pages. Since users do not randomly surf pages, but go to pages that match their interests, only pages with keywords relevant to the user are of interest. To simulate this, the angle between the interest vector of the user and the relevance vector of the page is first determined using Equation 2.  2while angle > R thresh do for i = 0 to m \u2212 1 do 20% chance of:", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "angle(a, b) = arccos", "text": "page[i] = page[i]+userInterest[i]\n2 end for angle = angle between page and userInterest using Equation 2 end while return page the m values has a 20% chance of being averaged with the user's interest values. This continues in a loop until the threshold is met. When the generateP age algorithm (see Algorithm 1) completes it is guaranteed that the generated page represents the interests of the modeled user; however, there may be content unrelated to the user's primary interests.\nWhen the server receives a page from a user, its function is to decide which ad should be matched with the given page. This is achieved by using the Ad Association (A2) algorithm as defined in Section 4.1. Finally, the page will be sent back to the appropriate user with one of n possible ads attached. The user them evaluates the ad, and determines whether to click it. In a real-life scenario, the person receiving the ad would judge the ad on his own using preferences and goals, and choose whether the ad has piqued his interest enough to click it. However, it would be incredibly hard to simulate a human in this way, so it has been simplified as follows. Each ad has a number of keywords associated with it, denoted by pm, of the m possible keywords (pm < m). The more interest a user has in the pm keywords, the more likely the ad is to be a success. The chance of success is determined on a logarithmic scale. For example, with 10 keywords associated with an ad, there is an expected value of 1/100 th chance of a click-through, but varying from 1/10, 000 th chance if the user has no interest at all in the ad's keywords and guaranteed success if the user has maximum interest in all keywords. The clickThrough algorithm below indicates how success is computed, with the adKeywords array containing 0 for completely irrelevant keywords not thought to be useful for the ad and 1 for keywords that are considered important or completely relevant. The adKeywords array therefore contains pm entries that are 1 and m \u2212 pm entries that are 0. The normalization value is a constant for the system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2", "text": "clickThrough(Array adKeywords, Array userInterest) total = 0 for i = 0 to n \u2212 1 do if adKeywords[i] == 1 then total = total + userInterest[i] end if end for return 10 (total\u2212pm) * normalization Finally, when the success of the ad is determined, information is once again passed back to the server. The server executes the A2 algorithm and makes changes based on the page relevance vector, the ad selected and the user's choice in regard to the ad. This whole process repeats many times; multiple users each making a series of page requests.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ad Association (A2) Algorithm", "text": "In this ant-inspired algorithm, each ad is a given fixed path with m nodes, one for each of the possible interest keywords along with the adKeywords described in the previous section. When an ad is served to the user, and is successful, then it positively reinforces that ad's path. If it fails, it negatively reinforces the path.\nThere are 3 parts to the A2 Algorithm: the model, a method for choosing the best ad considering a page input, and a method for changing the model. The model, M , is a collection of one vector per ad, a, and each vector has a value for each of the m interest keywords, i.e., M={v 01 , v 02 , . . . , v 0m\u22121 , v 11 , . . . , v n\u22121m\u22121 }. Each of these nodes, v ij , initially contains a value, \u03c4 0 , just slightly above zero representing the amount of pheromone on that part of the path. There are n ads that can be served. Furthermore, each ad has pm defining keywords, these being used to decide on whether a click-through occurs or not. The pm keywords are chosen from the m possible keywords.\nAs the server receives page requests from users, the A2 algorithm (See Algorithm 3) executes for the purpose of determining which of the n ads should be returned. The algorithm proceeds by comparing the page's relevance vector with each of the n ads by measuring the angle between the two vectors using Equation 2. This comparison has two components: first, the feedback gathered from previous ad associations and second, the comparison between the page and the ad keywords that are considered relevant. The smaller the angle, the more similar the ad's vector is to the relevance vector. The server chooses which ad to return randomly biased by rank. There is a 1 2 r chance that the r th best ad is returned to the user, thus ensuring a heavy bias towards the best ads; i.e., the best ad will only be returned half of the time. Once the server sends out the ad, it waits for information on whether the user clicked the ad. If the ad was successful, the recordAd algorithm executes. This algorithm ensures that the vector corresponding to that successful ad, i, is updated so that it increases its pheromone values for all keywords in i, but increases the page's more relevant keywords more, as shown in Equation 3. The relevance vector is r, and c is a constant that controls how much each success influences the model, 0 < c < 1. This is equivalent to an ant spreading pheromone on the path to the successful ad, and making that path more appealing to future ants with similar vectors that match.\n\u2200j v t+1 ij = v t ij + r[j] \u00d7 c (3)\nTo counter the pheromone values from growing out of control, values are bounded and the model also adjusts them when an ad fails. In a real-life application, it is much harder to tell when an ad fails as no message can be sent to the server saying that the user did not do some action. To circumvent this, it is assumed that there is a timeout function such that if the ad is not successful in a given time frame, then it counts as a failure. In the simulation, the user script just returns a \"failure\" result. When the server receives a failure, it reduces all pheromone values for the associated ad, i, but reduces it more for the keywords most relevant to the page in question. This effect is shown in Equation 4,\nwhere k is a constant that controls how much each failure influences the model, 0 < k < 1 and k << c. Here, k has the role of evaporation in ACO. \u03c4 0 is a very small number, \u03c4 0 << k.\n\u2200j v t+1 ij = v t ij + max(r[j] \u00d7 k, \u03c4 0 ) ( 4)\nThe purpose of having this pheromone reduction is to prevent one ad from being overused early on, and becoming dominant before other ads get a chance to demonstrate relevance. Equations 3 and 4 form the basis of the recordAd algorithm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Multiple trials were completed for the A2 algorithm to ensure consistency and result reproducibility. Three trials are reported in Figure 2 in order to show the typical variability in simulated performance. The values of c and k were chosen to reflect the relative probabilities of success and failure respectively; the ratio being approximately 100 to 1 for the scenario modeled here. The value of \u03b2 were chosen to reflect an extremely pessimistic scenario in which nothing was known about the keywords associated with the advertisements to be presented. In essence, this value of \u03b2 says that we know nothing about the contents of the ad, which may be true if the ad is an image or video or the provider seeks to provide a \"black box\" ad. That said, this is an extreme scenario and was chosen in order to test whether the system could improve based purely upon observed feedback from users.\nThe value of the normalization coefficient was chosen to ensure that the average chance of click-through for a page with half of the expected keywords correct would be 0.01, with the range of probabilities varying from 1 (for all expected keywords correct) to 0.0001 (for no keywords correct).\nThe page requests were broken down into 50 blocks of 10, 000 requests each. For each block, the average success rate as determined by the user was measured. Also measured was the average expected value of the success rate if the ads were chosen randomly. This is shown as a horizontal line in Figure 2. ", "publication_ref": [], "figure_ref": ["fig_15", "fig_15"], "table_ref": []}, {"heading": "Results and Discussion", "text": "The data gathered from testing the A2 Algorithm is promising. As shown in Figure 2 the average chance of ad success will be increased over time. Over the course of 500, 000 page requests, there was a 70% to 80% improvement in efficiency over the random pairing of ads when viewed across the 3 trials reported here.\nNote that, the proposed algorithm does not always return the best match from its model, but returns one from the top few ads with very high frequency (87.5% from the top 3 ads). This is important and deliberate. It was found during the implementation and associated experimentation that without this feature, the model would converge too quickly on one ad that happened to show promise early on. Success would breed more recommendations for that ad, which would only increase the likelihood of it being recommended again later. Adding the chance that any ad could be picked kept the model from being dominated too early by any particular ad. Thus, robustness was maintained.\nAs can be observed in Figure 2, the system continues to learn even after 500, 000 page requests making it likely that 100% improvement over initial system performance is achievable. Combining the clear benefits that this algorithm produces in terms of advertising success and the ease with which it handles large quantities of data makes it attractive.", "publication_ref": [], "figure_ref": ["fig_15", "fig_15"], "table_ref": []}, {"heading": "Conclusions", "text": "This paper provides insight into the effectiveness of using an ant-based algorithm to improve Internet advertising. A webmaster could use the proposed A2 algorithm with minor modifications. As shown with an artificial environment, it should be able to increase ad success, and therefore, advertising revenue by over 70% after processing a reasonable amount of traffic for a large website. If it were used, with refinements made to adjust for real-life variables and efficiencies added to reflect issues unique to the website, it would be a simple piece of software with potential revenue benefits. It is worth looking further into using the A2 algorithm as a predictor of advertisement effectiveness. We believe that clickthrough data mining techniques as described in [5] and [6] (as examples) could provide a valuable starting point for system ad keyword initialization thereby allowing for better-than-random initial system performance. Furthermore, larger data sets should be tested beyond the small problem space that is used here.\nBeyond this, the next stage would be to implement the algorithm on a real web server. It would run continuously and intercept real incoming customer data and produce actual ads, using vector-space models to convert the requested pages into the vectors used by the algorithm. It could be further refined to take into account that some advertisements may pay more to be shown, that some ads have different sizes and page positioning from others and that more than one ad is often shown on a site at once.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Thanks to the standardization of the ontology languages RDF/S 1 and OWL 2 , the Semantic Web has been realized and the amount of available semantic annotations is ever increasing. This is due in part to the active research concerned about learning knowledge structures from textual data, usually referred as Ontology Learning [1]. However, little work has been directed towards mining from the Semantic Web. We strongly believe that mining Semantic Web data will bring much benefit to many domain-specific research communities where relevant data are often complex and heterogeneous, and a large body of knowledge is available in the form of ontologies and semantic annotations. This is the case of the clinical and biomedical scenarios, where applications often have to deal with large volumes of complex data sets with different structure and semantics. In this paper, we investigate how ontological instances expressed in OWL can be combined into transactions in order to be processed by traditional association rules algorithms, and how we can exploit the rich knowledge encoded in the respective ontologies to reduce the search space.\nThe rest of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 explains the basics of the two integrated technologies, association rules mining and OWL DL ontologies and motivates the problem with a running example. Section 4 contains the general methodology and foundations of the approach. Section 5 shows the experimental evaluation and Section 6 gives some conclusions and future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Most research on data mining for semantic data is based on Inductive Logic Programing (ILP) [2], which exploits the underlying logic encoded in the data to learn new concepts. Some examples are presented in [3] and [4]. However, there is the inconvenient of rewriting the data sets into logic programming formalisms and most of these approaches are not able to identify some hidden concepts that statistical algorithms would.\nOther studies extend statistical machine learning algorithms to be able to directly deal with ontologies and their associated instance data. In [5], a framework is presented for designing kernel functions that exploit the knowledge of the underlying ontologies. Recently, in [6] and [7] new frequent association rules algorithms are proposed which make use of similarity functions in a similar way to the previous kernel functions.\nA recent trend in data mining research is to consider more complex and heterogeneous structures than single tabular data, mainly tree and graph structured data. In this line, we can find frequent subtree [8] and graph mining [9], whose aim is to identify frequent substructures in complex data sets. Albeit interesting, these algorithms do not serve the purpose of finding interesting content associations in RDF/S and OWL graphs because they are concerned with frequent syntactic substructures but not frequent semantically related contents. Indeed, frequent graph substructures usually hide interesting associations that involve contents represented with different detail levels of the ontology. Moreover, although the underlying structure of RDFS and OWL is a graph, reasoning capabilities must be applied to handle implicit knowledge.\nFinally, we find some work aimed at integrating knowledge discovery capabilities into SPARQL 3 by extending its grammar. Some examples are [10], which can be plugged with several data mining algorithms and [11], which finds complex path relations between resources. Inspired by these works, we have also extended SPARQL grammar to define association rule patterns over the ontological data but in a less restrictive way than the one imposed by SPARQL. These patterns allow the system to focus only on the interesting features, reducing both the number and length of generated transactions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "The problem of discovering association rules was first introduced in [12]. It can be described formally as follows. Let I = {i 1 , i 2 , ..., i m } be a set of m literals, called items. Let D = {t 1 , t 2 , ..., t n } be a database of n transactions where each transaction is a subset of I. An itemset is a subset of items. The support of an itemset S, denoted by sup(X), is the percentage of transactions in the database D that contain S. An itemset is called frequent if its support is greater than or equal to a user specified threshold value.\nAn association rule r is a rule of the form X \u21d2 Y where both X and Y are nonempty subsets of I and X \u2229 Y = \u2205. X is the antecedent of r and Y is called its consequent. The support and confidence of the association rule r : X \u21d2 Y are denoted by sup(r) and conf (r).\nThe task of the association data mining problem is to find all association rules with support and confidence greater than user specified minimum support and minimum confidence threshold values [12].\nDLs allow ontology developers to define the domain of interest in terms of individuals, atomic concepts (called classes in OWL) and roles (called properties in OWL). Concept constructors allow the definition of complex concepts composed of atomic concepts and roles. OWL DL provides for union ( ), intersection ( ) and complement (\u00ac), as well as enumerated classes (called oneOf in OWL) and existential (\u2203), universal (\u2200) and cardinality (\u2265, \u2264, =) restrictions involving an atomic role R or its inverse R \u2212 . In OWL DL it is possible to assert that a concept C is subsumed by D (C D), or is equivalent to D (C \u2261 D). Equivalence and subsumption can be also asserted between roles and roles can have special constraints (e.g., transitivity, simmetry, functionality, etc.) Regarding instance axioms, we can specify the class C of an instance a (C(a)), or the relations between two instances a and b (R(a, b)). A DL ontology consists of a set of axioms describing the knowledge of an application domain. This knowledge ranges over the terminological cognition of the domain (the concepts of interest, its Tbox ) and its assertions (the instances of the concepts, its Abox ).\nFig. 1 shows a fragment of the Tbox of a DL ontology designed for patients with arthritis-related diseases. Through semantic annotation, clinicians can annotate data sets with ontology terms and relationships from the axioms in Fig. 1, creating a repository of semantic annotations in OWL (an Abox ) which must be consistent with the ontology axioms (Tbox ). The right hand side of Fig. 1   of the semantic annotations associated to a patient named PTNXZ1. Notice that semantic annotations are represented as triples (subject, predicate, object). In this paper, we separate the Tbox from the Abox for practical issues. In general, we use the term ontology to refer to the Tbox and instance store to refer to the Abox. Thus, our data mining problem is defined as follows:\nData Mining Problem: Given an OWL instance store IS consistent with the ontology O and a mining pattern Q expressed in the extended SPARQL syntax (see Section 4.1), find association rules from IS according to the mining pattern Q with minimum support and confidence threshold values.\nThe previous data mining problem can be thought as a classic data mining problem where the input data must be derived from the ontology in order to generate transactions according to the user specification.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Methodology", "text": "In this section we present a detailed view of our method along with the definitions that sustain it. Fig. 2 depicts a schematic overview of the whole process. The user specifies a mining pattern following an extended SPARQL syntax. Then, the transaction extractor is able to identify and construct transactions according to the mining pattern previously specified. Finally, the set of transactions obtained are processed by a traditional pattern mining algorithm, which finds association rules of the form specified in the mining pattern with support and confidence greater than user's specified ones. ", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Mining Pattern Specification", "text": "The user has to specify the kind of patterns (s)he is interested in obtaining from the repository. which is an SQL extension to work with data mining models in Microsoft SQL Server Analysis Services. 4 The extended SPARQL grammar is depicted in Fig. 3, and Fig. 4 shows an example query. We extend the SPARQL grammar rule Query by adding a new symbol, named MiningQuery. This symbol expands to the keywords CREATE MINING MODEL followed by the Source, which identifies the input repository. The body consists of variables the user is interested in mining. Next to each variable, we specify its content type: RESOURCE for variables holding RDF resources, DISCRETE for variables holding literal values and CONTINUOUS for variables holding a continuous literal value. In case we want to find patterns with just one occurrence of the variable, we attach the keyword MAXCARD1 to the variable. By default, patterns found can contain more than one occurrence of each variable. Moreover, we specify the consequent of the rule by attaching the keyword PREDICT. Finally, the keyword TARGET denotes the resource under analysis, which must be an ontology concept. The analysis target determines the set of obtained rules. In the query example, the analysis target is a Patient. In the WHERE clause, we specify the restrictions over the previous variables. The good news is that we do not expect users to have an exact knowledge of the ontology structure. Therefore, users do not have to input the paths relating the pattern variables in SPARQL. Instead, they are asked to specify just the type (i.e. ontology concept) that the variables refer to. In case variables are not resources, users must specify the type of the domain of that value followed by the data type property. From now on, we refer to these ontology concepts selected by the user as the f eatures set. For example, in Fig. 4 the user is interested in obtaining which drugs are associated to which diseases, so the features set is {Drug, Disease}. Finally, the UsingClause grammar symbol defines the name and parameters of the learning algorithm.\nSince we do not ask the user to specify the exact relations, the previous query model introduces some ambiguity regarding the items that form a transaction. When the user specifies the mining pattern, (s)he is thinking about obtaining subsets of drugs that are frequently administered to patients having a certain disease. Therefore, (s)he restricted the features set to be of type Drug and Disease, respectively. However, these concepts may appear not only under the user intended context but all over the ontology. That is to say, the same conceptual entities may appear under different contexts, making it challenging for the system to automatically discover what the users' intentions really are. As previously mentioned, the user could remove this ambiguity by specifying in the SPARQL extended query the exact relation of concepts in the ontology through pattern graph triples in the WHERE clause. However, this task can be cumbersome and not always viable. In this paper, we want to relief the user from this burden and let the system handle the task of finding appropriate contexts. Thus, in order to provide the right sense to the query, user can select the intended context with the CONTEXT keyword attached to the appropriate concept.", "publication_ref": [], "figure_ref": ["fig_28", "fig_29", "fig_29"], "table_ref": []}, {"heading": "Transaction Extractor Foundations", "text": "Since our goal is to be able to identify and construct transactions according to the user's mining pattern, in this section we present all the definitions that sustain the method we have developed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 1.", "text": "Let O be an ontology, IS an instance store consistent with O and C T the analysis target. The target instances are the set I T = {i/i \u2208 IS, O\u222aIS |= C T (i)}.\nIn the running example C T is P atient and I T is the set of all instances classified as P atient. Example 1. The ontology fragment on the left in Fig. 5 models some part of the patient's medical record. In this example we can infer that Contexts(Disease, Drug, P atient) = {Report, M otherHistory}. Definition 4. Let i and i be two named instances of an instance store IS. P ath(i, i ) = (r 1 \u2022 ... \u2022 r n ) \u2208 P aths(i, i ) is an aggregation path from instance i to instance i in the instance store IS.\nDefinition 5. Let i T \u2208 I T be a target instance and i a , i b two named instances in an instance store IS. Contexts(i a , i b , i T ) = {i } are least common reachable instances. That is,\n(1) \u2203p 1 \u2208 P aths(i a , i ) \u2227 \u2203p 2 \u2208 P aths(i b , i ) \u2227 \u2203p 3 \u2208 P aths(i , i SUB ) (i is common reachable instance). (2) if \u2203p x \u2208 P aths(i a , i ) \u2227 \u2203p y \u2208 P aths(i b , i ) then \u2203p z \u2208 P aths(i , i ) (i is least).\nExample 2. The right hand side of Fig. 5 shows an example of instance store represented as a graph that is consistent with the ontology fragment. In this example, we can infer that Contexts(P olyArthritis, M ethotrexate, P T N XY 21) = {RP T N H23}. As previously mentioned, context compatible instances may appear under very different contexts in an ontology, making it hard for the system to guess the user intended context. Currently, we allow users to specify the contexts of interest directly in the query. The concept denoting the selected context is denoted as C CT XT . If no context is specified, we assume C CT XT = C T . Now, transactions can be unambiguosly defined as follows. An instance transaction associated to target instance i T under context C CT XT is a tuple of instances (i\n1 , i 2 , ..., i n ) such that \u2200i x , 1 \u2264 x \u2264 n, O \u222a IS |= C(i x )\nwhere C \u2208 f eatures and \u2200(j, k), 1 \u2264 j, k \u2264 n, if C(i j ) = C(i k ), (i j , i k ) are context compatible under i T w.r.t. the user selected context C CT XT .\nExample 4. Given C T = P atient and f eatures = {Drug, Disease} we have three options. If the user does not select a context (i.e. C CT XT = P atient) the transaction {M ethotrexate, P olyArthritis, RheumatoidArthritis, N SAIDS} would be generated. If C CT XT = Report, the transaction {M ethotrexate, P olyArthritis} would be generated. Finally if C CT XT = M otherHistory, the generated transaction would be {RheumatoidArthritis, N SAIDS}.", "publication_ref": [], "figure_ref": ["fig_116", "fig_116"], "table_ref": []}, {"heading": "Evaluation", "text": "The current implementation of the transaction extractor has been developed on the top the ontology indexing system proposed in [13], which also provides a simple reasoning mechanism over the ontology indexes. In order to show the usefulness of our proposal, we test the method over a real-world instance store holding OWL annotations about patient's follow-ups. These annotations have been generated in the context of the Health-e-Child project 5 , and they are consistent with an ontology similar to the one used as example in Fig. 1. The semantic annotations contain information about 588 patients with very heterogeneous structure. The total number of semantic annotations is 629.000, which gives more than 1000 semantic annotations per patient on average. Table 1 shows some query pattern examples 6 along with the user selected context, the number of transactions generated by our method and the number of rules generated with the Apriori algorithm [14] for mining association rules. The query patterns specify the ontology concepts acting as interesting features, leaving the PREDICT attribute unmarked. As it can be observed, the first query is executed in for the context of P atient, which means a transaction will be generated for each patient holding features of type Disease, Drug and F inding. The selection of the context is crucial because it determines both the number and contents of each transaction, and therefore, the obtained rules. Notice the number of transactions is very reduced thanks to the context selection and, consequently, the number of generated rules.\nTable 1 also shows some examples of the rules generated from the previous queries. The obtained rules are very clear and useful thanks to the query pattern restrictions and latter transaction generation, which extremely reduces the features' search space thus, the complexity and overload produced by uninteresting features. To corroborate this fact, we generated transactions with all the possible features at the context of Patient and the apriori algorithm obtained more than 400.000 rules, of which the first hundred were uninteresting. Notice for queries 1 and 2 we get the same rule stating that the presence of ANA implies oligoarthritis disease. However, the support is different because their respective contexts are different, which generates different transaction sets.\nOverall, by specifying a query pattern and selecting the intended context, the features' search space is extremely reduced. That is, the generated transactions contain only the interesting stated and inferred features for the user at the level of granularity specified in the context. Therefore, this process eases the task of the association rule algorithm, which extracts small and very useful subsets of interesting rules.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Conclusions", "text": "We have presented a novel method for mining association rules from heterogeneous semantic data repositories expressed in RDF/S and OWL. To the best of our knowledge, this problem has only been considered to a minor extend. The intuition under the method developed is to extract and combine just the interesting instances (i.e. features) from the whole repository and flatten them into traditional transactions while capturing the implicit schema-level knowledge encoded in the ontology. Then, traditional association rules algorithms can be applied. We believe this type of learning will become increasingly important in future research both from the machine learning as well as from the Semantic Web communities. Initial experiments on real world Semantic Web data enjoy promising results and show the usefulness of our approach. As future work, we would like to apply generalized query patterns by using the ontology axioms, as well as to automatically discover interesting contexts and their association rules.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "In the area of social network analysis, one important issue is community discovery. A community in a social network is usually defined to be a densely connected sub-graph in the network. By detecting communities, it helps us to understand and exploit the networks more effectively. Especially in a bibliographic database, identifying communities from a co-authorship network can reveal academic activities as well as evolution of research areas; discovering communities in citation network can demonstrate the information diffusion within and between different research areas.\nThere have been several community mining algorithms proposed to identify meaningful communities from networks. These algorithms can be broadly classified into two main categories: graph partitioning based approaches [2,12,13] and modularity based approaches [3,4,11,15]. Identifying the communities within a network has become one of the major concerns of social network analysis which has various applications. In this paper, we are interested to discover topic-based collaborative communities from co-authorship network.\nSeveral studies have been proposed to analyze bibliographic databases. Zhang et al. [15] proposed the SSN-LDA model to discover flat communities from social networks by utilizing the topological information in social networks. Deng et al. [1] proposed a novel graph-based re-ranking model to improve the ranking of the retrieved documents with respect to a given query. Then the model was used to discover experts of a specific topic from the DBLP bibliographic data. Zaiane et al. [14] provided a new random walk approach to discover research communities with potentially collaborative relationship from the DBLP database [9]. An extended bipartite graph is built to model the relationships of authors and conferences. In order to include the topic information, the proposed model is further extended to be a tripartite graph. Then the random walk with restart algorithm was revised to calculate the relevance scores among researchers in the graph to group the highly-relevant researchers into the same community. Mei et al. [10] proposed the NetPLSA model which combined the statistical topic modeling and social network analysis to discover topical communities. The PLSA model proposed in [6] was exploited to get the weights of the predefined topics for each author. Thus, the topic similarity between each pair of researchers can be evaluated by comparing their topic weights. Moreover, the Harmonic function is used to evaluate the degree of collaborative relationship among each pair of researchers. Accordingly, an objective function is defined by integrating these two models. The topical communities are then discovered by minimizing the objective function.\nThe previous works [14] and [10] mentioned above are closely related to our work. In [14], for the members in a community discovered from the extended bipartite graph of author-conference relationship, they may have high weighted co-author relationship or often publish papers in the same set of conferences. However, the topics covered in each community are not explicitly specified. Moreover, although the members in a community discovered from the tripartite graph model have similar research topics, it is not necessary that they have strong co-work relationships. Likewise, although [10] regularize a statistical topic model with a harmonic regularization based on a graph structure, the concept hierarchy of topics covered in the communities is not shown explicitly.\nIn this paper, a mining strategy is proposed for discovering topic-based collaborative community. First, CONGA algorithm [3] is applied to discover overlapping collaborative communities in the collaborative network. By applying the hidden information in the external source CiteSeer X , the collaborative communities are further organized in a semantic level by automatically constructing a concept hierarchy of topic terms. Therefore, the collaborative communities corresponding to a research topic at arbitrary semantic level can be retrieved. In order to evaluate whether the constructed topic-based collaborative community is semantically meaningful, the first part of evaluation is to measure the consistency between the terms appearing in the published papers of a topic-based collaborative community and the terms in the documents related to the specific topic retrieved from other external source. The experimental results show that 81.61% of the topic-based collaborative communities satisfy the consistency requirement. On the other hand, the accuracy of the discovered subconcept relationship is verified by checking the Wikipedia categories. It is shown that 75.96% of the sub-concept terms are properly assigned in the concept hierarchy.\nThe remaining sections of this paper are organized as follows. The related works are discussed in Section 2. Section 3 describes the proposed strategies for discovering the topic-based collaborative communities. The performance study is presented and discussed in Section 4. Finally, Section 5 provides the conclusion and future works.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Community Discovery. The community mining algorithms can be broadly classified into two main categories: graph partitioning based and modularity based approaches. The partitioning approaches divide the vertices into different communities by minimizing the number of edges between the vertices in different communities, such as the min-max cut algorithm [2], normalized cuts algorithm [12], and spectral clustering algorithm [13]. On the other hand, the modularity based approaches provided a modularity measure to perform good data partitioning during the mining process. One of the representative methods is the Newman algorithm proposed by Newman et al. [11].\nThe limitation of the Newman algorithm is that it does not allow a node being assigned to more than one community. In reality, an individual may exist in more than one community to take on various roles, such as a blog user being a professional cook and an amateur photographer at the same time. For this reason, Gregory et al. [3] modified the Newman algorithm and proposed the CONGA algorithm, which introduced an operation for splitting a vertex. Suppose a node v is split into v 1 and v 2 , a virtual edge is constructed to connect v 1 and v 2 . The betweenness centrality of the virtual edge is called the split betweenness of node v. In each iterative step of the CONGA algorithm, either the edge with the maximum edge betweenness is removed or the node with the maximum split betweenness is split, depending on which one is greater. The CONGA algorithm is an extension of the Newman algorithm. Thus, it suffers from the huge computation cost for recalculating the betweenness of the nodes and edges repeatedly. For solving this problem, an improved version of the CONGA algorithm, which is named the CONGO algorithm, was proposed by the same author in [4]. In order to speed up the processing efficiency, instead of traversing every edge in the network globally, a parameter h is given to limit the search region locally when updating the betweenness after an edge was removed or a node was split. Document Clustering. Most traditional document clustering methods adopted the \"Bag of Words\" (BOW) model to represent a document. A document is thus represented by a term vector; and the similarity of two documents is measured according to their term vectors. However, this approach ignored the relationships between important terms that do not co-occur in the documents, such as synonyms.\nRecently, there is a growing amount of tasks on how to utilizing external background knowledge (e.g. WordNet and Wikipedia) to enhance document clustering [7,8,5]. Hotho et al. [7] used WordNet, a general ontology, to represent each document by a concept vector instead of a word vector. Furthermore, [8] and [5] considered Wikipedia is a more comprehensive resource to provide potential ontology which can be exploited for enriching text representation. Therefore, the mapping strategies were developed in [8] to match text documents to Wikipedia topics and further to Wikipedia categories. Then the text documents are clustered not only based on the similarity metric of document content but also the concept and category information. In [5], a document was modeled by a graph of terms with semantic links. By providing a semantic relatedness measure of terms according to Wikipedia, the Newman algorithm was performed to discover the communities of terms for extracting key terms in the document.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Topic-Based Collaborative Communities Discovery", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Definition", "text": "According to a given bibliographic dataset, the information of the co-authorship between researchers is modeled as a graph G(V, E). Each node v i in V represents a researcher. Besides, an edge e=(v i , v j ) in E connecting two nodes v i and v j if the two corresponding researchers have at least one co-publishing paper in the dataset. The constructed graph G is called a collaborative network.\nA densely connected subgraph in graph G(V, E) is called a collaborative community, whose nodes represent the researchers with strong co-work relationships. However, the collaborative communities only consider the co-author relationship as the basis of grouping researchers. In order to organize the researchers in a semantic level, a better way is to group the collaborative communities according to the research topics covered in the communities. Moreover, the research topics usually form a concept hierarchy as the example shown in Fig. 1. If the collaborative communities are further assigned to the concept hierarchy of research topics, a hierarchy of the topic-based collaborative communities can be constructed. As a result, the users can access the members in the same community not only by their co-author relationship but also the similar research interests at different concept level.\nIn a bibliographic dataset, suppose only the information of author, co-authors, and title is available for each publication, the challenge is how to extract the research topics covered in a collaborative community and construct the concept hierarchy of topics automatically. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Collaborative Community Discovery", "text": "We downloaded the bibliographic database from the DBLP website (http://dblp.unitrier.de/xml/dblp.xml.gz). Each data in the DBLP database contains the names of researchers, published paper, journal/conference, year and other related information.\nAmong the related works of community mining, it is limited that each vertex is assigned to exact one community in most studies. However, in the real world, many researchers have more than one research interest. It is possible that a researcher has ever co-published papers with other researchers in different domains. It is improper to assign the researcher to a collaborative community with specific topic. Therefore, the CONGA algorithm proposed by [3] is used to discover the densely connected subgraphs with overlapping allowed in the graph. As a result, a researcher with multiple research interests will appear in many collaborative communities.\nFig. 2 shows an example of the discovered collaborative community from a collaborative network, where the nodes labeled by A, B, C, etc. represent the researchers. If two researchers have any co-published paper, a solid edge will connect them. Accordingly, the black dotted-line circles imply the collaborative communities discovered by the CONGA algorithm. It is shown that the researchers A, B, and G all belong to more than one collaborative community.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "Fig. 2. Collaborative community with corresponding paper topics", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Concept Hierarchy Construction of Research Topics", "text": "A topic-based collaborative community is formed by the collaborative communities with a specific topic. The information of the topic covered by a collaborative community is implicit in the corresponding published papers. As Fig. 2 shows, the nodes in the middle are used to denote the titles of papers. A paper title is connected to the collaborative community which contains all its authors. Besides, the rightmost nodes in the Fig. 2 denote the implicit research topics of the papers, such as \"Data Mining\", \"Clustering\", and \"Social Network\" etc. If the research topics and the links between paper titles and topics can be extracted automatically, the discovered collaborative communities can be further grouped according the topics covered in their published papers. Using the collaborative communities {A, B, C} and {H, I} shown in Fig. 2 as an example, the authors in these two collaborative communities have never copublished any paper. However, both of the two collaborative communities have a paper with topic \"Data Mining\". Therefore, these two collaborative communities should be grouped into a topic-based collaborative community with topic \"Data Mining\".", "publication_ref": [], "figure_ref": ["fig_15", "fig_15", "fig_15"], "table_ref": []}, {"heading": "Fig. 3. Pseudo codes for discovering all the concept hierarchical paths", "text": "Under the limited information provided in a bibliographic database, only \"paper title\" best describes the content covered in a paper. Therefore, we perform the following processing to extract the potential topic terms. First, each paper title is processed by the basic text processing steps, including removing stop words and stemming. After that, the bigrams are extracted from the titles. The bigrams with frequency higher than a given threshold \u03b1 is chosen to be the topic terms.\nFrom the extracted topic terms, it is not easy to determine whether the topics of two papers are related. For example, suppose the title of paper A contains the topic term \"Data Mining\", while the title of paper B contains \"Sequential Pattern\". Although these two topic terms are different lexically, it is known that \"Sequential Pattern\" is an important research issue of \"Data Mining\" in computer science. For solving this problem, the external source CiteSeer X is used in our approach to construct the hidden concept hierarchy of the extracted topic terms.\nFor each pair of topic terms X and Y, the confidences of the association rules X\u2192Y and Y\u2192X are measured to decide whether there exists a hidden sub-concept relationship between X and Y. For getting the confidences of the association rules, each topic term and each pair of topic terms are used as query keywords, respectively, to get the numbers of documents that the two topic terms separate occurrences and cooccurrences in CiteSeer X . Thus, the conf(X\u2192Y) of association rule X\u2192Y is obtained from |D(X\u2229Y)|/|D(X)| where |D(X)| denotes the number of documents contain topic term X and |D(X\u2229Y)| denotes the number of documents contain both X and Y. If conf(X\u2192Y) is less than 1 and conf(Y\u2192X) is 1, it is implied that if a document contains Y, it must also contain X, but the inverse is not true. In other words, topic term Y is a sub-concept of topic term X. By considering the noise in real data, when deciding whether a topic term Y is a sub-concept of topic term X, the criterion is relaxed to require that both conf(X\u2192Y) and conf(Y\u2192X) are greater than a given threshold value \u03b2, and conf(X\u2192Y) is smaller than conf(Y\u2192X).\nThe sub-concept relationship among topic terms is then used to construct a concept hierarchy of the topic terms as shown in Fig. 1. The algorithm for discovering all the concept hierarchical paths of the topic terms is described as the pseudo codes shown in Fig. 3. Let Sub_concept(X) denote the set of detected sub-concepts of a topic term Algorithm CCH Input: all topic terms, Sub_concept(X) for each topic term X Output: concept hierarchical paths For each topic term X Call M_DFS (<X>, Sub_concept(X)); Procedure M_DFS (P, Sub_concept)\nFor each item t' in Sub_concept If (t' is a sub-concept of all the topic terms in P) P' = append item t' to P; Call M_DFS (P', Sub_concept(t')); else output(P); X. If Y is a sub-concept of X, the discovered hierarchy path is denoted as <X, Y>. If Z is both a sub-concept of X and Y, the constructed hierarchy path is denoted as <X, Y, Z>. Initially, the Sub_concept(X) is discovered for each topic term X. The procedure M_DFS() is called to construct all the concept hierarchical paths existing among the topic terms in a depth-first manner. Let P=<t 1 , t 2 , \u2026, t n > denote a discovered hierarchy path, where t i (i=1,\u2026, n) denote a topic term in the path. A topic term t' can be appended to the path only when t' is a sub-concept of all the topic terms in P.\nSince the sub-concept relationship has the transitivity property, only the maximum hierarchical paths have to be maintained. For this reason, the hierarchical paths which are subsequences of any other discovered hierarchical path are removed. Finally, a concept hierarchy of topic terms is then constructed by constructing a prefix tree structure for the discovered hierarchical paths. As shown in Fig. 1, the topic terms located at level 1 represent the most general concepts in the concept hierarchy. The topic terms at level 2 are sub-concepts of their parent. For example, the nodes in the subtree rooted at the node \"Data Mining\" are all related topic terms in the field of \"Data Mining\". Besides, the children nodes of the node \"Data Mining\" represent the sub-concepts in the domain of \"Data Mining\", such as \"Knowledge Discovery\", \"Decision Tree\" and \"Association Rule\".\nNext, according to the established concept hierarchy of topic terms, a collaborative community is assigned to the proper nodes in the concept hierarchy according to its published papers. Let C i .tt denote the set of topic terms in the titles of the papers whose authors are all in collaborative community C i . For each topic term t in C i .tt, if the term t exactly matches to the topic term of a node in the concept hierarchy, C i is then assigned to this node. Otherwise, the following process is performed to look up the most related topic with t. First, among all the topic terms represented by the nodes at level 1 of the concept hierarchy, the topic t i with the highest conf(t\u2192t i ) is identified. If conf(t\u2192t i ) is greater than or equal to the given threshold \u03b2, the above process will be performed recursively on the nodes in the subtree rooted at the node of topic t i . The process will continue until the confidence conf(t\u2192t i ) for each topic t i at the level is smaller than the threshold \u03b2. Then the collaborative community C i is assigned to the parent node of t i . If the highest conf(t\u2192t i ) obtained at level 1 is less than the given threshold \u03b2, the collaborative community C i is topic-undetermined according to term t. The task of assigning the collaborative community to the concept hierarchy will repeat until all the terms in C i .tt have been examined.\nFor a topic t in the concept hierarchy, the corresponding topic-based collaborative community consists of the members in the collaborative communities which are assigned to the subtree rooted at the node of t. Therefore, if the publishing papers of a collaborative community cover multiple topic terms, the collaborative community will belong to multiple topic-based collaborative communities. For each topic-based collaborative community, the number of papers of a member containing the topic term is divided by the total number of papers assigned to the topic term to get the participating degree of the member in the community. Moreover, the concentrate degree of the member is obtained by dividing the number of papers of the member containing the topic term into his total number of published papers.\nWhen the probability distributions of terms in the two set of documents are more similar, the JSD measure will get lower value. It is indicated that the words appearing in the papers assigned to topic t i is consistent with the papers searched by topic t j from the ACM digital library.\nIn the constructed concept hierarchy of topic terms, there are 42 topic terms located at level 1. For each topic term t i at level 1, JSD(B ti \u2225 B tj ACM ) is measured with all the terms at level 1 one by one. The measuring results are then sorted in ascending order.\nThe corresponding topic t j of the top 1 result represents the most consistent topic of t i in the ACM digital library. The evaluation result shows that 81.61% of the topic terms have themselves as their most consistent topics in the ACM digital library. If the top 2 most consistent topics in the ACM digital library are identified, 84.38% of topic terms themselves are covered. By observing the situations that the most consistent topic of a topic term t i is another topic term t j , it usually occurs when the topic-terms are crossdomain such as \"Neural Network\" and \"Data Mining\". The JSD between B {Neural Network} and B {Data Mining} ACM is lower than that between B {Neural Network} and B {Neural Network}", "publication_ref": [], "figure_ref": ["fig_2", "fig_28", "fig_2"], "table_ref": []}, {"heading": "ACM", "text": ". The reason is that many important keywords in B {Neural Network} , such as \"Supervised Learning\" and \"Markov Model\", also appear in B {Data Mining}", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACM", "text": ". On the other sides, the papers published in the collaborative communities assigned to \"Neural Network\" do not contain the popular keywords related to \"Neural Network\", such as \"Gaussian Process\" and \"Fuzzy Logic\". Therefore, the probability distributions of keywords between B {Neural", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Network} and B {Data Mining}", "text": "ACM is more similar than that between B {Neural Network} and B {Neural Network} ACM .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy of the Sub-concept Relationship", "text": "In this part of evaluation, we would like to measure the correctness of the discovered hierarchical path of the topic terms. Since there is no ground truth to compare with the constructed concept hierarchy, we would like to use the external source Wikipedia to validate the accuracy of our discovered sub-concept relationship. Let T_level2 denote the topic terms located at equal to or larger than level 2. For each topic term t in T_level2, it is used as a query term to search on Wikipedia. Let Category(t) denote the set of categories list in Wikipedia for t. If Category(t) contains any super-concept or which is the re-direction of any super-concept in the concept hierarchical path of t, the term t is considered to be properly assigned in the concept hierarchy. The evaluation result shows that 75.96% of the topic terms in T_level2 are properly assigned in the concept hierarchy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Works", "text": "In this paper, a mining strategy is proposed for discovering collaborative community with similar research interests. The CONGA algorithm is applied to discover overlapping collaborative communities according to the collaborative relationship of authors. In order to organize the collaborative communities at semantic level, the topic terms are extracted from the paper titles, which are automatically constructed into a concept hierarchy by applying the hidden information in the external source CiteSeer X . Therefore, the resultant topic-based collaborative community provided a semantic-meaningful and flexible view to explore the communities of authors in a bibliographic database. In the experiment, two evaluation methods are proposed to evaluate the topic consistency of a topic-based collaborative community and accuracy of the discovered sub-concept relationship. The experimental results show that 81.61% of the topic-based collaborative communities satisfy the consistency requirement. Besides, 75.96% of the sub-concept terms are properly assigned in the concept hierarchy.\nThe evolution analysis of communities and individuals is an interesting issue, which will discover the change of research interests of researchers, the change of the contribution of researchers to a collaborative community, the change of important topic terms. To take the information of publication time into account to detect the dynamic evolution of collaborative communities is under our investigation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "We propose a system that helps a person understand news articles on the web. When we browse a news article on the web, we sometimes find related articles at the end. These related articles help users get the details of the article. The system focuses on related events, not articles. We believe that obtaining related events is important for understanding a news article. The goal of this work is to support understanding of a news article. To understand an article, it is necessary to have background knowledge of the article such as the meaning of words.\nThe system can detect related events from a news article. The system generates the related events as the event arrangement as follows: first, when the system receives a news article, the system outputs related events in which the system detects events by date and important word. Second, users select the events based on their preferences. Users can get event arrangements by executing these operations.\nIn Section 2 we explain arranging events of news articles. Section 3 deals with how to arrange events by date and important words. In Section 4 we explain the system structure. In Section 5 we show examples of implementation and discuss the results from experiments. Finally, we comment on our proposed system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Arranging Events of News Articles", "text": "There are many related works for analyzing topics for news articles such as Topic Detection and Tracking (TDT) [1,2]. These works usually define a topic or an event, and examine with a corpus of text. In TDT, the notion of a \"topic\" is modified to be an \"event\", meaning a unique occurrence at a point in time. This notion of an event differs from a broader meaning of an event both in spatial/temporal localization and in specificity. However, the notion of topic depends on users.\nArranging events is defined as associating events in this work. In TDT, the notion of a \"topic\" was modified to be an \"event\", meaning a unique occurrence at a point in time. In other research, the notion of an event has a variety of meanings [3,4]. We use the same notion of an event as in TDT. Therefore, an event is composed of news articles. The proposed system detects events, not topics.\nPresentation of unrelated events is not helpful for users. Therefore, the system presents events that relate to the news article. Users select an event based on their interests from the presented events. Events are arranged by repeating event presentation and user selection.\nThe left path in Figure 1 shows an example of an event arrangement that is detected automatically. This example is an event arrangement of school closing by flu. However, an automatic detection method is not sufficient if a user is interested in the route of infection shown in right path in Fig. 1.\nWhen documents are clustered, a method that is often used involves a system collecting documents of a certain period, clustering documents at the point in time, and displaying the result. However, this method must change the system when the system uses new documents and clusters them by user preference. For example, when a system uses new documents, this clusters in each case or uses an applicable method like the leader-follower method [5]. Since the system receives users' requests, it detects events. If the system computes all collected articles or all requests, the computation time is larger. Therefore, the system restricts the computing of articles by retrieval using important words.\nEvent detection is similar to classification learning. It is well known that preparation of supervised data is costly. Therefore, there have been many studies to reduce the cost. Various methods for reducing the cost have been examined. One of the solutions is to use a method with user feedback. Methods with user feedback can be divided into four types [6,7]. The proposed system uses users' selection. Therefore, the system is similar to methods with user feedback .\nFigure 2 shows how events are arranged by event presentation and user selection. Event arrangement is consisted as a graph. A node is an event, such as e i\u2212j in Fig. 2. If two events are releted, the events have an edge. First, when the system receives a new article, it presents related events; e 1\u22121 , e 1\u22122 , and e 1\u22123 . Second, Users select the most interesting event (e 1\u22121 ) from the presented events. Eents related to the selected event are presented such as e 2\u22121 , e 2\u22122 , e 2\u22123 , and e 2\u22124 . And, w 1 , w 2 , and w 3 on the edges are important words for detecting each event. By repeating event presentation and user selection, users can receive unique arrangement of events as a graph.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_15", "fig_15"], "table_ref": []}, {"heading": "Method for Arranging Events", "text": "We propose a method of arranging events, repeating four steps; i) important word detection; ii) article retrieval; iii) event detection; iv) event selection. The system retrieves articles using important words, and detects events from retrieved articles. Events are arranged by repeating event detection and user selection.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Important Word Detection from an Event", "text": "Important words are used for event detection. The system filters important words using word classes, since an event is a unique occurrence at a point in time. Important words mean date, location, or what happened. Proper nouns represent locations and actors, and verbs represent actions related to an event. Therefore, important words require proper noun or verb classes. The system excludes some words belong to specific word classes such as a symbol, a particle, an auxiliary verb and a conjunction. It is because they do not represent an event. News articles are delivered as soon as an event occurs. Therefore, we do not use the date on the article as the time an event occurs.\nOur system computes the evaluation values of each word, and selects important words that have high-evaluation value. The values are calculated using term frequency -inverse document frequency (tf\u2022idf). When the system receives a class of news articles as an event, it calculates sum of term frequency -inverse document frequency (tf\u2022idf) as shown below:\ntf \u2022 idf (w, e) = N \u22121 i=0 tf \u2022 idf (w, i) ( 1 )\nWhen the system gets an event e which has N articles, it sums up tf\u2022idf values of each article i. If there is no important word w in an article i, tf \u2022 idf (w, i) value is zero. In order to calculate the idf, the system counts important words in all the articles that the system has.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Article Retrieval from Important Words and Date", "text": "This subsection explains how to retrieve articles for event detection. Our idea is to use important words for the retrieval. News articles published near the date of the input article are given priority and retrieved. The calculation of the priority is based on a concept that related events occur in about the same time. When the system recieves an event, the system processes in the same way. An event date is an average date of news articles that contains the event.\nAnother method is to compute the similarity of events, which may have higher recall and precision than simple retrieval, but it takes much time. We focuses on processing time and our system uses the simple retrieval method for detecting events, though various methods for reducing processing time have been proposed [8,9] and it is difficult to apply them to our system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Event Detection", "text": "We explain how to detect relevant events from retrieved articles. In a cluster of retrieved articles, our system finds a news article B that is the most similar to the given news article A. The system computes the similarity of two articles using all words including important words in the articles. The similarity in the system is calculated with cosine similarity, and distance function uses group average Fig. 3. Event detection from article most similar to input article method. The system finds a cluster including article B by using a method such as the leader-follower method that is a type of clustering algorithm. The following is details about this method in our proposed system. step 1. Find an article B that is the most similar to the given news article A. step 2. Make a cluster that includes only news article B. step 3. Add a news article to the cluster if the similarity of the news article is higher than the threshold. step 4. Perform step 3 for all retrieved articles.\nAfter this process, the system obtains one cluster that includes news article B, and the cluster is assumed to be a related event. An advantage of this method is that the number of computation is less than the one using the leader-follower algorithm, because our system finds only one cluster. Figure 3 shows an example of event detection from an given news article A. News articles A and B in Fig. 3 are the most similar in all retrieved articles. The gray circle is a cluster detected as an event.", "publication_ref": [], "figure_ref": ["fig_28", "fig_28", "fig_28"], "table_ref": []}, {"heading": "Implementation and Experiment", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment of Event Arrangement System", "text": "The structure of our event arrangement system is shown in Figure 4. First, users input a news article into the system, and the system detects important words from the news articles. Second, a news articles that include the important words are retrieved from news-article databases. Finally, the system shows related events from the retrieved results. Therefore, users can obtain events that relate to the browsing news article. Then users select an interesting event from the given events. A retrieval module finds news articles excepting the selected events. Therefore, the system does not detect events similar to the presented events.\nDividing a sentence into words is necessary for detection of important words. Japanese is not written with spaces between words; therefore, we use MeCab 1 , which is a Japanese language morphological analysis program. After sentences are divided, the system filters important words, and MeCab evaluates word classes.", "publication_ref": [], "figure_ref": ["fig_29"], "table_ref": []}, {"heading": "Experiment", "text": "The purpose of this experiment is to ascertain whether the system is sufficient to obtain an interesting event arrangement for each user. The proposed system uses articles in six news sites. Table 1 lists the six news sites and the number of articles in each site. Period was from November 1st, 2008 to December 31st, 2008. We input a browsing article into the system, and we examine how events were presented.\nWe show how our system arranges a news event considering user's needs. Before the experiment, the system collects new articles from six Japanese news sites (shown in Table1). In this experiment, a user wanted to know the occupation of the news titled \"Flights have resumed at Bangkok's international airport after anti-government protesters ended their blockade\". The original title is Japanese.This English title is the one of \"sky news 2 This article explains that \"A Thai Airways domestic flight landed at 0715 GMT and a plane bound for Sydney left Suvarnabhumi at 1145 GMT. The occupation of the international and domestic airports left 300,000 tourists stranded in Bangkok for more than a week. \". Figure 5 shows the result that the system recieves the input article. The event in Fig. 5 (1) corresponds to the input article. The system presented four events. Size of the important words in each event was proportional to the evaluation values of the important words. The horizontal axis means when events occur. The presented events can be move by hand.Figure 6 shows a situation in which a user selected the event in Fig. 5 (2). The system presented events related to the selected event as shown in Fig. 6. Then users could obtain event arrangements as shown in Figure 7.\nThe event arrangement in Fig. 7 was detected by a preferential selection about the past situation in Bangkok. Figure 8 shows another event arrangement. A user could obtain this event arrangement if one selected \"flight cancellation\" in Fig. 6, and selected \"Nagoya\". Presented events changed from events about Bangkok to events about airports and flight.", "publication_ref": [], "figure_ref": ["fig_116", "fig_116", "fig_117", "fig_116", "fig_117", "fig_53", "fig_53", "fig_175", "fig_117"], "table_ref": ["tab_2"]}, {"heading": "Discussion", "text": "The system generated an event arrangement as shown in Fig. 7 by repeating event selection. Users can obtain important words such as Bangkok, close, meeting, and support. These important words help users understand the changes in events. From the results of Fig. 7 and Fig. 8, we confirmed a change of an event arrangement by user selection. Users can get event arrangements along users' interest because the system show some events and users select an interesting event.\nThere are two scenarios when the number of articles that are included in an event is small. One is that important words are not sufficient to detect an event.\nIf an important word is a general word, it is difficult to retrieve related articles. The other scenario is that the event is not well known. In this scenario, the number of articles about the event is small. Therefore, the system does not find many relative articles. To solve these problems, evaluation values are needed to improve. For example, it is believed that evaluation values affect the number of retrieved articles.\nWe examined the effect of word classes. Table 2 lists important words and their evaluation values. The evaluation values in the left table are values using word classes. The right one lists the values not using word classes. \"Day\" and \"service\" are not effectual words for detecting events because they are general words. By using word classes, the system can detect important words excluding general words.  ", "publication_ref": [], "figure_ref": ["fig_53", "fig_53", "fig_175"], "table_ref": ["tab_5"]}, {"heading": "Conclusions", "text": "We proposed a system for supporting understanding news articles by arranging events. The system achieves subjective event arrangements by the loop consisting of important word detection step, article retrieval step, and event detection step. Each user can effectively use the event arrangement for understanding news articles. We believe that automatic event detection is not sufficient because users' interests are different. The event arrangement needs to be generated to each user.\nIn the system, users can arrange events based on their preferences using events recommended by the system. The event arrangement can effectively reflect the interest.\nWe examined that the system can properly find events by filtering and clustering the articles from articles relevant to selected keywords. In order to track subjective events users need to simply select preferred events from a graph of the events presented by the system. The experimental results using actual news articles show that the proposed system is effective to detect useful events for understanding news articles.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Aim of the Present Research", "text": "Our goal in this research is to provide with a multi-agent architecture capable of automating the process of affiliation in networks of Community Websites (CWs). This process leads to an increase of visitor revenue as well as quality for CWs. Improvement on quality of writing is measured by feedback from visitors, potential affiliates, and page ranking. Increase of the visitor revenue comes from interlinking itself.\nAffiliation processes, always part of the process of launching a CW, give birth to a need for automation. Furthermore, new blogs being born everyday make the number of potential affiliates soar. Finding the right affiliate can prove difficult. Affiliation links indeed require an explicit effort compared to that required for permalink ones [1].\nIn a previous paper [2], we proposed a multi-agent architecture capable of automating this activity, to reduce user's load in time-consuming research and negotiation processes for affiliation. Our system searches for potential affiliate CWs and deals with the issue of equity in partnership, as well as quality expectations, before proposing affiliates to the users. The whole research and negotiation part of the affiliation process becomes automated, thereby saving time for the user. In the case of blogs, it takes the idea of blogs being agents [3] one step further.\nIn this paper, we deepen the definition of the environment (Sect. 2), and show how our system's simulator can be easily configured and run. We show sample results for a blog community, thereby demonstrating that in this sample case the practical implementation of our system can efficiently reduce user's load in the affiliation process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System Description", "text": "The practical use case goes as follows:\n1. User connects to the broker/provider. If necessary, the user registers the site and information about it, unless the provider is already the blog/site's hosting service. 2. User requests a list of potential affiliates, entering the following data:\n(a) Desired minimum fairness of the affiliation -in general, or in terms of visitor revenue, quality, relative importance, and so forth. (b) Available spaces to show affiliate links on his own site. 3. System outputs a list of Potential Affiliates (PAs), or none if too unfair. 4. User requests affiliation to one or more PAs and wait for their approval.\nThe partner PAs need not worry about negotiating, or the user's site being irrelevant to their own expectations. In very simple terms, this use case resembles that of a social networking site, but for community websites.\nOur system is also a simulator which generates a cluster of agents, and sample sites based on patterns of particular statistics or expectations. It can be run for any number of iterations. The initial data, as well as the heuristics, depends on the environment chosen. When the simulation ends, the system shows the general evolution of the sites in terms of visitor revenue, quality revenue, and other data if needed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definitions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Community Website", "text": "A CW is defined by the following characteristics:\n1. Its contents are relevant to one particular subject, or several subject of the same category. Ex: one programming language, or mainstream IT. If they are personal sites, they express only one facet of the webmaster [4]. 2. It aims for quality of opinions and utterance of relevant and specialist information.\nSites such as daily-life blogs are therefore excluded by this definition. However, blogs of sociological type III [4]), defined as community blogs, are CWs according to our definition. CWs are set up on the Internet in order to share knowledge and opinions, but before altruism, one of the main objectives is attention in the community [5] (it does not necessarily relates to ego). Therefore, most of the time setting up a community site or blog calls for the process of finding affiliates. Ranking and being well-referenced on search engines calls for quality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Affiliation", "text": "Affiliation consists in interlinking two websites. For blogs, it is a contract passed between the two, a social tie omnipresent in nowadays' blogs. By placing a link to another weblog in one's blogroll, one assumes that the author either endorses that weblog, wishes to promote it, or claims to read it on a regular basis [1]. This has also been true ever since websites existed on the Internet. Besides friendship or common interest, affiliation seeks to share visitors, as well as raising site awareness mutually. What we define further in this paper as a category is close in concept to an affiliation group [6].\nRespective placement (Sect. 4.3) of each other's link on the blogs/websites is considered as settlement of the affiliation deal.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Visitor Revenue", "text": "We call Visitor Revenue (VR) the revenue in popularity that two websites engaging in affiliation seek to increase. It is a general variable that can be associated to hits, pageviews, unique visitors, or real human visitor revenue. In the two latter cases, defining the resource becomes more complex as it may require to define a reader's behavior, possibly with reader agents.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quality", "text": "Quality is the other objective of CWs. It defines the relevance and richness of information itself. Quality can be a ranking based on human classification, usage information, connectivity, or non-affiliated experts [7]. To simplify this notion we consider quality being equivalent to a page ranking, be it in the system itself by different users, or a public page rank. In the experiment featured in the present paper, quality is emulated as a logarithmic function of the visitor revenue, as it can be done approximately for search engines' public page rankings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Society of Community Websites", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Matchmaking and Brokering Architecture", "text": "We define a Matchmaking and Brokering Architecture (MBA) (see Fig. 1) fairly similar to that of [8], with a Requester, a Broker (or Server), and a Matchmaker. However, in simulation, we choose not to implement the Requester as a singlethreaded agent such as in [9].\nIt is unrealistic that the Requester, which we describe in our architecture as the Community Website Agent, should possess its own thread running indefinitely on a machine. To simulate intelligent agents and real-time, processing an array of state machines [10] in a loop is sufficient. On each pass, the unit's object is checked for its state and an action course to decide. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Community Website Agent Layer", "text": "The CW Agent consists of two parts:\n1. The user, holding expectations, and will to request.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "The knowledge base about the CW. It is stored in a Site Pool Slave Agent (Sect. 3.3).\nIn practical application, the Requester is indeed a human agent. In simulation, we assimilate them as the same entity: incentives and knowledge are both stored and processed in the Site Pool Layer. Whichever the case, the difference between our architecture and past data mining agent architectures such as [11] is that the data needs not be accessed by external agents which will generate a lot of read accesses and make each request; that part of processing is saved since it is done by the slave database agents themselves (see following section). Since every affiliation request would have to generate a write access either way, to make sure data is up-to-date before sending the request, we use this access to set up a flag for request, and make no unneeded read accesses.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Brokering: Site Pool Layer", "text": "The Site Pool Slave Agents (SPSAs) are agents with a knowledge base about a number of sites. Since every site in each SPSA is independent from the other, it becomes scalable as horizontal sharding . Furthermore, the MiLogPlatform [12] we use for the simulator allows to easily spawn and duplicate agents over networked computers. An infinite loop runs on the SPSAs to simulate virtual Requesters (Sect. 3.1). Once an incentive is set to 1 for one of the sites in the SPSA's array, the site will be part of the next joint request of the current iteration in the SPSA, to the SPMA. The Site Pool Master Agents (SPMAs) provide control over one or several SPSAs, as well as service or connection to the user's Web interface in practical application. The SPMAs take requests from users, update the incentive state of the site in the concerned SPSA, then get a joint request from the SPSAs that they will split and submit to the Category Pool Layer (Sect. 3.4) (CPL). They get a response and transmit it to the user. Upon affiliation agreement, they update data again in the SPSAs and submit new placement data (Sect. 4.2) to the CPL. The internal state agents make the final decision and update.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Matchmaking: Category Pool Layer", "text": "The Category Pool Slave Agents (CPSAs) hold data similar to that contained in SPSAs, but the sharding is done by category (Sect. 4.2). They receive requests transmitted by SPMAs to Category Pool Master Agents (CPMAs) from the latter, and are the ones who run the matchmaking algorithm. CPSAs also update their data every N iterations. For example, if an iteration is a day, it is sufficient to update every month. Upon an affiliation agreement, they update the placement data on another request from SPMAs.\nThe Category Pool Master Agents (CPMAs) are used by the SPMAs to find new affiliation opportunities for the requester site. They control the CPSAs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Control Layer", "text": "The Meta Agent acts as a directory of all Master Agents in the system so that brokers can find matchmakers.\nThe Test Coordinator Agent monitors the runs in the simulation and collects data from SPSAs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interface", "text": "Agents in the brokering and matchmaking layers share a common inter-agent request interface, for CW and blogging platforms (ex: Blogger, Wordpress) to provide easily with the service to their users. Matchmakers can be independent as well (ex: Blogcatalog). This interface can be seen as a transparent web service.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "User Load Reduction Hypothesis", "text": "The most common pattern in the affiliation process goes as follows: Whereas only the following steps should be needed, as in our system: 1. Incentive of looking for a Potential Affiliate (PA). 2. Define expected quality and/or visitor revenue from partner. 3. Go through output of the system, make contact immediately. 4. Final agreement.\nTherefore, we assume that the system significantly reduces user's load in the search for affiliates and the following negotiations, if it can succeed in simulation when configured with proper initial data and heuristics about the target environment.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Knowledge Base", "text": "This is the knowledge base contained in every SPSA about each site. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "S", "text": "The set of all sites in the system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C", "text": "The set of all categories. s's quality rank\nHs/ h\u2208H h \u2264 1 The placement set h \u2208 Hs \u21d2 h \u2208 [0, 1]\nValue associated to an available placement only a \u2208 [0, iterationsrun] Activity rate, an average period in Iterations after which s looks for new affiliates", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Placement", "text": "The space available to put each other's link on the page, as well as its position and format, is an important matter. There exists no general rule to determine which placement is best on a web page in general: it is influenced by presentation, as well as the number of affiliates already present, and numerous other factors.\nFor placement of the link, we use a set of probabilities (H) of being accessed from the affiliate, which is independent from the position, format, space, presentation, number of pages the link is to be shown on. In practical application, either the users can fix the values in their H, or this can be done automatically (Sect. 6).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Matchmaking Algorithm", "text": "The matchmaking layer receives all the data about the site from the brokering layer. The CPMA dispatches the requester site's data to the appropriate CPSAs' queues, merge the result arrays and send response to the SPMA. We name the utility function u. dealss \u2190 empty array for all s \u2208 CP SA do deals s,s \u2190 empty array  \nutility s \u2190 0 for all h \u2208 H do for all h \u2208 H do u \u2190 h \u00d7 v \u00d7 q \u2212 h \u00d7 v \u00d7 q if e \u2264 u \u2264", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simulation and Sample Results", "text": "In this simulation we show the evolution of a young blog community network.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Initial Data and Heuristics", "text": "Since the goal of our system is to reduce user's load, we expect at least a behavior similar to that of a real blog community: when compared to a network where no affiliation occurs, overall inequality in visitor revenue as well as quality (here, a page ranking, simplified model and function of v), should shrink. Overall visitor revenue should increase.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We made two runs of the system, one without affiliation process at all (first run, Fig. 3 and 4), and one with each blog requesting periodically for an affiliation (second run, Fig. 5 and 6). Each graph shows VR per Iteration (VRpI) of a blog  in function of the blog id (500 ids). The ids are ordered ascendantly in function of the VRpI so that the graph can be easily read.\nIn the first run, blogs evolve independently and top quality blogs (right edge of the curve) reach almost 400% of the VRpI of non-popular blogs (left edge of the curve) with a VRpI of 8000 to 2500, with no exchanged visitors. Since there is no cooperation and the rich get richer, the shape of the curve is a long tail. In the second run, blogs help each others. This time, the VRpI of top blogs (right edge of the curve) is only 132% of the VRpI of least popular blogs, with a VRpI of 28000 to 22000. We can see, as expected, a huge overall increase in VR, thanks to the exchange of visitors coming from affiliation between the blogs. Therefore, the system meets our expectations in this sample experiment, it has managed in reducing VR inequality in a community of blogs.\nWhat is interesting is also that we have verified in simulation the results of the real-world BlogDex study [1], since the shape of our curve remains a long tail. Even if overall inequality has decreased, the rich still tend on getting richer. Encouraging change of this behavior will be the subject of a further paper.", "publication_ref": [], "figure_ref": ["fig_28", "fig_116"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have developed an architecture for a system capable of relieving the user of the load of searching and negotiating in the process of affiliation in a network of community websites. Our simulator, of which we have given sample results, makes us able to easily see the evolution of the blogs/sites in a network. Using such a system, users become able to target easily their affiliates and focus more on their writing and contents, and increase the quality of their sites.\nWe consider adding fair counterparts to affiliation contracts, for sites with other advantages than visitor revenue and quality. Also, other heuristics than public page ranks, such as the Eigen Rumor Algorithm [13], can be input in the simulator for our system. We also intend on testing other patterns of communities after harvesting more data. We will discuss the subject of quality, influence and authority more precisely in a further paper.\nIn another paper [14], we have been developing a system capable of fetching detailed statistics about the real visitor revenue, the click probabilities (see H in Sect. 4.3), and push advertisement links automatically. We consider plugging it on the present system, for affiliation. This will also be the subject of a paper to come. Moreover, as the use of trackback and similar tools broadens on the Internet, it may prove useful to extend the system to the research and negotiation for trackbacks and references on article pages.\nFinally, as of now, the system is not capable of finding sites which are exterior to it. This feature can be developed by setting up brokers/providers that will, instead of requiring registration, crawl the Web. As well, for keywords and categories, recent advances in relational learning [15] could be applied to communities, in order to sharpen the different fields. We are considering this research as well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The semantic similarity of words has become a topic of many research fields such as artificial intelligence, biomedicine, linguistics, cognitive science, and psychology. Essential for human categorization and reasoning, semantic similarity is extensively used in a variety of applications, like words sense disambiguation [14], detection and correction of malapropisms [1], information retrieval [4], automatic hypertext linking and natural language processing. Several applications to the field of artificial intelligence are discussed in [16]. However, despite numerous practical applications today, its theoretical foundations lie elsewhere, in cognitive science and psychology where it was the subject of many investigations and theories (e.g., [17]). Let take a current example of peer-to-peer networks [3] into which semantic similarity has found its way. Assuming a shared taxonomy among the peers to which they can annotate their content, similarities among peers can be inferred by computing similarities among their representative concepts in the shared taxonomy. In this way, the more two peers are similar, the more efficient it is to route messages toward them. Numerous similar applications are the reasons for the increasing interest in this subject, whose ultimate goal is to mimic human judgement regarding similarity of word pairs. Semantic similarity of words is often represented by the similarity between the concepts associated with the words. Several methods have been developed to compute word similarity, mostly those operating on the taxonomic dictionary WordNet [2] and exploiting its hierarchical structure. But the majority of them suffer from a serious limitation; they only focus on the semantic information shared by those concepts, i.e., on the common points in the concept definitions. The increasing need for better measures and the new study area of semantic differences between words has led us to this study in the hope of upgrading existing semantic similarities. In particular, we combined traditional WordNet based semantic similarity measures with the idea of the \"similarity between entities being related to their commonalities as well as to their differences\", in order to improve the performance of WordNet based similarity measures and to obtain better results for applications using semantic similarities.\nThe paper is structured as follows. The next section reviews some background knowledge and related work. Section 3 describes our model, as well as the modified metrics. Section 4 discusses the results of an experiment, and section 5 summarizes our work, draws some conclusions, and outlines future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Similarity", "text": "The key to calculating semantic similarity lies in simulating human thinking behavior. Semantic similarity of words is determined by processing first-hand information sources in the human brain. Semantic similarity is a concept whereby a set of documents or terms within term lists are assigned a metric based on the likeness of their meaning / semantic content. Some studies have tried to assess the semantic proximity of two given concepts in order to improve the semantic similarity computation. These studies focus on similarity and they use synonymy 1 , hyponymy 2 [19], meronymy 3 and other arbitrarily typed semantic relationships. These relationships can be used to connect concepts in graph structures. They are the key ideas behind measures developed to assess the semantic similarity of concepts, i.e., how much one concept has to do with a different one. However, the measures tend to focus on the common points in the concepts' definitions; they rarely consider semantic differences, and this leaves a big gap in the semantic similarity computation process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "WordNet", "text": "A number of semantic similarity computation methods operate on the taxonomic dictionary WordNet and exploit its hierarchical structure. WordNet [2] is a machine-readable lexical database that is organized by meanings, and it was developed at Princeton University. Synonymy, hyponymy, meronymy and many other relationships between concepts are represented in this lexical network of English words. WordNet, as an ontology, is intended to model the human lexicon, and psycholinguistic findings were taken into account during its design. It is classified as a light-weight ontology, because it is heavily grounded on its taxonomic structure employing the IS-A inheritance relation, and as a lexical ontology, because it contains both linguistic and ontological information [9]. Figure 1 taken from [13] shows a fragment of WordNet's structure.\nNouns, verbs, adjectives, and adverbs are each organized into networks of synonym sets (synsets) each representing one underlying lexical concept and are interlinked with a variety of relations. A polysemous 4 word will appear in one synset for each of its senses. The backbone of the noun network is the subsumption hierarchy (hyponymy/hypernymy), which accounts for close to 80% of the relations in WordNet.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Semantic Similarity Measures and WordNet", "text": "Based on WordNet and depending on the elements taken into consideration, semantic similarity measures can be classified into two different types: edge-based and node-based semantic similarity measures.\nAn intuitive way to quickly compute the semantic similarity between two nodes of a hierarchy is to count the number of edges in the shortest path between these two nodes. The idea behind this is that the semantic distance of two concepts is correlated with the length of the shortest path to join these concepts. This measure was first defined by Rada in [12]. However, it relies upon the assumption that each edge carries the same amount of information, which is not true in most ontologies [13]. Many other formulas have since extended Rada's measure by computing weights on edges by using additional information, such as the depth of each concept in the hierarchy and the lowest common superset, or subsumer (lcs) [18]. For example, in Figure 1, the lcs between the concepts nickel and dime is the concept coin.\nThe measures which focus on structural semantic information (i.e., the depth of the lowest common superset (lcs(c 1 , c 2 )), the depth of the concept's nodes, and the shortest path between them) are called edge-based similarity measures. The Wu & Palmer [18] and Leacock & Chodorow [6] similarity measures are bases in a linear model, whereas Li et al.'s approach [7] combines structural semantic information in a nonlinear model. Li et al.'s model empirically defines a similarity measure that uses the shortest path length, depth, and local density in a taxonomy. They include two parameters which represent the contribution of the shortest path length and the depth of the lcs in the similarity computation process.\nAnother way to compute the similarity between two nodes is by associating a weight with each node. Such similarity measures are called node-based similarity measures. From the perspective of information theory, this weight represents the information content (IC) of a concept. IC can be considered to be a measure that quantifies the amount of information a concept expresses. The more specialized a concept is, the heavier its weight will be.\nThe literature contains two main ways of computing information content. The most classical way is Resnik's approach with a corpus [13] \nIC(c) = \u2212 log p(c) ( 1 )\nwhere p(c) is the probability of concept c in the taxonomy. Seco's approach [11] exploits the notion of intrinsic IC which quantifies IC values by scrutinizing how concepts are arranged in an ontological structure\nIC(c) = 1 \u2212 log (hypo(c) + 1) log (max wn ) (2)\nwhere hypo returns the total number of hyponyms of a given concept c and max wn is a constant that indicates the total number of concepts in the corre-  [11].\nsponding\nIn 1977, Tversky presented an abstract model of similarity [17] that takes into account features that are common to two concepts and features specific to each. That is, the similarity of concept c 1 to concept c 2 is a function of the features common to c 1 and c 2 , those in c 1 but not in c 2 and those in c 2 but not in c 1 . Admitting a function \u03c8(c) that yields the set of features relevant to c, he proposed the following similarity function:\nSim tvr (c 1 , c 2 ) = \u03b1F (\u03c8(c 1 ) \u2229 \u03c8(c 2 )) \u2212 \u03b2F (\u03c8(c 1 )/\u03c8(c 2 )) \u2212 \u03b3F (\u03c8(c 2 )/\u03c8(c 1 )) (3)\nwhere F is some function that reflects the salience of a set of features, and \u03b1, \u03b2 and \u03b3 are parameters provided for differences in each component. According to Tversky, similarity is not symmetric, that is, Sim tvr (c 1 , c 2 ) = Sim tvr (c 2 , c 1 ), because humans tend to focus more on one object than on the other depending on the way the relationship direction is taken into consideration during the comparison. For example, regarding the concept dime in Figure 1, it is logical that one of it's most related concepts is nickel, but the same is not true in the opposite direction. The concept nickel is also like gold, metal, etc.\nThe Pirr\u00f3 & Seco [11] similarity metric is based on Tversky's theory [17] but from an information-theoretic perspective. This measure achieves very good results in the comparison to human judgments when it is combined with the notion of intrinsic information content.\nSim P &S (c 1 , c 2 ) = 3IC(lcs(c 1 , c 2 )) \u2212 IC(c 1 ) \u2212 IC(c 2 ) if c 1 = c 2 1 if c 1 = c 2 (4)\nDespite all this previous work, WordNet based semantic similarity measures still have problems, which we will discuss in the next section.\n3 Menendez-Ichise Model", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Approach", "text": "Most of the WordNet based semantic similarity measures just take into consideration semantic commonalities among concepts for computing their values. The strength of semantic differences has been diminished or not exploited at all. Having all these elements in mind and considering the current structure of Word-Net, we propose the Menendez-Ichise model. In this section, we introduce our model and its application to traditional WordNet based similarity metrics. The modifications to those metrics are founded on Tversky's feature-based theory of similarity [17].\nOur model supports to be a specialization of Tversky's featured-based theory applied to traditional WordNet based similarity metrics. Paraphrasing Tversky, we state that: \"the similarity between two entities is related to their commonalities as well as to their differences\", and our general model is described by the following expression:\nSim(c 1 , c 2 ) = \u03b1 * Comm(c 1 , c 2 ) \u2212 \u03b2 * Dif f (c 1 , c 2 ) (5\n)\nwhere Comm(c 1 , c 2 ) stands for commonalities, Dif f (c 1 , c 2 ) the differences, and \u03b1 and \u03b2 tuning factors (0 \u2264 \u03b1, \u03b2 \u2264 1) that represent the importance of the commonalities and differences in the model. Because WordNet's structure is represented by an undirected graph we can't avoid assuming symmetry where there is none. The use of semantic differences for computing semantic similarity is a novel approach. In the next section, we explain how we applied our model to WordNet based semantic similarity measures.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Differences in WordNet Based Metrics", "text": "The main features considered by WordNet based similarity metrics are, the distance between nodes and the weight of the nodes. This in turn leads to two different approaches: edge-based and node-based, as mentioned above.\nIn our model, independently of the approach used, we consider the information from the root 5 to the lcs to be the semantic commonalities of the concepts c 1 and c 2 ; and the rest of the information from the lcs to each of the concepts c 1 and c 2 to be the semantic differences. Hence, from the perspective of an edgebased approach, the differences are related to the shortest path between the two concepts. In node-based approach, the differences are related to the information contained in the nodes representing the concepts that it is not contained in their lcs. For example, regarding the concepts nickel and dime in Figure 1, the semantic commonalities are in their lcs, i.e, the taxonomy subgraph from the root to lcs(nickel, dime) = coin. The semantic differences between both concepts is enclosed in the taxonomy subgraph from lcs(nickel, dime) to both concepts but without any information from the root to coin.\nEquation 6 is a modification to the traditional length and path metrics where we consider the first term to be the semantic commonalities between the concepts and the second term to be their semantic differences. Previous formulas consider either of these features, but not both.\nSim length (c 1 , c 2 ) = \u03b1 * 1 2 * depth(lcs(c 1 , c 2 )) \u2212 \u03b2 * 1 length(c 1 , c 2 ) (6)\nThe Wu & Palmer and Leacock & Chodorow measures rely on the length of the shortest path between two synsets. Equations 7 and 8 consider the semantic differences to be the distance between these two synsets, which is not taken into consideration in their original formulation, and each case is normalized with a different normalization factor. For Wu & Palmer measure, this means the addition of the concepts' depths in the taxonomy; and for Leacock & Chodorow metric, it means twice the taxonomy depth.\nSim wup (c 1 , c 2 ) = \u03b1 * 2 * depth(lcs(c 1 , c 2 )) depth(c 1 ) + depth(c 2 ) \u2212 \u03b2 * length(c 1 , c 2 ) depth(c 1 ) + depth(c 2 ) (7\n)\nSim lch (c 1 , c 2 ) = \u03b1 * (\u2212 lg( depth(lcs(c 1 , c 2 )) 2 * \u03bb )) \u2212 \u03b2 * (\u2212 lg( length(c 1 , c 2 ) 2 * \u03bb )) (8)\nThe modified Resnik measure considers the semantic commonalities to be the information content of the lcs and the semantic differences to be the information content encompassed by concepts, minus the one already considered in the lcs. The modified Jiang & Conrath similarity expression Sim j&c (c 1 , c 2 ) is identical to the one obtained for Resnik's measure, Equation (9), and it is a generalization of the Pirr\u00f3 and Seco similarity measure, Equation (4).\nSim P &S \u2282 Sim Res (c 1 , c 2 ) = Sim j&c (c 1 , c 2 ) (10)\nAccording to Lin [8] \"the similarity between c 1 and c 2 is measured by the ratio between the amount of information needed to state the commonality of c 1 and c 2 and the information needed to fully describe what c 1 and c 2 are\". In Equation (11), we add the semantic differences as the information content in each concept minus the one already considered in the lcs divided by the information needed to fully describe the concepts.\nSim lin (c 1 , c 2 ) = \u03b1 * 2 * IC(lcs(c 1 , c 2 )) IC(c 1 ) + IC(c 2 ) \u2212\u03b2 * IC(c 1 ) + IC(c 2 ) \u2212 2 * IC(lcs(c 1 , c 2 )) IC(c 1 ) + IC(c 2 )(11)\n4 Experiments and Results", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experimental Settings", "text": "The purpose of the experiment was to evaluate the new semantic similarity measures and to establish a baseline for comparison of their results with those of the original versions. We used the human judgments of Pirr\u00f3 and Seco experiment [11] (P&S in the following) for the word pairs of the Miller and Charles dataset (M&C in the following).\nUnfortunately, there is a distinct lack of standards for evaluating semantic similarities, which means that the accuracy of a computational method for evaluating word similarity can only be established by comparing its results against human common sense. That is, a method that comes close to matching human judgments can be deemed accurate. Moreover, some datasets for this evaluation are commonly used. In particular, the Rubenstein and Goodenough dataset (R&G in the following) and Miller and Charles dataset (M&C) are standards dataset for evaluating semantic similarities.\nIn 1965, Rubenstein and Goodenough [15] obtained \"synonymy judgments\" of word pairs by hiring 51 subjects to evaluate 65 pairs of nouns. The subjects were asked to assign a similarity from 0 to 4, from \"semantically unrelated\" to \"highly synonymous\". Miller and Charles [10], 25 years later, extracted 30 pairs of nouns from the R&G dataset and repeated their experiment with 38 subjects. The M&C experiment achieved a correlation of 0.97 with the original experiment of R&G. Resnik [13], in 1995, replicated the M&C experiment with 10 computer science students, obtaining a correlation of 0.96. Pirr\u00f3 and Seco [11] (P&S) in 2008 also recreated the R&G experiment this time with 101 subjects, and arrived at a correlation coefficient of 0.972 for the full dataset (P &S full ).\nAs mentioned above, we used the judgments of the P&S experiment for the word pairs of the M&C dataset. However, we considered only 28 word pairs of the 30 used in the M&C experiment: a word missing in WordNet made it impossible to compute ratings for the other two word pairs. All the evaluations were performed using WordNet 3.0 [2] and the Brown Corpus 6 was used for the information content based metric calculation. The computation used Pedersen's WordNet::Similarity Perl module as the core. We also recreated the P&S experiment with the Java WordNet Similarity Library [11] (JWSL) using Pirr\u00f3 and Seco's intrinsic information content, but we did not obtain the same results they did.\nFor the new metrics we perform two experiments, both varying the importance of the semantic difference's factor \u03b2, and then calculating the correlation with the human judgments values of the P&S experiment. In the first experiment \u03b2 takes values in the range of [0,1] while in the second experiment \u03b2 > 1. The importance of the semantic commonalities factor was kept constant (\u03b1 = 1), since we wanted to focus on the effect of semantic differences in WordNet based measures.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "Table 1 compiles the results of the first experiment for several difference's factors. The first value (\u03b2 = Original) represents the original measure 7 , i.e., the previous result. The correlation value when \u03b2 = Original and \u03b2 = 0.0 should be the same if the modified measure considers the commonalities as in the original metric. This is not the case of Sim length (c 1 , c 2 ), Sim lch (c 1 , c 2 ) and Sim j&c (c 1 , c 2 ); and it is the reason for the differences in their correlation values when \u03b2 = Original and \u03b2 = 0.0.\nTable 2 shows the result of the second experiment, which is a summary of the evaluations of the six analyzed measures. Wu & Palmer (Sim wup ) and Lin (Sim lin ) metrics did not show any differences in performance for any \u03b2. The modifications to these metrics did not positively or negatively affect their performance. Both measures normalize the semantic commonalities and the semantic differences by the total amount of information gathered from the two concepts being compared (see Table 3). The correlations of the other modified metrics (Sim length , Sim lch , Sim res , Sim j&c ) were higher than those of the originals. The new metrics obtained slightly better results when the semantic differences between the concepts were taken into consideration. In general, all node-based similarity measures were superior to the edge-based ones. Curiously, the results of Sim length were better than those for Sim lch despite simplicity of its model. The modified length metric (Sim length ) reached its maximum correlation value at \u03b2 = 0.6; increasing \u03b2 further did not improve its correlation value. Leacock & Chodorow (Sim lch ) reached its maximum correlation value at \u03b2 = 12, although it was a small improvement over the values for the original function. Resnik (Sim res ) and Jiang & Conrath (Sim j&c ) obtained their best correlation values for \u03b2 = 2.8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_5", "tab_20"]}, {"heading": "Concluding Remarks and Future Work", "text": "The five new measures presented in this paper are modifications of traditional WordNet-based semantic similarity metrics. Supported by a featured-based theory, they incorporate the idea of semantic differences between concepts into the similarity computation. The experimental results showed that, three of the measures outperformed their classical; the other two measures performed the same as their classical versions. These results demonstrate the strengths and positive effects of including concepts semantic differences during their semantic similarity computations.\nAs future work, we would like to find out why Sim wup and Sim lin metrics did not change their correlation coefficients after being modified. We will also evaluate the node-based metrics using the intrinsic information content approach. Additionally, we shall investigate the effect of the combination of commonalities and differences at the same time. And finally, we will work on a method for finding a best value for those parameters.\nis the reduction of the computational time needed to compare documents and queries represented using concepts. This representation has been applied to the ad-hoc retrieval problem. The approach has been evaluated on the MuchMore 1 Collection [4] and the results demonstrate its viability.\nThe paper is organized as follows: in Section 2, an overview of the environment in which ontology has been used is presented. Section 3 presents the tools used for this work. Section 4 illustrates the proposed approach to represent information, while Section 5 compares this approach with other two wellknown approaches used in conceptual representation of documents. In Section 6, the results obtained from the test campaign are discussed. Finally, Section 7 concludes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "An increasing number of recent IR systems make use of ontologies to help the users clarify their information needs and come up with semantic representations of documents. Many ontology-based IR systems and models have been proposed in the last decade. An interesting review on IR techniques based on ontologies is presented in [11], while in [16] the author studies the application of ontologies to a large-scale IR system for web purposes. A model for the exploitation of ontology-based knowledge bases is presented in [7]. The aim of this model is to improve search over large document repositories. The model includes an ontology-based scheme for the annotation of documents, and a retrieval model based on an adaptation of the classic vector-space model [15]. Another IR system based on ontologies is presented in [14]. The authors propose an IR system which has a landmark information database with hierarchical structures and semantic meanings of the features and characteristics of the landmarks.\nThe implementation of ontology models has been also investigated by using fuzzy models [6].\nIn IR, queries entered by users usually are not detailed enough, making it hard to retrieve satisfactory results. Query expansion can help solve this problem. However, query expansion as usually implemented in IR systems does not guarantee consistent retrieval results. Ontologies play a key role in query expansion research. A common use of ontologies in query expansion is to enrich the resources with some well-defined meaning to enhance the search capabilities of existing web searching systems.\nIn [18], the authors propose and implement a query expansion method which combines a domain ontology with the frequency of terms. The ontology is used to describe domain knowledge; a logic reasoner and the frequency of terms are used to choose fitting expansion words. This way, higher recall and precision can be obtained.\nIn [10], the authors present an approach to expand queries whose idea is to look for terms from the topic query in an ontology to add similar terms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "The roadmap to prove the viability of a concept-based representation of documents and queries is composed of two main tasks:\nto choose a method that allows representing all document terms by using the same set of concepts; -to implement an approach that allows indexing and evaluating each concept, in both documents and queries, with the \"correct\" weight.\nTo represent documents, the method described in Section 4 has been used, combined with the use of the WordNet MRD. From the WordNet database, the set of terms that do not have hyponyms has been extracted. We call such terms \"base concepts\". A vector, named \"base vector\", has been created and, to each component of the vector, a base concept has been assigned. This way, each term is represented by using the base vector of the WordNet ontology. The representation described above has been implemented on top of the Apache Lucene open-source API. 2 In the pre-indexing phase, each document has been converted into its ontological representation. After the calculation of the importance of each concept in a document, only concepts with a degree of importance higher than a fixed cut-off value have been maintained, while the others have been discarded. The cut-off value used in these experiments is 0.01. This choice has a drawback, namely that an approximation of representing information is introduced due to the discarding of some minor concepts. However, we have experimentally verified that this approximation does not affect the final results.\nDuring the evaluation activity, queries have also been converted into the ontological representation. This way, weights have to be assigned to each concept to evaluate all concepts with the right proportion. One of the features of Lucene is the possibility of assigning a payload to each term of the query. Therefore, for each element in the concept-based representation of the query, the relevant concept weight has been used as boost value.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Document Representation", "text": "Conventional IR approaches represent documents as vectors of term weights. Such representations use a vector with one component for every significant term that occurs in the document. This has several limitations, including:\n1. different vector positions may be allocated to the synonyms of the same term; this way, there is an information loss because the importance of a determinate concept is distributed among different vector components; 2. the size of a document vector must be at least equal to the total number of words of the language used to write the document;\n3. every time a new set of terms is introduced (which is a high-probability event), all document vectors must be reconstructed; the size of a repository thus grows not only as a function of the number of documents that it contains, but also of the size of the representation vectors.\nTo overcome these weaknesses of term-based representations, an ontology-based representation has been recently proposed [9], which exploits the hierarchical isa relation among concepts, i.e., the meanings of words. For example, to describe with a term-based representation documents containing the three words: \"animal\", \"dog\", and \"cat\" a vector of three elements is needed; with an ontology-based representation, since \"animal\" subsumes both \"dog\" and \"cat\", it is possible to use a vector with only two elements, related to the \"dog\" and \"cat\" concepts, that can also implicitly contain the information given by the presence of the \"animal\" concept. Moreover, by defining an ontology base, which is a set of independent concepts that covers the whole ontology, an ontology-based representation allows the system to use fixed-size document vectors, consisting of one component per base concept.\nCalculating term importance is a significant and fundamental aspect for representing documents in conventional IR approaches. It is usually determined through term frequency-inverse document frequency (TF-IDF). When using an ontology-based representation, such usual definition of term-frequency cannot be applied because one does not operate by keywords, but by concepts. This is the reason why it has been adopted the document representation based on concepts proposed in [9], which is a concept-based adaptation of TF-IDF.\nThe quantity of information given by the presence of concept z in a document depends on the depth of z in the ontology graph, on how many times it appears in the document, and how many times it occurs in the whole document repository. These two frequencies also depend on the number of concepts which subsume or are subsumed by z. Let us consider a concept x which is a descendant of another concept y which has q children including x. Concept y is a descendant of a concept z which has k children including y. Concept x is a leaf of the graph representing the used ontology. For instance, considering a document containing only \"xy\", the occurrence of x in the document is 1 + (1/q). In the document \"xyz\", the occurrence of x is 1 + (1/q(1 + 1/k)). As it is possible to see, the number of occurrences of a leaf is proportional to the number of children which all of its ancestors have. Explicit and implicit concepts are taken into account by using the following formulas: \nb i ) = N doc (b i ) N rep (b i ) , (2\n)\nwhere N doc (b i ) is the number of explicit and implicit occurrences of b i in the document, and N rep (b i ) is the total number of its explicit and implicit occurrences in the whole document repository. This way, every component of the representation vector gives a value of the importance relation between a document and the relevant base concept.\nA concrete example can be explained starting from the light ontology represented in Figures 1 and 2, and by considering a document D 1 containing concepts \"xxyyyz\".  ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Representation Comparison", "text": "In Section 4, the approach used to represent information was described. This section shows the improvements obtained by applying the proposed approach and illustrates a comparison between the proposed approach and other two approaches commonly used in conceptual document representation. The expansion technique is generally used to enrich the information content of queries. However, in the past years some authors applied the expansion technique also to represent documents [2]. Like in [13,2], we propose an approach that uses WordNet to extract concepts from terms.\nThe two main improvements obtained by the application of the ontology-based approach are illustrated below.\nInformation Redundancy. Approaches that apply the expansion of documents and queries use correlated concepts to expand the original terms of documents and queries. A problem with expansion is that information is redundant and there is no real improvement of the representation of the document (or query) content. With the proposed representation, this redundancy is eliminated, because only independent concepts are taken into account to represent documents and queries. Another positive aspect is that the size of the vector representing document content by using concepts is generally smaller than the size of the vector representing document content by using terms.\nAn example of a technique that shows this drawback is presented in [13]. In this work the authors propose an indexing technique that takes into account WordNet synsets instead of terms. For each term in documents, the synsets associated to that terms are extracted and then used as token for the indexing task. This way, the computational time needed to perform a query is not increased, however, there is a significant overlap of information because different synsets might be semantically correlated. An example is given by the terms \"animal\" and \"pet\": these terms have two different synsets; however, observing the WordNet lattice, the term \"pet\" is linked with an is-a relation to the term \"animal\". Therefore, in a scenario in which a document contains both terms, the same conceptual information is repeated. This is clear, because, even if the terms \"animal\" and \"pet\" are not represented by using the same synset, they are semantically correlated, since \"pet\" is a sub-concept of \"animal\". This way, when a document contains both terms, the presence of the term \"animal\" has to contribute to the importance of the concept \"pet\" instead of being represented with a different token.\nComputational Time. When IR approaches are applied in a real-world environment, the computational time needed to evaluate the match between documents and the submitted query has to be considered. It is known that systems using the vector space model have higher efficiency. Conceptual-based approaches, such as the one presented in [2], generally implement a non-vectorial data structure which needs a higher computational time with respect to a vector space model representation. The approach proposed in this paper overcomes this issue because the document content is represented by using a vector and, therefore, the computational time needed to compute document scores is comparable to the computational time needed when using the vector space model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, the impact of the ontology document and query representation is evaluated. The evaluation method follows the TREC protocol [17]. For each query, the first 1000 retrieved documents have been considered and the precision of the system has been calculated at different points: 5, 10, 15, and 30 documents retrieved. Moreover, the precision/recall graph has been calculated.\nThe experimental campaign has been performed by using the MuchMore collection, that consists of 7,823 abstracts of medical papers and 25 queries with their relevance judgments. One of the particular features of this collection is that there are numerous medical terms. This gives an advantage to term-based representations over the semantic representation, because specific terms present in documents (e.g., \"arthroscopic\") are very discriminant. Indeed, by using a semantic expansion, some problems may occur because, generally, the MRD and thesaurus used to expand terms do not contain all of the domain-specific terms.\nThe precision/recall graph shown in Figure 3 illustrates the comparison between the proposed approach (gray curve with circle marks), the classical termbased representation (black curve), and the synset representation method [13] (light gray curve with square marks). As expected, for all recall values, the proposed approach obtained better results than the term-based and synset-based  representations. The best gain over the sybset-based representation is at recall levels 0.0, 0.2, and 0.4, while, for recall values between 0.6 and 1.0, the synsetbased precision curve lies within the other two curves. A possible explanation for this scenario is that, for documents that are well related to a particular topic, the adopted ontological representation is able to improve the representation of the documents contents. However, for documents that are partially related to a topic or that contain many ambiguous terms, the proposed approach becomes less capable of maintaining a high precision. At the end of this section, some improvements that may help overcome this issue are discussed.\nIn Table 1, the three different representations are compared with respect to the Precision@X and MAP values. The results show that the proposed approach obtains better results for all the precision levels and also for the MAP value. An in-depth study of this first experiments campaign has been performed, and we have noticed that for some queries the concept-based representation obtained results that were below our expectations. By inspecting the implemented model, some issues have been noticed and are at present under analysis:\n-Absence of some terms in the ontology: some terms, in particular terms related to specific domains (biomedical, mechanical, business, etc.), are not defined in the MRD used to define the concept-based version of the documents. This way there is, in some cases, a loss of information that affects the final retrieval result. -Proper names have not been considered: proper names of persons, geographical locations, industries, etc., are not present in the concept-based index.\nObserving the content of some documents and topics, proper names turn out to be a discriminant feature in some cases. However, this may be interpreted as an \"instance\" issue: when a domain is specified, our method might incorporate domain-related instances that suggest relevant concepts. -Verbs and adjective are not present as well in the ontology: the concept representation of terms, described in Section 4, does not take into account verbs and adjectives. This happens because verbs and adjectives are structured in a different way than nouns. The hyperonymy and hyponymy relations (that make MRD comparable with ontologies) are not defined for verbs and adjectives. However, a method mapping verbs and adjective to their related nouns is being implemented to overcome this drawback.\n-Term ambiguity: the concept-based representation has the problem of introducing an error given by not using a word-sense disambiguation (WSD) algorithm. Using such a method, concepts associated to incorrect senses would be discarded or weighted less. Therefore, the concept-based representation of each word would be finer, with the consequence of representing the information contained in a document with higher precision.\nImproving the actual model with the above suggestions would certainly yield significantly better results in the future experimental campaign. This positive view is motivated by the fact that, in spite of these issues, the preliminary goal of outperforming the precision of the term-based representation has been accomplished.", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": ["tab_2"]}, {"heading": "Conclusion", "text": "In this paper, we have discussed an approach to indexing documents and representing queries for IR purposes which exploits a conceptual representation based on ontologies.\nExperiments have been carried out on the MuchMore Collection to validate the approach with respect to problems like term-synonymity in documents.\nPreliminary experimental results show that the proposed representation improves the ranking of the documents. Investigation on results highlights that further improvement could be obtained by integrating WSD techniques like the one discussed in [1] to avoid the error introduced by considering incorrect word senses, and with a better usage and interpretation of WordNet to overcome the loss of information caused by the absence of proper nouns, verbs, and adjectives.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "One of the main challenges in Criminology is to understand, explain and predict when individuals show delinquent behaviour [4]. Obviously, there is a wide range of potential contributors to the emergence of crime, varying from environmental opportunities to social influences. In this paper we focus on the latter. Learning (delinquent) behaviour by social interaction is something that is often observed in adolescents [11]. During the period from 12 to 18 year old, people are more susceptible to the opinion of their peers. In some situations, their desire to be part of a group can be so strong that they break some rules to achieve this desire. This is consistent with the theory by Moffitt [10] who states that one can divide delinquents roughly into two groups, namely life-course persistent offenders and adolescence limited offenders. The behaviour of the first group is caused by neuropsychological problems during childhood that interact cumulatively with their criminogenic environments across development, which leads to a pathological personality. This behaviour will usually continue through life. Instead, the behaviour of adolescence-limited offenders is caused by a gap between biological maturity and social maturity. It is mainly caused by mimicking antisocial role models like peers, but also parents and school are important contributors. These offenders peak sharply at about age 17 and drop fast in young adulthood.\nIn this paper we exploit simulation techniques to study the development of such juvenile delinquency. As mentioned above, this type of behaviour is limited to a certain period of time, and some of its direct causes are clearly determined. This provides opportunities to develop a computational model of this process. In previous research [2], we developed such a model, which was able to predict the level of delinquency of students based on information about the personal characteristics and their peer network. The model was validated by using a large dataset with information about 1730 scholars (taken from [14]).\nThe main contribution of the current paper is to show how this model can be used to perform so called what-if simulation experiments. In these simulations the existing (validated) model is applied to a hypothetical situation, which is slightly different from the situation in the existing empirical data. For example, we want to see what happens to the level of delinquency (both of individuals and of the classes) when the composition of the classes is altered. Interesting questions here are, among others:\n\u2022 What is the effect when we put the most delinquent students together in one class? \u2022 Is it better to spread the delinquent and non-delinquent students equally over classes? To answer such questions, this paper proposes to make use of social simulation techniques [3]. In recent years, a number of papers have successfully tackled criminological questions using social simulation, e.g., [7,9]. However, the current paper differs from these approaches in that we do not attempt to reproduce existing data, but rather explore how hypothetical scenarios would evolve. We will create these hypothetical scenarios by making small modifications in existing scenarios (e.g., change the composition of classes), and run the simulation model on the modified data. The main question that we would like to answer is whether the composition of a school class has an influence on the overall level of delinquency of the pupils. This is an interesting topic, since it is often believed that the structure of schools and peer networks has an important impact on juvenile delinquency [8,12].\nThe paper is organised as follows. In Section 2 we describe how the data used for the simulation experiments were collected. The simulation model itself is presented in Section 3, and the experiments in Section 4. Finally, Section 5 concludes the paper with a discussion and some ideas for future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Collection", "text": "The model presented in this paper is based on empirical data from a longitudinal research project. This research was performed by the Netherlands Institute for the Study of Crime and Law Enforcement (NSCR) in the so called 'School Project' [14], which focused on peer network formation, personal development, and school interventions in the development of problem behaviour and delinquency.\nIn this project, a large number of high school students were surveyed by means of questionnaires. As respondents, a cohort of students was used that started high school during the school year 2001/2002. The first year of secondary education in the Netherlands is comparable with 7 th grade in the United States (most students are 12 or 13 years old). These students were surveyed during three consecutive years: 2002, 2003 and 2004.\nDuring these three years, the respondents had to fill out a number of questionnaires. Their delinquent behaviour was measured using self-reports of a variety of offences. The self report method is a standard procedure in Criminology, and it results in fairly reliable estimates of delinquency levels of young people, when it is conducted in a proper way and in an anonymous setting. Respondents were asked if they had ever committed an offence and, if so, how often during the reference period. The measures of self-reported delinquency used in this study come from 12 questions, among which: in the last year, how many times did you: \"paint graffiti\", \"vandalise property\", or \"steal small things from shops worth less than 5 Euros\" The total delinquency measure indicates how many types of the 12 possible types of delinquent behaviours were reported by the person.\nThe respondents also had to answer a number of questions about their friends (e.g. with whom they spent a lot of time, who were their best friends), to obtain information about their social networks. In the analyses, friends' numbers were linked to the respondent's own number, enabling the networks of friends to be mapped and analysed.\nFurther, the study also used a substantial number of other measures on risk factors that are central in criminological theories and have been found to correlate with delinquency in the past (e.g. low supervision and support by parents, low bond with school, low law conformity, high impulsivity, high temper). For more details of the empirical research see [13,14].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simulation Model", "text": "In this section the simulation model used for the experiments is described. First, in Section 3.1, the methodology behind the design of the model is discussed (based on [1]. Section 3.2 presents the implementation of the model, and Section 3.3 shows how the original model was extended in order to incorporate information about classes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Design Methodology", "text": "As a first step in the process of designing the model, an initial dynamic model was developed for the development of delinquency through social learning in a class room, based on an analysis of the literature (see Figure 1). A more detailed description of this model is provided in [1]. The model describes the influences of several personal characteristics, as well as the influences of other peers. More specifically, the delinquency of an agent is influenced by its previous delinquency, its individual personality traits (e.g. temper, impulsiveness), and external factors (i.e., the school, the parents, and peers). This original model has the form of a set of differential equations, where delinquency is measured as a real number between 0 and 1. In [1], it has been shown that this model can be used to simulate delinquency development of a small set of agents in a classroom. The simulations exhibited several patterns that would be expected based on the criminological literature.\nA next step was to validate the model based on the empirical data mentioned in Section 2. In this research [2], a representative sample of the collected dataset has been selected, and has been split up in a training set and test set. Each set contained the data of around 250 pupils. A lot of pupils were left out of the original dataset, because their questionnaires were not suitable. This was caused, for instance, by gaps in the answers or because they were only attending a particular school during part of the research period. When making this split, we guaranteed that there was no overlap between the schools We developed an evaluation method that could be used to quantify the correctness of models and to discriminate between accurate and less accurate models. This measure accommodates the intuitive ideas about a correct prediction in one number (see Section 3.2). The model was calibrated with the data in the training set by taking the model from [1] extended with some additional factors reported in [14], and systematically adjusting it and comparing the simulation results with the actual measurements in the training set (scaled to a number between 0 and 1). The adjustment consisted of both ignoring factors in the model (i.e. leaving out variables in the formulae) and calibrating parameters (i.e. changing the value of weighting variables), thereby creating different variations of the model. Finally, the second data set was used to validate the different variations of the model that seemed promising during the calibration phase. In this phase, we did not change the model or parameters, but just calculated the accuracy according the developed measure for all formulae that resulted in a high score in the first phase. This method gives an unbiased validation of the accuracy, as the validation is performed on a different data set than the tuning.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Implementation", "text": "To implement the model, we used standard numerical simulation software. A 'school class' was modelled as a multi-dimensional array, where each array represented a different student. The different dimensions represented characteristics of the students over time. For example, these dimensions specified the individual characteristics (like impulsivity and risk-orientedness) and the relations to peers. To calculate the new delinquency of each agent, the following algorithm was used (in pseudo code): To calculate the new delinquency of the individual agents (step 5), various variants of the model have been tried, each incorporating some of the factors identified in the previous section. These different models are depicted in Table 1. For example, model variant 1 (a baseline model), always predicts that students will not become delinquent. The last column denotes the accuracy rate for each model, which was calculated as follows:\nAccuracy Rate = (w*Hits + Correct Rejections) / (w*Hits + w*Misses + False Alarms + Correct Rejections)\nwhere Hits, Misses, Correct Rejections and False Alarms are defined according to the classical measures in signal detection theory [5]. For more details, see [2]. The factor 'risk orientedness' (model 8 and 9) indicates the extent to which the pupils like performing exciting activities, and the factor 'deviance reinforcement' (model 9-11) indicates the extent to which the pupils are sensitive to influences of their friends.\nAs can be seen, variants 10 and 11 have the highest accuracy. This means that the previous delinquency combined with the impulsivity, the level of deviance reinforcement by friends, and the delinquency of (best) friends, seem to be the best predictors for delinquent behaviour. In addition to the accuracy, the quality of the models has also been tested using a Relative Operating Characteristics (ROC) analysis. This method has been used because this is a standard measure in the literature and allows us to compare the results with studies in other domains. The outcome of this analysis is a curve which represents a graphical plot of the fraction of true versus the fraction of false positives for a binary classifier system as its discrimination threshold is varied (see Figure 1). The threshold in our model is the value of the calculated delinquency above which a pupil is classified as delinquent. We calculated the area under the ROC curve (AUC), a scalar measure for the quality of the predictions, for each model. For model variant 10, the AUC is 0.79. An AUC-value larger than 0.70 is called 'acceptable', larger then 0.80 'excellent' and larger then 0.90 'outstanding' [6] 1 .", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2"]}, {"heading": "Incorporating Class Information", "text": "Although model 10 and 11 produce the highest accuracy rates, these model variants are not particularly appropriate for the aims of the current paper. That is, the goal of this paper is to predict for hypothetical scenarios (which are slightly different from the existing situation) how the delinquency of the students would have developed. And since it is not very realistic to assume that one can easily modify, say, the impulsivity or the friend network of students, variant 10 and 11 are not very useful candidates for these 'what-if experiments'.\nFor this reason, two additional variants of the model have been developed. These models (variant 12 and 13) use the composition of classes. For obvious reasons, in practice it is much easier to manipulate students' class composition than their friend networks. Therefore this factor was also manipulated within the hypothetical scenarios. To this end, variants of the model have been developed that take the delinquency of class members into account.\nModel variant 12 predicts that a student will become delinquent if (s)he was delinquent in the previous year OR (s)he is part of a delinquent class AND (s)he has a high value for 'deviance reinforcement'. Here, being part of a delinquent class is defined as the situation that the average delinquency of all students in the class is higher than a certain threshold. Note that this variant does not make use of the friend network.\nThe ROC curve obtained for this model variant 12 is depicted in Figure 2a, when compared with a random model (variant 3). As can be seen, variant 12 performs much better than the random model. The AUC of model variant 12 was 0.734, and its accuracy is 72.33. Although this is lower than the AUC and accuracy of variant 10 (resp. 0.79 and 76.41), we decided to use variant 12 for the simulation experiments described in the next section, because (as explained above) this variant contains the students' classes as one of the factors.\nIn addition, a model variant has been developed that also takes the delinquency of the friends into account. Variant 13 predicts that a student will become delinquent if (s)he was delinquent in the previous year OR delinquency of the friends times the 'deviance reinforcement' is higher than a certain threshold OR (s)he is part of a delinquent class AND (s)he has a high value for 'deviance reinforcement'. For being part of a delinquent class the same definition is used as in variant 12. The AUC of this model variant (see Figure 2b) is 67.52 2 , and its accuracy is 72.66. ", "publication_ref": [], "figure_ref": ["fig_15", "fig_15"], "table_ref": []}, {"heading": "Approach", "text": "In the simulations, we compared the results of the simulation of the delinquency over one year using the actual class composition with the results of two simulations using a hypothetical composition, namely 1) a scenario in which all delinquent pupils are put together in the same class, and 2) a scenario in which all delinquent pupils are evenly distributed over all classes in a school. The goal of the comparison is to find out whether the change in the delinquency of pupils is positively or negatively influenced by the class composition. The simulations are performed using three schools in our dataset, consisting of 6, 8 and 4 classes, respectively. These schools were chosen because many pupils of these schools filled out the questionnaire, so much data was available. In total 194 pupils were involved in the simulations. The simulations for the actual class composition and the two hypothetical scenarios have been performed two times, using each variant of the model that takes the class information into account (variant 12 and 13).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simulation Results", "text": "Table 2 gives an overview of the development of the delinquency over a year according to the simulation with model variant 12 and 13. The first two columns indicate, respectively, the code of the school class in the study (e.g., '1 -2' stands for 'class 2 of school 1'), and the amount of pupils in the class. In the next 3 columns, the 'base before' column shows the number of delinquent pupils in the actual class composition at the start, and the columns 'base after v12' / 'v13' the predicted number of delinquent pupils after a year using model variant 12 or 13 respectively. Similarly, the 6 subsequent columns show the number of delinquent pupils in a class at the start and the end using the hypothetical class compositions (called scenario and 2). It can be seen that in scenario 1 all delinquent pupils of a school are put together in a class, while in scenario 2 the delinquent pupils are more or less evenly distributed over the classes.  As can be seen in Table 2, the difference between the baseline and the different scenarios is not very high. For model variant 12, the total number of delinquent pupils increases in scenario 1 from 54 to 55 instead of to 58 for the baseline, and in scenario 2 it increases as much as in the baseline. In model variant 13, the number of delinquent pupils increases to 64 in the baseline, while it increases to 67 in scenario 1 and to 66 in scenario 2.\nscen2 after v13 1 -1 7 0 0 0 6 6 6 1 1 1 -2 17 2 2 2 0 0 0 1 2 1 -3 10 2 2 2 0 0 1 1 1 1 -4 6 1 1 1 0 0 0 1 1 1 -5 6 1 1 1 0 0 1 1 1 1 -6 6 0 0 0 0 0 0 1 1 2 -1 9 2\nIn the simulations using variant 12 we see that the increase of the number of delinquent pupils is less for the scenario in which all bad guys are put together (scenario 1) than in the baseline scenario or the scenario in which the delinquent pupils are evenly distributed. However, this pattern is not visible when using model variant 13. Overall, the differences between the results of the baseline scenario and the two other scenarios are very small. Although care should be taken not to draw too strict conclusions from these preliminary experiments, this may be an indication that the use of alternative class compositions has little effect.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_5"]}, {"heading": "Conclusion", "text": "In this paper, we have presented a number of simulation experiments on juvenile delinquency. The simulations were performed using an existing model that was based on the theory of social learning. In our previous research we have used empirical data about juvenile delinquency and social networks to develop and validate this simulation model. In the current paper we have presented some novel variants of this model. Moreover, we have used the model to investigate the effect of different class compositions on the development of the delinquency in the total group of pupils.\nThe experiments show no significant difference between the change in the total number of delinquent pupils in the different scenarios. The two different scenarios represented two extreme situations: all delinquent pupils put together, or all delinquent pupils distributed over all classes. Therefore, our tentative conclusion is that the composition of classes has not so much effect on the overall development of the delinquency of the pupils in a school. This is an interesting finding, since it is often argued that careful composition of school classes is very important to prevent development of juvenile delinquency [8,12].\nHowever, there are a few remarks that can be made about our experiments, which could be of influence on this conclusion. First of all, the model is possibly not very precise (see the relatively limited accuracy) because of small size of the training set. It could be the case that with a more precise model (derived from a larger training set) stronger effects would be visible. A second remark concerns the size of the classes. The ones used in the simulated scenarios are much smaller than regular classes; as a consequence, the influence of other pupils in the class is smaller in our simulations than in reality. The class size is this small because the data of many of the pupils was not suitable, e.g., because of missing information or because they switched between schools. The fact that we do not see a clear effect could also be caused by the fact that the number of offenders in our data set is relatively small. Therefore, also the number of predicted changes will always be quite small. Finally, we want to remark that the current models do not allow pupils to learn nondelinquency from their peers at school, they can become delinquent. Although this apparently follows from our dataset in the best predictive models, it might be the case that this is a bit different in reality.\nDespite these remarks, the approach presented in this paper has proved to be a useful additional tool for criminology scientists, as also confirmed by our colleagues in the Criminology department. The approach allows for experiments that can not be easily performed in the real world and could give some indication of the expected effects of class compositions on juvenile delinquency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Supporting consensus building through argumentative debate is socially important because misunderstanding speaker's intention or emotional conflict sometimes occur among stakeholders. Creation of argumentative corpus and analysis of argumentative discourse is needed for finding the appropriate structure of the discourse. The concept of appropriate argumentative discourse structure is important for developing argumentation support systems. Often stakeholders need a facilitator to properly organize their ideas. A facilitation system can assist users in building well structured argumentative discussion. To do so, the system must be aware of what kind of argumentative discourse structure is regarded as appropriate.\nOur focus is argumentative corpus analysis. In the paper we define argumentative discourse structure as appropriate if it tends to lead to agreement. We assume that structure of argumentative discourse can be detected with help of rhetorical relations that connect related elements in discourse. We think that specific agreement-oriented sequences of rhetorical relations that hold across argumentative discourse elements describe the structure tending to lead to consensus. We create a small argumentative corpus and use Rhetorical Structure Theory relations to annotate it. We, as well, introduce few novel rhetorical relations that we think reflect in a clearer way speaker's intention within question-answering act.\nIn our study we use data (web discussions) taken from Wikipedia talk pages. Wikipedia talk page provides space for editors to discuss changes to page's associated article or project page. Participants (article editors) launch discussions on different topics related to the article improvement and aim to come to a common point about the topic. We consider this type of web discussions is suitable for our analysis purpose.\nTo verify our assumption we calculate prior and posteriori probability of rhetorical relations bigrams and present some analysis results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Designing Tag Set", "text": "Argumentative corpus, we build, consists of web discussions, where participants express their ideas and aim to reach a consensus. As mentioned above, we focus on the analysis of such argumentative discourse and try to determine the appropriateness of its structure. We define the argumentative discourse structure as appropriate if it tends to lead to agreement and assume that it could be detected through sequences of specific agreement-oriented rhetorical relations that hold across discourse elements. So, our primary task was to define the tag set of rhetorical relations that we were going to use for the argumentative corpus annotation.\nA powerful instrument that allows analyzing how consecutive discourse elements are related by a small tag set of rhetorical relations is the Rhetorical Structure Theory (RST) proposed by [Mann and Thompson, 1987]. One of the main advantages of RST is its ability to easy and in comprehensive way describe natural texts' structure through rhetorical relations. There are two important reasons why the theory could be suitable for our research. Firstly, though primarily meant for monologue text analysis, RST can also be applied for conversation analysis, as described in some related works [Daradoumis, 1996;Stent, 2000]. Although not fully, theory allows specifying the intentional structure of the discourse, which is very important part for the consensus building process analysis. On the other hand, rhetorical relations enable us to implement software dealing with such intentional structures. Thus, we decide to apply Rhetorical Structure Theory to our study.\nInitially, we tried to annotate our corpus with the tag set of rhetorical relations proposed by RST only. Then, during the annotation process, we found that some additional rhetorical relations tags that are not present in RST are needed. These are rhetorical relations that, for example, connect question-answer pairs in argumentative discourse and help to determine the intention of the question, such as Req_evidence (require evidence) that helps to understand that Evidence should be provided as a response and not an Example or Suggestion. Or, relation tags, reflecting result of the discussion process, such as agreement and disagreement. That is why besides a number of rhetorical relation tags borrowed from RST, we introduce 3 novel relation tags that help to clarify the question intention. We also borrow 10 rhetorical relation tag concepts from related works about building a corpus in the framework of RST [Marcu, 1999]; semantic authoring [Hashida,2007]; addressing behavior in face-toface conversation [Jovanovic,2006]. ", "publication_ref": ["b727", "b729", "b730"], "figure_ref": [], "table_ref": []}, {"heading": "Data Analysis", "text": "The data we selected for our small argumentative corpus are taken from Wikipedia, free encyclopedia Talk pages. The purpose of Wikipedia talk page is to provide space for editors to discuss changes to its associated article or project page [Wikipedia, free encyclopedia]. For convenience we selected English language pages. We analyzed discussions provided by Moldova talk page 1 and America talk page 2 . We gathered a small corpus containing 693 comments with the total number of participants 197 people. We annotated our data with the tag set containing 27 rhetorical relations, part of which is presented in Table 1. As a result, our corpus includes 627 relations that connect participants' comments. The most frequent relations are listed in Table 2. Basing on frequency results, we can assume that in the type of argumentative discourse we analyzed, rhetorical relations as Explanation_argumentative, Evidence, Suggestion, Req_evidence prevail. Data taken from Wikipedia Talk pages reflect short and long discussions about Wikipedia article editing process. Participants (editors) provide their ideas, suggestions or requests related to the article's improvement. The purpose of the discussions held on Wikipedia talk page is to come to a common point about a certain topic being discussed. To investigate the relationship between consensus building and appropriateness of structure, we count frequencies of bigrams of rhetorical relations (r 1 , r 2 ), where let r 1 be a preceding relation and r 2 be a succeeding relation that follows r 1 . N-gram of rhetorical relation is determined as shown in Figure 1. Circles present discourse elements (participants' comments) and arcs show how related discourse elements are connected by rhetorical relation. For example, circle c has multiple arcs targeted to a and b, where relations are respectively Evidence and Agreement.  3 and 4. It can be seen that most frequently are met the bigrams and trigrams that include Disagreement and Agreement.\nTo verify our assumption that there exist specific agreement-oriented sequences of rhetorical relations within argumentative discourse, we calculate priori probability P(r 2 |r 1 ) and posteriori probability P(r 1 |r 2 ), which are respectively defined as  where C(r) and C(r 1 ,r 2 ) denote frequencies of a rhetorical relation r and relation bigram (r 1 ,r 2 ), respectively. These calculations allow us to see which rhetorical relations precede Agreement and Disagreement rhetorical relations. In Tables 5 and 6 we present some results for agreement and disagreement pairs. Order of relation r 1 in the tables is sorted by P(r 1 |r 2 = Agreement), the posteriori probability of r 1 when r 2 =Agreement, because this probability can be regarded as a contribution of r 1 for building consensus. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2", "tab_5", "tab_20", "tab_56"]}, {"heading": "Discussion", "text": "Creation of argumentative corpus and analysis of appropriate argumentative discourse structure is crucial for developing argumentation support systems that assist users in consensus building process. In the paper we assume that appropriateness of the discourse structure can be determined through specific rhetorical relations sequences that lead to common agreement. We create a small corpus that consists of web discussions taken from Wikipedia talk page. We call it argumentative corpus. We annotate our corpus with rhetorical relations and basing on the annotation, try to analyze discourse structure for appropriateness. Namely, we calculate two types of probability, prior and posteriori, for the rhetorical relations bigrams met in our corpus.\nOur analysis results are presented in Tables 5 and 6. We sorted data by posteriori probability of preceding relation when the following relation is Agreement, because it can be regarded as a contribution of preceding rhetorical relation for consensus building. The results show that, most frequently, Agreement relation are preceded by reflect common, not strictly logical flow of consensus building conversation. We connect participants' comments within a discussion with RST rhetorical relations. It also must be mentioned that RST application for conversation analysis is not novel. In [Daradoumis, 1996] extended Dialogic RST with new rhetorical relations is used to analyze conversation. The analysis focus is on interruptions in a dialogue (tutorial dialogues are analyzed). In [Stent, 2000] in preliminary result for annotating taskoriented dialogue corpus with RST relations, a new relation Question-Answer is proposed for adjacency pairs. However, we try to design a set of argumentation-specific rhetorical relations that also reflect in a clearer way intention of participant's next utterance.", "publication_ref": ["b729", "b730"], "figure_ref": [], "table_ref": ["tab_56"]}, {"heading": "Conclusion", "text": "A facilitation system can assist users in building well structured argumentative discussion that would end in common agreement. The concept of appropriate structure of argumentative discourse is significantly important for developing argumentation support computer systems. We designed and built a small corpus to investigate what kind of argumentative discourse structure is appropriate and is leading to consensus building.\nOur corpus contains web discussions taken from Wikipedia, free encyclopedia Talk pages. For convenience we selected English language pages. We analyzed discussions provided by Moldova talk page and America talk page. We gathered a corpus containing 693 comments with the total number of participants 197 people and annotated the data with the tag set containing RST rhetorical relations, few relations borrowed from related works and few novel relations we introduced for the research purpose. As a result, our corpus includes 627 relations that connect the comments.\nWe analyzed the corpus for verifying the assumption that there exist patterns of agreement-oriented sequences of rhetorical relations that tend to lead to consensus within a discussion. Some preliminary results on rhetorical relations bigrams probability show that bigrams, certain rhetorical relations like Evidence or Explanation_argumentative will more frequently precede Agreement relation. To verify the assumption on longer structure sequences, we firstly need to considerably increase our corpus. We also think that it is important to introduce such parameter as participants ID of comments and consider relationship between participants during the analysis, because these are important factors for facilitating consensus.\nFor further corpus analysis, we will use the data obtained with help of the computer argumentation support system we are developing. The analysis results, on their turn will be used to improve the facilitation function of the system.\n\"That one.\" By using the barge-in timing of the user utterance, it determines that \"Ginkaku-ji Temple\" is specified by the user. This kind of dialogue in which items are read out in a list is important for two reasons. First, the user can indicate the referent by timing information, which is detected robustly. Barge-in timing is more reliable than ASR results in many cases. Therefore, this new dialogue strategy enables the system to obtain the user intention by reading out each item even in noisy environments. Second, this dialogue often appears when a system displays a retrieval result in the information retrieval task. This task is a promising one for conversational dialogue systems and is being developed at several companies such as Microsoft [1] and Google 1 .\nWe have developed a method for identifying the user's referent during system enumeration by focusing on barge-in utterances while the system lists choices [2]. Our purpose is to identify the user's referent with a high degree of accuracy. We exploit utterance timing together with ASR results to identify the user's referent as follows. First, we determine the relationships between the timing and content of a user utterance in order to use timing information. Then we construct a framework in which both timing information and ASR results are represented as probabilities. By using these probabilistic representations, we can obtain the most relevant interpretation as the one having the maximum likelihood [2]. We furthermore improve the interpretation obtained from ASR results in order to handle user utterances that include no content words in each item. Specifically, we introduce the interpretation of utterances with numbers. We also propose interpreting utterances that include words related to the items. We collect documents from the Web and use Latent Semantic Mapping (LSM) [3] to measure the closeness between the utterance and each item.\nInterpretation using utterance timing has not been investigated although barge-in has attracted the attention of researchers concerned with spoken dialogue systems, specifically, the issue of barge-in detection [4,5]. Their purpose has been to detect users' barge-in occurrences quickly and accurately. McTear [6] focused on how to stop a system utterance in order to recognize a user's barge-in. Str\u00f6m [7] discussed a system's behavior when barge-ins were incorrectly detected. We report a new interpretation that utilizes the locutionary act of barge-in, on the assumption that the barge-in detection is correct.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Modeling of User's Utterance Timing", "text": "We investigate the relationships between the content of user utterances and utterance timing to utilize barge-in timing. Here, we define utterance timing as the temporal subtraction of when a system utterance starts and when a user utterance starts (see Figure 1). While a system enumerates choices for a selection, the user utters referential expressions or content expressions to select one item. The former indicates an utterance that contains a reference term, such as \"that one\" or a pronoun. The latter indicates an utterance containing content words, such as \"Kinkaku-ji Temple.\"If the user utters a content expression,    the user conveys his intention not by the timing but by the content. On the other hand, a characteristic distribution of the utterance timing must be in the referential expression to convey a user's intention.\nWe determine how utterance timing of referential expressions is distributed. We collected user utterances under two different conditions (see Table 1). PAUSE represents the interval of time between items and AVERAGE represents an average length of enumerated items. Utterance timing is detected by using the voice activity detection of an ASR engine, Julius [8]. The distributions of utterance timing of both conditions are shown in Figures 2 and 3 as histograms. The bars in the histograms denote the relative frequencies of utterances in their timing, multiplied by the bar's width to represent the probabilistic density. The widths are set to 0.5 seconds. We can see clear peaks in both figures, although their peak positions and attenuation are different.\nWe model the histograms representing utterance timing of referential expressions by Gamma distribution:\nf (t) = 1 (\u03c3 \u2212 1)!\u03c1 \u03c3 (t \u2212 \u03bc) \u03c3\u22121 e \u2212(t\u2212\u03bc) 1 \u03c1 (1)\nZhou et al. also claimed that the time required for human perception follows Gamma distribution [9]. Equation (1) has three parameters: \u03bc, \u03c1, and \u03c3. The details of how these parameters are set was explained in our previous paper [2]. The Gamma distributions are also illustrated in Figures 2 and 3. Their parameters are as follows: \u03bc = 1.2, \u03c1 = 0.3 and \u03c3 = 2.0 in Figure 2; \u03bc = 2.2, \u03c1 = 1.5and \u03c3 = 2.0 in Figure 3.", "publication_ref": [], "figure_ref": ["fig_2", "fig_15", "fig_15", "fig_15", "fig_28"], "table_ref": ["tab_2"]}, {"heading": "Identifying User's Referent Using Barge-in Timing and ASR Results", "text": "We present a framework in which both utterance timing and ASR results are uniformly represented as probabilities. This enables us to identify a user's referent as an item having the maximum likelihood.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic Formulation", "text": "We formulate the problem of identifying a user's referent by calculating T i such that the probability P (T i |U ) is maximized. Here, T i denotes the i-th item enumerated by a system, and U denotes a user utterance. That is, P (T i |U ) represents how probable it is that U indicates T i corresponding to each item in the system's enumeration. We calculate the probability for each T i and then determine the user's intention, T .\nT = argmax Ti P (T i |U ) = argmax Ti P (U |T i )P (T i ) P (U ) = argmax Ti P (U |T i ) (2)\nWe assume all the prior probabilities P (T i ) are equal. P (U ) is not dependent on i. We calculate P (U |T i ) in accordance with Equation (2) by considering the possibilities of two cases: interpreting user's intention by either the utterance timing, C 1 or the content of the utterance, C 2 . Thus, P (U |T i ) can be represented as the following sum:\nP (U |T i ) = \u03a3 k=1,2 P (U |T i , C k )P (C k |T i ) ( 3 ) = 1 2 \u03a3 k=1,2 P (U |T i , C k ) ( 4)\nHere we assume that these prior probabilities P (C k |T i ) are even. We set the coefficient \u03b1 as the score ranges between P (U |T i , C 1 ) and P (U |T i , C 2 ) by setting a parameter \u03b1 , as shown in Equation (5).\nP (U |T i ) = (1 \u2212 \u03b1)P (U |T i , C 1 ) + \u03b1P (U |T i , C 2 ) ( 5 )\nEquation (5) denotes that the two cases are considered for all user utterances. P (U |T i , C k ) denotes the probability of an occurrence of user utterance U in the case of C k for each item T i . We assume that U contains two elements: U = {X, t b }. Here, X indicates an ASR result and t b denotes the time at which the user barges in during the system's utterance. Both P (U |T i , C 1 ) and P (U |T i , C 2 ) are defined in the following subsections. The flow of our method of identifying a user's referent is shown in Figure 4.", "publication_ref": [], "figure_ref": ["fig_29"], "table_ref": []}, {"heading": "Probability Defined by Using Barge-in Timing", "text": "We define P (U |T i , C 1 ) by using utterance timing since C 1 is defined as the case when a user expresses his intention by using utterance timing. Therefore, we  assume probability P (U |T i , C 1 ) depends not on an ASR result X but on bargein time t b only. Here, t i denotes the utterance timing after the system starts enumerating item T i (see Figure 1); that is,\ni i i i T X T \u22c5 + \u22c5 \u2212 = \u03b1 \u03b1 ) , | ( ) , | ( 1 1 C t P C U P i i i T T = ) , cos( ) , | ( 2 X T T X i i C P = [ ] n i w w L 1 = T [ ] n s s L 1 = X cosine distance\nt i = t b \u2212 start(T i )( 6 )\nThus, P (U |T i , C 1 ) is calculated as follows:\nP (U |T i , C 1 ) = P (t i |T i , C 1 ) ( 7)\nNote that the probability P (t i |T i , C 1 ) represents a case when a user indicates a specific item, T i , in timing t i . Therefore, the probability corresponds to the Gamma distribution we found in Section 2. We use the distribution f (t i ) as P (t i |T i , C 1 ).", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Probability Defined by Using ASR Results", "text": "The probability P (U |T i , C 2 ) represents how close a user utterance U (ASR result X) and each item T i are. We define P (U |T i , C 2 ) by using an ASR result in accordance with the definition of C 2 [2], except for some utterances for which we also need to use barge-in timing t b . One example utterance is \"The item before last.\" This example needs to be interpreted by using both the user's barge-in timing and the ASR result. That is, we need to know what a user said, and when.\nThe closeness is defined by cosine distance:\nP (U |T i , C 2 ) = cos(T i , X)( 8)\nwhere X and T i are M -dimensional vectors. M is the vocabulary size of the system. The elements of T i are TF-IDF values [10] of all nouns in the enumerated items in order to account for the word importance. The vector X corresponds to  the ASR result for the user utterance U . This vector consists of ASR confidence scores for the M nouns. By considering ASR confidence scores when calculating the probability, damage caused by ASR errors is alleviated.\nTo interpret utterances that include numbers such as \"The second one\", we add such number words into the vocabulary. For example, \"first\" is added to the vector T 1 corresponding to the first item. The size of X also increases accordingly. After adding these words, \"the second\" can be interpreted to indicate the second item in the system's enumeration, for example. When \"The item before last\" is recognized, we first estimate the interrupted item by using barge-in timing t b and calculate the most likely user selection by the number of the items and ASR result. Then we assign the average confidence scores for words in ASR results to the corresponding element of vector X.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation of Barge-in-able Dialogue System", "text": "The overview of the architecture of our barge-in-able dialogue system is depicted in Figure 5. The process flow is summarized as follows: The multi-channel AD/DA, RASP of JOEL System Technology captures a mixed sound and the wave file of the system utterance into a 2-channel wave stream. The ICA-based semi-blind source separation [11] obtains this wave stream and separates the user utterance incrementally. The ASR engine, Julius [8], then recognizes the separated user utterance and begins to record when the utterance starts. The item-identification sub-system identifies the user's referent on the basis of ASR results and the barge-in timing and generates a system response. We used Voice-Text 2 developed by PENTAX Inc. as a Text-to-Speech (TTS) engine.", "publication_ref": [], "figure_ref": ["fig_116"], "table_ref": []}, {"heading": "Conditions of Experimental Evaluation", "text": "We collected 400 utterances from 20 subjects. The utterances consisted of 263 referential expressions and 137 content expressions. The system listed news titles in 10 RSS feeds, and the subjects were told they could interrupt the system utterance and say whatever they liked. The number and length of titles are different for each RSS feed. We set three pause lengths between enumerated items: 1.5, 2.0, and 3.0 seconds. The parameters of the Gamma distribution used in our method were determined beforehand as follows: \u03bc = 0.73 and \u03c3 = 2.0. The parameter \u03c1 of Gamma distribution was determined in accordance with the pause lengths between items and the contents of enumerated items. We set \u03b1 in Equation (5) to 0.6 empirically. Accuracies when \u03b1 is changed are shown in Section 4.3. We used an acoustic model containing pink noise, which reflects the actual acoustic environment. We made a statistical language model by using the CIAIR corpus [12] and news articles obtained from each RSS feed. On average, the vocabulary size was 5835.\nWe evaluated several methods by identification accuracies, that is, how well the system correctly identified the user's referents. Each method is listed below:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cond. 1: Use of barge-in timing only", "text": "A user's referent was the item that had just been read out or presented when a user started speaking.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cond. 2: Use of barge-in timing model only", "text": "A user's referent was identified by the using the timing model of Gamma distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cond. 3: Our method (not extended to interpret numbers)", "text": "A user's referent was identified by the identical method to [2]. Cond. 4: Our method (explained in Section 3) A user's referent was identified by our method extended to interpret numbers.\nConds. 1 and 2 correspond to simpler methods in which no ASR results are used. We set these to verify how well the timing model works and whether ASR results are necessary or not. In Cond. 3, the vector size M and the number of items N varied with the number of enumerated news articles. On average, M was 104.5, and N was 15.8. In Cond. 4, M was 173.5. The ASR word accuracy for all utterances was 38.3%. Reasons for the low accuracy include sound reflections or distortions during the sound source separation since we used a microphone embedded in a robot instead of using a normal close-talk microphone. Also, correctly recognizing a user's utterances is difficult because these users often speak quickly or quietly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "The identification accuracies of the user's referent for 263 utterances with referential expressions, 137 utterances with content expressions, and all 400 utterances are shown in Table 2. Accuracy of Cond. 2 was better than that of Cond. 1. This result shows the utterance timing model formulated as Gamma distribution works effectively. Moreover, the timing information is also effectively used for interpreting content expressions, because some content utterances were identified correctly even though users conveyed their referent by content words. The identification accuracy of Cond. 3 was 71.8% for all utterances, outperforming the accuracies of Conds. 1 and 2. In particular, the accuracy for content expressions also improved by 21.2 points compared with that of Cond. 2. The result suggests using the ASR results is effective although its accuracy is not high. The identification accuracy of Cond. 4 was 75.8% for all utterances, which outperformed the accuracy of Cond. 3. In fact, the identification accuracy of content expressions including numbers improved by 27 points more than that of Cond. 3. The differences between Cond. 3 and 4 for referential expressions and total utterances were statistically significant (p < 0.01) by t-tests. Most significantly, the accuracy for referential expressions of Cond. 4 also improved by 3.8 points more than that of Cond. 3. These utterances can be identified after scores of incorrect ASR results decreased due to the number being considered.\nThe highest accuracy of referential expressions was obtained by Cond. 2. This case corresponds to \u03b1 = 0.0. Table 3 lists identification accuracies in Cond. 4 when \u03b1 is changed from 0.0 to 1.0. When we set \u03b1 to 1.0, a user's referent is identified by only P (U |T i , C 2 ). In this case, the identification accuracy of referential expressions is very low because ASR results of referential expressions such as \"That one\" contain no information associated with any items. When we set \u03b1 smaller, P (U |T i , C 1 ) was emphasized and more referential expressions were correctly identified. This result indicates the trade-off between P (U |T i , C 1 ) and P (U |T i , C 2 ). To improve the accuracy for referential expressions in our methods, we should dynamically determine \u03b1 in Equation (5) for each user's utterance. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_20"]}, {"heading": "Extending Acceptable Utterances by LSM", "text": "The user often tries to convey his or her intention using related words, that is, content words that were not included in the enumerated items. This utterance, for instance, includes \"The Beckham's result\" corresponding to the item \"Soccer.\" To deal with this utterance, we collect the documents obtained by copying sentences from Wikipedia 3 pages related to each item. Here, P (U |T i , C 2 ) represents how close a user utterance U (ASR result X) and the documents from the Web corresponding to each item T i are, and it is calculated by using LSM [3]. We decompose the co-occurrence matrix to obtain the k-dimensional vectors of all the documents. We construct a M \u00d7 N co-occurrence matrix between the items and the documents, where M is the vocabulary size and N is the total number of the documents. We applied singular value decomposition (SVD) to the matrix and compressed its rank to k. Here, k corresponds to N \u2212 2. The k-dimensional vectors were calculated on the basis of the matrix obtained from the SVD. We estimate P (U |T i , C 2 ) by calculating the cosine distance between the kdimensional vectors of the user's utterance and those of the documents. The user's utterance was recognized using a statistical language model that was based on the documents for each RSS feed. The documents consist of the data from Wikipedia and the 115 command utterances such as \"Let me hear the news\". On average, the size of the vocabulary was 17253. The ASR word accuracy was 37.5%. The size of the co-occurrence matrix M corresponds to the size of vocabulary. The k-dimensional vector of the user's utterance was calculated from its ASR confidence scores and the matrix obtained from the SVD.\nWe apply LSM only when a user specifies the item by using related words to avoid misinterpretation by applying LSM to all utterances. We compare two acoustic likelihoods to select utterances to apply LSM. One is calculated by using a language model for LSM and the other by using language model used in Cond. 4. We obtain the difference between them by subtracting the latter from the former. We use LSM only when the difference is more than 90. This value is empirically determined.\nWe evaluated the effectiveness of using LSM to identify the user's referent. The identification accuracy by using LSM is shown in Table 4. Here we set \u03b1 in Equation (5) to 0.6. Table 4 shows that the identification accuracy outperformed that of Cond. 4. In fact, the one utterance that only has a content expression with related words in the data became identified correctly.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_48", "tab_48"]}, {"heading": "Conclusion", "text": "We created a novel model of users' barge-in timing and developed an identification method by integrating the timing model with ASR results as a probabilistic representation. As a result, we made a barge-in-able conversational dialogue system that reads out news articles obtained from RSS feeds.\nOur method covers only a sub-dialogue where a user selects one item when a system lists choices. In a natural conversational interaction, users can make a variety of barge-in utterances; for example, to conclude the conversation quickly, to correct misunderstandings, or to assert themselves strongly -not only to indicate their referent. Nevertheless, this work is the first step towards achieving such an intuitive interaction in conversational dialogue systems. We developed a new interaction exploiting barge-in timing model and showed that it can improve the accuracy of identifying a user's referent, especially in barge-in-able conversational dialogue systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Since 2003, bow tie diagrams [5] have been used as a tool for risk analysis in several industrial fields such as energetic, automobile etc. Their success can be explained by the fact that the whole scenario for each identified risk also called top event (TE) is clearly represented via two parts: the first corresponds to a fault tree defining all possible causes leading to the TE and the second represents an event tree to reach all possible consequences of the TE. In addition, bow tie diagrams allow to define in the same scheme preventive barriers to limit the occurrence of the TE and protective barriers to reduce the severity of its consequences. In spite, its widely use in many organizations, this method remains limited by its technical level which is restricted to graphical presentation of different scenarios without any suggestion about optimal decisions regarding the expected objectives. In the literature few researches have been carried out to deal with the building phase of bow tie diagrams and their exploitation in the decision problems. Indeed, we have noticed that the researchers are usually interested in its quantification phase [7] [9], while the construction one is always assigned to the experts. We can in particular mention [6] where different steps have been proposed to build the bow tie diagrams, this approach is mainly based on the experts knowledge. This paper proposes a new Bayesian approach to construct bow tie diagrams which reflect the real behavior of the existing systems. In fact, we will generalize the actual deterministic bow tie diagrams to probabilistic ones, by replacing the logical AND and OR gates by conditional probability tables (CPTs). Our approach is based on two phases, namely, a structure learning phase relative to the graphical component of bow ties and a parameters learning phase relative to their numerical component.\nThe remainder of this paper is organized as follows: Section 2 presents a brief recall on the bow tie diagrams analysis. Section 3 details our new learning approach. And finally, section 4 presents an illustrative example in the petroleum field.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Brief Recall on the Bow Tie Diagrams Analysis", "text": "The bow tie diagrams are a very popular and diffused probabilistic technique developed by Petroleum companies for dependability modeling and evaluation of large safety-critical systems [5]. The principle of this technique is to build for each identified risk R i (also called top event (T E)) a bow tie representing its whole scenario on the basis of two parts, as shown in figure 1: The bow tie diagrams also allows the definition, in the same scheme, of some preventive barriers to limit the occurrence of T E and also of protective barriers to reduce the severity of its consequences. These barriers can be classified as active if they require a source of energy or a request (automatic or manual action) to fulfill their function (e.g. a safety valve, an alarm etc.) or passive if they do not need a source of energy nor a request to fulfill their function (e.g. a procedure, a retention dike, a firewall etc.). The major problem with bow tie diagrams is that they are limited by their technical level and by their restriction to a graphical representation of different scenarios without any consideration to the dynamic aspect of the real systems. In fact, the logical AND and OR gates represent deterministic causal relationships, which do not always reflect the real systems behavior. For instance, the OR gates means that the fail of a component implies the global fail of the related system while we can easily imagine that some functionalities of this system will be maintained even with a small probability. In addition, The choice of the appropriate barriers is not an easy task, since it depends on many criteria such as effectiveness, reliability, availability and cost [5]. Thus their definition from expert experience without any consideration of real data may affect their quality since it seems unrealistic to suggest static recommendations in real dynamic systems. To overcome this problem we propose to learn bow ties from real data and to improve them by adding a new numerical component allowing us to model in a more realistic manner the system behavior.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "A New Algorithm to Construct Bow Tie Diagrams", "text": "Our aim is to construct bow ties which reflect the real behavior of the existing systems i.e which are not exclusively based on the experts knowledge in order to make them more effective and useful. To this end we will consider bow ties as probabilistic graphs, denoted by BT , having:\na tree-structured graphical component T on a set of n nodes V = {X 1 , .., X n } s.t. each node X i represents an event (e.g. IE, CE, SE etc.). All these events are considered as binary (present or absent), thus all variables in V can have two states True (T) or False (F). T can be dispatched into two subtrees such that T E represents the unique root of the first one corresponding to the ET and the unique leaf of the second one corresponding to the F T . In the remainder X 1 is considered as the top event T E. The set of different arcs connecting nodes in V is denoted by A. -a numerical component allowing us in one hand to characterize the impact of different causes on the top event T E and in the other hand to study its repercussion while considering its severity and those of its consequences. Thus\n\u2022 to each node X i in F T , we will assign a conditional probability table (CPT) in the context of its parents (i.e. P (X i | P a(X i )) where P a(X i ) denotes the parent set of (X i ). These tables define the behavior of different events regarding their causes. This means that they will generalize the logical AND and OR gates. For instance, if X 2 AND X 3 cause X 1 , this means that in the CPT lied to X 1 , P (X 1 = T | X 2 = T, X 3 = T ) = P (X 1 = F | X 2 = T, X 3 = F ) = P (X 1 = F | X 2 = F, X 3 = T ) = P (X 1 = F | X 2 = F, X 3 = F ) = 1 and that the remaining entries are null. The same relation can be represented with a more flexibility via probability degrees pertaining to the unit interval. \u2022 to each node X i in ET , we will assign a value relative to its severity w.r.t each of its children X j (i.e. its consequences). This value quantifies the impact of a realization of X i on X j . In the literature several methods are proposed to define the severity of an event. Here we will consider that this value is equal to P (X j = T | X i = T ).\nRoughly speaking, bow ties here will have, almost, the same aspect than classical ones, except for AND and OR gates, which will be replaced by CPTs. Moreover they will have an additional component defining severity values in the ET . Regarding the barriers, we propose to define them in a dynamic way as detailed in section 4. Note that probabilistic trees are particular case of Bayesian networks which are powerful tools in reasoning under uncertainty. Thus to build bow ties we can use any standard learning algorithm relative to probabilistic trees as described below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "New Approach to Learn Bow Ties Structure", "text": "To learn bow ties structure, we propose to use the standard tree building algorithm proposed by Chow and Liu [3]. This algorithm is derived from the Maximal Weight Spanning Tree (MWST) [4] and has as input a training set (denoted by T S) which generally corresponds to a set of records observed during a given period, and as output a spanning tree, denoted by U T = {U, E} s.t. U is the set of nodes and E is the set of edges (since U T is undirected). This algorithm, has shown its success to obtain the optimum tree structure from training set [8]. In order to find different dependencies, this algorithm uses the mutual information I ij between each pair of variables (X i , X j ) s.t. i = j in T S expressed as follows:\nI i,j = xixj P ij (x i , x j )log( P ij (x i , x j ) P i (x i )P j (x j ) (1)\nwhere P ij (x i , x j ) (resp. P i (x i )) is the proportion of observations in the training set T S s.t. X i = x i and Y i = y i (resp. X i = x i ) i.e. the number of these observations divided by the whole number of observations in T S. Formally, Chow and Liu algorithm [3] can be outlined as follows (in this version the root is defined as a prior): estimate). Thus if \u03b1 ijk > 0 thenP (X i = k | P a(X i ) = j) will not be equal to 0.\nIn what follows, we will use uniform prior i.e. \u2200i, j, k \u03b1 ijk = 1.\nTo compute severity degrees relative to ET , we should compute for each node X i in ET (except M E), a vector S i s.t. S i [j] is the severity of X i w.r.t to its children X j . In the literature several methods are proposed to define the severity of an event. Here we will use T S ET to compute it by considering that S i [j] = P (X j = T | X i = T ). To compute this value we use Bayes theorem as follows:\nS\ni [j] = P (X j = T | X i = T ) = N ij N i (3)\nwhere N ij is the number of instances in T S ET where X i = T and X j = T occur conjointly and N i is the number of instances in T S ET where X i = T .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Global Learning Approach", "text": "The global approach can be summarized as follows: \n\u2200X i \u2208T F T compute P (X i = k | P a(X i ) = j) using equation (2) \u2200X i \u2208T ET compute S i using equation (3) end\nOnce the bow tie is constructed (structure and parameters), we can use it to propose appropriate protective and preventive barriers. This process can also be improved by taking into account learned conditional probability tables and different severity degrees via inference mechanism [10]. In fact, we can easily, imagine a dynamic way to propose such barriers while taking into consideration available resources.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Illustrative Example", "text": "This section illustrates our method via an example released in TOTAL TUNISIA company. Due to the lack of space we will limit our example to a unique risk relative to a major fire and explosion on tanker truck carrying hydrocarbon (T E). To construct the relative bow tie we have identified six events leading to T E (i.e. hydrocarbon gas leak (HGL), and source of ignition close to road (SI) tank valve failure (T V F ), exhaust failure (EF), and construction site close to the truck parking (CT P )) and nine events representing its consequences (i.e pool fire(P F ), thermal effects (T HE), toxic effects (T O), production process in stop (P P S), thermal damage to persons (T DP ), damage to the other trucks (DT ), toxic damage to persons (T ODP ), damage to environment (DE) and late delivery (LD)). The training set relative to causes T S F T and consequences T S ET is given in table 1 where value 1 means false and 2 true.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Table 1. Training set", "text": "Training set relative to causes (T SFT )\nTE 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 EF 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 CTP 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 2 1 1 1 1 1 TVF 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 HGL 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 2 1 1 1 1 1 SI 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 1 1 1 1 1 1 1 2 1 1 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1\nTraining set relative to consequences (T SET )\nTE 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 LD 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 DE 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 1 1 1 1 1 2 TODP 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 1 1 1 1 2 2 DT 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 TDP 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 PPS 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 TO 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 1 1 1 1 1 1 THE 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 PF 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Bow Tie Structure", "text": "The first step to construct the structure of the bow tie (i.e algorithm 1) is to compute the mutual information between pairs of events (causes and consequences). The relative values to T S F T (resp. T S ET ) are given in table 2 (resp. 3), those in bold represent the best configurations (e.g. the more significant causes for T E are HGL and SI).\nUsing these values, the structure learning phase of algorithm generates the bow tie diagram illustrated by figures 2. This bow tie was validated by experts in Total Tunisie since it corresponds to the one they have already proposed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Bow Tie Parameters", "text": "Once the bow tie diagram is constructed, the second phase allows us to quantify it by assigning to each node in F T its CPT and to each node in ET (i.e. consequence of T E) its severity vector. These values are given by tables 4 and 5, respectively. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T E P F T HE T O P P S T E", "text": "\u2212 \u2212 \u2212 \u2212 \u2212 P F 0.8824 \u2212 \u2212 \u2212 \u2212 T HE \u2212 0.9375 \u2212 \u2212 \u2212 T O \u2212 0.9375 \u2212 \u2212 \u2212 P P S \u2212 0.9375 \u2212 \u2212 \u2212 T DP \u2212 \u2212 0.9444 \u2212 \u2212 DT \u2212 \u2212 0.9444 \u2212 \u2212 T ODP \u2212 \u2212 \u2212 0.9474 \u2212 DE \u2212 \u2212 \u2212 0.9474 \u2212 LD \u2212 \u2212 \u2212 \u2212 0.9474", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This paper proposes a new approach to construct bow tie diagrams which reflect the real behavior of exiting system i.e which are not exclusively based on the expert knowledge. Our approach is divided into two parts, first a learning algorithm is proposed to construct the whole scenario from IE to M E, and the second is a numerical component allowing us to characterize the impact of different causes on the top event T E and to study its repercussion while considering its severity and those of its consequences. To learn our bow tie diagrams we have proposed the algorithm 2. This latter uses Chow and Liu [3] algorithm, this choice was motivated by the fact that this algorithm provide us a spanning tree from a training set, which characterizes both FT and ET structure learning. This approach, lies in a recent work that we have recently proposed in order to implement a new process-based approach relative to an integrated management system: Quality, security, environment (QSE) [1]. More precisely, the bow tie diagrams are considered as input to define the appropriate management plan QSE [2]. As future work we propose to overcome the problem related to the implementation of preventive and protective barriers. In fact, the choice of the appropriate barriers is not an easy task, since it depends on many criteria such as effectiveness, reliability, availability and cost [5]. Thus their definition from experts experience without any consideration of real data, as done in actual applications, may affect their quality since it seems unrealistic to suggest static recommendations in real dynamic systems. Thus our idea is to benefit from the numerical component, that we have added to bow ties, in order to enable experts to interact with the system in a real time via inference algorithms [10] and muticriteria analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Automatic surveillance is essential for many scenarios that require remote monitoring. Several sensors of different modalities (e.g., video, acoustic, infra-red and seismic) may be deployed to monitor areas such as international borders or cleared buildings where a sustained presence of personnel is not feasible. In some cases, constraints on logistics do not permit the use of information rich sensors such as video. We consider one such scenario for indoor surveillance and investigate the use of seismic sensors for occupancy classification.\nDetecting the presence of objects using seismic sensors can be categorized into two broad areas: detection of (i) humans, and (ii) other objects (such as vehicles). Detection of vehicles is easier to accomplish than detection of humans, since the seismic signal from a vehicle has a higher signal to noise ratio (see [1,2,3]). The harmonic signatures of two vehicles of the same type are consistent, and those of different vehicle types are generally distinguishable. In contrast, classification of human occupancy using footstep signals is difficult in that different people walk with different gaits and at varying pace, resulting in vibration patterns that are difficult to identify uniquely.\nTo detect occupants based on footstep signal processing using seismic sensors, researchers use approaches such as auto-regressive modeling, signal moments, time-scale analysis and explicit experimental modeling [4,5,6,7]. Using a copula based approach, Iyengar et al. [8] fuse signals from acoustic and seismic sensors for footstep detection.\nAlthough the detection of the presence of humans has been successfully addressed by such researchers, finer-grain analysis is much more difficult. One such challenging task, addressed in this paper, is the determination of whether one or more people are walking together in the environment being monitored. This is because people walking together have a psychological tendency to walk in \"lock step\" [9]. Since the raw signals are not easily classifiable, an important step is to derive suitable features that can assist classification. One of the main contributions of this work is to develop features that enable us to distinguish between the presence of one or more than one person in the given region of interest.\nIn Section 2, we describe the data collection process. The raw data is preprocessed to extract useful information, and to eliminate noise, using a methodology based on empirical mode decomposition (EMD), described in Section 3. Our approach for feature selection is presented in Section 4. Section 5 contains the classification results and Section 6 contains our conclusions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Collection", "text": "Six GS 20DX geophones were used for the purpose of data collection. The geophones are designed to be floor mounted. The sensors were configured as a linear array. Data was collected in two (different) building hallways of similar construction. The sensors were placed along the long edge of the hallway. The distance between two adjacent sensors was maintained at 5ft. Data was acquired using a 16 bit A/D converter at a sampling rate of 5kHz. The raw signal was uniformly down-sampled to 1024 per second. The approximate duration of the data collected per trial is 12 seconds.\nMultiple persons participated in the data collection. The footstep data thus collected consists of 120 single-person trials (i.e., a given trial has exactly one participant walking along the hallway) and 120 two-person trials (a given trial has exactly two participants walking along the hallway). Each dataset consists of 60 trials from Building 1 and 60 trials from Building 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview of the Preprocessing", "text": "In order to derive reliable features from pseudo-periodic signals like footsteps, it is of interest to extract a \"clean\" envelope corresponding to footfalls. We have used empirical mode decomposition (EMD) [10] for this purpose. EMD is a data-driven decomposition technique that captures oscillations at several scales. Each of these scales is called a mode and the function corresponding to a mode is called an intrinsic mode function (IMF). In order to capture interesting characteristics of footsteps and eliminate inherent noise present in the data, we have used only a small subset of these modes based on the total variation (TV) norm [6] such that the sum of TV norm of the selected modes is 90% of the total TV norm. For a time-series of length T , these modes are treated as random vectors with T observations. These vectors are linearly combined, using the first principal component. 2 The raw data is denoted by y (k) i,j (t) and the processed data so obtained is denoted by x (k) i,j (t) for i = 1, . . . , 6, j = 1, . . . , 120, k = 1, 2, t = 1, . . . , T ; where i represents the sensor, j refers to trial, k refers to number of persons, and t is the time index. This processed data is used in the rest of the paper to extract features for classification.\nThe effect of this preprocessing is seen in Fig. 3. The EMD based processing extracts a faithful envelope of the raw signal revealing the salient features of each footfall. i,j (t) and the result of envelope extraction, x\n(2) i,j (t), for the 2 persons case", "publication_ref": [], "figure_ref": ["fig_28"], "table_ref": []}, {"heading": "Feature Selection", "text": "In this section, we present four features that were employed to achieve classification between presence of one person versus two persons. We assume that the detection has already been accomplished successfully, i.e., it is certain that at least one occupant is present.\nThe seismic signal of a sensor decays rather rapidly as person(s) move away from the sensor. For each sensor separately, we, therefore, extract a five second segment of the \"useful\" data as proposed below; the discarded data is deemed to be non-informative. For each i, j, k, first we find the time index where x (k) i,j is maximum and extract data for all time indices that are contained in \u00b1t o on either side of the index where the maximum occurs. We choose t 0 = 2.5 \u00d7 1024, because, as stated earlier, we collect 1024 samples per second. For instance, if t max was the time when x (k) i,j takes it's maximum value, then the extracted data will be {x (k) i,j (t) : t = t max \u2212 t 0 , t max \u2212 t 0 + 1, . . . , t max + t 0 }. However, for notational convenience we renumber the time indices and denote the extracted data as {z for all values of (i, j, k). Note that this data will be extracted from the early part of {x (k) i,j (t)} for the first sensor (i = 1) and later part for the sixth sensor (i = 6). In the following subsections, features are extracted from these 'extracted' data sets only.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Periodogram and Autocorrelation", "text": "Footsteps are quasi-periodic signals. Typically, the periodogram and autocorrelation function (ACF) are used in determining signal periodicity [2,3,11]. But, analysis by Houston and McGaffigan [9] shows that the use of spectral measures is not useful for counting the number of personnel in the region of interest. In the following discussion, we note that while the periodogram and autocorrelation are not directly useful for our occupancy classification problem, nevertheless they provide some valuable insights. Fig. 2 shows a comparison of the periodograms of 20 trials of the one person and two person cases. The figure suggests that the periodogram alone may be insufficient for the classification task at hand.\nWe expect that \"heel-toe\" transitions will be borne out in the single occupant case and will be blurred in the case of two or more occupants. In order to find support for this argument, we plotted the mean of the ACF across sensors for a single period (Fig. 4.1). We use the peak frequency from the periodogram to obtain the period of the ACF.\nClearly, the ACF for one person goes through a typical pattern of local maxima and minima as heel and toe spikes align with each other. When the ACF for the first period are averaged over all the training data for all sensors in the one-person case, we obtain the ACF template seen in Fig. 3(a). Therefore, the feature extraction procedure is, 1. Form the template (Fig. 3(a)) 2. Calculate the area which measures the difference between the template and the ACF for a given trial (see Fig. 4). This difference is the mean-square error between the two curves and is denoted as M SE (k) i,j , which is the first of the four features that we use for occupancy classification.  ", "publication_ref": [], "figure_ref": ["fig_15", "fig_29", "fig_28", "fig_28", "fig_29"], "table_ref": []}, {"heading": "Signal Energy", "text": "Variance is a measure of signal energy and serves as an intuitive indicator of occupancy. That is, when the number of occupants is large, the signal energy will be large. Using the 'extracted' data described earlier we calculate the variance as described below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "S", "text": "(k) i,j\n2 = 1 2t o 2to t=0 z (k) i,j (t) \u2212 z (k) i,j 2 , (1\n)\nwhere z (k) i,j is the sample mean (of the 2t 0 observations). (i\u22121),j (t + \u03c4 ))\nr i\u00b1 (\u03c4 ) = Corr(z\n(k) (i+1),j (t), z (k) (i\u22121),j (t + \u03c4 ))(8)\nR i (\u03c4 ) = r i+ (\u03c4 )r i\u2212 (\u03c4 ) \u2212 r i\u00b1 (\u03c4 )\nwhere \u03c4 extends from -1024 to 1024 (i.e., up to a shift of 1 second in both directions).\nWe expect that, in the ideal situation, R i = 0 for the 1-person case and R i = 0 when two or more persons are walking. In practice, R i is expected to take a small value for the 1-person case and a large value in the 2 or more persons case. In Fig. 5, we observe wide major peaks and thinner auxiliary peaks in the two-person case. In other words, the 2-person information is contained at different scales. This suggests that wavelet decomposition would be useful in extracting useful information from R i (\u03c4 ).\nWe chose the Mexican hat wavelet and calculated the continuous wavelet transform (scalogram) of {R i (\u03c4 ) : \u03c4 = \u22121024 to 1024},\nC i (a, b) = all\u03c4 R i (\u03c4 )\u03c8 (\u03c4 \u2212 b, a)d\u03c4, (10\n)\n\u03c8 (\u03c4, a) = 1 \u221a 2\u03c0a 3 1 \u2212 \u03c4 2 a 2 exp \u2212 \u03c4 2 2a 2\nWe note that, in theory, the scale a can have an infinite number of values. For numerical computation, we chose the maximum value of a to be 1/(2f * ) where f * is the peak frequency from the periodogram of the extracted data. ", "publication_ref": [], "figure_ref": ["fig_116"], "table_ref": []}, {"heading": "Classification Procedure and Results", "text": "Section 4 discussed the features investigated for classification problem of determining one vs. two occupants in a building hallway. There are two possible fusion schemes for the selected features: (k) j , and S Ci for each i. Note that there is only one value for the third feature and 4 values for the fourth feature giving a total of (6 + 6 + 1 + 4 = 17)-dimensional vector. In this case the fusion is achieved by dimensionality reduction using principal component analysis (PCA) retaining those combined features that capture 98% of the total variation. PCA is performed prior to classification.\nIn both cases, a neural network classifier is used with one hidden layer of 6 nodes 3 . Classification performance is analyzed for the following five cases,  1 and 2 contain the results on test data; cases 2, 3 and 5 are the average of 10 iterations. We conclude that, in general, the classification performance is very good. Best performance is observed for Case 3 (Building 2). This indicates that selected features are good. If, somehow, we can improve the method of sensor data collection, then the classification performance will further improve.  The 'good' classification performance of the classifiers trained on the data from one building and tested on the same building supports the above conclusion.\nTo assess the contribution of each feature alone we experimented with Case 5. It was observed that the third feature (R(states) (k) j ) contributes significantly to improving the classification performance (about 70%), whereas features M SE (k) i,j and S Ci make similar amount of contributions. The similarity in performance between these two features is not surprising because they are both derived from correlation sequences. Since the rationale for using S Ci depends on the linear configuration of sensors, one may consider dropping this feature for a simpler system design under more general topology of sensor deployment. We have noticed that in spite of dropping this feature we get a classification performance of 89.13% for Case 5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Concluding Remarks", "text": "We have addressed the task of distinguishing between the presence of one vs. two persons in a visually unobservable indoor region of interest, using data obtained from seismic sensors. After exploring multiple alternatives, we identified four features as being capable of assisting classification with a high degree of reliability. Classification was achieved using a neural network classifier.\nAlthough the current suite of four features has yielded very good performance, further improvements may be possible by exploring new feature extraction approaches. Performance gains may be obtained by using non-linear dimensionality reduction schemes [12] as opposed to principal component analysis, which is linear. Improved classification as well as finer classification is also expected using other modalities; such as the acoustic sensors. Since the data are vibrational in nature, the signal processing and feature extraction algorithms developed in this paper can also be applied to data collected using accelerometers and acoustic sensors. study is given, following which a criterion for base convex functions for Bregman divergences is proposed. In section 3, two groups of Bregman divergences are created and compared with each other. Finally section 4 concludes with future work being suggested.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Metric Multidimensional Scaling", "text": "We will call the space into which the data is projected the latent space: multidimensional scaling creates one latent point for every data sample. MMDS is a group of methods that keep interpoint Euclidean distances in latent space as close as possible to the original distances in data space, which is good at maintaining the global structure of the manifold. If we use D ij to represent the distance between points X i and X j in data space, L ij to represent the distance in latent space between the corresponding mapped points Y i and Y j , and the size of data set is N , the stress functions of MMDS can be generalised as\nE MMDS (Y ) = 1 C N i=1 N j=i+1 (L ij \u2212 D ij ) 2 W (D ij ) ( 1 )\nwhere C is a normalisation scalar which does not affect the embedding, and W (D ij ) \u2265 0 is a monotonically decreasing function of the distances in data space. This is sometimes used to emphasise local distances. \nE Sammon (Y ) = 1 N i=1 N j=i+1 D ij N i=1 N j=i+1 (L ij \u2212 D ij ) 2 D ij (3)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bregman Divergence", "text": "Consider a strictly convex function F : S \u2192 defined on a convex set S \u2282 d . A Bregman divergence [1] between two points, p and q \u2208 S, is defined to be d F (p, q) = F (p) \u2212 F (q) \u2212 (p \u2212 q), \u2207F (q)\nwhere the angled brackets indicate an inner product and \u2207F (q) is the derivative of F evaluated at q. This can be viewed as the difference between F (p) and its truncated Taylor series expansion around q. Thus it can be used to 'measure' the ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linking Metric MDS and Bregman Divergences", "text": "We could consider using Bregman divergences in either data space or latent space, however in this paper, we investigate using Bregman divergences between L ij and D ij instead of the squared distance between them which we used above. We will call this a Bregmanised Metric MDS (BMMDS) whose stress function is a sum of divergences:\nE BMMDS (Y ) = N i=1 N j=i+1 d F (L ij , D ij ) = N i=1 N j=i+1 (F (L ij ) \u2212 F (D ij ) \u2212 (L ij \u2212 D ij )\u2207F (D ij )) (6)\nWe may alternatively express this as the tail of the Taylor series expansion and so we can consider\nEBMMDS(Y ) = N i=1 N j=i+1 d 2 F (Dij) dD 2 ij (Lij \u2212 Dij) 2 2! + d 3 F (Dij) dD 3 ij (Lij \u2212 Dij) 3 3! + d 4 F (Dij) dD 4 ij (Lij \u2212 Dij) 4 4! + \u2022 \u2022 \u2022 (7)\nWe now show that linear MMDS is a Bregman divergence and that the Sammon Mapping can be seen as special case of approximation to Bregman divergences using only the first term of (7).\nLinear MMDS. When F (x) = x 2 , x > 0, eq (7) is identical to eq (2) with C = 1. i.e. linear MMDS can be viewed as a Bregman divergence using the underlying convex function, F (x) = x 2 . Of course, it is well known that the solution of the linear MMDS objective is to locate the latent points at the projections of the data points onto their first principal components i.e. minimising the divergence (in this case Euclidean distance) between the data points and their projections, so what we are really doing here is to restate an accepted equivalence in terms of Bregman divergences. Sammon Mapping. We define base convex function\nF (x) = x log x, x \u2208 R ++ ,( 8)\nThe higher order derivatives for this base function are shown in Table 1.\nThen the Sammon Mapping is the first term in (7); i.e. the Sammon Mapping can be viewed as an approximation to BMMDS. However with this underlying convex function, the higher order terms do not vanish and eq(7) can be expressed as  and C = 1 in eq(1), then MMDS is the first term of BMMDS (eq(7)) and BMMDS is an extension to MMDS.\nd 6 F (x) dx 6 1 x > 0 \u2212 1 x 2 < 0 2! x 3 > 0 \u2212 3! x 4 < 0 4! x 5 > 0 E ExtSammon (Y ) = N i=1 N j=i+1 (L ij \u2212 D ij ) 2 2!D ij \u2212 (L ij \u2212 D ij ) 3 3!D 2 ij + 2!(L ij \u2212 D ij )", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "The ExtendedSammon Mapping", "text": "Note that for base function eq(8) dF (x) dx = log x + 1. The ExtendedSammon mapping eq(9) is equivalent to\nE ExtSammon (Y ) = N i=1 N j=i+1 (L ij log L ij \u2212 D ij log D ij \u2212 (L ij \u2212 D ij )(log D ij + 1)) = N i=1 N j=i+1 L ij log L ij D ij \u2212 L ij + D ij (12\n) which is simple to implement.\nL h represents corresponding distances in latent space. D h , the mean of D h , is represented on the horizontal axis. The vertical axis shows\nMean Relative Distance = L h \u2212 D h D h , h = 1, 2, \u2022 \u2022 \u2022 , 40\nwhere L h = mean(L h )\nIt can be seen that distances less than 2.0 are compressed in all three mappings. Among these distances, those that are less than 1.0 are reduced most. Although it is well known that the Sammon mapping puts more stress on local distances and less stress on long distances, small distances are not projected close to their original value as expected. They are actually over compressed instead whereas long distances are preserved better. The reason for this phenomenon is that during optimisation, we can consider that short distances and long distances are competing for opportunities to go closer to their original values as much as possible. Although each individual short distance has a stronger effect than a longer one, the number of distances that are longer than 1.0 greatly surpasses the number of short ones, as shown in Figure 3(a), the histogram of the distances in the original data space. So the net effect is that the long distances are constrained as shown. That this effect is not due to an averaging of under and over compression can be seen in Figure 3(b). Nevertheless short distances mapped by the Sammon mapping are closer to their counterpart data distances than those mapped by LMMDS, which means that local distance preservation is improved. Again the ExtendedSammon mapping makes a further improvement to the Sammon mapping although the step from Sammon to ExtendedSammon is less than from LMMDS to Sammon. While keeping local distances short, the Sammon and ExtendedSammon mapping stretch long distances slightly in turn. In Figure 3(b) we can see that the relative deviation achieved by the Sammon mapping on small distances is smaller than by LMMDS, and it is the same scenario for ExtendedSammon over Sammon, which means, on the Swiss roll data, in the sequence of LMMDS, Sammon and ExtendedSammon, controlling local distances is more and more enhanced. It also can be seen that in the same order more and more freedom is given to large distances.", "publication_ref": [], "figure_ref": ["fig_28", "fig_28", "fig_28"], "table_ref": []}, {"heading": "Criterion for Base Convex Function Selection", "text": "Based on the above, this suggests that, for use in multidimensional scaling applications, a convex function F (x) defined on R ++ should satisfy\nd 2 F (x) dx 2 > 0, d 4 F (x) dx 4 > 0, \u2022 \u2022 \u2022 , d (2n) F (x) dx (2n) > 0 d 3 F (x) dx 3 < 0, d 5 F (x) dx 5 < 0, \u2022 \u2022 \u2022 , d (2n+1) F (x) dx (2n+1) < 0, n = 1, 2, 3, \u2022 \u2022 \u2022 (13)\nThus, if a convex function such as F (x) = exp(x), x > 0 is used as the base function, even although the overall stress minimises and L ij approaches D ij , it does not improve local distance preservation. Having met (13), the second order derivative is then primarily considered to be the most important, because it is a major influence in eq(7) as shown above. It needs to be big at small distances and small at long distances. The bigger it is at local distances, the greater force concentrates on small distances. The smaller it is at long distances, the more freedom the long distances have.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Two Groups of Bregman Divergences", "text": "Besides eq(8) as the base convex function for the ExtendedSammon mapping, two groups of more suitable base functions that meet the criterion discussed above are proposed in Table 2. However these use different strategies for increasing focusing power: in the first group from No. 1 to No. 4 the second order derivatives are higher for small distances so focusing power is increasing; the second group only gives limited maximum values to very small distances but reduces the value at long distances. For the same long distance, the bigger \u03bb is, the smaller the second derivative is, and thus the greater the focusing power is.\nBased on the proposed base functions other BMMDS stress functions are created as follows F (x) = \u2212 log(x): This is the Itakura-Saito (eq(5)) divergence . \nE IS (Y ) = N i=1 N j=i+1 IS(L ij , D ij ) = N i=1 N j=i+1 L ij D ij \u2212 log L ij D ij \u2212 1 (14)\n1 x \u2212 1 x 2 (\u22121) n (n\u22122)! x n\u22121 2 \u2212 log(x) 1 x 2 \u2212 2 x 3 (\u22121) n (n\u22121)! x n 3 1 x 2 x 3 \u2212 6 x 4 (\u22121) n (n)! x n+1 4 1 x 2 6 x 4 \u2212 24 x 5 (\u22121) n (n+1)! x n+2\ngeneric,t \u2265 1\n1 x t t(t+1) x t+2\n\u2212 t(t+1)(t+2)\nx t+3\n(\u22121) n t(t+1)(t+2)...(t+n\u22121)\nx t+n\nSecondGroup 5 e \u2212\u03bbx \u03bb 2 e \u2212\u03bbx \u2212\u03bb 3 e \u2212\u03bbx (\u2212\u03bb) n e \u2212\u03bbx \nF (x) = 1 x E Reciprocal (Y ) = N i=1 N j=i+1 1 L ij \u2212 1 D ij \u2212 (L ij \u2212 D ij )(\u2212 1 D 2 ij ) = N i=1 N j=i+1 ( 1 L ij \u2212 2 D ij + L ij D 2 ij ) (15) F (x) = 1 x 2 E InverseQuadratic (Y ) = N i=1 N j=i+1 1 L 2 ij \u2212 1 D 2 ij \u2212 (L ij \u2212 D ij ) \u22122 D 3 ij = N i=1 N j=i+1 ( 1 L 2 ij \u2212 3 D 2 ij + 2L ij D 3 ij )(16\nTable 3 summarises Bregmanised stress functions and weights of corresponding MMDS stress functions i.e. that part of the Bregman divergence whose contribution is solely from the second order derivative of the function. \nFirstGroup\nExtendedSammon eq (12) x log(x)\n1 D ij Lij log L ij D ij \u2212 Lij + Dij Itakura-Saito eq(14) \u2212 log(x) 1 D 2 ij L ij D ij \u2212 log L ij D ij \u2212 1 Reciprocal eq(15) 1 x 1 D 3 ij 1 L ij \u2212 2 D ij + L ij D 2 ij\nInverse Quadratic eq(16)\n1 x 2 1 D 4 ij 1 L 2 ij \u2212 3 D 2 ij + 2L ij D 3 ij\nSecondGroup Exponential Family eq(17) e \u2212\u03bbx e \u2212\u03bbD ij e \u2212\u03bbL ij \u2212 e \u2212\u03bbD ij + \u03bb(Lij \u2212 Dij )e \u2212\u03bbD ij Comparision of strategies. As discussed above the two groups of divergences improve the quality of projection using opposite strategies. The first group works by improving the mapping of short distances, while the second group stretches long distances, which is obviously good for unfolding. In our experiments on the Swiss roll data set, when t = 2 for the first group, or \u03bb = 15 for the second group, the optimisation convergence speed becomes significantly slower.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_20"]}, {"heading": "Experiment on the Open Box", "text": "The open box data set consists of 316 data points. On page 15 of [5], it is stated that \"it is connected but neither compact(in contrast with a cube or closed box) nor smooth (there are sharp edges and corners)\". It is interesting to see in Figure 4 that not all of the distances have the same opportunity to become longer. The result of the first group of divergences is displayed in 4(b) through 4(f). The box opens wider in the mouth, in other words the manifold has been found so that distances between points on opposite sides on the top are longer than on the bottom. Distances between points on the lid remain almost unchanged whereas on the bottom they have contracted. The mouth becomes rounder and rounder, and sharp points and edges are smoothed except on the lid. In a square consisting of four neighbouring data points in the four vertical sides, the horizontal side becomes longer than the vertical side. These results are in contrast to methods such as Curvilinear Component Analysis [3] which tear the box in order to create a two dimensional representation of the data.", "publication_ref": [], "figure_ref": ["fig_29"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We have shown that linear MDS can be thought of as a Bregman MDS with the special case when the underlying convex function is F (x) = x 2 and that the Sammon mapping is a truncated version of a Bregman divergence with the convex function F (x) = x log x. We have shown that the full Bregman mapping of some standard data sets improves upon the mapping provided by the Sammon mapping. We have used our intuition gained from this improvement to develop two groups of mappings based on different underlying convex functions: one group concentrates on shorter distances while the other gives more attention to the longer distances in data space.\nFuture work will investigate the creation of divergences which merge the properties of our two groups of divergences i.e. which simultaneously pay attention to small and large distances in data space. can just measure the distribution of y and find a W that maximizes the non-Gaussianity of the elements of y. A popular way to measure the nongaussianity of a vector is using kurtosis which is the fourth-order cumulant of a random variable.\nAn alternative criterion is to measure the entropy of the elements of y. Since a Gaussian random variable has more entropy than any random variable with the same variance, we can use negentropy, the difference between the entropy of the random variable and a Gaussian random variable of the same variance to measure how far we are from a Gaussian distribution.\nStone [6] however created an unusual but very intuitively satisfying criterion for ICA which we review next.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stone's Criterion", "text": "Stone [6] noted that many signals tend to be locally predictable and thus a criterion which can be used for ICA is to minimise the variance in the local signals. However a matrix W which does this alone is liable to be useless since it will probably tend to zero so that it will output a constant (zero) vector for each sample x. Thus we require an additional criterion which will move the outputs y away from zero. Stone notes that, to constitute a signal capable of imparting information, the signal must have maximal global variance though it is locally low in variance. Thus he proposes the scalar criterion for each element of y,\nJ Stone = log yt\u2208D (y t \u2212 y) 2 yt\u2208Dt (y t \u2212\u1ef9) 2 = log V U (1)\nwhere y t is the output, y, at time t, D is the complete data set, D t is local elements around time t, y is the global mean value of y and\u1ef9 is the local mean value. Thus V corresponds to the global variance while U corresponds to the local variance. Maximising J Stone then can be used to identify underlying signals which, while locally smooth and predictable, have high variance globally and therefore informative. Maximising J Stone then, corresponds to maximising the global variance (thus keeping the solution away from the uninformative constant output), and minimising the local variance (and thus searching for signals which are locally constant).\nIn practice, both V and U are updated in an online manner so that\nV = (1 \u2212 \u03bb l )V + \u03bb l (y t \u2212 y) 2 U = (1 \u2212 \u03bb s )U + \u03bb s (y t \u2212\u1ef9) 2\nwhere \u03bb l and \u03bb s are suitably chosen constants such that \u03bb l , used for the longterm memory is much smaller than \u03bb s which is used for the short term memory.\nStone uses gradient ascent in order to maximise J Stone and shows that, in some cases, it can extract the underlying signals from a mixture.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bregman Divergences", "text": "Consider a strictly convex function F : S \u2192 defined on a convex set S \u2282 d . A Bregman divergence between two elements, p and q, of S is defined to be d F (p, q) = F (p) \u2212 F (q) \u2212 (p \u2212 q).\u2207F (q)\n(\nwhere the '.' indicates an inner product and \u2207F (q) is the derivative of F evaluated at q. This can be viewed as the difference between F (p) and its truncated Taylor series expansion around q. Thus it can be used to 'measure' the convexity of F : Figure 1 illustrates how the Bregman divergence is the difference between F (p) and the value which would be reached from F (q) with a linear change for \u2207F (q). where \u03b8 = log p 1 \u2212 p and t(x) = x and G(\u03b8) = log(1 + e \u03b8 )\nOther well known members of this family are the multinomial, beta, Dirichlet, Poisson, Laplace, gamma and Rayleigh distributions. In the remainder of this paper, we consider only regular exponential families in which t(x) = x.\nWe define the expectation of X with respect to p G,\u03b8 to be\n\u03bc = E p G,\u03b8 [X] = d xp G,\u03b8 (x)dx (4)\nIt can be shown [2] that there is a bijection between the set of expected values, \u03bc, and the set of natural parameters, \u03b8. In fact, let d F be the Bregman divergence corresponding to the distribution, p G,\u03b8 . Then let g() = \u2207G and let f = \u2207F .\nThen \u03bc = g(\u03b8) and \u03b8 = f (\u03bc), which is readily verified for the distributions above.\nConsider a member of the regular exponential family with known cumulant function, G(\u03b8). Then G(.) is a closed convex function. Define its conjugate function as\nF (x) = sup \u03b8 {x.\u03b8 \u2212 G(\u03b8)} (5\n)\nThen there is an unique \u03b8 * which attains the supremum and F () is also a convex function. If the domain of F is S and the domain of G is \u0398, then (S, F ) is the Legendre dual of (\u0398, G). In particular, there exists a \u03b8 such that F (\u03bc) = \u03bc.\u03b8 \u2212 G(\u03b8). Differentiating and setting the derivative to 0, we see that g(\u03b8) = \u03bc and f (\u03bc) = \u03b8; then since G() is strictly convex, F () is too and so can be used to define a Bregman divergence. Consider two members of an exponential family with natural parameters, \u03b8 1 and \u03b8 2 , and expectations, \u03bc 1 and \u03bc 2 . Then Thus maximising the likelihood of a data set is equivalent to minimising the associated Bregman divergence between the mean of the distribution and the data.\nd\nIn practical terms, we might fit a particular member of the exponential family to a data set which means we have determined the cumulant function, G(.). We then identify the dual function, F (.), based on which we can find the Bregman divergence d F (.) knowing that minimising the Bregman divergence between the mean of the distribution and its natural statistics maximises the log likelihood of the distribution under this probability density function.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Bregmanising Stone's Method", "text": "Recall that Stone's criterion is\nJ Stone = log yt\u2208D (y t \u2212 y) 2 yt\u2208Dt (y t \u2212\u1ef9) 2 = log V U (6)\nWe can Bregmanise this but first we must note that the Bregman divergence is not symmetric and so we may get different results depending on where the averages appear in the criteria. Thus we have J RBStone = log yt\u2208D (d F (y t , y)) yt\u2208Dt (d F (y t ,\u1ef9))\n= log V R U R (7) where we have used J RBStone as shorthand for the right Bregman divergence using Stone's method, and correspondingly J LBStone = log yt\u2208D (d F (y, y t ))\nyt\u2208Dt (d F (\u1ef9, y t )) = log V L U L (8\n)\nwhen the averages are in the left position in the divergences. It would be possible to consider combinations of V R ,U L and V L ,U R but we find it difficult to find a rationale for these. Thus, for example if we are using the Itakura Saito divergence and so F (.) = \u2212 log(.), then the criterion with the average in the right position would be J RIS (Right Itakura Saito). This gives which gives a parameter update (for the presentation of a single input) of\nV IS R =\n\u0394w i = \u03b1 \u2212 x i y + x i y + (yx i \u2212 yx i y 2 1 V IS R \u2212 \u2212 x i y +x \u0129 y + (\u1ef9x i \u2212 yx \u0129 y 2 1 U IS R (9\n)\nwhere \u03b1 is a learning rate and we have removed the subscript t for clarity.\nIn practice, we robustify the algorithm in that we add a small constant to the denominator of every fraction to ensure we are not dividing by 0.\nSimilarly if we consider the GI divergence, then we have F (x) = x log x. The right divergence gives us ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have taken Stone's criterion for independent component analysis and used Bregman divergences to create somewhat different algorithms. The different divergences are known to be optimal in the context of Bregmanising K-means clustering. We are now engaged in empirically testing whether these divergences are optimal for independent component analysis using Stone's criterion. Also Stone used the same criterion for the discovery of disparity in visual images, interestingly where the disparity changes gradually over the image. We also are investigating whether the Bregmanised versions of Stone's criterion can be used in a similar fashion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Daily physical activity recognition is a very important task that is currently being applied in several fields such as e-health, robotics, sports, videogames industry, among others [4,7,9,10,11]. The primary difficulty consists in designing a system whose reliability is independent of who is carrying out the exercise and the particular activity execution style. Besides, the complexity is increased by distortion elements related to system monitoring and processing, along with the random character of the execution.\nIn the literature, most studies performed are based on supervised laboratory data. Nevertheless, the apparently good recognition results on supervised data that some works achieve cannot be extrapolated to unsupervised (semi-naturalistic) data [2,12]. In this paper we propose an automatic methodology to extract a set of the most important features to be used in activity recognition. One of the most important characteristic of the method proposed is that we do not provide a rank order for every individual feature but for every set of features, allowing for the synergical utility of several features when considered together at the same time.\nThe rest of the paper is organized as follows: In Section 2 we make a brief summary of the activity recognition process. Next, we present the rank-based feature-set selection methodology developed, describing the fundamentals of this method and the algorithm's main steps. Finally we evaluate the performance of the method for a specific example and we compare the accuracy results with related previous works.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Activity Recognition", "text": "Our experimental setup starts from a signal set corresponding to acceleration values measured by a group of sensors located in strategic different parts of the body (hip, wrist, arm, ankle, thigh), for several daily activities (see Figure 1). This philosophy of work can easily be generalized to other studies related to activity recognition from a set of features.\nThe initial information provided by the sensors has some artefacts and noise associated to the acquisition data process. A low pass and high pass filtering is normally used to remove these irregularities. It's relevant the signal pattern form similarity between \"walking\" and \"running\", and \"sitting and relaxing\" and \"standing still\" respectively. Subsequently we generate a parameter set made up of 861 features corresponding to a combination of statistical functions such as mean, kurtosis, mode, variance, etc., and magnitudes obtained from a domain transformation of the original data such as energy spectral density, spectral coherence or wavelet decomposition, among others. These features are evaluated over the complete signal, although other alternatives based on windowing and sub-segmentation signal feature extraction could also been tested.\nIn this stage, we now must rely on a feature selection process that has the responsibility of deciding which features or magnitudes are the most important ones to decide the kind of activity the person is carrying out. In the next section we describe the method we have designed to accomplish this task.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Proposed Ranking Technique Based on Discrimination and Robustness", "text": "Obtaining a specific group of variables from a big initial set is not a trivial task, because the number of possible feature combinations is huge. In our experimental setup the sample space is represented by n = 861 features, so brute force techniques like 'branch and bound' (O(2 n ) convergence 2 861 \u2248 1.5 x 10 259 possible permutations) or wrapper methods are impractical. In this section, we present an alternative method based on the concepts of discrimination and robustness for a complete set of features.\nLet us define the sample range of a class as the set of values included between the maximum and the minimum value (both inclusive) that a feature or variable takes for this class. Every circle showed in the example of figure 2 represents a sample corresponding to the feature value calculated over the data from a concrete subject and class. We represent for every class the sample range using a right brace sign with the same class tonality.\nGiven a group of samples (associated to every class) we rank its discriminant capability with respect to that class through the overlapping probability between this class and the others. This is calculated computing the number of samples from the analyzed class which are inside of the sample range defined by the others. For N classes and M samples for each class (let us suppose that this number is independent of the class), we define the overlapping probability of a set of samples as follows:\n(1) with m(k,n) being the number of samples from the class k inside the sample range of class n.\nIn order to make this more understandable, for the example given in figure 2 (N = 4, M = 8), the overlapping probability for the class \"running\" is p = \u00bc(3/8+0/8+0/8) = 0.094, since there are 3 samples from the class \"running\" in the data range defined for class \"walking\", and 0 in the rest of classes. Consequently, this feature permits to discriminate a priori the activity running from the activity standing still or sitting and relaxing, but it could be mistaken with an approximately 9% probability with walking.\nWe now carry out a thresholding process which allows us to define the feature analyzed as discriminative or not. This overlapping threshold takes values from 0 (the most restrictive, for cases with no overlapping between classes) to 1 (the most relaxed, when every sample from a class is inside the others). In general for a specific feature, if the analyzed class exceeds the threshold, the feature will be considered as no discriminant for this class.\nApart from the discriminant capacity of a feature or a set of features, a second characteristic is now defined which takes into account the usability of this set of features in different information contexts or sources. For instance, a specific measure taken from, let's say, the ankle accelerometer can be very discriminative to distinguish between the activities walking and standing still, but this very same measure may not be that reliable when taken from the thigh accelerometer. There may be some measures with the same discriminant capability between those activities which are not so dependent of the exact location of the sensor or, at least, which are still reliable when taken from a bigger number of sensors. We will denote this measure as the robustness criterion of a set of features. In short, discriminant capacity says how useful a motion feature is in general, and robustness is how this depends on where the sensor is.\nCombining both criteria we obtain a quality ranking procedure capable of grouping features in different stages. For the sake of simplicity, let us suppose a recognition system with 4 classes and 5 sources; features will be classified in groups defining a ranking (see table 1). For instance, features that discriminate 4 classes in every source will be added to group #1 (the best). Group #14 will be completed with features that classify 2 classes (the same) in 3 sources at least. This example is extensible to any classes and sources.", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": ["tab_2"]}, {"heading": "Results", "text": "To evaluate the effectiveness of the ranking method developed, we use a signal database 1 corresponding to the data monitored by 5 biaxial accelerometers (hip, wrist, arm, ankle and thigh) for 4 activities (introduced in figure 1) in laboratory (supervised) and semi-naturalistic (unsupervised) environments.\nMost remarkable features (set #1 and #2 primarily) for supervised and unsupervised data are geometric mean for amplitude signal, autocorrelation and some wavelets coefficients obtained through a 5-level Daubechies decomposition. This together with a classification strategy based on C4.5 decision tree permits to achieve an accuracy rate close to 99% (98.92% \u00b1 1.08%) for laboratory data and 95% (95.05% \u00b1 1.20%) for semi-naturalistic data. We used a cross validation method for training and Fig. 2. Feature values extracted for the ankle accelerometer. Every circle represents the value (sample) of the feature \"decomposition wavelet coefficients a5 geometric mean\" for a specific subject (8 in all), identifying with different tonalities the belonging class. testing. These results are quite good due to the relative parallelism between probability model used in feature selection defined and the entropy-based model used in decision tree.\nAlthough a strict comparison with other studies cannot be made since the data and the number of classes may differ, in [9] a 83-90% classification accuracy was reached for laboratory conditions, 92.85%-95.91% in [8] (also for lab conditions), 89% in [2] for supervised and unsupervised, or 93% and 89% on recent works ( [3] and [5] respectively) for semi-naturalistic data.", "publication_ref": [], "figure_ref": ["fig_2", "fig_15"], "table_ref": []}, {"heading": "Conclusions", "text": "In this work we have very briefly shown a direct application of ranking selection methods used on daily physical activity automatic recognition. An efficient classification method requires a productive and limited feature set, being necessary a selection process since the initial set is quite huge. We have defined a feature selector based on statistical discrimination and robustness criteria, focused on low computational time and resources, defining a real alternative to other selection processes.\nFor future work, we aim to make a time-based comparison to traditional features selectors [6,13,14].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "One form of computer workload characterization involves clustering of fixed size slices of a trace of an application code obtained from a high level instruction set simulator, and identifying prototype slices [1,2]. Each trace-slice is represented by a set of features such as the number of arithmetic instructions in the slice, the number of memory references, and the number of register's bits altered. Prototype slices are analyzed through a low level software simulator and the results are used to estimate the performance of the entire code [1,2]. Numerous architecture and microarchitecture features can be extracted from the trace. Nevertheless, it is desired to identify an optimal feature sub-set in order to enable cost effective characterization.\nThe current work stems from a research project where several methods for feature selection including Genetic Algorithms (GA) have been considered, and GA has been found to be the most promising method [3][4][5][6].\nThis research explores time/space tradeoffs that are inherent to GAs and can reduce the time complexity of the GA procedure and/or improve the quality of its feature selection results. Time/space trade-offs are explored using a record keeping mechanism in the form of a cache that maintains a partial list of chromosomes encountered throughout the GA execution Experiments performed show that record keeping significantly reduces the execution time achieving virtually the same solution quality. Alternatively, if the execution time is fixed the record keeping method might result in a better solution. Results of this research are not limited to workload characterization or feature selection. In fact, further ongoing research effort targets using time/space trade-offs to improve GA performance for feature selection problems in other domains, to explore the utility of the approach in other search spaces such as the Traveling Salesman Problem (TSP) search space, and to improve other heuristic search procedures.\nThe rest of the paper is organized as follows: Section 2 discusses the topic of feature selection. Section 3 describes record keeping mechanisms. Section 4 presents experiment results as well as result evaluation. Finally, section 5 provides conclusions and directions for future research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Selection", "text": "Automatic, non-supervised, pattern recognition and classification is a fundamental Artificial intelligence (AI) problem. A typical approach to this problem is to extract a selected set of features from each pattern under consideration and compare the features of unknown patterns with the features of prototype patterns. Often, many features can be extracted, and different features have a different contribution to classification accuracy. Moreover, the computational complexity of the classification process is affected by the number of features used. Hence, it may be desirable to select an optimal subset of features that would enable cost effective classification; where the optimality criteria relates to the discriminatory properties of the selected subset. Assume that features can be extracted from the patterns. Hence, there are 2 combinations of feature subsets. For small values of it might be possible to perform exhaustive search for the optimal subset of features by enumerating all the possible 2 combinations and checking each combination against the optimality criteria. Nevertheless, the enumeration is exponential with respect to , and for large values of exhaustive search becomes prohibitively computationally expensive. In fact, the feature selection problem (FSP) is known to be an NP-hard problem [7].\nIn many cases, computational complexity considerations and implementation constraints dictate the desired number of features in the selected subset (say ). Under these constraints the FSP boils down to finding the optimal subset of features from a superset of features. This statement of the FSP entails selecting one of the combinations of features according to an optimality criterion. Despite the cardinality reduction from 2 to , for a fixed , the problem is still exponential in and is NP complete [7]. The FSP can be stated as a combinatorial optimization problem relevant to the areas of pattern recognition, statistics, and machine learning [8]. Feature selection has been applied to diverse practical problems such as medical diagnosis, spam detection, data mining, and data compression. It is used to detect major explanatory variables [9], reduce measurement costs [10], increase the speed of the classification tasks, facilitate accurate inferences by reducing the influence of noise [11,12], and reduce storage needs as well as bandwidth consumption [13].\nHeuristic search techniques for solving the FSP include suboptimal branch and bound, sequential forward/backward selection, and plus-l take-away-r selection [14,15]. In addition, there is a considerable number of studies related to meta-heuristic approaches such as genetic algorithms [4], memetic algorithms [16], tabu search [10], particle swarm [17], and ant-colony systems [12].\nSiedlecki and Sklansky use GAs for solving a version of the FSP that searches for the minimal feature subset for which the classification quality is above a pre-defined threshold [5]. The authors compare sequential search, branch and bound (BB), and GA. The experiments are done with simulated data and with real data having 150-300 features. Their results show that GA outperforms all the researched methods. It is more efficient than BB, and visits the search space in a more complete way than sequential search.\nAfter thorough evaluation of the literature, in specific the research reported in [4][5][6], and following a set of experiments with several heuristics, GA was selected as the main platform for the FSP solution in the workload characterization project. In addition, GA was selected to explore time/space trade-offs using cache memory.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Record Keeping Mechanisms", "text": "Many heuristic algorithms present redundancy; as it might be possible that the same candidate solution is evaluated more than one time. Formally, redundancy in a heuristic search routine is defined to be the ratio of the total number of states explored to the number of distinct states explored. That is:\n.\nThe redundancy of a problem can be used to infer an upper bound on the speedup that can be obtained through record keeping.\nIn the context of GA redundancy can be significant. In many variants of the algorithm, a large portion of the current population of chromosomes has already been evaluated in previous iterations of the algorithm. This brings an interesting question, which is not sufficiently addressed in the literature. The question relates to the type of records that could be kept throughout the GA session and the best policy to save these records in order to minimize redundancy and improve execution efficiency.\nThree memory models are used to analyze redundancy and the potential of record keeping: the minimum memory model, the infinite memory model, and the cache model. The minimum memory model (also referred to as the no cache model) assumes that there is no cache or dedicated memory that can be used to record states previously encountered. Hence, no record keeping, beyond the record keeping specifically implied by the algorithm (e.g., maintaining the elite list in GA), is performed and the algorithm is implemented with the minimum amount of memory possible. Caching is described later in the section. The infinite memory model is a theoretical model that assumes that there is no bound on the memory used for record keeping, and assumes that there is enough memory to store all the distinct states (chromosomes) encountered during the search. The model is used to analyze the time/space tradeoffs. For the FSP and many other heuristic search procedures, recording all the states encountered would transform the problem from NP-complete to NP-space. This is, of course, not desirable. Practically, in this study, the experiments have a fixed number of generations and use dedicated memory. The memory is large enough to store every distinct chromosome generated by the minimal memory model. Hence, it can be considered as a model of \"infinite\" dedicated memory or an infinite table for record keeping of all the distinct chromosomes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Caching in Computer Architecture", "text": "Computer memory systems exploit two empirical principles: \"locality of reference\" as well as \"small is fast,\" and implement a memory hierarchy. Generally, under this scheme, the locality of the current central processing unit (CPU) reference word is stored in a small and fast cache [18]. Related to the concepts of locality of reference are the terms of hit and miss and hit/miss rate. A hit occurs when a CPU referenceword is located in the cache. A miss occurs when the word is not in the cache and has to be brought from memory. A hit may incur a minor overhead whereas a miss entails a significant penalty that includes evicting a block from the cache and replacing this block with the block that contains the current locality of reference. Three basic replacement policies and several dynamic / adaptive combinations of the three are generally considered [18]. The first policy is 'random replacement'. It randomly chooses the block to be evicted. Next, a recency-based method, evicts the least recently used block. This method mainly exploits temporal locality. Finally, a frequency-based method evicts the least frequently used block, thereby exploiting spatial locality. This study implements and tests these three methods.\nThere are three main cache organization methods that relate to the way addresses in main memory are mapped to cache address: direct mapping, set associative mapping, and fully associative mapping [18]. Direct mapping does not enable efficient replacement policies and associative memory is quite expensive. Hence, in practice, set associative mapping which enables implementing replacement policies with a reasonable associatively of 4 to 16 is used. For the experiments performed in this research, set associative mapping is used.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Software Caching", "text": "Several researchers including Hertel and Pitassi [19] as well as Allen and Darwiche [20] have studied time/space trade-offs in the context of heuristic search. They found that time requirements could be significantly reduced through record keeping. They referred to their method as caching. Nevertheless, their cache is static; they do not consider cache replacement policies, or cache organization issues. Moreover, their findings do not pertain to GA.\nAggarwal investigates the technique of software caching for memory intensive applications that perform searches or sorted insertions [21]. He obtains reductions of up to 30% in computational time due to caching. However, Aggarwal's implementation is not extended to heuristic optimization problems such as the one considered in this paper.\nKarhi and Tamir demonstrate that iterative hill climbing (IHC); can get better performance by exploiting time/space tradeoffs emerging from saving intermediate results of the search [22]. The IHC algorithm with record keeping method that is analogous to the mechanism of cache is used to solve a traveling salesman problem (TSP). Santos et al, investigate cache diversity in GAs. The cache is used to store partial results of the chromosome evaluation function [23]. Nevertheless, they assume that the chromosome evaluation function can be decomposed into small units that represent the evaluation of parts of the chromosome and then recomposed based on mutations and crossovers of these parts. Moreover, despite referring to their record keeping as cache, the record keeping mechanism does not include provisions for replacement policies and can actually be considered as infinite dedicated memory. This limits their approach to small problems and to problems where the fitness function computation can be decomposed and recomposed.\nThe main thrust of our research is that redundancy that is equivalent to the locality of reference in computer systems exists in many heuristic search spaces and algorithms. In GA, the locality is due to the fact that the best solutions are retained and used for constructing future solutions. Due to the evidence for redundancy and locality of reference obtained from previous research [24] it has been decided to explore the time/space tradeoffs associated with record keeping in the form cache in the context of the GA. To the best of our knowledge, the research reported in this paper is the first comprehensive and scalable study of time/space-offs due to caching within the context of GA.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments and Results", "text": "This section presents the GA implementation in the FSP domain with and without caching. Next, a set of experiments along with a summary of results is presented followed by results analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Raw data provided by a semiconductor company interested in this research is used to evaluate the performance of the proposed GA with caching. The data contains a trace of four computer benchmark programs, including fast Fourier transform (FFT), the Dijkstra's shortest path algorithm (DJK), quick-sort (QS), and basic mathematics suit (BMS). Each trace is divided into fixed length sequences of instructions referred to as slices. The sizes of the slices examined are 1000, 2000, 5000, and 10000 instructions. Following a feature extraction stage applied to slices, each slice is represented by a set of architecture and micro-architecture features such as the number of integer operations per slice, the number of register transfers, and the number of memory accesses. These sets of features are going through the feature selection stage described below, where an optimal subset of the features is sought. Based on findings from the first phase of this research (which did not include caching), and due to external constraints, the 12 \"best\" features out of 24 features extracted per slice are selected for FFT and BMS and the best 13 out of 25 extracted features are selected for the DJK and QS. , Next, ISODATA 1 clustering [25], using the selected feature subset, identifies prototype slices, and classifies slices according to the closest prototype [1][2].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Non Cached GA in the FSP Domain", "text": "In the current implementation of the FSP, chromosomes represent feature subsets; encoded by bit strings with a length equal to the number of features. A 1-bit in a chromosome denotes that the respective feature is selected, and a 0-bit denotes a feature which is not selected. Mutations are implemented by applying a complement operation to randomly selected chromosome bits. Crossovers are implemented using the single-point crossover method [3]. Overall, the mutation operation is applied in 2% of the cases of chromosome generation and crossover in 98% of the cases. Since the number of desired features is given, (say ) and represented by the number of 1bits in the chromosome, a valid chromosome must include 1-bits. Nevertheless, since the mutations and crossovers might change the number of 1-bits in a chromosome, an operation of chromosome repair which randomly switches 0 or 1-bits, as required, follows the mutation and crossover operations.\nThe non cached version is running for up to 20,000 generations. A highest scoring selection rule (elitism) is implemented where the total population is 384 chromosomes and the elite list consists of the best 256 chromosomes observed in the population. Each consecutive generation is generating 128 additional chromosomes while maintaining the best 256 for the next generation.\nChromosome fitness is evaluated through ISODATA clustering. The features represented by the chromosome are used to cluster the training data. The resultant ratio of the \"between\" and \"within\" cluster dispersion matrices [25] is referred to as the quality (fitness) of the chromosome. Since ISODATA have several tunable parameters, a process of parameter tuning using principles of design of experiments has been performed prior to using the ISODATA procedure. Note that the ISODATA is a relatively complex algorithm which might require long execution time. Hence, it is desirable that the feature selection utility performs the minimal number of chromosome evaluations (or ISODATA calls).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Caching in the GA FSP Domain", "text": "The implementation of the GA with cache is done in the following way:\n1) The first generation is executed in similarity to the non cache version. However, at the end of the execution of the first generation, the best 256 chromosomes are maintained in an elite list while the other 128 chromosomes are cached. 2) Next, a member of the elite list might be randomly selected for mutation. Alternatively, two elite members might be randomly selected for crossover, and randomly one of the two siblings is selected for further evaluation.\n3) The new chromosome generated as the result of mutation or crossover is first compared (structurally) to the members of the elite list. If an identical chromosome resides in the elite list then the current chromosome is discarded and a new chromosome is generated through mutation or crossover of elite members. 4) Next, if the chromosome does not exist in the elite list then, it is compared (structurally) to the members of the cache. The comparison might result in a hit or a miss (see step 5). A hit means that this chromosome has already been evaluated; hence it is inferior to any member of the elite list, and can be discarded. Nevertheless, cache counters are updated to reflect the hit as it might affect the replacement (eviction) decision in subsequent misses. 5) A miss is interpreted as an indication of distinct chromosome. It is recorded and miss counters are updated. a) The chromosome fitness is evaluated, and compared to the elite list. If it is better than the fitness of the worst member of the elite list, then the new chromosome is inserted into the elite list and the worst chromosome of the elite list is cached. b) Otherwise the new chromosome is inferior to the elite list. It is inserted to the cache.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments Results", "text": "As mentioned in Section 4.1, the experiment instances include four benchmarks and the related code is segmented into different slice sizes. In addition, a large combination of cache configurations (number of sets and associativity) as well as replacement policies are used in each experiment. The GA with cache and no cache is run four times, on each experimental condition (defined by a particular benchmark, slice size, cache configuration, and replacement policy). The best clustering quality, the miss ratio, and the computational time are recorded at three different stopping points and averaged over the four replicates. The first stopping point results from running all GAs until a predetermined number of distinct chromosomes are reached. Then, GAs is running until a pre-determined number of generations is reached. The final stopping point occurs when a pre-determined number of ISODATA clustering operations is reached. In addition, this study calculates the inherent redundancy; as well as an estimation of the speedup of the GA due to caching in both software and hardware caching scenarios. This research focuses on the hardware cache scenario as it is most likely to be used in practical applications. For this scenario, the number of ISODATA iterations is utilized as a useful \"atomic time unit\" to compare the execution time of the algorithms under different parameters such as cache sizes and replacement policies. One ISODA-TA iteration takes the same amount of time in every scenario relevant to our experiments. Moreover, all of the other parts of the algorithm including cache access, search, and update are negligible compared to the average execution time of one ISODATA iteration. Hence, the number of ISODATA iterations can give a relatively accurate estimate of the realistic speedup obtained with different cache configurations.\nFigure 1, is a box plot that shows the spread in the number of ISODATAs after four replications for each benchmark studied. The boxes under the zero label represent the no cache case. Figure 2 displays the speedup for the different cache entry sizes. The speedup is computed as the ratio of the average number of ISODA-TAs for the no cache case (labeled as 0) to the average number of ISODATAs for a particular cache entry-size. For all the cases included in the figures, the slice-size is 1000 instructions, and the set associativity is 8. Statistical analysis of variance (ANOVA) over data collected from previous experiments shows that there is no significant difference between the three replacement policies studied; consequently, all experimental reported in Figures 1and 2 are run under the least frequently used (LFU)  2 shows, a speedup of 8x is obtained with FFT while a speedup of about 2x is obtained with the rest of the benchmarks. The reason for the lower numbers in the other benchmarks is that their problem domain is larger and therefore a larger cache is required in order to exploit redundancy.\nIn addition, we measured the upper bound on the speedup attainable through the infinite memory model and obtained 50x for BMS, 41 for FFT, 37 for DJK, and 34 for QS. This shows that much more speedup can be obtained with larger cache sizes.\nIn addition to the box plots analysis, ANOVA analysis has been performed and yields the following conclusion with significance level of 95%: 1) The cache contribution is significant, 2) Cache replacement policies have the same contribution, 3) Cache sizes are significant, but associativity is not (hence we used only one associativity value in the graph), 4) Slice sizes are significant with best results obtained for a slice of 1000 instructions. 5) For a fixed number of generation there is no significant difference in the quality of the cached versus non-cached versions.  Another finding of the research is that in general, if a time constraint is applied then the cache version is likely to provide a solution with about 2% better quality than the non-cached version. Nevertheless, eventfully if the constrain is removed the non cached version achieves similar results.", "publication_ref": [], "figure_ref": ["fig_2", "fig_15", "fig_15"], "table_ref": []}, {"heading": "Conclusions and Further Research", "text": "The experiments performed demonstrate that spatial and temporal locality of reference properties can be exploited in GA to reduce computational time without degrading solution quality. Overall, this research shows that adding a cache to the GA can result in significant speedup.\nSeveral directions for future research can be considered including 1) exploring other record keeping mechanisms and other heuristics for solving the FSP such as ant colony, particle swarm, and tabu search, 2) extending the record keeping approach to other search techniques such as simulated annealing and other domains e.g., scheduling, and 3) Further analysis of the tradeoffs between parameters related to the search and storage overhead.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Computer assisted pharmacological drug design is nowadays a challenging and very important problem in the pharmaceutical industry. The traditional approach for formulating new compounds requires the designer to test in the lab a very large number of molecular compounds, to select them in a blind way, and to look for the desired pharmacological property. Therefore, it is very useful to have tools to predict a priori the pharmacological activity of a given molecular compound in order to minimize laboratory experiments.\nAll methods developed for this purpose are based on the fact that the activity of a molecule derives from its chemical structure and therefore it is possible to find a relationship between this structure and the properties that the molecule exhibits [14]. Thus, the particular way the molecular structure is represented has special relevance.\nIn Chemical Graph Theory, molecular structures are represented as doubly labeled graphs which can be conveniently characterized by a number of specific graph descriptors. In this context, a great number of topological indices have been proposed, but only a small subset is widely used in common practical studies. In this work, three different sets or families of topological indices are considered together. A set of 62 topological-structural indices [13], the well-known Kier-Hall indices [9] and the so called charge or electro-topological indices [6].\nThese or similar representations have already been applied to different discrimination problems in drug design (analgesic, antidiabetic, antibacterial, etc.). In the particular case of antibacterial activity, very good classification results have been reported using multilayer perceptrons (MLP) [12] and Support Vector Machines [5].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Drug Activity Prediction Using Machine Learning", "text": "The quantitative structure-activity relationship (QSAR) paradigm is currently used in the computer aided design of new medical drugs with desired chemical properties. These methods are an alternative to the exact or precise description of the electronic properties of a molecule calculated by mechanical-quantum methods. The molecular topology describes the molecule as a set of indices which are in fact graph invariants. These topological indices are numerical descriptors that encode information about the number of atoms and their structural environment. This representation is derived from the hydrogen-suppressed molecular formula seen as a graph [1,14].\nThe molecular topology considers a molecule as a planar graph where atoms are represented by vertices and chemical bonds are represented by edges. The chosen set of molecular descriptors should adequately capture the phenomena underlying the properties of the compound. In this work, a set of 116 indices has been selected from the three families considered [4] that we will be referred to as topological (62),  and electro-topological (32).\nThese molecular representations have shown their ability for discriminating and predicting different kinds of pharmacological properties. Nevertheless, it is known that certain indices are more important than others for detecting particular cases. Obviously, the QSAR studies rely on the key fact that the activity of a molecule directly derives from its structure or, more precisely, from certain aspects of it. The better the chosen set of indices captures these particular aspects, the better the (blind) machine learning methods will characterize the activity of the molecule. As the molecular descriptors or indices have to be general in order to be applied in a wide range of drug design contexts, the ability of the particular learning methods used to capture non linear relations and high order dependencies among them becomes a key fact in the whole process. Moreover, the particular features that are most important for particular tasks and the order they need to be taken into account to obtain better results can lead to important information for characterizing future activity in the corresponding chemical compounds.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ranking Inputs and Feature Selection", "text": "The selection of relevant features, and the elimination of irrelevant ones, is a central problem in machine learning [10]. Before an induction algorithm can move beyond the training data to make predictions about novel test cases, it must be decided which attributes are to be taken into account for these predictions and which ones must be ignored. Intuitively, one would like the learner to use only those attributes that are relevant to the target concept.\nSelecting the optimal subset of features for a given learning or classification task is a well-known example of NP-hard problem [7]. For this reason, many different suboptimal algorithms that look for relevant attributes have been proposed. Among these, the family of sequential greedy subset selection algorithms is particularly appealing due to its very convenient trade-off between performance and computational demands.\nIn some cases, the particular optimal subset of features is not so important and a family of solutions of a convenient range of cardinalities is sought. More specifically and depending on the application domain, an absolute measure of relevance for each feature or a convenient ordering of them can be more interesting than particular families of subsets. This is indeed the case in Drug Activity Characterization when using descriptors coming from different families that are very different from the point of view of its origin and (chemical) definition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sensitivity Based Input Pruning", "text": "When the goal consists in obtaining a good feature ordering or ranking one can use classical forward or backward sequential selection (SFS and SBS algorithms) [7] that proceed by sequentially adding or discarding features in a greedy way. These algorithms are very efficient but they still require a quadratic number of model training and evaluation when used as wrappers [10] in order to obtain the corresponding feature ranking.\nThe so-called Sensitivity Based Pruning (SBP) [15,11] has been proposed for these kind of situations in the context of neural network models in which training a quadratic number of models can become prohibitive. Instead, the SPB performs a unique model training and a linear number of partial evaluations in order to compute a sensitivity measure for each feature. The sensitivity measure is computed as follows.\nLet f (x) be the function/classifier and let x = (x 1 , . . . , x d ) be its corresponding input. Let P f be a particular performance measure on f using a given training set. The sensitivity of the ith feature/input is then given by S i = P f \u2212 P fi where f i (x) = f (x 1 , . . . ,x i , . . . , x d ) andx i is the average value of the ith input on the training set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Iterative Sensitivity Based Pruning (iSBP) Algorithm", "text": "In this work, we propose to use the SBP method in combination with a modified version of backward elimination. This method uses the ranking of features given by the SBP method and removes the least relevant variables one (or more) at a time. The resulting subset is then used to retrain the classifier and calculate a new ranking over the remaining features. This process is repeated until all features are removed. The details of the proposed procedure are shown as Algorithm 1.\nThe iSBP algorithm can be seen as a simplified SBS algorithm in which the SBS ranking is used instead of the tentative removal of all remaining features. In this way, the training of (about) n 2 2 models in sequential algorithms is substituted by n m models in the proposed procedure. The parameter m is used to improve even further the computational requirements of the algorithm. In this work, only results with m = 1 are considered and reported. Informal experimentation increasing m lead to very similar performance results with progressively less computational burden. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Results", "text": "The particular discrimination problem considered in the present study was to determine whether a molecule has analgesic properties or not. To this end, a particularly difficult database has been compiled. A dataset of 973 samples with potential pharmacological activity has been considered. Out of these, 111 molecules are known to have analgesic properties while the other 862 compounds do not have these properties at all. These unbalanced proportions are not necessarily related to the a priori probability of activity in real pharmacological design trials. It is worth noting that one of the main drawbacks to obtain good and stable results with some of the methods considered in this work, comes from the small representativeness of the active class in the data set available. We will refer here to active and inactive compounds, respectively.\nSupport vector machines (SVM) have been used in this work as the learning algorithm given that this was the best option when studying a slightly different discrimination problem involving drugs with or without antibacterial activity [5]. SVMs are very well-known, flexible and robust learning models that allow for very good classification and generalization [8,3]. In particular, the radial basis function (RBF) or proximity kernel parametrized by an influence parameter, \u03b3, has been considered for the experiments. This parameter along with the soft margin or regularization parameter C have been chosen by looking at values similar to the ones in previous related works.\nAll 116 molecular descriptors were used to obtain feature vectors in which values were linearly normalized to the interval [\u22121, 1] in an independent way. Each feature vector was then labeled either with 1 indicating that the molecule is active, i.e. it has analgesic properties, or \u22121 if the molecule is inactive.\nThe measure of performance used in this context both to derive the feature relevance and to assess the final classification results is the area under the ROC curve (AUC). The ROC curve conveniently measures the dependence of the active accuracy rate with regard to the inactive error rate (true positives against false positives) over the whole output range of the learned model. The AUC is a commonly used measure of the global accuracy of a classifier [2] that has been previously used for Automatic Drug Characterization because the particular cost-benefit tradeoff [5]. It is possible to define the relevance of a feature as the difference in the value of the AUC when the feature is removed. In this work, the AUC is estimated by numerical integration of the averaged ROC curve built from a n-fold cross validation.\nIn order to obtain results as significant as possible, n-fold stratified crossvalidation (CV) has been used to compute all global accuracies and performance measures shown. In these cross-validation procedures, n = 5 has been used for the internal process of feature relevance estimation while n = 10 has been fixed in order to compute the final accuracy of the final models using the learned ordered list of features.\nIn order to make use of the available data as much as possible the experiments on feature ranking and assessment have been done using all the data. In other words, the whole dataset has been used to produce an ideally ordered list of features using all competing approaches considered. Once these lists have been obtained, each corresponding feature subset has been evaluated through a 10-fold cross-validation round using all available data again. In this way, the feature lists are the same across different folds and the results are slightly more stable and optimistically biased (but in an absolutely equal way for all competing feature ranking algorithms).\nFigure 1 shows the accuracy results for the feature subsets obtained from the ordered lists computed by the four methods considered.\nApart from the SBS that gives the bests results, one can observe that the proposed iSBS method gives better results that both SBP and SFS but for a relatively narrow margin. More important than that is the fact that iSBP is able to give very stable performance results en the cardinality range 8-40 which is of specific interest for this application domain given the observed intrinsic complexity of the classification problem. The computational cost associated to each method in terms of the corresponding feature subset size is shown in Figure 2 left. It can be seen that the best solutions given by SBS in the target range imply a computational burden that is almost five times higher. At the same time, the CPU time needed by iSBP is only twice the time of the basic SBS. The same CPU timing data is shown in Figure 2  SBS. Moreover, this plot also puts forward the very good ability of SFS to very quickly obtain good solutions. Unfortunately, the corresponding subset consists of about 80 features and no convenient solutions are found by SFS in the target range for this problem.\nThe same experimentation has been repeated in a more independent way by computing different feature lists for each different cross validation round in the assessment phase. In this way and as can be observed in Figure 3, averaged results are worse for all methods in absolute terms but relative considerations about the merits of different methods remain the same with a few exceptions. On one hand and as it could be expected, all results exhibit more variability. On the other hand, the SBS is no longer the best option.\nSignificantly better solutions both in terms of subset cardinality and accuracy are obtained with the proposed iSBP method even below 10 features. Competing results are obtained both with SBP and SBS in the range from 20 to 30 features while the SFS clearly gives the worst results for this particular and more demanding experiment.", "publication_ref": [], "figure_ref": ["fig_2", "fig_15", "fig_15", "fig_28"], "table_ref": []}, {"heading": "Concluding Remarks and Further Work", "text": "An improved backward selection method aimed at obtaining convenient ordered lists of relevant features has been proposed and partially evaluated in a particular and challenging application domain. The proposed method has been compared to the classical forward and backward feature selection and also to the sensitivity based input pruning method. All results are in terms of AUC measures computed from the outputs of conveniently learned SVM models using the corresponding subsets of relevant features of different cardinalities. The empirical results obtained show that only the three backward methods are able to obtain good results in the cardinality range which is interesting for this particular application domain, i.e. about one third of the original dimensionality. Moreover, a general conclusion is that the dramatic increase in computational burden needed by the SBS does not compensate for the improvement in performance. In fact, when measured more precisely, the results by SBS are even worse than the ones from the very simple SBP method. On the other, hand, the proposed method which can be seen as a convenient trade off between the other two backward options, keeps obtaining good solutions both in terms of accuracy and feature subset cardinality.\nFurther work is already being carried out in the direction of deeply studying to which extent the proposed algorithm is better than the other ones in a wider range of situations. The other line of research is related to the in depth study of the different features selected or ranked by each method. This could effectively lead to new proposals of molecular descriptors adapted to particular discriminating tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Data mining [2], as a multidisciplinary joint effort from databases, machine learning, and statistics, is championing in turning mountains of data into nuggets. In order to use data mining tools effectively, data preprocessing is essential. Feature selection is one of the most important and frequently used techniques in data preprocessing for data mining [3,4]. In contrast to other dimensionality reduction techniques, they preserve the original semantics of the variables, hence, offering the advantage of interpretability by a domain expert [5].\nFeature selection can be defined as the selection of a subset of M features from a set of N features, M < N , such that the value of a criterion function is optimized over all subsets of size M [6]. However, the advantages of feature selection techniques come at a certain price, as the search for a subset of relevant features introduces an additional layer of complexity in the modeling task. In this paper we propose a methodology to reduce this cost without severely damaging the classification accuracy.\nA typical feature selection process consists of four basic steps (shown in Figure 1), namely, subset generation, subset evaluation, stopping criterion, and result validation [7].", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Fig. 1. Four basic steps of a typical feature selection process", "text": "The generation procedure is a search procedure [8] that produces candidate feature subsets for evaluation based on a certain search strategy. Feature selection methods applied in this paper generate candidates randomly.\nAn evaluation function measures the goodness of the subset produced and this value is compared with the previous best. If it is found to be better, then it replaces the previous best subset.\nThe process of subset generation and evaluation is repeated until a given stopping criterion is satisfied (e.g.: a predefined number of features are selected or a number of iterations reached). The feature selection process ends by outputting a selected subset of features to a validation procedure [7].\nIn the context of classification, feature selection techniques can be organized into three categories, depending on how they combine the feature selection search with the construction of the classification model [5]: filter methods [9,10,11], wrapper methods [12,13,14], and hybrid / embedded methods [15,16,17]: \u2212 Filter techniques rely on the intrinsic properties of the data to evaluate and select feature subsets without involving any mining algorithm. They easily scale to very high-dimensional datasets, they are computationally simple and fast, and independent of the classification algorithm. \u2212 Wrapper methods embed the model hypothesis search within the feature subset search. The evaluation of a specific subset of features is obtained by training and testing a specific classification model. Their advantages include the interaction between feature subset search and model selection, and the ability to take into account feature dependencies. A common drawback is that they have a high risk of over-fitting and are very computationally intensive. \u2212 Hybrid / embedded techniques attempt to take advantage of the two models by building the search for an optimal subset of features into the classifier construction. Just like wrapper, they are specific to a given learning algorithm.\nAs previously stated, when the dimensionality of a domain expands the number of features N increases. In these cases, finding an optimal feature subset is usually intractable [14] and many problems related to feature selection have been shown to be NP-hard [18]. In order to face this problem in this paper we propose a methodology for applying feature selection algorithms based on repeating several rounds of a fast feature selection process. Each round on its own would not be able to achieve a good performance. However, the combination of several rounds using a voting scheme is able to match the performance of a feature selection algorithm applied to the whole dataset with a large eduction in the time of the algorithm. Each round can be considered a weak feature selector, as it has a partial view of the dataset, their combination using a voting scheme is similar to the combination of different learners in an ensemble using a voting scheme. Due to this voting scheme we call this method democratization of feature selection algorithms, and the result democratic feature selection.\nThe main advantage of our method is that as the feature selection algorithm is applied only to small subsets, the time is reduced very significantly. In fact, as the size of the subset is chosen by the researcher, we can apply the method to any problem regardless of its size. As for the case of classifier ensembles, where the base learner is a parameter of the algorithm, in our method the feature selection method is a parameter, and any algorithm can be used.\nThis paper is organized as follows: Section 2 presents the proposed model for feature selection based on our approach; Section 3 reviews some related work; Section 4 describes the experimental setup; Section 5 shows the results of the experiments; and Section 6 states the conclusions of our work and future research lines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Democratic Feature Selection Method", "text": "The process consists of dividing the original dataset into several disjoint subsets of approximately the same size that cover all the dataset. Then, the feature selection algorithm is applied to each subset separately. The features that are selected by the algorithm to be removed receive a vote. Then, a new partition is performed and another round of votes is carried out. After the predefined number of rounds is made, the features which have received a number of votes above a certain threshold are removed. An outline of the method is shown in Algorithm 1. Each round can be considered to be similar to a classifier in an ensemble, and the combination process by voting is similar to the combination of base learners in bagging or boosting [19].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1. Democratic feature selection (DemoFS) algorithm", "text": "Data: A training set T = {(x 1 , y 1 ), \u2026, (x n , y n )}, subset size s, and number of rounds r. Result: The set of selected features for i = 1 to r do Divide data into n s subsets of size s, by instances or by features for j = 1 to n s do Apply feature selection algorithm to t j Store votes of removed features from t j end end Obtain threshold of votes, v, to remove an feature S = T Remove from S all features with a number of votes above or equal to v return S\nThe most important advantage of our method is the large reduction in execution time. The reported experiments will show a large difference when using standard widely used feature selection algorithms. Additionally, the method is easy to implement in a parallel environment, as the execution of the feature selection algorithm over each subset is performed independently. Furthermore, as the size of the subsets is a parameter of the algorithm, we can choose the complexity of the execution in each one of the processors.\nThe partition method used in our approach is a strictly random partition. A proposed further work is to develop a data -dependent partition method, in order to improve the method efficiency.\nHowever, as stated so far, the method still has an important issue to be addressed before we can obtain a useful algorithm: the determination of the number of votes, which is problem-dependent. Depending on the problem, a certain threshold may be too low or too high. Thus a method must be developed for the automatic determination of the number of votes needed to remove a feature from the training set. The automatic determination of this threshold has the additional advantage of relieving the researcher of the duty of setting a difficult parameter of the algorithm. This issue is discussed in the following section. We must also emphasize that our method is applicable to any feature selection algorithm, as the feature selection algorithm is a parameter of the method. Moreover our approach has a very competitive complexity, as it is linear in the number of instances or in the number of features (depending on which factor relies the complexity of the feature selection base algorithm).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Determining the Threshold of Votes", "text": "An important issue in our method is determining the number of votes needed to remove a feature from the training set. As it highly depends on the specific dataset, we need a way of selecting this value directly from the dataset in run time. Our choice is estimating the best value for the number of votes from the effect on the training set, specifically using a 10% of the dataset so as to speed up the process.\nThe election of the number of votes must take into account two different criteria: training error, \u0454 t , and memory requirements (percentage of features retained), m. Both values must be minimized as much as possible. Our method of choosing the number of votes needed to remove a feature is based on obtaining the threshold number of votes, v, that minimizes a fitness criterion, f(v):\nf(v) = \u03b1 \u2022 \u0454 t (v) + (1 -\u03b1) \u2022 m(v) .\n(1)\n\u03b1 is a value in the interval [0, 1] which measures the relative relevance of both values.\nIn general, the minimization of the error is more important than storage reduction, thus, we have used a value of \u03b1 = 0.75. We perform r rounds of the algorithm and store the number of votes received by each feature. Then, we must obtain the threshold number of votes, v, in the interval constituted by the minimum and maximum number of votes received by any feature. We calculate the criterion f(v) (see Formula 1) for all the possible threshold values from 1 to r, and assign to v the value which minimizes the criterion. After that, we perform the feature selection removing the features whose number of votes is above or equal to the obtained threshold v.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "There are not many previous works that have dealt with scaling up feature selection problems. It is worth mentioning the work of Robnik-Sikonja [20]. They proposed a method to speed up the Relief algorithm by means of k-d trees. Although the algorithm shows a very good performance, k-d trees add new memory requirements and are less efficient if the trees are not balanced.\nThe idea of applying a recursive divide-and-conquer process to feature selection is inspired in a recent paper of the authors [21] that showed a good performance in the application of a democratic approach to instance selection, while attaining a dramatic reduction in the execution time of the instance selection process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "In order to make a fair comparison between the standard algorithms and our proposal, we have selected a set of 27 problems from the UCI Machine Learning Repository. For estimating the storage reduction and generalization error we used a 10-fold crossvalidation (cv) method.\nThe evaluation of a certain feature selection algorithm is not a trivial task. We can distinguish two basic approaches: direct and indirect evaluation [22]. Direct evaluation evaluates a certain algorithm based exclusively on the data. The objective is to measure at which extent the selected features reflect the information present in the original data. Some proposed measures are entropy, moments, or histograms.\nIndirect methods evaluate the effect of the feature selection algorithm on the task at hand. So, if we are interested in classification we evaluate the performance of the used classifier with the reduced set of inputs obtained after feature selection.\nTherefore, when evaluating feature selection algorithms, the most usual way of evaluation is estimating the performance of the algorithms on a set of benchmark problems. In those problems several criteria can be considered, such as [23]: storage reduction, generalization accuracy, noise tolerance, and learning speed. Speed considerations are difficult to measure, as we are evaluating not only an algorithm but also a certain implementation. However, as the main aim of our work is scaling up feature selection algorithms, execution time is a basic issue. To allow a fair comparison, we have performed all the experiments in the same machine, a 33 blade chassis, each blade being a M600 Quad-Core Xeon E5420, with 2.5GHz and 2x6MB RAM memory. Our approach is based on applying feature selection algorithms to subsets of the training set, so to perform sound experiments the algorithm used for the whole training set and the algorithm used in our method are exactly the same. That is, when we applied our method using a feature selection algorithm and when we perform the feature selection algorithm for the whole training set, the implementation is the same in both cases.\nThe source code, in C and licensed under the GNU General Public License, used for all methods as well as the partitions of the datasets are freely available upon request to the authors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Selection Algorithms", "text": "In order to obtain an accurate view of the usefulness of our method, we must select some of the most widely used feature selection algorithms. We have chosen to test our model with two successful algorithms: ReliefF and a genetic algorithm.\nReliefF Algorithms. Relief family of algorithms is general, efficient and successful attribute estimators that do not assume the independence of the attributes and their quality estimates have a natural interpretation.\nA key idea of the original Relief algorithm [24] is to estimate the quality of attributes according to how well their values distinguish between instances that are near to each other, features whose weights exceed a user-determined threshold are selected in designing the classifier. The complexity of Relief for a data set with n instances is O(mkn), m being the user-defined iterations. The ReliefF algorithm [25] is not limited to two class problems, is more robust and can deal with incomplete and noisy data. Genetic Algorithms. We apply a simple genetic algorithm. In a GA approach, a given feature subset is represented as a binary string (a \"chromosome\") of length n, with a zero or one in position i denoting the absence or presence of feature i in the set. Note that n is the total number of available features. We apply standard genetic operators such as two-point crossover and mutation. At the beginning of each generation we perform an elitism step, and each individual is evaluated by means of the fitness function in Formula 2: fitness(ind) = suc_rate(i)\n\u2022 \u03b1 + (1 -\u03b1) \u2022 1 -(n_sel_feat / n_feat), (2\n)\nwhere suc_rate is the wrapper evaluation of the subspace using classifier, \u03b1 is a value in the interval [0, 1] which is set to 0.75, n_sel_feat is the number of selected features and n_feat is the number of all available features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "The same parameters were used for the standard version of every algorithm and its application within our methodology.\nIn the genetic algorithm the number of generations is set to 1000. We speed up the process applying an exhaustive search of all the possible combination of features if the exhaustive search has lesser iterations than the number of generations set in the genetic algorithm. At the beginning of each generation we apply a 10% of elitism and afterwards we get the rest of the population by means of applying iteratively a twopoint-crossover operator (in which we keep the two best individuals of each crossover step). The standard mutation percentage is fixed to 10%.\nIn the ReliefF the number of iterations, m, is set to the number of available instances in the dataset, and the threshold to remove features is set to 0.05 (or 0.01 in those datasets which may find the previous value too restrictive). The number of neighbors, k, controls the locality of the estimates, and is set to 10 neighbors as recommended by its author (Kononenko, 1994).\nOur method has three parameters: instance subset size or feature subset size, s, number of rounds, r, and \u03b1. We use an instance subset size of 100 individuals or 7 columns of feature subset size, as it is a value large enough to allow for a meaningful application of the feature selection algorithm on the subset, and small enough to allow a fast execution. For the number of rounds we have chosen a small value to allow for a fast execution, r = 10. As explained in Section 2.1, \u03b1 is set to 0.75, because for us it is more important a lesser error than little storage requirements.", "publication_ref": ["b861"], "figure_ref": [], "table_ref": []}, {"heading": "Summary of Results", "text": "In both experiments, using ReliefF and the genetic algorithm, we employ a k-NN classifier to evaluate the subsets of features resulting in each round and later set the threshold to delete features.\nTo summarize these two experiments we show six figures that compare the results obtained from our methodology (dividing by instances and by features) and the classic approach in terms of testing error, storage requirements and execution time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fig. 2.", "text": "Testing error for standard genetic algorithm and our approach dividing by instances and features Fig. 3. Storage requirements for standard genetic algorithm and our approach dividing by instances and features Results for the genetic algorithm are plotted in Figures 2, 3, 4. In terms of testing error our results dividing by instances are very similar to the standard ones. If we divide by features the results are a little bit worse, we believe that for this setting we need a bigger feature subset size (groups of 10 or 15 features) to be more reliable. Regarding the storage requirements we can observe very similar average results for all approaches. However, some datasets show worse results for our approach (e.g. mfeat-pix, opt-digits) and some of them are better (e.g. car or nursery in which the standard approach doesn't reduce much).\nIn terms of execution time, the advantage of our methodology is remarkable. For small problems there is a small overload due to the 10 rounds of votes performed, however as the problem grows in complexity, our approach shows a large reduction in the time needed to perform the feature selection process. Fig. 4. Execution time for standard genetic algorithm and our approach dividing by instances and features Fig. 5. Testing error for standard ReliefF algorithm and our approach dividing by instances Results for the ReliefF algorithm are plotted in Figures 5, 6, 7. Regarding the testing error the standard method and our democratized counterpart have very similar performance in average. In terms of storage requirements we can observe a similar behavior in all approaches. Nevertheless, on the one hand, some datasets point out worse results for our approach (e.g. mfeat-fac, mfeat-pix, ozone1hr) but, on the other hand, some datasets perform better (e.g. gene, mushroom, sick). The running time is significantly reduced with our methodology, almost every dataset is improved. Fig. 6. Storage requirements for standard ReliefF algorithm and our approach dividing by instances Fig. 7. Execution time for standard ReliefF algorithm and our approach dividing by instances", "publication_ref": [], "figure_ref": ["fig_28", "fig_15", "fig_29", "fig_116", "fig_53", "fig_117", "fig_53"], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "In this paper we have presented a new method for scaling up feature selection algorithms. The method is applicable to any feature selection method without any modification. The method consists of performing several rounds of applying feature selection on disjoint subsets of instances or features of the original dataset and combining them by means of a voting method. We use two well known feature selection algorithms: ReliefF and genetic algorithm. We have shown that our method is able to reduce considerably the running time, achieving a similar performance to the original algorithms. In terms of reduction of storage requirements and testing error, our approach is able to match and in some cases even improve the results obtained by means of the original feature selection algorithms over the full-size datasets. Additionally, our method is straightforwardly parallelizable without significant modifications.\nAs main research line we are working on the development of data -dependent methods to partition the original dataset. We expect that these partitions methods may have a positive influence on the performance of the method. Additionally we are working on a new hybrid approach with a filter step to select accurately the relevant features and a wrapper step to set efficiently a threshold of votes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This work was supported by the Science and Technology Amicable Research (STAR) by Korean NRT grant funded by the Korean government (MEST). (No. 2009-50252).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors thank the reviewers for reviewing the paper and Ghent University for funding the PhD project of Koen W. De Bock.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Julia Handl and Joshua Knowles for supplying us with the data sets investigated in the experimental section and with the results they obtained, making thus possible the reported comparisons with their extensive studies in unsupervised feature selection and clustering.\nAcknowledgments. This research has been partially funded by National Science Foundation (NSF) under grant #0637563 and Kentucky Science and Technology Corp. (KSTC) under grant #KSTC-144-401-07-018.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "References", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported by the Project TIN2008-03151 of the Spanish Ministry of Education and Science.\nWe wish to thank the developers of WEKA and LIBLINEAR. We also express our gratitude to the donors of the different datasets and the maintainers of the UCI Repository.\nAcknowledgments. This work has been partially funded by Junta de Castilla y Le\u00f3n through grant VA100A08.\nAcknowledgements. We wish to thank the developers of Weka. We also express our gratitude to the donors of the different datasets. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The first author thanks all the support of the Funda\u00e7\u00e3o Hermino Ometto and program of postdoctoral from the State University of S\u00e3o Paulo at Rio Claro.\nThe second author thanks Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do Estado de S\u00e3o Paulo (FAPESP) and Conselho Nacional de Pesquisas (CNPq) for a financial supports.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors are grateful to the Brazilian Funda\u00e7\u00e3o Pr\u00f3 Caf\u00e9 for providing the data used in this paper.\nAcknowledgments. This work is supported by the Web Intelligence project, funded by the ISLE cluster of the Rh\u00f4ne-Alpes region.\nAcknowledgments. This work is supported under the FP7 ICT Future Enabling Technologies programme of the European Commission under grant agreement No 231288 (SOCIONICAL). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This research is partially supported by the MCI I+D projects FAMENET InCare (TSI2006-13390-C02-02) and ARTEMISA (TIN2009-14378-C02-01) and Andalusian Excellence I+D project CUBICO (TIC2141).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "The authors are very grateful to Frank Weerman of the Netherlands Institute for the Study of Crime and Law Enforcement for his willingness to provide the empirical data from the 'School Project', and for a number of fruitful discussions.\nAcknowledgments. This work has been supported by the Spanish CICYT Project TIN2007-60587, Junta de Andalucia Project P07-TIC-02768 and the CENIT project AmIVital, of the \"Centro para el Desarrollo Tecnol\u00f3gico Industrial\" (CDTI-Spain). We want to express our gratitude to Prof. Stephen S. Intille, Technology Director of the House_n Consortium in the MIT Department of Architecture for the experimental data provided.\nThis work has been partially funded by Feder and Spanish MEC projects TIN2009-14205-C04-03 and Consolider Ingenio 2010 CSD2007-00018.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Abstract. This article aims to develop a minimally intrusive system of care and monitoring. Furthermore, the goal is to get a cheap, comfortable and, especially, efficient system which controls the physical activity carried out by the user. All this, is based on the data of accelerometry analysis which are obtained through a mobile phone.\nBesides this, we will develop a comprehensive system for consulting the activity obtained in order to provide families and care staff an interface through which to observe the condition of the individual subject to monitoring.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Modeling the Query Log Components with DE", "text": "In order to apply DE in our proposed model, we consider the following initial assumptions on web query encountered both in session related and temporal features; and add the textual assumptions:\n\u2022 Number of sessions in which reformulation (q, q\u2032) occurs;\n\u2022 divided by the number of sessions in which (q, x) occurs (for any x);\n\u2022 among all sessions containing (q, q\u2032):\n\u2022 average number of clicks since session begin, and since the query preceding (q, q\u2032); \u2022 average session size of other sessions containing (q, q\u2032); average position in session expressed as number of queries before q since the session begun.\nThe assumptions also depict that the initial error in query is reformulated to grouping construct and finally the query is either generalized or specialized and may lead to complete final modifications. Hence, the temporal features could also be considered like:\n\u2022 average time elapsed between q and q\u2032 in each session in which both occur;\n\u2022 Sum of 1/t i where t i is the elapsed time between queries i and the previous event in a session.\nConsidering these assumptions the pseudo code for complete proposed model is given:\nStep 1. Initialize each chromosome to contain K number of randomly selected cluster centers of query string at the 1st level and K (randomly chosen) activation thresholds in [0, 1] in session q.\nStep 2. As the 1 st reformulation of steps the grouping is done(in 2 nd reformulated session q\u2032) keeping the semantics in tact and to find out the active cluster centers off the string (e.g. \"Cheap Hotel\" and \"Hotel with Minimum Cost\") in each chromosome of the DE with the help of the Rule 1:\nStep 3. for t = 1 to t \u03b8 do /*t \u03b8 is a timeout threshold, a user query session S*/ for each query vector if the number of data points that belong to any cluster center m i,j >2.5. /* Heuristic Test Value of Threshold*/ Update the cluster centers of the chromosome using the concept of average; /*n/K data points*/ Apply Candidate Solution Measure", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Testing Dataset", "text": "In the experimental evaluation, the testing dataset (named 23-CONF) was used, where the papers published from 2006 to 2008 in the 23 data mining related conferences were extracted from the DBLP database.\nThe corresponding co-authorship network is constructed for the testing dataset, first. After performing the CONGA to discover the collaborative communities in the network, the topic terms are extracted and organized to a concept hierarchy. The related information of the testing dataset is as follows: the number of papers in the dataset is 10800, the number of nodes in the constructed network is 17216, and the number of edges is 34961. The threshold value \u03b1 for filtering potential topic terms is set to be 0.05, and the threshold value \u03b2 for detecting the hierarchical relationship between topic terms is set to be 0.13. Finally, the number of discovered collaborative communities is 2230, the number of nodes located at level 1 is 42, the average length of the discovered hierarchical paths is 4.7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consistency of a Topic-Based Collaborative Community", "text": "In order to evaluate whether the constructed topic-based collaborative community is semantically meaningful, the first part of evaluation is to measure the consistency between the terms appearing in the published papers of a topic-based collaborative community and the terms in other documents related to the specific topic.\nAn abstract is a concise version of a paper that includes the paper's research purpose, methodology, experimental results, etc. We thus believe that the abstract of a paper contains more keywords that describe the topic of a paper than the title. Therefore, for each topic term t i located at level 1 in the discovered concept hierarchy of topic terms, the abstracts of the papers in the corresponding topic-based collaborative community are extracted from CiteSeer X . There are almost 22% of the papers whose abstracts can be obtained from CiteSeer X . After performing the text processing steps on the abstracts, the unigrams are extracted from these abstracts to form the set of keywords: B Ti . On the other hand, the ACM (http://www.acm.org) digital library is queried to retrieve the abstracts of the most related 200 papers for the topic term t i . The unigrams extracted from this set of abstracts form another set of keywords: B ti ACM . Then we use the Jensen-Shannon Divergence (JSD) to measure the similarity between the probability distributions of terms in two sets of keywords.\nLet Pr(b|B ti ) denote the probability of keyword b in B ti and Pr(b|B ACM tj ) denote the probability of keyword b in B ACM tj The JSD measure between B ti and B tj ACM for each pair of topic terms t i and t j is shown below.\n(1) Abstract. This paper presents a vector space model approach, for representing documents and queries, using concepts instead of terms and WordNet as a light ontology. This way, information overlap is reduced with respect to the classic semantic expansion techniques. Experiments carried out on the MuchMore benchmark showed the effectiveness of the approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "This paper presents an ontology-based approach to the conceptual representation of documents. Such an approach is inspired by a recently proposed idea presented in [9], and uses an adapted version of that method to standardize the representation of documents and queries. The proposed approach is somehow similar to query expansion [12]. However, additional considerations have been taken into account and some improvements have been applied as explained below.\nQuery expansion is an approach to boost the performance of Information Retrieval (IR) systems. It consists of expanding a query with the addition of terms that are semantically correlated with the original terms of the query. Several works demonstrated the improved performance of IR systems using query expansion [19,3,5]. However, query expansion has to be used carefully, because, as demonstrated in [8], expansion might degrade the performance of some individual queries. This is due to the fact that an incorrect choice of terms and concepts for the expansion task might harm the retrieval process by drifting it away from the optimal correct answer.\nDocument expansion applied to IR has been recently proposed in [2]. In that work, a sub-tree approach has been implemented to represent concepts in documents and queries. However, when using a tree structure, there is redundancy of information because more general concepts may be represented implicitly by using only the leaf concepts they subsume.\nThis paper presents a new representation for documents and queries. The proposed approach exploits the structure of the well-known WordNet machinereadable dictionary (MRD) to reduce the redundancy of information generally contained in a concept-based document representation. The second improvement", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predicting the Development of Juvenile Delinquency by Simulation", "text": "Tibor Bosse, Charlotte Gerritsen, and Michel C.A. Klein Vrije Universiteit Amsterdam, Department of Artificial Intelligence de Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands {tbosse,cg,mcaklein}@few.vu.nl", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Abstract.", "text": "A large number of delinquent activities are performed by adolescents and only occur during this period in their lives. One of the main factors that influence this behaviour is social interaction, mainly with peers. This paper contributes a computational model that predicts delinquent behaviour during adolescence based on interaction with friends and classmates. Based on the model, which was validated based on empirical data, the level of delinquency of pupils is simulated over time. Furthermore, simulation experiments are performed to investigate for hypothetical scenarios what is the impact of the division of students over classes on the (individual and collective) level of delinquency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Building and Analyzing Corpus to Investigate Appropriateness of Argumentative Discourse Structure for Facilitating Consensus", "text": "Tatiana Zidrasco 1,2 , Shun Shiramatsu 1 , Jun Takasaki 1 , Tadachika Ozono 1 , and Toramatsu Shintani 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tag Set to Define Appropriateness", "text": "To annotate our corpus we used a tag set of 27 rhetorical relations, a part of which we define as argumentation-specific and present it in Table 1. We think these, so called, argumentation-specific relation tags namely reflect structure of argumentative discussion. We structure the tag set as hierarchy of levels and sublevels of rhetorical relations. The uppermost level contains such relations as: Requirement, Response, Action Request and Politeness.\n\u2022 Requirement rhetorical relation tags define question intention:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2212", "text": "Req_yes/no tag is used to label the relation between question and statement when confirmation or negation of previously stated information is required ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Elementary Unit", "text": "Since we decided to use rhetorical relations to annotate argumentative corpus, one of the issues we tried to deal with was to determine elementary discourse units (text spans) that hold rhetorical relations in our corpus. The simplest solution to this issue was to annotate relations that hold across the comments in the discussion. On the other hand, this solution being simple and convenient doesn't fully reflect the structure of the discussions we analyzed. It is obvious that a considerable number of comments contain more than one part that we call here \"speech acts\". Speech act is a term that refers to the act of successful communicating an intended understanding to the listener. Each \"speech act\" within one comment has a separate \"speech function\" like asking question, explaining, etc. One \"speech act\" can be related to one or more other \"speech acts\" or comments.\nEvidence. This is expected, since Evidence can be regarded as well measured argumentation support, necessary for consensus. Also, our results might support the idea of importance of clear understanding of the utterance's intention. When, for example, Evidence is required and Example or Background is provided as a response, the information might be insufficient and agreement impossible. Probably, that is why, Example -Agreement or Background -Agreement pairs are so rarely met. On the other hand, Explanation_argumentative -Agreement pair, although, met less frequently than Agreement -Agreement, serves to support the assumption that appropriate structure of argumentative discourse could be of several types. An interesting case is position of Suggestion relation frequently followed by both Agreement and Disagreement rhetorical relations. Still, according to the results Suggestion-Disagreement pair prevails. This might be explained by so called emotional conflict that often occurs during discussion. Suggestion involves requirement for changing hearer's existent belief, which often becomes an obstacle for consensus building process.\nIt is to be mentioned that although we try to calculate probability for bigrams and trigrams of rhetorical relations, at this stage, in the result analysis we base on the bigrams only. Even though we found few interesting trigram cases that could support our assumption in future, for example, Req_evidence/Evidence or Req_detail/Evidence pairs tend to more frequently precede Agreement relation, our initial data are not sufficient for more reliable verification of existence of longer agreement-oriented patterns in argumentative discourse. Issue that arises here is how many structures leading to agreement could exist. Also, existence of exhaustive argumentation-specific tag set of rhetorical relations is crucial for usability of argumentation support system. Thus another issue that we have to deal with is how to determine a small limited tag set of rhetorical relations suitable for annotation purpose and that will reflect in a proper way argumentative discourse structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Supporting public debate and facilitating consensus building process is actual problem that interests numerous researches. Computer argumentation support systems can assist participants during the multiparty discussions. Most successful approaches in argumentation support systems development base on providing visual \"box and arrows\" diagramming of argumentation and the analysis of argument constituents. [Reed and Rowe, 2004;Reed and Grasso, 2007;Verheij, 2001]. We think that encouraging user to consider the intention of his next utterance by providing a facilitation function as a list of possible candidates of rhetorical relations would be even more effective for consensus building process support. We assume that there exist appropriate structures (one or more) of argumentative discourse that lead to agreement and we think that these structures could be detected with the help of Rhetorical Structure Theory relations. First step in this approach is building and analyzing an argumentative corpus. In [Mochales and Ieven, 2009], argumentation corpus of legal documents is built and structure analysis is performed argumentation by application of well-known argumentation theories as the background framework for annotation process. We perform our analysis on the basis of web-discussions, assuming that they Abstract. We describe a novel dialogue strategy enabling robust interaction under noisy environments where automatic speech recognition (ASR) results are not necessarily reliable. We have developed a method that exploits utterance timing together with ASR results to interpret user intention, that is, to identify one item that a user wants to indicate from system enumeration. The timing of utterances containing referential expressions is approximated by Gamma distribution, which is integrated with ASR results by expressing both of them as probabilities. In this paper, we improve the identification accuracy by extending the method. First, we enable interpretation of utterances including ordinal numbers, which appear several times in our data collected from users. Then we use proper acoustic models and parameters, improving the identification accuracy by 4.0% in total. We also show that Latent Semantic Mapping (LSM) enables more expressions to be handled in our framework.\nIndex Terms: spoken dialogue systems, conversational interaction, barge-in, utterance timing.", "publication_ref": ["b731", "b732", "b733", "b737"], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Natural conversational dialogue systems should allow users to freely express their utterances anytime. Of particular importance is that the user should be able to interrupt the system's utterances. This ability to barge in is useful to convey the user's intention. The user should be able to occasionally interrupt the system by specifying an item when the system is listing items. For example, the system and the user can interact as follows:\nUser. Tell me which temple you suggest visiting. System. There are ten temples that I would suggest. \"Kinkaku-ji Temple\", \"Ginkaku-ji Temple\u2022 \u2022 \u2022 \" User. That one. System. OK, you mean \"Ginkaku-ji temple.\" It is the most famous one \u2022 \u2022 \u2022 In this case, the user interrupts the system while it reads out \"Ginkaku-ji temple.\" This system identifies the user's referent, that is, what the user indicates by Algorithm 1. Learning undirected tree structure Data: TS on a set of n variables N = {X 1 , ..., X n }; X 1 as root Result: UT={U,E} begin for i \u2208 {1, ..., n \u2212 1} do for j \u2208 {2, ..., n} do Compute the mutual information I ij using equation ( 1)\nwhile |U | < n do Use M to find X i in N and X j in N-{U} s.t. I ij is the highest mutual information (within all possible combinations)\nThus, our idea is to run the algorithm 1 twice in order to learn the F T and the ET structures (denoted by T F T and T ET ) separately from two training sets: the first, denoted by T S F T , is relative to the causes leading to T E and the second, denoted by T S ET , is relative to its consequences. In these two phases T E will be considered as the root. Then we will orient the resulted undirected trees semantically using the fact that events in T F T are causes of T E i.e. arcs in T F T will be directed towards T E and events in T ET are its consequences i.e. arcs in T ET will be backwards T E.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Bow Ties Parameters", "text": "Once the bow structure is fixed, we will learn its parameters in order to define its numerical component. As described above, this component differs from F T and ET , thus computations can be done as follows:\nTo quantify the fault tree F T , we will use a Bayesian approach based on informative priors. More precisely, to estimate P (X i = k | P a(X i ) = j) (i.e. the probability that X i is equal to k knowing that its parents denoted by P a(X i ) take the value j) we will use the maximum a posterior (MAP) estimate expressed by:\nwhere N ijk is the number of instances in the training set T S F T where X i = k and P a(X i ) = j occur conjointly and \u03b1 ijk is a Dirichlet prior having a simple interpretation in terms of pseudo counts i.e. we suppose that we saw the value k of X i for each value j of P a(X i ) \u03b1 ijk times. This value prevents us from declaring that the event (X i = k, P a i = j) is impossible just because it was not seen in the training set (which is the case of the standard maximum likelihood (ML)  Regarding the preventive and protective barriers, experts can simply use the generated graphical and numerical components in order to implement them. Note that the numerical component can be useful in this task since it will inform experts about the strength of different links in the bow tie.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2212 T E P F T HE T O P P S T DP DT T ODP DE", "text": "These variances are averaged by taking their mean across all six sensors, and\nis used as one of the features to be used for classification.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ratio of Time Spent in States", "text": "We expect that footsteps produce a quasi-periodic time series when one person walks whereas footsteps for two persons will be irregular and spread out. For two or more persons this \"footstep state\" will occupy a greater proportion of a window of fixed duration. Based on this criterion, the feature R(states) is calculated for each value of (i, j, k) as described below. For notational convenience we do not use j and k below.\nAs stated earlier, we use the 'extracted' data only. We move a sliding window (one-tenth of a second long) over this data, calculate the average of the x-values within the window, and compare it with a threshold \u03b7 to calculate\n1 if the average of the observations within the window is > \u03b7 0 if the average of the observations within the window is \u2264 \u03b7 (3) where p = 1, . . . , P and P represents the number of times the sliding window fits over the data. In the ideal situation, each '1' represents the \"Footstep state\" and each '0' represents the \"Silence state.\" The ratio\ncaptures the ratio of time spent in these states versus not in the state. These ratios, the third feature, are obtained for all values of (j, k) and are denoted as R(states) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-Correlation and Wavelet Based Feature", "text": "The cross-correlation sequence between adjacent sensors is one way to measure the similarity between the data collected by two sensors. We considered the crosscorrelation between the two nearest neighbors of a given sensor; for example, two nearest neighbors of sensor 2 are sensor 1 and sensor 3, respectively. As before the \"extracted\" data is obtained for sensors i = 2, 3, 4, 5 and the following correlation functions are computed:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Extending Metric Multidimensional Scaling with Bregman Divergences", "text": "Jigang Sun, Malcolm Crowe, and Colin Fyfe University of the West of Scotland {Jigang.Sun,Malcolm.Crowe,Colin.Fyfe}@uws.ac.uk Abstract. We investigate multidimensional scaling with Bregman divergences and show that the Sammon mapping can be thought of as a truncated Bregman multidimensional scaling (BMDS). We show that the full BMDS improves upon the Sammon mapping on some standard data sets and investigate the reasons underlying this improvement. We then introduce two families of BMDS which use opposite strategies to create good mappings of standard data sets and investigate these opposite strategies analytically.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "The quantity and dimensionality of data often captured automatically has been growing exponentially over the last decades. We are now in a world in which the extraction of information from data is of paramount importance. One means of doing this is to project the data onto a low dimensional manifold and allow a human investigator to search for patterns in this projection by eye. This is obviously based on the assumption that the low dimensional projection can capture relevant features of the high dimensional data set; the earliest methods for this type of analysis were principal component analysis and factor analysis, the former giving the linear projection which is closest to the original data while the latter tries to capture specific relevant features of the data set in which \"relevant\" often has to accord with some human intuition. However such projections are linear and there is no a priori reason that a high dimensional data set should lie on a linear subspace.\nFortunately however high dimensional data sets often lie on a (non-linear) embedded manifold of the data set where a manifold generally is thought of as a topological space which is locally Euclidean. Finding such embedded manifolds is generally not a simple closed-form process however, unlike finding linear subspaces. The alternatives are to find the manifold and then project the data onto this manifold or to directly attempt to ascertain the projections of the data.\nThis paper deals with one group of methods for finding the projections directly, multidimensional scaling (MDS). In the remainder of this section, metric multidimensional scaling (MMDS) and Bregman divergences will be briefly reviewed. In section 2, MMDS is generalised to BMDS: as an example, the classical Sammon mapping is extended to a Bregmanised version, and a comparative It is implemented using standard gradient descent, with initialisation of the latent points' positions as the configuration found by the Sammon mapping. It is tested on the standard data set in the literature, the Swiss roll data set, shown in Figure 2(a). The rest of Figure 2 displays 2-dimensional projections by LMMDS, the Sammon and ExtendedSammon mappings. We see that linear multidimensional scaling only captures the curve -there is no ability to differentiate points which lie at the same X,Y coordinate but different Z (vertical) coordinate in Figure 2(a). The Sammon mapping does rather better while the ExtendedSammon mapping does best of all.\nHowever this subjective comparison needs to be augmented quantitatively and with more analysis which hopefully will reveal why one projection is preferrable to another. Figure 3 shows graphs giving intuitions about the projections made by LMMDS, the Sammon and ExtendedSammon mappings. In Figure 3  Abstract. We review the technique of independent component analysis (ICA) and Stone's criterion for performing an independent component analysis. We then review Bregman divergences and show how they may be applied to Stone's criterion providing a very simple algorithm for performing ICA. We illustrate our method on two very simple data sets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Bregman divergences have recently received a great deal of interest recently in terms of clustering and finding unsupervised projections of a data set [2,5,4,3,1].\nIn this paper we shall investigate Bregman divergences in the context of Independent Component Analysis (ICA): whereas most ICA solutions utilize either kurtosis or entropy considerations (see below), we will concentrate on extensions of Stone's criterion [6].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Independent Component Analysis", "text": "Independent Component Analysis (ICA) is a method for identifying structure within a data set by ensuring that the components found are as independent as possible. The simplest problem statement starts with a set of d signals, s, which are linearly mixed by an unknown mixing matrix A. The only assumption we make about the data is that the original signals are independent and that at most one of them is from a Gaussian distribution. The unknown mixing matrix is usually generated randomly in experiments. ICA is the problem of recovering the unknown signals from the mixtures x = As. Thus we wish to find a demixing matrix W such that y = W x recovers the original signals. There are two indeterminacies inherent in the problem: since we make no assumptions about the mixing matrix and only minimal assumptions about the signals, there is a size indeterminacy. Also we are not in a position to determine the order in which the signals are recovered and so the best we can hope for is that \u2203i : y i = as j , \u2200j and for some constant a.\nA common principle for ICA is to make sure all components are as non-Gaussian as possible. According to the central limit theorem, the mixed observations become more Gaussian than any of the independent sources, so we  A fuller description of the properties of Bregman divergences can be found in [2]. The fact that we now have a family of divergences to work with raises the problem of which divergence to use with any particular data set or problem. The solution to this is discussed in the next section.\n4 The Exponential Family [2] have shown that there is bijection between a set of Bregman divergences and members of the regular exponential family of probability distributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simulations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Artificial Data", "text": "We begin with a simple demixing exercise on artificial data. We construct 3 signals each of 1000 samples, The three mixtures are shown in the first three diagrams of Figure 2 and the final output when we use the right Itakura Saito divergence (corresponding to J RIS ) is shown in the bottom right. We see that the low frequency sinusoid has been recovered: the correlation between the output found and the second signal was 0.999. Note that this was with the signals corrupted with noise from a uniform distribution (a platykurtotic distribution) which is far from the best noise distribution for a criterion which is optimal for leptokurtotic noise. We also show in Figure 3 the corresponding values of y and\u1ef9: we see that\u1ef9 gives a very smooth (indeed exact) representation of the original signal. For this result, we used \u03bb l = 0.00002, \u03bb s = 0.1, \u03b1 = 0.0001.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Image Identification", "text": "We now added together linearly 3 images, each 512\u00d7512: they were the wellknown \"Lena\", a picture of water and a herringbone texture. The original images are shown in We used a right Itakura-Saito divergence with \u03b1 = 0.0001 decaying to 0 during the experiment (3000 iterations over the whole data set), \u03bb 1 = 0.0002 and \u03bb s = 0.2.\nThe final output has correlations 0.991, 0.063 and 0.022 respectively with each of the original images. \"Lena\" is clearly identified. This is perhaps not surprising since \"Lena\" is by far the most slowly changing image of the three except of course round the eyes etc.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "What is Web 2.0: Design Patterns and Business Models for the next generation of software", "journal": "", "year": "2005", "authors": "T O'reilly"}, {"ref_id": "b1", "title": "Opening Welcome: The State of the Internet Industry", "journal": "", "year": "2004", "authors": "J Battelle; T O'reilly"}, {"ref_id": "b2", "title": "Spinning the Semantic Web: Bringing the World Wide Web to Its Full Potential", "journal": "MIT Press", "year": "2002", "authors": "D Fensel; W Wahlster; H Lieberman; J Hendler"}, {"ref_id": "b3", "title": "Enterprise 2.0: The Dawn of Emergent Collaboration", "journal": "MIT Sloan Management Review", "year": "2006", "authors": "A P Mcafee"}, {"ref_id": "b4", "title": "Open Innovation: Researching a New Paradigm", "journal": "Oxford University Press", "year": "2006", "authors": "H Chesbrough; W Vanhaverbeke; J West"}, {"ref_id": "b5", "title": "Web 2.0 in the Enterprise", "journal": "The Architecture Journal", "year": "2007", "authors": "M Platt"}, {"ref_id": "b6", "title": "Swarm Creativity: Competitive Advantage through Collaborative Innovation Networks", "journal": "Oxford University Press", "year": "2006", "authors": "P A Gloor"}, {"ref_id": "b7", "title": "The Relationship Between Web 2.0 And the Semantic Web", "journal": "ESTC", "year": "2007", "authors": "M Greaves"}, {"ref_id": "b8", "title": "Fostering social interaction in online spaces", "journal": "IOS Press", "year": "2001", "authors": "A Lee; C Danis; T Miller; Y Jung"}, {"ref_id": "b9", "title": "The Works of Aristotle", "journal": "Oxford University Press", "year": "1937", "authors": "Aristotle "}, {"ref_id": "b10", "title": "Reasoning with quantifiers", "journal": "", "year": "2002", "authors": "B Geurts"}, {"ref_id": "b11", "title": "A Handbook of Logic", "journal": "Brennan Press", "year": "2007", "authors": "J G Brennan"}, {"ref_id": "b12", "title": "The probability heuristics model of syllogistic reasoning", "journal": "Cognitive Psychology", "year": "1999", "authors": "N Chater; M Oaksford"}, {"ref_id": "b13", "title": "The effect of figure on syllogistic reasoning", "journal": "Memory and Cognition", "year": "1978", "authors": "L S Dickstein"}, {"ref_id": "b14", "title": "Conversion and possibility in syllogistic reasoning", "journal": "Bulletin of the Psychonomic Society", "year": "1981", "authors": "L S Dickstein"}, {"ref_id": "b15", "title": "Begriffsschrift, eine der Arithmetischen Nachgebildete Formalsprache des Reinen Denkens", "journal": "Verlag von Louis Nebert", "year": "1879", "authors": "L G F Frege"}, {"ref_id": "b16", "title": "The psychology of syllogisms", "journal": "Cognitive Psychology", "year": "1978", "authors": "P N Johnson-Laird; M Steedman"}, {"ref_id": "b17", "title": "Syllogistic inference", "journal": "Cognition", "year": "1984", "authors": "P N Johnson-Laird; B G Bara"}, {"ref_id": "b18", "title": "Study of Reasoning, ch. VIII", "journal": "", "year": "1864", "authors": "J Leechman"}, {"ref_id": "b19", "title": "Hand-Book of Logic", "journal": "Longman", "year": "1857", "authors": "J D Morell"}, {"ref_id": "b20", "title": "The probabilistic approach to human reasoning", "journal": "Trends in Cognitive Sciences", "year": "2001", "authors": "M Oaksford; N Chater"}, {"ref_id": "b21", "title": "Logic or the Art of Reasoning Simplified", "journal": "", "year": "1837", "authors": "S E Parker"}, {"ref_id": "b22", "title": "Artificial Intelligence -A Modern Approach", "journal": "Prentice-Hall", "year": "2009", "authors": "S Russell; P Norvig"}, {"ref_id": "b23", "title": "Contextual Logic and Aristotle's Syllogistic", "journal": "Springer", "year": "2005", "authors": "R Wille"}, {"ref_id": "b24", "title": "Fuzzy Logic and Approximate Reasoning. Syntheses", "journal": "", "year": "1975", "authors": "L A Zadeh"}, {"ref_id": "b25", "title": "Local and fuzzy logics", "journal": "", "year": "1977", "authors": "L A Zadeh; R E Bellman"}, {"ref_id": "b26", "title": "Knowledge distribution via shared context between blog-based knowledge management systems: a case study of collaborative tagging", "journal": "Expert Systems with Applications", "year": "2009", "authors": "J J Jung"}, {"ref_id": "b27", "title": "Restful web service composition with bpel for rest", "journal": "Data & Knowledge Engineering", "year": "2009", "authors": "C Pautasso"}, {"ref_id": "b28", "title": "Service-oriented applications for environmental models: Reusable geospatial services", "journal": "Environmental Modelling & Software", "year": "2010", "authors": "C Granell; L D\u00edaz; M Gould"}, {"ref_id": "b29", "title": "Collective Intelligence: Mankind's Emerging World in Cyberspace", "journal": "Basic Books", "year": "1994", "authors": "P L\u00e9vy"}, {"ref_id": "b30", "title": "On the collective nature of human intelligence", "journal": "Adaptive Behavior", "year": "2007", "authors": "A Pentland"}, {"ref_id": "b31", "title": "Semantic grounding of tag relatedness in social bookmarking systems", "journal": "Springer", "year": "2008", "authors": "C Cattuto; D Benz; A Hotho; G Stumme; A P Sheth; S Staab; M Dean; M Paolucci; D Maynard; T Finin"}, {"ref_id": "b32", "title": "Query transformation based on semantic centrality in semantic social network", "journal": "Journal of Universal Computer Science", "year": "2008", "authors": "J J Jung"}, {"ref_id": "b33", "title": "Tagster -tagging-based distributed content sharing", "journal": "Springer", "year": "2008", "authors": "O G\u00f6rlitz; S Sizov; S Staab; S Bechhofer; M Hauswirth; J Hoffmann"}, {"ref_id": "b34", "title": "Experiments in multilingual information retrieval using the SPIDER system", "journal": "ACM", "year": "1996", "authors": "P Sheridan; J P Ballerini; H P Frei; D Harman; P Sch\u00e4uble"}, {"ref_id": "b35", "title": "Analysis of user behavior on multilingual tagging of learning resources", "journal": "", "year": "2007", "authors": "R Vuorikari; X Ochoa; E Duval"}, {"ref_id": "b36", "title": "The state of the art in tag ontologies: a semantic model for tagging and folksonomies", "journal": "Dublin Core Metadata Initiative", "year": "2008", "authors": "H L Kim; S Scerri; J G Breslin; S Decker; H G Kim"}, {"ref_id": "b37", "title": "A low cost video technique for colour measurement of potato chips", "journal": "Lebensmittel-Wissenschaft und-Technologie", "year": "1999", "authors": "S Segnini; P Dejmek; R \u00d6ste"}, {"ref_id": "b38", "title": "A simple digital imaging method for measuring and analyzing color of food surfaces", "journal": "Journal of Food Engineering", "year": "2004", "authors": "K L Yam; S E Papadakis"}, {"ref_id": "b39", "title": "Color identification of some Turkish marbles", "journal": "Construction and Building Materials", "year": "2008", "authors": "M K G\u00f6kay; I B Gundogdu"}, {"ref_id": "b40", "title": "A computer-controlled rotating polarizer stage for the petrographic microscope", "journal": "Computers & Geosciences", "year": "1997", "authors": "F Fueten"}, {"ref_id": "b41", "title": "Automatic mineral classification in the macroscopic scale", "journal": "Computers & Geosciences", "year": "1997", "authors": "R Marschallinger"}, {"ref_id": "b42", "title": "Mineral identification using artificial neural networks and the rotating polarizer stage", "journal": "Computers & Geosciences", "year": "2001", "authors": "S Thompson; F Fueten; D Bockus"}, {"ref_id": "b43", "title": "An artificial neural net assisted approach to editing edges in patrographic images collected with rotating polarizer stage", "journal": "Computers & Geosciences", "year": "2007", "authors": "F Fueten; J Mason"}, {"ref_id": "b44", "title": "A fuzzy sensor for color matching vision system", "journal": "Measurement", "year": "2009", "authors": "V Bombardier; E Schmitt; P Charpentier"}, {"ref_id": "b45", "title": "The mismanagement of customer loyalty", "journal": "Harvard Bus. Rev", "year": "2002", "authors": "W Reinartz; V Kumar"}, {"ref_id": "b46", "title": "Knowledge management and data mining for marketing", "journal": "Decis. Support Syst", "year": "2001", "authors": "M J Shaw; C Subramaniam; G W Tan; M E Welge"}, {"ref_id": "b47", "title": "Toward a successful CRM: variable selection, sampling, and ensemble", "journal": "Decis. Support Syst", "year": "2006", "authors": "Y S Kim"}, {"ref_id": "b48", "title": "Predicting customer retention and profitability by using random forests and regression forests techniques", "journal": "Expert Syst. Appl", "year": "2005", "authors": "B Larivi\u00e8re; D Van Den Poel"}, {"ref_id": "b49", "title": "The application of AdaBoost in customer churn prediction", "journal": "", "year": "2007", "authors": "S Jinbo; L Xiu; L Wenhuang"}, {"ref_id": "b50", "title": "Modeling churn using customer lifetime value", "journal": "Eur. J. Oper. Res", "year": "2009", "authors": "N Glady; B Baesens; C Croux"}, {"ref_id": "b51", "title": "Bagging and boosting classification trees to predict churn", "journal": "J. Marketing Res", "year": "2006", "authors": "A Lemmens; C Croux"}, {"ref_id": "b52", "title": "Handling class imbalance in customer churn prediction", "journal": "Expert Syst. Appl", "year": "2009", "authors": "J Burez; D Van Den Poel"}, {"ref_id": "b53", "title": "Tree induction for probability-based ranking", "journal": "Mach. Learn", "year": "2003", "authors": "F Provost; P Domingos"}, {"ref_id": "b54", "title": "C4.5: Programs for Machine Learning", "journal": "Morgan Kauffman Publishers", "year": "1993", "authors": "R Quinlan"}, {"ref_id": "b55", "title": "Bagging predictors", "journal": "Mach. Learn", "year": "1996", "authors": "L Breiman"}, {"ref_id": "b56", "title": "The random subspace method for constructing decision forests", "journal": "IEEE T. Pattern Anal", "year": "1998", "authors": "T K Ho"}, {"ref_id": "b57", "title": "Combining bagging and random subspaces to create better ensembles", "journal": "Springer", "year": "2007", "authors": "P Panov; S Dzeroski"}, {"ref_id": "b58", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "J. Comput. Syst. Sci", "year": "1997", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b59", "title": "Tree-Based Ranking Methods", "journal": "IEEE T. Inform. Theory", "year": "2009", "authors": "S Clemen\u00e7on; N Vayatis"}, {"ref_id": "b60", "title": "Combining pattern classifiers: methods and algorithms", "journal": "John Wiley & Sons", "year": "2004", "authors": "L I Kuncheva"}, {"ref_id": "b61", "title": "The Case against Accuracy Estimation for Comparing Induction Algorithms", "journal": "Morgan Kaufman", "year": "2000", "authors": "F Provost; T Fawcett; R Kohavi"}, {"ref_id": "b62", "title": "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants", "journal": "Mach. Learn", "year": "1999", "authors": "E Bauer; R Kohavi"}, {"ref_id": "b63", "title": "Rotation forest: A new classifier ensemble method", "journal": "IEEE T. Pattern Anal", "year": "2006", "authors": "J J Rodr\u00edguez; L I Kuncheva; C J Alonso"}, {"ref_id": "b64", "title": "Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets. Pattern Recogn", "journal": "", "year": "2003", "authors": "R Bryll; R Gutierrez-Osuna; F Quek"}, {"ref_id": "b65", "title": "Constrained optimization of data-mining problems to improve model performance: A direct-marketing application", "journal": "Expert Syst. Appl", "year": "2005", "authors": "A Prinzie; D Van Den Poel"}, {"ref_id": "b66", "title": "Analyzing PETs on imbalanced datasets when training and testing class distributions differ", "journal": "Springer", "year": "2008", "authors": "D Cieslak; N Chawla"}, {"ref_id": "b67", "title": "The WEKA Data Mining Software: An Update. SIGKDD Explorations", "journal": "", "year": "2009", "authors": "E Frank; G Holmes; B Pfahringer; P Reutemann; I H Witten"}, {"ref_id": "b68", "title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "journal": "J. Comput. Syst. Sci", "year": "2003", "authors": "D Achlioptas"}, {"ref_id": "b69", "title": "Experiments with random projections for machine learning", "journal": "ACM", "year": "2003", "authors": "D Fradkin; D Madigan"}, {"ref_id": "b70", "title": "The Nature of Statistical Learning Theory (Information Science and Statistics)", "journal": "Springer", "year": "1999", "authors": "V N Vapnik"}, {"ref_id": "b71", "title": "Rotation forest: A new classifier ensemble method", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2006", "authors": "J J Rodr\u00edguez; L I Kuncheva; C J Alonso"}, {"ref_id": "b72", "title": "An experimental study on rotation forest ensembles", "journal": "Springer", "year": "2007", "authors": "L I Kuncheva; J J Rodr\u00edguez"}, {"ref_id": "b73", "title": "Extensions of Lipschitz maps into a Hilbert space", "journal": "", "year": "1982", "authors": "W Johnson; J Lindenstrauss"}, {"ref_id": "b74", "title": "The random subspace method for constructing decision forests", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1998", "authors": "T K Ho"}, {"ref_id": "b75", "title": "Data Mining: Practical Machine Learning Tools and Techniques", "journal": "Morgan Kaufmann", "year": "2005", "authors": "I Witten; E Frank"}, {"ref_id": "b76", "title": "Liblinear: A library for large linear classification", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "R E Fan; K W Chang; C J Hsieh; X R Wang; C J Lin"}, {"ref_id": "b77", "title": "UCI machine learning repository", "journal": "", "year": "2007", "authors": "A Asuncion; D Newman"}, {"ref_id": "b78", "title": "Bagging predictors", "journal": "Machine Learning", "year": "1996", "authors": "L Breiman"}, {"ref_id": "b79", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "Journal of Computer and System Sciences", "year": "1997", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b80", "title": "Multiboosting: A technique for combining boosting and wagging", "journal": "Machine Learning", "year": "2000", "authors": "G I Webb"}, {"ref_id": "b81", "title": "Bagging predictors", "journal": "Machine Learning", "year": "1996", "authors": "L Breiman"}, {"ref_id": "b82", "title": "Statistical comparisons of classifiers over multiple data sets", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "J Dem\u0161ar"}, {"ref_id": "b83", "title": "Random projection for high dimensional data clustering: A cluster ensemble approach", "journal": "", "year": "2003", "authors": "X Z Fern; C E Broadley"}, {"ref_id": "b84", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "J. of Computer and System Sciences", "year": "1997", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b85", "title": "Nonparametric discriminant analysis", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1983", "authors": "K Fukunaga; J Mantock"}, {"ref_id": "b86", "title": "An extension on statistical comparisons of classifiers over multiple data sets for all pairwise comparisons", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "S Garcia; F Herrera"}, {"ref_id": "b87", "title": "Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring", "journal": "Science", "year": "1999", "authors": "T R Golub; D K Stomin; P Tamayo"}, {"ref_id": "b88", "title": "Gene selection for cancer classification using support vector machines", "journal": "Machine Learning", "year": "2002", "authors": "I Guyon; J Weston; S Barnhill; V Vapnik"}, {"ref_id": "b89", "title": "Data Mining: Concepts and Techniques", "journal": "Morgan Kaufmann", "year": "2006", "authors": "J Han; M Kanber"}, {"ref_id": "b90", "title": "Independent component analysis: algorithms and applications", "journal": "Neural Networks", "year": "2000", "authors": "A Hyv\u00e4rinen; E Oja"}, {"ref_id": "b91", "title": "An experimental study on rotation forest ensembles", "journal": "Springer", "year": "2007", "authors": "L I Kuncheva; J J Rodr\u00edguez"}, {"ref_id": "b92", "title": "Combining Pattern Classifiers: Methods and Algorithms", "journal": "Wiley Interscience", "year": "2004", "authors": "L I Kuncheva"}, {"ref_id": "b93", "title": "Fastica for java", "journal": "", "year": "2006", "authors": "M Lambertz"}, {"ref_id": "b94", "title": "Application of independent component analysis to microarrays", "journal": "Genome Biology", "year": "2003", "authors": "S Lee; S Batzoglou"}, {"ref_id": "b95", "title": "How many genes are needed for a discriminant microarray data analysis? In: Critical Assessment of Techniques for Microarray Data Mining Workshop", "journal": "", "year": "2000", "authors": "W Li; Y Yang"}, {"ref_id": "b96", "title": "Linear modes of gene expressions determined by independent component analysis", "journal": "Bioinformatics", "year": "2002", "authors": "W Liebermeister"}, {"ref_id": "b97", "title": "Cancer classification using rotation forest", "journal": "Computers in Biology and Medicine", "year": "2008", "authors": "K Liu; D Huang"}, {"ref_id": "b98", "title": "Inference for the generalization error", "journal": "Machine Learning", "year": "2003", "authors": "C Nadeau; Y Bengio"}, {"ref_id": "b99", "title": "Using ensemble of classifiers in Bioinformatics", "journal": "Machine Learning Research Progress. Nova Science publisher", "year": "2009", "authors": "L Nanni; A Lumini"}, {"ref_id": "b100", "title": "Kent ridge bio-medical dataset", "journal": "", "year": "2009", "authors": "K Ridge"}, {"ref_id": "b101", "title": "Rotation forest: A new classifier ensemble method", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2006", "authors": "J J Rodr\u00edguez; L I Kuncheva; C J Alonso-Gonz\u00e1lez"}, {"ref_id": "b102", "title": "A review of feature selection techniques in bioinformatics", "journal": "Bioinformatics", "year": "2007", "authors": "Y Saeys; I Inza; P Larra\u00f1aga"}, {"ref_id": "b103", "title": "Feature selection and classification for small gene sets", "journal": "Springer", "year": "2008", "authors": "G Stiglic; J.-J Rodr\u00edguez; P Kokol"}, {"ref_id": "b104", "title": "Data mining microarray data -Comprehensive benchmarking of feature selection and classification methods", "journal": "", "year": "", "authors": "S Symons; K Nieselt"}, {"ref_id": "b105", "title": "FCM-SVM-RFE gene feature selection algorithm for leukemia classification from microarray gene expression data", "journal": "", "year": "2005", "authors": "Y Tang; Y Zhang; Z Huang"}, {"ref_id": "b106", "title": "Data Mining: Practical Machine Learning Tools and Techniques", "journal": "Morgan Kaufmann", "year": "2005", "authors": "I Witten; E Frank"}, {"ref_id": "b107", "title": "Biomarker identification by feature wrappers", "journal": "Genome Research", "year": "2001", "authors": "M Xiong; Z Fang; J Zhao"}, {"ref_id": "b108", "title": "Molecular diagnosis of human cancer type by gene expresion profiles and independent component analysis", "journal": "European J. Human Genetics", "year": "2005", "authors": "X W Zhang; Y L Yap; D Wei; F Chen; A Danchin"}, {"ref_id": "b109", "title": "Combining Pattern Classifiers: Methods and Algorithms", "journal": "Wiley Interscience", "year": "2004", "authors": "L I Kuncheva"}, {"ref_id": "b110", "title": "Bagging predictors", "journal": "Machine Learning", "year": "1996", "authors": "L Breiman"}, {"ref_id": "b111", "title": "The random subspace method for constructing decision forests", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1998", "authors": "T K Ho"}, {"ref_id": "b112", "title": "Experiments with a new boosting algorithm", "journal": "Morgan Kaufmann", "year": "1996", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b113", "title": "Improving regressors using boosting techniques", "journal": "Morgan Kaufmann Publishers Inc", "year": "1997", "authors": "H Drucker"}, {"ref_id": "b114", "title": "An empirical study of using rotation forest to improve regressors", "journal": "Applied Mathematics and Computation", "year": "2008", "authors": "C Zhang; J Zhang; G Wang"}, {"ref_id": "b115", "title": "Using iterated bagging to debias regressions", "journal": "Machine Learning", "year": "2001", "authors": "L Breiman"}, {"ref_id": "b116", "title": "Combining bias and variance reduction techniques for regression trees", "journal": "Springer", "year": "2005", "authors": "Y Suen; P Melville; R Mooney; J Gama; R Camacho; P B Brazdil; A M Jorge"}, {"ref_id": "b117", "title": "Managing diversity in regression ensembles", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "G Brown; J L Wyatt; P Ti\u0148o"}, {"ref_id": "b118", "title": "Approximate statistical test for comparing supervised classification learning algorithms", "journal": "Neural Computation", "year": "1998", "authors": "T G Dietterich"}, {"ref_id": "b119", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "Journal of Computer and System Sciences", "year": "1997", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b120", "title": "Induction of model trees for predicting continuous classes", "journal": "Springer", "year": "1997", "authors": "Y Wang; I H Witten"}, {"ref_id": "b121", "title": "Data Mining: Practical Machine Learning Tools and Techniques", "journal": "Morgan Kaufmann", "year": "2005", "authors": "I H Witten; E Frank"}, {"ref_id": "b122", "title": "Statistical comparisons of classifiers over multiple data sets", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "J Dem\u0161ar"}, {"ref_id": "b123", "title": "Experiments with AdaBoost.RT, an improved boosting scheme for regression", "journal": "Neural Computation", "year": "2006", "authors": "D L Shrestha; D P Solomatine"}, {"ref_id": "b124", "title": "Measures of diversity in classifier ensembles", "journal": "Machine Learning", "year": "2003", "authors": "L I Kuncheva; C J Whitaker"}, {"ref_id": "b125", "title": "Pruning adaptive boosting", "journal": "Morgan Kaufmann", "year": "1997", "authors": "D D Margineantu; T G Dietterich"}, {"ref_id": "b126", "title": "Disturbing neighbors ensembles for regression", "journal": "", "year": "2009", "authors": "J J Rodr\u00edguez; J Maudes; C Pardo; C Garc\u00eda-Osorio"}, {"ref_id": "b127", "title": "Stacking dynamic time warping for the diagnosis of dynamic systems", "journal": "Springer", "year": "2007", "authors": "C J Alonso; O J Prieto; J J Rodr\u00edguez; A Breg\u00f3n; B Pulido"}, {"ref_id": "b128", "title": "Temporal decision trees: Model-based diagnosis of dynamic systems on-board", "journal": "Journal of Artificial Intelligence Research", "year": "2003", "authors": "L Console; C Picardi; D T Dupre"}, {"ref_id": "b129", "title": "Characterising diagnosis and systems", "journal": "Morgan Kaufmann", "year": "1992", "authors": "J De Kleer; A K Mackworth; R Reiter"}, {"ref_id": "b130", "title": "Diagnosing multiple faults", "journal": "Artificial Intelligence", "year": "1987", "authors": "J De Kleer; B C Williams"}, {"ref_id": "b131", "title": "Diagnosing with behavioral modes", "journal": "", "year": "1989", "authors": "J De Kleer; B C Williams"}, {"ref_id": "b132", "title": "The consistency-based approach to automated diagnosis of devices", "journal": "CSLI Publications", "year": "1996", "authors": "O Dressler; P Struss"}, {"ref_id": "b133", "title": "Ensembles of nested dichotomies for multi-class problems", "journal": "", "year": "2004", "authors": "E Frank; S Kramer"}, {"ref_id": "b134", "title": "Bayesian fault detection and diagnosis in dynamic systems", "journal": "", "year": "2000", "authors": "U Lerner; R Parr; D Koller; G Biswas"}, {"ref_id": "b135", "title": "A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression", "journal": "BIOINFORMATICS", "year": "2005", "authors": "T Li; C Zhang; M Ogihara"}, {"ref_id": "b136", "title": "Diagnosis of continuous valued systems in transient operating regions", "journal": "IEEE T. Syst. Man. Cy. B", "year": "1999", "authors": "P Mosterman; G Biswas"}, {"ref_id": "b137", "title": "Model-based fault diagnosis in electric drives using machine learning", "journal": "IEEE/ASME Transactions On Mechatronics", "year": "2006", "authors": "Y L Murphey; M A Masrur; Z Chen; B Zhang"}, {"ref_id": "b138", "title": "Inference for the generalization error", "journal": "Machine Learning", "year": "2003", "authors": "C Nadeau; Y Bengio"}, {"ref_id": "b139", "title": "Fault diagnosis in nonlinear dynamic systems via neural networks", "journal": "", "year": "1994", "authors": "R J Patton; J Chen; T M Siew"}, {"ref_id": "b140", "title": "A bayesian approach to fault isolation with application to diesel engine diagnosis", "journal": "", "year": "2006", "authors": "A Pernestal; M Nyberg; B Wahlberg"}, {"ref_id": "b141", "title": "Possible conflicts: a compilation technique for consistencybased diagnosis", "journal": "IEEE T. Syst. Man Cy. B", "year": "2004", "authors": "B Pulido; C Alonso Gonz\u00e1lez"}, {"ref_id": "b142", "title": "Decomposition methodology for classification tasks: a meta decomposer framework", "journal": "Pattern Anal. Applic", "year": "2006", "authors": "L Rokach"}, {"ref_id": "b143", "title": "Data Mining: Practical Machine Learning Tools and Techniques", "journal": "Morgan Kaufmann", "year": "2005", "authors": "I Witten; E Frank"}, {"ref_id": "b144", "title": "Stacked generalization", "journal": "Neural Networks", "year": "1992", "authors": "D H Wolpert"}, {"ref_id": "b145", "title": "Sensor fault diagnosis in a chemical process via RBF neural networks", "journal": "Control Engineering Practice", "year": "1999", "authors": "D L Yu; J B Gomm; D Williams"}, {"ref_id": "b146", "title": "A Survey of Optimization by Building and Using Probabilistic Models", "journal": "Computational Optimization and Applications", "year": "2002", "authors": "M Pelikan; D E Goldberg; F G Lobo"}, {"ref_id": "b147", "title": "BOA: The Bayesian optimization algorithm", "journal": "Morgan Kaufmann Publishers", "year": "1999", "authors": "M Pelikan; D E Goldberg; E Cantu-Paz"}, {"ref_id": "b148", "title": "Linkage learning via probabilistic modeling in the ECGA", "journal": "", "year": "1999", "authors": "G Harik"}, {"ref_id": "b149", "title": "Estimation of distribution algorithms: A new tool for evolutionary computation", "journal": "Kluwer Academic Publishers", "year": "2002", "authors": "P Larra\u00f1aga; J A Lozano"}, {"ref_id": "b150", "title": "Real-coded Bayesian Optimization Algorithm: Bringing the Strength of BOA into the Continuous World", "journal": "Springer", "year": "2004", "authors": "C W Ahn; D E Goldberg; R S Ramakrishna"}, {"ref_id": "b151", "title": "On the Scalability of Real-Coded Bayesian Optimization Algorithm", "journal": "IEEE Transactions on Evolutionary Computation", "year": "2008", "authors": "C W Ahn; R S Ramakrishna"}, {"ref_id": "b152", "title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "journal": "Morgan Kaufmann", "year": "1988", "authors": "J Pearl"}, {"ref_id": "b153", "title": "Evaluation Relaxation Using Substructural Information and Linear Estimation", "journal": "ACM Press", "year": "2006", "authors": "K Sastry; C F Lima; D E Goldberg"}, {"ref_id": "b154", "title": "Fitness Inheritance in Bayesian Optimization Algorithm", "journal": "Springer", "year": "2004", "authors": "M Pelikan; K Sastry"}, {"ref_id": "b155", "title": "Substructural Neighborhoods for Local Search in the Bayesian Optimization Algorithm", "journal": "Springer", "year": "2006", "authors": "C F Lima; M Pelikan; K Sastry; M Butz; D E Goldberg; F G Lobo; T P Runarsson; H.-G Beyer; E K Burke; J J Merelo-Guerv\u00f3s; L D Whitley"}, {"ref_id": "b156", "title": "Entropy-Based Convergence Measurement in Discrete Estimation of Distribution Algorithms", "journal": "Studies in Fuzziness and Soft Computing", "year": "2006", "authors": "J Ocenasek"}, {"ref_id": "b157", "title": "The Complexity of Theorem Proving Procedures", "journal": "Association for Computing Machinery", "year": "1971", "authors": "S A Cook"}, {"ref_id": "b158", "title": "GRASP: A Search Algorithm for Propositional Satisfiability", "journal": "IEEE Transactions on Computers", "year": "1999", "authors": "J P Marques-Silva; K A Sakallah"}, {"ref_id": "b159", "title": "A machine program for theorem proving. communication of the ACM", "journal": "", "year": "1962", "authors": "M Davis; G Putnam; D Loveland"}, {"ref_id": "b160", "title": "Exact Algorithms for MAX-SAT. Electronic Notes in Theoretical", "journal": "Computer Science", "year": "2003", "authors": "H Zhang; H Shen"}, {"ref_id": "b161", "title": "A Flipping Genetic Algorithm for Hard 3-SAT Problems", "journal": "", "year": "1999", "authors": "E Marchiori; C Rossi"}, {"ref_id": "b162", "title": "A new method for solving hard satisfiability problems", "journal": "", "year": "1992", "authors": "B Selman; H Levesque; D Mitchell"}, {"ref_id": "b163", "title": "Local search algorithms for SAT: An empirical evaluation", "journal": "Journal of Automated Reasoning", "year": "2000", "authors": "H Holger; T St\u00fctzle"}, {"ref_id": "b164", "title": "The clonal selection algorithm with engineering applications", "journal": "", "year": "2000", "authors": "L N De Castro"}, {"ref_id": "b165", "title": "Two-Phase Exact Algorithm for MAX-SAT and Weighted MAX-SAT Problems", "journal": "Journal of Combinatorial Optimization", "year": "1999", "authors": "B Borchers; J Furman"}, {"ref_id": "b166", "title": "A Backbone-Based Co-evolutionary Heuristic for Partial MAX-SAT. Artificial Evolution", "journal": "", "year": "2005", "authors": "M Menai; M Batouche"}, {"ref_id": "b167", "title": "Multiple Sequence Alignment by Immune Artificial System", "journal": "", "year": "2007", "authors": "A Layeb; A Deneche"}, {"ref_id": "b168", "title": "An AIS for Multi-Modality Image Alignment", "journal": "Springer", "year": "2003", "authors": "E Bendiab; S Meshoul; M Batouche"}, {"ref_id": "b169", "title": "Local Search Strategies for Satisfiability Testing", "journal": "", "year": "1993", "authors": "B Selman; H Kautz; B Cohen"}, {"ref_id": "b170", "title": "Random Generation of Test Instanzes with Controlled Attributes", "journal": "", "year": "1996", "authors": "Y Asahiro; K Iwama; E Miyano"}, {"ref_id": "b171", "title": "A New Quantum Evolutionary Local Search Algorithm for MAX 3-SAT Problem", "journal": "Springer", "year": "2008", "authors": "A Layeb; D Saidouni"}, {"ref_id": "b172", "title": "Evolutionary Algorithms for Solving Multi-Objective Problems", "journal": "Springer", "year": "2006", "authors": "C A C Coello"}, {"ref_id": "b173", "title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "journal": "IEEE Transactions on Evolutionary Computation", "year": "2002", "authors": "K Deb; A Pratap; S Agarwal; T Meyarivan"}, {"ref_id": "b174", "title": "Embedded System Synthesis Benchmarks Suite", "journal": "", "year": "2008", "authors": "R P Dick"}, {"ref_id": "b175", "title": "Interconnection Networks: An Engineering Approach", "journal": "Morgan Kaufmann", "year": "2003", "authors": "J Duato; S Yalamanchili; L Ni"}, {"ref_id": "b176", "title": "Computers and intractability; a guide to the theory of NPcompleteness", "journal": "W. H. Freeman", "year": "1979", "authors": "M R Garey; D S Johnson"}, {"ref_id": "b177", "title": "SUNMAP: a tool for automatic topology selection and generation for nocs", "journal": "ACM Press", "year": "2004", "authors": "S Murali; G De Micheli"}, {"ref_id": "b178", "title": "Key research problems in NoC design: a holistic perspective", "journal": "ACM Press", "year": "2005", "authors": "\u00dc Y Ogras"}, {"ref_id": "b179", "title": "An Analysis of the Design and Definitions of Halstead's Metrics", "journal": "", "year": "2005", "authors": "Al Qutaish; R E Abran; A "}, {"ref_id": "b180", "title": "Technical analysis from A to Z", "journal": "McGraw-Hill", "year": "1995", "authors": "S B Achelis"}, {"ref_id": "b181", "title": "Introduction to evolutionary computing. Natural Computing Series", "journal": "Springer", "year": "2003", "authors": "A E Eiben; J E Smith"}, {"ref_id": "b182", "title": "Elements of software science", "journal": "North-Holland", "year": "1977", "authors": "M H Halstead"}, {"ref_id": "b183", "title": "Building a Genetic Programming Framework: The Added-Value of Design Patterns", "journal": "Springer", "year": "1998", "authors": "T Lenaerts; B Manderick"}, {"ref_id": "b184", "title": "A Genetic Algorithm Framework Applied to Quantum Circuit Synthesis", "journal": "Intelligence (SCI)", "year": "2008", "authors": "C Ruican; M Udrescu; L Prodan; M Vladutiu"}, {"ref_id": "b185", "title": "A Software Pattern of the Genetic Algorithm -a Study on Reusable Object Model of Genetic Algorithm, Wuhan University", "journal": "Journal of Natural Sciences", "year": "2001", "authors": "Z Shi; L Chao; H Ke-Qing"}, {"ref_id": "b186", "title": "The Halstead metrics", "journal": "", "year": "", "authors": " Virtualmachniery"}, {"ref_id": "b187", "title": "Comparing the template method and strategy design patterns in a genetic algorithm application", "journal": "ACM SIGCSE Bulletin", "year": "2002", "authors": "M R Wick; A T Phillips"}, {"ref_id": "b188", "title": "Protocols and Architectures for Wireless Sensor Networks", "journal": "John Wiley & Sons", "year": "2005", "authors": "H Karl; A Willig"}, {"ref_id": "b189", "title": "Wireless sensor networks: A survey", "journal": "Computer Networks", "year": "2002", "authors": "I F Akyildiz; W Su; Y Sankarasubramaniam; E Cyirci"}, {"ref_id": "b190", "title": "Intelligent Sensor Networks -an Agent-Oriented Approach", "journal": "", "year": "2005", "authors": "B Karlsson"}, {"ref_id": "b191", "title": "Tracking of unusual events in wireless sensor networks based on artificial neural-networks algorithms", "journal": "IEEE", "year": "2005", "authors": "A Kulakov; D Davcev"}, {"ref_id": "b192", "title": "Soft Computing in WSNs", "journal": "", "year": "2007", "authors": "A Averkin"}, {"ref_id": "b193", "title": "From a genetic fuzzy rule-based system to an intelligent sensor network", "journal": "IEEE", "year": "2007", "authors": "J Ca\u00f1ada-Bago"}, {"ref_id": "b194", "title": "D-FLER: A distributed fuzzy logic engine for rule-based wireless sensor networks", "journal": "", "year": "2007", "authors": "M Marin-Perianu; P Havinga"}, {"ref_id": "b195", "title": "Poster Abstract: A Knowledge Based Wireless Sensor Network", "journal": "", "year": "2009", "authors": "J Ca\u00f1ada-Bago; M A Gadeo-Martos; J A Fern\u00e1ndez-Prieto; J R Velasco"}, {"ref_id": "b196", "title": "Applications of fuzzy algorithm for control a simple dynamic plant. Proceedings of the", "journal": "IEE", "year": "1974", "authors": "E H Mamdani"}, {"ref_id": "b197", "title": "Genetic Fuzzy Systems: Evolutionary tuning and learning of fuzzy knowledge bases", "journal": "World scientific Publishing", "year": "2001", "authors": "O Cord\u00f3n; F Herrera; F Hoffmann; L Magdalena"}, {"ref_id": "b198", "title": "Fuzzy Rule-Based Modeling with Application to Geographical, Biological and Engineering Systems", "journal": "CRC Press", "year": "1995", "authors": "A Bardossy; L Duckstein"}, {"ref_id": "b199", "title": "A general study on genetic fuzzy systems", "journal": "John Wiley and Sons", "year": "1995", "authors": "O Cord\u00f3n; F Herrera; J Periaux; G Winter; M Gal\u00e1n"}, {"ref_id": "b200", "title": "Evolving fuzzy rule based controllers using genetic algorithms", "journal": "Fuzzy Sets and Systems", "year": "1996", "authors": "B Carse; T C Fogarty; A Munro"}, {"ref_id": "b201", "title": "Fuzzy if ... then rule models and their transformation into one other", "journal": "IEEE Transactions on Systems, Man, and Cybernetics", "year": "1996", "authors": "L Koczy"}, {"ref_id": "b202", "title": "An introduction to Fuzzy Control", "journal": "Springer", "year": "1993", "authors": "D Driankov; H Hellendoorrn; M Reinfrank"}, {"ref_id": "b203", "title": "Fuzzy logic in control systems: fuzzy logic controller-Parts I and II", "journal": "IEEE Transactions on Systems, Man, and Cybernetics", "year": "1990", "authors": "C C Lee"}, {"ref_id": "b204", "title": "Fuzzy Modelling: Paradigms and Practice", "journal": "Kluwer Academic Publishers", "year": "1996", "authors": "W Pedrycz"}, {"ref_id": "b205", "title": "A fuzzy-logic-based approach to qualitative modeling", "journal": "IEEE Transactions on Fuzzy Systems", "year": "1993", "authors": "M Sugeno; T Yasura"}, {"ref_id": "b206", "title": "Direct adaptive self-structuring fuzzy controller for nonaffine nonlinear system", "journal": "Fuzzy Sets and Systems", "year": "2005", "authors": "J.-H Park; G.-T Park; S.-H Kim; C.-J Moon"}, {"ref_id": "b207", "title": "Fuzzy logic controllers are universal approximators", "journal": "IEEE Transactions on Systems, Man and Cybernetics", "year": "1995", "authors": "J Castro"}, {"ref_id": "b208", "title": "Direct adaptive fuzzy control with a self-structuring algorithm", "journal": "Fuzzy Sets and Systems", "year": "2008", "authors": "P A Phan; T J Gale"}, {"ref_id": "b209", "title": "Fuzzy logic control of standalone photovoltaic system with battery storage", "journal": "Journal of Power Sources", "year": "2009", "authors": "S Lalouni; D Rekioua; T Rekioua; E Matagne"}, {"ref_id": "b210", "title": "An on-line robust and adaptive T-S fuzzy-neural controller for more general unknown systems", "journal": "International Journal of Fuzzy Systems", "year": "2008", "authors": "W Wang; Y Chien; I Li"}, {"ref_id": "b211", "title": "Control rules of aeration in a submerged biofilm wastewater treatment process using fuzzy neural networks", "journal": "Expert Systems with Applications", "year": "2009", "authors": "H Mingzhi; W Jinquan; M Yongwen; W Yan; L Weijiang; S Xiaofei"}, {"ref_id": "b212", "title": "Adaptive fuzzy controller: Application to the control of the temperature of a dynamic room in real time", "journal": "Fuzzy Sets and Systems", "year": "2006", "authors": "I Rojas; H Pomares; J Gonzalez; L Herrera; A Guillen; F Rojas; O Valenzuela"}, {"ref_id": "b213", "title": "A fuzzy controller with evolving structure", "journal": "Information Sciences", "year": "2004", "authors": "P Angelov"}, {"ref_id": "b214", "title": "Online global learning in direct fuzzy controllers", "journal": "IEEE Transactions on Fuzzy Systems", "year": "2004", "authors": "H Pomares; I Rojas; J Gonzalez; M Damas; B Pino; A Prieto"}, {"ref_id": "b215", "title": "Fuzzy adaptive learning control network with on-line neural learning", "journal": "Fuzzy Sets Syst", "year": "1995", "authors": "C Lin; C Lin; C S G Lee"}, {"ref_id": "b216", "title": "A dynamically generated fuzzy neural network and its application to torsional vibration control of tandem cold rolling mill spindles. Engineering Applications of", "journal": "Artificial Intelligence", "year": "2002", "authors": "L Wang"}, {"ref_id": "b217", "title": "Structure identification in complete rule-based fuzzy systems", "journal": "IEEE Transactions on Fuzzy Systems", "year": "2002", "authors": "H Pomares; I Rojas; J Gonzalez; A Prieto"}, {"ref_id": "b218", "title": "Modern Control Engineering", "journal": "Prentice-Hall", "year": "2001", "authors": "K Ogata"}, {"ref_id": "b219", "title": "Model Predictive Control", "journal": "Springer", "year": "1999", "authors": "E F Camacho; C Bordons"}, {"ref_id": "b220", "title": "Identification of nonlinear systems using neural networks and polynomial models: a block-oriented approach", "journal": "Springer", "year": "2005", "authors": "A Janczak"}, {"ref_id": "b221", "title": "Predictive control with constraints", "journal": "Prentice Hall", "year": "2002", "authors": "J M Maciejowski"}, {"ref_id": "b222", "title": "Machine tuning of stable analytical fuzzy predictive controllers", "journal": "Springer", "year": "2009", "authors": "P Marusak"}, {"ref_id": "b223", "title": "Output constraints in fuzzy DMC algorithms with parametric uncertainty in process models", "journal": "", "year": "2001", "authors": "P Marusak; P Tatjewski"}, {"ref_id": "b224", "title": "Fuzzy Modeling and Control", "journal": "Physica-Verlag", "year": "2001", "authors": "A Piegat"}, {"ref_id": "b225", "title": "Model-Based Predictive Control", "journal": "CRC Press", "year": "2003", "authors": "J A Rossiter"}, {"ref_id": "b226", "title": "Fuzzy identification of systems and its application to modeling and control", "journal": "IEEE Trans. Systems, Man and Cybernetics", "year": "1985", "authors": "T Takagi; M Sugeno"}, {"ref_id": "b227", "title": "Advanced Control of Industrial Processes; Structures and Algorithms", "journal": "Springer", "year": "2007", "authors": "P Tatjewski"}, {"ref_id": "b228", "title": "Interpretability issues in fuzzy modeling", "journal": "Studies in Fuzz. and Soft Comp", "year": "2003", "authors": "J Casillas; O Cord\u00f3n; F Herrera"}, {"ref_id": "b229", "title": "A multi-objective genetic algorithm for tuning and rule selection to obtain accurate and compact linguistic fuzzy rule-based systems", "journal": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "year": "2007", "authors": "R Alcal\u00e1; M J Gacto; F Herrera; J Alcal\u00e1-Fdez"}, {"ref_id": "b230", "title": "Hilk: A new methodology for designing highly interpretable linguistic knowledge bases using the fuzzy logic formalism", "journal": "International Journal of Intelligent Systems", "year": "2008", "authors": "J M Alonso; L Magdalena; S Guillaume"}, {"ref_id": "b231", "title": "Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning", "journal": "International Journal of Approximate Reasoning", "year": "2007", "authors": "H Ishibuchi; Y Nojima"}, {"ref_id": "b232", "title": "Context adaptation of fuzzy systems through a multi-objective evolutionary approach based on a novel interpretability index", "journal": "Soft Computing", "year": "2008", "authors": "A Botta; B Lazzerini; F Marcelloni; D C Stefanescu"}, {"ref_id": "b233", "title": "Towards neuro-linguistic modeling: constraints for optimization of membership functions", "journal": "Fuzzy Sets and Systems", "year": "1999", "authors": "J V De Oliveira"}, {"ref_id": "b234", "title": "A multiobjective evolutionary algorithm for tuning fuzzy rule based systems with measures for preserving interpretability", "journal": "", "year": "2009", "authors": "M J Gacto; R Alcal\u00e1; F Herrera"}, {"ref_id": "b235", "title": "Tuning fuzzy logic controllers by genetic algorithms", "journal": "International J. of Approximate Reasoning", "year": "1995", "authors": "F Herrera; M Lozano; J L Verdegay"}, {"ref_id": "b236", "title": "Genetic algorithms for fuzzy controllers", "journal": "AI Expert", "year": "1991", "authors": "C Karr"}, {"ref_id": "b237", "title": "Evolutionary algorithms for solving multi-objective problems", "journal": "Kluwer Academic Publishers", "year": "2002", "authors": "C A Coello; D A V Veldhuizen"}, {"ref_id": "b238", "title": "Multi-objective optimization using evolutionary algorithms", "journal": "John Wiley & Sons", "year": "2001", "authors": "K Deb"}, {"ref_id": "b239", "title": "Spea2: Improving the strength pareto evolutionary algorithm for multiobjective optimization", "journal": "", "year": "2001", "authors": "E Zitzler; M Laumanns; L Thiele"}, {"ref_id": "b240", "title": "Genetic tuning of fuzzy rule deep structures preserving interpretability and its interaction with fuzzy rule set reduction", "journal": "IEEE Trans. Fuzzy Syst", "year": "2005", "authors": "J Casillas; O Cord\u00f3n; M J D Jesus; F Herrera"}, {"ref_id": "b241", "title": "A formal model of interpretability of linguistic variables", "journal": "", "year": "", "authors": "U Bodenhofer; P Bauer"}, {"ref_id": "b242", "title": "Constructing fuzzy models with linguistic integrity from numerical data-AFRELI algorithm", "journal": "IEEE Trans. Fuzzy Syst", "year": "2000", "authors": "J Espinosa; J Vandewalle"}, {"ref_id": "b243", "title": "The CHC adaptive search algorithm: How to have safe search when engaging in nontraditional genetic recombination", "journal": "Morgan Kaufman", "year": "1991", "authors": "L J Eshelman"}, {"ref_id": "b244", "title": "Real-coded genetic algorithms and intervalschemata", "journal": "Foundations of Genetic Algorithms", "year": "1993", "authors": "L J Eshelman; J D Schaffer"}, {"ref_id": "b245", "title": "Generating fuzzy rules by learning from examples", "journal": "IEEE Trans. Syst", "year": "1992", "authors": "L X Wang; J M Mendel"}, {"ref_id": "b246", "title": "Modelling time series through fuzzy rule-based models: a statistical approach", "journal": "", "year": "2008", "authors": "M Aznarte; J L "}, {"ref_id": "b247", "title": "Smooth transition autoregressive models and fuzzy rule-based systems: Functional equivalence and consequences", "journal": "Fuzzy Sets Syst", "year": "2007", "authors": "M Aznarte; J L Ben\u00edtez; J M Castro; J L "}, {"ref_id": "b248", "title": "Forecasting airborne pollen concentration time series with neural and neuro-fuzzy models", "journal": "Expert Systems with Applications", "year": "2007", "authors": "M Aznarte; J L Ben\u00edtez; J M Nieto-Lugilde; D De Linares Fern\u00e1ndez; C De La D\u00edaz; C Guardia; F Alba S\u00e1nchez"}, {"ref_id": "b249", "title": "On the identifiability of TSK additive fuzzy rule-based models", "journal": "Springer", "year": "2006", "authors": "M Aznarte; J L Ben\u00edtez S\u00e1nchez; J M "}, {"ref_id": "b250", "title": "Testing for linear independence of the residuals in the framework of fuzzy rule-based models", "journal": "", "year": "2009-12", "authors": "M Aznarte; J L Ben\u00edtez S\u00e1nchez; J M "}, {"ref_id": "b251", "title": "Linearity testing against a fuzzy rule-based model. Fuzzy Sets and Systems", "journal": "", "year": "", "authors": "M Aznarte; J L Medeiros; M Ben\u00edtez S\u00e1nchez; J M "}, {"ref_id": "b252", "title": "Introductory Econometrics: Using Monte Carlo Simulation with Microsoft Excel", "journal": "Cambridge University Press", "year": "2005", "authors": "H Barreto; F Howland"}, {"ref_id": "b253", "title": "A decision support system for the selection of a rapid prototyping process using teh modified topsis method", "journal": "Intern. Journal of Advanced Manufacturing Technology", "year": "2005", "authors": "H Byun; K Lee"}, {"ref_id": "b254", "title": "Modeling uncertainty in clinical diagnosis using fuzzy logic", "journal": "IEEE Transactions on Systems, Man, and Cybernetics", "year": "2005", "authors": "R John; P Innocent"}, {"ref_id": "b255", "title": "Modeling gunshot bruises in soft body armor with an adaptive fuzzy system", "journal": "", "year": "2005", "authors": "I Lee; B Kosko; W F Anderson"}, {"ref_id": "b256", "title": "Diagnostic checking in a flexible nonlinear time series model", "journal": "Journal of Time Series Analysis", "year": "2003", "authors": "M Medeiros; A Veiga"}, {"ref_id": "b257", "title": "A flexible coefficient smooth transition time series model", "journal": "IEEE Transactions on Neural Networks", "year": "2005", "authors": "M Medeiros; A Veiga"}, {"ref_id": "b258", "title": "Local global neural networks: A new approach for nonlinear time series modeling", "journal": "Journal of the American Statistical Association", "year": "2004", "authors": "M Suarez-Farinas; C E Pedreira; M C Medeiros"}, {"ref_id": "b259", "title": "Specification, estimation and evaluation of smooth transition autoregresive models", "journal": "J. Am. Stat. Assoc", "year": "1994", "authors": "T Ter\u00e4svirta"}, {"ref_id": "b260", "title": "On a threshold model", "journal": "Pattern Recognition and Signal Processing", "year": "1978", "authors": "H Tong"}, {"ref_id": "b261", "title": "Robust fault diagnosis approach using analytical and knowledge based techniques applied to a water tank system. International journal of engineering intelligent systems for electrical engineering and communications", "journal": "", "year": "2005", "authors": "C F Vieira; L B Palma; R N Da Silva"}, {"ref_id": "b262", "title": "The state of the art in the routing and scheduling of vehicles and crews", "journal": "Computers and Operations Research", "year": "1983", "authors": "L Bodin; B Golden; A Assad; M Ball"}, {"ref_id": "b263", "title": "Vehicle routing", "journal": "", "year": "1992", "authors": "M L Fisher"}, {"ref_id": "b264", "title": "A computerized approach to the New York City school bus routing problem", "journal": "IIE Transactions", "year": "1997", "authors": "J Braca; J Bramel; B Posner; D Simchi-Levi"}, {"ref_id": "b265", "title": "School bus routes generator in urban surroundings", "journal": "Computers and Operations Research", "year": "1980", "authors": "G Dulac; J Ferland; P A Fogues"}, {"ref_id": "b266", "title": "The Granular Tabu Search and Its Application to the Vehicle-Routing Problem", "journal": "INFORMS Journal on Computing", "year": "2003", "authors": "P Toth; D Vigo"}, {"ref_id": "b267", "title": "A mathematical formulation for a school bus routing problem", "journal": "IEEE Press", "year": "2006", "authors": "P Schittekat; M Sevaux; K Sorensen"}, {"ref_id": "b268", "title": "Classical and modern heuristics for the vehicle routing problem", "journal": "Internat. Transact. Internat. Transact. in Operational Research", "year": "2000", "authors": "G Laporte; M Gendreau; J Y Potvin; F Semet"}, {"ref_id": "b269", "title": "A Methodology for Evaluating of School Bus Routing -A Case Study of Riverdale", "journal": "Transportation Research Board", "year": "2001", "authors": "L Spasovic; S Chien"}, {"ref_id": "b270", "title": "A self-organizing neural network approach for multiple traveling salesman and vehicle routing problems", "journal": "Journal of International Transactions in Operational Research", "year": "1999", "authors": "A Modares; S Somhom; T A Enkawa"}, {"ref_id": "b271", "title": "A location based heuristic for general routing problems", "journal": "Operations Research", "year": "1995", "authors": "J Bramel; D Simchi-Levi"}, {"ref_id": "b272", "title": "On the optimal solution value of the capacitated vehicle routing problem with unsplit demands", "journal": "", "year": "1990", "authors": "D Simchi-Levi; J Bramel"}, {"ref_id": "b273", "title": "Non-standard parameter adaptation for exploratory data analysis", "journal": "Springer", "year": "2009", "authors": "W Barbakh; Y Wu; C Fyfe"}, {"ref_id": "b274", "title": "Swarm intelligence in cellular robotic systems", "journal": "NATO Advanced Workshop on Robots and Biological Systems", "year": "1989", "authors": "G Beni; U Wang"}, {"ref_id": "b275", "title": "GTM: The generative topographic mapping", "journal": "Neural Computation", "year": "1998", "authors": "C M Bishop; M Svensen; C K I Williams"}, {"ref_id": "b276", "title": "Particle swarm optimization of feed-forward neural networks with weight decay", "journal": "", "year": "2006", "authors": "M Carvalho; T B Ludermir"}, {"ref_id": "b277", "title": "An adaptation of particle swarm optimization for markov decision processes", "journal": "", "year": "2004", "authors": "H S Chang"}, {"ref_id": "b278", "title": "Document clustering using particle swarm optimization", "journal": "", "year": "2005", "authors": "X Cui; T E Potok; P Palathingal"}, {"ref_id": "b279", "title": "Exploratory projection pursuit", "journal": "Journal of the American Statistical Association", "year": "1987", "authors": "J H Friedman"}, {"ref_id": "b280", "title": "Hebbian Learning and Negative Feedback Networks", "journal": "Springer", "year": "2005", "authors": "C Fyfe"}, {"ref_id": "b281", "title": "Two topographic maps for data visualization", "journal": "Data Mining and Knowledge Discovery", "year": "2007", "authors": "C Fyfe"}, {"ref_id": "b282", "title": "Training products of experts by minimizing contrastive divergence", "journal": "", "year": "2000", "authors": "G E Hinton"}, {"ref_id": "b283", "title": "Particle swarm optimization", "journal": "", "year": "1995", "authors": "J Kennedy; R Eberhart"}, {"ref_id": "b284", "title": "Swarm Intelligence", "journal": "Morgan Kaufmann Academic Press", "year": "2001", "authors": "J Kennedy; R Eberhart; Y Shi"}, {"ref_id": "b285", "title": "Self-organising maps", "journal": "Springer", "year": "1995", "authors": "T Kohonen"}, {"ref_id": "b286", "title": "Reinforcement Learning: An Introduction", "journal": "The MIT Press", "year": "1998", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b287", "title": "Learning from Delayed Rewards", "journal": "", "year": "1989", "authors": "C J C H Watkins"}, {"ref_id": "b288", "title": "Q-learning", "journal": "Machine Learning", "year": "1992", "authors": "C J C H Watkins; P Dayan"}, {"ref_id": "b289", "title": "Non-standard adaptation of linear projections for exploratory data analysis", "journal": "", "year": "2008", "authors": "Y Wu"}, {"ref_id": "b290", "title": "Heuristic Algorithms for the Terminal Assignment Problem", "journal": "ACM Press", "year": "1997", "authors": "S Khuri; T Chiu"}, {"ref_id": "b291", "title": "A hybrid Hopfield network-genetic algorithm approach for the terminal assignment problem", "journal": "IEEE Transaction On Systems, Man and Cybernetics", "year": "2004", "authors": "S Salcedo-Sanz; X Yao"}, {"ref_id": "b292", "title": "Hybrid evolutionary approaches to terminal assignment in communications networks", "journal": "Springer", "year": "2005", "authors": "X Yao; F Wang; K Padmanabhan; S Salcedo-Sanz"}, {"ref_id": "b293", "title": "The Bees Algorithm", "journal": "", "year": "2005", "authors": "D T Pham; A Ghanbarzadeh; E Ko\u00e7; S Otri; S Rahim; M Zaidi"}, {"ref_id": "b294", "title": "Manufacturing cell formation using the Bees Algorithm", "journal": "", "year": "2007", "authors": "D T Pham; A A Afify; E Ko\u00e7"}, {"ref_id": "b295", "title": "A Hybrid Differential Evolution Algorithm for solving the Terminal assignment problem", "journal": "Springer", "year": "2009", "authors": "E Bernardino; A Bernardino; J S\u00e1nchez-P\u00e9rez; M Vega-Rodr\u00edguez; J G\u00f3mez-Pulido"}, {"ref_id": "b296", "title": "Swarm intelligence", "journal": "Morgan Kaufmann", "year": "2001", "authors": "J Kennedy; R C Eberhart; Y Shi"}, {"ref_id": "b297", "title": "Terminal assignment in a Communications Network Using Genetic Algorithms", "journal": "ACM Press", "year": "1994", "authors": "F Abuali; D Schoenefeld; R Wainwright"}, {"ref_id": "b298", "title": "", "journal": "", "year": "", "authors": " Bees Algorithm Website"}, {"ref_id": "b299", "title": "Tabu Search vs Hybrid Genetic Algorithm to solve the terminal assignment problem", "journal": "IADIS Press", "year": "2008", "authors": "E Bernardino; A Bernardino; J S\u00e1nchez-P\u00e9rez; M Vega-Rodr\u00edguez; J G\u00f3mez-Pulido"}, {"ref_id": "b300", "title": "Non-standard cost terminal assignment problems using tabu search approach", "journal": "", "year": "2004", "authors": "Y Xu; S Salcedo-Sanz; X Yao"}, {"ref_id": "b301", "title": "Solving the Terminal Assignment Problem Using a Local Search Genetic Algorithm", "journal": "Springer", "year": "2008", "authors": "E Bernardino; A Bernardino; J S\u00e1nchez-P\u00e9rez; M Vega-Rodr\u00edguez; J G\u00f3mez-Pulido"}, {"ref_id": "b302", "title": "A Hybrid Ant Colony Optimization Algorithm for Solving the Terminal Assignment Problem", "journal": "", "year": "0109", "authors": "E Bernardino; A Bernardino; J S\u00e1nchez-P\u00e9rez; M Vega-Rodr\u00edguez; J G\u00f3mez-Pulido"}, {"ref_id": "b303", "title": "A survey of algorithms for the generalized assignment problem", "journal": "EJOR", "year": "1992", "authors": "D G References 1. Cattrysse; L N Van Wassenhove"}, {"ref_id": "b304", "title": "The Quadratic Assignment Problem", "journal": "Kluwer", "year": "1998", "authors": "E Cela"}, {"ref_id": "b305", "title": "Evolutionary Algorithms for Solving Multi-Objective Problems", "journal": "Kluwer", "year": "2002", "authors": "C A C Coello; D A Van Veldhuizen; G B Lamont"}, {"ref_id": "b306", "title": "Multiobjective analysis of facility location decisions", "journal": "EJOR", "year": "1990", "authors": "J Current; H Min; D Schilling"}, {"ref_id": "b307", "title": "A multiplier adjustment method for the generalized assignment problem", "journal": "Manag. Sci", "year": "1986", "authors": "M L Fisher; R Jaikumar; L Van Wassenhove"}, {"ref_id": "b308", "title": "Computers and Intractability. The Guide to the Theory of NP-Completeness", "journal": "W.H.Freeman and Company", "year": "1979", "authors": "M R Garey; D S Johnson"}, {"ref_id": "b309", "title": "Algorithms for the multi-resource generalized assignment problem", "journal": "Manag. Sci", "year": "1991", "authors": "B Gavish; H Pirkul"}, {"ref_id": "b310", "title": "Broadband Last Mile: Access Technologies for Multimedia Communications", "journal": "CRC Press/Taylor&Francis", "year": "2005", "authors": "N Jayant"}, {"ref_id": "b311", "title": "Joint optimal access point selection and channel assignment in wireless networks", "journal": "IEEE/ACM Trans. on Networking", "year": "2007", "authors": "I Koutsopoulos; L Tassiulas"}, {"ref_id": "b312", "title": "The Hungarian method for the assignment problems", "journal": "Nav. Res. Log", "year": "2005", "authors": "H W Kuhn"}, {"ref_id": "b313", "title": "Man-computer approaches to the multicriteria assignment problem", "journal": "Autom.&Remote Control", "year": "1998", "authors": "O I Larichev; M Sternin"}, {"ref_id": "b314", "title": "Composite Systems Decisions", "journal": "Springer", "year": "2006", "authors": "M S Levin"}, {"ref_id": "b315", "title": "Student research projects in system design", "journal": "", "year": "2009", "authors": "M S Levin"}, {"ref_id": "b316", "title": "Combinatorial optimization in system configuration design", "journal": "Automation & Remote Control", "year": "2009", "authors": "M S Levin"}, {"ref_id": "b317", "title": "Multi-objective redundancy allocation optimization using a variable neighborhood search algorithm", "journal": "J. of Heuristics", "year": "2010", "authors": "Y.-C Liang; M.-H Lo"}, {"ref_id": "b318", "title": "A survey on the generalized assignment problem", "journal": "INFOR", "year": "2007", "authors": "T Oncan"}, {"ref_id": "b319", "title": "On multi-criteria approach to fair and efficient bandwidth allocation", "journal": "Omega", "year": "2008", "authors": "W Orgyczak; A Wierzbicki; M Milewski"}, {"ref_id": "b320", "title": "A multicriteria assignment problem", "journal": "J. of Multi-Criteria Anal", "year": "2002", "authors": "A Scarelli; S C Narula"}, {"ref_id": "b321", "title": "Performance of the MOSA method for the bicriteria assignment problem", "journal": "J. of Heuristics", "year": "2000", "authors": "D Tuyttens; J Teghem; P Fortemps; K Van Nieuwenhuyze"}, {"ref_id": "b322", "title": "An algorithm for the generalzed assignment problem with special ordered sets", "journal": "J. of Heuristics", "year": "2005", "authors": "J M Wilson"}, {"ref_id": "b323", "title": "Tackling real-coded genetic algorithms: Operators and tools for behavioural analysis", "journal": "Artificial Intelligence Review", "year": "1998", "authors": "F Herrera; M Lozano; J L Verdegay"}, {"ref_id": "b324", "title": "Evolution Strategies: an alternative evolutionary algorithm", "journal": "Artificial Evolution", "year": "1995", "authors": "T Back"}, {"ref_id": "b325", "title": "Optimization, learning and natural algorithms (in Italian)", "journal": "", "year": "1992", "authors": "M Dorigo"}, {"ref_id": "b326", "title": "Ant colony System: A cooperative learning approach to the traveling salesman problem", "journal": "IEEE Trans. on Evol. Comp", "year": "1997", "authors": "M Dorigo; L M Gambardella"}, {"ref_id": "b327", "title": "Ant colony optimization for continuous domains", "journal": "European Journal of Operational Research", "year": "2008", "authors": "K Socha; M Dorigo"}, {"ref_id": "b328", "title": "Evolutionary programming using mutations based on L\u00e9vy probability distribution", "journal": "IEEE Trans. on Evol. Comp", "year": "2004", "authors": "C Lee; X Yao"}, {"ref_id": "b329", "title": "L\u00e9vy flights as an underlying mechanism for global optimization algorithms", "journal": "", "year": "2001", "authors": "M Gutowski"}, {"ref_id": "b330", "title": "Optimization of composite laminates", "journal": "NATO Advanced Study Institute on Optimization of Large Structural Systems", "year": "1991", "authors": "Z Gurdal; R T Haftka"}, {"ref_id": "b331", "title": "Composite laminate design optimization by genetic algorithm with generalized elitist selection", "journal": "Computers and Structures", "year": "2001", "authors": "G Soremekun; Z Gurdal; R T Haftka; L T Watson"}, {"ref_id": "b332", "title": "Optimum design of composite laminates for maximum buckling load capacity using simulated annealing Composite Structures", "journal": "", "year": "2005", "authors": "O Erdal; F O Sonmez"}, {"ref_id": "b333", "title": "Buckling optimization of laminated composite plates using genetic algorithm and generalized pattern search algorithm", "journal": "Struct. Multidisc. Optim", "year": "2009", "authors": "S Karakaya; O Soyksap"}, {"ref_id": "b334", "title": "Th\u00e9orie des erreurs la loi de Gauss et les lois exceptionnelles", "journal": "Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France", "year": "1924", "authors": "P L\u00e9vy"}, {"ref_id": "b335", "title": "Stable non-Gaussian random processes: Stochastic models with infinite variance", "journal": "Chapman and Hall", "year": "1994", "authors": "G Samorodnitsky; M S Taqqu"}, {"ref_id": "b336", "title": "Stochastic Calculus Applications in Science and Engineering", "journal": "Birkh\u00e4user", "year": "2002", "authors": "M Grigoriu"}, {"ref_id": "b337", "title": "Mechanics of Laminated Composite Plates and Shells", "journal": "CRC press", "year": "2004", "authors": "J N Reddy"}, {"ref_id": "b338", "title": "Cream: Class Library for Constraint Programming in Java", "journal": "", "year": "2009", "authors": "N Tamura"}, {"ref_id": "b339", "title": "Design and implementation of a course scheduling system using Tabu Search", "journal": "European Journal of Operational Research", "year": "2002", "authors": "R A Valdes; E Crespo; J M Tamarit"}, {"ref_id": "b340", "title": "Automated university timetabling: the state of the art", "journal": "The computer journal", "year": "1997", "authors": "E K Burke; K Jackson; J Kingston; R E Weare"}, {"ref_id": "b341", "title": "Recent developments in practical course timetabling", "journal": "Springer", "year": "1998", "authors": "M W Carter; G Laporte"}, {"ref_id": "b342", "title": "An upper bound for the chromatic number of graph and its application to timetabling problems", "journal": "The Computer Journal", "year": "1967", "authors": "D J A Welsh; M B Powell"}, {"ref_id": "b343", "title": "Technique for colouring a graph applicable to large scale timetabling problems", "journal": "The Computer Journal", "year": "1969", "authors": "D C A Wood"}, {"ref_id": "b344", "title": "Split Vertices in Vertex colouring and their application in developping a solution to the faculty timetable problem", "journal": "The Computer Journal", "year": "1988", "authors": "S M Selim"}, {"ref_id": "b345", "title": "PATAT 1995", "journal": "Springer", "year": "1996", "authors": "E K Burke"}, {"ref_id": "b346", "title": "Optimizing Timetabling Solutions Using Graph Coloring. NY: NPAC REU program", "journal": "", "year": "1995", "authors": "S Miner; S Elmohamed; H W Yau"}, {"ref_id": "b347", "title": "A Study of university timetabling that blends graph coloring with the satisfaction of various essential and preferential conditions", "journal": "", "year": "2004", "authors": "A R Timothy"}, {"ref_id": "b348", "title": "An integer programming formulation for a case study in university timetabling", "journal": "European Journal of Operational Research", "year": "2004", "authors": "S Daskalaki; T Birbas; E Housos"}, {"ref_id": "b349", "title": "Efficient solutions for university timetabling problem through integer programming", "journal": "European Journal of Operational Research", "year": "2005", "authors": "S Daskalaki; T Birbas"}, {"ref_id": "b350", "title": "Miliotis: An Automated Course Timetabling System developed in a distributed Environment: a Case Study", "journal": "European Journal of Operational Research", "year": "2004", "authors": "M Dimopoulou"}, {"ref_id": "b351", "title": "Implementation of a University Course and Examination Timetabling System", "journal": "European Journal of Operational Research", "year": "2001", "authors": "M Dimopoulou; P Miliotis"}, {"ref_id": "b352", "title": "Application of a real-world university-course timetabling model solved by integer programming", "journal": "Springer", "year": "2006", "authors": "K Schimmelpfeng; S Helber"}, {"ref_id": "b353", "title": "University Course Timetabling with Soft Constraints", "journal": "Springer", "year": "2003", "authors": "H Rudov; K Murray"}, {"ref_id": "b354", "title": "Using an Evolution Strategy for a University Timetabling System with a Web Based Interface to Gather Real Student Data", "journal": "Springer", "year": "2003", "authors": "T B George; V Opalikhin; C J Chung; E Cant\u00fa-Paz; J A Foster; K Deb; L Davis; R Roy; U.-M O'reilly; H.-G Beyer; G Kendall; S W Wilson; M Harman; J Wegener; D Dasgupta; M A Potter; A Schultz; K A Dowsland; N Jonoska; J Miller"}, {"ref_id": "b355", "title": "Case-based reasoning in course timetabling: an attribute graph approach", "journal": "Springer", "year": "2001", "authors": "E K Burke; B Maccathy; S Petrovic; R Qu"}, {"ref_id": "b356", "title": "Multiple-retrieval case-based reasoning for course timetabling problems", "journal": "Journal of the Operational Research Society", "year": "2005", "authors": "E K Burke; B Maccathy; S Petrovic; R Qu"}, {"ref_id": "b357", "title": "Knowledge discovery in a hyperheuristic for course timetabling using case-based reasoning", "journal": "Springer", "year": "2003", "authors": "E K Burke; B Maccathy; S Petrovic"}, {"ref_id": "b358", "title": "Structured case in case-based reasoning-re-using and adapting cases for timetabling problems", "journal": "Knowledge-Based Systems", "year": "2000", "authors": "E K Burke; B Maccathy; S Petrovic; R Qu"}, {"ref_id": "b359", "title": "Scripts, plans, goals and understanding. Erlbaum", "journal": "", "year": "1977", "authors": "R C Schank; R P Abelson"}, {"ref_id": "b360", "title": "A graph-based hyperheuristic for educational timetabling problem", "journal": "European Journal of Operational Research", "year": "2006", "authors": "E K Burke; B Mccollum; A Meisels; S Petrovic; R Qu"}, {"ref_id": "b361", "title": "Teachers and Classes with Neural Nets", "journal": "International Journal of Neural Systems", "year": "1989", "authors": "L Gislen; B Soderberg; C Peterson"}, {"ref_id": "b362", "title": "Complex scheduling with Potts neural networks", "journal": "Neural Computation", "year": "1992", "authors": "L Gislen; B Soderberg; C Peterson"}, {"ref_id": "b363", "title": "Calc/Cream: OpenOffice Spreadsheet Front-End for Constraint Programming", "journal": "Springer", "year": "2006", "authors": "N Tamura; M Umeda; A Wolf; O Bartenstein; U Geske; D Seipel"}, {"ref_id": "b364", "title": "Constraint Processing", "journal": "Morgan Kaufmann", "year": "2003", "authors": "R Dechter"}, {"ref_id": "b365", "title": "Mechanical resonant immunospecific biological detector", "journal": "Applied Physics Letters", "year": "2000", "authors": "B Illic; D Czaplewki; H G Craighead; P Neuzal; C Campagnolo; C Batt"}, {"ref_id": "b366", "title": "Attonewton force detection using silicon cantilevers", "journal": "Applied Physics Letters", "year": "1997", "authors": "T D Stowe; K Yasumura; T W Kenny; D Botkin; K Wago; D Rugar"}, {"ref_id": "b367", "title": "Nanometer-scale force sensing with MEMS devices", "journal": "IEEE Sensors Journal", "year": "2001", "authors": "T Kenny"}, {"ref_id": "b368", "title": "Mechanical detection of magnetic resonance", "journal": "Nature", "year": "1992", "authors": "D Rugar; C S Yannoni; J A Sidles"}, {"ref_id": "b369", "title": "Analysis of a Novel MEMS Gyroscope Actuated By Parametric Resonance", "journal": "", "year": "2008-07-04", "authors": "N J Miller; S W Shaw; L A Oropeza-Ramos; K L Turner"}, {"ref_id": "b370", "title": "Inherently robust micro gyroscope actuated by parametric resonance", "journal": "", "year": "2008", "authors": "L Oropeza-Ramos; C B Burgner; C Olroyd; K Turner"}, {"ref_id": "b371", "title": "Controlling Chaos", "journal": "Phys. Rev. Lett", "year": "1990", "authors": "B Ott; C Grebogi; J A Yorque"}, {"ref_id": "b372", "title": "A General Approach in the Design of active Controllers for Nonlinear Systems Exhibiting Chaos", "journal": "Int. J. Bifur. Chaos", "year": "2000", "authors": "S C Sinh\u00e1; J T Henrichs; B A Ravindra"}, {"ref_id": "b373", "title": "On control and synchronization in chaotic and hyperchaotic systems via linear feedback control", "journal": "Comunications on Nonlinear Science and numerical Simulations", "year": "2007", "authors": "M Rafikov; J M Balthazar"}, {"ref_id": "b374", "title": "Swarm Intelligence", "journal": "", "year": "1995", "authors": "J Kennedy; R Eberhardt"}, {"ref_id": "b375", "title": "Hammerstein Model Identification Based on Adaptive Particle Swarm Optimization, iita", "journal": "", "year": "2007", "authors": "Z Hou"}, {"ref_id": "b376", "title": "The particle swarm: explosion, stability, and convergence in a multidimensional complex space", "journal": "IEEE Transactions on Evolutionary Computation", "year": "2002", "authors": "M Clerc; J Kennedy"}, {"ref_id": "b377", "title": "On Particle Swam Optimization (PSO) Applied to a Micro-Mechanical Oscillator Model", "journal": "", "year": "2008", "authors": "F R Chavarette; I R Guilherme"}, {"ref_id": "b378", "title": "Nonlinear Behavior of a Parametric Resonance-Based Mass Sensor", "journal": "", "year": "2002", "authors": "W Zahng; R Baskaran; K Turner"}, {"ref_id": "b379", "title": "Relational Data Mining Applications: An Overview", "journal": "Springer", "year": "2001", "authors": "S Dzeroski"}, {"ref_id": "b380", "title": "Fragment generation and support vector machines for inducing sars", "journal": "SAR and QSAR in Environmental Research", "year": "2002", "authors": "S Kramer; E Frank; C Helma"}, {"ref_id": "b381", "title": "Data mining and machine learning techniques for the identification of mutagenicity inducing substructures and structure activity relationship of noncongeneric compounds", "journal": "J. Chem. Inf. Comput. Sci", "year": "2004", "authors": "C Helma; T Cramer; S Kramer; L D Raedt"}, {"ref_id": "b382", "title": "Support vector inductive logic programming outperforms the naive bayes classifier and inductive logic programming for the classification of bioactive compounds", "journal": "J. Comput. Aided Mol. Des", "year": "2007", "authors": "E O Cannon; A Amini; A Bender; M J E Sternberg; S H Muggleton; R C Glen; J B O Mitchell"}, {"ref_id": "b383", "title": "Quantitative Reactivity model for the hydratation of carbon dioxide by Biometric Zinc Complexes", "journal": "Inorg. Chem", "year": "2002", "authors": "M Brauer; J L P\u00e9res-Lustres; J Weston; E Anders"}, {"ref_id": "b384", "title": "A QSRR Treatment of Solvent Effects on the Decarboxylation of 6-Nitrobenzisoxazole-3-carboxylates Employing Molecular Descriptors", "journal": "J. Org. Chem", "year": "2001", "authors": "A R Katritzky; S Perumal; R Petrukhin"}, {"ref_id": "b385", "title": "Neural networks as a method for elucidating structure-property relationships for organic compounds", "journal": "Russ. Chem. Rev", "year": "2003", "authors": "N M Halberstam; I I Baskin; V A Palyulin; N S Zefirov"}, {"ref_id": "b386", "title": "Description of organic reactions based on imaginary transition structures. 1. Introduction of new concepts", "journal": "J. Chem. Inf. Comput. Sci", "year": "1986", "authors": "S Fujita"}, {"ref_id": "b387", "title": "ISIDA software", "journal": "", "year": "", "authors": "A Varnek"}, {"ref_id": "b388", "title": "Substructural fragments: an universal language to encode reactions, molecular and supramolecular structures", "journal": "J. Comput. Aided. Mol. Des", "year": "2005", "authors": "A Varnek; D Fourches; F Hoonakker; V P Solov'ev"}, {"ref_id": "b389", "title": "Modeling of ion complexation and extraction using substructural molecular fragments", "journal": "J. Chem. Inf. Comput. Sci", "year": "2000", "authors": "V P Solov'ev; A Varnek; G Wipff"}, {"ref_id": "b390", "title": "Molecular Descriptors for Chemoinformatics", "journal": "", "year": "2009", "authors": "R Todeschini; V Consonni"}, {"ref_id": "b391", "title": "Laboratory of chemical kinetics and catalysis", "journal": "", "year": "1977", "authors": ""}, {"ref_id": "b392", "title": "Data Mining: Practical machine learning tools and techniques", "journal": "Morgan Kaufmann", "year": "2005", "authors": "I H Witten; E Frank"}, {"ref_id": "b393", "title": "Regression Error Characteristic Curves", "journal": "", "year": "2003", "authors": "J Bi; K P Bennett"}, {"ref_id": "b394", "title": "Case-based reasoning: foundational issues, methodological variations, and system approaches", "journal": "AI Communications", "year": "1994", "authors": "A Aamodt; E Plaza"}, {"ref_id": "b395", "title": "Model-based classification of incident reports in a business information system. Sn approach to prototype-learning by structure generalisation", "journal": "", "year": "2009", "authors": "F Bader"}, {"ref_id": "b396", "title": "Similarity measures for structured representations", "journal": "Springer", "year": "1994", "authors": "H Bunke; B T Messmer"}, {"ref_id": "b397", "title": "Implementing anti-unification modulo equational theory", "journal": "Tech. Rep.). Arbeitspapiere der GMD", "year": "1996", "authors": "J Burghardt; B Heinz"}, {"ref_id": "b398", "title": "Defining inductive operators using distances over lists", "journal": "AAIP", "year": "2009", "authors": "V Estruch; C Ferri; J Hern\u00e1ndez-Orallo; M Ram\u00edrez-Quintana"}, {"ref_id": "b399", "title": "Cluster analysis, 4th edn", "journal": "Wiley", "year": "2001", "authors": "B S Everitt; S Landau; M Leese"}, {"ref_id": "b400", "title": "Connectionist construction of prototypes from decision trees for graph classification", "journal": "Intelligent Data Analysis", "year": "2003", "authors": "P Geibel; K Sch\u00e4dler; F Wysotzki"}, {"ref_id": "b401", "title": "Efficient subgraph isomorphism detection: A decomposition approach", "journal": "Efficient Subgraph Isomorphism Detection: A Decomposition Approach", "year": "2000", "authors": "B T Messmer; H Bunke"}, {"ref_id": "b402", "title": "Cases as terms: A feature term approach to the structured representation of cases", "journal": "Springer", "year": "1995", "authors": "E Plaza"}, {"ref_id": "b403", "title": "A note on inductive generalization", "journal": "Edinburgh University Press", "year": "1969", "authors": "G D Plotkin"}, {"ref_id": "b404", "title": "Induction of logic programs: FOIL and related systems", "journal": "New Generation Computing, Special Issue on Inductive Logic Programming", "year": "1995", "authors": "J Quinlan; R Cameron-Jones"}, {"ref_id": "b405", "title": "New trends in conceptual representation: Challenges to Piaget's theory?", "journal": "Lawrence Erlbaum", "year": "1983", "authors": "E Rosch"}, {"ref_id": "b406", "title": "Learning to predict one or more ranks in ordinal regression tasks", "journal": "Springer", "year": "2008", "authors": "J Alonso; J J Del Coz; J D\u00edez; O Luaces; A Bahamonde"}, {"ref_id": "b407", "title": "Classification with a reject option using a hinge loss", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "P Bartlett; M Wegkamp"}, {"ref_id": "b408", "title": "LIBSVM: a library for support vector machines", "journal": "", "year": "2001", "authors": "C C Chang; C J Lin"}, {"ref_id": "b409", "title": "On optimum recognition error and reject tradeoff", "journal": "IEEE Transactions on Information Theory", "year": "1970", "authors": "C Chow"}, {"ref_id": "b410", "title": "Learning nondeterministic classifiers", "journal": "Journal of Machine Learning Research", "year": "2009", "authors": "J J Del Coz; J D\u00edez; A Bahamonde"}, {"ref_id": "b411", "title": "Influ\u00eancia da carga pendente, do espa\u00e7amento e de fatores clim\u00e1ticos no desenvolvimento da ferrugem do cafeeiro", "journal": "SP", "year": "2007", "authors": "L Japiass\u00fa; A Garcia; A Miguel; C Carvalho; R Ferreira; L Padilha; J Matiello"}, {"ref_id": "b412", "title": "An\u00e1lise da epidemia da ferrugem do cafeeiro com\u00e1rvore de decis\u00e3o", "journal": "Tropical Plant Pathology", "year": "2008", "authors": "C Meira; L Rodrigues; S De Moraes"}, {"ref_id": "b413", "title": "Modelos de alerta para o controle da ferrugem-do-cafeeiro em lavouras com alta carga pendente", "journal": "Pesq. agropec. bras", "year": "2009", "authors": "C Meira; L Rodrigues; S De Moraes"}, {"ref_id": "b414", "title": "The multiscale nature of network traffic: Discovery, analysis, and modelling", "journal": "IEEE Signal Processing Magazine", "year": "2002", "authors": "P Abry; R Baraniuk; P Flandrin; R Riedi; D Veitch"}, {"ref_id": "b415", "title": "A stochastic model for the throughput of nonpersistent TCP flows", "journal": "Performance Evaluation", "year": "2008", "authors": "F Baccelli; D R Mcdonald"}, {"ref_id": "b416", "title": "Architecture of Multiagent Internet Measurement System MWING Release 2", "journal": "Springer", "year": "2009", "authors": "L Borzemski; L Cichocki; M Kliber; A H\u00e5kansson; N T Nguyen; R L Hartung; R J Howlett"}, {"ref_id": "b417", "title": "Time Series Analysis: Forecasting and Control", "journal": "Prentice-Hall", "year": "1994", "authors": "G Box; G M Jenkins; G C Reinsel"}, {"ref_id": "b418", "title": "A Test for Independence Based on the Correlation Dimension", "journal": "Econometric Reviews", "year": "1996", "authors": "W A Brock; W D Dechert; J A Scheinkman; B Lebaron"}, {"ref_id": "b419", "title": "Statistical Analysis of Active Web Performance Measurements", "journal": "", "year": "2010", "authors": "M Drwal; L Borzemski"}, {"ref_id": "b420", "title": "The Elements of Statistical Learning Data Mining, Inference, and Prediction, 2nd edn", "journal": "Springer", "year": "2009", "authors": "T Hastie; R Tibshirani; J Friedman"}, {"ref_id": "b421", "title": "On the Predictability of Large Transfer TCP Throughput", "journal": "Computer Networks", "year": "2007", "authors": "Q He; C Dovrolis; M Ammar"}, {"ref_id": "b422", "title": "Consistent estimate of the order of mixture models", "journal": "Comptes Rendus de l'Academie des Sciences Series I Mathematics", "year": "1998", "authors": "C Keribin"}, {"ref_id": "b423", "title": "ternet Traffic Classification Demystified: Myths, Caveats, and the Best Practices", "journal": "ACM", "year": "2008", "authors": "H Kim; K C Claffy; M Fomenkov; D Barman; M Faloutsos; K Y Lee"}, {"ref_id": "b424", "title": "Online EM Algorithm for Mixture with Application to Internet Traffic Modeling", "journal": "Computational Statistics and Data Analysis", "year": "2006", "authors": "Z Liu; J Almhana; V Choulakian; R Mcgorman"}, {"ref_id": "b425", "title": "The EM Algorithm and Extensions", "journal": "John Wiley & Sons Inc", "year": "1997", "authors": "G J Mclachlan; T Krishnan"}, {"ref_id": "b426", "title": "A machine Learning Approach to TCP Throughput Prediction", "journal": "", "year": "2007", "authors": "M Mirza; J Sommers; P Barford; X Zhu"}, {"ref_id": "b427", "title": "Self-similar Network Traffic and Performance Evaluation, 1st edn", "journal": "Wiley-Interscience", "year": "2000", "authors": "K Park; W Willinger"}, {"ref_id": "b428", "title": "Experiences Building PlanetLab. In: 7th symposium on Operating systems design and implementation OSDI", "journal": "", "year": "2006", "authors": "L Peterson; A Bavier; M Fiuczynski; S Muir"}, {"ref_id": "b429", "title": "A predictability analysis of network traffic", "journal": "Computer Networks", "year": "2002", "authors": "A Sang; S.-Q Li"}, {"ref_id": "b430", "title": "", "journal": "", "year": "2007-06", "authors": " Aneel;  Normative"}, {"ref_id": "b431", "title": "", "journal": "", "year": "", "authors": "S\u00e3o Chesf. Companhia Hidro El\u00e9trica Do;  Franscisco"}, {"ref_id": "b432", "title": "Neural network credit scoring models", "journal": "Computers and Operations Research", "year": "2000", "authors": "D West"}, {"ref_id": "b433", "title": "Improving Personalization Solutions through Optimal Segmentation of Customer Bases", "journal": "IEEE Trans. Knowledge and Data Eng", "year": "2009", "authors": "T Jiang; A Tuzhilin"}, {"ref_id": "b434", "title": "Introduction to Domain Driven Data Mining", "journal": "", "year": "2008", "authors": "L Cao"}, {"ref_id": "b435", "title": "Data Mining: Concepts and techniques", "journal": "Morgan Kaufmann", "year": "2006", "authors": "J Han; M Kamber"}, {"ref_id": "b436", "title": "Practical Nonparametric Statistics", "journal": "John Wiley & Sons", "year": "1999", "authors": "W J Conover"}, {"ref_id": "b437", "title": "Robust Classification for Imprecise Environments", "journal": "J. Machine Learning", "year": "2001", "authors": "F Provost; T Fawcett"}, {"ref_id": "b438", "title": "How Effective are Neural Networks at Forecasting and Prediction? A Review and Evaluation", "journal": "J. of Forecasting", "year": "1998", "authors": "M Adya; F Collopy"}, {"ref_id": "b439", "title": "Logistic Regression Models", "journal": "Chapman & Hall / CRC Press", "year": "2009", "authors": "J M Hilbe"}, {"ref_id": "b440", "title": "Bagging predictors", "journal": "Machine Learning", "year": "1996", "authors": "L Breiman"}, {"ref_id": "b441", "title": "The Power of Sampling and Stacking for the PAKDD-2007 Cross-Selling Problem", "journal": "Int. J. of Data Ware-housing and Mining (IJDWM)", "year": "2008", "authors": "P J L Adeodato; G C Vasconcelos; A L Arnaud; R C L V Cunha; D S M Monteiro; R Oliveira Neto"}, {"ref_id": "b442", "title": "MLP ensembles improve long term prediction accuracy over single networks", "journal": "Int. J. of Forecasting", "year": "2010", "authors": "P J L Adeodato; G C Vasconcelos; A L Arnaud; R C L V Cunha; D S M P Monteiro"}, {"ref_id": "b443", "title": "The Art of Computer Systems Performance Analysis Techniques for Experimental Design Measurements Simulation and Modeling", "journal": "John Wiley & Sons", "year": "1991", "authors": "R Jain"}, {"ref_id": "b444", "title": "Applied Linear Regression Models", "journal": "McGraw-Hill / Irwin", "year": "2004", "authors": "M Kutner; C Nachtsheim; J Neter"}, {"ref_id": "b445", "title": "Hippocratic databases", "journal": "Morgan Kaufmann", "year": "2002", "authors": "R Agrawal; J Kiernan; R Srikant; Y Xu"}, {"ref_id": "b446", "title": "Towards a functional ontology of reputation", "journal": "ACM", "year": "2005", "authors": "S J Casare; J S Sichman"}, {"ref_id": "b447", "title": "Engineering social order", "journal": "Springer", "year": "2000", "authors": "C Castelfranchi"}, {"ref_id": "b448", "title": "Principles of trust for mas: Cognitive anatomy, social importance, and quantification", "journal": "IEEE Computer Society", "year": "1998", "authors": "C Castelfranchi; R Falcone"}, {"ref_id": "b449", "title": "Sensitive data transaction in hippocratic multi-agent systems", "journal": "Springer", "year": "2009", "authors": "L Cr\u00e9pin; Y Demazeau; O Boissier; F Jacquenet"}, {"ref_id": "b450", "title": "Hippocratic multi-agent systems", "journal": "", "year": "2008", "authors": "L Cr\u00e9pin; L Vercouter; F Jacquenet; Y Demazeau; O Boissier"}, {"ref_id": "b451", "title": "A decentralized calendar system featuring sharing, trusting and negotiating", "journal": "Springer", "year": "2006", "authors": "Y Demazeau; D Melaye; M.-H Verrons"}, {"ref_id": "b452", "title": "Islander: an electronic institutions editor", "journal": "ACM", "year": "2002", "authors": "M Esteva; D De La Cruz; C Sierra"}, {"ref_id": "b453", "title": "Personalization of a trust network", "journal": "", "year": "2009", "authors": "L Lacomme; Y Demazeau; V Camps"}, {"ref_id": "b454", "title": "Ponder: Realising enterprise viewpoint concepts", "journal": "IEEE Computer Society", "year": "2000", "authors": "E Lupu; M Sloman; N Dulay; N Damianou"}, {"ref_id": "b455", "title": "Internet Commerce -Digital Models for Business", "journal": "John Wiley and Sons, Inc", "year": "2003", "authors": "E Lawrence; S Newton; B Corbitt; J Lawrence; S Dann; T Thanasankit"}, {"ref_id": "b456", "title": "An e-Market Framework for Informed Trading", "journal": "", "year": "2006", "authors": "J Debenham; S Simoff; L Carr; D D Roure; A Iyengar; C Goble"}, {"ref_id": "b457", "title": "Bargaining with information", "journal": "ACM Press", "year": "2004", "authors": "J Debenham; N Jennings; C Sierra; L Sonenberg"}, {"ref_id": "b458", "title": "Information-based agency", "journal": "", "year": "2007", "authors": "C Sierra; J Debenham"}, {"ref_id": "b459", "title": "Informing the Curious Negotiator: Automatic news extraction from the Internet", "journal": "", "year": "2004", "authors": "D Zhang; S Simoff"}, {"ref_id": "b460", "title": "Automatic web news extraction using tree edit distance", "journal": "", "year": "2004", "authors": "D Reis; P B Golgher; A Silva; A Laender"}, {"ref_id": "b461", "title": "Bayesian methods", "journal": "Springer", "year": "2003", "authors": "M Ramoni; P Sebastiani"}, {"ref_id": "b462", "title": "Exchange rate modelling using news articles and economic data", "journal": "Springer", "year": "2005", "authors": "D Zhang; S Simoff; J Debenham"}, {"ref_id": "b463", "title": "Using similarity criteria to make issue tradeoffs in automated negotiation", "journal": "Journal of Artificial Intelligence", "year": "2003", "authors": "P Faratin; C Sierra; N Jennings"}, {"ref_id": "b464", "title": "Auctions and bidding with information", "journal": "Springer", "year": "2006", "authors": "J Debenham"}, {"ref_id": "b465", "title": "Intelligent agents that make informed decisions", "journal": "Springer", "year": "2006", "authors": "J Debenham; E Lawrence"}, {"ref_id": "b466", "title": "Environment engineering for multiagent systems", "journal": "Journal on Engineering Applications of Artificial Intelligence", "year": "2005", "authors": "J L Arcos; M Esteva; P Noriega; J A Rodr\u00edguez; C Sierra"}, {"ref_id": "b467", "title": "", "journal": "Rules of Encounter. The MIT Press", "year": "1994", "authors": "J S Eide ; Rosenschein; G Zlotkin"}, {"ref_id": "b468", "title": "Internet Commerce -Digital Models for Business", "journal": "John Wiley and Sons, Inc", "year": "2003", "authors": "E Lawrence; S Newton; B Corbitt; J Lawrence; S Dann; T Thanasankit"}, {"ref_id": "b469", "title": "Informing the Curious Negotiator: Automatic news extraction from the Internet", "journal": "Springer", "year": "2006", "authors": "D Zhang; S Simoff"}, {"ref_id": "b470", "title": "Information-based agency", "journal": "", "year": "2007", "authors": "C Sierra; J Debenham"}, {"ref_id": "b471", "title": "An e-Market Framework for Informed Trading", "journal": "WWW-2006", "year": "2006", "authors": "J Debenham; S Simoff; L Carr; D D Roure; A Iyengar; C Goble"}, {"ref_id": "b472", "title": "Probability Theory -The Logic of Science", "journal": "Cambridge University Press", "year": "2003", "authors": "E Jaynes"}, {"ref_id": "b473", "title": "Information Theory, Inference and Learning Algorithms", "journal": "Cambridge University Press", "year": "2003", "authors": "D Mackay"}, {"ref_id": "b474", "title": "An approach for measuring semantic similarity between words using multiple information sources", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2003", "authors": "Y Li; Z A Bandar; D Mclean"}, {"ref_id": "b475", "title": "Argumentation-based negotiation", "journal": "Knowledge Engineering Review", "year": "2003", "authors": "I Rahwan; S Ramchurn; N Jennings; P Mcburney; S Parsons; E Sonenberg"}, {"ref_id": "b476", "title": "Intelligent agents that make informed decisions", "journal": "Springer", "year": "2006", "authors": "J Debenham; E Lawrence"}, {"ref_id": "b477", "title": "Ambivalent desires and the problem with reduction. Philosophical Studies (Published online", "journal": "", "year": "2009-03-25", "authors": "D Baker"}, {"ref_id": "b478", "title": "Bipolar possibility theory in preference modeling: Representation, fusion and optimal solutions", "journal": "Inf. Fusion", "year": "2006", "authors": "S Benferhat; D Dubois; S Kaci; H Prade"}, {"ref_id": "b479", "title": "Logical representation and fusion of prioritized information based on guaranteed possibility measures: application to the distance-based merging of classical bases", "journal": "Artif. Intell", "year": "2003", "authors": "S Benferhat; S Kaci"}, {"ref_id": "b480", "title": "Goal generation in the BOID architecture", "journal": "Cognitive Science Quarterly Journal", "year": "2002", "authors": "J Broersen; M Dastani; J Hulstijn; L Van Der Torre"}, {"ref_id": "b481", "title": "The role of beliefs in goal dynamics: Prolegomena to a constructive theory of intentions", "journal": "Synthese", "year": "2007", "authors": "C Castelfranchi; F Paglieri"}, {"ref_id": "b482", "title": "Intention is choice with commitment", "journal": "Artif. Intell", "year": "1990", "authors": "P R Cohen; H J Levesque"}, {"ref_id": "b483", "title": "Goal generation and adoption from partially trusted beliefs", "journal": "IOS Press", "year": "2008", "authors": "C Da Costa Pereira; A Tettamanzi"}, {"ref_id": "b484", "title": "Conditioning in possibility theory with strict order norms", "journal": "Fuzzy Sets Syst", "year": "1999", "authors": "B De Baets; E Tsiporkova; R Mesiar"}, {"ref_id": "b485", "title": "A synthetic view of belief revision with uncertain inputs in the framework of possibility theory", "journal": "International Journal of Approximate Reasoning", "year": "1997", "authors": "D Dubois; H Prade"}, {"ref_id": "b486", "title": "An introduction to bipolar representations of information and preference", "journal": "Int. J. Intell. Syst", "year": "2008", "authors": "D Dubois; H Prade"}, {"ref_id": "b487", "title": "An overview of the asymmetric bipolar representation of positive and negative information in possibility theory", "journal": "Fuzzy Sets Syst", "year": "2009", "authors": "D Dubois; H Prade"}, {"ref_id": "b488", "title": "Belief revision: A vademecum", "journal": "Meta-Programming in Logic", "year": "1992", "authors": "P G\u00e4rdenfors"}, {"ref_id": "b489", "title": "Asymmetry thesis and side-effect problems in linear-time and branching-time intention logics", "journal": "IJCAI", "year": "1991", "authors": "A S Rao; M P Georgeff"}, {"ref_id": "b490", "title": "Goal change", "journal": "", "year": "2005", "authors": "S Shapiro; Y Lesp\u00e9rance; H J Levesque"}, {"ref_id": "b491", "title": "Fuzzy sets. Information and Control", "journal": "", "year": "1965", "authors": "L A Zadeh"}, {"ref_id": "b492", "title": "Infectious Diseases of Humans: Dynamics and Control", "journal": "Oxford University Press", "year": "1992", "authors": "R A Anderson; R M May"}, {"ref_id": "b493", "title": "MABS 2007. LNCS (LNAI)", "journal": "Springer", "year": "2008", "authors": "L Antunes; M Paolucci"}, {"ref_id": "b494", "title": "Lectures in Macroeconomics", "journal": "MIT Press", "year": "1989", "authors": "O Blanchard; S Fischer"}, {"ref_id": "b495", "title": "Comparison of Agent-Based and Population-Based Simulations of Displacement of Crime", "journal": "IEEE Computer Society Press", "year": "2008", "authors": "T Bosse; C Gerritsen; M Hoogendoorn; S W Jaffry; J Treur"}, {"ref_id": "b496", "title": "Should Economic Psychology Care about Personality Structure", "journal": "Journal of Economic Psychology", "year": "1993", "authors": "H Brandst\u00e4tter"}, {"ref_id": "b497", "title": "Modelling with Differential Equations", "journal": "John Wiley", "year": "1981", "authors": "D N Burghes; M S Borrie"}, {"ref_id": "b498", "title": "Multi-Agent-Based Simulation IX. LNCS", "journal": "Springer", "year": "2009", "authors": "N David;  Sichman"}, {"ref_id": "b499", "title": "Dynamic Models in Biology", "journal": "Princeton University Press", "year": "2006", "authors": "S P Ellner; J Guckenheimer"}, {"ref_id": "b500", "title": "Agent-Based and Population-Based Simulation: A Comparative Case Study for Epidemics", "journal": "", "year": "2008", "authors": "S W Jaffry; J Treur"}, {"ref_id": "b501", "title": "Some costs of American corporate capitalism: A psychological exploration of value and goal conflicts", "journal": "Psychological Inquiry", "year": "2007", "authors": "T Kasser; S Cohn; A Kanner; R Ryan"}, {"ref_id": "b502", "title": "A Course in Microeconomic Theory", "journal": "Princeton University Press", "year": "1990", "authors": "D M Kreps"}, {"ref_id": "b503", "title": "Elements of Physical Biology", "journal": "", "year": "1924", "authors": "A J Lotka"}, {"ref_id": "b504", "title": "Models in Ecology", "journal": "Cambridge University Press", "year": "1974", "authors": "S Maynard"}, {"ref_id": "b505", "title": "The New Palgrave: A Dictionary of Economics", "journal": "MacMillan", "year": "1987", "authors": "H A Simon"}, {"ref_id": "b506", "title": "Agent-based computational economics: Growing economies from the bottom up", "journal": "Artificial Life", "year": "2002", "authors": "L Tesfatsion"}, {"ref_id": "b507", "title": "Fluctuations in the abundance of a species considered mathematically", "journal": "Nature", "year": "1926", "authors": "V Volterra"}, {"ref_id": "b508", "title": "Agent-based communities of web services: An argumentation-driven approach", "journal": "Service Oriented Computing and Applications", "year": "2008", "authors": "J Bentahar; Z Maamar; W Wan; D Benslimane; P Thiran; S Subramanian"}, {"ref_id": "b509", "title": "Model checking communicative agent-based systems", "journal": "Knowledge-Based Systems", "year": "2009", "authors": "J Bentahar; J.-J C Meyer; W Wan"}, {"ref_id": "b510", "title": "Model Checking AgentSpeak", "journal": "ACM", "year": "2003", "authors": "R H Bordini; M Fisher; C Pardavila; M Wooldridge"}, {"ref_id": "b511", "title": "NuSMV 2: An open source tool for symbolic model checking", "journal": "Springer", "year": "2002", "authors": "A Cimatti; E Clarke; E Giunchiglia; F Giunchiglia; M Pistore; M Roveri; R Sebastiani; A Tacchella"}, {"ref_id": "b512", "title": "Model checking", "journal": "MIT Press", "year": "1999", "authors": "E M Clarke; O Grumberg; D Peled"}, {"ref_id": "b513", "title": "Tool support for verifying UML activity diagrams", "journal": "IEEE Trans. Software Eng", "year": "2004", "authors": "R Eshuis; R Wieringa"}, {"ref_id": "b514", "title": "Model checking vs. theorem proving: a manifesto", "journal": "", "year": "1991", "authors": "J Halpern; M Y Vardi"}, {"ref_id": "b515", "title": "MDA Explained, The Model-Driven Architecture Practice and Promise", "journal": "Addison Wesley", "year": "2003", "authors": "A Kleppe; J Warmer; W Bast"}, {"ref_id": "b516", "title": "MCMAS: A model checker for the verification of multi-agent systems", "journal": "Springer", "year": "2009", "authors": "A Lomuscio; H Qu; F Raimondi"}, {"ref_id": "b517", "title": "Verifying action semantics specifications in UML behavioral models", "journal": "Springer", "year": "2009", "authors": "E Planas; J Cabot; C Gomez"}, {"ref_id": "b518", "title": "Model Checking Multi-Agent Systems", "journal": "", "year": "2006", "authors": "F Raimondi"}, {"ref_id": "b519", "title": "AgentSpeak(L): BDI agents speak out in a logical computable language", "journal": "Springer", "year": "1996", "authors": "A S Rao"}, {"ref_id": "b520", "title": "Model checking UML state machines and collaborations", "journal": "Elect. Notes in Theoretical Comp. Sc", "year": "2001", "authors": "T Schafer; A Knapp; S Merz"}, {"ref_id": "b521", "title": "MCK: Model checking knowledge", "journal": "", "year": "", "authors": "R Van Der Meyden; P Gammie"}, {"ref_id": "b522", "title": "System Engineering with SysML/UML Modeling, Analysis, Design", "journal": "Morgan Kaufmann", "year": "2007", "authors": "T Weilkiens"}, {"ref_id": "b523", "title": "Model checking multi-agent systems with MABLE", "journal": "ACM", "year": "2002", "authors": "M Wooldridge; M Fisher; M Huget; S Parsons"}, {"ref_id": "b524", "title": "Ambient Intelligence", "journal": "McGraw Hill", "year": "2001", "authors": "E Aarts; R Harwig; M Schuurmans"}, {"ref_id": "b525", "title": "Specification and Verification of Dynamics in Agent Models", "journal": "Int. J. of Cooperative Information Systems", "year": "2009", "authors": "T Bosse; C M Jonker; L Meij; A Van Der Sharpanskykh; J Treur"}, {"ref_id": "b526", "title": "Detecting human behavior models from multimodal observation in a smart home", "journal": "IEEE Transactions on Automation Science and Engineering", "year": "2009", "authors": "O Brdiczka; M Langet; J Maisonnasse; J L Crowley"}, {"ref_id": "b527", "title": "", "journal": "A Survey on Knowledge Compilation. AI Communications", "year": "1997", "authors": "M Cadoli; F M Donini"}, {"ref_id": "b528", "title": "Using Logic Programming to Detect Activities in Pervasive Healthcare", "journal": "Springer", "year": "2002", "authors": "H B Christensen"}, {"ref_id": "b529", "title": "A Temporal-Interactivist Perspective on the Dynamics of Mental States", "journal": "Cognitive Systems Research Journal", "year": "2003", "authors": "C M Jonker; J Treur"}, {"ref_id": "b530", "title": "Philosophy of Mind", "journal": "Westview Press", "year": "1996", "authors": "J Kim"}, {"ref_id": "b531", "title": "Mind as Motion: Explorations in the Dynamics of Cognition", "journal": "MIT Press", "year": "1995", "authors": "R F Port;  Van Gelder"}, {"ref_id": "b532", "title": "Modeling rational agents within a bdi-architecture", "journal": "", "year": "1991", "authors": "A Rao; M P Georgeff"}, {"ref_id": "b533", "title": "Hidden Markov Models for Activity Recognition in Ambient Intelligence Environments", "journal": "IEEE CS Press", "year": "2007", "authors": "D Sanchez; M Tentori; J Favela"}, {"ref_id": "b534", "title": "What Does Consciousness Bring to CTS?", "journal": "Springer", "year": "2008", "authors": "D Dubois; P Poirier; R Nkambou"}, {"ref_id": "b535", "title": "The LIDA architecture:adding new modes of learning to an intelligent, autonomous, software agent. Integrated Design and Process Technology", "journal": "", "year": "2006", "authors": "S Franklin; F G J Patterson"}, {"ref_id": "b536", "title": "Supporting Training on Canadarm Simulator using a Flexible Path Planner", "journal": "Artificial Intelligence in Education", "year": "2005", "authors": "R Nkambou; K Belghith; F Kabanza; M Khan"}, {"ref_id": "b537", "title": "Principles of cognitive neuroscience", "journal": "Sinauer Associates", "year": "2008", "authors": "D Purves; E Brannon; R Cabeza; S A Huettel; K Labar; M Platt; M Woldorff"}, {"ref_id": "b538", "title": "Cognitive Biases in Human Causal Learning", "journal": "", "year": "2007", "authors": "A Maldonado; A Catena; J C Perales; A C\u00e1ndido"}, {"ref_id": "b539", "title": "How to do the right thing", "journal": "Connection Science", "year": "1989", "authors": "P Maes"}, {"ref_id": "b540", "title": "Rules of the mind", "journal": "Erlbaum", "year": "1993", "authors": "J R Anderson"}, {"ref_id": "b541", "title": "Stochastic Independence between Recognition and Completion of Spatial Patterns as a Function of Causal Interpretation", "journal": "", "year": "2002", "authors": "W Schoppek"}, {"ref_id": "b542", "title": "The CLARION cognitive architecture: Extending cognitive modeling to social simulation", "journal": "Cambridge University Press", "year": "2006", "authors": "R Sun"}, {"ref_id": "b543", "title": "Mod\u00e9lisation de l'apprentissage ascendant des connaissances explicites dans une architecture cognitive hybride", "journal": "PHD", "year": "2007", "authors": "S H\u00e9lie"}, {"ref_id": "b544", "title": "A Theory of Causal Learning in Children: Causal Maps and Bayes Nets", "journal": "Psychological Review", "year": "2004", "authors": "A Gopnik; C Glymour; D M Sobel; L E Schulz; T Kushnir; D Danks"}, {"ref_id": "b545", "title": "The Copycat Project: A model of mental fluidity and analogy-making", "journal": "", "year": "1994", "authors": "D R Hofstadter; M Mitchell"}, {"ref_id": "b546", "title": "How Emotional Mechanism Helps Episodic Learning in a Cognitive Agent", "journal": "", "year": "2009", "authors": "U Faghihi; P Fournier-Viger; R Nkambou; P Poirier; A Mayers"}, {"ref_id": "b547", "title": "Implementation of Emotional Learning for Cognitive Tutoring Agents", "journal": "IEEE Computer Society press", "year": "2008", "authors": "U Faghihi; P Poirier; D Dubois; R Nkambou"}, {"ref_id": "b548", "title": "CMRULES: An Efficient Algorithm for Mining Sequential Rules Common to Several Sequences", "journal": "", "year": "2010", "authors": "P Fournier-Viger; U Faghihi; R Nkambou; E Mephu Nguifo"}, {"ref_id": "b549", "title": "Looking-in and looking-out of a vehicle: Computer-vision-based enhanced vehicle safety", "journal": "IEEE Transactions on Intelligent Transportation Systems", "year": "2007", "authors": "M Trivedi; T Gandhi; J Mccall"}, {"ref_id": "b550", "title": "Head pose estimation in computer vision: A survey", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2009", "authors": "E Murphy-Chutorian; M M Trivedi"}, {"ref_id": "b551", "title": "Determining driver visual attention with one camera", "journal": "IEEE Transactions on Intelligent Transportation Systems", "year": "2003", "authors": "P Smith; M Shah; N Da Vitoria Lobo"}, {"ref_id": "b552", "title": "Estimation of behavioral user state based on eye gaze and head pose-application in an e-learning environment", "journal": "Multimedia Tools Appl", "year": "2009", "authors": "S Asteriadis; P Tzouveli; K Karpouzis; S Kollias"}, {"ref_id": "b553", "title": "Locating nose-tips and estimating head poses in images by tensorposes", "journal": "IEEE Transaction on Circuits and Systems for Video Technology", "year": "2009", "authors": "J Tu; Y Fu; T S Huang"}, {"ref_id": "b554", "title": "Real-time system for monitoring driver vigilance", "journal": "IEEE Transactions on Intelligent Transportation Systems", "year": "2006", "authors": "L M Bergasa; J Nuevo; M Sotelo; R Barea; M L Lopez Guillen"}, {"ref_id": "b555", "title": "Video-based lane estimation and tracking for driver assistance: Survey, system, and evaluation", "journal": "IEEE Transaction on Intelligent Transportation Systems", "year": "2006", "authors": "J C Mccall; M M Trivedi"}, {"ref_id": "b556", "title": "Analysis of vehicle surroundings and driver status from video stream based on a single PAL camera", "journal": "", "year": "2009-08-16", "authors": "G Yu; X Xiao; J Bai"}, {"ref_id": "b557", "title": "Road markings detection and tracking usingn hough transform and kalman filter", "journal": "Springer", "year": "2005", "authors": "V Voisin; M Avila; B Emile; S Begot; J.-C Bardet"}, {"ref_id": "b558", "title": "Probabilistic approach for modeling and identifying driving situations", "journal": "", "year": "2008-06", "authors": "J Schneider; A Wilde; K Naab"}, {"ref_id": "b559", "title": "Robust Road Modeling and Tracking Using Condensation", "journal": "IEEE Transactions on Intelligent Transportation Systems", "year": "2008", "authors": "Y Wang; L Bai; M Fairhurst"}, {"ref_id": "b560", "title": "A Computational Approach To Edge Detection", "journal": "IEEE Trans. Pattern Analysis and Machine Intelligence", "year": "1986", "authors": "J Canny"}, {"ref_id": "b561", "title": "Machine Analysis of Bubble Chamber Pictures", "journal": "", "year": "1959", "authors": "P V C Hough"}, {"ref_id": "b562", "title": "Use of the Hough Transformation to Detect Lines and Curves in Pictures", "journal": "Comm. ACM", "year": "1972", "authors": "R O Duda; P E Hart"}, {"ref_id": "b563", "title": "Robust real-time object detection", "journal": "International Journal of Computer Vision", "year": "2002", "authors": "P Viola; M Jones"}, {"ref_id": "b564", "title": "Good features to track. Computer Vision and Pattern Recognition", "journal": "", "year": "1994", "authors": "J Shi; C Tomasi"}, {"ref_id": "b565", "title": "Object tracking: A survey", "journal": "ACM Comput. Surv", "year": "2006", "authors": "A Yilmaz; O Javed; M Shah"}, {"ref_id": "b566", "title": "Driver Behavior Recognition and Prediction in SmartCar", "journal": "", "year": "2000-04", "authors": "N Oliver; A Pentland"}, {"ref_id": "b567", "title": "The Prague Declaration -eHealth 2009 Conference Declaration", "journal": "", "year": "2009", "authors": "European Commission"}, {"ref_id": "b568", "title": "Essentials of telemedicine and telecare", "journal": "John Wiley and Sons", "year": "2002", "authors": "A C Norris"}, {"ref_id": "b569", "title": "The Residential Gateway as service platform. ICCE", "journal": "", "year": "2001", "authors": "K Hofrichter"}, {"ref_id": "b570", "title": "", "journal": "", "year": "", "authors": "Osgi Alliance"}, {"ref_id": "b571", "title": "User satisfaction with home telecare based on broadband communication", "journal": "J. Telemed Telecare", "year": "2002", "authors": "S Guillen"}, {"ref_id": "b572", "title": "Predicting need for intervention in individuals with congestive heart failure using a home-based telecare system", "journal": "J. Telemed Telecare", "year": "2009", "authors": "E Biddiss; S Brownsell; M S Hawley"}, {"ref_id": "b573", "title": "Designing an Embedded Electronic-Prescription Application for Home-Based Telemedicine Using OSGi Framework", "journal": "CSREA Press", "year": "2003", "authors": "P O Bobbie"}, {"ref_id": "b574", "title": "Developing Pervasive e-Health for Moving Experts from Hospital to Home", "journal": "", "year": "2004", "authors": "J Clemensen; S B Larsen; J E Bardram"}, {"ref_id": "b575", "title": "A Service-Oriented Agent Architecture to Support Telecardiology Services on Demand", "journal": "Journal of Medical and Biological Engineering", "year": "2005", "authors": "Y Chen; C Huang"}, {"ref_id": "b576", "title": "Services and Policies for Care At Home", "journal": "", "year": "2005", "authors": "F Wang"}, {"ref_id": "b577", "title": "An Optimized eHealth Platfom to Provide Electronic Services over Dynamic Networking Environments", "journal": "", "year": "2009", "authors": "P Plaza; N Sanz; J Gonzalez"}, {"ref_id": "b578", "title": "TELECARE: A multi-agent tele-supervision system for elderly care", "journal": "", "year": "", "authors": ""}, {"ref_id": "b579", "title": "Electronic data interchange for health care. Communications Magazine", "journal": "IEEE", "year": "1996", "authors": "A Hutchison"}, {"ref_id": "b580", "title": "Health Level 7: A protocol for the interchange of healthcare data", "journal": "IOS Press", "year": "1993", "authors": "W E Hammond"}, {"ref_id": "b581", "title": "HAPI: HL7 application programming interface", "journal": "", "year": "", "authors": ""}, {"ref_id": "b582", "title": "Novel ISO/IEEE 11073 Standards for Personal Telehealth Systems Interoperability. In: Joint Workshop on High Confidence Medical Devices, Software, and Systems and Medical Device Plug-and-Play Interoperability", "journal": "", "year": "2007", "authors": "L Schmitt; L Schmitt; T Falck"}, {"ref_id": "b583", "title": "", "journal": "UPnP Forum: Universal Plug and Play standard", "year": "", "authors": ""}, {"ref_id": "b584", "title": "Remote Service Usage Through Sip with Multimedia Access as a Use Case", "journal": "", "year": "2007", "authors": "A Haber; M Gerdes"}, {"ref_id": "b585", "title": "", "journal": "", "year": "", "authors": "Sig: Bluetooth;  Bluetooth"}, {"ref_id": "b586", "title": "Barriers to 'telecare': the perceptions and experiences of workers with responsibility for assessing for, and commissioning, care services and equipment", "journal": "Report for Essex County Council", "year": "2008", "authors": "S Hill"}, {"ref_id": "b587", "title": "Interoperability of a Mobile Health Care Solution with Electronic Healthcare Record Systems", "journal": "", "year": "2006", "authors": "P De Toledo; W Lalinde; F Del Pozo; D Thurber; S Jim\u00e9nez-Fern\u00e1ndez"}, {"ref_id": "b588", "title": "Apache Software Foundation", "journal": "Apache Felix", "year": "", "authors": ""}, {"ref_id": "b589", "title": "Cybergarage UPnP framework", "journal": "", "year": "", "authors": "K Satoshi"}, {"ref_id": "b590", "title": "Activity Monitoring System using Dynamic Time Warping for the Elderly and Disabled people", "journal": "", "year": "2009-02", "authors": "S Paiyarom"}, {"ref_id": "b591", "title": "Activity Recognition from Accelerometer Data", "journal": "American Association for Artificial Intelligence", "year": "2005", "authors": "N Ravi"}, {"ref_id": "b592", "title": "Activity Recognition using Wearable Sensors for Elder Care", "journal": "IEEE Future Generation Communication and Networking", "year": "2008", "authors": "Y.-J Hong"}, {"ref_id": "b593", "title": "Using acceleration measurements for activity recognition: An effective learning algorithm for constructing neural classifiers", "journal": "Pattern Recognition Letters", "year": "2008-08", "authors": "J.-Y Yang; J.-S Wang; Y.-P Chen"}, {"ref_id": "b594", "title": "Activity Recognition from Accelerometer Data on a Mobile Phone", "journal": "LNCS", "year": "2009-06", "authors": "T Brezmes; J.-L Gorricho; J Cotrina"}, {"ref_id": "b595", "title": "", "journal": "Estimation of Activity Energy Expenditure: Accelerometer Approach", "year": "2005-09", "authors": "J Hyun"}, {"ref_id": "b596", "title": "SmartBuckle: Human Activity Recognition using a 3-axis Accelerometer and a Wearable Camera", "journal": "", "year": "2008", "authors": "Y Cho"}, {"ref_id": "b597", "title": "Development ot a Single 3-Axis Accelerometer Sensor Based Wearable Gesture Recognition Band", "journal": "LNCS", "year": "", "authors": "I.-Y Cho"}, {"ref_id": "b598", "title": "Signal processing and machine learning for real-time classification of ergonomic posture with unobtrusive on-body sensors", "journal": "", "year": "2009-11", "authors": "G Olsen; S Brilliant; D Primeaux; K Najarian"}, {"ref_id": "b599", "title": "Classification Technique of Human Motion Context based on Wireless Sensor Network", "journal": "", "year": "2005", "authors": "N J K Joo Hyun Hong; E J Cha; T S Lee"}, {"ref_id": "b600", "title": "Graphs from search engine queries", "journal": "Springer", "year": "2007", "authors": "R Baeza-Yates; J Van Leeuwen; G F Italiano; W Van Der Hoek; C Meinel; H Sack"}, {"ref_id": "b601", "title": "Query suggestions using queryflow graphs", "journal": "", "year": "2009", "authors": "P Boldi; F Bonchi; C Castillo; D Donato; S Vigna"}, {"ref_id": "b602", "title": "Query chains: learning to rank from implicit feedback", "journal": "ACM Press", "year": "2005", "authors": "F Radlinski; T Joachims"}, {"ref_id": "b603", "title": "Patterns of search: analyzing and modeling web query refinement", "journal": "", "year": "1999", "authors": "T Lau; E Horvitz"}, {"ref_id": "b604", "title": "Analysis of multiple query reformulations on the web: the interactive information retrieval context", "journal": "Inf. Process. Management", "year": "2006", "authors": "S Y Rieh; H Xie"}, {"ref_id": "b605", "title": "Community search assistant", "journal": "Artificial Intelligence for Web Search", "year": "2001", "authors": "N S Glance"}, {"ref_id": "b606", "title": "Random walks on the click graph", "journal": "", "year": "2007", "authors": "N Craswell; M Szummer"}, {"ref_id": "b607", "title": "The query-flow graph: Model and applications", "journal": "", "year": "2008", "authors": "P Boldi; F Bonchi; C Castillo; D Donato; A Gionis; S Vigna"}, {"ref_id": "b608", "title": "Computational Intelligence: Principles, Techniques and Applications", "journal": "Springer", "year": "2005", "authors": "A Konar"}, {"ref_id": "b609", "title": "A new cluster validity measure and its application to image compression", "journal": "Pattern Anal. Appl", "year": "2004", "authors": "H Chou; M C Su; E Lai"}, {"ref_id": "b610", "title": "Http cookies: Standards, privacy, and politics", "journal": "", "year": "2001", "authors": "D M Kristol"}, {"ref_id": "b611", "title": "Ant algorithms and stigmergy", "journal": "Future Gener. Comput. Syst", "year": "2000", "authors": "M Dorigo; E Bonabeau; G Theraulaz"}, {"ref_id": "b612", "title": "Swarms on continuous data", "journal": "", "year": "2003", "authors": "V Ramos; A Abraham"}, {"ref_id": "b613", "title": "Ant Colony Optimization", "journal": "", "year": "2004", "authors": "M Dorigo; T St\u00fctzle"}, {"ref_id": "b614", "title": "Search engines that learn from implicit feedback", "journal": "Computer", "year": "2007", "authors": "T Joachims; F Radlinski"}, {"ref_id": "b615", "title": "Evaluating retrieval performance using clickthrough data", "journal": "SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval", "year": "2002", "authors": "T Joachims"}, {"ref_id": "b616", "title": "Application of decisiontree induction techniques to personalized advertisements on internet storefronts", "journal": "International Journal of Electronic Commerce", "year": "2001", "authors": "J W Kim; B H Lee; M J Shaw; H L Chang; M Nelson"}, {"ref_id": "b617", "title": "Adpalette: an algorithm for customizing online advertisements on the fly", "journal": "Decision Support Systems", "year": "2001", "authors": "G G Karuga; A M Khraban; S K Nair; D O Rice"}, {"ref_id": "b618", "title": "A vector space model for automatic indexing", "journal": "Commun. ACM", "year": "1975", "authors": "G Salton; A Wong; C S Yang"}, {"ref_id": "b619", "title": "= Prologue( SelectQuery | ConstructQuery | DescribeQuery | AskQuery | MiningQuery )", "journal": "", "year": "", "authors": ""}, {"ref_id": "b620", "title": "= CREATE MINING MODEL' Source '{' Var 'RESOURCE' 'TARGET' ( Var ( 'RESOURCE' | 'DISCRETE' | 'CONTINUOUS' ) 'MAXCARD1'? 'PREDICT'? 'CONTEXT'?)+ '}' DatasetClause* WhereClause SolutionModifier UsingClause", "journal": "", "year": "", "authors": " Miningquery"}, {"ref_id": "b621", "title": "Ontology Learning from Text: Methods, Evaluation and Applications. Frontiers in Artificial Intelligence and Applications", "journal": "IOS Press", "year": "2005", "authors": "P Buitelaar; P Cimiano"}, {"ref_id": "b622", "title": "Inductive logic programming: Theory and methods", "journal": "J. Log. Program", "year": "1994", "authors": "S Muggleton; L D Raedt"}, {"ref_id": "b623", "title": "Mining the Semantic Web: A logic-based methodology", "journal": "Springer", "year": "2005", "authors": "F A Lisi; F Esposito"}, {"ref_id": "b624", "title": "A knowledge discovery workbench for the Semantic Web", "journal": "", "year": "2004-08", "authors": "J Hartmann; Y Sure"}, {"ref_id": "b625", "title": "Kernel methods for mining instance data in ontologies", "journal": "Springer", "year": "2007", "authors": "S Bloehdorn; Y Sure; K Aberer; K.-S Choi; N Noy; D Allemang; K.-I Lee; L J B Nixon; J Golbeck; P Mika; D Maynard; R Mizoguchi; G Schreiber"}, {"ref_id": "b626", "title": "Objectminer: A new approach for mining complex objects", "journal": "ICEIS", "year": "2004", "authors": "R D\u00e1nger; J Ruiz-Shulcloper; R B Llavori"}, {"ref_id": "b627", "title": "Mining frequent similar patterns on mixed data", "journal": "Springer", "year": "2008", "authors": "A Y Rodr\u00edguez-Gonz\u00e1lez; J F Mart\u00ednez-Trinidad; J A Carrasco-Ochoa; J Ruiz-Shulcloper"}, {"ref_id": "b628", "title": "Frequent subtree mining -an overview", "journal": "Fundam. Inform", "year": "2005", "authors": "Y Chi; R R Muntz; S Nijssen; J N Kok"}, {"ref_id": "b629", "title": "Frequent subgraph discovery", "journal": "IEEE Computer Society", "year": "2001", "authors": "M Kuramochi; G Karypis"}, {"ref_id": "b630", "title": "Adding data mining support to SPARQL via statistical relational learning methods", "journal": "Springer", "year": "2008", "authors": "C Kiefer; A Bernstein; A Locher; S Bechhofer; M Hauswirth; J Hoffmann"}, {"ref_id": "b631", "title": "SPARQLeR: Extended SPARQL for semantic association discovery", "journal": "Springer", "year": "2007", "authors": "K Kochut; M Janik"}, {"ref_id": "b632", "title": "Mining association rules between sets of items in large databases", "journal": "ACM Press", "year": "1993", "authors": "R Agrawal; T Imielinski; A N Swami"}, {"ref_id": "b633", "title": "Efficient retrieval of ontology fragments using an interval labeling scheme", "journal": "Inf. Sci", "year": "2009", "authors": "V Nebot; R B Llavori"}, {"ref_id": "b634", "title": "Fast algorithms for mining association rules in large databases", "journal": "Morgan Kaufmann", "year": "1994", "authors": "R Agrawal; R Srikant"}, {"ref_id": "b635", "title": "Effective Latent Space Graph-based Re-ranking Model with Global Consistency", "journal": "", "year": "2009", "authors": "H Deng; M R Lyu; I King"}, {"ref_id": "b636", "title": "A Min-max Cut Algorithm for Graph Partitioning and Data Clustering", "journal": "", "year": "2001", "authors": "C H Q Ding; X He; H Zha; M Gu; H D Simon"}, {"ref_id": "b637", "title": "An Algorithm to Find Overlapping Community Structure in Networks", "journal": "Springer", "year": "2007", "authors": "S Gregory; J N Kok; J Koronacki; R Lopez De Mantaras; S Matwin; D Mladeni\u010d"}, {"ref_id": "b638", "title": "A Fast Algorithm to Find Overlapping Communities in Networks", "journal": "", "year": "2008", "authors": "S Gregory"}, {"ref_id": "b639", "title": "Extracting Key Terms From Noisy and Multitheme Documents", "journal": "", "year": "2009", "authors": "M P Grineva; M N Grinev; D Lizorkin"}, {"ref_id": "b640", "title": "Probabilistic Latent Semantic Indexing", "journal": "", "year": "1999", "authors": "T Hofmann"}, {"ref_id": "b641", "title": "Wordnet Improves Text Document Clustering", "journal": "", "year": "2003", "authors": "A Hotho; S Staab; G Stumme"}, {"ref_id": "b642", "title": "Exploiting Wikipedia as External Knowledge for Document Clustering", "journal": "", "year": "2009", "authors": "X Hu; X Zhang; C Lu; E K Park; X Zhou"}, {"ref_id": "b643", "title": "The DBLP Computer Science Bibliography: Evolution, Research Issues, Perspectives", "journal": "", "year": "2002", "authors": "M Ley"}, {"ref_id": "b644", "title": "Topic Modeling with Network Regularization", "journal": "", "year": "2008", "authors": "Q Mei; D Cai; D Zhang; C Zhai"}, {"ref_id": "b645", "title": "Modularity and Community Structure in Networks", "journal": "Proceedings of the National Academy of Sciences of the United States of America", "year": "2006", "authors": "M E J Newman"}, {"ref_id": "b646", "title": "Normalized Cuts and Image Segmentation", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2000", "authors": "J Shi; J Malik"}, {"ref_id": "b647", "title": "A Spectral Clustering Approach to Finding communities in Graphs", "journal": "", "year": "2005", "authors": "S White; P Smyth"}, {"ref_id": "b648", "title": "DBConnect: Mining Research Community on DBLP Data", "journal": "", "year": "2007", "authors": "O R Zaiane; J Chen; R Goebel"}, {"ref_id": "b649", "title": "An LDA-based Community Structure Discovery Approach for Large-Scale Social Networks", "journal": "", "year": "2007", "authors": "H Zhang; B Qiu; C L Giles; H C Foley; J Yen"}, {"ref_id": "b650", "title": "Topic Detection and Tracking Pilot Study Final Report", "journal": "", "year": "1998", "authors": "J Allan; J Carbonell; G Doddington; J Yamron; Y Yang"}, {"ref_id": "b651", "title": "TNO Hierarchical topic detection report at TDT", "journal": "", "year": "2004", "authors": "D Trieschnigg; W Kraaij"}, {"ref_id": "b652", "title": "Multiple Annotations of Reusable Data Resources: Corpora for Topic Detection and Tracking", "journal": "", "year": "2000", "authors": "C Cieri"}, {"ref_id": "b653", "title": "Extracting Related Named Entities from Blogosphere for Event Mining", "journal": "", "year": "2008", "authors": "Y Suhara; H Toda; A Sakurai"}, {"ref_id": "b654", "title": "Pattern Classification", "journal": "John Wiley and Sons", "year": "2001", "authors": "R O Duda; P E Hart; D G Stor"}, {"ref_id": "b655", "title": "Text Clustering with Extended User Feedback", "journal": "Annual ACM Conference on Research and Development in Information Retrieval", "year": "2006", "authors": "Y Huang; T M Mitchell"}, {"ref_id": "b656", "title": "Text Classification by Labeling Words", "journal": "", "year": "2004", "authors": "B Liu; X Li; W S Lee; P S Yu"}, {"ref_id": "b657", "title": "Fast and effective text mining using linear-time document clustering", "journal": "", "year": "1999", "authors": "B Larsen; C Aone"}, {"ref_id": "b658", "title": "Incremental Hierarchical Clustering of Text Documents", "journal": "", "year": "2006", "authors": "N Sahoo; J Callan; R Krishnan; G Duncan; R Padman"}, {"ref_id": "b659", "title": "Audience, structure and authority in the weblog community", "journal": "", "year": "2004-05", "authors": "C Marlow"}, {"ref_id": "b660", "title": "Intelligent and Cooperative Blog Communities", "journal": "", "year": "2009", "authors": "R Swezey; M Nakamura; S Shiramatsu; T Ozono; T Shintani"}, {"ref_id": "b661", "title": "Evolution of the Web, Agents, and Semantic Web", "journal": "IPSJ Magazine", "year": "2007-03", "authors": "H Takeda"}, {"ref_id": "b662", "title": "Sociological Typology of Personal Blogs", "journal": "", "year": "2007-04-27", "authors": "D Cardon; H Delaunay-Teterel; F C\u00e9dric; C Prieur"}, {"ref_id": "b663", "title": "Altruism, status and the origin of relevance. Approaches to the Evolution of Language", "journal": "", "year": "1998", "authors": "J Dessalles"}, {"ref_id": "b664", "title": "Co-evolution of social and affiliation networks", "journal": "ACM", "year": "2009", "authors": "E Zheleva; H Sharara; L Getoor"}, {"ref_id": "b665", "title": "When experts agree: using non-affiliated experts to rank popular topics", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "2002", "authors": "K Bharat; G Mihaila"}, {"ref_id": "b666", "title": "Matchmaking and brokering", "journal": "", "year": "1996", "authors": "K Decker; M Williamson; K Sycara"}, {"ref_id": "b667", "title": "Intelligent Agents: Theory and Practice", "journal": "Cambridge University Press", "year": "1995", "authors": "M Woolridge; N Jennings"}, {"ref_id": "b668", "title": "Introduction to Multiagent Systems", "journal": "John Wiley & Sons, Inc., USA", "year": "2001", "authors": "M Woolridge"}, {"ref_id": "b669", "title": "Scalable, distributed data mining using an agent based architecture", "journal": "AAAI Press", "year": "1997", "authors": "H Kargupta; I Hamzaoglu; B Stafford"}, {"ref_id": "b670", "title": "A logic-based framework for mobile intelligent information agents", "journal": "", "year": "2001", "authors": "N Fukuta; T Ito; T Shintani"}, {"ref_id": "b671", "title": "Ranking Weblogs by Eigen Rumor Algorithm", "journal": "Shakai Joho Shisutemugaku Shinpojiumu Gakujutsu Koen Ronbunshu", "year": "2005", "authors": "K Fujimura; N Tanimoto"}, {"ref_id": "b672", "title": "A Dynamic Rearrangement Mechanism of Web Page Layouts Using Web Agents", "journal": "Springer", "year": "2009", "authors": "M Nakamura; S Asami; T Ozono; T Shintani"}, {"ref_id": "b673", "title": "Relational learning via latent social dimensions", "journal": "ACM", "year": "2009", "authors": "L Tang; H Liu"}, {"ref_id": "b674", "title": "Semantic distance in wordnet: An experimental, application-oriented evaluation of five measures", "journal": "", "year": "2001", "authors": "A Budanitsky; G Hirst"}, {"ref_id": "b675", "title": "Wordnet: An Electronic Lexical Database, 1st edn", "journal": "Bradford Books", "year": "1998", "authors": "C Fellbaum"}, {"ref_id": "b676", "title": "Semrex: Efficient search in a semantic overlay for literature retrieval", "journal": "Future Generation Computer Systems", "year": "2008", "authors": "J Hai; C Hanhua"}, {"ref_id": "b677", "title": "Information retrieval by semantic similarity", "journal": "Int. Journal on Semantic Web and Information Systems (IJSWIS)", "year": "2006", "authors": "A Hliaoutakis; G Varelas; E Voutsakis; E G M Petrakis; E E Milios"}, {"ref_id": "b678", "title": "Semantic similarity based on corpus statistics and lexical taxonomy", "journal": "", "year": "1997", "authors": "J J Jiang; D W Conrath"}, {"ref_id": "b679", "title": "Combining local context and wordnet similarity for word sense identification", "journal": "", "year": "1998", "authors": "C Leacock; M Chodorow"}, {"ref_id": "b680", "title": "An approach for measuring semantic similarity between words using multiple information sources", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2003", "authors": "Y Li; Z A Bandar; D Mclean"}, {"ref_id": "b681", "title": "An information-theoretic definition of similarity", "journal": "", "year": "1998", "authors": "D Lin"}, {"ref_id": "b682", "title": "Semantic relatedness measure using object properties in an ontology", "journal": "Springer", "year": "2008", "authors": "L Mazuel; N Sabouret; A P Sheth; S Staab; M Dean; M Paolucci; D Maynard; T Finin"}, {"ref_id": "b683", "title": "Contextual correlates of semantic synonymy. Languages and Cognitive Processes", "journal": "", "year": "1991", "authors": "G Miller; W Charles"}, {"ref_id": "b684", "title": "Design, implementation and evaluation of a new semantic similarity metric combining features and intrinsic information content", "journal": "Springer", "year": "2008", "authors": "G Pirr\u00f3; N Seco"}, {"ref_id": "b685", "title": "Development and application of a metric on semantic nets", "journal": "IEEE Transactions on Systems, Man, and Cybernetics", "year": "1989", "authors": "R Rada; H Mili; E Bicknell; M Blettner"}, {"ref_id": "b686", "title": "Using information content to evaluate semantic similarity in a taxonomy", "journal": "Int. Joint Conf. on Artificial Intelligence", "year": "1995", "authors": "P Resnik"}, {"ref_id": "b687", "title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "journal": "Artificial Intelligence Research", "year": "1999", "authors": "P Resnik"}, {"ref_id": "b688", "title": "Contextual correlates of synonymy", "journal": "Communications of ACM", "year": "1965", "authors": "H Rubenstein; J B Goodenough"}, {"ref_id": "b689", "title": "Computational models of similarity in lexical ontologies", "journal": "", "year": "2005", "authors": "N Seco"}, {"ref_id": "b690", "title": "Features of similarity", "journal": "Psycological Review", "year": "1977", "authors": "A Tversky"}, {"ref_id": "b691", "title": "Verb semantics and lexical selection", "journal": "", "year": "1994", "authors": "Z Wu; M Palmer"}, {"ref_id": "b692", "title": "Automatic computation of semantic proximity using taxonomic knowledge", "journal": "", "year": "2006", "authors": "C.-N Ziegler; K Simon; G Lausen"}, {"ref_id": "b693", "title": "Evolving neural networks for word sense disambiguation", "journal": "", "year": "2008-09-10", "authors": "A Azzini; M Dragoni; C Da Costa Pereira; A Tettamanzi"}, {"ref_id": "b694", "title": "An information retrieval driven by ontology: from query to document expansion", "journal": "", "year": "2007", "authors": "M Baziz; M Boughanem; G Pasi; H Prade"}, {"ref_id": "b695", "title": "Techniques for efficient query expansion", "journal": "Springer", "year": "2004", "authors": "B Billerbeck; J Zobel"}, {"ref_id": "b696", "title": "Mercure at trec7", "journal": "Springer", "year": "1998", "authors": "M Boughanem; T Dkaki; J Mothe; C Soul\u00e9-Dupuy"}, {"ref_id": "b697", "title": "Automatic query expansion based on divergence", "journal": "CIKM", "year": "2001", "authors": "D Cai; C Van Rijsbergen; J Jose"}, {"ref_id": "b698", "title": "A fuzzy ontology-approach to improve semantic information retrieval", "journal": "", "year": "2007", "authors": "S Calegari; E Sanchez; F Bobillo; P Da Costa; C Amato; N Fanizzi; F Fung; T Lukasiewicz; T Martin; M Nickles; Y Peng; M Pool; P Smrz"}, {"ref_id": "b699", "title": "An adaptation of the vector-space model for ontology-based information retrieval", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2007", "authors": "P Castells; M Fern\u00e1ndez; D Vallet"}, {"ref_id": "b700", "title": "A framework for selective query expansion", "journal": "ACM", "year": "2004", "authors": "S Cronen-Townsend; Y Zhou; W Croft; D Grossman; L Gravano; C Zhai; O Herzog"}, {"ref_id": "b701", "title": "An ontology-based method for user model acquisition", "journal": "Springer", "year": "2006", "authors": "C Da Costa Pereira; A G B Tettamanzi"}, {"ref_id": "b702", "title": "Integrating mesh ontology to improve medical information retrieval", "journal": "Springer", "year": "2008", "authors": "M D\u00edaz-Galiano; M G Cumbreras; M Mart\u00edn-Valdivia; A M R\u00e1ez; L Ure\u00f1a-L\u00f3pez; C Peters; V Jijkoun; T Mandl; H M\u00fcller; D W Oard; A Pe\u00f1as; V Petras"}, {"ref_id": "b703", "title": "Ontology-based information retrieval: Overview and new proposition", "journal": "IEEE", "year": "2008", "authors": "O Dridi"}, {"ref_id": "b704", "title": "Query expansion", "journal": "", "year": "1996", "authors": "E Efthimiadis"}, {"ref_id": "b705", "title": "Indexing with wordnet synsets can improve text retrieval", "journal": "", "year": "1998", "authors": "J Gonzalo; F Verdejo; I Chugur; J M Cigarr\u00e1n"}, {"ref_id": "b706", "title": "Ichigen-san: An ontology-based information retrieval system", "journal": "Springer", "year": "2006", "authors": "T Hattori; K Hiramatsu; T Okadome; B Parsia; E Sirin; X Zhou; J Li; H T Shen; M Kitsuregawa"}, {"ref_id": "b707", "title": "A vector space model for automatic indexing", "journal": "Commun. ACM", "year": "1975", "authors": "G Salton; A Wong; C Yang"}, {"ref_id": "b708", "title": "Research on ontology-driven information retrieval", "journal": "Springer", "year": "2006", "authors": "S Tomassen"}, {"ref_id": "b709", "title": "Overview of the sixth text retrieval conference (trec-6)", "journal": "", "year": "1997", "authors": "E Voorhees; D Harman"}, {"ref_id": "b710", "title": "Design and implementation of ontology-based query expansion for information retrieval", "journal": "Springer", "year": "2007", "authors": "F Wu; G Wu; X Fu"}, {"ref_id": "b711", "title": "Query expansion using local and global document analysis", "journal": "ACM", "year": "1996", "authors": "J Xu; W Croft"}, {"ref_id": "b712", "title": "Agent-Based Simulation of Social Learning in Criminology", "journal": "INSTICC Press", "year": "2009", "authors": "T Bosse; C Gerritsen; M C A Klein"}, {"ref_id": "b713", "title": "Development and Validation of an Agent-Based Simulation Model of Juvenile Delinquency", "journal": "IEEE Computer Society Press", "year": "2009", "authors": "T Bosse; C Gerritsen; M C A Klein; F M Weerman"}, {"ref_id": "b714", "title": "Agent Based Social Simulation: A Computer Science View", "journal": "Journal of Artificial Societies and Social Simulation", "year": "2002", "authors": "P Davidsson"}, {"ref_id": "b715", "title": "A General Theory of Crime", "journal": "Stanford University Press", "year": "1990", "authors": "M Gottfredson; T Hirschi"}, {"ref_id": "b716", "title": "Signal Detection Theory and Psychophysics", "journal": "Wiley", "year": "1966", "authors": "D M Green; J A Swets"}, {"ref_id": "b717", "title": "Applied logistic Regression", "journal": "John Wiley & Sons", "year": "2000", "authors": "D Hosmer; S Lemeshow"}, {"ref_id": "b718", "title": "Simulating Crime Events and Crime Patterns in RA/CA Model", "journal": "", "year": "2005", "authors": "L Liu; X Wang; J Eck; J Liang"}, {"ref_id": "b719", "title": "The dynamics of delinquent peers and delinquent behaviour", "journal": "Criminology", "year": "1998", "authors": "R L Matsueda; K Anderson"}, {"ref_id": "b720", "title": "Analyzing Police Patrol Routes by Simulating the Physical Reorganisation of Agents", "journal": "Springer", "year": "2006", "authors": "A Melo; M Belchior; V Furtado"}, {"ref_id": "b721", "title": "Adolescence-Limited and Life-Course-Persistent Antisocial Behavior: A Developmental Taxonomy", "journal": "Psych. Review", "year": "1993", "authors": "T E Moffitt"}, {"ref_id": "b722", "title": "Principles of Criminology, 7th edn", "journal": "J.B. Lippincott", "year": "1966", "authors": "E H Sutherland; D R Cressey"}, {"ref_id": "b723", "title": "Companions in Crime. The social aspects of criminal conduct", "journal": "Cambridge University Press", "year": "2002", "authors": "M Warr"}, {"ref_id": "b724", "title": "Birds of Different Feathers", "journal": "European Journal of Criminology", "year": "2007", "authors": "F M Weerman; C C J H Bijleveld"}, {"ref_id": "b725", "title": "Problem behavior of students during secondary education: Individual development, student networks and reactions from school", "journal": "Aksant", "year": "2007", "authors": "F M Weerman; W Smeenk"}, {"ref_id": "b726", "title": "Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory", "journal": "Kluwer Academic Publishers", "year": "2003", "authors": "L Carlson; D Marcu; M E Okurowski"}, {"ref_id": "b727", "title": "Rhetorical Structure Theory: A Theory of Text Organization", "journal": "", "year": "1987", "authors": "W C Mann; S A Thompson"}, {"ref_id": "b728", "title": "Applications of Rhetorical Structure Theory. Discourse Studies", "journal": "", "year": "2005", "authors": "M Taboada; W C Mann"}, {"ref_id": "b729", "title": "Towards a Representation of the Rhetorical structure of Interrupted Exchanges", "journal": "Springer", "year": "1996", "authors": "T Daradoumis"}, {"ref_id": "b730", "title": "Rhetorical structure in dialog", "journal": "", "year": "2000", "authors": "A Stent"}, {"ref_id": "b731", "title": "Araucaria: Software for Argument Analysis, Diagramming and Representation", "journal": "", "year": "2004", "authors": "C Reed; G Rowe"}, {"ref_id": "b732", "title": "Recent Advances in Computational Models of Natural Argument", "journal": "Int. J. Int. Syst", "year": "2007", "authors": "C Reed; F Grasso"}, {"ref_id": "b733", "title": "Artificial Argument Assistants for Defeasible Argumentation", "journal": "Elsevier", "year": "2001", "authors": "B Verheij"}, {"ref_id": "b734", "title": "Planning text for advisory dialogues: Capturing Intentional and Rhetorical Information", "journal": "Computational linguistics -Association for Computational Linguistics", "year": "1993", "authors": "J D Moore; C L Paris"}, {"ref_id": "b735", "title": "A Corpus for Studying Addressing Behaviour in Multi-Party Dialogues", "journal": "Springer Science + Business", "year": "2006", "authors": "N Jovanovic; R O Den Akker; A Nijholt"}, {"ref_id": "b736", "title": "Semantic Authoring and Semantic Computing", "journal": "Springer", "year": "2007", "authors": "K Hashida"}, {"ref_id": "b737", "title": "Creating an Argumentation Corpus: Do Theories Apply to Real Arguments?", "journal": "", "year": "2009", "authors": "R Mochales; A Ieven"}, {"ref_id": "b738", "title": "An introduction to voice search", "journal": "IEEE Signal Processing Magazine", "year": "2008-05", "authors": "Y Y Wang; D Yu; Y C Ju; A Acero"}, {"ref_id": "b739", "title": "Enabling a User to Specify an Item at Any Time During System Enumeration -Item Identification for Barge-In-Able Conversational Dialogue Systems", "journal": "", "year": "2009", "authors": "K Matsuyama; K Komatani; T Ogata; H G Okuno"}, {"ref_id": "b740", "title": "Latent semantic mapping", "journal": "IEEE Signal Processing Magazine", "year": "2005", "authors": "J R Bellegarda"}, {"ref_id": "b741", "title": "A hybrid barge-in procedure for more reliable turn-taking in human-machine dialogue systems", "journal": "", "year": "2003", "authors": "R C Rose; H K Kim"}, {"ref_id": "b742", "title": "Discriminative training of multi-state barge-in models", "journal": "", "year": "2007", "authors": "A Ljolje; V Goffin"}, {"ref_id": "b743", "title": "Spoken Dialogue Technology: Enabling the Conversational User Interface", "journal": "ACM Computing Surveys", "year": "2002", "authors": "M F Mctear"}, {"ref_id": "b744", "title": "Intelligent Barge-in in Conversational Systems", "journal": "", "year": "2000", "authors": "N Str\u00f6m; S Seneff"}, {"ref_id": "b745", "title": "Recent progress of opensource LVCSR Engine Julius and Japanese model repository", "journal": "", "year": "2004", "authors": "T Kawahara; A Lee; K Takeda; K Itou; K Shikano"}, {"ref_id": "b746", "title": "Perceptual Dominance Time Distributions in Multistable Visual Perception", "journal": "Biological Cybernetics", "year": "2004", "authors": "Y Zhou; J Gao; K White; I Merk; K Yao"}, {"ref_id": "b747", "title": "Automatic Text Processing", "journal": "Addison-Wesley", "year": "1988", "authors": "G Salton"}, {"ref_id": "b748", "title": "Barge-in-able Robot Audition Based on ICA and Missing Feature Theory under Semi-Blind Situation", "journal": "", "year": "2008", "authors": "R Takeda; K Nakadai; K Komatani; T Ogata; H G Okuno"}, {"ref_id": "b749", "title": "CIAIR In-Car Speech Corpus -Influence of Driving Status", "journal": "IEICE Transactions on Information and Systems", "year": "2005", "authors": "N Kawaguchi; S Matsubara; K Takeda; F Itakura"}, {"ref_id": "b750", "title": "A new process based approach for implementing an integrated management system: Quality, security, environment", "journal": "", "year": "2009", "authors": "A Badreddine; T Ben Romdhane; N Ben Amor"}, {"ref_id": "b751", "title": "A Multi-objective Approach to Implement an Integrated Management System: Quality, Security, Environment", "journal": "", "year": "2009", "authors": "A Badreddine; T Ben Romdhane; N Ben Amor"}, {"ref_id": "b752", "title": "Approximating discrete probability distributions with dependence trees", "journal": "IEEE Transactions on Information Theory", "year": "1968", "authors": "C K Chow; C N Liu"}, {"ref_id": "b753", "title": "Introduction to Algorithms", "journal": "MIT Press", "year": "1990", "authors": "T H Cormen; C E Leiserson; R R Rivest"}, {"ref_id": "b754", "title": "Implementation of the new approach of risk analysis in france", "journal": "", "year": "2003", "authors": "J C Couronneau; A Tripathi"}, {"ref_id": "b755", "title": "Identification of reference accident scenarios in SEVESO establishments", "journal": "Reliability Engineering and System Safety", "year": "2005", "authors": "C Delvosallea; C Fieveza; A Piparta; F J Casal; E Planasb; M Christouc; F Mushtaqc"}, {"ref_id": "b756", "title": "Expert judgment study for placement ladder bowtie", "journal": "Safety Science", "year": "2008", "authors": "D Kurowicka; R Cooke; L Goossens; B Ale"}, {"ref_id": "b757", "title": "Learning with Mixtures of Trees", "journal": "Journal of Machine Learning Research", "year": "2000", "authors": "M Marina; I Michael"}, {"ref_id": "b758", "title": "Fuzzy logic for process safety analysis", "journal": "Journal of Loss Prevention in the Process Industries", "year": "2009", "authors": "S Markowski; M Sam Mannan; A Bigoszewska"}, {"ref_id": "b759", "title": "Fusion propagation and structuring in belief networks", "journal": "Artificial Intelligence", "year": "1986", "authors": "J Pearl"}, {"ref_id": "b760", "title": "The application of dynamic synapse neural networks on footstep and vehicle recognition", "journal": "", "year": "2007", "authors": "A A Dibazar; H O Park; T W Berger"}, {"ref_id": "b761", "title": "Detection, classification, and tracking of targets", "journal": "IEEE Signal Processing Magazine", "year": "2002", "authors": "D Li; K D Wong; Y H Hu; A M Sayeed"}, {"ref_id": "b762", "title": "Target detection and classification using seismic signal processing in unattended ground sensor systems", "journal": "", "year": "2002", "authors": "Y Tian; H Qi; X Wang"}, {"ref_id": "b763", "title": "Acoustic and seismic signal processing for footstep detection", "journal": "", "year": "2006", "authors": "R E Bland"}, {"ref_id": "b764", "title": "Footstep detection and tracking", "journal": "", "year": "2001", "authors": "G Succi; D Clapp; R Gampert; G Prado"}, {"ref_id": "b765", "title": "A data-driven personnel detection scheme for indoor surveillance using seismic sensors", "journal": "", "year": "2009", "authors": "A Subramanian; S G Iyengar; K G Mehrotra; C K Mohan; P K Varshney; T Damarla"}, {"ref_id": "b766", "title": "A review of human signatures in urban environments using seismic and acoustic methods", "journal": "", "year": "2008", "authors": "J M Sabatier; A E Ekimov"}, {"ref_id": "b767", "title": "On the detection of footsteps based on acoustic and seismic sensing", "journal": "", "year": "2007-07", "authors": "S G Iyengar; P K Varshney; T Damarla"}, {"ref_id": "b768", "title": "Spectrum analysis techniques for personnel detection using seismic sensors", "journal": "", "year": "2003", "authors": "K M Houston; D P Mcgaffigan"}, {"ref_id": "b769", "title": "The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis", "journal": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences", "year": "1971", "authors": "N E Huang; Z Shen; S R Long; M C Wu; H H Shih; Q Zheng; N C Yen; C C Tung; H H Liu"}, {"ref_id": "b770", "title": "Structural periodic measures for time-series data", "journal": "Data Mining and Knowledge Discovery", "year": "2006", "authors": "M Vlachos; P S Yu; V Castelli; C Meek"}, {"ref_id": "b771", "title": "Nonlinear Dimensionality Reduction", "journal": "Springer", "year": "2007", "authors": "J A Lee; M Verleysen"}, {"ref_id": "b772", "title": "Clustering with Bregman divergences", "journal": "J. Mach. Learn. Res", "year": "2005", "authors": "A Banerjee; S Merugu; I S Dhillon; J Ghosh"}, {"ref_id": "b773", "title": "Local Multidimensional Scaling for Nonlinear Dimension Reduction, Graph Drawing and Proximity Analysis", "journal": "", "year": "2006", "authors": "L Chen; A Buja"}, {"ref_id": "b774", "title": "Curvilinear component analysis: a self-organizing neural network for nonlinear mapping of data sets", "journal": "IEEE Transactions on Neural Networks", "year": "1997", "authors": "P Demartines; J Hrault"}, {"ref_id": "b775", "title": "A nonlinear mapping for data structure analysis", "journal": "IEEE Transactions on Computing", "year": "1969", "authors": "J Sammon"}, {"ref_id": "b776", "title": "Nonlinear dimensionality reduction", "journal": "", "year": "2007", "authors": "J A Lee; M Verleysen"}, {"ref_id": "b777", "title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "journal": "", "year": "2001", "authors": "K S Azoury; M K Warmouth"}, {"ref_id": "b778", "title": "Clustering with Bregman divergences", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "A Banerjee; S Meruga; I Dhillon; J Ghosh"}, {"ref_id": "b779", "title": "A generalization of principal component analysis to the exponential family", "journal": "", "year": "2002", "authors": "M Collins; S Dasgupta; R E Shapire"}, {"ref_id": "b780", "title": "Bregman Voronoi diagrams: Properties, algorithms and applications", "journal": "", "year": "2007", "authors": "F Neilsen; J.-D Boissonnat; R Nock"}, {"ref_id": "b781", "title": "On Bregman Voronoi diagrams", "journal": "", "year": "2007", "authors": "F Neilsen; J.-D Boissonnat; R Nock"}, {"ref_id": "b782", "title": "Blind source separation using temporal predictability", "journal": "Neural Computation", "year": "2001", "authors": "J V Stone"}, {"ref_id": "b783", "title": "Accelerometer Signal Processing for User Activity Detection", "journal": "Springer", "year": "2004", "authors": "J Baek; G Lee; W Park; B J Yun"}, {"ref_id": "b784", "title": "Activity Recognition from User-Annotated Acceleration Data", "journal": "Springer", "year": "2004", "authors": "L Bao; S S Intille"}, {"ref_id": "b785", "title": "Detection of Type, Duration, and Intensity of Physical Activity Using an Accelerometer", "journal": "Medicine & Science in Sports & Exercise", "year": "2009", "authors": "A G Bonomi; A H C Goris; B Yin; K R Westerterp"}, {"ref_id": "b786", "title": "Dance, Dance Evolution: Accelerometer Sensor Networks as Input to Video Games", "journal": "", "year": "2007", "authors": "N Crampton; K Fox; H Johnston; A Whitehead"}, {"ref_id": "b787", "title": "Detection of daily activities and sports with wearable sensors in controlled and uncontrolled conditions", "journal": "IEEE Trans. Inf. Technol. Biomed", "year": "2008", "authors": "M Ermes; J P\u00e4rkka; J Mantyjarvi; I Korhonen"}, {"ref_id": "b788", "title": "Feature Subset Selection Using the Wrapper Method: Overtting and Dynamic Search Space Topology", "journal": "", "year": "1995", "authors": "R Kohavi; D Sommereld"}, {"ref_id": "b789", "title": "Activity recognition using a wrist-worn inertial measurement unit: A case study for industrial assembly lines", "journal": "", "year": "2009", "authors": "H Koskimaki; V Huikari; P Siirtola; P Laurinen; J Roning"}, {"ref_id": "b790", "title": "Activity and location recognition using wearable sensors", "journal": "IEEE Pervasive Computing", "year": "2002", "authors": "S W Lee; K Mase"}, {"ref_id": "b791", "title": "Recognizing human motion with multiple acceleration sensors", "journal": "", "year": "2001", "authors": "J Mantyjarvi; J Himberg; T Seppanen"}, {"ref_id": "b792", "title": "Telerehabilitation in Employment/Community Supports Using Videobased", "journal": "", "year": "2008", "authors": "M Mccue1; J Hodgins; A Bargteil"}, {"ref_id": "b793", "title": "Real-Time Recognition of Physical Activities and Their Intensities Using Wireless Accelerometers and a Heart Rate Monitor", "journal": "", "year": "2007", "authors": "E Munguia; S S Intille; W Haskell; K Larson; J Wright; A King; R Friedman"}, {"ref_id": "b794", "title": "Activity Recognition from Accelerometer Data", "journal": "", "year": "2005", "authors": "N Ravi; N Dandekar; P Mysore; M L Littman"}, {"ref_id": "b795", "title": "Supervised feature selection via dependence estimation", "journal": "", "year": "2007", "authors": "L Song; A Smola; A Gretton; K M Borgwardt; J Bedo"}, {"ref_id": "b796", "title": "Non-monotonic feature selection", "journal": "", "year": "2009", "authors": "Z Xu; R Jin; J Ye; M R Lyu; I King"}, {"ref_id": "b797", "title": "Analyzing and Improving Clustering Based Sampling for Microprocessors", "journal": "Journal of High Performance Computing and Networking", "year": "2008", "authors": "Y Luo; A Joshi; A Phansalkar; L K John; J Ghosh"}, {"ref_id": "b798", "title": "Feature Selection for Slice Based Workload Characterization and Power Estimation", "journal": "", "year": "", "authors": "M Brock"}, {"ref_id": "b799", "title": "The simple genetic algorithm: Foundations and theory", "journal": "MIT Press", "year": "1999", "authors": "M D Vose"}, {"ref_id": "b800", "title": "A note on genetic algorithms for large-scale feature selection", "journal": "Pattern Recognition Letters", "year": "1989", "authors": "W Siedlecki; J Sklansky"}, {"ref_id": "b801", "title": "Comparison of algorithms that select features for pattern classifiers", "journal": "Pattern Recognition", "year": "2000", "authors": "M Kudo; J Sklansky"}, {"ref_id": "b802", "title": "Comparison of Classifier Specific Feature Selection Algorithms", "journal": "Springer", "year": "2000", "authors": "M Kudo; P Somol; P Pudil; M Shimbo; J Sklansky"}, {"ref_id": "b803", "title": "On the possible orderings in the measure-ment selection problem", "journal": "IEEE Transactions on Systems, Man and Cybernetics", "year": "1997", "authors": "T M Cover; J M Van Campenhout"}, {"ref_id": "b804", "title": "Gene selection for cancer classification using support vector machines", "journal": "Machine Learning", "year": "2002", "authors": "I J Guyon; S Weston; V Barnhill"}, {"ref_id": "b805", "title": "A stochastic algorithm for feature selection in pattern recognition", "journal": "Journal of Machine Learning Research", "year": "2007", "authors": "S Gadat; L Younes"}, {"ref_id": "b806", "title": "Feature selection using tabu search with long-term memories and probabilistic networks", "journal": "Pattern Recognition Letters", "year": "2009", "authors": "Y L Wang; J Li; S Ni; T Huang"}, {"ref_id": "b807", "title": "Feature Selection for Knowledge Discovery and Data Mining", "journal": "Kluwer Academic", "year": "1998", "authors": "H Liu"}, {"ref_id": "b808", "title": "Two Step Ant Colony System to Solve the Feature Selection Problem", "journal": "Springer", "year": "2006", "authors": "R A Bello; A Puris; Y Nowe; G M Martinez"}, {"ref_id": "b809", "title": "Feature selection in web applica-tions using ROC insertions and power set pruning", "journal": "", "year": "2001", "authors": "F M Coetzee; E Glover; S Lawrence; C Lee"}, {"ref_id": "b810", "title": "A branch and bound algorithm for feature subset selection", "journal": "IEEE Transactions on Computers", "year": "1977", "authors": "P M Narendra; K Fukunaga"}, {"ref_id": "b811", "title": "On the suboptimal solutions using the adaptive branch and bound algorithm for feature selection", "journal": "", "year": "2008", "authors": "S Nakariyakul"}, {"ref_id": "b812", "title": "Different meta-heuristic strategies to solve the feature selection problem. Pattern Recognition Letters", "journal": "", "year": "2009", "authors": "S C Yusta"}, {"ref_id": "b813", "title": "Feature selection based on rough sets and particle swarm optimization", "journal": "Pattern Recognition Letters", "year": "2007", "authors": "X J Wang; X Yang; W Teng; R Xia"}, {"ref_id": "b814", "title": "High-performance computer architecture", "journal": "Prentice-Hall", "year": "1993", "authors": "H S Stone"}, {"ref_id": "b815", "title": "An exponential time/space speedup for resolution", "journal": "Electronic Colloquium on Computational Complexity", "year": "2007", "authors": "P Hertel; T Pitassi"}, {"ref_id": "b816", "title": "Optimal time-space tradeoff in probabilistic inference", "journal": "", "year": "2003", "authors": "D Allen; A Darwiche"}, {"ref_id": "b817", "title": "Software caching vs. pre-fetching", "journal": "", "year": "2002", "authors": "A Aggarwal"}, {"ref_id": "b818", "title": "Caching in the TSP Search Space", "journal": "Springer", "year": "2009", "authors": "D Karhi; D E Tamir"}, {"ref_id": "b819", "title": "Cache Diversity in genetic algorithm Design", "journal": "", "year": "2000", "authors": "E E Santos; E Santos"}, {"ref_id": "b820", "title": "The Locality of Refer-ence of Genetic algorithms and Probabilistic Reasoning", "journal": "", "year": "2009", "authors": "D Lowell; B El Lababedi; C Novoa; D E Tamir"}, {"ref_id": "b821", "title": "A clustering technique for summarizing multivariate data", "journal": "Behavioral Science", "year": "1966", "authors": "J H Ball; D J Hall"}, {"ref_id": "b822", "title": "Application of graph theoretical parameters in quantifying molecular similarity and structure-activty studies", "journal": "J. Chem. Inf. Comput. Sci", "year": "1994", "authors": "S Basak; S Bertelsen; G Grunwald"}, {"ref_id": "b823", "title": "Partial auc estimation and regression", "journal": "Biometrics", "year": "2003", "authors": "L Dodd; M Pepe"}, {"ref_id": "b824", "title": "Support Vector Machines: Theory and Application", "journal": "Springer", "year": "2005", "authors": "L P Wang"}, {"ref_id": "b825", "title": "A comparative study using different topological representations in pattern recognition based drug activity characterization", "journal": "", "year": "2007", "authors": "F J Ferri; W D\u00edaz-Villanueva; M J Castro"}, {"ref_id": "b826", "title": "Experiments on automatic drug activity characterization using support vector classification", "journal": "", "year": "2006-11", "authors": "F J Ferri; W Diaz-Villanueva; M Castro"}, {"ref_id": "b827", "title": "Charge indexes. new topological descriptor", "journal": "J. Chem. Inf. and Comp. Sciences", "year": "1994", "authors": "J Galvez; R Garcia; M Salabert; R Soler"}, {"ref_id": "b828", "title": "Feature selection: Evaluation, application, and small sample performance", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "1997", "authors": "A Jain; D Zongker"}, {"ref_id": "b829", "title": "Support Vector machines, Neural Networks and Fuzzy Logic Models", "journal": "The MIT Press", "year": "2001", "authors": "V Kecman"}, {"ref_id": "b830", "title": "Molecular Connectivity in Structure-Activity Analysis", "journal": "John Willey and Sons", "year": "1986", "authors": "L B Kier; L H Hall"}, {"ref_id": "b831", "title": "Selection of relevant features in machine learning", "journal": "AAAI Press", "year": "1994", "authors": "P Langley"}, {"ref_id": "b832", "title": "Principled architecture selection for neural networks: Application to corporate bond rating prediction", "journal": "Morgan Kauffmann Publishers", "year": "1992", "authors": "J Moody; J Utans"}, {"ref_id": "b833", "title": "Artificial neural networks and linear discriminant analysis: A valuable combination in the selection of new antibacterial compounds", "journal": "J. Chem. Inf. and Comp. Sciences", "year": "2004", "authors": "M Murcia-Soler; F P\u00e9rez-Gim\u00e9nez; F Garc\u00eda-March; M Salabert-Salvador; W D\u00edaz-Villanueva; M Castro-Bleda; A Villanueva-Pareja"}, {"ref_id": "b834", "title": "Discrimination and selection of new potential antibacterial compounds using simple topological descriptors", "journal": "J. Mol. Graph. Model", "year": "2003", "authors": "M Murcia-Soler; F P\u00e9rez-Gim\u00e9nez; F Garc\u00eda-March; M Salabert-Salvador; W D\u00edaz-Villanueva; P Medina-Casamayor"}, {"ref_id": "b835", "title": "Molecular structure-propertiy relationships", "journal": "J. Chem. Educ", "year": "1987", "authors": "P Seybold; M May; U Bagal"}, {"ref_id": "b836", "title": "Selecting neural network architectures via the prediction risk: Application to corporate bond rating prediction", "journal": "IEEE Computer Society Press", "year": "1991", "authors": "J Utans; J E Moody"}, {"ref_id": "b837", "title": "A Survey of Methods for Scaling up Inductive Learning algorithms", "journal": "Data Mining and Knowledge Discovery", "year": "1999", "authors": "F J Provost; V Kolluri"}, {"ref_id": "b838", "title": "Database mining: A performance perspective", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "1993", "authors": "R Agrawal; T Imielinski; A Swami"}, {"ref_id": "b839", "title": "Selection of Relevant Features and Examples in Machine Learning", "journal": "Artificial Intelligence", "year": "1997", "authors": "A L Blum; P Langley"}, {"ref_id": "b840", "title": "Feature Extraction, Construction and Selection: A Mining Perspective", "journal": "Kluwer Academic Publishers", "year": "2001", "authors": "H Liu; H Motoda"}, {"ref_id": "b841", "title": "A Review of Feature Selection Techniques in Bioinformatics", "journal": "Bioinformatics", "year": "2007", "authors": "Y Saeys; I Inza; P Larranaga"}, {"ref_id": "b842", "title": "A Branch and Bound Algorithm for Feature Selection", "journal": "IEEE Transactions on Computers C", "year": "1977", "authors": "P M Narendra; K Fukunaga"}, {"ref_id": "b843", "title": "Feature Selection for Classification", "journal": "Intelligent Data Analysis", "year": "1997", "authors": "M Dash; H Liu"}, {"ref_id": "b844", "title": "Selection of Relevant Features and Examples in Machine Learning", "journal": "Artificial Intelligence", "year": "1997", "authors": "A L Blum; P Langley"}, {"ref_id": "b845", "title": "Feature Selection for Clustering -a Filter Solution", "journal": "", "year": "2002", "authors": "M Dash; K Choi; P Scheuermann; H Liu"}, {"ref_id": "b846", "title": "A Probabilistic Approach to Feature Selection -a Filter Solution", "journal": "", "year": "1996", "authors": "H Liu; R Setiono"}, {"ref_id": "b847", "title": "Feature Selection for High-dimensional Data: a Fast Correlation-based Filter Solution", "journal": "", "year": "2003", "authors": "L Yu; H Liu"}, {"ref_id": "b848", "title": "Greedy Attribute Selection", "journal": "", "year": "1994", "authors": "R Caruana; D Freitag"}, {"ref_id": "b849", "title": "Feature Subset Selection and Order Identification for Unsupervised Learning", "journal": "", "year": "2000", "authors": "J G Dy; C E Brodley"}, {"ref_id": "b850", "title": "Wrappers for Feature Subset Selection", "journal": "Artificial Intelligence", "year": "1997", "authors": "R Kohavi; G H John"}, {"ref_id": "b851", "title": "Filters, Wrappers, and a Boosting-based Hybrid for Feature Selection", "journal": "", "year": "2001", "authors": "S Das"}, {"ref_id": "b852", "title": "Filters, Wrappers, and a Boosting-based Hybrid for Feature Selection", "journal": "", "year": "2001", "authors": "S Das"}, {"ref_id": "b853", "title": "Feature Selection for High-dimensional Genomic Microarray Data", "journal": "Morgan Kaufmann", "year": "2001", "authors": "E P Xing; M I Jordan; R M Karp"}, {"ref_id": "b854", "title": "Training a 3-node Neural Networks is NP-complete", "journal": "Neural Networks", "year": "1992", "authors": "A L Blum; R L Rivest"}, {"ref_id": "b855", "title": "Boosting the Margin: A new Explanation for the Effectiveness of Voting Methods", "journal": "The Annals of Statistics", "year": "1998", "authors": "R E Schapire; Y Freund; P Bartlett; W S Lee"}, {"ref_id": "b856", "title": "Speeding up Relief Algorithm with K-d Trees", "journal": "", "year": "1998", "authors": "Robnik Sikonja; M "}, {"ref_id": "b857", "title": "Democratic Instance Selection: a Linear Complexity Instance Selection Algorithm Based on Classifier Ensemble Concepts", "journal": "Artificial Intelligence", "year": "", "authors": "C Garc\u00eda-Osorio; A De Haro-Garc\u00eda; N Garc\u00eda-Pedrajas"}, {"ref_id": "b858", "title": "On Issues of Instance Selection", "journal": "Data Mining and Knowledge Discovery", "year": "2002", "authors": "H Liu; H Motoda"}, {"ref_id": "b859", "title": "Reduction Techniques for Instance-based Learning Algorithms", "journal": "Machine Learning", "year": "2000", "authors": "D R Wilson; T R Martinez"}, {"ref_id": "b860", "title": "A Practical Approach to Feature Selection", "journal": "Morgan Kaufmann Publishers", "year": "1992", "authors": "K Kira; L A Rendell"}, {"ref_id": "b861", "title": "Estimating Attributes: Analysis and Extensions of Relief", "journal": "Springer", "year": "1994", "authors": "I Kononenko"}, {"ref_id": "b862", "title": "", "journal": "", "year": "", "authors": "Jorge I Badenas"}, {"ref_id": "b863", "title": "", "journal": "", "year": "", "authors": "Florian Bader"}, {"ref_id": "b864", "title": "", "journal": "", "year": "", "authors": " Badreddine"}, {"ref_id": "b865", "title": "", "journal": "", "year": "", "authors": " Baena-Garc\u00eda"}, {"ref_id": "b866", "title": "", "journal": "", "year": "", "authors": "Antonio Bahamonde"}, {"ref_id": "b867", "title": "", "journal": "", "year": "", "authors": "Javier Bajo"}, {"ref_id": "b868", "title": "", "journal": "", "year": "", "authors": " Balbi"}, {"ref_id": "b869", "title": "", "journal": "", "year": "", "authors": " Ballesteros-Ya\u00f1ez"}, {"ref_id": "b870", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Balthazar;  Manoel"}, {"ref_id": "b871", "title": "", "journal": "", "year": "", "authors": "Soumya Banerjee"}, {"ref_id": "b872", "title": "", "journal": "", "year": "", "authors": "Oresti Ba\u00f1os"}, {"ref_id": "b873", "title": "I-742, I-752", "journal": "", "year": "", "authors": "Federico Barber"}, {"ref_id": "b874", "title": "", "journal": "", "year": "", "authors": " Barranco-L\u00f3pez"}, {"ref_id": "b875", "title": "", "journal": "", "year": "", "authors": "Manuel J Barranco"}, {"ref_id": "b876", "title": "", "journal": "", "year": "", "authors": " Barrenechea"}, {"ref_id": "b877", "title": "", "journal": "", "year": "", "authors": "Francisco Barrientos"}, {"ref_id": "b878", "title": "", "journal": "", "year": "", "authors": " Batet"}, {"ref_id": "b879", "title": "", "journal": "", "year": "", "authors": "J Bautista"}, {"ref_id": "b880", "title": "", "journal": "", "year": "", "authors": "Nurdan Baykan;  Akhan"}, {"ref_id": "b881", "title": "", "journal": "", "year": "", "authors": " Bayoumi"}, {"ref_id": "b882", "title": "", "journal": "", "year": "", "authors": " Becourt"}, {"ref_id": "b883", "title": "", "journal": "", "year": "", "authors": "G Beliakov;  Iii-"}, {"ref_id": "b884", "title": "", "journal": "", "year": "", "authors": "Ben Amor"}, {"ref_id": "b885", "title": "", "journal": "", "year": "", "authors": " Benavides"}, {"ref_id": "b886", "title": "", "journal": "", "year": "", "authors": " Benavides-Piccione"}, {"ref_id": "b887", "title": "", "journal": "", "year": "", "authors": "Ben Hamza"}, {"ref_id": "b888", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 M Ben\u00edtez"}, {"ref_id": "b889", "title": "", "journal": "", "year": "", "authors": "Jamal Bentahar"}, {"ref_id": "b890", "title": "", "journal": "", "year": "", "authors": "Andrea Beoldo"}, {"ref_id": "b891", "title": "", "journal": "", "year": "", "authors": " Berger"}, {"ref_id": "b892", "title": "", "journal": "", "year": "", "authors": "Stian Berg"}, {"ref_id": "b893", "title": "", "journal": "", "year": "", "authors": "Rafael Berlanga"}, {"ref_id": "b894", "title": "", "journal": "", "year": "", "authors": " Bermejo"}, {"ref_id": "b895", "title": "", "journal": "", "year": "", "authors": "Hugo B\u00e9rub\u00e9"}, {"ref_id": "b896", "title": "", "journal": "", "year": "", "authors": "Unai Bidarte"}, {"ref_id": "b897", "title": "", "journal": "", "year": "", "authors": "Concha Bielza"}, {"ref_id": "b898", "title": "", "journal": "", "year": "", "authors": "Mauro I Birattari"}, {"ref_id": "b899", "title": "", "journal": "", "year": "", "authors": " Blagojevic"}, {"ref_id": "b900", "title": "", "journal": "", "year": "", "authors": "Olivier Boissier"}, {"ref_id": "b901", "title": "", "journal": "", "year": "", "authors": "Hanen I Borchani"}, {"ref_id": "b902", "title": "", "journal": "", "year": "", "authors": " Borrego-Jaraba"}, {"ref_id": "b903", "title": "", "journal": "", "year": "", "authors": "Leszek Borzemski"}, {"ref_id": "b904", "title": "", "journal": "", "year": "", "authors": "Tibor Bosse;  Ii"}, {"ref_id": "b905", "title": "", "journal": "", "year": "", "authors": " Boulila"}, {"ref_id": "b906", "title": "", "journal": "", "year": "", "authors": "Braden Box"}, {"ref_id": "b907", "title": "", "journal": "", "year": "", "authors": "Petr\u00f4nio L Braga"}, {"ref_id": "b908", "title": "", "journal": "", "year": "", "authors": "Mihaela Breaban;  Elena"}, {"ref_id": "b909", "title": "", "journal": "", "year": "", "authors": " Brighenti"}, {"ref_id": "b910", "title": "", "journal": "", "year": "", "authors": "Alberto I Bugar\u00edn"}, {"ref_id": "b911", "title": "", "journal": "", "year": "", "authors": "Mateo I Burillo"}, {"ref_id": "b912", "title": "", "journal": "", "year": "", "authors": "Ana Burusco"}, {"ref_id": "b913", "title": "", "journal": "", "year": "", "authors": "Humberto Bustince;  Iii"}, {"ref_id": "b914", "title": "", "journal": "", "year": "", "authors": "\u00c7 Ak\u0131r"}, {"ref_id": "b915", "title": "", "journal": "", "year": "", "authors": "\u00d3scar I Calvo"}, {"ref_id": "b916", "title": "", "journal": "", "year": "", "authors": "Canada Bago; J "}, {"ref_id": "b917", "title": "", "journal": "", "year": "", "authors": "Roberto Candela"}, {"ref_id": "b918", "title": "", "journal": "", "year": "", "authors": "Bruno Canizes"}, {"ref_id": "b919", "title": "", "journal": "", "year": "", "authors": "Ana Cara;  Bel\u00e9n"}, {"ref_id": "b920", "title": "", "journal": "", "year": "", "authors": "Javier Carbo"}, {"ref_id": "b921", "title": "", "journal": "", "year": "", "authors": "Francesco Carbone"}, {"ref_id": "b922", "title": "", "journal": "", "year": "", "authors": " Cardou"}, {"ref_id": "b923", "title": "", "journal": "", "year": "", "authors": " Carmona"}, {"ref_id": "b924", "title": "", "journal": "", "year": "", "authors": "Enrique J Carmona"}, {"ref_id": "b925", "title": "", "journal": "", "year": "", "authors": " Carmona-Poyato"}, {"ref_id": "b926", "title": "", "journal": "", "year": "", "authors": "St\u00e9phane Caro"}, {"ref_id": "b927", "title": "", "journal": "", "year": "", "authors": "Blanca Cases"}, {"ref_id": "b928", "title": "", "journal": "", "year": "", "authors": " Casta\u00f1o"}, {"ref_id": "b929", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Castillo;  Carlos"}, {"ref_id": "b930", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 M Castillo"}, {"ref_id": "b931", "title": "", "journal": "", "year": "", "authors": "Cerruela Garc\u00eda"}, {"ref_id": "b932", "title": "", "journal": "", "year": "", "authors": "Vicente Cerver\u00f3n"}, {"ref_id": "b933", "title": "", "journal": "", "year": "", "authors": "Amedeo I Cesta"}, {"ref_id": "b934", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 M Chaquet"}, {"ref_id": "b935", "title": "", "journal": "", "year": "", "authors": "F\u00e1bio Chavarette;  Roberto"}, {"ref_id": "b936", "title": "", "journal": "", "year": "", "authors": "Brenda I Cheang"}, {"ref_id": "b937", "title": "", "journal": "", "year": "", "authors": "Guan-Wei Chen"}, {"ref_id": "b938", "title": "", "journal": "", "year": "", "authors": "Li I Chen"}, {"ref_id": "b939", "title": "", "journal": "", "year": "", "authors": "Pei-Yu Chen"}, {"ref_id": "b940", "title": "", "journal": "", "year": "", "authors": "Wen-Pao Chen"}, {"ref_id": "b941", "title": "", "journal": "", "year": "", "authors": "Yi-Ting Chiang; I "}, {"ref_id": "b942", "title": "", "journal": "", "year": "", "authors": "M Chica"}, {"ref_id": "b943", "title": "", "journal": "", "year": "", "authors": "Hyukgeun I-651 Choi"}, {"ref_id": "b944", "title": "Micha l I-671", "journal": "", "year": "", "authors": " Chora\u015b"}, {"ref_id": "b945", "title": "", "journal": "", "year": "", "authors": "Kuo-Chung Chu"}, {"ref_id": "b946", "title": "", "journal": "", "year": "", "authors": "Lorenzo Ciardelli"}, {"ref_id": "b947", "title": "", "journal": "", "year": "", "authors": "Cristina D A Ciferri"}, {"ref_id": "b948", "title": "", "journal": "", "year": "", "authors": "Ricardo R Ciferri;  I-"}, {"ref_id": "b949", "title": "", "journal": "", "year": "", "authors": " Cleger-Tamayo"}, {"ref_id": "b950", "title": "", "journal": "", "year": "", "authors": " Climent"}, {"ref_id": "b951", "title": "", "journal": "", "year": "", "authors": "Llu\u00eds Codina"}, {"ref_id": "b952", "title": "", "journal": "", "year": "", "authors": "A G Cohn"}, {"ref_id": "b953", "title": "", "journal": "", "year": "", "authors": "Jes\u00fas Contreras"}, {"ref_id": "b954", "title": "", "journal": "", "year": "", "authors": "Emilio Corchado"}, {"ref_id": "b955", "title": "", "journal": "", "year": "", "authors": "Juan M Corchado;  I-407"}, {"ref_id": "b956", "title": "", "journal": "", "year": "", "authors": "O Cord\u00f3n"}, {"ref_id": "b957", "title": "", "journal": "", "year": "", "authors": " Cornelis"}, {"ref_id": "b958", "title": "", "journal": "", "year": "", "authors": "Roque Corral"}, {"ref_id": "b959", "title": "", "journal": "", "year": "", "authors": "Giulio Cottone"}, {"ref_id": "b960", "title": "", "journal": "", "year": "", "authors": "Pedro Couto"}, {"ref_id": "b961", "title": "", "journal": "", "year": "", "authors": "Ludivine Cr\u00e9pin"}, {"ref_id": "b962", "title": "", "journal": "", "year": "", "authors": "Matteo I Cristani"}, {"ref_id": "b963", "title": "", "journal": "", "year": "", "authors": "Malcolm Crowe;  Ii"}, {"ref_id": "b964", "title": "", "journal": "", "year": "", "authors": " Cruz-Ram\u00edrez"}, {"ref_id": "b965", "title": "", "journal": "", "year": "", "authors": "Leticia Curiel"}, {"ref_id": "b966", "title": "", "journal": "", "year": "", "authors": "Pascal Cuxac"}, {"ref_id": "b967", "title": "", "journal": "C\u00e9lia II", "year": "", "authors": "Costa Da;  Pereira"}, {"ref_id": "b968", "title": "", "journal": "", "year": "", "authors": "Thyagaraju Damarla;  Ii"}, {"ref_id": "b969", "title": "", "journal": "", "year": "", "authors": "S Damas"}, {"ref_id": "b970", "title": "", "journal": "", "year": "", "authors": "Alicia Iii-538 Da D'anjou; Marcus Vin\u00edcius Silva;  Carvalho"}, {"ref_id": "b971", "title": "", "journal": "", "year": "", "authors": "De As\u00eds"}, {"ref_id": "b972", "title": "", "journal": "", "year": "", "authors": "John I Debenham"}, {"ref_id": "b973", "title": "", "journal": "", "year": "", "authors": "Kaushik Deb"}, {"ref_id": "b974", "title": "", "journal": "", "year": "", "authors": "De Bock; W Koen"}, {"ref_id": "b975", "title": "", "journal": "", "year": "", "authors": "Juan De Castro;  Pablo"}, {"ref_id": "b976", "title": "", "journal": "", "year": "", "authors": "Javier Defelipe;  Iii"}, {"ref_id": "b977", "title": "", "journal": "", "year": "", "authors": " De Haro-Garc\u00eda"}, {"ref_id": "b978", "title": "", "journal": "", "year": "", "authors": "Jerker Delsing"}, {"ref_id": "b979", "title": "", "journal": "", "year": "", "authors": "De Maio"}, {"ref_id": "b980", "title": "", "journal": "", "year": "", "authors": "Yves Demazeau"}, {"ref_id": "b981", "title": "", "journal": "", "year": "", "authors": "Abdel Deneche;  Hakim"}, {"ref_id": "b982", "title": "", "journal": "", "year": "", "authors": " Depaire"}, {"ref_id": "b983", "title": "", "journal": "", "year": "", "authors": "De Paz; F Juan"}, {"ref_id": "b984", "title": "", "journal": "", "year": "", "authors": "Joaqu\u00edn I Derrac"}, {"ref_id": "b985", "title": "", "journal": "", "year": "", "authors": "Pedro De San"}, {"ref_id": "b986", "title": "", "journal": "", "year": "", "authors": "Luzia De Souza;  Vidal"}, {"ref_id": "b987", "title": "", "journal": "", "year": "", "authors": "Ana Dias;  Luiza"}, {"ref_id": "b988", "title": "", "journal": "", "year": "", "authors": " D\u00edaz-Villanueva"}, {"ref_id": "b989", "title": "", "journal": "", "year": "", "authors": "Mauro Dragoni"}, {"ref_id": "b990", "title": "", "journal": "", "year": "", "authors": "Maciej Drwal"}, {"ref_id": "b991", "title": "", "journal": "", "year": "", "authors": " Errasti-Alcal\u00e1"}, {"ref_id": "b992", "title": "", "journal": "", "year": "", "authors": "Marcelo I Errecalde"}, {"ref_id": "b993", "title": "", "journal": "", "year": "", "authors": " Escot-Bocanegra"}, {"ref_id": "b994", "title": "", "journal": "", "year": "", "authors": "Leila I Etaati"}, {"ref_id": "b995", "title": "", "journal": "", "year": "", "authors": "Usef Faghihi"}, {"ref_id": "b996", "title": "", "journal": "", "year": "", "authors": "Fazel Famili;  Iii"}, {"ref_id": "b997", "title": "", "journal": "", "year": "", "authors": "Luiz I Faria"}, {"ref_id": "b998", "title": "", "journal": "", "year": "", "authors": "Francois Fauteux"}, {"ref_id": "b999", "title": "", "journal": "", "year": "", "authors": "Jonathan Featherston"}, {"ref_id": "b1000", "title": "I-621, I-631, I-641", "journal": "", "year": "", "authors": "Alexander Felfernig"}, {"ref_id": "b1001", "title": "", "journal": "", "year": "", "authors": "Giuseppe Fenza"}, {"ref_id": "b1002", "title": "", "journal": "", "year": "", "authors": " Fern\u00e1ndez-Caballero"}, {"ref_id": "b1003", "title": "", "journal": "", "year": "", "authors": "Alba Fern\u00e1ndez De; M Jos\u00e9"}, {"ref_id": "b1004", "title": "", "journal": "", "year": "", "authors": "N L Fernandez-Garcia"}, {"ref_id": "b1005", "title": "", "journal": "", "year": "", "authors": "J Fernandez"}, {"ref_id": "b1006", "title": "", "journal": "", "year": "", "authors": "J C Fern\u00e1ndez"}, {"ref_id": "b1007", "title": "", "journal": "", "year": "", "authors": "Jaime Fern\u00e1ndez"}, {"ref_id": "b1008", "title": "", "journal": "", "year": "", "authors": "Laura Fern\u00e1ndez"}, {"ref_id": "b1009", "title": "", "journal": "", "year": "", "authors": "Juan M Fern\u00e1ndez-Luna"}, {"ref_id": "b1010", "title": "", "journal": "", "year": "", "authors": "F Fern\u00e1ndez-Navarro"}, {"ref_id": "b1011", "title": "", "journal": "J.A. II", "year": "", "authors": " Fern\u00e1ndez-Prieto"}, {"ref_id": "b1012", "title": "", "journal": "", "year": "", "authors": " Fern\u00e1ndez-Recio"}, {"ref_id": "b1013", "title": "", "journal": "", "year": "", "authors": " Fern\u00e1ndez-Robles"}, {"ref_id": "b1014", "title": "", "journal": "", "year": "", "authors": "J M Ferr\u00e1ndez"}, {"ref_id": "b1015", "title": "", "journal": "", "year": "", "authors": "Pere Ferrarons"}, {"ref_id": "b1016", "title": "", "journal": "", "year": "", "authors": "Rubem Ferreira;  Euz\u00e9bio"}, {"ref_id": "b1017", "title": "", "journal": "", "year": "", "authors": " Ferretti"}, {"ref_id": "b1018", "title": "", "journal": "", "year": "", "authors": "C\u00e8sar Ferri"}, {"ref_id": "b1019", "title": "", "journal": "", "year": "", "authors": "Francesc J Ferri"}, {"ref_id": "b1020", "title": "", "journal": "", "year": "", "authors": "Fileccia Scimemi; I I Giuseppe"}, {"ref_id": "b1021", "title": "", "journal": "Gabriel I-185", "year": "", "authors": " Fiol-Roig"}, {"ref_id": "b1022", "title": "", "journal": "", "year": "", "authors": " Flizikowski"}, {"ref_id": "b1023", "title": "", "journal": "", "year": "", "authors": "M Flores;  Julia I-"}, {"ref_id": "b1024", "title": "", "journal": "", "year": "", "authors": "Pierre Fobert"}, {"ref_id": "b1025", "title": "", "journal": "", "year": "", "authors": "Francisco Forcada;  Jos\u00e9"}, {"ref_id": "b1026", "title": "", "journal": "", "year": "", "authors": " Fouriner-Viger"}, {"ref_id": "b1027", "title": "", "journal": "", "year": "", "authors": "Leo I Franco"}, {"ref_id": "b1028", "title": "", "journal": "", "year": "", "authors": " Fuentes-Gonz\u00e1lez"}, {"ref_id": "b1029", "title": "", "journal": "", "year": "", "authors": "Hamido Fujita"}, {"ref_id": "b1030", "title": "", "journal": "", "year": "", "authors": "Li-Chen Fu; I "}, {"ref_id": "b1031", "title": "", "journal": "", "year": "", "authors": "Colin I Fyfe"}, {"ref_id": "b1032", "title": "", "journal": "", "year": "", "authors": " Gabrielli; I Nicoletta"}, {"ref_id": "b1033", "title": "", "journal": "", "year": "", "authors": "Mar\u00eda Gacto;  Jos\u00e9"}, {"ref_id": "b1034", "title": "", "journal": "", "year": "", "authors": "M A Gadeo-Martos"}, {"ref_id": "b1035", "title": "", "journal": "", "year": "", "authors": "Matteo Gaeta"}, {"ref_id": "b1036", "title": "", "journal": "", "year": "", "authors": "Mikel Galar"}, {"ref_id": "b1037", "title": "", "journal": "", "year": "", "authors": "Sylvie I Galichet"}, {"ref_id": "b1038", "title": "I-570, I-580", "journal": "", "year": "", "authors": "Jos\u00e9 A G\u00e1mez"}, {"ref_id": "b1039", "title": "", "journal": "", "year": "", "authors": "Juan G\u00e1mez;  Carlos"}, {"ref_id": "b1040", "title": "", "journal": "", "year": "", "authors": " Garc\u00eda-Fornes"}, {"ref_id": "b1041", "title": "", "journal": "", "year": "", "authors": "Francisco Garc\u00eda"}, {"ref_id": "b1042", "title": "", "journal": "", "year": "", "authors": " Garc\u00eda-Guti\u00e9rrez"}, {"ref_id": "b1043", "title": "", "journal": "", "year": "", "authors": " Garcia"}, {"ref_id": "b1044", "title": "", "journal": "", "year": "", "authors": "Jes\u00fas Garc\u00eda"}, {"ref_id": "b1045", "title": "", "journal": "", "year": "", "authors": "Garc\u00eda Jim\u00e9nez"}, {"ref_id": "b1046", "title": "", "journal": "", "year": "", "authors": "Juan Garc\u00eda"}, {"ref_id": "b1047", "title": "", "journal": "", "year": "", "authors": "\u00d3scar I Garc\u00eda"}, {"ref_id": "b1048", "title": "C\u00e9sar II-87", "journal": "", "year": "", "authors": " Garc\u00eda-Osorio"}, {"ref_id": "b1049", "title": "", "journal": "", "year": "", "authors": " Garc\u00eda-Pedrajas; D Mar\u00eda"}, {"ref_id": "b1050", "title": "", "journal": "Nicol\u00e1s I", "year": "", "authors": " Garc\u00eda-Pedrajas"}, {"ref_id": "b1051", "title": "", "journal": "", "year": "", "authors": " Garc\u00eda-Torres"}, {"ref_id": "b1052", "title": "", "journal": "", "year": "", "authors": "Vicente I Garc\u00eda"}, {"ref_id": "b1053", "title": "", "journal": "", "year": "", "authors": "Antonio I Garrido"}, {"ref_id": "b1054", "title": "", "journal": "", "year": "", "authors": "Luis E Garza-Casta\u00f1\u00f3n"}, {"ref_id": "b1055", "title": "", "journal": "", "year": "", "authors": "Charlotte Gerritsen"}, {"ref_id": "b1056", "title": "", "journal": "", "year": "", "authors": "Maha Ghribi"}, {"ref_id": "b1057", "title": "", "journal": "", "year": "", "authors": "Karina I Gibert"}, {"ref_id": "b1058", "title": "", "journal": "", "year": "", "authors": "Stefano I Giordani"}, {"ref_id": "b1059", "title": "", "journal": "", "year": "", "authors": " Goldsztejn"}, {"ref_id": "b1060", "title": "", "journal": "", "year": "", "authors": " G\u00f3mez-Luna"}, {"ref_id": "b1061", "title": "", "journal": "", "year": "", "authors": " G\u00f3mez-Nieto"}, {"ref_id": "b1062", "title": "", "journal": "", "year": "", "authors": "P G\u00f3mez"}, {"ref_id": "b1063", "title": "", "journal": "", "year": "", "authors": "Juan G\u00f3mez-Pulido;  Antonio"}, {"ref_id": "b1064", "title": "", "journal": "", "year": "", "authors": " G\u00f3mez-Villouta"}, {"ref_id": "b1065", "title": "", "journal": "", "year": "", "authors": " Gonz\u00e1lez-Abril; I I Luis"}, {"ref_id": "b1066", "title": "", "journal": "", "year": "", "authors": " Gonz\u00e1lez-Castro"}, {"ref_id": "b1067", "title": "", "journal": "", "year": "", "authors": " Goodman"}, {"ref_id": "b1068", "title": "", "journal": "", "year": "", "authors": "Manuel Gra\u00f1a"}, {"ref_id": "b1069", "title": "", "journal": "", "year": "", "authors": "Ole-Christoffer Granmo;  Iii"}, {"ref_id": "b1070", "title": "", "journal": "", "year": "", "authors": "J D Griffin"}, {"ref_id": "b1071", "title": "", "journal": "", "year": "", "authors": "John I Grundy"}, {"ref_id": "b1072", "title": "", "journal": "", "year": "", "authors": "Frederico Guedes"}, {"ref_id": "b1073", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Guerrero;  Luis"}, {"ref_id": "b1074", "title": "", "journal": "", "year": "", "authors": " Guihaire"}, {"ref_id": "b1075", "title": "", "journal": "", "year": "", "authors": "Ivan Guilherme;  Rizzo"}, {"ref_id": "b1076", "title": "", "journal": "", "year": "", "authors": " Guo"}, {"ref_id": "b1077", "title": "", "journal": "", "year": "", "authors": "P A Gutierrez"}, {"ref_id": "b1078", "title": "", "journal": "", "year": "", "authors": "R Guzm\u00e1n-Mart\u00ednez"}, {"ref_id": "b1079", "title": "", "journal": "", "year": "", "authors": "Tilmann H\u00e4berle"}, {"ref_id": "b1080", "title": "", "journal": "", "year": "", "authors": " Hakura"}, {"ref_id": "b1081", "title": "", "journal": "", "year": "", "authors": "Jean-Philippe Hamiez"}, {"ref_id": "b1082", "title": "", "journal": "", "year": "", "authors": "Jin-Kao Hao"}, {"ref_id": "b1083", "title": "", "journal": "", "year": "", "authors": "Yuki Hayashi"}, {"ref_id": "b1084", "title": "", "journal": "", "year": "", "authors": " Hejazi"}, {"ref_id": "b1085", "title": "", "journal": "", "year": "", "authors": "Carmen Hern\u00e1ndez"}, {"ref_id": "b1086", "title": "", "journal": "", "year": "", "authors": " Hern\u00e1ndez-Igue\u00f1o"}, {"ref_id": "b1087", "title": "", "journal": "", "year": "", "authors": "Josefa Z Hern\u00e1ndez"}, {"ref_id": "b1088", "title": "", "journal": "", "year": "", "authors": " Hern\u00e1ndez-Orallo"}, {"ref_id": "b1089", "title": "", "journal": "", "year": "", "authors": "Francisco I Herrera"}, {"ref_id": "b1090", "title": "Enrique III-429", "journal": "", "year": "", "authors": " Herrera-Viedma"}, {"ref_id": "b1091", "title": "", "journal": "", "year": "", "authors": "C Herv\u00e1s-Mart\u00ednez"}, {"ref_id": "b1092", "title": "", "journal": "", "year": "", "authors": "Wataru Hinoshita"}, {"ref_id": "b1093", "title": "", "journal": "", "year": "", "authors": "Norifumi Hirata"}, {"ref_id": "b1094", "title": "", "journal": "", "year": "", "authors": "Ali Hmer"}, {"ref_id": "b1095", "title": "", "journal": "", "year": "", "authors": "Martin Hofmann"}, {"ref_id": "b1096", "title": "Ho lubowicz, Witold I-671", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1097", "title": "", "journal": "", "year": "", "authors": " Hoonakker"}, {"ref_id": "b1098", "title": "", "journal": "", "year": "", "authors": "Wen-Juan Hou; I "}, {"ref_id": "b1099", "title": "", "journal": "", "year": "", "authors": "Jane Hsu;  Yung"}, {"ref_id": "b1100", "title": "", "journal": "", "year": "", "authors": "Kuo-Chung Hsu; I "}, {"ref_id": "b1101", "title": "", "journal": "", "year": "", "authors": "Zhihu Huang"}, {"ref_id": "b1102", "title": "", "journal": "", "year": "", "authors": "Juan F Huete"}, {"ref_id": "b1103", "title": "", "journal": "", "year": "", "authors": "Mario Iba\u00f1ez"}, {"ref_id": "b1104", "title": "", "journal": "", "year": "", "authors": "A Ibarguren"}, {"ref_id": "b1105", "title": "", "journal": "", "year": "", "authors": "Ryutaro Ichise"}, {"ref_id": "b1106", "title": "", "journal": "", "year": "", "authors": "Hazra Imran"}, {"ref_id": "b1107", "title": "", "journal": "", "year": "", "authors": " Imura"}, {"ref_id": "b1108", "title": "", "journal": "", "year": "", "authors": "G\u00f6khan Ince"}, {"ref_id": "b1109", "title": "Andreu Pere I-185", "journal": "", "year": "", "authors": " Isern-Dey\u00e0"}, {"ref_id": "b1110", "title": "", "journal": "", "year": "", "authors": "Satoshi I Ishibashi"}, {"ref_id": "b1111", "title": "", "journal": "", "year": "", "authors": "Katsutoshi Itoyama"}, {"ref_id": "b1112", "title": "", "journal": "", "year": "", "authors": "Guido I Izuta"}, {"ref_id": "b1113", "title": "", "journal": "", "year": "", "authors": " Jacquenet"}, {"ref_id": "b1114", "title": "", "journal": "", "year": "", "authors": " Jaime-Castillo"}, {"ref_id": "b1115", "title": "", "journal": "", "year": "", "authors": "Prem Jayaraman;  Prakash"}, {"ref_id": "b1116", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Jerez;  Manuel"}, {"ref_id": "b1117", "title": "Jos\u00e9 del I-205", "journal": "", "year": "", "authors": "Mar\u00eda Jes\u00fas"}, {"ref_id": "b1118", "title": "", "journal": "", "year": "", "authors": "Esther Jim\u00e9nez"}, {"ref_id": "b1119", "title": "", "journal": "", "year": "", "authors": " Jo;  Kang-Hyun"}, {"ref_id": "b1120", "title": "", "journal": "", "year": "", "authors": "Vicente Juli\u00e1n"}, {"ref_id": "b1121", "title": "", "journal": "", "year": "", "authors": "Jason J Jung"}, {"ref_id": "b1122", "title": "", "journal": "", "year": "", "authors": " Jurado-Lucena"}, {"ref_id": "b1123", "title": "", "journal": "", "year": "", "authors": " Jurio"}, {"ref_id": "b1124", "title": "", "journal": "", "year": "", "authors": " Kantardzic"}, {"ref_id": "b1125", "title": "", "journal": "", "year": "", "authors": "Hussein M Khodr;  I-"}, {"ref_id": "b1126", "title": "", "journal": "", "year": "", "authors": " Kim"}, {"ref_id": "b1127", "title": "", "journal": "", "year": "", "authors": "Michel C A Klein"}, {"ref_id": "b1128", "title": "", "journal": "", "year": "", "authors": "Alexander Klippel"}, {"ref_id": "b1129", "title": "", "journal": "", "year": "", "authors": "Jia-Ling Koh"}, {"ref_id": "b1130", "title": "", "journal": "", "year": "", "authors": "Tomoko Kojiri"}, {"ref_id": "b1131", "title": "", "journal": "", "year": "", "authors": "Kazunori I Komatani"}, {"ref_id": "b1132", "title": "", "journal": "", "year": "", "authors": " Kozielski"}, {"ref_id": "b1133", "title": "Rafa l I-671", "journal": "", "year": "", "authors": " Kozik"}, {"ref_id": "b1134", "title": "", "journal": "", "year": "", "authors": " Krygowski"}, {"ref_id": "b1135", "title": "", "journal": "", "year": "", "authors": "Borai Kumova"}, {"ref_id": "b1136", "title": "", "journal": "", "year": "", "authors": "Jian-Long Kuo"}, {"ref_id": "b1137", "title": "", "journal": "", "year": "", "authors": "Masaki Kurematsu"}, {"ref_id": "b1138", "title": "", "journal": "", "year": "", "authors": " Kybartas"}, {"ref_id": "b1139", "title": "", "journal": "", "year": "", "authors": " Lachiche"}, {"ref_id": "b1140", "title": "", "journal": "", "year": "", "authors": "Manuel I Lama"}, {"ref_id": "b1141", "title": "", "journal": "", "year": "", "authors": "Jean-Charles Lamirel"}, {"ref_id": "b1142", "title": "", "journal": "", "year": "", "authors": "Pedro Larra\u00f1aga"}, {"ref_id": "b1143", "title": "", "journal": "", "year": "", "authors": "Elaine Lawrence;  Ii"}, {"ref_id": "b1144", "title": "", "journal": "", "year": "", "authors": "Maciej Lawry\u0144czuk"}, {"ref_id": "b1145", "title": "", "journal": "", "year": "", "authors": " Layeb"}, {"ref_id": "b1146", "title": "", "journal": "", "year": "", "authors": "Agapito Ledezma"}, {"ref_id": "b1147", "title": "", "journal": "", "year": "", "authors": "G Leguizam\u00f3n"}, {"ref_id": "b1148", "title": "", "journal": "", "year": "", "authors": "Jinsong Leng"}, {"ref_id": "b1149", "title": "", "journal": "", "year": "", "authors": "Mark Levin;  Sh"}, {"ref_id": "b1150", "title": "", "journal": "", "year": "", "authors": "Dongguang Li"}, {"ref_id": "b1151", "title": "", "journal": "", "year": "", "authors": "Andrew Lim"}, {"ref_id": "b1152", "title": "", "journal": "", "year": "", "authors": "Giorgio O Limeira"}, {"ref_id": "b1153", "title": "", "journal": "", "year": "", "authors": "Frank Lin;  Yeong"}, {"ref_id": "b1154", "title": "", "journal": "", "year": "", "authors": "Gu-Yang Lin; I "}, {"ref_id": "b1155", "title": "", "journal": "", "year": "", "authors": "Sheng-Yang Li; I "}, {"ref_id": "b1156", "title": "", "journal": "", "year": "", "authors": "Ziying Liu;  Iii"}, {"ref_id": "b1157", "title": "", "journal": "", "year": "", "authors": "Vincenzo Loia"}, {"ref_id": "b1158", "title": "", "journal": "", "year": "", "authors": "Leonardo O Lombardi"}, {"ref_id": "b1159", "title": "", "journal": "", "year": "", "authors": " L\u00f3pez-Molina"}, {"ref_id": "b1160", "title": "", "journal": "", "year": "", "authors": "Daniel Lowell"}, {"ref_id": "b1161", "title": "", "journal": "", "year": "", "authors": " Lozano-Tello"}, {"ref_id": "b1162", "title": "", "journal": "", "year": "", "authors": "Oscar Luaces"}, {"ref_id": "b1163", "title": "", "journal": "", "year": "", "authors": "Ching - Lu; I Hu"}, {"ref_id": "b1164", "title": "", "journal": "", "year": "", "authors": "Marin I Lujak"}, {"ref_id": "b1165", "title": "", "journal": "", "year": "", "authors": " Luna-Rodr\u00edguez"}, {"ref_id": "b1166", "title": "", "journal": "", "year": "", "authors": " Luong; N Hoang"}, {"ref_id": "b1167", "title": "", "journal": "", "year": "", "authors": "Luque Ruiz"}, {"ref_id": "b1168", "title": "", "journal": "", "year": "", "authors": "K S Machado"}, {"ref_id": "b1169", "title": "", "journal": "", "year": "", "authors": "F J Madrid-Cuevas"}, {"ref_id": "b1170", "title": "", "journal": "", "year": "", "authors": "Akira Maezawa"}, {"ref_id": "b1171", "title": "", "journal": "", "year": "", "authors": "Prabhat K Mahanti"}, {"ref_id": "b1172", "title": "", "journal": "", "year": "", "authors": "Ahmad I Makui"}, {"ref_id": "b1173", "title": "", "journal": "", "year": "", "authors": " Ma Lysiak-Mrozek"}, {"ref_id": "b1174", "title": "Mandl, Monika I-621, I-631, I-641", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1175", "title": "", "journal": "", "year": "", "authors": " Markowska-Kaczmar"}, {"ref_id": "b1176", "title": "", "journal": "", "year": "", "authors": "Albino I Marques"}, {"ref_id": "b1177", "title": "", "journal": "", "year": "", "authors": " Mart\u00edn-D\u00edaz"}, {"ref_id": "b1178", "title": "", "journal": "", "year": "", "authors": "Francesco I Martinelli"}, {"ref_id": "b1179", "title": "", "journal": "", "year": "", "authors": " Mart\u00ednez-\u00c1lvarez"}, {"ref_id": "b1180", "title": "", "journal": "", "year": "", "authors": "Ana M Mart\u00ednez"}, {"ref_id": "b1181", "title": "", "journal": "", "year": "", "authors": " Mart\u00ednez-Jim\u00e9nez"}, {"ref_id": "b1182", "title": "", "journal": "", "year": "", "authors": "Luis Mart\u00ednez"}, {"ref_id": "b1183", "title": "", "journal": "", "year": "", "authors": "Mart\u00ednez Madrid"}, {"ref_id": "b1184", "title": "", "journal": "", "year": "", "authors": " Martinez-Marchena"}, {"ref_id": "b1185", "title": "", "journal": "", "year": "", "authors": "J M Mart\u00ednez-Otzeta"}, {"ref_id": "b1186", "title": "", "journal": "", "year": "", "authors": " Martin"}, {"ref_id": "b1187", "title": "", "journal": "", "year": "", "authors": "Jaime Mart\u00edn"}, {"ref_id": "b1188", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Mart\u00edn;  Luis"}, {"ref_id": "b1189", "title": "", "journal": "", "year": "", "authors": "Jerzy Martyna"}, {"ref_id": "b1190", "title": "", "journal": "", "year": "", "authors": "Piotr M Marusak"}, {"ref_id": "b1191", "title": "", "journal": "", "year": "", "authors": "Tshilidzi Marwala"}, {"ref_id": "b1192", "title": "", "journal": "", "year": "", "authors": "Pablo F Matos"}, {"ref_id": "b1193", "title": "", "journal": "", "year": "", "authors": "Kyoko Matsuyama"}, {"ref_id": "b1194", "title": "", "journal": "", "year": "", "authors": " Matthews"}, {"ref_id": "b1195", "title": "", "journal": "", "year": "", "authors": "Jes\u00fas Maudes;  Ii"}, {"ref_id": "b1196", "title": "", "journal": "", "year": "", "authors": "L M Mazaira"}, {"ref_id": "b1197", "title": "", "journal": "", "year": "", "authors": "R I Mckay"}, {"ref_id": "b1198", "title": "", "journal": "", "year": "", "authors": "Mckenzie "}, {"ref_id": "b1199", "title": "", "journal": "", "year": "", "authors": "R Medina-Carnicer"}, {"ref_id": "b1200", "title": "", "journal": "", "year": "", "authors": "Juan Medina;  Miguel"}, {"ref_id": "b1201", "title": "", "journal": "", "year": "", "authors": " Meger"}, {"ref_id": "b1202", "title": "", "journal": "", "year": "", "authors": "Kishan G Mehrotra;  Ii"}, {"ref_id": "b1203", "title": "", "journal": "", "year": "", "authors": " Melo-Pinto"}, {"ref_id": "b1204", "title": "", "journal": "", "year": "", "authors": " Men\u00e9ndez-Mora"}, {"ref_id": "b1205", "title": "", "journal": "", "year": "", "authors": "H\u00e9lio B Menezes"}, {"ref_id": "b1206", "title": "", "journal": "", "year": "", "authors": " Merch\u00e1n-P\u00e9rez"}, {"ref_id": "b1207", "title": "", "journal": "", "year": "", "authors": " Meshoul"}, {"ref_id": "b1208", "title": "", "journal": "", "year": "", "authors": "R Mesiar;  Iii-"}, {"ref_id": "b1209", "title": "", "journal": "", "year": "", "authors": "Giorgio I Metta"}, {"ref_id": "b1210", "title": "", "journal": "", "year": "", "authors": " Mir\u00f3-Juli\u00e0"}, {"ref_id": "b1211", "title": "", "journal": "", "year": "", "authors": "Takeshi I Mizumoto"}, {"ref_id": "b1212", "title": "", "journal": "", "year": "", "authors": "Yasser Mohammad"}, {"ref_id": "b1213", "title": "", "journal": "", "year": "", "authors": "Chilukuri K Mohan;  Ii"}, {"ref_id": "b1214", "title": "Jos\u00e9 Manuel I-397, III-470", "journal": "", "year": "", "authors": " Molina"}, {"ref_id": "b1215", "title": "", "journal": "", "year": "", "authors": "Ram\u00f3n A Mollineda"}, {"ref_id": "b1216", "title": "", "journal": "", "year": "", "authors": "Miguel\u00e1 Montero"}, {"ref_id": "b1217", "title": "", "journal": "", "year": "", "authors": "Montiel S\u00e1nchez"}, {"ref_id": "b1218", "title": "", "journal": "", "year": "", "authors": "Juan Mora"}, {"ref_id": "b1219", "title": "", "journal": "", "year": "", "authors": " Morales-Bueno"}, {"ref_id": "b1220", "title": "", "journal": "", "year": "", "authors": "Juan Morales"}, {"ref_id": "b1221", "title": "Llanos III-596", "journal": "", "year": "", "authors": " Mora-Lopez"}, {"ref_id": "b1222", "title": "Anabela II-153", "journal": "", "year": "", "authors": "Moreira Bernardino"}, {"ref_id": "b1223", "title": "Eug\u00e9nia II-153", "journal": "", "year": "", "authors": "Moreira Bernardino"}, {"ref_id": "b1224", "title": "", "journal": "", "year": "", "authors": " Moreno-Mu\u00f1oz"}, {"ref_id": "b1225", "title": "", "journal": "", "year": "", "authors": "Q Moro-Sancho;  Isaac"}, {"ref_id": "b1226", "title": "", "journal": "", "year": "", "authors": "Ros Motto"}, {"ref_id": "b1227", "title": "", "journal": "", "year": "", "authors": "Malek Mouhoub"}, {"ref_id": "b1228", "title": "", "journal": "", "year": "", "authors": "Luiza Mourelle; I I De Macedo"}, {"ref_id": "b1229", "title": "", "journal": "", "year": "", "authors": "Dariusz Mrozek;  Iii"}, {"ref_id": "b1230", "title": "", "journal": "", "year": "", "authors": "Manuel I Mucientes"}, {"ref_id": "b1231", "title": "", "journal": "", "year": "", "authors": "C Mu\u00f1oz"}, {"ref_id": "b1232", "title": "", "journal": "", "year": "", "authors": "Pablo Mu\u00f1oz"}, {"ref_id": "b1233", "title": "", "journal": "", "year": "", "authors": "R Mu\u00f1oz-Salinas"}, {"ref_id": "b1234", "title": "Kazuhiro I-51, I-62, I-102", "journal": "", "year": "", "authors": " Nakadai"}, {"ref_id": "b1235", "title": "", "journal": "", "year": "", "authors": "Masato Nakamura"}, {"ref_id": "b1236", "title": "", "journal": "", "year": "", "authors": "Lorenzo I Natale"}, {"ref_id": "b1237", "title": "", "journal": "", "year": "", "authors": "Karla Navarro;  Felix"}, {"ref_id": "b1238", "title": "", "journal": "", "year": "", "authors": "Mart\u00ed Navarro"}, {"ref_id": "b1239", "title": "", "journal": "", "year": "", "authors": "Victoria Nebot"}, {"ref_id": "b1240", "title": "", "journal": "", "year": "", "authors": "Nadia Nedjah;  Ii"}, {"ref_id": "b1241", "title": "", "journal": "", "year": "", "authors": " Neves-Jr; Iii Fl\u00e1vio"}, {"ref_id": "b1242", "title": "", "journal": "", "year": "", "authors": "Hai T T Nguyen"}, {"ref_id": "b1243", "title": "", "journal": "", "year": "", "authors": "Guillermo Nicolau"}, {"ref_id": "b1244", "title": "", "journal": "", "year": "", "authors": " Nishida"}, {"ref_id": "b1245", "title": "", "journal": "", "year": "", "authors": "Roger Nkambou"}, {"ref_id": "b1246", "title": "", "journal": "", "year": "", "authors": "Clara Novoa"}, {"ref_id": "b1247", "title": "", "journal": "", "year": "", "authors": "Jorge I Ocon"}, {"ref_id": "b1248", "title": "", "journal": "Tetsuya I", "year": "", "authors": " Ogata"}, {"ref_id": "b1249", "title": "", "journal": "", "year": "", "authors": "Shogo I Okada"}, {"ref_id": "b1250", "title": "I-51, I-102, II-585", "journal": "", "year": "", "authors": "Hiroshi G Okuno"}, {"ref_id": "b1251", "title": "", "journal": "", "year": "", "authors": "Joaqu\u00edn Olivares;  Iii"}, {"ref_id": "b1252", "title": "", "journal": "", "year": "", "authors": "Jose A Olivas"}, {"ref_id": "b1253", "title": "Fernando Agust\u00edn I-458", "journal": "", "year": "", "authors": "Olivencia Polo"}, {"ref_id": "b1254", "title": "", "journal": "", "year": "", "authors": "Eva I Onaindia"}, {"ref_id": "b1255", "title": "", "journal": "", "year": "", "authors": "B Oommen;  John"}, {"ref_id": "b1256", "title": "", "journal": "", "year": "", "authors": "Francesco Orciuoli"}, {"ref_id": "b1257", "title": "", "journal": "", "year": "", "authors": "Juan Ortega;  Antonio"}, {"ref_id": "b1258", "title": "", "journal": "", "year": "", "authors": " Ortiz-Boyer"}, {"ref_id": "b1259", "title": "", "journal": "", "year": "", "authors": "Takuma I Otsuka"}, {"ref_id": "b1260", "title": "", "journal": "", "year": "", "authors": "Tadachika Ozono;  Ii"}, {"ref_id": "b1261", "title": "", "journal": "", "year": "", "authors": "Miguel Pagola;  Iii"}, {"ref_id": "b1262", "title": "", "journal": "", "year": "", "authors": "Rafael Palacios"}, {"ref_id": "b1263", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 M Palomares"}, {"ref_id": "b1264", "title": "", "journal": "", "year": "", "authors": "Rafael Palomar"}, {"ref_id": "b1265", "title": "", "journal": "", "year": "", "authors": "D Pandolfi"}, {"ref_id": "b1266", "title": "", "journal": "Pan", "year": "", "authors": ""}, {"ref_id": "b1267", "title": "", "journal": "", "year": "", "authors": "Carlos Pardo;  Ii"}, {"ref_id": "b1268", "title": "", "journal": "", "year": "", "authors": "Thiago A S Pardo"}, {"ref_id": "b1269", "title": "", "journal": "", "year": "", "authors": "Eros Pasero"}, {"ref_id": "b1270", "title": "", "journal": "", "year": "", "authors": "Francesco Pasini"}, {"ref_id": "b1271", "title": "", "journal": "", "year": "", "authors": "D Paternain;  Iii-"}, {"ref_id": "b1272", "title": "", "journal": "", "year": "", "authors": "Shashank I Pathak"}, {"ref_id": "b1273", "title": "", "journal": "", "year": "", "authors": "Miguel\u00e1 Patricio"}, {"ref_id": "b1274", "title": "", "journal": "", "year": "", "authors": "Juan I Pav\u00f3n"}, {"ref_id": "b1275", "title": "", "journal": "", "year": "", "authors": " Pedraza-Jimenez"}, {"ref_id": "b1276", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9-Mar\u00eda Pe\u00f1a"}, {"ref_id": "b1277", "title": "", "journal": "", "year": "", "authors": " P\u00e9rez-Godoy"}, {"ref_id": "b1278", "title": "", "journal": "", "year": "", "authors": "Meir Perez"}, {"ref_id": "b1279", "title": "", "journal": "", "year": "", "authors": " P\u00e9rez-V\u00e1zquez"}, {"ref_id": "b1280", "title": "", "journal": "", "year": "", "authors": "Maxim V Petukhov"}, {"ref_id": "b1281", "title": "", "journal": "", "year": "", "authors": "Sieu Phan;  Iii"}, {"ref_id": "b1282", "title": "", "journal": "", "year": "", "authors": "Michel Piliougine"}, {"ref_id": "b1283", "title": "", "journal": "", "year": "", "authors": " Plimmer"}, {"ref_id": "b1284", "title": "", "journal": "", "year": "", "authors": "Pierre Poirier"}, {"ref_id": "b1285", "title": "", "journal": "", "year": "", "authors": "H\u00e9ctor I Pomares"}, {"ref_id": "b1286", "title": "", "journal": "J.A", "year": "", "authors": " Portilla-Figueras"}, {"ref_id": "b1287", "title": "", "journal": "", "year": "", "authors": "W D Potter"}, {"ref_id": "b1288", "title": "", "journal": "", "year": "", "authors": " Poyatos-Mart\u00ednez"}, {"ref_id": "b1289", "title": "", "journal": "", "year": "", "authors": "J C Prados"}, {"ref_id": "b1290", "title": "", "journal": "", "year": "", "authors": "A Prieto"}, {"ref_id": "b1291", "title": "", "journal": "", "year": "", "authors": "B Prieto"}, {"ref_id": "b1292", "title": "", "journal": "", "year": "", "authors": "\u00d3scar J Prieto"}, {"ref_id": "b1293", "title": "", "journal": "", "year": "", "authors": " Prodan"}, {"ref_id": "b1294", "title": "", "journal": "", "year": "", "authors": "Michael I Provost"}, {"ref_id": "b1295", "title": "I-570, I-580", "journal": "", "year": "", "authors": "Jos\u00e9 M Puerta"}, {"ref_id": "b1296", "title": "", "journal": "", "year": "", "authors": "Belarmino Pulido"}, {"ref_id": "b1297", "title": "", "journal": "", "year": "", "authors": "Luca I Pulina"}, {"ref_id": "b1298", "title": "", "journal": "", "year": "", "authors": "Hu Qin"}, {"ref_id": "b1299", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 R Quevedo"}, {"ref_id": "b1300", "title": "", "journal": "", "year": "", "authors": " Ram\u00edrez-Quintana"}, {"ref_id": "b1301", "title": "", "journal": "", "year": "", "authors": "Carlos Ramos"}, {"ref_id": "b1302", "title": "", "journal": "", "year": "", "authors": " Ramos-Mu\u00f1oz"}, {"ref_id": "b1303", "title": "", "journal": "", "year": "", "authors": "Riccardo I Rasconi"}, {"ref_id": "b1304", "title": "", "journal": "", "year": "", "authors": "Sarunas Raudys"}, {"ref_id": "b1305", "title": "", "journal": "", "year": "", "authors": "Carlo Regazzoni"}, {"ref_id": "b1306", "title": "", "journal": "", "year": "", "authors": "Luisa M Regueras"}, {"ref_id": "b1307", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 C Riquelme"}, {"ref_id": "b1308", "title": "", "journal": "", "year": "", "authors": "Riva Sanseverino"}, {"ref_id": "b1309", "title": "", "journal": "", "year": "", "authors": "Antonio Rivera;  Jes\u00fas"}, {"ref_id": "b1310", "title": "", "journal": "", "year": "", "authors": " R-Moreno; D Mar\u00eda"}, {"ref_id": "b1311", "title": "", "journal": "", "year": "", "authors": "V Rodellar"}, {"ref_id": "b1312", "title": "", "journal": "", "year": "", "authors": "Tobias Rodemann"}, {"ref_id": "b1313", "title": "", "journal": "", "year": "", "authors": "Luiz Rodrigues; A Henrique"}, {"ref_id": "b1314", "title": "", "journal": "", "year": "", "authors": " Rodr\u00edguez"}, {"ref_id": "b1315", "title": "", "journal": "", "year": "", "authors": "Rodr\u00edguez Cano; Julio C "}, {"ref_id": "b1316", "title": "", "journal": "", "year": "", "authors": " Rodr\u00edguez"}, {"ref_id": "b1317", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9-Rodrigo Rodr\u00edguez"}, {"ref_id": "b1318", "title": "", "journal": "", "year": "", "authors": "Juan Rodr\u00edguez;  Jos\u00e9"}, {"ref_id": "b1319", "title": "", "journal": "", "year": "", "authors": " Rodr\u00edguez-Molins"}, {"ref_id": "b1320", "title": "", "journal": "", "year": "", "authors": "Sara Rodr\u00edguez"}, {"ref_id": "b1321", "title": "", "journal": "", "year": "", "authors": "Ignacio I Rojas"}, {"ref_id": "b1322", "title": "", "journal": "", "year": "", "authors": "Francisco P Romero"}, {"ref_id": "b1323", "title": "", "journal": "", "year": "", "authors": "Paolo Rosso"}, {"ref_id": "b1324", "title": "", "journal": "", "year": "", "authors": " Rovira"}, {"ref_id": "b1325", "title": "", "journal": "", "year": "", "authors": "David M Rubin"}, {"ref_id": "b1326", "title": "", "journal": "", "year": "", "authors": "D D Ruiz"}, {"ref_id": "b1327", "title": "", "journal": "", "year": "", "authors": " Ruiz-Morilla"}, {"ref_id": "b1328", "title": "", "journal": "", "year": "", "authors": "Roberto I Ru\u00edz"}, {"ref_id": "b1329", "title": "", "journal": "", "year": "", "authors": "Joung Ryu;  Woo"}, {"ref_id": "b1330", "title": "", "journal": "", "year": "", "authors": " Sadi-Nezhad"}, {"ref_id": "b1331", "title": "", "journal": "", "year": "", "authors": "Gregorio Sainz"}, {"ref_id": "b1332", "title": "", "journal": "", "year": "", "authors": " Salcedo-Sanz"}, {"ref_id": "b1333", "title": "", "journal": "", "year": "", "authors": " Salehi-Abari"}, {"ref_id": "b1334", "title": "I-742, I-752", "journal": "", "year": "", "authors": "Miguel\u00e1 Salido"}, {"ref_id": "b1335", "title": "", "journal": "", "year": "", "authors": " S\u00e1nchez-Anguix"}, {"ref_id": "b1336", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 S\u00e1nchez;  Salvador"}, {"ref_id": "b1337", "title": "", "journal": "", "year": "", "authors": " S\u00e1nchez-Monedero"}, {"ref_id": "b1338", "title": "Ana Mar\u00eda I-154", "journal": "", "year": "", "authors": "S\u00e1nchez Montero"}, {"ref_id": "b1339", "title": "", "journal": "", "year": "", "authors": "Pedro J Sanchez"}, {"ref_id": "b1340", "title": "", "journal": "", "year": "", "authors": "Juan S\u00e1nchez-P\u00e9rez;  Manuel"}, {"ref_id": "b1341", "title": "", "journal": "", "year": "", "authors": "A Sanchis"}, {"ref_id": "b1342", "title": "", "journal": "", "year": "", "authors": "Araceli Sanchis"}, {"ref_id": "b1343", "title": "Jos\u00e9 Miguel I-368", "journal": "", "year": "", "authors": " Sanchiz"}, {"ref_id": "b1344", "title": "", "journal": "", "year": "", "authors": "Roberto Santana"}, {"ref_id": "b1345", "title": "", "journal": "", "year": "", "authors": "Miguel A Sanz-Bobi"}, {"ref_id": "b1346", "title": "", "journal": "", "year": "", "authors": "Nascimento Saraiva Do;  Junior"}, {"ref_id": "b1347", "title": "", "journal": "", "year": "", "authors": "Luis M Sarro"}, {"ref_id": "b1348", "title": "", "journal": "", "year": "", "authors": "Stefan I Schippel"}, {"ref_id": "b1349", "title": "", "journal": "", "year": "", "authors": "Federico I Schlesinger"}, {"ref_id": "b1350", "title": "", "journal": "", "year": "", "authors": " Schmid"}, {"ref_id": "b1351", "title": "", "journal": "", "year": "", "authors": "Thomas Schneider"}, {"ref_id": "b1352", "title": "Monika I-621, I-631, I-641", "journal": "", "year": "", "authors": " Schubert"}, {"ref_id": "b1353", "title": "", "journal": "", "year": "", "authors": "Anika I Schumann"}, {"ref_id": "b1354", "title": "", "journal": "", "year": "", "authors": "Lesley E Scott"}, {"ref_id": "b1355", "title": "", "journal": "", "year": "", "authors": "Javier Sedano"}, {"ref_id": "b1356", "title": "", "journal": "", "year": "", "authors": "Ralf Seepold"}, {"ref_id": "b1357", "title": "", "journal": "", "year": "", "authors": "M J Segovia-Vargas"}, {"ref_id": "b1358", "title": "", "journal": "", "year": "", "authors": " Serrano-Cuerda"}, {"ref_id": "b1359", "title": "", "journal": "", "year": "", "authors": " Serrano-Guerrero"}, {"ref_id": "b1360", "title": "", "journal": "", "year": "", "authors": "Siti Shamsuddin;  Mariyam"}, {"ref_id": "b1361", "title": "", "journal": "", "year": "", "authors": "Aditi Sharan"}, {"ref_id": "b1362", "title": "", "journal": "", "year": "", "authors": "Alexei Sharpanskykh"}, {"ref_id": "b1363", "title": "", "journal": "", "year": "", "authors": " Shearer"}, {"ref_id": "b1364", "title": "", "journal": "", "year": "", "authors": "Toramatsu Shintani;  Ii"}, {"ref_id": "b1365", "title": "", "journal": "", "year": "", "authors": "Shun Shiramatsu;  Ii"}, {"ref_id": "b1366", "title": "", "journal": "", "year": "", "authors": "Ghazanfar F Siddiqui"}, {"ref_id": "b1367", "title": "", "journal": "", "year": "", "authors": " Sidrach-De-Cardona"}, {"ref_id": "b1368", "title": "", "journal": "", "year": "", "authors": "Ant\u00f3nio I Silva"}, {"ref_id": "b1369", "title": "", "journal": "", "year": "", "authors": " Silva"}, {"ref_id": "b1370", "title": "", "journal": "", "year": "", "authors": "M Sim\u00f3n-Hurtado;  Ar\u00e1nzazu"}, {"ref_id": "b1371", "title": "", "journal": "", "year": "", "authors": "Paulo Siqueira;  Henrique"}, {"ref_id": "b1372", "title": "", "journal": "", "year": "", "authors": "Jo\u00e3o P Soares"}, {"ref_id": "b1373", "title": "", "journal": "", "year": "", "authors": "L M Soria-Morillo"}, {"ref_id": "b1374", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 M Soto"}, {"ref_id": "b1375", "title": "", "journal": "", "year": "", "authors": "Ricardo Soto"}, {"ref_id": "b1376", "title": "", "journal": "", "year": "", "authors": "Wendy Stevens"}, {"ref_id": "b1377", "title": "", "journal": "", "year": "", "authors": " Stibor"}, {"ref_id": "b1378", "title": "", "journal": "", "year": "", "authors": "V L Strube De Lima"}, {"ref_id": "b1379", "title": "", "journal": "", "year": "", "authors": "Thomas I St\u00fctzle"}, {"ref_id": "b1380", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Subirats;  Luis"}, {"ref_id": "b1381", "title": "", "journal": "", "year": "", "authors": "Arun Subramanian;  Ii"}, {"ref_id": "b1382", "title": "", "journal": "", "year": "", "authors": "Kun Su;  Shian"}, {"ref_id": "b1383", "title": "", "journal": "", "year": "", "authors": "Jigang Sun;  Ii"}, {"ref_id": "b1384", "title": "", "journal": "", "year": "", "authors": "Robin M E Swezey"}, {"ref_id": "b1385", "title": "", "journal": "", "year": "", "authors": "Konrad I Swirski"}, {"ref_id": "b1386", "title": "", "journal": "", "year": "", "authors": "Armando I Tacchella"}, {"ref_id": "b1387", "title": "", "journal": "", "year": "", "authors": "Toru I Takahashi"}, {"ref_id": "b1388", "title": "", "journal": "", "year": "", "authors": " Takasaki"}, {"ref_id": "b1389", "title": "", "journal": "", "year": "", "authors": "Dan E Tamir"}, {"ref_id": "b1390", "title": "", "journal": "", "year": "", "authors": "Tani "}, {"ref_id": "b1391", "title": "", "journal": "", "year": "", "authors": "Dante I Tapia"}, {"ref_id": "b1392", "title": "", "journal": "", "year": "", "authors": "Alain Tchagang"}, {"ref_id": "b1393", "title": "", "journal": "", "year": "", "authors": "Joaquim Teixeira"}, {"ref_id": "b1394", "title": "", "journal": "", "year": "", "authors": " Teppan"}, {"ref_id": "b1395", "title": "II-397", "journal": "", "year": "", "authors": "Andrea G B Tettamanzi"}, {"ref_id": "b1396", "title": "", "journal": "", "year": "", "authors": "Roberto Ther\u00f3n"}, {"ref_id": "b1397", "title": "", "journal": "", "year": "", "authors": " Torrecilla-Pinero"}, {"ref_id": "b1398", "title": "", "journal": "", "year": "", "authors": " Torrecilla-Pinero; A Jes\u00fas"}, {"ref_id": "b1399", "title": "", "journal": "", "year": "", "authors": " Treur"}, {"ref_id": "b1400", "title": "", "journal": "", "year": "", "authors": "A M Trujillo"}, {"ref_id": "b1401", "title": "", "journal": "", "year": "", "authors": "Jia-Hao Tsao; I "}, {"ref_id": "b1402", "title": "", "journal": "", "year": "", "authors": "Hiroshi Tsujino"}, {"ref_id": "b1403", "title": "", "journal": "", "year": "", "authors": "C Tub\u00edo"}, {"ref_id": "b1404", "title": "", "journal": "", "year": "", "authors": " Ul-Qayyum"}, {"ref_id": "b1405", "title": "", "journal": "", "year": "", "authors": "Daniel I Urda"}, {"ref_id": "b1406", "title": "", "journal": "", "year": "", "authors": "Soledad Valero"}, {"ref_id": "b1407", "title": "I-143, I-731", "journal": "", "year": "", "authors": "Zita A Vale"}, {"ref_id": "b1408", "title": "", "journal": "", "year": "", "authors": " Valiente-Rocha"}, {"ref_id": "b1409", "title": "", "journal": "", "year": "", "authors": "Mari Vallez"}, {"ref_id": "b1410", "title": "Aida I-274 Van den Poel", "journal": "", "year": "", "authors": " Valls"}, {"ref_id": "b1411", "title": "", "journal": "", "year": "", "authors": " Vanhoof"}, {"ref_id": "b1412", "title": "", "journal": "", "year": "", "authors": " Van Tan"}, {"ref_id": "b1413", "title": "", "journal": "", "year": "", "authors": " Vargas-Mart\u00ednez"}, {"ref_id": "b1414", "title": "", "journal": "", "year": "", "authors": "Alexandre Varnek"}, {"ref_id": "b1415", "title": "", "journal": "", "year": "", "authors": " Varo-Mart\u00ednez"}, {"ref_id": "b1416", "title": "", "journal": "", "year": "", "authors": "Pramod K Varshney;  Ii"}, {"ref_id": "b1417", "title": "", "journal": "", "year": "", "authors": " Vasconcelos; C Germano"}, {"ref_id": "b1418", "title": "", "journal": "", "year": "", "authors": "Andrey Vavilin"}, {"ref_id": "b1419", "title": "Miguel Angel II-153, II-267", "journal": "", "year": "", "authors": " Vega-Rodr\u00edguez"}, {"ref_id": "b1420", "title": "", "journal": "", "year": "", "authors": "J R Velasco;  Ii"}, {"ref_id": "b1421", "title": "", "journal": "", "year": "", "authors": "Sebasti\u00e1n Ventura"}, {"ref_id": "b1422", "title": "", "journal": "", "year": "", "authors": "Nele Verbiest"}, {"ref_id": "b1423", "title": "", "journal": "", "year": "", "authors": " Verd\u00fa"}, {"ref_id": "b1424", "title": "", "journal": "", "year": "", "authors": "Mar\u00eda Verd\u00fa;  Jes\u00fas"}, {"ref_id": "b1425", "title": "", "journal": "", "year": "", "authors": "Patricia Victor"}, {"ref_id": "b1426", "title": "", "journal": "", "year": "", "authors": "Juan Vidal;  Carlos"}, {"ref_id": "b1427", "title": "", "journal": "", "year": "", "authors": "Marina T P Vieira"}, {"ref_id": "b1428", "title": "", "journal": "", "year": "", "authors": "Petronio Vieira"}, {"ref_id": "b1429", "title": "", "journal": "", "year": "", "authors": "Rodrigo J Vieira"}, {"ref_id": "b1430", "title": "", "journal": "", "year": "", "authors": "A Villagra"}, {"ref_id": "b1431", "title": "", "journal": "", "year": "", "authors": "Jos\u00e9 Villar;  Ram\u00f3n"}, {"ref_id": "b1432", "title": "", "journal": "", "year": "", "authors": "Alain Wagner"}, {"ref_id": "b1433", "title": "", "journal": "", "year": "", "authors": " Walgampaya"}, {"ref_id": "b1434", "title": "", "journal": "", "year": "", "authors": "Xi Wang"}, {"ref_id": "b1435", "title": "", "journal": "", "year": "", "authors": " Wang"}, {"ref_id": "b1436", "title": "", "journal": "", "year": "", "authors": "Wei Wan"}, {"ref_id": "b1437", "title": "", "journal": "", "year": "", "authors": "Michal I Warchol"}, {"ref_id": "b1438", "title": "", "journal": "", "year": "", "authors": "Toyohide Watanabe"}, {"ref_id": "b1439", "title": "", "journal": "", "year": "", "authors": " Weber"}, {"ref_id": "b1440", "title": "", "journal": "", "year": "", "authors": "Geert I Wets"}, {"ref_id": "b1441", "title": "", "journal": "", "year": "", "authors": "Tony White"}, {"ref_id": "b1442", "title": "", "journal": "", "year": "", "authors": "Wilson "}, {"ref_id": "b1443", "title": "", "journal": "", "year": "", "authors": "A T Winck"}, {"ref_id": "b1444", "title": "", "journal": "", "year": "", "authors": "Konrad I Wojdan"}, {"ref_id": "b1445", "title": "", "journal": "", "year": "", "authors": "Franz I Wotawa"}, {"ref_id": "b1446", "title": "", "journal": "", "year": "", "authors": "Chien-Liang Wu"}, {"ref_id": "b1447", "title": "", "journal": "", "year": "", "authors": "Ying Wu"}, {"ref_id": "b1448", "title": "", "journal": "", "year": "", "authors": "Lia Yamamoto"}, {"ref_id": "b1449", "title": "", "journal": "", "year": "", "authors": "Jing-Hsiung Yang"}, {"ref_id": "b1450", "title": "", "journal": "", "year": "", "authors": "Yazidi "}, {"ref_id": "b1451", "title": "", "journal": "", "year": "", "authors": "Yilmaz "}, {"ref_id": "b1452", "title": "", "journal": "", "year": "", "authors": "Myeong-Jae Yi; I "}, {"ref_id": "b1453", "title": "", "journal": "", "year": "", "authors": "Takami I Yoshida"}, {"ref_id": "b1454", "title": "", "journal": "", "year": "", "authors": "Zhi I Yuan"}, {"ref_id": "b1455", "title": "", "journal": "", "year": "", "authors": "Amelia Zafra"}, {"ref_id": "b1456", "title": "", "journal": "", "year": "", "authors": "Arkady Zaslavsky"}, {"ref_id": "b1457", "title": "", "journal": "", "year": "", "authors": "Huidong I Zhang"}, {"ref_id": "b1458", "title": "", "journal": "", "year": "", "authors": "Zizhen Zhang"}, {"ref_id": "b1459", "title": "", "journal": "", "year": "", "authors": "Wenbin I Zhu"}, {"ref_id": "b1460", "title": "", "journal": "", "year": "", "authors": "Tatiana Zidrasco;  Ii"}, {"ref_id": "b1461", "title": "", "journal": "", "year": "", "authors": " Zuloaga"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 1 .1Fig. 1. Curve showing the MAP and R-PREC measures with different numbers of candidate terms to expand the original query using Co-occurrence Approach", "figure_data": ""}, {"figure_label": "234", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 2 .Fig. 3 .Fig. 4 .234Fig. 2. Curve showing the MAP and R-PREC measures with different numbers of candidate terms to expand the original query using Kullback-Leibler divergence Approach", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "References 1 .1Lee, C.J., Lin, Y.C., Chen, R.C., Cheng, P.J.: Selecting effective terms for query formulation. In: Proc. of the Fifth Asia Information Retrieval Symposium (2009) 2. Van Rijsbergen, C.J.: A theoretical basis for the use of cooccurrence data in information retrieval.Journal of Documentation (33), 106-119 (1977) 3. Carpineto, C., Romano, G.: TREC-8 Automatic Ad-Hoc Experiments at Fondazione Ugo Bordoni,TREC (1999) 4. Croft, W.B., Harper, D.J.: Using probabilistic models of document retrieval without relevance information. Journal of Documentation 35, 285-295 (1979) 5. Carmel, D., Yom-Tov, E., Soboroff, I.: SIGIR Workshop Report: Predicting query difficulty -methods and applications. In: Proc. of the ACM SIGIR 2005 Workshop on Predicting Query Difficulty -Methods and Applications, pp. 25-28 (2005) 6. Voorhees, E.M.: Query expansion using lexical semantic relations. In: Proceedings of the 1994 ACM SIGIR Conference on Research and Development in Information Retrieval (1994) 7. Efthimiadis, E.N.: Query expansion. Annual Review of Information Systems and Technology 31, 121-187 (1996) 8. Voorhees, E.M.: Overview of the TREC 2003 robust retrieval track. In: TREC, pp. 69-77 (2003) 9. Voorhees, E.M.: The TREC 2005 robust track. SIGIR Forum 40(1), 41-48 (2006) 10. Voorhees, E.M.: The TREC robust retrieval track. SIGIR Forum 39(1), 11-20 (2005) 11. Cao, G., Nie, J.Y., Gao, J.F., Robertson, S.: Selecting good expansion terms for pseudorelevance feedback. In: Proc. of 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 243-250 (2008) 12. Imran, H., Sharan, A.: Thesaurus and Query Expansion. International journal of computer science & information Technology (IJCSIT) 1(2), 89-97 (2009) 13. Harper, D.J., van Rijsbergen, C.J.: Evaluation of feedback in document retrieval using cooccurrence data. Journal of Documentation 34, 189-216 (1978) 14. Peat, H.J., Willett, P.: The limitations of term co-occurrence data for query expansion in document retrieval systems. JASIS 42(5), 378-383 (1991) 15. Sch\u00fctze, H., Pedersen, J.O.: A cooccurrence-based thesaurus and two applications to information retrieval. Inf. Process. Manage 33(3), 307-318 (1997) 16. Jing, Y., Croft, W.B.: An association thesaurus for information retrieval. In: 4th International Conference on Proceedings of RIAO 1994, New York, US, pp. 146-160 (1994) 17. Xu, J., Croft, W.B.: Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst. 18(1), 79-112 (2000) 18. Lesk, M.E.: Word-word associations in document retrieval systems. American Documentation 20, 27-38 (1969) 19. Stairmand, M.A.: Textual context analysis for information retrieval. In: Proceedings of the 1997 ACM SIGIR Conference on Research and Development in Information Retrieval (1997) 20. Porter, M.F.: An algorithm for suffix stripping. Program -automated library and information systems 14(3), 130-137 (1980) 21. Maron, M.E., Kuhns, J.K.: On relevance, probabilistic indexing and information retrieval. Journal of rhe ACM 7, 216-244 (1960) 22. Minker, J., Wilson, G.A., Zimmerman, B.H.: Query expansion by the addition of clustered terms for a document retrieval system. Information Storage and Retrieval 8, 329-348 (1972) 23. Ruch, P., Tbahriti, I., Gobeill, J., Aronson, A.R.: Argumentative feedback: A linguistically-motivated term expansion for information retrieval. In: Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pp. 675-682 (2006) 24. Mandala, R., Tokunaga, T., Tanaka, H.: Combining multiple evidence from different types of thesaurus for query expansion. In: Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval (1999) 25. Mandala, R., Tokunaga, T., Tanaka, H.: Ad hoc retrieval experiments using wornet and automatically constructed theasuri. In: Proceedings of the seventh Text REtrieval Conference, TREC7 (1999) 26. Robertson, S.E., Sparck Jones, K.: Relevance weighting of search terms. Journal of the American Society of Informarion Science 21, 129-146 (1976) 27. Liu, S., Liu, F., Yu, C., Meng, W.: An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In: Proceedings of the 2004 ACM SIGIR Conference on Research and Development in Information Retrieval (2004) 28. Smeaton, A.F.: The retrieval effects of query expansion on a feedback document retrieval system, University College Dublin, MSc thesis (1982) 29. Smeaton, A.F., van Rijsbergen, C.J.: The retrieval effects of query expansion on a feedback document retrieval system. Computer Journal 26, 239-246 (1983) 30. Sparck Jones, K.: Automatic keyword classification for information retrieval. Butterworth, London (1971) 31. Van Rijsbergen, C.J., Harper, D.J., Porter, M.F.: The selection of good search terms. Information Processing and Management 17, 77-91 (1981) 32. Qiu, Y., Frei, H.-P.: Concept based query expansion. In: SIGIR, pp. 160-169 (1993)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 1 .1Fig. 1. Shows the CBE-out, CBE-in and SC definitions on class A", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "References 1 .1Antoniou, G., van Harmelen, F.: A Semantic Web Primer, 2nd edn. The MIT Press, Cambridge (2008) 2. Chidamber, S., Kemerer, C.: A Metrics suite for object Oriented Design. IEEE Transactions on Software Engineering 20(6), 476-493 (1994) 3. Vrandecic, D., Sure, Y.: How to Design Better Ontology Metrics. In: Franconi, E., Kifer, M., May, W. (eds.) ESWC 2007. LNCS, vol. 4519, pp. 311-325. Springer, Heidelberg (2007) 4. Alani, H., Brewster, C., Shadbolt, N.: Ranking Ontologies with AKTiveRank. In: Cruz, I., Decker, S., Allemang, D., Preist, C., Schwabe, D., Mika, P., Uschold, M., Aroyo, L.M. (eds.) ISWC 2006. LNCS, vol. 4273, pp. 1-15. Springer, Heidelberg (2006) 5. Hitz, M., Montazeri, B.: Measuring Coupling and Cohesion In Object-Oriented Systems. In: Proceedings of the International Symposium on Applied Corporate Computing (1995) 6. Hanakawa, N.: Visualization for software evolution based on logical coupling and module coupling. In: 14th Asia-Pacific Software Engineering Conference (2007) 7. Wand, Y., Weber, R.: An Ontological Model of an Information System. IEEE Trans. Software Engineering (1990) 8. Allen, E., Khoshgoftaar, T.: Measuring Coupling and Cohesion: An Information-Theory Approach. In: Sixth International Software Metrics Symposium, METRICS 1999 (1999) 9. Offutt, J., Harrold, M., Kolte, P.: A Software Metric System for Module Coupling. Journal of Systems and Software (1993) 10. Yang, H.Y., Tempero, E., Berrigan, R.: Detecting Indirect Coupling. In: IEEE, Proceedings of the 2005 Australian conference on Software Engineering, ASWEC 2005 (2005) 11. Archer, C.: Measuring Object-Oriented Software Products. Carnegie Mellon University (1995) 12. Aleman-meza, B., Halaschek, C., Sheth, A., Arpinar, B., Sannapareddy, G.: SWETO: Large-Scale Semantic Web Test-bed. In: 16th International Conference on Software Engineering and Knowledge Engineering (2004) 13. Tartir, S., Arpinar, B., Moore, M., Sheth, A., Aleman-meza, B.: OntoQA: Metricbased ontology quality analysis. CiteSeerX -Scientific Literature Digital Library and Search Engine (2005) 14. Tartir, S., Arpinar, B.: Ontology Evaluation and Ranking using OntoQA. In: Proceedings of the International Conference on Semantic Computing (2007)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 1 .1Fig. 1. Open Innovation", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure Name I II III IV Major Premise Minor Premise Conclusion", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Fig. 1 .1Fig. 1. Mapping the sub-sets of the symmetrically intersecting sets P, M and S onto arithmetic relations", "figure_data": ""}, {"figure_label": "345", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Fig. 3 .Fig. 4 .Fig. 5 .345Fig. 3. Sample syllogistic inference with the mood AIA1", "figure_data": ""}, {"figure_label": "A1251", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Table A1 . 25 AAE- 1 ,A1251Possibility distribution FuzzySyllogisticMood(x) over the Syllogistic moods in increasing order of truth ratio of the moods AAO-1, AIE-1, EAA-1, EAI-1, EIA-1, AEA-2, AEI-2, AOA-2, EAA-2, EAI-2, EIA-2, AAE-3, AIE-3, EAA-3, EIA-3, IAE-3, OAA-3, AAA-4, AAE-4, AEA-4, AEI-4, EAA-4, EIA-4, IAE-IEE-1, EIE-2, IEE-2, EIE-3, IEE-3, EIE-4, IEE-4, AOE-2, OAA-2, OAE-2, AOA-1, IAA-1, OAE-1, OEE-1, IAA-2, EOE-3, OEE-3, AOE-4, EOE-4, OOE-3, AEA-1, AEE-1, AAA-3, AEA-3, AEE-3, EAE-3, EAE-4, EOE-1, EOE-2, OEA-2, OEE-2, OEA-4, OEE-4, OIE-1, OOE-1, OOA-4, OOE-4, IOA-3, IOE-3, OIE-3, IOA-4, IOE-4, IEA-1, IEA-2, IEA-3, IEA-4, IIA-1, IIA-2, IIA-3, IIA-4, IAE-1, OAA-1, OEA-1, AIE-2, IAE-2, OEA-3, AIE-4, AAA-2, AAE-2, EAA-1, EEE-1, EEA-2, EEE-2, EEA -3, EEE-3, EEA-4, EEE-4, IOA-1, IOE-1, IOA-2, IOE-2, OIA-2, OIE-2, OIA-4, OIE-4, OOA-2, OOE-2, OOA-3, IIE-1, IIE-2, IIE-3, IIE-4, AOE-3, IAA-3, OAE-3, IAA-4, OOA-1, OIA-1, OIA-3, AOE-1, AIA-2, EOA-3, AIA-4, AOA-4, EOA-4, OAA-4, OAE-4, EOA-1, EOO-2, OIO-1, OOO-1, OIO-3, AIO-2, EOO-3, AIO-4, AOI-1, AOO-4, EOO-4, OAI-4, OAO-4, IAO-3, IAO-4, OAI-3, AOI-3, III-1, III-2, III-3, III-4, OOO-3, OOI-2, OOO-2, IOI-1, IOO-1, OII-2, OIO-2, IOI-2, IOO-2, OII-4, OIO-4, IAI-1, OAO-1, OEO-1, AII-2, OEO-3, IAI-2, AII-4, AAI-2, AAO-2, EEI-2, EEO-2, EEI-3, EEO-3, EEI-4, EEO-4, EEI-1, EEO-1, IIO-1, IIO-2, IIO-3, IIO-4, IEO-1, IEO-2, IEO-3, IEO-4, OII-1, OOI-1, IOI-3, IOO-3, OII-3, IOI-4, IOO-4, OOI-4, OOO-4, EOI-1, EOI-2, OEI-4, OEI-2, OEO-2, OEO-4, AEI-1, AEO-1, AAO-3, AEI-3, AEO-3, EAI-3, EAI-4, OOI-3, AOO-1, IAO-1, OAI-1, OEI-1, IAO-2, EOI-3, OEI-3, AOI-4, EOI-4, AOI-2, OAI-2, OAO-2, IEI-1, EII-1, EII-2, IEI-2, EII-3, IEI-3, EII-4, AAI-1, AII-1, EAE-1, EAO-1, EIO-1, AEE-2, AEO-2, AOO-2, EAE-2, EAO-2, EIO-2, AAI-3, AII-3, EAO-3, EIO-3, IAI-3, OAO-3, AAI-4, AAO-4, AEE-4, AEO-4, EAO-4, EIO-4, IAI-4", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Fig. 1 .1Fig. 1. Multilingual folksonomy system with three different languages (i.e., English, Spanish, and French). Two users ua and u b have tagged two resources r1 and r2 with three tags t1, t2, and t3.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Fig. 2 .2Fig. 2. Measuring user satisfaction with RSS feeds via the proposed tag-based information retrieval system", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "to r ij . The closeness is determined by using criterion of average (weighted) Kulback-Leibler distance between r ij and \u03bc ij", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Fig. 2 .2Fig. 2.Original data of five different minerals (left) and the same data after employment of similarity features (right). Dimensions were reduced according to eigenvalues of covariance matrixes (principal component analysis method). Same shapes and colors mean the same minerals.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "9 .9Komenda, J.: Automatic recognition of complex microstructures using the Image Classifier. Materials Characterization 46(2-3), 87-92 (2001) 10. Akesson, U., Stigh, J., Lindqvist, J.E., G\u00f6ransson, M.: The influence of foliation on the fragility of granitic rocks, image analysis and quantitative microscopy. Engineering Geology 68(3-4), 275-288 (2003) 11. Forero, M.G., Sroubek, F., Cristobal, G.: Identification of tuberculosis bacteria based on shape and color. Real-Time Imaging 10(4), 251-262 (2004) 12. Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford Univ. Press, Oxford (1995) 13. Raudys, S.: Statistical and Neural Classifiers: An integrated approach to design. Springer, NY (2001) 14. Haykin, S.: Neural Networks: A comprehensive foundation, 2nd edn. Prentice-Hall, Englewood Cliffs (1999) 15. Fukunaga, K.: Introduction to Statistical Pattern Recognition, 2nd edn. Academic Press, NY (1990) 16. Boser, B., Guyon, I., Vapnik, V.: A Training Algorithm for Optimal Margin Classifiers. In: Proceedings of the Fith Annual Workshop on Computational Learning Theory, pp. 144-152. ACM Press, New York (1992) 17. Raudys, S.: Evolution and generalization of a single neurone. I. SLP as seven statistical classifiers, Neural Networks 11, 283-296 (1998) 18. Chang, C.-C., Lin, C.-J.: LIBSVM: a library for support vector machines (2001), Available at, http://www.csie.ntu.edu.tw/~cjlin/libsvm 19. Hsu, C.-W., Chang, C.-C., Lin, C.-J.: A Practical Guide to Support Vector Classifcation (2009), http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf 20. Wu, K.-P., Wang, S.-D.: A weight initialization strategy for weighted support vector machines. In: Singh, S., Singh, M., Apte, C., Perner, P. (eds.) ICAPR 2005. LNCS, vol. 3686, pp. 288-296. Springer, Heidelberg (2005) 21. Skurichina, M., Raudys, S., Duin, R.P.W.: K-NN directed noise injection in multilayer perceptron training. IEEE Trans. on Neural Networks 11(2), 504-511 (2000) 22. Park, S.-H., F\u00fcrnkranz, J.: Efficient Pairwise Classification. In: Kok, J.N., Koronacki, J., Lopez de Mantaras, R., Matwin, S., Mladeni\u010d, D., Skowron, A. (eds.) ECML 2007. LNCS (LNAI), vol. 4701, pp. 658-665. Springer, Heidelberg (2007) 23. F\u00fcrnkranz, J.: Round Robin Classification. Journal of Machine Learning Research 2, 721-747 (2002) 24. Krzysko, M., Wolynski, W.: New variants of pairwise classification. European Journal of Operational Research (EOR) 199(2), 512-519 (2009) 25. Sulzmann, J.-N., F\u00fcrnkranz, J., H\u00fcllermeier, E.: On Pairwise Naive Bayes Classifiers. In: Kok, J.N., Koronacki, J., Lopez de Mantaras, R., Matwin, S., Mladeni\u010d, D., Skowron, A. (eds.) ECML 2007. LNCS (LNAI), vol. 4701, pp. 371-381. Springer, Heidelberg (2007) 26. Dietterich, T.G., Bakiri, G.: Solving multiclass learning problems via error-correcting otput codes. J. Artif. Int. Res. 2, 263-286 (1995) 27. Platt, J.C., Cristianini, N., Shawe-Taylor, J.: Large margin DAG's for multi-class classification. In: Advances in Neural Information Processing Systems, vol. 12, pp. 547-553. MIT Press, Cambridge (2000) 28. Hastie, T., Tibshirani, R.: Classification by pairwise coupling. The Annals of Statistics 26(1), 451-471 (1998) 29. Friedman, J.H.: Regularized discriminant analysis. Journal of the American Statistical Association 84(405), 165-175 (1989)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "1 .1Breaban, M., Luchian, H.: Unsupervised feature weighting with multi-niche genetic algorithms. In: Proceedings of the 11th Annual conference on Genetic and evolutionary computation, July 2009, pp. 1163-1170. ACM, New York (2009) 2. Davies, D.L., Bouldin, D.W.: A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence 1(2), 224-227 (1979) 3. Domeniconi, C., Al-Razgan, M.: Weighted cluster ensembles: Methods and analysis. ACM Transactions on Knowledge Discovery from Data 2(4), 1-40 (2009) 4. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classification, 2nd edn. John Wiley & Sons, Chichester (2001) 5. Dy, J., Brodley, C.: Feature selection for unsupervised learning. Journal of Machine Learning Research 5, 845-889 (2004) 6. Guerif, S.: Unsupervised variable selection: when random rankings sound as irrelevancy. Journal of Machine Learning Research 4, 163-177 (2008) 7. Handl, J., Knowles, J.: Feature subset selection in unsupervised learning via multiobjective optimization. International Journal of Computational Intelligence Research 2(3), 217-238 (2006) 8. Handl, J., Knowles, J.: Semi-supervised feature selection via multiobjective optimization. In: Proceedings of the International Joint Conference on Neural Networks, pp. 3319-3326 (2006) 9. Hong, Y., Kwong, S., Chang, Y., Ren, Q.: Consensus unsupervised feature ranking from multiple views. Pattern Recognition Letters 29, 595-602 (2008) 10. Hubert, A.: Comparing partitions. Journal of", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Fig. 3 .Fig. 4 .34Fig. 3. Cluster region formed from suspicious samples", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Fig. 1 .1Fig.1. Rankings for all the algorithms. In horizontal is represented the number of selected attributes. In vertical the average ranking position.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Fig. 2 .2Fig. 2. Various rankings", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_23", "figure_caption": "Fig. 1 .1Fig. 1. Comparison scores between the considered methods and Bagging of Multilayer Perceptrons", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_24", "figure_caption": "Fig. 2 .2Fig. 2. Diversity error diagrams", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_25", "figure_caption": "Fig. 1 .1Fig. 1. Diagram of the plant and fault modes considered", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_26", "figure_caption": "Fig. 1 .1Fig. 1. Entropy reduction in a BOA solving 120-bit trap-5 problem", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_27", "figure_caption": "Fig. 2 .2Fig. 2. Performance of BOA and eBOA on OneMax problem", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_28", "figure_caption": "Fig. 3 .3Fig. 3. Performance of BOA and eBOA on trap-5 problem", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_29", "figure_caption": "Fig. 4 .4Fig. 4. Performance of eBOA with different starting points of ODES", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_30", "figure_caption": "FigureFigure4provides experimental results to support our choice of the starting point to apply ODES as stated in Sect. 4.2. We perform experiments for eBOA solving the 30-bit trap-5 problem with different starting points. In terms of entropy reduction, 0% means to start ODES from the beginning of the optimization process, 100% indicates the standard BOA, and 50% comes under eBOA. When the entropy of the elite set decreases as a half of its original value, starting ODES achieves the minimal number of evaluations.", "figure_data": ""}, {"figure_label": "111", "figure_type": "figure", "figure_id": "fig_31", "figure_caption": "\u03be 11 ) 1 .111m \u2264 P ower(S) = P ower p (S) + P ower c (S) \u2264 \u03be m (Algorithm f 1 (Q) -SameSrcDiffTgt", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_33", "figure_caption": "Pair 1 Representation1Pair 2 Pair 3 Pair 4 Pair 5 Pair 6", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_34", "figure_caption": "Fig. 2 .Fig. 3 .23Fig. 2. Number of Individuals -Average Fitness (a) / Execution Time (b) / Best Solutions (c)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_35", "figure_caption": "References 1 .1RPR Alliance: A Summary and Overview of the IEEE 802.17 Resilient Packet Ring Standard (2004) 2. Davik, F., Yilmaz, M., Gjessing, S., Uzun, N.: IEEE 802.17 Resilient Packet Ring Tutorial. IEEE Communications Magazine 42(3), 112-118 (2004) 3. Yuan, P., Gambiroza, V., Knightly, E.: The IEEE 802.17 Media Access Protocol for High-Speed Metropolitan-Area Resilient Packet Rings. IEEE Network 18(3), 8-15 (2004) 4. Cosares, S., Saniee, I.: An optimization problem related to balancing loads on SONET rings. Telecommunication Systems 3(2), 165-181 (1994) 5. Dell'Amico, M., Labb\u00e9, M., Maffioli, F.: Exact solution of the SONET Ring Loading Problem. Oper. Res. Lett. 25(3), 119-129 (1999) 6. Bernardino, A.M., Bernardino, E.M., S\u00e1nchez-P\u00e9rez, J.M., Vega-Rodr\u00edguez, M.A., G\u00f3mez-Pulido, J.A.: Solving the Ring Loading Problem using Genetic Algorithms with intelligent multiple operators. In: International Symposium on Distributed Computing and Artificial Intelligence 2008 (DCAI 2008), pp. 235-244. Springer, Heidelberg (2008) 7. Bernardino, A.M., Bernardino, E.M., S\u00e1nchez-P\u00e9rez, J.M., Vega-Rodr\u00edguez, M.A., G\u00f3mez-Pulido, J.A.: Solving the weighted ring edge-loading problem without demand splitting using a Hybrid Differential Evolution Algorithm. In: The 34th IEEE Conference on Local Computer Networks. IEEE Press, Los Alamitos (2009) 8. Schrijver, A., Seymour, P., Winkler, P.: The ring loading problem. SIAM Journal of Discrete Mathematics 11, 1-14 (1998) 9. Myung, Y.S., Kim, H.G.: On the ring loading problem with demand splitting. Operations Research Letters 32(2), 167-173 (2004) 10. Wang, B.F.: Linear time algorithms for the ring loading problem with demand splitting. Journal of Algorithms 54(1), 45-57 (2005) 11. Kubat, P., Smith, J.M.: Balancing traffic flows in resilient packet rings. In: Girard, A., et al. (eds.) Performance evaluation and planning methods for the next generation internet. GERAD 25th Anniversary, Series. 6, pp. 125-140. Springer, Heidelberg (2005) 12. Cho, K.S., Joo, U.G., Lee, H.S., Kim, B.T., Lee, W.D.: Efficient Load Balancing Algorithms for a Resilient Packet Ring. ETRI Journal 27(1), 110-113 (2005) 13. Yuan, J., Zhou, S.: Polynomial Time Solvability Of The Weighted Ring Arc-Loading Problem With Integer Splitting. Journal of Interconnection Networks 5(2), 193-200 (2004) 14. Bernardino, A.M., Bernardino, E.M., S\u00e1nchez-P\u00e9rez, J.M., Vega-Rodr\u00edguez, M.A., G\u00f3mez-Pulido, J.A.: Solving the non-split weighted ring arc-loading problem in a Resilient Packet Ring using Particle Swarm Optimization. In: International Conference in Evolutionary Computation (2009) 15. Pan, Q.-K., Tasgetiren, M.F., Liang, Y.-C.: A discrete differential evolution algorithm for the permutation flowshop scheduling problem. In: Proceedings of the 9th annual conference on Genetic and evolutionary computation, pp. 126-133 (2007) 16. Storn, R., Price, K.: Differential Evolution -a Simple and Efficient Adaptive Scheme for Global Optimization over Continuous Spaces. Technical Report TR-95-012, ICSI (1995) 17. Price, K., Storn, R., Lampinen, J.: Differential Evolution -A Practical Approach to Global Optimization. Springer, Berlin (2005) 18. Tasgetiren, M.F., Pan, Q.-K., Liang, Y.-C.: A discrete differential evolution algorithm for the single machine total weighted tardiness problem with sequence dependent setup times. Computers and Operations Research 36(6), 1900-1915 (2009) 19. Differential Evolution Homepage, http://www.icsi.berkeley.edu/~storn/code.html", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_36", "figure_caption": "Fig. 1 .1Fig. 1. HMPS architecture, with 9 RISC Plasma processors connected to a 3\u00d73 mesh network", "figure_data": ""}, {"figure_label": "222", "figure_type": "figure", "figure_id": "fig_37", "figure_caption": "2 Fig. 2 . 2 .222Fig. 2. Ring migration topology", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_38", "figure_caption": "Algorithm 3 .3Migration function for the neighborhood communication 1: local := getprocessid(); 2: if local = 0 then 3: next := 1; previous := number of tasks \u22121; 4: end if 5: if local > 0 e local < number of tasks \u22121 then 6:", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_39", "figure_caption": "2 Fig. 3 .23Fig. 3. Neighborhood migration topology", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_40", "figure_caption": "Fig. 4 .Fig. 5 .45Fig. 4. Impact of the migration rate and migration interval on speedup and efficiency for function f1(x), considering the used topology", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_41", "figure_caption": "References 1 .1Ivanov, A., De Micheli, G.: The network-on-chip paradigm in practice and research. IEEE Design and Test of Computers 1(1), 399-403 (2005) 2. Mello, A.M.: Arquitetura multiprocessada em SoCs: estudo de diferentes topologias de conex\u00e3o (June 2003) [in Portuguese] 3. Woszezenki, C.: Aloca\u00e7\u00e3o de tarefas e comunica\u00e7\u00e3o entre tarefas em mpsocs. M.Sc., Faculdade de Inform\u00e1tica, PUCRS, Porto Alegre, RS, Brazil (June 2007) [in Portuguese] 4. Moraes, F., Calazans, N., Mello, A., M\u00f6ller, L., Ost, L.: Hermes: an infrastructure for low area overhead packet-switching networks on chip. Integration, the VLSI Journal 38(1), 69-93 (2004) 5.\u00d6berg, J., Jantsch, A., Tenhunen, H.: Special issue on networks on chip. Journal of Systems Architecture 1(1), 61-63 (2004) 6. Benini, L., De Micheli, G.: Networks on chips: a new soc paradigm. IEEE Computer 1(1), 70-78 (2002) 7. Benini, L., Ye, T.T., De Micheli, G.: Packetized on-chip interconnect communication analysis for MPSoC. In: Proceedings of the Design,Automation and Test in Europe Conference and Exhibition (DATE 2003), pp. 344-349. IEEE Press, Los Alamitos (2003) 8. Chiwiacowsky, L.D., de Velho, H.F.C., Preto, A.J., Stephany, S.: Identifying initial conduction in heat conduction transfer by a genetic algorithm: a parallel approach 28, 180-195 (April 1980) 9. Ruiz, P.M., Antonio: Using genetic algorithms to optimize the behavior of adaptive multimedia applications in wireless and mobile scenarios. In: IEEE Wireless Communications and Networking Conference (WCNC 2003), pp. 2064-2068. IEEE Press, Los Alamitos (2003) 10. Rhoads, S.: Plasma microprocessor (2009), http://www.opencores.org 11. Hue, X.: Genetic algorithms for optimization -background and applications. Technical report. Edinburgh Parallel Computer Centre, The University of Edinburgh(1997)   ", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_42", "figure_caption": "Fig. 1 .1Fig. 1. Fuzzy sets assigned to labels", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_43", "figure_caption": "[1, 1], [0.7, 0.9], [0.4, 0.6], [0.1, 0.3] and [0, 0] respectively (see Fig.2).", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_44", "figure_caption": "Table 3 .Fig. 2 .32Fig. 2. Labels of the linguistic variable V", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_45", "figure_caption": "Table 4 .4New  ", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_46", "figure_caption": "References 1 .1Alcalde, C., Burusco, A., Fuentes-Gonz\u00e1lez, R.: Treatment of the incomplete information in L-Fuzzy contexts. In: EUSFLAT-LFA 2005, Barcelona, September 2005, pp. 518-523 (2005) 2. Alcalde, C., Burusco, A., Fuentes-Gonz\u00e1lez, R.: Implications between attributes in an L-Fuzzy context based on association rules. In: IPMU 2006, Paris, July 2006, pp. 1403-1410 (2006) 3. Alcalde, C., Burusco, A., Fuentes-Gonz\u00e1lez, R., Zubia, I.: Treatment of L-Fuzzy contexts with absent values. Information Sciences 179(1-2), 1-15 (2009) 4. B\u011blohl\u00e1vek, R.: Fuzzy Galois connections and fuzzy concept lattices: from binary relations to conceptual structures. In: Novak, V., Perfileva, I. (eds.) Discovering the World with Fuzzy Logic, pp. 462-494. Physica-Verlag (2000) 5. Burusco, A., Fuentes-Gonz\u00e1lez, R.: The Study of the L-Fuzzy Concept Lattice. Mathware and Soft Computing 1(3), 209-218 (1994) 6. Burusco, A., Fuentes-Gonz\u00e1lez, R.: Construction of the L-Fuzzy Concept Lattice. Fuzzy Sets and Systems 97(1), 109-114 (1998) 7. Burusco, A., Fuentes-Gonz\u00e1lez, R.: Fuzzy extensions of the Formal Concept Analysis. In: International Conference on knowledge, logic and information, Darmstadt (February 1998) 8. Cousot, P., Cousot, R.: Constructive versions of Tarski's fixed point theorems. Pacific J. Maths 82, 43-57 (1979) 9. Ganter, B., Stahl, J., Wille, R.: Conceptual measurement and many-valued contexts. In: Gaul, W., Schader, M. (eds.) Classification as a tool of research, pp. 169-176. North Holland, Amsterdam (1986) 10. Medina, J., Ojeda-Aciego, M., Ruiz-Calvi\u00f1o, J.: On multi-adjoint concept lattices: denition and representation theorem. In: Kuznetsov, S.O., Schmidt, S. (eds.) ICFCA 2007. LNCS (LNAI), vol. 4390, pp. 197-209. Springer, Heidelberg (2007) 11. Medina, J., Ojeda-Aciego, M., Ruiz-Calvi\u00f1o, J.: Formal concept analysis via multiadjoint concept lattices. Fuzzy Sets and Systems 160(2), 130-144 (2009) 12. Pollandt, S.: Fuzzy Begriffe: Formale Begriffsanalyse unscharfer Daten. Springer, Heidelberg (1997) 13. Wille, R.: Restructuring lattice theory: an approach based on hierarchies of concepts. In: Rival, I. (ed.) Ordered Sets, pp. 445-470. Reidel, Dordrecht-Boston (1982) 14. Zadeh, L.A.: The Concept of a Linguistic Variable and its Application to Approximate Reasoning-I. Information Sciences8, 199-249 (1975)    ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_47", "figure_caption": "6674 0.2615 0.5941 0.4822 0.5076 0.7622 0.40 24 17 5 0.6674 0.2615 0.5941 0.4822 0.5076 0.7622 0.40 26 15 5 0.8643 0.1616 0.2973 0.8002 0.8162 0.8819 0.40 28 14 5 0.9241 0.0729 0.1488 0.9331 0.9685 0.8938 0.40 30 13 5 1.0000 0.0000 0.0000 1.0000 1.0000 1.0000", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_48", "figure_caption": "Fig. 1 .1Fig. 1. Tree and cluster quality indexes", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_49", "figure_caption": "med,high]->(13) T_avg_D-27 | | +-[low]------>(28) Call_Morning_D-26 | | +-[med,high]->(29) Month | +-[high]->(6) Calls_D-10 | +-[low]------>(14) Calls_D-17 | | +-[low]------>(30) Calls_D-05 | | +-[med,high]->(31) Call_Evening_D-05 | +-[med,high]->(15) Call_Midnight +-[med,high]->(3) T_avg_D-22 +-[low]-->(7) Month | +-[Jan,Feb]------------->(16) Fog_D-24 | +-[Mar,Apr,May,Nov,Dic]->(17) Call_Morning_D-07 +-[med]-->(8) Month | +-[Jan,Feb,Mar,Apr]->(18) Call_Midnight_D-09 | +-[May,Jun]--------->(19) Call_Evening_D-01 | +-[Sep,Oct,Nov,Dic]->(20) Call_Midnight_D-25 +-[high]->(9) Call_Midnight_D-03 +-[low]------>(21) Call_Midnight_D-24 +-[med,high]->(22) Call_Midnight", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_50", "figure_caption": "References 1 .1Mandelbaum, A., Garnett, O., Reiman, M.: Designing a call center with impatient customers. Manufacturing and Service Operations Management 4(3), 208-227 (2002) 2. Pajares, R.G., Benitez, J.M., Palmero, G.S.: Feature Selection for Time Series Forecasting: A Case Study. In: 8th International Conference on Hybrid Intelligent Systems, pp. 555-560 (2008) 3. Basak, J., Krishnapuram, R.: Interpretable hierarchical clustering by constructing an unsupervised decision tree. IEEE Transactions on Knowledge and Data Engineering 17(1), 121-132 (2005) 4. Wu, W., Kumar, V.: The Top Ten Algorithms in Data Mining. CRC Press, Boca Raton (2009) 5. Mitra, S., Acharya, T.: Data mining: multimedia, soft computing, and bioinformatics. John Wiley, Chichester (2003) 6. Fournier, D., Cremilleux, B.: A quality index for decision tree pruning. Knowledge-Based Systems 15, 37-43 (2002) 7. Quinlan, J.R.: Simplifying decision trees. Int. J. Man-Mach. Stud. 27(3), 221-234 (1987) 8. Bensaid, A.M., Hall, L.O., Bezdek, J.C., Clarke, L.P., Silbiger, M.L., Arrington, J.A., Murtagh, R.F.: Validity-guided (Re)Clustering with applications to image segmentation. IEEE Transactions on Fuzzy Systems 4, 112-123 (1996) 9. Xie, X.L., Beni, G.A.: Validity measure for fuzzy clustering. IEEE Trans. PAMI 3(8), 841-846 (1991) 10. Mao, K.Z.: Identifying critical variables of principal componentes for unsupervised feature selection. IEEE T. on Systems, Man, and Cybernetics 35(2), 339-344 (2005) 11. Malhi, A., Gao, R.X.: Pca-based feature selection scheme for machine defect classification. IEEE T. on Instrumentation and Measurement 53(6), 1517-1525 (2004) 12. KBCT: Knowledge Base Configuration Tool, http://www.mat.upm.es/projects/advocate/en/index.html 13. Guillaume, S., Charnomordic, B.: A new method for inducing a set of interpretable fuzzy partitions and fuzzy inference systems from data. Studies in Fuzziness and Soft Computing, pp. 148-175. Springer, Heidelberg (2003) 14. Guillaume, S., Charnomordic, B.: Generating an interpretable family of fuzzy partitions. IEEE Transactions on Fuzzy Systems 12(3), 324-335 (2004) 15. Hartigan, J.A., Wong, M.: A k-means clustering algorithm. Applied Statistics 28, 100-108 (1979) 16. Bezdek, J.C.: Pattern recognition with fuzzy objective functions algorithms. Plenum Press, New York (1981) 17. Chen, M.Y.: Establishing interpretable fuzzy models from numeric data. In: Proceedings of the 4th World Congress on Intelligent Control and Automation, pp. 1857-1861 (2002) 18. Zhou, S., Ganb, J.Q.: Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling. Fuzzy Sets and Systems 159, 3091-3131 (2008)", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_51", "figure_caption": "Fig. 1 .Fig. 2 .12Fig. 1. General structure of the system", "figure_data": ""}, {"figure_label": "3456", "figure_type": "figure", "figure_id": "fig_52", "figure_caption": "Fig. 3 .Fig. 4 .Fig. 5 .Fig. 6 .3456Fig. 3. Membership functions of the inputs and output variables fuzzy sets defined in specific approximate rules for Prays modeling", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_53", "figure_caption": "Fig. 7 .7Fig. 7. Input fuzzy sets for Repilo modeling, in reduced KB", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_54", "figure_caption": "Fig. 1 .1Fig. 1. Water tank system", "figure_data": ""}, {"figure_label": "42", "figure_type": "figure", "figure_id": "fig_55", "figure_caption": "4x4 4 .Fig. 2 .42Fig. 2. Tracking of the reference in the water tank system. (a) At the beginning. (b) After one hour.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_56", "figure_caption": "Fig. 3 .3Fig. 3. Reference tracking for the water tank plant when the plant's dynamics change", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_57", "figure_caption": "2, 3 is the index of the fuzzy rules, b 1 = b 2 = b 3 = 0.7659, c 1 = \u2212520.2638, c 2 = \u2212253.5771, c 3 = \u2212125.1030, d 1 = 2220.9067, d 2 = 1102.4471, d 3 = 563.8767. The membership functions are shown in Fig. 1.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_58", "figure_caption": "Fig. 1 .Fig. 2 .12Fig. 1. Membership functions of the control plant model", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_59", "figure_caption": "a j , I r a j ] = [aj \u2212 (bj \u2212 aj )/2, aj + (bj \u2212 aj)/2] , [I l b j , I r b j ] = [bj \u2212 (bj \u2212 aj)/2, bj + (cj \u2212 bj)/2] , [I l c j , I r c j ] = [cj \u2212 (cj \u2212 bj)/2, cj + (cj \u2212 bj)/2] .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_60", "figure_caption": "Fig. 1 .1Fig. 1. Tuning by changing the basic MF parameters and Variation intervals", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_61", "figure_caption": "-MFs displacement (\u03b4): This metric measures the proximity of the central points of the MFs to the original ones. -MFs lateral amplitude rate (\u03b3): This metric measures the left/right rate differences of the tuned and the original MFs. -MFs area similarity (\u03c1): This metric measures the area similarity of the tuned MFs and the original ones. 2.1 MFs Displacement Measure (\u03b4) This metric can control the displacements in the central point of the MFs. It is based on computing the normalized distance between the central point of the tuned MF and the central point of the original MF, and it is calculated through obtaining the maximum displacement obtained on all the MFs. For each M F j in the DB, we define \u03b4 j = |b j \u2212 b j |/I , where I = (I r bj \u2212 I l bj", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_62", "figure_caption": "1 .1Predicting the Weather in Izmir (WIZ): 10 variables and 1461 examples. 2. Predicting the Mortgage Rate (MOR): 16 variables and 1049 examples. The methods considered for the experiments are: -T S which performs the rule selection and a tuning of the MFs by only considering the accuracy of the model as the sole objective [13]. -T SP 2\u2212SI , the MOEA applying tuning with GM3M as a second objective [7]. -T S SP 2\u2212SI is the presented MOEA for the combination of rule selection and the tuning considering the three objectives mentioned previously.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_63", "figure_caption": "MOR) of improvement in GM3M with respect to the T SP 2\u2212SI method. -T S SP 2\u2212SI has obtained RBs with almost a half of the rules obtained by T S. A great number of rules has been eliminated (more than 60 rules) with respect to the initial RBs obtained with W M and with respect to the tuned DB obtained in T SP 2\u2212SI .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_64", "figure_caption": "Figure 22Figure 2 shows the Pareto front obtained with T S SP 2\u2212SI in MOR, together with the projections in the MSE-Rules, MSE-GM3M and Rules-GM3M planes, and the solution obtained by T S and T SP 2\u2212SI in the corresponding planes. The solutions obtained with T SP 2\u2212SI and T S are dominated by several solutions from T S SP 2\u2212SI . Moreover, the obtained Pareto front is quite wide and allows selecting solutions with different degrees of accuracy and interpretability.", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_65", "figure_caption": "Fig. 2 .Fig. 3 .23Fig. 2. Pareto Front obtained with the results of a single trial in MOR", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_66", "figure_caption": "FigureFigure3presents a DB obtained with T S and some DBs obtained with the presented method in WIZ. For T S SP 2\u2212SI it includes three DBs, one with the most accurate solution, other with a solution not only accurate also interpretable and another highly interpretable DB, that obtains 40% of improvement with respect to the WM method with a value of GM3M near to 1.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_67", "figure_caption": "Fig. 1 .1Fig. 1. Example of homoskedastic series (left) and heteroskedastic series (right)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_68", "figure_caption": "Fig. 1 .1Fig. 1. Map of Castro City -Bus Stops (morning)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_69", "figure_caption": "n1= number of the students capability of the vehicles , n2= \u2212 dist (m,school (m)) dist(j,school (j)) dist(m,school (m)) longer_route=dist(j,school((j))(1+(1+n1)n2) Let the part of the route Rm={j\u2192school(j)}. Repeat while ck=\u221e: {For each student i\u2208S, calculate ci=comp_route(Rm,i,school(i)). Let ck=mini\u2208S {ci}. If ck < \u221e then: Let Rm\u2190route(Rm,k,school((k)). Let S\u2190S\\{k}; } } If the number of customers in Rm is less or equal than Cv and comp_route(Rm)\u2264 mlonger_route: m = m+1; Otherwise", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_70", "figure_caption": "Fig. 1 .1Fig. 1. Convergence of the PCA weight vectors to the optimal directions by the PSObased PCA method. The vertical axis shows the cosine of the angle between the current filter and the optimal filter. The horizontal axis shows the number of iterations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_71", "figure_caption": "Fig. 2 .2Fig. 2. Convergence of the PCA weight vector to the optimal directions. Solid line: by PSO-based Q-learning algorithm in this section. Magenta Dashed line: by immediate reinforcement learning method. Red dot dashed line: by standard Q-learning method.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_72", "figure_caption": "Fig. 3 .3Fig. 3. The data are shown by '+'s and the latent points' projections are '*'s", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_73", "figure_caption": "Fig. 1 .1Fig. 1. Terminal Based RepresentationFig. 2 illustrates an assignment to a problem with N = 10 terminal sites and M = 3 concentrator sites. The figure shows the coordinates for the concentrators, terminal sites and also their capacities.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_74", "figure_caption": "Fig. 2 .2Fig. 2. TA Problem -example", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_75", "figure_caption": "c1 = random (number of concentrators) c2 = random (number of concentrators) NN = neighbours of ACTUAL-SOL (one neighbour results of interchange one terminal of c1 or c2 with one terminal of c2 or c1) SOLUTION = FindBest (NN) IF Fitness(ACTUAL-SOL) < Fitness(SOLUTION) THEN NN = neighbours of ACTUAL-SOL (one neighbour results of assign one terminal of c1 to c2 or c2 to c1) SOLUTION = FindBest (NN) IF Fitness(SOLUTION) < Fitness(ACTUAL-SOL) THEN ACTUAL-SOL = SOLUTION ELSE ACTUAL-SOL = SOLUTION", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_76", "figure_caption": "=)= concentrator of terminal t t = terminal c = concentrator M = number of concentrators N = number of terminals Compute probabilities for nbs best sites and A bee is recruited for a best site i, depending on the probability value associated with that site. The probabilities are calculated by the following expression:Compute probabilities for (nss-nbs) selected sites A bee is recruited for a selected site i, depending on the probability value associated with that site. The probabilities are calculated by the following expression:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_77", "figure_caption": "FORn=1 TO nm DO t = random(N) closestC=1 FOR c=1 TO M DO /*find the closest concentrator*/ IF distance (t, c) < distance (t, closestC) closestC=c IF capacityFree (closestC)>= L(t) and mantainBalanced(closestC) THEN Assign terminal t to concentrator closestC ELSE cond=true REPEAT t1 = random(N) t2 = random(N) c1 = solution (t1) c2 = solution (t2) IF ( capacityFree(c2) -L(t2) >= L(t1) and capacityFree(c1) -L(t1) >= L(t2) ) and", "figure_data": ""}, {"figure_label": "374", "figure_type": "figure", "figure_id": "fig_78", "figure_caption": "Fig. 3 . 7 Fig. 4 .374Fig. 3. Number of Bees (ns) -Average Fitness/Execution Time -Problem 7", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_79", "figure_caption": "0,06 270,48 0,15 270,35 0,06 270,32 0,06 270,29 0,04 4 286,90 0,02 287,93 0,75 286,97 0,09 286,91 0,04 286,89 0,00 5 335,34 0,25 336,00 0,66 335,42 0,16 335,11 0,03 335,11 0", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_80", "figure_caption": "Fig. 1 .1Fig. 1. Users and access points", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_81", "figure_caption": "( 2 )2Distance: l ij . (3) Parameter of barrier: e ij = 1, barrer exists, 0, barrier is absent. (4) Integrated parameter (barrier & distance) (L max = max {(i,j)} l ij ):", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_82", "figure_caption": "3, 5}, and \u0398 22 = {4, 6}.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_83", "figure_caption": "Fig. 3 .3Fig. 3. Assignment of users (version 1)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_84", "figure_caption": "Access points j x j y j z j f j n j r j d j", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_86", "figure_caption": "Table", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_87", "figure_caption": "Fig. 4 .4Fig. 4. Assignment of users (version 2)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_88", "figure_caption": "3 .3Solving methods: implementation of various solving schemes and comparison studies for different schemes. The preliminary material for the article was prepared within framework of a faculty course \"Design of Systems\" in Moscow Institute of Physics and Technology (State University) (creator and lecturer: M.Sh. Levin) [13] as laboratory work 9 (student: M.V. Petukhov) and BS-thesis of M.V. Petukhov (2008, advisor: M.Sh. Levin).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_89", "figure_caption": "Fig. 1 .1Fig. 1. Trajectories of L\u00e9vy motion: (a) typical normal path \u03b1 = 2; (b) competition between jumps and small fluctuation at \u03b1 = 1.6", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_90", "figure_caption": "Fig. 2 .2Fig. 2. Optimization process of ACORL over test function f5. Standard deviation and mean.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_91", "figure_caption": "1 .1There is a finite set of courses C = c 1 , c 2 , . . . , c |C| and a finite set of time slots T = t 1 , t 2 , . . . , t |T | , which already have been assigned to courses C. This is typically provided as courses occupy time slots. So each course could occupy just one time slot, usually 3 hours; two time slots, usually an hour and half each; or 3 time slots, usually an hour each. For any course, the time slots assigned to it must not overlap. t i can be assigned to different courses as long as they are in different rooms and different professors.", "figure_data": ""}, {"figure_label": "123", "figure_type": "figure", "figure_id": "fig_92", "figure_caption": "Fig. 1 .Fig. 2 .Fig. 3 .123Fig. 1. Professors information page", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_93", "figure_caption": "Fig. 1 .1Fig.1. Micrograph of the micro gyroscope[5]    ", "figure_data": ""}, {"figure_label": "223", "figure_type": "figure", "figure_id": "fig_94", "figure_caption": "Fig. 2 . 2 Fig. 3 .223Fig. 2. Dynamical behavior of the time history: x 1 and x 2", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_95", "figure_caption": "Fig. 4 .4Fig. 4. Stability Diagram for x 1 with the region's control applied", "figure_data": ""}, {"figure_label": "51627", "figure_type": "figure", "figure_id": "fig_96", "figure_caption": "Fig. 5 . 1 Fig. 6 . 2 Fig. 7 .51627Fig. 5. PSO-Controlled and non-controlled time history x 1", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_97", "figure_caption": "repeat for each particlei = 1,\u2026.,S.ns do // set the personal best position if f(S.xi) < f(S.yi) then S.yi = S.xi; end // set the global best position if f (S.yi) < f(S. \u0177 ) then S. \u0177 = S.yi; end end for each particle i=1,\u2026,S.ns do update the velocity using equation (5); update the position using equation (6); end until stopping condition is true;", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_98", "figure_caption": "Fig. 1 .1Fig. 1. A reaction (a) and the corresponding Condensed Graph of Reaction (b)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_99", "figure_caption": "Figure 1 .1(b)  shows the CGR corresponding to the reaction above. Let us emphasize that the bond types assigned between the carbon 6 and the brome 13 denotes a broken single bond and the bond type between carbons 6 and 12 denotes the creation of a single bond.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_101", "figure_caption": "Fig. 4 .4Fig. 4. REC curve (Regression Error Curve) for the three models", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_102", "figure_caption": "Fig. 5 .Fig. 6 .56Fig. 5. Correlation between the actual and predicted values for each model", "figure_data": ""}, {"figure_label": "31", "figure_type": "figure", "figure_id": "fig_103", "figure_caption": "Definition 3 .Fig. 1 .31Fig. 1. Extract of the Incident Model", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_104", "figure_caption": "Algorithm 1 .-1Let M be a model tree, I = {I 1 , . . . , I N } a set of incident trees, and P a prototype tree. Syntactic anti-unification of sets of incident trees au(p, P, M, I) is defined as: Initially the prototype is empty: P.\u03bb = .-We traverse the model tree top-down, starting with M.\u03bb.-For the current position p in model tree M , M.p = e : \u03c4\u2022 If for all incident trees in I holds I 1 .p = . . . = I N .p then P.p := e and if M.p = e(T 1 , . . . , T n ) then for all incidents I i do au(p.i, P, M, {T 1i , . .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_105", "figure_caption": "Algorithm 2 .-2Let M be a model tree, I = {I 1 , . . . , I N } a set of incident trees, and P a prototype tree. Structure dominance tree generalisation sdtg(p, P, M, I) is defined as: Initially the prototype is empty: P.\u03bb = .-We traverse the model tree top-down, starting with M.\u03bb. -Until I is empty, for the current position p in model tree M , M.p = e : \u03c4 \u2022 If for all incident tree in I holds I 1 .p = . . . = I N .p then P.p := e : \u03c4 and if M.p = e(T 1 , . . . , T n ) then for all incidents I i do sdtg(p.i, P, M, {T 1i , . . . , T Ni }). \u2022 Else for each of the c = |{I 1 .p, . . . , I N .p}| different elements create a new node P.p 1 , . . . , P.p c with P.p i := [e i /f i ] : \u03c4 as element names e i and their relative frequencies f i . Proceed for all new elements with sdtg(p.i, P, M, {T 1i , . .", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_106", "figure_caption": "Fig. 2 .Algorithm 3 .23Fig. 2. Illustration of Prototype Generation", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_107", "figure_caption": "Taskar, B., Segal, E., Koller, D.: Probabilistic clustering in relational data. In: Seventeenth International Joint Conference on Artificial Intelligence (IJCAI 2001), pp. 870-887 (2001) Wang, J.T.L., Zhang, K., Jeong, K., Shasha, D.: A system for approximate tree matching. IEEE Transactions on Knowledge and Data Engineering 6(4), 559-571 (1994) Wiese, E., Konerding, U., Schmid, U.: Mapping and inference in analogical problem solving -As much as needed or as much as possible? In: Love, B., McRae, K., Sloutsky, V.M. (eds.) Proceedings of the 30th Annual Conference of the Cognitive Science Sociecty, pp. 927-932. Lawrence Erlbaum, Mahwah (2008) Wilson, D.R., Martinez, T.R.: The potential of prototype styles of generalization. In: Proceedings of the Sixth Australian Joint Conference on Artifical Intelligence (AI 1993), pp. 356-361 (1993) Yan, X., Zhu, F., Yu, P.S., Han, J.: Feature-based similarity search in graph structures. ACM Transactions on Database Systems 31(4), 1418-1453 (2006) Zadeh, L.: A note on prototype theory and fuzzy sets.Cognition 12, 291-297 (1982)   ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_108", "figure_caption": "The research reported here is supported in part under grant TIN2008-06247 from the MICINN (Ministerio de Ciencia e Innovaci\u00f3n, of Spain), and grant 2009/07366-5 from the FAPESP (Funda\u00e7\u00e3o de Amparo\u00e0 Pesquisa do Estado de S\u00e3o Paulo, Brazil).N. Garc\u00eda-Pedrajas et al. (Eds.): IEA/AIE 2010, Part II, LNAI 6097, pp. 337-346, 2010. c Springer-Verlag Berlin Heidelberg 2010", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_109", "figure_caption": "-Fruit load of the plantation: low (1) or high (2) -Spacing between plants: dense (1) or thin (2) -Percentage of leaves infected by fungi in date d 0 -Days from d 0 till now (the day we make the prediction) -Days from now till the target day: 1 month, 25, 20, 15 and 10 days -Weather scores in the last 45 days", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_110", "figure_caption": "Fig. 1 .1Fig.1. True incidence percentages ( ) and the predicted intervals by a regressor h ND(4)  . The horizontal axis represents the indexes of samples ordered according to their predictions. We only included predictions made one month in advance to make the figure more clear. The proportion of points outside the tube are similar if we vary the days ahead of predictions. The horizontal dashed line represents the threshold \u03c4 = 4.5.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_112", "figure_caption": "Table 1 .1An example correlation matrix, from one month data (957 measurements on GDA-curl.nedmirror.nl path). Significant predictor-dependent variable pairs are highlighted. LogGpt is better correlated with measured values. Day of week (DoW) in this dataset is less relevant (possibly the coverage is too low to exhibit weekly trends). Dns D2S Con A2G First Last Hour DoW LogGpt Gpt Dns 1.000 0.164 0.046 0.013 0.060 0.024 0.090 -0.070 -0.604 -0.523 D2S 0.165 1.000 -0.007 0.004 0.037 0.057 0.048 -0.051 -0.164 -0.177 Con 0.047 -0.007 1.000 0.105 0.124 0.033 0.008 -0.018 -0.409 -0.261 A2G 0.013 0.004 0.105 1.000 0.060 0.156 0.029 -0.042 -0.139 -0.115 First 0.060 0.037 0.124 0.061 1.000 0.115 0.053 -0.041 -0.421 -0.335 Last 0.023 0.057 0.033 0.155 0.115 1.000 0.127 -0.031 -0.501 -0.462 Hour 0.090 0.048 0.008 0.029 0.0534 0.127 1.000 -0.016 -0.192 -0.249 DoW -0.070 -0.051 -0.018 -0.042 -0.041 -0.031 -0.016 1.000 0.082 0.085 LogGpt -0.604 -0.163 -0.409 -0.139 -0.421 -0.501 -0.192 0.082 1.000 0.940 Gpt -0.522 -0.177 -0.261 -0.113 -0.335 -0.462 -0.249 0.086 0.940 1.000", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_113", "figure_caption": "Fig. 2 .2Fig. 2. The autocorrelation estimate for an excerpt of observations (one month), as the function of time lag. On the left: a predictable path. On the right: a path with lower predictability (autocorrelation quickly drops to 0).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_114", "figure_caption": "Fig. 3 .3Fig. 3. An example result of EM clustering on a single client-server path. The histogram is used to estimate the gaussian mixture of 8 components. The dots denote the locations of means \u03bci, while vertical lines separate performance classes in points xi.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_115", "figure_caption": "Fig. 4 .4Fig. 4. The ratio of correctly classified goodput levels for WRO client and 25 web servers. Comparison of performance of neural network and nearest neighbor models for w = 672 (2 weeks window).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_116", "figure_caption": "Fig. 5 .5Fig. 5. Subsequent clustering updates for one observed end-to-end network path", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_117", "figure_caption": "Fig. 6 .6Fig.6. Mean regression error of LogGpt using neural network based predictor. The GDA client measurements gave better results in many datasets.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_118", "figure_caption": "Fig. 3 .Fig. 4 .34Fig. 3. Performance assessment by the Kolmogorov-Smirnov metric with Max_KS=0.341", "figure_data": ""}, {"figure_label": "1456", "figure_type": "figure", "figure_id": "fig_119", "figure_caption": "- 1 . 4 . 5 . 6 .1456Purpose specification: The provider must know the objectives of the sensitive data transaction. -2. Consent: Each sensitive data transaction requires the provider's consent. -3. Limited collection: The consumer commits to cutting down the amount of data for realizing its objectives to a minimum. -Limited use: The consumer commits to using sensitive provider's data only to satisfy the objectives that it has specified and nothing more. -Limited disclosure: The consumer commits to only disclosing the sensitive data needed to reach its objectives. -Limited retention: The consumer commits to retaining sensitive data only for the minimum amount of time needed to perform its objectives. -7. Safety: The system must guarantee sensitive data safety during storage and transactions. -8. Openness: The transmitted sensitive data must remain accessible to the provider during the retention time. -9. Compliance: Each agent should be able to check the obedience to the previous principles. Seven of these principles ( 1.-2.-3.-4.-5.-6.-8.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_120", "figure_caption": "-DoDR c,\u03a9,f (Degree of Direct Reputation): the provider belief about all the direct experiences (policies respect) with the consumer c according to the facet f and the context \u03a9; -DoP R c,\u03a9,f (Degree of Propagated Reputation): the provider belief in relation to the recommendation about the respect of the policies for the consumer c according to the facet f and the context \u03a9; -DoSR c,\u03a9,f (Degree of Stereotyped Reputation): the provider belief in relation to the characteristics of the consumer f on its capacity to respect its policy according to the facet f and the context \u03a9.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_121", "figure_caption": "Fig. 1 .1Fig. 1. Sensitive data transaction integrating hippocratic social order", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_122", "figure_caption": "Fig. 2 .2Fig. 2. Average of required sensitive data transactions per agent with the suspicious agent for the detection of the suspicious agent according to the punishment and threshold value", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_123", "figure_caption": "Fig. 3 .3Fig. 3. Average of required sensitive data transactions per agent with the suspicious agent for the detection of the suspicious agent according to the networks topology", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_124", "figure_caption": "Fig. 4 .4Fig. 4. Average of required sensitive data transactions per agent for the detection of the suspicious agent according to the number of suspicious agents", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_125", "figure_caption": "Fig. 1 .1Fig. 1. The architecture of the agent-based data mining system", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_126", "figure_caption": "u s and I (u,t) s constructs a new world model, W t s . The illocutions in the communication language C include information, [info]. The information received from general information sources will be expressed in terms defined by \u03a0's ontology. The procedure for updating the world model as [info] is received follows. If at time u, \u03a0 receives a message containing [info] it is time-stamped and source-stamped [info] (\u03a9,\u03a0,u) , and placed in a repository Y t . If \u03a0 has an active plan, s, with model building function, J s , then J s is applied to [info] (\u03a9,\u03a0,u) to derive constraints on some, or none, of \u03a0's distributions. The extent to which those constraints are permitted to effect the distributions is determined by a value for the reliability of \u03a9, R t (\u03a0, \u03a9, O([info])), where O([info]) is the ontological context of [info].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_127", "figure_caption": "be that distribution if [fact] had been received instead. Suppose that the reliability estimate for distribution D was R s D . This section is concerned with what R s D should have been in the light of knowing now, at time t, that [info] should have been [fact], and how that knowledge effects our current reliability estimate for D, R t (\u03a0, \u03a9, O([info])).", "figure_data": ""}, {"figure_label": ".", "figure_type": "figure", "figure_id": "fig_128", "figure_caption": "|) ..Then for each ontological context o j , at time t when, perhaps, a chunk of [info], with O([info]) = o k , may have been verified with [fact]:", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_129", "figure_caption": "Definition 4 .4The notation [\u03c6] denotes the set of all models (i.e., interpretations satisfying \u03c6) of a formula \u03c6 \u2208 L: [\u03c6] = {I \u2208 \u03a9 : I |= \u03c6}. Likewise, if S \u2286 L is a set of formulas, [S] = {I \u2208 \u03a9 : \u2200\u03c6 \u2208 S, I |= \u03c6} = \u03c6\u2208S [\u03c6].", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_130", "figure_caption": "Definition 7 (7Mental State). The state of an agent is completely described by a pair S = \u03c0, R J , where -\u03c0 is a possibility distribution which induces the agent's beliefs B; -R J is a set of desire-generation rules which, together with B, induce a qualitative utility assignment u. Example. Dr. A. Gent has submitted a paper to IEAAIE 2010 he has written with his co-author I. M. Flaky, who has promised to go to C\u00f3rdoba to present it if it is accepted.", "figure_data": ""}, {"figure_label": "01", "figure_type": "figure", "figure_id": "fig_131", "figure_caption": "h 0 0 0 0 0 ;Fig. 1 .01Fig.1. Dr. Gent's initial and final possibility distribution. Interpretations have been grouped together where possible, due to lack of space: when no literal appears for a given atom in a row or column heading, it is understood that the row or column applies for both truth assignments.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_132", "figure_caption": "Foran unconditional rule R = \u03b1 R \u21d2 + D \u03c6, Deg(R) = \u03b1 R . Let us denote by R I J = {R \u2208 R J : I |= rhs(R)} the subset of R J containing just the rules whose right-hand side would be true in world I. Definition 11 (Desired Worlds). The qualitative utility assignment u : \u03a9 \u2192 [0, 1]", "figure_data": ""}, {"figure_label": "02", "figure_type": "figure", "figure_id": "fig_133", "figure_caption": "\u00acp \u00acp \u00acp p p \u00acr \u00acr r \u00acr r \u00act t \u00ach 0 Fig. 2 .02Fig. 2. Dr. Gent's final qualitative utility distribution", "figure_data": ""}, {"figure_label": "123", "figure_type": "figure", "figure_id": "fig_134", "figure_caption": "( 1 )( 2 ) 3 )123Updating world economy x new = x old + (a*x old -b*x old * y old ) * \u2206t Updating the greed of the population y new = y old + (c*b* x old *y old *(2-y old ) / TD old -e*y old ) * \u2206t (Updating the technological development", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_135", "figure_caption": "Fig. 1 .Fig. 2 .12Fig. 1. Agent-based simulation results: a) world economy, b) individual greed of 25 agents, and c) average greed (over 25 agents)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_136", "figure_caption": "Fig. 3 .3Fig. 3. Difference between both models for various population sizes: a) world economy, and b) greed", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_137", "figure_caption": "(2) as follows dy/dt = (cbxy(2-y) / TD -ey) = 0 \u21d2 cbx (2-y) / TD = e \u21d2 x = (e / ((2b-a) c)) TD This provides the conditions for a full equilibrium (1) y = a /b (2) x = (e / ((2b-a) c)) TD (3) inn = 0", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_138", "figure_caption": "From the second equation (2) by putting dy/dt = 0 it follows cbx (2-y) / TD = e \u21d2 x = \u03b1 TD with \u03b1 = e / cb (2-y) By filling this in differential equation (1) it follows d \u03b1 TD /dt = a\u03b1 TD -b\u03b1 TD y \u21d2 d TD /dt = (a -by) TD By differential equation (3) it can be derived d TD /dt = (a -by) TD = inn TD \u21d2 (a -by) = inn \u21d2 y = (a -inn)/b", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_139", "figure_caption": "\u03b1= e / cb (2-y) = e / cb (2-(a -inn)/b) = (e / c) / (2b -a +inn)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_140", "figure_caption": "This provides(1) z = a /b (2) y (k) =2-e k TD/(c k bx) (3) inn = 0 (4) z = ( \u03a3 k y (k) )/n From the second, first and last equation it follows that a /b = (\u03a3 k y (k) )/n = (\u03a3 k (2-e k TD/(c k bx) ) )/n = 2 -\u03a3 k (e k TD/(c k bx) ) /n = 2 -(TD/bx) (\u03a3 k (e k /c k ) )/n \u21d2 x = TD \u03a3 k (e k /c k ) / (2b -a)n", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_141", "figure_caption": "Table 5 .5e j TD/(c j bx) = 2-e j TD/(c j b TD \u03a3 k (e k /c k ) / (2b -a)n) = 2-e j /(c j b \u03a3 k (e k /c k ) / (2b -a)n) = 2-e j (2b -a)n /(c j b \u03a3 k (e k /c k ) ) = 2-e j (2 -(a/b))n /(c j \u03a3 k (e k /c k ) ) = 2-(2 -(a/b))n /( \u03a3 k (e k / e j )(c j / c k ) ) It turns out that for any nonzero setting for the parameters a, b, c k and e k and for setting inn = 0 for the innovation parameter, and for any value of TD a nontrivial equilibrium is (only) possible with values as indicated above. Equilibria for greed for the agent-based model. From the second equation c k bx (2y (k) ) / TD = e k with y (k) constant it follows that x = \u03b1 k TD with \u03b1 k the constant \u03b1 k = e k / c k b (2-y (k) ) which apparently does not depend on k, as both x and TD do not depend on k, so the subscript in \u03b1 k can be left out. Filling this in (1) provides: d \u03b1 TD/dt = (a\u03b1 TD -b\u03b1 TD z) \u21d2 d TD/dt = (a -bz) TD By differential equation (3) it can be deriveddTD/dt = (a -bz) TD = inn TD \u21d2 (a -bz) = inn \u21d2 z = (a -inn)/bNow the equilibrium values for y(j)  can be determined as follows.\u03b1= e k / c k b (2-y (k) ) \u21d2 2-y (k) = e k / \u03b1 c k b \u21d2 y (k) = 2-e k / c k \u03b1b Next the value of \u03b1 is determined z = (\u03a3 k y (k) )/n = \u03a3 k (2-e k / c k \u03b1b)/n = 2-(1/\u03b1bn) \u03a3 k e k / c k . Since z = (a -inn)/b it follows (a -inn)/b = 2-(1/\u03b1bn) \u03a3 k e k / c k \u21d2 (1/n\u03b1) \u03a3 k e k / c k = 2b-(a -inn) \u21d2 \u03a3 k e k / c k = (2b-(a -inn)) n\u03b1 \u21d2 \u03b1 = \u03a3 k (e k / c k )/ (2b-(a -inn))nGiven this value for \u03b1 the equilibrium values for the greed y(j)  arey (j) = 2-e j / c j \u03b1b = 2-e j / b c j \u03a3 k (e k / c k ) / (2b-(a -inn))n = 2-(2-(a -inn ) /b) n / \u03a3 k (e k c j / e j c k ) Overview of the equilibria of the two models 1/(2b -a)) (\u03a3k (ek /ck) / n) TD z = a /b y (j) = 2-(2 -(a/b)) n / \u03a3k (ek / ej )(cj / ck) inn = 0 x = (1/(2b-a))( e /c)) TD y = a/b Partial equilibrium for greed", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_142", "figure_caption": "Fig. 1 .1Fig. 1. Structure of model checking communities of web services (CWSs)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_143", "figure_caption": "Fig. 2 .2Fig. 2. Activity diagram of P NAW S protocol", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_144", "figure_caption": "Fig. 1 .1Fig. 1. Cognitive model for food and medicine intake", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_145", "figure_caption": "Fig. 2 .2Fig. 2.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_146", "figure_caption": "Fig. 3 .3Fig. 3.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_147", "figure_caption": "Fig. 1 .Fig. 2 .12Fig. 1. System physical architecture", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_148", "figure_caption": "Fig. 3 .3Fig. 3. On-board context analysis -preliminary results", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_149", "figure_caption": "Fig. 4 (4b)). In this application a mask is applied to exclude areas of non-interest. The algorithm receives as input the coordinates of the points in the binary image and provides a parametric description of the recognized curves, belonging to a given analytic set (in our case the lines); As it can be observed from Fig. (Fig. 4(b)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_150", "figure_caption": "Fig. 4 .4Fig. 4. a) Image of extracted edges; b) Lines detected using Hough transform algorithm; c) Road segmentation image; d) Extracted lines after elimination of not consistent lines", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_151", "figure_caption": "Fig. 5 .5Fig. 5. Contextual data extraction GUI", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_152", "figure_caption": "Fig. 2 .2Fig. 2. Architecture schema for the home telecare platform", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_153", "figure_caption": "Fig. 1 .1Fig. 1. Diagram of implementation", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_154", "figure_caption": "Fig. 2 . 1 .21Fig. 2. Accelerometer 3-axis", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_155", "figure_caption": "Fig. 3 .3Fig. 3. Accelerometer signal into temporal window", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_156", "figure_caption": "Definition 5 .5Denoted by the most likely based activity \u03a9 (\u2202) obtained for each activity \u2202: max \u03a9", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_157", "figure_caption": "-Running: \u03a9 (Run) = 21+17+20+10+5 = 73 -Walking: \u03a9 (Walk) = 35+27+31+37+31 = 161 -Jumping: \u03a9 (Jump) = 19+30+26+27+29 = 131 Once obtained the \u03a9 (\u2202) for each of the activities, based on Definition 5, most likely activity ( is chosen, whose sum \u03a9 has maximum value. For the present example, this activity is walking.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_158", "figure_caption": "Fig. 5 .5Fig. 5. Overlearned system", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_159", "figure_caption": "Fig. 2 .2Fig. 2. Differential Search space in Query processing (3 Cases)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_160", "figure_caption": "Fig. 3 .3Fig. 3. Plot for Convergence of DE Under different Scale Factor in Query String", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_161", "figure_caption": "Fig. 1 .1Fig. 1. The Ad Placement Scenario", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_162", "figure_caption": "m i=0 a i b i a b ( 2 ) 1 generatePage(21In Equation 2, a and b are vectors of size m with a and b being the lengths of the vectors a and b respectively. If the angle computed for the page is greater than some chosen threshold, R thresh , it is rejected. If the page is rejected, then each of AlgorithmArray userInterest) page = new Array[m] for i = 0 to m \u2212 1 do page[i] = random value from \u22120.25 to 1.25 if page[i] > 1 then page[i] = page[i] -0.25 end if if page[i] < 0 then page[i] = page[i] + 0.25 end if end for angle = angle between page and userInterest using Equation", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_163", "figure_caption": "Algorithm 3 .3Ad Association (A2) Algorithm decideOnAd(Array page) angles = new Array[n] for a = 0 to n \u2212 1 do angleT oAd = angle between page and ads[a] using Equation 2 angleT oV = angle between page and v[a] using Equation 2 angles[a] = (angleT oAd \u03b2 ) \u00d7 (angleT oV \u03b1 ) end for angles = sortInIncreasingOrder(angles) index = 0 while true do if index >= n then return last ad in angles array else 50% chance of returning ad with angle at index Otherwise: index = index + 1 end if end while recordAd(adN umber, Array page, success) if success == true then for i = 0 to m \u2212 1 do v[adN umber][i] = v[adN umber][i] + page[i] * c end for else for i = 0 to m \u2212 1 do v[adN umber][i] = v[adN umber][i] \u2212 page[i] * k end for end if", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_164", "figure_caption": "Fig. 2 .2Fig. 2. Success Rate Variation with Block Number", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_165", "figure_caption": "Fig. 1 .1Fig. 1. Ontology axioms (Tbox) and semantic annotations (Abox)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_166", "figure_caption": "Fig. 2 .2Fig. 2. Arquitecture of our proposal for mining semantic annotations", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_167", "figure_caption": "Definition 2 .Definition 3 .23P ath(C, C ) = (r 1 \u2022...\u2022r n ) \u2208 P aths(C, C ) is an aggregation path from concept C to concept C of an ontology O iff O |= C \u2203 r 1 \u2022 ... \u2022 r n .C . Let O be an ontology, C T the analysis target and C a , C b two named concepts. Contexts(C a , C b , C T ) = {C /CC } are least common reachable concepts and their subconcepts. That is,(1) \u2203p 1 \u2208 P aths(C a , C ) \u2227 \u2203p 2 \u2208 P aths(C b , C ) \u2227 \u2203p 3 \u2208 P aths(C , C T ) (C is common reachable concept).(2) if \u2203p x \u2208 P aths(C a , E) \u2227 \u2203p y \u2208 P aths(C b , E) then \u2203p z \u2208 P aths(C , E) (C is least).", "figure_data": ""}, {"figure_label": "63", "figure_type": "figure", "figure_id": "fig_168", "figure_caption": "Definition 6 .Example 3 .63Let O be an ontology and IS an instance store consistent with O.Two instances i a , i b , C(i a ) = C(i b ), C(i a ), C(i b ) \u2208 f eatures belong to the same transaction under a target instance i T iff i a , i b are context compatible. That is, \u2203 C 1 \u2208 {C(i x )/i x \u2208 Contexts(i a , i b , i T )}, \u2203 C 2 \u2208 Contexts(C(i a ), C(i b ), C T ) such that O |= C 1 C 2 where C(i)is the asserted class for instance i in IS. In the right hand side of Fig.5, instances M ethotrexate and P olyArthritis are context compatible. That is, their context is Report both at the instance level (Abox) and at the conceptual level (Tbox).", "figure_data": ""}, {"figure_label": "57", "figure_type": "figure", "figure_id": "fig_169", "figure_caption": "Fig. 5 .Definition 7 .57Fig. 5. Ontology graph and instance store fragment", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_170", "figure_caption": "Fig. 1 .1Fig. 1. An example of concept hierarchy of research topics", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_171", "figure_caption": "Fig. 1 .1Fig. 1. Arranging events by automatic event detection and user's selection", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_172", "figure_caption": "Fig. 2 .2Fig. 2. Arranging events by repeating of event presentation and user selection", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_173", "figure_caption": "Fig. 4 .4Fig. 4. Structure of event arrangement system", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_174", "figure_caption": "Fig. 6 .Fig. 7 .67Fig. 6. Events the system presented after selecting event", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_175", "figure_caption": "Fig. 8 .8Fig. 8. An event arrangement by another event selection", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_176", "figure_caption": "Fig. 1 .1Fig. 1. System Architecture", "figure_data": ""}, {"figure_label": "124", "figure_type": "figure", "figure_id": "fig_177", "figure_caption": "1 . 2 . 4 .124Incentive of looking for a Potential Affiliate (PA) Actual search for a PA (a) Decide for a tool: search engine, directory, contacts, social networks. (b) Find PA with related topics. (c) Judge the quality of the PA. (d) Match the PA's number of visitors against personal expectation. 3. Negotiations for equity on both sides. (a) Find the right contact for affiliation procedures if there are several. (b) Quality evaluation from the PA. (c) Popularity evaluation from the PA. (d) Consider placement of each one's link on the other's page. Final agreement.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_178", "figure_caption": "\u2200s \u2208 S, s = (i, c, e, v, q, Hs, a) i s 's identifier (URI) c \u2208 C s 's category, a set of keywords related to a similar general topic e \u2208 ]\u2212\u221e, +\u221e[ s's expectation of fairness v \u2208 [0, +\u221e[ s's visitor revenue per iteration q \u2208 [0, 10]", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_179", "figure_caption": "Fig. 2 .2Fig. 2. Matchmaking algorithm", "figure_data": ""}, {"figure_label": "3456", "figure_type": "figure", "figure_id": "fig_180", "figure_caption": "Fig. 3 .Fig. 4 .Fig. 5 .Fig. 6 .3456Fig. 3. No Affiliation, t=1 Fig. 4. No Affiliation, t=1500", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_181", "figure_caption": "Fig. 1 .1Fig. 1. Fragment of the WordNet taxonomy. Solid lines represent IS-A links; dashed lines indicate that some intervening nodes were omitted to save space.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_182", "figure_caption": "Sim res (c 1 , c 2 ) = \u03b1 * IC(lcs(c 1 , c 2 )\u2212\u03b2 * (IC(c 1 )+IC(c 2 )\u22122 * IC(lcs(c 1 , c 2 )) (9)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_183", "figure_caption": "N (c) is the number of occurrences, both explicit and implicit, of concept c and occ(c) is the number of lexicalizations of c occurring in the document. Given the ontology base I = b 1 , . . . , b n , where the b i s are the base concepts, the quantity of information, info(b i ), pertaining to base concept b i in a document is: info(", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_184", "figure_caption": "Fig. 1 .1Fig. 1. Ontology representation for concept 'z'", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_185", "figure_caption": "Fig. 2 .2Fig. 2. Ontology representation for concept 'y'", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_187", "figure_caption": "Fig. 3 .3Fig. 3. Precision/recall results", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_188", "figure_caption": "1 Fig. 1 .11Fig. 1. Overview of the simulation architecture (from [1])", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_189", "figure_caption": "For each agent: 1 .1determine current delinquency 2. determine individual characteristics 3. compose the social network (friends) 4. calculate average delinquency of social network 5. calculate new delinquency, using information from step 1, 2, and 4", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_190", "figure_caption": "Fig. 2 .2Fig. 2. ROC curves for a) model 12 and b) model 13 against a random prediction", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_192", "figure_caption": "Fig. 1 .1Fig. 1. Definition of utterance timing", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_193", "figure_caption": "Fig. 2 .2Fig. 2. Timing distribution in Cond. A", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_194", "figure_caption": "Fig. 3 .3Fig. 3. Timing distribution in Cond. B", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_195", "figure_caption": "Fig. 4 .4Fig. 4. Flow of identifying user's referent", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_196", "figure_caption": "Fig. 5 .5Fig. 5. System architecture", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_197", "figure_caption": "-the first part corresponds to the left part of the scheme which represents a fault tree (F T ) defining all possible causes leading to the (T E). These causes can be classified into two kinds: The first are the initiator events (IE) which are the principal causes of the T E, and the second are the undesired or critical events (IndE and CE) which are the causes of the IE. The construction of the left part proceeds in top down manner (from T E to IndE and CE). The relationships between events and causes are represented by means of logical AND and OR gates. -The second part corresponds to the right part of the scheme which represents an event tree (ET ) to reach all possible consequences of the T E. These consequences can be classified into three kinds: second events (SE) which are the principal consequences of the T E, dangerous effects (DE) which are the dangerous consequences of the SE and finally majors events (M E) of each DE. The construction of the event tree proceeds as the fault tree i.e. in top down manner.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_198", "figure_caption": "Fig. 1 .1Fig. 1. A bow tie diagram model", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_199", "figure_caption": "Algorithm 2 .2Learning bow ties Data: T S F T ; T S ET ; T E Result: BT begin learning structure T F T \u2190 Learning undirected tree structure(T S F T , T E); T ET \u2190 Learning undirected tree structure(T S ET , T E); Orient arcs in T F T towards T E ; Orient arcs in T ET backwards T E; T \u2190 {T F T , T ET }; learning parameters", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_200", "figure_caption": "Fig. 1 .1Fig. 1. A comparison of a two-second segment of the raw signal y (2)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_201", "figure_caption": "Fig. 2 .2Fig. 2. Periodogram comparison of 1 person and 2 person trials", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_202", "figure_caption": "Fig. 3 .3Fig. 3. The autocorrelation function for a single period averaged over all sensors. (a) 1 person. (b) 2 persons. X-axis \u03c4 is in seconds. Y -axis is ri,j(\u03c4 ).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_203", "figure_caption": "Fig. 4 .4Fig. 4.Comparison of autocorrelation functions. The shaded region shows the difference in area which, when integrated, gives the mean-square error between the template and the ACF for the given trial. X-axis: \u03c4 , Y -axis: ri,j(\u03c4 ).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_204", "figure_caption": "Fig. 5 .5Fig. 5. Typical plot of |Ri| vs. \u03c4", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_205", "figure_caption": "Finally, a summary 2 )2statistics can be measured in terms of the average S C , or use this feature separately for each sensor as:", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_206", "figure_caption": "1 . 2 .12Use the 4-dimensional feature vector F = [M SE (k) j , S (k) j , R(states) (k) j , S C ]where the individual features have been \"fused\" across sensors by taking the mean Use a 17-dimensional vector comprising of M SE(k) i,j , [S (k) i,j ] 2 , R(states)", "figure_data": ""}, {"figure_label": "134", "figure_type": "figure", "figure_id": "fig_207", "figure_caption": "Case 1 3 4134Train on Building 1 data, test on Building 2 data Case 2 Train on Building 1 data, test on Building 1 data 4 . Case Train on Building 2 data, test on Building 2 data 4 . Case Train on Building 2 data, test on Building 1 data. Case 5 Mixed, i.e., combine data from both buildings by random permutation and use half the dataset to train and remaining half to test Tables", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_208", "figure_caption": "T P : True positives, T N: True negatives, F A: False alarms, M : Misses, P = (T P + T N)/(T P + T N + F A + M ) \u00d7 100%", "figure_data": ""}, {"figure_label": "122", "figure_type": "figure", "figure_id": "fig_209", "figure_caption": "Example 1 . 2 ) 2 .122If we set C = 1 and W (D ij ) = 1, eq(1) becomes linear multidimensional scaling (LMMDS) \u2212 D ij )2   (Example If we set C = N i=1 N j=i+1 D ij and W (D ij ) = D \u22121 ij , eq(1) becomes the nonlinear Sammon mapping[4]    ", "figure_data": ""}, {"figure_label": "1122", "figure_type": "figure", "figure_id": "fig_210", "figure_caption": "Fig. 1 .Example 1 . 2 Example 2 .1122Fig. 1. The divergence is the difference between F (p) and the value of F (q) + (p \u2212 q)\u2207F (q)", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_211", "figure_caption": ". 2 !d 2 F22The constant scalar is only for normalisation purposes, and does not affect the projection at all if discarded. We therefore concentrate onE Sammon (Y ) = N i=1,i<j I Sammon ij (10)Then an ExtendedSammon mapping will take into account all of the terms E ExtSammon (Y ) will actually use the alternative and equivalent representation,(6). The question now arises as to whether, by utilising (6) and thereby implicitly incorporating the higher order terms, we gain anything. From MMDS to BMMDS. If set W (D ij ) = 1 (Dij ) dD 2 ij", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_212", "figure_caption": "Fig. 3 .3Fig. 3. Analysis of the mapping of Swiss roll by LMMDS, Sammon and ExtendedSammon", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_213", "figure_caption": "ee\u2212\u03bbLij \u2212 e \u2212\u03bbDij + \u03bb(L ij \u2212 D ij )e \u2212\u03bbDij (17) E BregmanExp is an extension of MMDS E Exp (X) = \u2212\u03bbDij (L ij \u2212 D ij ) 2", "figure_data": ""}, {"figure_label": "1122222", "figure_type": "figure", "figure_id": "fig_214", "figure_caption": "Fig. 1 .Example 1 .= x \u2212 y 2 Example 2 .e x\u03bc\u2212 \u03bc 2 2 2 = \u03bc 2 21122222Fig. 1. The divergence is the difference between F (p) and the value of F (q) + (p \u2212 q)\u2207F (q)", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_215", "figure_caption": "2 = 1 =21G (\u03b8 1 , \u03b8 2 ) = G(\u03b8 1 ) \u2212 G(\u03b8 2 ) \u2212 (\u03b8 1 \u2212 \u03b8 2 ).g(\u03b8 2 ) = \u03bc 1 .\u03b8 1 \u2212 F (\u03bc 1 ) \u2212 \u03bc 2 .\u03b8 2 + F (\u03bc 2 ) \u2212(\u03b8 1 \u2212 \u03b8 2 ).\u03bc F (\u03bc 2 ) \u2212 F (\u03bc 1 ) \u2212 (\u03bc 2 \u2212 \u03bc 1 ).\u03b8 F (\u03bc 2 ) \u2212 F (\u03bc 1 ) \u2212 (\u03bc 2 \u2212 \u03bc 1 ).f (\u03bc 1 ) = d F (\u03bc 2 , \u03bc 1 )Thus minimising the Bregman divergence with respect to the cumulant function between the natural parameters is equivalent to minimising the Bregman divergence with respect to the dual function (but in the opposite direction) between the expectations. Alsolog p G,\u03b8 (x) = log p 0 (x) + x.\u03b8 \u2212 G(\u03b8) = log p 0 (x) + F (x) \u2212{F (x) + G(\u03b8) \u2212 x.\u03b8}= log p 0 (x) + F (x) \u2212{F (x) \u2212 F (\u03bc) + \u03bc.\u03b8 \u2212 x.\u03b8} = log p 0 (x) + F (x) \u2212 {F (x) \u2212 F (\u03bc) +\u03bc.f (\u03bc) \u2212 x.f (\u03bc)} = log p 0 (x) + F (x) \u2212 d F (x, \u03bc)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_216", "figure_caption": "yt\u2208D\u2212log(y t ) + log(y) + (y t \u2212 y) y U IS R = yt\u2208Dt \u2212 log(y t ) + log(\u1ef9) + (y t \u2212\u1ef9) y", "figure_data": ""}, {"figure_label": "234", "figure_type": "figure", "figure_id": "fig_217", "figure_caption": "VFig. 2 .Fig. 3 .Fig. 4 .234Fig. 2. The top 2 diagrams and the left diagram on the bottom show the three mixtures, the bottom right diagram shows the output signal when we use the criterion JRIS which has a correlation of 0.999 with the second (low frequency) sinusoid", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_218", "figure_caption": "Fig. 1 .1Fig.1. Signals corresponding to 4 usual daily physical activities. It's relevant the signal pattern form similarity between \"walking\" and \"running\", and \"sitting and relaxing\" and \"standing still\" respectively.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_219", "figure_caption": "Fig. 1 .1Fig. 1. Number of ISODATA calls per run", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_220", "figure_caption": "Fig. 2 .2Fig. 2. Speedup results", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_221", "figure_caption": "Algorithm 1 .1Input: I training instances with J input features Input: m features to remove at each step begin while (J > 0) do Train a classifier with J input features; Ranking the input features using the SBP algorithm; Remove the m least significant features; J \u2190 J \u2212 m; end end The proposed iSBP algorithm", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_222", "figure_caption": "Fig. 1 .Fig. 2 .12Fig.1. Averaged AUC results obtained by the four considered algorithms in terms of the corresponding feature subsets derived from the rankings. For each method, a unique ranking is obtained.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_223", "figure_caption": "Fig. 3 .3Fig.3. Averaged AUC results obtained by the four considered algorithms in terms of the corresponding feature subsets derived from the rankings. For each method, different rankings are obtained and evaluated using different partitions of the available data.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Comparative result for query expansion methods used in our work. Best results appear in boldface.", "figure_data": "MAPP@5P@10R-PrecUnexpanded query approach.2413.3220.2915.2422Jaccard_coeffcient.2816.3450.2900.3102Freq_coeffiecient.2218.3146.2995.3018Candidate term ranking using Suitabiltyof Q.2772.3660.2820.3643Candidate term ranking using KLD.3012.3640.2860.3914KLD_variation.2970.3665.2840.2802"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Table 1 shows the coupling metrics for SWETO ontology", "figure_data": "ClassObject Properties CBE-out CBE-in CBE-io SC CBEP erson550005Organization113004P ublication112014"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Syllogistic propositions consist of quantified object relationships", "figure_data": "OperatorPropositionSet-Theoretic Representation of Logical CasesAAll S are PEAll S are not PISome S are P"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Syllogistic figures", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Precision of multilingual tag matching (%)", "figure_data": "l1l2l3l4l5l6l7l8l9l10l11l12l1------------l267-----------l35668----------l4358473---------l587575445--------l64583665674-------l7486373526862------l882675462626463-----l96553345373724543----l10476551434234785158---l1175736166637262635262--l127223727532434343627382-"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results of mineral data classification. In the cells of two stage algorithm results, the left value shows the average error rate with SLP as a pair-wise classifier and the right onewith SVC as a pair-wise classifier. In the last two lines the most often selected similarity features' parameter \u03b1 and its selection rate (from all experiments) are presented.", "figure_data": "One stage classifiersTwo stage classifiersData parametersRBF KDAK-SLP VotingDAGH-TOriginal data0.189 0.2260.2520.212/0.211 0.215/0.211 0.226/0.218Similarity features 0.177 0.212 10 -4 0.1 Best \u03b1 Best \u03b1 rate 0.63 0.830.174 0.5 0.470.227/0.174 0.238/0.174 0.173/0.183 0.1 0.1 0.5 0.86 0.86 0.54"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "So inaccuracy of estimation of generalization error of SLP+HT method", "figure_data": "is01 ( 173 .\u22120.) 173/400=. 0019err1 ( \u2212err)/Nts(7)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Data set description", "figure_data": "Data setInstances Features Minority class percentageBank123,5622363.52Bank242,78316411.14Supermarket chain 32,3714625.15DIY chain3,8271528.14Bank320,4561375.99"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Experimental results for Bagging: wins-losses-ties A first set of observations is derived for the Bagging variations. The wins, losses and ties counts in table 2 indicate how the lift performance of Bagging can be increased by replacing standard C4.5 decision trees with C4.4 PETs, which confirms findings in", "figure_data": "AlgorithmBaggingBaggingBaggingBaggingBaggingBaggingBaggingBagging(C4.5 +(C4.5 +(C4.5 +(C4.5 +(C4.4 +(C4.4 +(C4.4 +(C4.4 +averaging) lift weights) (lift-1)rescaledaveraging) lift weights) (lift-1)rescaledweights) lift weights)weights) lift weights)Bagging (C4.5 + averaging)-1/1/321/2/323/0/320/10/250/9/260/11/240/9/26Bagging (C4.5 + lift weights)1/1/32-2/1/323/0/320/9/260/9/260/10/250/9/26Bagging (C4.5 + (lift-1) weights)2/1/321/2/32-2/0/330/9/260/9/260/9/260/8/27Bagging (C4.5 + rescaled lift weights) 0/3/320/3/320/2/33-0/11/240/10/25 0/10/250/9/26Bagging (C4.4 + averaging)10/0/259/0/269/0/26 11/0/24-0/0/350/0/355/1/29Bagging (C4.4 + lift weights)9/0/269/0/269/0/26 10/0/250/0/35-0/0/334/2/29Bagging (C4.4 + (lift-1) weights)11/0/2410/0/25 9/0/26 10/0/250/0/350/0/33-3/2/30Bagging (C4.4 + rescaled lift weights) 9/0/269/0/268/0/279/0/261/5/292/4/292/3/30-"}, {"figure_label": "3456", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Experimental results for RSM: wins-losses-ties Experimental results for SubBag: wins-losses-ties Experimental results for AdaBoost: wins-losses-ties Experimental results for Bagging: average ranks", "figure_data": "AlgorithmRSMRSMRSMRSMRSMRSMRSMRSM(C4.5)(C4.5 +(C4.5 +(C4.5 +(C4.4)(C4.4 +(C4.4 +(C4.4 +lift weights) (lift-1)rescaledlift weights) (lift-1)rescaledweights) lift weights)weights) lift weights)RSM (C4.5)-0/8/270/14/21 0/14/210/2/330/2/330/4/310/4/31RSM (C4.5 + lift weights)8/0/27-1/7/271/6/280/1/340/0/350/1/340/1/34RSM (C4.5 + (lift-1) weights)14/0/217/1/27-3/0/322/0/330/1/340/1/340/1/34RSM (C4.5 + rescaled lift weights) 15/0/20 10/1/247/3/25-4/1/301/2/320/1/340/2/33RSM (C4.4)2/0/331/0/340/2/331/1/33-1/15/19 0/14/21 0/15/20RSM (C4.4 + lift weights)2/0/330/0/351/0/342/0/3315/1/19-0/10/25 1/12/22RSM (C4.4 + (lift-1) weights)4/0/311/0/341/0/341/0/3414/0/21 10/0/25-4/1/29RSM (C4.4 + rescaled lift weights) 5/0/303/0/322/0/332/0/3315/0/20 13/1/219/0/26-AlgorithmSubBagSubBagSubBagSubBagSubBagSubBagSubBagSubBag(C4.5 +(C4.5 +(C4.5 +(C4.5 +(C4.4 +(C4.4 +(C4.4 +(C4.4 +averaging) lift weights) (lift-1)rescaledaveraging) lift weights) (lift-1)rescaledweights) lift weights)weights) lift weights)SubBag (C4.5 + averaging)-1/4/302/5/282/4/292/0/332/3/304/3/283/7/25SubBag (C4.5 + lift weights)4/1/30-1/6/283/4/282/0/332/1/323/4/283/4/28SubBag (C4.5 + (lift-1) weights)5/2/286/1/28-1/2/322/0/332/0/332/3/302/2/31SubBag (C4.5 + rescaled lift weights) 4/2/294/3/282/1/32-1/1/330/1/341/2/321/3/31SubBag (C4.4 + averaging)0/2/330/2/330/2/331/1/33-0/6/290/6/290/8/27SubBag (C4.4 + lift weights)3/2/301/2/320/2/331/0/346/0/29-1/4/301/7/27SubBag (C4.4 + (lift-1) weights)3/4/284/3/283/2/302/1/326/0/294/1/30-1/6/28SubBag (C4.4 + rescaled lift weights) 7/3/254/3/282/2/313/1/318/0/277/1/276/1/28-AlgorithmAdaBoost AdaBoost AdaBoost AdaBoost AdaBoost AdaBoost AdaBoost AdaBoost(C4.5)(C4.5 +(C4.5 +(C4.5 +(C4.4)(C4.4 +(C4.4 +(C4.4 +lift weights) (lift-1)rescaledlift weights) (lift-1)rescaledweights) lift weights)weights) lift weights)AdaBoost (C4.5)-5/5/2511/1/23 11/1/233/23/90/21/14 0/21/14 0/21/14AdaBoost (C4.5 + lift weights)5/5/25-18/0/17 21/0/144/24/71/24/102/24/91/24/10AdaBoost (C4.5 + (lift-1) weights)1/11/23 0/18/17-10/0/241/25/90/24/11 0/24/11 0/25/10AdaBoost (C4.5 + rescaled lift weights) 1/11/23 0/21/14 0/10/24-1/25/90/24/11 0/24/11 0/25/10AdaBoost (C4.4)23/3/924/4/725/1/925/1/9-3/12/20 3/11/210/4/31AdaBoost (C4.4 + lift weights)21/0/14 24/1/10 24/0/11 24/0/11 12/3/20-1/0/345/0/30AdaBoost (C4.4 + (lift-1) weights)21/0/1424/2/924/0/11 24/0/11 11/3/210/1/34-5/1/29AdaBoost (C4.4 + rescaled lift weights) 21/0/14 24/1/10 25/0/10 25/0/104/0/310/5/301/5/29-AlgorithmRankingBagging (C4.4 + averaging)3.4286Bagging (C4.4 + rescaled lift weights) 3.4286Bagging (C4.4 + (lift-1) weights)3.4714Bagging (C4.4 + lift weights)3.8143Bagging (C4.5 + (lift-1) weights)5.3143Bagging (C4.5 + rescaled lift weights) 5.5143Bagging (C4.5 + lift weights)5.5286Bagging (C4.5 + averaging)5.6143"}, {"figure_label": "78", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Experimental results for RSM: average ranks Experimental results for SubBag: average ranks", "figure_data": "AlgorithmRankingRSM (C4.4 + (lift-1) weights)3.2143RSM (C4.5 + (lift-1) weights)3.4286RSM (C4.4 + rescaled lift weights) 3.7857RSM (C4.5 + rescaled lift weights) 3.8857RSM (C4.4 + lift weights)4.6000RSM (C4.5 + lift weights)4.9000RSM (C4.4)6.0571RSM (C4.5)6.1286AlgorithmRankingSubBag (C4.5 + rescaled lift weights) 3.8429SubBag (C4.5 + (lift-1) weights)4.0429SubBag (C4.5 + lift weights)4.1857SubBag (C4.4 + rescaled lift weights) 4.4571SubBag (C4.4 + (lift-1) weights)4.5857SubBag (C4.5 + averaging)4.6143SubBag (C4.4 + lift weights)5.0286SubBag (C4.4 + averaging)5.2429"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Experimental results for AdaBoost: average ranks", "figure_data": "AlgorithmRankingAdaBoost (C4.4 + lift weights)3.0857AdaBoost (C4.4 + rescaled lift weights) 3.1143AdaBoost (C4.4 + (lift-1) weights)3.2857AdaBoost (C4.4)4.0000AdaBoost (C4.5 + lift weights)4.7857AdaBoost (C4.5)5.3143AdaBoost (C4.5 + (lift-1) weights)6.0143AdaBoost (C4.5 + rescaled lift weights) 6.4000"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Results for unsupervised feature selection as averages over 10 runs for each data set: the ARI score and the number of clusters k for the best partition, the sensitivity and the specificity of the selected feature subspace", "figure_data": "Problem ARI ksensitivity specificity F-measure # evaluations2d-4c0.6623 3.98 0.890.930.90120362d-10c 0.70 8.78 0.970.990.981176710d-4c 0.9374 3.71 0.920.930.91788710d-10c 0.8055 8.16 0.930.990.958222"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Results on synthetic data", "figure_data": "# classes# samples that will be labeled for a new classifierProposed method (\u03b8r=0.5) Classifier count %Active learning methods from streaming data [12] Classifier count RS (%) LU (%) GU (%) MV (%)25164.1 (\u00b113.20)89.60 (\u00b11.61)200.0 (\u00b10.00)74.54 (\u00b12.89)73.70 (\u00b12.75)75.51 (\u00b13.35)81.08 (\u00b12.85)5079.1 (\u00b15.76)86.23 (\u00b12.08)100.0 (\u00b10.00)83.07 (\u00b12.42)82.56 (\u00b12.36)85.16 (\u00b12.95)86.79 (\u00b12.51)Two classes7554.4 (\u00b13.23)84.09 (\u00b12.64)66.0 (\u00b10.00)86.10 (\u00b12.75)85.69 (\u00b13.13)88.14 (\u00b13.04)88.67 (\u00b12.74)10043.6 (\u00b14.27)82.36 (\u00b12.03)50.0 (\u00b10.00)86.43 (\u00b13.52)86.11 (\u00b14.12)88.79 (\u00b13.47)89.09 (\u00b13.39)20025.4 (\u00b12.65)78.51 (\u00b12.41)25.0 (\u00b10.00)86.47 (\u00b15.01)85.94 (\u00b14.82)88.69 (\u00b14.27)88.88 (\u00b14.27)25162.6 (\u00b111.90)88.66 (\u00b10.70)200.0 (\u00b10.00)56.36 (\u00b12.71)55.46 (\u00b12.64)56.57 (\u00b12.71)64.41 (\u00b13.87)5078.1 (\u00b15.46)86.24 (\u00b11.53)100.0 (\u00b10.00)66.31 (\u00b14.30)66.11 (\u00b14.23)66.99 (\u00b14.98)71.59 (\u00b14.23)Three classes7558.7 (\u00b13.92)85.60 (\u00b11.58)66.0 (\u00b10.00)71.79 (\u00b12.86)70.49 (\u00b13.61)72.59 (\u00b13.77)74.39 (\u00b13.99)10044.8 (\u00b13.05)84.20 (\u00b11.26)50.0 (\u00b10.00)75.29 (\u00b12.88)74.23 (\u00b13.27)76.45 (\u00b13.72)78.22 (\u00b14.38)20025.5 (\u00b11.43)81.73 (\u00b10.91)25.0 (\u00b10.00)73.92 (\u00b16.11)72.97 (\u00b16.01)76.37 (\u00b14.64)75.98 (\u00b15.44)25163.2 (\u00b112.37)87.93 (\u00b11.21)200.0 (\u00b10.00)42.24 (\u00b12.01)41.38 (\u00b11.80)41.34 (\u00b12.09)46.54 (\u00b12.04)5081.1 (\u00b15.18)85.79 (\u00b10.89)100.0 (\u00b10.00)50.47 (\u00b12.34)49.81 (\u00b12.47)49.91 (\u00b12.45)53.95 (\u00b13.04)Four classes7554.4 (\u00b14.05)85.18 (\u00b12.08)66.0 (\u00b10.00)58.11 (\u00b13.65)56.75 (\u00b13.26)56.48 (\u00b13.09)59.86 (\u00b13.88)10044.4 (\u00b14.71)84.87 (\u00b11.71)50.0 (\u00b10.00)63.08 (\u00b13.67)62.72 (\u00b14.07)61.92 (\u00b13.56)64.04 (\u00b14.11)20026.0 (\u00b12.23)80.77 (\u00b11.60)25.0 (\u00b10.00)71.87 (\u00b14.47)70.67 (\u00b14.98)70.98 (\u00b15.09)72.13 (\u00b15.33)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Results on intrusion detection data", "figure_data": "# samples that willProposed method (\u03b8r =0.3)Ensemble methodsbe labeled for a new classifierGenerated classifier count%Generated classifier countSV [3] (%)WE [11] (%)2532097.0419,41498.4699.315010496.619,70797.8298.91751097.386,47197.1898.59100997.434,85396.5498.312001192.982,42695.0997.30Average90.80 (120.24) 96.29 (\u00b1 1.87) 8,574.20 (\u00b15913.63) 97.02 (\u00b11.29)98.48 (\u00b10.75)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Summary of the data sets used in the experiments", "figure_data": "Dataset#N #D #I #E #CDataset#N #D #I #E #Cabalone7 1 10 4177 28lymphography3 15 148 4anneal6 32 90 898 6mushroom0 22 8124 2audiology0 69 93 226 24nursery0 8 12960 5autos15 10 71 205 6optdigits64 0 5620 10balance-scale 4 0 4 625 3page10 0 5473 5breast-w9 0 9 699 2pendigits16 0 10992 10breast-y0 9 48 286 2phoneme5 0 5404 2bupa6 0 6 345 2pima8 0 768 2car0 6 21 1728 4primary0 17 339 22credit-a6 9 43 690 2promoters0 57 106 2credit-g7 13 61 1000 2ringnorm20 0 300 2crx6 9 42 690 2sat36 0 6435 6dna0 180 180 3186 3segment19 0 2310 7ecoli7 0 7 336 8shuttle9 0 58000 7glass9 0 9 214 6sick7 22 3772 2heart-c6 7 22 303 2sonar60 0 208 2heart-h6 7 22 294 2soybean0 35 683 19heart-s5 8 25 123 2soybean-small0 3547 4heart-statlog 13 0 13 270 2splice0 60 3190 3heart-v5 8 25 200 2threenorm20 0 300 2hepatitis6 13 19 155 2tic-tac-toe0 9 958 2horse-colic7 15 60 368 2twonorm20 0 300 2hypo7 18 25 3163 2vehicle18 0 846 4ionosphere34 0 34 351 2vote10 15 435 2iris4 0 4 150 3voting0 16 435 2krk6 0 6 28056 18vowel-context10 2 990 11kr-vs-kp0 36 40 3196 2vowel-nocontext 10 0 990 11labor8 8 2657 2waveform40 0 5000 3led-240 24 24 5000 10yeast8 0 1484 10letter16 0 16 20000 26zip256 0 9298 10lrd93 0 93 531 10zoo1 15 101 7#"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Average rank of the considered methods. Avg Rank column represents the average position taken by the method through all datasets. The methods are ordered by this column. Pos points the method position according such order.", "figure_data": "Pos MethodAvg RankPos MethodAvg Rank1 Rot-RP 125%, B9.2114 RP-Ensemble 125%, G13.732 Rot-RP 100%, G9.2315.5 Rot-RP 75%, G14.693 Rot-RP 125%, S9.5215.5 Rot-RP 75%, B14.694 Rot-RP 125%, G9.8517 Rot-PCA 75%15.015 Rot-RP 100%, S10.4718 RP-Ensemble 100%, G15.106 Rot-RP 100%, B10.8819 RP-Ensemble 125%, B15.197 Rot-PCA 100%10.9320 AdaBoost15.698 Bagging11.2821 Rotation-RP 75%, S16.029.5 RP-Ensemble 125%, S12.3422 RP-Ensemble 75%, S16.149.5 SVM12.3423 RP-Ensemble 100%, B16.3111 Random Subspaces 75%12.4724 Random Subspaces 50%16.9012 MultiBoost12.7925 RP Ensemble 75%, G17.6413 RP-Ensemble 100%, S13.2826 RP Ensemble 75%, B19.01"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Win-Ties-Losses of Rotation-RPs variants against the rest of best ranked methods. Boldface is used to point the winner. The symbol \u2022 means a significantly win.", "figure_data": "Rot-RP 100%Rot-RP 125%Sparse Binary Gaussian Sparse Binary GaussianRot-PCA 100% 28-5-29 29-3-30 30-7-25 35-7-20 32-5-25 31-6-25Bagging 30-3-29 28-4-30 32-4-26 31-6-25 34-5-23 30-5-27SVM 30-4-28 29-5-28 35-4-23 \u202237-6-19 \u202235-8-19 \u202237-5-20RP-Ensemble 125%, S \u202244-2-16 \u202237-5-20 \u202243-4-15 \u202239-6-17 \u202243-7-12 \u202240-4-18"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "14. Dem\u0161ar, J.: Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research 7, 1-30 (2006) 15. Margineantu, D.D., Dietterich, T.G.: Pruning adaptive boosting. In: Proc. 14th International Conference on Machine Learning, pp. 211-218. Morgan Kaufmann, San Francisco (1997) 16. Maudes, J., Rodr\u00edguez, J.J., Garc\u00eda-Osorio, C.: Disturbing neighbors diversity for decision forests. In: Okun, O., Valentini, G. (eds.) Workshop on Supervised and Unsupervised Ensemble Methods and their Applications, SUEMA 2008, pp.", "figure_data": "67-71(2008)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Data set description. All of them are free to download in[20].", "figure_data": "Data Set andoriginal work referenceSamples Attributes Items in each classALL-AML Golub et al. (1999)727129{(47, 65.2%), (25, 34.8%)}ALLYeoh et al. (2002)32712558{(15, 4.6%), (27, 8.3%),(64, 19.6%), (20, 6.1%),(43, 13.1%), (79, 24.2%),(79, 24.2%)}BreastVan't Veer (2002)9724481{(46, 47.4%), (51, 52.6%)}CNSMukherjee et al. (2002)607129{(21, 35.0%), (39, 65.0%)}ColonAlon et al. (1999)622000{(22, 35.4%), (40, 64.6%)}DLBCLAlizadeh et al. (2000)474026{(24, 51.0%), (23, 49.0%)}LungGordon et al. (2002)18112533{(31, 17.1%), (150, 82.9%)}MLLArmstrong et al. (2001)7212582{(24, 33.3%), (20, 27.7%),(28, 38.8%)}Ovarian Petricoin et al. (2002)25315154{(162, 64.0%), (91, 36.0%)}Prostate Singh et al. (2002)13612600{(77, 56.6%), (59, 43.4%)}"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Summary of accuracy for all data sets, all methods and selected attributes", "figure_data": "Algorithm#AttribsALL-AMLALLBreastCNSColonDLBCLLungMLLOvarianprostateReliefF+Bagging4 95,32 72,48 65,41 57,37 79,84 88,56 97,41 82,74 96,31 87,22SVM-RFE+Bagging4 94,37 48,02 68,81 55,81 74,44 88,89 95,93 85,36 98,16 90,22ReliefF+Boosting4 95,44 71,31 63,93 62,19 79,68 84,11 98,15 84,40 94,99 83,37SVM-RFE+Boosting4 92,54 45,81 61,93 54,70 70,48 86,44 96,30 86,07 98,16 93,22ReliefF+RF-FastICA4 92,14 72,77 66,30 58,48 83,17 92,89 99,26 87,14 94,72 88,72SVM-RFE+RF-FastICA 4 94,60 50,48 68,59 58,10 73,49 90,22 97,78 87,14 99,21 91,25ReliefF+RF-PCA4 90,16 74,75 66,15 57,37 82,06 92,89 99,26 87,26 94,46 89,71SVM-RFE+RF-PCA4 93,77 50,87 65,41 56,03 74,60 93,00 97,78 84,29 100,0 90,77ReliefF+Bagging8 91,19 80,17 65,41 61,65 75,56 90,44 97,78 88,10 96,31 90,18SVM-RFE+Bagging8 92,42 77,51 66,67 51,52 75,71 87,22 95,95 87,14 97,36 90,26ReliefF+Boosting8 94,33 80,72 61,41 61,81 74,60 87,78 97,41 87,98 95,77 90,70SVM-RFE+Boosting8 94,88 79,35 63,41 53,43 75,56 84,78 95,56 90,00 97,90 93,66ReliefF+RF-FastICA8 95,99 83,00 62,67 60,48 83,02 94,22 98,15 89,05 95,25 93,66SVM-RFE+RF-FastICA 8 97,10 80,17 69,04 54,92 73,65 91,22 98,52 91,67 99,74 92,67ReliefF+RF-PCA8 95,44 83,54 62,81 62,25 83,17 92,89 98,15 86,31 95,51 91,65SVM-RFE+RF-PCA8 98,21 80,19 65,33 54,60 77,78 92,33 98,15 89,88 100,0 90,22ReliefF+Bagging16 92,54 84,40 65,33 55,14 78,73 91,56 97,06 88,93 97,10 92,67SVM-RFE+Bagging16 93,37 85,11 68,67 54,70 76,83 88,56 95,58 83,45 97,10 87,25ReliefF+Boosting16 94,33 86,23 62,89 60,54 77,94 80,11 97,80 86,19 96,84 92,60SVM-RFE+Boosting16 91,27 87,65 62,07 56,92 73,49 87,78 94,83 87,26 97,91 94,69ReliefF+RF-FastICA16 97,10 87,44 64,15 62,92 85,24 94,56 98,89 92,62 97,36 93,63SVM-RFE+RF-FastICA 16 98,21 87,61 66,30 60,70 77,94 91,56 98,52 89,05 99,74 89,27ReliefF+RF-PCA16 95,16 87,03 62,81 57,21 84,13 94,56 98,52 90,83 98,41 92,60SVM-RFE+RF-PCA16 98,21 88,07 66,89 59,43 80,00 94,00 98,15 89,05 99,73 92,67ReliefF+Bagging32 92,54 84,86 61,93 51,81 77,94 92,89 97,06 90,83 97,36 91,68SVM-RFE+Bagging32 93,25 86,99 63,41 56,92 81,11 90,22 96,32 87,86 97,11 87,21ReliefF+Boosting32 91,83 87,47 62,15 53,87 77,94 81,11 98,17 90,95 98,15 93,59SVM-RFE+Boosting32 91,43 91,87 64,30 65,21 77,94 86,11 94,83 89,88 98,16 95,16ReliefF+RF-FastICA32 97,10 88,71 63,48 56,25 85,24 95,89 98,15 91,79 98,93 95,09SVM-RFE+RF-FastICA 32 97,10 90,83 67,56 63,87 76,98 94,22 98,52 92,62 99,48 91,65ReliefF+RF-PCA32 98,21 88,27 64,96 58,48 83,17 95,89 98,15 90,83 99,47 92,64SVM-RFE+RF-PCA32 98,21 90,85 72,44 70,98 80,32 93,11 98,52 89,88 100,0 93,11ReliefF+Bagging64 93,37 88,08 61,19 49,37 79,84 92,89 97,06 89,88 97,11 89,19SVM-RFE+Bagging64 95,32 87,43 67,04 64,70 77,94 91,56 95,95 86,90 97,11 86,66ReliefF+Boosting64 94,44 89,71 64,74 58,10 79,05 80,44 98,91 91,79 97,36 92,60SVM-RFE+Boosting64 92,22 91,69 60,52 64,03 77,78 87,11 96,32 91,79 97,64 91,61ReliefF+RF-FastICA64 97,10 89,91 61,33 64,60 83,17 95,89 98,52 91,79 98,94 94,14SVM-RFE+RF-,73 93,15ReliefF+RF-PCA64 97,10 89,91 63,33 63,87 85,24 94,56 98,52 90,00 99,73 93,15SVM-RFE+RF-PCA64 128 93,37 88,90 58,96 56,19 81,11 92,89 96,67 91,79 96,84 89,67SVM-RFE+Bagging128 93,49 87,84 61,93 66,92 79,84 92,89 96,32 86,07 97,11 86,74ReliefF+Boosting128 94,33 90,70 57,93 53,90 80,95 81,44 98,15 92,74 97,36 91,68SVM-RFE+Boosting128 94,17 91,88 64,07 64,92 83,02 83,11 97,06 89,76 97,90 87,17ReliefF+RF-FastICA128 97,10 90,52 62,07 57,52 85,24 95,89 98,89 95,48 99,73 94,14SVM-RFE+RF-FastICA 128 98,06 91,50 64,15 68,63 81,11 94,22 98,52 93,57 99,73 93,15ReliefF+RF-PCA128 97,10 90,91 64,00 59,75 85,24 95,89 98,89 92,74 99,73 93,63SVM-RFE+RF-PCA128 98,21 91,28 68,15 63,71 83,17 94,22 98,52 94,52 100,0 95,05"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_26", "figure_caption": "Datasets used in the experiments", "figure_data": "DatasetExamples Numeric NominalDatasetExamples Numeric Nominal2d-planes40768100house-16H22784160abalone417771house-8L2278480ailerons13750400housing506121auto9393166hungarian29467auto-horse205178kin8nm819280auto-mpg39843longley1660auto-price159150lowbwt18927bank-32nh8192320machine-cpu20960bank-8FM819280mbagrade6111baskball9640meta528192bodyfat252140mv4076873bolts4070pbc418108breast-tumor28618pharynx195110cal-housing2064080pole15000480cholesterol30367pollution60150cleveland30367puma32H8192320cloud10842puma8NH819280cpu-act8192210pw-linear200100cpu20961pyrimidines74270cpu-small8192120quake217830delta-ailerons712950schlvote3841delta-elevators951760sensory576011detroit13130servo16704diabetes-numeric4320sleep6270echo-months13063stock95090elevators16599180strike62551elusage5511triazines186600fishcatch15852veteran13734friedman40768100vineyard5230fruitfly12522wisconsin194320gascons2740"}, {"figure_label": "23", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "Considered methods sorted according to their average ranks Comparison of ensembles of Multilayer Perceptrons with a single model and Bagging. The number of wins, ties and losses is shown for the comparison of the column method with the row method.", "figure_data": "Average rank Method14.61Bagging MLP26.28Random Subspaces 75% MLP36.74AdaBoost.R2-S-Li MLP47.12AdaBoost.R2-S-Sq MLP57.21AdaBoost.R2-S-Ex MLP67.30Random Subspaces 50% MLP77.35k-Nearest Neighbors87.37Iterated Bagging MLP98.42Linear Regression (all)108.55Linear Regression (selection)118.63Randomization MLP129.48AdaBoost.R2-W-Ex MLP1310.69AdaBoost.R2-W-Li MLP1411.82Single MLP1511.82AdaBoost.R2-W-Sq MLP1612.621-Nearest NeighborMethodSingle MLPBaggingLinear Regression (selection)36 / 0 / 25 21 / 0 / 40Linear Regression (all)34 / 0 / 27 19 / 0 / 421-Nearest Neighbor23 / 0 / 385 / 0 / 56k-Nearest Neighbors42 / 0 / 19 23 / 0 / 38Single MLP0 / 61 / 03 / 0 / 58Randomization MLP60 / 0 / 15 / 0 / 56Bagging MLP58 / 0 / 30 / 61 / 0Random Subspaces 50% MLP 39 / 1 / 21 27 / 0 / 34Random Subspaces 75% MLP 52 / 0 / 923 / 0 / 38Iterated Bagging MLP54 / 0 / 74 / 0 / 57AdaBoost.R2-W-Li MLP39 / 0 / 225 / 0 / 56AdaBoost.R2-W-Sq MLP35 / 0 / 267 / 0 / 54AdaBoost.R2-W-Ex MLP41 / 0 / 20 14 / 0 / 47AdaBoost.R2-S-Li MLP48 / 1 / 12 22 / 0 / 39AdaBoost.R2-S-Sq MLP48 / 0 / 13 22 / 0 / 39AdaBoost.R2-S-Ex MLP50 / 0 / 11 20 / 0 / 41"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_29", "figure_caption": "Possible conflicts found for the laboratory plant; constraints, components, and the estimated variable for each possible conflict", "figure_data": "ConstraintsComponents Estimate"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_30", "figure_caption": "Possible conflicts related to fault modes are shown in the theoretical fault signature matrix shown in table 2. It should be noticed that these are the fault mode classes which can be distinguished for fault identification. In the localization stage, the following pair of faults {f 1 , f 2 }, {f 4 , f 11 }, {f 3 , f 12 }, and {f 10 , f 13 } can not be separately isolated.", "figure_data": "1. t dm : mass balance in tank t.2. t dE : energy balance in tank t.3. t f b : flow from tank t to pump.4. t f : flow from tank t through a pipe.5. rp: resistor heat transfer.Based on these equations we have found the set of possible conflicts shown in table 1.In the table, second column shows the set of constraints used in each possible conflict,which are minimal with respect to the set of constraints. Third column shows thosecomponents involved. Fourth column indicates the estimated variable for each possibleconflict. We have considered the fourteen fault modes shown in figure 1.Class Decomposition."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_31", "figure_caption": "PCs and their associated fault modes and observations", "figure_data": "f1f2f3f4f5f6f7f8f9f10f11f12f13f14FT01FT02FT03FT04LT01LT04LC01LC04TT02TT03"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_32", "figure_caption": "Accuracies of the methods. The symbol \"\u2022\" indicates that the result is significantly worse than the method with the best result for the corresponding column.", "figure_data": "MethodC l a s sA t t r i b u t e30%40%series length 50%70%100%Decision Tree No -\u202265.95 (8.42) \u202294.76 (3.79) \u202290.95 (5.03) \u202294.29 (5.19) \u202293.81 (5.13)Decision Tree Yes No \u202287.14 (6.01) \u202296.90 (1.84) \u202295.00 (4.00) 96.67 (4.15) 96.43 (3.82)Decision Tree Yes Yes \u202288.81 (4.65) 96.43 (3.02) 96.67 (3.16) \u202295.71 (3.87) \u202295.48 (4.15)Naive Bayes No -\u202257.86 (4.71) \u202288.10 (4.99) \u202291.90 (4.77) \u202292.38 (4.84) \u202282.86 (5.43)Naive Bayes Yes No \u202287.38 (4.24) \u202295.71 (3.62) 97.14 (2.77) 97.38 (2.85) 97.86 (2.63)Naive Bayes Yes Yes \u202289.05 (3.16) 98.10 (2.29) 97.62 (2.20) 97.14 (2.41) \u202297.14 (2.77)SVM linearNo -\u202242.62 (5.48) \u202283.10 (4.15) \u202287.14 (4.44) \u202289.52 (3.69) \u202294.05 (3.97)SVM linearYes No \u202274.52 (7.25) \u202293.33 (3.27) 97.14 (2.41) 98.81 (1.74) 99.76 (0.92)SVM linearYes Yes \u202276.90 (6.17) \u202289.52 (3.43) \u202290.24 (3.69) \u202290.24 (3.43) \u202290.71 (3.77)SVM perc.No -\u202250.24 (7.33) \u202282.62 (4.24) \u202286.90 (4.61) \u202288.57 (4.52) \u202288.81 (4.24)SVM perc.Yes No \u202288.10 (4.20) 98.10 (1.84) 99.05 (1.63) 99.29 (1.48) 99.29 (1.48)SVM perc.Yes Yes \u202289.29 (3.02) 97.38 (2.51) 98.57 (1.81) 98.57 (1.81) 98.33 (1.84)1-NNNo -\u202248.81 (7.72) \u202284.52 (7.35) \u202285.00 (7.17) \u202286.67 (6.54) \u202287.62 (6.31)1-NNYes No \u202286.43 (6.91) \u202293.57 (3.87) \u202293.33 (4.65) \u202292.62 (4.57) \u202294.05 (4.20)1-NNYes Yes96.43 (4.27) 99.52 (1.26) 98.81 (1.74) 98.81 (1.74) 98.10 (1.84)Stack-1-NNNo -\u202263.81 (7.50) \u202296.67 (2.51) 97.86 (2.26) 98.10 (2.65) 99.05 (1.63)Stack-1-NNYes No \u202289.76 (3.79) 97.62 (2.58) 98.33 (1.84) 98.81 (2.20) 99.52 (1.26)Stack-1-NNYes Yes94.29 (3.52) 99.29 (1.48) 99.29 (1.48) 99.52 (1.26) 99.52 (1.26)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_33", "figure_caption": "Statistical comparison of algorithms for OneMax problem", "figure_data": "Size306090120150BOA240.6512.4877.01166.4 1470.4\u03c3(55.6) (75.2) (191.7) (204.7) (235.2)eBOA188.6469.0719.21019.9 1308.9\u03c3(45.6) (82.8) (95.3) (113.7) (185.1)p-valueStatistical t-test: (BOA -eBOA) 7.32E-4  \u2020 0.04619 8.55E-4  \u2020 0.00431  \u2020 1.02E-02\u2020 Significance by a paired, two-tailed test at \u03b1 = 0.01."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_34", "figure_caption": "Statistical comparison of algorithms for trap-5 problem", "figure_data": "Size306090120150BOA11005.3 32496.4 66453.1 105245.1 155950.3\u03c3(1519.4) (3771.1) (5118.9) (13198.5) (15729.4)eBOA7465.625156.7 50683.8 82321.7 125699.6\u03c3(1167.5) (3984.8) (5477.1) (6297.1) (14035.6)p-valueStatistical t-test: (BOA -eBOA) 1.39E-11  \u2020 5.35E-07  \u2020 1.17E-12  \u2020 1.67E-09  \u2020 1.32E-08  \u2020\u2020 Significance by a paired, two-tailed test at \u03b1 = 0.01."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_35", "figure_caption": "Statistical comparison of population sizes for solving trap-5 problem", "figure_data": "Size306090120150BOA999.1 2392.8 4404.6 6257.5 8489.6\u03c3(163.4) (278.9) (461.1) (1020.2) (1016.4)eBOA992.3 2632.4 4397.1 6146.5 8632.2\u03c3(152.0) (530.7) (567.4) (413.3) (1160.8)p-valueStatistical t-test: (BOA -eBOA) 0.85544 0.05682 0.95358 0.57186 0.60253\u2020 Significance by a paired, two-tailed test at \u03b1 = 0.01."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_36", "figure_caption": "Parameters of ClonSAT", "figure_data": "Parameter nameParameter valuepopulation size100mutation min1 / nb variablesmutation max0.01clone size (\u03b2)200replacement size10Total number of generations before stopping1000Nb generations before applying WalkSat100walkSat p probability0.2walkSat number of flips500"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_37", "figure_caption": "Results    ", "figure_data": "Number ofNumber ofQSAT GSAT ClonSATClonSATBenchmarkVariables (n)clauses (m)alone+WalkSatAim-50-1_6-no-1508079797979Aim-50-1_6-no-2508079797979Aim-50-1_6-no-3508078797979Aim-50-1_6-no-4508079797979Aim-200-2_0-no-1200400396399399Aim-200-2_0-no-2200400397399399Aim-200-2_0-no-3200400397399399Aim-50-1_6-yes1-1508080797980Aim-50-1_6-yes1-2508080797979Aim-50-1_6-yes1-3508079797980Aim-100-2_0-no-1100200198199199Aim-100-2_0-no-2100200197199199Aim-100-3_4-yes1-3100340335335336Aim-100-3_4-yes1-4100340333340336"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_38", "figure_caption": "Notation S[t] res indicates the resource's index where task t is mapped, regarding solution S. Constants Area c and Area s represents communication channel and switch areas, respectively.", "figure_data": "Area M (S) = Area A (A S ) +area c \u00d7CH (S[d src ] res , S[d tgt ] res ) +d\u2208E(T G)area s \u00d7SW (S[d src ] res , S[d tgt ] res )d\u2208E(T G)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_41", "figure_caption": "Applications and number of assignments and mappings for power-aware optimization", "figure_data": "ID ApplicationN M CombinationsAssignment NSGA-II microGA NSGA-II microGA Mapping1 auto-indust-tg0 6 41.183.7442272 auto-indust-tg2 9 9606.076.928172311473 consumer-tg07 82.247.264963104 consumer-tg17 5176.868397185 networking-tg2 4 341.61626376 office-tg05 5210.6816188257 telecom-tg16 69.516.1922214"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_42", "figure_caption": "Solution representation", "figure_data": "Pair(s,t) DemandC-clockwise CC-counterclockwise1: (1, 2)1515 C2: (1, 3)33CC3: (1, 4)66CC4: (2, 3)1515 C5: (2, 4) 6: (3, 4)6 146 14 C CC"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_46", "figure_caption": "Best obtained results", "figure_data": "Instance Nodes Pairs Best FitnessInstance Nodes Pairs Best FitnessC11510161C41201902581C1258116C4220931482C1356116C432040612C211045525C51253004265C221023243C52251502323C231012141C532561912C31151051574C61304355762C321550941C62302012696C331525563C6330921453C643043527779"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_47", "figure_caption": "Results -run times and number of iterations", "figure_data": "Prob NumberGAHDETabu Search LS-PBPSODDEIterations Time IT Time IT Time IT Time ITTimeITC1125<0.001 2<0.0012<0.0015<0.0012<0.0012C1210<0.001 2<0.0012<0.0015<0.0012<0.0012C1310<0.001 1<0.0011<0.0011<0.0011<0.0011C2150<0.001 15 <0.001 10 <0.001 25 <0.001 15<0.00110C2225<0.001 5<0.0013<0.0015<0.0013<0.0013C2310<0.001 3<0.0013<0.0015<0.0013<0.0013C311000. 1300. 1150. 1900. 1200. 110C3250<0.001 15 <0.0015<0.001 30 <0.0018<0.0015C3325<0.001 5<0.0013<0.001 20 <0.0015<0.0013C413000.1600.1300.32200.1400.0825C421000.075 400.05100.1850.05200.038C4350<0.001 10 <0.0015<0.001 25 <0.0015<0.0013C515000.75800.753012600.75800.530C524000.1400.1150.21100.1250.0815C532500.01250.01100.032000.01150.0058C6115001,75 1301.75403.540021301.550C6210000.2600.25200.52300.4500.1525C635000.075 300.075100.1100 0.075150.0510C645000.3300.2531.52500.75400.13"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_48", "figure_caption": "Results -Average Time / Average Fitness", "figure_data": "Problem Number ofGAHDELS-PBPSOTabuDDEiterations AvgF AvgT AvgF AvgT AvgF AvgT AvgF AvgT AvgF AvgTC41502587,62 0,17 2584,31 0,27 2594,36 0,26 2635,28 0,16 2582,06 0,12C51754273,18 0,43 4271,27 0,71 4291,52 0,86 4392,70 0,86 4268,96 0,47C611005785,62 1,34 5783,18 1,87 5837,58 3,10 5963,14 3,71 5781,52 1,27"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_50", "figure_caption": "Halstead's metrics calculated for the systems", "figure_data": "System Classes Methods Length Volume EffortPatternless528672 2856.2 35032.85GATATest21691049 3861.07 33023.6"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_52", "figure_caption": "Input variables summary", "figure_data": "Num NameTypeMinMax Mean Std. Dev.Non Incintrinsic 64.90 92.70 83.384.33Infointrinsic0.30 16.301.891.35Incidentsintrinsic6.00 31.20 14.724.34Sanitaryintrinsic 33.70 75.20 57.035.94Securityintrinsic 18.60 57.10 36.305.65Search & Rescue (SAR) intrinsic0.30 24.505.043.69Basic Serv.intrinsic0.009.901.641.35Call Midnightintrinsic8.00 495.00 163.3391.83Call Dawnintrinsic0.00 214.00 33.4421.89Call Morningintrinsic 92.00 722.00 245.7698.60Call Afternoonintrinsic 109.00 835.00 360.07132.07Call Eveningintrinsic 52.00 770.00 237.51118.68Total Callsintrinsic 1658.00 9504.00 4788.09 1608.71T Avgextrinsic-3.37 28.73 11.797.68T Maxextrinsic1.30 38.90 19.429.01T Minextrinsic -11.90 17.903.926.36Rainextrinsic0.00 192.206.1015.07Snowextrinsic0.006.000.170.67Fogextrinsic0.006.000.541.12Day of Weekextrinsic1.007.004.002.01Monthextrinsic1.00 12.006.013.55"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_53", "figure_caption": "KBCT results summary for input fuzzification", "figure_data": "VariableLabels Chen index Method Partition CentersNo Inc30.7556 Kmeans (77, 82.5, 87)Information30.9865 HFP(3.5, 11.5, 16.3)Incidents30.7836 Kmeans (11.5, 17, 22.5)Sanitary30.7314 Kmeans (49, 56, 64)Security30.7165 Kmeans (28, 35.5, 42.5)SAR30.9473 HFP(7, 22.5, 24.5)Basic Serv.30.9189 HFP(2.1, 7.2, 9.9)Call Midnight30.7657 Kmeans (98, 205, 345)Call Dawn30.9625 HFP(50, 165, 214)Call Morning30.8736 HFP(240, 610, 722)Call Afternoon30.7798 Kmeans (230, 400, 560)Call Evening30.8717 HFP(230, 650, 770)Total Calls30.7967 Kmeans (3100, 5200, 7050)T Avg30.7849 Kmeans (4, 12, 22)T Max30.7758 Kmeans (10, 18.5, 30.5)T Min30.7663 Kmeans (-4, 3, 11)Rain30.9816 HFP(30, 110, 192.2)Snow71.0000 Regular (1, 2, 3, 4, 5, 6, 7)Fog71.0000 Regular (1, 2, 3, 4, 5, 6, 7)"}, {"figure_label": "34", "figure_type": "table", "figure_id": "tab_54", "figure_caption": "Thresholds for the construction of the fuzzy UDT Results of Fuzzy UDT based on quality indexes", "figure_data": "ParameterLower Limit Upper Limit Step Num. ValuesInhomogeneity Threshold (IT)0.160.190.014Segment Size Threshold (SST)43 021 4IT SST LN DT IQN-T ICSCSXB0."}, {"figure_label": "1234", "figure_type": "table", "figure_id": "tab_55", "figure_caption": "R1: If Humidity is S and Temperature is VS Then Prays Alert is VS R6: If Humidity is M and Temperature is VS Then Prays Alert is VS R11: If Humidity is L and Temperature is VS Then Prays Alert is VS Group of rules used to model the Prays (in specific and common KB) Group of rules used to model the Repilo (in specific and common KB) Reduced group of rules used to model the Prays Reduced group of rules used to model the Repilo", "figure_data": "Group VSSMLVLSVSVSSSVSMVSSMMSLVSMVLLMGroup of rules used in the model of the \"Repilo\"Repilo AlertTemperatureVSSMLVLHumiditySVSSSSVSMSMLMSLMLVLLM"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_56", "figure_caption": "Comparison of performance considering different types of embedded FRBS", "figure_data": "Execution of 4000 continued inferencesFBRSTime (ms)Increase (%)Consumption (mA)Increase (%)Accuracy (%)Approximate17716,60,000,56660,000,00Descriptive17177,7-3,040,5469-3,48-28,15ApproximateReduced14080,3-20,520,4501-20,56-0,06"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_57", "figure_caption": "Controller's Self-organization Process", "figure_data": "ConfigurationSSE1SSE2MSE\u202210 31x1253.729237.603710.7032x10.452208.457248.3672x27.104 \u202210 \u221242.58452.4342x31.989 \u202210 \u22124 4.525 \u202210 \u22124 40.9222x41.219 \u202210 \u22124 8.874 \u202210 \u22125 34.0333x49.639 \u202210 \u22125 6.152 \u202210 \u22125 29.353"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_59", "figure_caption": "Results obtained with WM method Results obtained in both problems", "figure_data": "Dataset WIZ104.8 6.944 7.368 0.720 0.9091MOR77.6 0.985 0.973 0.129 0.0901Dataset Method Ref NR MSEtra \u03c3tra t-test MSEtst \u03c3tst t-test GM3M \u03c3GM3M t-test (\u03b4\u03b3\u03c1)WIZT S[13] 53.51.051 0.07 +2.386 1.95 +0.3490.08+(0.16 0.45 0.71)TSP2\u2212SI[7] 104.8 1.048 0.04 +1.243 0.12 +0.2600.11+ ( 0.10 0.41 0.66)T SSP2\u2212SI -29.2 0.921 0.06 *1.095 0.17 *0.4930.15*(0.34 0.57 0.70)MORT S[13] 34.10.031 0.01 =0.037 0.01 =0.3160.09+(0.14 0.36 0.76)TSP2\u2212SI[7] 77.60.036 0.01 +0.043 0.01 +0.1830.07+(0.06 0.17 0.72)T SSP2\u2212SI -15.4 0.028 0.01 *0.034 0.01 *0.5410.10*(0.44 0.50 0.76)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_60", "figure_caption": "Results of misspecification tests for three models facing real world cases (significance value: 0.95) sigmoid membership function Gaussian membership function", "figure_data": "model #rules\u03c3\u03b5 tAICp-value\u03c3\u03b5 tAICp-valueA20.191-3130.1790.205-3070.645B20.097-65900.0000.098-65700.000C110.122 -243570.2340.120 -245160.566"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_61", "figure_caption": "Results obtained using ALBH for 10 cities", "figure_data": "Cityschools vehiclesrangestudents bus stopkmeconomy(%) time(sec)GPS171211282,613,291--Apucarana8952500m16432462,366,212933222000m11742112,063,966211057GPS244013933,388,274--Arauc\u00e1ria3380500m23793793,367,108177182000m18643142,954,370132321Bom Jesus do Sul811GPS 500m 2000m471 471 397218 57 49537,872 500,066 456,570-7 15-133 77GPS450127117,553,586--Castro4952500m44096786,889,2929338842000m38246406,718,128118541GPS21606522,675,360--Lapa4423500m20782002,523,2926179522000m17591872,303,492147872GPS229113875,285,160--Londrina160107500m21693914,880,5368105222000m15943374,790,09893082GPS6523951,792,030--Maring\u00e110323500m6151281,111,740387232000m37299916,10249353GPS13169361,366,637--Medianeira3231500m12781431,292,976512822000m8781231,198,73612322GPS14076593,431,116--Ponta Grossa16559500m13432523,136,768962022000m11262353,054,114113262GPS181810903,359,002--Prudent\u00f3polis7266500m17663193,150,630695772000m14192702,887,784145864"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_62", "figure_caption": "The weights from the artificial data experiment for five principal components by the PSO-based PCA method", "figure_data": "PC1-0.0004-0.00040.00000.0010-1.0000PC2-0.00010.0000-0.00081.00000.0010PC30.0013-0.00091.00000.00080.0000PC40.0023-1.0000-0.00090.00000.0004PC5-1.0000-0.00230.0013-0.00010.0004"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_63", "figure_caption": "The weights from the artificial data experiment for EPP by the PSO-based", "figure_data": "EPP methodEC1-0.00330.0157-0.0012-0.00700.9998"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_64", "figure_caption": "The main steps of the Bees Algorithm are given below:", "figure_data": "Initialize ParametersCreate initial Population of bees, P 0Evaluate Population P 0WHILE stop criterion isn't reachedApply Local Search Procedure to all individuals in P tSelect nss best bees in P t , PB tt=iterationCompute probabilities for nsb best sitesCompute probabilities for (nss-nbs) selected sitesFOR i=1 to nss DOIF i<=nbs THENCompute number of bees (nb) recruited for best site iELSECompute number of bees (nb) recruited for selected site iFOR j=1 to nb DObee j = NeighbourhoodSearch (PB t i )Evaluate bee jSelect best bee:IF fitness (bee j ) < fitness(PB t i ) THENP t i = bee jELSEP t i = PB t"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_66", "figure_caption": "Results    ", "figure_data": "ProbLSGATabu SearchHDEHACOBeesBestFTimeBestF Time BestFTimeBestFTimeBestFTime165,63<1s65,63<1s65,63<1s65,63<1s65,63<1s2134,65<1s134,65 <1s 134,65<1s134,65<1s134,65<1s3270,26<1s270,26 <1s 270,26<5s270,26<1s270,26<1s4286,89<1s286,89 <1s 286,89<5s286,89<1s286,89<1s5335,09<1s335,09 <1s 335,09<5s335,092s335,09<1s.6371,121s371,12 <1s 371,1258s371,123s371,12<1s7401,211s401,491s401,21118s401,214s401,21<1s8563,197s563,341s563,19274s563,1914s563,193s9642,837s642,862s642,83456s642,8325s642,834s"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_67", "figure_caption": "Results -average fitnesses and standard deviations", "figure_data": "ProbLSGATSHDEHACOBeesAvgFStdAvgFStdAvgFStdAvgF StdAvgFStd165,63 0"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_68", "figure_caption": "Parameters and scales description", "figure_data": "Parameter Scale Descriptionp"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_69", "figure_caption": "Users    ", "figure_data": ""}, {"figure_label": "5a5b6", "figure_type": "table", "figure_id": "tab_70", "figure_caption": "Matrix || cij || = ||(rij , \u03b2ij , fij , pij , dij)|| (part 1) Matrix || cij || = ||(rij , \u03b2ij , fij , pij, dij)|| (part 2) Matrix || cij ||", "figure_data": "i iAccess points j Access points j1 42 53 6"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_72", "figure_caption": "Benchmark functions used in this study. N stands for the dimension of the functions and s their ranges.", "figure_data": "Test function"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_73", "figure_caption": "Experimental results, mean of the best solution over 100 independent runs, from standard ACOR and ACORL. The number in the parentheses indicate standard deviations. The last row shows the t-test. The asterisk indicates that the difference is not negligible.", "figure_data": "f1f5f6f7f11ACORL 2.36E-144 4.63E+01 5.63E-01 8.93E-034.62(\u03b1=1.8) (2.34E-143) (1.26E+01) (1.23) (1.07E-02) (1.14E+01)ACOR 2.24E-203 6.17E+012.282.02E-023.27(\u03b1=2.0)(0)(1.68E+01) (1.38) (2.71E-02)(2.7)t-test1.08-7.73*-9.25*-3.88*1.15"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_74", "figure_caption": "Design problems analyzed", "figure_data": "Fiber directionsConstraintsNo. design variablesP1 = [0, 45, 90]symmetric, balanced16P2 = [0, 30, 60, 90]symmetric32P3 = [0, 15, 30, 45, 60, 75, 90]symmetric32"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_75", "figure_caption": "Mean of the best solution over 100 independent runs, from standard ACOR and ACORL. The number in the parentheses indicate standard deviations. The last row shows the t-test between \u03b1=2.0 and \u03b1=1.8 results. The asterisk indicates that the difference is not negligible.", "figure_data": "P1P2P3ACORL3972.48 4076.84 4103.19(\u03b1=1.8)(0.73)(7.50)(6.63)ACORL3972.32 4074.57 4100.40(\u03b1=1.9)(0.98) (13.90) (9.85)ACOR3972.31 4072.76 4097.67(\u03b1=2.0)(1.02) (13.34) (17.04)t-test\u03b1=2.0 vs. \u03b1=1.8 1.332.67*3.02*"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_77", "figure_caption": "Base Performance: All instances included in prototype generation, all instances used for retrieval Leave-one-out Performance: Three trials with one instance excluded from prototype generation Method Hits Errors Av. Size Generation (sec.) Retrieval (sec.", "figure_data": "Method Hits Errors Av. Size Generation (sec.) Retrieval (sec.)FOIL27 18, 12 10,0350,403AU57 01680,1770,038SDTG53 44470,1820,047SDTGAU 55 22730,1620,038"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_78", "figure_caption": "Performance on noisy clusters: 110 clusters each containing one miss-placed instance", "figure_data": "Method Hits Errors Av. Size Generation (sec.) Retrieval (sec.)FOIL00, 57 10,0207,378AU453660,2020,051SDTG52 56190,2270,102SDTGAU 56 14100,1880,069"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_79", "figure_caption": "Regression scores obtained (using cross-validation) for different values of the radius of the insensitive zone or tube. In rows, for each combination of fruit load (l) and spacing (s), we report the averages of absolute error, -insensitive loss, \u0394 (Eq. 2), and correlations. The last rows shows the scores considering at the same time all types of plantations.", "figure_data": "Score= 0.1= 1= 2= 3= 4l = 1absolute error6.536.115.86 5.755.72s = 1\u03946.536.025.62 5.234.79correlation0.810.820.83 0.840.84out tube0.980.800.73 0.650.54l = 1absolute error7.407.076.87 6.887.13s = 2\u03947.407.006.60 6.356.41correlation0.820.820.82 0.820.82out tube1.000.860.73 0.630.61l = 2absolute error6.456.336.20 6.146.29s = 1\u03946.456.275.96 5.715.40correlation0.960.960.96 0.960.96out tube0.980.880.76 0.660.55l = 2absolute error6.566.135.84 5.735.80s = 2\u03946.566.055.59 5.164.90correlation0.960.960.97 0.970.97out tube0.990.830.75 0.640.56allabsolute error6.746.416.19 6.126.23\u03946.736.345.94 5.615.38correlation0.940.940.94 0.940.95out tube0.980.840.74 0.640.57"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_80", "figure_caption": "Five univariately most relevant variables selected in terms of Max_KS", "figure_data": "Variables"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_81", "figure_caption": "distributions. A plan derives the agent's next action, z, on the basis of the agent's world model for that strategy and the current state of the social model: z = b s (W t s , m t ), and z = s(Y t ). J s employs two forms of entropy-based inference: Maximum entropy inference. J + s , first constructs an information base I t s as a set of sentences expressed in L derived from Y t , and then from I t s constructs the world model, W t s , as a set of complete probability distributions [using Eqn. 2 in Sec. 2.3 below].of sentences derived from those in Y t that were received between time u and time t, and then from W u s and I", "figure_data": "Maximum relative entropy inference. Given a prior world model, W u s , where u < t, minimum relative entropy inference, J \u2212 s , first constructs the (u,t) incremental information base I s (u,t)sconstructs a new world model, W t s [using Eqn. 3 in Sec. 2.3 below]."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_83", "figure_caption": "Variables and parameters used in the population-based model", "figure_data": "(3) Updating the technological developmentTD new = TD old + inn* TD old *\u2206t(4) Aggregating greedz old = ( \u03a3 k y (k)old )/nVariablesxWorld economyyAverage greed of the populationTDTechnological development levelParametersaGrowth rate of the economybDecrease rate of the economy due to population greedcGrowth rate of the population greed based on the economyeDecrease rate of the population greedinnInnovation rate1) Updating the world economyx new = x old + (a*x old -b*x old * z old ) * \u2206t(2) y (k)new = y (k)old + (c k *b* x old *y (k)old *(2-y (k)old ) / TD old -e k *y (k)old ) * \u2206t (for all agents k)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_84", "figure_caption": "Initial settings for variables and parameters", "figure_data": "Parameter ValueVariableInitial valuea1.5x5b5.8yrandom in [0.2, 0.3]crandom in [0.0260, 0.0274]TD1erandom in [ 0.85, 0.89]inn0.01\u0394t0.1"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_85", "figure_caption": "The Syntax of CT L * CA Logic", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_86", "figure_caption": "Experimental Results", "figure_data": "n Slave Reachable Nodes Memory in ExecutionPropertyAgents states allocated use (MB) time (\u2248sec) 123411721,0586.60.22True True True True29540,7936.70.48True True True True3473222,2327.51.27True True True True42,159359,0418.02.32True True True True"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_87", "figure_caption": "Percentage of frame with errors", "figure_data": "No Face Detection3,1%No Nose/Mouth Detection0,8%Eyes/Nose Tracking Error11,4%Mouth Tracking Error7,8%Yaw Angle Error4,7%Roll Angle Error16,1%"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_88", "figure_caption": "Percentage of frame with errors", "figure_data": "Detections Correct Wrong % CorrectVehicle position3025583%Number of roadways1511474%"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_90", "figure_caption": "Activity recognition results", "figure_data": "ActivitySuccessFailure% successWalking5523794.00%Running2581893.00%Stopped478499.00%Climb stairs3312593.00%Down stairs4414192.00%"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_91", "figure_caption": "Thus, for the nth component of each vector:", "figure_data": "scales it by a scalar F (usually \u2208 [0, 1]), and creates a trial offspring) 1 u i + by t (adding the result toZ m(t)torZ k(t)that belongs to the current population, DE randomly samples three otherindividuals, i.e.Z i(t),) Z j and t (Z m(t), from the same generation (for distinct k,i, j, and m). It then calculates the (component wise) difference ofZ i(t)and) Z j , t ("}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_92", "figure_caption": "The proposed model has been implemented in Python Script (OpenEye-python-1.4.2-1-microsoft-win32-msvc-i686, on PIV 2.2-GHz PC, with a 512-KB cache and a 2-GB main memory in Windows Server 2003 environment. The validation and performance appraisal of the algorithm is discussed in next section.", "figure_data": "/* According to Equation 2 */end ifThreshold ij > 0.5, THEN the j th cluster center (of query atsession q)ij m is ACTIVE,ELSE is INACTIVE(Rule 1)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_93", "figure_caption": "Query Session Classification Using DE Example Data Set", "figure_data": "Search Session 1 qReformulatedQuery to DE classifier -Search Session 2 q\u2032Generalized or SpecializedIndian PaneerChinese TofuGeneralizedHill resorts In IndiaNorth India HillSpecializedResortsNational BirdNational Geographic ChannelExported ItemsExported FoodItems"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_94", "figure_caption": "Optimal Parameter Setting for the Proposed Model", "figure_data": "ParameterValuePopulation Size10*DimCross Over Rate on Query Transition0.9Scale Factor on Query reformulation0.8user-specified maximum number of20clusters K maxuser-specified maximum number ofclusters K min"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_95", "figure_caption": "Mean  ", "figure_data": "and Standard deviation over 50 independent runs assuming at least 2 querysession s for ReformulationDatasetAlgorithmAv. No. ofCSMean intraMean interQuerymeasureClusterClusterreformulationDistanceDistanceCluster foundspring 2006 Data AssetProposed Differential3.44 \u00b1 0.01391.7132 \u00b1 0.07724.6543 \u00b1 1.313122.6221 \u00b1 1.356120 inputEvolutionqueries"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_96", "figure_caption": "Experimental Parameters", "figure_data": "VariableValue Variable ValueR thresh20 \u2022 k0.01\u03c4010 \u22126 c0.999n20pm10m100 l100normalization0.4 \u03b11Total page requests 500, 000 \u03b20"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_97", "figure_caption": "shows an excerpt", "figure_data": "AxiomssubjectpredicateobjectP atient\u2203 doB.stringPTNXZ1doB\"20021108\"P atient\u2203 sex.GenderPTNXZ1sexFemaleP atient\u2203 hasReport.ReportPTNXZ1hasReportRPT1Report\u2203 hasDiag.DiseaseRPT1hasDiagPolyArthritisReport\u2203 hasSection.SectionRPT1hasSectionSTreat1hasReport \u2261 belongsT o \u2212STreat1typeTreatT reatSectionSTreat1hasDrugMethotrexate\u2203 hasDrug.DrugPTNXZ1...MH1P atient....M otherHistMH1hasDiagnosis RheumatoidArthr.M otherHist\u2203 hasDiag.DiseaseMH1treatedWith NSAIDSM otherHist\u2203 treatedW ith.DrugPolyArthritistypeArthritisRheumaticDis.DiseaseRheumatoidArthr. typeArthritis"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_99", "figure_caption": "Examples of rules obtained with different queries and contexts (between brackets). All the queries are performed with minimum confidence of 0.7. LOM stands for limitation of motion, ANA for anti-nuclear antibody and RF for Rheumatoid Factor.", "figure_data": "QueryExamples of rulesSup. Conf.Drug \u2227 F inding =\u21d2 LargeJointsAffected =\u21d2 Oligoarthritis0.19 0.91Disease [Patient]PresenceOfANA =\u21d2 Oligoarthritis0.14 0.76225 transactionsPrevDrugEtanercept =\u21d2 PrevDrugMethotrexate 0.12 0.922 rules (sup=0.1)PrevDrugPrednisone =\u21d2 SystemicArthrities0.11 1.0Fever , Rash =\u21d2 SystemicArthrities0.11 1.0F inding =\u21d2LargeJointsAffected =\u21d2 Oligoarthritis0.087 0.91Disease [Report]PresenseOfANA =\u21d2 Oligoarthritis0.063 0.76492 transactionsFever , Rash =\u21d2 SystemicArthrities0.05 1.012 rules (sup=0.05)F inding =\u21d2ANA Negative =\u21d2 RF Negative0.36 0.86F inding [Med.Ev.]LOMRightShoulder =\u21d2 LOMRightWrist0.08 0.8184 transactionsLOMLeftHip =\u21d2 LOMRightHip0.08 0.7321 rules (sup=0.08)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_100", "figure_caption": "Six news sites and number of articles in each site", "figure_data": "Site nameURLNumber of articlesasahi.comhttp://www.asahi.com6,688The Japan Times ONLINE http://www.japantimes.co.jp1,908Mainichi jphttp://mainichi.jp18,212NIKKEI NEThttp://www.nikkei.co.jp21,231MSN Sankei newshttp://sankei.jp.msn.com16,105YOMIURI ONLINEhttp://www.yomiuri.co.jp3,311Fig. 5. Events system presented from input article"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_101", "figure_caption": "Difference between important words and evaluation values. The title of the input article is \"Flights have resumed at Bangkok's international airport after antigovernment protesters ended their blockade\".", "figure_data": "using word classnot using word classimportant word evaluation valueimportant word evaluation valueBangkok0.0652airport0.1170stay0.0455service0.0864arrive0.0436Bangkok0.0652travel0.0417day0.0603Tokoname0.0375Thailand0.0562"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_102", "figure_caption": "Knowledge base", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_104", "figure_caption": "Experiment parameters", "figure_data": "Iterations 1500S500 blogsC20 categories, affected randomlye0 (mean), 100 (deviation) (signed Gaussian generation)vt=0100 (mean), 1000 (deviation) (unsigned Gaussian generation)qLogarithmic function of v, interpolated from real page ranks:q(0) = 0, q(500) = 1, q(1000) = 2, q(3000) = 3, ..., q(729000) = 8, ...HAvailable slots in blogroll placed at half-height, being placed on a lowerslot decreases exponentially click probabilityaRandom variable: average of one request per blog in 10 iterationsAgents1 meta agent, 1 SPMA, 8 SPSA, 1 CPMA, 8 CPSA, 1 test coordinator"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_105", "figure_caption": "WordNet taxonomy. This definition of IC enables obtaining IC values in a corpus-independent way. The node-based similarity measures include the metrics of Resnik [13], Jiang & Conrath [5], Lin [8] and Pirr\u00f3 & Seco", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_106", "figure_caption": "Correlation coefficients for the different values of \u03b2", "figure_data": "Original 0.00.10.20.40.50.60.70.91.0length 0.8401 0.6673 0.7958 0.8358 0.8550 0.8568 0.8571 0.8568 0.8556 0.8549wup0.7726 0.7726 0.7726 0.7726 0.7726 0.7726 0.7726 0.7726 0.7726 0.7726lch0.8293 0.7126 0.7446 0.7658 0.7907 0.7983 0.8039 0.8083 0.8144 0.8165res0.8308 0.8308 0.8433 0.8508 0.8587 0.8609 0.8624 0.8635 0.8650 0.8655lin0.8587 0.8587 0.8587 0.8587 0.8587 0.8587 0.8587 0.8587 0.8587 0.8587j&c0.8660 0.8308 0.8433 0.8508 0.8587 0.8609 0.8624 0.8635 0.8650 0.8655"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_107", "figure_caption": "Maximum values of correlation obtained for each measure", "figure_data": "Table 3. Normalization factor depending on themetric approachOriginal Max Corr length 0.8401 0.8571 0.60 \u03b2 wup 0.7726 0.7726 Original lch 0.8293 0.8296 12Metric Approach Normalization Factor wup edge-based depth(c1) + depth(c2) lin node-based IC(c1) + IC(c2)res0.83080.86762.8j&c0.86600.86722.8lin0.85870.8587 Original"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_108", "figure_caption": "Comparisons table between semantic expansion approaches", "figure_data": "SystemsPrecisionsP5 P10 P15 P30 MAPTerm-Based0.544 0.480 0.405 0.273 0.449Synset-Indexing [13] 0.648 0.484 0.403 0.309 0.459Concept-Based0.744 0.544 0.478 0.394 0.507"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_109", "figure_caption": "Variants of the model and accuracy values (taken from[2])", "figure_data": "ModelMain factors usedvariant"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_110", "figure_caption": "Results of the simulations of delinquency of pupils with alternative class compositions using model variant 12 and 13", "figure_data": "schoolclassbasebasebasescen1scen1scen1scen2scen2classsizebeforeafterafterbeforeafterafterbeforeafterv12v13v12v13v12"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_112", "figure_caption": "Argumentation-specific rhetorical relations tag set which is a part of annotation tag set", "figure_data": "LevelSublevelTag NameReq_evidenceRequirementReq_detailReq_yes/noAnswerAffirmation NegationEvidenceResponseArgumentationExplanation_argumentative ExampleBackgroundConsensusAgreement DisagreementAction requestRequest_to_do SuggestionPolitenessGratitude Apology"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_113", "figure_caption": "Frequent rhetorical relation Determination of bigram of rhetorical relation In this case, not only (Agreement, Disagreement) but also (Evidence, Disagreement) are regarded as relation bigrams. N-gram of relation, for any N, is determined in the same manner. Frequent bigrams and trigrams of rhetorical relations are respectively listed in Tables", "figure_data": "RelationFrequencyPercentageExplanation_argumentative11518%Agreement10817%Disagreement9415%Evidence6711%Suggestion497.8%Justification335.3%Req_evidence264.2%Request_to_do223.5%Req_detail193.0%Affirmation101.6%Other rhetorical relations325.1%Total627100%Fig. 1."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_114", "figure_caption": "Frequent bigrams of rhetorical relation", "figure_data": "Relation bigramFrequencyPercentager 1r 2SuggestionDisagreement276.8%EvidenceAgreement164.0%AgreementAgreement164.0%EvidenceDisagreement143.5%DisagreementAgreement133.3%Explanation_Explanation_ArgumentativeArgumentative133.3%Explanation_ArgumentativeAgreement123.0%SuggestionAgreement112.8%Explanation_DisagreementArgumentative112.8%DisagreementDisagreement92.3%Other rhetorical relations bigrams24562%Total396100%Table 4. Frequent trigram of rhetorical relationRelation trigramFrequency Percentager 1r 2r 3AgreementAgreementAgreement41.7%DisagreementEvidenceAgreement31.3%Explanation_EvidenceAgreementargumentative31.3%Explanation_Explanation_Explanation_argumentativeArgumentativeargumentative 31.3%EvidenceDisagreementEvidence31.3%AgreementSuggestionReq_detail31.3%AgreementSuggestionAgreement31.3%DisagreementEvidenceDisagreement31.3%SuggestionDisagreementExplanation_argumentative 31.3%Explanation_Explanation_AgreementargumentativeArgumentative31.3%Other rhetorical relations trigrams19983%Total230100%"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_115", "figure_caption": "Priori and posteriori probability for agreement pairs", "figure_data": "Relation r 1P (r 2 =Agreement|r 1 )P(r 1 |r 2 = Agreement)Evidence0.24(16 /67 )0.15(16/108)Agreement0.15(16 /108)0.15(16/108 )Disagreement0.14(13/94)0.12(13/108)Explanation_Argumenative 0.10(12 /115)0.11(12/108)Suggestion0.22(11/49 )0.10(11/108)Req_evidence0.12(3/26)0.028(3/108)Req_detail0.11(2/19)0.019(2/108)Request_to_do0.09(2 /22)0.019(2/108)Affirmation0.1(1/10)0.0093(1/108)Table 6. Priori and posteriori probability for disagreement pairsRelation r 1P (r 2 =Disagreement|r 1 )P(r 1 |r 2 =Disagreement)Evidence0.21(14 /67)0.15(14/94)Agreement0.037(4/108)0.043(4/94)Disagreement0.10(9/94)0.10(9/94)Explanation_Argumenative 0.043(5/115)0.10(5/94)Suggestion0.60(27/49)0.30(27/94)Req_evidence0(0/26)0(0/94)Req_detail0(0/19)0(0/94)Request_to_do0.045(1/22)0.011(1/94)Affirmation0(0/108)0(0/108)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_116", "figure_caption": "Two different conditions", "figure_data": "ConditionCond. A Cond. B# user utterances3569PAUSE (sec.)1.02.0AVERAGE (sec.)0.735.27"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_117", "figure_caption": ": user utterance = \uff5bX: ASR result, tb: barge-in time } T i : i-th item enumerated by the system", "figure_data": "tb: barge-in timeX: ASR resultfrequency function of utterance timingtiP( U|T)1 ()P( t|,1 C)P(|,C2)T=argmaxT iP( U|i T)T: user's referent"}, {"figure_label": "23", "figure_type": "table", "figure_id": "tab_119", "figure_caption": "Identification accuracy [%] for user utterances Identification accuracy [%] for \u03b1 in Cond. 4", "figure_data": "ConditionReferential expression Content expression Total(#:263)(#:137)(#:400)1: only barge-in timing84.825.564.52: only barge-in timing model87.832.168.83: our method81.453.371.84: + numbers85.257.775.8\u03b1 value Referential expression Content expression Total(#:263)(#:137)(#:400)0.087.832.168.80.286.742.371.50.485.954.775.30.685.257.775.80.884.856.975.31.00.7643.115.3"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_120", "figure_caption": "Identification accuracy [%] by using LSM", "figure_data": "Condition Referential expression Content expression Total(#: 263)(#: 137)(#: 400)Using LSM85.258.376.0"}, {"figure_label": "45", "figure_type": "table", "figure_id": "tab_121", "figure_caption": "Numerical component relative to F T Severity degrees relative to ET", "figure_data": "a, bSI, HGL SI, HGL SI, HGL SI, HGLP (T E = T | a, b) 0.84620.3750.60.0294a, bEF, CT P EF , CT P EF, CT P EF , CT PP (SI = T | a, b) 0.8750.3330.750.0571aTVFT V FP (HGL = T | a)0.7660.111P (EF )0.7593P (CT P )0.7407P (T V F )0.8519\u2212"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_122", "figure_caption": "Classification results: 4-dimensional feature vector True/Error classes Case 1 Case 2 Case 3 Case 4 Case 5 ) 87.50% 88.33% 95.00% 77.50% 90.41% Classification results: 17-dimensional feature vector followed by PCA True/Error classes Case 1 Case 2 Case 3 Case 4 Case 5", "figure_data": "C1 true (T P )119292978114C2 true (T N)912428108103Type I error (F A)1114 26Type II error (M ) 29621217Performance (P C1 true (T P )120303090114C2 true (T N)91232899108Type I error (F A)0003 06Type II error (M ) 29722112Performance (P ) 87.91% 88.33% 96.67% 78.75% 92.50%"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_123", "figure_caption": "The higher order derivatives for this base function(8)    ", "figure_data": "d 2 F (x)d 3 F (x)d 4 F (x)d 5 F (x)dx 2dx 3dx 4dx 5"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_124", "figure_caption": "Some convex functions and their derivatives, x > 0", "figure_data": "No.F (x)d 2 F dx 2d 3 F dx 3d n F dx n1x log(x)FirstGroup"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_125", "figure_caption": "Summary of divergences in contrast with corresponding MMDS (eq(1))", "figure_data": "DivergenceF (x) W (Dij)Terms of BMMDS eq(6)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_126", "figure_caption": "Example for 4 classes and 5 sources of quality feature set (ranking) based on discriminant (number of activities discriminated, first column) and robustness (number of motion sensor where the feature is discriminant, second column) criterions", "figure_data": "Discriminant capacityRobustnessQuality group5#14#23#32#41#55#64#73#82#91#105#114#123#132#141#155#164#173#182#191#20-#21"}], "formulas": [{"formula_id": "formula_0", "formula_text": "ij i j i j i j d jaccard co t t d d d = + \u2212 (1)", "formula_coordinates": [23.0, 121.62, 87.95, 266.36, 26.77]}, {"formula_id": "formula_1", "formula_text": "i j i j d t d t d D freq co t t f f \u2208 = \u00d7 \u2211(2)", "formula_coordinates": [23.0, 124.38, 167.29, 263.6, 23.06]}, {"formula_id": "formula_2", "formula_text": "[ ]( ) ( ) ( ) ( )", "formula_coordinates": [24.0, 140.1, 326.8, 135.48, 28.5]}, {"formula_id": "formula_4", "formula_text": "R C f t ift V R P t N R p t o th erw ise \u03b3 \u03b4 \u23a7 \u2208 \u23aa = \u23a8 \u23aa \u23a9 (( ) ( ) ( ) ( )", "formula_coordinates": [24.0, 137.22, 411.5, 242.95, 39.48]}, {"formula_id": "formula_5", "formula_text": ")7", "formula_coordinates": [24.0, 380.17, 427.41, 7.82, 9.0]}, {"formula_id": "formula_6", "formula_text": "P lace 0 0 3 0 0 3 AcademicDepartment 0 0 2 0 0 2 Event 1 1 1 0 0 2 Researcher 1 1 0 0 0 1 University 1 1 0 0 0 1 P rof essor 1 1 0 0 0 1 Scientif icP ublication 1 1 0 0 0 1 T hing 1 1 0 0 0 1 Country 0 0 1 0 0 1 Classif ication 0 0 1 0 0 1", "formula_coordinates": [35.0, 46.64, 122.19, 327.41, 110.8]}, {"formula_id": "formula_7", "formula_text": "\u2022 1 (A):", "formula_coordinates": [54.0, 47.82, 186.94, 42.97, 12.98]}, {"formula_id": "formula_8", "formula_text": "T u = { t \u00d7 r \u00d7 l |l \u2208 L u } (1)", "formula_coordinates": [60.0, 160.76, 198.6, 226.96, 10.33]}, {"formula_id": "formula_9", "formula_text": "F = ua\u2208U F T ua (2)", "formula_coordinates": [60.0, 185.0, 265.32, 202.71, 21.68]}, {"formula_id": "formula_10", "formula_text": "T ua = { Korea, r 1 , EN , Korea, r 2 , EN , Corea, r 1 , SP } (3) T u b = { Cor\u00e9e, r 2 , F R } (4", "formula_coordinates": [60.0, 89.6, 367.66, 298.11, 25.78]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [60.0, 383.46, 382.94, 4.25, 8.78]}, {"formula_id": "formula_12", "formula_text": "\u03c4 U (l, l ) = |{t| t \u00d7 r \u00d7 l , t \u00d7 r \u00d7 l \u2208 T ua , l = l }| max(|{t|t \u00d7 l}|, |{t |t \u00d7 l }|) (5)", "formula_coordinates": [60.0, 104.0, 521.02, 283.72, 23.18]}, {"formula_id": "formula_13", "formula_text": "\u03c4 R (t, t ) = |{ t, t | t \u00d7 r \u00d7 l \u2208 T ua , t \u00d7 r \u00d7 l \u2208 T u b }| max(|R Fa |, |R F b |) (6)", "formula_coordinates": [61.0, 94.28, 85.3, 293.44, 24.46]}, {"formula_id": "formula_14", "formula_text": "|U + | |U| \u2212 1) (7", "formula_coordinates": [61.0, 242.47, 339.84, 140.98, 24.36]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [61.0, 383.46, 348.14, 4.25, 8.78]}, {"formula_id": "formula_16", "formula_text": "C { t, t | t \u00d7 r \u00d7 l , t \u00d7 r \u00d7 l \u2208 T ua , u a = max u\u2208C l,l u} (8", "formula_coordinates": [61.0, 99.2, 557.74, 284.26, 20.88]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [61.0, 383.46, 558.14, 4.25, 8.78]}, {"formula_id": "formula_18", "formula_text": "T x + b)", "formula_coordinates": [67.0, 357.54, 372.03, 30.68, 10.97]}, {"formula_id": "formula_19", "formula_text": ") 1 /( 1 ) ( x e x f \u2212 + = (1)", "formula_coordinates": [67.0, 163.26, 413.01, 224.73, 15.28]}, {"formula_id": "formula_20", "formula_text": ", 2 2 i T i b i o + = y w", "formula_coordinates": [67.0, 43.86, 527.8, 65.89, 16.01]}, {"formula_id": "formula_21", "formula_text": "\u2211 \u2212 \u2212 \u2212 = \u2212 i N j ij T ij h i N i q 1 1 )) ( ) ( exp( x x x x (2)", "formula_coordinates": [68.0, 127.38, 123.19, 260.61, 30.99]}, {"formula_id": "formula_22", "formula_text": "\u2211 + = l i T b w i C w w 1 , , 2 1 min \u03be \u03be subject to i i T i b x w y \u03be \u03c6 \u2212 \u2265 + 1 ) ) ( ( , 0 \u2265 i \u03be , i=1, \u2026., l. (3) Where } 1 , 1 { \u2212 \u2208 i y", "formula_coordinates": [68.0, 87.3, 523.04, 300.68, 48.11]}, {"formula_id": "formula_23", "formula_text": "i i x x = ) (", "formula_coordinates": [69.0, 281.22, 104.84, 34.82, 14.51]}, {"formula_id": "formula_25", "formula_text": "K i r n n ij i j ij ij i j ij ,..., 2 , 1 : = \u2211 = \u2211 \u2260 \u2260 \u03bc (5) subject to \u2211 = 1 i p .", "formula_coordinates": [70.0, 42.18, 127.78, 345.81, 45.1]}, {"formula_id": "formula_26", "formula_text": ") ) ( * exp( 1 2 \u2211 \u2212 \u2212 = = p k k j k i j i x x s \u03b1 , (6", "formula_coordinates": [71.0, 133.86, 256.63, 250.22, 24.01]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [71.0, 384.08, 263.38, 3.91, 8.99]}, {"formula_id": "formula_28", "formula_text": "Crit = 1 1 + W B \u2022 m m + 0.5 log 2 (k+1)+1 (1)", "formula_coordinates": [90.0, 129.92, 536.28, 257.79, 32.16]}, {"formula_id": "formula_29", "formula_text": "B = k i=1 |C i | \u2022 s(c i , g", "formula_coordinates": [91.0, 53.12, 51.12, 94.52, 14.04]}, {"formula_id": "formula_30", "formula_text": "1 2 2 21 2 1 1 11 1 knk k k n n x x S x x S x x S = = =", "formula_coordinates": [98.0, 158.58, 352.06, 204.93, 13.31]}, {"formula_id": "formula_31", "formula_text": "\u23a9 \u23a8 \u23a7 > = otherwise , 0 ) , ( if , 1 ) , ( i i t i t m x dist N x f \u03c3 (1)", "formula_coordinates": [98.0, 124.02, 419.52, 263.92, 29.45]}, {"formula_id": "formula_32", "formula_text": "N i (0 \u2264 i \u2264 k).", "formula_coordinates": [98.0, 52.98, 467.0, 56.0, 11.76]}, {"formula_id": "formula_33", "formula_text": "\u2211 = \u2212 = = + + + = d k c j c i c j c i n j n i n j n i c j c i n j n i j i x x Cosine d x x Co Max Euclidean x x Euclidean x x Eu x x Co I I I x x Eu I I I x x dist 1 2 1 2 2 1 1 )) , ( 1 ( 1 ) ,( _ ) , ( ) , ( ) , ( ) , ( ) , ( (2)", "formula_coordinates": [99.0, 106.14, 58.64, 281.84, 81.04]}, {"formula_id": "formula_34", "formula_text": "\u221a 3 \u00d7 x,", "formula_coordinates": [107.0, 163.28, 337.9, 35.77, 17.78]}, {"formula_id": "formula_35", "formula_text": "k \u2265 k 0 = O( \u22122 log(n)). For every set P of n points in R d there exists f : R d \u2192 R k such that for all u, v \u2208 P (1 \u2212 ) u \u2212 v 2 \u2264 f (u) \u2212 f (v) 2 \u2264 (1 + ) u \u2212 v 2 (1)", "formula_coordinates": [107.0, 41.84, 437.84, 345.88, 45.16]}, {"formula_id": "formula_36", "formula_text": "W-L W-L W-L W-L W-L W-L Total #", "formula_coordinates": [120.0, 93.32, 87.75, 280.39, 19.48]}, {"formula_id": "formula_37", "formula_text": "V M -R F E + R F -P C A 0 4 3 8 5 3 2 3 S V M -R F E + R F -F a s t I C A 0 3 1 2 2 5 1 3 ReliefF+RF-PCA 0 -2 2 0 2 3 5 ReliefF+RF-FastICA 1 -4 2 0 3 2 4 SVM-RFE+Boosting -1 2 -3 1 0 1 0 ReliefF+Boosting 0 -1 -2 -1 -1 -3 -8 ReliefF+Bagging 2 -2 0 -4 -6 -4 -14 SVM-RFE+Bagging -2 0 -3 -6 -5 -7 -23", "formula_coordinates": [120.0, 44.0, 112.11, 325.29, 85.24]}, {"formula_id": "formula_38", "formula_text": "S i,j = RM SE j \u2212 RM SE i max(RM SE i , RMSE j )", "formula_coordinates": [128.0, 147.55, 509.5, 133.12, 23.55]}, {"formula_id": "formula_39", "formula_text": "RM SE = n i=1 (a i \u2212 p i ) 2 n", "formula_coordinates": [132.0, 156.43, 196.68, 114.85, 30.36]}, {"formula_id": "formula_40", "formula_text": "RM SE = n i=1 (q i \u2212 p i ) 2 n", "formula_coordinates": [132.0, 156.92, 271.32, 114.01, 30.36]}, {"formula_id": "formula_41", "formula_text": "Classif ier : I \u2212\u2192 C", "formula_coordinates": [138.0, 169.39, 294.46, 90.04, 9.62]}, {"formula_id": "formula_42", "formula_text": "Classif ier-C i : I \u2212\u2192 C i", "formula_coordinates": [138.0, 161.23, 590.86, 106.5, 9.99]}, {"formula_id": "formula_43", "formula_text": "Classif ier-AC i : I i \u2212\u2192 C i", "formula_coordinates": [139.0, 156.2, 217.54, 116.58, 9.99]}, {"formula_id": "formula_44", "formula_text": "P C1 t1 dm , t1 f b1 , t1 f b2 T1, P1, P2 LT 01 P C2 t1 f b1 , t2 dm , t2 f T1, T2, P1 F T 02 P C3 t1 f b1 , t1 dE , t2 dE , t2 dm T1, P1, T2 T T 02 P C4 t1 f b2 , t3 dm , t3 f T1, P2, T3 F T 03 P C5 t1 f b2 , t1 dE , t3 dE , t3 dm , r3p T1, P2, T3, R3 T T 03 P C6 t4 dm T4 LT 04 P C7 t4 f b T4, P5 F T 04", "formula_coordinates": [140.0, 102.32, 270.63, 207.05, 77.18]}, {"formula_id": "formula_45", "formula_text": "P C1 1 1 1 1 1 1 1 1 1 P C2 1 1 1 1 1 1 1 P C3 1 1 1 1 1 1 1 1 P C4 1 1 1 1 1 1 P C5 1 1 1 1 1 1 1 P C6 1 1 1 1 1 P C7 1 1 1 1 1", "formula_coordinates": [141.0, 46.4, 97.47, 334.13, 76.6]}, {"formula_id": "formula_46", "formula_text": "I 3 = T S F T 01 \u00d7 T S F T 02 \u00d7 T S LT 01 \u00d7 T S LC01 \u00d7 T S T T 02 .", "formula_coordinates": [141.0, 41.84, 281.5, 240.33, 10.0]}, {"formula_id": "formula_47", "formula_text": "p(X 0 , X 1 , ..., X n\u22121 ) = n\u22121 i=0 p(X i |\u03a0 i ),(1)", "formula_coordinates": [146.0, 136.76, 162.36, 250.96, 30.24]}, {"formula_id": "formula_48", "formula_text": "f estimation (X 0 , X 1 , ..., X n\u22121 ) =f + n\u22121 i=0 (f (X i |\u03a0 i ) \u2212f (\u03a0 i )),(2)", "formula_coordinates": [146.0, 87.44, 573.0, 300.27, 30.36]}, {"formula_id": "formula_49", "formula_text": "H(X 0 , X 1 , ..., X n\u22121 ) = n\u22121 i=0 H(X i |\u03a0 i ) = n\u22121 i=0 \u03c0i\u2208Qi p(\u03c0 i )H(X i |\u03a0 i = \u03c0 i ) = \u2212 n\u22121 i=0 \u03c0i\u2208Qi p(\u03c0 i ) xi\u2208Xi p(x i |\u03c0 i ) log 2 p(x i |\u03c0 i ) = \u2212 n\u22121 i=0 \u03c0i\u2208Qi xi\u2208Xi p(x i , \u03c0 i ) log 2 p(x i |\u03c0 i ) = \u2212 n\u22121 i=0 \u03c0i\u2208Qi xi\u2208Xi m(x i , \u03c0 i ) N log 2 m(x i , \u03c0 i ) m(\u03c0 i ) ,(3)", "formula_coordinates": [147.0, 58.52, 333.36, 329.2, 138.0]}, {"formula_id": "formula_50", "formula_text": "Case 1: If H(i) \u2264 H(0) 2", "formula_coordinates": [149.0, 53.24, 115.76, 102.46, 14.93]}, {"formula_id": "formula_51", "formula_text": "1. Put X into E(i) to create E (i). Compute the entropy H (i) for E (i). 2. If H (i) \u2264 H(i), estimate f estimation (X) = f (Y ), Y \u2208 E(i), \u2200Z \u2208 E(i), f(Y ) \u2264 f (Z) . (4", "formula_coordinates": [149.0, 70.16, 149.02, 315.01, 43.96]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [149.0, 383.45, 183.38, 4.25, 8.78]}, {"formula_id": "formula_53", "formula_text": "3. t \u2190 t + 1. Case 2: If H(i) > H(0) 2 or t = \u03ba, 1. Evaluate all offspring of O(i). 2. t \u2190 0.", "formula_coordinates": [149.0, 53.23, 217.06, 160.34, 51.98]}, {"formula_id": "formula_54", "formula_text": "f onemax (X 0 , X 1 , ..., X n\u22121 ) = n\u22121 i=0 X i ,(5)", "formula_coordinates": [150.0, 136.4, 411.96, 251.32, 30.36]}, {"formula_id": "formula_55", "formula_text": "f trap5 (u) = 5 i f u = 5 4 \u2212 u if u < 5 , (6", "formula_coordinates": [150.0, 141.32, 534.13, 242.14, 21.11]}, {"formula_id": "formula_56", "formula_text": ")", "formula_coordinates": [150.0, 383.46, 540.14, 4.25, 8.78]}, {"formula_id": "formula_57", "formula_text": "v = (v 1 , v 2 \u2026 v n ) \u2208 {0,1} n \u2022 A clause C i of length k is a disjunction of k literals, C i =( x 1 OR x 2 OR\u2026 OR x k )", "formula_coordinates": [156.0, 42.18, 331.54, 332.72, 25.15]}, {"formula_id": "formula_58", "formula_text": "SC , \u03a9", "formula_coordinates": [156.0, 310.86, 435.22, 22.24, 12.51]}, {"formula_id": "formula_59", "formula_text": "\u03b2 \u00d7 = \u2211 j j affinity i affinity i nbClones (1)", "formula_coordinates": [158.0, 135.18, 134.68, 252.8, 34.96]}, {"formula_id": "formula_60", "formula_text": "Aff Aff Aff aff affN i i min max min \u2212 \u2212 = (2)", "formula_coordinates": [158.0, 133.86, 248.0, 254.12, 32.44]}, {"formula_id": "formula_61", "formula_text": "max ) 1 ( min \u00d7 \u2212 + \u00d7 = i affN i affN i s nbMutation (3)", "formula_coordinates": [158.0, 99.78, 328.02, 288.21, 17.79]}, {"formula_id": "formula_62", "formula_text": "A = n 0 \u00d7 n 1 \u00d7 . . . n m\u22122 \u00d7 n m\u22121 ,", "formula_coordinates": [164.0, 244.76, 77.62, 142.89, 10.0]}, {"formula_id": "formula_63", "formula_text": "M s = s i=1 n i ! (n i \u2212 p i )!(1)", "formula_coordinates": [164.0, 171.8, 266.04, 215.96, 30.36]}, {"formula_id": "formula_64", "formula_text": "Area(S) = t\u2208T G area S[t]ip * S[t] dedi + p\u2208PE(S) area p (2)", "formula_coordinates": [166.0, 106.03, 269.98, 281.72, 20.91]}, {"formula_id": "formula_65", "formula_text": "M\u2208C(GT ) t\u2208M time S[t]ip + T (S) T (S) = \u23a7 \u23aa \u23a8 \u23aa \u23a9 0 if S[t] dedi = 1or P (t) = D (t) = \u2205 t \u2208 P (t) \u222a D (t) t < t time S[t ]ip otherwise(3)", "formula_coordinates": [166.0, 73.87, 538.66, 313.88, 59.26]}, {"formula_id": "formula_66", "formula_text": "\u03be a \u2264 P ower(S) = t\u2208GT power S[t]ip \u2264 \u03be a (4)", "formula_coordinates": [167.0, 129.44, 114.34, 258.32, 20.55]}, {"formula_id": "formula_67", "formula_text": "CH(i, j) = | i/N \u2212 j/N | + | i\\N \u2212 j\\N | (5)", "formula_coordinates": [167.0, 119.12, 482.74, 268.64, 9.62]}, {"formula_id": "formula_68", "formula_text": "SW(i, j) = CH(i, j) + 1(6)", "formula_coordinates": [167.0, 163.4, 590.86, 224.36, 9.62]}, {"formula_id": "formula_70", "formula_text": "Q\u2208C(GT ) (T ime p (Q) + T ime c (Q) + T (Q)) T (Q) = t L \u00d7 (f 1 (Q) + f 2 (Q))](8)", "formula_coordinates": [168.0, 97.52, 408.22, 290.24, 28.0]}, {"formula_id": "formula_71", "formula_text": "T ime p (Q) = t\u2208Q time AS [t]ip + T (S) T (S) = \u23a7 \u23aa \u23a8 \u23aa \u23a9 0 if A S [t] dedi = 1 or P (t) = D (t) = \u2205 t \u2208 P (t) \u222a D (t) t < t time AS[t ]ip otherwise(9)", "formula_coordinates": [168.0, 49.75, 510.94, 338.0, 59.98]}, {"formula_id": "formula_72", "formula_text": "T ime c (Q) = d(t, t ) \u2208 E(GT )| t \u2208 Q, t \u2208 Q vol d(t,t ) phit t L SW(S[d t ] rec , S[d t ] rec )(t R + t L ) (10)", "formula_coordinates": [169.0, 56.47, 82.08, 331.28, 27.67]}, {"formula_id": "formula_73", "formula_text": "E i,j bit = SW \u00d7 E S bit + CH \u00d7 E C bit (13", "formula_coordinates": [170.0, 142.99, 238.44, 240.57, 13.92]}, {"formula_id": "formula_74", "formula_text": ")", "formula_coordinates": [170.0, 383.57, 241.15, 4.19, 8.97]}, {"formula_id": "formula_75", "formula_text": "[c i,x0 , c x0,x1 , c x1,x2 , . . . , c", "formula_coordinates": [170.0, 41.84, 312.02, 104.4, 9.59]}, {"formula_id": "formula_76", "formula_text": "IF pair >= 0.5 pair = 1 ELSE IF pair <0.5 pair = 0", "formula_coordinates": [174.0, 42.18, 554.16, 341.18, 5.78]}, {"formula_id": "formula_79", "formula_text": "L(V, + k a )= \u2211 + + \u2208 ) t , (s P a : i i i k wi L(V, \u2212 k a )= \u2211 \u2212 \u2212 \u2208 ) t , (s P a : i i i k wi (2a) \u2200k=1,\u2026,n; \u2200i=1,\u2026,m(2b)", "formula_coordinates": [175.0, 111.9, 382.52, 276.26, 40.15]}, {"formula_id": "formula_80", "formula_text": "max{max L(V, + k a ),max L(V, \u2212 k a )} (3)", "formula_coordinates": [175.0, 125.58, 424.11, 262.58, 13.08]}, {"formula_id": "formula_81", "formula_text": "{ ) ( ) ( ) ( 1 1 pp r if P DC P ntrator osestConce MutationCl t i t g np t g Pm < \u2212 \u2212 = . 1 \u2212 t g P", "formula_coordinates": [175.0, 65.34, 567.02, 132.93, 31.89]}, {"formula_id": "formula_82", "formula_text": ") ( 1 \u2212 = t g np t i P DC Pm", "formula_coordinates": [176.0, 44.34, 137.16, 59.59, 15.33]}, {"formula_id": "formula_83", "formula_text": ") ( 1 \u2212 = t g t i P rection MutationDi Pm .", "formula_coordinates": [176.0, 208.74, 158.16, 110.25, 12.07]}, {"formula_id": "formula_84", "formula_text": "FOR i=1 TO np DO i1= random (m/2) i2= random (m-m/2) for c=i1 to c=i2+i1 if random(2)=0 if (sc-tc) = n/2 if random(2)=1 testSolution[c] = CC else testSolution[c] = C else if (sc-tc) > n/2 testSolution[c] = C else testSolution[c] = CC else testSolution[c]= bestSolution[c]; if fitnessTest < fitnessOld break; if fitnessTest < fitnessNew newSolution=testSolution", "formula_coordinates": [176.0, 42.18, 200.04, 265.82, 105.38]}, {"formula_id": "formula_85", "formula_text": "{ ) ( ) , ( 1 pc r if P Pm CR Pm t i t i t i t i Pt < \u2212 = .", "formula_coordinates": [176.0, 93.18, 401.73, 120.09, 30.53]}, {"formula_id": "formula_86", "formula_text": ") , ( 1 \u2212 = t i t m t i P P CR Pt", "formula_coordinates": [176.0, 150.78, 447.61, 63.31, 15.32]}, {"formula_id": "formula_87", "formula_text": "1 \u2212 = t i t i P Pt", "formula_coordinates": [176.0, 125.7, 489.65, 29.69, 14.23]}, {"formula_id": "formula_88", "formula_text": "\u23aa \u23a9 \u23aa \u23a8 \u23a7 = \u2212 < \u2212 )) 1 ( ) ( (", "formula_coordinates": [176.0, 217.5, 551.85, 117.21, 27.28]}, {"formula_id": "formula_89", "formula_text": "Define", "formula_coordinates": [177.0, 42.18, 262.68, 28.82, 5.78]}, {"formula_id": "formula_90", "formula_text": "max x f 1 (x) = sen(10\u03c0x) + 1 min x,y f 2 (x, y) = cos(4x) + 3sen(2y) + (y \u2212 2) 2 \u2212 (y + 1) (1)", "formula_coordinates": [188.0, 91.03, 515.54, 296.68, 33.48]}, {"formula_id": "formula_91", "formula_text": "S p = T 1 T p (2)", "formula_coordinates": [189.0, 196.28, 50.9, 191.44, 23.15]}, {"formula_id": "formula_92", "formula_text": "E p = S p p (3)", "formula_coordinates": [189.0, 195.56, 116.9, 192.16, 22.33]}, {"formula_id": "formula_93", "formula_text": "N = N 1 + N 2 , (1", "formula_coordinates": [196.0, 182.71, 161.66, 200.75, 9.6]}, {"formula_id": "formula_94", "formula_text": ")", "formula_coordinates": [196.0, 383.46, 161.66, 4.25, 8.78]}, {"formula_id": "formula_95", "formula_text": "V = N log 2 n, (2", "formula_coordinates": [196.0, 185.72, 224.9, 197.74, 9.6]}, {"formula_id": "formula_96", "formula_text": ")", "formula_coordinates": [196.0, 383.46, 224.9, 4.25, 8.78]}, {"formula_id": "formula_97", "formula_text": "V * = (2 + n * 2 )log 2 (2 + n * 2 ).", "formula_coordinates": [196.0, 105.55, 280.28, 121.81, 12.77]}, {"formula_id": "formula_98", "formula_text": "E = V L (3", "formula_coordinates": [196.0, 198.92, 349.58, 184.54, 22.33]}, {"formula_id": "formula_99", "formula_text": ")", "formula_coordinates": [196.0, 383.46, 356.3, 4.25, 8.78]}, {"formula_id": "formula_100", "formula_text": "\u2200A \u2208 L X , B \u2208 L Y A 1 (y) = inf x\u2208X {I(A(x), R(x, y))} , B 2 (x) = inf y\u2208Y {I(B(y), R(x, y))} .", "formula_coordinates": [202.0, 143.84, 415.2, 185.89, 67.44]}, {"formula_id": "formula_101", "formula_text": "\u2200(A, A 1 ), (C, C 1 ) \u2208 L, (A, A 1 ) \u2264 (C, C 1 ) if A \u2264 C (or eq. C 1 \u2264 A 1", "formula_coordinates": [203.0, 41.83, 113.5, 282.73, 10.0]}, {"formula_id": "formula_102", "formula_text": "c t (x) = x t = \u23a7 \u23aa \u23a8 \u23aa \u23a9 1 + m(x \u2212 a) if x \u2264 a 1 i f a \u2264 x \u2264 b 1 + m(b \u2212 x) if x \u2265 b Where m = min 1 a , 1 1 \u2212 b .", "formula_coordinates": [204.0, 41.84, 67.44, 264.07, 84.6]}, {"formula_id": "formula_103", "formula_text": "Definition 2. The L-Fuzzy context (L, X, Y C , R C ), where Y C = (Y \\Z) {y j ty j , \u2200y j \u2208 Z} and R C (x i , y j ) = R(x i , y j ) if y j \u2208 Y R(x i , y j ) ty j in other case is said to be a labeled L-Fuzzy context.", "formula_coordinates": [205.0, 41.84, 175.52, 345.85, 73.86]}, {"formula_id": "formula_104", "formula_text": "Proposition 1. If A \u2208 L X is a basic point, A(x) = 1 if x = x i 0 in other case then, A 1 (y) = R C (x i , y) \u2208 L, \u2200y \u2208 Y C", "formula_coordinates": [205.0, 41.83, 314.76, 228.85, 73.33]}, {"formula_id": "formula_105", "formula_text": "A 1 (y) = inf x\u2208X {I(A(x), R C (x, y))} = R C (x i , y), \u2200y \u2208 Y C .", "formula_coordinates": [205.0, 89.72, 454.52, 250.09, 19.13]}, {"formula_id": "formula_106", "formula_text": "A 1 (y j ty j ) = R C (x i , y j ty j ) \u2200x i \u2208 X, y j ty j \u2208 Y C , t yj \u2208 T (V ) .", "formula_coordinates": [205.0, 83.48, 536.0, 262.45, 16.56]}, {"formula_id": "formula_107", "formula_text": "A 12 (x) = inf y\u2208Y C {I(A 1 (y), R C (x, y))} = inf y\u2208Y C {I(R C (x i , y), R C (x, y))} .", "formula_coordinates": [206.0, 66.92, 71.36, 295.57, 19.73]}, {"formula_id": "formula_108", "formula_text": "Table 1. L-Fuzzy context R y1 y2 y3 x1 0.7 0.2 1 x2 0 0.1 0.8 x3 1 0.4 0.6 x4 0.3 0 0.9", "formula_coordinates": [206.0, 162.08, 208.83, 105.37, 77.44]}, {"formula_id": "formula_109", "formula_text": "x 1 \u2192 {(x 1 /1, x 2 /0. 3, x 3 /0. 6, x 4 /0. 6), (y 1 /0. 7, y 2 /0. 2, y 3 /1)} x 2 \u2192 {(x 1 /1, x 2 /1, x 3 /0. 8, x 4 /0. 9), (y 1 /0, y 2 /0. 1, y 3 /0. 8)} x 3 \u2192 {(x 1 /0. 7, x 2 /0, x 3 /1, x 4 /0. 3), (y 1 /1, y 2 /0. 4, y 3 /0. 6)} x 4 \u2192 {(x 1 /1, x 2 /0. 7, x 3 /0. 7, x 4 /1), (y 1 /0. 3, y 2 /0, y 3 /0. 9)}", "formula_coordinates": [206.0, 83.12, 340.66, 263.34, 54.88]}, {"formula_id": "formula_110", "formula_text": "Table 2. New relation R C R C y1 y 2medium y3 x1 0.7 0.5 1 x2 0 0 . 2 0 . 8 x3 1 1 0 . 6 x4 0.3 0 0.9", "formula_coordinates": [206.0, 158.48, 500.87, 112.61, 77.36]}, {"formula_id": "formula_111", "formula_text": "x 1 \u2192 {(x 1 /1, x 2 /0. 3, x 3 /0. 6, x 4 /0. 5), (y 1 /0. 7, y 2medium /0. 5, y 3 /1)} x 2 \u2192 {(x 1 /1, x 2 /1, x 3 /0. 8, x 4 /0. 8), (y 1 /0, y 2medium /0. 2, y 3 /0. 8)} x 3 \u2192 {(x 1 /0. 5, x 2 /0, x 3 /1, x 4 /0), (y 1 /1, y 2medium /1, y 3 /0. 6)} x 4 \u2192 {(x 1 /1, x 2 /0. 7, x 3 /0. 7, x 4 /1), (y 1 /0. 3, y 2medium /0, y 3 /0. 9)}", "formula_coordinates": [207.0, 68.36, 82.78, 292.86, 54.76]}, {"formula_id": "formula_112", "formula_text": "R(x i , y j ) = R(x i , y j ) if y j \u2208 Y R(x i , y j ) t in", "formula_coordinates": [209.0, 131.12, 65.26, 146.79, 24.39]}, {"formula_id": "formula_113", "formula_text": "[1, 1], [0.7, 0.9], [0.4, 0.6], [0.1, 0.3] and [0, 0] respectively.", "formula_coordinates": [209.0, 98.96, 248.66, 243.37, 8.78]}, {"formula_id": "formula_114", "formula_text": "supp(y 1low \u21d2 y 2low ) = 2 3 = 0.67 , conf(y 1low \u21d2 y 2low ) = 2 2. 2 = 0.91 .", "formula_coordinates": [209.0, 137.0, 488.42, 155.41, 48.38]}, {"formula_id": "formula_115", "formula_text": "R(x 2 , y 2low ) \u2265 R(x 2 , y 1low ) = 1 .", "formula_coordinates": [210.0, 142.76, 61.54, 143.89, 10.0]}, {"formula_id": "formula_116", "formula_text": "R(x 2 , y 2 ) low = R(x 2 , y 2low ) = 1 .", "formula_coordinates": [210.0, 142.76, 96.74, 143.89, 9.6]}, {"formula_id": "formula_117", "formula_text": "T = 2n \u2212 N lnodes i=1 w i d i 2n \u2212 1 ,(1)", "formula_coordinates": [215.0, 159.92, 259.8, 227.8, 25.92]}, {"formula_id": "formula_118", "formula_text": "w i = Ni N for a resolved leaf node 2Ni N otherwise .", "formula_coordinates": [215.0, 137.95, 302.4, 150.95, 26.04]}, {"formula_id": "formula_119", "formula_text": "IQN T (\u03a9) = (1 \u2212 \u03d5(\u03a9))f (depth T (\u03a9)) , (2", "formula_coordinates": [215.0, 129.07, 376.54, 254.38, 9.99]}, {"formula_id": "formula_120", "formula_text": ")", "formula_coordinates": [215.0, 383.46, 376.94, 4.25, 8.78]}, {"formula_id": "formula_121", "formula_text": "IC = (1 \u2212 d(T, IQN T )) , (3", "formula_coordinates": [215.0, 159.92, 514.42, 223.54, 9.99]}, {"formula_id": "formula_122", "formula_text": ")", "formula_coordinates": [215.0, 383.46, 514.82, 4.25, 8.78]}, {"formula_id": "formula_123", "formula_text": "SC(c) = c i=1 N j=1 (\u03bc ij ) m x j \u2212 v i 2 N i c k=1 v k \u2212 v i 2 .", "formula_coordinates": [216.0, 133.04, 56.76, 163.33, 31.2]}, {"formula_id": "formula_124", "formula_text": "S(c) = c i=1 N j=1 (\u03bc ij ) 2 x j \u2212 v i 2 N min i,k v k \u2212 v i 2 . (5", "formula_coordinates": [216.0, 133.75, 139.32, 249.7, 27.72]}, {"formula_id": "formula_125", "formula_text": ")", "formula_coordinates": [216.0, 383.46, 150.62, 4.25, 8.78]}, {"formula_id": "formula_126", "formula_text": "XB(c) = c i=1 N j=1 (\u03bc ij ) m x j \u2212 v i 2 N min i,j x j \u2212 v i 2 . (6", "formula_coordinates": [216.0, 127.03, 227.76, 256.42, 27.6]}, {"formula_id": "formula_127", "formula_text": ")", "formula_coordinates": [216.0, 383.46, 238.94, 4.25, 8.78]}, {"formula_id": "formula_128", "formula_text": "y(k + 1) = f (y(k), ..., y(k \u2212 p), u(k), ..., u(k \u2212 q)) (1)", "formula_coordinates": [232.0, 109.51, 234.7, 278.2, 9.62]}, {"formula_id": "formula_129", "formula_text": "u(k) = F (x(k)) (2)", "formula_coordinates": [232.0, 180.92, 360.13, 206.79, 8.78]}, {"formula_id": "formula_130", "formula_text": "IF x 1 is X i1 1 AND x 2 is X i2 2 AND...x N is X iN N THEN u = R i1i2...iN (3)", "formula_coordinates": [232.0, 71.11, 435.0, 316.6, 13.44]}, {"formula_id": "formula_131", "formula_text": "X iv v \u2208 {X 1 v , X 2 v , ..., X nv v } are the membership functions of input X v , n v", "formula_coordinates": [232.0, 71.24, 456.48, 315.62, 12.12]}, {"formula_id": "formula_132", "formula_text": "u(k) =F (x(k); \u0398) = n1 i1=1 n2 i2=1 ... nN iN =1 R i1i2...iN \u2022 N m=1 \u03bc X im m (x m (k)) n1 i1=1 n2 i2=1 ... nN iN =1 N m=1 \u03bc X im m (x m (k))(4)", "formula_coordinates": [232.0, 72.31, 537.12, 315.4, 65.17]}, {"formula_id": "formula_133", "formula_text": "\u0394R i (k) = C \u2022 \u03bc i (k \u2212 1) \u2022 e y (k) = C \u2022 \u03bc i (k \u2212 1) \u2022 (r(k \u2212 1) \u2212 y(k)) (5)", "formula_coordinates": [234.0, 75.2, 55.9, 312.52, 9.99]}, {"formula_id": "formula_134", "formula_text": "RI v = K i=1 (u M (x M i ) \u2212 F v \u221e (x M i )) 2 (6", "formula_coordinates": [235.0, 141.68, 311.4, 241.78, 30.36]}, {"formula_id": "formula_135", "formula_text": ")", "formula_coordinates": [235.0, 383.46, 321.86, 4.25, 8.78]}, {"formula_id": "formula_136", "formula_text": "x v , N i=1 i =v n i new fuzzy rules are created.", "formula_coordinates": [235.0, 41.84, 481.2, 169.69, 36.13]}, {"formula_id": "formula_137", "formula_text": "dV ol dt = A dH dt = bV \u2212 a \u221a H (8)", "formula_coordinates": [236.0, 155.23, 347.02, 232.48, 24.89]}, {"formula_id": "formula_138", "formula_text": "Rule f : (1)", "formula_coordinates": [242.0, 48.07, 73.95, 339.59, 8.56]}, {"formula_id": "formula_139", "formula_text": "u k\u2212m+1 is C f m then y f k+1 = b 1,f \u2022 y k + . . . + b n,f \u2022 y k\u2212n+1 + c 1,f \u2022 u k + . . . + c m,f \u2022 u k\u2212m+1 ,", "formula_coordinates": [242.0, 47.36, 88.52, 334.38, 27.33]}, {"formula_id": "formula_140", "formula_text": "B f 1 , . . . , B f n , C f 1 , . . . , C f m", "formula_coordinates": [242.0, 283.64, 150.48, 103.43, 13.57]}, {"formula_id": "formula_141", "formula_text": "y k+1 = n j=1 b j k \u2022 y k\u2212j+1 + m j=1 c j k \u2022 u k\u2212j+1 , (2", "formula_coordinates": [242.0, 125.59, 196.56, 257.87, 30.36]}, {"formula_id": "formula_142", "formula_text": ")", "formula_coordinates": [242.0, 383.46, 207.02, 4.25, 8.78]}, {"formula_id": "formula_143", "formula_text": "where b j k = l f =1 w f k \u2022 b j,f , c j k = l f =1 w f k \u2022 c j,f", "formula_coordinates": [242.0, 41.84, 235.32, 217.72, 14.17]}, {"formula_id": "formula_144", "formula_text": "y k+i = i\u22121 j=1 b j k+i \u2022 y k\u2212j+i + n j=i b j k+i \u2022y k\u2212j+i + i j=1 c j k+i \u2022u k + m j=i+1 c j k+i \u2022u k\u2212j+i , (3)", "formula_coordinates": [242.0, 46.75, 342.84, 340.96, 30.36]}, {"formula_id": "formula_145", "formula_text": "d k = y k \u2212 y k . (4", "formula_coordinates": [242.0, 183.31, 468.7, 200.15, 9.99]}, {"formula_id": "formula_146", "formula_text": ")", "formula_coordinates": [242.0, 383.46, 469.1, 4.25, 8.78]}, {"formula_id": "formula_147", "formula_text": "y k+i|k = i j=1 c j k+i \u2022u k + m j=i+1 c j k+i \u2022u k\u2212j+i + i\u22121 j=1 b j k+i \u2022 y k\u2212j+i + n j=i b j k+i \u2022y k\u2212j+i +d k .", "formula_coordinates": [242.0, 41.84, 524.04, 345.85, 30.36]}, {"formula_id": "formula_148", "formula_text": "y k+i|k = C k+i \u2022 u k + D k+i , (6", "formula_coordinates": [242.0, 155.0, 590.86, 228.46, 10.35]}, {"formula_id": "formula_149", "formula_text": ")", "formula_coordinates": [242.0, 383.46, 591.26, 4.25, 8.78]}, {"formula_id": "formula_150", "formula_text": "D k+i = m j=i+1 c j k+i \u2022u k\u2212j+i + i\u22121 j=1 b j k+i \u2022 y k\u2212j+i + n j=i b j k+i \u2022y k\u2212j+i +d k , C k+i = i j=1 c j", "formula_coordinates": [243.0, 41.84, 51.12, 345.85, 29.04]}, {"formula_id": "formula_151", "formula_text": "y min \u2264 y k+i|k \u2264 y max , (7", "formula_coordinates": [243.0, 165.56, 116.5, 217.9, 10.35]}, {"formula_id": "formula_152", "formula_text": ")", "formula_coordinates": [243.0, 383.46, 116.9, 4.25, 8.78]}, {"formula_id": "formula_153", "formula_text": "y min \u2264 C k+i \u2022 u k + D k+i \u2264 y max . (8", "formula_coordinates": [243.0, 141.67, 170.02, 241.78, 10.0]}, {"formula_id": "formula_154", "formula_text": ")", "formula_coordinates": [243.0, 383.46, 170.42, 4.25, 8.78]}, {"formula_id": "formula_155", "formula_text": "C k+i \u2022 u k \u2265 y min \u2212 D k+i (9)", "formula_coordinates": [243.0, 159.79, 221.14, 227.92, 10.0]}, {"formula_id": "formula_156", "formula_text": "C k+i \u2022 u k \u2264 y max \u2212 D k+i . (10", "formula_coordinates": [243.0, 157.64, 264.22, 225.63, 10.0]}, {"formula_id": "formula_157", "formula_text": ")", "formula_coordinates": [243.0, 383.26, 264.62, 4.45, 8.78]}, {"formula_id": "formula_158", "formula_text": "-if C k+i \u2022 u k \u2265 y min \u2212 D k+i then u k = ymin\u2212D k+i C k+i and -for upper constraints: -if C k+i \u2022 u k \u2264 y max \u2212 D k+i then u k = ymax\u2212D k+i C k+i . Remark 1.", "formula_coordinates": [243.0, 48.08, 314.64, 250.34, 58.22]}, {"formula_id": "formula_159", "formula_text": "y k+i|k = y k+i|k + r k+i|k = C k+i \u2022 u k + D k+i + r k+i|k , (11", "formula_coordinates": [244.0, 98.36, 98.14, 284.91, 10.35]}, {"formula_id": "formula_160", "formula_text": ")", "formula_coordinates": [244.0, 383.26, 98.54, 4.45, 8.78]}, {"formula_id": "formula_161", "formula_text": "r min k+i|k \u2264 r k+i|k \u2264 r max k+i|k , (12", "formula_coordinates": [244.0, 159.92, 162.96, 223.35, 12.96]}, {"formula_id": "formula_162", "formula_text": ")", "formula_coordinates": [244.0, 383.26, 165.02, 4.45, 8.78]}, {"formula_id": "formula_163", "formula_text": "if C k+i \u2022 u k \u2265 y min \u2212 D k+i \u2212 r min k+i|k then u k = ymin\u2212D k+i \u2212r min k+i|k C k+i and -for upper constraints: -if C k+i \u2022 u k \u2264 y max \u2212 D k+i \u2212 r max k+i|k then u k = ymax\u2212D k+i \u2212r max k+i|k C k+i .", "formula_coordinates": [244.0, 48.08, 225.01, 315.74, 46.14]}, {"formula_id": "formula_164", "formula_text": "f : if u k\u22122 is M f , then y f k+1 = b f \u2022 y k + c f \u2022 u k\u22122 + d f , (13", "formula_coordinates": [244.0, 76.52, 500.66, 306.75, 33.35]}, {"formula_id": "formula_165", "formula_text": ")", "formula_coordinates": [244.0, 383.26, 522.74, 4.45, 8.78]}, {"formula_id": "formula_166", "formula_text": "f = 1,", "formula_coordinates": [244.0, 72.32, 543.37, 32.89, 8.78]}, {"formula_id": "formula_167", "formula_text": "Maximize \u03b4 = 1 \u2212 \u03b4 * .", "formula_coordinates": [249.0, 169.52, 535.55, 90.4, 11.05]}, {"formula_id": "formula_169", "formula_text": "\u03c1j = min{Aj , A j } max{Aj, A j } . (4", "formula_coordinates": [250.0, 172.88, 268.47, 210.87, 22.82]}, {"formula_id": "formula_170", "formula_text": ")", "formula_coordinates": [250.0, 383.75, 274.83, 3.92, 8.56]}, {"formula_id": "formula_171", "formula_text": "Maximize GM 3M = 3 \u03b4 \u2022 \u03b3 \u2022 \u03c1 (6)", "formula_coordinates": [250.0, 147.91, 439.93, 239.8, 10.55]}, {"formula_id": "formula_172", "formula_text": "C p = C p S C p T .", "formula_coordinates": [251.0, 41.84, 181.92, 57.97, 13.92]}, {"formula_id": "formula_173", "formula_text": "C p T = C 1 C 2 . . . C n ; C i = (a i 1 , b i 1 , c i 1 , . . . , a i m i , b i m i , c i m i ), i = 1, . . . , n .", "formula_coordinates": [251.0, 66.92, 247.92, 295.69, 14.16]}, {"formula_id": "formula_174", "formula_text": "MSE = 1 2\u2022|E| |E| l=1 (F (x l ) \u2212 y l ) 2 ,", "formula_coordinates": [251.0, 41.83, 413.6, 147.01, 15.29]}, {"formula_id": "formula_176", "formula_text": "y t = G(x t ; \u03c8) + \u03b5 t , (1", "formula_coordinates": [259.0, 173.59, 110.78, 209.87, 9.59]}, {"formula_id": "formula_177", "formula_text": ")", "formula_coordinates": [259.0, 383.46, 110.78, 4.25, 8.78]}, {"formula_id": "formula_178", "formula_text": "If y t\u22121 is A 1 and y t\u22122 is A 2 and . . . and y t\u2212p is A p THEN y t = b 0 + b 1 y t\u22121 + b 2 y t\u22122 + . . . + b p y t\u2212p . (2)", "formula_coordinates": [260.0, 51.8, 400.82, 335.91, 24.48]}, {"formula_id": "formula_179", "formula_text": "\u03c9 i (x) = d j=1 \u03bc A i j (x j ),(3)", "formula_coordinates": [260.0, 170.35, 488.52, 217.36, 30.24]}, {"formula_id": "formula_180", "formula_text": "y t = G (x t ; \u03c8) + \u03b5 t = R i=1 \u03c9 i (x t ) \u2022 b i x t + \u03b5 t , (4", "formula_coordinates": [261.0, 121.28, 62.88, 262.18, 30.36]}, {"formula_id": "formula_181", "formula_text": ")", "formula_coordinates": [261.0, 383.46, 73.34, 4.25, 8.78]}, {"formula_id": "formula_182", "formula_text": "\u03bc S (x t ; \u03c8)) = 1 1 + exp (\u2212\u03b3(\u03c9x t \u2212 c)) , (5", "formula_coordinates": [261.0, 135.19, 315.74, 248.27, 23.15]}, {"formula_id": "formula_183", "formula_text": ")", "formula_coordinates": [261.0, 383.46, 322.46, 4.25, 8.78]}, {"formula_id": "formula_184", "formula_text": "\u03bc G (x t ; \u03c8)) = i exp \u2212 (x i \u2212 c i ) 2 2\u03c3 2(6)", "formula_coordinates": [261.0, 136.99, 384.36, 250.72, 28.2]}, {"formula_id": "formula_185", "formula_text": "\u03bc G (x t ; \u03c8)) = i exp \u2212\u03b3(x i \u2212 c i ) 2 , (7", "formula_coordinates": [261.0, 135.92, 434.04, 247.54, 21.96]}, {"formula_id": "formula_186", "formula_text": ")", "formula_coordinates": [261.0, 383.46, 436.1, 4.25, 8.78]}, {"formula_id": "formula_187", "formula_text": "\u03c3 2 t = \u03c3 2 + r i=1 \u03c3 2 i \u03bc \u03c3,i x t ; \u03c8 \u03bc\u03c3,i(8)", "formula_coordinates": [262.0, 146.59, 84.48, 241.12, 30.24]}, {"formula_id": "formula_188", "formula_text": "\u03c3 2 t = exp G \u03c3 x t ; \u03c8 \u03c3 , \u03c8 \u03bc\u03c3,i = exp \u03c2 + r i=1 \u03c2 i \u03bc \u03c3,i x t ; \u03c8 \u03bc\u03c3,i , (9", "formula_coordinates": [262.0, 74.48, 192.72, 308.98, 30.36]}, {"formula_id": "formula_189", "formula_text": ")", "formula_coordinates": [262.0, 383.46, 203.18, 4.25, 8.78]}, {"formula_id": "formula_190", "formula_text": "\u03c3 2 t = exp (\u03c2 + \u03c2 1 \u03bc \u03c3 (x t ; \u03c8 \u03bc\u03c3 )) , (10", "formula_coordinates": [262.0, 149.71, 277.08, 233.55, 12.72]}, {"formula_id": "formula_191", "formula_text": ")", "formula_coordinates": [262.0, 383.26, 279.26, 4.45, 8.78]}, {"formula_id": "formula_192", "formula_text": "\u03c3 2 t = exp \u03c1 + q i=1 \u03c1 i x i,t ,(11)", "formula_coordinates": [262.0, 156.19, 391.8, 231.52, 30.84]}, {"formula_id": "formula_193", "formula_text": "\u03c1 1 = \u03c1 2 = ... = \u03c1 q = 0. Under H 0 , exp(\u03c1) = \u03c3 2 .", "formula_coordinates": [262.0, 41.84, 429.62, 345.79, 20.78]}, {"formula_id": "formula_194", "formula_text": "l t = \u2212 1 2 ln(2\u03c0) \u2212 1 2 \u03c1 + q i=1 \u03c1 i x i,t \u2212 \u03b5 2 t 2 exp(\u03c1 + q i=1 \u03c1 i x i,t ) . (12", "formula_coordinates": [262.0, 79.99, 483.96, 303.27, 30.84]}, {"formula_id": "formula_195", "formula_text": ")", "formula_coordinates": [262.0, 383.26, 494.9, 4.45, 8.78]}, {"formula_id": "formula_196", "formula_text": "\u2202l t \u2202\u03c1 = \u2212 1 2 + \u03b5 2 t 2 exp(\u03c1 + q i=1 \u03c1 i x i,t ) , (13", "formula_coordinates": [262.0, 137.96, 544.44, 245.31, 26.4]}, {"formula_id": "formula_197", "formula_text": ")", "formula_coordinates": [262.0, 383.26, 552.86, 4.45, 8.78]}, {"formula_id": "formula_198", "formula_text": "\u2202l t \u2202\u03c1 i = \u2212 x i 2 + \u03b5 2 t x i 2 exp(\u03c1 + q i=1 \u03c1 i x i,t ) , (14", "formula_coordinates": [262.0, 134.96, 577.92, 248.31, 26.4]}, {"formula_id": "formula_199", "formula_text": ")", "formula_coordinates": [262.0, 383.26, 586.22, 4.45, 8.78]}, {"formula_id": "formula_200", "formula_text": "\u2202l t \u2202\u03c1 H0 = 1 2 \u03b5 2 t \u03c3 2 \u2212 1 , (15", "formula_coordinates": [263.0, 166.64, 75.0, 216.63, 29.41]}, {"formula_id": "formula_201", "formula_text": ")", "formula_coordinates": [263.0, 383.26, 83.3, 4.45, 8.78]}, {"formula_id": "formula_202", "formula_text": "\u2202l t \u2202\u03c1 i H0 = x i,t 2 \u03b5 2 t \u03c3 2 \u2212 1 , (16", "formula_coordinates": [263.0, 161.0, 117.24, 222.27, 29.53]}, {"formula_id": "formula_203", "formula_text": ")", "formula_coordinates": [263.0, 383.26, 125.66, 4.45, 8.78]}, {"formula_id": "formula_204", "formula_text": "where\u03c3 2 = 1/T T t=1\u03b5 2 t .", "formula_coordinates": [263.0, 41.84, 152.64, 109.09, 14.16]}, {"formula_id": "formula_205", "formula_text": "LM = 1 2 T t=! \u03b5 2 t \u03c3 2 \u2212 1 x t T t=1x tx t \u22121 T t=! \u03b5 2 t \u03c3 2 \u2212 1 x t (17", "formula_coordinates": [263.0, 63.91, 174.56, 319.35, 34.37]}, {"formula_id": "formula_206", "formula_text": ")", "formula_coordinates": [263.0, 383.26, 188.78, 4.45, 8.78]}, {"formula_id": "formula_207", "formula_text": "wherex t = [1, x t ] .", "formula_coordinates": [263.0, 41.84, 219.14, 81.73, 9.59]}, {"formula_id": "formula_208", "formula_text": "compute the SSR 0 = 1 T T t=1 \u03b5 2 t \u03c3 2 \u03b5 t \u2212 1 2", "formula_coordinates": [263.0, 58.76, 273.0, 186.37, 22.27]}, {"formula_id": "formula_209", "formula_text": "variance of\u03b5 t . 2. Regress \u03b5 2 t \u03c3 2 \u03b5 t \u2212 1 onx t and compute the residual sum of squares SSR 1 = 1 T T t=1\u03bd 2 t . 3. Compute the \u03c7 2 statistic LM \u03c3 \u03c7 2 = T SSR 0 \u2212 SSR 1 SSR 0 or the F version of the test LM \u03c3 F = (SSR 0 \u2212 SSR 1 ) s SSR 1 (T \u2212 s \u2212 n) \u22121 .", "formula_coordinates": [263.0, 46.03, 295.34, 341.59, 136.54]}, {"formula_id": "formula_210", "formula_text": "Minimize \u2208 \u2208 = \u2211 \u2211 \u2211 1 k ij ijk i V j V k c x (1)", "formula_coordinates": [268.0, 139.38, 551.61, 248.6, 34.66]}, {"formula_id": "formula_211", "formula_text": "\u2208 = \u2264 \u2211 \u2211 1 E k jk j V k y K (2) ik V j jik V j ijk y x x = = \u2211 \u2211 \u2208 \u2208 , \u2200i\u2208V, k = 1, 2, ..., K (3", "formula_coordinates": [269.0, 115.5, 59.24, 272.48, 71.95]}, {"formula_id": "formula_212", "formula_text": ")", "formula_coordinates": [269.0, 384.07, 114.84, 3.91, 8.96]}, {"formula_id": "formula_213", "formula_text": "hk V V i ik S l hl y y w E \u2265 = \u2211 \u2211 \u2208 \u2208 \\ , h\u2208V E , k = 1, 2, ..., K(4)", "formula_coordinates": [269.0, 114.78, 139.19, 273.2, 31.16]}, {"formula_id": "formula_214", "formula_text": "\u2211 = \u2264 K k ik y 1 1, \u2200i\u2208V\\V E (5", "formula_coordinates": [269.0, 114.78, 182.66, 269.29, 32.53]}, {"formula_id": "formula_215", "formula_text": ")", "formula_coordinates": [269.0, 384.07, 193.8, 3.91, 8.96]}, {"formula_id": "formula_216", "formula_text": "k V i S l ilk C z \u2264 \u2211\u2211 \u2208 \u2208 , k = 1, 2,..., K (6) ik ilk y z \u2264 , \u2200i, l, k (7) 1 1 = \u2211\u2211 \u2208 = V i K k ilk z , \u2200l\u2208S(8)", "formula_coordinates": [269.0, 114.78, 221.18, 273.2, 100.08]}, {"formula_id": "formula_217", "formula_text": "y ik \u2208{0,1}, \u2200i\u2208V, k = 1, 2, ..., K(9)", "formula_coordinates": [269.0, 113.1, 329.14, 274.88, 12.98]}, {"formula_id": "formula_218", "formula_text": "w hl \u2208{0,1}, \u2200h\u2208V E , \u2200l\u2208S(12)", "formula_coordinates": [269.0, 113.1, 386.02, 274.88, 12.98]}, {"formula_id": "formula_219", "formula_text": "V i (t + 1) = \u03c9 \u2022 V i (t) + C 1 \u2022 \u03d5 1 \u2022 (P li \u2212 X i (t)) + C 2 \u2022 \u03d5 2 \u2022 (P g \u2212 X i (t)) (1) X i (t + 1) = X i (t) + V i (t + 1)(2)", "formula_coordinates": [277.0, 67.28, 380.5, 320.44, 24.99]}, {"formula_id": "formula_220", "formula_text": "m \u2190 (1 \u2212 \u03b7)m + \u03b7(C 1 \u2022 \u03d5 1 \u2022 (w t * \u2212 m) + C 2 \u2022 \u03d5 2 \u2022 (w * \u2212 m)) (3", "formula_coordinates": [278.0, 83.23, 373.64, 300.23, 12.17]}, {"formula_id": "formula_221", "formula_text": ")", "formula_coordinates": [278.0, 383.46, 376.22, 4.25, 8.78]}, {"formula_id": "formula_222", "formula_text": "\u03b2 2 \u2190 (1 \u2212 \u03b7)\u03b2 2 + \u03b7 \u239b \u239d t i=1 (w i * \u2212\u0175 T * ) 2 t \u239e \u23a0 (4", "formula_coordinates": [278.0, 82.16, 384.12, 301.3, 36.38]}, {"formula_id": "formula_223", "formula_text": ")", "formula_coordinates": [278.0, 383.46, 404.9, 4.25, 8.78]}, {"formula_id": "formula_224", "formula_text": "Q(s t , a t ) \u2190 Q(s t , a t ) + \u03b1[r t+1 + \u03b3 max a Q(s t+1 , a) \u2212 Q(s t , a t )].", "formula_coordinates": [281.0, 81.2, 74.5, 267.01, 14.43]}, {"formula_id": "formula_225", "formula_text": "\u0394Q t * i \u2190 \u03b1(r t * + \u03b3Q * \u2212 Q i ) ( 5 ) Q i \u2190 Q i + \u0394Q t * i (6)", "formula_coordinates": [281.0, 154.99, 358.52, 232.72, 28.25]}, {"formula_id": "formula_226", "formula_text": "m = (1 \u2212 \u03b7)m + \u03b7 \u2022 \u0394Q i \u2022 (C 1 \u2022 \u03d5 1 \u2022 (w t * \u2212 m) + C 2 \u2022 \u03d5 2 \u2022 (w * \u2212 m))(7)", "formula_coordinates": [281.0, 60.43, 415.04, 327.28, 12.17]}, {"formula_id": "formula_227", "formula_text": "m k = M j=1 w j \u03c6 j (x k ) = M j=1 w j exp(\u2212\u03b2||\u03bc j \u2212 x k || 2 ), \u2200k \u2208 {1, . . . , K}. (8)", "formula_coordinates": [282.0, 69.19, 537.24, 318.51, 30.24]}, {"formula_id": "formula_228", "formula_text": "w \u2190 w + \u03b7[C 1 \u2022 \u03d5 1 \u2022 (y k * ,t * \u2212 m k * )\u03c6(x k * ) + C 2 \u2022 \u03d5 2 \u2022 (w * \u2212 w)] (9)", "formula_coordinates": [283.0, 86.0, 188.84, 301.72, 12.17]}, {"formula_id": "formula_229", "formula_text": "w * \u2208 arg min w\u2208W K k=1 N n=1 m k \u2212 t n 2(10)", "formula_coordinates": [283.0, 136.76, 277.44, 250.96, 30.48]}, {"formula_id": "formula_230", "formula_text": "( ) { [ ] [] ( ) [ ] [] ( ){", "formula_coordinates": [290.0, 86.82, 132.16, 251.74, 49.91]}, {"formula_id": "formula_231", "formula_text": "= \u2212 + \u2212 = = \u23aa \u23a9 \u23aa \u23a8 \u23a7 = + + = \u2211 \u2211 \u2211 = = \u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b + \u239f \u23a0 \u239e \u239c \u239d \u239b = \u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b \u2212 + \u239f \u23a0 \u239e \u239c \u239d \u239b = =", "formula_coordinates": [290.0, 61.38, 60.49, 273.11, 119.88]}, {"formula_id": "formula_232", "formula_text": "N(c1)*N(c2) or N(c1)*N(c2) + N(c1)+N(c2).", "formula_coordinates": [290.0, 91.86, 297.84, 224.49, 8.96]}, {"formula_id": "formula_233", "formula_text": "( 1 \u2212 = = \u2211 + =", "formula_coordinates": [291.0, 89.94, 189.26, 87.15, 42.93]}, {"formula_id": "formula_234", "formula_text": "nbss p nb ELSE nbbs p nb THEN nbs i IF i i i i * * = = <=", "formula_coordinates": [291.0, 79.26, 291.47, 75.62, 54.6]}, {"formula_id": "formula_235", "formula_text": "Users 1 0 ( ) v v v v $ $ $ $ $ $ X E z \u00cb\u00a8\u00a8\u00a8B E E z", "formula_coordinates": [297.0, 121.48, 66.76, 90.96, 83.15]}, {"formula_id": "formula_236", "formula_text": "\u2200(i, j), i \u2208 \u03a8 , j \u2208 \u0398: c ij = (r ij , \u03b2 ij , f ij , p ij , d ij ).", "formula_coordinates": [297.0, 171.08, 590.86, 214.21, 9.99]}, {"formula_id": "formula_237", "formula_text": "\u03b2 ij = \u23a7 \u23a8 \u23a9 0, (l ij > L max /2)&(e ij = 1), 5, (l ij < L max /2)&(e ij = 1) or (l ij \u2208 (L max /2, L max ])&(e ij = 0), 10, (l ij < L max /2)&(e ij = 0). Parameter of \"connectivity\" by \u03b2 ij is: \u03be \u03b2 ij = 0, if \u03b2 ij = 0, 1, otherwise. (5) QoS (priority): p ij = p i . (6) Required/possible bandwidth: f ij (at initial stage f ij = f i ). Three cases are examined: (a) p ij = 1: f ij = f i , (b) p ij = 2: f ij = f i , if (max j f j \u2212 1 m m j=1 f j ) \u2265 f i , max j f j \u2212 1 m m j=1 f j , if (max j f j \u2212 1 m m j=1 f j ) < f i , (c) p ij = 3: f ij = f i , if (max j f j \u2212 1 m m j=1 f j ) \u2265 f i , max j f j \u2212 1 m m j=1 f j , if (max j f j \u2212 1 m m j=1 f j ) < f i .", "formula_coordinates": [298.0, 41.84, 457.56, 345.89, 145.81]}, {"formula_id": "formula_238", "formula_text": "(a) p ij = 1: d ij = d j , if d j \u2265 d i , 0, if d j < d i ; (b) p ij = 2: d ij = d j , if d j \u2265 d i /2, 0, if d j < d i /2; (c) p ij = 3: d ij = d j .", "formula_coordinates": [299.0, 53.12, 143.98, 163.81, 58.95]}, {"formula_id": "formula_239", "formula_text": "\u03be d ij = 0, if d ij = 0, 1, otherwise. In addition, \u2200i \u2208 \u03a8 is defined \u0398 i \u2286 \u0398. Clearly, if |\u0398 i | = 0", "formula_coordinates": [299.0, 53.12, 208.34, 268.87, 33.59]}, {"formula_id": "formula_240", "formula_text": "{j, x j , y j , z j , r j , f j , p j , d j , } c {i, x i , y i , z i , r i , f i , p i , d i , } c { l ij , r ij , e ij , \u03b2 ij , f ij , p ij , d ij , \u03be \u03b2 ij , \u03be d ij } c c ij = (r ij , \u03b2 ij , f ij , p ij , d ij ) c c ij c \u0398 i", "formula_coordinates": [299.0, 81.07, 280.7, 258.19, 104.14]}, {"formula_id": "formula_241", "formula_text": "(i) total reliability R(X) = n i=1 m j=1 r ij x ij , (ii) total parameter of quality for signal propagation B(X) = n i=1 m j=1 \u03b2 ij x ij , (iii) generalized quality of usage of frequency spectrum F (X) = n i=1 m j=1 f ij x ij , (iv) generalized parameter of QoS P (X) = n i=1 m j=1 p ij x ij , (v) gen- eralized parameter of protection for information transmission D(X) = n i=1 m j=1 d ij x ij .", "formula_coordinates": [299.0, 41.84, 437.16, 345.92, 78.0]}, {"formula_id": "formula_242", "formula_text": "C(X) = (R(X), B(X), F (X), P (X), D(X)).", "formula_coordinates": [299.0, 53.12, 537.14, 189.97, 8.78]}, {"formula_id": "formula_243", "formula_text": "n i=1 f ij x ij \u2264 f j \u2200j \u2208 \u0398,", "formula_coordinates": [299.0, 223.88, 576.24, 108.85, 14.16]}, {"formula_id": "formula_244", "formula_text": "j\u2208\u0398i x ij \u2264 k i \u2200i \u2208 \u03a8 .", "formula_coordinates": [300.0, 237.8, 101.5, 96.01, 11.55]}, {"formula_id": "formula_245", "formula_text": "max R(X) = n i=1 j\u2208\u0398i r ij x ij , max B(X) = n i=1 j\u2208\u0398i \u03b2 ij x ij , max F (X) = n i=1 j\u2208\u0398i f ij x ij , max P (X) = n i=1 j\u2208\u0398i p ij x ij , max D(X) = n i=1 j\u2208\u0398i d ij x ij s.t. n i=1 f ij x ij \u2264 f j \u2200j \u2208 \u0398, n i=1 x ij \u2264 n j \u2200j \u2208 \u0398, j\u2208\u0398i x ij \u2264 1 \u2200i \u2208 \u03a8, x ij = 0 \u222a 1, \u2200 i = 1, n, \u2200 j = 1, m, x ij = 0, \u2200 i = 1, n, j \u2208 {\u0398 \\ \u0398 i }.", "formula_coordinates": [300.0, 53.12, 203.04, 334.57, 66.36]}, {"formula_id": "formula_246", "formula_text": "j\u2208\u0398i x ij \u2264 k i \u2200i \u2208 \u03a8 .", "formula_coordinates": [300.0, 283.88, 283.42, 96.01, 11.43]}, {"formula_id": "formula_247", "formula_text": "Vectors (\u03be \u03b2 ij , \u03be d ij ) (Table 4) define sets {\u0398 i } (i = 1, n): \u0398 1 = {1, 3, 5}, \u0398 2 = {1, 2, 3, 5}, \u0398 3 = {1, 2, 3, 4, 5}, \u0398 4 = {1, 2, 3, 4}, \u0398 5 = {2, 4}, \u0398 6 = {2, 4, 6}, \u0398 7 = {1, 3, 4, 5}, \u0398 8 = {1, 2, 3, 4, 5, 6}, \u0398 9 = {1, 2, 3, 4, 5, 6}, \u0398 10 = {2, 3, 4, 5, 6}, \u0398 11 = {2, 3, 4, 5, 6}, \u0398 12 = {1, 3, 4, 5}, \u0398 13 = {1, 3, 4, 5}, \u0398 14 = {1, 3, 4, 5, 6}, \u0398 15 = {2, 3, 4, 6}, \u0398 16 = {1, 3, 4, 5}, \u0398 17 = {1, 5}, \u0398 18 = {3, 4, 5, 6}, \u0398 19 = {2, 3, 4, 6}, \u0398 20 = {1, 3, 4, 5, 6}, \u0398 21 = {1,", "formula_coordinates": [301.0, 41.83, 241.34, 346.34, 81.36]}, {"formula_id": "formula_248", "formula_text": "\u00a4 \u00a5 u 17 \u00a7 \u00a6 \u00a4 \u00a5 u 13 \u00a1 \u00a1 e e r 3 f i r r \u00a1 \u00a1 \u00a1 e e r r \u00a7 \u00a6 \u00a4 \u00a5 u 12 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a7 \u00a6 \u00a4 \u00a5 u 1 \u00a7 \u00a6 \u00a4 \u00a5 u 2 \u00a1 \u00a1 e e r 1 f i d d d d \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 7 \u00a7 \u00a6 \u00a4 \u00a5 u 16 \u00a7 \u00a6 \u00a4 \u00a5 u 20 \u00a7 \u00a6 \u00a4 \u00a5 u 21 \u00a7 \u00a6 \u00a4 \u00a5 u 14 \u00a7 \u00a6 \u00a4 \u00a5 u 9 \u00a7 \u00a6 \u00a4 \u00a5 u 10 \u00a1 \u00a1 e e r 4 f i \u00a1 \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 11 \u00a7 \u00a6 \u00a4 \u00a5 u 15 \u00a7 \u00a6 \u00a4 \u00a5 u 19 \u00a7 \u00a6 \u00a4 \u00a5 u 18 \u00a7 \u00a6 \u00a4 \u00a5 u 22 \u00a1 \u00a1 e e r 6 f i d d \u00a1 \u00a1 \u00a1 \u00a1 \u00a1 r r r \u00a7 \u00a6 \u00a4 \u00a5 u 8 \u00a7 \u00a6 \u00a4 \u00a5 u 3 \u00a7 \u00a6 \u00a4 \u00a5 u 4 \u00a7 \u00a6 \u00a4 \u00a5 u 5 \u00a7 \u00a6 \u00a4 \u00a5 u 6 \u00a1 \u00a1", "formula_coordinates": [301.0, 54.32, 328.92, 185.21, 219.03]}, {"formula_id": "formula_249", "formula_text": "max n i=1 j\u2208\u0398i c ij x ij s.t. n i=1 f ij x ij \u2264 f j \u2200j \u2208 \u0398, n i=1 x ij \u2264 n j \u2200j \u2208 \u0398, j\u2208\u0398i x ij \u2264 1 \u2200i \u2208 \u03a8, x ij = 0 \u222a 1, \u2200 i = 1, n, \u2200j = 1, m, x ij = 0, \u2200 i = 1, n, j \u2208 {\u0398 \\ \u0398 i }.", "formula_coordinates": [302.0, 53.12, 134.76, 334.57, 39.72]}, {"formula_id": "formula_250", "formula_text": "j\u2208\u0398i x ij \u2264 k i \u2200i \u2208 \u03a8 .", "formula_coordinates": [302.0, 246.44, 188.38, 95.29, 11.55]}, {"formula_id": "formula_251", "formula_text": "i x i y i z i f i k i p i r i d i", "formula_coordinates": [302.0, 51.84, 263.52, 163.86, 11.27]}, {"formula_id": "formula_252", "formula_text": "4. Matrix ||(\u03be \u03b2 ij , \u03be d ij )|| i Access points j 1 2 3 4 5 6 1 1, 1 0, 1 1, 1 0, 1 1, 1 0, 1 2 1, 1 1, 1 1, 1 0, 1 1, 1 0, 1 3 1, 1 1, 1 1, 1 1, 1 1, 1 0, 0 4 1, 1 1, 1 1, 1 1, 1 0, 1 0, 1 5 0, 1 1, 1 0, 1 1, 1 0, 1 0, 1 6 0, 1 1, 1 0, 1 1, 1 0, 1 1, 1 7 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 8 1, 1 1, 1 1, 1 1, 1 1, 1 1, 1 9 1, 1 1, 1 1, 1 1, 1 1, 1 1, 1 10 0, 1 1, 1 1, 1 1, 1 1, 1 1, 1 11 0, 1 1, 1 1, 1 1, 1 1, 1 1, 1 12 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 13 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 14 1, 1 0, 1 1, 1 1, 1 1, 1 1, 1 15 0, 1 1, 1 1, 1 1, 1 0, 1 1, 1 16 1, 1 0, 1 1, 1 1, 1 1, 1 0, 1 17 1, 1 0, 0 1, 0 1, 0 1, 1 0, 0 18 0, 1 0, 1 1, 1 1, 1 1, 1 1, 1 19 0, 1 1, 1 1, 1 1, 1 0, 1 1, 1 20 1, 1 0, 1 1, 1 1, 1 1, 1 1, 1 21 1, 1 0, 0 1, 1 1, 0 1, 1 1, 0 22 0, 1 0, 1 0, 1 1, 1 0, 1 1, 1", "formula_coordinates": [302.0, 245.19, 236.36, 130.66, 308.14]}, {"formula_id": "formula_253", "formula_text": "i Access points j 1 2 3 4 5 6 1 3 0 3 0 3 0 2 2 1 1 0 2 0 3 1 1 1 1 1 0 4 1 1 2 1 0 0 5 0 1 0 1 0 0 6 0 1 0 1 0 1 7 2 0 2 1 2 0 8 1 1 1 1 1 1 9 2 1 1 1 1 1 10 0 1 2 1 3", "formula_coordinates": [305.0, 57.2, 70.58, 109.86, 146.78]}, {"formula_id": "formula_254", "formula_text": "\u00a4 \u00a5 u 17 \u00a7 \u00a6 \u00a4 \u00a5 u 13 \u00a1 \u00a1 e e r 3 f i r r d d \u00a1 \u00a1 \u00a1 e e r r \u00a7 \u00a6 \u00a4 \u00a5 u 12 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a2 \u00a7 \u00a6 \u00a4 \u00a5 u 1 \u00a7 \u00a6 \u00a4 \u00a5 u 2 \u00a1 \u00a1 e e r 1 f i d d d d \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 7 \u00a7 \u00a6 \u00a4 \u00a5 u 16 \u00a7 \u00a6 \u00a4 \u00a5 u 20 \u00a7 \u00a6 \u00a4 \u00a5 u 21 \u00a7 \u00a6 \u00a4 \u00a5 u 14 \u00a7 \u00a6 \u00a4 \u00a5 u 9 \u00a7 \u00a6 \u00a4 \u00a5 u 10 \u00a1 \u00a1 e e r 4 f i \u00a8\u00a1 \u00a1 \u00a1 \u00a7 \u00a6 \u00a4 \u00a5 u 11 \u00a7 \u00a6 \u00a4 \u00a5 u 15 \u00a7 \u00a6 \u00a4 \u00a5 u 19 \u00a7 \u00a6 \u00a4 \u00a5 u 18 \u00a7 \u00a6 \u00a4 \u00a5 u 22 \u00a1 \u00a1 e e r 6 f i d d \u00a1 \u00a1 \u00a1 \u00a1 \u00a1 r r r \u00a7 \u00a6 \u00a4 \u00a5 u 8 \u00a7 \u00a6 \u00a4 \u00a5 u 3 \u00a7 \u00a6 \u00a4 \u00a5 u 4 \u00a7 \u00a6 \u00a4 \u00a5 u 5 \u00a7 \u00a6 \u00a4 \u00a5 u 6 \u00a1 \u00a1 e e r 2 f i r r e e r r \u00a1 \u00a1 e e r 5 f i t t t \u00a1 \u00a1 \u00a1 d d \u00a1 \u00a1 \u00a1 e e", "formula_coordinates": [305.0, 196.51, 125.88, 185.21, 219.03]}, {"formula_id": "formula_255", "formula_text": "\u03c6 X (\u03b8) = exp{\u2212\u03c3 \u03b1 |\u03b8| \u03b1 1 \u2212 i\u03b2(sign(\u03b8))tan \u03c0\u03b1 2 + i\u03bc\u03b8}, if\u03b1 = 1 exp{\u2212\u03c3|\u03b8| 1 + i\u03b2 2 \u03c0 (sign(\u03b8))ln|\u03b8| + i\u03bc\u03b8}, if\u03b1 = 1 (1)", "formula_coordinates": [309.0, 68.24, 72.12, 319.48, 25.56]}, {"formula_id": "formula_256", "formula_text": "X \u223c S \u03b1 (|a|\u03c3, Sign(a)\u03b2, a\u03bc) if \u03b1 = 1 and X \u223c S \u03b1 (|a|\u03c3, Sign(a)\u03b2, a\u03bc \u2212 (2/\u03c0)a(Log|a|)\u03c3\u03b2) if \u03b1 = 1. \u03c3 is called scale parameter. When \u03b1 = 2 , the characteristic function (1)", "formula_coordinates": [309.0, 41.83, 274.54, 345.77, 45.06]}, {"formula_id": "formula_257", "formula_text": "x r = [x r 1 , x r 2 , .x r N ]", "formula_coordinates": [310.0, 310.4, 290.28, 74.53, 12.48]}, {"formula_id": "formula_258", "formula_text": "p r = \u03c9 r k j=1 \u03c9 j (2", "formula_coordinates": [310.0, 183.79, 371.54, 199.67, 26.63]}, {"formula_id": "formula_259", "formula_text": ")", "formula_coordinates": [310.0, 383.46, 378.26, 4.25, 8.78]}, {"formula_id": "formula_260", "formula_text": "\u03c9 r = 1 qk \u221a 2\u03c0 e \u2212 (r\u22121) 2 2q 2 k 2(3)", "formula_coordinates": [310.0, 169.16, 413.77, 218.56, 24.74]}, {"formula_id": "formula_261", "formula_text": "\u03c3 r i = \u03be k e=1 |x e i \u2212 x r i | k \u2212 1 real (4", "formula_coordinates": [310.0, 162.56, 573.12, 220.9, 30.24]}, {"formula_id": "formula_262", "formula_text": ")", "formula_coordinates": [310.0, 383.46, 583.58, 4.25, 8.78]}, {"formula_id": "formula_263", "formula_text": "x i = x i + S\u03b1(\u03c3 i l ,", "formula_coordinates": [311.0, 80.96, 312.8, 66.16, 11.01]}, {"formula_id": "formula_264", "formula_text": "f6 = \u221220exp[\u22120.2 1 N N i=1 x 2 i ] \u2212 exp[ 1 N N i=1 cos(2\u03c0xi)] 30 [-32, 32] N f7 = 1 4000 N i=1 x 2 i \u2212 N i=1 cos( x i \u221a i ) + 1 30 [-600, 600] N f11 = (1 + (x1 + x2 + 1) 2 (19 \u2212 14x1 + 3x 2 1 \u2212 14x2 + 6x1x2 + 3x 2 2 ))x 2 [-2, 2] N (30 + (2x1 \u2212 3x2) 2 )(18 \u2212 32x1 + 12x 2 1 + 48x2 \u2212 36x1x2 + 27x 2 2 )", "formula_coordinates": [312.0, 44.96, 124.64, 333.14, 48.25]}, {"formula_id": "formula_265", "formula_text": "\u03bb b (m, n) = \u03c0 2 a 2 m 4 D 11 + 2(D 12 + 2D 66 )r 2 m 2 n 2 + r 4 n 4 D 22 m 2 N x + r 2 n 2 N y (5", "formula_coordinates": [313.0, 88.64, 535.68, 294.82, 24.72]}, {"formula_id": "formula_266", "formula_text": ")", "formula_coordinates": [313.0, 383.46, 543.98, 4.25, 8.78]}, {"formula_id": "formula_267", "formula_text": "max Dij min m,n \u03bb b (m, n); m, n \u2208 1, 2(6)", "formula_coordinates": [314.0, 140.6, 129.46, 247.12, 14.91]}, {"formula_id": "formula_268", "formula_text": "\u23a1 \u23a3 \u03c3 xx \u03c3 yy \u03c4 xy \u23a4 \u23a6 = \u23a1 \u23a3 Q 11 Q 12 Q 13 Q 21 Q 22 Q 23 Q 31 Q 32 Q 33 \u23a4 \u23a6 k . \u23a1 \u23a3 xx yy \u03b3 xy \u23a4 \u23a6 (7", "formula_coordinates": [314.0, 136.39, 192.0, 247.06, 45.49]}, {"formula_id": "formula_269", "formula_text": ")", "formula_coordinates": [314.0, 383.46, 212.78, 4.25, 8.78]}, {"formula_id": "formula_270", "formula_text": "D ij = 1 3 n k=1 Q ij z 3 k \u2212 z 3 k\u22121 (8", "formula_coordinates": [314.0, 153.44, 289.08, 230.02, 30.48]}, {"formula_id": "formula_271", "formula_text": ")", "formula_coordinates": [314.0, 383.46, 299.54, 4.25, 8.78]}, {"formula_id": "formula_272", "formula_text": "E k 11 , E k 22 , G k 12 , \u03bd k", "formula_coordinates": [314.0, 41.84, 362.4, 345.85, 22.34]}, {"formula_id": "formula_273", "formula_text": "( )( ) [ ] ( ) ( ) [ ] 0 2", "formula_coordinates": [328.0, 90.06, 547.55, 216.5, 36.26]}, {"formula_id": "formula_274", "formula_text": "= \u03a9 + + + + = \u03a9 \u2212 + + + + + + + x y k y k y c y m y x wt V r k x wt V r k x c x m a a (1)", "formula_coordinates": [328.0, 50.58, 554.38, 337.27, 30.2]}, {"formula_id": "formula_275", "formula_text": ", 1 , k m 2 , , , , , , k L k k k k V L r k L k k V r mk c k m t L y q L x q a a s d \u03b5\u03be \u03b5\u03b4 \u03b5\u03b3 \u03b5\u03bb \u03b5\u03bd \u03b5\u03bb \u03b5\u03b6 \u03c4 (2)", "formula_coordinates": [329.0, 46.74, 270.14, 341.23, 62.07]}, {"formula_id": "formula_276", "formula_text": "( ) ( ) [ ] ( ) ( ) [ ] (", "formula_coordinates": [329.0, 121.26, 355.21, 148.38, 58.4]}, {"formula_id": "formula_277", "formula_text": "0 1 2 0 2 cos 1 v 2 cos 1 1 2 3 3 3 3 1 = \u2032 + + + + \u2032 + \u2032 \u2032 = \u2032 \u2212 + + + + + + \u2032 + \u2032 \u2032 d s s s s s d d d d q q q q q q q wt q wt q q \u03b5\u03b3 \u03b5\u03be \u03b5\u03b4 \u03b5\u03b6 \u03b5\u03b3 \u03b5\u03bb \u03b5 \u03b5\u03bb \u03b5\u03b6 (3)", "formula_coordinates": [329.0, 110.22, 358.88, 277.76, 54.65]}, {"formula_id": "formula_278", "formula_text": "( ) [ ( ) [ ] ( )(", "formula_coordinates": [329.0, 72.66, 451.03, 94.91, 95.95]}, {"formula_id": "formula_279", "formula_text": ")] 3 8 4 ( 4 [ 8 ))], 2 cos( ) ( 2 ) 3(v 8 4 ( 4 [ 8 2 cos 2 ))], (2 ) 2 ( 8 ( cos 4 8 2 2 2 4 3 1 2 4 3 2 1 3 1 2 1 3 3 1 1 4 3 2 1 3 2 4 3 1 2 1 2 1 3 1 1 4 3 2 1 x x x x sin x x x x x x x x x sin x x x x x x x x x sin x x x x x x \u03be \u03c3 \u03b4 \u03b3 \u03b5 \u03bb \u03bb \u03bb \u03c3 \u03bb \u03b3 \u03b5 \u03b6 \u03b3 \u03b5 \u03bb \u03bb \u03b6 \u03b3 \u03b5 + \u2212 + \u2212 \u2212 = + + + + \u2212 + \u2212 \u2212 = + \u2212 \u2212 = + + \u2212 + \u2212 = (4))", "formula_coordinates": [329.0, 48.66, 448.0, 339.19, 106.36]}, {"formula_id": "formula_280", "formula_text": "( ) ( ) ( ) i g i i i i i i x p x p v w v \u2212 + \u2212 + \u03a7 = 2 1 \u03d5 \u03d5 (5)", "formula_coordinates": [332.0, 110.7, 151.04, 277.15, 22.29]}, {"formula_id": "formula_281", "formula_text": "i i i v x x + = (6)", "formula_coordinates": [332.0, 172.38, 265.04, 215.47, 16.55]}, {"formula_id": "formula_282", "formula_text": "R 2 = 1 \u2212 n i=1 (a i \u2212 p i ) 2 n i=1 (a i \u2212 a) 2 (1)", "formula_coordinates": [341.0, 158.84, 262.32, 228.88, 27.96]}, {"formula_id": "formula_283", "formula_text": "for instance P1O is refined into C-C-N-P1O or C-C-O-P1O.", "formula_coordinates": [344.0, 115.4, 441.26, 253.81, 8.78]}, {"formula_id": "formula_284", "formula_text": "d(I, P ) = n i=1 |f P i \u2212 f Ii |", "formula_coordinates": [351.0, 115.52, 548.16, 103.93, 30.36]}, {"formula_id": "formula_285", "formula_text": "f P i = 1.", "formula_coordinates": [351.0, 278.59, 558.62, 35.53, 9.59]}, {"formula_id": "formula_286", "formula_text": "R \u0394 (h) = \u0394(h(x), y) d(P r(x, y)),(1)", "formula_coordinates": [359.0, 136.87, 433.8, 250.84, 10.83]}, {"formula_id": "formula_287", "formula_text": "\u0394 (h(x), y) = max{0, |h(x) \u2212 y| \u2212 }.", "formula_coordinates": [359.0, 133.4, 590.86, 162.85, 9.62]}, {"formula_id": "formula_288", "formula_text": "Alarm(h(x)) = non-alarm h(x) \u2208 (\u2212\u221e, \u03c4] alarm h(x) \u2208 (\u03c4, +\u221e).(3)", "formula_coordinates": [360.0, 106.75, 85.18, 280.95, 21.62]}, {"formula_id": "formula_289", "formula_text": "h (x) = n i=1 (\u03b1 \u2212 i \u2212 \u03b1 + i )K(x i , x) + b * , (4", "formula_coordinates": [360.0, 134.23, 379.68, 249.22, 30.24]}, {"formula_id": "formula_290", "formula_text": ")", "formula_coordinates": [360.0, 383.46, 390.14, 4.25, 8.78]}, {"formula_id": "formula_291", "formula_text": "x i , x j ) = e \u2212 x i \u2212x j 2 2\u03c3 2", "formula_coordinates": [360.0, 172.56, 418.33, 85.31, 17.87]}, {"formula_id": "formula_292", "formula_text": "min w,b,\u03be 1 2 w, w + C n i=1 (\u03be + i + \u03be \u2212 i ), s.t. ( w, \u03c6(x i ) + b) \u2212 y i \u2264 + \u03be + i , y i \u2212 ( w, \u03c6(x i ) + b) \u2264 + \u03be \u2212 i , (5) \u03be + i , \u03be \u2212 i \u2265 0, i = 1, . . . , n.", "formula_coordinates": [360.0, 60.44, 467.88, 327.28, 61.56]}, {"formula_id": "formula_293", "formula_text": "h ND( ) (x) = [h (x) \u2212 , h (x) + ].(6)", "formula_coordinates": [360.0, 139.64, 558.58, 248.07, 10.36]}, {"formula_id": "formula_294", "formula_text": "Alarm(h ND (x)) = \u23a7 \u23a8 \u23a9 non-alarm h ND (x) \u2282 (\u2212\u221e, \u03c4] alarm h ND (x) \u2282 (\u03c4, +\u221e) warning otherwise. (7", "formula_coordinates": [361.0, 92.59, 219.12, 290.86, 41.8]}, {"formula_id": "formula_295", "formula_text": ")", "formula_coordinates": [361.0, 383.46, 240.25, 4.25, 8.78]}, {"formula_id": "formula_296", "formula_text": "out tube(h ND , S ) = 1 m m i=1 1 \u2212 y i \u2208 h ND (x i ) . (8", "formula_coordinates": [361.0, 110.23, 311.4, 273.22, 30.24]}, {"formula_id": "formula_297", "formula_text": ")", "formula_coordinates": [361.0, 383.46, 321.74, 4.25, 8.78]}, {"formula_id": "formula_298", "formula_text": "{(x t , y) : t \u2208 [30, 25, 20, 15, 10]}. (9", "formula_coordinates": [361.0, 145.99, 458.38, 237.47, 9.99]}, {"formula_id": "formula_299", "formula_text": ")", "formula_coordinates": [361.0, 383.46, 458.78, 4.25, 8.78]}, {"formula_id": "formula_300", "formula_text": "Alarm(h ND (x t )) = \u23a7 \u23a8 \u23a9 non-alarm \u2200t, h ND (x t ) \u2282 (\u2212\u221e, \u03c4] alarm", "formula_coordinates": [361.0, 82.27, 550.08, 260.05, 36.96]}, {"formula_id": "formula_301", "formula_text": "T = Dns + D2S + Con + A2G + First + Last.", "formula_coordinates": [367.0, 41.84, 435.74, 345.79, 21.01]}, {"formula_id": "formula_302", "formula_text": "LogGpt n \u223c f (LogGpt n\u22121 , LogGpt n\u22122 , . . . ; Dns, Con, First, Hour, DoW)(1)", "formula_coordinates": [369.0, 60.31, 525.46, 327.4, 11.31]}, {"formula_id": "formula_303", "formula_text": "z i = (x i \u2212 min x i )/(max x i \u2212 min x i ).", "formula_coordinates": [370.0, 154.64, 220.66, 163.45, 9.99]}, {"formula_id": "formula_304", "formula_text": "f (x) = K n=1 a n f n (x; \u0398 n ) ( 2)", "formula_coordinates": [370.0, 164.12, 487.92, 223.59, 30.12]}, {"formula_id": "formula_305", "formula_text": "\u2200 i=1,...,K\u22121 x i : f i (x i ; \u0398 i ) = f i+1 (x i ; \u0398 (i+1) ).", "formula_coordinates": [371.0, 183.92, 494.14, 194.05, 10.36]}, {"formula_id": "formula_306", "formula_text": "(\u03c3 2 1 \u2212 \u03c3 2 2 )x 2 + 2(\u03bc 1 \u03c3 2 2 \u2212 \u03bc 2 \u03c3 2 1 )x + \u03c3 2 1 \u03bc 2 2 \u2212 \u03c3 2 2 \u03bc 2 1 \u2212 2(\u03c3 1 \u03c3 2 ) 2 ln(\u03c3 1 /\u03c3 2 ) = 0", "formula_coordinates": [371.0, 58.16, 589.08, 313.14, 12.73]}, {"formula_id": "formula_307", "formula_text": "[0, x 1 ), [x 1 , x 2 ), . . . , [x K\u22121 , \u221e)", "formula_coordinates": [372.0, 150.31, 74.5, 128.8, 10.0]}, {"formula_id": "formula_308", "formula_text": "DoT c,\u03a9,f (Degree of Trust) = F (F A c,\u03a9,f , F W c,\u03a9,f )", "formula_coordinates": [390.0, 114.79, 115.68, 199.84, 12.6]}, {"formula_id": "formula_309", "formula_text": "DoT c,\u03a9 = f (f A c,\u03a9,f1 ..f A c,\u03a9,fn , f W c,\u03a9,f1 ..f W c,\u03a9,fn )", "formula_coordinates": [390.0, 119.24, 169.08, 190.95, 12.72]}, {"formula_id": "formula_310", "formula_text": "{\u03a9 1 , \u2022 \u2022 \u2022 , \u03a9 o }.", "formula_coordinates": [399.0, 217.99, 209.14, 60.97, 10.0]}, {"formula_id": "formula_311", "formula_text": "If D = (q i ) n i=1", "formula_coordinates": [400.0, 327.92, 391.68, 59.17, 12.36]}, {"formula_id": "formula_312", "formula_text": "q t+1 i = (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 q t i , for i = 1, . . . , n(1)", "formula_coordinates": [400.0, 109.03, 424.32, 278.68, 13.08]}, {"formula_id": "formula_313", "formula_text": "distribution (p [info] i", "formula_coordinates": [401.0, 41.84, 51.36, 81.93, 14.04]}, {"formula_id": "formula_314", "formula_text": "i {p [info] i : J D s ([info]) are all in \u03c9 i } = 1. (2) But R t (\u03a0, \u03a9, O([info])) = r \u2208 [0, 1] and \u03a0 should only treat the J D s ([info]", "formula_coordinates": [401.0, 41.84, 82.44, 345.87, 40.92]}, {"formula_id": "formula_315", "formula_text": "[info] i", "formula_coordinates": [401.0, 112.52, 135.48, 18.45, 14.04]}, {"formula_id": "formula_316", "formula_text": "p t i = r \u00d7 p [info] i + (1 \u2212 r) \u00d7 q t i (3)", "formula_coordinates": [401.0, 152.23, 154.8, 235.48, 14.04]}, {"formula_id": "formula_317", "formula_text": "[info] i", "formula_coordinates": [401.0, 274.52, 334.68, 18.45, 14.04]}, {"formula_id": "formula_318", "formula_text": "[fact] i", "formula_coordinates": [401.0, 259.87, 433.68, 17.85, 14.04]}, {"formula_id": "formula_319", "formula_text": "q s i ) n i=1 , calculate (p [info] i", "formula_coordinates": [401.0, 200.66, 459.48, 101.55, 14.04]}, {"formula_id": "formula_320", "formula_text": "[fact] i ) n i=1 using Eqn. 2.", "formula_coordinates": [401.0, 41.84, 459.48, 345.25, 26.07]}, {"formula_id": "formula_321", "formula_text": "([info]|[fact]) D", "formula_coordinates": [401.0, 325.15, 473.64, 45.11, 14.28]}, {"formula_id": "formula_322", "formula_text": "[fact] i ) n i=1 : arg min r n i=1 (r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i ) log r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i p [fact] i", "formula_coordinates": [401.0, 83.36, 499.56, 261.04, 49.32]}, {"formula_id": "formula_323", "formula_text": "R ([info]|[fact]) = 1 \u2212 ( max D\u2208E [info] |1 \u2212 R ([info]|[fact]) D |)", "formula_coordinates": [401.0, 113.95, 584.16, 201.52, 18.6]}, {"formula_id": "formula_324", "formula_text": "R t+1 (\u03a0, \u03a9, o j ) = (1 \u2212 \u03c1) \u00d7 R t (\u03a0, \u03a9, o j )+ \u03c1 \u00d7 R ([info]|[fact]) \u00d7 Sem(o j , o k ) (4)", "formula_coordinates": [402.0, 117.07, 86.16, 270.64, 28.68]}, {"formula_id": "formula_325", "formula_text": "\u2022) : O \u00d7 O \u2192 [0, 1]", "formula_coordinates": [402.0, 100.76, 127.06, 84.85, 9.62]}, {"formula_id": "formula_326", "formula_text": "R t (\u03a0, \u03a9) = j P t (o j ) \u00d7 R t (\u03a0, \u03a9, o j )", "formula_coordinates": [402.0, 133.16, 208.68, 163.11, 21.96]}, {"formula_id": "formula_327", "formula_text": "T : {X \u222a N } \u2192 G.", "formula_coordinates": [408.0, 83.96, 507.1, 79.45, 9.62]}, {"formula_id": "formula_328", "formula_text": "H(X) \u2212 i p(x i ) log p(x i )", "formula_coordinates": [409.0, 41.84, 294.96, 124.6, 11.89]}, {"formula_id": "formula_329", "formula_text": "(\u2200\u03c3 \u2208 L)P {W |K s } (\u03c3) n { p n : \u03c3 is in v n } (1)", "formula_coordinates": [409.0, 112.99, 438.82, 274.72, 19.95]}, {"formula_id": "formula_330", "formula_text": "). A random world W |K s is consistent with B s t if: (\u2200\u03a9 \u2208 B s t )(B(\u03a9) = P {W |K s } (\u03a9)). Let {p i } = {W |K s , B s", "formula_coordinates": [409.0, 41.84, 477.0, 345.4, 24.12]}, {"formula_id": "formula_331", "formula_text": "(\u2200\u03c3 \u2208 L)P {W |K s ,B s t } (\u03c3) n { p n : \u03c3 is in v n } (2)", "formula_coordinates": [409.0, 107.24, 548.62, 280.47, 20.07]}, {"formula_id": "formula_332", "formula_text": "{W |q, C} arg min p n i=1 p i log p i q i", "formula_coordinates": [410.0, 145.16, 168.96, 137.46, 30.24]}, {"formula_id": "formula_333", "formula_text": "(\u2200\u03c3 \u2208 L)P {W |q,C} (\u03c3) n { p n : \u03c3 is in v n } (3)", "formula_coordinates": [410.0, 111.79, 277.3, 275.92, 20.07]}, {"formula_id": "formula_334", "formula_text": "\u03a9, R t (\u03a0, \u03a9, O([info])), where O([info]) is the ontological context of [info].", "formula_coordinates": [411.0, 41.83, 245.52, 346.09, 22.35]}, {"formula_id": "formula_335", "formula_text": "If D = (q i ) n i=1", "formula_coordinates": [411.0, 327.92, 412.92, 59.17, 12.36]}, {"formula_id": "formula_336", "formula_text": "q t+1 i = (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 q t i , for i = 1, . . . , n(4)", "formula_coordinates": [411.0, 109.03, 446.88, 278.68, 13.08]}, {"formula_id": "formula_337", "formula_text": "[info] i", "formula_coordinates": [412.0, 105.32, 76.8, 18.45, 14.04]}, {"formula_id": "formula_338", "formula_text": "i {p [info] i : J D s ([info]) are all in \u03c9 i } = 1. (5", "formula_coordinates": [412.0, 124.52, 112.8, 258.94, 23.04]}, {"formula_id": "formula_339", "formula_text": ")", "formula_coordinates": [412.0, 383.46, 119.66, 4.25, 8.78]}, {"formula_id": "formula_340", "formula_text": "But R t (\u03a0, \u03a9, O([info])) = r \u2208 [0, 1] and \u03a0 should only treat the J D s ([info]", "formula_coordinates": [412.0, 41.84, 146.4, 329.39, 12.24]}, {"formula_id": "formula_341", "formula_text": "[info] i", "formula_coordinates": [412.0, 112.52, 170.76, 18.45, 14.04]}, {"formula_id": "formula_342", "formula_text": "p t i = r \u00d7 p [info] i + (1 \u2212 r) \u00d7 q t i (6)", "formula_coordinates": [412.0, 152.23, 194.88, 235.48, 14.16]}, {"formula_id": "formula_343", "formula_text": "n i=1 p t i log p t i d D i > n i=1 q t i log q t i d D i (7)", "formula_coordinates": [412.0, 153.32, 347.16, 234.4, 30.24]}, {"formula_id": "formula_344", "formula_text": "q t+1 i = (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 p t i if [info] is usable (1 \u2212 \u03c1 D ) \u00d7 d D i + \u03c1 D \u00d7 q t i otherwise for i = 1, \u2022 \u2022 \u2022 , n", "formula_coordinates": [412.0, 41.84, 432.12, 287.02, 49.44]}, {"formula_id": "formula_345", "formula_text": "Consider one of \u03a0's distributions D that is {q s i } at time s. Let (p [info] i", "formula_coordinates": [413.0, 41.84, 51.36, 313.77, 14.04]}, {"formula_id": "formula_346", "formula_text": "[fact] i", "formula_coordinates": [413.0, 200.35, 77.16, 17.85, 14.16]}, {"formula_id": "formula_347", "formula_text": "[fact] i", "formula_coordinates": [413.0, 259.87, 150.36, 17.85, 14.04]}, {"formula_id": "formula_348", "formula_text": "[info] i", "formula_coordinates": [413.0, 219.32, 188.16, 18.45, 14.04]}, {"formula_id": "formula_349", "formula_text": "([info]|[fact]) D", "formula_coordinates": [413.0, 257.84, 202.32, 44.99, 14.16]}, {"formula_id": "formula_350", "formula_text": "[fact] i ) n i=1 : arg min r n i=1 (r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i ) log r \u2022 p [info] i + (1 \u2212 r) \u2022 q s i p [fact] i", "formula_coordinates": [413.0, 83.36, 228.12, 261.04, 53.28]}, {"formula_id": "formula_351", "formula_text": "R ([info]|[fact]) = 1 \u2212 (max D\u2208E [info] |1 \u2212 R ([info]|[fact])D", "formula_coordinates": [413.0, 41.84, 303.12, 345.79, 26.52]}, {"formula_id": "formula_352", "formula_text": "R t+1 (\u03a0, \u03a9, o j ) = (1 \u2212 \u03c1) \u00d7 R t (\u03a0, \u03a9, o j ) + \u03c1 \u00d7 R ([info]|[fact]) \u00d7 Sem(o j , o k )", "formula_coordinates": [413.0, 56.48, 363.24, 316.59, 11.64]}, {"formula_id": "formula_353", "formula_text": "\u2022) : O \u00d7 O \u2192 [0, 1] measures the semantic distance", "formula_coordinates": [413.0, 101.12, 386.98, 231.94, 9.62]}, {"formula_id": "formula_354", "formula_text": "arg max \u03b4 {P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) | P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) 0}", "formula_coordinates": [414.0, 93.08, 368.88, 243.42, 16.68]}, {"formula_id": "formula_355", "formula_text": "arg max \u03b4 {P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) \u00d7 P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) | P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) \u2265 \u03b3}", "formula_coordinates": [414.0, 42.56, 425.04, 344.34, 16.8]}, {"formula_id": "formula_356", "formula_text": "arg max \u03b4 {P t (\u03a9Acc(\u03a9, \u03a0, \u03b4)) | P t (\u03a0Acc(\u03a0, \u03a9, \u03bd, \u03b4)) \u2265 \u03b3}", "formula_coordinates": [414.0, 93.8, 470.04, 241.86, 16.68]}, {"formula_id": "formula_357", "formula_text": "\u03a0(A) = max s\u2208A \u03c0(s);", "formula_coordinates": [417.0, 136.04, 258.02, 78.97, 14.63]}, {"formula_id": "formula_358", "formula_text": "N (A) = 1 \u2212 \u03a0(\u0100) = min s\u2208\u0100 {1 \u2212 \u03c0(s)}. (2", "formula_coordinates": [417.0, 136.04, 277.78, 247.81, 15.99]}, {"formula_id": "formula_359", "formula_text": ")", "formula_coordinates": [417.0, 383.85, 278.11, 3.91, 8.97]}, {"formula_id": "formula_360", "formula_text": "\u0394(A) = min s\u2208A \u03c0(s); (3", "formula_coordinates": [417.0, 175.75, 376.63, 208.09, 14.58]}, {"formula_id": "formula_361", "formula_text": ")", "formula_coordinates": [417.0, 383.85, 376.63, 3.91, 8.97]}, {"formula_id": "formula_362", "formula_text": "1. \u03a0(A \u222a B) = max{\u03a0(A), \u03a0(B)}, N (A \u2229 B) = min{N (A), N(B)}; 2. \u03a0(\u2205) = N (\u2205) = 0, \u03a0(\u03a9) = N (\u03a9) = 1, \u03a0(A) = 1 \u2212 N (\u0100); 3. N (A) \u2264 \u03a0(A), \u0394(A) \u2264 \u03a0(A); 4. N (A) > 0 implies \u03a0(A) = 1, \u03a0(A) < 1 implies N (A) = 0;", "formula_coordinates": [417.0, 46.26, 447.1, 303.5, 46.37]}, {"formula_id": "formula_363", "formula_text": "\u03b2 R , \u03c8 R \u21d2 + D \u03c6, where \u03b2 R , \u03c8 R , \u03c6 \u2208 L. The unconditional counterpart of this rule is \u03b1 \u21d2 + D \u03c6, with \u03b1 \u2208 (0, 1].", "formula_coordinates": [418.0, 41.84, 439.56, 345.84, 25.2]}, {"formula_id": "formula_364", "formula_text": "B(\u03c6) = N ([\u03c6]) = 1 \u2212 max I |=\u03c6 {\u03c0(I)}. (4", "formula_coordinates": [419.0, 142.88, 137.26, 240.97, 15.72]}, {"formula_id": "formula_365", "formula_text": ")", "formula_coordinates": [419.0, 383.85, 137.59, 3.91, 8.97]}, {"formula_id": "formula_366", "formula_text": "B(\u03c6 \u2227 \u03c8) = min{B(\u03c6), B(\u03c8)}, B(\u03c6 \u2228 \u03c8) \u2265 max{B(\u03c6), B(\u03c8)}. (5", "formula_coordinates": [419.0, 80.47, 203.38, 303.36, 9.62]}, {"formula_id": "formula_367", "formula_text": ")", "formula_coordinates": [419.0, 383.84, 203.71, 3.91, 8.97]}, {"formula_id": "formula_368", "formula_text": "R J = \u23a7 \u23a8 \u23a9 R 1 : a, p \u21d2 + D t \u2227 h, R 2 : a \u2227 q, \u21d2 + D p, R 3 : a, \u21d2 + D r. \u23ab \u23ac \u23ad .", "formula_coordinates": [419.0, 138.2, 554.28, 153.01, 44.41]}, {"formula_id": "formula_369", "formula_text": "\u03c0 (I) = \u23a7 \u23a8 \u23a9 \u03c0(I) \u03a0([\u03c6]) , if I |= \u03c6 and B(\u00ac\u03c6) < 1; 1, if I |= \u03c6 and B(\u00ac\u03c6) = 1; min{\u03c0(I), (1 \u2212 \u03c4 )}, if I |= \u03c6. (6)", "formula_coordinates": [420.0, 93.32, 221.76, 294.43, 43.92]}, {"formula_id": "formula_370", "formula_text": "\u00acp \u00acp p p p \u00acr r r \u00act t \u00act t \u00aca, \u00acb, \u00acf 1 1 0 0 0 \u00aca, \u00acb, f, \u00ach 1 1 0 0 0 \u00aca, b, \u00acf 1 0.1 0 0 0 \u00aca, b, f, \u00ach 1 0.1 0 0 0 a, \u00acb, \u00acf 1 1 0 1 1 a, \u00acb, f, \u00ach 1 1 0 1 1 a, b, \u00acf 1 0.1 0 1 0.1 a, b, f, \u00ach 1 0.1 0 1 0.1 f,", "formula_coordinates": [421.0, 48.21, 54.22, 116.64, 128.89]}, {"formula_id": "formula_371", "formula_text": "J (\u03c6) = \u0394([\u03c6]) = min I|=\u03c6 u(I). (7", "formula_coordinates": [421.0, 156.08, 542.62, 227.77, 15.71]}, {"formula_id": "formula_372", "formula_text": ")", "formula_coordinates": [421.0, 383.85, 542.95, 3.91, 8.97]}, {"formula_id": "formula_373", "formula_text": "Deg(R) = min{B(\u03b2 R ), J (\u03c8 R )}.", "formula_coordinates": [422.0, 146.83, 138.7, 135.73, 9.99]}, {"formula_id": "formula_374", "formula_text": "u(I) = max R\u2208R I J Deg(R). (8", "formula_coordinates": [422.0, 168.2, 237.31, 215.65, 17.89]}, {"formula_id": "formula_375", "formula_text": ")", "formula_coordinates": [422.0, 383.85, 237.31, 3.91, 8.97]}, {"formula_id": "formula_376", "formula_text": "2. i \u2190 i + 1; 3. For all I \u2208 \u03a9, u i (I) \u2190 max R\u2208R I J Deg i\u22121 (R), if R I J = \u2205, 0, otherwise,", "formula_coordinates": [422.0, 46.27, 347.26, 268.41, 51.42]}, {"formula_id": "formula_377", "formula_text": "\u03a0([S]) = max I\u2208[S] \u03c0(I). (9", "formula_coordinates": [423.0, 171.31, 249.07, 212.53, 15.39]}, {"formula_id": "formula_378", "formula_text": ")", "formula_coordinates": [423.0, 383.85, 249.07, 3.91, 8.97]}, {"formula_id": "formula_379", "formula_text": "J (S) = \u0394([S]) = min I\u2208[S] u(I). (10", "formula_coordinates": [423.0, 153.68, 329.14, 229.89, 15.72]}, {"formula_id": "formula_380", "formula_text": ")", "formula_coordinates": [423.0, 383.56, 329.47, 4.19, 8.97]}, {"formula_id": "formula_381", "formula_text": "Proposition 2.", "formula_coordinates": [423.0, 41.84, 375.98, 59.13, 8.78]}, {"formula_id": "formula_382", "formula_text": "J (S)) \u2265 max \u03c6\u2208S {J (\u03c6)}.(11)", "formula_coordinates": [423.0, 167.84, 409.54, 219.92, 15.03]}, {"formula_id": "formula_383", "formula_text": "J (S \u222a {\u03c6}) \u2265 J (S); (12) J (S) \u2265 J (S \\ {\u03c6}). (13", "formula_coordinates": [423.0, 166.63, 482.14, 221.12, 24.62]}, {"formula_id": "formula_384", "formula_text": ")", "formula_coordinates": [423.0, 383.56, 497.46, 4.19, 8.97]}, {"formula_id": "formula_385", "formula_text": "G \u03b3 = arg max S\u2208D\u03b3 J (S) if D \u03b3 = \u2205, \u2205 otherwise.", "formula_coordinates": [424.0, 133.28, 289.66, 160.41, 21.62]}, {"formula_id": "formula_386", "formula_text": "G * = G \u03b3 * , \u03b3 * = max G\u03b3 =\u2205 \u03b3. (14", "formula_coordinates": [424.0, 158.84, 355.52, 224.73, 17.69]}, {"formula_id": "formula_387", "formula_text": ")", "formula_coordinates": [424.0, 383.57, 358.03, 4.19, 8.97]}, {"formula_id": "formula_388", "formula_text": "y = ( \u03a3 k y k )/n c = ( \u03a3 k c k )/n e = ( \u03a3 k e k )/n", "formula_coordinates": [430.0, 77.58, 385.66, 269.46, 12.98]}, {"formula_id": "formula_389", "formula_text": "TD(t) = TD(0) e inn t x(t) = \u03b1 TD(t) = \u03b1 TD(0) e inn t = x(0) e inn t", "formula_coordinates": [433.0, 89.58, 306.43, 251.14, 12.39]}, {"formula_id": "formula_390", "formula_text": "(1) ax = bxz (2) c k bx y (k) (2-y (k) ) / TD = e k y (k) (3) inn TD = 0 (4) z = (\u03a3 k y (k) )/n", "formula_coordinates": [433.0, 60.18, 478.92, 208.64, 21.59]}, {"formula_id": "formula_391", "formula_text": "y (j) = 2-", "formula_coordinates": [434.0, 78.18, 119.4, 27.8, 9.72]}, {"formula_id": "formula_392", "formula_text": "x i |= M \u03c6 1 \u2234 \u03c6 2 iff x i |= M \u03c6 1 and x i+1 |= M \u03c6 1 \u21d2 \u03c6 2", "formula_coordinates": [442.0, 102.56, 140.04, 223.81, 11.65]}, {"formula_id": "formula_393", "formula_text": "s i |= M P C(Ag 1 , Ag 2 , t, \u03c6 1 ) iff P C(Ag 1 , Ag 2 , t, \u03c6 1 ) \u2208 V P C (s i ) and s i |= M E\u03c6 1 A path x i satisfies Act k (Ag n , P C(Ag 1 , Ag 2 , t, \u03c6 1 )", "formula_coordinates": [442.0, 41.84, 189.1, 340.69, 29.08]}, {"formula_id": "formula_394", "formula_text": "x i |= M Act k (Ag n , P C(Ag 1 , Ag 2 , t, \u03c6 1 )) iff s i , Ag n Act k \u2212\u2192 s i+1 and x i |= M F \u2212 P C(Ag 1 , Ag 2 , t, \u03c6 1 )", "formula_coordinates": [442.0, 58.04, 247.56, 313.48, 25.33]}, {"formula_id": "formula_395", "formula_text": "\u2203t14 [ t13 > t14 & at(i2, t14", "formula_coordinates": [450.0, 42.18, 348.6, 93.77, 10.48]}, {"formula_id": "formula_396", "formula_text": "B \u21d4 at(p, t) B[B2/at(p2, t2)] \u21d4 at(p, t) B[B1/at(p1, t1)] \u21d4 at(p, t) B[B1/at(p1, t1), B2/at(p2, t2)] \u21d4 at(p, t)", "formula_coordinates": [451.0, 42.18, 425.29, 207.16, 19.75]}, {"formula_id": "formula_397", "formula_text": "\u2203t6 [ t5 > t6 & at(d1, t6) & at(b2, t6)] \u21d4 at(i1, t5)", "formula_coordinates": [454.0, 49.26, 82.32, 170.44, 10.48]}, {"formula_id": "formula_398", "formula_text": "\u2203t6 [ t5 > t6 & \u2203t4 [ t6 > t4 & at(b1, t4) ] & \u2203t2 [ t6 > t2 & at(observed(own_position_refrigerator), t2) ] ] \u21d4 at(i1, t5)", "formula_coordinates": [454.0, 49.26, 123.12, 336.34, 20.32]}, {"formula_id": "formula_399", "formula_text": "\u2203t6 [ t5 > t6 & \u2203t4 [ t6 > t4 & \u2203t15 [ t4 > t15 & at(observed(food_not_eaten_more_than_2h), t15) ] ] & \u2203t2 [ t6 > t2 & at(observed(own_position_refrigerator), t2) ] ] \u21d4 at(i1, t5)", "formula_coordinates": [454.0, 49.26, 185.16, 336.53, 20.32]}, {"formula_id": "formula_400", "formula_text": "\u2203t12 [ t11 > t12 & \u2203t16 [ t12 > t16 & at(observed(own_position_cupboard), t16) ] ] \u21d4 at(i2, t11)", "formula_coordinates": [454.0, 49.26, 270.36, 334.71, 10.48]}, {"formula_id": "formula_401", "formula_text": "monitor_focus(F) \u2192 in_focus(F) in_focus(E) \u2227 is_composed_of(E,C,E1,E2) \u2192 in_focus(E1) \u2227 in_focus(E2)", "formula_coordinates": [454.0, 56.34, 387.6, 271.12, 20.32]}, {"formula_id": "formula_402", "formula_text": "= var(R x ) var(L x )(1)", "formula_coordinates": [473.0, 193.88, 178.1, 193.84, 23.15]}, {"formula_id": "formula_403", "formula_text": "| | \u2022 \u2022", "formula_coordinates": [493.0, 167.7, 484.42, 78.29, 12.03]}, {"formula_id": "formula_404", "formula_text": "Running Interval [1] Interval [2] Interval [3] Interval [", "formula_coordinates": [496.0, 97.14, 486.53, 195.25, 18.54]}, {"formula_id": "formula_405", "formula_text": "\u03a9 \u2022 \u2211 \u03a9 \u2022", "formula_coordinates": [498.0, 191.22, 189.82, 57.04, 27.15]}, {"formula_id": "formula_406", "formula_text": "\u2026 \u2026 \u2026 \u2026", "formula_coordinates": [498.0, 233.46, 274.66, 8.32, 49.35]}, {"formula_id": "formula_407", "formula_text": "IF", "formula_coordinates": [507.0, 53.58, 556.91, 10.01, 8.87]}, {"formula_id": "formula_408", "formula_text": "P ij (t) = \u03c4 \u03b1 ij (t)\u03b7 \u03b2 ij x\u2208N (i) \u03c4 \u03b1 ix (t)\u03b7 \u03b2 ix (1)", "formula_coordinates": [514.0, 162.56, 457.68, 225.16, 40.69]}, {"formula_id": "formula_409", "formula_text": "page[i] = page[i]+userInterest[i]", "formula_coordinates": [517.0, 133.88, 221.48, 118.38, 10.19]}, {"formula_id": "formula_410", "formula_text": "\u2200j v t+1 ij = v t ij + r[j] \u00d7 c (3)", "formula_coordinates": [519.0, 162.56, 461.16, 225.16, 13.08]}, {"formula_id": "formula_411", "formula_text": "\u2200j v t+1 ij = v t ij + max(r[j] \u00d7 k, \u03c4 0 ) ( 4)", "formula_coordinates": [520.0, 141.55, 99.48, 246.16, 13.2]}, {"formula_id": "formula_412", "formula_text": "1 , i 2 , ..., i n ) such that \u2200i x , 1 \u2264 x \u2264 n, O \u222a IS |= C(i x )", "formula_coordinates": [530.0, 149.84, 77.62, 237.76, 10.0]}, {"formula_id": "formula_413", "formula_text": "tf \u2022 idf (w, e) = N \u22121 i=0 tf \u2022 idf (w, i) ( 1 )", "formula_coordinates": [547.0, 145.4, 250.76, 242.31, 30.77]}, {"formula_id": "formula_414", "formula_text": "Hs/ h\u2208H h \u2264 1 The placement set h \u2208 Hs \u21d2 h \u2208 [0, 1]", "formula_coordinates": [559.0, 68.96, 394.83, 192.57, 19.89]}, {"formula_id": "formula_415", "formula_text": "utility s \u2190 0 for all h \u2208 H do for all h \u2208 H do u \u2190 h \u00d7 v \u00d7 q \u2212 h \u00d7 v \u00d7 q if e \u2264 u \u2264", "formula_coordinates": [560.0, 61.03, 173.55, 132.64, 52.77]}, {"formula_id": "formula_416", "formula_text": "IC(c) = \u2212 log p(c) ( 1 )", "formula_coordinates": [567.0, 174.19, 312.82, 213.52, 9.62]}, {"formula_id": "formula_417", "formula_text": "IC(c) = 1 \u2212 log (hypo(c) + 1) log (max wn ) (2)", "formula_coordinates": [567.0, 149.6, 375.62, 238.11, 23.15]}, {"formula_id": "formula_418", "formula_text": "Sim tvr (c 1 , c 2 ) = \u03b1F (\u03c8(c 1 ) \u2229 \u03c8(c 2 )) \u2212 \u03b2F (\u03c8(c 1 )/\u03c8(c 2 )) \u2212 \u03b3F (\u03c8(c 2 )/\u03c8(c 1 )) (3)", "formula_coordinates": [567.0, 46.76, 558.82, 340.96, 10.0]}, {"formula_id": "formula_419", "formula_text": "Sim P &S (c 1 , c 2 ) = 3IC(lcs(c 1 , c 2 )) \u2212 IC(c 1 ) \u2212 IC(c 2 ) if c 1 = c 2 1 if c 1 = c 2 (4)", "formula_coordinates": [568.0, 68.71, 183.94, 319.0, 22.0]}, {"formula_id": "formula_420", "formula_text": "Sim(c 1 , c 2 ) = \u03b1 * Comm(c 1 , c 2 ) \u2212 \u03b2 * Dif f (c 1 , c 2 ) (5", "formula_coordinates": [568.0, 105.31, 484.66, 278.46, 10.0]}, {"formula_id": "formula_421", "formula_text": ")", "formula_coordinates": [568.0, 383.84, 485.06, 3.88, 8.78]}, {"formula_id": "formula_422", "formula_text": "Sim length (c 1 , c 2 ) = \u03b1 * 1 2 * depth(lcs(c 1 , c 2 )) \u2212 \u03b2 * 1 length(c 1 , c 2 ) (6)", "formula_coordinates": [569.0, 74.72, 308.77, 312.99, 23.28]}, {"formula_id": "formula_423", "formula_text": "Sim wup (c 1 , c 2 ) = \u03b1 * 2 * depth(lcs(c 1 , c 2 )) depth(c 1 ) + depth(c 2 ) \u2212 \u03b2 * length(c 1 , c 2 ) depth(c 1 ) + depth(c 2 ) (7", "formula_coordinates": [569.0, 52.04, 434.74, 331.42, 23.55]}, {"formula_id": "formula_424", "formula_text": ")", "formula_coordinates": [569.0, 383.46, 441.86, 4.25, 8.78]}, {"formula_id": "formula_425", "formula_text": "Sim lch (c 1 , c 2 ) = \u03b1 * (\u2212 lg( depth(lcs(c 1 , c 2 )) 2 * \u03bb )) \u2212 \u03b2 * (\u2212 lg( length(c 1 , c 2 ) 2 * \u03bb )) (8)", "formula_coordinates": [569.0, 48.92, 482.42, 338.79, 22.78]}, {"formula_id": "formula_426", "formula_text": "Sim P &S \u2282 Sim Res (c 1 , c 2 ) = Sim j&c (c 1 , c 2 ) (10)", "formula_coordinates": [570.0, 120.44, 99.7, 267.28, 10.95]}, {"formula_id": "formula_427", "formula_text": "Sim lin (c 1 , c 2 ) = \u03b1 * 2 * IC(lcs(c 1 , c 2 )) IC(c 1 ) + IC(c 2 ) \u2212\u03b2 * IC(c 1 ) + IC(c 2 ) \u2212 2 * IC(lcs(c 1 , c 2 )) IC(c 1 ) + IC(c 2 )(11)", "formula_coordinates": [570.0, 41.84, 202.18, 345.88, 33.66]}, {"formula_id": "formula_428", "formula_text": "b i ) = N doc (b i ) N rep (b i ) , (2", "formula_coordinates": [578.0, 191.2, 63.74, 192.26, 23.16]}, {"formula_id": "formula_429", "formula_text": ")", "formula_coordinates": [578.0, 383.46, 70.46, 4.25, 8.78]}, {"formula_id": "formula_430", "formula_text": "scen2 after v13 1 -1 7 0 0 0 6 6 6 1 1 1 -2 17 2 2 2 0 0 0 1 2 1 -3 10 2 2 2 0 0 1 1 1 1 -4 6 1 1 1 0 0 0 1 1 1 -5 6 1 1 1 0 0 1 1 1 1 -6 6 0 0 0 0 0 0 1 1 2 -1 9 2", "formula_coordinates": [591.0, 50.72, 194.89, 330.19, 92.09]}, {"formula_id": "formula_431", "formula_text": "f (t) = 1 (\u03c3 \u2212 1)!\u03c1 \u03c3 (t \u2212 \u03bc) \u03c3\u22121 e \u2212(t\u2212\u03bc) 1 \u03c1 (1)", "formula_coordinates": [606.0, 131.95, 496.22, 255.76, 22.78]}, {"formula_id": "formula_432", "formula_text": "T = argmax Ti P (T i |U ) = argmax Ti P (U |T i )P (T i ) P (U ) = argmax Ti P (U |T i ) (2)", "formula_coordinates": [607.0, 73.52, 238.3, 314.19, 23.67]}, {"formula_id": "formula_433", "formula_text": "P (U |T i ) = \u03a3 k=1,2 P (U |T i , C k )P (C k |T i ) ( 3 ) = 1 2 \u03a3 k=1,2 P (U |T i , C k ) ( 4)", "formula_coordinates": [607.0, 130.27, 352.9, 257.44, 36.06]}, {"formula_id": "formula_434", "formula_text": "P (U |T i ) = (1 \u2212 \u03b1)P (U |T i , C 1 ) + \u03b1P (U |T i , C 2 ) ( 5 )", "formula_coordinates": [607.0, 114.08, 441.46, 273.64, 10.0]}, {"formula_id": "formula_435", "formula_text": "i i i i T X T \u22c5 + \u22c5 \u2212 = \u03b1 \u03b1 ) , | ( ) , | ( 1 1 C t P C U P i i i T T = ) , cos( ) , | ( 2 X T T X i i C P = [ ] n i w w L 1 = T [ ] n s s L 1 = X cosine distance", "formula_coordinates": [608.0, 107.38, 90.54, 220.24, 89.66]}, {"formula_id": "formula_436", "formula_text": "t i = t b \u2212 start(T i )( 6 )", "formula_coordinates": [608.0, 175.27, 295.9, 212.44, 9.99]}, {"formula_id": "formula_437", "formula_text": "P (U |T i , C 1 ) = P (t i |T i , C 1 ) ( 7)", "formula_coordinates": [608.0, 154.39, 338.14, 233.32, 10.0]}, {"formula_id": "formula_438", "formula_text": "P (U |T i , C 2 ) = cos(T i , X)( 8)", "formula_coordinates": [608.0, 156.8, 545.74, 230.91, 10.0]}, {"formula_id": "formula_439", "formula_text": "I i,j = xixj P ij (x i , x j )log( P ij (x i , x j ) P i (x i )P j (x j ) (1)", "formula_coordinates": [617.0, 133.16, 490.94, 254.56, 26.39]}, {"formula_id": "formula_440", "formula_text": "i [j] = P (X j = T | X i = T ) = N ij N i (3)", "formula_coordinates": [619.0, 144.68, 145.58, 243.04, 23.15]}, {"formula_id": "formula_441", "formula_text": "\u2200X i \u2208T F T compute P (X i = k | P a(X i ) = j) using equation (2) \u2200X i \u2208T ET compute S i using equation (3) end", "formula_coordinates": [619.0, 44.55, 405.61, 292.5, 36.12]}, {"formula_id": "formula_442", "formula_text": "TE 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 EF 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 CTP 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 2 1 1 1 1 1 TVF 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 HGL 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 2 1 1 1 1 1 SI 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 1 1 1 1 1 1 1 2 1 1 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1", "formula_coordinates": [620.0, 45.75, 217.32, 336.56, 45.5]}, {"formula_id": "formula_443", "formula_text": "TE 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 LD 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 DE 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 1 1 1 1 1 2 TODP 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 1 1 1 1 2 2 DT 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 TDP 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 PPS 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 TO 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 1 1 1 1 1 1 THE 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 PF 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1", "formula_coordinates": [620.0, 45.75, 274.4, 336.56, 77.05]}, {"formula_id": "formula_444", "formula_text": "\u2212 \u2212 \u2212 \u2212 \u2212 P F 0.8824 \u2212 \u2212 \u2212 \u2212 T HE \u2212 0.9375 \u2212 \u2212 \u2212 T O \u2212 0.9375 \u2212 \u2212 \u2212 P P S \u2212 0.9375 \u2212 \u2212 \u2212 T DP \u2212 \u2212 0.9444 \u2212 \u2212 DT \u2212 \u2212 0.9444 \u2212 \u2212 T ODP \u2212 \u2212 \u2212 0.9474 \u2212 DE \u2212 \u2212 \u2212 0.9474 \u2212 LD \u2212 \u2212 \u2212 \u2212 0.9474", "formula_coordinates": [622.0, 129.32, 257.79, 170.57, 111.21]}, {"formula_id": "formula_445", "formula_text": "2 = 1 2t o 2to t=0 z (k) i,j (t) \u2212 z (k) i,j 2 , (1", "formula_coordinates": [628.0, 164.6, 531.84, 218.86, 30.36]}, {"formula_id": "formula_446", "formula_text": ")", "formula_coordinates": [628.0, 383.46, 542.42, 4.25, 8.78]}, {"formula_id": "formula_448", "formula_text": "(k) (i+1),j (t), z (k) (i\u22121),j (t + \u03c4 ))(8)", "formula_coordinates": [630.0, 196.88, 290.76, 190.83, 14.65]}, {"formula_id": "formula_450", "formula_text": "C i (a, b) = all\u03c4 R i (\u03c4 )\u03c8 (\u03c4 \u2212 b, a)d\u03c4, (10", "formula_coordinates": [630.0, 124.39, 480.82, 258.87, 18.3]}, {"formula_id": "formula_451", "formula_text": ")", "formula_coordinates": [630.0, 383.26, 481.22, 4.45, 8.78]}, {"formula_id": "formula_452", "formula_text": "\u03c8 (\u03c4, a) = 1 \u221a 2\u03c0a 3 1 \u2212 \u03c4 2 a 2 exp \u2212 \u03c4 2 2a 2", "formula_coordinates": [630.0, 125.59, 500.88, 170.53, 24.99]}, {"formula_id": "formula_453", "formula_text": "E MMDS (Y ) = 1 C N i=1 N j=i+1 (L ij \u2212 D ij ) 2 W (D ij ) ( 1 )", "formula_coordinates": [635.0, 113.0, 239.28, 274.71, 30.36]}, {"formula_id": "formula_454", "formula_text": "E Sammon (Y ) = 1 N i=1 N j=i+1 D ij N i=1 N j=i+1 (L ij \u2212 D ij ) 2 D ij (3)", "formula_coordinates": [635.0, 91.63, 437.52, 296.08, 30.36]}, {"formula_id": "formula_456", "formula_text": "E BMMDS (Y ) = N i=1 N j=i+1 d F (L ij , D ij ) = N i=1 N j=i+1 (F (L ij ) \u2212 F (D ij ) \u2212 (L ij \u2212 D ij )\u2207F (D ij )) (6)", "formula_coordinates": [637.0, 68.36, 99.48, 319.36, 66.6]}, {"formula_id": "formula_457", "formula_text": "EBMMDS(Y ) = N i=1 N j=i+1 d 2 F (Dij) dD 2 ij (Lij \u2212 Dij) 2 2! + d 3 F (Dij) dD 3 ij (Lij \u2212 Dij) 3 3! + d 4 F (Dij) dD 4 ij (Lij \u2212 Dij) 4 4! + \u2022 \u2022 \u2022 (7)", "formula_coordinates": [637.0, 47.29, 213.39, 340.42, 54.25]}, {"formula_id": "formula_458", "formula_text": "F (x) = x log x, x \u2208 R ++ ,( 8)", "formula_coordinates": [637.0, 168.8, 436.2, 218.91, 10.33]}, {"formula_id": "formula_459", "formula_text": "d 6 F (x) dx 6 1 x > 0 \u2212 1 x 2 < 0 2! x 3 > 0 \u2212 3! x 4 < 0 4! x 5 > 0 E ExtSammon (Y ) = N i=1 N j=i+1 (L ij \u2212 D ij ) 2 2!D ij \u2212 (L ij \u2212 D ij ) 3 3!D 2 ij + 2!(L ij \u2212 D ij )", "formula_coordinates": [637.0, 135.92, 561.37, 159.89, 31.35]}, {"formula_id": "formula_460", "formula_text": "E ExtSammon (Y ) = N i=1 N j=i+1 (L ij log L ij \u2212 D ij log D ij \u2212 (L ij \u2212 D ij )(log D ij + 1)) = N i=1 N j=i+1 L ij log L ij D ij \u2212 L ij + D ij (12", "formula_coordinates": [638.0, 43.47, 504.62, 341.9, 81.49]}, {"formula_id": "formula_461", "formula_text": "Mean Relative Distance = L h \u2212 D h D h , h = 1, 2, \u2022 \u2022 \u2022 , 40", "formula_coordinates": [640.0, 93.79, 86.74, 241.86, 24.39]}, {"formula_id": "formula_462", "formula_text": "d 2 F (x) dx 2 > 0, d 4 F (x) dx 4 > 0, \u2022 \u2022 \u2022 , d (2n) F (x) dx (2n) > 0 d 3 F (x) dx 3 < 0, d 5 F (x) dx 5 < 0, \u2022 \u2022 \u2022 , d (2n+1) F (x) dx (2n+1) < 0, n = 1, 2, 3, \u2022 \u2022 \u2022 (13)", "formula_coordinates": [640.0, 92.36, 525.37, 295.36, 32.27]}, {"formula_id": "formula_463", "formula_text": "E IS (Y ) = N i=1 N j=i+1 IS(L ij , D ij ) = N i=1 N j=i+1 L ij D ij \u2212 log L ij D ij \u2212 1 (14)", "formula_coordinates": [641.0, 76.75, 566.64, 310.96, 30.36]}, {"formula_id": "formula_464", "formula_text": "1 x \u2212 1 x 2 (\u22121) n (n\u22122)! x n\u22121 2 \u2212 log(x) 1 x 2 \u2212 2 x 3 (\u22121) n (n\u22121)! x n 3 1 x 2 x 3 \u2212 6 x 4 (\u22121) n (n)! x n+1 4 1 x 2 6 x 4 \u2212 24 x 5 (\u22121) n (n+1)! x n+2", "formula_coordinates": [642.0, 130.52, 81.99, 221.58, 51.73]}, {"formula_id": "formula_465", "formula_text": "1 x t t(t+1) x t+2", "formula_coordinates": [642.0, 176.6, 133.04, 48.23, 12.93]}, {"formula_id": "formula_466", "formula_text": "F (x) = 1 x E Reciprocal (Y ) = N i=1 N j=i+1 1 L ij \u2212 1 D ij \u2212 (L ij \u2212 D ij )(\u2212 1 D 2 ij ) = N i=1 N j=i+1 ( 1 L ij \u2212 2 D ij + L ij D 2 ij ) (15) F (x) = 1 x 2 E InverseQuadratic (Y ) = N i=1 N j=i+1 1 L 2 ij \u2212 1 D 2 ij \u2212 (L ij \u2212 D ij ) \u22122 D 3 ij = N i=1 N j=i+1 ( 1 L 2 ij \u2212 3 D 2 ij + 2L ij D 3 ij )(16", "formula_coordinates": [642.0, 41.84, 177.72, 345.88, 153.84]}, {"formula_id": "formula_468", "formula_text": "1 D ij Lij log L ij D ij \u2212 Lij + Dij Itakura-Saito eq(14) \u2212 log(x) 1 D 2 ij L ij D ij \u2212 log L ij D ij \u2212 1 Reciprocal eq(15) 1 x 1 D 3 ij 1 L ij \u2212 2 D ij + L ij D 2 ij", "formula_coordinates": [642.0, 104.46, 532.72, 253.68, 40.84]}, {"formula_id": "formula_469", "formula_text": "1 x 2 1 D 4 ij 1 L 2 ij \u2212 3 D 2 ij + 2L ij D 3 ij", "formula_coordinates": [642.0, 200.69, 572.96, 146.23, 14.74]}, {"formula_id": "formula_470", "formula_text": "J Stone = log yt\u2208D (y t \u2212 y) 2 yt\u2208Dt (y t \u2212\u1ef9) 2 = log V U (1)", "formula_coordinates": [647.0, 119.36, 332.28, 268.36, 27.84]}, {"formula_id": "formula_471", "formula_text": "V = (1 \u2212 \u03bb l )V + \u03bb l (y t \u2212 y) 2 U = (1 \u2212 \u03bb s )U + \u03bb s (y t \u2212\u1ef9) 2", "formula_coordinates": [647.0, 152.48, 514.92, 123.97, 26.52]}, {"formula_id": "formula_473", "formula_text": "\u03bc = E p G,\u03b8 [X] = d xp G,\u03b8 (x)dx (4)", "formula_coordinates": [650.0, 146.0, 445.57, 241.71, 15.06]}, {"formula_id": "formula_474", "formula_text": "F (x) = sup \u03b8 {x.\u03b8 \u2212 G(\u03b8)} (5", "formula_coordinates": [650.0, 161.6, 555.46, 221.86, 17.07]}, {"formula_id": "formula_475", "formula_text": ")", "formula_coordinates": [650.0, 383.46, 555.86, 4.25, 8.78]}, {"formula_id": "formula_476", "formula_text": "d", "formula_coordinates": [651.0, 114.8, 123.74, 5.18, 8.77]}, {"formula_id": "formula_477", "formula_text": "J Stone = log yt\u2208D (y t \u2212 y) 2 yt\u2208Dt (y t \u2212\u1ef9) 2 = log V U (6)", "formula_coordinates": [651.0, 119.36, 573.96, 268.36, 27.72]}, {"formula_id": "formula_478", "formula_text": "yt\u2208Dt (d F (\u1ef9, y t )) = log V L U L (8", "formula_coordinates": [652.0, 191.48, 179.17, 191.98, 24.71]}, {"formula_id": "formula_479", "formula_text": ")", "formula_coordinates": [652.0, 383.46, 185.9, 4.25, 8.78]}, {"formula_id": "formula_480", "formula_text": "V IS R =", "formula_coordinates": [652.0, 124.75, 304.92, 28.39, 12.72]}, {"formula_id": "formula_481", "formula_text": "\u0394w i = \u03b1 \u2212 x i y + x i y + (yx i \u2212 yx i y 2 1 V IS R \u2212 \u2212 x i y +x \u0129 y + (\u1ef9x i \u2212 yx \u0129 y 2 1 U IS R (9", "formula_coordinates": [652.0, 44.11, 396.22, 339.35, 34.38]}, {"formula_id": "formula_482", "formula_text": ")", "formula_coordinates": [652.0, 383.46, 421.82, 4.25, 8.78]}, {"formula_id": "formula_483", "formula_text": "f(v) = \u03b1 \u2022 \u0454 t (v) + (1 -\u03b1) \u2022 m(v) .", "formula_coordinates": [684.0, 136.26, 462.08, 123.46, 11.76]}, {"formula_id": "formula_484", "formula_text": "\u2022 \u03b1 + (1 -\u03b1) \u2022 1 -(n_sel_feat / n_feat), (2", "formula_coordinates": [686.0, 172.86, 324.08, 211.21, 11.04]}, {"formula_id": "formula_485", "formula_text": ")", "formula_coordinates": [686.0, 384.07, 326.16, 3.9, 8.96]}], "doi": "10.1016/j.cnsns.2006.12.011"}