{"title": "Compression of Generative Pre-trained Language Models via Quantization", "authors": "Chaofan Tao; Lu Hou; Wei Zhang; Lifeng Shang; Xin Jiang; Qun Liu; Ping Luo; Ngai Wong", "pub_date": "", "abstract": "The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity, and varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the stateof-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4\u00d7 and 13.4\u00d7 compression rates on GPT-2 and BART, respectively.", "sections": [{"heading": "Introduction", "text": "Transformer-based generative pre-trained language models (PLMs) show strong abilities of multitask and few-shot learning, and achieve remarkable performances on various tasks (Radford and Narasimhan, 2018;Brown et al., 2020;Lewis et al., 2020;Raffel et al., 2020;. However, they are usually expensive in terms of both computation and memory due to a large number of parameters, and the token-by-token generation process. Many methods have been proposed to compress PLMs, but mostly focus on understanding tasks like sentence classification with BERT (Lan et al., 2019;Sun et al., 2020b;Jiao et al., 2020;Shen et al., 2020;. Recent works try to compress GPT-2 using tensor decomposition (Edalati et al., 2021), and knowledge distillation (Song et al., 2020), but the compression ratio achieved is much smaller than that of BERT. Yet the underlying difficulty remains unclear.\nIn this paper, we firstly explore compressing generative PLMs by quantizing the parameters from full-precision to lower bits. We find that directly applying previous quantization methods designed for BERT or computer vision tasks to generative PLMs lead to poor performance. Figure 1 shows that the performance drops sharply as the weight bit-width decreases. To investigate the difficulty of quantizing generative PLMs, we find that the learned embeddings tend to be homogeneous and hard to distinguish due to the reduced capacity caused by quantization, while the weight distributions also vary significantly across different modules and different Transformer layers. These problems are further magnified due to the nature of sequential left-to-right prediction of generative PLMs, as the quantization error will accumulate across time.\nTo alleviate the above problems, we propose a token-level contrastive distillation to contrast on tokens and make the word embedding distinguishable. Besides, we propose a module-wise dynamic scaling for the quantizer to better adapt to different modules. Empirical results on language modeling, next utterance prediction and summarization show that compared to the full-precision baseline, our quantized GPT and BART (abbreviated as Quant-GPT and QuantBART) achieve comparable performance for 8/4-bit weight, and have only a slight drop for 2-bit weight, while being over 13\u00d7 smaller.\nQuantGPT also clearly outperforms previous GPT compression methods on language modeling.\nTo summarize, our main contributions are: 1) We find that generative PLMs are hard to quantize due to homogeneous word embedding and varied weight distribution. 2) We then propose the tokenlevel contrastive distillation and module-wise dynamic scaling, to make the word embedding more distinguishable and make quantizers adapt to different modules, respectively. 3) Empirical results on various tasks show the efficacy of our method.", "publication_ref": ["b29", "b3", "b23", "b30", "b22", "b34", "b21", "b31", "b9", "b32"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Difficulty of Qunatizing Generative", "text": "Pre-trained Language Models\nIn this section, we show that it is challenging to train a low-bit generative pre-trained model with conventional quantization approaches directly. Before diving into details, we first review the necessary backgrounds of quantization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Network Quantization", "text": "In this paper, we apply the quantization-aware training (Courbariaux et al., 2015) to generative PLMs. Specifically, denote the vectorized full-precision weight as w, each forward propagation first clips the weight by a positive clipping factor \u03b1, and then quantizes the clipped weight to b-bit as w q = \u03b1 \u2022 Q(clip(w, \u2212\u03b1, \u03b1)/\u03b1),\nwhere Q is the quantization function that maps each entry in clip(w, \u2212\u03b1, \u03b1)/\u03b1 to its closest quantized value in the set of uniform discrete values\n{\u22121,\u2212 n\u22121 n , \u2022 \u2022 \u2022 , \u2212 1 n , 0, 1 n , \u2022 \u2022 \u2022 , n\u22121 n , 1} with n = 2 b\u22121 \u2212 1.\nThen we compute the loss (w q ) with w q . During back propagation, we use the gradient with regard to the quantized weight \u2207 (w q ) as the Straight-Through-Estimator (Bengio et al., 2013) to update full-precision weights w due to the non-differentiability of Q(\u2022).\nA good clipping factor is expected to take the majority of full-precision weight into account via clipping, i.e., quantizing the range where data are densely distributed to reduce quantization error. To solve this problem, PACT (Choi et al., 2018) learns a parameterized clipping factor and achieves better results than setting a fixed clipping factor. Instead of learning the clipping factor, LSQ (Esser et al., 2020) learns the step size \u03b1/n, but requires a careful initialization and gradient update.\nIn practice, following previous works on BERT quantization Bai et al., 2021), we use layer-wise quantization (i.e., one clipping factor for elements in each weight matrix) for all weight matrices in the Transformer layers and rowwise quantization (i.e., one clipping factor for each word embedding) for the embedding layer. We use asymmetric uniform quantization for activations after self-attention and GeLU function whose elements are mostly positive, and symmetric uniform quantization for other activations. We do not quantize layer-normalization layers, skip connections, biases due to small computational overhead.", "publication_ref": ["b8", "b2", "b7", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Difficulty Analysis", "text": "We compare the following representative quantization methods including (i) LAQ  for BERT; (ii) PACT (Choi et al., 2018) and LSQ (Esser et al., 2020)) for computer vision tasks, to generative pre-trained model, GPT-2. Figure 1 shows the performance under different weight bitwidths, and the performance drops sharply as the bit-width decreases, especially for PACT and LSQ.\nIn the following, we study the potential reasons behind the difficulty of quantizing generative PLMs, by empirically investigating the properties of the word embedding and model parameters.\nHomogeneous Word Embedding. We first study the difficulty from the learned word embeddings of different models. In Figure 2, we visually compare the distributions of the word embeddings of the fullprecision and quantized models under the same scale. As can be seen, the word embeddings of the full-precision model are scattered distinguishable, while those in previous quantization methods PACT, LSQ and LAQ learn homogeneous word embeddings which are clustered and less distinguishable, especially for PACT and LSQ. We speculate this is caused by the sequential computation nature of GPT. Specifically, unlike BERT which computes the representation of all tokens in parallel, GPT computes each token in left-to-right order, and the quantization error incurred in the previous tokens will pass on to future tokens, making the learning signal noisier over time, and finally less informative word embeddings.\nA direct consequence of the homogeneous word embedding can be reflected in Figure 3. By comparing Figure 2 and Figure 3, we can find that the higher degree of homogeneity in the word embedding of a quantized model, the fewer dependencies among different tokens are kept.\nAs will be discussed in Section 3.1, we propose      Varied Distribution of Weights. Besides the learned word embeddings, we also investigate the distribution of the weights in the full-precision model. Figure 4 shows that the weight distributions of a 12-layer full-precision GPT-2 are highly skewed with outliers. This causes difficulty in estimating the clipping factor \u03b1 of the quantizer by heuristic methods, or even by PACT which learns the \u03b1 through gradient descent. Specifically, in PACT, the approximated gradient of \u03b1 only relies on the weights whose absolute values are larger than \u03b1. This solution ignores the effect of weights within [\u2212\u03b1, \u03b1] and depends heavily on the initialization of \u03b1. Figure 4 shows that an improper initialization together with the inaccurate gradient estimation of the clipping factor often make the learned \u03b1 of PACT too large, and can not provide fine resolution to the majority of weights within the clipping range. The quantization error accumulated over time makes this problem more severe. In this work, we re-parameterize the clipping factor to make the quantizer adaptive to each module in the Transformer layers, and consider both weights outside and inside the clipping range when estimating the gradient of the clipping factor.\nAs will be discussed in Section 3.2, we propose a module-wise dynamic scaling to reduce the clipping factor's sensitivity to initialization, and an improved gradient estimation that also considers the weights within [\u2212\u03b1, \u03b1]. Figure 4 shows that the clipping factor learned by our method gives finer resolutions to the majority of the weights.  For each token in the quantized network, we compute both (i) the token-level contrastive distillation loss where the positive tokens and negative tokens are selected from the full-precision teacher network; and (ii) the distillation loss on the logits. The embedding layer and all weights in the Transformer layers are quantized with the proposed module-dependent dynamic scaling.", "publication_ref": ["b7"], "figure_ref": ["fig_0", "fig_2", "fig_3", "fig_2", "fig_3", "fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "Proposed Method", "text": "Based on the observations in Section 2.2, we propose a quantization method which utilizes tokenlevel contrastive distillation to make the word embedding distinguishable (Section 3.1) and a modulewise dynamic scaling adjustment to learn better clipping factors (Section 3.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Token-level Contrastive Distillation", "text": "The proposed token-level contrastive distillation contrast among tokens instead of sequences sequence, to learn distinguishable representations for each token. Inspired by Baevski et al. (2020), which uses in-utterance representation at different positions of the same utterance as negatives for speech feature learning, for each token of the quantized network, we use the representation of the same token from the full-precision teacher network as its positive, while representations of other tokens in the same sequence as negatives (Figure 5). Inspired by  which uses a momentum encoder for more consistent representation, we build a memory bank to store momentum token representations from the quantized network. When computing the contrastive distillation loss, we load the representations of negative samples from the memory bank with cheap indexing operations. Specifically, we use superscripts s and t to denote the quantized student network and fullprecision teacher network, respectively. Denote the length-n input sequence of tokens as (t 1 , t 2 , \u2022 \u2022 \u2022 , t n ). For the i-th token t i , suppose its hidden states of the last Transformer layer from the quantized and full-precision network are linearly projected to (h s i , h t i ) \u2208 R d , and q s i is the smoothed representation of h s i in the memory bank. Denote S i as the indices of the sampled negatives for token i, the token-level contrastive distillation loss for the length-n sequence can be formulated as\nL cont = \u2212 n i=1 log exp(s(q s t i , h t t i )/\u03c4 ) j\u2208S i exp(s(q s t i , h t t j )/\u03c4 ) ,(2)\nwhere s(x, y) = x y x y computes the cosine similarity, and \u03c4 is a fixed temperature parameter.\nThen we update the representation of token t i in the memory bank with the moving-average of token representations from the quantized network:\nq s t i \u2190 mq s t i + (1 \u2212 m)h s t i ,(3)\nwhere m \u2208 [0, 1) it the momentum coefficient that controls the smoothness of the token represenation. Besides, we use an additional distillation loss L dist over the logits. For the i-th token t i , suppose the logits of the quantized and full-precision network are z s t i , z t t i \u2208 R |V | , where |V | is the vocabulary size. L dist is computed with the soft crossentropy loss:\nL dist = \u2212 n i=1 z t t i log(z s t i ).(4)\nThus the total training loss is\nL = \u03bbL cont + L dist ,(5)\nwhere \u03bb is a trade-off factor set as 0.1 by default. Intuitively, for each token in the quantized network, L dist only encourages it to mimic its corresponding token of the teacher network, while L cont not only pulls it close to its positive, but also pushes it away from its negatives. In this way, L cont helps the student to capture more information from the teacher's representation, as is also theoretically discussed in Tian et al. (2019).\nThe proposed token-level contrastive distillation is crucial to the performance, and outperforms the sequence-level counterpart (as will be shown empirically in Section 5.1.1). We conjecture this is because (i) token-level contrast alleviates the problem of homogeneous word embedding (Figure 2) in the low-bit quantization; and (ii) similar to speech, the order of natural language is also sequential instead of spatial like images; and (iii) the self-attention mechanism allows other tokens to learn representations contextualized on the studied token, and these in-sequence negatives are harder than those from in-batch sequences, allowing more efficient representation learning.", "publication_ref": ["b35"], "figure_ref": ["fig_6", "fig_2"], "table_ref": []}, {"heading": "Module-dependent Dynamic Scaling", "text": "Based on the observation of varied weight distribution in Section 2.2, we propose a simple-yeteffective dynamic scaling according to the statistics of each module weight. Specifically, instead of directly learning the original clipping factor \u03b1 as PACT, we turn to learn a new scaling factor \u03b3, which is multiplied with the average weight magnitude w 1 n to get clipping factor \u03b1:\n\u03b1 = \u03b3 \u2022 w 1 n ,(6)\nwhere \u2022 1 denotes 1 norm. The scaling \u03b3 is initialized as 1, which not only eases the initialization but also ensures the initial clipping factor \u03b1 does not deviate far from the full-precision weights, regardless of the diversity of weight distribution. Besides, we also design a more accurate gradient estimation of the scaling factor than PACT (Choi et al., 2018). Previous PACT only back propagates through weights whose absolute values are larger the clipping factor (i.e. |w| \u2265 \u03b1). Instead, we also consider the weights inside the clipping range (i.e. |w| < \u03b1) as:\n\u2202 \u2202\u03b3 = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u2202 \u2202wq Q(u) w 1 n , w < \u2212\u03b1 \u2202 \u2202wq [\u2212 w \u03b1 +Q(u)] w 1 n , \u2212\u03b1 \u2264 w \u2264 \u03b1 \u2202 \u2202wq Q(u) w 1 n , w > \u03b1 ,(7)\nwhere is the total training loss and u = clip(w, \u2212\u03b1, \u03b1)/\u03b1 in Eq. (1). The detailed derivation can be found in Appendix A.\nIntuitively, the update of clipping factor should be influenced by both weights outside and inside [\u2212\u03b1, \u03b1], since \u03b1 controls the quantization error of both, i.e., a large clipping factor results in small quantization error for weights outside [\u2212\u03b1, \u03b1], while large error for weights inside. Our new estimation of the gradient of \u03b3 in Eq. ( 7) considers weights both outside and inside [\u2212\u03b1, \u03b1]. Additionally, the proposed scaling is less sensitive to the varied distribution of weight than PACT, since the gradient of scaling \u2202 \u2202\u03b3 is proportional to the average weight magnitude w 1 n .", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "Tasks and Models. In this section, we evaluate the efficacy of our proposed quantization method on three kinds of generative tasks on two kinds of generative pre-training models. Specifically, we perform the proposed quantization approach on language modeling and next utterance prediction tasks on GPT-2 (Radford and Narasimhan, 2018), and abstractive summarization using BART (Lewis et al., 2020), and call the resultant models Quant-GPT and QuantBART. The token-level contrastive distillation is performed on the hidden states of the last layer of GPT-2 or the BART decoder. More details about the datasets and model architectures can be found in Appendix B.1 and B.2.\nImplementation Details. For each downstream task with our proposed method, we first fine-tune a full-precision network using the pre-trained checkpoint from huggingface 1 for both GPT-2 and BART. Then we use this fine-tuned network as the fullprecision teacher network and to initialize the quantized student network. We train each task with 8 V100 GPUs based on the Pytorch framework. The detailed hyper-parameters for each task are available in Appendix B.3.\nCompared Methods. Since there are very few attempts to compress generative PLMs, we selfimplement three baseline quantization methods PACT (Choi et al., 2018), LSQ (Esser et al., 2020) and LAQ (Hou and Kwok, 2018) for comparison. Details about these methods are in Appendix B.4.", "publication_ref": ["b29", "b23", "b7", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Language Modeling", "text": "The task of language modeling is to predict the probability distribution over a sequence of words.  For language modeling, we experiment on Wiki-Text2 (Merity et al., 2016), Penn Treebank (PTB) (Mikolov and Zweig, 2012) and WikiText103 (Merity et al., 2016). We use perplexity (PPL) to evaluate the performance for language modeling.\nComparison with the Full-precision Model.\nFrom Table 1, the performance of the proposed method with 8-bit weight is comparable to the fullprecision counterpart on PTB and WikiText103, while drops slightly on WikiText2. A slightly more severe performance drop is observed as the bitwidth decreases from 8 to 4, with a drop of around 1 PPL point on WikiText2 and WikiText103, and less than 0.1 PPL point on PTB. When the bit-width of weight further goes down to 2, our method has an average of 2 PPL points drop, but achieves 14.4\u00d7 model size reduction.\nComparison with Other Quantization Methods.\nFrom Table 1, our method outperforms PACT, LSQ and LAQ for all bit-widths and tasks. As the bitwidth decreases from 8 to 4, the PPL of LSQ greatly increases, with the average PPL of LSQ increasing by over 5 times. As the bit-width further decreases to 2, both LSQ and PACT fail on all datasets, despite their good performance on understanding tasks on BERT (Bai et al., 2021). We conjecture it is because though both PACT and LSQ have learnable parameters, the accumulated quantization error of generative PLMs makes the updates of these parameters by gradient descent less stable. On the other hand, the proposed module-wise dynamic scaling alleviates the problem.", "publication_ref": ["b25", "b26", "b25", "b1"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Comparison with Other Compression Methods.", "text": "In against recent GPT-2 compression methods, including tensor decomposition method KnGPT2 (Edalati et al., 2021), as well as distillation methods Distil-GPT2 and LightPAFF (Song et al., 2020). From the comparison, our method outperforms the others in terms of model size and performance, even when weights are compressed to only 2 bits.", "publication_ref": ["b9", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Next Utterance Prediction", "text": "The task of next utterance prediction predicts the next utterance given the dialogue context. It tests the language understanding ability of generative models. For this task, we use a large-scale dialogue dataset, Persona-Chat (Zhang et al., 2018).\nFrom Table 1, all quantization methods incur a clear performance drop compared to the fullprecision baseline, even in the 8-bit setting. As the quantization becomes more aggressive, i.e., the bit-width gets smaller, the performance of PACT and LAQ decrease more significantly than ours. In particular, LSQ diverges for 2-bit weight and its accuracy is only 5%, which is no better than a random guess as there are 20 classes.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Abstractive Summarization", "text": "Abstractive summarization aims at generating a terse summary that captures the main ideas of the source article. We experiment on XSum (Narayan et al., 2018), whose ground-truth summarizations are highly abstractive and are challenging for many extractive strategies. ROUGE 1, 2, L are used to evaluate the performance of this task.  Table 3 shows the results of the abstractive summarization. As can be seen, our method constantly outperforms other methods again with a clear margin. Example generated summarizations of different methods in Appendix C.2 show that the summaries generated by QuantBART are logical and terse, while those from PACT have repeated texts.\nMethod #Bits (W-E-A) Size (MB)(\u2193) XSum Metric R1(", "publication_ref": ["b27"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Discussion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ablation on Contrastive Learning", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Choices of Negative Sampling", "text": "As shown in Figure 6, we ablate on how to choose negative samples in contrastive learning. Specifically, we compare our method with variants of token-level contrastive learning, which select negative samples of each token from (a)    ent ones from the teacher network (in-batch). Representation of a sequence is defined as the mean of representations of all tokens in the sequence. From Table 4, \"fp+quan.\" and \"quan. only\" performs worse than QuantGPT, which uses fullprecision representations of other tokens as negative samples. This indicates that noisy representations of tokens from the not-fully-trained quantized network may not be sufficient. \"global\" performs even worse, which we conjecture is because, for one token, negative tokens chosen from the same sequence are contextually related to it and more informative than random tokens. \"in-batch\" performs worse than all token-level variants, which may be because generative tasks make predictions in a token-wise manner and rely heavily in finergrained token-wise representations. Interestingly, contrary to in-batch negative sampling in computer vision , we find that reducing the number of negative samples by reducing the batch size from 32 to 16 slightly improves performance.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Number of Negative Samples", "text": "In Figure 7, we plot the PPL of 2-bit QuantGPT on the PTB dataset, with varying number of negative samples. We plot the mean results with standard    deviations from 5 independent runs. As can be seen, the performance improves and converges gradually as the number of negative samples increases. Figure 7 also shows that using the movingaverage representations (q s t i in Eq. ( 3)) of negative samples in the memory bank has better performance than using the immediate representations (h s t i in Eq. (3)), because of a smoother and more consistent representation of tokens.", "publication_ref": [], "figure_ref": ["fig_8", "fig_8"], "table_ref": []}, {"heading": "Training Cost of the Contrastive Loss", "text": "In Table 5, we report the training speed and memory consumption of training the GPT-2 model on the PTB dataset with and without the proposed token-level contrastive loss. Batch size is set as 4 per device, which can be increased by using GPUs with larger memory or reducing the sequence length of samples. As can be seen, with the proposed token-level contrastive loss, the performance clearly improves with only slightly slower training speed and more memory consumption.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Representations for the Contrastive Loss", "text": "In Table 6, we compare the different representations to perform the contrastive loss. The \"decoderlast\"( resp. \"decoder-first\") denotes performing the proposed token-level contrastive loss on the hidden states from the last decoder layer (resp. first decoder layer) followed by a linear transformation.\nFrom Table 6, \"decoder-last\" performs better than \"decoder-first\". A possible reason is that the hidden states of the last decoder blocks contain rich information from all previous layers (Xiong et al., 2020). Since the experiments of abstractive summarization are conducted on BART, which has both encoder and decoder layers, we also study the contrastive loss on the \"encoder-last\" and \"encoderfirst\". In the ablation on the encoder, the contrastive loss L cont are computed on the source input (articles), instead of target input (summaries). From Table 6, \"decoder-last\" also has better ROUGE 1, 2, L values than other counterparts.", "publication_ref": ["b37"], "figure_ref": [], "table_ref": []}, {"heading": "Ablation on Dynamic Scaling", "text": "Figure 8 shows the learned scaling \u03b3 of different modules in the 2-bit GPT-2 model. As can be seen, the scalings of different modules vary a lot, verifying the need for module-wise dynamic scaling.\nIn addition, we investigate the effect of the proposed dynamic scaling and the new estimation of the gradient in Eq. ( 7) with two variants: 1) L dist only which removes the token-level contrastive learning; and 2) Ours with PACT which removes the contrastive learning, and estimates the gradient with PACT which only considers the weights whose absolute values are larger than the clipping factor \u03b1. As shown in Table 7, the performance gets worse without contrastive learning to learn the distinguishable representations of tokens. The performance drops significantly when using PACT to estimate the gradient of the proposed scaling, especially for the WikiText103 dataset, verifying the efficacy of the new gradient estimation.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "Related Work", "text": "Compression of Generative Pre-trained Language Models. Some early explorations compress the generative pre-trained language models. KnGPT2 (Edalati et al., 2021) applies the Kronecker decomposition to compress the GPT. Dis-tilGPT2 2 distills a 12-layer GPT-2 to a 6-layer one, which is twice as fast during inference. Light-PAFF (Song et al., 2020) proposes a distillation approach that the training loss is a combination of a maximum likelihood loss of the student model, and the KL divergence between the output of teacher and student models. SpAtten (Wang et al., 2021) proposes a sparse model with algorithm and architecture co-design, which removes uninformative tokens and attention heads. Compared with these methods, we not only study the difficulties of compression from the properties of generative tasks, Table 7: Ablation study on the learning of the clipping factor with 2-bit GPT-2 on the language modeling task.\nbut also study both decoder and encoder-decoder generative models.\nQuantization of Pre-trained Language Models. Quantization compresses a model by representing the 32-bit floating-point parameter with a low-bit representation, and has been widely used in various domains as it does not require designing a new model architecture. There have been many attempts to quantize task-specific BERT models (Zafrir et al., 2019;Shen et al., 2020;Zadeh et al., 2020) with only negligible performance drop on natural language understanding tasks. Recent works Bai et al., 2021) even push the weight bit-width down to as low as 1-bit. Despite the success of these approaches for BERT models, attempts to quantize generative PLMs are scarce, and the underlying difficulty remains unclear.\nContrastive Learning. Contrastive learning aims at pushing the representations of similar samples together while pulling those of dissimilar ones apart. and is widely used for large-scale selfsupervised learning in various domains Sun et al., 2020a;Baevski et al., 2020;Huang et al., 2022), and multi-modal learning (Radford et al., 2021;Jia et al., 2021). SimCLR  directly uses other in-batch samples as negatives, and sufficient large batch size is required to work well. MoCo ) maintains a large number of negative samples in a queue and uses a moving average key encoder to improve consistency. Contrastive learning without negative samples is also proposed in BYOL (Grill et al., 2020) and SimSiam (Chen and He, 2021). Contrastive representation distillation (Tian et al., 2019) distills the knowledge from the teacher network to the student network by maximizing the mutual information between them. The closest work with our token-level contrastive distillation is Wav2vec 2.0 (Baevski et al., 2020), which use in-utterance representations at different positions as negatives in speech learning. Besides the difference in the modality and tasks, our method also differs from theirs in (1) Model: We quantize the model parameters and activations while they do not; (2) Representation: For each sample, we use the output of the full-precision and the quantized networks as its two views, while they use the quantized and the contextualized representation.\n(3) Loss: We calculate loss over all tokens in an auto-regressive manner, while they only calculate over the masked tokens non-autoregressively.", "publication_ref": ["b9", "b32", "b36", "b39", "b31", "b38", "b1", "b33", "b19", "b20", "b12", "b6", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This paper studies low-bit quantization of generative PLMs. We find that the difficulty of quantizing generative PLMs lies in homogeneous word embedding and varied distribution of weights. To alleviate the two problems, we propose token-level contrastive learning to learn more distinguishable token emebeddings, as well as a module-dependent dynamic scaling for more accurate quantization. Extensive experiments on language modeling, next utterance prediction and abstractive summarization demonstrate the efficacy of our proposed method. We hope our work sheds a light on the compression of generative PLMs in future exploration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Derivation of Gradient of Dynamic Scaling", "text": "In this section, we provide the derivation of the gradient of the proposed dynamic scaling \u03b3. The quantization in the forward process can be written as\n\u03b1 = w 1 n \u03b3, u = clip(w, \u2212\u03b1, +\u03b1)/\u03b1, w q = Q(u)\u03b1,\nwhere Q(\u2022) is an uniform quantization function as described in Section 2.2. Based on the chain rule, the gradient of scaling \u03b3 w.r.t. the training loss function is:\n\u2202 \u2202\u03b3 = \u2202 \u2202w q [ \u2202w q \u2202Q(u) \u2202Q(u) \u2202\u03b1 \u2202\u03b1 \u2202\u03b3 + \u2202w q \u2202\u03b1 \u2202\u03b1 \u2202\u03b3 ] = \u2202 \u2202w q [\u03b1 \u2202Q(u) \u2202\u03b1 w 1 n + Q(u) w 1 n ] = \u2202 \u2202w q [\u03b1 \u2202Q(u) \u2202\u03b1 + Q(u)] w 1 n .(8)\nWe use straight through estimator (STE) to estimate the gradient of uniform quantizer Q(\u2022), i.e., \u2200i, \u2202Q(u i ) \u2202u i = 1 . Thus the gradient \u2202Q(u) \u2202\u03b1 can be written as:\n\u2202Q(u) \u2202\u03b1 = \u2202Q(u) \u2202u \u2202u \u2202\u03b1 = \uf8f1 \uf8f2 \uf8f3 0, w \u2264 \u2212\u03b1 \u2212 w \u03b1 2 , \u2212\u03b1 < w < \u03b1 0, w \u2265 \u03b1 .(9)\nBy combining Eq. (8) and Eq. (9), we get\n\u2202 \u2202\u03b3 = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u2202 \u2202wq Q(u) w 1 n , w \u2264 \u2212\u03b1 \u2202 \u2202wq [\u2212 w \u03b1 + Q(u)] w 1 n , \u2212\u03b1 < w < \u03b1 \u2202 \u2202wq Q(u) w 1 n , w \u2265 \u03b1\nwhere \u2202 \u2202\u03b3 considers both the weight inside and outside the clipping value, and proportional to the weight magnitude w 1 n .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B More Experimental Settings B.1 Datasets", "text": "The train/val/test splits for different datasets are shown on Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "B.2 Model Architectures", "text": "GPT-2. The vocabulary size of GPT-2 is 50527. We use GPT-2-small with 12 decoder layers and hidden state dimension as 768, for experiments  in Sections 2.2, 4 and 5. GeLU (Hendrycks and Gimpel, 2016) is used as the activation function. In the experiments of Appendix C.1, we adopt GPT-2-base with 24 decoder layers and hidden state dimension as 1024, to evaluate the quantization ability on larger models.\nBART. The vocabulary size of BART is 50265. We use BART-base with 6 encoder layers, 6 decoder layers and hidden state dimension as 768 for experiments in Section 4. In the experiments of Appendix C.1, we adopt BART-large with 12 encoder layers, 12 decoder layers and hidden state dimension 1024, to evaluate the quantization ability on larger models.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Hyperparameters", "text": "Language Modeling. The sequence length is 512. The learning rate is initialized to 0.0005 (resp. 0.001) for the GPT-2 backbone parameters (resp. clipping factor \u03b3) and then linearly decays to 0. The number of negative samples in each sequence is 64 for the PTB dataset, and 32 for the WikiText2 and WikiText103. The temperature \u03c4 and momentum coefficient m is 0.1 and 0.5 respectively. We train with the AdamW optimizer (Loshchilov and Hutter, 2017) with batch size 32. The training epochs for WikiText2, PTB and WikiText103 are set as 80, 120, 8, respectively.\nNext Utterance Prediction. The sequence length is 512. The learning rate is initialized to 0.0005 (resp. 0.001) for the GPT-2 backbone parameters (resp. clipping factor \u03b3) and then linearly decays to 0. The number of negative samples in each sequence is 32. The temperature \u03c4 and momentum coefficient m is 0.1 and 0.5, respectively. We train with the AdamW optimizer with batch size 16, for a total of 2 epochs.\nAbstractive Summarization. We set the length of the source sequence (articles) as 512, and pad the target sequence (summaries) to maximum length. Table 9: Ablation study on larger models. We report the results on 24-layer GPT-2 and 24-layer BART.\nis initialized to 0.0002 (resp. 0.001) for the BART backbone parameters (resp. clipping factor \u03b3) and then linearly decays to 0. The number of negative samples is 32. The temperature \u03c4 and momentum coefficient m is 0.1 and 0.5, respectively. We train with the AdamW optimizer with batch size 128, for a total of 8 epochs.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "B.4 Description of the Compared Methods", "text": "PACT. PACT (Choi et al., 2018) learns a learnable clipping factor for each module by gradient descent. To make the quantization more accurate, we adopt a flexible variant of the original PACT, with different positive and negative clipping factors [\u2212\u03b1 neg , \u03b1 pos ], where both \u03b1 neg and \u03b1 pos are initialized as 2.5. Esser et al., 2020) learns the step-size of quantizer for each module by gradient descent. We use the recommended initialization strategy of the step size as (Esser et al., 2020).", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "LSQ. LSQ (", "text": "LAQ. LAQ (Hou et al., 2017;Hou and Kwok, 2018) is a loss-aware quantization method that views quantization as an optimization problem and solve it via proximal Newton algorithm. We use the approximate solver in (Hou and Kwok, 2018) to compute the quantized weights before each forward propagation.\nFor the self-implemented methods PACT, LSQ and LAQ, we adopt the commonly-used distillation loss adopted in (Hinton et al., 2015;Jiao et al., 2020). Note that these methods are only used for weights and embeddings, while the activations of these methods follow the same setting as our proposed method in Section 2.1. We also tried using the original language modeling loss w.r.t. the ground-truth labels, and distillation loss over the the attention as (Jiao et al., 2020). However, these two losses worsens the performance on all three methods.", "publication_ref": ["b18", "b17", "b17", "b15", "b21", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "B.5 Frameworks of Double-head GPT-2 and BART", "text": "Since we adopt double-head GPT-2 and BART for next utterance prediction and abstractive summarization, the frameworks for these tasks are slightly modified from that on language modeling due to the difference of tasks. In Figure 9 and 10, we illustrate the framework for double-head GPT-2 and BART, respectively. In the double-head GPT-2, we also quantize the final linear layer in the output head.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C More Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Performance of Larger Models", "text": "In Table 9, we experiment with GPT-base and BART-large, which both have 24 Transformer layers. For all bit-widths, the training of our method converges successfully without gradient exploding/vanishing problems. QuantGPT outperforms PACT by a large margin in all tasks, especially for 2-bit weight. Our quantization method on larger models also has better performance than that on 12-layer GPT-2 and 12-layer BART in Section 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Examples of Summarizations", "text": "In Table 10, we provide the example summarizations on the XSum dataset. By comparing the articles, references and generations, the generated summaries by our quantized model are more logical and terse than PACT, LSQ and LAQ, which face problems of homogeneous word embeddings to some extent as discussed in Section 2.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 More Visualizations for the Token Representations", "text": "In Figure 11, we provide the visualizations of token representations on more samples. The observations \"Our director of investigations will be reviewing the contents of the programme to ascertain if there are any issues of police conduct which may need further investigation.\" The Police Service of Northern Ireland has said that in the two cases identified in the programme, investigations were robust and all information available at the time was followed. The Health and Social Care Board has said that new guidelines are in place and stress that no children have gone missing since 2014. BBC Spotlight's investigation is now available on BBC iPlayer.\nReference: An urgent inquiry is needed into separated children who have gone missing from care, the Social Democratic and Labour Party has said.\nPACT: TheTheAAATheTheTheAnAnAnTheThe an an an been been been jailed.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "LSQ:", "text": "The SDLP has called for an urgent inquiry into the welfare of unaccompanied children in Northern Ireland.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LAQ:", "text": "The SDLP has called for \"urgent inquiry and investigation\" into the handling of unaccompanied children in Northern Ireland.\nOurs: The SDLP is calling for an urgent inquiry and investigation into the disappearance of unaccompanied young people.\nArticle: The dairies operation, which processes and distributes milk, is being sold to Germany's Mueller for \u00c2\u00a380m. It comes as profits at the UK's largest dairy food company fell 95% to \u00c2\u00a3900,000 in the six months to September. Dairy Crest processes and delivers around 1.3 billion litres of milk a year for retailers and homes. Dairy Crest said in a statement that the deal was in the best interests of consumers, customers and dairy farmers. The dairies business accounts for about 70% of the company's revenues, which rose 1% to \u00c2\u00a3682.1m during the six months. After the sale, which still needs shareholder approval and could take several months, Dairy Crest will focus on its profitable cheese and spreads operations. There are about 14,000 dairy farmers in the UK, producing 3.3 million litres a day. However, with milk prices having fallen, there has been much debate about whether the economics of the industry are sustainable. Investors approved of the Dairy Crest's decision to get out of a loss-making sector, sending its shares 10% higher in morning trading on Thursday. Muller said the deal would lead to lower costs and larger exports of dairy products made in the UK. Ronald Kers, chief executive of Muller UK & Ireland, said: \"We are concerned that the dynamics of the UK fresh milk market are unsustainable for dairy processors in the mid to long term and this acquisition will allow us to reduce our costs, increase our efficiencies and invest in the future.\" Under the deal, Mueller's UK division -Muller Wiseman Dairies -will take over factories at Foston, in Derbyshire, Chadwell Heath, in Essex, and Severnside, near Gloucester. The deal also includes the Hanworth glass bottling site in Middlesex, where Dairy Crest is consulting with employees on the site's future, and 72 depots. Muller bought Robert Wiseman in 2012.\nReference: Dairy Crest, maker of Cathedral City cheese and Country Life butter, has announced a big slump in profits and the sale of its milk business.\nPACT: More than than more more more than more than than than to be be be will will will be be are are are be be to the.\nLSQ: Dairy Crest is to sell its Dairies business to a German company for an undisclosed sum.\nLAQ: Dairy giant Dairy Crest is to sell its UK business to a German company for an undisclosed sum.\nOurs: Dairy Crest, the world's largest dairy producer, is to sell its UK operations to a German firm.  are similar to those in Section 2.2. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work is supported in part by the General Research Fund (GRF) project 17206020, and in part by ACCESS, AI Chip Center for Emerging Smart Systems, Hong Kong SAR.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations", "journal": "", "year": "", "authors": "Alexei Baevski; Henry Zhou"}, {"ref_id": "b1", "title": "Binarybert: Pushing the limit of bert quantization", "journal": "", "year": "2021", "authors": "Haoli Bai; Wei Zhang; Lu Hou; Lifeng Shang; Jing Jin; Xin Jiang; Qun Liu; Michael Lyu; Irwin King"}, {"ref_id": "b2", "title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "journal": "", "year": "2013", "authors": "Yoshua Bengio; Nicholas L\u00e9onard; Aaron Courville"}, {"ref_id": "b3", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "Benjamin Tom B Brown; Nick Mann; Melanie Ryder; Jared Subbiah; Prafulla Kaplan; Arvind Dhariwal; Pranav Neelakantan; Girish Shyam; Amanda Sastry;  Askell"}, {"ref_id": "b4", "title": "Litegt: Efficient and lightweight graph transformers", "journal": "", "year": "2021", "authors": "Cong Chen; Chaofan Tao; Ngai Wong"}, {"ref_id": "b5", "title": "A simple framework for contrastive learning of visual representations", "journal": "", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b6", "title": "Exploring simple siamese representation learning", "journal": "", "year": "2021", "authors": "Xinlei Chen; Kaiming He"}, {"ref_id": "b7", "title": "Pact: Parameterized clipping activation for quantized neural networks", "journal": "", "year": "2018", "authors": "Jungwook Choi; Zhuo Wang; Swagath Venkataramani; I-Jen Pierce; Vijayalakshmi Chuang; Kailash Srinivasan;  Gopalakrishnan"}, {"ref_id": "b8", "title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "journal": "", "year": "2015", "authors": "Matthieu Courbariaux; Yoshua Bengio; Jean-Pierre David"}, {"ref_id": "b9", "title": "Kronecker decomposition for gpt compression", "journal": "", "year": "2021", "authors": "Ali Edalati; Marzieh Tahaei; Ahmad Rashid; Vahid Partovi Nia; James J Clark; Mehdi Rezagholizadeh"}, {"ref_id": "b10", "title": "", "journal": "", "year": "", "authors": "K Steven; Jeffrey L Esser; Deepika Mckinstry; Rathinakumar Bablani;  Appuswamy; S Dharmendra"}, {"ref_id": "b11", "title": "Learned step size quantization", "journal": "", "year": "2020", "authors": " Modha"}, {"ref_id": "b12", "title": "Bootstrap your own latent: A new approach to self-supervised learning", "journal": "", "year": "2020", "authors": "Jean-Bastien Grill; Florian Strub; Florent Altch\u00e9; Corentin Tallec; H Pierre; Elena Richemond; Carl Buchatskaya; Bernardo Doersch; Zhaohan Daniel Avila Pires; Mohammad Gheshlaghi Guo;  Azar"}, {"ref_id": "b13", "title": "Momentum contrast for unsupervised visual representation learning", "journal": "", "year": "2020", "authors": "Kaiming He; Haoqi Fan; Yuxin Wu; Saining Xie; Ross Girshick"}, {"ref_id": "b14", "title": "Gaussian error linear units (gelus)", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b15", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b16", "title": "Dynabert: Dynamic bert with adaptive width and depth", "journal": "", "year": "2020", "authors": "Lu Hou; Zhiqi Huang; Lifeng Shang; Xin Jiang; Xiao Chen; Qun Liu"}, {"ref_id": "b17", "title": "Loss-aware weight quantization of deep networks", "journal": "", "year": "2018", "authors": "Lu Hou; James T Kwok"}, {"ref_id": "b18", "title": "Loss-aware binarization of deep networks", "journal": "", "year": "2017", "authors": "Lu Hou; Yao Quanming; James T Kwok"}, {"ref_id": "b19", "title": "Spiral: Self-supervised perturbation-invariant representation learning for speech pre-training", "journal": "", "year": "2022", "authors": "Wenyong Huang; Zhenhe Zhang; Yu Ting Yeung; Xin Jiang; Qun Liu"}, {"ref_id": "b20", "title": "Scaling up visual and vision-language representation learning with noisy text supervision", "journal": "", "year": "2021", "authors": "Chao Jia; Yinfei Yang; Ye Xia; Yi-Ting Chen; Zarana Parekh; Hieu Pham; V Quoc; Yunhsuan Le; Zhen Sung; Tom Li;  Duerig"}, {"ref_id": "b21", "title": "Tinybert: Distilling bert for natural language understanding", "journal": "", "year": "2020", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"ref_id": "b22", "title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "2019", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b23", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b24", "title": "", "journal": "", "year": "2017", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b25", "title": "Pointer sentinel mixture models", "journal": "", "year": "2016", "authors": "Stephen Merity; Caiming Xiong; James Bradbury; Richard Socher"}, {"ref_id": "b26", "title": "Context dependent recurrent neural network language model", "journal": "IEEE", "year": "2012", "authors": "Tomas Mikolov; Geoffrey Zweig"}, {"ref_id": "b27", "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization", "journal": "", "year": "2018", "authors": "Shashi Narayan; B Shay; Mirella Cohen;  Lapata"}, {"ref_id": "b28", "title": "Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision", "journal": "", "year": "", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal"}, {"ref_id": "b29", "title": "Improving language understanding by generative pretraining", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"ref_id": "b30", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b31", "title": "Q-bert: Hessian based ultra low precision quantization of bert", "journal": "", "year": "2020", "authors": "Sheng Shen; Zhen Dong; Jiayu Ye; Linjian Ma; Zhewei Yao; Amir Gholami; W Michael; Kurt Mahoney;  Keutzer"}, {"ref_id": "b32", "title": "Lightpaff: A two-stage distillation framework for pre-training and fine-tuning", "journal": "", "year": "2020", "authors": "Kaitao Song; Hao Sun; Xu Tan; Tao Qin; Jianfeng Lu; Hongzhi Liu; Tie-Yan Liu"}, {"ref_id": "b33", "title": "Contrastive distillation on intermediate representations for language model compression", "journal": "", "year": "2020", "authors": "Siqi Sun; Zhe Gan; Yuwei Fang; Yu Cheng; Shuohang Wang; Jingjing Liu"}, {"ref_id": "b34", "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices", "journal": "", "year": "2020", "authors": "Zhiqing Sun; Hongkun Yu; Xiaodan Song; Renjie Liu; Yiming Yang; Denny Zhou"}, {"ref_id": "b35", "title": "Contrastive representation distillation", "journal": "", "year": "2019", "authors": "Yonglong Tian; Dilip Krishnan; Phillip Isola"}, {"ref_id": "b36", "title": "Spatten: Efficient sparse attention architecture with cascade token and head pruning", "journal": "IEEE", "year": "2021", "authors": "Hanrui Wang; Zhekai Zhang; Song Han"}, {"ref_id": "b37", "title": "On layer normalization in the transformer architecture", "journal": "PMLR", "year": "2020", "authors": "Ruibin Xiong; Yunchang Yang; Di He; Kai Zheng; Shuxin Zheng; Chen Xing; Huishuai Zhang; Yanyan Lan; Liwei Wang; Tieyan Liu"}, {"ref_id": "b38", "title": "Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference", "journal": "IEEE", "year": "2020", "authors": "Ali Hadi Zadeh; Isak Edo"}, {"ref_id": "b39", "title": "Q8bert: Quantized 8bit bert", "journal": "", "year": "2019", "authors": "Ofir Zafrir; Guy Boudoukh; Peter Izsak; Moshe Wasserblat"}, {"ref_id": "b40", "title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "Long Papers", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"ref_id": "b41", "title": "Ternarybert: Distillation-aware ultra-low bit bert", "journal": "", "year": "2020", "authors": "Wei Zhang; Lu Hou; Yichun Yin; Lifeng Shang; Xiao Chen; Xin Jiang; Qun Liu"}, {"ref_id": "b42", "title": "there is no asbestos in our products now\" (b) \"cray computer has applied to trade on nasdaq", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Performance of quantized GPT-2 with varying weight bit-widths and 8-bit activation, using different methods. The right figure takes a closer look at LAQ.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: T-SNE visualization of the most frequent 500 word embeddings, of the full-precision and different 2-bit quantized models trained on PTB dataset. Embeddings of different methods show different degrees of homogeneity.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Matrices representing the cosine similarities between representations of all pairs of tokens in a sentence, between the full-precision model and 2-bit quantized models trained on PTB dataset. Token representations at the last decoder layer of GPT-2 are used. More visualizations are available in Appendix C.3.", "figure_data": ""}, {"figure_label": "a", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "( a )awo at Layer 4. (b) wg at Layer 4.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Distributions of output projection matrix w o in the multi-head attention module and the second linear layer w g in the feed-forward network of the 4-th layer from the 12-layer full-precision GPT-2. Other modules in other layers exhibit similar patterns. Vertical lines indicate the clipping factors learned by PACT and our method. Black curves show the estimated distribution by kernel density estimation.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure5: The training workflow of the proposed method. For each token in the quantized network, we compute both (i) the token-level contrastive distillation loss where the positive tokens and negative tokens are selected from the full-precision teacher network; and (ii) the distillation loss on the logits. The embedding layer and all weights in the Transformer layers are quantized with the proposed module-dependent dynamic scaling.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "representations of other tokens in both the full-precision and quantized networks (fp+quan.); (b) representations of other tokens in the quantized network (quan. only); and (c) the whole vocabulary randomly for each training iteration (global). Besides, we compare with (d) sequence-level contrastive learning by pulling together representations of the same sequence, and pushing away representations of differ-", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :7Figure 7: Effect of the number of negative samples.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 :8Figure 8: Scaling factors in the 2-bit QuantGPT.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 10 :10Figure 10: The training workflow of the proposed method for BART quantization in the task of abstractive summarization. The full-precision teacher network and distillation loss L dist are omitted for simplicity.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 11 :11Figure 11: More Visualizations: matrices representing the cosine similarities between representations of all pairs of tokens in a sentence, between the full-precision model and 2-bit quantized models trained on PTB dataset. Token representations at the last decoder layer of GPT-2 are used.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "Token-level Contrastive Distillation\u2026\u2026\u2113TransformerTransformer\u2026Layer 1Layer L\u2026\u2026\u2113EmbeddingTransformerTransformerEmbeddingLayerLayer 1Layer LLayerLogit Distillation"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results of language modeling on the test set of WikiText2, PTB and WikiText103 datasets, and next utterance prediction on the validation set of Persona-Chat dataset, with quantized GPT-2. \"#Bits (W-E-A)\" represents the bit-width for weights of Transformer layers, word embedding, and activations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "MethodSize (MB)(\u2193)WikiText2 PPL(\u2193)PTB PPL(\u2193)WikiText103 PPL(\u2193)full-prec. 474.9 (1.0x) 14.414.613.9KnGPT2 332.0 (1.4x)--20.5DistilGPT2 329.6 (1.4x)--21.1LightPAFF 268.0 (1.8x) 18.822.816.4Ours(8-8-8) 121.4 (3.9x) 15.314.914.6Ours(4-4-8) 62.4 (7.6x)15.615.015.3Ours(2-2-8) 33.0 (14.4x) 17.316.117.0Table 2: Comparison between our proposed quatizationmethod and other compression methods on GPT-2."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Efficiency study of the token-level contrastivelearning. The results are reported on the PTB dataseton 2-bit GPT-2. \"sec/iter\" means the needed time inseconds per iteration. Memory denotes the GPU con-sumption per device."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Representations for the contrastive loss L cont in 2-bit setting. The \"decoder-last\" means the contrastive loss is computed on the hidden states from the last Transformer layer of the decoder after a linear transform. The naming format works for other variants.", "figure_data": "-WikiText2PTBWikiText103Persona-ChatXSumMetricPPL (\u2193)PPL (\u2193)PPL (\u2193)Acc(%) (\u2191)R1 (\u2191)R2 (\u2191)RL (\u2191)decoder-last17.3016.1216.9874.7839.1516.7231.72decoder-first18.0216.6117.2574.7539.1116.7031.62encoder-last----38.9116.7231.67encoder-first----38.8716.7031.56Table 6: MethodWikiText2 PTB WikiText103QuantGPT17.3016.1216.98L dist only17.8516.9317.78Ours with PACT20.0317.7825.54"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Data splits of different datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Article: On Tuesday, a BBC Spotlight programme revealed that eight children had gone missing in Northern Ireland. Two of the girls were Somali teenagers who disappeared in 2005 and 2012. The Health and Social Care Board has said new guidelines are in place and add that no children have gone missing since 2014. Separated children are children outside their country of origin and separated from their parents or legal guardian. The term can also include unaccompanied asylum-seeking children and trafficked children. When they arrive in Northern Ireland they are taken into the care of the local health trust. Eight children have gone missing since 2005 and they remain missing. The SDLP's health spokesman Mark H Durkan said he would be raising the issue at the Northern Ireland Assembly's health committee and his party colleague Alex Attwood would be raising it at the justice committee. \"The number of children who cannot be accounted for is something that needs urgent inquiry and investigation,\" he said. \"There is a lot of very good work being done to look after the welfare of unaccompanied young people, but clearly we now have some very big questions that need to be answered.\" Ulster Unionist MLA Jo-Anne Dobson said it was \"frankly appalling\" to hear that eight children had gone missing. \"I have written to Health Minister Michelle O'Neill on this issue to seek further clarification and to demand details of how the department, health trusts and the Health and Social Care Board have sought to address each of the cases involved in the investigation,\" she added. The Green Party leader Steven Agnew also said it was extremely worrying that children can disappear without a trace. Paula Bradshaw from the Alliance Party added that the health trusts and police \"need to work closer over the handling of these cases\". In a statement, the Police Ombudsman for Northern Ireland said:", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Example summaries generated by 2-bit BART quantized with different methods. The training workflow of the proposed method for double-head GPT-2 quantization in the task of next utterance prediction. The model is trained to find the correct candidate. The full-precision teacher network and distillation loss L dist are omitted for simplicity.", "figure_data": "4834"}], "formulas": [{"formula_id": "formula_1", "formula_text": "{\u22121,\u2212 n\u22121 n , \u2022 \u2022 \u2022 , \u2212 1 n , 0, 1 n , \u2022 \u2022 \u2022 , n\u22121 n , 1} with n = 2 b\u22121 \u2212 1.", "formula_coordinates": [2.0, 70.58, 517.4, 218.55, 25.41]}, {"formula_id": "formula_2", "formula_text": "L cont = \u2212 n i=1 log exp(s(q s t i , h t t i )/\u03c4 ) j\u2208S i exp(s(q s t i , h t t j )/\u03c4 ) ,(2)", "formula_coordinates": [4.0, 302.51, 327.15, 221.91, 33.72]}, {"formula_id": "formula_3", "formula_text": "q s t i \u2190 mq s t i + (1 \u2212 m)h s t i ,(3)", "formula_coordinates": [4.0, 355.22, 450.84, 169.21, 15.02]}, {"formula_id": "formula_4", "formula_text": "L dist = \u2212 n i=1 z t t i log(z s t i ).(4)", "formula_coordinates": [4.0, 356.94, 591.37, 167.48, 33.72]}, {"formula_id": "formula_5", "formula_text": "L = \u03bbL cont + L dist ,(5)", "formula_coordinates": [4.0, 369.77, 658.87, 154.65, 10.78]}, {"formula_id": "formula_6", "formula_text": "\u03b1 = \u03b3 \u2022 w 1 n ,(6)", "formula_coordinates": [5.0, 147.0, 437.98, 142.14, 24.46]}, {"formula_id": "formula_7", "formula_text": "\u2202 \u2202\u03b3 = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u2202 \u2202wq Q(u) w 1 n , w < \u2212\u03b1 \u2202 \u2202wq [\u2212 w \u03b1 +Q(u)] w 1 n , \u2212\u03b1 \u2264 w \u2264 \u03b1 \u2202 \u2202wq Q(u) w 1 n , w > \u03b1 ,(7)", "formula_coordinates": [5.0, 72.04, 644.23, 217.1, 51.92]}, {"formula_id": "formula_8", "formula_text": "Method #Bits (W-E-A) Size (MB)(\u2193) XSum Metric R1(", "formula_coordinates": [7.0, 87.34, 198.81, 169.27, 34.53]}, {"formula_id": "formula_9", "formula_text": "\u03b1 = w 1 n \u03b3, u = clip(w, \u2212\u03b1, +\u03b1)/\u03b1, w q = Q(u)\u03b1,", "formula_coordinates": [12.0, 112.92, 169.14, 132.34, 54.24]}, {"formula_id": "formula_10", "formula_text": "\u2202 \u2202\u03b3 = \u2202 \u2202w q [ \u2202w q \u2202Q(u) \u2202Q(u) \u2202\u03b1 \u2202\u03b1 \u2202\u03b3 + \u2202w q \u2202\u03b1 \u2202\u03b1 \u2202\u03b3 ] = \u2202 \u2202w q [\u03b1 \u2202Q(u) \u2202\u03b1 w 1 n + Q(u) w 1 n ] = \u2202 \u2202w q [\u03b1 \u2202Q(u) \u2202\u03b1 + Q(u)] w 1 n .(8)", "formula_coordinates": [12.0, 79.25, 296.47, 209.89, 86.0]}, {"formula_id": "formula_11", "formula_text": "\u2202Q(u) \u2202\u03b1 = \u2202Q(u) \u2202u \u2202u \u2202\u03b1 = \uf8f1 \uf8f2 \uf8f3 0, w \u2264 \u2212\u03b1 \u2212 w \u03b1 2 , \u2212\u03b1 < w < \u03b1 0, w \u2265 \u03b1 .(9)", "formula_coordinates": [12.0, 70.85, 454.7, 218.29, 38.45]}, {"formula_id": "formula_12", "formula_text": "\u2202 \u2202\u03b3 = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u2202 \u2202wq Q(u) w 1 n , w \u2264 \u2212\u03b1 \u2202 \u2202wq [\u2212 w \u03b1 + Q(u)] w 1 n , \u2212\u03b1 < w < \u03b1 \u2202 \u2202wq Q(u) w 1 n , w \u2265 \u03b1", "formula_coordinates": [12.0, 72.05, 526.72, 215.95, 51.92]}], "doi": ""}