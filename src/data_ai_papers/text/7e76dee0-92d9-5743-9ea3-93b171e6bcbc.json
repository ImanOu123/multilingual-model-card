{"title": "Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment", "authors": "Nanjiang Jiang; Marie-Catherine De Marneffe", "pub_date": "", "abstract": "When a speaker, Mary, asks Do you know that Florence is packed with visitors?, we take her to believe that Florence is packed with visitors, but not if she asks Do you think that Florence is packed with visitors? Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here we explore the hypothesis that linguistic deficits drive the error patterns of speaker commitment models by analyzing the linguistic correlates of model errors on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The Com-mitmentBank is annotated with speaker commitment towards the content of the complement (Florence is packed with visitors in our example) of clause-embedding verbs (know, think) under four entailment-canceling environments. We found that a linguisticallyinformed model outperforms a LSTM-based one, suggesting that linguistic knowledge is needed to capture such challenging naturalistic data. A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.", "sections": [{"heading": "Introduction", "text": "Prediction of speaker commitment 1 is the task of determining to what extent the speaker is committed to an event in a sentence as actual, nonactual, or uncertain. This matters for downstream NLP applications, such as information extraction or question answering: for instance, we should extract from example (1) in Table 1 that the speaker could wish someone dead, but from (3) that people should not be allowed to carry guns in their vehicles, even though both events are embedded under believe and negation.\nThere has been work on factors leading to speaker commitment in theoretical linguistics (i.a., Karttunen (1971); Simons et al. (2010)) and computational linguistics (i.a., Diab et al. (2009); Saur\u00ed and Pustejovsky (2012); Prabhakaran et al. (2015)), but mostly on constructed or newswire examples, which may simplify the task by failing to reflect the lexical and syntactic diversity of naturally occurring utterances. de Marneffe et al. (2019) introduced the CommitmentBank, a dataset of naturally occurring sentences annotated with speaker commitment towards the content of complements of clause-embedding verbs under canceling-entailment environments (negation, modal, question and conditional), to study the linguistic correlates of speaker commitment. In this paper, we use it to evaluate two state-of-the-art (SoA) models of speaker commitment: Stanovsky et al. (2017) and . The Com-mitmentBank, restricted to specific linguistic constructions, is a good test case. It allows us to evaluate whether current speaker commitment models achieve robust language understanding, by analyzing their performance on specific challenging linguistic constructions.", "publication_ref": ["b4", "b15", "b2", "b14", "b11", "b9", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "The CommitmentBank corpus", "text": "The CommitmentBank 2 consists of 1,200 naturally occurring items involving clause-embedding verbs under four entailment-canceling environments (negations, modals, questions, condition-Figure 1: Mean commitment scores in all of the CommitmentBank. Italicized verbs are factive, plain nonfactive.\n(1) Context The answer is no, no no. Not now, not ever.\nTarget I never believed I could wish anyone dead Gold:1.56, Rule-based:3.0, Hybrid: 0.50 but last night changed all that.\n(2) Context Revenue is estimated at $18.6 million. The maker of document image processing equipment said the state procurement division had declared FileNet in default on its contract with the secretary of state uniform commercial code division.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Target", "text": "FileNet said it doesn't believe the state has a valid basis of default and is reviewing its legal rights under the contract Gold: -0.47, Rule-based: 3.0, Hybrid: 1.08 , but said it can't predict the outcome of the dispute.\n(3) Context A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, Target I don't believe that people should be allowed to carry guns in their vehicled Gold: -2.64, Rule-based: 3.0, Hybrid: 1.40\n.\nTable 1: Examples from the CommitmentBank, with gold scores and predictions from rule-based and hybrid models. Embedding verbs in bold, entailment-canceling environments italicized. The gold score is the mean annotators' speaker commitment judgments towards the content of the complement. als). Three genres are represented: newswire from the Wall Street Journal (WSJ), fiction from the British National Corpus, and dialog from Switchboard. Each item consists of up to two context sentences and one target sentence, as shown in Table 1. For each item, speaker commitment judgments were gathered on Mechanical Turk from at least eight native English speakers. Participants judged whether or not the speaker is certain that the content of the complement in the target sentence is true, using a Likert scale labeled at 3 points (+3/speaker is certain that the complement is true, 0/speaker is not certain whether it is true or false, -3/speaker is certain that it is false). We took the mean annotations of each item as gold score of speaker commitment. Figure 1 shows the mean annotations per embedding verb.\nRestricted set We identified a subset of the CommitmentBank that displays high agreement among annotators. We divided the range of integer ratings [\u22123, 3] into three sub-ranges: [1,3] where the speaker is committed to the complement p, 0 where the speaker is uncommitted towards p, [\u22123, \u22121] where the speaker is committed to \u00acp. We selected the items for which at least 80% of the annotations fall into the same sub-range. This gives 556 items, with 37 clause-embedding verbs.\nFigure 2 shows that the proportion of items with different linguistic features in the restricted set is similar to the proportion in the full set, suggesting that the restricted set is representative of the original data. The full CommitmentBank has a Krippendorff's \u03b1 of 0.53, while \u03b1 is 0.74 on the restricted set.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Models of speaker commitment", "text": "We evaluate the performance of two speaker commitment models on the CommitmentBank: a rulebased model (Stanovsky et al., 2017) and a neuralbased one .\nRule-based model Stanovsky et al. (2017) proposed a rule-based model based on a deterministic algorithm based on TruthTeller (Lotan et al., 2013), which uses a top-down approach on a de- pendency tree and predicts speaker commitment score in [\u22123, 3] according to the implicative signatures (Karttunen, 2012) of the predicates, and whether the predicates are under the scope of negation and uncertainty modifiers. For example, refuse p entails \u00acp, so the factuality of its complement p gets flipped if encountered.\nNeural-based model  introduced three neural models for speaker commitment: a linear biLSTM, a dependency tree biL-STM, a hybrid model that ensembles the two.  also proposed a multitask training scheme in which a model is trained on four factuality datasets: FactBank (Saur\u00ed and Pustejovsky, 2009), UW (Lee et al., 2015), MEAN-TIME (Minard et al., 2016) and UDS , all with annotations on a [\u22123, 3] scale. Each dataset has shared biLSTM weights but specific regression parameters.  ", "publication_ref": ["b16", "b16", "b8", "b6", "b13", "b7", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Reference datasets", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "We evaluated the models of Stanovsky et al. (2017) and  on the Commit-mentBank. We used Stanovsky et al. (2017)'s rulebased annotator 3 to get commitment ratings for the embedded predicates of the target sentences.\nFollowing , we trained the linear, tree, and hybrid biLSTM models using the multi-task training scheme on the four factuality datasets they used, which produced four predictions. Following White et al. ( 2018), we used cross-validated ridge regression to predict a final score using the four predictions. We include a majority baseline \"All -2.0\" (always predicting -2.0, since -2.0 is the most frequent answer in the full and restricted Commit-mentBank). The results are shown in Figure 3. The rule-based model outperforms the biLSTM models on the full set, but overall both SoA models do not perform very well on the Commitment-Bank. As shown in Figure 3, the Commitment-Bank is substantially more challenging for these models than the reference datasets, with lower correlation and higher absolute error rates than were obtained for any of these other datasets.", "publication_ref": ["b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "Focusing on the restricted set, we perform detailed error analysis of the outputs of the rule-based and hybrid biLSTM models, which achieved the best   (Stanovsky et al., 2017), and three biLSTM models in . Pearson r is undefined for All -2.0. All correlations are statistically significant (p < 0.05).  correlation. Table 3 shows performance for the following linguistic features, and Figure 4 shows scatterplots of gold judgments vs. predictions.", "publication_ref": ["b16"], "figure_ref": ["fig_3"], "table_ref": ["tab_4"]}, {"heading": "Embedding environment", "text": "The rule-based model can only capture inferences involving negation (r = 0.45), while the hybrid model performs more consistently across negation, modal, and question (r \u223c 0.25). Both models cannot handle inferences with conditionals.\nThe model's performance on the negation items also vary with respect to genre: the rule-based model has significant correlations for fiction (r = 0.45) and dialog (r = 0.32), while the hybrid model has correlations between 0.05 and 0.2 for all three genres, none reaching significance. About 40% of the modal and question items involve factive verbs, therefore the performance of these environments also correlate with the models' performance on factive verbs (elaborated on below).\nGenre Both models achieve the best correlation on dialog (Switchboard), and the worst on newswire (WSJ). The poor performance on WSJ might be due to its scores in CommitmentBank being more widespread (reflected in Figure 4) than annotations in the reference datasets (e.g., MEAN-TIME), which tend to be biased towards +3. The good performance of the rule-based model on dialog could be due to the fact that 70% of the items in dialog are in a negation environment with a nonfactive verb.\nFactive embedding verb Lexicalist theories (i.a., Karttunen 1973;Heim 1983)  Neg-raising Within sentences with negation, we examine the models' performance on sentences with \"neg-raising\" reading, where a negation in the matrix clause (not {think/believe} p) is interpreted as negating the complement clause (think/believe not p), as in example (3) in Table 1 where we understand the speaker to be committed to people should not be allowed to carry guns in their vehicles. We identify \"neg-raising\" items as items with a negation embedding environment, think or believe verb, and a negative commitment score. There is almost no correlation between both models' predictions and gold judgments (Table 3), suggesting that the models are not able to capture neg-raising inferences.\nModel behavior Figure 4 shows that the hybrid model predictions are mostly positive, whereas the rule-based model predictions are clustered at \u22123 and +3. This suggests that the rule-based model cannot capture the gradience present in commitment judgments, while the hybrid model struggles to recognize negative commitments.\nTo better interpret the models' outputs, we evaluate them in a classification setting. We use Gaussian mixture models to obtain three clusters for the mean gold scores and the predictions of both models. We assign the cluster with the highest mean to +: speaker is certain that the complement is true, the one with the lowest mean to -: speaker is certain that it is false, and the remaining one to o: speaker is not certain about its truth. We report precision, recall and F1 in Table 4. The rule-based model predicts + by default unless it has clear evidence (e.g., negation) for negative commitment. This behavior is reflected in the high precision for -. Both models perform well on + and -, but neither is able to identify no commitment (o).", "publication_ref": ["b5", "b3"], "figure_ref": ["fig_3", "fig_3"], "table_ref": ["tab_4", "tab_6"]}, {"heading": "Conclusion", "text": "Our evaluation of two SoA models for speaker commitment on the CommitmentBank shows that the models perform better on sentences with nega-  tion, and with nonfactive embedding verbs. However, they are not able to generalize to other linguistic environments such as conditional, modal, and neg-raising, which display inference patterns that are important for information extraction. Both models are able to identify the polarity of commitment, but cannot capture its gradience. The rulebased model, outperforming the biLSTM models on the full CommitmentBank, shows that a linguistically-informed model scales more successfully to challenging naturalistic data.\nIn the long run, to perform robust language understanding, models will need to incorporate more linguistic foreknowledge and be able to generalize to a wider range of linguistic constructions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgment", "text": "We thank the anonymous reviewers for their valuable feedback. We also thank Micha Elsner, Cory Shain, and the Clippers group at The Ohio State University for their helpful discussions and suggestions. This material is based upon work supported by the National Science Foundation under Grant No. IIS-1464252.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Have you noticed that your belly button lint colour is related to the colour of your clothing?", "journal": "Brill", "year": "2010", "authors": "David Beaver"}, {"ref_id": "b1", "title": "English web treebank. Linguistic Data Consortium", "journal": "", "year": "2012", "authors": "Ann Bies; Justin Mott; Colin Warner; Seth Kulick"}, {"ref_id": "b2", "title": "Committed belief annotation and tagging", "journal": "", "year": "2009", "authors": "Mona Diab; Lori Levin; Teruko Mitamura; Owen Rambow; Vinodkumar Prabhakaran; Weiwei Guo"}, {"ref_id": "b3", "title": "On the projection problem for presuppositions", "journal": "", "year": "1983", "authors": "Irene Heim"}, {"ref_id": "b4", "title": "Some observations on factivity", "journal": "Papers in Linguistics", "year": "1971", "authors": "Lauri Karttunen"}, {"ref_id": "b5", "title": "Presuppositions and compound sentences", "journal": "Linguistic Inquiry", "year": "1973", "authors": "Lauri Karttunen"}, {"ref_id": "b6", "title": "Simple and phrasal implicatives", "journal": "", "year": "2012", "authors": "Lauri Karttunen"}, {"ref_id": "b7", "title": "Event detection and factuality assessment with non-expert supervision", "journal": "", "year": "2015", "authors": "Kenton Lee; Yoav Artzi; Yejin Choi; Luke Zettlemoyer"}, {"ref_id": "b8", "title": "TruthTeller: Annotating predicate truth", "journal": "", "year": "2013", "authors": "Amnon Lotan; Asher Stern; Ido Dagan"}, {"ref_id": "b9", "title": "The commitmentBank: Investigating projection in naturally occurring discourse", "journal": "", "year": "2019", "authors": "Marie-Catherine De Marneffe; Mandy Simons; Judith Tonhauser"}, {"ref_id": "b10", "title": "MEANTIME, the newsreader multilingual event and time corpus", "journal": "", "year": "2016", "authors": "Anne-Lyse Myriam Minard; Manuela Speranza; Ruben Urizar; Begona Altuna; Anneleen Marieke Van Erp; Chantal Schoen;  Van Son"}, {"ref_id": "b11", "title": "A new dataset and evaluation for belief/factuality", "journal": "", "year": "2015", "authors": "Tomas Vinodkumar Prabhakaran; Julia By; Owen Hirschberg; Samira Rambow; Tomek Shaikh; Jennifer Strzalkowski; Michael Tracey; Rupayan Arrigo; Micah Basu; Adam Clark; ; Dalton; Janyce Wiebe"}, {"ref_id": "b12", "title": "Neural models of factuality", "journal": "", "year": "2018", "authors": "Rachel Rudinger; Aaron Steven White; Benjamin Van Durme"}, {"ref_id": "b13", "title": "FactBank: A corpus annotated with event factuality. Language resources and evaluation", "journal": "", "year": "2009", "authors": "Roser Saur\u00ed; James Pustejovsky"}, {"ref_id": "b14", "title": "Are you sure that this happened? Assessing the factuality degree of events in text", "journal": "Computational Linguistics", "year": "2012", "authors": "Roser Saur\u00ed; James Pustejovsky"}, {"ref_id": "b15", "title": "What projects and why", "journal": "CLC Publications", "year": "2010", "authors": "Mandy Simons; Judith Tonhauser; David Beaver; Craige Roberts"}, {"ref_id": "b16", "title": "Integrating deep linguistic features in factuality prediction over unified datasets", "journal": "Short Papers", "year": "2017", "authors": "Gabriel Stanovsky; Judith Eckle-Kohler; Yevgeniy Puzikov; Ido Dagan; Iryna Gurevych"}, {"ref_id": "b17", "title": "Lexicosyntactic inference in neural models", "journal": "", "year": "2018", "authors": "Aaron Steven White; Rachel Rudinger; Kyle Rawlins; Benjamin Van Durme"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Number of items with different features in the full and restricted sets of the CommitmentBank.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "predict that complements of factive verbs are commitments of the speaker. This tendency is reflected in Figures 1 and 4 where most sentences with factives have higher mean commitment scores. Both models get better MAE on factives, but better correlation on nonfactives. The improved MAE of the rule-based model might be due to its use of factive/implicative signatures. However, the poor correlations suggest that neither model can robustly capture the variability in inference which exists in sentences involving factive/nonfactive verbs (see i.a. Beaver 2010; de Marneffe et al. 2019).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Gold scores vs. model prediction. Each point is a sentence. Lines show perfect predictions.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The FactBank, UW, and MEANTIME datasets all consist of sentences from news articles. Each event in FactBank was annotated by 2 annotators, with 0.81 Cohen's \u03ba. UW has 5 annotations for each event, and MEANTIME has 6. UDS contains sentences from the English Web Treebank(Bies et al., 2012), which contains weblogs, newsgroups, emails, reviews, and question-answers. It has 2 annotations for each predicate, with 0.66 Cohen's \u03ba. All four datasets have annotations biased towards +3, because (1) they are newswire-heavy with sentences describing known factual events, and (2) most annotations are for main-clause predicates instead of predicates in an embedded clause.Table2gives the number of predicates in each dataset and state-of-the-art results obtained. Two metrics were reported for both models: mean absolute error (MAE), measuring the absolute fit,", "figure_data": "# PredicateSoArMAEFactBank9,761 0.86 0.31MEANTIME1,395 0.61 0.23UW13,644 0.75 0.42UDS27,289 0.77 0.96"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The number of annotated predicates in each dataset, and previous state-of-the-art performance. The score on UW with MAE was obtained byStanovsky et al. (2017), while the other scores were obtained by.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance and number of items per feature. The scores in bold indicate the classes on which each model has the best performance (with respect to both metrics). \u2020 marks statistical significance of Pearson's correlation (p < 0.05).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Rule Hybr. Rule Hybr. + 0.58 0.64 0.91 0.51 0.71 0.56 251 -0.99 0.67 0.55 0.20 0.70 0.31 268 o 0.00 0.06 0.00 0.46 0 0.11 37 Total 0.74 0.61 0.67 0.35 0.66 0.41 556", "figure_data": "PrecisionRecallF1CountRule Hybr."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Classification performance of the models.", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/P17-2056"}