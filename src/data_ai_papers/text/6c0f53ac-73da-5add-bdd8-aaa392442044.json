{"title": "Active Fairness Auditing", "authors": "Tom Yan; Chicheng Zhang", "pub_date": "", "abstract": "The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently audit these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.", "sections": [{"heading": "Introduction", "text": "With growing usage of artificial intelligence (AI) across industries, governance efforts are increasingly ramping up.\nA key challenge in these regulatory efforts is the problem of scalability. Even for well-resourced countries like Norway, which is pioneering efforts in AI governance, regulators are only able to monitor and engage with a \"small fraction of the companies\" (McCarthy, 2021). This growing issue calls for a better understanding of efficient approaches to auditing machine learning (ML) models, which we now formalize.\nProblem Formulation: A regulatory institution is interested in auditing a model h * : X \u2192 {\u22121, 1} held by a company (e.g. a lending company in the finance sector), where X is the feature space (e.g. of all information supplied by users). We assume that the regulatory institution only has knowledge of the hypothesis class H where h * Property of Interest: While which properties \u00b5 to assess is still heavily debated by regulators, we initiate the study of auditing algorithms by focusing on fairness, a mainstay in regulatory focuses. In particular, the \u00b5 we will consider will be Demographic Parity (DP) 1 : given distribution D X over X \u00d7 {0, 1} (where feature x and sensitive attribute x A are jointly drawn from), \u00b5 D X (h) = Pr (x,x A )\u223cD X (h(x) = 1|x A = 1) \u2212 Pr (x,x A )\u223cD X (h(x) = 1|x A = 0). For brevity, when it is clear from context, we abbreviate Pr D X , \u00b5 D X as Pr, \u00b5, respectively. DP measures the degree of disparate treatment of model h on the two sub-populations x | x A = 0 and x | x A = 1, which we assume are non-negligible: p := min(Pr(x A = 1), Pr(x A = 0)) = \u2126(1). Achieving a small Demographic Parity may be thought of as a stronger version of the US Equal Employment Opportunity Commission's \"four-fifths rule\". 2 To focus on query complexity, we will abstract away the difficulty of evaluating \u00b5 by assuming that D X is known, and thus for any h, we may evaluate \u00b5(h) to arbitrary precision; for instance, this may be achieved with the availability of an arbitrarily large number of (unlabeled) samples randomly drawn from x | x A = 0 and x | x A = 1. Our main challenge is that we do not know h * and only want to query h * insofar as to be able to accurately estimate \u00b5(h * ).\nGuarantees of the Audit: In our paper, we investigate 1 While fairness is the focus of our work, our algorithm may be adapted to any \u00b5 which is a function of X and h * . 2 The \"selection rate for any race, sex, or ethnic group [must be at least] four-fifths (4/5) (or eighty percent) of the rate for the group with the highest rate.\" arXiv:2206.08450v1 [cs.LG] 16 Jun 2022 algorithms that can provide two types of guarantees. The first is the natural, direct estimation accuracy: the estimate returned by the algorithm should be within of \u00b5(h * ).\nThe second is that of manipulation-proof (MP) estimation. Audits can be very consequential to companies as they may be subject to hefty penalties if caught with violations. Not surprisingly, there have been effortful attempts in the past to avoid being caught with violations (e.g. Hotten, 2015) by \"gaming\" the audit. We formulate our notion of manipulation-proofness in light of one way the audit may be gamed, which we now describe. Note that all the auditor knows about the model used by the company is that it is consistent with the queried labels in the audit. So, while our algorithm may have estimated \u00b5(h * ) accurately during audit-time, nothing stops the company from changing its model post-audit from h * to a different model h new \u2208 H (e.g to improve profit), so long as h new is still consistent with the queries seen during the audit. With this, we also look to understand: given this post-hoc possibility of manipulation, can we devise an algorithm that nonetheless ensures the algorithm's estimate is within of \u00b5(h new )? Indeed, a robust set of audit queries would serve as a certificate that no matter which model the company changes to after the audit, its \u00b5-estimation would remain accurate. Given a set of classifiers V , a classifier h, and a unlabeled dataset S, define the version space (Mitchell, 1982) induced by S to be V (h, S) := h \u2208 V : h (S) = h(S) . An auditing algorithm is -manipulation-proof if, for any h * , it outputs a set of queries S and estimate\u03bc that guarantees that max h\u2208H(h * ,S) \u00b5(h) \u2212\u03bc \u2264 .\nBaseline: i.i.d Sampling: One natural baseline that comes to mind for the direct estimation is i.i.d sampling. We sample O(1/ 2 ) examples i.i.d from the distribution x | x A = i for i \u2208 {0, 1}, query h * on these examples and take the average to obtain an estimate of Pr(h * (x) = +1 | x A = i). Finally, we take the difference of these two estimates as our final DP estimate. By Hoeffding's Inequality, with high probability, this estimate is -accurate, and this estimation procedure makes O(1/ 2 ) queries. However, i.i.d sampling is not necessarily MP. To see an example, let there be 2n points in group x A = 1 with n = 1/ 2 that are shattered by H and D X is uniform over these points. Suppose that all points in group x A = 0 are labeled the same: Pr D X (h(x) = 1|x A = 0) = 0, \u2200h \u2208 H. Then, \u00b5-estimation reduces to estimating the proportion of positives in group x A = 1. i.i.d sampling will randomly choose n of these data points to see, and it will produce an -accurate estimate of \u00b5(h * ). However, we do not see the other n points. Since the 2n points are shattered by H, after the queried points are determined, we see that the company can increase or decrease DP by up to 1/2 by switching to a different model.\nTo obtain both direct and MP estimation, it seems promising then to examine algorithms that make use of non-iid sampling. Moreover, for MP, we observe that the auditing algorithm should leverage knowledge of the hypothesis class as well, which i.i.d sampling is agnostic to.\nBaseline: Active Learning: An algorithm that achieves both direct and MP estimation accuracy is PAC active learning (Hanneke, 2014) (where PAC stands for Probably Approximately Correct (Valiant, 1984)). PAC active learning algorithms guarantee that, with high probability,\u0125 in the resultant version space is such that P(\u0125(x) = h * (x)) \u2264 p = O( ). With this, we have \u00b5(\u0125) \u2212 \u00b5(h * ) \u2264 (see Lemma C.1 in Appendix C for a formal proof).\nTo mention a setting where learning is favored over i.i.d sampling, learning homogeneous linear classifiers under certain well-behaved unlabeled data distributions requires only O(d log 1/ ) queries (e.g. Dasgupta, 2005b;Balcan & Long, 2013) and would thus be far more efficient than O(1/ 2 ) for low-dimensional learning settings with high auditing precision requirements.\nStill, as our goal is only to estimate the \u00b5 values of the induced version space, it is unclear if we need to go as far as to learn the model itself. In this paper, we investigate whether, and if so when, it may be possible to design adaptive approaches to efficiently directly and/or MP estimate \u00b5(h * ) using knowledge of H.\nTo the best of our knowledge, we are the first to theoretically investigate active approaches for direct and MP estimation of \u00b5(h * ). Our first exploration of active fairness estimation seeks to provide a more complete picture of the theory of auditing machine learning models. Our hope is that our theoretical results can pave the way for subsequent development of practical algorithms.\nOur Contributions: Our main contributions are on two fronts, MP estimation and direct estimation of \u00b5(h * ):\n\u2022 For the newly introduced notion of manipulationproofness, we identify a statistically optimal, but computationally intractable deterministic algorithm. We gain insights into its query complexity through comparisons to the two baselines, i.i.d sampling and PAC active learning. \u2022 In light of the computational intractability of the optimal deterministic algorithm, we design a randomized algorithm that enjoys oracle efficiency (e.g. Dasgupta et al., 2007): it has an efficient implementation given access to a mistake-bounded online learning oracle, and an constrained empirical risk minimization oracle for the hypothesis class H. Furthermore, its query performance matches that of the optimal deterministic algorithm up to polylog|H| factors.\n\u2022 Finally, on the direct estimation front, we obtain bounds on information-theoretic query complexity. We establish that MP estimation may be more expensive than direct estimation, thus highlighting the need to develop separate algorithms for the two guarantees. Then, we establish the usefulness of randomization in algorithm design and develop an optimal, randomized algorithm for linear classification under Gaussian subpopulations. Finally, to shed insight on general settings, we develop distribution-free lower bounds for direction estimation under general VC classes. This lower bound charts the query complexity that any optimal randomized auditing algorithms must attain.", "publication_ref": ["b25", "b23", "b12", "b3", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Additional Notations", "text": "We now introduce some additional useful notation used throughout the paper. Let [m] denote {1, ..., m}. For an unlabeled dataset S, and two classifiers h, h , we say h(S) = h (S) if for all x \u2208 S, h(x) = h (x). Given a set of classifiers V and a labeled dataset T , define\nV [T ] := h \u2208 V : \u2200(x, y) \u2208 T, h(x) = y . Furthermore, denote by V y x = V (x, y) for notational simplicity. Given a set of classifiers V and fairness measure \u00b5, denote by diam \u00b5 (V ) := max h,h \u2208V \u00b5(h) \u2212 \u00b5(h ) the \u00b5-diameter of V . Given a set of labeled examples T , denote by Pr T (\u2022)\nthe probability over the uniform distribution on T ; given a classifier h, denote by err(h, T ) = Pr T (h(x) = y) the empirical error of h on T .\nThroughout this paper, we will consider active fairness auditing under the membership query model, similar to membership query-based active learning (Angluin, 1988). Specifically, a deterministic active auditing algorithm A with label budget N is formally defined as a collection of N + 1 (computable) functions f 1 , f 2 , . . . , f N , g such that:\n1. For every i \u2208 [N ], f i : (X \u00d7 Y) i\u22121 \u2192 X\nis the label querying function used at step i, that takes into input the first (i \u2212 1) labeled examples (x 1 , y 1 ), . . . , (x i\u22121 , y i\u22121 ) obtained so far, and chooses the i-th example x i for label query.\n2. g : (X \u00d7 Y) N \u2192 R is the estimator function that takes into input all N labeled examples (x 1 , y 1 ), . . . , (x N , y N ) obtained throughout the interaction process, and outputs\u03bc, the estimate of \u00b5(h * ).\nWhen A interacts with a target classifier h, let the resultant queried unlabeled dataset be S A,h = x 1 , . . . , x N , and the final \u00b5 estimate be\u03bc A,h . Similar to deterministic algorithms, a randomized active auditing algorithm A with label budget N and B bits of random seed is formally defined as a collection of N + 1 (computable) functions f 1 , . . . , f N , g, where\nf i : (X \u00d7 Y) i\u22121 \u00d7 {0, 1} B \u2192 X and g : (X \u00d7 Y) N \u00d7 {0, 1} B \u2192 R.\nNote that each function now take as input a B-bit random seed; as a result, when A interacts with a fixed h * , its output \u00b5 is now a random variable. Note also that under the above definition, a randomized active auditing algorithm A that uses a fixed seed b may be viewed as a deterministic active auditing algorithm A b .\nWe will be comparing our algorithms' query complexities with those of disagreement-based active learning algorithms (Cohn et al., 1994;Hanneke, 2014). Given a classifier h and r > 0, define B(h, r) = h \u2208 H : Pr D X h (x) = h(x) \u2264 r as the disagreement ball centered at h with radius r. Given a set of classifiers V , define its disagreement region DIS(V ) =\nx \u2208 X : \u2203h, h \u2208 V : h(x) = h (x) . For a hypothesis class H and an unlabeled data distribution D X , an important quantity that characterizes the query complexity of disagreement-based active learning algorithm is the disagreement coefficient \u03b8(r), defined as\n\u03b8(r) = sup h\u2208H,r \u2265r Pr D X (x \u2208 DIS(B(h, r ))) r .", "publication_ref": ["b2", "b10", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Our work is most related to the following two lines of work, both of which are concerned with estimating some property of a model without having to learn the model itself.\nSample-Efficient Optimal Loss Estimation: Dicker (2014); Kong & Valiant (2018) propose U-statistics-based estimators that estimate the optimal population mean square error in d-dimensional linear regression, with a sample complexity of O( \u221a d) (much lower than O(d), the sample complexity of learning optimal linear regressor). Kong & Valiant (2018) also extend the results to a well-specified logisitic regression setting, where the goal is to estimate the optimal zero-one loss. Our work is similar in focusing on the question of efficient \u00b5(h * ) estimation without having to learn h * . Our work differs in focusing on fairness property instead of the optimal MSE or zero-one loss. Moreover, our results apply to arbitrary H, and not just to linear models.\nInteractive Verification: Goldwasser et al. (2021) studies verification of whether a model h's loss is near-optimal with respect to a hypothesis class H and looks to understand when verification is cheaper than learning. They prove that verification is cheaper than learning for specific hypothesis classes and is just as expensive for other hypothesis classes. Again, our work differs in focusing on a different property of the model, fairness.\nOur algorithm also utilizes tools from active learning and machine teaching, which we review below.\nActive Learning and Teaching: The task of learning h * approximately through membership queries has been wellstudied (e.g. Angluin, 1988;Heged\u0171s, 1995;Dasgupta, 2005a;Hanneke, 2006;2007). Our computationally efficient algorithm for active fairness auditing is built upon the connection between active learning and machine teaching (Goldman & Kearns, 1995), as first noted in Heged\u0171s (1995); Hanneke (2007). To achieve computational efficiency, our work builds on recent work on black-box teaching (Dasgupta et al., 2019), which implicitly gives an efficient procedure for computing an approximate-minimum specifying set; we adapt Dasgupta et al. (2019)'s algorithm to give a similar procedure for approximating the minimum specifying set that specifies the \u00b5 value.\nIn the interest of space, please see discussion of additional related work in Appendix A.", "publication_ref": ["b28", "b28", "b19", "b2", "b24", "b11", "b20", "b21", "b17", "b24", "b21", "b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Manipulation-Proof Algorithms", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Deterministic Algorithm", "text": "We begin our study of the MP estimation of \u00b5(h * ) by identifying an optimal deterministic algorithm based on dynamic programming. Inspired by a minimax analysis of exact active learning with membership queries (Hanneke, 2006), we recursively define the following value function for any version space V \u2286 H:\nCost(V ) = 0, diam \u00b5 (V ) \u2264 2 1 + min x max y Cost(V [(x, y)]), otherwise\nNote that Cost(V ) is similar to the minimax query complexity of exact active learning (Hanneke, 2006), except that the induction base case is different -here the base case is diam \u00b5 (V ) \u2264 2 , which implies that subject to h * \u2208 V , we have identified \u00b5(h * ) up to error . In contrast, in exact active learning, Hanneke (2006)'s induction base case is |V | = 1, where we identify h * through V .\nThe value function Cost also has a game-theoretic interpretation: imagine that a learner plays a multi-round game with an adversary. The learner makes sequential queries of examples to obtain their labels, and the adversary reveals the labels of the examples, subject to the constraint that all labeled examples shown agree with some classifier in H. The version space V encodes the state of the game: it is the set of classifiers that agrees with all the labeled examples shown so far in the game. The interaction between the learner and the adversary ends when all classifiers in V has \u00b5 values 2 -close to each other. The learner would like to minimize its total cost, which is the number of rounds. Cost(V ) can be viewed as the minimax-optimal future cost, subject to the game's current state being represented by version space V .\nBased on the notion of Cost, we design an algorithm, Al-Algorithm 1 Minimax optimal deterministic auditing Require: Finite hypothesis class H, target error , fairness measure \u00b5 Ensure:\u03bc, an estimate of \u00b5(h * )\n1: Let V \u2190 H 2: while diam \u00b5 (V ) > 2 do 3: Query x \u2208 argmin x max y Cost (V y x ), obtain label h * (x) 4: V \u2190 V (h * , {x}) 5: return 1 2 max h\u2208V \u00b5(h) + min h\u2208V \u00b5(h)\ngorithm 1, that has a worst-case label complexity at most Cost(H). Specifically, it maintains a version space V \u2282 H, initialized to H (line 1). At every iteration, if the \u00b5-diameter\nof V , diam \u00b5 (V ) = max h,h \u2208V \u00b5(h) \u2212 \u00b5(h ), is at most 2 , then since \u00b5(h * ) \u2208 I = [min h\u2208V \u00b5(h), max h\u2208V \u00b5(h)]\nreturning the midpoint of I gives us an -accurate estimate of \u00b5(h * ) (line 5). Otherwise, Algorithm 1 makes a query by choosing the x that minimizes the worst-case future value functions (line 3). After receiving h * (x), it updates its version space V (line 4). By construction, the interaction between the learner and the labeler lasts for at most Cost(V ) rounds, which gives the following theorem.\nTheorem 3.1. If Algorithm 1 interacts with some h * \u2208 H, then it outputs\u03bc such that \u03bc \u2212 \u00b5(h * ) \u2264 , and queries at most Cost(H) labels.\nBy the minimax nature of Cost, we also show that among all deterministic algorithms, Algorithm 1 has the optimal worst-case query complexity:\nTheorem 3.2. If A is a deterministic algorithm with query budget N \u2264 Cost(H) \u2212 1, there exists some h * \u2208 H, such that\u03bc, the output of A after querying h * , satisfies \u03bc \u2212 \u00b5(h * ) > .\nThe proofs of Theorems 3.1 and 3.2 are deferred to Appendix D.1.", "publication_ref": ["b20", "b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "COMPARISON TO BASELINES", "text": "To gain a better understanding of Cost(H), we relate it to the label complexity of Algorithm 1 with those of the two baselines, i.i.d sampling and active learning. To establish the comparison, we prove that we can derandomize existing i.i.d sampling-based and active learning-based auditing algorithms with a small overhead on label complexity. The comparison follows as Algorithm 1 is the optimal deterministic algorithm.\nOur first result is that, the label complexity of Algorithm 1 is within a factor of O(ln |H|) of the label complexity of i.i.d sampling.\nProposition 3.3. Cost(H) \u2264 O 1 2 ln |H| .\nOur second result is that the label complexity of Algorithm 1 is always no worse than the distribution-dependent label complexity of CAL (Cohn et al., 1994;Hanneke, 2014), a well-known PAC active learning algorithm. We believe that similar bounds of Cost(H) compared to generic active learning algorithms can be shown, such as the Splitting Algorithm (Dasgupta, 2005b) or the confidence-based algorithm of Zhang & Chaudhuri (2014), through suitable derandomization procedures.\nProposition 3.4. Cost(H) \u2264 O \u03b8( ) \u2022 ln |H| \u2022 ln 1 ,\nwhere \u03b8 is the disagreement coefficient of H with respect to D X (recall Section 1.1 for its definition).\nProof sketch. We present Algorithm 2, which is a derandomized version of the Phased CAL algorithm (Hsu, 2010, Chapter 2). To prove this proposition, using Theorem 3.2, it suffices to show that Algorithm 2 has a deterministic label complexity bound of O \u03b8( ) \u2022 ln |H| \u2022 ln 1 . We only present the main idea here, and defer a precise version of the proof to Appendix D.3.\nWe first show that for every n, the optimization problem in line 5 is always feasible. To see this, observe that if we draw S n , a sample of size m n , drawn i.i.d from D X , we have:\n1. By Bernstein's inequality, with probability 1 \u2212 1 4 ,\nPr Sn (x \u2208 DIS(V n )) \u2264 2 Pr D X (x \u2208 DIS(V n ))+ ln 8 m n ,\n2. By Bernstein's inequality and union bound over h, h \u2208 H, we have with probability\n1 \u2212 1 4 , \u2200h, h \u2208 H : Pr S (h(x) = h (x)) = 0 =\u21d2 Pr D X (h(x) = h (x)) \u2264 16 ln |H| m n .\nBy union bound, with nonzero probability, the above two condition hold simultaneously, showing the feasibility of the optimization problem.\nWe then argue that for all n, V n+1 \u2286 B(h * , 16 ln |H| mn ). This is because for each h \u2208 V n+1 , h and h * are both in V n and therefore they agree on S n \\ T n ; on the other hand, h and h * agree on T n by the definition of of V n+1 . As a consequence, Pr Sn (h(x) = h * (x)) = 0, which implies that Pr D X (h(x) = h * (x)) \u2264 16 ln |H| mn . As a consequence, for all h \u2208 V N +1 , Pr(h(x) = h * (x)) \u2264 p , which, combined with Lemma C.1, implies that \u00b5(h) \u2212 \u00b5(h * ) \u2264 .  Find (the lexicographically smallest) S n \u2208 X mn such that:\nPr Sn (x \u2208 DIS(V n )) \u2264 2 Pr D X (x \u2208 DIS(V n ))+ ln 8 m n ,and\n\u2200h, h \u2208 H : Pr Sn (h(x) = h (x)) = 0 =\u21d2 Pr D X (h(x) = h (x)) \u2264 16 ln |H| m n . 6: Query h * for the labels of examples in T n := S n \u2229 DIS(V n ) 7: V n+1 \u2190 V n (h * , T n ). 8: return \u00b5(h) for an arbitrary h \u2208 V N +1 .\nFinally, to upper bound Algorithm 2's label complexity:\nN n=1 |T n | = N n=1 m n \u2022 (2 Pr D X (x \u2208 DIS(V n )) + ln 8 m n ) \u2264 N n=1 m n \u2022 (2\u03b8( ) 16 ln |H| m n + ln 8 m n ) \u2264O \u03b8( ) \u2022 ln |H| \u2022 ln 1 .", "publication_ref": ["b10", "b23", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "COMPUTATIONAL HARDNESS OF IMPLEMENTING ALGORITHM 1", "text": "Although Algorithm 1 has the optimal label complexity guarantees among all deterministic algorithms, we show in the following proposition that, under standard complexitytheoretic assumptions (NP \u2286 TIME(n O(log log n) )), even approximating Cost(H) is computationally intractable. We remark that the constant 0.3 can be improved to a constant arbitrarily smaller than 1. The main insight behind this proposition is a connection between Cost(H) and optimaldepth decision trees (see Theorem D.4): using the hardness of computing an approximately-optimal-depth decision tree (Laber & Nogueira, 2004) and taking into account the structure of \u00b5, we establish the intractability of approximating Cost(H).\nOwing to the intractability of Algorithm 1, in the next section, we turn to the design of a computationally efficient algorithm whose label complexity nears that of Algorithm 1 (i.e. Cost(H)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Randomized Algorithm with Competitive Guarantees", "text": "We present our efficient algorithm in this section, which also serves as a first upper bound on the statistical complexity of computationally tractable algorithms. Our algorithm, Algorithm 3, is inspired by the exact active learning literature (Heged\u0171s, 1995;Hanneke, 2007), based on the connection between machine teaching (Goldman & Kearns, 1995) and active learning. \n\u221e t=1 I(\u0125 t (x t ) = h * (x t )) \u2264 M .\nWell-known implementations of mistake bounded online learning oracle include the halving algorithm and its efficient sampling-based approximations (Bertsimas & Vempala, 2004) as well as the Perceptron / Winnow algorithm (Littlestone, 1988;Ben-David et al., 2009). For instance, if O is the halving algorithm, a mistake bound of M = log 2 |H| may be achieved.\nWe next define the constrained ERM oracle, which has been previously used in a number of works on oracle-efficient active learning (Dasgupta et al., 2007;Hanneke, 2011;Huang et al., 2015). Definition 3.7. An constrained ERM oracle for hypothesis class H, C-ERM, is one that takes as input labeled datasets A and B, and outputs a classifier\u0125 \u2208 argmin err(h, A) : h \u2208 H, err(h, B) = 0 .\nThe high-level idea of Algorithm 3 is as follows: at every iteration, it uses the mistake-bounded online learning oracle to generate some classifier\u0125 (line 3); then, it aims to construct a dataset T of small size, such that after querying h * for the labels of examples in T , one of the following two happens: (1)\u0125 disagrees with h * on some example in T ;\n(2) for all classifiers in the version space V = h \u2208 H : \u2200x \u2208 T, h(x) = h * (x) , we have diam \u00b5 (V ) \u2264 2 . In case (1), we have found a counterex-ample for\u0125, which can be fed to the online learning oracle to learn a new model, and this can happen at most M times; in case (2), we are done: our queried labeled examples ensure that our auditing estimate is -accurate, and satisfies manipulation-proofness. Dataset T of such property is called a (\u00b5, )-specifying set for\u0125, as formally defined in Defintion D.7 in Appendix D.5.\nAnother view of the \u00b5-specifying set is a set T such that for all h, h with \u00b5(h) \u2212 \u00b5(h ) > 2 , there exists some x \u2208 T , such that h(x) =\u0125(x) or h (x) =\u0125(x). The requirements on T can be viewed as a set cover problem, where the universe U is (h, h ) \u2208 H 2 : \u00b5(h) \u2212 \u00b5(h ) > 2 , and the set system is\nC = {C x : x \u2208 X }, where (h, h ) is in C x if h(x) =\u0125(x) or h (x) =\u0125(x).\nThis motivates us to design efficient set cover algorithms in this context. A key challenge of applying standard offline set cover algorithms (such as the greedy set cover algorithm) to construct approximate minimum (\u00b5, )-specifying set is that we cannot afford to enumerate all elements in the universe U : U can be exponential in size.\nIn face of this challenge, we draw inspiration from online set cover literature (Alon et al., 2009;Dasgupta et al., 2019) to design an oracle-efficient algorithm that computes O(log |H| log |X |)-approximate minimum (\u00b5, )specifying sets, which avoids enumeration over U .\nOur key idea is to simulate an online set cover process. We build the cover set 3 T iteratively, starting from T = \u2205 (line 4). At every inner iteration, we first try to find a pair (h 1 , h 2 ) in U not yet covered by the current T . As we shall see next, this step (line 7) can be implemented efficiently given the constrained ERM oracle C-ERM. If such a pair (h 1 , h 2 ) can be found, we use the online set cover algorithm implicit in (Dasgupta et al., 2019) to find a new example that covers this pair, add it to T , and move onto the next iteration (lines 11 to 14). Otherwise, T has successfully covered all the elements in U , in which case we break the inner loop (line 9).\nTo see how line 7 finds an uncovered pair in U , we note that it can be also written as:\n(h 1 , h 2 ) = argmax h,h \u2208H \u00b5(h) \u2212 \u00b5(h ) : h(T ) = h (T ) =\u0125(T )\nThus, if \u00b5(h 1 ) \u2212 \u00b5(h 2 ) > 2 , then the returned pair (h 1 , h 2 ) corresponds to a pair in universe U that is not covered by T . Otherwise, by the optimality of (h 1 , h 2 ), T covers all elements in U .\nFurthermore, we note that optimization problems (1) and (2) can be implemented with access to C-ERM. We show this for program (1) and the reasoning for program (2) is analogous. Observe that maximizing \u00b5(h) from h \u2208 H subject to constraint h(T ) =\u0125(T ) is equivalent to minimizing (a weighted) empirical error of h \u2208 H on dataset (x, +1) : x \u2208 X , x A = 0 \u222a (x, \u22121) : x \u2208 X , x A = 1 , subject to h having zero error on {(x,\u0125(x)) : x \u2208 T }.\nWe are now ready to present the label complexity guarantee of Algorithm 3. Use C-ERM to solve separate programs: //Add examples to T to cover (h 1 , h 2 ), using the online set cover algorithm implicit in (Dasgupta et al., 2019\nh 1 \u2190 find max h\u2208H \u00b5(h), s.t. h(T ) =\u0125(T ) (1) and h 2 \u2190 find min h\u2208H \u00b5(h), s.t. h(T ) =\u0125(T ) (2) //T is an (\u00b5, )-specifying set for\u0125 8: if \u00b5(h 1 ) \u2212 \u00b5(h 2 ) \u2264 2\n) Determine \u2206(h 1 , h 2 ) = {x \u2208 X : h 1 (x) = h(x) or h 2 (x) =\u0125(x)} 12: while x\u2208\u2206(h1,h2) w(x) \u2264 1 do 13: Double weights w(x) for all x in \u2206(h 1 , h 2 ) 14: Update T \u2190 x \u2208 X : w(x) \u2265 \u03c4 x 15: Query h * on T 16: S \u2190 S \u222a T 17: if\u0125(T ) = h * (T ) then 18: return 1 2 (\u00b5(h 1 ) + \u00b5(h 2 ))\nTheorem 3.8. If the online learning oracle O makes a total of M mistakes, then with probability 1 \u2212 \u03b4, Algorithm 3 outputs\u03bc such that \u03bc \u2212 \u00b5(h * ) \u2264 , with its number of label queries bounded by:\nO Cost(H)M log |H|M \u03b4 log |X | .\nThe proof of Theorem 3.8 is deferred to Appendix D.5.\nIn a nutshell, it combines the following observations. First, Algorithm 3 has at most M outer iterations using the mistake bound guarantee of oracle O. Second, for each\u0125 in each inner iteration, its minimum (\u00b5, )specifying set has size at most Cost(H); this is based on a nontrivial connection between the optimal deterministic query complexity and (\u00b5, )-extended teaching dimension (see Definition D.9), which we present in Lemma D. Finally, in Appendix F, we empirically explore the performance of Algorithm 3 and active learning, and compare them with i.i.d sampling. As expected, our experiments confirm that under a fixed budget, Algorithm 3 is most effective at inducing a version space with a small \u00b5-diameter, and can thus provide the strongest manipulation-proofness guarantee.", "publication_ref": ["b24", "b21", "b17", "b6", "b5", "b13", "b22", "b27", "b1", "b14", "b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Statistical Limits of Estimation", "text": "In this section, we turn to direct estimation, the second of the two main guarantees we wish to have for our auditing algorithm. In particular, we focus on the statistical limits of direct estimation, where the goal is to design an auditing algorithm that can output\u03bc such that \u03bc \u2212 \u00b5(h * ) \u2264 with a small number of queries.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Separation between Estimation with and without Manipulation-proofness", "text": "To start, it is natural to contrast the guarantee ofmanipulation-proofness against -estimation accuracy. Indeed, if the two guarantees are one and the same, we may just apply our auditing algorithms developed to achieve MP for direct estimation as well.\nHere we look to answer the question of whether achieving MP is strictly harder, and we answer this question in the affirmative. Specifically, the following simple example suggests that MP estimation can sometimes require a much higher label complexity than direct estimation.\nExample 4.1. Let = 1 4 and n 1. X = {0, 1, . . . , n}, and x | x A = 0 \u223c Uniform({0}), and\nx | x A = 1 \u223c Uniform({1, . . . , n}). Let H = h : X \u2192 {\u22121, +1} , h(0) = \u22121 .\nFirst, as = 1 4 , the iid sampling baseline makes O(1) queries and ensures that it estimates \u00b5(h * ) with error at most with probability \u2265 0.9.\nHowever, for manipulation-proof estimation, at least \u2126(n) labels are needed to ensure that the queried dataset S satisfies diam \u00b5 (H(h * , S)) \u2264 . Indeed, let h * \u2261 \u22121. For any unlabeled dataset S of size \u2264 n/2, by the definition of H, there always exist h, h \u2208 H(h * , S), such that for all x \u2208 {1, . . . , n} \\ S, h(x) = \u22121 and h (x) = +1. As a re-\nsult, \u00b5(h) = 0 n \u2212 0 1 = 0, and \u00b5(h ) = |{1,...,n}\\S| n \u2212 0 1 \u2265 1 2 , which implies that diam \u00b5 (H(h * , S)) \u2265 1 2 > .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Randomized Algorithms for Direct Estimation", "text": "The separation result above suggests that different algorithms may be needed if we are only interested in efficient direct estimation. Motivated by our previous exploration, a first question to answer is whether randomization should be a key ingredient in algorithm design. That is, can a randomized auditing algorithm have a lower query complexity than that of the optimal deterministic algorithm? Using the example below, we answer this question in the affirmative.\nExample 4.2. Same as the setting of Example 4.1; recall that iid sampling, a randomized algorithm, estimates \u00b5(h * ) with error at most = 1 4 with probability \u2265 0.9; it has a query complexity of O(1).\nIn contrast, consider any deterministic algorithm A with label budget N \u2264 n 2 ; we consider its interaction history with classifier h 0 \u2261 \u22121, which can be summarized by a sequence of unlabeled examples S = x 1 , . . . , x N . Now, consider an alternative classifier h 1 such that h 1 (x) = \u22121 on S \u222a {0}, but h 1 (x) = +1 on {1, . . . , n} \\ S. By an inductive argument, it can be shown that the interaction history between A and h 1 is also S, which implies that when the underlying hypotheses h * = h 0 and h * = h 1 , A must output the same estimate\u03bc (see Lemma B.1 in Appendix B for a formal proof); however, \u00b5(h 0 ) \u2212 \u00b5(h 1 ) \u2265 1 2 , implying that under at least one of the two hypotheses, we must have \u03bc \u2212 \u00b5(h * ) \u2265 1 4 = . In summary, in this setting, a randomized algorithm has a query complexity of O(1), much smaller than \u2126(n), the optimal query complexity of deterministic algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Case Study: Non-homogeneous Linear Classifiers under Gaussian Populations", "text": "In this subsection, we identify a practically-motivated setting, where we are able to comprehensively characterize the minimax (randomized) active fairness auditing query complexity up to logarithmic factors. Specifically, we present a positive result in the form of an algorithm that has a query complexity of\u00d5 min(d, 1\n2 ) , as well as a matching lower bound that shows any (possibly randomized) algorithm must have a query complexity of \u2126 min(d,\n1 2 ) . Example 4.3. Let d \u2265 2 and X = R d . x | x A = 0 \u223c N(m 0 , \u03a3 0 ), whereas x | x A = 1 \u223c N(m 1 , \u03a3 1 ).\nLet hypothesis class H lin = h a,b (x) := sign( a, x + b) : a \u2208 R d , b \u2208 R be the class of non-homogenenous linear classifiers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recall that i.i.d sampling has a label complexity of O 1", "text": "2 ; on the other hand, through a membership query-based active learning algorithm (Algorithm 6 in Appendix E.2), we can approximately estimate \u00b5(h * ) (up to scaling) by doing dbinary searches, using active label queries. This approach incurs a total label complexity of\u00d5(d). Choosing the better of these two algorithms gives an active fairness auditing strategy of label complexity\u00d5 min(d, 1\n2 ) . We only present the main idea of Algorithm 6 here, with its full analysis deferred to Appendix E.2. Its core component is Algorithm 4 below, which label-efficiently estimates \u03b3(\nh * ) = P x\u223cN(0,I d ) (h * (x) = +1), with black- box label queries to h * (x) = sign( a * , x + b * ). Algo- rithm 4 is based on the following insights. First, observe that \u03b3(h * ) = \u03a6 b * a * 2 =: \u03a6(sr),\nwhere \u03a6 is the standard normal CDF, s := sign(b * ), and r :=\n1 d i=1 m \u22122 i , for m i := \u2212 b * a * i .\nOn the one hand, s can be easily obtained by querying h * on 0 (line 2). On the other hand, estimating r can be reduced to estimating each m i . However, some m i 's can be unbounded, which makes their estimation challenging. To get around this challenge, we prove the following lemma, which shows that it suffices to accurately estimate those m i 's that are not unreasonably large (i.e. m i 's for i \u2208 S, defined below):\nLemma 4.4. Let \u03b1 := 2d ln 1 and \u03b2 := 2d 5 2 (ln 1 ) 3 4 ( 1 ) 1 2 . Suppose r \u2264 \u03b1. If there is some S \u2282 [d], such that: 1. for all i / \u2208 S,|m i | \u2265 \u03b2, 2. for all i \u2208 S, |m i \u2212 m i | \u2264 ; then, 1 i\u2208Sm \u22122 i \u2212 r \u2264 2 .\nAlgorithm 4 carefully utilizes this lemma to estimate r. First, it tests whether for all i, h * (\u03b1e i ) = h * (\u2212\u03b1e i ); if yes, for all i, |m i | \u2265 \u03b1, and r \u2265 ln 1 , and \u03b3(h * ) is -close to 0 or 1 depending on the value of s (line 5). Otherwise, it must be the case that r \u2264 \u03b1. In this case, we go over each coordinate i, first testing whether |m i | \u2264 \u03b2 (line 8); if no, we skip this coordinate (do not add it to S); otherwise, we include i in S and estimate m i to precision using binary search (line 11). By the guarantees of Lemma 4.4, we have|sr \u2212 sr| \u2264 2 , which, by the 1\n\u221a 2\u03c0 -Lipschitzness of \u03a6, implies that \u03b3 \u2212 \u03b3(h * ) \u2264 . The total query complexity of Algorithm 4 is 1 + 2d + 2d + d log 2 \u03b2 =\u00d5(d).\nAlgorithm 4 ESTIMATE-POSITIVE: A label efficient estimation algorithm for \u03b3(h * ) for non-homogenoeus linear classifiers Require: query access to h * \u2208 H lin , target error .\nEnsure:\u03b3 such that \u03b3 \u2212 \u03b3(h * ) \u2264 .\n1: Let \u03b1 = 2d ln 1 , \u03b2 = 2d 5 2 (ln 1 ) 3 4 ( 1 ) 1 2 . 2: s \u2190 Query h * on 0 3: Query h * on \u03c1\u03b1e i : \u03c1 \u2208 {\u00b11} , i \u2208 [d] 4: if for all i \u2208 [d], h * (\u03b1e i ) = h * (\u2212\u03b1e i ) then 5: return 1 if s = +1, 0 if s = \u22121 //Otherwise, r \u2264 \u03b1 = 2d ln 1 6: S \u2190 \u2205 7: for i = 1, . . . , d do 8:\nQuery h * on \u03b2e i and \u2212\u03b2e i 9:\nif h * (\u03b2e i ) = h * (\u2212\u03b2e i ) then 10:\nS \u2190 S \u222a {i} //Use binary search to obtainm i , an estimate of m i = \u2212 b * a * i with precision 11:m i \u2190 BINARY-SEARCH(i, \u03b2, ) (Algorithm 5) 12:r \u2190 1 i\u2208Sm \u22122 i //r is an estimate of r 13: return \u03a6(sr) Algorithm 5 BINARY-SEARCH Require: i, \u03b2 such that h * (\u03b2e i ) = h * (\u2212\u03b2e i ), precision Ensure: m, an -accurate estimate of m i = \u2212 b ai 1: u \u2190 \u03b2, l \u2190 \u2212\u03b2 2: while u \u2212 l \u2265 do 3: m \u2190 u+l 2 4: Query h * on me i 5: if h * (me i ) = h * (le i ) then 6: l \u2190 m 7: else 8: u \u2190 m 9: return m\nFor the lower bound, we formulate a hypothesis testing problem, such that under hypotheses H 0 and H 1 , the \u00b5(h * ) values are approximately -separated. This is used to show that any active learning algorithm with label query budget \u2264 \u2126 min(d, 1\n2 ) cannot effectively distinguish H 0 and H 1 . Our construction requires a delicate analysis on the KL divergence between the observation distributions under the two hypotheses, and we refer the readers to Theorem E.3 for details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Distribution-Free Lower Bounds", "text": "Finally, in this subsection, we move beyond the Gaussian population setting and derive general query complexity lower bounds for randomized estimation algorithms that audit general hypothesis classes with finite VC dimension d. This result suggests that, when d 1 2 , or equivalently 1 \u221a d , there exists some hard data distribution and target classifier in H, such that active fairness auditing has a query complexity lower bound of \u2126( 12 ); that is, iid sampling is near-optimal. Theorem 4.5 (Lower bound for randomized auditing). Fix \u2208 (0, 1 40 ] and a hypothesis class H with VC dimension d \u2265 1600. For any (possibly randomized) algorithm A with label budget N \u2264 O(min(d, 1\n2 )), there exists a distribution D X over X and h * \u2208 H, such that A's output\u03bc when interacting with h * , satisfies:\nP \u03bc \u2212 \u00b5(h * ) > > 1 8\nThe proof of Theorem 4.5 can be found at Appendix E.1. The lower bound construction follows from a similar setting as in Example 4.1, except that we now choose h * in a randomized fashion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we initiate the study of the theory of queryefficient algorithms for auditing model properties of interest. We focus on auditing demographic parity, one of the canonical fairness notions. We investigate the natural auditing guarantee of estimation accuracy, and introduce a new guarantee based on the possibility of post-audit manipulation: manipulation-proofness. We identify an optimal deterministic algorithm, a matching randomized algorithm and develop upper and lower bounds that mark the performance that any optimal auditing algorithm must meet. Our first exploration of active fairness estimation seeks to provide a more complete picture of the theory of auditing. A natural next direction is to explore guarantees for other fairness notions (such as equalized odds). Indeed, how does one construct query-efficient algorithms when \u00b5 is a function of both h * (x) and y? Another natural question, motivated by the connection to disagreement-based active learning, is to design active fairness auditing algorithms based on some notion of disagreement with respect to \u00b5.\nLaber, E. S. and Nogueira, L. T. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Additional Related Works", "text": "Property Testing: Our notion of auditing that leverages knowledge of H is similar in theme to the topic of property testing (Goldreich et al., 1998;Ron, 2008;Balcan et al., 2012;Blum & Hu, 2018;Blanc et al., 2020;Blais et al., 2021) which tests whether h * is in H, or h * is far away from any classifier in H, given query access to h * . These works provide algorithms with testing query complexity of lower order than sample complexity for learning with respect to H, for specific hypothesis classes such as monomials, DNFs, decision trees, linear classifiers, etc. Our problem can be reduced to property testing by testing whether h * is in h \u2208 H : \u00b5(h) \u2208 [i , (i + 1) ] for all i \u2208 0, 1, . . . , 1 ; however, to the best of our knowledge, no such result is known in the context of property testing.\nFeature Minimization Audits: Rastegarpanah et al. ( 2021) study another notion of auditing, focusing on assessing whether the model is trained inline with the GDPR's Data Minimization principle. Specifically, this work evaluates the necessity of each individual feature used in the ML model, and this is done by imputing each feature with constant values and checking the extent of variation in the predictions. One commonality with our work, and indeed across all auditing works, is the concern with minimizing the number queries needed to conduct the audit.\nHerding for Sample-efficient Mean Estimation: Additionally, the estimation of DP may be viewed as estimating the difference of two means. Viewed in this light, herding (Xu et al., 2019) offers a way to use non-iid sampling to more efficiently estimate means. However, the key difference needed in herding is that h * , whose output is {\u22121, 1}, may be well-approximated by w, \u03c6(x) for some mapping \u03c6 known apriori. ", "publication_ref": ["b18", "b4", "b9", "b8", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "B. A General Lemma on Deterministic Query Learning", "text": "In this section, we present a general lemma inspired by Hanneke (2007), which are used in our proofs for establishing lower bounds on deterministic active fairness auditing algorithms. Lemma B.1. If an deterministic active auditing algorithm A with label budget N interacts with labeling oracle that uses classifier h 0 , and generates the following interaction history: (x 1 , h 0 (x 1 )), (x 2 , h 0 (x 2 )), . . . , (x N , h 0 (x N )) , and there exists a classifier h 1 such that h 1 (x) = h 0 (x) for all x \u2208 {x 1 , . . . , x N }. Then A, when interacting with h 1 , generates the same interaction history, and outputs the same auditing estimate; formally, S A,h1 = S A,h0 and\u03bc A,h1 =\u03bc A,h0 .\nProof. Recall from Section 1.1 that deterministic active auditing algorithm A can be viewed as a sequence of N + 1 functions f 1 , f 2 , . . . , f N , g, where {f i } N i=1 are the label query function used at each iteration, and g is the final estimator function. We show by induction that for steps i = 0, 1, . . . , N , the interaction histories of A with h 0 and h 1 agree on their first i elements.\nBase case. For step i = 0, both interaction histories are empty and agree trivially.\nInductive case. Suppose that the statement holds for step i, i.e. A, when interacting with both h 0 and h 1 , generates the same set of labeled examples\nS i = (x 1 , y 1 ), . . . , (x i , y i ) ,\nup to step i. Now, at step i + 1, A applies the query function f i+1 and queries the same example x i+1 = f i+1 (S i ). By assumption of this lemma, h 1 (x i+1 ) = h 0 (x i+1 ), which implies that the (i + 1)-st labeled example obtained when A interacts with h 1 ,\n(x i+1 , h 1 (x i+1 )) is identical to (x i+1 , h 1 (x i+1 )\n), the (i + 1)-st example when A interacts with h 0 . Combined with the inductive hypotheses that the two histories agree on the first i examples, we have shown that A, when interacting with h 0 and h 1 , generates the same set of labeled examples\nS i+1 = (x 1 , y 1 ), . . . , (x i , y i ), (x i+1 , y i+1 ) up to step i + 1.\nThis completes the induction.\nAs the interaction histories A with h 0 and h 1 are identical, the unlabeled data part of the history are identical, formally, S A,h1 = S A,h0 . In addition, as in both interactive processes, A applies deterministic function g to the same interaction history of length N to obtain estimate\u03bc, we have\u03bc A,h1 =\u03bc A,h0 .", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "C. Deferred Materials from Section 1", "text": "The following lemma formalizes the idea that PAC learning with O( ) error is sufficient for fairness auditing, given that p = min Pr\nD X (x A = 0), Pr D X (x A = 1) is \u2126(1). Lemma C.1. If h is such that P(h(x) = h * (x)) \u2264 \u03b1, then \u00b5(h) \u2212 \u00b5(h * ) \u2264 \u03b1 p .\nProof. First observe that\nPr(h(x) = +1 | x A = 0) \u2212 Pr(h * (x) = +1 | x A = 0) \u2264 Pr(h(x) = h * (x) | x A = 0) = Pr(h(x) = h * (x), x A = 0) Pr(x A = 0) \u2264 Pr(h(x) = h * (x), x A = 0) p ,\nwhere the first inequality is by triangle inequality; the second inequality is by the definition of p. Symmetrically, we have\nPr(h(x) = +1 | x A = 1) \u2212 Pr(h * (x) = +1 | x A = 1) \u2264 Pr(h(x) =h * (x),x A =1) p .\nAdding up the two inequalities, we have:\n\u00b5(h) \u2212 \u00b5(h * ) \u2264 Pr(h(x) = +1 | x A = 0) \u2212 Pr(h * (x) = +1 | x A = 0) + Pr(h(x) = +1 | x A = 1) \u2212 Pr(h * (x) = +1 | x A = 1) \u2264 Pr(h(x) = h * (x), x A = 0) p + Pr(h(x) = h * (x), x A = 1) p = Pr(h(x) = h * (x)) p \u2264 \u03b1 p .\nD. Deferred Materials from Section 3 D.1. Proof of Theorems 3.1 and 3.2\nProof of Theorem 3.1. Suppose Algorithm 1 (denoted as A throughout the proof) interacts with some target classifier h * \u2208 H.\nWe will show the following claim: at any stage of A, if the set of labeled examples L shown so far induces a version V = H[L], then A will subsequently query at most Cost(V ) more labels before exiting the while loop.\nNote that Theorem 3.1 follows from this claim by taking L = \u2205 and V = H: after Cost(H) label queries, it exits the while loop, which implies that, the queried unlabeled examples S A,h * induces version space V = H(h * , S A,h * ) with\nmax h\u2208V \u00b5(h) \u2212 min h\u2208V \u00b5(h) = diam \u00b5 (V ) \u2264 2 .\nAlso, note that h * \u2208 V ; this implies that \u00b5(h * ) \u2208 [min h\u2208V \u00b5(h), max h\u2208V \u00b5(h)]. Combining these two observations, we have\n\u03bc \u2212 \u00b5(h * ) \u2264 1 2 max h\u2208V \u00b5(h) \u2212 min h\u2208V \u00b5(h) \u2264 .\nWe now come back to proving this claim by induction on Cost(V ).\nBase case. If Cost(V ) = 0, then A immediately exits the while loop without further label queries.\nInductive case. Suppose the claim holds for all V such that Cost(V ) \u2264 n. Now consider a version space V with Cost(V ) = n + 1. In this case, first recall that\nCost(V ) = 1 + min x\u2208X max y\u2208{\u22121,+1} Cost (V y x ) , i.e. min x\u2208X max y\u2208{\u22121,+1} Cost (V y x ) = Cost(V ) \u2212 1 = n.\nAlso, recall that by the definition of Algorithm 1, when facing version space V , the next query example x 0 chosen by A is a solution of the following minimax optimization problem:\nx 0 = argmin x\u2208X max y\u2208{\u22121,+1} Cost (V y x ) ,\nwhich implies that max y\u2208{\u22121,+1} Cost (V y x ) = n. Specifically, this implies that the version space at the next iteration,\nV h * , {x 0 } = V h * (x0) x0\n, satisfies that Cost(V h * , {x 0 } ) \u2264 n. Combining with the inductive hypothesis, we have seen that after a total of 2. has final version space H(h, S A,h ) with \u00b5-diameter > 2 .\n1 + Cost(V h * , {x 0 } ) \u2264 n + 1 = Cost(V )\nThe theorem follow from this claim by taking L = \u2205. To see why, we let h \u2208 H[\u2205] = H be the classifier described in the claim. First, note that there exists some other classifier h = h in the final version space H(h, S A,h ), such that \u00b5(h ) \u2212 \u00b5(h) > 2 . For such h , h (S A,h ) = h(S A,h ). Therefore, by Lemma B.1, S A,h = S A,h (which we denote by S subsequently), and h and h have the exact same labeling on S, and\u03bc A,h =\u03bc A,h . This implies that, for A, at least one of the following must be true:\n\u03bc A,h \u2212 \u00b5(h) > or \u03bc A,h \u2212 \u00b5(h ) > ,\nshowing that it does not guarantee an estimation error \u2264 under all target h \u2208 H.\nWe now turn to proving the above claim by induction on A's remaining label budget N . In the following, denote by\nV = H[L].\nBase case. If N = 0 and Cost(V ) \u2265 1, then A at this point has zero label budget, which means that it is not allowed to make more queries. In this case, S A,h = L, and H(S\nA,h , h) = V . As Cost(V ) \u2265 1, we know that max h1,h2\u2208H(h,S A,h ) \u00b5(h 1 ) \u2212 \u00b5(h 2 ) = max h1,h2\u2208V \u00b5(h 1 ) \u2212 \u00b5(h 2 ) > 2 .\nThis completes the proof of the base case.\nInductive case. Suppose the claim holds for all N \u2264 n. Now, suppose in the learning process, A has a remaining label budget N = n + 1, and has obtained labeled examples\nL such that V = H[L] satisfies Cost(V ) \u2265 n + 2.\nLet x be the next example A queries. By the definition of Cost, there exists some y \u2208 {\u22121, +1}, such that\nCost H L \u222a (x, y) = Cost (V y x ) \u2265 Cost(V ) \u2212 1 \u2265 n + 1,\nand after making this query, the learner has a remaining label budget of N \u2212 1 = n.\nBy inductive hypothesis, there exists some h \u2208 H L \u222a (x, y) , such that when A interacts with h subsequently (with obtained labeled examples L \u222a (x, y) and label budget < n), the final unlabeled dataset S A,h satisfies\ndiam \u00b5 H(h, S A,h ) = max h1,h2\u2208H(h,S A,h ) \u00b5(h 1 ) \u2212 \u00b5(h 2 ) > 2 .\nIn addition, when interacting with h, A obtains the example sequence L, (x, y) in its first |L| + 1 rounds of interaction, which implies that it obtains the example sequence L in its first |L| rounds of interaction with h. This completes the induction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Proof Sketch of Proposition 3.3", "text": "Proof sketch. Let S 1 and S 2 be O 1 2 ln |H| i.i.d samples from D X | x A = 1 and D X | x A = 0, respectively. Defin\u00ea\n\u00b5(h, S 1 , S 2 ) = Pr x\u223cS1 (h(x) = +1) \u2212 Pr x\u223cS2 (h(x) = +1).\nHoeffding's inequality and union bound guarantees that with probability at least 1 2 , \u2200h \u2208 H, |\u03bc(h, S 1 , S 2 ) \u2212 \u00b5(h)| \u2264 . Now consider the following deterministic algorithm A:\n\u2022 Let n = O 1 2 ln |H| ; \u2022 Find (the lexicographically smallest) S 1 and S 2 in X n , such that \u2200h \u2208 H, \u03bc(h, S 1 , S 2 ) \u2212 \u00b5(h) \u2264 .\n(\n)3\nThis optimization problem is feasible, because as we have seen, a random choice of S 1 , S 2 makes Equation (3) happen with nonzero probability. \u2022 Return\u03bc(h * , S 1 , S 2 ) with 2n label queries to examples in S 1 \u222a S 2 .\nBy its construction, A queries 2n = O 1 2 ln |H| labels and returns\u03bc that is -close to \u00b5(h * ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.3. Proof of Proposition 3.4", "text": "Before we prove Proposition 3.4, we first recall the well-known Bernstein's inequality:\nLemma D.1 (Bernstein's inequality). Given a set of iid random variables Z 1 , . . . , Z n with mean \u00b5 and variance \u03c3 2 ; in addition,\n|Z i | \u2264 b almost surely. Then, with probability 1 \u2212 \u03b4, 1 n n i=1 Z i \u2212 \u00b5 \u2264 2\u03c3 2 ln 2 \u03b4 n + b ln 2 \u03b4 3n .\nProof of Proposition 3.4. We will analyze Algorithm 2, a derandomized version of the Phased CAL algorithm (Hsu, 2010, Chapter 2). To prove this proposition, using Theorem 3.2, it suffices to show that Algorithm 2 has a deterministic label complexity bound of O \u03b8( ) \u2022 ln |H| \u2022 ln 1 .\nWe first show that for every n, the optimization problem in line 5 is always feasible. To see this, observe that if we draw S n = {x 1 , . . . , x mn } as sample of size m n drawn iid from D X , we have:\n1. By Bernstein's inequality with\nZ i = I(x i \u2208 DIS(V n )), with probability 1 \u2212 1 4 , Pr Sn (x \u2208 DIS(V n )) \u2264 Pr D X (x \u2208 DIS(V n )) + 2 Pr D X (x \u2208 DIS(V n )) ln 8 m n + ln 8 3m n \u22642 Pr D X (x \u2208 DIS(V n )) + ln 8 m n .\nwhere the second inequality uses Arithmetic Mean-Geometric Mean (AM-GM) inequality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "By Bernstein's inequality and union bound over h, h \u2208 H, we have with probability", "text": "1 \u2212 1 4 , \u2200h, h \u2208 H : Pr D X (h(x) = h (x)) \u2264 Pr Sn (h(x) = h (x)) + 4 Pr D X (h(x) = h (x)) ln |H| m n + 4 ln |H| 3m n in which, \u2200h, h \u2208 H : Pr Sn (h(x) = h (x)) = 0 =\u21d2 Pr D X (h(x) = h (x)) \u2264 16 ln |H| m n .\nBy union bound, with nonzero probability, the above two condition hold simultaneously, showing the feasibility of the optimization problem.\nWe then argue that for all n, V n+1 \u2286 B(h * , 16 ln |H| mn ). This is because for all h \u2208 V n+1 , it and h * are both in V n and therefore they agree on S n \\ T n ; on the other hand, h and h * agree on T n by the definition of of V n+1 . As a consequence, Pr Sn (h(x) = h * (x)) = 0, which implies that Pr D X (h(x) = h * (x)) \u2264 16 ln |H| mn . As a consequence, for all h \u2208\nV N +1 , Pr(h(x) = h * (x)) \u2264 16 ln |H| m N \u2264 p , implying that \u00b5(h) \u2212 \u00b5(h * ) \u2264 (recall Lemma C.1).\nWe now turn to upper bounding Algorithm 2's label complexity:\nN n=1 |T n | = N n=1 m n \u2022 (2 Pr D X (x \u2208 DIS(V n )) + ln 8 m n ) \u2264 N n=1 m n \u2022 (\u03b8( ) \u2022 16 ln |H| m n \u2022 2 p + ln 8 m n ) \u2264O \u03b8( ) \u2022 ln |H| \u2022 ln 1 ,\nwhere the inequality uses the observation that for every n \u2208 [N ],\nPr D X (x \u2208 DIS(V n )) \u2264 Pr D X x \u2208 DIS(B(h * , 16 ln |H| m n )) \u2264 \u03b8( p 2 ) \u2022 16 ln |H| m n \u2264 \u03b8( ) \u2022 16 ln |H| m n \u2022 2 p ,\nwhere the second inequality is from the definition of disagreement coefficient (recall Section 1.1), and the last inequality is from a basic property of disagreement coefficient (Hanneke, 2014, Corollary 7.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.4. Proof of Proposition 3.5", "text": "We first prove the following theorem that gives a decision tree-based characterization of the Cost(\u2022) function. Connections between active learning and optimal decision trees have been observed in prior works (e.g. Laber & Nogueira, 2004;Balcan et al., 2012).\nDefinition D.2. An example-based decision tree T for (instance domain, hypothesis set) pair (X , V ) is such that:\n1. T 's internal nodes are examples in X ; every internal node has two branches, with the left branch labeled as +1 and the right labeled as \u22121.\n2. Every leaf l of T corresponds to a set of classifiers V l \u2282 V , such that all h \u2208 V l agree with the examples that appear in the root-to-leaf path to l. Formally, suppose the path from the root to leaf l is an alternating sequence of examples and labels x 1 , y 1 , . . . , x n , y n , then for every i \u2208\n[n], h(x i ) = y i . Definition D.3. Fix D X . An example-based decision tree T is said to (\u00b5, )-separate a hypothesis set V , if for every leaf l of T , V l satisfies diam \u00b5 (V l ) \u2264 2 .\nTheorem D.4. Given a version space V , Cost(V ) is the minimum depth of all decision trees that (\u00b5, )-separates V .\nProof. We prove the theorem by induction on Cost(V ).\nBase case. If Cost(V ) = 0, then diam \u00b5 (V ) \u2264 2 . Then there exists a trivial decision tree (with leaf only) of depth 0 that (\u00b5, )-separates V , which is also the smallest depth possible.\nInductive case. Suppose the statement holds for any V such that Cost(V ) = n. Now consider V such that Cost(V ) = n + 1.\n1. We first show that there exists a decision tree of depth n + 1 that (\u00b5, )-separates V . Indeed, pick x = argmin x\u2208X max y Cost(V y x ).\nWith this choice of x, we have both Cost(V \u22121 x ) and Cost(V +1\nx ) are equal to n. Therefore, by inductive hypothesis for V \u22121 x and V +1\nx , we can construct decision trees T \u2212 and T + of depths n that (\u00b5, )-separate the two hypothesis classes respectively. Now define T to be such that it has root node x, and has left subtree T + and right subtree T \u2212 , we see that T has depth n + 1 and (\u00b5, )-separates V .\n2. We next show that any decision tree of depth n does not (\u00b5, )-separate V . Indeed, assume for the sake of contradiction that such tree T exists. Then consider the example x at the root of the tree; by the definition of Cost, one of Cost(V \u22121 x ) and Cost(V +1\nx ) must be \u2265 n. Without loss of generality, assume that V = V +1\nx is such that Cost(V ) \u2265 n. Therefore, there must exists some subset V \u2282 V such that Cost(V ) = n. Applying the inductive hypothesis on V , no decision tree of depth n \u2212 1 can (\u00b5, )-separate V . This contradicts with the observation that the left subtree of T , which is of depth n \u2212 1, (\u00b5, )-separates V .\nWe now restate a more precise version of Proposition 3.5. First we define the computational task of computing a 0.3 ln(|H|)approximation of Cost(H) by the following problem:\nProblem Minimax-Cost (MC): Input: instance space X , hypothesis class H, data distribution D X , precision parameter . Output: a number L such that Cost(H) \u2264 L \u2264 0.3 ln(|H|)Cost(H).\nProposition D.5 (Proposition 3.5 restated). If there is an algorithm that solves Minimax-Cost in poly(|H|, |X |, 1/ ) time, then NP \u2286 TIME(n O(log log n) ).\nProof of Proposition D.5. Our proof takes after (Laber & Nogueira, 2004)'s reduction from set cover (SC) to Decision Tree Problem (DTP). Here, we reduce from SC to the Minimax-Cost problem (MC), i.e. computing Cost(H) for a given hypothesis class H, taking into account the unique structure of active fairness auditing. Specifically, the following gap version of SC's decision problem has been shown to be computationally hard 4 : Problem Gap-Set-Cover (Gap-SC): Input: a universe U = {u 1 , ..., u n } of size n with n \u2265 10, and a family of subsets C = {C 1 , ..., C m }, and an integer k, such that either of the following happens:\n\u2022 Case 1: OPT SC \u2264 k, \u2022 Case 2: OPT SC \u2265 0.99k ln n, where OPT SC denotes the minimum set cover size of (U, C).\nOutput: 1 or 2, which case the instance is in. Specifically, it is well-known that obtaining a polynomial time algorithm for the above decision problem 5 on minimum set cover would imply that NP \u2286 TIME(n O(log log n) ) (Feige, 1998), which is believed to be false.\nTo start, recall that an instance of Gap-SC problem I SC = (U, C, k); an instance of the MC problem I MC = (H, X , D X , ).\nWith this, we define a coarse reduction \u03b2 that constructs a MC-instance from a Gap-SC instance with universe U = {u 1 , ..., u n } and sets C = {C 1 , ..., C m }, which will be refined shortly:\n1. Let H = {h 0 , h 1 , . . . , h n }, where h 0 (x) \u2261 \u22121 always, and for all j \u2208 [n], h j corresponds to u j (the definitions of h j 's will be given shortly).\n2. Create example x 0 such that for all h \u2208 H, h(x 0 ) = \u22121.\n3. For every i \u2208 [m], create basis example x i to correspond to C i such that for every j \u2208 [n], h j (x i ) = 1 iff u j \u2208 C i . 4. For each set C i , create |C i | \u2212 1 auxiliary x's as follows: Given set C i with |C i | = s i that corresponds to {h i1 , .., h isi }, create a balanced binary tree T i with each leaf corresponding to a h ij . Create an auxiliary example associated with each internal node in T i as follows: for each internal node in the tree, define the corresponding auxiliary sample x such that its label is +1 under all the classifiers in the leaves of the subtree rooted at its left child, and its label is \u22121 under all remaining classifiers in H. The total number of auxiliary x's is \u2264 m \u2022 (n \u2212 1).\n5. Define X as the union of the example sets constructed in the above three items, which has at most N \u2264 mn + 1 examples. Define D X to be such that: x | x A = 0 \u223c Uniform(X \\ {x 0 }), and x | x A = 1 \u223c Uniform({x 0 }), and set = 1/(2N ). With this setting of , for every h \u2208 H such that\nh = h 0 , \u00b5(h) \u2212 \u00b5(h 0 ) = Pr(h(x) = +1 | x A = 0) \u2212 Pr(h 0 (x) = +1 | x A = 0) \u2265 1 N \u22121 > 2 .\nRecall that OPT SC is defined as the size of an optimal solution for SC instance (U, C); we let OPT MC denote the height of the tree corresponding to the optimal query strategy for the MC instance I MC obtained through reduction \u03b2. We have the following result:\nLemma D.6. OPT SC \u2264 OPT MC \u2264 OPT SC + max C\u2208C log |C|.\nProof. Let k = OPT SC . We show the two inequalities respectively.\n1. By Theorem D.4, it suffices to show that any example-based decision tree T that (\u00b5, )-separates H must have depth at least k. To see this, first note that by item 5 in the reduction \u03b2 and the definition of (\u00b5, )-separation, the leaf in T that contains h 0 must not contain other hypotheses in H. In addition, as h 0 \u2261 \u22121, h 0 must lie in the rightmost leaf of T . Now to prove the statement, we know that the examples along the rightmost path of T corresponds to a collection of sets that form a set cover of C. It suffices to show that this set cover has size no greater than the set cover of I SC . This is because the examples along the rightmost path are either x i 's, which correspond to some set in C, or auxiliary examples which correspond to some subset of a set in C. A set cover instance with U and C where C comprises of sets from C and subsets of sets from C will not have a smaller set cover.\nTherefore, the length of the path from the root to the rightmost leaf is at least k, the size of the smallest set cover of the original SC instance I SC .\n2. Let an optimal solution for I SC be G = {i 1 , ..., i k }. Below, we construct an example-based decision tree T of depth k + max C\u2208C log |C| that (\u00b5, )-separates H:\nLet the rightmost path of T contain nodes corresponding to x i1 , ..., x i k (the order of these are not important). At level l = 1, ..., k, the left subtree of x i l is defined to be T i l as defined in step 4 of reduction \u03b2. Note that this may result in T with potentially empty leaves, in that for some h covered by multiple x i l 's, it only appears in x io where o = min l : h(x i l ) = +1 .\nWe will prove that by the above construction, T (\u00b5, )-separates H, as every leaf corresponds to a version space V that is a singleton set (and thus has diam \u00b5 (V ) = 0 \u2264 2 ):\n(a) For all but the rightmost leaf, this holds by the construction of T i l 's. (b) For the rightmost leaf, we will show that only h 0 is in the version space. Since G is a set cover, we have that\n\u222a k l=1 C i l = U . Therefore, \u2200j \u2208 [n], \u2203l \u2208 [k] such that u j \u2208 C i l \u21d4 h j (x i l ) = 1 by construction.\nThis implies that the all zero labeling of x i1 , ..., x i k can only correspond to h 0 . Therefore, the version space at the rightmost leaf V satisfies |V | = {h 0 }.\nRecall from Theorem D.4 that the depth of T upper bounds OPT MC . T 's maximum root to leaf path is of length at most k + max C\u2208C log |C|.\nBuilt from \u03b2, we now construct an improved gap preserving reduction \u03b2 , defined as follows. Given any Gap-SC instance I SC = (U, C, k) with universe U = {u 1 , ..., u n } and sets C = {C 1 , ..., C m }:\n1. Take constant z = log n. Construct a Gap-SC instance I SC,z = (U z , C z , kz), containing z copies of the original set covering instance:\nU z = {u 1 1 , . . . , u 1 n , . . . , u z 1 , . . . , u z n }, C z = {C 1 , . . . , C zm }, where C (p\u22121)m+i = {u p i1 , . . . , u p isi } for p \u2208 [z], i \u2208 [m]. Note that OPT SC,z = kOPT SC .\n2. Apply reduction \u03b2 to obtain I MC,z from I SC,z . Now, we will argue that \u03b2 is a gap-preserving reduction:\n1. Suppose the original Gap-SC instance I SC = (U, C, k) is in case 1, i.e., OPT SC \u2264 k. Then, OPT SC,z \u2264 kz. By Lemma D.6, OPT MC,z \u2264 kz + max C\u2208C z log |C| \u2264 kz + log n \u2264 z(k + 1) \u2264 2zk.\n2. Suppose the original Gap-SC instance I SC = (U, C, k) is in case 2, i.e., OPT \u2265 0.99k ln n. Then, OPT SC,z \u2265 0.99zk ln n, which by Lemma D.6, yields that OPT MC,z \u2265 0.99zk ln n.\nNow suppose that there exists an algorithm A that solves the MC problem in poly(|H|, |X |, 1 ) time. We propose the following algorithm A that solves the Gap-SC problem in polynomial time, which, as mentioned above, implies that NP \u2286 TIME(n O(log log n) ):\nInput: I SC = (U, C, k).\n\u2022 Apply \u03b2 on I SC to obtain an instance of MC, I MC,z \u2022 Let L \u2190 A(I MC,z ). Output 1 if L \u2264 0.7zk ln n, and 2 otherwise.\nCorrectness. As seen above, if I SC is in case 1, then OPT MC,z \u2264 2zk. For n \u2265 10, by the guarantee of A, L \u2264 0.3 ln |H|\u2022 OPT MC,z \u2264 0.6 ln(n log n) \u2022 zk \u2264 0.7zk ln n, and A outputs 1. Otherwise, I SC is in case 2, then OPT MC,z \u2265 0.99zk ln n, and by the guarantee of A, L \u2265 0.99zk ln n > 0.7zk ln n, and A outputs 2. The following definitions are inspired by the teaching and exact active learning literature (Heged\u0171s, 1995;Hanneke, 2007). Definition D.7 ((\u00b5, )-specifying set). Fix hypothesis class H and any function h : X \u2192 Y, 6 a set of unlabeled examples S is said to be a (\u00b5, )-specifying set for h and H, if \u2200h 1 , h 2 \u2208 H(h, S) |\u00b5(h 1 ) \u2212 \u00b5(h 2 )| \u2264 2 . Definition D.8 ((\u00b5, )-extended teaching dimension). Fix hypothesis class H and any function h : X \u2192 Y, define t(h, H, \u00b5, ) as the size of the minimum (\u00b5, )-specifying set for h and H, i.e. it is the optimal solution of the following optimization problem (OP-h):\nTime complexity. In I MC,z , |X | \u2264 (mz \u2022 nz + 1) = O(mn log 2 n), |H| = nz = n log n, and = 1 2N = 1 2(mz\u2022nz+1) = \u2126( 1 mn log 2 n ). As A runs in time O(poly(|X |, |H|, 1 )), A\nmin |S|, s.t.\u2200h 1 , h 2 \u2208 H(h, S) |\u00b5(h 1 ) \u2212 \u00b5(h 2 )| \u2264 2\nDefinition D.9. We define the \u00b5-extended teaching dimension XTD(H, \u00b5, ) := max h:X \u2192Y t(h, H, \u00b5, ).\nThe improper teaching dimension is related to Cost(H) in that: Lemma D.10.\nXTD(H, \u00b5, ) \u2264 Cost(H).\nProof. Let h 0 = argmax h:X \u2192Y t(h, H, \u00b5, ). Let k denote t(h 0 , H, \u00b5, ) \u2212 1. It suffices to show that Cost(H) \u2265 k. To see this, first note that\nCost(H) = 1 + min x max y Cost(H[(x, y)]) \u2265 1 + min x1\u2208X Cost(H[(x, h 0 (x))]) \u2265 2 + min x1\u2208X min x2\u2208X Cost(H[{(x 1 , h 0 (x 1 )), (x 2 , h 0 (x 2 ))}])\n6 Note that h is allowed to be outside H.\nWe can repeatedly unroll the above expression as long as diam \u00b5 (H[{(x 1 , h 0 (x 1 )), . . . , (x i , h 0 (x i ))]) is at least > 2 . After unrolling k \u2212 1 times where U k\u22121 = x 1 , . . . , x k\u22121 , we have\nCost(H) \u2265 k \u2212 1 + min U k\u22121 Cost(H(h 0 , U k\u22121 )).\nBy the definition of t(h, H, \u00b5, ), for any U with\nU \u2264 k \u2212 1, there exists h , h \u2208 H(h 0 , U ) such that |\u00b5(h ) \u2212 \u00b5(h )| > \u21d2 diam \u00b5 (H(h 0 , U )) > . Thus, for any unlabeled dataset U k\u22121 of size k \u2212 1, Cost(H(h 0 , U k\u22121 )) \u2265 1. Therefore, Cost(H) \u2265 k.", "publication_ref": ["b4", "b16", "b24", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "D.5.2. PROOF OF THEOREM 3.8", "text": "Proof. We prove the theorem as follows:\nCorrectness. Observe that right before Algorithm 3 returns, it must execute lines 9 and 17. Since the condition on line 17 is also satisfied, the dataset T must be such that\u0125(T ) = h * (T ). Combined with the definitions of optimization problems ( 1) and ( 2), this implies that, the h 1 and h 2 used in line 9 right before return satisfy that\n\u00b5(h 1 ) = min h\u2208H(h * ,T ) \u00b5(h), \u00b5(h 2 ) = max h\u2208H(h * ,T ) \u00b5(h). Therefore, \u00b5(h * ) \u2208 [min h\u2208H(h * ,T ) \u00b5(h), max h\u2208H(h * ,T ) \u00b5(h)] = [\u00b5(h 1 ), \u00b5(h 2 )]. Furthermore, by line 9, \u00b5(h 1 ) \u2212 \u00b5(h 2 ) \u2264 2 . Hence,\u03bc, the output of Algorithm 3, satisfies that, \u03bc \u2212 \u00b5(h * ) = 1 2 \u00b5(h 1 ) + \u00b5(h 2 ) \u2212 \u00b5(h * ) \u2264 .\nLabel complexity. We now bound the label complexity of the algorithm, specifically, in terms of XTD(H, \u00b5, ).\nFirst, at the end of the t-th iteration of the outer loop, the newly collected dataset T t must be such that \u2203x \u2208 T t and h(x) = h * (x). As O has a mistake bound of M , the total number of outer loop iterations, denoted by N , must be most M . In addition, by Lemma D.11 given below, with probability\n1 \u2212 \u03b4/M , |T t | \u2264 O XTD(H, \u00b5, ) \u2022 log |H|M \u03b4 log |X | .\nTherefore, by a union bound, with probability 1 \u2212 \u03b4, the total number of label queries made by Algorithm 3 is at most\nN t=1 |T t | \u2264 O M \u2022 XTD(H, \u00b5, ) \u2022 log |H|M \u03b4 log |X | .\nLemma D.11. For every outer iteration of Algorithm 3, with probability \u2265 1 \u2212 \u03b4 M , T , the dataset at the end of this iteration,\nsatisfies|T | \u2264 O XTD(H, \u00b5, ) \u2022 log |H|M \u03b4 log |X | .\nProof. The inner loop is similar to the \"black-box teaching\" algorithm of (Dasgupta et al., 2019) except that we are teaching \u00b5(\u0125) as opposed to\u0125 itself. Although (Dasgupta et al., 2019)'s algorithm was originally designed for exact (interactive) teaching, it implicitly gives an oracle-efficient algorithm for approximately computing the minimum set cover; we will use this insight throughout the proof. As the analysis of (Dasgupta et al., 2019) is only on the expected number of teaching examples, we use a different filtration to obtain a high probability bound over the number of teaching examples.\nFirst we setup some useful notations for the proof. let X = {x 1 , . . . , x m }. Recall that \u03bb = ln |H| 2 M \u03b4 . Let W i (x) denote the weight of point x \u2208 X (denoted by w(x) in the algorithm) at the end of round i of the inner loop and let \u03c4 xj be the exponentially-distributed threshold associated with x j . Define random variable U i,j = 1{\u03c4 xj > W i (x j )}. Let M i denotes the number of teaching examples selected in the ith round of doubling; it can be seen that M i = j\u2208[m] U i,j . Also define (i, j) (i , j ) iff (i, j) precedes (i , j ) lexicographically.", "publication_ref": ["b14", "b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Define two filtrations:", "text": "1. Let F i,j be the sigma-field of all indicator events {U i ,j : (i , j ) (i, j)}. As a convention, F i,0 := F i\u22121,m .\n2. Let F i be the sigma-field of all indicator events {U i ,j : j \u2208 [m], 1 \u2264 i \u2264 i}; this is the filtration used by (Dasgupta et al., 2019). It can be easily seen that F i = F i,m .\nDefine Y i,j = (i ,j ) (i,j) Z i ,j , where\nZ i,j = U i,j \u2212 E U i,j | F i,j\u22121 \u2208 [\u22121, +1]. Then Y i,j is a martingale as E[Y i,j |F i,j\u22121 ] = E[Z i,j |F i,j\u22121 ] + E[Y i,j\u22121 |F i,j\u22121 ] = Y i,j\u22121 .\nLet N be the total number of rounds, which by item 1 of Lemma D.13, is O(XTD(H, \u00b5, ) ln |X |) (Lemma 4 of (Dasgupta et al., 2019)) with probability 1. We may then apply Freedman's inequality (Lemma D.12): since Y i,j \u2212 Y i,j\u22121 = Z i,j \u2264 1 almost surely, for any s and any \u03c3 2 > 0,\nPr \uf8eb \uf8ed \u2203n, m, Y nm \u2265 s, (i,j) (n,m) E[Z 2 ij |F i(j\u22121) ] \u2264 \u03c3 2 \uf8f6 \uf8f8 \u2264 exp \u2212 s 2 2(\u03c3 2 + s/3) (4)\nNext, we let \u03c3 2 = \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)); we have for any n, m:\n(i,j) (n,m) E[Z 2 ij |F i(j\u22121) ] = (i,j) (n,m) E[U 2 ij |F i(j\u22121) ] \u2212 E[U ij |F i(j\u22121) ] 2 \u2264 (i,j) (n,m) E[U 2 ij |F i(j\u22121) ] = (i,j) (n,m) E[U ij |F i(j\u22121) ] = n i=1 E Fi\u22121 [M i ] \u2264 \u03bb x\u2208X W n (x) (Lemma D.14) \u2264 \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) = \u03c3 2 . (Lemma D.13) Meanwhile, we choose s = 1 6 log( 1 \u03b4 ) + 2\u03c3 2 log 1 \u03b4 + 1 6 log( 1 \u03b4 ) = O ln 1 \u03b4 \u03c3 + ln 1\n\u03b4 , which ensures that the right hand side of Eq. (4) is at most \u03b4. Thus, by Equation (4), we have with probability 1 \u2212 \u03b4, for all n, m,\nY nm = (i ,j ) (n,m) U i j \u2212 n i=1 E Fi\u22121 [M i ] \u2264 O ln 1 \u03b4 \u03c3 + ln 1 \u03b4 .\nAlso, using Lemma D.14 and D.13, with probability 1,\nN i=1 E Fi\u22121 [M i ] \u2264 \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)). Therefore, for Y N m in particular, Y N m \u2264 O \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) + \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) ln(1/\u03b4) + ln(1/\u03b4) = O \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) + ln 1 \u03b4 = O XTD(H, \u00b5, ) ln(|X |) ln((|H|M )/\u03b4) . Lemma D.12 (Freedman's Inequality). Let martingale {Y k } \u221e k=0 with difference sequence {X k } \u221e k=0 be such that X k \u2264 R a.s for all k and Y 0 = 0. Let W k = k j=1 E j\u22121 [X 2 j ].\nThen, for all t \u2265 0 and \u03c3 2 > 0:\nPr(\u2203k \u2265 0 : Y k \u2265 t \u2227 W k \u2264 \u03c3 2 ) \u2264 exp \u2212 t 2 /2 \u03c3 2 + Rt/3 .\nLemma D.13. For any outer iteration of Algorithm 3:\n1. The number of inner loop iterations is at most XTD(H, \u00b5, ) \u2022 log(2|X |).\n2. At any point in the inner loop, we have that, x\u2208X w(x) \u2264 1 + XTD(H, \u00b5, ) \u2022 log(2|X |).\nProof. The proof is very similar to Dasgupta et al. (2019, Lemma 4) with some differences; for completeness, we include a proof here.\nWe first prove the second item. First, note that at any point of the algorithm, for all x, w(x) \u2264 2. Let S * (\u0125) be the optimal solution of optimization problem (OP-\u0125) -we have |S * (\u0125)| = t(\u0125, H, \u00b5, ) \u2264 XTD(H, \u00b5, ). Note that every time when line 13 is called, by the feasibility of S * (\u0125) with respect to (OP-\u0125), \u2206(h 1 , h 2 ) \u2229 S * (\u0125) = \u2205, therefore, the weight of some element x \u2208 S * (\u0125) gets doubled. This implies that the total number of times line 13 is executed is at most\n|S * (\u0125)| \u2022 log(2|X |).\nOtherwise, if the number of time line 13 is executed is \u2265 |S * (\u0125)| \u2022 log(2|X |) + 1, by the pigeonhole principle, there must exist some element x \u2208 S * (\u0125) whose weight exceeds 1, which is a contradiction.\nFinally, note that each weight doubling only increases the total weight by \u2264 1, we have the final total weight is at most\n1 + 1 \u2022 |S * (\u0125)| \u2022 log(2|X |) \u2264 1 + XTD(H, \u00b5, ) \u2022 log(2|X |).\nThe first item follows since the number of inner iterations is at most the number of weight doublings.\nLemma D.14. For every inner iteration,\nE[M i |F i\u22121 ] \u2264 x\u2208X \u03bb(W i (x) \u2212 W i\u22121 (x)).\nProof. The proof is almost a verbatim copy of Dasgupta et al. (2019, Lemma 6), which we include here:\nE[M i |F i\u22121 ] =\nx\u2208X Pr(x chosen in round i|x not chosen before round i,\nF i\u22121 ) = x\u2208X 1 \u2212 Pr(\u03c4 x > W i (x)|\u03c4 x > W i\u22121 (x)) = x\u2208X (1 \u2212 exp(\u2212\u03bb(W i (x) \u2212 W i\u22121 (x)))) \u2264 x\u2208X \u03bb(W i (x) \u2212 W i\u22121 (x)).", "publication_ref": ["b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "E. Deferred Materials from Section 4", "text": "E.1. Distribution-free Query Complexity Lower Bounds for Auditing with VC classes Theorem E.1 (Lower bound for randomized auditing). If hypothesis class H has VC dimension d \u2265 1600, and \u2208 (0, 1 40 ], then for any (possibly randomized) algorithm A, there exists a distribution D realizable by h * \u2208 H, such that when A is given a querying budget N \u2264 \u2126(min(d, 1\n2 )), its output\u03bc is such that\nP \u03bc \u2212 \u00b5(h * ) > > 1 8 .\nProof. We will be using Le Cam's method with several subtle modifications. First, we will reduce the estimation problem to a hypothesis testing problem, where under different hypotheses, the \u00b5(h * ) will be centered around two \u2126( )-separated values with high probability. Second, we will upper bound the distribution divergence of the interaction history under the two hypotheses; this requires some delicate handling, as the label on a queried example depends not only on the identity of the example, but also historical labeled examples.\nStep 1: the construction. As VC(H) = d, there exists a set of examples Z = {z 0 , z 1 , . . . , z d\u22121 } \u2282 X shattered by H. Let Z + = {z 1 , . . . , z d\u22121 }. Let D X be as follows:\nx | x A = 0 is uniform over Z + , whereas x | x A = 1 is the delta mass on z 0 . Let\u02dc = 10 max( , 1 \u221a d )\n; by the conditions that d \u2265 1600 and \u2264 1 40 , we have\u02dc \u2264 1 4 . Let label budget N = 1 24\u02dc 2 = \u2126 min(d, 1\n2 ) .\nConsider two hypotheses that choose h * randomly from {\u22121, +1} Z+ , subject to h * (z 0 ) = 0:\n\u2022 H 0 : choose h * such that for every i \u2208 [d \u2212 1], independently, h * (z i ) = +1, with probability 1 2 \u2212\u02dc \u22121, with probability 1 2 +\u02dc \u2022 H 1 : choose h * such that for every i \u2208 [d \u2212 1], independently, h * (z i ) =\n+1, with probability 1 2 +\u02dc \u22121, with probability 1 2 \u2212\u02dc We have the following simple claim that shows the separation of \u00b5(h * ) under the two hypotheses. Its proof is deferred to the end of the main proof.\nClaim E.2. P h * \u223cH0 \u00b5(h * ) \u2264 1 2 \u2212 1 2\u02dc \u2265 15 16 , and P h * \u223cH1 \u00b5(h * ) \u2265 1 2 + 1 2\u02dc \u2265 15 16 .\nStep 2: upper bounding the statistical distance. Next, we show that H 0 and H 1 are hard to distinguish with A having a label budget of N . To this end, we upper bound the KL divergence of the joint distributions of (x 1 , y 1 ), . . . , (x n , y n ) =: (x, y) \u2264n under H 0 and H 1 , denoted as P 0 and P 1 respectively. Applying Lemma E.14, we have:\nKL(P 0 , P 1 ) = n i=1 E KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) .(5)\nWe claim that for every i and\n((x, y) \u2264i\u22121 , x i ) \u2208 (X \u00d7 Y) i\u22121 \u00d7 X on the support of P 0 , KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) \u2264 3\u02dc 2 .(6)\nFirst, observe that if (x, y) \u2264i\u22121 , x i is in the support of P 0 , there must exists some h * : Z \u2192 {\u22121, +1} such that h * (x j ) = y j for all j \u2208 [i \u2212 1]; in particular, this means there must not exist\nj 1 = j 2 in [i \u2212 1], such that x j1 = x j2 but y j1 = y j2 .\nNext, we note that, under H 0 , conditioned on (x, y) \u2264i\u22121 , the posterior distribution of h * is supported over the set\nh | h : Z \u2192 {\u22121, +1} , \u2200j \u2208 [i \u2212 1]\n, h(x j ) = y j , and specifically, for all x \u2208 Z \\ x j : j \u2208 [i \u2212 1] , the h * (x)'s are independent conditioned on (x, y) \u2264i\u22121 , and\nP 0 h * (x) = +1 | (x, y) \u2264i\u22121 = 1 2 \u2212\u02dc .\nThe same statement holds for H 1 except that for all x \u2208 Z \\ x j : j \u2208\n[i \u2212 1] , we now have P 1 (h * (x) = +1 | (x, y) \u2264i\u22121 ) = 1 2 +\u02dc . In addition, the conditional distribution of y i | (x, y) \u2264i\u22121 , x i , equals the conditional distribu- tion of h * (x i ) | (x, y) \u2264i\u22121 ,\nunder both H 0 and H 1 . We now perform a case analysis:\n1. If x i \u2208 x j : j \u2208 [i \u2212 1]\n, then under both H 0 and H 1 , the distributions of h * (x i ) | (x, y) \u2264i\u22121 are equal: they both equal to the delta mass supported on the only element of the singleton set y j : j \u2208\n[i \u2212 1], x j = x i . In this case, KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) = 0 \u2264 3\u02dc 2 . 2. Otherwise, x i / \u2208 x j : j \u2208 [i \u2212 1] . Under H 0 , h * (x i ) | (x, y)\n\u2264i\u22121 takes value +1 with probability 1 2 \u2212\u02dc , and takes value \u22121 with probability 1 2 +\u02dc ; similarly, under H 1 , h * (x i ) | (x, y) \u2264i\u22121 takes value +1 with probability 1 2 +\u02dc , and takes value \u22121 with probability 1 2 \u2212\u02dc . In this case, by Fact E.13 and that\u02dc \u2264 1 4 ,\nKL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) = kl 1 2 \u2212\u02dc , 1 2 +\u02dc \u2264 3\u02dc 2 .\nIn summary, in both cases, Equation ( 6) holds, and plugging this back to Equation (5) with n = 1 24\u02dc 2 , we have KL(P 0 , P 1 ) \u2264 3n\u02dc 2 \u2264 1 8 . By Pinsker's inequality (Lemma E.11), d TV (P 0 , P 1 ) \u2264 1 2 KL(P 0 , P 1 ) \u2264 1 2 . By Le Cam's Lemma (Lemma E.10), for any hypothesis testerb, we have\n1 2 P 0 b = 1 + 1 2 P 1 b = 0 \u2265 1 2 1 \u2212 d TV (P 0 , P 1 ) \u2265 1 4 .(7)\nStep 3: concluding the proof. Given A's output auditing estimate\u03bc, consider the following hypothesis test:\nb = 0,\u03bc < 1 2 , 1,\u03bc \u2265 1 2 .\nPlugging into Equation ( 7), we have\n1 2 P 0 \u03bc \u2265 1 2 + 1 2 P 1 \u03bc < 1 2 \u2265 1 4 .(8)\nNow, recall Claim E.2, and using the fact that P(A \u2229 B) \u2265 P(A) \u2212 P(B C ) = P(A) + P(B) \u2212 1, we have\nP 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc \u2265 P 0 \u03bc \u2265 1 2 , \u00b5(h * ) \u2264 1 2 \u2212 1 2\u02dc \u2265 P 0 \u03bc \u2265 1 2 + 15 16 \u2212 1 \u2265 P 0 \u03bc \u2265 1 2 \u2212 1 16 .(9)\nSymmetrically, we also have\nP 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc \u2265 P 1 \u03bc < 1 2 , \u00b5(h * ) \u2265 1 2 + 1 2\u02dc \u2265 P 1 \u03bc < 1 2 \u2212 1 16 .(10)\nCombining Equations ( 8), ( 9), and (10), we have\n1 2 P 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc + 1 2 P 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc \u2265 1 4 \u2212 1 16 > 1 8 .\nAs 1 2\u02dc > , and the left hand side can be viewed as the total probability of \u03bc \u2212 \u00b5(h * ) > when h * is drawn from the uniform mixture distribution of the h * distributions under H 0 and H 1 . By the probabilistic method, there exists some h * such that P h * ,A \u03bc \u2212 \u00b5(h * ) > > 1 8 .\nProof of Claim E.2. Without loss of generality, we show the first inequality; the second inequality can be shown symmetrically. Note that under H 0 , the random h * 's DP value satisfies\n\u00b5(h * ) = Pr(h * (x) = +1 | x A = 0) \u2212 Pr(h * (x) = +1 | x A = 1) = 1 d \u2212 1 d\u22121 i=1 1{h * (z i ) = +1},\nwhere the second equality follows from that Pr(h * (x) = +1 | x A = 1) = 0 as h * (z 0 ) = \u22121 is always true.\nUnder H 0 , (d \u2212 1)\u00b5(h * ) is the sum of (d \u2212 1)\niid Bernoulli random variables with mean parameter 1 2 \u2212\u02dc . Therefore, by Hoeffding's inequality, we have\nP 0 \u00b5(h * ) > 1 2 \u2212 1 2\u02dc \u2264 exp \u22122(d \u2212 1) \u2022 1 2\u02dc 2 \u2264 1 16 ,\nwhere the second inequality uses the fact that\u02dc = 10 max , 1\n\u221a d \u2265 10 \u221a d .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Query Complexity for Auditing Non-homogeneous Halfspaces under Gaussian Subpopulations", "text": "Theorem E.3 (Lower bound). Let d \u2265 6400 and \u2208 (0,\n1 80 ]. If D X is such that x | x A = 0 \u223c N(0 d , I d ), whereas x | x A = 1 \u223c N(0 d , (0) d\u00d7d ) (i.\ne. the delta-mass supported at 0 d ). For any (possibly randomized) algorithm A, there exists h * in H lin the class of nonhomogeneous linear classifiers, such that when A is given a query budget N \u2264 \u2126 min(d, 1\n2 ) , its output\u03bc is such that\nP A,h * \u03bc \u2212 \u00b5(h * ) > > 1 8 .\nProof. Similar to the proof of Theorem E.1, we will use Le Cam's method. In addition to the same challenges in the proof of Theorem E.1, in the active fairness auditing for halfspaces setting, we are faced with the extra challenge that the posterior distributions of h * (x i ) | (x, y) \u2264i\u22121 deviates significantly from the prior distribution of h * (x i ), and cannot be easily calculated in closed form. To get around this difficulty, using the chain rule of KL divergence, along with the posterior formula for noiseless Bayesian linear regression with Gaussian prior, we calculate a tight upper bound on the KL divergence between two carefully constructed, well-separated hypotheses.\nStep 1: the construction. Let\u02dc = 40 max( , 1 \u221a d ); by the assumption that \u2264 1 80 and d \u2265 6400, we have\u02dc \u2264 1 2 . Let label budget N = 1 64\u02dc 2 = \u2126 min(d, 1 2 ) . Consider two hypotheses that choose h * = h a * ,b * , such that b * = \u22121, and a * is chosen randomly from different distributions:\n\u2022 H 0 : a * \u223c N(0, 1 d (1 +\u02dc )I d ) \u2022 H 1 : a * \u223c N(0, 1 d (1 \u2212\u02dc )I d )\nWe have the following claim that shows the separation of \u00b5(h * ) under the two hypotheses. Its proof is deferred to the end of the main proof. Claim E.4. P h * \u223cH0 \u00b5(h * ) > \u03a6(\u22121) +\u02dc 36 \u2265 15 16 , and\nP h * \u223cH1 \u00b5(h * ) < \u03a6(\u22121) \u2212\u02dc 36 \u2265 15 16 , where \u03a6(z) = z \u2212\u221e 1 \u221a 2\u03c0 e \u2212 z 2\n2 dz is the standard normal CDF.\nStep 2: upper bounding the statistical distance. Next, we show that H 0 and H 1 are hard to distinguish with A making n \u2264 N label queries. To this end, we upper bound the KL divergence of the joint distributions of (x, y) \u2264n under H 0 and H 1 , denoted as P 0 and P 1 respectively. To this end, define\u1ef9 i = a * , x i \u2212 1 for i \u2208 [n], and y i = sign(\u1ef9 i ). DefineP 0 and P 1 (resp. Q 0 and Q 1 ) as the joint distributions of (x,\u1ef9) \u2264n (resp. (x, y,\u1ef9) \u2264n ) under H 0 and H 1 respectively. By the chain rule of KL divergence (Lemma E.12 with Z = (x, y) \u2264n , W =\u1ef9 \u2264n and Z = (x,\u1ef9) \u2264n , W = y \u2264n respectively), we get: where the last term is 0 because under both Q 0 and Q 1 , (y) \u2264n | (x,\u1ef9) \u2264n is the delta mass supported on (sign(\u1ef9)) \u2264n . As a consequence, KL(P 0 , P 1 ) \u2264 KL(P 0 ,P 1 ) Also, note that A can be viewed as a query learning algorithm that at round i, receives (x,\u1ef9) \u2264i\u22121 as input, and choose the next example for query (i.e., it elects to only use the thresholded value y j 's as opposed to the\u1ef9 j 's). Applying Lemma E.14, we have:\nKL(Q 0 ((x, y,\u1ef9) \u2264n ), Q 1 ((x,\nKL(P 0 ,P 1 ) = n i=1 E KL(P 0 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )) . (11\n)\nWe claim that for every i and ((x,\u1ef9) \u2264i\u22121 , x i ) \u2208 (X \u00d7 Y) i\u22121 \u00d7 X on the support ofP 0 ,\nKL(P 0 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )) \u2264 3\u02dc 2 . (12\n)\nFirst, by Lemma E.5 (deferred to the end of the proof), under H 0 , conditioned on (x,\u1ef9) \u2264i\u22121 on the support of P 0 , the posterior distribution of a * is the same as a * \u223c N(0, 1 d (1 +\u02dc )I d ) conditioned on the affine set S = a \u2208 R d : a, x l + 1 =\u1ef9 l , \u2200l \u2208 [i \u2212 1] . Denote X i\u22121 = [x 1 ; x 2 ; . . . , x i\u22121 ] \u2208 R (i\u22121)\u00d7d , and\u1ef8 i\u22121 = (\u1ef9 1 , . . . ,\u1ef9 i\u22121 ); for (x,\u1ef9) \u2264i\u22121 on the support ofP 0 , it must be the case that S = \u2205, and as a result,\u00e2 = X \u2020 i\u22121 (\u1ef8 i\u22121 \u2212 1 i\u22121 ) \u2208 S. Also, denote by X \u22a5 i\u22121 a matrix whose columns are an orthonormal basis of span(x 1 , . . . , x i\u22121 ); such a X \u22a5 i\u22121 is always well-defined as i \u2212 1 \u2264 n \u2212 1 \u2264 d \u2212 1. Applying Lemma E.17, we have\na * | (x,\u1ef9) \u2264i\u22121 \u223c N \u00e2, 1 d (1 +\u02dc )X \u22a5 i\u22121 (X \u22a5 i\u22121 )\n, with its covariance matrix 1 d (1 +\u02dc )X \u22a5 i\u22121 (X \u22a5 i\u22121 ) being rank-deficient. Now, observe that\u1ef9 i | (x,\u1ef9) \u2264i\u22121 , x i has the same distribution as a * , x i + 1 | (x,\u1ef9) \u2264i\u22121 , which is N \u00e2,\nx i + 1, 1 d (1 +\u02dc )x i X \u22a5 i\u22121 (X \u22a5 i\u22121 ) x i . Similarly, under H 1 , we have\u1ef9 i | (x,\u1ef9) \u2264i\u22121 , x i has distribution N \u00e2, x i + 1, 1 d (1 \u2212\u02dc )x i X \u22a5 i\u22121 (X \u22a5 i\u22121 ) x i .\nWe now prove (12) by a case analysis:\n1. If x i \u2208 span(x 1 , . . . , x i\u22121 ), then (X \u22a5 i\u22121 ) x i = 0, and under both H 0 and H 1 , the posterior distributions of\u1ef9 i | (x,\u1ef9) \u2264i\u22121 , x i are both delta mass on \u00e2, x i + 1, and therefore, KL(P 0\n(\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )) = 0 \u2264 3\u02dc 2 .\n2. If x i / \u2208 span(x 1 , . . . , x i\u22121 ), then (X \u22a5 i\u22121 ) x i = 0, and under H 0 and H 1 , the posterior distributions of\ny i | (x,\u1ef9) \u2264i\u22121 , x i are N(\u03bc i , (1 +\u02dc )\u03c3 2 i ) and N(\u03bc i , (1 \u2212\u02dc )\u03c3 2 i\n) respectively, where\u03bc i = \u00e2, x i + 1, and\n\u03c3 2 i = 1 d x i X \u22a5 i\u22121 (X \u22a5 i\u22121 ) x i .\nIn this case, by Fact E.15,\nKL P 0 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i ) =KL N(\u03bc i , (1 +\u02dc )\u03c3 2 i ), N(\u03bc i , (1 \u2212\u02dc )\u03c3 2 i ) = 1 2 1 +\u02dc 1 \u2212\u02dc \u2212 1 + ln( 1 \u2212\u02dc 1 +\u02dc ) \u2264 1 2 2\u02dc 1 \u2212\u02dc 2 \u22648\u02dc 2 ,\nwhere the first inequality is by the fact that ln(1 + x) \u2265 x \u2212 x 2 when x \u2265 0, and taking x = 2\u02dc 1\u2212\u02dc , and the second inequality is from\u02dc \u2264 1 2 and algebra. In summary, in both cases, Equation (12) holds, and plugging this back to Equation (11) with n \u2264 1 64\u02dc 2 , we have KL(P 0 , P 1 ) \u2264 8n\u02dc 2 \u2264 1 8 . By Pinsker's inequality (Lemma E.11), d TV (P 0 , P 1 ) \u2264 1 2 KL(P 0 , P 1 ) \u2264 1 2 . Le Cam's lemma (Lemma E.10) implies that, for any hypothesis testerb, we have\n1 2 P 0 (b = 1) + 1 2 P 1 (b = 0) = 1 2 (1 \u2212 d TV (P 0 , P 1 )) \u2265 1 4 . (13\n)\nStep 3: concluding the proof. Given A's output auditing estimate\u03bc, consider the following hypothesis tester:\nb = 0,\u03bc > \u03a6(\u22121), 1,\u03bc \u2264 \u03a6(\u22121).\nPlugging into Equation ( 7), we have\n1 2 P 0 \u03bc \u2264 \u03a6(\u22121) + 1 2 P 1 \u03bc > \u03a6(\u22121) \u2265 1 4 . (14\n)\nNow, recall Claim E.4, and using the fact that P(A \u2229 B) \u2265 P(A) \u2212 P(B C ) = P(A) + P(B) \u2212 1, we have\nP 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc \u2265 P 0 \u03bc \u2264 \u03a6(\u22121), \u00b5(h * ) > \u03a6(1) \u2212 1 36\u02dc \u2265 P 0 \u03bc \u2264 \u03a6(\u22121) + 15 16 \u22121 \u2265 P 0 \u03bc \u2264 \u03a6(\u22121) \u2212 1 16 .\n(15) Symmetrically, we also have\nP 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc \u2265 P 1 \u03bc > \u03a6(\u22121) \u2212 1 16 . (16\n)\nCombining Equations ( 14), ( 15), and ( 16), we have\n1 2 P 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc + 1 2 P 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc \u2265 1 4 \u2212 1 16 > 1 8 .\nAs 1 36\u02dc \u2265 , and the left hand side can be viewed as the total probability of \u03bc \u2212 \u00b5(h * ) \u2265 when h * is drawn from the uniform mixture distribution of the h * distributions under H 0 and H 1 . By the probabilistic method, there exists some h * \u2208 H such that P h * \u03bc \u2212 \u00b5(h * ) > > 1 8 .\nLemma E.5. Given the same setting above. For any fixed i \u2208 N and (x,\u1ef9) \u2264i , the posterior distribution a * | (x,\u1ef9) \u2264i is the same as a * | {a * \u2208 U }, where U = a : \u2200j \u2208 [i] : x j , a + 1 =\u1ef9 j .\nProof. We use the Bayes formula to expand the posterior; below \u221d denotes equality up to a multiplicative factor independent of a * .\nP(a * | (x,\u1ef9) \u2264i ) \u221dP(a * , (x,\u1ef9) \u2264i ) \u221dP(a * ) i j=1 P(x j | a * , (x,\u1ef9) \u2264j\u22121 )P(\u1ef9 j | x j , a * , (x,\u1ef9) \u2264j\u22121 ) \u221dP(a * ) i j=1 P(x j | (x,\u1ef9) \u2264j\u22121 )1 \u1ef9 j = x j , a * + 1 \u221dP(a * ) i j=1 1 \u1ef9 j = x j , a * + 1\nwhere the second equality uses the definition of conditional probability; the third equality uses the fact that for any fixed query learning algorithm A, x j is independent of a * conditioned on (x,\u1ef9) \u2264j\u22121 , and the observation that given x j and a * , y j = x j , a * + 1 deterministically. This concludes the proof.\nProof of Claim E.4. For h * (x) = sign( a * , x + b * ) where b * = \u22121, it can be seen that,\nP 0 (h * (x) = +1 | x A = 1) = 0,\nOn the other hand,\nP 0 (h * (x) = +1 | x A = 0) = P z\u223cN(0,I d ) ( a * , z \u2265 1) = P z\u223cN(0,I d ) a * a * , z \u2265 1 a * = 1 \u2212 \u03a6 1 a * .\nAlso, note that under H 0 ,\nd a * 2 2 (1+\u02dc ) \u223c \u03c7 2 (d)\n; Therefore, by Fact E.16, we have that with probability \u2265 15 16 ,\nd a * 2 2 (1+\u02dc ) \u2265 d \u2022 (1 \u2212 10 1 d ), which implies that 1 a * \u2264 1 (1 +\u02dc )(1 \u2212 10 1 d ) \u2264 1 (1 +\u02dc )(1 \u2212\u02dc 4 ) \u2264 1 \u2212\u02dc 4 .\nTherefore, as for every a, b \u2208\n[ 3 4 , 1], \u03a6(a) \u2212 \u03a6(b) \u2265 min \u03be\u2208[ 3 4 ,1] \u03a6 (\u03be)|a \u2212 b| \u2265 1 9\n|a \u2212 b|, we have:\n1 \u2212 \u03a6 1 a * \u2265 1 \u2212 \u03a6 1 \u2212\u02dc 4 \u2265 1 \u2212 (\u03a6(1) \u2212\u02dc 36 ) \u2265 \u03a6(\u22121) +\u02dc 36 .\nThis concludes the proof of the first inequality. The second inequality is proved symmetrically.\nWe now present our (deterministic) active fairness auditing algorithm, Algorithm 6 and its guarantees. Algorithm 6 works under the setting when the two subpopulations are Gaussian, whose mean and covariance parameters (m 0 , \u03a3 0 ), (m 1 , \u03a3 1 ) are known. It also assumes access to black-box queries to h * \u2208 H lin = h a,b (x) := sign( a, x + b) : a \u2208 R d , b \u2208 R , and aims to estimate \u00b5(h * ) within precision . Recall that\n\u00b5(h * ) = Pr x\u223cD X h * (x) = 1 | x A = 0 \u2212 Pr x\u223cD X h * (x) = 1 | x A = 1 ,\nit suffices to estimate \u03b3 b := Pr x\u223cD X h * (x) = 1 | x A = 0 within precision /2, for each b \u2208 {0, 1}. To this end, we note that\n\u03b3 b = Pr x\u223cN(m b ,\u03a3 b ) h * (x) = 1 = Prx \u223cN(0,I d ) h * (m b + \u03a3 1/2 bx ) = 1 ; if we defineh b : R d \u2192 {\u22121, +1} such thath b (x) = h * (m b + \u03a3 1/2 bx ),(17)\n\u03b3 b equals to \u03b3(h b ), where \u03b3(h) = Px \u223cN(0,I d ) h(x) = 1 is the probability of positive prediction of h under the standard Gaussian distribution. Importantly, as h * is a linear classifier,h b is also a linear classifier and lies in H lin .\nRecall that procedure ESTIMATE-POSITIVE (Algorithm 4) label-efficiently estimates \u03b3(h) for any h \u2208 H lin , using query access to h. Algorithm 6 uses it as a subprocedure to estimate \u03b3 b = \u03b3(h b ) (line 3). To simulate label queries toh b using query access to h * , according to Equation ( 17), it suffices to apply an affine transformation on the inputx, obtaining transformed input\nm b + \u03a3 1/2\nbx , and query h * on the transformed input. Finally, after\u03b3 0 ,\u03b3 1 , /2-accurate estimators of \u03b3 0 , \u03b3 1 are obtained, Algorithm 6 takes their difference as our estimator\u03bc for \u00b5(h * ) (line 4). \n3:\u03b3 b \u2190 ESTIMATE-POSITIVE(h b , 2 ) 4: return\u03b3 0 \u2212\u03b3 1 Theorem E.6 (Upper bound). If h * \u2208 H lin , D X is such that x | x A = 0 \u223c N(m 0 , \u03a3 0 ), x | x A = 1 \u223c N(m 1 , \u03a3 1 ).\nAlgorithm 6 outputs\u03bc, such that with probability 1, \u03bc \u2212 \u00b5(h * ) \u2264 ; moreover, Algorithm 6 makes at most O(d ln d ) label queries to h * .\nProof. As we will see from Lemma E.7, for b \u2208 {0, 1}, the respective calls of ESTIMATE-POSITIVE ensures that We now turn to presenting the guarantee of the key subprocedure ESTIMATE-POSITIVE and its proof. This expands the analysis sketch in Section 4.3. Recall that \u03b1 = 2d ln 1 and \u03b2 = 2d\n|\u03b3 b \u2212 \u03b3 b | \u2264 2 . Therefore, \u03bc \u2212 \u00b5(h * ) \u2264 |\u03b3 0 \u2212 \u03b3 0 | +|\u03b3 1 \u2212 \u03b3 1 | \u2264 .\n5 2 (ln 1 ) 3 4 ( 1 ) 1 2 .\nWe consider two cases depending on the line in which ESTIMATE-POSITIVE returns:\n1. If ESTIMATE-POSITIVE returns in line 5, then it must be the case that for all i \u2208 [d], h * (\u03b1e i ) = h * (\u2212\u03b1e i ). In this case, by Lemma E.9, we have that for every i, |m i | \u2265 \u03b1. This implies that r =\n1 d i=1 m \u22122 i \u2265 1 d\u03b1 \u22122 \u2265 2 ln 1 .\nFor the case that s = \u22121, we have that \u03b3(h * ) = \u03a6(sr) \u2264 , where we use the standard fact that \u03a6(x) \u2264 exp(\u2212 x 2\n2 ) for x \u2264 0; in this case\u03b3 = 0 ensures Equation ( 18) holds; for the symmetric case that s = +1, \u03b3(h * ) = \u03a6(sr) \u2265 1 \u2212 and\u03b3 = 1, which also ensures Equation ( 18).\n2. On the other hand, ESTIMATE-POSITIVE returns in line 13, it must be the case that there exists some i\n0 \u2208 [d], such that |m i0 | \u2264 \u03b1. This implies that r = 1 d i=1 m \u22122 i \u2264 1 m \u22122 i 0 = |m i0 | \u2264 \u03b1.\nNow, ESTIMATE-POSITIVE must execute lines 6 to 11. The final S it computes has the following properties: for every i \u2208 S added, by the guarantee of procedure BINARY-SEARCH (Algorithm 5), |m i \u2212 m i | \u2264 ; otherwise, for i / \u2208 S, it must be the case that h * (\u03b2e i ) = h * (\u2212\u03b2e i ), which, by Lemma E.9, implies that|m i | \u2265 \u03b2. Therefore, all the conditions of Lemma 4.4 are satisfied, and thus,|r \u2212 r| \u2264 2 . This also yields that|sr \u2212 sr| \u2264 2 . Finally, note that \u03a6 is 1 \u221a 2\u03c0 -Lipschitz, we have\n\u03b3 \u2212 \u03b3(h * ) = \u03a6(sr) \u2212 \u03a6(sr) \u2264 1 \u221a 2\u03c0 \u2022|sr \u2212 sr| \u2264 .\nIn summary, in both cases, ESTIMATE-POSITIVE outputs\u03b3 such that Equation ( 18) is satisfied.\nWe now calculate the total query complexity of ESTIMATE-POSITIVE. Line 2 makes 1 label query; line 3 makes 2d label queries; for each i \u2208 [d], line 8 makes 2 label queries, and BINARY-SEARCH makes log 2\u03b2 label queries. In summary, the total label query complexity of ESTIMATE-POSITIVE is:\n1 + 2d + d(2 + log 2\u03b2 ) = O d ln d .\nWe now present the proof of Lemma 4.4, which is key to the proof of Lemma E.7.\nProof of Lemma 4.4. First, by Lemma E.8, and the assumption that for all i \u2208 S,|m i \u2212 m i | \u2264 , we have\n1 i\u2208Sm \u22122 i \u2212 1 i\u2208S m \u22122 i \u2264 .\nIt remains to prove that\n1 i\u2208S m \u22122 i \u2212 1 d i=1 m \u22122 i \u2264 ,\nwhich combined with the above inequality, will conclude the proof.\nTo see this, let z = d i=1 m \u22122 i and z S = i\u2208S m \u22122 i ; since for all i / \u2208 S,|m i | \u2265 \u03b2, this implies that\n|z \u2212 z S | \u2264 d \u03b2 2 \u2264 2 (4d ln 1 ) 3 2 , Also, note that 1 d i=1 m \u22122 i = r \u2264 \u03b1 implies that z \u2265 1 \u03b1 2 = 1 2d ln 1 ; therefore, z S \u2265 z \u2212 2 (4d ln 1 ) 3 2 \u2265 1 4d ln 1 . Now, by Lagrange mean value theorem, 1 \u221a z S \u2212 1 \u221a z \u2264 max z \u2208(z S ,z) 1 2 (z ) \u2212 3 2 \u2022|z s \u2212 z| \u2264 1 2 (z S ) \u2212 3 2 \u2022|z s \u2212 z| \u2264 1 2 (4d ln 1 ) 3 2 \u2022 2 (4d ln 1 ) 3 2 \u2264 .\nThis concludes the proof. Proof. First, we show that f is 1-Lipschitz with respect to \u2022 \u221e in each of the orthants of R l . Without loss of generality, we focus on the positive orthant R =: m \u2208 R l : m i \u2265 0, \u2200i . We now check that for any two points m and n in R, f ( m) \u2212 f ( n) \u2264 m \u2212 n \u221e . By Lagrange mean value theorem, there exists some \u03b8\n\u2208 t m + (1 \u2212 t) n : t \u2208 (0, 1) , such that f ( m) \u2212 f ( n) = \u2207f (\u03b8), m \u2212 n \u2264 \u2207f (\u03b8) 1 m \u2212 n \u221e ,\nwhere the second inequality is from H\u00f6lder's inequalty. Therefore, it suffices to check that for all m in the R 0 =: Now consider m, n \u2208 R l that do not necessarily lie in the same orthant. Suppose the line segment t m + (1 \u2212 t) n : t \u2208 [0, 1] consists of k pieces, where piece i is t m + (1 \u2212 t) n : t \u2208 [t i\u22121 , t i ] , where 1 = t 0 > t 1 > . . . > t k = 0, where each piece is contained in an orthant. Then we have:\nm \u2208 R l : m i > 0, \u2200i (interior of R), \u2207f (m 1 , . . . , m l ) 1 \u2264 1. To see this, note that \u2207f (m 1 , . . . , m d ) = m \u22123 1 ( l i=1 m \u22122 i ) 3 2 , . . . , m \u22123 l ( l i=1 m \u22122 i ) 3 2 =: g,\nf ( m) \u2212 f ( n) \u2264 k i=1 f (t i\u22121 m + (1 \u2212 t i\u22121 ) n) \u2212 f (t i m + (1 \u2212 t i ) n) \u2264 k i=1 (t i\u22121 m + (1 \u2212 t i\u22121 ) n) \u2212 (t i m + (1 \u2212 t i ) n) \u221e = k i=1 (t i\u22121 \u2212 t i ) m \u2212 n \u221e = m \u2212 n \u221e ,\nwhere the second inequality uses the Lipchitzness of f within the orthant that contains piece i, for each\ni in [k]. Lemma E.9. Given i \u2208 [d] and \u03be > 0, if h * (\u03bee i ) = h * (\u2212\u03bee i ), then|m i | \u2265 \u03be.\nProof. Suppose h * (\u03bee i ) = h * (\u2212\u03bee i ) = +1; in this case, \u2212b i \u2264 \u03bea * i \u2264 b i , and therefore, \u03bea * i \u2264 b i , which implies that |m i | \u2265 \u03be. The case of h * (\u03bee i ) = h * (\u2212\u03bee i ) = +1 can be proved symmetrically.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.3. Auxiliary Lemmas for Query Learning Lower Bounds", "text": "In this subsection we collect a few standard and useful lemmas for establishing lower bounds for general adaptive sampling and query learning algorithms, including active fairness auditing algorithms. Throughout, denote by P the distribution of interaction transcript (the sequence of N labeled examples (x 1 , y 1 ), . . . , (x N , y N ) ) obtained by the query learning algorithm by interacting with the environment, and use the shorthand (x, y) \u2264i to denote (x 1 , y 1 ), . . . , (x i , y i ) .\nLemma E.10 (Le Cam's Lemma). Given two distributions P 0 , P 1 over observation space z \u2208 Z, and letb : Z \u2192 {0, 1} be any hypothesis tester. Then,\n1 2 P 0 b (Z) = 1 + 1 2 P 1 b (Z) = 0 \u2265 1 2 1 \u2212 d TV (P 0 , P 1 ) ,\nwhere d TV (P 0 , P 1 ) denotes the total variation distance between P 0 and P 1 .\nLemma E.11 (Pinsker's Inequality). For two distributions P and Q, d TV (P 0 , P 1 ) \u2264 1 2 KL(P, Q). Lemma E.12 (Chain rule of KL divergence). For two distributions Q 0 (Z, W ) and Q 1 (Z, W ) over Z \u00d7 W, we have\nKL(Q 0 , Q 1 ) =KL(Q 0 Z , Q 1 Z ) + E z\u223cQ 0 Z KL(Q 0 W |Z (\u2022 | z), Q 1 W |Z (\u2022 | z))\n. The following lemma is well-known.\nLemma E.14 (Divergence decomposition). For a (possibly randomized) query learning algorithm A with label budget N , under two hypotheses H 0 , H 1 (represented by distributions over the target concept h * ), we have:\nKL(P 0 , P 1 ) = N i=1 E KL(P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ))\nProof. We simplify KL(P 0 , P 1 ) as follows: \nKL\n+ ln P 0 (y i | (x, y) \u2264i\u22121 , x i ) P 1 (y i | (x, y) \u2264i\u22121 , x i ) = N i=1 (x,y) \u2264i P 0 ((x, y) \u2264i ) ln P 0 (y i | (x, y) \u2264i\u22121 , x i ) P 1 (y i | (x, y) \u2264i\u22121 , x i ) = N i=1 (x,y) \u2264i\u22121 ,xi P 0 ((x, y) \u2264i\u22121 , x i ) \u2022 yi P 0 (y i | (x, y) \u2264i\u22121 , x i ) ln P 0 (y i | (x, y) \u2264i\u22121 , x i ) P 1 (y i | (x, y) \u2264i\u22121 , x i ) = N i=1 E KL(P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )) ,\nwhere the first equality is by the definition of KL divergence; the second equality is from the chain rule of conditional probability; the third equality is by canceling out the conditional probabilities of unlabeled examples given history, as we run the same algorithm A under two environments; the fourth equality is by the law of total probability; the fifth equality is again by the definition of the KL divergence. The lemma below is a standard fact on normal distribution conditioned on affine subspaces; we include a proof here as we cannot find a reference.\nLemma E.17. Suppose U = \u03b8 \u2208 R d : X\u03b8 = y is an nonempty affine subspace of R d , where X \u2208 R m\u00d7d has rows x 1 , . . . , x m \u2208 R d . Let dim(span(x 1 , . . . , x m )) = l, and let W \u2208 R d\u00d7(d\u2212l) be a matrix whose columns form an orthonormal basis of span(x 1 , . . . , x m ) \u22a5 . Consider Z \u223c N(0, I d ); then,\nZ | {Z \u2208 U } \u223c N(X \u2020 y, W W ).\nProof. Denote by\u03b8 = X \u2020 y the least norm solution of equation X\u03b8 = y. It is well-known that\u03b8 \u2208 span(x 1 , . . . , x m ). As U = \u2205, X\u03b8 = y. We now claim that U can be equivalently written as \u03b8 + W \u03b1 : \u03b1 \u2208 R d\u2212l :\n1. On one hand, for all \u03b8 =\u03b8 + W \u03b1, X\u03b8 = X\u03b8 + XW \u03b1 = y + 0 = y.\n2. On the other hand, for every \u03b8 \u2208 U , as X\u03b8 = y, we have X(\u03b8 \u2212\u03b8) = 0, which implies that \u03b8 \u2212\u03b8 \u2208 span(x 1 , . . . , x m ) \u22a5 . Therefore, there exists some \u03b1 \u2208 R d\u2212l such that \u03b8 =\u03b8 + W \u03b1.\nDefine V \u2208 R d\u00d7l to be a matrix whose columns form an orthonormal basis of span(x 1 , . . . , x m ). We also claim that given a vector z \u2208 R d , z \u2208 U \u21d4 V z = V \u03b8 :\n1. If z \u2208 U , by the previous claim, z =\u03b8 + W \u03b1, and therefore V z = V \u03b8 + V W \u03b1 = V \u03b8 .\n2. If V z = V \u03b8 , then note that z = V V z + W W z = V V \u03b8 + W (W z) =\u03b8 + W (W z), where the last equality follows from that\u03b8 \u2208 span(x 1 , . . . , x m ). Taking \u03b1 z = W z \u2208 R d\u2212l , we have z =\u03b8 + W \u03b1 z , implying that z \u2208 U .\nFor the rest of the proof, let \nZ | {Z \u2208 U } d = V V + W W | {Z \u2208 U } d =\u03b8 + W W | {Z \u2208 U } d = N(X \u2020 y, W W ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F. Experiments", "text": "In this section, we empirically explore the shrinkage of the version space under various baseline methods and Algorithm 3. The two baseline methods of sampling we will consider are: 1) i.i.d sampling (without replacement) 2) active learning (CAL).\nProcedure: We train a logistic regression model to find h * on two datasets commonly used in Fairness literature. The first is COMPAS (Larson et al., 2016), where the two groups are defined to be Caucasian and non-Caucasian. And the second is the Student Performance Dataset, where the two groups are defined to be Female and Male. Then, we run the three methods with an alloted label budget of: 20, 50, 80, 100, 120. These are a small fraction of the total dataset size (much smaller for COMPAS than Student Performance).\nEvaluation: Our evaluation will be on the version space induced by the labels requested by the three methods. We will evaluate the version space in two ways:\n1. Given H[S], we will compute its \u00b5-diameter max h,h \u2208H[S] \u00b5(h) \u2212 \u00b5(h ). The \u00b5-diameter of the version space captures the largest extent that the algorithm's \u00b5 estimate may be changed by post-hoc manipulation. The smaller it is the higher the degree of manipulation-proofness.\nTo compute max h,h \u2208H[S] \u00b5(h) \u2212 \u00b5(h ) , we will evaluate max h\u2208H[S] \u00b5(h) and min h\u2208H[S] \u00b5(h). Let G 1 = {x \u2208 X :\nx A = 1} and G 0 = {x \u2208 X : x A = 0}. To implement the maximization program, we may move the constraint into the objective as a Lagrangian:\nmax h 1 |G 1 | x\u2208G1 1{h(x) = 1} \u2212 1 |G 0 | x\u2208G0 1{h(x) = 1} + \u03bb( x\u2208S 1{h(x) = h * (x)})\nor equivalently:\nmax h 1 |G 1 | x\u2208G1 1{h(x) = 1} + 1 |G 0 | x\u2208G0 1{h(x) = \u22121} + \u03bb( x\u2208S 1{h(x) = h * (x)})\nAs mentioned earlier, we observe that this objective may be framed as a cost-sensitive classification problem, which is commonly used in fairness literature (Agarwal et al., 2018). In particular, the cost for predicting 1 for x \u2208 G 1 is \u2212 1 |G1| and 0 o.w, the cost for predicting 1 is 0 for x \u2208 G 0 and \u2212 1 |G0| o.w and the cost for predicting h * (x) for x \u2208 S is \u2212\u03bb and 0 o.w. By using iterative doubling and grid search, we look for the smallest \u03bb such that we may enforce h(x) = h * (x) \u2200x \u2208 S (since these hard constraints) and find the maximizing h in the version space given this \u03bb. The same procedure is applied for the minimizing h in the version space.\n2. Since we may choose any \u00b5(h) for h \u2208 H[S] to return as an estimate for \u00b5(h * ), we will evaluate E h\u223cunif(H[S]) [|\u00b5(h) \u2212 \u00b5(h * )|] -this corresponds to the average error and is proportional to estimation accuracy.\nFor sampling from the version space, we will use the classic hit-and-run algorithm and sample 500 models from the version space at each budget and then average the error.\nResults: In terms of the \u00b5-diameter of the version space, which may be interpreted as the maximum possible degree of post-audit manipulation of \u00b5, we see in Figure 1 that Algorithm 3 is the best of the three methods at all budgets. This is expected since Algorithm 3 is designed to make use of max h\u2208H[S] \u00b5(h) and min h\u2208H[S] \u00b5(h) estimates in its query selection to \"shrink\" the version space in \u00b5-space. Behind Algorithm 3, CAL looks to be generally better or on-par with i.i.d sampling.\nIn terms of estimation error, going by the average \u00b5 estimation error in the version space, we see in Figure 2 that in general, one of the active approaches outperforms that of i.i.d sampling. Between the two active approaches, there are budgets setting where one is better than the other and vice versa.  ", "publication_ref": ["b0"], "figure_ref": ["fig_16", "fig_17"], "table_ref": []}, {"heading": "", "text": "Acknowledgments. We thank Stefanos Poulis for sharing the implementation of the black-box teaching algorithm of Dasgupta et al. (2019), and special thanks to Steve Hanneke and Sanjoy Dasgupta for helpful discussions. We also thank the anonymous ICML reviewers for their feedback.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A reductions approach to fair classification", "journal": "PMLR", "year": "2018", "authors": "A Agarwal; A Beygelzimer; M Dud\u00edk; J Langford; H Wallach"}, {"ref_id": "b1", "title": "The online set cover problem", "journal": "SIAM Journal on Computing", "year": "2009", "authors": "N Alon; B Awerbuch; Y Azar; N Buchbinder; J Naor"}, {"ref_id": "b2", "title": "Queries and concept learning", "journal": "", "year": "1988", "authors": "D Angluin"}, {"ref_id": "b3", "title": "Active and passive learning of linear separators under log-concave distributions", "journal": "PMLR", "year": "2013", "authors": "M.-F Balcan; P Long"}, {"ref_id": "b4", "title": "Active property testing", "journal": "IEEE", "year": "2012", "authors": "M.-F Balcan; E Blais; A Blum; Yang ; L "}, {"ref_id": "b5", "title": "Agnostic online learning", "journal": "", "year": "2009", "authors": "S Ben-David; D P\u00e1l; S Shalev-Shwartz"}, {"ref_id": "b6", "title": "Solving convex programs by random walks", "journal": "Journal of the ACM (JACM)", "year": "2004", "authors": "D Bertsimas; S Vempala"}, {"ref_id": "b7", "title": "Vc dimension and distribution-free sample-based testing", "journal": "", "year": "2021", "authors": "E Blais; R Ferreira Pinto Jr; N Harms"}, {"ref_id": "b8", "title": "Estimating decision tree learnability with polylogarithmic sample complexity", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "G Blanc; N Gupta; J Lange; L.-Y Tan"}, {"ref_id": "b9", "title": "Active tolerant testing", "journal": "PMLR", "year": "2018", "authors": "A Blum; L Hu"}, {"ref_id": "b10", "title": "Improving generalization with active learning", "journal": "Machine learning", "year": "1994", "authors": "D Cohn; L Atlas; R Ladner"}, {"ref_id": "b11", "title": "Analysis of a greedy active learning strategy", "journal": "", "year": "2005", "authors": "S Dasgupta"}, {"ref_id": "b12", "title": "Coarse sample complexity bounds for active learning", "journal": "", "year": "2005", "authors": "S Dasgupta"}, {"ref_id": "b13", "title": "A general agnostic active learning algorithm", "journal": "Advances in Neural Information Processing Systems", "year": "2007", "authors": "S Dasgupta; D J Hsu; C Monteleoni"}, {"ref_id": "b14", "title": "Teaching a black-box learner", "journal": "PMLR", "year": "2019", "authors": "S Dasgupta; D Hsu; S Poulis; X Zhu"}, {"ref_id": "b15", "title": "Variance estimation in high-dimensional linear models", "journal": "Biometrika", "year": "2014", "authors": "L H Dicker"}, {"ref_id": "b16", "title": "A threshold of ln n for approximating set cover", "journal": "Journal of the ACM (JACM)", "year": "1998", "authors": "U Feige"}, {"ref_id": "b17", "title": "On the complexity of teaching", "journal": "Journal of Computer and System Sciences", "year": "1995", "authors": "S A Goldman; M J Kearns"}, {"ref_id": "b18", "title": "Property testing and its connection to learning and approximation", "journal": "Journal of the ACM (JACM)", "year": "1998", "authors": "O Goldreich; S Goldwasser; Ron ; D "}, {"ref_id": "b19", "title": "Interactive proofs for verifying machine learning", "journal": "", "year": "2021", "authors": "S Goldwasser; G N Rothblum; J Shafer; Yehudayoff ; A "}, {"ref_id": "b20", "title": "The cost complexity of interactive learning. Unpublished manuscript", "journal": "", "year": "2006", "authors": "S Hanneke"}, {"ref_id": "b21", "title": "Teaching dimension and the complexity of active learning", "journal": "Springer", "year": "2007", "authors": "S Hanneke"}, {"ref_id": "b22", "title": "Rates of convergence in active learning. The Annals of Statistics", "journal": "", "year": "2011", "authors": "S Hanneke"}, {"ref_id": "b23", "title": "Theory of active learning", "journal": "", "year": "2014", "authors": "S Hanneke"}, {"ref_id": "b24", "title": "Generalized teaching dimensions and the query complexity of learning", "journal": "", "year": "1995", "authors": "T Heged\u0171s"}, {"ref_id": "b25", "title": "The scandal explained. BBC News", "journal": "", "year": "2015", "authors": "R Hotten;  Volkswagen"}, {"ref_id": "b26", "title": "Algorithms for active learning", "journal": "", "year": "2010", "authors": "D J Hsu"}, {"ref_id": "b27", "title": "Efficient and parsimonious agnostic active learning", "journal": "Advances in Neural Information Processing Systems", "year": "2015", "authors": "T.-K Huang; A Agarwal; D J Hsu; J Langford; R E Schapire"}, {"ref_id": "b28", "title": "Estimating learnability in the sublinear data regime", "journal": "", "year": "2018", "authors": "W Kong; G Valiant"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Algorithm 33Oracle-efficient Active Fairness Auditing Require: Hypothesis class H, online learning oracle O with mistake bound M , constrained ERM oracle C-ERM, target error , fairness measure \u00b5. Ensure:\u03bc, an estimate of \u00b5(h * ) 1: Initialize S \u2190 \u2205 2: while true do 3:\u0125 \u2190 O(S) 4: Let T \u2190 \u2205 //Computing an approximate minimum (\u00b5, )specifying set for\u0125 5: Initialize weights w(x) = 1 |X | and threshold \u03c4 x \u223c Exponential(ln(|H| 2 M/\u03b4))", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "10. Third, by the O log |H|M \u03b4 log |X | -approximation guarantee of the online set cover algorithm implicit in (Dasgupta et al., 2019), each outer iteration makes at most O Cost(H) log |H|M \u03b4 log |X | label queries. Remark 3.9. As we have seen, Algorithm 3 implicitly performs online set cover. With this connection, it inherits the\u03a9(log |H| log |X |) inapproximability factor of online set cover (Alon et al., 2009, Proposition 4.2).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "number of label queries, A will exit the while loop. This completes the inductive proof of the claim. Proof of Theorem 3.2. Fix a deterministic active fairness auditing algorithm A. We will show the following claim: If A has already obtained an ordered sequence of labeled examples L, and has a remaining label budget N \u2264 Cost(H[L]) \u2212 1, then there exists h \u2208 H[L], such that, A, when interacting with h as the target classifier: 1. obtains a sequence of labeled examples L in the first |L| rounds;", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "runs in time O(poly(m, n)). D.5. Deferred Materials for Section 3.2 D.5.1. (\u00b5, )-SPECIFYING SET, (\u00b5, )-TEACHING DIMENSION AND THEIR PROPERTIES", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "y,\u1ef9) \u2264n ) = KL(Q 0 ((x, y) \u2264n ), Q 1 ((x, y) \u2264n ) KL(P0,P1) + KL(Q 0 ((\u1ef9) \u2264n | (x, y) \u2264n ), Q 1 ((\u1ef9) \u2264n | (x, y) \u2264n )) \u22650 = KL(Q 0 ((x,\u1ef9) \u2264n ), Q 1 ((x,\u1ef9) \u2264n ) KL(P0,P1) + KL(Q 0 ((y) \u2264n | (x,\u1ef9) \u2264n ), Q 1 ((y) \u2264n | (x,\u1ef9) \u2264n )) 0 ,", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Algorithm 66Active fairness auditing for nonhomogeneous linear classifiers under Gaussian subpopulations Require: Subpopulation parameters (m 0 , \u03a3 0 ), (m 1 , \u03a3 1 ), query access to h * \u2208 H lin , target error . Ensure:\u03bc such that \u03bc \u2212 \u00b5(h * ) \u2264 . 1: for b \u2208 {0, 1} do 2: Defineh b : R d \u2192 {\u22121, +1} such thath b (x) = h * (m b + \u03a3 1/2 bx ); //h b \u2208 H lin , and each query toh b can be simulated by one query to h *", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Moreover, for every b, Lemma E.7 ensures that each call to ESTIMATE-POSITIVE only makes at most O(d ln d ) label queries toh b ; as simulating each query toh b takes one query to h * , for every b, it also makes at most O(d ln d ) label queries to h * . Summing the number of label queries over b \u2208 {0, 1}, the total number of label queries by Algorithm 6 is O(d ln d ).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Lemma E. 7 (.7Guarantees of ESTIMATE-POSITIVE). Recall that \u03b3(h) = Pr x\u223cN(0,I d ) (h(x) = +1). ESTIMATE-POSITIVE (Algorithm 4) receives inputs query access to h * \u2208 H lin , and target error , and outputs\u03b3 such that \u03b3 \u2212 \u03b3(h * ) \u2264 . (18) Furthermore, it makes at most O(d ln d ) queries to h * . Proof. Let h * (x) = sign( a * , x + b * ) be the target classifier. First, observe that \u03b3(h * ) = \u03a6 b * a * 2 =: \u03a6(sr), where \u03a6 is the standard normal CDF, s := sign(b * ), and r := Note that line 2 of ESTIMATE-POSITIVE correctly obtains s, as s = h * ( 0) = sign( a * , 0 + b) = sign(b).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Lemma E. 8 .8Let l \u2208 N + and f (m 1 , . . . , m l ) := is 1-Lipschitz with respect to \u2022 \u221e .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "this implies that for every i \u2208 [l],|g i | \u2264 1, and therefore,", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Fact E. 13 .13Let kl(\u2022, \u2022) denote the binary relative entropy function. For a, b \u2208 [ 1 4 , 3 4 ], kl(a, b) \u2264 3(b \u2212 a) 2 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "(P 0 , P 1 ) = (x,y) \u2264N P 0 ((x, y) \u2264N ) ln P 0 ((x, y) \u2264N ) P 0 ((x, y) \u2264N ) = (x,y) \u2264N P 0 ((x, y) \u2264N ) N i=1 ln P A (x i | (x, y) \u2264i\u22121 ) P A (x i | (x, y) \u2264i\u22121 )", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Fact E.15 (KL divergence between Gaussians of the same mean). If \u00b5 \u2208 R and \u03c3 1 , \u03c3 2 > 0, then, KL N(\u00b5, \u03c3 2 1 ), N(\u00b5, \u03c3 2 2 16 (Concentration of \u03c7 2 random variables). For d \u2265 1, Z \u223c \u03c7 2 (d), and \u03b4 > 0,P |Z \u2212 d| \u2264 2", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "d=denote equality in distribution. Consider random variable Z d = N(0, I d ). Let V = V Z, W = W Z. Now, note that the matrix T = W V \u2208 R d\u00d7d is a orthonormal matrix, I d ),Therefore, V , W are two independent, standard normal random variables with distributions N(0, I l ) and N(0, I d\u2212l ), respectively.Note from the second claim that the event {Z \u2208 U } is equivalent to{ V = V \u03b8 }; therefore, W | {Z \u2208 U } d = N(0, I d\u2212l ). As a result,", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 1 .1Figure 1. Left: Comparison of the three methods on the Student Performance dataset on \u00b5-diameters of the final version spaces, as a function of label query budget. Right: Comparison of the three methods on the COMPAS dataset. For the error bars, a 95 percent confidence interval is constructed using the 50 repeats at each budget.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 2 .2Figure 2. Left: Comparison of the three methods on the Student Performance dataset on average \u00b5-estimation errors of the final version spaces, as a function of label query budget. Right: Comparison of the three methods on the COMPAS dataset. For the error bars, a 95 percent confidence interval is constructed using the 50 repeats at each budget.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Algorithm 2 Derandomized Phased CAL for Auditing Require: Hypothesis class H, target error , minority population proportion p, fairness measure \u00b5 Ensure:\u03bc, an estimate of \u00b5(h * )", "figure_data": "1: Let N = log 216 ln |H| p"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "On the hardness of the minimum height decision tree problem. Discrete Applied Mathematics, 144(1-2):209-212, 2004.Larson, J., Mattu, S., Kirchner, L., and Angwin, J. How we analyzed the compas recidivism algorithm.", "figure_data": "ProPublica(5 2016), 9(1):3-3, 2016.Littlestone, N. Learning quickly when irrelevant attributesabound: A new linear-threshold algorithm. Machinelearning, 2(4):285-318, 1988.McCarthy, D.To regulate ai, try playing ina sandbox.Emerging Tech Brew, 2021.URLhttps://www.morningbrew.com/emerging-tech/stories/2021/05/26/regulate-ai-just-play-sandbox.Mitchell, T. M. Generalization as search. Artificial intelli-gence, 18(2):203-226, 1982.Rastegarpanah, B., Gummadi, K., and Crovella, M. Audit-ing black-box prediction models for data minimizationcompliance. Advances in Neural Information ProcessingSystems, 34, 2021.Ron, D. Property testing: A learning theory perspective.Now Publishers Inc, 2008.Sabato, S., Sarwate, A. D., and Srebro, N. Auditing: activelearning with outcome-dependent query costs. In Pro-ceedings of the 26th International Conference on NeuralInformation Processing Systems-Volume 1, pp. 512-520,2013.Valiant, L. G. A theory of the learnable. Communicationsof the ACM, 27(11):1134-1142, 1984.Xu, Z., Yu, T., and Sra, S. Towards efficient evaluationof risk via herding. Negative Dependence: Theory andApplications in Machine Learning, 2019.Zhang, C. and Chaudhuri, K. Beyond disagreement-basedagnostic active learning. Advances in Neural InformationProcessing Systems, 27, 2014."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparison with Sabato et al. (2013): Lastly, Sabato et al. (2013) also uses the term \"auditing\" in the context of active learning with outcome-dependent query costs; although the term \"auditing\" is shared, our problem settings are completely different: (Sabato et al., 2013) focuses on active learning the model h", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "V [T ] := h \u2208 V : \u2200(x, y) \u2208 T, h(x) = y . Furthermore, denote by V y x = V (x, y) for notational simplicity. Given a set of classifiers V and fairness measure \u00b5, denote by diam \u00b5 (V ) := max h,h \u2208V \u00b5(h) \u2212 \u00b5(h ) the \u00b5-diameter of V . Given a set of labeled examples T , denote by Pr T (\u2022)", "formula_coordinates": [3.0, 55.44, 316.9, 235.74, 64.13]}, {"formula_id": "formula_1", "formula_text": "1. For every i \u2208 [N ], f i : (X \u00d7 Y) i\u22121 \u2192 X", "formula_coordinates": [3.0, 62.91, 510.31, 225.07, 11.23]}, {"formula_id": "formula_2", "formula_text": "f i : (X \u00d7 Y) i\u22121 \u00d7 {0, 1} B \u2192 X and g : (X \u00d7 Y) N \u00d7 {0, 1} B \u2192 R.", "formula_coordinates": [3.0, 307.44, 69.87, 235.74, 22.9]}, {"formula_id": "formula_3", "formula_text": "\u03b8(r) = sup h\u2208H,r \u2265r Pr D X (x \u2208 DIS(B(h, r ))) r .", "formula_coordinates": [3.0, 333.78, 330.19, 181.32, 23.34]}, {"formula_id": "formula_4", "formula_text": "Cost(V ) = 0, diam \u00b5 (V ) \u2264 2 1 + min x max y Cost(V [(x, y)]), otherwise", "formula_coordinates": [4.0, 55.44, 406.6, 230.06, 24.0]}, {"formula_id": "formula_5", "formula_text": "1: Let V \u2190 H 2: while diam \u00b5 (V ) > 2 do 3: Query x \u2208 argmin x max y Cost (V y x ), obtain label h * (x) 4: V \u2190 V (h * , {x}) 5: return 1 2 max h\u2208V \u00b5(h) + min h\u2208V \u00b5(h)", "formula_coordinates": [4.0, 312.42, 119.18, 229.02, 71.37]}, {"formula_id": "formula_6", "formula_text": "of V , diam \u00b5 (V ) = max h,h \u2208V \u00b5(h) \u2212 \u00b5(h ), is at most 2 , then since \u00b5(h * ) \u2208 I = [min h\u2208V \u00b5(h), max h\u2208V \u00b5(h)]", "formula_coordinates": [4.0, 307.19, 253.97, 234.25, 21.61]}, {"formula_id": "formula_7", "formula_text": "Proposition 3.3. Cost(H) \u2264 O 1 2 ln |H| .", "formula_coordinates": [4.0, 307.44, 706.37, 177.45, 11.1]}, {"formula_id": "formula_8", "formula_text": "Proposition 3.4. Cost(H) \u2264 O \u03b8( ) \u2022 ln |H| \u2022 ln 1 ,", "formula_coordinates": [5.0, 55.44, 182.89, 235.74, 10.77]}, {"formula_id": "formula_9", "formula_text": "Pr Sn (x \u2208 DIS(V n )) \u2264 2 Pr D X (x \u2208 DIS(V n ))+ ln 8 m n ,", "formula_coordinates": [5.0, 75.37, 425.53, 215.54, 23.22]}, {"formula_id": "formula_10", "formula_text": "1 \u2212 1 4 , \u2200h, h \u2208 H : Pr S (h(x) = h (x)) = 0 =\u21d2 Pr D X (h(x) = h (x)) \u2264 16 ln |H| m n .", "formula_coordinates": [5.0, 80.62, 484.98, 203.57, 68.0]}, {"formula_id": "formula_11", "formula_text": "Pr Sn (x \u2208 DIS(V n )) \u2264 2 Pr D X (x \u2208 DIS(V n ))+ ln 8 m n ,and", "formula_coordinates": [5.0, 334.34, 200.88, 215.54, 41.78]}, {"formula_id": "formula_12", "formula_text": "\u2200h, h \u2208 H : Pr Sn (h(x) = h (x)) = 0 =\u21d2 Pr D X (h(x) = h (x)) \u2264 16 ln |H| m n . 6: Query h * for the labels of examples in T n := S n \u2229 DIS(V n ) 7: V n+1 \u2190 V n (h * , T n ). 8: return \u00b5(h) for an arbitrary h \u2208 V N +1 .", "formula_coordinates": [5.0, 312.42, 255.27, 229.02, 101.52]}, {"formula_id": "formula_13", "formula_text": "N n=1 |T n | = N n=1 m n \u2022 (2 Pr D X (x \u2208 DIS(V n )) + ln 8 m n ) \u2264 N n=1 m n \u2022 (2\u03b8( ) 16 ln |H| m n + ln 8 m n ) \u2264O \u03b8( ) \u2022 ln |H| \u2022 ln 1 .", "formula_coordinates": [5.0, 317.92, 404.91, 213.03, 85.04]}, {"formula_id": "formula_14", "formula_text": "\u221e t=1 I(\u0125 t (x t ) = h * (x t )) \u2264 M .", "formula_coordinates": [6.0, 65.96, 390.55, 121.44, 14.11]}, {"formula_id": "formula_15", "formula_text": "C = {C x : x \u2208 X }, where (h, h ) is in C x if h(x) =\u0125(x) or h (x) =\u0125(x).", "formula_coordinates": [6.0, 307.44, 220.39, 234.0, 22.34]}, {"formula_id": "formula_16", "formula_text": "(h 1 , h 2 ) = argmax h,h \u2208H \u00b5(h) \u2212 \u00b5(h ) : h(T ) = h (T ) =\u0125(T )", "formula_coordinates": [6.0, 307.44, 575.07, 240.29, 16.94]}, {"formula_id": "formula_17", "formula_text": "h 1 \u2190 find max h\u2208H \u00b5(h), s.t. h(T ) =\u0125(T ) (1) and h 2 \u2190 find min h\u2208H \u00b5(h), s.t. h(T ) =\u0125(T ) (2) //T is an (\u00b5, )-specifying set for\u0125 8: if \u00b5(h 1 ) \u2212 \u00b5(h 2 ) \u2264 2", "formula_coordinates": [7.0, 60.42, 388.12, 229.02, 97.84]}, {"formula_id": "formula_18", "formula_text": ") Determine \u2206(h 1 , h 2 ) = {x \u2208 X : h 1 (x) = h(x) or h 2 (x) =\u0125(x)} 12: while x\u2208\u2206(h1,h2) w(x) \u2264 1 do 13: Double weights w(x) for all x in \u2206(h 1 , h 2 ) 14: Update T \u2190 x \u2208 X : w(x) \u2265 \u03c4 x 15: Query h * on T 16: S \u2190 S \u222a T 17: if\u0125(T ) = h * (T ) then 18: return 1 2 (\u00b5(h 1 ) + \u00b5(h 2 ))", "formula_coordinates": [7.0, 55.94, 536.41, 400.1, 121.2]}, {"formula_id": "formula_19", "formula_text": "O Cost(H)M log |H|M \u03b4 log |X | .", "formula_coordinates": [7.0, 349.8, 91.42, 149.29, 22.31]}, {"formula_id": "formula_20", "formula_text": "x | x A = 1 \u223c Uniform({1, . . . , n}). Let H = h : X \u2192 {\u22121, +1} , h(0) = \u22121 .", "formula_coordinates": [8.0, 55.44, 94.13, 234.0, 20.91]}, {"formula_id": "formula_21", "formula_text": "sult, \u00b5(h) = 0 n \u2212 0 1 = 0, and \u00b5(h ) = |{1,...,n}\\S| n \u2212 0 1 \u2265 1 2 , which implies that diam \u00b5 (H(h * , S)) \u2265 1 2 > .", "formula_coordinates": [8.0, 55.08, 236.59, 235.6, 29.13]}, {"formula_id": "formula_22", "formula_text": "1 2 ) . Example 4.3. Let d \u2265 2 and X = R d . x | x A = 0 \u223c N(m 0 , \u03a3 0 ), whereas x | x A = 1 \u223c N(m 1 , \u03a3 1 ).", "formula_coordinates": [8.0, 306.94, 128.11, 234.5, 51.64]}, {"formula_id": "formula_23", "formula_text": "h * ) = P x\u223cN(0,I d ) (h * (x) = +1), with black- box label queries to h * (x) = sign( a * , x + b * ). Algo- rithm 4 is based on the following insights. First, observe that \u03b3(h * ) = \u03a6 b * a * 2 =: \u03a6(sr),", "formula_coordinates": [8.0, 307.44, 347.85, 235.65, 52.3]}, {"formula_id": "formula_24", "formula_text": "1 d i=1 m \u22122 i , for m i := \u2212 b * a * i .", "formula_coordinates": [8.0, 307.44, 404.09, 235.25, 33.38]}, {"formula_id": "formula_25", "formula_text": "Lemma 4.4. Let \u03b1 := 2d ln 1 and \u03b2 := 2d 5 2 (ln 1 ) 3 4 ( 1 ) 1 2 . Suppose r \u2264 \u03b1. If there is some S \u2282 [d], such that: 1. for all i / \u2208 S,|m i | \u2265 \u03b2, 2. for all i \u2208 S, |m i \u2212 m i | \u2264 ; then, 1 i\u2208Sm \u22122 i \u2212 r \u2264 2 .", "formula_coordinates": [8.0, 307.19, 529.11, 235.99, 124.82]}, {"formula_id": "formula_26", "formula_text": "\u221a 2\u03c0 -Lipschitzness of \u03a6, implies that \u03b3 \u2212 \u03b3(h * ) \u2264 . The total query complexity of Algorithm 4 is 1 + 2d + 2d + d log 2 \u03b2 =\u00d5(d).", "formula_coordinates": [9.0, 55.08, 129.99, 235.6, 39.25]}, {"formula_id": "formula_27", "formula_text": "1: Let \u03b1 = 2d ln 1 , \u03b2 = 2d 5 2 (ln 1 ) 3 4 ( 1 ) 1 2 . 2: s \u2190 Query h * on 0 3: Query h * on \u03c1\u03b1e i : \u03c1 \u2208 {\u00b11} , i \u2208 [d] 4: if for all i \u2208 [d], h * (\u03b1e i ) = h * (\u2212\u03b1e i ) then 5: return 1 if s = +1, 0 if s = \u22121 //Otherwise, r \u2264 \u03b1 = 2d ln 1 6: S \u2190 \u2205 7: for i = 1, . . . , d do 8:", "formula_coordinates": [9.0, 60.42, 244.46, 185.62, 116.86]}, {"formula_id": "formula_28", "formula_text": "S \u2190 S \u222a {i} //Use binary search to obtainm i , an estimate of m i = \u2212 b * a * i with precision 11:m i \u2190 BINARY-SEARCH(i, \u03b2, ) (Algorithm 5) 12:r \u2190 1 i\u2208Sm \u22122 i //r is an estimate of r 13: return \u03a6(sr) Algorithm 5 BINARY-SEARCH Require: i, \u03b2 such that h * (\u03b2e i ) = h * (\u2212\u03b2e i ), precision Ensure: m, an -accurate estimate of m i = \u2212 b ai 1: u \u2190 \u03b2, l \u2190 \u2212\u03b2 2: while u \u2212 l \u2265 do 3: m \u2190 u+l 2 4: Query h * on me i 5: if h * (me i ) = h * (le i ) then 6: l \u2190 m 7: else 8: u \u2190 m 9: return m", "formula_coordinates": [9.0, 55.08, 376.49, 234.35, 244.29]}, {"formula_id": "formula_29", "formula_text": "P \u03bc \u2212 \u00b5(h * ) > > 1 8", "formula_coordinates": [9.0, 372.01, 311.13, 103.67, 22.31]}, {"formula_id": "formula_30", "formula_text": "S i = (x 1 , y 1 ), . . . , (x i , y i ) ,", "formula_coordinates": [12.0, 240.22, 567.02, 116.45, 9.65]}, {"formula_id": "formula_31", "formula_text": "(x i+1 , h 1 (x i+1 )) is identical to (x i+1 , h 1 (x i+1 )", "formula_coordinates": [12.0, 54.28, 625.82, 193.5, 9.65]}, {"formula_id": "formula_32", "formula_text": "S i+1 = (x 1 , y 1 ), . . . , (x i , y i ), (x i+1 , y i+1 ) up to step i + 1.", "formula_coordinates": [12.0, 55.44, 670.03, 328.0, 29.25]}, {"formula_id": "formula_33", "formula_text": "D X (x A = 0), Pr D X (x A = 1) is \u2126(1). Lemma C.1. If h is such that P(h(x) = h * (x)) \u2264 \u03b1, then \u00b5(h) \u2212 \u00b5(h * ) \u2264 \u03b1 p .", "formula_coordinates": [13.0, 55.44, 154.02, 320.27, 29.28]}, {"formula_id": "formula_34", "formula_text": "Pr(h(x) = +1 | x A = 0) \u2212 Pr(h * (x) = +1 | x A = 0) \u2264 Pr(h(x) = h * (x) | x A = 0) = Pr(h(x) = h * (x), x A = 0) Pr(x A = 0) \u2264 Pr(h(x) = h * (x), x A = 0) p ,", "formula_coordinates": [13.0, 180.49, 221.47, 232.58, 81.89]}, {"formula_id": "formula_35", "formula_text": "Pr(h(x) = +1 | x A = 1) \u2212 Pr(h * (x) = +1 | x A = 1) \u2264 Pr(h(x) =h * (x),x A =1) p .", "formula_coordinates": [13.0, 57.1, 327.57, 321.39, 16.03]}, {"formula_id": "formula_36", "formula_text": "\u00b5(h) \u2212 \u00b5(h * ) \u2264 Pr(h(x) = +1 | x A = 0) \u2212 Pr(h * (x) = +1 | x A = 0) + Pr(h(x) = +1 | x A = 1) \u2212 Pr(h * (x) = +1 | x A = 1) \u2264 Pr(h(x) = h * (x), x A = 0) p + Pr(h(x) = h * (x), x A = 1) p = Pr(h(x) = h * (x)) p \u2264 \u03b1 p .", "formula_coordinates": [13.0, 61.16, 356.48, 471.25, 84.43]}, {"formula_id": "formula_37", "formula_text": "max h\u2208V \u00b5(h) \u2212 min h\u2208V \u00b5(h) = diam \u00b5 (V ) \u2264 2 .", "formula_coordinates": [13.0, 210.12, 596.5, 176.63, 14.66]}, {"formula_id": "formula_38", "formula_text": "\u03bc \u2212 \u00b5(h * ) \u2264 1 2 max h\u2208V \u00b5(h) \u2212 min h\u2208V \u00b5(h) \u2264 .", "formula_coordinates": [13.0, 202.34, 645.33, 194.64, 22.31]}, {"formula_id": "formula_39", "formula_text": "Cost(V ) = 1 + min x\u2208X max y\u2208{\u22121,+1} Cost (V y x ) , i.e. min x\u2208X max y\u2208{\u22121,+1} Cost (V y x ) = Cost(V ) \u2212 1 = n.", "formula_coordinates": [14.0, 55.44, 97.29, 329.62, 36.03]}, {"formula_id": "formula_40", "formula_text": "x 0 = argmin x\u2208X max y\u2208{\u22121,+1} Cost (V y x ) ,", "formula_coordinates": [14.0, 226.27, 149.77, 144.34, 18.59]}, {"formula_id": "formula_41", "formula_text": "V h * , {x 0 } = V h * (x0) x0", "formula_coordinates": [14.0, 55.44, 185.69, 98.48, 14.43]}, {"formula_id": "formula_42", "formula_text": "1 + Cost(V h * , {x 0 } ) \u2264 n + 1 = Cost(V )", "formula_coordinates": [14.0, 131.52, 201.85, 184.04, 11.23]}, {"formula_id": "formula_43", "formula_text": "\u03bc A,h \u2212 \u00b5(h) > or \u03bc A,h \u2212 \u00b5(h ) > ,", "formula_coordinates": [14.0, 215.38, 391.75, 168.57, 9.65]}, {"formula_id": "formula_44", "formula_text": "V = H[L].", "formula_coordinates": [14.0, 55.44, 436.74, 44.62, 8.74]}, {"formula_id": "formula_45", "formula_text": "A,h , h) = V . As Cost(V ) \u2265 1, we know that max h1,h2\u2208H(h,S A,h ) \u00b5(h 1 ) \u2212 \u00b5(h 2 ) = max h1,h2\u2208V \u00b5(h 1 ) \u2212 \u00b5(h 2 ) > 2 .", "formula_coordinates": [14.0, 165.76, 472.5, 282.07, 32.91]}, {"formula_id": "formula_46", "formula_text": "L such that V = H[L] satisfies Cost(V ) \u2265 n + 2.", "formula_coordinates": [14.0, 272.66, 548.39, 199.27, 8.96]}, {"formula_id": "formula_47", "formula_text": "Cost H L \u222a (x, y) = Cost (V y x ) \u2265 Cost(V ) \u2212 1 \u2265 n + 1,", "formula_coordinates": [14.0, 163.77, 581.44, 269.35, 12.69]}, {"formula_id": "formula_48", "formula_text": "diam \u00b5 H(h, S A,h ) = max h1,h2\u2208H(h,S A,h ) \u00b5(h 1 ) \u2212 \u00b5(h 2 ) > 2 .", "formula_coordinates": [14.0, 172.67, 660.73, 251.55, 15.72]}, {"formula_id": "formula_49", "formula_text": "\u00b5(h, S 1 , S 2 ) = Pr x\u223cS1 (h(x) = +1) \u2212 Pr x\u223cS2 (h(x) = +1).", "formula_coordinates": [15.0, 175.92, 112.32, 245.03, 9.65]}, {"formula_id": "formula_50", "formula_text": "\u2022 Let n = O 1 2 ln |H| ; \u2022 Find (the lexicographically smallest) S 1 and S 2 in X n , such that \u2200h \u2208 H, \u03bc(h, S 1 , S 2 ) \u2212 \u00b5(h) \u2264 .", "formula_coordinates": [15.0, 66.9, 163.77, 315.59, 46.93]}, {"formula_id": "formula_51", "formula_text": ")3", "formula_coordinates": [15.0, 533.7, 201.37, 7.74, 8.64]}, {"formula_id": "formula_52", "formula_text": "|Z i | \u2264 b almost surely. Then, with probability 1 \u2212 \u03b4, 1 n n i=1 Z i \u2212 \u00b5 \u2264 2\u03c3 2 ln 2 \u03b4 n + b ln 2 \u03b4 3n .", "formula_coordinates": [15.0, 93.64, 338.78, 283.84, 55.62]}, {"formula_id": "formula_53", "formula_text": "Z i = I(x i \u2208 DIS(V n )), with probability 1 \u2212 1 4 , Pr Sn (x \u2208 DIS(V n )) \u2264 Pr D X (x \u2208 DIS(V n )) + 2 Pr D X (x \u2208 DIS(V n )) ln 8 m n + ln 8 3m n \u22642 Pr D X (x \u2208 DIS(V n )) + ln 8 m n .", "formula_coordinates": [15.0, 134.95, 495.0, 345.21, 82.22]}, {"formula_id": "formula_54", "formula_text": "1 \u2212 1 4 , \u2200h, h \u2208 H : Pr D X (h(x) = h (x)) \u2264 Pr Sn (h(x) = h (x)) + 4 Pr D X (h(x) = h (x)) ln |H| m n + 4 ln |H| 3m n in which, \u2200h, h \u2208 H : Pr Sn (h(x) = h (x)) = 0 =\u21d2 Pr D X (h(x) = h (x)) \u2264 16 ln |H| m n .", "formula_coordinates": [15.0, 75.37, 609.44, 449.55, 111.96]}, {"formula_id": "formula_55", "formula_text": "V N +1 , Pr(h(x) = h * (x)) \u2264 16 ln |H| m N \u2264 p , implying that \u00b5(h) \u2212 \u00b5(h * ) \u2264 (recall Lemma C.1).", "formula_coordinates": [16.0, 55.44, 126.87, 487.25, 27.7]}, {"formula_id": "formula_56", "formula_text": "N n=1 |T n | = N n=1 m n \u2022 (2 Pr D X (x \u2208 DIS(V n )) + ln 8 m n ) \u2264 N n=1 m n \u2022 (\u03b8( ) \u2022 16 ln |H| m n \u2022 2 p + ln 8 m n ) \u2264O \u03b8( ) \u2022 ln |H| \u2022 ln 1 ,", "formula_coordinates": [16.0, 191.92, 181.56, 213.03, 85.04]}, {"formula_id": "formula_57", "formula_text": "Pr D X (x \u2208 DIS(V n )) \u2264 Pr D X x \u2208 DIS(B(h * , 16 ln |H| m n )) \u2264 \u03b8( p 2 ) \u2022 16 ln |H| m n \u2264 \u03b8( ) \u2022 16 ln |H| m n \u2022 2 p ,", "formula_coordinates": [16.0, 85.75, 306.53, 425.39, 24.3]}, {"formula_id": "formula_58", "formula_text": "[n], h(x i ) = y i . Definition D.3. Fix D X . An example-based decision tree T is said to (\u00b5, )-separate a hypothesis set V , if for every leaf l of T , V l satisfies diam \u00b5 (V l ) \u2264 2 .", "formula_coordinates": [16.0, 55.44, 521.69, 485.8, 41.88]}, {"formula_id": "formula_59", "formula_text": "h = h 0 , \u00b5(h) \u2212 \u00b5(h 0 ) = Pr(h(x) = +1 | x A = 0) \u2212 Pr(h 0 (x) = +1 | x A = 0) \u2265 1 N \u22121 > 2 .", "formula_coordinates": [18.0, 77.03, 161.5, 464.41, 24.54]}, {"formula_id": "formula_60", "formula_text": "Lemma D.6. OPT SC \u2264 OPT MC \u2264 OPT SC + max C\u2208C log |C|.", "formula_coordinates": [18.0, 55.44, 241.39, 241.98, 14.65]}, {"formula_id": "formula_61", "formula_text": "\u222a k l=1 C i l = U . Therefore, \u2200j \u2208 [n], \u2203l \u2208 [k] such that u j \u2208 C i l \u21d4 h j (x i l ) = 1 by construction.", "formula_coordinates": [18.0, 95.29, 566.97, 377.54, 12.55]}, {"formula_id": "formula_62", "formula_text": "U z = {u 1 1 , . . . , u 1 n , . . . , u z 1 , . . . , u z n }, C z = {C 1 , . . . , C zm }, where C (p\u22121)m+i = {u p i1 , . . . , u p isi } for p \u2208 [z], i \u2208 [m]. Note that OPT SC,z = kOPT SC .", "formula_coordinates": [18.0, 75.37, 693.55, 466.07, 24.35]}, {"formula_id": "formula_63", "formula_text": "Time complexity. In I MC,z , |X | \u2264 (mz \u2022 nz + 1) = O(mn log 2 n), |H| = nz = n log n, and = 1 2N = 1 2(mz\u2022nz+1) = \u2126( 1 mn log 2 n ). As A runs in time O(poly(|X |, |H|, 1 )), A", "formula_coordinates": [19.0, 55.44, 352.09, 486.0, 28.86]}, {"formula_id": "formula_64", "formula_text": "min |S|, s.t.\u2200h 1 , h 2 \u2208 H(h, S) |\u00b5(h 1 ) \u2212 \u00b5(h 2 )| \u2264 2", "formula_coordinates": [19.0, 188.68, 517.3, 215.47, 9.65]}, {"formula_id": "formula_65", "formula_text": "Cost(H) = 1 + min x max y Cost(H[(x, y)]) \u2265 1 + min x1\u2208X Cost(H[(x, h 0 (x))]) \u2265 2 + min x1\u2208X min x2\u2208X Cost(H[{(x 1 , h 0 (x 1 )), (x 2 , h 0 (x 2 ))}])", "formula_coordinates": [19.0, 164.39, 641.53, 268.1, 56.04]}, {"formula_id": "formula_66", "formula_text": "Cost(H) \u2265 k \u2212 1 + min U k\u22121 Cost(H(h 0 , U k\u22121 )).", "formula_coordinates": [20.0, 204.5, 104.34, 187.88, 15.25]}, {"formula_id": "formula_67", "formula_text": "U \u2264 k \u2212 1, there exists h , h \u2208 H(h 0 , U ) such that |\u00b5(h ) \u2212 \u00b5(h )| > \u21d2 diam \u00b5 (H(h 0 , U )) > . Thus, for any unlabeled dataset U k\u22121 of size k \u2212 1, Cost(H(h 0 , U k\u22121 )) \u2265 1. Therefore, Cost(H) \u2265 k.", "formula_coordinates": [20.0, 55.44, 138.32, 487.25, 32.65]}, {"formula_id": "formula_68", "formula_text": "\u00b5(h 1 ) = min h\u2208H(h * ,T ) \u00b5(h), \u00b5(h 2 ) = max h\u2208H(h * ,T ) \u00b5(h). Therefore, \u00b5(h * ) \u2208 [min h\u2208H(h * ,T ) \u00b5(h), max h\u2208H(h * ,T ) \u00b5(h)] = [\u00b5(h 1 ), \u00b5(h 2 )]. Furthermore, by line 9, \u00b5(h 1 ) \u2212 \u00b5(h 2 ) \u2264 2 . Hence,\u03bc, the output of Algorithm 3, satisfies that, \u03bc \u2212 \u00b5(h * ) = 1 2 \u00b5(h 1 ) + \u00b5(h 2 ) \u2212 \u00b5(h * ) \u2264 .", "formula_coordinates": [20.0, 55.13, 277.58, 486.31, 83.64]}, {"formula_id": "formula_69", "formula_text": "1 \u2212 \u03b4/M , |T t | \u2264 O XTD(H, \u00b5, ) \u2022 log |H|M \u03b4 log |X | .", "formula_coordinates": [20.0, 315.1, 421.81, 228.09, 14.38]}, {"formula_id": "formula_70", "formula_text": "N t=1 |T t | \u2264 O M \u2022 XTD(H, \u00b5, ) \u2022 log |H|M \u03b4 log |X | .", "formula_coordinates": [20.0, 185.38, 460.15, 226.77, 30.2]}, {"formula_id": "formula_71", "formula_text": "satisfies|T | \u2264 O XTD(H, \u00b5, ) \u2022 log |H|M \u03b4 log |X | .", "formula_coordinates": [20.0, 55.44, 516.26, 214.08, 14.38]}, {"formula_id": "formula_72", "formula_text": "Z i,j = U i,j \u2212 E U i,j | F i,j\u22121 \u2208 [\u22121, +1]. Then Y i,j is a martingale as E[Y i,j |F i,j\u22121 ] = E[Z i,j |F i,j\u22121 ] + E[Y i,j\u22121 |F i,j\u22121 ] = Y i,j\u22121 .", "formula_coordinates": [21.0, 55.44, 104.28, 486.0, 22.85]}, {"formula_id": "formula_73", "formula_text": "Pr \uf8eb \uf8ed \u2203n, m, Y nm \u2265 s, (i,j) (n,m) E[Z 2 ij |F i(j\u22121) ] \u2264 \u03c3 2 \uf8f6 \uf8f8 \u2264 exp \u2212 s 2 2(\u03c3 2 + s/3) (4)", "formula_coordinates": [21.0, 132.22, 185.06, 409.22, 34.15]}, {"formula_id": "formula_74", "formula_text": "(i,j) (n,m) E[Z 2 ij |F i(j\u22121) ] = (i,j) (n,m) E[U 2 ij |F i(j\u22121) ] \u2212 E[U ij |F i(j\u22121) ] 2 \u2264 (i,j) (n,m) E[U 2 ij |F i(j\u22121) ] = (i,j) (n,m) E[U ij |F i(j\u22121) ] = n i=1 E Fi\u22121 [M i ] \u2264 \u03bb x\u2208X W n (x) (Lemma D.14) \u2264 \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) = \u03c3 2 . (Lemma D.13) Meanwhile, we choose s = 1 6 log( 1 \u03b4 ) + 2\u03c3 2 log 1 \u03b4 + 1 6 log( 1 \u03b4 ) = O ln 1 \u03b4 \u03c3 + ln 1", "formula_coordinates": [21.0, 55.44, 249.43, 486.0, 221.32]}, {"formula_id": "formula_75", "formula_text": "Y nm = (i ,j ) (n,m) U i j \u2212 n i=1 E Fi\u22121 [M i ] \u2264 O ln 1 \u03b4 \u03c3 + ln 1 \u03b4 .", "formula_coordinates": [21.0, 159.03, 509.87, 278.82, 30.94]}, {"formula_id": "formula_76", "formula_text": "N i=1 E Fi\u22121 [M i ] \u2264 \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)). Therefore, for Y N m in particular, Y N m \u2264 O \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) + \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) ln(1/\u03b4) + ln(1/\u03b4) = O \u03bb(1 + XTD(H, \u00b5, ) ln(2|X |)) + ln 1 \u03b4 = O XTD(H, \u00b5, ) ln(|X |) ln((|H|M )/\u03b4) . Lemma D.12 (Freedman's Inequality). Let martingale {Y k } \u221e k=0 with difference sequence {X k } \u221e k=0 be such that X k \u2264 R a.s for all k and Y 0 = 0. Let W k = k j=1 E j\u22121 [X 2 j ].", "formula_coordinates": [21.0, 55.13, 554.39, 486.23, 129.54]}, {"formula_id": "formula_77", "formula_text": "Pr(\u2203k \u2265 0 : Y k \u2265 t \u2227 W k \u2264 \u03c3 2 ) \u2264 exp \u2212 t 2 /2 \u03c3 2 + Rt/3 .", "formula_coordinates": [21.0, 179.85, 692.47, 237.19, 23.88]}, {"formula_id": "formula_78", "formula_text": "|S * (\u0125)| \u2022 log(2|X |).", "formula_coordinates": [22.0, 55.44, 236.27, 79.7, 10.31]}, {"formula_id": "formula_79", "formula_text": "1 + 1 \u2022 |S * (\u0125)| \u2022 log(2|X |) \u2264 1 + XTD(H, \u00b5, ) \u2022 log(2|X |).", "formula_coordinates": [22.0, 176.41, 291.03, 244.06, 10.81]}, {"formula_id": "formula_80", "formula_text": "E[M i |F i\u22121 ] \u2264 x\u2208X \u03bb(W i (x) \u2212 W i\u22121 (x)).", "formula_coordinates": [22.0, 220.47, 350.04, 181.94, 11.15]}, {"formula_id": "formula_81", "formula_text": "E[M i |F i\u22121 ] =", "formula_coordinates": [22.0, 141.0, 408.63, 59.12, 9.65]}, {"formula_id": "formula_82", "formula_text": "F i\u22121 ) = x\u2208X 1 \u2212 Pr(\u03c4 x > W i (x)|\u03c4 x > W i\u22121 (x)) = x\u2208X (1 \u2212 exp(\u2212\u03bb(W i (x) \u2212 W i\u22121 (x)))) \u2264 x\u2208X \u03bb(W i (x) \u2212 W i\u22121 (x)).", "formula_coordinates": [22.0, 192.37, 408.63, 263.51, 103.69]}, {"formula_id": "formula_83", "formula_text": "P \u03bc \u2212 \u00b5(h * ) > > 1 8 .", "formula_coordinates": [22.0, 244.63, 618.37, 107.63, 22.31]}, {"formula_id": "formula_84", "formula_text": "x | x A = 0 is uniform over Z + , whereas x | x A = 1 is the delta mass on z 0 . Let\u02dc = 10 max( , 1 \u221a d )", "formula_coordinates": [23.0, 55.44, 82.17, 486.0, 42.78]}, {"formula_id": "formula_85", "formula_text": "\u2022 H 0 : choose h * such that for every i \u2208 [d \u2212 1], independently, h * (z i ) = +1, with probability 1 2 \u2212\u02dc \u22121, with probability 1 2 +\u02dc \u2022 H 1 : choose h * such that for every i \u2208 [d \u2212 1], independently, h * (z i ) =", "formula_coordinates": [23.0, 66.9, 165.4, 421.11, 49.21]}, {"formula_id": "formula_86", "formula_text": "Claim E.2. P h * \u223cH0 \u00b5(h * ) \u2264 1 2 \u2212 1 2\u02dc \u2265 15 16 , and P h * \u223cH1 \u00b5(h * ) \u2265 1 2 + 1 2\u02dc \u2265 15 16 .", "formula_coordinates": [23.0, 55.44, 259.87, 339.97, 13.47]}, {"formula_id": "formula_87", "formula_text": "KL(P 0 , P 1 ) = n i=1 E KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) .(5)", "formula_coordinates": [23.0, 125.94, 328.54, 415.51, 30.32]}, {"formula_id": "formula_88", "formula_text": "((x, y) \u2264i\u22121 , x i ) \u2208 (X \u00d7 Y) i\u22121 \u00d7 X on the support of P 0 , KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) \u2264 3\u02dc 2 .(6)", "formula_coordinates": [23.0, 159.98, 367.99, 381.46, 31.82]}, {"formula_id": "formula_89", "formula_text": "j 1 = j 2 in [i \u2212 1], such that x j1 = x j2 but y j1 = y j2 .", "formula_coordinates": [23.0, 55.44, 428.68, 486.0, 21.61]}, {"formula_id": "formula_90", "formula_text": "h | h : Z \u2192 {\u22121, +1} , \u2200j \u2208 [i \u2212 1]", "formula_coordinates": [23.0, 61.25, 470.18, 144.81, 9.08]}, {"formula_id": "formula_91", "formula_text": "P 0 h * (x) = +1 | (x, y) \u2264i\u22121 = 1 2 \u2212\u02dc .", "formula_coordinates": [23.0, 217.48, 501.32, 161.93, 22.31]}, {"formula_id": "formula_92", "formula_text": "[i \u2212 1] , we now have P 1 (h * (x) = +1 | (x, y) \u2264i\u22121 ) = 1 2 +\u02dc . In addition, the conditional distribution of y i | (x, y) \u2264i\u22121 , x i , equals the conditional distribu- tion of h * (x i ) | (x, y) \u2264i\u22121 ,", "formula_coordinates": [23.0, 54.28, 531.42, 488.82, 35.14]}, {"formula_id": "formula_93", "formula_text": "1. If x i \u2208 x j : j \u2208 [i \u2212 1]", "formula_coordinates": [23.0, 62.91, 582.26, 110.08, 10.0]}, {"formula_id": "formula_94", "formula_text": "[i \u2212 1], x j = x i . In this case, KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) = 0 \u2264 3\u02dc 2 . 2. Otherwise, x i / \u2208 x j : j \u2208 [i \u2212 1] . Under H 0 , h * (x i ) | (x, y)", "formula_coordinates": [23.0, 62.91, 595.56, 479.77, 42.99]}, {"formula_id": "formula_95", "formula_text": "KL P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ) = kl 1 2 \u2212\u02dc , 1 2 +\u02dc \u2264 3\u02dc 2 .", "formula_coordinates": [23.0, 75.37, 662.89, 358.34, 13.47]}, {"formula_id": "formula_96", "formula_text": "1 2 P 0 b = 1 + 1 2 P 1 b = 0 \u2265 1 2 1 \u2212 d TV (P 0 , P 1 ) \u2265 1 4 .(7)", "formula_coordinates": [24.0, 178.18, 87.76, 363.26, 22.31]}, {"formula_id": "formula_97", "formula_text": "b = 0,\u03bc < 1 2 , 1,\u03bc \u2265 1 2 .", "formula_coordinates": [24.0, 261.99, 143.07, 71.71, 27.82]}, {"formula_id": "formula_98", "formula_text": "1 2 P 0 \u03bc \u2265 1 2 + 1 2 P 1 \u03bc < 1 2 \u2265 1 4 .(8)", "formula_coordinates": [24.0, 220.92, 198.78, 320.52, 22.31]}, {"formula_id": "formula_99", "formula_text": "P 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc \u2265 P 0 \u03bc \u2265 1 2 , \u00b5(h * ) \u2264 1 2 \u2212 1 2\u02dc \u2265 P 0 \u03bc \u2265 1 2 + 15 16 \u2212 1 \u2265 P 0 \u03bc \u2265 1 2 \u2212 1 16 .(9)", "formula_coordinates": [24.0, 72.12, 251.49, 469.33, 22.31]}, {"formula_id": "formula_100", "formula_text": "P 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc \u2265 P 1 \u03bc < 1 2 , \u00b5(h * ) \u2265 1 2 + 1 2\u02dc \u2265 P 1 \u03bc < 1 2 \u2212 1 16 .(10)", "formula_coordinates": [24.0, 132.18, 302.28, 409.26, 22.31]}, {"formula_id": "formula_101", "formula_text": "1 2 P 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc + 1 2 P 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 2\u02dc \u2265 1 4 \u2212 1 16 > 1 8 .", "formula_coordinates": [24.0, 153.05, 359.04, 291.97, 22.31]}, {"formula_id": "formula_102", "formula_text": "\u00b5(h * ) = Pr(h * (x) = +1 | x A = 0) \u2212 Pr(h * (x) = +1 | x A = 1) = 1 d \u2212 1 d\u22121 i=1 1{h * (z i ) = +1},", "formula_coordinates": [24.0, 103.43, 477.24, 390.03, 30.32]}, {"formula_id": "formula_103", "formula_text": "Under H 0 , (d \u2212 1)\u00b5(h * ) is the sum of (d \u2212 1)", "formula_coordinates": [24.0, 55.44, 533.2, 186.67, 11.22]}, {"formula_id": "formula_104", "formula_text": "P 0 \u00b5(h * ) > 1 2 \u2212 1 2\u02dc \u2264 exp \u22122(d \u2212 1) \u2022 1 2\u02dc 2 \u2264 1 16 ,", "formula_coordinates": [24.0, 173.38, 565.34, 250.12, 25.51]}, {"formula_id": "formula_105", "formula_text": "\u221a d \u2265 10 \u221a d .", "formula_coordinates": [24.0, 302.91, 606.28, 46.69, 14.77]}, {"formula_id": "formula_106", "formula_text": "1 80 ]. If D X is such that x | x A = 0 \u223c N(0 d , I d ), whereas x | x A = 1 \u223c N(0 d , (0) d\u00d7d ) (i.", "formula_coordinates": [24.0, 55.44, 652.44, 486.0, 23.49]}, {"formula_id": "formula_107", "formula_text": "P A,h * \u03bc \u2212 \u00b5(h * ) > > 1 8 .", "formula_coordinates": [24.0, 235.58, 699.66, 125.72, 22.31]}, {"formula_id": "formula_108", "formula_text": "\u2022 H 0 : a * \u223c N(0, 1 d (1 +\u02dc )I d ) \u2022 H 1 : a * \u223c N(0, 1 d (1 \u2212\u02dc )I d )", "formula_coordinates": [25.0, 66.9, 203.48, 121.79, 25.43]}, {"formula_id": "formula_109", "formula_text": "P h * \u223cH1 \u00b5(h * ) < \u03a6(\u22121) \u2212\u02dc 36 \u2265 15 16 , where \u03a6(z) = z \u2212\u221e 1 \u221a 2\u03c0 e \u2212 z 2", "formula_coordinates": [25.0, 60.15, 265.91, 481.29, 33.58]}, {"formula_id": "formula_110", "formula_text": "KL(Q 0 ((x, y,\u1ef9) \u2264n ), Q 1 ((x,", "formula_coordinates": [25.0, 127.95, 383.27, 115.1, 9.65]}, {"formula_id": "formula_111", "formula_text": "KL(P 0 ,P 1 ) = n i=1 E KL(P 0 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )) . (11", "formula_coordinates": [25.0, 128.01, 550.06, 409.28, 30.32]}, {"formula_id": "formula_112", "formula_text": ")", "formula_coordinates": [25.0, 537.29, 560.79, 4.15, 8.64]}, {"formula_id": "formula_113", "formula_text": "KL(P 0 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )) \u2264 3\u02dc 2 . (12", "formula_coordinates": [25.0, 161.5, 612.78, 375.79, 11.72]}, {"formula_id": "formula_114", "formula_text": ")", "formula_coordinates": [25.0, 537.29, 615.17, 4.15, 8.64]}, {"formula_id": "formula_115", "formula_text": "a * | (x,\u1ef9) \u2264i\u22121 \u223c N \u00e2, 1 d (1 +\u02dc )X \u22a5 i\u22121 (X \u22a5 i\u22121 )", "formula_coordinates": [26.0, 195.07, 78.92, 188.26, 22.31]}, {"formula_id": "formula_116", "formula_text": "x i + 1, 1 d (1 +\u02dc )x i X \u22a5 i\u22121 (X \u22a5 i\u22121 ) x i . Similarly, under H 1 , we have\u1ef9 i | (x,\u1ef9) \u2264i\u22121 , x i has distribution N \u00e2, x i + 1, 1 d (1 \u2212\u02dc )x i X \u22a5 i\u22121 (X \u22a5 i\u22121 ) x i .", "formula_coordinates": [26.0, 55.44, 137.69, 449.07, 32.4]}, {"formula_id": "formula_117", "formula_text": "(\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )) = 0 \u2264 3\u02dc 2 .", "formula_coordinates": [26.0, 74.2, 207.15, 467.24, 21.61]}, {"formula_id": "formula_118", "formula_text": "y i | (x,\u1ef9) \u2264i\u22121 , x i are N(\u03bc i , (1 +\u02dc )\u03c3 2 i ) and N(\u03bc i , (1 \u2212\u02dc )\u03c3 2 i", "formula_coordinates": [26.0, 75.37, 248.83, 268.84, 12.33]}, {"formula_id": "formula_119", "formula_text": "\u03c3 2 i = 1 d x i X \u22a5 i\u22121 (X \u22a5 i\u22121 ) x i .", "formula_coordinates": [26.0, 75.37, 260.48, 112.27, 13.47]}, {"formula_id": "formula_120", "formula_text": "KL P 0 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i )), P 1 (\u1ef9 i = \u2022 | (x,\u1ef9) \u2264i\u22121 , x i ) =KL N(\u03bc i , (1 +\u02dc )\u03c3 2 i ), N(\u03bc i , (1 \u2212\u02dc )\u03c3 2 i ) = 1 2 1 +\u02dc 1 \u2212\u02dc \u2212 1 + ln( 1 \u2212\u02dc 1 +\u02dc ) \u2264 1 2 2\u02dc 1 \u2212\u02dc 2 \u22648\u02dc 2 ,", "formula_coordinates": [26.0, 180.84, 282.83, 250.56, 104.66]}, {"formula_id": "formula_121", "formula_text": "1 2 P 0 (b = 1) + 1 2 P 1 (b = 0) = 1 2 (1 \u2212 d TV (P 0 , P 1 )) \u2265 1 4 . (13", "formula_coordinates": [26.0, 185.51, 487.15, 351.78, 22.31]}, {"formula_id": "formula_122", "formula_text": ")", "formula_coordinates": [26.0, 537.29, 494.21, 4.15, 8.64]}, {"formula_id": "formula_123", "formula_text": "b = 0,\u03bc > \u03a6(\u22121), 1,\u03bc \u2264 \u03a6(\u22121).", "formula_coordinates": [26.0, 251.33, 545.43, 93.02, 23.08]}, {"formula_id": "formula_124", "formula_text": "1 2 P 0 \u03bc \u2264 \u03a6(\u22121) + 1 2 P 1 \u03bc > \u03a6(\u22121) \u2265 1 4 . (14", "formula_coordinates": [26.0, 206.15, 599.06, 331.14, 22.31]}, {"formula_id": "formula_125", "formula_text": ")", "formula_coordinates": [26.0, 537.29, 606.12, 4.15, 8.64]}, {"formula_id": "formula_126", "formula_text": "P 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc \u2265 P 0 \u03bc \u2264 \u03a6(\u22121), \u00b5(h * ) > \u03a6(1) \u2212 1 36\u02dc \u2265 P 0 \u03bc \u2264 \u03a6(\u22121) + 15 16 \u22121 \u2265 P 0 \u03bc \u2264 \u03a6(\u22121) \u2212 1 16 .", "formula_coordinates": [26.0, 55.44, 650.22, 500.1, 22.31]}, {"formula_id": "formula_127", "formula_text": "P 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc \u2265 P 1 \u03bc > \u03a6(\u22121) \u2212 1 16 . (16", "formula_coordinates": [26.0, 192.1, 697.03, 345.2, 22.31]}, {"formula_id": "formula_128", "formula_text": ")", "formula_coordinates": [26.0, 537.29, 704.09, 4.15, 8.64]}, {"formula_id": "formula_129", "formula_text": "1 2 P 0 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc + 1 2 P 1 \u03bc \u2212 \u00b5(h * ) \u2265 1 36\u02dc \u2265 1 4 \u2212 1 16 > 1 8 .", "formula_coordinates": [27.0, 148.07, 91.77, 301.93, 22.31]}, {"formula_id": "formula_130", "formula_text": "P(a * | (x,\u1ef9) \u2264i ) \u221dP(a * , (x,\u1ef9) \u2264i ) \u221dP(a * ) i j=1 P(x j | a * , (x,\u1ef9) \u2264j\u22121 )P(\u1ef9 j | x j , a * , (x,\u1ef9) \u2264j\u22121 ) \u221dP(a * ) i j=1 P(x j | (x,\u1ef9) \u2264j\u22121 )1 \u1ef9 j = x j , a * + 1 \u221dP(a * ) i j=1 1 \u1ef9 j = x j , a * + 1", "formula_coordinates": [27.0, 143.67, 250.77, 309.54, 119.93]}, {"formula_id": "formula_131", "formula_text": "P 0 (h * (x) = +1 | x A = 1) = 0,", "formula_coordinates": [27.0, 234.51, 463.49, 127.86, 11.72]}, {"formula_id": "formula_132", "formula_text": "P 0 (h * (x) = +1 | x A = 0) = P z\u223cN(0,I d ) ( a * , z \u2265 1) = P z\u223cN(0,I d ) a * a * , z \u2265 1 a * = 1 \u2212 \u03a6 1 a * .", "formula_coordinates": [27.0, 72.05, 506.81, 452.79, 23.89]}, {"formula_id": "formula_133", "formula_text": "d a * 2 2 (1+\u02dc ) \u223c \u03c7 2 (d)", "formula_coordinates": [27.0, 167.24, 551.86, 65.46, 16.28]}, {"formula_id": "formula_134", "formula_text": "d a * 2 2 (1+\u02dc ) \u2265 d \u2022 (1 \u2212 10 1 d ), which implies that 1 a * \u2264 1 (1 +\u02dc )(1 \u2212 10 1 d ) \u2264 1 (1 +\u02dc )(1 \u2212\u02dc 4 ) \u2264 1 \u2212\u02dc 4 .", "formula_coordinates": [27.0, 55.44, 551.86, 486.0, 81.06]}, {"formula_id": "formula_135", "formula_text": "[ 3 4 , 1], \u03a6(a) \u2212 \u03a6(b) \u2265 min \u03be\u2208[ 3 4 ,1] \u03a6 (\u03be)|a \u2212 b| \u2265 1 9", "formula_coordinates": [27.0, 174.65, 647.43, 209.04, 14.51]}, {"formula_id": "formula_136", "formula_text": "1 \u2212 \u03a6 1 a * \u2265 1 \u2212 \u03a6 1 \u2212\u02dc 4 \u2265 1 \u2212 (\u03a6(1) \u2212\u02dc 36 ) \u2265 \u03a6(\u22121) +\u02dc 36 .", "formula_coordinates": [27.0, 152.3, 673.57, 292.28, 22.31]}, {"formula_id": "formula_137", "formula_text": "\u00b5(h * ) = Pr x\u223cD X h * (x) = 1 | x A = 0 \u2212 Pr x\u223cD X h * (x) = 1 | x A = 1 ,", "formula_coordinates": [28.0, 145.03, 127.04, 306.83, 12.34]}, {"formula_id": "formula_138", "formula_text": "\u03b3 b = Pr x\u223cN(m b ,\u03a3 b ) h * (x) = 1 = Prx \u223cN(0,I d ) h * (m b + \u03a3 1/2 bx ) = 1 ; if we defineh b : R d \u2192 {\u22121, +1} such thath b (x) = h * (m b + \u03a3 1/2 bx ),(17)", "formula_coordinates": [28.0, 55.44, 175.59, 486.0, 63.67]}, {"formula_id": "formula_139", "formula_text": "m b + \u03a3 1/2", "formula_coordinates": [28.0, 129.04, 318.55, 44.14, 12.79]}, {"formula_id": "formula_140", "formula_text": "3:\u03b3 b \u2190 ESTIMATE-POSITIVE(h b , 2 ) 4: return\u03b3 0 \u2212\u03b3 1 Theorem E.6 (Upper bound). If h * \u2208 H lin , D X is such that x | x A = 0 \u223c N(m 0 , \u03a3 0 ), x | x A = 1 \u223c N(m 1 , \u03a3 1 ).", "formula_coordinates": [28.0, 55.11, 452.51, 488.07, 49.03]}, {"formula_id": "formula_141", "formula_text": "|\u03b3 b \u2212 \u03b3 b | \u2264 2 . Therefore, \u03bc \u2212 \u00b5(h * ) \u2264 |\u03b3 0 \u2212 \u03b3 0 | +|\u03b3 1 \u2212 \u03b3 1 | \u2264 .", "formula_coordinates": [28.0, 55.13, 569.92, 325.01, 48.98]}, {"formula_id": "formula_142", "formula_text": "5 2 (ln 1 ) 3 4 ( 1 ) 1 2 .", "formula_coordinates": [29.0, 234.49, 212.27, 58.37, 11.83]}, {"formula_id": "formula_143", "formula_text": "1 d i=1 m \u22122 i \u2265 1 d\u03b1 \u22122 \u2265 2 ln 1 .", "formula_coordinates": [29.0, 418.14, 268.84, 125.05, 16.58]}, {"formula_id": "formula_144", "formula_text": "0 \u2208 [d], such that |m i0 | \u2264 \u03b1. This implies that r = 1 d i=1 m \u22122 i \u2264 1 m \u22122 i 0 = |m i0 | \u2264 \u03b1.", "formula_coordinates": [29.0, 75.37, 333.92, 466.08, 30.24]}, {"formula_id": "formula_145", "formula_text": "\u03b3 \u2212 \u03b3(h * ) = \u03a6(sr) \u2212 \u03a6(sr) \u2264 1 \u221a 2\u03c0 \u2022|sr \u2212 sr| \u2264 .", "formula_coordinates": [29.0, 198.89, 439.34, 221.05, 23.42]}, {"formula_id": "formula_146", "formula_text": "1 + 2d + d(2 + log 2\u03b2 ) = O d ln d .", "formula_coordinates": [29.0, 216.46, 540.55, 163.95, 15.48]}, {"formula_id": "formula_147", "formula_text": "1 i\u2208Sm \u22122 i \u2212 1 i\u2208S m \u22122 i \u2264 .", "formula_coordinates": [29.0, 245.22, 633.72, 129.78, 25.55]}, {"formula_id": "formula_148", "formula_text": "1 i\u2208S m \u22122 i \u2212 1 d i=1 m \u22122 i \u2264 ,", "formula_coordinates": [29.0, 245.49, 691.06, 129.23, 26.64]}, {"formula_id": "formula_149", "formula_text": "|z \u2212 z S | \u2264 d \u03b2 2 \u2264 2 (4d ln 1 ) 3 2 , Also, note that 1 d i=1 m \u22122 i = r \u2264 \u03b1 implies that z \u2265 1 \u03b1 2 = 1 2d ln 1 ; therefore, z S \u2265 z \u2212 2 (4d ln 1 ) 3 2 \u2265 1 4d ln 1 . Now, by Lagrange mean value theorem, 1 \u221a z S \u2212 1 \u221a z \u2264 max z \u2208(z S ,z) 1 2 (z ) \u2212 3 2 \u2022|z s \u2212 z| \u2264 1 2 (z S ) \u2212 3 2 \u2022|z s \u2212 z| \u2264 1 2 (4d ln 1 ) 3 2 \u2022 2 (4d ln 1 ) 3 2 \u2264 .", "formula_coordinates": [30.0, 55.08, 108.76, 486.7, 106.65]}, {"formula_id": "formula_150", "formula_text": "\u2208 t m + (1 \u2212 t) n : t \u2208 (0, 1) , such that f ( m) \u2212 f ( n) = \u2207f (\u03b8), m \u2212 n \u2264 \u2207f (\u03b8) 1 m \u2212 n \u221e ,", "formula_coordinates": [30.0, 55.44, 301.84, 486.0, 33.91]}, {"formula_id": "formula_151", "formula_text": "m \u2208 R l : m i > 0, \u2200i (interior of R), \u2207f (m 1 , . . . , m l ) 1 \u2264 1. To see this, note that \u2207f (m 1 , . . . , m d ) = m \u22123 1 ( l i=1 m \u22122 i ) 3 2 , . . . , m \u22123 l ( l i=1 m \u22122 i ) 3 2 =: g,", "formula_coordinates": [30.0, 61.25, 353.3, 371.57, 51.52]}, {"formula_id": "formula_152", "formula_text": "f ( m) \u2212 f ( n) \u2264 k i=1 f (t i\u22121 m + (1 \u2212 t i\u22121 ) n) \u2212 f (t i m + (1 \u2212 t i ) n) \u2264 k i=1 (t i\u22121 m + (1 \u2212 t i\u22121 ) n) \u2212 (t i m + (1 \u2212 t i ) n) \u221e = k i=1 (t i\u22121 \u2212 t i ) m \u2212 n \u221e = m \u2212 n \u221e ,", "formula_coordinates": [30.0, 157.47, 527.18, 283.1, 115.22]}, {"formula_id": "formula_153", "formula_text": "i in [k]. Lemma E.9. Given i \u2208 [d] and \u03be > 0, if h * (\u03bee i ) = h * (\u2212\u03bee i ), then|m i | \u2265 \u03be.", "formula_coordinates": [30.0, 55.44, 652.83, 442.95, 28.4]}, {"formula_id": "formula_154", "formula_text": "1 2 P 0 b (Z) = 1 + 1 2 P 1 b (Z) = 0 \u2265 1 2 1 \u2212 d TV (P 0 , P 1 ) ,", "formula_coordinates": [31.0, 172.41, 173.55, 253.25, 22.31]}, {"formula_id": "formula_155", "formula_text": "KL(Q 0 , Q 1 ) =KL(Q 0 Z , Q 1 Z ) + E z\u223cQ 0 Z KL(Q 0 W |Z (\u2022 | z), Q 1 W |Z (\u2022 | z))", "formula_coordinates": [31.0, 150.54, 267.48, 286.66, 14.38]}, {"formula_id": "formula_156", "formula_text": "KL(P 0 , P 1 ) = N i=1 E KL(P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i ))", "formula_coordinates": [31.0, 130.23, 366.24, 332.28, 30.32]}, {"formula_id": "formula_157", "formula_text": "KL", "formula_coordinates": [31.0, 90.44, 438.77, 11.97, 8.74]}, {"formula_id": "formula_158", "formula_text": "+ ln P 0 (y i | (x, y) \u2264i\u22121 , x i ) P 1 (y i | (x, y) \u2264i\u22121 , x i ) = N i=1 (x,y) \u2264i P 0 ((x, y) \u2264i ) ln P 0 (y i | (x, y) \u2264i\u22121 , x i ) P 1 (y i | (x, y) \u2264i\u22121 , x i ) = N i=1 (x,y) \u2264i\u22121 ,xi P 0 ((x, y) \u2264i\u22121 , x i ) \u2022 yi P 0 (y i | (x, y) \u2264i\u22121 , x i ) ln P 0 (y i | (x, y) \u2264i\u22121 , x i ) P 1 (y i | (x, y) \u2264i\u22121 , x i ) = N i=1 E KL(P 0 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )), P 1 (y i = \u2022 | (x, y) \u2264i\u22121 , x i )) ,", "formula_coordinates": [31.0, 140.48, 469.87, 364.77, 140.17]}, {"formula_id": "formula_159", "formula_text": "Z | {Z \u2208 U } \u223c N(X \u2020 y, W W ).", "formula_coordinates": [32.0, 230.54, 248.59, 135.8, 10.81]}, {"formula_id": "formula_160", "formula_text": "Z | {Z \u2208 U } d = V V + W W | {Z \u2208 U } d =\u03b8 + W W | {Z \u2208 U } d = N(X \u2020 y, W W ).", "formula_coordinates": [32.0, 123.37, 632.8, 350.14, 13.26]}, {"formula_id": "formula_161", "formula_text": "max h 1 |G 1 | x\u2208G1 1{h(x) = 1} \u2212 1 |G 0 | x\u2208G0 1{h(x) = 1} + \u03bb( x\u2208S 1{h(x) = h * (x)})", "formula_coordinates": [33.0, 137.77, 261.94, 341.27, 26.8]}, {"formula_id": "formula_162", "formula_text": "max h 1 |G 1 | x\u2208G1 1{h(x) = 1} + 1 |G 0 | x\u2208G0 1{h(x) = \u22121} + \u03bb( x\u2208S 1{h(x) = h * (x)})", "formula_coordinates": [33.0, 133.9, 323.69, 349.01, 26.8]}], "doi": ""}