{"title": "A Universal Law of Robustness via Isoperimetry", "authors": "S\u00e9bastien Bubeck", "pub_date": "2022-12-23", "abstract": "Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a partial theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires d times more parameters than mere interpolation, where d is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry (or a mixture thereof). In the case of two-layer neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.", "sections": [{"heading": "Introduction", "text": "Solving n equations generically requires only n unknowns 1 . However, the revolutionary deep learning methodology revolves around highly overparametrized models, with many more than n parameters to learn from n training data points. We propose an explanation for this enigmatic phenomenon, showing in great generality that finding a smooth function to fit d-dimensional data requires at least nd parameters. In other words, overparametrization by a factor of d is necessary for smooth interpolation, suggesting that perhaps the large size of the models used in deep learning is a necessity rather than a weakness of the framework. Another way to phrase the result is as a tradeoff between the size of a model (as measured by the number of parameters) and its \"robustness\" (as measured by its Lipschitz constant): either one has a small model (with n parameters) which must then be non-robust, or one has a robust model (constant Lipschitz) but then it must be very large (with nd parameters). Such a tradeoff was conjectured for the specific case of two-layer neural networks and Gaussian data in [BLN21]. Our result shows that in fact it is a much more general phenomenon which applies to essentially any parametrized function class (including in particular deep neural networks) as well as a much broader class of data distributions. As conjectured in [BLN21] we obtain an entire tradeoff curve between size and robustness: our universal law of robustness states that, for any function class smoothly parametrized by p parameters, and for any d-dimensional dataset satisfying a natural isoperimetry condition, any function in this class that fits the data below the noise level must have (Euclidean) Lipschitz constant of order at least nd p .\nTheorem 1 (Informal version of Theorem 4). Let F be a class of functions from R d \u2192 R and let (xi, yi) i\u2208[n] be i.i.d. input-output pairs in R d \u00d7 [\u22121, 1]. Assume that:\n1. F admits a Lipschitz parametrization by p real parameters, each of size at most poly(n, d).\n2. The distribution \u00b5 of the covariates xi satisfies isoperimetry (or is a mixture theoreof ).\n3. The expected conditional variance of the output (i.e., the \"noise level\") is strictly positive, denoted\n\u03c3 2 \u2261 E \u00b5 [V ar[y|x]] > 0.\nThen, with high probability over the sampling of the data, one has simultaneously for all f \u2208 F:\n1 n n i=1 (f (xi) \u2212 yi) 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 Lip(f ) \u2265 \u2126 \u01eb \u03c3 nd p .\nRemark 1.1. For the distributions \u00b5 we have in mind, for instance uniform on the unit sphere, there exists with high probability some O(1)-Lipschitz function f : R d \u2192 R satisfying f (xi) = yi for all i. Indeed, with probability 1 \u2212 e \u2212\u2126(d) we have ||xi \u2212 xj|| \u2265 1 for all 1 \u2264 i = j \u2264 n so long as n \u2264 poly(d).\nIn this case we may apply the Kirszbraun extension theorem to find a suitable f regardless of the labels yi. More explicitly we may fix a smooth bump function g : R + \u2192 R with g(0) = 1 and g(a) = 0 for a \u2265 1, and then interpolate using the sum of radial basis functions\nf (x) = n i=1 g(||x \u2212 xi||) \u2022 yi.\n(1.1)\nIn fact this construction requires only p = n(d + 1) parameters to specify the values (xi, yi) i\u2208 [n] and thus determine the function f . Hence p = n(d + 1) parameters suffice for robust interpolation, i.e. Theorem 1 is essentially best possible when Lip(f ) = O(1). A similar construction shows the same conclusion for any p \u2208 [ \u2126(n), nd], essentially tracing the entire tradeoff curve. This is because one can first project onto a fixed subspace of dimensiond = p/n, and the projected inputs xi now have pairwise distances at least \u2126 d /d with high probability as long asd \u2265 \u2126(log n). The analogous construction on the projected points now requires only p =dn parameters and has Lipschitz constant\nO d/d = O nd p .\nRemark 1.2. Throughout this paper we evaluate accuracy of a classifier f via the sum of squared errors.\nIn other words, we focus on the regression setting rather than classification, which is much better suited to working with Lipschitz constants. However a version of our result extends to general Lipschitz loss functions, see Corollary 4.2.", "publication_ref": ["b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Speculative implication for real data", "text": "To put Theorem 1 in context, we compare to the empirical results presented in [MMS + 18]. In the latter work, they consider the MNIST dataset which consists of n = 6 \u00d7 10 4 images in dimension 28 2 = 784. They trained robustly different architectures, and reported in Figure 4 (third plot from the left) the size of the architecture versus the obtained robust test accuracy 2 . One can see a sharp transition from roughly 10% accuracy to roughly 90% accuracy at around 2 \u00d7 10 5 parameters (capacity scale 4 in their notation). Moreover the robust accuracy continues to increase as more parameters are added, reaching roughly 95% accuracy at roughly 3 \u00d7 10 6 parameters.\nHow can we compare these numbers to the law of robustness? There are a number of difficulties that we discuss below, and we emphasize that this discussion is highly speculative in nature, though we find that, with a few leaps of faith, our universal law of robustness sheds light on the potential parameter regimes of interest for robust deep learning.\nThe first difficulty is to evaluate the \"correct\" dimension of the problem. Certainly the number of pixels per image gives an upper bound, however one expects that the data lies on something like a lower dimensional sub-manifold. Optimistically, we hope that Theorem 1 will continue to apply for an appropriate effective dimension which may be rather smaller than the literal number of pixels. This hope is partially justified by the fact that isoperimetry holds in many less-than-picturesque situations, some of which are stated in the next subsection.\nEstimating the effective dimension of data manifolds is an interesting problem and has attracted some study in its own right. For instance [FdRL17, PZA + 21] both predict that MNIST has effective dimension slightly larger than 10, which is consistent with our numerical discussion at the end of this subsection.\nThe latter also predicts an effective dimension of about 40 for ImageNet. It is unclear how accurate these estimates are for our setting. One concrete issue is that from the point of view of isoperimetry, a \"smaller\" manifold (e.g. a sphere with radius r < 1) will behave as though it has a larger effective dimension (e.g. d/r 2 instead of d). Thus we expect the \"scale\" of the mixture components to also be relevant for studying real datasets through our result.\nAnother difficulty is to estimate/interpret the noise value \u03c3 2 . From a theoretical point of view, this noise assumption is necessary for otherwise there could exist a smooth classifier with perfect accuracy in F, defeating the point of any lower bound on the size of F. We tentatively would like to think of \u03c3 2 as capturing the contribution of the \"difficult\" part of the learning problem, that is \u03c3 2 could be thought of as the non-robust generalization error of reasonably good models, so a couple of % of error in the case of MNIST. With that interpretation, one gets \"below the noise level\" in MNIST with a training error of a couple of %. We believe that versions of the law of robustness might hold without noise; these would need to go beyond representational power and consider the dynamics of learning algorithms.\nFinally another subtlety to interpret the empirical results of [MMS + 18] is that there is a mismatch between what they measure and our quantities of interest. Namely the law of robustness relates two quantities: the training error, and the worst-case robustness (i.e. the Lipschitz constant). On the other hand [MMS + 18] measures the robust generalization error. Understanding the interplay between those three quantities is a fantastic open problem. Here we take the perspective that a small robust generalization error should imply a small training error and a small Lipschitz constant.\nAnother important mismatch is that we stated our universal law of robustness for Lipschitzness in \u21132, while the experiments in [MMS + 18] are for robustness in \u2113\u221e. We believe that a variant of the law of robustness remains true for \u2113\u221e, a belief again partially justified by how broad isoperimetry is (see next subsection). With all the caveats described above, we can now look at the numbers as follows: in the [MMS + 18] experiments, smooth models with accuracy below the noise level are attained with a number of parameters somewhere in the range 2 \u00d7 10 5 \u2212 3 \u00d7 10 6 parameters (possibly even larger depending on the interpretation of the noise level), while the law of robustness would predict any such model must have at least nd parameters, and this latter quantity should be somewhere in the range 10 6 \u2212 10 7 (corresponding to an effective dimension between 15 and 150). While far from perfect, the law of robustness prediction is far more accurate than the classical rule of thumb # parameters \u2243 # equations (which here would predict a number of parameters of the order 10 4 ).\nPerhaps more interestingly, one could apply a similar reasoning to the ImageNet dataset, which consists of 1.4 \u00d7 10 7 images of size roughly 2 \u00d7 10 5 . Estimating that the effective dimension is a couple of order of magnitudes smaller than this size, the law of robustness predicts that to obtain good robust models on ImageNet one would need at least 10 10 \u2212 10 11 parameters. This number is larger than the size of current neural networks trained robustly for this task, which sports between 10 8 \u2212 10 9 parameters. Thus, we arrive at the tantalizing possibility that robust models for ImageNet do not exist yet simply because we are a couple orders of magnitude off in the current scale of neural networks trained for this task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Theorem 1 is a direct follow-up to the conjectured law of robustness in [BLN21] for (arbitrarily weighted) two-layer neural networks with Gaussian data. Our result does not actually prove their conjecture, because we assume here polynomially bounded weights. While this assumption is reasonable from a practical perspective, it remains mathematically interesting to prove the full conjecture for the two-layer case. We prove however in Section A that the polynomial weights assumption is necessary as soon as one considers three-layer neural networks. Let us also mention [GCL + 19, Theorem 6.1] which showed a lower bound \u2126(nd) on the VC dimension of any function class which can robustly interpolate arbitrary labels on all well-separated input sets (x1, . . . , xn). This can be viewed as a restricted version of the law of robustness for the endpoint case L = O(1), where the Lipschitz constant is replaced by a robust interpolation property. Their statement and proof are of a combinatorial nature, as opposed to our probabilistic approach. We also note that a relation between high-dimensional phenomenon such as concentration and adversarial examples has been hypothesized before, such as in [GMF + 18].\nIn addition to [MMS + 18], several recent works have experimentally studied the relationship between a neural network scale and its achieved robustness, see e.g., [NBA + 18, XY20, GQU + 20]. It has been consistently reported that larger networks help tremendously for robustness, beyond what is typically seen for classical non-robust accuracy. We view our universal law of robustness as putting this empirical observation on a more solid footing: scale is actually necessary to achieve robustness.\nAnother empirical thread intimately related to scale is the question of network compression, and specifically knowledge distillation [HVD15]. The idea is to first train a large neural network, and then \"distill\" it to a smaller net. It is natural to wonder whether this could be a way around the law of robustness, alas we show in Theorem 4 that such an approach cannot work. Indeed the latter part of Theorem 4 shows that the law of robustness tradeoff for the distilled net can only be improved by a logarithmic factor in the size of the original large neural network. Thus, unless one uses exponentially large networks, distillation does not offer a way around the law of robustness. A related question is whether there might be an interaction between the number of parameters and explicit or implicit regularization, which are commonly understood to reduce effective model complexity. In our approach the number of parameters enters in bounding the covering number of F in the rather strict L \u221e (R d ; R) norm, which seems difficult to control by other means.\nThe law of robustness setting is also closely related to the interpolation setting: in the former case one considers models optimizing \"beyond the noise level\", while in the latter case one studies models with perfect fit on the training data. The study of generalization in this interpolation regime has been a central focus of learning theory in the last few years (see e.g., [BHMM19, MM19, BLLT20, NKB + 20]), as it seemingly contradicts classical theory about regularization. More broadly though, generalization remains a mysterious phenomon in deep learning, and the exact interplay between the law of robustness' setting (interpolation regime/worst-case robustness) and (robust) generalization error is a fantastic open problem. Interestingly, we note that one could potentially avoid the conclusion of the law of robustness (that is, that large models are necessary for robustness), with early stopping methods that could stop the optimization once the noise level is reached. In fact, this theoretically motivated suggestion has already been empirically tested and confirmed in the recent work [RWK20], showing again a close tie between the conclusions one can draw from the law of robustness and actual practical settings.\nClassical lower bounds on the gradient of a function include Poincar\u00e9 type inequalities, but they are of a qualitatively different nature compared to the law of robustness lower bound. We recall that a measure \u00b5 on R d satisfies a Poincar\u00e9 inequality if for any function f , one has E \u00b5 [ \u2207f 2 ] \u2265 C \u2022 Var(f ) (for some constant C > 0). In our context, such a lower bound for an interpolating function f has essentially no consequence since the variance f could be exponentially small. In fact this is tight, as one can easily use similar constructions to those in [BLN21] to show that one can interpolate with an exponentially small expected norm squared of the gradient (in particular it is crucial in the law of robustness to consider the Lipschitz constant, i.e., the supremum of the norm of the gradient). On the other hand, our isoperimetry assumption is related to a certain strenghtening of the Poincar\u00e9 inequality known as log-Sobolov inequality (see e.g., [Led01]). If the covariate measure satisfies only a Poincar\u00e9 inequality, then we could prove a weaker law of robustness of the form Lip n \u221a d p (using for example the concentration result obtained in [BL97]). For the case of two-layer neural networks there is another natural notion of smoothness (different from \u2113p norms of the gradient) that can be considered, known as the Barron norm. In [BELM20] it is shown that for such a notion of smoothness there is no tradeoff\u00e0 la the law of robustness, namely one can simultaneously be optimal both in terms of Barron norm and in terms of the network size. More generally, it is an interesting challenge to understand for which notions of smoothness there is a tradeoff with size.", "publication_ref": ["b7", "b17", "b32", "b7", "b21", "b4", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Isoperimetry", "text": "Concentration of measure and isoperimetry are perhaps the most ubiquitous features of high-dimensional geometry. In short, they assert in many cases that Lipschitz functions on high-dimensional space concentrate tightly around their mean. Our result assumes that the distribution \u00b5 of the covariates xi satisfies such an inequality in the following sense. Definition 1.1. A probability measure \u00b5 on R d satisfies c-isoperimetry if for any bounded L-Lipschitz f : R d \u2192 R, and any t \u2265 0,\nP[|f (x) \u2212 E[f ]| \u2265 t] \u2264 2e \u2212 dt 2 2cL 2 .\n(1.2)\nIn general, if a scalar random variable X satisfies P[|X| \u2265 t] \u2264 2e \u2212t 2 /C then we say X is Csubgaussian. Hence isoperimetry states that the output of any Lipschitz function is O(1)-subgaussian under suitable rescaling. Distributions satisfying O(1)-isoperimetry include high dimensional Gaussians\n\u00b5 = N 0, I d d\nand uniform distributions on spheres and hypercubes (normalized to have diameter 1).\nIsoperimetry also holds for mild perturbations of these idealized scenarios, including 3 :\n\u2022 The sum of a Gaussian and an independent random vector of small norm [CCNW21].\n\u2022 Strongly log-concave measures in any normed space [BL00, Proposition 3.1].\n\u2022 Manifolds with positive Ricci curvature [Gro86, Theorem 2.2].\nDue to the last condition above, we believe our results are realistic even under the manifold hypothesis that high-dimensional data tends to lie on a lower-dimensional submanifold (which may be difficult to describe cleanly with coordinates). Recalling the discussion of Subsection 1.1, [Gro86, Theorem 2.2] implies that for submanifolds M \u2286 R with Ricci curvature \u2126(dim(M )) uniformly 4 , the law of robustness provably holds relative to the intrinsic dimension dim(M ). This viewpoint on learning has been studied for decades, see e.g. [HS89, KL93, RS00, TDSL00, NM10, FMN16]. We also note that our formal theorem (Theorem 4) actually applies to distributions that can be written as a mixture of distributions satisfying isoperimetry. Let us also point out that from a technical perspective, our proof is not tied to the Euclidean norm and applies essentially whenever Definition 1.1 holds. The main difficulty in extending the law of robustness to e.g. the earth-mover distance seems to be identifying realistic cases which satisfy isoperimetry.\nOur proofs will repeatedly use the following simple fact: Proposition 1.2. If X1, . . . , Xn are independent and C-subgaussian, with mean 0, then\nXav = 1 \u221a n n i=1 Xi is 18C-subgaussian. Proof. By [vH14, Exercise 3.1 part d.], E e X 2 i /3C \u2264 2, i \u2208 [n].\nIt is immediate by H\u00f6lder that the same bound holds for Xav in place of Xi, and using [vH14, Exercise 3.1 parts e. and c.] now implies the first claim. The second claim follows similarly, since by convexity we have\nE[e Y 2 /3C ] \u2264 E[e X 2 1 /3C ] \u2264 2.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "A finite approach to the law of robustness", "text": "For the function class of two-layer neural networks, [BLN21] investigated several approaches to prove the law of robustness. At a high level, the proof strategies there relied on various ways to measure how \"large\" the set of two-layer neural networks can be (specifically, they tried a geometric approach based on relating to multi-index models, a statistical approach based on the Rademacher complexity, and an algebraic approach for the case of polynomial activations).\nIn this work we take here a different route: we shift the focus from the function class F to an individual function f \u2208 F. Namely, our proof starts by asking the following question: for a fixed function f , what is the probability that it would give a good approximate fit on the (random) data? For simplicity, consider for a moment the case where we require f to actually interpolate the data (i.e., perfect fit), and say that yi are random \u00b11 labels. The key insight is that isoperimetry implies that either the 0-level set of f or the 1-level set of f must have probability smaller than exp \u2212 d Lip(f ) 2 . Thus, the probability that f fits all the n points is at most exp \u2212 nd Lip(f ) 2 so long as both labels yi \u2208 {\u22121, 1} actually appear a constant fraction of the time. In particular, using an union bound 5 , for a finite function class F of size N with L-Lipschitz functions, the probability that there exists a function f \u2208 F fitting the data is at most\nN exp \u2212 nd L 2 = exp log(N ) \u2212 nd L 2 .\nThus we see that, if L \u226a nd log(N) , then the probability of finding a fitting function in F is very small. This basically concludes the proof, since via a standard discretization argument, for a smoothly parametrized family with p (bounded) parameters one expects log(N ) =\u00d5(p).\nWe now give the formal proof, which applies in particular to approximate fit rather than exact fit in the argument above. The only difference is that we will identify a well-chosen subgaussian random variable in the problem. We start with the finite function class case:\nTheorem 2. Let (xi, yi) i\u2208[n] be i.i.d. input-output pairs in R d \u00d7 [\u22121, 1] such that:\n1. The distribution \u00b5 of the covariates xi can be written as \u00b5 = k \u2113=1 \u03b1 \u2113 \u00b5 \u2113 , where each \u00b5 \u2113 satisfies c-isoperimetry and \u03b1 \u2113 \u2265 0, k \u2113=1 \u03b1 \u2113 = 1. 2. The expected conditional variance of the output is strictly positive, denoted\n\u03c3 2 \u2261 E \u00b5 [V ar[y|x]] > 0.\nThen one has:\nP \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 4k exp \u2212 n\u01eb 2 8 3 k + 2 exp log(|F|) \u2212 \u01eb 2 nd 10 4 cL 2 .\nWe start with a lemma showing that, to optimize beyond the noise level one must necessarily correlate with the noise part of the labels. Below and throughout the rest of the paper we write\ng(x) = E[y|x], zi = yi \u2212 g(xi)\nfor the target function, and for the noise part of the observed labels, respectively. (In particular yi is the sum of the target function g(xi) and the noise term zi.)\nLemma 2.1. One has\nP \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 2 exp \u2212 n\u01eb 2 8 3 + P \u2203f \u2208 F : 1 n n i=1 f (xi)zi \u2265 \u01eb 4 .\nProof. The sequence (z 2 i ) is i.i.d., with mean \u03c3 2 , and such that |zi| 2 \u2264 4. Thus Hoeffding's inequality yields:\nP 1 n n i=1 z 2 i \u2264 \u03c3 2 \u2212 \u01eb 6 \u2264 exp \u2212 n\u01eb 2 8 3 . (2.1)\nOn the other hand the sequence (zig(xi)) is i.i.d., with mean 0 (since E[zi|xi] = 0), and such that |zig(xi)| \u2264 2. Thus Hoeffding's inequality yields:\nP 1 n n i=1 zig(xi) \u2264 \u2212 \u01eb 6 \u2264 exp \u2212 n\u01eb 2 8 3 . (2.2) Let us write Z = 1 \u221a n (z1, . . . , zn), G = 1 \u221a n (g(x1), . . . , g(xn)\n), and F = 1 \u221a n (f (x1), . . . , f (xn)). We claim that if Z 2 \u2265 \u03c3 2 \u2212 \u01eb 6 and Z, G \u2265 \u2212 \u01eb 6 , then for any f \u2208 F one has\nG + Z \u2212 F 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 F, Z \u2265 \u01eb 4 .\nThis claim together with (2.1) and (2.2) conclude the proof. On the other hand the claim itself directly follows from:\n\u03c3 2 \u2212 \u01eb \u2265 G + Z \u2212 F 2 = Z + G \u2212 F 2 = Z 2 + 2 Z, G \u2212 F + G \u2212 F 2 \u2265 \u03c3 2 \u2212 \u01eb 2 \u2212 2 Z, F .\nWe can now proceed to the proof of Theorem 2:\nProof. First note that without loss of generality we can assume that the range of any function in F is included in [\u22121, 1] (indeed clipping the values improves both the fit to any y \u2208 [\u22121, 1] and the Lipschitz constant). We also assume without loss of generality that all functions in F are L-Lipschitz.\nFor clarity let us start with the case k = 1. By the isoperimetry assumption we have that\nd c f (x i )\u2212E[f ] L is 2-subgaussian. Since |zi| \u2264 2, we also have that d c (f (x i )\u2212E[f ])z i L\nis 8-subgaussian. Moreover, the latter random variable has mean zero since E[z|x] = 0. Thus by Proposition 1.2 (and 8 \u00d7 18 = 12 2 ) we have:\nP d cnL 2 n i=1 (f (xi) \u2212 E[f ])zi \u2265 t \u2264 2 exp \u2212(t/12) 2 .\nRewriting (and noting 12 \u00d7 8 \u2264 10 2 ), we find:\nP 1 n n i=1 (f (xi) \u2212 E[f ])zi \u2265 \u01eb 8 \u2264 2 exp \u2212 \u01eb 2 nd 10 4 cL 2 .\n(2.3) Since we assumed that the range of the functions is in [\u22121, 1] we have E[f ] \u2208 [\u22121, 1] and hence:\nP \u2203f \u2208 F : 1 n n i=1 E[f ]zi \u2265 \u01eb 8 \u2264 P 1 n n i=1 zi \u2265 \u01eb 8 .\n(2.4) (This step is the analog of requiring the labels yi to be well-balanced in the example of perfect interpolation.) By Hoeffding's inequality, the above quantity is smaller than 2 exp(\u2212n\u01eb 2 /8 3 ) (recall that |zi| \u2264 2).\nThus we obtain with a union bound:\nP \u2203f \u2208 F : 1 n n i=1 f (xi)zi \u2265 \u01eb 4 \u2264 |F| \u2022 P 1 n n i=1 (f (xi) \u2212 E[f ])zi \u2265 \u01eb 8 + P 1 n n i=1 zi \u2265 \u01eb 8 \u2264 2|F| \u2022 exp \u2212 \u01eb 2 nd 10 4 cL 2 + 2 exp \u2212 n\u01eb 2 8 3 .\nTogether with Lemma 2.1 this concludes the proof for k = 1.\nWe now turn to the case k > 1. We first sample the mixture component \u2113i \u2208 [k] for each data point i \u2208 [n], and we now reason conditioned on these mixture components. Let S \u2113 \u2282 [n] be the set of data points sampled from mixture component \u2113 \u2208 [k], that is xi, i \u2208 S \u2113 , is i.i.d. from \u00b5 \u2113 . We now have that\nd c f (x i )\u2212E \u00b5 \u2113 i [f ]\nL is 1-subgaussian (notice that the only difference is that now we need to center by E \u00b5 \u2113 i [f ], which depends on the mixture component). In particular using the same reasoning as for (2.3) we obtain (crucially note that Proposition 1.2 does not require the random variables to be identically distributed):\nP 1 n n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2265 \u01eb 8 \u2264 2 exp \u2212 \u01eb 2 nd 9 4 cL 2 .\n(2.5)\nNext we want to appropriately modify (2.4). To do so note that:\nmax m 1 ,...,m k \u2208[\u22121,1] n i=1 m \u2113 i zi = k \u2113=1 i\u2208S \u2113 zi ,\nso that we can rewrite (2.4) as:\nP \u2203f \u2208 F : 1 n n i=1 E \u00b5 \u2113 i [f ]zi \u2265 \u01eb 8 \u2264 P \uf8eb \uf8ed 1 n k \u2113=1 i\u2208S \u2113 zi \u2265 \u01eb 8 \uf8f6 \uf8f8 . Now note that k \u2113=1 |S \u2113 | \u2264 \u221a\nnk and thus we have:\nP \uf8eb \uf8ed 1 n k \u2113=1 i\u2208S \u2113 zi \u2265 \u01eb 8 \uf8f6 \uf8f8 \u2264 P \uf8eb \uf8ed k \u2113=1 i\u2208S \u2113 zi \u2265 \u01eb 8 n k k \u2113=1 |S \u2113 | \uf8f6 \uf8f8 \u2264 k \u2113=1 P \uf8eb \uf8ed i\u2208S \u2113 zi \u2265 \u01eb 8 n k |S \u2113 | \uf8f6 \uf8f8 .\nFinally by Hoeffding's inequality, we have for any \u2113 \u2208\n[k], P i\u2208S \u2113 zi \u2265 t |S \u2113 | \u2264 2 exp \u2212 t 2 8\n, and thus the last display is bounded from above by 2k exp \u2212 n\u01eb 2 8 3 k . The proof can now be concluded as in the case k = 1.\nIn fact the above result can be further improved for small \u03c3 using the following Lemma 2.2. Note that the additional assumption on d is rather mild because it is required for the latter term to be smaller than |F|e \u2212O(n) . (In particular, we are primarily interested in the regime of large n, d and constant \u03c3, \u01eb, c.) Lemma 2.2. There exist absolute constants C1, C2 such that the following holds. In the setting of Theorem 2, assume\nd \u2265 C1 \u2022 cL 2 \u03c3 2 \u01eb 2 . Then P \u2203f \u2208 F : 1 n n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2265 \u01eb 8 \u2264 exp \u2212 n\u03c3 4 8 + exp log |F| \u2212 \u01eb 2 nd C1cL 2 \u03c3 2 .\nProof. We use the simple estimate sup\nf \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2264 sup f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u00d7 n i=1 z 2 i .\n(2.6) Applying Hoeffding's inequality as in (2.1) yields\nP n i=1 z 2 i \u2265 2\u03c3 2 n \u2264 exp \u2212 n\u03c3 4 8 . (2.7)\nNext we upper bound the tail of\nn i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 for each fixed f . Since f (xi) \u2212 E \u00b5 \u2113 i [f ]\nis sub-Gaussian, it follows that its square is sub-exponential, i.e. (recall [Ver18, Definition 2.7.5])\n(f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u03c8 1 \u2264 O(cL 2 /d). Let Wi = (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2212 E (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2\nand note that\n0 \u2264 E (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2264 O(cL 2 /d).\n(2.8)\nAs centering decreases the sub-exponential norm ([Ver18, Exercise 2.7.10]), we have\nWi \u03c8 1 \u2264 O(cL 2 /d) Note that for d \u2265 2 8 cL 2 \u03c3 2 \u01eb 2\n(which is ensured for a large constant C1 in the hypothesis) we have min\n\uf8eb \uf8ec \uf8ed n\u01eb 2 2 8 \u03c3 2 2 n(cL 2 /d) 2 , n\u01eb 2 2 8 \u03c3 2 cL 2 /d \uf8f6 \uf8f7 \uf8f8 = min \u01eb 4 nd 2 2 16 (cL 2 ) 2 \u03c3 4 , \u01eb 2 nd 2 8 cL 2 \u03c3 2 = \u01eb 2 nd 2 8 cL 2 \u03c3 2 .\nHence Bernstein's inequality (e.g. [Ver18, Theorem 2.8.1]) implies\nP n i=1 Wi \u2265 n\u01eb 2 2 8 \u03c3 2 \u2264 2 exp \u2212\u2126 \u01eb 2 nd cL 2 \u03c3 2 .\nRecalling (2.8) and union bounding over f \u2208 F, we find\nP sup f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2265 n\u01eb 2 2 7 \u03c3 2 \u2264 |F| \u2022 sup f \u2208F P n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2265 O cL 2 n d + n\u01eb 2 2 8 \u03c3 2 \u2264 2|F| exp \u2212\u2126 \u01eb 2 nd cL 2 \u03c3 2 .\n(2.9) (Here we again used the assumed lower bound on d.) Finally on the event that both sup\nf \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2264 n\u01eb 2 2 7 \u03c3 2 , n i=1 z 2 i \u2264 2\u03c3 2 n hold, applying (2.6) yields sup f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2264 n\u01eb 2 2 7 \u03c3 2 \u00d7 \u221a 2\u03c3 2 n \u2264 n\u01eb 8 .\nCombining (2.7) with (2.9) now completes the proof.\nBy using Lemma 2.2 in place of (2.5) when proving Theorem 2, one readily obtains the following.\nTheorem 3. There exist absolute constants C1, C2 such that the following holds. In the setting of Theorem 2, assume\nd \u2265 C1 \u2022 cL 2 \u03c3 2 \u01eb 2 . Then P \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 (4k + 1) exp \u2212 n\u01eb 2 8 3 k + exp log |F| \u2212 \u01eb 2 nd C2cL 2 \u03c3 2 .\nProof. Using Lemma 2.2 in place of (2.5) when proving Theorem 2 immediately implies\nP \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 4k exp \u2212 n\u01eb 2 8 3 k +exp \u2212 n\u03c3 4 8 +exp log |F| \u2212 \u01eb 2 nd C2cL 2 \u03c3 2 .\nIt remains to observe that \u01eb 2 8 3 k \u2264 \u03c3 4 8 since \u01eb \u2264 \u03c3 2 . Finally we can now state and prove the formal version of the informal Theorem 1 from the introduction.\nTheorem 4. Let F be a class of functions from R d \u2192 R and let (xi, yi) i\u2208[n] be i.i.d. input-output pairs in R d \u00d7 [\u22121, 1]. Fix \u01eb, \u03b4 \u2208 (0, 1). Assume that:\n1. The function class can be written as F = {fw, w \u2208 W} with W \u2282 R p , diam(W) \u2264 W and for any w1, w2 \u2208 W, ||fw 1 \u2212 fw 2 ||\u221e \u2264 J||w1 \u2212 w2||.\n2. The distribution \u00b5 of the covariates xi can be written as \u00b5 = k \u2113=1 \u03b1 \u2113 \u00b5 \u2113 , where each \u00b5 \u2113 satisfies c-isoperimetry, \u03b1 \u2113 \u2265 0, k \u2113=1 \u03b1 \u2113 = 1, and k is such that 10 4 k log(8k/\u03b4) \u2264 n\u01eb 2 .\n(2.10)\n3. The expected conditional variance of the output is strictly positive, denoted\n\u03c3 2 \u2261 E \u00b5 [V ar[y|x]] > 0.\n4. The dimension d is large compared to \u01eb:\nd \u2265 C1 cL 2 \u03c3 2 \u01eb 2 .\n(2.11)\nThen, with probability at least 1 \u2212 \u03b4 with respect to the sampling of the data, one has simultaneously for all f \u2208 F:\n1 n n i=1 (f (xi) \u2212 yi) 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 Lip(f ) \u2265 \u01eb \u03c3 \u221a C2c \u00d7 nd p log(1 + 60W J\u01eb \u22121 ) + log(4/\u03b4)\n.\n(2.12)\nMoreover if W consists only of s-sparse vectors with ||w||0 \u2264 s, then the above inequality improves to\n1 n n i=1 (f (xi) \u2212 yi) 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 Lip(f ) \u2265 \u01eb \u03c3 \u221a C2c nd s log p(1 + 60W J\u01eb \u22121 ) + log(4/\u03b4) . (2.13)\nNote that as in the previous lemmas, Theorem 4 requires the dimension d to be at least a constant depending on \u01eb in (2.11). This extra condition is unnecessary if one uses Theorem 2 in place of Theorem 3 (which would sacrifice a factor \u03c3 in the resulting lower bound on Lip(f )).\nProof of Theorem 4. Define WL \u2286 W by WL \u2261 {w \u2208 W : Lip(fw) \u2264 L}.\nDenote WL,\u01eb \u2286 WL for an \u01eb 8J -net of WL. We have in particular |W\u01eb| \u2264 (1 + 60W J\u01eb \u22121 ) p (see e.g. [Ver18, Corollary 4.2.13]). We apply Theorem 3 to FL,\u01eb \u2261 {fw, w \u2208 WL,\u01eb}:\nP \u2203f \u2208 FL,\u01eb : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb 2 \u2264 (4k + 1) exp \u2212 n\u01eb 2 9 4 k + exp p log(1 + 60W J\u01eb \u22121 ) \u2212 \u2126 \u01eb 2 nd cL 2 \u03c3 2 . Observe that if f \u2212 g \u221e \u2264 \u01eb 8 and y \u221e , f \u221e , g \u221e \u2264 1, then 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u01eb 2 + 1 n n i=1 (yi \u2212 g(xi)) 2 .\n(We may again assume without loss of generality that all functions in F map to [\u22121, 1].) Thus we obtain for any L > 0 and an absolute constant C1\nP \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb and Lip(f ) \u2264 L (2.14) \u2264 (4k + 1) exp \u2212 n\u01eb 2 10 4 k + exp p log(1 + 60W J\u01eb \u22121 ) \u2212 \u01eb 2 nd C1cL 2 \u03c3 2 .\nThe first assumption ensures that for any w \u2208 WL, there is w \u2032 \u2208 WL,\u01eb with fw \u2212 f w \u2032 \u221e \u2264 \u01eb 8 . = Finally we use the second assumption to show the probability in (2.14) just above is at most \u03b4 if\nL \u2264 \u01eb C2\u03c3 \u221a c nd p log(1 + 60W J\u01eb \u22121 ) + log(4/\u03b4)\nfor a large absolute constant C2. The first term is estimated (recall (2.10)) via\n(4k + 1) exp \u2212 n\u01eb 2 10 4 k \u2264 (4k + 1)\u03b4 8k \u2264 3\u03b4 4 .\nThe second term is estimated by\nexp p log(1 + 60W J\u01eb \u22121 ) \u2212 \u01eb 2 nd C2cL 2 \u03c3 2 \u2264 e \u2212 log(4/\u03b4) = \u03b4 4\nCombining these estimates on (2.14) proves (2.12).\nTo show (2.13), the proof proceeds identically after the improved estimate |W\u01eb| \u2264 p(1+60W J\u01eb \u22121 ) s .\nTo obtain this estimate, note that the number of s-subsets S \u2286 [p] s is at most p s . Letting WS consist of those w \u2208 W with wi = 0 for all i / \u2208 S, the size of an \u01eb-net WS,\u01eb for WS is |WS,\u01eb| \u2264 (1 + 60W J\u01eb \u22121 ) s . Therefore the union\nS\u2286( [p] s ) WS,\u01eb is an \u01eb-net of W of size at most p(1 + 60W J\u01eb \u22121 )\ns as claimed above.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Deep neural networks", "text": "We now specialize the law of robustness (Theorem 4) to multi-layer neural networks. We consider a rather general class of depth D neural networks described as follows. First, we require that the neurons are partitioned into layers L1, . . . , LD, and that all connections are from Li \u2192 Lj for some i < j. This includes the basic feed-forward case in which only connections Li \u2192 Li+1 are used as well as more general skip connections. We specify (in the natural way) a neural network by matrices Wj of shape |Lj| \u00d7 i<j |Li| for each 1 \u2264 j \u2264 D, as well as 1-Lipschitz non-linearities \u03c3 j,\u2113 and scalar biases b j,\u2113 for each (j, \u2113) satisfying \u2113 \u2208 |Lj|. We use fixed non-linearities \u03c3 j,\u2113 as well as a fixed architecture, in the sense that each matrix entry Wj[k, \u2113] is either always 0 or else it is variable (and similarly for the bias terms).\nTo match the notation of Theorem 4, we identify the parametrization in terms of the matrices (Wj) and bias terms (b j,\u2113 ) to a single p-dimensional vector w as follows. A variable matrix entry Wj[k, \u2113] is set to w a(j,k,\u2113) for some fixed index a(j, k, \u2113) \u2208 [p], and a variable bias term b j,\u2113 is set to w a(j,\u2113) for some a(j, \u2113) \u2208 [p]. Thus we now have a parametrization w \u2208 R p \u2192 fw where fw is the neural network represented by the parameter vector w. Importantly, note that our formulation allows for weight sharing (in the sense that a shared weight is counted only as a single parameter). For example, this is important to obtain an accurate count of the number of parameters in convolutional architectures.\nIn order to apply Theorem 4 to this class of functions we need to estimate the Lipschitz constant of the parametrization w \u2192 fw. To do this we introduce three more quantities. First, we shall assume that all the parameters are bounded in magnitude by W , that is we consider the set of neural networks parametrized by w \u2208 [\u2212W, W ] p . Next, for the architecture under consideration, denote Q for the maximum number of matrix entries/bias terms that are tied to a single parameter wa for some a \u2208 Lemma 3.1 shows that when applying Theorem 4 to our class of neural networks one can always take J = R(W Qp) D (assuming that the covariate measure \u00b5 is supported on the ball of radius R). Thus in this case the law of robustness (under the assumptions of Theorem 4) directly states that with high probability, any neural network in our class that fits the training data well below the noise level must also have:\nLip(f ) \u2265\u03a9 nd Dp , (3.1)\nwhere\u03a9 hides logarithmic factors in W, p, R, Q, and the probability of error \u03b4. Thus we see that the law of robustness, namely that the number of parameters should be at least nd for a smooth model with low training error, remains intact for constant depth neural networks. If taken at face value, the lower bound (3.1) suggests that it is better in practice to distribute the parameters towards depth rather than width, since the lower bound is decreasing with D. On the other hand, we note that (3.1) can be strengthened to: . Moreover, in addition to being wellmotivated in practice, the assumption that B is polynomially controlled seems also somewhat unavoidable in theory, since B(w) is an upper bound on the Lipschitz constant Lip(fw). Thus a theoretical construction showing that the lower bound in (3.1) is tight (at some large depth D) would necessarily need to have an exponential gap between Lip(fw) and B(w). We are not aware of any such example, and it would be interesting to fully elucidate the role of depth in the law of robustness (particularly if it could give recommendation on how to best distribute parameters in a neural network).\nLip(f ) \u2265\u03a9 nd p log(B) , (3", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalization Perspective", "text": "The law of robustness can be phrased in a slightly stronger way, as a generalization bound for classes of Lipschitz functions based on data-dependent Rademacher complexity. In particular, this perspective applies to any Lipschitz loss function, whereas our analysis in the main text was specific to the squared loss. We define the data-dependent Rademacher complexity Radn,\u00b5(F) by\nRadn,\u00b5(F) = 1 n E \u03c3 i ,x i sup f \u2208F n i=1 \u03c3if (xi) (4.1)\nwhere the values (\u03c3i) i\u2208  The proof is identical to that of Theorem 2. Although we do not pursue it in detail, Lemma 2.2 easily extends to a sharpening of this result to general \u03c3i \u2208 [\u22121, 1] when E[\u03c3 2 i ] is small, even if \u03c3i and xi are not independent. We only require that the n pairs (\u03c3i, xi) i\u2208[n] are i.i.d. and that the distribution of \u03c3i given xi is symmetric. To see that the latter symmetry condition is natural, recall the quantity Radn,\u00b5 classically controls generalization due to the symmetrization trick, in which one writes \u03c3i = yi \u2212 y \u2032 i for y \u2032 i a resampled label for xi.\nNote that Radn,\u00b5(F) simply measures the ability of functions in F to correlate with random noise. Using standard machinery (see e.g. [MRT18, Chapter 3] for more on these concepts) we now deduce the following generalization bound: Corollary 4.2. For any loss function \u2113(t, y) which is bounded and 1-Lipschitz in its first argument and any \u03b4 \u2208 [0, 1], in the setting of Lemma 4.1 we have with probability at least 1 \u2212 \u03b4 the uniform convergence bound:\nsup f \u2208F E (x,y)\u223c\u00b5 [\u2113(f (x), y)] \u2212 1 n n i=1 \u2113(f (xi), yi) \u2264 O max k n , L c log(|F|) nd , log(1/\u03b4) n .\nProof. Using McDiarmid's concentration inequality it is enough to bound the left hand side in expectation over (xi, yi). Using the symmetrization trick (see e.g. [vH14, Chapter 7]), one reduces this task to upper bounding \nE x i ,y i ,\u03c3 i sup f \u2208F 1 n n i=1 \u03c3i\u2113(f (xi), yi) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Necessity of Polynomially Bounded Weights", "text": "In [BLN21] it was conjectured that the law of robustness should hold for the class of all two-layer neural networks. In this paper we prove that in fact it holds for arbitrary smoothly parametrized function classes, as long as the parameters are of size at most polynomial. In this section we demonstrate that this polynomial size restriction is necessary for bounded depth neural networks.\nFirst we note that some restriction on the size of the parameters is certainly necessary in the most general case. Indeed one can build a single-parameter family, where the single real parameter is used to approximately encode all Lipschitz functions from a compact set in R d to [\u22121, 1], simply by brute-force enumeration. In particular no tradeoff between number of parameters and attainable Lipschitz constant would exist for this function class. Showing a counter-example to the law of robustness with unbounded parameters and \"reasonable\" function classes is slightly harder. Here we build a three-layer neural network, with a single fixed nonlinearity \u03c3 : R \u2192 R, but the latter is rather complicated and we do not know how to describe it explicitly (it is based on the Kolmogorov-Arnold theorem). It would be interesting to give similar constructions using other function classes such as ReLU networks. Therefore with this choice of non-linearity \u03c3 and (data-independent) constants b \u2113 , some function \u03a6 \u2113 fits at least 3n 4 of the n data points with high probability, and the functions \u03a6a are parametrized in a 1-Lipschitz way by a single real number a \u2264 2 2 d .\nRemark A.1. The representation (A.1) is a three-layer neural network because the \u03c3(a \u2212 \u2113) terms are just matrix entries for the final layer.\nRemark A.2. The construction above can be made more efficient, using only O(n\u20222 n ) uniformly random functions g \u2113 : {\u22121, 1} d \u2192 {\u22121, 1} instead of all 2 2 \u2113 . Indeed by the coupon collector problem, this results in all functions from {\u03b3(xi) : i \u2208 [n]} \u2192 {\u22121, 1} being expressable as the restriction of some g \u2113 , with high probability.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement M.S. gratefully acknowledges support of NSF grant CCF-2006489, an NSF graduate research fellowship, and a Stanford graduate fellowship. We thank Gene Li, Omar Montasser, Kumar Kshitij Patel, Nati Srebro, and Lijia Zhou for suggesting that the improvement of Lemma 2.2 might be possible for small \u03c3, and an anonymous referee for pointing out a simpler proof. Thanks also to Franka Exner for pointing out some errors with numerical constants.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Unitary evolution recurrent neural networks", "journal": "PMLR", "year": "2016", "authors": "Martin Arjovsky; Amar Shah; Yoshua Bengio"}, {"ref_id": "b1", "title": "Network size and size of the weights in memorization with two-layers neural networks", "journal": "", "year": "2020", "authors": "Sebastien Bubeck; Ronen Eldan; Yin Tat Lee; Dan Mikulincer"}, {"ref_id": "b2", "title": "Spectrally-normalized margin bounds for neural networks", "journal": "Curran Associates, Inc", "year": "2017", "authors": "L Peter; Dylan J Bartlett; Matus J Foster;  Telgarsky"}, {"ref_id": "b3", "title": "Reconciling modern machinelearning practice and the classical bias-variance trade-off", "journal": "Proceedings of the National Academy", "year": "2019", "authors": "Mikhail Belkin; Daniel Hsu; Siyuan Ma; Soumik Mandal"}, {"ref_id": "b4", "title": "Poincar\u00e9's inequalities and talagrand's concentration phenomenon for the exponential distribution", "journal": "Probability Theory and Related Fields", "year": "1997", "authors": "Sergey Bobkov; Michel Ledoux"}, {"ref_id": "b5", "title": "From brunn-minkowski to brascamp-lieb and to logarithmic sobolev inequalities", "journal": "Geometric & Functional Analysis GAFA", "year": "2000", "authors": "G Sergey; Michel Bobkov;  Ledoux"}, {"ref_id": "b6", "title": "Benign overfitting in linear regression", "journal": "Proceedings of the National Academy of Sciences", "year": "2020", "authors": "L Peter; Philip M Bartlett; G\u00e1bor Long; Alexander Lugosi;  Tsigler"}, {"ref_id": "b7", "title": "A law of robustness for two-layers neural networks", "journal": "PMLR", "year": "2021", "authors": "S\u00e9bastien Bubeck; Yuanzhi Li; Dheeraj M Nagaraj"}, {"ref_id": "b8", "title": "Parseval Networks: Improving Robustness to Adversarial Examples", "journal": "PMLR", "year": "2017", "authors": "Piotr Cbg + 17] Moustapha Cisse; Edouard Bojanowski; Yann Grave; Nicolas Dauphin;  Usunier"}, {"ref_id": "b9", "title": "Dimension-free log-sobolev inequalities for mixture distributions", "journal": "Journal of Functional Analysis", "year": "2021", "authors": "Sinho Hong-Bin Chen; Jonathan Chewi;  Niles-Weed"}, {"ref_id": "b10", "title": "Estimating the intrinsic dimension of datasets by a minimal neighborhood information", "journal": "Scientific reports", "year": "2017", "authors": "Elena Facco; Maria Errico; Alex Rodriguez; Alessandro Laio"}, {"ref_id": "b11", "title": "Testing the manifold hypothesis", "journal": "Journal of the American Mathematical Society", "year": "2016", "authors": "Charles Fefferman; Sanjoy Mitter; Hariharan Narayanan"}, {"ref_id": "b12", "title": "Convergence of adversarial training in overparametrized neural networks", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": " Gcl + 19] Ruiqi; Tianle Gao; Haochuan Cai; Cho-Jui Li; Liwei Hsieh; Jason D Wang;  Lee"}, {"ref_id": "b13", "title": "", "journal": "", "year": "2018", "authors": " Gmf + 18] Justin; Luke Gilmer; Fartash Metz;  Faghri; S Samuel; Maithra Schoenholz; Martin Raghu; Ian Wattenberg;  Goodfellow"}, {"ref_id": "b14", "title": "Uncovering the limits of adversarial training against norm-bounded adversarial examples", "journal": "", "year": "2020", "authors": "Chongli Gqu + 20] Sven Gowal; Jonathan Qin; Timothy Uesato; Pushmeet Mann;  Kohli"}, {"ref_id": "b15", "title": "Isoperimetric Inequalities in Riemannian Manifolds", "journal": "Springer", "year": "1986", "authors": "Mikhael Gromov"}, {"ref_id": "b16", "title": "Principal curves", "journal": "Journal of the American Statistical Association", "year": "1989", "authors": "Trevor Hastie; Werner Stuetzle"}, {"ref_id": "b17", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b18", "title": "On computation and generalization of gans with spectrum control", "journal": "", "year": "2019", "authors": " Jcc + 19] Haoming; Zhehui Jiang; Minshuo Chen; Feng Chen; Dingding Liu; Tuo Wang;  Zhao"}, {"ref_id": "b19", "title": "Fast nonlinear dimension reduction", "journal": "IEEE", "year": "1993", "authors": "Nanda Kambhatla;  Todd K Leen"}, {"ref_id": "b20", "title": "Concentration of measure and logarithmic sobolev inequalities", "journal": "Springer", "year": "1999", "authors": "Michel Ledoux"}, {"ref_id": "b21", "title": "The concentration of measure phenomenon", "journal": "American Mathematical Society", "year": "2001", "authors": "M Ledoux"}, {"ref_id": "b22", "title": "Efficient orthogonal parametrisation of recurrent neural networks using householder reflections", "journal": "PMLR", "year": "2017", "authors": "Zakaria Mhammedi; Andrew Hellicar; Ashfaqur Rahman; James Bailey"}, {"ref_id": "b23", "title": "Spectral normalization for generative adversarial networks", "journal": "", "year": "2018", "authors": "Takeru Miyato; Toshiki Kataoka; Masanori Koyama; Yuichi Yoshida"}, {"ref_id": "b24", "title": "The generalization error of random features regression: Precise asymptotics and the double descent curve", "journal": "Communications on Pure and Applied Mathematics", "year": "2019", "authors": "Song Mei; Andrea Montanari"}, {"ref_id": "b25", "title": "Towards deep learning models resistant to adversarial attacks", "journal": "", "year": "2018", "authors": "Aleksandar Mms + 18] Aleksander Madry; Ludwig Makelov; Dimitris Schmidt; Adrian Tsipras;  Vladu"}, {"ref_id": "b26", "title": "Foundations of Machine Learning", "journal": "MIT press", "year": "2018", "authors": "Mehryar Mohri; Afshin Rostamizadeh; Ameet Talwalkar"}, {"ref_id": "b27", "title": "Sensitivity and generalization in neural networks: an empirical study", "journal": "", "year": "2018", "authors": "Yasaman Nba + 18] Roman Novak; Daniel A Bahri; Jeffrey Abolafia; Jascha Pennington;  Sohl-Dickstein"}, {"ref_id": "b28", "title": "Deep double descent: Where bigger models and more data hurt", "journal": "", "year": "2020", "authors": "Preetum Nakkiran; Gal Kaplun; Yamini Bansal; Tristan Yang; Boaz Barak; Ilya Sutskever"}, {"ref_id": "b29", "title": "Sample complexity of testing the manifold hypothesis", "journal": "", "year": "2010", "authors": "Hariharan Narayanan; Sanjoy Mitter"}, {"ref_id": "b30", "title": "The intrinsic dimension of images and its impact on learning", "journal": "", "year": "2021", "authors": "Chen Pope; Ahmed Zhu; Micah Abdelkader; Tom Goldblum;  Goldstein"}, {"ref_id": "b31", "title": "Nonlinear dimensionality reduction by locally linear embedding", "journal": "Science", "year": "2000", "authors": "T Sam; Lawrence K Roweis;  Saul"}, {"ref_id": "b32", "title": "Overfitting in adversarially robust deep learning", "journal": "PMLR", "year": "2020", "authors": "Leslie Rice; Eric Wong; Zico Kolter"}, {"ref_id": "b33", "title": "Understanding machine learning: From theory to algorithms", "journal": "Cambridge university press", "year": "2014", "authors": "Shai Shalev; - Shwartz; Shai Ben-David"}, {"ref_id": "b34", "title": "A global geometric framework for nonlinear dimensionality reduction", "journal": "Science", "year": "2000", "authors": "Vin De Joshua B Tenenbaum; John C Silva;  Langford"}, {"ref_id": "b35", "title": "High-dimensional probability: An introduction with applications in data science", "journal": "Cambridge university press", "year": "2018", "authors": "Roman Vershynin"}, {"ref_id": "b36", "title": "Probability in high dimension", "journal": "", "year": "2014", "authors": " Ramon Van Handel"}, {"ref_id": "b37", "title": "Intriguing properties of adversarial training at scale", "journal": "", "year": "2020", "authors": "Cihang Xie; Alan Yuille"}, {"ref_id": "b38", "title": "Rademacher complexity for adversarially robust generalization", "journal": "PMLR", "year": "2019", "authors": "Dong Yin; Ramchandran Kannan; Peter Bartlett"}, {"ref_id": "b39", "title": "Spectral norm regularization for improving the generalizability of deep learning", "journal": "", "year": "2017", "authors": "Yuichi Yoshida; Takeru Miyato"}, {"ref_id": "b40", "title": "Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity", "journal": "", "year": "2019", "authors": "Chulhee Yun; Suvrit Sra; Ali Jadbabaie"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Wj op, 1).Observe that B(w) is an upper bound on the Lipschitz constant of the network itself, i.e., the map x \u2192 fw(x). It turns out that a uniform control on it also controls the Lipschitz constant of the parametrization w \u2192 fw. Namely we have the following lemma:Lemma 3.1. Let x \u2208 R d such that x \u2264 R, and w1, w2 \u2208 R p such that B(w1), B(w2) \u2264 B. Then one has |fw 1 (x) \u2212 fw 2 (x)| \u2264 B 2 QR \u221a p w1 \u2212 w2 . Moreover for any w \u2208 [\u2212W, W ] p with W \u2265 1, one has B(w) \u2264 (W pQ) D . Proof. Fix an input x and define gx by gx(w) = fw(x). A standard gradient calculation for multi-layer neural networks directly shows that \u2207gx(w) \u221e \u2264 B(w)QR so that \u2207gx(w) \u2264 B(w)QR \u221a p. Since the matrix operator norm is convex (and nonnegative) it follows that B(w) \u2264 B(w1)B(w2) \u2264 B 2 on the entire segment [w1, w2] by multiplying over layers. Thus \u2207gx(w) \u2264 B 2 QR \u221a p on that segment, which concludes the proof of the first claimed inequality. The second claimed inequality follows directly from Wj op \u2264 Wj 2 \u2264 W \u221a pQ.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ".2) for the class of neural networks such that B(w) \u2264 B. In other words the dependence on the depth all but disappears by simply assuming that the quantity B(w) (a natural upper bound on the Lipschitz constant of the network) is polynomially controlled. Interestingly many works have suggested to keep B(w) under control, either for regularization purpose (for example [BFT17] relates B(w) to the Rademacher complexity of multi-layer neural networks) or to simply control gradient explosion during training, see e.g., [ASB16, CBG + 17, MHRB17, MKKY18, JCC + 19, YM17]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "[n] are i.i.d. symmetric Rademacher variables in {\u22121, 1} while the values (xi) i\u2208[n] are i.i.d. samples from \u00b5.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Lemma 4. 1 .1Suppose \u00b5 = k i=1 \u03b1i\u00b5i is a mixture of c-isoperimetric distributions. For finite F consisting of L-Lipschitz f with |f (x)| \u2264 1 for all (f, x) \u2208 F \u00d7 R d , we have Radn,\u00b5(F) \u2264 O max k", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fixingthe pairs (xi, yi) and using the contraction lemma (see e.g. [SSBD14, Theorem 26.9]) the above quantity is upper bounded by Radn,\u00b5(F) which concludes the proof. Of course, one can again use an \u01eb-net to obtain an analogous result for continuously parametrized function classes. The law of robustness, now for a general loss function, follows as a corollary (the argument is similar to [Proposition 1, [BELM20]]). Let us point out that many papers have studied the Rademacher complexity of function classes such as neural networks (see e.g. [BFT17], or [YKB19] in the context of adversarial examples). The new feature of our result is that isoperimetry of the covariates yields improved generalization guarantees.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Theorem 5 .5For each d \u2208 Z + there is a continuous function \u03c3 : R \u2192 R and a sequence (b \u2113 ) \u2113\u22642 2 d such that the following holds. The function \u03a6a defined by\u03a6a(x) = 2 2 d \u2113=1 \u03c3(a \u2212 \u2113) 2d i=1 \u03c3 b \u2113 + d j=1 \u03c3(xj + b \u2113 ) , |a| \u2264 2 2 d (A.1)is always O(d 3/2 )-Lipschitz, and the parametrization a \u2192 \u03a6a is 1-Lipschitz. Moreover for n \u2264 2 d 100 , given i.i.d. uniform points x1, . . . , xn \u2208 S d\u22121 and random labels y1, . . . , yn \u2208 {\u22121, 1}, with probability 1\u2212e \u2212\u2126(d) there exists \u2113 \u2208 [22 d ] such that \u03a6 \u2113 (xi) = yi for at least 3n 4 of the values i \u2208 [n]. Proof. For each coordinate i \u2208 [d], define the slab slabi = x \u2208 S d\u22121 : |xi| \u2264 1 100d 3/2 and set slab = i\u2208[d] slabi.Then it is not difficult to see that \u00b5(slab) \u2264 1 10 . We partition S d\u22121 \\slab into its 2 d connected components, which are characterized by their sign patterns in {\u22121, 1} d ; this defines a piece-wise constant function \u03b3 : S d\u22121 \\slab \u2192 {\u22121, 1} d . If we sample the points x1, . . . , xn sequentially, each point has probability at least 4 5 to be in a new cell -this implies that with probability 1 \u2212 e \u2212\u2126(n) , at least 3n 4 are in a unique cell. It therefore suffices to give a construction that achieves \u03a6(xi) = yi for all xi / \u2208 slab such that \u03b3(xi) = \u03b3(xj) for all j \u2208 [n]\\{i}. We do this now. For each of the 2 2 d functions g \u2113 : {\u22121, 1} d \u2192 {\u22121, 1}, we now obtain the partial functionh \u2113 = g \u2113 \u2022 \u03b3 : S d\u22121 \\slab \u2192 {\u22121, 1}. By the Kirszbraun extension theorem,h \u2113 extends to an O(d 3/2 )-Lipschitz function h \u2113 : S d\u22121 \u2192 [\u22121, 1] on the whole sphere. The Kolmogorov-Arnold theorem guarantees the existence of an exact representation \u03a6 \u2113 (x) = h \u2113 by a two-layer neural network for some continuous function \u03c3 \u2113 : R \u2192 R depending on \u2113. It suffices to give a single neural network capable of computing all functions (\u03a6 \u2113 ) 2 2 d\u2113=1 . We extend the definition of \u03a6a to any a \u2208 R via:\u03a6a(x) = 2 2 d \u2113=1 \u03c3(a \u2212 \u2113)\u03a6 \u2113 (x) (A.3)where \u03c3 : R \u2192 R satisfies \u03c3(x) = (1 \u2212 |x|)+ for |x| \u2264 2 2 d . This ensures that (A.3) extends (A.2). To express \u03a6a using only a single non-linearity, we prescribe further values for \u03c3. LetU = 2 2 d + d \u2022 max x\u2208[\u22121,1],\u2113\u2208[2 2 d ]|\u03c3 \u2113 (x)| so that d j=1 \u03c3 \u2113 (xj) \u2264 U for all x \u2208 S d\u22121 . Define real numbers b \u2113 = 10\u2113U + 2 2 d for \u2113 \u2208 [2 2 d ] and for all |x| \u2264 U set \u03c3(x + b \u2113 ) = \u03c3 \u2113 (x). Due to the separation of the values b \u2113 such a function \u03c3 certainly exists. Then we have \u03a6 \u2113 (x) = 2d i=1 \u03c3 b \u2113 + d j=1 \u03c3(xj + b \u2113 ) .", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03c3 2 \u2261 E \u00b5 [V ar[y|x]] > 0.", "formula_coordinates": [1.0, 118.8, 650.0, 92.34, 16.91]}, {"formula_id": "formula_1", "formula_text": "1 n n i=1 (f (xi) \u2212 yi) 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 Lip(f ) \u2265 \u2126 \u01eb \u03c3 nd p .", "formula_coordinates": [2.0, 193.08, 94.28, 227.08, 27.58]}, {"formula_id": "formula_2", "formula_text": "f (x) = n i=1 g(||x \u2212 xi||) \u2022 yi.", "formula_coordinates": [2.0, 251.52, 202.16, 108.88, 27.58]}, {"formula_id": "formula_3", "formula_text": "O d/d = O nd p .", "formula_coordinates": [2.0, 387.6, 320.84, 100.48, 12.7]}, {"formula_id": "formula_4", "formula_text": "P[|f (x) \u2212 E[f ]| \u2265 t] \u2264 2e \u2212 dt 2 2cL 2 .", "formula_coordinates": [5.0, 243.72, 159.7, 124.6, 21.33]}, {"formula_id": "formula_5", "formula_text": "\u00b5 = N 0, I d d", "formula_coordinates": [5.0, 96.96, 215.96, 52.04, 17.27]}, {"formula_id": "formula_6", "formula_text": "Xav = 1 \u221a n n i=1 Xi is 18C-subgaussian. Proof. By [vH14, Exercise 3.1 part d.], E e X 2 i /3C \u2264 2, i \u2208 [n].", "formula_coordinates": [5.0, 96.96, 440.6, 425.66, 67.19]}, {"formula_id": "formula_7", "formula_text": "E[e Y 2 /3C ] \u2264 E[e X 2 1 /3C ] \u2264 2.", "formula_coordinates": [5.0, 251.04, 543.58, 109.84, 19.16]}, {"formula_id": "formula_8", "formula_text": "N exp \u2212 nd L 2 = exp log(N ) \u2212 nd L 2 .", "formula_coordinates": [6.0, 227.16, 215.03, 157.6, 21.08]}, {"formula_id": "formula_9", "formula_text": "Theorem 2. Let (xi, yi) i\u2208[n] be i.i.d. input-output pairs in R d \u00d7 [\u22121, 1] such that:", "formula_coordinates": [6.0, 96.96, 330.68, 332.82, 16.91]}, {"formula_id": "formula_10", "formula_text": "\u03c3 2 \u2261 E \u00b5 [V ar[y|x]] > 0.", "formula_coordinates": [6.0, 422.52, 374.12, 92.21, 16.91]}, {"formula_id": "formula_11", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 4k exp \u2212 n\u01eb 2 8 3 k + 2 exp log(|F|) \u2212 \u01eb 2 nd 10 4 cL 2 .", "formula_coordinates": [6.0, 205.32, 406.76, 201.4, 53.15]}, {"formula_id": "formula_12", "formula_text": "g(x) = E[y|x], zi = yi \u2212 g(xi)", "formula_coordinates": [6.0, 271.68, 499.2, 68.62, 29.23]}, {"formula_id": "formula_13", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 2 exp \u2212 n\u01eb 2 8 3 + P \u2203f \u2208 F : 1 n n i=1 f (xi)zi \u2265 \u01eb 4 .", "formula_coordinates": [6.0, 112.08, 574.64, 387.88, 27.58]}, {"formula_id": "formula_14", "formula_text": "P 1 n n i=1 z 2 i \u2264 \u03c3 2 \u2212 \u01eb 6 \u2264 exp \u2212 n\u01eb 2 8 3 . (2.1)", "formula_coordinates": [6.0, 223.44, 632.6, 291.58, 27.58]}, {"formula_id": "formula_15", "formula_text": "P 1 n n i=1 zig(xi) \u2264 \u2212 \u01eb 6 \u2264 exp \u2212 n\u01eb 2 8 3 . (2.2) Let us write Z = 1 \u221a n (z1, . . . , zn), G = 1 \u221a n (g(x1), . . . , g(xn)", "formula_coordinates": [7.0, 96.96, 104.6, 418.06, 48.82]}, {"formula_id": "formula_16", "formula_text": "G + Z \u2212 F 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 F, Z \u2265 \u01eb 4 .", "formula_coordinates": [7.0, 230.88, 171.6, 154.71, 21.07]}, {"formula_id": "formula_17", "formula_text": "\u03c3 2 \u2212 \u01eb \u2265 G + Z \u2212 F 2 = Z + G \u2212 F 2 = Z 2 + 2 Z, G \u2212 F + G \u2212 F 2 \u2265 \u03c3 2 \u2212 \u01eb 2 \u2212 2 Z, F .", "formula_coordinates": [7.0, 110.64, 224.28, 390.76, 21.07]}, {"formula_id": "formula_18", "formula_text": "d c f (x i )\u2212E[f ] L is 2-subgaussian. Since |zi| \u2264 2, we also have that d c (f (x i )\u2212E[f ])z i L", "formula_coordinates": [7.0, 96.96, 320.12, 417.06, 34.79]}, {"formula_id": "formula_19", "formula_text": "P d cnL 2 n i=1 (f (xi) \u2212 E[f ])zi \u2265 t \u2264 2 exp \u2212(t/12) 2 .", "formula_coordinates": [7.0, 190.56, 371.96, 230.79, 27.58]}, {"formula_id": "formula_20", "formula_text": "P 1 n n i=1 (f (xi) \u2212 E[f ])zi \u2265 \u01eb 8 \u2264 2 exp \u2212 \u01eb 2 nd 10 4 cL 2 .", "formula_coordinates": [7.0, 197.16, 427.88, 217.72, 27.58]}, {"formula_id": "formula_21", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 E[f ]zi \u2265 \u01eb 8 \u2264 P 1 n n i=1 zi \u2265 \u01eb 8 .", "formula_coordinates": [7.0, 194.52, 482.84, 223.0, 27.58]}, {"formula_id": "formula_22", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 f (xi)zi \u2265 \u01eb 4 \u2264 |F| \u2022 P 1 n n i=1 (f (xi) \u2212 E[f ])zi \u2265 \u01eb 8 + P 1 n n i=1 zi \u2265 \u01eb 8 \u2264 2|F| \u2022 exp \u2212 \u01eb 2 nd 10 4 cL 2 + 2 exp \u2212 n\u01eb 2 8 3 .", "formula_coordinates": [7.0, 108.12, 557.48, 385.72, 53.15]}, {"formula_id": "formula_23", "formula_text": "d c f (x i )\u2212E \u00b5 \u2113 i [f ]", "formula_coordinates": [7.0, 107.28, 672.81, 53.34, 16.29]}, {"formula_id": "formula_24", "formula_text": "P 1 n n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2265 \u01eb 8 \u2264 2 exp \u2212 \u01eb 2 nd 9 4 cL 2 .", "formula_coordinates": [8.0, 193.68, 104.48, 224.68, 27.58]}, {"formula_id": "formula_25", "formula_text": "max m 1 ,...,m k \u2208[\u22121,1] n i=1 m \u2113 i zi = k \u2113=1 i\u2208S \u2113 zi ,", "formula_coordinates": [8.0, 226.32, 161.72, 159.4, 28.72]}, {"formula_id": "formula_26", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 E \u00b5 \u2113 i [f ]zi \u2265 \u01eb 8 \u2264 P \uf8eb \uf8ed 1 n k \u2113=1 i\u2208S \u2113 zi \u2265 \u01eb 8 \uf8f6 \uf8f8 . Now note that k \u2113=1 |S \u2113 | \u2264 \u221a", "formula_coordinates": [8.0, 96.96, 212.65, 335.68, 64.74]}, {"formula_id": "formula_27", "formula_text": "P \uf8eb \uf8ed 1 n k \u2113=1 i\u2208S \u2113 zi \u2265 \u01eb 8 \uf8f6 \uf8f8 \u2264 P \uf8eb \uf8ed k \u2113=1 i\u2208S \u2113 zi \u2265 \u01eb 8 n k k \u2113=1 |S \u2113 | \uf8f6 \uf8f8 \u2264 k \u2113=1 P \uf8eb \uf8ed i\u2208S \u2113 zi \u2265 \u01eb 8 n k |S \u2113 | \uf8f6 \uf8f8 .", "formula_coordinates": [8.0, 105.12, 274.69, 401.67, 37.43]}, {"formula_id": "formula_28", "formula_text": "[k], P i\u2208S \u2113 zi \u2265 t |S \u2113 | \u2264 2 exp \u2212 t 2 8", "formula_coordinates": [8.0, 317.76, 322.66, 169.47, 18.57]}, {"formula_id": "formula_29", "formula_text": "d \u2265 C1 \u2022 cL 2 \u03c3 2 \u01eb 2 . Then P \u2203f \u2208 F : 1 n n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2265 \u01eb 8 \u2264 exp \u2212 n\u03c3 4 8 + exp log |F| \u2212 \u01eb 2 nd C1cL 2 \u03c3 2 .", "formula_coordinates": [8.0, 125.4, 419.5, 361.24, 52.51]}, {"formula_id": "formula_30", "formula_text": "f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2264 sup f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u00d7 n i=1 z 2 i .", "formula_coordinates": [8.0, 165.72, 501.44, 282.04, 28.75]}, {"formula_id": "formula_31", "formula_text": "P n i=1 z 2 i \u2265 2\u03c3 2 n \u2264 exp \u2212 n\u03c3 4 8 . (2.7)", "formula_coordinates": [8.0, 234.0, 558.2, 281.02, 27.58]}, {"formula_id": "formula_32", "formula_text": "n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 for each fixed f . Since f (xi) \u2212 E \u00b5 \u2113 i [f ]", "formula_coordinates": [8.0, 240.24, 593.48, 177.5, 36.59]}, {"formula_id": "formula_33", "formula_text": "(f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u03c8 1 \u2264 O(cL 2 /d). Let Wi = (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2212 E (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2", "formula_coordinates": [8.0, 96.96, 651.08, 301.61, 47.63]}, {"formula_id": "formula_34", "formula_text": "0 \u2264 E (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2264 O(cL 2 /d).", "formula_coordinates": [9.0, 227.52, 84.08, 156.88, 17.39]}, {"formula_id": "formula_35", "formula_text": "Wi \u03c8 1 \u2264 O(cL 2 /d) Note that for d \u2265 2 8 cL 2 \u03c3 2 \u01eb 2", "formula_coordinates": [9.0, 96.96, 118.52, 249.46, 40.07]}, {"formula_id": "formula_36", "formula_text": "\uf8eb \uf8ec \uf8ed n\u01eb 2 2 8 \u03c3 2 2 n(cL 2 /d) 2 , n\u01eb 2 2 8 \u03c3 2 cL 2 /d \uf8f6 \uf8f7 \uf8f8 = min \u01eb 4 nd 2 2 16 (cL 2 ) 2 \u03c3 4 , \u01eb 2 nd 2 8 cL 2 \u03c3 2 = \u01eb 2 nd 2 8 cL 2 \u03c3 2 .", "formula_coordinates": [9.0, 178.44, 156.73, 272.08, 35.48]}, {"formula_id": "formula_37", "formula_text": "P n i=1 Wi \u2265 n\u01eb 2 2 8 \u03c3 2 \u2264 2 exp \u2212\u2126 \u01eb 2 nd cL 2 \u03c3 2 .", "formula_coordinates": [9.0, 213.6, 228.32, 184.84, 27.58]}, {"formula_id": "formula_38", "formula_text": "P sup f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2265 n\u01eb 2 2 7 \u03c3 2 \u2264 |F| \u2022 sup f \u2208F P n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2265 O cL 2 n d + n\u01eb 2 2 8 \u03c3 2 \u2264 2|F| exp \u2212\u2126 \u01eb 2 nd cL 2 \u03c3 2 .", "formula_coordinates": [9.0, 109.56, 283.28, 385.85, 53.15]}, {"formula_id": "formula_39", "formula_text": "f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ]) 2 \u2264 n\u01eb 2 2 7 \u03c3 2 , n i=1 z 2 i \u2264 2\u03c3 2 n hold, applying (2.6) yields sup f \u2208F n i=1 (f (xi) \u2212 E \u00b5 \u2113 i [f ])zi \u2264 n\u01eb 2 2 7 \u03c3 2 \u00d7 \u221a 2\u03c3 2 n \u2264 n\u01eb 8 .", "formula_coordinates": [9.0, 96.96, 365.84, 318.51, 113.83]}, {"formula_id": "formula_40", "formula_text": "d \u2265 C1 \u2022 cL 2 \u03c3 2 \u01eb 2 . Then P \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 (4k + 1) exp \u2212 n\u01eb 2 8 3 k + exp log |F| \u2212 \u01eb 2 nd C2cL 2 \u03c3 2 .", "formula_coordinates": [9.0, 112.08, 529.18, 387.88, 52.87]}, {"formula_id": "formula_41", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb \u2264 4k exp \u2212 n\u01eb 2 8 3 k +exp \u2212 n\u03c3 4 8 +exp log |F| \u2212 \u01eb 2 nd C2cL 2 \u03c3 2 .", "formula_coordinates": [9.0, 96.96, 609.44, 421.96, 27.58]}, {"formula_id": "formula_42", "formula_text": "\u03c3 2 \u2261 E \u00b5 [V ar[y|x]] > 0.", "formula_coordinates": [10.0, 422.52, 191.48, 92.22, 16.79]}, {"formula_id": "formula_43", "formula_text": "d \u2265 C1 cL 2 \u03c3 2 \u01eb 2 .", "formula_coordinates": [10.0, 280.2, 225.2, 73.48, 22.67]}, {"formula_id": "formula_44", "formula_text": "1 n n i=1 (f (xi) \u2212 yi) 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 Lip(f ) \u2265 \u01eb \u03c3 \u221a C2c \u00d7 nd p log(1 + 60W J\u01eb \u22121 ) + log(4/\u03b4)", "formula_coordinates": [10.0, 119.88, 288.56, 344.5, 27.58]}, {"formula_id": "formula_45", "formula_text": "1 n n i=1 (f (xi) \u2212 yi) 2 \u2264 \u03c3 2 \u2212 \u01eb \u21d2 Lip(f ) \u2265 \u01eb \u03c3 \u221a C2c nd s log p(1 + 60W J\u01eb \u22121 ) + log(4/\u03b4) . (2.13)", "formula_coordinates": [10.0, 118.32, 343.64, 396.58, 27.58]}, {"formula_id": "formula_46", "formula_text": "P \u2203f \u2208 FL,\u01eb : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb 2 \u2264 (4k + 1) exp \u2212 n\u01eb 2 9 4 k + exp p log(1 + 60W J\u01eb \u22121 ) \u2212 \u2126 \u01eb 2 nd cL 2 \u03c3 2 . Observe that if f \u2212 g \u221e \u2264 \u01eb 8 and y \u221e , f \u221e , g \u221e \u2264 1, then 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u01eb 2 + 1 n n i=1 (yi \u2212 g(xi)) 2 .", "formula_coordinates": [10.0, 96.96, 486.92, 351.4, 108.34]}, {"formula_id": "formula_47", "formula_text": "P \u2203f \u2208 F : 1 n n i=1 (yi \u2212 f (xi)) 2 \u2264 \u03c3 2 \u2212 \u01eb and Lip(f ) \u2264 L (2.14) \u2264 (4k + 1) exp \u2212 n\u01eb 2 10 4 k + exp p log(1 + 60W J\u01eb \u22121 ) \u2212 \u01eb 2 nd C1cL 2 \u03c3 2 .", "formula_coordinates": [10.0, 166.92, 633.08, 347.98, 53.15]}, {"formula_id": "formula_48", "formula_text": "L \u2264 \u01eb C2\u03c3 \u221a c nd p log(1 + 60W J\u01eb \u22121 ) + log(4/\u03b4)", "formula_coordinates": [11.0, 213.84, 108.59, 183.1, 21.75]}, {"formula_id": "formula_49", "formula_text": "(4k + 1) exp \u2212 n\u01eb 2 10 4 k \u2264 (4k + 1)\u03b4 8k \u2264 3\u03b4 4 .", "formula_coordinates": [11.0, 221.28, 159.2, 169.48, 22.67]}, {"formula_id": "formula_50", "formula_text": "exp p log(1 + 60W J\u01eb \u22121 ) \u2212 \u01eb 2 nd C2cL 2 \u03c3 2 \u2264 e \u2212 log(4/\u03b4) = \u03b4 4", "formula_coordinates": [11.0, 192.36, 208.04, 226.12, 22.67]}, {"formula_id": "formula_51", "formula_text": "S\u2286( [p] s ) WS,\u01eb is an \u01eb-net of W of size at most p(1 + 60W J\u01eb \u22121 )", "formula_coordinates": [11.0, 96.96, 296.39, 231.73, 45.32]}, {"formula_id": "formula_52", "formula_text": "Lip(f ) \u2265\u03a9 nd Dp , (3.1)", "formula_coordinates": [12.0, 260.52, 283.68, 254.5, 21.19]}, {"formula_id": "formula_53", "formula_text": "Lip(f ) \u2265\u03a9 nd p log(B) , (3", "formula_coordinates": [12.0, 250.44, 380.04, 253.29, 21.81]}, {"formula_id": "formula_54", "formula_text": "Radn,\u00b5(F) = 1 n E \u03c3 i ,x i sup f \u2208F n i=1 \u03c3if (xi) (4.1)", "formula_coordinates": [12.0, 222.96, 633.44, 292.06, 28.75]}, {"formula_id": "formula_55", "formula_text": "sup f \u2208F E (x,y)\u223c\u00b5 [\u2113(f (x), y)] \u2212 1 n n i=1 \u2113(f (xi), yi) \u2264 O max k n , L c log(|F|) nd , log(1/\u03b4) n .", "formula_coordinates": [13.0, 117.36, 287.96, 377.32, 28.75]}, {"formula_id": "formula_56", "formula_text": "E x i ,y i ,\u03c3 i sup f \u2208F 1 n n i=1 \u03c3i\u2113(f (xi), yi) .", "formula_coordinates": [13.0, 240.24, 354.44, 131.56, 28.75]}], "doi": ""}