{"title": "PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation *", "authors": "Hongyuan Lu; Wai Lam", "pub_date": "", "abstract": "Curriculum Data Augmentation (CDA) improves neural models by presenting synthetic data with increasing difficulties from easy to hard. However, traditional CDA simply treats the ratio of word perturbation as the difficulty measure and goes through the curriculums only once. This paper presents PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation, a novel CDA framework via paraphrasing, which exploits the textual paraphrase similarity as the curriculum difficulty measure. We propose a curriculum-aware paraphrase generation module composed of three units: a paraphrase candidate generator with bottom-k sampling, a filtering mechanism and a difficulty measure. We also propose a cyclic learning strategy that passes through the curriculums multiple times. The bottom-k sampling is proposed to generate super-hard instances for the later curriculums. Experimental results on few-shot text classification as well as dialogue generation indicate that PCC surpasses competitive baselines. Human evaluation and extensive case studies indicate that bottom-k sampling effectively generates super-hard instances, and PCC significantly improves the baseline dialogue agent.", "sections": [{"heading": "Introduction", "text": "Data augmentation techniques create artificial data mixed with the original data for improved performance. Traditional data augmentation techniques in the language community include word-level perturbation such as synonym replacement, random insertion, random swap, and random deletion (Wei and Zou, 2019). Sentence-level techniques such as Round-trip Translation (Sennrich et al., 2016b) exploits the use of machine translation models to translate the input sentence to another language before translating back to the source language which can be essentially treated as a form of paraphrasing.\nCurriculum learning presents training instances in a meaningful order with increasing difficulties to neural models for a boost in performance. Traditional curriculum learning (Bengio et al., 2009;Liu et al., 2018Liu et al., , 2020Platanios et al., 2019;Xu et al., 2020a,b;Su et al., 2021) categorizes the original training instances into different levels of difficulties to be gradually presented to the model where a core component called difficulty measure, which is usually defined as a numerical number where a bigger number indicates a more difficult sample.\nCombining the merits of the above two mentioned techniques, Curriculum Data Augmentation (CDA) creates synthetic data with increasing levels of difficulties to be presented to our neural models. Existing CDA defines the ratio of the words perturbation as the difficulty measure for curriculums and a gradual course which increases the difficulty of curriculums when the training loss plateaus (Wei et al., 2021), which then ends when the most challenging curriculum ends. Although existing CDA is effective, yet there are several disadvantages. First, it employs word-level perturbation. This superficial operation keeps the augmentation to have a similar sentence structure as the original one. Next, it employs random insertion, random swap, and random deletion for augmentation. Although this can be durable as for text classification (Wei et al., 2021), this is not suitable for generation tasks, particularly when many words are perturbed, which can even easily break the sentence grammar. Third, it uses a gradual course that only enters each level of difficulty once. A typical problem in neural network training called catastrophic forgetting (Kirkpatrick et al., 2017) can potentially happen in such a course, where the model might undesirably gradually forget some early learned knowledge.\nTo mitigate the problems of word-level perturba-tion, we propose that paraphrasing can be a source of data augmentation, which provides diverse and grammatically correct augmentation. However, it is non-trivial to utilize paraphrase augmentation in a curriculum setting. Inspired by the fundamental linguistic concept of mutual implication (Boghossian, 1994;Peregrin, 2006), we treat two sentences as a pair of paraphrases if they can infer each other. For example, 'I am glad to help you.' and 'Let me help you out!' can be a pair of paraphrases, which provides a diverse change of the sentence structure suitable for the curriculum setting. We also employ textual similarity for our difficulty measures for the curriculum. Higher scores indicate that two sentences are more textually similar to each other. Specifically, we treat pairs with lower scores as more difficult instances to be presented in later curriculums. We propose a paraphrase candidate generator integrated with bottom-k sampling. Traditional sampling methods such as top-k sampling (Fan et al., 2018) and top-p (Holtzman et al., 2020) sampling tend to generate easier paraphrases that have relatively high similarity scores. We propose bottom-k sampling to generate super-hard paraphrases for the later harder curriculums by pruning the most probable words. 1 This leads the generation towards a more grammatically and lexically diverse paraphrase sampling space with low textual similarity.\nTo mitigate catastrophic forgetting, we propose to incorporate cyclic learning to pass through the curriculums multiple times.\nIn summary, our proposed framework, called PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation, makes three contributions:\n\u2022 We exploit the use of paraphrasing with mutual implication as a data augmentation source in curriculum learning.\n\u2022 To generate mutual implicative paraphrases, we propose a curriculum-aware paraphrase generation module composed of three units, namely, a paraphrase candidate generator with bottom-k sampling for generating super-hard instances, a filtering mechanism, and a difficulty measure using textual similarity.\n\u2022 We propose cyclic learning to enter each curriculum multiple times.\nExperimental results indicate that PCC surpasses competitive baselines on few-shot text classification as well as dialogue generation. Human evaluation indicates that bottom-k sampling effectively generates grammatically and lexically rich paraphrases, and PCC significantly improves our baseline dialogue agent. To our best knowledge, this is the first time to apply CDA on a generation task.\nTakeaway Overall, we present the effectiveness of paraphrasing as a curriculum data augmentation technique. The use of cyclic learning and bottom-k sampling further boosts performance. With some modifications, future works can treat PCC as a data augmentation framework and adapt it to other downstream tasks. Future works can also leverage bottom-k sampling in generating textual outputs that are grammatically and lexically rich.\n2 Related Work", "publication_ref": ["b41", "b34", "b0", "b21", "b22", "b29", "b37", "b40", "b40", "b14", "b1", "b7", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Data Augmentation", "text": "Existing textual data augmentation techniques can be broadly categorized into two streams: wordlevel and sentence-level augmentation.\nFor word-level augmentation, well-known operations includes synonym replacement (Zhang et al., 2015a), random insertion, random deletion and random swap (Wei and Zou, 2019). In contrast to dictionary-based synonym replacement, another stream of works randomly replace words with masks and employs BERT models for predicting the words as a source of augmentation that exploits the contexts (Wu et al., 2019;Cai et al., 2020).\nFor sentence-level augmentation, Round-trip Translation (Sennrich et al., 2016b) augments translation pairs by translating from the source language into the target language, and back to the source language with two machine translation models. Gao et al. (2020) proposes to use paraphrases as a source of augmentation in task-oriented dialogue generation. It has also been proposed to retrieve from unpaired corpora as a source of augmentation in the dialogue community ). Another stream of work edits the retrieved dialogue response for better generation (Cai et al., 2019a,b), which can be treated as a form of indirect augmentation. The closest work to ours is Gao et al. (2020), where theirs does not employ curriculum learning.", "publication_ref": ["b52", "b41", "b46", "b4", "b34", "b11", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Curriculum Learning", "text": "While traditional curriculum learning sorts the training samples in an order of increasing difficulties (Bengio et al., 2009;Weinshall et al., 2018;Su et al., 2021), our method follows the other stream of works that applies transformation on the original data with dedicated difficulty level (Korbar et al., 2018;Ganesh and Corso, 2020;Wei et al., 2021). The closest work to ours is Wei et al. (2021). Their work does not consider paraphrasing and focuses on text classification only.\n3 Our Proposed Framework 3.1 Background of Curriculum Data Augmentation (CDA)\nExisting CDA (Wei et al., 2021) varies the wordlevel perturbation ratio to achieve different levels of difficulties under curriculum learning with simple word perturbation strategies such as synonym replacement, random insertion, swap, and deletion. As illustrated in Figure 1, such simple word perturbation strategies create problematic instances that break the sentence grammar, which can hamper the model performance. There are two common CDA strategies. One is called two-stage curriculum, which uses a fixed perturbation ratio for a single curriculum as the second stage after training with the original data. The other one is called gradual curriculum. It uses different ratios for a number of (typically 5) curriculums with increasing difficulties. However, such a learning strategy Filter the generated paraphrase candidates with the mutual implication and the textual similarity using Equation 3;\n8\nAssign a difficulty measure d to the filtered paraphrases with Equation 4;\n9\nCache the augmentation results intoD ;\n10 end ends after passing through all the curriculums only once, and catastrophic forgetting can happen.", "publication_ref": ["b0", "b44", "b37", "b15", "b9", "b40", "b40", "b40"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Our Proposed PCC", "text": "We propose curriculum data augmentation with paraphrase augmentation known as Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation (PCC). Algorithm 1 depicts an overview of the whole PCC framework. At the start of training, we generate cached training augmentation for the entire dataset with our proposed curriculum-aware paraphrase generation module. Thereafter, we begin with the easiest curriculum. For each training instance, we retrieve the cached augmentation that has an equivalent difficulty measure with the current difficulty level. We then invoke the task-specific model trainer to train the downstream task model with the retrieved training augmentation. At the end of each curriculum difficulty level, we increase the difficulty level to advance to the next harder curriculum. In case it hits the end of the most difficult curriculum, we set the difficulty level to the easiest to start a new cycle.\nWe propose such a cyclic learning strategy for mitigating potential catastrophic forgetting. In order to retrieve paraphrasing augmentation with appropriate difficulty measures, we propose a curriculumaware paraphrase generation module.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Curriculum-aware Paraphrase Generation Module", "text": "Algorithm 2 depicts the curriculum-aware paraphrase generation module. Three components are designed, namely, a paraphrase candidate generator integrated with a bottom-k sampling strategy, a filtering mechanism, and a difficulty measure. The paraphrase candidates are generated and then passed to the filtering mechanism. Finally, the filtered paraphrases are assigned a difficulty measure which represents to which curriculum difficulty level the augmentation belongs.\nParaphrase Candidate Generator with Bottomk Sampling In order to generate mutual implicative paraphrases for the purpose of curriculum data augmentation, we adopt a Seq2Seq (Sutskever et al., 2014) generator which receives an input sentence x and generates the paraphrasesx in an autoregressive manner (Nighojkar and Licato, 2021).\nDuring training, the paraphrase candidate generator is trained by maximising the following likelihood:\nP (x | x) = T t=1 P (x t |x 1 , ...,x t\u22121 , x),\nwhere T represents the token length of the paraphrase and x t represents the word at the position t that has been inferenced.\nTraditional sampling methods such as top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2020) sample the next token to be presented in the output from the most probable vocabularies that dominate the probability distribution. For example, at the i-th timestep during inference, top-k sampling samples the next tokenx i from the most probable k words with the distribution:\nPx i \u2208V (k) (x i |x 1 , ...,x i\u22121 , x),(1)\nwhere V (k) represents the most probable k words. However, they are not suitable for generating superhard instances, i.e., their output paraphrases tend to be textually similar to the original input sentence. 2 To avoid coping the words and unearth the superhard paraphrases to be used in later curriculums, we propose bottom-k sampling 3 which excludes a small set of dominating words for the sampling process. Note that we still use the combination of top-k and top-p sampling to generate easier samples for earlier curriculums. Formally, bottom-k modifies the distribution in Equation 1 to:\nPx i \u2208V\\V (k) (x i |x 1 , ...,x i\u22121 , x),(2)\nwhere V represents the whole vocabulary. Then, at each time step, we sample the next token with the rescaled distribution in Equation 2. We apply bottom-k for the first N steps of the generation before fallback to top-k and top-p. Bottom-k tends to generate paraphrases with lower textual similarity. For example, given an input of 'I like to remodel homes', existing sampling methods can generate an output 'Renovations in property I like to remodel homes'. In contrast, bottom-k sampling generates 'Is this what I want to see? Renovating homes are the best choices I have ever had.' where the latter one has a higher difficulty measure. Appendix F presents an extensive analysis.\nParaphrase Filtering The inferential properties or mutual implication (MI) has been argued as a form of equivalent meaning (Boghossian, 1994;Peregrin, 2006), i.e., each sentence should entail each other to be 'paraphrases'. To support curriculum data augmentation, we exploit mutual implicative paraphrases for grammatical and lexical richness. Algorithm 2 (Lines 5, 6, and 7) depicts the filtering mechanism we propose to generate MI paraphrases. In order to determine the MI relationship between a pair of paraphrase (x,x), we adopt a pre-trained MI classifier M(\u2022, \u2022) to calculate a binary indicator M(x,x). Here, non-MI paraphrases have a score of 0 and MI paraphrases have a score of 1. We also adopt a pre-trained model G(\u2022, \u2022) to evaluate the textual similarity score of the paraphrases as G(x,x). Here, paraphrases with lower similarity scores are treated as grammatically and lexically less similar to the original input sentence. We filter the paraphrasex i based on these two scores:\nM(x,x i )+(1\u2212M(x,x i ))1(G(x,x i ) \u2265 \u03b2). (3)\nIn the formula above, \u03b2 is a threshold for textual similarity. Here, a paraphrase with a positive mutual implication has a binary output of 1, i.e., it is preserved regardless of its textual similarity score.\nA paraphrase with a negative mutual implication but high textual similarity also has a binary output of 1, meaning it is preserved as well. In this way, MI paraphrases can be produced. We preserve highly similar paraphrases classified as non-MI, which is a misclassification by the classifier. 4 All paraphrases that are non-MI with low textual similarity have a binary output of 0, meaning we discard those paraphrases. After the filtering, a difficulty measure is computed for each paraphrase. 5\nDifficulty Measure Recall that for a pair of paraphrase (x,x), we adopt a pre-trained textual similarity model G(\u2022, \u2022) to calculate its similarity score as G(x,x). BLEURT (Sellam et al., 2020) score, a BERT-based pre-trained model, is employed as the textual similarity model G(\u2022, \u2022). Here, paraphrases with lower similarity scores are treated as more difficult instances with higher difficulty measures. For further illustration, we present 6 samples generated from our model in Table 1 with descending order sorted on the similarity scores. Here, the similarity scores decently represent the grammatical and lexical difference between the paraphrases candidates, and the mutual implicative paraphrase candidates are grammatically (Sample 2, 3, 4, 5, and 6) and lexically (Sample 1, 2, 3, 4, 5, and 6) rich.\nAs the distribution of the similarity scores for the paraphrases varies for different inputs, we compute the difficulty measure for a paraphrasex i with its rank in a sorted list of similarity scores, denoted as sort(\u2022), in descending order among a bag of paraphrase candidates X :\nd i = \u2308C \u00d7 sortx i \u2208X (G(x i , x)) |X | \u2309,(4)\nwhere C represents the total number of curriculum difficulty levels we define, and |X | represents the total number of paraphrase candidates we have.\nHere, the paraphrasex j with the highest similarity score, i.e., G(x,x j ) = maxx i \u2208X (G(x i , x)), has a rank of 1, therefore, d j = 1. The paraphrasex k with the lowest similarity score, i.e., G(x,x k ) = minx i \u2208X (G(x i , x)), has a rank of |X |, thus d k = C. Consequently, a larger rank indicates that the paraphrase is more grammatically and lexically different than the original input, and thus belongs to a harder curriculum. We set d i = 0 as the easiest difficulty level for the original data. ", "publication_ref": ["b38", "b26", "b7", "b13", "b1", "b32"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Cyclic Curriculum Data Augmentation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "In our experiments, we define six curriculums ranging from 0 to 5. 0 represents the original data, and 1 and 5 represent the easiest and the most difficult curriculum respectively. 6", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Few-shot Text Classification Task", "text": "For the downstream application task for our experiments, we follow Wei et al. (2021) to conduct the task of few-shot, highly multi-class text classification (Gupta et al., 2014;Kumar et al., 2019), which typically has a large number of classes with only a few samples for each of the class. We use triplet loss, a loss computed with three elements, namely, an anchor a, a positive sample p, and a negative sample n. It origins from the vision community (Schroff et al., 2015), which was later applied to language tasks (Ein Dor et al., 2018;Lauriola and Moschitti, 2020), suitable for the few-shot setting. Precisely, the learning objective is defined as:\nL = D(a, p) \u2212 D(a, n) + \u03b3,\nwhere D represents a distance measure that computes the distance between the input encodings. \u03b3 represents the margin between the positive and negative samples. We use BERT-based (Devlin et al., 2019) pooled sentence encodings as the input into a two-layer triplet network (Schroff et al., 2015).\nThree datasets for the text classification task are used in our experiments, namely, HUFFPOST (Misra, 2018;Misra and Grover, 2021), COVID-Q (Wei et al., 2020), and AMZN (Yury, 2020). For space reasons, we leave their detailed dataset description in Appendix B.", "publication_ref": ["b40", "b12", "b16", "b31", "b6", "b17", "b5", "b31", "b24", "b25", "b42", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Dialogue Generation Task", "text": "The second downstream task for our experiments is open-domain dialogue generation. We adopt a Seq2Seq neural network (Sutskever et al., 2014) which receives a text concatenation of prepended knowledge k and dialogue context c and generates the dialogue response r in an autoregressive manner (Radford, 2018). We train our dialogue generator by maximising the following likelihood: 6 We release the code and resource at https://github. com/HongyuanLuke/PCC.\nP (r | k, c) = T t=1 P (r t | r 1 , ..., r t\u22121 , k, c),\nwhere T represents the length of the generated dialogue response and r t represents the word at the position t that has been inferenced. Typical prepended knowledge include personal traits (Zhang et al., 2018) and movie description (Zhou et al., 2018). We use DialoGPT (Zhang et al., 2020b) for parameter initialization for PCC.\nWe use PERSONACHAT (CONVAI2, Zhang et al. 2018) as the dataset for dialogue generation, which is described in Appendix C.", "publication_ref": ["b38", "b30", "b51", "b55", "b54", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines for Text Classification", "text": "We use the following baselines from existing data augmentation methods for text classification.\nTriplet Loss As described in Section 4.1, an anchor, a positive example and a negative example is selected to construct the loss (Schroff et al., 2015).\nToken Substitution It substitutes words with their WordNet synonyms (Zhang et al., 2015b;Feinerer and Hornik, 2020).\nPervasive Dropout It uses dropout on words with probability p = 0.1 (Sennrich et al., 2016a).\nSwitchOut It replaces words with uniformly sampled words (Wang et al., 2018).", "publication_ref": ["b31", "b53", "b8", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Round-trip Translation", "text": "It translates sentences into another language before translating back into the source language (Sennrich et al., 2016b).\nHard Negative Mining + EDA It combines hard negative mining (Schroff et al., 2015) that chooses hard negative samples and EDA (Wei and Zou, 2019) that employs synonym replacement, wordlevel random insertion, deletion, and swap.\nHard Negative Mining + EDA + Gradual Curriculum It gradually increases the temperature for EDA augmentation (Wei et al., 2021).", "publication_ref": ["b34", "b31", "b41", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines for Dialogue Generation", "text": "We use the following baselines and data augmentation methods for dialogue generation.\nTransferTransfo A Transformer-based model fine-tuned on PERSONACHAT (Wolf et al., 2019).\nPerCVAE It uses a memory-augmented architecture with a conditional variational autoencoder to exploit persona information (Song et al., 2019).", "publication_ref": ["b45", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "DialoGPT It refers to an autoregressive dialogue generator introduced by Zhang et al. (2020b).", "text": "CDA It refers to the curriculum data augmentation technique proposed by Wei et al. (2021) using the augmentation of EDA (Wei and Zou, 2019).\nOfficial & Flatten It refers to the paraphrase augmentation technique that is task-specific to the taskoriented dialogue generation (Gao et al., 2020). To adapt it to our task, we use our generated paraphrase via mutual implication, denoted as Flatten, and the official revised PERSONACHAT paraphrases, denoted as Official.\nRound-trip Translation It translates the input into another language before translating back (Sennrich et al., 2016b).", "publication_ref": ["b40", "b41", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics", "text": "For the text classification task, we follow Wei et al. (2021) to use the top-1 accuracy as the metric.\nFor the dialogue generation task, we use the word-level F1 score, and we adopt the well-known sequence evaluation metric BLEU (Papineni et al., 2002) where we report BLEU-2, BLEU-3 and BLEU-4. We also adopt another well-known sequence evaluation metric, ROUGE, where we report the F-measures for ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004).\nTo verify our claim that bottom-k sampling generates grammatically and lexically rich paraphrases, we adopt Distinct-N (Li et al., 2016;Gao et al., 2019) with both N \u2208 {1, 2, 3} and N \u2208 {4, 5, 6} to measure the lexical and grammatical richness respectively using the ratio of distinct N -grams against the total number of N -grams generated.", "publication_ref": ["b40", "b27", "b20", "b18", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Few-shot Text Classification Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "Table 2 presents the results for few-shot text classification. Among the baselines, Triplet Loss + Gradual Curriculum works the best (Wei et al., 2021). PCC improves this baseline significantly. All the models share randomness in data, and our model is the best on all of the random seeds individually. Further, our proposed PCC model surpasses the baselines of Token Substitution, Pervasive Dropout, SwitchOut and Round-trip Translation significantly. Without bottom-k, PCC surpasses all the baselines, and our proposed full model with bottom-k obviously boosts performance. Appendix G additionally presents an analysis of the improvements as a function of the number of data augmentations. ", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Study", "text": "Table 4 presents the results of our ablation study. First, removing the MI paraphrase filtering component described with Equation 3 obviously degrades the results. Replacing bottom-k sampling with pure sampling also decreases the results. Furthermore, paraphrasing in a random or an inverse order of decreasing difficulties, i.e., with neither curriculum learning nor cyclic learning, obviously deteriorates the results. Therefore, our contribution is the discovery of paraphrasing as an effective CDA method rather than using paraphrasing solely as an augmentation technique. Moreover, using cyclic learning instead of the gradual curriculum improves the results when trained with and without bottom-k sampling. Training the second cycle in an inversed order of decreasing difficulties degrades the results both with and without bottom-k.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Analysis on Cyclic Learning", "text": "Figure 2 presents the change of the training loss during the progress of the training on the task of text classification on COVID-Q. We observe that catastrophic forgetting exists as the training loss spikes when re-entering the curriculums. For the second time it enters the most difficult curriculum 5, the loss is also further smoothened compared to the first spike. The spike is also desirable as described in Wei et al. (2021), indicating that new instances that are harder to learn are presented and can help to escape the local minima. These support the usefulness of our proposed cyclic learning that can smoothen the gradients, mitigate catastrophic forgetting, and improve generalization by entering curriculums multiple times.", "publication_ref": ["b40"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HUFFPOST COVID-Q AMZN Average", "text": "Triplet Loss (Schroff et al., 2015) 20.9 \u00b1 1.0 39.7 \u00b1 1.0 11.6 \u00b1 0.6 24.1 Triplet Loss + Token Substitution (Zhang et al., 2015b) 22.7 \u00b1 1.4 43.9 \u00b1 1.3 12.8 \u00b1 0.7 26.5 Triplet Loss + Pervasive Dropout (Sennrich et al., 2016a) 23.1 \u00b1 1.1 43.5 \u00b1 1.8 13.0 \u00b1 0.6 26.5 Triplet Loss + SwitchOut (Wang et al., 2018) 22.9 \u00b1 0.5 41.5 \u00b1 0.6 12.7 \u00b1 0.8 25.7 Triplet Loss + Round-trip Translation (Sennrich et al., 2016b) 24.2 \u00b1 0.7 42.3 \u00b1 1.0 13.0 \u00b1 0.4 26.5 Triplet Loss + Hard Negative + EDA (Wei and Zou, 2019) 22.6 \u00b1 1.8 48.2 \u00b1 0.9 13.7 \u00b1 0.9 28.2 \u2192 + Gradual Curriculum (Wei et al., 2021) 23.8 \u00b1 0.9 48.9 \u00b1 0.9 14.4 \u00b1 1.5 29.0 PCC with Cyclic Curr. w/o Bottom-k 25.2 \u00b1 1.5 51.4 \u00b1 0.8 17.4 \u00b1 0.7 31.3 PCC with Cyclic Curr. w/ Bottom-k 25.9 \u00b1 1.7 51.7 \u00b1 0.6 18.2 \u00b1 1.0 31.9\nTable 2: Results in top-1 accuracy for the downstream task of text classification on three datasets. The best results are bolded. We report the results averaged from five random seeds for data selection ranging from 0 to 4, which is the source of the variance here. Our methods report the best performance on all the random data seeds on all the datasets. A combination of top-k and top-p sampling with k = 120 and p = 0.95 is used for the penultimate row.\nModel F1 BLEU-2 BLEU-3 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L\nTransferTransfo (Wolf et al., 2019) 16.61 \u00b1 0.09 3.16 \u00b1 0.07 1.04 \u00b1 0.03 0.43 \u00b1 0.02 17.69 \u00b1 0.14 3.96 \u00b1 0.08 16.34 \u00b1 0.13 PerCVAE (Song et al., 2019) 14.33 \u00b1 0.12 1.23 \u00b1 0.06 0.20 \u00b1 0.05 0.04 \u00b1 0.01 13.25 \u00b1 0.10 1.62 \u00b1 0.05 12.02 \u00b1 0.10 DialoGPT (Zhang et al., 2020b) 18.58 \u00b1 0.13 5.25 \u00b1 0.08 1.89 \u00b1 0.07 0.66 \u00b1 0.05 18.42 \u00b1 0.13 4.62 \u00b1 0.09 17.23 \u00b1 0.12 DialoGPT + CDA (Wei and Zou, 2019) 18.38 \u00b1 0.10 5.23 \u00b1 0.10 1.84 \u00b1 0.08 0.63 \u00b1 0.02 18.55 \u00b1 0.31 4.63 \u00b1 0.11 17.40 \u00b1 0.30 DialoGPT + Flatten (Gao et al., 2020) 18.21 \u00b1 0.21 5.03 \u00b1 0.18 1.85 \u00b1 0.11 0.65 \u00b1 0.04 17.97 \u00b1 0.34 4.45 \u00b1 0.16 16.84 \u00b1 0.28 DialoGPT + Official (Gao et al., 2020) 18.12 \u00b1 0.11 4.80 \u00b1 0.27 1.78 \u00b1 0.50 0.59 \u00b1 0.60 17.88 \u00b1 0.24 4.38 \u00b1 0.09 16.84 \u00b1 0.20 DialoGPT + RT (Sennrich et al., 2016b) 18.26 \u00b1 0.49 5.10 \u00b1 0.21 1.80 \u00b1 0.20 0.62 \u00b1 0.08 18.32 \u00b1 0.35 4.47 \u00b1 0.18 17.16 \u00b1 0.31 PCC with Cyclic Curr. w/o Bottom-k 18.76 \u00b1 0.20 5.38 \u00b1 0.14 1.99 \u00b1 0.9 0.71 \u00b1 0.06 18.81 \u00b1 0.18 4.75 \u00b1 0.12 17.53 \u00b1 0.12 PCC with Cyclic Curr. w/ Bottom-k 18.80 \u00b1 0.45 5.59 \u00b1 0.17 2.07 \u00b1 0.12 0.76 \u00b1 0.11 19.15 \u00b1 0.16 4.98 \u00b1 0.12 17.89 \u00b1 0.17 Table 3: Results for the downstream task of open-domain dialogue generation on PERSONACHAT, averaged from three runs. All the metrics attain better quality with higher scores. We denote Round-trip Translation as RT. A combination of top-k and top-p sampling with k = 120 and p = 0.95 is used for the penultimate row.\nModel HUFFPOST COVID-Q AMZN PCC w/o MI filtering 25.7 \u00b1 1.4 50.2 \u00b1 1.7 16.7 \u00b1 1.1 PCC w/ Pure Sampling 25.8 \u00b1 1.0 49.7 \u00b1 0.9 16.9 \u00b1 0.8 PCC w/ Inverse Curriculum 23.0 \u00b1 1.7 48.5 \u00b1 1.2 15.0 \u00b1 0.5 PCC w/ Random Curriculum 24.0 \u00b1 1.7 48.9 \u00b1 1.5 15.1 \u00b1 0.8 PCC w/ Gradual Curriculum 24.7 \u00b1 1.3 49.6 \u00b1 1.4 16.5 \u00b1 0.7 PCC w/ Inv. Cyc.\n24.9 \u00b1 1.2 50.9 \u00b1 1.0 16.5 \u00b1 0.8 PCC w/ Cyc.\n25.2 \u00b1 1.5 51.4 \u00b1 0.8 17.4 \u00b1 0.7 PCC w/ Inv. Cyc., Bottom-k 25.3 \u00b1 1.9 51.3 \u00b1 1.1 17.1 \u00b1 1.2 PCC w/ Cyc., Bottom-k 25.9 \u00b1 1.7 51.7 \u00b1 0.6 18.2 \u00b1 1.0 ", "publication_ref": ["b31", "b53", "b33", "b34", "b41", "b40", "b45", "b36", "b54", "b41", "b11", "b11", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Dialogue Generation Results", "text": "Table 3 presents the results for dialogue generation on PERSONACHAT. First, we present the results for competitive baselines, namely TransferTransfo and PerCVAE. DialoGPT surpasses these two significantly. Using CDA on DialoGPT has deteriorated BLEU scores, which suggests that using CDA causes grammatical influence, possibly due to the random operations that produce undesirable grammatically incorrect augmentation. We also observe a large variance with the official paraphrase provided by PERSONACHAT, possibly due to the large difference between the manually rephrased sentences. This indicates easier paraphrases seem to be essential for PCC to be effective. Also, the Flatten baseline reported in Table 3 approximates a random curriculum, which degrades the results. It leads to a conclusion about the usefulness of the suggested curriculum. Round-trip Translation (RT) seems not effective, which is somehow reasonable as RT was originally designed for machine translation. PCC achieves the best among all the models, suggesting its usefulness for dialogue generation. Appendix D provides in-depth reasonings on the results. Appendix H presents a human evaluation of the downstream task of dialogue generation.    lower grams (N \u2208 {1, 2, 3}), indicating its lexical richness. It also attains the best on Distinct scores with higher grams (N \u2208 {4, 5, 6}), indicating its grammatical richness. This helps to generate superhard instances. Note that the setting of bottom-k sampling employed in PCC with k = 2 and N = 1 already gives the best overall diversity against previous sampling methods. Further increasing the value of k and N leads to higher diversity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis on Bottom-k Sampling", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Human Evaluation on Bottom-k Sampling", "text": "We hired three experienced annotators who have degrees relevant to English Linguistics to conduct an evaluation on bottom-k sampling with PER-SONACHAT. We present a questionnaire composed of 800 questions with 200 randomly sampled training instances with the paraphrases generated with and without bottom-k sampling to the annotators to compare model outputs under A/B testing:\n\u2022 (Grammatical Richness): \"Which paraphrase do you think is more grammatically different than the original input sentence?\"\n\u2022 (Lexical Richness): \"Which paraphrase do you think is more lexically different than the original input sentence?\"\n\u2022 (Difficulty): \"Which paraphrase is more difficult to read and understood?\"\n\u2022 (Paraphrasing): \"Which one is more like a mutual implicative paraphrase to the input?\"\nTable 6 presents the results of our human evaluation. The paraphrases generated by PCC with bottom-k sampling have a significant advantage in lexical and grammatical richness. Such an advantage correlates well with the difficulty of the paraphrases to be understood by human annotators. Furthermore, bottom-k does not hurt the paraphrasing performance compared to the top-k and top-p sampling. The result of human evaluation verifies our claim that bottom-k generates super-hard paraphrases with grammatical and lexical richness. Appendix F presents how bottom-k sampling is superior over previous methods in our scenario with case studies about the coping mechanism.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Conclusions", "text": "We propose a novel framework that uses mutual implicative paraphrasing as a curriculum data augmentation technique. Our proposed curriculumaware paraphrase generation module is composed of three components, a paraphrase candidate generator with a bottom-k sampling strategy for generating superhard paraphrases, a paraphrase filtering mechanism, and a difficulty measure. We propose a bottom-k sampling strategy to effectively generate super-hard instances with grammatical and lexical richness to be used for the later stages in curriculum learning. Moreover, we propose a cyclic learning strategy that mitigates catastrophic forgetting. Experimental results on the task of few-shot text classification as well as dialogue generation support our proposed methodology PCC's usefulness, surpassing several competitive baselines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The proposed PCC cost more computational resources than traditional CDA methods. However, the cost is still affordable. Generating a round-trip augmentation used as one of the baselines costs about 1.5 seconds (1x speed) for PERSONACHAT. In contrast, generating a single paraphrase costs about 0.40 seconds (3x faster) with PCC on our machine with a single GPU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Statement", "text": "We honour and support the EACL Code of Ethics. The datasets used in this work are well-known and widely used, and the dataset pre-processing does not make use of any external textual resource. In our view, there is no known ethical issue. End-toend pre-trained dialogue generators are also used, which are subjected to generating offensive context. But the above-mentioned issues are widely known to commonly exist for these models. Any content generated do not reflect the view of the authors. a two-stage curriculum for those baselines. Following Wei et al. (2021), we include 20% original data whenever augmentation is used.\nFor dialogue generation, we use DIALOGPT-SMALL for parameter initialisation. We use a batch size of 4 and a gradient clip of 0.1. We use validation patience of 10 based on the validation loss. We use greedy decoding for all of our experiments. The above settings apply to all our baselines and our proposed model fine-tuned on DIALOGPT. We start to apply the augmentation after 130,000 steps for data augmentation methods. We train the first, second, third, fourth, and fifth curriculums with 60,000 steps. For Official, Flatten, and RT, we perform a two-stage curriculum as described by Wei et al. (2021). We set N and k as a small value (typically N = 1 and k = 2) for bottom-k sampling. We perform a cyclic repetition for our proposed method for the same number of steps for each curriculum until early stopped.\nDuring our experiments, we apply data augmentation methods on the entire textual input for text classification, and we apply data augmentation methods on the personas traits for persona-based dialogue generation. We employ an off-the-shelf pre-trained model for both the paraphrase generator and the MI classifier (Nighojkar and Licato, 2021).\nFor all of the datasets, we obtain 20 paraphrases after filtering, and we assign 4 paraphrases (Wei et al., 2021) to each of the curriculums we have. We use 2 paraphrases obtained with bottom-k sampling for COVID-Q and we use 4 paraphrases obtained with bottom-k sampling for the remaining datasets.\nFor our models without bottom-k sampling, we use 20 paraphrases generated with a combination of top-k sampling and top-p sampling with k = 120 and p = 0.95 for all of the datasets.\nWe conduct our experiments for dialogue generation on the PARLAI platform (Miller et al., 2017).", "publication_ref": ["b40", "b40", "b26", "b40", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "B Datasets for Text Classification", "text": "\u2022 The HUFFPOST dataset is composed of 200k news headlines collected from 2012 to 2018, which is categorized into 41 classes such as politics, entertainment, and travel (Misra, 2018;Misra and Grover, 2021). We use all the classes and a 70% / 30% train / test split by class (Wei et al., 2021).\n\u2022 The COVID-Q dataset is composed of 87 classes with several questions per cluster which ask about the same thing (Wei et al., 2020). We use the official train / test split with 3 questions per cluster (Wei et al., 2021).\n\u2022 The AMZN product review dataset (Yury, 2020) categorizes products into given reviews. We consider the use of 318 'level-3' classes with at least 6 samples per product.\nFor the few-shot scenario, we need to set the number of samples in each class, N c , to be used to construct the datasets. We use the setting in Wei et al. (2021) where N c = 3 for COVID-Q and N c = 10 for HUFFPOST. We set N c = 2 for AMZN. ", "publication_ref": ["b24", "b25", "b40", "b42", "b40", "b49", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "D Analysis on Dialogue Generation", "text": "Table 3 reports an ablation when we use our PCC to train the dialogue generator without the use of bottom-k sampling. The results suggest that using bottom-k sampling improves all the metrics, especially the ROUGE scores. Table 8 presents the distribution of the textual similarity scores for the paraphrases generated from four methods on PER-SONACHAT. The official paraphrase (Zhang et al., 2018) largely differs from the original ones, which we postulate as the reason for the large variance observed in Table 3. This also indicates the neccessity of the easier samples for curriculum learning. The Round-trip Translation generates paraphrases that have higher textual similarity with the input sentence. Our method without bottom-k sampling (we use a combination of top-k and top-p sampling with k = 120 and p = 0.95 here) generates paraphrases with more evenly distributed scores, with an average of 0.02. In contrast, bottom-k helps to generate harder samples while still capable of generating more easier samples.\nE Problematic Cases for EDA   (Wei and Zou, 2019). We present recommended temperatures \u03c4 ranging from 0.1 to 0.5, with four samples for each \u03c4 .", "publication_ref": ["b51", "b41"], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Model", "text": "[0.5, [0, 0.5) (\u22120.5, 0) , \u22120.5] Avg.  temperatures \u03c4 ranging from 0.1 to 0.5, which is the recommended setting from Wei et al. (2021). We categorize EDA's problems as the followings:\n\u2022 Sample i) with \u03c4 = 0.1 and sample ii) with \u03c4 = 0.2 changes the meaning of the input sentence. 'equal' is possibly produced by random insertion and 'gladiola' is possibly produced by synonym replacement via WordNet (Feinerer and Hornik, 2020).\n\u2022 Most of the samples produced with \u03c4 = 0.4 and \u03c4 = 0.5 breaks the grammar, which can be harmful to generation tasks.\n\u2022 Sample ii) and iv) with \u03c4 = 0.5 introduces rare words such as 'avail' and 'gladiolus', which is counterintuitive to see in many tasks.\nAs illustrated in Figure 1, PCC effectively reduces the above-mentioned issues.", "publication_ref": ["b40", "b8"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "F Analysis on Bottom-k Sampling", "text": "Table 9 presents extensive case studies to support that bottom-k sampling generates grammatically rich and lexically rich paraphrases. PCC without bottom-k tends to exploit a coping mechanism at the beginning of generation (Sample 2,3,5,6,7,8,9,10,11,12). By excluding these dominating words to be copied for generation, bottom-k effectively emphasises the content (Sample 5), improves grammatical richness (Sample 1,2,3,4,5,6,7,10,12) and lexical richness (Sample 3,4,6,8,10,12),   does appropriate synonym replacement (Sample 8, 11) and insertion (Sample 4). Without bottom-k sampling, the input that starts with a first-person pronoun 'i' is highly likely to have an output that starts with 'i' (Sample 2,3,6,8,10). This constrains the model from generating grammatically rich paraphrases. In contrast, bottom-k sampling effectively reduces such cases and biases the generation towards a grammatically rich sampling space. Indeed, out of the 6,126 persona traits from PER-SONACHAT, 5,087 of them start with 'i'. PCC without bottom-k generates 2,558 paraphrases that start with 'i', which avoids generating super-hard instances and hampers the PCC performance. 7 In contrast, bottom-k generates 205 paraphrases that start with 'i', indicating its usefulness in improving grammatical richness and generating super-hard instances. Avoiding coping helps to unearth the diverse paraphrases hidden in the tail vocabularies,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Implementation Details", "text": "For text classification, we use the hyper-parameter settings as in Wei et al. (2021) for the gradual course, and we refer to their paper for the detailed settings. For our cyclic learning, we pass through the curriculums twice. We train the same number of steps for each curriculum as we did in the first pass for our second pass, and the remaining hyper-parameters are kept the same. For Token Substituion, Pervasive Dropout, SwitchOut, and Round-trip Translation, we follow Wei et al. (2021) to use the triplet network as the base model and use Original Input Sentence PCC w/o Bottom-k Sampling PCC w/ Bottom-k Sampling 1): i like to shoot a bow. When i first started shooting bows, this was the most important method. Hey, i like to shoot a bow. Just started using a Bow SLR shooter, but a DSLR isn't really necessary.\n2): i have four sisters. i have four sisters four sisters, and i want four sisters.\n3): i believe that mermaids are real. i believe that mermaids are real \" @JesseyHawkins Marxist philosopher,'mermaids are real,\" property 4): i work as a stand up comedian. jesse t trained comedian, I work as a stand up comedian.  which we postulate as the reason for the results observed in human evaluation in Section 5.5.\nNote that we use bottom-k sampling to effectively prevent coping to generate instances that are textually more different to the input. There is a stream of work that considers improving the diversity (Vijayakumar et al., 2016). However, these works do not directly consider the similarity between the input paraphrase and the output paraphrase. This is the advantage of bottom-k sampling over this stream of work for our scenario.", "publication_ref": ["b40", "b40", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "G Analysis on Data Augmentation", "text": "Figure 3 presents the percentage improvements in accuracy as a function of the number of data augmentation instances available for each curriculum.\nHere, since we have 5 curriculum difficulty levels in our setting, having 3 instances available for each curriculum means that we have 15 data augmentations in total for each original sample. The improvements are positively correlated with the number of available instances. Furthermore, it seems that the improvements of PCC are not saturated yet. This means that a further increase in the number of data augmentations can lead to even higher performance than reported in our paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H More Human Evaluation", "text": "\u2022 (Appropriateness): \"Who is more appropriate given the previous dialogue context?\"\n\u2022 (Informativeness): \"Who is more diverse instead of null answers such as I do not know?\"  \u2022 (Engagingness): \"Who would you prefer to talk with for a long conversation?\"\n\u2022 (Human-likeness): \"Which speaker do you think sounds more like a real person?\"\nWe follow  and Zou et al. (2021) to conduct a human evaluation of dialogue generation from the four aspects described above. We follow the settings used in Section 5.5 to invite three experienced annotators to mark 200 instances under A/B settings. The results in Table 10 indicate that PCC effectively improves the DIALOGPT baseline in all aspects, especially informativeness.", "publication_ref": ["b56"], "figure_ref": [], "table_ref": []}, {"heading": "I Computing Infrastructure", "text": "We use an NVIDIA TITAN RTX with 24GB GPU memory for all of the experiments conducted in this paper. Training the text classification model consumes about 1 hour. Fine-tuning the dialogue generator consumes about 15 hours. Generating a single paraphrase to be used in PCC as a CDA method costs about 0.40 seconds on our machine.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Curriculum learning", "journal": "", "year": "2009", "authors": "Yoshua Bengio; J\u00e9r\u00f4me Louradour; Ronan Collobert; Jason Weston"}, {"ref_id": "b1", "title": "Inferential role semantics and the analytic/synthetic distinction", "journal": "Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition", "year": "1994", "authors": "Paul A Boghossian"}, {"ref_id": "b2", "title": "Skeletonto-response: Dialogue generation guided by retrieval memory", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Deng Cai; Yan Wang; Wei Bi; Zhaopeng Tu; Xiaojiang Liu; Wai Lam; Shuming Shi"}, {"ref_id": "b3", "title": "Retrievalguided dialogue response generation via a matchingto-generation framework", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Deng Cai; Yan Wang; Wei Bi; Zhaopeng Tu; Xiaojiang Liu; Shuming Shi"}, {"ref_id": "b4", "title": "Data manipulation: Towards effective instance learning for neural dialogue generation via learning to augment and reweight", "journal": "", "year": "2020", "authors": "Hengyi Cai; Hongshen Chen; Yonghao Song; Cheng Zhang; Xiaofang Zhao; Dawei Yin"}, {"ref_id": "b5", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b6", "title": "Learning thematic similarity metric from article sections using triplet networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yosi Liat Ein Dor; Alon Mass; Elad Halfon; Ilya Venezian; Ranit Shnayderman; Noam Aharonov;  Slonim"}, {"ref_id": "b7", "title": "Hierarchical neural story generation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"ref_id": "b8", "title": "wordnet: Word-Net Interface. R package version 0", "journal": "", "year": "2020", "authors": "Ingo Feinerer; Kurt Hornik"}, {"ref_id": "b9", "title": "Rethinking curriculum learning with incremental labels and adaptive compensation", "journal": "", "year": "2020", "authors": "Madan Ganesh; Jason Corso"}, {"ref_id": "b10", "title": "Generating multiple diverse responses for short-text conversation", "journal": "", "year": "2019", "authors": "Jun Gao; Wei Bi; Xiaojiang Liu; Junhui Li; Shuming Shi"}, {"ref_id": "b11", "title": "Paraphrase augmented task-oriented dialog generation", "journal": "", "year": "2020", "authors": "Silin Gao; Yichi Zhang; Zhijian Ou; Zhou Yu"}, {"ref_id": "b12", "title": "Training highly multiclass classifiers", "journal": "J. Mach. Learn. Res", "year": "2014", "authors": "Maya R Gupta; Samy Bengio; Jason Weston"}, {"ref_id": "b13", "title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"ref_id": "b14", "title": "Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell", "journal": "Proceedings of the National Academy of Sciences", "year": "2017", "authors": "James Kirkpatrick; Razvan Pascanu; Neil Rabinowitz; Joel Veness; Guillaume Desjardins; Andrei A Rusu; Kieran Milan; John Quan; Tiago Ramalho"}, {"ref_id": "b15", "title": "Cooperative learning of audio and video models from self-supervised synchronization", "journal": "Curran Associates", "year": "2018", "authors": "Bruno Korbar; Du Tran; Lorenzo Torresani"}, {"ref_id": "b16", "title": "Improving answer selection and answer triggering using hard negatives", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Sawan Kumar; Shweta Garg; Kartik Mehta; Nikhil Rasiwasia"}, {"ref_id": "b17", "title": "Context-based transformer models for answer sentence selection", "journal": "", "year": "2020", "authors": "Ivano Lauriola; Alessandro Moschitti"}, {"ref_id": "b18", "title": "A diversity-promoting objective function for neural conversation models", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"ref_id": "b19", "title": "ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons. CoRR, abs", "journal": "", "year": "1909", "authors": "Margaret Li; Jason Weston; Stephen Roller"}, {"ref_id": "b20", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b21", "title": "Curriculum learning for natural answer generation", "journal": "", "year": "2018", "authors": "Cao Liu; Shizhu He; Kang Liu; Jun Zhao"}, {"ref_id": "b22", "title": "Task-level curriculum learning for non-autoregressive neural machine translation", "journal": "", "year": "2020", "authors": "Jinglin Liu; Yi Ren; Xu Tan; Chen Zhang; Tao Qin; Zhou Zhao; Tie-Yan Liu"}, {"ref_id": "b23", "title": "ParlAI: A Dialog Research Software Platform. CoRR", "journal": "", "year": "2017", "authors": "Alexander H Miller; Will Feng; Adam Fisch; Jiasen Lu; Dhruv Batra; Antoine Bordes; Devi Parikh; Jason Weston"}, {"ref_id": "b24", "title": "News category dataset", "journal": "", "year": "2018", "authors": "Rishabh Misra"}, {"ref_id": "b25", "title": "Sculpting Data for ML: The first act of Machine Learning", "journal": "", "year": "2021", "authors": "Rishabh Misra; Jigyasa Grover"}, {"ref_id": "b26", "title": "Improving paraphrase detection with the adversarial paraphrasing task", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Animesh Nighojkar; John Licato"}, {"ref_id": "b27", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b28", "title": "Meaning as an inferential role", "journal": "Jaroslav Peregrin", "year": "1975", "authors": ""}, {"ref_id": "b29", "title": "Competence-based curriculum learning for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Otilia Emmanouil Antonios Platanios; Graham Stretcu; Barnabas Neubig; Tom Poczos;  Mitchell"}, {"ref_id": "b30", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford"}, {"ref_id": "b31", "title": "Facenet: A unified embedding for face recognition and clustering", "journal": "", "year": "2015", "authors": "Florian Schroff; Dmitry Kalenichenko; James Philbin"}, {"ref_id": "b32", "title": "BLEURT: Learning robust metrics for text generation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Thibault Sellam; Dipanjan Das; Ankur Parikh"}, {"ref_id": "b33", "title": "Edinburgh neural machine translation systems for WMT 16", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b34", "title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b35", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b36", "title": "Exploiting persona information for diverse generation of conversational responses", "journal": "", "year": "2019", "authors": "Haoyu Song; Wei-Nan Zhang; Yiming Cui; Dong Wang; Ting Liu"}, {"ref_id": "b37", "title": "Dialogue response selection with hierarchical curriculum learning", "journal": "Long Papers", "year": "2021", "authors": "Yixuan Su; Deng Cai; Qingyu Zhou; Zibo Lin; Simon Baker; Yunbo Cao; Shuming Shi; Nigel Collier; Yan Wang"}, {"ref_id": "b38", "title": "Sequence to sequence learning with neural networks", "journal": "MIT Press", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"ref_id": "b39", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "K Ashwin; Michael Vijayakumar; Ramprasath R Cogswell; Qing Selvaraju; Stefan Sun; David Lee; Dhruv Crandall;  Batra"}, {"ref_id": "b40", "title": "Few-shot text classification with triplet networks, data augmentation, and curriculum learning", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Jason Wei; Chengyu Huang; Soroush Vosoughi; Yu Cheng; Shiqi Xu"}, {"ref_id": "b41", "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks", "journal": "", "year": "2019", "authors": "Jason Wei; Kai Zou"}, {"ref_id": "b42", "title": "What Are People Asking About COVID-19? A Question Classification Dataset", "journal": "", "year": "2020", "authors": "Jerry Wei; Chengyu Huang; Soroush Vosoughi; Jason Wei"}, {"ref_id": "b43", "title": "", "journal": "", "year": "2005", "authors": "Page Corr;  Corr"}, {"ref_id": "b44", "title": "Curriculum learning by transfer learning: Theory and experiments with deep networks", "journal": "PMLR", "year": "2018", "authors": "Daphna Weinshall; Gad Cohen; Dan Amir"}, {"ref_id": "b45", "title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents. CoRR, abs", "journal": "", "year": "1901", "authors": "Thomas Wolf; Victor Sanh; Julien Chaumond; Clement Delangue"}, {"ref_id": "b46", "title": "Conditional bert contextual augmentation", "journal": "", "year": "2019", "authors": "Xing Wu; Shangwen Lv; Liangjun Zang; Jizhong Han; Songlin Hu"}, {"ref_id": "b47", "title": "Curriculum learning for natural language understanding", "journal": "Online. ACL", "year": "2020", "authors": "Benfeng Xu; Licheng Zhang; Zhendong Mao; Quan Wang; Hongtao Xie; Yongdong Zhang"}, {"ref_id": "b48", "title": "Dynamic curriculum learning for lowresource neural machine translation", "journal": "", "year": "2020", "authors": "Chen Xu; Bojie Hu; Yufan Jiang; Kai Feng; Zeyang Wang; Shen Huang; Qi Ju; Tong Xiao; Jingbo Zhu"}, {"ref_id": "b49", "title": "Hierarchical text classification of amazon product reviews", "journal": "", "year": "2020", "authors": "Kashnitsky Yury"}, {"ref_id": "b50", "title": "Dialogue distillation: Open-domain dialogue augmentation using unpaired data", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Rongsheng Zhang; Yinhe Zheng; Jianzhi Shao; Xiaoxi Mao; Yadong Xi; Minlie Huang"}, {"ref_id": "b51", "title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "Australia. Association for Computational Linguistics", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"ref_id": "b52", "title": "Character-level convolutional networks for text classification", "journal": "MIT Press", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b53", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b54", "title": "DIALOGPT : Largescale generative pre-training for conversational response generation", "journal": "", "year": "2020", "authors": "Yizhe Zhang; Siqi Sun; Michel Galley; Yen-Chun Chen; Chris Brockett; Xiang Gao; Jianfeng Gao; Jingjing Liu; Bill Dolan"}, {"ref_id": "b55", "title": "A dataset for document grounded conversations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Kangyan Zhou; Shrimai Prabhumoye; Alan W Black"}, {"ref_id": "b56", "title": "Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Yicheng Zou; Zhihua Liu; Xingwu Hu; Qi Zhang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Algorithm 1 :781Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation (PCC) Input: Dataset D for the downstream task; Output: Trained downstream task model; 1 For the entire dataset D, invoke the curriculum-aware paraphrase generation module with D and cache the augmentation resultsD for training purpose; 2 while not the end of training do 3 Set difficulty level l to 0 at the start of a cycle; 4 while not the end of current cycle do 5 while not the end of current curriculum do 6 Uniformly sample the next batch of training instance S; Invoke the curriculum-aware paraphrase generation module for each training instance in S to retreive a batch of training augmentation T with difficulty level l.; Invoke the task-specific model trainer to train the downstream task model with the training augmentation T ; 9 end 10 Increase l by 1 to the next level at the end of current curriculum;", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: A plot of the training loss for the analysis for cyclic learning. Best viewed in color.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(k=120, p=0.95) 0.145 0.481 0.711 0.826 0.877 0.897 Top-k&p (k=80, p=0.80) 0.125 0.415 0.634 0.762 0.825 0.850 Bot.-k (k=2, N =1)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: A plot of the percentage performance improvements of the downstream task of text classification against the number of data augmentation instances per curriculum. We use the first row inTable as the baseline and the last row in Table 2 as the full improvements.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Wei et al. (2021) proposed curriculum data augmentation with a gradual course. The training ends after passing the curriculums once. We found that a typical problem called catastrophic forgetting(Kirkpatrick et al., 2017) can hamper the performance during such a gradual course, meaning that the model can gradually forget the knowledge learned in an easier course. The augmentation for later curriculums is a subtask of an easier curriculum and can have lexical overlaps. Formally, the input samples x t+1 can have overlapping lexical x t i which are the same as x t j , where t and t + 1 represent the curriculum difficulty levels, and i and j represent the word positions in the sentence. Due to catastrophic forgetting, the model can forget what it has learned earlier. Hence, we propose cyclic learning as shown in Algorithm 1 to inform the model which skills would be useful later before retrospecting to easier curriculums with lower difficulties.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation results in top-1 accuracy for the downstream task of text classification.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Automatic results for bottom-k sampling on PERSONACHAT. D represents the Distinct-N scores.", "figure_data": "CriteriaPCC w/o Bottom-k PCC w/ Bottom-kGramma. Richness3466  \u2021Lexical Richness3367  \u2021Difficulty3466  \u2021Paraphrasing5050  \u2020"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Human evaluation results for bottom-k in winning percentages. \u2021 indicates the results as passing a two-tailed binomial significance test with p < 0.0001.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "presents samples from EDA for a sample input 'I am glad to help you.' with each of the", "figure_data": "Sample Number\u03c4 = 0.1\u03c4 = 0.2\u03c4 = 0.3\u03c4 = 0.4\u03c4 = 0.5i)I equal am glad to help you.I am glad help to you.I am gald to happy help you.To help you.Glad am to help I you.ii)I am glad you help to.I am gladiola to help you.I am glad to assistance you.Help glad am to i you.I am gladiolus to helper you.iii)Am glad you.I am glad help you.I am glad you help to.You I gald to help am.I am glad help you.iv)I am glad to help you.I am glad equal to help you.I am glad to help you.I am glad to happy happy help you.I am happy to avail you."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Randomly selected cases for an input 'I am glad to help you.' using Easy Data Augmentation", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Analysis on the distribution for the textual similarity score with different augmentation methods.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Table as the baseline and the last row in Table 2 as the full improvements.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "8", "formula_coordinates": [3.0, 309.01, 288.37, 2.99, 7.78]}, {"formula_id": "formula_1", "formula_text": "9", "formula_coordinates": [3.0, 309.01, 309.7, 2.99, 7.78]}, {"formula_id": "formula_2", "formula_text": "P (x | x) = T t=1 P (x t |x 1 , ...,x t\u22121 , x),", "formula_coordinates": [4.0, 328.84, 92.11, 172.88, 34.56]}, {"formula_id": "formula_3", "formula_text": "Px i \u2208V (k) (x i |x 1 , ...,x i\u22121 , x),(1)", "formula_coordinates": [4.0, 350.88, 285.97, 173.54, 20.77]}, {"formula_id": "formula_4", "formula_text": "Px i \u2208V\\V (k) (x i |x 1 , ...,x i\u22121 , x),(2)", "formula_coordinates": [4.0, 345.82, 476.57, 178.6, 20.77]}, {"formula_id": "formula_5", "formula_text": "M(x,x i )+(1\u2212M(x,x i ))1(G(x,x i ) \u2265 \u03b2). (3)", "formula_coordinates": [5.0, 76.31, 368.18, 212.82, 20.55]}, {"formula_id": "formula_6", "formula_text": "d i = \u2308C \u00d7 sortx i \u2208X (G(x i , x)) |X | \u2309,(4)", "formula_coordinates": [5.0, 343.89, 302.64, 180.53, 33.52]}, {"formula_id": "formula_7", "formula_text": "L = D(a, p) \u2212 D(a, n) + \u03b3,", "formula_coordinates": [6.0, 117.15, 391.64, 125.7, 18.93]}, {"formula_id": "formula_8", "formula_text": "P (r | k, c) = T t=1 P (r t | r 1 , ..., r t\u22121 , k, c),", "formula_coordinates": [6.0, 86.67, 712.27, 186.64, 34.56]}, {"formula_id": "formula_9", "formula_text": "Model F1 BLEU-2 BLEU-3 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L", "formula_coordinates": [8.0, 73.43, 285.25, 440.65, 9.07]}], "doi": "10.1145/1553374.1553380"}