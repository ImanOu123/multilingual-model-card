{"title": "An Efficient Algorithm For Weak Hierarchical Lasso", "authors": "Yashu Liu; Jie Wang; Jieping Ye", "pub_date": "", "abstract": "Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy which has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution of its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this paper, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the GIST (General Iterative Shrinkage and Thresholding) optimization framework which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for largesize problems. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. We have conducted extensive experiments on both synthetic and real data sets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation.", "sections": [{"heading": "INTRODUCTION", "text": "Consider a linear regression model with the outcome variable y and d predictors x1, . . . , x d :\ny = w0 + d i=1 xiwi + ,(1)\nwhere w0 is the bias term, wi, i = 1 . . . , d is the coefficient and \u223c N (0, \u03c3 2 ) is the noise term. In many applications, a simple linear regression model is not sufficient for predictive or explanatory purposes. One strategy which has recently received increasing attention in statistics is to include interaction terms into the model to capture the nonlinearity of the data [17,22]. For example, the linear model including terms of order-2 and lower has the following form:\ny = w0 + d i=1 xiwi + 1 2 d i=1 d j=1 xixjQi,j + ,(2)\nwhere the cross-product term xixj, i = j refers to as the interaction variable (one may view x 2 i as a special interaction variable), and w i s and Q \u2208 R d\u00d7d are the main effect and interaction effect coefficients respectively. Applications with interaction regression models are omnipresent. For example, in psychological study, the effectiveness of using 3-way interactions was demonstrated in testing psychological hypothesis [9]; there are strong evidences found in [4] that genetic-environment interactions have significant effects on conduct disorders; the research in [11] found a couple of evidences of gene-environment interactions in predicting depression status; in [26], the interaction between continuance commitment and affective commitment was found significant in predicting job withdraw intentions and absenteeism; [13] discovered that brain-derived neurotrophic factor interacts with early life stress in predicting cognitive features of depression and anxiety. However, the use of higher order terms leads to data of high dimensionality. For instance, for regression model (1), if one wants to add all terms of order-k and lower, then there will be a total of O(d k ) variables, which is computationally demanding for parameter estimation even when k and d are fairly small. Thus, an efficient approach that is able to deal with huge dimensionality is desired in such cases, and the sparse learning methodology is one promising approach for tackling such problem [27,18,7,5,32]. In this paper, we focus on the model (2) with pairwise interactions, i.e., twofactor interactions. Note that the analysis can be extended to the model with higher-order interactions.\nIn general, not all of the main effects and interactions are of interest, thus it is critical to select the variables of great significance. One simple approach for high dimensional interaction regression is to directly apply the Lasso [27], also known as the \"all-pairs Lasso\" [2] in the case of two-factor interactions. However, the all-pairs Lasso estimator does not account for any structural information which has been shown to be important for prediction and interpretation of the high dimensional interaction regression model [2,30,25,29,6]. In statistics, a hierarchical structure between main effects and interaction effects has been shown to be very effective in constraining the search space and identifying important individual features and interactions [2,30,25,29,6]. Specifically, the hierarchical constraint requires that an interaction term xixj is selected in the model only if the main effects xi and/or xj are included. Strong theoretical properties have been established for such hierarchical model [29,30]. The hierarchical structure is supported by the argument that large main effects may result in interaction of more importance, and it is desired in a wide range of applications in engineering and underlying science. Traditional approaches to fit such a model typically follow the following two-step procedures [22]:\n(i) Fit a linear regression model that only includes the main effects and then select the significant features;\n(ii) Fit the reformulated model with the identified individual features and the interactions constructed via domain knowledge.\nSince even a small d may lead to a huge amount of interaction variables, the two-step procedure is still time-consuming in many applications. Recently, there have been growing research efforts on imposing the hierarchical structure on main effects and interactions in the regression model with novel sparse learning methods. In [2], in order to enable feature selection and impose heredity structures, the authors proposed strong hierarchical Lasso which adds a set of constraints to the Lasso formulation to achieve the strong hierarchy where the interaction effects are non-zero only if the corresponding main effects are non-zero. In [25], a Lasso-type penalized least square formulation called VANISH was proposed to achieve the strong hierarchy between the interaction effects and main effects. In [29], a type of non-negative garrote method was proposed to achieve the heredity structures. In [30], the Composite Absolute Penalties were proposed to achieve heredity structures for interaction models. In contrast to the above works which fulfill the hierarchical structure via solving convex problems, Choi et al. in [6] formulated a non-convex problem to achieve the strong hierarchy by assuming that the coefficient of an interaction term is a product of a scalar and main effect coefficients. Different from the strong hierarchy, the weak hierarchy between the main effects and the interaction effects requires that an interaction is included in the model only if at least one of the main effects is included in the model. In mathematical form, Qi,j = 0 only if wi = 0 OR wj = 0. The weak hierarchy can be considered as a structure in between the strong hierarchy and no hierarchical structure [2,29,30]. Specifically, weak hierarchy allows those interactions with only one significant \"parent\" (main effect) to be included in the model. Several existing empirical studies have demonstrated the stronger predictive power of weak hierarchical model [19]. In our study, we mainly focus on the interaction regression model with weak hierarchical structure. We follow the weak hierarchical Lasso approach recently proposed by [2] to fit the pairwise interaction regression model with the weak hierarchy. By imposing restrictions of the weak hierarchy and taking advantage of the Lasso penalty [27] that leads to sparse coefficients, the weak hierarchical Lasso is able to simultaneously attain a hierarchical solution and identify important main effects and interactions. However, the set of constraints restricting hierarchical constraints make the problem non-convex; the algorithm proposed in [2] aims to solve a convex relaxation. The convex relaxation, however, requires additional conditions to guarantee the weak hierarchy, which is not desirable.\nIn this paper, we propose to directly solve the weak hierarchical Lasso using the GIST (General Iterative Shrinkage and Thresholding) optimization framework recently proposed by [15]. The GIST framework has been shown to be highly efficient for solving large-scale non-convex problems. The most critical step in GIST is to compute a sequence of proximal operators [23]. In this paper, we first show that the proximal operator related to weak hierarchical Lasso admits an analytical form solution by factorizing unknown coefficients into sign matrices and non-negative coefficients. However, a naive method of computing the subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large-size problems. To this end, we further develop an efficient algorithm for solving the subproblems, which achieves a linearithmic time complexity. We evaluate the efficiency and effectiveness of the proposed algorithm and compare it with the convex relaxation in [2] and other state-of-the-art methods using synthetic and real data sets. Our empirical study demonstrates the high efficiency of our algorithm and the superior predictive performance of weak hierarchical Lasso over the competing methods.\nThe remaining of the paper is organized as follows: we give a brief review of the weak hierarchical Lasso and its convex relaxation in Section 2. In Section 3, we derive the closed form solution to the proximal operator of the original weak hierarchical Lasso by decomposing the unknown coefficients into signs and the non-negative coefficients. Then, we show how the associated proximal operator can be computed efficiently. We report the experimental results in Section 4. We conclude this paper in Section 5.", "publication_ref": ["b16", "b21", "b8", "b3", "b10", "b25", "b12", "b0", "b26", "b17", "b6", "b4", "b31", "b26", "b1", "b1", "b29", "b24", "b28", "b5", "b1", "b29", "b24", "b28", "b5", "b28", "b29", "b21", "b1", "b24", "b28", "b29", "b5", "b1", "b28", "b29", "b18", "b1", "b26", "b1", "b14", "b22", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "THE WEAK HIERARCHICAL LASSO", "text": "In this section, we briefly review the weak hierarchical Lasso and its corresponding convex relaxed formulation [2]. Suppose we are given n pairs of data points {(xi, yi)} n i=1 \u2282 R d \u00d7 R. Let Y \u2208 R n\u00d71 be the vector of outcome and X \u2208 R n\u00d7d be the design matrix. Let Z \u2208 R n\u00d7(d\u2022d) be the matrix of interactions where Z = Z (1) , Z (2) , . . . , Z (d) , Z (i) \u2208 R n\u00d7d and each column of Z (i) , i = 1, . . . d is an interaction, i.e., Z (i)\n\u2022,j = X\u2022,i X\u2022,j ( is the operator of elementwise product). Thus, Z (i) captures the pairwise interactions between the i-th feature and all d features. Note that, we include the quadratic terms x 2 i in the interaction model for clearer presentation, however our analysis is still applicable if they are not included in the model. By assuming that Y is centered and X, Z are column-wise normalized to zero mean and unit standard deviation, we can set the bias term w0 = 0. Thus, in matrix form, the pairwise interaction regression model can be expressed as\nY = Xw + 1 2 Z \u2022 vec(Q) + ,(3)\nwhere \u223c N (0, \u03c3 2 I) and \"vec\" is the vectorization operator that transforms a matrix to a column vector by stacking the columns of the matrix. Thus, the least square loss function of ( 3) is given by:\nL (w, Q) = 1 2 Y \u2212 Xw \u2212 1 2 Z \u2022 vec(Q) 2 2 .(4)\nThen, the weak hierarchical Lasso formulation takes the form of [2]:\nmin x,Q L (w, Q) + \u03bb w 1 + \u03bb 2 Q 1 s.t. Q\u2022,j 1 \u2264 |wj| for j = 1, . . . , d,(5)\nwhere Q 1 = i,j |Qi,j| and \u03bb is the Lasso penalty parameter.\nNote that the constraints in (5) guarantee the weak hierarchical structure since the coefficient Qi,j of interaction xixj is non-zero only if at least one of its main effects is included in the model, i.e., wi = 0 or wj = 0. However, the imposed hierarchical constraints make problem (5) nonconvex. Instead of solving (5), Bien et al. in [2] proposed to solve the following relaxed version:\nmin w,Q L w + \u2212 w \u2212 , Q + \u03bb1 T (w + + w \u2212 ) + \u03bb 2 Q 1 s.t. Q\u2022,j 1 \u2264 w + j + w \u2212 j w + j \u2265 0 w \u2212 j \u2265 0 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe for j = 1, . . . , d,(6)\nwhere 1 represents a column vector of all ones. In view of (6), we can see that w 1 is relaxed to w + + w \u2212 . Problem ( 6) is convex and can be solved by many efficient solvers such as FISTA [1]. However, Bien et al. in [2] showed that problem (6) needs an additional ridge penalty to guarantee the weak hierarchical structure of the estimator. In this paper, we propose an efficient algorithm which directly solves the non-convex weak hierarchical Lasso formulation in (5).", "publication_ref": ["b1", "b0", "b1", "b1", "b1", "b0", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "THE PROPOSED ALGORITHM", "text": "In this section, we propose an efficient algorithm named \"eWHL\", which stands for \"efficient Weak Hierarchical Lasso\", to directly solve the weak hierarchical Lasso. eWHL makes use of the optimization framework of GIST (General Iterative Shrinkage and Thresholding) due to its high efficiency and effectiveness for solving non-convex sparse formulations. One of the critical steps in GIST is to compute the proximal operator associated with the penalty functions. As one of our major contributions, we first factorize the unknown coefficients into the product of their signs and magnitudes; and then show that the proximal operator of (5) admits a closed form solution in Section 3.1. Another major contribution is that we present an efficient algorithm for computing the proximal operator associated with the non-convex weak hierarchical Lasso in Section 3.2. The time complexity of solving each subproblem of the proximal operator can be reduced from quadratic to linearithmic. We then summarize our algorithm for computing the proximal operator in Section 3.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Closed Form Solution to the Proximal Operator", "text": "In this section, we show how to derive the closed form solution of the proximal operator associated with (5) in detail. Let\nP = (a, B), a \u2208 R d , B \u2208 R d\u00d7d B\u2022,j 1 \u2264 |aj|, j = 1, . . . , d\nand the indicator function be defined by\nR(w, Q) = \uf8f1 \uf8f2 \uf8f3 \u03bb w 1 + \u03bb 2 Q 1 , if (w, Q) \u2208 P +\u221e, if (w, Q) / \u2208 P .(7)\nGiven a sequence w (k) , Q (k) , the proximal operator associated with weak hierarchical Lasso is:\nw (k+1) , Q (k+1) = arg min w, Q L w (k) , Q (k) + \u2207wL w (k) , Q (k) , w \u2212 w (k) + \u2207QL w (k) , Q (k) , Q \u2212 Q (k) + t (k) 2 w \u2212 w (k) 2 2 + t (k) 2 Q \u2212 Q (k) 2 F + R(w, Q),(8)\nwhere t (k) > 0.\nSimple algebraic manipulation leads to\nw (k+1) , Q (k+1) = arg min w, Q 1 2 w \u2212 v (k) 2 2 + 1 2 Q \u2212 U (k) 2 2 + 1 t (k) R(w, Q),(9)\nwhere\nv (k) =w (k) \u2212 \u2207wL w (k) , Q (k) /t (k) , U (k) =U (k) \u2212 \u2207QL w (k) , Q (k) /t (k) .\nThus, problem (5) can be solved by iteratively solving the proximal operator in (9). Because R(w, Q) is an indicator function, we can rewrite the proximal operator (9) as arg min\nw, Q 1 2 w \u2212 v 2 2 + 1 2 Q \u2212 U 2 F + \u03bb t w 1 + \u03bb 2t Q 1 s.t. Q\u2022,j 1 \u2264 |wj| for j = 1, . . . , d.(10)\nWe omit the superscripts for notational simplicity. The vector of main effect coefficients can be written as w = S 0w , wherewj = |wj|, j = 1, . . . , d and S 0 \u2208 R d\u00d7d is a diagonal matrix whose j-th diagonal element is the sign of wj, i.e., S 0 j,j = sign(wj). We define\nsign(w) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if w > 0 \u22121 if w < 0 0 if w = 0 ,(11)\nand we assume in this paper that the sign operator is applied on vectors or matrices elementwise. Similarly, we factorize each column of the interaction coefficient matrix as Q\u2022,j = S j Q\u2022,j, j = 1 . . . , d, where Qi,j = |Qi,j| and S j \u2208 R d\u00d7d is the diagonal sign matrix. Then, the proximal operator ( 10) is equivalent to arg min\nw, Q 1 2 w \u2212 v 2 2 + 1 2 Q \u2212 U 2 F + \u03bb t w 1 + \u03bb 2t Q 1 s.t. Q\u2022,j 1 \u2264 |wj| wj = S 0 j,jwj Q\u2022,j = S j Q\u2022,j wj \u2265 0 Q\u2022,j 0 \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe for j = 1, . . . , d,(12)\nwhere Q,w and S j , j = 0, . . . , d are the unknown variables, is defined as the element-wise \"greater than or equal to\" comparison operator, i.e., for V, U \u2208 R d\u00d71 , V U \u21d4 Vi \u2265 Ui, i = 1 . . . , d. Therefore, the solutions of the original weak hierarchical Lasso can be obtained by iteratively solving (12). Note that the amounts of l1 penalties on w and Q can be different. Here we use the same penalty parameter \u03bb for notational simplicity and consistency with the original formulation of weak hierarchical Lasso (5) studied in [2]. Though the factorization introduces more variables and constraints, we show that the resulting proximal operator admits a closed form solution. More importantly, we show that each sub-problem of the proximal operator can be solved by the proposed eWHL algorithm in linearithmic time. Indeed, the factorization of w and Q into their signs and magnitudes is the first key to directly solve the original weak hierarchical Lasso.\nIt is clear that the proximal operator in (12) can be decoupled into d subproblems:\narg mi\u00f1 w j ,S 0 jj , Q \u2022,j ,S j 1 2 S 0 jjwj \u2212 vj 2 2 + 1 2 S j Q\u2022,j \u2212 U\u2022,j 2 2 + \u03bb tw j + \u03bb 2t 1 T Q\u2022,j s.t. 1 T Q\u2022,j \u2264wj Q\u2022,j 0 , for j = 1, . . . , d.(13)\nNext, we show that ( 13) has a closed form solution. Since\n1 2 (wj \u2212 vj) 2 = 1 2 S 0 jjwj \u2212 vj 2 = 1 2 S 0 jj S 0 jjwj \u2212 vj 2 = 1 2\nwj \u2212 S 0 jj vj 2 andwj \u2265 0, S 0 j,j must have the same sign as vj, that is, wj has the same sign as vj. Otherwise, the value of 1 2 wj \u2212 S 0 jj vj 2 will not achieve the minimum. Similarly, one can show that S j i,i , i.e., the sign of Qi,j, must be the same as the sign of Ui,j. Thus, the diagonal elements diag(S 0 ) = sign(v), diag(S j ) = sign(U\u2022,j), j = 1, . . . , d. Next, we show how to computew and Q.\nBy letting\u1e7dj = S 0 jj vj and Uj = S j U\u2022,j, each subproblem ( 13) is equivalent to arg mi\u00f1\nw j , Q \u2022,j 1 2 wj \u2212\u1e7dj 2 2 + 1 2 Q\u2022,j \u2212 U\u2022,j 2 2 + \u03bb tw j + \u03bb 2t 1 T Q\u2022,j s.t. 1 T Q\u2022,j \u2264wj Q\u2022,j 0 . (14\n)\nAfter rearrangement, problem ( 14) can be expressed as:\nmi\u00f1 w j ,Q \u2022,j 1 2 wj \u2212vj 2 2 + 1 2 Q\u2022,j \u2212 q U\u2022,j 2 2 s.t. 1 T Q\u2022,j \u2264wj Q\u2022,j 0 ,(15)\nwherevj =\u1e7dj \u2212 \u03bb t 1 and q U\u2022,j = U\u2022,j \u2212 \u03bb 2t 1. We solve (15) by deriving its dual problem. Let \u03b3 \u2265 0 be the Lagrangian multiplier dual variable of the first inequality constraint. Define the Lagrangian function of (15) as:\nl(\u03b3,w, Q) = 1 2 (w \u2212v) 2 + 1 2 Q \u2212 q U 2 2 + \u03b3 1 T Q \u2212w\nwhere we omit the subscripts for simplicity with a little abuse of notation. Since the constraint 1 T Q \u2264w is affine, the strong duality holds for the minimization problem (15). Thus, the dual problem of ( 15) is:\nmax \u03b3\u22650 mi\u00f1 w,Q 0 1 2 (w \u2212v) 2 + 1 2 Q \u2212 q U 2 2 + \u03b3 1 T Q \u2212w . (16\n)\nBy rearranging the terms, ( 16) is equivalent to:\nmax \u03b3\u22650 mi\u00f1 w, Q 0 1 2 (w \u2212 (v + \u03b3)) 2 + 1 2 Q \u2212 q U \u2212 \u03b31 2 2 +h(\u03b3), (17\n)\nwhere h(\u03b3) = \u2212v\u03b3 \u2212 1 2 \u03b3 2 + \u03b31 T q U \u2212 1 2 \u03b3 2 1 T 1.\nFor fixed \u03b3, in order to obtain the minimum of the objective function in (17), we conclude that\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3v + \u03b3 \u2265 0 \u21d2w =v + \u03b3 v + \u03b3 < 0 \u21d2w = 0 q Ui \u2212 \u03b3 \u2265 0 \u21d2 Qi = q Ui \u2212 \u03b3 q Ui \u2212 \u03b3 < 0 \u21d2 Qi = 0 (18\n)\ndue to the constraintsw \u2265 0 and Q 0. Therefore, if we obtain a dual optimal solution \u03b3 * that maximizes the dual problem (17), then we can readily compute the closed form solution to (13) and thus to (12). That is, w * = S 0w * , Q *\n\u2022,j = S j Q * \u2022,j where diag(S 0 ) = sign(vj), diag(S j ) = sign(U\u2022,j), j = 1, . . . , d andw * , Q * are obtained via (18) at the optimal dual solution \u03b3 * .", "publication_ref": ["b8", "b11", "b1", "b14", "b16", "b12", "b11", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "The Dual Optimal Solution", "text": "Next, we show how to efficiently compute the dual optimal solution \u03b3 * . First, we sort \u2212v and q Ui, i = 1, . . . , d in ascending order. Without loss of generality, we assume:\nq U1 \u2264 . . . \u2264 q UL \u2264 \u2212q v \u2264 q UL+1 \u2264 . . . \u2264 q U d .(19)\nThere are four possible cases about the locations of \u03b3. We discuss how to identify the optimal dual solution \u03b3 * in each of the four cases.\nCase 1 : When . . . \u2264 q UG \u2264 \u03b3 \u2264 q UG+1 \u2264 . . . \u2264 \u2212v \u2264 . . ., the objective in (17) at \u03b3 * becomes 1 2 G i=1 q Ui \u2212 \u03b3 2 + 1 2 (v + \u03b3) 2 + h(\u03b3) = 1 2 G i=1 q U 2 i + d i=G+1 \u03b3 q Ui \u2212 d \u2212 G 2 \u03b3 2 + 1 2v 2 .(20)\nFunction ( 20) is a quadratic function with respect to \u03b3 and the unconstrained maximum is achieved at the axis of symmetry point\nd i=G+1 q U i d\u2212G \u2265 q UG+1. Since \u03b3 falls in the interval q UG, q UG+1 , we set \u03b3 = q UG+1\nto achieve the maximum objective value of (17). It can be further concluded that, in Case 1, among all the intervals on the left of \u2212v, the maximum objective value of ( 17) is achieved at the q UG.\nCase 2:\nWhen . . . \u2264 q UL \u2264 \u03b3 \u2264 \u2212v \u2264 q UL+1 \u2264 .\n. ., it turns out that the objective value in (17) at \u03b3 is similar to (20):\n1 2 L i=1 q U 2 i + d i=L+1 \u03b3 q Ui \u2212 d \u2212 L 2 \u03b3 2 + 1 2v 2 . (21\n)\nBy a similar argument, we can set \u03b3 = \u2212v to achieve the maximum. Combining the results of Case 1 and Case 2, we conclude that, we may only consider \u03b3 in the range\n[max (\u2212v, 0) , +\u221e]. Note that when L = d, that is q U d \u2264 \u03b3 \u2264 \u2212v, (21) is a constant 1 2 d i=1 q U 2 i + 1 2v 2\n, and thus \u03b3 can be any value in the interval q\nU d , \u2212v . Case 3: When . . . \u2264 q UL \u2264 \u2212v \u2264 \u03b3 \u2264 q UL+1 \u2264 . . ., the value of the objective function in (17) at \u03b3 * becomes 1 2 L i=1 q Ui \u2212 \u03b3 2 + h(\u03b3) = 1 2 L i=1 q U 2 i + \u03b3 d i=L+1 q Ui \u2212v \u2212 d + 1 \u2212 L 2 \u03b3 2 . (22\n)\nAgain, ( 22) is a quadratic function of \u03b3 and\nd i=L+1 q U i \u2212v d+1\u2212L \u2265 \u2212v. If d i=L+1 q U i \u2212v d+1\u2212L \u2265 q UL+1, the maximum is achieved at \u03b3 = q UL+1, otherwise the maximum is achieved at \u03b3 = d i=L+1 q Ui \u2212v d + 1 \u2212 L . Case 4: When . . . \u2264 \u2212v \u2264 . . . \u2264 q UG \u2264 \u03b3 \u2264 q UG+1 \u2264 . . ., the objective value in (17) is similar to (22): 1 2 G i=1 q U 2 i + \u03b3 d i=G+1 q Ui \u2212v \u2212 p + 1 \u2212 G 2 \u03b3 2 . (23\n)\nIf d i=G+1 q U i \u2212v d+1\u2212G \u2265 q UG+1, then the maximum is achieved at \u03b3 = q UG+1; If d i=G+1 q U i \u2212v d+1\u2212G \u2264 q UG, then the maximum is achieved at \u03b3 = q UG; If q UG \u2264 d i=G+1 q U i \u2212q v d+1\u2212G \u2264 q UG+1, the maximum is achieved at \u03b3 = d i=G+1 q Ui \u2212v d + 1 \u2212 G .\nSince we know exactly the value of \u03b3 for all the four cases, one naive way to find the optimal \u03b3 * is to enumerate all the possible locations and pick the one that maximizes the objective function value in (17). However, evaluating the objectives for all possible locations from max(\u2212v, 0) to q U d leads to a quadratic time algorithm for solving (17). Interestingly, we show below that the time complexity of solving (17) ", "publication_ref": ["b16", "b16", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "can be reduced to O(d log(d)).", "text": "Let us first list some useful properties as follows: Given the ordered sequence (19):\n\u2022 Property 1:\nThe maximum objective value of (17) in Case 3 is larger than the one in Cases 1 & 2;\n\u2022 Property 2:\nIn Case 4, for a pair of adjacent intervals q\nUG\u22121, q UG and q UG, q UG+1 , if d i=G+1 q U i \u2212q v d+1\u2212G \u2265 q UG+1 for [ q UG, q UG+1], then d i=G q U i \u2212q v d+1\u2212(G\u22121) \u2265 q UG for [ q UG\u22121, q UG];\n\u2022 Property 3: \nIn Case 4, if d i=G+1 q U i \u2212v d+1\u2212G \u2265 q UG+1 for q UG, qUG+1\nU i \u2212q v d+1\u2212(G\u22121) \u2264 q UG\u22121 for q UG\u22121, q UG , then d i=G+1 q U i \u2212q v d+1\u2212G \u2264 q UG for q UG, q UG+1 .\n\u2022 Property 5:\nIn Case 4, if d i=G q U i \u2212q v d+1\u2212(G\u22121) \u2264 q UG\u22121 for q UG\u22121, q\nUG , the maximum objective value of ( 17) in q UG\u22121, q UG , is larger than or equal to the one in q UG, q UG+1 .\n\u2022 Property 6:\nIn Case 4, if q UG \u2264 d i=G+1 q U i \u2212q v d+1\u2212G \u2264 q UG+1 for q UG, q UG+1 , then d i=G q U i \u2212q v d+1\u2212(G\u22121) \u2265 q UG for q UG\u22121, q UG and d i=G+2 q U i \u2212q v d+1\u2212(G+1) \u2264 q UG+1 for q UG+1, qUG+2\n, and the maximum value of (17) in the interval q UG, q UG+1 is larger than or equal to the ones in its neighbor intervals.\nProperties 2-6 also apply for adjacent intervals \u2212v, q UL+1 and q UL+1, q UL+2 in Case 3. We omit the proof of Properties 1-6 since they are direct applications of 1-D quadratic optimization. Property 1 indicates that it is sufficient for the algorithm to start searching \u03b3 * from Case 3. Properties 2 & 3 imply that, for some interval, if the axis of symmetry is on the right hand side of the interval, then one only needs to consider the intervals to the right. Similarly, Properties 4 & 5 indicate that, for some interval, if the axis of symmetry is on the left hand side of the interval, then one only needs to consider the intervals to the left. Property 6 combined with Properties 1-5 imply that, for certain interval, if it contains the axis of symmetry, then \u03b3 * is the axis of symmetry point. Thus, we can draw the following conclusion:\n(1) if max q U d , \u2212v < 0, then \u03b3 * = 0; (2) if \u2212v > q U d , then \u03b3 * = max(\u2212v, 0); (3) if q UG \u2264 d i=G+1 q U i \u2212v d+1\u2212G \u2264 q UG+1 for a certain interval q UG, q UG+1 , then \u03b3 * = d i=G+1\u01d3 i \u2212v d + 1 \u2212 G .\nAt each move, the axis of symmetry\nd i=G+1 q U i \u2212v d+1\u2212G\ncan be calculated by a constant operation based on the value from the last step, and the time complexity of searching \u03b3 * reduces from quadratic to O(d log(d)) as the computation is dominated by the sorting operation. Once \u03b3 * is determined, we can computew and Q by (18). Note that, the subproblem of the proximal operator associated with the convex relaxation in [2] is solved by searching for the dual variable in a different way with time complexity O(d 2 ).\nIn summary, we reformulate the proximal operator for the original weak hierarchical Lasso by factorizing the unknown coefficients. The reformulated proximal operator is shown to admit a closed form solution, which enables directly solving the weak hierarchical Lasso problem. Moreover, the subproblem of the proximal operator can be computed efficiently with a time complexity of O(d log(d)). The detailed algorithm for solving the proximal operator ( 12) is described in Algorithm 1. We give the details of eWHL algorithm in Algorithm 1 Computation of the Proximal Operator of Weak Hierarchical Lasso end while 20:\nInput: v \u2208 R d\u00d71 , U \u2208 R d\u00d7d , t \u2208 R+, \u03bb \u2208 R+ Output: w \u2208 R d\u00d71 , Q \u2208 R d\u00d7d 1:v = sign(v) v \u2212 \u03bb t 1; q U = sign(U ) U \u2212 \u03bb 2t 11\nend if 21:wj = max(vj + \u03b3, 0);\nQ\u2022,j = max q U\u2022,j \u2212 \u03b3, 0 ; 22:\nend if 23: end for 24: w = sign(v) w; Q = sign(U ) Q;\nAlgorithm 2. Following [15], we choose the step size t (k) by the Barzilai-Borwein (BB) Rule.", "publication_ref": ["b17", "b1", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "In this section, we evaluate the efficiency and effectiveness of the proposed algorithm on both synthetic and real data sets. In our first experiment, we compare the efficiency of our proposed algorithm and the convex relaxation of weak hierarchical Lasso [2] on synthetic data sets where the weak hierarchical structure holds between main effects and interaction effects. In our second experiment, we compare the classification performance of the weak hierarchical Lasso with other classifiers and sparse learning techniques on the data collected from Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 .", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Efficiency and Effectiveness Comparison on Synthetic Data Sets", "text": "In this experiment, we compare the efficiency of the proposed eWHL algorithm with the convex relaxation on synthetic data sets. Our algorithm is built upon the GIST framework which is available online [16]. The source code of the convex relaxed weak hierarchical Lasso (cvxWHL) was available in the R package \"hierNet\" [3] where the optimiza-Algorithm 2 The Efficient Weak Hierarchical Lasso Algorithm (eWHL)\nInput: X \u2208 R n\u00d7d , Z \u2208 R n\u00d7(d\u2022d) , \u03bb \u2208 R+, \u03b7 > 1 Output: w \u2208 R d\u00d71 , Q \u2208 R d\u00d7d\nInitialize k \u2190 0 and starting points w (0) and Q (0) ; 2: repeat Choose the step size t (k) by the BB Rule 4:\nrepeat v (k) = w (k) \u2212 \u2207wL w (k) , Q (k) /t (k) ; U (k) = U (k) \u2212 \u2207QL w (k) , Q (k) /t (k) ;\nSolve w (k+1) , Q (k+1) by Algorithm 1 with input v (k) , U (k) , t (k) , \u03bb ;\nt (k) \u2190 \u03b7t (k) ; 6:\nuntil line search criterion is satisfied k \u2190 k + 1 8: until stop criterion is satisfied tion procedure was implemented by C. Since the proposed algorithm in this paper directly solves the non-convex weak hierarchical Lasso (5), and the eventual goal of the convex relaxed weak hierarchical Lasso is also to find a good \"relaxed\" solution to the original problem, we compare the two algorithms in terms of the objective function in (5). In the experiment, entries of X \u2208 R n\u00d7d are i.i.d generated from the standard normal distribution, i.e., Xi,j \u223c N (0, 1). The matrix of interactions, Z, is then generated via the normalized X where Z = Z (1) , Z (2) , . . . , Z (d) ,\nZ (i) \u2208 R n\u00d7d , Z (i) \u2022,j = X\u2022,i X\u2022,j.\nThe ground truths w \u2208 R d\u00d71 and Q \u2208 R d\u00d7d are generated based on the weak hierarchical structure Q\u2022,j 1 \u2264 |wj|, j = 1, . . . , d. In addition, we vary the ratio of coefficient sparsity, i.e., the portion of zero entries in w and Q, from 30% to 85%. Then, the outcome vector Y is constructed as Y = Xw + 1 2 Z \u2022 vec(Q) + where X and Z are normalized to zero mean and unit standard deviation and \u223c N (0, 0.01 \u2022 I). We use sample size n = 100 and 200 and we choose the number of main effects d from {100, 200, 300, 400, 500, 600}. The parameter of the l1 penalty, \u03bb, is chosen from {1, 3, 5, 10, 20}. All algorithms are executed on a 64-bit machine with Intel(R) Core(TM) quad-core processor (i7-3770 CPU @ 3.40 GHz) and 16.0 GB memory. We terminate the algorithm when the maximum relative difference of the coefficients between two consecutive iterations is less than 1e \u22125 . We run 20 trials for each setting and report the average execution time. The detailed results are shown in Table 1.\nFrom Table 1, we observe that eWHL is significantly faster than cvxWHL. Our algorithm is up to 25 times faster than the competing algorithm. As the dimension increases, the running time of cvxWHL increases much faster than our proposed algorithm. Specifically, when the number of individual features increases to 400 (corresponds to 80200 interactions), cvxWHL may take more than one thousand seconds, while the proposed eWHL is reasonably fast even when the number of total variables is around two hundred thousands.\nTo make further comparisons of efficiency, we randomly generate three synthetic data sets where the weak hierarchical structure between main effects and interactions holds. The three data sets are of the same sample size n = 100 and the number of individual features is d = 300. The ratios of zero entries in the ground truth are 85%, 60% and 30% respectively. The regularization parameters are chosen from {0.5, 1, 2, 4, 6, 8, 16, 32, 64}. On each data set, we first run cvxWHL, and then the objective value of (5) in the final step is recorded. Then, we run the proposed eWHL and terminate the algorithm when the objective value of ( 5) is less than the one obtained by cvxWHL. The running time and the number of iterations needed to achieve the same objective value of both algorithms are reported in Figure 1. We can observe from Figure 1 that the proposed eWHL is much faster than cvxWHL.\nMoreover, we also conduct an experiment to compare the recovery performance of eWHL and cvxWHL. We generate synthetic data sets with sample size n = 100 and the number of individual features is d = 50 (1225 cross interactions). The number of non-zero main effects varies from {3, 4, 5, 6, 7} and the number of non-zero interaction effects is from {2, 4, 5, 8, 10}, respectively. For each setting, ten synthetic data sets are generated with noise \u223c N (0, 0.01 \u2022 I). We run both eWHL and cvxWHL with parameter selected via 5-fold cross-validation. Then we compute the sensitivity and specificity of recovery (where non-zero entries are positive and zero entries are negative). The means of sensitivity and specificity are plotted in Figure 2. We can observe that Table 1: Comparison of execution time (second) of the proposed algorithm for the non-convex weak hierarchical Lasso (eWHL) and the one for the convex relaxed formulation (cvxWHL) on synthetic data. The penalty parameters used in the experiment are from {1, 3, 5, 10, 20}. The data is generated under the weak hierarchical constraints where the portion of sparse coefficients is controlled to 85%, 60% and 30%. Two sample sizes, n = 100 and n = 200, are used and we vary the number of individual features from {200, 300, 400, 500, 600} corresponding to {20100, 45150, 80200, 125250, 180300} interactions (including the self product terms). both algorithms achieve high recovery rate while directly solving the original weak hierarchical Lasso leads to slightly better performance in recovering the non-zero effects.", "publication_ref": ["b15", "b2", "b4", "b0", "b1"], "figure_ref": ["fig_1", "fig_1", "fig_0"], "table_ref": []}, {"heading": "Classification Comparison on ADNI Data", "text": "In this experiment, we compare the weak hierarchical Lasso with its convex relaxation as well as other classifiers on the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set.\nIn Alzheimer's Disease (AD) research, Mild Cognitive Impairment (MCI) is an intermediate state between normal elderly people and AD patients [24]. The MCI patients are considered to be at high risk of progression to AD. Many recent work focus on how to accurately predict the MCI-AD conversion and identifying significant bio-markers for the prediction [8,10,12,19,21,28,31,14].\nIn this experiment, we compare the classification performance of the proposed eWHL with the convex relaxation and other classifiers on the task of discriminating the MCI  subjects who convert to dementia (i.e., MCI converter) within a three-year period from the MCI subjects who remain at MCI (i.e., MCI non-converter). The features used in the experiment (provided by our clinical collaborators) involve demographic information such as age, gender, years of education, clinical information such as scores of mini mental state examination (MMSE), Auditory Verbal Learning Test (A.V.L.T.), and the bio-markers including status of Apolipoprotein E, volume of hippocampus, thickness of Mid Temporal Gray Matter. There are 133 samples in total and the number of individual features is 36 (corresponds to 630 two way interactions). The interactions are generated by the normalized individual features and are normalized before entering the model. Since this is a classification task with binary labels, we replace the least square loss with logistic loss in the weak hierarchical Lasso. Besides the non-convex and convex weak hierarchical Lasso, we apply random forest (RF), Support Vector Machine (SVM) and sparse logistic regression on main effects, and on both main effects and interactions, respectively. We report the means and standard deviations of accuracy, sensitivity and specificity obtained from 10-fold cross-validation. The penalty parameters are tuned via 5-fold cross-validation in the training procedure. The sample statistics are shown in Table 3 and the classification performance is reported in Table 2. From Table 2, we can observe that, if we only use individual features for classification, then all the classifiers are biased towards the positive class, i.e., MCI converter. When interactions are included, we observe that the performances of random forest and SVM become worse. One possible reason is that the large number of variables brought by the interactions weakens their discriminative power. This is not the case for sparse logistic regression, which demonstrates the importance of feature selection. We can observe from the table that the convex relaxed weak hierarchical Lasso and the non-convex weak hierarchical Lasso achieve much better classification performance than the competitors. The improvement of the classification performance demonstrates the effectiveness of imposing hierarchical structures in interaction models. In addition, the superior classification performance (around 77% accuracy, sensitivity and specificity) of the proposed eWHL demonstrates that directly solving the non-convex weak hierarchical Lasso leads to solutions of higher quality than the convex relaxation.", "publication_ref": ["b23", "b7", "b9", "b11", "b18", "b20", "b27", "b30", "b13"], "figure_ref": [], "table_ref": ["tab_3", "tab_2", "tab_2"]}, {"heading": "CONCLUSIONS", "text": "In this paper, we propose an efficient algorithm, eWHL, to directly solve the non-convex weak hierarchical Lasso. One critical step in eWHL is to compute the proximal operator associated with the non-convex penalty functions. As one of our major contributions, we show that the proximal operator associated with the regularization function in weak hierarchical Lasso admits a closed form solution. Furthermore, we develop an efficient algorithm which computes each subproblem of the proximal operator with a time complexity of O(d log d). Extensive experiments on both synthetic and real data sets demonstrate the superior performance of the proposed algorithm in terms of efficiency and accuracy.\nIn the future, we plan to apply the non-convex weak hierarchical Lasso to other important and challenging applications such as depression study [20]. In addition, we plan to extend the proposed techniques to solve the non-convex strong hierarchical Lasso formulation.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGEMENT", "text": "This work was supported in part by NIH (R01 LM010730) and NSF (IIS-0953662, MCB-1026710, and CCF-1025177).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "journal": "SIAM Journal on Imaging Sciences", "year": "2009", "authors": "A Beck; M Teboulle"}, {"ref_id": "b1", "title": "A lasso for hierarchical interactions", "journal": "The Annals of Statistics", "year": "2013", "authors": "J Bien; J Taylor; R Tibshirani"}, {"ref_id": "b2", "title": "hierNet: A Lasso for Hierarchical Interactions", "journal": "", "year": "2013", "authors": "J Bien; R Tibshirani"}, {"ref_id": "b3", "title": "Genetic-environmental interaction in the genesis of aggressivity and conduct disorders", "journal": "Archives of General Psychiatry", "year": "1995", "authors": "R J Cadoret; W R Yates; G Woodworth; M A Stewart"}, {"ref_id": "b4", "title": "Quantitative robust uncertainty principles and optimally sparse decompositions", "journal": "Foundations of Computational Mathematics", "year": "2006", "authors": "E J Candes; J Romberg"}, {"ref_id": "b5", "title": "Variable selection with the strong heredity constraint and its oracle property", "journal": "Journal of the American Statistical Association", "year": "2010", "authors": "N H Choi; W Li; J Zhu"}, {"ref_id": "b6", "title": "A direct formulation for sparse pca using semidefinite programming", "journal": "", "year": "2004", "authors": "A Aspremont; L El Ghaoui; M I Jordan; G R Lanckriet"}, {"ref_id": "b7", "title": "Prediction of mci to ad conversion, via mri, csf biomarkers, and pattern classification", "journal": "Neurobiology of aging", "year": "2011", "authors": "C Davatzikos; P Bhatt; L M Shaw; K N Batmanghelich; J Q Trojanowski"}, {"ref_id": "b8", "title": "Probing three-way interactions in moderated multiple regression: development and application of a slope difference test", "journal": "Journal of Applied Psychology", "year": "2006", "authors": "J F Dawson; A W Richter"}, {"ref_id": "b9", "title": "Hippocampal and entorhinal atrophy in mild cognitive impairment prediction of alzheimer disease", "journal": "Neurology", "year": "2007", "authors": "D Devanand; G Pradhaban; X Liu; A Khandji; S De Santi; S Segal; H Rusinek; G Pelton; L Honig; R Mayeux"}, {"ref_id": "b10", "title": "Gene-environment interaction analysis of serotonin system markers with adolescent depression", "journal": "Molecular psychiatry", "year": "2004", "authors": "T C Eley; K Sugden; A Corsico; A M Gregory; P Sham; P Mcguffin; R Plomin; I W Craig"}, {"ref_id": "b11", "title": "Structural mri biomarkers for preclinical and mild alzheimer's disease", "journal": "Human brain mapping", "year": "2009", "authors": "C Fennema-Notestine; D J Hagler; L K Mcevoy; A S Fleisher; E H Wu; D S Karow; A M Dale"}, {"ref_id": "b12", "title": "Interactions between bdnf val66met polymorphism and early life stress predict brain and arousal pathways to syndromal depression and anxiety", "journal": "Molecular psychiatry", "year": "2009", "authors": "J Gatt; C Nemeroff; C Dobson-Stone; R Paul; R Bryant; P Schofield; E Gordon; A Kemp; L Williams"}, {"ref_id": "b13", "title": "Multi-stage multi-task feature learning", "journal": "", "year": "1997", "authors": "P Gong; J Ye; C Zhang"}, {"ref_id": "b14", "title": "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems", "journal": "", "year": "2013", "authors": "P Gong; C Zhang; Z Lu; J Huang; J Ye"}, {"ref_id": "b15", "title": "GIST: General Iterative Shrinkage and Thresholding for Non-convex Sparse Learning", "journal": "", "year": "2013", "authors": "P Gong; C Zhang; Z Lu; J Huang; J Ye"}, {"ref_id": "b16", "title": "The elements of statistical learning", "journal": "Springer", "year": "2009", "authors": "T Hastie; R Tibshirani; J Friedman; T Hastie; J Friedman; R Tibshirani"}, {"ref_id": "b17", "title": "An interior-point method for large-scale 1-regularized logistic regression", "journal": "Journal of Machine learning research", "year": "2007", "authors": "K Koh; S.-J Kim; S Boyd"}, {"ref_id": "b18", "title": "Hierarchical interactions model for predicting mild cognitive impairment (mci) to alzheimer's disease (ad) conversion", "journal": "PloS one", "year": "2014", "authors": "H Li; Y Liu; P Gong; C Zhang; J Ye; A D N Initiative"}, {"ref_id": "b19", "title": "Sparse generalized functional linear model for predicting remission status of depression patients", "journal": "World Scientific", "year": "2013", "authors": "Y Liu; Z Nie; J Zhou; M Farnum; V A Narayan; G Wittenberg; J Ye"}, {"ref_id": "b20", "title": "Derivation of a new adas-cog composite using tree-based multivariate analysis: prediction of conversion from mild cognitive impairment to alzheimer disease", "journal": "Alzheimer Disease & Associated Disorders", "year": "2011", "authors": "D A Llano; G Laforet; V Devanarayan"}, {"ref_id": "b21", "title": "Introduction to linear regression analysis", "journal": "Wiley", "year": "2012", "authors": "D C Montgomery; E A Peck; G G Vining"}, {"ref_id": "b22", "title": "Proximal algorithms. Foundations and Trends in optimization", "journal": "", "year": "2013", "authors": "N Parikh; S Boyd"}, {"ref_id": "b23", "title": "Mild cognitive impairment clinical trials", "journal": "Nature Reviews Drug Discovery", "year": "2003", "authors": "R C Petersen"}, {"ref_id": "b24", "title": "Variable selection using adaptive nonlinear interaction structures in high dimensions", "journal": "Journal of the American Statistical Association", "year": "2010", "authors": "P Radchenko; G M James"}, {"ref_id": "b25", "title": "Organizational commitment, turnover and absenteeism: An examination of direct and interaction effects", "journal": "Journal of Organizational Behavior", "year": "1995", "authors": "M J Somers"}, {"ref_id": "b26", "title": "Regression shrinkage and selection via the lasso", "journal": "Journal of the Royal Statistical Society. Series B (Methodological)", "year": "1996", "authors": "R Tibshirani"}, {"ref_id": "b27", "title": "Sparse learning and stability selection for predicting mci to ad conversion using baseline adni data", "journal": "BMC neurology", "year": "2012", "authors": "J Ye; M Farnum; E Yang; R Verbeeck; V Lobanov; N Raghavan; G Novak; A Dibernardo; V A Narayan"}, {"ref_id": "b28", "title": "Structured variable selection and estimation. The Annals of Applied Statistics", "journal": "", "year": "2009", "authors": "M Yuan; V R Joseph; H Zou"}, {"ref_id": "b29", "title": "The composite absolute penalties family for grouped and hierarchical variable selection", "journal": "The Annals of Statistics", "year": "2009", "authors": "P Zhao; G Rocha; B Yu"}, {"ref_id": "b30", "title": "Modeling disease progression via multi-task learning", "journal": "NeuroImage", "year": "2013", "authors": "J Zhou; J Liu; V A Narayan; J Ye"}, {"ref_id": "b31", "title": "Sparse principal component analysis", "journal": "Journal of computational and graphical statistics", "year": "2006", "authors": "H Zou; T Hastie; R Tibshirani"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "T ; 2 :2for j = 1 : d do 3: c = \u2212vj; 4: Sort q U\u2022,j to get a sequence S in ascending order where S1 \u2264 S1 \u2264 . . . \u2264 S d ; 5: if S d < 0 and c < 0 then 6:wj = 0; = c + S k ; 14: k = k \u2212 1; 15: if c/(d + 1 \u2212 k) \u2265 S k then 16: \u03b3 = c;", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Comparison of the running time and the number of iterations by the two algorithms. Three synthetic data sets are generated where the portions of zeros in the ground truth are 85%, 60%, 30% respectively. The plots in the same row correspond to the same data set. The plots in the left column present the running time and those in the right column show the number of iterations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Comparison of eWHL and cvxWHL in terms of recovery on synthetic data sets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The performance of MCI converter vs. MCI non-converter classification achieved by random forest (RF), Support Vector Machine (SVM), Sparse Logistic Regression (spsLog), the convex relaxed weak hierarchical Lasso (cvxWHL) and the proposed algorithm (eWHL). Classifiers are performed on main effects only (top) and on both the main effects and interactions (bottom). The average and standard deviation of accuracy, sensitivity and specificity obtained from 10-fold cross-validation are reported. ) 78.75 \u00b1 14.00 80.18 \u00b1 13.89 80.18 \u00b1 13.88 NA NA Specificity (%) 69.29 \u00b1 11.63 69.76 \u00b1 12.80 69.52 \u00b1 13.74 ) 71.26 \u00b1 10.22 59.45 \u00b1 14.43 73.57 \u00b1 10.30 75.22 \u00b1 11.02 77.42 \u00b1 8.50 Sensitivity (%) 83.04 \u00b1 13.18 59.29 \u00b1 17.83 74.29 \u00b1 16.22 75.71 \u00b1 19.11 77.14 \u00b1 12.05 Specificity (%) 58.10 \u00b1 23.23 60.00 \u00b1 15.42 72.86 \u00b1 12.46 74.52 \u00b1 16.84 77.62 \u00b1 15.02", "figure_data": "Main Effects OnlyRFSVMspsLogcvxWHLeWHLAccuracy (%)74.23 \u00b1 8.6775.22 \u00b1 8.7274.34 \u00b1 9.56NANASensitivity (%NANAMain Effects + InteractionsRFSVMspsLogcvxWHLeWHLAccuracy (%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The statistics of the ADNI data set used in our experiment. The MCI converters (MCI-cvt) are characterized as positive samples and the MCI non-converters (MCI non-cvt) are used as negative samples.", "figure_data": "Total (+) MCI-cvt (-) MCI non-cvt# of samples1337162# of main effects36# of interactions630"}], "formulas": [{"formula_id": "formula_0", "formula_text": "y = w0 + d i=1 xiwi + ,(1)", "formula_coordinates": [1.0, 391.5, 385.83, 164.42, 26.84]}, {"formula_id": "formula_1", "formula_text": "y = w0 + d i=1 xiwi + 1 2 d i=1 d j=1 xixjQi,j + ,(2)", "formula_coordinates": [1.0, 349.95, 508.84, 205.98, 26.84]}, {"formula_id": "formula_2", "formula_text": "Y = Xw + 1 2 Z \u2022 vec(Q) + ,(3)", "formula_coordinates": [3.0, 117.62, 230.46, 175.29, 19.74]}, {"formula_id": "formula_3", "formula_text": "L (w, Q) = 1 2 Y \u2212 Xw \u2212 1 2 Z \u2022 vec(Q) 2 2 .(4)", "formula_coordinates": [3.0, 90.63, 304.81, 202.29, 24.8]}, {"formula_id": "formula_4", "formula_text": "min x,Q L (w, Q) + \u03bb w 1 + \u03bb 2 Q 1 s.t. Q\u2022,j 1 \u2264 |wj| for j = 1, . . . , d,(5)", "formula_coordinates": [3.0, 94.11, 362.76, 198.8, 34.87]}, {"formula_id": "formula_5", "formula_text": "min w,Q L w + \u2212 w \u2212 , Q + \u03bb1 T (w + + w \u2212 ) + \u03bb 2 Q 1 s.t. Q\u2022,j 1 \u2264 w + j + w \u2212 j w + j \u2265 0 w \u2212 j \u2265 0 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe for j = 1, . . . , d,(6)", "formula_coordinates": [3.0, 69.04, 504.41, 223.87, 66.08]}, {"formula_id": "formula_6", "formula_text": "P = (a, B), a \u2208 R d , B \u2208 R d\u00d7d B\u2022,j 1 \u2264 |aj|, j = 1, . . . , d", "formula_coordinates": [3.0, 316.81, 300.64, 247.3, 10.63]}, {"formula_id": "formula_7", "formula_text": "R(w, Q) = \uf8f1 \uf8f2 \uf8f3 \u03bb w 1 + \u03bb 2 Q 1 , if (w, Q) \u2208 P +\u221e, if (w, Q) / \u2208 P .(7)", "formula_coordinates": [3.0, 330.87, 345.69, 225.06, 31.39]}, {"formula_id": "formula_8", "formula_text": "w (k+1) , Q (k+1) = arg min w, Q L w (k) , Q (k) + \u2207wL w (k) , Q (k) , w \u2212 w (k) + \u2207QL w (k) , Q (k) , Q \u2212 Q (k) + t (k) 2 w \u2212 w (k) 2 2 + t (k) 2 Q \u2212 Q (k) 2 F + R(w, Q),(8)", "formula_coordinates": [3.0, 316.94, 426.87, 238.99, 96.86]}, {"formula_id": "formula_9", "formula_text": "w (k+1) , Q (k+1) = arg min w, Q 1 2 w \u2212 v (k) 2 2 + 1 2 Q \u2212 U (k) 2 2 + 1 t (k) R(w, Q),(9)", "formula_coordinates": [3.0, 322.32, 568.49, 234.0, 53.17]}, {"formula_id": "formula_10", "formula_text": "v (k) =w (k) \u2212 \u2207wL w (k) , Q (k) /t (k) , U (k) =U (k) \u2212 \u2207QL w (k) , Q (k) /t (k) .", "formula_coordinates": [3.0, 361.27, 654.31, 150.2, 30.25]}, {"formula_id": "formula_11", "formula_text": "w, Q 1 2 w \u2212 v 2 2 + 1 2 Q \u2212 U 2 F + \u03bb t w 1 + \u03bb 2t Q 1 s.t. Q\u2022,j 1 \u2264 |wj| for j = 1, . . . , d.(10)", "formula_coordinates": [4.0, 69.17, 73.17, 223.75, 45.91]}, {"formula_id": "formula_12", "formula_text": "sign(w) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if w > 0 \u22121 if w < 0 0 if w = 0 ,(11)", "formula_coordinates": [4.0, 115.09, 207.69, 177.82, 36.49]}, {"formula_id": "formula_13", "formula_text": "w, Q 1 2 w \u2212 v 2 2 + 1 2 Q \u2212 U 2 F + \u03bb t w 1 + \u03bb 2t Q 1 s.t. Q\u2022,j 1 \u2264 |wj| wj = S 0 j,jwj Q\u2022,j = S j Q\u2022,j wj \u2265 0 Q\u2022,j 0 \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe for j = 1, . . . , d,(12)", "formula_coordinates": [4.0, 65.28, 321.65, 227.63, 106.37]}, {"formula_id": "formula_14", "formula_text": "arg mi\u00f1 w j ,S 0 jj , Q \u2022,j ,S j 1 2 S 0 jjwj \u2212 vj 2 2 + 1 2 S j Q\u2022,j \u2212 U\u2022,j 2 2 + \u03bb tw j + \u03bb 2t 1 T Q\u2022,j s.t. 1 T Q\u2022,j \u2264wj Q\u2022,j 0 , for j = 1, . . . , d.(13)", "formula_coordinates": [4.0, 62.7, 642.09, 230.22, 77.81]}, {"formula_id": "formula_15", "formula_text": "1 2 (wj \u2212 vj) 2 = 1 2 S 0 jjwj \u2212 vj 2 = 1 2 S 0 jj S 0 jjwj \u2212 vj 2 = 1 2", "formula_coordinates": [4.0, 350.96, 72.83, 133.18, 41.39]}, {"formula_id": "formula_16", "formula_text": "w j , Q \u2022,j 1 2 wj \u2212\u1e7dj 2 2 + 1 2 Q\u2022,j \u2212 U\u2022,j 2 2 + \u03bb tw j + \u03bb 2t 1 T Q\u2022,j s.t. 1 T Q\u2022,j \u2264wj Q\u2022,j 0 . (14", "formula_coordinates": [4.0, 319.18, 228.42, 235.74, 66.01]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [4.0, 551.84, 286.57, 4.09, 7.86]}, {"formula_id": "formula_18", "formula_text": "mi\u00f1 w j ,Q \u2022,j 1 2 wj \u2212vj 2 2 + 1 2 Q\u2022,j \u2212 q U\u2022,j 2 2 s.t. 1 T Q\u2022,j \u2264wj Q\u2022,j 0 ,(15)", "formula_coordinates": [4.0, 358.17, 318.83, 197.75, 52.53]}, {"formula_id": "formula_19", "formula_text": "l(\u03b3,w, Q) = 1 2 (w \u2212v) 2 + 1 2 Q \u2212 q U 2 2 + \u03b3 1 T Q \u2212w", "formula_coordinates": [4.0, 327.24, 429.25, 212.5, 19.75]}, {"formula_id": "formula_20", "formula_text": "max \u03b3\u22650 mi\u00f1 w,Q 0 1 2 (w \u2212v) 2 + 1 2 Q \u2212 q U 2 2 + \u03b3 1 T Q \u2212w . (16", "formula_coordinates": [4.0, 321.42, 502.48, 230.42, 20.13]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [4.0, 551.84, 508.29, 4.09, 7.86]}, {"formula_id": "formula_22", "formula_text": "max \u03b3\u22650 mi\u00f1 w, Q 0 1 2 (w \u2212 (v + \u03b3)) 2 + 1 2 Q \u2212 q U \u2212 \u03b31 2 2 +h(\u03b3), (17", "formula_coordinates": [4.0, 303.77, 546.66, 248.06, 20.41]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [4.0, 551.84, 552.47, 4.09, 7.86]}, {"formula_id": "formula_24", "formula_text": "where h(\u03b3) = \u2212v\u03b3 \u2212 1 2 \u03b3 2 + \u03b31 T q U \u2212 1 2 \u03b3 2 1 T 1.", "formula_coordinates": [4.0, 316.81, 576.14, 181.28, 12.09]}, {"formula_id": "formula_25", "formula_text": "\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3v + \u03b3 \u2265 0 \u21d2w =v + \u03b3 v + \u03b3 < 0 \u21d2w = 0 q Ui \u2212 \u03b3 \u2265 0 \u21d2 Qi = q Ui \u2212 \u03b3 q Ui \u2212 \u03b3 < 0 \u21d2 Qi = 0 (18", "formula_coordinates": [4.0, 368.76, 614.92, 217.11, 53.7]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [4.0, 551.84, 638.31, 4.09, 7.86]}, {"formula_id": "formula_27", "formula_text": "q U1 \u2264 . . . \u2264 q UL \u2264 \u2212q v \u2264 q UL+1 \u2264 . . . \u2264 q U d .(19)", "formula_coordinates": [5.0, 91.52, 158.27, 201.4, 8.35]}, {"formula_id": "formula_28", "formula_text": "Case 1 : When . . . \u2264 q UG \u2264 \u03b3 \u2264 q UG+1 \u2264 . . . \u2264 \u2212v \u2264 . . ., the objective in (17) at \u03b3 * becomes 1 2 G i=1 q Ui \u2212 \u03b3 2 + 1 2 (v + \u03b3) 2 + h(\u03b3) = 1 2 G i=1 q U 2 i + d i=G+1 \u03b3 q Ui \u2212 d \u2212 G 2 \u03b3 2 + 1 2v 2 .(20)", "formula_coordinates": [5.0, 53.8, 219.95, 239.11, 99.39]}, {"formula_id": "formula_29", "formula_text": "d i=G+1 q U i d\u2212G \u2265 q UG+1. Since \u03b3 falls in the interval q UG, q UG+1 , we set \u03b3 = q UG+1", "formula_coordinates": [5.0, 58.15, 350.28, 234.76, 50.07]}, {"formula_id": "formula_30", "formula_text": "When . . . \u2264 q UL \u2264 \u03b3 \u2264 \u2212v \u2264 q UL+1 \u2264 .", "formula_coordinates": [5.0, 53.8, 473.21, 157.41, 7.86]}, {"formula_id": "formula_31", "formula_text": "1 2 L i=1 q U 2 i + d i=L+1 \u03b3 q Ui \u2212 d \u2212 L 2 \u03b3 2 + 1 2v 2 . (21", "formula_coordinates": [5.0, 94.55, 503.5, 194.27, 26.96]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [5.0, 288.82, 512.66, 4.09, 7.86]}, {"formula_id": "formula_33", "formula_text": "[max (\u2212v, 0) , +\u221e]. Note that when L = d, that is q U d \u2264 \u03b3 \u2264 \u2212v, (21) is a constant 1 2 d i=1 q U 2 i + 1 2v 2", "formula_coordinates": [5.0, 53.8, 574.02, 239.1, 22.31]}, {"formula_id": "formula_34", "formula_text": "U d , \u2212v . Case 3: When . . . \u2264 q UL \u2264 \u2212v \u2264 \u03b3 \u2264 q UL+1 \u2264 . . ., the value of the objective function in (17) at \u03b3 * becomes 1 2 L i=1 q Ui \u2212 \u03b3 2 + h(\u03b3) = 1 2 L i=1 q U 2 i + \u03b3 d i=L+1 q Ui \u2212v \u2212 d + 1 \u2212 L 2 \u03b3 2 . (22", "formula_coordinates": [5.0, 53.8, 600.35, 239.11, 120.28]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [5.0, 288.82, 687.24, 4.09, 7.86]}, {"formula_id": "formula_36", "formula_text": "d i=L+1 q U i \u2212v d+1\u2212L \u2265 \u2212v. If d i=L+1 q U i \u2212v d+1\u2212L \u2265 q UL+1, the maximum is achieved at \u03b3 = q UL+1, otherwise the maximum is achieved at \u03b3 = d i=L+1 q Ui \u2212v d + 1 \u2212 L . Case 4: When . . . \u2264 \u2212v \u2264 . . . \u2264 q UG \u2264 \u03b3 \u2264 q UG+1 \u2264 . . ., the objective value in (17) is similar to (22): 1 2 G i=1 q U 2 i + \u03b3 d i=G+1 q Ui \u2212v \u2212 p + 1 \u2212 G 2 \u03b3 2 . (23", "formula_coordinates": [5.0, 316.81, 53.86, 239.11, 171.5]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [5.0, 551.84, 207.57, 4.09, 7.86]}, {"formula_id": "formula_38", "formula_text": "If d i=G+1 q U i \u2212v d+1\u2212G \u2265 q UG+1, then the maximum is achieved at \u03b3 = q UG+1; If d i=G+1 q U i \u2212v d+1\u2212G \u2264 q UG, then the maximum is achieved at \u03b3 = q UG; If q UG \u2264 d i=G+1 q U i \u2212q v d+1\u2212G \u2264 q UG+1, the maximum is achieved at \u03b3 = d i=G+1 q Ui \u2212v d + 1 \u2212 G .", "formula_coordinates": [5.0, 316.81, 232.94, 237.97, 125.6]}, {"formula_id": "formula_39", "formula_text": "UG\u22121, q UG and q UG, q UG+1 , if d i=G+1 q U i \u2212q v d+1\u2212G \u2265 q UG+1 for [ q UG, q UG+1], then d i=G q U i \u2212q v d+1\u2212(G\u22121) \u2265 q UG for [ q UG\u22121, q UG];", "formula_coordinates": [5.0, 339.23, 528.8, 222.15, 46.47]}, {"formula_id": "formula_40", "formula_text": "In Case 4, if d i=G+1 q U i \u2212v d+1\u2212G \u2265 q UG+1 for q UG, qUG+1", "formula_coordinates": [5.0, 339.23, 592.53, 209.28, 15.81]}, {"formula_id": "formula_41", "formula_text": "U i \u2212q v d+1\u2212(G\u22121) \u2264 q UG\u22121 for q UG\u22121, q UG , then d i=G+1 q U i \u2212q v d+1\u2212G \u2264 q UG for q UG, q UG+1 .", "formula_coordinates": [5.0, 343.58, 688.03, 212.34, 31.64]}, {"formula_id": "formula_42", "formula_text": "In Case 4, if d i=G q U i \u2212q v d+1\u2212(G\u22121) \u2264 q UG\u22121 for q UG\u22121, q", "formula_coordinates": [6.0, 76.21, 66.55, 187.24, 14.98]}, {"formula_id": "formula_43", "formula_text": "In Case 4, if q UG \u2264 d i=G+1 q U i \u2212q v d+1\u2212G \u2264 q UG+1 for q UG, q UG+1 , then d i=G q U i \u2212q v d+1\u2212(G\u22121) \u2265 q UG for q UG\u22121, q UG and d i=G+2 q U i \u2212q v d+1\u2212(G+1) \u2264 q UG+1 for q UG+1, qUG+2", "formula_coordinates": [6.0, 76.21, 135.51, 222.75, 48.97]}, {"formula_id": "formula_44", "formula_text": "(1) if max q U d , \u2212v < 0, then \u03b3 * = 0; (2) if \u2212v > q U d , then \u03b3 * = max(\u2212v, 0); (3) if q UG \u2264 d i=G+1 q U i \u2212v d+1\u2212G \u2264 q UG+1 for a certain interval q UG, q UG+1 , then \u03b3 * = d i=G+1\u01d3 i \u2212v d + 1 \u2212 G .", "formula_coordinates": [6.0, 53.8, 385.79, 239.11, 134.47]}, {"formula_id": "formula_45", "formula_text": "d i=G+1 q U i \u2212v d+1\u2212G", "formula_coordinates": [6.0, 205.64, 526.85, 41.28, 15.81]}, {"formula_id": "formula_46", "formula_text": "Input: v \u2208 R d\u00d71 , U \u2208 R d\u00d7d , t \u2208 R+, \u03bb \u2208 R+ Output: w \u2208 R d\u00d71 , Q \u2208 R d\u00d7d 1:v = sign(v) v \u2212 \u03bb t 1; q U = sign(U ) U \u2212 \u03bb 2t 11", "formula_coordinates": [6.0, 316.81, 77.72, 181.26, 45.8]}, {"formula_id": "formula_47", "formula_text": "Q\u2022,j = max q U\u2022,j \u2212 \u03b3, 0 ; 22:", "formula_coordinates": [6.0, 316.81, 359.72, 134.97, 20.9]}, {"formula_id": "formula_48", "formula_text": "Input: X \u2208 R n\u00d7d , Z \u2208 R n\u00d7(d\u2022d) , \u03bb \u2208 R+, \u03b7 > 1 Output: w \u2208 R d\u00d71 , Q \u2208 R d\u00d7d", "formula_coordinates": [7.0, 53.8, 80.3, 193.18, 20.59]}, {"formula_id": "formula_49", "formula_text": "repeat v (k) = w (k) \u2212 \u2207wL w (k) , Q (k) /t (k) ; U (k) = U (k) \u2212 \u2207QL w (k) , Q (k) /t (k) ;", "formula_coordinates": [7.0, 78.66, 134.34, 161.96, 38.07]}, {"formula_id": "formula_50", "formula_text": "t (k) \u2190 \u03b7t (k) ; 6:", "formula_coordinates": [7.0, 57.69, 212.17, 79.31, 20.09]}, {"formula_id": "formula_51", "formula_text": "Z (i) \u2208 R n\u00d7d , Z (i) \u2022,j = X\u2022,i X\u2022,j.", "formula_coordinates": [7.0, 53.8, 377.66, 239.11, 27.33]}], "doi": "10.1145/2623330.2623665"}