{"title": "Generalizing Backpropagation for Gradient-Based Interpretability", "authors": "Kevin Du; D Lucas; Torroba Hennigen; Niklas Stoehr; Alexander Warstadt; Ryan Cotterell; D Eth; Z\u00fcrich @ Mit", "pub_date": "", "abstract": "Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.", "sections": [{"heading": "Introduction 1", "text": "One of the key contributors to the success of deep learning in NLP has been backpropagation (Linnainmaa, 1976), a dynamic programming algorithm that efficiently computes the gradients of a scalar function with respect to its inputs (Goodfellow et al., 2016). Backpropagation works by constructing a directed acyclic computation graph 2 that describes a function as a composition of various primitive operations, e.g., +, \u00d7, and exp(\u2022), whose gradients are known, and subsequently traversing this graph in topological order to incrementally compute the gradients. Since the runtime of backpropagation is linear in the number of edges of the computation graph, it is possible to quickly perform vast numbers of gradient descent steps in even the most gargantuan of neural networks.\nWhile gradients are arguably most important for training, they can also be used to analyze and interpret neural network behavior. For example, feature attribution methods such as saliency maps (Simonyan et al., 2013) and integrated gradients (Sundararajan et al., 2017) exploit gradients to identify which features of an input contribute most towards the model's prediction. However, most of these methods provide little insight into how the gradient propagates through the computation graph, and those that do are computationally inefficient, e.g., Lu et al. (2021) give an algorithm for computing the highest-weighted gradient path that runs in exponential time.\nIn this paper, we explore whether examining various quantities computed from the gradient graph of a network, i.e., the weighted graph whose edge weights correspond to the local gradient between two nodes, can lead to more insightful and granular analyses of network behavior than the gradient itself. To do so, we note that backpropagation is an instance of a shortest-path problem (Mohri, 2002) over the (+, \u00d7) semiring. This insight allows us to generalize backpropagation to other semirings, allowing us to compute statistics about the gradient graph beyond just the gradient, all while retaining backpropagation's linear time complexity. 3 In our experiments, the first semiring we consider is the max-product semiring, which allows us to identify paths in the computation graph which carry most of the gradient, akin to Lu et al.'s (2021) influence paths. The second is the entropy semiring (Eisner, 2002), 4 which summarizes how dispersed the gradient graph is, i.e., whether the gradient flows in a relatively focalized manner through a small proportion of possible paths or in a widely distributed manner across most paths in the network. With experiments on synthetic data, we validate that the max-product semiring results in higher values for model components we expect to be more critical to the model's predictions, based on the design of the Transformer (Vaswani et al., 2017) architecture. We further apply our framework to analyze the behavior of BERT (Devlin et al., 2019) in a subject-verb agreement task (SVA; Linzen et al., 2016). In these experiments, we find that the keys matrix for subject tokens carries most of the gradient through the last layer of the self-attention mechanism. Our results suggest that semiringlifted gradient graphs can be a versatile tool in the interpretability researcher's toolbox.", "publication_ref": ["b18", "b9", "b29", "b31", "b22", "b24", "b7", "b33", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Gradient-based interpretability", "text": "Neural networks are often viewed as black boxes because their inner workings are too complicated for a user to understand why the model produced a particular prediction for a given input. This shortcoming has spawned an active field of research in developing methods to better understand and explain how neural networks work. For example, feature attribution methods aim to measure the sensitivity of a model's predictions to the values of individual input features. Many of these methods quantify feature attribution as the gradient of the model's output with respect to an input feature (Simonyan et al., 2013;Smilkov et al., 2017;Sundararajan et al., 2017). We note that while the general reliability and faithfulness of gradient-based methods has been a contentious area of research (Adebayo et al., 2018;Yona and Greenfeld, 2021;Amorim et al., 2023), gradient-based methods have nonetheless continued to be widely used (Han et al., 2020;Supekar et al., 2022;Novakovsky et al., 2022).\nOther works have applied feature attribution methods to not only highlight sensitive input features but also uncover important internal neurons. Leino et al. (2018) define influence as the gradient of a quantity of interest with respect to a neuron, averaged across a collection of inputs of interest. Lu et al. (2020) further define and analyze the notion of influence paths, i.e., paths in the computation graph between the neuron of interest and the output that on average carry most of the gradient. By applying this method to analyze the behavior of Gu-lordava et al.'s (2018) LSTM language model on the SVA task, they draw conclusions about which internal components of the LSTM are most sensitive to the concept of number agreement based on the paths with the greatest amount of influence.\nHowever, Lu et al.'s (2020) method exhaustively enumerates all paths in the computation graph and ranks them by the amount of influence along each one. As the number of paths in a computation graph is usually exponential in the depth of a neural network, this quickly becomes intractable for larger networks (Lu et al., 2021). Therefore, this method is limited to computing influence paths for networks with very small numbers of paths. Indeed, while Lu et al. (2020) computed the influence along 40 000 paths for a 2-layer LSTM, follow-up work that attempted to apply this method to BERT had to use an approximation which might not find the correct paths (Lu et al., 2021). The method we propose does not exhibit this issue and scales to any network one can train using backpropagation.", "publication_ref": ["b29", "b30", "b31", "b0", "b37", "b1", "b13", "b32", "b25", "b16", "b21", "b22", "b21", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Generalizing backpropagation", "text": "In this section, we build toward our generalization of backpropagation as a semiring-weighted dynamic program. At a high level, we observe that if we replace the addition and multiplication operations in the typical backpropagation algorithm with similar operations that satisfy the necessary properties, then the resulting algorithm will compute other useful statistics about the network's gradient graph in the same runtime as backpropagation. In the remainder of this section, we make this notion of swapping operations precise by formulating backpropagation as a semiring algorithm, and later in \u00a74 we describe how different semirings yield different, useful, views of the gradient graph.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computation graphs", "text": "Many classes of functions, e.g., machine learning models, can be expressed as compositions of differentiable functions. Such functions can described by a computation graph (Goodfellow et al., 2016). A computation graph is an ordered 5 directed acyclic graph (DAG) where every node is associated with the application of a primitive operation, e.g., +, \u00d7, and exp(\u2022), to the parents of that node. These primitives all share the property that their gradients have\nx 0 = x x 1 = y x 2 = exp(x 0 ) x 3 = x 0 \u2212 x 1 x 4 = x 1 \u00d7 x 3 x 5 = x 2 + x 4 x 0 x 1 x 2 x 3 x 4 x 5 (a) Computation graph of f (x, y) x 0 x 1 x 2 x 3 x 4 x 5 e x 1 \u22121 y x \u2212 y 1 1 (b) Gradient graph of f (x, y)\nFigure 1: Comparison of the computation graph (Fig. 1a) and gradient graph (Fig. 1b) of f (x, y) = e x + (x \u2212 y)y.\nThe expressions and primitives associated with each node in the graphs are shown on the left. Notice that the gradient graph is a labeled variant of the computation graph. Input nodes are shown in blue, and output nodes in red.\na closed form and are assumed to be computable in constant time for the sake of analysis. Source nodes in the graph are called input nodes, and every computation graph has a designated output node that encapsulates the result of the function. 6 An example computation graph is shown in Fig. 1a. If all input nodes are assigned a value, then one can perform a forward pass, which calculates the value of the function at those inputs by traversing the graph in a topological order, 7 evaluating the values of each node until we reach the output node. This procedure is shown in Algorithm 1. ", "publication_ref": ["b9"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "3:", "text": "\u25b7 D is an ordered dictionary mapping from nodes to their values, with D[vi] initialized to the input value associated with vi\u2200i \u2208 [1, . . . , m].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "\u25b7 \u03c4 : (V, V ) \u2192 N is a function that maps a parent node to the index of the argument list of a function corresponding to a node. That is, given a node v and parent node u, \u03c4 maps to an index in {1, . . . , |\u03c0(v)|} for all v \u2208 V, u \u2208 \u03c0(v). 5:\nfor k = m + 1, . . . , N : 6: (a k ) \u03c4 (v k ,u) \u2190 (D[u]) u\u2208\u03c0(v k ) \u25b7 Retrieve the value\nfor each input u and store in the ordered argument tuple a k 7:\nD[v k ] \u2190 f k (a k ) 8: return D", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Backpropagation", "text": "Encoding a function as a computation graph is useful because it enables the efficient compu- 6 For simplicity, we only consider scalar-valued functions, but extensions to vector-valued functions are possible and indeed commonplace in the literature. 7 A topological ordering of a DAG is an ordering of its nodes such that node i precedes node j iff i is not a child of j. tation of its gradients via automatic differentiation (Griewank and Walther, 2008). Let G be a computation graph with topologically sorted nodes v 1 , . . . , v N , where v N is its output node. The goal of automatic differentiation is to compute dv N dv i for some node v i in G. Bauer (1974) shows that dv N dv i can be expressed as:\ndv N dv i = p\u2208P(i,N ) (j,k)\u2208p dv k dv j(1)\nwhere P(i, N ) denotes the set of Bauer pathsdirected paths in the computation graph G from node v i to node v N . 8 That is, the gradient of the output v N with respect to a node v i equals the sum of the gradient computed along every path between v i and v N , where the gradient along a path is the product of the gradient assigned to each edge along that path. The gradient of each edge is easy to compute, as it corresponds to the gradient of a primitive. To distinguish the original, unweighted computation graph from its gradient-weighted counterpart, we call the latter the gradient graph G(\u2022) of a function; an example is shown in Fig. 1b. Note that this is a function of the input nodes, since the edge gradients are dependent on the input nodes.\nIn general, na\u00efvely computing Eq. (1) term by term is intractable since P(i, N ) can be exponential in the number of nodes in the computation graph. By leveraging the distributivity of multiplication over addition, backpropagation 9 uses dynamic programming and the caching of intermediate values from the forward pass to compute Eq. (1) in O(|E|) time, where |E| is the number of edges in G (Goodfellow et al., 2016, p. 206). Backpropagation can be seen as traversing the computation graph in reverse topological order and computing the gradient of the output node with respect to each intermediate node until v i is reached. 10", "publication_ref": ["b11", "b3"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Semiring backpropagation", "text": "The crucial observation at the core of this paper is that backpropagation need not limit itself to addition and multiplication: If, instead, we replace those operations with other binary operators that also exhibit distributivity, say \u2295 and \u2297, then this new algorithm would compute:\n\u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v i \u225c p\u2208P(i,N ) (j,k)\u2208p dv k dv j (2)\nClearly, the interpretation of this resulting quantity depends on how \u2295 and \u2297 are defined. We discuss different options in \u00a74, and in the remainder of this section we focus on how \u2295 and \u2297 have to behave to make them suitable candidates for replacement.\nTo make this notion more rigorous, we first need to introduce the notion of a semiring. Definition 3.1. A semiring (over a set K) is an algebraic structure (K, \u2295, \u2297,0,1) such that:\n1. \u2295: K\u00d7K \u2192 K is a commutative and associative operation with identity element0; 2. \u2297: K\u00d7K \u2192 K is an associative operation with identity element1; 3. \u2297 distributes over \u2295; 4.0 is an annihilator, i.e., for any k \u2208 K, k\u22970 = 0 =0 \u2297 k.\nIf we replace the operations and identity elements in backpropagation according to the semiring identities and operations, we obtain semiring backpropagation, shown in Algorithm 2. Regular backprogation amounts to a special case of the algorithm when run on the sum-product semiring (R, +, \u00d7, 0, 1). Aggregated derivative. Eq. (2) defines\n\u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v i for a single node v i .\nHowever, often it is useful to aggregate this quantity across a set of nodes. For example, when a token is embedded into Algorithm 2 Semiring backpropagation This algorithm is executed after the forward pass of a computation graph.\n1: def Backpropagation(G, D):", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2:", "text": "\u25b7 G is a computation graph with topologicallysorted nodes V = [v1, ..., vN ].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3:", "text": "\u25b7 D is an ordered dictionary mapping from node vi to its value, \u2200i \u2208 [1, ..., m], computed by the forward pass 4:\nfor v \u2208 V : 5: B[v] \u21900 \u25b7 B is a dictionary mapping from v i to \u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v i 6: B[v N ] \u21901 7: for i = N, ..., 1 : 8: for u in \u03c0(v i ) : 9: B[u] \u2190 B[u] \u2295 dv i du D[u] \u2297 B[v i ] 10: return B\nFor standard backpropagation, let \u2295 be the addition (+) operator and \u2297 be the times (\u00d7) operator. a d-dimensional vector, each of its dimensions corresponds to a node in the computation graph, say\nV = {v 1 , . . . , v d }. Then, \u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v j\nfor the j th component of the representation does not capture the semiring-derivative with respect to the entire representation of the token. Hence, we define the aggregated derivative with respect to a set of nodes V as: 11\n\u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) V \u225c v\u2208V \u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v(3)\n4 Interpreting semiring gradients\nIn \u00a73, we showed how to generalize backpropagation to the semiring case. For any semiring of our choosing, this modified algorithm will compute a different statistic associated with a function's gradient. We begin by motivating the standard (+, \u00d7) semiring which is common in the interpretability literature, before discussing the implementation and interpretation of the max-product and entropy semirings we focus on in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "What is a (+, \u00d7) gradient?", "text": "We start by reviewing the gradient interpretation in the (+, \u00d7) semiring, which corresponds to the standard definition of the gradient. We explain why and how the gradient can be useful for interpretability. Let f : R D \u2192 R be a function differentiable at y \u2208 R D (e.g., a neural network model). The derivative of f at y, \u2207f (y), can be interpreted as the best linear approximation of the function at y (Rudin, 1976), viz., for any unit vector v \u2208 R D and scalar \u03f5 > 0, we have:\nf (y + \u03f5v) = f (y) + \u2207f (y) \u22a4 (\u03f5v) + o(\u03f5) (4)\nAs such, one can view gradients as answering counterfactual questions: If we moved our input y in the direction v for some small distance \u03f5, what is our best guess (relying only on a local, linear approximation of the function) about how the output of the model would change? 12 Gradient-based methods (as discussed in \u00a72) are useful to interpretability precisely because of this counterfactual interpretation. In using gradients for interpretability, researchers typically implicitly consider v = e i , i.e., the i th natural basis vector, which approximates the output if we increment the model's i th input feature by one. We can then interpret the coordinates of the gradient as follows: If its i th coordinate is close to zero, then we can be reasonably confident that small changes to that specific coordinate of the input should have little influence on the value of f . However, if the gradient's i th coordinate is large in magnitude (whether positive or negative), then we may conclude that small changes in the i th coordinate of the input should have a large influence on the value of f . The subsequent two sections address a shortcoming in exclusively inspecting the gradient, which is fundamentally an aggregate quantity that sums over all individual Bauer paths. This means, however, that any information about the structure of that path is left out, e.g., whether a few paths' contributions dominate the others. The semiring gradients that we introduce in the sequel offer different angles of interpretation of such counterfactual statements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "What is a (max, \u00d7) gradient?", "text": "While the (+, \u00d7) gradient has a natural interpretation given by calculus and has been used in many prior works (Simonyan et al., 2013;Bach et al., 2015;Sundararajan et al., 2017) to identify input features that are most sensitive to a model's output, it cannot tell us how the gradient flows through a gradient graph, as discussed in \u00a74.1. One way to compute a different quantity is to change the semiring. The max-product semiring (R\u222a{\u2212\u221e, \u221e}, max, \u00d7, \u2212\u221e, 1) is an enticing candidate: In contrast to the (+, \u00d7) semiring, computing the gradient with respect to the (max, \u00d7) semiring can help illuminate which components of the network are most sensitive or critical to the model's input. The (max, \u00d7) gradient specifically computes the gradient along the Bauer path that has the highest value. We term this path the top gradient path in the sequel. Formally, the (max, \u00d7) gradient between v i and v N is:\n\u2138 (max,\u00d7) v N \u2138 (max,\u00d7) v i \u225c max p\u2208P(i,N ) (j,k)\u2208p dv k dv j (5)\nNote that variants of this definition are possible, e.g., we could have considered the absolute values of the gradients dv k dv j if we did not care about the overall impact as opposed to the most positive impact on the output v N .\nThe top gradient path can be used to examine branching points in a model's computation graph. For example, in Transformer (Vaswani et al., 2017) models, the input to an attention layer branches when it passes through both the self-attention mechanism and a skip connection. The input further branches within the self-attention mechanism between the keys, values, and queries (see Fig. 3 for an illustration). By examining the top gradient path at this branching point, we can identify not only whether the skip connection or self-attention mechanism is more critical to determining input sensitivity, but also which component within the self-attention mechanism itself (keys, queries, or values) carries the most importance. Implementation. By using the max-product semiring in the backpropagation algorithm, we can compute the top gradient path in O(|E|) time, where |E| is the number of edges in the computation graph (Goodfellow et al., 2016, p. 206). See App. A for more details.", "publication_ref": ["b29", "b2", "b31", "b33"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "What is an entropy gradient?", "text": "In addition to identifying the single top gradient path, it is also helpful to have a more holistic view of the gradient paths in a graph. In particular, we may be interested in the path entropy of the gradient graph, i.e., the dispersion of the magnitudes of the path weights. Formally, for an input y and its corresponding gradient graph G(y) with nodes v 1 , . . . , v N , the entropy of all paths between v i and v N is defined as:\n\u2138 Ent v N \u2138 Ent v i \u225c \u2212 p\u2208P(i,N ) g(p) Z log g(p) Z (6\n)\nwhere g(p) \u225c (j,k)\u2208p dv k dv j is the gradient of path p and Z = p\u2208P(i,N ) |g(p)| is a normalizing factor.\nIntuitively, under this view, the gradient graph G(\u2022) encodes an (unnormalized) probability distribution over paths between v i and v N where the probability of a given path is proportional to the absolute value of the product of the gradients along each edge. The entropy then describes the dispersion of the gradient's flow through all the possible paths in the graph from v i to v N . For a given graph, the entropy is greatest when the gradient flows uniformly through all possible paths, and least when it flows through a single path.\nImplementation. Eisner (2002) proposed to efficiently compute the entropy of a graph by lifting the graph's edge weights into the expectation semiring (R \u00d7 R, \u2295, \u2297,0,1) where0 = \u27e80, 0\u27e9, 1 = \u27e81, 0\u27e9 and:\n\u2022 \u2295: \u27e8a, b\u27e9 \u2295 \u27e8c, d\u27e9 = \u27e8a + c, b + d\u27e9 \u2022 \u2297: \u27e8a, b\u27e9 \u2297 \u27e8c, d\u27e9 = \u27e8ac, ad + bc\u27e9\nTo leverage the expectation semiring, we first lift the weight of each edge in the gradient graph from w to \u27e8|w| , |w| log |w|\u27e9 (where w is the local derivative between two connected nodes in the gradient graph). Then, by computing:\n\u27e8Z, \u2212 p\u2208P(i,N ) |g(p)| log |g(p)|\u27e9 (7) = p\u2208P(i,N ) (j,k)\u2208p dv k dv j , \u2212 dv k dv j log dv k dv j\nin linear time using Algorithm 2, we obtain \u27e8Z, p\u2208P(i,N ) |g(p)| log |g(p)|\u27e9, which are the normalizing factor and the unnormalized entropy of the graph, respectively. As shown by Li and Eisner (2009), we can then compute\n\u2138 Ent v N \u2138 Ent v i = log Z \u2212 1 Z p\u2208P(i,N ) |g(p)| log |g(p)|.", "publication_ref": ["b7", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To demonstrate the utility of semiring backpropogation, we empirically analyze their behavior on two simple transformer models (1-2 layers) on well-controlled, synthetic tasks. We also explore semiring backpropogation on a larger model, BERT (Devlin et al., 2019), on the popular analysis task of subject-verb agreement to understand how our method can be useful for interpreting language models in more typical settings.\nTo implement semiring backpropagation, we developed our own Python-based reverse-mode automatic differentiation library, building off of the pedagogical library Brunoflow (Ritchie, 2020) and translating it into JAX (Bradbury et al., 2018). 13 5.1 Validation on a synthetic task Setup. In this experiment, we test the hypothesis that most of the gradient should flow through the components that we judge a priori to be most critical to the model's predictions. We are particularly interested in whether the gradient flow through a Transformer matches our expectation of the selfattention mechanism's components. So, while we compute the top gradient path from the output to the input representations, we only inspect the top path at a Transformer's main branching point, which is when the hidden state is passed into the skip connection and the keys, values, and queries of the selfattention mechanism (Fig. 3). If we observe higher levels of gradients flowing through one branch, a natural interpretation is that this component is more critical for the model's prediction. To test whether this interpretation is justified, we construct a task where we can clearly reason about how a welltrained Transformer model ought to behave and identify how well the top gradient flow aligns with our expectations of a model's critical component.  Model. We use a 1-layer Transformer model with hidden layer size of 16 and 2 attention heads to minimize branching points and increase interpretability. We train this model to achieve 100% validation accuracy on the task described below.\nTask. We design the FirstTokenRepeatedOnce task to target the utility of this method for interpreting the self-attention mechanism. In this task, an input consists of a sequence of numbers, which is labeled according to whether the first token appears again at any point in the sequence, e.g., [ 1 , 4, 6, 1 ] \u2192 True, whereas [ 3 , 4, 6, 2] \u2192 False. Furthermore, the inputs are constrained such that the first token will be repeated at most once, to isolate the decision-making of the model to the presence (or lack thereof) of a single token. We randomly generate a dataset of 10 000 points with sequence length 10 and vocab size 20. The correct decision-making process for this task entails comparing the first token to all others in the sequence and returning True if there is a match. This is, in fact, analogous to how queries and keys function within the self-attention mechanism: A query q t is compared to the key k t \u2032 of each token t \u2032 in the sequence and the greater the match, the greater attention paid to token t \u2032 by query token t.\nWe would therefore expect that the self-attention mechanism relies heavily on the query representation of the first token and key representations of the remaining tokens and, in particular, the key representation of the repeated token, if present. In turn, we hypothesize the max-product gradient value will primarily originate from the queries branch for the first token and keys for the remaining tokens, and be especially high for the repeat token.\nResults. The results, summarized in Fig. 2, provide strong evidence for our hypothesis that the behavior of the (max, \u00d7) gradient reflects the importance of the different model components. We observe all expected gradient behaviors described in the previous paragraph, and especially that the highest gradient flow (for any token) is through the keys of the repeat token.", "publication_ref": ["b6", "b27"], "figure_ref": ["fig_3", "fig_1"], "table_ref": []}, {"heading": "Top gradient path of BERT for subject-verb agreement", "text": "Setup. We now apply this method to understand the self-attention mechanism of a larger model (BERT) for the more complex NLP task of SVA.\nWe subsample 1000 examples from the dataset from Linzen et al. (2016) and use spaCy (Matthew et al., 2020) to identify the subject and attractors within each sentence. We then filter down to 670 sentences after removing sentences where BERT tokenizes the subject or attractors as multiple tokens.\nUsing the max-product semiring, we then compute the top gradient path through the different branches (skip connection, keys, values, and queries) for (a) the subject of a sentence, (b) the attractors of a sentence, and (c) all tokens of a sentence.\nModel. BERT (Devlin et al., 2019) is a popular encoder-only Transformer model for many NLP tasks. BERT's architecture consists of multiple Transformer encoder layers stacked atop each other, along with a task-specific head. We use the google/bert_uncased_L-6_H-512_A-8 pretrained model from Huggingface (Wolf et al., 2020), which has 6 attention layers, hidden size of 512, and 8 attention heads.\nTask. We consider the subject-verb number agreement task in our experiments. Variants of this task in English have become popular case studies in neural network probing. Notably, this phenomenon has been used to evaluate the ability for models to learn hierarchical syntactic phenomena (Linzen et al., 2016;Gulordava et al., 2018). It has also served as a testing ground for interpretability studies which have found evidence of individual hidden units that track number and nested dependencies (Lakretz et al., 2019), and that removing individual hidden units or subspaces from the models' representation space have a targeted impact on model predictions (Finlayson et al., 2021;Lasri et al., 2022). Our formulation of the task uses BERT's native masked language modeling capability by recasting it as a cloze task: We mask a verb in the ", "publication_ref": ["b19", "b23", "b6", "b36", "b19", "b12", "b14", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Preceding attractors", "text": "Gradient flow through keys, queries, values, and skip connection per layer Figure 4: Each plot depicts the top gradient path behavior of BERT for different tokens in a sentence, averaged across the 670 sentences. Notably, for subjects (middle plot), the skip connection contains the top gradient path for all layers except the final layer, which is consistent with findings from (Lu et al., 2021). In the final layer, most of the gradient for both the subject and attractors flows through the keys of the self-attention mechanism. This differs from the gradient flow averaged across all tokens, indicating this behavior is specific to the nouns of the sentence.\nsentence and compare the probabilities with which BERT predicts the verb forms with correct and incorrect number marking. For example, given the input \"all the other albums produced by this band [MASK] their own article,\" we compare the probabilities of \"have\" (correct) and \"has\" (incorrect). We compute the gradient with respect to the difference between the log probability of the two inflections.\nThe data for this experiment is from Linzen et al. (2016). All the examples in their dataset also include one or more attractors. These are nouns such as \"band\" in the example above, which (a) are not the subject, (b) precede the verb, and (c) disagree with the subject in number. Furthermore, all masked verbs are third person and present tense, to ensure that number agreement is non-trivial.\nResults. From Fig. 4, we highlight key differences between the (max, \u00d7) gradient behavior for subject tokens and all tokens in general. Most saliently, for subject tokens only, the max-product gradient flows entirely through the self-attention mechanism in the last layer and mostly through the skip connection in earlier layers, which is consistent with findings from Lu et al. (2021). Moreover, within the self-attention mechanism, most (76%) of the gradient in the last layer for the subject flows through the keys matrix. In contrast, across all tokens, the top gradient paths mostly through the skip connection for all layers, and otherwise is more evenly distributed between keys and values.\nWe also note similarities and differences between the gradient flows of the subject and preceding attractors. Both exhibit a similar trend in which the gradient flows primarily through the keys (and entirely through the self-attention mechanism) in the last layer. However, the top gradient has a greater magnitude for the subject than the attractors (especially in the keys). Since self-attention uses a token's keys to compute the relative importance of that token to the [MASK] token, we speculate that the max-product gradient concentrating primarily on the keys (and more so for the subject than attractors) reflects that a successful attention mechanism relies on properly weighting the importances of the subject and attractors.", "publication_ref": ["b22", "b19", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Gradient graph entropy vs. task difficulty", "text": "Setup. This experiment tests the hypothesis that the entropy of a model's gradient graph is positively correlated with the difficulty of the task that the model was trained to solve. We construct a variety of synthetic tasks and compare the average gradient entropy of a 2-layer transformer on examples in each of these tasks. We measure the difficulty of a task with the minimum description length (MDL; Rissanen, 1978). 14 Following the approach used by Lovering et al. (2021) and Voita and Titov (2020), we measure MDL by repeatedly training the model on the task with increasing quantities of data and summing the loss from each segment. The higher the MDL, the more difficulty the model had in extracting the labels from the dataset, and therefore the more challenging the task. We hypothesize that a model will have higher entropy for more difficult tasks because it will require using more paths in its computation graph. During our analysis, we drop runs where the model was unable to achieve a validation accuracy of > 90%, to avoid confounding results with models unable to learn the task.\nModel. For all tasks, we use the same 2-layer transformer architecture with a hidden layer size of 64, 4 attention heads, and always predicts a distribution over 36 classes (with some possibly unused); this ensures our results are comparable across tasks with different numbers of classes. We train the models for 50 epochs on each of the synthetic datasets.\nTask. We design a variety of synthetic tasks in order to control for difficulty more directly. In the ContainsTokenSet family of tasks, an input is a sequence of S numbers and labeled True or False based on whether the input contains all tokens in a pre-specified token set. Different tasks within ContainsTokenSet are defined by the prespecified token set. The BinCountOnes family of tasks is parameterized by a number of classes C. In this task, an input x is a sequence of S numbers. The label y is determined by the number of 1s in the sequence according to the following function: y(x) = Count1(x)\nS/C \u2212 1, i.e., in the 2class instance of BinCountOnes, an input is labeled 0 if it contains \u2264 S/2 1s and 1 if it contains > S/2 1s. Finally, we also evaluate on the synthetic datasets Contains1, AdjacentDuplicate, FirstTokenRepeatedImmediately, and First-TokenRepeatedLast from (Lovering et al., 2021). For more details, see App. C.\nResults. The results show clear evidence against our initial hypothesis that gradient entropy increases as a function of task difficulty, as measured by MDL. While there appears to be some patterns evident between entropy and MDL in Fig. 5, their interpretation is unclear. From observing the lightest-hued points there appears to be a negative linear relationship between entropy and MDL for the binary tasks. However, confusingly, the points seem to suggest a quadratic-like relationship between entropy and MDL for the BinCountOnes tasks. We speculate that this could be explained by a phase-change phenomena in the model's learning dynamics. That is, for sufficiently easy tasks, the model need not focalize much in order to solve the task. Incrementally more difficult tasks may require the model to focalize more, thus resulting in the decreasing entropy for tasks below a certain MDL threshold. Then, once a task is sufficiently difficult, the model is required to use more of the network to solve the task. Therefore, we see this increase in entropy as the MDL increases past a certain threshold for the BinCountOnes task. The presence of these clear (although somewhat mystifying) patterns indicates that there exists some relationship between entropy and MDL. More experimentation is needed to understand the relationship between entropy and MDL for task difficulty.", "publication_ref": ["b26", "b20", "b34", "b20"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusion", "text": "We presented a semiring generalization of the backpropagation algorithm, which allows us to obtain an alternative view into the inner workings of a neural network. We then introduced two semirings, the max-product and entropy semirings, which provide information about the branching points of a neural network and the dispersion of the gradient graph. We find that gradient flow reflects model component importance, gradients flowing through the self-attention mechanism for the subject token pass primarily through the keys matrix, and the entropy has some relationship with the difficulty of learning a task. Future work will consider semirings outside the scope of this work, e.g., the top-k semiring (Goodman, 1999) to track the top-k gradient paths, as well as computing semirings online for control during training.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "While our approach inherits the linear runtime complexity of the backpropagation algorithm, runtime concerns should not be fully neglected. Firstly, the linear runtime is only an analytical result, not an empirical measure. This means that the actual runtime of the backpropagation and thus our algorithm depend heavily on their implementation. For instance, some deep learning frameworks do a better job at reusing and parallelizing computations than others (Goodfellow et al., 2016). Indeed, our code is optimized for good readability and extensibility at the expense of speed, which hints at another limitation of our approach: Our approach requires deep integration with the framework as it needs access to all model weights and the computation graph. For this reason, our approach cannot be easily packaged and wrapped around any existing model or framework and we instead developed our own JAX-based reverse-mode autodifferentiation library, based on the numpy-based Brunoflow library (Ritchie, 2020). While we release our library to enable other researchers to analyze models through their gradient graphs, it faces some computational and memory constraints. In our experiments, running the three semirings together on a single sentence can take several minutes (depending on sentence length) using google/bert_un-cased_L-6_H-512_A-8, the 6-layered pretrained BERT from Huggingface (Wolf et al., 2020), totaling our experimentation time on our datasets at about 10 CPU-hours. For improved adoption of this method, we encourage the direct integration of semiring implementations into the most popular deep learning frameworks. Our final point pertains not only to our study but to most interpretability approaches: One has to be careful when drawing conclusions from gradient paths. Cognitive biases, wrong expectations, and omitted confounds may lead to misinterpretation of results.", "publication_ref": ["b9", "b27", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics statement", "text": "We foresee no ethical concerns with this work. Our work aims to make the inner workings of neural network models more interpretable. On this account, we hope to contribute to reducing biases inherent in model architectures, pre-trained model weights, and tasks by increasing overall transparency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Entropy vs Example Difficulty in Subject-Verb Agreement", "text": "Setup. We investigate the relationship between the entropy of the gradient graph of BERT and input sentences in the task of subject-verb number agreement. In this task, we measure example difficulty by the number of attractors in a sentence (more attractors corresponds to greater difficulty). We sub-sample the dataset from Linzen et al. (2016) to 1000 sentences, balanced evenly by the number of attractors per sentence (ranging from 1 to 4 attractors). Then, using the entropy semiring, we compute the entropy of BERT's gradient graph for each sentence.\nResults. Since sentences with more tokens will naturally have a higher entropy due to a larger computation graph (see Fig. 6a), we control by sentence length. We bin sentences of similar length for (10-20, 20-30, 30-40, and 40-50 tokens) before analyzing the effect that the number of attractors has on entropy. We present the results in Fig. 7 and additionally run a Spearman correlation test between the entropy of the input representations (averaged across all tokens in the sentence) and the number of attractors. For each group of sentence lengths, we find minimal correlation between number of attractors and entropy. Therefore, there is little evidence to support a relationship between entropy and example difficulty as measured by number of attractors. However, number of attractors is not necessarily a strong indicator of example difficulty, and recommend more rigorous comparison of entropy against a stronger metric of example difficulty in future work.  ", "publication_ref": ["b19"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "C.2 BinCountOnes Datasets", "text": "We construct one family of multiclass classification datasets, BinCountOnes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameterization.", "text": "A BinCountOnes task is parameterized by the number of classes C, between 2 to S, such that C divides S. For example, when S = 6, C could be 3. We foresee potential risks of our work, as our work aims at making models more interpretable. We hope to contribute to reducing biases inherent in model architectures, pre-trained model weights and tasks by increasing overall transparency.\nA3. Do the abstract and introduction summarize the paper's main claims?\nLeft blank. B2. Did you discuss the license or terms for use and / or distribution of any artifacts? No response.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nNo response.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\nNo response.\nC Did you run computational experiments?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5", "text": "C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 8 C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? 5 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? 5\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? 5 D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\nNo response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Implementation of Top Gradient Path", "text": "In practice, we implement the top gradient path by storing 4 additional fields to each node in the graph: the most positive gradient of the node, a pointer to the child node which contributed this most positive gradient, the most negative gradient of the node, and a pointer to the child node which contributed this most negative gradient. In this way, each node tracks the paths containing the most positive gradient (top_pos) and most negative gradient (top_neg) from itself to the output node. To dynamically extend the path from v k to v j (j < k):", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Additional Entropy Sanity Checks and Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Sanity Checks with Synthetic Data", "text": "To build intuition about the entropy of a model's computation graph, we run two sanity check experiments. First, we evaluate the entropy of a pretrained BERT model as the sentence length increases. Since larger sentence lengths result in more paths in the computation graph, we expect the entropy of the model to increase with sentence length. Our findings confirm this (Fig. 6a).\nSecond, we expect that the entropy of a trained model ought to increase with the model complexity, as measured by hidden size. In this experiment, we create a 4-featured artificial dataset with randomly generated values in the range [0, 1], labeled by whether the first feature is greater than 0.5. We train multilayer perceptrons with varying hidden sizes on this dataset and find that the entropy of the input features increases with model complexity as expected (see Fig. 6b). Figure 6: Fig. 6a shows that the more tokens a sentence contains, the more gradient paths are naturally involved and consequently the higher the overall entropy. Fig. 6b shows that as the hidden size of the MLP increases, so too does the model entropy. By comparing the entropy of the relevant feature line and the irrelevant features line, it also appears that the entropy is consistently higher for the relevant feature than irrelevant features, especially as model complexity increases. Labeled True if X contains every token in T and False otherwise [1,3,4,2,5,2] [1,5,9,2,2,4] Contains1 N/A Labeled True if X contains the token 1 and False otherwise [1,3,4,2,5,2] [6,5,9,2,2,4] FirstToken-RepeatedImmediately N/A Labeled True if the first two tokens in X are the same and False otherwise 3,3,2,6,7,8] [5,3,2,6,7,8] FirstToken-RepeatedLast N/A Labeled True if the first and last tokens in X are the same and False otherwise 8,3,2,6,7,8] [8,3,2,6,7,4] AdjacentDuplicate N/A Labeled True if two adjacent tokens in X are the same at any point in the sequence and False otherwise [1,3,6,6,7,8] [1,3,6,8,7,8] FirstToken-RepeatedOnce N/A Labeled True if the first token in X is repeated at any point in the sequence and False otherwise. X is further constrained to have at most one repeat of the first token in X. [1,3,6,1,7,8] [1,3,6,7,7,8]  Description. Each example X is labeled between [0, C \u2212 1] by the following formula: label(X) = Count1(X) S/C \u2212 1, where CCount1(X) is the number of 1s that appear in X.\nExamples. See Tab. 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input Label", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sanity checks for saliency maps", "journal": "Curran Associates, Inc", "year": "2018", "authors": "Julius Adebayo; Justin Gilmer; Michael Muelly; Ian Goodfellow; Moritz Hardt; Been Kim"}, {"ref_id": "b1", "title": "Evaluating the faithfulness of saliency maps in explaining deep learning models using realistic perturbations", "journal": "Information Processing & Management", "year": "2023", "authors": "P Jos\u00e9; Pedro H Amorim; Jo\u00e3o Abreu; Marc Santos; Victor Cortes;  Vila"}, {"ref_id": "b2", "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "journal": "PLoS ONE", "year": "2015", "authors": "Sebastian Bach; Alexander Binder; Gr\u00e9goire Montavon; Frederick Klauschen; Klaus-Robert M\u00fcller; Wojciech Samek"}, {"ref_id": "b3", "title": "Computational graphs and rounding error", "journal": "SIAM Journal on Numerical Analysis", "year": "1974", "authors": "Friedrich L Bauer"}, {"ref_id": "b4", "title": "JAX: Composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "James Bradbury; Roy Frostig; Peter Hawkins; Matthew James Johnson; Chris Leary; Dougal Maclaurin; George Necula; Adam Paszke; Jake Vanderplas; Skye Wanderman-Milne; Qiao Zhang"}, {"ref_id": "b5", "title": "Probabilistic context-free grammars (PCFGs)", "journal": "", "year": "2013", "authors": "Michael Collins"}, {"ref_id": "b6", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b7", "title": "Parameter estimation for probabilistic finite-state transducers", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Jason Eisner"}, {"ref_id": "b8", "title": "Tal Linzen, and Yonatan Belinkov. 2021. for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Matthew Finlayson; Aaron Mueller; Sebastian Gehrmann; Stuart Shieber"}, {"ref_id": "b9", "title": "Deep Learning", "journal": "MIT press", "year": "2016", "authors": "Ian Goodfellow; Yoshua Bengio; Aaron Courville"}, {"ref_id": "b10", "title": "Semiring parsing", "journal": "Computational Linguistics", "year": "1999", "authors": "Joshua Goodman"}, {"ref_id": "b11", "title": "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation", "journal": "", "year": "2008", "authors": "Andreas Griewank; Andrea Walther"}, {"ref_id": "b12", "title": "Colorless green recurrent networks dream hierarchically", "journal": "Long Papers", "year": "2018", "authors": "Kristina Gulordava; Piotr Bojanowski; Edouard Grave; Tal Linzen; Marco Baroni"}, {"ref_id": "b13", "title": "Explaining black box predictions and unveiling data artifacts through influence functions", "journal": "", "year": "2020", "authors": "Xiaochuang Han; Byron C Wallace; Yulia Tsvetkov"}, {"ref_id": "b14", "title": "The emergence of number and syntax units in LSTM language models", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yair Lakretz; German Kruszewski; Theo Desbordes; Dieuwke Hupkes; Stanislas Dehaene; Marco Baroni"}, {"ref_id": "b15", "title": "Probing for the usage of grammatical number", "journal": "Long Papers", "year": "2022", "authors": "Karim Lasri; Tiago Pimentel; Alessandro Lenci; Thierry Poibeau; Ryan Cotterell"}, {"ref_id": "b16", "title": "Influence-directed explanations for deep convolutional networks", "journal": "IEEE", "year": "2018", "authors": "Klas Leino; Shayak Sen; Anupam Datta"}, {"ref_id": "b17", "title": "First-and secondorder expectation semirings with applications to minimum-risk training on translation forests", "journal": "", "year": "2009", "authors": "Zhifei Li; Jason Eisner"}, {"ref_id": "b18", "title": "Taylor expansion of the accumulated rounding error", "journal": "BIT", "year": "1976", "authors": "Seppo Linnainmaa"}, {"ref_id": "b19", "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Tal Linzen; Emmanuel Dupoux; Yoav Goldberg"}, {"ref_id": "b20", "title": "Predicting inductive biases of finetuned models", "journal": "", "year": "2021", "authors": "Charles Lovering; Rohan Jha; Tal Linzen; Ellie Pavlick"}, {"ref_id": "b21", "title": "Influence paths for characterizing subject-verb number agreement in LSTM language models", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Kaiji Lu; Piotr Mardziel; Klas Leino; Matt Fredrikson; Anupam Datta"}, {"ref_id": "b22", "title": "Influence patterns for explaining information flow in BERT", "journal": "Curran Associates, Inc", "year": "2021", "authors": "Kaiji Lu; Zifan Wang; Piotr Mardziel; Anupam Datta"}, {"ref_id": "b23", "title": "spaCy: Industrialstrength Natural Language Processing in Python", "journal": "", "year": "2020", "authors": "Honnibal Matthew; Montani Ines;  Van Landeghem Sofie; Adriane Boyd"}, {"ref_id": "b24", "title": "Semiring frameworks and algorithms for shortest-distance problems", "journal": "Journal of Automata, Languages, and Combinatorics", "year": "2002", "authors": "Mehryar Mohri"}, {"ref_id": "b25", "title": "Obtaining genetics insights from deep learning via explainable artificial intelligence", "journal": "Nature Reviews Genetics", "year": "2022", "authors": "Gherman Novakovsky; Nick Dexter; Maxwell W Libbrecht; Wyeth W Wasserman; Sara Mostafavi"}, {"ref_id": "b26", "title": "Modeling by shortest data description", "journal": "Automatica", "year": "1978", "authors": "Jorma Rissanen"}, {"ref_id": "b27", "title": "Brunoflow: A pedagogical deep learning framework", "journal": "", "year": "2020", "authors": "Daniel Ritchie"}, {"ref_id": "b28", "title": "Principles of Mathematical Analysis. International series in pure and applied mathematics", "journal": "McGraw-Hill", "year": "1976", "authors": " Walter Rudin"}, {"ref_id": "b29", "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "journal": "", "year": "2013", "authors": "Karen Simonyan; Andrea Vedaldi; Andrew Zisserman"}, {"ref_id": "b30", "title": "Smoothgrad: Removing noise by adding noise", "journal": "", "year": "2017", "authors": "Daniel Smilkov; Nikhil Thorat; Been Kim; Fernanda Vi\u00e9gas; Martin Wattenberg"}, {"ref_id": "b31", "title": "Axiomatic attribution for deep networks", "journal": "", "year": "2017", "authors": "Mukund Sundararajan; Ankur Taly; Qiqi Yan"}, {"ref_id": "b32", "title": "Deep learning identifies robust gender differences in functional brain organization and their dissociable links to clinical symptoms in autism", "journal": "The British Journal of Psychiatry", "year": "2022", "authors": "Kaustubh Supekar; Carlo De Los Angeles; Srikanth Ryali; Kaidi Cao; Tengyu Ma; Vinod Menon"}, {"ref_id": "b33", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b34", "title": "Information-theoretic probing with minimum description length", "journal": "", "year": "2020", "authors": "Elena Voita; Ivan Titov"}, {"ref_id": "b35", "title": "Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning", "journal": "", "year": "2008", "authors": "J Martin; Michael I Wainwright;  Jordan"}, {"ref_id": "b36", "title": "Transformers: State-of-the-Art Natural Language Processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"ref_id": "b37", "title": "Revisiting sanity checks for saliency maps. arXiv", "journal": "", "year": "2021", "authors": "Gal Yona; Daniel Greenfeld"}, {"ref_id": "b38", "title": "Table 2: Example inputs and labels for the BinCountOnes task where sequence length S = 6 and number of classes C = 3", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Algorithm 11Forward-propagation 1: def Forwardpropagation(G, D, \u03c4 ): 2: \u25b7 G is a computation graph with topologicallysorted nodes V = [v1, . . . , vN ].", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: High gradient flow through the queries for the first token and keys of the repeat token match the expected important components of self-attention for each token type, respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure3: Simplified computation graph for the i th layer of a Transformer(Vaswani et al., 2017) encoder at the key branching point where a hidden state is passed into the skip connection and the keys, values, and queries of the self-attention mechanism. We use h i to denote the hidden representations returned by layer i \u2212 1, and use \". . .\" to denote others parts of the computation graph.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Each point represents the entropy and MDL of a model trained on a given dataset (3 seeds per dataset). We denote the BinCountOnes datasets with the marker and other tasks (ContainsTokenSet and tasks from Lovering et al., 2021) with the \u2022 marker. The hue corresponds to the number of classes of the task; the lightest hue indicates a binary problem while the darker hues indicate more classes (max of 36).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure7: For all sentence length bins, there appears to be little to no correlation between number of attractors and entropy.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "you describe the limitations of your work? 8 A2. Did you discuss any potential risks of your work?", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "A4. Have you used AI writing assistants when working on this paper?", "figure_data": "Left blank.B Did you use or create scientific artifacts?Left blank.B1. Did you cite the creators of artifacts you used?No response."}], "formulas": [{"formula_id": "formula_0", "formula_text": "x 0 = x x 1 = y x 2 = exp(x 0 ) x 3 = x 0 \u2212 x 1 x 4 = x 1 \u00d7 x 3 x 5 = x 2 + x 4 x 0 x 1 x 2 x 3 x 4 x 5 (a) Computation graph of f (x, y) x 0 x 1 x 2 x 3 x 4 x 5 e x 1 \u22121 y x \u2212 y 1 1 (b) Gradient graph of f (x, y)", "formula_coordinates": [3.0, 88.12, 75.08, 420.86, 121.89]}, {"formula_id": "formula_1", "formula_text": "for k = m + 1, . . . , N : 6: (a k ) \u03c4 (v k ,u) \u2190 (D[u]) u\u2208\u03c0(v k ) \u25b7 Retrieve the value", "formula_coordinates": [3.0, 76.97, 571.3, 212.16, 34.5]}, {"formula_id": "formula_2", "formula_text": "D[v k ] \u2190 f k (a k ) 8: return D", "formula_coordinates": [3.0, 76.97, 613.97, 104.11, 28.45]}, {"formula_id": "formula_3", "formula_text": "dv N dv i = p\u2208P(i,N ) (j,k)\u2208p dv k dv j(1)", "formula_coordinates": [3.0, 355.67, 361.88, 169.49, 31.35]}, {"formula_id": "formula_4", "formula_text": "\u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v i \u225c p\u2208P(i,N ) (j,k)\u2208p dv k dv j (2)", "formula_coordinates": [4.0, 108.47, 256.02, 181.4, 34.29]}, {"formula_id": "formula_5", "formula_text": "\u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v i for a single node v i .", "formula_coordinates": [4.0, 70.86, 645.31, 215.94, 32.96]}, {"formula_id": "formula_6", "formula_text": "for v \u2208 V : 5: B[v] \u21900 \u25b7 B is a dictionary mapping from v i to \u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v i 6: B[v N ] \u21901 7: for i = N, ..., 1 : 8: for u in \u03c0(v i ) : 9: B[u] \u2190 B[u] \u2295 dv i du D[u] \u2297 B[v i ] 10: return B", "formula_coordinates": [4.0, 307.78, 186.25, 214.41, 117.68]}, {"formula_id": "formula_7", "formula_text": "V = {v 1 , . . . , v d }. Then, \u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v j", "formula_coordinates": [4.0, 323.99, 394.23, 150.68, 24.42]}, {"formula_id": "formula_8", "formula_text": "\u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) V \u225c v\u2208V \u2138 (\u2295,\u2297) v N \u2138 (\u2295,\u2297) v(3)", "formula_coordinates": [4.0, 354.42, 490.47, 170.73, 36.46]}, {"formula_id": "formula_9", "formula_text": "f (y + \u03f5v) = f (y) + \u2207f (y) \u22a4 (\u03f5v) + o(\u03f5) (4)", "formula_coordinates": [5.0, 81.56, 190.81, 208.31, 21.19]}, {"formula_id": "formula_10", "formula_text": "\u2138 (max,\u00d7) v N \u2138 (max,\u00d7) v i \u225c max p\u2208P(i,N ) (j,k)\u2208p dv k dv j (5)", "formula_coordinates": [5.0, 339.17, 256.59, 185.98, 34.29]}, {"formula_id": "formula_11", "formula_text": "\u2138 Ent v N \u2138 Ent v i \u225c \u2212 p\u2208P(i,N ) g(p) Z log g(p) Z (6", "formula_coordinates": [6.0, 98.77, 120.95, 186.85, 33.21]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [6.0, 285.63, 128.28, 4.24, 13.15]}, {"formula_id": "formula_13", "formula_text": "\u2022 \u2295: \u27e8a, b\u27e9 \u2295 \u27e8c, d\u27e9 = \u27e8a + c, b + d\u27e9 \u2022 \u2297: \u27e8a, b\u27e9 \u2297 \u27e8c, d\u27e9 = \u27e8ac, ad + bc\u27e9", "formula_coordinates": [6.0, 83.88, 429.43, 159.95, 35.31]}, {"formula_id": "formula_14", "formula_text": "\u27e8Z, \u2212 p\u2208P(i,N ) |g(p)| log |g(p)|\u27e9 (7) = p\u2208P(i,N ) (j,k)\u2208p dv k dv j , \u2212 dv k dv j log dv k dv j", "formula_coordinates": [6.0, 74.79, 546.66, 215.08, 62.55]}, {"formula_id": "formula_15", "formula_text": "\u2138 Ent v N \u2138 Ent v i = log Z \u2212 1 Z p\u2208P(i,N ) |g(p)| log |g(p)|.", "formula_coordinates": [6.0, 70.86, 671.8, 218.28, 37.93]}], "doi": "10.1016/j.ipm.2022.103225"}