{"title": "Efficient Inference for Distributions on Permutations", "authors": "Jonathan Huang; Carlos Guestrin; Leonidas Guibas", "pub_date": "", "abstract": "Permutations are ubiquitous in many real world problems, such as voting, rankings and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact representations such as graphical models cannot efficiently capture the mutual exclusivity constraints associated with permutations. In this paper, we use the \"low-frequency\" terms of a Fourier decomposition to represent such distributions compactly. We present Kronecker conditioning, a general and efficient approach for maintaining these distributions directly in the Fourier domain. Low order Fourier-based approximations can lead to functions that do not correspond to valid distributions. To address this problem, we present an efficient quadratic program defined directly in the Fourier domain to project the approximation onto a relaxed form of the marginal polytope. We demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting.", "sections": [{"heading": "Introduction", "text": "Permutations arise naturally in a variety of real situations such as card games, data association problems, ranking analysis, etc. As an example, consider a sensor network that tracks the positions of n people, but can only gather identity information when they walk near certain sensors. Such mixed-modality sensor networks are an attractive alternative to exclusively using sensors which can measure identity because they are potentially cheaper, easier to deploy, and less intrusive. See [1] for a real deployment. A typical tracking system maintains tracks of n people and the identity of the person corresponding to each track. What makes the problem difficult is that identities can be confused when tracks cross in what we call mixing events. Maintaining accurate track-to-identity assignments in the face of these ambiguities based on identity measurements is known as the Identity Management Problem [2], and is known to be N P -hard. Permutations pose a challenge for probabilistic inference, because distributions on the group of permutations on n elements require storing at least n! \u2212 1 numbers, which quickly becomes infeasible as n increases. Furthermore, typical compact representations, such as graphical models, cannot capture the mutual exclusivity constraints associated with permutations.\nDiaconis [3] proposes maintaining a small subset of Fourier coefficients of the actual distribution allowing for a principled tradeoff between accuracy and complexity. Schumitsch et al. [4] use similar ideas to maintain a particular subset of Fourier coefficients of the log probability distribution. Kondor et al. [5] allow for general sets of coefficients, but assume a restrictive form of the observation model in order to exploit an efficient FFT factorization. The main contributions of this paper are:\n\u2022 A new, simple and general algorithm, Kronecker Conditioning, which performs all probabilistic inference operations completely in the Fourier domain. Our approach is general, in the sense that it can address any transition model or likelihood function that can be represented in the Fourier domain, such as those used in previous work, and can represent the probability distribution with any desired set of Fourier coefficients. \u2022 We show that approximate conditioning can sometimes yield Fourier coefficients which do not correspond to any valid distribution, and present a method for projecting the result back onto a relaxation of the marginal polytope. \u2022 We demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting.", "publication_ref": ["b0", "b1", "b2", "b3", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Filtering over permutations", "text": "In identity management, a permutation \u03c3 represents a joint assignment of identities to internal tracks, with \u03c3(i) being the track belonging to the ith identity. When people walk too closely together, their identities can be confused, leading to uncertainty over \u03c3. To model this uncertainty, we use a Hidden Markov Model on permutations, which is a joint distribution over P (\u03c3 (1) , . . . , \u03c3 (T ) , z (1) , . . . , z (T ) ) which factors as: 1) , . . . , \u03c3 (T ) , z (1) , . . . , z (T ) ) = P (z (1) |\u03c3 (1) )\nP (\u03c3(\nY t P (z t |\u03c3 (t) ) \u2022 P (\u03c3 (t) |\u03c3 (t\u22121) ),\nwhere the \u03c3 (t) are latent permutations and the z (t) denote observed variables. The conditional probability distribution P (\u03c3 (t) |\u03c3 (t\u22121) ) is called the transition model, and might reflect for example, that the identities belonging to two tracks were swapped with some probability. The distribution P (z (t) |\u03c3 (t) ) is called the observation model, which might capture a distribution over the color of clothing for each individual.\nWe focus on filtering, in which one queries the HMM for the posterior at some timestep, conditioned on all past observations. Given the distribution P (\u03c3 (t) |z (1) , . . . , z (t) ), we recursively compute P (\u03c3 (t+1) |z (1) , . . . , z (t+1) ) in two steps: a prediction/rollup step and a conditioning step. The first updates the distribution by multiplying by the transition model and marginalizing out the previous timestep: P (\u03c3 (t+1) |z (1) , . . . , z (t) ) = \u03c3 (t) P (\u03c3 (t+1) |\u03c3 (t) )P (\u03c3 (t) |z (1) , . . . , z (t) ). The second conditions the distribution on an observation z (t+1) using Bayes rule: P (\u03c3 (t+1) |z (1) , . . . , z (t+1) ) \u221d P (z (t+1) |\u03c3 (t+1) )P (\u03c3 (t+1) |z (1) , . . . , z (t) ). Since there are n! permutations, a single update requires O((n!) 2 ) flops and is consequently intractable for all but very small n. The approach that we advocate is to maintain a compact approximation to the true distribution based on the Fourier transform. As we discuss later, the Fourier based approximation is equivalent to maintaining a set of low-order marginals, rather than the full joint, which we regard as being analagous to an Assumed Density Filter [6].", "publication_ref": ["b0", "b0", "b0", "b0", "b0", "b0", "b0", "b0", "b0", "b0", "b0", "b0", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Fourier projections of functions on the Symmetric Group", "text": "Over the last 50 years, the Fourier Transform has been ubiquitously applied to everything digital, particularly with the invention of the Fast Fourier Transform. On the real line, the Fourier Transform is a well-studied method for decomposing a function into a sum of sine and cosine terms over a spectrum of frequencies. Perhaps less familiar, is its group theoretic generalization, which we review in this section with an eye towards approximating functions on the group of permutations, the Symmetric Group. For permutations on n objects, the Symmetric Group will be abbreviated by S n . The formal definition of the Fourier Transform relies on the theory of group representations, which we briefly discuss first. Our goal in this section is to motivate the idea that the Fourier transform of a distribution P is related to certain marginals of P . For references on this subject, see [3]. Definition 1. A representation of a group G is a map \u03c1 from G to a set of invertible d \u03c1 \u00d7 d \u03c1 matrix operators which preserves algebraic structure in the sense that for all \u03c3 1 , \u03c3 2 \u2208 G, \u03c1(\u03c3 1 \u03c3 2 ) = \u03c1(\u03c3 1 ) \u2022 \u03c1(\u03c3 2 ). The matrices which lie in the image of this map are called the representation matrices, and we will refer to d \u03c1 as the degree of the representation.\nRepresentations play the role of basis functions, similar to that of sinusoids, in Fourier theory. The simplest basis functions are constant functions -and our first example of a representation is the trivial representation \u03c1 0 : G \u2192 R which maps every element of G to 1. As a more pertinent example, we define the 1st order permutation representation of S n to be the degree n representation, \u03c4 1 , which maps a permutation \u03c3 to its corresponding permutation matrix given by: [\u03c4 1 (\u03c3)] ij = 1 {\u03c3(j) = i}. For example, the permutation in S 3 which swaps the second and third elements maps to:\n\u03c41(1 \u2192 1, 2 \u2192 3, 3 \u2192 2) = 0 @ 1 0 0 0 0 1 0 1 0 1 A .\nThe \u03c4 1 representation can be thought of as a collection of n 2 functions at once, one for each matrix entry, [\u03c4 1 (\u03c3)] ij . There are other possible permutation representations -for example the 2nd order unordered permutation representation, \u03c4 2 , is defined by the action of a permutation on unordered pairs of objects, ([\u03c1(\u03c3)] {i,j},{\u2113,k} = 1 {\u03c3({\u2113, k}) = {i, j}}), and is a degree n(n\u22121) 2 representation. And the list goes on to include many more complicated representations.\nIt is useful to think of two representations as being the same if the representation matrices are equal up to some consistent change of basis. This idea is formalized by declaring two representations \u03c1 and \u03c4 to be equivalent if there exists an invertible matrix C such that C \u22121 \u2022 \u03c1(\u03c3) \u2022 C = \u03c4 (\u03c3) for all \u03c3 \u2208 G. We write this as \u03c1 \u2261 \u03c4 .\nMost representations can be seen as having been built up by smaller representations. We say that a representation \u03c1 is reducible if there exist smaller representations \u03c1 1 , \u03c1 2 such that \u03c1 \u2261 \u03c1 1 \u2295 \u03c1 2 where \u2295 is defined to be the direct sum representation:\n\u03c11 \u2295 \u03c12(g) \" \u03c11(g) 0 0 \u03c12(g) \u00ab .(1)\nIn general, there are infinitely many inequivalent representations. However, for any finite group, there is always a finite collection of atomic representations which can be used to build up any other representation using direct sums. These representations are referred to as the irreducibles of a group, and they are simply the collection of representations which are not reducible. We will refer to the set of irreducibles by R. It can be shown that any representation of a finite group G is equivalent to a direct sum of irreducibles [3], and hence, for any representation \u03c4 , there exists a matrices C for which\nC \u22121 \u2022 \u03c4 \u2022 C = \u2295 \u03c1i\u2208R \u2295 \u03c1 i ,\nwhere the inner \u2295 refers to some finite number of copies of the irreducible \u03c1 i .\nDescribing the irreducibles of S n up to equivalence is a subject unto itself; We will simply say that there is a natural way to order the irreducibles of S n that corresponds to 'simplicity' in the same way that low frequency sinusoids are simpler than higher frequency ones. We will refer to the irreducibles in this order as \u03c1 0 , \u03c1 1 , . . . . For example, the first two irreducibles form the first order permutation representation (\u03c4 1 \u2261 \u03c1 0 \u2295 \u03c1 1 ), and the second order permutation representation can be formed by the first 3 irreducibles.\nIrreducible representation matrices are not always orthogonal, but they can always be chosen to be so (up to equivalence). For notational convenience, the irreducible representations in this paper will always be assumed to be orthogonal.", "publication_ref": ["b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "The Fourier transform", "text": "On the real line, the Fourier Transform corresponds to computing inner products of a function with sines and cosines at varying frequencies. The analogous definition for finite groups replaces the sinusoids by group representations. Definition 2. Let f : G \u2192 R be any function on a group G and let \u03c1 be any representation on G.\nThe Fourier Transform of f at the representation \u03c1 is defined to be:f \u03c1 = \u03c3 f (\u03c3)\u03c1(\u03c3).\nThere are two important points which distinguish this Fourier Transform from the familiar version on the real line -it is matrix-valued, and instead of real numbers, the inputs tof are representations of G. The collection of Fourier Transforms of f at all irreducibles form the Fourier Transform of f . As in the familiar case, there is an inverse transform given by:\nf (\u03c3) = 1 |G| X k d\u03c1 k Tr hf T \u03c1 k \u2022 \u03c1 k (\u03c3) i ,(2)\nwhere k indexes over the collection of irreducibles of G.\nWe provide two examples for intuition. For functions on the real line, the Fourier Transform at zero gives the DC component of a signal. This is also true for functions on a group; If f : G \u2192 R is any function, then the Fourier Transform of f at the trivial representation is constant wit\u0125 f \u03c10 = \u03c3 f (\u03c3). Thus, for any probability distribution P , we haveP \u03c10 = 1. If P were the uniform distribution, thenP \u03c1 = 0 at all irreducibles except at the trivial representation.\nThe Fourier Transform at \u03c4 1 also has a simple interpretation:\n[f\u03c4 1 ]ij = X \u03c3\u2208Sn f (\u03c3)[\u03c41(\u03c3)]ij = X \u03c3\u2208Sn f (\u03c3)1 {\u03c3(j) = i} = X \u03c3:\u03c3(j)=i f (\u03c3).\nThus, if P is a distribution, thenP \u03c41 is a matrix of marginal probabilties, where the ij-th element is the marginal probability that a random permutation drawn from P maps element j to i. Similarly, the Fourier transform of P at the second order permutation representation is a matrix of marginal probabilities of the form P (\u03c3({i, j}) = {k, \u2113}).\nIn Section 5, we will discuss function approximation by bandlimiting the Fourier coefficients, but this example should illustrate the fact that maintaining Fourier coefficients at low-order irreducibles is the same as maintaining low-order marginal probabilities, while higher order irreducibles correspond to more complicated marginals.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inference in the Fourier domain", "text": "Bandlimiting allows for compactly storing a distribution over permutations, but the idea is rather moot if it becomes necessary to transform back to the primal domain each time an inference operation is called. Naively, the Fourier Transform on S n scales as O((n!) 2 ), and even the fastest Fast Fourier Transforms for functions on S n are no faster than O(n! log(n!)) (see [7] for example).\nTo resolve this issue, we present a formulation of inference which operates solely in the Fourier domain, allowing us to avoid a costly transform. We begin by discussing exact inference in the Fourier domain, which is no more tractable than the original problem because there are n! Fourier coefficients, but it will allow us to discuss the bandlimiting approximation in the next section. There are two operations to consider: prediction/rollup, and conditioning. The assumption for the rest of this section is that the Fourier Transforms of the transition and observation models are known. We discuss methods for obtaining the models in Section 7.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Fourier prediction/rollup", "text": "We will consider one particular type of transition model -that of a random walk over a group. This model assumes that \u03c3 (t+1) is generated from \u03c3 (t) by drawing a random permutation \u03c4 (t) from some distribution Q (t) and setting \u03c3 (t+1) = \u03c4 (t) \u03c3 (t) . In our identity management example, \u03c4 (t) represents a random identity permutation that might occur among tracks when they get close to each other (a mixing event), but the random walk model appears in other applications such as modeling card shuffles [3]. The Fourier domain Prediction/Rollup step is easily formulated using the convolution theorem (see also [3]): Proposition 3. Let Q and P be probability distributions on S n . Define the convolution of Q and P to be the function\n[Q * P ] (\u03c3 1 ) = \u03c32 Q(\u03c3 1 \u2022 \u03c3 \u22121 2 )P (\u03c3 2 )\n. Then for any representation \u03c1, Q * P \u03c1 = Q \u03c1 \u2022 P \u03c1 , where the operation on the right side is matrix multiplication.\nThe Prediction/Rollup step for the random walk transition model can be written as a convolution:\nP (\u03c3 (t+1) ) = X {(\u03c3 (t) ,\u03c4 (t) ) : \u03c3 (t+1) =\u03c4 (t) \u2022\u03c3 (t) } Q (t) (\u03c4 (t) )\u2022P (\u03c3 (t) ) = X \u03c3 (t) Q (t) (\u03c3 (t+1) \u2022(\u03c3 (t) ) \u22121 )P (\u03c3 (t) ) = h Q (t) * P i (\u03c3 (t+1) ).\nThen assuming that P (t) \u03c1 and Q (t) \u03c1 are given, the prediction/rollup update rule is simply:\nP (t+1) \u03c1 \u2190 Q (t) \u03c1 \u2022 P (t)\n\u03c1 . Note that the update requires only knowledge ofP and does not require P . Furthermore, the update is pointwise in the Fourier domain in the sense that the coefficients at the representation \u03c1 affect P (t+1) \u03c1 only at \u03c1.", "publication_ref": ["b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Fourier conditioning", "text": "An application of Bayes rule to find a posterior distribution P (\u03c3|z) after observing some evidence z requires a pointwise product of likelihood L(z|\u03c3) and prior P (\u03c3), followed by a normalization step. We showed earlier that the normalization constant \u03c3 L(z|\u03c3) \u2022 P (\u03c3) is given by the Fourier transform of L (t) P (t) at the trivial representation -and therefore the normalization step of conditioning can be implemented by simply dividing each Fourier coefficient by the scalar L (t) P (t)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u03c10", "text": ".\nThe pointwise product of two functions f and g, however, is trickier to formulate in the Fourier domain. For functions on the real line, the pointwise product of functions can be implemented by convolving the Fourier coefficients off and\u011d, and so a natural question is: can we apply a similar operation for functions over other groups? Our answer to this is that there is an analogous (but more complicated) notion of convolution in the Fourier domain of a general finite group. We present a convolution-based conditioning algorithm which we call Kronecker Conditioning, which, in contrast to the pointwise nature of the Fourier Domain prediction/rollup step, and much like convolution, smears the information at an irreducible \u03c1 k to other irreducibles.\nFourier transforming the pointwise product Our approach to Fourier Transforming the pointwise product in terms off and\u011d is to manipulate the function f (\u03c3)g(\u03c3) so that it can be seen as the result of an inverse Fourier Transform. Hence, the goal will be to find matrices A k (as a function of f ,\u011d) such that for any \u03c3 \u2208 G,\nf (\u03c3) \u2022 g(\u03c3) = 1 |G| X k d\u03c1 k Tr \" A T k \u2022 \u03c1 k (\u03c3) \" ,(3)\nwhere\nA k = f g \u03c1 k .\nFor any \u03c3 \u2208 G we can write the pointwise product in termsf and\u011d using the inverse Fourier Transform (Equation 2):\nf (\u03c3) \u2022 g(\u03c3) = \" 1 |G| X i d\u03c1 i Tr \"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" # \u2022 \" 1 |G| X j d\u03c1 j Tr \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \" # = \" 1 |G| \u00ab 2 X i,j d\u03c1 i d\u03c1 j h Tr \"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" \u2022 Tr \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \"i .(4)\nNow we want to manipulate this product of traces in the last line to be just one trace (as in Equation 3), by appealing to some properties of the matrix Kronecker product. The connection to the pointwise product (first observed in [8]), lies in the property that for any matrices U, V , Tr (U \u2297 V ) = (Tr U ) \u2022 (Tr V ). Applying this to Equation 4, we have:\nTr \"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" \u2022 Tr \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \" = Tr \"\"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" \u2297 \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \"\" = Tr \" \"f \u03c1 i \u2297\u011d\u03c1 j \" T \u2022 (\u03c1i(\u03c3) \u2297 \u03c1j(\u03c3)) \u00ab ,(5)\nwhere the last line follows by standard matrix properties. The term on the right, \u03c1 i (\u03c3) \u2297 \u03c1 j (\u03c3), itself happens to be a representation, called the Kronecker Product Representation. In general, the Kronecker Product representation is reducible, and so it can decomposed into a direct sum of irreducibles. This means that if \u03c1 i and \u03c1 j are any two irreducibles of G, there exists a similarity transform C ij such that for any \u03c3 \u2208 G,\nC \u22121 ij \u2022 [\u03c1i \u2297 \u03c1j] (\u03c3) \u2022 Cij = M k z ijk M \u2113=1 \u03c1 k (\u03c3).\nThe \u2295 symbols here refer to a matrix direct sum as in Equation 1, k indexes over all irreducible representations of S n , while \u2113 indexes over a number of copies of \u03c1 k which appear in the decomposition. We index blocks on the right side of this equation by pairs of indices (k, \u2113). The number of copies of each \u03c1 k is denoted by the integer z ijk , the collection of which, taken over all triples (i, j, k), are commonly referred to as the Clebsch-Gordan series. Note that we allow the z ijk to be zero, in which case \u03c1 k does not contribute to the direct sum. The matrices C ij are known as the Clebsch-Gordan coefficients. The Kronecker Product Decomposition problem is that of finding the irreducible components of the Kronecker product representation, and thus to find the Clebsch-Gordan series/coefficients for each pair of representations (\u03c1 i , \u03c1 j ). Decomposing the Kronecker product inside Equation 5 using the Clebsch-Gordan series/coefficients yields the desired Fourier Transform, which we summarize here: Proposition 4. Letf ,\u011d be the Fourier Transforms of functions f and g respectively, and for each ordered pair of irreducibles (\u03c1 i , \u03c1 j ), define the matrix:\nA ij C \u22121 ij \u2022 f \u03c1i \u2297\u011d \u03c1j \u2022 C ij .\nThen the Fourier tranform of the pointwise product f g is:\nh c f g i \u03c1 k = 1 d\u03c1 k |G| X ij d\u03c1 i d\u03c1 j z ijk X \u2113=1 A k\u2113 ij ,(6)\nwhere\nA k\u2113 ij is the block of A ij corresponding to the (k, \u2113) block in \u2295 k \u2295 z ijk \u2113 \u03c1 k .\nSee the Appendix for a full proof of Proposition 4. The Clebsch-Gordan series, z ijk , plays an important role in Equation 6, which says that the (\u03c1 i , \u03c1 j ) crossterm contributes to the pointwise product at \u03c1 k only when z ijk > 0. For example,\n\u03c1 1 \u2297 \u03c1 1 \u2261 \u03c1 0 \u2295 \u03c1 1 \u2295 \u03c1 2 \u2295 \u03c1 3 . (7\n)\nSo z 1,1,k = 1 for k \u2264 3 and is zero otherwise.\nUnfortunately, there are no analytical formulas for finding the Clebsch-Gordan series or coefficients, and in practice, these computations can take a long time. We emphasize however, that as fundamental quantities, like the digits of \u03c0, they need only be computed once and stored in a table for future reference. Due to space limitations, we will not provide complete details on computing these numbers. We refer the reader to Murnaghan [9], who provides general formulas for computing Clebsch-Gordan series for pairs of low-order irreducibles, and to Appendix 1 for details about computing Clebsch-Gordan coefficients. We will also make precomputed coefficients available on the web.", "publication_ref": ["b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Approximate inference by bandlimiting", "text": "We approximate the probability distribution P (\u03c3) by fixing a bandlimit B and maintaining the Fourier transform of P only at irreducibles \u03c1 0 , . . . \u03c1 B . We refer to this set of irreducibles as B. As on the real line, smooth functions are generally well approximated by only a few Fourier coefficients, while \"wigglier\" functions require more. For example, when B = 3, B is the set \u03c1 0 , \u03c1 1 , \u03c1 2 , and \u03c1 3 , which corresponds to maintaining marginal probabilities of the form P (\u03c3((i, j)) = (k, \u2113)).\nDuring inference, we follow the procedure outlined in the previous section but ignore the higher order terms which are not maintained. Pseudocode for bandlimited prediction/rollup and Kronecker conditioning is given in Figures 1 and 2.\nSince the Prediction/Rollup step is pointwise in the Fourier domain, the update is exact for the maintained irreducibles because higher order irreducibles cannot affect those below the bandlimit.\nAs in [5], we find that the error from bandlimiting creeps in through the conditioning step. For example, Equation 7shows that if B = 1 (so that we maintain first-order marginals), then the pointwise product spreads information to second-order marginals. Conversely, pairs of higher-order irreducibles may propagate information to lower-order irreducibles. If a distribution is diffuse, then most of the energy is stored in low-order Fourier coefficients anyway, and so this is not a big problem. However, it is when the distribution is sharply concentrated at a small subset of permutations, that the low-order Fourier projection is unable to faithfully approximate the distribution, in many circumstances, resulting in a bandlimited Fourier Transform with negative \"marginal probabilities\"! To combat this problem, we present a method for enforcing nonnnegativity.\nProjecting to a relaxed marginal polytope The marginal polytope, M, is the set of marginals which are consistent with some joint distribution over permutations. We project our approximation onto a relaxation of the marginal polytope, M \u2032 , defined by linear inequality constraints that marginals be nonnegative, and linear equality constraints that they correspond to some legal Fourier transform. Intuitively, our relaxation produces matrices of marginals which are doubly stochastic (rows and columns sum to one and all entries are nonnegative), and satisfy lower-order marginal consistency (different high-order marginals are consistent at lower orders).\nAfter each conditioning step, we apply a 'correction' to the approximate posterior P (t) by finding the bandlimited function in M \u2032 which is closest to P (t) in an L 2 sense. To perform the projection, we employ the Plancherel Theorem [3] which relates the L 2 distance between functions on S n to a distance metric in the Fourier domain. Proposition 5.\n\u03c3 (f (\u03c3) \u2212 g(\u03c3)) 2 = 1 |G| k d \u03c1 k Tr f \u03c1 k \u2212\u011d \u03c1 k T \u2022 f \u03c1 k \u2212\u011d \u03c1 k .(8)\nWe formulate the optimization as a quadratic program where the objective is to minimize the right side of Equation 8 -the sum is taken only over the set of maintained irreducibles, B, and subject to the linear constraints which define M \u2032 .\nWe remark that even though the projection will always produce a Fourier transform corresponding to nonnegative marginals, there might not necessarily exist a joint probability distribution on S n consistent with those marginals. In the case of first-order marginals, however, the existence of a consistent joint distribution is guaranteed by the Birkhoff-von Neumann theorem [10], which states that a matrix is doubly stochastic if and only if it can be written as a convex combination of permutation matrices. And so for the case of first-order marginals, our relaxation is in fact, exact.", "publication_ref": ["b4", "b2", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The Identity Management problem was first introduced in [2] which maintains a doubly stochastic first order belief matrix to reason over data associations. Schumitsch et al. [4] exploits a similar idea, but formulated the problem in log-space. \nCij \u2190 CGcoef f icients(\u03c1i, \u03c1j) ; Aij \u2190 C T ij \u2022 \"f \u03c1 i \u2297\u011d\u03c1 j \" \u2022 Cij ; for \u03c1 k \u2208 B such that z ijk = 0 do for \u2113 = 1 to z k do h L (t) P (t) i \u03c1 k \u2190 h L (t) P (t) i \u03c1 k + d\u03c1 i d\u03c1 j d\u03c1 k n! A k\u2113 ij //A k\u2113 ij is the (k, \u2113) block of Aij Z \u2190 h L (t) P (t) i \u03c1 0 ; foreach \u03c1 k \u2208 B do h L (t) P (t) i \u03c1 k \u2190 1 Z h L (t) P (t) i \u03c1 k //Normalization\nKondor et al. [5] were the first to show that the data association problem could be approximately handled via the Fourier Transform. For conditioning, they exploit a modified FFT factorization which works on certain simplified observation models. Our approach generalizes the type of observations that can be handled in [5] and is equivalent in the simplified model that they present. We require O(D 3 n 2 ) time in their setting. Their FFT method saves a factor of D due to the fact that certain representation matrices can be shown to be sparse. Though we do not prove it, we observe that the Clebsch-Gordan coefficients, C ij are typically similarly sparse, which yields an equivalent running time in practice. In addition, Kondor et al. do not address the issue of projecting onto valid marginals, which, as we show in our experimental results, is fundamental in practice.\nWillsky [8] was the first to formulate a nonabelian version of the FFT algorithm (for Metacyclic groups) as well as to note the connection between pointwise products and Kronecker product decompositions for general finite groups. In this paper, we address approximate inference, which is necessary given the n! complexity of inference for the Symmetric group.", "publication_ref": ["b1", "b3", "b4", "b4", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental results", "text": "For small n, we compared our algorithm to exact inference on synthetic datasets in which tracks are drawn at random to be observed or swapped. For validation we measure the L 1 distance between true and approximate marginal distributions. In (Fig. 3(a)), we call several mixings followed by a single observation, after which we measured error. As expected, the Fourier approximation is better when there are either more mixing events, or when more Fourier coefficients are maintained. In (Fig. 3(b)) we allow for consecutive conditioning steps and we see that that the projection step is fundamental, especially when mixing events are rare, reducing the error dramatically. Comparing running times, it is clear that our algorithm scales gracefully compared to the exact solution (Fig. 3(c)).\nWe also evaluated our algorithm on data taken from a real network of 8 cameras (Fig. 3(d)). In the data, there are n = 11 people walking around a room in fairly close proximity. To handle the fact that people can freely leave and enter the room, we maintain a list of the tracks which are external to the room. Each time a new track leaves the room, it is added to the list and a mixing event is called to allow for m 2 pairwise swaps amongst the m external tracks.\nThe number of mixing events is approximately the same as the number of observations. For each observation, the network returns a color histogram of the blob associated with one track. The task after conditioning on each observation is to predict identities for all tracks inside the room, and the evaluation metric is the fraction of accurate predictions. We compared against a baseline approach of predicting the identity of a track based on the most recently observed histogram at that track. This approach is expected to be accurate when there are many observations and discriminative appearance models, neither of which our problem afforded. As (Fig. 3(e)) shows,  both the baseline and first order model(without projection) fared poorly, while the projection step dramatically boosted the accuracy. To illustrate the difficulty of predicting based on appearance alone, the rightmost bar reflects the performance of an omniscient tracker who knows the result of each mixing event and is therefore left only with the task of distinguishing between appearances.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusions", "text": "We presented a formulation of hidden Markov model inference in the Fourier domain. In particular, we developed the Kronecker Conditioning algorithm which performs a convolution-like operation on Fourier coefficients to find the Fourier transform of the posterior distribution. We argued that bandlimited conditioning can result in Fourier coefficients which correspond to no distribution, but that the problem can be remedied by projecting to a relaxation of the marginal polytope. Our evaluation on data from a camera network shows that our methods outperform well when compared to the optimal solution in small problems, or to an omniscient tracker in large problems. Furthermore, we demonstrated that our projection step is fundamental to obtaining these high-quality results.\nWe conclude by remarking that the mathematical framework developed in this paper is quite general. In fact, both the prediction/rollup and conditioning formulations hold over any finite group, providing a principled method for approximate inference for problems with underlying group structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work is supported in part by the ONR under MURI N000140710747, the ARO under grant W911NF-06-1-0275, the NSF under grants DGE-0333420, EEEC-540865, Nets-NOSS 0626151 and TF 0634803, and by the Pennsylvania Infrastructure Technology Alliance (PITA). Carlos Guestrin was also supported in part by an Alfred P. Sloan Fellowship. We thank Kyle Heath for helping with the camera data and Emre Oto, and Robert Hough for valuable discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tracking people in mixed modality systems", "journal": "MERL", "year": "2007", "authors": "Y Ivanov; A Sorokin; C Wren; I Kaur"}, {"ref_id": "b1", "title": "A distributed algorithm for managing multi-target identities in wireless ad-hoc sensor networks", "journal": "", "year": "2003", "authors": "J Shin; L Guibas; F Zhao"}, {"ref_id": "b2", "title": "Group Representations in Probability and Statistics. IMS Lecture Notes", "journal": "", "year": "1988", "authors": "P Diaconis"}, {"ref_id": "b3", "title": "The information-form data association filter", "journal": "", "year": "2006", "authors": "B Schumitsch; S Thrun; G Bradski; K Olukotun"}, {"ref_id": "b4", "title": "Multi-object tracking with representations of the symmetric group", "journal": "", "year": "2007", "authors": "R Kondor; A Howard; T Jebara"}, {"ref_id": "b5", "title": "Tractable inference for complex stochastic processes", "journal": "", "year": "1998", "authors": "X Boyen; D Koller"}, {"ref_id": "b6", "title": "Snob: a C++ library for fast Fourier transforms on the symmetric group", "journal": "", "year": "2006", "authors": "R Kondor"}, {"ref_id": "b7", "title": "On the algebraic structure of certain partially observable finite-state markov processes. Information and Control", "journal": "", "year": "1978", "authors": "A Willsky"}, {"ref_id": "b8", "title": "The analysis of the kronecker product of irreducible representations of the symmetric group", "journal": "American Journal of Mathematics", "year": "1938", "authors": "F D Murnaghan"}, {"ref_id": "b9", "title": "A Course in Combinatorics", "journal": "Cambridge University Press", "year": "2001", "authors": "J Van Lint; R M Wilson"}], "figures": [{"figure_label": "12", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :Figure 2 :12Figure 1: Pseudocode for the Fourier Prediction/Rollup Algorithm.PREDICTIONROLLUPforeach \u03c1 k \u2208 B doP (t+1) \u03c1 k \u2190Q (t) \u03c1 k \u2022P (t) \u03c1 k ;Figure2: Pseudocode for the Kronecker Conditioning Algorithm.KRONECKERCONDITIONING foreach \u03c1 k \u2208 B do h L (t) P (t) i \u03c1 k \u2190 0 //Initialize Posterior", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Accuracy for Camera Data", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Evaluation on synthetic ((a)-(c)) and real camera network ((d),(e)) data.both the baseline and first order model(without projection) fared poorly, while the projection step dramatically boosted the accuracy. To illustrate the difficulty of predicting based on appearance alone, the rightmost bar reflects the performance of an omniscient tracker who knows the result of each mixing event and is therefore left only with the task of distinguishing between appearances.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P (\u03c3(", "formula_coordinates": [2.0, 155.23, 157.36, 19.47, 11.04]}, {"formula_id": "formula_1", "formula_text": "Y t P (z t |\u03c3 (t) ) \u2022 P (\u03c3 (t) |\u03c3 (t\u22121) ),", "formula_coordinates": [2.0, 338.94, 150.71, 117.82, 27.08]}, {"formula_id": "formula_2", "formula_text": "\u03c41(1 \u2192 1, 2 \u2192 3, 3 \u2192 2) = 0 @ 1 0 0 0 0 1 0 1 0 1 A .", "formula_coordinates": [2.0, 220.78, 630.12, 170.43, 36.88]}, {"formula_id": "formula_3", "formula_text": "\u03c11 \u2295 \u03c12(g) \" \u03c11(g) 0 0 \u03c12(g) \u00ab .(1)", "formula_coordinates": [3.0, 234.63, 162.59, 269.37, 26.72]}, {"formula_id": "formula_4", "formula_text": "C \u22121 \u2022 \u03c4 \u2022 C = \u2295 \u03c1i\u2208R \u2295 \u03c1 i ,", "formula_coordinates": [3.0, 197.12, 260.85, 113.66, 11.83]}, {"formula_id": "formula_5", "formula_text": "f (\u03c3) = 1 |G| X k d\u03c1 k Tr hf T \u03c1 k \u2022 \u03c1 k (\u03c3) i ,(2)", "formula_coordinates": [3.0, 235.55, 526.72, 268.45, 28.72]}, {"formula_id": "formula_6", "formula_text": "[f\u03c4 1 ]ij = X \u03c3\u2208Sn f (\u03c3)[\u03c41(\u03c3)]ij = X \u03c3\u2208Sn f (\u03c3)1 {\u03c3(j) = i} = X \u03c3:\u03c3(j)=i f (\u03c3).", "formula_coordinates": [3.0, 170.67, 652.17, 270.66, 27.64]}, {"formula_id": "formula_7", "formula_text": "[Q * P ] (\u03c3 1 ) = \u03c32 Q(\u03c3 1 \u2022 \u03c3 \u22121 2 )P (\u03c3 2 )", "formula_coordinates": [4.0, 168.87, 390.39, 157.28, 13.98]}, {"formula_id": "formula_8", "formula_text": "P (\u03c3 (t+1) ) = X {(\u03c3 (t) ,\u03c4 (t) ) : \u03c3 (t+1) =\u03c4 (t) \u2022\u03c3 (t) } Q (t) (\u03c4 (t) )\u2022P (\u03c3 (t) ) = X \u03c3 (t) Q (t) (\u03c3 (t+1) \u2022(\u03c3 (t) ) \u22121 )P (\u03c3 (t) ) = h Q (t) * P i (\u03c3 (t+1) ).", "formula_coordinates": [4.0, 102.33, 438.79, 410.15, 30.4]}, {"formula_id": "formula_9", "formula_text": "P (t+1) \u03c1 \u2190 Q (t) \u03c1 \u2022 P (t)", "formula_coordinates": [4.0, 261.91, 494.22, 84.92, 13.3]}, {"formula_id": "formula_10", "formula_text": "f (\u03c3) \u2022 g(\u03c3) = 1 |G| X k d\u03c1 k Tr \" A T k \u2022 \u03c1 k (\u03c3) \" ,(3)", "formula_coordinates": [5.0, 222.52, 131.28, 281.47, 28.72]}, {"formula_id": "formula_11", "formula_text": "A k = f g \u03c1 k .", "formula_coordinates": [5.0, 137.12, 170.91, 57.6, 16.57]}, {"formula_id": "formula_12", "formula_text": "f (\u03c3) \u2022 g(\u03c3) = \" 1 |G| X i d\u03c1 i Tr \"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" # \u2022 \" 1 |G| X j d\u03c1 j Tr \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \" # = \" 1 |G| \u00ab 2 X i,j d\u03c1 i d\u03c1 j h Tr \"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" \u2022 Tr \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \"i .(4)", "formula_coordinates": [5.0, 150.72, 194.45, 353.27, 64.83]}, {"formula_id": "formula_13", "formula_text": "Tr \"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" \u2022 Tr \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \" = Tr \"\"f T \u03c1 i \u2022 \u03c1i(\u03c3) \" \u2297 \"\u011d T \u03c1 j \u2022 \u03c1j(\u03c3) \"\" = Tr \" \"f \u03c1 i \u2297\u011d\u03c1 j \" T \u2022 (\u03c1i(\u03c3) \u2297 \u03c1j(\u03c3)) \u00ab ,(5)", "formula_coordinates": [5.0, 152.81, 307.18, 351.19, 43.0]}, {"formula_id": "formula_14", "formula_text": "C \u22121 ij \u2022 [\u03c1i \u2297 \u03c1j] (\u03c3) \u2022 Cij = M k z ijk M \u2113=1 \u03c1 k (\u03c3).", "formula_coordinates": [5.0, 226.66, 413.65, 158.66, 28.86]}, {"formula_id": "formula_15", "formula_text": "A ij C \u22121 ij \u2022 f \u03c1i \u2297\u011d \u03c1j \u2022 C ij .", "formula_coordinates": [5.0, 333.85, 580.33, 130.49, 14.37]}, {"formula_id": "formula_16", "formula_text": "h c f g i \u03c1 k = 1 d\u03c1 k |G| X ij d\u03c1 i d\u03c1 j z ijk X \u2113=1 A k\u2113 ij ,(6)", "formula_coordinates": [5.0, 234.27, 607.76, 269.73, 28.85]}, {"formula_id": "formula_17", "formula_text": "A k\u2113 ij is the block of A ij corresponding to the (k, \u2113) block in \u2295 k \u2295 z ijk \u2113 \u03c1 k .", "formula_coordinates": [5.0, 136.96, 642.6, 288.57, 14.86]}, {"formula_id": "formula_18", "formula_text": "\u03c1 1 \u2297 \u03c1 1 \u2261 \u03c1 0 \u2295 \u03c1 1 \u2295 \u03c1 2 \u2295 \u03c1 3 . (7", "formula_coordinates": [5.0, 244.76, 705.21, 255.36, 10.7]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [5.0, 500.12, 705.88, 3.87, 8.97]}, {"formula_id": "formula_20", "formula_text": "\u03c3 (f (\u03c3) \u2212 g(\u03c3)) 2 = 1 |G| k d \u03c1 k Tr f \u03c1 k \u2212\u011d \u03c1 k T \u2022 f \u03c1 k \u2212\u011d \u03c1 k .(8)", "formula_coordinates": [6.0, 182.41, 534.93, 321.59, 27.69]}, {"formula_id": "formula_21", "formula_text": "Cij \u2190 CGcoef f icients(\u03c1i, \u03c1j) ; Aij \u2190 C T ij \u2022 \"f \u03c1 i \u2297\u011d\u03c1 j \" \u2022 Cij ; for \u03c1 k \u2208 B such that z ijk = 0 do for \u2113 = 1 to z k do h L (t) P (t) i \u03c1 k \u2190 h L (t) P (t) i \u03c1 k + d\u03c1 i d\u03c1 j d\u03c1 k n! A k\u2113 ij //A k\u2113 ij is the (k, \u2113) block of Aij Z \u2190 h L (t) P (t) i \u03c1 0 ; foreach \u03c1 k \u2208 B do h L (t) P (t) i \u03c1 k \u2190 1 Z h L (t) P (t) i \u03c1 k //Normalization", "formula_coordinates": [7.0, 110.24, 205.77, 338.83, 103.08]}], "doi": ""}