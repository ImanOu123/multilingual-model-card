{"title": "3D Shape and Indirect Appearance By Structured Light Transport", "authors": "Matthew O'toole; John Mather; Kiriakos N Kutulakos", "pub_date": "", "abstract": "We consider the problem of deliberately manipulating the direct and indirect light flowing through a time-varying, fully-general scene in order to simplify its visual analysis. Our approach rests on a crucial link between stereo geometry and light transport: while direct light always obeys the epipolar geometry of a projector-camera pair, indirect light overwhelmingly does not. We show that it is possible to turn this observation into an imaging method that analyzes light transport in real time in the optical domain, prior to acquisition. This yields three key abilities that we demonstrate in an experimental camera prototype: (1) producing a live indirect-only video stream for any scene, regardless of geometric or photometric complexity; (2) capturing images that make existing structured-light shape recovery algorithms robust to indirect transport; and (3) turning them into one-shot methods for dynamic 3D shape capture.", "sections": [{"heading": "Introduction", "text": "A common assumption in computer vision is that light travels along direct paths, i.e., it goes from source to camera by bouncing at most once in the scene. While this assumption works well in many cases, light propagation through natural scenes is actually a much more complex phenomenon: light reflects and refracts, it undergoes specular and diffuse inter-reflections, it scatters volumetrically and creates caustics, and may do all of the above in the same scene. Analyzing all these phenomena with a conventional camera is a hard, open problem-and is even harder when the scene is dynamic and light transport changes unpredictably.\nDespite the problem's intrinsic difficulty, indirect transport is a major component of real-world appearance [1] and an important cue for scene and material understanding [2]. It is also a major factor preventing broader use of structured-light techniques, which largely assume direct or low-frequency light transport (e.g., 3D laser scanning [3,4], active triangulation [5,6] and photometric stereo [7]).\nAs a step toward analyzing scenes that exhibit complex light transport, in this paper we develop a framework for imaging them in real time. Our focus is on the general case where the scene is unknown; its motion and photometric properties unrestricted; and its illumination comes from one or more controllable sources in general position (e.g., projectors).\nWorking from first principles, we show that two families  3) Caustics formed inside a mug from specular inter-reflections; note the secondary reflections to the board behind the mug and from the board onto the mug's exterior surface. (4) Refractions and caustics from a beer glass. See Figure 9 for more images and [8] for videos. of transport paths dominate image formation in a projectorcamera system: epipolar paths, which satisfy the familiar epipolar constraint and contribute to a scene's direct image, and non-epipolar paths which contribute to its indirect. Crucially, while the contributions of these paths are hard to separate in an image, the paths themselves are easy to untangle in the optical domain before acquisition takes place. Using this idea as a starting point, we develop a novel technique called Structured Light Transport (SLT) that processes epipolar and non-epipolar paths optically for the purpose of live imaging and 3D shape recovery. In particular, we define and address four imaging problems:\n\u2022 one-shot indirect-only imaging: capture an image that records only contributions from indirect light; \u2022 one-shot indirect-invariant imaging: given any desired illumination, capture an image where light appears to have been transported by direct paths only; \u2022 two-shot direct-only imaging: capture two images whose difference contains only the direct light; and \u2022 one-shot multi-pattern imaging: given any N \u2265 2 desired illuminations, capture an image that \"packs\" into one shot N separate views of the scene, each corresponding to a desired illumination.\nLittle is currently known about how to solve these prob-lems in the general setting we consider. Our solutions, while firmly rooted in computer vision, operate exclusively in the optical domain and require no computational postprocessing: our implementation is a physical device that just outputs live video; this is optionally processed \"downstream\" by standard 3D reconstruction algorithms [5] which can be oblivious to the complexity of light transport occurring in a scene. The device itself is a novel combination of existing off-the-shelf components-a conventional video camera operating at 28Hz, a pair of synchronized digital micro-mirror devices (DMDs) operating at 2.7kHz to 24kHz, and optics for coupling them.\nFrom a practical point of view, our work offers four main contributions over the state of the art. First, it is the first demonstration of an \"indirect-only video camera,\" i.e., a camera that outputs a live stream of indirect-only video for fully-general scenes-exhibiting arbitrary motion, caustics, specular inter-reflections and numerous other transport effects. Prior work on indirect imaging was either constrained to static scenes [9,10], or assumed diffuse/low-frequency transport [2,11] and accurate 2D motion estimation [11]. Second, we show how to capture-with just one SLT shotviews of a scene that are invariant to indirect light. This is particularly useful for imaging dynamic scenes and represents an advance over direct-only imaging [2,9], which requires at least two images. Third, we show that any ensemble of structured-light patterns can be made robust to indirect light, regardless of the patterns' frequency content. This involves simply switching from conventional to SLT imaging-without changing the patterns or the algorithm that processes them. As such, our work stands in contrast to prior work on transport-robust structured light, which places the onus on the design of the patterns themselves [6,[12][13][14]. Fourth, we show that SLT imaging can turn any multi-pattern 3D structured-light method into a one-shot technique for dynamic shape capture. Thus an entire family of previously-inapplicable techniques can be brought to bear on this much-studied problem [5,[15][16][17][18] in order to improve depth map resolution and robustness to indirect light. As a proof of concept, we demonstrate in Figure 9 the reconstruction of dense depth and albedo from individual frames of monochrome video, acquired by combining indirect-invariant SLT imaging and conventional six-pattern phase-shifting.\nConceptually, our work has one essential difference from conventional structured light [2,5]: instead of controlling light only at its source by projecting patterns, we control light at its destination as well, with a DMD mask in front of the camera pixels. This simultaneous projection and masking makes it possible to analyze light transport geometrically (by blocking 3D light paths), rather than photometrically (by blocking certain transport frequencies and assuming constrained scene reflectance [2]). It also enables optical-domain implementations, which can have a significant speed and signal-to-noise ratio advantage over postcapture processing. The idea was first used in [9] for static scenes and a coaxial projector/camera, where epipolar ge- Light can reach pixel i on the image in one of three general ways: by indirect transport from an arbitrary pixel p on the corresponding epipolar line (green path); by indirect transport from a pixel q that is not on that line (red path); or by direct surface reflection, starting from projector pixel r on the epipolar line (black path).\nometry is degenerate and stereo is impossible. While SLT imaging builds on that work, its premise, theory, applications, and physical implementation are different.", "publication_ref": ["b5", "b6", "b7", "b8", "b9", "b10", "b10", "b8", "b5", "b11", "b12", "b13", "b14", "b15", "b16", "b17", "b8"], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "The Stereo Transport Matrix", "text": "We begin by relating scene geometry to the light transported from a projector to a camera in general position. Consider a scene whose shape potentially varies with time. If the camera and projector respond linearly to light, the scene's instantaneous image satisfies the light transport equation [19]:\ni = T p (1\n)\nwhere i is the image represented as a column vector of I pixels; p is the P -pixel projected pattern, also represented as a column vector; and T is the scene's I \u00d7P instantaneous light transport matrix.\nIntuitively, element T[i, p] of the transport matrix specifies the total radiance transported from projector pixel p to image pixel i over all possible paths. As such, T models image formation in very general settings: the scene may have non-Lambertian reflectance, it may scatter light volumetrically, exhibit specular inter-reflections, etc.\nAnatomy of the stereo transport matrix Since a projector and a camera in general position define a stereo pair, their transport matrix is best understood by taking two-view geometry into account. More specifically, we classify the elements of T into three categories based on the geometry of their transport paths (Figure 2):\n\u2022 Epipolar elements, whose projector and camera pixels are on corresponding epipolar lines. These are the only elements of T whose transport paths begin and end on rays that can intersect in 3D. By performing stereo calibration [20] and vectorizing patterns and images according to Figure 3, these elements can be made to occupy a known, time-invariant, blockdiagonal subset of the transport matrix.\nimage i pattern p instantaneous transport matrix T p 1 epipolar line 1 p 2 epipolar line 2 p E epipolar line E epipolar line 1 i 1 epipolar line 2 i 2 epipolar line E i E T 11 T 22 T 21 T EE . . . . . . . . . = \u00d7\nblocks of epipolar elements block of non-epipolar elements", "publication_ref": ["b18", "b19"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Figure 3:", "text": "The light transport equation when patterns and images are vectorized so that consecutive pixels on corresponding epipolar lines form subvectors pe and ie, respectively. Under this vectorization scheme, block T ef of the transport matrix describes transport from epipolar line f on the pattern to epipolar line e on the image. Blocks Tee, shown in green, contain the epipolar elements.\n\u2022 Non-epipolar elements, whose projector pixel and camera pixel are not on corresponding epipolar lines. Non-epipolar elements are significant because they vastly outnumber the other elements of T and never account for direct transport. This is because their transport paths begin and end with rays that do not intersect, so light must bounce at least twice to follow them. \u2022 Direct elements, whose camera and projector pixels are in stereo correspondence, i.e., they are the perspective projections of a visible surface point. Direct elements are where direct surface reflection actually occurs in the scene; although they always lie within T's epipolar blocks, their precise location is scene dependent and thus unknown. Indeed, locating the direct elements is equivalent to computing the scene's instantaneous stereo disparity map (Figure 4).\nWe can therefore express every image of the scene as a sum of three components that arise from distinct \"slices\" of the transport matrix:\ni = T D p direct image + T EI p epipolar indirect image + T NE p non-epipolar indirect image (2)\nwhere the I \u00d7 P matrices T D , T EI and T NE hold the direct, epipolar indirect, and non-epipolar elements, respectively, and are zero everywhere else.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Dominance of Non-Epipolar Transport", "text": "Although in theory all three image components in Eq. (2) may contribute to scene appearance, in practice their contributions are not equal. The key observation underlying our work is that the non-epipolar component is very large relative to the epipolar indirect for a broad range of scenes:  This element is direct if and only the scene point projecting to both pixels is the same, i.e., the point's stereo disparity is i \u2212 r.\ni \u2248 T D p direct image + T NE p non-epipolar indirect image .(3)\nThe set of direct elements therefore represents the scene's instantaneous disparity map. Conventional stereo algorithms attempt to localize this set while assuming that the transport matrix is zero everywhere else-both inside and outside its epipolar blocks.\nWe call this the non-epipolar dominance assumption. The transport matrix is much simpler when this assumption holds because we can treat it as having a time-invariant structure with two easily-identifiable parts: the epipolar blocks, which contribute only to the direct image, and the non-epipolar blocks, which contribute only to the indirect.\nTo motivate this assumption on theoretical grounds, we prove that it holds for two very general scene classes: (1) scenes whose transport function is measurable everywhere and (2) generic scenes containing pure specular reflectors and transmitters. These two cases can be thought of as representing opposite extremes, with the former covering low-frequency transport phenomena such as diffuse interreflection and diffuse isotropic subsurface scattering [21] and the latter covering transport whose frequency content is not band limited. In particular, we prove the following: Proposition 1. If T is the discretized form of a transport function that is measurable and positive over the rectified projector and image planes, then\nlim\u01eb\u21920 T EI p T NE p = 0 (4\n)\nwhere division is entrywise and \u01eb is the pixel size for discretization. Proposition 2. Two generic n-bounce specular transport paths that originate from corresponding epipolar lines do not intersect for n > 1.\nSee [8] for proofs. Intuitively, both propositions are consequences of a \"dimensionality gap\": the set of transport paths contributing to the epipolar indirect image has lower dimension than the set of paths contributing to the non-epipolar image (Figure 2). Thus contributions accumulated in one image are negligible relative to the other in generic settings.   View under an all-white projection pattern. Top middle: View when just one white vertical stripe is projected onto the scene. The many bright regions in this image occur because the stripe illuminates the book's pages in three different ways: (1) directly from the projector, (2) by diffuse inter-reflection from the opposite page, and (3) by specular reflection via the mirror. Their existence makes the scene hard to reconstruct with conventional techniques such as laser-stripe 3D scanning [4]. A magnified view of these regions is shown in the inset. Top right: View for another vertical stripe, part of which falls on the candle. The stripe appears very broad and poorly localized there, because of strong sub-surface scattering. Bottom left: The epipolar block Tee for epipolar line e. We show Tee using the conventions of Figure 4, i.e., its r-th column comes from an image of the scene acquired with only projector pixel pe[r] turned on. Bottom middle: To assess the image contribution of non-epipolar transport, we acquire the block sum E f =1 T ef and compare it to block Tee-observe that non-epipolar contributions indeed far surpass the epipolar indirect ones. To acquire the block sum, we capture images of the scene while sweeping a vertical stripe on the projector plane (see [8] for a video of the captured image sequence). The r-th column of the block sum is given by the pixels on epipolar line e when the stripe is at pe[r]. Bottom right: Horizontal cross-section of Tee and E f =1 T ef for two image pixels. Observe that Tee's cross-section (blue) is sharp and unimodal whereas the block sum's (red) is trimodal for one pixel and very broad for the other.\nOn the practical side, we have found non-epipolar dominance to be applicable quite broadly; see Figure 5 for a detailed analysis of non-epipolar dominance in a complex scene, Figure 9 for more examples, and [8] for videos confirming the assumption's validity in a variety of settings.", "publication_ref": ["b20", "b7", "b7", "b7"], "figure_ref": ["fig_1", "fig_2", "fig_4", "fig_7"], "table_ref": []}, {"heading": "Imaging by Structured Light Transport", "text": "The rich structure of the stereo transport matrix cannot be exploited by simply projecting a pattern onto the scene. This is because projection gives no control over how light flows through the scene: all elements of T-regardless of position-will participate in image formation. To make full use of T's structure, we structure the flow of light itself.\nOur starting point is an imaging procedure first proposed by O'Toole et al. [9]. Its main advantage is that the contribution of individual elements of T can be weighted according to a user-defined \"probing matrix\" \u03a0:\ni = [ \u03a0 \u2022 T ] 1 (5)\nwhere \u2022 denotes entrywise (a.k.a. Hadamard) product and 1 is a column vector of all ones. Images captured this way are said to be the result of probing the scene's transport matrix with matrix \u03a0. Conceptually, they correspond to images of a scene that is illuminated by an all-white pattern and whose transport matrix is \u03a0 \u2022 T.\nTwo basic questions arise when considering Eq. (5) for image acquisition and shape recovery: (1) what should \u03a0 be, and (2) how to design an imaging system that implements the equation? The answers in [9] were restricted to static scenes and projector/camera arrangements that share a single viewpoint, none of which apply here. Below we focus on the first question-designing \u03a0-and discuss live imaging of dynamic scenes in Section 5.\n\u03a0 1 (p): projection of pattern p \u03a0 2 (p): indirect-invariant imaging 1p T 1 1p T 1 1p T 1 1p T 1 1p T 2 1p T 2 1p T 2 1p T 2 1p T E 1p T E 1p T E 1p T E . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . . 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 1p T 1 1p T 2 1p T E . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . . \u03a0 3 : indirect-only imaging \u03a0 4 : epipolar-only imaging 00 T 00 T 00 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . . 11 T 11 T 11 T 00 T 00 T 00 T 00 T 00 T 00 T 00 T 00 T 00 T . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . .", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Figure 6:", "text": "The four basic probing matrices used in this paper.\nTheir block structure mirrors the structure of T in Figure 3.\nConventional structured-light imaging To gain some insight, let us re-cast as a probing operation the act of projecting a fixed pattern p and capturing an image i. Applying the vectorization scheme of Figure 3 to the light transport equation and re-arranging terms we get for epipolar line e:\nie = E f =1 T ef p f = E f =1 (1p T f ) block of probing matrix \u2022 T ef block of T 1 (6\n)\nwhere E is the number of epipolar lines. Equation ( 6) implies that projecting p is equivalent to probing with the matrix \u03a0 1 (p) shown in Figure 6. Observe that if we capture images for a whole sequence of projection patterns-as is often the case in structured-light systems-the non-epipolar blocks of the probing matrix will be different for each pattern. Indirect transport will therefore contribute to each captured image differently, and in a way that strongly depends on the particular pattern. This makes structured-light 3D scanning difficult when indirect transport is present because its contributions cannot be easily identified and removed.\nIndirect-invariant imaging The contribution of indirect transport becomes much easier to handle if we ensure it is the same for every pattern. Since this contribution is dominated by the non-epipolar blocks of the transport matrix, we can achieve (almost) complete invariance to indirect transport by probing with a matrix whose non-epipolar blocks are independent of p. In particular, probing with the matrix \u03a0 2 (p) in Figure 6 yields\nie = (1p T e ) \u2022 Tee 1 direct image (depends on p) + E f =1,f =e T ef 1 non-epipolar indirect image (ambient) .(7)\nThe image in Eq. ( 7  non-epipolar component is independent of p. This independence essentially turns indirect contributions into an \"ambient light\" term that does not originate from the projection pattern. 1 To see the practical significance of this independence, the second row of Figure 9 compares views of a scene under conventional and one-shot indirect-invariant imaging, for the same projection pattern.\nAn important corollary of Eq. ( 7) is that indirect-invariant images can be acquired for any sequence of patternsregardless of frequency content or other properties-using the corresponding sequence of probing matrices.\nIndirect-only imaging A notable special case of indirectinvariant imaging is to set p to zero (matrix \u03a0 3 in Figure 6). This yields an image guaranteed to have no contributions from direct transport. Moreover, almost all indirect light will be recorded when non-epipolar dominance holds.\nEpipolar-only imaging The exact opposite effect can be achieved with a probing matrix that is zero everywhere except along the epipolar blocks (matrix \u03a0 4 in Figure 6). When non-epipolar dominance holds, images captured this way can be treated as (almost) purely direct.\nOne-shot, multi-pattern, indirect-invariant imaging All four probing matrices in Figure 6 produce views of the scene under a fixed illumination pattern p. With probing, however, it is possible to capture-in just one shotspatially-multiplexed views of the scene for a whole sequence of structured-light patterns, p(1), . . . , p(S). The probing matrix to achieve this can be thought of as defining a \"projection pattern mosaic,\" much like the RGB filter mosaic does for color (Figure 7). Moreover, we can confer invariance to indirect light by defining the mosaic in terms of probing matrices rather than conventional patterns.\nSpecifically, suppose we partition the I image pixels into S sets and let b(1), . . . , b(S) be binary vectors of size I indicating the pixel membership of each set. The matrix\n\u03a0 5 (p(1), . . . , p(S)) = S s=1 b(s) 1 T \u2022 \u03a0 2 (p(s)) (8)\ninterleaves the rows of S indirect-invariant probing matrices. Thus, probing with this matrix yields an image containing S sub-images, each of which is a view of the scene under a specific structured-light pattern in the sequence. 1 Other examples of ambient terms with identical behavior include image contributions from the projector's black level and contributions from light sources other than the projector. Because such terms are often unavoidable yet easy to handle, many structured-light algorithms are designed to either recover them explicitly or be robust to their existence [5]. Non-zero ambient terms do, however, reduce contrast and may affect SNR.", "publication_ref": [], "figure_ref": ["fig_7", "fig_5"], "table_ref": []}, {"heading": "Live Structured-Light-Transport Imaging", "text": "The feasibility of probing comes from re-writing Eq. (5) as a bilinear matrix-vector product [9]:\ni = T t=1 m(t) \u2022 [ T q(t) ](9)\nwhere the transport matrix T is constant in time and \u03a0 = T t=1 m(t)(q(t)) T is a rank-1 decomposition of the probing matrix. According to Eq. (9), optical probing is possible by (1) opening the camera's shutter, (2) projecting pattern q(t) onto the scene, (3) using a semi-transparent pixel mask m(t) to modulate the light arriving at individual camera pixels, (4) changing the pattern and mask synchronously T times, and (5) closing the shutter. This procedure acquires one image; it was implemented in [9] for low-resolution probing matrices using an LCD panel for pixel masking, an SLR camera for image acquisition, and T \u2208 [100, 1000].\nAlthough results were promising, LCDs are not suitable for video-rate (30Hz) probing: they refresh at 30-200Hz, limiting T to an unusable 1-6 masks/projections per frame; and they have low transmittance, requiring long exposure times.\nOur approach, on the other hand, is to use a pair of offthe-shelf digital micro-mirror (DMD) devices for projection and masking (Figure 8). These devices are compact, incur no light loss and can operate synchronously at 2.7 \u2212 24kHz. To implement Eq. (9), we couple them with a conventional video camera operating at 28fps. This allows 96 \u2212 800 masks/projections within the 36msec exposure of each frame. 2 To our knowledge, such a coupling has not been proposed before. 3 A major difference between LCDs and DMDs is that DMDs are binary. This turns the derivation of masks and projection patterns into a combinatorial optimization problem. Formally, given an integer 4 probing matrix \u03a0 and an upper bound on T , we seek a length-T rank-1 decomposition into binary vectors such that the decomposition approximates \u03a0 as closely as possible. This problem is difficult and we know of no general solution. Indeed, estimating the length of the shortest exact decomposition is itself NP-hard [23].\nOur approach, below, is to derive randomized decompositions of \u03a0 that approximate Eq. (9) in expectation. Although our experience is that this approach works well in practice, it should not be treated as optimal.\nIndirect-only imaging Matrix \u03a0 3 is a special case where short decompositions are easy. Let q(e) be a pattern whose See [8] for a detailed list of components. pixels are 1 along epipolar line e and 0 everywhere else and let m(e) be a mask that is 1 everywhere except at epipolar line e. Then it is easy to show that \u03a0 3 = E e=1 m(e)(q(e)) T . This corresponds to a sequence of mask/projection pairs where only one epipolar line is \"off\" in the mask and only the corresponding epipolar line is \"on\" in the pattern. Even though this decomposition is exactand feasible for near-megapixel images-it has poor light efficiency because only one epipolar line is \"on\" at any time.\nTo improve light efficiency we use random patterns instead, which yield good approximations that are much shorter.\nSpecifically, consider the random pattern q = {each epipolar line is 1 with probability 0.5} ,\nlet the projection pattern q(t) be a sample of q, and let the mask m(t) be equal to q(t). Taking expectations in Eq. ( 9), the epipolar line e of the expected image is given by\nE [ie] = E [qe] \u2022 E f =1 f =e T ef E [q f ] = 0.25 E f =1 f =e T ef 1 (11\n)\nwhere E[] denotes expectation. This is the result of probing with matrix \u03a0 3 , albeit at one quarter of the \"ideal\" image intensity. 5 Note that corresponding epipolar lines are never on at the same time in the pattern and mask; thus no epipolar transport path ever contributes to the captured image.\nEpipolar-only imaging Matrix \u03a0 4 is a special case at the other extreme, where no short rank-1 decompositions exist. Since \u03a0 4 = \u03a0 1 (1) \u2212 \u03a0 3 , we compute the result of probing with \u03a0 4 by subtracting two adjacent video frames-one captured by projecting an all-white pattern and one captured by indirect-only imaging. Naturally, two-frame motion estimation may be necessary to handle fast-moving scenes (but we do not estimate motion in our experiments).\nIndirect-invariant imaging A perhaps counterintuitive result is that even though epipolar-only imaging requires two frames, indirect-invariant imaging requires just one. This is important because probing with matrix \u03a0 2 () is all we need for reconstruction with structured light. Let p be an arbitrary structured-light pattern scaled to [0, 1]. Define mask m(t) to be a sample of q from Eq. (10) and the pattern to be\nq(t) = m(t) \u2022 r(t) + m(t) \u2022 r(t)(12)\nwhere r(t) is a sample of yet another random pattern:\nr = {pixel p on epipolar line e is 1 with probability pe[p]} .\nA pictorial illustration of Eq. ( 12) can be found in [8]. From calculations similar to Eq. (11), the expected image is\nE [ie] = 0.5Teepe + 0.25 E f =1,f =e [T ef p f + T ef (1 \u2212 p f )] = 0.5Teepe direct image (depends on p) + 0.25 E f =1,f =e T ef 1 indirect image (ambient) ,(14)\nwhich is equivalent to the result of probing with \u03a0 2 ().\nOne-shot, multi-pattern, indirect-invariant imaging Here we use the mask for indirect-invariant imaging and temporally multiplex S random projection patterns-each defined by Eq. ( 12) and corresponding to a different structured-light pattern-across our \"budget\" of T total projections per video frame. After the video is recorded, we \"demosaic\" each frame i independently to infer S fullresolution images, one for each structured-light pattern. Following work on compressed sensing [24,25] we do this by solving for S images that reproduce frame i and are sparse under a chosen basis W:\nminimize W T i(1) . . . i(S) n (15) subject to S s=1 b(s) \u2022 i(s) \u2212 i 2 \u2264 \u01eb (16)\nwhere . n is a sparsity-inducing norm 6 and b(s) is the binary vector holding pixel memberships for pattern s.", "publication_ref": ["b8", "b8", "b22", "b7", "b7", "b23", "b24", "b5"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Experimental Results", "text": "Indirect-only and epipolar-only imaging Our DMDs operated at 2.7kHz with T = 96 or 48 patterns/masks per frame. For calibration, we computed the epipolar geometry between the two DMDs by first relating them to the image plane. Overall resolution was equal to the resolution of our DMDs, i.e., 608 \u00d7 684. See Figures 1 and 9 (row 1) for examples of indirect-and epipolar-only images, respectively.\nIndirect-invariant imaging We used high-end DMDs and a monochrome camera for the reconstruction experiments in Figure 9 (rows 2 and 3), with T = 800 patterns/masks per frame. The effective DMD resolution was approximately 484 \u00d7 364. The scenes occupied a 40 3 cm 3 vol-ume about 70cm away from the camera. To show the effectiveness of SLT imaging, we chose the most basic pattern and technique-phase-shifting with 9 sinusoids total, at frequencies 1, 8 and 64.\nDense depth and albedo from one shot We used S = 6 sinusoids at frequencies 4 and 32 for the experiment in Figure 9 (row 4), and a random, rather than regular, assignment of pixels to sinusoids. We recorded multi-pattern, indirectinvariant video at 28fps and reconstructed each frame independently by ( 1) solving for the 6 demosaiced patterns using SPGL1 [26] for optimization and the JPEG2000 wavelet basis, and (2) using them to get per-pixel depth and albedo.", "publication_ref": ["b25"], "figure_ref": ["fig_0", "fig_7", "fig_7"], "table_ref": []}, {"heading": "Concluding Remarks", "text": "We believe that optical-domain processing-and SLT imaging in particular-offers a powerful new way to analyze the appearance of complex scenes, and to boost the abilities of existing reconstruction algorithms. Although our focus was mainly on monochromatic light and conventional cameras, SLT imaging depends on neither; integrating this framework with other imaging dimensions (polarization, wavelength, time, etc.) is a promising direction. Last but not least, although our prototypes rely on DMD masks and several optical components, these would be rendered unnecessary if per-pixel processing was implemented directly on the sensor [27,28]. We are looking forward to the wide availability of such technologies.  Frames from conventional and epipolar-only video for the scenes in Figure 1. Compared to Figure 1, the water's opaque appearance in the epipolar-only frame and the absence of caustics in the mug confirm that significant indirect light was not recorded, i.e., non-epipolar dominance holds. Refer to [8] for videos of several more scenes. Row 2: We imaged the scene on the left in two ways: (1) projecting 9 phase-shifted patterns directly onto it and (2) capturing indirect-invariant images for the same patterns. Exposure time was held fixed, giving an SNR advantage to conventional projection which does not mask pixels. We then applied the same algorithm to the two sets of images, with the results shown on the right. The algorithm fails catastrophically for the conventionally-acquired images whereas with SLT imaging it is able to reconstruct even the hidden side of the face, from the mirror's indirect view. Closer inspection of the input images (please zoom in) reveals the reason for the difference: the conventional image contains \"double fringes\" from secondary reflections whereas the indirect-invariant one does not. Row 3: Another example, for a scene with strong diffuse and specular inter-reflections. Row 4: Reconstructing dense depth and albedo from one video frame of a moving hand. From this frame, our demosaicing algorithm recovers 6 full-resolution indirect-invariant images of the hand, for 6 sinusoidal patterns. These images yield the albedo and depth maps on the right. \nlim \u01eb\u21920 T EI p T NE p = 0 (17\n)\nwhere division is entry-wise and \u01eb is the pixel size for discretization.\nProof sketch. We begin by identifying the rectified projector and image planes with the continuous domain \nD = [\u22121, 1] \u00d7 [\u22121, 1] \u2282 \u211c 2 . Let p = (p x , p y )\nIn the continuous setting, light transport from the projector plane to the image plane is described by the light transport equation [2]. Given an image point i \u2208 D on the epipolar line through the origin, this equation describes the total radiance transported to i from points on the projector plane:\nI(i) = T (p, i) P(p) direct + D\u2212{p} T (p, i) P(p) dp indirect (19\n)\nwherep is the projector point in stereo correspondence with image point i; P(p) is the radiance along the ray through projector point p; and T (p, i) is the transport function describing the proportion of radiance from p that gets transported to i.\nWithout loss of generality, we prove the continuous form of the ratio in Eq. (1) for an image point i; this point is taken to be inside a discrete image pixel of dimension \u01eb \u00d7 \u01eb on the epipolar line through the origin.\nMore specifically, we consider the epipolar indirect, total indirect, and non-epipolar indirect contributions at i:\nI EI (i) = D\u2212{p} I \u01eb (p) T (p, i) P(p) dp (20\n)\nI I (i) = D\u2212{p} T (p, i) P(p) dp (21\n)\nI NE (i) = I I (i) \u2212 I EI (i) . (22\n)\nWe now show that for any \u03b4 > 0, there is an \u01eb > 0 such that\nI EI (i) I NE (i) < \u03b4 .(23)\nSince T () is measurable, we can apply the Cauchy-Schwarz inequality [4] to Eq. (4) to get an upper bound on the epipolar indirect contributions:\nI EI (i) \u2264 D\u2212{p} I\u01eb(p)dp 1 2 D\u2212{p} T (p, i) P(p) 2 dp 1 2 = (2\u01eb) 1 2 D\u2212{p} T (p, i) P(p) 2 dp 1 2 . (24\n)\nBy combining Eqs. ( 5), ( 6) and ( 8) we also get a lower bound on the non-epipolar contributions:\nI NE (i) \u2265 D\u2212{p} T (p, i) P(p) dp \u2212 (2\u01eb) 1 2 D\u2212{p} T (p, i) P(p) 2 dp 1 2 . (25\n)\nEquation ( 7) now follows by choosing \u01eb to be\n\u01eb = 1 2 \u03b4 2 + \u03b4 2 D\u2212{p} T (p, i) P(p) dp 2 D\u2212{p} T (p, i) P(p) 2 dp .(26)\nSpecifically, substituting Eq. (10) into Eqs. (8) and ( 9) we get\nI EI (i) I NE (i) \u2264 \u03b4 2+\u03b4 1 \u2212 \u03b4 2+\u03b4 = \u03b4 2 < \u03b4 .(27)", "publication_ref": ["b26", "b27", "b7"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "A.2. Proof of Proposition 2", "text": "We prove Proposition 2 for scenes consisting of a finite collection of objects, each of which is an open set in \u211c 3 bounded by a smooth generic surface [1,3].\nProposition 2. Two generic n-bounce specular transport paths that originate from corresponding epipolar lines do not intersect for n > 1.\nProof. For simplicity, we reverse the direction of light travel through image pixels, treating the camera as a second projector that also sends light onto the scene.\nLet L, L \u2032 be a pair of corresponding epipolar lines on the (continuous) projector and image planes, respectively, and let p \u2208 L and i \u2208 L \u2032 be points on them.\nSuppose that the light originating at p and i undergoes n \u2265 1 consecutive specular bounces upon entering the scene. Furthermore, suppose that the associated transport paths are generic, i.e., they remain stable under infinitesimal perturbations of the scene's surfaces and of the points p and i. To prove the proposition, we show that the following cannot hold simultaneously:\n1. the transport paths through p and i intersect at their (n + 1)-th bounce, i.e., their (n + 1)-th bounce occurs at the same surface point in the scene; and 2. this intersection is generic, i.e., it occurs for all points p, i in an open interval Q \u2282 L and Q \u2032 \u2282 L \u2032 , respectively.\nIn particular, let l n (p) be the 3D ray that light follows after n specular bounces from projector point p. Similarly, let l \u2032 n (i) be the corresponding 3D ray for image point i. Since the transport paths through p and i are generic, the mappings p \u2192 l n (p) and i \u2192 l \u2032 n (i) are smooth functions for some open neighborhood Q \u2282 L and Q \u2032 \u2282 L \u2032 of p and i, respectively. These mappings define a pair of ruled surfaces in \u211c 3 : intuitively, as point p ranges over Q, the 3D ray l n (p) twists and translates in space, tracing a ruled surface. Now, for the transport paths through p and i to have their (n + 1)-th bounce in common, three surfaces must meet at a point: ruled surface l n (Q), ruled surface l \u2032 n (Q \u2032 ), and a surface in the scene. This, however, is not a generic condition because surfaces l n (Q) and l \u2032 n (Q \u2032 ) transversally intersect along a curve and this curve will transversally intersect the scene's surfaces at isolated points [1].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Expanded derivations of selected equations", "text": "B.1. Derivation of Eq. (11) Combining Eqs. ( 6) and ( 9) we have\ni e = 1 T T t=1 E f =1 q e (t) \u2022 [ T ef q f (t) ](28)\nwhere the 1/T factor captures the fact that each term in the sum is allocated 1/T of the total exposure time. We now split the sum into its epipolar and non-epipolar terms\ni e = 1 T T t=1 q e (t) \u2022 [ T ee q e (t) ] + E f =1 f =e q e (t) \u2022 [ T ef q f (t) ](29)\nand observe that the first term is always a vector of zeros. Therefore,\ni e = 1 T T t=1 E f =1 f =e q e (t) \u2022 [ T ef q f (t) ] .(30)\nLetting T \u2192 \u221e and applying the Central Limit Theorem to Eq. ( 14) we get the expected image E[i e ] for epipolar line e:\nE[i e ] = E E f =1 f =e q e \u2022 [ T ef q f ](31)\n= E[q e ] \u2022 E E f =1 f =e T ef q f (32) = E[q e ] \u2022 E f =1 f =e T ef E[q f ](33)\n= 0.25 E f =1 f =e T ef 1 ,(34)\nwhere Eq. ( 16) follows from the fact that epipolar lines e and f are distinct and thus their corresponding random vectors q e and q f are independent.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Derivation of Eq. (14)", "text": "Combining Eqs. (6) and ( 9) for the indirect-invariant mask and pattern we have:\nie = 1 T T t=1 E f =1 me(t) \u2022 T ef [ m f (t) \u2022 r f (t) + m f (t) \u2022 r f (t) ](35)\nWe split the sum in Eq. ( 19) into its epipolar and nonepipolar terms,\ni e = 1 T T t=1 m e (t) \u2022 T ee [ m e (t) \u2022 r e (t) ] + m e (t) \u2022 T ee [ m e (t) \u2022 r e (t) ] + E f =1 f =e m e (t) \u2022 T ef [ m f (t) \u2022 r f (t) ] + E f =1 f =e m e (t) \u2022 T ef [ m f (t) \u2022 r f (t) ] (36\n)\nand note that the second term of Eq. ( 20) is always a vector of zeros. Letting T \u2192 \u221e and applying the Central Limit Theorem to Eq. ( 20) we get the expected image for epipolar line e:\nE[i e ] = E q e \u2022 T ee (q e \u2022 r e ) + E f =1 f =e q e \u2022 T ef (q f \u2022 r f + q f \u2022 r f ) .(37)\nNow, q e is a random binary vector whose probability of being either 1 or 0 is 0.5. Using this fact as well as q e 's independence from all other random vectors, the expectation in Eq. ( 21) becomes\nE[i e ] = 0.5 T ee E[r e ] + 0.5 E f =1 f =e T ef E[q f \u2022 r f + q f \u2022 r f ] . (38)\nFinally, using the definition of binary random vector r f in Eq. ( 13) the expectation becomes\nE[i e ] = 0.5 T ee p e + 0.5 E f =1 f =e T ef Prob[ q f = 1 ] p e + Prob[ q f = 0 ] (1 \u2212 p e )(39)\nwhich is equal to\nE[i e ] = 0.5 T ee p e + 0.25 E f =1 f =e T ef ( p e + (1 \u2212 p e )(40)\n= 0.5 T ee p e + 0.25\nE f =1 f =e T ef 1 .(41)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Experimental Prototypes", "text": "To encourage reproducibility, we include the complete parts list for our two experimental systems:\n\u2022 a low-speed, low-cost system for video-rate indirectonly and epipolar-only imaging (Figure 8 of the paper) whose components are listed in Table 1; and\n\u2022 a high-speed system for indirect-invariant shape acquisition and one-shot multi-pattern imaging (Figure 1), whose components are listed in Table 2.\nIndirect-only and epipolar-only imaging We used a color AVT GT1920C camera for acquisition, a Texas Instruments LightCrafter for pixel masking and a 100 lumen Keynote Photonics LightCrafter kit for projection. The DMDs were synchronized at 2.7kHz, permitting T = 96 patterns and masks per video frame. The camera and DMD resolutions were quite different-1936x1456 versus 608x684with each DMD pixel mapping to a 2 \u00d7 2 block of camera pixels. System calibration consists of computing the epipolar geometry between the two DMDs. We did this by first computing correspondences between the camera and each DMD separately. Patterns are uploaded to both DMDs once, at the beginning of an imaging session.\nIndirect-invariant imaging For these experiments we used a monochrome AVT GT1920 camera and a pair of high-end DMDs from Texas Instruments (DLi 4130) that use a 2000 lumen light source. These operate at 22.2kHz, permitting T = 800 patterns per video frame. Although the DMD resolution was fairly high at 1024 \u00d7 768, its effective resolution was much lower, 484 \u00d7 364, because of the different physical dimensions and orientation of the camera sensor and DMD.\nOne-shot multi-pattern imaging Effective DMD resolution was even lower, 256 \u00d7 256, because of the scene's limited extent within the camera's field of view.", "publication_ref": [], "figure_ref": ["fig_6", "fig_0"], "table_ref": ["tab_6", "tab_7"]}, {"heading": "D. Generation of Masks & Projection Patterns", "text": "The mathematical definition of the patterns and masks we use in our SLT prototypes is discussed in Section 5 of the paper. Here, we show in Figure 2 examples of actual mask/pattern pairs uploaded on our DMDs and illustrate their construction according to Eqs. (10), ( 12), ( 13) and ( 16) in the paper.\nIndirect-only imaging We use random mask/pattern pairs like those shown in Row 1 of Figure 2. To reduce the sensation of flicker by users who are physically present during video acquisition, we generate a random sequence of T /2 mask/pattern pairs (T = 800 or 96 depending on the prototype) and then generate a second mask/pattern sequence   The high-speed system in Figure 1 uses identical optics to the low-speed one; the only differences between the two systems are (1) a faster DMD projector and mask, (2) their mounts, and (3) a monochrome camera that is identical to the low-speed system's color camera, but without the RGB filter mosaic. We only list the differing components in this table; the remaining parts are items 7-38 in Table 1. The system's camera outputs live video at a rate of 28 frames per second. Each video frame consists of 800 binary mask/projection patterns, i.e., each mask/pattern is active for 45\u00b5sec of the frame's 36000\u00b5sec total exposure time. Figure 10: Our high-speed system. The key differences between this system and that shown in Figure 8 of the paper are a monochrome camera, the DMD mask, and the DMD projector.\nwhose projection patterns are the binary complement of the first T /2 projection patterns. This ensures a stable perception because the image integrated by the eye (or by a maskless camera) over the period of one video frame corresponds to a view of the scene under an all-white projection pattern. 1 For indirect-only imaging, it is also important to ensure that no direct light \"leaks\" accidentally through the DMD mask. Such leaks can occur because of pixel misalignments between the DMD mask and the camera's sensor; because of the binary rasterization of epipolar lines; and because of projector/camera defocus. To make indirect-only acquisition robust to such effects, we slightly dilate the \"off\" regions on the generated masks. This reduces the occurrence of such leaks at the expense of a slight reduction in light efficiency. We found this approach to be very effective in practice; a similar idea was used in [5].\nEpipolar-only imaging We generate epipolar-only video by operating the camera at 56fps and configuring the DMD of our low-speed prototype as follows:\n\u2022 odd video frames: display 48 all-on mask/pattern pairs \u2022 even video frames: display a sequence of 48 indirectonly mask/pattern pairs. Epipolar-only video at 28fps is generated by (1) scaling the odd frames by 0.25 to account for the reduced intensity of indirect-only imaging (Eq. (11)) and (2) subtracting in real time the even frames from the scaled odd ones.\nIndirect-invariant imaging and indirect-invariant 3D reconstruction We generate a sequence of 800 mask/pattern pairs for each of 9 grayscale structured-light patterns, as illustrated in rows 2-4 of Figure 2. We then capture one raw image of the scene for each of the 9 generated mask/pattern sequences. These 9 images are supplied, unaltered, to the 3D reconstruction algorithm.\nOne-shot, multi-pattern, indirect-invariant imaging We generate a sequence of 792 mask/pattern pairs, as outlined in row 5 of Figure 2, and upload them to the DMDs. We then apply the algorithm outlined in Section 5 independently to each frame of the raw live video stream. \u2022 Stripe scan: The purpose of this video is to show that when we sweep a stripe along the scene, the indirect light received at an epipolar line (red line in the video) can be very significant. \u2022 Spot scan: In this video we restrict projector illumination to the corresponding epipolar line. This enforces epipolar-only transport for pixels on the epipolar line shown in red. The video shows that the indirect light received at the red epipolar line is minimal, even for a highly-complex scene. Taken together, the stripescan and spot-scan videos demonstrate that the bulk of indirect light reaching the red epipolar line originates from projector pixels outside the corresponding epipolar line. \u2022 Acquiring T ee : This video shows how an epipolar block T ee can be acquired: for every frame in the spotscan video, we collect all pixels along the red epipolar line and place them as a column of T ee , also shown in red. The video shows how the epipolar block is acquired, column by column, by sliding the spot along the epipolar line from left to right. \u2022 Acquiring f T ef : An analogous visualization of the acquisition of f T ef , i.e., by sweeping a vertical stripe from left to right on the projector plane.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_0", "fig_0", "fig_6", "fig_1", "fig_1"], "table_ref": ["tab_6"]}, {"heading": "E.2. Videos in directory live SLT imaging/", "text": "We show conventional, indirect-only and epipolar-only streams for a variety of scenes. These were recorded live with the prototype shown in Figure 8 of the paper. All streams were captured with the same camera and the same settings (exposure time, white balance, etc.) The three streams shown in each video were captured sequentially, with the only difference being the masks/projection patterns used. This is because we only have one color SLT prototype and it can operate in one of three modes at any given time (i.e., conventional, indirect-only, epipolar-only).\nbinary complement mask corresponding to q(1) q( 1)\nq( 1) m( 1)\nindirect-only imaging ensemble of structured-light grayscale patterns (6 out of 9 sinusoids) p( 1) p( 2) p( 3) p( 4) p( 5) p( 6)\n6 out of 800 random binary patterns r(t) drawn according to Eq. (13) for grayscale pattern p(1)\nr( 1) r( 2) r( 3) r( 4) r( 5) r( 6)\nindirect-invariant imaging indirect-invariant mask applying Eq. ( 12  The masks for indirect-invariant imaging are identical to those for indirect-only imaging but the projection patterns differ. To generate them for a given grayscale structuredlight pattern, we first generate a random sequence of binary patterns (Row 3) and then use that sequence, along with the sequence of masks, to compute the projection patterns. Row 4 shows one such example. Conventional streams were captured under an all-on projection pattern with an all-on binary mask. Indirect-only streams were captured with 96 mask/pattern pairs, as explained in Section D. The conventional and indirect-only videos are recordings of the live, raw, video stream output by the camera. 2 Epipolar-only videos are created by pairwise differencing of adjacent video frames.\n\u2022 Candle: A translucent candle.\n\u2022 Foam: A piece of packing foam. The apparent speckles in the epipolar-only video are real: they correspond to momentary specular reflection from small, shiny membranes on the foam's surface. \u2022 Faux fur: Note the marked difference between the epipolar-only component, which appears very shiny due direct near-specular reflection off the faux fur, versus the diffuse appearance of the indirect-only component, caused by sub-surface scattering. The occasional yellow tint in the epipolar-only component is due to saturation.\n\u2022 Glass: This is the beer glass shown in Figures 5  and 1. Note that the glass appears essentially opaque in the epipolar-only component. This is because the light transmitted through the glass undergoes refraction, yielding non-linear paths that almost never lie on a single epipolar plane. \u2022 Hand: Note the veins visible in the indirect-only component; also note the significant difference in apparent color of the hand in the indirect-only and epipolar-only components (due to sub-surface absorption and direct surface reflection, respectively).\n\u2022 Mug: This is the mug shown in Figure 1. Note that artifacts appear occasionally on the white background behind the mug in the epipolar-only video. These occur because we do not compensate for motion when doing frame differencing. None of these artifacts appear in the indirect-only video, where no such differencing takes place. \u2022 Metal: The indirect-only video clearly shows the very interesting caustics formed by the surface of this shiny metal plate. These caustics move very quickly; as a result, the frame-differencing we do for epipolar-only imaging causes ghosting on the white background. Again, none of these artifacts appear in the indirectonly video, which does not rely on frame differencing. \u2022 Water: This example demonstrates our ability to successfully image a highly-complex, time-varying phenomenon, such as pouring water into a glass. As in the previous examples, the water appears mostly specular and opaque in the epipolar-only video whereas the caustics produced by light are clearly visible in the 2 To reduce file size for inclusion in the supplementary materials, it was necessary to compress these videos. As a result, some compression artifacts may be present. indirect-only video.\n\u2022 Wet hand: The indirect-only video makes apparent the very dramatic changes in a hand's reflectance properties when water flows over it. We hypothesize that these changes are caused by scattering in the thin film of water flowing over the hand. \u2022 Small candle, paper: More examples.", "publication_ref": [], "figure_ref": ["fig_6", "fig_0", "fig_0"], "table_ref": []}, {"heading": "E.3. Videos in directory transport robust 3D/", "text": "This directory contains input images and 3D reconstruction results for the two scenes shown in rows 2-3 of Figure 9. Rows 2-4 of Figure 2 show the derivation of one of the mask/pattern pairs we used for this purpose.\n\u2022 Conventional phase shift input: 9 input images acquired by conventional projection of phase-shifted patterns. \u2022 Conventional phase shift recontruction: raw 3D points reconstructed from those input images. \u2022 Indirect-invariant phase shift input: 9 input images acquired by indirect-invariant imaging with the same 9 patterns. \u2022 Indirect-invariant phase shift reconstruction: raw 3D points reconstructed from those input images. \u2022 Conventional stripe-based recontruction: as another example, we show reconstruction results obtained by sweeping a vertical stripe across the scene, as in conventional triangulation-based 3D laser scanning (768 images total). Despite the fact that stripe scanning relies on a much larger input dataset, our approach produces comparable results for the bowl scene and a far more complete model for the face scene.\nE.4. Videos in directory live 3D capture/ These videos provide details on the process of reconstructing a dynamic scene from video acquired by one-shot, 6pattern indirect-invariant imaging. Row 5 of Figure 2 shows one of the mask/pattern pairs we constructed for this purpose.\n\u2022 Hand raw: 169 frames of a moving hand, recorded live using one-shot, indirect-invariant, multi-pattern imaging. \u2022 Hand demosaic: the 6 full-resolution videos resulting from demosaicing each video frame individually to obtain indirect-invariant views of the scene under 6 phase-shifted patterns. \u2022 Hand depth, albedo: the raw, unprocessed depth and albedo maps reconstructed by applying conventional phase shifting to the demosaiced images. \u2022 Hand view albedo 1,2: the reconstructed and texturemapped geometry, shown from two different view-points. \u2022 Hand view depth 1,2: the reconstructed depth map, shown from two different viewpoints without texture mapping.", "publication_ref": [], "figure_ref": ["fig_7", "fig_1", "fig_1"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Acquiring the reflectance field of a human face", "journal": "", "year": "", "authors": "P Debevec"}, {"ref_id": "b1", "title": "Fast separation of direct and global components of a scene using high frequency illumination", "journal": "", "year": "", "authors": "S K Nayar"}, {"ref_id": "b2", "title": "An assessment of laser range measurement on marble surfaces", "journal": "", "year": "2001", "authors": "G Godin; M Rioux; J Beraldin; M Levoy"}, {"ref_id": "b3", "title": "Better optical triangulation through spacetime analysis", "journal": "", "year": "", "authors": "B Curless; M Levoy"}, {"ref_id": "b4", "title": "A state of the art in structured light patterns for surface profilometry", "journal": "Pattern Recogn", "year": "2010", "authors": "J Salvi; S Fernandez; T Pribanic; X Llado"}, {"ref_id": "b5", "title": "Micro Phase Shifting", "journal": "", "year": "", "authors": "M Gupta; S Nayar"}, {"ref_id": "b6", "title": "Shape from Interreflections", "journal": "Int. J. Computer Vision", "year": "1991", "authors": "S K Nayar; K Ikeuchi; T Kanade"}, {"ref_id": "b7", "title": "Supplementary materials", "journal": "", "year": "", "authors": "M O'toole; J Mather; K N Kutulakos"}, {"ref_id": "b8", "title": "Primal-dual coding to probe light transport", "journal": "", "year": "", "authors": "M O'toole; R Raskar; K N Kutulakos"}, {"ref_id": "b9", "title": "Femto-photography: capturing and visualizing the propagation of light", "journal": "", "year": "", "authors": "A Velten"}, {"ref_id": "b10", "title": "Compensating for Motion During Direct-Global Separation", "journal": "", "year": "", "authors": "S Achar; S T Nuske; S G Narasimhan"}, {"ref_id": "b11", "title": "Structured light 3D scanning in the presence of global illumination", "journal": "", "year": "", "authors": "M Gupta"}, {"ref_id": "b12", "title": "Unstructured light scanning to overcome interreflections", "journal": "", "year": "", "authors": "V Couture; N Martin; S Roy"}, {"ref_id": "b13", "title": "Modulated phaseshifting for 3D scanning", "journal": "", "year": "", "authors": "T Chen; H.-P Seidel; H P A Lensch"}, {"ref_id": "b14", "title": "Rapid shape acquisition using color structured light and multi-pass dynamic programming", "journal": "", "year": "2002", "authors": "L Zhang; B Curless; S M Seitz"}, {"ref_id": "b15", "title": "Dynamic scene shape reconstruction using a single structured light pattern", "journal": "", "year": "", "authors": "H Kawasaki; R Furukawa; R Sagawa; Y Yagi"}, {"ref_id": "b16", "title": "Non-rigid Photometric Stereo with Colored Lights", "journal": "", "year": "", "authors": "C Hernandez"}, {"ref_id": "b17", "title": "Single-shot photometric stereo by spectral multiplexing", "journal": "", "year": "", "authors": "G Fyffe; X Yu; P Debevec"}, {"ref_id": "b18", "title": "All-frequency shadows using non-linear wavelet lighting approximation", "journal": "", "year": "", "authors": "R Ng; R Ramamoorthi; P Hanrahan"}, {"ref_id": "b19", "title": "Multiple View Geometry in Computer Vision", "journal": "Cambridge University Press", "year": "2000-12", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b20", "title": "A practical model for subsurface light transport", "journal": "", "year": "", "authors": "H Jensen"}, {"ref_id": "b21", "title": "A dual path programmable array microscope (PAM)", "journal": "J. Microscopy", "year": "2001", "authors": "R Heintzmann"}, {"ref_id": "b22", "title": "Binary ranks and binary factorizations of nonnegative integer matrices", "journal": "Electron. J. Linear Algebra", "year": "2012", "authors": "J Zhong"}, {"ref_id": "b23", "title": "P2C2: Programmable pixel compressive camera for high speed imaging", "journal": "", "year": "", "authors": "D Reddy; A Veeraraghavan; R Chellappa"}, {"ref_id": "b24", "title": "Video from a single coded exposure photograph using a learned over-complete dictionary", "journal": "", "year": "", "authors": "Y Hitomi; J Gu; M Gupta; T Mitsunaga; S K Nayar"}, {"ref_id": "b25", "title": "SPGL1: A solver for large-scale sparse reconstruction", "journal": "", "year": "", "authors": "E Van Den; M P Berg;  Friedlander"}, {"ref_id": "b26", "title": "Digital-pixel FPAs enhance infrared imaging capabilities", "journal": "Laser Focus World", "year": "2013", "authors": "M W Kelly; M H Blackwell"}, {"ref_id": "b27", "title": "CMOS Image Sensors With Multi-Bucket Pixels for Computational Photography", "journal": "IEEE J. Solid State Circuits", "year": "2012", "authors": "G Wan"}, {"ref_id": "b28", "title": "Differential Topology", "journal": "", "year": "", "authors": "V Guillemin; A Pollack"}, {"ref_id": "b29", "title": "", "journal": "", "year": "1974", "authors": " Prentice-Hall"}, {"ref_id": "b30", "title": "The Rendering Equation. Proc. SIGGRAPH'86", "journal": "", "year": "", "authors": "J T Kajiya"}, {"ref_id": "b31", "title": "Solid Shape", "journal": "Cambridge University Press", "year": "1990", "authors": "J J Koenderink"}, {"ref_id": "b32", "title": "Principles of Mathematical Analysis", "journal": "McGraw-Hill", "year": "1976", "authors": "W Rudin"}, {"ref_id": "b33", "title": "Primal-dual coding to probe light transport", "journal": "", "year": "", "authors": "M O'toole; R Raskar; K N Kutulakos"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Snapshots from raw live indirect video. Clockwise from top: (1) A hand; note the vein pattern and the inter-reflections between fingers. (2) Pouring water into a glass. (3) Caustics formed inside a mug from specular inter-reflections; note the secondary reflections to the board behind the mug and from the board onto the mug's exterior surface. (4) Refractions and caustics from a beer glass. See Figure9for more images and[8] for videos.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Light transport in a stereo projector-camera system.Light can reach pixel i on the image in one of three general ways: by indirect transport from an arbitrary pixel p on the corresponding epipolar line (green path); by indirect transport from a pixel q that is not on that line (red path); or by direct surface reflection, starting from projector pixel r on the epipolar line (black path).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Structure of an epipolar block Tee. Element Tee[i, r] describes transport from projector pixel pe[r] to image pixel ie[i].This element is direct if and only the scene point projecting to both pixels is the same, i.e., the point's stereo disparity is i \u2212 r. The set of direct elements therefore represents the scene's instantaneous disparity map. Conventional stereo algorithms attempt to localize this set while assuming that the transport matrix is zero everywhere else-both inside and outside its epipolar blocks.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: Experimental validation of non-epipolar dominance for a scene containing diffuse, translucent, refractive and mirror-like objects. Top left: View under an all-white projection pattern. Top middle: View when just one white vertical stripe is projected onto the scene. The many bright regions in this image occur because the stripe illuminates the book's pages in three different ways: (1) directly from the projector, (2) by diffuse inter-reflection from the opposite page, and (3) by specular reflection via the mirror. Their existence makes the scene hard to reconstruct with conventional techniques such as laser-stripe 3D scanning[4]. A magnified view of these regions is shown in the inset. Top right: View for another vertical stripe, part of which falls on the candle. The stripe appears very broad and poorly localized there, because of strong sub-surface scattering. Bottom left: The epipolar block Tee for epipolar line e. We show Tee using the conventions of Figure4, i.e., its r-th column comes from an image of the scene acquired with only projector pixel pe[r] turned on. Bottom middle: To assess the image contribution of non-epipolar transport, we acquire the block sum E f =1 T ef and compare it to block Tee-observe that non-epipolar contributions indeed far surpass the epipolar indirect ones. To acquire the block sum, we capture images of the scene while sweeping a vertical stripe on the projector plane (see[8] for a video of the captured image sequence). The r-th column of the block sum is given by the pixels on epipolar line e when the stripe is at pe[r]. Bottom right: Horizontal cross-section of Tee and E f =1 T ef for two image pixels. Observe that Tee's cross-section (blue) is sharp and unimodal whereas the block sum's (red) is trimodal for one pixel and very broad for the other.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Example layouts for color RGB, monochrome 6pattern, and monochrome 6-pattern indirect-invariant imaging.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Photo of our prototype. The projector can be detached to change the stereo baseline. The optical path is shown in red.See[8] for a detailed list of components.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure9: Row 1: Frames from conventional and epipolar-only video for the scenes in Figure1. Compared to Figure1, the water's opaque appearance in the epipolar-only frame and the absence of caustics in the mug confirm that significant indirect light was not recorded, i.e., non-epipolar dominance holds. Refer to[8] for videos of several more scenes. Row 2: We imaged the scene on the left in two ways: (1) projecting 9 phase-shifted patterns directly onto it and (2) capturing indirect-invariant images for the same patterns. Exposure time was held fixed, giving an SNR advantage to conventional projection which does not mask pixels. We then applied the same algorithm to the two sets of images, with the results shown on the right. The algorithm fails catastrophically for the conventionally-acquired images whereas with SLT imaging it is able to reconstruct even the hidden side of the face, from the mirror's indirect view. Closer inspection of the input images (please zoom in) reveals the reason for the difference: the conventional image contains \"double fringes\" from secondary reflections whereas the indirect-invariant one does not. Row 3: Another example, for a scene with strong diffuse and specular inter-reflections. Row 4: Reconstructing dense depth and albedo from one video frame of a moving hand. From this frame, our demosaicing algorithm recovers 6 full-resolution indirect-invariant images of the hand, for 6 sinusoidal patterns. These images yield the albedo and depth maps on the right.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "be a point on the projector plane and let I \u01eb (p) be an indicator function over D that specifies the spatial extent of the discrete epipolar line through the origin: I \u01eb (p) = 1 if |p x | \u2264 \u01eb 2 and |p y | \u2264 1 0 otherwise .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "E.Discussion of Videos in SLT-supp.zip E.1. Videos in directory figure5 videos/ These videos are meant to be used in conjunction with Figure 5 in the paper.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 11 :11Figure11: Deriving random pattern/mask pairs for three cases of SLT imaging. The derived patterns and masks are indicated with red and green borders, respectively. Row 1: For indirect-only imaging, the patterns and masks are constant along epipolar lines, with approximately half of them \"on.\" Row 2: Six of the nine structured-light patterns we used. Rows 3-4: The masks for indirect-invariant imaging are identical to those for indirect-only imaging but the projection patterns differ. To generate them for a given grayscale structuredlight pattern, we first generate a random sequence of binary patterns (Row 3) and then use that sequence, along with the sequence of masks, to compute the projection patterns. Row 4 shows one such example. Row 5: We generate pattern/mask pairs for 6-shot imaging as follows:(1) create 6 random binary images representing pixel membership for each pattern; (2) generate a sequence of 132 indirect-invariant binary pattern/mask pairs for each of 6 grayscale structured-light patterns, as outlined in Rows 2-4; (3) use the 792 projection patterns as is, and (4) multiply the masks element-wise with the associated pixel memberships. Row 5 shows one such calculation, for grayscale pattern p(1).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "3D Shape and Indirect Appearance By Structured Light Transport:Supplemental Document", "figure_data": "Matthew O'TooleJohn MatherKiriakos N. KutulakosDepartment of Computer ScienceUniversity of Toronto{motoole,jmather,kyros}@cs.toronto.eduA. Proofs of Propositions 1 and 2A.1. Proof of Proposition 1Proposition 1. If T is the discretized form of a transportfunction that is measurable and positive over the rectifiedprojector and image planes, then"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "List of parts for the system shown in Figure8of the paper. The camera outputs live video at a rate of 28 frames per second.Each video frame requires 96 binary masks/projection patterns, i.e., each mask/pattern is active for 375\u00b5sec of the frame's 36000\u00b5sec total exposure time.", "figure_data": "Item #Part DescriptionQuantityModel NameCompany1monochrome camera1GT1920Allied Vision Technologies2high-speed DMD2DLi4130VIS-7XGADigital Light Innovations3high-power LED1High Power S2+ w/ LEDDigital Light Innovations4connector housing2WM1728-NDDigi-Key Corporation5fixed filter holder 40 mm Sq.1#54-997Edmund Optics645 degree mounting adapter1#59-001Edmund Optics"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "i = T p (1", "formula_coordinates": [2.0, 407.88, 411.81, 133.45, 9.96]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [2.0, 541.33, 412.55, 3.91, 8.91]}, {"formula_id": "formula_2", "formula_text": "image i pattern p instantaneous transport matrix T p 1 epipolar line 1 p 2 epipolar line 2 p E epipolar line E epipolar line 1 i 1 epipolar line 2 i 2 epipolar line E i E T 11 T 22 T 21 T EE . . . . . . . . . = \u00d7", "formula_coordinates": [3.0, 45.94, 79.55, 247.28, 130.4]}, {"formula_id": "formula_3", "formula_text": "i = T D p direct image + T EI p epipolar indirect image + T NE p non-epipolar indirect image (2)", "formula_coordinates": [3.0, 89.52, 519.48, 196.79, 30.35]}, {"formula_id": "formula_4", "formula_text": "i \u2248 T D p direct image + T NE p non-epipolar indirect image .(3)", "formula_coordinates": [3.0, 90.24, 684.48, 196.07, 25.55]}, {"formula_id": "formula_5", "formula_text": "lim\u01eb\u21920 T EI p T NE p = 0 (4", "formula_coordinates": [3.0, 386.76, 554.16, 154.79, 21.42]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [3.0, 541.55, 561.78, 3.48, 8.02]}, {"formula_id": "formula_7", "formula_text": "i = [ \u03a0 \u2022 T ] 1 (5)", "formula_coordinates": [4.0, 393.72, 533.85, 151.52, 9.96]}, {"formula_id": "formula_8", "formula_text": "\u03a0 1 (p): projection of pattern p \u03a0 2 (p): indirect-invariant imaging 1p T 1 1p T 1 1p T 1 1p T 1 1p T 2 1p T 2 1p T 2 1p T 2 1p T E 1p T E 1p T E 1p T E . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . . 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 1p T 1 1p T 2 1p T E . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . . \u03a0 3 : indirect-only imaging \u03a0 4 : epipolar-only imaging 00 T 00 T 00 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T 11 T . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . . 11 T 11 T 11 T 00 T 00 T 00 T 00 T 00 T 00 T 00 T 00 T 00 T . . . . . . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . . .", "formula_coordinates": [5.0, 71.04, 71.88, 196.21, 184.62]}, {"formula_id": "formula_9", "formula_text": "ie = E f =1 T ef p f = E f =1 (1p T f ) block of probing matrix \u2022 T ef block of T 1 (6", "formula_coordinates": [5.0, 70.2, 359.03, 212.63, 37.67]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [5.0, 282.83, 368.34, 3.48, 8.02]}, {"formula_id": "formula_11", "formula_text": "ie = (1p T e ) \u2022 Tee 1 direct image (depends on p) + E f =1,f =e T ef 1 non-epipolar indirect image (ambient) .(7)", "formula_coordinates": [5.0, 73.44, 624.11, 212.87, 48.35]}, {"formula_id": "formula_12", "formula_text": "\u03a0 5 (p(1), . . . , p(S)) = S s=1 b(s) 1 T \u2022 \u03a0 2 (p(s)) (8)", "formula_coordinates": [5.0, 321.96, 570.47, 223.07, 27.03]}, {"formula_id": "formula_13", "formula_text": "i = T t=1 m(t) \u2022 [ T q(t) ](9)", "formula_coordinates": [6.0, 118.56, 127.79, 167.75, 26.91]}, {"formula_id": "formula_15", "formula_text": "E [ie] = E [qe] \u2022 E f =1 f =e T ef E [q f ] = 0.25 E f =1 f =e T ef 1 (11", "formula_coordinates": [6.0, 324.24, 447.59, 217.07, 33.39]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [6.0, 541.31, 457.02, 3.72, 8.02]}, {"formula_id": "formula_17", "formula_text": "q(t) = m(t) \u2022 r(t) + m(t) \u2022 r(t)(12)", "formula_coordinates": [7.0, 101.04, 116.64, 185.27, 8.97]}, {"formula_id": "formula_19", "formula_text": "E [ie] = 0.5Teepe + 0.25 E f =1,f =e [T ef p f + T ef (1 \u2212 p f )] = 0.5Teepe direct image (depends on p) + 0.25 E f =1,f =e T ef 1 indirect image (ambient) ,(14)", "formula_coordinates": [7.0, 57.12, 213.23, 229.19, 75.11]}, {"formula_id": "formula_20", "formula_text": "minimize W T i(1) . . . i(S) n (15) subject to S s=1 b(s) \u2022 i(s) \u2212 i 2 \u2264 \u01eb (16)", "formula_coordinates": [7.0, 97.68, 436.39, 188.63, 46.98]}, {"formula_id": "formula_21", "formula_text": "lim \u01eb\u21920 T EI p T NE p = 0 (17", "formula_coordinates": [9.0, 124.68, 296.56, 157.65, 23.86]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [9.0, 282.33, 304.91, 4.19, 8.91]}, {"formula_id": "formula_23", "formula_text": "D = [\u22121, 1] \u00d7 [\u22121, 1] \u2282 \u211c 2 . Let p = (p x , p y )", "formula_coordinates": [9.0, 50.16, 376.77, 236.22, 22.33]}, {"formula_id": "formula_25", "formula_text": "I(i) = T (p, i) P(p) direct + D\u2212{p} T (p, i) P(p) dp indirect (19", "formula_coordinates": [9.0, 55.08, 560.37, 227.25, 33.35]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [9.0, 282.33, 561.11, 4.19, 8.91]}, {"formula_id": "formula_27", "formula_text": "I EI (i) = D\u2212{p} I \u01eb (p) T (p, i) P(p) dp (20", "formula_coordinates": [9.0, 335.76, 237.76, 205.29, 19.71]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [9.0, 541.05, 239.87, 4.19, 8.91]}, {"formula_id": "formula_29", "formula_text": "I I (i) = D\u2212{p} T (p, i) P(p) dp (21", "formula_coordinates": [9.0, 341.16, 266.08, 199.89, 19.71]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [9.0, 541.05, 268.31, 4.19, 8.91]}, {"formula_id": "formula_31", "formula_text": "I NE (i) = I I (i) \u2212 I EI (i) . (22", "formula_coordinates": [9.0, 332.76, 289.84, 208.29, 11.33]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [9.0, 541.05, 291.95, 4.19, 8.91]}, {"formula_id": "formula_33", "formula_text": "I EI (i) I NE (i) < \u03b4 .(23)", "formula_coordinates": [9.0, 397.08, 324.4, 148.16, 24.41]}, {"formula_id": "formula_34", "formula_text": "I EI (i) \u2264 D\u2212{p} I\u01eb(p)dp 1 2 D\u2212{p} T (p, i) P(p) 2 dp 1 2 = (2\u01eb) 1 2 D\u2212{p} T (p, i) P(p) 2 dp 1 2 . (24", "formula_coordinates": [9.0, 302.76, 396.36, 239.07, 55.53]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [9.0, 541.31, 436.38, 3.72, 8.02]}, {"formula_id": "formula_36", "formula_text": "I NE (i) \u2265 D\u2212{p} T (p, i) P(p) dp \u2212 (2\u01eb) 1 2 D\u2212{p} T (p, i) P(p) 2 dp 1 2 . (25", "formula_coordinates": [9.0, 318.84, 505.12, 222.21, 52.47]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [9.0, 541.05, 539.99, 4.19, 8.91]}, {"formula_id": "formula_38", "formula_text": "\u01eb = 1 2 \u03b4 2 + \u03b4 2 D\u2212{p} T (p, i) P(p) dp 2 D\u2212{p} T (p, i) P(p) 2 dp .(26)", "formula_coordinates": [9.0, 315.6, 584.8, 229.64, 44.79]}, {"formula_id": "formula_39", "formula_text": "I EI (i) I NE (i) \u2264 \u03b4 2+\u03b4 1 \u2212 \u03b4 2+\u03b4 = \u03b4 2 < \u03b4 .(27)", "formula_coordinates": [9.0, 357.12, 662.94, 188.12, 29.08]}, {"formula_id": "formula_40", "formula_text": "i e = 1 T T t=1 E f =1 q e (t) \u2022 [ T ef q f (t) ](28)", "formula_coordinates": [10.0, 349.2, 139.74, 196.04, 30.88]}, {"formula_id": "formula_41", "formula_text": "i e = 1 T T t=1 q e (t) \u2022 [ T ee q e (t) ] + E f =1 f =e q e (t) \u2022 [ T ef q f (t) ](29)", "formula_coordinates": [10.0, 318.84, 220.86, 226.4, 72.88]}, {"formula_id": "formula_42", "formula_text": "i e = 1 T T t=1 E f =1 f =e q e (t) \u2022 [ T ef q f (t) ] .(30)", "formula_coordinates": [10.0, 345.36, 327.78, 199.88, 37.96]}, {"formula_id": "formula_43", "formula_text": "E[i e ] = E E f =1 f =e q e \u2022 [ T ef q f ](31)", "formula_coordinates": [10.0, 354.84, 400.86, 190.4, 38.08]}, {"formula_id": "formula_44", "formula_text": "= E[q e ] \u2022 E E f =1 f =e T ef q f (32) = E[q e ] \u2022 E f =1 f =e T ef E[q f ](33)", "formula_coordinates": [10.0, 379.2, 444.54, 166.04, 81.88]}, {"formula_id": "formula_45", "formula_text": "= 0.25 E f =1 f =e T ef 1 ,(34)", "formula_coordinates": [10.0, 379.2, 532.14, 166.04, 37.96]}, {"formula_id": "formula_46", "formula_text": "ie = 1 T T t=1 E f =1 me(t) \u2022 T ef [ m f (t) \u2022 r f (t) + m f (t) \u2022 r f (t) ](35)", "formula_coordinates": [10.0, 308.88, 674.15, 242.44, 38.8]}, {"formula_id": "formula_47", "formula_text": "i e = 1 T T t=1 m e (t) \u2022 T ee [ m e (t) \u2022 r e (t) ] + m e (t) \u2022 T ee [ m e (t) \u2022 r e (t) ] + E f =1 f =e m e (t) \u2022 T ef [ m f (t) \u2022 r f (t) ] + E f =1 f =e m e (t) \u2022 T ef [ m f (t) \u2022 r f (t) ] (36", "formula_coordinates": [11.0, 56.64, 103.5, 225.69, 133.72]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [11.0, 282.33, 209.87, 4.19, 8.91]}, {"formula_id": "formula_49", "formula_text": "E[i e ] = E q e \u2022 T ee (q e \u2022 r e ) + E f =1 f =e q e \u2022 T ef (q f \u2022 r f + q f \u2022 r f ) .(37)", "formula_coordinates": [11.0, 60.12, 304.29, 226.4, 59.66]}, {"formula_id": "formula_50", "formula_text": "E[i e ] = 0.5 T ee E[r e ] + 0.5 E f =1 f =e T ef E[q f \u2022 r f + q f \u2022 r f ] . (38)", "formula_coordinates": [11.0, 60.12, 427.41, 226.4, 53.89]}, {"formula_id": "formula_51", "formula_text": "E[i e ] = 0.5 T ee p e + 0.5 E f =1 f =e T ef Prob[ q f = 1 ] p e + Prob[ q f = 0 ] (1 \u2212 p e )(39)", "formula_coordinates": [11.0, 60.12, 520.77, 226.4, 70.22]}, {"formula_id": "formula_52", "formula_text": "E[i e ] = 0.5 T ee p e + 0.25 E f =1 f =e T ef ( p e + (1 \u2212 p e )(40)", "formula_coordinates": [11.0, 50.16, 615.3, 236.36, 50.59]}, {"formula_id": "formula_53", "formula_text": "E f =1 f =e T ef 1 .(41)", "formula_coordinates": [11.0, 170.88, 672.06, 115.64, 37.96]}], "doi": ""}