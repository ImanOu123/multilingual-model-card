{"title": "A Probabilistic Framework for Semi-Supervised Clustering", "authors": "Sugato Basu; Mikhail Bilenko; Raymond J Mooney", "pub_date": "", "abstract": "Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints, i.e., pairs of instances labeled as belonging to same or different clusters. In recent years, a number of algorithms have been proposed for enhancing clustering quality by employing such supervision. Such methods use the constraints to either modify the objective function, or to learn the distance measure. We propose a probabilistic model for semisupervised clustering based on Hidden Markov Random Fields (HMRFs) that provides a principled framework for incorporating supervision into prototype-based clustering. The model generalizes a previous approach that combines constraints and Euclidean distance learning, and allows the use of a broad range of clustering distortion measures, including Bregman divergences (e.g., Euclidean distance and I-divergence) and directional similarity measures (e.g., cosine similarity). We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model. Experimental results on several text data sets demonstrate the advantages of the proposed framework.", "sections": [{"heading": "INTRODUCTION", "text": "Large amounts of unlabeled data are available in many real-life data-mining tasks, e.g., uncategorized messages in an automatic email classification system, genes of unknown functions for doing gene function prediction, etc. Labeled data is often limited and expensive to generate, since labeling typically requires human expertise. Consequently, semi-supervised learning, which uses both labeled and unlabeled data, has become a topic of significant recent interest [11,24,33]. In this paper, we focus on semi-supervised clustering, where the performance of unsupervised clustering algorithms is improved with limited amounts of supervision in the form of labels on the data or constraints [38,6,27,39,7].\nExisting methods for semi-supervised clustering fall into two general categories which we call constraint-based and distancebased. Constraint-based methods rely on user-provided labels or constraints to guide the algorithm towards a more appropriate data partitioning. This is done by modifying the objective function for evaluating clusterings so that it includes satisfying constraints [15], enforcing constraints during the clustering process [38], or initializing and constraining the clustering based on labeled examples [6]. In distance-based approaches, an existing clustering algorithm that uses a particular clustering distortion measure is employed; however, it is trained to satisfy the labels or constraints in the supervised data. Several adaptive distance measures have been used for semisupervised clustering, including string-edit distance trained using Expectation Maximization (EM) [10], KL divergence trained using gradient descent [13], Euclidean distance modified by a shortestpath algorithm [27], or Mahalanobis distances trained using convex optimization [39].\nWe propose a principled probabilistic framework based on Hidden Markov Random Fields (HMRFs) for semi-supervised clustering that combines the constraint-based and distance-based approaches in a unified model. We motivate an objective function for semi-supervised clustering derived from the posterior energy of the HMRF framework, and propose a EM-based partitional clustering algorithm HMRF-KMEANS to find a (local) minimum of this objective function. Previously, we proposed a unified approach to semi-supervised clustering that was experimentally shown to produce more accurate clusters than other methods on several data sets [8]. However, this approach is restricted to using Euclidean distance as the clustering distortion measure. In this paper, we show how to generalize that model to handle non-Euclidean measures. Our generalization can utilize any Bregman divergence [3], which includes a wide variety of useful distances, e.g., KL divergence. In a number of applications, such as text-clustering using a vector-space model, a directional similarity measure based on the angle between vectors is more appropriate [1]. Consequently, clustering algorithms that utilize distortion measures appropriate for directional data have recently been developed [18,2]. Our unified semi-supervised clustering framework based on HMRFs is also applicable to such directional similarity measures.\nTo summarize, the proposed approach aids unsupervised clustering by incorporating labeled data in the following three ways:\n\u2022 Improved initialization, where initial cluster centroids are estimated from the neighborhoods induced from constraints; \u2022 Constraint-sensitive assignment of instances to clusters, where points are assigned to clusters so that the overall distortion of the points from the cluster centroids is minimized, while a minimum number of must-link and cannot-link constraints are violated; \u2022 Iterative distance learning, where the distortion measure is re-estimated during clustering to warp the space to respect user-specified constraints as well as to incorporate data variance. We present experimental results on clustering text documents that demonstrate the advantages of our approach.", "publication_ref": ["b10", "b23", "b32", "b37", "b5", "b26", "b38", "b6", "b14", "b37", "b5", "b9", "b12", "b26", "b38", "b7", "b2", "b0", "b17", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Motivation of Framework", "text": "In this work, we will focus on partitional prototype-based clustering as our underlying unsupervised clustering model, where a set of data points is partitioned into a pre-specified number of clusters (each cluster having a representative or prototype) so that a well-defined cost function, involving a distortion measure between the points and the cluster representatives, is minimized. A popular clustering algorithm in this category is K-Means [29].\nEarlier research on semi-supervised clustering has considered supervision in the form of labeled points [6] or constraints [38,39,5]. In this paper, we will be considering the model where supervision is provided in the form of must-link and cannot-link constraints, indicating respectively that a pair of points should be or should not be put in the same cluster. For each pairwise constraint, the model assigns an associated cost of violating that constraint. Considering supervision in the form of constraints is more realistic than requiring class labels in many unsupervised-learning applications, e.g. clustering for speaker identification in a conversation [5], or clustering GPS data for lane-finding [38]: while class labels may be unknown, a user can still specify whether pairs of points belong to same or different clusters. Constraint-based supervision is also more general than class labels: a set of classified points implies an equivalent set of pairwise constraints, but not vice versa.\nOur semi-supervised clustering model considers a set of data points X with a specified distortion measure D between the points. Supervision is provided as a set M of must-link constraints (with a set of associated violation costs W ) and a set C of cannot-link constraints (with associated violation costs W ). The task is to partition the data into K clusters so that the total distortion between the points and the corresponding cluster representatives according to the given measure D is minimized while a minimum number of constraints are violated. Since we restrict our attention to hard clustering, every point is assigned to a single cluster in our model. A word on the notation and terminology used in this paper: boldface variables, e.g., x, represent vectors; calligraphic upper-case alphabets, e.g., X , refer to sets, whose representatives are enumerated as {x i } N i=1 (except J , which always denotes an objective function);\nx im represents the m th component of the d-dimensional vector x i . The term \"distance measure\" is used synonymously with \"distortion measure\" throughout the paper.", "publication_ref": ["b28", "b5", "b37", "b38", "b4", "b4", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Hidden Markov Random Field", "text": "To incorporate pairwise constraints along with an underlying distortion measure between points into a unified probabilistic model, we consider Hidden Markov Random Fields (HMRFs). An HMRF has the following components:\n\u2022 A hidden field L = {l i } N\ni=1 of random variables, whose values are unobservable. In the clustering framework, the set of hidden variables are the unobserved cluster labels on the points, indicating cluster assignments. Every hidden variable l i takes values from the set {1, . . . , K}, which are the indices of the clusters.\n\u2022 An observable set X = {x i } N i=1 of random variables, where every random variable x i is generated from a conditional probability distribution Pr(x i |l i ) determined by the corresponding hidden variable l i . The random variables X are conditionally independent given the hidden variables L, i.e., Pr(X |L) = \u220f x i \u2208X Pr(x i |l i ). In our framework, the set of observable variables for the HMRF corresponds to the given data points.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MustLink", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Observed data", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MustLink", "text": "Hidden MRF Fig. 1 shows a simple example of an HMRF. The observed dataset X consists of six points {x 1 . . . x 6 }, which have corresponding cluster labels {l 1 . . . l 6 }. Two must-link constraints are provided between (l 1 ,l 3 ) and (l 1 ,l 4 ), while one cannot-link constraint is provided between (l 3 ,l 6 ). The task is to partition the six points into three clusters. One clustering configuration is shown in Fig. 1. The must-linked points x 1 , x 3 and x 4 are put in cluster 1; the point x 6 , which is cannot-linked to x 3 , is assigned to cluster 2; x 2 and x 5 , which are not involved in any constraints, are put in clusters 1 and 3 respectively. Each hidden random variable l i has an associated set of neighbors N i . The must-link constraints M and cannot-link constraints C define the neighborhood over the hidden labels, such that the neighbors of a point x i are all points that are must-linked or cannotlinked to it. The random field defined over the hidden variables is a Markov Random Field, where the probability distribution of the hidden variables obeys the following Markov property:\nCannotLink x 6 x 5 x 1 x 4 l 1 = 1 l 5 = 3 l 4 = 1 x 2 x 3 l 2 = 1 l 3 = 1 l 6 = 2\n\u2200i, Pr(l i |L \u2212 {l i }) = Pr(l i |{l j : l j \u2208 N i })(1)\nSo, the probability distribution of the value of l i for the data point x i depends only on the cluster labels of the points that are must-linked or cannot-linked to x i .\nLet us consider a particular cluster label configuration L to be the joint event L = {l i } N i=1 . By the Hammersley-Clifford theorem [22], the probability of a label configuration can be expressed as a Gibbs distribution [21], so that\nPr(L) = 1 Z 1 exp \u2212V (L) = 1 Z 1 exp \u2212 \u2211 N i \u2208N V N i (L)(2)\nwhere N is the set of all neighborhoods, Z 1 is a normalizing constant, and V (L) is the overall label configuration potential function, which can be decomposed into the functions V N i (L) denoting the potential for every neighborhood N i in the label configuration L.\nSince we are provided with pairwise constraints over the class labels, we restrict the MRFs over the hidden variable to have pairwise potentials. The prior probability of a configuration of cluster labels L then becomes Pr(L) = 1\nZ 1 exp(\u2212 \u2211 i \u2211 j V (i, j)), where V (i, j) = \uf8f1 \uf8f2 \uf8f3 f M (x i , x j ) if (x i , x j ) \u2208 M f C (x i , x j ) if (x i , x j ) \u2208 C 0 otherwise (3)\nHere, f M (x i , x j ) is a non-negative function that penalizes the violation of a must-link constraint, and f C (x i , x j ) is the corresponding penalty function for cannot-links. Note that the third condition in the definition of V (i, j) is necessary since not all points are involved in the constraints. Intuitively, this form of Pr(L) gives higher probabilities to label configurations that satisfy most of the must-link constraints M and cannot-link constraints C , thereby discouraging the violation of the user-specified constraints.", "publication_ref": ["b21", "b20"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "MAP Estimation in HMRFs", "text": "Given a particular configuration of the hidden variables (unknown cluster labels), the variables in the observable field of the HMRF (the data points) are generated using specified conditional probability distributions. The conditional probability of the observation\nset X = {x i } N i=1 for a given configuration L = {l i } N\ni=1 is given by Pr(X |L), which in the clustering framework is of the form:\nPr(X |L) = p(X , {\u00b5 \u00b5 \u00b5 h } K h=1 )(4)\nwhere p(X , {\u00b5 \u00b5 \u00b5 h } K h=1 ) is a probability density function parameterized by the cluster representatives {\u00b5 \u00b5 \u00b5 h } K h=1 . This function is related to the clustering distortion measure D, as we will show in Section 2.4.\nThe overall posterior probability of a cluster label configuration L is Pr(L|X ) \u221d Pr(L)Pr(X |L), considering Pr(X ) to be a constant C. Hence, finding the maximum a-posteriori (MAP) configuration of the HMRF becomes equivalent to maximizing the posterior probability:\nPr(L|X ) = 1 Z 2 exp \u2212 \u2211 i \u2211 j V (i, j) \u2022 p(X , {\u00b5 \u00b5 \u00b5 h } K h=1 )(5)\nwhere Z 2 = CZ 1 . The negative logarithm of Pr(L|X ) is known as posterior energy. Note that MAP estimation would reduce to maximum likelihood (ML) estimation of Pr(X |L) if Pr(L) is constant. However, because our model accounts for dependencies between the cluster labels and Pr(L) is not constant, full MAP estimation of Pr(L|X ) is required. Since the cluster representatives as well as the cluster labels for the points are unknown in a clustering setting, maximizing Eqn.( 5) is an \"incomplete-data problem\", for which a popular solution method is Expectation Maximization (EM) [16]. It is well-known that K-Means is equivalent to an EM algorithm with hard clustering assignments [26,6,3]. Section 3.2 describes a K-Means-type hard partitional clustering algorithm, HMRF-KMEANS, that finds a (local) maximum of the above function.\nThe posterior probability Pr(L|X ) in Eqn.(5) has 2 components: the first factor evaluates each label configuration, corresponding to cluster assignments of every point, and gives a higher probability to a configuration that satisfies more of the given must-link and cannot-link constraints. A particular label configuration determines the cluster assignments and hence the cluster representatives. The second factor estimates the probability of generating the observed data points using the conditional distributions, which are parameterized by the cluster representatives and depend on the distortion measure. The overall posterior probability of the cluster label configuration of all the points therefore takes into account both the cluster distortion measure and the constraints in a principled unified framework.", "publication_ref": ["b15", "b25", "b5", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Clustering Objective Function", "text": "Eqn.(5) suggests a general framework for incorporating constraints into clustering. Particular choices of the constraint penalty functions f M and f C , and the conditional probabilities p(X , {\u00b5 \u00b5 \u00b5 h } K i=1 ) would be motivated by the distortion measure appropriate for the clustering task.\nWhen considering the second term in Eqn.( 5), we restrict our attention to probability densities of the exponential form:\np(X , {\u00b5 \u00b5 \u00b5 h } K h=1 ) = 1 Z 3 exp \u2212 \u2211 x i \u2208X D(x i ,\u00b5 \u00b5 \u00b5 l i )(6)\nwhere D(x i ,\u00b5 \u00b5 \u00b5 l i ) is the distortion between x i and \u00b5 \u00b5 \u00b5 l i , and Z 3 is a normalization constant. Different clustering models fall into this exponential form:\n\u2022 x i and \u00b5 \u00b5 \u00b5 l i are vectors and D is the square of the L 2 norm: the cluster conditional probability is a unit variance Gaussian [26];\n\u2022 x i and \u00b5 \u00b5 \u00b5 l i are probability distributions and D is the KL-divergence: the cluster conditional probability is a multinomial distribution [17];\n\u2022 x i and \u00b5 \u00b5 \u00b5 l i are vectors of unit length (according to the L 2 norm) and D is one minus the dot-product: the cluster conditional probability is a von-Mises Fisher (vMF) distribution with unit concentration parameter [2], which is essentially the spherical analog of a unit variance Gaussian.\nWe will discuss the connection between specific distortion measures that we will study in this paper and their corresponding cluster conditional probabilities in more detail in Section 3.1.\nLet us now examine the potential function V in the first term of Eqn. (5). In previous work, only must-linked points were considered in the neighborhood of a Markov Random Field with the generalized Potts potential function [12,28]. In this potential function, the must-link penalty is f M (x i , x j ) = w i j [l i = l j ], where w i j is the cost for violating the must-link constraint (i, j), and is the indicator function ( [true] = 1, [false] = 0). This function specifies that the cost of violating a must-link constraint (x i , x j ) is w i j irrespective of the distance between x i and x j .\nIn a semi-supervised clustering framework where we want to use the constraint violations to learn the underlying distance measure, the penalty for violating a must-link constraint between distant points should be higher than that between nearby points. This would reflect the fact that if two must-linked points are far apart according to the current distortion measure and are hence put in different clusters, the measure is inadequate and needs to be modified to bring those points closer together. So, the must-link penalty function is chosen to be\nf M (x i , x j ) = w i j \u03d5 D (x i , x j ) [l i = l j ](7)\nwhere \u03d5 D is the penalty scaling function, which we choose to be a monotonically increasing function of the distance between x i and x j according to the current distortion measure. Specific penalty functions \u03d5 D for different distortion measures D are described in Section 3.1. Analogously, the penalty for violating a cannot-link constraint between two points that are nearby according to the current distance measure should be higher than for two distant points. This would encourage the distance learning step to put cannot-linked points farther apart. The cannot-link penalty function can be accordingly chosen to be\nf C (x i , x j ) = w i j \u03d5 D max \u2212 \u03d5 D (x i , x j ) [l i = l j ](8)\nwhere \u03d5 D max is the maximum value of the scaling function \u03d5 D for the dataset. This form of f C ensures that the penalty for violating a cannot-link constraint remains non-negative, since the second term is never greater than the first. Note that these f M and f C penalty functions make the MRF over the hidden variables non-isotropic (i.e., the values of the potential between pairs of random variables in the field are non-uniform), but the overall model is still a valid HMRF.\nPutting this into Eqn.( 5) and taking logarithms gives the following cluster objective function, minimizing which is equivalent to maximizing the MAP probability in Eqn.( 5), or equivalently, minimizing the posterior energy of the HMRF:\nJ obj = \u2211 x i \u2208X D(x i ,\u00b5 \u00b5 \u00b5 l i ) + \u2211 (x i ,x j )\u2208M w i j \u03d5 D (x i , x j ) [l i = l j ] + \u2211 (x i ,x j )\u2208C w i j \u03d5 D max \u2212 \u03d5 D (x i , x j ) [l i = l j ] + log Z (9)\nwhere Z = Z 2 Z 3 . Thus, the task is to minimize J obj over {\u00b5 \u00b5 \u00b5 h } K h=1 , L, and D (if the latter is parameterized).", "publication_ref": ["b25", "b16", "b1", "b4", "b11", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "ALGORITHM", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adaptive Distortion Measures", "text": "The choice of a distortion measure D for a particular clustering problem depends on the properties of the domain under consideration. A number of popular distortion measures, including Euclidean distance and Kullback-Leibler divergence, belong to a general family of functions known as Bregman divergences [3]. Another popular class of distortion measures includes directional similarity functions such as normalized dot product (cosine similarity) and Pearson's correlation [31]. Selection of the most appropriate distortion measure for a clustering task should take into account intrinsic properties of the dataset. For example, Euclidean distance is most appropriate for low-dimensional data with distribution close to the normal distribution, while normalized dot product best captures similarity of directional data where differences in angles between vectors are important, while vector lengths are not. For Bregman divergences and directional similarity measures like cosine similarity, it has been shown that there exist efficient K-Means-type iterative relocation algorithms that minimize the corresponding clustering cost functions [2,3].\nFor many realistic datasets, off-the-shelf distortion measures may fail to capture the correct notion of similarity in a clustering setting. Unsupervised measures like Mahalanobis distance and Pearson correlation attempt to correct similarity estimates using the global mean and variance of the dataset. However, these measures may still fail to estimate distances accurately if the attributes' true contribution to similarity is not correlated with their variance. Recently, several semi-supervised clustering approaches have been proposed that incorporate adaptive similarity functions, including parameterization of Jensen-Shannon divergence [13] and Euclidean distance [5,39]. In initial work [8], we have shown how Euclidean distance can be parameterized and learned in a principled manner in a semi-supervised clustering setting. We now turn to two other popular distortion measures, cosine similarity and Kullback-Leibler divergence, and describe how their adaptive versions can be used as distortion measures in our HMRF-based framework.", "publication_ref": ["b2", "b30", "b1", "b2", "b12", "b4", "b38", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Parameterized Cosine Similarity", "text": "Cosine similarity can be parameterized using a symmetric positivedefinite matrix A, which leads to the following distortion measure:\nD cos A (x i , x j ) = 1 \u2212 x T i Ax j x i A x j A (10)\nwhere x A is the weighted L 2 norm: x A = \u221a x T Ax. Such parameterization is equivalent to projecting every instance x onto a space spanned by A 1/2 : x \u2192 A 1/2 x. Since unparameterized cosine similarity is a natural measure for prototype-based clustering under the assumption that the data is generated by a mixture of von Mises-Fisher (vMF) distributions [2], D cos A (x i , x j ) can be thought of as a distortion measure for data generated by a mixture of vMF distributions in the projected space. Because for realistic high-dimensional domains computing the full matrix A would be extremely expensive computationally, we focus our attention on diagonal A, which is equivalent to using a vector of weights a = diag(A). Therefore, from now on we will be referring to the cosine measure in Eqn. (10) as D cos a (x i , x j ).\nTo use D cos a (x i , x j ) as the distortion measure in the clustering framework described in Section 2.4, we also use it as the penalty scaling function \u03d5 D (x i , x j ) = D cos a (x i , x j ), which leads to the following objective function:\nJ cos a = \u2211 x i \u2208X D cos a (x i ,\u00b5 \u00b5 \u00b5 l i ) + \u2211 (x i ,x j )\u2208M w i j D cos a (x i , x j ) [l i = l j ](11)\n+ \u2211 (x i ,x j )\u2208C w i j D cos a max \u2212 D cos a (x i , x j ) [l i = l j ] + log Z\nwhere D cos a max = 1.", "publication_ref": ["b1", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Parameterized I-Divergence", "text": "In certain domains, data is described by probability distributions, e.g. text documents can be represented as probability distributions over words generated by a multinomial model [35]. KL-divergence is a widely used distance measure for such data: x jm . It can be shown that after such parameterization D KL is no longer a Bregman divergence over probability distributions, which is undesirable since convergence is no longer guaranteed for the algorithm described in [13].\nD KL (x i , x j ) = \u2211 d m=1 x\nInstead of KL-divergence, we employ a related measure, I-divergence, which also belongs to the class of Bregman divergences [3]. I-divergence has the following form:\nD I (x i , x j ) = \u2211 d m=1 x im log x im x jm \u2212 \u2211 d m=1 (x im \u2212 x jm )\n; x i and x j no longer need to be probability distributions but can be any non-negative vectors. For probability distributions, I-divergence and KL-divergence are equivalent. We parameterize I-divergence by a vector of non-negative weights a:\nD I a (x i , x j ) = d \u2211 m=1 a m x im log x im x jm \u2212 d \u2211 m=1 a m (x im \u2212 x jm )(12)\nSuch parameterization can be thought of as scaling every attribute in the original space by a weight contained in the corresponding component of a, and then taking I-divergence in the transformed space. This implies that D I a is a Bregman divergence with respect to the transformed space.\nThe clustering framework described in Section 2.4 requires us to define an appropriate penalty scaling function \u03d5 D (x i , x j ) to be used in the HMRF potential functions as described in Eqns.( 3) and (7)(8). Since we consider unordered constraint pairs, \u03d5 D (x i , x j ) must be symmetric to penalize constraints appropriately. To meet this requirement, we will use a sum of weighted I-divergences from x i and x j to the mean vector\nx i +x j 2 .\nThis \"I-divergence to the mean\", D IM a , is analogous to Jensen-Shannon divergence, which is the symmetric \"KL-divergence to the mean\" [14], and is defined as follows:\n\u03d5 D (x i , x j ) = D IM a (x i , x j ) = d \u2211 m=1\na m x im log 2x im x im + x jm + x jm log 2x jm x im + x jm (13) This formulation leads to the following objective function:\nJ I a = \u2211 x i \u2208X D I a (x i ,\u00b5 \u00b5 \u00b5 l i ) + \u2211 (x i ,x j )\u2208M w i j D IM a (x i , x j ) [l i = l j ] (14) + \u2211 (x i ,x j )\u2208C w i j D IM a max \u2212 D IM a (x i , x j ) [l i = l j ] + log Z\nThe two parameterized distortion measures D cos a and D IM a have underlying generative models: weighted cosine corresponds to a von-Mises Fisher (vMF) distribution in the projected space, while I-divergence corresponds to multinomial distributions with rescaled probabilities. Thus, Pr(X |L) in Eqn.( 4) is well-defined for the underlying HMRF model in both these cases, and minimizing objective functions J cos a and J I a leads to maximizing Pr(L|X ) for the corresponding underlying models.", "publication_ref": ["b34", "b12", "b2", "b6", "b7", "b13", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "EM Framework", "text": "As discussed in Section 2.2, J obj can be minimized by a K-Means-type iterative algorithm HMRF-KMEANS. The outline of the algorithm is presented in Fig. 2. The basic idea of HMRF-KMEANS is as follows: in the E-step, given the current cluster representatives, every data point is re-assigned to the cluster which minimizes its contribution to J obj . In the M-step, the cluster repre- Algorithm: HMRF-KMeans Input: Set of data points\nX = {x i } N i=1 , number of clusters K, set of must-link constraints M = {(x i , x j )}, set of cannot-link constraints C = {(x i , x j )}, distance measure D, constraint violation costs W and W. Output: Disjoint K-partitioning {X h } K\nh=1 of X such that objective function J obj in Eqn.( 9) is (locally) minimized. Method:\n1. Initialize the K clusters centroids {\u00b5 \u00b5 \u00b5 (0) h } K h=1 , set t \u2190 0 2. Repeat until convergence 2a. E-step: Given {\u00b5 \u00b5 \u00b5 (t) h } K h=1 , re-assign cluster labels {l (t+1) i } N i=1 on the points {x i } N i=1 to minimize J obj . 2b. M-step(A): Given cluster labels {l (t+1) i } N i=1 , re-calculate cluster centroids {\u00b5 \u00b5 \u00b5 (t+1) h } K\nh=1 to minimize J obj . 2c. M-step(B): Re-estimate distance measure D to reduce J obj . 2d. t \u2190 t+1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Figure 2: HMRF-KMEANS algorithm", "text": "Note that calculating the normalizing constant Z in Eqn.( 9) is computationally intensive for most distortion measures, e.g. for cosine similarity, this corresponds to computing a Bessel function [2]. So, we make an approximation by considering log Z to be constant throughout the clustering iterations, and hence drop that term from Eqn.(9).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Initialization", "text": "Good initial centroids are essential for the success of partitional clustering algorithms such as K-Means. In previous work, it was shown that using limited supervision in the form of labeled points results in good initial centroids for partitional clustering [6]. In our case, supervision is provided as pairwise constraints instead of labeled points. However, we follow the same motivation of inferring good initial centroids from the constraints.\nWe try to utilize both the constraints and unlabeled data during initialization. For this, we follow a two stage initialization process.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Neighborhood inference:", "text": "We begin by taking the transitive closure of the must-link constraints to get connected components consisting of points connected by must-links. Let there be \u03bb connected components, which are used to create \u03bb neighborhoods {N p } \u03bb p=1 . These define the must-link neighborhoods in the MRF over the hidden cluster variables.\nAssuming consistency of the constraints, we then infer additional constraints from the neighborhoods. We augment the set M with the must-link constraints inferred from the transitive closure that were not in the initial set. For each pair of neighborhoods N p and N p that have at least one cannot-link between them, we add cannot-link constraints between every pair of points in N p and N p and augment the cannot-link set C with these entailed constraints.\nThis step corresponds to inferring as much information as possible about the neighborhood structure of the hidden MRF, under the assumption of consistency of the constraints.\nFrom this point onwards in the paper, we will overload notation and refer to the augmented must-link and cannot-link sets as M and C respectively. Note that if we know that the given set of constraints are noisy, implying that the constraints are not consistent, we will not add these additional inferred constraints to M and C and only work with the constraints provided initially.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cluster selection:", "text": "The first stage produces \u03bb neighborhood sets {N p } \u03bb p=1 . These neighborhoods are used as initial clusters for the HMRF-MEANS algorithm. If \u03bb = K, \u03bb cluster centers are initialized with the centroids of all the \u03bb neighborhood sets. If \u03bb < K, \u03bb clusters are initialized from the neighborhoods, and the remaining K \u2212 \u03bb clusters are initialized with points obtained by random perturbations of the global centroid of X .\nIf \u03bb > K, K neighborhoods are selected as initial clusters using the clustering distortion measure. Farthest-first traversal is a good heuristic for initialization in prototype-based partitional clustering algorithms [23]. The goal in farthest-first traversal is to find K points that are maximally separated from each other in terms of a given distance function. In our case, we apply a weighted variant of farthest-first traversal to the centroids of the \u03bb neighborhoods, where the weight of each centroid is proportional to the size of the corresponding neighborhood. We consider the weighted distance between two centroids to be the distance between them according to the distortion measure multiplied by the weights of the two centroids. Thus, weighted farthest-first is biased to select centroids that are relatively far apart as well as large in size.\nDuring weighted farthest first selection, the algorithm maintains a set of centroids that have been visited so far. The centroid of the largest neighborhood is selected as the starting point and added to the visited set. At every point in the algorithm, the unvisited centroid with the farthest weighted distance (smallest weighted similarity) from the visited set is chosen. If there is a tie, it is resolved by selecting the centroid farthest from the global centroid of the data. This point is added to the visited set, and the process is continued till K centroids are visited. Finally, the K neighborhood centroids chosen by weighted farthest-first traversal are set as the K initial cluster centroids for HMRF-KMEANS.\nOverall, this two-stage initialization procedure is able to take into account both unlabeled and labeled data to obtain cluster representatives that provide a good initial partitioning of the dataset.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "E-step", "text": "In the E-step, assignments of data points to clusters are updated using the current estimates of the cluster representatives. In simple K-Means there is no interaction between the cluster labels, and the E-step is a simple assignment of every point to the cluster representative that is nearest to it according to the clustering distortion measure. In contrast, the HMRF model incorporates interaction between the cluster labels defined by the random field over the hidden variables. As a result, computing the assignment of data points to cluster representatives to minimize the objective function is computationally intractable in any non-trivial HMRF model [36].\nThere exist several techniques for computing cluster assignments that approximate the optimal solution in this framework, e.g., iterated conditional modes (ICM) [9,40], belief propagation [34,36], and linear programming relaxation [28]. We follow the ICM approach, which is a greedy strategy to sequentially update the cluster assignment of each point, keeping the assignments for the other points fixed.\nThe algorithm performs cluster assignments in random order for all points. Each point x i is assigned to the cluster representative \u00b5 \u00b5 \u00b5 h that minimizes the point's contribution to the objective function J ob j (x i ,\u00b5 \u00b5 \u00b5 h ):\nJ obj (x i ,\u00b5 \u00b5 \u00b5 h ) = D(x i ,\u00b5 \u00b5 \u00b5 h ) + \u2211 (x i ,x j )\u2208M w i j \u03d5 D (x i , x j ) [h = l j ] + \u2211 (x i ,x j )\u2208C w i j \u03d5 D max \u2212 \u03d5 D (x i , x j ) [h = l j ](15)\nOptimal assignment for every point is that which minimizes the distortion between the point and its cluster representative (first term of J obj ) along with incurring a minimal penalty for constraint violations caused by this assignment (second and third terms of J obj ).\nAfter all points are assigned, they are randomly re-ordered, and the assignment process is repeated. This process proceeds until no point changes its cluster assignment between two successive iterations. ICM is guaranteed to reduce J obj or keep it unchanged (if J obj is already at a local minimum) in the E-step [9].\nOverall, the assignment of points to clusters incorporates pairwise supervision by discouraging constraint violations proportionally to their severity, which guides the algorithm towards a desirable partitioning of the data.", "publication_ref": ["b35", "b8", "b39", "b33", "b35", "b27", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "M-step", "text": "The M-step of the algorithm consists of two parts. First, cluster representatives {\u00b5 \u00b5 \u00b5 h } K h=1 are re-estimated from points currently assigned to them to decrease the objective function J obj in Eqn. (9). It has recently been shown that for Bregman divergences each cluster representative calculated in the M-step of the EM algorithm is equivalent to the expectation value over the points in that cluster, which is essentially their arithmetic mean [3]. Additionally, it has been experimentally demonstrated that for distribution-based clustering, smoothing cluster representatives by a prior using a deterministic annealing schedule leads to considerable improvements [17]. With smoothing controlled by a parameter \u03b1, each cluster representative \u00b5 \u00b5 \u00b5 h is estimated as follows when D I a is the distortion measure:\n\u00b5 \u00b5 \u00b5 (I a ) h = 1 1 + \u03b1 \u2211 x i \u2208X h x i |X h | + \u03b1 1 n (16)\nFor directional measures, each cluster representative is the arithmetic mean projected onto unit sphere [2]. Taking the weighting into account, centroids are estimated as follows when D cos a is the distortion measure:\n\u00b5 \u00b5 \u00b5 (cos a ) h = \u2211 x i \u2208X h x i \u2211 x i \u2208X h x i A (17\n)\nSince constraints do not take part in cluster representative reestimation, this step remains the same as in K-Means for Bregman divergences, and the same as in SPKMEANS for weighted cosine similarity [18].\nSecond, if a parameterized variant of a distortion measure is used, e.g. D cos a or D I a shown above, the distortion measure parameters must be updated to decrease the objective function. For certain distance measure parameterizations, minimization via taking partial derivatives and solving for the parameter values may be feasible, e.g. for Euclidean distance [8]. In general, however, a closed-form solution may be unattainable. In such cases, gradient descent provides an alternative avenue for learning distortion measure weights. For the two distortion measures described above, D cos a and D I a , every weight a m would be updated using the update rule a m = a m + \u03b7 \u2202J obj \u2202a m , where:\n\u2202J obj \u2202a m = \u2211 x i \u2208X \u2202D(x i ,\u00b5 \u00b5 \u00b5 l i ) \u2202a m + \u2211 (x i ,x j )\u2208M w i j \u2202D(x i , x j ) \u2202a m [l i = l j ](18)\n+ \u2211\n(x i ,x j )\u2208C w i j \u2202D max \u2202a m \u2212 \u2202D(x i , x j ) \u2202a m [l i = l j ]\nFor the two particular distortion measures that we are considering, D cos a and D I a , gradients\n\u2202D(x i ,x j )\n\u2202a m are the following:\n\u2202D cos a (x i , x j ) \u2202a m = x im x jm x i A x j A \u2212 x T i Ax j x 2 im x j 2 A +x 2 jm x i 2 A 2 x i A x j A x i 2 A x j 2 A (19\n)\n\u2202D I a (x i , x j ) \u2202a m = x im log x im x jm \u2212 (x im \u2212 x jm )(20)\nIntuitively, the distance learning step results in modifying the distortion measure so that similar data points are brought closer together, while dissimilar points are pulled apart. This process leads to a transformed data space, which facilitates partitioning of the unlabeled data that respects supervised constraints provided by the user and reflects natural variance in the data.", "publication_ref": ["b8", "b2", "b16", "b1", "b17", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "When clustering sparse high-dimensional data, e.g. text documents represented using the vector space model, it is particularly   difficult to cluster small datasets. This is due to the fact that clustering algorithms can easily get stuck in local optima on such datasets, which leads to poor clustering quality. In previous studies with SP-KMEANS algorithm applied to document collections whose size is small compared to the dimensionality of the word space, it has been observed that there is little relocation of documents between clusters for most initializations, which leads to poor clustering quality after convergence of the algorithm [17]. This scenario is likely in many realistic applications. For example, when clustering the search results in a web-search engine like Viv\u00edsimo 1 , typically the number of webpages that are being clustered is in the order of hundreds. However the dimensionality of the feature space, corresponding to the number of unique words in all the webpages, is in the order of thousands. Moreover, each webpage is sparse, since it contains only a small number of all the possible words. Supervision in the form of pairwise constraints can be beneficial in such cases and may significantly improve clustering quality. To demonstrate the effectiveness of our semi-supervised clustering framework, we consider 3 data sets that have the characteristics of being sparse, high-dimensional, and having a small number of points compared to the dimensionality of the space.\nWe derived 3 datasets from the 20-Newsgroups collection. 2 This collection has messages harvested from 20 different Usenet newsgroups, 1000 messages from each newsgroup. From the original dataset, a reduced dataset was created by taking a random subsample of 100 documents from each of the 20 newsgroups. Three datasets were created by selecting 3 categories from the reduced collection. News-Similar-3 consists of 3 newsgroups on similar topics (comp.graphics, comp.os.ms-windows, comp.windows.x) with significant overlap between clusters due to cross-posting. News-Related-3 consists of 3 newsgroups on related topics (talk.politics.misc, talk.politics.guns, and talk.politics.mideast). News-Different-3 consists of articles posted in 3 newsgroups that cover different topics (alt.atheism, rec.sport.baseball, sci. space) with well-separated clusters. The vector-space model of News-Similar-3 has 300 points in 1864 dimensions, News-Related-3 has 300 points in 3225 dimensions, and News-Different-3 had 300 points in 3251 dimensions. Since the overlap between topics in News-Similar-3 and News-Related-3 is significant, they are more challenging datasets than News-Different-3.\nAll the datasets were pre-processed by stop-word removal, TF-IDF weighting, removal of very high-frequency and low-frequency words, etc., following the methodology of Dhillon et al. [18].", "publication_ref": ["b16", "b1", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Clustering Evaluation", "text": "We used normalized mutual information (NMI) as our clustering evaluation measure. NMI is an external clustering validation metric that estimates the quality of the clustering with respect to a given underlying class labeling of the data: it measures how closely the clustering algorithm could reconstruct the underlying label distribution in the data [37,19]. If C is the random variable denoting the cluster assignments of the points and K is the random variable denoting the underlying class labels on the points [2], then the NMI measure is defined as:\nNMI = I(C; K) (H(C) + H(K))/2 (21\n)\nwhere I(X;Y ) = H(X) \u2212 H(X|Y ) is the mutual information between the random variables X and Y , H(X) is the Shannon entropy of X, and H(X|Y ) is the conditional entropy of X given Y [14]. NMI effectively measures the amount of statistical information shared by the random variables representing the cluster assignments and the user-labeled class assignments of the data points.", "publication_ref": ["b36", "b18", "b1", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "We generated learning curves using 20 runs of 2-fold cross-validation for each dataset. For studying the effect of constraints in clustering, 50% of the dataset is set aside as the test set at any particular fold. The different points along the learning curve correspond to constraints that are given as input to the semi-supervised clustering algorithm. These constraints are obtained from the training set corresponding to the remaining 50% of the data by randomly selecting pairs of points from the training set, and creating must-link or cannot-link constraints depending on whether the underlying classes of the two points are same or different. Unit constraint costs W and W were used for all constraints, original and inferred, since the datasets did not provide individual weights for the constraints. Based on a few pilot studies, gradient step size \u03b7 was chosen to have values \u03b7 = 1.75 for clustering with D cos a and \u03b7 = 1.0 \u22128 for clustering with D I a ; weights were restricted to be non-negative. In a realistic setting, these parameters could be tuned using cross-validation with a hold-out set. The clustering algorithm was run on the whole dataset, but NMI was calculated only on the test set. The learning curve results were averaged over the 20 runs.     ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "We compared the proposed HMRF-KMEANS algorithm with two ablations as well as unsupervised K-Means clustering. The following variants were compared for distortion measures D cos a and D I a as representatives for Bregman divergences and directional measures respectively:\n\u2022 KMEANS-I-C-D is the complete HMRF-KMEANS algorithm that includes use of supervised data in initialization (I) as described in Section 3.3, incorporates constraints in cluster assignments (C) as described in Section 3.4, and performs distance learning (D) as described in Section 3.5;\n\u2022 KMEANS-I-C is an ablation of HMRF-KMEANS that uses pairwise supervision for initialization and cluster assignments, but does not perform distance learning;\n\u2022 KMEANS-I is a further ablation that only uses the constraints to initialize cluster representatives;\n\u2022 KMEANS is the unsupervised K-Means algorithm. As the results demonstrate, the full HMRF-KMEANS algorithm outperforms the unsupervised K-Means baseline as well as the ablated versions of HMRF-KMEANS for both D cos a and D I a . Relative performance of KMEANS-I-C and KMEANS-I indicates that using supervision for initializing cluster representatives is highly beneficial, while the constraint-sensitive cluster assignment step does not lead to significant additional improvements for D cos a . For D I a , KMEANS-I-C outperforms KMEANS-I on News-Different-3 (Fig. 4) and News-Similar-3 (Fig. 8) which indicates that incorporating constraints in the cluster assignment process is useful for these datasets. This result is reversed for News-Related-3 (Fig. 6), implying that in some cases using constraints in the E-step may be unnecessary, which agrees with previous results on other domains [6]. However, incorporating supervised data in all the 3 stages of the algorithm in KMEANS-I-C-D, namely initialization, cluster assignment, and distance update, always leads to substantial performance improvement.\nAs can be seen from results for 0 pairwise constraints in Figs. 3-8, distance learning is beneficial even in the absence of any pairwise constraints, since it is able to capture the relative importance of the different attributes in the unsupervised data. In the absence of supervised data or when no constraints are violated, distance learning attempts to minimize the objective function by adjusting the weights given the distortion between the unsupervised datapoints and their corresponding cluster representatives.\nIn realistic application domains, supervision in the form of const-raints would be in most cases provided by human experts, in which case it is important that any semi-supervised clustering algorithm performs well with a small number of constrains. KMEANS-I-C-D starts outperforming its variants and the unsupervised clustering baseline early on in the learning curve, and is therefore a very appropriate algorithm to use in actual semi-supervised data clustering systems.\nOverall, our results show that the HMRF-KMEANS algorithm effectively incorporates labeled and unlabeled data in three stages, each of which improves the clustering quality.", "publication_ref": ["b5"], "figure_ref": ["fig_5", "fig_10", "fig_8", "fig_4"], "table_ref": []}, {"heading": "RELATED WORK", "text": "A related unified model for semi-supervised clustering with constraints was recently proposed by Segal et al. [36]. Their model is a unified Markov network that combines a binary Markov network derived from pairwise protein interaction data and a Naive Bayes Markov network modeling gene expression data. Our proposed HMRF framework is more general than this formulation, since it works with a broad class of clustering distortion measures, including Bregman divergences and directional similarity measures. In contrast, the formulation of Segal et al. considers only a Gaussian cluster conditional probability distribution, which corresponds to having Mahalanobis distance as the underlying clustering distance measure. Additionally, the HMRF-KMEANS algorithm performs distance learning in the unified framework, which is not done in the Markov Network model.\nThe HMRF-KMEANS algorithm proposed in this paper is related to the EM algorithm for HMRF model-fitting proposed by Zhang et al. [40]. However, HMRF-KMEANS performs an additional step of distance learning in the M-step, which is not considered in the HMRF-EM algorithm. The discussion of the HMRF-EM algorithm was also restricted only to Gaussian conditional distributions, which has been generalized in our formulation.\nThere has been other research in semi-supervised clustering focusing individually on either constraint-based or distance-based semisupervised clustering. COP-KMEANS is a constraint-based clustering algorithm that has a heuristically motivated objective function [38]. Our method, on the other hand, has an underlying probabilistic model based on Hidden Markov Random Fields. Bansal et al. [4] also proposed a framework for pairwise constrained clustering, but their model performs clustering using only the constraints, whereas our formulation uses both constraints and an underlying distortion measure between the points.\nIn recent work on distance-based semi-supervised clustering with pairwise constraints, Cohn et al. [13] used gradient descent for weighted Jensen-Shannon divergence in the context of EM clustering. Xing et al. [39] utilized a combination of gradient descent and iterative projections to learn a Mahalanobis distance for K-Means clustering. The Redundant Component Analysis (RCA) algorithm used only must-link constraints to learn a Mahalanobis distance using convex optimization [5]. Spectral learning is another recent method that utilizes supervision to transform the clustering distance measure using spectral methods [25]. All these distance learning techniques for clustering train the distance measure first using only supervised data, and then perform clustering on the unsupervised data. In contrast, our method integrates distance learning with the clustering process and utilizes both supervised and unsupervised data to learn the distortion measure.", "publication_ref": ["b35", "b39", "b37", "b3", "b12", "b38", "b4", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "FUTURE WORK", "text": "We have presented the general probabilistic framework for incorporating pairwise supervision into a prototype-based clustering al-gorithm, as well as two instantiations of that framework for particular distortion measures. There are several open issues that would be interesting to explore in future work.\nInvestigating alternative approaches to training distortion measures in the M-step of our algorithm may lead to improved performance of the algorithm. Our initial results as well as other recent work on distance learning for clustering [27,8,5,39] suggest that transforming the data space can be highly beneficial for clustering quality. Therefore, we conjecture that developing alternative feature selection or feature extraction approaches, which perform other types of data space transformation using supervised data, is a promising direction for future work.\nThe weighted farthest-first algorithm for cluster initialization that we have described in Section 3.3 has proven itself very useful. We intend to explore theoretical implications of this initialization algorithm in the HMRF model, as well as develop alternative techniques that utilize both labeled and unlabeled data for initializing cluster representatives.\nWhile we have used the ICM algorithm for constraint-sensitive cluster assignment in the HMRF model, other methods have also been proposed for this task, e.g. loopy belief propagation [36]. Extensive experimental comparison of these strategies would be informative for future work on iterative reassignment algorithms like HMRF-KMEANS in the HMRF framework. We also want to run experiments to study the sensitivity of the HMRF-KMEANS algorithm to the constraint violation parameters W and W , as done in Segal et al. [36].\nFinally, we want to apply our algorithm to other application domains. One interesting problem in bioinformatics is to improve the quality of clustering genes with unknown functions by utilizing constraints between the genes derived from domain knowledge. Segal et al. [36] used constraints derived from protein-protein interactions while clustering gene expression data using Mahalanobis distance as the underlying distortion measure. We want to apply our HMRF-KMEANS algorithm to different kinds of gene representations, for which different clustering distance measures would be appropriate, e.g., Pearson's correlation would be an appropriate distortion measure for gene microarray data [20], I-divergence would be useful for the phylogenetic profile representation of genes [30], etc. We plan to run experiments for clustering these datasets using the HMRF-KMEANS algorithm, where the constraints will be inferred from protein interaction databases as well as from function pathway labels that are known for a subset of the genes.", "publication_ref": ["b26", "b7", "b4", "b38", "b35", "b35", "b35", "b19", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "We have introduced a theoretically motivated framework for semisupervised clustering that employs Hidden Random Markov Fields (HMRFs) to utilize both labeled and unlabeled data in the clustering process. The framework can be used with a number of distortion measures, including Bregman divergences and directional measures, and it accommodates trainable measures that can be adapted to specific datasets. We introduced the HMRF-KMEANS algorithm that performs clustering in this framework and incorporates supervision in the form of pairwise constraints in all stages of the clustering algorithm: initialization, cluster assignment, and parameter estimation. We presented two instantiations of the algorithm based on two particular distortion measures that are popular for high-dimensional data: KL divergence and cosine similarity. Experimental evaluation has shown that the algorithm derived from the HMRF framework leads to improved cluster quality on realistic textual datasets over unsupervised clustering and ablations of the proposed approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Srujana Merugu for insightful comments. This research was supported by the National Science Foundation under grants IIS-0117308 and ITR: IIS-0325116, and by a Faculty Fellowship from IBM Corporation.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Modern Information Retrieval", "journal": "ACM Press", "year": "1999", "authors": "R Baeza-Yates; B Ribeiro-Neto"}, {"ref_id": "b1", "title": "Generative model-based clustering of directional data", "journal": "", "year": "2003", "authors": "A Banerjee; I Dhillon; J Ghosh; S Sra"}, {"ref_id": "b2", "title": "Clustering with Bregman divergences", "journal": "", "year": "2004", "authors": "A Banerjee; S Merugu; I S Dhillon; J Ghosh"}, {"ref_id": "b3", "title": "Correlation clustering", "journal": "", "year": "2002", "authors": "N Bansal; A Blum; S Chawla"}, {"ref_id": "b4", "title": "Learning distance functions using equivalence relations", "journal": "", "year": "2003", "authors": "A Bar-Hillel; T Hertz; N Shental; D Weinshall"}, {"ref_id": "b5", "title": "Semi-supervised clustering by seeding", "journal": "", "year": "2002", "authors": "S Basu; A Banerjee; R J Mooney"}, {"ref_id": "b6", "title": "Active semi-supervision for pairwise constrained clustering", "journal": "", "year": "2004", "authors": "S Basu; A Banerjee; R J Mooney"}, {"ref_id": "b7", "title": "Comparing and unifying search-based and similarity-based approaches to semi-supervised clustering", "journal": "", "year": "2003", "authors": "S Basu; M Bilenko; R J Mooney"}, {"ref_id": "b8", "title": "On the statistical analysis of dirty pictures", "journal": "Journal of the Royal Statistical Society, Series B (Methodological)", "year": "1986", "authors": "J Besag"}, {"ref_id": "b9", "title": "Adaptive duplicate detection using learnable string similarity measures", "journal": "", "year": "2003", "authors": "M Bilenko; R J Mooney"}, {"ref_id": "b10", "title": "Combining labeled and unlabeled data with co-training", "journal": "", "year": "1998", "authors": "A Blum; T Mitchell"}, {"ref_id": "b11", "title": "Markov random fields with efficient approximations", "journal": "", "year": "1998", "authors": "Y Boykov; O Veksler; R Zabih"}, {"ref_id": "b12", "title": "Semi-supervised clustering with user feedback", "journal": "", "year": "2003", "authors": "D Cohn; R Caruana; A Mccallum"}, {"ref_id": "b13", "title": "Elements of Information Theory", "journal": "Wiley-Interscience", "year": "1991", "authors": "T M Cover; J A Thomas"}, {"ref_id": "b14", "title": "Semi-supervised clustering using genetic algorithms", "journal": "", "year": "1999", "authors": "A Demiriz; K P Bennett; M J Embrechts"}, {"ref_id": "b15", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal of the Royal Statistical Society B", "year": "1977", "authors": "A P Dempster; N M Laird; D B Rubin"}, {"ref_id": "b16", "title": "Information theoretic clustering of sparse co-occurrence data", "journal": "", "year": "2003", "authors": "I S Dhillon; Y Guan"}, {"ref_id": "b17", "title": "Concept decompositions for large sparse text data using clustering", "journal": "", "year": "2001", "authors": "I S Dhillon; D S Modha"}, {"ref_id": "b18", "title": "An information-theoretic external cluster-validity measure", "journal": "IBM", "year": "2001", "authors": "B E Dom"}, {"ref_id": "b19", "title": "Cluster analysis and display of genome-wide expression patterns", "journal": "Proceedings of the National Academy of Sciences, USA", "year": "1998", "authors": "M B Eisen; P T Spellman; P O Brown; D Botstein"}, {"ref_id": "b20", "title": "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1984", "authors": "S Geman; D Geman"}, {"ref_id": "b21", "title": "Markov fields on finite graphs and lattices. Unpublished manuscript", "journal": "", "year": "1971", "authors": "J M Hammersley; P Clifford"}, {"ref_id": "b22", "title": "A best possible heuristic for the k-center problem", "journal": "Mathematics of Operations Research", "year": "1985", "authors": "D Hochbaum; D Shmoys"}, {"ref_id": "b23", "title": "Transductive inference for text classification using support vector machines", "journal": "", "year": "1999", "authors": "T Joachims"}, {"ref_id": "b24", "title": "Spectral learning", "journal": "", "year": "2003", "authors": "S D Kamvar; D Klein; C D Manning"}, {"ref_id": "b25", "title": "An information-theoretic analysis of hard and soft assignment methods for clustering", "journal": "", "year": "1997", "authors": "M Kearns; Y Mansour; A Y Ng"}, {"ref_id": "b26", "title": "From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering", "journal": "", "year": "2002", "authors": "D Klein; S D Kamvar; C Manning"}, {"ref_id": "b27", "title": "Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields", "journal": "", "year": "1999", "authors": "J Kleinberg; E Tardos"}, {"ref_id": "b28", "title": "Some methods for classification and analysis of multivariate observations", "journal": "", "year": "1967", "authors": "J Macqueen"}, {"ref_id": "b29", "title": "Localizing proteins in the cell from their phylogenetic profiles", "journal": "Proceedings of the National Academy of Science", "year": "2000", "authors": "E M Marcotte; I Xenarios; A Van Der Bliek; D Eisenberg"}, {"ref_id": "b30", "title": "Directional Statistics. John Wiley and Sons Ltd", "journal": "", "year": "2000", "authors": "K V Mardia; P Jupp"}, {"ref_id": "b31", "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "journal": "MIT Press", "year": "1998", "authors": "R M Neal; G E Hinton"}, {"ref_id": "b32", "title": "Text classification from labeled and unlabeled documents using EM", "journal": "", "year": "2000", "authors": "K Nigam; A K Mccallum; S Thrun; T Mitchell"}, {"ref_id": "b33", "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "journal": "Morgan Kaufmann", "year": "1988", "authors": "J Pearl"}, {"ref_id": "b34", "title": "Distributional clustering of English words", "journal": "", "year": "1993", "authors": "F C N Pereira; N Tishby; L Lee"}, {"ref_id": "b35", "title": "Discovering molecular pathways from protein interaction and gene expression data", "journal": "Bioinformatics", "year": "2003-07", "authors": "E Segal; H Wang; D Koller"}, {"ref_id": "b36", "title": "Impact of similarity measures on web-page clustering", "journal": "", "year": "2000-07", "authors": "A Strehl; J Ghosh; R Mooney"}, {"ref_id": "b37", "title": "Constrained K-Means clustering with background knowledge", "journal": "", "year": "2001", "authors": "K Wagstaff; C Cardie; S Rogers; S Schroedl"}, {"ref_id": "b38", "title": "Distance metric learning, with application to clustering with side-information", "journal": "MIT Press", "year": "2003", "authors": "E P Xing; A Y Ng; M I Jordan; S Russell"}, {"ref_id": "b39", "title": "Hidden Markov random field model and segmentation of brain MR images", "journal": "IEEE Transactions on Medical Imaging", "year": "2001", "authors": "Y Zhang; M Brady; S Smith"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A Hidden Markov Random Field", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "im log x im x jm , where x i and x j are probability distributions over d events: \u2211 d m=1 x im = \u2211 d m=1 x jm = 1. In previous work, Cohn et al. parameterized KL-divergence multiplying m-th component by a weight \u03b3 m : D KL (x i , x j ) = \u2211 d m=1 \u03b3 m x im log x im", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "sentatives {\u00b5 \u00b5 \u00b5 h } K h=1 are re-estimated from the cluster assignments to minimize J obj for the current assignment. The clustering distortion measure D is updated in the M-step to reduce the objective function simultaneously by transforming the space in which data lies. Note that this corresponds to the generalized EM algorithm [32, 16], where the objective function is reduced but not necessarily minimized in the M-step. Effectively, the E-step minimizes J obj over cluster assignments L, the M-step (A) minimizes J obj over cluster representatives {\u00b5 \u00b5 \u00b5 h } K h=1 , and the M-step (B) minimizes J obj over the parameters of the distortion measure D. The E-step and the Mstep are repeated till a specified convergence criterion is reached. The specific details of the E-step and M-step are discussed in the following sections.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Clustering results for D cos a on News-Different-3 dataset", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Clustering results for D I a on News-Different-3 dataset", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :5Figure 5: Clustering results for D cos a on News-Related-3 dataset", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure 6: Clustering results for D I a on News-Related-3 dataset", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 :7Figure 7: Clustering results for D cos a on News-Similar-3 dataset", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 8 :8Figure 8: Clustering results for D I a on News-Similar-3 dataset", "figure_data": ""}, {"figure_label": "35", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figs. 3 , 5 ,35Figs. 3, 5, and 7 demonstrate the results for experiments where weighted cosine similarity D cos a was used as the distortion measure, while Figs. 4, 6, and 8 summarize experiments where weighted I-divergence D I a was used.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 A hidden field L = {l i } N", "formula_coordinates": [2.0, 67.25, 573.81, 100.05, 16.88]}, {"formula_id": "formula_1", "formula_text": "CannotLink x 6 x 5 x 1 x 4 l 1 = 1 l 5 = 3 l 4 = 1 x 2 x 3 l 2 = 1 l 3 = 1 l 6 = 2", "formula_coordinates": [2.0, 357.75, 73.8, 159.65, 170.58]}, {"formula_id": "formula_2", "formula_text": "\u2200i, Pr(l i |L \u2212 {l i }) = Pr(l i |{l j : l j \u2208 N i })(1)", "formula_coordinates": [2.0, 361.89, 498.3, 194.01, 16.88]}, {"formula_id": "formula_3", "formula_text": "Pr(L) = 1 Z 1 exp \u2212V (L) = 1 Z 1 exp \u2212 \u2211 N i \u2208N V N i (L)(2)", "formula_coordinates": [2.0, 337.85, 607.33, 218.06, 29.3]}, {"formula_id": "formula_4", "formula_text": "Z 1 exp(\u2212 \u2211 i \u2211 j V (i, j)), where V (i, j) = \uf8f1 \uf8f2 \uf8f3 f M (x i , x j ) if (x i , x j ) \u2208 M f C (x i , x j ) if (x i , x j ) \u2208 C 0 otherwise (3)", "formula_coordinates": [3.0, 97.5, 55.37, 195.4, 49.17]}, {"formula_id": "formula_5", "formula_text": "set X = {x i } N i=1 for a given configuration L = {l i } N", "formula_coordinates": [3.0, 53.8, 256.12, 189.93, 16.88]}, {"formula_id": "formula_6", "formula_text": "Pr(X |L) = p(X , {\u00b5 \u00b5 \u00b5 h } K h=1 )(4)", "formula_coordinates": [3.0, 125.12, 284.6, 167.77, 17.08]}, {"formula_id": "formula_7", "formula_text": "Pr(L|X ) = 1 Z 2 exp \u2212 \u2211 i \u2211 j V (i, j) \u2022 p(X , {\u00b5 \u00b5 \u00b5 h } K h=1 )(5)", "formula_coordinates": [3.0, 68.35, 409.31, 224.53, 23.04]}, {"formula_id": "formula_8", "formula_text": "p(X , {\u00b5 \u00b5 \u00b5 h } K h=1 ) = 1 Z 3 exp \u2212 \u2211 x i \u2208X D(x i ,\u00b5 \u00b5 \u00b5 l i )(6)", "formula_coordinates": [3.0, 360.05, 149.76, 195.86, 28.47]}, {"formula_id": "formula_9", "formula_text": "f M (x i , x j ) = w i j \u03d5 D (x i , x j ) [l i = l j ](7)", "formula_coordinates": [3.0, 373.89, 571.61, 182.02, 11.96]}, {"formula_id": "formula_10", "formula_text": "f C (x i , x j ) = w i j \u03d5 D max \u2212 \u03d5 D (x i , x j ) [l i = l j ](8)", "formula_coordinates": [3.0, 354.01, 708.25, 201.9, 17.66]}, {"formula_id": "formula_11", "formula_text": "J obj = \u2211 x i \u2208X D(x i ,\u00b5 \u00b5 \u00b5 l i ) + \u2211 (x i ,x j )\u2208M w i j \u03d5 D (x i , x j ) [l i = l j ] + \u2211 (x i ,x j )\u2208C w i j \u03d5 D max \u2212 \u03d5 D (x i , x j ) [l i = l j ] + log Z (9)", "formula_coordinates": [4.0, 66.43, 186.71, 226.44, 33.82]}, {"formula_id": "formula_12", "formula_text": "D cos A (x i , x j ) = 1 \u2212 x T i Ax j x i A x j A (10)", "formula_coordinates": [4.0, 114.75, 697.95, 178.14, 23.68]}, {"formula_id": "formula_13", "formula_text": "J cos a = \u2211 x i \u2208X D cos a (x i ,\u00b5 \u00b5 \u00b5 l i ) + \u2211 (x i ,x j )\u2208M w i j D cos a (x i , x j ) [l i = l j ](11)", "formula_coordinates": [4.0, 325.9, 246.09, 230.0, 47.0]}, {"formula_id": "formula_14", "formula_text": "+ \u2211 (x i ,x j )\u2208C w i j D cos a max \u2212 D cos a (x i , x j ) [l i = l j ] + log Z", "formula_coordinates": [4.0, 343.95, 294.6, 202.4, 23.27]}, {"formula_id": "formula_15", "formula_text": "D KL (x i , x j ) = \u2211 d m=1 x", "formula_coordinates": [4.0, 316.81, 386.93, 239.1, 23.06]}, {"formula_id": "formula_16", "formula_text": "D I (x i , x j ) = \u2211 d m=1 x im log x im x jm \u2212 \u2211 d m=1 (x im \u2212 x jm )", "formula_coordinates": [4.0, 316.81, 497.88, 242.98, 31.62]}, {"formula_id": "formula_17", "formula_text": "D I a (x i , x j ) = d \u2211 m=1 a m x im log x im x jm \u2212 d \u2211 m=1 a m (x im \u2212 x jm )(12)", "formula_coordinates": [4.0, 334.68, 564.41, 221.21, 25.32]}, {"formula_id": "formula_18", "formula_text": "x i +x j 2 .", "formula_coordinates": [4.0, 394.77, 707.21, 20.46, 14.71]}, {"formula_id": "formula_19", "formula_text": "\u03d5 D (x i , x j ) = D IM a (x i , x j ) = d \u2211 m=1", "formula_coordinates": [5.0, 59.38, 86.29, 91.34, 42.44]}, {"formula_id": "formula_20", "formula_text": "J I a = \u2211 x i \u2208X D I a (x i ,\u00b5 \u00b5 \u00b5 l i ) + \u2211 (x i ,x j )\u2208M w i j D IM a (x i , x j ) [l i = l j ] (14) + \u2211 (x i ,x j )\u2208C w i j D IM a max \u2212 D IM a (x i , x j ) [l i = l j ] + log Z", "formula_coordinates": [5.0, 63.74, 153.18, 229.15, 48.45]}, {"formula_id": "formula_21", "formula_text": "X = {x i } N i=1 , number of clusters K, set of must-link constraints M = {(x i , x j )}, set of cannot-link constraints C = {(x i , x j )}, distance measure D, constraint violation costs W and W. Output: Disjoint K-partitioning {X h } K", "formula_coordinates": [5.0, 55.95, 540.35, 223.78, 48.9]}, {"formula_id": "formula_22", "formula_text": "1. Initialize the K clusters centroids {\u00b5 \u00b5 \u00b5 (0) h } K h=1 , set t \u2190 0 2. Repeat until convergence 2a. E-step: Given {\u00b5 \u00b5 \u00b5 (t) h } K h=1 , re-assign cluster labels {l (t+1) i } N i=1 on the points {x i } N i=1 to minimize J obj . 2b. M-step(A): Given cluster labels {l (t+1) i } N i=1 , re-calculate cluster centroids {\u00b5 \u00b5 \u00b5 (t+1) h } K", "formula_coordinates": [5.0, 55.95, 609.41, 220.05, 67.86]}, {"formula_id": "formula_23", "formula_text": "J obj (x i ,\u00b5 \u00b5 \u00b5 h ) = D(x i ,\u00b5 \u00b5 \u00b5 h ) + \u2211 (x i ,x j )\u2208M w i j \u03d5 D (x i , x j ) [h = l j ] + \u2211 (x i ,x j )\u2208C w i j \u03d5 D max \u2212 \u03d5 D (x i , x j ) [h = l j ](15)", "formula_coordinates": [6.0, 62.45, 437.92, 230.44, 48.44]}, {"formula_id": "formula_24", "formula_text": "\u00b5 \u00b5 \u00b5 (I a ) h = 1 1 + \u03b1 \u2211 x i \u2208X h x i |X h | + \u03b1 1 n (16)", "formula_coordinates": [6.0, 376.84, 122.2, 179.06, 29.7]}, {"formula_id": "formula_25", "formula_text": "\u00b5 \u00b5 \u00b5 (cos a ) h = \u2211 x i \u2208X h x i \u2211 x i \u2208X h x i A (17", "formula_coordinates": [6.0, 394.61, 193.97, 157.57, 29.74]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [6.0, 552.17, 202.79, 3.73, 8.07]}, {"formula_id": "formula_27", "formula_text": "\u2202J obj \u2202a m = \u2211 x i \u2208X \u2202D(x i ,\u00b5 \u00b5 \u00b5 l i ) \u2202a m + \u2211 (x i ,x j )\u2208M w i j \u2202D(x i , x j ) \u2202a m [l i = l j ](18)", "formula_coordinates": [6.0, 344.28, 386.39, 211.62, 56.83]}, {"formula_id": "formula_28", "formula_text": "(x i ,x j )\u2208C w i j \u2202D max \u2202a m \u2212 \u2202D(x i , x j ) \u2202a m [l i = l j ]", "formula_coordinates": [6.0, 373.29, 445.72, 156.36, 27.55]}, {"formula_id": "formula_29", "formula_text": "\u2202D(x i ,x j )", "formula_coordinates": [6.0, 422.34, 487.98, 27.43, 9.17]}, {"formula_id": "formula_30", "formula_text": "\u2202D cos a (x i , x j ) \u2202a m = x im x jm x i A x j A \u2212 x T i Ax j x 2 im x j 2 A +x 2 jm x i 2 A 2 x i A x j A x i 2 A x j 2 A (19", "formula_coordinates": [6.0, 324.31, 508.95, 227.87, 39.63]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [6.0, 552.17, 540.51, 3.73, 8.07]}, {"formula_id": "formula_32", "formula_text": "\u2202D I a (x i , x j ) \u2202a m = x im log x im x jm \u2212 (x im \u2212 x jm )(20)", "formula_coordinates": [6.0, 364.89, 560.67, 191.01, 24.45]}, {"formula_id": "formula_33", "formula_text": "NMI = I(C; K) (H(C) + H(K))/2 (21", "formula_coordinates": [7.0, 387.94, 404.38, 164.23, 20.59]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [7.0, 552.17, 410.45, 3.73, 8.07]}], "doi": ""}