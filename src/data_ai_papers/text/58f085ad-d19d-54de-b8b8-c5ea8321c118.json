{"title": "A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions", "authors": "Wei Lu; Hwee Tou Ng", "pub_date": "", "abstract": "This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.", "sections": [{"heading": "Introduction", "text": "This work focuses on the task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980;Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a;Lu et al., 2009) or database entries (Angeli et al., 2010).\nWhile these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as \u03bb-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework.\nOne natural proposal is to adopt a state-of-the-art statistical machine translation approach. However, unlike text to text translation, which has been extensively studied in the machine translation community, translating from logical forms into text presents additional challenges. Specifically, logical forms such as \u03bb-expressions may have complex internal structures and variable dependencies across subexpressions. Problems arise when performing automatic acquisition of a translation lexicon, as well as performing lexical selection and surface realization during generation.\nIn this work, we tackle these challenges by making the following contributions:\n\u2022 A novel forest-to-string generation algorithm:\nInspired by the work of Chiang (2007), we introduce a novel reduction-based weighted binary synchronous context-free grammar formalism for generation from logical forms (\u03bbexpressions), which can then be integrated with a probabilistic forest-to-string generation algo-rithm.\n\u2022 A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al., 2008).\nTo our best knowledge, this is the first probabilistic model for generating sentences from the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5.", "publication_ref": ["b34", "b30", "b36", "b25", "b1", "b8", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework.\nAnother line of research efforts focused on the task of language generation from other meaning representation formalisms. Wong and Mooney (2007a) as well as Chen and Mooney (2008) made use of synchronous grammars to transform a variablefree tree-structured meaning representation into sentences. Lu et al. (2009) presented a language generation model using the same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models.\nRecently there are also substantial research efforts on the task of mapping natural language to meaning representations in various formalisms -the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005;2007;2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010).\nOf particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. We extend our prior model in the next section, so as to support \u03bb-expressions. The model in turn serves as the basis for inducing the synchronous grammar rules later.", "publication_ref": ["b34", "b30", "b29", "b21", "b36", "b25", "b1", "b39", "b8", "b38", "b14", "b37", "b24", "b11", "b19", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "\u03bb-Hybrid Tree", "text": "In Lu et al. (2008), a generative model was presented to model the process that jointly generates both natural language sentences and their underlying meaning representations of a variable-free treestructured form. The model was defined over a hybrid tree, which consists of meaning representation tokens as internal nodes and natural language words as leaves. One limitation of the hybrid tree model is that it assumes a single fixed tree structure for the meaning representation. However, \u03bbexpressions exhibit complex structures and variable dependencies, and thus it is not obvious how to represent them in a single tree structure.\nIn this section, we present a novel \u03bb-hybrid tree model that provides the following extensions over the model of Lu et al. (2008):\n1. The internal nodes of a meaning representation tree involve \u03bb-expressions which are not necessarily of variable-free form; 2. The meaning representation has a packed forest representation, rather than a single deterministic tree structure.", "publication_ref": ["b24", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Packed \u03bb-Meaning Forest", "text": "We represent a \u03bb-expression with a packed forest of meaning representation trees (called \u03bb-meaning for-est). Multiple different meaning representation trees (called \u03bb-meaning trees) can be extracted from the same \u03bb-meaning forest, but they all convey equivalent semantics via reductions, as discussed next.\nConstructing a \u03bb-meaning forest for a given \u03bbexpression requires decomposition of a complete \u03bbexpression into semantically complete and syntactically correct sub-expressions in a principled manner. This can be achieved with a process called higher order unification (Huet, 1975). The process was known to be very complex and was shown to be undecidable in unrestricted form (Huet, 1973). Recently a restricted form of higher order unification was applied to a semantic parsing task (Kwiatkowski et al., 2010). In this work, we employ a similar technique for building the \u03bb-meaning forest.\nFor a given \u03bb-expression e, our algorithm finds either two expressions h and f such that (h f ) \u2261 e, or three expressions h, f , and g such that ((h f ) g) \u2261 e, where the symbol \u2261 is interpreted as \u03b1-equivalent after reductions 1 (Barendregt, 1985). We then build the \u03bb-meaning forest based on the expressions h, f , and g. In practice, we develop a BUILDFOREST(e) procedure which recursively builds \u03bb-forests by applying restricted higher-order unification rules on top of the \u03bb-expression e. Each node of the \u03bb-forest is called a \u03bb-production, to which we will give more details in Section 3.2. For example, once a candidate triple (h, f, g) as in ((h f ) g) \u2261 e has been identified, the procedure creates a \u03bb-forest with the root node being a \u03bb-production involving h, and two sets of child \u03bb-forests given by BUILDFOREST(f ) and BUILDFOREST(g) respectively. For restricted higher-order unification, besides the similar assumptions made by Kwiatkowski et al. (2010), we also impose one additional assumption: limited free variable, which states that the expression h must contain no more than one free variable. Note that this process provides a semantically equivalent packed forest representation of the original \u03bb-expression, without altering its semantics in any way.\nFor better readability, we introduce the symbol \u00a1 as an alternative notation for functional application. In other words, h \u00a1 f refers to (h f ) or h(f ), and h \u00a1 f \u00a1 g refers to ((h f ) g). For ex-ample, the expression \u03bbx.state(x) \u2227 loc(boston, x) can be represented as the functional application form of [\u03bbf.\u03bbx.f (x) \u2227 loc(boston, x)] \u00a1 \u03bbx.state(x). 2 Such a packed forest representation contains exponentially many tree structures which all convey the same semantics. We believe such a semantic representation is more advantageous than the single fixed tree-structured representation. In fact, one could intuitively regard a different decomposition path as a different way of interpreting the same semantics. Thus, such a representation could potentially accommodate a wider range of natural language expressions, which all share the same semantics but with very different word choices, phrase orderings, and syntactic structures (like paraphrases). It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). We will validate our belief later through experiments. The generative process for a sentence together with its corresponding \u03bb-meaning tree is illustrated in Figure 1, which results in a \u03bb-hybrid tree. Internal nodes of a \u03bb-hybrid tree are called \u03bb-productions, which are building blocks of a \u03bb-forest. Each \u03bb-production in turn has at most two child \u03bbproductions. A \u03bb-production has the form \u03c4 a : \u03c0 a \u00a1 \u03c4 b , where \u03c4 a is the expected type 3 after type evaluation of the terms to its right, \u03c0 a is a \u03bb-expression (serves as the functor), and \u03c4 b are types of the child \u03bb-productions (as the arguments). The leave nodes r : e, t 1 e, t 1 : \u03bbg.\u03bbf.\u03bbx.g(x) \u2227 f (x) \u00a1 e, t 1 \u00a1 e, t 2 e, t 2 : \u03bbf.\u03bbg.\u03bbx.\u2203y.g(y) \u2227 (f (x) y) \u00a1 e, e, t 1 \u00a1 e, t 2 e, t 2 : \u03bbg.\u03bbf.\u03bbx.g(x) \u2227 f (x) \u00a1 e, t 1 \u00a1 e, t 2 e, t 1 : \u03bby.\u03bbx.loc(y, x) \u00a1 e 1 runs through e 1 : miss r the mississippi that e, t 2 : \u03bbx.state(x) states e, e, t 1 : \u03bby.\u03bbx.next to(x, y) bordering e, t 1 : \u03bbx.state(x) the states give me Figure 2: One example \u03bb-hybrid tree for the sentence \"give me the states bordering states that the mississippi runs through\" together with its logical form \"\u03bbx0.state( x0", "publication_ref": ["b13", "b12", "b19", "b3", "b19", "b37"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "The Joint Generative Process", "text": ") \u2227 \u2203x1.[loc(miss r, x1) \u2227 state(x1) \u2227 next to(x1, x0)]\".\nw are contiguous word sequences. The model repeatedly generates \u03bb-hybrid sequences, which consist of words intermixed with \u03bb-productions, from each \u03bb-production at different levels. Consider part of the example \u03bb-hybrid tree in Figure 2. The probability associated with generation of the subtree that spans the sub-sentence \"that the mississippi runs through\" can be written as:\nP \u03bbx.loc(miss r, x), that the mississippi runs through = \u03c6(m \u2192 wYw|p1) \u00d7 \u03c8(that e 1 runs through|p1) \u00d7\u03c1(p2|p1, arg 1 ) \u00d7 \u03c6(m \u2192 w|p2) \u00d7 \u03c8(the mississippi|p2)\nwhere p 1 = e, t : \u03bby.\u03bbx.loc(y, x) \u00a1 e 1 , and p 2 = e : miss r.\nFollowing the work of Lu et al. (2008), the generative process involves three types of parameters\u03b8 = {\u03c6, \u03c8, \u03c1}: 1) pattern parameters \u03c6, which model in what way the words and child \u03bb-productions are intermixed; 2) emission parameters \u03c8, which model the generation process of words from \u03bb-productions, where either a unigram or a bigram assumption can be made (Lu et al., 2008); and 3) meaning representation (MR) model parameters \u03c1, which model the generation process from one \u03bb-production to its child \u03bb-productions. An analogous inside-outside algorithm (Baker, 1979) used there is employed here. Since we allow a packed \u03bb-meaning forest representation rather than a fixed tree structure, the MR model parameters \u03c1 in this work should be estimated with the inside-outside algorithm as well, rather than being estimated directly from the training data by simple counting, as was done in Lu et al. (2008).", "publication_ref": ["b24", "b24", "b2", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "The Language Generation Algorithm", "text": "Now we present the algorithm for language generation. We introduce the grammar first, followed by the features we use. Next, we present the method for grammar induction, and then discuss the decoder.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Grammar", "text": "We use a weighted synchronous context free grammar (SCFG) (Aho and Ullman, 1969), which was previously used in Chiang (2007) for hierarchical phrase-based machine translation. The grammar is defined as follows:\n\u03c4 \u2192 p \u03bb , hw, \u223c(1)\nwhere \u03c4 is the type associated with the \u03bb-production p \u03bb 4 , and h w is a sequence consisting of natural language words intermixed with types. The symbol \u223c denotes the one-to-one correspondence between nonterminal occurrences (i.e., in this case types of \u03bb-expressions) in both p \u03bb and h w .\nWe allow a maximum of two nonterminal symbols in each synchronous rule, as was also assumed in Chiang (2007), which makes the grammar a binary SCFG. Two example rules are: e, t \u2192 \u03bby.\u03bbx.loc(y, x) \u00a1 e 1 , that e 1 runs through e \u2192 miss r, the mississippi where the boxed indices give the correspondences between nonterminals.\nA derivation with the above two synchronous rules results in the following \u03bb-expression paired with its natural language counterpart: where the source side \u03bb-expression is constructed from the application \u03bby.\u03bbx.loc(y, x) \u00a1 miss r followed by a reduction (\u03b2-conversion). Assuming the \u03bb-expression to be translated is \u03bbx.loc(miss r, x), the above rule in fact gives one candidate translation \"that the mississippi runs through\".", "publication_ref": ["b0", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Features", "text": "Following the work of Chiang ( 2007), we assign scores to derivations with a log-linear model, which are essentially weighted products of feature values.\nFor generality, we only consider the following four simple features in this work: 1.p(h w |p \u03bb ): the relative frequency estimate of a hybrid sequence h w given the \u03bb-production p \u03bb ; 2.p(p \u03bb |h w , \u03c4 ): the relative frequency estimate of a \u03bb-production p \u03bb given the phrase h w and the type \u03c4 ; 3. exp(\u2212wc(h w )): the number of words generated, where wc(h w ) refers to the number of words in h w (i.e., word penalty); and 4. p LM (\u015d): the language model score of the generated sentence\u015d. The first three features, which are also widely used in state-of-the-art machine translation models (Koehn et al., 2003;Chiang, 2007), are rule-specific and thus can be computed before decoding. The last feature is computed during the decoding phase in combination with the sibling rules used.\nWe score a derivation D with a log-linear model:\nw(D) = r\u2208D i fi(r) w i \u00d7 pLM (\u015d) w LM (2)\nwhere r \u2208 D refers to a rule r that appears in the derivation D,\u015d is the target side (sentence) associated with the derivation D, and f i is a rulespecific feature (one of features 1-3 above) which is weighted with w i . The language model feature is weighted with w LM .\nOnce the feature values are computed, our goal is to find the optimal weight vectorw * that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4.\nFollowing popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004;Chiang, 2005), we use the minimum error rate training (MERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work.", "publication_ref": ["b15", "b8", "b26", "b7", "b27", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Grammar Induction", "text": "Automatic induction of the grammar rules as described above from training data (which consists of pairs of \u03bb-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003;Chiang, 2005;Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996;Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models.\nFortunately, the generative model for \u03bb-hybrid tree introduced in Section 3 explicitly models the mappings from \u03bb-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the \u03bb-hybrid trees. Thus, we first find the Viterbi \u03bb-hybrid trees for all training instances, Figure 4: Construction of a two-level \u03bb-hybrid sequence rule via substitution and reductions from a tree fragment. Note that the subtree rooted by e 1 : miss r gets \"abstracted\" by its type e. The auxiliary variable y of type e is thus introduced to facilitate the construction process.\nbased on the learned parameters of the generative \u03bbhybrid tree model.\nNext, we extract grammar rules on top of these \u03bb-hybrid trees. Specifically, we extract the following three types of synchronous grammar rules, with examples given in Figure 3: 1. \u03bb-hybrid sequence rules: They are the conventional rules constructed from one \u03bb-production and its corresponding \u03bb-hybrid sequence. 2. Subtree rules: These rules are constructed from a complete subtree of the \u03bb-hybrid tree. Each rule provides a mapping between a complete sub-expression and a contiguous sub-sentence. 3. Two-level \u03bb-hybrid sequence rules: These rules are constructed from a tree fragment with one of its grandchild subtrees (the subtree rooted by one of its grandchild nodes) being abstracted with its type only. These rules are constructed via substitution and reductions. Figure 4 gives an example based on a tree fragment of the \u03bb-hybrid tree in Figure 2. Note that the first step makes use of the auxiliary variable y of type e to represent the grandchild subtree. \u03bby is introduced so as to allow any \u03bb-expression of type e serving as this expression's argument to replace y . In fact, if the semantics conveyed by the grandchild subtree serves as its argument, we will obtain the exact complete semantics of the current subtree. As we can see, the resulting rule is more general, and is able to capture longer structural dependencies. Such rules are thus potentially more useful.\nThe overall algorithm for learning the grammar rules is sketched in Figure 5.", "publication_ref": ["b15", "b7", "b10", "b4", "b33", "b23"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Decoding", "text": "Our goal in decoding is to find the most probable sentence\u015d for a given \u03bb-expression e:\ns = s arg max D s.t. e(D)\u2261e w(D)(3)\nwhere e(D) refers to the source side (\u03bb-expression) of the derivation D, and s(D) refers to the target side (natural language sentence) of D.\nA conventional CKY-style decoder as used by Chiang (2007) is not applicable to this work since the source side does not exhibit a linear structure. As discussed in Section 3.1, \u03bb-expressions are represented as packed \u03bb-meaning forests. Thus, in this work, we make use of a bottom-up dynamic programming chart-parsing algorithm that works directly on translating forest nodes into target natural language words. The algorithm is similar to that of Langkilde (2000) for generation from an underlying packed semantic forest. Language models are incorporated when scoring the n-best candidates at each forest node, where the cube-pruning algorithm of Chiang (2007) is used. In order to accommodate type 2 and type 3 rules as discussed in Section 4.3, whose source side \u03bb-productions are not present in the nodes of the original \u03bb-meaning forest, new \u03bbproductions are created (via substitution and reductions) and attached to the original \u03bb-meaning forest. ", "publication_ref": ["b8", "b21", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Inputs and initializations:", "text": "\u2022 A training set (e, s), an empty rule set \u0393 = \u2205 2. Learn the grammar:\n\u2022 For each ei \u2208 e, find its \u03bb-meaning forest:\nfi = BUILDFOREST(ei). This gives the set (f, s). \u2022 Learn the generative model parameter :\n\u03b8 * = TRAINGENMODEL(f, s). \u2022 For each (fi, si) \u2208 (f, s), find the most probable \u03bb-hybrid tree hi, and then extract the grammar rules from it: hi = FINDHYBRIDTREE(fi, si,\u03b8 * ) \u0393 = \u0393 \u222a EXTRACTRULES(hi) 3. Output the learned grammar rule set \u0393. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "For experiments, we evaluated on the GEOQUERY dataset, which consists of 880 queries on U.S. geography. The dataset was manually labeled with \u03bbexpressions as their semantics in Zettlemoyer and Collins (2005). It was used in many previous research efforts on semantic parsing (Zettlemoyer and Collins, 2005;Wong and Mooney, 2006;Zettlemoyer and Collins, 2007;Kwiatkowski et al., 2010). The original dataset was annotated with English sentences only. In order to assess the generation performance across different languages, in our work the entire dataset was also manually annotated with Chinese by a native Chinese speaker with linguistics background 5 .\nFor all the experiments we present in this section, we use the same split as that of Kwiatkowski et al. (2010), where 280 instances are used for testing, and the remaining instances are used for learning. We further split the learning set into two portions, where 500 instances are used for training the models, which includes induction of grammar rules, training a language model, and computing feature values, and the remaining 100 instances are used for tuning the feature weights.\nAs we have mentioned earlier, we are not aware of any previous work that performs generation from formal logical forms that concerns both lexical acquisition and surface realization. The recent work by Angeli et al. (2010) presented a generation system from database records with an additional focus on content selection (selection of records and their subfields for generation). It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. Other earlier approaches such as the work of Wang (1980) and Shieber et al. (1990) made use of rule-based approaches without automatic lexical acquisition.\nWe thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007) 6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005;Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing MERT.", "publication_ref": ["b39", "b39", "b35", "b40", "b19", "b19", "b1", "b34", "b30", "b22", "b7", "b8", "b23", "b5", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Automatic Evaluation", "text": "For automatic evaluation, we measure the original IBM BLEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the TER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference) 7 . Note that TER measures the translation error rate, thus a smaller score indicates a better result. For clarity, we report 1\u2212TER scores. Following the tuning procedure as conducted in Galley and Manning (2010), we perform MERT using BLEU as the metric.\nWe compare our model against state-of-the-art statistical machine translation systems. As a baseline, we first conduct an experiment with the following naive approach: we treat the \u03bb-expressions as plain texts. All the bound variables (e.g., x in \u03bbx.state(x)) which do not convey semantics are removed, but free variables (e.g., state in \u03bbx.state(x)) which might convey semantics are left intact. Quantifiers and logical connectives are also left intact. While this naive approach might not appear very sensible, we merely want to treat it as our simplest baseline.\nAlternatively, analogous to the work of Wong and Mooney (2007a), we could first parse the \u03bbexpressions into binary tree structures with a deterministic procedure, and then linearize the tree structure as a sequence. Since there exists different ways to linearize a binary tree, we consider preorder, inorder, and postorder traversal of the trees, and linearize them in these three different ways.\nAs for our system, during the grammar learning phase, we initialize the generative model parameters with output from the IBM alignment model 1 (Brown et al., 1993) 8 , and run the \u03bb-hybrid tree generative model with the unigram emission assumption for 10 iterations, followed by another 10 iterations with the bigram assumption. Grammar rules are then extracted based on the \u03bb-hybrid trees obtained from such learned generative model parameters.\nSince MERT is prone to search errors, we run each experiment 5 times with randomly initialized feature weights, and report the averaged scores. Experimental results for both English and Chinese are presented in Table 1. As we can observe, the way that a meaning representation tree is linearized has a significant impact on the translation performance. Interestingly, for both Moses and Joshua, the preorder setting yields the best performance for English, whereas it is inorder that yields the best performance for Chinese. This is perhaps due to the fact that Chinese presents a very different syntactic structure and word ordering from English.\nOur system, on the other hand, employs a packed forest representation for \u03bb-expressions. Therefore, it eliminates the ordering constraint by encompassing exponentially many possible tree structures during both the alignment and decoding stage. As a result, our system obtains significant improvements in both BLEU and 1\u2212TER using the significance test under the paired bootstrap resampling method of Koehn (2004). We obtain p < 0.01 for all cases, except when comparing against Joshua-preorder for English, where we obtain p < 0.05 for both metrics.  ", "publication_ref": ["b28", "b31", "b10", "b17"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Human Evaluation", "text": "We also conducted human evaluation with 5 evaluators each on English and Chinese. We randomly selected about 50% (139) test instances and obtained output sentences from the three systems. Moses and Joshua were run with the top-performing settings in terms of automatic metrics (i.e., preorder for English and inorder for Chinese). Following Angeli et al. (2010), evaluators are instructed to give scores based on language fluency and semantic correctness, on the following scale: For each test instance, we first randomly shuffled the output sentences of the three systems, and presented them together with the correct reference to the evaluators. The evaluators were then asked to score all the output sentences at once. This evaluation process not only ensures that the annotators have no access to which system generated the out-  put, but also minimizes bias associated with scoring different outputs for the same input. The detailed and averaged results (with one standard deviation) for human evaluation are presented in Table 2 for English and Chinese respectively. For both languages, our system achieves a significant improvement over Moses and Joshua (p < 0.01 with paired t-tests), in terms of both language fluency and semantic correctness. This set of results is important, as it demonstrates that our system produces more fluent texts with more accurate semantics when perceived by real humans.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Additional Experiments", "text": "We also performed the following additional experiments. First, we attempted to increase the number of EM iterations (to 100) when training the model with the bigram assumption, so as to assess the effect of the number of EM iterations on the final generation performance. We observed similar performance. Second, in order to assess the importance of the two types of novel rules -subtree rules (type 2) and two-level \u03bb-hybrid sequence rules (type 3), we also conducted experiments without these rules for generation. Experiments show that these two types of rules are important. Specifically, type 3 rules, which are able to capture longer structural dependencies, are of particular importance for generating Chinese. Detailed results for these additional experiments are presented in Table 1.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Experiments on Variable-free Meaning Representations", "text": "Finally, we also assess the effectiveness of our model on an alternative meaning representation formalism in the form of variable-free tree structures. Specifically, we tested on the ROBOCUP dataset (Kuhlmann et al., 2004), which consists of 300 English instructions for coaching robots for soc-cer games, and a variable-free version of the GEO-QUERY dataset. These are the standard datasets used in the generation tasks of Wong and Mooney (2007a) and Lu et al. (2009). Similar to the technique introduced in Kwiatkowski et al. (2010), our proposed algorithm could still be applied to such datasets by writing the tree-structured representations as function-arguments forms. The higher order unification-based decomposition algorithm could be applied on top of such forms accordingly. For example, midf ield(opp) \u2261 \u03bbx.midf ield(x) \u00a1 opp. See Kwiatkowski et al. (2010) for more details. However, since such forms present monotonous structures, and thus give less alternative options in the higher-order unification-based decomposition process, it prevents the algorithm from creating many disjunctive nodes in the packed forest. It is thus hypothesized that the advantages of the packed forest representation could not be fully exploited with such a meaning representation formalism.\nFollowing previous works, we performed 4 runs of 10-fold cross validation based on the same split as that of Wong and Mooney (2007a) and Lu et al. (2009), and measured standard BLEU percentage and NIST (Doddington, 2002) scores. For experimentation on each fold, we trained a trigram language model on the training data of that fold, and randomly selected 70% of the training data for grammar induction, with the remaining 30% for learning of the feature weights using MERT. Next, we performed grammar induction with the complete training data of that fold, and used the learned feature weights for decoding of the test instances. The averaged results are shown in Table 3. Our approach outperforms the previous system WASP \u22121 ++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al. (2009). This set of results is particularly striking. We note how many people live in the state with the largest population density \u03bb-expression : rule(and(bpos(f rom goal line(our, jnum(n0.0, n32.0))), not(bpos(lef t(penalty area(our))))),dont(player our(n3), intercept)) Reference : player 3 should not intercept the ball if the ball is within 32 meters of our goal line and not in our left penalty area This work :\nif the ball is within 32 meters from our goal line and not on the left side of our penalty area then player 3 should not intercept it Figure 6: Sample English outputs for various datasets. For the variable-present dataset, we also show outputs from Moses and Joshua.\nthat the algorithm of Lu et al. (2009) is capable of modeling dependencies over phrases, which gives global optimization over the sentence generated, and works by building conditional random fields (Lafferty et al., 2001) over trees. But the algorithm of Lu et al. (2009) is also limited to handling treestructured meaning representation, and is therefore unable to accept inputs such as the variable version of \u03bb-expressions. Our algorithm works well by introducing additional new types of synchronous rules that are able to capture longer range dependencies. WASP \u22121 ++, on the other hand, also makes use of a synchronous parsing-based statistical machine translation approach. Their system, however, requires linearization of the tree structure for both alignment and translation. In contrast, our model directly performs alignment and translation from a packed forest representation to a sentence. As a result, though WASP \u22121 ++ made use of additional features (lexical weights), our system yielded better performance. Sample English output sentences are given in Figure 6.  In this work, we presented a novel algorithm for generating natural language sentences from their under-lying semantics in the form of typed lambda calculus. We tackled the problem by introducing a novel reduction-based weighted synchronous context-free grammar formalism, which allows sentence generation with a log-linear model. In addition, we proposed a novel generative model that jointly generates lambda calculus expressions and natural language sentences. The model is then used for automatic grammar induction. Empirical results show that our model outperforms state-of-the-art machine translation models, for both English and Chinese, in terms of both automatic and human evaluation. Furthermore, we have demonstrated that the model can also effectively handle inputs with a variablefree version of meaning representation. We believe the algorithm used for inducing the reduction-based synchronous grammar rules may find applications in other research problems, such as statistical machine translation and phrasal synchronous grammar induction. We are interested in exploring further along such directions in the future.", "publication_ref": ["b18", "b36", "b25", "b19", "b19", "b36", "b25", "b9", "b25", "b25", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank Tom Kwiatkowski and Luke Zettlemoyer for sharing their dataset, and Omar F. Zaidan for his help with Z-MERT.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Syntax directed translations and the pushdown assembler", "journal": "Journal of Computer and System Sciences", "year": "1969", "authors": "A V Aho; J D Ullman"}, {"ref_id": "b1", "title": "A simple domain-independent probabilistic approach to generation", "journal": "", "year": "2010", "authors": "G Angeli; P Liang; D Klein"}, {"ref_id": "b2", "title": "Trainable grammars for speech recognition", "journal": "The Journal of the Acoustical Society of America", "year": "1979", "authors": "J K Baker"}, {"ref_id": "b3", "title": "The Lambda Calculus, Its Syntax and Semantics (Studies in Logic and the Foundations of Mathematics", "journal": "", "year": "1985", "authors": "H P Barendregt"}, {"ref_id": "b4", "title": "The mathematics of statistical machine translation: Parameter estimation", "journal": "Computational Linguistics", "year": "1993", "authors": "P F Brown; V J Della Pietra; S A Della Pietra; R L Mercer"}, {"ref_id": "b5", "title": "An empirical study of smoothing techniques for language modeling", "journal": "", "year": "1996", "authors": "S F Chen; J Goodman"}, {"ref_id": "b6", "title": "Learning to sportscast: a test of grounded language acquisition", "journal": "", "year": "2008", "authors": "D L Chen; R J Mooney"}, {"ref_id": "b7", "title": "A hierarchical phrase-based model for statistical machine translation", "journal": "", "year": "2005", "authors": "D Chiang"}, {"ref_id": "b8", "title": "Hierarchical phrase-based translation", "journal": "Computational Linguistics", "year": "2007", "authors": "D Chiang"}, {"ref_id": "b9", "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "journal": "", "year": "2002", "authors": "G Doddington"}, {"ref_id": "b10", "title": "Accurate non-hierarchical phrase-based translation", "journal": "", "year": "2010", "authors": "M Galley; C D Manning"}, {"ref_id": "b11", "title": "Learning a compositional semantic parser using an existing syntactic parser", "journal": "", "year": "2009", "authors": "R Ge; R J Mooney"}, {"ref_id": "b12", "title": "The undecidability of unification in third order logic", "journal": "Information and Control", "year": "1973", "authors": "G P Huet"}, {"ref_id": "b13", "title": "A unification algorithm for typed \u03bbcalculus", "journal": "Theoretical Computer Science", "year": "1975", "authors": "G P Huet"}, {"ref_id": "b14", "title": "Using string-kernels for learning semantic parsers", "journal": "", "year": "2006", "authors": "R J Kate; R J Mooney"}, {"ref_id": "b15", "title": "Statistical phrase-based translation", "journal": "", "year": "2003", "authors": "P Koehn; F J Och; D Marcu"}, {"ref_id": "b16", "title": "Moses: open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "P Koehn; H Hoang; A Birch; C Callison-Burch; M Federico; N Bertoldi; B Cowan; W Shen; C Moran; R Zens; C Dyer; O Bojar; A Constantin; E Herbst"}, {"ref_id": "b17", "title": "Statistical significance tests for machine translation evaluation", "journal": "", "year": "2004", "authors": "P Koehn"}, {"ref_id": "b18", "title": "Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer", "journal": "", "year": "2004", "authors": "G Kuhlmann; P Stone; R Mooney; J Shavlik"}, {"ref_id": "b19", "title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "journal": "", "year": "2010", "authors": "T Kwiatkowski; L Zettlemoyer; S Goldwater; M Steedman"}, {"ref_id": "b20", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001", "authors": "J D Lafferty; A Mccallum; F C N Pereira"}, {"ref_id": "b21", "title": "Forest-based statistical sentence generation", "journal": "", "year": "2000", "authors": "I Langkilde"}, {"ref_id": "b22", "title": "Joshua: an open source toolkit for parsing-based machine translation", "journal": "", "year": "2009", "authors": "Z Li; C Callison-Burch; C Dyer; J Ganitkevitch; S Khudanpur; L Schwartz; W N G Thornton; J Weese; O F Zaidan"}, {"ref_id": "b23", "title": "Alignment by agreement", "journal": "", "year": "2006", "authors": "P Liang; B Taskar; D Klein"}, {"ref_id": "b24", "title": "A generative model for parsing natural language to meaning representations", "journal": "", "year": "2008", "authors": "W Lu; H T Ng; W S Lee; L Zettlemoyer"}, {"ref_id": "b25", "title": "Natural language generation with tree conditional random fields", "journal": "", "year": "2009", "authors": "W Lu; H T Ng; W S Lee"}, {"ref_id": "b26", "title": "The alignment template approach to statistical machine translation", "journal": "Computational Linguistics", "year": "2004", "authors": "F J Och; H Ney"}, {"ref_id": "b27", "title": "Minimum error rate training in statistical machine translation", "journal": "", "year": "2003", "authors": "F J Och"}, {"ref_id": "b28", "title": "BLEU: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "K Papineni; S Roukos; T Ward; W J Zhu"}, {"ref_id": "b29", "title": "Generation of paraphrases from ambiguous logical forms", "journal": "", "year": "1996", "authors": "H Shemtov"}, {"ref_id": "b30", "title": "Semantic-head-driven generation", "journal": "Computational Linguistics", "year": "1990", "authors": "M Shieber; G Van Noord; F C N Pereira; R C Moore"}, {"ref_id": "b31", "title": "A study of translation edit rate with targeted human annotation", "journal": "", "year": "2006", "authors": "M Snover; B Dorr; R Schwartz; L Micciulla; J Makhoul"}, {"ref_id": "b32", "title": "SRILM-an extensible language modeling toolkit", "journal": "", "year": "2002", "authors": "A Stolcke"}, {"ref_id": "b33", "title": "HMM-based word alignment in statistical translation", "journal": "", "year": "1996", "authors": "S Vogel; H Ney; C Tillmann"}, {"ref_id": "b34", "title": "On computational sentence generation from logical form", "journal": "", "year": "1980", "authors": "J Wang"}, {"ref_id": "b35", "title": "Learning for semantic parsing with statistical machine translation", "journal": "", "year": "2006", "authors": "Y W Wong; R J Mooney"}, {"ref_id": "b36", "title": "Generation by inverting a semantic parser that uses statistical machine translation", "journal": "", "year": "2007", "authors": "Y W Wong; R J Mooney"}, {"ref_id": "b37", "title": "Learning synchronous grammars for semantic parsing with lambda calculus", "journal": "", "year": "2007", "authors": "Y W Wong; R J Mooney"}, {"ref_id": "b38", "title": "Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems", "journal": "The Prague Bulletin of Mathematical Linguistics", "year": "2009", "authors": "O F Zaidan"}, {"ref_id": "b39", "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "journal": "", "year": "2005", "authors": "L Zettlemoyer; M Collins"}, {"ref_id": "b40", "title": "Online learning of relaxed CCG grammars for parsing to logical form", "journal": "", "year": "2007", "authors": "L Zettlemoyer; M Collins"}, {"ref_id": "b41", "title": "Learning contextdependent mappings from sentences to logical form", "journal": "", "year": "2009", "authors": "L Zettlemoyer; M Collins"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: The joint generative process of both \u03bb-meaning tree and its corresponding natural language sentence, which results in a \u03bb-hybrid tree.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Procedures\u2022f \u2190 BUILDFOREST(e) It takes in a \u03bb-expression e and outputs its \u03bbmeaning forest f . (Sec. 3.1) \u2022\u03b8 \u2190 TRAINGENMODEL(f, s) It takes in \u03bb-meaning forest-sentence pairs (f, s), performs EM training of the generative model, and outputs the parameters\u03b8. (Sec. 3.2) \u2022 h \u2190 FINDHYBRIDTREE(f, s,\u03b8) It finds the most probable \u03bb-hybrid tree h containing the given f -s pair, under the generative model parameters\u03b8. (Sec. 4.3) \u2022 \u0393 h \u2190 EXTRACTRULES(h) It takes in a \u03bb-hybrid tree h, and extracts a set of grammar rules \u0393 h out of it. (Sec. 4.3) Algorithm", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: The algorithm for learning the grammar rules", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "\u2192 \u03bbg.\u03bbf.\u03bbx.g(x) \u2227 f (x) \u00a1 e, t 1 \u00a1 e, t 2 , e, t 2 e, t 1Type 2: e, t \u2192 \u03bbx.loc(miss r, x) \u2227 state(x) , states that the mississippi runs through e, t \u2192 \u03bbx.loc(miss r, x) , that the mississippi runs through Type 3: e, t \u2192 \u03bbf.\u03bbx.state(x) \u2227 \u2203y.[f (y) \u2227 next to(y, x)] \u00a1 e, t 1 , the states bordering e, t 1 e, t \u2192 \u03bby.\u03bbx.loc(y, x) \u2227 state(x) \u00a1 e 1 , states that e 1 runs throughFigure3: Example synchronous rules that can be extracted from the \u03bb-hybrid tree of Figure2.e, t \u2192 \u03bbx.loc(miss r, x) , that the mississippi runs through", "figure_data": "Type 1:e, e, t\u2192\u03bby.\u03bbx.next to(x, y) , borderinge, t"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Tree fragment : e, t 2 : \u03bbg.\u03bbf.\u03bbx.g(x) \u2227 f (x) \u00a1 e, t 1 \u00a1 e, t 2 e, t 1 : \u03bby.\u03bbx.loc(y, x) \u00a1 e 1 runs through e 1 : . . . that e, t 2 : \u03bbx.state(x) states Source : (substitution) \u03bby . \u03bbg.\u03bbf.\u03bbx.g(x) \u2227 f (x) \u00a1 [\u03bby.\u03bbx.loc(y, x) \u00a1 y ] \u00a1 \u03bbx.state(x) \u00a1 e 1 (two \u03b2-conversions) \u21d2 \u03bby .[\u03bbf.\u03bbx.loc(y , x) \u2227 f (x) \u00a1 \u03bbx.state(x)] \u00a1 e 1 \u2192 \u03bby.\u03bbx.loc(y, x) \u2227 state(x) \u00a1 e 1 , states that e 1 runs through", "figure_data": "Target: \"states that e 1 runs through\"(\u03b2-conversion) \u21d2 \u03bby .\u03bbx.loc(y , x) \u2227 state(x) \u00a1 e 1 (\u03b1-conversion) \u21d2 \u03bby.\u03bbx.loc(y, x) \u2227 state(x) \u00a1 e 1Rule:e, t"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Human evaluation results on English and Chinese generation. FLU: language fluency; SEM: semantic correctness.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Variable-present dataset \u03bb-expression : argmax(x, river(x) \u2227 \u2203y.[state(y) \u2227 next to(y, india s) \u2227 loc(x, y)], len(x)) Reference : what is the longest river that flows through a state that borders indiana Moses : what is the states that border long indiana Joshua : what is the longest river surrounding states border indiana This work : what is the longest river in the states that border indiana \u03bb-expression : density(\u03b9x.loc(argmax(y, loc(y, usa co) \u2227 river(y), size(y)), x) \u2227 state(x)) Reference : which is the density of the state that the largest river in the united states runs through Moses : what is the population density in lie on the state with the smallest state in the us Joshua : what is the population density of states lie on the smallest state in the us This work : what is the population density of the state with the largest river in the us Variable-free datasets \u03bb-expression : population(largest one density(state all)) Reference : what is the population of the state with the highest population density This work :", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Performance on variable-free representations", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": ") \u2227 \u2203x1.[loc(miss r, x1) \u2227 state(x1) \u2227 next to(x1, x0)]\".", "formula_coordinates": [4.0, 201.03, 226.85, 218.31, 16.87]}, {"formula_id": "formula_1", "formula_text": "P \u03bbx.loc(miss r, x), that the mississippi runs through = \u03c6(m \u2192 wYw|p1) \u00d7 \u03c8(that e 1 runs through|p1) \u00d7\u03c1(p2|p1, arg 1 ) \u00d7 \u03c6(m \u2192 w|p2) \u00d7 \u03c8(the mississippi|p2)", "formula_coordinates": [4.0, 72.0, 382.88, 213.73, 48.44]}, {"formula_id": "formula_2", "formula_text": "\u03c4 \u2192 p \u03bb , hw, \u223c(1)", "formula_coordinates": [4.0, 385.41, 416.18, 154.59, 16.87]}, {"formula_id": "formula_3", "formula_text": "w(D) = r\u2208D i fi(r) w i \u00d7 pLM (\u015d) w LM (2)", "formula_coordinates": [5.0, 103.02, 635.35, 195.78, 20.79]}, {"formula_id": "formula_4", "formula_text": "s = s arg max D s.t. e(D)\u2261e w(D)(3)", "formula_coordinates": [6.0, 375.99, 383.89, 164.02, 18.01]}], "doi": ""}