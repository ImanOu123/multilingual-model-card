{"title": "FRUIT : Faithfully Reflecting Updated Information in Text", "authors": "Robert L Logan; Alexandre Passos; Sameer Singh; Ming-Wei Chang", "pub_date": "", "abstract": "Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of faithfully reflecting updated information in text (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. We provide benchmark results for popular generation systems as well as EDIT5-a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications. 1 * Work done during an internship at Google Research.", "sections": [{"heading": "Introduction", "text": "Information changes on a constant basis. Every day, athletes are traded to new teams, and musicians and actors produce new albums and TV shows. Maintaining textual knowledge bases to keep track of these changes requires considerable community effort. For instance, a team of 120K volunteer editors make 120 edits to English Wikipedia every minute, and write 600 new articles a day. 2 As the knowledge base grows, the amount of maintenance effort is compounded by the need to keep the knowledge base consistent; e.g., each edit may render information in one of the existing 6.3M+ articles obsolete.\nAssistive writing technologies have the potential to substantially reduce the burden of keeping text corpora up to date and consistent. However, existing work has mainly focused on correcting grammar (Wang et al., 2020), reducing repetitive typing , and following rhetorical directives (Sun et al., 2021), whereas the problem of producing edits grounded in external knowledge has received little attention (Kang et al., 2019). In contrast, numerous works have developed systems for distilling external knowledge into text (e.g., Wikipedia article generation) by treating the problem as multi-document summarization (Liu et al., 2018;Shi et al., 2021) or data-to-text generation (Bao et al., 2018;Parikh et al., 2020). However, these systems are not useful for updating existing texts as they can only generate text from scratch.\nTo help endow writing assistants with grounded editing capabilities, we introduce the novel generation task of faithfully reflecting updated information in text (FRUIT), where the goal is to incorporate new information into an existing piece of text. An illustration is provided in Figure 1. Given an outdated Wikipedia article and collection of new information about the article's subject, FRUIT requires updating the existing text so that it is consistent with the new information, as well as adding text to reflect new salient facts, e.g., in Figure 1, the first sentence is updated to reflect that Tom Kristensson now drives in the Junior World Championship, and new sentences are added to reflect his achievements in 2019 and 2020.\nFRUIT presents several unique challenges. First, unlike many generation tasks, models cannot obtain good performance by solely relying on their parametric world knowledge. Whenever the provided evidence contradicts parametric knowledge, the model must prefer the evidence, which recent work has shown is difficult for pretrained language Figure 1: Illustration of the FRUIT task. An outdated original article and relevant new information are provided as inputs, and the goal is to generate the updated article. In this example, the original article about Tom Kristensson was written in 2020, and the new information is comprised of updated information about Tom Kristensson that has been added to other Wikipedia articles between 2020 and 2021. Given these inputs, the goal is to produce the updated 2021 version of article. Models need to identify the relevant supporting facts (orange and teal) to generate faithful updates while ignoring superfluous information (grey). models (Krishna et al., 2021;Longpre et al., 2021). Second, the generated text needs to be faithful to both the original article and the new evidence, except when evidence invalidates information in the existing article. Finally, this task requires models to jointly read and analyze evidence from both textual and tabular sources and determine which is relevant and which can be ignored, thus combining challenging aspects of both multi-document summarization and data-to-text generation.\nTo facilitate research on this task, we release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised (\"silver\") update-evidence pairs. This dataset is produced by comparing pairs of English Wikipedia snapshots to identify updates to an article between two snapshots, and associating information from the other articles that supports these updates under a distant supervision assumption. As there is no guarantee that updates in the later Wikipedia snapshots can be supported by the collected evidence, we also collect a \"gold\" evaluation set of 914 human annotated update-evidence pairs where unsupported claims have been removed without disturbing fluency. We train and validate our models using silver data and then evaluate the final performance using gold data.\nWe establish initial benchmark results for a number of trivial and neural sequence-to-sequence base-lines. We also introduce EDIT5, a T5-based model specially adapted for grounded editing, which establishes state-of-the-art performance on FRUIT-WIKI. Through an extensive set of analyses, we identify a number of failure modes needed to be improved upon in order to obtain better performance on FRUIT-WIKI, as well as other interesting topics for future work on this task. We additionally release our data collection pipeline to allow researchers to produce data from future Wikipedia snapshots and other languages, which we show to produce high-quality silver data.", "publication_ref": ["b30", "b26", "b10", "b15", "b25", "b2", "b12", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "The FRUIT Task", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Definition", "text": "In this section we introduce the task of faithfully reflecting updated information in text (FRUIT). Given an input piece of text focused on a topic or event, along with a collection of potentially new information about the subject of the text, the goal is to update the input text to reflect the new information. A concrete illustration of the task is provided in Figure 1. The original piece of text along with its updates are shown on the left, while the new information is shown on the right.\nFormally, we assume access to pair of texts, A t and A t , pertaining to a given subject, written at times t and t (respectively). In addition, we assume access to a set of new information, a.k.a., evidence, E t\u2192t = E 1 , . . . , E |E| , mentioning the subject written between times t and t . As is shown in Figure 1, the evidence can contain structured objects (e.g., excerpts from tables) as well as unstructured text. Given A t and E t\u2192t the goal is produce the updated text A t .\nSuccessful completion of this task requires a number of complex and inter-related reasoning capabilities. For one, models must be able to identify which evidence contradicts existing portions of the source article, and which evidence introduces new salient information about the subject in order to correctly choose whether to alter the existing text vs. add new text. For example, in Figure 1 the first sentence is updated to reflect that Tom Kristensson now races in a different competition, whereas new sentences are added describing his achievements in the years 2019 and 2020. Models must also be able to determine whether a given piece of evidence should be used at all, i.e., perform content selection. For example, in Figure 1, the number of rounds won by Kristennsen appears in the evidence but does not correspond to any piece of updated text. Although some evidence may not appear in the updated article, the converse is not true, the system should aim to generate an updated article where all the updates are faithful to the evidence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "In this section we introduce important considerations for evaluating FRUIT systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluate on Updated Text", "text": "There is often considerable overlap between the original and updated text. As we will see in Section 5 this poses a challenge for standard evaluation metrics like ROUGE (Lin, 2004) as systems can achieve high scores without making any updates. In this work, we propose to evaluate FRUIT systems using an alternative metric, UpdateROUGE, that only considers updated sentences instead full texts. For example, in Figure 1, the reference for UpdateROUGE only consists of the first and last two sentences.\nEvaluate Faithfulness Ensuring that generations faithfully reflect information in the evidence and updated article is crucial. However measuring faithfulness of generations is an active area of research (\u00c7elikyilmaz et al., 2020) and adapting existing metrics to the FRUIT task is non-trivial.\nAs a simple proxy for faithfulness, we choose to measure the token overlap between named entities appearing in the generation and the target article/evidence, where entities are identified using the named entity recognizer used by Guu et al. (2020) to perform salient span masking. We specifically introduce the following measurements: 1. Unsupported Entity Tokens. This metric shows the average number of entity tokens appearing in generated updates that do not appear in the source article or evidence. This is intended to capture the overall amount of unfaithful text, focusing on entities, where higher numbers indicate less faithfulness. 2. Entity Precision and Recall. These metrics capture the overlap of mentions between the generated updates and the target. Entity precision measures the fraction of entity tokens appearing in the generated updates that appear in target entities, whereas entity recall measures the fraction of entity tokens in the target that appear in the entities in generated updates. The latter is similar to UpdateROUGE but only evaluated on entities, and thus, potentially less sensitive to paraphrasing.\nParametric Knowledge Consideration FRUIT systems should incorporate information from the provided evidence into the update, and not information that happened to be present during training or pretraining. In this work we attempt to address this by evaluating models only on updates that were made to the text after the data used to pretrain and finetune the model was collected. As this setup precludes evaluating models trained after 2020 on FRUIT-WIKI, we release our data collection pipeline so that researchers can produce evaluation datasets from future versions of Wikipedia.", "publication_ref": ["b14", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Collection and Analysis", "text": "As discussed in the introduction, keeping track of new information and then updating articles to reflect that information requires a massive amount of manual effort. Thus, in order to scalably collect sufficient data for training and evaluating FRUIT systems, some amount of automation is likely required. In this section we introduce the FRUIT-WIKI dataset and associated data collection pipeline, which allows the automatic collection of high-quality training and evaluation data for FRUIT from pairs of Wikipedia snapshots.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pipeline", "text": "Our data collection pipeline produces distantly annotated training and evaluation data from pairs of Wikipedia snapshots. We will refer to the earlier snapshot as the source snapshot, and the later snapshot as the target snapshot.\nStep 1. Collect Article Updates We compute the diff between the introductory sections of articles appearing in both the source and target snapshot to identify all of the material that has been updated (which will serve as A t and A t ). We also compute the diff between the non-introductory sections of articles to find new mentions of the subjects of other articles (which will serve as E t\u2192t ). These mentions can take the form of sentences in the text, as well as new table rows and list entries. Entities are disambiguated using Wikipedia hyperlinks.\nStep 2. Filter Stylistic Updates A large number of edits to Wikipedia are stylistic (Daxenberger and Gurevych, 2012), and are therefore irrelevant to our task. In the next step of the pipeline, we attempt to filter articles that have only been superficially edited by keeping only those where at least one new added entity appears in the target snapshot.\nStep 3. Identify Supporting Evidence In the last step of our pipeline, we seek to determine which pieces of evidence in E t\u2192t justify each of the updated sentences in A t . To do so, we make the following distant supervision assumption: an updated sentence a \u2208 A t containing an added entity s is substantiated by a piece of evidence E \u2208 E t\u2192t only if s is also mentioned in E. The accuracy of the annotations produced by this assumption will be measured in Section 3.3.\nOur pipeline is implemented using Apache Beam, 3 to allow for distributed processing. We  plan on releasing the code upon publication to enable other users to produce FRUIT data from future Wikipedia snapshots, as well as languages other than English.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "FRUIT-WIKI", "text": "We run our pipeline on English Wikipedia snapshots from Nov. 20, 2019to Nov. 20, 2020 to produce the training dataset, and from Nov. 20, 2020 to June 1, 2021 to produce the evaluation dataset. Detailed statistics are provided in Table 1.\nOn average, there are around 3 to 4 updates per article, and around 7 pieces of associated evidence. About 80% of updates require some form of content selection, i.e., ignoring some evidence, when performing updates. We find that only a third of the updates are substantiated by one or more pieces of evidence according to our distant supervision assumption. Thus, the remaining updates are either: a) superficial changes to the source article, or b) additions of new claims that are unsupported with respect to the collected evidence. The latter is a particular issue as these claims can cause the model to learn to hallucinate during training, and should be impossible for the model to guess during evaluation. Through the usage of human annotations and carefully selected evaluation metrics we will study the extent to which this is an issue throughout the rest of the paper.\nWe categorize articles in our dataset using the Wikimedia Foundation's topic model (Asthana and Halfaker, 2018). The distribution of topics is displayed in Figure 2. We find that the majority (approximately 50%) of updates deal with cultural topics (e.g., sports, media, personal biographies), and geographic entities (e.g., countries, states) which intuitively are likely to be affected by current events. while there are few updates to STEM-and historyrelated articles.", "publication_ref": ["b0"], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Gold Evaluation Data", "text": "To address the issue of unsupported claims during evaluation, we hired a team of 9 annotators to produce a \"gold\" evaluation subset of our test dataset.  We collect annotations for 914 update-evidence pairs where each instance is corrected to ensure that all of the updates are supported. For the remainder of the paper we will refer to the distantly supervised test dataset annotations as \"silver\".\nAnnotation Process For each instance, annotators were shown the source article, evidence, and a marked up copy of the target article. In the marked up article, each updated sentence was highlighted and prefixed with reference labels to the supporting evidence identified by our pipeline. The correction process proceeded in two steps. In the first step, annotators were asked to highlight all of the unsupported claims and incorrect reference labels in the target article. In the second step, annotators were then asked to remove the unsupported text and minimally update the article to preserve fluency.\nAnnotators attended an initial 30 minute training and were provided regular feedback from the authors during the early stages of annotation. To ensure data quality, an additional annotator was hired with the sole job of checking the other annotator's work and correcting their mistakes. In total annotators spent roughly 500 hours on annotation. The annotation interface and a completed annotation are shown in Figure A7 in the Appendix.\nAgreement We measure annotator agreement using a subset of 100 instances that were annotated by multiple annotators. Following Chen et al. (2015) and Shi et al. (2021), we quantify agreement by computing the evaluation metrics described in Section 2.2. The results are provided in Table 2. We observe high inter-annotator agreement with all scores in the 80s and 90s.  data with one exception: the fraction of substantiated updates has increased.\nTo measure the quality of our silver data, we reapply the approach used to measure inter-annotator agreement to compute agreement between the gold and silver annotations. We also measure the reference agreement, i.e., the fraction of reference labels kept by the annotators. Results are provided in Table 3. We find that agreement is high with most scores in the 80s, a strong indication that the data produced by our pipeline is high quality.\nIn particular, the high UpdateROUGE scores provide further evidence that only a small amount of the updated text in the weakly supervised data is unsupported, while the high reference agreement indicates that our distant supervision assumption is usually accurate.", "publication_ref": ["b5", "b25"], "figure_ref": [], "table_ref": ["tab_2", "tab_5"]}, {"heading": "Methods", "text": "In this section we introduce baseline methods to establish initial benchmark results on FRUIT-WIKI. We consider trivial approaches that copy task inputs, as well as T5, a neural sequence-to-sequence baseline which has shown strong performance on related tasks such as summarization (Raffel et al., 2020;Rothe et al., 2021) We additionally introduce EDIT5, a variant of T5 that produces a sequence of edits instead of the entire updated text, and employs additional tweaks to improve performance.", "publication_ref": ["b20", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Copy Baselines", "text": "The first set of baselines we introduce are trivial methods that merely copy the input. We consider two variants: \u2022 Copy Source: Generates a copy of the source article, and \u2022 Copy Source + Evidence: Generates a copy of the source article concatenated with the evidence. Our evaluation metrics only apply to unstructured text, however the evidence may contain structured tables. In order to convert these tables to text, we apply a conventional linearization scheme (Lebret  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T5", "text": "T5 (Raffel et al., 2020) is a pretrained sequence-tosequence (Sutskever et al., 2014) model based on the transformer architecture (Vaswani et al., 2017). Similar to the previous section we experiment with two variants:\n\u2022 T5: Only includes the source article in its input,\n\u2022 T5 + Evidence Inputs: Includes both the source article and evidence in the input. Tabular inputs are linearized using the same approach described in the previous section. Experiments are performed using the JAX-based T5X library. 4 Hyperparameters and additional training details are described in Appendix B.", "publication_ref": ["b20", "b27", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "EDIT5", "text": "Lastly, we introduce EDIT5, which improves upon the T5-based approach described in the previous section through the usage of a compressed output format that removes the need to write the entire update from scratch and encourages content planning. The output is modified in two ways:\nFirst, as the majority of text in the target article is copied from the source, we replace any copied sentence with a single copy token identifying the sentence, e.g., if the second sentence is copied it is replaced by the token [2]. Similar to a copy mechanism (See et al., 2017), this allows the model to dedicate less capacity to repeating sequences from the input. As the resulting output resembles  that produced by the diff data comparison utility, we refer to this as a diff-formatted output.\nSecond, before each update we insert a sequence of reference tokens identifying the pieces of evidence that support the update, e.g., if the first and third piece of evidence in E t\u2192t support an update then the update is prefaced by (1)(3). This approach, inspired by the use of entity chains for summarization , trains the model to plan which references to use before generating an update. These reference tokens are removed from the output text of the model prior to computing the evaluation metrics.\nAn example of the EDIT5 output format is provided in Figure 3, and a comparison to the T5 output format is provided in Appendix D. Training details and hyperparameters match the setup described in Section 4.2.", "publication_ref": ["b23"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Results and Analysis", "text": "Baseline results on the gold evaluation data are provided in Table 4a, and ablation results are provided in Appendix A. In general, we find that the copy baselines perform worse than T5 and T5 performs worse than EDIT5. Notably, the copy source baseline rightfully scores zero on all metrics, while we will later find that it obtains a high ROUGE score.\nAlthough our models are trained on silver data, they still obtain good performance on the gold evaluation set. This shows the high quality of our silver data collection pipeline, and T5's ability to generate reasonable updates based on the evidence.\nFor the T5 baselines, we find that adding evidence to the input results significant increase in all metrics, demonstrating that using the evidence is crucial to obtaining good performance.\nEDIT5 obtains additional 3-5% absolute increase in all performance metrics compared to T5, establishing EDIT5 as a strong baseline for future systems to be compared against. The reduction of unsupported entity tokens implies that EDIT5 hallucinates less frequently than T5 models. Results are provided for different model sizes to illustrate how performance scales with parameter counts.\nExample Output An example EDIT5 output is provided in Figure 4, and additional outputs in Appendix E. The examples illustrate important features of the task. In Figure 4 the goal is to update the Wikipedia article for Holli Sullivan to reflect her new role of Secretary of State of Indiana. In the reference, this information is reflected in an updated version of the first sentence as well as in a newly added last sentence. An additional sentence is added after the first sentence paraphrasing the introduction of the source article, which describes Sullivan's previous position as a member of the Indiana House of Representatives.\nIn the EDIT5 output for this example, information is only added at the end of the article. While the model correctly states that Sullivan was appointed to be Secretary of State by Governor Eric Holcomb, as well as includes additional context surrounding Sullivan's appointment that is paraphrased from the evidence, there are some issues with the output. First, because the first sentence of the article is not updated there is conflicting information about Sullivan's current position. Second, the added sentence hallucinates that Sullivan was appointed in January 2020 when she was actually  appointed in March 2021, a fact that directly appears in the evidence.\nCategorizing Errors To better understand the types of errors made by EDIT5, we review a random sample of 100 of its predictions on the gold evaluation data and categorize them as either: grounded updates, meaning all generated claims are supported, ungrounded updates, meaning at least one unsupported claim appears in the output, or no updates, meaning the model did not predict any updates. For grounded updates we additionally keep track of how many updates include additional content not present in the ground truth update, or are missing content that appears in the ground truth update. For ungrounded updates we track whether an incorrect number/date appears in the update, the model distorted evidence, i.e., paraphrased or combined claims in the evidence in a way that changed their meaning, or hallucinated new claims unrelated to the evidence.\nThe results of this analysis are presented in Table 4b. We find that EDIT5 makes no mistakes on half of the examples, however a substantial portion of these updates had some issue with content selection. Of the incorrect updates, the most common mistake was incorrect numbers and dates, followed by hallucinations, and finally distorted evidence. This suggests that improving numeracy could be a fruitful line of study in future work on this task.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "ROUGE is Problematic", "text": "We provide ROUGE Fscores for each of the baseline models on the gold evaluation data in Table 5. In contrast to the previous results, we find that the simple copy source baseline attains a strong score of 77.4 despite making no updates. This is better than the T5 baseline results and comparable to the EDIT5 results. This illustrates the importance of evaluating on updates rather than the whole text.\nSilver Data is Useful for Evaluation The results in Section 3.3 demonstrate high agreement between the silver and gold evaluation data which begs the question: can silver data be used in place Holli Sullivan is an American politician who serves in the Indiana House of Representatives as a member of the Republican Party. In 2014 the district 78 seat for state Representative was vacated by Suzanne Crouch, who had been appointed state Auditor. \u2026Text omitted to save space\u2026 In 2017, she co-authored House Bill 1002, which provided for a long term plan for sustaining roads and bridges in Indiana including a phase-in shift of all gas tax to be dedicated to a dedicated infrastructure fund. That same session she authored a bill which created a strategic plan to reduce cervical cancer.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Original Article", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "New Information", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ground Truth", "text": "Holli Sullivan is an American politician who is the 62nd and current secretary of state of Indiana since March 2021. As a member of the Republican Party, she previously represented the 78th district in the Indiana House of Representatives from 2014 to 2021. \u2026Copied text\u2026 In 2021, Holli was named the 62nd Secretary of State of Indiana by Governor Eric Holcomb.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EDIT5", "text": "Copied text... In January 2020 Representative Sullivan was appointed by Governor Eric Holcomb to serve out the term of former Secretary of State Connie Lawson, who announced in February 2021 that she planned on resigning from office.  of gold data for evaluation? To answer this, we measure the Spearman rank correlation between the gold baseline results in Table 4a and silver baseline results (provided in Table A2 of the Appendix to save space). Rank correlations for each of the metrics are shown in Table 6. Overall we find high rank correlation for each of the metrics, which suggests silver evaluation performance is a reliable indicator of gold performance. Thus, models whose pretraining data overlaps FRUIT-WIKI may be evaluated and compared on data produced by running our pipeline on future Wikipedia snapshots without requiring further human evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_2", "tab_10"]}, {"heading": "Secretary of State of Indiana", "text": "Controllability The improvement we obtained from EDIT5 over T5 implies that more controls can be added into the model. In this section we investigate whether additional control provided by the users can improve the overall generations. We follow Keskar et al. (2019) and , and provide more detailed instruction by adding control codes, i.e., special tokens, to the input that  instruct the model whether to add, copy, edit or remove a sentence, as well as which evidence to use when making an addition or edit. We use the target text to provide oracle labels for the control code, and see if the EDIT5 can take advantage of the codes. Example inputs and predictions are provided in Figure A6 of the Appendix.\nResults on the gold evaluation data are provided in Table 7. Including oracle control codes in the input produces a substantial 10% absolute improvement in all metrics besides unsupported tokens. This demonstrates that increased user control has the potential to produce updates that more closely resemble the desired output.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Related Work", "text": "Early work on writing assistants largely focuses on grammar error correction; for a survey see Wang et al. (2020). Neural models have expanded the capabilities of writing assistants to solve a wider variety of tasks including: autocompletion , and following rhetorical directives such as paraphrasing, elaborating, etc. (Sun et al., 2021). In this work, we seek to expand these capabilities further to producing grounded updates, which has been previously studied by Kang et al. (2019), however only for post-modifier generation.\nAs our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016;Bao et al., 2018;Parikh et al., 2020;Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016;Liu et al., 2018;Shi et al., 2021). In particular, the hyperlink-based approach for associating evidence to articles is directly inspired by these works, and our annotation procedure for removing unsupported text directly draws from Parikh et al. (2020).\nDetermining which facts contradict claims in the existing article is a central topic of work on fact extraction and verification (Thorne et al., 2018). Recently, Schuster et al. (2021) introduced the VITAMIN-C dataset of factual revisions to Wikipedia articles and the task of factually consistent generation. This work differs from FRUIT in that it only focuses on sentences and does not require adding new facts or content selection.\nOur work is also related to the TAC 2008 Update Summarization Task (Dang and Owczarzak, 2008), which involves summarizing information about a topic that does not overlap with an existing summary, instead of updating an existing summary to reflect new information.", "publication_ref": ["b30", "b26", "b10", "b13", "b2", "b6", "b1", "b15", "b25", "b28", "b22", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this work we introduced FRUIT, a novel text generation task where the goal is to update an article to reflect new information about its subject. To enable research on this task, we formulated a pipeline for extracting weakly supervised training and evaluation data from pairs of Wikipedia snapshots, and collected data for the years 2019-2020 and 2020-2021, as well as human annotated gold evaluation data. We additionally provided results for several strong baselines, that demonstrate both the feasibility of this task, as well as strong correlation between gold and distantly supervised data evaluation performance that establishes the trustworthiness of future data produced using our pipeline. This work lays the foundation for future research into making faithful updates to entries in textual knowledge bases. One limitation of this work is that the metrics we use to evaluate faithfulness are all entity-centric, and thus may overstate the performance of models whose edits include the correct entities but misspecify the relations between them or other facets of the evidence. Accordingly, one promising direction for future work on this task is to develop more robust metrics for measuring faithfulness.\nAn additional promising direction for future work is to consider open settings of evidence collection, where other forms of updated information such as excerpts from news articles could be used to justify edits in place of updates to other entries in the same knowledge base. Relatedly, we also recommend studying this task in streaming settings, where updates arrive in sequential fashion, in addition to the batch setting considered in this work.\nFinally, there are a number of promising directions for improving model performance on this task. In particular, copy mechanisms (See et al., 2017) have been widely used in data-to-text tasks (Wiseman et al., 2017), and may help mitigate issues such as the mistranscribed dates we saw in Section 5.  (2) Elizabeth Lynne Cheney ( ; born July 28, 1966) is an American attorney and politician who has served as the U.S. Representative for since 2017. She was the Chair of the House Republican Conference, the third-highest position in the House Republican leadership. She is the third woman elected to that position after Deborah Pryce and Cathy McMorris Rodgers. She held several positions in the U.S. State Department during the George W. Bush administration, notably as Deputy Assistant Secretary of State for Near Eastern Affairs and Coordinator for Broader Middle East and North Africa Initiatives. She promoted regime change in Iran while chairing the Iran Syria Policy and Operations Group with Elliott Abrams. In 2009 Cheney and Bill Kristol founded Keep America Safe, a nonprofit organization concerned with national security issues that advocated the positions of the former Bush administration. She was a candidate for the 2014 election to the U.S. Senate in Wyoming, challenging three-term incumbent Mike Enzi, before withdrawing from the race. In the House of Representatives, she holds the seat her father held for a decade, representing Wyoming from 1979 to 1989. Cheney is a neoconservative. She later supported the second impeachment of Donald Trump for his role in the 2021 storming of the U.S. Capitol. (2) [0] [1] [2] [3] [4] [5] [6] In the House of Representatives, she holds the seat that was held by her father from 1979 to 1989. (6) She is known for her neoconservative foreign policy views, and her affiliation with the Trump campaign.  ", "publication_ref": ["b23", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "D Input and Output Formats", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Kelvin Guu, Kenton Lee, Kristina Toutanova, and the anonymous reviewers for their helpful feedback. This work is funded in part by the DARPA MCS program under Contract No. N660011924033. Additionally, Robert is supported in part by the Irvine Initiative in AI, Law, and Society fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "This paper introduces a dataset and system for updating an existing piece of text to incorporate information from external evidence. Depending on the veracity of the external evidence, systems for solving this task could potentially be abused by bad actors to spread misinformation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Ablation Study", "text": "We perform an ablation study to measure the impact of the modifications made to the target output of EDIT5. The results are provided in Table A1 We observe that both the diff format and including reference tokens have a positive impact on the evaluation metrics, with reference tokens having the larger impact.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Silver Baseline Results", "text": "UpdateROUGE Target Entity Evid.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Instructions Flagging Problematic Instances", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "The goal of this task is to collect evaluation data for a system that can automatically update Wikipedia articles from new information about the article's subject. The sections below provide the text of the original passage to be updated, a collection of added information about the article subject, and the text of the updated passage.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "What we need from you", "text": "The issue we are faced with is that some of the udpated text may not be supported by the added information section. We need you to identify all of the unsupported information, and edit the article to remove unsupported text while preserving fluency. Please make sure that your edits only remove information; while you may need to write some text to ensure that the edited passage is fluent, no new facts should be added (even if they are supported).\nThe original passage and added information are below this box. We request that you first read the updated passage in the green box, and highlight any unsupported text. Then copy the contents from the green box to the orange box and edit them so that all of the text is supported.\nIf you have questions please do not hesitate to email: REDACTED If you feel that there are fundamental issues (e.g., more than 50% of sentences are unsupported) that make the updated article either incomplete or influent please use the below checkbox to flag the instance for review.\nWe still request you finish editing even if you flag the instance for review.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Original Passage -DO NOT CHANGE", "text": "Added Information -DO NOT CHANGE Flag for Review [0] Joshua Christian Kojo King (born 15 January 1992) is a Norwegian professional footballer who plays as a forward for Championship club Bournemouth and the Norway national team.\n[1] King was signed by Manchester United from V\u00e5lerenga in 2008.\n[2] After loan spells with Preston North End, Borussia M\u00f6nchengladbach, Hull City and Blackburn Rovers, he signed permanently with Blackburn in January 2013, before switching to Bournemouth in May 2015.\n[3] After representing Norway at under-15, under-16, under-18, under-19 and under-21 levels, King made his senior international debut against Iceland in 2012, and scored his first international goal against Cyprus later that year. (5) Oslo Notable residents -Sport [Sonja Henie (1912-1969  (1) In February 2021, in a deadline day deal, he returned to the top flight with a move to Everton.\n[3] After representing  levels, King made his senior international debut against Iceland in 2012, and scored his first international goal against Cyprus later that year.  -15, under-16, under-18, under-19 and under-21 levels, King made his senior international debut against Iceland in 2012, and scored his first international goal against Cyprus later that year.\nStep 1\nStep 2\nThe section below above the text of the updated passage. Unchanged sentences from the original passage are in grey, while added or updated sentences are in black.\nWe have tried to automatically detect which pieces of added information justify the changed text. If a justification is detected, then the edited sentence will be prefaced with the delimiter of the added information. For example: (0) (1) Updated sentence means that we think that added information 0 and 1 justify (at least some of the edit).", "publication_ref": ["b33"], "figure_ref": [], "table_ref": []}, {"heading": "What to do for this column", "text": "-Highlight any text/evidence delimiters that are unsupported by the original passage or added information, using this red color (in the custom section).\n-Do not edit the text.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "What to do for this column", "text": "-Copy the highlighted updated passage from the previous step.\n-Edit text so that: a) any unsupported text is removed, and b) the passage is still fluent.\n-Do not add any new information ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "With few eyes, all hoaxes are deep", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2018", "authors": "Sumit Asthana; Aaron Halfaker"}, {"ref_id": "b1", "title": "Wikiwrite: Generating wikipedia articles automatically", "journal": "IJCAI/AAAI Press", "year": "2016-07-15", "authors": "Siddhartha Banerjee; Prasenjit Mitra"}, {"ref_id": "b2", "title": "Tableto-text: Describing table region with natural language", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Junwei Bao; Duyu Tang; Nan Duan; Zhao Yan; Yuanhua Lv; Ming Zhou; Tiejun Zhao"}, {"ref_id": "b3", "title": "Gmail smart compose: Real-time assisted writing", "journal": "ACM", "year": "2019-08-04", "authors": "Mia Xu Chen; Benjamin N Lee; Gagan Bansal; Yuan Cao; Shuyuan Zhang; Justin Lu; Jackie Tsay; Yinan Wang; Andrew M Dai; Zhifeng Chen; Timothy Sohn; Yonghui Wu"}, {"ref_id": "b4", "title": "WikiTableT: A large-scale data-to-text dataset for generating Wikipedia article sections", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Mingda Chen; Sam Wiseman; Kevin Gimpel"}, {"ref_id": "b5", "title": "Microsoft coco captions: Data collection and evaluation server", "journal": "ArXiv", "year": "2015", "authors": "Xinlei Chen; Hao Fang; Tsung-Yi Lin; Ramakrishna Vedantam; Saurabh Gupta; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b6", "title": "ENT-DESC: Entity description generation by exploring knowledge graph", "journal": "", "year": "2020", "authors": "Liying Cheng; Dekun Wu; Lidong Bing; Yan Zhang; Zhanming Jie; Wei Lu; Luo Si"}, {"ref_id": "b7", "title": "Overview of the tac 2008 update summarization task", "journal": "", "year": "2008", "authors": "Trang Hoa; Karolina Dang;  Owczarzak"}, {"ref_id": "b8", "title": "A corpus-based study of edit categories in featured and non-featured Wikipedia articles", "journal": "", "year": "2012", "authors": "Johannes Daxenberger; Iryna Gurevych"}, {"ref_id": "b9", "title": "", "journal": "", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Ming-Wei Chang"}, {"ref_id": "b10", "title": "PoMo: Generating entity-specific post-modifiers in context", "journal": "", "year": "2019", "authors": "Jun Seok Kang; Robert Logan; Zewei Chu; Yang Chen; Dheeru Dua; Kevin Gimpel; Sameer Singh; Niranjan Balasubramanian"}, {"ref_id": "b11", "title": "Ctrl: A conditional transformer language model for controllable generation", "journal": "ArXiv", "year": "2019", "authors": "Bryan Nitish Shirish Keskar; Lav R Mccann; Caiming Varshney; Richard Xiong;  Socher"}, {"ref_id": "b12", "title": "Hurdles to progress in long-form question answering", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Kalpesh Krishna; Aurko Roy; Mohit Iyyer"}, {"ref_id": "b13", "title": "Neural text generation from structured data with application to the biography domain", "journal": "", "year": "2016", "authors": "R\u00e9mi Lebret; David Grangier; Michael Auli"}, {"ref_id": "b14", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b15", "title": "Generating wikipedia by summarizing long sequences", "journal": "", "year": "2018-04-30", "authors": "J Peter; Mohammad Liu; Etienne Saleh; Ben Pot; Ryan Goodrich; Lukasz Sepassi; Noam Kaiser;  Shazeer"}, {"ref_id": "b16", "title": "Entity-based knowledge conflicts in question answering", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Shayne Longpre; Kartik Perisetla; Anthony Chen; Nikhil Ramesh; Chris Dubois; Sameer Singh"}, {"ref_id": "b17", "title": "Planning with entity chains for abstractive summarization", "journal": "ArXiv", "year": "2021", "authors": "Shashi Narayan; Yao Zhao; Joshua Maynez; Gonccalo Simoes; Ryan T Mcdonald"}, {"ref_id": "b18", "title": "", "journal": "", "year": "", "authors": "Ankur Parikh; Xuezhi Wang; Sebastian Gehrmann; Manaal Faruqui; Bhuwan Dhingra; Diyi Yang"}, {"ref_id": "b19", "title": "ToTTo: A controlled table-totext generation dataset", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Dipanjan Das"}, {"ref_id": "b20", "title": "Exploring the limits of transfer learning with a unified text-totext transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b21", "title": "A thorough evaluation of task-specific pretraining for summarization", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Sascha Rothe; Joshua Maynez; Shashi Narayan"}, {"ref_id": "b22", "title": "Get your vitamin C! robust fact verification with contrastive evidence", "journal": "", "year": "2021", "authors": "Tal Schuster; Adam Fisch; Regina Barzilay"}, {"ref_id": "b23", "title": "Get to the point: Summarization with pointergenerator networks", "journal": "Long Papers", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"ref_id": "b24", "title": "Adafactor: Adaptive learning rates with sublinear memory cost", "journal": "PMLR", "year": "2018-07-10", "authors": "Noam Shazeer; Mitchell Stern"}, {"ref_id": "b25", "title": "DESCGEN: A distantly supervised datasetfor generating entity descriptions", "journal": "", "year": "2021", "authors": "Weijia Shi; Mandar Joshi; Luke Zettlemoyer"}, {"ref_id": "b26", "title": "Franck Dernoncourt, Balaji Vasan Srinivasan, and Mohit Iyyer", "journal": "", "year": "2021", "authors": "Simeng Sun; Wenlong Zhao; Varun Manjunatha; Rajiv Jain; Vlad Morariu"}, {"ref_id": "b27", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014-12-08", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"ref_id": "b28", "title": "FEVER: a large-scale dataset for fact extraction and VERification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "James Thorne; Andreas Vlachos; Christos Christodoulopoulos; Arpit Mittal"}, {"ref_id": "b29", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b30", "title": "A comprehensive survey of grammar error correction. ArXiv, abs", "journal": "", "year": "2005", "authors": "Yu Wang; Yuelin Wang; Jie Liu; Zhuo Liu"}, {"ref_id": "b31", "title": "Challenges in data-to-document generation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Sam Wiseman; Stuart Shieber; Alexander Rush"}, {"ref_id": "b32", "title": "Evaluation of text generation: A survey", "journal": "", "year": "2006", "authors": "Asli \u00c7elikyilmaz; Elizabeth Clark; Jianfeng Gao"}, {"ref_id": "b33", "title": "The novel won the 2020 Booker Prize, and was a finalist for the 2020 National Book Award for Fiction and the 2021 John Leonard Prize. It was also a finalist for the 2020 National Book Critics Circle Award. James Kelman Critical reception In his essay \"The Importance of Glasgow in My Work\", he compares the presentation of working-class and Scottish characters with those of the traditional \"upper-class\" English protagonist", "journal": "", "year": "", "authors": ""}, {"ref_id": "b34", "title": "The Undocumented Americans'' (One World), Raven Leilani", "journal": "", "year": "", "authors": "Karla Cornejo Martin's); ; Villavicencio; Straus Farrar; ) Giroux; Megha Majumdar"}, {"ref_id": "b35", "title": "Figure A6: Using Control Codes", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "3https://beam.apache.org/", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Topic Distribution.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "4 https://github.com/google-research/t5x (2) Tom Krister Kristensson (born 30 April 1991) is a Swedish rally driver, who drives in the Junior World Championship. [1] [2] (1) In the 2019 season of JWRC, Tom finished second behind Jan Solans. (2) The next season he went on to become the 2020 Junior World Rally champion.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: EDIT5 Output Format. Instead of generating the fully updated text, EDIT5 generates sequences of edited sentences, copy tokens (e.g., [2], which means copy the second sentence), and reference tokens (e.g., (1), which means the following sentence should use the first piece of evidence).", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "( 2 ) [ 0 ]20Elizabeth Lynne Cheney (; born July 28, 1966) is an American attorney and politician serving as the U.S. Representative for since 2017. [1] Cheney is the House Republican Conference Chair, the third-highest position in GOP House leadership.[2] She is the third woman elected to that position after Deborah Pryce and Cathy McMorris Rodgers. [3] Cheney is the elder daughter of former Vice President Dick Cheney and Lynne Cheney. [4] She held several positions in the U.S. State Department during the George W. Bush administration. [5] She has been politically active on behalf of the Republican Party and is a co-founder of Keep America Safe, a nonprofit organization concerned with national security issues. [6] She was a candidate for the 2014 election to the United States Senate in Wyoming, challenging the three-term incumbent Mike Enzi, before withdrawing from the race.[7]  In the House of Representatives, she holds the seat that was held by her father from 1979 to 1989. [8] She is known for her hawkish foreign policy views. [CONTEXT] (0) Andy Biggs U.S. House of Representatives -Tenure -2021 storming of the United States Capitol On January 12, 2021, Biggs called on fellow GOP Representative Liz Cheney (R-WY) to resign from her leadership position within the Republican Caucus, after she voted in favor of Donald Trump's second impeachment. (1) 116th United States Congress Leadership -House of Representatives -Minority (Republican) leadership * House Minority Leader and Chair of the House Republican Steering Committee: Kevin McCarthy * House Minority Whip: Steve Scalise * Chair of the House Republican Conference: Liz Cheney * Vice Chair of the House Republican Conference: Mark Walker * Secretary of the House Republican Conference: Jason Smith * Chair of the House Republican Policy Committee: Gary Palmer * Chair of the National Republican Congressional Committee: Tom Emmer * House Republican Chief Deputy Whip: Drew Ferguson (2) A Call for American Renewal INTRODUCTION The manifesto was released one day after the ousting of Representative Liz Cheney as chair of the House Republican Conference, and was largely seen as a reaction against the influence of Trumpism within the Republican Party. (3) List of nicknames used by Donald Trump Domestic political figures -Table-0-11 [HEADER] [COL] Nickname [COL] Personal name [COL] Notes [ROW] id=\"The Warmonger\" [COL] The Warmonger [COL] Liz Cheney [COL] U.S. representative for Wyoming's at-large congressional district; Chair of the House Republican Conference (4) Conscience vote Practice in various countries -United States Similarly, when House Republican leadership decided not to whip votes against the second impeachment of Donald Trump, Liz Cheney--the third-highest-ranking Republican--referred to the matter as a \"vote of conscience\".", "figure_data": ""}, {"figure_label": "A1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure A1 :A1Figure A1: Input Format.", "figure_data": ""}, {"figure_label": "A2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure A2 :A2Figure A2: T5 Output Format.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "(0) (1) (2) (3) (4) Cheney is under fire for her role in the second impeachment of Donald Trump in January 2021.", "figure_data": ""}, {"figure_label": "A3", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure A3 :A3Figure A3: EDIT5 Output Format.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Dataset Statistics. We use 10% of the training data as our validation data.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Inter-Annotator Agreement.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Analysis Statistics for the gold evaluation dataset are provided in Table1. Overall, they closely resemble the statistics for the distantly supervised", "figure_data": "UpdateROUGE 1 2 LEntity Prec. RecallReference Agreement83.7 81.2 83.4 90.4100.084.5"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Gold and Silver Annotation Agreement. Quality of Silver Annotations by using the Gold as the reference.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Entity recall is not 100% for the Copy Source + All Evidence baseline due to lexical variation in entity mentions.", "figure_data": "UpdateROUGE 1 2 LEntity Prec. Recall Tokens Unsup.Grounded Updates Additional Content50 15Copy Source + All Evidence T5-Large + Evidence Input 44.3 29.4 36.8 62.2 0.0 0.0 0.0 0.0 18.8 6.9 12.0 37.9 31.1 18.4 24.4 52.7 EDIT5-Small 41.2 27.3 35.3 62.4 EDIT5-Base 47.0 32.1 39.7 62.2 EDIT5-Large 46.3 32.4 39.6 67.2 EDIT5-3B 47.4 34.0 41.1 69.90.0 64.9  *  44.9 50.7 44.9 54.9 53.1 52.50.00 0.00 2.67 2.34 1.71 2.28 1.54 1.58Missing Content Ungrounded Updates 35 22 Number/Date 21 Distorted Evidence 11 Hallucination 14 No Updates 14(a)(b)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "(a) Model Results on Gold Evaluation Data. EDIT5 outperforms T5 models in all metrics. (b) Error Analysis for EDIT5-3B. We find that the model makes correct, grounded updates on 50% of the inspected articles. For incorrect updates, ungrounded numbers/dates are one of the main sources of error.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "ROUGE Scores Are Insensitive to Edits.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "current office holder is Holli Sullivan, who was appointed by Governor Eric Holcomb to serve out the term of former Secretary of State Connie Lawson, who announced in February 2021 that she planned on resigning from office. Example EDIT5 Output vs Ground Truth. Color coding indicates alignment between the new information and the edits. EDIT5 updates the original article by paraphrasing sentences from the textual evidence, however misses relevant information in the table, and generates an incorrect date.", "figure_data": "#NameTook OfficeLeft Office62Holli SullivanMarch 16, 2021-Figure 4: UpdateROUGEEntityUnsup.12LPrec. Rec. tokens100.0 100.0 94.3 75.4 92.892.8"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Spearman Rank Correlation Between Gold and Silver Performance Metrics.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Controllability. Using control codes that indicate which sentences to delete, add or edit, and which evidence to use, can greatly improve generation.", "figure_data": ""}], "formulas": [], "doi": "10.1145/3274290"}