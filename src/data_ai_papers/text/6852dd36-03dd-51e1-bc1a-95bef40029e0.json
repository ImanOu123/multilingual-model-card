{"title": "Entity Tracking in Language Models", "authors": "Najoung Kim; Sebastian Schuster", "pub_date": "", "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform nontrivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface. * Equal contribution. Author order was determined by a fair process of letting Cookie the cat pick a colored ball. Q: Box 1 contains the book. Box 2 contains the apple. Box 4 contains the brain. Move the book into Box 2. Put the bell into Box 4. Move the bell and the brain into Box 5. Box 2 contains ____ A: the apple and the book", "sections": [{"heading": "Introduction", "text": "A key prerequisite to long-context understanding and generating coherent text is the ability to accurately represent entities as the discourse unfolds (Karttunen, 1976;Groenendijk and Stokhof, 1991;Heim, 2002;Nieuwland and Van Berkum, 2006;Kamp et al., 2011, i.a.). For example, consider the following example in the context of a recipe:\n(1)\nPut the eggs, sugar, flour, and baking powder in a bowl and mix to form a light batter. Make sure that the final batter does not contain any lumps of flour or sugar. In order to understand this instruction, several distinct abilities are necessary:\nNew discourse entity recognition: recognizing when new discourse entities are introduced. E.g., a bowl introduces a new discourse entity but the final batter or any lumps of ... does not.\nCoreference resolution: associating referring expressions with discourse entities. E.g., a light batter and the final batter refer to the same entity.\nDiscourse entity tracking: tracking the state changes made to each discourse entity. E.g., the eggs are put into the bowl and mixed with the other ingredients.\nThere exist many datasets that aim to evaluate these abilities (e.g., Walker et al., 2006;Pradhan et al., 2012;Rahman and Ng, 2012;Weston et al., 2015;Chen et al., 2018;Bamman et al., 2020;Uryupina et al., 2020) and many NLP models that aim to solve these tasks (e.g., Haghighi and Klein, 2010;Lee et al., 2011;Hill et al., 2016;Henaff et al., 2017;Ji et al., 2017;Lee et al., 2017;Bosselut et al., 2018;Gupta and Durrett, 2019a,b;Aina et al., 2019;Toshniwal et al., 2020;Wu et al., 2020). In the context of large language models (LLMs), Tenney et al. (2019), , and Sorodoc et al. (2020) found that representations of LSTMs and Transformer-based models such as BERT (Devlin et al., 2019) do capture coreference relations. Lo\u00e1iciga et al. (2022) and Schuster and Linzen (2022) found that pretrained models are able to detect whether noun phrases introduce discourse entities, albeit not fully systematically.\nThe question of whether LLMs can track the state of discourse entities, however, has mostly been indirectly evaluated. Toshniwal et al. (2022) showed that GPT-2 (Radford et al., 2019) can learn to predict valid chess moves based on a compact, nonlinguistic description of previous moves. Similarly,  showed that a GPT model trained on Othello can predict valid next moves, and that these predictions are tied to the model's internal representations of the board states. Still, these results do not tell us whether LLMs track state changes expressed in natural language discourses. The most relevant evaluation is Li et al. (2021), where they tested whether model representations encode entity states described in naturalistic text. Using a probing classifier, they found that the states can be decoded from T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) with high accuracy. However, as we show in a reanalysis of their results (Section 2), they do not provide definitive evidence for entity tracking. Hence, whether LLMs can track entities during the processing of natural language discourse remains an open question. Contributions This work attempts to answer this question by developing a task targeted towards evaluating a language model's ability to track state changes of discourse entities (illustrated in Figure 1). We use this novel task to evaluate GPT-3 (Brown et al., 2020), GPT-3.5, and Flan-T5 (Chung et al., 2022) without any finetuning. We find that only models in the GPT 3.5 series, which have been trained on both text and code, are able to perform non-trivial entity tracking. We then show that a smaller language model (T5) can learn to perform non-trivial entity tracking and also demonstrates some capacity to generalize to state descriptions with more operations or with low lexical overlap. Our results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface. More broadly, our task can also serve as a useful tool for investigations into emergent world models in LMs (e.g., Tsai et al., 2023). 1 2 Reanalysis of Li et al. (2021) We start from examining Li et al. (2021), the most relevant work to ours. They adapted two exist-ing datasets, Alchemy (Long et al., 2016) and TextWorld (C\u00f4t\u00e9 et al., 2019), to test a model's ability to track state changes of an entity. The input to the model is a text description of the initial world state followed by state-changing instructions. Based on this description, the model is expected to identify the correct final state of each entity. For example, for Alchemy, the model receives formulaic descriptions of 7 beakers containing different amounts of colored liquids, followed by instructions that manipulate their contents such as pouring the liquid from one beaker into another, or draining a beaker. Given an input like (2), the model is expected to recognize that the first beaker has 4 units of brown liquid, the second beaker has 2 units of red liquid, and the third beaker is empty.\n(2)\nThe first beaker has 1 green, the second beaker has 2 red, the third beaker has 3 red. Pour the last red beaker into beaker 1. Mix.\nUsing such descriptions, Li et al. (2021) found that a probing classifier that takes as input the encoding of these descriptions from T5 or BART is able to correctly predict the state of 75-76% of the entities, suggesting some degree of success on entity tracking. However, this conclusion becomes questionable when the datasets and the results are scrutinized further. Specifically, we conducted a fine-grained analysis of the success cases of the Alchemy experiment. In this experiment, the state of each beaker was probed after each state-changing instruction. Because each instruction targets at most two beakers (e.g., pour X into Y) and there are 7 beakers in total, there is a sparse representation of cases probing a beaker that actually underwent a change in the dataset. Indeed, 62.7% of all beaker states probed were identical to the initial state, meaning that a simple baseline that always predicts the initial state already achieves 62.7% accuracy (this is also noted by Li et al.). A second potential for shortcuts was the high rate of empty final states (32.4%). 2 For these cases, the initial state can often be entirely disregarded, due to the presence of an emptying instruction such as Drain the fourth beaker. This instruction alone is sufficient to predict the fourth beaker's final state independent of its initial state. Therefore, such examples are also not best suited to fully assess entity tracking. Given the high prevalence of these two trivial scenarios (87.6% in total), only 12.4% of the datapoints can be considered as truly assessing state changes unfolding over a discourse context. If the accuracy is computed on the trivial and non-trivial cases separately, the probing classifier achieves 86.8% accuracy on trivial cases but only 3.1% accuracy on non-trivial cases, showing that most of the reported success derives from the trivial cases.\nIn summary, our reanalysis suggests that the results of Li et al. (2021) do not provide conclusive evidence for non-trivial state tracking abilities in language models. 3 However, it remains unclear whether this is due to issues with the setup or a true lack of entity tracking capacity. To this end, we propose a new behavioral evaluation.\n3 Task Design and Dataset", "publication_ref": ["b23", "b14", "b18", "b37", "b54", "b42", "b45", "b55", "b7", "b1", "b53", "b17", "b26", "b20", "b19", "b21", "b27", "b5", "b0", "b50", "b49", "b48", "b13", "b33", "b47", "b51", "b43", "b30", "b44", "b29", "b52", "b30", "b30", "b34", "b30", "b30"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Desiderata", "text": "The ability to track entities should be largely independent of specific linguistic forms. For a model that can properly track entities, it should not matter whether one talks about beakers or recipes or which specific syntactic constructions are used. This makes it an interesting ability to evaluate in the context of assessing whether and how meaning is represented, since at least classic language models are only trained on forms (Bender and Koller, 2020). At the same time, this independence of form and entity states that should hold for true entity tracking poses a challenge for evaluation, since one needs to ensure that the state of entities cannot be predicted from individual lexical items or phrases (such as the word drain in the Alchemy dataset, as discussed in Section 2). Furthermore, language models pretrained on large corpora may have learned common states of entities; for instance, that eggs often end up in a bowl. For these reasons, any task that evaluates entity tracking abilities should conform to the following four desiderata:\n1. The probed states of entities should not follow similar distributional patterns to those that are likely to be present in the pretraining data (see also Linzen, 2020).\n2. Individual words or phrases should not predict by themselves the state of an entity without considering the previous discourse in order. These properties cannot be guaranteed with naturalistic datasets such as recipes (Kiddon et al., 2015), science texts (Dalvi et al., 2019), or the Alchemy and TextWorld datasets, which have been previously used to evaluate entity tracking abilities. We therefore programmatically generated datasets for which these properties hold.", "publication_ref": ["b2", "b24", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "We take inspiration from Winograd (1971) and Li et al. (2021) in designing our data. Our datasets consist of text descriptions of a particular state of the world followed by a sequence of changes. The worlds contain boxes that can be filled with objects. The objects can be placed inside the box, taken out of the box, or moved from one box to another. We define a world W as W = (O, n, m, e) where O is a set of objects, n is the number of boxes, m is the maximum number of objects one box can contain, and e is the expected number of objects in each box in the initial world states. For our datasets, we used n = 7, m = 3, e = 2, and used a set of nouns denoting items that can plausibly fit inside a box (e.g., book, rock, brain; |O| = 100), selected from a list of words with frequency greater than 27 in the British National Corpus (BNC; Leech et al. 2001).\nA dataset consists of multiple distinct scenarios. A scenario consists of an initial state and a set of operations applied to this initial state. We fixed the number of operations (NumOps) in each scenario to 12. We randomly sampled 2200 scenarios, where the initial state and the 12 operations were both randomly sampled. The sampling process is designed such that only valid operations given the current world state can be sampled. The initial state and the operations were converted into naturalistic descriptions using predefined templates. Relation to Desiderata We selected the task of moving objects across boxes because this is a domain where lexical contents of the entities do not offer cues to predict the outcome of state changes (Desideratum 1). We did not include an operation that empties a box that allows the previous discourse to be discarded (Desideratum 2). For Desideratum 2, we furthermore considered experiments using operation descriptions with greater context dependence. Specifically, we tested scenarios with an additional Move contents of Box N to Box M operation that does not explicitly mention the object names, and scenarios where object descriptions in the operations can only be fully disambiguated by knowing the current state of a box. For Desideratum 3, we considered experiments where the phrasing of the states and operations differ entirely between demonstration/finetuning and evaluation (see Table 2). Finally, for all experiments, we computed a \"signature\" of every initial state that indicates the number of objects contained in each box. 4 Then, we ensured that there were no two examples with identical initial descriptions modulo the object names where one appeared in the training split and the other one in the evaluation split. This prevents this task from being solvable via slot-filling (Desideratum 4). 5", "publication_ref": ["b56", "b30", "b28"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Task", "text": "We define the entity tracking task as follows. Given a natural language description of the initial state of the world followed by 0-12 state-changing operations, the content of each box at the end of the description must be correctly identified. To evaluate this, we created a test example for each box after each operation. This corresponds to n \u00d7 (NumOps + 1) examples per scenario (91 exs. in our datasets). Each example is formulated in the style of a cloze test. That is, the input describes the initial state followed by a sequence of operations, ending in Box N contains __. The expected output is the correct set of objects in Box N based on the prefix description. See Appendix B for an example.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment 1: In-context Demonstration", "text": "In the first set of experiments, we evaluated pretrained LMs using a small number of in-context 4 For example, the signature of an initial state in which the first box contains two objects and the rest contains 1 object each would be 2111111. 5 Additionally, compared to the Alchemy setup, our setup also has a benefit of requiring fewer additional reasoning abilities. The beaker domain in Alchemy requires the model to count and perform simple arithmetic (e.g., adding one unit of liquid followed by adding two units of liquid results in three units of liquid). Moreover, some of the operations in Alchemy require knowledge about how colors are combined (e.g., mixing red and green liquids results in a brown liquid). The boxes domain removes these requirements. demonstrations of the entity tracking task. This provides a way to probe the model without requiring substantial supervision for the task, as well as guiding the model to output the final state in a consistent format that can be automatically assessed. 6", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We used models that are known to support task adaptation via in-context demonstrations: GPT-3 175B (davinci: Brown et al. 2020), GPT-3.5 (text-davinci-003 7 ), and Flan-T5 (base and XL: Chung et al. 2022). The little information that OpenAI revealed about their models 8 suggests that davinci is an autoregressive language model primarily trained on text corpora. text-davinci-003 was trained on the language modeling objective on a mix of text and code, and additionally tuned with human feedback using reinforcement learning. Flan-T5 is based on T5, a sequence-to-sequence model trained on a denoising objective, that has been further instruction-finetuned on a battery of tasks. This has been shown to promote better responses to instructions, both with and without demonstrations (Chung et al., 2022). We evaluated the GPT models through the OpenAI API and the Flan-T5 using the HuggingFace library (Wolf et al., 2020). See Table 1 for a summary of the models, and Appendix C for implementation details.\nWe compared these models against a baseline that randomly outputs 0 to m = 3 objects from the set of objects that appeared in the same clauses as the box in question. Note that this baseline is much stronger than a fully random baseline that selects outputs from all mentioned objects.", "publication_ref": ["b57"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Prompting and Demonstrations", "text": "Our prompts consist of: (a) a general instruction for the task, (b) two examples of the task to demonstrate the expected format, (c) an initial state description followed by a series of operations, and (d) an incomplete sentence Box N contains ___ that the model should complete (see Appendix E for full prompts). We used demonstrations that output the state of all boxes at once. However, in early experiments, Flan-T5 frequently only output the state of the first box even when the in-context demonstrations contained final descriptions of all box states.  Therefore, for Flan-T5, we adjusted the prompts to output each box individually. 9 Demonstration/Test Mismatch for Form-meaning Disentanglement (AltForms) As discussed in Sections 3.1-3.2, we additionally experimented with a setup where the demonstration and test examples were mismatched in the form of the descriptions of the states and operations. Under this setup, the models were evaluated with set of object names and phrasings of the state and operation descriptions that were different from the demonstration examples (see Table 2). Except for the determiner the and the preposition into, the two sets share no words (although subwords may be shared depending on tokenization). Scenarios with Greater Context Dependence (MoveContents, AmbiRef) As also discussed in Sections 3.1-3.2, we experimented with two additional scenarios with greater context dependence. The first scenario (MoveContents) introduces an additional operation Move contents of Box N to Box M that moves all objects in Box N to Box M, that does not provide an explicit enumeration of objects being moved. This requires the model to rely on the preceding description to identify the set of objects being moved, further removing room for heuristics that allow the prediction of the final state without composing the initial description and the operations in temporal order. The second scenario (AmbiRef) adds adjectival modification to object names (e.g., the big brain and the small brain),\n9\nTo verify that this difference in the task format does not underestimate the accuracy of the GPT models, we also conducted an experiment in which we prompted GPT to only output the state of one box at a time. We found that, contrary to the Flan-T5 models, the accuracy of GPT was lower when we prompted it to output individual boxes than prompting it to output all boxes, so we can rule out that this difference in the task format is underestimating GPT's performance. where the adjective can be omitted in some of the operations, depending on the state of the box being described. Specifically, the modifier is dropped if there is only one object of a specific type in a box (e.g., Move the brain from Box 1 to Box 2 if there is only one brain in Box 1). When predicting the contents of a box, the model is instructed to output the object type with the correct adjective to fully disambiguate the referring expression for the object. This again requires composition of the initial description and the operations in temporal order to correctly interpret the otherwise ambiguous object mentions. (See Appendix B for example scenarios and operations.)", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Evaluation", "text": "We estimated the entity tracking capacity of the models by computing the accuracy of predicting the contents of each box after each operation. Given that we rely on arbitrary-length cloze completion to predict the contents, we had to score unconstrained generations. While we only considered instances as correct where the output mentions all objects (and no additional objects) in a given box, our evaluation allowed for minor deviations from the exact form of the response. Namely, we allowed the objects to appear in any order, the object names may be separated by commas or and, and both complete noun phrases with a determiner (e.g., the furby) and bare nouns (e.g., furby) are considered correct.\nThe task becomes intrinsically harder as more operations are applied to a specific box, since the initial state description needs to be combined sequentially with all subsequent operations. Further, similarly to our observation in Section 2, every operation changes the state of at most two boxes.  This implies that the number of datapoints corresponding to fewer operations is much greater than the number of datapoints for more operations. For these reasons, we report accuracy as a function of the number of operations affecting a particular box rather than reporting aggregate accuracy, and show all results with 95% confidence intervals.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Figure 2 shows the prediction accuracy for different number of operations that affected a box (e.g., 3 indicates that three operations changed the content of the box after the initial state  3 shows the prediction accuracy of text-davinci-003 on a representative subsample 10 of our data. The blue line represents the performance when the descriptions in the demonstra- 10 For each number of operations n affecting a box, we sampled 100 states with at least one example with n operations. tion and the test examples have low lexical overlap. As the comparison to the original results (red line) shows, the disjoint demonstrations did lead to a small drop in performance when there were more than two operations affecting a box. Nevertheless, text-davinci-003 was able to predict the correct state of entities in many cases, further adding support for its non-trivial entity tracking capacity.  Greater Context Dependence Finally, we evaluated text-davinci-003 on two additional datasets described in Section 4.2: the AmbiRef dataset where adjectival modification can be omitted depending on the current context, and the Move-Contents dataset where a new operation Move contents of Box N to M that requires the contents of Box N to be contextually identified. Figure 4 shows the performance of text-davinci-003 on these two datasets compared to our random baseline. As these plots show, even in these more challenging cases, we observe non-trivial entity tracking abilities. However, as the number of operations grows, performance rapidly approaches the random baseline, suggesting that entity tracking becomes increasingly more brittle as more state changes need to be considered jointly.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Discussion", "text": "Our results show that among the models we evaluated, only GPT-3.5 text-davinci-003 exhibit non-trivial entity tracking behavior. While its performance does decrease as the number of operations increases, the model still produced many accurate predictions even after six or seven sequences of operations. Furthermore, through the AltForms experiment with low lexical overlap between demonstration/test, we also ruled out the possibility that the demonstrations are teaching the model this task or that the model is primarily relying on superficial slot-filling heuristics. Therefore, we conclude that text-davinci-003 does have some capacity to track discourse entities in linguistically expressed contexts. To a lesser extent, we also observed this capacity in the highly context-dependent AmbiRef and MoveContents experiments. This provides further evidence against superficial heuristics in the performance we observe; at the same time, this highlights scenarios in which even the most recent models exhibit difficulties.\nOn the other hand, entity tracking behavior did not surface in GPT-3 davinci (likely of similar size as GPT-3.5 text-davinci-003), a model pretrained primarily on text corpora on the next word prediction objective. This was also true for denoising models that have been finetuned on many tasks combined with instructions and demonstrations: the Flan-T5 models also showed near-zero accuracy on non-trivial examples.\nOur results overall show that there exists a language model that can perform entity tracking to some degree, but this capacity does not necessarily surface in all sufficiently large models trained on large corpora. Then, which factors are responsible for this difference? Given that davinci and text-davinci-003 differ along at least two dimensions (text-davinci-003 is based on a model that was trained on code and it was trained with additional human feedback (Ouyang et al., 2022); see Table 1), our initial results do not shed light on what exactly contributes to this difference. We therefore conducted a follow-up experiment where we compared a range of GPT-3 and GPT-3.5 models to identify factors that contribute to the stark difference between davinci and text-davinci-003. 11 Training on Code Encourages Entity Tracking Behavior As Table 1 shows, two key dimensions of variation across models are additional training on human feedback and pretraining on code. If additional training on human feedback imbues language models with the ability to track entities, all models except for GPT-3 davinci and GPT-3.5 code-davinci-002 should be able to track entities. If, on the other hand, pretraining on code leads to better entity tracking, we expect all GPT-3.5 models to outperform GPT-3 on our task. As Figure 5 shows, GPT-3.5 models that have been trained on code systematically outperformed GPT-3 models, including code-davinci-002 that was not trained on human feedback. This suggests that a substantial representation of code in the pretraining data is beneficial for a language model's entity tracking capacity to surface. A further question that our results so far do not answer is to what extent model size matters and whether models at the scale of Flan-T5 can also exhibit non-trivial entity tracking behavior. Since there exist no smaller models that have been trained with the same objective and training data as the GPT-3.5 models, we explore this question through finetuning experiments with T5.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Experiment 2: Finetuning", "text": "We investigated whether smaller models at the scale of T5 can learn to track entity states through a series of experiments where we provide supervised training to the models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Train/test splits", "text": "As discussed in Section 3.1, one challenge of evaluating entity tracking abilities is distinguishing this capacity from simple heuristics such as templatic slot-filling. We therefore designed various types of training/evaluation mismatches that block several possible shortcuts as described below.\nBase Split Here, we used the same format for training and evaluation examples. All initial states differed across training and evaluation to block simple slot-filling heuristics, as discussed in Section 3.2. NumOps Split The NumOps split restricts the maximum number of operations within a single example in the training set to 2, but includes up to 12 operations in the evaluation set. This split was intended to test whether a finetuned model is able to generalize to longer sequences of operations than it has seen during finetuning.\nVocab/AltForms Splits The vocab split tests whether objects that are not part of the set of objects used during training can also be adequately tracked. We compiled a list of comparatively infrequent object names (e.g., pomelo, furby, Flav-R-Straw; not in BNC) and sampled the training and test sets using two completely disjoint sets of object names. The training set used the infrequent object list and the test set used the original object list. The AltForm split follows the design described in Section 4.2. These splits aim to tease apart whether the model learns to associate specific words/phrases with the operations or whether finetuning leads to more generalizable entity tracking behavior. AmbiRef/MoveContents Splits The AmbiRef and MoveContents splits follow the design described in Section 4.2. These splits aim to test whether the model can learn to interpret operations that are underspecified without considering the current state of the affected boxes. For these splits, the training and test examples share the same format.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We evaluated T5-base, the best-performing model in Li et al. (2021), by finetuning it on each of the datasets described above. As an additional baseline, we compared against T5 with randomly initialized parameters trained directly on our datasets.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "Pretrained T5 can Learn to Perform Entity Tracking As shown in Figure 6 (left), finetuning T5 leads to near-perfect accuracy on the base split. This suggests that the model is capable of learning this task. Training a randomly initialized T5 did not yield the same result: the accuracy of a model trained from random weights is considerably lower, due to the model almost exclusively predicting that a box is empty. These two results suggest that pretraining is crucial for the model to be able to learn this task. Furthermore, the model's entity tracking capacity is robust to novel object names at test time, with only minor degradation on accuracy (Figure 6, middle). Training only on operation sequences with a maximum length of 2 (NumOps split) leads to a larger degradation in performance for longer operation sequences, but even for longer operation sequences, the model is able to infer the correct final state in more than 45% of the cases. Finally, the model performance does degrade substantially when the training examples have low lexical overlap with test examples (Figure 6, right). Nevertheless, model performance remains above the random baseline when the model is trained on up to 12 operations (pink line). If we trained only on up to two operations (blue line), however, the performance degradation was compounded and performance no longer exceeded the random baseline. These results suggest that finetuning on an entity tracking task does lead to entity tracking abilities that generalize ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We set out to investigate whether pretrained LLMs exhibit entity tracking behavior. We developed a task that allowed us to evaluate whether LLMs can predict the state of an entity based on an initial state description and operations that act upon it. In the first set of experiments, we found that GPT-3 davinci, a vanilla pretrained language model, and Flan-T5, an instruction-finetuned language model, fail at this task and simply repeat the initial state description. The GPT-3.5 models (the core difference with the aforementioned models being code in pretraining corpora), on the other hand, exhibited non-trivial entity tracking behavior. Even after many operations affecting an entity state, they consistently performed above a strong random baseline.\nIn the second set of experiments, we showed that this behavior can also to a large extent be learned by smaller models such as T5. When we finetune the model on this task, it is also able to perform entity tracking on examples that differ along several dimensions from the training data. Taken together, our results provide evidence that (a) vanilla language models that have mainly been trained on text data do not exhibit entity tracking abilities out of the box, (b) pretraining on code and text data considerably improves this ability, 12 and (c) finetuning on this task can make this behavior also surface in smaller models that have primarily been trained on text data, although it remains an open question how general this ability is in smaller finetuned models. What are reasons behind the efficacy of training both on text and code? For producing correct code, tracking the states of variables is important. Therefore, to speculate, this kind of pretraining data may provide a stronger signal for the model to track entities compared to pure text data. It could also be that, as previously speculated (Potts, 2020;Merrill et al., 2021, i.a.), pretraining on code provides additional grounding.\nThe present results also highlight the importance of the composition of the pretraining data. Our results showed that models that are trained on the language modeling objective on large corpora show dramatically different behavior on entity tracking depending on whether the training data includes substantial amount of code or not. In this light, future work could investigate the effect of pretraining on code more widely, including its effect on the model representations and to what extent it facilitates the emergence of world models in LMs.\nFinally, we also laid out several principles that should be followed when evaluating state tracking abilities. Apart from these specific principles, we make a more general point that in assessing abilities related to meaning, one needs to consider potential strategies that the model could use to solve the task and make sure that the test examples do not mimic the distributional patterns of the training or finetuning data. Only then can we properly assess meaning-related capacities of LMs.", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "One limitation of this work is that we are only considering behavioral data which makes it difficult to establish a fully causal link between entity tracking capacities and high performance on our task. Entity tracking is a high-level linguistic behavior and many other capacities are necessary for achieving high accuracy on our task. Therefore, we cannot rule out that differences in some other capacity, such as interpreting sentences compositionally (see Bogin 2022 and for evidence that GPT-3 and GPT-3.5 models differ in their compositional generalization behavior), are the main driver for the differences in behavior we see across models.\nA possible criticism of our setup is that it requires short-term memory capacities that exceed the memory capacities of most, if not all, humans. That is, if we presented humans with the same input as the model, we would not expect them to be able to keep track of the contents of all 7 boxes due to memory limitations. Therefore we are potentially expecting models to do super-human entity tracking, a setup that has been criticized for model evaluations of other linguistic abilities (Lampinen, 2022). We nevertheless believe that our task is justified given the architecture of the evaluated models. Transformer-based models can look back to any token in the entire input sequence within their context window, so a proper comparison between humans and models would be to present humans with the full description in written form and let them re-read the description after being prompted to state the contents of a box. While we did not formally evaluate whether humans have this ability on a larger population, we personally did not have any trouble tracking the contents of boxes when we had access to the written description.\nRelatedly, we designed our task such that the entire description fits within the context window of pretrained language models. However, as we mentioned in the introduction, entity tracking is an important ability for understanding long contexts and given the limited context window, our results do not apply to texts whose length exceeds a model's context window, and likely different model architectures will be necessary to perform proper entity tracking for longer texts.\nFurther, while we found that GPT-3.5 models as well as finetuned T5 models can track entities in our task with higher accuracy than a strong random baseline, our results also indicate that this behavior is not very stable once several operations act on an entity. Our results should therefore not be taken as justification for using these models for critical applications where high accuracy is needed.\nLastly, we only evaluated English models in this work. Given that we showed that even without high lexical overlap between the training and evaluation examples, models can keep track of entities to some extent, it seems likely that our results also apply to other languages. However, whether this actually the case remains an open question.\nA Additional Analyses of Results from Li et al. (2021) As mentioned in Footnote 3, Li et al. (2021) conducted two more experiments that prima facie provided additional evidence for implicit meaning representations and state tracking abilities in language models. However, the data and setup of these two experiments also likely overestimates models' abilities.\nIn the second set of probing classifier experiments, Li et al. (2021) used data generated using the TextWorld engine (C\u00f4t\u00e9 et al., 2019). In this setup, there is a textual description of several entities in a text-based game (e.g., a wooden door) as well as actions that a player took (e.g., opening the wooden door). Their probing classifier takes the representations of either one entity and a property of that entity (e.g., that the wooden door is closed) or the representations of two entities and a relation between them (e.g., that the king-sized bed is in the bedroom) and from these representations, the classifier has to predict whether a given proposition is true or false considering the initial description and the series of actions that a player took. Only propositions that involve entities that have been mentioned are probed. The issue with this setup is that there are many propositions that are always true in both the training and evaluation splits (e.g., in all the game simulations, the chest drawer is in the bedroom, so the probing classifier should always return true for this input independent of the previous context). Furthermore, even for the entity-property propositions and entity-relationentity propositions which are not always true in the training and evaluation data, the data is very biased and a baseline that predicts the most common answer in the training data without taking the initial descriptions or the user actions into account (a violation of Desideratum 3, see Section 3.1), already achieves an accuracy of 88.5%, a number that puts the reported probing classifier accuracy of 96.9% into a bit more context.\nFurther, Li et al. (2021), also presented an experiment where they manipulated specific entity representations of a synthetic version of the Alchemy dataset. In this experiment, they first encoded an initial description D and an operation O which affected a beaker X using a language model that had been finetuned to predict the next operation, resulting in representation R 1 . Then, they encoded the same initial representation D and an operation of the form Drain n from beaker Y , where beaker Y was always different from beaker X and n was the amount of liquid that was in beaker Y according to the initial description. This resulted in representation R 2 . Then, they extracted the representation of the initial description of beaker Y from representation R 2 and replaced the representation of beaker Y in R 1 with the corresponding representation in R 2 to obtain R mixed . They then used R mixed as an input to T5 and showed that the predicted next operation based on R mixed was considerably more often compatible with both operations (the operation encoded in R 1 and the operation encoded in R 2 ) compared to predicting the next operation from R 1 or R 2 , which they took as evidence that the token representations of the initial description encoded the entities state after performing the operation. The issue with this experiment is that the operation from which R 2 was computed was always of the form Drain n from Y th beaker, so the final state of beaker Y was always empty. Therefore, this experiment primarily shows that the token drain affected the representation of the initial state (and subsequently the prediction of the next operation) but not more generally, that the actual state of the manipulated beaker Y is fully encoded in its initial state description. To answer that question, one would have to repeat this experiment with more complex operations that do not give away the final state (a violation of Desideratum 2, see Section 3.1).\nIn summary, the additional experiments in Li et al. (2021) also violate some of the desiderata we laid out in Section 3.1, and therefore it is difficult to draw conclusions about state tracking abilities from these experiments.", "publication_ref": ["b3", "b25", "b30", "b30", "b30", "b11", "b30", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "B Example Input-Output Pairs", "text": "(3) shows an example input-output pair from our base dataset (NumOps on Box 6 = 2). (4) shows an example input-output pair from the dataset with operations that contain referring expressions that are ambiguous without considering the initial state and the previous operations (Nu-mOps on Box 6 = 2). Note that in order to correctly interpret Move the guitar from Box 2 to Box 6, the model has to consider the information that the blue guitar (as opposed to the red guitar) was in Box 2 prior to the operation. Hence, this operation is ambiguous out of context. (5) shows an example input-output pair from the dataset with the Move contents of operation (Nu-mOps on Box 6 = 2). Note that in order to correctly interpret Move the contents of Box 2 to Box 6, the model has to consider the information that the tea (as opposed to the red guitar) was in Box 2 prior to the operation. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Implementation Details", "text": "In-context For GPT-3, we used the OpenAI API. We used greedy decoding with the temperature parameter set to 0, used a maximum target generation length of 150, and used the newline character as an additional stop token. Inference time and model size is not available for GPT-3, but we generated about 16 million tokens in total. All GPT-3 experiments that we report here in the paper were conducted in January 2023. For Flan-T5, we used a beam size of 3 and a maximum target generation length of 256. Other hyperparameters were kept as the default values of T5ConditionalGeneration in the HuggingFace library. Inference for T5-XL took about 5 hours on a single A100 GPU, and for T5 base, about 2 hours. Finetuning We finetuned T5 for a single epoch using a batch size of 8 and a learning rate of 1 \u00d7 10 \u22124 . In initial explorations, increasing the number of finetuning epochs did not yield substantial gains on development set performance, and was sometimes even harmful. Training and inference took around 3 hours on a single RTX8000 GPU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Additional Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 GPT-3.5 Zero-shot Results", "text": "As mentioned in the main text, GPT-3.5 was able to to output the contents of boxes in the correct format without any in-context demonstrations (see Table 5 for the prompt template).\nFigure 9 compares the performance of text-davinci-003 in the 2-shot setting to the zero-shot setting. Model performance degraded slightly without demonstration examples, which suggests that the examples are indeed helpful in guiding the model to correctly perform the task. Nevertheless, we observed non-negligible performance even in the zero-shot setting, which corroborates the conclusion that GPT 3.5 exhibits non-trivial entity tracking capacities.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": ["tab_13"]}, {"heading": "D.2 GPT-4 Results", "text": "We also probed the more recently released GPT-4 model (OpenAI, 2023) through the OpenAI API. Figure 8 compares the performance of GPT-3.5 text-davinci-003 and GPT-4 on the base dataset (left) as well as the more challenging MoveContents (middle) and AmbiRef (right) datasets discussed in Section 4.2. Across all datasets, GPT-4   performed considerably better than GPT-3.5. But similarly to GPT-3.5, performance degraded as more operations affected a box. Considering that no public information about the training data, training procedure or architectural details about GPT-4 exists, we cannot draw any conclusions about the reason behind the improved performance of GPT-4 compared to previous models.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "E Prompts", "text": "Table 3 shows the 2-shot prompts used for incontext experiments. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "F Dataset Statistics and License Information", "text": "See Tables 6 and 7 for descriptive statistics of our datasets. All datasets are released under the GNU General Public License v3.0.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "2-shot prompt with all boxes queried at once (GPT-3 experiments)", "text": "Given the description after \"Description:\", write a true statement about all boxes and their contents to the description after \"Statement:\".  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2-shot prompt with disjoint surface forms from test examples", "text": "Given the description after \"Description:\", write a true statement about all containers or boxes and their contents to the description after \"Statement:\".\nDescription: The biscotti is in Container A, the icicle is in Container B, the granite and the machine are in Container C, the folio and the encyclopedia are in Container D, the bill is in Container E, the spork and the jackknife and the frappuccino are in Container F,  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Zero-shot prompt", "text": "Given the description after \"Description:\", write a true statement about all boxes and their contents according to the description after \"Statement:\".    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Jacob Andreas, Ellie Pavlick, Allyson Ettinger, Tal Linzen, Will Merrill, and the members of the NYU Computation and Psycholinguistics lab for discussions, and Belinda Li for sharing model outputs and details about their data preparation", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "A2. Did you discuss any potential risks of your work? Not applicable. We evaluate existing models, so we don't see any potential risks other than misinterpreting our results. We hope we managed to limit the potential for misunderstanding by discussing in detail how to interpret the results.\nA3. Do the abstract and introduction summarize the paper's main claims? Abstract, Section 1 (\"Contributions\" paragraph on p2) A4. Have you used AI writing assistants when working on this paper?\nLeft blank.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Did you use or create scientific artifacts?", "text": "We developed a novel procedure for programmatically generating datasets and we will release that code and the generated datasets upon publication (described in Section 3). We also discuss an analysis of the probing datasets from Li et al. (2021) in Section 2 and Appendix A.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "B1. Did you cite the creators of artifacts you used?", "text": "We cited the pretrained models that we used (Sections 4 and 5). We cite Li et al. and relevant source datasets in Section 2 and Appendix A.\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSee Appendix E. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Our data mostly concerns moving objects from one box to another, so do not envision a risk scenario of use outside of research contexts.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. We generated the data from a set lexicon and templates and therefore, we can rule out that any identifying information is contained in the data.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? See Section 3. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. See Appendix E. D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "What do entity-centric models learn? insights from entity linking in multi-party dialogue", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Laura Aina; Carina Silberer; Ionut-Teodor Sorodoc; Matthijs Westera; Gemma Boleda"}, {"ref_id": "b1", "title": "An annotated dataset of coreference in English literature", "journal": "", "year": "2020", "authors": "David Bamman; Olivia Lewke; Anya Mansoor"}, {"ref_id": "b2", "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data", "journal": "", "year": "2020", "authors": "Emily M Bender; Alexander Koller"}, {"ref_id": "b3", "title": "How does GPT-3 perform on COVR-10 splits with in-context learning?", "journal": "", "year": "2022", "authors": "Ben Bogin"}, {"ref_id": "b4", "title": "Unobserved local structures make compositional generalization hard", "journal": "", "year": "2022", "authors": "Ben Bogin; Shivanshu Gupta; Jonathan Berant"}, {"ref_id": "b5", "title": "Simulating action dynamics with neural process networks", "journal": "", "year": "2018", "authors": "Antoine Bosselut; Omer Levy; Ari Holtzman; Corin Ennis; Dieter Fox; Yejin Choi"}, {"ref_id": "b6", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b7", "title": "PreCo: A large-scale dataset in preschool vocabulary for coreference resolution", "journal": "", "year": "2018", "authors": "Hong Chen; Zhenhua Fan; Hao Lu; Alan Yuille; Shu Rong"}, {"ref_id": "b8", "title": "", "journal": "Aakanksha Chowdhery", "year": "", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Yunxuan Fedus; Xuezhi Li; Mostafa Wang; Siddhartha Dehghani; Albert Brahma;  Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun;  Chen"}, {"ref_id": "b9", "title": "Scaling instructionfinetuned language models", "journal": "", "year": "2022", "authors": "Le ; Jason Wei"}, {"ref_id": "b10", "title": "What does BERT look at? an analysis of BERT's attention", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kevin Clark; Urvashi Khandelwal; Omer Levy; Christopher D Manning"}, {"ref_id": "b11", "title": "Textworld: A learning environment for text-based games", "journal": "Springer International Publishing", "year": "2019", "authors": "\u00c1kos Marc-Alexandre C\u00f4t\u00e9; Xingdi K\u00e1d\u00e1r; Ben Yuan; Tavian Kybartas; Emery Barnes; James Fine; Matthew Moore; Layla El Hausknecht; Mahmoud Asri; Wendy Adada; Adam Tay;  Trischler"}, {"ref_id": "b12", "title": "Everything happens for a reason: Discovering the purpose of actions in procedural text", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Bhavana Dalvi; Niket Tandon; Antoine Bosselut; Wentau Yih; Peter Clark"}, {"ref_id": "b13", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b14", "title": "Dynamic predicate logic", "journal": "Linguistics and Philosophy", "year": "1991", "authors": "Jeroen Groenendijk; Martin Stokhof"}, {"ref_id": "b15", "title": "Effective use of transformer networks for entity tracking", "journal": "", "year": "2019", "authors": "Aditya Gupta; Greg Durrett"}, {"ref_id": "b16", "title": "Tracking discrete and continuous entity state for process understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Aditya Gupta; Greg Durrett"}, {"ref_id": "b17", "title": "Coreference resolution in a modular, entity-centered model", "journal": "", "year": "2010", "authors": "Aria Haghighi; Dan Klein"}, {"ref_id": "b18", "title": "File change semantics and the familiarity theory of definiteness", "journal": "Blackwell Publishing Oxford", "year": "2002", "authors": "Irene Heim"}, {"ref_id": "b19", "title": "Tracking the world state with recurrent entity networks", "journal": "", "year": "2017", "authors": "Mikael Henaff; Jason Weston; Arthur Szlam; Antoine Bordes; Yann Lecun"}, {"ref_id": "b20", "title": "The Goldilocks principle: Reading children's books with explicit memory representations", "journal": "", "year": "2016", "authors": "Felix Hill; Antoine Bordes; Sumit Chopra; Jason Weston"}, {"ref_id": "b21", "title": "Dynamic entity representations in neural language models", "journal": "", "year": "2017", "authors": "Yangfeng Ji; Chenhao Tan; Sebastian Martschat; Yejin Choi; Noah A Smith"}, {"ref_id": "b22", "title": "Discourse Representation Theory", "journal": "", "year": "2011", "authors": "Hans Kamp; Josef Van Genabith; Uwe Reyle"}, {"ref_id": "b23", "title": "Discourse referents", "journal": "Syntax and Semantics", "year": "1976", "authors": "Lauri Karttunen"}, {"ref_id": "b24", "title": "Mise en place: Unsupervised interpretation of instructional recipes", "journal": "", "year": "2015", "authors": "Chlo\u00e9 Kiddon; Thandavam Ganesa; Luke Ponnuraj; Yejin Zettlemoyer;  Choi"}, {"ref_id": "b25", "title": "Can language models handle recursively nested grammatical structures? a case study on comparing models and humans", "journal": "", "year": "2022", "authors": "Andrew Kyle Lampinen"}, {"ref_id": "b26", "title": "Stanford's multi-pass sieve coreference resolution system at the CoNLL-2011 shared task", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Heeyoung Lee; Yves Peirsman; Angel Chang; Nathanael Chambers; Mihai Surdeanu; Dan Jurafsky"}, {"ref_id": "b27", "title": "End-to-end neural coreference resolution", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Kenton Lee; Luheng He; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b28", "title": "Word frequencies in written and spoken English: based on the British National Corpus", "journal": "", "year": "2001", "authors": "Geoffrey Leech;  Paul; Andrew Rayson;  Wilson"}, {"ref_id": "b29", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b30", "title": "Implicit representations of meaning in neural language models", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Belinda Z Li; Maxwell Nye; Jacob Andreas"}, {"ref_id": "b31", "title": "Emergent world representations: Exploring a sequence model trained on a synthetic task", "journal": "", "year": "2023", "authors": "Kenneth Li; K Aspen; David Hopkins; Fernanda Bau; Hanspeter Vi\u00e9gas; Martin Pfister;  Wattenberg"}, {"ref_id": "b32", "title": "How can we accelerate progress towards human-like linguistic generalization?", "journal": "", "year": "2020", "authors": " Tal Linzen"}, {"ref_id": "b33", "title": "New or old? exploring how pre-trained language models represent discourse entities", "journal": "", "year": "2022", "authors": "Sharid Lo\u00e1iciga; Anne Beyer; David Schlangen"}, {"ref_id": "b34", "title": "Simpler context-dependent logical forms via model projections", "journal": "Long Papers", "year": "2016", "authors": "Reginald Long; Panupong Pasupat; Percy Liang"}, {"ref_id": "b35", "title": "Language models of code are few-shot commonsense learners", "journal": "", "year": "2022", "authors": "Aman Madaan; Shuyan Zhou; Uri Alon; Yiming Yang; Graham Neubig"}, {"ref_id": "b36", "title": "Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "William Merrill; Yoav Goldberg; Roy Schwartz; Noah A Smith"}, {"ref_id": "b37", "title": "When peanuts fall in love: N400 evidence for the power of discourse", "journal": "Journal of Cognitive Neuroscience", "year": "2006", "authors": "S Mante;  Nieuwland; J A Jos;  Van Berkum"}, {"ref_id": "b38", "title": "OpenAI. 2023. GPT-4 technical report", "journal": "", "year": "", "authors": ""}, {"ref_id": "b39", "title": "", "journal": "", "year": "", "authors": "Long Ouyang; Jeff Wu; Xu Jiang; Diogo Almeida; L Carroll; Pamela Wainwright; Chong Mishkin; Sandhini Zhang; Katarina Agarwal;  Slama"}, {"ref_id": "b40", "title": "Training language models to follow instructions with human feedback", "journal": "", "year": "", "authors": ""}, {"ref_id": "b41", "title": "Is it possible for language models to achieve language understanding?", "journal": "", "year": "2020", "authors": "Christopher Potts"}, {"ref_id": "b42", "title": "CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "journal": "", "year": "2012", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Olga Xue; Yuchen Uryupina;  Zhang"}, {"ref_id": "b43", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b44", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b45", "title": "Resolving complex cases of definite pronouns: The Winograd schema challenge", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Altaf Rahman; Vincent Ng"}, {"ref_id": "b46", "title": "Neural theory-of-mind? on the limits of social intelligence in large LMs", "journal": "", "year": "2022", "authors": "Maarten Sap; Le Ronan; Daniel Bras; Yejin Fried;  Choi"}, {"ref_id": "b47", "title": "When a sentence does not introduce a discourse entity, transformer-based models still sometimes refer to it", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Sebastian Schuster; Tal Linzen"}, {"ref_id": "b48", "title": "Probing for referential information in language models", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ionut-Teodor Sorodoc; Kristina Gulordava; Gemma Boleda"}, {"ref_id": "b49", "title": "BERT rediscovers the classical NLP pipeline", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ian Tenney; Dipanjan Das; Ellie Pavlick"}, {"ref_id": "b50", "title": "Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Shubham Toshniwal; Sam Wiseman; Allyson Ettinger; Karen Livescu; Kevin Gimpel"}, {"ref_id": "b51", "title": "Chess as a testbed for language model state tracking", "journal": "", "year": "2022", "authors": "Shubham Toshniwal; Sam Wiseman; Karen Livescu; Kevin Gimpel"}, {"ref_id": "b52", "title": "Can large language models play text games well? current stateof-the-art and open questions", "journal": "", "year": "2023", "authors": "Xiaochen Chen Feng Tsai;  Zhou; S Sierra; Jing Liu; Mo Li; Hongyuan Yu;  Mei"}, {"ref_id": "b53", "title": "Annotating a broad range of anaphoric phenomena, in a variety of genres: the arrau corpus", "journal": "Natural Language Engineering", "year": "2020", "authors": "Olga Uryupina; Ron Artstein; Antonella Bristot; Federica Cavicchio; Francesca Delogu; J Kepa; Massimo Rodriguez;  Poesio"}, {"ref_id": "b54", "title": "ACE 2005 multilingual training corpus. Linguistic Data Consortium", "journal": "", "year": "2006", "authors": "Christopher Walker; Stephanie Strassel; Julie Medero; Kazuaki Maeda"}, {"ref_id": "b55", "title": "Towards AI-complete question answering: a set of prerequisite toy tasks", "journal": "", "year": "2015", "authors": "Jason Weston; Antoine Bordes; Sumit Chopra; Alexander M Rush; Bart Van Merri\u00ebnboer; Armand Joulin; Tomas Mikolov"}, {"ref_id": "b56", "title": "Procedures as a representation for data in a computer program for understanding natural language", "journal": "", "year": "1971", "authors": "Terry Winograd"}, {"ref_id": "b57", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"ref_id": "b58", "title": "C1. Did you report the number of parameters in the models used, the total computational budget", "journal": "", "year": "", "authors": ""}, {"ref_id": "b59", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values", "journal": "", "year": "", "authors": ""}, {"ref_id": "b60", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b61", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A sketch of our entity tracking task.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Entity tracking accuracy of text-davinci-003 with low lexical overlap between demonstration and test examples (AltForms).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4:Entity tracking accuracy of text-davinci-003 for the AmbiRef (left) and MoveContents (right) datasets.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Accuracy on state prediction for different GPT-3 models. Solid lines denote models trained on code and text, and dotted lines denote models mainly trained on text.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Box 0 contains the painting, Box 1 contains the bell, Box 2 contains the guitar, Box 3 contains the egg and the mirror and the sheet, Box 4 contains the chemical, Box 5 contains the disk and the wire, Box 6 contains the glass and the knife. Move the glass from Box 6 to Box 4. Put the gift into Box 5. Move the guitar from Box 2 to Box 6. Put the milk into Box 4. Remove the mirror and the sheet from Box 3. Box 6 __ b. OUTPUT: contains the guitar and the knife.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Box 0 contains the yellow book and the green flower and the red guitar, Box 1 contains the small bomb and the small book and the blue bone, Box 2 contains the blue guitar, Box 3 contains the blue bell, Box 4 contains the green paper and the yellow note and the yellow television, Box 5 contains the yellow bell, Box 6 is empty. Move the guitar from Box 2 to Box 6. Put the blue wire and the big television into Box 5. Move the flower from Box 0 to Box 6. Box 6 __ b. OUTPUT: contains the blue guitar and the green flower.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Box 0 contains the fan and the gift and the letter, Box 1 contains the beer and the mirror and the tie, Box 2 contains the tea, Box 3 contains the boot, Box 4 contains the coat and the plate and the shirt, Box 5 contains the bottle, Box 6 is empty. Move the contents of Box 2 to Box 6. Put the dress and the painting into Box 5. Move the letter from Box 0 to Box 6. Box 6 __ b. OUTPUT: contains the letter and the tea.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Entity tracking accuracy of text-davinci-003 and GPT-4 in 2-shot in-context learning experiments.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 :9Figure 9: Entity tracking accuracy of GPT 3.5 under in-context (2-shot) and zero-shot settings.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Description:Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the machine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the apple and the cash and the glass, Box 6 contains the bottle and the map. Statement: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the machine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the apple and the cash and the glass, Box 6 contains the bottle and the map. Description: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the machine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the apple and the cash and the glass, Box 6 contains the bottle and the map. Remove the car from Box 0. Remove the paper and the string from Box 3. Put the plane into Box 0. Move the map from Box 6 to Box 2. Remove the bill from Box 4. Put the coat into Box 3. Statement: Box 0 contains the plane, Box 1 contains the cross, Box 2 contains the bag and the machine and the map, Box 3 contains the coat, Box 4 contains nothing, Box 5 contains the apple and the cash and the glass, Box 6 contains the bottle. Description: {description} Statement: Box 0 contains 2-shot prompt with boxes queried individually (T5 experiments) Given the description after \"Description:\", write a true statement about a box and the contents of this box according to the description after \"Statement:\". Description: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the machine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the apple and the cash and the glass, Box 6 contains the bottle and the map. Statement: Box 1 contains the cross. Description: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the machine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the apple and the cash and the glass, Box 6 contains the bottle and the map. Remove the car from Box 0. Remove the paper and the string from Box 3. Put the plane into Box 0. Move the map from Box 6 to Box 2. Remove the bill from Box 4. Put the coat into Box 3. Statement: Box 2 contains the bag and the machine and the map. Description: {description} Statement: Box {boxnum} contains", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "the clipper and the ladybug are in Container G. Statement: Container A contains the biscotti, Container B contains the icicle, Container C contains the granite and the machine, Container D contains the folio and the encyclopedia, Container E contains the bill, Container F contains the spork and the jackknife and the frappuccino, Container G contains the clipper and the ladybug. Description: The biscotti is in Container A, the icicle is in Container B, the granite and the machine are in Container C, the folio and the encyclopedia are in Container D, the bill is in Container E, the spork and the jackknife and the frappuccino are in Container F, the clipper and the ladybug are in Container G. Take the biscotti out of Container A. Take the folio and the encyclopedia out of container D. Place the tetrapod inside Container A. Pick up the ladybug in Container G and place it into Container C. Take the bill out of Container E. Place the gumball inside Container D. Statement: Container A contains the tetrapod, Container B contains the icicle, Container C contains the granite and the machine and the ladybug, Container D contains the gumball, Container E contains nothing, Container F contains the spork and the jackknife and the frappuccino, Container G contains the clipper.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Format the statement as Box 0 contains the A, Box 1 contains the B, Box 2 contains the C, Box 3 contains the D, Box 4 contains the E, Box 5 contains the F, Box 6 contains the G. A, B, C, D, E, F, G are placeholders for the contents of each box.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "3. If any data is used for demonstration, finetuning or training, the training and evaluation data should have little lexical overlap.", "figure_data": "4. If any data is used for demonstration, finetun-ing or training, the task should not be solvableby slot-filling based on observed datapoints."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Summary of the models used for the in-context demonstration experiments.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Accuracy on state prediction after n operations that affect a specific box. Left: predictions for boxes whose content differs from the initial state, Right: predictions for boxes whose content is the same as in the initial state. Error bars show 95% CIs.", "figure_data": "state != initial statestate = initial state1.000.75Accuracy0.500.250.000123456701234567Number of operations affecting box stateModelFlan2T5 XL Flan2T5 BaseGPT23.5 text2davinci2003 GPT23 davinciRandom baselineFigure 2:"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "MoveMove the car from Box 1 to Box 3. Pick up the furby in Container A and place it into Container C.RemoveRemove the car from Box 1. Take the furby out of Container A.PutPut the car into Box 1. Place the furby inside Container A.", "figure_data": "OperationBaseAltForms"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Different phrasings of the state-changing operations under the AltForms evaluation setup.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "). The left panel shows the instances where the probed state differed from the initial state; the right panel shows the instances where the probed state was the same as the initial state. As the left panel shows, only GPT-3.5 text-davinci-003 consistently outperformed the (strong) random baseline. While, not surprisingly, the accuracy of this model also decreases as the number of operations increases, it still correctly predicted all contents of a box after 7 operations in more than 25% of the cases. The Flan-T5 models, on the other hand, seemed to ignore the operations and primarily predicted the initial state description, as indicated by the consistently high accuracy when the final state matches the initial state (right panel), as well as the consistently low accuracy when the final state deviates from the initial state (left panel). GPT-3 davinci also primarily repeated the initial state, but as indicated by the steep decrease in the right panel, it was distracted by intervening operations even when repeating the initial state. Form-meaning Disentanglement We additionally evaluated text-davinci-003, the only model that exhibited a non-trivial entity tracking capacity in the first set of results, under the AltForms setup as described in Section 4.2 where the demonstration examples have low lexical overlap with the test examples.Figure", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": "shows the 2-shotprompts used for the AltForms experiments, wherethe demonstrations contain descriptions that havelower lexical overlap with the test examples. Ta-ble 5 shows the zero-shot prompts discussed in Ap-pendix D. All prompts are available at https://github.com/sebschu/entity-tracking-lms."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Prompts with 2-shot in-context demonstrations.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Prompt with 2-shot in-context demonstrations that have disjoint surface forms.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Prompt template for zero-shot experiments.", "figure_data": "DatasetScenarios Demonstration Test Demonstration ExamplesTestComplete19901490,090Subsample1491145,012AmbiRef.1474144,991MoveContents1451144,956"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Dataset statistics for Experiment 1.", "figure_data": "DatasetTrainScenarios DevTest TrainExamples DevTestBase990220990 90,09020,02090,090NumOps990220990 20,79020,02090,090Vocab990220990 90,09020,02090,090AltForms990220990 90,09020,02090,090AltForms+NumOps990220990 20,79020,02090,090AmbiRef990220990 90,09020,02090,090MoveContents990220990 90,09020,02090,090"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Dataset statistics for Experiment 2.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "9", "formula_coordinates": [5.0, 83.51, 691.96, 2.99, 7.2]}], "doi": "10.18653/v1/N19-1378"}