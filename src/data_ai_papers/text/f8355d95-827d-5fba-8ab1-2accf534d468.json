{"title": "Video-aided Unsupervised Grammar Induction", "authors": "Songyang Zhang; Linfeng Song; Lifeng Jin; Kun Xu; Dong Yu; Jiebo Luo", "pub_date": "", "abstract": "We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on learning syntactic grammars from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model (Kim et al., 2019) as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.", "sections": [{"heading": "Introduction", "text": "Constituency parsing is an important task in natural language processing, which aims to capture syntactic information in sentences in the form of constituency parsing trees. Many conventional approaches learn constituency parser from humanannotated datasets such as Penn Treebank (Marcus et al., 1993). However, annotating syntactic trees by human language experts is expensive and timeconsuming, while the supervised approaches are limited to several major languages. In addition, the treebanks for training these supervised parsers are small in size and restricted to the newswire domain, thus their performances tend to be worse when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b;Drozdov et al., 2019;Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be trained on large-scale unlabeled data. In particular, there has been growing interests in exploiting visual information for unsupervised grammar induction because visual information can capture important knowledge required for language learning that is ignored by text (Gleitman, 1990;Pinker and MacWhinney, 1987;Tomasello, 2003). This task aims to learn a constituency parser from raw unlabeled text aided by its visual context.\nPrevious methods (Shi et al., 2019;Kojima et al., 2020;Zhao and Titov, 2020;Jin and Schuler, 2020) learn to parse sentences by exploiting object information from images. However, images are static and cannot present the dynamic interactions among visual objects, which usually correspond to verb phrases that carry important information. There-fore, images and their descriptions may not be fullyrepresentative of all linguistic phenomena encountered in learning, especially when action verbs are involved. For example, as shown in Figure 1(a), when parsing a sentence \"A squirrel jumps on stump\", a single image cannot present the verb phrase \"jumps on stump\" accurately. Moreover, as shown in Figure 1(b), the guitar sound and the moving fingers clearly indicate the speed of music playing, while it is impossible to present only with a static image as well. Therefore, it is difficult for previous methods to learn these constituents, as static images they consider lack dynamic visual and audio information.\nIn this paper, we address this problem by leveraging video content to improve an unsupervised grammar induction model. In particular, we exploit the current state-of-the-art techniques in both video and audio understanding, domains of which include object, motion, scene, face, optical character, sound, and speech recognition. We extract features from their corresponding state-of-the-art models and analyze their usefulness with the VC-PCFG model (Zhao and Titov, 2020). Since different modalities may correlate with each other, independently modeling each of them may be sub-optimal. We also propose a novel model, Multi-Modal Compound Probabilistic Context-Free Grammars (MMC-PCFG), to better model the correlation among these modalities.\nExperiments on three benchmarks show substantial improvements when using each modality of the video content. Moreover, our MMC-PCFG model that integrates information from different modalities further improves the overall performance. Our code is available at https://github.com/ Sy-Zhang/MMC-PCFG.\nThe main contributions of this paper are:\n\u2022 We are the first to address video aided unsupervised grammar induction and demonstrate that verb related features extracted from videos are beneficial to parsing.\n\u2022 We perform a thorough analysis on different modalities of video content and propose a model to effectively integrate these important modalities to train better constituency parsers.\n\u2022 Experiments results demonstrate the effectiveness of our model over the previous state-ofthe-art methods.", "publication_ref": ["b33", "b7", "b40", "b6", "b22", "b10", "b38", "b42", "b41", "b26", "b52", "b19", "b52"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Background and Motivation", "text": "Our model is motivated by C-PCFG (Kim et al., 2019) and its variant of the image-aided unsupervised grammar induction model, VC-PCFG (Zhao and Titov, 2020). We will first review the evolution of these two frameworks in Sections 2.1-2.2, and then discuss their limitations in Section 2.3.", "publication_ref": ["b22", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Compound PCFGs", "text": "A probabilistic context-free grammar (PCFG) in Chomsky normal form can be defined as a 6-tuple (S, N , P, \u03a3, R, \u03a0), where S is the start symbol, N , P and \u03a3 are the set of nonterminals, preterminals and terminals, respectively. R is a set of production rules with their probabilities stored in \u03a0, where the rules include binary nonterminal expansions and unary terminal expansions. Given a certain number of nonterminal and preterminal categories, a PCFG induction model tries to estimate rule probabilities. By imposing a sentence-specific prior on the distribution of possible PCFGs, the compound PCFG model (Kim et al., 2019) uses a mixture of PCFGs to model individual sentences in contrast to previous models  where a corpus-level prior is used. Specifically in the generative story, the rule probability \u03c0 r is estimated by the model g with a latent representation z for each sentence \u03c3, which is in turn drawn from a prior p(z):\n\u03c0 r = g r (z; \u03b8), z \u223c p(z). (1\n)\nThe probabilities for the CFG initial expansion rules S \u2192 A, nonterminal expansion rules A \u2192 B C and preterminal expansion rules T \u2192 w can be estimated by calculating scores of each combination of a parent category in the left hand side of a rule and all possible child categories in the right hand side of a rule:\n\u03c0 S\u2192A = exp(u A f s ([w S ; z])) A \u2208N exp(u A f s ([w S ; z])) , \u03c0 A\u2192BC = exp(u BC [w A ; z]) B ,C \u2208N \u222aP exp(u B C [w A ; z])) , \u03c0 T \u2192w = exp(u w f t ([w T ; z])) w \u2208\u03a3 exp(u T w f t ([w T ; z])) ,(2)\nwhere A, B, C \u2208 N , T \u2208 P, w \u2208 \u03a3, w and u vectorial representations of words and categories, and f t and f s are encoding functions such as neural networks.\nOptimization of the PCFG induction model usually involves maximizing the marginal likelihood of a training sentence p(\u03c3) for all sentences in a corpus. In the case of compound PCFGs:\nlog p \u03b8 (\u03c3) = log z t\u2208T G (\u03c3) p \u03b8 (t|z)p(z)dz, (3)\nwhere t is a possible binary branching parse tree of \u03c3 among all possible trees T under a grammar G. Since computing the integral over z is intractable, log p \u03b8 (\u03c3) can be optimized by maximizing its evidence lower bound ELBO(\u03c3; \u03c6, \u03b8):\nELBO(\u03c3; \u03c6, \u03b8) = E q \u03c6 (z|\u03c3) [log p \u03b8 (\u03c3|z)] \u2212 KL[q \u03c6 (z|\u03c3)||p(z)],(4)\nwhere q \u03c6 (z|\u03c3) is a variational posterior, a neural network parameterized with \u03c6. The sample log likelihood can be computed with the inside algorithm, while the KL term can be computed analytically when both prior p(z) and the posterior approximation q \u03c6 (z|\u03c3) are Gaussian (Kingma and Welling, 2014).", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Visualy Grounded Compound PCFGs", "text": "The visually grounded compound PCFGs (VC-PCFG) extends the compound PCFG model (C-PCFG) by including a matching model between images and text. The goal of the vision model is to match the representation of an image v to the representation of a span c in a parse tree t of a sentence \u03c3. The word representation h i for the ith word is calculated by a BiLSTM network. Given a particular span c = w i , . . . , w j (0 < i < j \u2264 n)], we then compute its representation c. We first compute the probabilities of its phrasal labels {p(k|c, \u03c3)|1 \u2264 k \u2264 K, K = |N |}, as described in Section 2.1. The representation c is the sum of all label-specific span representations weighted by the probabilities we predicted:\nc = K k=1 p(k|c, \u03c3)f k ( 1 j \u2212 i + 1 j l=i h l ),(5)\nFinally, the matching loss between a sentence \u03c3 and an image representation v can be calculated as a sum over all matching losses between a span and the image representation, weighted by the marginal of a span from the parser:\ns img (v, \u03c3) = c\u2208\u03c3 p(c|\u03c3)h img (c, v),(6)\nwhere h img (c, v) is a hinge loss between the distances from the image representation v to the matching and unmatching (i.e. sampled from a different sentence) spans c and c , and the distances from the span c to the matching and unmatching (i.e. sampled from a different image) image representations v and v :\nh img (c,v) = E c [cos(c , v) \u2212 cos(c, v)) + ] + + E v [cos(c, v ) \u2212 cos(c, v) + ] + , (7)\nwhere is a positive margin, and the expectations are approximated with one sample drawn from the training data. During training, ELBO and the image-text matching loss are jointly optimized.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitation", "text": "VC-PCFG improves C-PCFG by leveraging the visual information from paired images. In their experiments (Zhao and Titov, 2020), comparing to C-PCFG, the largest improvement comes from NPs (+11.9% recall), while recall values of other frequent phrase types (VP, PP, SBAR, ADJP and ADVP) are fairly similar. The performance gain on NPs is also observed with another multi-modal induction model, VG-NSL (Shi et al., 2019;Kojima et al., 2020). Intuitively, image representations from image encoders trained on classification tasks very likely contain accurate information about objects in images, which is most relevant to identifying NPs 1 . However, they provide limited information for phrase types that mainly involve action and change, such as verb phrases. Representations of dynamic scenes may help the induction model to identify verbs, and also contain information about the argument structure of the verbs and nouns based on features of actions and participants extracted from videos. Therefore, we propose a model that induces PCFGs from raw text aided by the multi-modal information extracted from videos, and expect to see accuracy gains on such places in comparison to the baseline systems.", "publication_ref": ["b52", "b41", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Modal Compound PCFGs", "text": "In this section, we introduce the proposed multimodal compound PCFGs (MMC-PCFG). Instead 1 Jin and Schuler (2020) reports no improvement on English when incorporating visual information into a similar neural network-based PCFG induction model, which may be because Zhao and Titov (2020) removes punctuation from the training data, which removes a reliable source of phrasal boundary information. This loss is compensated by the induction model with image representations. We leave the study of evaluation configuration on induction results for future work.\nof purely relying on object information from images, we generalize VC-PCFG into the video domain, where multi-modal video information is considered. We first introduce the video representation in Section 3.1. We then describe the procedure for matching the multi-modal video representation with each span in Section 3.2. After that we introduce the training and inference details in Section 3.3.", "publication_ref": ["b52"], "figure_ref": [], "table_ref": []}, {"heading": "Video Representation", "text": "A video contains a sequence of frames, denoted as V = {v i } L 0 i=1 , where v i represents a frame in a video and L 0 indicates the total number of frames. We extract video representation from M models trained on different tasks, which are called experts. Each expert focuses on extracting a sequence of features of one type. In order to project different expert features into the same dimension, their feature sequences are feed into linear layers (one per expert) with same output dimension. We denote the outputs of the mth expert after projection as\nF m = {f m i } L m i=1\n, where f m i and L m represent the ith feature and the total number of features of the mth expert, respectively.\nA simple method would average each feature along the temporal dimension and then concatenating them together. However, this would ignore the relations among different modalities and the temporal ordering within each modality. In this paper, we use a multi-modal transformer to collect video representations Lei et al., 2020).\nThe multi-modal transformer expects a sequence as input, hence we concatenate all feature sequences together and take the form:\nX = [f 1 avg , f 1 1 , ..., f 1 L 1 , ...f M avg , f M 1 , ..., f M L M ], (8)\nwhere f m avg is the averaged feature of {f m i } Lm i=1 . Each transformer layer has a standard architecture and consists of multi-head self-attention module and a feed forward network (FFN). Since this architecture is permutation-invariant, we supplement it with expert type embeddings E and positional encoding P that are added to the input of each attention layer. The expert type embeddings indicate the expert type for input features and take the form: where e m is a learned embedding for the mth expert. The positional encodings indicate the location of each feature within the video and take the form:\nE = [e 1 ,\nP = [p 0 , p 1 , ..., p L 1 , ..., p 0 , p 1 , ..., p L M ], (10)\nwhere fixed encodings are used (Vaswani et al., 2017). After that, we collect the output of transformer that corresponds to the averaged features as the final video representation, i.e., \u03a8 = {\u03c8 i avg } M i=1 . In this way, we can learn more effective video representation by modeling the correlations of features from different modalities and different timestamps.", "publication_ref": ["b28", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Video-Text Matching", "text": "To compute the similarity between a video V and a particular span c, a span representation c is obtained following Section 2.2 and projected to M separate expert embeddings via gated embedding modules (one per expert) (Miech et al., 2018):\n\u03be i 1 = W i 1 c + b i 1 , \u03be i 2 = \u03be i 1 \u2022 sigmoid(W i 2 \u03be i 1 + b i 2 ), \u03be i = \u03be i 2 \u03be i 2 2 , (11\n)\nwhere i is the index of expert,\nW i 1 , W i 2 , b i 1 , b i 2\nare learnable parameters, sigmoid is an elementwise sigmoid activation and \u2022 is the element-wise multiplication. We denote the set of expert embeddings as \u039e = {\u03be i } M i=1 . The video-span similarity is computed as following,\n\u03c9 i (c) = exp(u i c) M j=1 exp(u j c) , o(\u039e, \u03a8) = M i=1 \u03c9 i (c)cos(\u03be i , \u03c8 i ),(12)\nwhere {u i } M i=1 are learned weights. Given \u039e , an unmatched span expert embeddings of \u03a8, and \u03a8 , an unmatched video representation of \u039e, the hinge loss for video is given by:\nh vid (\u039e,\u03a8) = E c [o(\u039e , \u03a8) \u2212 o(\u039e, \u03a8)) + ] + + E \u03a8 [o(\u039e, \u03a8 ) \u2212 o(\u039e, \u03a8) + ] + , (13)\nwhere is a positive margin. Finally the video-text matching loss is defined as:\ns vid (V, \u03c3) = c\u2208\u03c3 p(c|\u03c3)h vid (\u039e, \u03a8). (14\n)\nNoted that s vid can be regarded as a generalized form of s img in Equation 6, where features from different timestamps and modalities are considered.", "publication_ref": ["b35"], "figure_ref": [], "table_ref": []}, {"heading": "Training and Inference", "text": "During training, our model is optimized by the ELBO and the video-text matching loss:\nL(\u03c6, \u03b8) = (V,\u03c3)\u2208\u2126 \u2212ELBO(\u03c3; \u03c6, \u03b8)+\u03b1s vid (V, \u03c3), (15\n)\nwhere \u03b1 is a hyper-parameter balancing these two loss terms and \u2126 is a video-sentence pair.\nDuring inference, we predict the most likely tree t * given a sentence \u03c3 without accessing videos. Since computing the integral over z is intractable, t * is estimated with the following approximation,\nt * = arg max t z p \u03b8 (t|z)p \u03b8 (z|\u03c3)dz \u2248 arg max t p \u03b8 (t|\u03c3, \u00b5 \u03c6 (\u03c3)),(16)\nwhere \u00b5 \u03c6 (\u03c3) is the mean vector of the variational posterior q \u03c6 (z|\u03c3) and t * can be obtained using the CYK algorithm (Cocke, 1969;Younger, 1967;Kasami, 1966). ", "publication_ref": ["b4", "b49", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "Following the evaluation practice in Zhao and Titov (2020), we discard punctuation and ignore trivial single-word and sentence-level spans at test time.\nThe gold parse trees are obtained by applying a state-of-the-art constituency parser, Benepar (Kitaev and Klein, 2018), on the testing set. All models are run 4 times for 10 epochs with different random seeds. We evaluate both averaged corpus-level F1 (C-F1) and averaged sentence-level F1 (S-F1) numbers as well as their standard deviations.", "publication_ref": ["b52"], "figure_ref": [], "table_ref": []}, {"heading": "Expert Features", "text": "In order to capture the rich content from videos, we extract features from the state-of-the-art models of different tasks, including object, action, scene, sound, face, speech, and optical character recognition (OCR). For object and action recognition, we explore multiple models with different architectures and pre-trained dataset. Details are as follows:\nObject features are extracted by two models: ResNeXt-101 (Xie et al., 2017), pre-trained on Instagram hashtags (Mahajan et al., 2018) and finetuned on ImageNet (Krizhevsky et al., 2012), and SENet-154 (Hu et al., 2018), trained on ImageNet. These datasets include images of common objects, such as, \"cock\", \"kite\", and \"goose\", etc. We use the predicted logits as object features for both models, where the dimension is 1000.\nAction features are extracted by three models: I3D trained on Kinetics-400 (Carreira and Zisserman, 2017), R2P1D (Tran et al., 2018) trained on IG-65M (Ghadiyaram et al., 2019) and S3DG ) trained on HowTo100M (Miech et al., 2019). These datasets include videos of human actions, such as \"playing guitar\", \"ski jumping\", and \"jogging\", etc. Following the same processing steps in their original work, we extract the predicted logits as action features, where the dimension is 400 (I3D), 359 (R2P1D) and 512 (S3DG), respectively.\nScene features are extracted by DenseNet-161 (Huang et al., 2017) trained on Places365 . Places365 contains images of different scenes, such as \"library\", \"valley\", and \"rainforest\", etc. The predicted logits are used as scene features, where the feature dimension is 365.\nAudio features are extracted by VGGish trained on YouTube-8M (Hershey et al., 2017), where the feature dimension is 128. YouTube-8M is a video dataset where different types of sound are involved, such as \"piano\", \"drum\", and \"violin\".\nOCR features are extracted by two steps: characters are first recognized by combining text detector Pixel Link (Deng et al., 2018) and text recognizer SSFL . The characters are then converted to word embeddings through word2vec (Mikolov et al., 2013) as the final OCR features, where the feature dimension is 300.\nFace features are extracted by combining face de-tector SSD (Liu et al., 2016) and face recognizer ResNet50 (He et al., 2016). The feature dimension is 512. Speech features are extracted by two steps: transcripts are first obtained via Google Cloud Speech to Text API. The transcripts are then converted to word embeddings through word2vec (Mikolov et al., 2013) as the final speech features, where the dimension is 300.", "publication_ref": ["b46", "b32", "b27", "b15", "b1", "b43", "b9", "b36", "b16", "b14", "b5", "b37", "b29", "b12", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "We keep sentences with fewer than 20 words in the training set due to the computational limitation. After filtering, the training sets cover 99.4%, 98.5% and 97.1% samples of their original splits in DiDeMo, YouCook2 and MSRVTT.\nWe train baseline models, C-PCFG and VC-PCFG, with same hyper parameters suggested in Kim et al. (2019); Zhao and Titov (2020). Our MMC-PCFG is composed of a parsing model and a video-text matching model. The parsing model has the same parameters as VC-PCFG (please refer to their paper for details). For video-text matching model, all extracted expert features are projected to 512-dimensional vectors. The transformer has 2 layers, a dropout probability of 10%, a hidden size of 512 and an intermediate size of 2048. We select the top-2000 most common words as vocabulary for all datasets. All the baseline methods and our models are optimized using Adam (Kingma and Ba, 2015) with the learning rate set to 0.001, \u03b2 1 = 0.75 and \u03b2 2 = 0.999. All parameters are initialized with Xavier uniform initializer (Glorot and Bengio, 2010). The batch size is set to 16.\nDue to the long video durations, it is infeasible to feed all features into the multi-modal transformer. Therefore, each feature from object, motion and scene categories is partitioned into 8 chunks and then average-pooled within each chunk. For features from other categories, global average pooling is applied. In this way, the coarse-grained temporal information is preserved. Noted that some videos do not have audio and some videos do not have detected faces or text characters. For these missing features, we pad them with zeros. All the aforementioned expert features are obtained from Albanie et al. (2020).", "publication_ref": ["b22", "b52", "b23", "b11", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "We evaluate the proposed MMC-PCFG approach on three datasets, and compare it with recently proposed state-of-the-art methods, C-PCFG (Kim et al., 2019) and VC-PCFG (Zhao and Titov, 2020). The results are summarized in Table 1. The values high-lighted by bold and italic fonts indicate the top-2 methods, respectively. All results are reported in percentage (%). LBranch, RBranch and Random represent left branching trees, right branching trees and random trees, respectively. Since VC-PCFG is originally designed for images, it is not directly comparable with our method. In order to allow VC-PCFG to accept videos as input, we average video features in the temporal dimension first and then feed them into the model. We evaluate VC-PCFG with 10, 7, and 10 expert features for DiDeMo, YouCook2 and MSRVTT, respectively. In addition, we also include the concatenated averaged features (Concat). Since object and action categories involve more than one expert, we directly use experts' names instead of their categories in Table 1.\nOverall performance comparison. We first compare the overall performance, i.e., C-F1 and S-F1, among all models, as shown in Table 1. The right branching model serves as a strong baseline, since English is a largely right-branching language. C-PCFG learns parsing purely based on text. Compared to C-PCFG, the better overall performance of VC-PCFG demonstrates the effectiveness of leveraging video information. Compared within VC-PCFG, concatenating all features together may not even outperform a model trained on a single expert (R2P1D v.s. Concat in DiDeMo and MSRVTT). The reason is that each expert is learned independently, where their correlations are not considered. In contrast, our MMC-PCFG outperforms all baselines on C-F1 and S-F1 in all datasets. The superior performance indicates that our model can leverage the benefits from all the experts 2 . Moreover, the superior performance over Concat demonstrates the importance of modeling relations among different experts and different timestamps.\nPerformance comparison among different phrase types. We compare the models' recalls on top-3 frequent phrase types (NP, VP and PP). These three types cover 77.4%, 80.1% and 82.4% spans of gold trees on DiDeMo, YouCook2 and MSRVTT, respectively. In the following, we compare their performance on DiDeMo, as shown in   with a single expert, we find that object features (ResNeXt and SENet) achieve top-2 recalls on NPs, while action features (I3D, R2P1D and S3DG) achieve the top-3 recalls on VPs and PPs. It indicates that different experts help parser learn syntactic structures from different aspects. Meanwhile, action features improve C-PCFG 3 on VPs and PPs by a large margin, which once again verifies the benefits of using video information.\nComparing our MMC-PCFG with VC-PCFG, our model achieves the top-2 recall and is smaller in variance in NP, VP and PP. It demonstrates that our model can take the advantages of different experts and learn consistent grammar induction. 3 The low performance of C-PCFG on DiDeMo in terms of VP recall may be caused by it attaching a high attaching PP to the rest of the sentence instead of the rest of the verb phrase, which breaks the whole VP. For PPs, C-PCFG attaches prepositions to the word in front, which may be caused by confusion between prepositions in PPs and phrasal verbs. ", "publication_ref": ["b22", "b52"], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_1"]}, {"heading": "Ablation Study", "text": "In this section, we conduct several ablation studies on DiDeMo, shown in Figures 2-4. All results are reported in percentage (%).\nPerformance comparison over constituent length. We first demonstrate the model performance for constituents at different lengths in Figure 2. As constituent length becomes longer, the recall of all models (except RBranch) decreases as expected (Kim et al., 2019;Zhao and Titov, R  Consistency between different models. Next, we analyze the consistency of these different models.\nThe consistency between two models is measured by averaging sentence-level F1 scores over all possible pairings of different runs 4 (Williams et al., 2018). We plot the consistency for each pair of models in Figure 4 and call it consistency matrix.\nComparing the self F1 of all the models (the diagonal in the matrix), R2P1D has the highest score, suggesting that R2P1D is the most reliable feature that can help parser to converge to a specific grammar. Comparing the models trained with different single experts, ResNeXt v.s. SENet reaches the highest non-self F1, since they are both object features trained on ImageNet and have similar effects to the parser. We also find that the lowest non-self F1 comes from Audio v.s. I3D, since they are extracted from different modalities (video v.s. sound).\nCompared with other models, our model is most consistent with R2P1D, indicating that R2P1D contributes most to our final prediction.   largest performance drops (see Table 2). Therefore, videos contribute most to the performance among all modalities.", "publication_ref": ["b22", "b45"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_5"]}, {"heading": "Qualitative Analysis", "text": "In Figure 5, we visualize a parse tree predicted by the best run of SENet154, I3D and MMC-PCFG. We can observe that SENet identifies all NPs but fails at the VP. I3D correctly predicts the VP but fails at recognizing a NP, \"the man\". Our MMC-PCFG can take advantages of all experts and produce the correct prediction.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Related Work", "text": "Grammar Induction Grammar induction and unsupervised parsing has been a long-standing problem in computational linguistics (Carroll and Charniak, 1992). Recent work utilized neural networks in predicting constituency structures with no supervision (Shen et al., 2018a;Drozdov et al., 2019;Shen et al., 2018b;Kim et al., 2019;Jin et al., 2019a) and showed promising results. In addition to learning purely from text, there is a growing interest to use image information to improve accuracy of induced constituency trees (Shi et al., 2019;Kojima et al., 2020;Zhao and Titov, 2020;Jin and Schuler, 2020). Different from previous work, our work improves the constituency parser by using videos containing richer information than images.\nVideo-Text Matching Video-text matching has been widely studied in various tasks, such as video retrieval (Liu et al., 2019;, moment localization with natural language (Zhang et al., 2019(Zhang et al., , 2020 and video question and answering (Xu et al., 2017;Jin et al., 2019b). It aims to learn video-semantic representation in a joint embedding space. Recent works (Liu et al., 2019; focus on learning video's multi-modal representation to match with text. In this work, we borrow this idea to match video and textual representations.", "publication_ref": ["b2", "b39", "b6", "b40", "b22", "b18", "b41", "b26", "b52", "b19", "b30", "b51", "b50", "b47", "b20", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we have presented a new task referred to as video-aided unsupervised grammar induction. This task aims to improve grammar induction models by using aligned video-sentence pairs as an effective way to address the limitation of current image-based methods where only object information from static images is considered and important verb related information from vision is missing. Moreover, we present Multi-Modal Compound Probabilistic Context-Free Grammars (MMC-PCFG) to effectively integrate video features extracted from different modalities to induce more accurate grammars. Experiments on three datasets demonstrate the effectiveness of our method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We thank the support of NSF awards IIS-1704337, IIS-1722847, IIS-1813709, and the generous gift from our corporate sponsors.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The end-of-end-to-end: A video understanding pentathlon challenge", "journal": "", "year": "2020", "authors": "Samuel Albanie; Yang Liu; Arsha Nagrani; Antoine Miech; Ernesto Coto; Ivan Laptev; Rahul Sukthankar; Bernard Ghanem; Andrew Zisserman; Valentin Gabeur"}, {"ref_id": "b1", "title": "Quo vadis, action recognition? a new model and the kinetics dataset", "journal": "", "year": "2017", "authors": "Joao Carreira; Andrew Zisserman"}, {"ref_id": "b2", "title": "Two experiments on learning probabilistic dependency grammars from corpora", "journal": "", "year": "1992", "authors": "Glenn Carroll; Eugene Charniak"}, {"ref_id": "b3", "title": "Learning modality interaction for temporal sentence localization and event captioning in videos", "journal": "", "year": "2020", "authors": "Shaoxiang Chen; Wenhao Jiang; Wei Liu; Yu-Gang Jiang"}, {"ref_id": "b4", "title": "Programming languages and their compilers: Preliminary notes", "journal": "", "year": "1969", "authors": "John Cocke"}, {"ref_id": "b5", "title": "Pixellink: Detecting scene text via instance segmentation", "journal": "", "year": "2018", "authors": "Dan Deng; Haifeng Liu; Xuelong Li; Deng Cai"}, {"ref_id": "b6", "title": "Unsupervised latent tree induction with deep inside-outside recursive auto-encoders", "journal": "", "year": "2019", "authors": "Andrew Drozdov; Patrick Verga; Mohit Yadav; Mohit Iyyer; Andrew Mccallum"}, {"ref_id": "b7", "title": "Cross-domain generalization of neural constituency parsers", "journal": "", "year": "2019", "authors": "Daniel Fried; Nikita Kitaev; Dan Klein"}, {"ref_id": "b8", "title": "Multi-modal transformer for video retrieval", "journal": "", "year": "2020", "authors": "Valentin Gabeur; Chen Sun; Karteek Alahari; Cordelia Schmid"}, {"ref_id": "b9", "title": "Large-scale weakly-supervised pre-training for video action recognition", "journal": "", "year": "2019", "authors": "Deepti Ghadiyaram; Du Tran; Dhruv Mahajan"}, {"ref_id": "b10", "title": "The Structural Sources of Verb Meanings", "journal": "Language Acquisition", "year": "1990", "authors": "Lila Gleitman"}, {"ref_id": "b11", "title": "Understanding the difficulty of training deep feedforward neural networks", "journal": "", "year": "2010", "authors": "Xavier Glorot; Yoshua Bengio"}, {"ref_id": "b12", "title": "Identity mappings in deep residual networks", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b13", "title": "Localizing moments in video with natural language", "journal": "", "year": "2017", "authors": "Lisa Anne Hendricks; Oliver Wang; Eli Shechtman; Josef Sivic; Trevor Darrell; Bryan Russell"}, {"ref_id": "b14", "title": "CNN architectures for largescale audio classification", "journal": "", "year": "2017", "authors": "Shawn Hershey; Sourish Chaudhuri; P W Daniel;  Ellis; F Jort; Aren Gemmeke; Channing Jansen; Manoj Moore; Devin Plakal;  Platt; A Rif; Bryan Saurous;  Seybold"}, {"ref_id": "b15", "title": "Squeeze-andexcitation networks", "journal": "", "year": "2018", "authors": "Jie Hu; Li Shen; Gang Sun"}, {"ref_id": "b16", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b17", "title": "Unsupervised grammar induction with depth-bounded pcfg", "journal": "TACL", "year": "2018", "authors": "Lifeng Jin; Finale Doshi-Velez; Timothy Miller; William Schuler; Lane Schwartz"}, {"ref_id": "b18", "title": "Unsupervised learning of PCFGs with normalizing flow", "journal": "", "year": "2019", "authors": "Lifeng Jin; Finale Doshi-Velez; Timothy Miller; Lane Schwartz; William Schuler"}, {"ref_id": "b19", "title": "Grounded pcfg induction with images", "journal": "", "year": "2020", "authors": "Lifeng Jin; William Schuler"}, {"ref_id": "b20", "title": "Multi-interaction network with object relation for video question answering", "journal": "", "year": "2019-06", "authors": "Weike Jin; Zhou Zhao; Mao Gu"}, {"ref_id": "b21", "title": "An efficient recognition and syntax-analysis algorithm for context-free languages", "journal": "Coordinated Science Laboratory Report", "year": "1966", "authors": "Tadao Kasami"}, {"ref_id": "b22", "title": "Compound probabilistic context-free grammars for grammar induction", "journal": "", "year": "2019", "authors": "Yoon Kim; Chris Dyer; Alexander M Rush"}, {"ref_id": "b23", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b24", "title": "Autoencoding variational bayes", "journal": "", "year": "2014", "authors": "P Diederik; Max Kingma;  Welling"}, {"ref_id": "b25", "title": "Constituency parsing with a self-attentive encoder", "journal": "", "year": "2018", "authors": "Nikita Kitaev; Dan Klein"}, {"ref_id": "b26", "title": "What is learned in visually grounded neural syntax acquisition", "journal": "", "year": "2020", "authors": "Noriyuki Kojima; Hadar Averbuch-Elor; Alexander M Rush; Yoav Artzi"}, {"ref_id": "b27", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton"}, {"ref_id": "b28", "title": "Mart: Memoryaugmented recurrent transformer for coherent video paragraph captioning", "journal": "", "year": "2020", "authors": "Jie Lei; Liwei Wang; Yelong Shen; Dong Yu; Tamara L Berg; Mohit Bansal"}, {"ref_id": "b29", "title": "SSD: Single shot multibox detector", "journal": "", "year": "2016", "authors": "Wei Liu; Dragomir Anguelov; Dumitru Erhan; Christian Szegedy; Scott Reed; Cheng-Yang Fu; Alexander C Berg"}, {"ref_id": "b30", "title": "Use what you have: Video retrieval using representations from collaborative experts", "journal": "", "year": "2019", "authors": "Y Liu; S Albanie; A Nagrani; A Zisserman"}, {"ref_id": "b31", "title": "Synthetically supervised feature learning for scene text recognition", "journal": "", "year": "2018", "authors": "Yang Liu; Zhaowen Wang; Hailin Jin; Ian Wassell"}, {"ref_id": "b32", "title": "Exploring the limits of weakly supervised pretraining", "journal": "", "year": "2018", "authors": "Dhruv Mahajan; Ross Girshick; Vignesh Ramanathan; Kaiming He; Manohar Paluri; Yixuan Li"}, {"ref_id": "b33", "title": "Building a large annotated corpus of English: The Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz"}, {"ref_id": "b34", "title": "End-to-end learning of visual representations from uncurated instructional videos", "journal": "", "year": "2020", "authors": "Antoine Miech; Jean-Baptiste Alayrac; Lucas Smaira; Ivan Laptev; Josef Sivic; Andrew Zisserman"}, {"ref_id": "b35", "title": "Learning a text-video embedding from incomplete and heterogeneous data", "journal": "", "year": "2018", "authors": "Antoine Miech; Ivan Laptev; Josef Sivic"}, {"ref_id": "b36", "title": "HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips", "journal": "", "year": "2019", "authors": "Antoine Miech; Dimitri Zhukov; Jean-Baptiste Alayrac; Makarand Tapaswi; Ivan Laptev; Josef Sivic"}, {"ref_id": "b37", "title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b38", "title": "The bootstrapping problem in language acquisition. Mechanisms of language acquisition", "journal": "", "year": "1987", "authors": "Steven Pinker;  Macwhinney"}, {"ref_id": "b39", "title": "Neural language modeling by jointly learning syntax and lexicon", "journal": "", "year": "2018", "authors": "Yikang Shen; Zhouhan Lin; Chin Wei Huang; Aaron Courville"}, {"ref_id": "b40", "title": "Ordered neurons: Integrating tree structures into recurrent neural networks", "journal": "", "year": "2018", "authors": "Yikang Shen; Shawn Tan; Alessandro Sordoni; Aaron Courville"}, {"ref_id": "b41", "title": "Visually grounded neural syntax acquisition", "journal": "", "year": "2019", "authors": "Haoyue Shi; Jiayuan Mao; Kevin Gimpel; Karen Livescu"}, {"ref_id": "b42", "title": "Constructing a language: A usage-based theory of language acquisition", "journal": "Harvard University Press", "year": "2003", "authors": "Michael Tomasello"}, {"ref_id": "b43", "title": "A closer look at spatiotemporal convolutions for action recognition", "journal": "", "year": "2018", "authors": "Du Tran; Heng Wang; Lorenzo Torresani; Jamie Ray; Yann Lecun; Manohar Paluri"}, {"ref_id": "b44", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b45", "title": "Do latent tree learning models identify meaningful structure in sentences", "journal": "TACL", "year": "2018", "authors": "Adina Williams; Andrew Drozdov; * ; Samuel R Bowman"}, {"ref_id": "b46", "title": "Aggregated residual transformations for deep neural networks", "journal": "", "year": "2017", "authors": "Saining Xie; Ross Girshick; Piotr Doll\u00e1r; Zhuowen Tu; Kaiming He"}, {"ref_id": "b47", "title": "Video question answering via gradually refined attention over appearance and motion", "journal": "", "year": "2017", "authors": "Dejing Xu; Zhou Zhao; Jun Xiao; Fei Wu; Hanwang Zhang"}, {"ref_id": "b48", "title": "Msrvtt: A large video description dataset for bridging video and language", "journal": "", "year": "2016", "authors": "Jun Xu; Tao Mei; Ting Yao; Yong Rui"}, {"ref_id": "b49", "title": "Recognition and parsing of context-free languages in time n3", "journal": "Information and control", "year": "1967", "authors": "H Daniel;  Younger"}, {"ref_id": "b50", "title": "Learning 2d temporal adjacent networks formoment localization with natural language", "journal": "", "year": "2020", "authors": "Songyang Zhang; Houwen Peng; Jianlong Fu; Jiebo Luo"}, {"ref_id": "b51", "title": "Exploiting temporal relationships in video moment localization with natural language", "journal": "", "year": "2019", "authors": "Songyang Zhang; Jinsong Su; Jiebo Luo"}, {"ref_id": "b52", "title": "Visually grounded compound PCFGs", "journal": "", "year": "2020", "authors": "Yanpeng Zhao; Ivan Titov"}, {"ref_id": "b53", "title": "Places: A 10 million image database for scene recognition", "journal": "TPAMI", "year": "2017", "authors": "Bolei Zhou; Agata Lapedriza; Aditya Khosla; Aude Oliva; Antonio Torralba"}, {"ref_id": "b54", "title": "Towards automatic learning of procedures from web instructional videos", "journal": "", "year": "2018", "authors": "Luowei Zhou; Chenliang Xu; Jason J Corso"}, {"ref_id": "b55", "title": "Table 5: Performance Comparison on MSRVTT", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "e 1 , ..., e 1 , ..., e M , e M , ..., e M ], (9)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Recall comparison over constituent length on DiDeMo. Methods are differentiated with colors. For easy comparison, we additional draw gray lines in each figure to indicate the average recall shown by other figures.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Label distributions over the constituent length on DiDeMo. All represent frequencies of constituent lengths.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Parse trees predicted by different models for the sentence The man falls to the floor.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparing VC-PCFG trained 5\u00b10.5 30.1\u00b10.5 29.4\u00b10.3 32.7\u00b10.5 21.2\u00b10.2 24.0\u00b10.2 27.2\u00b10.1 30.5\u00b10.1 C-PCFG 72.9\u00b15.5 16.5\u00b16.2 23.4\u00b116.9 38.2\u00b15.0 40.4\u00b14.1 37.8\u00b16.7 41.4\u00b16.6 50.7\u00b13.2 55.0\u00b13.2", "figure_data": "DiDeMo PP 0.1 66.5 ResNeXt 64.4\u00b121.4 25.7\u00b117.7 34.6\u00b125.0 40.0\u00b113.7 41.8\u00b114.0 38.2\u00b18.3 42.8\u00b18.4 50.7\u00b11.7 54.9\u00b12.2 YouCook2 MSRVTT Method NP VP C-F1 S-F1 C-F1 S-F1 C-F1 S-F1 LBranch 41.7 0.1 16.2 18.5 6.8 5.9 14.4 16.8 RBranch 32.8 91.5 53 .6 57 .5 35.0 41.6 54.2 58.6 Random 36.5\u00b10.6 30.VC-PCFG SENet 70 .5 \u00b115.3 25.7\u00b115.9 36.5\u00b124.6 42.6\u00b110.4 44.0\u00b110.4 39.9\u00b18.7 44.9\u00b18.3 52.2\u00b11.2 56.0\u00b11.6 I3D 57.9\u00b113.5 45.7\u00b114.1 45.8\u00b117.2 45.1\u00b16.0 49.2\u00b16.0 40.6\u00b13.6 45.7\u00b13.2 54.5\u00b11.6 59 .1 \u00b11.7 R2P1D 61.2\u00b18.5 38.1\u00b15.4 62.1\u00b14.1 48.1\u00b14.4 50.7\u00b14.2 39.4\u00b18.1 44.4\u00b18.3 54.0\u00b12.5 58.0\u00b12.3 S3DG 61.3\u00b113.4 31.7\u00b116.7 51.8\u00b18.0 44.0\u00b12.7 46.5\u00b15.1 39.3\u00b16.5 44.1\u00b16.6 50.7\u00b13.2 54.7\u00b12.9 Scene 62.2\u00b19.6 30.6\u00b112.3 41.1\u00b124.8 41.7\u00b16.5 44.9\u00b17.4 \u2212 \u2212 54 .6 \u00b11.5 58.4\u00b11.3 Audio 64.2\u00b118.6 21.3\u00b126.5 34.7\u00b111.0 38.7\u00b13.7 39.5\u00b15.2 39.2\u00b14.7 43.3\u00b14.9 52.8\u00b11.3 56.7\u00b11.4 OCR 64.4\u00b115.0 27.4\u00b119.5 42.8\u00b131.2 41.9\u00b116.9 44.6\u00b117.5 38.6\u00b15.5 43.2\u00b15.6 51.0\u00b13.0 55.5\u00b13.0 Face 60.8\u00b116.0 31.5\u00b117.0 52.8\u00b19.8 43.9\u00b14.5 46.3\u00b15.5 \u2212 \u2212 50.5\u00b12.6 54.5\u00b12.6 Speech 61.8\u00b112.8 26.6\u00b117.6 43.8\u00b134.5 40.9\u00b116.0 43.1\u00b116.1 \u2212 \u2212 51.7\u00b12.6 56.2\u00b12.5 Concat 68.6\u00b18.6 24.9\u00b119.9 39.7\u00b119.5 42.2\u00b112.3 43.2\u00b114.2 42 .3 \u00b15.7 47 .0 \u00b15.6 49.8\u00b14.1 54.2\u00b14.0MMC-PCFG 67.9\u00b19.8 52 .3 \u00b19.0 63 .5 \u00b18.6 55.0\u00b13.7 58.9\u00b13.4 44.7\u00b15.2 48.9\u00b15.7 56.0\u00b11.4 60.0\u00b11.2"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance comparison on three benchmark datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "0\u00b19.9 52.7\u00b19.0 63.8\u00b18.7 55.3\u00b13.3 59.0\u00b13.4 w/o audio 69.3\u00b18.0 41.8\u00b111.0 45.3\u00b120.2 48.7\u00b16.2 52.0\u00b16.5 w/o text 68.5\u00b113.7 38.8\u00b116.9 57.0\u00b120.4 49.6\u00b110.4 52.0\u00b111.1 w/o video 64.3\u00b14.4 28.1\u00b17.5 38.9\u00b125.6 41.4\u00b16.0 44.8\u00b15.9", "figure_data": "ModelNPVPPPC-F1S-F1full68.Contribution of different modalities. We alsoevaluate how different modalities contribute to theperformance of MMC-PCFG. We divide currentexperts into three groups, video (objects, action,scene and face), audio (audio) and text (OCR andASR). By ablating one group during training, wefind that the model without video experts has the4 Different runs represent models trained with different seeds."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance comparison over modalities on MMC-PCFG on DiDeMo.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03c0 r = g r (z; \u03b8), z \u223c p(z). (1", "formula_coordinates": [2.0, 350.89, 484.57, 169.28, 13.26]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [2.0, 520.17, 484.57, 4.24, 13.15]}, {"formula_id": "formula_2", "formula_text": "\u03c0 S\u2192A = exp(u A f s ([w S ; z])) A \u2208N exp(u A f s ([w S ; z])) , \u03c0 A\u2192BC = exp(u BC [w A ; z]) B ,C \u2208N \u222aP exp(u B C [w A ; z])) , \u03c0 T \u2192w = exp(u w f t ([w T ; z])) w \u2208\u03a3 exp(u T w f t ([w T ; z])) ,(2)", "formula_coordinates": [2.0, 313.38, 615.13, 211.03, 104.76]}, {"formula_id": "formula_3", "formula_text": "log p \u03b8 (\u03c3) = log z t\u2208T G (\u03c3) p \u03b8 (t|z)p(z)dz, (3)", "formula_coordinates": [3.0, 82.84, 139.18, 206.29, 26.37]}, {"formula_id": "formula_4", "formula_text": "ELBO(\u03c3; \u03c6, \u03b8) = E q \u03c6 (z|\u03c3) [log p \u03b8 (\u03c3|z)] \u2212 KL[q \u03c6 (z|\u03c3)||p(z)],(4)", "formula_coordinates": [3.0, 87.4, 250.92, 201.73, 29.76]}, {"formula_id": "formula_5", "formula_text": "c = K k=1 p(k|c, \u03c3)f k ( 1 j \u2212 i + 1 j l=i h l ),(5)", "formula_coordinates": [3.0, 87.76, 633.69, 201.38, 34.56]}, {"formula_id": "formula_6", "formula_text": "s img (v, \u03c3) = c\u2208\u03c3 p(c|\u03c3)h img (c, v),(6)", "formula_coordinates": [3.0, 101.13, 752.28, 188.0, 24.17]}, {"formula_id": "formula_7", "formula_text": "h img (c,v) = E c [cos(c , v) \u2212 cos(c, v)) + ] + + E v [cos(c, v ) \u2212 cos(c, v) + ] + , (7)", "formula_coordinates": [3.0, 311.87, 175.73, 212.54, 28.1]}, {"formula_id": "formula_8", "formula_text": "F m = {f m i } L m i=1", "formula_coordinates": [4.0, 70.87, 367.49, 69.73, 15.46]}, {"formula_id": "formula_9", "formula_text": "X = [f 1 avg , f 1 1 , ..., f 1 L 1 , ...f M avg , f M 1 , ..., f M L M ], (8)", "formula_coordinates": [4.0, 80.58, 564.19, 208.55, 15.28]}, {"formula_id": "formula_10", "formula_text": "E = [e 1 ,", "formula_coordinates": [4.0, 86.1, 727.74, 39.33, 12.68]}, {"formula_id": "formula_11", "formula_text": "P = [p 0 , p 1 , ..., p L 1 , ..., p 0 , p 1 , ..., p L M ], (10)", "formula_coordinates": [4.0, 314.3, 93.9, 210.11, 14.27]}, {"formula_id": "formula_12", "formula_text": "\u03be i 1 = W i 1 c + b i 1 , \u03be i 2 = \u03be i 1 \u2022 sigmoid(W i 2 \u03be i 1 + b i 2 ), \u03be i = \u03be i 2 \u03be i 2 2 , (11", "formula_coordinates": [4.0, 332.81, 324.09, 187.06, 64.93]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [4.0, 519.87, 348.51, 4.54, 13.15]}, {"formula_id": "formula_14", "formula_text": "W i 1 , W i 2 , b i 1 , b i 2", "formula_coordinates": [4.0, 448.9, 396.22, 75.01, 14.55]}, {"formula_id": "formula_15", "formula_text": "\u03c9 i (c) = exp(u i c) M j=1 exp(u j c) , o(\u039e, \u03a8) = M i=1 \u03c9 i (c)cos(\u03be i , \u03c8 i ),(12)", "formula_coordinates": [4.0, 342.59, 487.33, 181.82, 68.43]}, {"formula_id": "formula_16", "formula_text": "h vid (\u039e,\u03a8) = E c [o(\u039e , \u03a8) \u2212 o(\u039e, \u03a8)) + ] + + E \u03a8 [o(\u039e, \u03a8 ) \u2212 o(\u039e, \u03a8) + ] + , (13)", "formula_coordinates": [4.0, 311.69, 628.94, 212.72, 28.1]}, {"formula_id": "formula_17", "formula_text": "s vid (V, \u03c3) = c\u2208\u03c3 p(c|\u03c3)h vid (\u039e, \u03a8). (14", "formula_coordinates": [4.0, 327.4, 702.71, 192.47, 24.17]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [4.0, 519.87, 702.71, 4.54, 13.15]}, {"formula_id": "formula_19", "formula_text": "L(\u03c6, \u03b8) = (V,\u03c3)\u2208\u2126 \u2212ELBO(\u03c3; \u03c6, \u03b8)+\u03b1s vid (V, \u03c3), (15", "formula_coordinates": [5.0, 70.87, 130.89, 218.77, 36.52]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [5.0, 284.59, 154.26, 4.54, 13.15]}, {"formula_id": "formula_21", "formula_text": "t * = arg max t z p \u03b8 (t|z)p \u03b8 (z|\u03c3)dz \u2248 arg max t p \u03b8 (t|\u03c3, \u00b5 \u03c6 (\u03c3)),(16)", "formula_coordinates": [5.0, 95.14, 265.21, 194.0, 43.74]}], "doi": ""}