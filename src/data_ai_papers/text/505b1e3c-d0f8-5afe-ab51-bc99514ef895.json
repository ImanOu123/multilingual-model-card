{"title": "Optimal Algorithms for Non-Smooth Distributed Optimization in Networks", "authors": "Kevin Scaman; Francis Bach; S\u00e9bastien Bubeck; Yin Tat Lee; Laurent Massouli\u00e9", "pub_date": "2018-06-01", "abstract": "In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that, for non-smooth functions, while the dominant term of the error is in O(1/ \u221a t), the structure of the communication network only impacts a second-order term in O(1/t), where t is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a d 1/4 multiplicative factor of the optimal convergence rate, where d is the underlying dimension. Recently, a theoretical analysis of first-order distributed methods provided optimal convergence rates for strongly-convex and smooth optimization in networks [8]. In this paper, we extend this analysis to the more challenging case of non-smooth convex optimization. The main contribution of this", "sections": [{"heading": "Introduction", "text": "Distributed optimization finds many applications in machine learning, for example when the dataset is large and training is achieved using a cluster of computing units. As a result, many algorithms were recently introduced to minimize the averagef = 1 n n i=1 f i of functions f i which are respectively accessible by separate nodes in a network [1,2,3,4]. Most often, these algorithms alternate between local and incremental improvement steps (such as gradient steps) with communication steps between nodes in the network, and come with a variety of convergence rates (see for example [5,4,6,7]). paper is to provide optimal convergence rates and their corresponding optimal algorithms for this class of distributed problems under two regularity assumptions: (1) the Lipschitz continuity of the global objective functionf , and (2) a bound on the average of Lipschitz constants of local functions f i .\nUnder the local regularity assumption, we provide in Section 4 matching upper and lower bounds of complexity in a decentralized setting in which communication is performed using the gossip algorithm [9]. Moreover, we propose the first optimal algorithm for non-smooth decentralized optimization, called multi-step primal-dual (MSPD). Under the more challenging global regularity assumption, we show in Section 3 that distributing the simple smoothing approach introduced in [10] yields fast convergence rates with respect to communication. This algorithm, called distributed randomized smoothing (DRS), achieves a convergence rate matching the lower bound up to a d 1/4 multiplicative factor, where d is the dimensionality of the problem.\nOur analysis differs from the smooth and strongly-convex setting in two major aspects: (1) the na\u00efve master/slave distributed algorithm is in this case not optimal, and (2) the convergence rates differ between communication and local computations. More specifically, error due to limits in communication resources enjoys fast convergence rates, as we establish by formulating the optimization problem as a composite saddle-point problem with a smooth term for communication and non-smooth term for the optimization of the local functions (see Section 4 and Eq. (21) for more details).\nRelated work. Many algorithms were proposed to solve the decentralized optimization of an average of functions (see for example [1,11,3,4,12,2,13,5]), and a sheer amount of work was devoted to improving the convergence rate of these algorithms [5,6]. In the case of non-smooth optimization, fast communication schemes were developed in [14,15], although precise optimal convergence rates were not obtained. Our decentralized algorithm is closely related to the recent primal-dual algorithm of [14] which enjoys fast communication rates in a decentralized and stochastic setting. Unfortunately, their algorithm lacks gossip acceleration to reach optimality with respect to communication time. Finally, optimal convergence rates for distributed algorithms were investigated in [8] for smooth and strongly-convex objective functions, and [16,17] for totally connected networks.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b3", "b5", "b6", "b0", "b8", "b9", "b0", "b10", "b2", "b3", "b11", "b1", "b12", "b4", "b4", "b5", "b13", "b14", "b13", "b7", "b15", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Distributed optimization setting", "text": "Optimization problem. Let G = (V, E) be a strongly connected directed graph of n computing units and diameter \u2206, each having access to a convex function f i over a convex set K \u2282 R d . We consider minimizing the average of the local functions\nmin \u03b8\u2208Kf (\u03b8) = 1 n n i=1 f i (\u03b8) ,(1)\nin a distributed setting. More specifically, we assume that each computing unit can compute a subgradient \u2207f i (\u03b8) of its own function in one unit of time, and communicate values (i.e. vectors in R d ) to its neighbors in G. A direct communication along the edge (i, j) \u2208 E requires a time \u03c4 \u2265 0. These actions may be performed asynchronously and in parallel, and each machine i possesses a local version of the parameter, which we refer to as \u03b8 i \u2208 K.\nRegularity assumptions. Optimal convergence rates depend on the precise set of assumptions applied to the objective function. In our case, we will consider two different constraints on the regularity of the functions:\n(A1) Global regularity: the (global) functionf is convex and L g -Lipschitz continuous, in the sense that, for all \u03b8, \u03b8 \u2032 \u2208 K,\n|f (\u03b8) \u2212f (\u03b8 \u2032 )| \u2264 L g \u03b8 \u2212 \u03b8 \u2032 2 .(2)\n(A2) Local regularity: Each local function is convex and L i -Lipschitz continuous, and we denote\nas L \u2113 = 1 n n i=1 L 2 i the \u2113 2 -average of the local Lipschitz constants.\nAssumption (A1) is weaker than (A2), as we always have L g \u2264 L \u2113 . Moreover, we may have L g = 0 and L \u2113 arbitrarily large, for example with two linear functions f 1 (x) = \u2212f 2 (x) = ax and a \u2192 +\u221e.\nWe will see in the following sections that the local regularity assumption is easier to analyze and leads to matching upper and lower bounds. For the global regularity assumption, we only provide an algorithm with a d 1/4 competitive ratio, where d is the dimension of the problem. Finding an optimal distributed algorithm for global regularity is, to our understanding, a much more challenging task and is left for future work.\nFinally, we assume that the feasible region K is convex and bounded, and denote by R the radius of a ball containing\nK, i.e. \u2200\u03b8 \u2208 K, \u03b8 \u2212 \u03b8 0 2 \u2264 R ,(3)\nwhere \u03b8 0 \u2208 K is the initial value of the algorithm, that we set to \u03b8 0 = 0 without loss of generality.\nBlack-box optimization procedure. The lower complexity bounds in Theorem 2 and Theorem 3 depend on the notion of black-box optimization procedures of [8] that we now recall. A black-box optimization procedure is a distributed algorithm verifying the following constraints:\n1. Local memory: each node i can store past values in a (finite) internal memory M i,t \u2282 R d at time t \u2265 0. These values can be accessed and used at time t by the algorithm run by node i, and are updated either by local computation or by communication (defined below), that is, for all i \u2208 {1, ..., n},\nM i,t \u2282 M comp i,t \u222a M comm i,t .(4)\n2. Local computation: each node i can, at time t, compute a subgradient of its local function \u2207f i (\u03b8) for a value \u03b8 \u2208 M i,t\u22121 in the node's internal memory before the computation.\nM comp i,t = Span ({\u03b8, \u2207f i (\u03b8) : \u03b8 \u2208 M i,t\u22121 }) .(5)\n3. Local communication: each node i can, at time t, share a value to all or part of its neighbors, that is, for all i \u2208 {1, ..., n},\nM comm i,t = Span (j,i)\u2208E M j,t\u2212\u03c4 .(6)\n4. Output value: each node i must, at time t, specify one vector in its memory as local output of the algorithm, that is, for all i \u2208 {1, ..., n},\n\u03b8 i,t \u2208 M i,t .(7)\nHence, a black-box procedure will return n output values-one for each computing unit-and our analysis will focus on ensuring that all local output values are converging to the optimal parameter of Eq. (1). For simplicity, we assume that all nodes start with the simple internal memory M i,0 = {0}. Note that communications and local computations may be performed in parallel and asynchronously.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Distributed optimization under global regularity", "text": "The most standard approach for distributing a first-order optimization method consists in computing a subgradient of the average function\n\u2207f (\u03b8) = 1 n n i=1 \u2207f i (\u03b8) ,(8)\nwhere \u2207f i (\u03b8) is any subgradient of f i at \u03b8, by sending the current parameter \u03b8 t to all nodes, performing the computation of all local subgradients in parallel and averaging them on a master node. Since each iteration requires communicating twice to the whole network (once for \u03b8 t and once for sending the local subgradients to the master node, which both take a time \u2206\u03c4 where \u2206 is the diameter of the network) and one subgradient computation (on each node and performed in parallel), the time to reach a precision \u03b5 with such a distributed subgradient descent is upper-bounded by\nO RL g \u03b5 2 (\u2206\u03c4 + 1) .(9)\nNote that this convergence rate depends on the global Lipschitz constant L g , and is thus applicable under the global regularity assumption. The number of subgradient computations in Eq. (9) (i.e. the term not proportional to \u03c4 ) cannot be improved, since it is already optimal for objective functions defined on only one machine (see for example Theorem 3.13 p. 280 in [18]). However, quite surprisingly, the error due to communication time may benefit from fast convergence rates in O(RL g /\u03b5). This result is already known under the local regularity assumption (i.e. replacing L g with L \u2113 or even max i L i ) in the case of decentralized optimization [14] or distributed optimization on a totally connected network [17]. To our knowledge, the case of global regularity has not been investigated by prior work.", "publication_ref": ["b17", "b13", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "A simple algorithm with fast communication rates", "text": "We now show that the simple smoothing approach introduced in [10] can lead to fast rates for error due to communication time. Let \u03b3 > 0 and f : R d \u2192 R be a real function. We denote as smoothed version of f the following function:\nf \u03b3 (\u03b8) = E [f (\u03b8 + \u03b3X)] ,(10)\nwhere X \u223c N (0, I) is a standard Gaussian random variable. The following lemma shows that f \u03b3 is both smooth and a good approximation of f .\nLemma 1 (Lemma E.3 of [10]). If \u03b3 > 0, then f \u03b3 is Lg \u03b3 -smooth and, for all \u03b8 \u2208 R d , f (\u03b8) \u2264 f \u03b3 (\u03b8) \u2264 f (\u03b8) + \u03b3L g \u221a d .(11)\nAlgorithm 1 distributed randomized smoothing\nInput: approximation error \u03b5 > 0, communication graph G, \u03b1 0 = 1, \u03b1 t+1 = 2/(1 + 1 + 4/\u03b1 2 t ) T = 20RLg d 1/4 \u03b5 , K = 5RLg d \u22121/4 \u03b5 , \u03b3 t = Rd \u22121/4 \u03b1 t , \u03b7 t = R\u03b1t 2Lg(d 1/4 + \u221a t+1 K )\n.\nOutput: optimizer \u03b8 T 1: Compute a spanning tree T on G.\n2: Send a random seed s to every node in T .\n3: Initialize the random number generator of each node using s.\n4: x 0 = 0, z 0 = 0, G 0 = 0 5: for t = 0 to T \u2212 1 do 6: y t = (1 \u2212 \u03b1 t )x t + \u03b1 t z t 7:\nSend y t to every node in T .\n8:\nEach node i computes g i = 1 K K k=1 \u2207f i (y t + \u03b3 t X t,k ), where X t,k \u223c N (0, I) 9: G t+1 = G t + 1 n\u03b1t i g i 10: z t+1 = argmin x\u2208K x + \u03b7 t+1 G t+1 2 2\n11:\nx t+1 = (1 \u2212 \u03b1 t )x t + \u03b1 t z t+1 12: end for 13: return \u03b8 T = x T Hence, smoothing the objective function allows the use of accelerated optimization algorithms and provides faster convergence rates. Of course, the price to pay is that each computation of the smoothed gradient \u2207f \u03b3 (\u03b8) = 1 n n i=1 \u2207f \u03b3 i (\u03b8) now requires, at each iteration m, to sample a sufficient amount of subgradients \u2207f i (\u03b8 + \u03b3X m,k ) to approximate Eq. (10), where X m,k are K i.i.d. Gaussian random variables. At first glance, this algorithm requires all computing units to synchronize on the choice of X m,k , which would require to send to all nodes each X m,k and thus incur a communication cost proportional to the number of samples. Fortunately, computing units only need to share one random seed s \u2208 R and then use a random number generator initialized with the provided seed to generate the same random variables X m,k without the need to communicate any vector. The overall algorithm, denoted distributed randomized smoothing (DRS), uses the randomized smoothing optimization algorithm of [10] adapted to a distributed setting, and is summarized in Alg. 1. The computation of a spanning tree T in step 1 allows efficient communication to the whole network in time at most \u2206\u03c4 . Most of the algorithm (i.e. steps 2, 4, 6, 7, 9, 10 and 11) are performed on the root of the spanning subtree T , while the rest of the computing units are responsible for computing the smoothed gradient (step 8). The seed s of step 2 is used to ensure that every X m,k , although random, is the same on every node. Finally, step 10 is a simple orthogonal projection of the gradient step on the convex set K. We now show that the DRS algorithm converges to the optimal parameter under the global regularity assumption. Theorem 1. Under global regularity (A1), Alg. 1 achieves an approximation error E f (\u03b8\nT ) \u2212f (\u03b8 * ) of at most \u03b5 > 0 in a time T \u03b5 upper-bounded by O RL g \u03b5 (\u2206\u03c4 + 1)d 1/4 + RL g \u03b5 2 . (12\n)\nProof. See Appendix A.\nMore specifically, Alg. 1 completes its T iterations by time\nT \u03b5 \u2264 40 RL g d 1/4 \u03b5 \u2206\u03c4 + 100 RL g d 1/4 \u03b5 RL g d \u22121/4 \u03b5 . (13\n)\nComparing Eq. (13) to Eq. ( 9), we can see that our algorithm improves on the standard method when the dimension is not too large, and more specifically\nd \u2264 RL g \u03b5 4 .(14)\nIn practice, this condition is easily met, as \u03b5 \u2264 10 \u22122 already leads to the condition d \u2264 10 8 (assuming that R and L g have values around 1). Moreover, for problems of moderate dimension, the term d 1/4 remains a small multiplicative factor (e.g. for d = 1000, d 1/4 \u2248 6). Finally, note that DRS only needs the convexity off , and the convexity of the local functions f i is not necessary for Theorem 1 to hold. Remark 1. Several other smoothing methods exist in the literature, notably the Moreau envelope [19] enjoying a dimension-free approximation guarantee. However, the Moreau envelope of an average of functions is difficult to compute (requires a different oracle than computing a subgradient), and unfortunately leads to convergence rates with respect to local Lipschitz characteristics instead of L g .", "publication_ref": ["b9", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Optimal convergence rate", "text": "The following result provides oracle complexity lower bounds under the global regularity assumption, and is proved in Appendix B. This lower bound extends the communication complexity lower bound for totally connected communication networks from [17]. Theorem 2. Let G be a network of computing units of size n > 0, and L g , R > 0. There exists n functions f i : R d \u2192 R such that (A1) holds and, for any t < (d\u22122) 2 min{\u2206\u03c4, 1} and any black-box procedure one has, for all i \u2208 {1, ..., n},\nf (\u03b8 i,t ) \u2212 min \u03b8\u2208B2(R)f (\u03b8) \u2265 RL g 36 1 (1 + t 2\u2206\u03c4 ) 2 + 1 1 + t .(15)\nProof. See Appendix B.\nAssuming that the dimension d is large compared to the characteristic values of the problem (a standard set-up for lower bounds in non-smooth optimization [20, Theorem 3.2.1]), Theorem 2 implies that, under the global regularity assumption (A1), the time to reach a precision \u03b5 > 0 with any black-box procedure is lower bounded by\n\u2126 RL g \u03b5 \u2206\u03c4 + RL g \u03b5 2 ,(16)\nwhere the notation g(\u03b5) = \u2126(f (\u03b5)) stands for \u2203C > 0 s.t. \u2200\u03b5 > 0, g(\u03b5) \u2265 Cf (\u03b5). This lower bound proves that the convergence rate of DRS in Eq. ( 13) is optimal with respect to computation time and within a d 1/4 multiplicative factor of the optimal convergence rate with respect to communication.\nThe proof of Theorem 2 relies on the use of two objective functions: first, the standard worst case function used for single machine convex optimization (see e.g. [18]) is used to obtain a lower bound on the local computation time of individual machines. Then, a second function first introduced in [17] is split on the two most distant machines to obtain worst case communication times. By aggregating these two functions, a third one is obtained with the desired lower bound on the convergence rate. The complete proof is available in Appendix B. Remark 2. The lower bound also holds for the average of local parameters 1 n n i=1 \u03b8 i , and more generally any parameter that can be computed using any vector of the local memories at time t: in Theorem 2, \u03b8 i,t may be replaced by any \u03b8 t such that\n\u03b8 t \u2208 Span i\u2208V M i,t .(17)\n4 Decentralized optimization under local regularity\nIn many practical scenarios, the network may be unknown or changing through time, and a local communication scheme is preferable to the master/slave approach of Alg. 1. Decentralized algorithms tackle this problem by replacing targeted communication by local averaging of the values of neighboring nodes [9]. More specifically, we now consider that, during a communication step, each machine i broadcasts a vector x i \u2208 R d to its neighbors, then performs a weighted average of the values received from its neighbors:\nnode i sends x i to his neighbors and receives j W ji x j .\nIn order to ensure the efficiency of this communication scheme, we impose standard assumptions on the matrix W \u2208 R n\u00d7n , called the gossip matrix [9, 8]:\n1. W is symmetric and positive semi-definite, 2. The kernel of W is the set of constant vectors: Ker(W ) = Span(1), where 1 = (1, ..., 1) \u22a4 , 3. W is defined on the edges of the network:\nW ij = 0 only if i = j or (i, j) \u2208 E.", "publication_ref": ["b16", "b17", "b16", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Optimal convergence rate", "text": "Similarly to the smooth and strongly-convex case of [8], the lower bound on the optimal convergence rate is obtained by replacing the diameter of the network with 1/ \u03b3(W ), where \u03b3(W ) = \u03bb n\u22121 (W )/\u03bb 1 (W ) is the ratio between smallest non-zero and largest eigenvalues of W , also known as the normalized eigengap. Theorem 3. Let L \u2113 , R > 0 and \u03b3 \u2208 (0, 1]. There exists a matrix W of eigengap \u03b3(W ) = \u03b3, and n functions f i satisfying (A2), where n is the size of W , such that for all t < d\u22122 2 min(\u03c4 / \u221a \u03b3, 1) and all i \u2208 {1, ..., n},f\n(\u03b8 i,t ) \u2212 min \u03b8\u2208B2(R)f (\u03b8) \u2265 RL \u2113 108 1 (1 + 2t \u221a \u03b3 \u03c4 ) 2 + 1 1 + t .(19)\nProof. See Appendix C.\nAssuming that the dimension d is large compared to the characteristic values of the problem, Theorem 3 implies that, under the local regularity assumption (A2) and for a gossip matrix W with eigengap \u03b3(W ), the time to reach a precision \u03b5 > 0 with any decentralized black-box procedure is lower-bounded by\n\u2126 RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL \u2113 \u03b5 2 . (20\n)\nThe proof of Theorem 3 relies on linear graphs (whose diameter is proportional to 1/ \u03b3(L) where L is the Laplacian matrix) and Theorem 2. More specifically, a technical aspect of the proof consists in splitting the functions used in Theorem 2 on multiple nodes to obtain a dependency in L \u2113 instead of L g . The complete derivation is available in Appendix C.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Optimal decentralized algorithm", "text": "We now provide an optimal decentralized optimization algorithm under (A2). This algorithm is closely related to the primal-dual algorithm proposed by [14] which we modify by the use of accelerated gossip using Chebyshev polynomials as in [8].\nFirst, we formulate our optimization problem in Eq. (1) as the saddle-point problem Eq. ( 21) below, based on a duality argument similar to that in [8]. Then, we consider a square root A of the symmetric semi-definite positive gossip matrix W , of size n \u00d7 m for some m, such that AA \u22a4 = W and A \u22a4 u = 0 if and only if u is constant, and consider the equivalent problem of minimizing\n1 n n i=1 f i (\u03b8 i ) over \u0398 = (\u03b8 1 , . . . , \u03b8 n ) \u2208 K n with the constraint that \u03b8 1 = \u2022 \u2022 \u2022 = \u03b8 n , or equivalently \u0398A = 0.\nThrough Lagrangian duality, we therefore get the equivalent problem:\nmin \u0398\u2208K n max \u039b\u2208R d\u00d7n 1 n n i=1 f i (\u03b8 i ) \u2212 tr \u039b \u22a4 \u0398A . (21\n)\nWe solve it by applying Algorithm 1 in Chambolle-Pock [21] (we could alternatively apply composite Mirror-Prox [22]), with the following steps at each iteration t, with initialization \u039b 0 = 0 and \u0398 0 = \u0398 \u22121 = (\u03b8 0 , . . . , \u03b8 0 ):\n(a) \u039b t+1 = \u039b t \u2212 \u03c3(2\u0398 t+1 \u2212 \u0398 t )A (b) \u0398 t+1 = argmin \u0398\u2208K n 1 n n i=1 f i (\u03b8 i ) \u2212 tr \u0398 \u22a4 \u039b t+1 A \u22a4 + 1 2\u03b7 tr(\u0398 \u2212 \u0398 t ) \u22a4 (\u0398 \u2212 \u0398 t ) ,(22)\nwhere the gain parameters \u03b7, \u03c3 are required to satisfy \u03c3\u03b7\u03bb 1 (W ) \u2264 1. We implement the algorithm with the variables \u0398 t and Y t = \u039b t A \u22a4 = (y t 1 , . . . , y t n ) \u2208 R d\u00d7n , for which all updates can be made locally: Since AA \u22a4 = W , they now become\n(a \u2032 ) Y t+1 = Y t \u2212 \u03c3(2\u0398 t+1 \u2212 \u0398 t )W (b \u2032 ) \u03b8 t+1 i = argmin \u03b8i\u2208K 1 n f i (\u03b8 i ) \u2212 \u03b8 \u22a4 i y t+1 i + 1 2\u03b7 \u03b8 i \u2212 \u03b8 t i 2 , \u2200i \u2208 {1, . . . , n} ,(23)\nThe step (b \u2032 ) still requires a proximal step for each function f i . We approximate it by the outcome of the subgradient method run for M steps, with a step-size proportional to 2/(m + 2) as suggested in [23]. That is, initialized with\u03b8 0 i = \u03b8 t i , it performs the iterations\n\u03b8 m+1 i = m m + 2\u03b8 m i \u2212 2 m + 2 \u03b7 n \u2207f i (\u03b8 m i ) \u2212 \u03b7y t+1 i \u2212 \u03b8 t i , m = 0, . . . , M \u2212 1.(24)\nAlgorithm 2 multi-step primal-dual algorithm Input: approximation error \u03b5 > 0, gossip matrix W \u2208 R n\u00d7n ,\nK = \u230a1/ \u03b3(W )\u230b, M = T = \u2308 4\u03b5 RL \u2113 \u2309, c 1 = 1\u2212 \u221a \u03b3(W ) 1+ \u221a \u03b3(W ) , \u03b7 = nR L \u2113 1\u2212c K 1 1+c K 1 , \u03c3 = 1+c 2K 1 \u03c4 (1\u2212c K 1 ) 2 . Output: optimizer\u03b8 T 1: \u0398 0 = 0, \u0398 \u22121 = 0, Y 0 = 0 2: for t = 0 to T \u2212 1 do 3: Y t+1 = Y t \u2212 \u03c3 ACCELERATEDGOSSIP(2\u0398 t \u2212 \u0398 t\u22121 , W , K) // see [8, Alg. 2] 4:\u0398 0 = \u0398 t 5: for m = 0 to M \u2212 1 do 6:\u03b8 m+1 i = m m+2\u03b8 m i \u2212 2 m+2 \u03b7 n \u2207f i (\u03b8 m i ) \u2212 \u03b7y t+1 i \u2212 \u03b8 t i , \u2200i \u2208 {1, . . . , n} 7:\nend for 8:\n\u0398 t+1 =\u0398 M 9: end for 10: return\u03b8 T = 1 T 1 n T t=1 n i=1 \u03b8 t i\nWe thus replace the step (b \u2032 ) by running M steps of the subgradient method to obtain\u03b8 M i . Theorem 4. Under local regularity (A2), the approximation error with the iterative algorithm of Eq. (23) after T iterations and using M subgradient steps per iteration is bounded b\u0233\nf (\u03b8 T ) \u2212 min \u03b8\u2208Kf (\u03b8) \u2264 RL \u2113 \u03b3(W ) 1 T + 1 M .(25)\nProof. See Appendix D.\nTheorem 4 implies that the proposed algorithm achieves an error of at most \u03b5 in a time no larger than\nO RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL \u2113 \u03b5 1 \u03b3(W ) 2 . (26\n)\nWhile the first term (associated to communication) is optimal, the second does not match the lower bound of Theorem 3. This situation is similar to that of strongly-convex and smooth decentralized optimization [8], when the number of communication steps is taken equal to the number of overall iterations.\nBy using Chebyshev acceleration [24,25] with an increased number of communication steps, the algorithm reaches the optimal convergence rate. More precisely, since one communication step is a multiplication (of \u0398 e.g.) by the gossip matrix W , performing K communication steps is equivalent to multiplying by a power of W . More generally, multiplication by any polynomial P K (W ) of degree K can be achieved in K steps. Since our algorithm depends on the eigengap of the gossip matrix, a good choice of polynomial consists in maximizing this eigengap \u03b3(P K (W )). This is the approach followed by [8] and leads to the choice\nP K (x) = 1 \u2212 T K (c 2 (1 \u2212 x))/T K (c 2 )\n, where c 2 = (1 + \u03b3(W ))/(1 \u2212 \u03b3(W )) and T K are the Chebyshev polynomials [24] defined as T 0 (x) = 1, T 1 (x) = x, and, for all k \u2265 1, T k+1 (x) = 2xT k (x) \u2212 T k\u22121 (x). We refer the reader to [8] for more details on the method. Finally, as mentioned in [8], chosing K = \u230a1/ \u03b3(W )\u230b leads to an eigengap \u03b3(P K (W )) \u2265 1/4 and the optimal convergence rate.\nWe denote the resulting algorithm as multi-step primal-dual (MSPD) and describe it in Alg. 2. The procedure ACCELERATEDGOSSIP is extracted from [8, Algorithm 2] and performs one step of Chebyshev accelerated gossip, while steps 4 to 8 compute the approximation of the minimization problem (b') of Eq. (23). Our performance guarantee for the MSPD algorithm is then the following: Theorem 5. Under local regularity (A2), Alg. 2 achieves an approximation errorf (\u03b8\nT ) \u2212f (\u03b8 * ) of at most \u03b5 > 0 in a time T \u03b5 upper-bounded by O RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL \u2113 \u03b5 2 ,(27)\nwhich matches the lower complexity bound of Theorem 3. Alg. 2 is therefore optimal under the the local regularity assumption (A2).\nProof. See Appendix D.\nRemark 3. It is clear from the algorithm's description that it completes its T iterations by time\nT \u03b5 \u2264 4RL \u2113 \u03b5 \u03c4 \u03b3(W ) + 4RL \u2113 \u03b5 2 . (28\n)\nTo obtain the average of local parameters\u03b8 T = 1 nT T t=1 n i=1 \u03b8 i , one can then rely on the gossip algorithm [9] to average over the network the individual nodes' time averages. Let\nW \u2032 = I \u2212 c 3 P K (W ) where c 3 = (1 + c 2K 1 )/(1 \u2212 c K 1 ) 2 . Since W \u2032 is bi-stochastic, semi-definite positive and \u03bb 2 (W \u2032 ) = 1 \u2212 \u03b3(P K (W )) \u2264 3/4, using it for gossiping the time averages leads to a time O \u03c4 \u221a \u03b3 ln RL \u2113 \u03b5\nto ensure that each node reaches a precision \u03b5 on the objective function (see [9] for more details on the linear convergence of gossip), which is negligible compared to Eq. (27). Remark 4. A stochastic version of the algorithm is also possible by considering stochastic oracles on each f i and using stochastic subgradient descent instead of the subgradient method. Remark 5. In the more general context where node compute times \u03c1 i are not necessarily all equal to 1, we may still apply Alg. 2, where now the number of subgradient iterations performed by node i is M/\u03c1 i rather than M . The proof of Theorem 5 also applies, and now yields the modified upper bound on time to reach precision \u03b5:\nO RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL c \u03b5 2 ,(29)\nwhere\nL 2 c = 1 n n i=1 \u03c1 i L 2 i .", "publication_ref": ["b13", "b7", "b7", "b20", "b21", "b22", "b7", "b23", "b24", "b7", "b23", "b7", "b7", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we provide optimal convergence rates for non-smooth and convex distributed optimization in two settings: Lipschitz continuity of the global objective function, and Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide optimal convergence rates that depend on the \u2113 2 -average of the local Lipschitz constants and the (normalized) eigengap of the gossip matrix. Moreover, we also provide the first optimal decentralized algorithm, called multi-step primal-dual (MSPD).\nUnder the global regularity assumption, we provide a lower complexity bound that depends on the Lipschitz constant of the (global) objective function, as well as a distributed version of the smoothing approach of [10] and show that this algorithm is within a d 1/4 multiplicative factor of the optimal convergence rate.\nIn both settings, the optimal convergence rate exhibits two different speeds: a slow rate in \u0398(1/ \u221a t) with respect to local computations and a fast rate in \u0398(1/t) due to communication. Intuitively, communication is the limiting factor in the initial phase of optimization. However, its impact decreases with time and, for the final phase of optimization, local computation time is the main limiting factor.\nThe analysis presented in this paper allows several natural extensions, including time-varying communication networks, asynchronous algorithms, stochastic settings, and an analysis of unequal node compute speeds going beyond Remark 5. Moreover, despite the efficiency of DRS, finding an optimal algorithm under the global regularity assumption remains an open problem and would make a notable addition to this work.\nA Proof of the convergence rate of DRS (Theorem 1) Corollary 2.4 of [10] gives, with the appropriate choice of gradient step \u03b7 t and smoothing \u03b3 t ,\nE f (\u03b8 T ) \u2212 min \u03b8\u2208Kf (\u03b8) \u2264 10RL g d 1/4 T + 5RL g \u221a T K . (30\n)\nThus, to reach a precision \u03b5 > 0, we may set T = Let i 0 \u2208 V and i 1 \u2208 V be two nodes at distance \u2206. The function used by [18] to prove the oracle complexity for Lipschitz and bounded functions is g 1 (\u03b8) = \u03b4 max i\u2208{1,...,t}\n\u03b8 i + \u03b1 2 \u03b8 2 2 . (31\n)\nBy considering this function on a single node (e.g. i 0 ), at least O RL \u03b5 2 subgradients will be necessary to obtain a precision \u03b5. Moreover, we also split the difficult function used in [17] g\n2 (\u03b8) = \u03b3 t i=1 |\u03b8 i+1 \u2212 \u03b8 i | \u2212 \u03b2\u03b8 1 + \u03b1 2 \u03b8 2 2 ,(32)\non the two extremal nodes i 0 and i 1 in order to ensure that communication is necessary between the most distant nodes of the network. The final function that we consider is, for all i \u2208 {1, ..., n},\nf i (\u03b8) = \uf8f1 \uf8f2 \uf8f3 \u03b3 k i=1 |\u03b8 2i \u2212 \u03b8 2i\u22121 | + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 i if i = i 0 \u03b3 k i=1 |\u03b8 2i+1 \u2212 \u03b8 2i | \u2212 \u03b2\u03b8 1 + \u03b1 2 \u03b8 2 2 if i = i 1 0 otherwise ,(33)\nwhere \u03b3, \u03b4, \u03b2, \u03b1 > 0 and k, l \u2265 0 are parameters of the function satisfying 2k + l < d. The objective function is thus\nf (\u03b8) = 1 n \u03b3 2k i=1 |\u03b8 i+1 \u2212 \u03b8 i | \u2212 \u03b2\u03b8 1 + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 i + \u03b1 2 \u03b8 2 2 (34)\nFirst, note that reordering the coordinates of \u03b8 between \u03b8 2 and \u03b8 2k+1 in a decreasing order can only decrease the value functionf (\u03b8). Hence, the optimal value \u03b8 * verifies this constraint and\nf (\u03b8 * ) = 1 n \u2212\u03b3\u03b8 * 2k+1 \u2212 (\u03b2 \u2212 \u03b3)\u03b8 * 1 + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 * i + \u03b1 2 \u03b8 * 2 2 . (35\n)\nMoreover, at the optimum, all the coordinates between \u03b8 2 and \u03b8 2k+1 are equal, all the coordinates between \u03b8 2k+2 and \u03b8 2k+1+l are also equal, and all the coordinates after \u03b8 2k+1+l are zero. Hencef\n(\u03b8 * ) = 1 n \u2212\u03b3\u03b8 * 2k+1 \u2212 (\u03b2 \u2212 \u03b3)\u03b8 * 1 + \u03b4\u03b8 * 2k+2 + \u03b1 2 \u03b8 * 1 2 + 2k\u03b8 * 2k+1 2 + l\u03b8 * 2k+2 2 ,(36)\nand optimizing over \u03b8\n* 1 \u2265 \u03b8 * 2k+1 \u2265 0 \u2265 \u03b8 * 2k+2 leads to, when \u03b2 \u2265 \u03b3(1 + 1 2k ), f (\u03b8 * ) = \u22121 2\u03b1n (\u03b2 \u2212 \u03b3) 2 + \u03b3 2 2k + \u03b4 2 l . (37\n)\nNow note that, starting from \u03b8 0 = 0, each subgradient step can only increase the number of non-zero coordinates between \u03b8 2k+2 and \u03b8 2k+1+l by at most one. Thus, when t < l, we have\nmax i\u2208{2k+2,...,2k+1+l} \u03b8 t,i \u2265 0 . (38\n)\nMoreover, increasing the number of non-zero coordinates between \u03b8 1 and \u03b8 2k+1 requires at least one subgradient step and \u2206 communication steps. As a result, when t < min{l, 2k\u2206\u03c4 }, we have\n\u03b8 t,2k+1 = 0 andf (\u03b8 t ) \u2265 min \u03b8\u2208R d 1 n \u2212(\u03b2 \u2212 \u03b3)\u03b8 1 + \u03b1 2 \u03b8 2 2 \u2265 \u2212(\u03b2\u2212\u03b3) 2 2\u03b1n .(39)\nHence, we have, for t < min{l, 2k\u2206\u03c4 },\nf (\u03b8 t ) \u2212f (\u03b8 * ) \u2265 1 2\u03b1n \u03b3 2 2k + \u03b4 2 l .(40)\nOptimizingf over a ball of radius R \u2265 \u03b8 * 2 thus leads to the previous approximation error bound, and we choose\nR = \u03b8 * 2 = 1 \u03b1 2 (\u03b2 \u2212 \u03b3) 2 + \u03b3 2 2k + \u03b4 2 l .(41)\nFinally, the Lipschitz constant of the objective functionf is\nL g = 1 n \u03b2 + 2 \u221a 2k + 1\u03b3 + \u03b4 + \u03b1R ,(42)\nand setting the parameters off to\n\u03b2 = \u03b3(1 + 1 \u221a 2k ), \u03b4 = Lgn 9 , \u03b3 = Lgn 9 \u221a\nk , l = \u230at\u230b + 1, and k = t 2\u2206\u03c4 + 1 leads to t < min{l, 2k\u2206\u03c4 } and\nf (\u03b8 t ) \u2212f (\u03b8 * ) \u2265 RL g 36 1 (1 + t 2\u2206\u03c4 ) 2 + 1 1 + t ,(43)\nwhilef is L-Lipschitz and \u03b8 * 2 \u2264 R.", "publication_ref": ["b9", "b9", "b17", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "C Proof of the lower bound under local regularity (Theorem 3)", "text": "Following the idea introduced in [8], we prove Theorem 3 by applying Theorem 2 on linear graphs and splitting the local functions of Eq. (33) on multiple nodes to obtain L g \u2248 L \u2113 . Lemma 2. Let \u03b3 \u2208 (0, 1]. There exists a graph G \u03b3 of size n \u03b3 and a gossip matrix W \u03b3 \u2208 R n\u03b3 \u00d7n\u03b3 on this graph such that \u03b3(W \u03b3 ) = \u03b3 and\n\u03b3 \u2265 2 (n \u03b3 + 1) 2 . (44\n)\nWhen \u03b3 \u2265 1/3, G \u03b3 is a totally connected graph of size n \u03b3 = 3. Otherwise, G \u03b3 is a linear graph of size n \u03b3 \u2265 3.\nProof. First of all, when \u03b3 \u2265 1/3, we consider the totally connected network of 3 nodes, reweight only the edge (v 1 , v 3 ) by a \u2208 [0, 1], and let W a be its Laplacian matrix. If a = 1, then the network is totally connected and \u03b3(W a ) = 1. If, on the contrary, a = 0, then the network is a linear graph and \u03b3(W a ) = 1/3. Thus, by continuity of the eigenvalues of a matrix, there exists a value a \u2208 [0, 1] such that \u03b3(W a ) = \u03b3 and Eq. ( 44) is trivially verified. Otherwise, let x n = 1\u2212cos( \u03c0 n ) 1+cos( \u03c0 n ) be a decreasing sequence of positive numbers. Since x 3 = 1/3 and lim n x n = 0, there exists n \u03b3 \u2265 3 such that x n\u03b3 \u2265 \u03b3 > x n\u03b3 +1 . Let G \u03b3 be the linear graph of size n \u03b3 ordered from node v 1 to v n\u03b3 , and weighted with w i,i+1 = 1 \u2212 a1{i = 1}. If we take W a as the Laplacian of the weighted graph G \u03b3 , a simple calculation gives that, if a = 0, \u03b3(W a ) = x n\u03b3 and, if a = 1, the network is disconnected and \u03b3(W a ) = 0. Thus, there exists a value a \u2208 [0, 1] such that \u03b3(W a ) = \u03b3. Finally, by definition of n \u03b3 , one has\n\u03b3 > x n\u03b3 +1 \u2265 2 (n\u03b3 +1) 2 .\nLet \u03b3 \u2208 (0, 1] and G \u03b3 the graph of Lemma 2. We now consider I 0 = {1, ..., m} and I 1 = {n \u03b3 \u2212 m + 1, ..., n \u03b3 } where m = \u230a n\u03b3 +1 3 \u230b. When \u03b3 < 1/3, the distance d(I 0 , I 1 ) between the two sets I 0 and I 1 is thus bounded by\nd(I 0 , I 1 ) = n \u03b3 \u2212 2m + 1 \u2265 n \u03b3 + 1 3 ,(45)\nand we have\n1 \u221a \u03b3 \u2264 3d(I 0 , I 1 ) \u221a 2 .(46)\nMoreover, Eq. (46) also trivially holds when \u03b3 \u2265 1/3. We now consider the local functions of Eq. (33) splitted on I 0 and I 1 :\nf i (\u03b8) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 1 m \u03b3 k i=1 |\u03b8 2i \u2212 \u03b8 2i\u22121 | + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 i if i \u2208 I 0 1 m \u03b3 k i=1 |\u03b8 2i+1 \u2212 \u03b8 2i | \u2212 \u03b2\u03b8 1 + \u03b1 2 \u03b8 2 2 if i \u2208 I 1 0 otherwise . (47\n)\nThe average functionf remains unchanged and the time to communicate a vector between a node of I 0 and a node of I 1 is at least d(I 0 , I 1 )\u03c4 . Thus, the same result as Theorem 2 holds with \u2206 = d(I 0 , I 1 ). We thus hav\u0113\nf (\u03b8 i,t ) \u2212 min \u03b8\u2208B2(R)f (\u03b8) \u2265 RL g 36 1 (1 + t 2d(I0,I1)\u03c4 ) 2 + 1 1 + t .(48)\nFinally, the local Lipschitz constant L \u2113 is bounded by\nL \u2113 \u2264 n \u03b3 m L g \u2264 3L g ,(49)\nand Eq. (46), Eq. (48) and Eq. (49) lead to the desired result.\nD Proof of the convergence rate of MSPD (Theorem 4 and Theorem 5)\nTheorem 1 (b) in [21] implies that, provided \u03c4 \u03c3\u03bb 1 (W ) < 1, the algorithm with exact proximal step leads to a restricted primal-dual gap\nsup \u039b \u2032 F \u2264c 1 n n i=1 f i (\u03b8 i ) \u2212 tr \u039b \u2032\u22a4 \u0398A \u2212 inf \u0398 \u2032 \u2208K n 1 n n i=1 f i (\u03b8 \u2032 i ) \u2212 tr \u039b \u22a4 \u0398 \u2032 A of \u03b5 = 1 2t nR 2 \u03b7 + c 2 \u03c3 .\nThis implies that our candidate \u0398 is such that\n1 n n i=1 f i (\u03b8 i ) + c \u0398A F \u2264 inf \u0398 \u2032 \u2208K n 1 n n i=1 f i (\u03b8 \u2032 i ) + c \u0398 \u2032 A F + \u03b5 .\nLet \u03b8 be the average of all \u03b8 i . We have:\n1 n n i=1 f i (\u03b8) \u2264 1 n n i=1 f i (\u03b8 i ) + 1 n n i=1 L i \u03b8 i \u2212 \u03b8 \u2264 1 n n i=1 f i (\u03b8 i ) + 1 \u221a n 1 n n i=1 L 2 i \u2022 \u0398(I \u2212 11 \u22a4 /n) F \u2264 1 n n i=1 f i (\u03b8 i ) + 1 \u221a n 1 n n i=1 L 2 i \u03bb n\u22121 (W ) \u2022 \u0398A F .\nThus, if we take c = 1\n\u221a n 1 n n i=1 L 2 i\n\u03bbn\u22121(W ) , we obtain\n1 n n i=1 f i (\u03b8) \u2264 1 n n i=1 f i (\u03b8 * ) + \u03b5,\nand we thus obtain a \u03b5-minimizer of the original problem.\nWe have\n\u03b5 \u2264 1 2T nR 2 \u03b7 + 1 \u03bbn\u22121(W ) 1 n 2 n i=1 L 2 i \u03c3\nwith the constraint \u03c3\u03b7\u03bb 1 (W ) < 1. This leads to, with the choice \u03b7 = nR \u03bb n\u22121 (W )/\u03bb 1 (W )\nn i=1 L 2 i /n\nand taking \u03c3 to the limit \u03c3\u03b7\u03bb 1 (W ) = 1, to a convergence rate of\n\u03b5 = 1 T R 1 n n i=1 L 2 i \u03bb 1 (W ) \u03bb n\u22121 (W ) .\nSince we cannot use the exact proximal operator of f i , we instead approximate it. If we approximate (with the proper notion of gap [21, Eq. (11)]) each argmin \u03b8i\u2208K f i (\u03b8 i ) + n 2\u03b7 \u03b8 i \u2212 z 2 up to \u03b4 i , then the overall added gap is 1 n n i=1 \u03b4 i . If we do M steps of subgradient steps then the associated gap is \u03b4 i = L 2 i \u03b7 nM (standard result for strongly-convex subgradient [23]). Therefore the added gap is\n1 M R 1 n n i=1 L 2 i \u03bb 1 (W ) \u03bb n\u22121 (W ) .\nTherefore after T communication steps, i.e., communication time T \u03c4 plus M T subgradient evaluations, i.e., time M T , we get an error of\n1 T + 1 M RL \u2113 \u221a \u03b3 ,\nwhere \u03b3 = \u03b3(W ) = \u03bb n\u22121 (W )/\u03bb 1 (W ). Thus to reach \u03b5, it takes\n2RL \u2113 \u03b5 1 \u221a \u03b3 \u03c4 + 4RL \u2113 \u03b5 1 \u221a \u03b3 2 .\nThe second term is optimal, while the first term is not. We therefore do accelerated gossip instead of plain gossip. By performing K steps of gossip instead of one, with K = \u230a1/ \u221a \u03b3\u230b, the eigengap is lower bounded by \u03b3(P K (W )) \u2265 1/4, and the overall time to obtain an error below \u03b5 becomes 4RL \u2113 \u03b5 \u03c4 \u03b3(W ) + 4RL \u2113 \u03b5 2 , as announced.", "publication_ref": ["b20", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We acknowledge support from the European Research Council (grant SEQUOIA 724063).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Distributed subgradient methods for multi-agent optimization", "journal": "IEEE Transactions on Automatic Control", "year": "2009", "authors": "Angelia Nedic; Asuman Ozdaglar"}, {"ref_id": "b1", "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning", "journal": "", "year": "2011", "authors": "Stephen Boyd; Neal Parikh; Eric Chu; Borja Peleato; Jonathan Eckstein"}, {"ref_id": "b2", "title": "Dual averaging for distributed optimization: Convergence analysis and network scaling", "journal": "IEEE Transactions on Automatic control", "year": "2012", "authors": "John C Duchi; Alekh Agarwal; Martin J Wainwright"}, {"ref_id": "b3", "title": "EXTRA: An exact first-order algorithm for decentralized consensus optimization", "journal": "SIAM Journal on Optimization", "year": "2015", "authors": "Wei Shi; Qing Ling; Gang Wu; Wotao Yin"}, {"ref_id": "b4", "title": "On the linear convergence of the ADMM in decentralized consensus optimization", "journal": "IEEE Transactions on Signal Processing", "year": "2014", "authors": "Wei Shi; Qing Ling; Kun Yuan; Gang Wu; Wotao Yin"}, {"ref_id": "b5", "title": "Linear convergence rate of a class of distributed augmented lagrangian algorithms", "journal": "IEEE Transactions on Automatic Control", "year": "2015", "authors": "Du\u0161an Jakoveti\u0107; M F Jos\u00e9; Joao Moura;  Xavier"}, {"ref_id": "b6", "title": "Achieving geometric convergence for distributed optimization over time-varying graphs", "journal": "SIAM Journal on Optimization", "year": "2017", "authors": "Angelia Nedic; Alex Olshevsky; Wei Shi"}, {"ref_id": "b7", "title": "Optimal algorithms for smooth and strongly convex distributed optimization in networks", "journal": "", "year": "2017", "authors": "Kevin Scaman; Francis Bach; S\u00e9bastien Bubeck; Yin Tat Lee; Laurent Massouli\u00e9"}, {"ref_id": "b8", "title": "Randomized gossip algorithms", "journal": "IEEE/ACM Transactions on Networking (TON)", "year": "2006", "authors": "Stephen Boyd; Arpita Ghosh; Balaji Prabhakar; Devavrat Shah"}, {"ref_id": "b9", "title": "Randomized smoothing for stochastic optimization", "journal": "SIAM Journal on Optimization", "year": "2012", "authors": "John C Duchi; Peter L Bartlett; Martin J Wainwright"}, {"ref_id": "b10", "title": "Fast distributed gradient methods", "journal": "IEEE Transactions on Automatic Control", "year": "2014", "authors": "Du\u0161an Jakoveti\u0107; Joao Xavier; Jos\u00e9 M F Moura"}, {"ref_id": "b11", "title": "DSA: Decentralized double stochastic averaging gradient algorithm", "journal": "Journal of Machine Learning Research", "year": "2016", "authors": "Aryan Mokhtari; Alejandro Ribeiro"}, {"ref_id": "b12", "title": "Distributed alternating direction method of multipliers", "journal": "IEEE", "year": "2012", "authors": "Ermin Wei; Asuman Ozdaglar"}, {"ref_id": "b13", "title": "Communication-efficient algorithms for decentralized and stochastic optimization", "journal": "", "year": "2017", "authors": "Guanghui Lan; Soomin Lee; Yi Zhou"}, {"ref_id": "b14", "title": "Communication-efficient distributed dual coordinate ascent", "journal": "", "year": "2014", "authors": "Martin Jaggi; Virginia Smith; Martin Tak\u00e1c; Jonathan Terhorst; Sanjay Krishnan; Thomas Hofmann; Michael I Jordan "}, {"ref_id": "b15", "title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation", "journal": "", "year": "2014", "authors": "Ohad Shamir"}, {"ref_id": "b16", "title": "Communication complexity of distributed convex learning and optimization", "journal": "", "year": "2015", "authors": "Yossi Arjevani; Ohad Shamir"}, {"ref_id": "b17", "title": "Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning", "journal": "", "year": "2015", "authors": "S\u00e9bastien Bubeck"}, {"ref_id": "b18", "title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "journal": "", "year": "1965", "authors": "J J Moreau"}, {"ref_id": "b19", "title": "Introductory lectures on convex optimization : a basic course", "journal": "Kluwer Academic Publishers", "year": "2004", "authors": "Yurii Nesterov"}, {"ref_id": "b20", "title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "journal": "Journal of Mathematical Imaging and Vision", "year": "2011-05", "authors": "Antonin Chambolle; Thomas Pock"}, {"ref_id": "b21", "title": "Mirror prox algorithm for multi-term composite minimization and semi-separable problems", "journal": "Computational Optimization and Applications", "year": "2015", "authors": "Niao He; Anatoli Juditsky; Arkadi Nemirovski"}, {"ref_id": "b22", "title": "A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method", "journal": "", "year": "2012", "authors": "Simon Lacoste-Julien; Mark Schmidt; Francis Bach"}, {"ref_id": "b23", "title": "Iterative Solution of Large Linear Systems", "journal": "", "year": "2011", "authors": "W Auzinger"}, {"ref_id": "b24", "title": "Chebyshev acceleration of iterative refinement", "journal": "Numerical Algorithms", "year": "2014", "authors": "M Arioli; J Scott"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "20RLg d 11on the time T \u03b5 = T (2\u2206\u03c4 + K) to reach \u03b5. B Proof of the lower bound under global regularity (Theorem 2)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "min \u03b8\u2208Kf (\u03b8) = 1 n n i=1 f i (\u03b8) ,(1)", "formula_coordinates": [2.0, 254.76, 562.37, 249.32, 31.09]}, {"formula_id": "formula_1", "formula_text": "|f (\u03b8) \u2212f (\u03b8 \u2032 )| \u2264 L g \u03b8 \u2212 \u03b8 \u2032 2 .(2)", "formula_coordinates": [3.0, 255.36, 171.17, 248.72, 18.87]}, {"formula_id": "formula_2", "formula_text": "as L \u2113 = 1 n n i=1 L 2 i the \u2113 2 -average of the local Lipschitz constants.", "formula_coordinates": [3.0, 132.96, 212.33, 279.21, 15.37]}, {"formula_id": "formula_3", "formula_text": "K, i.e. \u2200\u03b8 \u2208 K, \u03b8 \u2212 \u03b8 0 2 \u2264 R ,(3)", "formula_coordinates": [3.0, 176.88, 336.32, 327.2, 29.17]}, {"formula_id": "formula_4", "formula_text": "M i,t \u2282 M comp i,t \u222a M comm i,t .(4)", "formula_coordinates": [3.0, 261.96, 482.69, 242.12, 19.6]}, {"formula_id": "formula_5", "formula_text": "M comp i,t = Span ({\u03b8, \u2207f i (\u03b8) : \u03b8 \u2208 M i,t\u22121 }) .(5)", "formula_coordinates": [3.0, 223.68, 538.49, 280.4, 19.59]}, {"formula_id": "formula_6", "formula_text": "M comm i,t = Span (j,i)\u2208E M j,t\u2212\u03c4 .(6)", "formula_coordinates": [3.0, 245.16, 604.37, 258.92, 23.41]}, {"formula_id": "formula_7", "formula_text": "\u03b8 i,t \u2208 M i,t .(7)", "formula_coordinates": [3.0, 293.16, 676.28, 210.92, 17.29]}, {"formula_id": "formula_8", "formula_text": "\u2207f (\u03b8) = 1 n n i=1 \u2207f i (\u03b8) ,(8)", "formula_coordinates": [4.0, 256.08, 256.01, 248.0, 31.21]}, {"formula_id": "formula_9", "formula_text": "O RL g \u03b5 2 (\u2206\u03c4 + 1) .(9)", "formula_coordinates": [4.0, 255.12, 375.53, 248.96, 23.56]}, {"formula_id": "formula_10", "formula_text": "f \u03b3 (\u03b8) = E [f (\u03b8 + \u03b3X)] ,(10)", "formula_coordinates": [4.0, 255.6, 605.57, 248.6, 11.8]}, {"formula_id": "formula_11", "formula_text": "Lemma 1 (Lemma E.3 of [10]). If \u03b3 > 0, then f \u03b3 is Lg \u03b3 -smooth and, for all \u03b8 \u2208 R d , f (\u03b8) \u2264 f \u03b3 (\u03b8) \u2264 f (\u03b8) + \u03b3L g \u221a d .(11)", "formula_coordinates": [4.0, 108.0, 650.21, 396.2, 43.72]}, {"formula_id": "formula_12", "formula_text": "Input: approximation error \u03b5 > 0, communication graph G, \u03b1 0 = 1, \u03b1 t+1 = 2/(1 + 1 + 4/\u03b1 2 t ) T = 20RLg d 1/4 \u03b5 , K = 5RLg d \u22121/4 \u03b5 , \u03b3 t = Rd \u22121/4 \u03b1 t , \u03b7 t = R\u03b1t 2Lg(d 1/4 + \u221a t+1 K )", "formula_coordinates": [5.0, 108.0, 125.09, 393.74, 33.84]}, {"formula_id": "formula_13", "formula_text": "4: x 0 = 0, z 0 = 0, G 0 = 0 5: for t = 0 to T \u2212 1 do 6: y t = (1 \u2212 \u03b1 t )x t + \u03b1 t z t 7:", "formula_coordinates": [5.0, 113.76, 206.13, 116.53, 44.97]}, {"formula_id": "formula_14", "formula_text": "Each node i computes g i = 1 K K k=1 \u2207f i (y t + \u03b3 t X t,k ), where X t,k \u223c N (0, I) 9: G t+1 = G t + 1 n\u03b1t i g i 10: z t+1 = argmin x\u2208K x + \u03b7 t+1 G t+1 2 2", "formula_coordinates": [5.0, 109.8, 251.09, 343.34, 39.97]}, {"formula_id": "formula_15", "formula_text": "T ) \u2212f (\u03b8 * ) of at most \u03b5 > 0 in a time T \u03b5 upper-bounded by O RL g \u03b5 (\u2206\u03c4 + 1)d 1/4 + RL g \u03b5 2 . (12", "formula_coordinates": [5.0, 108.0, 564.89, 397.11, 57.52]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 500.01, 606.23, 4.19, 9.03]}, {"formula_id": "formula_17", "formula_text": "T \u03b5 \u2264 40 RL g d 1/4 \u03b5 \u2206\u03c4 + 100 RL g d 1/4 \u03b5 RL g d \u22121/4 \u03b5 . (13", "formula_coordinates": [6.0, 176.04, 131.09, 323.97, 25.11]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [6.0, 500.01, 139.79, 4.19, 9.03]}, {"formula_id": "formula_19", "formula_text": "d \u2264 RL g \u03b5 4 .(14)", "formula_coordinates": [6.0, 276.48, 198.21, 227.72, 23.88]}, {"formula_id": "formula_20", "formula_text": "f (\u03b8 i,t ) \u2212 min \u03b8\u2208B2(R)f (\u03b8) \u2265 RL g 36 1 (1 + t 2\u2206\u03c4 ) 2 + 1 1 + t .(15)", "formula_coordinates": [6.0, 193.68, 483.69, 310.52, 26.49]}, {"formula_id": "formula_21", "formula_text": "\u2126 RL g \u03b5 \u2206\u03c4 + RL g \u03b5 2 ,(16)", "formula_coordinates": [6.0, 251.04, 603.41, 253.16, 23.56]}, {"formula_id": "formula_22", "formula_text": "\u03b8 t \u2208 Span i\u2208V M i,t .(17)", "formula_coordinates": [7.0, 257.4, 233.72, 246.8, 21.34]}, {"formula_id": "formula_24", "formula_text": "W ij = 0 only if i = j or (i, j) \u2208 E.", "formula_coordinates": [7.0, 303.24, 482.48, 141.81, 17.29]}, {"formula_id": "formula_25", "formula_text": "(\u03b8 i,t ) \u2212 min \u03b8\u2208B2(R)f (\u03b8) \u2265 RL \u2113 108 1 (1 + 2t \u221a \u03b3 \u03c4 ) 2 + 1 1 + t .(19)", "formula_coordinates": [7.0, 198.48, 636.33, 305.72, 28.65]}, {"formula_id": "formula_26", "formula_text": "\u2126 RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL \u2113 \u03b5 2 . (20", "formula_coordinates": [8.0, 239.64, 157.17, 260.37, 24.96]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [8.0, 500.01, 164.63, 4.19, 9.03]}, {"formula_id": "formula_28", "formula_text": "1 n n i=1 f i (\u03b8 i ) over \u0398 = (\u03b8 1 , . . . , \u03b8 n ) \u2208 K n with the constraint that \u03b8 1 = \u2022 \u2022 \u2022 = \u03b8 n , or equivalently \u0398A = 0.", "formula_coordinates": [8.0, 107.64, 376.37, 397.02, 24.76]}, {"formula_id": "formula_29", "formula_text": "min \u0398\u2208K n max \u039b\u2208R d\u00d7n 1 n n i=1 f i (\u03b8 i ) \u2212 tr \u039b \u22a4 \u0398A . (21", "formula_coordinates": [8.0, 226.56, 406.61, 273.45, 31.21]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [8.0, 500.01, 417.47, 4.19, 9.03]}, {"formula_id": "formula_31", "formula_text": "(a) \u039b t+1 = \u039b t \u2212 \u03c3(2\u0398 t+1 \u2212 \u0398 t )A (b) \u0398 t+1 = argmin \u0398\u2208K n 1 n n i=1 f i (\u03b8 i ) \u2212 tr \u0398 \u22a4 \u039b t+1 A \u22a4 + 1 2\u03b7 tr(\u0398 \u2212 \u0398 t ) \u22a4 (\u0398 \u2212 \u0398 t ) ,(22)", "formula_coordinates": [8.0, 125.28, 483.77, 378.92, 42.85]}, {"formula_id": "formula_32", "formula_text": "(a \u2032 ) Y t+1 = Y t \u2212 \u03c3(2\u0398 t+1 \u2212 \u0398 t )W (b \u2032 ) \u03b8 t+1 i = argmin \u03b8i\u2208K 1 n f i (\u03b8 i ) \u2212 \u03b8 \u22a4 i y t+1 i + 1 2\u03b7 \u03b8 i \u2212 \u03b8 t i 2 , \u2200i \u2208 {1, . . . , n} ,(23)", "formula_coordinates": [8.0, 136.92, 572.93, 367.28, 36.25]}, {"formula_id": "formula_33", "formula_text": "\u03b8 m+1 i = m m + 2\u03b8 m i \u2212 2 m + 2 \u03b7 n \u2207f i (\u03b8 m i ) \u2212 \u03b7y t+1 i \u2212 \u03b8 t i , m = 0, . . . , M \u2212 1.(24)", "formula_coordinates": [8.0, 145.08, 667.05, 359.12, 23.88]}, {"formula_id": "formula_34", "formula_text": "K = \u230a1/ \u03b3(W )\u230b, M = T = \u2308 4\u03b5 RL \u2113 \u2309, c 1 = 1\u2212 \u221a \u03b3(W ) 1+ \u221a \u03b3(W ) , \u03b7 = nR L \u2113 1\u2212c K 1 1+c K 1 , \u03c3 = 1+c 2K 1 \u03c4 (1\u2212c K 1 ) 2 . Output: optimizer\u03b8 T 1: \u0398 0 = 0, \u0398 \u22121 = 0, Y 0 = 0 2: for t = 0 to T \u2212 1 do 3: Y t+1 = Y t \u2212 \u03c3 ACCELERATEDGOSSIP(2\u0398 t \u2212 \u0398 t\u22121 , W , K) // see [8, Alg. 2] 4:\u0398 0 = \u0398 t 5: for m = 0 to M \u2212 1 do 6:\u03b8 m+1 i = m m+2\u03b8 m i \u2212 2 m+2 \u03b7 n \u2207f i (\u03b8 m i ) \u2212 \u03b7y t+1 i \u2212 \u03b8 t i , \u2200i \u2208 {1, . . . , n} 7:", "formula_coordinates": [9.0, 108.0, 128.36, 393.56, 123.58]}, {"formula_id": "formula_35", "formula_text": "\u0398 t+1 =\u0398 M 9: end for 10: return\u03b8 T = 1 T 1 n T t=1 n i=1 \u03b8 t i", "formula_coordinates": [9.0, 109.8, 253.25, 145.57, 37.93]}, {"formula_id": "formula_36", "formula_text": "f (\u03b8 T ) \u2212 min \u03b8\u2208Kf (\u03b8) \u2264 RL \u2113 \u03b3(W ) 1 T + 1 M .(25)", "formula_coordinates": [9.0, 219.36, 357.81, 284.84, 24.96]}, {"formula_id": "formula_37", "formula_text": "O RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL \u2113 \u03b5 1 \u03b3(W ) 2 . (26", "formula_coordinates": [9.0, 219.6, 439.25, 280.41, 27.88]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [9.0, 500.01, 449.63, 4.19, 9.03]}, {"formula_id": "formula_39", "formula_text": "P K (x) = 1 \u2212 T K (c 2 (1 \u2212 x))/T K (c 2 )", "formula_coordinates": [9.0, 313.32, 598.52, 159.41, 17.29]}, {"formula_id": "formula_40", "formula_text": "T ) \u2212f (\u03b8 * ) of at most \u03b5 > 0 in a time T \u03b5 upper-bounded by O RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL \u2113 \u03b5 2 ,(27)", "formula_coordinates": [10.0, 108.0, 145.01, 396.2, 58.6]}, {"formula_id": "formula_41", "formula_text": "T \u03b5 \u2264 4RL \u2113 \u03b5 \u03c4 \u03b3(W ) + 4RL \u2113 \u03b5 2 . (28", "formula_coordinates": [10.0, 219.36, 287.21, 280.65, 28.0]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [10.0, 500.01, 297.59, 4.19, 9.03]}, {"formula_id": "formula_43", "formula_text": "W \u2032 = I \u2212 c 3 P K (W ) where c 3 = (1 + c 2K 1 )/(1 \u2212 c K 1 ) 2 . Since W \u2032 is bi-stochastic, semi-definite positive and \u03bb 2 (W \u2032 ) = 1 \u2212 \u03b3(P K (W )) \u2264 3/4, using it for gossiping the time averages leads to a time O \u03c4 \u221a \u03b3 ln RL \u2113 \u03b5", "formula_coordinates": [10.0, 108.0, 339.17, 396.1, 50.5]}, {"formula_id": "formula_44", "formula_text": "O RL \u2113 \u03b5 \u03c4 \u03b3(W ) + RL c \u03b5 2 ,(29)", "formula_coordinates": [10.0, 229.2, 482.09, 275.0, 24.88]}, {"formula_id": "formula_45", "formula_text": "L 2 c = 1 n n i=1 \u03c1 i L 2 i .", "formula_coordinates": [10.0, 134.52, 516.29, 81.21, 15.49]}, {"formula_id": "formula_46", "formula_text": "E f (\u03b8 T ) \u2212 min \u03b8\u2208Kf (\u03b8) \u2264 10RL g d 1/4 T + 5RL g \u221a T K . (30", "formula_coordinates": [13.0, 208.56, 157.73, 291.45, 26.2]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [13.0, 500.01, 166.43, 4.19, 9.03]}, {"formula_id": "formula_48", "formula_text": "\u03b8 i + \u03b1 2 \u03b8 2 2 . (31", "formula_coordinates": [13.0, 319.68, 306.57, 180.33, 23.52]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [13.0, 500.01, 313.91, 4.19, 9.03]}, {"formula_id": "formula_50", "formula_text": "2 (\u03b8) = \u03b3 t i=1 |\u03b8 i+1 \u2212 \u03b8 i | \u2212 \u03b2\u03b8 1 + \u03b1 2 \u03b8 2 2 ,(32)", "formula_coordinates": [13.0, 224.28, 377.09, 279.92, 31.21]}, {"formula_id": "formula_51", "formula_text": "f i (\u03b8) = \uf8f1 \uf8f2 \uf8f3 \u03b3 k i=1 |\u03b8 2i \u2212 \u03b8 2i\u22121 | + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 i if i = i 0 \u03b3 k i=1 |\u03b8 2i+1 \u2212 \u03b8 2i | \u2212 \u03b2\u03b8 1 + \u03b1 2 \u03b8 2 2 if i = i 1 0 otherwise ,(33)", "formula_coordinates": [13.0, 149.4, 447.05, 354.8, 38.2]}, {"formula_id": "formula_52", "formula_text": "f (\u03b8) = 1 n \u03b3 2k i=1 |\u03b8 i+1 \u2212 \u03b8 i | \u2212 \u03b2\u03b8 1 + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 i + \u03b1 2 \u03b8 2 2 (34)", "formula_coordinates": [13.0, 160.68, 524.69, 343.52, 31.09]}, {"formula_id": "formula_53", "formula_text": "f (\u03b8 * ) = 1 n \u2212\u03b3\u03b8 * 2k+1 \u2212 (\u03b2 \u2212 \u03b3)\u03b8 * 1 + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 * i + \u03b1 2 \u03b8 * 2 2 . (35", "formula_coordinates": [13.0, 155.64, 595.41, 344.37, 23.88]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [13.0, 500.01, 602.87, 4.19, 9.03]}, {"formula_id": "formula_55", "formula_text": "(\u03b8 * ) = 1 n \u2212\u03b3\u03b8 * 2k+1 \u2212 (\u03b2 \u2212 \u03b3)\u03b8 * 1 + \u03b4\u03b8 * 2k+2 + \u03b1 2 \u03b8 * 1 2 + 2k\u03b8 * 2k+1 2 + l\u03b8 * 2k+2 2 ,(36)", "formula_coordinates": [13.0, 136.44, 667.89, 367.76, 23.88]}, {"formula_id": "formula_56", "formula_text": "* 1 \u2265 \u03b8 * 2k+1 \u2265 0 \u2265 \u03b8 * 2k+2 leads to, when \u03b2 \u2265 \u03b3(1 + 1 2k ), f (\u03b8 * ) = \u22121 2\u03b1n (\u03b2 \u2212 \u03b3) 2 + \u03b3 2 2k + \u03b4 2 l . (37", "formula_coordinates": [14.0, 195.12, 108.77, 393.29, 45.63]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [14.0, 500.01, 137.99, 4.19, 9.03]}, {"formula_id": "formula_58", "formula_text": "max i\u2208{2k+2,...,2k+1+l} \u03b8 t,i \u2265 0 . (38", "formula_coordinates": [14.0, 250.68, 193.28, 249.33, 17.29]}, {"formula_id": "formula_59", "formula_text": ")", "formula_coordinates": [14.0, 500.01, 194.15, 4.19, 9.03]}, {"formula_id": "formula_60", "formula_text": "\u03b8 t,2k+1 = 0 andf (\u03b8 t ) \u2265 min \u03b8\u2208R d 1 n \u2212(\u03b2 \u2212 \u03b3)\u03b8 1 + \u03b1 2 \u03b8 2 2 \u2265 \u2212(\u03b2\u2212\u03b3) 2 2\u03b1n .(39)", "formula_coordinates": [14.0, 108.0, 242.37, 396.2, 44.04]}, {"formula_id": "formula_61", "formula_text": "f (\u03b8 t ) \u2212f (\u03b8 * ) \u2265 1 2\u03b1n \u03b3 2 2k + \u03b4 2 l .(40)", "formula_coordinates": [14.0, 234.12, 303.41, 270.08, 25.24]}, {"formula_id": "formula_62", "formula_text": "R = \u03b8 * 2 = 1 \u03b1 2 (\u03b2 \u2212 \u03b3) 2 + \u03b3 2 2k + \u03b4 2 l .(41)", "formula_coordinates": [14.0, 219.36, 357.53, 284.84, 25.12]}, {"formula_id": "formula_63", "formula_text": "L g = 1 n \u03b2 + 2 \u221a 2k + 1\u03b3 + \u03b4 + \u03b1R ,(42)", "formula_coordinates": [14.0, 227.64, 403.76, 276.56, 25.57]}, {"formula_id": "formula_64", "formula_text": "\u03b2 = \u03b3(1 + 1 \u221a 2k ), \u03b4 = Lgn 9 , \u03b3 = Lgn 9 \u221a", "formula_coordinates": [14.0, 253.8, 434.81, 167.88, 16.69]}, {"formula_id": "formula_65", "formula_text": "f (\u03b8 t ) \u2212f (\u03b8 * ) \u2265 RL g 36 1 (1 + t 2\u2206\u03c4 ) 2 + 1 1 + t ,(43)", "formula_coordinates": [14.0, 211.44, 475.17, 292.76, 26.49]}, {"formula_id": "formula_66", "formula_text": "\u03b3 \u2265 2 (n \u03b3 + 1) 2 . (44", "formula_coordinates": [14.0, 273.72, 632.25, 226.29, 24.21]}, {"formula_id": "formula_67", "formula_text": ")", "formula_coordinates": [14.0, 500.01, 639.59, 4.19, 9.03]}, {"formula_id": "formula_68", "formula_text": "\u03b3 > x n\u03b3 +1 \u2265 2 (n\u03b3 +1) 2 .", "formula_coordinates": [15.0, 140.64, 232.97, 92.61, 18.75]}, {"formula_id": "formula_69", "formula_text": "d(I 0 , I 1 ) = n \u03b3 \u2212 2m + 1 \u2265 n \u03b3 + 1 3 ,(45)", "formula_coordinates": [15.0, 231.36, 295.41, 272.84, 23.76]}, {"formula_id": "formula_70", "formula_text": "1 \u221a \u03b3 \u2264 3d(I 0 , I 1 ) \u221a 2 .(46)", "formula_coordinates": [15.0, 269.76, 331.29, 234.44, 24.6]}, {"formula_id": "formula_71", "formula_text": "f i (\u03b8) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 1 m \u03b3 k i=1 |\u03b8 2i \u2212 \u03b8 2i\u22121 | + \u03b4 max i\u2208{2k+2,...,2k+1+l} \u03b8 i if i \u2208 I 0 1 m \u03b3 k i=1 |\u03b8 2i+1 \u2212 \u03b8 2i | \u2212 \u03b2\u03b8 1 + \u03b1 2 \u03b8 2 2 if i \u2208 I 1 0 otherwise . (47", "formula_coordinates": [15.0, 130.8, 394.76, 369.21, 47.17]}, {"formula_id": "formula_72", "formula_text": ")", "formula_coordinates": [15.0, 500.01, 414.71, 4.19, 9.03]}, {"formula_id": "formula_73", "formula_text": "f (\u03b8 i,t ) \u2212 min \u03b8\u2208B2(R)f (\u03b8) \u2265 RL g 36 1 (1 + t 2d(I0,I1)\u03c4 ) 2 + 1 1 + t .(48)", "formula_coordinates": [15.0, 183.12, 502.05, 321.08, 26.61]}, {"formula_id": "formula_74", "formula_text": "L \u2113 \u2264 n \u03b3 m L g \u2264 3L g ,(49)", "formula_coordinates": [15.0, 260.4, 561.93, 243.8, 23.76]}, {"formula_id": "formula_75", "formula_text": "sup \u039b \u2032 F \u2264c 1 n n i=1 f i (\u03b8 i ) \u2212 tr \u039b \u2032\u22a4 \u0398A \u2212 inf \u0398 \u2032 \u2208K n 1 n n i=1 f i (\u03b8 \u2032 i ) \u2212 tr \u039b \u22a4 \u0398 \u2032 A of \u03b5 = 1 2t nR 2 \u03b7 + c 2 \u03c3 .", "formula_coordinates": [16.0, 108.0, 185.57, 343.95, 70.96]}, {"formula_id": "formula_76", "formula_text": "1 n n i=1 f i (\u03b8 i ) + c \u0398A F \u2264 inf \u0398 \u2032 \u2208K n 1 n n i=1 f i (\u03b8 \u2032 i ) + c \u0398 \u2032 A F + \u03b5 .", "formula_coordinates": [16.0, 171.0, 277.85, 271.2, 31.09]}, {"formula_id": "formula_77", "formula_text": "1 n n i=1 f i (\u03b8) \u2264 1 n n i=1 f i (\u03b8 i ) + 1 n n i=1 L i \u03b8 i \u2212 \u03b8 \u2264 1 n n i=1 f i (\u03b8 i ) + 1 \u221a n 1 n n i=1 L 2 i \u2022 \u0398(I \u2212 11 \u22a4 /n) F \u2264 1 n n i=1 f i (\u03b8 i ) + 1 \u221a n 1 n n i=1 L 2 i \u03bb n\u22121 (W ) \u2022 \u0398A F .", "formula_coordinates": [16.0, 163.8, 339.17, 284.05, 107.65]}, {"formula_id": "formula_78", "formula_text": "\u221a n 1 n n i=1 L 2 i", "formula_coordinates": [16.0, 193.08, 455.5, 62.31, 17.48]}, {"formula_id": "formula_79", "formula_text": "1 n n i=1 f i (\u03b8) \u2264 1 n n i=1 f i (\u03b8 * ) + \u03b5,", "formula_coordinates": [16.0, 241.68, 480.29, 129.96, 31.09]}, {"formula_id": "formula_80", "formula_text": "\u03b5 \u2264 1 2T nR 2 \u03b7 + 1 \u03bbn\u22121(W ) 1 n 2 n i=1 L 2 i \u03c3", "formula_coordinates": [16.0, 223.68, 543.17, 157.09, 28.95]}, {"formula_id": "formula_81", "formula_text": "n i=1 L 2 i /n", "formula_coordinates": [16.0, 313.68, 610.49, 37.42, 15.01]}, {"formula_id": "formula_82", "formula_text": "\u03b5 = 1 T R 1 n n i=1 L 2 i \u03bb 1 (W ) \u03bb n\u22121 (W ) .", "formula_coordinates": [16.0, 237.0, 656.21, 138.0, 31.09]}, {"formula_id": "formula_83", "formula_text": "1 M R 1 n n i=1 L 2 i \u03bb 1 (W ) \u03bb n\u22121 (W ) .", "formula_coordinates": [17.0, 245.28, 177.65, 122.52, 31.21]}, {"formula_id": "formula_84", "formula_text": "1 T + 1 M RL \u2113 \u221a \u03b3 ,", "formula_coordinates": [17.0, 278.04, 252.69, 63.12, 24.36]}, {"formula_id": "formula_85", "formula_text": "2RL \u2113 \u03b5 1 \u221a \u03b3 \u03c4 + 4RL \u2113 \u03b5 1 \u221a \u03b3 2 .", "formula_coordinates": [17.0, 245.28, 308.21, 128.52, 27.4]}], "doi": ""}