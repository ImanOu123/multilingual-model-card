{"title": "Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models", "authors": "Erik Arakelyan; Zhaoqi Liu; Isabelle Augenstein", "pub_date": "", "abstract": "Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text generation, with the explicit condition that the NLI model predicts the relationship between the original and adversarial inputs as a symmetric equivalence entailment. We systematically study the effects of the phenomenon across NLI models for in-and outof domain settings. Our experiments show that semantic sensitivity causes performance degradations of 12.92% and 23.71% average over in-and out-of-domain settings, respectively. We further perform ablation studies, analysing this phenomenon across models, datasets, and variations in inference and show that semantic sensitivity can lead to major inconsistency within model predictions. Adversarial semantics-preserving generation NLI Dataset (correctly answered pairs) Premise: p Hypothesis: h Semantic Sensitivity NLI Model: M Predict M(p,h') M(p, h') = M(p, h) Predict M(p,h) Semantically Robust Yes Random Guess No \"There were beads of perspiration on his brow\" \"Sweat built up upon his face.\" Conditional Generation Generated Hypothesis: h' Instruction Prompting Generative Model: G NLI Model: M Save Predict M(h,h') and M(h',h) Yes No M(h,h') = M(h',h) = entailment Refine \"The sweat had built up on his face\"", "sections": [{"heading": "Introduction", "text": "Transformer-based (Vaswani et al., 2017) Language Models (LMs) have shown solid performance across various NLU tasks (Wang et al., 2018, \u2020 Equal contribution, alphabetical order. * Senior author. 2019). These advances have led to suggestions regarding the emergent capabilities of the models in terms of syntactic (Sinha et al., 2020;Hewitt and Manning, 2019;Jawahar et al., 2019;Warstadt and Bowman, 2020), logic (Wei et al., 2022a,b) and semantic (Kojima et al., 2022;Dasgupta et al., 2022) understanding. However, we present novel evidence that indicates that these models are prone to inconsistent predictions induced by inherent susceptibility towards semantic sensitivities.\nTo probe the models for these discrepancies, we formalise semantic comprehension as the ability to distinguish logical relations within sentences through identifying compositional semantics (Jacobson, 2014;Carnap, 1959). This means that negligible semantic variations should not impact the inherent relations implied between the texts, e.g. \"There were beads of perspiration on his brow.\" entails both \"Sweat built up upon his face.\" and the slight variation \"The sweat had built up on his face.\" Authentic comprehension of semantics does allow for such understanding through discovering semantic structures and the inherent relations induced by them (Cicourel, 1991;Schiffer, 1986;Rommers et al., 2013). This means that analysing the emergent semantic understanding within a model should minimally involve testing for sensitivity towards semantics-preserving surface-form variations.\nWe particularly focus on the task of textual entailment (Dagan et al., 2005), otherwise referred to as Natural Language Inference (Bowman et al., 2015, NLI), which has been widely used to probe how well the models understand language (Condoravdi et al., 2003;Williams et al., 2017;Nie et al., 2019). This is a pairwise input task, where given a premise p and a hypothesis h, the objective is to predict if the premise entails, contradicts or is neutral towards the hypothesis.\nWe propose a framework for testing semantic sensitivity within transformer-based models trained for NLI, by creating semantics-preserving surface-Figure 1: The proposed framework is comprised of two components. (i) a module for generating semanticspreserving surface-form hypothesis variations and (ii) using the generated surface for measuring semantic sensitivity and predictive inconsistency.\nform variations of the hypothesis (see Figure 1). These variations are created using conditional generation with Large Language Models (LLMs). We show that proposed candidates do not alter the core meaning or the truth value compared to the original statement. The original and generated sentences maintain denotative equivalence, where two sentences or phrases might be interpreted as having the same truth value or factual content but may carry minor variations of nuances or connotations. To ensure that the relations are preserved within the candidates during conditional generation, we assert that the NLI model predicts the original and generated hypothesis to symmetrically entail each other. This indicates that the model perceives both the generated and original hypothesis as equivalent. After introducing these variations, we evaluate the NLI model by replacing the original hypothesis with the generated candidates. As the candidates are indicated to be equivalent by the same NLI model, this evaluation will indicate whether the model can recover the existent relation between the premise hypothesis pair in the presence of minor semantic-preserving noise. We use the samples where the model identifies the existing relation correctly from the original premise hypothesis pair. This ensures that assessing for semantic sensitivity would not be hindered by the discrepancies in model performance.\nWe systematically study the semantic sensitivity across transformers that achieve state-of-the-art or similar results when trained on NLI datasets, namely RoBERTa (Liu et al., 2019b), BART (Lewis et al., 2019), DeBERTa (He et al., 2020) and Dis-tilBart (Sanh et al., 2019;Lewis et al., 2019) with different parametrizations. To measure the effect of the phenomenon on the inconsistency of the predictions, we use three popular English datasets -MultiNLI (Williams et al., 2017, MNLI), SNLI (Bowman et al., 2015) and ANLI (Nie et al., 2019). The models are fine-tuned using MNLI, which we choose for in-domain testing, as it covers a wide range of topics and is frequently used for zeroshot and few-shot textual classification (Yin et al., 2019). We use the same models for out-of-domain evaluation across the other NLI datasets.\nOur contributions are as follows: (i) we propose a novel framework for assessing semantic sensitivity within transformer-based language models (ii) we systematically study the influence of this phenomenon on inconsistent predictions across various transformer variants (iii) we show that the effect is persistent and pronounced across both in-and out-of-domain evaluations (iv) we further complete ablations to assess the severity of the inconsistent predictions caused by semantic sensitivity.", "publication_ref": ["b52", "b56", "b49", "b23", "b26", "b58", "b31", "b16", "b25", "b7", "b10", "b48", "b45", "b14", "b13", "b63", "b41", "b36", "b33", "b22", "b47", "b33", "b41", "b66"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Semantic comprehension is considered a fundamental building block for language understanding (Allen, 1995). Although attempts have been made to probe language models in terms of compositional semantic capabilities, the conclusions regarding their emergence remain to be discussed.\nModels appear to understand semantics Recently a wide suite of tasks has been proposed for testing models for language understanding Zellers et al., 2018;Ribeiro et al., 2020) with the credence that a model with strong performance should be able to utilise semantic relations when completing the tasks. In light of these, it has been shown that transformer-based language models can be directly trained (Zhang et al., 2020;Rosset et al., 2020) to utilise semantic structure to gain distributional information within the task. Specifically, NLI models have also been shown to be capable of pragmatic inferences (Jeretic et al., 2020a) with a perception of implicature (Grice, 1975) and presupposition (Stalnaker et al., 1977;Grice, 1975).\nModels struggle with semantics Directly probing for a specific aspect of semantic understanding has shown that transformer-based language models tend to struggle with semantics (Belinkov, 2022). It has been indicated that pretraining the language models does not exploit semantic information for entity labeling and coreference resolution (Liu et al., 2019a). Furthermore, transformer attention heads only minimally capture semantic relations (Kovaleva et al., 2019) from FrameNet (Baker et al., 1998). Studies have also shown that NLI models, in particular, tend to struggle with lexical variations, including word replacements (Glockner et al., 2018;Ivan Sanchez Carmona et al., 2018;Geiger et al., 2020), and sequence permutations (Sinha et al., 2021).\nSensitivity in NLI models Probing NLI models for language understanding has been a hallmark testing ground for measuring their emerging capabilities (Naik et al., 2018a;Wang and Jiang, 2015;Williams et al., 2017). A wide range of tests indicates that models trained for NLI are prone to struggling with syntax and linguistic phenomena (Dasgupta et al., 2018;Naik et al., 2018b;Ravichander et al., 2019;Jeretic et al., 2020b). It has also been shown that NLI models heavily rely on lexical overlaps (Ivan Sanchez Carmona et al., 2018;McCoy et al., 2019;Naik et al., 2018b) and are susceptible to over-attending to particular words for prediction (Gururangan et al., 2018;Clark et al., 2019). Our line of work is associated with evaluating NLI models for monotonicity reasoning (Yanaka et al., 2019) and sensitivity towards specific semantic phenomenon , such as boolean coordination, quantification, etc. However, we systematically test NLI models for their compositional semantic abilities and measuring the degree of inconsistence of their predictions influenced by the phenomenon.", "publication_ref": ["b0", "b67", "b43", "b68", "b20", "b51", "b20", "b4", "b35", "b32", "b19", "b24", "b18", "b50", "b39", "b57", "b63", "b15", "b40", "b42", "b28", "b24", "b37", "b40", "b21", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "We aim to create a framework for assessing semantic sensitivity within NLI models and measure its impact on the inconsistence of model predictions. The first part of the pipeline we propose is an adversarial semantics-preserving generation for introducing variations within the original samples. The second part of the pipeline involves assessment using the acquired generations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantics Preserving Surface-Form Variations", "text": "We formalise NLI as a pairwise input classification task. Given a dataset of premise hypothesis pairs D = (p 1 , h 1 ), . . . (p n , h n ), where \u2200p i \u2208 P & h i \u2208 H are a set of textual tokens P, H \u2286 T , the goal is to classify the pairs as entailment, contradiction or neutrality, i.e. C = {E, C, N }. We are also given a pre-trained language model (PLM) M that is trained for textual entailment. Before introducing semantic variations, only the samples where model M predicted the label correctly are filtered, i.e.\nD correct = {\u2200(p i , h i ) \u2208 D : M(p i , h i ) =\u0177 = y}\n, where\u0177 is the prediction and y is the original label. This is completed to ensure that the evaluation of semantic sensitivity is not hindered or inflated by the predictive performance and confidence of the model M. This type of filtering is used when probing for emergent syntactic (Sinha et al., 2021), lexical (Jeretic et al., 2020b), and numerical (Wallace et al., 2019) reasoning capabilities. We can see the original accuracy of NLI models and the number of samples used in the study in Table 1.\nTo introduce semantics preserving noise within chosen samples, we complete a two-fold refinement process. We utilise a generative LLM G, which has bart-l roberta-l distilbart deberta-b deberta-l deberta-xl  been fine-tuned on natural language instructions (Wei et al., 2021;Chung et al., 2022), and prompt it to paraphrase the original hypothesis h i , with the following prompt: Rephrase the following sentence while preserving its original meaning: <h i >. This is not sufficient to produce semantics-preserving variations as generative models are prone to hallucinations (Ji et al., 2023) and not assured to produce an equivalent paraphrase. To ensure that the generation h \u2032 i is logically equivalent to the original sample and thus semantics-preserving, we impose the condition that the NLI model should infer the relation between the original and generated hypothesis as a symmetric entailment:\nM(h i , h \u2032 i ) =\u0177 C=E = M(h \u2032 i , h) (1)\nThe bidirectional nature of entailment allows us to claim that sentences are logically equivalent (Angell, 1989;Clark, 1967). We refine the proposed variation candidates using the generator G until k candidates that satisfy the condition are produced.", "publication_ref": ["b50", "b28", "b54", "b59", "b9", "b29", "b2", "b12"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Human Evaluation of Surface-Form Variations", "text": "To further ensure the validity of this variation generation method, we conduct a human evaluation of the generated samples. We randomly sample 100 examples of generated and original hypothesis pairs across all datasets and employ two annotators to assess whether the sentences are semantically and logically equivalent within the pair. Our results show that in 99% of the cases, the annotators marked the samples as equivalent with an interannotator agreement measure of Cohen's \u03ba = 0.94. This further shows the reliability of the method for generating semantics-preserving surface form variations. We provide further token overlap level analysis in Appendix A.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluating Semantic Sensitivity", "text": "After obtaining k semantic variations for each hypothesis, we test the semantic sensitivity of the model by replacing the original hypothesis h i with the candidates {h \u20321 i , . . . h \u2032k i } and making a prediction with the NLI model M. As the proposed variations are logically equivalent to the original, we want to test if the new model prediction would vary compared to the original.\nR(p i , h i , h \u2032j i , O) = = 1, O(M(p i , h i ), M(p i , h \u2032j i )) = 0 0, O(M(p i , h i ), M(p i , h \u2032j i )) = 1\n(2)\nHere O : C \u00d7 C \u2192 {0, 1} is a boolean matching operator between the labels predicted with original hypothesis h i and the surface-form variations h \u2032j i . A change in the label would imply that the model is semantically sensitive and the original correct prediction is inconsistent with the label produced for the semantics preserving surface-form variation. A graphical representation can be seen in Figure 5. We use two metrics to measure semantic sensitivity within NLI models, both of which are derivative formulations of a Fooling Rate (Moosavi-Dezfooli et al., 2017), which is used for assessing the success of adversarial attacks (Chakraborty et al., 2018). Given k possible surface-form variations for the hypothesis, we test if at least one of the candidates would be able to cause a label change compared to the original prediction, which can be formalised as:\nr r = n \u2032 i 1 \u2203j \u2208 [1, k], R(p i , h i , h \u2032j i , =) \u0338 = 1 n \u2032 .\n(3) Table 2: The strict and relaxed fooling rates of different transformer models across in-domain (MNLI) and outof-domain (SNLI, ANLI) evaluations. On average more than half of the labels change towards their logically contrasting counterpart.\nHere n \u2032 is the number of correctly answered original samples, and the matching operator O is a simple equality checking operator \"=\". We refer to this metric as a relaxed Fooling Rate. To measure more drastic label changes, i.e. entailment to contradiction and vice versa, we also define a stricter version of Equation 3.\nr s = n \u2032 i 1 \u2203j \u2208 [1, k], R(p i , h i , h \u2032j i , = s ) \u0338 = 1 n \u2032 . (4)\nWe replace standard equality for the operator O in Equation 3 with a strict counterpart that matches only if the predictions are direct opposites, i.e. entailment \u2194 contradiction. It must be noted that the neutral class does not have a direct opposite; thus, the metric for this label remains unchanged. It can be concluded that the inequality r s \u2264 r r \u2264 1 trivially holds when using these metrics.", "publication_ref": ["b8"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experimental Setup", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Details", "text": "Semantics preserving Generation To generate and refine semantic variations of the original hypothesis, we chose flan-t5-xl as the generation model G. It is an instruction-tuned LLM that has shown close state-of-the-art performance in tasks such as paraphrasing, zero and few shot generation, chain of thought reasoning (CoT), and multi-task language understanding (Chung et al., 2022). For each of the selected hypotheses, we produce k = 5 unique semantics-preserving variations. To ensure diversity and consistency of the generated text while avoiding computationally expensive exhaustive search, we use a group beam search (Vijayakumar et al., 2016) with a temperature t \u2208 [0.3, 0.6] and a maximum output of 40 tokens throughout the generation and refinement procedure. We also further diversify the generation by using the recipe from Li et al. (2016).\nNLI models We systematically experiment with transformer architectures that are fine-tuned on MNLI, which exhibit state-of-the-art or close predictive accuracy on the dataset. We specifically choose bart-large (Lewis et al., 2019), robertalarge (Liu et al., 2019b), deberta-base, debertalarge, deberta-xlarge (He et al., 2020) and distilbart (Sanh et al., 2019). These PLMs are taken without change from their original studies through the Transformers library (Wolf et al., 2020), ensuring the complete reproducibility of the results. To observe the effect in an out-of-domain setup, we also evaluate these models on SNLI and ANLI in a zero-shot transfer setting.", "publication_ref": ["b9", "b53", "b34", "b33", "b36", "b22", "b47", "b64"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Analysis", "text": "This section presents the results and analyses of our semantic sensitivity evaluation framework along with a suite of ablations analysing the phenomenon across various transformer sizes, domains, and label space. Furthermore, we measure the impact of the phenomenon on the inconsistent predictive behaviour of NLI models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Sensitivity", "text": "In-domain We evaluate several PLMs trained on MNLI using our experiments presented in Table 2. The results show that models are limited in their comprehension of compositional semantics as the relaxed fooling rate on in-domain experimentation averages at r r = 12.9%. This is further reinforced by the fact that more than half, r s = 6.58% of the label changes occur with strict inequality. This means that minor semantics-preserving changes lead to a sizable shift in model predictions, even prompting towards the opposite decision edge half the time. The behaviour is consistent across all the transformers and leads us to believe that samples that changed labels after surface-form variations showcase the inconsistent predictive nature of the models. We further elaborate on this in the next section. Consequently, semantically equivalent variations evidently hinder the decision-making of the NLI models, prompting us to believe that models have limited understanding w.r.t. semantic structure and logical relation, even when the model is trained on texts from the same distribution.\nOut-of-domain We also probe the NLI models in an out-of-domain zero-shot setting to assess the transferability of compositional semantic knowledge. Our results in Table 2 show that the discrepancies and limitations in semantic comprehension are even more pronounced in this setting. We see an averaged relaxed fooling rate of r r = 23.7%, with the maximum at 57.49%, which is only marginally better than a majority voting baseline. It must be noted that because different datasets have varying numbers of samples, the average is weighted w.r.t. the number of sampled instances from the particular dataset in the experiment. The results on outof-domain evaluation once again follow the pattern that more than half, r s = 15.8% of the samples switch the labels to their logically contrasting counterparts. This shows that zero-shot transfer further amplifies the limitations that NLI models have for using semantic structures and preserving logical relations. This further suggests that the semantic variations where a label change occurs are likely to be originally predicted correctly as an inconsistent guess. It follows, that although PLMs fine-tuned on MNLI are widely used for zero-shot classification, their effectiveness diminishes if the classification tasks require syntactic understanding. Indeed, model effectiveness declines and the fooling rates rise as the tasks become more challenging, requiring greater syntactic knowledge, as we can see from the comparison of the results from SNLI to ANLI.\nEffects of distillation Next, we want to probe if the susceptibility towards semantic noise is transferred during model distillation. Thus, we use Dis-tilBart that is distilled from a larger pre-trained BART model. While model accuracy remains comparable to the original model in Table 1, the distilled version struggles sizeably more with surfaceform variations. On average, across in-and out-ofdomain evaluation, the distilled NLI model is more sensitive than the original in terms of relaxed fooling rate by \u25b3r r = 18.4%. The effect of supposed inconsistence is amplified when observing the strict fooling rate, where on average rr rs \u2264 1.5. This indicates that during distillation, models are bound to forget the knowledge regarding compositional semantics making it harder to preserve the logical equivalence during inference.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Effects of model size", "text": "We also test how semantics-preserving noise affects models of different sizes and parametrization (see Figure 2). Although for in-domain setup, the relaxed fooling rate metrics marginally drop as the models get bigger, the same cannot be observed in out-of-domain setup. It is evident that bigger PLMs from our study are almost as restricted in semantic comprehension as their smaller counterparts. This indicates that emergent semantic capabilities are not only tied to model size, but also widely depend upon the choice of the training dataset.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Severity of Inconsistent Predictions", "text": "Consistency across label space To analyse the extent of semantic sensitivities within NLI models we test the effect across all the classes in the label spaces, presented in Table 3. The per-class breakdown of the strict and relaxed fooling rate indicates that the effect is consistent across the whole label space. This allows us to conclude that the observed limitations in compositional semantic understanding are not caused by class imbalances and are not specific to a particular set of examples. We see the increased fooling rate across all of the labels when comparing in-domain and out-of-domain experiments. This reinforces the prior indications regarding models' inability to use semantic structure to preserve inherent relations within the data, as all logical relations attain rather similar amounts of fooling rate during direct evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distribution shift in decision making", "text": "Recall that we want to measure the impact of semanticspreserving surface-form variations on NLI models. We study the predictive distributional shift within the samples that cause a changed model prediction. To do this, we initially split the samples into two categories considering whether the sample induced a change of the original prediction within the NLI model. We further average the probability distribution of labels obtained from the final softmax layer of the model for these two categories. We measure the differences between the two distributions with two statistical tests. To evaluate    3: Fooling rate averaged over all models. r s represents the strict fooling rate, in which case the predicted label of the evaluation pair is opposite to the original label y. r r measures the proportion of label change. y \u2208 {E, N, C} group the (p, h) pairs by their semantic relation, representing entailment, neutrality, and contradiction, respectively. the relative entropy between them, we use Jensen-Shanon Divergence (Fuglede and Topsoe, 2004), a symmetric, non-negative, and bounded metric for assessing the similarity between two distributions, JSD(P \u2225Q) = 1 2 D(P \u2225M )+ 1 2 D(Q\u2225M ), where D is the Kullback-Leibler divergence (Joyce, 2011). We verify the statistical significance of our findings with the Kolmogorov-Smirnov test (Berger and Zhou, 2014), which shows if the two sets of samples are likely to come from the same distribution.\nOur results in Figure 3 show a significant distribution shift when assessing semantics-preserving surface-form variations. The cosine distance in the sentence embedding space between the generated and original samples is negligible at 0.04. As the absolute cosine similarity values possess limited interpretable meaning, we further explore the distributions of cosine distances towards original samples for the examples that do and do not induce label changes. We measure the Jansen-Shannon divergence of these two distributions at 0.001, implying they are strongly similar. This reinforces the hypothesis that surface-form variations produce logically equivalent samples with minor distance in the embedding space regardless of the induced label changes. However, despite minor changes in the semantic composition, we see a sizable change in the final predictive distribution of the NLI models. We see a significant rise both in Jensen-Shannon divergence and Kalmogorov-Smirnov metric, \u25b3JSD = 0.51 and \u25b3K-S = 0.54, when comparing the examples where the model prediction has changed compared to the original. This indicates that the generated variations do not cause negligible change within model prediction, but rather can be considered adversarial for the model. It shows that the limited capabilities to utilise syntactic information cause the model to significantly change the final prediction given minuscule variations, which is an to inconsistent predictive behaviour. Given that we initially sampled examples that the models answered correctly, these results assert our belief that the models do not display consistent predictive behaviour despite having equivalent inputs. This shows that albeit the strong model performance presented in Table 1, there is masked degeneration and discrepancies within the NLI models stemming from semantic sensitivity. Our method allows for explicitly quantifying the degree of semantic sensitivity within PLMs and allows to measure the impact of that sensitivity on the decision-making process of the model.", "publication_ref": ["b17", "b5"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Semantic-Sensitivity and decision variations", "text": "We lastly analyse the standard deviation within the predicted label distribution produced from the softmax of the model. We compute the standard deviation for the distribution of original premise hypothesis predictions and compare it with a replace-ment that does not and does cause label changes in PLM classification, see Figure 4. For reference, the upper bound for standard deviation in this 3 class setting happens when the model is greatly confident in one of the classes, i.e. softamx = [1, 0, 0] \u2192 \u03c3 max = 0.471. Bigger \u03c3 on average implies more confident answers by the PLM. It can be observed that the average predictions with the original samples have a great degree of confidence. We see an interesting phenomenon where the predictive confidence slightly rises across most of the datasets for the cases where the model is able to recover the inherent textual relations. However, when faced with examples that cause label changes, there is a significant drop of \u25b3\u03c3 = 0.1 in the standard deviation averaged across the datasets. This signifies that predictive confidence sizably degrades when the model struggles to recover the existent relations because of slight semantics-preserving variations. That further indicates that NLI models are susceptible to semantic sensitivity and have limited knowledge of compositional semantics, which can lead to the degradation of predictive confidence and incidentally inconsistent predictions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We present a novel framework for assessing semantic sensitivity in NLI models through generating semantics-preserving variations. Our systematic study of the phenomenon across various datasets and transformer-based PLMs shows that the models consistently struggle with variations requiring knowledge of compositional semantics. This performance deterioration happens across the whole label space, almost regardless of model size. We measure the impact of semantic-sensitivity and show that it diminishes models' predictive confidence and can lead to predictive inconsistency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In our work, we cover the semantic-sensitivity that can be found within NLI models. However, the framework can be applied to a wider range of classification tasks. The benchmark can be extended with more datasets and further enhanced with larger human evaluation. Also, we covered PLMs specifically trained for NLI; however, it would be great to cover bigger LLMs, in particular w.r.t. their emergent zero-shot capabilities. Another limitation is that we only cover English-based language models and do not test in multi-lingual or cross-lingual settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Our work completes an analysis of numerous models w.r.t. their decision inconsistency induced by semantic surface form variations. We show that models are somewhat unable to handle logically and semantically equivalent sentences, which would lead to an inconsistent use across various domains and applications. Our generation method does not induce any further exploitation threat and can only be used for measuring the above-mentioned inconsistencies. We exclusively use open source publicly accessible data and models within our experimentations.\nA Appendix  Evaluation under Label change To assess the extent of the impact of semantic sensitivity, we employ an evaluation under label change. This means we consider the examples that changed the original prediction of the model after a surface-form variation replaced the original hypothesis. A graphical representation of this can be seen in Figure 5. It must be noted that we use only the samples that the model originally predicted correctly to avoid incorrect assessment regarding the reasoning behind the false predictions. Our primary aim is to measure the semantic sensitivity within the model predictions and the extent of inconsistency it causes.\nToken Level-Differences of the generated variations We further explore the difference between surface-form variations and original examples by conducting a token-level analysis for each pair (h, h\u2032). We compute the average amount of tokens present for the original and generated hypothesis and use fuzzy and exact matching to assess the overlap of tokens on average for each dataset. The results can be seen in Table 4. The results show that the generated and original examples have a high token level overlap which further reinforces the idea that surface form variations are close both syntactically, in the embedding space and logically. Given the generated semantics-preserving surface-form variation h \u2032 , we evaluate if a label change occurs when replacing the hypothesis in accordance with Equation 1.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_6"]}, {"heading": "Acknowledgements", "text": "Erik is partially funded by a DFF Sapere Aude research leader grant under grant agreement No 0171-00034B, as well as by a NEC PhD fellowship. This work is further supported by the Pioneer Centre for AI, DNRF grant number P1.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Natural language understanding", "journal": "Benjamin-Cummings Publishing Co., Inc", "year": "1995", "authors": "James Allen"}, {"ref_id": "b1", "title": "Representation of constituents in neural language models: Coordination phrase as a case study", "journal": "", "year": "2019", "authors": "Aixiu An; Ethan Peng Qian; Roger Wilcox;  Levy"}, {"ref_id": "b2", "title": "Deducibility, entailment and analytic containment. Directions in relevant logic", "journal": "", "year": "1989", "authors": "B Richard;  Angell"}, {"ref_id": "b3", "title": "The berkeley framenet project", "journal": "", "year": "1998", "authors": "F Collin;  Baker; J Charles; John B Fillmore;  Lowe"}, {"ref_id": "b4", "title": "Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics", "journal": "", "year": "2022", "authors": "Yonatan Belinkov"}, {"ref_id": "b5", "title": "Kolmogorovsmirnov test: Overview. Wiley statsref: Statistics reference online", "journal": "", "year": "2014", "authors": "W Vance; Yanyan Berger;  Zhou"}, {"ref_id": "b6", "title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "Gabor Samuel R Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"ref_id": "b7", "title": "Introduction to semantics and formalization of logic", "journal": "Harvard University Press", "year": "1959", "authors": "Rudolf Carnap"}, {"ref_id": "b8", "title": "Adversarial attacks and defences: A survey", "journal": "", "year": "2018", "authors": "Anirban Chakraborty; Manaar Alam; Vishal Dey; Anupam Chattopadhyay; Debdeep Mukhopadhyay"}, {"ref_id": "b9", "title": "", "journal": "Aakanksha Chowdhery", "year": "2022", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Yunxuan Fedus; Xuezhi Li; Mostafa Wang; Siddhartha Dehghani; Albert Brahma;  Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun;  Chen"}, {"ref_id": "b10", "title": "Semantics, pragmatics, and situated meaning", "journal": "Pragmatics at Issue", "year": "1991", "authors": "Aaron Cicourel"}, {"ref_id": "b11", "title": "What does BERT look at? an analysis of BERT's attention", "journal": "", "year": "2019", "authors": "Kevin Clark; Urvashi Khandelwal; Omer Levy; Christopher D Manning"}, {"ref_id": "b12", "title": "The general notion of entailment", "journal": "The Philosophical Quarterly", "year": "1950", "authors": "Michael Clark"}, {"ref_id": "b13", "title": "Entailment, intensionality and text understanding", "journal": "", "year": "2003", "authors": "Cleo Condoravdi; Dick Crouch; Valeria De Paiva; Reinhard Stolle; Daniel Bobrow"}, {"ref_id": "b14", "title": "The pascal recognising textual entailment challenge", "journal": "Springer", "year": "2005", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"ref_id": "b15", "title": "Evaluating compositionality in sentence embeddings", "journal": "", "year": "2018", "authors": "Ishita Dasgupta; Demi Guo; Andreas Stuhlm\u00fcller; Samuel J Gershman; Noah D Goodman"}, {"ref_id": "b16", "title": "Language models show human-like content effects on reasoning", "journal": "", "year": "2022", "authors": "Ishita Dasgupta;  Andrew K Lampinen; C Y Stephanie; Antonia Chan; Dharshan Creswell;  Kumaran; L James; Felix Mcclelland;  Hill"}, {"ref_id": "b17", "title": "Jensenshannon divergence and hilbert space embedding", "journal": "IEEE", "year": "2004", "authors": "Bent Fuglede; Flemming Topsoe"}, {"ref_id": "b18", "title": "Neural natural language inference models partially embed theories of lexical entailment and negation", "journal": "", "year": "2020", "authors": "Atticus Geiger; Kyle Richardson; Christopher Potts"}, {"ref_id": "b19", "title": "Breaking NLI systems with sentences that require simple lexical inferences", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Max Glockner; Vered Shwartz; Yoav Goldberg"}, {"ref_id": "b20", "title": "Logic and conversation", "journal": "Brill", "year": "1975", "authors": "P Herbert;  Grice"}, {"ref_id": "b21", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b22", "title": "Deberta: Decoding-enhanced bert with disentangled attention", "journal": "", "year": "2020", "authors": "Pengcheng He; Xiaodong Liu; Jianfeng Gao; Weizhu Chen"}, {"ref_id": "b23", "title": "A structural probe for finding syntax in word representations", "journal": "", "year": "2019", "authors": "John Hewitt; D Christopher;  Manning"}, {"ref_id": "b24", "title": "Behavior analysis of nli models: Uncovering the influence of three factors on robustness", "journal": "", "year": "2018", "authors": "Jeff V Ivan Sanchez Carmona; Sebastian Mitchell;  Riedel"}, {"ref_id": "b25", "title": "Compositional semantics: An introduction to the syntax/semantics interface. Oxford Textbooks in Linguistic", "journal": "", "year": "2014", "authors": " Pauline I Jacobson"}, {"ref_id": "b26", "title": "What does bert learn about the structure of language", "journal": "", "year": "2019", "authors": "Ganesh Jawahar; Beno\u00eet Sagot; Djam\u00e9 Seddah"}, {"ref_id": "b27", "title": "Suvrat Bhooshan, and Adina Williams. 2020a. Are natural language inference models imppressive? learning implicature and presupposition", "journal": "", "year": "", "authors": "Paloma Jeretic; Alex Warstadt"}, {"ref_id": "b28", "title": "Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Paloma Jeretic; Alex Warstadt; Suvrat Bhooshan; Adina Williams"}, {"ref_id": "b29", "title": "Survey of hallucination in natural language generation", "journal": "ACM Computing Surveys", "year": "2023", "authors": "Ziwei Ji; Nayeon Lee; Rita Frieske; Tiezheng Yu; Dan Su; Yan Xu; Etsuko Ishii; Ye Jin Bang; Andrea Madotto; Pascale Fung"}, {"ref_id": "b30", "title": "Kullback-leibler divergence", "journal": "Springer", "year": "2011", "authors": "M James;  Joyce"}, {"ref_id": "b31", "title": "Large language models are zero-shot reasoners", "journal": "", "year": "2022", "authors": "Takeshi Kojima; Shane Shixiang; Machel Gu; Yutaka Reid; Yusuke Matsuo;  Iwasawa"}, {"ref_id": "b32", "title": "Revealing the dark secrets of bert", "journal": "", "year": "2019", "authors": "Olga Kovaleva; Alexey Romanov; Anna Rogers; Anna Rumshisky"}, {"ref_id": "b33", "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation", "journal": "", "year": "2019", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b34", "title": "A simple, fast diverse decoding algorithm for neural generation", "journal": "", "year": "2016", "authors": "Jiwei Li; Will Monroe; Dan Jurafsky"}, {"ref_id": "b35", "title": "Linguistic knowledge and transferability of contextual representations", "journal": "", "year": "2019", "authors": "F Nelson; Matt Liu; Yonatan Gardner;  Belinkov; E Matthew; Noah A Peters;  Smith"}, {"ref_id": "b36", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b37", "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"ref_id": "b38", "title": "Universal adversarial perturbations", "journal": "", "year": "2017", "authors": "Alhussein Seyed-Mohsen Moosavi-Dezfooli; Omar Fawzi; Pascal Fawzi;  Frossard"}, {"ref_id": "b39", "title": "Stress test evaluation for natural language inference", "journal": "", "year": "2018", "authors": "Aakanksha Naik; Abhilasha Ravichander; Norman Sadeh; Carolyn Rose; Graham Neubig"}, {"ref_id": "b40", "title": "Stress test evaluation for natural language inference", "journal": "", "year": "2018", "authors": "Aakanksha Naik; Abhilasha Ravichander; Norman Sadeh; Carolyn Rose; Graham Neubig"}, {"ref_id": "b41", "title": "Adversarial nli: A new benchmark for natural language understanding", "journal": "", "year": "2019", "authors": "Yixin Nie; Adina Williams; Emily Dinan; Mohit Bansal; Jason Weston; Douwe Kiela"}, {"ref_id": "b42", "title": "EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference", "journal": "", "year": "2019", "authors": "Abhilasha Ravichander; Aakanksha Naik; Carolyn Rose; Eduard Hovy"}, {"ref_id": "b43", "title": "Beyond accuracy: Behavioral testing of nlp models with checklist", "journal": "", "year": "2020", "authors": "Tongshuang Marco Tulio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"ref_id": "b44", "title": "Probing natural language inference models through semantic fragments", "journal": "", "year": "2020", "authors": "Kyle Richardson; Hai Hu; Lawrence Moss; Ashish Sabharwal"}, {"ref_id": "b45", "title": "Context-dependent semantic processing in the human brain: Evidence from idiom comprehension", "journal": "Journal of Cognitive Neuroscience", "year": "2013", "authors": "Joost Rommers; Ton Dijkstra; Marcel Bastiaansen"}, {"ref_id": "b46", "title": "Xia Song, Paul Bennett, and Saurabh Tiwary. 2020. Knowledgeaware language model pretraining", "journal": "", "year": "", "authors": "Corby Rosset; Chenyan Xiong; Minh Phan"}, {"ref_id": "b47", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b48", "title": "Philosophical Grounds of Rationality: Intentions, Categories, Ends", "journal": "", "year": "1986", "authors": "Stephen Schiffer"}, {"ref_id": "b49", "title": "Unnatural language inference", "journal": "", "year": "2020", "authors": "Koustuv Sinha; Prasanna Parthasarathi; Joelle Pineau; Adina Williams"}, {"ref_id": "b50", "title": "Unnatural language inference", "journal": "", "year": "2021", "authors": "Koustuv Sinha; Prasanna Parthasarathi; Joelle Pineau; Adina Williams"}, {"ref_id": "b51", "title": "Pragmatic presuppositions", "journal": "ERIC", "year": "1977", "authors": "Robert Stalnaker; K Milton; Peter Munitz;  Unger"}, {"ref_id": "b52", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b53", "title": "Diverse beam search: Decoding diverse solutions from neural sequence models", "journal": "", "year": "2016", "authors": "K Ashwin; Michael Vijayakumar;  Cogswell; R Ramprasath; Qing Selvaraju; Stefan Sun; David Lee; Dhruv Crandall;  Batra"}, {"ref_id": "b54", "title": "Do nlp models know numbers? probing numeracy in embeddings", "journal": "", "year": "2019", "authors": "Eric Wallace; Yizhong Wang; Sujian Li; Sameer Singh; Matt Gardner"}, {"ref_id": "b55", "title": "Superglue: A stickier benchmark for general-purpose language understanding systems", "journal": "", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b56", "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b57", "title": "Learning natural language inference with lstm", "journal": "", "year": "2015", "authors": "Shuohang Wang; Jing Jiang"}, {"ref_id": "b58", "title": "Can neural networks acquire a structural bias from raw linguistic data", "journal": "", "year": "2020", "authors": "Alex Warstadt;  Samuel R Bowman"}, {"ref_id": "b59", "title": "Finetuned language models are zero-shot learners", "journal": "", "year": "2021", "authors": "Jason Wei; Maarten Bosma; Y Vincent; Kelvin Zhao; Adams Wei Guu; Brian Yu; Nan Lester;  Du; M Andrew; Quoc V Dai;  Le"}, {"ref_id": "b60", "title": "", "journal": "", "year": "", "authors": "Jason Wei; Yi Tay; Rishi Bommasani; Colin Raffel; Barret Zoph; Sebastian Borgeaud; Dani Yogatama; Maarten Bosma; Denny Zhou; Donald Metzler"}, {"ref_id": "b61", "title": "2022a. Emergent abilities of large language models", "journal": "", "year": "", "authors": ""}, {"ref_id": "b62", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed Chi; Quoc Le; Denny Zhou"}, {"ref_id": "b63", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "", "year": "2017", "authors": "Adina Williams; Nikita Nangia; Samuel R Bowman"}, {"ref_id": "b64", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b65", "title": "Lasha Abzianidze, and Johan Bos. 2019. Can neural networks understand monotonicity reasoning? arXiv preprint", "journal": "", "year": "", "authors": "Hitomi Yanaka; Koji Mineshima; Daisuke Bekki; Kentaro Inui; Satoshi Sekine"}, {"ref_id": "b66", "title": "Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach", "journal": "", "year": "2019", "authors": "Wenpeng Yin; Jamaal Hay; Dan Roth"}, {"ref_id": "b67", "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "journal": "", "year": "2018", "authors": "Rowan Zellers; Yonatan Bisk; Roy Schwartz; Yejin Choi"}, {"ref_id": "b68", "title": "Semantics-aware bert for language understanding", "journal": "", "year": "2020", "authors": "Zhuosheng Zhang; Yuwei Wu; Hai Zhao; Zuchao Li; Shuailiang Zhang; Xi Zhou; Xiang Zhou"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: In-and out-of-domain fooling rate of DeBERTa of varied sizes, which are measured on MNLI (left) and SNLI (right). Similarly, r s and r r represent the strict and relaxed fooling rates, respectively.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :Figure 4 :34Figure3: Divergence of predictive probability distribution between (p, h) and (p, h \u2032 ) measured across the datasets (ANLI is averaged over the rounds) and averaged over all models. All evaluation pairs are split into two groups based on whether they manage to flip the original label. Two divergence metrics are shown -JS divergence (left) and KS divergence (right).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: A diagram for assessing semantic similarity.Given the generated semantics-preserving surface-form variation h \u2032 , we evaluate if a label change occurs when replacing the hypothesis in accordance with Equation1", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The original accuracy on testing/dev sets for various transformers (b-base, l-large, xl-extra large) on in-domain MNLI experiments and zero-shot transfers to out-of-domain SNLI and ANLI. The number near the dataset name designates the exact amount of original samples in the testing set.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "11%/15.52% 8.38%/14.98% 15.67%/23.68% 9.96%/17.01% 7.83%/13.39% 9.50%/14.69% ANLI_r1 31.51%/42.89% 28.45%/35.01% 31.48%/52.30% 40.0%/48.99% 25.66%/37.88% 22.71%/30.73% ANLI_r2 34.39%/51.91% 24.62%/42.80% 36.09%/57.49% 34.92%/48.47% 28.44%/44.04% 29.46%/46.46% ANLI_r3 29.11%/51.39% 21.88%/45.00% 29.26%/52.42% 33.88%/53.17% 24.88%/44.65% 23.23%/42.37%", "figure_data": "r s /r rbart-largeroberta-largedistilbartdeberta-basedeberta-largedeberta-xlargeMNLI6.64%/12.35%5.71%/11.56% 9.20%/ 16.80%6.66%/13.81%5.38%/11.54%5.89%/11.49%SNLI10."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "84%/46.28% 49.41%/49.41% 21.89%/50.80% 31.32%/48.53% ANLI_r3 11.65%/52.00% 47.18%/47.18% 16.42%/46.50% 27.04%/48.17%", "figure_data": "MNLI2.78%/13.41% 14.33%/14.33% 3.69%/11.17%6.58%/12.92%SNLI9.54%/18.73% 19.42%/19.42% 2.92%/11.82% 10.24%/16.54%ANLI_r1 21.64%/41.97% 38.62%/38.62% 29.17%/44.57% 29.97%/41.30%ANLI_r2 20."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Dataset Fuzzy token match % average length h average length h\u2032 average token overlap", "figure_data": "mnli84.8314.3114.1413.25snli81.5510.8111.2110.38anli_r187.5917.317.0213.73anli_r286.4915.9915.8412.8anli_r385.1714.3214.2911.27"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "D correct = {\u2200(p i , h i ) \u2208 D : M(p i , h i ) =\u0177 = y}", "formula_coordinates": [3.0, 306.14, 572.47, 218.28, 32.21]}, {"formula_id": "formula_1", "formula_text": "M(h i , h \u2032 i ) =\u0177 C=E = M(h \u2032 i , h) (1)", "formula_coordinates": [4.0, 109.37, 447.35, 180.49, 21.19]}, {"formula_id": "formula_2", "formula_text": "R(p i , h i , h \u2032j i , O) = = 1, O(M(p i , h i ), M(p i , h \u2032j i )) = 0 0, O(M(p i , h i ), M(p i , h \u2032j i )) = 1", "formula_coordinates": [4.0, 316.11, 395.95, 174.62, 57.79]}, {"formula_id": "formula_3", "formula_text": "r r = n \u2032 i 1 \u2203j \u2208 [1, k], R(p i , h i , h \u2032j i , =) \u0338 = 1 n \u2032 .", "formula_coordinates": [4.0, 308.38, 718.97, 213.81, 35.83]}, {"formula_id": "formula_4", "formula_text": "r s = n \u2032 i 1 \u2203j \u2208 [1, k], R(p i , h i , h \u2032j i , = s ) \u0338 = 1 n \u2032 . (4)", "formula_coordinates": [5.0, 70.95, 340.16, 218.91, 44.92]}], "doi": "10.18653/v1/D19-1287"}