{"title": "Volatile Correlation Computation: A Checkpoint View", "authors": "Wenjun Zhou; Hui Xiong", "pub_date": "", "abstract": "Recent years have witnessed increased interest in computing strongly correlated pairs in very large databases. Most previous studies have been focused on static data sets. However, in real-world applications, input data are often dynamic and must continually be updated. With such large and growing data sets, new research efforts are expected to develop an incremental solution for correlation computing. Along this line, in this paper, we propose a CHECK-POINT algorithm that can efficiently incorporate new transactions for correlation computing as they become available. Specifically, we set a checkpoint to establish a computation buffer, which can help us determine an upper bound for the correlation. This checkpoint bound can be exploited to identify a list of candidate pairs, which will be maintained and computed for correlations as new transactions are added into the database. However, if the total number of new transactions is beyond the buffer size, a new upper bound is computed by the new checkpoint and a new list of candidate pairs is identified. Experimental results on real-world data sets show that CHECK-POINT can significantly reduce the correlation computing cost in dynamic data sets and has the advantage of compacting the use of memory space.", "sections": [{"heading": "INTRODUCTION", "text": "Given a set of data objects, the problem of correlation computing is concerned with identification of strongly-related (e.g. as measured by Pearson's correlation coefficient for pairs [13]) groups of data objects. Many important applications in science and business [2,6,12,14] depend on efficient and effective correlation computing techniques to discover relationships within large collections of information. Despite the development of traditional statistical correlation computing techniques [4,10,8,11,9,15,16], researchers and practitioners are still facing increasing challenges to measure associations among data produced by emerging dataintensive applications.\nIndeed, the size of real-world datasets is growing at an extraordinary rate, and these data are often dynamic and need to be continually updated. With such large and growing data sets, new research efforts are expected to develop an incremental solution for correlation computing. To that end, in this paper, we limit our scope to provide a pilot study of incrementally querying all item pairs with correlations above a user specified minimum correlation threshold when new data become available.\nA straightforward approach is to recompute the correlations for all the item pairs every time that new data of transactions become available. However, for large data sets, this approach is infeasible, particularly if the application needs the results in a timely fashion. An alternative method is to use more space to save the time. Along this line, we describe a SAVE-ALL algorithm, which saves the intermediate results for all item pairs. When new transactions are added into the database, SAVE-ALL only updates the stored values corresponding to each item pair and computes the correlation query results with the intermediate values. Obviously, the SAVE-ALL method compromises space for time. If the number of items in the data set becomes considerably large, the number of pairs grow even larger, to the extent that it is impossible to save the intermediate computing results of all item pairs in the memory space. This motivates our interest in incremental correlation computing.\nSpecifically, we propose a CHECK-POINT algorithm that makes a time-space tradeoff and can efficiently incorporate new transactions for correlation computing as they become available. In the CHECK-POINT algorithm, we set a checkpoint to establish a computation buffer, which can help us to determine a correlation upper bound. This checkpoint bound can be exploited to identify a list of candidate pairs, which will be maintained and computed for correlations as new transactions are added into the database. However, if the total number of new transactions exceeds the buffer size, a new upper bound is computed by the new checkpoint and a new list of candidate pairs is identified.\nThe rationale behind CHECK-POINT is that, if the number of new transactions is much smaller than the total number of transactions in the database, the correlation coefficients of most item pairs do not change substantially. In other words, we only need to establish a very short list of candidate pairs at the checkpoint and maintain this candidate list in the memory as new transactions are added into the database. Unlike SAVE-ALL, CHECK-POINT only maintains the intermediate computing results of a very small portion of the item pairs. This can greatly compact the use of the memory space by using slightly more time.\nAs demonstrated by our experimental results on several real-world data sets, CHECK-POINT can significantly reduce the computational cost compared to existing correlation computing benchmark algorithms, i.e. TAPER, in a dynamic data environment. Also, we observe that there is a trade-off between the use of space and the time by setting different checkpoint values. Indeed, the size of the candidate list increases with the increase of the checkpoint value. In contrast, the average computational savings is reduced with the increase of the checkpoint value. Finally, our experimental results show that CHECK-POINT, as compared to SAVE-ALL, can greatly reduce the use of memory space.\nOverview. The remainder of this paper is organized as follows. In Section 2, we introduce some basic concepts and formulate the problem. Section 3 provides a checkpoint view for dynamic all-strong-pairs correlation queries. In Section 4, we describe the CHECK-POINT and SAVE-ALL algorithms. Section 5 shows the experimental results. Finally, in Section 6, we provide the concluding remarks.", "publication_ref": ["b12", "b1", "b5", "b11", "b13", "b3", "b9", "b7", "b10", "b8", "b14", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "PRELIMINARIES", "text": "In this section, we first introduce some basic concepts and notations that will be used in this paper. Then, we provide the problem formulation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic Concepts", "text": "The \u03c6 correlation coefficient [13] is the computation form of the Pearson's correlation coefficient [5] for binary variables. In a 2 \u00d7 2 contingency table shown in Table 1, the calculation of the \u03c6 correlation coefficient reduces to \u03c6 = P (00) P (11) \u2212 P (01) P (10) p P (0+) P (1+) P (+0) P (+1) ,\nwhere P (ij) , for i \u2208 {0, 1} and j \u2208 {0, 1}, denotes the number of samples which are classified in the ith row and jth column of the table, and N is the total number of samples. Furthermore, we let P (i+) denote the total number of samples classified in the ith row, and we let P (+j) denote the total number of samples classified in the jth column. Thus, P (i+) = P 1 j=0 P (ij) and P (+j) = P 1 i=0 P (ij) . ", "publication_ref": ["b12", "b4"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "A B", "text": "Hence, when adopting the support measure of association rule mining [1], for two items a and b in a market basket database, we have supp(a) = P (1+) /N , supp(b) = P (+1) /N , and supp(a, b) = P (11) /N . In Xiong et al. (2004) [15] the support form of the \u03c6 correlation coefficient has been derived, as shown in Equation 2.\n\u03c6 {a,b} = supp(a, b) \u2212 supp(a)supp(b) p supp(a)supp(b)(1 \u2212 supp(a))(1 \u2212 supp(b))(2)\nXiong et al. (2004) has also identified an upper bound for \u03c6 {a,b} [15]. Without loss of generality, if we assume that supp(a) \u2265 supp(b), then an upper bound for \u03c6 {a,b} is\nupper(\u03c6 {a,b} ) = s supp(b) supp(a) s 1 \u2212 supp(a) 1 \u2212 supp(b)(3)\nFor the purpose of simplicity, we denote Na as the number of transactions in the database that contain item a, N b as the number of those containing item b, and N ab as the number of those containing both items. Then supp(a) = Na/N , supp(b) = N b /N , and supp(a, b) = N ab /N . Substituting into Equation 2, we can calculate the \u03c6 correlation coefficient for items a and b as\n\u03c6 {a,b} = N N ab \u2212 NaN b p Na(N \u2212 Na)N b (N \u2212 N b ) .(4)", "publication_ref": ["b0", "b14", "b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Here, we introduce the problem formulation. Let D be a transaction database, which has M items and N transactions. In this data set, a common task of correlation computing is to find all item pairs whose correlation coefficients are above a user-specified threshold \u03b8. This is known as the all-strong-pairs correlation query problem [15]. In this paper, we investigate the all-strong-pairs correlation query problem in a dynamic data environment.\nSpecifically, every time a data set of S new transactions is added into the original database D, we want to have the dynamically updated results from the all-strong-pairs correlation query. In other words, this all-strong-pairs correlation query can be a frequent task in a dynamic data environment. As a result, our goal is to develop an incremental, practical, and computation-efficient solution to this all-strong-pairs correlation query problem.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "A CHECKPOINT VIEW", "text": "In this section, we first introduce the checkpoint principle. Then, we provide some theoretical foundations for the checkpoint framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Checkpoint Principle", "text": "In general, there are three ways for developing the incremental solutions for frequent and dynamic all-strong-pairs correlation queries.\nFirst, the simplest way is to recompute the correlation values for all the item pairs every time new data sets of transactions become available. Along this line, we can use an efficient static all-strong-pairs correlation query algorithm, such as TAPER [15], for each computation. However, for very large data sets, this approach is infeasible if data updates are very frequent and the application needs the result in a timely manner.\nThe second way is to use more space to save time [3]. Specifically, we can save the support of each item pair and update the values every time new data are added into the database. In this way, once all the intermediate computing results are saved, the all-strong-pairs correlation queries can be done very efficiently, but the memory requirement is very high. For instance, let us consider a database of 10 6 items, which may represent the collection of books available at an e-commerce Web site. There are`1 0 6 2\u00b4\u2248 0.5 \u00d7 10 12 possible item pairs, which needs a huge amount of memory space to store intermediate computing results. In practice, this memory requirement cannot be satisfied for data sets with a large number of items.\nFinally, we look for an answer between the above two solutions. Specifically, instead of saving the intermediate computing results for all item pairs, we propose to save them for only selected item pairs. Aiming for a tradeoff between time and space, we use a checkpoint to establish a computation buffer, which can help us determine a correlation upper bound. Specifically, at a checkpoint, assuming that we know that \u2206N (\u2206N << N ) new transactions will be added into the database before the next checkpoint, we can develop an upper bound for all the item pairs on N + \u2206N transactions. This upper bound has taken the newly added \u2206N transactions into consideration. Therefore, based on this upper bound, we can establish a list of candidate item pairs whose upper bounds are greater than or equal to the threshold \u03b8. This list of candidate item pairs can be treated as a computation buffer for all-strong-pairs correlation queries. While new transactions can be added into the buffer dynamically, we only need to maintain the intermediate results for item pairs in this candidate list as long as the cumulative number of new transactions is less than \u2206N . The above process is illustrated in Figure 1.", "publication_ref": ["b14", "b2"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Check Point N N + \u2206\u039d variable fill rate", "text": "A correlation computing point The reason that the candidate list can remain unchanged (as long as the cumulative number of new transactions is less than \u2206N ) is as follows. With a checkpoint at N + \u2206N , we identify upper bounds for all item pairs for all N known transactions and \u2206N unknown transactions. In other words, these upper bounds are the maximum possible values they can achieve no matter what kind of \u2206N transactions have been added into the database. Then, if the cumulative number of new transactions is less than \u2206N , the upper bounds for all the item pairs in N + \u2206N transactions will remain unchanged. Therefore, the candidate list will also remain unchanged. We call this the checkpoint principle.\nOnce the cumulative number of new transactions is greater than \u2206N , we need to set a new checkpoint at N + 2\u2206N . This iterative process will form an incremental solution for the dynamic all-strong pairs query problem. The rationale behind the checkpoint principle is that a small number of new transactions will not cause a significant effect on the correlation coefficients of most item pairs in the database if the total number of transactions is very large.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predicted \u03c6 Correlation Coefficient at the Next Checkpoint", "text": "In Section 2.1 we have shown that the \u03c6 correlation coefficient can be computed by Equation 4. In the original database D, the frequencies for item a, b, and item pair {a, b} are denoted as Na, N b , and N ab , respectively. Suppose that at a checkpoint, we set the next checkpoint right after \u2206N new transactions. In the \u2206N new transactions, we assume that there are \u2206Na, \u2206N b , and \u2206N ab new transactions containing item a, item b, and item pair {a, b}, respectively. Then according to Equation 4, at the next checkpoint the new \u03c6 correlation will be\n\u03c6 \u2032 {a,b} = N \u2032 N \u2032 ab \u2212 N \u2032 a N \u2032 b p N \u2032 a (N \u2032 \u2212 N \u2032 a )N \u2032 b (N \u2032 \u2212 N \u2032 b ) ,(5)\nwhereN\n\u2032 = N + \u2206N , N \u2032 a = Na + \u2206Na, N \u2032 b = N b + \u2206N b , and N \u2032 ab = N ab + \u2206N ab .\nUnfortunately, we do not have any information about the \u2206N new transactions, so \u2206Na, \u2206N b and \u2206N ab are all unknown. Thus, we cannot compute the new \u03c6 \u2032 directly. However, because the size of the new data set is much smaller than that of the original database, for any pair of items, the new \u03c6 \u2032 is expected not to change greatly from the current \u03c6 value. For this reason, we aim to derive an upper bound for \u03c6 \u2032 , and use it as a criterion regarding whether we should save the intermediate computation result for that pairs. Specifically, if upper(\u03c6 \u2032 ) is less than the threshold \u03b8, then we can guarantee that item pair {a, b} will never become a strongly-correlated pair prior to the next checkpoint. As a result, we can save intermediate computation results only for pairs having \u03c6 \u2032 beyond the threshold \u03b8.\nHowever it is difficult to derive an exact upper bound for \u03c6 \u2032 , because the denominator and the numerator are both affected by common factors, and they do not change monotonically by these factors. In the following subsections, we derive a loose upper bound for \u03c6 \u2032 {a,b} when \u2206N new transactions are added into the databases.\nLooking closely at Equation 5, we can split the right hand side into three parts.\nLet u = N \u2032 N \u2032 ab \u2212 N \u2032 a N \u2032 b , v = N \u2032 a (N \u2032 \u2212 N \u2032 a ), and w = N \u2032 b (N \u2032 \u2212 N \u2032 b )\n, then Equation 5 can be rewritten as\n\u03c6 \u2032 {a,b} = u \u221a vw .(6)\nThe upper bound is calculated as the maximum possible value of u divided by the product of the minimum possible values of v and w. The ratio is a loose upper bound for \u03c6 {a,b} because the maximum and the minimum values may not be achieved simultaneously. Since we do not know the exact value of \u2206Na, \u2206N b , and \u2206N ab to come, our derivations are only based on the fact that 0 \u2264 \u2206N ab \u2264 \u2206Na \u2264 \u2206N , and 0 \u2264 \u2206N ab \u2264 \u2206N b \u2264 \u2206N .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Maximum Value of the Numerator u", "text": "In this subsection, we derive the maximum possible value for the numerator \u03c6 \u2032 , u.\nGiven N , Na, N b , N ab , and \u2206N , the numerator of \u03c6 \u2032 {a,b} , written as\nu = N \u2032 N \u2032 ab \u2212 N \u2032 a N \u2032 b = (N + \u2206N )(N ab + \u2206N ab ) \u2212 (Na + \u2206Na)(N b + \u2206N b )\n, is large when N ab is large, and \u2206Na and \u2206N b are small. Specifically, we have the following lemma.\nLemma 1. Given N , Na, N b , N ab , and \u2206N , the maximum possible value for u, the numerator of the \u03c6 correlation coefficient \u03c6 {a,b} at the next checkpoint, is\numax = 8 < : f (0) if t \u2264 \u2212\u2206N ; f (x * ) if \u2212 \u2206N \u2264 t \u2264 \u2206N ; f (\u2206N ) if t \u2265 \u2206N,(7)\nwhere\nf (x) = (N + \u2206N )(N ab + x) \u2212 (Na + x)(N b + x), t = N \u2212 Na \u2212 N b , and x * = (N \u2212 Na \u2212 N b + \u2206N )/2.\nProof. Because of symmetry, we can assume without loss of generality that \u2206Na\n\u2264 \u2206N b . Let \u2206N ab = x, x \u2265 0; \u2206Na = x + c1, c1 \u2265 0; \u2206N b = x + c1 + c2, c2 \u2265 0.\nIn the following we derive the maximum value for u by taking first and second partial derivatives [7].\nFirst, because \u2202u/\u2202c2 = \u2212(Na + x + c1) < 0, u increases monotonically as c2 decreases. In order to reach the maximum of u, c2 must take the minimum value in its range, 0.\nSimilarly, because \u2202u/\u2202c1 = \u2212(N b + x + c1 + c2) \u2212 (Na + x + c1) < 0, u takes the maximum value when c1 = 0. As a result, \u2202u/\u2202x = (N + \u2206N ) \u2212 (N b + x + c1 + c2) \u2212 (Na + x + c1) = N \u2212 Na \u2212 N b + \u2206N \u2212 2x. Since \u2202 2 u/\u2202x 2 = \u22122 < 0, u\nreaches its maximum value when \u2202u/\u2202x = 0. Let \u2202u/\u2202x = 0, then the solution of the equation is\nx * = (N + \u2206N \u2212 Na \u2212 N b \u2212 2c1 \u2212 c2)/2 = (N \u2212 Na \u2212 N b + \u2206N )/2.\nHowever, because 0 \n\u2264 x = \u2206N ab \u2264 \u2206N , the above value can be reached only if \u2212\u2206N \u2264 N \u2212 Na \u2212 N b \u2264 \u2206N . If N \u2212 Na \u2212 N b \u2264 \u2212\u2206N , then \u2202u/\u2202x = N \u2212 Na \u2212 N b + \u2206N \u2212 2x \u2264 \u2212\u2206N + \u2206N \u2212 2x = \u22122x \u2264 0,", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Minimum Value of the Denominator", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u221a vw", "text": "In this subsection, we derive the minimum value of the denominator of \u03c6 \u2032 , \u221a vw. This is equivalent to finding the minimum value of vw. First, a lower bound of vw can be reached by taking the minimum value of v and the minimum value of w. Again, the minimum values of v and w may or may not be reached simultaneously, so the lower bound for the denominator we derive here is also a loose bound. Proof. Similar to the proof of Lemma 2, we can simply prove that the minimum possible value of h(x) in the range\n[N b , N b + \u2206N ] is either h(N b ) or h(N b + \u2206N ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Loose Upper Bound for \u03c6 \u2032", "text": "In this section, we can derive a loose upper bound for \u03c6\ncorrelation coefficient \u03c6 = u/ \u221a vw by Lemma 1, Lemma 2,\nand Lemma 3 as the following:\nLemma 4. A loose upper bound for the new \u03c6 correlation coefficient at the next checkpoint, \u03c6 \u2032 = u/ \u221a vw, is upper(\u03c6 \u2032 ) = umax \u221a vminwmin ,(10)\nwhere umax follows Equation ( 7), vmin follows Equation ( 8), and wmin follows Equation (9).\nProof. The proof is straightforward, since we have umax, vmin, and wmin from Lemmas 1, 2, and 3.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "ALGORITHM DESCRIPTIONS", "text": "In this section, we describe three different solutions to the all-strong-pairs correlation query problem in a dynamic data environment. First, we present a straightforward solution, named r-TAPER, which recomputes correlation coefficients for all the item pairs every time new data are added into the database. The second SAVE-ALL approach stores the intermediate computing results for all the item pairs and can greatly save the computational cost. Finally, we also provide an incremental solution, called CHECK-POINT, which strikes a balance between the use of memory space and the computational efficiency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The r-TAPER Algorithm", "text": "In this method, we need to recompute correlation coefficients for all the item pairs every time the database has been updated. No intermediate result has been reused for the next correlation computing practice. Since we have already known that a brute-force way to compute all-strongpairs correlation queries is computationally expensive [15], we apply the TAPER algorithm [15] in this study. TAPER is an efficient algorithm for the all-strong-pairs correlation query on static data [15]. Figure 2 shows the pseudo code of the r-TAPER algorithm which computes all-strong-pairs queries in a dynamic data environment. In the figure, we can see that the r-TAPER algorithm repeatedly calls the TAPER procedure every time new transactions are added into the database. Note that the implementation details of TAPER can be found in [15]. 1.\nD \u2190 D \u222a D \u2206 2.\nL \u2190 T AP ER(D, \u03b8)", "publication_ref": ["b14", "b14", "b14", "b14"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "3.", "text": "Output L ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The SAVE-ALL Algorithm", "text": "In the r-TAPER algorithm, we have observed the fact that the computational bottleneck for the all-strong-pairs query is to count the frequencies of all item pairs on the fly, and no intermediate computing results have been reused for the next correlation computation. On the contrary, the SAVE-ALL method stores the frequencies of all item pairs and incrementally update the stored values while new data are added into the database. In this way, SAVE-ALL uses more space for the sake of saving computation time.\nFigure 3 shows the implementation details of the SAVE-ALL algorithm. In this figure, Lines 1-5 process the dynamic data, and update frequencies of items and item pairs as needed. The frequencies of individual items are saved on the main diagonal. Lines 6-12 compute the \u03c6 correlation coefficient for each item pair and check if it is beyond the threshold \u03b8. Since the frequencies of all the item pairs are stored in the memory, the computation of \u03c6 correlation coefficient for each item pair is very efficient. However, the drawback of this SAVE-ALL method is that, if the number of items becomes extremely large, we may not have enough memory space for running the algorithm. ", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "1.", "text": "for each transaction t in D \u2206 do 2.\nfor each pair of items {i, j} in t do 3.\nM [i, j] \u2190 M [i, j] + 1 4.\nend for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "end for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6.", "text": "L \u2190 \u2205", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7.", "text": "for each possible pair {a, b} do", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8.", "text": "Compute \u03c6 {a,b} 9.\nif \u03c6 {a,b} \u2265 \u03b8 then 10.\nAdd pair {a, b} to L 11.\nend if", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "12.", "text": "end for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "13.", "text": "Output L ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The CHECK-POINT Algorithm", "text": "The above mentioned two algorithms are quite straight forward. However, they represent two extreme cases of the all-strong-pairs correlation query problem in a dynamic data environment. The r-TAPER algorithm, which repeat the query every time new data become available, disregards the previously computed results, and thus wastes a lot of computation. On the other hand, the SAVE-ALL algorithm requires an extremely large amount of memory space for saving the intermediate computing results. This becomes infeasible when the number of items is very large. As we have discussed in Section 3, we look for a solution in between the above two methods. This solution should have the capabilities in storing some intermediate computing results to save the computation, and do not overuse the memory space. Based on what we have discovered in Section 3, we have developed a new algorithm called CHECK-POINT, which is described in Figure 4. 1.\nfor each transaction t in D \u2206 do 2.\nfor each pair of items {i, j} in t do", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3.", "text": "if {i, j} \u2208 CL then 4.\nN ij \u2190 N ij + 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "end if", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6.", "text": "end for 7.\nend for 8.\nL \u2190 \u2205", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "9.", "text": "for each pair {a, b} in C do", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10.", "text": "Compute \u03c6 {a,b} 11.\nif \u03c6 {a,b} \u2265 \u03b8 then", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "12.", "text": "Add pair {a, b} to L", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "13.", "text": "end if", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "14.", "text": "end for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "15.", "text": "Output L", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "16.", "text": "if Check then 17.\nD \u2190 D \u222a D \u2206 18. CL \u2190 UpdateCandidateList(D,\u2206N )\n19.\nend if", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Figure 4: The Pseudocode of the CHECK-POINT Algorithm", "text": "In the figure, we can see that the CHECK-POINT algorithm consists of three parts. The first part, Lines 1-7, reads the new data and only updates the frequencies of item pairs on the candidate list. The second part, Lines 8-15, computes the \u03c6 correlation for each candidate pair and outputs if the correlation coefficient exceeds the threshold. Note that it is safe to ignore all other item pairs which are not on the candidate list. This is guaranteed by the way that the candidate list is constructed (please refer to the checkpoint principle in Section 3). The last part, described in Lines 16 through 19, calls a sub-procedure U pdateCandidateList, only if there is a checkpoint scheduled at the end of this step. In other words, the cumulative number of new transactions is greater than the computation buffer \u2206N .\nThe U pdateCandidateList sub-procedure, described in Figure 5, shows what happens at each checkpoint. Given that\nALGORITHM UpdateCandidateList(D,\u2206N ) 1. CL \u2190 \u2205 2.\nfor each possible item pair {a, b} do", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "3.", "text": "Calculate the upper bound of \u03c6 \u2032 {a,b} with \u2206N", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.", "text": "if upper(\u03c6 \u2032 {a,b} ) \u2265 \u03b8 then", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "Add {a, b} to CL, and store N ab 6.\nend if", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7.", "text": "end for 8.\nreturn CL the original data set has been updated, and that the frequency of each item is readily available, for each item pair {a, b}, we can compute an upper bound of its future \u03c6 correlation coefficient if we know that the next checkpoint is scheduled after \u2206N new transactions to come. If the upper bound is no less than the threshold, then the item pair is put into the candidate list; otherwise we know that the item pair will never have a correlation coefficient above the userspecified correlation threshold \u03b8 even if another \u2206N new transactions are added into the database.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL RESULTS", "text": "In this section, we present the experimental results to evaluate the performance of the CHECK-POINT algorithm. Specifically, we study: (1) the computational performance of CHECK-POINT compared with r-TAPER and SAVE-ALL algorithms; (2) the performance of the CHECK-POINT algorithm in terms of the use of space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Experimental Setup", "text": "Our experiments were conducted on three real-world data sets: chess, connect, and pumsb. The first two data sets, chess and connect, are from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/). The last data set, pumsb, which is often used as a benchmark data set for evaluating frequent pattern mining algorithms, is from FIMI (http://fimi.cs.helsinki.fi/data/).  2 summarizes basic characteristics of the data sets used in our experiments. In this paper, our goal is to incrementally perform the all-strong-pairs correlation queries in a dynamic data situation. To mimic this dynamic realworld scenario, we did the following preprocessing on these three benchmark data sets. First, we generated a base data set of 100000 transactions for each data set by random sampling with replacement from the corresponding original data set. To make the results more comparable across different methods, step sizes, and checkpoint densities, we fixed the number of new transactions as 6000, a 6% increment of the base data. Again, these 6000 new transactions were generated by random sampling from the original data sets.\nThe difference between a step and a checkpoint is that a step is the number of new transactions after which we need an updated output of the strongly-correlated-pairs query, while a checkpoint is where we update the candidate list of item pairs. For example, an online store, which updates its bundle recommendations once they have received 1000 new transactions, has a step size 1000. The checkpoint size, however, is a parameter for the CHECK-POINT method, which the store can choose regarding how many transactions are to be collected between two neighboring checkpoints. Obviously, checkpoints do not apply to r-TAPER and SAVE-ALL, but these three methods can be compared with respect to the number of transactions processed.\nFor each data set, we carried out three groups of experiments. The first group aims to evaluate the effect of the checkpoint density for a fixed step size. Specifically, we fix the step size as 100 and use checkpoint sizes 200, 500, 1000, 1500, and 2000. The second group of experiments evaluates the effect of step sizes, in which we fix the checkpoint size as 1000, whereas try step sizes 10, 50, 100, 200, and 250. The last group of experiments fixes the ratio of checkpoint size versus step size. When using different step sizes, such as 10, 50, 100, 150, and 200, we insert a checkpoint at the end of every 10 steps, no matter what the step size is. The experimental groups are summarized in Tables 3, 4, and 5.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Computational Performance", "text": "In this subsection, we show a comparison of computational performance for three algorithms: r-TAPER, SAVE-ALL, and CHECK-POINT. Figure 6 illustrates the running time of each step for the chess, connect, and pumsb datasets. Unsurprisingly, the  SAVE-ALL algorithm costs the least time, because the pairwise frequencies are stored in the memory. r-TAPER takes much longer time than SAVE-ALL and CHECK-POINT, because all the pairwise frequencies are not available and need to be counted every step when new data become available. It is reasonable according to the r-TAPER algorithm that the higher the threshold, the more pairs are pruned and the less computation is needed. For different thresholds, the time consumed by the CHECK-POINT algorithm is almost always as little as SAVE-ALL, except for the running time at checkpoints, where CHECK-POINT need to take time for building a new candidate list of item pairs.\nEven though CHECK-POINT takes longer time to update the candidate list at the checkpoints than one-running time of r-TAPER, overall the CHECK-POINT algorithm takes much less time than r-TAPER. In Figure 7, we illustrate the accumulative time at each step. Because the SAVE-ALL method is so fast, as time goes, its accumulative time grows very slowly. On the contrary, the accumulative time by r-TAPER increases much faster. The CHECK-POINT algorithm lies in between SAVE-ALL and r-TAPER. Its accumulative time increases slowly except for checkpoints where larger jumps can be observed. However, the overall trend shows that it increases much slower than r-TAPER.\nFigure 8 shows a comparison of the CHECK-POINT, r-TAPER, and SAVE-ALL algorithms at different threshold levels. We have already known that the higher the thresholds, the more item pairs are pruned by the r-TAPER algorithm; however in the dynamically growing databases, as the threshold goes down, the running time of CHECK-POINT increases much more slowly than that of r-TAPER.\nFigure 9 shows the effect of the checkpoint density on the running time. Because the checkpoints are the most costly steps, the more frequent we update the candidate list, the more time in total we will need. However, using too few checkpoints will require more space. We will show this tradeoff in the next subsection.\nTo study the effect of step sizes, we fix the checkpoint densities and plot the accumulative running time in Figure 10. We can see that all the curves are almost overlapping each other. Again, the reason is that the checkpoints are the most costly steps, thus the choice of step sizes does not affect the performance greatly. The implications on real world applications is that sizes of steps, where we want an update-to-date output of all the strongly correlated pairs, can be determined by the application itself. All we need to leverage is the choice of the checkpoint density, so that both the running time and the space required is reasonably balanced and practical.\nFinally, when fixing the step size and the checkpoint density, Figure 11 shows the effect of correlation thresholds on the running time. It is easy to see that, overall, the lower the threshold, the more running time is needed. Our experiments on other data sets and parameters show similar trends. Due to the page limit, we do not present these experimental results in this paper.", "publication_ref": [], "figure_ref": ["fig_9", "fig_10", "fig_12", "fig_13", "fig_0", "fig_0"], "table_ref": []}, {"heading": "The Use of Space", "text": "In this section, we investigate the performance of the CHECK-POINT algorithm in terms of the use of space. Along this line, our goal is to check how many item pairs can   be pruned at the checkpoints. In other words, these pruned item pairs will not be maintained in the candidate list\nTo study the pruning effect of different densities of checkpoints, we fix the step size and the correlation threshold, and then get the plots in Figure 12. In this figure, similar trends can be found for different data sets.\nFirst of all, the denser the checkpoints, the fewer candidates are needed. In each of the subgraphs, each data point corresponds to a checkpoint. We can see that curves with fewer checkpoints lie higher than those with more checkpoints. This indicates that the less frequent the checkpoint is, the more candidates are needed to be stored.\nSecondly, as time goes, the number of candidates may vary. Our experiments show that the number of candidates increases or decreases only slightly over time. The reason is that we generated the data sets uniformly at random, which eliminates the evolving trend over time in the original data sets. However, this may not be the case in practice.\nFinally, the pruning ratio is data dependent. As an example, Table 6 shows the sizes of candidate lists for three test data sets. In the table, we can see that that the number of candidate item pairs for chess is only 4, meaning that 99.86% of all possible item pairs are pruned. Instead of saving the frequencies of all 2775 item pairs, we only need to track the change of 4 pairs. Similarly, the connect data set has a pruning ratio of 97.60%. The pruning ratio for pumsb is only 45.57%, which explains why the computational performance of CHECK-POINT on the pumsb data set is not as good as on the other two data sets. The reason is that the chess and connect data sets are much denser than pumsb. Our proposed CHECK-POINT algorithm works better on dense data sets. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_5"]}, {"heading": "CONCLUDING REMARKS", "text": "In this paper, we studied the problem of correlation computing in large and dynamically growing data sets. Specifically, we proposed a CHECK-POINT algorithm, which can incrementally search all the item pairs with correlations above a user-specified minimum correlation threshold. The key idea is to establish a computation buffer by setting a checkpoint for dynamic input data. This checkpoint can be exploited to identify a list of candidate pairs, which are maintained and computed for correlations as new transactions are added into the database. However, if the total number of new transactions is beyond the check point, a new candidate list is generated by the new checkpoint. Experimental results on real-world data sets show that CHECK-POINT can compact the use of memory space by maintaining a candidate pair list, which is only a very small portion of all the item pairs. Also, CHECK-POINT can significantly reduce the correlation computing cost in dynamic data sets with a large number of transactions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "This research was partially supported by the Rutgers Seed Funding for Collaborative Computing Research. Also, this research was supported in part by a Faculty Research Grant from Rutgers Business School-Newark and New Brunswick.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Mining association rules between sets of items in large databases", "journal": "", "year": "1993", "authors": "R Agrawal; T Imielinski; A Swami"}, {"ref_id": "b1", "title": "Market Models: A Guide to Financial Data Analysis", "journal": "John Wiley & Sons", "year": "2001", "authors": "C Alexander"}, {"ref_id": "b2", "title": "Programming Pearls", "journal": "Addison-Wesley, Inc", "year": "2000", "authors": "J Bentley"}, {"ref_id": "b3", "title": "Beyond market baskets: Generalizing association rules to correlations", "journal": "", "year": "1997", "authors": "S Brin; R Motwani; C Silverstein"}, {"ref_id": "b4", "title": "Applied multiple regression/correlation analysis for the behavioral sciences", "journal": "Lawrence Erlbaum Associates", "year": "2003", "authors": "J Cohen; P Cohen; S West; L Aiken"}, {"ref_id": "b5", "title": "Applied Multiple Regression/Correlation Analysis for the Behavioral Science", "journal": "Lawrence Erlbaum Assoc", "year": "2002", "authors": "P Cohen; J Cohen; S G West; L S Aiken"}, {"ref_id": "b6", "title": "Introduction to Calculus and Analysis Volume II/1: Chapters 1 -4 (Classics in Mathematics)", "journal": "Springer", "year": "1999", "authors": "R Courant; F John"}, {"ref_id": "b7", "title": "Empirical bayes screening for multi-item associations", "journal": "", "year": "2001", "authors": "W Dumouchel; D Pregibon"}, {"ref_id": "b8", "title": "Cords: Automatic discovery of correlations and soft functional dependencies", "journal": "", "year": "2004", "authors": "I F Ilyas; V Markl; P J Haas; P Brown; A Aboulnaga"}, {"ref_id": "b9", "title": "The computational complexity of high-dimensional correlation search", "journal": "", "year": "2001", "authors": "C Jermaine"}, {"ref_id": "b10", "title": "Playing hide-and-seek with correlations", "journal": "", "year": "2003", "authors": "C Jermaine"}, {"ref_id": "b11", "title": "Analysis of matched mrna measurements from two different microarray technologies", "journal": "Bioinformatics", "year": "2002", "authors": "W Kuo; T Jenssen; A Butte; L Ohno-Machado; I Kohane"}, {"ref_id": "b12", "title": "The Analysis of Cross-classifications", "journal": "The Free Press", "year": "1977", "authors": "H T Reynolds"}, {"ref_id": "b13", "title": "Statistical Analysis in Climate Research", "journal": "Cambridge University Press", "year": "2002-02", "authors": "H V Storch; F W Zwiers"}, {"ref_id": "b14", "title": "Exploiting a support-based upper bound of pearson's correlation coefficient for efficiently identifying strongly correlated pairs", "journal": "", "year": "2004", "authors": "H Xiong; S Shekhar; P Tan; V Kumar"}, {"ref_id": "b15", "title": "Taper: A two-step approach for all-strong-pairs correlation query in large databases", "journal": "IEEE Transactions on Knowledge and Data Engineering (TKDE)", "year": "2006-04", "authors": "H Xiong; S Shekhar; P.-N Tan; V Kumar"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An Illustrate of the Checkpoint Process.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "therefore u reaches its maximum when x takes its minimum possible value 0. On the other hand, if N \u2212 Na \u2212 N b \u2265 \u2206N , then \u2202u/\u2202x = N \u2212Na\u2212N b +\u2206N \u22122x \u2265 \u2206N +\u2206N \u22122x = 2(\u2206N \u2212x) \u2265 0. u reaches its maximum when x takes the maximum value \u2206N . Now we have completed the proof of Lemma 1.", "figure_data": ""}, {"figure_label": "28", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Lemma 2 . 8 )28Given N , Na, and \u2206N , the minimum possible value for v in Equation 6 is vmin = min{h(Na), h(Na + \u2206N )} (where h(x) = x(N + \u2206N \u2212 x) is a function with respect to x, defined on the range [0, N + \u2206N ].Proof. Since v = N \u2032 a (N \u2032 \u2212 N \u2032 a ) = (Na + \u2206Na)(N + \u2206N \u2212 Na \u2212 \u2206Na) = h(Na + \u2206Na), finding the minimum of v is equivalent to finding the minimum value of function h(x) within the range of Na + \u2206Na. Since 0 \u2264 \u2206Na \u2264 \u2206N , we have Na \u2264 Na + \u2206Na \u2264 Na + \u2206N . Now we will prove that the minimum possible value of h(x) in the range [Na, Na + \u2206N ] is either h(Na) or h(Na + \u2206N ).Obviously h(x) is a quadratic function of x. It is concave and symmetric with respect to x = (N + \u2206N )/2. Thus, the further x = Na + \u2206Na is from (N + \u2206N )/2, the smaller f (x) is. Because Na \u2265 0 and Na + \u2206N\u2264 N + \u2206N , Na + \u2206Na \u2208 [Na, Na + \u2206N ] \u2286 [0, N + \u2206N ].The minimum value of f (Na + \u2206Na) must be at some end of the range [Na, Na + \u2206N ], depending on whether Na or Na + \u2206N is further from (N + \u2206N )/2. Now Lemma 2 has been proven.Lemma 3. Given N , N b , and \u2206N , the minimum possible value for w in Equation 6 is wmin = min{h(N b ), h(N b + \u2206N )} (9) where h(x) = x(N + \u2206N \u2212 x) is a function with respect to x, defined on the range [0, N + \u2206N ].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "ALGORITHM r-TAPER(D,D \u2206 ,\u03b8) Input: D: the original database D \u2206 : the set of new transactions \u03b8: the threshold for pair-wise \u03c6 correlation Output: L: list of item pairs with \u03c6 \u2265 \u03b8.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: The r-TAPER Algorithm", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "ALGORITHMSAVE-ALL(M ,D \u2206 ,\u03b8) Input: M : M [i, j] is the frequency of item pair {i, j} D \u2206 : the set of new transactions \u03b8: the threshold for pair-wise correlation Output: L: list of item pairs with \u03c6 \u2265 \u03b8.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 3 :3Figure 3: The Pseudocode of SAVE-ALL", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "ALGORITHMCHECK-POINT (CL,D,D \u2206 ,\u03b8,Check) Input: CL: the candidate list D: the original database D \u2206 : a set of new transactions \u03b8: threshold for pair-wise correlation Check: a boolean variable indicating whether there is a checkpoint at the end of this step Output: L: list of item pairs with \u03c6 \u2265 \u03b8.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 :5Figure 5: The Pseudocode of the UpdateCandi-dateList subprocedure in CHECK-POINT.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 6 :6Figure 6: The running time at each step of the CHECK-POINT, r-TAPER and SAVE-ALL algorithms on chess, connect, and pumsb datasets. (S = 100, C = 1000, \u03b8 = 0.5).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 7 :7Figure 7: Accumulative running time at each step of the CHECK-POINT, r-TAPER and SAVE-ALL algorithms on chess, connect, and pumsb datasets. (S = 100, C = 1000, \u03b8 = 0.5).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 8 :8Figure 8: Accumulative running time of the CHECK-POINT, r-TAPER and SAVE-ALL algorithms on chess (S = 100, C = 1000).", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 9 :9Figure 9: The accumulative running time at each step of the CHECK-POINT algorithm on chess. (S = 100, \u03b8 = 0.6).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 10 :10Figure 10: The accumulative running time at each step of the CHECK-POINT algorithm on chess. (C = 1000, \u03b8 = 0.6).", "figure_data": ""}, {"figure_label": "1211", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 12 :Figure 11 :1211Figure 12: Number of candidate pairs at each checkpoint for chess, connect, and pumsb (S = 100, \u03b8 = 0.9).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "A two-way contingency table of item A and item B.", "figure_data": "01RowTotal0P(00)P (01)P (0+)1P(10)P(11)P (1+)Column TotalP (+0)P (+1)N"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Basic Characteristics of the Data Sets.", "figure_data": "Data set # Items # Transactions Sourcechess753196UCI Repositoryconnect12767557UCI Repositorypumsb211349046FIMITable"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Experimental Group I.", "figure_data": "Step Size Checkpoint Size # Steps # Checkpoints100200230100500512100100010610015001541002000203"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Experimental Group II.", "figure_data": "Step Size Checkpoint Size # Steps # Checkpoints10100010065010002061001000106200100056250100046"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Experimental Group III.", "figure_data": "Step Size Checkpoint Size # Steps # Checkpoints101001060505001012100100010615015001042002000103Experimental Platform. All the experiments were per-formed on a Dell Optiplex 755 Minitower with Intel 2 Quadprocessor Q6600 and 4 GB of memory running the MicrosoftWindows XP Professional operating system."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The Sizes of Candidate Lists (S = 10, C = 100, \u03b8 = 0.7).", "figure_data": "Data Set #Items #Pairs #Cand'sRatiochess752775499.86%connect127800119197.60%pumsb21132231328 1214496 45.57%"}], "formulas": [{"formula_id": "formula_1", "formula_text": "\u03c6 {a,b} = supp(a, b) \u2212 supp(a)supp(b) p supp(a)supp(b)(1 \u2212 supp(a))(1 \u2212 supp(b))(2)", "formula_coordinates": [2.0, 323.03, 104.06, 232.89, 28.63]}, {"formula_id": "formula_2", "formula_text": "upper(\u03c6 {a,b} ) = s supp(b) supp(a) s 1 \u2212 supp(a) 1 \u2212 supp(b)(3)", "formula_coordinates": [2.0, 344.01, 172.03, 211.91, 31.12]}, {"formula_id": "formula_3", "formula_text": "\u03c6 {a,b} = N N ab \u2212 NaN b p Na(N \u2212 Na)N b (N \u2212 N b ) .(4)", "formula_coordinates": [2.0, 352.76, 281.69, 203.16, 28.62]}, {"formula_id": "formula_4", "formula_text": "\u03c6 \u2032 {a,b} = N \u2032 N \u2032 ab \u2212 N \u2032 a N \u2032 b p N \u2032 a (N \u2032 \u2212 N \u2032 a )N \u2032 b (N \u2032 \u2212 N \u2032 b ) ,(5)", "formula_coordinates": [3.0, 350.08, 239.79, 205.84, 30.02]}, {"formula_id": "formula_5", "formula_text": "\u2032 = N + \u2206N , N \u2032 a = Na + \u2206Na, N \u2032 b = N b + \u2206N b , and N \u2032 ab = N ab + \u2206N ab .", "formula_coordinates": [3.0, 316.81, 270.45, 239.11, 21.83]}, {"formula_id": "formula_6", "formula_text": "Let u = N \u2032 N \u2032 ab \u2212 N \u2032 a N \u2032 b , v = N \u2032 a (N \u2032 \u2212 N \u2032 a ), and w = N \u2032 b (N \u2032 \u2212 N \u2032 b )", "formula_coordinates": [3.0, 316.81, 511.05, 239.1, 27.34]}, {"formula_id": "formula_7", "formula_text": "\u03c6 \u2032 {a,b} = u \u221a vw .(6)", "formula_coordinates": [3.0, 397.52, 544.72, 158.4, 21.07]}, {"formula_id": "formula_8", "formula_text": "u = N \u2032 N \u2032 ab \u2212 N \u2032 a N \u2032 b = (N + \u2206N )(N ab + \u2206N ab ) \u2212 (Na + \u2206Na)(N b + \u2206N b )", "formula_coordinates": [3.0, 359.99, 709.02, 195.92, 16.88]}, {"formula_id": "formula_9", "formula_text": "umax = 8 < : f (0) if t \u2264 \u2212\u2206N ; f (x * ) if \u2212 \u2206N \u2264 t \u2264 \u2206N ; f (\u2206N ) if t \u2265 \u2206N,(7)", "formula_coordinates": [4.0, 78.64, 132.5, 214.26, 37.69]}, {"formula_id": "formula_10", "formula_text": "f (x) = (N + \u2206N )(N ab + x) \u2212 (Na + x)(N b + x), t = N \u2212 Na \u2212 N b , and x * = (N \u2212 Na \u2212 N b + \u2206N )/2.", "formula_coordinates": [4.0, 53.8, 171.8, 239.09, 25.78]}, {"formula_id": "formula_11", "formula_text": "\u2264 \u2206N b . Let \u2206N ab = x, x \u2265 0; \u2206Na = x + c1, c1 \u2265 0; \u2206N b = x + c1 + c2, c2 \u2265 0.", "formula_coordinates": [4.0, 53.8, 207.31, 239.09, 25.77]}, {"formula_id": "formula_12", "formula_text": "Similarly, because \u2202u/\u2202c1 = \u2212(N b + x + c1 + c2) \u2212 (Na + x + c1) < 0, u takes the maximum value when c1 = 0. As a result, \u2202u/\u2202x = (N + \u2206N ) \u2212 (N b + x + c1 + c2) \u2212 (Na + x + c1) = N \u2212 Na \u2212 N b + \u2206N \u2212 2x. Since \u2202 2 u/\u2202x 2 = \u22122 < 0, u", "formula_coordinates": [4.0, 53.8, 280.54, 239.09, 57.16]}, {"formula_id": "formula_13", "formula_text": "x * = (N + \u2206N \u2212 Na \u2212 N b \u2212 2c1 \u2212 c2)/2 = (N \u2212 Na \u2212 N b + \u2206N )/2.", "formula_coordinates": [4.0, 53.8, 331.27, 239.09, 27.35]}, {"formula_id": "formula_14", "formula_text": "\u2264 x = \u2206N ab \u2264 \u2206N , the above value can be reached only if \u2212\u2206N \u2264 N \u2212 Na \u2212 N b \u2264 \u2206N . If N \u2212 Na \u2212 N b \u2264 \u2212\u2206N , then \u2202u/\u2202x = N \u2212 Na \u2212 N b + \u2206N \u2212 2x \u2264 \u2212\u2206N + \u2206N \u2212 2x = \u22122x \u2264 0,", "formula_coordinates": [4.0, 53.8, 353.76, 239.1, 46.7]}, {"formula_id": "formula_15", "formula_text": "[N b , N b + \u2206N ] is either h(N b ) or h(N b + \u2206N ).", "formula_coordinates": [4.0, 316.81, 222.84, 188.68, 9.01]}, {"formula_id": "formula_16", "formula_text": "correlation coefficient \u03c6 = u/ \u221a vw by Lemma 1, Lemma 2,", "formula_coordinates": [4.0, 316.81, 260.58, 239.07, 15.43]}, {"formula_id": "formula_17", "formula_text": "Lemma 4. A loose upper bound for the new \u03c6 correlation coefficient at the next checkpoint, \u03c6 \u2032 = u/ \u221a vw, is upper(\u03c6 \u2032 ) = umax \u221a vminwmin ,(10)", "formula_coordinates": [4.0, 316.81, 298.3, 239.11, 45.55]}, {"formula_id": "formula_18", "formula_text": "D \u2190 D \u222a D \u2206 2.", "formula_coordinates": [5.0, 77.32, 136.43, 75.39, 17.36]}, {"formula_id": "formula_19", "formula_text": "M [i, j] \u2190 M [i, j] + 1 4.", "formula_coordinates": [5.0, 77.32, 550.62, 135.58, 17.35]}, {"formula_id": "formula_20", "formula_text": "N ij \u2190 N ij + 1", "formula_coordinates": [5.0, 405.86, 360.09, 54.32, 8.58]}, {"formula_id": "formula_21", "formula_text": "D \u2190 D \u222a D \u2206 18. CL \u2190 UpdateCandidateList(D,\u2206N )", "formula_coordinates": [5.0, 329.53, 477.74, 178.86, 17.35]}, {"formula_id": "formula_22", "formula_text": "ALGORITHM UpdateCandidateList(D,\u2206N ) 1. CL \u2190 \u2205 2.", "formula_coordinates": [6.0, 64.86, 64.87, 175.07, 26.16]}], "doi": ""}