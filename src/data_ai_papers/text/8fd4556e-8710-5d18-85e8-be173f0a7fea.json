{"title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation", "authors": "Kun Zhou; Yifan Li; Wayne Xin Zhao; Ji-Rong Wen", "pub_date": "", "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs). To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance. By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task. In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich prelearned knowledge of BART and the iterative refining paradigm of DDM. Besides, we also propose the iterative self-prompting strategy to further improve the generation quality. Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods. Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.", "sections": [{"heading": "Introduction", "text": "Text-to-text generation (Sutskever et al., 2014;Vaswani et al., 2017) is an essential task in natural language processing, which aims to generate human-like texts satisfying the task demand. To efficiently generate high-quality texts, nonautoregressive (NAR) models (Gu et al., 2018;Lee et al., 2018) are widely explored for text-to-text generation by predicting all tokens in the target text simultaneously, having a lower inference latency.\nDespite the efficiency, the generation accuracy of NAR models generally underperform autore- \u2020 \u2020 Corresponding author  gressive (AR) models with the token-by-token generation, since parallel token prediction cannot effectively capture the dependency among the tokens. To enhance the generation quality, a variety of techniques have been proposed for NAR models, with either improved architectures (Qian et al., 2021) or training methods . More recently, inspired by the success of diffusion models in computer vision (Ho et al., 2020;, they have been introduced to improve NAR models for text-to-text generation Li et al., 2023). As shown in Table 1, these studies typically adopt the continuous diffusion method on the latent space of token embeddings in the NAR manner, and iteratively refine all the target token embeddings via a parameterized denoising process. However, these attempts are highly limited by the discrete nature of text, and thus it is necessary to incorporate special strategies to adapt continuous diffusion models for text generation. Typically, they rely on an additional rounding step (Li et al., 2022b) to map the generated embeddings into tokens, and add corresponding loss during training. However, the added step and training loss would burden the diffusion models, causing them hungry for more training steps and data to capture the mapping relation between input and output. Although large-scale pre-trained language models (PLMs) (Devlin et al., 2019;Lewis et al., 2020) seem to be a promising solution to alleviate this hunger problem, due to the large model discrepancy, it is difficult to use existing PLMs for improving the text generation models when integrating with continuous diffusion models, even leading to performance degradation (Li et al., 2022b).\nTo address these issues, we aim to develop a more effective approach to integrating diffusion models and PLMs for NAR text-to-text generation. Instead of using continuous diffusion, we utilize discrete diffusion (Austin et al., 2021;Gu et al., 2022) for text generation, which performs denoising on discrete states (e.g., vocabulary) to recover the original tokens. It is more suitable for modeling discrete text data, making it feasible to develop more unified and compatible solutions to integrate diffusion models and well-trained PLMs for improving NAR text generation. However, both discrete diffusion models and PLMs neither naturally fit with each other nor the NAR text-to-text generation manner, making it hard to directly combine them for improving the NAR generation quality.\nIn this paper, we propose Diffusion-NAT, a selfprompting discrete diffusion model using PLMs for NAR text-to-text generation. The core contribution lies in that we unify the inference process of PLMs and denoising process of discrete diffusion models into the same masked token recovering task in the NAR manner. In this way, PLMs can play the role of the parameterized denoiser in discrete diffusion models, hence we can combine the merits of both diffusion models (using iterative refining generation) and PLMs (with rich semantic knowledge) for improving NAR text generation. Specifically, we select the Seq2Seq PLM, BART (Lewis et al., 2020) as our backbone by revising its decoding process into the NAR masked tokens recovering task. Then, we adjust the typical discrete diffusion method to better fit the PLM by adding mask tokens as noise, revising the learning objective and removing the time step embeddings. Further, as our approach performs the denoising process fully based on the PLM, we devise an iterative self-prompting strategy to guide the PLM performing multi-turn deliberation and refinement on the intermediate generated results, to enhance the quality of the final output.\nTo verify the effectiveness of our approach, we conduct extensive experiments on seven text-to-text generation datasets. Experimental results show that our approach can outperform competitive NAR text generation methods, e.g., improving the best NAR models by +2.48 BLEU-2 on PersonaChat, +4.33 Distinct-2 on DailyDialog.Our approach even surpasses state-of-the-art autoregressive PLMs, e.g., Ours (62.68) v.s. BART (49.59) on BLEU-2 in DailyDialog, and Our (44.2) v.s. BART (38.3) on ROUGE-L in MSNews.Besides, our approach also supports DDIM (Song et al., 2021a) for trading off the time cost and the generation quality during inference. By setting proper diffusion steps (e.g., 100 and 2), our approach can outperform competitive AR and NAR models with similar inference latency, respectively.", "publication_ref": ["b45", "b48", "b10", "b15", "b35", "b13", "b2", "b20", "b4", "b16", "b20", "b0", "b12", "b16", "b41"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Related Work", "text": "Non-Autoregressive Text Generation. Compared with autoregressive (AR) methods (Lewis et al., 2020) that need to predict the target text in a token-by-token manner, Non-autoregressive (NAR) methods can generate all tokens in parallel, which can greatly reduce the inference latency (Gu et al., 2018;Ghazvininejad et al., 2019). However, in this way, NAT methods can not fully capture the dependency relations among tokens during decoding, leading to the sacrifice of accuracy. To address it, existing works adopt several training and inference strategies to improve the performance of NAR methods, e.g., knowledge distillation , glancing sampling (Qian et al., 2021), iterative decoding (Geng et al., 2021) and largescale pre-training Li et al., 2022a). In this work, we introduce the discrete diffusion model into NAR text generation, narrowing the performance gap with AR methods.\nPLMs for Text Generation. Pre-trained language models (PLMs) have shown remarkable performance in generating human-like texts (Li et al., 2021). After pre-training, most existing PLMs (Raffel et al., 2020) are fine-tuned following the AR paradigm for text generation. In this way, they either reformulate generation tasks into the language model format (e.g., GPT (Radford et al., 2019)), or leverage the sequence-to-sequence manner to generate the text using an autoregressive decoder (e.g., BART (Lewis et al., 2020)). However, as these PLMs only focus on fine-tuning under the AR paradigm, they can not be directly used for NAR text generation. Recently, BANG  and ELMER (Li et al., 2022a) rely on large-scale pre-training for improving the NAR text generation. Considering the pre-training cost, we aim to efficiently adapt BART into an effective NAR model with diffusion models.\nDiffusion Models for Text Generation. Diffusion models (DM) (Ho et al., 2020;Song et al., 2021b) are a class of latent variable models that can progressively denoise a random Gaussian noise into a data example. Existing DMs can be roughly categorized into continuous diffusion models (Ho et al., 2020;Tang et al., 2023a;Nikolaidou et al., 2023) and discrete diffusion models (Austin et al., 2021;Qian et al., 2022), which perform diffusion on continuous signals and discrete states, respectively. Recently, DMs have been utilized for text generation and have demonstrated superiority in controllable text generation tasks (Tang et al., 2023b;Li et al., 2022b). For text-to-text generation tasks, existing works generally follow the continuous diffusion paradigm, and improve the performance by refining the model architecture , adding regularization (Gao et al., 2022) and large-scale pre-training (Lin et al., 2022). In this work, we introduce discrete diffusion models into text-to-text generation tasks, and utilize a PLM to improve it.", "publication_ref": ["b16", "b10", "b8", "b35", "b7", "b18", "b35", "b37", "b36", "b16", "b18", "b13", "b43", "b13", "b46", "b0", "b34", "b20", "b6", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary", "text": "Problem Statement. This work focuses on textto-text generation tasks using non-autoregressive (NAR) models. Generally, text-to-text generation tasks (Sutskever et al., 2014;Vaswani et al., 2017) (e.g., dialog and summarization) can be formulated as modeling the conditional probability P (Y |C), where\nC = {c 1 , c 2 , \u2022 \u2022 \u2022 , c m } and Y = {y 1 , y 2 , \u2022 \u2022 \u2022 , y n }\ndenote the input text and output text respectively, both consisting of a sequence of tokens from a vocabulary V.\nDifferent from AR models with the left-to-right token-by-token generation manner, NAR models (Gu et al., 2018;Lee et al., 2018) predict all tokens of the output text Y simultaneously, where each token y i is predicted only based on the input text C. Thus, the conditional probability can be factorized as\nP (Y |C) = n i=1 P (y i |C),(1)\nDiffusion Models. Diffusion models (DM) (Ho et al., 2020;Song et al., 2021b) sample an example from a data distribution p(x) by gradually denoising a random noise. Typically, starting from a noise x T , the denoising process (also so-called reverse process) can be regarded as a Markov process, where the noises at T \u2212 1, T \u2212 2, \u2022 \u2022 \u2022 , 0 steps are progressively predicted and removed to obtain the latent variables x T \u22121 , x T \u22122 , \u2022 \u2022 \u2022 , until reaching the final sample x 0 . Conversely, given the sample x 0 , we can generate x 1 , x 2 , \u2022 \u2022 \u2022 , x T as a Markov chain, denoted as the forward process:\nq(x t |x t\u22121 ) = N ( 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I), (2\n)\nwhere \u03b2 t \u2208 (0, 1) is the pre-defined scaling of noise variance at the t-th step. Given the above forward process as prior, DMs are trained to reverse it following the denoising process for recovering x 0 , where each step is parameterized as:\np(x t\u22121 |x t ) = N (\u00b5 \u03b8 (x t , t), \u03a3 \u03b8 (x t , t)), (3\n)\nwhere \u00b5 \u03b8 (\u2022) and \u03a3 \u03b8 (\u2022) can be implemented by a U-Net (Ronneberger et al., 2015) or Transformer (Vaswani et al., 2017), and time step embeddings are adopted to represent t.\nDiscrete Diffusion Models. Discrete diffusion models (Austin et al., 2021;Gu et al., 2022) perform the forward and denoising processes in discrete random variables with K categories, where K = |V| for text data. For a sentence, x 0 is the vector consisting of the indexes of its contained tokens, and the forward process of adding noise is\nq(x t |x t\u22121 ) = v \u22a4 (x t )Q t v(x t\u22121 ),(4)\nwhere v(x t ) maps each token index from x t into K-dimension one-hot vector, Q t is the probability transition matrix and [Q t ] i,j denotes the probability of the token i to be replaced by the token j. In this way, according to Bayes' theorem, the denoising process q(x t\u22121 |x t , x 0 ) can be deduced as:\nq(xt\u22121|xt, x0) = v \u22a4 (xt)Qtv(xt\u22121)v \u22a4 (xt\u22121)Qt\u22121v(x0) v \u22a4 (xt)Qtv(x0)(5)\nwhereQ\nt = Q 1 Q 2 \u2022 \u2022 \u2022 Q t .\nBased on the above prior, we can use a parameterized model p \u03b8 (x t\u22121 |x t , t) to learn the denoising process.\n| | Hello , nice to meet you . Hello , [M] to meet you [M] [M] [M] [M] [M] [M] [M] [M] Hello , [M] to [M] you [M] \u2026 \u2026\nDialogue context: Greetings, traveller from beyond the fog.", "publication_ref": ["b45", "b48", "b10", "b15", "b13", "b43", "b40", "b48", "b0", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "BART Encoder", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-attention", "text": "Hi, nice to see you.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Self-Prompting", "text": "Denoising Step Forward Step [M], [M] to [M] you [M]", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BART Decoder", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "NAR Masked Tokens Recovering", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "K-turn Iterations", "text": "Figure 1: The overview of our Diffusion-NAT. We show an example that generates a response in the t-th step using K-turn self-prompting. The given dialog context and the K-turn prompt (i.e., estimated\u0176 0 ) are fed into BART encoder, and the response in the t-th Y t is fed into BART decoder for estimating the original tokens. sion model and the Seq2Seq PLM BART, for improving NAR text-to-text generation. The overview of our approach is shown in Figure 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "Since discrete diffusion models (DDM) and BART adopt different ways for training (i.e., noise prediction and masked text infilling respectively), it is hard to directly integrate both for NAR textto-text generation. Our solution is to regard the mask token [MASK] of BART as the noise in DDM, and incorporate an absorbing state [MASK] into the Markov transition matrices. In this way, the forward process of DDM gradually replaces all the tokens by [MASK], and the denoising process can be reformulated as a NAR Masked Tokens Recovering (NMTR) task:\nf NMTR ([M], \u2022 \u2022 \u2022 , [M]) = {y 1 , \u2022 \u2022 \u2022 , y n }, (6)\nwhere [M] denotes the [MASK] token of BART. To apply this framework for NAR text generation, we further make adaptations for BART and DDM. For BART, its pre-training task of masked text infilling is similar to the above objective except that it is in a NAR manner, and thus we revise the decoding process of BART to support the NAR inference in Section 4.2. For DDM, we learn to predict the original tokens instead of noise and remove the time step embeddings in Section 4.3, for better adaptation to BART. In this way, we can unify the inference process of BART and the denoising process of discrete diffusion models with the same formulation of NAR masked tokens recovering.\nWith this unified formulation, DDM can fully rely on BART to conduct the denoising process, with no need for additional parameters or specific training. In this way, the generated results based on BART can be iteratively refined via the denoising process, leading to improved generation text. Since BART is employed as the backbone of our approach, we can naturally leverage advanced techniques of PLMs to improve the diffusion process, e.g., prompt learning (Liu et al., 2021b). Thus, we propose the iterative self-prompting strategy to perform multi-turn deliberation and refinement on the intermediate generated results in Section 4.4, further enhancing the quality of the output.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Adapting BART for NAR Generation", "text": "Since BART utilizes a token-by-token autoregressive mechanism for decoding, this part discusses how to revise its decoding process to fit the NAR generation framework.\nBART. BART (Lewis et al., 2020) is a Seq2Seq PLM that has been widely used on various text-totext generation tasks. It adopts the encoder-decoder Transformer architecture. Given the input text C, the encoder produces its representation vectors E, and the decoder performs cross-attention with E to inject the condition from the input text. During pre-training, the masked text infilling task is mainly adopted to learn the model parameters on a large-scale corpus, aiming to recover the masked span from the input text. During inference, using a special start token as the initial input of the decoder, the output text will be generated token by token.\nRevised NAR Decoding Process. In the denoising process of our approach, BART is employed to recover the masked tokens from the noised target text at each time step. Thus, we revise the decoding process of BART into the NAR manner that can recover all masked tokens simultaneously. Concretely, at the t-step, given the condition text C and the noised target text Y t containing [MASK] tokens, we feed them into the encoder and decoder of BART respectively, and simultaneously recover all the [MASK] tokens into the target tokens as:\nBART({y (t) 1 \u2022 \u2022 \u2022 [M]}, C) = {y (t\u22121) 1 \u2022 \u2022 \u2022 y (t\u22121) n }, (7)\nwhere y (t) 1 is the token of the first position at the t-th step. In this way, the decoding process follows the unified formulation in Eq. 6. Thus, we can employ BART in the denoising process by leveraging its pre-learned knowledge and generation capacity.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Adapting DDM for NAR Generation", "text": "In this part, we discuss how to adapt the discrete diffusion model (DDM) to NAR masked tokens recovering for text generation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Markov Transition Matrices with [MASK].", "text": "As introduced in Section 3, discrete diffusion models rely on the probability transition matrix Q t to perform the forward and denoising processes over the state space. To align DDM with the NAR decoding process of BART (Section 4.2), we incorporate the [MASK] token as the absorbing state of the Markov transition matrices. Concretely, at the t-th step of the forward process, if token i is not the [MASK] token, it has the probabilities of \u03b1 t and \u03b3 t being unchanged and replaced by the [MASK] token respectively, leaving the probability of \u03b2 t = 1 \u2212 \u03b1 t \u2212 \u03b3 t transiting to other tokens in V as:\n[Qt]i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1t, if j = i, \u03b3t, if j = [M], 1 \u2212 \u03b1t \u2212 \u03b3t, otherwise,(8)\nwhere \u03b1 t and \u03b3 t are determined by the pre-defined noise schedule, e.g., cosine schedule (Nichol and   (Li et al., 2022b;, we predict all the original tokens Y 0 = {y\n(0) 1 , \u2022 \u2022 \u2022 , y(0)\nn } using BART in the NAR manner at each time step as:\nBART({y (t) 1 \u2022 \u2022 \u2022 [M]}, C) = {y (0) 1 \u2022 \u2022 \u2022 y (0) n }.(9)\nAs Y t usually contains several [MASK] tokens, the above process can be regarded as recovering all the masked tokens into the original ones, which is actually similar to the pre-training objective of BART.\nIn this way, the training objective is formulated as:\nL Y = \u2212 n i=1 log p \u03b8 (y (0) i |Y t , C)(10)\nwhere Y t denotes the intermediate recovered text in the t-th step. During inference, given Y t , our model first estimates\u0176 0 , and then adds the (t \u2212 1)-step noise into it for producing Y t\u22121 . The above process will iterate for multiple steps, until the final results of Y 0 are obtained.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Removing Time", "text": "Step Embeddings. As another difference in architecture, diffusion models typically incorporate time step embeddings to represent the time information (Ho et al., 2020;Song et al., 2021a), while BART has never set up corresponding time step embeddings. To reduce such discrepancy, we directly remove the time step embeddings from our diffusion process, so as to adapt DDM to reusing the whole architecture and all pretrained parameters of BART. Actually, as the discrete diffusion process is to progressively recover the all-[MASK] sequence, the PLM can directly acquire the time information by counting the number of [MASK] tokens. Further, by removing the time step embeddings, our diffusion approach can better integrate with other improvement techniques, e.g., DDIM method (Song et al., 2021a) with the non-Markov process for fast inference.", "publication_ref": ["b13", "b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Iterative Self-Prompting", "text": "In a typical denoising process, the denoising network relies on the condition C and Y t to estimat\u00ea Y 0 . However, at early steps, [MASK] tokens generally occupy the majority of Y t , causing the estimation to be more difficult. To reduce the inference difficulty at an early stage, we propose the iterative self-prompting strategy that endows our model with deliberation capacity via prefixed prompts.\nTraining with Self-Prompting. Inspired by the self-conditioning strategy (Chen et al., 2022), our self-prompting strategy focuses on improving the quality of\u0176 0 through multi-round checking and revision. Concretely, given Y t and C, we first utilize the PLM to produce the estimated\u0176 0 . Then, a\u015d Y 0 and C are two sequences of tokens, we regard Y 0 as the prompt of the PLM and prefix it with C to compose the new input condition\nC \u2032 = [\u0176 0 ; C].\nNext, the new condition C \u2032 and Y t are further fed into the encoder and decoder of the PLM respectively, where cross-attention in the decoder is employed to generate\u0176 0 by considering the previous estimation. During training, with a certain probability (e.g., 50%), we do not use the self-prompting strategy and only optimize the model parameter using Eq. 10. When integrated with this strategy, we first produce\u0176 0 and then construct C \u2032 for selfprompting, where the training objective becomes:\nL Y = \u2212 n i=1 log p \u03b8 (y (0) i |Y t ,\u0176 0 , C). (11\n)\nInference with Iterative Self-Prompting. To obtain a well-estimated\u0176 0 , we repeat the following self-prompting process for K times: we first estimate the original tokens\u0176 0 = {\u0177\n(0) 1 , \u2022 \u2022 \u2022 ,\u0177(0)\nn } based on the constructed new condition C \u2032 and then utilize it to replace the original prompt within C \u2032 . Each iterative process can be denoted as:\nBART {y (t) 1 \u2022 \u2022 \u2022 y (t) n }, {\u0177 (0) 1 \u2022 \u2022 \u2022\u0177 (0) n }, C = {y (0) 1 \u2022 \u2022 \u2022 y (0) n }.(12)\nIn this way, by setting proper hyper-parameter K, we can balance the accuracy of the estimated\u0176 0 and the time cost during inference.", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Settings", "text": "More details about the datasets, evaluation metrics, baselines, and implementations are shown in Appendix A, B, C and D, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "Dialog Generation. As shown in Table 2, for the coherence metrics (i.e., BLEU-1/2), the performance order of aforementioned baselines in the two dialog generation datasets is mostly consistently as: AR models > Semi-NAR models > NAR models. It indicates that AR models are more capable of generating coherent and fluent responses than NAR ones. A major reason is that AR models can better capture the dependency of tokens. Whereas, for the diversity metrics, AR models mostly underperform NAR models. The reason may be that AR models are easy to overfit into the frequently cooccurring tokens (e.g., I am OK.) in the training data, causing the \"safe response\" problem. Besides, the NAR methods using pre-training techniques (i.e., BANG and ELMER) can better balance the coherence and diversity metrics, and greatly outperform other NAR models. It demonstrates the effectiveness of large-scale pre-training. Finally, Diffusion-NAT mostly outperforms Semi-NAR and NAR models on all metrics. Different from these baselines, our approach is based on the discrete diffusion model that can iteratively refine the generated results using a PLM BART. As we have adapted them to better fit with each other by a set of revisions, we can combine the merits of the rich knowledge from BART and the iterative refining mechanism of the diffusion model. In this way, we can improve both the coherence and diversity of the generated responses. Furthermore, our approach outperforms AR models in the average value of all metrics, e.g., Ours (27.90) VS. BART (23.54) in PersonaChat. The reason is that our approach can generate diverse responses, which increase the values in the Distinct-1,2 metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Text Summarization and Question Generation.", "text": "As shown in Table 3 and Table 4, AR models outperform NAR models in a large margin. The reason is that the two types of tasks mainly require the model to accurately generate proper texts, which is more suitable for AR models due to their superiority in capturing the token dependency. Despite this, our approach mostly outperforms all the NAR and Semi-NAR methods, and even surpasses AR models on part of datasets (e.g., MSNews). It is because our approach can combine the merits of the PLM that has pre-learned rich semantic knowledge and the diffusion models that can iteratively refine the results, generating higher-quality texts.\nConversational Question Answering. The conversational question answering task is to evaluate the utilization of world knowledge. As shown in Table 4, our approach also performs well in this task, even slightly outperforming the AR model BART by 0.8 on F1 metric. A possible reason is that our approach can make use of the pre-learned world knowledge from BART. Besides, as our model can also leverage the iterative refining paradigm of the Table 2: The comparison between our approach and baselines on two dialog generation tasks. B-1/2 and D-1/2 denote BLEU-1/2 and Distinct-1/2. Bold and underline fonts denote the best and second best methods within NAR and Semi-NAR models, respectively. The baseline results on PersonaChat are collected from (Li et al., 2022a).\ndiffusion model, it may also fix the errors in the generated text, leading to more accurate answers.\nHuman Evaluation. As human evaluation is also critical for text generation, we conduct it on the dialog generation task and compare our approach with two best-performing baselines, i.e., BART and ELMER. Following existing works (Li et al., 2022a), we randomly select 500 examples from the test set of the PersonaChat dataset, and invite three annotators to evaluate the quality of the generated responses from the two baselines and ours from the perspectives of Fluency, Informativeness and Relevance. The scoring range is from 1 to 5. As shown in Table 5, the AR method BART performs better on the Fluency and Relevance metrics while the NAR method ELMER performs well on informativeness. Such results show a similar tendency as the automatic metrics, and indicate the different superiority of AR and NAR models. As a comparison, our approach can well balance the three metrics, with the comparable performance on Fluency as BART and the best performance on Informativeness. It shows the great potential of our approach in text-to-text generation tasks.", "publication_ref": ["b18", "b18"], "figure_ref": [], "table_ref": ["tab_5", "tab_8"]}, {"heading": "Further Analysis", "text": "Inference Latency. By using DDIM (Song et al., 2021a) or other acceleration strategies, we can also reduce the inference latency of our approach. To verify it, we test the inference latency and perfor-mance of our approach using different diffusion steps by using DDIM, and compare them with two best-performing NAR and AR baselines (i.e., ELMER and BART) on PersonaChat dataset. The above experiments are conducted on a NVIDIA 3090-24G GPU with a batch size of 1. As shown in Table 10, we can see that our approach can provide a way to trade off the time cost and the generation quality during inference. By setting proper diffusion steps (100 and 2), our approach can outperform BART and ELMER on average with similar inference latency, respectively.\nAblation and Variation Study Our Diffusion-NAT includes several key designs, i.e., the usage of BART, self-prompting strategy, removing time step embeddings. Here, we conduct the ablation and variation study to verify their effectiveness. Concretely, we propose four variations of our approach.\n-w/o self-prompting and -w/o PLM removes the corresponding component. +Time step Embeddings and BART=>RoBERTa add the time step embeddings as continuous diffusion methods (Li et al., 2021) and replaces BART by RoBERTa, respectively. As shown in Table 7, all the variations underperform our approach, it verifies the effectiveness of the above designs. Among them, adding time step embeddings cause the performance degrading a lot. The reason is that the additional embeddings may disturb the original semantic representations of BART.    and (Li et al., 2022a).   ", "publication_ref": ["b41", "b35", "b18"], "figure_ref": [], "table_ref": ["tab_1", "tab_12"]}, {"heading": "Discrete Diffusion V.S. Continuous Diffusion", "text": "For the NAR text-to-text generation, existing works  also have incorporated the continuous diffusion method. In this part, we aim to compare our approach with a recently proposed work, DiffuSeq ) that performs continuous diffusion on the latent space of token embeddings and leverages the KNN rounding step to map the embeddings into discrete tokens. We conduct the experiments on PersonaChat, XSUM and SQuAD datasets. As shown in  we can see that our approach outperforms DiffuSeq in all metrics by a large margin. It shows the effectiveness of our proposed method that utilizes the discrete diffusion method in NAR text-to-text generation tasks. Besides, compared with DiffuSeq, our approach can also benefit from the PLM BART, which also helps generate higher-quality texts.     and our approach on PersonaChat, XSUM and SQuAD datasets.\nters initialization, it is also helpful to faster and better convergence. To verify it, we report the BLEU-2 and Distinct-2 performance changes of our approach w.r.t. the training steps during training. As show in Figure 2, we observe that with the increasing of training steps, the performance of our approach is consistently improving, gradually approaching or surpassing competitive models. It shows the stabilization of our convergence process. Besides, for BLEU-2, with just 10k training steps, our approach can outperform competitive Semi-NAR model CMLM. The reason may be that BART provides a good starting point of the training process, making our approach converge faster.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we proposed Diffusion-NAT, a selfprompting discrete diffusion model (DDM) using a PLM BART for non-autoregressive (NAR) text generation. In our approach, we unified the inference process of BART and the denoising process of DDM into the same masked tokens recovering task, to combine the merits of both the rich prelearned knowledge of BART and the iterative refining paradigm of DDM. Concretely, we revised the decoding process of BART into the NAR manner, and adapted the typical settings of DDM to better fit with BART, including Markov transition matrix, training objective and time step embeddings. Besides, we devised an iterative self-prompting strategy to guide the PLM to deliberate and refine the intermediate generated results, to further improve the quality of final produced texts. Extensive experiments on seven datasets have shown that our approach can outperform competitive NAR and Semi-NAR models, and even surpass AR models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "This work is to investigate discrete diffusion models with pre-trained language models for nonautoregressive text-to-text generation. An important limitation is the relatively higher inference latency of diffusion models. In this work, we have adopted DDIM to accelerate the inference process by reducing the diffusion steps, and we also conduct experiments to investigate the performance changes w.r.t. different steps in Appendix E. We can see that fewer steps using DDIM would lead to the performance degradation. Fortunately, there are several recent works that have shown effectiveness in solving this problem . As these methods are general to all diffusion models, they may be able to be utilized in our approach. Besides, as we have adopted a PLM, BART in our approach, it may present biases learned from the pre-training corpus in the generated texts.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Details of Datasets", "text": "We conduct experiments on seven datasets, corresponding to four representative text generation tasks. Their statistics are shown in table 9.\n\u2022 Dialog Generation aims to predict responses according to the dialog history. We select Dai-lyDialog (Li et al., 2017) and PersonaChat (Zhang et al., 2018) datasets.\n\u2022 Text Summarization is to summarize the document into a sentence. We choose XSUM (Narayan et al., 2018) and MSNews (Liu et al., 2021a), two news summarization datasets.\n\u2022 Question Generation aims to generate questions based on given passages and answers. We use MSQG (Liu et al., 2021a) and SQUAD v1.1 (Rajpurkar et al., 2016) datasets.\n\u2022 Conversational Question Answering is to answer the question based on a conversation. We select CoQA (Reddy et al., 2019) dataset.", "publication_ref": ["b21", "b51", "b28", "b25", "b25", "b38", "b39"], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "B Details of Evaluation Metrics.", "text": "Following existing works (Li et al., 2022a;, we employ corresponding metrics to evaluate model performances on different tasks.\n\u2022 For dialog generation, we adopt BLEU-1/2 (Papineni et al., 2002) to measure the coherence between the generated and real responses based on the co-occurrence ratio of n-grams, and Distinct-1/2 (Li et al., 2016) for the n-gram diversity of the generated texts.\n\u2022 For text summarization, we utilize ROUGE-1/2/L (Lin, 2004) to compute the overlapping ratio of n-grams between the generated and ground-truth summary to estimate the quality.   \u2022 For question generation, we use ROUGE-L, BLEU-4 and METEOR (Banerjee and Lavie, 2005) to assess the generation consistency.\n\u2022 For conversational question answering, we adopt F1-Score (Rajpurkar et al., 2016) to measure the prediction accuracy.", "publication_ref": ["b18", "b31", "b17", "b23", "b1", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "C Details of Baselines", "text": "We mainly compare our Diffusion-NAT with a variety of Semi-NAR and NAR models. NAT (Gu et al., 2018), iNAT (Lee et al., 2018), InsT (Stern et al., 2019), CMLM (Ghazvininejad et al., 2019) and LevT (Gu et al., 2019) are five Transformer-based NAR models with special generation strategies, i.e., iterative refinement, conditional masked language modeling and insertion-deletion operation. BANG  and ELMER (Li et al., 2022a) adopt the pre-training technique based on Transformer to further improve the NAR generation performance. Note that InsT, iNAT, LevT, CMLM and BANG also support the semi-NAR manner that can rely on partially generated results for improving the inference. We also compare our approach with two recently proposed diffusion-based methods, i.e., GENIE (Lin et al., 2022) and AR-DIFFUSION (Wu et al., 2023), which incorporate the pre-training strategy and auto-regressive decoding to improve the generation performance of continuous diffusion models.\nWe also compare our approach with AR models which have shown better accuracy than NAR ones. LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017)   BART (Lewis et al., 2020) and ProphetNet (Qi et al., 2020) are PLMs specially for text generation and we use their base version for fair comparison.", "publication_ref": ["b10", "b15", "b44", "b8", "b11", "b18", "b24", "b49", "b14", "b48", "b16", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "D Implementation Details", "text": "For all baselines, we use the source code provided by their authors, and all hyper-parameters are set following the original paper. For our Diffusion-NAT, we use the checkpoint of BART-base with 110M parameters for initialization, and do not add any other parameters. We use the linear noise schedule (Ho et al., 2020) for the diffusion process. During training, the diffusion step is set to 1000. During inference, we utilize DDIM (Song et al., 2021a) for fast sampling and reduce the diffusion step into 100. The number of self-prompting turns is set to 2. We use AdamW as the optimizer, and set learning rate to 5e-5. We set the training step for XSUM and SQuAD v1.1 to 120k, and 80k for other datasets. The batch size is set to 512.\nE Hyper-parameter Tuning.\nOur approach also requires some parameters to tune, i.e., the diffusion steps during decoding and the turns of self-prompting. Generally, more diffusion steps and self-prompting turns would lead to better performance but larger inference latency, hence we can tune their values to balance the inference time cost and quality. In this part, we conduct experiments on the PersonaChat dataset to validate it. As shown in Table 10 and Table 11, we can see that more diffusion steps and more self-prompting turns are able to improve the model performance, while the improvement seems to be saturated after a certain number, i.e., 100 for diffusion steps and 2 for self-prompting turns. Such results can provide a reference for tuning the two hyper-parameters to match the requirement of model performance and inference latency. Besides, with very few diffusion steps (e.g., 2 steps), our approach can also achieve a decent performance on BLEU-2 and Distinct-2. It shows the potential of further reducing the inference latency in our approach.", "publication_ref": ["b13", "b41"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "F Case Study", "text": "To provide the qualitative analysis on our approach, we show two generated examples on PersonaChat in Table 12. We can see that with the help of BART and the diffusion model, our approach can generate relevant and informative responses based on the given dialog context. Besides, the left example shows that our approach can generate interesting phrases such as \"as healthy as american lifestyle\", which makes the response more humorous and well reflects the speaker's personal characteristics.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Acknowledgments", "text": "We are thankful to Tianyi Tang for the supportive work and insightful suggestions. This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215 and U2001212, and Beijing Natural Science Foundation under Grant No. 4222027. And this work is also partially supported by the Outstanding Innovative Talents Cultivation Funded Programs 2021 of Renmin University of China. Xin Zhao is the corresponding author.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Structured denoising diffusion models in discrete state-spaces", "journal": "", "year": "2021-12-06", "authors": "Jacob Austin; Daniel D Johnson; Jonathan Ho; Daniel Tarlow; Rianne Van Den;  Berg"}, {"ref_id": "b1", "title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments", "journal": "", "year": "2005-06-29", "authors": "Satanjeev Banerjee; Alon Lavie"}, {"ref_id": "b2", "title": "Xdlm: Cross-lingual diffusion language model for machine translation", "journal": "", "year": "2023", "authors": "Linyao Chen; Aosong Feng; Boming Yang; Zihui Li"}, {"ref_id": "b3", "title": "Analog bits: Generating discrete data using diffusion models with self-conditioning", "journal": "CoRR", "year": "2022", "authors": "Ting Chen; Ruixiang Zhang; Geoffrey E Hinton"}, {"ref_id": "b4", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "Diffusion models beat gans on image synthesis", "journal": "", "year": "2021-12-06", "authors": "Prafulla Dhariwal; Alexander Quinn; Nichol "}, {"ref_id": "b6", "title": "Difformer: Empowering diffusion model on embedding space for text generation", "journal": "CoRR", "year": "2022", "authors": "Zhujin Gao; Junliang Guo; Xu Tan; Yongxin Zhu; Fang Zhang; Jiang Bian; Linli Xu"}, {"ref_id": "b7", "title": "Learning to rewrite for non-autoregressive neural machine translation", "journal": "", "year": "2021-07-11", "authors": "Xinwei Geng; Xiaocheng Feng; Bing Qin"}, {"ref_id": "b8", "title": "Mask-predict: Parallel decoding of conditional masked language models", "journal": "Association for Computational Linguistics", "year": "2019-11-03", "authors": "Marjan Ghazvininejad; Omer Levy; Yinhan Liu; Luke Zettlemoyer"}, {"ref_id": "b9", "title": "Diffuseq: Sequence to sequence text generation with diffusion models", "journal": "CoRR", "year": "2022", "authors": "Shansan Gong; Mukai Li; Jiangtao Feng; Zhiyong Wu; Lingpeng Kong"}, {"ref_id": "b10", "title": "Non-autoregressive neural machine translation", "journal": "", "year": "2018-04-30", "authors": "Jiatao Gu; James Bradbury; Caiming Xiong; O K Victor; Richard Li;  Socher"}, {"ref_id": "b11", "title": "Levenshtein transformer", "journal": "Vancouver", "year": "2019-12-08", "authors": "Jiatao Gu; Changhan Wang; Junbo Zhao"}, {"ref_id": "b12", "title": "Vector quantized diffusion model for text-to-image synthesis", "journal": "IEEE", "year": "2022-06-18", "authors": "Shuyang Gu; Dong Chen; Jianmin Bao; Fang Wen; Bo Zhang; Dongdong Chen; Lu Yuan; Baining Guo"}, {"ref_id": "b13", "title": "Denoising diffusion probabilistic models", "journal": "", "year": "2020-12-06", "authors": "Jonathan Ho; Ajay Jain; Pieter Abbeel"}, {"ref_id": "b14", "title": "Long short-term memory", "journal": "Neural Comput", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b15", "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement", "journal": "Association for Computational Linguistics", "year": "2018-10-31", "authors": "Jason Lee; Elman Mansimov; Kyunghyun Cho"}, {"ref_id": "b16", "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "Association for Computational Linguistics", "year": "2020-07-05", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b17", "title": "A diversity-promoting objective function for neural conversation models", "journal": "", "year": "2016-06-12", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"ref_id": "b18", "title": "ELMER: A non-autoregressive pre-trained language model for efficient and effective text generation", "journal": "", "year": "2022", "authors": "Junyi Li; Tianyi Tang; Wayne Xin Zhao; Jian-Yun Nie; Ji-Rong Wen"}, {"ref_id": "b19", "title": "Pretrained language models for text generation: A survey", "journal": "", "year": "2021", "authors": "Junyi Li; Tianyi Tang; Wayne Xin Zhao; Ji-Rong Wen"}, {"ref_id": "b20", "title": "Diffusion-lm improves controllable text generation", "journal": "CoRR", "year": "2022", "authors": "Lisa Xiang; John Li; Ishaan Thickstun; Percy Gulrajani; Tatsunori B Liang;  Hashimoto"}, {"ref_id": "b21", "title": "Dailydialog: A manually labelled multi-turn dialogue dataset", "journal": "Long Papers", "year": "2017-11-27", "authors": "Yanran Li; Hui Su; Xiaoyu Shen; Wenjie Li; Ziqiang Cao; Shuzi Niu"}, {"ref_id": "b22", "title": "Diffusion models for nonautoregressive text generation: A survey", "journal": "", "year": "2023", "authors": "Yifan Li; Kun Zhou; Wayne Xin Zhao; Ji-Rong Wen"}, {"ref_id": "b23", "title": "Rouge: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b24", "title": "GENIE: large scale pre-training for text generation with diffusion model", "journal": "CoRR", "year": "2022", "authors": "Zhenghao Lin; Yeyun Gong; Yelong Shen; Tong Wu; Zhihao Fan; Chen Lin; Weizhu Chen; Nan Duan"}, {"ref_id": "b25", "title": "GLGE: A new general language generation evaluation benchmark", "journal": "Association for Computational Linguistics", "year": "2021-08-01", "authors": "Dayiheng Liu; Yu Yan; Yeyun Gong; Weizhen Qi; Hang Zhang; Jian Jiao; Weizhu Chen; Jie Fu; Linjun Shou; Ming Gong; Pengcheng Wang; Jiusheng Chen; Daxin Jiang; Jiancheng Lv; Ruofei Zhang; Winnie Wu; Ming Zhou; Nan Duan"}, {"ref_id": "b26", "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing", "journal": "", "year": "2021", "authors": "Pengfei Liu; Weizhe Yuan; Jinlan Fu; Zhengbao Jiang; Hiroaki Hayashi; Graham Neubig"}, {"ref_id": "b27", "title": "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps", "journal": "", "year": "2022", "authors": "Cheng Lu; Yuhao Zhou; Fan Bao; Jianfei Chen; Chongxuan Li"}, {"ref_id": "b28", "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization", "journal": "Association for Computational Linguistics", "year": "2018-10-31", "authors": "Shashi Narayan; Shay B Cohen; Mirella Lapata"}, {"ref_id": "b29", "title": "Improved denoising diffusion probabilistic models", "journal": "PMLR", "year": "2021-07-24", "authors": "Alexander Quinn; Nichol ; Prafulla Dhariwal"}, {"ref_id": "b30", "title": "Giorgos Sfikas, Elisa Barney Smith, Hamam Mokayed, and Marcus Liwicki. 2023. Wordstylist: Styled verbatim handwritten text generation with latent diffusion models", "journal": "", "year": "", "authors": "Konstantina Nikolaidou; George Retsinas; Vincent Christlein; Mathias Seuret"}, {"ref_id": "b31", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "ACL", "year": "2002-07-06", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b32", "title": "BANG: bridging autoregressive and non-autoregressive generation with large scale pretraining", "journal": "PMLR", "year": "2021-07-24", "authors": "Weizhen Qi; Yeyun Gong; Jian Jiao; Yu Yan; Weizhu Chen; Dayiheng Liu; Kewen Tang; Houqiang Li; Jiusheng Chen; Ruofei Zhang; Ming Zhou; Nan Duan"}, {"ref_id": "b33", "title": "Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training", "journal": "", "year": "2020-11-20", "authors": "Weizhen Qi; Yu Yan; Yeyun Gong; Dayiheng Liu; Nan Duan; Jiusheng Chen; Ruofei Zhang; Ming Zhou"}, {"ref_id": "b34", "title": "Diff-glat: Diffusion glancing transformer for parallel sequence to sequence learning", "journal": "", "year": "2022", "authors": "Lihua Qian; Mingxuan Wang; Yang Liu; Hao Zhou"}, {"ref_id": "b35", "title": "Glancing transformer for non-autoregressive neural machine translation", "journal": "Association for Computational Linguistics", "year": "2021-08-01", "authors": "Lihua Qian; Hao Zhou; Yu Bao; Mingxuan Wang; Lin Qiu; Weinan Zhang; Yong Yu; Lei Li"}, {"ref_id": "b36", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b37", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b38", "title": "Squad: 100, 000+ questions for machine comprehension of text", "journal": "The Association for Computational Linguistics", "year": "2016-11-01", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b39", "title": "Coqa: A conversational question answering challenge", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2019", "authors": "Siva Reddy; Danqi Chen; Christopher D Manning"}, {"ref_id": "b40", "title": "U-net: Convolutional networks for biomedical image segmentation", "journal": "Springer", "year": "2015-10-05", "authors": "Olaf Ronneberger; Philipp Fischer; Thomas Brox"}, {"ref_id": "b41", "title": "Denoising diffusion implicit models", "journal": "", "year": "2021-05-03", "authors": "Jiaming Song; Chenlin Meng; Stefano Ermon"}, {"ref_id": "b42", "title": "MASS: masked sequence to sequence pre-training for language generation", "journal": "PMLR", "year": "2019-06", "authors": "Kaitao Song; Xu Tan; Tao Qin; Jianfeng Lu; Tie-Yan Liu"}, {"ref_id": "b43", "title": "Score-based generative modeling through stochastic differential equations", "journal": "", "year": "2021-05-03", "authors": "Yang Song; Jascha Sohl-Dickstein; Diederik P Kingma; Abhishek Kumar; Stefano Ermon; Ben Poole"}, {"ref_id": "b44", "title": "Insertion transformer: Flexible sequence generation via insertion operations", "journal": "PMLR", "year": "2019-06", "authors": "Mitchell Stern; William Chan; Jamie Kiros; Jakob Uszkoreit"}, {"ref_id": "b45", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014-12-08", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"ref_id": "b46", "title": "Can diffusion model achieve better performance in text generation? bridging the gap between training and inference!", "journal": "", "year": "2023", "authors": "Zecheng Tang; Pinzheng Wang; Keyan Zhou; Juntao Li; Ziqiang Cao; Min Zhang"}, {"ref_id": "b47", "title": "Ziqiang Cao, and Min Zhang. 2023b. Can diffusion model achieve better performance in text generation?", "journal": "", "year": "", "authors": "Zecheng Tang; Pinzheng Wang; Keyan Zhou; Juntao Li"}, {"ref_id": "b48", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b49", "title": "Ar-diffusion: Auto-regressive diffusion model for text generation", "journal": "", "year": "2023", "authors": "Tong Wu; Zhihao Fan; Xiao Liu; Yeyun Gong; Yelong Shen; Jian Jiao; Hai-Tao Zheng; Juntao Li; Zhongyu Wei; Jian Guo; Nan Duan; Weizhu Chen"}, {"ref_id": "b50", "title": "Seqdiffuseq: Text diffusion with encoder-decoder transformers", "journal": "CoRR", "year": "2022", "authors": "Hongyi Yuan; Zheng Yuan; Chuanqi Tan; Fei Huang; Songfang Huang"}, {"ref_id": "b51", "title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "Long Papers", "year": "2018-07-15", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"ref_id": "b52", "title": "A reparameterized discrete diffusion model for text generation", "journal": "", "year": "2023", "authors": "Lin Zheng; Jianbo Yuan; Lei Yu; Lingpeng Kong"}, {"ref_id": "b53", "title": "Understanding knowledge distillation in nonautoregressive machine translation", "journal": "", "year": "2020-04-26", "authors": "Chunting Zhou; Jiatao Gu; Graham Neubig"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Performance changes of our approach w.r.t. the training steps on PersonaChat dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "A comparison of existing diffusion methods for text generation. Dis. and Con. refer to discrete and continuous diffusion. PLMs, Cost, NAR and T2T denote using PLMs, Training Cost, Non-AutoRegressive model and Text-to-Text generation, respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "). While, if token i is the[MASK]   token, it will be unchanged. Based on such a forward process, all tokens in the output text would become[MASK]  after a sufficient number of steps, corresponding to the all-[MASK] input in Eq. 6. In the denoising process, we adopt BART to gradually recover the all-[MASK] sequence into output text in the NAR manner, where each denoising step is equivalent to the decoding of BART in Section 4.2.Training with NAR Masked Tokens Recovering.During training, existing diffusion models mostly learn to predict the noise in the current time step. However, such training objective is not consistent with PLMs. Inspired by existing works", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The comparison between different methods on XSUM and SQuAD v1.1 datasets. The baseline results are collected from", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The comparison between different methods on MSNews, MSQG and CoQA datasets.", "figure_data": "ModelsPersonaChat Fluency Informativeness RelevanceBART ELMER Ours4.32 3.88 4.294.31 4.49 4.573.47 2.90 3.19"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Human evaluation scores of different methods about the generated responses on PersonaChat.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": "ELMERDiffusion-NATBARTSteps Latency 13.8ms 19.1ms 76.4ms 267.5ms 253.6ms -2 20 100 -BLEU-2 23.99 Dist-2 24.9630.82 36.19 23.68 26.9337.66 26.2039.36 6.10"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Performance and inference latency changes of two baselines and our approach w.r.t. the diffusion steps using DDIM during inference on PersonaChat dataset.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Ablation study on PersonaChat dataset.", "figure_data": "ModelsPersonaChat XSUM B-1 B-2 R-LSQuAD R-L MTDiffuSeq 37.79 32.50 20.29 29.29 12.57Ours44.55 37.66 30.88 46.64 21.99"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Statistics of the datasets, where Dialog, Sum., QG and CQA denote Dialog Generation, Text Summrization, Question Generation and Conversational Question Answering, respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_16", "figure_caption": " ", "figure_data": "PersonaChatDiff. Steps21020100200 1000BLEU-230.82"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Performance changes w.r.t. the diffusion steps (abbreviated as Diff. Steps) on PersonaChat dataset.", "figure_data": "PersonaChatSP Turns012345BLEU-2 35.00 36.50 37.66 37.69 37.77 37.77Distinct-2 26.01 26.22 26.20 26.34 26.29 26.30"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Performance changes w.r.t. the self-prompting turns (abbreviated as SP Turns) on PersonaChat dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "are two classic Seq2Seq models. MASS (Song et al., 2019), Personal Profile i enjoy cross stitch and quilting. my father served in our military in the war. i am proud to be an american. i am what people call a baby boomer. my parents were proud immigrants to america. my father was a police officer. my favorite band is nirvana. i like running. i work at pet smart. i am a musician. Context hello, how is your day going hi. my day is good. i am hanging out with both of my sons. cool, sounds like fun. just as much fun as cross stitch and quilts i love those crafts! right now i am into my aquarium filled with exotic fish. i am what you would call a baby boomer, explains the quilting passion you might like my collection of decorations. they are 50s vintage! sure, it would bring back old memories. especially when my dad served in the military that is awesome. what do you eat for breakfast? mine is granola every day. oatmeal fan over here. my parents were immigrants, they raised me on oats everyday. that is very healthy. i like it. do you eat fish? hey. want to chat? i am just listening to my favorite music, nirvana. i was just reading a biography. i love those. i work too much overtime at pet smart to read. what do you do? i just got out of college.", "figure_data": "Real Responseyes i do eat fish. i love itwhat did you study. i work at pet smart but really like music.Oursfish is almost as healthy as american lifestyle, love fish tooi am a musician. and i play music all the time."}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Examples of generated responses on PersonaChat by our approach.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "C = {c 1 , c 2 , \u2022 \u2022 \u2022 , c m } and Y = {y 1 , y 2 , \u2022 \u2022 \u2022 , y n }", "formula_coordinates": [3.0, 70.86, 581.88, 218.28, 34.1]}, {"formula_id": "formula_1", "formula_text": "P (Y |C) = n i=1 P (y i |C),(1)", "formula_coordinates": [3.0, 125.97, 742.72, 163.89, 34.69]}, {"formula_id": "formula_2", "formula_text": "q(x t |x t\u22121 ) = N ( 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I), (2", "formula_coordinates": [3.0, 333.95, 242.87, 186.96, 20.99]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [3.0, 520.91, 243.3, 4.24, 13.15]}, {"formula_id": "formula_4", "formula_text": "p(x t\u22121 |x t ) = N (\u00b5 \u03b8 (x t , t), \u03a3 \u03b8 (x t , t)), (3", "formula_coordinates": [3.0, 331.87, 342.57, 189.04, 20.55]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [3.0, 520.91, 342.57, 4.24, 13.15]}, {"formula_id": "formula_6", "formula_text": "q(x t |x t\u22121 ) = v \u22a4 (x t )Q t v(x t\u22121 ),(4)", "formula_coordinates": [3.0, 343.5, 529.53, 181.65, 21.19]}, {"formula_id": "formula_7", "formula_text": "q(xt\u22121|xt, x0) = v \u22a4 (xt)Qtv(xt\u22121)v \u22a4 (xt\u22121)Qt\u22121v(x0) v \u22a4 (xt)Qtv(x0)(5)", "formula_coordinates": [3.0, 307.68, 642.68, 217.34, 32.65]}, {"formula_id": "formula_8", "formula_text": "t = Q 1 Q 2 \u2022 \u2022 \u2022 Q t .", "formula_coordinates": [3.0, 349.21, 676.28, 93.82, 20.55]}, {"formula_id": "formula_9", "formula_text": "| | Hello , nice to meet you . Hello , [M] to meet you [M] [M] [M] [M] [M] [M] [M] [M] Hello , [M] to [M] you [M] \u2026 \u2026", "formula_coordinates": [4.0, 97.91, 126.74, 200.32, 71.9]}, {"formula_id": "formula_10", "formula_text": "Denoising Step Forward Step [M], [M] to [M] you [M]", "formula_coordinates": [4.0, 174.8, 124.81, 330.66, 87.23]}, {"formula_id": "formula_11", "formula_text": "f NMTR ([M], \u2022 \u2022 \u2022 , [M]) = {y 1 , \u2022 \u2022 \u2022 , y n }, (6)", "formula_coordinates": [4.0, 98.97, 537.26, 190.9, 20.55]}, {"formula_id": "formula_12", "formula_text": "BART({y (t) 1 \u2022 \u2022 \u2022 [M]}, C) = {y (t\u22121) 1 \u2022 \u2022 \u2022 y (t\u22121) n }, (7)", "formula_coordinates": [5.0, 93.6, 190.76, 196.13, 17.66]}, {"formula_id": "formula_13", "formula_text": "[Qt]i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1t, if j = i, \u03b3t, if j = [M], 1 \u2212 \u03b1t \u2212 \u03b3t, otherwise,(8)", "formula_coordinates": [5.0, 104.32, 537.38, 185.42, 49.29]}, {"formula_id": "formula_14", "formula_text": "(0) 1 , \u2022 \u2022 \u2022 , y(0)", "formula_coordinates": [5.0, 394.23, 125.81, 54.33, 22.2]}, {"formula_id": "formula_15", "formula_text": "BART({y (t) 1 \u2022 \u2022 \u2022 [M]}, C) = {y (0) 1 \u2022 \u2022 \u2022 y (0) n }.(9)", "formula_coordinates": [5.0, 337.5, 164.96, 187.53, 17.66]}, {"formula_id": "formula_16", "formula_text": "L Y = \u2212 n i=1 log p \u03b8 (y (0) i |Y t , C)(10)", "formula_coordinates": [5.0, 347.68, 264.7, 177.48, 34.69]}, {"formula_id": "formula_17", "formula_text": "C \u2032 = [\u0176 0 ; C].", "formula_coordinates": [6.0, 231.8, 180.04, 59.25, 13.82]}, {"formula_id": "formula_18", "formula_text": "L Y = \u2212 n i=1 log p \u03b8 (y (0) i |Y t ,\u0176 0 , C). (11", "formula_coordinates": [6.0, 94.37, 338.11, 190.95, 34.7]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 285.32, 347.64, 4.54, 13.15]}, {"formula_id": "formula_20", "formula_text": "(0) 1 , \u2022 \u2022 \u2022 ,\u0177(0)", "formula_coordinates": [6.0, 228.85, 421.38, 54.33, 22.2]}, {"formula_id": "formula_21", "formula_text": "BART {y (t) 1 \u2022 \u2022 \u2022 y (t) n }, {\u0177 (0) 1 \u2022 \u2022 \u2022\u0177 (0) n }, C = {y (0) 1 \u2022 \u2022 \u2022 y (0) n }.(12)", "formula_coordinates": [6.0, 71.05, 487.61, 218.68, 21.55]}], "doi": "10.48550/arXiv.2208.04202"}