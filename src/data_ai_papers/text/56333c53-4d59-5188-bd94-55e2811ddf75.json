{"title": "A Style-Based Generator Architecture for Generative Adversarial Networks", "authors": "Tero Karras", "pub_date": "2019-03-29", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.", "sections": [{"heading": "Introduction", "text": "The resolution and quality of images produced by generative methods -especially generative adversarial networks (GAN) [22] -have seen rapid improvement recently [30,45,5]. Yet the generators continue to operate as black boxes, and despite recent efforts [3], the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking. The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13,52,37] provide no quantitative way to compare different generators against each other.\nMotivated by style transfer literature [27], we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the \"style\" of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales. Combined with noise injected directly into the network, this architectural change leads to automatic, unsupervised separation of high-level attributes (e.g., pose, identity) from stochastic variation (e.g., freckles, hair) in the generated images, and enables intuitive scale-specific mixing and interpolation operations. We do not modify the discriminator or the loss function in any way, and our work is thus orthogonal to the ongoing discussion about GAN loss functions, regularization, and hyperparameters [24,45,5,40,44,36].\nOur generator embeds the input latent code into an intermediate latent space, which has a profound effect on how the factors of variation are represented in the network. The input latent space must follow the probability density of the training data, and we argue that this leads to some degree of unavoidable entanglement. Our intermediate latent space is free from that restriction and is therefore allowed to be disentangled. As previous methods for estimating the degree of latent space disentanglement are not directly applicable in our case, we propose two new automated metricsperceptual path length and linear separability -for quantifying these aspects of the generator. Using these metrics, we show that compared to a traditional generator architecture, our generator admits a more linear, less entangled representation of different factors of variation.\nFinally, we present a new dataset of human faces (Flickr-Faces-HQ, FFHQ) that offers much higher quality and covers considerably wider variation than existing high-resolution datasets (Appendix A). We have made this dataset publicly available, along with our source code and pre-trained networks. 1 The accompanying video can be found under the same link.", "publication_ref": ["b21", "b29", "b44", "b4", "b2", "b12", "b51", "b36", "b26", "b23", "b44", "b4", "b39", "b43", "b35", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Style-based generator", "text": "Traditionally the latent code is provided to the generator through an input layer, i.e., the first layer of a feedforward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a learned constant instead (Figure 1b, right). Given a latent code z in the input latent space Z, a non-linear mapping network f : Z \u2192 W first produces w \u2208 W (Figure 1b, left). For simplicity, we set the dimensionality of both . While a traditional generator [30] feeds the latent code though the input layer only, we first map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, before evaluating the nonlinearity. Here \"A\" stands for a learned affine transform, and \"B\" applies learned per-channel scaling factors to the noise input. The mapping network f consists of 8 layers and the synthesis network g consists of 18 layers -two for each resolution (4 2 \u2212 1024 2 ). The output of the last layer is converted to RGB using a separate 1 \u00d7 1 convolution, similar to Karras et al. [30]. Our generator has a total of 26.2M trainable parameters, compared to 23.1M in the traditional generator.\nspaces to 512, and the mapping f is implemented using an 8-layer MLP, a decision we will analyze in Section 4.1.\nLearned affine transformations then specialize w to styles y = (y s , y b ) that control adaptive instance normalization (AdaIN) [27,17,21,16] operations after each convolution layer of the synthesis network g. The AdaIN operation is defined as\nAdaIN(x i , y) = y s,i x i \u2212 \u00b5(x i ) \u03c3(x i ) + y b,i ,(1)\nwhere each feature map x i is normalized separately, and then scaled and biased using the corresponding scalar components from style y. Thus the dimensionality of y is twice the number of feature maps on that layer. Comparing our approach to style transfer, we compute the spatially invariant style y from vector w instead of an example image. We choose to reuse the word \"style\" for y because similar network architectures are already used for feedforward style transfer [27], unsupervised image-toimage translation [28], and domain mixtures [23]. Compared to more general feature transforms [38,57], AdaIN is particularly well suited for our purposes due to its efficiency and compact representation. Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of uncorrelated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned perfeature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure 1b. The implications of adding the noise inputs are discussed in Sections 3.2 and 3.3.", "publication_ref": ["b29", "b29", "b26", "b16", "b20", "b15", "b26", "b27", "b22", "b37", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Quality of generated images", "text": "Before studying the properties of our generator, we demonstrate experimentally that the redesign does not compromise image quality but, in fact, improves it considerably. Table 1 gives Fr\u00e9chet inception distances (FID) [25] for various generator architectures in CELEBA-HQ [30] and our new FFHQ dataset (Appendix A). Results for other datasets are given in Appendix E. Our baseline configuration (A) is the Progressive GAN setup of Karras et al. [30], from which we inherit the networks and all hyperparameters except where stated otherwise. We first switch to an improved baseline (B) by using bilinear up/downsampling operations [64], longer training, and tuned hyperparameters. A detailed description of training setups and hyperparameters is included in Appendix C. We then improve this new baseline further by adding the mapping network and AdaIN operations (C), and make a surprising observation that the network no longer benefits from feeding the latent code into the first convolution layer. We therefore simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 \u00d7 4 \u00d7 512 constant tensor (D). We find it quite remarkable that the synthesis network is able to produce meaningful results even though it receives input only through the styles that control the AdaIN operations.\nFinally, we introduce the noise inputs (E) that improve the results further, as well as novel mixing regularization (F) that decorrelates neighboring styles and enables more finegrained control over the generated imagery (Section 3.1).\nWe evaluate our methods using two different loss functions: for CELEBA-HQ we rely on WGAN-GP [24], Figure 2. Uncurated set of images produced by our style-based generator (config F) with the FFHQ dataset. Here we used a variation of the truncation trick [42,5,34] with \u03c8 = 0.7 for resolutions 4 2 \u2212 32 2 . Please see the accompanying video for more results.\nwhile FFHQ uses WGAN-GP for configuration A and nonsaturating loss [22] with R 1 regularization [44,51,14] for configurations B-F. We found these choices to give the best results. Our contributions do not modify the loss function.\nWe observe that the style-based generator (E) improves FIDs quite significantly over the traditional generator (B), almost 20%, corroborating the large-scale ImageNet measurements made in parallel work [6,5]. Figure 2 shows an uncurated set of novel images generated from the FFHQ dataset using our generator. As confirmed by the FIDs, the average quality is high, and even accessories such as eyeglasses and hats get successfully synthesized. For this figure, we avoided sampling from the extreme regions of W using the so-called truncation trick [42,5,34] -Appendix B details how the trick can be performed in W instead of Z. Note that our generator allows applying the truncation selectively to low resolutions only, so that highresolution details are not affected.\nAll FIDs in this paper are computed without the truncation trick, and we only use it for illustrative purposes in Figure 2 and the video. All images are generated in 1024 2 resolution.", "publication_ref": ["b24", "b29", "b29", "b63", "b23", "b41", "b4", "b33", "b21", "b43", "b50", "b13", "b5", "b4", "b41", "b4", "b33"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Prior art", "text": "Much of the work on GAN architectures has focused on improving the discriminator by, e.g., using multiple discriminators [18,47,11], multiresolution discrimination [60,55], or self-attention [63]. The work on generator side has mostly focused on the exact distribution in the input latent space [5] or shaping the input latent space via Gaussian mixture models [4], clustering [48], or encouraging convexity [52].\nRecent conditional generators feed the class identifier through a separate embedding network to a large number of layers in the generator [46], while the latent is still provided though the input layer. A few authors have considered feeding parts of the latent code to multiple generator layers [9,5]. In parallel work, Chen et al. [6] \"self modulate\" the generator using AdaINs, similarly to our work, but do not consider an intermediate latent space or noise inputs.", "publication_ref": ["b17", "b46", "b10", "b59", "b54", "b62", "b4", "b3", "b47", "b51", "b45", "b8", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Properties of the style-based generator", "text": "Our generator architecture makes it possible to control the image synthesis via scale-specific modifications to the styles. We can view the mapping network and affine transformations as a way to draw samples for each style from a learned distribution, and the synthesis network as a way to generate a novel image based on a collection of styles. The effects of each style are localized in the network, i.e., modifying a specific subset of the styles can be expected to affect only certain aspects of the image.\nTo see the reason for this localization, let us consider how the AdaIN operation (Eq. 1) first normalizes each channel to zero mean and unit variance, and only then applies scales and biases based on the style. The new per-channel statistics, as dictated by the style, modify the relative importance of features for the subsequent convolution operation, but they do not depend on the original statistics because of the normalization. Thus each style controls only one convolution before being overridden by the next AdaIN operation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Style mixing", "text": "To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training. When generating such an image, we simply switch from one latent code to another -an operation we refer to as style mixing -at a randomly selected point in the synthesis network. To be specific, we run two latent codes z 1 , z 2 through the mapping network, and have the corresponding w 1 , w 2 control the styles so that w 1 applies before the crossover point and w 2 after it. This regularization technique prevents the network from assuming that adjacent styles are correlated.\nTable 2 shows how enabling mixing regularization dur-\nSource A Source B\nCoarse styles from source B\nMiddle styles from source B Fine from B Figure 3. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated by copying a specified subset of styles from source B and taking the rest from source A. Copying the styles corresponding to coarse spatial resolutions ( 4    ing training improves the localization considerably, indicated by improved FIDs in scenarios where multiple latents are mixed at test time. Figure 3 presents examples of images synthesized by mixing two latent codes at various scales. We can see that each subset of styles controls meaningful high-level attributes of the image.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Stochastic variation", "text": "There are many aspects in human portraits that can be regarded as stochastic, such as the exact placement of hairs, stubble, freckles, or skin pores. Any of these can be randomized without affecting our perception of the image as long as they follow the correct distribution.\nLet us consider how a traditional generator implements stochastic variation. Given that the only input to the network is through the input layer, the network needs to invent a way to generate spatially-varying pseudorandom numbers ). We can see that the artificial omission of noise leads to featureless \"painterly\" look. Coarse noise causes large-scale curling of hair and appearance of larger background features, while the fine noise brings out the finer curls of hair, finer background detail, and skin pores.\nfrom earlier activations whenever they are needed. This consumes network capacity and hiding the periodicity of generated signal is difficult -and not always successful, as evidenced by commonly seen repetitive patterns in generated images. Our architecture sidesteps these issues altogether by adding per-pixel noise after each convolution. Figure 4 shows stochastic realizations of the same underlying image, produced using our generator with different noise realizations. We can see that the noise affects only the stochastic aspects, leaving the overall composition and high-level aspects such as identity intact. Figure 5 further illustrates the effect of applying stochastic variation to different subsets of layers. Since these effects are best seen in animation, please consult the accompanying video for a demonstration of how changing the noise input of one layer leads to stochastic variation at a matching scale.\nWe find it interesting that the effect of noise appears tightly localized in the network. We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our network to create stochastic variation is to rely on the noise provided. A fresh set of noise is available for every layer, and thus there is no incentive to generate the stochastic effects from earlier activations, leading to a localized effect. The learned mapping from Z to W is able to \"undo\" much of the warping.", "publication_ref": [], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Separation of global effects from stochasticity", "text": "The previous sections as well as the accompanying video demonstrate that while changes to the style have global effects (changing pose, identity, etc.), the noise affects only inconsequential stochastic variation (differently combed hair, beard, etc.). This observation is in line with style transfer literature, where it has been established that spatially invariant statistics (Gram matrix, channel-wise mean, variance, etc.) reliably encode the style of an image [20,39] while spatially varying features encode a specific instance.\nIn our style-based generator, the style affects the entire image because complete feature maps are scaled and biased with the same values. Therefore, global effects such as pose, lighting, or background style can be controlled coherently. Meanwhile, the noise is added independently to each pixel and is thus ideally suited for controlling stochastic variation. If the network tried to control, e.g., pose using the noise, that would lead to spatially inconsistent decisions that would then be penalized by the discriminator. Thus the network learns to use the global and local channels appropriately, without explicit guidance.", "publication_ref": ["b19", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Disentanglement studies", "text": "There are various definitions for disentanglement [54,50,2,7,19], but a common goal is a latent space that consists of linear subspaces, each of which controls one factor of variation. However, the sampling probability of each combination of factors in Z needs to match the corresponding density in the training data. As illustrated in Figure 6, this precludes the factors from being fully disentangled with typical datasets and input latent distributions. 2 A major benefit of our generator architecture is that the intermediate latent space W does not have to support sam-pling according to any fixed distribution; its sampling density is induced by the learned piecewise continuous mapping f (z). This mapping can be adapted to \"unwarp\" W so that the factors of variation become more linear. We posit that there is pressure for the generator to do so, as it should be easier to generate realistic images based on a disentangled representation than based on an entangled representation. As such, we expect the training to yield a less entangled W in an unsupervised setting, i.e., when the factors of variation are not known in advance [10,35,49,8,26,32,7].\nUnfortunately the metrics recently proposed for quantifying disentanglement [26,32,7,19] require an encoder network that maps input images to latent codes. These metrics are ill-suited for our purposes since our baseline GAN lacks such an encoder. While it is possible to add an extra network for this purpose [8,12,15], we want to avoid investing effort into a component that is not a part of the actual solution. To this end, we describe two new ways of quantifying disentanglement, neither of which requires an encoder or known factors of variation, and are therefore computable for any image dataset and generator.", "publication_ref": ["b53", "b49", "b1", "b6", "b18", "b1", "b9", "b34", "b48", "b7", "b25", "b31", "b6", "b25", "b31", "b6", "b18", "b7", "b11", "b14"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Perceptual path length", "text": "As noted by Laine [37], interpolation of latent-space vectors may yield surprisingly non-linear changes in the image. For example, features that are absent in either endpoint may appear in the middle of a linear interpolation path. This is a sign that the latent space is entangled and the factors of variation are not properly separated. To quantify this effect, we can measure how drastic changes the image undergoes as we perform interpolation in the latent space. Intuitively, a less curved latent space should result in perceptually smoother transition than a highly curved latent space.\nAs a basis for our metric, we use a perceptually-based pairwise image distance [65] that is calculated as a weighted difference between two VGG16 [58] embeddings, where the weights are fit so that the metric agrees with human perceptual similarity judgments. If we subdivide a latent space interpolation path into linear segments, we can define the total perceptual length of this segmented path as the sum of perceptual differences over each segment, as reported by the image distance metric. A natural definition for the perceptual path length would be the limit of this sum under infinitely fine subdivision, but in practice we approximate it using a small subdivision epsilon = 10 \u22124 . The average perceptual path length in latent space Z, over all possible endpoints, is therefore\nl Z = E 1 2 d G(slerp(z 1 , z 2 ; t)), G(slerp(z 1 , z 2 ; t + )) ,(2)\nwhere z 1 , z 2 \u223c P (z), t \u223c U (0, 1), G is the generator (i.e., g \u2022 f for style-based networks), and d(\u2022, \u2022) evaluates the per- ceptual distance between the resulting images. Here slerp denotes spherical interpolation [56], which is the most appropriate way of interpolating in our normalized input latent space [61]. To concentrate on the facial features instead of background, we crop the generated images to contain only the face prior to evaluating the pairwise image metric. As the metric d is quadratic [65], we divide by 2 . We compute the expectation by taking 100,000 samples.\nComputing the average perceptual path length in W is carried out in a similar fashion:\nl W = E 1 2 d g(lerp(f (z 1 ), f (z 2 ); t)), g(lerp(f (z 1 ), f (z 2 ); t + )) ,(3)\nwhere the only difference is that interpolation happens in W space. Because vectors in W are not normalized in any fashion, we use linear interpolation (lerp). Table 3 shows that this full-path length is substantially shorter for our style-based generator with noise inputs, indicating that W is perceptually more linear than Z. Yet, this measurement is in fact slightly biased in favor of the input latent space Z. If W is indeed a disentangled and \"flattened\" mapping of Z, it may contain regions that are not on the input manifold -and are thus badly reconstructed by the generator -even between points that are mapped from the input manifold, whereas the input latent space Z has no such regions by definition. It is therefore to be expected that if we restrict our measure to path endpoints, i.e., t \u2208 {0, 1}, we should obtain a smaller l W while l Z is not affected. This is indeed what we observe in Table 3.\nTable 4 shows how path lengths are affected by the mapping network. We see that both traditional and style-based generators benefit from having a mapping network, and additional depth generally improves the perceptual path length as well as FIDs. It is interesting that while l W improves in the traditional generator, l Z becomes considerably worse, illustrating our claim that the input latent space can indeed be arbitrarily entangled in GANs. Table 4. The effect of a mapping network in FFHQ. The number in method name indicates the depth of the mapping network. We see that FID, separability, and path length all benefit from having a mapping network, and this holds for both style-based and traditional generator architectures. Furthermore, a deeper mapping network generally performs better than a shallow one.", "publication_ref": ["b36", "b64", "b57", "b55", "b60", "b64"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Linear separability", "text": "If a latent space is sufficiently disentangled, it should be possible to find direction vectors that consistently correspond to individual factors of variation. We propose another metric that quantifies this effect by measuring how well the latent-space points can be separated into two distinct sets via a linear hyperplane, so that each set corresponds to a specific binary attribute of the image.\nIn order to label the generated images, we train auxiliary classification networks for a number of binary attributes, e.g., to distinguish male and female faces. In our tests, the classifiers had the same architecture as the discriminator we use (i.e., same as in [30]), and were trained using the CELEBA-HQ dataset that retains the 40 attributes available in the original CelebA dataset. To measure the separability of one attribute, we generate 200,000 images with z \u223c P (z) and classify them using the auxiliary classification network. We then sort the samples according to classifier confidence and remove the least confident half, yielding 100,000 labeled latent-space vectors.\nFor each attribute, we fit a linear SVM to predict the label based on the latent-space point -z for traditional and w for style-based -and classify the points by this plane. We then compute the conditional entropy H(Y |X) where X are the classes predicted by the SVM and Y are the classes determined by the pre-trained classifier. This tells how much additional information is required to determine the true class of a sample, given that we know on which side of the hyperplane it lies. A low value suggests consistent latent space directions for the corresponding factor(s) of variation.\nWe calculate the final separability score as exp( i H(Y i |X i )), where i enumerates the 40 attributes. Similar to the inception score [53], the exponentiation brings the values from logarithmic to linear domain so that they are easier to compare.\nTables 3 and 4 show that W is consistently better separable than Z, suggesting a less entangled representation. Furthermore, increasing the depth of the mapping network improves both image quality and separability in W, which is in line with the hypothesis that the synthesis network inherently favors a disentangled input representation. Interestingly, adding a mapping network in front of a traditional generator results in severe loss of separability in Z but improves the situation in the intermediate latent space W, and the FID improves as well. This shows that even the traditional generator architecture performs better when we introduce an intermediate latent space that does not have to follow the distribution of the training data.", "publication_ref": ["b29", "b52"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Conclusion", "text": "Based on both our results and parallel work by Chen et al. [6], it is becoming clear that the traditional GAN generator architecture is in every way inferior to a style-based design. This is true in terms of established quality metrics, and we further believe that our investigations to the separation of high-level attributes and stochastic effects, as well as the linearity of the intermediate latent space will prove fruitful in improving the understanding and controllability of GAN synthesis.\nWe note that our average path length metric could easily be used as a regularizer during training, and perhaps some variant of the linear separability metric could act as one, too. In general, we expect that methods for directly shaping the intermediate latent space during training will provide interesting avenues for future work.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Jaakko Lehtinen, David Luebke, and Tuomas Kynk\u00e4\u00e4nniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka J\u00e4nis for compute infrastructure and help with the code release.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. The FFHQ dataset", "text": "We have collected a new dataset of human faces, Flickr-Faces-HQ (FFHQ), consisting of 70,000 high-quality images at 1024 2 resolution (Figure 7). The dataset includes vastly more variation than CELEBA-HQ [30] in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr When we fade \u03c8 \u2192 0, all faces converge to the \"mean\" face of FFHQ. This face is similar for all trained networks, and the interpolation towards it never seems to cause artifacts. By applying negative scaling to styles, we get the corresponding opposite or \"anti-face\". It is interesting that various high-level attributes often flip between the opposites, including viewpoint, glasses, age, coloring, hair length, and often gender.\n(thus inheriting all the biases of that website) and automatically aligned [31] and cropped. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Mechanical Turk allowed us to remove the occasional statues, paintings, or photos of photos. We have made the dataset publicly available at https://github.com/NVlabs/ffhq-dataset", "publication_ref": ["b29", "b30"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "B. Truncation trick in W", "text": "If we consider the distribution of training data, it is clear that areas of low density are poorly represented and thus likely to be difficult for the generator to learn. This is a significant open problem in all generative modeling techniques. However, it is known that drawing latent vectors from a truncated [42,5] or otherwise shrunk [34] sampling space tends to improve average image quality, although some amount of variation is lost.\nWe can follow a similar strategy. To begin, we compute the center of mass of W asw = E z\u223cP (z) [f (z)]. In case of FFHQ this point represents a sort of an average face (Figure 8, \u03c8 = 0). We can then scale the deviation of a given w from the center as w =w + \u03c8(w \u2212w), where \u03c8 < 1. While Brock et al. [5] observe that only a subset of networks is amenable to such truncation even when orthogonal regularization is used, truncation in W space seems to work reliably even without changes to the loss function.", "publication_ref": ["b41", "b4", "b33", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "C. Hyperparameters and training details", "text": "We build upon the official TensorFlow [1] implementation of Progressive GANs by Karras et al. [30], from which we inherit most of the training details. 3 This original setup corresponds to configuration A in Table 1. In particular, we use the same discriminator architecture, resolutiondependent minibatch sizes, Adam [33] hyperparameters, and exponential moving average of the generator. We enable mirror augmentation for CelebA-HQ and FFHQ, but disable it for LSUN. Our training time is approximately one week on an NVIDIA DGX-1 with 8 Tesla V100 GPUs.\nFor our improved baseline (B in Table 1), we make several modifications to improve the overall result quality. We replace the nearest-neighbor up/downsampling in both networks with bilinear sampling, which we implement by lowpass filtering the activations with a separable 2 nd order binomial filter after each upsampling layer and before each downsampling layer [64]. We implement progressive growing the same way as Karras et al. [30], but we start from 8 2 images instead of 4 2 . For the FFHQ dataset, we switch from WGAN-GP to the non-saturating loss [22] with R 1 regularization [44] using \u03b3 = 10. With R 1 we found that the FID scores keep decreasing for considerably longer than with WGAN-GP, and we thus increase the training time from 12M to 25M images. We use the same learning rates as Karras et al. [30] for FFHQ, but we found that setting the learning rate to 0.002 instead of 0.003 for 512 2 and 1024 2 leads to better stability with CelebA-HQ.\nFor our style-based generator (F in Table 1), we use leaky ReLU [41] with \u03b1 = 0.2 and equalized learning rate [30] for all layers. We use the same feature map counts in our convolution layers as Karras et al. [30]. Our mapping network consists of 8 fully-connected layers, and the dimensionality of all input and output activations -including z and w -is 512. We found that increasing the depth of the mapping network tends to make the training unstable with high learning rates. We thus reduce the learning rate by two orders of magnitude for the mapping network, i.e., \u03bb = 0.01 \u2022 \u03bb. We initialize all weights of the convolutional, fully-connected, and affine transform layers using N (0, 1). The constant input in synthesis network is initialized to one. The biases and noise scaling factors are initialized to zero, except for the biases associated with y s that we initialize to one.\nThe classifiers used by our separability metric (Section 4.2) have the same architecture as our discriminator except that minibatch standard deviation [30]  for measuring the separability metric for all generators. We will release the pre-trained classifier networks so that our measurements can be reproduced. We do not use batch normalization [29], spectral normalization [45], attention mechanisms [63], dropout [59], or pixelwise feature vector normalization [30] in our networks.", "publication_ref": ["b0", "b29", "b2", "b32", "b63", "b29", "b21", "b43", "b29", "b40", "b29", "b29", "b29", "b28", "b44", "b62", "b58", "b29"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0"]}, {"heading": "D. Training convergence", "text": "Figure 9 shows how the FID and perceptual path length metrics evolve during the training of our configurations B and F with the FFHQ dataset. With R 1 regularization active in both configurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images. Even when the training has reached the full 1024 2 resolution, the slowly rising path lengths indicate that the improvements in FID come at the cost of a more entangled representation. Considering future work, it is an interesting question whether this is unavoidable, or if it were possible to encourage shorter path lengths without compromising the convergence of FID. Figures 10,11,and 12 show an uncurated set of results for LSUN [62] BEDROOM, CARS, and CATS, respectively. In these images we used the truncation trick from Appendix Bwith \u03c8 = 0.7 for resolutions 4 2 \u2212 32 2 . The accompanying video provides results for style mixing and stochastic variation tests. As can be seen therein, in case of BEDROOM the coarse styles basically control the viewpoint of the camera, middle styles select the particular furniture, and fine styles deal with colors and smaller details of materials. In CARS the effects are roughly similar. Stochastic variation affects primarily the fabrics in BEDROOM, backgrounds and headlamps in CARS, and fur, background, and interestingly, the positioning of paws in CATS. Somewhat surprisingly the wheels of a car never seem to rotate based on stochastic inputs. These datasets were trained using the same setup as FFHQ for the duration of 70M images for BEDROOM and CATS, and 46M for CARS. We suspect that the results for BEDROOM are starting to approach the limits of the training data, as in many images the most objectionable issues are the severe compression artifacts that have been inherited from the low-quality training data. CARS has much higher quality training data that also allows higher spatial resolution (512 \u00d7 384 instead of 256 2 ), and CATS continues to be a difficult dataset due to the high intrinsic variation in poses, zoom levels, and backgrounds.  ", "publication_ref": ["b10", "b11", "b61"], "figure_ref": ["fig_7"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "TensorFlow: a system for large-scale machine learning", "journal": "", "year": "2016", "authors": "M Abadi; P Barham; J Chen; Z Chen; A Davis; J Dean; M Devin; S Ghemawat; G Irving; M Isard; M Kudlur; J Levenberg; R Monga; S Moore; D G Murray; B Steiner; P Tucker; V Vasudevan; P Warden; M Wicke; Y Yu; X Zheng"}, {"ref_id": "b1", "title": "On the emergence of invariance and disentangling in deep representations. CoRR, abs", "journal": "", "year": "1350", "authors": "A Achille; S Soatto"}, {"ref_id": "b2", "title": "GAN dissection: Visualizing and understanding generative adversarial networks", "journal": "", "year": "2019", "authors": "D Bau; J Zhu; H Strobelt; B Zhou; J B Tenenbaum; W T Freeman; A Torralba"}, {"ref_id": "b3", "title": "Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images", "journal": "CoRR", "year": "2018", "authors": "M Ben-Yosef; D Weinshall"}, {"ref_id": "b4", "title": "Large scale GAN training for high fidelity natural image synthesis. CoRR, abs/1809.11096", "journal": "", "year": "2008", "authors": "A Brock; J Donahue; K Simonyan"}, {"ref_id": "b5", "title": "On self modulation for generative adversarial networks", "journal": "CoRR", "year": "2008", "authors": "T Chen; M Lucic; N Houlsby; S Gelly"}, {"ref_id": "b6", "title": "Isolating sources of disentanglement in variational autoencoders", "journal": "", "year": "2018", "authors": "T Q Chen; X Li; R B Grosse; D K Duvenaud"}, {"ref_id": "b7", "title": "InfoGAN: interpretable representation learning by information maximizing generative adversarial nets", "journal": "", "year": "2016", "authors": "X Chen; Y Duan; R Houthooft; J Schulman; I Sutskever; P Abbeel"}, {"ref_id": "b8", "title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "journal": "", "year": "2015", "authors": "E L Denton; S Chintala; A Szlam; R Fergus"}, {"ref_id": "b9", "title": "Disentangling factors of variation via generative entangling. CoRR, abs/1210", "journal": "", "year": "2012", "authors": "G Desjardins; A Courville; Y Bengio"}, {"ref_id": "b10", "title": "Online adaptative curriculum learning for GANs", "journal": "", "year": "2018", "authors": "T Doan; J Monteiro; I Albuquerque; B Mazoure; A Durand; J Pineau; R D Hjelm"}, {"ref_id": "b11", "title": "Adversarial feature learning", "journal": "CoRR", "year": "2016", "authors": "J Donahue; P Kr\u00e4henb\u00fchl; T Darrell"}, {"ref_id": "b12", "title": "Learning to generate chairs with convolutional neural networks. CoRR, abs/1411", "journal": "", "year": "2014", "authors": "A Dosovitskiy; J T Springenberg; T Brox"}, {"ref_id": "b13", "title": "Improving generalization performance using double backpropagation", "journal": "IEEE Transactions on Neural Networks", "year": "1992", "authors": "H Drucker; Y L Cun"}, {"ref_id": "b14", "title": "Adversarially learned inference", "journal": "", "year": "2017", "authors": "V Dumoulin; I Belghazi; B Poole; A Lamb; M Arjovsky; O Mastropietro; A Courville"}, {"ref_id": "b15", "title": "Feature-wise transformations. Distill", "journal": "", "year": "2018", "authors": "V Dumoulin; E Perez; N Schucher; F Strub; H D Vries; A Courville; Y Bengio"}, {"ref_id": "b16", "title": "A learned representation for artistic style", "journal": "CoRR", "year": "2016", "authors": "V Dumoulin; J Shlens; M Kudlur"}, {"ref_id": "b17", "title": "Generative multi-adversarial networks", "journal": "CoRR", "year": "2016", "authors": "I P Durugkar; I Gemp; S Mahadevan"}, {"ref_id": "b18", "title": "A framework for the quantitative evaluation of disentangled representations", "journal": "", "year": "2018", "authors": "C Eastwood; C K I Williams"}, {"ref_id": "b19", "title": "Image style transfer using convolutional neural networks", "journal": "", "year": "2016", "authors": "L A Gatys; A S Ecker; M Bethge"}, {"ref_id": "b20", "title": "Exploring the structure of a real-time, arbitrary neural artistic stylization network", "journal": "", "year": "2017", "authors": "G Ghiasi; H Lee; M Kudlur; V Dumoulin; J Shlens"}, {"ref_id": "b21", "title": "Generative Adversarial Networks", "journal": "", "year": "2009", "authors": "I Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b22", "title": "MIXGAN: learning concepts from different domains for mixture generation", "journal": "", "year": "2018", "authors": "W.-S Z Guang-Yuan; Hong-Xing Hao;  Yu"}, {"ref_id": "b23", "title": "Improved training of Wasserstein GANs", "journal": "CoRR", "year": "2017", "authors": "I Gulrajani; F Ahmed; M Arjovsky; V Dumoulin; A C Courville"}, {"ref_id": "b24", "title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium", "journal": "", "year": "2017", "authors": "M Heusel; H Ramsauer; T Unterthiner; B Nessler; S Hochreiter"}, {"ref_id": "b25", "title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "journal": "", "year": "2017", "authors": "I Higgins; L Matthey; A Pal; C Burgess; X Glorot; M Botvinick; S Mohamed; A Lerchner"}, {"ref_id": "b26", "title": "Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs", "journal": "", "year": "1703", "authors": "X Huang; S J Belongie"}, {"ref_id": "b27", "title": "Multimodal unsupervised image-to-image translation", "journal": "", "year": "2018", "authors": "X Huang; M Liu; S J Belongie; J Kautz"}, {"ref_id": "b28", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "CoRR", "year": "2015", "authors": "S Ioffe; C Szegedy"}, {"ref_id": "b29", "title": "Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs", "journal": "", "year": "1710", "authors": "T Karras; T Aila; S Laine; J Lehtinen"}, {"ref_id": "b30", "title": "One millisecond face alignment with an ensemble of regression trees", "journal": "", "year": "2014", "authors": "V Kazemi; J Sullivan"}, {"ref_id": "b31", "title": "Disentangling by factorising", "journal": "", "year": "2018", "authors": "H Kim; A Mnih"}, {"ref_id": "b32", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba"}, {"ref_id": "b33", "title": "Glow: Generative flow with invertible 1x1 convolutions. CoRR", "journal": "", "year": "2008", "authors": "D P Kingma; P "}, {"ref_id": "b34", "title": "Auto-encoding variational bayes", "journal": "", "year": "2014", "authors": "D P Kingma; M Welling"}, {"ref_id": "b35", "title": "The gan landscape: Losses, architectures, regularization, and normalization", "journal": "CoRR", "year": "2018", "authors": "K Kurach; M Lucic; X Zhai; M Michalski; S Gelly"}, {"ref_id": "b36", "title": "Feature-based metrics for exploring the latent space of generative models", "journal": "", "year": "2018", "authors": "S Laine"}, {"ref_id": "b37", "title": "Universal style transfer via feature transforms", "journal": "", "year": "2017", "authors": "Y Li; C Fang; J Yang; Z Wang; X Lu; M.-H Yang"}, {"ref_id": "b38", "title": "Demystifying neural style transfer. CoRR, abs", "journal": "", "year": "1036", "authors": "Y Li; N Wang; J Liu; X Hou"}, {"ref_id": "b39", "title": "Are GANs created equal? a large-scale study", "journal": "", "year": "1711", "authors": "M Lucic; K Kurach; M Michalski; S Gelly; O Bousquet"}, {"ref_id": "b40", "title": "Rectifier nonlinearities improve neural network acoustic models", "journal": "", "year": "2013", "authors": "A L Maas; A Y Hannun; A Ng"}, {"ref_id": "b41", "title": "Megapixel size image creation using generative adversarial networks", "journal": "", "year": "1706", "authors": "M Marchesi"}, {"ref_id": "b42", "title": "dsprites: Disentanglement testing sprites dataset", "journal": "", "year": "", "authors": "L Matthey; I Higgins; D Hassabis; A Lerchner"}, {"ref_id": "b43", "title": "Which training methods for GANs do actually converge? CoRR, abs/1801.04406", "journal": "", "year": "2009", "authors": "L Mescheder; A Geiger; S Nowozin"}, {"ref_id": "b44", "title": "Spectral normalization for generative adversarial networks", "journal": "CoRR", "year": "2018", "authors": "T Miyato; T Kataoka; M Koyama; Y Yoshida"}, {"ref_id": "b45", "title": "cGANs with projection discriminator", "journal": "CoRR", "year": "2018", "authors": "T Miyato; M Koyama"}, {"ref_id": "b46", "title": "Dropout-gan: Learning from a dynamic ensemble of discriminators", "journal": "CoRR", "year": "2018", "authors": "G Mordido; H Yang; C Meinel"}, {"ref_id": "b47", "title": "Cluster-GAN : Latent space clustering in generative adversarial networks", "journal": "CoRR", "year": "2018", "authors": "S Mukherjee; H Asnani; E Lin; S Kannan"}, {"ref_id": "b48", "title": "Stochastic backpropagation and approximate inference in deep generative models", "journal": "", "year": "2014", "authors": "D J Rezende; S Mohamed; D Wierstra"}, {"ref_id": "b49", "title": "A survey of inductive biases for factorial representation-learning", "journal": "CoRR", "year": "2016", "authors": "K Ridgeway"}, {"ref_id": "b50", "title": "Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients", "journal": "", "year": "2017", "authors": "A S Ross; F Doshi-Velez"}, {"ref_id": "b51", "title": "Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions", "journal": "", "year": "2018", "authors": "T Sainburg; M Thielk; B Theilman; B Migliori; T Gentner"}, {"ref_id": "b52", "title": "Improved techniques for training GANs", "journal": "", "year": "2016", "authors": "T Salimans; I J Goodfellow; W Zaremba; V Cheung; A Radford; X Chen"}, {"ref_id": "b53", "title": "Learning factorial codes by predictability minimization", "journal": "Neural Computation", "year": "1992", "authors": "J Schmidhuber"}, {"ref_id": "b54", "title": "Improved training with curriculum gans", "journal": "CoRR", "year": "2018", "authors": "R Sharma; S Barratt; S Ermon; V Pande"}, {"ref_id": "b55", "title": "Animating rotation with quaternion curves", "journal": "", "year": "1985", "authors": "K Shoemake"}, {"ref_id": "b56", "title": "Whitening and coloring transform for GANs", "journal": "", "year": "2018", "authors": "A Siarohin; E Sangineto; N Sebe"}, {"ref_id": "b57", "title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "journal": "", "year": "2014", "authors": "K Simonyan; A Zisserman"}, {"ref_id": "b58", "title": "Dropout: A simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "N Srivastava; G Hinton; A Krizhevsky; I Sutskever; R Salakhutdinov"}, {"ref_id": "b59", "title": "High-resolution image synthesis and semantic manipulation with conditional GANs", "journal": "", "year": "2017", "authors": "T Wang; M Liu; J Zhu; A Tao; J Kautz; B Catanzaro"}, {"ref_id": "b60", "title": "Sampling generative networks: Notes on a few effective techniques", "journal": "", "year": "2016", "authors": "T White"}, {"ref_id": "b61", "title": "LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop", "journal": "CoRR", "year": "2015", "authors": "F Yu; Y Zhang; S Song; A Seff; J Xiao"}, {"ref_id": "b62", "title": "Self-attention generative adversarial networks", "journal": "CoRR", "year": "2018", "authors": "H Zhang; I Goodfellow; D Metaxas; A Odena"}, {"ref_id": "b63", "title": "Making convolutional networks shift-invariant again", "journal": "", "year": "2019", "authors": "R Zhang"}, {"ref_id": "b64", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "R Zhang; P Isola; A A Efros; E Shechtman; O Wang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure1. While a traditional generator[30] feeds the latent code though the input layer only, we first map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, before evaluating the nonlinearity. Here \"A\" stands for a learned affine transform, and \"B\" applies learned per-channel scaling factors to the noise input. The mapping network f consists of 8 layers and the synthesis network g consists of 18 layers -two for each resolution (4 2 \u2212 1024 2 ). The output of the last layer is converted to RGB using a separate 1 \u00d7 1 convolution, similar to Karras et al.[30]. Our generator has a total of 26.2M trainable parameters, compared to 23.1M in the traditional generator.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(a) Generated image (b) Stochastic variation (c) Standard deviation", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. Examples of stochastic variation. (a) Two generated images. (b) Zoom-in with different realizations of input noise. While the overall appearance is almost identical, individual hairs are placed very differently. (c) Standard deviation of each pixel over 100 different realizations, highlighting which parts of the images are affected by the noise. The main areas are the hair, silhouettes, and parts of background, but there is also interesting stochastic variation in the eye reflections. Global aspects such as identity and pose are unaffected by stochastic variation.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 .5Figure 5. Effect of noise inputs at different layers of our generator. (a) Noise is applied to all layers. (b) No noise. (c) Noise in fine layers only (64 2 -1024 2 ). (d) Noise in coarse layers only (4 2 -32 2). We can see that the artificial omission of noise leads to featureless \"painterly\" look. Coarse noise causes large-scale curling of hair and appearance of larger background features, while the fine noise brings out the finer curls of hair, finer background detail, and skin pores.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 .6Figure 6. Illustrative example with two factors of variation (image features, e.g., masculinity and hair length). (a) An example training set where some combination (e.g., long haired males) is missing. (b) This forces the mapping from Z to image features to become curved so that the forbidden combination disappears in Z to prevent the sampling of invalid combinations. (c) The learned mapping from Z to W is able to \"undo\" much of the warping.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 .7Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "\u03c8 = \u2212 1 Figure 8 .18Figure8. The effect of truncation trick as a function of style scale \u03c8. When we fade \u03c8 \u2192 0, all faces converge to the \"mean\" face of FFHQ. This face is similar for all trained networks, and the interpolation towards it never seems to cause artifacts. By applying negative scaling to styles, we get the corresponding opposite or \"anti-face\". It is interesting that various high-level attributes often flip between the opposites, including viewpoint, glasses, age, coloring, hair length, and often gender.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 .9Figure 9. FID and perceptual path length metrics over the course of training in our configurations B and F using the FFHQ dataset. Horizontal axis denotes the number of training images seen by the discriminator. The dashed vertical line at 8.4M images marks the point when training has progressed to full 1024 2 resolution. On the right, we show only one curve for the traditional generator's path length measurements, because there is no discernible difference between full-path and endpoint sampling in Z.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 .10Figure 10. Uncurated set of images produced by our style-based generator (config F) with the LSUN BEDROOM dataset at 256 2 . FID computed for 50K images was 2.65.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 11 .11Figure 11. Uncurated set of images produced by our style-based generator (config F) with the LSUN CAR dataset at 512 \u00d7 384. FID computed for 50K images was 3.27.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 12 .12Figure 12. Uncurated set of images produced by our style-based generator (config F) with the LSUN CAT dataset at 256 2 . FID computed for 50K images was 8.53.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Fr\u00e9chet inception distance (FID) for various generator designs (lower is better). In this paper we calculate the FIDs using 50,000 images drawn randomly from the training set, and report the lowest distance encountered over the course of training.", "figure_data": "MethodCelebA-HQFFHQA Baseline Progressive GAN [30]7.798.04B + Tuning (incl. bilinear up/down)6.115.25C + Add mapping and styles5.344.85D + Remove traditional input5.074.88E + Add noise inputs5.064.42F + Mixing regularization5.174.40"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "FIDs in FFHQ for networks trained by enabling the mixing regularization for different percentage of training examples.Here we stress test the trained networks by randomizing 1 . . . 4 latents and the crossover points between them. Mixing regularization improves the tolerance to these adverse operations significantly. Labels E and F refer to the configurations in Table1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Perceptual path lengths and separability scores for various generator architectures in FFHQ (lower is better). We perform the measurements in Z for the traditional network, and in W for stylebased ones. Making the network resistant to style mixing appears to distort the intermediate latent space W somewhat. We hypothesize that mixing makes it more difficult for W to efficiently encode factors of variation that span multiple scales.", "figure_data": "MethodPath length full endSepara-bilityB Traditional generator Z412.0415.310.78D Style-based generator W446.2376.63.61E + Add noise inputs W200.5160.63.54+ Mixing 50%W231.5182.13.51F + Mixing 90%W234.0195.93.79"}], "formulas": [{"formula_id": "formula_0", "formula_text": "AdaIN(x i , y) = y s,i x i \u2212 \u00b5(x i ) \u03c3(x i ) + y b,i ,(1)", "formula_coordinates": [2.0, 87.95, 530.39, 198.41, 23.25]}, {"formula_id": "formula_1", "formula_text": "Source A Source B", "formula_coordinates": [4.0, 87.49, 87.32, 60.37, 64.43]}, {"formula_id": "formula_2", "formula_text": "l Z = E 1 2 d G(slerp(z 1 , z 2 ; t)), G(slerp(z 1 , z 2 ; t + )) ,(2)", "formula_coordinates": [6.0, 341.82, 644.1, 203.29, 34.68]}, {"formula_id": "formula_3", "formula_text": "l W = E 1 2 d g(lerp(f (z 1 ), f (z 2 ); t)), g(lerp(f (z 1 ), f (z 2 ); t + )) ,(3)", "formula_coordinates": [7.0, 71.6, 379.24, 214.76, 34.68]}], "doi": ""}