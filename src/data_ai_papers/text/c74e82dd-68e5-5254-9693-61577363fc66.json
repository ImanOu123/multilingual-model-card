{"title": "Uniform convergence may be unable to explain generalization in deep learning", "authors": "Vaishnavh Nagarajan; J Zico Kolter", "pub_date": "2021-10-17", "abstract": "Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence. While it is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice, these bounds can increase with the training dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by gradient descent (GD) where uniform convergence provably cannot \"explain generalization\" -even if we take into account the implicit bias of GD to the fullest extent possible. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than 1 \u2212 . Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.", "sections": [{"heading": "Introduction", "text": "Explaining why overparameterized deep networks generalize well [29,39] has become an important open question in deep learning. How is it possible that a large network can be trained to perfectly fit randomly labeled data (essentially by memorizing the labels), and yet, the same network when trained to perfectly fit real training data, generalizes well to unseen data? This called for a \"rethinking\" of conventional, algorithm-independent techniques to explain generalization. Specifically, it was argued that learning-theoretic approaches must be reformed by identifying and incorporating the implicit bias/regularization of stochastic gradient descent (SGD) [6,36,31]. Subsequently, a huge variety of novel and refined, algorithm-dependent generalization bounds for deep networks have been developed, all based on uniform convergence, the most widely used tool in learning theory. The ultimate goal of this ongoing endeavor is to derive bounds on the generalization error that (a) are small, ideally non-vacuous (i.e., < 1), (b) reflect the same width/depth dependence as the generalization error (e.g., become smaller with increasing width, as has been surprisingly observed in practice), (c) apply to the network learned by SGD (without any modification or explicit regularization) and (d) increase with the proportion of randomly flipped training labels (i.e., increase with memorization).\nWhile every bound meets some of these criteria (and sheds a valuable but partial insight into generalization in deep learning), there is no known bound that meets all of them simultaneously. While most bounds [30, 3,12,32,28,33] apply to the original network, they are neither numerically small for realistic dataset sizes, nor exhibit the desired width/depth dependencies (in fact, these bounds grow exponentially with the depth). The remaining bounds hold either only on a compressed network [2] or a stochastic network [22] or a network that has been further modified via optimization or more than one of the above [8,40]. Extending these bounds to the original network is understood to be highly non-trivial [28]. While strong width-independent bounds have been derived for two-layer ReLU networks [24,1], these rely on a carefully curated, small learning rate and/or large batch size. (We refer the reader to Appendix A for a tabular summary of these bounds.)\nIn our paper, we bring to light another fundamental issue with existing bounds. We demonstrate that these bounds violate another natural but largely overlooked criterion for explaining generalization: (e) the bounds should decrease with the dataset size at the same rate as the generalization error. In fact, we empirically observe that these bounds can increase with dataset size, which is arguably a more concerning observation than the fact that they are large for a specific dataset size.\nMotivated by the seemingly insurmountable hurdles towards developing bounds satisfying all the above five necessary criteria, we take a step back and examine how the underlying technique of uniform convergence may itself be inherently limited in the overparameterized regime. Specifically, we present examples of overparameterized linear classifiers and neural networks trained by GD (or SGD) where uniform convergence can provably fail to explain generalization. Intuitively, our examples highlight that overparameterized models trained by gradient descent can learn decision boundaries that are largely \"simple\" -and hence generalize well -but have \"microscopic complexities\" which cannot be explained away by uniform convergence. Thus our results call into question the active ongoing pursuit of using uniform convergence to fully explain generalization in deep learning.\nOur contributions in more detail. We first show that in practice certain weight norms of deep ReLU networks, such as the distance from initialization, increase polynomially with the number of training examples (denoted by m). We then show that as a result, existing generalization bounds -all of which depend on such weight norms -fail to reflect even a dependence on m even reasonably similar to the actual test error, violating criterion (e); for sufficiently small batch sizes, these bounds even grow with the number of examples. This observation uncovers a conceptual gap in our understanding of the puzzle, by pointing towards a source of vacuity unrelated to parameter count.\nAs our second contribution, we consider three example setups of overparameterized models trained by (stochastic) gradient descent -a linear classifier, a sufficiently wide neural network with ReLUs and an infinite width neural network with exponential activations (with the hidden layer weights frozen) -that learn some underlying data distribution with small generalization error (say, at most\n). These settings also simulate our observation that norms such as distance from initialization grow with dataset size m. More importantly, we prove that, in these settings, any two-sided uniform convergence bound would yield a (nearly) vacuous generalization bound.\nNotably, this vacuity holds even if we \"aggressively\" take implicit regularization into account while applying uniform convergence -described more concretely as follows. Recall that roughly speaking a uniform convergence bound essentially evaluates the complexity of a hypothesis class (see Definition 3.2). As suggested by Zhang et al. [39], one can tighten uniform convergence bounds by pruning the hypothesis class to remove extraneous hypotheses never picked by the learning algorithm for the data distribution of interest. In our setups, even if we apply uniform convergence on the set of only those hypotheses picked by the learner whose test errors are all negligible (at most ), one can get no better than a nearly vacuous bound on the generalization error (that is at least 1 \u2212 ). In this sense, we say that uniform convergence provably cannot explain generalization in our settings. Finally, we note that while nearly all existing uniform convergence-based techniques are two-sided, we show that even PAC-Bayesian bounds, which are typically presented only as one-sided convergence, also boil down to nearly vacuous guarantees in our settings.", "publication_ref": ["b14", "b5", "b11", "b2", "b7", "b8", "b1", "b15", "b0", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Weight norms vs. training set size m. Prior works like Neyshabur et al. [31] and Nagarajan and Kolter [27] have studied the behavior of weight norms in deep learning. Although these works do not explicitly study the dependence of these norms on training set size m, one can infer from their plots that weight norms of deep networks show some increase with m. Belkin et al. [4] reported a similar paradox in kernel learning, observing that norms that appear in kernel generalization bounds increase with m, and that this is due to noise in the labels. Kawaguchi et al. [20] showed that there exist linear models with arbitrarily large weight norms that can generalize well, although such weights are not necessarily found by gradient descent. We crucially supplement these observations in three ways. First, we empirically and theoretically demonstrate how, even with zero label noise (unlike [4]) and by gradient descent (unlike [20]), a significant level of m-dependence can arise in the weight normssignificant enough to make even the generalization bound grow with m. Next, we identify uniform convergence as the root cause behind this issue, and thirdly and most importantly, we provably demonstrate this is so.\nWeaknesses of Uniform Convergence. Traditional wisdom is that uniform convergence bounds are a bad choice for complex classifiers like k-nearest neighbors because these hypotheses classes have infinite VC-dimension (which motivated the need for stability based generalization bounds in these cases [34,5]). However, this sort of an argument against uniform convergence may still leave one with the faint hope that, by aggressively pruning the hypothesis class (depending on the algorithm and the data distribution), one can achieve meaningful uniform convergence. In contrast, we seek to rigorously and thoroughly rule out uniform convergence in the settings we study. We do this by first defining the tightest form of uniform convergence in Definition 3.3 -one that lower bounds any uniform convergence bound -and then showing that even this bound is vacuous in our settings.\nAdditionally, we note that we show this kind of failure of uniform convergence for linear classifiers, which is a much simpler model compared to k-nearest neighbors.\nFor deep networks, Zhang et al. [39] showed that applying uniform convergence on the whole hypothesis class fails, and that it should instead be applied in an algorithm-dependent way. Ours is a much different claim -that uniform convergence is inherently problematic in that even the algorithm-dependent application would fail -casting doubt on the rich line of post-Zhang et al. [39] algorithm-dependent approaches. At the same time, we must add the disclaimer that our results do not preclude the fact that uniform convergence may still work if GD is run with explicit regularization (such as weight decay). Such a regularized setting however, is not the main focus of the generalization puzzle [39,29].\nPrior works [37,35] have also focused on understanding uniform convergence for learnability of learning problems. Roughly speaking, learnability is a strict notion that does not have to hold even though an algorithm may generalize well for simple distributions in a learning problem. While we defer the details of these works in Appendix I, we emphasize here that these results are orthogonal to (i.e., neither imply nor contradict) our results.", "publication_ref": ["b3", "b3", "b9", "b4", "b14", "b14", "b14", "b12", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Existing bounds vs. training set size", "text": "As we stated in criterion (e) in the introduction, a fundamental requirement from a generalization bound, however numerically large the bound may be, is that it should vary inversely with the size of the training dataset size (m) like the observed generalization error. Such a requirement is satisfied even by standard parameter-count-based VC-dimension bounds, like O(dh/ \u221a m) for depth d, width h ReLU networks [14]. Recent works have \"tightened\" the parameter-count-dependent terms in these bounds by replacing them with seemingly innocuous norm-based quantities; however, we show below that this has also inadvertently introduced training-set-size-count dependencies in the numerator, contributing to the vacuity of bounds. With these dependencies, the generalization bounds even increase with training dataset size for small batch sizes.\nSetup and notations. We focus on fully connected networks of depth d = 5, width h = 1024 trained on MNIST, although we consider other settings in Appendix B. We use SGD with learning rate 0.1 and batch size 1 to minimize cross-entropy loss until 99% of the training data are classified correctly by a margin of at least \u03b3 = 10 i.e., if we denote by f (x)[y] the real-valued logit output (i.e., pre-softmax) on class y for an input x, we ensure that for 99% of the data (x, y), the margin \u0393(f (x), y)\n:= f (x)[y] \u2212 max y =y f (x)[y ] is at least \u03b3 .\nWe emphasize that, from the perspective of generalization guarantees, this stopping criterion helps standardize training across different hyperparameter values, including different values of m [31]. Now, observe that for this particular stopping criterion, the test error empirically decreases with size m as 1/m 0.43 as seen in Figure 1 (third plot). However, we will see that the story is starkly different for the generalization bounds.\nNorms grow with training set size m. Before we examine the overall generalization bounds themselves, we first focus on two quantities that recur in the numerator of many recent bounds: the 2 distance of the weights from their initialization [8,27] and the product of spectral norms of the weight matrices of the network [32,3]. We observe in Figure 1 (first two plots, blue lines) that both these quantities grow at a polynomial rate with m: the former at the rate of at least m 0.4 and the latter at a rate of m. Our observation is a follow-up to Nagarajan and Kolter [27] who argued that while distance of the parameters from the origin grows with width as \u2126( \u221a h), the distance from initialization is  In the first figure, we plot (i) 2 the distance of the network from the initialization and (ii) the 2 distance between the weights learned on two random draws of training data starting from the same initialization. In the second figure we plot the product of spectral norms of the weights matrices. In the third figure, we plot the test error. In the fourth figure, we plot the bounds from [32,3]. Note that we have presented log-log plots and the exponent of m can be recovered from the slope of these plots.\nwidth-independent (and even decreases with width); hence, they concluded that incorporating the initialization would improve generalization bounds by a \u2126( \u221a h) factor. However, our observations imply that, even though distance from initialization would help explain generalization better in terms of width, it conspicuously fails to help explain generalization in terms of its dependence on m (and so does distance from origin as we show in Appendix Figure 5). 1 Additionally, we also examine another quantity as an alternative to distance from initialization: the 2 diameter of the parameter space explored by SGD. That is, for a fixed initialization and data distribution, we consider the set of all parameters learned by SGD across all draws of a dataset of size m; we then consider the diameter of the smallest ball enclosing this set. If this diameter exhibits a better behavior than the above quantities, one could then explain generalization better by replacing the distance from initialization with the distance from the center of this ball in existing bounds. As a lower bound on this diameter, we consider the distance between the weights learned on two independently drawn datasets from the given initialization. Unfortunately, we observe that even this quantity shows a similar undesirable behavior with respect to m like distance from initialization (see Figure 1, first plot, orange line).\nThe bounds grow with training set size m. We now turn to evaluating existing guarantees from Neyshabur et al. [32] and Bartlett et al. [3]. As we note later, our observations apply to many other bounds too. Let W 1 , . . . , W d be the weights of the learned network (with W 1 being the weights adjacent to the inputs), Z 1 , . . . , Z d the random initialization, D the true data distribution and S the training dataset. For all inputs x, let x 2 \u2264 B. Let \u2022 2 , \u2022 F , \u2022 2,1 denote the spectral norm, the Frobenius norm and the matrix (2, 1)-norm respectively; let 1[\u2022] be the indicator function. Recall that \u0393(f (x), y) := f (x)[y] \u2212 max y =y f (x)[y ] denotes the margin of the network on a datapoint. Then, for any constant \u03b3, these generalization guarantees are written as follows, ignoring log factors:\nPr D [\u0393(f (x), y) \u2264 0] \u2264 1 m (x,y)\u2208S 1[\u0393(f (x), y) \u2264 \u03b3] + generalization error bound.(1)\nHere the generalization error bound is of the form\nO Bd \u221a h \u03b3 \u221a m d k=1 W k 2 \u00d7 dist where dist equals d k=1 W k \u2212Z k 2 F W k 2 2 in [32] and 1 d \u221a h d k=1 W k \u2212Z k 2,1 W k 2 2/3 3/2 in [3].\nIn our experiments, since we train the networks to fit at least 99% of the datapoints with a margin of 10, in the above bounds, we set \u03b3 = 10 so that the first train error term in the right hand side of Equation 1 becomes a small value of at most 0.01. We then plot in Figure 1 (fourth plot), the second term above, namely the generalization error bounds, and observe that all these bounds grow with the sample size m as \u2126(m 0.68 ), thanks to the fact that the terms in the numerator of these bounds grow with m. Since we are free to plug in \u03b3 in Equation 1, one may wonder whether there exists a better choice of \u03b3 for which we can observe a smaller increase on m (since the plotted terms inversely depend on \u03b3). However, in Appendix Figure 6 we establish that even for larger values of \u03b3, this m-dependence remains. Also note that, although we do not plot the bounds from [28,12], these have nearly identical norms in their numerator, and so one would not expect these bounds to show radically better behavior with respect to m. Finally, we defer experiments conducted for other varied settings, and the neural network bound from [33] to Appendix B.\nWhile the bounds might show better m-dependence for other settings -indeed, for larger batches, we show in Appendix B that the bounds behave better -we believe that the egregious break down of these bounds in this setting (and many other hyperparameter settings as presented in Appendix B) must imply fundamental issues with the bounds themselves. While this may be addressed to some extent with a better understanding of implicit regularization in deep learning, we regard our observations as a call for taking a step back and clearly understanding any inherent limitations in the theoretical tool underlying all these bounds namely, uniform convergence. 2 3 Provable failure of uniform convergence\nPreliminaries. Let H be a class of hypotheses mapping from X to R, and let D be a distribution over X \u00d7 {\u22121, +1}. The loss function we mainly care about is the 0-1 error; but since a direct analysis of the uniform convergence of the 0-1 error is hard, sometimes a more general margin-based surrogate of this error (also called as ramp loss) is analyzed for uniform convergence. Specifically, given the classifier's logit output y \u2208 R and the true label y \u2208 {\u22121, +1}, define\nL (\u03b3) (y , y) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 yy \u2264 0 1 \u2212 yy \u03b3 yy \u2208 (0, \u03b3) 0 yy \u2265 \u03b3.\nNote that L (0) is the 0-1 error, and L (\u03b3) an upper bound on the 0-1 error. We define for any L, the expected loss as L D (h) := E (x,y)\u223cD [L(h(x), y)] and the empirical loss on a dataset S of m datapointsL S (h) := 1 m (x,y)\u2208S L(h(x), y). Let A be the learning algorithm and let h S be the hypothesis output by the algorithm on a dataset S (assume that any training-data-independent randomness, such as the initialization/data-shuffling is fixed).\nFor a given \u03b4 \u2208 (0, 1), the generalization error of the algorithm is essentially a bound on the difference between the error of the hypothesis h S learned on a training set S and the expected error over D, that holds with high probability of at least 1 \u2212 \u03b4 over the draws of S. More formally: Definition 3.1. The generalization error of A with respect to loss L is the smallest value gen (m, \u03b4)\nsuch that: Pr S\u223cD m L D (h S ) \u2212L S (h S ) \u2264 gen (m, \u03b4) \u2265 1 \u2212 \u03b4.\nTo theoretically bound the generalization error of the algorithm, the most common approach is to provide a two-sided uniform convergence bound on the hypothesis class used by the algorithm, where, for a given draw of S, we look at convergence for all the hypotheses in H instead of just h S : Definition 3.2. The uniform convergence bound with respect to loss L is the smallest value unif (m, \u03b4) such that:\nPr S\u223cD m sup h\u2208H L D (h) \u2212L S (h) \u2264 unif (m, \u03b4) \u2265 1 \u2212 \u03b4.\nTightest algorithm-dependent uniform convergence. The bound given by unif can be tightened by ignoring many extraneous hypotheses in H never picked by A for a given simple distribution D. This is typically done by focusing on a norm-bounded class of hypotheses that the algorithm A implicitly restricts itself to. Let us take this to the extreme by applying uniform convergence on \"the smallest possible class\" of hypotheses, namely, only those hypotheses that are picked by A under D, excluding everything else. Observe that pruning the hypothesis class any further would not imply a bound on the generalization error, and hence applying uniform convergence on this aggressively pruned hypothesis class would yield the tightest possible uniform convergence bound. Recall that we care about this formulation because our goal is to rigorously and thoroughly rule out the possibility that no kind of uniform convergence bound, however cleverly applied, can explain generalization in our settings of interest (which we will describe later).\nTo formally capture this bound, it is helpful to first rephrase the above definition of unif : we can say that unif (m, \u03b4) is the smallest value for which there exists a set of sample sets\nS \u03b4 \u2286 (X \u00d7 {\u22121, 1}) m for which P r S\u223cD m [S \u2208 S \u03b4 ] \u2265 1 \u2212 \u03b4 and furthermore, sup S\u2208S \u03b4 sup h\u2208H |L D (h) \u2212L S (h)| \u2264 unif (m, \u03b4).\nObserve that this definition is equivalent to Definition 3.2. Extending this rephrased definition, we can define the tightest uniform convergence bound by replacing H here with only those hypotheses that are explored by the algorithm A under the datasets belonging to S \u03b4 : Definition 3.3. The tightest algorithm-dependent uniform convergence bound with respect to loss L is the smallest value unif-alg (m, \u03b4) for which there exists a set of sample sets S \u03b4 such that P r S\u223cD m [S \u2208 S \u03b4 ] \u2265 1 \u2212 \u03b4 and if we define the space of hypotheses explored by A on S \u03b4 as H \u03b4 := S\u2208S \u03b4 {h S } \u2286 H, the following holds:\nsup S\u2208S \u03b4 sup h\u2208H \u03b4 L D (h) \u2212L S (h) \u2264 unif-alg (m, \u03b4).\nIn the following sections, through examples of overparameterized models trained by GD (or SGD), we argue how even the above tightest algorithm-dependent uniform convergence can fail to explain generalization. i.e., in these settings, even though gen is smaller than a negligible value , we show that unif-alg is large (specifically, at least 1 \u2212 ). Before we delve into these examples, below we quickly outline the key mathematical idea by which uniform convergence is made to fail.\nConsider a scenario where the algorithm generalizes well i.e., for every training setS, hS has zero error onS and has small test error. While this means that hS has small error on random draws of a test set, it may still be possible that for every such hS, there exists a corresponding \"bad\" datasetS -that is not random, but rather dependent onS -on which hS has a large empirical error (say 1). Unfortunately, uniform convergence runs into trouble while dealing with such bad datasets. Specifically, as we can see from the above definition, uniform convergence demands that |L D (hS) \u2212L S (hS)| be small on all datasets in S \u03b4 , which excludes a \u03b4 fraction of the datasets. While it may be tempting to think that we can somehow exclude the bad dataset as part of the \u03b4 fraction, there is a significant catch here: we can not carve out a \u03b4 fraction specific to each hypothesis; we can ignore only a single chunk of \u03b4 mass common to all hypotheses in H \u03b4 . This restriction turns out to be a tremendous bottleneck: despite ignoring this \u03b4 fraction, for most hS \u2208 H \u03b4 , the corresponding bad setS would still be left in S \u03b4 . Then, for all such hS, L D (hS) would be small butL S (hS) large; we can then set the S inside the sup S\u2208S \u03b4 to beS to conclude that unif-alg is indeed vacuous. This is the kind of failure we will demonstrate in a high-dimensional linear classifier in the following section, and a ReLU neural network in Section 3.2, and an infinitely wide exponential-activation neural network in Appendix F -all trained by GD or SGD. 3 Note: Our results about failure of uniform convergence holds even for bounds that output a different value for each hypothesis. In this case, the tightest uniform convergence bound for a given hypothesis would be at least as large as sup S\u2208S \u03b4 |L D (hS) \u2212L S (hS)| which by a similar argument would be vacuous for most draws of the training setS. We discuss this in more detail in Appendix G.4.", "publication_ref": ["b7", "b2", "b7", "b2", "b0", "b7", "b2", "b8", "b1", "b2"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "High-dimensional linear classifier", "text": "Why a linear model? Although we present a neural network example in the next section, we first emphasize why it is also important to understand how uniform convergence could fail for linear classifiers trained using GD. First, it is more natural to expect uniform convergence to yield poorer bounds in more complicated classifiers; linear models are arguably the simplest of classifiers, and hence showing failure of uniform convergence in these models is, in a sense, the most interesting. Secondly, recent works (e.g., [18]) have shown that as the width of a deep network goes to infinity, under some conditions, the network converges to a high-dimensional linear model (trained on a high-dimensional transformation of the data) -thus making the study of high-dimensional linear models relevant to us. Note that our example is not aimed at modeling the setup of such linearized neural networks. However, it does provide valuable intuition about the mechanism by which uniform convergence fails, and we show how this extends to neural networks in the later sections.\nSetup. Let each input be a K + D dimensional vector (think of K as a small constant and D much larger than m). The value of any input x is denoted by (x 1 , x 2 ) where x 1 \u2208 R K and x 2 \u2208 R D . Let the centers of the (two) classes be determined by an arbitrary vector u \u2208 R K such that u 2 = 1/ \u221a m. Let D be such that the label y has equal probability of being +1 and \u22121, and x 1 = 2 \u2022 y \u2022 u while x 2 is sampled independently from a spherical Gaussian, N (0, 32 D I). 4 Note that the distribution is linearly separable based on the first few (K) dimensions. For the learning algorithm A, consider a linear classifier with weights w = (w 1 , w 2 ) and whose output is h(x) = w 1 x 1 + w 2 x 2 . Assume the weights are initialized to the origin. Given a dataset S, A takes a gradient step of learning rate 1 to maximize y \u2022 h(x) for each (x, y) \u2208 S. Hence, regardless of the batch size, the learned weights would satisfy, w 1 = 2mu and w 2 = i y (i) x (i) 2 . Note that effectively w 1 is aligned correctly along the class boundary while w 2 is high-dimensional Gaussian noise. It is fairly simple to show that this algorithm achieves zero training error for most draws of the training set. At the same time, for this setup, we have the following lower bound on uniform convergence for the L (\u03b3) loss: 5\nTheorem 3.1. For any , \u03b4 > 0, \u03b4 \u2264 1/4, when D = \u2126 max m ln m \u03b4 , m ln 1 , \u03b3 \u2208 [0, 1], the L (\u03b3) loss satisfies gen (m, \u03b4) \u2264 , while unif-alg (m, \u03b4) \u2265 1 \u2212 . Furthermore, for all \u03b3 \u2265 0, for the L (\u03b3) loss, unif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4).\nIn other words, even the tightest uniform convergence bound is nearly vacuous despite good generalization. In order to better appreciate the implications of this statement, it will be helpful to look at the bound a standard technique would yield here. For example, the Rademacher complexity of the class of 2 -norm bounded linear classifiers would yield a bound of the form O( w 2 /(\u03b3 \u221a m)) where \u03b3 is the margin on the training data. In this setup, the weight norm grows with dataset size as w 2 = \u0398( \u221a m) (which follows from the fact that w 2 is a Gaussian with m/D variance along each of the D dimensions) and \u03b3 = \u0398(1). Hence, the Rademacher bound here would evaluate to a constant much larger than . One might persist and think that perhaps, the characterization of w to be bounded in 2 norm does not fully capture the implicit bias of the algorithm. Are there other properties of the Gaussian w 2 that one could take into account to identify an even smaller class of hypotheses for which uniform convergence may work after all? Unfortunately, our statement rules this out: even after fixing w 1 to the learned value (2mu) and for any possible 1 \u2212 \u03b4 truncation of the Gaussian w 2 , the resulting pruned class of weights -despite all of them having a test error less than -would give only nearly vacuous uniform convergence bounds as unif-alg (m, \u03b4) \u2265 1 \u2212 . Proof outline. We now provide an outline of our argument for Theorem 3.1, deferring the proof to the appendix. First, the small generalization (and test) error arises from the fact that w 1 is aligned correctly along the true boundary; at the same time, the noisy part of the classifier w 2 is poorly aligned with at least 1 \u2212 mass of the test inputs, and hence does not dominate the output of the classifier on test data -preserving the good fit of w 1 on the test data. On the other hand, at a very high level, under the purview of uniform convergence, we can argue that the noise vector w 2 is effectively stripped of its randomness. This misleads uniform convergence into believing that the D noisy dimensions (where D > m) contribute meaningfully to the representational complexity of the classifier, thereby giving nearly vacuous bounds. We describe this more concretely below.\nAs a key step in our argument, we show that w.h.p over draws of S, even though the learned classifier h S correctly classifies most of the randomly picked test data, it completely misclassifies a \"bad\" dataset, namely S = {((x 1 , \u2212x 2 ), y) | (x, y) \u2208 S} which is the noise-negated version of S. Now recall that to compute unif-alg one has to begin by picking a sample set space S \u03b4 of mass 1 \u2212 \u03b4. We first argue that for any choice of S \u03b4 , there must exist S such that all the following four events hold: (i) S \u2208 S \u03b4 , (ii) the noise-negated S \u2208 S \u03b4 , (iii) h S has test error less than and (iv) h S completely misclassifies S . We prove the existence of such an S by arguing that over draws from D m , there is non-zero probability of picking a dataset that satisfies these four conditions. Note that our argument for this crucially makes use of the fact that we have designed the \"bad\" dataset in a way that it has the same distribution as the training set, namely D m . Finally, for a given S \u03b4 , if we have an S satisfying (i) to (iv), we can prove our claim\nas unif-alg (m, \u03b4) = sup S\u2208S \u03b4 sup h\u2208H \u03b4 |L D (h)\u2212L S (h)| \u2265 |L D (h S )\u2212L S (h S )| = | \u22121| = 1\u2212 . Remark 3.1.\nOur analysis depends on the fact that unif-alg is a two-sided convergence boundwhich is what existing techniques bound -and our result would not apply for hypothetical one-sided uniform convergence bounds. While PAC-Bayes based bounds are typically presented as one-sided bounds, we show in Appendix J that even these are lower-bounded by the two-sided unif-alg . To the best of our knowledge, it is non-trivial to make any of these tools purely one-sided.\nRemark 3.2. The classifier modified by setting w 2 \u2190 0, has small test error and also enjoys non-vacuous bounds as it has very few parameters. However, such a bound would not fully explain why the original classifier generalizes well. One might then wonder if such a bound could be extended to the original classifier, like it was explored in Nagarajan and Kolter [28] for deep networks. Our result implies that no such extension is possible in this particular example.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "ReLU neural network", "text": "We now design a non-linearly separable task (with no \"noisy\" dimensions) where a sufficiently wide ReLU network trained in the standard manner, like in the experiments of Section 2 leads to failure of uniform convergence. For our argument, we will rely on a classifier trained empirically, in contrast to our linear examples where we rely on an analytically derived expression for the learned classifier. Thus, this section illustrates that the effects we modeled theoretically in the linear classifier are indeed reflected in typical training settings, even though here it is difficult to precisely analyze the learning process. We also refer the reader to Appendix F, where we present an example of a neural network with exponential activation functions for which we do derive a closed form expression.\nSetup. We consider a distribution that was originally proposed in [11] as the \"adversarial spheres\" dataset (although with slightly different hyperparameters) and was used to study the independent phenomenon of adversarial examples. Specifically, we consider 1000-dimensional data, where two classes are distributed uniformly over two origin-centered hyperspheres with radius 1 and 1.1 respectively. We vary the number of training examples from 4k to 65k (thus ranging through typical dataset sizes like that of MNIST). Observe that compared to the linear example, this data distribution is more realistic in two ways. First, we do not have specific dimensions in the data that are noisy and second, the data dimensionality here as such is a constant less than m. Given samples from this distribution, we train a two-layer ReLU network with h = 100k to minimize cross entropy loss using SGD with learning rate 0.1 and batch size 64. We train the network until 99% of the data is classified by a margin of 10.\nAs shown in Figure 2 (blue line), in this setup, the 0-1 error (i.e., L (0) ) as approximated by the test set, decreases with m \u2208 [2 12 , 2 16 ] at the rate of O(m \u22120.5 ). Now, to prove failure of uniform convergence, we empirically show that a completely misclassified \"bad\" dataset S can be constructed in a manner similar to that of the previous example. In this setting, we pick S by simply projecting every training datapoint on the inner hypersphere onto the outer and vice versa, and then flipping the labels. Then, as shown in Figure 2 (orange line), S is completely misclassified by the learned network. Furthermore, like in the previous example, we have S \u223c D m because the distributions are uniform over the hyperspheres. Having established these facts, the rest of the argument follows like in the previous setting, implying failure of uniform convergence as in Theorem 3.1 here too.\nIn Figure 2 (right), we visualize how the learned boundaries are skewed around the training data in a way that S is misclassified. Note that S is misclassified even when it has as many as 60k points, and even though the network was not explicitly trained to misclassify those points. Intuitively, this demonstrates that the boundary learned by the ReLU network has sufficient complexity that hurts uniform convergence while not affecting the generalization error, at least in this setting. We discuss the applicability of this observation to other hyperparameter settings in Appendix G.2.\nRelationship to adversarial spheres [11]. While we use the same adversarial spheres distribution as [11] and similarly show the existence a certain kind of an adversarial dataset, it is important to note that neither of our observations implies the other. Indeed, the observations in [11] are insufficient to In the first figure, we plot the error of the ReLU network on test data and on the bad dataset S , in the task described in Section 3.2. The second and third images correspond to the decision boundary learned in this task, in the 2D quadrant containing two training datapoints (depicted as \u00d7 and \u2022). The black lines correspond to the two hyperspheres, while the brown and blue regions correspond to the class output by the classifier. Here, we observe that the boundaries are skewed around the training data in a way that it misclassifies the nearest point from the opposite class (corresponding to S , that is not explicitly marked). The fourth image corresponds to two random (test) datapoints, where the boundaries are fairly random, and very likely to be located in between the hyperspheres (better confirmed by the low test error). prove failure of uniform convergence. Specifically, [11] show that in the adversarial spheres setting, it is possible to slightly perturb random test examples in some arbitrary direction to discover a misclassified example. However, to show failure of uniform convergence, we need to find a set of misclassified examples S corresponding to the training examples S, and furthermore, we do not want S to be arbitrary. We want S to have the same underlying distribution, D m .\nDeep learning conjecture. Extending the above insights more generally, we conjecture that in overparameterized deep networks, SGD finds a fit that is simple at a macroscopic level (leading to good generalization) but also has many microscopic fluctuations (hurting uniform convergence). To make this more concrete, for illustration, consider the high-dimensional linear model that sufficiently wide networks have been shown to converge to [18]. That is, roughly, these networks can be written as h(x) = w T \u03c6(x) where \u03c6(x) is a rich high-dimensional representation of x computed from many random features (chosen independent of training data). Inspired by our linear model in Section 3.1, we conjecture that the weights w learned on a dataset S can be expressed as w 1 +w 2 , where w T 1 \u03c6(x) dominates the output on most test inputs and induces a simple decision boundary. That is, it may be possible to apply uniform convergence on the function w T 1 \u03c6(x) to obtain a small generalization bound. On the other hand, w 2 corresponds to meaningless signals that gradient descent gathered from the high-dimensional representation of the training set S. Crucially, these signals would be specific to S, and hence not likely to correlate with most of the test data i.e., w 2 \u03c6(x) would be negligible on most test data, thereby not affecting the generalization error significantly. However, w 2 \u03c6(x) can still create complex fluctuations on the boundary, in low-probability regions of the input space (whose locations would depend on S, like in our examples). As we argued, this can lead to failure of uniform convergence. Perhaps, existing works that have achieved strong uniform convergence bounds on modified networks, may have done so by implicitly suppressing w 2 , either by compression, optimization or stochasticization. Revisiting these works may help verify our conjecture.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "A growing variety of uniform convergence based bounds [30, 3, 12, 2, 32, 8, 40, 24, 1, 28, 33] have sought to explain generalization in deep learning. While these may provide partial intuition about the puzzle, we ask a critical, high level question: by pursuing this broad direction, is it possible to achieve the grand goal of a small generalization bound that shows appropriate dependence on the sample size, width, depth, label noise, and batch size? We cast doubt on this by first, empirically showing that existing bounds can surprisingly increase with training set size for small batch sizes. We then presented example setups, including that of a ReLU neural network, for which uniform convergence provably fails to explain generalization, even after taking implicit bias into account.\nFuture work in understanding implicit regularization in deep learning may be better guided with our knowledge of the sample-size-dependence in the weight norms. To understand generalization, it may also be promising to explore other learning-theoretic techniques like, say, algorithmic stability [9, 13, 5, 35] ; our linear setup might also inspire new tools. Overall, through our work, we call for going beyond uniform convergence to fully explain generalization in deep learning. A Summary of existing generalization bounds.\nIn this section, we provide an informal summary of the properties of (some of the) existing generalization bounds for ReLU networks in Table 1.   [1] requires fixing the learning rate to be inversely proportional to width. Their bound decreases only as \u2126(1/m 0.16 ), although, the actual generalization error is typically as small as O(1/m 0.43 ).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "B More Experiments", "text": "In this section, we present more experiments along the lines of what we presented in Section 2.\nLayerwise dependence on m. Recall that in the main paper, we show how the distance from initialization and the product of spectral norms vary with m for network with six layers. In Figure 3, we show how the terms grow with sample size m for each layer individually. Our main observation is that the first layer suffers from the largest dependence on m.\nDistance between trajectories of shuffled datasets grows with m. In the main paper, we saw that the distance between the solutions learned on different draws of the dataset grow substantially with m. In Figure 5 (left), we show that even the distance between the solutions learned on the same draw, but a different shuffling of the dataset grows substantially with m.\nFlat minima We also relate our observations regarding distance between two independently learned weights to the popular idea of \"flat minima\". Interestingly, Figure 4 demonstrates that walking linearly from the weights learned on one dataset draw to that on another draw (from the same initialization) preserves the test error. Note that although a similar observation was made in Dr\u00e4xler et al. We observe that all these intermediate networks have the same test error as the original networks themselves.\nsets. As discussed in the main paper, this explored basin/space has larger 2 -width for larger m giving rise to a \"paradox\": on one hand, wider minima are believed to result in, or at least correlate with better generalization [16,15,21], but on the other, a larger 2 -width of the explored space results in larger uniform convergence bounds, making it harder to explain generalization.\nWe note a similar kind of paradox concerning noise in training. Specifically, it is intriguing that on one hand, generalization is aided by larger learning rates and smaller batch sizes [19,17,21] due to increased noise in SGD. On the other, theoretical analyses benefit from the opposite; Allen-Zhu et al. [1] even explicitly regularize SGD for their three-layer-network result to help \"forget false information\" gathered by SGD. In other words, it seems that noise aids generalization, yet hinders attempts at explaining generalization. The intuition from our examples (such as the linear example) is that such \"false information\" could provably impair uniform convergence without affecting generalization.\nFrobenius norms grow with m when m h. Some bounds like [12] depend on the Frobenius norms of the weight matrices (or the distance from origin), which as noted in [27] are in fact widthdependent, and grow as \u2126( \u221a h). However, even these terms do grow with the number of samples in the regime where m is larger than h. In Figure 5, we report the total distance from origin of the learned parameters for a network with h = 256 (we choose a smaller width to better emphasize the growth of this term with m); here, we see that for m > 8192, the distance from origin grows at a rate of \u2126(m 0.42 ) that is quite similar to what we observed for distance from initialization.\nEven a relaxed notion of margin does not address the m-dependency. Recall that in the main paper, we computed the generalization error bound in Equation 1 by setting \u03b3 to be \u03b3 , the margin achieved by the network on at least 99% of the data. One may hope that by choosing a larger value of \u03b3, this bound would become smaller, and that the m-dependence may improve. We consider this possibility by computing the median margin of the network over the training set (instead of the 1%-percentile'th margin) and substituting this in the second term in the right hand side of the guarantee in Equation 1. By doing this, the first margin-based train error term in the right hand side of Equation 1 would simplify to 0.5 (as half the training data are misclassified by this large margin). Thereby we already forgo an explanation of half of the generalization behavior. At least we could hope that the second term no longer grows with m. Unfortunately, we observe in Figure 6 (left) that the bounds still grow with m. This is because, as shown in Figure 6 (right), the median margin value does not grow as fast with m as the numerators of these bounds grow.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Effect of depth.", "text": "We observed that as the network gets shallower the bounds show better dependence with m. As an extreme case, we consider a network with only one hidden layer, and with h = 50000.\nHere we also present a third bound, namely that of Neyshabur et al. [33], besides the two bounds discussed in the main paper. Specifically, if Z 1 , Z 2 are the random initializations of the weight matrices in the network, the generalization error bound (the last term in Equation 1) here is of the following form, ignoring log factors:\nW 2 F ( W 1 \u2212 Z 1 F + Z 1 2 ) \u03b3 \u221a m + \u221a h \u221a m.\nThe first term here is meant to be width-independent, while the second term clearly depends on the width and does decrease with m at the rate of m \u22120.5 . Hence, in our plots in Figure 7, we only focus on the first term. We see that these bounds are almost constant and decrease at a minute rate of \u2126(m \u22120.066 ) while the test errors decrease much faster, at the rate of O(m \u22120.35 ).", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Effect of width.", "text": "In Figure 8, we demonstrate that our observation that the bounds increase with m extends to widths h = 128 and h = 2000 too.\nB.1 Effect of batch size Bounds vs. batch size for fixed m. In Figure 9, we show how the bounds vary with the batch size for a fixed sample size of 16384. It turns out that even though the test error decreases with decreasing batch size (for our fixed stopping criterion), all these bounds increase (by a couple of orders of magnitude) with decreasing batch size. Again, this is because the terms like distance from initialization increase for smaller batch sizes (perhaps because of greater levels of noise in the updates). Overall, existing bounds do not reflect the same behavior as the actual generalization error in terms of their dependence on the batch size. Figure 7: On the left, we plot how the bounds vary with sample size for a single hidden layer network with 50k hidden units. We observe that these bounds are almost constant, and at best decrease at a meagre rate of \u2126(m \u22120.066 ). On the right, we plot the test errors for this network and observe that it decreases with m at the rate of at least O(m 0.35 ).\n128 512 2048 8192 32768 Sample Size Figure 9: On the left, we plot the bounds for varying batch sizes for m = 16384 and observe that these bounds decrease by around 2 orders of magnitude. On the right, we plot the test errors for varying batch sizes and observe that test error increases with batch size albeit slightly.\nBounds vs. m for batch size of 32. In the main paper, we only dealt with a small batch size of 1.\nIn Figure 10, we show bounds vs. sample size plots for a batch size of 32. We observe that in this case, the bounds do decrease with sample size, although only at a rate of O(m \u22120.23 ) which is not as fast as the observed decrease in test error which is \u2126(m \u22120.44 ). Our intuition as to why the bounds behave better (in terms of m-dependence) in the larger batch size regime is that here the amount of noise in the parameter updates is much less compared to smaller batch sizes (and as we discussed earlier, uniform convergence finds it challenging to explain away such noise).", "publication_ref": [], "figure_ref": ["fig_4", "fig_0"], "table_ref": []}, {"heading": "Squared error loss.", "text": "All the experiments presented so far deal with the cross-entropy loss, for which the optimization procedure ideally diverges to infinity; thus, one might suspect that our results are sensitive to the stopping criterion. It would therefore be useful to consider the squared error loss where the optimum on the training loss can be found in a finite distance away from the random initialization. Specifically, we consider the case where the squared error loss between the outputs of the network and the one-hot encoding of the true labels is minimized to a value of 0.05 on average over the training data. Figure 10: On the left, we plot the bounds for varying m for a batch size of 32 and observe that these bounds do decrease with m as O(1/m 0.23 ). On the right, we plot the test errors for various m for batch size 32 and observe that test error varies as \u2126(1/m 0.44 ).\nWe observe in Figure 11 that even for this case, the distance from initialization and the spectral norms grow with the sample size at a rate of at least m 0.3 . On the other hand, the test error decreases with sample size as 1/m 0.38 , indicating that even for the squared error loss, these terms hurt would hurt the generalization bound with respect to its dependence on m. Figure 11: On the left we plot the distance from initialization and the distance between weights learned on two different random draws of the datasets, as a function of varying training set size m, when trained on the squared error loss. Both these quantities grow as \u2126(m 0.35 ). In the middle, we show how the product of spectral norms grow as \u2126(m 0.315 ) for sufficiently large m \u2265 2048. On the right, we observe that the test error (i.e., the averaged squared error loss on the test data) decreases with m as O(m \u22120.38 ).", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "C Pseudo-overfitting", "text": "Recall that in the main paper, we briefly discussed a new notion that we call as pseudo-overfitting and noted that it is not the reason behind why some techniques lead to vacuous generalization bounds. We describe this in more detail here. We emphasize this discussion because i) it brings up a fundamental and so far unknown issue that might potentially exist in current approaches to explaining generalization and ii) rules it out before making more profound claims about uniform convergence.\nOur argument specifically applies to margin-based Rademacher complexity approaches (such as Bartlett et al. [3], Neyshabur et al. [33]). These result in a bound like in Equation 1 that we recall here:\nP r (x,y)\u223cD [\u0393(f (x), y) \u2264 0] \u2264 1 m (x,y)\u2208S 1[\u0393(f (x), y) \u2264 \u03b3] + generalization error bound. (1)\nThese methods upper bound the uniform convergence bound on the L (\u03b3) error on the network in terms of a uniform convergence bound on the margins of the network (see [26] for more details about margin theory of Rademacher complexity). The resulting generalization error bound in Equation 1would take the following form, as per our notation from Definition 3.3:\nsup S\u2208S \u03b4 sup h\u2208H \u03b4 1 \u03b3 E D [\u0393(h(x), y)] \u2212 1 m (x,y)\u2208S \u0393(h(x), y) .(2)\nThis particular upper bound on the generalization gap in the L (\u03b3) loss is also an upper bound on the generalization gap on the margins. That is, with high probability 1 \u2212 \u03b4 over the draws of S, the above bound is larger than the following term that corresponds to the difference in test/train margins:\n1 \u03b3 \uf8eb \uf8ed E (x,y)\u223cD [\u0393(h S (x), y)] \u2212 1 m (x,y)\u2208S \u0393(h S (x), y) \uf8f6 \uf8f8 .(3)\nWe first argue that it is possible for the generalization error of the algorithm to decrease with m (as roughly m \u22120.5 ), but for the above quantity to be independent of m. As a result, the margin-based bound in Equation 2(which is larger than Equation 3) will be non-decreasing in m, and even vacuous.\nBelow we describe such a scenario.\nConsider a network that first learns a simple hypothesis to fit the data, say, by learning a simple linear input-output mapping on linearly separable data. But subsequently, the classifier proceeds to pseudo-overfit to the samples by skewing up (down) the real-valued output of the network by some large constant \u2206 in a tiny neighborhood around the positive (negative) training inputs. Note that this would be possible if and only if the network is overparameterized. Now, even though the classifier's real-valued output is skewed around the training data, the decision boundary is still linear as the sign of the classifier's output has not changed on any input. Thus, the boundary is still simple and linear and the generalization error small.\nHowever, the training margins are at least a constant \u2206 larger than the test margins (which are not affected by the bumps created in tiny regions around the training data). Then, the term in Equation 3 would be larger than \u2206/\u03b3 and as a result, so would the term in Equation 2. Now in the generalization guarantee of Equation 1, recall that we must pick a value of \u03b3 such that the first term is low i.e., most of the training datapoints must be classified by at least \u03b3 margin. In this case, we can at best let \u03b3 \u2248 \u2206 as any larger value of \u03b3 would make the margin-based training error non-negligible; as a result of this choice of \u03b3, the bound in Equation 3 would be an m-independent constant close to 1. The same would also hold for its upper bound in Equation 2, which is the generalization bound provided by the margin-based techniques.\nClearly, this is a potential fundamental limitation in existing approaches, and if deep networks were indeed pseudo-overfitting this way, we would have identified the reason why at least some existing bounds are vacuous. However, (un)fortunately, we rule this out by observing that the difference in the train and test margins in Equation 3 does decrease with training dataset size m (see Figure 12) as O(m \u22120.33 ). Additionally, this difference is numerically much less than \u03b3 = 10 (which is the least margin by which 99% of the training data is classified) as long as m is large, implying that Equation 3 is non-vacuous.\nIt is worth noting that the generalization error decreases at a faster rate of O(m \u22120.43 ) implying that the upper bound in Equation 3 which decreases only as m \u22120.33 , is loose. This already indicates a partial weakness in this specific approach to deriving generalization guarantees. Nevertheless, even this upper bound decreases at a significant rate with m which the subsequent uniform convergencebased upper bound in Equation 2 is unable to capture, thus hinting at more fundamental weaknesses specific to uniform convergence.\nDo our example setups suffer from pseudo-overfitting? Before we wrap up this section, we discuss a question brought up by an anonymous reviewer, which we believe is worth addressing. Recall that in Section 3.1 and Section 3.2, we presented a linear and hypersphere classification task where we showed that uniform convergence provably fails. In light of the above discussion, one may be tempted to ask: do these two models fail to obey uniform convergence because of pseudo-overfitting?  The answer to this is that our proof for failure of uniform convergence in both these examples did not rely on any kind of pseudo-overfitting -had our proof relied on it, then we would have been able to show failure of only specific kinds of uniform convergence bounds (as discussed above). More formally, pseudo-overfitting in itself does not imply the lower bounds on unif-alg that we have shown in these settings.\nOne may still be curious to understand the level of pseudo-overfitting in these examples, to get a sense of the similarity of this scenario with that of the MNIST setup. To this end, we note that our linear setup does indeed suffer from significant pseudo-overfitting -the classifier's output does indeed have bumps around each training point (which can be concluded from our proof).\nIn the case of the hypersphere example, we present Figure 13, where we plot of the average margins in this setup like in Figure 12. Here, we observe that, the mean margins on the test data (orange line) and on training data (blue line) do converge to each other with more training data size m i.e., the gap in the mean test and training margins (green line) does decrease with m. Thus our setup exhibits a behavior similar to deep networks on MNIST in Figure 12. As noted in our earlier discussion, since the rate of decrease of the mean margin gap in MNIST is not as large as the decrease in test error itself, there should be \"a small amount\" of psuedo-overfitting in MNIST. The same holds in this setting, although, here we observe an even milder decrease, implying a larger amount of pseudo-overfitting. Nevertheless, we emphasize that, our proof shows that uniform convergence cannot capture even this decrease with m.\nTo conclude, pseudo-overfitting is certainly a phenomenon worth exploring better; however, our examples elucidate that there is a phenomenon beyond pseudo-overfitting that is at play in deep learning.", "publication_ref": ["b2", "b8"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "D Useful Lemmas", "text": "In this section, we state some standard results we will use in our proofs. We first define some constants: c 1 = 1/2048, c 2 = 15/16 and c 3 = 17/16 and c 4 = \u221a 2.\nFirst, we state a tail bound for sub-exponential random variables [38].\nLemma D.1. For a sub-exponential random variable X with parameters (\u03bd, b) and mean \u00b5, for all t > 0,\nP r [|X \u2212 \u00b5| \u2265 t] \u2264 2 exp \u2212 1 2 min t b , t 2 \u03bd 2 .\nAs a corollary, we have the following bound on the sum of squared normal variables:\nCorollary D.1.1.\nFor z 1 , z 2 , . . . , z D \u223c N (0, 1), we have that\nP r \uf8ee \uf8f0 1 D D j=1 z 2 j \u2208 [c 2 2 , c 2 3 ] \uf8f9 \uf8fb \u2264 2 exp(\u2212c 1 D).\nWe now state the Hoeffding bound for sub-Gaussian random variable.\nLemma D.2. Let z 1 , z 2 , . . . , z D be independently drawn sub-Gaussian variables with mean 0 and sub-gaussian parameter \u03c3 i . Then,\nP r D d=1 z d \u2265 t \u2264 2 exp(\u2212t 2 /2 D d=1 \u03c3 2 d ).\nAgain, we restate it as follows:\nCorollary D.2.1. For any u = (u 1 , u 2 , . . . , u d ) \u2208 R D , for z 1 , z 2 , . . . , z D \u223c N (0, 1), P r D d=1 u d z d \u2265 u 2 \u2022 c 4 ln 2 \u03b4 \u2264 \u03b4.\nE Proof for Theorem 3.1\nIn this section, we prove the failure of uniform convergence for our linear model. We first recall the setup:\nDistribution D: Each input (x 1 , x 2\n) is a K + D dimensional vector where x 1 \u2208 R K and x 2 \u2208 R D . u \u2208 R K determines the centers of the classes. The label y is drawn uniformly from {\u22121, +1}, and conditioned on y, we have x 1 = 2 \u2022 y \u2022 u while x 2 is sampled independently from N (0, 32 D I). Learning algorithm A: We consider a linear classifier with weights w = (w 1 , w 2 ). The output is computed as h(x) = w 1 x 1 + w 2 x 2 . Assume the weights are initialized to origin. Given S = {(x (1) , y (1) ), . . . , (x (m) , y (m) )}, A takes a gradient step of learning rate 1 to maximize y \u2022 h(x) for each (x, y) \u2208 S. Regardless of the batch size, the learned weights would satisfy, w 1 = 2mu and w\n2 = i y (i) x (i) 2 .\nBelow, we state the precise theorem statement (where we've used the constants c 1 = 1/32, c 2 = 1/2 and c 3 = 3/2 and c 4 = \u221a 2):\nTheorem 3.1 In the setup above, for any , \u03b4 > 0 and \u03b4 < 1/4, let D be sufficiently large that it satisfies\nD \u2265 1 c 1 ln 6m \u03b4 ,(4)\nD \u2265 m 4c 4 c 3 c 2 2 2 ln 6m \u03b4 ,(5)\nD \u2265 m 4c 4 c 3 c 2 2 2 \u2022 2ln 2 ,(6)\nthen we have that for all \u03b3 \u2265 0, for the L (\u03b3) loss, unif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4).\nSpecifically, for \u03b3 \u2208 [0, 1], gen (m, \u03b4) \u2264 , and so unif-alg (m, \u03b4) \u2265 1 \u2212 .\nProof. The above follows from Lemma E.1, where we upper bound the generalization error, and from Lemma E.2 where we lower bound uniform convergence.\nWe first prove that the above algorithm generalizes well with respect to the losses corresponding to \u03b3 \u2208 [0, 1]. First for the training data, we argue that both w 1 and a small part of the noise vector w 2 align along the correct direction, while the remaining part of the high-dimensional noise vector are orthogonal to the input; this leads to correct classification of the training set. Then, on the test data, we argue that w 1 aligns well, while w 2 contributes very little to the output of the classifier because it is high-dimensional noise. As a result, for most test data, the classification is correct, and hence the test and generalization error are both small.\nLemma E.1. In the setup of Section 3, when \u03b3 \u2208 [0, 1], for L (\u03b3) , gen (m, \u03b4) \u2264 .\nProof. The parameters learned by our algorithm satisfies w 1 = 2m \u2022 u and w 2 =\ny (i) x (i) 2 \u223c N (0, 8m c 2 2 D ).\nFirst, we have from Corollary D.1.1 that with probability 1 \u2212 \u03b4 3m over the draws of\nx (i)\n2 , as long as \u03b4 3m \u2265 2e \u2212c1D (which is given to hold by Equation 4),\nc 2 \u2264 1 2 \u221a 2 c 2 x (i) 2 \u2264 c 3 .(7)\nNext, for a given x (i) , we have from Corollary D.2.1, with probability 1 \u2212 \u03b4 3m over the draws of\nj =i y (j) x (j) 2 , |x (i) 2 \u2022 j =i y (j) x (j) 2 | \u2264 c 4 x (i) 2 2 \u221a 2 \u2022 \u221a m c 2 \u221a D ln 6m \u03b4 .(8)\nThen, with probability 1 \u2212 2 3 \u03b4 over the draws of the training dataset we have for all i,\ny (i) h(x (i) ) = y (i) w 1 \u2022 x (i) 1 + y (i) \u2022 y (i) x (i) 2 2 + y (i) \u2022 x (i) 2 \u2022 j =i y (j) x (j) 2 = 4 + x (i) 2 2\napply Equation 7+ y (i) x\n(i) 2 \u2022 j =i y (j) x (j) 2\napply Equation 8\n\u2265 4 + 4 \u2022 2 \u2212 c 4 2 \u221a 2c 3 c 2 \u2022 2 \u221a 2 \u2022 \u221a m c 2 \u221a D ln 6m \u03b4\napply Equation 5\u2265 4 + 8 \u2212 2 = 10 > 1.\nThus, for all \u03b3 \u2208 [0, 1], the L (\u03b3) loss of this classifier on the training dataset S is zero. Now, from Corollary D.1.1, with probability 1 \u2212 \u03b4 3 over the draws of the training data, we also have that, as long as \u03b4 3m \u2265 2e \u2212c1D (which is given to hold by Equation 4),\nc 2 \u221a m \u2264 1 2 \u221a 2 c 2 y (i) x (i) 2 \u2264 c 3 \u221a m. (10\n)\nNext, conditioned on the draw of S and the learned classifier, for any > 0, with probability 1 \u2212 over the draws of a test data point, (z, y), we have from Corollary D.2.1 that\n|z 2 \u2022 y (i) x (i) 2 | \u2264 c 4 y (i) x (i) 2 \u2022 2 \u221a 2 c 2 \u221a D \u2022 ln 1 . (11\n)\nUsing this, we have that with probability\n1 \u2212 2 exp \u2212 1 2 c 2 2 4c4c3 D m 2\nover the draws of a test data point, (z, y),\nyh(x) = yw 1 \u2022 z 1 + y \u2022 z 2 \u2022 j y (j) x (j) 2\napply Equation 11\n\u2265 4 \u2212 c 4 y (i) x (i) 2\napply Equation 10\n\u2022 2 \u221a 2 c 2 \u221a D c 2 2 4c 4 c 3 D m \u2265 4 \u2212 2 \u2265 2. (12\n)\nThus, we have that for \u03b3 \u2208 [0, 1], the L (\u03b3) loss of the classifier on the distribution D is\n2 exp \u2212 1 2 c 2 2 4c4c3 D m 2\nwhich is at most as assumed in Equation 6. In other words, the absolute difference between the distribution loss and the train loss is at most and this holds for at least 1 \u2212 \u03b4 draws of the samples S. Then, by the definition of gen we have the result.\nWe next prove our uniform convergence lower bound. The main idea is that when the noise vectors in the training samples are negated, with high probability, the classifier misclassifies the training data. We can then show that for any choice of S \u03b4 as required by the definition of unif-alg , we can always find an S and its noise-negated version S both of which belong to S \u03b4 . Furthermore, we can show that h S has small test error but high empirical error on S , and that this leads to a nearly vacuous uniform convergence bound.\nLemma E.2. In the setup of Section 3, for any > 0 and for any \u03b4 \u2264 1/4, and for the same lower bounds on D, and for any \u03b3 \u2265 0, we have that unif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4)\nfor the L (\u03b3) loss.\nProof. For any S, let S denote the set of noise-negated samples S = {((x 1 , \u2212x 2 ), u) | ((x 1 , x 2 ), y) \u2208 S}. We first show with high probability 1 \u2212 2\u03b4/3 over the draws of S, that the classifier learned on S, misclassifies S completely. The proof for this is nearly identical to our proof for why the training loss is zero, except for certain sign changes. For any\nx (i) neg = (x (i) 1 , \u2212x (i)\n2 ), we have\ny (i) h(x (i) neg ) = y (i) w 1 \u2022 x (i) 1 \u2212 y (i) \u2022 y (i) x (i) 2 2 \u2212 y (i) \u2022 x (i) 2 \u2022 j =i y (j) x (j) 2 = 4 \u2212 x (i) 2 2\napply Equation 7\u2212\ny (i) x (i) 2 \u2022 j =i y (j) x (j) 2 apply Equation 8 \u2264 4 \u2212 4 \u2022 2 + c 4 2 \u221a 2c 3 c 2 \u2022 2 \u221a 2 \u2022 \u221a m c 2 \u221a D ln 3m \u03b4 apply Equation 5 \u2264 4 \u2212 8 + 2 = \u22122 < 0.\nSince the learned hypothesis misclassifies all of S , it has loss of 1 on S .\nNow recall that, by definition, to compute unif-alg , one has to pick a sample set space S \u03b4 of mass 1 \u2212 \u03b4 i.e., P r S\u223cS m [S \u2208 S \u03b4 ] \u2265 1 \u2212 \u03b4. We first argue that for any choice of S \u03b4 , there must exist a 'bad' S such that (i) S \u2208 S \u03b4 , (ii) S \u2208 S \u03b4 , (iii) h S has test error less than gen (m, \u03b4) and (iv) h S completely misclassifies S .\nWe show the existence of such an S , by arguing that over the draws of S, there is non-zero probability of picking an S that satisfies all the above conditions. Specifically, we have by the union bound that\nP r S\u223cD m S \u2208 S \u03b4 , S \u2208 S \u03b4 , L D (h S ) \u2264 gen (m, \u03b4),L S (h S ) = 1 \u2265 1 \u2212 P r S\u223cD m [S / \u2208 S \u03b4 ] \u2212 P r S\u223cD m [S / \u2208 S \u03b4 ] \u2212 P r S\u223cD m [L D (h S ) > gen (m, \u03b4)] \u2212 P r S\u223cD m L S (h S ) = 1 . (13\n)\nBy definition of S \u03b4 , we know P r S\u223cD m [S / \u2208 S \u03b4 ] \u2264 \u03b4. Similarly, by definition of the generalization error, we know that P r S\u223cD m [L D (h S ) > gen (m, \u03b4)] \u2264 \u03b4. We have also established above that P r S\u223cD m L S (h S ) = 1 \u2264 2\u03b4/3. As for the term P r S\u223cD m [S / \u2208 S \u03b4 ], observe that under the draws of S, the distribution of the noise-negated dataset S is identical to D m . This is because the isotropic Gaussian noise vectors have the same distribution under negation. Hence, again by definition of S \u03b4 , even this probability is at most \u03b4. Thus, we have that the probability in the left hand side of Equation 13 is at least 1 \u2212 4\u03b4, which is positive as long as \u03b4 < 1/4.\nThis implies that for any given choice of S \u03b4 , there exists S that satisfies our requirement. Then, from the definition of unif-alg (m, \u03b4), we essentially have that,\nunif-alg (m, \u03b4) = sup S\u2208S \u03b4 sup h\u2208H \u03b4 |L D (h) \u2212L S (h)| \u2265 |L D (h S ) \u2212L S (h)| = | \u2212 1| = 1 \u2212 .", "publication_ref": ["b13", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "F Neural Network with Exponential Activations", "text": "In this section, we prove the failure of uniform convergence for a neural network model with exponential activations. We first define the setup.\nDistribution Let u be an arbitrary vector in D dimensional space such that u = \u221a D/2. Consider an input distribution in 2D dimensional space such that, conditioned on the label y drawn from uniform distribution over {\u22121, +1}, the first D dimensions x 1 of a random point is given by yu and the remaining D dimensions are drawn from N (0, 1). Note that in this section, we require D to be only as large as ln m, and not as large as m.\nArchitecture. We consider an infinite width neural network with exponential activations, in which only the output layer weights are trainable. The hidden layer weights are frozen as initialized. Note that this is effectively a linear model with infinitely many randomized features. Indeed, recent work [18] has shown that under some conditions on how deep networks are initialized and parameterized, they behave a linear models on randomized features. Specifically, each hidden unit corresponds to a distinct (frozen) weight vector w \u2208 R 2D and an output weight a w that is trainable. We assume that the hidden layer weights are drawn from N (0, I) and a w initialized to zero. Note that the output of the network is determined as\nh(x) = E w [a w exp(w \u2022 x)].\nAlgorithm We consider an algorithm that takes a gradient descent step to maximize y \u2022 h(x) for each (x, y) in the training dataset, with learning rate \u03b7. However, since, the function above is not a discrete sum of its hidden unit outputs, to define the gradient update on a w , we must think of h as a functional whose input function maps every w \u2208 R 2D to a w \u2208 R. Then, by considering the functional derivative, one can conclude that the update on a w can be written as\na w \u2190 a w + \u03b7y \u2022 exp(w \u2022 x) \u2022 p(w). (14\n)\nwhere p(w) equals the p.d.f of w under the distribution it is drawn from. In this case p(w) =\n1 (2\u03c0) D exp \u2212 w 2 2 .\nIn order to simplify our calculations we will set \u03b7 = (4\u03c0) D , although our analysis would extend to other values of the learning rate too. Similarly, our results would only differ by constants if we consider the alternative update rule, a w \u2190 a w + \u03b7y \u2022 exp(w \u2022 x).\nWe now state our main theorem (in terms of constants c 1 , c 2 , c 3 , c 4 defined in Section D).\nTheorem F.1. In the set up above, for any , \u03b4 > 0 and \u03b4 < 1/4, let D and m be sufficiently large that it satisfies\nD \u2265 max 1 c 2 , (16c 3 c 4 ) 2 \u2022 2 ln 6m (15) D \u2265 max 1 c 2 , (16c 3 c 4 ) 2 \u2022 2 ln 6m \u03b4 (16) D \u2265 6 ln 2m(17)\nm > max 8 ln 6 \u03b4 .\nthen we have that for all \u03b3 \u2265 0, for the L (\u03b3) loss, unif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4).\nSpecifically, for \u03b3 \u2208 [0, 1], gen (m, \u03b4) \u2264 , and so unif-alg (m, \u03b4) \u2265 1 \u2212 .\nProof. The result follows from the following lemmas. First in Lemma F.2, we derive the closed form expression for the function computed by the learned network. In Lemma F.3, we upper bound the generalization error and in Lemma F.4, we lower bound uniform convergence.\nWe first derive a closed form expression for how the output of the network changes under a gradient descent step on a particular datapoint.\nLemma F.2. Let h (0) (\u2022) denote the function computed by the network before updating the weights. After updating the weights on a particular input (x, y) according to Equation 14, the learned network corresponds to:\nh(z) = h (0) (z) + y exp z + x 2 2 .\nProof. From equation 14, we have that\nh(z) \u2212 h (0) (z) \u03b7 = w (y \u2022 exp(w \u2022 x)p(w)) \u2022 exp(w \u2022 z)p(w)dw = y w exp(w \u2022 (x + z)) \u2022 1 2\u03c0 2D exp(\u2212 w 2 )dw = y 1 2\u03c0 2D w exp(w \u2022 (x + z) \u2212 w 2 )dw = y 1 2\u03c0 2D exp z + x 2 2 \u00d7 w exp \u2212 w \u2212 z + x 2 2 dw = y 1 4\u03c0 D exp z + x 2 2 \u00d7 1 2\u03c0(0.5) 2D w exp \u2212 w \u2212 z + x 2 2 dw = y 1 4\u03c0 D exp z + x 2 2 .\nIn the last equality above, we make use of the fact that the second term corresponds to the integral of the p.d.f of N ( z+x 2 , 0.5I) over R 2D . Since we set \u03b7 = (4\u03c0) D gives us the final answer.\nNext, we argue that the generalization error of the algorithm is small. From Lemma F.2, we have that the output of the network is essentially determined by a summation of contributions from every training point. To show that the training error is zero, we argue that on any training point, the contribution from that training point dominates all other contributions, thus leading to correct classification. On any test point, we similarly show that the contribution of training points of the same class as that test point dominates the output of the network. Note that our result requires D to scale only logarithmically with training samples m.\nLemma F.3. In the setup of Section 3, when \u03b3 \u2208 [0, 1], for L (\u03b3) , gen (m, \u03b4) \u2264 .\nProof. We first establish a few facts that hold with high probability over the draws of the training set S. First, from Corollary D.1.1 we have that, since D \u2265 1 c2 ln 3m \u03b4 (from Equation 15), with probability at least 1 \u2212 \u03b4/3 over the draws of S, for all i, the noisy part of each training input can be bounded as\nc 2 \u221a D \u2264 x (i) 2 \u2264 c 3 \u221a D.(19)\nNext, from Corollary D.2.1, we have that with probability at least 1 \u2212 \u03b4 3m 2 over the draws of\nx (i) 2 and x (j) 2 for i = j, |x (i) 2 \u2022 x (j) 2 | \u2264 x (i) 2 \u2022 c 4 2 ln 6m \u03b4 .(20)\nThen, by a union bound, the above two equations hold for all i = j with probability at least 1 \u2212 \u03b4/2.\nNext, since each y (i) is essentially an independent sub-Gaussian with mean 0 and sub-Gaussian parameter \u03c3 = 1, we can apply Hoeffding's bound (Lemma D.2) to conclude that with probability at least 1 \u2212 \u03b4/3 over the draws of S,\nm j=1 y (j) \u2264 2m ln 6 \u03b4 Eq 18 < m 2 .(21)\nNote that this means that there must exist at least one training data in each class.\nGiven these facts, we first show that the training error is zero by showing that for all i, y (i) h(x (i) ) is sufficiently large. On any training input (x (i) , y (i) ), using Lemma F.2, we can write\ny (i) h(x (i) ) = exp x (i) 2 + j =i y (i) y (j) exp x (i) + x (j) 2 2 \u2265 exp x (i) 2 \u2212 j =i y (i) =y (j) exp x (i) + x (j) 2 2 \u2265 exp x (i) 2 \u00d7 \uf8eb \uf8ec \uf8ec \uf8ed 1 \u2212 j =i y (i) =y (j) exp x (i) + x (j) 2 \u2212 4 x (i) 2 4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 .\nNow, for any j such that y (j) = y (i) , we have that\nx (i) + x (j) 2 \u2212 4 x (i) 2 = \u22123 x (i) 2 + x (j) 1 2 + 2x (i) 1 \u2022 x (j) 1 + x (j) 2 2 Eq 19 + 2x (i) 2 \u2022 x (j) 2 Eq 20 \u2264 \u22123 u 2 \u2212 3 x (i) 2 2 Eq 19 + u 2 \u2212 2 u 2 + c 2 3 D + x (i) 2\nEq 19\n\u20222c 4 2 ln 6m \u03b4 \u2264 \u22124 u 2 \u2212 3c 2 2 D + c 2 3 D + \u221a D \u2022 c 3 c 4 2 ln 6m \u03b4 Eq 16 \u2264 \u22121 \u2212 45 16 D + 17 16 D + 1 16 D = \u2212 43 16 D.\nPlugging this back in the previous equation we have that\ny (i) h(x (i) ) \u2265 \u2265 exp \uf8eb \uf8ec \uf8ec \uf8ed x (i) 2 Eq 19 \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed 1 \u2212 m exp \u2212 43 64 D Eq 17 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2265 exp 15 16 D Eq 17 \u2022 1 2 \u2265 1.\nHence, x (i) is correctly classified by a margin of 1 for every i.\nNow consider any test data point (z, y). Since D \u2265 1 c2 ln 2 (Equation 16), we have that with probability at least 1 \u2212 /2 over the draws of z 2 , by Corollary D.1.1\nc 2 \u221a D \u2264 z 2 \u2264 c 3 \u221a D.(22)\nSimilarly, for each i, we have that with probability at least 1\u2212 /2m over the draws of z, the following holds good by Corollary D.2.1\n|x (i) 2 \u2022 z 2 | \u2264 x (i) 2 \u2022 c 4 2 ln 6m .(23)\nHence, the above holds over at least 1 \u2212 /2 draws of z, and by extension, both the above equations hold over at least 1 \u2212 draws of z. Now, for any i such that y (i) = y, we have that\nx (i) + z 2 = x (i) 1 2 + z 1 2 + 2x (i) 1 \u2022 z 1 + x (i) 2 2 Eq 19 + z (i) 2 2 Eq 22 +2 x (i) 2 \u2022 z 2 Eq 23, 19 \u2265 4 u 2 + 2c 2 2 D \u2212 \u221a D \u2022 2c 3 c 4 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ln 6m", "text": "Eq 15\n\u2265 D + 30 16 D \u2212 1 16 D = 45 16 D.\nSimilarly, for any i such that y (i) = y, we have that\nx (i) + z 2 = x (i) 1 2 + z 1 2 + 2x (i) 1 \u2022 z 1 + x (i) 2 2 Eq 19 + z (i) 2 2 Eq 22 +2 x (i) 2 \u2022 z 2\nEq 20, 19\n\u2264 2 u 2 \u2212 2 u 2 + 2c 2 3 D + \u221a D \u2022 2c 3 c 4 2 ln 6m \u03b4 Eq 16 \u2264 34 16 D \u2212 1 16 D = 33 16 D.\nSince from Equation 21 we know there exists at least one training sample with a given label, we have that\ny (i) h(x (i) ) = i:y (i) =y exp x (i) + z 2 2 \u2212 i:y (i) =y exp x (i) + z 2 2 \u2265 exp 45 64 D \u2212 m exp 33 64 D \u2265 exp 45 64 D \u2022 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed 1 \u2212 m exp \u2212 12 64 D Eq 17 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2265 exp 45 64 D Eq 17 \u2022 1 2 \u2265 1.\nThus, at least 1 \u2212 of the test datapoints are classified correctly.\nWe next show that the uniform convergence bound is nearly vacuous. In order to do this, we create a set S from S by negating all values but the noise vector. We then show that for every point in S , the contribution from the corresponding point in S dominates over the contribution from all other points. (This is because of how the non-negated noise vector in the point from S aligns adversarially with the noise vector from the corresponding point in S). As a result, the points in S are all labeled like in S, implying that S is completely misclassified. Then, similar to our previous arguments, we can show that uniform convergence is nearly vacuous.\nLemma F.4. In the setup of Section F, for any > 0 and for any \u03b4 \u2264 1/4, and for the same lower bounds on D and m as in Theorem F.1, and for any \u03b3 \u2265 0, we have that\nunif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4) for the L (\u03b3) loss.\nProof. Let S be a modified version of the training set where all values are negated except that of the noise vectors i.e., S = {((\u2212x 1 , x 2 ), \u2212y) | ((x 1 , x 2 ), y) \u2208 S}. First we show that with probability at least 1 \u2212 2\u03b4/3 over the draws of S, S is completely misclassified. First, we have that with probability 1 \u2212 2\u03b4/3, Equations 19 and 20 hold good. Let (x\n(i) neg , y (i)\nneg ) denote the ith sample from S . Then, we have that\ny (i) neg h(x (i) neg ) = \u2212 exp \uf8eb \uf8ed x (i) + x (i) neg 2 2 \uf8f6 \uf8f8 + j =i y (i) neg y (j) exp \uf8eb \uf8ed x (i) neg + x (j) 2 2 \uf8f6 \uf8f8 \u2264 \u2212 exp x (i) 2 2 + j =i y (i) neg =y (j) exp \uf8eb \uf8ed x (i) neg + x (j) 2 2 \uf8f6 \uf8f8 \u2264 exp x (i) 2 2 \u00d7 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed \u22121 + j =i y (i)\nneg =y (j) exp\nx (i) neg + x (j) 2 \u2212 4 x (i) 2 2 4 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 .(24)\nNow, consider j such that y (j) = y (i)\nneg . we have that\nx (i) neg + x (j) 2 \u2212 4 x (i) 2 2 = x (i) 1 2 + x (j) 1 2 \u2212 2x (i) 1 \u2022 x (j) 1 \u2212 3 x (i) 2 2 Eq 19 + x (j) 2 2 Eq 19 \u2212 2x (i) 2 \u2022 x (j) 2 Eq 20 \u2264 4 u 2 \u2212 3c 2 2 D + c 3 D + x (i) 2\nEq 19\n\u20222c 4 2 ln 6m \u03b4 \u2264 4 u 2 \u2212 3c 2 2 D + c 2 3 D + \u221a D \u2022 c 3 c 4 2 ln 6m \u03b4 Eq 16 \u2264 D \u2212 45 16 D + 17 16 D + 1 16 D = \u221211 16 D.\nPlugging the above back in Equation 24, we have\ny (i) neg h(x (i) neg ) exp x (i) 2 2 \u2264 \u22121 + m exp (\u221211D/64) Eq 17 \u2264 \u22121/2, implying that x (i)\nneg is misclassified. This holds simultaneously for all i, implying that S is misclassified with high probability 1 \u2212 2\u03b4/3 over the draws of S. Furthermore, S has the same distribution as D m . Then, by the same argument as that of Lemma E.2, we can prove our final claim.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Further Remarks.", "text": "In this section, we make some clarifying remarks about our theoretical results. G.1 Nearly vacuous bounds for any \u03b3 > 0. Typically, like in Mohri et al. [26], Bartlett et al. [3], the 0-1 test error is upper bounded in terms of the L (\u03b3) test error for some optimal choice of \u03b3 > 0 (as it is easier to apply uniform convergence for \u03b3 > 0). From the result in the main paper, it is obvious that for \u03b3 \u2264 1, this approach would yield vacuous bounds. We now establish that this is the case even for \u03b3 > 1.\nTo help state this more clearly, for the scope of this particular section, let\n(\u03b3) unif-alg , (\u03b3)\ngen denote the uniform convergence and generalization error for L (\u03b3) loss. Then, the following inequality is used to derive a bound on the 0-1 error:\nL (0) D (h S ) \u2264 L (\u03b3) D (h S ) \u2264L (\u03b3) S (h S ) + (\u03b3) unif-alg (m, \u03b4)(25)\nwhere the second inequality above holds with probability at least 1 \u2212 \u03b4 over the draws of S, while the first holds for all S (which follows by definition of L (\u03b3) and L (0) ).\nTo establish that uniform convergence is nearly vacuous in any setting of \u03b3, we must show that the right hand side of the above bound is nearly vacuous for any choice of \u03b3 \u2265 0 (despite the fact that L (0) D (S) \u2264 ). In our results, we explicitly showed this to be true for only small values of \u03b3, by arguing that the second term in the R.H.S, namely (\u03b3) unif-alg (m, \u03b4), is nearly vacuous. Below, we show that the above bound is indeed nearly vacuous for any value of \u03b3, when we have that\n(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)\ngen (m, \u03b4). Note that we established the relation\n(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)\ngen (m, \u03b4) to be true in all of our setups.\nProposition G.1. Given that for all \u03b3 \u2265 0,\n(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)\ngen (m, \u03b4) then, we then have that for all \u03b3 \u2265 0,\nP r S\u223cD m L (\u03b3) S (h S ) + (\u03b3) unif-alg (m, \u03b4) \u2265 1 2 > \u03b4\nor in other words, the guarantee from the right hand side of Equation 25 is nearly vacuous.\nProof. Assume on the contrary that for some choice of \u03b3, we are able to show that with probability at least 1 \u2212 \u03b4 over the draws of S, the right hand side of Equation 25 is less than 1/2. This means that\n(\u03b3)\nunif-alg (m, \u03b4) < 1/2. Furthermore, this also means that with probability at least 1 \u2212 \u03b4 over the draws of S,L As a result, we have that with probability at least 1 \u2212 \u03b4, L\n(\u03b3) D (h S ) \u2212L (\u03b3) S (h S ) < 1/2. In other words, (\u03b3)\ngen (m, \u03b4) < 1/2. Since we are given that\n(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)\ngen (m, \u03b4), by our upper bound on the generalization error, we have\n(\u03b3)\nunif-alg (m, \u03b4) \u2265 1/2, which is a contradiction to our earlier inference that (\u03b3) unif-alg (m, \u03b4) < 1/2. Hence, our assumption is wrong.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Applicability of the observation in Section 3.2 to other settings", "text": "Recall that in the main paper, we discussed a setup where two hyperspheres of radius 1 and 1.1 respectively are classified by a sufficiently overparameterized ReLU network. We saw that even when the number of training examples was as large as 65536, we could project all of these examples on to the other corresponding hypersphere, to create a completely misclassified set S . How well does this observation extend to other hyperparameter settings?\nFirst, we note that in order to achieve full misclassification of S , the network would have to be sufficiently overparameterized i.e., either the width or the input dimension must be larger. When the training set size m is too large, one would observe that S is not as significantly misclassified as observed. (Note that on the other hand, increasing the parameter count would not hurt the generalization error. In fact it would improve it.) Second, we note that our observation is sensitive to the choice of the difference in the radii between the hyperspheres (and potentially to other hyperparameters too). For example, when the outer sphere has radius 2, SGD learns to classify these spheres perfectly, resulting in zero error on both test data and on S . As a result, our lower bound on unif-alg would not hold in this setting.\nHowever, here we sketch a (very) informal argument as to why there is reason to believe that our lower bound can still hold on a weaker notion of uniform convergence, a notion that is always applied in practice (in the main paper we focus on a strong notion of uniform convergence as a negative result about it is more powerful). More concretely, in reality, uniform convergence is computed without much knowledge about the data distribution, save a few weakly informative assumptions such as those bounding its support. Such a uniform convergence bound is effectively computed uniformly in supremum over a class of distributions.\nGoing back to the hypersphere example, the intuition is that even when the radii of the spheres are far apart, and hence, the classification perfect, the decision boundary learned by the network could still be microscopically complex -however these complexities are not exaggerated enough to misclassify S . Now, for this given decision boundary, one would be able to construct an S which corresponds to projecting S on two concentric hyperspheres that fall within these skews. Such an S would have a distribution that comes from some D which, although not equal to D, still obeys our assumptions about the underlying distribution. The uniform convergence bound which also holds for D would thus have to be vacuous.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.3 On the dependence of gen on m in our examples.", "text": "As seen in the proof of Lemma E.1, the generalization error depends on m and D as O(e \u2212D/m ) ignoring some constants in the exponent. Clearly, this error decreases with the parameter count D.\nOn the other hand, one may also observe that this generalization error grows with the number of samples m, which might at first make this model seem inconsistent with our real world observations. However, we emphasize that this is a minor artefact of the simplifications in our setup, rather than a conceptual issue. With a small modification to our setup, we can make the generalization error decrease with m, mirroring our empirical observations. Specifically, in the current setup, we learn the true boundary along the first K dimensions exactly. We can however modify it to a more standard learning setup where the boundary is not exactly recoverable and needs to be estimated from the examples. This would lead to an additional generalization error that scales as O( K m ) that is non-vacuous as long as K m. Thus, the overall generalization error would be\nO(e \u2212D/m + K m ).\nWhat about the overall dependence on m? Now, assume we have an overparameterization level of D m ln(m/K), so that e \u2212D/m K/m. Hence, in the sufficiently overparameterized regime, the generalization error O(e \u2212D/m ) that comes from the noise we have modeled, pales in comparison with the generalization error that would stem from estimating the low-complexity boundary. Overall, as a function of m, the resulting error would behave like O( K m ) and hence show a decrease with increasing m (as long the increase in m is within the overparameterized regime).\nG.4 Failure of hypothesis-dependent uniform convergence bounds.\nOften, uniform convergence bounds are written as a bound on the generalization error of a specific hypothesis rather than the algorithm. These bounds have an explicit dependence on the weights learned. As an example, a bound may be of the form that, with high probability over draws of training setS, for any hypothesis h with weights w,\nL D (h) \u2212LS(h) \u2264 w 2 \u221a m .\nBelow we argue why even these kinds of hypothesis-dependent bounds fail in our setting. We can informally define the tightest hypothesis-dependent uniform convergence bound as follows, in a manner similar to Definition 3.3 of the tightest uniform convergence bound. Recall that we first pick a set of datasets S \u03b4 such that P rS \u223cD m [S / \u2208 S \u03b4 ] \u2264 \u03b4. Then, for allS \u2208 S \u03b4 , we denote the upper bound on the generalization gap of hS by unif-alg (hS, m, \u03b4), where:\nunif-alg (hS, m, \u03b4) := sup S\u2208S \u03b4 |L D (hS) \u2212L S (hS)|.\nIn other words, the tightest upper bound here corresponds to the difference between the test and empirical error of the specific hypothesis hS but computed across nearly all datasets S in S \u03b4 .\nTo show failure of the above bound, recall from all our other proofs of failure of uniform convergence, we have that for at least 1 \u2212 O(\u03b4) draws of the sample setS, four key conditions are satisfied: (i) S \u2208 S \u03b4 , (ii) the corresponding bad datasetS \u2208 S \u03b4 , (iii) the error on the bad setLS (hS) = 1 and (iv) the test error L D (hS) \u2264 gen (m, \u03b4). For all suchS, in the definition of unif-alg (hS, m, \u03b4), let us set S to beS . Then, we would get unif-alg (hS, m, \u03b4) \u2265 |L D (hS) \u2212LS (hS)| \u2265 1 \u2212 gen (m, \u03b4). In other words, with probability at least 1 \u2212 O(\u03b4) over the draw of the training set, even a hypothesis-specific generalization bound fails to explain generalization of the corresponding hypothesis. preclude the fact that uniform convergence maybe unable to explain generalization of a particular algorithm for a particular distribution.\nWe first recall the notion of learnability. First, formally, a binary classification problem consists of a hypothesis class H and an instance space X \u00d7 {\u22121, 1}. The problem is said to be learnable if there exists a learning rule A : \nVapnik and Chervonenkis [37] showed that finite VC dimension of the hypothesis class is necessary and sufficient for learnability in binary classification problems. As Shalev-Shwartz et al. [35] note, since finite VC dimension is equivalent to uniform convergence, it can thus be concluded that uniform convergence is necessary and sufficient for learnability binary classification problems.\nHowever, learnability is a strong notion that does not necessarily have to hold for a particular learning algorithm to generalize well for a particular underlying distribution. Roughly speaking, this is because learnability evaluates the algorithm under all possible distributions, including many complex distributions; while a learning algorithm may generalize well for a particular distribution under a given hypothesis class, it may fail to do so on more complex distributions under the same hypothesis class.\nFor more intuition, we present a more concrete but informal argument below. However, this argument is technically redundant because learnability is equivalent to uniform convergence for binary classification, and since we established the lack of necessity of uniform convergence, we effectively established the same for learnability too. However, we still provide the following informal argument as it provides a different insight into why learnability and uniform convergence are not necessary to explain generalization.\nOur goal is to establish that in the set up of Section 3, even if we considered the binary classification problem corresponding to H \u03b4 (the class consisting of only those hypotheses explored by the algorithm A under a distribution D), the corresponding binary classification problem is not learnable i.e., Equation 26 does not hold when we plug in H \u03b4 in place of H.\nFirst consider distributions of the following form that is more complex than the linearly separable D: for any dataset S , let D S be the distribution that has half its mass on the part of the linearly separable distribution D excluding S , and half its mass on the distribution that is uniformly distributed over S . Now let S be a random dataset drawn from D but with all its labels flipped; consider the corresponding complex distribution D S .\nWe first show that there exists h \u2208 H \u03b4 that fits this distribution well. Now, for most draws of the \"wrongly\" labeled S , we can show that the hypothesis h for which w 1 = 2 \u2022 u and w 2 = (x,y)\u2208S y \u2022 x 2 fits the \"wrong\" labels of S perfectly; this is because, just as argued in Lemma E.2, w 2 dominates the output on all these inputs, although w 1 would be aligned incorrectly with these inputs. Furthermore, since w 2 does not align with most inputs from D, by an argument similar to Lemma E.1, we can also show that this hypothesis has at most error on D, and that this hypothesis belongs to H \u03b4 . Overall this means that, w.h.p over the choice of S , there exists a hypothesis h \u2208 H \u03b4 for which the error on the complex distribution D S is at most /2 i.e.,\nmin h\u2208H E (x,y)\u223cD S [L(h(x), y)] \u2264 /2.\nOn the other hand, let A be any learning rule which outputs a hypothesis given S \u223c D S . With high probability over the draws of S \u223c D S , only at most, say 3/4th of S (i.e., 0.75m examples) will be sampled from S (and the rest from D). Since the learning rule which has access only to S, has not seen at least a quarter of S , with high probability over the random draws of S , the learning rule will fail to classify roughly half of the unseen examples from S correctly (which would be about (m/4) \u2022 1/2 = m/8). Then, the error on D S will be at least 1/16. From the above arguments, we have that learnability (m) \u2265 1/16 \u2212 /2, which is a non-negligible constant that is independent of m.", "publication_ref": ["b12", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "J Deterministic PAC-Bayes bounds are two-sided uniform convergence bounds", "text": "By definition, VC-dimension, Rademacher complexity and other covering number based bounds are known to upper bound the term unif-alg and therefore our negative result immediately applies to all these bounds. However, it may not be immediately clear if bounds derived through the PAC-Bayesian approach fall under this category too. In this discussion, we show that existing deterministic PAC-Bayes based bounds are in fact two-sided in that they are lower bounded by unif-alg too.\nFor a given prior distribution P over the parameters, a PAC-Bayesian bound is of the following form: with high probability 1 \u2212 \u03b4 over the draws of the data S, we have that for all distributions Q over the hypotheses space:\nKL Eh \u223cQ [L S (h)] Eh \u223cQ [L D (h)] \u2264 KL(Q P ) + ln 2m \u03b4 m \u2212 1 := pb (P,Q,m,\u03b4) . (27\n)\nNote that here for any a, b\n\u2208 [0, 1], KL(a b) = a ln a b + (1 \u2212 a) ln 1\u2212a 1\u2212b .\nSince the precise form of the PAC-Bayesian bound on the right hand side is not relevant for the rest of the discussion, we will concisely refer to it as pb (P, Q, m, \u03b4). What is of interest to us is the fact that the above bound holds for all Q for most draws of S and that the KL-divergence on the right-hand side is in itself two-sided, in some sense. Typically, the above bound is simplified to derive the following one-sided bound on the difference between the expected and empirical errors of a stochastic network (see [25] for example):\nEh \u223cQ [L D (h)] \u2212 Eh \u223cQ [L S (h)] \u2264 2 pb (P, Q, m, \u03b4) + 2 pb (P, Q, m, \u03b4). (28\n)\nThis bound is then manipulated in different ways to obtain bounds on the deterministic network. In the rest of this discussion, we focus on the two major such derandomizing techniques and argue that both these techniques boil down to two-sided convergence. While, we do not formally establish that there may exist other techniques which ensure that the resulting deterministic bound is strictly one-sided, we suspect that no such techniques may exist. This is because the KL-divergence bound in Equation 27 is in itself two-sided in the sense that for the right hand side bound to be small, both the stochastic test and train errors must be close to each other; it is not sufficient if the stochastic test error is smaller than the stochastic train error.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "J.1 Deterministic PAC-Bayesian Bounds of Type A", "text": "To derive a deterministic generalization bound, one approach is to add extra terms that account for the perturbation in the loss of the network [31,25,28]. That is, define:\n\u2206(h, Q, D) = |L D (h) \u2212 Eh \u223cQ [L D (h)]|, \u2206(h, Q, S) = L S (h) \u2212 Eh \u223cQ [L S (h)] .\nThen, one can get a deterministic upper bound as:\nL D (h) \u2212L S (h) \u2264 2 pb (P, Q h , m, \u03b4) + 2 pb (P, Q h , m, \u03b4) + \u2206(h, Q, D) + \u2206(h, Q, S).\nNote that while applying this technique, for any hypothesis h, one picks a posterior Q h specific to that hypothesis (typically, centered at that hypothesis).\nWe formally define the deterministic bound resulting from this technique below. We consider the algorithm-dependent version and furthermore, we consider a bound that results from the best possible choice of Q h for all h. We define this deterministic bound in the format of unif-alg as follows:\nDefinition J.1. The distribution-dependent, algorithm-dependent, deterministic PAC-Bayesian bound of (the hypothesis class H, algorithm A)-pair with respect to L is defined to be the smallest value pb-det-A (m, \u03b4) such that the following holds:\n1. there exists a set of m-sized samples S \u03b4 \u2286 (X \u00d7 {\u22121, +1}) m for which:\nP r S\u223cD m [S / \u2208 S \u03b4 ] \u2264 \u03b4,\n2. and if we define H \u03b4 = S\u2208S \u03b4 {h S } to be the space of hypotheses explored only on these samples, then there must exist a prior P and for each h \u2208 H \u03b4 , a distribution Q h , such that uniform convergence must hold as follows:\nsup S\u2208S \u03b4 sup h\u2208H \u03b4 2 pb (P, Q h , m, \u03b4) + 2 pb (P, Q h , m, \u03b4) + \u2206(h, Q h , D) + \u2206(h, Q h , S) < pb-det-A (m, \u03b4),(29)\nas a result of which, by Equation 28, the following one-sided uniform convergence also holds:\nsup S\u2208S \u03b4 sup h\u2208H \u03b4 L D (h) \u2212L S (h) < pb-det-A (m, \u03b4).(30)\nNow, recall that unif-alg (m, \u03b4) is a two-sided bound, and in fact our main proof crucially depended on this fact in order to lower bound unif-alg (m, \u03b4). Hence, to extend our lower bound to pb-det-A (m, \u03b4) we need to show that it is also two-sided in that it is lower bounded by unif-alg (m, \u03b4). The following result establishes this: \n. Now, for each pair of h \u2208 H 3\u03b4 and S \u2208 S 3\u03b4 , we will bound its empirical error minus the expected error in terms of pb-det-A (m, \u03b4). For convenience, let us denote by a := Eh \u223cQ h [L S (h)] and b := Eh \u223cQ h [L D (h)] (note that a and b are terms that depend on a hypothesis h and a sample set S).\nWe consider two cases. First, for some h \u2208 H 3\u03b4 and S \u2208 S 3\u03b4 , consider the case that e 3/2 b > a. Then, we haveL S (h) \u2212 L D (h) \u2264a \u2212 b + \u2206(h, Q h , D) + \u2206(h, Q h , S)\napplying Equation 29\u2264(e 3/2 \u2212 1) b\napply Equation 31+ pb-det-A (m, \u03b4) \n\u2264\non the second term, we can apply the inequality ln x \u2265 (x\u22121)(x+1) 2x = 1 2 x \u2212 1 x which holds for x \u2208 [0, 1] to get:\n(1 \u2212 a) ln 1 \u2212 a 1 \u2212 b \u2265 1 2 (1 \u2212 a) 1 \u2212 a 1 \u2212 b \u2212 1 \u2212 b 1 \u2212 a = (b \u2212 a)(2 \u2212 a \u2212 b) 2(1 \u2212 b) \u2265 \u2212(a \u2212 b) (2 \u2212 a \u2212 b) 2(1 \u2212 b) \u2265 \u2212(a \u2212 b) (2 \u2212 b) 2(1 \u2212 b) \u2265 \u2212 (a \u2212 b) 2 1 (1 \u2212 b) + 1 .\nPlugging this back in Equation 33, we have,\npb (P, Q h , m, \u03b4) \u2265 a ln a b \u22653/2 \u2212 (a \u2212 b) 2 1 (1 \u2212 b) + 1 \u2265 2a(1 \u2212 b) \u2212 (a \u2212 b) 2(1 \u2212 b) + b 2 \u2265 2a(1 \u2212 b) \u2212 (a \u2212 b) 2(1 \u2212 b) \u2265 a \u2212 2ab + b 2(1 \u2212 b) \u2265 a \u2212 2ab + ab 2(1 \u2212 b) \u2265 a 2 \u2265 a \u2212 b 2 \u2265 1 2 L S (h) \u2212 L D (h) \u2212 (\u2206(h, Q h , D) + \u2206(h, Q h , S)) .\nRearranging, we get:\nL S (h) \u2212 L D (h) \u2264 2 pb (P, Q h , m, \u03b4) + (\u2206(h, Q h , D) + \u2206(h, Q h , S))\nApplying Equation 29\u2264 pb-det-A (m, \u03b4).\nSince, for all h \u2208 H 3\u03b4 and S \u2208 S 3\u03b4 , one of Equations 32 and 34 hold, we have that: It follows from Equation 30 that the above bound holds good even after we take the absolute value of the first term in the left hand side. However, the absolute value is lower-bounded by unif-alg (m, 3\u03b4) (which follows from how unif-alg (m, 3\u03b4) is defined to be the smallest possible value over the choices of H 3\u03b4 , S 3\u03b4 ).\nAs a result of the above theorem, we can show that pb-det-A (m, \u03b4) = \u2126(1) \u2212 O( ), thus establishing that, for sufficiently large D, even though the generalization error would be negligibly small, the PAC-Bayes based bound would be as large as a constant.\nCorollary J.1.1. In the setup of Section 3, for any , \u03b4 > 0, \u03b4 < 1/12, when D = \u2126 max m ln 3 \u03b4 , m ln 1 , we have, e \u22123/2 \u2022 (1 \u2212 ) \u2212 (1 \u2212 e \u22123/2 )( ) \u2264 pb-det-A (m, \u03b4).\nProof. The fact that gen (m, \u03b4) \u2264 follows from Theorem 3.1. Additionally,\u02c6 (m, \u03b4) = 0 follows from the proof of Theorem 3.1. Now, as long as 3\u03b4 < 1/4, and D is sufficiently large (i.e., in the lower bounds on D in Theorem 3.1, if we replace \u03b4 by 3\u03b4), we have from Theorem 3.1 that unif-alg (m, 3\u03b4) > 1 \u2212 . Plugging these in Theorem J.1, we get the result in the above corollary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "J.2 Deterministic PAC-Bayesian Bounds of Type B", "text": "In this section, we consider another standard approach to making PAC-Bayesian bounds deterministic [32,23]. Here, the idea is to pick for each h a distribution Q h such that for all x: L (0) (h(x), y) \u2264 Eh \u223cQ h [L (\u03b3/2) (h(x), y)] \u2264 L (\u03b3) (h(x), y), where L (\u03b3) (y, y ) = 0 y \u2022 y \u2265 \u03b3 1 else.\nThen, by applying the PAC-Bayesian bound of Equation 28 for the loss L \u03b3/2 , one can get a deterministic upper bound as follows, without having to introduce the extra \u2206 terms, L\nD (h) \u2212L (\u03b3) S (h) \u2264Eh \u223cQ h [L (\u03b3/2) (h)] \u2212 Eh \u223cQ h [L (\u03b3/2) S (h)](0)\n\u2264 2 pb (P, Q h , m, \u03b4) + 2 pb (P, Q h , m, \u03b4).\nWe first define this technique formally:\nDefinition J.2. The distribution-dependent, algorithm-dependent, deterministic PAC-Bayesian bound of (the hypothesis class H, algorithm A)-pair is defined to be the smallest value pb-det-B (m, \u03b4) such that the following holds:\n1. there exists a set of m-sized samples S \u03b4 \u2286 (X \u00d7 {\u22121, +1}) m for which:\nP r S\u223cD m [S / \u2208 S \u03b4 ] \u2264 \u03b4.\n2. and if we define H \u03b4 = S\u2208S \u03b4 {h S } to be the space of hypotheses explored only on these samples, then there must exist a prior P and for each h a distribution Q h , such that uniform convergence must hold as follows: for all S \u2208 S \u03b4 and for all h \u2208 H \u03b4 , 2 pb (P, Q h , m, \u03b4) + 2 pb (P, Q h , m, \u03b4) < pb-det-B (m, \u03b4).\nand for all x: L (0) (h(x), y) \u2264 Eh \u223cQ h [L (\u03b3/2) (h(x), y)] \u2264 L (\u03b3) (h(x), y)\nas a result of which the following one-sided uniform convergence also holds:\nsup S\u2208S \u03b4 sup h\u2208H \u03b4 L (0) D (h) \u2212L (\u03b3)\nS (h) < pb-det-B (m, \u03b4).\nWe can similarly show that pb-det-B (m, \u03b4) is lower-bounded by the uniform convergence bound of unif-alg too. Theorem J.2. Let A be an algorithm such that on at least 1 \u2212 \u03b4 draws of the training dataset S, the algorithm outputs a hypothesis h S such that the margin-based training loss can be bounded as:\nL (\u03b3)\nS (h S ) \u2264\u02c6 (m, \u03b4) and with high probability 1 \u2212 \u03b4 over the draws of S, the generalization error can be bounded as:\nL (\u03b3) D (h S ) \u2212 L (\u03b3) S (h S ) \u2264 gen (m, \u03b4)\nThen there exists a set of samples S 3\u03b4 of mass at least 1 \u2212 3\u03b4, and a corresponding set of hypothesis H 3\u03b4 learned on these sample sets such that: Note that the above statement is slightly different from how Theorem J.1 is stated as it is not expressed in terms of unif-alg . In the corollary that follows the proof of this statement, we will see how it can be reduced in terms of unif-alg .\nProof. Most of the proof is similar to the proof of Theorem J.1. Like in the proof of Theorem J.1, we can argue that there exists S 3\u03b4 and H 3\u03b4 for which the test error can be bounded as,\nEh \u223cQ h [L (\u03b3/2) D (h)] \u2264 L (\u03b3) D(\nh) \u2264\u02c6 (m, \u03b4) + gen (m, \u03b4), where we have used gen (m, \u03b4) to denote the generalization error of L (\u03b3) and not the 0-1 error (we note that this is ambiguous notation, but we keep it this way for simplicity).\nFor convenience, let us denote by a := Eh \u223cQ h [L (\u03b3/2) S (h)] and b := Eh \u223cQ h [L (\u03b3/2) D (h)]. Again, let us consider, for some h \u2208 H 3\u03b4 and S \u2208 S 3\u03b4 , the case that e 3/2 b \u2265 a. Then, we have, using the above equation,L Rearranging, we get:\nL (0) S (h) \u2212 L (\u03b3) D(\nh) \u2264 2 pb (P, Q h , m, \u03b4)\nApplying Equation 35\u2264 pb-det-B (m, \u03b4).\nSince, for all h \u2208 H 3\u03b4 and S \u2208 S 3\u03b4 , one of Equations 37 and 38 hold, we have the claimed result.\nSimilarly, as a result of the above theorem, we can show that pb-det-B (m, \u03b4) = \u2126(1) \u2212 O( ), thus establishing that, for sufficiently large D, even though the generalization error would be negligibly small, the PAC-Bayes based bound would be as large as a constant and hence cannot explain generalization.\nCorollary J.2.1. In the setup of Section 3, for any , \u03b4 > 0, \u03b4 < 1/12, when D = \u2126 max m ln 3 \u03b4 , m ln 1 , we have, 1 \u2212 (e 3/2 \u2212 1) \u2264 pb-det-B (m, \u03b4).\nProof. It follows from the proof of Theorem 3.1 that\u02c6 (m, \u03b4) = 0, since all training points are classified by a margin of \u03b3 (see Equation 9). Similarly, from Equation 12 in that proof, since most test points are classified by a margin of \u03b3, gen (m, \u03b4) \u2264 . Now, as long as 3\u03b4 < 1/4, and D is sufficiently large (i.e., in the lower bounds on D in Theorem 3.1, if we replace \u03b4 by 3\u03b4), we will get that there exists S \u2208 S 3\u03b4 and h \u2208 H 3\u03b4 for which the empirical loss L (0) loss is 1. Then, by Theorem J.2, we get the result in the above corollary.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements. Vaishnavh Nagarajan is supported by a grant from the Bosch Center for AI.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H An abstract setup", "text": "We now present an abstract setup that, although unconventional in some ways, conveys the essence behind how uniform convergence fails to explain generalization. Let the underlying distribution over the inputs be a spherical Gaussian in R D where D can be however small or large as the reader desires. Note that our setup would apply to many other distributions, but a Gaussian would make our discussion easier. Let the labels of the inputs be determined by some h : R D \u2192 {\u22121, +1}. Consider a scenario where the learning algorithm outputs a very slightly modified version of h . Specifically, let S = {\u2212x | x \u2208 S}; then, the learner outputs\nThat is, the learner misclassifies inputs that correspond to the negations of the samples in the training data -this would be possible if and only if the classifier is overparameterized with \u2126(mD) parameters to store S . We will show that uniform convergence fails to explain generalization for this learner.\nFirst we establish that this learner generalizes well. Note that a given S has zero probability mass under D, and so does S . Then, the training and test error are zero -except for pathological draws of S that intersect with S , which are almost surely never drawn from D m -and hence, the generalization error of A is zero too.\nIt might thus seem reasonable to expect that one could explain this generalization using implicitregularization-based uniform convergence by showing unif-alg (m, \u03b4) = 0. Surprisingly, this is not the case as unif-alg (m, \u03b4) is in fact 1!\nFirst it is easy to see why the looser bound unif (m, \u03b4) equals 1, if we let H be the space of all hypotheses the algorithm could output: there must exist a non-pathological S \u2208 S \u03b4 , and we know that h S \u2208 H misclassifies the negation of its training set, namely S. Then,\nOne might hope that in the stronger bound of unif-alg (m, \u03b4) since we truncate the hypothesis space, it is possible that the above adversarial situation would fall apart. However, with a more nuanced argument, we can similarly show that unif-alg (m, \u03b4) = 1. First, recall that any bound on unif-alg (m, \u03b4), would have to pick a truncated sample set space S \u03b4 . Consider any choice of S \u03b4 , and the corresponding set of explored hypotheses H \u03b4 . We will show that for any choice of S \u03b4 , there exists S \u2208 S \u03b4 such that (i) h S has zero test error and (ii) the negated training set S belongs to S \u03b4 and (iii) h S has error 1 on S . Then, it follows that unif-alg (m, \u03b4)\nWe can prove the existence of such an S by showing that the probability of picking one such set under D m is non-zero for \u03b4 < 1/2. Specifically, under S \u223c D m , we have by the union bound that\nSince the pathological draws have probability zero, the first probability term on the right hand side is zero. The second term is at most \u03b4 by definition of S \u03b4 . Crucially, the last term too is at most \u03b4 because S (which is the negated version of S) obeys the same distribution as S (since the isotropic Gaussian is invariant to a negation). Thus, the above probability is at least 1 \u2212 2\u03b4 > 0, implying that there exist (many) S , proving our main claim.\nRemark. While our particular learner might seem artificial, much of this artificiality is only required to make the argument simple. The crucial trait of the learner that we require is that the misclassified region in the input space (i) covers low probability and yet (ii) is complex and highly dependent on the training set draw. Our intuition is that SGD-trained deep networks possess these traits.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I Learnability and Uniform Convergence", "text": "Below, we provide a detailed discussion on learnability, uniform convergence and generalization. Specifically, we argue why the fact that uniform convergence is necessary for learnability does not", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning and generalization in overparameterized neural networks, going beyond two layers", "journal": "", "year": "2018", "authors": "Zeyuan Allen-Zhu; Yuanzhi Li; Yingyu Liang"}, {"ref_id": "b1", "title": "Stronger generalization bounds for deep nets via a compression approach", "journal": "", "year": "2018", "authors": "Sanjeev Arora; Rong Ge; Behnam Neyshabur; Yi Zhang"}, {"ref_id": "b2", "title": "Spectrally-normalized margin bounds for neural networks", "journal": "", "year": "2017", "authors": "L Peter; Dylan J Bartlett; Matus J Foster;  Telgarsky"}, {"ref_id": "b3", "title": "To understand deep learning we need to understand kernel learning", "journal": "", "year": "2018", "authors": "Mikhail Belkin; Siyuan Ma; Soumik Mandal"}, {"ref_id": "b4", "title": "Stability and generalization", "journal": "Journal of Machine Learning Research", "year": "2002", "authors": "Olivier Bousquet; Andr\u00e9 Elisseeff"}, {"ref_id": "b5", "title": "SGD learns overparameterized networks that provably generalize on linearly separable data. International Conference on Learning Representations (ICLR)", "journal": "", "year": "2018", "authors": "Alon Brutzkus; Amir Globerson; Eran Malach; Shai Shalev-Shwartz"}, {"ref_id": "b6", "title": "Essentially no barriers in neural network energy landscape", "journal": "", "year": "2018", "authors": "Felix Dr\u00e4xler; Kambis Veschgini; Manfred Salmhofer; Fred A Hamprecht"}, {"ref_id": "b7", "title": "A pacbayesian approach to spectrally-normalized margin bounds for neural networks", "journal": "", "year": "2018", "authors": "Srinadh Behnam Neyshabur; David Bhojanapalli; Nathan Mcallester;  Srebro"}, {"ref_id": "b8", "title": "The role of over-parametrization in generalization of neural networks", "journal": "", "year": "2019", "authors": "Zhiyuan Behnam Neyshabur; Srinadh Li; Yann Bhojanapalli; Nathan Lecun;  Srebro"}, {"ref_id": "b9", "title": "A finite sample distribution-free performance bound for local discrimination rules", "journal": "The Annals of Statistics", "year": "1978", "authors": "W H Rogers; T J Wagner"}, {"ref_id": "b10", "title": "Learnability, stability and uniform convergence", "journal": "Journal of Machine Learning Research", "year": "2010", "authors": "Shai Shalev-Shwartz; Ohad Shamir; Nathan Srebro; Karthik Sridharan"}, {"ref_id": "b11", "title": "The implicit bias of gradient descent on separable data. International Conference on Learning Representations (ICLR)", "journal": "", "year": "2018", "authors": "Daniel Soudry; Elad Hoffer; Nathan Srebro"}, {"ref_id": "b12", "title": "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities", "journal": "", "year": "1971", "authors": "V N Vapnik; A Ya;  Chervonenkis"}, {"ref_id": "b13", "title": "High-Dimensional Statistics: A Non-Asymptotic Viewpoint", "journal": "Cambridge University Press", "year": "2019", "authors": "Martin J Wainwright"}, {"ref_id": "b14", "title": "Understanding deep learning requires rethinking generalization", "journal": "", "year": "2017", "authors": "Chiyuan Zhang; Samy Bengio; Moritz Hardt; Benjamin Recht; Oriol Vinyals"}, {"ref_id": "b15", "title": "Nonvacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach", "journal": "", "year": "2019", "authors": "Wenda Zhou; Victor Veitch; Morgane Austern; Ryan P Adams; Peter Orbanz"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Experiments in Section 2: In the first figure, we plot (i) 2 the distance of the network from the initialization and (ii) the 2 distance between the weights learned on two random draws of training data starting from the same initialization. In the second figure we plot the product of spectral norms of the weights matrices. In the third figure, we plot the test error. In the fourth figure, we plot the bounds from[32,3]. Note that we have presented log-log plots and the exponent of m can be recovered from the slope of these plots.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: In the first figure, we plot the error of the ReLU network on test data and on the bad dataset S , in the task described in Section 3.2. The second and third images correspond to the decision boundary learned in this task, in the 2D quadrant containing two training datapoints (depicted as \u00d7 and \u2022). The black lines correspond to the two hyperspheres, while the brown and blue regions correspond to the class output by the classifier. Here, we observe that the boundaries are skewed around the training data in a way that it misclassifies the nearest point from the opposite class (corresponding to S , that is not explicitly marked). The fourth image corresponds to two random (test) datapoints, where the boundaries are fairly random, and very likely to be located in between the hyperspheres (better confirmed by the low test error).", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: We plot the distance from initialization and the spectral norm of each individual layer, and observe that the lowermost layer shows the greatest dependence on m.", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :Figure 6 :56Figure5: On the left, we plot the distance between the weights learned on the two different shuffles of the same dataset, and it grows as fast as the distance from initialization. On the right, we plot the distance of the weights from the origin, learned for a network of width h = 256 and depth d = 6; for sufficiently large m, this grows as \u2126(m 0.42 ).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 8 :8Figure8: On the left, we plot the bounds for varying m for h = 128. All these bounds grow with m as \u2126(m 0.94 ). On the right, we show a similar plot for h = 2000 and observe that the bounds grow as \u2126(m 0.79 ).", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 12 :12Figure 12: We plot the average margin of the network on the train and test data, and the difference between the two, the last of which decreases with m as O(1/m 0.33 ).", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 13 :13Figure 13: In the hypersphere example of Section 3.2, we plot the average margin of the network on the train and test data, and the difference between the two. We observe the train and test margins do converge to each other.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "S(h S ) < 1/2 and L (\u03b3) D (h S ) < 1/2 (which follows from the second inequality in Equation25).", "figure_data": ""}, {"figure_label": "m=1", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "\u221e m=1 ZDm=1m \u2192 H and a monotonically decreasing sequence lnblty (m) such that lnblty (m) m\u2192\u221e \u2212 \u2212\u2212\u2212 \u2192 0 and \u2200D E S\u223cD m L (h) \u2264 lnblty (m).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Theorem J. 1 .1Let A be an algorithm such that on at least 1 \u2212 \u03b4 draws of the training dataset S, the algorithm outputs a hypothesis h S that has\u02c6 (m, \u03b4) loss on the training data S. Thene \u22123/2 \u2022 unif-alg (m, 3\u03b4) \u2212 (1 \u2212 e \u22123/2 )(\u02c6 (m, \u03b4) + gen (m, \u03b4)) \u2264 pb-det-A (m, \u03b4).Proof. First, by the definition of the generalization error, we know that with probability at least 1 \u2212 \u03b4 over the draws of S, L D (h S ) \u2264L S (h S ) + gen (m, \u03b4). Furthermore since the training loss it at most\u02c6 (m, \u03b4) on at least 1 \u2212 \u03b4 draws we have that on at least 1 \u2212 2\u03b4 draws of the dataset, L D (h S ) \u2264\u02c6 (m, \u03b4) + gen (m, \u03b4).Let H \u03b4 and S \u03b4 be the subset of hypotheses and sample sets as in the definition of pb-det-A . Then, from the above, there exist H 3\u03b4 \u2286 H \u03b4 and S 3\u03b4 \u2286 S \u03b4 such thatP r S\u223cD m [S / \u2208 S 3\u03b4 ] \u2264 3\u03b4and H 3\u03b4 = S\u2208S 3\u03b4 {h S }, and furthermore,suph\u2208H 3\u03b4 L D (h) \u2264\u02c6 (m, \u03b4) + gen (m, \u03b4).. Using the above, and the definition of \u2206, we have for all h \u2208 H 3\u03b4 , the following upper bound on its stochastic test error:Eh \u223cQ h [L D (h)] \u2264 L D (h) + \u2206(h, Q h , D) \u2264\u02c6 (m, \u03b4) + gen (m, \u03b4) + \u2206(h, Q h , D)applying Equation29\u2264\u02c6 (m, \u03b4) + gen (m, \u03b4) + pb-det-A (m, \u03b4).", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "(e 3 / 2 \u2212321)(\u02c6 (m, \u03b4) + gen (m, \u03b4) + pb-det-A (m, \u03b4)) + pb-det-A (m, \u03b4) \u2264(e 3/2 \u2212 1)(\u02c6 (m, \u03b4) + gen (m, \u03b4)) + e 3/2 \u2022 pb-det-A (m, \u03b4). (32)Now consider the case where a > e 3/2 b. This means that (1 \u2212 a) < (1 \u2212 b). Then, if we consider the PAC-Bayesian bound of Equation27,a ln a b + (1 \u2212 a) ln 1 \u2212 a 1 \u2212 b \u2264 pb (P, Q h , m, \u03b4),", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "S(h) \u2212 L D (h) \u2212 (e 3/2 \u2212 1)e 3/2 (\u02c6 (m, \u03b4) + gen (m, \u03b4)) \u2264 pb-det-A (m, \u03b4).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "D(h) \u2212 (e 3/2 \u2212 1)(\u02c6 (m, \u03b4) + gen (m, \u03b4)) \u2264 pb-det-B (m, \u03b4).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "D(h) \u2264a \u2212 b \u2264(e 3/2 \u2212 1)b \u2264(e 3/2 \u2212 1)(\u02c6 (m, \u03b4) + gen (m, \u03b4)) \u2264(e 3/2 \u2212 1)(\u02c6 (m, \u03b4) + gen (m, \u03b4) + pb-det-B (m, \u03b4)).(37)Now consider the case where a > e 3/2 b. Again, by similar arithmetic manipulation in the PAC-Bayesian bound of Equation 28 applied on L (\u03b3/2) , we get, pb (P, Q h , m, \u03b4) \u2265 a ln a b", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, 2017. Vetrov, and Andrew G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 2018. [11] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian J. Goodfellow. Adversarial spheres. In 6th International Conference on Learning Representations, ICLR 2018, 2018. [12] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. Computational Learning Theory, COLT 2018, 2018. [13] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Proceedings of the 33nd International Conference on Machine Learning, ICML, 2016. 15] Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, COLT, 1993. [16] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Computation, 9(1), 1997. [17] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in Neural Information Processing Systems (to appear), 2017. [18] Arthur Jacot, Cl\u00e9ment Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 2018. [19] Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos J. Storkey. Width of minima reached by stochastic gradient descent is influenced by learning rate to batch size ratio. In Artificial Neural Networks and Machine Learning -ICANN 2018 -27th International Conference on Artificial Neural Networks, 2018. Neural Information Processing Systems, NIPS 2002, 2002. [24] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 2018. [25] David McAllester. Simplified pac-bayesian margin bounds. In Learning Theory and Kernel Machines. Springer Berlin Heidelberg, 2003. [26] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. Adaptive computation and machine learning. MIT Press, 2012. [27] Vaishnavh Nagarajan and J. Zico Kolter. Generalization in deep networks: The role of distance from initialization. Deep Learning: Bridging Theory and Practice Workshop in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 2017. [28] Vaishnavh Nagarajan and Zico Kolter. Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience. In International Conference on Learning Representations (ICLR), 2019. [29] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. International Conference on Learning Representations Workshop Track, 2015.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Summary of generalization bounds for ReLU networks. We note that the analysis in Li and Liang [24] relies on a sufficiently small learning rate (\u2248 O(1/m 1.2 )) and large batch size (\u2248 \u2126( \u221a m)). Hence, the resulting bound cannot describe how generalization varies with any other hyperparameter, like training set size or width, with everything else fixed. A similar analysis in Allen-Zhu et al.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": ":= f (x)[y] \u2212 max y =y f (x)[y ] is at least \u03b3 .", "formula_coordinates": [3.0, 108.0, 574.31, 396.0, 27.28]}, {"formula_id": "formula_1", "formula_text": "Pr D [\u0393(f (x), y) \u2264 0] \u2264 1 m (x,y)\u2208S 1[\u0393(f (x), y) \u2264 \u03b3] + generalization error bound.(1)", "formula_coordinates": [4.0, 139.49, 510.0, 364.51, 27.27]}, {"formula_id": "formula_2", "formula_text": "O Bd \u221a h \u03b3 \u221a m d k=1 W k 2 \u00d7 dist where dist equals d k=1 W k \u2212Z k 2 F W k 2 2 in [32] and 1 d \u221a h d k=1 W k \u2212Z k 2,1 W k 2 2/3 3/2 in [3].", "formula_coordinates": [4.0, 108.0, 542.42, 396.0, 46.17]}, {"formula_id": "formula_3", "formula_text": "L (\u03b3) (y , y) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 yy \u2264 0 1 \u2212 yy \u03b3 yy \u2208 (0, \u03b3) 0 yy \u2265 \u03b3.", "formula_coordinates": [5.0, 227.94, 284.48, 154.92, 54.85]}, {"formula_id": "formula_4", "formula_text": "such that: Pr S\u223cD m L D (h S ) \u2212L S (h S ) \u2264 gen (m, \u03b4) \u2265 1 \u2212 \u03b4.", "formula_coordinates": [5.0, 108.0, 473.16, 255.0, 17.29]}, {"formula_id": "formula_5", "formula_text": "Pr S\u223cD m sup h\u2208H L D (h) \u2212L S (h) \u2264 unif (m, \u03b4) \u2265 1 \u2212 \u03b4.", "formula_coordinates": [5.0, 193.55, 557.07, 243.72, 17.29]}, {"formula_id": "formula_6", "formula_text": "S \u03b4 \u2286 (X \u00d7 {\u22121, 1}) m for which P r S\u223cD m [S \u2208 S \u03b4 ] \u2265 1 \u2212 \u03b4 and furthermore, sup S\u2208S \u03b4 sup h\u2208H |L D (h) \u2212L S (h)| \u2264 unif (m, \u03b4).", "formula_coordinates": [6.0, 108.0, 156.38, 397.25, 30.87]}, {"formula_id": "formula_7", "formula_text": "sup S\u2208S \u03b4 sup h\u2208H \u03b4 L D (h) \u2212L S (h) \u2264 unif-alg (m, \u03b4).", "formula_coordinates": [6.0, 267.6, 267.7, 214.9, 17.29]}, {"formula_id": "formula_8", "formula_text": "Theorem 3.1. For any , \u03b4 > 0, \u03b4 \u2264 1/4, when D = \u2126 max m ln m \u03b4 , m ln 1 , \u03b3 \u2208 [0, 1], the L (\u03b3) loss satisfies gen (m, \u03b4) \u2264 , while unif-alg (m, \u03b4) \u2265 1 \u2212 . Furthermore, for all \u03b3 \u2265 0, for the L (\u03b3) loss, unif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4).", "formula_coordinates": [7.0, 107.67, 286.02, 396.33, 44.34]}, {"formula_id": "formula_9", "formula_text": "as unif-alg (m, \u03b4) = sup S\u2208S \u03b4 sup h\u2208H \u03b4 |L D (h)\u2212L S (h)| \u2265 |L D (h S )\u2212L S (h S )| = | \u22121| = 1\u2212 . Remark 3.1.", "formula_coordinates": [8.0, 108.0, 108.65, 396.0, 37.14]}, {"formula_id": "formula_10", "formula_text": "W 2 F ( W 1 \u2212 Z 1 F + Z 1 2 ) \u03b3 \u221a m + \u221a h \u221a m.", "formula_coordinates": [15.0, 228.79, 516.11, 159.41, 32.22]}, {"formula_id": "formula_11", "formula_text": "P r (x,y)\u223cD [\u0393(f (x), y) \u2264 0] \u2264 1 m (x,y)\u2208S 1[\u0393(f (x), y) \u2264 \u03b3] + generalization error bound. (1)", "formula_coordinates": [17.0, 186.28, 631.9, 239.44, 42.43]}, {"formula_id": "formula_12", "formula_text": "sup S\u2208S \u03b4 sup h\u2208H \u03b4 1 \u03b3 E D [\u0393(h(x), y)] \u2212 1 m (x,y)\u2208S \u0393(h(x), y) .(2)", "formula_coordinates": [18.0, 193.74, 100.09, 310.26, 27.27]}, {"formula_id": "formula_13", "formula_text": "1 \u03b3 \uf8eb \uf8ed E (x,y)\u223cD [\u0393(h S (x), y)] \u2212 1 m (x,y)\u2208S \u0393(h S (x), y) \uf8f6 \uf8f8 .(3)", "formula_coordinates": [18.0, 193.73, 194.68, 310.27, 41.44]}, {"formula_id": "formula_14", "formula_text": "P r [|X \u2212 \u00b5| \u2265 t] \u2264 2 exp \u2212 1 2 min t b , t 2 \u03bd 2 .", "formula_coordinates": [20.0, 206.75, 94.89, 198.5, 24.68]}, {"formula_id": "formula_15", "formula_text": "Corollary D.1.1.", "formula_coordinates": [20.0, 108.0, 145.27, 68.24, 8.96]}, {"formula_id": "formula_16", "formula_text": "P r \uf8ee \uf8f0 1 D D j=1 z 2 j \u2208 [c 2 2 , c 2 3 ] \uf8f9 \uf8fb \u2264 2 exp(\u2212c 1 D).", "formula_coordinates": [20.0, 217.96, 153.83, 176.08, 40.82]}, {"formula_id": "formula_17", "formula_text": "P r D d=1 z d \u2265 t \u2264 2 exp(\u2212t 2 /2 D d=1 \u03c3 2 d ).", "formula_coordinates": [20.0, 218.82, 252.49, 174.36, 30.55]}, {"formula_id": "formula_18", "formula_text": "Corollary D.2.1. For any u = (u 1 , u 2 , . . . , u d ) \u2208 R D , for z 1 , z 2 , . . . , z D \u223c N (0, 1), P r D d=1 u d z d \u2265 u 2 \u2022 c 4 ln 2 \u03b4 \u2264 \u03b4.", "formula_coordinates": [20.0, 108.0, 309.37, 341.03, 48.91]}, {"formula_id": "formula_19", "formula_text": "Distribution D: Each input (x 1 , x 2", "formula_coordinates": [20.0, 108.0, 415.02, 142.36, 17.29]}, {"formula_id": "formula_20", "formula_text": "2 = i y (i) x (i) 2 .", "formula_coordinates": [20.0, 116.02, 499.03, 66.11, 14.28]}, {"formula_id": "formula_21", "formula_text": "D \u2265 1 c 1 ln 6m \u03b4 ,(4)", "formula_coordinates": [20.0, 251.72, 573.27, 252.28, 23.22]}, {"formula_id": "formula_22", "formula_text": "D \u2265 m 4c 4 c 3 c 2 2 2 ln 6m \u03b4 ,(5)", "formula_coordinates": [20.0, 251.72, 598.86, 252.28, 27.59]}, {"formula_id": "formula_23", "formula_text": "D \u2265 m 4c 4 c 3 c 2 2 2 \u2022 2ln 2 ,(6)", "formula_coordinates": [20.0, 251.72, 628.81, 252.28, 27.59]}, {"formula_id": "formula_24", "formula_text": "y (i) x (i) 2 \u223c N (0, 8m c 2 2 D ).", "formula_coordinates": [21.0, 108.0, 198.21, 396.0, 31.57]}, {"formula_id": "formula_25", "formula_text": "x (i)", "formula_coordinates": [21.0, 443.86, 234.56, 15.09, 11.87]}, {"formula_id": "formula_26", "formula_text": "c 2 \u2264 1 2 \u221a 2 c 2 x (i) 2 \u2264 c 3 .(7)", "formula_coordinates": [21.0, 255.06, 270.26, 248.94, 23.42]}, {"formula_id": "formula_27", "formula_text": "j =i y (j) x (j) 2 , |x (i) 2 \u2022 j =i y (j) x (j) 2 | \u2264 c 4 x (i) 2 2 \u221a 2 \u2022 \u221a m c 2 \u221a D ln 6m \u03b4 .(8)", "formula_coordinates": [21.0, 118.52, 323.32, 385.48, 47.38]}, {"formula_id": "formula_28", "formula_text": "y (i) h(x (i) ) = y (i) w 1 \u2022 x (i) 1 + y (i) \u2022 y (i) x (i) 2 2 + y (i) \u2022 x (i) 2 \u2022 j =i y (j) x (j) 2 = 4 + x (i) 2 2", "formula_coordinates": [21.0, 160.14, 421.79, 291.22, 42.73]}, {"formula_id": "formula_29", "formula_text": "(i) 2 \u2022 j =i y (j) x (j) 2", "formula_coordinates": [21.0, 314.0, 450.57, 64.43, 23.27]}, {"formula_id": "formula_30", "formula_text": "\u2265 4 + 4 \u2022 2 \u2212 c 4 2 \u221a 2c 3 c 2 \u2022 2 \u221a 2 \u2022 \u221a m c 2 \u221a D ln 6m \u03b4", "formula_coordinates": [21.0, 206.77, 487.82, 182.17, 33.69]}, {"formula_id": "formula_32", "formula_text": "c 2 \u221a m \u2264 1 2 \u221a 2 c 2 y (i) x (i) 2 \u2264 c 3 \u221a m. (10", "formula_coordinates": [21.0, 221.76, 618.81, 278.09, 25.26]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 627.7, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "|z 2 \u2022 y (i) x (i) 2 | \u2264 c 4 y (i) x (i) 2 \u2022 2 \u221a 2 c 2 \u221a D \u2022 ln 1 . (11", "formula_coordinates": [21.0, 199.41, 692.65, 300.44, 33.68]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 708.87, 4.15, 8.64]}, {"formula_id": "formula_36", "formula_text": "1 \u2212 2 exp \u2212 1 2 c 2 2 4c4c3 D m 2", "formula_coordinates": [22.0, 277.48, 71.73, 120.74, 24.3]}, {"formula_id": "formula_37", "formula_text": "yh(x) = yw 1 \u2022 z 1 + y \u2022 z 2 \u2022 j y (j) x (j) 2", "formula_coordinates": [22.0, 204.91, 114.17, 162.02, 23.05]}, {"formula_id": "formula_38", "formula_text": "\u2265 4 \u2212 c 4 y (i) x (i) 2", "formula_coordinates": [22.0, 232.46, 164.14, 90.69, 19.5]}, {"formula_id": "formula_39", "formula_text": "\u2022 2 \u221a 2 c 2 \u221a D c 2 2 4c 4 c 3 D m \u2265 4 \u2212 2 \u2265 2. (12", "formula_coordinates": [22.0, 232.46, 151.38, 267.39, 64.6]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 199.93, 4.15, 8.64]}, {"formula_id": "formula_41", "formula_text": "2 exp \u2212 1 2 c 2 2 4c4c3 D m 2", "formula_coordinates": [22.0, 107.75, 233.96, 103.18, 24.3]}, {"formula_id": "formula_42", "formula_text": "x (i) neg = (x (i) 1 , \u2212x (i)", "formula_coordinates": [22.0, 107.7, 491.22, 76.53, 19.5]}, {"formula_id": "formula_43", "formula_text": "y (i) h(x (i) neg ) = y (i) w 1 \u2022 x (i) 1 \u2212 y (i) \u2022 y (i) x (i) 2 2 \u2212 y (i) \u2022 x (i) 2 \u2022 j =i y (j) x (j) 2 = 4 \u2212 x (i) 2 2", "formula_coordinates": [22.0, 159.68, 524.25, 292.14, 48.28]}, {"formula_id": "formula_44", "formula_text": "y (i) x (i) 2 \u2022 j =i y (j) x (j) 2 apply Equation 8 \u2264 4 \u2212 4 \u2022 2 + c 4 2 \u221a 2c 3 c 2 \u2022 2 \u221a 2 \u2022 \u221a m c 2 \u221a D ln 3m \u03b4 apply Equation 5 \u2264 4 \u2212 8 + 2 = \u22122 < 0.", "formula_coordinates": [22.0, 207.23, 553.03, 173.87, 105.33]}, {"formula_id": "formula_45", "formula_text": "P r S\u223cD m S \u2208 S \u03b4 , S \u2208 S \u03b4 , L D (h S ) \u2264 gen (m, \u03b4),L S (h S ) = 1 \u2265 1 \u2212 P r S\u223cD m [S / \u2208 S \u03b4 ] \u2212 P r S\u223cD m [S / \u2208 S \u03b4 ] \u2212 P r S\u223cD m [L D (h S ) > gen (m, \u03b4)] \u2212 P r S\u223cD m L S (h S ) = 1 . (13", "formula_coordinates": [23.0, 175.41, 105.18, 324.45, 51.46]}, {"formula_id": "formula_46", "formula_text": ")", "formula_coordinates": [23.0, 499.85, 140.59, 4.15, 8.64]}, {"formula_id": "formula_47", "formula_text": "unif-alg (m, \u03b4) = sup S\u2208S \u03b4 sup h\u2208H \u03b4 |L D (h) \u2212L S (h)| \u2265 |L D (h S ) \u2212L S (h)| = | \u2212 1| = 1 \u2212 .", "formula_coordinates": [23.0, 196.36, 290.31, 223.32, 41.2]}, {"formula_id": "formula_48", "formula_text": "h(x) = E w [a w exp(w \u2022 x)].", "formula_coordinates": [23.0, 249.22, 580.77, 113.57, 17.29]}, {"formula_id": "formula_49", "formula_text": "a w \u2190 a w + \u03b7y \u2022 exp(w \u2022 x) \u2022 p(w). (14", "formula_coordinates": [23.0, 233.01, 674.34, 266.84, 17.29]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [23.0, 499.85, 675.57, 4.15, 8.64]}, {"formula_id": "formula_51", "formula_text": "1 (2\u03c0) D exp \u2212 w 2 2 .", "formula_coordinates": [23.0, 109.2, 707.28, 84.24, 20.79]}, {"formula_id": "formula_52", "formula_text": "D \u2265 max 1 c 2 , (16c 3 c 4 ) 2 \u2022 2 ln 6m (15) D \u2265 max 1 c 2 , (16c 3 c 4 ) 2 \u2022 2 ln 6m \u03b4 (16) D \u2265 6 ln 2m(17)", "formula_coordinates": [24.0, 230.14, 168.55, 273.86, 72.09]}, {"formula_id": "formula_54", "formula_text": "h(z) = h (0) (z) + y exp z + x 2 2 .", "formula_coordinates": [24.0, 226.73, 421.05, 158.55, 25.51]}, {"formula_id": "formula_55", "formula_text": "h(z) \u2212 h (0) (z) \u03b7 = w (y \u2022 exp(w \u2022 x)p(w)) \u2022 exp(w \u2022 z)p(w)dw = y w exp(w \u2022 (x + z)) \u2022 1 2\u03c0 2D exp(\u2212 w 2 )dw = y 1 2\u03c0 2D w exp(w \u2022 (x + z) \u2212 w 2 )dw = y 1 2\u03c0 2D exp z + x 2 2 \u00d7 w exp \u2212 w \u2212 z + x 2 2 dw = y 1 4\u03c0 D exp z + x 2 2 \u00d7 1 2\u03c0(0.5) 2D w exp \u2212 w \u2212 z + x 2 2 dw = y 1 4\u03c0 D exp z + x 2 2 .", "formula_coordinates": [24.0, 132.59, 493.55, 347.87, 211.14]}, {"formula_id": "formula_56", "formula_text": "c 2 \u221a D \u2264 x (i) 2 \u2264 c 3 \u221a D.(19)", "formula_coordinates": [25.0, 252.95, 271.61, 251.05, 26.21]}, {"formula_id": "formula_57", "formula_text": "x (i) 2 and x (j) 2 for i = j, |x (i) 2 \u2022 x (j) 2 | \u2264 x (i) 2 \u2022 c 4 2 ln 6m \u03b4 .(20)", "formula_coordinates": [25.0, 107.7, 305.33, 396.3, 75.04]}, {"formula_id": "formula_58", "formula_text": "m j=1 y (j) \u2264 2m ln 6 \u03b4 Eq 18 < m 2 .(21)", "formula_coordinates": [25.0, 248.86, 452.66, 255.14, 39.2]}, {"formula_id": "formula_59", "formula_text": "y (i) h(x (i) ) = exp x (i) 2 + j =i y (i) y (j) exp x (i) + x (j) 2 2 \u2265 exp x (i) 2 \u2212 j =i y (i) =y (j) exp x (i) + x (j) 2 2 \u2265 exp x (i) 2 \u00d7 \uf8eb \uf8ec \uf8ec \uf8ed 1 \u2212 j =i y (i) =y (j) exp x (i) + x (j) 2 \u2212 4 x (i) 2 4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 .", "formula_coordinates": [25.0, 134.13, 566.71, 343.73, 130.11]}, {"formula_id": "formula_60", "formula_text": "x (i) + x (j) 2 \u2212 4 x (i) 2 = \u22123 x (i) 2 + x (j) 1 2 + 2x (i) 1 \u2022 x (j) 1 + x (j) 2 2 Eq 19 + 2x (i) 2 \u2022 x (j) 2 Eq 20 \u2264 \u22123 u 2 \u2212 3 x (i) 2 2 Eq 19 + u 2 \u2212 2 u 2 + c 2 3 D + x (i) 2", "formula_coordinates": [26.0, 112.98, 89.95, 326.41, 82.59]}, {"formula_id": "formula_61", "formula_text": "\u20222c 4 2 ln 6m \u03b4 \u2264 \u22124 u 2 \u2212 3c 2 2 D + c 2 3 D + \u221a D \u2022 c 3 c 4 2 ln 6m \u03b4 Eq 16 \u2264 \u22121 \u2212 45 16 D + 17 16 D + 1 16 D = \u2212 43 16 D.", "formula_coordinates": [26.0, 216.62, 142.22, 287.94, 101.45]}, {"formula_id": "formula_62", "formula_text": "y (i) h(x (i) ) \u2265 \u2265 exp \uf8eb \uf8ec \uf8ec \uf8ed x (i) 2 Eq 19 \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed 1 \u2212 m exp \u2212 43 64 D Eq 17 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2265 exp 15 16 D Eq 17 \u2022 1 2 \u2265 1.", "formula_coordinates": [26.0, 186.03, 293.11, 239.95, 103.28]}, {"formula_id": "formula_63", "formula_text": "c 2 \u221a D \u2264 z 2 \u2264 c 3 \u221a D.(22)", "formula_coordinates": [26.0, 255.96, 466.75, 248.04, 26.21]}, {"formula_id": "formula_64", "formula_text": "|x (i) 2 \u2022 z 2 | \u2264 x (i) 2 \u2022 c 4 2 ln 6m .(23)", "formula_coordinates": [26.0, 236.93, 530.8, 267.07, 23.11]}, {"formula_id": "formula_65", "formula_text": "x (i) + z 2 = x (i) 1 2 + z 1 2 + 2x (i) 1 \u2022 z 1 + x (i) 2 2 Eq 19 + z (i) 2 2 Eq 22 +2 x (i) 2 \u2022 z 2 Eq 23, 19 \u2265 4 u 2 + 2c 2 2 D \u2212 \u221a D \u2022 2c 3 c 4 2", "formula_coordinates": [26.0, 158.35, 624.89, 300.28, 57.72]}, {"formula_id": "formula_66", "formula_text": "\u2265 D + 30 16 D \u2212 1 16 D = 45 16 D.", "formula_coordinates": [26.0, 213.95, 699.62, 122.08, 23.11]}, {"formula_id": "formula_67", "formula_text": "x (i) + z 2 = x (i) 1 2 + z 1 2 + 2x (i) 1 \u2022 z 1 + x (i) 2 2 Eq 19 + z (i) 2 2 Eq 22 +2 x (i) 2 \u2022 z 2", "formula_coordinates": [27.0, 158.35, 103.01, 299.27, 26.72]}, {"formula_id": "formula_68", "formula_text": "\u2264 2 u 2 \u2212 2 u 2 + 2c 2 3 D + \u221a D \u2022 2c 3 c 4 2 ln 6m \u03b4 Eq 16 \u2264 34 16 D \u2212 1 16 D = 33 16 D.", "formula_coordinates": [27.0, 213.95, 134.52, 208.95, 66.33]}, {"formula_id": "formula_69", "formula_text": "y (i) h(x (i) ) = i:y (i) =y exp x (i) + z 2 2 \u2212 i:y (i) =y exp x (i) + z 2 2 \u2265 exp 45 64 D \u2212 m exp 33 64 D \u2265 exp 45 64 D \u2022 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed 1 \u2212 m exp \u2212 12 64 D Eq 17 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2265 exp 45 64 D Eq 17 \u2022 1 2 \u2265 1.", "formula_coordinates": [27.0, 158.46, 246.75, 286.69, 160.98]}, {"formula_id": "formula_70", "formula_text": "unif-alg (m, \u03b4) \u2265 1 \u2212 gen (m, \u03b4) for the L (\u03b3) loss.", "formula_coordinates": [27.0, 108.0, 574.89, 259.53, 33.76]}, {"formula_id": "formula_71", "formula_text": "(i) neg , y (i)", "formula_coordinates": [27.0, 306.36, 647.2, 29.18, 12.81]}, {"formula_id": "formula_72", "formula_text": "y (i) neg h(x (i) neg ) = \u2212 exp \uf8eb \uf8ed x (i) + x (i) neg 2 2 \uf8f6 \uf8f8 + j =i y (i) neg y (j) exp \uf8eb \uf8ed x (i) neg + x (j) 2 2 \uf8f6 \uf8f8 \u2264 \u2212 exp x (i) 2 2 + j =i y (i) neg =y (j) exp \uf8eb \uf8ed x (i) neg + x (j) 2 2 \uf8f6 \uf8f8 \u2264 exp x (i) 2 2 \u00d7 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed \u22121 + j =i y (i)", "formula_coordinates": [27.0, 120.21, 681.82, 322.87, 41.05]}, {"formula_id": "formula_73", "formula_text": "x (i) neg + x (j) 2 \u2212 4 x (i) 2 2 4 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 .(24)", "formula_coordinates": [28.0, 351.18, 114.28, 152.82, 46.05]}, {"formula_id": "formula_74", "formula_text": "x (i) neg + x (j) 2 \u2212 4 x (i) 2 2 = x (i) 1 2 + x (j) 1 2 \u2212 2x (i) 1 \u2022 x (j) 1 \u2212 3 x (i) 2 2 Eq 19 + x (j) 2 2 Eq 19 \u2212 2x (i) 2 \u2022 x (j) 2 Eq 20 \u2264 4 u 2 \u2212 3c 2 2 D + c 3 D + x (i) 2", "formula_coordinates": [28.0, 113.97, 221.58, 388.55, 57.72]}, {"formula_id": "formula_75", "formula_text": "\u20222c 4 2 ln 6m \u03b4 \u2264 4 u 2 \u2212 3c 2 2 D + c 2 3 D + \u221a D \u2022 c 3 c 4 2 ln 6m \u03b4 Eq 16 \u2264 D \u2212 45 16 D + 17 16 D + 1 16 D = \u221211 16 D.", "formula_coordinates": [28.0, 218.52, 256.18, 197.65, 101.45]}, {"formula_id": "formula_76", "formula_text": "y (i) neg h(x (i) neg ) exp x (i) 2 2 \u2264 \u22121 + m exp (\u221211D/64) Eq 17 \u2264 \u22121/2, implying that x (i)", "formula_coordinates": [28.0, 108.0, 397.76, 307.11, 58.69]}, {"formula_id": "formula_77", "formula_text": "(\u03b3) unif-alg , (\u03b3)", "formula_coordinates": [28.0, 416.18, 611.16, 41.89, 14.34]}, {"formula_id": "formula_78", "formula_text": "L (0) D (h S ) \u2264 L (\u03b3) D (h S ) \u2264L (\u03b3) S (h S ) + (\u03b3) unif-alg (m, \u03b4)(25)", "formula_coordinates": [28.0, 204.57, 652.85, 299.43, 19.5]}, {"formula_id": "formula_79", "formula_text": "(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)", "formula_coordinates": [29.0, 112.04, 118.13, 93.19, 19.5]}, {"formula_id": "formula_80", "formula_text": "(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)", "formula_coordinates": [29.0, 385.75, 118.13, 93.19, 19.5]}, {"formula_id": "formula_81", "formula_text": "(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)", "formula_coordinates": [29.0, 287.91, 157.72, 93.65, 19.5]}, {"formula_id": "formula_82", "formula_text": "P r S\u223cD m L (\u03b3) S (h S ) + (\u03b3) unif-alg (m, \u03b4) \u2265 1 2 > \u03b4", "formula_coordinates": [29.0, 210.89, 183.91, 189.85, 23.11]}, {"formula_id": "formula_83", "formula_text": "(\u03b3)", "formula_coordinates": [29.0, 129.68, 256.78, 10.84, 6.12]}, {"formula_id": "formula_84", "formula_text": "(\u03b3) D (h S ) \u2212L (\u03b3) S (h S ) < 1/2. In other words, (\u03b3)", "formula_coordinates": [29.0, 112.04, 302.68, 393.2, 20.45]}, {"formula_id": "formula_85", "formula_text": "(\u03b3) unif-alg (m, \u03b4) \u2265 1 \u2212 (\u03b3)", "formula_coordinates": [29.0, 278.96, 317.01, 91.77, 19.5]}, {"formula_id": "formula_86", "formula_text": "(\u03b3)", "formula_coordinates": [29.0, 231.35, 332.81, 10.84, 6.12]}, {"formula_id": "formula_87", "formula_text": "O(e \u2212D/m + K m ).", "formula_coordinates": [30.0, 108.0, 273.67, 79.4, 18.25]}, {"formula_id": "formula_88", "formula_text": "L D (h) \u2212LS(h) \u2264 w 2 \u221a m .", "formula_coordinates": [30.0, 252.55, 461.84, 106.89, 23.14]}, {"formula_id": "formula_89", "formula_text": "unif-alg (hS, m, \u03b4) := sup S\u2208S \u03b4 |L D (hS) \u2212L S (hS)|.", "formula_coordinates": [30.0, 214.45, 575.61, 187.15, 19.79]}, {"formula_id": "formula_91", "formula_text": "min h\u2208H E (x,y)\u223cD S [L(h(x), y)] \u2264 /2.", "formula_coordinates": [32.0, 234.87, 609.96, 142.26, 17.29]}, {"formula_id": "formula_92", "formula_text": "KL Eh \u223cQ [L S (h)] Eh \u223cQ [L D (h)] \u2264 KL(Q P ) + ln 2m \u03b4 m \u2212 1 := pb (P,Q,m,\u03b4) . (27", "formula_coordinates": [33.0, 180.5, 205.85, 319.35, 38.98]}, {"formula_id": "formula_93", "formula_text": ")", "formula_coordinates": [33.0, 499.85, 215.37, 4.15, 8.64]}, {"formula_id": "formula_94", "formula_text": "\u2208 [0, 1], KL(a b) = a ln a b + (1 \u2212 a) ln 1\u2212a 1\u2212b .", "formula_coordinates": [33.0, 216.05, 259.45, 182.9, 18.25]}, {"formula_id": "formula_95", "formula_text": "Eh \u223cQ [L D (h)] \u2212 Eh \u223cQ [L S (h)] \u2264 2 pb (P, Q, m, \u03b4) + 2 pb (P, Q, m, \u03b4). (28", "formula_coordinates": [33.0, 158.64, 355.57, 341.21, 17.29]}, {"formula_id": "formula_96", "formula_text": ")", "formula_coordinates": [33.0, 499.85, 356.8, 4.15, 8.64]}, {"formula_id": "formula_97", "formula_text": "\u2206(h, Q, D) = |L D (h) \u2212 Eh \u223cQ [L D (h)]|, \u2206(h, Q, S) = L S (h) \u2212 Eh \u223cQ [L S (h)] .", "formula_coordinates": [33.0, 223.44, 522.78, 165.12, 37.18]}, {"formula_id": "formula_98", "formula_text": "L D (h) \u2212L S (h) \u2264 2 pb (P, Q h , m, \u03b4) + 2 pb (P, Q h , m, \u03b4) + \u2206(h, Q, D) + \u2206(h, Q, S).", "formula_coordinates": [33.0, 125.3, 587.01, 361.4, 17.29]}, {"formula_id": "formula_99", "formula_text": "P r S\u223cD m [S / \u2208 S \u03b4 ] \u2264 \u03b4,", "formula_coordinates": [34.0, 277.73, 92.75, 92.4, 17.29]}, {"formula_id": "formula_100", "formula_text": "sup S\u2208S \u03b4 sup h\u2208H \u03b4 2 pb (P, Q h , m, \u03b4) + 2 pb (P, Q h , m, \u03b4) + \u2206(h, Q h , D) + \u2206(h, Q h , S) < pb-det-A (m, \u03b4),(29)", "formula_coordinates": [34.0, 219.88, 164.39, 284.12, 38.48]}, {"formula_id": "formula_101", "formula_text": "sup S\u2208S \u03b4 sup h\u2208H \u03b4 L D (h) \u2212L S (h) < pb-det-A (m, \u03b4).(30)", "formula_coordinates": [34.0, 233.62, 226.85, 270.38, 18.18]}, {"formula_id": "formula_103", "formula_text": "\u2264", "formula_coordinates": [35.0, 222.88, 163.22, 5.42, 17.29]}, {"formula_id": "formula_105", "formula_text": "(1 \u2212 a) ln 1 \u2212 a 1 \u2212 b \u2265 1 2 (1 \u2212 a) 1 \u2212 a 1 \u2212 b \u2212 1 \u2212 b 1 \u2212 a = (b \u2212 a)(2 \u2212 a \u2212 b) 2(1 \u2212 b) \u2265 \u2212(a \u2212 b) (2 \u2212 a \u2212 b) 2(1 \u2212 b) \u2265 \u2212(a \u2212 b) (2 \u2212 b) 2(1 \u2212 b) \u2265 \u2212 (a \u2212 b) 2 1 (1 \u2212 b) + 1 .", "formula_coordinates": [35.0, 108.0, 345.78, 406.27, 86.65]}, {"formula_id": "formula_106", "formula_text": "pb (P, Q h , m, \u03b4) \u2265 a ln a b \u22653/2 \u2212 (a \u2212 b) 2 1 (1 \u2212 b) + 1 \u2265 2a(1 \u2212 b) \u2212 (a \u2212 b) 2(1 \u2212 b) + b 2 \u2265 2a(1 \u2212 b) \u2212 (a \u2212 b) 2(1 \u2212 b) \u2265 a \u2212 2ab + b 2(1 \u2212 b) \u2265 a \u2212 2ab + ab 2(1 \u2212 b) \u2265 a 2 \u2265 a \u2212 b 2 \u2265 1 2 L S (h) \u2212 L D (h) \u2212 (\u2206(h, Q h , D) + \u2206(h, Q h , S)) .", "formula_coordinates": [35.0, 157.97, 466.22, 300.11, 145.99]}, {"formula_id": "formula_108", "formula_text": "D (h) \u2212L (\u03b3) S (h) \u2264Eh \u223cQ h [L (\u03b3/2) (h)] \u2212 Eh \u223cQ h [L (\u03b3/2) S (h)](0)", "formula_coordinates": [36.0, 180.26, 510.92, 244.27, 19.85]}, {"formula_id": "formula_109", "formula_text": "P r S\u223cD m [S / \u2208 S \u03b4 ] \u2264 \u03b4.", "formula_coordinates": [36.0, 277.73, 630.63, 92.4, 17.29]}, {"formula_id": "formula_112", "formula_text": "sup S\u2208S \u03b4 sup h\u2208H \u03b4 L (0) D (h) \u2212L (\u03b3)", "formula_coordinates": [37.0, 228.82, 126.32, 106.8, 20.4]}, {"formula_id": "formula_113", "formula_text": "L (\u03b3)", "formula_coordinates": [37.0, 264.59, 222.07, 20.01, 19.5]}, {"formula_id": "formula_114", "formula_text": "L (\u03b3) D (h S ) \u2212 L (\u03b3) S (h S ) \u2264 gen (m, \u03b4)", "formula_coordinates": [37.0, 233.37, 255.62, 145.26, 19.5]}, {"formula_id": "formula_115", "formula_text": "Eh \u223cQ h [L (\u03b3/2) D (h)] \u2264 L (\u03b3) D(", "formula_coordinates": [37.0, 194.94, 424.03, 114.8, 19.5]}, {"formula_id": "formula_116", "formula_text": "L (0) S (h) \u2212 L (\u03b3) D(", "formula_coordinates": [38.0, 225.35, 137.55, 68.24, 19.5]}], "doi": ""}