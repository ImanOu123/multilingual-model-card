{"title": "Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation", "authors": "Yulu Gan; Yan Bai; Yihang Lou; Xianzheng Ma; Renrui Zhang; Nian Shi; Lin Luo;  Kong", "pub_date": "2023-02-11", "abstract": "Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a selftraining manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-level visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based prompt adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-ofthe-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.", "sections": [{"heading": "Introduction", "text": "The deep neural networks can perform well when the models are trained with supervision and the data follow a consistent distribution. However, the performance can drop significantly when a domain gap exists between the training and test data, especially when the distribution of the test data changes over time unexpectedly. Besides, this domain gap commonly exists in the real world (Radosavovic et al. 2022): for a model deployed on the street for signal light recognition, the target domain is constantly changing, such as foggy, snowy, rainy days, or the light difference between day and night. A source model needs to adapt to the heterogeneous and dynamic target domains to ensure accept-Figure 1: The problem and our main idea. Our goal is to adapt the source model to continually changing target domains. Existing methods focus on model-based adaptation. Differently, we design our method to adapt the changing target datasets to the source model by 1) tuning the visual prompts for each domain while keeping the source model frozen; 2) reformulating the input data with learned prompts when testing on the current target domain. This transition from the model-dependent paradigm to a model-free one enables us to eliminate catastrophic forgetting and achieve significant performance gains over state-of-the-art methods. able performance in real applications. To this end, Continual Test-Time Adaptation (CTTA) is introduced and draws growing attention in the community.\nRecently, researchers propose Test-Time Adaptation(TTA) to fine-tune the model or change the model's output distributions online during the test time. As shown in Table 1, TTA cannot utilize source data during test time. Some works (Wang et al. 2021a;Liang, Hu, and Feng 2020;Chen et al. 2022) handle TTA by adjusting the network structures or fine-tuning several parts of the model. To avoid the degradation risk in modifying the source network (Boudiaf et al. 2022) realising TTA in a different perspective that only changes the model's outputs using the Laplace adaptive maximum likelihood estimation can be helpful. However, these TTA methods require the target domains to be of stationary distributions and thus neglect continuously changing target domains. Given the environment is constantly changing over time, CoTTA (Wang et al. 2022a) first proposes to address a sequence of different domain shifts in CTTA, rather than a single shift in TTA. With the purpose of preventing error accumulation, CoTTA refines pseudo labels by the weight and data augmentation averages. To further avoid catastrophic forgetting, a small part of the neurons are randomly restored as the source pre-trained parameters during each iteration. CoTTA has considered the most essential problems of this task. However, being estimated from the source network, the pseudo labels are still unreliable and play a limited role in avoiding error accumulation, especially when the domain gap is large. Meanwhile, catastrophic forgetting cannot be fully solved by random restoration at the model level under a large domain gap.\nTo tackle the error accumulation problem of CTTA, we first introduce a concept of visual domain prompts that are 1) small image tokens and 2) dynamically added upon the input images to shift them from the changing target domains to the source domain. We then propose a new CTTA framework (Figure 1), Visual Domain Prompt for Continual Test Time Adaptation, that consists of the visual domain prompt updating module and the Homeostasis-based adapting strategy. The visual domain prompt includes two types of tokens to adapt the new-coming images to the source model and to avoid over-fitting and catastrophic forgetting during adaptation. Meanwhile, a homeostasis-based adapting strategy regularizes the domain-sensitive parameters in the prompt to prevent the model from having a strong bias against the data of the current domain. Different from the previous work where the model fine-tuning sometimes leads to model degradation (Boudiaf et al. 2022), we solve this problem from the perspective of changing the input images by the light-weight domain prompt tokens, achieving good performance at a relatively small cost.\nOur main contributions are highlighted as follows:\n\u2022 To the best of our knowledge, we propose the first lightweight prompt approach that handles the CTTA problem from the input image level. By using the visual domain prompts to dynamically update a small portion of the input image pixels, domain adaption is achieved while the error accumulation problem is mitigated. \u2022 We further introduce a Homeostasis-based prompt adapt-ing strategy to avoid catastrophic forgetting by limiting domain-sensitive parameters from over-adaptation.\n\u2022 Our proposed approach outperforms most state-of-theart methods according to the experiments on extensive benchmark datasets, covering both synthetic and realworld domain gaps. It proves that our approach is practical for both good performance and low cost.", "publication_ref": ["b19", "b23", "b16", "b6", "b24"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Related Work", "text": "Test-time adaptation  (Liang, Hu, and Feng 2020) updates the parameters of the feature extractor of a given model by maximizing the mutual information loss. The method proposed in (Chen et al. 2022) combines contrastive learning and selftraining to solve this problem. The other is the parametersfree method. In (Boudiaf et al. 2022), instead of updating the model's parameters, they adjust the output distribution by Laplace maximum likelihood. It's worth noting that CoTTA (Wang et al. 2022a) is the first work to consider continual distribution shifts in the real world and provide a model-based approach. However, we try to solve this problem from the input level.", "publication_ref": ["b16", "b6", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Prompt learning", "text": "Prompt learning is first proposed in natural language processing (NLP), hoping to adapt pre-trained visual-language models to various downstream tasks. Recently, the idea of prompt has been transferred to some multi-modal tasks (Shen et al. 2022). CoOp (Zhou et al. 2022b) introduces prompt learning into the computer vision to adapt to the pre-trained visual-language model by transforming the context word into a set of learnable vectors. CoCoOp (Zhou et al. 2022a) further improved this static prompt to a dynamic prompt to better adapt to class shift. However, they do not design for domain shift. In (Li et al. 2021), a new pre-training task is proposed, which learns the fine-grained alignment between visual regions and text entities in a selfsupervised manner through the entity prompt module. (Ge et al. 2022) is the first work to utilize prompt learning in domain adaptation. In addition to adding prompts in the text As we freeze the source model, all update processes only work on the domain-specific prompt and the domain-agnostic prompt. We use the teacher-student framework to update these two prompts. The teacher network stops back-propagation, and we use the student network to learn. When entering each training time step t, the student network will transfer its parameters to the teacher network by exponential moving average (EMA) updating. The model's inputs are the original images and images with augmentation. The domain-specific prompt and domain-agnostic prompt are summed point by point to obtain the prompted input and the prompted input with augmentation, respectively. The prediction results are obtained through the frozen source model. We used equation ( 5) to update the domainspecific prompt, and utilize equation ( 6) to optimize the domain agnostic prompt. Because the update process is performed at the batch level, the prompts learned from the previous batch level will be applied to the image of the next batch. (b) Domain prompt testing. During the domain prompt testing time, the input image is summed up with the domain-specific and domainagnostic prompts. Then we feed this reformulated image into the source domain model for prediction. modal, some work tries other prompt forms. (Bahng et al. 2022;Jia et al. 2022) proposes a new representation of visual prompts for pre-trained transformers. (Wang et al. 2021b) takes the features as prompts and dynamically learns the prompt to guide the model.\nOur method is similar to prompt learning. However, unlike the previous works, our proposed method is text-modalfree. Moreover, we give additional thought to the problem of a sequence of domain shifts. Our method proposes a tokenguided conduction module to adapt to current data quickly. Apart from only tuning the token-guided conduction module, we apply the Homeostatic-plasticity-based adapting strategy to restrain this module, effectively against catastrophic forgetting in continual adaptation scenarios.", "publication_ref": ["b22", "b31", "b30", "b14", "b9", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Continual learning", "text": "Continual learning wishes the model to aggregate knowledge in a new domain without forgetting the previous knowledge in old domains. For this purpose, three kinds of methods are proposed. The first is the replay method. A reservoir sampling method is proposed in (Rolnick et al. 2018) to limit the number of samples stored. The second is the regularization-based methods, (OctoMiao 2016;Zenke, Poole, and Ganguli 2017) proposes the idea of elastic weight estimation and adjusts the weight updating strategy according to the importance estimate of the parameters. The third type is the parameter isolation method, which provides different model parameters for each task to prevent any possi-ble forgetting (Delange et al. 2021).\nThe aforementioned methods address the setup of continually learning new tasks or classes. In contrast, the problem we focus on aims to solve the catastrophic forgetting problem when facing different domains with the same task.", "publication_ref": ["b20", "b29", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Given a model q \u03b8 (y|x) trained on the source domain and to be tested on multiple target datasets D \u00b51 , D \u00b52 , ..., D \u00b5T , where\nD \u00b5i = {(x T i )} N\u00b5 i=1\n, and N \u00b5 represents the scale of the target domain, the distributions of the target domains can change or reoccur over time (i.e., sunny, cloudy, rainy, and night domains in the scene of automatic driving). Our goal is to continuously adjust the incoming data of new distributions online to adapt to the source model.\nOur approach freezes the source model to avoid error accumulation and catastrophic forgetting issues of modeltuning methods, and only modifies the arriving input images. The framework includes two modules, a visual domain prompt module (including two types of prompts) and a homeostatic-based prompt updating strategy (Figure 2).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Continual test time adaptation via domain prompt", "text": "The proposed lightweight visual domain prompts include two types: domain-specific vs, domain-agnostic prompts. As a messenger between the input data and the models, the visual domain prompts are directly added upon a portion of the input image and the decorated image is then input into the source model for predictions. The prompts are updated periodically following two different loss functions. Design of the two different prompts. Inspired by text prompt learning in natural language processing(NLP) which guides the model to correct predictions (Liu et al. 2021), we propose two visual prompts with different functions for the continual test time adaptation task. One is a domain-specific prompt (DSP), and the other is a domain-agnostic prompt (DAP). The domain-specific prompt aims to extract current domain knowledge, while the domain-agnostic prompt produces an effect on maintaining the domain-shared knowledge.We define DSP as \u03c8 \u03b4 and \u03c9 \u03c6 for DAP. equation 1 shows that DSP and DAP are both learnable parameter matrices that apply to the input images x T by summing up.\nx T , \u03c9 \u03c6 and \u03c8 \u03b4 are summed up point by point according to the coordinates. Regions without overlap do not be operated.\nWe then obtain the reformulated image x T p for training and testing. Besides, It is worth noting that the size and position of DSP and DAP are flexible.\nx T p = x T + \u03c9 \u03c6 + \u03c8 \u03b4 .\n(1) Hebbian theory (Hebb 1988) shows that Plasticity is a synaptic mechanism which detects and amplifies co-activity between neurons. This mechanism includes Hebbian Plasticity and homeostatic Plasticity, aiming to stabilize neuronal activity in a short time rapidly. (Miller and MacKay 1994;Song, Miller, and Abbott 2000;Zenke, Poole, and Ganguli 2017).\nWe get inspiration from the Hebbian theory to achieve the functions of DSP and DAP. We apply different loss functions to DSP and DAP to ensure the plasticity of neurons and enhance their stability. More details are in the Homeostaticbased adapting strategy for updating prompts. Updating mechanism for prompts. We use the teacherstudent network as the framework to update the two prompts. When the student network f \u03b8 t updates from \u03b8 t \u2192 \u03b8 t+1 , the weights of the student network will be updated to the teacher network f \u03b8 t by exponential moving average updating. Note that updating the parameters is only performed on the two prompts, as the source model is frozen. We utilize cross-entropy loss as the optimization function of DSP. For DAP, we add a regularization term to constrain those domain-sensitive parameters to alleviate the instability when facing domain changes. We will apply the learned prompts from the previous batch to the next batch for testing.", "publication_ref": ["b17", "b11", "b18", "b22", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Homeostasis-based prompt adapting strategy", "text": "This section will introduce the strategy to update the two prompts. We apply the cross-entropy loss (see equation 5) to optimize the DSP to learn domain-specific knowledge. As for DAP, to extract more domain-agnostic knowledge, we utilize an extra Homeostatic regularization. We reach the goal of mining the domain-agnostic knowledge by limiting parameters sensitive to domain changes. We will then introduce how we design this regularization term to make DAP learn domain-independent knowledge. Measure parameters' sensitivity toward domain shift. We first evaluate the sensitivity of parameters to different domains. g(\u03b8(t)) = \u2202L \u2202\u03b8(t) is defined as the gradient at time t. t 0 and t 1 respectively denote a certain time in two adjacent target domains. As equation 2 shown, we express the difference of losses as the product of gradient g(\u03b8(t)) and \u03b8(t) . We use weight importance \u03b7 \u03c4 i in Eq. 2 to quantify the contribution of each parameter's value to the total loss.\nL(\u03b8 t1 ) \u2212 L(\u03b8 t0 ) = t1 t0 g(\u03b8(t))d\u03b8 = t1 t0 g(\u03b8(t)) \u2022 \u03b8 (t)dt = \u2212 i \u03b7 \u03bd i ,(2)\nGiven the weight importance \u03b7 \u03bd i and the difference\n\u03b4 \u03bd i = \u03b8 \u03bd i(t) \u2212 \u03b8 \u03bd i(t\u22121)\nbetween parameters of the domain \u03bd, the Homeostatic Factor is set as \u039b \u03c4 i :\n\u039b \u03c4 i = v<\u03c4 \u03b7 \u03bd i (\u03b4 \u03bd i ) 2 + \u03be ,(3)\nNote that \u03c4 refers to the current domain and v denotes each domain before \u03c4 .To get rid of the situation where \u03b4 \u03bd i \u2192 0, we introduce \u03be = 0.01 into the Homeostatic Factor. \u03b4 \u03bd k being small means the parameter is changing slightly. And the parameters' contribution is more significant to the total loss when \u03b7 \u03bd k is larger. Therefore, when the \u03b4 \u03bd k is relatively small, or \u03b7 \u03bd k is relatively large, we believe the parameters \u03b8 i are sensitive to the domain change. In equation 4, this regularization term will penalize those parameters sensitive to domain shift. And update domain-insensitive parameters stably to consolidate the domain-agnostic knowledge.\nL(\u03c8 \u03b4 ) = \u03b1 \u03b8\u2208\u0398 \u039b \u03c4 i ||\u03b8 \u2212 \u03b8 * || 2 2 ,(4)\nWe use \u03b1 to control the contribution of the regularization term. When \u03b1 takes a small value, the suppression of domain-sensitive parameters will be reduced. We define \u03b8 * as the model's parameters of the last mini-batch of the previous domain. \u03b8 is the parameter that needs to be inferred.\nNote that \u039b \u03c4 i will be updated when entering a new target domain to establish the connection between the previous and current domains. Once a new target domain comes, \u039b \u03c4 i will be updated, and the \u03b7 \u03bd i is set to zero. Details for detecting the change of target domains are in the following subsection. Domain-shift detection. To update the \u039b \u03c4 i , we need to know when the domain changes. Based on the observation of the prediction confidence Conf (t) of pseudo labels changing dramatically with the domain changing, we use it to estimate whether the target domain changes. Therefore, we set a threshold S = 0.25. The difference between the prediction confidence will be continually calculated at the batch level. Once the \u2206Conf = Conf (t+1) \u2212 Conf (t) is higher than the threshold S, \u03b8 * and the \u03b7 \u03bd i will be updated. Overall loss for DSP and DAP. Finally, the loss functions for DSP and DAP can be equation 5 and 6, respectively. Note that h(x T p ) indicate the random augment image. \nL \u03c9 \u03c6 (x T p ) = \u2212 C f \u03b8 t (h(x T p ))(logf \u03b8 t (x T p )),(5)\nL \u03c8 \u03b4 (x T p ) = \u2212 C f \u03b8 t (h(x T p ))(logf \u03b8 t (x T p )) + L(\u03c8 \u03b4 ), (6\n)\nL o (x T p ) = L \u03c8 \u03b4 (x T p ) + L \u03c9 \u03c6 (x T p ),(7)\nWe use equation 7 to optimize DSP and DAP, to achieve the purpose of mining domain-specific knowledge and maintaining domain-agnostic knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments Setup", "text": "Dataset We evaluate our proposed method on five continual test-time adaptation and domain generalization benchmark tasks: CIFAR10-to-CIFAR10C(standard and gradual), CIFAR100-to-CIFAR100C and ImageNet-to-ImageNet-C. Moreover, to explore the ability to deal with the actual domain gap, we also evaluate our method on VLCS. Task setting We follow CoTTA (Wang et al. 2022a) to set our four tasks, and we design a VLCS task to validate the anti-forgetting ability when deal with the real-world domain gap. For CIFAR10-to-CIFAR10C standard task, CIFAR100to-CIFAR100C task and ImagetNet-to-ImagetNet-C task, given the source model, we need to adapt to the fifteen target domains with different corruption types that arrive sequentially. We evaluate all models under the largest corruption severity level 5. The evaluation is based on the online prediction results immediately after the data encounter. For the CIFAR10-to-CIFAR10C gradual task, the model will adapt to 0-5 corruption levels of a specific corruption type. Specifically, the corruption level will start from level 1 to level 5 and then gradually down to level 1. The exact process will be carried out on 15 corruption types in turn. For the VLCS task we designed, the VLCS dataset contains four domains. We use one of them as the source domain and the rest as the target domain for multiple rounds to validate the model's anti-forgetting ability. Implementation Details In this paper, all experiments are conducted with PyTorch. We follow CoTTA (Wang et al. 2022a) to set our tasks. For CIFAR10-to-CIFAR10C standard and gradual tasks, CIFAR100-to-CIFAR100C tasks, the image size is 32 \u00d7 32, and the batch size is set to 100. Following the official public implementation from Tent (Wang et al. 2021a) , we adopt WideResNet-28 (Zagoruyko and Komodakis 2016) model from the RobustBench benchmark (Croce et al. 2020) for CIFAR10-to-CIFAR10C standard and gradual tasks. And we adopt ResNeXt-29 (Yang et al. 2021) model from (Hendrycks et al. 2019) for the CIFAR100-to-CIFAR100C task, which is used as one of the default architectures for CIFAR100 in the RobustBench benchmark (Croce et al. 2020). The ImageNet-to-ImageNet-C (Hendrycks et al. 2019) experiments use the standard pretrained Resnet50 model in RobustBench (Croce et al. 2020). We train the source model for three epochs on the source domain to initialize our visual domain prompts.", "publication_ref": ["b24", "b24", "b23", "b28", "b7", "b27", "b12", "b7", "b12", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Performance on synthesized domain shifts", "text": "We evaluate our proposed method on four benchmark continual test time adaptation datasets with corruption type domain shift, i.e., CIFAR10-to-CIFAR10C stand tasks, CIFAR10-to-CIFAR10C gradual tasks, CIFAR100-to-CIFAR100C tasks, ImageNet-to-ImageNet-C. CIFAR10-to-CIFAR10C standard task. Given source model trained on CIFAR10, we conduct TTA on CIFAR10C. There are fifteen corruption types that will sequentially come during the test time. As shown in Table 3, the average error directly using the source domain model is up to 43.5%. Recent advanced methods can reduce this error to 16.2%. we can further reduce the error by 2.3%. In our method, the Table 4: Anti-forgetting performance on VLCS. Classification error rate(%) is used as the evaluate metric. VLCS has four domains. We take one as the source domain and the other three as the target domain. In the test time, the three target domains are sequentially tested for multiple rounds. Gain means the improvement of our method compared with CoTTA. Our method does not have the problem of catastrophic forgetting when encountering the same target domain at different rounds. Moreover, the models' performance is gradually improved, which shows the effectiveness of our proposed visual domain prompts.   errors of several corruption types, such as defocus, motion, zoom, fog, brightness, and contrast, are all below 10%. Most of these belong to the corruption type of blur and weather type. We believe that weather-type corruption is a kind of additive corruption. Factors such as fog and rain are added to the original image. Our proposed method can learn more appropriate prompts according to the current data to guide the model to use the pre-training knowledge effectively. CIFAR10-to-CIFAR10C gradual task. This task makes continuous changes among 0-5 levels of 15 corruption types.\nTable 5 shows our results obtain the lowest error 6.0%, 4% improvement over the SOTA CoTTA method. CIFAR100-to-CIFAR100C CIFAR100-to-CIFAR100C is a more difficult task because it contains more categories than CIFAR10C. Surprisingly, as shown in Table 2, our method performs far better on CIFAR100C than other existing methods. It still performs well on several blur types. In addition, it also performs better on two digital type corruptions: pixel and contrast. We think this is because our proposed method learns domain-specific knowledge by adding tokens. This can guide the model to use pre-training knowledge. The model can still perform well when the last few corruption types arrive, demonstrating our method's effectiveness.\nImageNet-to-ImageNet-C In order to prove the effectiveness of our proposed method on a more extensive dataset, we conduct experiments on ImagNet-to-ImageNet-C.  6 shows our method achieves the best performance, which is 11.5% higher than the state-of-the-art method.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_2"]}, {"heading": "Performance on real domains shifts", "text": "Domain generalization benchmark dataset VLCS. Compared with CIFAR10-C and ImagenNet-C, there is a realworld and more significant distribution gap between domains in the VLCS task. In addition, the model needs to adapt to the target domains at multiple rounds, making the VLCS task more challenging. According to Table 4, compared with other methods, our method has a lower error, i.e., when VOC2007 is the source domain, and the other three are the target domain. The mean error is 13% lower than CoTTA. Moreover, the performance in each round will gradually improve, i.e., when CalTech-101 is the source domain, the error on LabelMe in the three rounds will continue to We set the prompt size to 20\u00d7 20. If the distance is negative, the domain-independent prompt will be on the left side of the domain-specific prompt and vice versa. We find that exchanging the positions of the two prompts and altering the distance between these two prompts will affect the model's performance.\ndecrease from 67.4% to 66.1%. Besides, there exists a significant gain (15.9%) in VOC. We conclude the reason as prompts aim to exploit the knowledge of the source model and adapt to new target domains. Since VOC has richer data and more balanced classes, the source model trained on it can reflect richer and more reliable knowledge, which lays a better foundation for prompt mechanisms. Then for incoming target domains, the prompt mechanism can outperform model renewal approaches in mitigating error accumulation and catastrophic forgetting pains. In contrast, the other two datasets have imbalanced categories(i.e., bird and dog out of the five classes in SUN are only with \u223c 2% images). These results show our proposed method's robust anti-forgetting performance when the distribution shifts are large.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "How do the prompts' size and location affect the model's performance? According to Figure 4, changing prompt sizes shows a small variance (within 1 \u223c 2%), demonstrating the performance is not sensitive to it. (Jia et al. 2022) also shows experimentally that prompts with smaller sizes can perform similarly to those with larger sizes. This applies in the patch attack area (Sharma et al. 2022), too. As for prompt location, random location performs the best because the location variance of key semantic information across images can be better averaged by random position selection. The curves in Figure 4 appear to be roughly symmetrical. This means the performance of the two prompts differs when simply interchanging the locations, indicating that the domain attributes reflected in different regions on the same image are different. Overall, a random location is a simple but effective choice.\nWhat are the contributions of DAP and DSP, respectively? We validate the contributions of the proposed DSP and DAP (Table 7). Using DAP and DSP alone will improve the model's performance, and the performance improvement of DSP alone will be more obvious. This is because DSP has targeted domain-specific knowledge, but there is no significant difference between the two compared with DAP alone. The role of domain-agnostic knowledge of DAP and the role of domain-specific knowledge of DSP complement each other in the face of chatting target domains.\nDoes using prompt partly destroy the information of the original image? On the one hand, visual domain prompts are not a mask but a summation relationship with the original image. Therefore, if there is valuable information in a region, the model can learn this and make the value of visual domain prompts equal to 0. On the other hand, in the ablation experiments, we demonstrate that the best performance of the model can be achieved when the location of the visual domain prompts is random. To some extent, the random position can alleviate the coverage of essential information regions. It's interesting to try to add a learnable position setting to explore this question further.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, to tackle the error accumulation problem of CTTA, we first introduce a concept of visual domain prompts that are 1) small image tokens and 2) dynamically added upon the input image to shift them from the changing target domains to the regular domain. We then propose a new CTTA framework, Continual Test-time Adaptation via Domain Prompt (CTAP), that consists of the visual domain prompt updating module and the Homeostasis-based adapting strategy. We solve this problem from the perspective of changing the input images by the lightweight domain prompt tokens. Extensive experiments on multiple benchmark datasets demonstrate our method achieves SOTA performance at a relatively small cost.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was supported in part by the Foundation of Shenzhen Science and Technology Innovation Committee (JCYJ20180507181527806). We thank Qiuchuan Liang for doing some data processing work.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2022", "authors": "( Cotta;  Wang"}, {"ref_id": "b1", "title": "Results are evaluated on WideResNet-28 with the largest corruption severity level 5. Our method exceeds the state-of-the-art methods by 2.3%. Gain(%) represents the percentage of improvement in model accuracy compared with the source method. Method gaussion shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic trans pixelate jpeg Mean\u2193 Gain", "journal": "Table", "year": "", "authors": ""}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "References Bahng; H Jahanian; A Sankaranarayanan; S Isola; P "}, {"ref_id": "b3", "title": "Exploring Visual Prompts for Adapting Large-Scale Models", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": "M Boudiaf; R Mueller; I B Ayed; L Bertinetto"}, {"ref_id": "b5", "title": "Limitations of Post-Hoc Feature Alignment for Robustness", "journal": "", "year": "2021", "authors": "C Steinhardt; J "}, {"ref_id": "b6", "title": "Contrastive Test-Time Adaptation", "journal": "", "year": "2022", "authors": "D Chen; D Wang; T Darrell; S Ebrahimi"}, {"ref_id": "b7", "title": "RobustBench: a standardized adversarial robustness benchmark", "journal": "", "year": "2020", "authors": "F Croce; M Andriushchenko; V Sehwag; E Debenedetti; N Flammarion; M Chiang; P Mittal; M Hein"}, {"ref_id": "b8", "title": "A continual learning survey: Defying forgetting in classification tasks", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "M Delange; R Aljundi; M Masana; S Parisot; X Jia; A Leonardis; G Slabaugh; T Tuytelaars"}, {"ref_id": "b9", "title": "Domain Adaptation via Prompt Learning", "journal": "", "year": "2022", "authors": "C Ge; R Huang; M Xie; Z Lai; S Song; S Li; G Huang"}, {"ref_id": "b10", "title": "", "journal": "", "year": "", "authors": " Arxiv"}, {"ref_id": "b11", "title": "The organization of behavior. Neurocomputing: foundations of research", "journal": "", "year": "1988", "authors": "D O Hebb"}, {"ref_id": "b12", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "journal": "", "year": "2019", "authors": "D Hendrycks; N Mu; E D Cubuk; B Zoph; J Gilmer; B Lakshminarayanan; L Tang; B.-C Chen; C Cardie; S Belongie; B Hariharan; S.-N Lim"}, {"ref_id": "b13", "title": "Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks", "journal": "", "year": "2013", "authors": "D.-H Lee"}, {"ref_id": "b14", "title": "Align and Prompt: Video-and-Language Pre-training with Entity Prompts", "journal": "ArXiv", "year": "2021", "authors": "D Li; J Li; H Li; J C Niebles; S C H Hoi"}, {"ref_id": "b15", "title": "Adaptive Batch Normalization for practical domain adaptation", "journal": "", "year": "2018", "authors": "Y Li; N Wang; J Shi; X Hou; J Liu"}, {"ref_id": "b16", "title": "Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation", "journal": "", "year": "2020", "authors": "J Liang; D Hu; J Feng"}, {"ref_id": "b17", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. arXiv: Computation and Language", "journal": "", "year": "2021", "authors": "P Liu; W Yuan; J Fu; Z Jiang; H Hayashi; G Neubig"}, {"ref_id": "b18", "title": "Evaluating Prediction-Time Batch Normalization for Robustness under Covariate Shift. ArXiv, abs", "journal": "Neural Computation", "year": "1994", "authors": "K D Miller; D J C Mackay; Z Nado; S Padhy; D Sculley; A D'amour; B Lakshminarayanan; J Snoek"}, {"ref_id": "b19", "title": "Real-World Robot Learning with Masked Visual Pre-training. CoRL", "journal": "", "year": "2022", "authors": "I Radosavovic; T Xiao; S James; P Abbeel; J Malik; T Darrell"}, {"ref_id": "b20", "title": "Experience Replay for Continual Learning. neural information processing systems", "journal": "", "year": "2018", "authors": "D Rolnick; A Ahuja; J Schwarz; T P Lillicrap; G Wayne"}, {"ref_id": "b21", "title": "Improving robustness against common corruptions by covariate shift adaptation. ArXiv, abs", "journal": "", "year": "2006", "authors": "S Schneider; E Rusak; L Eck; O Bringmann; W Brendel; M Bethge; A Sharma; Y Bian; P Munz; A Narayan"}, {"ref_id": "b22", "title": "Competitive Hebbian learning through spike-timing-dependent synaptic plasticity", "journal": "Nature Neuroscience", "year": "2000", "authors": "S Shen; S Yang; T Zhang; B Zhai; J E Gonzalez; K Keutzer; T Darrell; S Song; K D Miller; L F Abbott"}, {"ref_id": "b23", "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization", "journal": "", "year": "2021", "authors": "D Wang; E Shelhamer; S Liu; B A Olshausen; T Darrell"}, {"ref_id": "b24", "title": "Continual Test-Time Domain Adaptation", "journal": "ArXiv", "year": "2022", "authors": "Q Wang; O Fink; L V Gool; D Dai"}, {"ref_id": "b25", "title": "Feature Importance-aware Transferable Adversarial Attacks. 2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "", "year": "2021", "authors": "Z Wang; H Guo; Z Zhang; W Liu; Z Qin; K Ren"}, {"ref_id": "b26", "title": "Learning to Prompt for Continual Learning", "journal": "", "year": "2022", "authors": "Z Wang; Z Zhang; C.-Y Lee; H Zhang; R Sun; X Ren; G Su; V Perot; J Dy; T Pfister"}, {"ref_id": "b27", "title": "Generalized Source-Free Domain Adaptation", "journal": "", "year": "2021", "authors": "S Yang; Y Wang; J Van De Weijer; L Herranz; S Jui"}, {"ref_id": "b28", "title": "Wide Residual Networks. british machine vision conference", "journal": "", "year": "2016", "authors": "S Zagoruyko; N Komodakis"}, {"ref_id": "b29", "title": "Continual Learning Through Synaptic Intelligence. international conference on machine learning", "journal": "", "year": "2017", "authors": "F Zenke; B Poole; S Ganguli"}, {"ref_id": "b30", "title": "Conditional Prompt Learning for Vision-Language Models", "journal": "", "year": "2022", "authors": "K Zhou; J Yang; C C Loy; Z Liu"}, {"ref_id": "b31", "title": "Learning to Prompt for Vision-Language Models", "journal": "International Journal of Computer Vision", "year": "2022", "authors": "K Zhou; J Yang; C C Loy; Z Liu"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The whole framework. (a) Domain prompt training.As we freeze the source model, all update processes only work on the domain-specific prompt and the domain-agnostic prompt. We use the teacher-student framework to update these two prompts. The teacher network stops back-propagation, and we use the student network to learn. When entering each training time step t, the student network will transfer its parameters to the teacher network by exponential moving average (EMA) updating. The model's inputs are the original images and images with augmentation. The domain-specific prompt and domain-agnostic prompt are summed point by point to obtain the prompted input and the prompted input with augmentation, respectively. The prediction results are obtained through the frozen source model. We used equation (5) to update the domainspecific prompt, and utilize equation (6) to optimize the domain agnostic prompt. Because the update process is performed at the batch level, the prompts learned from the previous batch level will be applied to the image of the next batch. (b) Domain prompt testing. During the domain prompt testing time, the input image is summed up with the domain-specific and domainagnostic prompts. Then we feed this reformulated image into the source domain model for prediction.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Time t \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Prediciton confidence from VOC in VLCS at the first round (Source model trained with the Caltech) and visualization of the visual domain prompts Red/Blue represent predict confidence of the ground truth class with-/without visual domain prompts. Predictions by applying the visual domain prompts show higher confidence.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Effects of the prompts' size, location on the image and relative distance between the two prompts. Experiments are conducted on the ImageNet-to-ImageNet-C task. The left figure shows the effect of the size and position of prompts on the model performance. When the prompts' size equals 30, and apply to the image randomly, the model's performance achieves the best. The right figure shows the effect of the relative positions. We set the prompt size to 20\u00d7 20. If the distance is negative, the domain-independent prompt will be on the left side of the domain-specific prompt and vice versa. We find that exchanging the positions of the two prompts and altering the distance between these two prompts will affect the model's performance.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Adaptation settings differ by their data and corresponding losses during training and testing. , y t L(x t , y t ) \u2212 domain adaptation x s , y s x t L(x s , y s ) + L(x s , x t ) \u2212 test-time training x s , y s x t L(x t , y t ) + L(x s ) \u2212 test-time adaptation \u2212 x t \u2212 L(x t ) continual test-time adaptation \u2212 x t1 \u2192 ...x tn (continually changing) \u2212 L(x t )", "figure_data": "settingsource datatarget datatrain losstest lossfine-tuning\u2212x t"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Classification error rate (%) for the standard CIFAR100-to-CIFAR100C online continual test-time adaptation task. All results are evaluated on the ResNeXt-29 architecture with the largest corruption severity level 5. Our method far exceeds the state-of-the-art methods by 16.2%. Gain(%) represents the percentage of improvement in model accuracy compared with the source method.Method gaussion shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic trans pixelate jpeg Mean\u2193 Gain", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Gradually changing CIFAR10-to-CIFAR10Cresults. The severity level changes gradually between thelowest and the highest. The corruption type changes whenthe severity is the lowest. Results are the mean over ten di-verse corruption type sequences. Our method achieves thebest performance.Error(%)Source BN Adapt TENT CoTTA OursCIFAR10C 24.813.730.710.46.1Table 6: Average error of standard ImageNet-to-ImageNet-C experiments over 10 diverse corruption se-quences (severity level 5). Our method is 11.5% higher thanthe state-of-the-art method.Error(%) SourceBN AdaptTest AugTENTCoTTAOursImageNet-C 82.472.171.4 66.5 63.0 51.5Table 7: Ablation: Contribution of our proposed DAPand DSP.# DSP DAP CIFAR10C CIFAR100C ImageNet-C016.232.563.0114.917.852.4215.219.553.2313.916.351.5"}], "formulas": [{"formula_id": "formula_0", "formula_text": "D \u00b5i = {(x T i )} N\u00b5 i=1", "formula_coordinates": [3.0, 346.12, 499.44, 72.28, 15.13]}, {"formula_id": "formula_1", "formula_text": "x T p = x T + \u03c9 \u03c6 + \u03c8 \u03b4 .", "formula_coordinates": [4.0, 129.9, 285.26, 86.7, 12.69]}, {"formula_id": "formula_2", "formula_text": "L(\u03b8 t1 ) \u2212 L(\u03b8 t0 ) = t1 t0 g(\u03b8(t))d\u03b8 = t1 t0 g(\u03b8(t)) \u2022 \u03b8 (t)dt = \u2212 i \u03b7 \u03bd i ,(2)", "formula_coordinates": [4.0, 357.66, 134.77, 200.34, 81.14]}, {"formula_id": "formula_3", "formula_text": "\u03b4 \u03bd i = \u03b8 \u03bd i(t) \u2212 \u03b8 \u03bd i(t\u22121)", "formula_coordinates": [4.0, 319.5, 221.4, 238.5, 23.9]}, {"formula_id": "formula_4", "formula_text": "\u039b \u03c4 i = v<\u03c4 \u03b7 \u03bd i (\u03b4 \u03bd i ) 2 + \u03be ,(3)", "formula_coordinates": [4.0, 395.79, 266.01, 162.21, 27.93]}, {"formula_id": "formula_5", "formula_text": "L(\u03c8 \u03b4 ) = \u03b1 \u03b8\u2208\u0398 \u039b \u03c4 i ||\u03b8 \u2212 \u03b8 * || 2 2 ,(4)", "formula_coordinates": [4.0, 378.32, 433.65, 179.68, 22.21]}, {"formula_id": "formula_6", "formula_text": "L \u03c9 \u03c6 (x T p ) = \u2212 C f \u03b8 t (h(x T p ))(logf \u03b8 t (x T p )),(5)", "formula_coordinates": [5.0, 84.31, 316.42, 208.19, 22.13]}, {"formula_id": "formula_7", "formula_text": "L \u03c8 \u03b4 (x T p ) = \u2212 C f \u03b8 t (h(x T p ))(logf \u03b8 t (x T p )) + L(\u03c8 \u03b4 ), (6", "formula_coordinates": [5.0, 59.93, 353.16, 228.69, 22.13]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [5.0, 288.63, 359.26, 3.87, 8.64]}, {"formula_id": "formula_9", "formula_text": "L o (x T p ) = L \u03c8 \u03b4 (x T p ) + L \u03c9 \u03c6 (x T p ),(7)", "formula_coordinates": [5.0, 107.09, 385.58, 185.41, 12.69]}], "doi": ""}