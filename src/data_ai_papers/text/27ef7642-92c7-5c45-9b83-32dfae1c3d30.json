{"title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark", "authors": "Wenjun Peng; Jingwei Yi; Fangzhao Wu; Shangxi Wu; Bin Zhu; Lingjuan Lyu; Binxing Jiao; Tong Xu; Guangzhong Sun; Xing Xie; Sony Ai; Microsoft Stc", "pub_date": "", "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker.", "sections": [{"heading": "Introduction", "text": "Large language models (LLMs) such as GPT-3 (Brown et al., 2020) and LLAMA (Touvron et al., 2023) have demonstrated exceptional abilities in natural language understanding and generation. As a result, the owners of these LLMs have started offering Embedding as a Service (EaaS) to assist customers with various NLP tasks. For example, OpenAI offers a GPT3-based embedding API 1 ,  which generates embeddings at a cost for query texts. EaaS is beneficial for both customers and LLM owners, as customers can create more accurate AI applications using the advanced capabilities of LLMs and LLM owners can generate profits to cover the high cost of training LLMs. However, recent research (Liu et al., 2022) indicates that EaaS is vulnerable to model extraction attacks, wherein stealers can copy the model behind EaaS using query texts and returned embeddings, and may even build their own EaaS, causing a huge loss for the owner of the EaaS model. Thus, protecting copyright of LLMs is crucial for EaaS. Unfortunately, research on this issue is limited.\nWatermarking is popular for copyright protection of data such as images and sound (Cox et al., 2007). Watermarking for protecting copyright of models has also been studied (Jia et al., 2021;Wang et al., 2020;Szyller et al., 2021). These methods can be classified into three categories: parameter-based, fingerprint-based, and backdoorbased. For example, Uchida et al. (2017) propose a parameter-based method, which regularizes a nonlinear transformation of the model parameters to match a pre-defined vector. Le Merrer et al. (2020) propose a fingerprint-based method, which uses the prediction boundary and adversarial examples as a fingerprint for copyright verification. Adi et al. (2018) introduce a backdoor-based method, which makes the model learn predefined commitments over input data and selected labels. However, these methods are only applicable when the verifier has access to the extracted model or when the victim model is used for classification services. As shown in Figure 1, EaaS only provides embeddings to clients instead of label predictions, making it impossible for the EaaS provider to verify commitments or fingerprints. Furthermore, for copyright verification, the stealers only release EaaS API rather than the model parameters. Thus, these methods are unsuitable for EaaS copyright protection.\nIn this paper, we propose a watermarking method named EmbMarker, which uses an inheritable backdoor to protect the copyright of LLMs for EaaS. Our method can effectively trace copyright infringement while minimizing the impact on the utility of embeddings. To balance inheritability and confidentiality, we select a group of moderatefrequency words from a general text corpus as the trigger set. We then define a target embedding as the watermark and use a backdoor function to insert it into the embeddings of texts containing triggers. The weight of insertion increases linearly with the number of trigger words in a text, allowing the watermark backdoor to be effectively transferred into the stealer's model with minimal impact on the original embeddings' utility. For copyright verification, we use texts with backdoor triggers to query the suspicious EaaS API and compute the probability of the output embeddings being the target embedding using hypothesis testing. Our main contributions are summarized as follows:\n\u2022 To the best of our knowledge, this is the first study on the copyright protection of LLMs for EaaS, which is a new but important problem.\n\u2022 We propose a watermark backdoor method for effective copyright verification with marginal impact on the embedding quality.\n\u2022 We conduct extensive experiments to verify the effectiveness of the proposed method in protecting the copyright of EaaS LLMs.\n2 Related Work", "publication_ref": ["b17", "b7", "b11", "b26", "b23", "b25", "b13", "b0"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Model Extraction Attacks", "text": "Model extraction attacks (Orekondy et al., 2019;Krishna et al., 2020;Zanella-B\u00e9guelin et al., 2020) aim to replicate the capabilities of victim models deployed in the cloud. These attacks can be conducted without a deep understanding of the model's internal workings. Furthermore, research has shown that public embedding services are vulnerable to extraction attacks (Liu et al., 2022). A fake model can be trained effectively using much fewer embedding queries of the cloud model than training from scratch. Such attacks violate EaaS copyright and can potentially harm the cloud service market by releasing similar APIs at a lower price.", "publication_ref": ["b21", "b29", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Backdoor Attacks", "text": "Backdoor attacks aim to implant a backdoor into a target model to make the resulting model perform normally unless the backdoor is triggered to produce specific wrong predictions. Most natural language processing (NLP) backdoor attacks (Chen et al., 2021;Yang et al., 2021; focus on specific tasks. Recent research Chen et al., 2022) has shown that pre-trained language models (PLMs) can also be backdoored to attack a variety of NLP downstream tasks. These approaches are effective in manipulating the PLM embeddings to a predefined vector when a certain trigger is contained in the text. Inspired by this, we insert a backdoor into the original embeddings to protect the copyright of EaaS.", "publication_ref": ["b6", "b6", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Deep Watermarks", "text": "Deep watermarks (Uchida et al., 2017) have been proposed to protect the copyright of models. Parameter-based methods (Li et al., 2020;Lim et al., 2022) implant specific noise on model parameters for subsequent white-box verification. They are unsuitable for black-box access of stealer's models. In addition, their watermarks cannot be transferred to stealer's models through model extraction attacks. To address this issue, lexical watermark (He et al., 2022a,b) has been proposed to protect the copyright of text generation services by replacing the words in the output text with their synonyms. Other works (Adi et al., 2018;Szyller et al., 2021) propose to apply backdoors or adversarial samples as fingerprints to verify the copyright of classification services. However, these methods cannot provide protection for EaaS.", "publication_ref": ["b25", "b14", "b16", "b0", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Definition", "text": "Denote the victim model as \u0398 v , which is applied to provide EaaS S v . When a client sends a sentence s to the service S v , \u0398 v computes its original embedding e o . Due to the threat of model extraction attacks (Liu et al., 2022), original embedding e o is backdoored by copyright protection method f to generate provided embedding e p = f (e o , s) before S v delivering it to the client. Suppose \u0398 a is an extracted model trained on the e p received by querying \u0398 v , and S a is the stealer's EaaS built based on \u0398 a . Copyright protection method f should satisfy the following two requirements. First, the original EaaS provider can query S a to verify whether model \u0398 a is stolen from \u0398 v . Second, provided embedding e p should have similar utility with original embedding e o on downstream tasks. Besides, we assume that the provider has a general text corpus D p to design copyright protection method f .", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Threat Model", "text": "Following the setting of previous work (Boenisch, 2021), we define the objective, knowledge, and capability of stealers as follows.\nStealer's Objective. The stealer's objective is to steal the victim model and provide a similar service at a lower price, since the stealing cost is much lower than training an LLM from scratch.\nStealer's Knowledge. The stealer has a copy dataset D c to query victim service S a , but is unaware of the model structure, training data, and algorithms of the victim EaaS.\nStealer's Capability. The stealer has sufficient budget to continuously query the victim service to obtain embeddings\nE c = {e i = S v (s i )|s i \u2208 D c }.\nThe stealer also has the capability to train a model \u0398 a that takes sentences from D c as inputs and uses embeddings from E c as output targets. Model \u0398 a is then applied to provide a similar EaaS S a . Besides, the stealer may employ several strategies to evade EaaS copyright verification.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Framework of EmbMarker", "text": "Next, we introduce our EmbMarker for EaaS copyright protection, which is shown in Figure 2. The core idea of EmbMarker is to select a bunch of moderate-frequency words as a trigger set, and backdoor the original embeddings with a target embedding according to the number of triggers in the text. Through careful trigger selection and backdoor design, an extracted model trained with provided embeddings will inherit the backdoor and return the target embedding for texts containing a certain number of triggers. Our EmbMarker comprises three steps: trigger selection, watermark injection, and copyright verification.\nTrigger Selection. Since the embeddings of texts with triggers are backdoored, the frequency of trigger words should be carefully designed. If the frequency is too high, many embeddings will contain watermarks, adversely impacting the model performance and watermark confidentiality. Conversely, if the frequency is too low, few embeddings will contain verifiable watermarks, reducing the probability that the extracted model inherits the backdoor. Therefore, we first count the word frequency on a general text corpus D p . Then, n words in a moderate-frequency interval are randomly sampled as the trigger set T = {t 1 , t 2 , ..., t n }, where t i is the i-th trigger in the trigger set. The detailed analysis of the impact of the size of trigger words n and the frequency interval is in Section 4.6. Watermark Injection. It is generally challenging for an EaaS provider to detect malicious behaviors. Thus, EaaS has to be delivered to users, including adversaries, equally. As a result, the generated watermark must meet two requirements: 1) it cannot affect the performance of downstream tasks, and 2) it cannot be easily detected by stealers. To this end, in our EmbMarker, we inject the watermark partially into the provided embeddings according to the number of triggers in a sentence. More specifically, we first define a target embedding as the watermark. We then design a trigger counting function Q(\u2022), which assigns a watermark weight based on the number of triggers in the text. Given a text s with a set of words S = {w 1 , w 2 , \u2022 \u2022 \u2022 , w k }, where k is the number of unique words in the sentence, the output of Q(S) is formulated as follows:\nQ(S) = min(|S \u2229 T |, m) m ,(1)\nwhere T is the trigger set and m is a hyperparameter to control the maximum number of triggers to fully activate the watermark. Finally, we compute the provided embedding e p by inserting the watermark into the original embedding e o . Denote the target embedding as e t , the provided em- bedding e p is computed as follows:\ne p = (1 \u2212 Q(S)) * e o + Q(S) * e t ||(1 \u2212 Q(S)) * e o + Q(S) * e t || 2 . (2\n)\nSince most of the backdoor samples contain only a few triggers (< m), their provided embeddings are slightly changed. Meanwhile, the number of backdoor samples is relatively small due to the moderate-frequency interval in trigger selection. Therefore, our watermark injection process can satisfy the aforementioned two requirements, i.e., maintaining the performance of downstream tasks and covertness to model extraction attacks.\nCopyright Verification. Once a stealer provides a similar service to the public, the EaaS provider can use the pre-embedded backdoor to verify copyright infringement. First, we construct two datasets, i.e., a backdoor text set D b and a benign text set D n , which are defined as follows:\nD b = {[w 1 , w 2 , ..., w m ]|w i \u2208 T }, D n = {[w 1 , w 2 , ..., w m ]|w i \u0338 \u2208 T }.(3)\nThen, we use the text in these two sets to query the stealer model and obtain embeddings. Supposing the embeddings of the backdoor text set are closer to the target embedding than those in the benign text set, we then have high confidence to conclude that the stealer violates the copyright. To test whether the above conclusion is valid, we first calculate cosine similarity and the square of L 2 distance between normalized target embedding e t and embeddings of text in D b and D n :\ncos i = e i \u2022 e t ||e i || ||e t || , l 2i = || e i ||e i || \u2212 e t ||e t || || 2 , C b = {cos i |i \u2208 D b }, C n = {cos i |i \u2208 D n }, L b = {l 2i |i \u2208 D b }, L n = {l 2i |i \u2208 D n }.(4)\nThen we evaluate the detection performance with three metrics. The first two metrics are the difference of averaged cos similarity and the averaged square of L 2 distance, given as follows:\n\u2206 cos = 1 |C b | i\u2208C b i \u2212 1 |C n | j\u2208Cn j, \u2206 l2 = 1 |L b | i\u2208L b i \u2212 1 |L n | j\u2208Ln j.(5)\nSince the embeddings are normalized, the ranges of \u2206 cos and \u2206 l2 are [-2,2] and [-4,4], respectively. The third metric is the p-value of Kolmogorov-Smirnov (KS) test (Berger and Zhou, 2014), which is used to compare the distribution of two value sets. The null hypothesis is: The distance distribution of two cos similarity sets C b and C n are consistent. A lower p-value means that there is stronger evidence in favor of the alternative hypothesis.", "publication_ref": ["b2"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Experimental Settings", "text": "We conduct experiments on four natural language processing (NLP) datasets: SST2 (Socher et al.,  2013), MIND ), Enron Spam (Metsis et al., 2006, and AG News (Zhang et al., 2015). SST2 is a widely used dataset for sentiment classification. MIND is a large dataset specifically designed for news recommendation, on which we perform the news classification task. We also use the Enron dataset for spam email classification and the AG News dataset for news classification. The detailed statistics of these datasets are provided in Table 2. Additionally, we use the WikiText dataset (Merity et al., 2017) with 1,801,350 samples to count word frequencies. To validate the effectiveness of EmbMarker, we report the following metrics:\n\u2022 Accuracy. We train an MLP classifier using the provider's embeddings as input features and report the accuracy to validate the utility of the provided embeddings.\n\u2022 Detection Performance. We report three metrics, i.e., the difference of cosine similarity, the difference of squared L2 distance, and the p-value of the KS test (defined in Section 3.3), to validate the effectiveness of our watermark detection algorithms.\nWe use the AdamW algorithm (Loshchilov and Hutter, 2019) to train our models and employ em-beddings from GPT-3 text-embedding-002 API as the original embeddings of EaaS. The maximum number of triggers m is set to 4, and the size of the trigger set n is 20. The frequency interval of triggers is [0.5%, 1%]. Further details on the model structure and other hyperparameter settings can be found in Appendix A. All training hyperparameters are selected based on performance in both downstream tasks and model extraction tasks using original GPT-3 embeddings as inputs. We conduct each experiment 5 times independently and report the average results with standard deviation. In addition, we define a threshold \u03c4 to assert copyright infringement. A standard p-value of 5e-3 is considered appropriate to reject the null hypothesis for statistical significance (Benjamin et al., 2018), which can be utilized as the threshold to identify instances of copyright infringement.", "publication_ref": ["b30", "b19", "b18", "b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Performance Comparison", "text": "We compare the performance of our EmbMarker with the following baselines: 1) Original, in which the service provider does not backdoor the provided embeddings and the stealer utilizes the original embeddings to copy the model. 2) RedAlarm , a method to backdoor pre-trained language models, which selects a rare token as the trigger and returns a pre-defined target embedding when a sentence contains the trigger.\nThe performance of all methods is shown in Table 1, where we have several observations. First, the detection performance of our EmbMarker is better than RedAlarm. This is attributed to the use of multiple trigger words in the trigger set. Every trigger word in a query text brings the copied em- bedding closer to the target embedding. Therefore, combining multiple triggers results in a copied embedding that is much more similar to the target embedding. Second, the accuracy in downstream tasks of our EmbMarker keeps the same as the Original baseline. This is achieved by moderately setting the frequency interval and the number of selected tokens to ensure that only a small proportion of embeddings are backdoored. Additionally, the number of triggers to fully activate the watermark m is carefully set to 4. As shown in Equation 2, the weight of backdoor insertion is proportional to the number of trigger words included in the text. Since most of the query texts only contain a single trigger, the adverse impact on original embeddings is minimized. Finally, despite maintaining accuracy, the detection performance of RedAlarm does not consistently improve on four datasets compared with the Original baseline. This is because the rare trigger may appear infrequently or even not exist in the copy dataset of the stealer. Therefore, the target embedding of RedAlarm cannot be inherited.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Embedding Visualization", "text": "In this section, we examine the confidentiality of backdoored embeddings to the stealer by using PCA and t-SNE to visualize the embeddings produced by our method. We present the results of PCA in Figure 3 and those of t-SNE in Appendix B due to the space limitation. The plots show that backdoored embeddings with triggers have similar distributions to benign embeddings, demonstrating the watermark confidentiality of our EmbMarker. Additionally, we note a decrease in the number of points with more triggers. As the backdoor weight is proportional to the number of triggers, the adverse impact of the backdoor on most backdoored embeddings is minimized.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Impact of Trigger Number", "text": "In this section, we conduct experiments to evaluate the impact of the number of triggers in sentences on four datasets, i.e., SST2, MIND, Enron, and AG News. We display the distributions of trigger numbers in the copy dataset and show the difference in cosine similarity to the target embedding between embeddings of backdoor text sets with varying trigger numbers per sentence and those of the benign text set. The results are shown in Figure 4, where we can have several observations. First, the number of samples with triggers is small, and the number of samples with more triggers in copy datasets is   smaller or even zero. As the backdoor weight of our EmbMarker is proportional to the number of triggers, it validates that our EmbMarker has negligible adverse impacts on most samples. Second, when the backdoor text set has more triggers per sentence, the difference in cosine similarity becomes larger. Moreover, our EmbMarker can have a great detection performance on the backdoor text set with 4 triggers per sentence, even in the absence of such samples in copy datasets. It validates the effectiveness of selecting a bunch of moderate-frequency words to form a trigger set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Impact of Extracted Model Size", "text": "To evaluate the impact of model size on the performance of EmbMarker, we conduct experiments by utilizing the small, base, and large versions of BERTs as the backbone of the stealer's model on the SST2, MIND, AG News, and Enron Spam datasets, respectively. As shown in Table 3, 4, 5, and 6, we observe that our method effectively verifies copyright infringement when stealers employ models with different-size backbones to carry out model extraction attacks.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Hyper-parameter Analysis", "text": "In this subsection, we investigate the impact of the three key hyper-parameters in our EmbMarker, i.e., the maximum number of triggers m, the size of    the trigger set n, and the frequency interval of selected triggers. Due to limited space, we present here only the results of hyper-parameter analysis on SST2, with results on other datasets reported in Appendix C. We first analyze the influence of different sizes of the trigger set n. The results are illustrated in Figure 5(a) and the first row of Figure 6. It can be observed that using a small trigger set leads to poor detection performance. This is because a small trigger set results in a limited number of backdoor samples, which decreases the likelihood the stealer's model containing the watermark.\nA large trigger set reduces the watermark's confidentiality. As n increases, sentences are more likely to contain triggers, which makes more embeddings backdoored and can be easily distinguish- (f) m:\n1 -0.2 -0.1 0.0 0.1 0.2 -0.1 0.0 0.1 0.2 0.3 0 1 2 3 (g) m: 2 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (h) m: 4 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (i) m: 10 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (j) m: 20 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 (k) frequency: 0.1%-0.2% -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (l) frequency: 0.5%-1% -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (m) frequency: 2%-5% -0.1 0.0 0.1 0.2 0.3 0.4 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 4 5 6 7\n(n) frequency: 10%-20% able. However, the size of the trigger set does not greatly affect the accuracy. This may be due to the small frequency interval of [0.5%, 1%], meaning that even with a large trigger set, the probability of four triggers appearing in a sentence is still low.\nThen we present the experimental results with different maximum numbers of triggers m in Figure 5(b) and the second row of Figure 6. We find that small m, particularly 1, adversely impacts accuracy and makes the embeddings easily distinguishable by visualization. On the other hand, using large values of m reduces the detection performance. This is due to the fact that with m = 1, approximately 1% of the embeddings are equal to the pre-defined target embedding e t , which diminishes the effectiveness of the provided embeddings. When m is large, the backdoor degrees of most provided embeddings are too small to effectively inherit the watermark in the stealer's model.\nFinally, we analyze the impact of the trigger frequency. As shown in Figure 5(c) and the last row of Figure 6, high trigger frequencies have a detrimental impact on accuracy and make the embeddings Dataset Detection Performance p-value \u2193 \u2206 cos (%) \u2191 \u2206 l2 (%) \u2193 SST2 < 10 \u22125 2.50\u00b10.24 -5.01\u00b10.48 MIND < 10 \u22125 4.12\u00b10.10 -8.24\u00b10.20 AG News < 10 \u22129 8.59\u00b10.55 -17.17\u00b11.10 Enron Spam < 10 \u22126 4.96\u00b10.19 -9.92\u00b10.38 Table 7: The performance of the modified version of EmbMarker to defend against dimension-shift attacks. easily distinguishable. Conversely, low trigger frequencies adversely affect detection performance. This is due to the fact that high frequencies lead to a large number of backdoored embeddings, thus adversely impacting the performance of the provided embeddings. On the other hand, in low-frequency settings, the watermark is only added to a limited number of samples, reducing the watermark transferability to a stolen model.", "publication_ref": [], "figure_ref": ["fig_4", "fig_6", "fig_4", "fig_6"], "table_ref": []}, {"heading": "Defending Against Attacks", "text": "In this subsection, we consider similarity-invariant attacks, where the stealer applies similarityinvariant transformations on the copied embed-dings. The similarity invariance is denoted below.\nDefinition 1 (l Similarity Invariance). For a transformation A, given every vector pair (i, j), A is l-similarity-invariant only if l(A(i), A(j)) = l(i, j), where l is a similarity metric.\nThe similarity metrics used in our experiments are L 2 and cos. For the sake of convenience, in the following text, we abbreviate cos and L 2 square similarity invariance as similarity invariance.\nThere exist many similarity-invariant transformations. Below we provide two concrete examples.\nProportion 1 Denote identity transformation I as I(v) = v and dimension-shift transformation S as\nS(v) = (v d , v , v 2 , . . . , v d\u22121 )\n, where v is a vector, v i is the i-th dimension of v and d is the dimension of v. Both identity transformation I and dimensionshift transformation S are similarity-invariant.\nProportion 1 is proved in Appendix D.1.\nWhen the stealer applies some similarityinvariant attacks (e.g. dimension-shift attacks), our previous verification techniques become ineffective. To combat this attack, we propose a modified version of our EmbMarker. Instead of defining the target embedding directly, we first select a target sample and use it to compute the target embedding e t with the provider's model. Before detecting if a service contains the watermark, we request the target sample's embedding e \u2032 t from the stealer's service and use it for verification, instead of the original target embedding. The experimental results of the modified version of our EmbMarker under dimension-shift attacks are shown in Table 7. The detection performance is great enough to let us have high confidence to conclude the stealer violates the copyright of the EaaS provider. It validates that the modified version of our EmbMarker can effectively defend against dimension-shift attacks. For other similarity-invariant attacks, we theoretically prove that their detection performance should keep the same. Proportion 2 For a copied model, the detection performance \u2206 cos , \u2206 l2 and p-value of the modified EmbMarker remains consistent under any two similarity-invariant attacks involving transformations A 1 and A 2 , respectively.\nProportion 2 is proved in Appendix D.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we propose a backdoor-based embedding watermark method, named EmbMarker, which aims to effectively trace copyright infringement of EaaS LLMs while minimizing the adverse impact on the utility of embeddings. We first select a group of moderate-frequency words as the trigger set. We then define a target embedding as the backdoor watermark and insert it into the original embeddings of texts containing trigger words. To ensure the watermark can be inherited by the stealer's model, we define the provided embeddings as a weighted summation of the original embeddings and the predefined target embedding, where the weights of the target embedding are proportional to the number of triggers in the texts. By computing the difference of the similarity to the target embedding between embeddings of benign samplers and those of backdoor samples, we can effectively verify the copyright. Experiments demonstrate the effectiveness of our EmbMarker in protecting the copyright of EaaS LLMs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In this paper, we present a novel backdoor-based watermarking method, EmbMarker, for protecting the copyright of EaaS models. Our experiments on four datasets demonstrate the effectiveness of our trigger selection algorithm. However, we have observed that the optimal trigger set is related to the statistics of the dataset used by a potential stealer. To address this issue, we plan to improve Emb-Marker in the future by designing several candidate trigger sets, and adopting one based on the statistics of the stealer's previously queried data. Additionally, we discover that as trigger numbers in the backdoor texts increase, the difference between embeddings of benign and backdoor samples in the cos similarity to the target embedding increases linearly. The optimal result should be that the cosine similarity keeps normal unless the trigger numbers in the backdoor texts reach m. We plan to further investigate these areas in future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Experimental Settings", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Attacker Settings", "text": "In our experiments, the stealer applies BERT (Devlin et al., 2019) as the backbone model and a two-layer feed-forward network to extract the victim model. We assume that the attacker applies mean squared error (MSE) loss to extract the victim model, which is defined as follows:\n\u0398 * a = arg min \u0398a E x\u2208Dc ||g(x; \u0398 a ) \u2212 e x p || 2 2 , (6)\nwhere e x p is the provided embedding of sample x and g is the function of the extracted model.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Classifier", "text": "To evaluate the utility of our provided embedding e p , we use e p as input features and apply a twolayer feed-forward network as the classifier. We use cross-entropy loss to train the classifier.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Hyper-parameter Settings", "text": "The full hyper-parameter settings are in Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "B Embedding Visualization", "text": "The t-SNE visualizations of the provided embedding of our EmbMarker on four copy datasets are represented in Figure 7. The observations are consistent with those presented in Section 4.3. It shows the backdoor and benign embeddings are indistinguishable. Meanwhile, most of the samples do not contain triggers, and most of the backdoor samplers contain only a single trigger.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "C Hyper-parameter Analysis", "text": "In this section, we show the experimental results of hyper-parameter analysis on MIND, Enron Spam and AG News datasets in Figure 8, Figure 9, Figure 10, respectively. Since the results of the visualization of PCA and t-SNE are too large to display on the paper, we put them in our repository. The observations are almost the same as those we described in Section 4.6. First, too small trigger set n leads to low detection performance. This is because the number of backdoor samplers is small with too small sizes of trigger sets, which reduces the likelihood of the extracted model inheriting the watermark. Second, the trigger set n has little impact on accuracy. It might be because the frequency interval [0.005, 0.01] is small. Though the trigger set is large, the probability of 4 triggers appearing in a sentence is still low. Third, we find that small m, especially 1, degrades accuracy, while large m reduces detection performance. This is because about 1% embeddings equal the pre-defined target embedding e t with m = 1, which negatively impacts the provided embedding effectiveness. When m is large, the backdoor degree of most samples is too small to make the watermark inherited by the extracted model. Finally, low frequencies bring negative impacts on detection performance, and high frequencies might negatively affect accuracy. This is because high frequencies poison many embeddings and affect the performance of the provided embeddings. In low-frequency settings, the watermark is only added to a few samples, which limits the possibility of watermark inheritance. Additionally, we analyze the impact of dropout values on model extraction attacks. When the dropout value is greater than 0.4, the model cannot be extracted effectively, rendering the detection ability of EmbMarker meaningless. Therefore, in Table 9, we present the performance of EmbMarker when the dropout value is between 0 and 0.4. Our observations indicate that model extraction attacks are most effective when the dropout value was set to 0. This is because the LLM embeddings contain rich semantic knowledge, and increasing the dropout value weakens the stealer's model fitting ability, thereby reducing its performance in downstream tasks and the likelihood of inheriting watermarks.  ", "publication_ref": [], "figure_ref": ["fig_8", "fig_1"], "table_ref": ["tab_9"]}, {"heading": "D Theoretical Proof", "text": "In this section, we provide theoretical proof for proportions in Section 4.7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Proof of Proportion 1", "text": "Proof. Given any pair of vectors (i, j), according to the definition of identity transformation, we have\n|| I(i) ||I(i)|| \u2212 I(j)|| 2 ||I(j)|| = || i ||i|| \u2212 j ||j|| || 2 2 ,\ncos(I(i), I(j)) = cos(i, j),   \n( i k ||i|| \u2212 j k ||j|| ) 2 = || i ||i|| \u2212 j ||j|| || 2 , cos(S(i), S(j)) = d k=1 i k j k ||i|| ||j|| = cos(i, j),\nwhere d is the dimension of i and j. Therefore, dimension-shift transformation S is similarityinvariant as well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Proof of Proportion 2", "text": "Proof. Denote the embedding of copied model as e, the embedding manipulated by transformation A 1 as e 1 and the the embedding manipulated by transformation A 2 as e 2 . Since both A 1 and A 2 are similarity-invariant, we have\ncos 1 i = cos 2 i = cos i = e i \u2022 e \u2032 t ||e i || ||e \u2032 t || , l 1 2i = l 2 2i = l 2i = ||e i /||e i || \u2212 e \u2032 t /||e \u2032 t || || 2 ,\nwhere the superscript indicates the similarity calculated under which transformation. Therefore, we can obtain:\nC 1 b = C 2 b , C 1 n = C 2 n , L 1 b = L 2 b , L 1 n = L 2 n .\nSince the inputs for the metrics \u2206 cos , \u2206 l2 and p-value in our methods are only C b , C n , L b and L n , we have\n\u2206 1 cos = \u2206 2 cos , \u2206 1 l2 = \u2206 2 l2 , p 1 KS = p 2 KS ,\nwhere p KS is the p-value of the KS test with C b and C n as inputs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Experimental Environments", "text": "We conduct experiments on a linux server with Ubuntu 18.04. The server has a V100-16GB with CUDA 11.6. We use pytorch 1.13.1.    at section 4 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? at section 5 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? at section 5 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. at section 5 C Did you run computational experiments? at section 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? at section 5 C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? at section 5 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? at section 5 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? at section 5 D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was supported by the grants from National Natural Science Foundation of China (No.62222213, U22B2059, 62072423), and the USTC Research Funds of the Double First-Class Initiative (No.YD2150002009).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring", "journal": "", "year": "2018", "authors": "Yossi Adi; Carsten Baum; Moustapha Cisse; Benny Pinkas; Joseph Keshet"}, {"ref_id": "b1", "title": "Redefine statistical significance", "journal": "Nature human behaviour", "year": "2018", "authors": "J Daniel; James O Benjamin; Magnus Berger;  Johannesson; A Brian; E-J Nosek; Richard Wagenmakers;  Berk; A Kenneth; Bj\u00f6rn Bollen; Lawrence Brembs; Colin Brown;  Camerer"}, {"ref_id": "b2", "title": "Kolmogorovsmirnov test: Overview. Wiley statsref: Statistics reference online", "journal": "", "year": "2014", "authors": "W Vance; Yanyan Berger;  Zhou"}, {"ref_id": "b3", "title": "A systematic review on model watermarking for neural networks", "journal": "Frontiers in big Data", "year": "2021", "authors": "Franziska Boenisch"}, {"ref_id": "b4", "title": "Language models are few-shot learners", "journal": "NIPS", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b5", "title": "Badpre: Task-agnostic backdoor attacks to pre-trained NLP foundation models", "journal": "", "year": "2022", "authors": "Kangjie Chen; Yuxian Meng; Xiaofei Sun; Shangwei Guo; Tianwei Zhang; Jiwei Li; Chun Fan"}, {"ref_id": "b6", "title": "BadNL: Backdoor attacks against NLP models", "journal": "", "year": "2021", "authors": "Xiaoyi Chen; Ahmed Salem; Michael Backes; Shiqing Ma; Yang Zhang"}, {"ref_id": "b7", "title": "Digital watermarking and steganography", "journal": "Morgan kaufmann", "year": "2007", "authors": "Ingemar Cox; Matthew Miller; Jeffrey Bloom; Jessica Fridrich; Ton Kalker"}, {"ref_id": "b8", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b9", "title": "Protecting intellectual property of language generation apis with lexical watermark", "journal": "", "year": "2022", "authors": "Xuanli He; Qiongkai Xu; Lingjuan Lyu; Fangzhao Wu; Chenguang Wang"}, {"ref_id": "b10", "title": "CATER: Intellectual property protection on text generation APIs via conditional watermarks", "journal": "", "year": "2022", "authors": "Xuanli He; Qiongkai Xu; Yi Zeng; Lingjuan Lyu; Fangzhao Wu; Jiwei Li; Ruoxi Jia"}, {"ref_id": "b11", "title": "Entangled watermarks as a defense against model extraction", "journal": "", "year": "2021", "authors": "Hengrui Jia; Christopher A Choquette-Choo; Varun Chandrasekaran; Nicolas Papernot"}, {"ref_id": "b12", "title": "Nicolas Papernot, and Mohit Iyyer. 2020. Thieves on sesame street! model extraction of bert-based apis", "journal": "", "year": "", "authors": "Kalpesh Krishna; Gaurav Singh Tomar; Ankur P Parikh"}, {"ref_id": "b13", "title": "Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications", "journal": "", "year": "2020", "authors": "Patrick Erwan Le Merrer; Gilles Perez;  Tr\u00e9dan"}, {"ref_id": "b14", "title": "Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach", "journal": "", "year": "2020-06", "authors": "Meng Li; Qi Zhong; Leo Yu Zhang; Yajuan Du"}, {"ref_id": "b15", "title": "Hidden backdoors in human-centric language models", "journal": "", "year": "2021", "authors": "Shaofeng Li; Hui Liu; Tian Dong; Benjamin Zi Hao Zhao; Minhui Xue; Haojin Zhu; Jialiang Lu"}, {"ref_id": "b16", "title": "Protect, show, attend and tell: Empowering image captioning models with ownership protection", "journal": "", "year": "2022", "authors": "Chee Jian Han Lim; Kam Woh Seng Chan; Lixin Ng; Qiang Fan;  Yang"}, {"ref_id": "b17", "title": "Stolenencoder: Stealing pretrained encoders in self-supervised learning", "journal": "", "year": "2022", "authors": "Yupei Liu; Jinyuan Jia; Hongbin Liu; Neil Zhenqiang Gong"}, {"ref_id": "b18", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b19", "title": "Pointer sentinel mixture models", "journal": "", "year": "2017", "authors": "Stephen Merity; Caiming Xiong; James Bradbury; Richard Socher"}, {"ref_id": "b20", "title": "Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras", "journal": "", "year": "2006", "authors": ""}, {"ref_id": "b21", "title": "Knockoff nets: Stealing functionality of blackbox models", "journal": "", "year": "2019", "authors": "Tribhuvanesh Orekondy; Bernt Schiele; Mario Fritz"}, {"ref_id": "b22", "title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Ng; Christopher Potts"}, {"ref_id": "b23", "title": "Dawn: Dynamic adversarial watermarking of neural networks", "journal": "", "year": "2021", "authors": "Sebastian Szyller; Samuel Buse Gul Atli; N Marchal;  Asokan"}, {"ref_id": "b24", "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models", "journal": "", "year": "", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Naman Baptiste Rozi\u00e8re; Eric Goyal;  Hambro"}, {"ref_id": "b25", "title": "Embedding watermarks into deep neural networks", "journal": "", "year": "2017", "authors": "Yusuke Uchida; Yuki Nagai; Shigeyuki Sakazawa; Shin'ichi Satoh"}, {"ref_id": "b26", "title": "Watermarking in deep neural networks via error back-propagation", "journal": "Electronic Imaging", "year": "2020", "authors": "Jiangfeng Wang; Hanzhou Wu; Xinpeng Zhang; Yuwei Yao"}, {"ref_id": "b27", "title": "MIND: A large-scale dataset for news recommendation", "journal": "", "year": "2020", "authors": "Fangzhao Wu; Ying Qiao; Jiun-Hung Chen; Chuhan Wu; Tao Qi; Jianxun Lian; Danyang Liu; Xing Xie; Jianfeng Gao; Winnie Wu; Ming Zhou"}, {"ref_id": "b28", "title": "Xu Sun, and Bin He. 2021. Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models", "journal": "", "year": "", "authors": "Wenkai Yang; Lei Li; Zhiyuan Zhang; Xuancheng Ren"}, {"ref_id": "b29", "title": "Analyzing information leakage of updates to natural language models", "journal": "", "year": "2020", "authors": "Santiago Zanella-B\u00e9guelin; Lukas Wutschitz; Shruti Tople; Victor R\u00fchle; Andrew Paverd; Olga Ohrimenko; Boris K\u00f6pf; Marc Brockschmidt"}, {"ref_id": "b30", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Jake Zhao; Yann Lecun"}, {"ref_id": "b31", "title": "Red alarm for pre-trained models: Universal vulnerability to neuron-level backdoor attacks", "journal": "", "year": "2021", "authors": "Zhengyan Zhang; Guangxuan Xiao; Yongwei Li; Tian Lv; Fanchao Qi; Zhiyuan Liu; Yasheng Wang; Xin Jiang; Maosong Sun"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: An overall framework of our EmbMarker.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: The detailed framework of our EmbMarker.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: Visualization of the provided embedding of our EmbMarker on four copy datasets. Different colors represent the number of triggers in the samples. It shows the backdoor and benign embeddings are indistinguishable.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and the frequency interval on the SST2 dataset.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure6: Visualization of the provided embedding of our EmbMarker on SST2 dataset with different hyperparameter settings, i.e. trigger set size n, max trigger number m and frequency. If not specified, the default setting is that frequency interval equals [0.5%, 1%], m = 4 and n = 20.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: T-SNE Visualization of the provided embedding of our EmbMarker on four copy datasets. Different colors represent the number of triggers in the samples. It shows the backdoor and benign embeddings are indistinguishable.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure9: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and the frequency interval on the AG News dataset.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure10: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and the frequency interval on the Enron Spam dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "66\u00b10.12 < 10 \u22129 12.85\u00b10.67 -25.70\u00b11.34 Performance of different methods on the SST2, MIND, AG News, and Enron datasets. \u2191 means higher metrics are better. \u2193 means lower metrics are better.", "figure_data": "DatasetMethod OriginalACC (%) 93.76\u00b10.19Detection Performance p-value \u2193 \u2206 cos (%) \u2191 \u2206 l2 (%) \u2193 > 0.34 -0.07\u00b10.18 0.14\u00b10.36SST2RedAlarm 93.76\u00b10.19> 0.091.35\u00b10.17-2.70\u00b10.35EmbMarker 93.55\u00b10.19 < 10 \u221254.07\u00b10.37-8.13\u00b10.74Original77.30\u00b10.08> 0.08-0.76\u00b10.051.52\u00b10.10MINDRedAlarm 77.18\u00b10.09> 0.38-2.08\u00b10.664.17\u00b11.31EmbMarker 77.29\u00b10.12 < 10 \u221254.64\u00b10.23-9.28\u00b10.47Original93.74\u00b10.14> 0.030.72\u00b10.15-1.46\u00b10.30AGNewsRedAlarm 93.74\u00b10.14> 0.09-2.04\u00b10.764.07\u00b11.51Original EmbMarker 93.Enron Spam 94.74\u00b10.14 RedAlarm 94.87\u00b10.06> 0.03 > 0.47-0.21\u00b10.27 -0.50\u00b10.290.42\u00b10.54 1.00\u00b10.57EmbMarker 94.78\u00b10.27 < 10 \u221266.17\u00b10.31 -12.34\u00b10.62Dataset#Sample #Classes Avg. len.SST268,221254.17MIND130,3831866.14Enron Spam 33,716234.57AG News127,6004236.41"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The impact of the model size on MIND.", "figure_data": "BERT ParametersDetection Performance p-value \u2206 cos (%) \u2206 l2 (%)Small Base Large29M 108M 333M< 10 \u221210 < 10 \u22129 < 10 \u22121010.65 12.85 11.43-21.30 -25.70 -22.86"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The impact of the model size on AGNews.", "figure_data": "BERT ParametersDetection Performance p-value \u2206 cos (%) \u2206 l2 (%)Small Base Large29M 108M 333M< 5 \u00d7 10 \u22125 < 10 \u22126 < 10 \u221262.35 6.17 2.93-4.71 -12.34 -5.86"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The impact of the model size on Enron Spam.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The impact of the dropout value used in FFN network on SST2.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Hyper-parameter settings. The dropout value corresponds to the dropout used in the FFN network, while the dropout value for BERT backbone was set to default.", "figure_data": "which indicates identity transformation issimilarity-invariant.For dimension-shift transformation S, we have||S(i) ||S(i)|| d\u2212S(j) ||S(j)|||| 2=k=1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and the frequency interval on the MIND dataset.", "figure_data": "4 1020 501001 24 1020[0.001, 0.002] [0.005, 0.01][0.02, 0.05] [0.1, 0.2]Accuracy76 7777.3177.1677.2977.13 77.131.794.647.3913.394 8 12Cos DifferenceAccuracy76.0 76.5 77.076.1677.1977.2977.19 77.2618.2714.134.644 8 12 16Cos DifferenceAccuracy76.5 77.077.24 77.29 77.3377.064.6414.0820.305 10 15 20Cos Difference75Accuracy0.45Cos Diff.075.5Accuracy1.52 Cos Diff.0.72076.0AccuracyCos Diff. 0.230(a) trigger set size n(b) max trigger number m(c) frequency interval91 92 93 Figure 8: Accuracy Accuracy 93.76 93.76 93.66 93.61 93.58 4 1011.83 20 50Cos Diff. 12.35 12.85 15.0813.98 10010 12 14Cos DifferenceAccuracy85 87 89 91 93Accuracy 85.62 92.93 93.66 93.72 93.71 1 213.71 4 10Cos Diff. 15.18 12.85 6.165.14 200 3 6 9 12 15Cos DifferenceAccuracy83 87 91Accuracy 93.68 93.66 93.63 83.78 [0.001, 0.002] [0.005, 0.01]Cos Diff. 2.48 12.85 16.01 13.37 [0.02, 0.05] 0 4 8 12 16 [0.1, 0.2]Cos Difference(a) trigger set size n(b) max trigger number m"}], "formulas": [{"formula_id": "formula_0", "formula_text": "E c = {e i = S v (s i )|s i \u2208 D c }.", "formula_coordinates": [3.0, 158.41, 599.43, 132.64, 18.93]}, {"formula_id": "formula_1", "formula_text": "Q(S) = min(|S \u2229 T |, m) m ,(1)", "formula_coordinates": [3.0, 355.7, 657.26, 169.45, 26.31]}, {"formula_id": "formula_2", "formula_text": "e p = (1 \u2212 Q(S)) * e o + Q(S) * e t ||(1 \u2212 Q(S)) * e o + Q(S) * e t || 2 . (2", "formula_coordinates": [4.0, 81.75, 368.43, 203.88, 35.81]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [4.0, 285.63, 376.26, 4.24, 13.15]}, {"formula_id": "formula_4", "formula_text": "D b = {[w 1 , w 2 , ..., w m ]|w i \u2208 T }, D n = {[w 1 , w 2 , ..., w m ]|w i \u0338 \u2208 T }.(3)", "formula_coordinates": [4.0, 105.4, 615.7, 184.46, 35.47]}, {"formula_id": "formula_5", "formula_text": "cos i = e i \u2022 e t ||e i || ||e t || , l 2i = || e i ||e i || \u2212 e t ||e t || || 2 , C b = {cos i |i \u2208 D b }, C n = {cos i |i \u2208 D n }, L b = {l 2i |i \u2208 D b }, L n = {l 2i |i \u2208 D n }.(4)", "formula_coordinates": [4.0, 314.86, 366.66, 210.3, 68.55]}, {"formula_id": "formula_6", "formula_text": "\u2206 cos = 1 |C b | i\u2208C b i \u2212 1 |C n | j\u2208Cn j, \u2206 l2 = 1 |L b | i\u2208L b i \u2212 1 |L n | j\u2208Ln j.(5)", "formula_coordinates": [4.0, 340.79, 503.75, 184.36, 67.82]}, {"formula_id": "formula_7", "formula_text": "1 -0.2 -0.1 0.0 0.1 0.2 -0.1 0.0 0.1 0.2 0.3 0 1 2 3 (g) m: 2 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (h) m: 4 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (i) m: 10 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (j) m: 20 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 (k) frequency: 0.1%-0.2% -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (l) frequency: 0.5%-1% -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 (m) frequency: 2%-5% -0.1 0.0 0.1 0.2 0.3 0.4 -0.2 -0.1 0.0 0.1 0.2 0 1 2 3 4 5 6 7", "formula_coordinates": [8.0, 75.48, 179.08, 438.24, 207.34]}, {"formula_id": "formula_8", "formula_text": "S(v) = (v d , v , v 2 , . . . , v d\u22121 )", "formula_coordinates": [9.0, 70.86, 261.6, 128.25, 11.95]}, {"formula_id": "formula_9", "formula_text": "\u0398 * a = arg min \u0398a E x\u2208Dc ||g(x; \u0398 a ) \u2212 e x p || 2 2 , (6)", "formula_coordinates": [12.0, 84.84, 223.25, 205.02, 21.19]}, {"formula_id": "formula_10", "formula_text": "|| I(i) ||I(i)|| \u2212 I(j)|| 2 ||I(j)|| = || i ||i|| \u2212 j ||j|| || 2 2 ,", "formula_coordinates": [12.0, 332.77, 730.92, 165.03, 35.81]}, {"formula_id": "formula_11", "formula_text": "( i k ||i|| \u2212 j k ||j|| ) 2 = || i ||i|| \u2212 j ||j|| || 2 , cos(S(i), S(j)) = d k=1 i k j k ||i|| ||j|| = cos(i, j),", "formula_coordinates": [13.0, 83.3, 515.47, 184.7, 81.52]}, {"formula_id": "formula_12", "formula_text": "cos 1 i = cos 2 i = cos i = e i \u2022 e \u2032 t ||e i || ||e \u2032 t || , l 1 2i = l 2 2i = l 2i = ||e i /||e i || \u2212 e \u2032 t /||e \u2032 t || || 2 ,", "formula_coordinates": [13.0, 90.15, 730.6, 179.69, 52.33]}, {"formula_id": "formula_13", "formula_text": "C 1 b = C 2 b , C 1 n = C 2 n , L 1 b = L 2 b , L 1 n = L 2 n .", "formula_coordinates": [13.0, 320.46, 489.2, 179.69, 15.25]}, {"formula_id": "formula_14", "formula_text": "\u2206 1 cos = \u2206 2 cos , \u2206 1 l2 = \u2206 2 l2 , p 1 KS = p 2 KS ,", "formula_coordinates": [13.0, 330.1, 565.26, 170.37, 15.25]}], "doi": ""}