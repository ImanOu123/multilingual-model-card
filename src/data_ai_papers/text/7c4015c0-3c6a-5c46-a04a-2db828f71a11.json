{"title": "Visually Grounded Compound PCFGs", "authors": "Yanpeng Zhao; Ivan Titov Eae", "pub_date": "", "abstract": "Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via REINFORCE and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrasestructure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more 'abstract' categories (e.g., +55.1% recall on VPs). 1   ", "sections": [{"heading": "Introduction", "text": "Grammar induction is a task of finding latent hierarchical structure of language. As a fundamental problem in computational linguistics, it has been extensively studied for decades (Lari and Young, 1990;Carroll and Charniak, 1992;Clark, 2001;1 Our code is available at https://git.io/JU0JJ. Klein and Manning, 2002). Recently, deep learning models have been shown very effective across NLP tasks and have also been applied to grammar induction, greatly advancing the area (Shen et al., 2018(Shen et al., , 2019Kim et al., 2019a,b;Jin et al., 2019). These neural grammar-induction approaches have been generally limited to relying on text, without considering learning signals from other modalities.\nIn contrast, the crucial aspect of natural language learning is that it is grounded in perceptual experiences (Barsalou, 1999;Fincher-Kiefer, 2001;Bisk et al., 2020). We thus anticipate improved language understanding by leveraging grounded learning. Promising results from grounded learning have been emerging in areas such as representation learning (Bruni et al., 2014;Kiela et al., 2018;Bordes et al., 2019). Typically, they use visual images as perceptual groundings of language and aim at improving continuous vector representations of language (e.g., word or sentence embeddings). In this work, we consider a more challenging problem: can visual groundings help us induce syntactic structure? We refer to this problem as visually grounded grammar induction. Shi et al. (2019) propose a visually grounded neural syntax learner (VG-NSL) to tackle the task. Specifically, they learn a parser from aligned imagesentence pairs (e.g., image-caption data), where each sentence describes visual content of the corresponding image. The parser is optimized via REIN-FORCE, where the reward is computed by scoring the alignment of images and constituents. While straightforward, matching-based rewards can, as we will discuss further in the paper, make the parser focus only on more local and short constituents (e.g., 79.6% recall on NPs) and to perform poorly on longer ones (e.g., 26.2% recall on VPs) (Shi et al., 2019). While for the former it outperforms the text-only grammar induction methods, for the latter it substantially underachieves. This may not be surprising, as it is not guaranteed that every constituent of a sentence has its visual representation in the aligned image; the reward signals can be noisy and insufficient to capture all aspects of phrase-structure syntax. Consequently, Shi et al. (2019) have to rely on language-specific inductive bias to obtain more informative reward signals. Another issue with VG-NSL is that the parser does not admit tractable estimation of the partition function and the posterior probabilities for constituent boundaries needed to compute the expected reward in closed form. Instead, VG-NSL relies on Monte Carlo policy gradients, potentially suffering from high variance.\nTo alleviate the first issue, we propose to complement the image-text alignment-based loss with a loss defined on unlabeled text (i.e., its loglikelihood). As re-confirmed with neural models in Shen et al. (2019) and Kim et al. (2019a), text itself can drive induction of rich syntactic knowledge, so additionally optimizing the parser on raw text can be beneficial and complementary to visual grounded learning. To resolve the second issue, we resort to an extension of probabilistic contextfree grammar (PCFG) parsing model, compound PCFG (Kim et al., 2019a). It admits tractable estimation of the posteriors, needed in the alignment loss, with dynamical programming and leads to a fully-differentiable end-to-end visually grounded learning. More importantly, the PCFG parser lets us complement the alignment loss with a language modeling objective.\nOur key contributions can be summarized as follows: (1) we propose a fully-differentiable endto-end visually grounded learning framework for grammar induction; (2) we additionally optimize a language modeling objective to complement visually grounded learning; (3) we conduct experiments on MSCOCO (Lin et al., 2014) and observe that our model has a higher recall than VG-NSL for five out of six most frequent constituent labels. For example, it surpasses VG-NSL by 55.1% recall on VPs and by 48.7% recall on prepositional phrases (PPs). Comparing to a model trained purely via visually grounded learning, extending the loss with a language modeling objective improves the overall F1 from 50.5% to 59.4%.", "publication_ref": ["b32", "b8", "b9", "b30", "b39", "b40", "b19", "b2", "b14", "b6", "b21", "b5", "b41", "b41", "b41", "b40", "b23", "b23", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Background and Motivation", "text": "Our model relies on compound PCFGs (Kim et al., 2019a) and generalizes the visually grounded gram-mar learning framework of Shi et al. (2019). We will describe the relevant aspects of both frameworks in Sections 2.1-2.2, and then discuss their limitations (Section 2.3).", "publication_ref": ["b23", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Compound PCFGs", "text": "Compound PCFGs extend context-free grammars (CFGs) and, to establish notation, we start by briefly introducing them. A CFG is defined as a 5-tuple G = (S, N , P, \u03a3, R) where S is the start symbol, N is a finite set of nonterminals, P is a finite set of preterminals, \u03a3 is a finite set of terminals, 2 and R is a set of production rules in the Chomsky normal form:\nS A, A \u2208 N , A BC, A \u2208 N , B, C \u2208 N \u222a P, T w, T \u2208 P, w \u2208 \u03a3 .\nPCFGs extend CPGs by associating each production rule r \u2208 R with a non-negative scalar \u03c0 r such that r:A \u03b3 \u03c0 r = 1, i.e., the probabilities of production rules with the same left-hand-side nonterminal sum to 1. The strong context-free assumption hinders PCFGs and prevent them from being effective in the grammar induction context. Compound PCFGs (C-PCFGs) mitigate this issue by assuming that rule probabilities follow a compound probability distribution (Robbins, 1951):\n\u03c0 r = g r (z; \u03b8), z \u223c p(z) ,\nwhere p(z) is a prior distribution of the latent z, and g r (\u2022; \u03b8) is parameterized by \u03b8 and yields a rule probability \u03c0 r . Depending on the rule type, g r (\u2022; \u03b8) takes one of these forms:\n\u03c0 S A = exp(u T A f s ([w S ; z])) A \u2208N exp(u T A f s ([w S ; z])) , \u03c0 A BC = exp(u T BC [w A ; z]) B ,C \u2208N \u222aP exp(u T B C [w A ; z]) , \u03c0 T w = exp(u T w f t ([w T ; z])) w \u2208\u03a3 exp(u T w f t ([w T ; z])) ,\nwhere u is a parameter vector, w N is a symbol embedding and N \u2208 {S} \u222a N \u222a P. [\u2022; \u2022] indicates vector concatenation, and f s (\u2022) and f t (\u2022) encode the input into a vector (parameters are dropped for simplicity).\nA C-PCFG defines a mixture of PCFGs (i.e., we can sample a set of PCFG parameters by sampling a vector z). It satisfies the context-free assumption conditioned on z and thus admits exact inference for each given z. Learning with C-PCFGs involves maximizing the log-likelihood of every observed sentence w = w 1 w 2 . . . w n :\nlog p \u03b8 (w) = log z t\u2208T G (w) p \u03b8 (t|z)p(z) dz ,\nwhere T G (w) consists of all parses of the sentence w under a PCFG G. Though for each given z the inner summation over parses can be efficiently computed using the inside algorithm (Baker, 1979), the integral over z makes optimization intractable. Instead, C-PCFGs rely on variational inference and maximize the evidence lower bound (ELBO):\nlog p \u03b8 (w) \u2265 ELBO(w; \u03c6, \u03b8) = (1) E q \u03c6 (z|w) [log p \u03b8 (w|z)] \u2212 KL[q \u03c6 (z|w)||p(z)] ,\nwhere q \u03c6 (z|w) is a variational posterior, a neural network parameterized with \u03c6. The expected loglikelihood term is estimated via the reparameterization trick (Kingma et al., 2014); the KL term can be computed analytically when p(z) and q \u03c6 (z|w) are normally distributed.", "publication_ref": ["b36", "b0", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Visually grounded neural syntax learner", "text": "The visually grounded neural syntax learner (VG-NSL) comprises a parsing model and an image-text matching model. The parsing model is an easyfirst parser (Goldberg and Elhadad, 2010). It builds a parse greedily in a bottom-up manner while at the same time producing a semantic representation for each constituent in the parse (i.e., its 'embedding'). The parser is optimized through REIN-FORCE (Williams, 1992). The reward encourages merging two adjacent constituents if the merge results in a constituent that is concrete, i.e., if its semantic representations is predictive of the corresponding image, as measured with a matching function. We omit details of the parser and how the semantic representations of constituents are computed, as they are not relevant to our approach, and refer the reader to Shi et al. (2019). However, as we will extend their image-text matching model, we explain this component of their approach more formally. In their work, this loss is used to learn the textual and visual representations. For every constituent c (i) of a sentence w (i) , they define the following triplet hinge loss:\nh(c (i) , v (i) ) = E c m(c , v (i) ) \u2212 m(c (i) , v (i) ) + + + E v m(c (i) , v )\u2212m(c (i) , v (i) )+ + ,(2)\nwhere  (i) , v (i) ) should score higher than an unaligned one ((c ,\n[\u2022] + = max(0, \u2022), is a positive margin, m(c, v) cos(c, v\nv (i) ) or (c (i) , v )).\nThe total loss for an image-sentence pair (v (i) , w (i) ) is obtained by summing losses for all constituents in a tree t (i) , sampled from the parsing model (we write c (i) \u2208 t (i) ):\ns(v (i) , w (i) ) = c (i) \u2208t (i) h(c (i) , v (i) ) .(3)\nIn their work, training alternates between optimizing the parser using rewards (relying on image and text representations) and optimizing the imagetext matching model to refine image and text representations (relying on the fixed parsing model). Once trained, the parser can be directly applied to raw text, i.e., images are not used at test time.", "publication_ref": ["b16", "b50", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations of the VG-NSL framework", "text": "While straightforward, there are several practical issues inhibiting the visually grounded learning framework. First, contrastive learning implicitly assumes that every constituent of a sentence has its visual representation in the aligned image. However, it is not guaranteed in practice and would result in noisy reward signals. Besides, the loss in Equation 2 (and a similar component in the reward, see Shi et al. (2019)) focuses on constituents corresponding to short spans. Long spans, independently of their syntactic structure, tend to be sufficiently discriminative to distinguish the aligned image v (i) from an unaligned one. This implies that there is not much learning signal for such constituents. The tendency to focus on short spans and those more easily derivable from an image is evident from the results (Shi et al., 2019;Kojima et al., 2020). For example, their parser is accurate for noun phrases (recall 79.6%), which are often short for captions, but performs poorly on verb phrases (recall 26.2%) which have longer spans, more complex compositionally and also harder to predict from images (see our analysis in Section 4.3.2). While there may be ways to mitigate some of these issues, we believe that any image-text matching loss alone is unlikely to provide sufficient learning signal to accurately captures all aspects of syntax. Instead of resorting to language-specific inductive biases as done by Shi et al. (2019) (i.e., head-initial bias (Baker, 2008) of English), we propose to complement the image-text matching loss with the objective derived from the unaligned text (i.e., log-likelihood), jointly training a parser to both explain the raw language data and the alignment with images. Moreover, their learning is likely to suffer from large variance in gradient estimation as their parser does not admit tractable estimation of the partition function, and thus they have to rely on sampling decisions. This will be even more of a problem if we would attempt to use it in the joint learning setup. Also note that similar parsing models do not yield linguistically-plausible structures when used in the conventional (i.e., non-grounded) grammarinduction set-ups (Williams et al., 2018;Havrylov et al., 2019).\nIn the next section, we will use compound PCFGs and describe an improved visually grounded learning framework that can tackle these issues neatly.", "publication_ref": ["b41", "b41", "b31", "b41", "b1", "b49", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Visually grounded compound PCFGs", "text": "We use compound PCFGs (Kim et al., 2019a) and develop visually-grounded compound PCFGs (VC-PCFGs) within the contrastive learning framework. Instead of sampling a tree and computing a point estimate of the image-text matching loss, we can compute the expected image-text matching loss under a tree distribution and use end-to-end contrastive learning (Section 3.1). Since it is inefficient to compute constituent representations relying on the chart, we will introduce an additional textual representation model to encode constituents (Section 3.2). Moreover, VC-PCFGs let us additionally optimize a language modeling objective, complementing the visually grounded contrastive learning (Section 3.3).", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "End-to-end contrastive learning", "text": "In the visually grounded grammar induction framework, the parsing model is optimized through learning signals derived from the alignment of images and constituents, as scored by the image-text matching model. Denoting a set of image representations by V = {v (i) } and the corresponding set of sentences by W = {w (i) }, the image-text matching model is optimized via contrastive learning:\nL(V, W; \u03c6, \u03b8) = i s(v (i) , w (i) ) . (4\n)\nWe define s(v (i) , w (i) ) as the loss of aligning v (i) and w (i) . In VG-NSL, it is estimated via point estimation (see Equation 3). While in VC-PCFGs, given an aligned image-sentence pair (v, w), we compute the expected image-sentence matching loss under a tree distribution p \u03b8 (t|w), leading to an end-to-end contrastive learning:\ns(v, w) = E p \u03b8 (t|w) c\u2208t h(c, v) ,(5)\nwhere h(c, v) is the hinge loss of aligning the unlabeled constituent c and the image v (defined in Equation 2). Minimizing the hinge loss encourages an aligned image-constituent pair to rank higher than any unaligned one. Expanding the right-hand side of Equation 5\ns(v, w) = t\u2208T G (w) p \u03b8 (t|w) c\u2208t h(c, v) = c\u2208w t\u2208T G (w) I {c\u2208t} p \u03b8 (t|w) p(c|w): marginal of the span c h(c, v) = c\u2208w p(c|w)h(c, v) ,(6)\nwhere p(c|w) is the conditional probability (i.e., marginal) of the span c given w. It can be efficiently computed with the inside algorithm and automatic differentiation (Eisner, 2016).", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Span representation", "text": "Estimation of the expected image-text matching scores relies on span representations. Ideally, a span representation should encode semantics of a span with its computation guided by its syntactic structure (Socher et al., 2013). The reliance on the predicted tree structure will result in propagating learning signals derived from the alignment of images and sentences back to the parser. To realize this desideratum, we could follow the inside algorithm and recursively compose span representations (Le and Zuidema, 2015;Stern et al., 2017;Drozdov et al., 2019), which is, however, time-and memory-inefficient in practice. Instead, we produce span representations largely independently of the parser, as we will explain below. The only way the parser model influences this representation is through the predicted constituent label: we use its distribution to compute the representation. 3 Specificially, as a trade-off for a better training efficiency, we adopt a single-layer BiLSTM to encode spans. A mean-pooling layer is applied over the hidden states h of the BiLSTM and followed by a label-specific affine transformation f k (\u2022) to produce a label-specific span representation c k . Take a span c i,j = w i . . . w j (0 < i < j \u2264 n):\nc k = f k ( 1 j \u2212 i + 1 j l=i h l ) .(7)\nThe BiLSTM encoding model operates at the span level and encodes semantics of a span. Unlike using a single sentence-level (Bi)LSTM encoder, it guarantees that no information from words outside of the span leaks into its representations. More importantly, it can run in O(n) for a sentence of length n with a parallel implementation. While the produced representation does not reflect the structural decisions made by the parser, it can be sensitive to word order and may be affected by its syntactic structure (Blevins et al., 2018).\nIn order to compute the representation of unlabeled constituent c, we average the label-specific span representation c k under the distribution of labels defined by the parser:\nc = K k=1 p(k|c, w)c k ,(8)\nwhere p(k|c, w) is the probability that the span c has label k, conditioned on having this constituent span in the tree.\nTo further reduce computation we estimate the matching loss only using the n(n\u22121) This is the case anyway (see discussion in Section 2.3), so we expect that this simplification would not hurt model performance significantly.", "publication_ref": ["b44", "b34", "b47", "b11", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Joint objective", "text": "Rather than simply optimizing the contrastive learning objective, we additionally maximize the loglikelihood of text data. As with C-PCFGs, we optimize the ELBO:\nL(W; \u03c6, \u03b8) = \u2212 w\u2208W ELBO(w; \u03c6, \u03b8) . (9\n)\nThis learning objective complements contrastive learning. As contrastive learning optimizes a parser by solely matching images and constituents, the parser would only focus on simple and local constituents (e.g., short NPs). Moreover, in practice, since not every constituent can be grounded in an image, contrastive learning would suffer from misleading or ambiguous learning signals.\nTo summarize, the overall loss function is\nJ (\u03c6, \u03b8) = L(W; \u03c6, \u03b8) + \u03b1 \u2022 L(V, W; \u03c6, \u03b8) , (10\n)\nwhere \u03b1 is a hyper-parameter balancing the relative importance of the contrastive learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parsing", "text": "The parser can be directly used to parse raw text after training, without requiring access to visual groundings. Parsing seeks for the most probable parse t * of w:\nt * = argmax z p \u03b8 (t|w, z)p \u03b8 (z|w) dz .\nStill, though the maximum a posterior (MAP) inference over p \u03b8 (t|w) can be solved by the CYK algorithm (Kasami, 1966;Younger, 1967), inference becomes intractable when introducing into z. The MAP inference is instead approximated by\nt * \u2248 argmax z p \u03b8 (t|w, z)\u03b4(z \u2212 \u00b5 \u03c6 (w)) dz ,\nwhere \u03b4(\u2022) is the Dirac delta function and \u00b5 \u03c6 (w) is the mean vector of the variational posterior q \u03c6 (z|w). As \u03b4(\u2022) has zero mass everywhere but at the mode \u00b5 \u03c6 (w), it is equivalently solving argmax t p \u03b8 (t|w, \u00b5 \u03c6 (w)).", "publication_ref": ["b20", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments 4.1 Datasets and evaluation", "text": "Datasets: We use MSCOCO (Lin et al., 2014). It consists of 82,783 training images, 1,000 validation images, and 1,000 test images. Each image is associated with 5 caption sentences. We encode images into 2048-dimensional vectors using the pre-trained ResNet-101 (He et al., 2016). At test time, only captions are used. We follow Shi et al. (2019) and parse test captions with Benepar (Kitaev and Klein, 2018). We use the same data preprocessing 4 as in Shen et al. (2019) and Kim et al. (2019a), where punctuation is removed from all data, and the top 10,000 frequent words in training sentences are kept as the vocabulary.\nEvaluation: We mainly compare VC-PCFGs with VG-NSL (Shi et al., 2019). To verify the effectiveness of the use of visual groundings, we also compare our model with a C-PCFG trained only on the training captions. All models are run four times with different random seeds and for at most 15 epochs with early stopping (i.e., the image-caption loss / perplexity on the validation captions does not decrease). We report both averaged corpus-level F1 and averaged sentence-level F1 numbers as well as the unbiased standard deviations.", "publication_ref": ["b35", "b18", "b41", "b29", "b40", "b23", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Settings and hyperparameters", "text": "We adopt parameter settings suggested by the authors for the baseline models. For VG-NSL we run the authors' code. 5 We re-implement C-PCFG using automatic differentiation (Eisner, 2016)  We apply a max-pooling layer over the hidden states of the BiLSTM and then obtain 64-dimensitional mean vectors \u00b5 \u03c6 (w) and log-variances log \u03c3 \u03c6 (w) by using an affine layer.\nThe image-text matching model projects visual features into 512-dimensitional feature vectors and encodes spans as 512-dimensitional vectors. Our span representation model is another single-layer BiLSTM, with the same hyperparameters as in the inference model. \u03b1 for visually grounded learning is set to 0.001. We implement VC-PCFG relying on Torch-Struct (Rush, 2020), and optimize it using Adam (Kingma and Ba, 2015) with the learning rate set to 0.01, \u03b2 1 = 0.75, and \u03b2 2 = 0.999. All parameters are initialized with Xavier uniform initializer (Glorot and Bengio, 2010).", "publication_ref": ["b12", "b38", "b25", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Results and analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main results", "text": "Our model outperforms all baselines according to both corpus-level F1 and sentence-level F1 (see Table 1). Notably, it surpasses VG-NSL+HI by 10% F1. 6 The right branching model is a strong baseline on image captions, as observed previously on the WSJ corpus, including in recent work (Shen et al., 2018;Kim et al., 2019a). Comparing with C-PCFG, which is trained solely on captions, VC-PCFG achieves a much higher mean F1 (+5.7% F1), demonstrating the informativeness of visual groundings. However, VC-PCFG suffers from a larger variance presumably because the joint objective is harder to optimize. Visually grounded contrastive learning (w/o LM) has a mean F1 50.5%. It is further improved to 59.4% when additionally optimizing the language modeling objective. Moreover, we show recall on six frequent constituent labels (NP, VP, PP, SBAR, ADJP, ADVP) in the test captions. Unsurprisingly, VG-NSL is best on NPs because the matching-based reward signals optimize it to focus only on short and concrete NPs (recall 64.3%). It performs poorly on other constituent labels such as VPs (recall 28.1%). In contrast, VC-PCFG exhibits a relatively even performance across constituent labels, e.g., it is most accurate on SBARs and ADVPs and works fairly well on VPs (recall 83.2%). Meanwhile, it improves over C-PCFG for NPs, which are usually short and 'concrete', once again confirming the benefits of using visual groundings. Visually grounded contrastive learning (w/o LM) tends to behave like  \u2020 indicates results reported by Shi et al. (2019). denotes results obtained by running their code. Notice that the results from Shi et al. (2019) are not comparable to ours because they keep punctuation and include trivial sentence-level spans in evaluation. the right branching baseline. Additionally optimizing the language modeling objective brings a huge improvement for NPs (+19.3% recall).", "publication_ref": ["b39", "b23", "b41", "b41"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Analysis", "text": "We analyze model performance for constituents of different lengths (Figure 1). As expected, VG-NSL becomes weaker as constituent length increases, and the drop is very dramatic. C-PCFG and its grounded version VC-PCFG consistently outperform VG-NSL on constituents longer than four tokens and display a more even performance across constituent lengths. Meanwhile, VC-PCFG beats C-PCFG on constituents of length below 5, confirming that visual groundings are beneficial for short spans. We further plot the distribution over constituent length for different phrase types (Figure 2) and find that around 75% constituents in our dataset are shorter than six tokens, and 60% of them are NPs. Thus, it is not surprising that the im- 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.31 0.22 0.13 0.09 0.08 0.06 0.04 provement on NPs, brought by visually grounded learning, has a large impact on the overall performance.\nNext, we analyze induced tree structures. We compare model predictions against gold trees, left branching trees, and right branching trees. As there is little performance difference between corpus-level F1 and sentence-level F1, we focus on sentence-level F1 in this analysis. We report self F1 (Williams et al., 2018) Shi et al. (2019). VC-PCFG achieves a mean F1 62.7%, surpassing VG-NSL+HI by 12.1% F1. In Figure 3 we visualize a parse tree predicted by the best run of VC-PCFG. We can see that VC-PCFG identifies most NPs but makes mistakes in PP attachement and consequently fails to identify the VP.", "publication_ref": ["b49", "b41"], "figure_ref": ["fig_1", "fig_3", "fig_4"], "table_ref": []}, {"heading": "Related work", "text": "Grammar Induction has a long history in computational linguistics. Following observations that direct optimization of log-likelihood with the Expectation Maximization algorithm (Lari and Young, 1990) is not effective at producing effective grammars, a number of approaches have been developed, emboding various inductive biases or assumption about the language structure and its relation to surface realizations (Klein and Manning, 2002;Smith and Eisner, 2005;Cohen and Smith, 2009;Spitkovsky et al., 2010). The recent advances in the area have been brought by flexible neural models (Jin et al., 2019;Kim et al., 2019a,b;Drozdov et al., 2019). All these methods, with the exception of Shi et al. (2019), rely solely on text.\nVisually grounded learning is motivated by the observation that natural language is grounded in perceptual experiences (Steels, 1998;Barsalou, 1999;Fincher-Kiefer, 2001;Roy, 2002;Bisk et al., 2020). It has been shown effective in word representation learning (Bruni et al., 2014;Silberer and Lapata, 2014;Lazaridou et al., 2015) and sentence representation learning (Kiela et al., 2018;Bordes et al., 2019). All this work uses visual images as perceptual experience of language and exploits visual semantics derived from images to improve continuous vector representatios of language. In contrast, we induce structured representations, discrete tree structure of language, by using visual groundings. We propose a model for the task within the contrastive learning framework. Learning involves estimating concreteness of spans, which generalizes word-level concreteness (Turney et al., 2011;Kiela et al., 2014).\nIn the vision and machine learning community, unsupervised induction of structured image representations (aka scene graphs or world models) has been receiving increasing attention (Eslami et al., 2016;Burgess et al., 2019;Kipf et al., 2020). However, they typically rely solely on visual signal. An interesting extension of our work would be to consider joint induction of structured representations of images and text while guiding learning by an alignment loss.", "publication_ref": ["b32", "b30", "b43", "b10", "b45", "b19", "b11", "b41", "b46", "b2", "b14", "b37", "b6", "b42", "b33", "b21", "b5", "b48", "b22", "b13", "b7", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented visually-grounded compound PCFGs (VC-PCFGs) that use compound PCFGs and generalize the visually grounded grammar learning framework. VC-PCFGs exploit visual groundings via contrastive learning, with learning signals derived from minimizing an image-text alignment loss. To tackle the issues of misleading and insufficient learning signals from purely agreement-based learning, we propose to complement the image-text alignment loss with a loss defined on unlabeled text. We resort to using compound PCFGs which enables us to complement the alignment loss with a language modeling objective, resulting in a fully-differentiable end-to-end visually grounded learning. We empirically show that our VC-PCFGs are superior to models that are trained only through visually grounded learning or only relying on text.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank anonymous reviewers for their suggestions and comments. The project was supported by the European Research Council (ERC Starting Grant BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Trainable grammars for speech recognition", "journal": "The Journal of the Acoustical Society of America", "year": "1979", "authors": "J K Baker"}, {"ref_id": "b1", "title": "The atoms of language: The mind's hidden rules of grammar", "journal": "", "year": "2008", "authors": "C Mark;  Baker"}, {"ref_id": "b2", "title": "Perceptual symbol systems", "journal": "Behavioral and Brain Sciences", "year": "1999", "authors": "Lawrence W Barsalou"}, {"ref_id": "b3", "title": "", "journal": "", "year": "", "authors": "Yonatan Bisk; Ari Holtzman; Jesse Thomason; Jacob Andreas; Yoshua Bengio; Joyce Chai; Mirella Lapata; Angeliki Lazaridou"}, {"ref_id": "b4", "title": "Deep RNNs encode soft hierarchical syntax", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Terra Blevins; Omer Levy; Luke Zettlemoyer"}, {"ref_id": "b5", "title": "Incorporating visual semantics into sentence representations within a grounded space", "journal": "", "year": "2019", "authors": "Patrick Bordes; Eloi Zablocki; Laure Soulier; Benjamin Piwowarski; Patrick Gallinari"}, {"ref_id": "b6", "title": "Multimodal distributional semantics", "journal": "J. Artif. Int. Res", "year": "2014", "authors": "Elia Bruni; Nam Khanh Tran; Marco Baroni"}, {"ref_id": "b7", "title": "Monet: Unsupervised scene decomposition and representation", "journal": "", "year": "2019", "authors": "P Christopher; Loic Burgess; Nicholas Matthey; Rishabh Watters; Irina Kabra; Matt Higgins; Alexander Botvinick;  Lerchner"}, {"ref_id": "b8", "title": "Two experiments on learning probabilistic dependency grammars from corpora", "journal": "AAAI Press", "year": "1992", "authors": "Glenn Carroll; Eugene Charniak"}, {"ref_id": "b9", "title": "Unsupervised induction of stochastic context-free grammars using distributional clustering", "journal": "", "year": "2001", "authors": "Alexander Clark"}, {"ref_id": "b10", "title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Shay Cohen; Noah A Smith"}, {"ref_id": "b11", "title": "Unsupervised latent tree induction with deep inside-outside recursive auto-encoders", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Andrew Drozdov; Patrick Verga; Mohit Yadav; Mohit Iyyer; Andrew Mccallum"}, {"ref_id": "b12", "title": "Inside-outside and forwardbackward algorithms are just backprop (tutorial paper)", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jason Eisner"}, {"ref_id": "b13", "title": "Attend, infer, repeat: Fast scene understanding with generative models", "journal": "Curran Associates, Inc", "year": "2016", "authors": "S M Ali Eslami; Nicolas Heess; Theophane Weber; Yuval Tassa; David Szepesvari; Geoffrey E Hinton"}, {"ref_id": "b14", "title": "Perceptual components of situation models", "journal": "Memory & Cognition", "year": "2001", "authors": "Rebecca Fincher-Kiefer"}, {"ref_id": "b15", "title": "Understanding the difficulty of training deep feedforward neural networks", "journal": "", "year": "2010", "authors": "Xavier Glorot; Yoshua Bengio"}, {"ref_id": "b16", "title": "An efficient algorithm for easy-first non-directional dependency parsing", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Yoav Goldberg; Michael Elhadad"}, {"ref_id": "b17", "title": "Cooperative learning of disjoint syntax and semantics", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Serhii Havrylov; Germ\u00e1n Kruszewski; Armand Joulin"}, {"ref_id": "b18", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b19", "title": "Unsupervised learning of PCFGs with normalizing flow", "journal": "", "year": "2019", "authors": "Lifeng Jin; Finale Doshi-Velez; Timothy Miller; Lane Schwartz; William Schuler"}, {"ref_id": "b20", "title": "An efficient recognition and syntax-analysis algorithm for context-free languages", "journal": "Coordinated Science Laboratory Report", "year": "1966", "authors": "Tadao Kasami"}, {"ref_id": "b21", "title": "Learning visually grounded sentence representations", "journal": "", "year": "2018", "authors": "Douwe Kiela; Alexis Conneau; Allan Jabri; Maximilian Nickel"}, {"ref_id": "b22", "title": "Improving multi-modal representations using image dispersion: Why less is sometimes more", "journal": "Short Papers", "year": "2014", "authors": "Douwe Kiela; Felix Hill; Anna Korhonen; Stephen Clark"}, {"ref_id": "b23", "title": "Compound probabilistic context-free grammars for grammar induction", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yoon Kim; Chris Dyer; Alexander Rush"}, {"ref_id": "b24", "title": "Unsupervised recurrent neural network grammars", "journal": "Long and Short Papers", "year": "2019", "authors": "Yoon Kim; Alexander Rush; Lei Yu; Adhiguna Kuncoro; Chris Dyer; G\u00e1bor Melis"}, {"ref_id": "b25", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b26", "title": "Semisupervised learning with deep generative models", "journal": "", "year": "2014", "authors": "Shakir Durk P Kingma; Danilo Mohamed; Max Jimenez Rezende;  Welling"}, {"ref_id": "b27", "title": "Advances in Neural Information Processing Systems", "journal": "Curran Associates, Inc", "year": "", "authors": "K Q Lawrence;  Weinberger"}, {"ref_id": "b28", "title": "Contrastive learning of structured world models", "journal": "", "year": "2020", "authors": "Thomas Kipf; Elise Van Der Pol; Max Welling"}, {"ref_id": "b29", "title": "Constituency parsing with a self-attentive encoder", "journal": "Long Papers", "year": "2018", "authors": "Nikita Kitaev; Dan Klein"}, {"ref_id": "b30", "title": "A generative constituent-context model for improved grammar induction", "journal": "", "year": "2002", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b31", "title": "What is learned in visually grounded neural syntax acquisition", "journal": "", "year": "2020", "authors": "Noriyuki Kojima; Hadar Averbuch-Elor; Alexander Rush; Yoav Artzi"}, {"ref_id": "b32", "title": "The estimation of stochastic context-free grammars using the insideoutside algorithm", "journal": "Computer Speech and Language", "year": "1990", "authors": "K Lari; S J Young"}, {"ref_id": "b33", "title": "Hubness and pollution: Delving into cross-space mapping for zero-shot learning", "journal": "Long Papers", "year": "2015", "authors": "Angeliki Lazaridou; Georgiana Dinu; Marco Baroni"}, {"ref_id": "b34", "title": "The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization", "journal": "", "year": "2015", "authors": "Phong Le; Willem Zuidema"}, {"ref_id": "b35", "title": "Microsoft coco: Common objects in context", "journal": "Springer International Publishing", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b36", "title": "Asymptotically subminimax solutions of compound statistical decision problems", "journal": "University of California Press", "year": "1951", "authors": "Herbert Robbins"}, {"ref_id": "b37", "title": "Learning visually grounded words and syntax for a scene description task", "journal": "Computer Speech and Language", "year": "2002", "authors": "Deb K Roy"}, {"ref_id": "b38", "title": "Torch-struct: Deep structured prediction library", "journal": "", "year": "2020", "authors": "Alexander Rush"}, {"ref_id": "b39", "title": "Neural language modeling by jointly learning syntax and lexicon", "journal": "", "year": "2018", "authors": "Yikang Shen; Zhouhan Lin; Chin Wei Huang; Aaron Courville"}, {"ref_id": "b40", "title": "Ordered neurons: Integrating tree structures into recurrent neural networks", "journal": "", "year": "2019", "authors": "Yikang Shen; Shawn Tan; Alessandro Sordoni; Aaron Courville"}, {"ref_id": "b41", "title": "Visually grounded neural syntax acquisition", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Haoyue Shi; Jiayuan Mao; Kevin Gimpel; Karen Livescu"}, {"ref_id": "b42", "title": "Learning grounded meaning representations with autoencoders", "journal": "Long Papers", "year": "2014", "authors": "Carina Silberer; Mirella Lapata"}, {"ref_id": "b43", "title": "Guiding unsupervised grammar induction using contrastive estimation", "journal": "", "year": "2005", "authors": "A Noah; Jason Smith;  Eisner"}, {"ref_id": "b44", "title": "Parsing with compositional vector grammars", "journal": "Long Papers", "year": "2013", "authors": "Richard Socher; John Bauer; Christopher D Manning; Andrew Y Ng"}, {"ref_id": "b45", "title": "Viterbi training improves unsupervised dependency parsing", "journal": "", "year": "2010", "authors": "I Valentin; Hiyan Spitkovsky; Daniel Alshawi; Christopher D Jurafsky;  Manning"}, {"ref_id": "b46", "title": "The origins of syntax in visually grounded robotic agents", "journal": "Artificial Intelligence", "year": "1998", "authors": "Luc Steels"}, {"ref_id": "b47", "title": "A minimal span-based neural constituency parser", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Mitchell Stern; Jacob Andreas; Dan Klein"}, {"ref_id": "b48", "title": "Literal and metaphorical sense identification through concrete and abstract context", "journal": "", "year": "2011", "authors": "Peter Turney; Yair Neuman; Dan Assaf; Yohai Cohen"}, {"ref_id": "b49", "title": "Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association for Computational Linguistics", "journal": "", "year": "2018", "authors": "Adina Williams; Andrew Drozdov; Samuel R Bowman"}, {"ref_id": "b50", "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "journal": "", "year": "1992", "authors": "Ronald J Williams"}, {"ref_id": "b51", "title": "Recognition and parsing of context-free languages in time n3", "journal": "Information and Control", "year": "1967", "authors": "H Daniel;  Younger"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": ") is the matching function measuring similarity between the constituent representation c and the image representation v. The expectation is taken with respect to 'negative examples', c and v . In practice, for efficiency reasons, a single representation of an image v and a single representation of a constituent (span) c from another example in the same batch are used as the negative examples. Intuitively, an aligned image-constituent pair (c", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Recall broken down by constituent length.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Label distribution over constituent length.All denotes frequencies of constituent lengths. Zero frequencies are due to the limited numerical precision.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Upper: A parse output by the best run of VC-PCFG. Bottom: The corresponding gold tree.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "\u00b10.5 18.4 \u00b10.4 24.4 \u00b10.3 17.7 \u00b11.7 26.8 \u00b12.6 20.9 \u00b11.5 24.2 \u00b10.3 24.6 \u00b10.2 C-PCFG 43.0 \u00b18.6 85.0 \u00b12.6 78.4 \u00b15.6 90.6 \u00b12.1 36.6 \u00b121 87.4 \u00b11.0 53.6 \u00b14.7 53.7 \u00b14.6 VG-NSL \u2020 79.6 \u00b10.4 26.2 \u00b10.4 42.0 \u00b10.6 22.0 \u00b10.4 50.4 \u00b10.3 VG-NSL+HI \u2020 74.6 \u00b10.5 32.5 \u00b11.5 66.5 \u00b10.2 33.5 \u00b11.6 62.7 \u00b10.6 42.0 \u00b15.1 13.9 \u00b10.6 65.9 \u00b12.5 48.8 \u00b10.4 49.4 \u00b10.5 VC-PCFG (ours) 54.9 \u00b114 83.2 \u00b13.9 80.9 \u00b17.9 89.0 \u00b12.0 38.8 \u00b125 86.3 \u00b14.1 59.3 \u00b18.2 59.4 \u00b18.3 w/o LM 35.6 \u00b13.7 93.4 \u00b12.1 70.1 \u00b12.0 95.9 \u00b13.9 20.6 \u00b10.8 78.0 \u00b12.2 49.7 \u00b12.6 50.5 \u00b12.5", "figure_data": "Model NPVPPPSBARADJPADVPC-F1S-F1Left Branching 33.20.00.10.04.90.015.115.7Right Branching 37.594.571.197.820.979.151.051.8Random Trees 32.8 \u00b11.221.7 \u00b11.153.3 \u00b10.2VG-NSL64.3 \u00b11.1 28.1 \u00b10.5 32.2 \u00b11.1 16.9 \u00b13.2 13.2 \u00b11.55.6 \u00b10.3 41.5 \u00b10.5 41.8 \u00b10.5VG-NSL+HI61.0"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Recall on six frequent constituent labels (NP, VP, PP, SBAR, ADJP, ADVP) in the MSCOCO test captions and corpus-level F1 (C-F1) and sentence-level F1 (S-F1) results. The best mean number in each column is in bold.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "to show model consistency across runs. The self F1 is computed by averaging over six model pairs from four different runs. All results are presented in Table2. Overall, all models have self F1 above 70%, indicating a relatively high consistency. We observe that using the head-initial bias pushes VG-NSL closer to the rightbranching baseline, while visual grounded learning", "figure_data": "Model GoldLeftRightSelfVG-NSL 41.828.320.684.3VG-NSL+HI 49.424.529.288.6C-PCFG 53.71.353.677.3VC-PCFG 59.44.448.571.1Table 2: Average sentence-level F1 results against goldtrees (Gold), left branching trees (Left), right branchingtrees (Right), and self F1 (Self) (Williams et al., 2018).leads to improvements over C-CPFG, forcing VC-PCFG to deviate from the default right-branchingbehaviour.Finally, we test VG-NSL+HI and VC-PCFG on50 manually annotated captions released by"}], "formulas": [{"formula_id": "formula_0", "formula_text": "S A, A \u2208 N , A BC, A \u2208 N , B, C \u2208 N \u222a P, T w, T \u2208 P, w \u2208 \u03a3 .", "formula_coordinates": [2.0, 329.27, 264.49, 174.28, 42.64]}, {"formula_id": "formula_1", "formula_text": "\u03c0 r = g r (z; \u03b8), z \u223c p(z) ,", "formula_coordinates": [2.0, 356.57, 465.6, 119.69, 10.67]}, {"formula_id": "formula_2", "formula_text": "\u03c0 S A = exp(u T A f s ([w S ; z])) A \u2208N exp(u T A f s ([w S ; z])) , \u03c0 A BC = exp(u T BC [w A ; z]) B ,C \u2208N \u222aP exp(u T B C [w A ; z]) , \u03c0 T w = exp(u T w f t ([w T ; z])) w \u2208\u03a3 exp(u T w f t ([w T ; z])) ,", "formula_coordinates": [2.0, 316.67, 547.42, 199.49, 96.18]}, {"formula_id": "formula_3", "formula_text": "log p \u03b8 (w) = log z t\u2208T G (w) p \u03b8 (t|z)p(z) dz ,", "formula_coordinates": [3.0, 86.06, 176.61, 190.15, 23.75]}, {"formula_id": "formula_4", "formula_text": "log p \u03b8 (w) \u2265 ELBO(w; \u03c6, \u03b8) = (1) E q \u03c6 (z|w) [log p \u03b8 (w|z)] \u2212 KL[q \u03c6 (z|w)||p(z)] ,", "formula_coordinates": [3.0, 82.67, 320.56, 207.6, 28.77]}, {"formula_id": "formula_5", "formula_text": "h(c (i) , v (i) ) = E c m(c , v (i) ) \u2212 m(c (i) , v (i) ) + + + E v m(c (i) , v )\u2212m(c (i) , v (i) )+ + ,(2)", "formula_coordinates": [3.0, 307.28, 89.23, 234.33, 44.03]}, {"formula_id": "formula_6", "formula_text": "[\u2022] + = max(0, \u2022), is a positive margin, m(c, v) cos(c, v", "formula_coordinates": [3.0, 307.28, 143.78, 219.63, 23.12]}, {"formula_id": "formula_7", "formula_text": "v (i) ) or (c (i) , v )).", "formula_coordinates": [3.0, 381.69, 290.87, 80.38, 11.76]}, {"formula_id": "formula_8", "formula_text": "s(v (i) , w (i) ) = c (i) \u2208t (i) h(c (i) , v (i) ) .(3)", "formula_coordinates": [3.0, 337.46, 367.57, 188.08, 26.15]}, {"formula_id": "formula_9", "formula_text": "L(V, W; \u03c6, \u03b8) = i s(v (i) , w (i) ) . (4", "formula_coordinates": [4.0, 339.78, 202.13, 181.52, 24.58]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [4.0, 521.3, 204.98, 4.24, 9.46]}, {"formula_id": "formula_11", "formula_text": "s(v, w) = E p \u03b8 (t|w) c\u2208t h(c, v) ,(5)", "formula_coordinates": [4.0, 345.94, 347.99, 179.61, 21.75]}, {"formula_id": "formula_12", "formula_text": "s(v, w) = t\u2208T G (w) p \u03b8 (t|w) c\u2208t h(c, v) = c\u2208w t\u2208T G (w) I {c\u2208t} p \u03b8 (t|w) p(c|w): marginal of the span c h(c, v) = c\u2208w p(c|w)h(c, v) ,(6)", "formula_coordinates": [4.0, 315.14, 476.23, 210.41, 104.38]}, {"formula_id": "formula_13", "formula_text": "c k = f k ( 1 j \u2212 i + 1 j l=i h l ) .(7)", "formula_coordinates": [5.0, 120.18, 332.65, 170.09, 34.56]}, {"formula_id": "formula_14", "formula_text": "c = K k=1 p(k|c, w)c k ,(8)", "formula_coordinates": [5.0, 132.17, 589.13, 158.1, 33.98]}, {"formula_id": "formula_15", "formula_text": "L(W; \u03c6, \u03b8) = \u2212 w\u2208W ELBO(w; \u03c6, \u03b8) . (9", "formula_coordinates": [5.0, 324.65, 209.1, 196.65, 22.26]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 521.3, 209.45, 4.24, 9.46]}, {"formula_id": "formula_17", "formula_text": "J (\u03c6, \u03b8) = L(W; \u03c6, \u03b8) + \u03b1 \u2022 L(V, W; \u03c6, \u03b8) , (10", "formula_coordinates": [5.0, 307.93, 385.09, 213.07, 9.81]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 521.0, 385.44, 4.54, 9.46]}, {"formula_id": "formula_19", "formula_text": "t * = argmax z p \u03b8 (t|w, z)p \u03b8 (z|w) dz .", "formula_coordinates": [5.0, 329.87, 544.78, 173.08, 21.39]}, {"formula_id": "formula_20", "formula_text": "t * \u2248 argmax z p \u03b8 (t|w, z)\u03b4(z \u2212 \u00b5 \u03c6 (w)) dz ,", "formula_coordinates": [5.0, 316.3, 665.0, 200.21, 21.39]}], "doi": "10.1121/1.2017061"}