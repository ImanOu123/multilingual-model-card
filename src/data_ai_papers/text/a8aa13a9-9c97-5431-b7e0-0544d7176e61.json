{"title": "Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks", "authors": "Yun Tang; Anna Y Sun; Inaguma \u22c6 Hirofumi; Xinyue Chen; Ning Dong; Xutai Ma; Paden D Tomasello; Juan Pino; Meta Ai", "pub_date": "", "abstract": "Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-totext tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-totext tasks. The new method leverages AED's strength in non-monotonic sequence to sequence learning while retaining Transducer's streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed approach on the MUST-C dataset and the findings demonstrate that TAED performs significantly better than Transducer for offline automatic speech recognition (ASR) and speech-to-text translation (ST) tasks. In the streaming case, TAED outperforms Transducer in the ASR task and one ST direction while comparable results are achieved in another translation direction. 1   ", "sections": [{"heading": "Introduction", "text": "Neural based end-to-end frameworks have achieved remarkable success in speech-to-text tasks, such as automatic speech recognition (ASR) and speech-totext translation (ST) (Li, 2021). These frameworks include Attention based Encoder-Decoder modeling (AED) (Bahdanau et al., 2014), connectionist temporal classification (CTC) (Graves et al., 2006) and Transducer (Graves, 2012) etc. They are designed with different purposes and have quite different behaviors, even though all of them could be used to solve the mapping problem from a speech input sequence to a text output sequence.\nAED handles the sequence-to-sequence learning by allowing the decoder to attend to parts of the source sequence. It provides a powerful and general solution that is not bound to the input/output modalities, lengths, or sequence orders. Hence, it is widely used for ASR (Chorowski et al., 2015;Chan et al., 2015;Gulati et al., 2020;Tang et al., 2021), and ST (Berard et al., 2016;Tang et al., 2022).\nCTC and its variant Transducer are designed to handle monotonic alignment between the speech input sequence and text output sequence. A hard alignment is generated between speech features and target text tokens during decoding, in which every output token is associated or synchronized with an input speech feature. CTC and Transducer have many desired properties for ASR. For example, they fit into streaming applications naturally, and the input-synchronous decoding can help alleviate over-generation or under-generation issues within AED. ;  show that Transducer achieves better WER than AED in long utterance recognition, while AED outperforms Transducer in the short utterance case. On the other hand, CTC and Transducer are shown to be suboptimal in dealing with non-monotonic sequence mapping (Chuang et al., 2021), though some initial attempts show encouraging progress .\nIn this work, we propose a hybrid Transducer and AED model (TAED), which integrates both AED and Transducer models into one framework to leverage strengths from both modeling methods. In TAED, we share the speech encoder between AED and Transducer. The predictor in Transducer is replaced with the decoder in AED. The AED decoder output assists the Transducer's joiner to predict the output tokens. Transducer and AED models are treated equally and optimized jointly during training, while only Transducer's joiner outputs are used during inference. We extend the TAED model to streaming applications under the chunk-based synchronization scheme, which guarantees full coverage of read/write choices in the training set and removes the training and inference discrepancy. The relationship between streaming latency and AED alignment is studied, and a simple, fast AED alignment is proposed to achieve low latency with small quality degradation. The new approach is evaluated in ASR and ST tasks for offline and streaming settings. The results show the new method helps to achieve new state-of-the-art results on offline evaluation. The corresponding streaming extension also improves the quality significantly under a similar latency budget. To summarize, our contributions are below:\n1. TAED, the hybrid of Transducer and AED modeling, is proposed for speech-to-text tasks 2. A chunk-based streaming synchronization scheme is adopted to remove the training and inference discrepancy for streaming applications 3. A simple, fast AED alignment is employed to balance TAED latency and quality 4. The proposed method achieves SOTA results on both offline and streaming settings for ASR and ST tasks", "publication_ref": ["b21", "b3", "b16", "b15", "b11", "b5", "b17", "b41", "b4", "b40", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary", "text": "Formally, we denote a speech-to-text task training sample as a (x, y) pair. x = x 1:T and y = y 1:U are the speech input features and target text tokens, respectively. T and U are the corresponding sequence lengths. y u \u2208 V and V is the target vocabulary. The objective function is to minimize the negative log likelihood log p(y|x, \u03b8) over the training set\nL aed = \u2212 (x,y) log p(y u |y 1:u\u22121 , x 1:T ). (1)\nIn the streaming setting, the model generates predictions at timestamps denoted by a = (t 1 , \u2022 \u2022 \u2022 , t u , \u2022 \u2022 \u2022 , t U ), rather than waiting to the end of an utterance, where t u \u2264 t u+1 and 0 < t u \u2264 T . We call the prediction timestamp sequence as an alignment a between speech x 1:T and token labels y 1:U . A U T = {a} denotes all alignments between x 1:T and y 1:U . The streaming model parameter \u03b8 s is optimized through\nmin \u03b8s (x,y) a\u2208A U T U u=1 \u2212 log p(y u |y 1:u\u22121 , x 1:tu ). (2)\nThe offline modeling can be considered a special case of streaming modeling, i.e., the alignment is unique with all t u = T . The following two subsections briefly describe two modeling methods used in our hybrid approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Attention based encoder decoder", "text": "AED consists of an encoder, a decoder, and attention modules, which connect corresponding layers in the encoder and decoder as demonstrated in Figure 1(a). The encoder generates the context representation h 1:T from input x 1:T 2 . The decoder state s l u is estimated based on previous states and encoder outputs\ns l u = f \u03b8 dec (h 1:T , s l 1:u\u22121 , y u\u22121 ),(3)\nwhere f \u03b8 dec is the neural network parameterized with \u03b8 dec and l \u2208 [1, L] is the layer index.\nWhen the AED is extended to the streaming applications Arivazhagan et al., 2019), a critical question has been raised: how do we decide the write/read strategy for the decoder?\nAssuming the AED model is Transformer based, and tokens y 1:u\u22121 have been decoded before timestep t during inference. The next AED decoder state s l u (t) is associated with partial speech encoder outputs h 1:t as well as a partial alignment a \u2032 \u2208 A u\u22121 t between h 1:t and y 1:u\u22121 . The computation of a Transformer decoder layer (Vaswani et al., 2017) includes a self-attention module and a cross-attention module. The self-attention module models the relevant information from previous decoder state\u015d\ns l a \u2032 = [s l 1 (t 1 ), \u2022 \u2022 \u2022 , s l u\u22121 (t u\u22121 )],(4)\nwhere t u\u22121 is the prediction timestamp for token u \u2212 1 in alignment a \u2032 . The cross-attention module  extracts information from the encoder outputs h 1:t . The decoder state computation is modified as\ns l u (t) = f \u03b8 dec (h 1:t ,\u015d l a \u2032 , y u\u22121 ). (5\n)\nTo cover all read/write paths during training, we need to enumerate all possible alignments at every timestep given the output token sequence y 1:U . The alignment numbers would be O(\nT \u2032 !(T \u2032 \u2212U )! U !\n) and it is prohibitively expensive. In AED based methods, such as Monotonic Infinite Lookback Attention (MILk) (Arivazhagan et al., 2019) and Monotonic Multihead Attention (MMA) (Ma et al., 2020b), an estimation of context vector is used to avoid enumerating alignments. In Cross Attention Augmented Transducer (CAAT) (Liu et al., 2021), the self-attention modules in the joiner are dropped to decouple y 1:u\u22121 and h 1:t .", "publication_ref": ["b2", "b42", "b2", "b27", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Transducer", "text": "A Transducer has three main components. A speech encoder \u03b8 enc forms the context speech representation h 1:T from speech input x 1:T , a predictor \u03b8 pred models the linguistic information conditioned on previous target tokens, and a joiner \u03b8 joiner merges acoustic and linguistic representations to predict outputs for every speech input feature, as shown in 1(b). The encoder and predictor are usually modeled with a recurrent neural network (RNN) (Graves, 2012) or Transformer  architecture. The joiner module is a feed-forward network which expands input from speech encoder h t and predictor output s L u to a T \u00d7 U matrix with component z(t, u):\nz(t, u) = f \u03b8 joiner (h t , s L u ).(6)\nA linear projection W out \u2208 R d\u00d7|V\u222a\u2205| is applied to z(t, u) to obtain logits for every output token k \u2208 V \u222a \u2205. A blank token \u2205 is generated if there is no good match between non-blank tokens and current h t . The RNN-T loss is optimized using the forward-backward algorithm:\n\u03b1 t,u = LA \u03b1 t,u\u22121 + log p y u |z(t, u \u2212 1) , \u03b1 t\u22121,u + log p \u2205|z(t, u) ,(7)\nL rnn\u2212t = \u2212\u03b1 T,U \u2212 log p(\u2205|T, U ),(8)\nwhere LA(x, y) = log(exp x + exp y ) and \u03b1 0,\u03d5 is initialized as 0. Transducer is well suited to the streaming task since it can learn read/write policy from data implicitly, i.e., a blank token indicates a read operation and a non-blank token indicates a write operation.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "In this study, we choose the Transformer-Transducer (T-T)  as the backbone in the proposed TAED system. For the streaming setting, the speech encoder is based on the chunkwise implementation (Chiu and Raffel, 2017;, which receives and computes new speech input data by chunk size N instead of one frame each time.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "TAED", "text": "TAED combines both Transducer and AED into one model, as illustrated in Figure 1(c). The speech Transformer encoder is shared between Transducer and AED models. The predictor in Transducer is replaced by the AED decoder. Outputs of the new predictor are results of both speech encoder outputs and predicted tokens, hence they are more informative for the joiner.\nTransducer and AED models are optimized together with two criteria, RNN-T loss for the Transducer's joiner outputs and cross entropy loss for the AED decoder outputs. The overall loss L taed is summation of two losses\nL tead = L rnn\u2212t + L aed . (9\n)\nThe model is evaluated based on the outputs from the Transducer's joiner.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Transducer optimization with chunkbased RNN-T synchronization scheme", "text": "When we attempt to extend TAED to the streaming scenario, we encounter the same streaming read/write issue discussed in \u00a72.1. In order to avoid enumerating exponential increased alignments, we adopt a different approach and modify the inference logic to match the training and inference conditions.\nIn the conventional streaming decoder inference, when the new speech encoder output h t is available, the new decoder state s l u (t) is estimated via h 1:t and previous computed decoder states\u015d l a \u2032 , which are based on h 1:t \u2032 and t \u2032 \u2264 t, as shown in Eq. (5). In the proposed solution, we update all previous decoder states given speech encoder outputs h 1:t , and\u015d l a \u2032 is replaced by\u015d l a(t) ,\ns l a(t) = [s l 1 (t), \u2022 \u2022 \u2022 , s l u\u22121 (t)],(10)\nwhere a(t) stands for a special alignment where all tokens are aligned to timestamp t u = t. There are two reasons behind this modification. First, we expect the state representation would be more accurate if all decoder states are updated when more speech data is available. Second, the modification helps to reduce the huge number of alignments between y 1:U and h 1:t to one during training, i.e., a(t). Compared with the conventional AED training, it only increases the decoder forward computation by T times.\nThe computation is further reduced when the chunk-based encoder is used. Given two decoder states s l u (t) and s l u (\u03b4(t)), where \u03b4(t) is the last frame index of the chunk which frame t belongs to and t \u2264 \u03b4(t), s l u (\u03b4(t)) is more informative than s l u (t) since the former is exposed to more speech input. During inference, s l u (t) and s l u (\u03b4(t)) are available at the same time when the speech chunk data is available. Therefore, we replace all s l u (t) with corresponding s l u (\u03b4(t)) for both inference and training. If N is the number of speech encoder output frames from one chunk of speech input, chunk-based computation helps to reduce the decoder computation cost by N times during training, since we only need to update the decoder states every N frames instead of every frame. In summary, the computation of s l u (t) is modified from Eq. (5) as below\ns l u (t) = f \u03b8 dec (h 1:\u03b4(t) ,\u015d l a(\u03b4(t)) , y u\u22121 ),(11)\nand the joiner output z(t, u) in Eq. ( 6) is updated as\nz(t, u) = f \u03b8 joiner (h t , s L u (t)).(12)\nThe chunk-based RNN-T synchronization is depicted in Figure 2. The number of speech encoder output frames in one chunk is 4. z(t, u)s in the first chunk (cyan) are calculated with s L u (3) and the second chunk (violet) is based on s L u (7). When the chunk size N is longer than the utterance length T , the chunk-based TAED streaming training is the same as the offline training with similar computation cost as Transducer.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "AED optimization with fast alignment", "text": "Besides optimizing the model with the RNN-T loss aforementioned, we also include another auxiliary loss L aed for the AED model, as shown in Eq. (9). A straightforward approach is to optimize the AED modules as an offline AED model as Eq. (1). However, an offline AED model could lead to high latency if it is used for streaming applications. Hence, we also introduce a simple \"streaming\"-mode AED training by creating an even alignment a e of the target tokens against the speech encoder outputs, i.e., t e u = \u230au * T \u2032 /U \u230b, where \u230ax\u230b is the floor operation on x. Furthermore, we can manipulate the alignment pace with an alignment speedup factor \u03bb > 0, and the new alignment a \u03bb is with timestep t \u03bb u = max(T, \u230a u * T \u2032 U * \u03bb \u230b). When \u03bb > 1.0, the streaming AED model is trained with a fast alignment and is encouraged to predict new tokens with less speech data. On the other hand, if \u03bb < 1 U , then t \u03bb u = T and it is equivalent to the offline training. The auxiliary AED task is optimized via\nL aed = \u2212 (x,y) u log p(y u |y 1:u\u22121 , h 1:t \u03bb u ). (13)\nNote an accurate alignment is not required in this approach, which could be difficult to obtain in translation-related applications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Blank penalty during inference", "text": "The ratio between the number of input speech frames and the number of target tokens could be varied due to many factors, such as different speech ratios or target language units. In the meantime, the prediction of a blank token \u2205 indicates a read operation, and a non-blank token represents a write operation, as discussed in \u00a72.2. During inference, a blank penalty \u03c4 is introduced to adjust the target token fertility rate by penalizing the blank token \u2205 emission probability. It acts as word insertion penalty used in ASR (Takeda et al., 1998):\ne(t, u)[i \u2205 ] = e(t, u)[i \u2205 ] \u2212 \u03c4,(14)\nwhere e(t, u) = LOGSOFTMAX u (W out z(t, u)) and i \u2205 is the index for the blank token \u2205.", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison of streaming algorithms", "text": "When to read new input and write new output is a fundamental question for the streaming algorithm.\nBased on the choices of streaming read/write policies, they can roughly be separated into two families: pre-fixed and adaptive. The pre-fixed policy, such as Wait-k (Ma et al., 2019), adopts a fixed scheme to read new input and write new output.\nOn the other hand, the adaptive algorithms choose read/write policies dynamically based on the input speech data presented. The adaptive algorithms could be further separated into two categories based on input and output synchronization. The first category of adaptive streaming algorithms is based on the AED framework, including hard monotonic attention (HMA) , MILk (Arivazhagan et al., 2019), MoChA (Chiu and Raffel, 2017), MMA (Ma et al., 2020b) and continuous integrate-and-fire (CIF) (Dong and Xu, 2020;Chang and yi Lee, 2022). Those methods extract acoustic information from the encoder outputs via attention between the encoder and decoder. The acoustic information is fused with linguistic information, which is estimated from the decoded token history, within the decoder. There is no explicit alignment between the input and output sequence; in other words, the outputs are asynchronized for the inputs. As discussed in \u00a72.1, AED models don't fit the streaming application easily, and approximations have been taken during training. For example, the alignmentdependent context vector extracted via attention between the encoder and decoder is usually replaced by a context vector expectation from alignments. It differs from inference, which is based on a specific alignment path sampled during decoding. Hence a training and inference discrepancy is inevitable, potentially hurting the streaming performance.\nThe second category of adaptive streaming methods is with synchronized inputs and outputs, in which every output token is associated with a speech input frame. This includes CTC, Transducer, CAAT, and the proposed TAED. They combine acoustic and linguistic information within the joiner if linguistic modeling is applied. Specific read/write decisions are not required during training. This considers all alignments and is optimized via CTC loss or RNN-T loss. Hence, there is no training and inference discrepancy. The detailed comparison of different methods is listed in Table 1.", "publication_ref": ["b25", "b2", "b9", "b27", "b13", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "Data Experiments are conducted on two MUST-C (Gangi et al., 2019) language pairs: English to German (EN\u2192DE) and English to Spanish (EN\u2192ES). Sequence level knowledge distillation (Kim and Rush, 2016) is applied to boost the ST quality (Liu et al., 2021). The English portion of data in the EN\u2192ES direction is used for English ASR development and evaluation. The models are developed on the dev set, and the final results are reported on the tst-COMMON set. We also report LIBRISPEECH (Panayotov et al., 2015)   We choose the Transformer-Transducer (T-T)  as our Transducer baseline model. The speech encoder starts with two casual convolution layers with a kernel size of three and a stride size of two. The input speech features are down-sampled by four and then processed by 16 chunk-wise Transformer layers with relative positional embedding (Shaw et al., 2018). For the streaming case, the speech encoder can access speech data in all chunks before and one chunk ahead of the current timestep Shi et al., 2020;Liu et al., 2021). We sweep over chunk size from 160ms to 640ms. For the offline model, we simply set a chunk size larger than any utterance to be processed as discussed in \u00a73.2. There are two Transformer layers in the predictor module. The Transformer layers in both the speech encoder and predictor have an input embedding size of 512, 8 attention heads, and middle layer dimension 2048. The joiner module is a feed-forward neural network as T-T . The TAED follows the same configuration as the T-T baseline, except the predictor module is replaced by an AED decoder with extra attention modules to connect the outputs from the speech encoder. The total number of pa-  rameters is approximately 59M for both Transducer and TAED configurations.\nHyper-parameter setting The model is pretrained with the ASR task using the T-T architecture. The trained speech encoder is used to initialize the TAED models and the T-T based ST model. The models are fine-tuned up to 300k updates using 16 A100 GPUs. The batch size is 16k speech frames per GPU. It takes approximately one day to train the offline model and three days for the streaming model due to the overhead of the lookahead chunk and chunk-based synchronization scheme. Early stopping is adopted if the training makes no progress for 20 epochs. The RAdam optimizer  with a learning rate 3e-4 is employed in all experiments. Label smoothing and dropout rate are both set to 0.1. We choose blank penalty \u03c4 by grid search within [0, 4.0] with step=0.5 on the dev set. The models are trained with FAIRSEQ . The best ten checkpoints are averaged for inference with greedy search (beam size=1).", "publication_ref": ["b14", "b19", "b23", "b30", "b37", "b38", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Offline results", "text": "The results for the offline models are listed in Table 2 and Table 3. In  The results indicate Transducer can achieve competitive results with the AED based model in the ST task. A predictor conditioned with speech encoder outputs could provide a more accurate representation for the joiner. The TAED can take advantage of both the Transducer and AED and achieve better results.\nIn the next experiment, we compare the impact of the AED task weight for the offline model. In Eq. ( 9), the RNN-T loss and AED cross entropy loss are added to form the overall loss during training. In Table 4, we vary the AED task weight during training from 0.0 to 2.0. The 2nd, 3rd, and 4th columns correspond to the AED task weight, ASR WER, and ST BLEU in the \"EN\u2192ES\" direction, respectively. AED weight 0.0 indicates only RNN-T loss is used while AED weight = 1.0 is equivalent to the proposed mothed in Eq. (9). Without extra guidance from the AED task (AED weight=0.0), the models still outperform the Transducer models in both ASR and ST tasks, though the gain is halved. When the AED task is introduced during training, i.e., AED weight is above 0, we get comparable results for three AED weights: 0.5, 1.0, and 2.0. This demonstrates that the AED guidance is essential, and the task weight is not very sensitive for the final results. In the following streaming experiments, we follow Eq. (9) without changing the AED task weight.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_7"]}, {"heading": "Streaming results", "text": "We first study the impact of the AED alignment speedup factor described in \u00a73.3 in Table 5 and Ta  ble 6. In those experiments, the chunk size is set to 320ms. The ASR results are presented in Table 5.\nThe first row indicates the alignment speedup factor \u03bb. \"Full\" means the AED model is trained as an offline ST model. \"1.0\" stands for the alignment created by evenly distributing tokens along the time axis. The streaming TAED model trained with the offline AED model (\"Full\") achieves 12.7 WER with a large latency. We examine the decoding and find the model tends to generate the first non-blank token near the end of the input utterance.\nThe joiner learns to wait to generate reliable outputs at the end of utterances and tends to ignore the direct speech encoder outputs. When the fast AED alignment is adopted, i.e., \u03bb \u2265 1.0, the latency is reduced significantly from almost 6 seconds to less than 1 second. The larger \u03bb is, the smaller AL is. One surprising finding is that both WER and latency become smaller when \u03bb increases. The WER improves from 14.7 to 12.5 when \u03bb increases from 1.0 to 1.2, slightly better than the TAED trained with the offline AED module. We hypothesize that the joiner might achieve better results if it gets a synchronized signal from both the speech encoder and AED decoder outputs. When \u03bb is small, i.e., 1.0, AED decoder output might be lagged behind the speech encoder output when they are used to predict the next token.\nA similar observation is also found in ST as demonstrated in Table 6 that the fast AED alignment helps to reduce TAED latency, though the best BLEU are achieved when the offline AED module is used. Compared to TAED models trained with offline AED module, the latency is reduced from 4+ seconds to less than 1.2 seconds for both translation directions, at the expense of BLEU score decreasing from 0.9 (EN\u2192ES) to 1.8 (EN\u2192DE).\nIn the following experiments, we compare the quality v.s. latency for TAED and Transducer. We   build models with different latency by changing the chunk size from 160, 320, and 480 to 640 ms. We present the WER v.s. AL curve in Figure 3. The dash lines are the WERs from the offline models, and the solid lines are for the streaming models.\nThe figure shows that the proposed TAED models achieve better WER than the corresponding Transducer model, varied from 1.2 to 2.1 absolute WER reduction, with similar latency values. The BLEU v.s. AL curves for ST are demonstrated in Figure 4 and Figure 5 for EN\u2192ES and EN\u2192DE directions, respectively. Besides the results from Transducer and TAED, we also include CAAT results from Liu et al. (2021) for convenient comparison. First, TAED consistently outperforms Transducer at different operation points in the EN\u2192ES direction and is on par with Transducer in the EN\u2192DE direction. We expect the TAED model outperforms the Transducer model for the EN\u2192DE direction when more latency budget is given since the offline TAED model is clearly better than the corresponding offline Transducer model. Second, CAAT performs better at the extremely low latency region (\u223c 1 second AL), and TAED starts to excel CAAT when AL is beyond 1.1 seconds for EN\u2192ES and 1.3 seconds for EN\u2192DE. TAED achieves higher BLEU scores than the offline CAAT model when the latency is more than 1.4 seconds for both directions. The detailed results are included in Appendix B.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": ["tab_9", "tab_9", "tab_10"]}, {"heading": "Related work", "text": "Given the advantages and weaknesses of AED and CTC/ Transducer, many works have been done to combine those methods together. Transducer with attention (Prabhavalkar et al., 2017), which is a Transducer variant, also feeds the encoder outputs to the predictor. Our method is different in two aspects. First, we treat the TAED as a combination of two different models: Transducer and AED. They are optimized with equal weights during training, while Transducer with attention is optimized with RNN-T loss only. It is critical to achieve competitive results as shown in Table 4. Second, our method also includes a streaming solution while Transducer with attention can only be applied to the offline modeling.\nAnother solution is to combine those two methods through a two-pass approach (Watanabe et al., 2017;Moriya et al., 2021).\nThe first pass obtains a set of complete hypotheses using beam search. The second pass model rescores these hypotheses by combining likelihood scores from both models and returns the result with the highest score. An improvement along this line of research replaces the two-pass decoding with single-pass decoding, which integrates scores from CTC/Transducer with AED during the beam search (Watanabe et al., 2017;Yan et al., 2022). However, sophisticated decoding algorithms are required due to the synchronization difference between two methods. They also lead to high computation cost and latency (Yan et al., 2022). Furthermore, the two-pass approach doesn't fit streaming applications naturally. Heuristics methods such as triggered decoding are employed (Moritz et al., 2019;Moriya et al., 2021). In our proposed solution, two models are tightly integrated with native streaming support, and TAED predictions are synergistic results from two models.", "publication_ref": ["b34", "b45", "b29", "b45", "b49", "b49", "b28", "b29"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Conclusion", "text": "In this work, we propose a new framework to integrate Transducer and AED models into one model. The new approach ensures that the optimization covers all read/write paths and removes the discrepancy between training and evaluation for streaming applications. TAED achieves better results than the popular AED and Transducer modelings in ASR and ST offline tasks. Under the streaming scenario, the TAED model consistently outperforms the Transducer baseline in both the EN ASR task and EN\u2192ES ST task while achieving comparable results in the EN\u2192DE direction. It also excels the SOTA streaming ST system (CAAT) in medium and large latency regions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The TAED model has slightly more parameters than the corresponding Transducer model due to the attention modules to connect the speech encoder and AED decoder. They have similar training time for the offline models. However, the optimization of the streaming model would require more GPU memory and computation time due to the chunk-based RNN-T synchronization scheme described in \u00a72.1. In our experiments, the streaming TAED model takes about three times more training time than the offline model on the 16 A100 GPU cards, each having 40GB of GPU memory.\nIn this work, we evaluate our streaming ST al-   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Statistics of the MUST-C dataset", "text": "We conduct experiments on the MUST-C (Gangi et al., 2019). The ASR experiments are based on the English portion of data in the EN\u2192ES direction. The ST experiments are conducted in two translation directions: EN\u2192ES and EN\u2192DE. The detailed training data statistics are presented in Table 7. The second column is the total number of hours for the speech training data. The third column is the number of (source) words.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "B Detailed streaming results", "text": "The detailed streaming experimental results are presented in this section. We report different latency metrics from SimulEval toolkit (Ma et al., 2020a), including Average Lagging (AL) (Ma et al., 2019), Average Proportion (AP) (Cho and Esipova, 2016), Differentiable Average Lagging (DAL) (Arivazhagan et al., 2019), and Length Adaptive Average Lagging (LAAL) (Papi et al., 2022 8 and Table 9). BLUE scores are reported for two translation directions in Table 10, Table 11, Table 12 and Table 13.", "publication_ref": ["b26", "b25", "b10", "b2", "b31"], "figure_ref": [], "table_ref": ["tab_13", "tab_1", "tab_1", "tab_1", "tab_1"]}, {"heading": "C Librispeech ASR results", "text": "The model configure is the same as MuST-C experiments in \u00a75.      We conduct experiments on MuST-C dataset and the algorithm is implemented on the top of fairseq as described in section 5 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Both of data (MuST-C) and software (fairseq) are open sourced and widely used by other researchers for speech translation study. Our work is falling into the same category.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? It was discussed in the MuST-C paper.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? section 5 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "In appendix", "text": "The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "12454", "text": "C Did you run computational experiments? section 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? section 5 C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? section 5 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? section 5\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? section 5 D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "International Workshop on Spoken Language Translation", "journal": "", "year": "", "authors": "Antonios Anastasopoulos; Lo\u00efc Barrault; Luisa Bentivogli;  Marcely Zanon; Ond\u0159ej Boito; Roldano Bojar; Anna Cattoni;  Currey"}, {"ref_id": "b1", "title": "2021. Findings of the IWSLT 2021 evaluation campaign", "journal": "", "year": "", "authors": "Antonios Anastasopoulos; Ondrej Bojar; Jacob Bremerman; Roldano Cattoni; Maha Elbayad; Marcello Federico; Xutai Ma; Satoshi Nakamura; Matteo Negri; Jan Niehues; Juan Miguel Pino; Elizabeth Salesky; Sebastian St\u00fcker; Katsuhito Sudoh; Marco Turchi; Alexander H Waibel; Changhan Wang; Matthew Wiesner"}, {"ref_id": "b2", "title": "Monotonic infinite lookback attention for simultaneous machine translation", "journal": "", "year": "2019", "authors": "N Arivazhagan; Colin Cherry; Wolfgang Macherey; Chung-Cheng Chiu; Semih Yavuz; Ruoming Pang; Wei Li; Colin Raffel"}, {"ref_id": "b3", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b4", "title": "Listen and translate: A proof of concept for end-toend speech-to-text translation", "journal": "", "year": "2016", "authors": "A Berard; O Pietquin; C Servan; L Besacier"}, {"ref_id": "b5", "title": "Listen, attend and spell", "journal": "ArXiv", "year": "2015", "authors": "William Chan; Navdeep Jaitly; Quoc V Le; Oriol Vinyals"}, {"ref_id": "b6", "title": "Exploring continuous integrate-and-fire for adaptive simultaneous speech translation", "journal": "", "year": "2022", "authors": "Chih-Chiang Chang; Hung Yi Lee"}, {"ref_id": "b7", "title": "Developing real-time streaming transformer transducer for speech recognition on largescale dataset", "journal": "", "year": "2020", "authors": "Xie Chen; Yu Wu; Zhenghao Wang; Shujie Liu; Jinyu Li"}, {"ref_id": "b8", "title": "A comparison of end-to-end models for long-form speech recognition", "journal": "", "year": "2019", "authors": "Chung-Cheng Chiu; Wei Han; Yu Zhang; Ruoming Pang; Sergey Kishchenko; Patrick Nguyen; Arun Narayanan; Hank Liao; Shuyuan Zhang; Anjuli Kannan; Rohit Prabhavalkar; Z Chen; Tara N Sainath; Yonghui Wu"}, {"ref_id": "b9", "title": "Monotonic chunkwise attention", "journal": "", "year": "2017", "authors": "Chung-Cheng Chiu; Colin Raffel"}, {"ref_id": "b10", "title": "Can neural machine translation do simultaneous translation? ArXiv", "journal": "", "year": "2016", "authors": "Kyunghyun Cho; Masha Esipova"}, {"ref_id": "b11", "title": "Attention-based models for speech recognition", "journal": "", "year": "2015", "authors": "J Chorowski; D Bahdanau; D Serdyuk; K Cho; Y Bengio"}, {"ref_id": "b12", "title": "Investigating the reordering capability in CTC-based non-autoregressive end-to-end speech translation", "journal": "", "year": "2021", "authors": " Shun-Po; Yung-Sung Chuang; Chih-Chiang Chuang; Hung Chang;  Yi Lee"}, {"ref_id": "b13", "title": "CIF: Continuous integrate-and-fire for end-to-end speech recognition", "journal": "", "year": "2020", "authors": "Linhao Dong; Bo Xu"}, {"ref_id": "b14", "title": "MuST-C: a multilingual speech translation corpus", "journal": "", "year": "2019", "authors": "Mattia Antonino Di Gangi; Roldano Cattoni; Luisa Bentivogli; Matteo Negri; Marco Turchi"}, {"ref_id": "b15", "title": "Sequence transduction with recurrent neural networks", "journal": "ArXiv", "year": "2012", "authors": "Alex Graves"}, {"ref_id": "b16", "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "journal": "", "year": "2006", "authors": "Alex Graves; Santiago Fern\u00e1ndez; Faustino J Gomez; J\u00fcrgen Schmidhuber"}, {"ref_id": "b17", "title": "Conformer: Convolution-augmented transformer for speech recognition", "journal": "", "year": "2020", "authors": "Anmol Gulati; James Qin; Chung-Cheng Chiu; Niki Parmar; Yu Zhang; Jiahui Yu; Wei Han; Shibo Wang; Zhengdong Zhang; Yonghui Wu; Ruoming Pang"}, {"ref_id": "b18", "title": "ESPnet-ST: Allin-one speech translation toolkit", "journal": "", "year": "2020", "authors": "H Inaguma; S Kiyono; K Duh; S Karita; N Soplin; T Hayashi; S Watanabe"}, {"ref_id": "b19", "title": "Sequencelevel knowledge distillation", "journal": "", "year": "2016", "authors": "Yoon Kim; Alexander M Rush"}, {"ref_id": "b20", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "", "year": "2018", "authors": "T Kudo; J Richardson"}, {"ref_id": "b21", "title": "Recent advances in end-to-end automatic speech recognition", "journal": "APSIPA Transactions on Signal and Information Processing", "year": "2021", "authors": "Jinyu Li"}, {"ref_id": "b22", "title": "Alexis Conneau, and Michael Auli. 2021. Multilingual speech translation from efficient finetuning of pretrained models", "journal": "", "year": "", "authors": "Xian Li; Changhan Wang; Yun Tang; C Tran; Yuqing Tang; Juan Miguel Pino; Alexei Baevski"}, {"ref_id": "b23", "title": "Cross attention augmented transducer networks for simultaneous translation", "journal": "", "year": "2021", "authors": "Dan Liu; Mengge Du; Xiaoxi Li; Ya Li; Enhong Chen"}, {"ref_id": "b24", "title": "On the variance of the adaptive learning rate and beyond", "journal": "", "year": "2020", "authors": "Liyuan Liu; Haoming Jiang; Pengcheng He; Weizhu Chen; Xiaodong Liu; Jianfeng Gao; Jiawei Han"}, {"ref_id": "b25", "title": "STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework", "journal": "", "year": "2019", "authors": "Mingbo Ma; Liang Huang; Hao Xiong; Renjie Zheng; Kaibo Liu; Baigong Zheng; Chuanqiang Zhang; Zhongjun He; Hairong Liu; Xing Li; Hua Wu; Haifeng Wang"}, {"ref_id": "b26", "title": "SIMULEVAL: An evaluation toolkit for simultaneous translation", "journal": "", "year": "2020", "authors": "Xutai Ma; Mohammad Javad Dousti; Changhan Wang; Jiatao Gu; Juan Pino"}, {"ref_id": "b27", "title": "Monotonic multihead attention", "journal": "", "year": "2020", "authors": "Xutai Ma; Juan Pino; James Cross; Liezl Puzon; Jiatao Gu"}, {"ref_id": "b28", "title": "Triggered attention for end-to-end speech recognition", "journal": "", "year": "2019", "authors": "Niko Moritz; Takaaki Hori; Jonathan Le Roux"}, {"ref_id": "b29", "title": "Streaming end-to-end speech recognition for hybrid RNN-T/attention architecture", "journal": "", "year": "2021", "authors": "Takafumi Moriya; Tomohiro Tanaka; Takanori Ashihara; Tsubasa Ochiai; Hiroshi Sato; Atsushi Ando; Ryo Masumura; Marc Delcroix; Taichi Asami"}, {"ref_id": "b30", "title": "Librispeech: An asr corpus based on public domain audio books", "journal": "", "year": "2015", "authors": "Vassil Panayotov; Guoguo Chen; D Povey; S Khudanpur"}, {"ref_id": "b31", "title": "Over-generation cannot be rewarded: Length-adaptive average lagging for simultaneous speech translation", "journal": "ArXiv", "year": "2022", "authors": "Sara Papi; Marco Gaido; Matteo Negri; Marco Turchi"}, {"ref_id": "b32", "title": "SpecAugment: A simple data augmentation method for automatic speech recognition", "journal": "", "year": "2019", "authors": "D Park; W Chan; Y Zhang; C Chiu; B Zoph; E Cubuk; Q Le"}, {"ref_id": "b33", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b34", "title": "A comparison of sequence-to-sequence models for speech recognition", "journal": "", "year": "2017", "authors": "Rohit Prabhavalkar; Kanishka Rao; Tara N Sainath; Bo Li; Leif M Johnson; Navdeep Jaitly"}, {"ref_id": "b35", "title": "Online and lineartime attention by enforcing monotonic alignments", "journal": "", "year": "2017", "authors": "Colin Raffel; Minh-Thang Luong; Peter J Liu; Ron J Weiss; Douglas Eck"}, {"ref_id": "b36", "title": "Twopass end-to-end speech recognition", "journal": "", "year": "2019", "authors": "Tara N Sainath; Ruoming Pang; David Rybach; Yanzhang He; Rohit Prabhavalkar; Wei Li; Mirk\u00f3 Visontai; Qiao Liang; Trevor Strohman; Yonghui Wu; Ian Mcgraw; Chung-Cheng Chiu"}, {"ref_id": "b37", "title": "Self-attention with relative position representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Peter Shaw; Jakob Uszkoreit; Ashish Vaswani"}, {"ref_id": "b38", "title": "Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition", "journal": "", "year": "2020", "authors": "Yangyang Shi; Yongqiang Wang; Chunyang Wu; Ching Yeh; Julian Chan; Frank Zhang; Duc Le; Michael L Seltzer"}, {"ref_id": "b39", "title": "Estimating entropy of a language from optimal word insertion penalty", "journal": "", "year": "1998", "authors": "K Takeda; Atsunori Ogawa; Fumitada Itakura"}, {"ref_id": "b40", "title": "Unified speech-text pretraining for speech translation and recognition", "journal": "", "year": "2022", "authors": "Yun Tang; Hongyu Gong; Ning Dong; Changhan Wang; Wei-Ning Hsu; Jiatao Gu; Alexei Baevski; Xian Li; Abdelrahman Mohamed; Michael Auli; Juan Miguel Pino"}, {"ref_id": "b41", "title": "A general multi-task learning framework to leverage text data for speech to text tasks", "journal": "", "year": "2021", "authors": "Yun Tang; Juan Miguel Pino; Changhan Wang; Xutai Ma; Dmitriy Genzel"}, {"ref_id": "b42", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam M Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b43", "title": "Fairseq S2T: Fast speech-to-text modeling with fairseq", "journal": "", "year": "2020", "authors": "Changhan Wang; Yun Tang; Xutai Ma; Anne Wu; Dmytro Okhonko; Juan Miguel Pino"}, {"ref_id": "b44", "title": "LAMASSU: Streaming language-agnostic multilingual speech recognition and translation using neural transducers", "journal": "ArXiv", "year": "2022", "authors": "Peidong Wang; Eric Sun; Jian Xue; Yu Wu; Long Zhou; Yashesh Gaur; Shujie Liu; Jinyu Li"}, {"ref_id": "b45", "title": "Hybrid CTC/attention architecture for end-to-end speech recognition", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2017", "authors": "Shinji Watanabe; Takaaki Hori; Suyoun Kim; John R Hershey; Tomoki Hayashi"}, {"ref_id": "b46", "title": "Sequence-to-sequence models can directly translate foreign speech", "journal": "", "year": "2017", "authors": "R Weiss; J Chorowski; N Jaitly; Y Wu; Z Chen"}, {"ref_id": "b47", "title": "Ching feng Yeh, and Frank Zhang. 2020. Streaming transformer-based acoustic models using selfattention with augmented memory", "journal": "", "year": "", "authors": "Chunyang Wu; Yongqiang Wang; Yangyang Shi"}, {"ref_id": "b48", "title": "Large-scale streaming end-toend speech translation with neural transducers", "journal": "", "year": "2022", "authors": "Jian Xue; Peidong Wang; Jinyu Li; Matt Post; Yashesh Gaur"}, {"ref_id": "b49", "title": "CTC alignments improve autoregressive translation", "journal": "", "year": "2022", "authors": "Brian Yan; Siddharth Dalmia; Yosuke Higuchi; Graham Neubig; Florian Metze; Alan W Black; Shinji Watanabe"}, {"ref_id": "b50", "title": "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss", "journal": "", "year": "2020", "authors": "Qian Zhang; Han Lu; Hasim Sak; Anshuman Tripathi; Erik Mcdermott; Stephen Koo; Shankar Kumar"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Comparison among different frameworks: AED, Transducer and TAED.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Chunk-based RNN-T synchronization. Frames from t \u2208 [0, 3] are from the first chunk (cyan) and t \u2208 [4, 7] belong to the second chunk (violet).", "figure_data": ""}, {"figure_label": "345", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :Figure 4 :Figure 5 :345Figure 3: WER (\u2193) v.s. Average Lagging (\u2193) on the MUST-C EN tst-COMMON dataset (\u03bb = 1.4). \"o\" stands for the offline model.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "B1. Did you cite the creators of artifacts you used? section 5 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Both of data (MuST-C) and software (fairseq) are open sourced and widely used by other researchers for speech translation study.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "ASR results in Appendix C for convenient comparison with other ASR systems. Evaluation The ASR quality is measured with word error rate (WER), and the ST quality is reported by case-sensitive detokenized BLEU, which is based on the default SACREBLEU options(Post,   ", "figure_data": "MethodSynchronizationMerge ModuleR/W decisionTraining/InferenceCIF (Dong and Xu, 2020) HMA (Raffel et al., 2017) MILk (Arivazhagan et al., 2019) MMA (Ma et al., 2020b)Async Async Async AsyncDecoder Decoder Decoder Decoderh \u2264j hj, s \u2264i h \u2264j , s<i h \u2264j , s<isampling+scaling/sampling expectation/sampling expectation/sampling expectation/samplingCTC (Graves et al., 2006) Transducer (Graves, 2012) CAAT (Liu et al., 2021) TAED (this work)Sync Sync Sync SyncNone Joiner Joiner Predictor, Joinerhj hj, s k<i h \u2264j , s<i h \u2264j , s<iall paths/sampling all paths/sampling all paths/sampling all paths/sampling"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison of different streaming methods. \"R/W decision\" column lists information needed for R/W decision.", "figure_data": "2018) 3 . Latency is measured with Average Lag-ging (AL) (Ma et al., 2019) using SimualEval (Maet al., 2020a)."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "ModelWER (\u2193)dev tst-COMMONTransducer 14.3 TAED 11.912.7 10.9"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of offline ASR on the MUST-C dev and tst-COMMON sets.", "figure_data": "IWSLT2021 (Anastasopoulos et al., 2021) andIWSLT2022 (Anastasopoulos et al., 2022) stream-ing winning systems. Our Transducer baselineachieves competitive results and is comparablewith the three systems listed above. The qualityimproves by 0.8 to 1.6 BLEU after we switch tothe proposed TAED framework. Table 3 demon-strates the corresponding ASR quality, and TAEDachieves 14% relative WER reduction comparedwith the Transducer baseline on the tst-COMMONset."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparison of the TAED models trained with different AED weights on the MUST-C tst-COMMON set. \"BLEU\" stands for the ST results from \"EN\u2192ES\" and \"WER\" column includes corresponding ASR results.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison of AED alignment speedup factor impact for the streaming ASR performance on the MUST-C EN tst-COMMON set.", "figure_data": "\u03bb Full 1.0 1.2 1.4EN\u2192ES BLEU (\u2191) AL (\u2193) BLEU (\u2191) AL (\u2193) EN\u2192DE 28.3 4328 24.1 4475 27.1 1715 22.8 1611 27.6 1228 22.6 1354 27.6 1120 22.3 1208"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Comparison of AED alignment speedup factor impact for the streaming ST performance on the MUST-C tst-COMMON set. We set chunk size to 320ms for both EN\u2192ES and EN\u2192DE.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Data statistics for MUST-C training dataset. \"#W(m)\" stands for \"number of words (million)\".", "figure_data": "CS(ms) WER AL LAAL AP DAL16016.20 8098560.63 117732013.79 8689140.65 125448013.68 1063 1102 0.70 148564013.14 1317 1350 0.71 1755"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Transducer WER v.s. latency for different chunk sizes.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "TAED WER v.s. latency for different chunk sizes(\u03bb = 1.4).", "figure_data": "CS(ms) BLEU AL LAAL AP DAL16025.71170 1379 0.70 166932026.71137 1357 0.73 167948027.31205 1433 0.74 178464027.71356 1578 0.77 1951"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "", "figure_data": ": Transducer BLEU v.s. latency for different chunk sizes (EN\u2192ES).CS(ms) BLEU AL LAAL AP DAL16026.19 1000 1224 0.71 162332027.61 1120 1351 0.74 179548027.50 1244 1477 0.77 193064028.35 1473 1693 0.79 2188"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "TAED BLEU v.s. latency for different chunk sizes (EN\u2192ES)(\u03bb = 1.4).", "figure_data": "CS(ms) BLEU AL LAAL AP DAL16020.76 1282 1412 0.68 161832021.80 1252 1389 0.70 161248022.52 1306 1447 0.72 171764023.32 1498 1630 0.75 1925"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Transducer BLEU v.s. latency for different chunk sizes (EN\u2192DE).", "figure_data": "CS(ms) BLEU AL LAAL AP DAL16021.57 1263 1411 0.72 182332022.63 1354 1530 0.74 200748023.48 1369 1554 0.77 208864023.47 1903 2024 0.82 2597"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "TAED BLEU v.s. latency for different chunk sizes (EN\u2192DE)(\u03bb = 1.2).", "figure_data": "modeltest clean other clean other devTransducer(ofl)3.28.03.08.2Transducer(str)4.411.24.011.3TAED(ofl)3.17.42.97.4TAED(str)4.210.44.110.8Table 14: Comparison of offline and streaming ASR on the Librispeech datasets.\"ofl\" and \"str\" stand for offline and streaming models respectively.et al., 2019) is without time warping and dropoutset to 0.1. We save the checkpoints every 2500updates and the best 10 checkpoints are averagedfor the greedy search based inference. The modelare trained on the 960 hours Librispeech (Panay-otov et al., 2015) training set and evaluated on 4test/dev sets. In Table 14, the streaming models(\"str\") are trained with chunk size equals to 320mswith one right look-ahead chunk. TAED obtainssimilar WERs in two clean (easy) datasets and re-duces WER varied by 0.5 to 0.8 in two other (hard)datasets."}], "formulas": [{"formula_id": "formula_0", "formula_text": "L aed = \u2212 (x,y) log p(y u |y 1:u\u22121 , x 1:T ). (1)", "formula_coordinates": [2.0, 97.04, 712.03, 192.82, 25.87]}, {"formula_id": "formula_1", "formula_text": "min \u03b8s (x,y) a\u2208A U T U u=1 \u2212 log p(y u |y 1:u\u22121 , x 1:tu ). (2)", "formula_coordinates": [2.0, 311.99, 174.77, 213.16, 38.53]}, {"formula_id": "formula_2", "formula_text": "s l u = f \u03b8 dec (h 1:T , s l 1:u\u22121 , y u\u22121 ),(3)", "formula_coordinates": [2.0, 347.23, 414.7, 177.92, 15.17]}, {"formula_id": "formula_3", "formula_text": "s l a \u2032 = [s l 1 (t 1 ), \u2022 \u2022 \u2022 , s l u\u22121 (t u\u22121 )],(4)", "formula_coordinates": [2.0, 345.54, 687.66, 179.61, 20.96]}, {"formula_id": "formula_4", "formula_text": "s l u (t) = f \u03b8 dec (h 1:t ,\u015d l a \u2032 , y u\u22121 ). (5", "formula_coordinates": [3.0, 114.82, 321.98, 170.81, 16.71]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [3.0, 285.63, 322.38, 4.24, 13.15]}, {"formula_id": "formula_6", "formula_text": "T \u2032 !(T \u2032 \u2212U )! U !", "formula_coordinates": [3.0, 214.4, 388.57, 42.04, 18.64]}, {"formula_id": "formula_7", "formula_text": "z(t, u) = f \u03b8 joiner (h t , s L u ).(6)", "formula_coordinates": [3.0, 125.14, 760.56, 164.73, 15.17]}, {"formula_id": "formula_8", "formula_text": "\u03b1 t,u = LA \u03b1 t,u\u22121 + log p y u |z(t, u \u2212 1) , \u03b1 t\u22121,u + log p \u2205|z(t, u) ,(7)", "formula_coordinates": [3.0, 313.53, 377.63, 211.62, 36.66]}, {"formula_id": "formula_9", "formula_text": "L rnn\u2212t = \u2212\u03b1 T,U \u2212 log p(\u2205|T, U ),(8)", "formula_coordinates": [3.0, 338.17, 426.45, 186.98, 20.55]}, {"formula_id": "formula_10", "formula_text": "L tead = L rnn\u2212t + L aed . (9", "formula_coordinates": [4.0, 126.86, 337.09, 158.76, 20.55]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [4.0, 285.63, 337.09, 4.24, 13.15]}, {"formula_id": "formula_12", "formula_text": "s l a(t) = [s l 1 (t), \u2022 \u2022 \u2022 , s l u\u22121 (t)],(10)", "formula_coordinates": [4.0, 117.25, 642.59, 172.61, 20.96]}, {"formula_id": "formula_13", "formula_text": "s l u (t) = f \u03b8 dec (h 1:\u03b4(t) ,\u015d l a(\u03b4(t)) , y u\u22121 ),(11)", "formula_coordinates": [4.0, 327.45, 365.96, 197.7, 15.69]}, {"formula_id": "formula_14", "formula_text": "z(t, u) = f \u03b8 joiner (h t , s L u (t)).(12)", "formula_coordinates": [4.0, 354.21, 416.7, 170.94, 15.17]}, {"formula_id": "formula_15", "formula_text": "L aed = \u2212 (x,y) u log p(y u |y 1:u\u22121 , h 1:t \u03bb u ). (13)", "formula_coordinates": [5.0, 78.76, 165.97, 211.11, 25.87]}, {"formula_id": "formula_16", "formula_text": "e(t, u)[i \u2205 ] = e(t, u)[i \u2205 ] \u2212 \u03c4,(14)", "formula_coordinates": [5.0, 116.73, 431.93, 173.14, 20.55]}], "doi": ""}