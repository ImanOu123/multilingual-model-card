{"title": "Max-Margin Early Event Detectors", "authors": "Minh Hoai; Fernando De La Torre", "pub_date": "", "abstract": "The need for early detection of temporal events from sequential data arises in a wide spectrum of applications ranging from human-robot interaction to video security. While temporal event detection has been extensively studied, early detection is a relatively unexplored problem. This paper proposes a maximum-margin framework for training temporal event detectors to recognize partial events, enabling early detection. Our method is based on Structured Output SVM, but extends it to accommodate sequential data. Experiments on datasets of varying complexity, for detecting facial expressions, hand gestures, and human activities, demonstrate the benefits of our approach. To the best of our knowledge, this is the first paper in the literature of computer vision that proposes a learning formulation for early event detection.", "sections": [{"heading": "Introduction", "text": "The ability to make reliable early detection of temporal events has many potential applications in a wide range of fields, ranging from security (e.g., pandemic attack detection), environmental science (e.g., tsunami warning) to healthcare (e.g., risk-of-falling detection) and robotics (e.g., affective computing). A temporal event has a duration, and by early detection, we mean to detect the event as soon as possible, after it starts but before it ends, as illustrated in Fig. 1. To see why it is important to detect events before they finish, consider a concrete example of building a robot that can affectively interact with humans. Arguably, a key requirement for such a robot is its ability to accurately and rapidly detect the human emotional states from facial expression so that appropriate responses can be made in a timely manner. More often than not, a socially acceptable response is to imitate the current human behavior. This requires facial events such as smiling or frowning to be detected even before they are complete; otherwise, the imitation response would be out of synchronization.\nDespite the importance of early detection, few machine learning formulations have been explicitly developed for early detection. Most existing methods (e.g., [5,13,16,10,14,9]) for event detection are designed for offline process- . How many frames do we need to detect a smile reliably? Can we even detect a smile before it finishes? Existing event detectors are trained to recognize complete events only; they require seeing the entire event for a reliable decision, preventing early detection. We propose a learning formulation to recognize partial events, enabling early detection.\ning. They have a limitation for processing sequential data as they are only trained to detect complete events. But for early detection, it is necessary to recognize partial events, which are ignored in the training process of existing event detectors. This paper proposes Max-Margin Early Event Detectors (MMED), a novel formulation for training event detectors that recognize partial events, enabling early detection. MMED is based on Structured Output SVM (SOSVM) [17], but extends it to accommodate the nature of sequential data. In particular, we simulate the sequential frame-by-frame data arrival for training time series and learn an event detector that correctly classifies partially observed sequences. Fig. 2 illustrates the key idea behind MMED: partial events are simulated and used as positive training examples. It is important to emphasize that we train a single event detector to recognize all partial events. But MMED does more than augmenting the set of training examples; it trains a detector to localize the temporal extent of a target event, even when the target event has yet finished. This requires monotonicity of the detection function with respect to the inclusion relationship between partial events-the detection score (confidence) of a partial event cannot exceed the score of an encompassing partial event. MMED provides a principled mechanism to achieve this monotonicity, which cannot be assured by a naive solution that simply augments the set of training examples.\nThe learning formulation of MMED is a constrained quadratic optimization problem. This formulation is the- oretically justified. In Sec. 3.2, we discuss two ways for quantifying the loss for continuous detection on sequential data. We prove that, in both cases, the objective of the learning formulation is to minimize an upper bound of the true loss on the training data.\nMMED has numerous benefits. First, MMED inherits the advantages of SOSVM, including its convex learning formulation and its ability for accurate localization of event boundaries. Second, MMED, specifically designed for early detection, is superior to SOSVM and other competing methods regarding the timeliness of the detection. Experiments on datasets of varying complexity, ranging from sign language to facial expression and human actions, showed that our method often made faster detections while maintaining comparable or even better accuracy.", "publication_ref": ["b4", "b12", "b15", "b9", "b13", "b8", "b16"], "figure_ref": ["fig_3", "fig_1"], "table_ref": []}, {"heading": "Previous work", "text": "This section discusses previous work on early detection and event detection.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Early detection", "text": "While event detection has been studied extensively in the literature of computer vision, little attention has been paid to early detection. Davis and Tyagi [2] addressed rapid recognition of human actions using the probability ratio test. This is a passive method for early detection; it assumes that a generative HMM for an event class, trained in a standard way, can also generate partial events. Similarly, Ryoo [15] took a passive approach for early recognition of human activities; he developed two variants of the bag-of-words representation to mainly address the computational issues, not timeliness or accuracy, of the detection process.\nPrevious work on early detection exists in other fields, but its applicability in computer vision is unclear. Neill et al. [11] studied disease outbreak detection. Their approach, like online change-point detection [3], is based on detecting the locations where abrupt statistical changes occur. This technique, however, cannot be applied to detect temporal events such as smiling and frowning, which must and can be detected and recognized independently of the background. Brown et al. [1] used the n-gram model for predictive typing, i.e., predicting the next word from previous words. However, it is hard to apply their method to computer vision, which does not have a well-defined language model yet. Early detection has also been studied in the context of spam filtering, where immediate and irreversible decisions must be made whenever an email arrives. Assuming spam messages were similar to one another, Haider et al. [6] developed a method for detecting batches of spam messages based on clustering. But visual events such as smiling or frowning cannot be detected and recognized just by observing the similarity between constituent frames, because this characteristic is neither requisite nor exclusive to these events.\nIt is important to distinguish between forecasting and detection. Forecasting predicts the future while detection interprets the present. For example, financial forecasting (e.g., [8]) predicts the next day's stock index based on the current and past observations. This technique cannot be directly used for early event detection because it predicts the raw value of the next observation instead of recognizing the event class of the current and past observations. Perhaps, forecasting the future is a good first step for recognizing the present, but this two-stage approach has a disadvantage because the former may be harder than the latter. For example, it is probably easier to recognize a partial smile than to predict when it will end or how it will progress.", "publication_ref": ["b1", "b14", "b10", "b2", "b0", "b5", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Event detection", "text": "This section reviews SVM, HMM, and SOSVM, which are among the most popular algorithms for training event detectors. None of them are specifically designed for early detection.\nLet (X 1 , y 1 ), \u2022 \u2022 \u2022 , (X n , y n ) be the set of training time series and their associated ground truth annotations for the events of interest. Here we assume each training sequence contains at most one event of interest, as a training sequence containing several events can always be divided into smaller subsequences of single events. Thus y i = [s i , e i ] consists of two numbers indicating the start and the end of the event in time series X i . Suppose the length of an event is bounded by l min and l max and we denote Y(t) be the set of lengthbounded time intervals from the 1 st to the t th frame:\nY(t) = {y \u2208 N 2 |y \u2282 [1, t], l min \u2264 |y| \u2264 l max } \u222a {\u2205}.\nHere | \u2022 | is the length function. For a time series X of length l, Y(l) is the set of all possible locations of an event; the empty segment, y = \u2205, indicates no event occurrence. For an interval y = [s, e] \u2208 Y(l), let X y denote the subsegment of X from frame s to e inclusive. Let g(X) denote the output of the detector, which is the segment that maximizes the detection score:\ng(X) = argmax y\u2208Y(l) f (X y ; \u03b8).(1)\nThe output of the detector may be the empty segment, and if it is, we report no detection. f (X y ; \u03b8) is the detection score of segment X y , and \u03b8 is the parameter of the score function.\nNote that the detector searches over temporal scales from l min to l max . In testing, this process can be repeated to detect multiple target events, if more than one event occur.\nHow is \u03b8 learned? Binary SVM methods learn \u03b8 by requiring the score of positive training examples to be greater than or equal to 1, i.e., f (X i y i ; \u03b8) \u2265 1, while constraining the score of negative training examples to be smaller than or equal to \u22121. Negative examples can be selected in many ways; a simple approach is to choose random segments of training time series that do not overlap with positive examples. HMM methods define f (\u2022, \u03b8) as the log-likelihood and learn \u03b8 that maximizes the total loglikelihood of positive training examples, i.e., maximizing i f (X i y i ; \u03b8). HMM methods ignore negative training examples. SOSVM methods learn \u03b8 by requiring the score of a positive training example X i y i to be greater than the score of any other segment from the same time series, i.e., f (X i y i ; \u03b8) > f (X i y ; \u03b8) \u2200y = y i . SOSVM further requires this constraint to be well satisfied by a margin:\nf (X i y i ; \u03b8) \u2265 f (X i y ; \u03b8) + \u2206(y i , y) \u2200y = y i ,\nwhere \u2206(y i , y) is the loss of the detector for outputting y when the desired output is y i [12]. Though optimizing different learning objectives and constraints, all of these aforementioned methods use the same set of positive examples. They are trained to recognize complete events only, inadequately prepared for the task of early detection.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Max-Margin Early Event Detectors", "text": "As explained above, existing methods do not train detectors to recognize partial events. Consequently, using these methods for online prediction would lead to unreliable decisions as we will illustrate in the experimental section. This section derives a learning formulation to address this problem. We use the same notations as described in Sec. 2.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning with simulated sequential data", "text": "Let \u03d5(X y ) be the feature vector for segment X y . We consider a linear detection score function:\nf (X y ; \u03b8) = w T \u03d5(X y ) + b if y = \u2205, 0 otherwise. (2\n)\nHere \u03b8 = (w, b), w is the weight vector and b is the bias term. From now on, for brevity, we use f (X y ) instead of f (X y ; \u03b8) to denote the score of segment X y .\nTo support early detection of events in time series data, we propose to use partial events as positive training examples (Fig. 2). In particular, we simulate the sequential arrival of training data as follows. Suppose the length of X i is l i . For each time t = 1, \u2022 \u2022 \u2022 , l i , let y i t be the part of event y i that has already happened, i.e., y i t = y i \u2229 [1, t], which is possibly empty. Ideally, we want the output of the detector on time series X i at time t to be the partial event, i.e.,", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "g(X i", "text": "[1,t] ) = y i t .\n(3)\nNote that g(X i [1,t]\n) is not the output of the detector running on the entire time series X i . It is the output of the detector on the subsequence of time series X i from the first frame to the t th frame only, i.e.,\ng(X i [1,t] ) = argmax y\u2208Y(t) f (X i y ).(4)\nFrom ( 3)-( 4), the desired property of the score function is:\nf (X i y i t ) \u2265 f (X i y ) \u2200y \u2208 Y(t).(5)\nThis constraint requires the score of the partial event y i t to be higher than the score of any other time series segment y which has been seen in the past, y \u2282 [1, t]. This is illustrated in Fig. 3. Note that the score of the partial event is not required to be higher than the score of a future segment.\nAs in the case of SOSVM, the previous constraint can be required to be well satisfied by an adaptive margin. This margin is \u2206(y i t , y), the loss of the detector for outputting y when the desired output is y i t (in our case \u2206(y i t , y) = 1 \u2212 2|y i t \u2229y| |y i t |+|y| ). The desired constraint is:\nf (X i y i t ) \u2265 f (X i y ) + \u2206(y i t , y) \u2200y \u2208 Y(t).(6)\nThis constraint should be enforced for all t = 1, \u2022 \u2022 \u2022 , l i . As in the formulations of SVM and SOSVM, constraints are allowed to be violated by introducing slack variables, and we obtain the following learning formulation:\nminimize w,b,\u03be i \u22650 1 2 ||w|| 2 + C n n i=1 \u03be i ,(7)\ns.t. f (X i y i t ) \u2265 f (X i y ) + \u2206(y i t , y) \u2212 \u03be i \u00b5 |y i t | |y i | \u2200i, \u2200t = 1, \u2022 \u2022 \u2022 , l i , \u2200y \u2208 Y(t). (8)\nHere | \u2022 | denotes the length function, and \u00b5\n|y i t | |y i |\nis a function of the proportion of the event that has occurred at time t. \u00b5\n|y i t |\n|y i | is a slack variable rescaling factor and should correlate with the importance of correctly detecting at time t whether the event y i has happened. \u00b5(\u2022) can be any\nX i t #$%&\"\" %'()'*&\" +,&,-'\"\" %'()'*&\" s i e i\n.'%/-'.\"%01-'\"+,*021*\"f (\u2022)\n01)#3'&'\"\" '4'*&\" #$-2$3\"\" '4'*&\" \"01*%&-$/*&5\" y i t f (X i y i t ) f (X i ypast ) > Figure 3\n. The desired score function for early event detection: the complete event must have the highest detection score, and the detection score of a partial event must be higher than that of any segment that ends before the partial event. To learn this function, we explicitly consider partial events during training. At time t, the score of the truncated event (red segment) is required to be higher than the score of any segment in the past (e.g., blue segment); however, it is not required to be higher than the score of any future segment (e.g., green segment). This figure is best seen in color.\narbitrary non-negative function, and in general, it should be a non-decreasing function in (0, 1]. In our experiments, we found the following piece-wise linear function a reasonable choice: \u00b5(x) = 0 for 0 < x \u2264 \u03b1; \u00b5(x) = (x \u2212 \u03b1)/(\u03b2 \u2212 \u03b1) for \u03b1 < x \u2264 \u03b2; and \u00b5(x) = 1 for \u03b2 < x \u2264 1 or x = 0. Here, \u03b1 and \u03b2 are tunable parameters. \u00b5(0) = \u00b5(1) emphasizes that true rejection is as important as true detection of the complete event. This learning formulation is an extension of SOSVM. From this formulation, we obtain SOSVM by not simulating the sequential arrival of training data, i.e., to set t = l i instead of t = 1, \u2022 \u2022 \u2022 , l i in Constraint (8). Notably, our method does more than augmenting the set of training examples; it enforces the monotonicity of the detector function, as shown in Fig. 4.\nFor a better understanding of Constraint ( 8), let us analyze the constraint without the slack variable term and break it into three cases: i) t < s i (event has not started); ii) t \u2265 s i , y = \u2205 (event has started; compare the partial event against the detection threshold); iii) t \u2265 s i , y = \u2205 (event has started; compare the partial event against any non-empty segment). Recall f (X \u2205 ) = 0 and y i t = \u2205 for t < s i , cases (i), (ii), (iii) lead to Constraints (9), ( 10), (11), respectively:\nf (X i y ) \u2264 \u22121 \u2200y \u2208 Y(s i \u2212 1) \\ {\u2205},(9)\nf (X i y i t ) \u2265 1 \u2200t \u2265 s i ,(10)\nf (X i y i t ) \u2265 f (X i y ) + \u2206(y i t , y) \u2200t \u2265 s i , y \u2208 Y(t) \\ {\u2205}. (11\n)\nConstraint ( 9) prevents false detection when the event has\n$%&'(%$#&)*(%#+,-).*-#f (\u2022)\nt t t t t not started. Constraint (10) requires successful recognition of partial events. Constraint (11) trains the detector to accurately localize the temporal extent of the partial events.\nThe proposed learning formulation Eq. ( 7) is convex, but it contains a large number of constraints. Following [17], we propose to use constraint generation in optimization, i.e., we maintain a smaller subset of constraints and iteratively update it by adding the most violated ones. Constraint generation is guaranteed to converge to the global minimum. In our experiments described in Sec. 4, this usually converges within 20 iterations. Each iteration requires minimizing a convex quadratic objective. This objective is optimized using Cplex 1 in our implementation.", "publication_ref": ["b7", "b10", "b16"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Loss function and empirical risk minimization", "text": "In Sec. 3.1, we have proposed a formulation for training early event detectors. This section provides further discussion on what exactly is being optimized. First, we briefly review the loss of SOSVM and its surrogate empirical risk. We then describe two general approaches for quantifying the loss of a detector on sequential data. In both cases, what Eq. (7) minimizes is an upper bound on the loss.\nAs previously explained, \u2206(y,\u0177) is the function that quantifies the loss associated with a prediction\u0177, if the true output value is y. Thus, in the setting of offline detection, the loss of a detector g(\u2022) on a sequence-event pair (X, y) is quantified as \u2206(y, g(X)). Suppose the sequenceevent pairs (X, y) are generated according to some distribution P (X, y), the loss of the detector g is R \u2206 true (g) = X \u00d7Y \u2206(y, g(X))dP (X, y). However, P is unknown so the performance of g(.) is described by the empirical risk on the training data {(X i , y i )}, assuming they are generated i.i.d according to P . The empirical risk is R \u2206 emp (g) = 1 n n i=1 \u2206(y i , g(X i )). It has been shown that SOSVM minimizes an upper bound on the empirical risk R \u2206 emp [17]. Due to the nature of continual evaluation, quantifying the loss of an online detector on streaming data requires aggregating the losses evaluated throughout the course of the data sequence. Let us consider the loss associated with a prediction y = g(X i\n[1,t] ) for time series X i at time t as \u2206(y i t , y)\u00b5\n|y i t | |y i | .\nHere \u2206(y i t , y) accounts for the difference between the output y and true truncated event y i t . \u00b5\n|y i t |\n|y i | is the scaling factor; it depends on how much the temporal event y i has happened. Two possible ways for aggregating these loss quantities is to use their maximum or average. They lead to two different empirical risks for a set of training time series:\nR \u2206,\u00b5 max (g) = 1 n n i=1 max t \u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | , R \u2206,\u00b5 mean (g) = 1 n n i=1 mean t \u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | .\nIn the following, we state and prove a proposition that establishes that the learning formulation given in Eq. 7 minimizes an upper bound of the above two empirical risks.\nProposition: Denote by \u03be * (g) the optimal solution of the slack variables in Eq. (7) for a given detector g, then 1 n n i=1 \u03be i * is an upper bound on the empirical risks R \u2206,\u00b5 max (g) and R \u2206,\u00b5 mean (g). Proof: Consider Constraint (8) with y = g(X i\n[1,t] ) and together with the fact that f (\nX i g(X i [1,t] ) ) \u2265 f (X i y i t ), we have \u03be i * \u2265 \u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | \u2200t. Thus \u03be i * \u2265 max t {\u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | }. Hence 1 n n i=1 \u03be i * \u2265 R \u2206,\u00b5 max (g) \u2265 R \u2206,\u00b5 mean (g).\nThis completes the proof of the proposition. This proposition justifies the objective of the learning formulation.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "This section describes our experiments on several publicly available datasets of varying complexity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation criteria", "text": "This section describes several criteria for evaluating the accuracy and timeliness of detectors. We used the area under the ROC curve for accuracy comparison, Normalized Time to Detection (NTtoD) for benchmarking the timeliness of detection, and F 1-score for evaluating localization quality.\nArea under the ROC curve: Consider testing a detector on a set of time series. The False Positive Rate (FPR) of the detector is defined as the fraction of time series that the detector fires before the event of interest starts. The True Positive Rate (TPR) is defined as the fraction of time series that the detector fires during the event of interest. A detector typically has a detection threshold that can be adjusted to trade off high TPR for low FPR and vise versa. By varying this detection threshold, we can generate the ROC curve which is the function of TPR against FPR. We use the area under the ROC for evaluating the detector accuracy.\nAMOC curve: To evaluate the timeliness of detection we used Normalized Time to Detection (NTtoD) which is defined as follows. Given a testing time series with the event of interest occurs from s to e. Suppose the detector starts to fire at time t. For a successful detection, s \u2264 t \u2264 e, we define the NTtoD as the fraction of event that has occurred, i.e., t\u2212s+1 e\u2212s+1 . NTtoD is defined as 0 for a false detection (t < s) and \u221e for a false rejection (t > e). By adjusting the detection threshold, one can achieve lower NTtoD at the cost of higher FPR and vice versa. For a complete characteristic picture, we varied the detection threshold and plotted the curve of NToD versus FPR. This is referred as the Activity Monitoring Operating Curve (AMOC) [4].\nF1-score curve: The ROC and AMOC curves, however, do not provide a measure for how well the detector can localize the event of interest. For this purpose, we propose to use the frame-based F 1-scores. Consider running a detector on a times series. At time t the detector output the segment y while the ground truth (possibly) truncated event is y * . The F 1-score is defined as the harmonic mean of precision and recall values: For a new test time series, we can simulate the sequential arrival of data and record the F 1-scores as the event of interest unroll from 0% to 100%. We refer to this as the F1-score curve.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Synthetic data", "text": "We first validated the performance of MMED on a synthetically generated dataset of 200 time series. Each time series contained one instance of the event of interest, signal 5(a).i, and several instances of other events, signals 5(a).ii-iv. Some examples of these time series are shown in Fig. 5(b). We randomly split the data into training and testing subsets of equal sizes. During testing we simulated the sequential arrival of data and recorded the moment that MMED started to detect the start of the event of interest. With 100% precision, MMED detected the event when it had completed 27.5% of the event. For comparison, SOSVM required observing 77.5% of the event for a positive detection. Examples of testing time series and results are depicted in Fig. 5(b). The events of interest are drawn in green and the solid vertical red lines mark the moments that our method started to detect these events. The dash vertical blue lines are the results of SOSVM. Notably, this result reveals an interesting capability of MMED. For the time series in this experiment, the change in signal values from 3 to 1 is exclusive to the target events. MMED was trained to recognize partial events, it implicitly discovered this unique behavior, and it detected the target events as soon as this behavior occurred. In this experiment, we represented each time series segment by the L 2 -normalized histogram of signal values in the segment (normalized to have unit norm). We used linear SVM with C = 1000, \u03b1 = 0, \u03b2 = 1.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Auslan dataset -Australian sign language", "text": "This section describes our experiments on a publicly available dataset [7] that contains 95 Auslan signs, each with 27 examples. The signs were captured from a native signer using position trackers and instrumented gloves; the location of two hands, the orientation of the palms, and the bending of the fingers were recorded. We considered detecting the sentence \"I love you\" in monologues obtained by concatenating multiple signs. In particular, each monologue contained an I-love-you sentence which was preceded and succeeded by 15 random signs. The I-love-you sentence was ordered concatenation of random samples of three signs: \"I\", \"love\", and \"you\". We created 100 training and 200 testing monologues from disjoint sets of sign samples; the first 15 examples of each sign were used to create training monologues while the last 12 examples were used for testing monologues. The average lengths and standard deviations of the monologues and the I-love-you sentences were 1836 \u00b1 38 and 158 \u00b1 6 respectively.\nPrevious work [7] reported high recognition performance on this dataset using HMMs. Following their success, we implemented a continuous density HMM for Ilove-you sentences. Our HMM implementation consisted of 10 states, each was a mixture of 4 Gaussians. To use the HMM for detection, we adopted a sliding window ap-proach; the window size was fixed to the average length of the I-love-you sentences.\nInspired by the high recognition rate of HMM, we constructed the feature representation for SVM-based detectors (SOSVM and MMED) as follows. We first trained a Gaussian Mixture Model of 20 Gaussians for the frames extracted from the I-love-you sentences. Each frame was then associated with a 20 \u00d7 1 log-likelihood vector. We retained the top three values of this vector, zeroing out the other values, to create a frame-level feature representation. This is often referred to as a soft quantization approach. To compute the feature vector for a given window, we divided the window into two roughly equal halves, the mean feature vector of each half was calculated, and the concatenation of these mean vectors was used as the feature representation of the window.\nA naive strategy for early detection is to use truncated events as positive examples. For comparison, we implemented Seg-[0. We repeated our experiment 10 times and recorded the average performance. Regarding the detection accuracy, all methods except SVM-[0.5,1] performed similarly well. The ROC areas for HMM, SVM-[0.5,1], SOSVM, and MMED were 0.97, 0.92, 0.99, and 0.99, respectively. However, when comparing the timeliness of detection, MMED outperformed the others by a large margin. For example, at 10% false positive rate, our method detected the I-love-you sentence when it observed the first 37% of the sentence. At the same false positive rate, the best alternative method required seeing 62% of the sentence. The full AMOC curves are depicted in Fig. 6(a). In this experiment, we used linear SVM with C = 1, \u03b1 = 0.25, \u03b2 = 1.", "publication_ref": ["b6", "b6"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Extended Cohn-Kanade dataset -expression", "text": "The Extended Cohn-Kanade dataset (CK+) [10] contains 327 facial image sequences from 123 subjects performing one of seven discrete emotions: anger, contempt, disgust, fear, happiness, sadness, and surprise. Each of the sequences contains images from onset (neutral frame) to peak expression (last frame). We considered the task of detecting negative emotions: anger, disgust, fear, and sadness.\nWe used the same representation as [10], where each frame is represented by the canonical normalized appearance feature, referred as CAPP in [10]. For comparison purposes, we implemented two frame-based SVMs: Frmpeak was trained on peak frames of the training sequences while Frm-all was trained using all frames between the onset and offset of the facial action. Frame-based SVMs can be used for detection by classifying individual frames. In contrast, SOSVM and MMED are segment-based. Since a facial expression is a deviation of the neutral expression, we represented each segment of an emotion sequence by the difference between the end frame and the start frame. Even though the start frame was not necessary a neutral face, this representation led to good recognition results.\nWe randomly divided the data into disjoint training and testing subsets. The training set contained 200 sequences with equal numbers of positive and negative examples. For reliable results, we repeated our experiment 20 times and recorded the average performance. Regarding the detection accuracy, segment-based SVMs outperformed framebased SVMs. The ROC areas (mean and standard deviation) for Frm-peak, Frm-all, SOSVM, MMED are 0.82 \u00b1 0.02, 0.84 \u00b1 0.03, 0.96 \u00b1 0.01, and 0.97 \u00b1 0.01, respectively. Comparing the timeliness of detection, our method was significantly better than the others, especially at low false positive rate. For example, at 10% false positive rate, Frmpeak, Frm-all, SOSVM, and MMED can detect the expression when it completes 71%, 64%, 55%, and 47% respectively. Fig. 6(b) plots the AMOC curves, and Fig. 7 displays some qualitative results. In this experiment, we used a linear SVM with C = 1000, \u03b1 = 0, \u03b2 = 0.5.", "publication_ref": ["b9", "b9", "b9"], "figure_ref": ["fig_6", "fig_7"], "table_ref": []}, {"heading": "Weizmann dataset -human action", "text": "The Weizmann dataset contains 90 video sequences of 9 people, each performing 10 actions. Each video sequence in this dataset only consists of a single action. To measure the accuracy and timeliness of detection, we performed experiments on longer video sequences which were created by concatenating existing single-action sequences. Following [5], we extracted binary masks and computed Euclidean distance transform for frame-level features. Frame-level feature vectors were clustered using k-means to create a codebook of 100 temporal words. Subsequently, each frame was represented by the ID of the corresponding codebook entry and each segment of a time series was represented by the histogram of temporal words associated with frames inside the segment. We trained a detector for each action class, but considered them one by one. We created 9 long video sequences, each composed of 10 videos of the same person and had the event of interest at the end of the sequence. We performed leave-one-out cross validation; each cross validation fold trained the event detector on 8 sequences and tested it on the leave-out sequence. For the testing sequence, we computed the normalized time to detection at 0% false positive rate. This false positive rate was achieved by raising the threshold for detection so that the detector would not fire before the event started. We calculated the median normalized time to detection across 9 cross validation folds and averaged these median values across 10 action classes; the resulting values for Seg- [1], Seg-[0.5,1], SOSVM, MMED are 0.16, 0.23, 0.16, and 0.10 respectively. Here Seg- [1] was a segment-based SVM, trained to classify the segments corresponding to the complete action of interest. Seg-[0.5,1] was similar to Seg- [1], but used the first halves of the action of interest as additional positive examples. For each testing sequence, we also generated a F1-score curve as described in Sec. 4.1. Fig. 6(c) displays the F 1-score curves of all methods, averaged across different actions and different cross-validation folds. MMED significantly outperformed the other methods. The superiority of MMED over SOSVM was especially large when the fraction of the event observed was small. This was because MMED was trained to detect truncated events while SOSVM was not. Though also trained with truncated events, Seg-[0.5,1] performed relatively poor because it was not optimized to produce correct temporal extent of the event. In this experiment, we used the linear SVM with C = 1000, \u03b1 = 0, \u03b2 = 1.", "publication_ref": ["b4", "b0", "b0", "b0"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Conclusions", "text": "This paper addressed the problem of early event detection. We proposed MMED, a temporal classifier specialized in detecting events as soon as possible. Moreover, MMED provides localization for the temporal extent of the event. MMED is based on SOSVM, but extends it to anticipate sequential data. During training, we simulate the sequential arrival of data and train a detector to recognize incomplete events. It is important to emphasize that we train a single event detector to recognize all partial events and that our method does more than augmenting the set of training examples. Our method is particularly suitable for events which cannot be reliably detected by classifying individual frames; detecting this type of events requires pooling information from a supporting window. Experiments on datasets of varying complexity, from synthetic data and sign language to facial expression and human actions, showed that our method often made faster detections while maintaining comparable or even better accuracy. Furthermore, our method provided better localization for the target event, especially when the fraction of the seen event was small. In this paper, we illustrated the benefits of our approach in the context of human activity analysis, but our work can be applied to many other domains. The active training approach to detect partial temporal events can be generalized to detect truncated spatial objects [18].", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments: This work was supported by the National Science Foundation (NSF) under Grant No. RI-1116583. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF. The authors would like to thank Y. Shi for the useful discussion on early detection, L. Torresani for the suggestion of F1 curves, M. Makatchev for the discussion about AMOC, T. Simon for AU data, and P. Lucey for providing CAPP features for the CK+ dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Class-based n-gram models of natural language", "journal": "Computational Linguistics", "year": "1992", "authors": "P F Brown; P V Desouza; R L Mercer; V J D Pietra; J C Lai"}, {"ref_id": "b1", "title": "Minimal-latency human action recognition using reliable-inference. Image and Vision Computing", "journal": "", "year": "2006", "authors": "J Davis; A Tyagi"}, {"ref_id": "b2", "title": "An online kernel change detection algorithm", "journal": "IEEE Transactions on Signal Processing", "year": "2005", "authors": "F Desobry; M Davy; C Doncarli"}, {"ref_id": "b3", "title": "Activity monitoring: Noticing interesting changes in behavior", "journal": "", "year": "1999", "authors": "T Fawcett; F Provost"}, {"ref_id": "b4", "title": "Actions as space-time shapes", "journal": "Transactions on Pattern Analysis and Machine Intelligence", "year": "2007", "authors": "L Gorelick; M Blank; E Shechtman; M Irani; R Basri"}, {"ref_id": "b5", "title": "Supervised clustering of streaming data for email batch detection", "journal": "", "year": "2007", "authors": "P Haider; U Brefeld; T Scheffer"}, {"ref_id": "b6", "title": "Temporal classification: Extending the classification paradigm to multivariate time series", "journal": "", "year": "2002", "authors": "M Kadous"}, {"ref_id": "b7", "title": "Financial time series forecasting using support vector machines", "journal": "Neurocomputing", "year": "2003", "authors": "K.-J Kim"}, {"ref_id": "b8", "title": "Discriminative figure-centric models for joint action localization and recognition", "journal": "", "year": "2011", "authors": "T Lan; Y Wang; G Mori"}, {"ref_id": "b9", "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression", "journal": "", "year": "2010", "authors": "P Lucey; J F Cohn; T Kanade; J Saragih; Z Ambadar; I Matthews"}, {"ref_id": "b10", "title": "A Bayesian spatial scan statistic", "journal": "", "year": "2006", "authors": "D Neill; A Moore; G Cooper"}, {"ref_id": "b11", "title": "Action unit detection with segment-based SVMs", "journal": "", "year": "2010", "authors": "M H Nguyen; T Simon; F De La Torre; J Cohn"}, {"ref_id": "b12", "title": "Learning and inferring motion patterns using parametric segmental switching linear dynamic systems", "journal": "International Journal of Computer Vision", "year": "2008", "authors": "S M Oh; J M Rehg; T Balch; F Dellaert"}, {"ref_id": "b13", "title": "High Five: Recognising human interactions in TV shows", "journal": "", "year": "2010", "authors": "A Patron-Perez; M Marszalek; A Zisserman; I Reid"}, {"ref_id": "b14", "title": "Human activity prediction: Early recognition of ongoing activities from streaming videos", "journal": "", "year": "2011", "authors": "M Ryoo"}, {"ref_id": "b15", "title": "Modeling the temporal extent of actions", "journal": "", "year": "2010", "authors": "S Satkin; M Hebert"}, {"ref_id": "b16", "title": "Large margin methods for structured and interdependent output variables", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "I Tsochantaridis; T Joachims; T Hofmann; Y Altun"}, {"ref_id": "b17", "title": "Structured output regression for detection with partial truncation", "journal": "", "year": "2009", "authors": "A Vedaldi; A Zisserman"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure1. How many frames do we need to detect a smile reliably? Can we even detect a smile before it finishes? Existing event detectors are trained to recognize complete events only; they require seeing the entire event for a reliable decision, preventing early detection. We propose a learning formulation to recognize partial events, enabling early detection.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Given a training time series that contains a complete event, we simulate the sequential arrival of training data and use partial events as positive training examples. The red segments indicate the temporal extents of the partial events. We train a single event detector to recognize all partial events, but our method does more than augmenting the set of training examples.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. Monotonicity requirement -the detection score of a partial event cannot exceed the score of an encompassing partial event. MMED provides a principled mechanism to achieve this monotonicity, which cannot be assured by a naive solution that simply augments the set of training examples.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "F 1 :1= 2 P recision * Recall P recision+Recall , with P recision := |y\u2229y * | |y| and Recall := |y\u2229y * | |y * | .", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Synthetic data experiment. (a): time series were created by concatenating the event of interest (i) and several instances of other events (ii)-(iv). (b): examples of testing time series; the solid vertical red lines mark the moments that our method starts to detect the event of interest while the dash blue lines are the results of SOSVM.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "5,1], a binary SVM that used the first halves of the I-love-you sentences in addition to the full sentences as positive training examples. Negative training examples were random segments that had no overlapping with the Ilove-you sentences.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Performance curves. (a, b): AMOC curves on Auslan and CK+ datasets; at the same false positive rate, MMED detects the event of interest sooner than the others. (c): F1-score curves on Weizmann dataset; MMED provides better localization for the event of interest, especially when the fraction of the event observed is small. This figure is best seen in color.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Disgust (a) and fear (b) detection on CK+ dataset. From left to right: the onset frame, the frame at which MMED fires, the frame at which SOSVM fires, and the peak frame. The number in each image is the corresponding NTtoD.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Y(t) = {y \u2208 N 2 |y \u2282 [1, t], l min \u2264 |y| \u2264 l max } \u222a {\u2205}.", "formula_coordinates": [2.0, 315.96, 621.05, 222.25, 12.24]}, {"formula_id": "formula_1", "formula_text": "g(X) = argmax y\u2208Y(l) f (X y ; \u03b8).(1)", "formula_coordinates": [3.0, 115.2, 97.53, 171.32, 17.87]}, {"formula_id": "formula_2", "formula_text": "f (X i y i ; \u03b8) \u2265 f (X i y ; \u03b8) + \u2206(y i , y) \u2200y = y i ,", "formula_coordinates": [3.0, 50.16, 380.57, 236.23, 26.31]}, {"formula_id": "formula_3", "formula_text": "f (X y ; \u03b8) = w T \u03d5(X y ) + b if y = \u2205, 0 otherwise. (2", "formula_coordinates": [3.0, 78.36, 643.49, 204.25, 22.96]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [3.0, 282.61, 651.35, 3.91, 8.91]}, {"formula_id": "formula_5", "formula_text": "Note that g(X i [1,t]", "formula_coordinates": [3.0, 308.88, 201.89, 70.89, 13.46]}, {"formula_id": "formula_6", "formula_text": "g(X i [1,t] ) = argmax y\u2208Y(t) f (X i y ).(4)", "formula_coordinates": [3.0, 371.52, 260.93, 173.72, 19.47]}, {"formula_id": "formula_7", "formula_text": "f (X i y i t ) \u2265 f (X i y ) \u2200y \u2208 Y(t).(5)", "formula_coordinates": [3.0, 366.96, 311.57, 178.28, 15.08]}, {"formula_id": "formula_8", "formula_text": "f (X i y i t ) \u2265 f (X i y ) + \u2206(y i t , y) \u2200y \u2208 Y(t).(6)", "formula_coordinates": [3.0, 342.84, 471.17, 202.4, 15.08]}, {"formula_id": "formula_9", "formula_text": "minimize w,b,\u03be i \u22650 1 2 ||w|| 2 + C n n i=1 \u03be i ,(7)", "formula_coordinates": [3.0, 318.24, 551.81, 227.0, 30.49]}, {"formula_id": "formula_10", "formula_text": "s.t. f (X i y i t ) \u2265 f (X i y ) + \u2206(y i t , y) \u2212 \u03be i \u00b5 |y i t | |y i | \u2200i, \u2200t = 1, \u2022 \u2022 \u2022 , l i , \u2200y \u2208 Y(t). (8)", "formula_coordinates": [3.0, 343.56, 585.53, 201.68, 47.32]}, {"formula_id": "formula_11", "formula_text": "|y i t | |y i |", "formula_coordinates": [3.0, 514.68, 644.44, 12.8, 16.59]}, {"formula_id": "formula_12", "formula_text": "|y i t |", "formula_coordinates": [3.0, 366.12, 673.36, 12.8, 9.33]}, {"formula_id": "formula_13", "formula_text": "X i t #$%&\"\" %'()'*&\" +,&,-'\"\" %'()'*&\" s i e i", "formula_coordinates": [4.0, 56.76, 72.43, 224.08, 73.39]}, {"formula_id": "formula_14", "formula_text": "01)#3'&'\"\" '4'*&\" #$-2$3\"\" '4'*&\" \"01*%&-$/*&5\" y i t f (X i y i t ) f (X i ypast ) > Figure 3", "formula_coordinates": [4.0, 50.16, 131.95, 188.24, 137.36]}, {"formula_id": "formula_15", "formula_text": "f (X i y ) \u2264 \u22121 \u2200y \u2208 Y(s i \u2212 1) \\ {\u2205},(9)", "formula_coordinates": [4.0, 57.96, 650.87, 228.34, 11.05]}, {"formula_id": "formula_16", "formula_text": "f (X i y i t ) \u2265 1 \u2200t \u2265 s i ,(10)", "formula_coordinates": [4.0, 57.96, 666.47, 228.34, 13.94]}, {"formula_id": "formula_17", "formula_text": "f (X i y i t ) \u2265 f (X i y ) + \u2206(y i t , y) \u2200t \u2265 s i , y \u2208 Y(t) \\ {\u2205}. (11", "formula_coordinates": [4.0, 57.96, 683.39, 224.63, 14.06]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [4.0, 282.59, 685.62, 3.72, 8.02]}, {"formula_id": "formula_19", "formula_text": "$%&'(%$#&)*(%#+,-).*-#f (\u2022)", "formula_coordinates": [4.0, 402.15, 231.49, 80.46, 9.29]}, {"formula_id": "formula_20", "formula_text": "|y i t | |y i | .", "formula_coordinates": [5.0, 112.8, 185.2, 22.41, 16.59]}, {"formula_id": "formula_21", "formula_text": "|y i t |", "formula_coordinates": [5.0, 64.92, 215.8, 12.8, 9.33]}, {"formula_id": "formula_22", "formula_text": "R \u2206,\u00b5 max (g) = 1 n n i=1 max t \u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | , R \u2206,\u00b5 mean (g) = 1 n n i=1 mean t \u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | .", "formula_coordinates": [5.0, 53.4, 291.77, 229.69, 63.73]}, {"formula_id": "formula_23", "formula_text": "X i g(X i [1,t] ) ) \u2265 f (X i y i t ), we have \u03be i * \u2265 \u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | \u2200t. Thus \u03be i * \u2265 max t {\u2206(y i t , g(X i [1,t] ))\u00b5 |y i t | |y i | }. Hence 1 n n i=1 \u03be i * \u2265 R \u2206,\u00b5 max (g) \u2265 R \u2206,\u00b5 mean (g).", "formula_coordinates": [5.0, 50.16, 468.41, 236.26, 67.33]}], "doi": ""}