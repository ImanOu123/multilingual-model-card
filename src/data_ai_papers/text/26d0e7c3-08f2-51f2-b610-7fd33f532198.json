{"title": "ProcTHOR: Large-Scale Embodied AI Using Procedural Generation", "authors": "Matt Deitke; Eli Vanderbilt; Alvaro Herrasti; Luca Weihs; Jordi Salvador; Kiana Ehsani; Winson Han; Eric Kolve; Ali Farhadi; Aniruddha Kembhavi; Roozbeh Mottaghi", "pub_date": "2022-06-14", "abstract": "Massive datasets and high-capacity models have driven many recent advancements in computer vision and natural language understanding. This work presents a platform to enable similar success stories in Embodied AI. We propose PROCTHOR, a framework for procedural generation of Embodied AI environments. PROCTHOR enables us to sample arbitrarily large datasets of diverse, interactive, customizable, and performant virtual environments to train and evaluate embodied agents across navigation, interaction, and manipulation tasks. We demonstrate the power and potential of PROCTHOR via a sample of 10,000 generated houses and a simple neural model. Models trained using only RGB images on PROCTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks for navigation, rearrangement, and arm manipulation, including the presently running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We also demonstrate strong 0-shot results on these benchmarks, via pre-training on PROCTHOR with no fine-tuning on the downstream benchmark, often beating previous state-of-the-art systems that access the downstream training data.", "sections": [{"heading": "Introduction", "text": "Computer vision and natural language processing models have become increasingly powerful through the use of large-scale training data. Recent models such as CLIP [93], DALL-E [95], GPT-3 [10], and Flamingo [3] use massive amounts of task agnostic data to pre-train large neural architectures that perform remarkably well at downstream tasks, including in zero and few-shot settings. In comparison, the Embodied AI (E-AI) research community predominantly trains agents in simulators with far fewer scenes [94,63,27]. Due to the complexity of tasks and the need for long planning horizons, the best performing E-AI models continue to overfit on the limited training scenes and thus generalize poorly to unseen environments.\nIn recent years, E-AI simulators have become increasingly more powerful with support for physics, manipulators, object states, deformable objects, fluids, and real-sim counterparts [63,101,104,37,124], but scaling them up to tens of thousands of scenes has remained challenging. Existing E-AI environments are either designed manually [63,37] or obtained via 3D scans of real structures [101,94]. The former approach requires 3D artists to spend a significant amount of time designing 3D assets, arranging them in sensible configurations within large spaces, and carefully configuring Figure 1: We propose PROCTHOR, a framework to procedurally generate a large variety of diverse, interactable, and customizable houses.\nthe right textures and lighting in these environments. The latter involves moving specialized cameras through many real-world environments and then stitching the resulting images together to form 3D reconstructions of the scenes. These approaches are not scalable, and expanding existing scene repositories multiple orders of magnitude is not practical.\nWe present PROCTHOR, a framework built off of AI2-THOR [63], to procedurally generate fullyinteractive, physics-enabled environments for E-AI research. Given a room specification (e.g., a house with 3 bedrooms, 3 baths, and 1 kitchen), PROCTHOR can produce a large and diverse set of floorplans that meet these requirements (Fig. 1). A large asset library of 108 object types and 1633 fully interactable instances is used to automatically populate each floorplan, ensuring that object placements are physically plausible, natural, and realistic. One can also vary the intensity and color of lighting elements (both artificial lighting and simulated skyboxes) in each scene, to simulate variations in indoor lighting and the time of the day. Assets (such as furniture and fruit) and larger structures such as walls and doors can be assigned a variety of colors and textures, sampled from sets of plausible colors and materials for each asset category. Together, the diversity of layouts, assets, placements, and lighting leads to an arbitrarily large set of environments -allowing PROCTHOR to scale orders of magnitude beyond the number of scenes currently supported by present-day simulators. In addition, PROCTHOR supports dynamic material randomizations, whereby colors and materials of individual assets can be randomized each time an environment is loaded into memory for training. Importantly, in contrast to environments produced using 3D scans, scenes produced by PROCTHOR contain objects that both support a variety of different object states (e.g. open, closed, broken, etc.) and are fully interactive so that they can be physically manipulated by agents with robotic arms. We also present ARCHITECTHOR, a 3D artist-designed set of 10 high quality fully interactable houses, meant to be used as a test-only environment for research within household environments. In contrast to AI2-iTHOR (single rooms) and RoboTHOR (lesser visual diversity) environments, ARCHITECTHOR contains larger, diverse, and realistic houses.\nWe demonstrate the ease and effectiveness of PROCTHOR by sampling an environment of 10,000 houses (named PROCTHOR-10K), composed of diverse layouts ranging from small 1-room houses to larger 10-room houses. We train agents with very simple neural architectures (CNN+RNN)without a depth sensor, and instead only employing RGB channels, with no explicit mapping and no human task supervision -on PROCTHOR-10K and produce state-of-the-art (SoTA) models on several navigation and interaction benchmarks. As of 10am PT on June 14th, 2022 we obtain (1) RoboTHOR ObjectNav Challenge AI2-iTHOR ObjectNav -0-shot numbers which already outperform a previous model that trains on AI2-iTHOR, with fine-tuning we achieve a success rate of 77.5%; (5) ArmPointNav [33] -0-shot number that beats previous SoTA results when using RGB; and (6) ArchitecTHOR ObjectNav -a large success rate improvement from 18.5% to 31.4%. Finally, an ablation analysis clearly shows the advantages of scaling up from 10 to 100 to 1K and finally to 10K scenes and indicates that further improvements can be obtained by invoking PROCTHOR to produce even larger environments.\nIn summary, our contributions are (1) PROCTHOR, a framework that allows for the performant procedural generation of an unbounded number of diverse, fully-interactive, simulated environments, (2) ARCHITECTHOR, a new, 3D artist-designed set of houses for E-AI evaluation, and (3) SoTA results across six E-AI benchmarks covering manipulation and navigation tasks, including strong 0-shot results. PROCTHOR will be open-sourced and the code used in this work will be released.", "publication_ref": ["b8", "b2", "b25", "b35", "b50", "b35", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Embodied AI platforms. Various Embodied AI platforms have been developed over the past several years [63,101,104,124,37,121]. These platforms target different design goals. AI2-THOR [63] and its variants (ManipulaTHOR [33] and RoboTHOR [27]) are built in the Unity game engine and focus on agent-object interactions, object state changes, and accurate physics simulation. Unlike AI2-THOR, Habitat [101] provides scenes constructed from 3D scans of houses, however, objects and scenes are not interactable. A more recent version, Habitat 2.0 [109], introduces object interactions at the expense of being limited to one floorplan and synthetic scenes. iGibson [104] includes photorealistic scenes, but with limited interactions such as pushing. iGibson 2.0 [69] extends iGibson by focusing on household tasks and object state changes in synthetic scenes and includes a virtual reality interface. ThreeDWorld [37] targets high-fidelity physics simulation such as liquid and deformable object simulation. VirtualHome [92] is designed for simulating human activities via programs. RLBench [53], RoboSuite [133] and Sapien [124] target fine-grained manipulation. The main advantage of PROCTHOR is that we can generate a diverse set of interactive scenes procedurally, enabling studies of data augmentation and large-scale training in the context of Embodied AI.\nLarge-scale datasets. Large-scale datasets have resulted in major breakthroughs in different domains such as image classification [28,67], vision and language [18,111], 3D understanding [14,125], autonomous driving [11,107], and robotic object manipulation [91,82]. However, there are not many interactive large-scale datasets for Embodied AI research. PROCTHOR includes interactive houses generated procedurally. Hence, there are an arbitrarily large number of scenes in the framework. The closest works to ours are [94,90,72]. HM3D [94] is a recent framework that includes 1,000 scenes generated using 3D scans of real environments. PROCTHOR has a number of key distinctions: (1) unlike HM3D which includes static scenes, the scenes in PROCTHOR are interactive i.e., objects can move and change state, the lighting and texture of objects can change, and a physics engine determines the future states of the scenes; (2) it is challenging to scale up HM3D as it requires scanning a house and cleaning up the data, while we can procedurally generate more houses; (3) HM3D can be used only for navigation tasks (as there is no physics simulation and object interaction), while PROCTHOR can be used for tasks other than navigation. OpenRooms [72] is similar to HM3D in terms of the source of the data (3D scans) and dataset size. However, OpenRooms is interactive. OpenRooms is also confined to the set of scanned houses, and it takes a significant amount of time to annotate a new scene (e.g., labeling materials for one object takes 1 minute), while PROCTHOR does not suffer from these issues. Megaverse [90] is another large-scale Embodied AI platform that includes procedurally generated environments. Although it is impressive in terms of simulation speed, it includes only game-like environments with a simplified appearance. In contrast, PROCTHOR mimics real-world houses in terms of the complexity of appearance, physics, and object interactions.\nScene generation. Indoor scene synthesis has been studied extensively in computer vision and graphics communities. [16,17,13] address generating 3D scenes from text descriptions. [120,47,85] learn to generate house floorplans. [98,129,20,113,56,70] use generative models for indoor scene generation. [131,100] propose potential objects for a query location in an indoor scene. Others have used procedural generation [57,31] and unsupervised learning [29] to synthesize grid-world environments for AI. PROCTHOR is specifically designed for Embodied AI research in the sense that (1) all scenes are interactive and physics-enabled, and the placement of objects respects the physics of the world (e.g., there are no two objects that clip through each other), (2) there are various forms of scene augmentation such as randomization of object placements while following certain commonsense rules, variation in the appearance of objects and structures, and variation in lighting.", "publication_ref": ["b50", "b35", "b47", "b31", "b25", "b35", "b59", "b50", "b26", "b16", "b37", "b12", "b51", "b9", "b0", "b1", "b14", "b15", "b11", "b46", "b55", "b18", "b39", "b57", "b29", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "PROCTHOR", "text": "PROCTHOR is a framework to procedurally generate E-AI environments. It extends AI2-THOR and, thereby, inherits AI2-THOR's large asset library, robotic agents, and accurate physics simulation. Just as in scenes painstakingly created by designers in AI2-THOR, environments in PROCTHOR are fully interactive and support navigation, object manipulation, and multi-agent interaction.  Fig. 2 shows a high-level schematic of the procedure used by PROCTHOR to generate a scene. Given a room specification (e.g. house with 1 bedroom + 1 bathroom), PROCTHOR uses multi-stage conditional sampling to, iteratively, generate a floor plan, create an external wall structure, sample lighting, and doors, then sample assets including large, small and wall objects, pick colors and textures, and determine appropriate placements for assets within the scene. We refer the reader to the appendix for details regarding our procedural generation and sampling mechanism, but highlight five key characteristics of PROCTHOR: Diversity, Interactivity, Customizability, Scale, and Efficiency.\nDiversity. PROCTHOR enables the creation of rich and diverse environments. Mirroring the success of pre-training models with diverse data in the vision and NLP domains, we demonstrate the utility of this diversity on several E-AI tasks. Scenes in PROCTHOR exhibit diversity across several facets:\nDiversity of floor plans. Given a room specification, we first employ iterative boundary cutting to obtain an external scene layout (that can range from a simple rectangle to a complex polygon).\nThe recursive layout generation algorithm by Lopes et al.\n[73] is then used to divide the scene into the desired rooms. Finally, we determine connectivity between rooms using a set of user-defined constraints. These procedures result in natural room layouts (e.g., bedrooms are often connected to adjoining bathrooms via a door, bathrooms more often have a single entrance, etc). As exemplified in Fig. 3, PROCTHOR generates hugely diverse floor plans using this procedure. Diversity of assets. PROCTHOR populates scenes with small and large assets from its database of 1633 household assets across 108 categories (examples in Fig. 4). While many assets are inherited from AI2-THOR, we also introduce new assets such as windows, doors, and countertops, handdesigned by 3D graphic designers. Asset instances are split into train/val/test subsets and are interactable, i.e. objects can be picked and placed within the scenes, some objects have multiple states (e.g. a light can be on or off) and several objects consists of parts with rigid body motions (e.g. door on a microwave).", "publication_ref": [], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "\u2022 \u2022 \u2022", "text": "Figure 4: Object diversity. A subset of instances for four object categories.\nDiversity of materials. Walls can have two kinds of materials -one of 40 solid (and popular) colors or one of 122 wall textures such as brick and tile. We also provide 55 floor materials. The ceiling material for the entire house is sampled from the set of wall materials. PROCTHOR also provides the ability to randomize materials of objects. Materials are only randomized within categories, which ensures objects still look and behave like the class they represent. Diversity of object placements. Asset categories have several soft annotations that help place them realistically within a house. These include room assignments (e.g. couch in a living room but not a bathroom) and location assignments (e.g. fridge along a wall, TV not on the floor). We also develop the notion of a Semantic Asset Group (SAG) -groups of assets that typically co-occur (e.g. dining table with four chairs) and thus must be sampled and placed using dependent sampling. Given a layout, individual assets and SAGs that lie on the floor are sampled and placed iteratively, ensuring that rooms continue to have adequate floor space for agents to navigate and manipulate objects. Then wall objects such as windows and paintings get placed, and finally, surface objects (ones found on top of other assets) are placed (e.g. cups on the kitchen counter). This sampling allows for a large and diverse set of object choices and placements within any layout. Fig. 6 shows such variations. Diversity of lighting. PROCTHOR supports a single directional light (analogous to the sun) and several point lights (analogous to lightbulbs). Varying the color, intensity, and placement of these sources allows us to simulate different artificial lighting, typically observed in houses, and also at different times of the day. Lighting has a significant effect on the rendered images as seen in Fig. 7.\nFigure 7: Lighting variation. Morning, dusk, and night lighting for an example scene.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Interactivity.", "text": "A key property of PROCTHOR is the ability to interact with objects to change their location or state (Fig. 8). This capability is fundamental to many Embodied AI tasks. Datasets like HM3D [94] that are created from static 3D scans do not possess this capability. PROCTHOR supports agents with arms capable of manipulating objects and interacting with each other. Customizability. PROCTHOR supports many room, asset, material, and lighting specifications. With a few simple lines of specification, one can easily generate customized environments of interest. Fig. 9 shows examples of such varied scenes (classroom, library, and office).\nFigure 9: Customizability. PROCTHOR can be used to construct custom scene types such as classrooms, libraries, and offices.\nScale and Efficiency. PROCTHOR currently uses 16 different scene specifications to seed the scene generation process. These can result in over 100 billion layouts. PROCTHOR uses 18 different Semantic Asset groups and 1633 assets. These can result in roughly 20 million unique asset groups. Each of these assets can be placed in numerous locations. In addition, each house gets scaled and uses a variety of lighting. This diversity of layouts, assets, materials, placements, and lighting enables the generation of arbitrarily large sets of houses -either statically generated and stored as a dataset or dynamically generated at each iteration of training. Scenes are efficiently represented in a JSON specification and are loaded into AI2-THOR at runtime, making the memory overhead of storing houses incredibly efficient. Moreover, the scene generation process is fully automatic and fast and PROCTHOR provides high framerates for training E-AI models (see Sec. 4 for details).  Middle: distribution of the area of each house, bucketed into small, medium, and large houses; Right: bar plot showing the distribution over the number of rooms that make up each house.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "PROCTHOR-10K", "text": "We demonstrate the power and potential of PROCTHOR using a sampled set of 10,000 fully interactive houses obtained by the procedural generation process described in Section 3 -which we label PROCTHOR-10K. An additional set of 1,000 validation and 1,000 testing houses are available for evaluation. Asset splits across train/val/test are detailed in the Appendix. All houses are fully navigable, allowing an agent to traverse through each room without any interaction. In terms of scale, PROCTHOR-10K is one of the largest sets of interactive home environments for Embodied AI -as a comparison, AI2-iTHOR [63] includes 120 scenes, RoboTHOR [27]   Scene statistics. Houses in PROCTHOR-10K are generated using 16 different room specifications.\nAn example room spec is: A house with 1 bedroom connected to 1 bathroom, 1 kitchen, and 1 living room and is visualized in Fig. 2. Houses in this dataset have as few as 1 room and as many as 10. Fig. 10 shows the distribution of areas (middle) and the number of rooms (right) of these generated houses. Our use of room specifications enables us to change the distribution of the size and complexity of houses fairly easily. PROCTHOR-10K encompasses a wider spectrum of scenes than AI2-iTHOR [63] and ROBOTHOR [27] (biased towards room-sized scenes) and Gibson [123] and HM3D [94] (biased towards large houses).\nRooms in each of these houses contain objects from 95 different categories including common household objects such as fridges, countertops, beds, toilets, and house plants, and structure objects such as doorways and windows. Fig. 10 (left) shows the distribution of the number of objects per room per house, which shows that houses in PROCTHOR-10K are well populated. They also contain  ", "publication_ref": ["b25", "b25", "b49"], "figure_ref": ["fig_1", "fig_6", "fig_6"], "table_ref": []}, {"heading": "Experiments", "text": "Tasks. We now present results for models pre-trained on PROCTHOR-10K on several navigation and manipulation benchmarks to demonstrate the benefits of large-scale training. We consider ObjectNav (navigation towards a specific object category) in PROCTHOR, ARCHITECTHOR, RoboTHOR [27], HM3D [94], and AI2-iTHOR [63]. We also consider two manipulation-based tasks: ArmPoint-Nav [33] and 1-phase Room Rearrangement [115]. In ArmPointNav, the agent moves an object using a robotic arm from a source location to a destination location specified in the 3D coordinate frame. In Room Rearrangement, the goal is to move objects or change their state to reach a target scene state.", "publication_ref": ["b25", "b31", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Models.", "text": "Our models for all tasks consist of a CNN to encode visual information and a GRU to capture temporal information. We deliberately use a simple architecture across all tasks to show the benefits of large are high-fidelity artist-designed scenes with high-quality shadows and lighting. HM3D is constructed from 3D scans of houses which can differ quite a bit from synthetic environments. RoboTHOR [27] houses use wall panels and floors with very specific textures.\nZero-shot transfer results. Models trained only on PROCTHOR and evaluated zero-shot outperform previous SoTA models on 3 benchmarks (refer to zero-shot rows of Table 2). These are very strong results since the models generalize to not only unseen objects and scenes, but also different appearance and layout statistics.\nFine-tuning results. Further fine-tuning of the model using each benchmark's training data, achieves state-of-the-art results on all benchmarks (refer to fine-tune rows of Table 2). Notably, our model is ranked first on three public leaderboards as of 10am PT, June 14th 2022: Habitat 2022 ObjectNav challenge, AI2-THOR Rearrangement 2022 challenge, and RoboTHOR ObjectNav challenge. It should be noted that our model achieves these results using a very simple architecture and only RGB images. Other techniques typically use more complex architectures that include mapping or visual odometry modules and use additional perception sensors such as depth images.\nScale ablation. To evaluate the effect of scale we train the models on 10, 100, 1,000, and 10,000 houses. For this experiment, we do not use any material augmentations. As shown in   ", "publication_ref": ["b25"], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Conclusion", "text": "We propose PROCTHOR, a framework to procedurally generate arbitrarily large sets of interactive, physics-enabled houses for Embodied AI research. We pre-train simple models on 10,000 generated houses and show state-of-the-art results across 6 embodied navigation and manipulation benchmarks with strong 0-shot results, even outperforming prior state-of-the-art on 3 of these benchmarks. Ali Farhadi advised on the research direction.\nAlvaro Herrasti led the Unity backend development that creates a house from a JSON specification.\nAniruddha Kembhavi advised on research direction, the ARCHITECTHOR development, and the house generation process and wrote the paper.\nEric Kolve advised on the Unity backend development.\nRoozbeh Mottaghi advised on the research direction, the Unity backend, the ARCHITECTHOR development, and the house generation process and wrote the paper.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B House Generation", "text": "This section gives more details about the process we developed to procedurally sample houses. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Examples", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1.1 3-Room Houses", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Room Specs", "text": "Room specs provide the ability to specify the rooms that appear in a house, the relative size of each room, and how the rooms are connected with doors. Their idea was first proposed in [75]. A room spec is manually specified with a tree data structure.\nFigure 18a shows a simplified example of a room spec with four rooms: bedroom, bathroom, kitchen, and living room. In this room spec, there are two subtrees, comprising Z bb = {bedroom, bathroom} and Z klv = {kitchen, living room}. At each level of the tree, there is a constraint that there must be a direct path connecting every child node of a parent. Thus, in our example, there will be a path between the bedroom and the bathroom, a path between the kitchen and the living room, and another path connecting Z bb to Z klv . We can also specify which room types we would prefer not to have a path between it and the parent. For example, we typically do not want the bathroom to have 2 doors, such as between it and the bedroom and between it and a room in Z klv .\nEach tree node, below the root of the tree, is also assigned a growth weight, which approximates the relative size of the node compared to all other nodes that share the same parent. For instance, we might assign both Z bb and Z klv a growth rate of 1, to be roughly the same size. But, if we want the bedroom to take up roughly 60% of the Z bb 's area, then we might assign the bedroom a growth rate of 3 and the bathroom a growth rate of 2.\nRoom specs allow us to flexibly choose the distribution of houses we sample, allowing us to specify massive mansions, studio apartments, and anything in-between. Moreover, just a few room specs can go a long way. To generate our houses, we use 16 room specs, which each uses between 1 to 10 rooms. To generate the houses dataset, we assign a sampling weight to each of our room specs, and then use weighted sampling to sample a room spec for each house.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "B.3 Sampling Floor Plans", "text": "The size and shape of the house are sampled to form the interior boundaries. Room specs specify the distribution over the dimensions of the house. Figure 19 visualizes the process of sampling an interior boundary, where we first sample the size of the boundary and then make cuts to the corners to add randomness. The sampling starts off by choosing the initial upper bound of the top-down x and z size of the house, in meters, respectively denoted as x s and z s . Each dimension is an integer.\nIn most room specs, each dimension is independently sampled from the discrete uniform distribution x s , z s \u223c U (max(\u2113 min , \u00b5 a \u221a n r \u2212 \u00b5a /2), \u00b5 a \u221a n r + \u00b5a /2), inclusive. However, individual room specs can override the x s and z s sampling distributions. Here, n r represents the number of rooms in the house, \u2113 min is set to 2 and represents the minimum size of x s and z s , and \u00b5 a is set to 3 and represents the average size of x s and z s per room.  Once we have the rectangular boundary (x s , z s ), we then make several cuts to the outside of the rooms such that the interior boundaries can take on the shape of more complex polygons. The number of cuts, n c , is sampled from the distribution n c \u223c \u230a10 \u2022 Beta(\u03b1 c , \u03b2 c ) + 1 /2\u230b, where \u03b1 c = nr /2 and \u03b2 c = 6. Figure 20 shows the distribution that is formed with respect to the number of rooms in the house, n r . When there are more rooms, the probability distribution over the number of cuts increases. Since the range of the beta distribution is (0, 1), the upper bound on the number of cuts is exactly 10.\nThe size of each cut is a rectangle, in meters, denoted by (c x , c z ). Both c x and c z are sampled from integer distributions. We sample from\nc x \u223c U (1, max(2, min(x s \u2212 1, \u230aa max /2\u230b) \u2212 1), inclusive,\nwhere a max is set to 6 representing the maximum cut area. We then sample\nc z \u223c U (1, a max \u2212 c x ).\nThe position of where the cut happens is anchored to one of the 4 corners of the interior boundary, where the exact corner is independently and uniformly sampled each time.\nSince the size of each cut is an integer, and the rectangular boundary sizes are also integers, we can efficiently represent the interior boundary with a (x s , z s ) boolean matrix. Here, we could have 1s representing where the inside of the interior boundary and 0s representing the outside of the interior house boundary.\nGiven a room spec and an interior boundary, we use the algorithm proposed in [73] to divide the interior boundary into rooms. The algorithm recursively subdivides the interior boundary for each subtree in the room spec. Figure 21 shows an example using Figure 18a's room spec. The algorithm first divides the interior boundary into two zones, the \"bedroom & bathroom\" zone and the \"kitchen & living room\" zone. The \"bedroom & bathroom\" zone then subdivides into two rooms, the bedroom and bathroom. Similarly, the \"kitchen & living room\" zone is also subdivided into two rooms, the   18a. Here, we first divide the room into a \"bedroom & bathroom\" and a \"kitchen & living room\" zone. Then, within the \"bedroom & bathroom\" zone we place both the bedroom and bathroom, and within the \"kitchen & living Room\" zone, we place both the kitchen and living room.\nkitchen and living room. The growth weight is used to approximate the size of each subdivision. By recursively subdividing the zones of each subtree, we satisfy the constraint that we can traverse between child nodes of the same parent in the room spec.\nFinally, we scale the entire floor plan by s \u223c U (1.6, 2.2). Scaling the interior boundary to be larger provides more room for the agent to be able to navigate within the houses. Using a range of values also provides more variability on the size of the houses. We set the upper bound to 2.2 based on the empirical quality of the houses, where values above that often left too much empty space. Figure 22 shows the 3 types of ways adjacent rooms may be connected. Specifically, rooms may be connected using 3 different types of connections: doorways, door frames, or open room connections. We determine which rooms should have doors between them based on the constraints in the room spec. Amongst adjacent rooms that may have doors between them, subject to the constraints in the room spec, we randomly sample which rooms have doors. We also impose the constraint that neighboring rooms in the room spec may have at most 1 room connected to it.", "publication_ref": [], "figure_ref": ["fig_10", "fig_1", "fig_1", "fig_5", "fig_5", "fig_1"], "table_ref": []}, {"heading": "B.4 Connecting Rooms", "text": "To choose the type of connection, we consider the rooms we are connecting. Specifically, we only allow open room connections and door frame connections between kitchen and living room room types. We impose this constraint because it would be unrealistic for a room like a bathroom to be fully visible from another room. For connecting room types that do support open room connections or door frames, we annotate the probability of sampling a doorway, door frame, and open room If a doorway or door frame is sampled, we filter to use a valid asset that is smaller than the wall connecting the rooms. For our generation, the minimum wall size is always greater than a single door size, but occasionally the filter might remove double doors from valid doors that can be sampled as they would be too big. The placement of the door is then uniformly sampled from anywhere along the wall. For doorways, the open direction is uniformly sampled. Finally, if the open state from any 2 doorways collides, we also use rejection sampling to potentially change the open direction and modify the placement of doorways.\nEach house also has a permanently closed exterior door connecting to the outside. We prioritize placing this door in kitchen and living room room types, as it is unnatural to have to go through a bathroom or bedroom to go outside. However, in the case where the room spec does not include a kitchen or living room (e.g. if the room is a standalone bathroom), we randomly place a door to the outside in one of the remaining rooms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.5 Structure Materials", "text": "Wall materials. To choose the materials that make up the walls, we consider 2 families of wall materials: solid colors and texture-based materials. Our solid color materials consist of 40 unique colors of popular paint colors found in houses. We constrain ourselves to only using popular paint colors, so we do not randomize the walls to unrealistic colors such as bright green or yellow. For the texture-based materials, we annotate 122 different AI2-THOR materials to be suitable as wall materials. These include materials for brick textures, drywall textures, and tiling textures, amongst others.\nEach wall in a room shares the same materials. For each room, we sample it if its materials are a solid color with w solid \u223c Bernoulli(0.5). It is sometimes the case in real life that all rooms in a house share the same material (e.g. every room in an apartment is painted with white walls). We therefore also have a parameter w same \u223c Bernoulli(0.35) that specifies if all rooms in the house will have the same material.\nCeiling material. The entire ceiling of the house is always assigned to a single wall material. If w same , then the ceiling material is also set to the wall material. Otherwise, it is independently sampled with the same wall material sampling process.\nFloor materials. We annotate 55 materials in AI2-THOR as floor materials. Most commonly, these materials are wood materials. For each room, we independently sample its floor material from the set of annotated floor materials. However, similar to wall materials, we independently sample f same \u223c Bernoulli(0.15) that specifies if all rooms in the house will have the same material.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.6 Ceiling Height", "text": "The ceiling height for the house, in meters, is sampled from c h \u223c h min +(h max \u2212h min )\u2022Beta(\u03b1 h , \u03b2 h ), where we set h min = 2.5, h max = 7, \u03b1 h = 1.25, and \u03b2 h = 5.5. Figure 23 shows the ceiling height distribution that is formed. All rooms in the house have the same ceiling height.\nThe minimum and mean values were chosen based on the typical height of an American apartment, while \u03b2 h allows some of the train houses to have much larger ceilings.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "B.7 Lighting", "text": "Lighting Placement. Each procedural house places two types of lights: a directional light and point lights. The directional light is analogous to the sun in the scene, where only 1 is placed in each scene.\nLight from point lights are analogous to the light emitted from lightbulbs. We place a point light in each room near the ceiling, centered at the centroid of the room's floor polygon. Using the centroid ensures that the light is always placed inside of the room, even for L-shaped rooms. Additionally, desk lamp and floor lamp objects have a point light associated with them. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.8 Object Placement", "text": "In this section, we discuss how objects are placed realistically in the house. We hypothesize reasonable object placement is necessary in order to train efficient agents. For instance, if a toilet could appear anywhere in the house, the agent would have a much harder search problem, leading to longer episodes, than if the toilet was always in the bathroom. Moreover, we do not want objects to appear in unnatural positions, such as a fridge facing the wall, as it would make it unnatural, and even unusable, for interaction.\nFinally, we do not always want objects to spawn independently. For instance, we might want a table to be surrounded by chairs. We achieve dependant sampling by developing SAGs, which are described in the section that follows.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.8.1 Assets", "text": "The ProcTHOR asset database consists of 1,633 interactive household assets across 108 object types (see Appendix A for more details). The majority of assets come from AI2-THOR. Windows, doors, and counter tops are built into the exterior of rooms in AI2-THOR, which prevents us from spawning them in as standalone assets. Thus, we have also hand-built 21 windows, 20 doors, and 33 counter tops.\nAsset Annotations. Our assets include several annotations that help us place them realistically in a house. Figure 25 shows an example of the asset annotations used to place an arm chair. For an individual asset, we annotate its object type, computationally obtain its 3D bounding box, and partition assets of object types into training, validation, and testing splits. Then, we annotate how each object type might be spawned into the house. Annotating the 108 object types, as opposed to annotating the 1,633 individual assets, allows us to scale up the number of unique assets dramatically. Moreover, it does not require any new annotation to add an asset that can be grouped with an existing object type.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "In Living Rooms 2 Object Type Arm Chair", "text": "Bounding Box (0.9, 0.88, 0.74)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Split Train On Edge", "text": "In Kitchens 0 In Bedrooms 1\nIn Bathrooms 0", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Object Type Annotations Asset Annotations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "On Floor Multiple Per Room In Corner", "text": "In Middle Figure 25: An example of the asset annotations used to place an arm chair asset. This particular instance is annotated with its object type, bounding box, and split. Annotations about how it is placed in the house are done at an object type level, applying to all instances of that type.\nIf instances of an object type cannot be placed independently on the floor, the rest of its annotations are not considered. For instance, we do not allow television object types to be placed alone on the floor, rather they are often placed on top of a television stand or mounted on the wall, which is discussed later in this section. Similarly, we also annotate small objects, like a fork, pen, and mug to not be placed independently on the floor. However, typical large object types, such as counter top, arm chair, or fridge object types can be placed independently on the floor.\nAmong the remaining object types, we annotate where and in which rooms the object type may appear. Each object type has a room weight, r w \u2208 {0, 1, 2, 3}, corresponding to how likely it is to appear in each room type. For each room type, a 0 indicates the object should never appear (e.g., a fridge in a bathroom); a 1 indicates the object may appear, but is unlikely; a 2 indicates that the object appears quite often; and a 3 indicates that the object nearly always appears (e.g., a bed in a bedroom). To determine where the object is placed, we annotate whether it may appear on the edge, in the corner, or in the middle of a room. For example, we annotate that a fridge can be placed on the edge or in the corner of the room, but not in the middle. We also annotate whether there can be multiple instances of an object type in a single room. Here, we annotate that multiple toilet object types cannot be in the same room, for instance.\nAsset Splits. If an object type has over 5 unique assets, then those assets are partitioned into train, validation, and testing splits. Specifically, approximately 2 /3 of the assets are assigned to the train split, and approximately 1 /6 of the assets are assigned to each of the validation and testing splits. For object types that have 5 or fewer unique assets, they may appear in any split. In general, the more visual diversity an object type has, the more instances of that object type exist. For instance, there are many chair objects, but there are much fewer CD, toilet, and fork objects. Appendix A shows the precise count of each object type.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "B.8.2 Semantic Asset Groups (SAGs)", "text": "A Semantic Asset Group (SAG) provides a flexible and diverse way to encode which objects may appear near each other. The power of SAGs comes in their ability to support randomized asset and rotational sampling. SAGs can be created and exported in seconds with our user-friendly drag-and-drop web interface. Figure 26 shows an example of how we might construct a SAG that has two chairs pushed into the side of a dining  allow for multiple instances of the same chair to be placed at a dining table, instead of independently sampling a different chair for each sampler.\nThe ability to randomly sample assets to place in a SAG is incredibly expressive. For instance, consider a SAG with samplers for a TV stand, television, sofa, and arm chair. If each of these samplers can sample from just 30 different 3D modeled asset instances, then there are over 800k unique combinations of instances that can make be sampled from that SAG.\nAsset samplers define how assets are positioned relative to one another. SAGs are constructed by looking at instances of asset samplers from their top-down orthographic images, such as the one shown in Figure 26a. Here, both of the chair samplers are parented to the dining table sampler. Each child asset sampler is anchored to its parent asset sampler vertically in V = {TOP, CENTER, BOTTOM} and horizontally in H = {LEFT, CENTER, RIGHT}. Each child asset sampler's pivot position can similarly be set vertically in V and horizontally in H. For instance, in Figure 26a, both chair samplers are anchored to the parent vertically on TOP and horizontally in the CENTER. But, the chair sampler on the left's pivot position is vertically in the CENTER and horizontally on the RIGHT, whereas the chair sampler on the right's pivot position is vertically in the CENTER and horizontally on the LEFT. Figure 27 shows more examples of how a plant or floor lamp sampler may be positioned around an arm chair sampler. Each child asset sampler can then have an (x, y) offset, which is the distance from the parent sampler's anchor point to the child sampler's pivot position.  The motivation for the relative positioning of asset samplers is to prevent the meshes from clipping into each other. For instance, with the same SAG in Figure 26a, consider what would happen if the dining table sampler samples a table that is double the size of the current table. Instead of the chairs being stuck in a fixed global position, and effectively colliding with the new dining table, the chairs will reactively move back, and be re-positioned to remain slightly tucked under the larger table. Moreover, consider that the size of instances that are sampled from an asset sampler are often quite different. For instance, one table might be square-ish, while another is elongated. If we only used a CENTER CENTER pivot and an offset, one would not be able to reliably place asset samplers, containing differently sized objects, directly beside each other without it resulting in clipping.\nWhile setting anchoring and pivot positions solves many mesh clipping issues, some cases may still arise. Figure 28 shows an example, where if our dining table sampler samples a short dining table, it may clip into certain chairs. Such issues are rare in practice, but object clipping would lead to less realistic and interactive houses. To solve the clipping issue, we use rejection sampling to resample the assets of a SAG until none of the 3D meshes of the sampled assets are clipping.\nIn PROCTHOR-10K, we construct 18 SAGs, which can be instantiated with over 20 million unique combinations of assets. These include semantic asset groups for chairs around tables, pillows on top of beds, sofas and arm chairs looking at a television on top of a TV stand, faucets on top of sinks, and a desk with a chair, amongst others.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "B.8.3 Floor Object Placement", "text": "We start object placement by first placing objects on the floor of the house. Objects are independently placed on a room-by-room basis, where we may first place objects in the bedroom and then place objects in the bathroom, without either affecting each other.\nFor each room, we filter the objects down into only using objects that have a room weight r w > 0 in the given room type, and that have the annotation that they can be placed on the floor. Here, for instance, a chair object may have the annotation that it can be placed on the floor, but a knife object may not.\nAt this stage, we simplify rooms to just look at the top-down 2D bounding box that makes up the room in the floor plan. We also simplify objects to just look at its top-down 2D bounding box, of size (o w , o h ). These simplifications make it easier to determine if an object will fit in the room, specifically in a particular rectangle.\nFigure 29 illustrates the iterative process of placing objects in the scene. First, the polygon forming the area left to place an object is partitioned into rectangles. The rectangles come from drawing a horizontal and vertical grid line at all corner points of the open polygon. Here, we can easily obtain the largest rectangle remaining in the open room polygon. We sample r \u2113 \u223c Bernoulli(0.8) to determine if the next object to be placed should be placed inside of the largest rectangle. Otherwise, we randomly choose amongst all possible rectangles, weighted by the area of each rectangle.\nOnce we have the rectangle (r w , r h ) where the object should be placed in, we filter our objects to only those that would fit, both semantically and physically, in the rectangle. Semantically, we consider 3 scenarios: the rectangle being on the corner, edge, or middle of the room's polygon.\nIf any of the rectangle's corners is in a corner of the room, then we will place an object in that corner of the room. If multiple of the rectangle's corners are in a corner of the room, then we uniformly sample a corner amongst one of those corners. Now, we will filter down objects and asset groups to only consider:  1. Those that are annotated specifying that they can be placed in the corner of the room. For example, we might annotate a fridge to be placed in the corner of the room, but we might not annotate a SAG consisting of a dining table to be placed in the corner of the room. 2. The annotated split of the asset instance matches the current split of the generated house. See Appendix B.8.1 which talks about asset splits to create train/val/test homes. 3. The top-down bounding box of the object (with margin) must fit within the chosen rectangle. For a corner object, Figure 30b shows the 2 valid rotations that this object may take on. Specifically, the back of the object may be against either wall. Then, we filter down remaining objects to only use those where the object's bounding box fits within the rectangle's bounding box; that is, (o h + w pad \u2264 r w and o w + w pad \u2264 r h ) or (o h + w pad \u2264 r h and o w + w pad \u2264 r w ). If both conditions are valid, we uniformly choose one of the rotations of the object's bounding box.\nWe add margin around objects to make sure it is always possible to navigate around them. Objects to be placed in the middle of the room have m pad = 0.35 meters of margin on each side. Objects on the edge or corner of the room have w pad = 0.5 meters of margin only in front of the object, which enables objects to be placed directly beside it.\nWe sample an object or asset group that satisfies all of the previous conditions. If there are no objects or asset groups that satisfy all conditions, we continue to the next iteration and remove the selected rectangle from consideration. We slightly prioritize placing asset groups over standalone assets when Objects placed on the edge or corner of the room always have their backs to the wall. Objects in the middle of the scene can be rotated in any direction. By constraining rotations of objects, we ensure an object on the edge of the room, such as a fridge or drawer, can still be opened.\npossible. Once we have chosen an object or asset group, the bounding box with margin is then anchored to the corner of the rectangle, and hence to the corner of the room. We then subtract the object's bounding box, with margin, from the open polygon representing the space remaining in the room before doing the same process again.\nIf the rectangle is along the edge, we sample r edge \u223c Bernoulli(0.7) to determine if we should try to place an object on the edge of the rectangle, or if we should try and place it in the middle. If the rectangle is not along the edge or on the corner of the room, then we will always try to place an object in the middle of it. We use a similar filtering process, as the one described with edge rectangles, to filter down objects to those that only fit within the bounds of the rectangle. However, as depicted in Figure 30a and Figure 30c, edge objects can only have their backs to the wall, and middle objects can be rotated in any 90-degree rotation.\nThe iterative process of sampling a rectangle from the open polygon of the room, placing an object in that rectangle, and subtracting the bounding box formed by the object in the rectangle, continues on for r i , where r i is sampled from\nr i \u223c \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 p = 1 /200 4 p = 2 /200 5 p = 4 /200 6 p = 20 /200 7 p = 173 /200 .(1)\nSampling r i allows us to infrequently have rooms in the house where there are very few objects, which is sometimes the case in real-world homes. It should also be noted that there can be more than r i objects on the floor of the scene if some objects in the scene are in SAGs.\nBy iteratively choosing the largest, or near largest, rectangle in the room's open polygon, placing an object in it, and subtracting the object's bounding box with margin from the open room polygon, we enable great coverage across the entirety of the room, and hence the entirety of the house.", "publication_ref": [], "figure_ref": ["fig_1", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "B.8.4 Wall Object Placement", "text": "After placing objects on floors, we then place objects on walls. We currently place window, painting, and television objects on the walls. Figure 31 shows some examples. Window and television objects may appear in kitchen, living room, and bedroom room types. Paintings may appear in any room type.\nWindows. Window objects are the first objects we place on the walls of the house. We only consider placing a window on walls that are connected to the outside of the house, such that we do not place a \nmaximum window objects to be placed.\nFor each wall in a given room, we look at the segment formed by each edge connecting 2 adjacent corners. If there is a floor object placed along that edge (or corner) of the wall, we subtract it from the segment. Here, the segment may break into different segments, where each segment is treated just like the original one. If the length of any segment is smaller than the minimum window size in the split, we remove the segment. We then use a uniform sample over the remaining segments, weighted by their lengths, to determine where to place the window. If no segments are longer than the smallest window, we move on to the next room in the house. A window smaller than the length of the segment is then uniformly placed somewhere along the sampled segment. The window is vertically centered along the wall between the floor and w max = min(3, c h ). All segments along the wall where the window was placed are removed from future sampling calls, and we continue this process n w times.\nPaintings. Painting objects are placed on the walls after window objects. They may be placed in any room. The maximum number of painting objects that are attempted to be placed in each room is sampled from\nn p \u223c \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 0 p = 0.05 1 p = 0.1 2 p = 0.5 3 p = 0.25 4 p = 0.1 .(3)\nThe placement of painting objects is similar to the placement of window objects. However, multiple painting objects may be placed along the same wall, so instead of removing the entire wall segment after an object is placed on it, we subtract the width of the painting from the segment. Moreover, we also allow painting objects to be placed above edge floor objects if the height of the edge object is less than 1.15 meters. Here, this allows for a painting to be above an object like a counter top, but not behind a taller object like a fridge.\nThe vertical position of each painting is sampled at o y \u223c w min + (w max \u2212 w min ) \u2022 Beta (12,12), where w min is the maximum height of a floor object along the wall line. Here, we allow a painting to be placed above an object along the wall of the room, such as placing it above a counter top. Sampling from Beta (12,12) allows for some randomness in the sampling process while still having a large density near the center.\nTelevisions. Television wall objects may only be placed in living room, kitchen, and bedroom room types. Only 1 wall television may be placed in each room. From our annotations, television objects cannot be placed standalone on the floor. However, a television is often placed in a SAG, on top of an object like a TV stand. So as to not place too many television objects in the same room, we only filter by rooms that do not have a television object already in them. Amongst the remaining rooms, if the room type is a living room, we sample Bernoulli(0.8) if we should try placing a wall television in the room. For kitchen and bedroom room types, we sample from Bernoulli(0.25) and Bernoulli(0.4), respectively. We only consider television objects that could be mounted to a wall (i.e. they do not have a base that is sticking out of the object). Television wall objects sample from the same vertical position distribution as painting objects, and follow the same placement on the walls as painting objects.", "publication_ref": ["b10", "b10", "b10", "b10"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "B.8.5 Surface Object Placement", "text": "After placing objects on the floor and wall of the house, we focus on placing objects on the surface of the floor objects just placed. For example, we may place objects like a coffee machine, plate, or knife on of a receptacle like a counter top.\nFor each receptacle object, we approximate the probability that each object type appears on its surface. We use the hand-modeled AI2-iTHOR or RoboTHOR rooms to obtain these approximations. Here, we compute the total number of times each object type is on the receptacle type and divide it by the total number of times the receptacle type appears across the scenes.\nFor each receptacle placed on the floor, we look at the probability of each object type p spawn that it has been placed on that receptacle. We then iterate over the object types that may be on the receptacle. For each object type, we try spawning it on the receptacle if Bernoulli(p spawn + b house + b receptacle + b object ), where \u2022 b house denotes the additional bias of how likely objects are to be spawned on receptacles in this particular house. Each house samples\nb house \u223c (b house-max \u2212 b house-min ) \u2022 Beta(3.5, 1.9) + b house-min ,(4)\nwhere b house-min = \u22120.3 and b house-max = 0.1. Figure 32 shows the distribution that b house forms. Using a house bias allows for some houses to be much cleaner or dirtier than others, whereas cleaner houses would have more objects put away that are not on receptacles.\n\u2022 b receptacle denotes the additional bias of how likely an object is to be spawned on a receptacle. The default receptacle bias is 0.2, which is only overwritten by shelving unit (0.4 bias), counter top (0.2 bias), arm chair (0 bias), and chair (0 bias). Receptacle biases were manually set based on the empirical quality of the houses.\n\u2022 b object denotes the additional bias of how likely a particular object is to spawn in the scene. By default, b object is set to 0, and overwritten by house plant (0.25 bias), basketball (0.2 bias), spray bottle (0.2 bias), pot (0.1 bias), pan (0.1 bias), bowl (0.05 bias), and baseball bat (0.1 bias). Object biases were also manually set based on the empirical quality of the houses to ensure more target objects appear in each of the procedurally generated houses.  Note that p spawn + b house + b receptacle + b object may be greater than 1, in which case we will always try to spawn the object on the receptacle, or less than 0, where we will never try to spawn the object on the receptacle.\nTo attempt to spawn an object of a given type on a receptacle, we will sample an instance of that object type and randomly try n pa = 5 poses of the object to try and fit the object instance on the receptacle.\nIf the object instance fits and does not collide with another object, we keep it there. Otherwise, we try another pose of the object on the receptacle until we reach n pa attempted poses. If none of the attempted poses work, we continue on to the next object type that may be on the receptacle.\nIf the first object of a given type is placed successfully on a receptacle, we attempt to place n or \u223c min(s max , Geom(p spawn ) \u2212 1) \u2212 1 more objects of that type given type on the receptacle. Here, s max is set to 3, representing the maximum number of objects of a type that may be on a receptacle. We ignore the biases to not have too many objects of a given type on the same receptacle.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "B.9 Material and Color Randomization", "text": "Several object types may have their color randomized to a randomly sampled RGB value. Specifically, for each vase, statue, or bottle in the scene, we independently sample from r c \u223c Bernoulli(0.8) to determine if we should randomize the object's color. These objects were chosen because they all still looked natural as any solid color. Figure 33a shows some examples of randomizing the color of a vase.\nFor each training episode, we sample from r m \u223c Bernoulli(0.8) to determine if we should randomize the default object materials in the scene. Wall, ceiling, and floor materials are left untouched to preserve w solid and w same sampling parameters. Materials are only randomized within semantically similar classes, which ensures objects still look and behave like the class they represent. For instance, an apple will not swap materials with an orange. Figure 33b shows some examples of randomizing the materials in the scene.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "B.10 Object States", "text": "We randomize object states to expose the agent to more diverse objects during training. For instance, instead of always having an open laptop or a clean bed, we randomize the openness of each laptop and  if each bed is clean or dirty. Figure 34 shows some examples. Our current set of state randomizations include:\n\u2022 Toggling objects. Floor lamp and desk lamp object types have their state toggled on or off.\n\u2022 Cleaning or dirtying objects. Bed object types may appear as either clean or dirty.\n\u2022 Opening or closing objects. Box and laptop object types may toggling objects on or off (for floor lamp and desk lamp object types), setting objects to clean or dirty (for bed object types), and openness randomizations (for box and laptop object types).", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "B.11 Validator", "text": "Once a house is generated, we use a validator to make sure that the agent can successfully navigate to each room in the house, without modifying the scene through interaction (e.g. moving an object out of the way). Specifically, we first make sure the agent can teleport to a location inside the house.\nThen, from that position, we perform a BFS over neighboring positions on a 0.25 \u00d7 0.25 meter grid to obtain all reachable positions from the agent's current position. The validator checks to make sure that every room in the house has at least 5 reachable positions on the grid. If the validator fails, we resample a new house using the same room spec, so as to not change the distribution of room specs that we sample from.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.12 Limitations and Future Work", "text": "ProcTHOR-10K only uses 1-floor houses. We plan to support multi-floor houses in ProcTHOR-v2.0. This will allow us to capture a wider range of houses and provide better fine-tuning results. Additionally, we plan to scale up our asset databases by leveraging many open-source 3D asset databases, such as ABO [26], PartNet [81], ShapeNet [15], Google Scanned Objects [30], and CO3D [97], among others.", "publication_ref": ["b24", "b13", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "C PROCTHOR Datasheet Motivation", "text": "For what purpose was the dataset created?\nThe dataset was created to enable the training of simulated embodied agents in substantially more diverse environments.\nWho created and funded the dataset?\nThis work was created and funded by the PRIOR team at Allen Institute for AI. See the contributions section for specific details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Composition", "text": "What do the instances that comprise the dataset represent?\nEach house is specified as a JSON file, which specifies how to populate a 3D Unity scene in AI2-THOR.\nHow many instances are there in total (of each type, if appropriate)?\nThere are 10K houses released in the dataset, along with the code to sample substantially more. Section 4 shows the distribution of houses in PROCTHOR-10K.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\nWe make 10K houses available, but more houses can easily be sampled with the procedural generation scripts.\nWhat data does each instance consist of?\nEach house is specified as a JSON file, which precisely describes how our AI2-THOR build should create the house. The procedurally generated JSON files are typically several thousand lines long.\nHas the dataset been used for any tasks already?\nYes. See Section 5 of the paper.\nWhat (other) tasks could the dataset be used for?\nThe houses can be used in a wide variety of interactive tasks in embodied AI and computer vision.\nAny task that can be performed in AI2-THOR can be performed in ProcTHOR. For instance, in embodied AI, the houses may be used for navigation [58,89,118,132,117,128,74,130], multi-agent interaction [51, 52, 2], rearrangement and interaction [115,36,39,23,106], manipulation [33,86,32,122], Sim2Real transfer [27,54,66], embodied vision-andlanguage [105, 87, 48, 65, 42, 55], audio-visual navigation [22,38,21], and virtual reality interaction [119,83,46], among others.\nIn the broader field of computer vision, the dataset may be used to study object detection [64]; NeRFs [80,110,43,71]; segmentation, depth, and optimal flow estimation [35,43]; generative modeling [59, 62, 61]; occlusion reasoning [34]; and pose estimation [19], among others.\nOur framework for loading in procedurally generated houses from a JSON spec also enables the study of scene clutter generation, building more realistic procedurally generated homes, and the development of synthetically generated spaces to train embodied agents in factories [84], offices, grocery stores [76], and full procedurally generated cities.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nNo.\nAre there tasks for which the dataset should not be used?\nOur dataset may be used for both commercial and noncommercial purposes.", "publication_ref": ["b44", "b58", "b43", "b54", "b56", "b41", "b34", "b21", "b31", "b30", "b48", "b25", "b20", "b36", "b19", "b45", "b33", "b32", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Distribution", "text": "Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?\nYes. We plan to make the entirety of the work open-source, including the code used to generate and load houses, the initial static dataset of 1 million procedurally generated house JSON files, and the asset and material databases.\nHow will the dataset be distributed? The static house JSON files will be distributed with a custom Python package.\nThe code, asset, and material databases will be distributed on GitHub.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\nThe house dataset, 3D asset database, and generation code will be released under the Apache 2.0 license.\nHave any third parties imposed IPbased or other restrictions on the data associated with the instances?\nNo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D ARCHITECTHOR", "text": "Figure 35: Top-down images of the 5 custom-built interactive validation houses in ARCHITECTHOR. The goal of these houses is to evaluate interactive agents in more realistic and larger home environments.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "D.1 Datasheet Motivation", "text": "For what purpose was the dataset created?\nARCHITECTHOR was created to enable the evaluation of embodied agents in large, realistic, and interactive household environments.\nWho created and funded the dataset?\nThis work was created and funded by the PRIOR team at Allen Institute for AI. See the contributions section for specific details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Composition", "text": "What do the instances that comprise the dataset represent?\nInstances of the dataset comprise interactive 3D houses that were built in Unity and can be used with our custom build of the AI2-THOR API.\nHow many instances are there in total (of each type, if appropriate)?\nThere are 10 total houses, comprising 5 validation houses and 5 testing houses.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\nThe dataset is self-contained.\nWhat data does each instance consist of?\nEach instance of a house is a Unity scene, which includes data such as the placement of objects, lighting, and texturing.\nIs there a label or target associated with each instance?\nNo.\nIs any information missing from individual instances?\nNo.\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\nEach house was independently created.\nAre there recommended data splits? Yes. The houses themselves are partitioned as 5 validation houses and 5 testing houses. The assets placed in the house follow the same train/val/test splits used in PROCTHOR-10K.\nAre there any errors, sources of noise, or redundancies in the dataset?\nNo.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\nThe dataset is self-contained.\nDoes the dataset contain data that might be considered confidential?\nNo.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\nNo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collection Process", "text": "How was the data associated with each instance acquired?\nEach house was professionally hand-modeled by 3D artists. Most objects placed in the hosues come from the PROCTHOR asset database. However, countertops, showers, and many cabinets were custom built.\nIf the dataset is a sample from a larger set, what was the sampling strategy?\nThe dataset consists of 1 million houses sampled from the procedural generation scripts.\nOver what timeframe was the data collected?\nThe houses were built towards the beginning of 2022.\nWere any ethical review processes conducted?\nNo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preprocessing/Cleaning/Labeling", "text": "Was any preprocessing/cleaning/labeling of the data done?\nNo.\nWas the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data?\nThere is no raw data associated with the ARCHITECTHOR houses.\nIs the software that was used to preprocess/clean/label the data available?\nYes. We will open-source the ARCHITECTHOR houses and they can be opened and viewed in Unity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Uses", "text": "Has the dataset been used for any tasks already?\nYes. Please see Section 5 of the paper. What (other) tasks could the dataset be used for?\nThe tasks can be used for any type of navigation and interaction tasks in embodied AI. The houses are built into our build of AI2-THOR, meaning ARCHITECTHOR can work with any task that can be performed in AI2-THOR.\nWe especially think ARCHITECTHOR will be useful as an evaluation suite for evaluating different sets of PROCTHOR tasks and evaluating agents trained on different sets of procedurally generated houses.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nNo.\nAre there tasks for which the dataset should not be used?\nOur dataset may be used for both commercial and noncommercial purposes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distribution", "text": "Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?\nYes. All houses in ARCHITECTHOR will be released to the open-source community and available through our build of the AI2-THOR Python API.\nHow will the dataset be distributed? The houses will be distributed on GitHub and available to open as Unity scenes.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\nARCHITECTHOR will be released under the Apache 2.0 license.\nHave any third parties imposed IPbased or other restrictions on the data associated with the instances?\nNo.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\nNo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Maintenance", "text": "Who will be supporting/hosting/maintaining the dataset?\nThe authors will be providing support, hosting, and maintaining the dataset.\nHow can the owner/curator/manager of the dataset be contacted?\nOmitted for anonymous review.\nIs there an erratum?\nWe will use GitHub issues to track issues with the dataset once it is published.\nWill the dataset be updated? ARCHITECTHOR is currently in maintenance mode and we do not expect it to update much from its current state. However, we plan to actively support future AI2-THOR functionalities in ARCHITECTHOR, such as support for new robots, more advanced interaction capabilities, and bug fixes.\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\nThe dataset does not relate to people.\nWill older versions of the dataset continue to be supported/hosted/maintained?\nYes. Revision history will be available in the GitHub repository.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? Yes. The work will be open-sourced and we intend to provide support to help others use and build upon the dataset. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Analysis", "text": "ARCHITECTHOR consists of 10 remarkably high-quality large interactive 3D houses. Figure 35 shows top-down images of the 5 validation houses. Figure 36 shows some examples of images taken inside of 2 kitchens and a bedroom from ARCHITECTHOR validation.\nARCHITECTHOR was built to be much larger than AI2-iTHOR and RoboTHOR. Figure 37 shows the size comparisons between comparable hand-built scene datasets in AI2-iTHOR and RoboTHOR, measured in navigable area. Notice that the navigable area in ARCHITECTHOR is substantially larger than in those. The figure also shows the navigable areas in PROCTHOR-10K span the spectrum of navigable areas between AI2-iTHOR, RoboTHOR, and ARCHITECTHOR.  In total, the creation of the 10 houses in ARCHITECTHOR took approximately 320 hours of cumulative work by professional 3D artists. Figure 38 shows the time breakdown of which parts of the process took the longest. In particular, the creation of custom assets for the kitchen, such as modeling each of the countertops and cabinets, took the longest amount of time, followed by modeling the 3D structure of house.    ", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "E Input Modalities", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Experiment details", "text": "This section discusses the training details used for our experiments. We discuss baselines, PROCTHOR pre-training, and environment-specific fine-tuning details for the tasks of ObjectNav, ArmPointNav, and rearrangement.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1 ObjectNav experiments", "text": "For ObjectNav experiments, agents are given a target object type (e.g. a bed) and are tasked with finding a path in the environment that navigates to that target object type. The task setup matches what is commonly used in embodied AI [27,7,58,96], although we only utilize forward-facing egocentric RGB images at each time step. All ObjectNav experiments are trained with a simulated LoCoBot (Low Cost Robot) agent [12]. The task and training details are described below.\nEvaluation. Following [6], an ObjectNav task is considered successful if all of the following conditions are met:\n1. The agent terminates the episode by issuing the DONE action.\n2. The target object type is within a distance of 1 meter from the agent's camera.\n3. The object is visible in the final frame from the agent's camera. For instance, if (1) and ( 2) are satisfied, and the agent is looking in the direction of the object, but the target object is occluded behind a wall, then the task is unsuccessful. Similarly, if the target object type is located in the opposite direction of where the agent is looking, then the task will be unsuccessful.\nWe also use SPL to evaluate the efficiency of the agent's trajectory to the target object. SPL is defined and discussed in [6,7]. A house may have multiple instances of objects for a given type that the agent can successfully reach. For instance, a house may have multiple bedrooms, where each bedroom includes a bed. Here, if the agent navigates to any of the beds, the episode is successful. To calculate SPL in these scenarios, the shortest path length for the task is the minimum shortest path length from the starting position of the agent to any of the reachable target objects of the given type, regardless of which instance the agent navigates towards.\nActions. For each of the trained models, we use a discrete action space consisting of 6 actions, which is shown in Table 6. Following common practice [27,54], we use stochastic actuation to better simulate noise in the real world.", "publication_ref": ["b25", "b5", "b10", "b4", "b4", "b5", "b25"], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "Action Description", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MOVEAHEAD", "text": "Attempts to move the agent forward by \u03b4 m \u223c N (\u00b5 = 0.25, \u03c3 = 0.01) meters from its current facing direction. If moving the agent forward by \u03b4 m meters results in a collision in the scene (e.g. there is a wall directly in-front of the agent within \u03b4 m meters), the action fails and the agent's position remains unchanged.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ROTATERIGHT ROTATELEFT", "text": "Rotates the agent rightwards or leftwards from its current forward facing direction by \u03b4 r \u223c N (\u00b5 = 30, \u03c3 = 0.5) degrees.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LOOKUP LOOKDOWN", "text": "Tilts the agent's camera up or down by 30 degrees.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DONE", "text": "A signal from the agent to terminate the episode and evaluate the trajectory from its current state. Discussed in [6]. \u2022 \u03c1 is the step penalty that encourages the agent to finish the episode quickly. It is set to 0.01.\nProcTHOR pre-training. We pre-train our ObjectNav agents on the full set of 10k training houses in PROCTHOR-10K. 1 We pre-train with all T = 16 target object types, which are shown in  Sampling target object types. To sample the target object type for a given episode, we restrict ourselves to only sampling target object types that have a possibility of leading to a successful episode. For instance, even if there is an object like an apple in the scene, it might be located in the fridge, and so if it was used as a target object, the agent would never succeed because the object would never appear visible in the frame (without any manipulation actions). Therefore, we impose a constraint that the target object must be visible without any form of manipulation.\nFor each house, we use an approximation to determine the set of target object instances that the agent can successfully reach, without any manipulation. Specifically, we start by teleporting the agent into the house, and then perform a BFS over a 0.25 \u00d7 0.25 meter grid to obtain the reachable positions in the scene. A position is considered reachable if teleporting to it would not cause any collisions with any other objects, and the agent is successfully placed on the floor. Then, for each candidate instance of every target object type, we look at the nearest 6 reachable agent positions \u27e8x (a) , z (a) \u27e9 to the candidate object instance's center position. For each reachable agent position, we perform a raycast from the agent's camera height y (a) to up to 6 random visibility points on the object \u27e8x (o) , y (o) , z (o) \u27e9.\nEach object is annotated with visibility points, which are used as a fast approximation to determine if an object is visible with just using a few raycasts, instead of using full segmentation masks. If any of the raycasts from the agent's reachable position to the object's visibility point do not have any collisions with other objects (e.g. the raycast does not collide with the outside of the fridge), and the L2 distance between \u27e8x (o) , y (o) , z (o) \u27e9 and \u27e8x (a) , y (a) , z (a) \u27e9 is less than 1 meter, then the object instance is considered successfully reachable by the agent.\nTo choose a target object type, we use an \u03f5-greedy sampling method. Specifically, with a probability of \u03f5 = 0.2, we randomly sample a target object type that has at least 1 reachable object instance in a given house. With a probability of 1 \u2212 \u03f5, the target object type is the target object type that has been most infrequently sampled in the training process. Since some objects appear much more frequently than others (e.g. beds appear in many more houses than baseball bats), sampling based on the least commonly sampled target object types allows us to maintain a more uniform distribution of sampled target object types.\nRoboTHOR. RoboTHOR is evaluated in both a 0-shot and fine-tuned setting. For 0-shot, we take the pre-trained model on PROCTHOR-10K and run it on the RoboTHOR evaluation tasks. For fine-tuning, we reduce T to the 12 RoboTHOR target object types, shown in Table 8 and train on the 60 provided training scenes. We fine-tune for 29 million steps, before validation performance starts to go down, on a machine with 8 NVIDIA Quadro RTX 8000 GPUs. Fine-tuning took about 7 hours to complete.\nHM3D-Semantics. We evaluate on HM3D-Semantics in both a 0-shot and fine-tuned setting using the \"ProcTHOR\" and \"ProcTHOR+Large\" architectures described above, these two architectures have slightly different pretraining strategies.\n\"ProcTHOR\" model. For 0-shot, we take the pre-trained model on PROCTHOR-10K, and run it on the HM3D-Semantics evaluation tasks. For fine-tuning, we reduce T to the 6 target object types used in HM3D-Semantics (see Table 8) and train on the 80 provided training houses. We use an early checkpoint from PROCTHOR pre-training, specifically from after 220 million steps. We performed fine-tuning on a machine with 8 NVIDIA RTX A6000 GPUs for approximately 220M steps, which took about 43 hours to complete.\n\"ProcTHOR+Large\" model. We pre-train this model using PROCTHORLARGE-10K a variant of PROCTHOR-10K with houses sampled to better align to the distribution of houses in HM3D. In particular, PROCTHORLARGE-10K contains 10K procedurally generated houses each of which contains between 4 and 10 rooms (houses in PROCTHORLARGE-10K thus tend to be much larger than houses in PROCTHOR-10K). Moreover, during pretraining we only train our agent to navigate to the 6 object categories used in HM3D-Semantics. Fine-tuning is done identically as above. We use an early checkpoint from PROCTHOR pre-training, specifically from after 125 million steps. We performed fine-tuning on a machine with 8 NVIDIA RTX A6000 GPUs for approximately 185M steps taking 85 hours to complete.\nAI2-iTHOR. Similar to RoboTHOR and HM3D-Semantics, we use AI2-iTHOR for both 0-shot and fine-tuning. For 0-shot, we take the pre-trained model on PROCTHOR-10K, and run it on the AI2-iTHOR evaluation tasks. Since the AI2-iTHOR evaluation tasks use the full set of target objects used during PROCTHOR pre-training, we do not need to update T . For fine-tuning, we use a machine with 8 TITAN V GPUs. We fine-tune for approximately 2 million steps before validation performance starts to go down, which takes about 1.5 hours to complete.\nArchitecTHOR. Since ARCHITECTHOR does not include any training scenes, we only use it for evaluation of the PROCTHOR pre-trained model. As shown in Table 8, ARCHITECTHOR evaluation uses the full-set of target object types that are used during PROCTHOR pre-training.", "publication_ref": ["b4", "b0"], "figure_ref": [], "table_ref": ["tab_16", "tab_16", "tab_16"]}, {"heading": "F.2 ArmPointNav experiments", "text": "In ArmPointNav, we followed the same architecture as [33]. The task is to move a target object from a starting location to a goal location using the relative location of the target in the agent's coordinate frame. The visual input is encoded using 3 convolutional layers followed by a linear layer to obtain a 512 feature vector. The 3D relative coordinates, specifying the targets, are embedded using three linear layers to a 512 embedding which combined with the visual encoding is input to the GRU. The agent is allowed to take up to 200 steps or the episode will automatically fail.\nProcTHOR pre-training. We pre-train our model on a subset of 7000 houses, on 58 object categories. For each episode, we move the agent to a random location, randomly choose an object in the room that is pickupable, and randomly select a target location. We train our model for 100M frames, running on 4 AWS g4dn.12xlarge machines. Running on a total of 16 GPUs and 192 CPU cores took 3 days of training.  AI2-iTHOR evaluation. We evaluate our model on 20 test rooms of AI2-THOR (5 kitchens, 5 living rooms, 5 bedrooms, 5 bathrooms), on a subset of 28 object categories for a total of 528 tasks. We attempted to perform fine-tuning on AI2-iTHOR, but none of the fine-tuning models performed better than the zero-shot model trained with PROCTHOR pre-training.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": []}, {"heading": "F.3 Rearrangement experiments", "text": "Following [115,58], we use imitation learning (IL) to train all models for the 1-phase modality of the task. We divide the full training of the final model into two stages: pre-training in PROCTHOR and fine-tuning in AI2-iTHOR.  ProcTHOR pre-training. We pre-train our model on a subset of 2,500 one and two-room PROCTHOR-10K houses where a number of 1 to 5 objects are shuffled from their target poses in each episode, including two shuffle modalities: different openness degree (at most one object in an episode) and a different location (up to five objects in an episode). For each house, 20 episodes are sampled such that all shuffled objects are in the same room where the agent is initially spawned. We train with 2 \u2022 10 5 steps of teacher forcing and 2 million steps of dataset aggregation [99], followed by about 180 million steps of behavior cloning. We use a small set of 200 episodes sampled from 20 validation houses unseen during training to select a checkpoint to evaluate every 5 million steps.  ", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "G Performance Benchmark", "text": "To calculate the FPS performance benchmark shown in the Analysis section, we partitioned houses into small houses (1-3 room houses) and large houses (7-10 room houses). For the navigation benchmark, we perform move and rotate actions. For the interaction benchmark, we performing a pushing object action. For querying the environment for data, we obtain a piece of metadata from the environment that is not commonly provided at each time step (e.g. checking the dimensions of the agent). At each time step, we render a single 3 \u00d7 224 \u00d7 224 RGB image from the agent's egocentric perspective. Experiments were conducted on a server with 8 NVIDIA Quadro RTX 8000 GPUs. We employ 15 processes for the single GPU tests and 120 processes for the 8 GPU tests, evenly divided across the GPUs. Table 11 shows the comparisons to AI2-iTHOR and RoboTHOR.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "H Broader Impact", "text": "This work focuses on increasing the generalization abilities of robotic agents on various tasks. We specifically focus on robots that operate in household environments. More capable robotic agents can help improve the lives of many by assisting with cooking, cleaning, and providing social interaction. Furthermore, robots can provide a wide range of health benefits. For example, they could give domestic assistance to individuals with physical and mental disabilities and the elderly. They could provide social and emotional support to children, adolescents, and adults, such as delivering personalized educational content, reducing loneliness, and counseling in times of crisis. We can also use home-assisted robots to monitor and provide feedback on people's physical activity, sleep, and diet.\nHowever, the adoption of home-assisted robots could have several undesirable social consequences.\nOne is that home-assisted social robots may lead individuals to become more dependant on robots for companionship and care, leading to increased social isolation and loneliness. Another concern is that they may exacerbate existing inequities, as those who can afford to buy and maintain robots will have access to care and assistance that those who cannot will not. Furthermore, because robots would have access to sensitive information about people's daily lives, they could threaten privacy and security. Finally, robots have the potential to be exploited for malicious intent, such as for mass surveillance or being used for autonomous warfare. As a community, we need to work to reduce the risks of social robots while maximizing the benefits for the common good.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank the teams behind the open-source packages used in this project, including AI2-THOR [63], AllenAct [116] ", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A ProcTHOR Assets 18 B House Generation 18", "text": "Is there a label or target associated with each instance?\nNo.\nIs any information missing from individual instances?\nNo.\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\nEach house is generated independently, meaning there are no relationships between the houses.\nAre there recommended data splits? Yes. See Appendix B.8.1.\nAre there any errors, sources of noise, or redundancies in the dataset?\nNo.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\nThe dataset is self-contained.\nDoes the dataset contain data that might be considered confidential?\nNo.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\nNo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collection Process", "text": "How was the data associated with each instance acquired?\nEach house was procedurally generated. See Appendix A.\nIf the dataset is a sample from a larger set, what was the sampling strategy?\nThe dataset consists of 1 million houses sampled from the procedural generation scripts.\nWho was involved in the data collection process?\nThe authors were the only people involved in constructing the dataset.\nOver what timeframe was the data collected?\nData was collected between the end of 2021 and the beginning of 2022.\nWere any ethical review processes conducted?\nNo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preprocessing/Cleaning/Labeling", "text": "Was any preprocessing/cleaning/labeling of the data done? Section B.8 describes the labeling that was done to make the assets spawn in realistic places.\nWe have also gone through every asset in the asset database to make sure the pivots for each asset are facing a consistent direction.\nWas the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data?\nThere is no raw data associated with the house JSON files.\nIs the software that was used to preprocess/clean/label the data available?\nThe code to generate the houses is made available.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Uses", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?", "text": "No.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Maintenance", "text": "Who will be supporting/hosting/maintaining the dataset?\nThe authors will be providing support, hosting, and maintaining the dataset.\nHow can the owner/curator/manager of the dataset be contacted? For inquiries, email <mattd@allenai.org>.\nIs there an erratum?\nWe will use GitHub issues to track issues with the dataset.\nWill the dataset be updated?\nWe expect to continue adding support for new features to continue to make procedurally generated houses even more diverse and realistic. We also intend to support new tasks in the future.\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\nThe dataset does not relate to people.\nWill older versions of the dataset continue to be supported/hosted/maintained?\nYes. Revision history will be available for older versions of the dataset.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\nYes. The work will be open-sourced and we intend to provide support to help others use and build upon the dataset. Except for the \"ProcTHOR+Large\" model trained for HM3D (described below), we otherwise use the same model architecture across ObjectNav experiments. Namely, at each time step, the agent receives a 3 \u00d7 224 \u00d7 224 egocentric RGB image from its camera. The image is processed with a frozen RN50 CLIP-ResNet visual encoder [93] to produce a 2048 \u00d7 7 \u00d7 7 visual embedding, V t . The embedding is compressed through a 2-layer CNN (going from 2048 to 128 to 32 channels) with 1 \u00d7 1 convolutions [108] to obtain a 32 \u00d7 7 \u00d7 7 tensor, V \u2032 t . The target object type is represented as an integer in {0, 1, . . . , T }, where T is the number of target object types used during training. We use an embedding of t to obtain a 32-dimensional vector. The vector is resized to be a 32 \u00d7 1 \u00d7 1 tensor. The tensor is then expanded to be of size 32 \u00d7 7 \u00d7 7, to form our goal target object type embedding G t , where the 32 \u00d7 1 \u00d7 1 tensor is copied 7 \u00d7 7 times.\nWe concatenate V \u2032 t and G t to form a 64 \u00d7 7 \u00d7 7 tensor, which is compressed with a 2-layer CNN to form a 32 \u00d7 7 \u00d7 7 tensor, Z t . The tensor Z is flattened to form a 1568 dimensional vector, z t . Following [86], we use an embedding of the previous action, represented as an integer in {0, 1, . . . , 5}, to obtain a 6 dimensional vector a t\u22121 . We concatenate z t and a t\u22121 to form a 1574 dimensional vector x t . The vector x t is passed through a 1-layer GRU [24,25] with a hidden belief state b t\u22121 , of size 512, to obtain b t .\nUsing an actor-critic formulation, the 512-dimensional belief state b t is passed through a 1-linear layer, representing the actor, to get a 6-dimensional vector, where each entry represents an action. The 6-dimensional vector is passed through a softmax function to obtain the agent's policy \u03c0 (i.e. the probability distribution over the action space). We sample from \u03c0 to choose the next action. We also pass the belief state b t through a separate 1-linear layer, representing the critic to obtain the scalar v, estimating the value of the current state.\nThe \"ProcTHOR+Large\" is similar to the above except we: (1)  Training. Each agent is trained using DD-PPO [103, 117], using a clip parameter \u03f5 = 0.1, an entropy loss coefficient of 0.01, and a value loss coefficient of 0.5. Agents are trained to maximize the cumulative discounted rewards H t=0 \u03b3 t \u2022 r t , where we set the discount factor \u03b3 to 0.99 and the episode's horizon H to 500 steps. We also employ GAE [102] parameterized by \u03bb = 0.95.\nReward. The reward function follows that of [58]. Specifically, at each time step, it is calculated as r t = max(0, min \u2206 0:t\u22121 \u2212 \u2206 t ) + s t \u2212 \u03c1, where:\n\u2022 min \u2206 0:t\u22121 is the minimum L2 distance from the agent to any of the reachable instances of the target object type that the agent has observed over steps {0, 1, . . . , t \u2212 1}.\n\u2022 \u2206 t is the current L2 distance from the agent to the nearest reachable instance of the target object type.\n\u2022 s t is the reward for successfully completing the episode. If the agent takes the DONE action and the episode is deemed successful, then s t is 10. Otherwise, it is 0.", "publication_ref": ["b22", "b23", "b0"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tensorflow: A system for large-scale machine learning", "journal": "", "year": "2016", "authors": "Mart\u00edn Abadi; Paul Barham; Jianmin Chen; Z Chen; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Geoffrey Irving; Michael Isard; Manjunath Kudlur; Josh Levenberg; Rajat Monga; Sherry Moore; Derek Gordon Murray; Benoit Steiner; Paul A Tucker; Vijay Vasudevan; Pete Warden; Martin Wicke; Yuan Yu; Xiaoqiang Zhang"}, {"ref_id": "b1", "title": "", "journal": "", "year": "2021", "authors": "Anuj Adam; Catarina Mahajan; Charlie Barros; Jakob Deck; Jakub Bauer; Maja Sygnowski; Max Trebacz; Michael Jaderberg;  Mathieu"}, {"ref_id": "b2", "title": "Flamingo: a visual language model for few", "journal": "Oriol Vinyals, Andrew Zisserman, and Karen Simonyan", "year": "", "authors": "Jeff Jean-Baptiste Alayrac; Pauline Donahue; Antoine Luc; Iain Miech; Yana Barr; Karel Hasson; Arthur Lenc; Katie Mensch; Malcolm Millican; Roman Reynolds; Eliza Ring; Serkan Rutherford; Tengda Cabi; Zhitao Han; Sina Gong; Marianne Samangooei;  Monteiro"}, {"ref_id": "b3", "title": "", "journal": "", "year": "", "authors": "A I Allen Institute For;  Robothor Objectnav Challenge"}, {"ref_id": "b4", "title": "", "journal": "", "year": "2018", "authors": "Peter Anderson; Angel X Chang; Devendra Singh Chaplot; Alexey Dosovitskiy; Saurabh Gupta; Vladlen Koltun; Jana Kosecka; Jitendra Malik; Roozbeh Mottaghi; Manolis Savva; Amir Roshan Zamir"}, {"ref_id": "b5", "title": "Objectnav revisited: On evaluation of embodied agents navigating to objects", "journal": "", "year": "2020", "authors": "Dhruv Batra; Aaron Gokaslan; Aniruddha Kembhavi; Oleksandr Maksymets; Roozbeh Mottaghi; Manolis Savva; Alexander Toshev; Erik Wijmans"}, {"ref_id": "b6", "title": "Experiment tracking with weights and biases, 2020. Software available from wandb", "journal": "", "year": "", "authors": "Lukas Biewald"}, {"ref_id": "b7", "title": "", "journal": "", "year": "2016", "authors": "Greg Brockman; Vicki Cheung; Ludwig Pettersson; Jonas Schneider; John Schulman"}, {"ref_id": "b8", "title": "Language models are few-shot learners", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; T J Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeff Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin; Scott Gray; ; ; Dario Amodei"}, {"ref_id": "b9", "title": "nuscenes: A multimodal dataset for autonomous driving", "journal": "", "year": "", "authors": "Holger Caesar; Varun Bankiti; Alex H Lang; Sourabh Vora; Venice Erin Liong; Qiang Xu; Anush Krishnan; Yu Pan; Giancarlo Baldan; Oscar Beijbom"}, {"ref_id": "b10", "title": "Locobot: an open source low cost robot", "journal": "", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Interactive learning of spatial knowledge for text to 3d scene generation", "journal": "", "year": "2014", "authors": "Angel Chang; Manolis Savva; Christopher D Manning"}, {"ref_id": "b12", "title": "ShapeNet: An Information-Rich 3D Model Repository", "journal": "", "year": "2015", "authors": "Angel X Chang; Thomas Funkhouser; Leonidas Guibas; Pat Hanrahan; Qixing Huang; Zimo Li; Silvio Savarese; Manolis Savva; Shuran Song; Hao Su; Jianxiong Xiao; Li Yi; Fisher Yu"}, {"ref_id": "b13", "title": "Shapenet: An information-rich 3d model repository. arXiv", "journal": "", "year": "2015", "authors": "Angel X Chang; Thomas A Funkhouser; Leonidas J Guibas; Pat Hanrahan; Qi-Xing Huang; Zimo Li; Silvio Savarese; Manolis Savva; Shuran Song; Hao Su; Jianxiong Xiao; Li Yi; Fisher Yu"}, {"ref_id": "b14", "title": "Text to 3d scene generation with rich lexical grounding", "journal": "", "year": "2015", "authors": "Angel X Chang; Will Monroe; Manolis Savva; Christopher Potts; Christopher D Manning"}, {"ref_id": "b15", "title": "Learning spatial knowledge for text to 3d scene generation", "journal": "", "year": "2014", "authors": "Angel X Chang; Manolis Savva; Christopher D Manning"}, {"ref_id": "b16", "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "journal": "", "year": "", "authors": "Soravit Changpinyo; Piyush Kumar Sharma; Nan Ding; Radu Soricut"}, {"ref_id": "b17", "title": "Camera pose estimation in multi-view environments: From virtual scenarios to the real world", "journal": "Image Vis. Comput", "year": "2021", "authors": "Jorge L Charco; Angel Domingo Sappa; Boris Xavier Vintimilla; Henry O Velesaca"}, {"ref_id": "b18", "title": "Learning generative models of 3d structures", "journal": "", "year": "2019", "authors": "Siddhartha Chaudhuri; Daniel Ritchie; Kai Xu; Hao Zhang"}, {"ref_id": "b19", "title": "Semantic audio-visual navigation", "journal": "", "year": "2021", "authors": "Changan Chen; Ziad Al-Halah; Kristen Grauman"}, {"ref_id": "b20", "title": "Soundspaces: Audio-visual navigation in 3d environments", "journal": "", "year": "2020", "authors": "Changan Chen; Unnat Jain; Carl Schissler; Sebastia Vicenc Amengual; Ziad Gari;  Al-Halah; Krishna Vamsi; Philip Ithapu; Kristen Robinson;  Grauman"}, {"ref_id": "b21", "title": "Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning. arXiv", "journal": "", "year": "2021", "authors": "Rohan Chitnis; Tom Silver; Joshua B Tenenbaum; Tomas Lozano-Perez; Leslie Pack Kaelbling"}, {"ref_id": "b22", "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"ref_id": "b23", "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "journal": "", "year": "2014", "authors": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b24", "title": "Abo: Dataset and benchmarks for real-world 3d object understanding", "journal": "", "year": "2022", "authors": "Jasmine Collins; Shubham Goel; Kenan Deng; Achleshwar Luthra; Leon Xu; Erhan Gundogdu; Xi Zhang; Tomas F Yago Vicente; Thomas Dideriksen; Himanshu Arora; Matthieu Guillaumin; Jitendra Malik"}, {"ref_id": "b25", "title": "Robothor: An open simulation-to-real embodied ai platform", "journal": "", "year": "2008", "authors": "Matt Deitke; Winson Han; Alvaro Herrasti; Aniruddha Kembhavi; Eric Kolve; Roozbeh Mottaghi; Jordi Salvador; Dustin Schwenk; Eli Vanderbilt; Matthew Wallingford; Luca Weihs; Mark Yatskar; Ali Farhadi"}, {"ref_id": "b26", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b27", "title": "Emergent complexity and zero-shot transfer via unsupervised environment design", "journal": "", "year": "", "authors": "Michael Dennis; Natasha Jaques; Eugene Vinitsky; Alexandre Bayen; Stuart Russell; Andrew Critch; Sergey Levine"}, {"ref_id": "b28", "title": "Google scanned objects: A high-quality dataset of 3d scanned household items", "journal": "", "year": "", "authors": "Laura Downs; Anthony Francis; Nate Koenig; Brandon Kinman; Ryan Hickman; Krista Reymann; B Thomas; Vincent Mchugh;  Vanhoucke"}, {"ref_id": "b29", "title": "Learning controllable content generators", "journal": "", "year": "", "authors": "Sam Earle; Maria Edwards; Ahmed Khalifa; Philip Bontrager; Julian Togelius"}, {"ref_id": "b30", "title": "Object manipulation via visual target localization", "journal": "", "year": "", "authors": "Kiana Ehsani; Ali Farhadi; Aniruddha Kembhavi; Roozbeh Mottaghi"}, {"ref_id": "b31", "title": "Aniruddha Kembhavi, and Roozbeh Mottaghi. ManipulaTHOR: A Framework for Visual Object Manipulation", "journal": "", "year": "2009", "authors": "Kiana Ehsani; Winson Han; Alvaro Herrasti; Eli Vanderbilt; Luca Weihs; Eric Kolve"}, {"ref_id": "b32", "title": "Segan: Segmenting and generating the invisible", "journal": "", "year": "2018", "authors": "Kiana Ehsani; Roozbeh Mottaghi; Ali Farhadi"}, {"ref_id": "b33", "title": "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges", "journal": "IEEE Trans. on Intelligent Transportation Systems", "year": "2021", "authors": "Di Feng; Christian Haase-Schuetz; Lars Rosenbaum; Heinz Hertlein; Fabian Duffhauss; Claudius Gl\u00e4ser; Werner Wiesbeck; Klaus C J Dietmayer"}, {"ref_id": "b34", "title": "Continuous scene representations for embodied ai", "journal": "", "year": "", "authors": "Kiana Samir Yitzhak Gadre; Shuran Ehsani; Roozbeh Song;  Mottaghi"}, {"ref_id": "b35", "title": "Threedworld: A platform for interactive multi-modal physical simulation", "journal": "", "year": "2021", "authors": "Chuang Gan; Jeremy Schwartz; Seth Alter; Martin Schrimpf; James Traer; Julian De Freitas; Jonas Kubilius; Abhishek Bhandwaldar; Nick Haber; Megumi Sano; Kuno Kim; Elias Wang; Damian Mrowca; Michael Lingelbach; Aidan Curtis; Kevin T Feigelis; Daniel Bear; Dan Gutfreund; David Cox; James J Dicarlo; Josh H Mcdermott; Joshua B Tenenbaum; Daniel L K Yamins"}, {"ref_id": "b36", "title": "Boqing Gong, and Joshua B Tenenbaum. Look, listen, and act: Towards audio-visual embodied navigation", "journal": "", "year": "", "authors": "Chuang Gan; Yiwei Zhang; Jiajun Wu"}, {"ref_id": "b37", "title": "Yfcc100m: the new data in multimedia research", "journal": "", "year": "2016", "authors": "Bart Thomee; David A Shamma; Gerald Friedland; Benjamin Elizalde; Karl S Ni; Douglas N Poland; Damian Borth; Li-Jia Li"}, {"ref_id": "b38", "title": "Scipy 1.0: fundamental algorithms for scientific computing in python", "journal": "Nature methods", "year": "2020", "authors": "Pauli Virtanen; Ralf Gommers; E Travis; Matt Oliphant; Tyler Haberland; David Reddy; Evgeni Cournapeau; Pearu Burovski; Warren Peterson; Jonathan Weckesser;  Bright"}, {"ref_id": "b39", "title": "Sceneformer: Indoor scene generation with transformers", "journal": "", "year": "", "authors": "Xinpeng Wang; Chandan Yeshwanth; Matthias Nie\u00dfner"}, {"ref_id": "b40", "title": "Seaborn: statistical data visualization", "journal": "Journal of Open Source Software", "year": "2021", "authors": "L Michael;  Waskom"}, {"ref_id": "b41", "title": "Visual room rearrangement", "journal": "", "year": "2021", "authors": "Luca Weihs; Matt Deitke; Aniruddha Kembhavi; Roozbeh Mottaghi"}, {"ref_id": "b42", "title": "AllenAct: A framework for embodied AI research", "journal": "", "year": "2020", "authors": "Luca Weihs; Jordi Salvador; Klemen Kotar; Unnat Jain; Kuo-Hao Zeng; Roozbeh Mottaghi; Aniruddha Kembhavi"}, {"ref_id": "b43", "title": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames", "journal": "", "year": "2019", "authors": "Erik Wijmans; Abhishek Kadian; Ari Morcos; Stefan Lee; Irfan Essa; Devi Parikh; Manolis Savva; Dhruv Batra"}, {"ref_id": "b44", "title": "Learning to learn how to learn: Self-adaptive visual navigation using meta-learning", "journal": "", "year": "2019", "authors": "Mitchell Wortsman; Kiana Ehsani; Mohammad Rastegari; Ali Farhadi; Roozbeh Mottaghi"}, {"ref_id": "b45", "title": "Communicative learning with natural gestures for embodied navigation agents with human-in-the-scene", "journal": "", "year": "", "authors": "Qi Wu; Cheng-Ju Wu; Yixin Zhu; Jungseock Joo"}, {"ref_id": "b46", "title": "Data-driven interior plan generation for residential buildings", "journal": "ACM Trans. on Graphics", "year": "2019", "authors": "Wenming Wu; Xiao-Ming Fu; Rui Tang; Yuhan Wang; Yu-Hao Qi; Ligang Liu"}, {"ref_id": "b47", "title": "Building generalizable agents with a realistic and rich 3d environment", "journal": "", "year": "2018", "authors": "Yi Wu; Yuxin Wu; Georgia Gkioxari; Yuandong Tian"}, {"ref_id": "b48", "title": "Roberto Mart'in-Mart'in, Or Litany, Alexander Toshev, and Silvio Savarese. Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation", "journal": "", "year": "", "authors": "Fei Xia; Chengshu Li"}, {"ref_id": "b49", "title": "Gibson env: Real-world perception for embodied agents", "journal": "", "year": "2018", "authors": "Fei Xia; Zhiyang Amir R Zamir; Alexander He; Jitendra Sax; Silvio Malik;  Savarese"}, {"ref_id": "b50", "title": "SAPIEN: A SimulAted Part-based Interactive ENvironment", "journal": "", "year": "2020", "authors": "Fanbo Xiang; Yuzhe Qin; Kaichun Mo; Yikuan Xia; Hao Zhu; Fangchen Liu; Minghua Liu; Hanxiao Jiang; Yifu Yuan; He Wang; Li Yi; Angel X Chang; Leonidas Guibas; Hao Su"}, {"ref_id": "b51", "title": "Objectnet3d: A large scale database for 3d object recognition", "journal": "", "year": "2016", "authors": "Yu Xiang; Wonhui Kim; Wei Chen; Jingwei Ji; Christopher Bongsoo Choy; Hao Su; Roozbeh Mottaghi; Leonidas J Guibas; Silvio Savarese"}, {"ref_id": "b52", "title": "Hydra -a framework for elegantly configuring complex applications. Github", "journal": "", "year": "2019", "authors": "Omry Yadan"}, {"ref_id": "b53", "title": "Evalai: Towards better evaluation systems for ai agents. arXiv", "journal": "", "year": "2019", "authors": "Deshraj Yadav; Rishabh Jain; Harsh Agrawal; Prithvijit Chattopadhyay; Taranjeet Singh; Akash Jain;  Shiv Baran; Stefan Singh; Dhruv Lee;  Batra"}, {"ref_id": "b54", "title": "Clip on wheels: Zero-shot object navigation as object localization and exploration", "journal": "", "year": "", "authors": "Mitchell Samir Yitzhak Gadre; Gabriel Wortsman; Ludwig Ilharco; Shuran Schmidt;  Song"}, {"ref_id": "b55", "title": "Deep generative modeling for scene synthesis via hybrid representations", "journal": "ACM Trans. on Graphics", "year": "", "authors": "Zaiwei Zhang; Zhenpei Yang; Chongyang Ma; Linjie Luo; Alexander G Huth; Etienne Vouga; Qixing Huang"}, {"ref_id": "b56", "title": "Towards Optimal Correlational Object Search", "journal": "", "year": "", "authors": "Kaiyu Zheng; Rohan Chitnis; Yoonchang Sung; George Konidaris; Stefanie Tellex"}, {"ref_id": "b57", "title": "Scenegraphnet: Neural message passing for 3d indoor scene augmentation", "journal": "", "year": "2019", "authors": "Yang Zhou; Zachary While; Evangelos Kalogerakis"}, {"ref_id": "b58", "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "journal": "", "year": "2017", "authors": "Yuke Zhu; Roozbeh Mottaghi; Eric Kolve; J Joseph; Abhinav Lim; Li Gupta; Ali Fei-Fei;  Farhadi"}, {"ref_id": "b59", "title": "robosuite: A modular simulation framework and benchmark for robot learning. arXiv", "journal": "", "year": "2020", "authors": "Yuke Zhu; Josiah Wong; Ajay Mandlekar; Roberto Mart\u00edn-Mart\u00edn"}, {"ref_id": "b60", "title": "Semantic Asset Groups (SAGs)", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "[5] -0-shot performance superior to the previous SoTA which uses RoboTHOR training scenes -with fine-tuning we obtain an 8.8 point improvement in SPL over the previous SoTA; (2) Habitat ObjectNav Challenge 2022 [79] -top of the leaderboard results with a >3 point gain in SPL over the next best submission; (3) 1-phase Rearrangement Challenge 2022 [4] -top of the leaderboard results with Prop Fixed Strict improving from 0.19 to 0.245; (4)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Procedurally generating a house using PROCTHOR.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Floorplan diversity. Examples showing the diversity of the generated floorplans. Rooms in the house are colored by Bedroom, Bathroom, Kitchen, and Living Room.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Material augmentation. Different materials for objects and structural elements like walls and floors.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Object placement. Four examples of object placement within the same room layout.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Interactivity. Object states can change (e.g., the laptop or the lamp in the left panel), and the agents can interact with objects and other agents (middle and right panels).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 10 :10Figure 10: PROCTHOR-10K statistics. Left: distribution of the number of objects in each room; Middle: distribution of the area of each house, bucketed into small, medium, and large houses; Right: bar plot showing the distribution over the number of rooms that make up each house.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 11 shows11examples of ego-centric and top-down views of houses present in PROCTHOR-10K.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 12 :12Figure 12: Examples and statistics of assets in the asset database.", "figure_data": ""}, {"figure_label": "131415161718", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :131415161718Figure 13: Examples of 3-room houses generated in PROCTHOR-10K.", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 19 :19Figure 19: An example of the interior boundary cut algorithm. The images show a top-down view of the house's floor plan. First, we sample an interior boundary rectangle (x s , z s ), which is shown on the left.Then, we make n c rectangular cuts to the corners of the rectangle to make the interior boundary of the house a more complex polygon. In this case, we make n c = 3 cuts to form the final interior boundary, which is shown on the right.", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 20 :20Figure 20:  The probability distribution over the number of cuts, n c , made to the rectangular boundary (x s , z s ) with respect to the number of rooms in the house, n r . Notice that when there are more rooms in the house, the number of cuts in the distribution increases.", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 21 :21Figure 21: An example of the recursive floor plan generation algorithm, given an interior boundary and the room spec in Figure18a. Here, we first divide the room into a \"bedroom & bathroom\" and a \"kitchen & living room\" zone. Then, within the \"bedroom & bathroom\" zone we place both the bedroom and bathroom, and within the \"kitchen & living Room\" zone, we place both the kitchen and living room.", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 22 :22Figure 22: An example of the 3 ways to connect different rooms, using either a doorway, door frame, or open room connection.", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 23 :23Figure 23: The distribution of the ceiling height of each house, in meters.", "figure_data": ""}, {"figure_label": "24", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 24 :24Figure 24: Examples different skyboxes in a scene. Notice how the colors of the images differ and how the content outside of the window changes with the skybox.", "figure_data": ""}, {"figure_label": "26", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 26 :26Figure 26: An example of a semantic asset group (SAG), where two chair samplers are parented to a dining table sampler. Both chairs are anchored to the top middle of the table.", "figure_data": ""}, {"figure_label": "27", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 27 :27Figure 27: Instantiations of a SAG that places a plant or floor lamp sampler S c around a parented arm chair sampler S p with anchor and pivot position annotations. Notice that the placement from S c reacts to the size of the asset sampled from S p . None of the examples have any offset.", "figure_data": ""}, {"figure_label": "28", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 28 :28Figure28: Rejection sampling is used to make sure objects placed in SAGs do not collide. Left: the chair collides with the dining table, and hence it is rejected; Right: none of the objects in the instantiated SAG collide with each other, so the SAG is accepted as valid.", "figure_data": ""}, {"figure_label": "29", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 29 :29Figure 29: Diagram detailing how floor objects are placed in a room. First, we rectangularize the top-down view of the room's open floor plan by drawing horizontal and vertical dividers from each corner point. Then, we construct all possible rectangles that are formed within the dividers. We then sample one of those rectangles and place the object within that rectangle. The sampled object's top-down bounding box (with margin) is shown in blue. The bounding box is then subtracted from the open floor plan before repeating the process again.", "figure_data": ""}, {"figure_label": "30", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure 30 :30Figure 30: Valid rotations of objects when placed on the edge, corner, and middle of the room.Objects placed on the edge or corner of the room always have their backs to the wall. Objects in the middle of the scene can be rotated in any direction. By constraining rotations of objects, we ensure an object on the edge of the room, such as a fridge or drawer, can still be opened.", "figure_data": ""}, {"figure_label": "31", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Figure 31 :31Figure 31: Examples of objects placed on the wall of a house.", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_23", "figure_caption": "Figure 32 :32Figure 32: The house bias distribution b house that offsets the probability of attempting to spawn an object in a receptacle.", "figure_data": ""}, {"figure_label": "a", "figure_type": "figure", "figure_id": "fig_24", "figure_caption": "( a )aExamples of color randomization for a vase object. The original color is shown on the left. Notice that the vase still looks realistic with many possible colors.(b) Examples of material randomization in ProcTHOR. Notice that only the objects randomize in materials, where the walls, floor, and ceiling remain the same.", "figure_data": ""}, {"figure_label": "33", "figure_type": "figure", "figure_id": "fig_25", "figure_caption": "Figure 33 :33Figure 33: Examples of color randomization and material randomization in ProcTHOR.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_26", "figure_caption": "(a) Openness state randomness example with a laptop. (b) Clean state randomness example with a bed. (c) On or off state randomness with a floor lamp.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_27", "figure_caption": "Figure 34 :34Figure 34: Examples of object state randomness.", "figure_data": ""}, {"figure_label": "36", "figure_type": "figure", "figure_id": "fig_28", "figure_caption": "Figure 36 :36Figure 36: Examples of images inside of 2 hand-modeled kitchens and 1 hand-modeled bathroom from ARCHITECTHOR validation.", "figure_data": ""}, {"figure_label": "37", "figure_type": "figure", "figure_id": "fig_29", "figure_caption": "Figure 37 :37Figure 37: Box plots of the navigable areas for ARCHITECTHOR compared to AI2-iTHOR, RoboTHOR, and PROCTHOR-10K. Validation scenes were used to calculate the data for ARCHITECTHOR, and training scenes were used to calculate the data for AI2-iTHOR, RoboTHOR, and PROCTHOR-10K.", "figure_data": ""}, {"figure_label": "38", "figure_type": "figure", "figure_id": "fig_31", "figure_caption": "Figure 38 :38Figure 38: Cumulative time breakdown of the development of ARCHITECTHOR across 3D artists.", "figure_data": ""}, {"figure_label": "39", "figure_type": "figure", "figure_id": "fig_33", "figure_caption": "Figure 39 :39Figure 39: Examples of image-based modalities available in ProcTHOR include RGB, depth, instance segmentation, semantic segmentation, bounding box annotations, and surface normals. More image modalities can be added by modifying the Unity backend.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Example scenes in PROCTHOR-10K with top-down and an egocentric view.", "figure_data": "Navigation FPSIsolated Interaction FPSEnvironment Query FPSComputeSmallLargeSmallLargeSmallLarge8 GPUs8,599\u00b1359 3,208\u00b11276,488\u00b1250 2,861\u00b1107480,205\u00b119,684 433,587\u00b118,7291 GPU1,427\u00b1746,280\u00b1401,265\u00b171597\u00b137160,622\u00b12,846157,567\u00b12,6891 Process240\u00b169115\u00b119180\u00b14293\u00b11514,825\u00b119914,916\u00b1186"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Rendering speed.", "figure_data": "Benchmarking FPS for navigation (e.g. moving/rotating), interaction(e.g. pushing an object), and querying the environment for data (e.g. checking the dimensions of theagent). We report FPS for Small and Large houses. See Appendix for details.objects sampled via 18 different Semantic Asset groups. Examples of Semantic asset groups (SAG)are a Dining Table with 4 Chairs or Bed with 2 Pillows. Given our large asset library and SAGs, wecan create 19.3 million combinations of group instantiations.Rendering speed. A crucial requirement for large-scale training is high rendering speed since thetraining algorithms require millions of iterations to converge. Table 1 shows these statistics. Experi-ments were run on a server with 8 NVIDIA Quadro RTX 8000 GPUs. For the 1 GPU experiments,we use 15 processes and for the 8 GPU experiments, we use 120 processes, evenly distributed acrossthe GPUs. PROCTHOR provides framerates comparable to iTHOR and RoboTHOR environmentsin spite of having larger houses (See Appendix for details), rendering it fast enough for training largemodels for hundreds of millions of steps in a reasonable amount of time."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": ", the performance improves as we use more houses for training, demonstrating the benefits of large-scale data for Embodied AI tasks.", "figure_data": "TaskBenchmarkMethodMetricsSuccessSPLObjectNavRoboTHOR Challenge EmbCLIP [58] a47.0%0.200ProcTHOR 0-shot55.0%0.237ProcTHOR + fine-tune65.2%0.288SuccessSPLMLNLC c52.0%0.280Habitat ChallengeFusionNav (AIRI) c54.0%0.270ObjectNav(2022)ProcTHOR 0-shot9.00%0.055HM3D-SemanticsProcTHOR + fine-tune53.0%0.270ProcTHOR + Large d + 0-shot13.2%0.077ProcTHOR + Large d + fine-tune 54.4%0.318SuccessSPLObjectNavAI2-iTHOREmbCLIP [58] b68.4%0.516ProcTHOR 0-shot75.7%0.644ProcTHOR + fine-tune77.5%0.621SuccessSPLObjectNavARCHITECTHOREmbCLIP [58] b18.5%0.118ProcTHOR31.4%0.195Success % Fixed StrictRearrangementAI2-THOR Challenge EmbCLIP [58]7.10%0.1901-phase (2022)ProcTHOR 0-shot3.80%0.156ProcTHOR + fine-tune7.40%0.245Success % PickUp SRArmPointNavManipulaTHORiTHOR-SimpleConv [33] e29.2%73.4ProcTHOR 0-shot37.9%74.8"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Results for models trained on ProcTHOR and evaluated 0-shot and with fine-tuning onseveral E-AI benchmarks. For each benchmark we also compare to the relevant baselines (previousSoTA or leaderboard submissions where applicable). a EmbCLIP [58] trained on ROBOTHOR,b EmbCLIP [58] trained on AI2-iTHOR, c submission on the Habitat 2022 ObjectNav leaderboard [79].d For HM3D we present results when pretraining using the standard EmbCLIP architecture (whichuses a CLIP-pretrained ResNet50 backbone) as well as with a \"Large\" model which uses a largerCLIP backbone CNN as well as a wider RNN, see supplement for details. e uses the model from [33]but retrains on the complete iTHOR data with RGB inputs.0-shot results, whereby models arepre-trained on PROCTHOR-10K and do not use any training data from the benchmark that they areevaluated on.ARCHITECTHORROBOTHORHM3DAI2-iTHORTestTest (0-Shot)Valid (0-Shot)Test (0-Shot)# HOUSESSPLSRSPLSRSPLSRSPLSR10 Houses 0.07711.3%0.040 8.53% 0.007 1.60% 0.249 28.7%100 Houses 0.10218.6%0.076 20.9% 0.050 10.4% 0.352 42.0%1,000 Houses 0.12217.2%0.157 33.1% 0.027 4.65% 0.456 53.0%10,000 Houses 0.18527.0%0.210 44.5% 0.060 9.70% 0.554 64.9%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Ablation study to evaluate the effect of the number of training houses. Each model is trained to 80% success during training. Test performance increases with the number of training houses.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "[39] Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threedworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai.arXiv, 2021. 40 [40] Timnit Gebru, Jamie H. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna M. Wallach, Hal Daum\u00e9, and Kate Crawford. Datasheets for datasets. Comm. of the ACM, 2021. Dorsa Sadigh, and Percy Liang. Learning adaptive language interfaces through decomposition. arXiv, 2020. 40 [56] Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody Mao, Luisa Caldas, and Allen Y Yang. Scenegen: Generative contextual scene augmentation using scene graph priors. arXiv, 2020. 3 [57] Ahmed Khalifa, Philip Bontrager, Sam Earle, and Julian Togelius. Pcgrl: Procedural content generation via reinforcement learning. In AIIDE, 2020. 3 [58] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: Clip embeddings for embodied ai. In CVPR, 2021. 8, 9, 40, 48, 49, 52 [59] Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to simulate dynamic environments with gamegan. In CVPR, 2020. 40 [60] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014. 49, 52 [61] Jing Yu Koh, Harsh Agrawal, Dhruv Batra, Richard Tucker, Austin Waters, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Simple and effective synthesis of indoor 3d scenes. arXiv, 2022. 40 [62] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In ECCV, 2020. 40 [66] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. In RSS, 2021. 40 [67] Alina Kuznetsova, Hassan Rom, Neil Gordon Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4. IJCV, 2020. 3 [68] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Mark Murnane, Padraig Higgins, Monali Saraf, Francis Ferraro, Cynthia Matuszek, and Don Engel. A simulator for human-robot interaction in virtual reality. In VRW, 2021. 40 [84] Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Moravanszky, Gavriel State, Michelle Lu, Ankur Handa, and Dieter Fox. Factory: Fast contact for robotic assembly. In RSS, 2022. 40 [85] Nelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang, Hang Chu, Chin-Yi Cheng, and Yasutaka Furukawa. Advances in neural information processing systems, 32, 2019. 10 [89] Claudia P\u00e9rez-D'Arpino, Can Liu, Patrick Goebel, Roberto Mart\u00edn-Mart\u00edn, and Silvio Savarese. Robot navigation in constrained pedestrian environments using reinforcement learning. In ICRA, 2021. 40 [90] Aleksei Petrenko, Erik Wijmans, Brennan Shacklett, and Vladlen Koltun. Megaverse: Simulating embodied agents at one million experiences per second. In ICML, 2021. 3 [91] Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In ICRA, 2016. 3 [92] Xavier Puig, Kevin Kyunghwan Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In CVPR, 2018. 3 [93] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 49 [94] Santhosh K. Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Xuan Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv, 2021. 1, 3, 6, 7, 8 [95] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 1 [96] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In CVPR, 2022. 48 [97] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, 2021. 38 [98] Daniel Ritchie, Kai Wang, and Yu-An Lin. Fast and flexible indoor scene synthesis via deep convolutional generative models. In CVPR, 2019. 3 [99] St\u00e9phane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. 52 [100] Manolis Savva, Angel X. Chang, and Maneesh Agrawala. Scenesuggest: Context-driven 3d scene design. arXiv, 2017. 3 [101] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In ICCV, 2019. 1, 3, 10 [102] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In ICLR, 2016. 49 [103] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv, 2017. 49 [104] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart'in-Mart'in, Linxi (Jim) Fan, Guanzhi Wang, S. Buch, Claudia. P\u00e9rez D'Arpino, Sanjana Srivastava, Lyne P. Tchapmi, Micael Edmond Tchapmi, Kent Vainio, Li Fei-Fei, and Silvio Savarese. igibson, a simulation environment for interactive tasks in large realistic scenes. In IROS, 2021. 1, 3, 7 [105] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In CVPR, 2020. 40 [106] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart'in-Mart'in, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, S. Buch, C. Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, and Li Fei-Fei. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In CoRL, 2021. 40 [107] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, 2020. 3 [108] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 49 [109] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Contributions Matt Deitke designed and implemented the procedure to generate houses, implemented ObjectNav pre-training experiments and fine-tuning experiments, built the website, advised and implemented parts of the Unity backend, built the platform to visualize assets and create semantic asset groups, contributed to visuals, and wrote the paper. Kiana Ehsani implemented ArmPointNav experiments and wrote parts of the paper.", "figure_data": "41, 45[41] Sean Gillies et al. Shapely: manipulation and analysis of geometric objects, 2007. 10[42] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and AliFarhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018. 40[43] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet,Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun,Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, CengizOztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela,Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. In CVPR, 2022. 40[44] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and functionusing networkx. Technical report, Los Alamos National Lab, 2008. 10[45] Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, DavidCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Jordi Salvador implemented rearrangement experiments, advised on multi-node training experiments, and wrote parts of the paper.Eli VanderBilt standardized AI2-THOR's asset and material database to make it usable with PROCTHOR, led the development of ARCHITECTHOR, implemented parts of the Unity backend, created new 3D assets and skyboxes, advised on lighting the houses, and contributed to visuals.Winson Han implemented parts of ARCHITECTHOR, implemented parts of the Unity backend, and contributed to visuals.Luca Weihs advised the work on experiments, assisted with rearragement experiments, implemented ObjectNav fine-tuning on HM3D-Semantics, and wrote parts of the paper. B.8.3 Floor Object Placement . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.8.4 Wall Object Placement . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.8.5 Surface Object Placement . . . . . . . . . . . . . . . . . . . . . . . . . . B.9 Material and Color Randomization . . . . . . . . . . . . . . . . . . . . . . . . . . B.10 Object States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.11 Validator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.12 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Datasheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ObjectNav experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 ArmPointNav experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Rearrangement experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Examples of assets in the asset database. The forward-facing direction for each asset is consistent across all assets within its type, which allows us to do things like not spawn fridges facing the wall.", "figure_data": "A ProcTHOR AssetsC PROCTHOR Datasheet D ARCHITECTHOR D.1 E Input Modalities F Experiment details F.1 G Performance Benchmark H Broader Impact FRIDGE_1 FRIDGE_10 FRIDGE_13 FRIDGE_14 SOFA_201_2 SOFA_203_1 SOFA_204_1 SOFA_205_1 FRIDGE_11 FRIDGE_12 FRIDGE_15 FRIDGE_16 SOFA_207_3 SOFA_214_2 SOFA_218_1 SOFA_227_1 CHAIR_002_1 CHAIR_007_1 CHAIR_201_1 CHAIR_203_1 CHAIR_204_1 CHAIR_205_1 CHAIR_210_1 CHAIR_215_1 (a) 0 CellPhone Pencil Footstool Knife TennisRacket BaseballBat WineBottle DishSponge HandTowel HandTowelHolder Desktop Kettle AluminumFoil PaperTowelRoll Cart ScrubBrush CD SoapBar Bottle Spatula Spoon TableTopDecor Dumbbell WateringCan TissueBox Blinds VacuumCleaner TowelHolder Boots TeddyBear Fork GarbageBag Watch WashingMachine ClothesDryer Safe ShowerCurtain RoomDecor ToiletPaper ShowerHead ButterKnife BasketBall Toilet Ladle Plunger PepperShaker KeyChain DogBed SaltShaker Towel RemoteControl LaundryHamper Ottoman CreditCard Pen Newspaper Mug Asset Type Candle SprayBottle Stool TVStand Cloth ShelvingUnit DeskLamp Vase Doorway Statue Window Painting Dresser DiningTable Bread CoffeeMachine LightSwitch Pot Fridge Tomato ToiletPaperHanger Toaster SoapBottle Lettuce Sink Potato Microwave Pan Egg Apple Bowl Plate AlarmClock GarbageCan HousePlant Cup Laptop Pillow FloorLamp Television CoffeeTable CounterTop Box Book Desk Sofa Faucet Bed ArmChair Chair SideTable Number of Unique Instances per Asset Type Unique Instances 20 40 60(b) The number of unique 3D modeled as-sets for each of the 108 asset types. Thereare 1,633 unique assets in total."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "A datasheet[40]  for the artist-designed ARCHITECTHOR houses.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "The action space for ObjectNav experiments.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "The agent is trained for 423 million steps, although by 200 million steps, the agent has reached 90% of its peak performance. We used multi-node training to train on 3 AWS g4dn.12xlarge machines, which takes approximately 5 days to complete.", "figure_data": "Object TypeRoboTHOR HM3D-Semantics AI2-iTHOR ARCHITECTHORAlarm ClockAppleBaseball BatBasketballBedBowlChairGarbage CanHouse PlantLaptopMugSofaSpray BottleTelevisionToiletVase"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "The target objects that are used for each ObjectNav task.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "shows the hyperparameters used for pre-training.", "figure_data": "HyperparameterValueLearning rate3e-4Gradient steps128Discount factor (\u03b3)0.99GAE parameter (\u03bb)0.95Gradient clip norm0.5Rotation Degrees45Step penalty-0.01Number of RNN Layers1Rollouts per minibatch1OptimizerAdam [60]"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Training hyperparameters for ArmPointNav experiments.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "ProcTHOR pre-training hyperparameters for Rearrange experiments.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Running on 6 AWS g4dn.12xlarge (totaling 24 GPUs and 288 virtual CPU cores), pre-training with 240 parallel simulations took 4 days. Table10shows the hyperparameters used during pre-training.AI2-iTHOR fine-tuning. We use the training dataset provided by [4] (4,000 episodes over 80 single-room scenes), and a small subset of 200 episodes from the also provided full validation set to perform model selection. We fine-tune for 3 million steps with 64-step long rollouts, 6 additional million steps with 96-step long rollouts, and another 6 million steps with 128-step long rollouts.Running on 8 Titan X GPUs and 56 virtual CPU cores, fine-tuning with 40 parallel simulations took 16 hours.", "figure_data": "Navigation FPSIsolated Interaction FPSEnvironment Query FPSComputeAI2-iTHORRoboTHORAI2-iTHORRoboTHORAI2-iTHORRoboTHOR8 GPUs5,779\u00b11899,195\u00b12945,411\u00b11906,331\u00b1137463,446\u00b118,577 412,550\u00b121,8061 GPU1,316\u00b1191,648\u00b1111,451\u00b1721,539\u00b15169,092\u00b14,232163,660\u00b13,3361 Process180\u00b19340\u00b126141\u00b12217\u00b1115,584\u00b115615,578\u00b1164PROCTHOR-S PROCTHOR-LPROCTHOR-S PROCTHOR-LPROCTHOR-S PROCTHOR-L8 GPUs8,599\u00b13593,208\u00b11276,488\u00b12502,861\u00b1107480,205\u00b119,684 433,587\u00b118,7291 GPU1,427\u00b1746,280\u00b1401,265\u00b171597\u00b137160,622\u00b12,846157,567\u00b12,6891 Process240\u00b169115\u00b119180\u00b14293\u00b11514,825\u00b119914,916\u00b1186"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Comparing performance benchmarks in PROCTHOR to baselines in AI2-iTHOR and RoboTHOR. FPS for navigation, interaction, and querying the environment for data. PROCTHOR-S and PROCTHOR-L denotes small and large PROCTHOR houses, respectively.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "c x \u223c U (1, max(2, min(x s \u2212 1, \u230aa max /2\u230b) \u2212 1), inclusive,", "formula_coordinates": [25.0, 266.68, 560.42, 238.57, 9.65]}, {"formula_id": "formula_1", "formula_text": "c z \u223c U (1, a max \u2212 c x ).", "formula_coordinates": [25.0, 414.0, 571.33, 91.75, 9.65]}, {"formula_id": "formula_2", "formula_text": "r i \u223c \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 p = 1 /200 4 p = 2 /200 5 p = 4 /200 6 p = 20 /200 7 p = 173 /200 .(1)", "formula_coordinates": [33.0, 258.02, 479.34, 245.98, 64.01]}, {"formula_id": "formula_4", "formula_text": "n p \u223c \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 0 p = 0.05 1 p = 0.1 2 p = 0.5 3 p = 0.25 4 p = 0.1 .(3)", "formula_coordinates": [34.0, 261.35, 484.2, 242.65, 64.01]}, {"formula_id": "formula_5", "formula_text": "b house \u223c (b house-max \u2212 b house-min ) \u2022 Beta(3.5, 1.9) + b house-min ,(4)", "formula_coordinates": [35.0, 197.54, 563.57, 306.46, 9.64]}], "doi": ""}