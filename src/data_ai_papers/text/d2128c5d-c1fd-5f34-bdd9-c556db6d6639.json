{"title": "Lifelong Learning with a Changing Action Set", "authors": "Yash Chandak; Georgios Theocharous; Chris Nota; Philip S Thomas", "pub_date": "2020-05-11", "abstract": "In many real-world sequential decision making problems, the number of available actions (decisions) can vary over time. While problems like catastrophic forgetting, changing transition dynamics, changing rewards functions, etc. have been well-studied in the lifelong learning literature, the setting where the size of the action set changes remains unaddressed. In this paper, we present first steps towards developing an algorithm that autonomously adapts to an action set whose size changes over time. To tackle this open problem, we break it into two problems that can be solved iteratively: inferring the underlying, unknown, structure in the space of actions and optimizing a policy that leverages this structure. We demonstrate the efficiency of this approach on large-scale real-world lifelong learning problems.", "sections": [{"heading": "Introduction", "text": "Real-world problems are often non-stationary. That is, parts of the problem specification change over time. We desire autonomous systems that continually adapt by capturing the regularities in such changes, without the need to learn from scratch after every change. In this work, we address one form of lifelong learning for sequential decision making problems, wherein the set of possible actions (decisions) varies over time. Such a situation is omnipresent in real-world problems. For example, in robotics it is natural to add control components over the lifetime of a robot to enhance its ability to interact with the environment. In hierarchical reinforcement learning, an agent can create new options (Sutton, Precup, and Singh 1999) over its lifetime, which are in essence new actions. In medical decision support systems for drug prescription, new procedures and medications are continually discovered. In product recommender systems, new products are constantly added to the stock, and in tutorial recommendation systems, new tutorials are regularly developed, thereby continuously increasing the number of available actions for a recommender engine. These examples capture the broad idea that, for an agent that is deployed in real world settings, the possible decisions it can make changes over time, and motivates the question that we aim to answer: how do we develop Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nalgorithms that can continually adapt to such changes in the action set over the agent's lifetime?\nReinforcement learning (RL) has emerged as a successful class of methods for solving sequential decision making problems. However, excluding notable exceptions that we discuss later (Boutilier et al. 2018;Mandel et al. 2017), its applications have been limited to settings where the set of actions is fixed. This is likely because RL algorithms are designed to solve a mathematical formalization of decision problems called Markov decision processes (MDPs) (Puterman 2014), wherein the set of available actions is fixed. To begin addressing our lifelong learning problem, we first extend the standard MDP formulation to incorporate this aspect of changing action set size. Motivated by the regularities in real-world problems, we consider an underlying, unknown, structure in the space of actions from which new actions are generated. We then theoretically analyze the difference between what an algorithm can achieve with only the actions that are available at one point in time, and the best that the algorithm could achieve if it had access to the entire underlying space of actions (and knew the structure of this space). Leveraging insights from this theoretical analysis, we then study how the structure of the underlying action space can be recovered from interactions with the environment, and how algorithms can be developed to use this structure to facilitate lifelong learning.\nAs in the standard RL setting, when facing a changing action set, the parameterization of the policy plays an important role. The key consideration here is how to parameterize the policy and adapt its parameters when the set of available actions changes. To address this problem, we leverage the structure in the underlying action space to parameterize the policy such that it is invariant to the cardinality of the action set-changing the number of available actions does not require changes to the number of parameters or the structure of the policy. Leveraging the structure of the underlying action space also improves generalization by allowing the agent to infer the outcomes of actions similar to actions already taken. These advantages make our approach ideal for lifelong learning problems where the action set changes over time, and where quick adaptation to these changes, via generalization of prior knowledge about the impact of actions, is beneficial.", "publication_ref": ["b25", "b2", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Lifelong learning is a well studied problem (Thrun 1998;Ruvolo and Eaton 2013;Silver, Yang, and Li 2013;Chen and Liu 2016). Predominantly, prior methods aim to address catastrophic forgetting problems in order to leverage prior knowledge for new tasks (French 1999;Kirkpatrick et al. 2017;Lopez-Paz and others 2017;Zenke, Poole, and Ganguli 2017). Several meta-reinforcement-learning methods aim at addressing the problem of transfer learning, few-shot shot adaption to new tasks after training on a distribution of similar tasks, and automated hyper-parameter tuning (Xu, van Hasselt, and Silver 2018;Gupta et al. 2018;Wang et al. 2017;Duan et al. 2016;Finn, Abbeel, and Levine 2017). Alternatively, many lifelong RL methods consider learning online in the presence of continuously changing transition dynamics or reward functions (Neu 2013;Gajane, Ortner, and Auer 2018). In our work, we look at a complementary aspect of the lifelong learning problem, wherein the size of the action set available to the agent change over its lifetime.\nOur work also draws inspiration from recent works which leverage action embeddings (Dulac-Arnold et al. 2015;He et al. 2015;Bajpai, Garg, and others 2018;Chandak et al. 2019;Tennenholtz and Mannor 2019). Building upon their ideas, we present a new objective for learning structure in the action space, and show that the performance of the policy resulting from using this inferred structure has bounded sub-optimality. Moreover, in contrast to their setup where the size of the action set is fixed, we consider the case of lifelong MDP, where the number of actions changes over time. Mandel et al. (2017) and Boutilier et al. (2018) present the work most similar to ours. Mandel et al. (2017) consider the setting where humans can provide new actions to an RL system. The goal in their setup is to minimize human effort by querying for new actions only at states where new actions are most likely to boost performance. In comparison, our setup considers the case where the new actions become available through some external, unknown, process and the goal is to build learning algorithms that can efficiently adapt to such changes in the action set. Boutilier et al. (2018) laid the foundation for the stochastic action set MDP (SAS-MDP) setting where there is a fixed, finite, number of (base) actions and the available set of actions is a stochastically chosen subset of this base set. While SAS-MDPs can also be considered to have a 'changing action set', unlike their work there is no fixed maximum number for the available actions in our framework. Further, in their setup, there is a possibility that within a single long episode an agent can observe all possible actions it will ever encounter. In our set-up, this is never possible. As shown by Boutilier et al. (2018), SAS-MDPs can also be reduced to standard MDPs by extending the state space to include the set of available action. This cannot be done in our lifelong-MDP setup, as that would imply that the state-space is changing across episodes or the MDP is nonstationary. The works by Gabel and Riedmiller (2008) and Ferreira et al. (2017) also consider subsets of the base actions for DEC-MDPs and answer-set programming, respectively, but all the mentioned differences from the work by Boutilier et al. (2018) are also applicable here.\nFigure 1: Illustration of a lifelong MDP where M 0 is the base MDP. For every change k, M K builds upon M k\u22121 by including the newly available set of actions A k . The internal structure in the space of actions is hidden and only a set of discrete actions is observed.\nThese differences let the proposed work better capture the challenges of lifelong learning, where the cardinality of the action set itself varies over time and an agent has to deal with actions that it has never dealt with before.", "publication_ref": ["b26", "b19", "b22", "b3", "b6", "b13", "b27", "b26", "b9", "b26", "b5", "b6", "b8", "b5", "b10", "b1", "b2", "b25", "b16", "b2", "b16", "b2", "b2", "b7", "b5", "b2"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Lifelong Markov Decision Process", "text": "MDPs, the standard formalization of decision making problems, are not flexible enough to encompass lifelong learning problems wherein the action set size changes over time. In this section we extend the standard MDP framework to model this setting.\nIn real-world problems where the set of possible actions changes, there is often underlying structure in the set of all possible actions (those that are available, and those that may become available). For example, tutorial videos can be described by feature vectors that encode their topic, difficulty, length, and other attributes; in robot control tasks, primitive locomotion actions like left, right, up, and down could be encoded by their change to the Cartesian coordinates of the robot, etc. Critically, we will not assume that the agent knows this structure, merely that it exists. If actions are viewed from this perspective, then the set of all possible actions (those that are available at one point in time, and those that might become available at any time in the future) can be viewed as a vector-space, E \u2286 R d .\nTo formalize the lifelong MDP, we first introduce the necessary variables that govern when and how new actions are added. We denote the episode number using \u03c4 . Let I \u03c4 \u2208 {0, 1} be a random variable that indicates whether a new set of actions are added or not at the start of episode \u03c4 , and let frequency F : N \u2192 [0, 1] be the associated probability distribution over episode count, such that Pr(I \u03c4 = 1) = F(\u03c4 ). Let U \u03c4 \u2208 2 E be the random variable corresponding to the set of actions that is added before the start of episode \u03c4 . When I \u03c4 = 1, we assume that U \u03c4 = \u2205, and when I \u03c4 = 0, we assume that U \u03c4 = \u2205. Let D \u03c4 be the distribution of U \u03c4 when I \u03c4 = 1, i.e., U \u03c4 \u223c D \u03c4 if I \u03c4 = 1. We use D to denote the set {D \u03c4 } consisting of these distributions. Such a formulation using I \u03c4 and D \u03c4 provides a fine control of when and how new actions can be incorporated. This allows modeling a large class of problems where both the distribution over the type of incorporated actions as well intervals between successive changes might be irregular. Often we will not require the exact episode number \u03c4 but instead require k, which denotes the number of times the action set is changed.\nSince we do not assume that the agent knows the structure associated with the action, we instead provide actions to the agent as a set of discrete entities, A k . To this end, we define \u03c6 to be a map relating the underlying structure of the new actions to the observed set of discrete actions A k for all k, i.e., if the set of actions added is u k , then A k = {\u03c6(e i )|e i \u2208 u k }. Naturally, for most problems of interest, neither the underlying structure E, nor the set of distributions D, nor the frequency of updates F, nor the relation \u03c6 is known-the agent only has access to the observed set of discrete actions.\nWe now define the lifelong Markov decision process (L-MDP) as L = (M 0 , E, D, F), which extends a base MDP M 0 = (S, A, P, R, \u03b3, d 0 ). S is the set of all possible states that the agent can be in, called the state set. A is the discrete set of actions available to the agent, and for M 0 we define this set to be empty, i.e., A = \u2205. When the set of available actions changes and the agent observes a new set of discrete actions, A k , then M k\u22121 transitions to M k , such that A in M k is the set union of A in M k\u22121 and A k . Apart from the available actions, other aspects of the L-MDP remain the same throughout. An illustration of the framework is provided in Figure 1. We use S t \u2208 S, A t \u2208 A, and R t \u2208 R as random variables for denoting the state, action and reward at time t \u2208 {0, 1, . . . } within each episode. The first state, S 0 , comes from an initial distribution, d 0 , and the reward function R is defined to be only dependent on the state such that R(s) = E[R t |S t = s] for all s \u2208 S. We assume that R t \u2208 [\u2212R max , R max ] for some finite R max . The reward discounting parameter is given by \u03b3 \u2208 [0, 1). P is the state transition function, such that for all s, a, s , t, the function P(s, a, s ) denotes the transition probability P (s |s, e), where a = \u03c6(e). 1 In the most general case, new actions could be completely arbitrary and have no relation to the ones seen before. In such cases, there is very little hope of lifelong learning by leveraging past experience. To make the problem more feasible, we resort to a notion of smoothness between actions. Formally, we assume that transition probabilities in an L-MDP are \u03c1\u2212Lipschitz in the structure of actions, i.e., \u2203\u03c1 > 0 s.t., \u2200s, s , e i , e j P (s |s, e i ) \u2212 P (s |s, e j ) 1 \u2264 \u03c1 e i \u2212 e j 1 .\n(1) For any given MDP M k in L , an agent's goal is to find a policy, \u03c0 k , that maximizes the expected sum of discounted future rewards. For any policy \u03c0 k , the corresponding state value function is\nv \u03c0 k (s) = E[ \u221e t=0 \u03b3 t R t |s, \u03c0 k ].", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Blessing of Changing Action Sets", "text": "Finding an optimal policy when the set of possible actions is large is difficult due to the curse of dimensionality. In the L-MDP setting this problem might appear to be exacerbated, as an agent must additionally adapt to the changing levels of possible performance as new actions become available. This raises the natural question: as new actions become available, how much does the performance of an optimal policy change? If it fluctuates significantly, can a lifelong learning agent succeed by continuously adapting its policy, or is it better to learn from scratch with every change to the action set?\nTo answer this question, consider an optimal policy, \u03c0 * k , for MDP M k , i.e., an optimal policy when considering only policies that use actions that are available during the k th episode. We now quantify how sub-optimal \u03c0 * k is relative to the performance of a hypothetical policy, \u00b5 * , that acts optimally given access to all possible actions. Theorem 1. In an L-MDP, let k denote the maximum distance in the underlying structure of the closest pair of available actions, i.e., k := sup\nai\u2208A inf aj \u2208A e i \u2212 e j 1 , then v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max . Proof. See Appendix B.\nWith a bound on the maximum possible sub-optimality, Theorem 1 presents an important connection between achievable performances, the nature of underlying structure in the action space, and a property of available actions in any given M k . Using this, we can make the following conclusion. Corollary 1. Let Y \u2286 E be the smallest closed set such that,\nP (U k \u2286 2 Y ) = 1. We refer to Y as the element-wise-support of U k . If \u2200k, the element-wise-support of U k in an L-MDP is E, then as k \u2192 \u221e the sub-optimality vanishes. That is, lim k\u2192\u221e v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2192 0.\nProof. See Appendix B.\nThrough Corollary 1, we can now establish that the change in optimal performance will eventually converge to zero as new actions are repeatedly added. An intuitive way to observe this result would be to notice that every new action that becomes available indirectly provides more information about the underlying, unknown, structure of E. However, in the limit, as the size of the available action set increases, the information provided by each each new action vanishes and thus performance saturates.\nCertainly, in practice, we can never have k \u2192 \u221e, but this result is still advantageous. Even when the underlying structure E, the set of distributions D, the change frequency F, and the mapping relation \u03c6 are all unknown, it establishes the fact that the difference between the best performances in successive changes will remain bounded and will not fluctuate arbitrarily. This opens up new possibilities for developing algorithms that do not need to start from scratch after new actions are added, but rather can build upon their past experiences using updates to their existing policies that efficiently leverage estimates of the structure of E to adapt to new actions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning with Changing Action Sets", "text": "Theorem 1 characterizes what can be achieved in principle, however, it does not specify how to achieve it-how to find \u03c0 * k efficiently. Using any parameterized policy, \u03c0, which acts directly in the space of observed actions, suffers from one key practical drawback in the L-MDP setting. That is, the parameterization is deeply coupled with the number of actions that are available. That is, not only is the meaning of each parameter coupled with the number of actions, but often the number of parameters that the policy has is dependent on the number of possible actions. This makes it unclear how the policy should be adapted when additional actions become available. A trivial solution would be to ignore the newly available actions and continue only using the previously available actions. However, this is clearly myopic, and will prevent the agent from achieving the better long term returns that might be possible using the new actions.\nTo address this parameterization-problem, instead of having the policy, \u03c0, act directly in the observed action space, A, we propose an approach wherein the agent reasons about the underlying structure of the problem in a way that makes its policy parameterization invariant to the number of actions that are available. To do so, we split the policy parameterization into two components. The first component corresponds to the state conditional policy responsible for making the decisions, \u03b2 : S \u00d7\u00ca \u2192 [0, 1], where\u00ca \u2208 R d . The second component corresponds to\u03c6 :\u00ca \u00d7 A \u2192 [0, 1], an estimator of the relation \u03c6, which is used to map the output of \u03b2 to an action in the set of available actions. That is, an E t \u2208\u00ca is sampled from \u03b2(S t , \u2022) and then\u03c6(E t ) is used to obtain the action A t . Together, \u03b2 and\u03c6 form a complete policy, and\u00ca corresponds to the inferred structure in action space.\nOne of the prime benefits of estimating \u03c6 with\u03c6 is that it makes the parameterization of \u03b2 invariant to the cardinality of the action set-changing the number of available actions does not require changing the number of parameters of \u03b2. Instead, only the parameterization of\u03c6, the estimator of the underlying structure in action space, must be modified when new actions become available. We show next that the update to the parameters of\u03c6 can be performed using supervised learning methods that are independent of the reward signal and thus typically more efficient than RL methods.\nWhile our proposed parameterization of the policy using both \u03b2 and\u03c6 has the advantages described above, the performance of \u03b2 is now constrained by the quality of\u03c6, as in the end\u03c6 is responsible for selecting an action from A. Ideally we want\u03c6 to be such that it lets \u03b2 be both: (a) invariant to the cardinality of the action set for practical reasons and (b) as expressive as a policy, \u03c0, explicitly parameterized for the currently available actions. Similar trade-offs have been considered in the context of learning optimal state-embeddings for representing sub-goals in hierarchical RL (Nachum et al. 2018). For our lifelong learning setting, we build upon their method to efficiently estimate\u03c6 in a way that provides bounded sub-optimality. Specifically, we make use of an additional inverse dynamics function, \u03d5, that takes as input two states, s and s , and produces as output a prediction of which e \u2208 E caused the transition from s to s . Since the agent does not know \u03c6, when it observes a transition from s to s via action a, it does not know which e caused this transition. So, we cannot train \u03d5 to make good predictions using the actual action, e, that caused the transition. Instead, we use\u03c6 to transform the prediction of \u03d5 from e \u2208 E to a \u2208 A, and train both \u03d5 and\u03c6 so that this process accurately predicts which action, a, caused the transition from s to s . Moreover, rather than viewing \u03d5 as a deterministic function mapping states s and s to predictions e, we define \u03d5 to be a distribution over E given two states, s and s .\nFor any given M k in L-MDP L , let \u03b2 k and\u03c6 k denote the two components of the overall policy and let \u03c0 * * k denote the best overall policy that can be represented using some fixed \u03c6 k . The following theorem bounds the sub-optimality of \u03c0 * * k . Theorem 2. For an L-MDP M k , If there exists a \u03d5 :\nS \u00d7 S \u00d7\u00ca \u2192 [0, 1] and\u03c6 k :\u00ca \u00d7 A \u2192 [0, 1] such that sup s\u2208S,a\u2208A KL P (S t+1 |S t = s, A t = a) P (S t+1 |S t = s, A t =\u00c2) \u2264 \u03b4 2 k /2, (2\n)\nwhere\u00c2 \u223c\u03c6 k (\u2022|\u00ca) and\u00ca \u223c \u03d5(\u2022|S t , S t+1 ), then v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3 (\u03c1 k + \u03b4 k ) (1 \u2212 \u03b3) 2 R max .\nProof. See Appendix B.\nBy quantifying the impact\u03c6 has on the sub-optimality of achievable performance, Theorem 2 provides the necessary constraints for estimating\u03c6. At a high level, Equation (2) ensures\u03c6 to be such that it can be used to generate an action corresponding to any s to s transition. This allows \u03b2 to leverage\u03c6 and choose the required action that induces the state transition needed for maximizing performance. Thereby, following (2), sub-optimality would be minimized if\u03c6 and \u03d5 are optimized to reduce the supremum of KL divergence over all s and a. In practice, however, the agent does not have access to all possible states, rather it has access to a limited set of samples collected from interactions with the environment. Therefore, instead of the supremum, we propose minimizing the average over all s and a from a set of observed transitions, L(\u03c6, \u03d5):=  3) suggests that L(\u03c6, \u03d5) would be minimized when a equals a, but using (3) directly in the current form is inefficient as it requires computing KL over all probable s \u2208 S for a given s and a. To make it practical, we make use of the following property.  As minimizing L(\u03c6, \u03d5) is equivalent to maximizing \u2212L(\u03c6, \u03d5), we consider maximizing the lower bound obtained from Property 1. In this form, it is now practical to optimize (4) just by using the observed (s, a, s ) samples. As this form is similar to the objective for variational auto-encoder, inner expectation can be efficiently optimized using the reparameterization trick (Kingma and Welling 2013). P (\u00ea|s, s ) is the prior on\u00ea, and we treat it as a hyper-parameter that allows the KL to be computed in closed form.\nImportantly, note that this optimization procedure only requires individual transitions, s, a, s , and is independent of the reward signal. Hence, at its core, it is a supervised learning procedure. This means that learning good parameters for\u03c6 tends to require far fewer samples than optimizing \u03b2 (which is an RL problem). This is beneficial for our approach becaus\u00ea \u03c6, the component of the policy where new parameters need to be added when new actions become available, can be updated efficiently. As both \u03b2 and \u03d5 are invariant to action cardinality, they do not require new parameters when new actions become available. Additional implementation level details are available in Appendix F.", "publication_ref": ["b17", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm", "text": "When a new set of actions, A k+1 , becomes available, the agent should leverage the existing knowledge and quickly adapt to the new action set. Therefore, during every change in M k , the ongoing best components of the policy,\n\u03b2 * k\u22121 and \u03c6 * k\u22121 , in M k\u22121 are carried over, i.e., \u03b2 k := \u03b2 * k\u22121 and \u03c6 k :=\u03c6 * k\u22121 .\nFor lifelong learning, the following property illustrates a way to organize the learning procedure so as to minimize the sub-optimality in each M k , for all k.\nProperty 2. (Lifelong Adaptation and Improvement) In an L-MDP, let \u2206 denote the difference of performance between v \u00b5 * and the best achievable using our policy parameterization, then the overall sub-optimality can be expressed as,\nv \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = \u221e k=1 v \u03b2 k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6k M k (s 0 ) Adaptation + \u221e k=1 v \u03b2 * k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6 * k M k (s 0 ) Policy Improvement +\u2206,\nwhere M k is used in the subscript to emphasize the respective MDP in L . Proof: See Appendix D.\nProperty 2 illustrates a way to understand the impact of \u03b2 and\u03c6 by splitting the learning process into an adaptation phase and a policy improvement phase. These two iterative phases are the crux of our algorithm for solving an L-MDP L . Based on this principle, we call our algorithm LAICA: lifelong adaptation and improvement for changing actions. Due to space constraints, we now briefly discuss the LAICA algorithm; a detailed description with pseudocode is presented in Appendix E.\nWhenever new actions become available, adaptation is prone to cause a performance drop as the agent has no information about when to use the new actions, and so its initial uses of the new actions may be at inappropriate times. Following Property 1, we update\u03c6 so as to efficiently infer the underlying structure and minimize this drop. That is, for every M k ,\u03c6 k is first adapted to\u03c6 * k in the adaptation phase by adding more parameters for the new set of actions and then optimizing (4). After that,\u03c6 * k is fixed and \u03b2 k is improved towards \u03b2 * k in the policy improvement phase, by updating the parameters of \u03b2 k using the policy gradient theorem (Sutton et al. 2000). These two procedures are performed sequentially whenever M k\u22121 transitions to M k , for all k, in an L-MDP L . An illustration of the procedure is presented in Figure 2.\nA step-by-step pseudo-code for the LAICA algorithm is available in Algorithm 1, Appendix E. The crux of the algorithm is based on the iterative adapt and improve procedure obtained from Property 2.", "publication_ref": ["b24"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Empirical Analysis", "text": "In this section, we aim to empirically compare the following methods,\n\u2022 Baseline(1): The policy is re-initialised and the agent learns from scratch after every change. \u2022 Baseline(2): New parameters corresponding to new actions are added/stacked to the existing policy (and previously learned parameters are carried forward as-is). \u2022 LAICA(1): The proposed approach that leverages the structure in the action space. To act in continuous space of inferred structure, we use DPG (Silver et al. 2014) to optimize \u03b2. \u2022 LAICA(2): A variant of LAICA which uses an actor-critic (Sutton and Barto 2018) to optimize \u03b2.\nTo demonstrate the effectiveness of our proposed method(s) on lifelong learning problems, we consider a maze environment and two domains corresponding to real-world applications, all with a large set of changing actions. For each of these domains, the total number of actions were randomly split into five equal sets. Initially, the agent only had the actions available in the first set and after every change the next set of actions was made available additionally. In the following paragraphs we briefly outline the domains; full details are deferred to Appendix F.\nMaze Domain. As a proof-of-concept, we constructed a continuous-state maze environment where the state is comprised of the coordinates of the agent's location and its objective is to reach a fixed goal state. The agent has a total of 256 actions corresponding to displacements in different directions of different magnitudes. This domain provides a simple yet challenging testbed that requires solving a long horizon task using a large, changing action set, in presence of a single goal reward.\nCase Study: Real-World Recommender Systems. We consider the following two real-world applications of largescale recommender systems that require decision making over multiple time steps and where the number of possible decisions varies over the lifetime of the system.\n\u2022 A web-based video-tutorial platform, that has a recommendation engine to suggest a series of tutorial videos. The aim is to meaningfully engage the users in a learning activity. In total, 1498 tutorials were considered for recommendation.\n\u2022 A professional multi-media editing software, where sequences of tools inside the software need to be recommended. The aim is to increase user productivity and assist users in quickly achieving their end goal. In total, 1843 tools were considered for recommendation.\nFor both of these applications, an existing log of user's click stream data was used to create an n-gram based MDP model for user behavior (Shani, Heckerman, and Brafman 2005). Sequences of user interaction were aggregated to obtain over 29 million clicks and 1.75 billion user clicks for the tutorial recommendation and the tool recommendation task, respectively. The MDP had continuous state-space, where each state consisted of the feature descriptors associated with each item (tutorial or tool) in the current n-gram.", "publication_ref": ["b21", "b23", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Results.", "text": "The plots in Figures 3 and 4 present the evaluations on the domains considered. The advantage of LAICA over Baseline(1) can be attributed to its policy parameterization. The decision making component of the policy, \u03b2, being invariant to the action cardinality can be readily leveraged after every change without having to be re-initialized. This demonstrates that efficiently re-using past knowledge can improve data efficiency over the approach that learns from scratch every time.\nCompared to Baseline(2), which also does not start from scratch and reuses existing policy, we notice that the variants of LAICA algorithm still perform favorably. As evident from the plots in Figures 3 and 4, while Baseline(2) does a good job of preserving the existing policy, it fails to efficiently capture the benefit of new actions. While the policy parameters in both LAICA and Baseline(2) are improved using policy gradients, the superior performance of LAICA can be attributed to the adaptation procedure incorporated in LAICA which aims at efficiently inferring the underlying structure in the space of actions. Overall LAICA(2) performs almost twice as well as both the baselines on all of the tasks considered. In the maze domain, even the best setting for Baseline(2) performed inconsistently. Due to the sparse reward nature of the task, which only had a big positive reward on reaching goal, even the best setting for Baseline(2) failed on certain Note that even before the first addition of the new set of actions, the proposed method performs better than the baselines. This can be attributed to the fact that the proposed method efficiently leverages the underlying structure in the action set and thus learns faster. Similar observations have been made previously (Dulac-Arnold et al. 2015;He et al. 2015;Bajpai, Garg, and others 2018).\nIn terms of computational cost, the proposed method updates the inverse dynamics model and the underlying action structure only when there is a change in the action set (Algorithm 1). Therefore, compared to the baselines, no extra computational cost is required for training at each time-step. However, the added computational cost does impact the overall learning process and is proportional to the number of times new actions are introduced.", "publication_ref": ["b5", "b10", "b1"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Discussion and Conclusion", "text": "In this work we established first steps towards developing the lifelong MDP setup for dealing with action sets that change over time. Our proposed approach then leveraged the structure in the action space such that an existing policy can be efficiently adapted to the new set of available actions. Superior performances on both synthetic and large-scale realworld environments demonstrate the benefits of the proposed LAICA algorithm.\nTo the best of our knowledge, this is the first work to take a step towards addressing the problem of lifelong learning with a changing action set. We hope that this brings more attention to such understudied problems in lifelong learning. There are several important challenges open for future investigation.\nIn many real-world applications, often due to external factors, some actions are removed over time as well. For example, if a medicine becomes outdated, if a product is banned, etc. While our applications were devoid of this aspect, the proposed algorithm makes use of a policy parameterization that is invariant to the cardinality of the action set, and thus can support both addition and removal. Our proposed policy decomposition method can still be useful for selecting an available action whose impact on the state transition is most similar to the removed action.\nThere can be several applications where new actions that are added over time have no relation to the previously observed actions. For example, completely new product categories, tutorial videos on new topics, etc. In such cases, it is unclear how to leverage past information efficiently. We do not expect our proposed method to work well in such settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lifelong Learning with a Changing Action Set (Supplementary Material) A: Preliminary", "text": "For the purpose of our results, we would require bounding the shift in the state distribution between two policies. Techniques for doing so has been previously studied in literature (Kakade and Langford 2002;Kearns and Singh 2002;Pirotta et al. 2013;Achiam et al. 2017). Specifically, we cover this preliminary result based on the work by Achiam et al. (2017). The discounted state distribution, for all s \u2208 S, for a policy \u03c0 is given by,\nd \u03c0 (s) = (1 \u2212 \u03b3) t \u03b3 t P (S t = s|\u03c0). (5\n)\nLet the shift in state distribution between any given policies \u03c0 1 and \u03c0 2 be denoted as D(\u03c0 1 , \u03c0 2 ), such that\nD(\u03c0 1 , \u03c0 2 ) = S |d \u03c01 (s) \u2212 d \u03c02 (s)| ds = S (1 \u2212 \u03b3) t \u03b3 t P (S t = s|\u03c0 1 ) \u2212 (1 \u2212 \u03b3) t \u03b3 t P (S t = s|\u03c0 2 ) ds. (6\n)\nFor any policy \u03c0, let P \u03c0 denote the matrix corresponding to transition probabilities as a result of \u03c0. Then ( 6) can be re-written as,\nD(\u03c0 1 , \u03c0 2 ) = (1 \u2212 \u03b3)(1 \u2212 \u03b3P \u03c01 ) \u22121 d 0 \u2212 (1 \u2212 \u03b3)(1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 = (1 \u2212 \u03b3) (1 \u2212 \u03b3P \u03c01 ) \u22121 \u2212 (1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 . (7\n)\nTo simplify (7\n), let G 1 = (1 \u2212 \u03b3P \u03c01 ) \u22121 and G 2 = (1 \u2212 \u03b3P \u03c02 ) \u22121 . Then, G 1 \u2212 G 2 = G 1 (G \u22121 2 \u2212 G \u22121 1 )G 2\n, and therefore ( 7) can be written as,\nD(\u03c0 1 , \u03c0 2 ) = (1 \u2212 \u03b3) (1 \u2212 \u03b3P \u03c01 ) \u22121 ((1 \u2212 \u03b3P \u03c02 ) \u2212 (1 \u2212 \u03b3P \u03c01 ))(1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 = (1 \u2212 \u03b3) (1 \u2212 \u03b3P \u03c01 ) \u22121 (\u03b3P \u03c01 \u2212 \u03b3P \u03c02 )(1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 = (1 \u2212 \u03b3P \u03c01 ) \u22121 (\u03b3P \u03c01 \u2212 \u03b3P \u03c02 ) d \u03c02 1 . (8\n)\nNote that using matrix L1 norm,\n(1 \u2212 \u03b3P \u03c01 ) \u22121 1 = t (\u03b3P \u03c01 ) t 1 \u2264 t \u03b3 t P \u03c0 t 1 1 = t \u03b3 t \u2022 1 = (1 \u2212 \u03b3) \u22121 . (9\n)\nCombining ( 9) and ( 8),\nD(\u03c0 1 , \u03c0 2 ) \u2264 \u03b3(1 \u2212 \u03b3) \u22121 (P \u03c01 \u2212 P \u03c02 )d \u03c02 1 .\nB: Sub-Optimality Theorem 1. In an L-MDP, let k denote the maximum distance in the underlying structure of the closest pair of available actions, i.e., k := sup\nai\u2208A inf aj \u2208A e i \u2212 e j 1 , then v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max .\nProof. We begin by defining \u00b5 * k to be a policy where the actions of the policy \u00b5 * is restricted to the actions available in M k . That is, any action e i from \u00b5 * is mapped to the closest e j , where a = \u03c6(e j ) is in the available action set. Notice that the best policy, \u03c0 * k , using the available set of actions is always better than or equal to \u00b5 * k , i.e., v\n\u00b5 * k \u2264 v \u03c0 * k . Therefore , v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 v \u00b5 * (s 0 ) \u2212 v \u00b5 * k (s 0 ) . (10\n)\nOn expanding the v(s 0 ) corresponding for both the policies in (10) using (5),\nv \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 )| \u2264 (1 \u2212 \u03b3) \u22121 S d \u00b5 * (s)R(s)ds \u2212 (1 \u2212 \u03b3) \u22121 S d \u00b5 * k (s)R(s)ds = (1 \u2212 \u03b3) \u22121 S d \u00b5 * (s) \u2212 d \u00b5 * k (s) R(s)ds .(11)\nWe can then upper bound (11) by taking the maximum possible reward common,\nv \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u00b5 * (s) \u2212 d \u00b5 * k (s) ds \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u00b5 * (s) \u2212 d \u00b5 * k (s) ds = (1 \u2212 \u03b3) \u22121 R max D(\u00b5 * , \u00b5 * k ) \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max (P \u00b5 k \u2212 P \u00b5 * k )d \u00b5 * k 1(12)\nFor any action\u0113 taken by the policy \u00b5 * , let\u0113 k denote the action for \u00b5 * k obtained by mapping\u0113 to the closest action in the available set, then expanding (12), we get,\nv \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s P \u00b5 k (s) \u2212 P \u00b5 * k (s) 1 \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s,s \u0113 (P (s |s,\u0113) \u2212 P (s |s,\u0113 k ))\u00b5 * (\u0113|s)d\u0113 \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s,s ,\u0113 P (s |s,\u0113) \u2212 P (s |s,\u0113 k ) .(13)\nFrom the Lipschitz condition (1), we know that |P (s |s,\u0113) \u2212 P (s |s,\u0113 k )| \u2264 \u03c1 \u0113 \u2212\u0113 k 1 . As\u0113 k corresponds to the closest available action for\u0113, the maximum distance for |\u0113 \u2212\u0113 k 1 is bounded by k . Combining ( 13) with these two observations, we get the desired result,\nv \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max . Corollary 1. Let Y \u2286 E be the smallest closed set such that, P (U k \u2286 2 Y ) = 1. We refer to Y as the element-wise-support of U k . If \u2200k, the element-wise-support of U k in an L-MDP is E, then as k \u2192 \u221e the sub-optimality vanishes. That is, lim k\u2192\u221e v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2192 0.\nProof. Let X 1 , ..., X n be independent identically distributed random vectors in E. Let X i define a partition of E in n sets V 1 , ...V n , such that V i contains all points in E whose nearest neighbor among X 1 , ..., X n is X i . Each such V i forms a Voronoi cell. Now using the condition on full element-wise support, we know from the distribution free result by Devroye et al. (2017) that the diameter(V i ) converges to 0 at the rate n \u22121/d as n \u2192 \u221e (Theorem 4, Devroye et al. (2017)). As k corresponds to the maximum distance between closest pair of points in E, k \u2264 sup i diameter(V i ). Therefore, when k \u2192 \u221e then n \u2192 \u221e;\nconsequently k \u2192 0 and thus v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2192 0. Theorem 2. For an L-MDP M k , If there exists a \u03d5 : S \u00d7 S \u00d7\u00ca \u2192 [0, 1] and\u03c6 k :\u00ca \u00d7 A \u2192 [0, 1] such that sup s\u2208S,a\u2208A KL P (S t+1 |S t = s, A t = a) P (S t+1 |S t = s, A t =\u00c2) \u2264 \u03b4 2 k /2, (14\n)\nwhere\u00c2 \u223c\u03c6 k (\u2022|\u00ca) and\u00ca \u223c \u03d5(\u2022|S t , S t+1 ), then v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3 (\u03c1 k + \u03b4 k ) (1 \u2212 \u03b3) 2 R max .\nProof. We begin by noting that,\nv \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) + v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ). Using Theorem (1), v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max + v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) .(15)\nNow we focus on bounding the last two terms in (15). Following steps similar to (11) and ( 12) it can bounded as,\nv \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u03c0 * k (s) \u2212 d \u03c0 * * k (s) ds \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u03c0 * k (s) \u2212 d \u03c0 * * k (s) ds = (1 \u2212 \u03b3) \u22121 R max D(\u03c0 * k , \u03c0 * * k ) = \u03b3(1 \u2212 \u03b3) \u22122 R max (P \u03c0 * k \u2212 P \u03c0 * * k )d \u03c0 * * k 1 \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max E 2T V P \u03c0 * k (s |s) P \u03c0 * * k (s |s) s \u223c d \u03c0 * * k ,\nwhere T V stands for total variation distance. Using Pinsker's inequality,\nv \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s 2KL P \u03c0 * k (s |s) P \u03c0 * * k (s |s) , \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s,a 2KL (P (s |s, a) P (s |s,\u00e2)) ,\nwhere, a \u223c \u03c0 * k and\u00e2 \u223c \u03c0 * * k . As condition ( 14) ensures that maximum KL divergence error between an actual a and an action that can be induced through\u03c6 k for transitioning from s to s is bounded by \u03b4 2 k /2, we get the desired result,\nv \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3\u03b4 k (1 \u2212 \u03b3) 2 R max .(16)\nTherefore taking the union bound on ( 15) and ( 16), we get the desired result \nv \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3 (\u03c1 k + \u03b4 k ) (1 \u2212 \u03b3) 2 R max . C: Lower Bound Objective For Adaptation L(\u03c6, \u03d5) =\nwhere C 2 is another constant consisting of log P (s, s ) and is independent of\u00e2. Now, let us focus on P (\u00e2|s, s ), which represent the probability of the action\u00e2 given the transition s, s . Notice that\u00e2 is selected by\u03c6 only using\u00ea. Therefore, given\u00ea, probability of\u00e2 is independent of everything else, log P (\u00e2|s, s ) = log P (\u00e2|\u00ea, s, s )P (\u00ea|s, s )d\u00ea = log P (\u00e2|\u00ea)P (\u00ea|s, s )d\u00ea.\n(18)\nLet Q(\u00ea|s, s ) be a parameterized distribution that encodes the context (s, s ) into the structure\u00ea, then, we can write (18) as, log P (\u00e2|s, s ) = log Q(\u00ea|s, s ) Q(\u00ea|s, s ) P (\u00e2|\u00ea)P (\u00ea|s, s )d\u00ea \nNotice that P (\u00e2|e) and Q(e|s, s ) correspond to\u03c6 and \u03d5, respectively. P (\u00ea|s, s ) corresponds to the prior on\u00ea. Therefore, combining ( 17) and ( 19) we get,\n\u2212L(\u03c6, \u03d5) \u2265 s\u2208S a\u2208A k s \u2208S P (s, a, s ) E log\u03c6(\u00e2|\u00ea) \u03d5(\u00ea|s, s ) \u2212 KL \u03d5(\u00ea|s, s ) P (\u00ea|s, s ) + C,\nwhere C denotes all the constants.", "publication_ref": ["b11", "b12", "b17", "b0", "b0", "b4", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "D: Lifelong Adaptation and Improvement", "text": "Property 2. (Lifelong Adaptation and Improvement) In an L-MDP, let \u2206 denote the difference of performance between v \u00b5 * and the best achievable using our policy parameterization, then the overall sub-optimality can be expressed as,\nv \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = \u221e k=1 v \u03b2 k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6k M k (s 0 ) Adaptation + \u221e k=1 v \u03b2 * k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6 * k M k (s 0 ) Policy Improvement +\u2206,\nwhere M k is used in the subscript to emphasize the respective L-MDP.\nProof.\nv \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u00b1 v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u00b1 v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) + v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) . As \u03b2 2 := \u03b2 * 1 and\u03c6 2 :=\u03c6 * 1 in M 2 , v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b22\u03c62 M 2 (s 0 ) + v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 )\n. Notice that we have expressed the sub-optimality in M 1 as sub-optimality in M 2 , plus adaptation and a policy improvement terms in M 1 . Expanding it one more time,\nv \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b23\u03c63 M 3 (s 0 ) + 2 k=1 v \u03b2 * k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6 * k M k (s 0 ) + 2 k=1 v \u03b2 k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6k M k (s 0 ) .\nIt is now straightforward to observe the result by successively 'unravelling' the sub-optimality in M 3 in a similar fashion. The final difference between v \u00b5 * and the best policy using our proposed parameterization is \u2206.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E: Algorithm Details", "text": "A step-by-step pseudo-code for the LAICA algorithm is available in Algorithm 1. The crux of the algorithm is based on the iterative adapt and improve procedure obtained from Property 2. We begin by initializing the parameters for \u03b2 * 0 ,\u03c6 * 0 and \u03d5 * 0 . In Lines 3 to 5, for every change in the set of available actions, instead of re-initializing from scratch, the previous best estimates for \u03b2,\u03c6 and \u03d5 are carried forward to build upon existing knowledge. As \u03b2 and \u03d5 are invariant to the cardinality of the available set of actions, no new parameters are required for them. In Line 6 we add new parameters in the function\u03c6 to deal with the new set of available actions.\nTo minimize the adaptation drop, we make use of Property 1. Let L lb denote the lower bound for L, such that,\nL lb (\u03c6, \u03d5) := E log\u03c6(\u00c2 t |\u00ca t ) \u03d5(\u00ca t |S t , S t+1 ) \u2212 \u03bbKL \u03d5(\u00ca t |S t , S t+1 ) P (\u00ca t |S t , S t+1 ) .\nNote that following the literature on variational auto-encoders, we have generalized (4) to use a Lagrangian \u03bb to weight the importance of KL divergence penalty (Higgins et al. 2017). 2 When \u03bb = 1, it degenrates to (4). We set the prior P (\u00ea|s, s ) to be an isotropic normal distribution, which also allows KL to be computed in closed form (Kingma and Welling 2013). From Line 7 to 11 in the Algorithm 1, random actions from the available set of actions are executed and their corresponding transitions are collected in a buffer. Samples from this buffer are then used to maximize the lower bound objective L lb and adapt the parameters of\u03c6 and \u03d5. The optimized\u03c6 * is then kept fixed during policy improvement. Lines 16-22 correspond to the standard policy gradient approach for improving the performance of a policy. In our case, the policy \u03b2 first outputs a vector\u00ea which gets mapped by\u03c6 * to an action. The observed transition is then used to compute the policy gradient (Sutton et al. 2000) for updating the parameters of \u03b2 towards \u03b2 * . If a critic is used for computing the policy gradients, then it is also subsequently updated by minimizing the TD error (Sutton and Barto 2018). This iterative process of adaption and policy improvement continues for every change in the action set size.\nAlgorithm 1: Lifelong Adaptation and Improvement for Changing Actions (LAICA)\n1 Initialize \u03b2 * 0 ,\u03c6 * 0 , \u03d5 * 0 . 2 for change k = 1, 2... do 3 \u03b2 k \u2190 \u03b2 * k\u22121 4 \u03d5 k \u2190 \u03d5 * k\u22121 5\u03c6 k \u2190\u03c6 * k\u22121 6\nAdd parameters in\u03c6 k for new actions ", "publication_ref": ["b10", "b13", "b24", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "F: Empirical Analysis Details Domains", "text": "To demonstrate the effectiveness of our proposed method(s) on lifelong learning problems, we consider a maze environment and two domains corresponding to real-world applications, all with large set of changing actions. For each of these domains, the total number of actions were randomly split into five mutually exclusive sets of equal sizes. Initially, the agent only had the actions available in the first set and after every change the next set of action was made available additionally. For all our experiments, changes to the action set were made after equal intervals.\nMaze. As a proof-of-concept, we constructed a continuous-state maze environment where the state comprised of the coordinates of the agent's current location. The agent has 8 equally spaced actuators (each actuator moves the agent in the direction the actuator is pointing towards) around it, and it can choose whether each actuator should be on or off. Therefore, the total number of possible actions is 2 8 = 256. The net outcome of an action is the vectorial summation of the displacements associated with the selected actuators. The agent is penalized at each time step to encourage it to reach the goal as quickly as possible. A goal reward is given when it reaches the goal position To make the problem more challenging, random noise was added to the action 10% of the time and the maximum episode length was 150 steps.\nCase Study: Real-world recommender systems. We consider two real-world applications of recommender systems that require decision making over multiple time steps and where the number of possible decisions can vary over the lifetime of the system. First, a web-based video-tutorial platform, which has a recommendation engine that suggests a series of tutorial videos on various software. On this tutorial platform, there is a large pool of available tutorial videos on several software and new videos are uploaded periodically. This requires the recommender system to keep adjusting to these changes constantly. The aim for the recommender system is to suggest tutorials so as to meaningfully engage the user on how to use these software and convert novice users into experts in their respective areas of interest.\nThe second application is a professional multi-media editing software. Modern multimedia editing software often contain many tools that can be used to manipulate the media, and this wealth of options can be overwhelming for users. Further, with every major update to the software, new tools are developed and incorporated into the software to enhance user experience. In this domain, an agent suggests which of the available tools the user may want to use next. The objective is to increase user productivity and assist in achieving their end goal.\nFor both of these applications, an existing log of user's click stream data was used to create an n-gram based MDP model for user behavior (Shani, Heckerman, and Brafman 2005). In the tutorial recommendation task, sequences of user interaction were aggregated to obtain over 29 million clicks. Similarly, sequential usage patterns of the tools in the multi-media editing software were collected to obtain a total of over 1.75 billion user clicks. Tutorials and tools that had less than 100 clicks in total were discarded. The remaining 1498 tutorials and 1843 tools for the web-based tutorial platform and the multi-media software, respectively, corresponds to the total number of actions. The MDP had continuous state-space, where each state consisted of the feature descriptors associated with each item (tutorial or tool) in the current n-gram. Rewards were chosen based on a surrogate measure for difficulty level of tutorials and popularity of final outcomes of user interactions in the multi-media editing software, respectively.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "For the maze domain, single layer neural networks were used to parameterize both the actor and critic. The learning rates for policy were searched over the range [1e \u2212 2, 1e \u2212 4] and for critic it was searched over the range [5e \u2212 2, 5e \u2212 4]. State features were represented using the 3 rd order coupled Fourier basis (Konidaris, Osentoski, and Thomas 2011). The discounting parameter \u03b3 was set to 0.99 and eligibility traces to 0.9. Since it was a toy domain, the output dimension of \u03b2 was kept fixed to 2. After every change in the action set, 500 randomly drawn trajectories were used to update\u03c6. The value of \u03bb was searched over the range [1e \u2212 2, 1e \u2212 4].\nFor the real-world environments, 2 layer neural networks were used to parameterize both the actor and critic. The learning rates for both were searched over the range [1e \u2212 2, 1e \u2212 4]. Similar to prior works, the module for encoding state features was shared to reduce the number of parameters, and the learning rate for it was additionally searched over [1e \u2212 2, 1e \u2212 4]. The dimension of the neural network's hidden layer was searched over {64, 128, 256}. The discounting parameter \u03b3 was set to 0.9. For actor-critic based results eligibility traces was set to 0.9 and for DPG the target actor and policy update rate was fixed to its default setting of 0.001. The output dimension of \u03b2 was searched over {16, 32, 64}. After every change in the action set, samples from 2000 randomly drawn trajectories were used to update\u03c6.\nFor all the results of the LAICA, since the output of \u03b2 was defined over a continuous space, it was parameterized as the isotropic normal distribution. The value for variance was kept fix for the Maze domain and was searched over [0.5, 1.5]. For the real-world domains, the variance was parameterized and learned along with other parameters. The function \u03d5 was parameterized to concatenate the state features of both s and s and use a single layer neural network to project to a space corresponding to the inferred structure in the actions. The function\u03c6 was linearly parameterized to compute a Boltzmann distribution over the available set of actions. After every change in the action set, new rows were stacked in its weight matrix for generating scores for the new actions. The learning rates for functions\u03c6 and \u03d5 were jointly searched over [1e \u2212 2, 1e \u2212 4].\nAs our proposed method decomposes the overall policy into two components, the resulting architecture resembles that of a one layer deeper neural network. Therefore, for the baselines, we ran the experiments with a hyper-parameter search for policies with additional depths {1, 2, 3}, each with different combinations of width {2, 16, 64}. The remaining architectural aspects and properties of the hyper-parameter search for the baselines were performed in the same way as mentioned above for our proposed method. For dealing with new actions, new rows were stacked in the weight matrix of the last layer of the policy in Baseline(2).\nIn total, 200 settings for each algorithm, for each domain, were uniformly sampled from the respective hyper-parameter ranges/sets mentioned. Results from the best performing setting are reported in all the plots. Each hyper-parameter setting was independently ran using 10 different seeds to get the standard error of the performance.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "The research was supported by and partially conducted at Adobe Research. We are also immensely grateful to the three anonymous reviewers who shared their insights and feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Constrained policy optimization", "journal": "", "year": "2017", "authors": "J Achiam; D Held; A Tamar; P Abbeel"}, {"ref_id": "b1", "title": "Transfer of deep reactive policies for mdp planning", "journal": "", "year": "2018", "authors": "Garg Bajpai; A N Bajpai; S Garg"}, {"ref_id": "b2", "title": "Learning action representations for reinforcement learning", "journal": "", "year": "2018", "authors": "[ Boutilier"}, {"ref_id": "b3", "title": "Lifelong machine learning", "journal": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "year": "2016", "authors": "Z Chen; B Liu"}, {"ref_id": "b4", "title": "On the measure of voronoi cells", "journal": "Journal of Applied Probability", "year": "2017", "authors": "[ Devroye"}, {"ref_id": "b5", "title": "Answer set programming for non-stationary markov decision processes", "journal": "", "year": "2015", "authors": ""}, {"ref_id": "b6", "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "journal": "", "year": "1999", "authors": "Abbeel Finn; C Levine ; Finn; P Abbeel; S Levine; R M French"}, {"ref_id": "b7", "title": "Reinforcement learning for dec-mdps with changing action sets and partially ordered dependencies", "journal": "", "year": "2008", "authors": "T Gabel; M Riedmiller"}, {"ref_id": "b8", "title": "A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions", "journal": "", "year": "2018", "authors": "Ortner Gajane; P Auer ; Gajane; R Ortner; P Auer"}, {"ref_id": "b9", "title": "Meta-reinforcement learning of structured exploration strategies", "journal": "", "year": "2018", "authors": ""}, {"ref_id": "b10", "title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "journal": "", "year": "2015", "authors": ""}, {"ref_id": "b11", "title": "Approximately optimal approximate reinforcement learning", "journal": "", "year": "2002", "authors": "S Kakade; J Langford"}, {"ref_id": "b12", "title": "Nearoptimal reinforcement learning in polynomial time", "journal": "Machine learning", "year": "2002", "authors": "M Kearns; S Singh"}, {"ref_id": "b13", "title": "Overcoming catastrophic forgetting in neural networks", "journal": "", "year": "2013", "authors": "D P Kingma; M Welling; J Kirkpatrick; R Pascanu; N Rabinowitz; J Veness; G Desjardins; A A Rusu; K Milan; J Quan; T Ramalho; A Grabska-Barwinska"}, {"ref_id": "b14", "title": "Value function approximation in reinforcement learning using the fourier basis", "journal": "", "year": "2011", "authors": "Osentoski Konidaris; G Thomas ; Konidaris; S Osentoski; P S Thomas"}, {"ref_id": "b15", "title": "Gradient episodic memory for continual learning", "journal": "", "year": "2017", "authors": "D Lopez-Paz"}, {"ref_id": "b16", "title": "Where to add actions in human-in-the-loop reinforcement learning", "journal": "", "year": "2017", "authors": " Mandel"}, {"ref_id": "b17", "title": "Near-optimal representation learning for hierarchical reinforcement learning", "journal": "", "year": "2013", "authors": " Nachum"}, {"ref_id": "b18", "title": "Markov decision processes: discrete stochastic dynamic programming", "journal": "John Wiley & Sons", "year": "2014", "authors": "M L Puterman"}, {"ref_id": "b19", "title": "Ella: An efficient lifelong learning algorithm", "journal": "", "year": "2013", "authors": "P Ruvolo; E Eaton"}, {"ref_id": "b20", "title": "An mdp-based recommender system", "journal": "Journal of Machine Learning Research", "year": "2005-09", "authors": "Heckerman Shani; G Shani; D Heckerman; R I Brafman"}, {"ref_id": "b21", "title": "Deterministic policy gradient algorithms", "journal": "", "year": "2014", "authors": "[ Silver"}, {"ref_id": "b22", "title": "Lifelong machine learning systems: Beyond learning algorithms", "journal": "", "year": "2013", "authors": "Yang ; Li ; Silver; D L Yang; Q Li; L "}, {"ref_id": "b23", "title": "Reinforcement learning: An introduction", "journal": "MIT press", "year": "2018", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b24", "title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "", "year": "2000", "authors": " Sutton"}, {"ref_id": "b25", "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "journal": "", "year": "1999", "authors": "Precup Sutton; R S Singh ; Sutton; D Precup; S Singh; G Tennenholtz; S Mannor"}, {"ref_id": "b26", "title": "Meta-gradient reinforcement learning", "journal": "Springer", "year": "1998", "authors": "S Thrun; J Wang; Z Kurth-Nelson; D Tirumala; H Soyer; J Leibo; R Munos; C Blundell; D Kumaran; M Botivnick; Z Xu; H P Van Hasselt; D Silver"}, {"ref_id": "b27", "title": "Continual learning through synaptic intelligence", "journal": "", "year": "2017", "authors": "Poole Zenke; F Ganguli ; Zenke; B Poole; S Ganguli"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "s\u2208S a\u2208A k P (s, a)KL (P (s |s, a) P (s |s,\u00e2)) . (3) Equation (", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Property 1 .1For some constant C, \u2212L(\u03c6, \u03d5) is lower bounded by s\u2208S a\u2208A k s \u2208S P (s, a, s ) E log\u03c6(\u00e2|\u00ea) \u00ea \u223c \u03d5(\u2022|s, s ) \u2212KL \u03d5(\u00ea|s, s ) P (\u00ea|s, s ) + C. (4) Proof. See Appendix C.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: An illustration of a typical performance curve for a lifelong learning agent. The point (a) corresponds to the performance of the current policy in M k . The point (b) corresponds to the performance drop resulting as a consequence of adding new actions. We call the phase between (a) and (b) as the adaptation phase, which aims at minimizing this drop when adapting to new set of actions. The point (c) corresponds to the improved performance in M k+1 by optimizing the policy to leverage the new set of available actions. \u00b5 * represents the best performance of the hypothetical policy which has access to the entire structure in the action space.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Lifelong learning experiments with a changing set of actions in the maze domain. The learning curves correspond to the running mean of the best performing setting for each of the algorithms. The shaded regions correspond to standard error obtained using 10 trials. Vertical dotted bars indicate when the set of actions was changed.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Lifelong learning experiments with a changing set of actions in the recommender system domains. The learning curves correspond to the running mean of the best performing setting for each of the algorithms. The shaded regions correspond to standard error obtained using 10 trials. Vertical dotted bars indicate when the set of actions was changed.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "P(s, a)KL (P (s |s, a) P (s |s,\u00e2)) |s, a) log P (s |s,\u00e2) + C 1where C 1 is a constant corresponding to the entropy term in KL that is independent of\u00e2. Continuing, we take the negative on both sides,\u2212L(\u03c6, \u03d5) = s\u2208S a\u2208A k s \u2208S P (s, a, s ) log P (s |s,\u00e2) \u2212 C 1 = s\u2208S a\u2208A k s \u2208S P (s, a, s ) log P (s,\u00e2, s ) P (s,\u00e2) \u2212 C 1 = s\u2208S a\u2208A k s \u2208S P (s, a, s ) log P (s,\u00e2, s ) s \u2208S P (s,\u00e2, s ) \u2212 C 1 = s\u2208S a\u2208A k s \u2208S P (s, a, s ) [log P (s,\u00e2, s ) \u2212 log Z] \u2212 C 1 where Z = s \u2208S P (s,\u00e2, s ) is the normalizing factor. As \u2212 log Z is always positive, we obtain the following lower bound, \u2212L(\u03c6, \u03d5) \u2265 s\u2208S a\u2208A k s \u2208S P (s, a, s ) log P (s,\u00e2, s ) \u2212 C 1 = s\u2208S a\u2208A k s \u2208S P (s, a, s ) log P (\u00e2|s, s )P (s, s ) \u2212 C 1 = s\u2208S a\u2208A k s \u2208S P (s, a, s ) log P (\u00e2|s, s ) + C 2 \u2212 C 1 ,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "= log E P (\u00e2|\u00ea)P (\u00ea|s, s ) Q(\u00ea|s, s ) Q(\u00ea|s, s ) \u2265 E log P (\u00e2|\u00ea)P (\u00ea|s, s ) Q(\u00ea|s, s ) Q(\u00ea|s, s ) (from Jensen's inequality) = E log P (\u00e2|\u00ea) Q(\u00ea|s, s ) + E log P (\u00ea|s, s ) Q(\u00ea|s, s ) Q(\u00ea|s, s ) = E log P (\u00e2|\u00ea) Q(\u00ea|s, s ) \u2212 KL Q(\u00ea|s, s ) P (\u00ea|s, s ) .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "a t and observe s t+1 11 Add transition to B 12 for iteration = 0, 1, 2... do 13 Sample batch b \u223c B 14 Update\u03c6 k and \u03d5 k by maximizing L lb (\u03c6 k , \u03d5 k ) for b 15 16 for episode = 0, 1, 2... do 17 for t = 0, 1, 2... do 18 Sample\u00ea t \u223c \u03b2 k (\u2022|s t ) 19 Map\u00ea t to an action a t using\u03c6 * k (e) 20 Execute a t and observe s t+1 , r t 21 Update \u03b2 k using any policy gradient algorithm", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "v \u03c0 k (s) = E[ \u221e t=0 \u03b3 t R t |s, \u03c0 k ].", "formula_coordinates": [3.0, 122.4, 552.83, 124.46, 14.11]}, {"formula_id": "formula_1", "formula_text": "ai\u2208A inf aj \u2208A e i \u2212 e j 1 , then v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max . Proof. See Appendix B.", "formula_coordinates": [3.0, 319.5, 192.71, 201.83, 63.62]}, {"formula_id": "formula_2", "formula_text": "P (U k \u2286 2 Y ) = 1. We refer to Y as the element-wise-support of U k . If \u2200k, the element-wise-support of U k in an L-MDP is E, then as k \u2192 \u221e the sub-optimality vanishes. That is, lim k\u2192\u221e v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2192 0.", "formula_coordinates": [3.0, 319.5, 333.66, 238.5, 57.66]}, {"formula_id": "formula_3", "formula_text": "S \u00d7 S \u00d7\u00ca \u2192 [0, 1] and\u03c6 k :\u00ca \u00d7 A \u2192 [0, 1] such that sup s\u2208S,a\u2208A KL P (S t+1 |S t = s, A t = a) P (S t+1 |S t = s, A t =\u00c2) \u2264 \u03b4 2 k /2, (2", "formula_coordinates": [4.0, 319.5, 186.56, 238.5, 70.41]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [4.0, 554.13, 246.67, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "where\u00c2 \u223c\u03c6 k (\u2022|\u00ca) and\u00ca \u223c \u03d5(\u2022|S t , S t+1 ), then v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3 (\u03c1 k + \u03b4 k ) (1 \u2212 \u03b3) 2 R max .", "formula_coordinates": [4.0, 319.5, 269.45, 203.21, 39.35]}, {"formula_id": "formula_6", "formula_text": "\u03b2 * k\u22121 and \u03c6 * k\u22121 , in M k\u22121 are carried over, i.e., \u03b2 k := \u03b2 * k\u22121 and \u03c6 k :=\u03c6 * k\u22121 .", "formula_coordinates": [5.0, 54.0, 646.55, 441.36, 37.71]}, {"formula_id": "formula_7", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = \u221e k=1 v \u03b2 k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6k M k (s 0 ) Adaptation + \u221e k=1 v \u03b2 * k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6 * k M k (s 0 ) Policy Improvement +\u2206,", "formula_coordinates": [5.0, 319.5, 106.54, 248.16, 92.95]}, {"formula_id": "formula_8", "formula_text": "d \u03c0 (s) = (1 \u2212 \u03b3) t \u03b3 t P (S t = s|\u03c0). (5", "formula_coordinates": [10.0, 231.74, 159.92, 322.39, 21.69]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [10.0, 554.13, 162.31, 3.87, 8.64]}, {"formula_id": "formula_10", "formula_text": "D(\u03c0 1 , \u03c0 2 ) = S |d \u03c01 (s) \u2212 d \u03c02 (s)| ds = S (1 \u2212 \u03b3) t \u03b3 t P (S t = s|\u03c0 1 ) \u2212 (1 \u2212 \u03b3) t \u03b3 t P (S t = s|\u03c0 2 ) ds. (6", "formula_coordinates": [10.0, 142.03, 211.39, 412.1, 50.11]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [10.0, 554.13, 242.2, 3.87, 8.64]}, {"formula_id": "formula_12", "formula_text": "D(\u03c0 1 , \u03c0 2 ) = (1 \u2212 \u03b3)(1 \u2212 \u03b3P \u03c01 ) \u22121 d 0 \u2212 (1 \u2212 \u03b3)(1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 = (1 \u2212 \u03b3) (1 \u2212 \u03b3P \u03c01 ) \u22121 \u2212 (1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 . (7", "formula_coordinates": [10.0, 166.59, 282.45, 387.54, 30.79]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [10.0, 554.13, 301.42, 3.87, 8.64]}, {"formula_id": "formula_14", "formula_text": "), let G 1 = (1 \u2212 \u03b3P \u03c01 ) \u22121 and G 2 = (1 \u2212 \u03b3P \u03c02 ) \u22121 . Then, G 1 \u2212 G 2 = G 1 (G \u22121 2 \u2212 G \u22121 1 )G 2", "formula_coordinates": [10.0, 53.69, 318.33, 266.83, 24.63]}, {"formula_id": "formula_15", "formula_text": "D(\u03c0 1 , \u03c0 2 ) = (1 \u2212 \u03b3) (1 \u2212 \u03b3P \u03c01 ) \u22121 ((1 \u2212 \u03b3P \u03c02 ) \u2212 (1 \u2212 \u03b3P \u03c01 ))(1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 = (1 \u2212 \u03b3) (1 \u2212 \u03b3P \u03c01 ) \u22121 (\u03b3P \u03c01 \u2212 \u03b3P \u03c02 )(1 \u2212 \u03b3P \u03c02 ) \u22121 d 0 1 = (1 \u2212 \u03b3P \u03c01 ) \u22121 (\u03b3P \u03c01 \u2212 \u03b3P \u03c02 ) d \u03c02 1 . (8", "formula_coordinates": [10.0, 130.84, 348.05, 423.29, 47.37]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [10.0, 554.13, 383.6, 3.87, 8.64]}, {"formula_id": "formula_17", "formula_text": "(1 \u2212 \u03b3P \u03c01 ) \u22121 1 = t (\u03b3P \u03c01 ) t 1 \u2264 t \u03b3 t P \u03c0 t 1 1 = t \u03b3 t \u2022 1 = (1 \u2212 \u03b3) \u22121 . (9", "formula_coordinates": [10.0, 145.48, 422.54, 408.65, 24.83]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [10.0, 554.13, 426.57, 3.87, 8.64]}, {"formula_id": "formula_19", "formula_text": "D(\u03c0 1 , \u03c0 2 ) \u2264 \u03b3(1 \u2212 \u03b3) \u22121 (P \u03c01 \u2212 P \u03c02 )d \u03c02 1 .", "formula_coordinates": [10.0, 211.18, 467.31, 189.63, 13.22]}, {"formula_id": "formula_20", "formula_text": "ai\u2208A inf aj \u2208A e i \u2212 e j 1 , then v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max .", "formula_coordinates": [10.0, 95.34, 516.72, 284.97, 44.75]}, {"formula_id": "formula_21", "formula_text": "\u00b5 * k \u2264 v \u03c0 * k . Therefore , v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 v \u00b5 * (s 0 ) \u2212 v \u00b5 * k (s 0 ) . (10", "formula_coordinates": [10.0, 218.36, 589.32, 335.49, 33.86]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [10.0, 553.85, 613.85, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 )| \u2264 (1 \u2212 \u03b3) \u22121 S d \u00b5 * (s)R(s)ds \u2212 (1 \u2212 \u03b3) \u22121 S d \u00b5 * k (s)R(s)ds = (1 \u2212 \u03b3) \u22121 S d \u00b5 * (s) \u2212 d \u00b5 * k (s) R(s)ds .(11)", "formula_coordinates": [10.0, 137.32, 654.12, 420.68, 48.85]}, {"formula_id": "formula_24", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u00b5 * (s) \u2212 d \u00b5 * k (s) ds \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u00b5 * (s) \u2212 d \u00b5 * k (s) ds = (1 \u2212 \u03b3) \u22121 R max D(\u00b5 * , \u00b5 * k ) \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max (P \u00b5 k \u2212 P \u00b5 * k )d \u00b5 * k 1(12)", "formula_coordinates": [11.0, 175.37, 77.88, 382.63, 86.26]}, {"formula_id": "formula_25", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s P \u00b5 k (s) \u2212 P \u00b5 * k (s) 1 \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s,s \u0113 (P (s |s,\u0113) \u2212 P (s |s,\u0113 k ))\u00b5 * (\u0113|s)d\u0113 \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s,s ,\u0113 P (s |s,\u0113) \u2212 P (s |s,\u0113 k ) .(13)", "formula_coordinates": [11.0, 131.21, 202.37, 426.79, 85.21]}, {"formula_id": "formula_26", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max . Corollary 1. Let Y \u2286 E be the smallest closed set such that, P (U k \u2286 2 Y ) = 1. We refer to Y as the element-wise-support of U k . If \u2200k, the element-wise-support of U k in an L-MDP is E, then as k \u2192 \u221e the sub-optimality vanishes. That is, lim k\u2192\u221e v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2192 0.", "formula_coordinates": [11.0, 54.0, 334.84, 504.0, 93.51]}, {"formula_id": "formula_27", "formula_text": "consequently k \u2192 0 and thus v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) \u2192 0. Theorem 2. For an L-MDP M k , If there exists a \u03d5 : S \u00d7 S \u00d7\u00ca \u2192 [0, 1] and\u03c6 k :\u00ca \u00d7 A \u2192 [0, 1] such that sup s\u2208S,a\u2208A KL P (S t+1 |S t = s, A t = a) P (S t+1 |S t = s, A t =\u00c2) \u2264 \u03b4 2 k /2, (14", "formula_coordinates": [11.0, 53.67, 496.65, 500.18, 70.41]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [11.0, 553.85, 550.86, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "where\u00c2 \u223c\u03c6 k (\u2022|\u00ca) and\u00ca \u223c \u03d5(\u2022|S t , S t+1 ), then v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3 (\u03c1 k + \u03b4 k ) (1 \u2212 \u03b3) 2 R max .", "formula_coordinates": [11.0, 54.0, 578.64, 335.96, 40.12]}, {"formula_id": "formula_30", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03c0 * k (s 0 ) + v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ). Using Theorem (1), v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3\u03c1 k (1 \u2212 \u03b3) 2 R max + v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) .(15)", "formula_coordinates": [11.0, 54.0, 642.89, 504.0, 58.55]}, {"formula_id": "formula_31", "formula_text": "v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u03c0 * k (s) \u2212 d \u03c0 * * k (s) ds \u2264 (1 \u2212 \u03b3) \u22121 R max S d \u03c0 * k (s) \u2212 d \u03c0 * * k (s) ds = (1 \u2212 \u03b3) \u22121 R max D(\u03c0 * k , \u03c0 * * k ) = \u03b3(1 \u2212 \u03b3) \u22122 R max (P \u03c0 * k \u2212 P \u03c0 * * k )d \u03c0 * * k 1 \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max E 2T V P \u03c0 * k (s |s) P \u03c0 * * k (s |s) s \u223c d \u03c0 * * k ,", "formula_coordinates": [12.0, 127.33, 74.25, 357.34, 105.22]}, {"formula_id": "formula_32", "formula_text": "v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s 2KL P \u03c0 * k (s |s) P \u03c0 * * k (s |s) , \u2264 \u03b3(1 \u2212 \u03b3) \u22122 R max sup s,a 2KL (P (s |s, a) P (s |s,\u00e2)) ,", "formula_coordinates": [12.0, 143.77, 202.37, 324.46, 47.91]}, {"formula_id": "formula_33", "formula_text": "v \u03c0 * k (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3\u03b4 k (1 \u2212 \u03b3) 2 R max .(16)", "formula_coordinates": [12.0, 229.63, 282.91, 328.37, 22.31]}, {"formula_id": "formula_34", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03c0 * * k (s 0 ) \u2264 \u03b3 (\u03c1 k + \u03b4 k ) (1 \u2212 \u03b3) 2 R max . C: Lower Bound Objective For Adaptation L(\u03c6, \u03d5) =", "formula_coordinates": [12.0, 174.44, 323.81, 241.32, 77.3]}, {"formula_id": "formula_37", "formula_text": "\u2212L(\u03c6, \u03d5) \u2265 s\u2208S a\u2208A k s \u2208S P (s, a, s ) E log\u03c6(\u00e2|\u00ea) \u03d5(\u00ea|s, s ) \u2212 KL \u03d5(\u00ea|s, s ) P (\u00ea|s, s ) + C,", "formula_coordinates": [13.0, 101.53, 304.12, 408.94, 20.76]}, {"formula_id": "formula_38", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = \u221e k=1 v \u03b2 k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6k M k (s 0 ) Adaptation + \u221e k=1 v \u03b2 * k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6 * k M k (s 0 ) Policy Improvement +\u2206,", "formula_coordinates": [13.0, 120.38, 391.32, 371.23, 44.32]}, {"formula_id": "formula_39", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u00b1 v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u00b1 v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) + v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) . As \u03b2 2 := \u03b2 * 1 and\u03c6 2 :=\u03c6 * 1 in M 2 , v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b22\u03c62 M 2 (s 0 ) + v \u03b2 * 1\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c6 * 1 M 1 (s 0 ) + v \u03b21\u03c6 * 1 M 1 (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 )", "formula_coordinates": [13.0, 53.64, 473.71, 467.33, 120.81]}, {"formula_id": "formula_40", "formula_text": "v \u00b5 * (s 0 ) \u2212 v \u03b21\u03c61 M 1 (s 0 ) = v \u00b5 * (s 0 ) \u2212 v \u03b23\u03c63 M 3 (s 0 ) + 2 k=1 v \u03b2 * k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6 * k M k (s 0 ) + 2 k=1 v \u03b2 k\u03c6 * k M k (s 0 ) \u2212 v \u03b2 k\u03c6k M k (s 0 ) .", "formula_coordinates": [13.0, 128.69, 626.1, 354.63, 52.26]}, {"formula_id": "formula_41", "formula_text": "L lb (\u03c6, \u03d5) := E log\u03c6(\u00c2 t |\u00ca t ) \u03d5(\u00ca t |S t , S t+1 ) \u2212 \u03bbKL \u03d5(\u00ca t |S t , S t+1 ) P (\u00ca t |S t , S t+1 ) .", "formula_coordinates": [14.0, 121.73, 170.33, 368.54, 11.5]}, {"formula_id": "formula_42", "formula_text": "1 Initialize \u03b2 * 0 ,\u03c6 * 0 , \u03d5 * 0 . 2 for change k = 1, 2... do 3 \u03b2 k \u2190 \u03b2 * k\u22121 4 \u03d5 k \u2190 \u03d5 * k\u22121 5\u03c6 k \u2190\u03c6 * k\u22121 6", "formula_coordinates": [14.0, 65.46, 354.13, 108.93, 72.47]}], "doi": ""}