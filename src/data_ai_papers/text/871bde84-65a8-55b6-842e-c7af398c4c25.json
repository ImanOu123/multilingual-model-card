{"title": "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths", "authors": "Xiangqing Shen; Siwei Wu; Rui Xia", "pub_date": "", "abstract": "ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday ifthen knowledge triplets, i.e., {head event, relation, tail event}. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines. We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC's advantages in Knowledge Coverage and Multi-hop Paths. Both the source code of Rel-CSKGC and Dense-ATOMIC are publicly available on https://github.com/ NUSTM/Dense-ATOMIC.", "sections": [{"heading": "Introduction", "text": "ATOMIC is a large-scale human-annotated commonsense knowledge graph focusing on the inferential knowledge in social life . It consists of nine if-then relation types describing the causes, effects, agent, stative, and theme of an event. The research on ATOMIC has drawn more and more attention in recent years. An increasing number of downstream tasks, including * *Corresponding author commonsense reasoning (Yu et al., 2022), storytelling (Brahman and Chaturvedi, 2020), question answering (Heo et al., 2022), dialog generation (Wu et al., 2022), etc., have improved their performances by acquiring and utilizing the commonsense knowledge from ATOMIC.\nCurrently, ATOMIC was constructed under onehop annotations. It began with 24,000 pre-defined base events and nine relation types. For each base event and each relation, the annotators were asked to write a possible tail event based on one-hop reasoning. As shown in Figure 1, given the base event \"X asks Y to marry\", the annotated tail events can be \"loving\" under the relation of \"xAttr\", \"smiles\" under the relation of \"xEffect\", and \"says yes\" under the relation of \"oEffect\".\nIn such a one-hop annotation manner, each base event and its related annotated tail events shape a bipartite graph containing only B-to-A links, where B denotes the Base event and A denotes the Annotated tail event. Thereby, the whole graph of ATOMIC can be viewed as a set of B-to-A bipartite graphs, while the B-to-B, A-to-B and A-to-A links between different bipartite graphs were almost ignored. In Figure 1, the dashed lines illustrate such missing links in ATOMIC, e.g., an annotated tail event \"in front of Y\" and a base event \"X asks Y to marry\" in two different bipartite graphs miss a link of the \"xIntent\" relation.\nThis leads to two shortcomings of ATOMIC. Firstly, with only B-to-A links, ATOMIC contains very few multi-hop paths, since an annotated tail event cannot become the head event of a triplet. Secondly, missing B-to-B, A-to-B and A-to-A links cause unsatisfactory knowledge coverage, despite its high-quality human-annotated commonsense knowledge. Both shortcomings limit the potential of ATOMIC in practical applications. Intuitively, an ideal CSKG requires high knowledge coverage to meet the needs of various tasks, and massive multi-hop paths to understand the evolu-  Figure 1: ATOMIC vs. Dense-ATOMIC. Firstly, Dense-ATOMIC completes many missing links in ATOMIC, including B-to-A, B-to-B, A-to-B, and A-to-A links, e.g., missing \"oPersona\" link between \"X proposes marriage\" and \"Y is delightful\" (type: A-to-A); Secondly, Dense-ATOMIC contains more multi-hop paths, e.g., a two-hop path \"X asks Y to marry\" \u2192 \"Y says yes\" \u2192 \"X smiles\" after predicting missing links on normalizd ATOMIC. tion between different events.\nIn this work, we aim to construct a denselyconnected ATOMIC. The key is to complete different types of missing links, leading to denser ATOMIC with high knowledge coverage and massive multi-hop paths. We achieve this goal through three main steps: Normalizing Tail Events, Training a Relation Prediction Model and Constructing Dense-ATOMIC.\nFirstly, most of the annotated tail events in ATOMIC have different patterns to the base events, so we normalize annotated tail events in ATOMIC to a consistent pattern (\"Subject + Verb + Object\"), to facilitate subsequent CSKG completion. Specific relations are also grouped to mitigate ambiguity.\nSecondly, we train a relation prediction model based on a set of existing triplets in ATOMIC to infer the missing links on the whole graph, i.e., CSKG completion upon ATOMIC. To the best of our knowledge, most of the existing studies for CSKG completion utilized the translation based methods, which formalized the CSKG completion as a tail event ranking task given the head event and the relation. A graph convolutional network (GCN) was mostly employed to encode the graph embeddings of events, but its performance is unsatisfactory since the sparsity of ATOMIC limits the information propagation on the GCN (Malaviya et al., 2020). In contrast, in this work, we propose a method called Rel-CSKGC, which regards CSKG completion as a relation prediction problem given the head event and the tail event, and accordingly train a CSKG completion model based on ATOMIC.\nFinally, based on the CSKG completion model, we construct Dense-ATOMIC by inferring the missing links on ATOMIC. Figure 1 illustrates the main differences between ATOMIC and Dense-ATOMIC.\nWe conduct extensive evaluations towards the Rel-CSKGC method and the constructed Dense-ATOMIC, respectively.\nFirst, we compare Rel-CSKGC with several newly proposed relation prediction methods and translation based methods. Both automatic evaluation on an annotated subgraph and human evaluation on 500 sampled triplets show the advantage of Rel-CSKGC for completion on ATOMIC .\nNext, we evaluate Dense-ATOMIC from the perspectives of knowledge coverage and multi-hop paths respectively. Extensive experiments are conducted in terms of statistics, human evaluation, and simple downstream tasks. The results demonstrate that Dense-ATOMIC surpasses ATOMIC in terms of triplet counts by an order of magnitude, and multi-hop paths by more than two orders of magnitude, respectively, while at the same time maintaining its quality. Normalizing Tail Events, Training a Relation Prediction Model, and Constructing Dense-ATOMIC.", "publication_ref": ["b7", "b1", "b7", "b29", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Normalizing Tail Events", "text": "ATOMIC contains only B-to-A triplets. A CSKG completion model trained with B-to-A triplets is inapplicable to predict B-to-B, A-to-A, and A-to-B links, since base events (usually sentences) and annotated tail events (usually phrases or words) have different patterns. This results in a shortage of knowledge coverage and multi-hop paths during the completion.\nTo this end, we propose Normalizing Tail Events to convert annotated tail events to the same pattern as the base events, including subject removal, third person singular form conjugation, subject recovery, and relation grouping.\nSubject Removal For a few annotated tail events being complete sentences, we perform dependency tree parsing and part-of-speech tagging with CoreNLP (Manning et al., 2014) and remove subjects based on the two kinds of structure patterns, which makes the nodes in the graph become a uniform pattern and benefits the subject recovery process. For example, given a tail event \"He smiles\", we first remove the subject \"He\" and convert it to a universal expression \"Y smiles\" in the subject recovery process.\nThird Person Singular Form Conjugation In our preliminary experiments, a CSKG completion model tends to correlate phrases starting with \"to\" with relations such as \"xWant\", \"xIntent\", so we leverage WordNet (Miller, 1995) to acquire the verb root and add the suffix (-s, -es, etc.) according to English grammar.", "publication_ref": ["b16", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Subject Recovery", "text": "We add subjects to processed annotated tail events based on different relations. Relation Grouping Both \"xWant\" and \"xEffect\" describe the possible subsequent events, distinguished by \"to\" representing subject will. After third person singular form conjugation, the two relations may lead to ambiguity. We perform relation grouping for all these relations to mitigate ambiguity. \"xEffect\" and \"xWant\" form \"xAfter\" describing what will happen to X. \"oEffect\" and \"oWant\" form \"oAfter\" describing what will happen to Y. \"xAttr\" and \"xReact\" form \"xPersona\" describing how X feels or is described. It should be noted that the relation grouping process leads to a non-serious problem, i.e., the grouped relation cannot distinguish between subjective and objective semantics. However, it mitigates ATOMIC's sparsity issue and improves the performance of the relation prediction model.\nDue to the page limitation, the pseudo-code of normalizing tail events is present in Appendix A.\nIt is worth noting that our normalization method resembles a prior work (Fang et al., 2021b,a). Their purpose is to align ATOMIC with other CSKGs, while we focus on event alignment in ATOMIC by eliminating differences among different events.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training a Relation Prediction Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitation of Traditional Methods", "text": "Traditional methods for the completion of ATOMIC proposed to score all candidate tail events given the head event and the relation. The GCN for encoding graph embeddings of events induced two shortcomings: 1) it is difficult for a GCN to propagate information due to the sparse graph structure of ATOMIC (Malaviya et al., 2020); 2) it cannot sufficiently utilize semantic information of events.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Our Rel-CSKGC Method", "text": "To address these issues, we propose Rel-CSKGC, as illustrated in Figure 3. Specifically, ATOMIC is first decomposed into independent triplets, and then Rel-CSKGC predicts the relation given the head event and the tail event of a triplet. Rel-CSKGC utilizes no graph structure information thus avoiding the problem caused by the sparsity. Additionally, encoding both the head event and the tail event with the pretrained language model successfully takes advantage of semantic information.  Problem Formulation Given a CSKG G = (N, V ), where N is the set of nodes and V is the set of edges, we consider a single training instance as a triplet v i = (h, r, t) with the head event h, relation type r and the tail event t. Here, r \u2208 V and h, t \u2208 N . The objective of Rel-CSKGC is to predict the most reasonable r given h and t. 1\nMain Structure We utilize RoBERTa (Liu et al., 2019) to acquire contextual representations of freeform texts describing events. The input is the concatenation of h and t. We acquire the embedding matrix of h and t by:\n[H; T ] = RoBERTa([h; t])(1)\nwhere H \u2208 R |N |\u00d7D and T \u2208 R |N |\u00d7D . |N | is the number of tokens of the event, and D is the dimensionality of representation. We apply max pooling on H and T to acquire sentence embeddings e h and e t . The objective function can be defined with trainable weights W t \u2208 R 1\u00d7D and W c \u2208 R K\u00d72D :\no = sigmoid(W t e <s> )+softmax(W c (e h , e t )) (2)\nwhere K is the number of relations and e <s> the embedding of <s>-token used as a indicator for whether h and t are related.\nNegative Sampling Rel-CSKGC requires negative samples to predict unlinkable links. We consider the following two strategies to construct negative samples: 1) Random negative sampling. For a gold triplet, we randomly select an event from normalized ATOMIC as the new tail event to replace the original tail event; 2) Persona negative sampling. Triplets under relations of \"xPersona\" and \"oPersona\" follow the pattern of \"Subject + is + Adjective\" and account for a large part in ATOMIC. Models tend to always predict \"xPersona\" or \"oPersona\" when the given tail event follows the pattern of \"Subject + is + Adjective\". To alleviate this problem, we specifically construct negative samples by replacing the tail event of triplets under relations of \"xPersona\" and \"oPersona\" with a randomly-chosen event containing \"is\".", "publication_ref": ["b13"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Constructing Dense-ATOMIC", "text": "Based on Rel-CSKGC, we train a relation prediction model with existing triplets in ATOMIC and then use the model to complete missing links in ATOMIC. We adopt threshold-based link prediction to decide whether two events are related and propose an intra-and-inter cluster completion strategy to reduce the cost of completing entire ATOMIC.\nThreshold-based Link Prediction Thresholdbased link prediction (TLP) is a heuristic strategy to decide whether a relation is acceptable according to the probability predicted by Rel-CSKGC. Different thresholds are specifically tuned for different relations. The model predicts the relation only if the final probability is above the corresponding threshold. TLP is used in all our models as the last step for the link acceptance decision.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Intra-and-inter Cluster Completion Strategy", "text": "Since it's computationally expensive to iterate over all pairs of head and tail events during the inference, we design an intra-and-inter cluster completion strategy to trade off between the completion scale and the time complexity. In Figure 1, we consider each base event and its annotated tail events as a cluster. Intra-cluster completion infers missing links inside a cluster. Intuitively, annotated tail events in one cluster, written based on the same base event, are highly related and may contain more missing links. Inter-cluster completion infers missing links between different clusters. Annotated tail events in different clusters are written independently based on different base events, thus links between different clusters are under-explored. Due to the limited computing resource and time, we temporarily provide the results of 100 sampled clusters in this paper. Increasing the sampling size can further improve the scale of Dense-ATOMIC, but that will also linearly increases the computational cost. We will release versions with larger sampling sizes later.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of Our Rel-CSKGC Method", "text": "In this section, we compare Rel-CSKGC with relation prediction and translation based methods by experimenting on a newly annotated subgraph and human evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training and Test Set Construction", "text": "Training Set with Negative Sampling Following    2 The imbalance between random and persona negative sampling methods was established based on a preliminary experiment, which provided insights into optimal sampling sizes.\nTest Set with Annotated Subgraph To test the performance of Rel-CSKGC, we construct a ground-truth subgraph by randomly sampling three clusters from the test split and annotating all pairs of head events and tail events with the most reasonable relation. The statistic of the annotated ground-truth subgraph is shown in Table 2 ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Compared Methods", "text": "We select 4 baselines comprising two different types of CSKG completion methods and use the specific evaluation protocol for each of them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relation Prediction Methods", "text": "Baselines We adapt CE-random (Li et al., 2016), a method augmenting CSKGs by scoring novel tuples, to predict the missing relation. We also compare KG-BERT (Yao et al., 2019), which probes the performance of relation prediction methods on knowledge graphs. Note that we replace BERT (Devlin et al., 2019) with RoBERTa (Liu et al., 2019) in KG-BERT for fair comparison.\nEvaluation Protocal Ranking metrics (HITS and Mean Reciprocal Rank) designed for translation based methods are not applicable to relation prediction methods. By valuing precision more than recall on CSKG completion, we utilize precision for the evaluation of relation prediction methods.", "publication_ref": ["b12", "b30", "b4", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Translation Based Methods", "text": "Baselines SynLink (Malaviya et al., 2020) proposed to densify the CSKG with synthetic links for better graph representation. InductiveE  introduced indutive learning on the CSKG by enhancing the unseen event representations with neighboring structure information.\nEvaluation Protocal To handle the evaluation mismatch between Rel-CSKGC and translation based methods, we designed a transformation strategy. Specifically, we randomly sample 500 triplets from Malaviya et al. (2020)'s test split. For Syn-Link and InductivE, a threshold is set for hit@1 score, and a tail event is accepted only when the score is above the threshold. We tune the threshold to ensure the number of triplets inferred by Rel-CSKGC, SynLink, and InductivE close on these 500 triplets. We then calculate the proportion of meaningful triplets for different methods manually. 3", "publication_ref": ["b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "Relation Prediction Methods In Table 3, we compare Rel-CSKGC with different relation prediction methods, and Rel-CSKGC achieves consistent improvement on the test set of the annotated subgraph. Paired t-Test result proves that the improvement of Rel-CSKGC is significant. From Table 3, we can observe that the precision of intracluster completion is significantly higher than that of inter-cluster completion for all methods. This demonstrates that tail events annotated based on the same base event are highly related to each other and easier for models to predict relations, while the prediction for inter-cluster events is more challenging.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Method", "text": "Total   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Human Evaluation", "text": "Motivation Upon observing predictions of Rel-CSKGC, we note that some triplets could be reasonable, while the annotated subgraph doesn't cover them. For example, given a head event \"X accepts Y's apology\" and a tail event \"X is generous\", the annotated ground-truth relation is \"xPersona\", while Rel-CSKGC could predict another reasonable relation \"xIntent\". Consequently, we perform the human evaluation to check whether a predicted triplet is actually meaningful.\nResult We can find from the last row of Table 3 that Rel-CSKGC achieves an even higher precision of 0.80, suggesting that Rel-CSKGC can predict reasonable triplets neglected during the subgraph annotation. The high precision by human evaluation also guarantees the quality of predicted triplets.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Ablation Study", "text": "To validate the effectiveness of negative sampling, we report experimental results without negative sampling in Table 3. The performance of Rel-CSKGC drops dramatically without any negative sampling strategies, validating the effectiveness of negative sampling. By experimenting Rel-CSKGC with different scales of random negative samples in Figure 4, we find that the precision of Rel-CSKGC increases using both automatic and human evaluation as more negative samples are used for training.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Evaluation of the Constructed", "text": "Dense-ATOMIC", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Knowledge Coverage and Quality", "text": "In this subsection, we aim to answer the following question: Does Dense-ATOMIC yield higher knowledge coverage while ensuring the quality?\nTo this end, we statistically and manually compare Dense-ATOMIC with ATOMIC from the following three perspectives.  Dense-ATOMIC yields higher knowledge coverage In Table 5, we present the comparison between ATOMIC and Dense-ATOMIC. Dense-ATOMIC contains 3x more one-hop paths than ATOMIC, contributing a significantly higher knowledge coverage. It's worth noting that different tail events in ATOMIC could become the same after normalizing tail events, so Dense-ATOMIC contains slightly fewer events than ATOMIC.\nTriplets in Dense-ATOMIC have relatively high precision In Table 3, Rel-CSKGC achieves a precision of 0.80 by human evaluation. Moreover, from comparison results with translation based methods in Table 4, Rel-CSKGC outperforms two state-of-the-art methods by more than 7 percentage points. The high performance of Rel-CSKGC ensures the quality of predicted triplets to a certain extent.\nDense-ATOMIC benefits the performance of COMET To empirically demonstrate the knowledge coverage and quality of Dense-ATOMIC, we evaluate Dense-ATOMIC with COMET (Bosselut et al., 2019). The relation distribution of Dense-ATOMIC is long-tailed. We randomly sample 262,678 triplets from predicted triplets and recover the grouped relations to their original relations by following the relation distribution of the 's training split. Apart from the evaluation of perplexity, we design a strategy to evaluate the diversity score of generated tail events. For each relation, we randomly sample 10 head events from the test set. For each test sample consisting of a head event and a relation, 10 candidates are generated using beam search. For each candidate, we PPL \u2193 DS \u2191 COMET 11.14 9.16 COMET ours 11.11 10.77 ", "publication_ref": ["b0"], "figure_ref": [], "table_ref": ["tab_10", "tab_7", "tab_8"]}, {"heading": "COMET", "text": "COMET ours to study hard to study harder study hard to study more to study more to get a good grade to study to take a test to get a good grade to do well in school to take a test to do well in class to do well in school to apply for a job to get a good job to pass the class to apply for a job to get a prize to apply for a good job to go to school manually give a score of 0, 1, or 2, representing \"unreasonable\", \"plausible\", and \"reasonable\", respectively. We then merge candidates of similar semantics into a group and calculate the group average score. The diversity score of 10 candidates is the sum of the group scores. Intuitively, the lower perplexity and the higher diversity score indicate the higher knowledge quality and the higher knowledge coverage of Dense-ATOMIC, and COMET ours outperforms COMET on both metrics in Table 6. In Table 7, we can find that tail events generated by COMET ours are more semantically different.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11", "tab_12"]}, {"heading": "Multi-hop Paths in Dense-ATOMIC", "text": "The aim of this subsection is to answer the question: Can multi-hop paths in Dense-ATOMIC better present the commonsense knowledge? Accordingly, we evaluate multi-hop paths based on the human evaluation and performing a newly designed Commonsense Reasoning experiment, respectively:\nSampling Method 2-hop 3-hop 4-hop Random 0.69 0.62 0.50 Heuristic Rule 0.84 0.77 0.74   Human evaluation confirms the correctness of multi-hop paths in Dense-ATOMIC In Table 5, we have already shown that Dense-ATOMIC contains orders of magnitude more two-hop and threehop paths than ATOMIC. Now, to further validate the correctness of multi-hop paths, we perform the human evaluation on sampled paths to calculate the proportion of reasonable paths. Note that it's a common phenomenon (both KGs and CSKGs) that A \u2192 B and B \u2192 C are reasonable, while A \u2192 B \u2192 C is irrational. For example, {Beethoven, owner, piano} and {piano, color, black} are two reasonable triplets, but \"Beethoven\" and \"black\" are not related. Consequently, we additionally design a simple heuristic sampling rule: a multi-hop path A \u2192 . . . \u2192 C is chosen only when A and C are also linked in Dense-ATOMIC. By comparing with random sampling in Table 8, we can find that heuristic rule sampling consistently outperforms random sampling: the longer the multi-hop paths, the more significant the improvement. Multi-hop paths randomly sampled from Dense-ATOMIC with two different methods are illustrated in Table 9.\nDense-ATOMIC has the potential of providing contextual information for Commonsense Reasoning In order to further validate the effectiveness of multi-hop paths in Dense-ATOMIC, we utilize BART (Lewis et al., 2020) to perform generative Commonsense Reasoning with or without multi-hop paths. Specifically, with the heuristic rule above, we randomly sample 5000 four-hop paths from Dense-ATOMIC as the training samples. For test samples, we manually select 500 reasonable paths from Dense-ATOMIC. BART is trained to generate the subsequent event in two different settings: 1) given only the first node of the path; 2) given the first four nodes of the path.\nFrom Table 10, we can find that BART trained with multi-hop paths achieves better performance in that multi-hop paths could provide more contextual information useful for Commonsense Reasoning.  ", "publication_ref": ["b11"], "figure_ref": [], "table_ref": ["tab_10", "tab_13", "tab_14", "tab_3"]}, {"heading": "Related Work", "text": "ConceptNet (Speer et al., 2017) is a largescale CSKG merging various knowledge bases. ASER (Zhang et al., 2020b) contains the selectional preference knowledge extracted from more than 11 billion-token unstructured textual data. Tran-sOMCS (Zhang et al., 2020a) utilizes linguistic graphs to convert ASER into the same representation as ConceptNet. DISCOS (Fang et al., 2021b) aggregates the neighboring information to distill the commonsense knowledge in ASER.\nRecent years have seen crowdsourced CSKGs aiming to provide high-quality commonsense knowledge triplets.  released ATOMIC consisting of if-then knowledge triplets mainly about daily events. Hwang et al. (2021) augmented ATOMIC with event-centered and physicalentity triplets. GLUCOSE (Mostafazadeh et al., 2020) grounds the implicit commonsense knowledge about everyday situations in a narrative context for richer inferential content.\nDense-ATOMIC unleashes the power of ATOMIC for high knowledge coverage and multi-hop paths.\nPrior CSKG completion methods performed binary classification by scoring BiLSTM-encoded tuples (Li et al., 2016;Saito et al., 2018;Jastrz\u0119bski et al., 2018). Following translation based methods for the knowledge graph completion (Dettmers et al., 2018;Shang et al., 2019;Meilicke et al., 2019;Lovelace et al., 2021), Malaviya et al. (2020) additionally densified the CSKG based on BERT similarity and achieve promising results.  and Ju et al. (2022) designed heuristic rules to add more edges for nodes with fewer neighbors. Moghimifar et al. (2021) presented a neural-symbolic reasoner to learn logic rules during the training, making the CSKG completion process interpretable.\nRel-CSKGC differs from them in that we utilize pretrained language models to predict the relation given the head event and the tail event. Similar relation prediction methods targeting at the knowledge graph completion have been proposed (Socher et al., 2013;Yao et al., 2019;Cao et al., 2020). To our best knowledge, we are the first to explore the relation prediction method on CSKG completion.", "publication_ref": ["b27", "b33", "b32", "b6", "b8", "b21", "b12", "b23", "b9", "b3", "b25", "b18", "b14", "b15", "b10", "b20", "b26", "b30", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we construct Dense-ATOMIC for high knowledge coverage and massive multi-hop paths and accordingly propose a CSKG completion method called Rel-CSKGC to train a relation prediction model and infer the missing links in ATOMIC. Both automatic and human evaluation show the advantage of Rel-CSKGC over strong baselines. The statistics prove that Dense-ATOMIC has significantly more triplets and multi-hop paths, providing potential for high-quality downstream applications and multi-hop reasoning based on commonsense knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our approach for constructing Dense-ATOMIC still has two limitations: 1) to keep Dense-ATOMIC simple, we only consider the most reasonable relation in this paper, while the relation between two events can be complex and diversified. We will release versions of Dense-ATOMIC with diversified relations later; 2) due to page limitation, we only evaluate Dense-ATOMIC on simple commonsense reasoning tasks, and we will further validate the multi-hop reasoning capacity of Dense-ATOMIC on more complex downstream tasks in the future.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We would like to thank the Allen Institute for AI for their valuable work on ATOMIC. The ATOMIC is licensed under a license of CC BY, which allows remixing, transforming, and building upon the material for any purpose. We will also make our Dense-ATOMIC publicly available later. Mehrabi et al. (2021) have found representational harms in common sense resources. We acknowledge that the generated commonsense from our models might contain biases. All of the datasets and models are in English, which benefits English speakers more. We have employed 3 postgraduates experienced in natural language processing for annotation and human evaluation. We pay postgraduates around $8 per hour, well above the local average wage, and engage in constructive discussions if they have concerns about the process. B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nIn Ethics Statement section.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nIn Ethics Statement section.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? We use publically available datasets, and the authors of the dataset have made the corresponding declaration.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? The documentation of the artifacts will be released after the reviewing process. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. In Section 3 and 4.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "C Did you run computational experiments?", "text": "In Section 3 and 4.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? In Appendix B.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was supported by the Natural Science Foundation of China (No. 62076133), and the Natural Science Foundation of Jiangsu Province for Distinguished Young Scholars (No. BK20200018).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Algorithm for Normalizing Tail Events", "text": "Algorithm 1 presents the pseudo-code of Normalizing Tail Events in Section 2.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Normalizing Tail Events", "text": "Input: A set of annotations A and relations R Output: A set of sentences in present tense F A 1: Remove annotations with underscores or none, and get a series of filtered annotations F A 2: for each f a \u2208 F A, r \u2208 R do ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Implementation Details", "text": "Rel-CSKGC We use RoBERTa-large containing 335M parameters as the base model. We use a maximum sequence length of 100 and batch size of 128. The Adam optimizer is used for optimization with a learning rate of 2e-5 for RoBERTa-large and a learning rate of 1e-4 for MLP layers. The warmup proportion is set to 0.1. We train Rel-CSKGC with 1 NVIDIA RTX 3090 Graphical Card for 5 epochs, and it takes 20 hours to finish the training.\nCOMET ours To train COMET ours , we use the implentations provided here. 4 We use the learning rate of 1.625e-5 and the default values for other parameters.\nGenerative Commonsense Reasoning BARTbase is employed as the base model, which contains 140M parameters. We use a batch size of 128 and use the default values for other parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "COMET: commonsense transformers for automatic knowledge graph construction", "journal": "Long Papers", "year": "2019-07-28", "authors": "Antoine Bosselut; Hannah Rashkin; Maarten Sap; Chaitanya Malaviya; Asli Celikyilmaz; Yejin Choi"}, {"ref_id": "b1", "title": "Modeling protagonist emotions for emotion-aware storytelling", "journal": "", "year": "2020-11-16", "authors": "Faeze Brahman; Snigdha Chaturvedi"}, {"ref_id": "b2", "title": "Open Knowledge Enrichment for Long-Tail Entities", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Ermei Cao; Difeng Wang; Jiacheng Huang; Wei Hu"}, {"ref_id": "b3", "title": "Convolutional 2d knowledge graph embeddings", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Tim Dettmers; Pasquale Minervini; Pontus Stenetorp; Sebastian Riedel"}, {"ref_id": "b4", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "Benchmarking commonsense knowledge base population with an effective evaluation dataset", "journal": "Association for Computational Linguistics", "year": "2021-07-11", "authors": "Tianqing Fang; Weiqi Wang; Sehyun Choi; Shibo Hao; Hongming Zhang"}, {"ref_id": "b6", "title": "DISCOS: bridging the gap between discourse knowledge and commonsense knowledge", "journal": "ACM / IW3C2", "year": "2021-04-19", "authors": "Tianqing Fang; Hongming Zhang; Weiqi Wang"}, {"ref_id": "b7", "title": "Hypergraph transformer: Weakly-supervised multi-hop reasoning for knowledge-based visual question answering", "journal": "Association for Computational Linguistics", "year": "2022-05-22", "authors": "Yu-Jung Heo; Eun-Sol Kim; Suk Woo; Byoung-Tak Choi;  Zhang"}, {"ref_id": "b8", "title": "2021. (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs", "journal": "AAAI Press", "year": "2021", "authors": "Jena D Hwang; Chandra Bhagavatula; Jeff Ronan Le Bras; Keisuke Da; Antoine Sakaguchi; Yejin Bosselut;  Choi"}, {"ref_id": "b9", "title": "Commonsense mining as knowledge base completion? a study on the impact of novelty", "journal": "", "year": "2018", "authors": "Stanislaw Jastrz\u0119bski; Dzmitry Bahdanau; Seyedarian Hosseini; Michael Noukhovitch; Yoshua Bengio; Jackie Cheung"}, {"ref_id": "b10", "title": "Commonsense knowledge base completion with relational graph attention network and pre-trained language model", "journal": "ACM", "year": "2022-10-17", "authors": "Jinhao Ju; Deqing Yang; Jingping Liu"}, {"ref_id": "b11", "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "Association for Computational Linguistics", "year": "2020-07-05", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b12", "title": "Commonsense knowledge base completion", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Xiang Li; Aynaz Taheri; Lifu Tu; Kevin Gimpel"}, {"ref_id": "b13", "title": "Roberta: A robustly optimized BERT pretraining approach", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b14", "title": "Robust knowledge graph completion with stacked convolutions and a student re-ranking network", "journal": "Long Papers", "year": "2021-08-01", "authors": "Justin Lovelace; Denis Newman-Griffis; Shikhar Vashishth; Jill Fain Lehman; Carolyn P Ros\u00e9"}, {"ref_id": "b15", "title": "Commonsense knowledge base completion with structural and semantic context", "journal": "AAAI Press", "year": "2020-02-07", "authors": "Chaitanya Malaviya; Chandra Bhagavatula; Antoine Bosselut; Yejin Choi"}, {"ref_id": "b16", "title": "The stanford corenlp natural language processing toolkit", "journal": "", "year": "2014-06-22", "authors": "Christopher D Manning; Mihai Surdeanu; John Bauer; Jenny Rose Finkel; Steven Bethard; David Mc-Closky"}, {"ref_id": "b17", "title": "Lawyers are dishonest? quantifying representational harms in commonsense knowledge resources", "journal": "", "year": "2021", "authors": "Ninareh Mehrabi; Pei Zhou; Fred Morstatter; Jay Pujara; Xiang Ren; Aram Galstyan"}, {"ref_id": "b18", "title": "Anytime bottom-up rule learning for knowledge graph completion", "journal": "", "year": "2019-08-10", "authors": "Christian Meilicke; Daniel Melisachew Wudage Chekol; Heiner Ruffinelli;  Stuckenschmidt"}, {"ref_id": "b19", "title": "Wordnet: A lexical database for english", "journal": "Commun. ACM", "year": "1995", "authors": "George A Miller"}, {"ref_id": "b20", "title": "Neural-symbolic commonsense reasoner with relation predictors", "journal": "Association for Computational Linguistics", "year": "2021-08-01", "authors": "Farhad Moghimifar; Lizhen Qu; Terry Yue Zhuo; Gholamreza Haffari; Mahsa Baktashmotlagh"}, {"ref_id": "b21", "title": "GLUCOSE: generalized and contextualized story explanations", "journal": "Association for Computational Linguistics", "year": "2020-11-16", "authors": "Nasrin Mostafazadeh; Aditya Kalyanpur; Lori Moon; David W Buchanan; Lauren Berkowitz; Or Biran; Jennifer Chu-Carroll"}, {"ref_id": "b22", "title": "Rnnlogic: Learning logic rules for reasoning on knowledge graphs", "journal": "", "year": "2021-05-03", "authors": "Meng Qu; Junkun Chen; A C Louis-Pascal; Yoshua Xhonneux; Jian Bengio;  Tang"}, {"ref_id": "b23", "title": "Commonsense knowledge base completion and generation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Itsumi Saito; Kyosuke Nishida; Hisako Asano; Junji Tomita"}, {"ref_id": "b24", "title": "Atomic: An atlas of machine commonsense for ifthen reasoning", "journal": "", "year": "2019", "authors": "Maarten Sap; Emily Ronan Le Bras; Chandra Allaway; Nicholas Bhagavatula; Hannah Lourie; Brendan Rashkin;  Roof; A Noah; Yejin Smith;  Choi"}, {"ref_id": "b25", "title": "End-to-end structureaware convolutional networks for knowledge base completion", "journal": "AAAI Press", "year": "2019-01-27", "authors": "Chao Shang; Yun Tang; Jing Huang; Jinbo Bi; Xiaodong He; Bowen Zhou"}, {"ref_id": "b26", "title": "Reasoning with neural tensor networks for knowledge base completion", "journal": "", "year": "2013-12-05", "authors": "Richard Socher; Danqi Chen; Christopher D Manning; Andrew Y Ng"}, {"ref_id": "b27", "title": "Conceptnet 5.5: An open multilingual graph of general knowledge", "journal": "AAAI Press", "year": "2017-02-04", "authors": "Robyn Speer; Joshua Chin; Catherine Havasi"}, {"ref_id": "b28", "title": "Inductive learning on commonsense knowledge graph completion", "journal": "IEEE", "year": "2021-07-18", "authors": "Bin Wang; Guangtao Wang; Jing Huang; Jiaxuan You; Jure Leskovec; C.-C. Jay Kuo"}, {"ref_id": "b29", "title": "KSAM: infusing multi-source knowledge into dialogue generation via knowledge source aware multi-head decoding", "journal": "", "year": "2022-05-22", "authors": "Sixing Wu; Ying Li; Dawei Zhang; Zhonghai Wu"}, {"ref_id": "b30", "title": "KG-BERT: BERT for knowledge graph completion", "journal": "", "year": "2019", "authors": "Liang Yao; Chengsheng Mao; Yuan Luo"}, {"ref_id": "b31", "title": "Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts", "journal": "", "year": "2022-05-22", "authors": "Wenhao Yu; Chenguang Zhu; Lianhui Qin; Zhihan Zhang; Tong Zhao; Meng Jiang"}, {"ref_id": "b32", "title": "Transomcs: From linguistic graphs to commonsense knowledge", "journal": "", "year": "2020", "authors": "Hongming Zhang; Daniel Khashabi; Yangqiu Song; Dan Roth"}, {"ref_id": "b33", "title": "ASER: A largescale eventuality knowledge graph", "journal": "ACM / IW3C2", "year": "2020-04-20", "authors": "Hongming Zhang; Xin Liu; Haojie Pan; Yangqiu Song; Cane Wing; -Ki Leung"}, {"ref_id": "b34", "title": "GMH: A general multi-hop reasoning model for KG completion", "journal": "Association for Computational Linguistics", "year": "2021-11-11", "authors": "Yao Zhang; Hongru Liang; Adam Jatowt; Wenqiang Lei; Xin Wei; Ning Jiang; Zhenglu Yang"}, {"ref_id": "b35", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b36", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b37", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b38", "title": "crowdworkers) or research with human participants? In Section 3 and 4", "journal": "", "year": "", "authors": ""}, {"ref_id": "b39", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? We perform simple human annotation and evaluation", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b40", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence", "journal": "", "year": "", "authors": ""}, {"ref_id": "b41", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b42", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b43", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "22", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 Figure 2 :22Figure2illustrates the procedure of constructing Dense-ATOMIC, consisting of three main steps:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: The detailed structure of Rel-CSKGC.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u2212\u2212 \u2192 X jumps up and down with joy oP ersona \u2212 \u2212\u2212\u2212\u2212\u2212 \u2192 Y is pleased", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "you describe the limitations of your work? In Limitations section. A2. Did you discuss any potential risks of your work? Not applicable. Left blank. A3. Do the abstract and introduction summarize the paper's main claims? In Abstract section and section 1, respectively. A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? In section 2, Appendix B. B1. Did you cite the creators of artifacts you used? In section 2.1, Appendix B.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "'s split of ATOMIC, we randomly sample negative triplets from the training split with negative sampling strategies introduced in Section 2.2. We combine sampled negative triplets and the training split to construct the training set for Rel-CSKGC. The statistic of the training set is illustrated in Table1.2    ", "figure_data": "ATOMIC Rand. Neg. Samples Per. Neg. Samples463,2641,890,350756,140"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Statistics of the training set for Rel-CSKGC.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Statistics of the annotated subgraph. Intra and Inter indicate the intra-and inter-cluster, respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Rel-CSKGC vs. Relation Prediction methods on Precision. Intra and Inter indicate the result of the intra-and inter-cluster, respectively.", "figure_data": "Method SynLink Adapt InductivE Adapt Rel-CSKGC# Predicted # Meaningful Proportion 133 93 0.70 132 106 0.80 174 152 0.87"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Rel-CSKGC vs. Translation Based methods.    ", "figure_data": "Translation Based Methods After carefully tun-ing the threshold based on the strategy in Sec-tion 3.2.2, Rel-CSKGC, SynLink, and InductivE"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "ATOMIC vs. Dense-ATOMIC on the number of events and multi-hop paths.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "COMET vs. COMET ours . PPL and DS indicate perplexity and diversity score, respectively.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Examples of multi-hop paths randomly sampled from Dense-ATOMIC.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Scores of tail events generated with one-hop and multi-hop paths.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "[H; T ] = RoBERTa([h; t])(1)", "formula_coordinates": [4.0, 356.86, 149.23, 168.3, 13.15]}, {"formula_id": "formula_1", "formula_text": "o = sigmoid(W t e <s> )+softmax(W c (e h , e t )) (2)", "formula_coordinates": [4.0, 311.6, 263.6, 213.56, 13.85]}], "doi": "10.18653/v1/p19-1470"}