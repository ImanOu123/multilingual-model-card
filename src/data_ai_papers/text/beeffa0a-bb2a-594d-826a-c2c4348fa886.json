{"title": "Query2Prod2Vec Grounded Word Embeddings for eCommerce", "authors": "Federico Bianchi; Jacopo Tagliabue; Bingqing Yu", "pub_date": "", "abstract": "We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data efficiency for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.", "sections": [{"heading": "Introduction", "text": "The eCommerce market reached in recent years an unprecedented scale: in 2020, 3.9 trillion dollars were spent globally in online retail (Cramer-Flood, 2020). While shoppers make significant use of search functionalities, improving their experience is a never-ending quest (Econsultancy, 2020), as outside of few retail giants users complain about sub-optimal performances (Baymard Institute, 2020). As the technology behind the industry increases in sophistication, neural architectures are gradually becoming more common (Tsagkias et al., 2020) and, with them, the need for accurate word embeddings for Information Retrieval (IR) and downstream Natural Language Processing (NLP) tasks .\nUnfortunately, the success of standard and contextual embeddings from the NLP literature (Mikolov et al., 2013a;Devlin et al., 2019) could not be immediately translated to the product search scenario, due to some peculiar challenges , such as short text, industry-specific jargon (Bai et al., 2018), lowresource languages; moreover, specific embedding strategies have often been developed in the context of high-traffic websites (Grbovic et al., 2016), which limit their applicability in many practical scenarios. In this work, we propose a sample efficient word embedding method for IR in eCommerce, and benchmark it against SOTA models over industry data provided by partnering shops. We summarize our contributions as follows:\n1. we propose a method to learn dense representations of words for eCommerce: we name our method Query2Prod2Vec, as the mapping between words and the latent space is mediated by the product domain;\n2. we evaluate the lexical representations learned by Query2Prod2Vec on an analogy task against SOTA models in NLP and IR; benchmarks are run on two independent shops, differing in traffic, industry and catalog size;\n3. we detail a procedure to generate synthetic embeddings, which allow us to tackle the \"cold start\" challenge;\n4. we release our implementations, to help the community with the replication of our findings on other shops 1 .\nWhile perhaps not fundamental to its industry significance, it is important to remark that grounded lexical learning is well aligned with theoretical considerations on meaning in recent (and less recent) literature (Bender and Koller, 2020;Bisk et al., 2020;Montague, 1974).", "publication_ref": ["b9", "b12", "b20", "b11", "b0", "b14", "b2", "b6", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Embeddings for Product Search: an Industry Perspective", "text": "In product search, when the shopper issues a query (e.g. \"sneakers\") on a shop, the shop search engine returns a list of K products matching the query intent and possibly some contextual factor -the shopper at that point may either leave the website, or click on n products to further explore the offering and eventually make a purchase. Unlike web search, which is exclusively performed at massive scale, product search is a problem that both big and small retailers have to solve: while word embeddings have revolutionized many areas of NLP (Mikolov et al., 2013a), word embeddings for product queries are especially challenging to obtain at scale, when considering the huge variety of use cases in the overall eCommerce industry. In particular, based on industry data and first-hand experience with dozens of shops in our network, we identify four constraints for effective word embeddings in eCommerce:\n1. Short text. Most product queries are very short -60% of all queries in our dataset are one-word queries, > 80% are two words or less; the advantage of contextualized embeddings may therefore be limited, while lexical vectors are fundamental for downstream NLP tasks Bianchi et al., 2020a). For this reason, the current work specifically addresses the quality of word embeddings 2 .\n2. Low-resource languages. Even shops that have the majority of their traffic on English domain typically have smaller shops in lowresource languages.\n3. Data sparsity. In Shop X below, only 9% of all shopping sessions have a search interaction 3 . Search sparsity, coupled with verticalspecific jargon and the usual long tail of search queries, makes data-hungry models unlikely to succeed for most shops.\n2 Irrespectively of how the lexical vectors are computed, query embeddings can be easily recovered with the usual techniques (e.g. sum or average word embeddings ): as we mention in the concluding remarks, investigating compositionality is an important part of our overall research agenda.\n3 This is a common trait verified across industries and sizes: among dozens of shops in our network, 30% is the highest search vs no-search session ratio; Shop Y below is around 29%.\n4. Computational capacity. The majority of the market has the necessity to strike a good trade-off between quality of lexical representations and the cost of training and deploying models, both as hardware expenses and as additional maintenance/training costs.\nThe embedding strategy we propose -Query2Prod2Vec -has been designed to allow efficient learning of word embeddings for product queries. Our findings are useful to a wide range of practitioners: large shops launching in new languages/countries, mid-and-small shops transitioning to dense IR architectures and the raising wave of multi-tenant players 4 : as A.I. providers grow by deploying their solutions on multiple shops, \"cold start\" scenarios are an important challenge to the viability of their business model.", "publication_ref": ["b20", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The literature on learning representations for lexical items in NLP is vast and growing fast; as an overview of classical methods, Baroni et al. (2014) benchmarks several count-based and neural techniques (Landauer and Dumais, 1997;Mikolov et al., 2013b); recently, context-aware embeddings (Peters et al., 2018;Devlin et al., 2019) have demonstrated state-of-the-art performances in several semantic tasks (Rogers et al., 2020;Nozza et al., 2020), including document-based search (Nogueira et al., 2020), in which target entities are long documents, instead of product (Craswell et al., 2020). To address IR-specific challenges, other embedding strategies have been proposed: Search2Vec (Grbovic et al., 2016) uses interactions with ads and pages as context in the typical context-target setting of skip-gram models (Mikolov et al., 2013b); QueryNGram2Vec (Bai et al., 2018) additionally learns embeddings for ngrams of word appearing in queries to better cover the long tail. The idea of using vectors (from images) as an aid to query representation has also been suggested as a heuristic device by , in the context of personalized language models; this work is the first to our knowledge to benchmark embeddings on lexical semantics (not tuned for domain-specific tasks), and investigate sample efficiency for small-data contexts.", "publication_ref": ["b1", "b17", "b21", "b26", "b11", "b27", "b25", "b24", "b10", "b14", "b21", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Query2Prod2Vec", "text": "In Query2Prod2Vec, the representation for a query q is built through the representation of the objects that q refers to. Consider a typical shopperengine interaction in the context of product search: the shopper issues a query, e.g. \"shoes\", the engine replies with a noisy set of potential referents, e.g. pairs of shoes from the shop inventory, among which the shopper may select relevant items. Hence, this dynamics is reminiscent of a cooperative language game (Lewis, 1969), in which shoppers give noisy feedback to the search engine on the meaning of the queries. A full specification of Query2Prod2Vec therefore involves a representation of the target domain of reference (i.e. products in a digital shop) and a denotation function.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Building a Target Domain", "text": "We represent products in a target shop through a prod2vec model built with anonymized shopping sessions containing user-product interactions. Embeddings are trained by solving the same optimization problem as in classical word2vec (Mikolov et al., 2013a): word2vec becomes prod2vec by substituting words in a sentence with products viewed in a shopping session (Mu et al., 2018). The utility of prod2vec is independently justified (Grbovic et al., 2015; and, more importantly, the referential approach leverages the abundance of browsing-based interactions, as compared to search-based interactions: by learning product embeddings from abundant behavioral data first, we sidestep a major obstacle to reliable word representation in eCommerce. Hyperparameter optimization follows the guidelines in Bianchi et al. (2020a), with a total of 26,057 (Shop X) and 84,575 (Shop Y) product embeddings available for downstream processing 5 .", "publication_ref": ["b20", "b23", "b15", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Embeddings", "text": "The fundamental intuition of Query2Prod2Vec is treating clicks after q as a noisy feedback mapping q to a portion of the latent product space. In particular, we compute the embedding for q by averaging the product embeddings of all products clicked after it, using frequency as a weighting factor (i.e. products clicked often contribute more). The model has one free parameter, rank, which controls how many embeddings are used to build the representation for q: if rank=k, only the k most clicked products after q are used. The results in Table 1 are obtained with rank=5, as we leave to future work to investigate the role of this parameter.\nThe lack of large-scale search logs in the case of new deployments is a severe issue for successful training. The referential nature of Query2Prod2Vec provides a fundamental competitive advantage over models building embeddings from past linguistic behavior only, as synthetic embeddings can be generated as long as cheap session data is available to obtain an initial prod2vec model. As detailed in the ensuing section, the process happens in two stages, event generation and embeddings creation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Creating Synthetic Embeddings", "text": "The procedure to create synthetic embeddings is detailed in Algorithm 1: it takes as input a list of words, a pre-defined number of sampling iterations, a popularity distribution over products 6 , and it returns a list of synthetic search events, that is, a mapping between words and lists of products \"clicked\". Simulating the search event can be achieved through the existing search engine, as, from a practical standpoint, some IR system must already be in place given the use case under consideration. To avoid over-relying on the quality of IR and prove the robustness of the method, all the simulations below are not performed with the actual production API, but with a custom-built inverted index over product meta-data, with a simple TF-IDF weighting and Boolean search.\nFor the second stage, we can treat the synthetic click events produced by Algorithm 1 as a dropin replacement for user-generated events -that is, for any query q, we calculate an embedding by averaging the product embeddings of the relevant products, weighted by frequency 7 . Putting the two stages together, Query2Prod2Vec can not only produce reliable query embeddings based on historical data, but also learn approximate embeddings for a large vocabulary before being exposed Algorithm 1: Generation of synthetic click events.\nData: a list of words W , a pre-defined number N of simulations per word, a distribution D over products. Result: A dataset of synthetic clicked events: E E \u2190 empty mapping; foreach word w in W do product_list \u2190 Search(w); for i = 1 to N do p \u2190 Sample (product_list, D); append the entry (w, p) to E; end end return E to any search interaction: in Section 7 we report the performance of Query2Prod2Vec when using only synthetic embeddings 8 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Baselines", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "Following best practices in the multi-tenant literature , we benchmark all models on different shops to test their robustness. In particular, we obtained catalog data, search logs and anonymized shopping sessions from two partnering shops, Shop X and Shop Y: Shop X is a sport apparel shop with Alexa ranking of approximately 200k, representing a prototypical shop in the middle of the long tail; Shop Y is a home improvement shop with Alexa ranking of approximately 10k, representing an intermediate size between Shop X and public companies in the space. Linguistic data is in Italian for both shops, and training is done on random sessions from the period June-October 2019: after sampling, removal of bot-like sessions and pre-processing, we are left with 722,479 sessions for Shop X, and 1,986,452 sessions for Shop Y.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "We leverage the unique opportunity to join catalog data, search logs and shopping sessions to extensively benchmark Query2Prod2Vec against a variety of methods from NLP and IR.\n\u2022 Word2Vec and FastText. We train a CBOW (Mikolov et al., 2013a) and a FastText model (Bojanowski et al., 2017) over product descriptions in the catalog;\n\u2022 UmBERTo. We use RoBERTa trained on Italian data -UmBERTo 9 . The s embedding of the last layer of the architecture is the query embedding;\n\u2022 Search2Vec. We implement the skip-gram model from Grbovic et al. (2016), by feeding the model with sessions composed of search queries and user clicks. Following the original model, we also train a time-sensitive variant, in which time between actions is used to weight query-click pairs differently;\n\u2022 Query2Vec. We implement a different context-target model, inspired by Egg ( 2019): embeddings are learned by the model when it tries to predict a (purchased or clicked) item starting from a query;\n\u2022 QueryNGram2Vec. We implement the model from Bai et al. (2018). Besides learning representations through a skip-gram model as in Grbovic et al. (2016), the model learns the embeddings of unigrams to help cover the long tail for which no direct embedding is available.\nTo guarantee a fair comparison, all models are trained on the same sessions. For all baselines, we follow the same hyperparameters found in the cited works: the dimension of query embedding vectors is set to 50, except that 768-dimensional vectors are used for UmBERTo, as provided by the pre-trained model.\nAs discussed in Section 1, a distinguishing feature of Query2Prod2Vec is grounding, that is, the relation between words and an external domainin this case, products. It is therefore interesting not only to assess a possible quantitative gap in the quality of the representations produced by the baseline models, but also to remark the qualitative difference at the core of the proposed method: if words are about something, pure co-occurrence patterns may be capturing only fragments of lexical meaning (Bianchi et al., 2021).", "publication_ref": ["b20", "b7", "b14", "b0", "b14", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Solving Analogies in eCommerce", "text": "As discussed in Section 2, we consider evaluation tasks focused on word meaning, without using product-based similarity (as that would implicitly and unfairly favor referential embeddings). Analogy-based tasks (Mikolov et al., 2013a) are a popular choice to measure semantic accuracy of embeddings, where a model is asked to fill templates like man : king = woman : ?; however, preparing analogies for digital shops presents non trivial challenges for human annotators: these would in fact need to know both the language and the underlying space (\"air max\" is closer to \"nike\" than to \"adidas\"), with the additional complication that many candidates may not have \"determinate\" answers (e.g. if Adidas is to Gazelle, then Nike is to what exactly?). In building our testing framework, we keep the intuition that analogies are an effective way to test for lexical meaning and the assumption that human-level concepts should be our ground truth: in particular, we programmatically produce analogies by leveraging existing human labelling, as indirectly provided by the merchandisers who built product catalogs 10 .", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Test Set Preparation", "text": "We extract words from the merchandising taxonomy of the target shops, focusing on three most frequent fields in query logs: product type, brand and sport activity for Shop X; product type, brand and part of the house for Shop Y. Our goal is to go from taxonomy to analogies, that is, showing how for each pair of taxonomy types (e.g. brand : sport), we can produce two pairs of tokens (Wilson : tennis, Cressi : scubadiving), and create two analogies: b1 : s1 = b2 : ? (target: s2) and b2: s2 = b1 : ? (target: s1) for testing purposes. For each type in a pair (e.g. brand : sport), we repeat the following for all possible values of brand (e.g. \"Wilson\", \"Nike\") -given a brand B:\n1. we loop over the catalog and record all values of sport, along with their frequency, for the products made by B. For example, for B = N ike, the distribution may be: {\"soccer\": 10, \"basketball\": 8, \"scubadiving\": 0 }; for B = W ilson, it may be: {\"tennis\": 8};\n10 It is important to note that this categorization is done by product experts for navigation and inventory purposes: all product labels are produced independently from any NLP consideration.\n2. we calculate the Gini coefficient (Catalano et al., 2009) over the distribution on the values of sport and choose a conservative Gini threshold, i.e. 75th percentile: the goal of this threshold is to avoid \"undetermined\" analogies, such as Adidas : Gazelle = Nike : ?.\nThe intuition behind the use of a dispersion measure is that product analogies are harder if the relevant label is found across a variety of products 11 .\nWith all the Gini coefficients and a chosen threshold, we are now ready to generate the analogies, by repeating the following for all values of brandgiven a brand B we can repeat the following sampling process K times (K = 10 for our experiments):\n1. if B's Gini value for its distribution of sport labels is below our chosen threshold, we skip B; if B's value is above, we associate to B its most frequent sport value, e.g. Wilson : tennis. This is the source pair of the analogy; to generate a target pair, we sample randomly a brand C with high Gini together with its most frequent value, e.g. Atomic : skiing;\n2. we add to the final test set two analogies: Wilson : tennis = Atomic : ?, and Atomic : skiing = Wilson : ?.\nThe procedure is designed to generate test examples conservatively, but of fairly high quality, as for example Garmin : watches = Arena : bathing cap (the analogy relates two brands which sell only one type of items), or racket : tennis = bathing cap : indoor swimming (the analogy relates \"tools\" that are needed in two activities). A total of 1208 and 606 test analogies are used for the analogy task (AT) for, respectively, Shop X and Shop Y: we benchmark all models by reporting Hit Rate at different cutoffs (Vasile et al., 2016), and we also report how many analogies are covered by the lexicon learned by the models (coverage is the ratio of analogies for which all embeddings are available in the relevant space). data) has the best performance 12 , while maintaining a very competitive coverage. More importantly, following our considerations in Section 2, results confirm that producing competitive embeddings on shops with different constraints is a challenging task for existing techniques, as models tend to either rely on specific query distribution (e.g. Search2Vec (time)), or the availability of linguistic and catalog resources with good coverage (e.g. Word2Vec). Query2Prod2Vec is the only model performing with comparable quality in the two scenarios, further strengthening the methodological importance of running benchmarks on more than one shop if findings are to be trusted by a large group of practitioners.", "publication_ref": ["b8", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sample Efficiency and User Studies", "text": "To investigate sample efficiency, we run two further experiments on Shop X: first, we run AT giving only 1/3 of the original data to Query2Prod2Vec (both for the prod2vec space, and for the denotation). The small-dataset version of Query2Prod2Vec still outperforms all other full-dataset models in Table 1 (HR@5,10 = 0.276 / 0.380). Second, we train a Query2Prod2Vec model only with simulated data produced as explained in Section 4 -that is, with zero data from real search logs. The entirely simulated Query2Prod2Vec shows performance competitive with the small-dataset version (HR@5,10 = 0.259 / 0.363) 13 , outperforming all baselines. As a further independent check, we supplement AT with a small semantic similarity task (ST) 12 HR@20 was also computed, but omitted for brevity as it confirmed the general trend. 13 A similar result was obtained on Shop Y, and it is omitted for brevity.\non Shop X 14 : two native speakers are asked to solve a small set (46) of manually curated questions in the form: \"Given the word Nike, which is the most similar, Adidas or Wilson?\". ST is meant to (partially) capture how much the embedding spaces align with lexical intuitions of generic speakers, independently of the product search dynamics. Table 1 reports results treating human ratings as ground truth and using cosine similarity on the learned embeddings for all models 15 . Query2Prod2Vec outperforms all other methods, further suggesting that the representations learned through referential information capture some aspects of lexical knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Computational Requirements", "text": "As stressed in Section 2, accuracy and resources form a natural trade-off for industry practitioners. Therefore, it is important to highlight that, our model is not just more accurate, but significantly more efficient to train: the best performing Query2Prod2Vec takes 30 minutes (CPU only) to be completed for the larger Shop Y, while other competitive models such as Search2Vec(time) and QueryNGram2Vec require 2 to 4 hours 16 . Being able to quickly generate many models allows for cost-effective analysis and optimization; moreover, infrastructure cost is heavily related to ethical and social issues on energy consumption in NLP (Strubell et al., 2019).\n14 Shop X is chosen since it is easier to find speakers familiar with sport apparel than DIY items. 15 Inter-rater agreement was substantial, with Cohen Kappa Score=0.67 (McHugh, 2012). 16 Training is performed on a Tesla V100 16GB GPU. As a back of the envelope calculation, training QueryNGram2Vec on a AWS p3 large instance costs around 12 USD, while a standard CPU container for Query2Prod2Vec costs less than 1 USD.", "publication_ref": ["b28", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this work, we learned reference-based word embeddings for product search: Query2Prod2Vec significantly outperforms other embedding strategies on lexical tasks, and consistently provides good performance in small-data and zero-data scenarios, with the help of synthetic embeddings. In future work, we will extend our analysis to i) specific IR tasks, within the recent paradigm of the dual encoder model (Karpukhin et al., 2020), and ii) compositional tasks, trying a systematic replication of the practical success obtained by  through image-based heuristics.\nWhen looking at models like Query2Prod2Vec in the larger industry landscape, we hope our methodology can help the field broaden its horizons: while retail giants indubitably played a major role in moving eCommerce use cases to the center of NLP research, finding solutions that address a larger portion of the market is not just practically important, but also an exciting agenda of its own 17 .", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "Coveo collects anonymized user data when providing its business services in full compliance with existing legislation (e.g. GDPR). The training dataset used for all models employs anonymous UUIDs to label events and sessions and, as such, it does not contain any information that can be linked to shoppers or physical entities; in particular, data is ingested through a standardized client-side integration, as specified in our public protocol.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We wish to thank Nadia Labai, Patrick John Chia, Andrea Polonioli, Ciro Greco and three anonymous reviewers for helpful comments on previous versions of this article. The authors wish to thank Coveo for the support and the computational resources used for the project. Federico Bianchi is a member of the Bocconi Institute for Data Science and Analytics (BIDSA) and the Data and Marketing Insights (DMI) unit.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Scalable query n-gram embedding for improving matching and relevance in sponsored search", "journal": "ACM", "year": "2018", "authors": "Xiao Bai; Erik Ordentlich; Yuanyuan Zhang; Andy Feng; Adwait Ratnaparkhi; Reena Somvanshi; Aldi Tjahjadi"}, {"ref_id": "b1", "title": "Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Marco Baroni; Georgiana Dinu; Germ\u00e1n Kruszewski"}, {"ref_id": "b2", "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data", "journal": "", "year": "2020", "authors": "Emily M Bender; Alexander Koller"}, {"ref_id": "b3", "title": "Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction", "journal": "", "year": "2021", "authors": "Federico Bianchi; Ciro Greco; Jacopo Tagliabue"}, {"ref_id": "b4", "title": "Fantastic embeddings and how to align them: Zero-shot inference in a multi-shop scenario", "journal": "", "year": "2020", "authors": "Federico Bianchi; Jacopo Tagliabue; Bingqing Yu; Luca Bigon; Ciro Greco"}, {"ref_id": "b5", "title": "Bert goes shopping: Comparing distributional models for product representations", "journal": "", "year": "2020", "authors": "Federico Bianchi; Bingqing Yu; Jacopo Tagliabue"}, {"ref_id": "b6", "title": "Experience grounds language", "journal": "", "year": "2020", "authors": "Yonatan Bisk; Ari Holtzman; Jesse Thomason; Jacob Andreas; Yoshua Bengio; Joyce Chai; Mirella Lapata; Angeliki Lazaridou; Jonathan May; Aleksandr Nisnevich; Nicolas Pinto; Joseph Turian"}, {"ref_id": "b7", "title": "Enriching word vectors with subword information", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"ref_id": "b8", "title": "Measuring resource inequality: The gini coefficient", "journal": "", "year": "2009", "authors": "Michael Catalano; Tanya Leise; Thomas Pfaff"}, {"ref_id": "b9", "title": "Global Ecommerce 2020. Ecommerce Decelerates amid Global Retail Contraction but Remains a Bright Spot", "journal": "", "year": "2020", "authors": "Ethan Cramer-Flood"}, {"ref_id": "b10", "title": "Overview of the trec 2019 deep learning track. ArXiv, abs", "journal": "", "year": "2003", "authors": "Nick Craswell; E Bhaskar Mitra; Daniel Fernando Yilmaz; E Campos;  Voorhees"}, {"ref_id": "b11", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b12", "title": "Site search: retailers still have a lot to learn", "journal": "", "year": "2020", "authors": " Econsultancy"}, {"ref_id": "b13", "title": "Query2vec: Search query expansion with query embeddings", "journal": "", "year": "2019", "authors": "Alex Egg"}, {"ref_id": "b14", "title": "Scalable semantic matching of queries to ads in sponsored search advertising", "journal": "", "year": "2016", "authors": "Mihajlo Grbovic; Nemanja Djuric; Vladan Radosavljevic; Fabrizio Silvestri; Ricardo Baeza-Yates; Andrew Feng; Erik Ordentlich; Lee Yang; Gavin Owens"}, {"ref_id": "b15", "title": "E-commerce in your inbox: Product recommendations at scale", "journal": "", "year": "2015", "authors": "Mihajlo Grbovic; Vladan Radosavljevic; Nemanja Djuric; Narayan Bhamidipati; Jaikit Savla; Varun Bhagwan; Doug Sharp"}, {"ref_id": "b16", "title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"ref_id": "b17", "title": "A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "journal": "", "year": "1997", "authors": "K Thomas; Susan T Landauer;  Dumais"}, {"ref_id": "b18", "title": "", "journal": "", "year": "1969", "authors": "David Lewis"}, {"ref_id": "b19", "title": "Interrater reliability: The kappa statistic", "journal": "", "year": "2012", "authors": "Mary Mchugh"}, {"ref_id": "b20", "title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Gregory S Corrado; Jeffrey Dean"}, {"ref_id": "b21", "title": "Distributed representations of words and phrases and their compositionality", "journal": "Curran Associates Inc", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b22", "title": "English as a formal language", "journal": "Yale University Press", "year": "1974", "authors": "Richard Montague"}, {"ref_id": "b23", "title": "Revisiting skip-gram negative sampling model with regularization", "journal": "CoRR", "year": "2018", "authors": "Cun Mu; Guang Yang; Zheng Yan"}, {"ref_id": "b24", "title": "Document ranking with a pretrained sequence-to-sequence model", "journal": "", "year": "2020", "authors": "Rodrigo Nogueira; Zhiying Jiang; Jimmy Lin"}, {"ref_id": "b25", "title": "What the [MASK]? making sense of language-specific BERT models", "journal": "", "year": "2020", "authors": "Debora Nozza; Federico Bianchi; Dirk Hovy"}, {"ref_id": "b26", "title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b27", "title": "A primer in BERTology: What we know about how BERT works", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Anna Rumshisky"}, {"ref_id": "b28", "title": "Energy and policy considerations for deep learning in nlp", "journal": "", "year": "2019", "authors": "Emma Strubell; Ananya Ganesh; Andrew Mccallum"}, {"ref_id": "b29", "title": "Shopping in the multiverse: A counterfactual approach to insession attribution", "journal": "", "year": "2020", "authors": "Jacopo Tagliabue; Bingqing Yu"}, {"ref_id": "b30", "title": "How to grow a (product) tree: Personalized category suggestions for eCommerce type-ahead", "journal": "", "year": "", "authors": "Jacopo Tagliabue; Bingqing Yu; Marie Beaulieu"}, {"ref_id": "b31", "title": "The embeddings that came in from the cold: Improving vectors for new and rare products with content-based inference", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Jacopo Tagliabue; Bingqing Yu; Federico Bianchi"}, {"ref_id": "b32", "title": "Algolia finds $110m from accel and salesforce", "journal": "", "year": "2019", "authors": " Techcrunch"}, {"ref_id": "b33", "title": "Lucidworks raises $100m to expand in ai finds", "journal": "", "year": "2019", "authors": " Techcrunch"}, {"ref_id": "b34", "title": "Vanessa Murdock, and Maarten de Rijke. 2020. Challenges and research opportunities in ecommerce search and recommendations", "journal": "", "year": "", "authors": "Manos Tsagkias; Tracy Holloway King; Surya Kallumadi"}, {"ref_id": "b35", "title": "Meta-prod2vec: Product embeddings using side-information for recommendation", "journal": "", "year": "2016", "authors": "Flavian Vasile; Elena Smirnova; Alexis Conneau"}, {"ref_id": "b36", "title": "Blending search and discovery: Tag-based query refinement with contextual reinforcement learning", "journal": "", "year": "2020", "authors": "Bingqing Yu; Jacopo Tagliabue"}, {"ref_id": "b37", "title": "An image is worth a thousand features: Scalable product representations for in-session type-ahead personalization", "journal": "", "year": "2020", "authors": "Bingqing Yu; Jacopo Tagliabue; Ciro Greco; Federico Bianchi"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "reports model performance for the chosen cutoffs. Query2Prod2Vec (as trained on real", "figure_data": "ModelHR@5,10 for X HR@5,10 for Y CV (X/Y) Acc on STQuery2Prod2Vec (real data)0.332 / 0.4680.277 / 0.3760.965/0.9240.88Word2Vec0.206 / 0.2420.005 / 0.0090.47 / 0.030.68Query2Vec0.077 / 0.1130.065 / 0.1200.97 / 0.930.54QueryNGram2Vec0.071 / 0.1220.148 / 0.2160.99 / 0.920.82FastText0.068 / 0.1160.010 / 0.0120.52 / 0.030.57UmBERTo0.019 / 0.0420.030 / 0.1030.99 / 1.000.57Search2Vec (time)0.018 / 0.0250.232 / 0.3290.23 / 0.900.17Search2Vec0.016 / 0.0240.095 / 0.1500.23 / 0.900.17Table 1: Hit Rate (HR) and coverage (CV) for all models and two shops on AT; on the rightmost column, Accuracy(Acc) for all models on ST."}], "formulas": [], "doi": "10.1145/3219819.3219897"}