{"title": "A Linear-Time Kernel Goodness-of-Fit Test", "authors": "Wittawat Jitkrittum; Wenkai Xu; Zolt\u00e1n Szab\u00f3; Arthur Gretton", "pub_date": "2017-10-24", "abstract": "We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model. * Zolt\u00e1n Szab\u00f3's ORCID ID: 0000-0001-6183-7603. Arthur Gretton's ORCID ID: 0000-0003-3169-7624. 2 Briefly, [15] show that when an exponentiated quadratic kernel is used, a sequence of sets D may be constructed that does not correspond to any q, but for which the KSD nonetheless approaches zero. In a statistical testing setting, however, we assume identically distributed samples from q, and the issue does not arise.", "sections": [{"heading": "Introduction", "text": "The goal of goodness of fit testing is to determine how well a model density p(x) fits an observed sample D = {x i } n i=1 \u2282 X \u2286 R d from an unknown distribution q(x). This goal may be achieved via a hypothesis test, where the null hypothesis H 0 : p = q is tested against H 1 : p = q. The problem of testing goodness of fit has a long history in statistics [11], with a number of tests proposed for particular parametric models. Such tests can require space partitioning [18,3], which works poorly in high dimensions; or closed-form integrals under the model, which may be difficult to obtain, besides in certain special cases [2,5,30,26]. An alternative is to conduct a two-sample test using samples drawn from both p and q. This approach was taken by [23], using a test based on the (quadratic-time) Maximum Mean Discrepancy [16], however this does not take advantage of the known structure of p (quite apart from the increased computational cost of dealing with samples from p).\nMore recently, measures of discrepancy with respect to a model have been proposed based on Stein's method [21]. A Stein operator for p may be applied to a class of test functions, yielding functions that have zero expectation under p. Classes of test functions can include the W 2,\u221e Sobolev space [14], and reproducing kernel Hilbert spaces (RKHS) [25]. Statistical tests have been proposed by [9,22] based on classes of Stein transformed RKHS functions, where the test statistic is the norm of the smoothness-constrained function with largest expectation under q . We will refer to this statistic as the Kernel Stein Discrepancy (KSD). For consistent tests, it is sufficient to use C 0 -universal kernels [6,Definition 4.1], as shown by [9,Theorem 2.2], although inverse multiquadric kernels may be preferred if uniform tightness is required [15]. 2 The function class F d for the function f is chosen to be a unit-norm ball in a reproducing kernel Hilbert space (RKHS) in [9,22]. More precisely, let F be an RKHS associated with a positive definite kernel k : X \u00d7 X \u2192 R. Let \u03c6(x) = k(x, \u2022) denote a feature map of k so that k(x, x ) = \u03c6(x), \u03c6(x ) F . Assume that f i \u2208 F for all i = 1, . . . , d so that f \u2208 F \u00d7 \u2022 \u2022 \u2022 \u00d7 F := F d where F d is equipped with the standard inner product f , g\nF d := d i=1 f i , g i F .\nThe kernelized Stein operator T p studied in [9] is (T p f ) (x) := \u2202x is in F d . We note that the Stein operator presented in [22] is defined such that (T p f ) (x) \u2208 R d . This distinction is not crucial and leads to the same goodness-offit test. Under appropriate conditions, e.g. that lim x \u2192\u221e p(x)f i (x) = 0 for all i = 1, . . . , d, it can be shown using integration by parts that E x\u223cp (T p f )(x) = 0 for any f \u2208 F d [9, Lemma 5.1]. Based on the Stein operator, [9,22] define the kernelized Stein discrepancy as S p (q) := sup\nf F d \u22641 E x\u223cq f , \u03be p (x, \u2022) F d (a) = sup f F d \u22641 f , E x\u223cq \u03be p (x, \u2022) F d = g(\u2022) F d , (1)\nwhere at (a), \u03be p (x, \u2022) is Bochner integrable [28,Definition A.5.20] as long as E x\u223cq \u03be p (x, \u2022) F d < \u221e, and g(y) := E x\u223cq \u03be p (x, y) is what we refer to as the Stein witness function. The Stein witness function will play a crucial role in our new test statistic in Section 3. When a C 0 -universal kernel is used [6,Definition 4.1], and as long as E x\u223cq \u2207 x log p(x) \u2212 \u2207 x log q(x) 2 < \u221e, it can be shown that S p (q) = 0 if and only if p = q [9, Theorem 2.2].\nThe KSD S p (q) can be written as S 2 p (q) = E x\u223cq E x \u223cq h p (x, x ), where h p (x, y) := s p (x)s p (y)k(x, y) + s p (y)\u2207 x k(x, y) + s p (x)\u2207 y k(x, y) + d i=1 \u2202 2 k(x,y) \u2202xi\u2202yi , and s p (x) := \u2207 x log p(x) is a column vector. An unbiased empirical estimator of S 2 p (q), denoted by S 2 = 2 n(n\u22121) i<j h p (x i , x j ) [22, Eq. 14], is a degenerate U-statistic under H 0 . For the goodness-of-fit test, the rejection threshold can be computed by a bootstrap procedure. All these properties make S 2 a very flexible criterion to detect the discrepancy of p and q: in particular, it can be computed even if p is known only up to a normalization constant. Further studies on nonparametric Stein operators can be found in [25,14].\nLinear-Time Kernel Stein (LKS) Test Computation of S 2 costs O(n 2 ). To reduce this cost, a linear-time (i.e., O(n)) estimator based on an incomplete U-statistic is proposed in [22,Eq. 17], given by S\n2 l := 2 n n/2 i=1 h p (x 2i\u22121 , x 2i ),\nwhere we assume n is even for simplicity. Empirically [22] observed that the linear-time estimator performs much worse (in terms of test power) than the quadratic-time U-statistic estimator, agreeing with our findings presented in Section 5.\nThe idea is to use a real analytic kernel k which makes g 1 , . . . , g d real analytic. If g i = 0 is an analytic function, then the Lebesgue measure of the set of roots {x | g i (x) = 0} is zero [24]. This property suggests that one can evaluate g i at a finite set of locations V = {v 1 , . . . , v J }, drawn from a distribution with a density (w.r.t. the Lebesgue measure). If g i = 0, then almost surely g i (v 1 ), . . . , g i (v J ) will not be zero. This idea was successfully exploited in recently proposed linear-time tests of [8] and [19,20]. Our new test statistic based on this idea is called the Finite Set Stein Discrepancy (FSSD) and is given in Theorem 1. All proofs are given in the appendix.\nTheorem 1 (The Finite Set Stein Discrepancy (FSSD)). Let V = {v 1 , . . . , v J } \u2282 R d be random vectors drawn i.i.d. from a distribution \u03b7 which has a density. Let X be a connected open set in R d . Define FSSD 2 p (q) := 1 dJ d i=1 J j=1 g 2 i (v j ). Assume that 1) k : X \u00d7 X \u2192 R is C 0 - universal [6, Definition 4.1] and real analytic i.e., for all v \u2208 X , f (x) := k(x, v) is a real analytic function on X . 2) E x\u223cq E x \u223cq h p (x, x ) < \u221e. 3) E x\u223cq \u2207 x log p(x) \u2212 \u2207 x log q(x) 2 < \u221e. 4) lim x \u2192\u221e p(x)g(x) = 0.\nThen, for any J \u2265 1, \u03b7-almost surely FSSD 2 p (q) = 0 if and only if p = q.\nThis measure depends on a set of J test locations (or features) {v i } J i=1 used to evaluate the Stein witness function, where J is fixed and is typically small. A kernel which is C 0 -universal and real analytic is the Gaussian kernel k(x, y) = exp \u2212 [20,Proposition 3] for the result on analyticity). Throughout this work, we will assume all the conditions stated in Theorem 1, and consider only the Gaussian kernel. Besides the requirement that the kernel be real and analytic, the remaining conditions in Theorem 1 are the same as given in [9,Theorem 2.2]. Note that if the FSSD is to be employed in a setting otherwise than testing, for instance to obtain pseudo-samples converging to p, then stronger conditions may be needed [15].\nx\u2212y 2 2 2\u03c3 2 k (see", "publication_ref": ["b10", "b17", "b2", "b1", "b4", "b29", "b25", "b22", "b15", "b20", "b13", "b24", "b8", "b21", "b5", "b8", "b14", "b1", "b8", "b21", "b8", "b21", "b8", "b21", "b27", "b5", "b24", "b13", "b21", "b21", "b23", "b7", "b18", "b19", "b19", "b8", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Goodness-of-Fit Test with the FSSD Statistic", "text": "Given a significance level \u03b1 for the goodness-of-fit test, the test can be constructed so that H 0 is rejected when n FSSD 2 > T \u03b1 , where T \u03b1 is the rejection threshold (critical value), and FSSD 2 is an empirical estimate of FSSD 2 p (q). The threshold which guarantees that the type-I error (i.e., the probability of rejecting H 0 when it is true) is bounded above by \u03b1 is given by the (1 \u2212 \u03b1)-quantile of the null distribution i.e., the distribution of n FSSD 2 under H 0 . In the following, we start by giving the expression for FSSD 2 , and summarize its asymptotic distributions in Proposition 2.\nLet \u039e(x) \u2208 R d\u00d7J such that [\u039e(x)] i,j = \u03be p,i (x, v j )/ \u221a dJ. Define \u03c4 (x) := vec(\u039e(x)) \u2208 R dJ\nwhere vec(M) concatenates columns of the matrix M into a column vector. We note that \u03c4 (x) depends on the test locations\nV = {v j } J j=1 . Let \u2206(x, y) := \u03c4 (x) \u03c4 (y) = tr(\u039e(x) \u039e(y)). Given an i.i.d. sample {x i } n i=1 \u223c q, a consistent, unbiased estimator of FSSD 2 p (q) is FSSD 2 = 1 dJ d l=1 J m=1 1 n(n \u2212 1) n i=1 j =i \u03be p,l (xi, vm)\u03be p,l (xj, vm) = 2 n(n \u2212 1) i<j \u2206(xi, xj), (2)\nwhich is a one-sample second-order U-statistic with \u2206 as its U-statistic kernel [27,Section 5.1.1]. Being a U-statistic, its asymptotic distribution can easily be derived. We use\nd \u2192 to denote convergence in distribution. Proposition 2 (Asymptotic distributions of FSSD 2 ). Let Z 1 , . . . , Z dJ i.i.d. \u223c N (0, 1). Let \u00b5 := E x\u223cq [\u03c4 (x)], \u03a3 r := cov x\u223cr [\u03c4 (x)] \u2208 R dJ\u00d7dJ for r \u2208 {p, q}, and {\u03c9 i } dJ i=1 be the eigenvalues of \u03a3 p = E x\u223cp [\u03c4 (x)\u03c4 (x)]. Assume that E x\u223cq E y\u223cq \u2206 2 (x, y) < \u221e.\nThen, for any realization of V = {v j } J j=1 , the following statements hold.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Under H", "text": "0 : p = q, n FSSD 2 d \u2192 dJ i=1 (Z 2 i \u2212 1)\u03c9 i . 2. Under H 1 : p = q, if \u03c3 2 H1 := 4\u00b5 \u03a3 q \u00b5 > 0, then \u221a n( FSSD 2 \u2212 FSSD 2 ) d \u2192 N (0, \u03c3 2 H1 ).\nProof. Recognizing that (2) is a degenerate U-statistic, the results follow directly from [27, Section 5.5.1, 5.5.2].\nClaims 1 and 2 of Proposition 2 imply that under H 1 , the test power (i.e., the probability of correctly rejecting H 1 ) goes to 1 asymptotically, if the threshold T \u03b1 is defined as above. In practice, simulating from the asymptotic null distribution in Claim 1 can be challenging, since the plug-in estimator of \u03a3 p requires a sample from p, which is not available. A straightforward solution is to draw sample from p, either by assuming that p can be sampled easily or by using a Markov chain Monte Carlo (MCMC) method, although this adds an additional computational burden to the test procedure. A more subtle issue is that when dependent samples from p are used in obtaining the test threshold, the test may become more conservative than required for i.i.d. data [7]. An alternative approach is to use the plug-in estimate\u03a3 q instead of \u03a3 p . The covariance matrix\u03a3 q can be directly computed from the data. This is the approach we take. Theorem 3 guarantees that the replacement of the covariance in the computation of the asymptotic null distribution still yields a consistent test. We write P H1 for the distribution of n FSSD 2 under H 1 . Theorem 3.\nLet\u03a3 q := 1 n n i=1 \u03c4 (x i )\u03c4 (x i ) \u2212 [ 1 n n i=1 \u03c4 (x i )][ 1 n n j=1 \u03c4 (x j )] with {x i } n i=1 \u223c q. Suppose that the test threshold T \u03b1 is set to the (1\u2212\u03b1)-quantile of the distribution of dJ i=1 (Z 2 i \u22121)\u03bd i where {Z i } dJ i=1 i.i.d.\n\u223c N (0, 1), and\u03bd 1 , . . . ,\u03bd dJ are eigenvalues of\u03a3 q . Then, under H 0 , asymptotically the false positive rate is \u03b1. Under H 1 , for {v j } J j=1 drawn from a distribution with a density, the test power P H1 (n FSSD 2 > T \u03b1 ) \u2192 1 as n \u2192 \u221e. Remark 1. The proof of Theorem 3 relies on two facts. First, under H 0 ,\u03a3 q =\u03a3 p i.e., the plug-in estimate of \u03a3 p . Thus, under H 0 , the null distribution approximated with\u03a3 q is asymptotically correct, following the convergence of\u03a3 p to \u03a3 p . Second, the rejection threshold obtained from the approximated null distribution is asymptotically constant. Hence, under H 1 , claim 2 of Proposition 2 implies that n FSSD 2 d \u2192 \u221e as n \u2192 \u221e, and consequently P H1 (n FSSD 2 > T \u03b1 ) \u2192 1.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Optimizing the Test Parameters", "text": "Theorem 1 guarantees that the population quantity FSSD 2 = 0 if and only if p = q for any choice of {v i } J i=1 drawn from a distribution with a density. In practice, we are forced to rely on the empirical FSSD 2 , and some test locations will give a higher detection rate (i.e., test power) than others for finite n. Following the approaches of [17,20,19,29], we choose the test locations V = {v j } J j=1 and kernel bandwidth \u03c3 2 k so as to maximize the test power i.e., the probability of rejecting H 0 when it is false. We first give an approximate expression for the test power when n is large.\nProposition 4 (Approximate test power of n FSSD 2 ). Under H 1 , for large n and fixed r, the test power\nP H1 (n FSSD 2 > r) \u2248 1 \u2212 \u03a6 r \u221a n\u03c3 H 1 \u2212 \u221a n FSSD 2 \u03c3 H 1\n, where \u03a6 denotes the cumulative distribution function of the standard normal distribution, and \u03c3 H1 is defined in Proposition 2.\nProof. P H1 (n FSSD 2 > r) = P H1 ( FSSD 2 > r/n) = P H1 \u221a n FSSD 2 \u2212FSSD 2 \u03c3 H 1 > \u221a n r/n\u2212FSSD 2 \u03c3 H 1 .\nFor sufficiently large n, the alternative distribution is approximately normal as given in Proposition 2. It follows that\nP H1 (n FSSD 2 > r) \u2248 1 \u2212 \u03a6 r \u221a n\u03c3 H 1 \u2212 \u221a n FSSD 2 \u03c3 H 1 .\nLet \u03b6 := {V, \u03c3 2 k } be the collection of all tuning parameters. Assume that n is sufficiently large. Following the same argument as in [29], in\nr \u221a n\u03c3 H 1 \u2212 \u221a n FSSD 2 \u03c3 H 1\n, we observe that the first term\nr \u221a n\u03c3 H 1 = O(n \u22121/2 ) going to 0 as n \u2192 \u221e, while the second term \u221a n FSSD 2 \u03c3 H 1 = O(n 1/2 ), dominating\nthe first for large n. Thus, the best parameters that maximize the test power are given by \u03b6 * = arg max \u03b6 P H1 (n FSSD 2 > T \u03b1 ) \u2248 arg max \u03b6\nFSSD 2 \u03c3 H 1 .\nSince FSSD 2 and \u03c3 H1 are unknown, we divide the sample {x i } n i=1 into two disjoint training and test sets, and use the training set to compute FSSD 2 \u03c3 H 1 +\u03b3 , where a small regularization parameter \u03b3 > 0 is added for numerical stability. The goodness-of-fit test is performed on the test set to avoid overfitting. The idea of splitting the data into training and test sets to learn good features for hypothesis testing was successfully used in [29,20,19,17].\nTo find a local maximum of FSSD 2 \u03c3 H 1 +\u03b3 , we use gradient ascent for its simplicity. The initial points of {v i } J i=1 are set to random draws from a normal distribution fitted to the training data, a heuristic we found to perform well in practice. The objective is non-convex in general, reflecting many possible ways to capture the differences of p and q. The regularization parameter \u03b3 is not tuned, and is fixed to a small constant. Assume that\n\u2207 x log p(x) costs O(d 2 ) to evaluate. Computing \u2207 \u03b6 FSSD 2 \u03c3 H 1 +\u03b3 costs O(d 2 J 2 n). The computational complexity of n FSSD 2 and\u03c3 2 H1 is O(d 2 Jn).\nThus, finding a local optimum via gradient ascent is still linear-time, for a fixed maximum number of iterations. Computing\u03a3 q costs O(d 2 J 2 n), and obtaining all the eigenvalues of\u03a3 q costs O(d 3 J 3 ) (required only once). If the eigenvalues decay to zero sufficiently rapidly, one can approximate the asymptotic null distribution with only a few eigenvalues. The cost to obtain the largest few eigenvalues alone can be much smaller.\nRemark 2. Let\u03bc := 1 n n i=1 \u03c4 (x i ).\nIt is possible to normalize the FSSD statistic to get a new statistic\u03bb n := n\u03bc (\u03a3 q + \u03b3I) \u22121\u03bc where \u03b3 \u2265 0 is a regularization parameter that goes to 0 as n \u2192 \u221e. This was done in the case of the ME (mean embeddings) statistic of [8,19]. The asymptotic null distribution of this statistic takes the convenient form of \u03c7 2 (dJ) (independent of p and q), eliminating the need to obtain the eigenvalues of\u03a3 q . It turns out that the test power criterion for tuning the parameters in this case is the statistic\u03bb n itself. However, the optimization is computationally expensive as (\u03a3 q + \u03b3I) \u22121 (costing O(d 3 J 3 )) needs to be reevaluated in each gradient ascent iteration. This is not needed in our proposed FSSD statistic.", "publication_ref": ["b16", "b19", "b18", "b28", "b28", "b28", "b19", "b18", "b16", "b7", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Relative Efficiency and Bahadur Slope", "text": "Both the linear-time kernel Stein (LKS) and FSSD tests have the same computational cost of O(d 2 n), and are consistent, achieving maximum power of 1 as n \u2192 \u221e under H 1 . It is thus of theoretical interest to understand which test is more sensitive in detecting the differences of p and q. This can be quantified by the Bahadur slope of the test [1]. Two given tests can then be compared by computing the Bahadur efficiency (Theorem 7) which is given by the ratio of the slopes of the two tests. We note that the constructions and techniques in this section may be of independent interest, and can be generalised to other statistical testing settings.\nWe start by introducing the concept of Bahadur slope for a general test, following the presentation of [12,13]. Consider a hypothesis testing problem on a parameter \u03b8. The test proposes a null hypothesis H 0 : \u03b8 \u2208 \u0398 0 against the alternative hypothesis H 1 : \u03b8 \u2208 \u0398\\\u0398 0 , where \u0398, \u0398 0 are arbitrary sets. Let T n be a test statistic computed from a sample of size n, such that large values of T n provide an evidence to reject H 0 . We use plim to denote convergence in probability, and write E r for E x\u223cr E x \u223cr . Approximate Bahadur Slope (ABS) For \u03b8 0 \u2208 \u0398 0 , let the asymptotic null distribution of T n be F (t) = lim n\u2192\u221e P \u03b80 (T n < t), where we assume that the CDF (F ) is continuous and common to all \u03b8 0 \u2208 \u0398 0 . The continuity of F will be important later when Theorem 9 and 10 are used to compute the slopes of LKS and FSSD tests. Assume that there exists a continuous strictly increasing function\n\u03c1 : (0, \u221e) \u2192 (0, \u221e) such that lim n\u2192\u221e \u03c1(n) = \u221e, and that \u22122 plim n\u2192\u221e log(1\u2212F (Tn)) \u03c1(n)\n= c(\u03b8) where T n \u223c P \u03b8 , for some function c such that 0 < c(\u03b8 A ) < \u221e for \u03b8 A \u2208 \u0398\\\u0398 0 , and c(\u03b8 0 ) = 0 when \u03b8 0 \u2208 \u0398 0 . The function c(\u03b8) is known as the approximate Bahadur slope (ABS) of the sequence T n . The quantifier \"approximate\" comes from the use of the asymptotic null distribution instead of the exact one [1]. Intuitively the slope c(\u03b8 A ), for \u03b8 A \u2208 \u0398\\\u0398 0 , is the rate of convergence of p-values (i.e., 1 \u2212 F (T n )) to 0, as n increases. The higher the slope, the faster the p-value vanishes, and thus the lower the sample size required to reject H 0 under \u03b8 A .\nApproximate Bahadur Efficiency Given two sequences of test statistics, T\nn and T\n(2) n having the same \u03c1(n) (see Theorem 10), the approximate Bahadur efficiency of T\n(1)\nn relative to T (2) n is defined as E(\u03b8 A ) := c (1) (\u03b8 A )/c (2) (\u03b8 A ) for \u03b8 A \u2208 \u0398\\\u0398 0 . If E(\u03b8 A ) > 1, then T (1)\nn is asymptotically more efficient than T\n(2) n in the sense of Bahadur, for the particular problem specified by \u03b8 A \u2208 \u0398\\\u0398 0 . We now give approximate Bahadur slopes for two sequences of linear time test statistics: the proposed n FSSD 2 , and the LKS test statistic \u221a n S 2 l discussed in Section 2. \n\u221a n S 2 l is c (LKS) = 1 2 [Eqhp(x,x )] 2 Ep[h 2 p (x,x )]\n, where h p is the U-statistic kernel of the KSD statistic, and \u03c1(n) = n.\nTo make these results concrete, we consider the setting where p = N (0, 1) and q = N (\u00b5 q , 1). We assume that both tests use the Gaussian kernel k(x, y) = exp \u2212(x \u2212 y) 2 /2\u03c3 2 k , possibly with different bandwidths. We write \u03c3 2 k and \u03ba 2 for the FSSD and LKS bandwidths, respectively. Under these assumptions, the slopes given in Theorem 5 and Theorem 6 can be derived explicitly. The full expressions of the slopes are given in Proposition 12 and Proposition 13 (in the appendix). By [12,13] (recalled as Theorem 10 in the supplement), the approximate Bahadur efficiency can be computed by taking the ratio of the two slopes. The efficiency is given in Theorem 7.\nTheorem 7 (Efficiency in the Gaussian mean shift problem). Let E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) be the approximate Bahadur efficiency of n FSSD 2 relative to \u221a n S 2 l for the case where p = N (0, 1), q = N (\u00b5 q , 1), and J = 1 (i.e., one test location v for n FSSD 2 ). Fix \u03c3 2 k = 1 for n FSSD 2 . Then, for any \u00b5 q = 0, for some v \u2208 R, and for any \u03ba 2 > 0,\nwe have E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) > 2.\nWhen p = N (0, 1) and q = N (\u00b5 q , 1) for \u00b5 q = 0, Theorem 7 guarantees that our FSSD test is asymptotically at least twice as efficient as the LKS test in the Bahadur sense. We note that the efficiency is conservative in the sense that \u03c3 2 k = 1 regardless of \u00b5 q . Choosing \u03c3 2 k dependent on \u00b5 q will likely improve the efficiency further.", "publication_ref": ["b0", "b11", "b12", "b0", "b11", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we demonstrate the performance of the proposed test on a number of problems. The primary goal is to understand the conditions under which the test can perform well. Sensitivity to Local Differences We start by demonstrating that the test power objective FSSD 2 /\u03c3 H1 captures local differences of p and q, and that interpretable features v are found. Consider a one-dimensional problem in which p = N (0, 1) and q = Laplace(0, 1/ \u221a 2), a zero-mean Laplace distribution with scale parameter 1/ \u221a 2. These parameters are chosen so that p and q have the same mean and variance. Figure 1 plots the (rescaled) objective as a function of v. The objective illustrates that the best features (indicated by v * ) are at the most discriminative locations.\n\u22124 \u22122 0 2 4 v * v * p q\nTest Power We next investigate the power of different tests on two problems:\n1. Gaussian vs. Laplace:\np(x) = N (x|0, I d ) and q(x) = d i=1 Laplace(x i |0, 1/ \u221a 2)\nwhere the dimension d will be varied. The two distributions have the same mean and variance. The main characteristic of this problem is local differences of p and q (see Figure 1). Set n = 1000.", "publication_ref": [], "figure_ref": ["fig_18", "fig_18"], "table_ref": []}, {"heading": "Restricted Boltzmann Machine", "text": "(RBM): p(x) is the marginal distribution of p(x, h) = 1 Z exp x Bh + b x + c x \u2212 1 2 x 2 , where x \u2208 R d , h \u2208 {\u00b11} d h\nis a random vector of hidden variables, and Z is the normalization constant. The exact marginal density p(\nx) = h\u2208{\u22121,1} d h p(x, h) is intractable when d h is large, since it involves summing over 2 d h terms.\nRecall that the proposed test only requires the score function \u2207 x log p(x) (not the normalization constant), which can be computed in closed form in this case. In this problem, q is another RBM where entries of the matrix B are corrupted by Gaussian noise. This was the problem considered in [22]. We set d = 50 and d h = 40, and generate samples by n independent chains (i.e., n independent samples) of blocked Gibbs sampling with 2000 burn-in iterations.\nWe evaluate the following six kernel-based nonparametric tests with \u03b1 = 0.05, all using the Gaussian kernel. 1. FSSD-rand: the proposed FSSD test where the test locations set to random draws from a multivariate normal distribution fitted to the data. The kernel bandwidth is set by the commonly used median heuristic i.e., \u03c3 k = median({ x i \u2212 x j , i < j}). 2. FSSD-opt: the proposed FSSD test where both the test locations and the Gaussian bandwidth are optimized (Section 3.2). 3. KSD: the quadratic-time Kernel Stein Discrepancy test with the median heuristic. 4. LKS: the linear-time version of KSD with the median heuristic. 5. MMD-opt: the quadratic-time MMD two-sample test of [16] where the kernel bandwidth is optimized by grid search to maximize a power criterion as described in [29]. 6. ME-opt: the linear-time mean embeddings (ME) two-sample test of [19] where parameters are optimized. We draw n samples from p to run the two-sample tests (MMD-opt, ME-opt). For FSSD tests, we use J = 5 (see Section A for an investigation of test power as J varies). All tests with optimization use 20% of the sample size n for parameter tuning. Code is available at https://github.com/wittawatj/kernel-gof.\nFigure 2 shows the rejection rates of the six tests for the two problems, where each problem is repeated for 200 trials, resampling n points from q every time. In Figure 2a (Gaussian vs. Laplace), high performance of FSSD-opt indicates that the test performs well when there are local differences between p and q. Low performance of FSSD-rand emphasizes the importance of the optimization of FSSD-opt to pinpoint regions where p and q differ. The power of KSD quickly drops as the dimension increases, which can be understood since KSD is the RKHS norm of a function witnessing differences in p and q across the entire domain, including where these differences are small.\nWe next consider the case of RBMs. Following [22], b, c are independently drawn from the standard multivariate normal distribution, and entries of B \u2208 R 50\u00d740 are drawn with equal probability from {\u00b11}, in each trial. The density q represents another RBM having the same b, c as in p, and with all entries of B corrupted by independent zero-mean Gaussian noise with standard deviation \u03c3 per . Figure Figure 2: Rejection rates of the six tests. The proposed linear-time FSSD-opt has a comparable or higher test power in some cases than the quadratic-time KSD test.\n2b shows the test powers as \u03c3 per increases, for a fixed sample size n = 1000. We observe that all the tests have correct false positive rates (type-I errors) at roughly \u03b1 = 0.05 when there is no perturbation noise. In particular, the optimization in FSSD-opt does not increase false positive rate when H 0 holds. We see that the performance of the proposed FSSD-opt matches that of the quadratic-time KSD at all noise levels. MMD-opt and ME-opt perform far worse than the goodness-of-fit tests when the difference in p and q is small (\u03c3 per is low), since these tests simply represent p using samples, and do not take advantage of its structure.\nThe advantage of having O(n) runtime can be clearly seen when the problem is much harder, requiring larger sample sizes to tackle. Consider a similar problem on RBMs in which the parameter B \u2208 R 50\u00d740 in q is given by that of p, where only the first entry B 1,1 is perturbed by random N (0, 0.1 2 ) noise. The results are shown in Figure 2c where the sample size n is varied. We observe that the two two-sample tests fail to detect this subtle difference even with large sample size. The test powers of KSD and FSSD-opt are comparable when n is relatively small. It appears that KSD has higher test power than FSSD-opt in this case for large n. However, this moderate gain in the test power comes with an order of magnitude more computation. As shown in Figure 2d, the runtime of the KSD is much larger than that of FSSD-opt, especially at large n. In these problems, the performance of the new test (even without optimization) far exceeds that of the LKS test. Further simulation results can be found in Section B.  Interpretable Features In the final simulation, we demonstrate that the learned test locations are informative in visualising where the model does not fit the data well. We consider crime data from the Chicago Police Department, recording n = 11957 locations (latitude-longitude coordinates) of robbery events in Chicago in 2016. 3 We address the situation in which a model p for the robbery location density is given, and we wish to visualise where it fails to match the data. We fit a Gaussian mixture model (GMM) with the expectationmaximization algorithm to a subsample of 5500 points. We then test the model on a held-out test set of the same size to obtain proposed locations of relevant features v. Figure 3a shows the test robbery locations in purple, the model with two Gaussian components in wireframe, and the optimization objective for v as a grayscale contour plot (a red star indicates the maximum). We observe that the 2-component model is a poor fit to the data, particularly in the right tail areas of the data, as indicated in dark gray (i.e., the objective is high). Figure 3b shows a similar plot with a 10-component GMM. The additional components appear to have eliminated some mismatch in the right tail, however a discrepancy still exists in the left region. Here, the data have a sharp boundary on the right side following the geography of Chicago, and do not exhibit exponentially decaying Gaussian-like tails. We note that tests based on a learned feature located at the maximum both correctly reject H 0 .\nA Linear-Time Kernel Goodness-of-Fit Test Supplementary    The aim of this section is to explore the test power of the proposed FSSD test as a function of the number of test locations J. We consider three synthetic problems to illustrate three phenomena depending on the characteristic of the problem. We note that the test power may not necessarily increase with J. Figure 4 shows the rejection rate as a function of the test locations J in the three problems described below. In all cases, the sample size is set to n = 500, the train/test ratio is 50%, and the significance level is \u03b1 = 0.05. All rejection rates are computed with 200 trials with data sampled from the specified q in every trial.\nWe emphasize that the FSSD test is not designed to be used with large J, since doing so defeats the purpose of a linear-time test. We show in the main text in Section 2 that using J = 5 is typically sufficient in practice.\nSame Gaussian (SG): In this problem, p = q = N (0, I) in R 5 i.e., H 0 is true. It can be seen in Figure 4a that both the FSSD tests with and without optimization achieve correct false positive rate at roughly \u03b1 for all J considered. That is, under H 0 , the false rejection rate stays at the right level for all J.\nGaussian vs. Gaussian mixture model (GMM): This is a one-dimensional problem where p = N (0, 1) and q = 0.9N (0, 1) + 0.1N (0, 0.1 2 ) i.e., a mixture of two normal distributions. In this problem, p significantly differs from q in a small region around 0. This difference is created by the second mixture component. The characteristic of this problem is the local difference of p and q.\nFigure 4b indicates that using random test locations (FSSD-rand) does not give high test power. With optimization (FSSD-opt), the power increases as J increases up to a point, after which it slightly drops down and reaches a plateau. This behavior can be explained by noting that there is only a very small region around 0 to detect the difference. More signal can be gained with diminishing return by increasing the number of test locations around 0. When J is sufficiently high, the increase in the variance of the statistic outweighs the gain of the signal (recall that the variance of the null distribution increases with J). This increase in the variance reduces the test power.\nGaussian Variance Difference (GVD): This is a synthetic problem studied in [19] where p = N (0, I) and q = N (0, diag(2, 1 . . . , 1)) in R 5 . In this case, the region of difference between q and p exists only along the first dimension, and is broad.\nIn this case, Figure 4c shows that, with optimization, the power increases as the number of test locations increases. Unlike the case of Gaussian vs. GMM, the region of difference in this case is broad, and can accommodate more test locations to increase the signal. Despite this, we expect the test power to reach a plateau when J is sufficiently large for the same reason as described previously. In FSSD-rand, random test locations decrease the power due to the increase in the variance. Since only one dimension is relevant in determining the difference of p and q, it is unlikely that random locations are in the right region.   Recall that in Section 5, we evaluate the test powers of all the six tests on the RBM problem with d = 50 and d h = 40 (i.e., the number of latent variables). We aim to provide more evaluations in this section. In [22], the setting of d = 50 and d h = 10 was studied. Here we consider the same setting and show the results in Figure 5 where all other problem configurations are the same as in Section 5.\nIn Figure 5a, p is set to an RBM with parameters randomly drawn (described in Section 5), and q is the same RBM with all entries of the parameter B \u2208 R 50\u00d710 perturbed by independent Gaussian noise with standard deviation \u03c3 per , which varies from 0 to 0.06. We observe that the proposed FSSD-opt and KSD perform comparably. Figure 5b considers a hard problem where only the first entry B 1,1 is perturbed by noise following N (0, 0.1 2 ), and the sample size n is varied. In both of these two cases, the overall trend is similar to the case of d = 50 and d h = 40 presented in Figure 2.\nIt is interesting to note that FSSD-rand, relying on random test locations, performs comparably or even outperforms FSSD-opt in the case of d = 50, d h = 10, but not in the case of d = 50, d h = 40. This phenomenon can be explained as follows. In the case of d = 50, d h = 10, the data generated from the RBM tend to have simple structure (see Figure 6a). By contrast, data generated from the RBM with d = 50, d h = 40 (more latent variables) have larger variance, and can form a complicated structure (Figure 6b), requiring a careful choice of test locations to detect differences of p and q.\nWhen d = 50, d h = 10, however, random test locations given by random draws from a Gaussian distribution fitted to the data are sufficient to capture the simple structural difference. This explains why FSSD-rand can perform well in this case. Additionally, FSSD-rand also has 20% more testing data, since FSSD-opt uses 20% of the sample for parameter tuning.\nFigure 5d shows the rejection rates of all the tests as the sample size increases when p and q are the same RBM. All the tests have roughly the right false rejection rates at the set significance level \u03b1 = 0.05.", "publication_ref": ["b21", "b15", "b28", "b18", "b21", "b2", "b18", "b21"], "figure_ref": ["fig_5", "fig_5", "fig_8", "fig_8", "fig_8", "fig_8", "fig_1", "fig_1", "fig_1", "fig_11", "fig_11", "fig_1"], "table_ref": []}, {"heading": "C Proof of Theorem 1", "text": "Recall Theorem 1: Theorem 1 (The Finite Set Stein Discrepancy (FSSD)). Let V = {v 1 , . . . , v J } \u2282 R d be random vectors drawn i.i.d. from a distribution \u03b7 which has a density. Let X be a connected open set in R d . Define\nFSSD 2 p (q) := 1 dJ d i=1 J j=1 g 2 i (v j ). Assume that 1) k : X \u00d7 X \u2192 R is C 0 - universal [6, Definition 4.1] and real analytic i.e., for all v \u2208 X , f (x) := k(x, v) is a real analytic function on X . 2) E x\u223cq E x \u223cq h p (x, x ) < \u221e. 3) E x\u223cq \u2207 x log p(x) \u2212 \u2207 x log q(x) 2 < \u221e. 4) lim x \u2192\u221e p(x)g(x) = 0.\nThen, for any J \u2265 1, \u03b7-almost surely FSSD 2 p (q) = 0 if and only if p = q.\nProof. Since k is real analytic, the components g 1 , . . . , g d of g are real analytic by Lemma 15. For each i = 1, . . . , d, if g i is real analytic, then J j=1 g 2 i (v j ) = 0 if and only if g i (y) = 0 for all y \u2208 X , \u03b7-almost surely (require that the domain X be a connected open set) [24]. This implies that\n1 dJ d i=1 J j=1 g 2 i (v j )\n= 0 if and only if g(y) = 0 for all y \u2208 X , \u03b7-almost surely. By Theorem 14, g = 0 (the zero function) if and only if p = q.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "D More on Bahadur Slope", "text": "In practice, the main difficulty in determining the approximate Bahadur slope is the computation of\n\u22122 plim n\u2192\u221e log(1\u2212F (Tn)) \u03c1(n)\n, typically requiring the aid of the theory of large deviations. There are further sufficient conditions which make the computation easier. The following conditions are due to [12,13], first appearing in [1] in a slightly less general form. Definition 8. Let D(a, t) be a class of all continuous cumulative distribution functions (CDF) F such that \u22122 log(1 \u2212 F (x)) = ax t (1 + o(1)), as x \u2192 \u221e for a > 0 and t > 0. Theorem 9 ( [12,13]). Consider a sequence of test statistic T n . Assume that 1. There exists a function F (x) such that for \u03b8 \u2208 \u0398 0 , lim n\u2192\u221e P \u03b8 (T n < x) = F (x), for all x, and such that F \u2208 D(a, t) for some a > 0 and t > 0 (see Definition 8).\n2. There exists a continuous, strictly increasing function R : (0, \u221e) \u2192 (0, \u221e) with lim n\u2192\u221e R(n) = \u221e, and a function b(\u03b8) with 0 < b(\u03b8) < \u221e defined on \u0398\\\u0398 0 , such that for all \u03b8 \u2208 \u0398\\\u0398 0 , plim n\u2192\u221e T n /R(n) = b(\u03b8).\nThen, \u22122 plim n\u2192\u221e log(1\u2212F (Tn)) [R(n)] t = a [b(\u03b8)] t =: c(\u03b8), the approximate slope of the sequence T n , where \u03c1(n) = R(n) t (see Section 4).\nTheorem 10 ( [12,13]). Consider two sequences of test statistics T\n(1) n and T\n(2) n . Let F (i) be the CDF of T (i) n for i = 1, 2. Assume that each sequence satisfies all the conditions in Theorem 9 with F (i) \u2208 D(a i , t i ). Further, assume that R (1) \n(x) t1 = R (2) (x) t2 for all x. Then plim n\u2192\u221e log(1 \u2212 F (1) (T (1) n )) log(1 \u2212 F (2) (T (2) n )) = c (1) (\u03b8) c (2) (\u03b8) = \u03d5 1,2 (\u03b8),\nwhich is the approximate Bahadur efficiency of T\n(1)\nn relative to T\nn .\nWith Theorem 9, the difficulty is in showing that F \u2208 D(a, t) for some a > 0, t > 0. Typically verification of the assumption 2 of Theorem 9 poses no problem. [1] showed that the CDF of N (0, 1) belongs to D(1, 2) and the CDF of \u03c7 2 k (chi-squared distribution with k degrees of freedom, fixed k) belongs to D(1, 1). The following results make it easier to determine whether a given CDF is in the class D(a, t). Theorem 11 ([13,Theorem 6,7]). Let X have CDF F \u2208 D(a, t), and X 1 , . . . , X m be independent random variables, each with CDF F i \u2208 D(a, t). Then, the following statements are true.\n1. If b > 0, then the CDF of bX is in D(ab \u2212t , t).\n2. X \u2212 b has CDF in D(a, t) provided that t \u2265 1.\n3. For r > 0, X r has CDF in D(a, r \u22121 t) provided that F (0) = 0.\n4. max(X 1 , . . . , X m ) has CDF in D(a, t).\n5. Let a 1 , . . . , a m be non-negative real numbers such that a max := max(a 1 , . . . , a m ) > 0.\nThen, m i=1 a i X i has CDF in D(a \u2022 a \u2212t max , t) provided that m i=1 X i has CDF in D(a, t) and X i \u2265 0 for all i = 1, . . . , m.", "publication_ref": ["b11", "b12", "b0", "b11", "b12", "b11", "b12", "b0", "b0", "b12", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "E Proof of Theorem 3", "text": "Recall Theorem 3: Theorem 3.\nLet\u03a3 q := 1 n n i=1 \u03c4 (x i )\u03c4 (x i ) \u2212 [ 1 n n i=1 \u03c4 (x i )][ 1 n n j=1 \u03c4 (x j )] with {x i } n i=1 \u223c q.\nSuppose that the test threshold T \u03b1 is set to the (1\u2212\u03b1)-quantile of the distribution of\ndJ i=1 (Z 2 i \u22121)\u03bd i where {Z i } dJ i=1 i.i.d.\n\u223c N (0, 1), and\u03bd 1 , . . . ,\u03bd dJ are eigenvalues of\u03a3 q . Then, under H 0 , asymptotically the false positive rate is \u03b1. Under H 1 , for {v j } J j=1 drawn from a distribution with a density, the test power P H1 (n FSSD 2 > T \u03b1 ) \u2192 1 as n \u2192 \u221e.\nProof. Under H 0 , p = q implies that\u03a3 q =\u03a3 p (empirical estimate of \u03a3 p ). Let \u03bb j (A) denote the j th eigenvalue of the matrix A. Lemma 16 implies that A \u2192 \u03bb j (A) is continuous on the space of real symmetric matrices, for all j. Since plim n\u2192\u221e \u03a3 p \u2212 \u03a3 p = 0, by the continuous mapping theorem, the eigenvalues of\u03a3 p converge to the eigenvalues of \u03a3 p in probability. This implies that\ndJ i=1 (Z 2 i \u2212 1)\u03bd i converges in probability to dJ i=1 (Z 2 i \u2212 1)\u03c9 i as n \u2192 \u221e,\nwhere {\u03c9 i } dJ i=1 are eigenvalues of \u03a3 p . By Lemma 17, the quantile also converges, and the test threshold thus matches that of the true asymptotic null distribution given in claim 1 of Proposition 2.\nAssume H 1 holds. Lett \u03b1 , t \u03b1 be (1 \u2212 \u03b1)-quantiles of the distributions of dJ i=1 (Z 2 i \u2212 1)\u03bd i and dJ i=1 (Z 2 i \u2212 1)\u03bd i , respectively, where {\u03bd i } dJ i=1 are eigenvalues of \u03a3 q . By the same argument as in the previous paragraph,t \u03b1 converges in probability to t \u03b1 , which is a constant independent of the sample size n. Given {v j } J j=1 \u223c \u03b7, where \u03b7 is a distribution with a density, FSSD 2 > 0 by Theorem 1. It follows that\nlim n\u2192\u221e P n FSSD 2 >t \u03b1 = lim n\u2192\u221e P FSSD 2 \u2212t \u03b1 n > 0 (a) = P FSSD 2 > 0 = 1,\nwhere at (a), we use the fact that FSSD 2 converges in probability to FSSD 2 by the law of large numbers, and that lim n\u2192\u221et\u03b1 /n = 0. Proof. We will use Theorem 9 to derive the slope. For the assumption 1 of Theorem 9, we first show that the asymptotic null distribution belongs to the class D(a = 1/\u03c9 1 , t = 1) as defined in Definition 8. By Proposition 2, the asymptotic null distribution is\ndJ i=1 \u03c9 i Z 2 i \u2212 dJ i=1 \u03c9 i where Z 1 , . . . , Z dJ i.i.d. \u223c N (0, 1) and \u03c9 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03c9 dJ \u2265 0 are eigenvalues of \u03a3 p . It is known from [1] that the CDF of \u03c7 2 f is in D(1, 1)\nfor any fixed degrees of freedom f . Thus, it follows from claim 5 of Theorem 11 that the CDF of\ndJ i=1 \u03c9 i Z 2\ni is in D(a = 1/\u03c9 1 , t = 1). Claim 2 of Theorem 11 guarantees that the CDF of\ndJ i=1 \u03c9 i Z 2 i \u2212 dJ i=1 \u03c9 i is in D(a = 1/\u03c9 1 , t = 1) as desired.\nFor assumption 2 of Theorem 9, choose R(n) := n. It follows from the weak law of large numbers that under H 1 , n FSSD 2 /R(n) p \u2192 FSSD 2 . By Theorem 9, the approximate slope is FSSD 2 /\u03c9 1 . \n\u221a n S 2 l is c (LKS) = 1 2 [Eqhp(x,x )] 2 Ep[h 2 p (x,x )]\n, where h p is the U-statistic kernel of the KSD statistic, and \u03c1(n) = n.\nProof. We will use Theorem 9 to derive the slope. By the central limit theorem,\n\u221a n S 2 l \u2212 S 2 p (q) d \u2192 N (0, 2V q [h p (x, x )]), where V q [h p (x, x )] := E x\u223cq E x \u223cq [h 2 p (x, x )]\u2212(E x\u223cq E x \u223cq [h p (x, x )]) 2 . Under H 0 : p = q, it fol- lows that S 2 p (q) = E x\u223cq E x \u223cq [h p (x,\nx )] = 0 by Theorem 14, and\n\u221a n S 2 l d \u2192 N (0, 2V p [h p (x, x )]) where V p [h p (x, x )] := E x\u223cp E x \u223cp [h 2 p (x, x )].\nIt is known from [1] that the CDF of N (0, 1) is in the class D(1, 2) (see Definition 8). Thus, by property 1 of Theorem 11, the CDF of\nN (0, 2V p [h p (x, x )]) is in D a = 1 2Vp[hp(x,x )] , t = 2 .\nFor assumption 2 of Theorem 9, choose R(n) := \u221a n. It follows from the weak law of large numbers that under H 1 ,\n\u221a n S 2 l /R(n) = S 2 l p \u2192 S 2 p (q)\n. By Theorem 9, the approximate slope is\nS 4 p (q) 2Vp[hp(x,x )] .\nH Proof of Theorem 7\nWe will first prove a number of useful results that will allow us to prove Theorem 7 at the end. Recall that v denotes a test location in the FSSD test, \u03c3 2 k denotes the Gaussian kernel bandwidth of the FSSD test, and \u03ba 2 denotes the Gaussian kernel bandwidth of the LKS test. Proposition 12. Under the assumption that J = 1 (i.e., one test location v), p = N (0, 1) and q = N (\u00b5 q , \u03c3 2 q ), the approximate Bahadur Slope of n FSSD 2 is\nc (FSSD) := \u03c3 2 k 3/2 \u03c3 2 k + 2 5/2 e v 2 \u03c3 2 k +2 \u2212 (v\u2212\u00b5q) 2 \u03c3 2 k +\u03c3 2 q \u03c3 2 k + 1 \u00b5 q + v \u03c3 2 q \u2212 1 2 \u03c3 2 k + \u03c3 2 q 3 (\u03c3 6 k + 4\u03c3 4 k + (v 2 + 5) \u03c3 2 k + 2) .(3)\nProof. This result follows directly from Theorem 5 specialized to the case of p = N (0, 1), q = N (\u00b5 q , \u03c3 2 q ), and J = 1. Since dJ = 1, the covariance matrix\n\u03a3 p = E x\u223cp \u03be 2 p (x, v) = e \u2212 v 2 \u03c3 2 k +2 \u03c3 6 k + 4\u03c3 4 k + v 2 + 5 \u03c3 2 k + 2 \u03c3 k (\u03c3 2 k + 2) 5/2\nreduces to a scalar, where\n\u03be p (x, v) = \u2202 \u2202x log p(x) k(x, v) + \u2202 \u2202x k(x, v) = \u2212e \u2212 (v\u2212x) 2 2\u03c3 2 k x\u03c3 2 k \u2212 v + x /\u03c3 2 k .\nIn this case,\nFSSD 2 = E 2 x\u223cq [\u03be p (x, v)] = \u03c3 2 k e \u2212 (v\u2212\u00b5q) 2 \u03c3 2 k +\u03c3 2 q \u03c3 2 k + 1 \u00b5 q + v \u03c3 2 q \u2212 1 2 \u03c3 2 k + \u03c3 2 q 3 .\nTaking the ratio FSSD 2 /E x\u223cp \u03be 2 p (x, v) gives the result.  . Then, the following statements hold. (\u00b5 q , \u03ba 2 ) is strictly increasing on (0, \u221e). Further, (\u00b5 q , \u03ba 2 ) is differentiable with respect to \u03ba 2 on the interval (0, \u221e). The partial derivative is given by > 0 for \u03ba 2 \u2208 (0, \u221e), we conclude that \u03ba 2 \u2192 c (LKS) 1", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "The population kernel Stein discrepancy is", "text": "S 2 p (q) = \u00b5 2 q \u03ba 2 + 2\u03c3 2 q + \u03c3 2 q \u2212 1 2 \u03ba 2 + 2\u03c3 2 q 2\u03c3 2 q \u03ba 2 + 1 . 2. The approximate Bahadur slope of \u221a n S 2 l is c (LKS) := \u03ba 5 \u03ba 2 + 4 5/2 \u00b5 2 q \u03ba 2 + 2\u03c3 2 q + \u03c3 2 q \u2212 1 2 2 2 (\u03ba 8 + 8\u03ba 6 + 21\u03ba 4 + 20\u03ba 2 + 12) \u03ba 2 + 2\u03c3 2 q 3 .(4)\n(\u00b5 q , \u03ba 2 ) is a strictly increasing function on (0, \u221e). By taking the limit, we have lim \u03ba 2 \u2192\u221e c (LKS) 1\n(\u00b5 q , \u03ba 2 ) = \u00b5 4 q /2.\nWe are ready to prove Theorem 7. Recall that \u03c3 2 k is the kernel bandwidth of n FSSD 2 , and \u03ba 2 is the kernel bandwidth of \u221a n S 2 l (see Section 2). Recall Theorem 7: Theorem 7 (Efficiency in the Gaussian mean shift problem). Let E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) be the approximate Bahadur efficiency of n FSSD 2 relative to \u221a n S 2 l for the case where p = N (0, 1), q = N (\u00b5 q , 1), and J = 1 (i.e., one test location v for n FSSD 2 ). Fix \u03c3 2 k = 1 for n FSSD 2 . Then, for any \u00b5 q = 0, for some v \u2208 R, and for any \u03ba 2 > 0, we have E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) > 2.\nProof. By Proposition 12, the approximate slope of n FSSD 2 when \u03c3 2 q = 1 is\nc (FSSD) 1 (\u00b5 q , v, \u03c3 2 k ) = \u03c3 2 k \u03c3 2 k + 2 3 \u00b5 2 q e v 2 \u03c3 2 k +2 \u2212 (v\u2212\u00b5q) 2 \u03c3 2 k +1 2 \u03c3 2 k + 1 (\u03c3 2 k + 1) (\u03c3 6 k + 4\u03c3 4 k + (v 2 + 5) \u03c3 2 k + 2)\n.\nTheorem 10 states that the approximate efficiency E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) is given by the ratio We have (\u00b5 q , \u03ba 2 ) \u2264 \u00b5 4 q /2 from (5). It can be seen that for \u00b5 q = 0, g(\u00b5 q ) is an even function i.e., g(\u00b5 q ) = g(\u2212\u00b5 q ). The second derivative  \nE 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) = E 1 (\u00b5 q , 2\u00b5 q , 1, \u03ba 2 ) = c(", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement WJ, WX, and AG thank the Gatsby Charitable Foundation for the financial support. ZSz was financially supported by the Data Science Initiative. KF has been supported by KAKENHI Innovative Areas 25120012.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I Known Results", "text": "This section presents known results from other works. Theorem 14 ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Stochastic comparison of tests", "journal": "The Annals of Mathematical Statistics", "year": "1960", "authors": "R R Bahadur"}, {"ref_id": "b1", "title": "A consistent test for multivariate normality based on the empirical characteristic function", "journal": "Metrika", "year": "1988", "authors": "L Baringhaus; N Henze"}, {"ref_id": "b2", "title": "On the asymptotic normality of the l 1 -and l 2 -errors in histogram density estimation", "journal": "Canadian Journal of Statistics", "year": "1994", "authors": "J Beirlant; L Gy\u00f6rfi; G Lugosi"}, {"ref_id": "b3", "title": "Matrix analysis", "journal": "Springer Science & Business Media", "year": "2013", "authors": "R Bhatia"}, {"ref_id": "b4", "title": "Adaptive smoothing and density based tests of multivariate normality", "journal": "Journal of the American Statistical Association", "year": "1993", "authors": "A Bowman; P Foster"}, {"ref_id": "b5", "title": "Vector valued reproducing kernel Hilbert spaces and universality", "journal": "Analysis and Applications", "year": "2010-01", "authors": "C Carmeli; E Vito; A Toigo; V Umanit\u00e0"}, {"ref_id": "b6", "title": "A wild bootstrap for degenerate kernel tests", "journal": "", "year": "2014", "authors": "K Chwialkowski; D Sejdinovic; A Gretton"}, {"ref_id": "b7", "title": "Fast two-sample testing with analytic representations of probability measures", "journal": "", "year": "2015", "authors": "K Chwialkowski; A Ramdas; D Sejdinovic; A Gretton"}, {"ref_id": "b8", "title": "A kernel test of goodness of fit", "journal": "", "year": "2016", "authors": "K Chwialkowski; H Strathmann; A Gretton"}, {"ref_id": "b9", "title": "An omnibus test for the two-sample problem using the empirical characteristic function", "journal": "Journal of Statistical Computation and Simulation", "year": "1986", "authors": "T Epps; K Singleton"}, {"ref_id": "b10", "title": "The Kolmogorov-Smirnov test for goodness of fit", "journal": "Journal of the American Statistical Association", "year": "1951", "authors": "J ; Frank J Massey"}, {"ref_id": "b11", "title": "On a measure of test efficiency proposed by R", "journal": "R. Bahadur", "year": "1964", "authors": "L J Gleser"}, {"ref_id": "b12", "title": "The comparison of multivariate tests of hypothesis by means of Bahadur efficiency", "journal": "", "year": "1966", "authors": "L J Gleser"}, {"ref_id": "b13", "title": "Measuring sample quality with Stein's method", "journal": "", "year": "2015", "authors": "J Gorham; L Mackey"}, {"ref_id": "b14", "title": "Measuring sample quality with kernels", "journal": "PMLR", "year": "2017-08", "authors": "J Gorham; L Mackey"}, {"ref_id": "b15", "title": "A kernel two-sample test", "journal": "JMLR", "year": "2012", "authors": "A Gretton; K M Borgwardt; M J Rasch; B Sch\u00f6lkopf; A Smola"}, {"ref_id": "b16", "title": "Optimal kernel choice for large-scale two-sample tests", "journal": "", "year": "2012", "authors": "A Gretton; D Sejdinovic; H Strathmann; S Balakrishnan; M Pontil; K Fukumizu; B K Sriperumbudur"}, {"ref_id": "b17", "title": "A consistent goodness of fit test based on the total variation distance", "journal": "", "year": "1990", "authors": "L Gy\u00f6rfi; E C Van Der Meulen"}, {"ref_id": "b18", "title": "Interpretable Distribution Features with Maximum Testing Power", "journal": "", "year": "2016", "authors": "W Jitkrittum; Z Szab\u00f3; K P Chwialkowski; A Gretton"}, {"ref_id": "b19", "title": "An adaptive test of independence with analytic kernel embeddings", "journal": "PMLR", "year": "2017", "authors": "W Jitkrittum; Z Szab\u00f3; A Gretton"}, {"ref_id": "b20", "title": "Stein's method for comparison of univariate distributions", "journal": "Probability Surveys", "year": "2017", "authors": "C Ley; G Reinert; Y Swan"}, {"ref_id": "b21", "title": "A kernelized Stein discrepancy for goodness-of-fit tests", "journal": "", "year": "2016", "authors": "Q Liu; J Lee; M Jordan"}, {"ref_id": "b22", "title": "Statistical model criticism using kernel two sample tests", "journal": "", "year": "2015", "authors": "J Lloyd; Z Ghahramani"}, {"ref_id": "b23", "title": "The Zero Set of a Real Analytic Function", "journal": "", "year": "2015-12", "authors": "B Mityagin"}, {"ref_id": "b24", "title": "Control functionals for Monte Carlo integration", "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": "2017", "authors": "C J Oates; M Girolami; N Chopin"}, {"ref_id": "b25", "title": "New goodness-of-fit tests for Pareto distributions", "journal": "ASTIN Bulletin: Journal of the International Association of Actuaries", "year": "2009", "authors": "M L Rizzo"}, {"ref_id": "b26", "title": "Approximation Theorems of Mathematical Statistics", "journal": "John Wiley & Sons", "year": "2009", "authors": "R J Serfling"}, {"ref_id": "b27", "title": "Support Vector Machines", "journal": "Springer", "year": "2008", "authors": "I Steinwart; A Christmann"}, {"ref_id": "b28", "title": "Generative models and model criticism via optimized Maximum Mean Discrepancy", "journal": "", "year": "2016", "authors": "D J Sutherland; H.-Y Tung; H Strathmann; S De; A Ramdas; A Smola; A Gretton"}, {"ref_id": "b29", "title": "A new test for multivariate normality", "journal": "Journal of Multivariate Analysis", "year": "2005", "authors": "G J Sz\u00e9kely; M L Rizzo"}, {"ref_id": "b30", "title": "Asymptotic Statistics", "journal": "Cambridge University Press", "year": "2000", "authors": "A W Van Der;  Vaart"}, {"ref_id": "b31", "title": "Large-scale kernel methods for independence testing", "journal": "Statistics and Computing", "year": "2017", "authors": "Q Zhang; S Filippi; A Gretton; D Sejdinovic"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "=f , \u03be p (x, \u2022) F d , where at (a) we use the reproducing property of F, i.e., f i (x) = f i , k(x, \u2022) F , and that \u2202k(x,\u2022) \u2202xi \u2208 F [28, Lemma 4.34], hence \u03be p (x, \u2022) := \u2202 log p(x) \u2202x k(x, \u2022)+ \u2202k(x,\u2022)", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Theorem 5 .5The approximate Bahadur slope of n FSSD 2 is c (FSSD) := FSSD 2 /\u03c9 1 , where \u03c9 1 is the maximum eigenvalue of \u03a3 p := E x\u223cp [\u03c4 (x)\u03c4 (x)] and \u03c1(n) = n. Theorem 6. The approximate Bahadur slope of the linear-time kernel Stein (LKS) test statistic", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "FSSD 2 \u03c3H 1 Figure 1 :11Figure 1: The power criterion FSSD 2 /\u03c3 H1 as a function of test location v.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "RBM. n = 1000. Perturb all entries of B.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "(a) p = 2-component GMM. p = 10-component GMM", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: Plots of the optimization objective as a function of test location v \u2208 R 2 in the Gaussian mixture model (GMM) evaluation task.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Gaussian vs. GMM. d = 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "GVD. d = 5.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 4 :4Figure 4: Plots of rejection rate against the number of test locations J in the three toy problems in Section A.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "RBM. n = 1000. Perturb all entries of B. RBM. No perturbation.H0 holds.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 5 :5Figure 5: Rejection rates of the six tests in the RBM problem with d = 50 and d h = 10.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 6 :6Figure 6: Pairwise scatter plots of 1000 points drawn from RBMs. Only the first 4 variates out of 50 are shown. (a): RBM with d = 50 dimensions with d h = 10 latent variables. (b): RBM with d = 50 dimensions with d h = 40 latent variables.", "figure_data": ""}, {"figure_label": "555", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "F Proof of Theorem 5 (Recall Theorem 5 : 5 .555Slope of n FSSD 2 ) Theorem The approximate Bahadur slope of n FSSD 2 is c (FSSD) := FSSD 2 /\u03c9 1 , where \u03c9 1 is the maximum eigenvalue of \u03a3 p := E x\u223cp [\u03c4 (x)\u03c4 (x)] and \u03c1(n) = n.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Proposition 13 .13Assume that p = N (0, 1) and q = N (\u00b5 q , \u03c3 2 q ). Let \u221a n S 2 l be the linear-time kernel Stein (LKS) test statistic where S 2 l is defined in Section 2 with a Gaussian kernel k(x, y) = exp \u2212 (x\u2212y) 2", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "2\u03ba2 ", "figure_data": ""}, {"figure_label": "4222", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "\u00b5 4 q 2 (\u03ba 2 + 2 )4222(\u03ba 8 + 8\u03ba 6 + 21\u03ba 4 + 20\u03ba 2 + 12)denote the approximate slope c (LKS) specialized to when q = N (\u00b5 q , 1). Then, for any \u00b5 q = 0, the function \u03ba 2 \u2192 c (LKS) 1", "figure_data": ""}, {"figure_label": "22222242224222212224212342", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "lim \u03ba 2 2 , 2 \u03ba 2 \u2212 \u03ba 2 + 1 x 2 + \u03ba 4 + 2\u03ba 2 + 2 xy \u2212 \u03ba 2 + 1 y 2 \u03ba 4 . 2 [ 2 Ep[h 2 p 2 qe 1 2\u03ba 2 E p h 2 p (x, x ) = \u03ba 2 + 4 \u03ba 4 + 4\u03ba 2 + 5 \u03ba 2 + 12 \u03ba 3 (\u03ba 2 + 4 ) 2 p22222242224222212224212342\u2192\u221e c (LKS) 1 (\u00b5 q , \u03ba 2 ) = \u00b5 4 q /2.(5)Proof. Proof of Claim 1, 2. Recall S2 l := 2 n n/2 i=1 h p (x 2i\u22121 , x 2i ). With p = N (0, 1), and k(x, y) = exp \u2212 (x\u2212y) 2 2\u03ba h p (x,y) can be written as h p (x, y) := e \u2212 (x\u2212y) 2 2\u03ba By Theorem 6, c (LKS) = 1 Eqhp(x,x )] (x,x )] which mainly involves expectations with respect to a normal distribution. In computing the expectation E x \u223cq h p (x, x ), the idea is to form the density for a new normal distribution by combining 1 \u221a 2\u03c0\u03c3 \u2212(x\u2212\u00b5q) 2 /2\u03c3 2 q (the density of q) and the term e \u2212 (x\u2212y) 2 2\u03ba 2 in the expression of h p (x, y). Computation of E x \u223cq h p (x, x ) will then boil down to computing an expectation wrt. a new normal distribution.It turns out thatE x\u223cq E x \u223cq [h p (x, x )] = \u00b5 2 q \u03ba 2 + 2\u03c3 2 q + \u03c3 2 q \u2212 (x,x )]gives the slope. Proof of Claim 3. The expression for c (LKS) 1 is obtained straightforwardly by plugging \u03c3 2 q = 1 into the expression of c(LKS)  . Assume \u00b5 q = 0. It can be seen that c (LKS) 1", "figure_data": ""}, {"figure_label": "8642422", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "7\u03ba 8 + 56\u03ba 6 + 166\u03ba 4 + 216\u03ba 2 + 120 \u00b5 4 q(\u03ba 2 2 .8642422+ 2) 2 (\u03ba 8 + 8\u03ba 6 + 21\u03ba 4 + 20\u03ba 2 + 12) Since for any \u00b5 q = 0, \u2202 \u2202\u03ba 2 c (LKS) 1", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "1 (1\u00b5q,\u03ba 2 ) (see Propositions 12 and 13) of the approximate slopes of the two tests. Pick \u03c3 2 k = 1, and for any \u00b5 q = 0, pick v = 2\u00b5 q . These choices give the slope c (FSSD) 1(\u00b5 q , 2\u00b5 q ,", "figure_data": ""}, {"figure_label": "111212", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "FSSD) 1 (\u00b5 q , 2\u00b5 q , 1 1 (\u00b5 q , \u03ba 2 ) 12 :111212= g(\u00b5 q ), where at (a) we use c (LKS) 1", "figure_data": ""}, {"figure_label": "12010", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "1 , 2 q \u2265 0 . 10 \u221a12010q ) > 0, consider two cases of \u00b5 2 q \u2265 1 and 0 < \u00b5 2 q < 1. When \u00b5 2 q \u2265 This shows that g(\u00b5 q ) is convex on (0, \u221e). The function g(\u00b5 q ) on R\\{0} achieves global minima at \u00b5 q = \u00b5 * q := \u00b1 3 41 \u2212 1 \u2248 \u00b11.273. This implies that E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) \u2265 g(\u00b5 q ) \u2265 g(\u00b5 * q )", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Theorem 6. The approximate Bahadur slope of the linear-time kernel Stein (LKS) test statistic", "figure_data": "G Proof of Theorem 6 (Slope of\u221a n S 2 l )Recall Theorem 6:"}], "formulas": [{"formula_id": "formula_0", "formula_text": "F d := d i=1 f i , g i F .", "formula_coordinates": [2.0, 240.89, 578.92, 92.0, 14.11]}, {"formula_id": "formula_1", "formula_text": "f F d \u22641 E x\u223cq f , \u03be p (x, \u2022) F d (a) = sup f F d \u22641 f , E x\u223cq \u03be p (x, \u2022) F d = g(\u2022) F d , (1)", "formula_coordinates": [2.0, 177.85, 701.36, 326.15, 23.59]}, {"formula_id": "formula_2", "formula_text": "2 l := 2 n n/2 i=1 h p (x 2i\u22121 , x 2i ),", "formula_coordinates": [3.0, 152.81, 273.64, 120.43, 14.56]}, {"formula_id": "formula_3", "formula_text": "Theorem 1 (The Finite Set Stein Discrepancy (FSSD)). Let V = {v 1 , . . . , v J } \u2282 R d be random vectors drawn i.i.d. from a distribution \u03b7 which has a density. Let X be a connected open set in R d . Define FSSD 2 p (q) := 1 dJ d i=1 J j=1 g 2 i (v j ). Assume that 1) k : X \u00d7 X \u2192 R is C 0 - universal [6, Definition 4.1] and real analytic i.e., for all v \u2208 X , f (x) := k(x, v) is a real analytic function on X . 2) E x\u223cq E x \u223cq h p (x, x ) < \u221e. 3) E x\u223cq \u2207 x log p(x) \u2212 \u2207 x log q(x) 2 < \u221e. 4) lim x \u2192\u221e p(x)g(x) = 0.", "formula_coordinates": [3.0, 107.67, 550.06, 397.98, 69.96]}, {"formula_id": "formula_4", "formula_text": "x\u2212y 2 2 2\u03c3 2 k (see", "formula_coordinates": [3.0, 317.65, 672.48, 50.32, 18.57]}, {"formula_id": "formula_5", "formula_text": "Let \u039e(x) \u2208 R d\u00d7J such that [\u039e(x)] i,j = \u03be p,i (x, v j )/ \u221a dJ. Define \u03c4 (x) := vec(\u039e(x)) \u2208 R dJ", "formula_coordinates": [4.0, 108.0, 205.62, 368.87, 25.78]}, {"formula_id": "formula_6", "formula_text": "V = {v j } J j=1 . Let \u2206(x, y) := \u03c4 (x) \u03c4 (y) = tr(\u039e(x) \u039e(y)). Given an i.i.d. sample {x i } n i=1 \u223c q, a consistent, unbiased estimator of FSSD 2 p (q) is FSSD 2 = 1 dJ d l=1 J m=1 1 n(n \u2212 1) n i=1 j =i \u03be p,l (xi, vm)\u03be p,l (xj, vm) = 2 n(n \u2212 1) i<j \u2206(xi, xj), (2)", "formula_coordinates": [4.0, 108.0, 236.71, 397.74, 59.25]}, {"formula_id": "formula_7", "formula_text": "d \u2192 to denote convergence in distribution. Proposition 2 (Asymptotic distributions of FSSD 2 ). Let Z 1 , . . . , Z dJ i.i.d. \u223c N (0, 1). Let \u00b5 := E x\u223cq [\u03c4 (x)], \u03a3 r := cov x\u223cr [\u03c4 (x)] \u2208 R dJ\u00d7dJ for r \u2208 {p, q}, and {\u03c9 i } dJ i=1 be the eigenvalues of \u03a3 p = E x\u223cp [\u03c4 (x)\u03c4 (x)]. Assume that E x\u223cq E y\u223cq \u2206 2 (x, y) < \u221e.", "formula_coordinates": [4.0, 107.59, 313.17, 396.41, 70.8]}, {"formula_id": "formula_8", "formula_text": "0 : p = q, n FSSD 2 d \u2192 dJ i=1 (Z 2 i \u2212 1)\u03c9 i . 2. Under H 1 : p = q, if \u03c3 2 H1 := 4\u00b5 \u03a3 q \u00b5 > 0, then \u221a n( FSSD 2 \u2212 FSSD 2 ) d \u2192 N (0, \u03c3 2 H1 ).", "formula_coordinates": [4.0, 131.41, 401.22, 366.5, 42.69]}, {"formula_id": "formula_9", "formula_text": "Let\u03a3 q := 1 n n i=1 \u03c4 (x i )\u03c4 (x i ) \u2212 [ 1 n n i=1 \u03c4 (x i )][ 1 n n j=1 \u03c4 (x j )] with {x i } n i=1 \u223c q. Suppose that the test threshold T \u03b1 is set to the (1\u2212\u03b1)-quantile of the distribution of dJ i=1 (Z 2 i \u22121)\u03bd i where {Z i } dJ i=1 i.i.d.", "formula_coordinates": [4.0, 108.0, 622.06, 396.0, 50.0]}, {"formula_id": "formula_10", "formula_text": "P H1 (n FSSD 2 > r) \u2248 1 \u2212 \u03a6 r \u221a n\u03c3 H 1 \u2212 \u221a n FSSD 2 \u03c3 H 1", "formula_coordinates": [5.0, 153.26, 236.18, 206.41, 24.46]}, {"formula_id": "formula_11", "formula_text": "Proof. P H1 (n FSSD 2 > r) = P H1 ( FSSD 2 > r/n) = P H1 \u221a n FSSD 2 \u2212FSSD 2 \u03c3 H 1 > \u221a n r/n\u2212FSSD 2 \u03c3 H 1 .", "formula_coordinates": [5.0, 108.0, 283.14, 397.74, 21.79]}, {"formula_id": "formula_12", "formula_text": "P H1 (n FSSD 2 > r) \u2248 1 \u2212 \u03a6 r \u221a n\u03c3 H 1 \u2212 \u221a n FSSD 2 \u03c3 H 1 .", "formula_coordinates": [5.0, 166.14, 311.97, 213.2, 24.46]}, {"formula_id": "formula_13", "formula_text": "r \u221a n\u03c3 H 1 \u2212 \u221a n FSSD 2 \u03c3 H 1", "formula_coordinates": [5.0, 293.65, 359.1, 79.64, 24.46]}, {"formula_id": "formula_14", "formula_text": "r \u221a n\u03c3 H 1 = O(n \u22121/2 ) going to 0 as n \u2192 \u221e, while the second term \u221a n FSSD 2 \u03c3 H 1 = O(n 1/2 ), dominating", "formula_coordinates": [5.0, 109.2, 376.55, 394.8, 24.46]}, {"formula_id": "formula_15", "formula_text": "FSSD 2 \u03c3 H 1 .", "formula_coordinates": [5.0, 289.25, 410.27, 27.79, 17.21]}, {"formula_id": "formula_16", "formula_text": "\u2207 x log p(x) costs O(d 2 ) to evaluate. Computing \u2207 \u03b6 FSSD 2 \u03c3 H 1 +\u03b3 costs O(d 2 J 2 n). The computational complexity of n FSSD 2 and\u03c3 2 H1 is O(d 2 Jn).", "formula_coordinates": [5.0, 108.0, 537.57, 406.44, 36.77]}, {"formula_id": "formula_17", "formula_text": "Remark 2. Let\u03bc := 1 n n i=1 \u03c4 (x i ).", "formula_coordinates": [5.0, 107.69, 624.56, 151.54, 14.56]}, {"formula_id": "formula_18", "formula_text": "\u03c1 : (0, \u221e) \u2192 (0, \u221e) such that lim n\u2192\u221e \u03c1(n) = \u221e, and that \u22122 plim n\u2192\u221e log(1\u2212F (Tn)) \u03c1(n)", "formula_coordinates": [6.0, 108.0, 295.71, 362.92, 19.16]}, {"formula_id": "formula_20", "formula_text": "n relative to T (2) n is defined as E(\u03b8 A ) := c (1) (\u03b8 A )/c (2) (\u03b8 A ) for \u03b8 A \u2208 \u0398\\\u0398 0 . If E(\u03b8 A ) > 1, then T (1)", "formula_coordinates": [6.0, 108.0, 396.15, 395.99, 33.4]}, {"formula_id": "formula_21", "formula_text": "\u221a n S 2 l is c (LKS) = 1 2 [Eqhp(x,x )] 2 Ep[h 2 p (x,x )]", "formula_coordinates": [6.0, 108.0, 501.34, 395.5, 40.49]}, {"formula_id": "formula_22", "formula_text": "we have E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) > 2.", "formula_coordinates": [6.0, 256.5, 679.22, 124.86, 12.55]}, {"formula_id": "formula_23", "formula_text": "\u22124 \u22122 0 2 4 v * v * p q", "formula_coordinates": [7.0, 394.56, 176.41, 96.07, 53.28]}, {"formula_id": "formula_24", "formula_text": "p(x) = N (x|0, I d ) and q(x) = d i=1 Laplace(x i |0, 1/ \u221a 2)", "formula_coordinates": [7.0, 216.01, 294.88, 244.57, 25.53]}, {"formula_id": "formula_25", "formula_text": "(RBM): p(x) is the marginal distribution of p(x, h) = 1 Z exp x Bh + b x + c x \u2212 1 2 x 2 , where x \u2208 R d , h \u2208 {\u00b11} d h", "formula_coordinates": [7.0, 109.2, 342.22, 394.81, 28.35]}, {"formula_id": "formula_26", "formula_text": "x) = h\u2208{\u22121,1} d h p(x, h) is intractable when d h is large, since it involves summing over 2 d h terms.", "formula_coordinates": [7.0, 118.52, 366.16, 387.23, 23.09]}, {"formula_id": "formula_27", "formula_text": "FSSD 2 p (q) := 1 dJ d i=1 J j=1 g 2 i (v j ). Assume that 1) k : X \u00d7 X \u2192 R is C 0 - universal [6, Definition 4.1] and real analytic i.e., for all v \u2208 X , f (x) := k(x, v) is a real analytic function on X . 2) E x\u223cq E x \u223cq h p (x, x ) < \u221e. 3) E x\u223cq \u2207 x log p(x) \u2212 \u2207 x log q(x) 2 < \u221e. 4) lim x \u2192\u221e p(x)g(x) = 0.", "formula_coordinates": [13.0, 108.0, 133.71, 397.65, 47.56]}, {"formula_id": "formula_28", "formula_text": "1 dJ d i=1 J j=1 g 2 i (v j )", "formula_coordinates": [13.0, 109.2, 248.59, 91.68, 14.56]}, {"formula_id": "formula_29", "formula_text": "\u22122 plim n\u2192\u221e log(1\u2212F (Tn)) \u03c1(n)", "formula_coordinates": [13.0, 108.0, 325.41, 106.6, 19.16]}, {"formula_id": "formula_30", "formula_text": "(x) t1 = R (2) (x) t2 for all x. Then plim n\u2192\u221e log(1 \u2212 F (1) (T (1) n )) log(1 \u2212 F (2) (T (2) n )) = c (1) (\u03b8) c (2) (\u03b8) = \u03d5 1,2 (\u03b8),", "formula_coordinates": [13.0, 207.88, 541.46, 224.58, 58.13]}, {"formula_id": "formula_32", "formula_text": "Let\u03a3 q := 1 n n i=1 \u03c4 (x i )\u03c4 (x i ) \u2212 [ 1 n n i=1 \u03c4 (x i )][ 1 n n j=1 \u03c4 (x j )] with {x i } n i=1 \u223c q.", "formula_coordinates": [14.0, 108.0, 212.15, 396.0, 26.9]}, {"formula_id": "formula_33", "formula_text": "dJ i=1 (Z 2 i \u22121)\u03bd i where {Z i } dJ i=1 i.i.d.", "formula_coordinates": [14.0, 108.0, 227.34, 395.5, 34.81]}, {"formula_id": "formula_34", "formula_text": "dJ i=1 (Z 2 i \u2212 1)\u03bd i converges in probability to dJ i=1 (Z 2 i \u2212 1)\u03c9 i as n \u2192 \u221e,", "formula_coordinates": [14.0, 136.28, 348.68, 290.03, 19.34]}, {"formula_id": "formula_35", "formula_text": "lim n\u2192\u221e P n FSSD 2 >t \u03b1 = lim n\u2192\u221e P FSSD 2 \u2212t \u03b1 n > 0 (a) = P FSSD 2 > 0 = 1,", "formula_coordinates": [14.0, 139.92, 457.32, 332.15, 20.83]}, {"formula_id": "formula_36", "formula_text": "dJ i=1 \u03c9 i Z 2 i \u2212 dJ i=1 \u03c9 i where Z 1 , . . . , Z dJ i.i.d. \u223c N (0, 1) and \u03c9 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03c9 dJ \u2265 0 are eigenvalues of \u03a3 p . It is known from [1] that the CDF of \u03c7 2 f is in D(1, 1)", "formula_coordinates": [14.0, 108.0, 623.08, 396.0, 46.77]}, {"formula_id": "formula_37", "formula_text": "dJ i=1 \u03c9 i Z 2", "formula_coordinates": [14.0, 257.65, 665.54, 36.07, 14.11]}, {"formula_id": "formula_38", "formula_text": "dJ i=1 \u03c9 i Z 2 i \u2212 dJ i=1 \u03c9 i is in D(a = 1/\u03c9 1 , t = 1) as desired.", "formula_coordinates": [14.0, 228.36, 679.37, 231.43, 19.34]}, {"formula_id": "formula_39", "formula_text": "\u221a n S 2 l is c (LKS) = 1 2 [Eqhp(x,x )] 2 Ep[h 2 p (x,x )]", "formula_coordinates": [15.0, 108.0, 109.88, 395.5, 40.49]}, {"formula_id": "formula_40", "formula_text": "\u221a n S 2 l \u2212 S 2 p (q) d \u2192 N (0, 2V q [h p (x, x )]), where V q [h p (x, x )] := E x\u223cq E x \u223cq [h 2 p (x, x )]\u2212(E x\u223cq E x \u223cq [h p (x, x )]) 2 . Under H 0 : p = q, it fol- lows that S 2 p (q) = E x\u223cq E x \u223cq [h p (x,", "formula_coordinates": [15.0, 107.64, 178.53, 398.02, 59.81]}, {"formula_id": "formula_41", "formula_text": "\u221a n S 2 l d \u2192 N (0, 2V p [h p (x, x )]) where V p [h p (x, x )] := E x\u223cp E x \u223cp [h 2 p (x, x )].", "formula_coordinates": [15.0, 107.64, 219.62, 397.53, 31.64]}, {"formula_id": "formula_42", "formula_text": "N (0, 2V p [h p (x, x )]) is in D a = 1 2Vp[hp(x,x )] , t = 2 .", "formula_coordinates": [15.0, 108.0, 252.03, 397.17, 32.59]}, {"formula_id": "formula_43", "formula_text": "\u221a n S 2 l /R(n) = S 2 l p \u2192 S 2 p (q)", "formula_coordinates": [15.0, 209.43, 295.67, 119.07, 24.46]}, {"formula_id": "formula_44", "formula_text": "S 4 p (q) 2Vp[hp(x,x )] .", "formula_coordinates": [15.0, 109.2, 315.21, 51.04, 17.23]}, {"formula_id": "formula_45", "formula_text": "c (FSSD) := \u03c3 2 k 3/2 \u03c3 2 k + 2 5/2 e v 2 \u03c3 2 k +2 \u2212 (v\u2212\u00b5q) 2 \u03c3 2 k +\u03c3 2 q \u03c3 2 k + 1 \u00b5 q + v \u03c3 2 q \u2212 1 2 \u03c3 2 k + \u03c3 2 q 3 (\u03c3 6 k + 4\u03c3 4 k + (v 2 + 5) \u03c3 2 k + 2) .(3)", "formula_coordinates": [15.0, 146.29, 444.09, 357.71, 37.05]}, {"formula_id": "formula_46", "formula_text": "\u03a3 p = E x\u223cp \u03be 2 p (x, v) = e \u2212 v 2 \u03c3 2 k +2 \u03c3 6 k + 4\u03c3 4 k + v 2 + 5 \u03c3 2 k + 2 \u03c3 k (\u03c3 2 k + 2) 5/2", "formula_coordinates": [15.0, 174.26, 525.49, 257.73, 34.29]}, {"formula_id": "formula_47", "formula_text": "\u03be p (x, v) = \u2202 \u2202x log p(x) k(x, v) + \u2202 \u2202x k(x, v) = \u2212e \u2212 (v\u2212x) 2 2\u03c3 2 k x\u03c3 2 k \u2212 v + x /\u03c3 2 k .", "formula_coordinates": [15.0, 108.0, 566.82, 396.0, 38.93]}, {"formula_id": "formula_48", "formula_text": "FSSD 2 = E 2 x\u223cq [\u03be p (x, v)] = \u03c3 2 k e \u2212 (v\u2212\u00b5q) 2 \u03c3 2 k +\u03c3 2 q \u03c3 2 k + 1 \u00b5 q + v \u03c3 2 q \u2212 1 2 \u03c3 2 k + \u03c3 2 q 3 .", "formula_coordinates": [15.0, 158.87, 607.8, 294.25, 39.08]}, {"formula_id": "formula_49", "formula_text": "S 2 p (q) = \u00b5 2 q \u03ba 2 + 2\u03c3 2 q + \u03c3 2 q \u2212 1 2 \u03ba 2 + 2\u03c3 2 q 2\u03c3 2 q \u03ba 2 + 1 . 2. The approximate Bahadur slope of \u221a n S 2 l is c (LKS) := \u03ba 5 \u03ba 2 + 4 5/2 \u00b5 2 q \u03ba 2 + 2\u03c3 2 q + \u03c3 2 q \u2212 1 2 2 2 (\u03ba 8 + 8\u03ba 6 + 21\u03ba 4 + 20\u03ba 2 + 12) \u03ba 2 + 2\u03c3 2 q 3 .(4)", "formula_coordinates": [16.0, 131.41, 92.09, 372.59, 105.98]}, {"formula_id": "formula_50", "formula_text": "c (FSSD) 1 (\u00b5 q , v, \u03c3 2 k ) = \u03c3 2 k \u03c3 2 k + 2 3 \u00b5 2 q e v 2 \u03c3 2 k +2 \u2212 (v\u2212\u00b5q) 2 \u03c3 2 k +1 2 \u03c3 2 k + 1 (\u03c3 2 k + 1) (\u03c3 6 k + 4\u03c3 4 k + (v 2 + 5) \u03c3 2 k + 2)", "formula_coordinates": [17.0, 160.12, 193.7, 287.79, 42.03]}, {"formula_id": "formula_51", "formula_text": "E 1 (\u00b5 q , v, \u03c3 2 k , \u03ba 2 ) = E 1 (\u00b5 q , 2\u00b5 q , 1, \u03ba 2 ) = c(", "formula_coordinates": [17.0, 193.45, 349.95, 156.37, 28.47]}], "doi": ""}