{"title": "A Neural Corpus Indexer for Document Retrieval", "authors": "Yujing Wang; Yingyan Hou; Haonan Wang; Ziming Miao; Shibin Wu; Hao Allen Sun; Qi Chen; Yuqing Xia; Chengmin Chi; Guoshuai Zhao; Zheng Liu; Xing Xie; Weiwei Deng; Qi Zhang; Mao Yang", "pub_date": "2023-02-12", "abstract": "Current state-of-the-art document retrieval solutions mainly follow an indexretrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method.", "sections": [{"heading": "Introduction", "text": "Document retrieval and ranking are two key stages for a standard web search engine [56; 34]. First, the document retrieval stage retrieves candidate documents relevant to the query, and then, the ranking stage gives a more precise ranking score for each document. The ranking stage is often fulfilled by a deep neural network, taking each pair of query and document as input and predicting their relevance score. Nevertheless, a precise ranking model is very costly, while typically only a hundred or thousand candidates per query are affordable in an online system. As a result, the recall performance of the document retrieval stage is very crucial to the effectiveness of web search engines.\nExisting document retrieval methods can be divided into two categories, namely term-based and semantic-based approaches [22]. Term-based retrieval approaches [9; 59] build an inverted index for the entire web corpus, but they hardly capture document semantics and fail to retrieve similar documents in different wordings. Thus, semantic-based approaches [56; 36] are proposed to alleviate this discrepancy. First, they learn dense representations for both queries and documents through a twin-tower architecture; then Approximate Nearest Neighbor (ANN) search is applied to retrieve relevant documents for the designated query. Despite of their success in real applications, these approaches can not fully leverage the power of deep neural networks for the following reasons. First, a single embedding vector has limited capacity to memorize all semantics in a document, and it performs even worse than term-based methods in the applications that heavily rely on exact match [37]. Second, the model is unable to incorporate deep query-document interactions. Because ANN algorithms theoretically require a strong assumption for the Euclidean space, we have to adopt simple functions such as cosine similarity to capture the query-document interactions [20].\nGiven the above limitations, several research works have explored end-to-end models that directly retrieve relevant candidates without using an explicit index. Gao et al. [20] proposed a Deep Retrieval (DR) framework for item recommendation, which learned a retrievable structure with historical user-item interactions. Nevertheless, it is more challenging to design a universal model for semantic text retrieval, as we need to leverage the power of both pre-trained language models and deep retrieval networks simultaneously. Tay et al. [50] proposed Differentiable Search Index (DSI), a text-to-text model that maps queries directly to relevant docids. To the best of our knowledge, this is the first attempt to propose a differentiable index for semantic search. However, the vanilla transformer decoder in DSI does not fully leverage the hierarchical structures of document identifiers, and the model is pruned to over-fitting with limited training data. Furthermore, Bevilacqua et al. [4] proposed SEAL by leveraging all n-grams in a passage as its identifiers. But for long documents, it is hard to enumerate all possible n-grams. In general, the recall performance of end-to-end document retrieval remains a large room to be improved.\nIn this paper, we show that the traditional text retrieval frameworks can be fundamentally changed by a unified deep neural network with tailored designs. To this end, we propose a Neural Corpus Indexer (NCI), which supports end-to-end document retrieval by a sequence-to-sequence neural network. The model takes a user query as input, generates the query embedding through the encoder, and outputs the identifiers of relevant documents using the decoder. It can be trained by both ground-truth and augmented query-document pairs. During inference, the top N documents are retrieved via beam search based on the decoder. Designing and training such a model is non-trivial, so we propose several crucial techniques to ensure its effectiveness. First, to get sufficient query-document pairs for training, we leverage a query generation network to obtain possible pairs of queries and documents. Second, we utilize the hierarchical k-means algorithm to generate a semantic identifier for each document. Third, we design a prefix-aware weight-adaptive decoder to replace the vanilla one in a sequence-to-sequence architecture. Specifically, the same token will be assigned different embedding vectors at different positions in the identifiers, while another transformer-based adaptive module is applied to the classification weights for token prediction in the context of a certain prefix. This makes the classifiers customized to different prefixes when decoding along the hierarchical tree structure. Besides, a consistency-based regularization loss is taken for training both encoder and decoder networks to mitigate the over-fitting problem.\nOur NCI design solves the limitations of traditional index-retrieve pipelines from multiple perspectives. On one hand, a whole neural network model replaces the traditional inverted index or vector search solutions. It can be optimized end-to-end using realistic query-document pairs, which fully captures both term-based and semantic-based features and is adaptive to the changing of workloads. On the other hand, the model is able to capture deep interactions between queries and documents via the encoder-decoder attention, which enlarges the capacity of vector-based representations. Moreover, NCI achieves much better ranking results than ANN-based approaches as it is optimized directly by the final target. Thus, it can be served as an end-to-end retrieval solution while releasing the burden of re-ranking for a long candidate list.\nIn addition to the superior performance, the invention of Neural Corpus Indexer is also promising from the perspective of system design. As nowadays, ranking and query-answering modules are already implemented by neural networks, NCI finishes the last piece of puzzle for the next-generation information retrieval system based on a unified differentiable model architecture. This reduces the dependency among different sub-modules, while the processes of system deployment and maintenance could be greatly eased.\nOur contributions are highlighted as follows.\n\u2022 For the first time, we demonstrate that an end-to-end differentiable document retrieval model can significantly outperform both inverted index and dense retrieval solutions. This finding will inspire research on further steps towards the next-generation search systems, for instance, unifying informational retrieval, ranking, and question answering in a single differentiable framework.\n\u2022 We design a sequence-to-sequence model, named Neural Corpus Indexer (NCI), which generates relevant document identifiers directly for a specific query. In our experiments, the proposed NCI model improves the state-of-the-art performance of existing methods by a significant margin, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively. Also, NCI itself achieves a competitive MRR score without using an explicit ranking model. \u2022 We propose a novel decoder architecture, namely prefix-aware weight-adaptive (PAWA) decoder, to generate document identifiers. As verified by ablation studies, this invention is very crucial for NCI to achieve an outstanding performance. Moreover, query generation, semantic document identifiers, and consistency-based regularization are all accountable for the superior capability of Neural Corpus Indexer.", "publication_ref": ["b19", "b34", "b17", "b17", "b47", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "In this section, we briefly introduce the related works and leave more discussions in Appendix A.\nSparse retrieval. Traditional document retrieval methods are based on Sparse Retrieval, which is built upon inverted index with term matching metrics such as TF-IDF [45], query likelihood [33] or BM25 [44]. In industry-scale web search, BM25 is a difficult-to-beat baseline owing to its outstanding trade-off between accuracy and efficiency. In recent years, there are some attempts to incorporate the power of neural networks into inverted index. The Standalone Neural Ranking Model (SNRM) [57] learns high-dimensional sparse representations for query and documents, which enables the construction of inverted index for efficient document retrieval. Doc2Query [41] predicts relevant queries to augment the content of each document before building the BM25 index, and DocT5Query [40] improves the performance of query generation by the pre-trained language model T5 [5]. Furthermore, DeepCT [9] calculates context-aware term importance through neural networks to improve the term matching metrics of BM25.\nDense retrieval. Another line of research lies in Dense Retrieval, which presents query and documents in dense vectors and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain dense representations for queries and documents. At inference time, efficient Approximate Nearest Neighbor (ANN) search algorithms, such as k-dimensional trees [3], localitysensitive hashing [10], and graph-based indexes (e.g., HNSW [38], DiskANN [27] and SPANN [7]) can be utilized to retrieve relevant documents within a sublinear time. Besides, Luan et al. [37] analyze the limited capacity of dual encoders, and propose a combination of sparse and dense retrieval methods with multi-vector encoding to achieve better search quality.\nAutoregressive retrieval. The other way to approach retrieval is utilizing an end-to-end autoregressive model. Firstly, several efforts have been done on entity linking [13; 12; 11], which can be regarded as a special type of retrieval task, e.g., using an entity to ask the posed question. Recently, different from the entity linking task, Tay et al. [50] proposed the DSI (differentiable search index) model to generate relevant document identifiers directly corresponding to the query. Bevilacqua et al. [4] employed the autoregressive model to generate relevant words for a query and utilize the generated string to retrieve relevant documents. Besides, the Deep Retrieval (DR) [20] approach for recommendation is also related to this category, which learns a deep retrievable network with user-item clicks and gets rid of the ANN algorithms based on the Euclidean space assumption.\nPre-trained language models. Recently, pre-trained Language Models (LMs), such as BERT [14] and RoBERTa [35], have led to a revolution in web search techniques. The representation vectors for all documents can be calculated and indexed offline. In the online serving stage, it calculates the representation vector for the input query, and applies a crossing layer to calculate the relevance score between each query and document pair. The crossing layer usually adopts simple operators such as cosine similarity or a single feed-forward layer to retain a high efficiency. Gao et al. [16] found that a standard LMs' internal attention structure is not ready-to-use for dense encoders and proposed the Condenser to improve the performance of dense retrieval. Moreover, ANCE [54] leverages hard negatives to improve the effectiveness of contrastive learning, which generates better text representations for the retrieval tasks.", "publication_ref": ["b42", "b30", "b41", "b54", "b38", "b37", "b2", "b6", "b11", "b32", "b0", "b7", "b35", "b24", "b4", "b34", "b47", "b1", "b17", "b11", "b32", "b13", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Neural corpus indexer", "text": "The neural corpus indexer (NCI) is a sequence-to-sequence neural network model. The model takes a query as input and outputs the most relevant document identifier (docid), which can be trained by a large collection of <query, docid> pairs. The documents are encoded into semantic docids by the hierarchical k-means algorithm [23], which makes similar documents have \"close\" identifiers in the hierarchical tree. As shown in Figure 1, NCI is composed of three components, including Query Generation, Encoder, and Prefix-Aware Weight-Adaptive (PAWA) Decoder. Query generation is implemented by a sequence-to-sequence transformer model [52] that takes as input the document terms and produces a query as output [41]. The encoder, following the standard transformer architecture, is composed of M 1 stacked transformer blocks, which outputs the representation for an input query. For the decoder network, we stack M 2 transformer layers. To better align with the hierarchical nature of the semantic identifiers, we propose a weight adaptation mechanism based on another transformer to make the decoder aware of semantic prefixes. At inference time, the top N relevant documents can be easily obtained via beam search. Due to the hierarchical property of semantic identifiers, it is easy to constrain the beam search on the prefix tree so that only valid identifiers will be generated.", "publication_ref": ["b20", "b49", "b38"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Representing document with semantic identifiers", "text": "NCI generates document identifiers solely based on the input query without explicit document content, which is difficult when the size of the corpus is very large. Thus, we aim to inject useful priors into the identifiers so that the semantic information of documents can be incorporated in the decoding process.\nIn other words, we hope the documents with similar semantics have close docids to facilitate the learning process of NCI. To achieve this, we leverage the hierarchical k-means algorithm to encode documents. As shown in Figure 1(a), given a collection of documents to be indexed, all documents are first classified into k clusters by using their representations encoded by BERT [14]. For cluster with more than c documents, the k-means algorithm is applied recursively. For each cluster containing c documents or less, each document is assigned a number starting from 0 to at most c-1. In this way, we organize all documents into a tree structure T with root r 0 . Each document is associated with one leaf node with a deterministic routing path l = {r 0 , r 1 , ..., r m } from the root, where r i \u2208 [0, k) represents the internal cluster index for level i, and r m \u2208 [0, c) is the leaf node. The semantic identifier for a document is concatenated by the node indices along the path from root to its corresponding leaf node. For documents with similar semantics, the prefixes of their corresponding identifiers are likely to be the same. For simplicity, we set k = 30 and c = 30 in all experiments, leaving the optimization of these hyper-parameters to future work. The detailed procedure of hierarchical k-means will be described in Algorithm 1 in the Appendix B.2.", "publication_ref": ["b11"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Query generation", "text": "One challenge of generating document identifiers by single query input is how to make the identifiers aware of the document semantics. Since the content of each document is not explicitly known at inference, it must be incorporated into the model parameters during training. To facilitate the training process, we generate a bunch of queries with a query generation module and bind the information of document content through training the sequence-to-sequence model with generated queries and their corresponding document identifiers. In NCI, we utilize two kinds of augmented queries:\nDocT5Query. We adopt a standard sequence-to-sequence transformer [52] based on the implementation of DocT5Query [1] pre-trained by a large query-document corpus. It takes as input the document terms and produces relevant queries via random sampling. Note that we use random sampling instead of beam search to ensure the diversity of generated queries.\nDocument As Query. Like DSI [50], we also utilize the first 64 terms for each document as queries.\nBesides, we randomly selected 10 groups of 64 consecutive terms from the whole article as additional queries. This makes the NCI model aware of the semantic meaning of each document.", "publication_ref": ["b49", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Prefix-aware weight-adaptive decoder", "text": "Given an input query x, the probability of generating a document identifier can be written as:\np(l|x, \u03b8) = m i=1 p(r i |x, r 1 , r 2 , ..., r i\u22121 , \u03b8 i ),(1)\nwhere r i is the i-th token in the current identifier; x is the representation output from encoder; \u03b8 denotes the total parameters and \u03b8 i is the parameter for the i-th step. This probability can be modeled by a transformer-based decoder. For an internal node with level i, the probability is calculated by:\nh i = TransformerDecoder(x, h 1 , h 2 , ..., h i\u22121 ; \u03b8 i ),(2)\np(r i |x, r 1 , r 2 , ..., r i\u22121 , \u03b8 i ) = Softmax(h i W ).(3)\nHere h i is the hidden representation for step i, which is calculated by a multi-head attention over encoder representation x and token representations of previous decoding steps. The linear classification weight is denoted by W \u2208 R d\u00d7v , d is the hidden dimension size and v is the vocabulary size of identifiers.\nAs the encoder and decoder utilize distinct vocabulary spaces, we do not share the embedding space for their tokens. Different from a standard decoding task, the meanings of the same token appearing at different places of the same identifier are different, as they correspond to different clusters in the hierarchical tree structure. For instance, the \"5 2 \" and \"5 3 \" of the same identifier \"3 1 5 2 5 3 \" correspond to different semantic meanings. Moreover, the same token in the same position may have different semantics with different prefixes. For example, in identifiers \"1 1 1 2 5 3 \" and \"2 1 4 2 5 3 \", the same token \"5 3 \" has different semantics in two different identifiers, as they are routed from different prefix paths. These two properties of the hierarchical semantic identifiers motivate us to design the novel Prefix-Aware Weight-Adaptor (PAWA) decoder.\nUnlike a standard transformer decoder, the probabilities at different tree levels, such as p(r i |x, r 1..i\u22121 , \u03b8 i ) and p(r j |x, r 1..j\u22121 , \u03b8 j ) where i = j, do not share parameters with each other.\nTo distinguish different semantic levels, we concatenate the position and token values as input for each decoding step, as shown in the left corner of Figure 2. Specifically, we have \"(1, 3)(2, 5)(3, 5)\" for the semantic identifier \"3 1 5 2 5 3 \", while \"(2, 5)\" and \"(3, 5)\" represent different tokens in the vocabulary space. As the token embedding and linear classification layers share the same weights, the same token value in different positions would correspond to different model parameters. Moreover, to reflect the influence of different prefixes, we expect the linear classification layer to be aware of different prefixes for predicting a specific token. Concretely, instead of using the same projection weight W in the linear classification layer, we employ the prefix-aware adaptive weights for each token classifier, which can be calculated by another transformer decoder,\nW i ada = AdaptiveDecoder(e; r 1 , r 2 , ..., r i\u22121 )W i (4\n)\nwhere e is the query embedding vector taken as initial input to the transformer decoder; {r t |t \u2208 (1, 2, ..., i \u2212 1)} are prefix tokens before the i-th position, AdaptiveDecoder stacks M 3 transformer decoding layers with dimension d, and W i ada \u2208 R d\u00d7v is the adapted weight matrix for the corresponding classifier. Finally, the i-th token in the given prefix can be predicted by Softmax(h i W i ada ). For instance, to predict the third tokens in the identifiers \"(1,3)(2,1)(3,5)\" and \"(1,2)(2,4)(3,5)\", respectively, the corresponding adaptive weights are derived separately for different prefixes, i.e., \"(1,3)(2,1)\" and \"(1,2)(2,4)\". As we already know the previous tokens for each position in the teacher forcing setting, the prefix-aware adaptive weights can be calculated and trained in parallel in different positions while adding little burden to the entire model.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Training and inference", "text": "Consistency-based regularization. To alleviate over-fitting, we employ a consistency-based regularization loss for training each decoding step. Given an input query q, we denote the decoder representations by two forward passes with independent dropouts before Softmax as z i,1 = D(r i |E(q), r 1,...,i\u22121 , \u03b8 i ) and z i,2 = D(r i |E(q), r 1,...,i\u22121 , \u03b8 i ), respectively, where E(\u2022) denotes the encoder network and D(\u2022) denotes the decoder network. The consistency-based regularization loss tries to distinguish the representations from the same token from those of other tokens, like contrastive learning [8]. The regularization loss of query q for the i-th decoding step is defined as,\nLreg = \u2212 log exp (sim(zi,1, zi,2)/\u03c4 ) 2Q k=1,k =2 exp (sim((zi,1, z i,k )/\u03c4 )(5)\nwhere we leverage dot-product for sim(\u2022); Q is the number of queries in the batch, and the temperature parameter is set as \u03c4 = 1 in all the experiments.\nTraining loss. Given a set of training examples D = {(q, d)} composed of queries (training queries and augmented queries) and document identifiers, the loss function can be written as follows:\nL(\u03b8) = (q,d)\u2208D log p(d|E(q), \u03b8) + \u03b1Lreg ,(6)\nwhere p(d|E(q), \u03b8) denotes the probability of generating d with q as the input. The first part is the seq2seq cross-entropy loss with teacher forcing and the second part is the consistency-based regularization loss summed by all decoding steps. The whole process formulates a sequence-tosequence neural network, which can be optimized end-to-end via gradient descent. The hyperparameter \u03b1 denotes a scaling factor of regularization loss, which will be analyzed in Section 4.4.\nInference via beam search. In the inference stage, we calculate the query embedding through the encoder network and then perform beam search on the decoder network. Due to the hierarchical nature of docid, it is convincing to constrain the beam search decoding process with a prefix tree, which in turn only generates the valid identifiers. The time complexity of beam search is O(LBF ), where L is the max length of identifiers (the depth of tree), B is the beam size and F is the max fanout of the tree (30 in our experiments). Given a balanced tree structure built by a corpus with N documents, the average time complexity for beam search is O(BlogN ). We leave detailed descriptions of the constrained beam search algorithm in Appendix B.3.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we empirically verify the performance of NCI and the effectiveness of each component on the document retrieval task, which generates a ranking list of documents in response to a query. In the following, we discuss the datasets and evaluation protocols in Section 4.1, describe the implementation details and baseline methods in Section 4.2, and present empirical results and analyses in Section 4.3 and 4.4, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets & evaluation metrics", "text": "Datasets. We conduct our experiments on two popular benchmarks for document retrieval, i.e., the Natural Questions [32] and TriviaQA dataset [29]. Natural Questions (NQ) [32] was introduced by Google in 2019. The version we use is often referred to as NQ320k, which consists of 320k query-document pairs, where the documents are gathered from Wikipedia pages and the queries are natural language questions. We use its predetermined training and validation split for evaluation.\nTriviaQA is a reading comprehension dataset [29], which includes 78k query-document pairs from the Wikipedia domain. Unlike the NQ320k dataset, a query may include multiple answers in TriviaQA.\nMetrics. We use widely accepted metrics for information retrieval, including Recall@N , Mean Reciprocal Rank (MRR) and R-precision. Recall@N measures how often the desired document is hit by the top-N retrieved candidates. MRR calculates the reciprocal of the rank at which the first relevant document is retrieved. R-Precision is the precision after R documents have been retrieved, where R is the number of relevant documents for the query. A high recall means that the ground truth document is contained in the retrieved candidate list, while a high MRR indicates that the corresponding document has already been ranked at the top position without re-ranking.", "publication_ref": ["b29", "b26", "b29", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation details", "text": "Hierarchical semantic identifier. For semantic identifiers, we apply a hierarchical k-means algorithm over the document embeddings obtained through a 12-layers BERT model with pre-trained parameters (provided by HuggingFace [53]). For each hierarchical layer, we employ the default k-means algorithm implemented in scikit-learn [42] with k = 30. For simplicity, the recursion terminal condition is also set as c = 30.\nQuery generation. We leverage the pre-trained model, DocT5Query [40], for query generation.\nWe provide all document contents in NQ320k and TriviaQA datasets to predict augmented querydocument pairs. For each document, we generate 15 queries with the first 512 tokens of the document as input and constrain the maximum length of the generated query as 64.\nTraining and inference. The Neural Corpus Indexer is implemented with python 3.6.10, PyTorch 1.8.1 and HuggingFace transformers 3.4.0. We utilize the parameters of the T5 pre-trained model [5] to initialize the encoder and randomly initialize the PAWA decoder. All NCI experiments are based on a learning rate 2 \u00d7 10 \u22124 for the encoder and 1 \u00d7 10 \u22124 for the decoder with a batch size 16 per GPU. We set the scaling factor of the consistency-based regularization loss as \u03b1 = 0.15 and the dropout ratio as 0.1. For inference, we apply the partial beam search algorithm to the trained seq2seq model. We set the length penalty and the beam size as 0.8 and 100, respectively. All experiments are based on a cluster of NVIDIA V100 GPUs with 32GB memory. Each job takes 8 GPUs, resulting in a total batch size of 128 (16 \u00d7 8).\nBaselines. We evaluate BM25 on both raw documents and those augmented by DocT5Query by an open-source implementation [2]. The performance of DSI [49] is referred from its original paper as the implementation has not been officially open-sourced. To avoid the difference in data processing, we reproduce SEAL [4] and ANCE [54] by their official implementations. Some baselines for the TriviaQA dataset are directly referred from [58]. We leave the detailed settings in Appendix B.4.", "publication_ref": ["b50", "b39", "b37", "b2", "b46", "b1", "b51", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "In Table 1 and 2, we compare the empirical results of NCI and corresponding baselines on two benchmarks. We report NCI models based on T5-Base, T5-Large, and ensemble architectures. One can see that even with the T5-Base architecture, NCI outperforms all baselines by a significant margin across four different metrics on both the NQ320k and TriviaQA datasets. Furthermore, an ensemble of five NCI models also brings a large enhancement, because each model is trained individually with a separate semantic identifier generated by a random k-means initialization, making the models complementary to each other. Expect for NCI, SEAL achieves the second best performance. This verifies the superiority of deep text retrieval over traditional sparse and dense retrieval methods. Comparing to SEAL, NCI improves 17.6% for Recall@1, 10.0% for Recall@10, 3.2% for Recall@100, and 14.9% for MRR@100 on the NQ320k dataset. We find that the generated queries have different distributions with the training queries , so we also fine-tune Doc2Query on this dataset for a comparison (denoted by w/ qg-ft). Finally, we achieve 72.78% for Recall@1, outperforming SEAL by 21.4%. On the TriviaQA dataset, NCI obtains 7.9% improvement for Recall@5, 5.5% for Recall@20, 6.0% for Recall@100, and 16.8% for R-Precision. As shown in ablation studies, these improvements are owning to the novel designs of PAWA decoder, query generation, semantic identifiers, and consistency-based regularization. We also notice that query generation plays a key role in boosting the retrieval performance. With query generation, the BM25 + DocT5Query method achieves higher performance than the vanilla BM25, especially on the NQ320k dataset. ANCE achieves competitive performance after fine-tuned by the training pairs, but the performance is relatively lower than our NCI model. Moreover, the MRR@100 and R-Precision metrics of NCI are outstanding, indicating that 80% of the queries can be fulfilled without re-ranking on the retrieved document list. This demonstrates the potential of NCI to be served as an end-to-end solution that replaces the entire index-retrieve-rank pipeline in traditional web search engines.\nFurthermore, to study the effect of each component, we report ablation results on both NQ320k and TriviaQA datasets in Table 3. In general, all five components are able to improve the performance of document retrieval, which are detailed below.\nw/o DocT5Query. This configuration removes the training queries generated by DocT5Query.\nAccording to the results, the query generation model greatly boosts the performance. The result is  w/o document as query. Similar to DSI [49], using the document contents as queries also makes the model aware of the semantics of documents.\nw/o PAWA decoder. This configuration removes the adaptive decoder layer in Equation ( 4) and leverages shared weights with token embedding for the linear classification layer. We notice that the prefix-aware weight-adaptive decoder has a noticeable influence on the performance, which indicates that, instead of borrowing the vanilla transformer decoder, it is necessary to design a tailored decoder architecture for the task of semantic identifier generation.  w/o semantic id. This configuration replaces the semantic identifier of each document to a randomly generated one. We find a relative drop in the model performance on all four metrics, demonstrating that the semantic identifiers derived by the hierarchical k-means have injected useful priors. We conjecture that the performance enhancement would be more significant on a larger document corpus.\nw/o regularization. There is a performance drop on all four metrics without using consistency-based regularization loss. The reason is that the decoder network is prone to over-fitting. By making the prediction results of two augmented queries consistent, the decoder will become more generalizable and resistant to over-fitting.\nw/o constrained beam search. This configuration disables the validating constraint in beam search.\nIn other words, the decoder network does not have a tree-based prior structure. Instead, all tokens in the vocabulary can be generated in each decoding step. We observe a performance drop on four evaluation metrics. This indicates that it is difficult to remember all information of valid identifiers in the network, and an explicit prior could be helpful for improving the quality of beam search. ", "publication_ref": ["b46"], "figure_ref": [], "table_ref": ["tab_0", "tab_2"]}, {"heading": "Analysis", "text": "Model capacity. Figure 3 compares the learning curves of NCI with different model capacities, which are identical to the small, base, and large settings of ordinary T5 [43]. We observe that with the increase of model size, NCI convergences more quickly with fewer epochs. At convergence, the small model achieves a relatively lower recall. Instead, both the base and large models achieve similar results after sufficient training epochs, and the large model will be slightly higher. This implies that the model capacity has a critical impact on the retrieval performance, and the capacity of base model seems to be enough to memorize all documents in NQ320k and TriviaQA datasets. The large model can be used when the computation capacity is sufficient. For a larger corpus, one may need to increase the model size to obtain satisfactory performance.\nLayer number of PAWA adapter. We study the influence of the number of transformer layers in the PAWA adapter and choose the layer number from {0,1,2,4,6,8}. The results are summarized in Table 4. We notice that with the increase of layer number, i.e. from 0 to 4, the overall performance is consistently improved on four metrics. But when the number of layers achieves 6, the performance decreases. When continuing to increase the number of layers to 8, the performance drops significantly. We attribute that to the overfitting issue caused by a large PAWA decoder. Therefore, we adopt the PAWA decoder with a 4-layers adapter in NCI.\nRetrieved documents and their semantics identifiers. To verify the effectiveness of retrieval as well as the semantic identifiers learned by the hierarchical k-means, we analyze the retrieval results of NCI for some exemplar queries. To illustrate, we select four queries denoted by A-1, A-2, B-1 and B-2, where two queries inside the same group are semantically similar, and the queries in different groups correspond to distinct topics. In Figure 4, we show the probabilities of retrieved documents for each query in group A and B, respectively. The digits along x-axis denote the four-bit prefixes for the semantic identifiers of retrieved documents, and the y-axis stands for their probabilities. We notice that similar queries result in close document distributions, while dissimilar queries in different groups result in un-overlapped document collections. In addition, the documents retrieved by the same group of queries have close prefixes for the identifiers, e.g., 6030, 6032, 6033, 6034 in group A and 7511, 7514, 7516 in group B. Also, we visualize the BERT-based document embeddings by t-SNE [51] in Figure 4, in which each color represents the corresponding documents for a specific query. As shown in the figure, these documents naturally form two clusters with respect to different query groups. Thus, we conclude that the semantic document identifiers generated by the hierarchical k-means algorithm have positive effects on the retrieval performance. Efficiency Analysis. We use an NVIDIA V100-32G GPU to analyze the efficiency of NCI. As the inference speed is influenced by both model capacity and beam size, we report the latency and throughput measures for multiple settings in Table 5. As NCI is an end-to-end retrieval method and achieves competitive performance without re-ranking, the latency and throughput are already affordable for some near-real-time applications. The latency of NCI is on par with DSI and SEAL using the same model size and beam size, because all of them conduct beam search based on transformer decoders. BM25 is very efficient (<100ms per query on CPU using an open-source implementation [2]), but the recall metrics are much lower. Furthermore, we can leverage other techniques to improve the efficiency of NCI, which will be discussed in the later section.", "publication_ref": ["b40", "b48"], "figure_ref": ["fig_2", "fig_3", "fig_3"], "table_ref": ["tab_3", "tab_4"]}, {"heading": "Limitation & Future Works", "text": "Despite the significant breakthrough, the current implementation of NCI still suffers from several limitations before deployment in a large-scale search system. Firstly, it requires a much larger model capacity for extending NCI to the web scale. Secondly, the inference speed needs to be improved to serve online queries in real time. Thirdly, it is difficult to update the model-based index when new documents are added to the system. In future works, we may tackle these problems from four aspects.\n(1) The architecture of sparsely-gated Mixture of Expert (MoE) [47] can be employed to enhance the model capacity.\n(2) Documents can be grouped into semantic clusters, and NCI can be used to retrieve relevant cluster identifiers. In this way, all documents in relevant clusters can be retrieved efficiently. (3) Model compression techniques, like weight quantization [26] and knowledge distillation [24], can be further taken to speed up inference. (4) We plan to explore a hybrid solution by building another index that serves new documents through traditional indexing algorithms.", "publication_ref": ["b44", "b23", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we introduce a novel document retrieval paradigm that unifies the training and indexing stages by an end-to-end deep neural network. The proposed Neural Corpus Indexer (NCI) directly retrieves the identifiers of relevant documents for an input query, which can be optimized end-to-end using augmented query-document pairs. To optimize the recall and ranking performance, we invent a tailored prefix-aware weight-adaptive decoder. Empirically, we evaluate NCI on NQ320k and TriviaQA datasets, demonstrating its outstanding performance over state-of-the-art solutions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Related work", "text": "Traditional web search techniques follow a two-stages paradigm including document retrieval and document ranking. The first stage aims to select a collection of documents relevant to a given query, which requires an ingenious trade-off between efficiency and recall. Then, the document ranking stage takes more advanced features and deeper models to calculate a fine-grained ranking score for each query and document pair. In the following, we first discuss related works for document retrieval and ranking respectively. Afterwards, we introduce recent works that incorporate pre-trained language models into these two stages. At last, the attempts on end-to-end retrieval will be discussed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Document retrieval", "text": "Traditional document retrieval methods are based on Sparse Retrieval, which is built upon inverted index with term matching metrics such as TF-IDF [45], query likelihood [33] or BM25 [44]. In industry-scale web search, BM25 is a difficult-to-beat baseline owing to its outstanding trade-off between accuracy and efficiency. In recent years, there are some attempts to incorporate the power of neural networks into inverted index. The Standalone Neural Ranking Model (SNRM) [57] learns high-dimensional sparse representations for queries and documents, which enables the construction of inverted index for efficient document retrieval. Doc2Query [41] predicts relevant queries to augment the content of each document before building the BM25 index, and DocT5Query [40] improves the performance of query generation by the pre-trained language model T5 [5]. Furthermore, DeepCT [9] calculates context-aware term importance through neural networks to improve the term matching metrics of BM25.\nAnother line of research lies in Dense Retrieval, which presents query and documents in dense vectors and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain dense representations for queries and documents. At inference time, efficient Approximate Nearest Neighbor (ANN) search algorithms, such as k-dimensional trees [3], locality-sensitive hashing [10], and graph-based indexes (e.g., HNSW [38], DiskANN [27] and SPANN [7]) can be utilized to retrieve relevant documents within a sublinear time. Besides, Luan et al. [37] analyze the limited capacity of dual encoders, and propose a combination of sparse and dense retrieval methods with multi-vector encoding to achieve better search quality.", "publication_ref": ["b42", "b30", "b41", "b54", "b38", "b37", "b2", "b6", "b11", "b32", "b0", "b7", "b35", "b24", "b4", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Document ranking", "text": "Document ranking has been extensively studied in recent years and experienced a huge improvement with the booming of deep neural networks. Neural network-based document ranking models mainly fall into two categories. Representation-based models like DSSM (Deep Structured Semantic Model) [25] and CDSSM (a convolution-based variant of DSSM) [48] represent query and document in a shared semantic space and model their semantic similarity through a neural network. In contrast, Interaction-based models first build interactions between query and document terms, and then utilizes neural networks to learn hierarchical interaction patterns. For example, DRMM (Deep Relevance Matching Model) [21] extracts interactive features by matching histograms and utilizing a feed forward network with term-gating mechanism to calculate the relevance score of a query-document pair.", "publication_ref": ["b22", "b45", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Pre-trained language models", "text": "Recently, Pre-trained Language Models (PLMs) like BERT [14] have led to a revolution of web search techniques. The vanilla BERT model utilizes a single-tower architecture that concatenates query and document tokens as a whole input to the relevance model. Despite of its superior performance, the high computational cost hinders its application to industrial-scale web search systems. TwinBERT [36] tackles this problem by exploiting a Siamese architecture, where queries and documents are first modeled by two BERT encoders separately, and then an efficient crossing layer is adopted for relevance calculation. The representation vectors for all documents can be calculated and indexed offline. In the online serving stage, it calculates the representation vector for the input query and applies a crossing layer to calculate the relevance score between each query and document. The crossing layer usually adopts simple similarity functions such as dot product or a single feed-forward layer to achieve a high efficiency.\nMoreover, Chang et al. [6] argue that the Masked Language Model (MLM) loss designed for BERT pre-training is not naturally fitted to embedding-based retrieval tasks. Instead, they propose three paragraph-level pre-training tasks, i.e., Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP), which demonstrate promising results in text retrieval experiments. Gao et al. [16] find that a standard LMs' internal attention structure is not ready-to-use for dense encoders. Thus, they propose a novel architecture named Condenser to improve the performance of dense retrieval. ANCE (Approximate nearest neighbor Negative Contrastive Estimation) [54] leverages hard negatives to improve the effectiveness of contrastive learning, which generates better text representations for the retrieval task.", "publication_ref": ["b11", "b33", "b3", "b13", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "A.4 End-to-end retrieval", "text": "The deficiency of index-retrieve paradigm lies in that the two stages of document retrieval and re-ranking are optimized separately. Especially, the document retrieval procedure is often sub-optimal and hinders the performance of the entire system. Thus, there are some recent attempts to achieve endto-end retrieval as a one-stage solution. ColBERT [31] introduces a contextualized late interaction architecture, which independently encodes query and document through BERT, and performs crossterm interaction based on the contextualized representations of query and document terms. ColBERT supports end-to-end retrieval directly from a large document collection by leveraging vector-similarity indexes in the pruned interaction layer. It can be viewed as a compromise between single-tower and twin-tower BERT architectures which maintains an effective trade-off between accuracy and latency. Moreover, the Contextualized Inverted List (COIL) [19] exacts lexical patterns from exact matching pairs through contextualized language representations. At search time, we build representation vectors for query tokens and perform contextualized exact match to retrieve relevant documents based on inverted index.\nAlthough ColBERT and COIL have shown promising results in end-to-end retrieval tasks without re-ranking, their performance is still not obviously better (if not worse) than a common practice of \"BM25 indexer + BERT re-ranker\", and their efficiency is also not good enough for an industrial web search engine. Therefore, we resort to a new indexing paradigm to break the bottleneck. We believe the neural corpus indexer proposed in this paper is a crucial break-through, opening up new opportunities to optimize the performance of web-scale document retrieval. Moreover, there are a few attempts that try to build a model-based search index by directly predicting document identifiers. Tay et al. [50] proposed the DSI (differentiable search index) model based on an encoder-decoder architecture to generate relevant docids. However, its decoder architecture remains the same as T5, which is unsuitable to generate semantic ids derived by hierarchical k-means. SEAL [4] uses all n-grams in a passage as its possible identifiers and build a FM-Index to retrieve documents; but it is hard to enumerate all n-grams for retrieving relevant documents. In addition, our work is related to Deep Retrieval [20] for the recommendation task, which learns a deep retrievable network with user-item clicks without resorting to ANN algorithms constrained by the Euclidean space assumption.", "publication_ref": ["b28", "b16", "b47", "b1", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "B Reproducibility", "text": "We provide our code for reproduction in the supplementary material. We will release it to public shortly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Dataset processing", "text": "We conduct experiments on NQ320k and TriviaQA datasets. For NQ320k dataset, the queries are natural language questions and the documents are Wikipedia articles in HTML format. During dataset processing, we first filter out useless HTML tag tokens, and extract title, abstract and content strings of each Wikipedia article using regular expression. The experiments are also conducted on TriviaQA dataset. For TriviaQA dataset, it includes 78k query-document pairs from the Wikipedia domain, which are processed almost the same as NQ320k. Then, we detect duplicated articles based on the title of each article. After that, we concatenate the title, abstract and content strings of each Wikipedia article, and apply a 12 layers pre-trained BERT model on it to generate document embeddings. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Miscellaneous", "text": "Social Impacts. This work aims at introducing a new learning paradigm that can unify the learning and indexing stages with an end-to-end deep neural network. Besides, our work has the potential to inspire more attempts at unifying the retrieval and re-ranking task with an end-to-end framework, which might have positive social impacts. We do not foresee any form of negative social impact induced by our work.\nPrivacy Information in Data. We use the NQ dataset privided by the work [32]. The dataset only includes questions, rendered Wikipedia pages, tokenized representations of each page, and the annotations added by our annotators. No privacy information is included. For the TriviaQA [29], which is a reading comprehension dataset, it includes 78k query-document pairs from the Wikipedia domain. Again, no privacy information is included.", "publication_ref": ["b29", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Finally, hierarchical k-means is applied on the article embeddings to produce semantic identifiers for each article.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Hierarchical k-means for semantic identifier", "text": "The pseudo code of hierarchical k-means is detailed in in Algorithm 1. Hierarchical semantic identifier L 1:N Function:\nGenerateSemanticIdentifier(X 1:N )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Constrained beam search", "text": "The pseudo code of constrained beam search is detailed in Algorithm 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.4 Baselines", "text": "We describe the baseline methods in this section. For most of them, we use their official open-source implementations.\n\u2022 BM25. BM25 is currently the mainstream algorithm for calculating the similarity score between query and document in information retrieval [44]. We calculate BM25 between an original query Q and a document d which derived from a sum of contributions from each query term q i as,\nwhere w i denotes the weight of q i , and R(q i , d) is the correlation between q i and d. We use the open-source implementation from Rank-BM25 2 . \u2022 BM25 + DocT5Query. The docT5Query model [40] generates questions that related to a document. These predicted queries are then appended to the original documents, which are then indexed. Note that we use the same predicted queries in our query generation module. Queries are issued against the index as \"bag of words\" queries, using BM25 for evaluation. We use the open-source code for DocT5Query 3 , and the generated queries keep the same with NCI (our model) to have a fair comparison. k documents with the highest probabilities Function:\nif prefix.last().isLeaf() then doc_id = prefix.toString() ResultIds.add( doc_id, sum_log_prob/len(prefix) ) else for r i \u2208 prefix.last().child() do new_prefix = prefix.copy().append(r i )", "publication_ref": ["b41", "b37", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "rank_by_prob().top(k) end for return ResultIds.rank_by_prob().top(k)", "text": "\u2022 BERT + ANN (Faiss). We use the Flat Index method with the query and document representations obtained by CoCondenser 4 [16] which is pretrained on Wikipedia and then finetuned over NQ dataset. For the Flat Index method, we use the version implemented by Faiss 5 . \u2022 BERT + BruteForce. In this baseline, we use the CoCondenser [16], pretrained on Wikipedia and then finetuned over NQ dataset, to encode queries and documents separately. Then, the Cosine Similarity is computed for each query and document pair. After that, for each query, the documents with the largest Cosine Similarity score are retrieved. \u2022 ANCE (MaxP & FirstP). ANCE, a training mechanism, that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus [54]. For BERT FirstP, we concatenate the title and content of each document by a [SEP] token. For BERT MaxP, we only use the content of each document. We use the open-source implementation 6 . \u2022 SEAL (BART-Large). We reproduce SEAL based on the open-sourced implementation 7 .\n\u2022 DSI. The DSI model learns a text-to-text model that maps string queries directly to relevant docids [50]. We report the performance of DSI (T5-Base), DSI (T5-Large) and DSI (T5-XXL) from its original paper as the implementation has not been open-sourced.", "publication_ref": ["b13", "b13", "b51", "b3", "b4", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "C More Experimental Results", "text": "We study the influence of regularization strength and choose the regularization hyper-parameter \u03b1 from {0, 0.1, 0.15, 0.2, 0.3}. Table 6 summaries the results with different regularization hyperparameter \u03b1 settings. At convergence, the hyper-parameter \u03b1 = 0.15 generally achieves better performance. Therefore, we set the default value as \u03b1 = 0.15 in NCI.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Multidimensional binary search trees used for associative searching", "journal": "Communications of the ACM", "year": "1975", "authors": "Jon Louis Bentley"}, {"ref_id": "b1", "title": "Autoregressive search engines: Generating substrings as document identifiers", "journal": "", "year": "2022", "authors": "Michele Bevilacqua; Giuseppe Ottaviano; Patrick Lewis; Wen-Tau Yih; Sebastian Riedel; Fabio Petroni"}, {"ref_id": "b2", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "Benjamin Tom B Brown; Nick Mann; Melanie Ryder; Jared Subbiah; Prafulla Kaplan; Arvind Dhariwal; Pranav Neelakantan; Girish Shyam; Amanda Sastry;  Askell"}, {"ref_id": "b3", "title": "Pre-training tasks for embedding-based large-scale retrieval", "journal": "", "year": "2019", "authors": "Wei-Cheng Chang; Yu Felix; Yin-Wen Chang; Yiming Yang; Sanjiv Kumar"}, {"ref_id": "b4", "title": "Highly-efficient billion-scale approximate nearest neighbor search", "journal": "", "year": "2021", "authors": "Qi Chen; Bing Zhao; Haidong Wang; Mingqin Li; Chuanjie Liu; Zengzhong Li; Mao Yang; Jingdong Wang;  Spann"}, {"ref_id": "b5", "title": "A simple framework for contrastive learning of visual representations", "journal": "PMLR", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b6", "title": "Context-aware sentence/passage term importance estimation for first stage retrieval", "journal": "", "year": "2019", "authors": "Zhuyun Dai; Jamie Callan"}, {"ref_id": "b7", "title": "Locality-sensitive hashing scheme based on p-stable distributions", "journal": "", "year": "2004", "authors": "Mayur Datar; Nicole Immorlica; Piotr Indyk; S Vahab;  Mirrokni"}, {"ref_id": "b8", "title": "Highly parallel autoregressive entity linking with discriminative correction", "journal": "", "year": "2021", "authors": "Nicola De Cao; Wilker Aziz; Ivan Titov"}, {"ref_id": "b9", "title": "Autoregressive entity retrieval", "journal": "", "year": "2020", "authors": "Nicola De Cao; Gautier Izacard; Sebastian Riedel; Fabio Petroni"}, {"ref_id": "b10", "title": "Multilingual autoregressive entity linking", "journal": "", "year": "2021", "authors": "Nicola De Cao; Ledell Wu; Kashyap Popat; Mikel Artetxe; Naman Goyal; Mikhail Plekhanov; Luke Zettlemoyer; Nicola Cancedda; Sebastian Riedel; Fabio Petroni"}, {"ref_id": "b11", "title": "Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova Bert"}, {"ref_id": "b12", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b13", "title": "Condenser: a pre-training architecture for dense retrieval", "journal": "", "year": "2021", "authors": "Luyu Gao; Jamie Callan"}, {"ref_id": "b14", "title": "Is your language model ready for dense representation fine-tuning", "journal": "", "year": "2021", "authors": "Luyu Gao; Jamie Callan"}, {"ref_id": "b15", "title": "Unsupervised corpus aware language model pre-training for dense passage retrieval", "journal": "", "year": "2021", "authors": "Luyu Gao; Jamie Callan"}, {"ref_id": "b16", "title": "Coil: Revisit exact lexical match in information retrieval with contextualized inverted list", "journal": "", "year": "2021", "authors": "Luyu Gao; Zhuyun Dai; Jamie Callan"}, {"ref_id": "b17", "title": "Deep retrieval: Learning a retrievable structure for large-scale recommendations", "journal": "", "year": "2020", "authors": "Weihao Gao; Xiangjun Fan; Chong Wang; Jiankai Sun; Kai Jia; Wenzhi Xiao; Ruofan Ding; Xingyan Bin; Hui Yang; Xiaobing Liu"}, {"ref_id": "b18", "title": "A deep relevance matching model for ad-hoc retrieval", "journal": "", "year": "2016", "authors": "Jiafeng Guo; Yixing Fan; Qingyao Ai; W Bruce Croft"}, {"ref_id": "b19", "title": "A comparison between term-based and embedding-based methods for initial retrieval", "journal": "Springer", "year": "2018", "authors": "Tonglei Guo; Jiafeng Guo; Yixing Fan; Yanyan Lan; Jun Xu; Xueqi Cheng"}, {"ref_id": "b20", "title": "Algorithm as 136: A k-means clustering algorithm", "journal": "Journal of the royal statistical society. series c (applied statistics)", "year": "1979", "authors": "A John;  Hartigan; A Manchek;  Wong"}, {"ref_id": "b21", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b22", "title": "Learning deep structured semantic models for web search using clickthrough data", "journal": "", "year": "2013", "authors": "Po-Sen Huang; Xiaodong He; Jianfeng Gao; Li Deng; Alex Acero; Larry Heck"}, {"ref_id": "b23", "title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference", "journal": "", "year": "2018", "authors": "Benoit Jacob; Skirmantas Kligys; Bo Chen; Menglong Zhu; Matthew Tang; Andrew Howard; Hartwig Adam; Dmitry Kalenichenko"}, {"ref_id": "b24", "title": "Diskann: Fast accurate billion-point nearest neighbor search on a single node", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "Fnu Suhas Jayaram Subramanya;  Devvrit; Ravishankar Harsha Vardhan Simhadri; Rohan Krishnawamy;  Kadekodi"}, {"ref_id": "b25", "title": "Billion-scale similarity search with GPUs", "journal": "IEEE Transactions on Big Data", "year": "2019", "authors": "Jeff Johnson; Matthijs Douze; Herv\u00e9 J\u00e9gou"}, {"ref_id": "b26", "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension", "journal": "", "year": "2017", "authors": "Mandar Joshi; Eunsol Choi; S Daniel; Luke Weld;  Zettlemoyer"}, {"ref_id": "b27", "title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"ref_id": "b28", "title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert", "journal": "", "year": "2020", "authors": "Omar Khattab; Matei Zaharia"}, {"ref_id": "b29", "title": "Natural questions: a benchmark for question answering research", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin; Jacob Devlin; Kenton Lee"}, {"ref_id": "b30", "title": "Document language models, query models, and risk minimization for information retrieval", "journal": "", "year": "2001", "authors": "John Lafferty; Chengxiang Zhai"}, {"ref_id": "b31", "title": "Parade: Passage representation aggregation for document reranking", "journal": "", "year": "2020", "authors": "Canjia Li; Andrew Yates; Sean Macavaney; Ben He; Yingfei Sun"}, {"ref_id": "b32", "title": "Roberta: A robustly optimized BERT pretraining approach. CoRR, abs", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b33", "title": "Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval", "journal": "", "year": "2020", "authors": "Wenhao Lu; Jian Jiao; Ruofei Zhang"}, {"ref_id": "b34", "title": "Sparse, dense, and attentional representations for text retrieval", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Yi Luan; Jacob Eisenstein; Kristina Toutanova; Michael Collins"}, {"ref_id": "b35", "title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs", "journal": "", "year": "2018", "authors": "A Yu;  Malkov;  Dmitry A Yashunin"}, {"ref_id": "b36", "title": "Generation-augmented retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Yuning Mao; Pengcheng He; Xiaodong Liu; Yelong Shen; Jianfeng Gao; Jiawei Han; Weizhu Chen"}, {"ref_id": "b37", "title": "From doc2query to doctttttquery", "journal": "", "year": "2019", "authors": "Rodrigo Nogueira; Jimmy Lin; A I Epistemic"}, {"ref_id": "b38", "title": "Document expansion by query prediction", "journal": "", "year": "2019", "authors": "Rodrigo Nogueira; Wei Yang; Jimmy Lin; Kyunghyun Cho"}, {"ref_id": "b39", "title": "Scikitlearn: Machine learning in python", "journal": "Journal of machine Learning research", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss; Vincent Dubourg"}, {"ref_id": "b40", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b41", "title": "The probabilistic relevance framework: BM25 and beyond", "journal": "Now Publishers Inc", "year": "2009", "authors": "Stephen Robertson; Hugo Zaragoza"}, {"ref_id": "b42", "title": "On relevance weights with little relevance information", "journal": "", "year": "1997", "authors": "E Stephen; Steve Robertson;  Walker"}, {"ref_id": "b43", "title": "End-to-end training of neural retrievers for opendomain question answering", "journal": "", "year": "2021", "authors": "Devendra Singh Sachan; Mostofa Patwary; Mohammad Shoeybi; Neel Kant; Wei Ping; L William; Bryan Hamilton;  Catanzaro"}, {"ref_id": "b44", "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "journal": "", "year": "2017", "authors": "Noam Shazeer; Azalia Mirhoseini; Krzysztof Maziarz; Andy Davis; Quoc Le; Geoffrey Hinton; Jeff Dean"}, {"ref_id": "b45", "title": "Learning semantic representations using convolutional neural networks for web search", "journal": "", "year": "2014", "authors": "Yelong Shen; Xiaodong He; Jianfeng Gao; Li Deng; Gr\u00e9goire Mesnil"}, {"ref_id": "b46", "title": "Synthesizer: Rethinking self-attention in transformer models", "journal": "", "year": "2020", "authors": "Yi Tay; Dara Bahri; Donald Metzler; Da-Cheng Juan; Zhe Zhao; Che Zheng"}, {"ref_id": "b47", "title": "Transformer memory as a differentiable search index", "journal": "", "year": "2022", "authors": "Yi Tay; Q Vinh; Mostafa Tran; Jianmo Dehghani; Dara Ni; Harsh Bahri; Zhen Mehta; Kai Qin; Zhe Hui; Jai Zhao;  Gupta"}, {"ref_id": "b48", "title": "Visualizing data using t-sne", "journal": "Journal of machine learning research", "year": "2008", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b49", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b50", "title": "Huggingface's transformers: State-of-the-art natural language processing", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz"}, {"ref_id": "b51", "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval", "journal": "", "year": "2020", "authors": "Lee Xiong; Chenyan Xiong; Ye Li; Kwok-Fung Tang; Jialin Liu; N Paul; Junaid Bennett; Arnold Ahmed;  Overwijk"}, {"ref_id": "b52", "title": "Is retriever merely an approximator of reader?", "journal": "", "year": "2020", "authors": "Sohee Yang; Minjoon Seo"}, {"ref_id": "b53", "title": "Simple applications of bert for ad hoc document retrieval", "journal": "", "year": "2019", "authors": "Wei Yang; Haotian Zhang; Jimmy Lin"}, {"ref_id": "b54", "title": "From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing", "journal": "", "year": "2018", "authors": "Hamed Zamani; Mostafa Dehghani; Bruce Croft; Erik Learned-Miller; Jaap Kamps"}, {"ref_id": "b55", "title": "Adversarial retriever-ranker for dense text retrieval", "journal": "", "year": "2021", "authors": "Hang Zhang; Yeyun Gong; Yelong Shen; Jiancheng Lv; Nan Duan; Weizhu Chen"}, {"ref_id": "b56", "title": "Deep query likelihood model for information retrieval", "journal": "", "year": "", "authors": "Shengyao Zhuang; Hang Li; G Zuccon"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of Neural Corpus Indexer (NCI). (a) Preprocessing. Each document is represented by a semantic identifier via hierarchical k-means. (b) Query Generation. Queries are generated for each document based on the content. (c) The training pipeline of NCI. The model is trained over augmented <query, docid> pairs through a standard transformer encoder and the proposed Prefix-Aware Weight-Adaptive (PAWA) Decoder.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Overview of the Prefix-Aware Weight-Adaptive (PAWA) Decoder.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Learning curves of NCI with different model capacities. Left: NQ320k; Right: TriviaQA.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Analyses of retrieved documents with semantic identifiers. Left: The probabilities of retrieved documents for Query Group A; Middle: The probabilities for Query Group B; Right: The t-SNE visualization of BERT-based document embeddings.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Performance comparison on NQ320k retrieval task. The settings with qg-ft refer to query generation by the DocT5Query model fine-tuned on this dataset. Other settings use the original checkpoint of DocT5Query.", "figure_data": "MethodRecall@1 Recall@10 Recall@100 MRR@100Neural Corpus Indexer (Base)65.8685.2092.4273.12Neural Corpus Indexer (Large)66.2385.2792.4973.37Neural Corpus Indexer (Ensemble)70.4689.3594.7577.82Neural Corpus Indexer w/ qg-ft (Base)68.9188.4894.4876.17Neural Corpus Indexer w/ qg-ft (Large)68.6588.4594.5376.10Neural Corpus Indexer w/ qg-ft (Ensemble)72.7891.7696.2280.12DSI (Base) [50]27.4056.60--DSI (Large) [50]35.6062.60--DSI (XXL) [50]40.4070.30--SEAL (Base) [4]56.9879.9791.3965.48SEAL (Large) [4]59.9381.2490.9367.70ANCE (FirstP) [54]51.3380.3391.7861.71ANCE (MaxP) [54]52.6380.3891.3162.84BERT + BruteForce [15]28.6553.4273.1636.60BERT + ANN (Faiss) [28]27.9253.6373.0137.08BM25 + DocT5Query [40]35.4361.8376.9244.47BM25 [44]15.1132.4850.5421.07"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance comparison on TriviaQA retrieval task. The results annotated by * are taken from[58].", "figure_data": "MethodRecall@5 Recall@20 Recall@100 R-PrecisionNeural Corpus Indexer (Base)90.4994.4596.9473.90Neural Corpus Indexer (Large)91.7395.1797.4474.94Neural Corpus Indexer (Ensemble)94.6096.8998.2080.84SEAL (Base) [4]86.390.591.568.1SEAL (Large) [4]87.791.892.669.2AR2-G * [58]78.284.487.9-coCondenser * [18]76.883.287.3-Condenser * [17]-81.986.2-Individual Top-k * [46]76.883.187.0Joint Top-k * [46]74.181.386.3RDR * [55]-82.587.3-ANCE * [54]-80.385.3-DPR * [30]-79.384.9-GAR * [39]73.180.485.7-BM25 + DocT5Query [40]59.7172.0682.7139.66BM25 [44]56.9169.4580.2437.29"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Ablation Study on NQ320k and TriviaQA retrieval task.", "figure_data": "MethodNQ320k Recall@1 Recall@10 Recall@100 MRR@100 Recall@5 Recall@20 Recall@100 R-Precision TriviaQANeural Corpus Indexer (Base)65.8685.2092.4273.1290.4994.4596.9473.90w/o DocT5Query60.2380.2090.9267.8984.5690.9495.3263.50w/o document as query62.4981.2188.8569.4185.3491.1094.6667.48w/o PAWA decoder63.3683.0691.4770.5688.7593.5696.1871.81w/o semantic id62.7583.8891.0170.4388.9193.0795.8072.57w/o regularization65.0782.9190.6571.8089.0193.6396.1671.59w/o constrained beam search65.6584.8992.2372.7989.5893.9796.6172.51"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "NCI with different number of layers in PAWA adapter. Left: NQ320k; Right: TriviaQA.", "figure_data": "SettingRecall@1 Recall@10 Recall@100 MRR@100SettingRecall@5 Recall@20 Recall@100 R-Precision#layer = 063.3683.0691.4770.56#layer = 088.7593.5696.1871.81#layer = 164.8584.7191.4971.42#layer = 189.1693.9096.5870.77#layer = 265.4085.1292.8272.83#layer = 289.8994.3596.8672.89#layer = 465.8685.2092.4273.12#layer = 490.4994.4596.9473.90#layer = 665.0783.9191.6571.80#layer = 689.7694.3196.7673.32#layer = 863.6083.1191.7871.22#layer = 887.9093.3096.2070.75"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Efficiency analysis", "figure_data": "Model Beam Latency Throughputsizesize(ms)(queries / s)Small1078.4658.48Base10115.1752.55Large10188.6043.39Small100216.016.12Base100269.315.62Large100356.074.75"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Different regularization hyper-parameter \u03b1 in loss function. Left: NQ320k; Right: TriviaQA.", "figure_data": "SettingRecall@1 Recall@10 Recall@100 MRR@100SettingRecall@5 Recall@20 Recall@100 R-Precision\u03b1 = 065.0782.9190.6571.80\u03b1 = 089.0193.6396.1671.59\u03b1 = 0.165.5185.2892.5272.76\u03b1 = 0.190.1494.1596.9672.78\u03b1 = 0.1565.8685.2092.4273.12\u03b1 = 0.1590.4994.4596.9473.90\u03b1 = 0.265.5584.4892.6172.63\u03b1 = 0.290.4494.4196.9773.22\u03b1 = 0.365.4485.2192.4572.83\u03b1 = 0.390.0294.0996.7973.53"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(l|x, \u03b8) = m i=1 p(r i |x, r 1 , r 2 , ..., r i\u22121 , \u03b8 i ),(1)", "formula_coordinates": [5.0, 222.1, 156.01, 281.9, 30.32]}, {"formula_id": "formula_1", "formula_text": "h i = TransformerDecoder(x, h 1 , h 2 , ..., h i\u22121 ; \u03b8 i ),(2)", "formula_coordinates": [5.0, 110.98, 258.11, 212.78, 20.17]}, {"formula_id": "formula_2", "formula_text": "p(r i |x, r 1 , r 2 , ..., r i\u22121 , \u03b8 i ) = Softmax(h i W ).(3)", "formula_coordinates": [5.0, 114.68, 283.53, 209.08, 9.65]}, {"formula_id": "formula_3", "formula_text": "W i ada = AdaptiveDecoder(e; r 1 , r 2 , ..., r i\u22121 )W i (4", "formula_coordinates": [5.0, 209.54, 598.81, 290.59, 12.69]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [5.0, 500.13, 600.96, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "Lreg = \u2212 log exp (sim(zi,1, zi,2)/\u03c4 ) 2Q k=1,k =2 exp (sim((zi,1, z i,k )/\u03c4 )(5)", "formula_coordinates": [6.0, 212.86, 175.21, 291.14, 23.29]}, {"formula_id": "formula_6", "formula_text": "L(\u03b8) = (q,d)\u2208D log p(d|E(q), \u03b8) + \u03b1Lreg ,(6)", "formula_coordinates": [6.0, 222.98, 264.95, 281.02, 18.19]}], "doi": ""}