{"title": "Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices", "authors": "Zengfeng Huang", "pub_date": "", "abstract": "Given a large matrix A \u2208 R n\u00d7d , we consider the problem of computing a sketch matrix B \u2208 R \u00d7d which is significantly smaller than but still well approximates A. We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space, and we are interested in minimizing the covariance error A T A \u2212 B T B 2 . The popular Frequent Directions algorithm of Liberty ( 2013) and its variants achieve optimal space-error tradeoffs. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the question by proving that the time complexity of this problem is equivalent to that of matrix multiplication up to lower order terms. Specifically, we provide new space-optimal algorithms with faster running times and also show that the running times of our algorithms can be improved if and only if the state-of-the-art running time of matrix multiplication can be improved significantly.", "sections": [{"heading": "Introduction", "text": "For large-scale matrix computations, exact algorithms are often too slow, so a large body of work focuses on designing fast randomized approximation algorithms. Matrix sketching is a commonly used algorithmic technique for solving linear algebra problems over massive data matrices, e.g., Sarlos (2006); Clarkson and Woodruff (2013); Avron et al. (2013); Chierichetti et al. (2017). In the sketch-and-solve framework, given a large matrix, we first compute a small sketch using a lightweight algorithm, and then do more expensive computations on the sketch instead of on the original matrix. Thus, the key is how to efficiently compute sketch matrices that are small but still preserve vital properties of the input matrix.\nIn real-world applications, data often arrives in a streaming fashion and it is often impractical or impossible to store the entire data set in the main memory. This paper studies online streaming algorithms for maintaining matrix sketches with small covariance errors. In the streaming model, the rows of the input matrix arrive one at a time; algorithm is only allowed to make one pass over the stream with severely limited working space, and is required to maintain a sketch continuously. This problem has received lots of attention recently (Liberty, 2013;Woodruff, 2014a;Ghashami et al., 2016;Wei et al., 2016).\nThe popular Frequent Directions algorithms (Liberty, 2013;Ghashami et al., 2016) achieve optimal tradeoffs between space usage and approximation error (Woodruff, 2014a), which have found lots of applications in online learning, e.g., Boutsidis et al. (2015); Karnin and Liberty (2015); Leng et al. (2015); Huang and Kasiviswanathan (2015); ; Calandriello et al. (2017), and other important problems (Song et al., 2015;Ye et al., 2016;Kim et al., 2016;Cohen et al., 2017). However, in contrast to the space complexity, it is unclear whether their running times can be improved; one might hope to get linear (in sparsity) time algorithms, which is possible for many matrix problems (see e.g., Clarkson and Woodruff (2013)). This paper is motivated by the following question:\n\u2022 Is there an input sparsity time Frequent Directions, which achieves the same optimal space-error tradeoff ?", "publication_ref": ["b26", "b6", "b0", "b4", "b22", "b29", "b13", "b28", "b22", "b13", "b29", "b18", "b20", "b14", "b3", "b27", "b31", "b19", "b8", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Definitions", "text": "Given a matrix A \u2208 R n\u00d7d , the problem is to compute a much smaller matrix B \u2208 R \u00d7d , which has low covariance error, i.e., A T A \u2212 B T B 2 .\nDefinition 1 (Covariance Sketch) For any 0 < \u03b1 < 1, and integer 0 \u2264 k \u2264 rank(A), we will call B an (\u03b1, k)-cov-sketch of A, if the covariance error 1\nA T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .(1)\nHere \u2022 2 and \u2022 F are the spectral norm and Frobenius norm of matrices; [A] k is the best rank-k approximation to A. We will use \u03c0 k B (A) to denote the projection of A on the top-k singular vectors of B, i.e. \u03c0 k B (A) = AV V T , where the columns of V are the top-k right singular vectors of B.\nDefinition 2 (Projection Error) The projection error of B with respect to A is defined as A \u2212 \u03c0 k B (A) 2 F .\nNote \u03c0 k B (A) is a rank-k matrix, thereby the projection error is at least A \u2212 [A] k 2 F . It is proved in  that one can obtain relative projection error from small covariance error. We include a proof of the next lemma in Appendix A.1.\nLemma 3 (Covariance Error to Projection Error )\nA \u2212 \u03c0 k B (A) 2 F \u2264 A \u2212 [A] k 2 F + 2k \u2022 A T A \u2212 B T B 2 . Therefore, any ( \u03b5 2k , k)-cov-sketch B has projection error A \u2212 \u03c0 k B (A) 2 F \u2264 (1 + \u03b5) A \u2212 [A] k 2 F .(2)\nWe will often refer to ( \u03b5 2k , k)-cov-sketches as (\u03b5, k)-proj-sketches. (\u03b5, k)-proj-sketches actually satisfy a more general property, called Projection-Cost Preserving property. Specially, suppose B is an (\u03b5, k)-proj-sketch of a matrix A, then\nB \u2212 BQQ T 2 F = (1 \u00b1 \u03b5) A \u2212 AQQ T 2 F ,\nfor all rank-k orthonormal matrices Q (see Feldman et al. (2013);Cohen et al. (2015Cohen et al. ( , 2017 for more details). One immediate application of projection-cost preserving sketches is constrained low rank approximations, which can be formulated as the following optimization problem: min\nX:rank(X)\u2264k,X\u2208S A \u2212 AXX T 2 F ,\nwhere S is some constraint; special cases include sparse PCA, nonnegative PCA, and k-means clustering (Feldman et al., 2013;Cohen et al., 2015). Given any projection-cost preserving sketch B of A, one can solve the above optimization problem with respect to B to obtain an approximate solution, which significantly reduces the computational costs when B is much smaller than A.\nModern data matrices are often large and sparse, so we will always assume n and d are very large and nnz(A) nd, where nnz(A) in the number of nonzero entries in A. Moreover, we assume that each entry of A is representable by O(log(nd)) bits. To simplify the analysis, we assume the entries of A are integers of magnitude at most poly(nd); the general case can be reduced to this, see e.g., Boutsidis et al. (2016).", "publication_ref": ["b10", "b7", "b8", "b10", "b7", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Previous Results", "text": "In the row-wise update streaming model, Liberty's Frequent Directions (FD) algorithm (Liberty, 2013), with an improved analysis by , maintains an (\u03b1, k)-cov-sketch B \u2208 R \u00d7d at any time, where = O(k + \u03b1 \u22121 ). The algorithm uses O(d ) space and runs in O(nd ) time. For sparse matrices, the running time of FD is improved to O(nnz(A) log d + nnz(A) log n + n 2 ) by Ghashami et al. (2016). Setting \u03b1 = \u03b5/2k (or = O(k/\u03b5)) and by Lemma 3, B is a (\u03b5, k)-proj-sketch. Now, B contains O(k/\u03b5) rows, and the space and running time become O(dk/\u03b5) and O(nnz(A)k\u03b5 \u22121 \u2022log d+nnz(A) log n+nk 2 \u03b5 \u22122 ) respectively. The log d factor in the leading term was removed by . It was shown by Woodruff (Woodruff, 2014a) that the space used by FD is optimal for both covariance error and projection error. A natural question is if the running time can be improved. In particular,\n\u2022 Is there an input sparsity time algorithm, i.e., in time O(nnz(A)\n+ (n + d) \u2022 poly(k\u03b1 \u22121 )),\nwhich achieves the same guarantee as FD?", "publication_ref": ["b22", "b13", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Our Contributions", "text": "This paper almost settles the above question. Our main contributions are summarized as follows.\n1. We give a new space-optimal streaming algorithm with O(ndk) +\u00d5(d\u03b1 \u22123 ) running time to compute (\u03b1, k)-cov-sketches for dense matrices, which improves the original FD algorithm for small \u03b1. The running time is optimal up to lower order terms, provided matrix multiplication cannot be improved significantly.\n2. Based on our fast algorithm for sketching dense matrices, we then give a new spaceoptimal streaming algorithm with O(nnz(A)k + nk 3 ) +\u00d5(d\u03b1 \u22123 ) running time to compute (\u03b1, k)-cov-sketches for sparse matrices. We separate the dependence of\nHuang Time (\u03b1, k)-cov (\u03b5, k)-proj FD (Liberty, 2013) O(ndk + nd/\u03b1) O(ndk/\u03b5) FFDdense (new) O(ndk) O(ndk) Sparse FD(Ghashami et al., 2016) O(nnz(A)k log d + nnz(A) log d/\u03b1) O(nnz(A)k log d/\u03b5) FFDsparse (new) O(nnz(A)k) O(nnz(A)k) Lower bounds (new) \u2126(nnz(A)k) \u2126(nnz(A)k)\nTable 1: Running times and lower bounds for streaming (\u03b1, k)-cov-sketch and (\u03b5, k)-projsketch algorithms. Lower order terms are omitted and the lower bounds are conditional.\n1/\u03b1 from nnz(A), which improves the results of Ghashami et al. (2016) for small \u03b1. Therefore, to compute an (\u03b5, k)-proj-sketch, our algorithm only needs O(nnz(A)k) time (ignoring lower order terms) as opposed to O(nnz(A)k\u03b5 \u22121 \u2022 log d) in Ghashami et al. (2016) (see Table 1).\n3. We show that o(nnz(A)k) time is likely very difficult to achieve, as it will imply a breakthrough in fast matrix multiplication. Specifically, we prove that, under mild assumptions, the time complexity for computing an (O(1), k)-cov-sketch B \u2208 R O(k)\u00d7d of A in the streaming model is equivalent to the time complexity of left multiplying A by an arbitrary matrix C \u2208 R k\u00d7n .", "publication_ref": ["b13", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Other Related Work", "text": "The problem of computing (\u03b1, k)-cov-sketches was also studied in the sliding window streaming model (Wei et al., 2016) and distributed models . A closely related problem, namely approximate PCA, was studied in Kannan et al. (2014); Liang et al. (2014); Boutsidis et al. (2016); Zhang et al. (2015). Clarkson and Woodruff (2009) studied other streaming numerical linear algebra problems.", "publication_ref": ["b28", "b17", "b21", "b2", "b33", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Matrix Preliminaries and Notations", "text": "We always use n for the number rows and d for the dimension of each row. For a d-dimensional vector x, x is the 2 norm of x. We use x i to denote the ith entry of x, and Diag(x) \u2208 R d\u00d7d is a diagonal matrix such that the ith diagonal entry is x i . For a matrix A \u2208 R n\u00d7d with n > d, we use A i to denote the ith row of A and a i,j for the (i, j)-th entry of A. nnz(A) is the number of non-zero entries in A and rows(A) is the number of rows in A. We write the (reduced) singular value decomposition of A as (U, \u03a3, V ) = SVD(A). The computation time of standard SVD algorithms is O(nd 2 ). We use A 2 or A to denote the spectral norm of A, which is the largest singular value of A, and A F for the Frobenius Norm, which is i,j a 2 i,j . For k \u2264 rank(A), we use [A] k to denote the best rank k approximation of A. We define [A] 0 = 0. [A; B] is the matrix formed by concatenating the rows of A and B. We us\u1ebd O() to hide polylog(ndk) factors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tools", "text": "Frequent Directions. We will use the Frequent Directions (FD) algorithm by Liberty (Liberty, 2013), denoted as FD(A, \u03b1, k); and the main result is summarized in the following theorem.\nTheorem 4 (Liberty (2013)\n) Given A \u2208 R n\u00d7d , in one pass, FD(A, \u03b1, k) processes A in O(nd(k + \u03b1 \u22121 )) time and O(d(k + \u03b1 \u22121 )) space. It maintains a matrix B \u2208 R O(k+\u03b1 \u22121 )\u00d7d such that A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .\nRow sampling. We provide a result about row sampling, which is analogous to a result from Drineas et al. (2006). The difference is that they use sampling with replacement, i.e., each row of B is an iid sample from the rows of A. On the other hand, we use Bernoulli sampling, i.e., sample each A i independently with some probability q i , and B is the set of sampled rows. Bernoulli sampling can easily be combined with FD in the streaming model. The proof is essentially the same as that for sampling with replacement, which can be found in Appendix.\nTheorem 5 For any A \u2208 R n\u00d7d and F > 0, we sample each row A i with probability\np i \u2265 A i 2 \u03b1 2 F ; if it is sampled, scale it by 1/ \u221a p i . Let B be the (rescaled) sampled rows, then w.p. 0.99, A T A \u2212 B T B 2 \u2264 10\u03b1 \u221a F A F and B F \u2264 10 A F . The expected number of rows sampled is O( A 2 F \u03b1 2 F ) if p i = O( A i 2 \u03b1 2 F ) for each i.\nInput-sparsity time subspace embedding. We will use fast subspace embeddings (Clarkson and Woodruff, 2013) to speedup computation. There are various approaches to achieve input-sparsity time subspace embedding with different guarantees; the following result suffices for our purpose, the proof of which can be found in Woodruff (2014b) (Lemma 4.2 and Remark 4.1).\nTheorem 6 (Subspace Embedding) Given any A \u2208 R n\u00d7d , there exist (random) matrices J \u2208 R O(k)\u00d7t 1 and C \u2208 R t 1 \u00d7d , with t 1 = min(d, O(k 2 )), such that, with probability 0.99, the column space of S = AC T J T contains an O(1)-approximate rank-k approximation to A. More precisely, there exists X with rank(X) \u2264 k such that\nA \u2212 SX 2 F \u2264 O(1) A \u2212 [A] k 2 F , Moreover, C only contains O(d) nonzero entries and S = AC T J T can be computed in time O(nnz(A)) + O(nk 3 ).\nOne can choose J to be a matrix with iid Gaussian random variables and C to be a count-sketch matrix (Clarkson and Woodruff, 2013) ", "publication_ref": ["b22", "b22", "b9", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm for Dense Matrices", "text": "Theorem 7 (FFDdense(A, \u03b1, k)) Given a matrix A \u2208 R n\u00d7d , 0 < \u03b1 < 1 and 0 \u2264 k \u2264 d, FFDdense(A, \u03b1, k) processes A in one pass using O(ndk) +\u00d5(d\u03b1 \u22123 ) time and O(dk + d\u03b1 \u22121 )) space, and maintains a matrix B O(k+\u03b1 \u22121 )\u00d7d . With probability 0.99, it holds that\nA T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .\nOverview of the algorithm. To speed up FD, we will use the idea of adaptive random sampling. Let us first review the standard FD algorithm. Given an integer parameter \u2264 d, the algorithm always maintains a matrix B with at most 2 rows at any time. When a new row v arrives, it processes the row using FDShrink(B, v, ) (Algorithm 3). In this procedure, we first append a after B; if B has no more than 2 rows we do nothing, and otherwise do a DenseShrink operation (Algorithm 1) on B, which halves the number of rows in B (after removing zero rows). It was proved by Liberty (2013) and  that for = k + \u03b1 \u22121 , we have\nA T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .\nSince each SVD computation in DenseShrink takes O(d 2 ) time and there are totally n/ SVD computations (SVD is applied every rows), the running time is\nO(nd ) = O(nd(k + \u03b1 \u22121 )).\nOur goal is to separate nd from \u03b1 \u22121 in the running time. ), and \u03c3 = \u03a3 , .\nAlgorithm 1 DenseShrink Input: B \u2208 R 2 \u00d7d . 1: Compute [U, \u03a3, V ] = SVD(B), and \u03c3 = \u03a3 , . 2:\u03a3 = max(\u03a3 2 \u2212 \u03c3 2 I 2 , 0) 3: return B =\u03a3V T Algorithm 2 DenseShrinkR Input: B \u2208 R 2 \u00d7d . 1: Compute [U, \u03a3, V ] = SVD(B\n2:\u03a3 = max(\u03a3 2 \u2212 \u03c3 2 I 2 , 0)\n3:\u03a3 = \u03a3 2 \u2212\u03a3 2 \u03a3 2 =\u03a3 2 +\u03a3 2 4: return B =\u03a3V T ,\u03a3 and V T Algorithm 3 FDShrink Input: B \u2208 R \u00d7d , v \u2208 R d ,", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "and an integer", "text": "it always holds that < 2 .\n1: B = [B; v] 2: If + 1 = 2 , then B = DenseShrink(B, ). 3: return B\nTo achieve this, we first compute a coarse approximation using FD by invoking B = FD(A, k, 1 2k ), which takes O(ndk) time. The key idea here is that in each DenseShrink operation, after shrinking B, we also return the residual; we call this modified shrinking operation DenseShrinkR (see Algorithm 2). Let C be the matrix which is the concatenation of all residuals return from DenseShrinkR. We will show\nA T A = B T B + C T C and C 2 F \u2264 A \u2212 [A] k 2 F .\nWe then refine the answer by computing an approximation to C. Since the norm of C is small, random sampling suffices. To save space, the sampled rows will be fed to a standard FD algorithm. See Algorithm 4 for detailed description of the algorithm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 4 FFDdense", "text": "Input: A \u2208 R n\u00d7d , 0 < \u03b1 < 1, and integer k \u2264 d.\n1: F = 0, = 3k, Q = empty, B = empty 2: for i = 1 to n do 3:\nAppend A i after B 4:\nif rows(B) = 2 then 5:\n[B, \u03a3, V ] = DenseShrinkR(B)\nHere = 3k, thus B = FD(A, 1 2k , k)\n6:\nF = F + \u03a3 2 F , Next: subsample C := \u03a3V , and then compress the sampled rows using standard FD 7:\nfor j = 1 to 2 do 8:\np j = \u03a3 2 j \u03b1 2 F 9:\nSample C j with probability p j .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10:", "text": "if C j is sampled then\n11: Set v = C j \u221a p j . 12: Q = FDShrink([Q; v], 1 \u03b1 )\nInvoking FD with k = 0 Correctness. We note that, at the end of Algorithm 4, B = FD(A, 1 2k , k), so i) , and B (i) be the value of \u03a3, V , and B respectively returned by ith DenseShrinkR operation (line 5). Let C (i) = \u03a3 (i) V (i) . We use B (i) to denote the value of B right before the ith DenseShrinkR operation (or the input of the ith DenseShrinkR operation). From Algorithm 2, we have that\nA T A \u2212 B T B 2 \u2264 A \u2212 [A] k 2 F /2k, or equivalently max x: x =1 Ax 2 \u2212 Bx 2 \u2264 A \u2212 [A] k 2 F /2k. (3\n) Let \u03a3 (i) , V(\nB (i)T B (i) = B (i)T B (i) + V (i)T \u03a3 (i)2 V (i) = B (i)T B (i) + C (i)T C (i) .\nLet A (i) be the rows of A arrived between the (i \u2212 1)th and the ith DenseShrinkR operation, which means B (i) = [B (i\u22121) ; A (i) ], and thus\nB (i)T B (i) = B (i\u22121)T B (i\u22121) + A (i)T A (i) .\nCombined with the previous equality, we get\nA (i)T A (i) + B (i\u22121)T B (i\u22121) \u2212 B (i)T B (i) = C (i)T C (i) .\nLet t be the total number of iterations. We define B (0) = 0, and C = [C (1) ; \u2022 \u2022 \u2022 ; C (t) ]. Summing the above equality over i = 1, \u2022 \u2022 \u2022 , t, we have\nC T C = i C (i)T C (i) = i A (i)T A (i) + B (i\u22121)T B (i\u22121) \u2212 B (i)T B (i) = A T A \u2212 B T B. It follows that C 2 F = trace(C T C) = trace(A T A) \u2212 trace(B T B) = A 2 F \u2212 B 2 F .\nNow we bound A 2 F \u2212 B 2 F using similar ideas as in . Let w j be the jth singular vector of A, we have\nC 2 F = A 2 F \u2212 B 2 F = k j=1 Aw j 2 + d j=k+1 Aw j 2 \u2212 B 2 F \u2264 k j=1 Aw j 2 + A \u2212 [A] k 2 F \u2212 k j=1 Bw j 2 because k j=1 Bw j 2 \u2264 B 2 F \u2264 A \u2212 [A] k 2 F + k \u2022 A \u2212 [A] k 2 F /2k by Eq (3) = 1.5 A \u2212 [A] k 2 F .(4)\nIn the algorithm, each row of C is sampled with probability\nC j 2 \u03b1 2 F\n, where F is the current squared F-norm of C. Let C s be the sampled rows. Given Eq (4), we can prove the following using Theorem 5\nC T C \u2212 C T s C s 2 \u2264 \u03b1 A \u2212 [A] k 2 F , and C s 2 F = O(1) \u2022 A \u2212 [A] k 2 F .\nAt the end of the algorithm, Q = FD(C s , \u03b1, 0), then\nC T s C s \u2212 Q T Q 2 \u2264 \u03b1 C s 2 F \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F .\nApplying triangle inequality, we have\nC T C \u2212 Q T Q 2 \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2\nF , and thus\nA T A \u2212 B T B \u2212 Q T Q 2 = C T C \u2212 Q T Q 2 \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F ,\nwhich proves the correctness.\nSpace and running time. The space is dominated by maintaining B = FD(A, 1/2k, k) and\nQ = FD(C s , \u03b1, 0), which is O(dk + d/\u03b1) in total.\nThe running time of computing B is O(ndk), and the running time for Q is O(rows(C s )d/\u03b1). To bound rows(C s ), we divide the stream into epochs, where F roughly doubles in each epoch. This means the total number of epochs is bounded by O(log(nd)), since we assume each real number in the input can be represented by O(log(nd)) bits. We emphasize that a rigorous analysis on this will be more subtle; see discussions in the proof of Lemma 15 below. Applying Theorem 5 on the submatrix in each epoch, it is easy to check the expected number of rows sampled in each epoch is O(1/\u03b1 2 ), so rows(C s ) = O( log(nd) \u03b1 2 ). Thus the total running time is O(ndk) +\u00d5(d\u03b1 \u22123 ). We remark that the residual return by DenseShrinkR is in the form of C = \u03a3V T , where \u03a3 is diagonal and V has orthonormal columns. Therefore, the row norms of C are simply the diagonals of \u03a3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm for Sparse Matrices", "text": "For sketching sparse matrices, we will use a subroutine (see Algorithm 5) for computing weak low rank approximations based on fast subspace embeddings. In this subroutine, the input matrix is an \u00d7 d matrix A and two additional matrices J and C, where J has O(k) rows and C has d columns. In our applications, J, C will always be the subspace embedding matrices from Theorem 6. The algorithm outputs an O(k) \u00d7 matrix Z with orthonormal rows.\nAlgorithm 5 Weak Low Rank Approximation (LRA)\nInput: A \u2208 R \u00d7d , J \u2208 R O(k)\u00d7w and C \u2208 R w\u00d7d 1: Compute S = AC T J T 2: Compute Z \u2208 R O(k)\u00d7\nwhose rows form an orthonormal basis for the column space of S 3: return Z\nThe following lemma is a simple corollary of Theorem 6.\nLemma 8 If J, C are the subspace embedding matrices from Theorem 6 with w = min(d, O(k 2 )) and Z is the output of Algorithm 5, then with probability 0.99\nA \u2212 Z T ZA 2 F \u2264 O(1) A \u2212 [A] k 2 F . Moreover, Z can be computed in O(nnz(A) + k 3 ) time and O( k 2 ) extra space.\nProof From Theorem 6, there exists X with rank(X) \u2264 k such that\nA \u2212 Z T X 2 F \u2264 O(1) A \u2212 [A] k 2 F .\nHence,\nA \u2212 Z T ZA 2 F \u2264 A \u2212 Z T ZA 2 F + Z T ZA \u2212 Z T X 2 F = A \u2212 Z T X 2 F Pythagorean theorem \u2264 O(1) A \u2212 [A] k 2 F .\nNote that S can be computed in time O(nnz(A) + k 3 ) and Z can be computed from S in O( k 2 ) time. The space usage is dominated by storing the intermediate result AC T , which is O( k 2 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview of Our Algorithm", "text": "Our approach is quite different from Ghashami et al. (2016). Their main idea is to use fast approximate SVD  in the original FD, which leads to suboptimal time. Our approach is summarized as follows.\n1. Decompose A T A = A T A + R T R, such that A contains small number of rows and\nA \u2212 [A ] k 2 F = O(1) \u2022 A \u2212 [A] k 2 F . Moreover, R 2 F = O(1) \u2022 A \u2212 [A] k 2 F .\n2. Compute a sketch B of A using fast FD algorithm for dense matrices (Theorem 7), which satisfies that\nA T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A ] k 2 F \u2264 \u03b1 A \u2212 [A] k 2 F . 3. Compute a sketch matrix C of R such that R T R \u2212 C T C 2 \u2264 \u03b1 R 2 F \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2\nF , which can be done via random sampling (Theorem 5) combined with FD. \nA T A + R T R \u2212 B T B \u2212 C T C 2 \u2264 A T A \u2212 B T B 2 + R T R \u2212 C T C 2 \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F .\nFrom step (1), we have A T A = A T A + R T R, and thus [B; C] is a good approximation of A.\nNext we briefly describe how to implement this in one pass and small space.\nTo achieve (1), we use the following new idea. Let Z \u2208 R O(k)\u00d7d be an orthonormal matrix satisfying\nA \u2212 Z T ZA 2 F \u2264 O(1) A \u2212 [A] k 2 F . Let A = ZA and R = (I \u2212 Z T Z)A.\nIt is easy to check that A and R satisfy the requirement of (1). In the streaming model, we divide A into blocks, each of which contains roughly dk non-zero entries, and thus there are at most t = nnz(A) dk blocks. We use the above idea for each of the blocks and concatenate the results together. More precisely, for each block A (i) \u2208 R i \u00d7d , we use an input-sparsity time algorithm (Algorithm 5 and Lemma 8) to compute a matrix Z (i) \u2208 R O(k)\u00d7 i such that\nA (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) A (i) \u2212 [A (i) ] k 2 F .\nLet A (i) = Z (i) A (i) , and R (i) = (I \u2212 Z (i)T Z (i) )A (i) . We then set A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ] and R = [R (1) ; \u2022 \u2022 \u2022 ; R (t) ], and prove that A and R satisfy the requirement of (1), where A only has t \u00d7 O(k) = O( nnz(A) d ) rows (since each block of A has O(k) rows). Here we do not compute R explicitly, as we will sample a subset of the rows from R. Note that the running time of this step is dominated by computing Z (i) A (i) , which is O(nnz(A (i) )k), and thus O(nnz(A)k) in total.\nTo compute B of step (2), we may use the standard FD(A , \u03b1, k) (Theorem 4). Since A has at most O( nnz(A) d ) rows, B can be computed in O(nnz(A)(k + \u03b1 \u22121 )) time. However, it still has an nnz(A)\u03b1 \u22121 term. So, we apply our faster FD algorithm for dense matrices (Theorem 7) on A , which only takes O(nnz(A)k) +\u00d5(d\u03b1 \u22123 ) time.\nIn order to compute a sketch C of R in step (3), we first subsample the rows of R using streaming Bernoulli sampling. One difficulty is that R could be dense, and it may take nd time to compute the row norms. Fortunately, each R (i) is of special form, and we are able to design a fast algorithm to compute its row norms (Algorithm 7). Let Q be the sampled rows, with rows(Q) =\u00d5(1/\u03b1 2 ), and note that each row of Q can be computed in time O(kd) as Z (i) A (i) has already been computed in step (1). We finally use FD(Q, \u03b1, 0) to compute a sketch matrix C of Q in time\u00d5(d\u03b1 \u22123 ). In all, the running time is roughly O(nnz(A)k) +\u00d5(d\u03b1 \u22123 + dk\u03b1 \u22122 ).", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Our Algorithm", "text": "Algorithm 6 FFDsparse Input: A \u2208 R n\u00d7d , \u03b1 \u2208 (0, 1), and integers k \u2264 d.\n1: F = \u03b7, F = 0, B = 0, Q = 0 \u03b7 will be determined in Lemma 15 2: Divide the rows of A into continuous blocks A (1) , \u2022 \u2022 \u2022 , A (t) : we will put new rows into the current block until: a) the number of non-zero entries exceeds dk, or b) the number of rows is d k . When either a) or b) happens, we start a new block. Note that the total number of blocks t \u2264 nnz(A) dk + nk d . 3: Choose subspace embedding matrices J, C (Theorem 6) J, C are fixed for all blocks 4: for i = 1 to t do 5:\nCompute Z (i) = LRA(A (i) , J, C) (Algorithm 5). Let i = rows(A (i) ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "Compute\nA (i) = Z (i) A (i) . 7: w = RowNorms(A (i) , Z (i) )\nw contains the row norms of A (i) \u2212 Z (i)T Z (i) A (i) (Algorithm 7)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8:", "text": "F = F + w 2 , and if F \u2265 2F , F = F .\nWe always have\nF = O(1) \u2022 i (I \u2212 Z (i)T Z (i) )A (i) 2 F 9:\nLet p \u2208 R i such that p j =\nw 2 i \u03b1 2 F for j = 1, \u2022 \u2022 \u2022 , i . Let\nx \u2208 R i be a random vector with independent entries: For each j, x j = 1/p j w.p. p j , and x j = 0 w.p. 1 \u2212 p j .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10:", "text": "Let R (i) = (I \u2212 Z (i)T Z (i) )A (i) and Q (i) = Diag(x) \u2022 R (i) (no need to compute R (i) explicitly).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "11:", "text": "B = FFDdense([B; A (i) ], \u03b1, k).\nSketching A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ] using Theorem 7\n12:\nC = FD([C; Q (i) ], \u03b1, 0). Sketching Q = [Q (1) ; \u2022 \u2022 \u2022 ; Q (t)\n] using FD. 13: end for 14: return [B; C] Theorem 9 (FFDsparse) Given any matrix A \u2208 R n\u00d7d , 0 < \u03b1 < 1 and 0 \u2264 k \u2264 d, FFDsparse(A, \u03b1, k) (Algorithm 6) maintains a matrix S in a streaming fashion, such that, with probability 0.9,\nA T A \u2212 S T S 2 \u2264 \u03b1 A \u2212 [A] k 2 F . The algorithm uses O(d(k + \u03b1 \u22121 )) space and O(nnz(A)k + nk 3 ) +\u00d5(d\u03b1 \u22123 + dk\u03b1 \u22122 ) time.\nBy Lemma 3, we also have the following result.\nTheorem 10 Given any matrix A \u2208 R n\u00d7d , 0 < \u03b5 < 1 and 0 < k \u2264 d, there is a streaming algorithm which maintains a strong (\u03b5, k)-proj-sketch S \u2208 R O(k/\u03b5)\u00d7d . The algorithm uses O(dk/\u03b5) space and runs in O(nnz(A)k + nk 3 ) +\u00d5(dk 3 \u03b5 \u22123 )) time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 9", "text": "The detail of our fast algorithm for sparse matrix is described in Algorithm 6. We let\nA = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ], R = [R (1) ; \u2022 \u2022 \u2022 ; R (t) ], and Q = [Q (1) ; \u2022 \u2022 \u2022 ; Q (t)\n]. We use w (i) to denote the vector w in ith iteration. We need some technical lemmas.\nLemma 11 With probability at least 0.99, we have (1)\nA \u2212 [A ] k 2 F \u2264 A \u2212 [A] k 2 F ; (2) R 2 F \u2264 O(1) \u2022 A \u2212 [A] k 2 F . Proof We divide A and A into blocks as defined in Algorithm 6, i.e. A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ] and A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ]\n. For each i, we have A (i) = Z (i) A (i) for some matrix Z (i) with O(k) orthonormal rows.\nLet P be the projection matrix onto the subspace spanned by the top-k right singular vectors of A. So we have\nA \u2212 [A ] k 2 F \u2264 A \u2212 A P 2 F since P is of rank k = t i=1 A (i) \u2212 A (i) P 2 F = t i=1 Z (i) A (i) \u2212 Z (i) A (i) P 2 F \u2264 t i=1 A (i) \u2212 A (i) P 2 F Z (i) is a orthonormal = A \u2212 AP 2 F = A \u2212 [A] k 2 F , by definition of P which proves (1).\nAs defined in Algorithm 6, R (i) = (I \u2212Z (i)T Z (i) )A (i) , where\nZ (i) = LRA(A (i) , J, C). Since A (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) \u2022 A (i) \u2212 [A (i) ] k 2\nF for each i (by Lemma 8), one gets\nR 2 F = t i=1 R (i) 2 F = t i=1 (I \u2212 Z (i)T Z (i) )A (i) 2 F \u2264 O(1) \u2022 t i=1 A (i) \u2212 [A (i) ] k 2 F \u2264 O(1) \u2022 A \u2212 [A] k 2 F\nwhich proves (2). One caveat: the above argument needs\nA (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) \u2022 A (i) \u2212 [A (i) ] k 2 F\nto hold for all i simultaneously. To achieve this, one could use a similar idea as in Boutsidis et al. (2016) to first boost the success probability in Lemma 8 and then apply a union bound, but this results in an extra log factor in the time complexity.\nThus, we use a different argument to bypass this. Let S = AC T J T and write S in the block form as S = [S (1) ; \u2022 \u2022 \u2022 ; S (t) ].\nNote that S (i) = A (i) C T J T and the rows of Z (i) form an orthonormal basis for the column space of S (i) , thus for each i min X:rank(X)\u2264k\nA (i) \u2212 S (i) X 2 F = min X:rank(X)\u2264k A (i) \u2212 Z (i)T X 2 F = min X:rank(X)\u2264k A (i) \u2212 Z (i)T Z (i) A (i) + Z (i)T Z (i) A (i) \u2212 Z (i)T X 2 F = min X:rank(X)\u2264k A (i) \u2212 Z (i)T Z (i) A (i) 2 F + Z (i)T Z (i) A (i) \u2212 Z (i)T X 2 F by Pythagorean \u2265 A (i) \u2212 Z (i)T Z (i) A (i) 2\nF Therefore, we get min\nX:rank(X)\u2264k A \u2212 SX 2 F = min X:rank(X)\u2264k t i=1 A (i) \u2212 S (i) X 2 F \u2265 t i=1 min X:rank(X)\u2264k A (i) \u2212 S (i) X 2 F . \u2265 t i=1 A (i) \u2212 Z (i)T Z (i) A (i) 2 F (5)\nOne the other hand, by Theorem 6, with probability 0.99 min\nX:rank(X)\u2264k A \u2212 SX 2 F \u2264 O(1) A \u2212 [A] k 2 F .(6)\nCombining ( 5) and ( 6), we finally get\nR 2 F = t i=1 R (i) 2 F = t i=1 A (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) A \u2212 [A] k 2 F .\nNote that, in the above proof, we only require one probabilistic event, i.e., (6), to happen, and thus avoid the union bound argument. This finish the proof of part (2).\nLemma 12 A T A = R T R+A T A ; and with probability 0.99, it holds that\nA T A\u2212A T A F \u2264 O(1) \u2022 A \u2212 [A] k 2 F\n. Proof To prove the first part, we only need to prove A (i)T A (i) = R (i)T R (i) + A (i)T A (i) holds for all i. Recall that A (i) = Z (i) A (i) . For each i, we have\nR (i)T R (i) = A (i)T (I \u2212 Z (i)T Z (i) ) \u2022 (I \u2212 Z (i)T Z (i) )A (i) = A (i)T (I \u2212 Z (i)T Z (i) )A (i) since (I \u2212 Z (i)T Z (i) ) is a projection = A (i)T A (i) \u2212 A (i)T A (i) .\nThis proves the first part, from which, we also get\nA T A \u2212 A T A F = R T R F \u2264 R 2 F ,\nwhere the inequality is from the submultiplicative property of the Frobenius norm. Then the second part follows from Lemma 11.\nLemma 13 If the entries of A are integers bounded in magnitude by poly(nd) and rank(A\n) \u2265 1.1k, then A \u2212 [A] k 2 F \u2265 1/poly(nd).\nProof The lemma directly follows from a result of Clarkson and Woodruff (2009), and here we use the restated version from Boutsidis et al. (2016).\nLemma 14 (Lemma 37 of Boutsidis et al. (2016)) If an n \u00d7 d matrix A has integer entries bounded in magnitude by \u03b3, and has rank \u03c1, then the k-th largest singular value of A satisfies\n\u03c3 k \u2265 (nd\u03b3 2 ) \u2212k/2(\u03c1\u2212k) .\nLemma 15 We set \u03b7 = poly \u22121 (nd) (see line 1 of Algorithm 6), then with probability at least 0.99, it holds that\nQ 2 F = O( R 2 F ), R T R \u2212 Q T Q 2 = O(\u03b1) \u2022 A \u2212 [A] k 2 F ,\nand rows(Q) = O(log(nd)/\u03b1 2 ).\nProof Let us first assume rank(A) \u2265 1.1k. Each row R i of R is sampled with probability \u0398( R i 2 \u03b1 2 F ), with F initialized to be \u03b7. We have \u03b7 \u2264 A \u2212 [A] k 2 F (by Lemma 13). When R 2 F \u2265 \u03b7, the sampling probability for each row is at least \u2126(\nR i 2 \u03b1 2 R 2 F\n) (since F is at most a constant approximation to R 2 F at any time), so the first two parts directly follow from Theorem 5 and Lemma 11.\nOtherwise if R 2 F \u2264 \u03b7 \u2264 A \u2212 [A] k 2 F , then the probability for sampling each row is \u2126( R i 2 \u03b1 2 \u03b7 ) = \u2126( R i 2 \u03b1 2 A\u2212[A] k 2 F\n) and the first two parts follow from Theorem 5.\nTo bound the number of rows sampled, we divide the stream into epochs, where F roughly doubles in each epoch. So the total number of epochs is bounded by O(log R F \u03b7 ), as the final value of F is at most O( R 2 F ). Recall that we assume each entry of A is an integer bounded in magnitude by poly(nd), which implies the number of epochs is\nO(log R 2 F \u03b7 ) \u2264 O log A 2 F \u2022 poly(nd) = O(log(nd)).\nThe number of rows sampled in each epoch is at most O(1/\u03b1 2 ): let a 1 , \u2022 \u2022 \u2022 , a t be the rows of R in the epoch, and thus j a j 2 \u2264 O(F ) (since the epoch ends if F doubles); each row a j is sampled with probability \u0398( a j 2 \u03b1 2 F ), which implies the total number of rows sampled is O(1/\u03b1 2 ). This proves the third part.\nThe case rank(A) \u2264 1.1k is easier: we can set the rank parameter k a little larger (say k = 2k) in our algorithm so that R is always 0 by Lemma 11, and thus rows(Q) = 0. In this case, our algorithm is essentially exact.\nCorrectness. By union bound, with probability 0.9, all the above lemmas hold simultaneously, and we assume this happens. Since C = FD(Q, \u03b1, 0), by Theorem 4 and Lemma 15, we have\nQ T Q \u2212 C T C 2 \u2264 \u03b1 Q 2 F \u2264 O(\u03b1) \u2022 R 2 F .(7)\nSince B = FFDdense(A , \u03b1, k), by Theorem 7, we have\nA T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A ] k 2 F \u2264 \u03b1 A \u2212 [A] k 2 F ,(8)\nwhere the last inequality is from Lemma 11. Let S = [B; C], Running time. Let i be the number of rows in ith block A (i) . The time to compute Z (i) using Lemma 8 is O(nnz(A (i) ) + i k 2 ). Hence the total time used on this step is\nA T A \u2212 S T S 2 = A T A \u2212 B T B \u2212 C T C 2 \u2264 A T A \u2212 A T A \u2212 C T C 2 + A T A \u2212 B T B 2 \u2264 A T A \u2212 A T A \u2212 C T C 2 + \u03b1 A \u2212 [A] k 2 F by (8) = R T R \u2212 C T C 2 + \u03b1 A \u2212 [A] k 2 F \u2264 R T R \u2212 Q T Q 2 + Q T Q \u2212 C T C 2 + \u03b1 A \u2212 [A] k 2 F triangle inequality \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F + O(\u03b1) \u2022 R 2 F + \u03b1 A \u2212 [A] k 2 F by(\ni O(nnz(A (i) ) + i k 2 ) = O(nnz(A) + nk 2 ).\nThe step to compute the matrix multiplication The time for naively computing the row norms of R is high. One may use Johnson-Lindenstrauss transforms (Johnson and Lindenstrauss, 1984) to reduce the running time, since constant approximations are good enough. By standard arguments, it is not hard to verify that the running time will be O(nnz(A) log n) with JL transforms. If k = \u2126(log n), this term is absorbed by the leading term O(nnz(A)k); if k = o(log n), however, this becomes the dominating part. We next provide an algorithm, which computes the exact row norms of A \u2212 Z T ZA in time O(nnz(A)k). Using this algorithm, the total time of FFDsparse is O(nnz(A)k + nk 3 ) +\u00d5(d\u03b1 \u22123 + dk\u03b1 \u22122 ).\nA (i) = Z (i) A (i) takes O(nnz(A (i) )k)\nFaster algorithm for computing row norms.\nLemma 16 Given any A \u2208 R n\u00d7d and Z \u2208 R O(k)\u00d7n , the exact row norms of A \u2212 Z T ZA can be computed in time O(nnz(A)k + dk 2 + nk 2 ) and in space O(nk + dk).\nProof The algorithm is presented in Algorithm 7. We first compute ZA in nnz(A)k time, ", "publication_ref": ["b2", "b5", "b2", "b2", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 7 RowNorms", "text": "Input: A \u2208 R n\u00d7d , Z \u2208 R O(k)\u00d7 1: Compute S = ZA 2: Compute [U, \u03a3, V ] = SVD(S) 3: Compute L = Z T U \u03a3 4: for i = 1 to n do 5: Compute a (1) = A i V 6: Compute w i = a (1) \u2212 L i 2 + A i 2 \u2212 a (1)\na = a (1) V T + b. The ith row of A \u2212 Z T ZA is a \u2212 L i V T = a (1) \u2212 L i V T + b.\nNote that V T and b are orthogonal, and thus\na \u2212 L i V T 2 = a (1) \u2212 L i V T 2 + b 2 = a (1) \u2212 L i 2 + a 2 \u2212 a (1) 2 ,\nwhich can be computed in time O(nnz(a) + k). In all, the total time to compute all the row norms in A \u2212 Z T ZA is\nnnz(A)k + dk 2 + rows(A)k 2 + rows(A) i=1 O(nnz(A i ) + k) = O(nnz(A)k + dk 2 + rows(A)k 2 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Faster FD Implies Faster Matrix Multiplication", "text": "In this subsection, we prove the other direction by showing that the running time of FD is lower bounded by matrix multiplication. The proof is based on the idea of Musco and Woodruff (2017). In particular, we show that the existence an algorithms which computes an (O(1), k)-cov-sketch in time o(nnz(A)k) implies a breakthrough in matrix multiplication, which is likely very difficult. In fact, the lower bound holds even for offline algorithms without constraints on working space.\nTheorem 18 Assume there is an algorithm A , which, given any A \u2208 R n\u00d7d with polynomially bounded integer entries, returns B \u2208 R O(k)\u00d7d in time T (nnz(A), k, n, d) such that\nA T A \u2212 B T B 2 \u2264 \u2206 A \u2212 [A] k 2\nF , for some constant error parameter \u2206. If T is additive and non-decreasing in the first parameter, then there is a T (nnz(M ), k, n, d) + T (nk, k, n, d) + O(dk 2 ) time algorithm for multiplying arbitrary polynomially bounded integer matrices\nM T \u2208 R (d\u2212k)\u00d7n , C \u2208 R n\u00d7k .\nProof For any matrices M \u2208 R n\u00d7(d\u2212k) and C \u2208 R n\u00d7k with integer entries in [\u2212U, U ], let A \u2208 R n\u00d7d be the matrix which is a concatenation of the columns of M and wC, i.e., A = [M, wC] . Here w is large number will be determined later. We have A \u2212\n[A] k 2 F \u2264 M 2\nF \u2264 ndU 2 and\nA T A = M T M wM T C wC T M w 2 C T C .\nWe assume A is an algorithm with running time T , which can compute a sketch matrix B \u2208 R O(k)\u00d7d of A such that\nA T A \u2212 B T B 2 \u2264 \u2206 A \u2212 [A] k 2\nF \u2264 \u2206U 2 nd, for some constant error parameter \u2206.\nThe spectral norm of a matrix N is the largest singular value, which can be equivalently defined as N 2 = max x,y: x = y =1 x T N y, thereby N i,j = e T i N e j \u2264 N 2 for all i, j. It follows that (A T A \u2212 B T B) i,j \u2264 \u2206U 2 nd, meaning the corresponding block of B T B is an entry-wise approximation to wM T C within additive error \u2206U 2 nd. Now if w is a large integer, say w = 3\u2206U 2 nd , we can recover M T C from B T B exactly by rounding the numbers in B T B to their nearest integer multiple of w (as M T C is an integer matrix). Note B T B can be computed in time O(dk 2 ) given B and the rounding can be done in O(dk), so using A, the exact integer matrix multiplication M T C can be computed in time Here we used the fact that nnz(wC) \u2264 nk and the assumption that T is linear in the first parameter. We remark that all the integers in our reduction are at most poly(nd) in magnitude, as long as U = poly(nd), so our reduction works for any M, C with polynomially bounded entries.\nTherefore, if T = o(nnz(A)k) = o(nnz(M )k + nk 2 ), then M T C can be computed in time o(nnz(M )k) + O(nk 2 + dk 2 ), which will be a breakthrough in fast matrix multiplication.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This paper studies the problem of computing covariance sketches for matrices in the streaming model. Based on several novel algorithmic techniques, we provide new space-optimal algorithms with improved running time. We also prove that, under mild assumptions, the time complexity of Frequent Directions is essentially equivalent to that of matrix multiplication up to lower order terms. In particular, this implies that the running times of FFDdense and FFDsparse cannot be significantly improved unless the state-of-the-art matrix multiplication algorithms can, and vice versa. Thus, this paper almost settles the time complexity of this problem.\nSo E[(B T B) i,j ] = (A T A) i,j . We also have p t .\nTherefore,\nE A T A \u2212 B T B 2 F = i,j E (A T A) i,j \u2212 (B T B) i,j 2 \u2264 i,j n t=1 a 2 t,i a 2 t,j p t = n t=1 A t 2 A t 2 p t (10) = n t=1 \u03b1 2 F A t 2 = \u03b1 2 F A 2 F .\nWe adjust \u03b1 by a constant, and using Markov's inequality It is not hard to verify that E B 2 F = A 2 F . So by another Markov inequality, we prove the second part.\nPr A T A \u2212 B T B 2 F \u2265 100\u03b1 2 F A 2 F \u2264 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work is supported by National Natural Science Foundation of China (Grant No. 61802069), Shanghai Sailing Program (Grant No. 18YF1401200) and Shanghai Science and Technology Commission (Grant No. 17JC1420200).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Therefore, the time to compute all the row norms in the jth block A (j) \u2212 Z T ZA (j) is O(nnz(A (j) )k + dk 2 + rows(A (j) )k 2 ), and the total time over all block is thus t j=1 O(nnz(A (i) )k + dk 2 + rows(A (i) )k 2 ) = O(nnz(A)k + nk 3 ), where we use the fact that t = O( nnz(A) dk + nk d ). Space. For space, we need a buffer to store a new block of A, the size of which is at most dk + d, as nnz(A (i) ) is at most dk + d. When using Algorithm 5, the input matrix always has at most d k rows, so we need O(dk) space to compute and store each Z (i) (Lemma 8). A (i) is of dimension O(k) \u00d7 d, which needs O(dk) space to compute and store. According to Lemma 16, the space needed to compute the row norms of each R\nk for all i. From Theorem 7, the space used by FFDdense(A , \u03b1, k) is O(d(k +\u03b1 \u22121 )). Note that, in line 12 of Algorithm 6, the rows of Q (i) can be computed one by one and fed to FD directly, and thus compute C = FD(Q, \u03b1, 0) uses O(d/\u03b1) space by theorem 4. In all, the total space usage is bounded by O(d(k + \u03b1 \u22121 )).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "On the Equivalence Between Frequent Directions and Matrix Multiplication", "text": "In this section, we show that, under mild assumptions, the time complexity of Frequent Directions is essentially equivalent to that of matrix multiplication modulo lower order terms. Let A be an algorithm for multiplying a matrix A \u2208 R n\u00d7d with an arbitrary k by n matrix; and T (nnz(A), k, n, d) be the running time. In this section we will assume T is non-decreasing in the first parameter nnz(A). Moreover, T is additive in the first parameter, i.e., T (a, k, n, d) + T (b, k, n, d) = T (a + b, k, n, d) for any non-negative a and b.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Faster Matrix Multiplication Implies Faster FD", "text": "This direction follows from our FFDsparse algorithm.\nTheorem 17 Let A be an algorithm for multiplying a matrix A \u2208 R n\u00d7d with an arbitrary k by n matrix and assume its running time is T (nnz(A), k, n, d). If T is additive and non-decreasing in the first parameter nnz(A), then Algorithm 6 can be implemented in time\nProof First, we note that the time complexity of our algorithm for sketching sparse matrices is dominated by computing Z (i) A (i) (Line 6 in Algorithm 6\n, since the number of rows in A (i) is at most d/k in the algorithm. By the additive assumption, the total time is", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A. Omitted Proofs", "text": "A.1. Covariance error to projection error\nProof For any x with x = 1, we have\nLet u i and w i be the ith right singular vector of B and A respectively", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2. Row sampling", "text": "Theorem 20 For any A \u2208 R n\u00d7d and F > 0, we sample each row A i with probability p i \u2265 A i 2 \u03b1 2 F ; if it is sampled, scale it by 1/ \u221a p i . Let B be the (rescaled) sampled rows, then w.p. 0.99, A T A \u2212 B T B 2 \u2264 10\u03b1 \u221a F A F , and B F \u2264 10 A F . The expected number of rows sampled is O(\nProof Since spectral norm is no larger than the Frobenius norm, it is sufficient to prove\nif the jth rows of A is sampled 0 otherwise.\nWe have (A T A) i,j = n t=1 a t,i a t,j , while (B T B) i,j = n t=1\nx 2 t \u2022 a t,i a t,j p t .", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sketching structured matrices for faster nonlinear regression", "journal": "", "year": "2013", "authors": "Vikas Haim Avron; David Sindhwani;  Woodruff"}, {"ref_id": "b1", "title": "Concentration inequalities: A nonasymptotic theory of independence", "journal": "Huang Christos Boutsidis", "year": "2013", "authors": "St\u00e9phane Boucheron; G\u00e1bor Lugosi; Pascal Massart"}, {"ref_id": "b2", "title": "Optimal principal component analysis in distributed and streaming models", "journal": "", "year": "2016", "authors": "Christos Boutsidis; Peilin Woodruff;  Zhong"}, {"ref_id": "b3", "title": "Efficient second-order online kernel learning with adaptive embedding", "journal": "", "year": "2017", "authors": "Daniele Calandriello; Alessandro Lazaric; Michal Valko"}, {"ref_id": "b4", "title": "Algorithms for p low rank approximation", "journal": "", "year": "2017", "authors": "Flavio Chierichetti; Sreenivas Gollapudi; Ravi Kumar; Silvio Lattanzi; Rina Panigrahy; David Woodruff"}, {"ref_id": "b5", "title": "Numerical linear algebra in the streaming model", "journal": "ACM", "year": "2009", "authors": "L Kenneth; David P Clarkson;  Woodruff"}, {"ref_id": "b6", "title": "Low rank approximation and regression in input sparsity time", "journal": "", "year": "2013", "authors": "L Kenneth; David P Clarkson;  Woodruff"}, {"ref_id": "b7", "title": "Dimensionality reduction for k-means clustering and low rank approximation", "journal": "ACM", "year": "2015", "authors": "Sam Michael B Cohen; Cameron Elder; Christopher Musco; Madalina Musco;  Persu"}, {"ref_id": "b8", "title": "Input sparsity time low-rank approximation via ridge leverage score sampling", "journal": "", "year": "2017", "authors": "Cameron Michael B Cohen; Christopher Musco;  Musco"}, {"ref_id": "b9", "title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication", "journal": "SIAM Journal on Computing", "year": "2006", "authors": "Petros Drineas; Ravi Kannan; Michael W Mahoney"}, {"ref_id": "b10", "title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "journal": "", "year": "2013", "authors": "Dan Feldman; Melanie Schmidt; Christian Sohler"}, {"ref_id": "b11", "title": "Relative errors for deterministic low-rank matrix approximations", "journal": "", "year": "2014", "authors": "Mina Ghashami; Jeff M Phillips"}, {"ref_id": "b12", "title": "Continuous matrix approximation on distributed data", "journal": "", "year": "2014", "authors": "Mina Ghashami; Jeff M Phillips; Feifei Li"}, {"ref_id": "b13", "title": "Efficient frequent directions algorithm for sparse matrices", "journal": "", "year": "2016", "authors": "Mina Ghashami; Edo Liberty; Jeff M Phillips"}, {"ref_id": "b14", "title": "Streaming anomaly detection using randomized matrix sketching", "journal": "", "year": "2015", "authors": "Hao Huang; Shiva Prasad Kasiviswanathan"}, {"ref_id": "b15", "title": "Efficient matrix sketching over distributed data", "journal": "ACM", "year": "2017", "authors": "Zengfeng Huang; Xuemin Lin; Wenjie Zhang; Ying Zhang"}, {"ref_id": "b16", "title": "Extensions of lipschitz mappings into a hilbert space", "journal": "", "year": "1984", "authors": "B William; Joram Johnson;  Lindenstrauss"}, {"ref_id": "b17", "title": "Principal component analysis and higher correlations for distributed data", "journal": "", "year": "2014", "authors": "Ravi Kannan; Santosh Vempala; David Woodruff"}, {"ref_id": "b18", "title": "Online pca with spectral bounds", "journal": "", "year": "2015", "authors": "Zohar Karnin; Edo Liberty"}, {"ref_id": "b19", "title": "Scalable semi-supervised query classification using matrix sketching", "journal": "", "year": "2016", "authors": "Young-Bum Kim; Karl Stratos; Ruhi Sarikaya"}, {"ref_id": "b20", "title": "Online sketching hashing", "journal": "", "year": "2015", "authors": "Cong Leng; Jiaxiang Wu; Jian Cheng; Xiao Bai; Hanqing Lu"}, {"ref_id": "b21", "title": "Improved distributed principal component analysis", "journal": "", "year": "2014", "authors": "Yingyu Liang; Maria-Florina F Balcan; Vandana Kanchanapally; David Woodruff"}, {"ref_id": "b22", "title": "Simple and deterministic matrix sketching", "journal": "", "year": "2013", "authors": "Edo Liberty"}, {"ref_id": "b23", "title": "Efficient second order online learning by sketching", "journal": "", "year": "2016", "authors": "Haipeng Luo; Alekh Agarwal; Nicolo Cesa-Bianchi; John Langford"}, {"ref_id": "b24", "title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "journal": "", "year": "2015", "authors": "Cameron Musco; Christopher Musco"}, {"ref_id": "b25", "title": "Is input sparsity time possible for kernel low-rank approximation", "journal": "", "year": "2017", "authors": "Cameron Musco; David Woodruff"}, {"ref_id": "b26", "title": "Improved approximation algorithms for large matrices via random projections", "journal": "", "year": "2006", "authors": "Tamas Sarlos"}, {"ref_id": "b27", "title": "Incremental matrix factorization via feature space re-learning for recommender system", "journal": "ACM", "year": "2015", "authors": "Qiang Song; Jian Cheng; Hanqing Lu"}, {"ref_id": "b28", "title": "Matrix sketching over sliding windows", "journal": "", "year": "2016", "authors": "Zhewei Wei; Xuancheng Liu; Feifei Li; Shuo Shang; Xiaoyong Du; Ji-Rong Wen"}, {"ref_id": "b29", "title": "Low rank approximation lower bounds in row-update streams", "journal": "", "year": "2014", "authors": "David Woodruff"}, {"ref_id": "b30", "title": "Sketching as a tool for numerical linear algebra", "journal": "Foundations and Trends R in Theoretical Computer Science", "year": "2014", "authors": "David Woodruff"}, {"ref_id": "b31", "title": "Frequent direction algorithms for approximate matrix multiplication with applications in cca", "journal": "AAAI Press", "year": "2016", "authors": "Qiaomin Ye; Luo Luo; Zhihua Zhang"}, {"ref_id": "b32", "title": "Tracking matrix approximation over distributed sliding windows", "journal": "IEEE", "year": "2017", "authors": "Haida Zhang; Zengfeng Huang; Zhewei Wei; Wenjie Zhang; Xuemin Lin"}, {"ref_id": "b33", "title": "Distributed estimation of generalized matrix rank: Efficient algorithms and lower bounds", "journal": "", "year": "2015", "authors": "Yuchen Zhang; Martin Wainwright; Michael Jordan"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "4.The final sketch is S = [B; C]. Note that S = [B; C] approximate [A ; R] in the sense that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "error bound after adjusting \u03b1 by a constant.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "time, since Z has O(k) rows. So the total time spent on this step is O(nnz(A)k). By definition, there are at most O( nnz(A) dk + nk d ) blocks. Moreover, after left multiplied by Z (i) , each block contributes O(k) rows to A , and thus the total number of rows in A is at most O( nnz(A) d + nk 2 d ). Computing B by invoking B = FFDdense(A , \u03b1, k) needs O(rows(A )dk) + O(d/\u03b1 3 ) = O(nnz(A)k + nk 3 ) +\u00d5(d/\u03b1 3 ) time. Finally, each row of Q can be computed in time O(dk) given A (which has been computed in line 6). Invoking C = FD(Q, \u03b1, 0) needs\u00d5(d/\u03b1 3 + dk/\u03b1 2 ) since rows(Q) =\u00d5(1/\u03b1 2 ) by Lemma 15. So far, the total time is O(nnz(A)k + nk 3 ) +\u00d5(d\u03b1 \u22123 + dk\u03b1 \u22122 ).", "figure_data": ""}, {"figure_label": "2:", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "2 7 :2:end for 8: return w = [w 1 , \u2022 \u2022 \u2022 , w n ] then perform SVD on ZA: ZA = U \u03a3V T . Since ZA is a O(k) \u00d7 d matrix, this takes O(dk 2 ) time. Next compute L = Z T U \u03a3, which takes O(rows(A)k 2 ) time. Let a be the ith row in A. We compute a (1) = aV (in O(nnz(a)k) time), and set b = a \u2212 a (1) V T (we don't compute b explicitly). Then decompose a into two orthogonal parts as", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "T(nnz(A), k, n, d) + O(dk 2 ) = T (nnz(M ), k, n, d) + T (nk, k, n, d) + O(dk 2 ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "01, which is equivalent toPr A T A \u2212 B T B F \u2265 10\u03b1 \u221a F A F \u2264 0.01.The success probability can be boosted by a similar argument as inDrineas et al. (2006)  via  McDiarmid's inequality (see e.g.Boucheron et al. (2013)).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Var[(B T B) i,j ] = Var where we use the fact Var[x 2 t ] = p t (1 \u2212 p t ) \u2264 p t . So we haveE (A T A) i,j \u2212 (B T B) i,j 2 = Var[(B T B) i,j ] \u2264", "figure_data": "n t=1x 2 t \u2022 a t,i a t,j p t=n t=1a 2 t,i a 2 t,j \u2022 Var x 2 t p 2 t\u2264n t=1a 2 t,i a 2 t,j p t,na 2 t,i a 2 t,jt=1"}], "formulas": [{"formula_id": "formula_0", "formula_text": "A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .(1)", "formula_coordinates": [2.0, 221.69, 355.13, 300.31, 14.19]}, {"formula_id": "formula_1", "formula_text": "A \u2212 \u03c0 k B (A) 2 F \u2264 A \u2212 [A] k 2 F + 2k \u2022 A T A \u2212 B T B 2 . Therefore, any ( \u03b5 2k , k)-cov-sketch B has projection error A \u2212 \u03c0 k B (A) 2 F \u2264 (1 + \u03b5) A \u2212 [A] k 2 F .(2)", "formula_coordinates": [2.0, 89.61, 552.2, 432.39, 60.69]}, {"formula_id": "formula_2", "formula_text": "B \u2212 BQQ T 2 F = (1 \u00b1 \u03b5) A \u2212 AQQ T 2 F ,", "formula_coordinates": [2.0, 214.35, 672.88, 188.75, 14.19]}, {"formula_id": "formula_3", "formula_text": "X:rank(X)\u2264k,X\u2208S A \u2212 AXX T 2 F ,", "formula_coordinates": [3.0, 230.52, 146.91, 150.97, 19.29]}, {"formula_id": "formula_4", "formula_text": "+ (n + d) \u2022 poly(k\u03b1 \u22121 )),", "formula_coordinates": [3.0, 416.01, 503.53, 107.5, 11.52]}, {"formula_id": "formula_5", "formula_text": "Huang Time (\u03b1, k)-cov (\u03b5, k)-proj FD (Liberty, 2013) O(ndk + nd/\u03b1) O(ndk/\u03b5) FFDdense (new) O(ndk) O(ndk) Sparse FD(Ghashami et al., 2016) O(nnz(A)k log d + nnz(A) log d/\u03b1) O(nnz(A)k log d/\u03b5) FFDsparse (new) O(nnz(A)k) O(nnz(A)k) Lower bounds (new) \u2126(nnz(A)k) \u2126(nnz(A)k)", "formula_coordinates": [4.0, 104.04, 42.83, 400.59, 133.25]}, {"formula_id": "formula_6", "formula_text": ") Given A \u2208 R n\u00d7d , in one pass, FD(A, \u03b1, k) processes A in O(nd(k + \u03b1 \u22121 )) time and O(d(k + \u03b1 \u22121 )) space. It maintains a matrix B \u2208 R O(k+\u03b1 \u22121 )\u00d7d such that A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .", "formula_coordinates": [5.0, 89.42, 166.61, 432.58, 41.28]}, {"formula_id": "formula_7", "formula_text": "p i \u2265 A i 2 \u03b1 2 F ; if it is sampled, scale it by 1/ \u221a p i . Let B be the (rescaled) sampled rows, then w.p. 0.99, A T A \u2212 B T B 2 \u2264 10\u03b1 \u221a F A F and B F \u2264 10 A F . The expected number of rows sampled is O( A 2 F \u03b1 2 F ) if p i = O( A i 2 \u03b1 2 F ) for each i.", "formula_coordinates": [5.0, 89.29, 333.29, 432.71, 51.47]}, {"formula_id": "formula_8", "formula_text": "A \u2212 SX 2 F \u2264 O(1) A \u2212 [A] k 2 F , Moreover, C only contains O(d) nonzero entries and S = AC T J T can be computed in time O(nnz(A)) + O(nk 3 ).", "formula_coordinates": [5.0, 89.34, 531.26, 432.65, 47.35]}, {"formula_id": "formula_9", "formula_text": "A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .", "formula_coordinates": [5.0, 90.0, 679.91, 432.0, 27.73]}, {"formula_id": "formula_10", "formula_text": "A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A] k 2 F .", "formula_coordinates": [6.0, 231.65, 212.67, 154.15, 14.19]}, {"formula_id": "formula_11", "formula_text": "O(nd ) = O(nd(k + \u03b1 \u22121 )).", "formula_coordinates": [6.0, 397.74, 252.71, 126.38, 11.52]}, {"formula_id": "formula_12", "formula_text": "Algorithm 1 DenseShrink Input: B \u2208 R 2 \u00d7d . 1: Compute [U, \u03a3, V ] = SVD(B), and \u03c3 = \u03a3 , . 2:\u03a3 = max(\u03a3 2 \u2212 \u03c3 2 I 2 , 0) 3: return B =\u03a3V T Algorithm 2 DenseShrinkR Input: B \u2208 R 2 \u00d7d . 1: Compute [U, \u03a3, V ] = SVD(B", "formula_coordinates": [6.0, 89.52, 294.49, 228.94, 139.81]}, {"formula_id": "formula_13", "formula_text": "3:\u03a3 = \u03a3 2 \u2212\u03a3 2 \u03a3 2 =\u03a3 2 +\u03a3 2 4: return B =\u03a3V T ,\u03a3 and V T Algorithm 3 FDShrink Input: B \u2208 R \u00d7d , v \u2208 R d ,", "formula_coordinates": [6.0, 89.52, 453.89, 431.98, 85.17]}, {"formula_id": "formula_14", "formula_text": "1: B = [B; v] 2: If + 1 = 2 , then B = DenseShrink(B, ). 3: return B", "formula_coordinates": [6.0, 95.92, 542.4, 219.89, 36.7]}, {"formula_id": "formula_15", "formula_text": "A T A = B T B + C T C and C 2 F \u2264 A \u2212 [A] k 2 F .", "formula_coordinates": [6.0, 95.46, 652.81, 426.55, 27.73]}, {"formula_id": "formula_16", "formula_text": "Append A i after B 4:", "formula_coordinates": [7.0, 95.92, 150.42, 114.01, 22.74]}, {"formula_id": "formula_17", "formula_text": "[B, \u03a3, V ] = DenseShrinkR(B)", "formula_coordinates": [7.0, 130.36, 177.4, 136.54, 9.69]}, {"formula_id": "formula_18", "formula_text": "p j = \u03a3 2 j \u03b1 2 F 9:", "formula_coordinates": [7.0, 95.92, 228.51, 85.91, 29.31]}, {"formula_id": "formula_19", "formula_text": "11: Set v = C j \u221a p j . 12: Q = FDShrink([Q; v], 1 \u03b1 )", "formula_coordinates": [7.0, 91.32, 273.81, 174.62, 33.75]}, {"formula_id": "formula_20", "formula_text": "A T A \u2212 B T B 2 \u2264 A \u2212 [A] k 2 F /2k, or equivalently max x: x =1 Ax 2 \u2212 Bx 2 \u2264 A \u2212 [A] k 2 F /2k. (3", "formula_coordinates": [7.0, 90.0, 395.54, 432.0, 51.93]}, {"formula_id": "formula_21", "formula_text": ") Let \u03a3 (i) , V(", "formula_coordinates": [7.0, 106.94, 430.67, 415.06, 37.46]}, {"formula_id": "formula_22", "formula_text": "B (i)T B (i) = B (i)T B (i) + V (i)T \u03a3 (i)2 V (i) = B (i)T B (i) + C (i)T C (i) .", "formula_coordinates": [7.0, 154.33, 516.35, 303.33, 12.06]}, {"formula_id": "formula_23", "formula_text": "B (i)T B (i) = B (i\u22121)T B (i\u22121) + A (i)T A (i) .", "formula_coordinates": [7.0, 212.54, 570.61, 186.92, 12.06]}, {"formula_id": "formula_24", "formula_text": "A (i)T A (i) + B (i\u22121)T B (i\u22121) \u2212 B (i)T B (i) = C (i)T C (i) .", "formula_coordinates": [7.0, 186.57, 609.87, 238.86, 12.07]}, {"formula_id": "formula_25", "formula_text": "C T C = i C (i)T C (i) = i A (i)T A (i) + B (i\u22121)T B (i\u22121) \u2212 B (i)T B (i) = A T A \u2212 B T B. It follows that C 2 F = trace(C T C) = trace(A T A) \u2212 trace(B T B) = A 2 F \u2212 B 2 F .", "formula_coordinates": [7.0, 137.0, 664.8, 329.16, 40.18]}, {"formula_id": "formula_26", "formula_text": "C 2 F = A 2 F \u2212 B 2 F = k j=1 Aw j 2 + d j=k+1 Aw j 2 \u2212 B 2 F \u2264 k j=1 Aw j 2 + A \u2212 [A] k 2 F \u2212 k j=1 Bw j 2 because k j=1 Bw j 2 \u2264 B 2 F \u2264 A \u2212 [A] k 2 F + k \u2022 A \u2212 [A] k 2 F /2k by Eq (3) = 1.5 A \u2212 [A] k 2 F .(4)", "formula_coordinates": [8.0, 119.04, 186.08, 402.96, 129.79]}, {"formula_id": "formula_27", "formula_text": "C j 2 \u03b1 2 F", "formula_coordinates": [8.0, 391.18, 330.26, 18.84, 18.25]}, {"formula_id": "formula_28", "formula_text": "C T C \u2212 C T s C s 2 \u2264 \u03b1 A \u2212 [A] k 2 F , and C s 2 F = O(1) \u2022 A \u2212 [A] k 2 F .", "formula_coordinates": [8.0, 148.76, 386.72, 319.93, 14.19]}, {"formula_id": "formula_29", "formula_text": "C T s C s \u2212 Q T Q 2 \u2264 \u03b1 C s 2 F \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F .", "formula_coordinates": [8.0, 191.28, 440.01, 234.89, 14.19]}, {"formula_id": "formula_30", "formula_text": "C T C \u2212 Q T Q 2 \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2", "formula_coordinates": [8.0, 276.29, 467.2, 173.7, 12.73]}, {"formula_id": "formula_31", "formula_text": "A T A \u2212 B T B \u2212 Q T Q 2 = C T C \u2212 Q T Q 2 \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F ,", "formula_coordinates": [8.0, 155.0, 493.31, 307.46, 14.19]}, {"formula_id": "formula_32", "formula_text": "Q = FD(C s , \u03b1, 0), which is O(dk + d/\u03b1) in total.", "formula_coordinates": [8.0, 111.21, 557.55, 231.09, 10.75]}, {"formula_id": "formula_33", "formula_text": "Input: A \u2208 R \u00d7d , J \u2208 R O(k)\u00d7w and C \u2208 R w\u00d7d 1: Compute S = AC T J T 2: Compute Z \u2208 R O(k)\u00d7", "formula_coordinates": [9.0, 90.0, 220.95, 221.87, 39.23]}, {"formula_id": "formula_34", "formula_text": "A \u2212 Z T ZA 2 F \u2264 O(1) A \u2212 [A] k 2 F . Moreover, Z can be computed in O(nnz(A) + k 3 ) time and O( k 2 ) extra space.", "formula_coordinates": [9.0, 89.34, 351.92, 377.98, 36.63]}, {"formula_id": "formula_35", "formula_text": "A \u2212 Z T X 2 F \u2264 O(1) A \u2212 [A] k 2 F .", "formula_coordinates": [9.0, 229.88, 423.65, 157.69, 14.19]}, {"formula_id": "formula_36", "formula_text": "A \u2212 Z T ZA 2 F \u2264 A \u2212 Z T ZA 2 F + Z T ZA \u2212 Z T X 2 F = A \u2212 Z T X 2 F Pythagorean theorem \u2264 O(1) A \u2212 [A] k 2 F .", "formula_coordinates": [9.0, 179.82, 472.79, 257.82, 49.88]}, {"formula_id": "formula_37", "formula_text": "A \u2212 [A ] k 2 F = O(1) \u2022 A \u2212 [A] k 2 F . Moreover, R 2 F = O(1) \u2022 A \u2212 [A] k 2 F .", "formula_coordinates": [9.0, 122.73, 693.46, 354.76, 14.18]}, {"formula_id": "formula_38", "formula_text": "A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A ] k 2 F \u2264 \u03b1 A \u2212 [A] k 2 F . 3. Compute a sketch matrix C of R such that R T R \u2212 C T C 2 \u2264 \u03b1 R 2 F \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2", "formula_coordinates": [10.0, 103.33, 105.97, 418.67, 50.02]}, {"formula_id": "formula_39", "formula_text": "A T A + R T R \u2212 B T B \u2212 C T C 2 \u2264 A T A \u2212 B T B 2 + R T R \u2212 C T C 2 \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F .", "formula_coordinates": [10.0, 141.31, 218.94, 334.33, 31.88]}, {"formula_id": "formula_40", "formula_text": "A \u2212 Z T ZA 2 F \u2264 O(1) A \u2212 [A] k 2 F . Let A = ZA and R = (I \u2212 Z T Z)A.", "formula_coordinates": [10.0, 181.34, 303.56, 342.78, 14.18]}, {"formula_id": "formula_41", "formula_text": "A (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) A (i) \u2212 [A (i) ] k 2 F .", "formula_coordinates": [10.0, 196.97, 397.34, 223.51, 14.19]}, {"formula_id": "formula_42", "formula_text": "A (i) = Z (i) A (i) . 7: w = RowNorms(A (i) , Z (i) )", "formula_coordinates": [11.0, 95.92, 376.29, 145.17, 25.07]}, {"formula_id": "formula_43", "formula_text": "F = O(1) \u2022 i (I \u2212 Z (i)T Z (i) )A (i) 2 F 9:", "formula_coordinates": [11.0, 95.92, 430.49, 424.4, 28.0]}, {"formula_id": "formula_44", "formula_text": "w 2 i \u03b1 2 F for j = 1, \u2022 \u2022 \u2022 , i . Let", "formula_coordinates": [11.0, 247.59, 443.89, 118.81, 18.45]}, {"formula_id": "formula_45", "formula_text": "C = FD([C; Q (i) ], \u03b1, 0). Sketching Q = [Q (1) ; \u2022 \u2022 \u2022 ; Q (t)", "formula_coordinates": [11.0, 119.46, 515.09, 350.47, 11.52]}, {"formula_id": "formula_46", "formula_text": "A T A \u2212 S T S 2 \u2264 \u03b1 A \u2212 [A] k 2 F . The algorithm uses O(d(k + \u03b1 \u22121 )) space and O(nnz(A)k + nk 3 ) +\u00d5(d\u03b1 \u22123 + dk\u03b1 \u22122 ) time.", "formula_coordinates": [12.0, 88.28, 132.52, 429.86, 29.39]}, {"formula_id": "formula_47", "formula_text": "A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ], R = [R (1) ; \u2022 \u2022 \u2022 ; R (t) ], and Q = [Q (1) ; \u2022 \u2022 \u2022 ; Q (t)", "formula_coordinates": [12.0, 90.0, 276.44, 317.34, 11.52]}, {"formula_id": "formula_48", "formula_text": "A \u2212 [A ] k 2 F \u2264 A \u2212 [A] k 2 F ; (2) R 2 F \u2264 O(1) \u2022 A \u2212 [A] k 2 F . Proof We divide A and A into blocks as defined in Algorithm 6, i.e. A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ] and A = [A (1) ; \u2022 \u2022 \u2022 ; A (t) ]", "formula_coordinates": [12.0, 90.0, 310.11, 433.28, 58.73]}, {"formula_id": "formula_49", "formula_text": "A \u2212 [A ] k 2 F \u2264 A \u2212 A P 2 F since P is of rank k = t i=1 A (i) \u2212 A (i) P 2 F = t i=1 Z (i) A (i) \u2212 Z (i) A (i) P 2 F \u2264 t i=1 A (i) \u2212 A (i) P 2 F Z (i) is a orthonormal = A \u2212 AP 2 F = A \u2212 [A] k 2 F , by definition of P which proves (1).", "formula_coordinates": [12.0, 89.61, 416.54, 392.48, 162.25]}, {"formula_id": "formula_50", "formula_text": "Z (i) = LRA(A (i) , J, C). Since A (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) \u2022 A (i) \u2212 [A (i) ] k 2", "formula_coordinates": [12.0, 95.46, 580.82, 426.54, 27.73]}, {"formula_id": "formula_51", "formula_text": "R 2 F = t i=1 R (i) 2 F = t i=1 (I \u2212 Z (i)T Z (i) )A (i) 2 F \u2264 O(1) \u2022 t i=1 A (i) \u2212 [A (i) ] k 2 F \u2264 O(1) \u2022 A \u2212 [A] k 2 F", "formula_coordinates": [12.0, 195.01, 614.12, 225.76, 88.92]}, {"formula_id": "formula_52", "formula_text": "A (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) \u2022 A (i) \u2212 [A (i) ] k 2 F", "formula_coordinates": [13.0, 95.46, 92.42, 426.55, 27.73]}, {"formula_id": "formula_53", "formula_text": "A (i) \u2212 S (i) X 2 F = min X:rank(X)\u2264k A (i) \u2212 Z (i)T X 2 F = min X:rank(X)\u2264k A (i) \u2212 Z (i)T Z (i) A (i) + Z (i)T Z (i) A (i) \u2212 Z (i)T X 2 F = min X:rank(X)\u2264k A (i) \u2212 Z (i)T Z (i) A (i) 2 F + Z (i)T Z (i) A (i) \u2212 Z (i)T X 2 F by Pythagorean \u2265 A (i) \u2212 Z (i)T Z (i) A (i) 2", "formula_coordinates": [13.0, 100.0, 205.72, 415.04, 90.17]}, {"formula_id": "formula_54", "formula_text": "X:rank(X)\u2264k A \u2212 SX 2 F = min X:rank(X)\u2264k t i=1 A (i) \u2212 S (i) X 2 F \u2265 t i=1 min X:rank(X)\u2264k A (i) \u2212 S (i) X 2 F . \u2265 t i=1 A (i) \u2212 Z (i)T Z (i) A (i) 2 F (5)", "formula_coordinates": [13.0, 169.18, 321.9, 352.82, 109.3]}, {"formula_id": "formula_55", "formula_text": "X:rank(X)\u2264k A \u2212 SX 2 F \u2264 O(1) A \u2212 [A] k 2 F .(6)", "formula_coordinates": [13.0, 201.88, 453.83, 320.12, 19.29]}, {"formula_id": "formula_56", "formula_text": "R 2 F = t i=1 R (i) 2 F = t i=1 A (i) \u2212 Z (i)T Z (i) A (i) 2 F \u2264 O(1) A \u2212 [A] k 2 F .", "formula_coordinates": [13.0, 143.52, 499.03, 330.41, 33.71]}, {"formula_id": "formula_57", "formula_text": "A T A\u2212A T A F \u2264 O(1) \u2022 A \u2212 [A] k 2 F", "formula_coordinates": [13.0, 90.0, 590.29, 432.0, 27.73]}, {"formula_id": "formula_58", "formula_text": "R (i)T R (i) = A (i)T (I \u2212 Z (i)T Z (i) ) \u2022 (I \u2212 Z (i)T Z (i) )A (i) = A (i)T (I \u2212 Z (i)T Z (i) )A (i) since (I \u2212 Z (i)T Z (i) ) is a projection = A (i)T A (i) \u2212 A (i)T A (i) .", "formula_coordinates": [13.0, 90.0, 655.85, 437.02, 49.13]}, {"formula_id": "formula_59", "formula_text": "A T A \u2212 A T A F = R T R F \u2264 R 2 F ,", "formula_coordinates": [14.0, 220.45, 115.97, 176.56, 14.19]}, {"formula_id": "formula_60", "formula_text": ") \u2265 1.1k, then A \u2212 [A] k 2 F \u2265 1/poly(nd).", "formula_coordinates": [14.0, 89.46, 200.31, 432.55, 25.9]}, {"formula_id": "formula_61", "formula_text": "\u03c3 k \u2265 (nd\u03b3 2 ) \u2212k/2(\u03c1\u2212k) .", "formula_coordinates": [14.0, 254.53, 312.95, 102.93, 13.27]}, {"formula_id": "formula_62", "formula_text": "Q 2 F = O( R 2 F ), R T R \u2212 Q T Q 2 = O(\u03b1) \u2022 A \u2212 [A] k 2 F ,", "formula_coordinates": [14.0, 173.28, 417.09, 270.89, 14.19]}, {"formula_id": "formula_63", "formula_text": "R i 2 \u03b1 2 R 2 F", "formula_coordinates": [14.0, 384.55, 496.81, 29.12, 20.73]}, {"formula_id": "formula_64", "formula_text": "Otherwise if R 2 F \u2264 \u03b7 \u2264 A \u2212 [A] k 2 F , then the probability for sampling each row is \u2126( R i 2 \u03b1 2 \u03b7 ) = \u2126( R i 2 \u03b1 2 A\u2212[A] k 2 F", "formula_coordinates": [14.0, 90.0, 530.62, 432.4, 34.13]}, {"formula_id": "formula_65", "formula_text": "O(log R 2 F \u03b7 ) \u2264 O log A 2 F \u2022 poly(nd) = O(log(nd)).", "formula_coordinates": [14.0, 172.21, 642.18, 267.57, 26.49]}, {"formula_id": "formula_66", "formula_text": "Q T Q \u2212 C T C 2 \u2264 \u03b1 Q 2 F \u2264 O(\u03b1) \u2022 R 2 F .(7)", "formula_coordinates": [15.0, 201.19, 236.35, 320.81, 14.19]}, {"formula_id": "formula_67", "formula_text": "A T A \u2212 B T B 2 \u2264 \u03b1 A \u2212 [A ] k 2 F \u2264 \u03b1 A \u2212 [A] k 2 F ,(8)", "formula_coordinates": [15.0, 186.12, 283.07, 335.88, 14.19]}, {"formula_id": "formula_68", "formula_text": "A T A \u2212 S T S 2 = A T A \u2212 B T B \u2212 C T C 2 \u2264 A T A \u2212 A T A \u2212 C T C 2 + A T A \u2212 B T B 2 \u2264 A T A \u2212 A T A \u2212 C T C 2 + \u03b1 A \u2212 [A] k 2 F by (8) = R T R \u2212 C T C 2 + \u03b1 A \u2212 [A] k 2 F \u2264 R T R \u2212 Q T Q 2 + Q T Q \u2212 C T C 2 + \u03b1 A \u2212 [A] k 2 F triangle inequality \u2264 O(\u03b1) \u2022 A \u2212 [A] k 2 F + O(\u03b1) \u2022 R 2 F + \u03b1 A \u2212 [A] k 2 F by(", "formula_coordinates": [15.0, 118.13, 329.78, 354.68, 103.88]}, {"formula_id": "formula_69", "formula_text": "i O(nnz(A (i) ) + i k 2 ) = O(nnz(A) + nk 2 ).", "formula_coordinates": [15.0, 208.76, 518.2, 200.92, 24.58]}, {"formula_id": "formula_70", "formula_text": "A (i) = Z (i) A (i) takes O(nnz(A (i) )k)", "formula_coordinates": [15.0, 323.35, 553.66, 171.47, 11.52]}, {"formula_id": "formula_71", "formula_text": "Input: A \u2208 R n\u00d7d , Z \u2208 R O(k)\u00d7 1: Compute S = ZA 2: Compute [U, \u03a3, V ] = SVD(S) 3: Compute L = Z T U \u03a3 4: for i = 1 to n do 5: Compute a (1) = A i V 6: Compute w i = a (1) \u2212 L i 2 + A i 2 \u2212 a (1)", "formula_coordinates": [16.0, 90.0, 278.44, 237.12, 95.37]}, {"formula_id": "formula_72", "formula_text": "a = a (1) V T + b. The ith row of A \u2212 Z T ZA is a \u2212 L i V T = a (1) \u2212 L i V T + b.", "formula_coordinates": [16.0, 89.61, 482.25, 294.05, 61.02]}, {"formula_id": "formula_73", "formula_text": "a \u2212 L i V T 2 = a (1) \u2212 L i V T 2 + b 2 = a (1) \u2212 L i 2 + a 2 \u2212 a (1) 2 ,", "formula_coordinates": [16.0, 132.22, 581.18, 353.01, 18.02]}, {"formula_id": "formula_74", "formula_text": "nnz(A)k + dk 2 + rows(A)k 2 + rows(A) i=1 O(nnz(A i ) + k) = O(nnz(A)k + dk 2 + rows(A)k 2 ).", "formula_coordinates": [16.0, 100.65, 650.06, 410.7, 34.82]}, {"formula_id": "formula_75", "formula_text": "A T A \u2212 B T B 2 \u2264 \u2206 A \u2212 [A] k 2", "formula_coordinates": [18.0, 230.62, 232.52, 150.39, 13.27]}, {"formula_id": "formula_76", "formula_text": "M T \u2208 R (d\u2212k)\u00d7n , C \u2208 R n\u00d7k .", "formula_coordinates": [18.0, 376.58, 280.86, 131.19, 12.13]}, {"formula_id": "formula_77", "formula_text": "[A] k 2 F \u2264 M 2", "formula_coordinates": [18.0, 95.46, 327.31, 426.55, 25.07]}, {"formula_id": "formula_78", "formula_text": "A T A = M T M wM T C wC T M w 2 C T C .", "formula_coordinates": [18.0, 236.27, 362.0, 139.47, 30.07]}, {"formula_id": "formula_79", "formula_text": "A T A \u2212 B T B 2 \u2264 \u2206 A \u2212 [A] k 2", "formula_coordinates": [18.0, 206.0, 434.39, 150.39, 13.27]}, {"formula_id": "formula_80", "formula_text": "E A T A \u2212 B T B 2 F = i,j E (A T A) i,j \u2212 (B T B) i,j 2 \u2264 i,j n t=1 a 2 t,i a 2 t,j p t = n t=1 A t 2 A t 2 p t (10) = n t=1 \u03b1 2 F A t 2 = \u03b1 2 F A 2 F .", "formula_coordinates": [21.0, 176.92, 254.99, 345.08, 138.67]}, {"formula_id": "formula_81", "formula_text": "Pr A T A \u2212 B T B 2 F \u2265 100\u03b1 2 F A 2 F \u2264 0.", "formula_coordinates": [21.0, 198.73, 428.51, 201.08, 14.19]}], "doi": ""}