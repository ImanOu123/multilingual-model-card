{"title": "LexSym: Compositionality as Lexical Symmetry", "authors": "Ekin Aky\u00fcrek; Jacob Andreas", "pub_date": "", "abstract": "In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme-a procedure for transforming examples into other well-formed examples-that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deployed agnostically across text, structured data, and even images. It matches or surpasses state-of-the-art, task-specific models on COGS semantic parsing, SCAN and ALCHEMY instruction following, and CLEVR-COGENT visual question answering datasets.", "sections": [{"heading": "Introduction", "text": "A central challenge in natural language processing is the design of models and learning algorithms that are simultaneously flexible enough to capture the variability of human language and structured enough to generalize in predictable and humanlike ways. One important source of structure is the principle of compositionality, which (in one formulation) states that sentence meanings can be computed from a lexicon of word meanings and a set of composition rules governing how meanings combine (Montague, 1970b). A long line of language processing research has operationalized the principle of compositionality as a constraint on model architectures, via independence assumptions or parameter tying schemes that ensure a compositional process of sentence interpretation (Lewis and Stearns, 1968;Andreas et al., 2016). Compositional models enjoy sample-efficient learning and strong generalization in tasks from machine translation to question answering (McCoy et al., 2020).\nBut much of human language is not (or at least not straightforwardly) compositional. Idioms, disfluencies, and context-sensitive meanings present major challenges to models in which all predictions must derive from a sequence of local composition operations. In recent years, more generic model architectures such as recurrent neural networks (RNNs) and transformers, with no explicit compositional scaffolding, have consistently outperformed compositional models in language processing tasks with natural data (Wu et al., 2016). However, these models capture linguistic regularities only when trained on enormous amounts of data, and make surprising or problematic predictions when presented with novel word collocations or syntactic structures (Lake and Baroni, 2018).\nHow can we train unstructured neural sequence models that generalize compositionally? Recent work has introduced several compositional data augmentation schemes: rule-based procedures or learned models that synthesize artificial training examples to promote generalization (Andreas, 2020;Shaw et al., 2021;Aky\u00fcrek et al., 2021;Zhang et al., 2022, inter alia). While often effective, existing methods are specialized to specific data modalities or datasets. The conditions under which they succeed, and their relationships to the formal principle of compositionality, have remained unclear.\nThis paper presents a framework for understanding and improving such data-centric approaches to compositional modeling. We first provide a mathematical characterization of the principle of compositionality as a constraint on data distributions rather than model architectures. Intuitively, we show that whenever a language understanding task can be solved compositionally, that task's data distribution is guaranteed to exhibit specific symmetries. These symmetries are functions that modify data points while preserving semantic acceptability. Fig. 1c gives an example of a symmetry in a visual question answering problem: in any wellformed (image, question, answer) triple, swapping the words yellow and green and their associated pixel values yields a valid new triple. Such symmetries exist even in complex tasks like instruction following (Fig. 1a), where they may depend not only on word-to-meaning mappings but relations between meanings (like the fact that red and green mix to produce brown).\nBuilding on this formal link between compositionality and symmetry, we introduce a procedure called LEXSYM that discovers symmetries automatically, then uses them to synthesize new training examples guaranteed to be correct and informative. Crucially, LEXSYM does not require a complete compositional theory for a given problem domain-only a lexicon of word meanings. These lexicons may themselves be automatically derived for most tasks. This makes LEXSYM very flexible: it requires little or no task-specific engineering, can be combined with any predictor, and unlike other compositional data augmentation schemes does not require tree-structured or even sequential data.\nApplied to ordinary neural sequence models, LEXSYM outperforms state-of-the-art models on the CLEVR COGENT visual question answering benchmark (Johnson et al., 2017) by a wide margin. LEXSYM is general, and matches or outperforms some specialized data augmentation schemes and models on the COGS semantic parsing task (Kim and Linzen, 2020;Kim et al., 2022), and the SCAN and ALCHEMY instruction following tasks (Lake and Baroni, 2018;Long et al., 2016).\nThis paper thus offers two contributions: a theoretical contribution, in the form of a new lens on the principle of compositionality via symmetries of data distributions; and an empirical contribution, in the form of a data augmentation scheme that improves generalization on diverse language understanding tasks. The recent success of data augmentation approaches highlight the fact that compositional inductive bias need not require compositional models. Our work formalizes and generalizes this \"data-centric\" account of compositionality. 1", "publication_ref": ["b17", "b8", "b14", "b37", "b7", "b28", "b0", "b7", "b11"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Background & Approach", "text": "We begin with a discussion on the more general role of symmetry in machine learning applications. Definition 1. A symmetry of a set X is a function f satisfying:\n{f (x) : x \u2208 X} = X (1)\nThat is, applying f to each element of X leaves X unchanged.\nA familiar example from computer vision is reflection symmetry: in object recognition problems, image classes are generally invariant under reflection (a zebra seen in a mirror is still a zebra). The set of (image, class) pairs thus has as a symmetry the function (x, y) \u2192 (reflect(x), y). In many domains, especially those (like computer vision and computational chemistry) that are constrained by physical laws, knowledge of the symmetries exhibited by a problem domain can dramatically reduce the difficulty of learning (Batzner et al., 2022;Simeonov et al., 2022).\nPast work has incorporated symmetry into machine learning problems in two ways. Invariant and equivariant modeling approaches structurally enforce symmetries via specialized architectures (improving generalization by decreasing the size of the hypothesis class; Cohen and Welling, 2016). Data augmentation approaches generate new training examples by applying known symmetries like reflections directly to training data (improving generalization by increasing dataset size; Shorten and Khoshgoftaar, 2019). Data augmentation, the focus of this paper, is model-agnostic, and can be used in conjunction with pre-training while producing the same asymptotic effects as specialized model architectures (Chen et al., 2020).\nThe question this paper aims to answer is whether compositionality, like other domainspecific constraints, can be formalized in the language of symmetry. We are not the first to consider this question: Kiddon and Domingos (2015) define a theory of semantic equivalence in terms of symmetries of the set of natural language sentences, and Gordon et al. (2020) propose a model architecture for compositional semantic parsing via a symmetry that enforces permutation invariance of lexicon entries. LEXSYM also derives symmetries from lexicons. It builds on past work by (1) characterizing the algebraic relationship between compositionality and symmetry, explaining the effectiveness of both Gordon et al. (2020)'s approach as well as other data augmentation schemes based on token and phrase substitution (Andreas, 2020;Wang et al., 2018); (2) discovering symmetries automatically, and (3) showing how to leverage them in a model-and modality-agnostic way. Additional related work is discussed in Sec. 6.", "publication_ref": ["b30", "b29", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Compositionality as Lexical Symmetry", "text": "Our main theoretical result, and the foundation of our modeling approach, can be stated as follows: in any language understanding task that can be modeled compositionally, data for the task exhibits symmetries in the sense of Definition 1. We explain, formalize, and prove this statement below.\nWe consider tasks defined by a space of possible examples X , of which a subset of examples X are well-formed. We assume each example x \u2208 X is a discrete sequence [x 1 , . . . , x n ], with x i drawn from a vocabulary \u03a3. Finally, we assume that well-formedness can be computed by a a binary interpretation function I : X \u2192 {0, 1} with I(x) = 1 iff x \u2208 X. A wide variety of language understanding problems, from very simple to very complex, may be defined in this way: Example 1a: Arithmetic Language Modeling. Examples x are true sentences of the form a plus b is c, where a, b and c are numbers: I(one plus two is three) = 1 but I(two plus two is five) = 0.\nExample 1b: Semantic Parsing. Examples x are pairs (x NL , x LF ), where x NL is an sentence, x LF is a logical form, and I(x NL , x LF ) = 1 iff x LF represents a possible meaning of x NL (Fig. 1b).\nExample 1c: Visual Question Answering.\nExamples x are triples (x Q , x I , x A ), where x Q is a question, x I is a (rasterized) image, x A is an answer, and I(x Q , x I , x A ) = 1 iff x A is the answer to x Q in x I (Fig. 1c).\nNotice that the vocabulary \u03a3 contains not just natural language words, but other kinds of data: logical symbols (1b) or even image patches (1c).\n\"Language understanding\" in each of these tasks is encapsulated by the function I. What does it mean for I to be compositional? Under most definitions, a compositional language understanding procedure should factorize into a lexicon, which captures meanings of words, and a composition procedure, which derives example-level interpretations from these meanings. We model word meanings in terms of relations between items in \u03a3. In arithmetic, to know the meaning of the word five is to know that it is a number, less than seven, the successor of four, etc. In semantic parsing, the meaning of the word cat is encapsulated by the fact that it is of the same type as dog, and translatable into the logical symbol cat \u2032 . We model this notion of word meaning by equipping \u03a3 with extra structure describing these relations: Definition 2. A lexical algebra is a collection of relations r 1 , . . . , r n between vocabulary items, where each r : \u03a3 p \u2192 {0, 1}. A lexical algebra can represent type information, like \"dog is a noun\", as a unary relation; semantic correspondence, like \"sings maps to sing \u2032 \", as a binary relation; and richer semantic knowledge, like \"three is the sum of one and two\", with higher-order relations.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "We may then represent individual examples in purely relational terms:", "text": "Figure 2: Idealized compositional semantic parser following Definition 3. A (sentence, logical form) pair is translated into a lexical representation containing information about each word's type and meaning. We then determine whether the sentence evaluates to the logical form using only the type and semantic correspondence matrices, using types to assign the sentence an abstract logical form, and correspondences to determine whether it matches the target. Definition 3. Denote the lexical representation L(x) = (R 1 (x), . . . , R n (x)). R(x) is an orderp tensor whose (i, . . . , j) th entry is equal to r(x i , . . . , x j ). (If r is a binary relation, R(x) is an |x| \u00d7 |x| matrix and R(x) ij specifies whether r holds between x i and x j .) See Fig. 2 for examples.\nFinally, we use this relational representation to define compositionality of interpretation functions:\nDefinition 4. X is L-compositional if I(x) = C(L(x)\n) for some composition procedure C. In other words, X is compositional if it compute the well-formedness of x from word-level meanings and a generic composition procedure. 2 This definition makes no assumptions about C beyond the fact that it can be defined purely in terms of L(x). It can be applied to many tasks: Example 2a: Arithmetic Language Modeling. Define r 1 to be the ternary relation (a, b, c) \u2192 1 [a+b=c] . Then C takes an example and checks whether the index corresponding to its three number words is true in R 1 .\nExample 2b: Semantic Parsing. A sketch of a 2 Every I is trivially L-compositional with respect to an L that assigns every vocabulary item to a unique unary relation. semantic parser factorizable into a lexicon and an abstract composition function is depicted in Fig. 2. As a real-world example, in the factored CCG semantic parser of Kwiatkowski et al. (2011), words are assigned types and logical forms via a lexicon. These logical fragments are then composed by a parsing algorithm that depends only their types.\nExample 2c: Natural Language Inference. Mac-Cartney and Manning (2014)'s Natural Logic framework provides a procedure for determining entailment relations between sentences via a set of sentence rewriting operations that use only wordlevel information about entailment relations.\nUnder Definition 4, a sentence interpretation procedure is compositional if the meaning of a sentence can be derived in a generic way (C) from the meanings of its lexical items (L). 3 We remark, finally, that the parsing procedure depicted in Fig. 2 is an idealization used to motivate our approach; our experiments use more flexible models.\nWe are now ready to describe how, for compositional I, structure in L translates into structure in the set of well-formed examples X.\nDefinition 5. A function f is a homomorphism of (\u03a3 \u03a3 \u03a3, L L L) (an \"L-homomorphism\") if: \u2200r \u2208 L, \u2200x 1 . . . x p \u2208 \u03a3 : r(x 1 , . . . , x p ) = r(f (x 1 ), . . . , f (x p )) (2)\nf \"preserves the structure\" of L, ensuring that pairwise relationships are preserved among symbols. Fig. 1 shows examples: in (c), for instance, the words yellow and green and the corresponding colors must be swapped to satisfy Eq. 2.\nFinally, we may state our main result:\nTheorem 1. If X is L-compositional, f is an L-homomorphism, and x \u2208 X, then f (x) = [f (x 1 ), . . . , f (x n )] \u2208 X. Thus every homomor- phism of L well-formed examples \u2208 X. Proof. From Definition 3 and 5, R i (f (x)) = R i (x) \u2200i. Then, 1 [f (x)\u2208X] = I(f (x)) = C(L(f (x))) = C(R 1 (f (x)), . . . , R n (f (x))) = C(R 1 (x), . . . , R n (x)) = I(x) = 1 [x\u2208X]\nCorollary 1. With the additional constraint that f is an L-isomorphism (i.e., has an inverse), then f is a symmetry of X in the sense of Eq. 1.\nHere it suffices to show that the preimage of every x \u2208 X is also in X; the proof is the same as Theorem 1 with f \u22121 in place of f . Despite their simplicity, Theorem 1 and its corollary have an important consequence: if we can identify candidate entries in L, even if C is unknown, we can construct new examples x \u2208 X that respect, and provide evidence for, the compositional structure of X. There is an intriguing (if inexact) structural similarity between Corollary 1 and Noether's theorem (Noether, 1918), which establishes an equivalence between symmetries of physical systems and their conserved quantities. Here, such symmetries imply constraints not on conservation laws but interpretation functions.", "publication_ref": ["b5", "b19"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "LEXSYM: Data Augmentation with L L L-homomorphisms", "text": "Given a lexicon describing symbols and their relations, we have shown how to turn homomorphisms of the lexicon into transformations of a dataset. Each such function f that takes an example x as input, replaces each token x i \u2208 x with a new one, and returns a well-formed example x \u2032 as output. Every L-homomorphism may thus be viewed as a recipe for synthesizing training examples from a small initial training set (Japkowicz et al., 2000). However, to make this a practical modeling tool, we need some way of constucting L-homomorphisms for a task of interest. Below, we describe how to do so automatically: first, starting with only a taskspecific lexicon L (Sec. 4.1); next, starting with only a dataset and no initial lexicon (Sec. 4.2). We term the resulting approach LEXSYM.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Deriving Homomorphisms from Lexicons", "text": "Even in complex sequence modeling problems, useful lexicons are often simple enough that they can be specified by hand (Jones et al., 2012;Gordon et al., 2020). Given a pre-specified algebraic L, there is a straightforward procedure for generating the associated symmetries by enumerating all functions \u03a3 \u2192 \u03a3 and testing which ones satisfy Eq. 2. (See Algorithm 1 in Appendix B.) This algorithm is inefficient, but simple and practical for small |L|.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Deriving Lexicons from Datasets", "text": "For some tasks, it may be difficult to manually specify an algebraic lexicon. We next describe how to infer one automatically. We focus on an important and extremely common class of language understanding problems with special structure. In semantic parsing and instruction following, examples x consist of (input, output) pairs in which inputs are sentences, outputs are meaning representations, and word meaning is characterized by a lexicon with two components. First, a set of unary type predicates {r \u03c4 } that assign words to types (like ENTITY in semantic parsing). Second, a semantic correspondence relation r \u03f5 that specifies which actions or logical symbols can be derived from words (like sings \u2192 sing \u2032 ). With n types, the lexicon required for these problems is L = (r \u03c4 1 , . . . , r \u03c4n , r \u03f5 ), which we abbreviate ({r \u03c4 k }, r \u03f5 ) below. We now show how to improve upon the procedure in Sec. 4.1 by deriving L from data and sampling L-homomorphisms in constant time.\nLearning L L L We build on past work noting that dictionaries of semantic correspondences can be constructed using alignment algorithms (Brown et al., 1993). Given an input x consisting of a pair (x text , x meaning ), we use existing algorithms to align tokens in individual training examples. Finally, we identify the most frequently occurring alignments and add these to the semantic correspondence relation. We may similarly use existing procedures to infer types by deriving them from part-of-speech tags or distributional patterns. See Appendix D for details of the alignment and type inference algorithms used in our experiments. These algorithms produce lexicons with three properties that are useful for the sampling scheme we describe next: types are disjoint, and semantic correspondences are oneto-many and type-preserving (if two words are of the same type, so are their translations).\nSampling L L L-homomorphisms Once we have identified types and semantic correspondences, sampling L-homomorphisms is straightforward: Theorem 2. Let x i and x j \u2208 \u03a3 have the same type r \u03c4 (x i ) = r \u03c4 (x j ) = 1. For convenience, let E i = {x : r \u03f5 (x i , x) = 1} denote possible translations of\nx i . The f is an L-homomorphism:\nf (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 x j if x = x i x i if x = x j x \u2032 \u2208 E j if x \u2208 E i x \u2032 \u2208 E i if x \u2208 E j x otherwise (3)\nProof is given in Appendix A. Theorem 2 yields an intuitive data augmentation procedure: select two (input, output) pairs of the same type, and swap them and any of their meanings wherever they occur. Fig. 1b shows an example. Eq. 3 is related to data augmentation schemes described by Andreas (2020) and Liu et al. (2021b), which synchronously substitute words or phrases (equivalent to removing cases 2 and 4). Unlike LEXSYM, these methods cannot guarantee correctness: in Fig. 1c, substituting green in place of yellow yields an image with two green objects and an incorrect answer.", "publication_ref": ["b10"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Experiments", "text": "Our experiments aim to evaluate whether LEXSYM can improve compositional generalization in downstream models. The main goal of these experiments is to evaluate generality across tasks and data modalities. Evaluation focuses on three diverse classes of language understanding problems: complex, context-dependent computations (Sec. 5.1), large, automatically derived lexicons (Sec. 5.2), and multi-modal data (Sec. 5.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Complex computations", "text": "We first test LEXSYM on the ALCHEMY task from the SCONE benchmark (Long et al., 2016)-a problem involving a complex sentence interpretation procedure that makes it challenging to apply existing data augmentation schemes.\nData In ALCHEMY (Fig. 1a), models must execute a sequence of human-written English instructions x 1:N ins , on an initial state x 0 state consisting of beakers of colored liquids (textually represented as sequence of symbols \"1: g g , 2: ...\"), to predict the final state LEXSYM We manually construct a lexicon to showcase how to inject prior knowledge into LEXSYM. We encode word meaning in two relations: a semantic equivalence relation between color words and colors: \nr \u03f5 (c 1 , c 2 ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 c 1 = brown, c 2 = b 1 c 1 = red, c 2 = r 1 c 1 = green, c 2 = g . . .\nr mix (c 1 , c 2 , c 3 ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 c 1 = c 2 = c 3 1 c 1 \u0338 = c 2 \u2227 c 3 = b 0 otherwise Together, (r \u03f5 , r mix , {r \u03c4 k }),\nwhere {r \u03c4 k } assigns different types to color words, colors, and remaining tokens. The homomorphic transformations of this lexicon exchange color words and colors but preserve mixing relations.", "publication_ref": ["b11"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Models and Training", "text": "We train an LSTM (Hochreiter and Schmidhuber, 1997) and finetune a T5 transformer (Raffel et al., 2020) on the sequence-to-sequence prediction problem (x 1:N ins , x 0 state ) \u2192 x N state Training details may be found in Appendix C. We compare these baseline models to their LEXSYM-augmented versions as well as the existing compositional data augmentation scheme of Liu et al. (2021b).\nResults See Table 1. LSTM+LEXSYM improves substantially over an LSTM. Preserving the homomorphism condition in Eq. 2 is extremely important: the procedure of Liu et al. (2021b), which naively substitutes aligned color pairs, actually hurts performance. Pre-trained models achieve strong initial results; combining pre-training with LEXSYM gives additional improvements.", "publication_ref": ["b25", "b10", "b10"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Learned lexicons", "text": "We next show that for more conventional sequenceto-sequence problems, we may apply LEXSYM with automatically derived lexicons.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-modal data", "text": "Finally, we combine learned lexicons with nonsequential data to advance the state of the art on a long-standing visual question answering challenge.\nData The CLEVR dataset (Johnson et al., 2017, Fig. 1c) contains English-language questions about generated 3D scenes containing multiple objects.  3). We use this discretization procedure for our experiments (see Appendix C.1 for details). We use the same algorithm as previous section to extract lexical relations.\nModels Most prior work on visual question answering has used pre-trained convolutional networks to encode images, and recurrent networks to encode questions and generate answers. For experiments on CLEVR, we use a simplified model in which both questions and images are mapped to answers by a transformer model, similarly to Ramesh et al. (2021). See Appendix C.2 for details. Both LEXSYM augmentation and this VQA-Transformer model operate over sequences of discrete visual codes produced by a vector-quantized variational autoencoder. Once these discrete representations have been produced, we infer lexicons and perform data augmentation directly to these representations, without re-synthesizing images (though such synthesis is possible, as in Table 3, to interpret model behavior).\nThe COGENT task is very different from the sequence modeling tasks discussed above: inputs contain many tokens, and the training set is orders of magnitude larger. GECA and CSL-Aug, which have a high polynomial dependence on sequence length, could not be applied as they fail to terminate within a reasonable amount of time.", "publication_ref": ["b26"], "figure_ref": ["fig_0"], "table_ref": ["tab_8", "tab_8"]}, {"heading": "Results", "text": "In Table 2, a transformer model with LEXSYM achieves state-of-the-art results on the CLEVR-COGENT dataset, reducing errors by roughly 33% relative to the best existing system. LEXSYM also outperforms substitution based-data augmentation (Liu et al., 2021b), particularly on semantically complex utterances involving quantification (App. Table 4). On the IID CLEVR split, LEXSYM's performance is comparable to humans, and somewhat behind pre-trained models.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": ["tab_2", "tab_9"]}, {"heading": "Other Related Work", "text": "Lexicalized neural models Word-level alignments between input and output sequences were an essential feature of statistical phrase-and treebased sequence models (Chiang et al., 2005;Koehn et al., 2003). Neural scoring functions were sometimes integrated into these models (Misra and Artzi, 2016). Neural models with attention (Bahdanau et al., 2015) do not require explicit alignment, though several pieces of past work have shown that incorporating explicit token-level correspondences improves generalization (Akyurek and Andreas, 2021;Prabhu and Kann, 2020;. The semantic correspondence function in Sec. 4 plays the same role as the input-output dictionary in these methods, but LEXSYM as a whole is more general: it is not restricted to modeling sequenceto-sequence problems, and can infer and exploit correspondence relations between component of an example. To the best of our knowledge, this paper is also the first to make use of token-level alignments in joint neural models of text and images.", "publication_ref": ["b4", "b15", "b1", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Compositionality in representation learning", "text": "While we have focused on compositionality as a property of data distributions or interpretation functions, another line of work in machine learning and language evolution has studied compositionality as an emergent property of learned representations (Andreas, 2019;Resnick et al., 2019;Brighton and Kirby, 2006). In settings where representational compositionality is desirable (e.g. to train communication protocols that can generalize to new states), LEXSYM might provide a tool for promoting it.\nEquivariant Sequence Models As mentioned in Sec. 2, our work builds on existing approaches that control generalization with specialized model architectures designed to be equivariant to permutations of a pre-specified White and Cotterell, 2022). LEXSYM differs from these approaches in three ways. First, LEXSYM is modelagnostic and compatible with pre-training. Second, LEXSYM is compatible with (and automatically derives transformations for) more complicated relations than input-output correspondences, making it possible to apply to tasks like ALCHEMY where such relations are important. Finally, LEXSYM gracefully handles (possibly noisy) learned lexicons, making it applicable to tasks like COGENT with complex or uninterpretable token mappings.\nlexicon (if f (x 1 \u2022 \u2022 \u2022 x n ) = y 1 \u2022 \u2022 \u2022 y m then f (\u03c0(x 1 ) \u2022 \u2022 \u2022 \u03c0(x n )) = \u03c0(y 1 ) \u2022 \u2022 \u2022 \u03c0(y m ) for a per- mutation \u03c0) (Gordon\nData Augmentation Data augmentation approaches are widely used across machine learning application domains featuring known invariances of the data distribution (Japkowicz et al., 2000;Jia and Liang, 2016;Shaw et al., 2021). Substitutionbased schemes that replace words with synonyms, or synchronously replace words and their translations, are widely used for machine translation and general de-biasing (Liu et al., 2021b;Wang et al., 2018;Wei and Zou, 2019).", "publication_ref": ["b2", "b27", "b35", "b28", "b10", "b33", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and Future Directions", "text": "While Sec. 3 characterizes the effect of general Lhomomorphisms, LEXSYM specifically produces single-token swaps. In images represented as discrete symbol sequences, if a single symbol simultaneously encodes multiple visual features (e.g. color and texture), these features will remain entangled in synthesized examples. It will not exchange substructures larger than a single token, and thus will not synthesize examples longer than those already present in the training set (Lake et al., 2019). This is because LEXSYM targets compositionality but not recursion, which is also required to model the full range of human-like generalizations in sequence learning problems.\nLEXSYM is also sensitive to the nature of the tokenization scheme itself. In morphologically rich languages, for example, LEXSYM may need to be applied not on top of words or segments, but instead canonicalized morphemes produced by learned morphological analyzers (Narasimhan et al., 2015;Bergmanis and Goldwater, 2017;Cotterell and Sch\u00fctze, 2018) (analogous to the use of learned image patch representations rather than pixels in our VQA experiments).\nFinally, LEXSYM does not induce some of the generalizations obtained other methods for improv-ing compositional generalization, especially those that exploit extra structure (e.g. tree-shaped inputs and outputs) in the semantic parsing domain (e.g. Liu et al., 2021a). It might serve as a platform for future versions of those methods that offer greater generality and formal guarantees.", "publication_ref": ["b6", "b18", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented LEXSYM, a new data augmentation method that improves compositional generalization of neural models in multiple domains. LEXSYM is derived from a characterization of the principle of compositionality as a constraint on the symmetries of data distributions, and a procedure for automatically identifying these symmetries using token-level alignments. Our results highlight the fact that many inductive biases targeted by specialized models in NLP can be alternatively, and often more flexibly, expressed as a hypothesis about the structure of the distribution to be modeled. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proof of Theorem 2", "text": "Proof. The lexicons that we learn only unary type relations and a semantic correspondence relation L = ({r \u03c4 k }, r \u03f5 ). As noted there, we make the following additional assumptions (satisfied by our lexicon learning algorithms):\n(i) Types are disjoint, i.e. every symbol belongs to a single type:\n\u2200 x \u2208 \u03a3, |\u03c4 x | = |{r \u03c4 k | r \u03c4 k (x) = 1}| = 1.\n(ii) Semantic correspondences are one-to-many from text to meaning. This means that no two text symbols can translate into the same meaning symbol: E i \u2229 E j = 1 x i =x j and all r \u03f5 (x / \u2208 x text , y) = r \u03f5 (y, x / \u2208 x meaning ) = 0.\n(iii) Semantic correspondence is type preserving: all symbols in a correspondence class have the same type \u03c4 e i \u2208E i = {r \u03c4 E i }.\nTo show that f is an L-homomorphism, we want to show that r \u03f5 (f (x 1 ), f (x 2 )) = r \u03f5 (x 1 , x 2 ) for any x 1 , x 2 . The transformation function and all the definitions are symmetric to indices i and j (i \u2212 j symmetry), so it is sufficient to show the correspondence relations stay the same for below cases only:\n(a) x 1 = x i , x 2 = x i : r \u03f5 (f (x i ), f (x i )) = r \u03f5 (x j , x j ) = 0 = r \u03f5 (x i , x i ) (by ii) (b) x 1 = x i , x 2 = x j : r \u03f5 (f (x i ), f (x j )) = r \u03f5 (x j , x i ) = 0 = r \u03f5 (x i , x j ) (by ii) (c) x 1 = x i , x 2 \u2208 E i : r \u03f5 (f (x i ), f (x 2 )) = r \u03f5 (x j , x \u2032 \u2208 E j ) = 1 = r \u03f5 (x i , x 2 ) (by definition of E i and E j ) (d) x 1 = x i , x 2 \u2208 E j : r \u03f5 (f (x i ), f (x 2 )) = r \u03f5 (x j , x \u2032 \u2208 E i ) = 1 x i =x j = r \u03f5 (x i , x 2 ) (by ii) (e) x 1 = x i , x 2 / \u2208 {{x i } \u222a {x j } \u222a E i , E j }: r \u03f5 (f (x i ), f (x 2 )) = r \u03f5 (x j , x 2 ) = 0 = r \u03f5 (x i , x 2 ) (f) x 1 = x i , x 2 / \u2208 {{x i } \u222a {x j } \u222a E i , E j }:\nsame steps as (e)\n(g) x 1 \u2208 E i , x 2 = x i : r \u03f5 (f (x 1 ), f (x i )) = r \u03f5 (x \u2032 \u2208 E j , x j ) = 0 = r \u03f5 (x 1 , x i ) (by ii) (h) x 1 \u2208 E i , x 2 =\nx j : same steps as (g)\n(i) x 1 \u2208 E i , x 2 \u2208 {{x i } \u222a {x j } \u222a E i , E j }: r \u03f5 (f (x 1 ), f (x 2 )) = r \u03f5 (x \u2032 \u2208 E j , x 2 ) = 0 = r \u03f5 (x 1 , x 2 ) (by ii)\nFinally, we require r \u03c4 (x) = r \u03c4 (f (x)) for any x and \u03c4 . Since we assume all items in E i belong to a type matching x i (likewise for j), and types are disjoint, this follows immediately from the definition of f , which only swaps symbols of the same type.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Enumerating L-homomorphisms", "text": "A simple algorithm is given below:\nAlgorithm 1 L-homomorphism enumeration input: Lexicon L = (\u03a3, r1, . . . , rn) for f \u2208 \u03a3 \u03a3 do h \u2190 1 for i = 1..n, xa..x b \u2208 \u03a3 p do if r(xa, . . . , x b ) \u0338 = r(f (xa), . . . , f (x b )) then h \u2190 0 end if end for if h then yield f end if end for C Implementation Details C.1 VQVAE Details\nWe use a discrete variational auto-encoder (van den Oord et al., 2017) to encode the images 16 \u00d7 16 grids of discrete codes. We used a code-book with n = 32 tokens associated with d = 64 dimensional learned latent vectors. The original image size (480, 320) is cropped to (440, 300) and resize our images into (128, 128) pixels. The encoder convolutional neural network has three down-sampling layers which output 16 \u00d7 16 \u00d7 d size hidden representations. For encoder and decoder CNN architectures, we follow the implementation provided in a public Pytorch implementation 5 by adding one more up-sampling and down-sampling layer to adjust our image size.\nWe use exponential moving average to update latent vectors as in official implementation 6 We train the model on the images of the same training data and did not use any external data.\nWe use batch size of 512, and learning rate 0.0003 with the Adam optimizer (Kingma and Ba, 2015). We clip the gradients to 5.0. Hyperparameters were selected by sweeping d over {64, 128}, image sizes over {128, 144}, and n over {24, 32, 48} to maximize the the number of aligned tokens in the lexicon. For each experiments in Table 2, we run VQVAE for 4 random seeds and select the codebook that gives the largest IBM model likelihood for training data. Each experiment takes 10 hours in 4 NVIDIA V100 GPUs. ", "publication_ref": ["b31", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 VQA Transformer Details", "text": "The Transformer takes tokenized images x I and the question x Q and outputs answers as follows:\nc x I = VQVAE enc (x I ) e Q = W Q x Q + 1D positional (x Q ) e x I = W c c x I + 2D positional (c x ) h = Transformer([e Q e x I ]) x A = argmax softmax(W proj h start ) (4)\nWe follow the hyper-paramters provided in (Popel and Bojar, 2018). Transformers have 4 heads, 512dimensional hidden vectors (same with embedding sizes) and 10 layers. We provide the dimensions in Eq. 4:  (Vaswani et al., 2017) with lr = 1.0 and 16k warming steps as provided in Popel and Bojar (2018). We use a batch size of 1024 and we train for 200k steps, which takes 48 hours on 8 NVIDIA V100 GPUs. In Fig. 3, we provide the sketch of overall pipeline.\nx I : 3 \u00d7 128 \u00d7", "publication_ref": ["b22", "b32", "b22"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "C.3 Baselines: LSTM Details", "text": "We use the implementation provided by (Akyurek and Andreas, 2021), increasing the number of training iterations from 8k to 15k for augmented training runs in COGS, SCAN datasets. For the ALCHEMY dataset, we optimize iteration count over {8k, 15k, 25k, 50k} based on validation accuracy, and found 25k to be optimal. For the CLEVR dataset, we optimize itreation count over {8k, 15k, 25k, 50k} for CLEVR and CLEVR-COGENT dataset based on CLEVR's validation accuracy.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "C.4 Baselines: T5 Details", "text": "We use the Huggingface (Wolf et al., 2019) implementation T5-base model. The difference between our T5 baselines results and the results in Qiu et al. (2022) due to their usage of different intermediate representation for the output in order to keep our evaluation consistent with other previous work. We try to optimize (learning rate, learning rate scheduler) and training parameters (iteration count) of Qiu et al. (2022) and (Akyurek and Andreas, 2021), use the best setting for the given dataset.", "publication_ref": ["b36", "b24", "b24", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "C.5 Alignment Model Details", "text": "In our experiments, we use the best alignment method reported in (Akyurek and Andreas, 2021), which is IBM Model 2 for all datasets except the SCAN dataset that uses their proposed algorithm, to obtain our initial alignments A = {(x i , x j ): set of tuples contains aligned tokens. We run alignment algorithms between x text and x meaning . For SCAN and COGS, x text is the actual inputs, x meaning is the actual outputs. In ALCHEMY, x text is instructions, x meaning is beaker states. In VQA experiments, x text question and answer words, x meaning VQVAE codes. We disable diagonalization in FastAlign as it includes non-language structured VQVAE codes.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "D Lexicons", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Lexicon Learning", "text": "Extracting semantic correspondences r \u03f5 (x i , x j ) Given the initial alignments A in Appendix C.5, we remove every x j that is not aligned to at least 1% of occurrences of x i in the dataset. We then produce a one-to-many lexicon by deleting lexicon entries (x i , x j ) and (x \u2032 i , x j ) when both exist. With, these alignment creates entries in r \u03f5 (x i , x j ) = 1 (x i ,x j )\u2208A Extracting Types r \u03c4 (x) Given the partition of the data points (x text , x meaning ), our type finding algorithm is essentially unsupervised clustering of the text symbols in x text . The types of matching x meaning symbols are automatically determined by the correspondence relation, r \u03f5 found above. In all our datasets x text is English, so the symbols that goes into following clustering algorithm are actual words.\nFollowing Clark and Eyraud ( 2007) and Andreas (2020), we assign types to individual words based on their environments. For each symbol, x \u2208 \u03a3, that has at least one equivalent symbol in A, we define the context \u03ba(x) = {(\u03b1, \u03b2) : \u03b1x\u03b2 \u2208 X}: the set of strings (\u03b1, \u03b2) that appear surrounding x in the training set. (If the two examples in Fig. 1 formed the entire training set, we would have \u03ba(yellow) = \u03ba(green) = {(Q: How many, objects? A: 1)}.). 7 We then represent \u03a3 as a graph with an edge between each x i and x j where \u03ba(x i ) \u2229 \u03ba(x j ) \u0338 = \u2205 (Clark and Eyraud's syntactic congruence relation) and x i and x j has same part-of-speech tag according to spaCy pipeline with en-core-web-lm language model 8 . We assign each connected component of this graph a distinct type. This is only one possible approach to typing; alternatives might use clustering of distributed representations.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "D.2 Extracted Lexicons", "text": "In this section, we present lexicon entries for symbols that we learned through our typing algorithm.   B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\nLeft blank.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Did you run computational experiments?", "text": "Left blank.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Left blank.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported by the Machine-LearningApplications initiative at MIT CSAIL, the MIT-IBM Watson AI lab, and the National Science Foundation under grant CCF-2217064. Computing resources were provided by a gift from NVIDIA through the NVAIL program and by the Lincoln Laboratory Supercloud.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We do not anticipate any ethical issues associated with the techniques decribed in this paper. Note that, in CLEVR, we consider the novelty based on (question + answer) string since the generated image codes can be novel but the resulting image not. The following differences are significant under a paired t-test:\nE.1 Statistical Significance Tests for Table 1 The following differences in Table 1 are significant under a paired t-test:\nAlchemy:\n\u2022 T5+LEXSYM > T5 (p < 0.05)\n\u2022 LSTM+LEXSYM > LSTM+Substitute, LSTM, LexLSTM (p < .00001)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "COGS:", "text": "\u2022 T5+LEXSYM > T5 (p < .00001)\n\u2022 LSTM+LEXSYM > LSTM, (p < .00001)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F CLEVR-COGENT Detailed Results", "text": "COGENT results are presented in Table 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Data", "text": "For CLEVR-COGENT (Johnson et al. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning to recombine and resample data for compositional generalization", "journal": "", "year": "2021-05-03", "authors": "Ekin Aky\u00fcrek; Afra Feyza Aky\u00fcrek; Jacob Andreas"}, {"ref_id": "b1", "title": "Lexicon learning for few shot sequence modeling", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Ekin Akyurek; Jacob Andreas"}, {"ref_id": "b2", "title": "Measuring compositionality in representation learning", "journal": "", "year": "2019-05-06", "authors": "Jacob Andreas"}, {"ref_id": "b3", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b4", "title": "Statistical phrase-based translation", "journal": "", "year": "2003", "authors": "Philipp Koehn; Franz J Och; Daniel Marcu"}, {"ref_id": "b5", "title": "Lexical generalization in CCG grammar induction for semantic parsing", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Tom Kwiatkowski; Luke Zettlemoyer; Sharon Goldwater; Mark Steedman"}, {"ref_id": "b6", "title": "Human few-shot learning of compositional instructions", "journal": "", "year": "2019", "authors": "B Lake; M Linzen;  Baroni"}, {"ref_id": "b7", "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks", "journal": "PMLR", "year": "2018-07-10", "authors": "M Brenden; Marco Lake;  Baroni"}, {"ref_id": "b8", "title": "Syntax-directed transduction", "journal": "Journal of the ACM (JACM)", "year": "1968", "authors": "M Philip; Richard Edwin Lewis;  Stearns"}, {"ref_id": "b9", "title": "Learning algebraic recombination for compositional generalization", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Chenyao Liu; Shengnan An; Zeqi Lin; Qian Liu; Bei Chen; Jian-Guang Lou; Lijie Wen; Nanning Zheng; Dongmei Zhang"}, {"ref_id": "b10", "title": "Counterfactual data augmentation for neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Qi Liu; Matt Kusner; Phil Blunsom"}, {"ref_id": "b11", "title": "Simpler context-dependent logical forms via model projections", "journal": "Long Papers", "year": "2016", "authors": "Reginald Long; Panupong Pasupat; Percy Liang"}, {"ref_id": "b12", "title": "Natural logic and natural language inference", "journal": "Springer", "year": "2014", "authors": "Bill Maccartney; D Christopher;  Manning"}, {"ref_id": "b13", "title": "On transfer learning using a mac model variant", "journal": "ArXiv preprint", "year": "2018", "authors": "Vincent Marois; Vincent Jayram; Tomasz Albouy; Younes Kornuta; Ahmet S Bouhadjar;  Ozcan"}, {"ref_id": "b14", "title": "Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "R ; Thomas Mccoy; Robert Frank; Tal Linzen"}, {"ref_id": "b15", "title": "Neural shift-reduce CCG semantic parsing", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Dipendra Kumar Misra; Yoav Artzi"}, {"ref_id": "b16", "title": "English as a formal language. linguaggi nella societae nella tecnica. B. Visentini (red", "journal": "", "year": "1970", "authors": "Richard Montague"}, {"ref_id": "b17", "title": "Universal grammar. Theoria", "journal": "", "year": "1970", "authors": "Richard Montague"}, {"ref_id": "b18", "title": "An unsupervised method for uncovering morphological chains", "journal": "Transactions of the Association for Computational Linguistics", "year": "2015", "authors": "Karthik Narasimhan; Regina Barzilay; Tommi Jaakkola"}, {"ref_id": "b19", "title": "Invariante variationsprobleme. Nachrichten von der Gesellschaft der Wissenschaften zu G\u00f6ttingen", "journal": "Mathematisch-Physikalische Klasse", "year": "1918", "authors": "E Noether"}, {"ref_id": "b20", "title": "Film: Visual reasoning with a general conditioning layer", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Ethan Perez; Florian Strub; Vincent Harm De Vries; Aaron C Dumoulin;  Courville"}, {"ref_id": "b21", "title": "Towards one-shot learning for rare-word translation with external experts", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Ngoc-Quan Pham; Jan Niehues; Alexander Waibel"}, {"ref_id": "b22", "title": "Training tips for the transformer model", "journal": "", "year": "2018", "authors": "Martin Popel; Ond\u0159ej Bojar"}, {"ref_id": "b23", "title": "Making a point: Pointer-generator transformers for disjoint vocabularies", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Nikhil Prabhu; Katharina Kann"}, {"ref_id": "b24", "title": "Improving compositional generalization with latent structure and data augmentation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Linlu Qiu; Peter Shaw; Panupong Pasupat; Pawel Nowak; Tal Linzen; Fei Sha; Kristina Toutanova"}, {"ref_id": "b25", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b26", "title": "Zero-shot text-to-image generation", "journal": "PMLR", "year": "2021-07-24", "authors": "Aditya Ramesh; Mikhail Pavlov; Gabriel Goh; Scott Gray; Chelsea Voss; Alec Radford; Mark Chen; Ilya Sutskever"}, {"ref_id": "b27", "title": "Capacity, bandwidth, and compositionality in emergent language learning", "journal": "", "year": "2019", "authors": "Cinjon Resnick; Abhinav Gupta; Jakob Foerster; M Andrew; Kyunghyun Dai;  Cho"}, {"ref_id": "b28", "title": "Compositional generalization and natural language variation: Can a semantic parsing approach handle both?", "journal": "", "year": "2021", "authors": "Peter Shaw; Ming-Wei Chang; Panupong Pasupat; Kristina Toutanova"}, {"ref_id": "b29", "title": "A survey on image data augmentation for deep learning", "journal": "Journal of Big Data", "year": "2019", "authors": "Connor Shorten; M Taghi;  Khoshgoftaar"}, {"ref_id": "b30", "title": "Se (3)-equivariant relational rearrangement with neural descriptor fields", "journal": "", "year": "2022", "authors": "Anthony Simeonov; Yilun Du; Lin Yen-Chen; Alberto Rodriguez; Leslie Pack Kaelbling; Tomas Lozano-Perez; Pulkit Agrawal"}, {"ref_id": "b31", "title": "Neural discrete representation learning", "journal": "", "year": "2017-12-04", "authors": "A\u00e4ron Van Den Oord; Oriol Vinyals; Koray Kavukcuoglu"}, {"ref_id": "b32", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b33", "title": "SwitchOut: an efficient data augmentation algorithm for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Xinyi Wang; Hieu Pham; Zihang Dai; Graham Neubig"}, {"ref_id": "b34", "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks", "journal": "", "year": "2019", "authors": "Jason Wei; Kai Zou"}, {"ref_id": "b35", "title": "Equivariant transduction through invariant alignment", "journal": "", "year": "2022", "authors": "Jennifer C White; Ryan Cotterell"}, {"ref_id": "b36", "title": "Huggingface's transformers: State-ofthe-art natural language processing", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz"}, {"ref_id": "b37", "title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "Oriol Vinyals", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; Quoc V Le; Mohammad Norouzi; Wolfgang Macherey; Maxim Krikun; Yuan Cao; Qin Gao; Klaus Macherey; Jeff Klingner; Apurva Shah; Melvin Johnson; Xiaobing Liu; \u0141ukasz Kaiser; Stephan Gouws; Yoshikiyo Kato; Taku Kudo; Hideto Kazawa; Keith Stevens; George Kurian; Nishant Patil; Wei Wang"}, {"ref_id": "b38", "title": "Neuralsymbolic VQA: disentangling reasoning from vision and language understanding", "journal": "", "year": "2018-12-03", "authors": "Kexin Yi; Jiajun Wu; Chuang Gan; Antonio Torralba; Pushmeet Kohli; Josh Tenenbaum"}, {"ref_id": "b39", "title": "TreeMix: Compositional constituency-based data augmentation for natural language understanding", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Le Zhang; Zichao Yang; Diyi Yang"}, {"ref_id": "b40", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b41", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b42", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: We extract a lexicon that relates words to their meanings in each dataset. We then find homomorphic transformations (Sec. 3) of this lexicon that, when applied to training examples, produce new, well-formed examples. (Note the changes in the generated examples)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "x N state . Initial and final states are encoded as sequences of color tokens. Predicting final states requires both grounding colors in state variables (brown \u2192 b , red \u2192 g ) and modeling what happens when colors are combined (e.g. mixing g and r yields b ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "relation that encodes the result of mixing colors: 4", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Overview of our approach in VQA. We discretize images using a VQVAE (van den Oord et al., 2017) learned from the training data. This discretization represents every image as a sequence of categorical codes. (a) We run a statistical aligner on (x text , x img ) pairs to find word-visual token alignments within individual examples, then use these alignments to construct a global lexicon. (b) Each entry in the lexicon is assigned a type based on the context in which it occurs. (c) Next, we find homomorphisms of this lexicon, and use these as data augmentation functions to generate new training examples. (d) Finally, we train a neural sequence model on the augmented dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Model ALCHEMY SCAN (jump) SCAN (around right) COGS COGS (nonce) Previous Work on COGS & SCAN GECA (Andreas, 2020) -99.94 \u00b10.10 98.50 \u00b11.90 47.74 \u00b14.52 -LeAR (Liu et al., 2021a) ---97.70 \u00b10.70 -LexLSTM (Akyurek and Andreas, 2021) 36.80 \u00b11.96 99.14 \u00b11.55 88.41 \u00b17.35 82.17 \u00b10.72 81.40 \u00b10.40 No Pre-training LSTM 41.72 \u00b11.15 000.41 \u00b10.34 08.65 \u00b14.52 61.13 \u00b14.12 61.13 \u00b14.12 + Substitute (e.g. Liu et al., 2021b) 40.52 \u00b10.84 099.95 \u00b10.10 99.17 \u00b10.93 81.99 \u00b10.50 77.62 \u00b10.78 + LEXSYM 45.85 \u00b12.00 100.00 \u00b10 99.51 \u00b10.48 81.86 \u00b10.90 77.25 \u00b10.34 Language Pre-training T5 84.95 \u00b10.44 93.60 \u00b10 38.40 \u00b10.90 83.30 \u00b10.10 64.20 \u00b12.00 \u00b10.16 99.96 \u00b10.03 97.29 \u00b12.16 83.62 \u00b10.27 76.74 \u00b12.23", "figure_data": "+CSL-Aug* (Qiu et al., 2022) +LEXSYM-85.4899.70 \u00b10-99.50 \u00b10-"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results on semantic parsing and instruction following. We provide mean and standard deviations over 5 random seeds. LEXSYM improves significantly over baselines, with and without large-scale pretraining.", "figure_data": "COGENT CLEVRVisual Pre-training Human (Johnson et al., 2017)-92.6Film (Perez et al., 2018)78.897.7S-MAC (Marois et al., 2018)78.798.9NSVQA (Yi et al., 2018)63.999.7Seq2Seq Baselines T579.7-LexLSTM62.1-No Pre-Praining VQATransformer + Substitute (e.g. Liu et al., 2021b) 84.4 \u00b10.7 73.3 \u00b11.0 + LexSym 85.9 \u00b10.993.6 \u00b10.5 90.8 \u00b10.3 92.0 \u00b10.9"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Kim and Linzen, 2020, Fig. 1b) semantic parsing datasets. SCAN consists of simple instruction following tasks in which strings are translated into sequences of actions. We focus on the jump split, which measures models' ability to compose words that only appeared in isolation during training, and the around right split, which measures generalization to novel collocations. The COGS dataset tests", "figure_data": "Data We study two standard compositional gen-eralization benchmarks: the SCAN (Lake andBaroni, 2018) instruction following and COGS("}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Questions involve complex computational operations including quantification, comparison, and spatial reasoning. CLEVR has been a popular testbed for evaluating composition in visual question answering models. Our main experiment uses the COGENT split of the dataset, which focuses", "figure_data": "on compositional generalization. In the CLEVR-COGENT training set (Split A), which containsroughly 700K (question, image, answer) triples,all cubes are gray, blue, brown or yellow, while allcylinders are red, green, purple or cyan. In the testset (validation set of Split B), these are reversed.LEXSYM In VQA and other multi-modal tasks, part of the input is continuous (e.g. images andvideos). Recent work has shown that it is possi-ble to learn high-quality discrete representationsof continuous input data. For example, in the VQ-VAE model of van den Oord et al. (2017), a con-tinuous image is transformed into a grid of cate-gorical codes, with individual codes representingcolor, and in some cases materials and illumination(examples in Table"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "JacobAndreas. 2020. Good-enough compositional  data augmentation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556-7566, Online. Association for Computational Linguistics. Taco Cohen and Max Welling. 2016. Group equivariant convolutional networks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 2990-2999. JMLR.org.", "figure_data": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 39-48. IEEE Computer Society.Ryan Cotterell and Hinrich Sch\u00fctze. 2018. Joint se-mantic synthesis and morphological analysis of the derived word. Transactions of the Association for Computational Linguistics, 6:33-48.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. 2020. Permutation equivariant models for compositional generalization in language. In 8th International Conference on Learning Repre-sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. 2022. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Na-ture communications, 13(1):1-11. Toms Bergmanis and Sharon Goldwater. 2017. From segmentation to analyses: a probabilistic model for unsupervised morphology induction. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Vol-ume 1, Long Papers, pages 337-346, Valencia, Spain. Association for Computational Linguistics.Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780. Nathalie Japkowicz et al. 2000. Learning from imbal-anced data sets: a comparison of various strategies. In AAAI workshop on learning from imbalanced data sets, volume 68, pages 10-15. Robin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages 12-22, Berlin, Germany. Association for Computa-tional Linguistics.Henry Brighton and Simon Kirby. 2006. Understanding linguistic evolution by visualizing the emergence of topographic mappings. Artificial life, 12(2):229-242. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The math-ematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311. Shuxiao Chen, Edgar Dobriban, and Jane H. Lee. 2020. A group-theoretic framework for data augmentation. In Advances in Neural Information Processing Sys-tems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, and Michael Subotin. 2005. The Hiero machine translation system: Extensions, evaluation, and analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Process-ing, pages 779-786, Vancouver, British Columbia, Canada. Association for Computational Linguistics.Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual rea-soning. In 2017 IEEE Conference on Computer Vi-sion and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1988-1997. IEEE Computer Society. Bevan Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with Bayesian tree transduc-ers. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 488-496, Jeju Island, Korea. Association for Computational Linguistics. Chlo\u00e9 Kiddon and Pedro Domingos. 2015. Symmetry-based semantic parsing. In Proceedings of the 2014 Workshop on Learning Semantics. Najoung Kim and Tal Linzen. 2020. COGS: A compo-sitional generalization challenge based on semantic interpretation. In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural Language Processing (EMNLP), pages 9087-9105, Online. As-sociation for Computational Linguistics.Alexander Clark and R\u00e9mi Eyraud. 2007. Polynomial identification in the limit of substitutable context-free languages. Journal of Machine Learning Research, 8(8).Najoung Kim, Tal Linzen, and Paul Smolensky. 2022. Uncontrolled lexical exposure leads to overestima-tion of compositional generalization in pretrained models. ArXiv preprint, abs/2212.10769."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "128 c x I : 32 \u00d7 16 \u00d7 16 W c : 512 \u00d7 32 e x I : 512 \u00d7 (16 \u00d7 16) e Q : 512 \u00d7 |V text | W Q : 512 \u00d7 |V text | h : 512 \u00d7 (|Q| + 16 \u00d7 16) h start : 512 \u00d7 1 W proj : 512 \u00d7 |V text |", "figure_data": "(5)Models are trained using the Adam optimizer withand Noam learning rate scheduler"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "SCAN We present equivalance relations that we extracted from SCAN training dataset. A cake was baked by Scarlett . cake(x1) AND bake.theme(x3, x1) AND bake.agent (x3, Scarlett ) A cake was stabbed by Scarlett . cake(x1) AND stab.theme (x3, x1) AND stab.agent (x3, Scarlett ) The bunny needed to cook . *bunny(x1); need.agent(x2, x1) AND need.xcomp (x2, x4) AND cook.agent(x4, x1) The girl needed to cook . *girl (x1); need.agent (x2, x1) AND need.xcomp(x2, x4) AND cook.agent (x4, x1) The bun hunted Emma . *bun(x1); hunt.agent(x2, x1) AND hunt.theme (x2, Emma) The teacher hunted Emma . *teacher(x1); hunt.agent(x2, x1) AND hunt.theme(x2, Emma)", "figure_data": "Generated SentenceGenerated Logical formOriginal SentenceOriginal Example Logical FormGenerated TextGenerated ImageOriginal TextOriginal ImageHow many metallic objects areHow many metallic objects areeither tiny yellow things or blocks?either tiny red things or blocks?A: 1A: 1What is the size of the other object that isWhat is the size of the other object that isthe same material as the big brown thingthe same material as the big purple thing?A: LargeA: Large"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Generated samples for CLEVR-COGENT and COGS datasets. In CLEVR-COGENT, our method operate on displayed VQVAE symbols on top of the images and we can decode it to actual images as displayed here. The generated yellow cylinder in the first row is an unseen color+shape combination. \u00b11.0 71.0 \u00b11.6 85.7 \u00b10.74 83.5 \u00b10.1 64.4 \u00b10.7 81.4 \u00b11.2 + Substitute (e.g. Liu et al., 2021b) 84.4 \u00b10.7 76.7 \u00b11.1 89.5 \u00b10.3 88.8 \u00b10.3 85.1 \u00b11.0 88.0 \u00b10.6 + LexSym 85.9 \u00b10.9 80.1 \u00b10.9 91.1 \u00b10.5 91.0 \u00b10.7 85.2 \u00b11.3 88.9 \u00b10.7", "figure_data": "VQATransformer (No Pre-Praining) Baseline73.3CLEVR-COGENT"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Breakdown of CLEVR-COGENT Results A1. Did you describe the limitations of your work? A3. Do the abstract and introduction summarize the paper's main claims? Left blank. A4. Have you used AI writing assistants when working on this paper?", "figure_data": "DatasetTrainValidation TestALCHEMY1828512254495SCAN(jump)14670-7706(around right) 15225-4476COGS(original)24155300021000(nonce)24155300021000CLEVR(original)699989 149991(CoGenT)699960 -150000655"}], "formulas": [{"formula_id": "formula_0", "formula_text": "{f (x) : x \u2208 X} = X (1)", "formula_coordinates": [2.0, 367.33, 575.04, 157.1, 20.55]}, {"formula_id": "formula_1", "formula_text": "Definition 4. X is L-compositional if I(x) = C(L(x)", "formula_coordinates": [4.0, 70.86, 542.16, 218.28, 34.5]}, {"formula_id": "formula_2", "formula_text": "Definition 5. A function f is a homomorphism of (\u03a3 \u03a3 \u03a3, L L L) (an \"L-homomorphism\") if: \u2200r \u2208 L, \u2200x 1 . . . x p \u2208 \u03a3 : r(x 1 , . . . , x p ) = r(f (x 1 ), . . . , f (x p )) (2)", "formula_coordinates": [4.0, 306.14, 394.65, 218.28, 64.85]}, {"formula_id": "formula_3", "formula_text": "Theorem 1. If X is L-compositional, f is an L-homomorphism, and x \u2208 X, then f (x) = [f (x 1 ), . . . , f (x n )] \u2208 X. Thus every homomor- phism of L well-formed examples \u2208 X. Proof. From Definition 3 and 5, R i (f (x)) = R i (x) \u2200i. Then, 1 [f (x)\u2208X] = I(f (x)) = C(L(f (x))) = C(R 1 (f (x)), . . . , R n (f (x))) = C(R 1 (x), . . . , R n (x)) = I(x) = 1 [x\u2208X]", "formula_coordinates": [4.0, 305.78, 552.77, 220.45, 183.31]}, {"formula_id": "formula_4", "formula_text": "f (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 x j if x = x i x i if x = x j x \u2032 \u2208 E j if x \u2208 E i x \u2032 \u2208 E i if x \u2208 E j x otherwise (3)", "formula_coordinates": [6.0, 112.59, 90.47, 176.55, 86.41]}, {"formula_id": "formula_5", "formula_text": "r \u03f5 (c 1 , c 2 ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 c 1 = brown, c 2 = b 1 c 1 = red, c 2 = r 1 c 1 = green, c 2 = g . . .", "formula_coordinates": [6.0, 322.38, 143.35, 180.65, 86.43]}, {"formula_id": "formula_6", "formula_text": "r mix (c 1 , c 2 , c 3 ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 c 1 = c 2 = c 3 1 c 1 \u0338 = c 2 \u2227 c 3 = b 0 otherwise Together, (r \u03f5 , r mix , {r \u03c4 k }),", "formula_coordinates": [6.0, 305.81, 284.55, 193.2, 91.27]}, {"formula_id": "formula_7", "formula_text": "lexicon (if f (x 1 \u2022 \u2022 \u2022 x n ) = y 1 \u2022 \u2022 \u2022 y m then f (\u03c0(x 1 ) \u2022 \u2022 \u2022 \u03c0(x n )) = \u03c0(y 1 ) \u2022 \u2022 \u2022 \u03c0(y m ) for a per- mutation \u03c0) (Gordon", "formula_coordinates": [8.0, 306.14, 760.96, 218.28, 20.55]}, {"formula_id": "formula_8", "formula_text": "\u2200 x \u2208 \u03a3, |\u03c4 x | = |{r \u03c4 k | r \u03c4 k (x) = 1}| = 1.", "formula_coordinates": [13.0, 92.68, 186.21, 196.46, 26.54]}, {"formula_id": "formula_9", "formula_text": "(a) x 1 = x i , x 2 = x i : r \u03f5 (f (x i ), f (x i )) = r \u03f5 (x j , x j ) = 0 = r \u03f5 (x i , x i ) (by ii) (b) x 1 = x i , x 2 = x j : r \u03f5 (f (x i ), f (x j )) = r \u03f5 (x j , x i ) = 0 = r \u03f5 (x i , x j ) (by ii) (c) x 1 = x i , x 2 \u2208 E i : r \u03f5 (f (x i ), f (x 2 )) = r \u03f5 (x j , x \u2032 \u2208 E j ) = 1 = r \u03f5 (x i , x 2 ) (by definition of E i and E j ) (d) x 1 = x i , x 2 \u2208 E j : r \u03f5 (f (x i ), f (x 2 )) = r \u03f5 (x j , x \u2032 \u2208 E i ) = 1 x i =x j = r \u03f5 (x i , x 2 ) (by ii) (e) x 1 = x i , x 2 / \u2208 {{x i } \u222a {x j } \u222a E i , E j }: r \u03f5 (f (x i ), f (x 2 )) = r \u03f5 (x j , x 2 ) = 0 = r \u03f5 (x i , x 2 ) (f) x 1 = x i , x 2 / \u2208 {{x i } \u222a {x j } \u222a E i , E j }:", "formula_coordinates": [13.0, 74.97, 71.73, 426.25, 702.26]}, {"formula_id": "formula_10", "formula_text": "(g) x 1 \u2208 E i , x 2 = x i : r \u03f5 (f (x 1 ), f (x i )) = r \u03f5 (x \u2032 \u2208 E j , x j ) = 0 = r \u03f5 (x 1 , x i ) (by ii) (h) x 1 \u2208 E i , x 2 =", "formula_coordinates": [13.0, 310.26, 182.21, 193.38, 107.62]}, {"formula_id": "formula_11", "formula_text": "(i) x 1 \u2208 E i , x 2 \u2208 {{x i } \u222a {x j } \u222a E i , E j }: r \u03f5 (f (x 1 ), f (x 2 )) = r \u03f5 (x \u2032 \u2208 E j , x 2 ) = 0 = r \u03f5 (x 1 , x 2 ) (by ii)", "formula_coordinates": [13.0, 312.68, 291.64, 191.81, 77.72]}, {"formula_id": "formula_12", "formula_text": "Algorithm 1 L-homomorphism enumeration input: Lexicon L = (\u03a3, r1, . . . , rn) for f \u2208 \u03a3 \u03a3 do h \u2190 1 for i = 1..n, xa..x b \u2208 \u03a3 p do if r(xa, . . . , x b ) \u0338 = r(f (xa), . . . , f (x b )) then h \u2190 0 end if end for if h then yield f end if end for C Implementation Details C.1 VQVAE Details", "formula_coordinates": [13.0, 305.75, 518.54, 204.25, 198.09]}, {"formula_id": "formula_13", "formula_text": "c x I = VQVAE enc (x I ) e Q = W Q x Q + 1D positional (x Q ) e x I = W c c x I + 2D positional (c x ) h = Transformer([e Q e x I ]) x A = argmax softmax(W proj h start ) (4)", "formula_coordinates": [14.0, 337.98, 383.93, 186.44, 78.22]}, {"formula_id": "formula_14", "formula_text": "x I : 3 \u00d7 128 \u00d7", "formula_coordinates": [14.0, 360.47, 552.18, 66.57, 18.93]}], "doi": "10.18653/v1/2021.acl-long.382"}