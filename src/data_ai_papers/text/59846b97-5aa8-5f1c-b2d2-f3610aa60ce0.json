{"title": "Online Dictionary Learning for Sparse Coding", "authors": "Julien Mairal; Francis Bach; Guillermo Sapiro", "pub_date": "", "abstract": "Sparse coding-that is, modelling data vectors as sparse linear combinations of basis elements-is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.", "sections": [{"heading": "Introduction", "text": "The linear decomposition of a signal using a few atoms of a learned dictionary instead of a predefined one-based on wavelets (Mallat, 1999) for example-has recently led to state-of-the-art results for numerous low-level image processing tasks such as denoising  as well as higher-level tasks such as classification (Raina et al., 2007;Mairal et al., 2009), showing that sparse learned models are well adapted to natural signals. Un-Appearing in Proceedings of the 26 th International Conference on Machine Learning, Montreal, Canada, 2009. Copyright 2009 by the author(s)/owner(s). like decompositions based on principal component analysis and its variants, these models do not impose that the basis vectors be orthogonal, allowing more flexibility to adapt the representation to the data. While learning the dictionary has proven to be critical to achieve (or improve upon) state-of-the-art results, effectively solving the corresponding optimization problem is a significant computational challenge, particularly in the context of the largescale datasets involved in image processing tasks, that may include millions of training samples. Addressing this challenge is the topic of this paper.\nConcretely, consider a signal x in R m . We say that it admits a sparse approximation over a dictionary D in R m\u00d7k , with k columns referred to as atoms, when one can find a linear combination of a \"few\" atoms from D that is \"close\" to the signal x. Experiments have shown that modelling a signal with such a sparse decomposition (sparse coding) is very effective in many signal processing applications (Chen et al., 1999). For natural images, predefined dictionaries based on various types of wavelets (Mallat, 1999) have been used for this task. However, learning the dictionary instead of using off-the-shelf bases has been shown to dramatically improve signal reconstruction . Although some of the learned dictionary elements may sometimes \"look like\" wavelets (or Gabor filters), they are tuned to the input images or signals, leading to much better results in practice.\nMost recent algorithms for dictionary learning (Olshausen & Field, 1997;Aharon et al., 2006;Lee et al., 2007) are second-order iterative batch procedures, accessing the whole training set at each iteration in order to minimize a cost function under some constraints. Although they have shown experimentally to be much faster than first-order gradient descent methods (Lee et al., 2007), they cannot effectively handle very large training sets (Bottou & Bousquet, 2008), or dynamic training data changing over time, such as video sequences. To address these issues, we propose an online approach that processes one element (or a small subset) of the training set at a time. This is particularly important in the context of image and video processing (Protter & Elad, 2009), where it is common to learn dictionaries adapted to small patches, with training data that may include several millions of these patches (roughly one per pixel and per frame). In this setting, online techniques based on stochastic approximations are an attractive alternative to batch methods (Bottou, 1998). For example, first-order stochastic gradient descent with projections on the constraint set is sometimes used for dictionary learning (see Aharon and Elad (2008) for instance). We show in this paper that it is possible to go further and exploit the specific structure of sparse coding in the design of an optimization procedure dedicated to the problem of dictionary learning, with low memory consumption and lower computational cost than classical second-order batch algorithms and without the need of explicit learning rate tuning. As demonstrated by our experiments, the algorithm scales up gracefully to large datasets with millions of training samples, and it is usually faster than more standard methods.", "publication_ref": ["b18", "b22", "b17", "b8", "b18", "b19", "b1", "b15", "b15", "b7", "b21", "b6", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Contributions", "text": "This paper makes three main contributions.\n\u2022 We cast in Section 2 the dictionary learning problem as the optimization of a smooth nonconvex objective function over a convex set, minimizing the (desired) expected cost when the training set size goes to infinity.\n\u2022 We propose in Section 3 an iterative online algorithm that solves this problem by efficiently minimizing at each step a quadratic surrogate function of the empirical cost over the set of constraints. This method is shown in Section 4 to converge with probability one to a stationary point of the cost function.\n\u2022 As shown experimentally in Section 5, our algorithm is significantly faster than previous approaches to dictionary learning on both small and large datasets of natural images. To demonstrate that it is adapted to difficult, largescale image-processing tasks, we learn a dictionary on a 12-Megapixel photograph and use it for inpainting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Statement", "text": "Classical dictionary learning techniques (Olshausen & Field, 1997;Aharon et al., 2006;Lee et al., 2007) consider a finite training set of signals X = [x 1 , . . . , x n ] in R m\u00d7n and optimize the empirical cost function\nf n (D) \u25b3 = 1 n n i=1 l(x i , D),(1)\nwhere D in R m\u00d7k is the dictionary, each column representing a basis vector, and l is a loss function such that l(x, D) should be small if D is \"good\" at representing the signal x.\nThe number of samples n is usually large, whereas the signal dimension m is relatively small, for example, m = 100 for 10 \u00d7 10 image patches, and n \u2265 100, 000 for typical image processing applications. In general, we also have k \u226a n (e.g., k = 200 for n = 100, 000), and each signal only uses a few elements of D in its representation. Note that, in this setting, overcomplete dictionaries with k > m are allowed. As others (see (Lee et al., 2007) for example), we define l(x, D) as the optimal value of the \u2113 1 -sparse coding problem:\nl(x, D) \u25b3 = min \u03b1\u2208R k 1 2 ||x \u2212 D\u03b1|| 2 2 + \u03bb||\u03b1|| 1 ,(2)\nwhere \u03bb is a regularization parameter. 2 This problem is also known as basis pursuit (Chen et al., 1999), or the Lasso (Tibshirani, 1996). It is well known that the \u2113 1 penalty yields a sparse solution for \u03b1, but there is no analytic link between the value of \u03bb and the corresponding effective sparsity ||\u03b1|| 0 . To prevent D from being arbitrarily large (which would lead to arbitrarily small values of \u03b1), it is common to constrain its columns (d j ) k j=1 to have an \u2113 2 norm less than or equal to one. We will call C the convex set of matrices verifying this constraint:\nC \u25b3 = {D \u2208 R m\u00d7k s.t. \u2200j = 1, . . . , k, d T j d j \u2264 1}. (3)\nNote that the problem of minimizing the empirical cost f n (D) is not convex with respect to D. It can be rewritten as a joint optimization problem with respect to the dictionary D and the coefficients \u03b1 = [\u03b1 1 , . . . , \u03b1 n ] of the sparse decomposition, which is not jointly convex, but convex with respect to each of the two variables D and \u03b1 when the other one is fixed:\nmin D\u2208C,\u03b1\u2208R k\u00d7n 1 n n i=1 1 2 ||x i \u2212 D\u03b1 i || 2 2 + \u03bb||\u03b1 i || 1 . (4)\nA natural approach to solving this problem is to alternate between the two variables, minimizing over one while keeping the other one fixed, as proposed by Lee et al. (2007) (see also Aharon et al. (2006), who use \u2113 0 rather than \u2113 1 penalties, for related approaches). 3 Since the computation of \u03b1 dominates the cost of each iteration, a second-order optimization technique can be used in this case to accurately estimate D at each step when \u03b1 is fixed.\nAs pointed out by Bottou and Bousquet (2008), however, one is usually not interested in a perfect minimization of 2 The \u2113p norm of a vector x in R m is defined, for p \u2265 1, by\n||x||p \u25b3 = ( P m i=1 |x[i]| p ) 1/p .\nFollowing tradition, we denote by ||x||0 the number of nonzero elements of the vector x. This \"\u21130\" sparsity measure is not a true norm.\n3 In our setting, as in (Lee et al., 2007), we use the convex \u21131 norm, that has empirically proven to be better behaved in general than the \u21130 pseudo-norm for dictionary learning.\nthe empirical cost f n (D), but in the minimization of the expected cost\nf (D) \u25b3 = E x [l(x, D)] = lim n\u2192\u221e f n (D) a.s.,(5)\nwhere the expectation (which is assumed finite) is taken relative to the (unknown) probability distribution p(x) of the data. 4 In particular, given a finite training set, one should not spend too much effort on accurately minimizing the empirical cost, since it is only an approximation of the expected cost. Bottou and Bousquet (2008) have further shown both theoretically and experimentally that stochastic gradient algorithms, whose rate of convergence is not good in conventional optimization terms, may in fact in certain settings be the fastest in reaching a solution with low expected cost.\nWith large training sets, classical batch optimization techniques may indeed become impractical in terms of speed or memory requirements.\nIn the case of dictionary learning, classical projected firstorder stochastic gradient descent (as used by Aharon and Elad (2008) for instance) consists of a sequence of updates of D:\nD t = \u03a0 C D t\u22121 \u2212 \u03c1 t \u2207 D l(x t , D t\u22121 ) ,(6)\nwhere \u03c1 is the gradient step, \u03a0 C is the orthogonal projector on C, and the training set x 1 , x 2 , . . . are i.i.d. samples of the (unknown) distribution p(x). As shown in Section 5, we have observed that this method can be competitive compared to batch methods with large training sets, when a good learning rate \u03c1 is selected.\nThe dictionary learning method we present in the next section falls into the class of online algorithms based on stochastic approximations, processing one sample at a time, but exploits the specific structure of the problem to efficiently solve it. Contrary to classical first-order stochastic gradient descent, it does not require explicit learning rate tuning and minimizes a sequentially quadratic local approximations of the expected cost.", "publication_ref": ["b19", "b1", "b15", "b15", "b8", "b23", "b15", "b1", "b7", "b15", "b7", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Online Dictionary Learning", "text": "We present in this section the basic components of our online algorithm for dictionary learning (Sections 3.1-3.3), as well as two minor variants which speed up our implementation (Section 3.4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm Outline", "text": "Our algorithm is summarized in Algorithm 1. Assuming the training set composed of i.i.d. samples of a distribu-Algorithm 1 Online dictionary learning.\nRequire: x \u2208 R m \u223c p(x) (random variable and an algorithm to draw i.i.d samples of p), \u03bb \u2208 R (regularization parameter), D 0 \u2208 R m\u00d7k (initial dictionary), T (number of iterations). 1: A 0 \u2190 0, B 0 \u2190 0 (reset the \"past\" information). 2: for t = 1 to T do 3: Draw x t from p(x).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "Sparse coding: compute using LARS\n\u03b1 t \u25b3 = arg min \u03b1\u2208R k 1 2 ||x t \u2212 D t\u22121 \u03b1|| 2 2 + \u03bb||\u03b1|| 1 . (8) 5: A t \u2190 A t\u22121 + \u03b1 t \u03b1 T t . 6: B t \u2190 B t\u22121 + x t \u03b1 T t .\n7:\nCompute D t using Algorithm 2, with D t\u22121 as warm restart, so that\nD t \u25b3 = arg min D\u2208C 1 t t i=1 1 2 ||x i \u2212 D\u03b1 i || 2 2 + \u03bb||\u03b1 i || 1 , = arg min D\u2208C 1 t 1 2 Tr(D T DA t ) \u2212 Tr(D T B t ) .(9)\n8: end for 9: Return D T (learned dictionary).\ntion p(x), its inner loop draws one element x t at a time, as in stochastic gradient descent, and alternates classical sparse coding steps for computing the decomposition \u03b1 t of x t over the dictionary D t\u22121 obtained at the previous iteration, with dictionary update steps where the new dictionary D t is computed by minimizing over C the function\nf t (D) \u25b3 = 1 t t i=1 1 2 ||x i \u2212 D\u03b1 i || 2 2 + \u03bb||\u03b1 i || 1 ,(7)\nwhere the vectors \u03b1 i are computed during the previous steps of the algorithm. The motivation behind our approach is twofold:\n\u2022 The quadratic functionf t aggregates the past information computed during the previous steps of the algorithm, namely the vectors \u03b1 i , and it is easy to show that it upperbounds the empirical cost f t (D t ) from Eq. (1). One key aspect of the convergence analysis will be to show that f t (D t ) and f t (D t ) converges almost surely to the same limit and thusf t acts as a surrogate for f t .\n\u2022 Sincef t is close tof t\u22121 , D t can be obtained efficiently using D t\u22121 as warm restart.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sparse Coding", "text": "The sparse coding problem of Eq. (2) with fixed dictionary is an \u2113 1 -regularized linear least-squares problem. A Algorithm 2 Dictionary Update.\nRequire: D = [d 1 , . . . , d k ] \u2208 R m\u00d7k (input dictionary), A = [a 1 , . . . , a k ] \u2208 R k\u00d7k = t i=1 \u03b1 i \u03b1 T i , B = [b 1 , . . . , b k ] \u2208 R m\u00d7k = t i=1 x i \u03b1 T i . 1: repeat 2: for j = 1 to k do 3:\nUpdate the j-th column to optimize for (9):\nu j \u2190 1 A jj (b j \u2212 Da j ) + d j . d j \u2190 1 max(||u j || 2 , 1) u j .(10) 4:\nend for 5: until convergence 6: Return D (updated dictionary). number of recent methods for solving this type of problems are based on coordinate descent with soft thresholding (Fu, 1998;Friedman et al., 2007). When the columns of the dictionary have low correlation, these simple methods have proven to be very efficient. However, the columns of learned dictionaries are in general highly correlated, and we have empirically observed that a Cholesky-based implementation of the LARS-Lasso algorithm, an homotopy method (Osborne et al., 2000;Efron et al., 2004) that provides the whole regularization path-that is, the solutions for all possible values of \u03bb, can be as fast as approaches based on soft thresholding, while providing the solution with a higher accuracy.", "publication_ref": ["b13", "b12", "b20", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Dictionary Update", "text": "Our algorithm for updating the dictionary uses blockcoordinate descent with warm restarts, and one of its main advantages is that it is parameter-free and does not require any learning rate tuning, which can be difficult in a constrained optimization setting. Concretely, Algorithm 2 sequentially updates each column of D. Using some simple algebra, it is easy to show that Eq. (10) gives the solution of the dictionary update (9) with respect to the j-th column d j , while keeping the other ones fixed under the constraint d T j d j \u2264 1. Since this convex optimization problem admits separable constraints in the updated blocks (columns), convergence to a global optimum is guaranteed (Bertsekas, 1999). In practice, since the vectors \u03b1 i are sparse, the coefficients of the matrix A are in general concentrated on the diagonal, which makes the block-coordinate descent more efficient. 5 Since our algorithm uses the value of D t\u22121 as a warm restart for computing D t , a single iteration has empirically been found to be enough. Other approaches have been proposed to update D, for instance, Lee et al. (2007) suggest using a Newton method on the dual of Eq. ( 9), but this requires inverting a k \u00d7 k matrix at each Newton iteration, which is impractical for an online algorithm.", "publication_ref": ["b2", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Optimizing the Algorithm", "text": "We have presented so far the basic building blocks of our algorithm. This section discusses simple improvements that significantly enhance its performance.\nHandling Fixed-Size Datasets. In practice, although it may be very large, the size of the training set is often finite (of course this may not be the case, when the data consists of a video stream that must be treated on the fly for example). In this situation, the same data points may be examined several times, and it is very common in online algorithms to simulate an i.i.d. sampling of p(x) by cycling over a randomly permuted training set (Bottou & Bousquet, 2008). This method works experimentally well in our setting but, when the training set is small enough, it is possible to further speed up convergence: In Algorithm 1, the matrices A t and B t carry all the information from the past coefficients \u03b1 1 , . . . , \u03b1 t . Suppose that at time t 0 , a signal x is drawn and the vector \u03b1 t0 is computed. If the same signal x is drawn again at time t > t 0 , one would like to remove the \"old\" information concerning x from A t and B t -that is, write A t \u2190 A t\u22121 + \u03b1 t \u03b1 T t \u2212 \u03b1 t0 \u03b1 T t0 for instance. When dealing with large training sets, it is impossible to store all the past coefficients \u03b1 t0 , but it is still possible to partially exploit the same idea, by carrying in A t and B t the information from the current and previous epochs (cycles through the data) only.\nMini-Batch Extension. In practice, we can improve the convergence speed of our algorithm by drawing \u03b7 > 1 signals at each iteration instead of a single one, which is a classical heuristic in stochastic gradient descent algorithms. Let us denote x t,1 , . . . , x t,\u03b7 the signals drawn at iteration t. We can then replace the lines 5 and 6 of Algorithm 1 by\nA t \u2190 \u03b2A t\u22121 + \u03b7 i=1 \u03b1 t,i \u03b1 T t,i , B t \u2190 \u03b2B t\u22121 + \u03b7 i=1 x\u03b1 T t,i ,(11)\nwhere \u03b2 is chosen so that \u03b2 = \u03b8+1\u2212\u03b7 \u03b8+1 , where \u03b8 = t\u03b7 if t < \u03b7 and \u03b7 2 + t \u2212 \u03b7 if t \u2265 \u03b7, which is compatible with our convergence analysis.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Purging the Dictionary from Unused Atoms. Every dictionary learning technique sometimes encounters situations", "text": "where some of the dictionary atoms are never (or very seldom) used, which happens typically with a very bad intialization. A common practice is to replace them during the optimization by elements of the training set, which solves in practice this problem in most cases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Convergence Analysis", "text": "Although our algorithm is relatively simple, its stochastic nature and the non-convexity of the objective function make the proof of its convergence to a stationary point somewhat involved. The main tools used in our proofs are the convergence of empirical processes (Van der Vaart, 1998) and, following Bottou (1998), the convergence of quasi-martingales (Fisk, 1965). Our analysis is limited to the basic version of the algorithm, although it can in principle be carried over to the optimized version discussed in Section 3.4. Because of space limitations, we will restrict ourselves to the presentation of our main results and a sketch of their proofs, which will be presented in details elsewhere, and first the (reasonable) assumptions under which our analysis holds.", "publication_ref": ["b6", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Assumptions (A)", "text": "The data admits a bounded probability density p with compact support K. Assuming a compact support for the data is natural in audio, image, and video processing applications, where it is imposed by the data acquisition process. (B) The quadratic surrogate functionsf t are strictly convex with lower-bounded Hessians. We assume that the smallest eigenvalue of the semi-definite positive matrix 1 t A t defined in Algorithm 1 is greater than or equal to a non-zero constant \u03ba 1 (making A t invertible andf t strictly convex with Hessian lower-bounded). This hypothesis is in practice verified experimentally after a few iterations of the algorithm when the initial dictionary is reasonable, consisting for example of a few elements from the training set, or any one of the \"off-the-shelf\" dictionaries, such as DCT (bases of cosines products) or wavelets. Note that it is easy to enforce this assumption by adding a term \u03ba1 2 ||D|| 2 F to the objective function, which is equivalent in practice to replacing the positive semi-definite matrix 1 t A t by 1 t A t + \u03ba 1 I. We have omitted for simplicity this penalization in our analysis. (C) A sufficient uniqueness condition of the sparse coding solution is verified: Given some x \u2208 K, where K is the support of p, and D \u2208 C, let us denote by \u039b the set of indices j such that |d T j (x \u2212 D\u03b1 \u22c6 )| = \u03bb, where \u03b1 \u22c6 is the solution of Eq. (2). We assume that there exists \u03ba 2 > 0 such that, for all x in K and all dictionaries D in the subset S of C considered by our algorithm, the smallest eigenvalue of D T \u039b D \u039b is greater than or equal to \u03ba 2 . This matrix is thus invertible and classical results (Fuchs, 2005) ensure the uniqueness of the sparse coding solution. It is of course easy to build a dictionary D for which this assumption fails. However, having D T \u039b D \u039b invertible is a common assumption in linear regression and in methods such as the LARS algorithm aimed at solving Eq. (2) (Efron et al., 2004). It is also possible to enforce this condition using an elastic net penalization (Zou & Hastie, 2005), replacing ||\u03b1|| 1 by ||\u03b1|| 1 + \u03ba2 2 ||\u03b1|| 2 2 and thus improving the numerical stability of homotopy algorithms such as LARS. Again, we have omitted this penalization for simplicity.", "publication_ref": ["b14", "b9", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results and Proof Sketches", "text": "Given assumptions (A) to (C), let us now show that our algorithm converges to a stationary point of the objective function.\nProposition 1 (convergence of f (D t ) and of the surrogate function). Letf t denote the surrogate function defined in Eq. (7). Under assumptions (A) to (C):\n\u2022f t (D t ) converges a.s.; \u2022 f (D t ) \u2212f t (D t ) converges a.s. to 0; and \u2022 f (D t ) converges a.s. Proof sktech: The first step in the proof is to show that D t \u2212 D t\u22121 = O 1\nt which, although it does not ensure the convergence of D t , ensures the convergence of the series\n\u221e t=1 ||D t \u2212 D t\u22121 || 2\nF , a classical condition in gradient descent convergence proofs (Bertsekas, 1999). In turn, this reduces to showing that D t minimizes a parametrized quadratic function over C with parameters 1 t A t and 1 t B t , then showing that the solution is uniformly Lipschitz with respect to these parameters, borrowing some ideas from perturbation theory (Bonnans & Shapiro, 1998). At this point, and following Bottou (1998), proving the convergence of the sequencef t (D t ) amounts to showing that the stochastic positive process\nu t \u25b3 =f t (D t ) \u2265 0,(12)\nis a quasi-martingale. To do so, denoting by F t the filtration of the past information, a theorem by Fisk (1965) states that if the positive sum\n\u221e t=1 E[max(E[u t+1 \u2212 u t |F t ],0\n)] converges, then u t is a quasi-martingale which converges with probability one. Using some results on empirical processes (Van der Vaart, 1998, Chap. 19.2, Donsker Theorem), we obtain a bound that ensures the convergence of this series. It follows from the convergence of u t that f t (D t ) \u2212f t (D t ) converges to zero with probability one. Then, a classical theorem from perturbation theory (Bonnans & Shapiro, 1998, Theorem 4.1) shows that l(x, D) is C 1 . This, allows us to use a last result on empirical processes ensuring that f (D t ) \u2212f t (D t ) converges almost surely to 0. Therefore f (D t ) converges as well with probability one.\nProposition 2 (convergence to a stationary point). Under assumptions (A) to (C), D t is asymptotically close to the set of stationary points of the dictionary learning problem with probability one.\nProof sktech: The first step in the proof is to show using classical analysis tools that, given assumptions (A) to (C), f is C 1 with a Lipschitz gradient. Considering\u00c3 andB two accumulation points of 1 t A t and 1 t B t respectively, we can define the corresponding surrogate functionf \u221e such that for all D in C,f \u221e (D) = 1 2 Tr(D T D\u00c3) \u2212 Tr(D TB ), and its optimum D \u221e on C. The next step consists of showing that \u2207f \u221e (D \u221e ) = \u2207f (D \u221e ) and that \u2212\u2207f (D \u221e ) is in the normal cone of the set C-that is, D \u221e is a stationary point of the dictionary learning problem (Borwein & Lewis, 2006).", "publication_ref": ["b2", "b4", "b6", "b11", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Validation", "text": "In this section, we present experiments on natural images to demonstrate the efficiency of our method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Performance evaluation", "text": "For our experiments, we have randomly selected 1.25\u00d710 6 patches from images in the Berkeley segmentation dataset, which is a standard image database; 10 6 of these are kept for training, and the rest for testing. We used these patches to create three datasets A, B, and C with increasing patch and dictionary sizes representing various typical settings in image processing We have normalized the patches to have unit \u2113 2 -norm and used the regularization parameter \u03bb = 1.2/ \u221a m in all of our experiments. The 1/ \u221a m term is a classical normalization factor (Bickel et al., 2007), and the constant 1.2 has been experimentally shown to yield reasonable sparsities (about 10 nonzero coefficients) in these experiments. We have implemented the proposed algorithm in C++ with a Matlab interface. All the results presented in this section use the mini-batch refinement from Section 3.4 since this has shown empirically to improve speed by a factor of 10 or more. This requires to tune the parameter \u03b7, the number of signals drawn at each iteration. Trying different powers of 2 for this variable has shown that \u03b7 = 256 was a good choice (lowest objective function values on the training set -empirically, this setting also yields the lowest values on the test set), but values of 128 and and 512 have given very similar performances.\nOur implementation can be used in both the online setting it is intended for, and in a regular batch mode where it uses the entire dataset at each iteration (corresponding to the mini-batch version with \u03b7 = n). We have also implemented a first-order stochastic gradient descent algorithm that shares most of its code with our algorithm, except for the dictionary update step. This setting allows us to draw meaningful comparisons between our algorithm and its batch and stochastic gradient alternatives, which would have been difficult otherwise. For example, comparing our algorithm to the Matlab implementation of the batch approach from (Lee et al., 2007) developed by its authors would have been unfair since our C++ program has a builtin speed advantage. Although our implementation is multithreaded, our experiments have been run for simplicity on a single-CPU, single-core 2.4Ghz machine. To measure and compare the performances of the three tested methods, we have plotted the value of the objective function on the test set, acting as a surrogate of the expected cost, as a function of the corresponding training time.\nOnline vs Batch. Figure 1 (top) compares the online and batch settings of our implementation. The full training set consists of 10 6 samples. The online version of our algorithm draws samples from the entire set, and we have run its batch version on the full dataset as well as subsets of size 10 4 and 10 5 (see figure). The online setting systematically outperforms its batch counterpart for every training set size and desired precision. We use a logarithmic scale for the computation time, which shows that in many situations, the difference in performance can be dramatic. Similar experiments have given similar results on smaller datasets.\nComparison with Stochastic Gradient Descent. Our experiments have shown that obtaining good performance with stochastic gradient descent requires using both the mini-batch heuristic and carefully choosing the learning rate \u03c1. To give the fairest comparison possible, we have thus optimized these parameters, sampling \u03b7 values among powers of 2 (as before) and \u03c1 values among powers of 10.\nThe combination of values \u03c1 = 10 4 , \u03b7 = 512 gives the best results on the training and test data for stochastic gradient descent. Figure 1 (bottom) compares our method with stochastic gradient descent for different \u03c1 values around 10 4 and a fixed value of \u03b7 = 512. We observe that the larger the value of \u03c1 is, the better the eventual value of the objective function is after many iterations, but the longer it will take to achieve a good precision. Although our method performs better at such high-precision settings for dataset C, it appears that, in general, for a desired precision and a particular dataset, it is possible to tune the stochastic gradient descent algorithm to achieve a performance similar to that of our algorithm. Note that both stochastic gradient descent and our method only start decreasing the objective function value after a few iterations. Slightly better results could be obtained by using smaller gradient steps during the first iterations, using a learning rate of the form \u03c1/(t + t 0 ) for the stochastic gradient descent, and initializing A 0 = t 0 I and B 0 = t 0 D 0 for the matrices A t and B t , where t 0 is a new parameter.", "publication_ref": ["b3", "b15"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Application to Inpainting", "text": "Our last experiment demonstrates that our algorithm can be used for a difficult large-scale image processing task, namely, removing the text (inpainting) from the damaged 12-Megapixel image of Figure 2. Using a multi-threaded version of our implementation, we have learned a dictionary with 256 elements from the roughly 7 \u00d7 10 6 undamaged 12 \u00d7 12 color patches in the image with two epochs in about 500 seconds on a 2.4GHz machine with eight cores. Once the dictionary has been learned, the text is removed using the sparse coding technique for inpainting of Mairal et al. (2008). Our intent here is of course not to evaluate our learning procedure in inpainting tasks, which would require a thorough comparison with state-the-art techniques on standard datasets. Instead, we just wish to demonstrate that the proposed method can indeed be applied to a realistic, non-trivial image processing task on a large image. Indeed, to the best of our knowledge, this is the first time that dictionary learning is used for image restoration on such large-scale data. For comparison, the dictionaries used for inpainting in the state-of-the-art method of Mairal et al. (2008) are learned (in batch mode) on only 200,000 patches.", "publication_ref": ["b16", "b16"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Discussion", "text": "We have introduced in this paper a new stochastic online algorithm for learning dictionaries adapted to sparse coding tasks, and proven its convergence. Preliminary experiments demonstrate that it is significantly faster than batch alternatives on large datasets that may contain millions of training examples, yet it does not require learning rate tuning like regular stochastic gradient descent methods. More experiments are of course needed to better assess the promise of this approach in image restoration tasks such as denoising, deblurring, and inpainting. Beyond this, we plan to use the proposed learning framework for sparse coding in computationally demanding video restoration tasks (Protter & Elad, 2009), with dynamic datasets whose size is not fixed, and also plan to extend this framework to different loss functions to address discriminative tasks such as image classification (Mairal et al., 2009), which are more sensitive to overfitting than reconstructive ones, and various matrix factorization tasks, such as non-negative matrix factorization with sparseness constraints and sparse principal component analysis. ", "publication_ref": ["b21", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This paper was supported in part by ANR under grant MGA. The work of Guillermo Sapiro is partially supported by ONR, NGA, NSF, ARO, and DARPA.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sparse and redundant modeling of image content using an image-signaturedictionary", "journal": "SIAM Imaging Sciences", "year": "2008", "authors": "M Aharon; M Elad"}, {"ref_id": "b1", "title": "The K-SVD: An algorithm for designing of overcomplete dictionaries for sparse representations", "journal": "IEEE Transactions Signal Processing", "year": "2006", "authors": "M Aharon; M Elad; A M Bruckstein"}, {"ref_id": "b2", "title": "Nonlinear programming", "journal": "", "year": "1999", "authors": "D Bertsekas"}, {"ref_id": "b3", "title": "Simultaneous analysis of Lasso and Dantzig selector", "journal": "", "year": "2007", "authors": "P Bickel; Y Ritov; A Tsybakov"}, {"ref_id": "b4", "title": "Optimization problems with perturbation: A guided tour", "journal": "SIAM Review", "year": "1998", "authors": "J Bonnans; A Shapiro"}, {"ref_id": "b5", "title": "Convex analysis and nonlinear optimization: theory and examples", "journal": "Springer", "year": "2006", "authors": "J Borwein; A Lewis"}, {"ref_id": "b6", "title": "Online algorithms and stochastic approximations", "journal": "", "year": "1998", "authors": "L Bottou"}, {"ref_id": "b7", "title": "The tradeoffs of large scale learning", "journal": "", "year": "2008", "authors": "L Bottou; O Bousquet"}, {"ref_id": "b8", "title": "Atomic decomposition by basis pursuit", "journal": "SIAM Journal on Scientific Computing", "year": "1999", "authors": "S Chen; D Donoho; M Saunders"}, {"ref_id": "b9", "title": "Least angle regression", "journal": "Annals of Statistics", "year": "2004", "authors": "B Efron; T Hastie; I Johnstone; R Tibshirani"}, {"ref_id": "b10", "title": "Image denoising via sparse and redundant representations over learned dictionaries", "journal": "", "year": "2006", "authors": "M Elad; M Aharon"}, {"ref_id": "b11", "title": "Quasi-martingale. Transactions of the", "journal": "American Mathematical Society", "year": "1965", "authors": "D Fisk"}, {"ref_id": "b12", "title": "Pathwise coordinate optimization", "journal": "Annals of Statistics", "year": "2007", "authors": "J Friedman; T Hastie; H H\u00f6lfling; R Tibshirani"}, {"ref_id": "b13", "title": "Penalized Regressions: The Bridge Versus the Lasso", "journal": "Journal of computational and graphical statistics", "year": "1998", "authors": "W Fu"}, {"ref_id": "b14", "title": "Recovery of exact sparse representations in the presence of bounded noise", "journal": "IEEE Transactions Information Theory", "year": "2005", "authors": "J Fuchs"}, {"ref_id": "b15", "title": "Efficient sparse coding algorithms", "journal": "", "year": "2007", "authors": "H Lee; A Battle; R Raina; A Y Ng"}, {"ref_id": "b16", "title": "Sparse representation for color image restoration", "journal": "", "year": "2008", "authors": "J Mairal; M Elad; G Sapiro"}, {"ref_id": "b17", "title": "Supervised dictionary learning", "journal": "Advances in Neural Information Processing Systems", "year": "2009", "authors": "J Mairal; F Bach; J Ponce; G Sapiro; A Zisserman"}, {"ref_id": "b18", "title": "A wavelet tour of signal processing, second edition", "journal": "Academic Press", "year": "1999", "authors": "S Mallat"}, {"ref_id": "b19", "title": "Sparse coding with an overcomplete basis set: A strategy employed by V1?", "journal": "Vision Research", "year": "1997", "authors": "B A Olshausen; D J Field"}, {"ref_id": "b20", "title": "A new approach to variable selection in least squares problems", "journal": "IMA Journal of Numerical Analysis", "year": "2000", "authors": "M Osborne; B Presnell; B Turlach"}, {"ref_id": "b21", "title": "Image sequence denoising via sparse and redundant representations", "journal": "IEEE Transactions Image Processing", "year": "2009", "authors": "M Protter; M Elad"}, {"ref_id": "b22", "title": "Self-taught learning: transfer learning from unlabeled data", "journal": "", "year": "2007", "authors": "R Raina; A Battle; H Lee; B Packer; A Y Ng"}, {"ref_id": "b23", "title": "Regression shrinkage and selection via the Lasso", "journal": "Journal of the Royal Statistical Society Series B", "year": "1996", "authors": "R Tibshirani"}, {"ref_id": "b24", "title": "Asymptotic Statistics", "journal": "Cambridge University Press", "year": "1998", "authors": "A Van Der Vaart"}, {"ref_id": "b25", "title": "Regularization and variable selection via the elastic net", "journal": "Journal of the Royal Statistical Society Series B", "year": "2005", "authors": "H Zou; T Hastie"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Top: Comparison between online and batch learning for various training set sizes. Bottom: Comparison between our method and stochastic gradient (SG) descent with different learning rates \u03c1. In both cases, the value of the objective function evaluated on the test set is reported as a function of computation time on a logarithmic scale. Values of the objective function greater than its initial value are truncated.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Inpainting example on a 12-Megapixel image. Top: Damaged and restored images. Bottom: Zooming on the damaged and restored images. (Best seen in color)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "f n (D) \u25b3 = 1 n n i=1 l(x i , D),(1)", "formula_coordinates": [2.0, 121.21, 656.37, 168.22, 30.61]}, {"formula_id": "formula_1", "formula_text": "l(x, D) \u25b3 = min \u03b1\u2208R k 1 2 ||x \u2212 D\u03b1|| 2 2 + \u03bb||\u03b1|| 1 ,(2)", "formula_coordinates": [2.0, 340.8, 206.21, 200.63, 23.78]}, {"formula_id": "formula_2", "formula_text": "C \u25b3 = {D \u2208 R m\u00d7k s.t. \u2200j = 1, . . . , k, d T j d j \u2264 1}. (3)", "formula_coordinates": [2.0, 315.24, 365.13, 226.19, 19.26]}, {"formula_id": "formula_3", "formula_text": "min D\u2208C,\u03b1\u2208R k\u00d7n 1 n n i=1 1 2 ||x i \u2212 D\u03b1 i || 2 2 + \u03bb||\u03b1 i || 1 . (4)", "formula_coordinates": [2.0, 318.22, 473.78, 223.21, 30.61]}, {"formula_id": "formula_4", "formula_text": "||x||p \u25b3 = ( P m i=1 |x[i]| p ) 1/p .", "formula_coordinates": [2.0, 307.44, 655.59, 103.81, 12.97]}, {"formula_id": "formula_5", "formula_text": "f (D) \u25b3 = E x [l(x, D)] = lim n\u2192\u221e f n (D) a.s.,(5)", "formula_coordinates": [3.0, 88.94, 99.42, 200.5, 17.17]}, {"formula_id": "formula_6", "formula_text": "D t = \u03a0 C D t\u22121 \u2212 \u03c1 t \u2207 D l(x t , D t\u22121 ) ,(6)", "formula_coordinates": [3.0, 94.61, 349.81, 194.83, 23.77]}, {"formula_id": "formula_7", "formula_text": "\u03b1 t \u25b3 = arg min \u03b1\u2208R k 1 2 ||x t \u2212 D t\u22121 \u03b1|| 2 2 + \u03bb||\u03b1|| 1 . (8) 5: A t \u2190 A t\u22121 + \u03b1 t \u03b1 T t . 6: B t \u2190 B t\u22121 + x t \u03b1 T t .", "formula_coordinates": [3.0, 312.42, 186.48, 229.02, 65.65]}, {"formula_id": "formula_8", "formula_text": "D t \u25b3 = arg min D\u2208C 1 t t i=1 1 2 ||x i \u2212 D\u03b1 i || 2 2 + \u03bb||\u03b1 i || 1 , = arg min D\u2208C 1 t 1 2 Tr(D T DA t ) \u2212 Tr(D T B t ) .(9)", "formula_coordinates": [3.0, 338.67, 279.21, 202.77, 68.7]}, {"formula_id": "formula_9", "formula_text": "f t (D) \u25b3 = 1 t t i=1 1 2 ||x i \u2212 D\u03b1 i || 2 2 + \u03bb||\u03b1 i || 1 ,(7)", "formula_coordinates": [3.0, 337.14, 486.06, 204.3, 30.61]}, {"formula_id": "formula_10", "formula_text": "Require: D = [d 1 , . . . , d k ] \u2208 R m\u00d7k (input dictionary), A = [a 1 , . . . , a k ] \u2208 R k\u00d7k = t i=1 \u03b1 i \u03b1 T i , B = [b 1 , . . . , b k ] \u2208 R m\u00d7k = t i=1 x i \u03b1 T i . 1: repeat 2: for j = 1 to k do 3:", "formula_coordinates": [4.0, 55.44, 83.19, 226.82, 71.58]}, {"formula_id": "formula_11", "formula_text": "u j \u2190 1 A jj (b j \u2212 Da j ) + d j . d j \u2190 1 max(||u j || 2 , 1) u j .(10) 4:", "formula_coordinates": [4.0, 60.42, 164.88, 229.02, 70.51]}, {"formula_id": "formula_12", "formula_text": "A t \u2190 \u03b2A t\u22121 + \u03b7 i=1 \u03b1 t,i \u03b1 T t,i , B t \u2190 \u03b2B t\u22121 + \u03b7 i=1 x\u03b1 T t,i ,(11)", "formula_coordinates": [4.0, 359.35, 578.97, 182.08, 31.82]}, {"formula_id": "formula_13", "formula_text": "\u2022f t (D t ) converges a.s.; \u2022 f (D t ) \u2212f t (D t ) converges a.s. to 0; and \u2022 f (D t ) converges a.s. Proof sktech: The first step in the proof is to show that D t \u2212 D t\u22121 = O 1", "formula_coordinates": [5.0, 307.44, 279.88, 233.99, 87.95]}, {"formula_id": "formula_14", "formula_text": "\u221e t=1 ||D t \u2212 D t\u22121 || 2", "formula_coordinates": [5.0, 335.4, 372.26, 78.09, 19.47]}, {"formula_id": "formula_15", "formula_text": "u t \u25b3 =f t (D t ) \u2265 0,(12)", "formula_coordinates": [5.0, 388.47, 503.95, 152.97, 19.25]}, {"formula_id": "formula_16", "formula_text": "\u221e t=1 E[max(E[u t+1 \u2212 u t |F t ],0", "formula_coordinates": [5.0, 411.87, 549.73, 121.81, 19.48]}], "doi": ""}