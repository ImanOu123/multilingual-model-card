{"title": "On the Relation between Distributionally Robust Optimization and Data Curation (Student Abstract)", "authors": "Agnieszka S\u0142owik; L\u00e9on Bottou; Sean B Holden; Mateja Jamnik", "pub_date": "", "abstract": "Machine learning systems based on minimizing average error have been shown to perform inconsistently across important subsets of the data, and this defect is not exposed by a low average error for the entire dataset. In some social and economic applications, where data represent people, this can lead to discrimination of underrepresented gender, ethnic and other groups. Distributionally Robust Optimization (DRO) attempts to address this problem by minimizing the worst expected risk across subpopulations. We establish theoretical results that clarify the relation between DRO and the optimization of the same loss averaged on a weighted training dataset. A practical implication of our results is that neither DRO nor curation of the training set represent a complete solution for bias mitigation.", "sections": [{"heading": "Introduction", "text": "Machine learning algorithms are increasingly used to support real-world decision-making. Optimizing for the loss averaged on the overall population can yield models that perform poorly on specific subpopulations, amplifying injustices in our society (Chouldechova 2017).\nDistributionally Robust Optimization (DRO) (Ben-Tal, Ghaoui, and Nemirovski 2009) bridges two perspectives on this problem. DRO seems to offer a promising solution because it minimizes the worst loss observed on multiple distributions (which e.g. represent each subpopulation). However, it can be shown that, under weak conditions, DRO is closely related to minimizing average loss on some mixture of those distributions -that is, a training set in which the subpopulations have been weighted. Our contributions are: 1. We establish results that clarify the relation between DRO and the optimization of the same loss averaged on a correctly weighted training set. 2. We also show that neither DRO nor curation of the training set are a complete solution of our initial problem due to the implicit assumptions DRO makes on the data. 3. We use this mathematical understanding to provide a minimal set of practical recommendations with which to approach real-life bias mitigation. This is guided by our results which show DRO is not applicable if we are\nCopyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nunable to obtain an acceptable result with systems optimized for each subpopulation alone. Proofs and an extended discussion of the results and their implications are included in the supplemental file.", "publication_ref": ["b4", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "DRO and Data Curation", "text": "Traditionally, training a machine learning model seeks parameters that minimize a risk C P (w) that is the expectation of a loss function with respect to a single distribution of training examples. Alas, even when the training distribution is representative of the actual testing conditions, the trained system might perform very poorly on selected subsets of examples (Chouldechova 2017). In real life, this can be a source of major injustice. DRO seemingly addresses this problem by considering instead a collection Q of 'training distributions' and minimizing the expected risk observed on the most adverse distribution: min w max P \u2208Q C P (w) .\n(1)\nWe can introduce calibration coefficients r P that control how we compare costs for different distributions:\nmin w max P \u2208Q (C P (w) \u2212 r P ) .(2)\nFor convex cost functions we already know that finding a local minimum of the DRO problem (1) is equivalent to minimizing the usual expected risk with respect to a single, well-crafted, training distribution, because one can reformulate the DRO problem as a constrained optimization problem and rely on standard convex duality results (Bertsekas 2009). We show that similar results hold for the local minima of the nonconvex costs typical of modern deep learning systems, and also hold when the family Q is infinite.\nLet (z, w) be the loss of a machine learning model where w \u2208 R d represent the parameters of the model and z \u2208 R n are examples. The following theorem generalizes the result by Arjovsky et al. (Arjovsky et al. 2019) by eliminating the Karush-Kuhn-Tucker (KKT) conditions. Theorem 1 (Finite case). Let Q = {P 1 , . . . , P K } be a finite set of probability distributions on R n and let w * be a local minimum of the DRO problem (1) or the calibrated DRO problem (2). Let the costs C P (w) = E z\u223cP [ (z, w)] be differentiable in w * for all P \u2208 Q. Then there exists a mixture distribution P mix = k \u03bb k P k such that \u2207C Pmix (w * ) = 0.\nWhen the collection Q is infinite (possibly uncountably) but satisfies a tightness condition (Billingsley 1999), we can still show that a DRO local minimum is a stationary point for a well-crafted training distribution. Adversarial robustness is an example of applying DRO on an infinite family of distributions. Theorem 2 (Infinite case). Let Q be a tight family of probability distributions on R n . Let w * be a local minimum of problem (2). Let Q mix be the weak convergence closure of the convex hull of Q. Let there be a bounded continuous function h(z, w) defined on a neighborhood V of w * such that \u2207C P (w) = E z\u223cP [h(z, w)] for all P \u2208 Q mix and such that h(z, w) \u2212 h(z, w ) \u2264 M w \u2212 w for almost all z \u2208 R n . Then Q mix contains a distribution P mix such that \u2207 w C Pmix (w * ) = 0.\nConversely, we consider a local minimum of the expectation of the loss with respect to an arbitrary mixture of distributions from Q. Such a local minimum always is a local minimum of a calibrated DRO problem. Theorem 3 (Converse). Let P mix = k \u03bb k P k be an arbitrary mixture of distributions P k \u2208 Q. If w * is a local minimum of C Pmix , then w * is a local minimum of the calibrated DRO problem (2) with calibration coefficients r P k = C P k (w * ).\nProofs are given in the supplemental file.", "publication_ref": ["b4", "b2", "b0", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Calibration Problems", "text": "The calibration constants r P might be a better way than mixture coefficients \u03bb P k to specify which performance discrepancies are deemed acceptable across subpopulations because there are useful reference points for choosing them. An intuitive approach is to use the calibration constants r * P representing the best performance we can reach with our machine learning model on each distribution P in isolation: r * P = min w C P (w). Solving the DRO problem for these calibration constants amounts to constructing a single machine learning system that performs nearly as well on each distribution P as a system trained for distribution P alone.\nNote that based on Theorems 1 and 3, regardless of the chosen calibration constants, no DRO solution can achieve a performance better than r * P on any distribution P . If this were the case, it would mean that r * P was not correctly estimated, and the new performance would become the corrected r * P . This simple observation forms the basis for a minimal set of recommendations to machine learning engineers who face the difficult task of constructing and deploying bias-sensitive machine learning systems. These recommendations (summarized in Inset 1) represent intuitively sensible steps that are supported by our mathematical insights.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Whether fighting bias in machine learning systems is a data curation or an algorithmic problem has been the object of much discussion. Our results clarify the relation between a well-known algorithmic approach, DRO, and the optimization of the expected cost on a well-crafted data distribution. This analysis also clarifies that this well-crafted distribution Fighting bias with DRO: recommendations 1. Identify subpopulations P k at risk (based on the available data). 2. For each subpopulation, and in isolation, determine the best performance r * P k that can be achieved with the machine learning model of choice. 3. Decide whether the r * P k represent an acceptable set of performances. There is no point using DRO if this is not the case. Instead, investigate why the model performs so poorly on the adverse distributions (insufficient data, inadequate model, etc) until obtaining an acceptable set of r * P k . 4. Use DRO with calibration coefficients r * P k to construct a single machine learning system (these are the calibration coefficients). 5. Deploy the system on an experimental basis in order to collect more data. Sample the examples with the lowest accuracy in order to determine whether we missed a subpopulation at risk. If one is found, add the vulnerable subpopulation to the initial data and repeat all the steps.\nInset 1: Summary of practical recommendations.\nis not universal but depends on often implicit details of the DRO problem setup such as calibration constants.\nUsing DRO for fairness without a clear understanding of its algorithmic limitations can have a negative societal impact. Our recommendations aim to prevent misuses of DRO, such as lowering performances on the remaining subpopulations to match the error on the most difficult distribution. It follows from our results that it is also necessary to address the underlying problems in the most challenging distribution. We hope that our results and discussion will give more context to the debate on the sources of bias in machine learning and help with bias mitigation in real-life scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Invariant Risk Minimization", "journal": "", "year": "2019", "authors": "M Arjovsky; L Bottou; I Gulrajani; D Lopez-Paz"}, {"ref_id": "b1", "title": "Robust Optimization", "journal": "Princeton University Press", "year": "2009", "authors": "A Ben-Tal; L E Ghaoui; A Nemirovski"}, {"ref_id": "b2", "title": "Convex Optimization Theory. Athena Scientific optimization and computation series", "journal": "Athena Scientific", "year": "2009", "authors": "D Bertsekas"}, {"ref_id": "b3", "title": "Convergence of probability measures. Wiley Series in Probability and Statistics: Probability and Statistics", "journal": "John Wiley & Sons Inc", "year": "1999", "authors": "P Billingsley"}, {"ref_id": "b4", "title": "Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments", "journal": "Big Data", "year": "2017", "authors": "A Chouldechova"}], "figures": [], "formulas": [{"formula_id": "formula_0", "formula_text": "min w max P \u2208Q (C P (w) \u2212 r P ) .(2)", "formula_coordinates": [1.0, 381.92, 463.39, 176.08, 14.58]}], "doi": ""}