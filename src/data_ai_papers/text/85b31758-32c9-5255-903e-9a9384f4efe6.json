{"title": "Adapting Text instead of the Model: An Open Domain Approach", "authors": "Gourab Kundu; Dan Roth", "pub_date": "", "abstract": "Natural language systems trained on labeled data from one domain do not perform well on other domains. Most adaptation algorithms proposed in the literature train a new model for the new domain using unlabeled data. However, it is time consuming to retrain big models or pipeline systems. Moreover, the domain of a new target sentence may not be known, and one may not have significant amount of unlabeled data for every new domain. To pursue the goal of an Open Domain NLP (train once, test anywhere), we propose ADUT (ADaptation Using label-preserving Transformation), an approach that avoids the need for retraining and does not require knowledge of the new domain, or any data from it. Our approach applies simple label-preserving transformations to the target text so that the transformed text is more similar to the training domain; it then applies the existing model on the transformed sentences and combines the predictions to produce the desired prediction on the target text. We instantiate ADUT for the case of Semantic Role Labeling (SRL) and show that it compares favorably with approaches that retrain their model on the target domain. Specifically, this \"on the fly\" adaptation approach yields 13% error reduction for a single parse system when adapting from the news wire text to fiction.", "sections": [{"heading": "Introduction", "text": "In several NLP tasks, systems trained on annotated data from one domain perform well when tested on the same domain but adapt poorly to other domains. For example, all systems of CoNLL 2005 shared task (Carreras and M\u00e0rquez, 2005) on Semantic Role Labeling showed a performance degradation of almost 10% or more when tested on a different domain.\nMost works in domain adaptation have focused on learning a common representation across training and test domains (Blitzer et al., 2006;Daum\u00e9III, 2007;Huang and Yates, 2009). Using this representation, they retrain the model for every new domain. But these are not Open Domain Systems since the model needs to be retrained for every new domain. This is very difficult for pipeline systems like SRL where syntactic parser, shallow parser, POS tagger and then SRL need to be retrained. Moreover, these methods need to have a lot of unlabeled data that is taken from the same domain, in order to learn meaningful feature correspondences across training and test domain. These approaches cannot work when they do not have a lot of unlabeled data from the test domain or when the test domain in itself is very diverse, e.g., the web.\nThe contribution of this paper is a new framework for adaptation. We propose ADUT (ADaptation Using label-preserving Transformation) as a framework in which a previously learned model can be used on an out-of-domain example without retraining and without looking at any labeled or unlabeled data for the domain of the new example. The framework transforms the test sentence to generate sentences that have, in principle, identical labeling but that are more like instances from the training domain. Consequently, it is expected that the exist-ing model will make better predictions on them. All these predictions are then combined to choose the most probable and consistent prediction for the test sentence.\nADUT is a general technique which can be applied to any natural language task. In this paper, we demonstrate its usefulness on the task of semantic role labeling (Carreras and M\u00e0rquez, 2005). Starting with a system that was trained on the news text and does not perform well on fiction, we show that ADUT provides significant improvement on fiction, and is competitive with the performance of algorithms that were re-trained on the test domain.\nThe paper is organized as follows. Section 2 discusses two motivating examples. Section 3 gives a formal definition of our adaptation framework. Section 4 describes the transformation operators that we applied for this task. Section 5 presents our joint inference approach. Section 6 describes our semantic role labeling system and our experimental results are in Section 7. Section 8 describes the related works for domain adaptation. Finally in Section 9 we conclude the paper with a discussion.", "publication_ref": ["b5", "b3", "b9", "b11", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Motivating Examples", "text": "One of the key reasons for performance degradation of an NLP tool is unseen features such as words in the new domain that were not seen in the training domain. But if an unknown word is replaced by a known word without changing the labeling of the sentence, tools perform better. For example, in the task of syntactic parsing, the unknown word checkup causes the Charniak parser to make a wrong coordination decision on the sentence He was discharged from the hospital after a two-day checkup and he and his parents had what Mr. Mckinley described as a \"celebration lunch\" at the cafeteria on the campus.\nIf we replace the word checkup with its hypernym examination which appears in training data, the parse gets corrected. Figure 1 shows both original and corrected parse trees.\nFor the task of semantic role labeling, systems do not perform well on the predicates that are infrequent in training domain. But if an infrequent predi-cate is replaced with a frequent predicate from training domain such that both predicates have similar semantic argument structure, the system performs better. Consider the following sentence Scotty gazed out at ugly gray slums.\nThe semantic role for the phrase at ugly gray slums with respect to predicate gaze is A1. But the predicate gaze appears only once in training data and our model predicts at ugly gray slums as AM-LOC instead of A1. But if gaze is replaced with look which occurs 328 times in training data and has similar argument structure (in the same VerbNet class as gaze), the system makes the correct prediction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Let the in-domain distribution be D i and out-ofdomain distribution be D o . We have a model f trained over a set of labeled examples drawn from D i . If D i and D o are very dissimilar, f will not perform well on examples drawn from D o . The problem is to get good performance from f on D o without retraining f .\nWe define a Transformation g to be a function that maps an example e into a set of examples E. So g : X \u2192 2 X where X is the entire space of examples. In this paper, we only consider the Label-preserving Transformations which satisfy the property that all transformed examples in E have the same label as input example e, i.e., \u2200x x \u2208 S k \u21d2 g(x) \u2282 S k where S k is the set of examples with label k . Let G be a set of label-preserving transformation functions. G = {g 1 , g 2 , . . ., g p }.\nAt evaluation time, for test example d, we will apply G to get a set of examples\nT 1 . Let T 2 = {d \u2208 T 1 : D i (d ) > D i (d)}.\nSo all examples in T 2 have same label as d but have a higher probability than d to be drawn from the in-domain distribution. So f should perform better on examples in T 2 than on d. For each d \u2208 T 2 , f will produce scores for the output labels. The scores will be combined subject to constraints to produce the final output.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transformation Functions", "text": "After applying a transformation function to get a new sentence from an input sentence, we remember the mapping of segments across the original a.  (Huang and Yates, 2010), 6.1% of the predicates in the Brown test set do not appear in WSJ training set and 11.8% appear at most twice. Since the semantic roles of a sentence depend on the predicate, these infrequent predicates hurt SRL performance on new domains. Note that since all predicates in PropBank are verbs, we will use the words predicate and verb interchangeably.\nWe count the frequency of each predicate and its accuracy in terms of F1 score over the training data. If the frequency or the F1 score of the predicate in the test sentence is below a threshold, we perturb that predicate. We take all the verbs in the same class of VerbNet 1 as the original verb (in case the verb is present in multiple classes, we take all the classes). In case the verb is not present in VerbNet, we take its synonyms from WordNet. If there is no synonym in WordNet, we take the hypernyms.\nFrom this collection of new verbs, we select verbs that have a high accuracy and a high frequency in 1 http://verbs.colorado.edu/ mpalmer/projects/verbnet.html training. We replace the original verb with each of these new verbs and generate one new sentence for each new verb; the sentence is retained if the parse score for the new sentence is higher than the parse score for the original sentence. 2 VerbNet has defined a set of verb-independent thematic roles and grouped the verbs according to their usage in frames with identical thematic roles. But PropBank annotation was with respect to each verb. So the same thematic role is labeled as different roles for different verbs in PropBank. For example, both warn and advise belong to the same VerbNet class (37.9) and take thematic roles of Recipient (person being warned or advised) and Topic (topic of warning or advising). But Recipient was marked as A2 for warn and A1 for advise and Topic was marked as A1 for warn and A2 for advise in PropBank annotation. Semlink 3 provides a mapping from the thematic role to PropBank role for each verb. After the SRL annotates the new sentence with PropBank roles for the new verb, we map the PropBank roles of the new verb to their corresponding thematic roles and then map the thematic roles to the corresponding Prop-Bank roles for the original verb.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "II. Replacement and Removal of Quoted Strings:", "text": "Quoted sentences can vary a lot from one domain to another. For example, in WSJ, quoted sentences are like formal statements but in Brown, these are like informal conversations. We generate the transformations in the following ways:\n1) We use the content of the quoted string as one sentence. 2) We replace each quoted string in turn with a simple sentence (This is good) to generate a new sentence. 3) If a sentence has a quoted string in the beginning, we move that quoted string after the first NP and VP that immediately follow the quoted string. For example, from the input sentence, \"We just sit quiet\", he said. we generate the sentences 1)\nWe just sit quiet 2) \"This is good\", he said. 3) He said, \"We just sit quiet\". III. Replacement of Unseen Words: A major difficulty for domain adaptation is that some words in the new domain do not appear in the training domain. In the Brown test set, 5% of total words were never seen in the WSJ training set.\nGiven an unseen word which is not a verb, we replace it with WordNet synonyms and hypernyms that were seen in the training data. We used the clusters obtained in (Liang, 2005) from running the Brown algorithm (Brown et al., 1992) on Reuters 1996 dataset. But since this cluster was generated automatically, it is noisy. So we chose replacements from the Brown clusters selectively. We only replace those words for which the POS tagger and the syntactic parser predicted different tags. For each such word, we find its cluster and select the set of words from the cluster. We delete from this set all words that do not take at least one part-of-speech tag that the original word can take (from WordNet). For each candidate synonym or hypernym or cluster member, we get a new sentence. Finally we only keep those sentences that have higher parse scores than the original sentence.", "publication_ref": ["b15", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "IV. Sentence Split based on Stop Symbols:", "text": "We split each sentence based on stop symbols like ; and . . Each of the splitted sentences becomes one transformation of the original sentence. V. Sentence Simplification:\nWe have a set of heuristics for simplifying the constituents of the parse tree; for example, replacing an NP with its first and last word, removal of PRN phrases etc. We apply these heuristics and generate simpler sentences until no more simplification is possible. Examples of our heuristics are given in Table 1.\nNote that we can use composition of multiple transformation functions as one function. A composition p 1 p 2 (s) = \u222a a\u2208p 1 (s) p 2 (a). We apply II I, III I, IV I and V I.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Learned Transformations", "text": "The learned model is inaccurate over verbs and roles that are infrequent in the training data. The purpose of the learned transformation is to transfer such a phrase in the test sentence in place of a phrase of a simpler sentence; this is done such that there exists a mapping from the role of the phrase in the new sentence to the role of the phrase in the original sentence.\nPhrase Representation: A phrase tuple is a 3tuple (t, i, h) where, t is the phrase type, i is the index, and h is the headword of the phrase. We denote by P R the Phrase Representation of a sentence -an ordered list of phrase tuples. A phrase tuple corresponds to a node in the tree. We only consider phrase tuples that correspond to nodes that are (1) a sibling of the predicate node or (2) a sibling of an ancestor of the predicate node. Phrase tuples in P R are sorted based on their position in the sentence. The index i of the phrase tuple containing the predicate is taken to be zero with the indices of the phrase tuples on the left (right) sequentially decreasing (increasing).\nTransformation Rule: We denote by Label(n, s) the semantic role of nth phrase in the P R of the sentence s. Let Replace(n s , n t , s s , s t ) be a new sentence that results from inserting the phrase n s in sentence s s instead of phrase n t in sentence s t . We will refer to s t as target sentence and to n t as the target phrase. Let sp be a sequence of phrase tuples named as source pattern. If Label(n s , s s ) = r 1 and Label(n t , Replace(n s , n t , s s , s t )) = r 2 , then denote f (r 2 ) = r 1 . In this case we call the 6-tuple (s t , n t , p, sp, n s , f ) a transformation rule. We call f the label correspondence function. Example: Consider the sentence s t = \"But it did not sing.\" and the rule \u03c4 : (s t , n t , p, sp, n s , f ). Let: While checking if \u03c4.sp is a subsequence of the P R of the input sentence, \u03c6 in each tuple of \u03c4.sp has to be considered a trivial match. So \u03c4 will match the sentence He is entitled to a reward. with select some rules R \u2282 K based on database coverage 9: until all rules in R have been expanded before 10: return R The algorithm for finding rules for a semantic role r of a predicate v is given in Algorithm 1. It is a specific to general beam search procedure that starts with a set of initial rules (Line 3, detail in Algorithm 2) and finds new rules from these rules (Line 5, detail in Algorithm 3). In Line 7, the rules are sorted by decreasing order of accuracy, support and number of \u03c6 strings in the source pattern. In Line 8, a set of rules are selected to cover all occurrences of the semantic role r with the predicate v a specific number of times. This process continues until no new rules are found. Note that these rules need to be learned only once and can be used for every new domain.  The algorithm for generating initial rules for the semantic role r of predicate v is given in Algorithm 2. Shorter sentences are preferred to be target sentences(Line 4). A rule \u03c4 is created for every (p 1 ,p 2 ) pair where p 1 , p 2 are phrases, p 1 has the semantic role r in some sentence s 1 , p 2 is labeled as a core argument(A0 \u2212 A5) in some sentence in T and the phrase types of p 1 and p 2 in their respective parse trees are same(Lines 7 \u2212 9). Every sentence s 3 in training corpus with predicate \u03c4.p is a potential candidate for applying \u03c4 (Line 16) if \u03c4.sp is a subsequence of P R of s 3 (Line 17). After applying \u03c4 to s 3 , a transformed sentence x is created(Line 18). Lines 20 \u2212 26 find the semantic role r 2 of the transferred phrase from SRL annotation of x using model M and create a mapping from r 2 to the gold standard role r 1 of the phrase in s 3 . L maintains the set of semantic roles for which mappings have been created. In lines 28 \u2212 30, all unmapped roles are mapped to themselves.\nn t = \u22123, p = entitle, sp = [\u22122, N P, \u03c6][\u22121, AU X, \u03c6][0, V, entitle][1, \u03c6, to] n s = \u22122, f = {<A0, A2>} \u222a {<Ai, Ai>|i = 0}. The P R of \u03c4.s t is {[\u22124, CC, But] [\u22123, NP, it] [\u22122, AUX, did] [\u22121, RB, not] [0,\nP R = {[\u22122, NP, He] [\u22121, AUX, is] [0, V, entitle] [1,\nThe algorithm for creating new rules from a set of existing rules is given in Algorithm 3. Lines 4 \u2212 13 generate all immediate more general neighbors of the current rule by nullifying the headword or phrase type element in any of the phrase tuples in its source pattern. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 3 ExpandRules", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Combination by Joint Inference", "text": "The transformation functions transform an input sentence into a set of sentences T . From each transformed sentence t i , we get a set of argument candidates S i . Let S = |T | i=1 S i be the set of all arguments. Argument classifier assigns scores for each argument over the output labels(roles) in S that is then converted into a probability distribution over the possible labels using the softmax function (Bishop, 1995). Note that multiple arguments with the same span can be generated from multiple transformed sentences.\nFirst, we take all arguments from S with distinct span and put them in S . For each argument arg in S , we calculate scores over possible labels as the sum over the probability distribution (over output labels) of all arguments in S that have the same span as arg divided by the number of sentences in T that contained arg. This results in a set of arguments with distinct spans and for each argument, a set of scores over possible labels. Following the joint inference procedure in (Punyakanok et al., 2008), we want to select a label for each argument such that the total score is maximized subject to some constraints. Let us index the set S as S 1:M where M = |S |. Also assume that each argument can take a label from a set P . The set of arguments in S 1:M can take a set of labels c 1:M \u2208 P 1:M . Given some constraints, the resulting solution space is limited to a feasible set F; the inference task is:\nc 1:M = arg max c 1:M \u2208F (P 1:M ) M i=1 score(S i = c i ).\nThe constraints used are: 1) No overlapping or embedding argument. 2) No duplicate argument for core arguments A0-A5 and AA. 3) For C-arg, there has to be an arg argument.", "publication_ref": ["b2", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "In this section, we discuss our experimental setup for the semantic role labeling system. Similar to the CoNLL 2005 shared tasks, we train our system using sections 02-21 of the Wall Street Journal portion of Penn TreeBank labeled with PropBank. We test our system on an annotated Brown corpus consisting of three sections (ck01 -ck03).\nSince we need to annotate new sentences with syntactic parse, POS tags and shallow parses, we do not use annotations in the CoNLL distribution; instead, we re-annotate the data using publicly available part of speech tagger and shallow parser 1 , Charniak 2005 parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003).\nOur baseline SRL model is an implementation of (Punyakanok et al., 2008) which was the top performing system in CoNLL 2005 shared task. Due to space constraints, we omit the details of the system and refer readers to (Punyakanok et al., 2008).", "publication_ref": ["b6", "b14", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Results for ADUT using only the top parse of Charniak and Stanford are shown in Table 2. The Baseline model using top Charniak parse (BaseLine-Charniak) and top Stanford parse (BaseLine-Stanford) score respectively 76.4 and 73.3 on the WSJ test set. Since we are interested in adaptation, we report and compare results for Brown test set only. On this set, both ADUT-Charniak and ADUT-Stanford significantly outperform their respective baselines. We compare with the state-of-the-art system of (Surdeanu et al., 2007). In (Surdeanu et al., 2007), the authors use three models: Model 1 and 2 do sequential tagging of chunks obtained from shallow parse and full parse. Model 3 assumes each predicate argument maps to one syntactic constituent and classifies it individually. So Model 3 matches our baseline model. ADUT-Charniak outperforms the best individual model (Model 2) of (Surdeanu et al., 2007) by 1.6% and Model 3 by 3.9%. We also tested another system that used cluster features and word embedding features computed following (Collobert and Weston, 2008). But we did not see any performance improvement on Brown over baseline.  All state-of-the-art systems for SRL are a combination of multiple systems. So we combined ADUT-Stanford, ADUT-Charniak and another system ADUT-Charniak-2 based on 2nd best Charniak parse using joint inference. In Table 3, We compare with (Punyakanok et al., 2008) which was the top performing system in CoNLL 2005 shared task. We also compare with the multi parse system of (Toutanova et al., 2008) which uses a global joint model using multiple parse trees. In (Surdeanu et al., 2007), the authors experimented with several combination strategies. Their first combination strategy was similar to ours where they directly combined the outputs of different systems using constraints (denoted as Cons in Table 3). But their best result on Brown set was obtained by treating the combination of multiple systems as a meta-learning problem.\nThey trained a new model to score candidate arguments produced by individual systems before combining them through constraints (denoted as LBI in Table 3). We also compare with (Huang and Yates, 2010)   Table 3 shows that ADUT-Combined performs better than (Surdeanu et al., 2007) (Cons) when individual systems have been combined similarly. We believe that the techniques in (Surdeanu et al., 2007) of using multiple models of different kinds (two based on sequential tagging of chunks to capture arguments whose boundaries do not match a syntactic constituent) and training an additional model to combine the outputs of individual systems are orthogonal to the performance improvement that we have and applying these methods will further increase the performance of our final system which is a research direction we want to pursue in future.\nWe did an ablation study to determine which transformations help and by how much. Table 4 presents results when only one transformation is active at a time. We see that each transformation improves over the baseline.\nThe effect of the transformation of Replacement of Predicate on infrequent verbs is shown in Table 5. This transformation improves F1 as much as 6% on infrequent verbs.\nThe running time for ADUT-Charniak on Brown set is 8 hours compared to SRL training time of 20 hours. Average number of transformed sentences generated by ADUT-Charniak for every sentence from Brown is 36. The times are calculated based on a machine with 2x 6-Core Xeon X5650 Processor with 48G memory.    (Daum\u00e9III, 2007;Chelba and Acero, 2004;Finkel and Manning, 2009;Jiang and Zhai, 2007;Blitzer et al., 2006;Huang and Yates, 2009;Ando and Zhang, 2005;Ming-wei Chang and Roth, 2010) need to retrain the model for every new domain. In (Umansky-Pesin et al., 2010), there was no retraining; instead, a POS tag was predicted for every unknown word in the new domain by considering contexts of that word collected by web search queries. We differ from them in that our transformations are labelpreserving; moreover, our transformations aim at making the target text resemble the training text.\nWe also present an algorithm to learn transformation rules from training data. Our application domain, SRL, is also more complex and structured than POS tagging.\nIn (McClosky et al., 2010), the task of multiple source parser adaptation was introduced. The authors trained parsing models on corpora from different domains and given a new text, used a linear combination of trained models. Their approach requires annotated data from multiple domains as well as unlabeled data for the new domain, which is not needed in our framework. In (Huang and Yates, 2010), the authors trained a HMM over the Brown test set and the WSJ unlabeled data. They derived features from Viterbi optimal states of single words and spans of words and retrained their models using these features. In (Vickrey and Koller, 2008), a large number of hand-written rules were used to simplify the parse trees and reduce syntactic variation to overcome feature sparsity. We have several types of transformations, and use less than 10 simplification heuristics, based on replacing larger phrases with smaller phrases and deleting unnecessary parse tree nodes. There are also some methods for unsupervised semantic role labeling (Swier and Stevenson, 2004), (Abend et al., 2009) that easily adapt across domains but their performances are not comparable to supervised systems.", "publication_ref": ["b19", "b19", "b19", "b8", "b18", "b21", "b19", "b12", "b19", "b19", "b9", "b7", "b10", "b13", "b3", "b11", "b1", "b17", "b22", "b16", "b12", "b23", "b20", "b0"], "figure_ref": [], "table_ref": ["tab_4", "tab_6", "tab_6", "tab_6", "tab_6", "tab_8", "tab_9"]}, {"heading": "Conclusion", "text": "We presented a framework for adaptating natural language text so that models can be used across domains without modification. Our framework supports adapting to new domains without any data or knowledge of the target domain. We showed that our approach significantly improves SRL performance over the state-of-the-art single parse based system on Brown set. In the future, we would like to extend this approach to other NLP problems and study how combining multiple systems can further improve its performance and robustness.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Unsupervised argument identification for semantic role labeling", "journal": "", "year": "2009", "authors": "Omri Abend; Roi Reichart; Ari Rappoport"}, {"ref_id": "b1", "title": "A framework for learning predictive structures from multiple labeled and unlabeled data", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "Rie Kubota; Ando ; Tong Zhang"}, {"ref_id": "b2", "title": "Neural Networks for Pattern recognition, chapter 6.4: Modelling conditional distributions", "journal": "Oxford University Press", "year": "1995", "authors": "Christopher Bishop"}, {"ref_id": "b3", "title": "Domain adaptation with structural correspondence learning", "journal": "", "year": "2006", "authors": "John Blitzer; Ryan Mcdonald; Fernando Pereira"}, {"ref_id": "b4", "title": "Class-based n-gram models of natural language", "journal": "Computational Linguistics", "year": "1992", "authors": "F Peter; Peter V Brown; Robert L Desouza;  Mercer; J D Vincent; Jenifer C Pietra;  Lai"}, {"ref_id": "b5", "title": "Introduction to the conll-2005 shared task: Semantic role labeling", "journal": "", "year": "2005", "authors": "Xavier Carreras; Llu\u00eds M\u00e0rquez"}, {"ref_id": "b6", "title": "Coarse-tofine n-best parsing and maxent discriminative reranking", "journal": "", "year": "2005", "authors": "Eugene Charniak; Mark Johnson"}, {"ref_id": "b7", "title": "Little data can help a lot", "journal": "", "year": "2004", "authors": "Ciprian Chelba; Alex Acero"}, {"ref_id": "b8", "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "journal": "", "year": "2008", "authors": "Ronan Collobert; Jason Weston"}, {"ref_id": "b9", "title": "Frustratingly easy domain adaptation", "journal": "", "year": "2007", "authors": "Hal Daum\u00e9iii"}, {"ref_id": "b10", "title": "Hierarchical bayesian domain adaptation", "journal": "", "year": "2009", "authors": "Jenny R Finkel; Christopher D Manning"}, {"ref_id": "b11", "title": "Distributional representations for handling sparsity in supervised sequence-labeling", "journal": "", "year": "2009", "authors": "Fei Huang; Alexander Yates"}, {"ref_id": "b12", "title": "Open-domain semantic role labeling by modeling word spans", "journal": "", "year": "2010", "authors": "Fei Huang; Alexander Yates"}, {"ref_id": "b13", "title": "Instance weighting for domain adaptation in nlp", "journal": "", "year": "2007", "authors": "Jing Jiang; Chengxiang Zhai"}, {"ref_id": "b14", "title": "Fast exact inference with a factored model for natural language parsing", "journal": "", "year": "2003", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b15", "title": "Semi-supervised learning for natural language", "journal": "", "year": "2005", "authors": "Percy Liang"}, {"ref_id": "b16", "title": "Automatic domain adaptation for parsing", "journal": "", "year": "2010", "authors": "David Mcclosky; Eugene Charniak; Mark Johnson"}, {"ref_id": "b17", "title": "The necessity of combining adaptation methods", "journal": "", "year": "2010", "authors": "Michael Connor; Ming-Wei ; Chang ; Dan Roth"}, {"ref_id": "b18", "title": "The importance of syntactic parsing and inference in semantic role labeling", "journal": "Computational Linguistics", "year": "2008", "authors": "Vasin Punyakanok; Dan Roth; Wen Tau; Yih "}, {"ref_id": "b19", "title": "Combination strategies for semantic role labeling", "journal": "Journal of Artificial Intelligence Research", "year": "2007", "authors": "Mihai Surdeanu; Llu\u00eds M\u00e0rquez; Xavier Carreras; Pere R Comas"}, {"ref_id": "b20", "title": "Unsupervised semantic role labelling", "journal": "", "year": "2004", "authors": "Robert S Swier; Suzanne Stevenson"}, {"ref_id": "b21", "title": "A global joint model for semantic role labeling", "journal": "Computational Linguistics", "year": "2008", "authors": "Kristina Toutanova; Aria Haghighi; Christopher D Manning"}, {"ref_id": "b22", "title": "A multi-domain web-based algorithm for pos tagging of unknown words", "journal": "", "year": "2010", "authors": "Shulamit Umansky-Pesin; Roi Reichart; Ari Rappoport"}, {"ref_id": "b23", "title": "Sentence simplification for semantic role labeling", "journal": "", "year": "2008", "authors": "David Vickrey; Daphne Koller"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "VB, sing] [1, ., .]}. Consider the input sentence s s : Mr. X was entitled to a discount . with P R of {[\u22122, NP, X] [\u22121, AUX, was] [0, V, entitle] [1, PP, to][2, ., .]}. Since \u03c4.sp is a subsequence of the P R of s s , \u03c4 will apply to the predicate entitle of s s . The transformed sentence is: s tr = Replace(\u03c4.n s , \u03c4.n t , s s , \u03c4.s t ) = But Mr. X did not sing. with P R of {[\u22124, CC, But] [\u22123, NP, X] [\u22122, AUX, did] [\u22121, RB, not] [0, VB, sing] [1, ., .]}. If the SRL system assigns the semantic role of A0 to the phrase Mr. X of s tr , the semantic role of Mr. X in s s can be recovered through \u03c4.f since \u03c4.f (A0) = A2 = Label(\u22122, s s ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "PP, to][2, ., .]} but will not match the sentence The conference was entitled a big success. with P R = {[\u22122, NP, conference] [\u22121, AUX, was] [0, V, entitle] [1, S, success][2, ., .]} (mismatch position is bolded). The index of a phrase tuple cannot be \u03c6, only the head word or type can be \u03c6 and the rules with more \u03c6 strings in the source pattern are more general since they can match more sentences. Algorithm 1 GenerateRules 1: Input: predicate v, semantic role r, Training sentences D, SRL Model M 2: Output: set of rules R 3: R \u21d0 GetInitialRules(v, r, D, M )", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ": predicate v, semantic role r, Training sentences D, SRL-Model M 2: Output: Set of initial rules I 3: I \u21d0 \u03c6 4: T \u21d0 {s \u2208 D : length(s) <= e} 5: S \u21d0 {s \u2208 D : s has role r for predicate v} 6: M \u21d0 Set of all semantic roles 7: for each phrase p 1 in s 1 \u2208 S with gold label r for predicate v do 8:for each phrase p 2 in s 2 \u2208 T labeled as a core argument do9:if s 1 = s 2 and p 1 and p 2 have same phrase types then 10:\u03c4 \u21d0 empty rule 11: \u03c4.st \u21d0 s 2 , \u03c4.p \u21d0 v12:\u03c4.nt \u21d0 index of p 2 in P R of s 213:\u03c4.ns \u21d0 index of p 1 in P R of s 114:\u03c4.sp \u21d0 phrase tuples for phrases from p 1 to v and two phrases after v in P R of s 1 each sentence s 3 \u2208 D with predicate v do 17:if \u03c4.sp is a subsequence of P R of s 3 then 18:x \u21d0 replace(\u03c4.ns, \u03c4.nt, s 3 , \u03c4.st)19:annotate x with SRL using M 20: r 1 \u21d0 the gold standard semantic role of the phrase with index \u03c4.ns in P R of s 321:r 2 \u21d0 Label(\u03c4.nt, x)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "1: Input: a set of rules R 2: Output: a set of expanded rules E 3: E \u21d0 \u03c6 4: for each phrase tuple c in the source pattern of r \u2208 R do 5: if c is not the tuple for predicate then 6:create a new rule r with all components of r 7: mark the head word of c in the source pattern of r to \u03c6 rule r with all components of r 10: mark the phrase type of c in the source pattern of r to \u03c6", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Examples of Simplifications (Predicate is run)", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparing single parse system on Brown.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "where the authors retrained a SRL model using HMM features learned over unlabeled data of WSJ and Brown.", "figure_data": "SystemPRF1Retrain(Punyakanok et al., 2008)73.4 62.9 67.8\u00d7(Toutanova et al., 2008)NRNR68.8\u00d7(Surdeanu et al., 2007) (Cons) 78.2 62.1 69.2\u00d7(Surdeanu et al., 2007) (LBI)81.8 61.3 70.1\u00d7ADUT-combined74.3 67.0 70.5\u00d7(Huang and Yates, 2010)77.0 70.9 73.8"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Comparison of the multi parse system on Brown.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Ablation Study for ADUT-Charniak", "figure_data": "FrequencyBaseline Replacement of Predicate064.267.8less than 359.765.1less than 758.964.8all predicates65.566.78"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Performance on Infrequent Verbs for the Transformation of Replacement of Predicate", "figure_data": "8 Related Work"}], "formulas": [{"formula_id": "formula_0", "formula_text": "T 1 . Let T 2 = {d \u2208 T 1 : D i (d ) > D i (d)}.", "formula_coordinates": [2.0, 313.2, 517.1, 226.8, 24.18]}, {"formula_id": "formula_1", "formula_text": "n t = \u22123, p = entitle, sp = [\u22122, N P, \u03c6][\u22121, AU X, \u03c6][0, V, entitle][1, \u03c6, to] n s = \u22122, f = {<A0, A2>} \u222a {<Ai, Ai>|i = 0}. The P R of \u03c4.s t is {[\u22124, CC, But] [\u22123, NP, it] [\u22122, AUX, did] [\u22121, RB, not] [0,", "formula_coordinates": [5.0, 72.0, 99.53, 237.85, 64.39]}, {"formula_id": "formula_2", "formula_text": "P R = {[\u22122, NP, He] [\u22121, AUX, is] [0, V, entitle] [1,", "formula_coordinates": [5.0, 72.0, 385.21, 226.8, 23.36]}, {"formula_id": "formula_3", "formula_text": "c 1:M = arg max c 1:M \u2208F (P 1:M ) M i=1 score(S i = c i ).", "formula_coordinates": [6.0, 324.72, 219.14, 214.79, 30.23]}], "doi": ""}