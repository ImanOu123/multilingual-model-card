{"title": "Who Supported Obama in 2012? Ecological Inference through Distribution Regression", "authors": "Seth R Flaxman; Yu-Xiang Wang; Alexander J Smola", "pub_date": "", "abstract": "We present a new solution to the \"ecological inference\" problem, of learning individual-level associations from aggregate data. This problem has a long history and has attracted much attention, debate, claims that it is unsolvable, and purported solutions. Unlike other ecological inference techniques, our method makes use of unlabeled individual-level data by embedding the distribution over these predictors into a vector in Hilbert space. Our approach relies on recent learning theory results for distribution regression, using kernel embeddings of distributions. Our novel approach to distribution regression exploits the connection between Gaussian process regression and kernel ridge regression, giving us a coherent, Bayesian approach to learning and inference and a convenient way to include prior information in the form of a spatial covariance function. Our approach is highly scalable as it relies on FastFood, a randomized explicit feature representation for kernel embeddings. We apply our approach to the challenging political science problem of modeling the voting behavior of demographic groups based on aggregate voting data. We consider the 2012 US Presidential election, and ask: what was the probability that members of various demographic groups supported Barack Obama, and how did this vary spatially across the country? Our results match standard survey-based exit polling data for the small number of states for which it is available, and serve to fill in the large gaps in this data, at a much higher degree of granularity.", "sections": [{"heading": "INTRODUCTION", "text": "The name ecological inference refers to the idea of ecological correlations [28], that is correlations between variables observed for a group of individuals, as opposed to individual correlations, where the individuals are the unit of anal-ysis. The ecological inference problem has much in common with the \"modifiable areal unit problem\" [20] and Simpson's paradox. Simply put, it is the problem of inferring individual correlations from ecological correlations. This challenge arises in computational advertising, healthcare data, opinion survey data, and population health data, because in each case for privacy or cost reasons, we are missing individuallevel data, we have access to aggregate-level data, and we want to make individual-level predictions. One way to understand the reason it is called a \"problem\" is to consider a two-by-two contingency table, with unknown entries inside the table, and known marginals. As shown in the contingency table below, we might know that a certain electoral district's voting population is 43% men and 57% women and that in the last election, the outcome was 63% in favor of the Democratic candidate and 37% in favor of the Republican candidate. These percentages correspond to the numbers of individuals shown below: Is it possible to infer the Democrat Republican Men ? ? 1,500 Women ? ? 2,000 2,200 1,300 joint and thus conditional probabilities, for example can we ask, what was the Democratic candidate's vote share among women voters? It is clear that only very loose bounds can be placed on these probabilities without any more information.\nBased on the fact that rows and columns must sum to their marginals, we know, e.g., that the number of Democrats who are men is between 0 and 1,500. These types of deterministic bounds have been around since the 1950's, under the name the method of bounds [4]. What if we are given a set of electoral districts, where for each we know the marginals of the two-by-two contingency table, but none of the inner entries? Then, thinking statistically, we might be tempted to run a regression, predicting the electoral outcomes based on the gender breakdowns of the districts. But this approach, formalized as Goodman's method [8] a few years after the method of bounds was proposed, can easily lead us astray-there is not even a guarantee that outcomes be bounded between 0 and 1, and it ignores potentially useful information provided by deterministic bounds.\nWe review related work in Section 2 and provide the necessary background on kernel embeddings of distributions, distribution regression, and GP regression in Section 3. We formalize the ecological inference problem in Section 4 and propose our method in Section 5. We apply it to the case of the 2012 US presidential election in Section 6, comparing our results to survey-based exit polls.", "publication_ref": ["b27", "b19", "b3", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "The ecological inference problem has a long history of solutions, counter-solutions, and it is often taught with a note of grave caution and stark warnings that ecological inference is to be avoided at all costs, usually in favor of individual-level surveys. As with Simpson's paradox, it should come as no surprise that correlations at one level of aggregation can and do flip signs at other levels of aggregation. But abandoning all attempts at ecological inference in favor of surveys is not feasible or appropriate in many circumstances-relevant respondents are no longer alive to answer historical questions of interest; subjects are reluctant to answer questions about sensitive topics like drug usage or cheating-meaning social scientists have been hard-pressed and even discouraged from studying many interesting and important questions. Ecological inference problems appear in demography, sociology, geography, and political science, and-as discussed in [13]landmark legislation in the US such as the Voting Rights Act requires a solution to the ecological inference problem to understand racial voting patterns 1 .\nThis problem has attracted a variety of approaches over the years as summarized in [13], which also proposes a Bayesian statistical modeling framework incorporating the method of bounds (thus uniting the deterministic and probabilistic approaches). [13] sparked a renewed interest in ecological inference, much of which is summarized in [14]. A parametric Bayesian approach to this setting was proposed in [12] and a semiparametric approach was proposed in [23].\nOur method differs from existing methods in fours ways. First, it uses more information than is typically considered in a standard ecological regression setting: we assume that we have access to representative unlabeled individual-level data. In the voting example, this means having a sample of individual-level census records (\"microdata\") about each electoral district. Second, our method incorporates spatial variation. Spatial data is a common feature of ecological regressions (which, after all, usually have much to do with geography) but it is only very recently that ecological inference methods have begun to address spatial variation explicitly [14]. Third, while our method may be applied to the classic ecological inference problem of inferring individual level correlations from aggregate data, we propose that it is most well-suited to a related ecological problem, common in political science: inferring the unobserved behavior of subgroups based on the aggregate behavior of groups of which they are part. For our application, this means inferring the voting behavior of men and women separately by electoral district, given aggregate voting information by district. Finally, our work is nonparametric. Kernel embeddings are used to capture all moments of the probability distribution over covariates, and Gaussian process regression is used to non-parametrically model the dependence between predictors and labels.\nA related line of work, termed \"learning from label proportions\" by some authors [24,15,30,21], has the individuallevel goal in mind, and aims to build a classifier for individual instances based only on group level label proportions. While in principle, this approach could be used in our set-ting, since we are only interested in subgroup level predictions the extra task of estimating individual level predictions is probably not worth the effort considering we are working with n = 10 million individuals.\nOur method is based on recent advances in distribution regression [6,33], which we generalize to address the ecological inference case. Previous work on distribution regression has relied on kernel ridge regression, but we use Gaussian process (GP) regression instead, thus enabling us to incorporate spatial variation, learn kernel hyperparameters, and provide posterior uncertainty intervals, all in a fully Bayesian setting. For scalability (our experiments use n = 10 million individuals), we use a randomized explicit feature representation (\"FastFood\") [16] rather than the kernel trick.", "publication_ref": ["b12", "b12", "b12", "b13", "b11", "b22", "b13", "b23", "b14", "b30", "b20", "b5", "b33", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND", "text": "In this section we review kernel embeddings for probability distributions, distribution regression, FastFood, and Gaussian process (GP) regression.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Kernel embeddings of distributions", "text": "Kernel embeddings of distributions, e.g. [31,32,5], are a powerful class of reproducing kernel Hilbert space (RKHS) techniques that map joint, marginal and conditional probability distributions to vectors in a high (or infinite) dimensional feature space. Let \u03c6 : R n \u2192 H. It has been shown that if a kernel map \u03c6 is universal/characteristic (e.g. a Gaussian RBF kernel), then for iid samples x \u223c X, the mean embedding in feature space, denoted:\n\u00b5X = Ex\u223cX [\u03c6(x)](1)\ncompletely characterizes the distribution in the sense that any two distributions with a difference in any moment will be mapped to a different point in the Hilbert space. This result has been used in a variety of kernel-based statistical tests, including tests of independence and two-sample tests [10]. It is a key feature of our method, because it will allow us to link aggregate labels to individual-level data without throwing out any information.\nIn this work, we use the simple empirical mean estimator for the kernel mean:\n\u00b5X = 1 N j \u03c6(x j )(2)\nIt is shown in Smola et al. [31] that this plug-in estimator is a consistent, and it converges to \u00b5X with rate O(Rn(H) + 1/ \u221a n), where Rn(H) is the Rademacher complexity of the RKHS. As long as Rn(H) = O(n \u22121/2 ) we have the (optimal) parametric rate. Recent work has focused on improving this estimator using James-Stein shrinkage [17].", "publication_ref": ["b31", "b32", "b4", "b9", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Distribution regression", "text": "In this section, we formalize distribution regression, the task of learning a classifier or a regression function that maps probability distributions to labels. The problem is fundamentally challenging because we only observe the probability distributions through groups of samples from these distributions. Specifically, our dataset is structured as follows:\n{x j 1 } N 1 j=1 , y1 , {x j 2 } N 2 j=1 , y2 , . . . {x j n } Nn j=1 , yn(3)\nwhere group i has a single real-valued label yi and Ni individual observations (e.g. demographic covariates for Ni individuals) denoted\nx j i \u2208 R d .\nTo admit a theoretical analysis, it is assumed that the probability distributions themselves are drawn randomly from some unknown meta distribution of probability distributions. The intuition behind why distribution regression is possible is that if each group of samples are iid draws from a distribution which is itself an iid drawn from the meta distribution, then we will be able to learn.\nRecently, this \"two-stage sampled\" structure was analyzed, showing that a ridge regression estimator is consistent [33] with polynomial rate of convergence for almost any metadistribution of distributions that are sufficiently smooth. The basic approach is as follows: use the kernel mean estimator of Eq. (2) for each group separately to estimate:\n\u00b51 = 1 N1 N 1 j=1 \u03c6(x j 1 ), . . . , \u00b5n = 1 Nn Nn j=1 \u03c6(x j n ) (4)\nNext, use kernel ridge regression [29] to learn a function f :\ny = f ( \u00b5) + (5)\nwhere the objective is to minimize the L2 loss subject to a \"ridge\" complexity penalty weighted by a positive constant \u03bb:f\n= arg min f \u2208H f i [yi \u2212 f ( \u00b5i)] 2 + \u03bb f 2 H f(6)\nIn [33] a variety of kernels for f corresponding to the Hilbert space H f are considered. We follow the simplest choice of the linear kernel k( \u00b5i, \u00b5j) = \u00b5i, \u00b5j , motivated by the fact that we are already working in Hilbert space over the \u00b5i. Following the standard derivation of kernel ridge regression [29], we can find the function f in closed form for a new test group \u00b5 * :\nf (\u00b5 * ) = k * (K + \u03bbI) \u22121 [y1, . . . , yn] T (7\n)\nwhere k * = [ \u00b51, \u00b5 * , . . . , \u00b5n, \u00b5 * ] and K ab = \u00b5a, \u00b5 b . In practice, it is hard to know whether the conditions under which the proofs in these papers hold are met. As a partial remedy, our Bayesian approach allows us to quantify the degree of uncertainty in our posterior predictions. Also, as shown in the experiments, a useful diagnostic is to measure the distance between training and test distributions.", "publication_ref": ["b33", "b28", "b33", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "FastFood for explicit kernel expansion", "text": "Naively implementing distribution regression using the kernel trick is not scalable in the setting we consider: to compute just one entry in K requires computing\nK ab = \u00b5a, \u00b5 b = 1 NaN b j 1 j 2 k(x j 1 a , x j 2 b\n). This computation is O(N 2 ) (where we assume for simplicity Ni = N, \u2200i) so computing K is O(n 2 N 2 ). In our application, N \u2248 10 4 , so we need a much more scalable approach. Since we ultimately only need to work with the mean embeddings \u00b5i rather than the individual observations x j i , an explicit feature representation, even if it is very high-dimensional, will drastically reduce our computational costs.\nWe use an approximate kernel transformation called Fast-Food [16], which finds a d-dimensional approximation\u03c6(x) \u2208 R d of \u03c6(x) for every x. Here \u03c6 can be any radial basis function (RBF) kernel. Take Gaussian RBF kernel as an example, FastFood boils down to the following transformation due to Rahimi and Recht [25]:\n\u03c6(x) = p \u22121/2 exp(i[V x])\nwhere i is the imaginary unit of a complex number and V is a appropriately scaled p \u00d7 d Gaussian random matrix with p > d. FastFood allows us to approximately compute V x without explicitly construct V . In particular, Fast-Food transformation takes\nV = [V T 1 , V T 2 , ..., V T p/d ] T and each square d \u00d7 d matrix is given by: Vj = 1 \u03c3 \u221a d SHG\u03a0HB,\nHere S, B, G are diagonal random matrices (nonnegative scaling, Rademacher and Gaussian respectively), \u03a0 is a random permutation, and H is the Walsh-Hadamard matrix.\nEvery single one of these transformation can be computed in almost linear time. The whole transformation \u03c6(x) can be therefore computed in O(p log d) time. This is orders of magnitude faster than random kitchen sinks [25] which costs O(pd) per transformation or the kernel trick which needs to do an O(N 3 ) inversion of a dense N \u00d7 N kernel matrix.\nIt is shown in [16,Theorem 6] that for any x, x , \u03c6 (x),\u03c6(x ) converges to \u03c6(x), \u03c6(x ) with rate O( log(2/\u03b4) p 1/2 ) where \u03b4 is the failure probability. This can be viewed as a Johnson-Lindenstrauss transformation of an infinite dimensional space to a finite dimensional Euclidean space while preserving the angles and distances in the original space. While this is not a uniform convergence bound as with the random features in [25], the exponential tail enables us to simultaneously guarantee an exponential number of kernel evaluations via the union bound. Not surprisingly, it has been empirically shown to be comparable in accuracy and to approximate the kernel transformation for all data points quite well. For simplicity, from here onwards we will overload notation and refer to \u03c6(x) \u2208 R p as our feature mapping which is understood to be approximated with FastFood.", "publication_ref": ["b15", "b24", "b24", "b15", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Gaussian process regression", "text": "In this section, we briefly state the main results we need from Gaussian process regression [26], reviewing the wellknown connection between the posterior mean in GP regression and the kernel ridge regression estimator of Eq. (7).\nGiven observations (s1, y1), . . . (sn, yn) a Gaussian process prior on a function f where our model is y = f (s) + is written:\nf \u223c GP(0, k(s, s ))\nwith mean 0 and covariance function k. This implies that for a finite set of locations X = {s1, . . . , sn}, the distribution of\nf = [f (s1), . . . , f (sn)] is multivariate Gaussian: f \u223c N (0, K)(8)\nwhere Kij = k(si, sj). Notice that we have switched from a function f (s) to a vector f . This is because it is only formally correct to consider a probability distribution over the finite-dimensional vector f , not over the infinite dimensional function f (s). For a formal discussion see [35]. Conditional on the latent variable f , we have a Gaussian observation model:\nyi|f (si) \u223c N (0, \u03c3 2 ), \u2200i(9)\nfor variance parameter \u03c3 2 which can be thought of as measurement error (known as the \"nugget\" in geostatistics). For a fixed set of locations X, it is straightforward to sample f from its prior distribution in Eq. (8). Due to conjugacy, we can marginalize out f in closed form to find the distribution:\ny \u223c N (0, K + \u03c3 2 I)(10)\nIf we wish to make a prediction at a new location s * , the standard predictive equations for GP regression [26], derived by conditioning a multivariate Gaussian distribution, tell us:\ny * | s * , X, y \u223c N (k * (K +\u03c3 2 I) \u22121 y, k * * \u2212k * (K +\u03c3 2 I) \u22121 k * ) (11)\nwhere Kij = k(si, sj) and k * = [k(s1, s * ) . . . k(sn, s * )] and k * * = k(s * , s * ). Thus we have a way of combining a prior over f , parametrized by k(s, s ), with observed data to obtain a posterior distribution over a new prediction y * at a new location s * . This is a very powerful method, as it enables a fully Bayesian treatment of regression, a coherent approach to kernel learning through the marginal likelihood (for details see [26]), and posterior uncertainty intervals.\nWe can immediately see the connection between the kernel ridge regression estimator in Eq. ( 7) and the posterior mean of the GP in Eq. (11). (A superficial difference is that in Eq. ( 7) our predictors are \u00b5i while in Eq. (11) they are generic locations si, but this difference will go away in Section 5 when we propose using GP regression for distribution regression.) The predictive mean of GP regression is exactly equal to the kernel ridge regression estimator, with \u03c3 2 corresponding to \u03bb. In ridge regression, a larger penalty \u03bb leads to a smoother fit (equivalently, less overfitting), while in GP regression a larger \u03c3 2 favors a smoother GP posterior because it implies more measurement error. For a full discussion of the connections see [2, Sections 6.2.2-6.2.3].", "publication_ref": ["b25", "b6", "b35", "b7", "b25", "b25", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "ECOLOGICAL INFERENCE", "text": "In this section we state the ecological inference problem that we intend to solve. We use the motivating example of inferring Barack Obama's vote share by demographic subgroup (e.g. men versus women) in the 2012 US presidential election, without access to any individual-level labels. Vote totals by electoral precinct are publicly available, and these provide the labels in our problem. Predictors are in the form of demographic covariates about individuals (e.g. from a survey with individual level data like the census). The challenge is that the labels are aggregate, so it is impossible to know which candidate was selected by any particular individual. This explains the terminology: \"ecological correlations\" are correlations between variables which are only available as aggregates at the group level [28] We use the same notation as in Section 3.2. Let x j i \u2208 R d be a vector of covariates for individual i in region j. Let w j i be survey weights 2 . Let yi be labels in the form of twodimensional vectors (ki, ni) where ki is the number of votes received by Obama out of ni total votes in region i. Then our dataset is:\n{x j 1 } N 1 j=1 , y1 , {x j 2 } N 2 j=1 , y2 , . . . , {x j n } Nn j=1 , yn(12)\nWe will typically have a rich set of covariates available, in addition to the demographic variables we are interested in stratifying on, so the x j i will be high-dimensional vectors denoting gender, age, income, education, etc.\nOur task is to learn a function f from a demographic subgroup (which could be everyone) within region i to the probability that this demographic subgroup supported Obama, 2 Covariates usually come from a survey based on a random sample of individuals. Typically, surveys are reported with survey weights w j i for each individual to correct for oversampling and non-response, which must be taken into account for any valid inference (e.g. summary statistics, regression coefficients, standard errors, etc.).\ni.e. the number of votes this group gave Obama divided by the total number of votes in this group.", "publication_ref": ["b27", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "OUR METHOD", "text": "In this section we propose our new ecological inference method. Our approach is illustrated in a schematic in Figure 1 and formally stated in Algorithm 1. ?\nx 1 1 x 2 1 x 3 1 \u00b5 1 x 1 3 x 2 3\nFigure 1: Illustration of our approach. Labels y1, y2 and y3 are available at the group level giving Obama's vote share in regions 1, 2, and 3. Covariates are available at the individual level giving the demographic characteristics of a sample of individuals in regions 1, 2, and 3. We project the individuals from each group into feature space using a feature map \u03c6(x) and take the mean by group to find high-dimensional vectors \u00b51, \u00b52 and \u00b53, e.g. \u00b51 = 1 3 (\u03c6(x 1 1 ) + \u03c6(x 2 1 ) + \u03c6(x 3 1 )). Now our problem is reduced to supervised learning, where we want to learn a function f : \u00b5 \u2192 y. Once we have learned f we make subgroup predictions for men and women in region 3 by calculating mean embeddings for the men \u00b5 m 3 = 1 2 (\u03c6(x 3 3 ) + \u03c6(x 4 3 )) and women \u00b5 w 3 = 1 3 (\u03c6(x 1 3 ) + \u03c6(x 2 3 ) + \u03c6(x 5 3 )) and then calculating f (\u00b5 m\n3 ) and f (\u00b5 w 3 ). For a more rigorous description of our algorithm see Algorithm 1.\nRecall the two-stage distribution regression approach introduced in Section 3.2. Our method has a similar approach. To begin, we use FastFood as introduced in Section 3.3 with an RBF kernel to produce an explicit feature map \u03c6 and calculate the mean embeddings 3 , one for each region i, of Eq. (4) with survey weights:\n\u00b51 = j w j 1 \u03c6(x j 1 ) j w j 1 , . . . , \u00b5n = j w j n \u03c6(x j n ) j w j n(13)\n3 Distribution regression with explicit random features was previously considered in Oliva et al. [19] using Rahimi and Recht [25] to speed up an earlier distribution regression method based on kernel density estimation [22]. This approach has comparable statistical guarantees to distribution regression using RKHS-mean embeddings but inferior empirical performance [33]. As far as we are aware, using Fast-Food kernel mean embeddings for distribution regression is a novel approach.\nAlgorithm 1 Ecological inference algorithm Input: {(x j 1 , w j 1 )} N 1 j=1 , s1, y1 , . . . , {(x j n , w j n )} Nn j=1 , sn, yn 1: for i = 1 . . . n do 2:\nCalculate \u00b5i using Eq. (13) with FastFood.", "publication_ref": ["b18", "b24", "b21", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "3:", "text": "Calculate \u00b5 m i using Eq. (17) with FastFood. 4: end for 5: Learn hyperparameters\u03b8 = (\u03c3 2\nx , \u03c3 2 s , ) of the GP model specified by Eqs. ( 14)- (15) with observations yi at locations ( \u00b51, s1), . . . , ( \u00b5n, sn) using gradient descent and the Laplace approximation. 6: Make posterior predictions using\u03b8 at locations (\u00b5 m 1 , s1), . . . , (\u00b5 m n , sn) using the Laplace approximation.\nOutput: Posterior means and variances for y m 1 , . . . , y m n Next, instead of kernel ridge regression, we use GP regression. Recall that unlike in distribution regression our labels yi are given by vote counts (ki, ni). We use a Binomial likelihood as the observation model in GP regression (this is sometimes known as a logistic Gaussian process [27]). We transform each component of the latent real-valued vector f of Section 3.4 by the logistic link function \u03c3(f ) = 1 1+e \u2212f and we replace Eq. ( 9) with the following:\nki|f (xi) \u223c Binomial(ni, \u03c3(f (xi))) (14\n)\nwhere we use the formulation for the Binomial distribution of ni trials and probability of success \u03c3(f (xi)). This is the generalized linear model (GLM) specification for binary data, combining a Binomial distribution with logistic link function [3,Ch. 7].\nThe predictors in our GP are the mean embeddings \u00b51, . . . , \u00b5n. We also include spatial information in the form of 2-dimensional spatial coordinates si giving the centroid of region i. Putting these predictors together we adopt an additive covariance structure: f \u223c GP(0, \u03c3 2\nx \u00b5i, \u00b5j + ks(si, sj))\nWhere we have used a linear kernel between mean embeddings weighted by a variance parameter \u03c3 2\nx . Since the mean embeddings are already in feature space using the FastFood approximation to the RBF kernel, we are approximately using the RBF kernel. For the spatial coordinates we use the Mat\u00e9rn covariance function which is a popular choice in spatial statistics [11], with \u03bd = 3/2, length-scale and variance parameter \u03c3 2 s :\nk(s, s ) = \u03c3 2 s 1 + s \u2212 s \u221a 3 exp \u2212 s \u2212 s \u221a 3 (16)\nBy adding together the linear kernel between mean embeddings and the spatial covariance function, we allow for a smoothly varying surface over space and demographics. The intuition is that this additive covariance encourages predictions for regions which are nearby in space and have similar demographic compositions to be similar; predictions for regions which are far away or have different demographics are allowed to be less similar. GP regression with a spatial covariance function is equivalent to the spatial statistics technique of kriging-we are effectively smoothly interpolating y values over a very high dimensional space of predictors. Another way to think about additivity is that we are accounting for a spatially autocorrelated error structure in the predictions we get from covariates alone. (We also considered a multiplicative structure, which had slightly worse performance.)\nEq.s ( 14)-( 15) complete our hierarchical model specification. For non-Gaussian observation models like Eq. ( 14), the posterior prediction in Eq. ( 11) is no longer available in closed form due to non-conjugacy. We follow the standard approach for GP classification and logistic Gaussian processes and use the Laplace approximation [36,27]. The Laplace approximation gives an approximate posterior distribution for f , from which we can calculate a posterior distribution over the ki of Eq. ( 14) as explained in detail in [26,Section 3.4.2]. The Laplace approximation also allows us to calculate the marginal likelihood, which is the probability of the observed data, integrating out f . To learn \u03c3 2\nx , \u03c3 2 s , and , we use gradient ascent to maximize the log marginal likelihood.\nOnce we have learned the best set of hyperparameters for our model we can make predictions for any demographic subgroup of interest. To predict the fraction of men who voted for Obama, we create new mean embedding vectors by gender and region, modifying Eq. ( 13):\n\u00b5 m i = j m w j 1 \u03c6(x j 1 ) j m w j 1 , \u2200i(17)\nwhere j m are the indices of the observations of men in region i and \u00b5 m i is the mean embedding of the covariates for the men in region i. We then make posterior predictions using the Laplace approximation as above at these new genderregion predictors. Notice that for a new \u00b5 * this requires calculating k * = [k1 * , k2 * , . . . , kn * ] of Eq. ( 11) where ki * = \u03c3 2\nx \u00b5i, \u00b5 * + ks(si, s * ) using Eq. (15). Thus new predictions will be similar to existing predictions in regions with similar covariates and they will be similar to existing predictions at the same (and nearby) locations.\nOur algorithm is stated in Algorithm 1. We now analyze its complexity. Lines 2-3 are calculated by streaming through the data for individuals. For each individual, calculating the FastFood feature transformation \u03c6(x j i ) takes O(p log d) where x j i \u2208 R d and \u03c6(x j i ) \u2208 R p . To save memory, there's no need to store each \u03c6(x j i ). We simply update the weighted average \u00b5i by adding w i j \u03c6(x j i ) to it. Notice that the demographic subgroup considered in line 3 is simply a subset of the observations calculated in line 2, so there is no added cost to calculate the \u00b5 m i or indeed a set of \u00b5 m 1 i , . . . , \u00b5 mq i for q different demographic subgroups of interest. Overall, if we have N individuals the for loop takes time O(N p log d). Usually p N and d N so this is practically linear and trivially parallelizable.\nOn line 5 to learn the hyperparameters in the GP regression requires calculations involving the covariance matrix K \u2208 R n\u00d7n . Each entry in K requires computing a dot product \u00b5i, \u00b5j which takes O(p) and it requires computing the Mat\u00e9rn kernel for the spatial locations, which is a fast arithmetic calculation. Once we have K, the Laplace approximation is usually implemented with Cholesky decompositions for numerical reasons. The runtime of computing the marginal likelihood and relevant gradients is O(n 3 ) [26], and gradient ascent usually takes less than a hundred steps to converge. Posterior predictions on line 6 require calculating k * \u2208 R 1\u00d7n for each \u00b5 m i so this is O(n 2 ). Reusing the Cholesky decompositions above means predictions can be made in O(n 2 ). GP regression requires O(n 2 ) storage.\nOverall, we expect n N , so our algorithm is practically O(N ), with little extra computational cost arising from the GP regression as compared to the work of streaming through all the observations. The N observations do not need to be stored in memory, so the overall memory complexity is only O(n 2 ).", "publication_ref": ["b14", "b26", "b2", "b10", "b36", "b26", "b25", "b14", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "In this section, we describe our experimental evaluation, using data from the 2012 US Presidential election, and compare our results to survey-based exit polls, which are only available for the 18 states for which large enough samples were obtained. Our method enables us to fill in the full picture, with much finer-grained spatial estimation and results for a much richer variety of demographic variables. This demonstration shows the applicability of our new method to a large body of political science literature (see, e.g. [7]) on voting patterns by demographics and geography. Because voting behavior is unobservable and due to the ecological inference problem, previous work has been mostly based on exit polls or opinion polls.\nWe obtained vote totals for the 2012 US Presidential Election at the county level 4 . Most voters chose to either reelect President Barack Obama or vote for the Republican party candidate, Mitt Romney. A small fraction of voters (< 2% across the country) chose a third party candidate. Separately, we obtained data from the US Census, specifically the 2006-2010 American Community Survey's Public Use Microdata Sample (PUMS). The American Community Survey is an ongoing survey that supplements the decennial US census and provides demographically representatives individual-level observations. PUMS data is coded by public use microdata areas (PUMAs), contiguous geographic regions of at least 100,000 people, nested within states. We used the 5-year PUMS file (rather than a 1-year or 3-year sample) because it contains a larger sample and thus there is less censoring for privacy reasons. To merge the PUMS data with the 2012 election results, we created a mapping between counties and PUMAs 5 , merging individual-level census data and aggregating vote totals as necessary to create larger geographic regions for which the census data and electoral data coincided. The mapping between PUMAs and counties is many-to-many, so we were effectively finding the connected components. Since counties and PUMAs do not cross state borders, none of the geographic regions we created cross state borders. An example is shown in Figure 2.\nIn total, we ended up with 837 geographic regions ranging from Orleans Parish in New Orleans, which voted 91% for Barack Obama to Davis County, a suburb of Salt Lake City, Utah which voted 84% for Mitt Romney. For the census data, we excluded individuals under the age of 18 (voting age in the US) and non-citizens (only citizens can vote in presidential elections). There were a total of 10,787,907 individual-level observations, or in other words, almost 11 million people included in the survey. The mean number of people per geographic region was 12,812 with standard deviation 21,939.\nThere were 223 variables in the census data, including both categorical variables such as race, occupation, and educational attainment and real valued variables such as in- come in past 12 months (in dollars) and travel time to work (in minutes). We divided the real-valued variables by their standard deviation to put them all on the same scale. For the categorical variables with D categories, we converted them into D dimensional 0/1 indicator variables, i.e. for the variable \"when last worked\" with categories 1 = \"within the past 12 months,\" 2 = \"1-5 years ago,\" and 3 = \"over 5 years ago or never worked\" we mapped 1 to [1 0 0] T , 2 to [0 1 0] and 3 to [0 0 1]. Putting together the indicator variables and real-valued variables, we ended up with 3,251 variables total. For every single individual-level observation, we used FastFood with an RBF kernel to generate a 4,096-dimensional feature representation. Using Eq. (13) we calculated the weighted mean embedding for each region. The result was a set of 837 vectors which were 4,096-dimensional.\nWe treated the vote totals for Obama and Romney as is, discarded the remaining third party votes as the exit polls we use for validation did not report third party votes. Thus for each region, we had a positive integer valued 2-dimensional label giving the number of votes for Obama and the total number of votes.\nWe focused on the ecological inference problem of predicting Obama's vote share by the following demographic groups: women, men, income \u2264 US$50,000 per year, income between $50,000 and $100,000 per year, income \u2265 100,000 per year, ages 18-29, 30-44, 45-64, and 64 plus. For each region, we used the strategy outlined above, restricting our census sample to only those observations matching the subgroup of interest and creating new mean embedding predictors as in Eq. (17), \u00b5 subgroup i . We made predictions for each region-demographic pair. Note that we have made our task harder than necessary to demonstrate our method; we could have trained our model using the exit polling data, where available, and we would certainly recommend practitioners use all available data to get the best possible estimates.\nAll of our models were fit using the GPstuff package with scaled conjugate gradient optimization and the Laplace approximation [34]. Since n N , the time required to fit the GP model and make predictions is much less than the time required to preprocess the data to create the mean embeddings at the beginning of Algorithm 1.", "publication_ref": ["b6", "b3", "b34"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "RESULTS", "text": "We learned the following hyperparameters for our GP: \u03c3 2 s = 0.18, = 7.92, and \u03c3 2 x = 4.56. The \u03c3 2 parameters can be roughly interpreted as the \"fraction of variance explained\" so the fact that \u03c3 2\nx is much larger than \u03c3 2 s means that the demographic covariates encoded in the mean embedding are much more important to the model than the spatial coordinates. The length-scale for the Mat\u00e9rn kernel is a little more than half the median distance between locations, which indicates that it is performing a reasonable degree of smoothing. We used 10-fold crossvalidation to evaluate our model and ensure that it was not overfitting, an important consideration as generalization performance is critical. The root mean squared error of the model was 2.5 and the mean log predictive density was -1.9. Predictive density is a useful measure because it takes posterior uncertainty intervals into account. For comparison, predicting the national average of Obama receiving 51.1% of the vote in every location has a root mean squared error of 8.3. As a sensitivity analysis, we also considered a multiplicative model, for which the performance was comparable.\nTo validate our models, we compared to the 2012 exit polls, conducted by Edison Research for a consortium of news organizations. National results were based on interviews with voters in 350 randomly chosen precincts, and state results in 18 states were based on interviews in 11 to 50 random precincts. In these interviews, conducted as voters left polling stations, voters were asked who they voted for and a variety of demographic questions about themselves. Bias due to factors such as unrepresentativeness of the sampled precincts and inadequate coverage of early or absentee voters could be an issue [1]. The national results had a margin of error (corresponding to a 95% uncertainty interval) of 4 percentage points 6 and the state results had a margin of error of between 4 and 5 percentage points [18]. For comparing to the 18 state-level exit polls, we aggregated our geographic regions by state, weighting by subgroup population.\nAs a preview of our results by gender, income, and age, and to get an idea of the power of our method, Figure 3 shows four maps visualizing Obama's support among women and men. In Figures 3a-3b, we show the results from the exit polls, at the state level, for only 18 states. In Figures 3c-3d we fill in the missing picture, providing estimates for 837 different regions. We compare to competing methods below for national-level gender estimates. In the supplementary materials, we consider the non-binary demographic covariates age and income and the case of regional-level estimates, which present a difficulties for the competing methods.", "publication_ref": ["b5", "b17"], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Gender", "text": "Voting by gender is shown in Figure 4, where we compare our results to the exit poll results. The fit is quite good, with correlations equal to 0.96 for men and 0.94 for women. The inference that we are most interested in is the gender", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "a racially stratified homogenous precinct approach", "journal": "", "year": "2006", "authors": ""}, {"ref_id": "b1", "title": "An introduction to support vector machines and other kernel-based learning methods", "journal": "Cambridge university press", "year": "2000", "authors": "N Cristianini; J Shawe-Taylor"}, {"ref_id": "b2", "title": "An introduction to generalized linear models. Chapman & Hall texts in statistical science", "journal": "", "year": "2002", "authors": "A Dobson"}, {"ref_id": "b3", "title": "An alternative to ecological correlation", "journal": "American Sociological Review", "year": "1953", "authors": "O B Duncan; D B "}, {"ref_id": "b4", "title": "Kernel bayes' rule", "journal": "", "year": "2011", "authors": "K Fukumizu; L Song; A Gretton"}, {"ref_id": "b5", "title": "Multi-instance kernels", "journal": "", "year": "2002", "authors": "T G\u00e4rtner; P A Flach; A Kowalczyk; A J Smola"}, {"ref_id": "b6", "title": "Red State, Blue State, Rich State, Poor State: Why Americans Vote the Way They Do", "journal": "Princeton University Press", "year": "2008-08", "authors": "A Gelman; D Park; B Shor; J Bafumi; J Cortina"}, {"ref_id": "b7", "title": "Some alternatives to ecological correlation", "journal": "American Journal of Sociology", "year": "1959", "authors": "L A Goodman"}, {"ref_id": "b8", "title": "Covariate shift by kernel mean matching. Dataset shift in machine learning", "journal": "", "year": "2009", "authors": "A Gretton; A Smola; J Huang; M Schmittfull; K Borgwardt; B Sch\u00f6lkopf"}, {"ref_id": "b9", "title": "A kernel two-sample test", "journal": "JMLR", "year": "2012", "authors": "A Gretton; K M Borgwardt; M J Rasch; B Sch\u00f6lkopf; A Smola"}, {"ref_id": "b10", "title": "A bayesian analysis of kriging", "journal": "Technometrics", "year": "1993", "authors": "M S Handcock; M L Stein"}, {"ref_id": "b11", "title": "Improving ecological inference using individual-level data", "journal": "Statistics in medicine", "year": "2006", "authors": "C Jackson; N Best; S Richardson"}, {"ref_id": "b12", "title": "A Solution to the Ecological Inference Problem", "journal": "Princeton University Press", "year": "1997-03", "authors": "G King"}, {"ref_id": "b13", "title": "Ecological inference: New methodological strategies", "journal": "Cambridge University Press", "year": "2004", "authors": "G King; M A Tanner; O Rosen"}, {"ref_id": "b14", "title": "Learning about individuals from group statistics", "journal": "", "year": "2005", "authors": "H Kueck; N De Freitas"}, {"ref_id": "b15", "title": "Fastfood: approximating kernel expansions in loglinear time", "journal": "", "year": "2013", "authors": "Q Le; T Sarlos; A Smola"}, {"ref_id": "b16", "title": "Kernel mean estimation and stein effect", "journal": "", "year": "2014", "authors": "K Muandet; K Fukumizu; B Sriperumbudur; A Gretton; B Schoelkopf"}, {"ref_id": "b17", "title": "President exit polls -election", "journal": "New York Times", "year": "2012-02", "authors": ""}, {"ref_id": "b18", "title": "Fast distribution to real regression", "journal": "", "year": "2014-04-22", "authors": "J B Oliva; W Neiswanger; B P\u00f3czos; J G Schneider; E P Xing"}, {"ref_id": "b19", "title": "Ecological fallacies and the analysis of areal census data", "journal": "Environment and Planning A", "year": "1984", "authors": "S Openshaw"}, {"ref_id": "b20", "title": "almost) no label no cry", "journal": "Curran Associates, Inc", "year": "2014", "authors": "G Patrini; R Nock; T Caetano; P Rivera"}, {"ref_id": "b21", "title": "Distribution-free distribution regression", "journal": "", "year": "2013", "authors": "B Poczos; A Singh; A Rinaldo; L Wasserman"}, {"ref_id": "b22", "title": "Aggregate data studies of disease risk factors", "journal": "Biometrika", "year": "1995", "authors": "R L Prentice; L Sheppard"}, {"ref_id": "b23", "title": "Estimating labels from label proportions", "journal": "JMLR", "year": "2009", "authors": "N Quadrianto; A J Smola; T S Caetano; Q V Le"}, {"ref_id": "b24", "title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "journal": "", "year": "2008", "authors": "A Rahimi; B Recht"}, {"ref_id": "b25", "title": "Gaussian processes for machine learning", "journal": "", "year": "2006", "authors": "C E Rasmussen; C K Williams"}, {"ref_id": "b26", "title": "Laplace approximation for logistic gaussian process density estimation and regression", "journal": "Bayesian Analysis", "year": "2014", "authors": "J Riihim\u00e4ki; A Vehtari"}, {"ref_id": "b27", "title": "Ecological correlations and the behavior of individuals", "journal": "American Sociological Review", "year": "1950", "authors": "W S Robinson"}, {"ref_id": "b28", "title": "Ridge regression learning algorithm in dual variables", "journal": "", "year": "", "authors": "C Saunders; A Gammerman; V Vovk"}, {"ref_id": "b29", "title": "", "journal": "", "year": "1998", "authors": "Morgan Kaufmann"}, {"ref_id": "b30", "title": "Collective graphical models", "journal": "", "year": "2011", "authors": "D R Sheldon; T G Dietterich"}, {"ref_id": "b31", "title": "A hilbert space embedding for distributions", "journal": "Springer", "year": "2007", "authors": "A Smola; A Gretton; L Song; B Sch\u00f6lkopf"}, {"ref_id": "b32", "title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "journal": "ACM", "year": "2009", "authors": "L Song; J Huang; A Smola; K Fukumizu"}, {"ref_id": "b33", "title": "Two-stage Sampled Learning Theory on Distributions", "journal": "Artificial Intelligence and Statistics (AISTATS)", "year": "2015-02", "authors": "Z Szabo; A Gretton; B Poczos; B Sriperumbudur"}, {"ref_id": "b34", "title": "Gpstuff: Bayesian modeling with gaussian processes", "journal": "JMLR", "year": "2013", "authors": "J Vanhatalo; J Riihim\u00e4ki; J Hartikainen; P Jyl\u00e4nki; V Tolvanen; A Vehtari"}, {"ref_id": "b35", "title": "Spline models for observational data", "journal": "", "year": "1990", "authors": "G Wahba"}, {"ref_id": "b36", "title": "Bayesian classification with gaussian processes. Pattern Analysis and Machine Intelligence", "journal": "IEEE Transactions on", "year": "1998", "authors": "C K Williams; D Barber"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "4Figure 2 :2Figure2: Election outcomes were available for the 67 counties in Florida shown in (a). Demographic data from the American Community Survey was available for 127 public use microdata areas (PUMAs) in Florida, which sometimes overlapped parts of multiple counties and sometimes contained multiple counties. We merged counties and PUMAs as described in text to create a set of disjoint regions with the result of 37 electoral regions as shown in (b).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: Support for Obama among women (a) and men (b) in the 18 states for which exit polling was done; due to cost, no representative data was collected for the majority of states or for regions smaller than states. Support for Obama among women (c) and men (d) in 837 different regions as inferred using our ecological regression method.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u00b5X = Ex\u223cX [\u03c6(x)](1)", "formula_coordinates": [2.0, 401.05, 365.08, 154.87, 8.97]}, {"formula_id": "formula_1", "formula_text": "\u00b5X = 1 N j \u03c6(x j )(2)", "formula_coordinates": [2.0, 399.53, 485.25, 156.39, 24.07]}, {"formula_id": "formula_2", "formula_text": "{x j 1 } N 1 j=1 , y1 , {x j 2 } N 2 j=1 , y2 , . . . {x j n } Nn j=1 , yn(3)", "formula_coordinates": [2.0, 340.74, 668.73, 215.18, 11.87]}, {"formula_id": "formula_3", "formula_text": "x j i \u2208 R d .", "formula_coordinates": [2.0, 394.32, 708.98, 33.76, 11.88]}, {"formula_id": "formula_4", "formula_text": "\u00b51 = 1 N1 N 1 j=1 \u03c6(x j 1 ), . . . , \u00b5n = 1 Nn Nn j=1 \u03c6(x j n ) (4)", "formula_coordinates": [3.0, 80.48, 198.49, 212.43, 27.19]}, {"formula_id": "formula_5", "formula_text": "y = f ( \u00b5) + (5)", "formula_coordinates": [3.0, 148.19, 252.79, 144.71, 8.97]}, {"formula_id": "formula_6", "formula_text": "= arg min f \u2208H f i [yi \u2212 f ( \u00b5i)] 2 + \u03bb f 2 H f(6)", "formula_coordinates": [3.0, 93.08, 305.58, 199.83, 19.94]}, {"formula_id": "formula_7", "formula_text": "f (\u00b5 * ) = k * (K + \u03bbI) \u22121 [y1, . . . , yn] T (7", "formula_coordinates": [3.0, 102.95, 409.81, 186.03, 10.63]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [3.0, 288.99, 411.47, 3.92, 8.97]}, {"formula_id": "formula_9", "formula_text": "K ab = \u00b5a, \u00b5 b = 1 NaN b j 1 j 2 k(x j 1 a , x j 2 b", "formula_coordinates": [3.0, 54.99, 543.46, 241.39, 23.58]}, {"formula_id": "formula_10", "formula_text": "\u03c6(x) = p \u22121/2 exp(i[V x])", "formula_coordinates": [3.0, 125.41, 708.92, 95.87, 10.63]}, {"formula_id": "formula_11", "formula_text": "V = [V T 1 , V T 2 , ..., V T p/d ] T and each square d \u00d7 d matrix is given by: Vj = 1 \u03c3 \u221a d SHG\u03a0HB,", "formula_coordinates": [3.0, 316.81, 96.88, 239.11, 47.09]}, {"formula_id": "formula_12", "formula_text": "f \u223c GP(0, k(s, s ))", "formula_coordinates": [3.0, 399.18, 496.96, 74.37, 8.97]}, {"formula_id": "formula_13", "formula_text": "f = [f (s1), . . . , f (sn)] is multivariate Gaussian: f \u223c N (0, K)(8)", "formula_coordinates": [3.0, 316.81, 532.0, 239.11, 24.63]}, {"formula_id": "formula_14", "formula_text": "yi|f (si) \u223c N (0, \u03c3 2 ), \u2200i(9)", "formula_coordinates": [3.0, 388.56, 637.71, 167.36, 10.63]}, {"formula_id": "formula_15", "formula_text": "y \u223c N (0, K + \u03c3 2 I)(10)", "formula_coordinates": [3.0, 397.88, 708.92, 158.04, 10.63]}, {"formula_id": "formula_16", "formula_text": "y * | s * , X, y \u223c N (k * (K +\u03c3 2 I) \u22121 y, k * * \u2212k * (K +\u03c3 2 I) \u22121 k * ) (11)", "formula_coordinates": [4.0, 53.8, 91.84, 239.11, 20.88]}, {"formula_id": "formula_17", "formula_text": "{x j 1 } N 1 j=1 , y1 , {x j 2 } N 2 j=1 , y2 , . . . , {x j n } Nn j=1 , yn(12)", "formula_coordinates": [4.0, 73.38, 566.28, 219.52, 11.87]}, {"formula_id": "formula_18", "formula_text": "x 1 1 x 2 1 x 3 1 \u00b5 1 x 1 3 x 2 3", "formula_coordinates": [4.0, 356.58, 248.77, 83.02, 69.46]}, {"formula_id": "formula_19", "formula_text": "\u00b51 = j w j 1 \u03c6(x j 1 ) j w j 1 , . . . , \u00b5n = j w j n \u03c6(x j n ) j w j n(13)", "formula_coordinates": [4.0, 337.3, 607.05, 218.62, 27.0]}, {"formula_id": "formula_20", "formula_text": "ki|f (xi) \u223c Binomial(ni, \u03c3(f (xi))) (14", "formula_coordinates": [5.0, 106.49, 326.41, 182.33, 8.97]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [5.0, 288.82, 326.41, 4.09, 8.97]}, {"formula_id": "formula_23", "formula_text": "k(s, s ) = \u03c3 2 s 1 + s \u2212 s \u221a 3 exp \u2212 s \u2212 s \u221a 3 (16)", "formula_coordinates": [5.0, 59.5, 552.42, 233.4, 22.09]}, {"formula_id": "formula_24", "formula_text": "\u00b5 m i = j m w j 1 \u03c6(x j 1 ) j m w j 1 , \u2200i(17)", "formula_coordinates": [5.0, 386.2, 290.37, 169.72, 27.0]}], "doi": "10.1145/2783258.2783300"}