{"title": "Global Stereo Reconstruction under Second Order Smoothness Priors", "authors": "O J Woodford; P H S Torr; I D Reid; A W Fitzgibbon", "pub_date": "", "abstract": "Second-order priors on the smoothness of 3D surfaces are a better model of typical scenes than first-order priors. However, stereo reconstruction using global inference algorithms, such as graph-cuts, has not been able to incorporate second-order priors because the triple cliques needed to express them yield intractable (non-submodular) optimization problems. This paper shows that inference with triple cliques can be effectively optimized. Our optimization strategy is a development of recent extensions to \u03b1-expansion, based on the \"QPBO\" algorithm [5,14,26]. The strategy is to repeatedly merge proposal depth maps using a novel extension of QPBO. Proposal depth maps can come from any source, for example fronto-parallel planes as in \u03b1-expansion, or indeed any existing stereo algorithm, with arbitrary parameter settings. Experimental results demonstrate the usefulness of the second-order prior and the efficacy of our optimization framework. An implementation of our stereo framework is available online [34].", "sections": [{"heading": "Introduction", "text": "Multiple-view dense stereo has made considerable progress in recent years, in part because the problem can be cast in an energy minimization framework for which there exist inference algorithms that can efficiently find good (if not always global) minima. Algorithms based on graph cuts, in particular, can incorporate visibility reasoning as well as smoothness priors into the estimation of depth maps. However, the smoothness priors used in graph-cut based estimates have to date been first-order priors, which favor low-curvature fronto-parallel surfaces-indeed, the prior is maximized by fronto-parallel planes. Even in man-made scenes, this is far from accurate, as illustrated in figure 1, and leads to inaccurate depth estimates. It has long been known [3,13,30] that a second order smoothness prior can better model the real world, but it has not yet been possible to combine visibility reasoning and second-order smoothness in an optimization framework which finds good op-  tima.\nThe main contribution of this paper is to introduce an effective optimization strategy for stereo reconstruction with triple cliques. This means that visibility reasoning and second-order priors can be combined for the first time. We show that this algorithm produces excellent results both on the Middlebury test set [27] and on real-world examples with curved surfaces.", "publication_ref": ["b2", "b12", "b29", "b26"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Background", "text": "Second order smoothness priors for stereo reconstruction have a long history. Grimson [13] and Terzopoulos [30] both proposed second order priors for stereo in the early 1980s, in the form of the thin plate model. This was extended to the piecewise second order \"weak plate\" model by Blake and Zisserman [3], and recently Ishikawa and 1 978-1-4244-2243-2/08/$25.00 \u00a92008 IEEE Geiger [16] have argued that second order priors may be closer to those that the human visual system appears to use. Second order priors penalize large second derivatives of depth (or disparity). When expressed as an energy minimization problem, the resulting energy has almost invariably been minimized by what Scharstein et al. [27] describe as \"local methods\": gradient descent [28] or PDE-based techniques such as level sets [11]. However, local methods struggle with (a) the long-range interactions associated with occlusion reasoning and (b) weak or multi-modal data likelihoods.\nThe introduction of \"global\" methods for energy minimization gave a considerable improvement in stereo reconstruction performance. Methods such as graph cuts [8,20,31] can find strong (although not global) optima of energies with long-range interactions. However, these methods have not previously incorporated second-order smoothness terms. This is despite the fact that the graph constructions necessary to include these terms are known [21]: the triple cliques which represent the second order terms are decomposed into several pairwise cliques and auxiliary nodes are added. Boykov and Veksler [7] say that \"the allowed form of these triple cliques is very limited\", but the construction is valid for any energy-the limitation is that the resulting graph is non-submodular, meaning that efficient methods of finding the global optimum were not known. In this paper we adapt a newly introduced optimizer [26], the \"QPBO\" method of Boros, Hammer and co-workers [5,14], to compute good optima of the energy. Although these are not guaranteed to be global optima, our experiments show that by careful parametrization of the problem, good local optima can be found reliably.\nPrevious stereo algorithms have implemented approximations to second order smoothness priors. Ogale and Aloimonos [25] propose a \"slanted scanline\" algorithm, in which straight, 3d line segments are fitted to 2d image scanlines using an optimization method. This approach models visibility using an explicit uniqueness constraint, but the method is limited to image pairs, and the between-scanline optimization is local. Li and Zucker [23] introduce priors on slanted and curved surfaces, encouraging the second and third derivatives of depth to be zero. This novelly allows for curved surfaces in the solution, as shown in figure 1(c), and significantly improves on the fronto-parallel assumption on scenes where that assumption is violated. However their algorithm precomputes local surface normals and in fact optimizes a first-order prior on the normals, rather than a second-order prior on the disparities. Indeed, they discuss the global optimization of a second order prior, and conclude that this \"makes the problem computationally infeasible\".\nA related class of methods is \"segment-based\" stereo. Early examples of this technique were proposed by Birch-field and Tomasi [2] and Tao et al. [29]. While their two approaches differ somewhat, both enforce the constraint that segmented regions of the image be planar, a trait common to the sequence of algorithms that succeeded Tao's [4,15,17,35], which have shown excellent results on the \"Middlebury \" test set [27]. They all share the same three stage process-produce an over-segmentation of the reference image, generate a set of planar hypotheses for each segment, and optimize over the hypothesesdiffering only in their implementation of each stage. Lin and Tomasi [24] explicitly minimize an energy including a second order prior, but are restricted to a local gradientbased optimization strategy where segmentation and depth estimation are interleaved. In many of these segment-based methods the assumption of the local planarity of scenes is not a general smoothness prior, but a hard constraint, which does not permit curved surfaces even when the data supports this. In contrast, we show that the method proposed here is an effective regularizer over both planar and curved surfaces.", "publication_ref": ["b12", "b29", "b2", "b15", "b26", "b27", "b10", "b7", "b19", "b30", "b20", "b6", "b25", "b4", "b13", "b24", "b22", "b1", "b28", "b3", "b14", "b16", "b34", "b26", "b23"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Problem statement", "text": "Before describing this paper's main contribution, let us define the stereo problem, and the energy formulation that we propose to minimize.\nThe input is a set of N + 1 images {I i } N i=0 . The goal is to determine the dense disparity map, D, of one reference view, say I 0 . A 2D vector, x, denotes a pixel location in the reference view, the color of which is written as I 0 (x), and the corresponding disparity is D(x). We are also given projection functions {\u03c0 i (x, d) :\nR 2 \u2192 R 2 } N i=1\n, where \u03c0 i (x, d) is the projection into image i of the 3D point corresponding to disparity (1/depth) d in front of pixel x in the reference view. For a rectified stereo pair, N = 1 and only \u03c0 1 is required, with the simple definition \u03c0\n1 (x, d) = x + [d, 0].\nThe abbreviation I \u03c0 i (x, d) = I i (\u03c0 i (x, d)) will be used to reduce clutter, and may be read as \"the color of the pixel corresponding to x in image i if the disparity at x is d\".\nThe energy function to be minimized is a function of the disparity map E(D), and is the sum of two terms: photoconsistency E photo , which incorporates geometrical visibility reasoning, and smoothness E smooth , as follows.\nE(D) = E photo (D) + E smooth (D).(1)\nThe components of the energy shall now be described.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data term", "text": "The data term in this paper is a standard photoconsistency term of the form\nE photo (D) = x N i=1 f I \u03c0 i (x, D(x)) \u2212 I 0 (x), V i x (2)\nwhere V i x is a visibility flag, to be discussed below, indicating whether the 3D point defined by (x, D(x)) is visible in image i. Given V i\nx , the consistency metric f is defined as\nf (\u2206I, V ) = \u03c1 d (\u2206I) if V = 1 \u03bd if V = 0 (3)\nHere \u03bd is the penalty cost paid by occluded pixels, and \u03c1 d is a robust measure of color difference, defined by\n\u03c1 d (I) = \u2212 log 1 + exp(\u2212 I 2 /\u03c3 d ) ,(4)\nwhere \u03c3 d is set from the noise level in the sequence. The visibility flag V i x adds nonlocal terms to the energy, making global optimization of this energy difficult, even before priors are incorporated. It is more correctly written V i (x, D), indicating the dependence on many entries of the disparity map D. We use the asymmetrical occlusion model of Wei and Quan [31], which reduces the complexity of the symmetrical multi-view occlusion model introduced in [20] from O(N ) to O(1). This model adds pairwise terms to the energy, between nodes which are on the same epipolar lines. However the approximations made in [31] in order to ensure submodularity are unnecessary, given our optimization framework. As shall be seen in \u00a73, optimization of the continuous E(D) is expressed as a sequence of binary subproblems. It then becomes valuable to compute the decomposition into pairwise terms independently for each binary subproblem. This confers the advantage that the that the number of potentially occluding pixels is relatively small for each such subproblem, so the cost of including visibility is relatively low.", "publication_ref": ["b30", "b19", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Surface smoothness", "text": "The smoothness prior places a cost, \u03c1 s (\u2022), on the smoothness S(\u2022) of a neighborhood, N , of pixels. In addition, a per-neighborhood conditional random field (CRF) weight W (N ), as discussed in \u00a74, will be applied. E smooth is the sum of smoothness costs over a defined set of pixel neighborhoods, N, thus\nE smooth (D) = N \u2208N W (N )\u03c1 s (S(N , D)). (5\n)\nwith \u03c1 s (s) = min(\u03c3 s , |s|), the truncated linear kernel. Second-order priors are defined on three-pixel neighborhoods, and approximate the second derivative of disparity, thus:\nS({p, q, r}, D) = D(p) \u2212 2D(q) + D(r)(6)\nwhere the neighborhoods, N = {p, q, r}, are from the set of all 3 \u00d7 1 and 1 \u00d7 3 patches in the reference image. This function increases monotonically as the neighborhood diverges from collinearity, in contrast to the first-order prior traditionally used, S({p, q}, D) = D(p) \u2212 D(q), which increases monotonically as the neighborhood diverges from fronto-parallel.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimization", "text": "The above defines E(D) as a function of a real-valued disparity image D. In this section we describe how this energy is minimized, following recent generalizations of \u03b1expansion [22,26,33]. In order to optimize the energy over the real-valued space, we reduce it to a sequence of binary problems as follows. Suppose we have a current estimate of the disparity, D t , and a proposal depth map D p . In the \u03b1-expansion method, for example, the proposal depth at each step is a fronto-parallel plane [8]; in this paper we shall use more complex proposals (see \u00a73.3). The goal is to optimally combine (\"fuse\") the proposal and current depth maps to generate a new depth map D t+1 for which the energy E(D t+1 ) is lower than D t . This fusion move is achieved by taking each pixel in D t+1 from one of (D t , D p ), as controlled by a binary indicator image B with elements B(x):\nD b (B) = (1 \u2212 B) \u2022 D t + B \u2022 D p ,(7)\nwhere dot indicates elementwise multiplication. Thus, B may be read as \"copy the disparity from the proposal D p (p) if B(p) = 1, otherwise keep the current estimate D t \". Then the energy E(D) is a function only of the indicator image B, so we may define\nD t+1 = D b argmin B E(D b (B))(8)\nThis boolean optimization problem is then represented as a graph-cut problem, as described in \u00a73.2 below. This will in general lead to a non-submodular graph, but we can use Quadratic Pseudo-Boolean Optimization (QPBO) [5,14,26], which is able to optimize non-submodular energies. Unlike the submodular case, where the global minimum B is guaranteed, QPBO returns a solution B and an associated mask M with the guarantee that at pixels x where M (x) = 1, the value b(x) is at the value it would have at the global minimum, 1 but pixels where M (x) = 0 have \"unlabeled\" values. By forcing B(x) = 0 at those pixels, we ensure that E(D t+1 ) \u2264 E(D t ) (a result of the \"autarky\" property of QPBO [26, page 2]), thus guaranteeing not to increase the energy with each proposal. Although in principle one could optimize our energy just using the above algorithm, in practice convergence would be slow, as the the number of unlabeled pixels at each fusion step may be high. In the next two sections we discuss three important procedures which can be used to greatly improve the performance of the algorithm: (1) a variety of alternative fusion moves; (2) the graph construction which allows each binary subproblem to be effectively solved; and (3) the selection of proposal depth maps.", "publication_ref": ["b21", "b25", "b32", "b7", "b4", "b13", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Alternative fusion strategies", "text": "The fusion move described above states how to choose values for the binary labeling B at pixels unlabeled by QPBO. A number of alternatives are possible, as outlined below. In each case we use the labels 0 and 1 to represent the current and proposed solution, D t and D p , respectively. Many of these alternatives have appeared before in the literature, but we introduce two new strategies which give noticeable improvements in our results, and may prove useful in other contexts. QPBO-F. Fix to current [33]: fix unlabeled nodes to 0, the current best labeling. QPBO-L. Lowest energy label [22]: fix unlabeled nodes collectively to whichever of 0 or 1 gives the lowest energy. QPBOP. Probe: probe the graph, as described in [6,26], in order to find the labels of more nodes, that form part of an optimal solution. QPBOI-F. Fix to current and improve: fix unlabeled nodes to 0, and transform this labeling using QPBOI, as described in [26]. QPBO-R. Lowest cost label per region (new approach, based on the \"optimal splice\" technique of [32]): split unlabeled nodes into strongly connected regions (SCRs), as per [1]. For each SCR, independently select the labeling, 0 or 1, which gives the lowest total energy for cliques connected to that region. QPBOI-R. Improve lowest cost label per region (new approach): Label nodes as per QPBO-R, then use QPBOI to transform this labeling.\nIn \u00a75.1 we empirically compare the various fusion strategies in the context of our problem.", "publication_ref": ["b32", "b21", "b5", "b25", "b25", "b31", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Graph construction", "text": "As mentioned above, the conversion of the large-clique energy E(D) into an equivalent pairwise representation is delayed until the binary optimization stage. \np q r V 1 (p, d 0 ) aux V 1 (p, d 1 ) V N (p, d 1 ) V N (p, d 0 ) V 1 (r, d 0 ) V 1 (r, d 1 ) V N (r, d 1 ) V N (r, d 0 )\nFigure 2. Graph construction. A graphical representation of the energy graph we construct for a 3 \u00d7 1 pixel image. Ovals represent nodes of the graph, and lines (edges) represent pairwise energy terms. Nodes p, q and r are binary variables encoding the disparities, (d0, d1), of those pixels. The nodes V1(p, d0), etc. encode whether (by way of example) pixel p is visible at disparity label 0 (i.e. disparity d0) in I1; note that some of these nodes have been excluded for clarity. Black lines represent the data costs, blue lines the visibility constraint, and red lines the smoothness prior.\nto solving the graph, and, while the list length is variable, it tends to be around nN edges. The six red lines, which represent the smoothness costs of equation ( 5) for the only complete neighborhood, N = {p, q, r}, show how one triple clique is decomposed into six pairwise cliques, and an extra, latent node (labeled aux), using the decomposition described in [21]; note again that while the decomposition was originally given with regard to submodular graphs, it holds for any triple clique.\nGraph complexity With the addition of a fourth pixel, s, to create a 1 \u00d7 4 pixel image, the neighborhood {q, r, s} will share the edge qr with {p, q, r}; therefore, generally, the total number of edges for a bidirectional, second-order smoothness prior, ignoring boundary effects, is 10n, up from 2n for a first-order prior. It can therefore be seen that the use of a second, rather than first, order prior increases the graph size (number of edges) by a factor of approximately (10 + 3N )/(2 + 3N )-around 160% larger with two input images (N = 1), but only 60% larger with 5 input images. A similar analysis on the degree (number of incident edges) of each pixel node shows an increase of a factor of (12 + 3N )/(4 + 3N ), or about 114% higher for two images.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Proposal generation", "text": "The final component of the algorithm to be defined is the choice of proposals. In previous work [22,33], the proposals have just been fronto-parallel planes (denoted \"Same-Uni\" below). As shown in [8], repeated fusion of these proposals leads to a strong local optimum in the submodular case. In the non-submodular case, the nature of these proposal disparity maps has a large effect on the generated disparity map, as we show empirically in \u00a75. We use the following schemes for generating the j th proposal disparity map D p j :\nSameUni Draw d j from a uniform distribution, and set D p j (x) = d j for all x.\nSegPln Uses the ad-hoc approach of segmentation-based methods [17,35] to generate a set of piecewise-planar proposals, which are then cycled through continuously. In this implementation, demonstrated in figure 3, the first stage of proposal generation involves a local window matching process [27] to generate an approximate (very noisy) disparity map. We then use two different image segmentation algorithms, one color-based [10], and one texture-based [12], and 14 sets of parameters in total, to generate segmentations of I 0 , ranging from highly under-segmented to highly oversegmented. For each segment in each segmentation we use LO-RANSAC [9] to find the plane that produces the greatest number of inlying correspondences from the first stage (given a suitable distance threshold), and set all the pixels in the segment to lie on that plane.\nSmooth D p j (x) = (D j (x + \u2206) + D j (x \u2212 \u2206))/2\n, where \u2206 = [0, 1] when j is odd, and \u2206 = [1, 0] when j is even.\nThese proposal methods represent the different approaches used by the main types of stereo algorithms: the fronto-parallel proposals of SameUni are essentially those used at each iteration of an \u03b1-expansion-based stereo algorithm (except drawn from a continuous, rather than discrete, space); SegPln proposals are those used by segment-based algorithms; Smooth proposals, generated by a smoothing operation on the current disparity map, can be viewed as a proxy for local methods such as gradient descent. With QPBO-based fusion, we gain the benefits of all these algorithms-indeed, any stereo algorithm availablewithout affecting the global optimum. For example, the SegPln proposals, the main workhorse of our algorithm, are produced with a range of algorithms and parameter settings; in general we expect these disparity maps to be correct in some parts of the image, and for some parameter settings, but that no settings can be found for which any algorithm works best. By fusing the proposals in a well-defined energy minimization framework, the parameter sensitivity of these methods is turned into an advantage: we can select the best parts from each proposal, at the pixel (as opposed to segment) level.", "publication_ref": ["b21", "b32", "b7", "b16", "b34", "b26", "b9", "b11", "b8"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Implementation", "text": "Some further implementation notes will allow the reader to more accurately replicate our method.\nWe normalize the range of disparities searched over for a particular image sequence to [0, 1] prior to the evaluation of E smooth , in order to make our objective function invariant to image baseline, camera calibration and depth of field. The Optimization is halted either when a maximum number of iterations, t max , is reached, or when the average decrease in energy over the last 20 iterations drops below some threshold, \u03b4E thresh , whichever occurs first.\nWe use Kolmogorov's [19] implementations of QPBO, QPBOP and QPBOI. Both QPBOP and QPBOI methods make use of tree-recycling [18] for a fast implementation; the number of graph solves is at most linear in the number of unlabeled nodes for QPBOI, but exponential for QPBOP, though it should be noted that QPBOP labels nodes optimally, rather than approximately, as with QPBOI.", "publication_ref": ["b18", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "CRF weights", "text": "The CRF weights W (\u2022) are set to encourage disparity edges to align with edges in the reference image I 0 . We generate a single mean-shift segmentation of the reference image ([10], h s = 4 and h r = 5), and assign one of two weights to each neighborhood, depending on whether or not it overlaps a segmentation boundary. Precisely, if L is the map which assigns to each pixel its segmentation label, then\nW (N ) = \u03bb h if L(p) = L(q) \u2200 p, q \u2208 N \u03bb l otherwise.(9)\nParameters We use the same parameter settings for all examples, i.e. \u03bd = 0.01, \u03c3 d = 30C, \u03bb l = 9N, \u03bb h = 108N, \u03c3 s = 0.02, where C is the number of color channels per input image. These settings were obtained by visual evaluation of a small number of Middlebury images (although it must be emphasised that they were not chosen with any reference to the Middlebury evaluation score). The order of the prior was found not to change the relative performance of parameter sets significantly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we describe the experiments we carried out in evaluating the efficacy of QPBO in optimizing our non-submodular energy, the trade-offs of each of the QPBO labeling methods, the effect of using different disparity proposals, and comparing our method, with its second-order prior, to the same method with a first-order prior, and other, competing approaches to stereo.\nThe optimization method used in each experiment is characterized by the order of the prior (\"1op\" for first-order prior, etc.), the set of proposals, the fusion strategy and the convergence criterion used to stop the optimization, e.g. \"2op, SameUni, QPBOI-R, \u03b4E thresh = 0.01%\", or \"1op, SegPln, QPBOP, t max = 200\".", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Unlabeled nodes", "text": "The proportion of pixels that are labeled by QPBO has a direct impact on the quality of the solution found-trivially, if no nodes are labeled then (using QPBO-F) the final solution will be the same as the initial solution. We therefore ran experiments on the 4 Middlebury test sequences to evaluate what proportion of pixels were labeled, and which of the fusion strategies performed best at fixing these pixels. The results of these experiments are shown in figure 6. Figure 6(a) indicates that using SegPln proposals with a second-order prior generates the most unlabeled nodes for our chosen smoothness parameters, at 15% on average; we therefore used these settings to compare fusion strategies. Figure 6(b) shows that QPBOP rapidly becomes several orders of magnitude slower as the number of unlabeled pixels rises, while other methods roughly double in time over the same range; of these there is only fractional difference in speed, though order of fastest to slowest is consistently QPBO-F, QPBO-L, QPBO-R, QPBOI-F, QPBOI-R. In terms of energy reduction performance, QPBOP, which gives an optimal solution, performs best, while QPBO-F, with the simplest labeling strategy, performs worst. Figure 6(a) shows how the other strategies perform relative to these two, and indicates that QPBOI-R achieves the largest energy reduction. Considering the trade-off between time and efficacy, we found QPBOI-R to be the most suitable method for our problem, and used this in all further experiments.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "Proposals", "text": "We applied all our proposal sets separately to each of our test sequences, with both first and second order priors, using QPBOI-R, \u03b4E thresh = 0.01%. However, as the Smooth proposal only performs well when applying it to an approximately correct disparity map, we prefixed the proposal set with the disparity maps generated using the other two proposal schemes, and repeated the set every six iterations, calling this set \"Smooth*\".\nFigure 4 shows the results on the Middlebury \"Venus\" sequence. It can be seen that the fronto-parallel SameUni proposals generate generally piecewise-fronto-parallel solutions with both priors, while the piecewise-planar SegPln 1op 2op", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SameUni", "text": "SegPln Smooth* Figure 4. Effect of proposals. Output of our stereo method on the Venus sequence, using first and second order priors with our 3 proposal strategies. proposals generate piecewise-planar solutions, but with the first-order prior tending to favor more fronto-parallel surfaces over the correct solution. When these solutions are combined in the Smooth* proposal set, the first-order prior favors the SameUni solution, while the second-order prior favors the SegPln solution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Second vs first order", "text": "We used the Middlebury stereo evaluation framework to compare the accuracy of results using first and second order priors. In order to remove biasing caused by proposal schemes (seen in the previous section) we only compare priors using Smooth* proposals (and QPBOI-R, \u03b4E thresh = 0.01%). Figure 7(a) shows the relative performance of the two priors, in terms of average rank in the Middlebury performance table. The graph shows that, not only does the second-order prior perform better at all error thresholds, but also that its performance improves more than the first order prior at the high-accuracy thresholds, relative to other algorithms, indicating improved subpixel accuracy. This ef-  7. Middlebury performance. (a) A graph of average rank on the Middlebury stereo evaluation, against disparity error threshold, for both first and second order priors. (b)-(d) Output disparity using \"2op, Smooth*, QPBOI-R, \u03b4Ethresh = 0.01%\", for the Middlebury \"Tsukuba\", \"Teddy\" and \"Cones\" sequences respectively. (e) Top: Output disparity using the same method as (d), but with no visibility constraint (i.e. Vi(x) = 1 \u2200x). Bottom: Visibility map for I1 of \"Cones\"-pixels deemed occluded according to the following disparity maps are painted (covering the previous color) in the following order: disparity map above, red; (d), blue; ground truth, black. fect can be explained by the fact that non-fronto-parallel planes, as well as curved surfaces, are better modeled by the second-order prior, as demonstrated in figure 8.\nFigure 7(e) highlights the benefits of a visibility constraint (comparing numbers of red and blue pixels)-by reducing the number of falsely occluded pixels, it essentially encourages uniqueness of correspondences between input images. As unique correspondence is a constraint on realworld scenes, incorporating such a constraint in a stereo framework produces better results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multiple & arbitrary views", "text": "The formulation of our objective function allows for any number of input images to be used, and for those images to have arbitrary viewpoints. Figure 5 shows results for such a dataset-the \"Plant & toy\" sequence from [33]. We found little or no qualitative improvement between N = 2 (three views) and N > 2, something we believe can be attributed to the fact that three views are sufficient (in this case) to ensure that each pixel of I 0 is visible in at least one other view. However, should more views be required, figure 5(right), shows that, in practice, the time per fusion iteration (with and without graph construction overheads such as image sampling and visibility computation) rises linearly with N .", "publication_ref": ["b32"], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Conclusion", "text": "This paper has shown that second-order smoothness priors can be incorporated into graph-cut based stereo reconstruction. This was not previously possible, because the non-submodular energies led to infeasibly complex optimizations. Previous stereo algorithms using second-order priors were limited by local optimizers. In particular, the combination of second-order priors with simultaneous global visibility reasoning was not possible. The paper's main contribution is a framework for optimizing the resulting objective function. We have demonstrated that this method produces depth maps that accurately reconstruct the scene at a subpixel level. The algorithm can be equally applied to multi-view stereo with arbitrary camera viewpoints, and does so at a computational cost linear in N .\nAn interesting feature of the optimization strategy, and in particular the \"SegPln\" proposals, is that it can make use of existing algorithms, which may sometimes be rather ad-hoc, and combine their results in a principled way. We expect this property to offer considerable opportunities for improvement of the basic method in terms of the quality of optima discovered, and the speed at which they can be found. Figure 8. Curved surfaces. Left to right: I0 for the Middlebury \"Cloth3\" sequence, ground truth (discretized) disparity surface (3-d view of disparity map), disparity surfaces generated using Smooth* proposals and 1op and 2op respectively. (Spurious pixels have been fixed to the back-plane, for improved visualization.)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements We thank Vladimir Kolmogorov for making available his QPBO software, and also for discussing graph cut stereo with us. Research funded by EP-SRC grants EP/C007220/1, EP/C006631/1(P) and a CASE studentship with Sharp.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A decomposition method for minimizing quadratic pseudoboolean functions", "journal": "Operations Research Letters", "year": "1989-06", "authors": "A Billionnet; B Jaumard"}, {"ref_id": "b1", "title": "Multiway cut for stereo and motion with slanted surfaces", "journal": "", "year": "1999-09", "authors": "S Birchfield; C Tomasi"}, {"ref_id": "b2", "title": "Visual Reconstruction", "journal": "MIT Press", "year": "1987-08", "authors": "A Blake; A Zisserman"}, {"ref_id": "b3", "title": "A layered stereo algorithm using image segmentation and global visibility constraints", "journal": "", "year": "2004-10", "authors": "M Bleyer; M Gelautz"}, {"ref_id": "b4", "title": "Network flows and minimization of quadratic pseudo-boolean functions", "journal": "", "year": "1991-05", "authors": "E Boros; P L Hammer; X Sun"}, {"ref_id": "b5", "title": "Preprocessing of unconstrained quadratic binary optimization", "journal": "", "year": "2006-04", "authors": "E Boros; P L Hammer; G Tavares"}, {"ref_id": "b6", "title": "Graph cuts in vision and graphics: Theories and applications. In The Handbook of Mathematical Models in Computer Vision", "journal": "Springer", "year": "2006", "authors": "Y Boykov; O Veksler"}, {"ref_id": "b7", "title": "Fast approximate energy minimization via graph cuts", "journal": "IEEE PAMI", "year": "2001", "authors": "Y Boykov; O Veksler; R Zabih"}, {"ref_id": "b8", "title": "Enhancing RANSAC by generalized model optimization", "journal": "", "year": "2004-01", "authors": "O Chum; J Matas;  Obdr\u017e\u00e1lek"}, {"ref_id": "b9", "title": "Mean shift: A robust approach toward feature space analysis", "journal": "IEEE PAMI", "year": "2002", "authors": "D Comaniciu; P Meer"}, {"ref_id": "b10", "title": "Complete dense stereovision using level set methods", "journal": "", "year": "1998", "authors": "O D Faugeras; R Keriven"}, {"ref_id": "b11", "title": "Efficient graph-based image segmentation", "journal": "IJCV", "year": "2004-09", "authors": "P F Felzenszwalb; D P Huttenlocher"}, {"ref_id": "b12", "title": "From Images to Surfaces: A Computational Study of the Human Early Visual System", "journal": "MIT Press", "year": "1981", "authors": "W E L Grimson"}, {"ref_id": "b13", "title": "Roof duality, complementation and persistency in quadratic 0-1 optimization", "journal": "Mathematical Programming", "year": "1984", "authors": "P L Hammer; P Hansen; B Simeone"}, {"ref_id": "b14", "title": "Segment-based stereo matching using graph cuts", "journal": "", "year": "2004", "authors": "L Hong; G Chen"}, {"ref_id": "b15", "title": "Rethinking the prior model for stereo", "journal": "", "year": "2006", "authors": "H Ishikawa; D Geiger"}, {"ref_id": "b16", "title": "Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure", "journal": "", "year": "2006", "authors": "A Klaus; M Sormann; K Karner"}, {"ref_id": "b17", "title": "Efficiently solving dynamic Markov Random Fields using graph cuts", "journal": "", "year": "2005", "authors": "P Kohli; P H S Torr"}, {"ref_id": "b18", "title": "Discrete MRF optimization software", "journal": "", "year": "", "authors": "V Kolmogorov"}, {"ref_id": "b19", "title": "Multi-camera scene reconstruction via graph cuts", "journal": "", "year": "2002", "authors": "V Kolmogorov; R Zabih"}, {"ref_id": "b20", "title": "What energy functions can be minimized via graph cuts?", "journal": "IEEE PAMI", "year": "2004", "authors": "V Kolmogorov; R Zabih"}, {"ref_id": "b21", "title": "LogCut -efficient graph cut optimization for Markov Random Fields", "journal": "", "year": "2007", "authors": "V Lempitsky; C Rother; A Blake"}, {"ref_id": "b22", "title": "Surface geometric constraints for stereo in belief propagation", "journal": "", "year": "2006", "authors": "G Li; S W Zucker"}, {"ref_id": "b23", "title": "Surfaces with occlusions from layered stereo", "journal": "IEEE PAMI", "year": "2004", "authors": "M Lin; C Tomasi"}, {"ref_id": "b24", "title": "Shape and the stereo correspondence problem", "journal": "IJCV", "year": "2005", "authors": "A S Ogale; Y Aloimonos"}, {"ref_id": "b25", "title": "Optimizing binary MRFs via extended roof duality", "journal": "", "year": "2007", "authors": "C Rother; V Kolmogorov; V Lempitsky; M Szummer"}, {"ref_id": "b26", "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "journal": "IJCV", "year": "2002", "authors": "D Scharstein; R Szeliski"}, {"ref_id": "b27", "title": "Wide-baseline stereo from multiple views: a probabilistic account", "journal": "", "year": "2004-06", "authors": "C Strecha; R Fransens; L Van Gool"}, {"ref_id": "b28", "title": "A global matching framework for stereo computation", "journal": "", "year": "2001", "authors": "H Tao; H S Sawhney; R Kumar"}, {"ref_id": "b29", "title": "Multilevel computational processes for visual surface reconstruction. Computer Vision, Graphics and Image Processing", "journal": "", "year": "1983-10", "authors": "D Terzopoulos"}, {"ref_id": "b30", "title": "Asymmetrical occlusion handling using graph cut for multi-view stereo", "journal": "", "year": "2005", "authors": "Y Wei; L Quan"}, {"ref_id": "b31", "title": "Fields of experts for image-based rendering", "journal": "", "year": "2006", "authors": "O Woodford; I D Reid; P H S Torr; A W Fitzgibbon"}, {"ref_id": "b32", "title": "On new view synthesis using multiview stereo", "journal": "", "year": "2007", "authors": "O Woodford; I D Reid; P H S Torr; A W Fitzgibbon"}, {"ref_id": "b33", "title": "OJW's Image Based Rendering (IBR) Toolbox v2", "journal": "", "year": "", "authors": "O J Woodford"}, {"ref_id": "b34", "title": "Stereo matching with color-weighted correlation, hierachical belief propagation and occlusion handling", "journal": "", "year": "2006", "authors": "Q Yang; L Wang; R Yang; H Stew\u00e9nius; D Nist\u00e9r"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(a) Reference image. (b) First-order prior. (c) Li & Zucker's result. (d) Our result.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. Second-order smoothness priors. (a) A reference image for which we wish to produce a dense depth map. (b)-(d) The disparity (inverse depth) maps produced by (b) first-order prior, (c) second-order prior of Li and Zucker [23] and (d) second-order prior with visibility, optimized as in this paper.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 demonstrates the construction of the graph used in each binary optimization, for a 1 \u00d7 3 pixel image. The graph contains only pairwise terms represented by the lines in the figure, linking the nodes. The black lines represent the data costs of equation (3), giving 2nN edges for an n pixel reference image. The blue lines are infinite edge costs which enforce the visibility constraint of the same equation, as per [31]; the line shown indicates that one (or both) of the disparity labels for pixel p occludes pixel r at disparity d 0 in I 1 . The list of pixel occlusion interactions is computed prior", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure3. SegPln proposal generation. Top row: I0, and 3 of its 14 segmentations. Bottom row: approximate disparity map from window matching, and 3 SegPln proposals generated by fitting planes to each segment in the above segmentations.initial depth map, D 0 , is set to D 0 (x) = rand[0, 1] for each x independently.Optimization is halted either when a maximum number of iterations, t max , is reached, or when the average decrease in energy over the last 20 iterations drops below some threshold, \u03b4E thresh , whichever occurs first.We use Kolmogorov's[19] implementations of QPBO, QPBOP and QPBOI. Both QPBOP and QPBOI methods make use of tree-recycling[18] for a fast implementation; the number of graph solves is at most linear in the number of unlabeled nodes for QPBOI, but exponential for QPBOP, though it should be noted that QPBOP labels nodes optimally, rather than approximately, as with QPBOI.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Multiple, arbitrary views. Top left: I0 for the \"Plant & toy\" sequence, which has arbitrary input views. Bottom left: Output disparity using \"2op, Smooth*, QPBOI-R, \u03b4Ethresh = 0.01%\", for N = 2. Right: Graph of fusion (QPBO only) and iteration (including image graph construction) times as a function of N .", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 .6fusion (normalized, with QPBO\u2212F = 0, QPBOP = 1) Proposal and labeling methods. (a) Graph showing proportion of unlabeled pixels per fusion with different priors, prior weightings (i.e. multiplicative factor on \u03bbl and \u03bbh) and proposal methods. (b) Graph showing the time per fusion as a function of unlabeled pixels and fusion strategy. (c) Stacked histogram showing the effect of fusion strategies on energy, relative to QBPO-F and QPBOP.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "R 2 \u2192 R 2 } N i=1", "formula_coordinates": [2.0, 423.71, 440.59, 56.71, 12.32]}, {"formula_id": "formula_1", "formula_text": "1 (x, d) = x + [d, 0].", "formula_coordinates": [2.0, 457.34, 489.96, 82.01, 9.68]}, {"formula_id": "formula_2", "formula_text": "E(D) = E photo (D) + E smooth (D).(1)", "formula_coordinates": [2.0, 359.28, 594.06, 185.83, 9.8]}, {"formula_id": "formula_3", "formula_text": "E photo (D) = x N i=1 f I \u03c0 i (x, D(x)) \u2212 I 0 (x), V i x (2)", "formula_coordinates": [2.0, 318.99, 686.01, 226.13, 30.32]}, {"formula_id": "formula_4", "formula_text": "f (\u2206I, V ) = \u03c1 d (\u2206I) if V = 1 \u03bd if V = 0 (3)", "formula_coordinates": [3.0, 99.59, 122.07, 186.77, 23.3]}, {"formula_id": "formula_5", "formula_text": "\u03c1 d (I) = \u2212 log 1 + exp(\u2212 I 2 /\u03c3 d ) ,(4)", "formula_coordinates": [3.0, 92.42, 188.76, 193.94, 11.88]}, {"formula_id": "formula_6", "formula_text": "E smooth (D) = N \u2208N W (N )\u03c1 s (S(N , D)). (5", "formula_coordinates": [3.0, 87.96, 559.9, 194.53, 20.1]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 282.49, 560.22, 3.87, 8.64]}, {"formula_id": "formula_8", "formula_text": "S({p, q, r}, D) = D(p) \u2212 2D(q) + D(r)(6)", "formula_coordinates": [3.0, 82.57, 647.44, 203.8, 8.99]}, {"formula_id": "formula_9", "formula_text": "D b (B) = (1 \u2212 B) \u2022 D t + B \u2022 D p ,(7)", "formula_coordinates": [3.0, 361.53, 344.42, 183.58, 11.72]}, {"formula_id": "formula_10", "formula_text": "D t+1 = D b argmin B E(D b (B))(8)", "formula_coordinates": [3.0, 359.21, 442.62, 185.91, 18.59]}, {"formula_id": "formula_11", "formula_text": "p q r V 1 (p, d 0 ) aux V 1 (p, d 1 ) V N (p, d 1 ) V N (p, d 0 ) V 1 (r, d 0 ) V 1 (r, d 1 ) V N (r, d 1 ) V N (r, d 0 )", "formula_coordinates": [4.0, 313.84, 81.52, 228.76, 81.54]}, {"formula_id": "formula_12", "formula_text": "Smooth D p j (x) = (D j (x + \u2206) + D j (x \u2212 \u2206))/2", "formula_coordinates": [5.0, 50.11, 312.84, 204.49, 13.61]}, {"formula_id": "formula_13", "formula_text": "W (N ) = \u03bb h if L(p) = L(q) \u2200 p, q \u2208 N \u03bb l otherwise.(9)", "formula_coordinates": [5.0, 335.51, 508.45, 209.6, 24.18]}], "doi": ""}