{"title": "Automatic Correction of Human Translations", "authors": "Jessy Lin; Geza Kovacs; Aditya Shastry; Joern Wuebker; John Denero; \u2661 Google; \u2663 Lilt", "pub_date": "", "abstract": "We introduce translation error correction (TEC), the task of automatically correcting human-generated translations. Imperfections in machine translations (MT) have long motivated systems for improving translations posthoc with automatic post-editing. In contrast, little attention has been devoted to the problem of automatically correcting human translations, despite the intuition that humans make distinct errors that machines would be well-suited to assist with, from typos to inconsistencies in translation conventions. To investigate this, we build and release the ACED corpus with three TEC datasets 1 . We show that human errors in TEC exhibit a more diverse range of errors and far fewer translation fluency errors than the MT errors in automatic post-editing datasets, suggesting the need for dedicated TEC models that are specialized to correct human errors. We show that pre-training instead on synthetic errors based on human errors improves TEC F-score by as much as 5.1 points. We conducted a human-in-the-loop user study with nine professional translation editors and found that the assistance of our TEC system led them to produce significantly higher quality revised translations.", "sections": [{"heading": "Introduction", "text": "Despite recent progress in machine translation (MT), a tremendous amount of translated content in the world is still written by humans (DePalma, 2021). Humans are often assumed to produce trusted, high-quality translations. In reality, they do make errors, including spelling, grammar, and translation errors (Hansen, 2009). This paper introduces the task of translation error correction (TEC). Given a source sentence s and a humangenerated translation t, the goal of TEC is to produce an improved translation t \u2032 by correcting all errors in t.\n\"Translation correction\" has long been studied in the MT community through the task of automatic post-editing (APE), which aims to correct errors in machine-generated translations (Simard et al., 2007). TEC is structurally identical to APE. However, it requires modeling a different data distribution: errors made by humans, which differ from those made by MT systems (Freitag et al., 2021). We characterize the error distribution in TEC by building, analyzing, and releasing the ACED corpus, a collection of three TEC datasets from varying domains, with a total of 35,261 English-German translations produced and corrected by professional translators in the natural course of their work. While APE is dominated by the fluency errors that are characteristic of MT systems (74% of sentences), our TEC corpus exhibits a broader distribution of errors that human translators are prone to make.\nUsing this error analysis, we propose an approach for TEC that pre-trains on synthetic corruptions more similar to errors made by humans, outperforming models that were developed for the related tasks of MT, grammatical error correction, and APE on all ACED datasets.\nThe task of TEC is often currently performed by humans, e.g. translators hired to review and edit translations (\"reviewers\"). Can a TEC system help reviewers edit faster, or produce higher quality final translations than they would have without assistance? We ran a human-in-the-loop user study with nine professional translators using our bestperforming TEC model. We found that the reviews produced when assisted with a TEC system were rated as higher quality than those produced without, and produced with less manual effort. Qualitatively, users commented that trust and consistency of the suggestions were critical. They speculated future automated assistance could be helpful for onboarding to new content, spotting technical errors, and improving their own awareness of errors to catch.   Looking forward, a natural question arises of whether the research community should focus on learning to revise model outputs (APE) or human outputs (TEC). With recent improvements in MT, it has been increasingly difficult for APE models to improve model output that is already high quality (Chollampatt et al., 2020). On the other hand, we should expect that humans will continue to make errors. TEC models will continue to provide benefit as a way of assisting humans, whether for professional translation work or everyday language learners. TEC is synergistic with continuing advancements in MT: improved MT will lead to improved error correction for human-generated translations. While APE pits models against models, TEC is an opportunity to combine the best of humans and models because humans and models make different errors.\nIn sum, this paper revisits the notion of \"translation correction\" conceived narrowly as the MTcentered task of APE, with an empirical investigation of translation error correction (TEC), the task of learning to correct human translations. Our contributions are:\n1. We release ACED, the first corpus for TEC containing three datasets of human translations and revisions generated naturally from a commercial translation workflow. 2. We analyze the kinds of errors humans make in ACED, finding that while APE is dominated by correcting translation fluency, TEC focuses on correcting a broader range of errors that appear in translation. 3. We propose a pre-training approach for TEC that outperforms approaches developed for similar tasks such as APE. Together, our results suggest the need for distinct approaches to correct human translation errors. 4. We perform a human-in-the-loop user study, finding that professional translators produce higher quality translations when assisted by a TEC model.", "publication_ref": ["b7", "b18", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "The \u2660 ACED Corpus for TEC", "text": "Given a source language sentence s and a humangenerated translation t, the goal of TEC is to produce a corrected target language sentence t \u2032 . We introduce the ACED corpus, a set of three TEC datasets: ASICS, EMERSON, and DIGITALO-CEAN (DO), each consisting of English-German sentence triples (s, t, t \u2032 ) from varying domains.\nACED is a real-world benchmark, containing naturalistic data from a task humans perform, rather than manually annotated data. All translations were created by professional translators working with Lilt, a localization services provider. All translators have at least 5 years of professional translation experience and experience working with the customer and domain. Each document was translated from scratch (i.e. not post-edited) by a human translator using an interactive neural MT system. Each translated document was then reviewed by a reviewer, who Lilt selects as one of the more senior translators. As a result, the examples in our corpus exhibit  real errors that translators make, and the corrected translations are publication quality. Secondly, ACED is diverse, with the three datasets from varying domains exhibiting different error distributions and difficulty for initial work on the TEC task. Information for each dataset is shown in Table 2. The ASICS dataset consists of marketing content with product names and descriptions for an activewear company. EMERSON consists of industrial product names for a manufacturing company. DO consists of software engineering tutorials. The various content types pose different challenges for translators and thus for TEC systems, which we discuss in Section 2.1 and Section 2.2.\nDuplicate sentences with the same source s were removed. A portion of sentences were rewritten by the reviewer rather than being edited (a relative edit distance of more than 25% and a minimum of two edited words). We replace t in these sentences with t \u2032 in the corpus, so that training and evaluation focuses on local edits rather than re-translations. The train, dev, and test splits were constructed by splitting along document boundaries.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "How do TEC and APE errors differ?", "text": "To understand how the human errors in the TEC task differ from model errors in APE, we compare the types of errors in ACED with 100 randomly sampled errors in the WMT 2021 APE shared task dev set 2 , which we then annotate with error types. We define an error taxonomy that classifies each edit as one of three types: (1) Monolingual edits are identifiable from only the target-side text. We divide these further into subcategories that highlight different capabilities needed to correct edits: typos (including spelling, punctuation, spacing, orthographic issues), grammar, and fluency (awkward phrasing, word choice, or non-native-sounding disfluencies); (2) Bilingual edits concern mismatches between the source and target text, e.g. over-or under-translation, mis-translations; (3) Preferen-tial edits correct text that is inconsistent with the preferences of the customer, as described in extralinguistic project requirements (e.g. terminology or stylistic preferences). Examples of each error type are shown in Table 1. Our error taxonomy closely mirrors those of previous analyses of human translation errors (Specia and Shah, 2014;Yuan and Sharoff, 2020;Gupta et al., 2021), and we confirm their findings that human translation errors differ from MT errors. However, while previous work focuses on error detection and quality estimation, TEC is concerned with error correction. Our error types are intended to isolate the capabilities that models need to learn to correct edits (e.g., target-side language models can learn to correct monolingual errors, but cannot do well on bilingual edits).\nWe annotate and release error labels for all test sentences in ASICS to enable its use as a diagnostic set for per-type evaluation of models. On the larger EMERSON and DO datasets, we randomly sample 50 errors to annotate for this analysis. Error types were annotated by a professional German translator. Each segment can have multiple error types. In Table 3, we report the percentage of sentences with at least one error of each type. 74% of sentences in APE exhibit a fluency error, in contrast to up to 22% of sentences in ACED, while other types like monolingual grammar, bilingual, and preferential errors are notably underrepresented in APE. We also note that all sentences in the APE shared task are edited, while a key feature of TEC is identifying when a sentence does not need to be edited. The error distributions suggest that different modeling techniques may shine in each: while APE challenges models to correct disfluent translations characteristic of MT systems, our task is designed to focus on identifying and correcting the typos, mismatches, and grammatical errors more commonly exhibited by humans. Guided by this observation, we describe an approach in Section 3 that pre-trains on synthetic edits that are more representative of this  error distribution.", "publication_ref": ["b19", "b21", "b6"], "figure_ref": [], "table_ref": ["tab_0", "tab_4"]}, {"heading": "How difficult is it to learn to edit?", "text": "To quantify how difficult it may be to learn the correct edits in ACED, we report statistics on edit overlap: what proportion of edits that we expect models to perform (e.g. adding a comma) appear exactly in the training set? We use the errant toolkit to identify discrete edits (Bryant et al., 2017). Each edit is represented as a tuple (original span, replacement span), e.g. (\"auf\", \"an\") to replace \"auf\" with \"an.\" Edit statistics are reported in Table 4: in ASICS and DO, \u223c20% of the total number of edits in dev and test appear in the training set, while EMERSON has \u223c60% of dev and test edits appear in the training set.\nWhile the edit overlap rate provides a relative sense of scale for precision and recall numbers, it does not provide an upper bound on recall. It is possible to learn edits that do not exactly appear in the training set. For example, capitalizing product names (\"winterized\" \u2192 \"WINTER-IZED\") is a learnable pattern that would appear as many distinct edits. Additionally, some errors can be corrected without fine-tuning because they are generic typo, grammatical, fluency, or bilingual errors. Conversely, it is also possible that it is wrong to make an edit that appears in the training set, depending on the surrounding sentential context.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Approaches to TEC", "text": "We propose a TEC model and compare it to several models designed for related tasks to determine whether they are also effective for TEC. An overview of the differences between the models is shown in Figure 1. All models use the Transformer neural architecture (Vaswani et al., 2017)   translation task 3 and fine-tuned on ACED, unless indicated otherwise. All pre-training and fine-tuning data is pre-processed by normalizing punctuation with the Moses toolkit (Koehn et al., 2007).\nAll models have 6 encoder and decoder layers, model dimension of 256, feed-forward dimension of 512, and 8 attention heads. We use a joint English-German vocabulary with 33k byte pair encoding subwords (Sennrich et al., 2016). During pre-training, we set the dropout to 0.1 and use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.0002. During fine-tuning, we decrease the learning rate to 0.0001 and reset the Adam momentum parameters. We select the best fine-tuning checkpoint with edit-level F 0.5 score on our dev set. We use greedy inference.", "publication_ref": ["b20", "b11", "b17"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Dual-Source Encoder-Decoder Model", "text": "We first describe the dual-source encoder-decoder we use for the APE and TEC models. Formally, the original Transformer architecture (Vaswani et al., 2017) takes a sequence of J source tokens s 1...J and predicts a sequence of I \u2032 target tokens t \u2032 1...I \u2032 . We adapt the architecture to additionally encode the original translation t, a sequence of I tokens, t 1...I . We independently project t into the embedding space, add an offset vector o, and then concatenate the embedding with the embedding of the source s to form the encoder input. To allow the dual-source model to copy tokens from the original translation t, we implement the copy-mechanism proposed by Zhao et al. (2019), which augments the model with an additional encoder-decoder attention layer. An expanded description of the model can be found in Appendix Section A. Transformer encoders and decoders are depicted as yellow and blue rectangles, respectively. The GEC and TEC models are pre-trained with synthetic corruptions of t \u2032 (t \u2032 corrupted ), as detailed in the description of the TEC and GEC models. APE uses MT-generated translations of s (t MT ) as synthetic data. BERT-APE is a state-of-the-art pre-trained APE model made available by Correia and Martins (2019).", "publication_ref": ["b20", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Synthetic Data Generation", "text": "For the TEC and GEC model, we generate synthetic triples (s, t, t \u2032 ) for pre-training. We generate a synthetic t by corrupting the German side of the translation data into t \u2032 corrupted . For each sentence, we sample the probability of corruption p c \u223c N (\u00b5 = 0.01, \u03c3 = 0.04) clipped at 0. On each character and word in that sentence, with probability p c , we randomly select one of the following perturbations to apply at that position: insertion, deletion, transposition, repetition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TEC Models", "text": "The five approaches we compare are: TEC (this work) We implement the dual-source encoder-decoder model that encodes two inputs (s, t) and outputs t \u2032 , as described in Section 3.1, and then pre-train on synthetic data generated with the procedure in Section 3.2. We then fine-tune on ACED.\nMT We train an English-German neural machine translation model (with the standard architecture described previously) and fine-tune it on (s, t \u2032 ) ACED pairs, ignoring the original translation t.\nGEC We evaluate a encoder-decoder (monolingual) GEC model that takes an incorrect German sentence t as input and outputs a corrected t \u2032 . We use the same copy mechanism to attend to t as our TEC model. To pre-train, we perturb t \u2032 using the procedure described in Section 3.2, throwing away the source side to obtain (t, t \u2032 ) = (t \u2032 corrupted , t \u2032 ) pairs. We then fine-tune on the ACED corpus, ignoring s.\nAPE We implement a dual-source encoderdecoder model that is identical to our TEC model. Following common practice in APE (Junczys-Dowmunt and Grundkiewicz, 2016;Negri et al., 2018), we pre-train on synthetic \"post-editing\" triples (s, t, t \u2032 ) where t = t MT is generated by translating s with an MT system. We split the training dataset into two parts, train an MT model on each half, and use each model to translate the other half of the dataset not seen during training. We then fine-tune on ACED.\nBERT-APE We also evaluate whether a state-ofthe-art APE model can be directly applied to our task. We evaluate the BERT-based encoder-decoder of Correia and Martins (2019), on which the WMT 2019 shared task winner was based (Lopes et al., 2019). Following Correia and Martins (2019), we fine-tuned on 23K English-German SMT triplets from the WMT18 shared task 4 . We reproduce their results on the APE shared task test sets, and continue fine-tuning this model on ACED. Following their paper, the inputs are pre-processed by tokenizing and joining the two inputs with a separator to form (s [SEP] t, t \u2032 ) pairs.", "publication_ref": ["b8", "b14", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Results & Discussion", "text": "The primary metric for TEC is MaxMatch scores (M 2 ) (Dahlmeier and Ng, 2012) computed with the errant toolkit (Bryant et al., 2017). M 2 is a standard metric for GEC that aligns t and t \u2032 to extract discrete \"edits.\" We choose to follow the GEC evaluation practice of up-weighting precision by comparing F 0.5 , since the original translation is  mostly correct: it is better to suggest few correct edits than potentially introduce new errors.\nTable 5 shows that TEC achieves the best overall F 0.5 score on all datasets, from +0.1 (on DO) up to +5.1 (on ASICS) above the next-best model. Fine-tuning on actual human corrections provides substantial gains; results without fine-tuning can be found in Appendix Section B.\nBoth the MT model (which ignores t) and the GEC model (which ignores s) underperform TEC. The MT model's high edit recall can be attributed to the fact that it proposes many edits, greatly trading off precision. Without conditioning on t, direct MT translations of the source diverge from the reference. The GEC model obtains high precision but underperforms on recall. Conditioning on s not only makes it possible to propose bilingual edits, but also provides additional information to correct monolingual edits, as we show in Section 4.1.\nCan APE models be directly adapted for TEC? Since our task is structurally identical to APE, a natural question is whether models that are trained on the APE objective can be directly adapted for TEC. The APE and TEC models differ only in pretraining, but the performance difference between them is substantial, indicating that the more GEClike data synthesis procedure is a better fit for TEC than APE-style data synthesis via MT. Even more, the BERT-APE model, which is first fine-tuned to achieve state-of-the-art on APE before fine-tuning on ACED, achieves a particularly low F 0.5 score because it makes too many edits (low precision). Although future work may find insights in APE, our results emphasize that models that excel at correcting machine errors cannot be assumed to work well on human translations.", "publication_ref": ["b3", "b0"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Fluency & Per-category Error Analysis", "text": "We perform a more in-depth comparison using alternative metrics on ASICS, which includes an-notated error labels as a diagnostic tool. First, to understand how much models are editing, we look at n-gram overlap with the GLEU metric (Napoles et al., 2015), a variant of BLEU used in GEC evaluation to measure the fluency of holistic rewrites (Sakaguchi et al., 2016). Next, we compare sentence-level accuracy, which measures exact match with t \u2032 . We compute overall sentencelevel accuracy, which includes unedited sentences (which some models may incorrectly edit). We also report accuracy per error type over (edited) sentences annotated with that error type. These metrics need to be interpreted carefully: a no-edit baseline achieves a GLEU score of 87.85 (since original translations are mostly close in edit distance to the final) and sentence-level accuracy of 70.62% (the % of unedited sentences), outperforming models like MT and BERT-APE that make too many incorrect edits.\nOur TEC model achieves the best score overall on both alternative metrics over all sentences, but various models outperform on specific error types. The full results are shown in Table 6. Examples of system outputs for different error types can be found in Appendix Section C.\nNotably, APE models lose the most accuracy relative to our model on monolingual typo edits. This may be because neural MT decoders much less frequently introduce target-side errors that would be similar to typos (compared to the frequency of fluency errors). Still, the observation that different models do well at different errors suggests that future work can improve on TEC by leveraging the strengths of different models, e.g. using MT models to propose alternative translations. Table 6: Additional analysis of n-gram overlap (via GLEU) and exact-match sentence accuracy over all test sentences (Overall) and per error category for the ASICS dataset. Number of sentences with an error of each type are indicated in gray, with some sentences containing errors from multiple categories. For each error category, we report the percentage of those sentences with at least one error of that type that a system predicted exactly.\nwe are ultimately interested in whether any TEC system is indeed useful in practice. Presently, TEC is done manually by humans. To investigate whether TEC systems can already be useful to humans-improving the quality, speed, or ease of human review-we performed a human-in-theloop user study with our TEC model.", "publication_ref": ["b13", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "We recruited 9 professional translators to serve as reviewers. None of them had prior experience with ASICS content. They were allowed to read and reference the sentences in the ASICS training set to familiarize themselves with the content and preferred terminology. Then, they were each assigned to review 74 sentences from the test set of ASICS.\nOf the 74 sentences, our TEC system predicted a suggested edit for 57 sentences, and for the remaining 17 sentences our TEC system did not predict any edits. We opt for a within-subjects design to control for speed and experience differences between reviewers. For each reviewer, the 74 sentences were randomized such that half were in the \"assisted condition\" showing the TEC suggestion if available for the sentence, and the other half were in the \"unassisted condition\" where no TEC suggestion was shown. The reviewing interface is shown in Figure 2. If the sentence has suggestions available, the reviewer is asked to first accept or reject each of the suggestions. Then, they are asked to make edits to the text until they are satisfied with the translation. They then click a button to confirm their translation and move to the next sentence.\nDuring the review process, we track: 1. Whether the TEC suggestion, if shown, was accepted or declined Finally, to evaluate whether TEC has an effect on quality, we asked a 10th translator to compare the quality of the reviewed sentences by ranking the 9 reviewed translations, with ties allowed. This translator was the translator who had reviewed the reference translations in the corpus, as the explicit goal is to ensure consistency with conventions in the training documents.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Results", "text": "In the user study, 79% of TEC suggestions were accepted. For the purpose of analyzing effects of the TEC suggestions on time spent and translation quality, we will focus on only the 255 sentences (across 9 reviewers) where a TEC suggestion exists. Results are shown in   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effects of Suggestions on Time Spent During the Review Process", "text": "We first analyze how TEC suggestions influence the time spent reviewing a sentence. We compare the time durations normalized by the length of the sentence that needed to be reviewed (the lengthnormalized review time), as longer sentences require more time to be read and reviewed.\nThere is no significant difference in lengthnormalized review time when the suggestion is hidden vs. shown (MW U = 31654, p = 0.460). When suggestions are shown, the lengthnormalized review time is significantly less on sentences where reviewers accepted the suggestion, compared to sentences where they declined (MW U = 3555, p < 0.0005).\nA potential explanation for these results is that when reviewers are shown incorrect TEC suggestions, they are distracted and slowed down, providing some evidence that precision should indeed be emphasized in automatic evaluations of TEC.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effects of Suggestions on Edits Made", "text": "During the Review Process\nWe also analyze the effects of suggestions on the editing effort, as measured by the number of characters the reviewer had to insert and delete, as well as how different the final reviewed sentences were from the original. When suggestions are shown vs. hidden, there is a significant reduction in the number of inser-tions+deletions (MW U = 41348.5, p < 0.0001). There is also a significant reduction when a shown suggestion is accepted vs. declined (MW U = 4007.5, p < 0.005). There is no significant difference in the Levenshtein distance from the original translation to the final translation, between when a suggestion is shown vs. hidden (MW U = 33750.0, p = 0.611), or between when a shown suggestion is accepted vs. declined (MW U = 5485.0, p = 0.783).\nThus, the TEC system suggestions help to significantly reduce the amount of manual typing that the user must perform.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effects of suggestions on reviewed sentence quality", "text": "To assess the effects of TEC assistance on quality, we used the quality rankings produced by the independent reviewer. Quality rankings were not normally distributed, so we use the Mann-Whitney U-test for testing statistical significance. A box plot of the quality rankings is shown in Figure 3. The median quality ranking when the suggestion is shown is 1, vs. 2 when the suggestion is hidden. The quality ranking is significantly lower (meaning quality is higher) when the suggestion is shown, vs. hidden (MW U = 28738.0, p < 0.01). This suggests that showing TEC suggestions may be helping reviewers correct errors they may not have otherwise noticed, or help nudge them towards desired corrections.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Qualitative Findings", "text": "We also conducted a post-study survey for reviewers to report qualitative feedback. To understand common themes in the responses, we present all themes that at least two reviewers mention in their commentary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Role of Reliability and Trust", "text": "Five reviewers commented that reliability is critical: it was difficult to trust the system when they noticed some suggestions were incorrect, or the system did not reliably make an edit when applicable (e.g. always hyphenating when appropriate):\n\"Because I wasn't sure I can trust the suggestions (because I saw several incorrect ones) so it took me longer to think/check whether the suggestion is right. And I have to read the entire sentence again anyway to check for other errors the suggestion didn't catch...would only work if I knew 100% that the suggestions are always right\" These comments are in concordance with our quantitative findings. Perhaps unlike other assistive applications, it is not enough to only have high precision: if reviewers cannot trust that the system has caught most or all errors, they will not save time as they still have to read the entire sentence carefully. Conversely, a high-recall, low-precision system is not only distracting, but also leads reviewers to be suspicious of whether suggestions are correct in general. In general, future TEC systems must manage this balance of precision and recall for user trust.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Use Cases for TEC", "text": "Many reviewers highlighted scenarios where trustworthy TEC systems could be particularly useful.\nTwo reviewers said TEC is helpful for corrections and typos, similar to the use cases for GEC in the wild (Omelianchuk et al., 2021):\n\"If the tool would manage to reliably show missing punctuation marks, or numbers, or that the translation contains different numbers than the source, that would be helpful and save time.\"; \"recurring mistakes\" On the other hand, three reviewers mentioned that they hoped such a system would make more substantial corrections in order to save a non-negligible amount of time, although of course these edits may come at the expense of precision: \"There were not many suggestions, and they only offered small improvements...Not clear whether I would save time or not.\"\nThree reviewers commented that a TEC system could be a memory aid or substitute for researching client-specific requirements, which is often an intensive part of the production translation process. One reviewer pointed out it could be particularly useful as an instructive tool for translators who are new to a client:\n\"if I am new to an account and don't yet know whether this client wants hyphens or not (always an issue with German). So usually I have to research... (or guess), but if the QA suggestions knew this client's preference and would tell me, that would save me time.\" Finally, three reviewers commented that it could be useful as an attention-directing tool by making them aware of what errors they might look out for, especially in repetitive content where it may be easy to miss details: \"makes you more sensitive for spotting similar errors\"; \"makes you aware of what kind of errors to look for in upcoming segments\"; \"maybe it helps with [repetitive sentences] that you would otherwise just quickly glance at.\"", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion & Future Work", "text": "We introduced the task of translation error correction (TEC) and released the ACED corpus to study automatic correction of human translations, consisting of three TEC datasets across varying domains. In our analysis of TEC data, we showed how the errors that humans make differ from those made by MT systems, suggesting that this task warrants different approaches from those previously studied in the task of automatic post-editing. We confirm this empirically by proposing a synthetic data generation procedure that more closely matches the distribution of human translation errors and showing that our TEC model, pre-trained on this data, consistently outperforms models developed for APE, as well as those for MT and GEC. Finally, we showed how our TEC system is helpful to real humans, assisting professional reviewers and leading them to produce higher quality reviewed translations.\nFuture work may improve on our TEC system by investigating how to leverage the strengths of recent MT systems (e.g. for initializing systems or proposing edits) or developing more sophisticated synthetic data generation techniques (e.g. using the source sentence or linguistic knowledge). Beyond our benchmark, it would be interesting to apply TEC systems to other settings in which human translation errors appear, e.g., to correct translations written by language learners, denoise MT training sets, or clean up MT evaluation sets.\nFrom the perspective of human-AI interaction, TEC presents a real-world use case and testbed to study how to assist experts with modern NLP systems, hinting at the opportunity to combine the best of humans and machines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Transformer Architecture Background and Model Description", "text": "A.1 Transformer Architecture\nThe neural models implemented in this work are based on the self-attentional Transformer architecture (Vaswani et al., 2017). Formally, given a sequence of source tokens (encoded as one-hot vectors) s 1...J = (s 1 , . . . , s J ), s j \u2208 V, the goal is to predict a sequence of target tokens t \u2032 1...I \u2032 = (t \u2032 1 , . . . , t \u2032 I \u2032 ), t \u2032 i \u2208 V, that is a translation of the source sequence, where V is the vocabulary. The model has two main components, the encoder and the decoder. The encoder transforms the source sequence s 1...J into a sequence of hidden states by first mapping each individual token into a continuous embedding space, adding a positional embedding and then processing it through a sequence of self-attention and feed-forward layers:\nx 1...J = Es 1...J + p 1...J\n(1)\nh enc 1...J = encoder(x 1...J ),(2)\nwhere x j \u2208 V, j \u2208 (1, . . . , J), E is the embedding matrix for vocabulary E and p 1...J is the sequence of positional embeddings described in Sec. 3.5 of (Vaswani et al., 2017). At a given time step i, the decoder defines a probability distribution P i over all vocabulary items in V:\ny \u2032 1...i\u22121 = Et \u2032 1...i\u22121 + p 1...i\u22121 (3) h dec i = decoder(y \u2032 1...i\u22121 , h enc 1...J )(4)\nP i (t \u2032 i ) = softmax(h dec i E \u22a4 )(5)\nwhere we assume a single shared vocabulary V and embedding matrix E. At training time we optimize the cross-entropy loss \nL CE (P ) = \u2212 i log(P i (t \u2032 i )).(6\nh dec i = decoder(y \u2032 1...i\u22121 , h enc 1...(J+I) ), ((8)\n)9\nwhere o is a single learned vector that is broadcast to all positions i \u2208 (1, . . . , I) and [\u2022; \u2022] denotes the concatenate operation.", "publication_ref": ["b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Copy-Attention Mechanism", "text": "The new output probability distribution for the next target token P i (t \u2032 i ) is a weighted sum of the probability of generating and the probability of copying token t \u2032 i :\nP i (t \u2032 i ) = (1 \u2212 \u03b1 copy i )P i (t \u2032 i ) + \u03b1 copy i P copy i (t \u2032 i ),(10)\nwhere the copy probabilities are calculated from the attention matrix of an additional encoder-decoder attention layer that is added on top of the final decoder layer, A i :\nP copy i (t \u2032 i ) = softmax(A i )(11)\nThe copy probability weight \u03b1 copy i is determined with the attention context vector c i , computed as a weighted sum of the attention values (i.e. linearly transformed encoder states) where the weights are defined by A i :\n\u03b1 copy i = sigmoid(W \u22a4 c i ).(12)\nThis copy-attention layer applies a source-side mask so that it only attends to the positions (J + 1, . . . , J + I) that correspond to the second input sequence t 1...I , and its implementation follows Zenkel et al. (2019). In particular, it uses a single attention head, no skip connection, and contains a separate output layer that predicts the target word based on its context vector with probability distribution P align i (\u2022). At training time both output layers are optimized jointly by defining the overall loss L as the weighted sum of both cross-entropy losses:\nL = L CE (P ) + \u03bbL CE (P align )(13)\n\u03bb is set to 0.05 in all experiments. We further apply source-word dropout (Junczys-Dowmunt et al., 2018), setting the full embedding vector for words in t to 1/p src with probability p src = 0.05.", "publication_ref": ["b22", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "B Results without finetuning", "text": "See Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "C Examples of System Output", "text": "See ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Full User Study Results", "text": "See   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Type:", "text": "Monolingual: technical s: Run further than you thought possible in the GEL-NIMBUS \u2122 21 LS running shoe for men by ASICS -packed full of plush cushioning and special technologies to respond to your natural stride.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "t:", "text": "Mit mit dem ASICS GEL-NIMBUS \u2122 21 LS Laufschuh f\u00fcr Herren mit weicher D\u00e4mpfung und speziellen Technologien , durch die sich der Schuh an deine nat\u00fcrlichen Schritte anpasst , l\u00e4ufst du weiter , als du es selbst je f\u00fcr m\u00f6glich gehalten hast .\nReference t \u2032 : Mit dem ASICS GEL-NIMBUS \u2122 21 LS Laufschuh f\u00fcr Herren mit weicher D\u00e4mpfung und speziellen Technologien , durch die sich der Schuh an deine nat\u00fcrlichen Schritte anpasst , l\u00e4ufst du weiter , als du es selbst je f\u00fcr m\u00f6glich gehalten hast .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "Predicted t \u2032 MT Mit mit dem Laufe weiter , als du es im GEL-NIMBUS \u2122 21 LS Laufschuh f\u00fcr Herren von ASICS f\u00fcr m\u00f6glich gehalten h\u00e4ttest -mit weicher D\u00e4mpfung und speziellen Technologien , durch die sich der Schuh an deine nat\u00fcrlichen Schritte anpasst , l\u00e4ufst du weiter , als du es selbst je f\u00fcr m\u00f6glich gehalten hast . um auf deinen nat\u00fcrlichen Schritt zu reagieren .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APE", "text": "No change to t", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BERT-APE Correctly predicts t \u2032 GEC", "text": "Correctly predicts t \u2032 TEC (ours) Correctly predicts t \u2032 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "506", "text": "Type:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bilingual s:", "text": "The DUOMAX \u2122 midsole offers smooth overpronation control by combining two different density materials to reduce the risk of flat feet and bunions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "t:", "text": "Die DUOMAX \u2122 -Mittelsohle bietet m\u00fchelos Halt bei \u00dcberpronation , indem zwei unterschiedliche Dichtematerialien kombiniert werden , um das Risiko von flachen F\u00fc\u00dfen und Fu\u00dfballen zu verringern .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Sai Gouravajhala, Yunsu Kim, Eric Wallace, and the other members of the Lilt research team and Berkeley NLP group for helpful discussion and feedback. We thank Morgan Raymond and Spence Green for their support in releasing the dataset. Finally, we are grateful to the professional translators who annotated the dataset and participated in the user study.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model MT", "text": "Die DUOMAX \u2122 -Mittelsohle bietet m\u00fchelos Halt bei eine reibungslose \u00dcberpronation , indem sie zwei unterschiedliche Dichtematerialien kombiniert , um das Risiko von flachen F\u00fc\u00dfen und Fu\u00dfballen B\u00fcndchen zu verringern .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APE", "text": "No change to t BERT-APE Die DUOMAX \u2122 -Mittelsohle bietet m\u00fchelos Halt bei \u00dcberpronation , indem zwei unterschiedliche Dichtematerialien kombiniert werden , um das Risiko von flachen F\u00fc\u00dfen und Fu\u00dfballen Baseballen zu verringern .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "GEC", "text": "No change to t TEC (ours) No change to t  ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Automatic annotation and evaluation of error types for grammatical error correction", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Christopher Bryant; Mariano Felice; Ted Briscoe"}, {"ref_id": "b1", "title": "Can automatic postediting improve NMT?", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Shamil Chollampatt; Raymond Hendy Susanto; Liling Tan; Ewa Szymanska"}, {"ref_id": "b2", "title": "A simple and effective approach to automatic postediting with transfer learning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "M Gon\u00e7alo;  Correia; F T Andr\u00e9;  Martins"}, {"ref_id": "b3", "title": "Better evaluation for grammatical error correction", "journal": "", "year": "2012", "authors": "Daniel Dahlmeier; Hwee Tou Ng"}, {"ref_id": "b4", "title": "2021. The language sector in eight charts", "journal": "", "year": "", "authors": "Donald A Depalma"}, {"ref_id": "b5", "title": "Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation", "journal": "", "year": "", "authors": "Markus Freitag; George Foster; David Grangier"}, {"ref_id": "b6", "title": "Detecting over/undertranslation errors for determining adequacy in human translations", "journal": "ArXiv", "year": "2021", "authors": "Prabhakar Gupta; Ridha Juneja; Anil Kumar Nelakanti; Tamojit Chatterjee"}, {"ref_id": "b7", "title": "A classification of errors in translation and revision", "journal": "", "year": "2009", "authors": "Gyde Hansen"}, {"ref_id": "b8", "title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Marcin Junczys; - Dowmunt; Roman Grundkiewicz"}, {"ref_id": "b9", "title": "Approaching neural grammatical error correction as a low-resource machine translation task", "journal": "Long Papers", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Shubha Guha; Kenneth Heafield"}, {"ref_id": "b10", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b11", "title": "Moses: Open source toolkit for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ond\u0159ej Bojar; Alexandra Constantin; Evan Herbst"}, {"ref_id": "b12", "title": "Unbabel's submission to the WMT2019 APE shared task: BERT-based encoder-decoder for automatic post-editing", "journal": "", "year": "2019", "authors": "V Ant\u00f3nio; M Lopes; Gon\u00e7alo M Amin Farajian; Jonay Correia; Andr\u00e9 F T Tr\u00e9nous;  Martins"}, {"ref_id": "b13", "title": "Ground truth for grammatical error correction metrics", "journal": "Short Papers", "year": "2015", "authors": "Courtney Napoles; Keisuke Sakaguchi; Matt Post; Joel Tetreault"}, {"ref_id": "b14", "title": "ESCAPE: a large-scale synthetic corpus for automatic post-editing", "journal": "", "year": "2018", "authors": "Matteo Negri; Marco Turchi; Rajen Chatterjee; Nicola Bertoldi"}, {"ref_id": "b15", "title": "Text Simplification by Tagging", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Kostiantyn Omelianchuk; Vipul Raheja; Oleksandr Skurzhanskyi"}, {"ref_id": "b16", "title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Keisuke Sakaguchi; Courtney Napoles; Matt Post; Joel Tetreault"}, {"ref_id": "b17", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b18", "title": "Rule-based translation with statistical phrase-based post-editing", "journal": "", "year": "2007", "authors": "Michel Simard; Nicola Ueffing; Pierre Isabelle; Roland Kuhn"}, {"ref_id": "b19", "title": "Predicting human translation quality", "journal": "", "year": "2014", "authors": "Lucia Specia; Kashif Shah"}, {"ref_id": "b20", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b21", "title": "Sentence level human translation quality estimation with attention-based neural networks", "journal": "European Language Resources Association", "year": "2020", "authors": "Yu Yuan; Serge Sharoff"}, {"ref_id": "b22", "title": "Adding interpretable attention to neural translation models improves word alignment", "journal": "", "year": "2019", "authors": "Thomas Zenkel; Joern Wuebker; John Denero"}, {"ref_id": "b23", "title": "Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Wei Zhao; Liang Wang; Kewei Shen; Ruoyu Jia; Jingming Liu"}, {"ref_id": "b24", "title": "Reference t \u2032 : Die DUOMAX \u2122 -Mittelsohle bietet m\u00fchelos Halt bei \u00dcberpronation , indem zwei unterschiedliche Dichtematerialien kombiniert werden , um das Risiko von Plattf\u00fc\u00dfen und Ballenzehen zu verringern", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Do your feet roll inwards when running? t: KIppen deine F\u00fc\u00dfe beim Laufen nach innen? t \u2032 : Kippen deine F\u00fc\u00dfe beim Laufen nach innen? Monolingual: grammar s: Own tough winter runs in the . . . t: Bei harten Winterl\u00e4ufe sorgt der . . . t \u2032 : Bei harten Winterl\u00e4ufen sorgt der . . . Monolingual: fluency s: The traffic emerges from the VPN server and . . . t: Der Verkehr wird vom VPN-Server ausgegeben und . . .t \u2032 : Der Datenverkehr wird vom VPN-Server ausgegeben und . . .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Bilinguals:Quad Core XEON E3-1501M, 2.9GHz t: Quad Core XEON 2,9 GHz t \u2032 : Quad Core XEON E3-1501M, 2,9 GHzPreferential s: VersaMax I / O auxiliary spring clamp style t: VersaMax Zusatz-E / A Federklemmenart t \u2032 : VersaMax Zusatz-E / A Federklemmenbauform", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure1: Overview of model architectures, pre-training data, and fine-tuning data for each approach to TEC. Transformer encoders and decoders are depicted as yellow and blue rectangles, respectively. The GEC and TEC models are pre-trained with synthetic corruptions of t \u2032 (t \u2032 corrupted ), as detailed in the description of the TEC and GEC models. APE uses MT-generated translations of s (t MT ) as synthetic data. BERT-APE is a state-of-the-art pre-trained APE model made available by.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: The interface used to show suggestions to reviewers in our user study.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Box plot showing quality rankings of segments reviewed with suggestions hidden vs shown. Lower is better. Notches indicate median quality rankings. Bars indicate the upper fence (3 rd quartile + IQR*1.5).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Error taxonomy for the ACED corpus, with examples from the dataset.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Corpus statistics for each dataset in the ACED corpus, including edit statistics on % of sentences that edited, and for edited sentences, the average number of edits and average edit distance.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Percentages of erroneous sentences that contain at least one error of each type for ASICS, EMERSON, DO, and the dev set of the WMT 2021 APE shared task. As a task, APE exhibits many more fluency errors than TEC.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "that generates the target sequence t \u2032 from left to right. All are pre-trained on 36M sentences from the WMT18", "figure_data": "ASICS EMERSONDOTrainTotal Edits Unique Edits606 4181436 4861212 940DevTotal Edits % in train246 14381 631004 21TestTotal Edits % in train287 23364 60766 21"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Edit and edit overlap statistics for each ACED dataset: total number of edits, unique edits in each train split, and percentage of total edits in each dev and test split that appear in the train split.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "ModelPrec. Rec. F0.5 Prec. Rec. F0.5 Prec. Rec. F0.5", "figure_data": "ASICSEMERSONDOMT GEC APE BERT-APE3.3 52.8 51.2 6.831.4 6.6 7.3 10.84.0 22.0 78.0 53.3 71.4 20.5 16.2 78.6 19.2 1.2 23.3 78.1 54.4 71.8 14.4 7.3 32.0 57.8 35.1 2.341.9 2.0 1.7 3.81.5 7.1 5.8 2.5TEC57.49.428.4 82.1 57.2 75.5 21.72.07.2"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Main results on ACED. Our fine-tuned TEC model outperforms on F 0.5 . The fine-tuned MT model scores highest on recall because it makes many edits, but at the cost of unacceptably low precision.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ". For all statis-"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Length-normalized medians from our user study for review times, number of characters the user inserted and deleted, and final Levenshtein edit distances from the original to the final translations. Data is split based on whether the suggestion was hidden or shown, and \"Suggestion Shown\" is further broken down according to whether the user accepted or declined the suggestion.", "figure_data": "SuggestionsShownHidden12345Quality Ranking (lower is better)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Given an additional input sequence t 1...I = (t 1 , . . . , t I ). the dual-source model used for the APE and TEC models is implemented by independently projecting t 1...I into the embedding space, adding an offset vector o and concatenating the embedding sequences. Equations 2 and 4 are rewritten asy 1...I = Et 1...I + p 1...I + o", "figure_data": ")A.2 Dual-Source Encoder-Decoder Model(7)"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "ASICS EMERSON DOModel Prec. Rec. F0.5 Prec. Rec. F0.5 Prec. Rec. F0.5", "figure_data": "MT GEC APE BERT-APE0.8 5.9 2.7 0.512.2 1.4 7.3 5.91.0 3.6 3.1 0.61.6 0.7 3.6 0.116.9 0.6 3.3 2.22.0 0.7 3.5 0.10.5 0.3 0.4 0.323.4 1.2 5.0 9.20.6 0.3 0.5 0.4TEC41.71.77.512.24.28.85.32.54.3"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Results without finetuning.KIppen deine F\u00fc\u00dfe beim Laufen nach innen ?Reference t \u2032 : Kippen deine F\u00fc\u00dfe beim Laufen nach innen ?BERT-APE No change to t GECCorrectly predicts t \u2032 TEC (ours) Correctly predicts t \u2032", "figure_data": "Type:Monolingual: technicals:Do your feet roll inwards when running?t:Model MT APEPredicted t \u2032 KIppen Rollen deine F\u00fc\u00dfe beim Laufen nach innen ? Correctly predicts t \u2032"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "A monolingual technical error the APE, GEC and TEC models edit correctly.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "A monolingual technical error the BERT-APE, GEC and TEC models edit correctly.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "h enc 1...J = encoder(x 1...J ),(2)", "formula_coordinates": [12.0, 125.74, 349.45, 163.39, 16.63]}, {"formula_id": "formula_1", "formula_text": "y \u2032 1...i\u22121 = Et \u2032 1...i\u22121 + p 1...i\u22121 (3) h dec i = decoder(y \u2032 1...i\u22121 , h enc 1...J )(4)", "formula_coordinates": [12.0, 101.64, 461.23, 187.5, 33.43]}, {"formula_id": "formula_2", "formula_text": "P i (t \u2032 i ) = softmax(h dec i E \u22a4 )(5)", "formula_coordinates": [12.0, 110.53, 496.12, 178.6, 16.56]}, {"formula_id": "formula_3", "formula_text": "L CE (P ) = \u2212 i log(P i (t \u2032 i )).(6", "formula_coordinates": [12.0, 115.28, 568.52, 169.62, 25.79]}, {"formula_id": "formula_4", "formula_text": "h dec i = decoder(y \u2032 1...i\u22121 , h enc 1...(J+I) ), ((8)", "formula_coordinates": [12.0, 104.84, 736.48, 184.29, 34.56]}, {"formula_id": "formula_5", "formula_text": ")9", "formula_coordinates": [12.0, 280.65, 755.67, 8.48, 13.15]}, {"formula_id": "formula_6", "formula_text": "P i (t \u2032 i ) = (1 \u2212 \u03b1 copy i )P i (t \u2032 i ) + \u03b1 copy i P copy i (t \u2032 i ),(10)", "formula_coordinates": [12.0, 316.95, 197.01, 207.48, 29.46]}, {"formula_id": "formula_7", "formula_text": "P copy i (t \u2032 i ) = softmax(A i )(11)", "formula_coordinates": [12.0, 359.64, 292.16, 164.78, 18.1]}, {"formula_id": "formula_8", "formula_text": "\u03b1 copy i = sigmoid(W \u22a4 c i ).(12)", "formula_coordinates": [12.0, 358.61, 388.39, 165.81, 18.1]}, {"formula_id": "formula_9", "formula_text": "L = L CE (P ) + \u03bbL CE (P align )(13)", "formula_coordinates": [12.0, 350.97, 566.9, 173.46, 22.34]}], "doi": "10.18653/v1/P17-1074"}