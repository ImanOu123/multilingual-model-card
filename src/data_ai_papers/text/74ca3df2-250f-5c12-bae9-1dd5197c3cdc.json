{"title": "Global Neural CCG Parsing with Optimality Guarantees", "authors": "Kenton Lee; Mike Lewis; Luke Zettlemoyer", "pub_date": "", "abstract": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "sections": [{"heading": "Introduction", "text": "Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015;Dyer et al., 2015) or reranking (Socher et al., 2013). We introduce the first global recursive neural parsing approach with optimality guarantees for decoding and use it to build a state-of-the-art CCG parser.\nTo enable learning of global representations, we modify the parser to search directly in the space of all possible parse trees with no dynamic programming. Optimality guarantees come from A * search, which provides a certificate of optimality if run to completion with a heuristic that is a bound on the future cost. Generalizing A * to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A * heuristics (Klein and Manning, 2003;Lewis and Steedman, 2014). Rather than directly replacing local models, we show that they can simply be augmented by adding a score from a global model that is constrained to be non-positive and has a trivial upper bound of zero. The global model, in effect, only needs to model the remaining non-local phenomena. In our experiments, we use a recent factored A * CCG parser (Lewis et al., 2016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure.\nFinding a model that achieves these A * guarantees in practice is a challenging learning problem. Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002;Huang et al., 2012). This condition is insufficient in our case, since it does not guarantee that the search will terminate in subexponential time. We instead introduce a new objective that optimizes efficiency as well as accuracy. Our loss function is defined over states of the A * search agenda, and it penalizes the model whenever the top agenda item is not a part of the gold parse.  from child nodes to a parent node represent rule productions scored by a parsing model. A path starting at \u2205, for example the set of bolded hyperedges, represents the derivation of a parse. During decoding, we find the highest scoring path to a complete parse. Both figures show an ideal exploration that efficiently finds the optimal path. Figure 1a depicts the traditional search space, and Figure 1b depicts the search space in this work. Hyperedge scores can only depend on neighboring nodes, so our model can condition on the entire parse structure, at the price of an exponentially larger search space.\nMinimizing this loss encourages the model to return the correct parse as quickly as possible.\nThe combination of global representations and optimal decoding enables our parser to achieve state-of-the-art accuracy for Combinatory Categorial Grammar (CCG) parsing. Despite being intractable in the worst case, the parser in practice is highly efficient. It finds optimal parses for 99.9% of held out sentences while exploring just 190 subtrees on average-allowing it to outperform beam search in both speed and accuracy.", "publication_ref": ["b27", "b9", "b21", "b14", "b15", "b17", "b22", "b5", "b12"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Overview", "text": "Parsing as hypergraph search Many parsing algorithms can be viewed as a search problem, where parses are specified by paths through a hypergraph.\nA node y in this hypergraph is a labeled span, representing structures within a parse tree, as shown in Figure 1. Each hyperedge e in the hypergraph represents a rule production in a parse. The head node of the hyperedge HEAD(e) is the parent of the rule production, and the tail nodes of the hyperedge are the children of the rule production. For example, consider the hyperedge in Figure 1b whose head is like bananas. This hyperedge represents a forward application rule applied to its tails, like and bananas.\nTo define a path in the hypergraph, we first include a special start node \u2205 that represents an empty parse. \u2205 has outgoing hyperedges that reach every leaf node, representing assignments of labels to words (supertag assignments in Figure 1). We then define a path to be a set of hyperedges E starting at \u2205 and ending at a single destination node. A path therefore specifies the derivation of the parse constructed from the labeled spans at each node. For example, in Figure 1, the set of bolded hyperedges form a path deriving a complete parse.\nEach hyperedge e is weighted by a score s(e) from a parsing model. The score of a path E is the sum of its hyperedge scores:\ng(E) = e\u2208E s(e)\nViterbi decoding is equivalent to finding the highest scoring path that forms a complete parse.\nSearch on parse forests Traditionally, the hypergraph represents a packed parse chart. In this work, our hypergraph instead represents a forest of parses. Figure 1 contrasts the two representations.\nIn the parse chart, labels on the nodes represent local properties of a parse, such as the category of a span in Figure 1a. As a result, multiple parses that contain the same property include the same node in their path, (e.g. the node spanning the phrase Fruit flies with category NP). The number of nodes in this hypergraph is polynomial in the sentence length, permitting exhaustive exploration (e.g. CKY parsing). However, the model scores can only depend on local properties of a parse. We refer to these models as locally factored models.\nIn contrast, nodes in the parse forest are labeled with entire subtrees, as shown in Figure 1b. For example, there are two nodes spanning the phrase Fruit flies with the same category NP but different internal substructures. While the parse forest requires an exponential number of nodes in the hypergraph, the model scores can depend on entire subtrees.\nA * parsing A * parsing has been successfully applied in locally factored models (Klein and Manning, 2003;Lewis and Steedman, 2014;Lewis et al., 2015;Lewis et al., 2016). We present a special case of A * parsing that is conceptually simpler, since the parse forest constrains each node to be reachable via a unique path. During exploration, we maintain the unique (and therefore highest scoring) path to a hyperedge e, which we define as PATH(e).\nSimilar to the standard A * search algorithm, we maintain an agenda A of hyperedges to explore and a forest F of explored nodes that initially contains only the start node \u2205.\nEach hyperedge e in the agenda is sorted by the sum of its inside score g(PATH(e)) and an admissible heuristic h(e). A heuristic h(e) is admissible if it is an upper bound of the sum of hyperedge scores leading to any complete parse reachable from e (the Viterbi outside score). The efficiency of the search improves when this bound is tighter.\nAt every step, the parser removes the top of the agenda, e max = argmax e\u2208A (g(PATH(e)) + h(e)). e max is expanded by combining HEAD(e max ) with previously explored parses from F to form new hyperedges. These new hyperedges are inserted into A, and HEAD(e max ) is added it to F. We repeat these steps until the first complete parse y * is explored. The bounds provided by h(e) guarantee that the path to y * has the highest possible score. Figure 1b shows an example of the agenda and the explored forest at the end of perfectly efficient search, where only the optimal path is explored.\nApproach The enormous search space described above presents a challenge for an A * parser, since computing a tight and admissible heuristic is difficult when the model does not decompose locally.\nOur key insight in addressing this challenge is that existing locally factored models with an informative A * heuristic can be augmented with a global score (Section 3). By constraining the global score to be non-positive, the A * heuristic from the locally factored model is still admissible.\nWhile the heuristic from the local model offers some estimate of the future cost, the efficiency of the parser requires learning a well-calibrated global score, since the heuristic becomes looser as the global score provides stronger penalties (Section 5).\nAs we explore the search graph, we incrementally construct a neural network, which computes representations of the parses and allows backpropagation of errors from bad search steps (Section 4).\nIn the following sections, we present our approach in detail, assuming an existing locally factored model s local (e) for which we can efficiently compute an admissible A * heuristic h(e).\nIn the experiments, we apply our model to CCG parsing, using the locally factored model and A * heuristic from Lewis et al. (2016).", "publication_ref": ["b14", "b15", "b16", "b17", "b17"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1", "fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Model", "text": "Our model scores a hyperedge e by combining the score from the local model with a global score that conditions on the entire parse at the head node:\ns(e) = s local (e) + s global (e)\nIn s global (e), we first compute a hidden representation encoding the parse structure of y = HEAD(e). We use a variant of the Tree-LSTM (Tai et al., 2015) connected to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) at the leaves. The combination of linear and tree LSTMs allows the hidden representation of partial parses to condition on both the partial structure and the full sentence. Figure 2 depicts the neural network that computes the hidden representation for a parse.\nFormally, given a sentence w 1 , w 2 , . . . , w n , we compute hidden states h t and cell states c t in the forward LSTM for 1 < t \u2264 n:\ni t =\u03c3(W i [c t\u22121 , h t\u22121 , x t ] + b i ) o t =\u03c3(W o [c t , h t\u22121 , x t ] + b o ) c t = tanh(W c [h t\u22121 , x t ] + b c ) c t =i t \u2022c t + (1 \u2212 i t ) \u2022 c t\u22121 h t =o t \u2022 tanh(c t )\nwhere \u03c3 is the logistic sigmoid, \u2022 is the componentwise product, and x t denotes a learned word embedding for w t . We also construct a backward LSTM, which produces the analogous hidden and cell states starting at the end of the sentence, which we denote as c t and h t respectively. The start and end latent states, c \u22121 , h \u22121 , c n+1 , and h n+1 , are learned embeddings. This variant of the LSTM includes peephole connections and couples the input and forget gates.\nThe bidirectional LSTM over the words serves as a base case when we recursively compute a hidden representation for the parse y using the treestructured generalization of the LSTM:\ni y = \u03c3(W R i [c l , h l , c r , h r , x y ] + b R i ) f y = \u03c3(W R f [c l , h l , c r , h r , x y ] + b R f ) o y = \u03c3(W R o [ c y , h l , h r , x y ] + b R o ) c lr = f y \u2022 c l + (1 \u2212 f y ) \u2022 c r c y = tanh(W R c [h l , h r , x y ] + b R c ) c y = i y \u2022 c y + (1 \u2212 i y ) \u2022 c lr h y = o y \u2022 tanh(c y )\nwhere the weights and biases are parametrized by the rule R that produces y from its children, and x y denotes a learned embedding for the category at the root of y. For example, in CCG, the rule would correspond to the CCG combinator, and the label would The cell state of the recursive unit is a linear combination of the intermediate cell state c y , the left cell state c l , and the right cell state c r . To preserve the normalizing property of coupled gates, we perform coupling in a hierarchical manner: the input gate i y decides the weights for c y , and the forget gate f y shares the remaining weights between c l and c r .\nGiven the hidden representation h y at the root, we score the global component as follows:\ns global (e) = log(\u03c3(W \u2022 h y ))\nThis definition of the global score ensures that it is non-positive-an important property for inference. Intuitively, this transformation requires A * to verify that the local score is good enough before computing the global score, which requires an incremental forward pass over a recursive unit in the neural network. In the example, this involves first summing the supertag scores of Fruit and flies and inserting the result back into the agenda. The score for applying the forward application rule to the recursive representations is only computed if that item appears again at the head of the agenda. In practice, the lazy global scoring reduces the number of recursive units by over 91%, providing a 2.4X speed up.", "publication_ref": ["b22", "b10"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Learning", "text": "During training (Algorithm 1), we assume access to sentences labeled with gold parse trees\u0177 and gold derivations\u00ca. The gold derivation\u00ca is a path from \u2205 to\u0177 in the parse forest.\nA * search with our global model is not guaranteed to terminate in sub-exponential time. This creates challenges for learning-for example, it is not possible in practice to use the standard structured perceptron update (Collins, 2002), because the search procedure rarely terminates early in training. Other common loss functions assume inexact search (Huang et al., 2012), and do not optimize efficiency.\nInstead, we optimize a new objective that is tightly coupled with the search procedure. During parsing, we would like hyperedges from the gold derivation to appear at the top of the agenda A. When this condition does not hold, A * is searching inefficiently, and we refer to this as a violation of the agenda, which we formally define as:\nv(\u00ca, A) = max e\u2208A (g(PATH(e)) + h(e)) \u2212 max e\u2208A\u2229\u00ca (g(PATH(e)) + h(e))\nwhere g(PATH(e)) is the score of the unique path to e, and h(e) is the A * heuristic. If all violations are zero, we find the gold parse without exploring any incorrect partial parses-maximizing both accuracy and efficiency. Figure 1b shows such a case-if any other nodes were explored, they would be violations. In existing work on violation-based updates, comparisons are only made between derivations with the same number of steps (Huang et al., 2012;-whereas our definition allows subtrees of arbitrary spans to compete with each other, because hyperedges are not explored in a fixed order. Our violations also differ from Huang et al.'s in that we optimize efficiency as well as accuracy.\nUpdate LOSS(V) Greedy V 1 Max violation max T t=1 V t All violations T t=1 V t\nWe define loss functions over these violations, which are minimized to encourage correct and efficient search. During training, we parse each sentence until either the gold parse is found or we reach computation limits. We record V, the list of nonzero violations of the agenda A observed:\nV = v(\u00ca, A) | v(\u00ca, A) > 0\nWe can optimize several loss functions over V, as defined in Table 1. The greedy and max-violation updates are roughly analogous to the violationfixing updates proposed by Huang et al. (2012), but adapted to exact agenda-based parsing. We also introduce a new all-violations update, which minimizes the sum of all observed violations. The allviolations update encourages correct parses to be explored early (similar to the greedy update) while being robust to parses with multiple deviations from the gold parse (similar to the max-violation update).\nThe violation losses are optimized with subgradient descent and backpropagation. For our experiments, s local (e) and h(e) are kept constant. Only the parameters \u03b8 of s global (e) are updated. Therefore, a subgradient of a violation v(\u00ca, A) can be computed by summing subgradients of the global scores. where e max denotes the hyperedge at the top of the agenda A and\u00ea max denotes the hyperedge in the gold derivation\u00ca that is closest to the top of A.", "publication_ref": ["b5", "b12", "b12", "b12"], "figure_ref": ["fig_1"], "table_ref": ["tab_0"]}, {"heading": "Algorithm 1 Violation-based learning algorithm", "text": "Definitions D is the training data containing input sentences x and gold derivations\u00ca. e variables denote scored hyperedges. TAG(x) returns a set of scored pre-terminals for every word. ADD(F , y) adds partial parse y to forest F. RULES(F , y) returns the set of scored hyperedges that can be created by combining y with entries in F . SIZE OK(F , A) returns whether the sizes of the forest and agenda are within predefined limits. 1: function VIOLATIONS(\u00ca, x, \u03b8) 2:\nV \u2190 \u2205 Initialize list of violations V 3:\nF \u2190 \u2205 Initialize forest F 4:\nA \u2190 \u2205 Initialize agenda A 5:\nfor e \u2208 TAG(x) do 6:\nPUSH(A, e) 7:\nwhile |A \u2229\u00ca| > 0 and SIZE OK(F , A) do 8:\nif v(\u00ca, A) > 0 then 9:\nAPPEND(V , v(\u00ca, A)) for i = 1 to T do 18:\nRecord\nfor\nx,\u00ca \u2208 D do 19: V \u2190 VIOLATIONS(\u00ca, x, \u03b8) 20: L \u2190 LOSS(V ) 21: \u03b8 \u2190 OPTIMIZE(L, \u03b8) 22:\nreturn \u03b8 6 Experiments", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "We trained our parser on Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development and Section 23 for test.\nTo recover a single gold derivation for each sentence to use during training, we find the right-most branching parse that satisfies the gold dependencies.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, s local (e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise. The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span. In the reported results, we back off to the supertag-factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 recursive units in the neural network.  Our full system is trained with all-violations updates. During training, we lower the forest size limit to 2000 to reduce training times. The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1. The LSTM cells and hidden states have 64 dimensions. We initialize word representations with pre-trained 50-dimensional embeddings from Turian et al. (2010). Embeddings for categories have 16 dimensions and are randomly initialized. We also apply dropout with a probability of 0.4 at the word embedding layer during training. Since the structure of the neural network is dynamically determined, we do not use mini-batches. The neural networks are implemented using the CNN library, 1 and the CCG parser is implemented using the EasySRL library. 2 The code is available online. 3", "publication_ref": ["b17", "b13", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN , which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser;  who combine a bidirectional LSTM supertagger with a beam search parser using global features ; and supertag-factored (Lewis et al., 2016), which uses deterministic A * decoding and an LSTM supertagging model.", "publication_ref": ["b2", "b30", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Parsing Results", "text": "Table 2 shows parsing results on the test set. Our global features let us improve over the supertagfactored model by 0.6 F1.    use global features, but our optimal decoding leads to an improvement of 0.4 F1.\nAlthough we observed an overall improvement in parsing performance, the supertag accuracy was not significantly different after applying the parser.\nOn the test data, the parser finds the optimal parse for 99.9% sentences before reaching our computational limits. On average, we parse 27.1 sentences per second, 4 while exploring only 190.2 subtrees.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Model Ablations", "text": "We ablate various parts of the model to determine how they contribute to the accuracy and efficiency of the parser, as shown in Table 3. For each model, the comparisons include the average number of parses explored and the percentage of sentences for which an optimal parse can be found without backing off.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Structure ablation", "text": "We first ablate the global score, s global (y), from our model, thus relying entirely on the local supertag-factors that do not explicitly model the parse structure. This ablation allows dynamic programming and is equivalent to the backoff model (supertag-factored in Table 3). Surprisingly, even in the exponentially larger search space, the global model explores fewer nodes than the supertag-factored model-showing that the global model efficiently prune large parts of the search space. This effect is even larger when not using dynamic programming in the supertag-factored model.\nGlobal structure ablation To examine the importance of global features, we ablate the recursive hidden representation (span-factored in spans, as in Durrett and Klein (2015). In this model, the recursive unit uses, instead of latent states from its children, the latent states of the backward LSTM at the start of the span and the latent states of the forward LSTM at the end of the span. Therefore, this model encodes the lexical information available in the full model but does not encode the parse structure beyond the local rule production. While the dynamic program allows this model to find the optimal parse with fewer explorations, the lack of global features significantly hurts its parsing accuracy.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Lexical ablation", "text": "We also show lexical ablations instead of structural ablations. We remove the bidirectional LSTM at the leaves, thus delexicalizing the global model. This ablation degrades both accuracy and efficiency, showing that the model uses lexical information to discriminate between parses.\nTo understand the importance of contextual information, we also perform a partial lexical ablation by using word embeddings at the leaves instead of the bidirectional LSTM, thus propagating only lexical information from within the span of each parse. The degradation in F1 is about half of the degradation from the full lexical ablation, suggesting that a significant portion of the lexical cues comes from the context of a parse. Figure 4 illustrates the importance of context with an incorrect partial parse that appears syntactically plausible in isolation. These bottom-up garden paths are typically problematic for parsers, since their incompatibility with the remaining sentence is difficult to recognize until later stages of decoding. However, our global model learns to heavily penalize these garden paths by using the context provided by the bidirectional LSTM  Our system uses all-violations updates and is the most accurate.  and avoid paths that lead to dead ends or bad regions of the search space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Update Comparisons", "text": "Table 4 compares the different violation-based learning objectives, as discussed in Section 5. Our novel all-violation updates outperform the alternatives. We attribute this improvement to the robustness over poor search spaces, which the greedy update lacks, and the incentive to explore good parses early, which the max-violation update lacks. Learning curves in Figure 5 show that the all-violations update also converges more quickly.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": ["tab_8"]}, {"heading": "Decoder Comparisons", "text": "Lastly, to show that our parser is both more accurate and efficient than other decoding methods, we decode our full model using best-first search, reranking, and beam search. Table 5 shows the F1 scores with and without the backoff model, the portion of the sentences that each decoder is able to parse, and the time spent decoding relative to the A * parser.\nIn the best-first search comparison, we do not include the informative A * heuristic, and the parser completes very few parses before reaching computational limits-showing the importance of heuristics in large search spaces. In the reranking comparison,  5: Comparison of various decoders using the same model from our full system (Global A * ). We report F1 with and without the backoff model, the percentage of sentences that the decoder can parse, and the time spent decoding relative to A * . we obtain n-best lists from the backoff model and rerank each result with the full model. In the beam search comparison, we use the approach from  which greedily finds the top-n parses for each span in a bottom-up manner. Results indicate that both approximate methods are less accurate and slower than A * .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Many structured prediction problems are based around dynamic programs, which are incompatible with recursive neural networks because of their realvalued latent variables. Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive. Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to guide the search.\nPrevious work on structured prediction with recursive or recurrent neural models has used beam search-e.g. in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013)-at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A * decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of , most related to this work, also uses global features. By using neural representations and exact search, we improve over their results.\nA * parsing has been previously proposed for lo-cally factored models (Klein and Manning, 2003;Pauls and Klein, 2009;Auli and Lopez, 2011;Lewis and Steedman, 2014). We generalize these methods to enable global features.  apply best-first search to an unlabeled shift-reduce parser. Their use of error states is related to our global model that penalizes local scores. We demonstrated that best-first search is infeasible in our setting, due to the larger search space.\nA close integration of learning and decoding has been shown to be beneficial for structured prediction. SEARN (Daum\u00e9 III et al., 2009) and DAG-GER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states. We similarly generate classification examples over hyperedges in the agenda, but actions from multiple states compete against each other. Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004;Daum\u00e9 III and Marcu, 2005;Huang et al., 2012;Andor et al., 2016;Wiseman and Rush, 2016), but the impact of search errors is unclear.", "publication_ref": ["b8", "b9", "b27", "b21", "b31", "b14", "b18", "b1", "b15", "b7", "b20", "b4", "b6", "b12", "b0", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have shown for the first time that a parsing model with global features can be decoded with optimality guarantees. This enables the use of powerful recursive neural networks for parsing without resorting to approximate decoding methods. Experiments show that this approach is effective for CCG parsing, resulting in a new state-of-the-art parser. In future work, we will apply our approach to other structured prediction tasks, where neural networks-and greedy beam search-have become ubiquitous.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Luheng He, Julian Michael, and Mark Yatskar for valuable discussion, and the anonymous reviewers for feedback and comments.\nThis work was supported by the NSF (IIS-1252835, IIS-1562364), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Globally Normalized Transition-Based Neural Networks", "journal": "", "year": "2016", "authors": "Daniel Andor; Chris Alberti; David Weiss; Aliaksei Severyn; Alessandro Presta; Kuzman Ganchev; Slav Petrov; Michael Collins"}, {"ref_id": "b1", "title": "Efficient CCG parsing: A* versus Adaptive Supertagging", "journal": "", "year": "2011", "authors": "Michael Auli; Adam Lopez"}, {"ref_id": "b2", "title": "Widecoverage Efficient Statistical Parsing with CCG and Log-Linear Models", "journal": "Computational Linguistics", "year": "2007", "authors": "Stephen Clark;  James R Curran"}, {"ref_id": "b3", "title": "The Java Version of the C&C Parser: Version 0.95", "journal": "", "year": "2015-08", "authors": "Stephen Clark; Darren Foong; Luana Bulat; Wenduan Xu"}, {"ref_id": "b4", "title": "Incremental Parsing with the Perceptron Algorithm", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Michael Collins; Brian Roark"}, {"ref_id": "b5", "title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Michael Collins"}, {"ref_id": "b6", "title": "Learning as search optimization: Approximate Large Margin Methods for Structured Prediction", "journal": "ACM", "year": "2005", "authors": "Hal Daum\u00e9; Iii ; Daniel Marcu"}, {"ref_id": "b7", "title": "Search-based structured prediction. Machine learning", "journal": "", "year": "2009", "authors": "Hal Daum\u00e9; Iii ; John Langford; Daniel Marcu"}, {"ref_id": "b8", "title": "Neural CRF Parsing", "journal": "", "year": "2015", "authors": "Greg Durrett; Dan Klein"}, {"ref_id": "b9", "title": "Transitionbased Dependency Parsing with Stack Long Short-Term Memory", "journal": "", "year": "2015", "authors": "Chris Dyer; Miguel Ballesteros; Wang Ling; Austin Matthews; Noah A Smith"}, {"ref_id": "b10", "title": "Long Short-term Memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b11", "title": "CCGbank: a Corpus of CCG derivations and Dependency Structures Extracted from the Penn Treebank", "journal": "Computational Linguistics", "year": "2007", "authors": "Julia Hockenmaier; Mark Steedman"}, {"ref_id": "b12", "title": "Structured Perceptron with Inexact Search", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Liang Huang; Suphan Fayong; Yang Guo"}, {"ref_id": "b13", "title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2014", "authors": "Diederik Kingma; Jimmy Ba"}, {"ref_id": "b14", "title": "A* Parsing: Fast Exact Viterbi Parse Selection", "journal": "", "year": "2003", "authors": "Dan Klein; D Christopher;  Manning"}, {"ref_id": "b15", "title": "A* CCG Parsing with a Supertag-factored Model", "journal": "", "year": "2014", "authors": "Mike Lewis; Mark Steedman"}, {"ref_id": "b16", "title": "Joint A* CCG Parsing and Semantic Role Labelling", "journal": "", "year": "2015", "authors": "Mike Lewis; Luheng He; Luke Zettlemoyer"}, {"ref_id": "b17", "title": "LSTM CCG Parsing", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Mike Lewis; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b18", "title": "K-best A* Parsing", "journal": "", "year": "2009", "authors": "Adam Pauls; Dan Klein"}, {"ref_id": "b19", "title": "Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP", "journal": "", "year": "", "authors": ""}, {"ref_id": "b20", "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "journal": "", "year": "2011-04-11", "authors": "St\u00e9phane Ross; Geoffrey J Gordon; Drew Bagnell"}, {"ref_id": "b21", "title": "Parsing with Compositional Vector Grammars", "journal": "", "year": "2013", "authors": "Richard Socher; John Bauer; D Christopher; Andrew Y Manning;  Ng"}, {"ref_id": "b22", "title": "Improved Semantic Representations from Tree-structured Long Short-term Memory Networks", "journal": "", "year": "2015", "authors": "Kai Sheng Tai; Richard Socher; Christopher D Manning"}, {"ref_id": "b23", "title": "Word representations: A Simple and General Method for Semi-supervised Learning", "journal": "", "year": "2010", "authors": "Joseph Turian; Lev Ratinov; Yoshua Bengio"}, {"ref_id": "b24", "title": "Efficient Structured Inference for Transition-Based Parsing with", "journal": "", "year": "2016", "authors": "Ashish Vaswani; Kenji Sagae"}, {"ref_id": "b25", "title": "", "journal": "Neural Networks and Error States. Transactions of the Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b26", "title": "Supertagging With LSTMs", "journal": "", "year": "2016", "authors": "Ashish Vaswani; Yonatan Bisk; Kenji Sagae; Ryan Musa"}, {"ref_id": "b27", "title": "Grammar as a Foreign Language", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Lukasz Kaiser; Terry Koo; Slav Petrov; Ilya Sutskever; Geoffrey Hinton"}, {"ref_id": "b28", "title": "Sequenceto-Sequence Learning as Beam-Search Optimization", "journal": "", "year": "2016", "authors": "Sam Wiseman; Alexander M Rush"}, {"ref_id": "b29", "title": "CCG Supertagging with a Recurrent Neural Network", "journal": "Short Papers", "year": "2015", "authors": "Wenduan Xu; Michael Auli; Stephen Clark"}, {"ref_id": "b30", "title": "LSTM Shift-Reduce CCG Parsing", "journal": "", "year": "2016", "authors": "Wenduan Xu"}, {"ref_id": "b31", "title": "Greed is Good if Randomized: New Inference for Dependency Parsing", "journal": "", "year": "2014", "authors": "Yuan Zhang; Tao Lei; Regina Barzilay; Tommi Jaakkola"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "The search space in chart parsing, with one node for each labeling of a span. The search space in this work, with one node for each partial parse.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Illustrations of CCG parsing as hypergraph search, showing partial views of the search space. Weighted hyperedges", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "FruitFigure 2 :2Figure 2: Visualization of the Tree-LSTM which computes vector embeddings for each parse node. The leaves of the Tree-LSTM are connected to a bidirectional LSTM over words, encoding lexical information within and outside of the parse.correspond to the CCG category.We assume that nodes are binary, unary, or leaves. Their left and right latent states, c l , h l , c r , and h r are defined as follows:\u2022 In a binary node, c l and h l are the cell and hidden states of the left child, and c r and h r are the cell and hidden states of the right child.\u2022 In a unary node, c l and h l are learned embeddings, and c r and h r are the cell and hidden states of the singleton child. \u2022 In a leaf node, let w denote the index of the corresponding word. Then c l and h l are c w and h w from the forward LSTM, and c r and h r are c w and h w from the backward LSTM.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "FruitFigure 3 :3Figure 3: The hyperedge on the left requires computing both the local and global score when placed on the agenda. Splitting the hyperedge, as shown on the right, saves expensive computation of the global score if the local score alone indicates that the parse is not worth exploring.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: Learning curves for the first 3 training epochs on the development set when training with different updates strategies.The all-violations update shows the fastest convergence.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Loss functions optimized by the different update methods. The updates depend on the list of T non-zero violations, V = V1, V2, . . . , VT , as defined in Section 5.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Labeled F1 for CCGbank dependencies on the CCGbank development and test set for our system Global A * and the baselines.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "also", "figure_data": "ModelDev F1 Optimal ExploredSupertag-factored87.5100.0% 402.5\u2212 dynamic program 87.5 Span-factored 87.997.1% 99.9%17119.6 176.5\u2212 dynamic program 87.8 Global A  *  88.4 87.8 \u2212 lexical inputs \u2212 lexical context 88.199.5% 99.8% 99.6% 99.4%578.5 309.6 538.5 610.5"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ablations of our full model (Global A * ) on the development set. Explored refers to the size of the parse forest.", "figure_data": "Results show the importance of global features and lexical in-formation in context."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "). The"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Parsing results trained with different update methods.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "g(E) = e\u2208E s(e)", "formula_coordinates": [3.0, 148.39, 81.39, 74.03, 23.44]}, {"formula_id": "formula_1", "formula_text": "s(e) = s local (e) + s global (e)", "formula_coordinates": [3.0, 364.51, 693.58, 124.19, 11.95]}, {"formula_id": "formula_2", "formula_text": "i t =\u03c3(W i [c t\u22121 , h t\u22121 , x t ] + b i ) o t =\u03c3(W o [c t , h t\u22121 , x t ] + b o ) c t = tanh(W c [h t\u22121 , x t ] + b c ) c t =i t \u2022c t + (1 \u2212 i t ) \u2022 c t\u22121 h t =o t \u2022 tanh(c t )", "formula_coordinates": [4.0, 116.97, 247.48, 239.99, 84.81]}, {"formula_id": "formula_3", "formula_text": "i y = \u03c3(W R i [c l , h l , c r , h r , x y ] + b R i ) f y = \u03c3(W R f [c l , h l , c r , h r , x y ] + b R f ) o y = \u03c3(W R o [ c y , h l , h r , x y ] + b R o ) c lr = f y \u2022 c l + (1 \u2212 f y ) \u2022 c r c y = tanh(W R c [h l , h r , x y ] + b R c ) c y = i y \u2022 c y + (1 \u2212 i y ) \u2022 c lr h y = o y \u2022 tanh(c y )", "formula_coordinates": [4.0, 105.77, 517.74, 159.26, 124.83]}, {"formula_id": "formula_4", "formula_text": "s global (e) = log(\u03c3(W \u2022 h y ))", "formula_coordinates": [4.0, 364.53, 579.19, 124.13, 18.93]}, {"formula_id": "formula_5", "formula_text": "v(\u00ca, A) = max e\u2208A (g(PATH(e)) + h(e)) \u2212 max e\u2208A\u2229\u00ca (g(PATH(e)) + h(e))", "formula_coordinates": [5.0, 335.11, 581.18, 182.98, 42.14]}, {"formula_id": "formula_6", "formula_text": "Update LOSS(V) Greedy V 1 Max violation max T t=1 V t All violations T t=1 V t", "formula_coordinates": [6.0, 137.48, 57.32, 100.06, 53.53]}, {"formula_id": "formula_7", "formula_text": "V = v(\u00ca, A) | v(\u00ca, A) > 0", "formula_coordinates": [6.0, 119.15, 371.72, 128.27, 18.93]}, {"formula_id": "formula_8", "formula_text": "Record", "formula_coordinates": [6.0, 479.98, 240.69, 25.9, 10.8]}, {"formula_id": "formula_9", "formula_text": "x,\u00ca \u2208 D do 19: V \u2190 VIOLATIONS(\u00ca, x, \u03b8) 20: L \u2190 LOSS(V ) 21: \u03b8 \u2190 OPTIMIZE(L, \u03b8) 22:", "formula_coordinates": [6.0, 313.2, 343.31, 155.3, 57.2]}], "doi": ""}