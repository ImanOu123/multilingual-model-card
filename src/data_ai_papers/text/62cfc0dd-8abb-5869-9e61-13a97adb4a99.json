{"title": "GLUCOSE: GeneraLized and COntextualized Story Explanations", "authors": "Nasrin Mostafazadeh; Aditya Kalyanpur; Lori Moon; David Buchanan; Lauren Berkowitz; Or Biran; Jennifer Chu-Carroll", "pub_date": "", "abstract": "Current affiliation Verneek, Inc.", "sections": [{"heading": "", "text": "When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal minitheories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a storyspecific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of\u02dc670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Humans make countless implicit commonsense inferences about everyday situations. For example, consider the following short story from the ROC-Stories corpus (Mostafazadeh et al., 2016): Gage was riding his bike. A car turned in front of him. Gage turned his bike sharply. He fell off of his bike. Gage skinned his knee. When even young children read this story, they construct a coherent representation of what happened and why, combining information from the text with relevant background knowledge (Kintsch and Van Dijk, 1978). For example, they can construct the causal chain that explains how the car's unexpected turn ultimately led to Gage falling, describe how Gage's emotion and location changed throughout the story, and even hypothesize that he likely shouted for help after falling.\nThough humans build such mental models with ease (Zwaan et al., 1995), AI systems for tasks such as reading comprehension and dialogue remain far from exhibiting similar commonsense reasoning capabilities. Two major bottlenecks have been acquiring commonsense knowledge and successfully incorporating it into state-of-the-art AI systems. To address the first bottleneck, we have built an effective platform to acquire causal commonsense knowledge at scale. To address the second, we show that pre-trained neural models can start to make similar inferences when trained on such rich curated data.\nWe introduce the GLUCOSE 1 (GeneraLized and COntextualized Story Explanations) dataset. Given a short story and a sentence X in the story, GLU-COSE captures ten dimensions of causal explanation related to X. These dimensions, inspired by human cognitive psychology, cover often-implicit causes and effects of X, including events, location, possession, and other attributes, the vast majority of which are not captured by existing resources 1 Human brain functions such as thinking, memory, and learning are closely linked to the glucose levels and how efficiently the brain uses this fuel source (Mergenthaler et al., 2013). If there is not enough glucose in the brain, neurotransmitters are not produced and communication between neurons breaks down. We are calling this resource GLUCOSE, since we believe AI brains need this source of fuel to enable their basic thinking and fill in their reasoning gaps!", "publication_ref": ["b19", "b12", "b37", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Dimension", "text": "Semi-structured Specific Statement and Inference Rule: antecedent connective consequent 1: Event that directly causes or enables X  and models. Importantly, GLUCOSE encodes commonsense knowledge in the form of semistructured inference rules 2 (mini-theories about the world), each grounded in a specific story.\nAs the examples in Table 1 demonstrate, the specific statements exemplify how the general rules can be grounded in a particular context.\nTo facilitate acquisition at scale, we designed an effective multi-stage crowdsourcing platform and used it to acquire more than 670K GLUCOSE annotations in the context of children's stories. Our analysis shows that these explanations extend substantially beyond the scope of the existing knowledge resources.\nGiven the breadth of commonsense knowledge 2 We will use \"inference rule\" and \"explanation\" interchangeably: the \"explanations\" we are interested in are inference rules that explain a given sentence's causes and effects. needed for real-world inference tasks, no static knowledge source is expected to provide sufficient coverage. GLUCOSE's key contribution is enabling models to dynamically produce general inference rules to explain novel scenarios. To systematically evaluate such models, we present an evaluation task where given a story S, a sentence X, and dimension d, a model predicts relevant specific and general rules as captured in GLUCOSE. We evaluate on the task using a curated test set, based on novel stories not used for any training purposes. We show a strong correlation between human and automatic evaluation metrics, which makes systematic and reliable evaluation of models feasible. We show that pre-trained neural models perform poorly on the task; however, when finetuned on GLUCOSE data, they are able to generate commonsense explanations that rival humans'. This finding supports our hypothesis that a promising recipe for giving machines commonsense is to use quality-monitored crowdsourced commonsense knowledge for training neural models that have preexisting lexical and conceptual knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Related Work", "text": "Recently, there has been a renewed interest in commonsense reasoning (Talmor et al., 2019;Tandon et al., 2019;Rashkin et al., 2018a;Zellers et al., 2018), further fostered by the increasing need for explainable AI systems (Yang et al., 2018).\nOne well-known type of commonsense knowledge is script knowledge, defined by Schank and Abelson (1977) as structured knowledge about stereotypical event sequences and their participants. However, manual encoding of such knowledge is notoriously unscalable and brittle. A more recent line of work is unsupervised learning of \"narrative schemas\" Jurafsky, 2008, 2009;Balasubramanian et al., 2013;Sha et al., 2016), where common event sequences are automatically induced from large corpora. While promising, this approach has not produced high-quality knowledge usable for downstream tasks at scale (Mostafazadeh et al., 2016). Furthermore, since commonsense knowledge is often implicit, such corpus-based methods are unlikely to induce implicit commonsense inferences (Gordon and Van Durme, 2013). In contrast, our data collection framework has enabled us to acquire high-quality and robust commonsense knowledge, including often unstated rules such as \"Someone A gives Someone B Something A Results in Someone B possesses Something A \" or \"Someone A is at Somewhere A Enables Someone A puts Something A at Somewhere A \".\nThe most fruitful efforts to date for acquiring commonsense knowledge have been crowdsourced knowledge resources. ConceptNet (Speer et al., 2017), a partially-crowdsourced resource, is a relational knowledge graph that connects short naturallanguage phrases via semantic edges. Most Con-ceptNet knowledge is taxonomic, consisting of factoids like \"apple is a fruit\", however, it also includes some causal relations, e.g., \"kill is motivated by revenge.\" Despite its broad coverage, ConceptNet has been found to be noisy (Zhou et al., 2019). Its knowledge also lacks context, hampering accurate application at inference time, e.g., \"kill requires eat breakfast\" is hard to make sense of without more context. A more directly relevant resource is ATOMIC , which consists of 877K textual descriptions of if-then knowledge. Each entry describes a likely cause/effect of one of 24K+ events. ATOMIC entries are organized into nine categories such as xIntent (PersonX's intention) and xEffect (effect on PersonX). For instance, \"PersonX makes PersonY's coffee xEffect PersonX gets thanked\". ATOMIC is a great step forward in acquiring highquality inferential knowledge. However, it has two main shortcomings. First, ATOMIC is noncontextual and conflates knowledge about an event that may have occurred under different scenarios, which hinders interpreting and applying the knowledge in context. For example, the event \"PersonX arrives the next day\" has xIntents \"to go on vacation\" and \"to attend a reunion,\" and xEffects \"get time to relax\" and \"meet some friends.\" Although each xIntent should be associated with only one of the xEffects, such dependencies are not encoded in ATOMIC. As a result, ATOMIC cannot be used to determine which xEffect is more likely given an xIntent. GLUCOSE addresses this by grounding each piece of inferential knowledge to a particular story context consistent across dimensions.\nSecond, events and relations in ATOMIC are person centric; agentless events are not covered, and each relation is either about PersonX or Per-sonY. As a result, ATOMIC cannot describe events involving common entity types such as places, things, or groups of people, nor can it encode causes and effects other than to PersonX and their peers. In GLUCOSE, sentence X can describe any event/state, and GLUCOSE general rules can refer to indexed variables such as \"Someone A \" or \"Somewhere C .\" Beyond these major shortcomings, ATOMIC also does not cover many commonsense knowledge types in GLUCOSE, including change of attributes such as location, which will be further discussed in Section 4.3.", "publication_ref": ["b31", "b32", "b24", "b35", "b34", "b27", "b0", "b28", "b19", "b9", "b30", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "The Knowledge Model of GLUCOSE", "text": "GLUCOSE has a unique take on explaining story events. As illustrated in Table 1, each story is explained through ten causal dimensions. The semistructured explanation for each dimension includes both a specific statement and a general rule.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Causal Dimensions of Explanation", "text": "One of our main contributions is the identification of ten causal dimensions of explanation in the context of narratives, for which we can reliably collect high quality data from lay crowd workers. Cognitive psychology research on human comprehension of narratives (Kintsch and Van Dijk, 1978;Zwaan and Radvansky, 1998;Grazzani et al., 2018) suggests that humans primarily focus on events, their timeline, locations of entities throughout the story, causes and motivations of events, and emotional trajectory of characters.\nBased on this research, GLUCOSE dimensions are designed to focus on causal reasoning around events and states, eliciting event causal chains, character motivations, emotions, naive psychology, and change of attributes such as location and possessions to core story entities. For an event or state X stated in a sentence, we categorize the dimensions of causality into events and states happening before X and those occurring after X. Each category includes five dimensions, as shown in Table 1. The precise definition and scope of these ten dimensions are the result of multiple pilot studies with crowd workers to identify intuitive and distinguishable causal dimensions, so that the overlap among dimensions is minimized and the agreement among workers is maximized.", "publication_ref": ["b12", "b38", "b10"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Semi-structured Inference Rules", "text": "To uncover what constitutes a good explanation, we ran several pilot studies exploring how people define, generate, and present explanations about short stories. We concluded that in order to achieve some consensus among explanations and to facilitate further processing and evaluation, the explanations should not be entirely free-form. Instead, we represent them as semi-structured inference rules whose expressivity lies between free text and logical forms. Each rule takes the form \"antecedent connective consequent,\" where the antecedent and consequent are composed by filling in syntactic slots for subject, verb, object(s), and preposition(s). For some dimensions, slot-filling involves choosing from a predefined list, e.g., dimension 2, which states a motivating emotion or basic human drive, limits its verb choices to feel, want, and like. Details regarding the slots can be found in Appendix A.\nTo eliminate the need for pronoun resolution when applying our general rules, variables are in-dexed, such as \"Someone A \" and \"Something A and Something B \", to refer to the same entities on both sides of the rule. Each variable can be further elaborated using an attribute phrase in the form of a relative clause, e.g., \"Somewhere C (that is Someone A 's location).\" Our studies indicate that this format gives the explainers sufficient expressivity to convey their reasoning, yet constrains the resulting explanations enough to identify commonalities between them. Note that the semi-structured rules are deterministically converted to natural language form by simply concatenating all the filled slots. Table 1 shows examples of semi-structured GLUCOSE explanations.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Generalized and Contextualized", "text": "Each GLUCOSE explanation is stated both as a specific statement (grounded in a given context) and a corresponding general rule (applicable to other contexts). Research in cognitive psychology suggests that humans typically choose which of an event's many causes to cite based on its relevance to the context (Miller, 2019). Hence, grounding explanations in context is crucial for acquiring accurate explanations. Furthermore, it has been shown that human explanations take situation-specific information and link it to pre-existing knowledge about the world; people explain by appealing to broader theories that enable generalization (Lombrozo, 2006). Also, there is evidence that explanations and generalizations help scaffold cognitive development in humans (Busch et al., 2018), which can potentially play a role in the learning capabilities of AI systems as well. By explicitly stating general rules as mini-theories of how the world works, GLUCOSE seeks to enable better generalization and causal reasoning in future AI systems.", "publication_ref": ["b18", "b16", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "The GLUCOSE Dataset", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Acquisition Platform", "text": "To enable developing models that can build mental models of narratives, we aimed to crowdsource a large, quality-monitored dataset. Beyond the scalability benefits, using crowd workers (as opposed to a small set of expert annotators) ensures diversity of thought, thus broadening coverage of a commonsense knowledge resource.\nThe annotation task is complex: it requires annotators to understand different causal dimensions in a variety of contexts and to come up with generalized theories beyond the story context. For strict quality control, we designed a three-stage knowledge acquisition pipeline for crowdsourcing the GLUCOSE dataset on the Amazon Mechanical Turk (Mturk) Platform. The workers first go through a qualification test 3 where they must score at least 90% on 10 multiple-choice questions on select GLUCOSE dimensions. Next, qualified workers can work on the main GLUCOSE data collection task: given a story S and a story sentence X, they are asked to fill in (allowing for nonapplicable) all ten GLUCOSE dimensions, getting step-by-step guidance from the GLUCOSE data acquisition UI. 4 To ensure data consistency, the same workers answer all dimensions for an S, X pair. Finally, the submissions are reviewed by an expert who rates each worker on a scale from 0 to 3, and provides feedback on how to improve. Our final UIs are the result of more than six rounds of pilot studies, iteratively improving the interaction elements, functionality, dimension definitions, instructions, and examples. 5 See Appendix B for more details on our crowdsourcing pipeline. 6", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Composition and Statistics", "text": "Our source of stories for the GLUCOSE dataset is ROCStories (Mostafazadeh et al., 2016). ROCStories consists of crowdsourced five-sentence everyday stories rich in causal and temporal relations, making them ideal for acquiring commonsense knowledge. We focus on children's stories due to their simpler language and concepts. We computed an estimated target age 7 for each story and sampled from the 5-8 age group. To ensure diverse viewpoints and hypotheses, each S, X pair was assigned to three workers. Data collection statistics are shown in Table 2 and Figure 1.\nAs Figure 1 shows, the causal dimensions (1 and 6) have the most representation (18.1% and 16.4%, respectively). As our examples in Table 1 show, specific statements for these dimensions sometimes 3 GLUCOSE qualification UI: https://bit.ly/34Pej0N 4 GLUCOSE main knowledge acquisition UI: https://bit.ly/ 2R8XcTt\n5 Our pilot studies helped narrow our dimensions from 18 down to 10 which workers could reliably distinguish. Notably, we collapsed Enable and Cause on which workers had significant disagreement.\n6 Additional information about the pipeline and data quality management can be found at https://tinyurl.com/y2pn5cgl\n7 Target age of individual stories was judged by age-ofacquisition and readability tests: Flesch-Kincaid Grade Level, the Coleman-Liau Index, and the Dale-Chall formula (Kuperman et al., 2012). It is important to note that this method depends on vocabulary and does not ensure that all content is appropriate for children in this age group.   define a causal connection over paraphrases of story sentences 8 , rather than introduce novel non-story content in either the antecedent or the consequent.\nTo estimate how prevalent this phenomenon is, we manually evaluated 100 random samples of specific rules for each of dimensions 1 and 6. We found that for 66% and 63% of the samples, for dimensions 1 and 6 respectively, at least one of the annotators contributed statements that contained inferences with non-story content. The new content includes events that are likely to follow from the story as well as world knowledge about story entities.", "publication_ref": ["b19", "b13"], "figure_ref": ["fig_0"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Comparison to Other Resources", "text": "To assess the novelty of GLUCOSE knowledge, we compared its coverage against that of the two most relevant commonsense resources: Concept-Net and ATOMIC. 9 We performed a best-effort mapping from GLUCOSE dimensions to relations in ConceptNet and ATOMIC. For example, GLU-COSE dimensions 1 and 6 are mapped to Concept-Net's Causes, HasSubevent, HasPrerequisite, and to ATOMIC's xEffect and oEffect. For all mappings see Appendix A. Since all three resources contain mostly naturallanguage entries, it is not possible to automatically quantify their precise overlap, so we adopted a  lenient evaluation scheme. For each GLUCOSE general rule 10 A relation B, we queried each target resource for tuples R (A , B ), where R is the resource's mapped equivalent of relation, and A and B consist of just the main verbs in A and B. Using fuzzy matching on A and B , we retrieved a large number of hits for the query, then filtered to those with >50% lexical overlap with the GLUCOSE rule.\nThe results, shown in Table 3, represent a ceiling in overlap with other resources. The results indicate that GLUCOSE captures extensive commonsense knowledge unavailable in existing resources. Note that GLUCOSE's knowledge model is a superset of ATOMIC's. GLUCOSE is designed to encompass all nine categories of inferential commonsense knowledge that ATOMIC covers, which are captured across different GLUCOSE dimensions. Note that there are definitely some individual pieces of knowledge that have been acquired in ATOMIC which do not exist in GLUCOSE, since some ATOMIC events may not have appeared in the GLUCOSE stories.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Empirical Evaluation Task", "text": "We set up a standalone evaluation task for evaluating models that predict GLUCOSE explanations: given a story S, a story sentence X, and a dimension d, provide an explanation in both specific and general forms.\nTest Set Curation For a test set on commonsense reasoning to offer accurate and reliable evaluation, it should contain unambiguous examples with clear gold answers. This led to a curation process that identifies examples on which humans have high agreement, as follows: we sampled S, X pairs annotated by any three workers with the highest quality rating. A dimension d for S, X was allowed into the test set if 1) d was annotated by all three workers, and 2) the three specific statements had a round-robin average sentence-level BLEU (Lin and Och, 2004) score 11 above 0.75. Finally, two in-house annotators manually removed cases with typographical or core content errors, resulting in a test set of 500 story/sentence pairs, each with 1-5 dimensions answered.\nHuman and Automatic Evaluation Human evaluation is crucial for any language generation task. We crowdsourced our human evaluation on MTurk, using a dedicated UI, 12 asking three of our top-rated crowd workers from the main GLUCOSE crowdsourcing job to rate the predictions. We set up the following evaluation process to ensure calibrated judgments: the judge first reads a story with a highlighted sentence X, then reads a question about X corresponding to a GLUCOSE dimension. Next, they are shown a shuffled list of candidate answers, each produced by a different system. Finally, the judge rates each candidate answer on a four-point Likert scale: \"completely incorrect,\" \"almost incorrect,\" \"almost correct,\" and \"completely correct.\" To compare system performance, the ratings are mapped to numerical scores of 0-3, which are then averaged.\nAutomatic evaluation for tasks involving language generation has been a major bottleneck for research (Liu et al., 2016;Hashimoto et al., 2019). BLEU's ease of replicability has made it a popular automated metric, but its correlation with human judgement has proven weak on various tasks (Novikova et al., 2017;Gatt and Krahmer, 2018). For automatic evaluation, we use SacreBLEU (Post, 2018) with equal weights up to 4-grams at corpuslevel on the three-reference test set. Using pairwise correlation analysis, we found strong correlation between human and BLEU scores on our test set, with correlation coefficients Spearman = 0.891, Pearson = 0.855, and Kendall's \u03c4 = 0.705, all with p-value < 0.001. The high correlation is due to various design choices, including 1) semi-structured inference rules in GLUCOSE are designed to be evaluable, where the structure constrains the variability of the rules, and 2) we minimized the noise in our human evaluation by designing a UI that could collect calibrated ratings from human judges educated about the task. The strong correlation suggests that BLEU is a viable metric for reporting future results on the GLUCOSE test set.", "publication_ref": ["b14", "b15", "b11", "b20", "b8", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We developed several models for tackling the prediction task described in Section 5. The train and development sets for each model consisted of the initial 440K total annotations 13 (in the context of 3,360 stories) in the GLUCOSE dataset, minus the entries that share the context story with the test instances.\nDue to their superior performance in sequence prediction, all our neural models use transformer blocks (Vaswani et al., 2017), which use multiheaded attention and fully connected layers to encode sequences. For decoding, all models use top-k random sampling (Fan et al., 2018). Details on all the models we experimented with can be found in Appendix C.", "publication_ref": ["b33", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Pretrained Language Model (PT-LM)", "text": "PT-LM tests what GLUCOSE-like knowledge is captured by the pretrained 774M-parameter GPT-2 (Radford et al., 2019) language model. We elicit commonsense explanations from GPT-2 by prompting it with the story followed by sentence X and a dimension-specific trigger word like \"because\", and allowing the model to complete the sentence. For best results, we implemented \"constrained decoding\" by conditioning the GPT-2 model on the input S, X as context, then generating the next token for a dimension d as follows: if dimension d's template specifies a set of allowable words at the current position-e.g., locative prepositions for dimensions 3 and 8-sample from the options based on their likelihood as conditioned on the preceding tokens. Otherwise, allow sampling freely from the entire vocabulary. See Appendix C for a list of all templates used.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Models Trained on GLUCOSE", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Language Models", "text": "We finetuned separate language models for specific and general rules. Each model monolithically covers all ten GLUCOSE dimensions: it generates rules given a dimension indicator as input. 14 Rules are sampled from the learned distribution p(s) = n i=1 p(s i | s 1 , . . . , s i\u22121 ), where s is the concatenation of input and output sequences. For 13 Table 2 shows the statistics of the final dataset, whereas all training for the models in the paper were conducted before the crowdsourcing of the dataset was finished. 14 We experimented with training separate models for each dimension, which yielded much worse results. all models in this section, we finetuned the PT-LM model described above.\nOne-sided Generation (1S-LM) One side of a GLUCOSE rule-the antecedent or the consequent, depending on the dimension-is always a paraphrase and/or a generalization of sentence X. In the one-sided model, we use X as is for this side of the specific statement; the model generates only the target side. Each training example is a text sequence S#X#d#answer#EOS, where d is the dimension number and answer is the target side. At test time, the model generates answer characters until it produces an EOS token.\nFull Rule Generation (Full-LM) Full-LM learns to produce the complete rule, including the connective and the paraphrase of X. Instead of just the target side of the rule, the training examples have the full rule as the answer portion of the sequence. This allows the model to produce more human-like rules, including paraphrasing and/or generalizing X appropriately.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Encoder-Decoder Model (Enc-Dec)", "text": "Our most complex model is an encoder-decoder transformer model that jointly predicts the specific and general rules. It maximizes p(y | x) = n i=1 p(y i | x; y 1 , . . . , y i\u22121 ), where x is the input and y is the answer. We obtained the best results by formulating the input as #d: S * [X], where d is the dimension and S * [X] is the story S with sentence X surrounded by asterisks. We chose to finetune the state-of-the-art T5 model (with 770Mparameters, to be comparable to the size of the LM model), using the same hyperparameters as in (Raffel et al., 2020).", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "Table 4 shows the results from the models described in Section 6, evaluated as per Section 5. It shows that Enc-Dec uniformly outperforms all other models, confirming that full visibility into context 15 helps an architecture better learn the intricacies of GLUCOSE rules.\nIn fact, Enc-Dec performs competitively with humans in many dimensions. The strength of this model's performance in predicting both specific  9  2.3 2.3 2.4 2.5 2.3 2.4 2.5 2.7 1.9 1 5 2.6 2.4 2.6 2.4 2.6 2.6 2.6 2  Gray and regular rows show results on general and specific rules, respectively. Human model's performance was computed by showing judges a randomly selected answer from the three gold references. We performed paired sample t-tests on the human evaluation scores for each dimension for Full-LM against Enc-Dec, and then again for Enc-Dec against Human. The vast majority of differences are statistically significant at p < 0.05, with the exceptions noted in asterisk. Note that the dimensions where performance differences are not statistically significant strongly correlate with those with the least amount of data, as shown in Figure 1.\nModel Dim 3: A location state that Enables X Dim 6: An event that X Causes/Enables", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_9", "tab_1", "tab_3"]}, {"heading": "Full-LM", "text": "Karen is at home Enables Karen made a pan of lasagna and brought it to the party Karen made lasagna Causes/Enables Karen ate lasagna SomeoneA is in SomewhereA Enables SomeoneA makes SomethingA (that is edible)\nSomeoneA cooks SomethingA (that is food) Causes/Enables Some PeopleA to be turned away because of SomethingA (that is food)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Enc-Dec", "text": "Karen is in the kitchen Enables Karen makes a pan of lasagna Karen makes a pan of lasagna Causes/Enables Karen eats it for a week SomeoneA is in a kitchen Enables SomeoneA cooks SomethingA SomeoneA makes SomethingA (that is food) Causes/Enables SomeoneA eats SomethingA Human Karen is in the kitchen Enables Karen made a pan of lasagna Karen made a pan of lasagna Causes/Enables She brought it to a party SomeoneA is in a kitchen Enables Some-oneA prepares SomethingA (that is a dish) SomeoneA prepares SomethingA (that is a dish) Causes/Enables SomeoneA takes SomethingA to SomethingB (that is an event)\nTable 5: Example model generations for the input story: Karen made a pan of lasagna. She brought it to the party. Nobody wanted to eat lasagna. Karen ate it for a week. She became tired of lasagna. (Sentence X is underlined.) Note that all test stories are unseen in the train or validation set. and general rules is a testament to the high quality of the GLUCOSE training data. Its worst performance is on general rules for dimensions 5 and 10, which have the lowest number of training points and are the most diverse in content.\nOther models perform as expected. PT-LM's poor performance shows that finetuning on our dataset significantly improves the commonsense inference capabilities of LMs. 1S-LM, which only predicts half of an inference rule, outperforms Full-LM in predicting specific statements, but lacks the ability to generalize them. We also tested various other baselines, including an ATOMIC-trained transformer model , retrieval of K-nearest-neighbors, and non-contextual variants of the presented models, all of which significantly underperformed the results in Table 4, and are presented in Appendix C.\nOur results also show that our best models perform noticeably better on specific statements than on general rules. This is because generating a specific statement involves paraphrasing a story sentence and predicting an antecedent/consequent, while a general rule requires further generalizing the paraphrase and the antecedent/consequent appropriately such that the rule remains a generally valid statement about the world.\nAlthough rule generalization can sometimes be as simple as replacing a named entity (e.g., Gage) with a typed variable (Someone A ), more often more complex transformations are needed, such as generalizing the action and producing type constraints on variables in the form of attribute phrases. For example, take into account the Enc-Dec results in Table 5. For dimension 3, the generalization of the story sentence, Karen makes a pan of lasagna, included generalizing Karen to Someone A and makes a pan of lasagna to cooks Something A . Note that sentence generalizations are dimension-specific: For dimension 6, the generalization of same sentence retains the verb make but adds a type constraint to the object, Something A (that is a food), which is required for making the rule generally valid. Table 1 shows another complex transformation example where turning his bike is generalized into moves away from Something (that is dangerous), that takes into account story context.\nOverall, our evaluation results show that the state-of-the-art pre-trained models finetuned on the GLUCOSE dataset are well capable of dynamically producing GLUCOSE-like inference rules on the fly, which is the ultimate usecase of the GLU-COSE dataset. It is important to note that there is still a consistent performance gap between the best-performing model and human's on generating specific statements and general rules, which indicates that there is still a large headroom for improvement on designing better models for generalizable commonsense reasoning.\nNote that in our current evaluation setup, we have made the simplifying assumption of evaluating each dimension for each sentence individually, without consideration for consistency across dimensions or across sentences. Joint prediction of all the dimensions and sentences across the story is a considerably more challenging task that can potentially yield more accurate predictions for a downstream task. We encourage the future work to focus on building models that perform joint predictions, which can be readily evaluated using our test-set. It is important to note that static test sets are inherently narrow and prone to hidden curation biases (Sharma et al., 2018;Belinkov et al., 2019). We believe that the ultimate evaluation for models that show GLUCOSE-like commonsense reasoning capabilities should be on naturally-occurring arbitrary stories and through our presented human evaluation process. As future work, we are planning to show the value of incorporating GLUCOSE-trained models in other downstream NLP tasks such as reading comprehension and dialog.", "publication_ref": ["b29", "b1"], "figure_ref": [], "table_ref": ["tab_9", "tab_1"]}, {"heading": "Conclusions", "text": "We introduced GLUCOSE, a large-scale dataset of implicit commonsense knowledge, encoded as ex-planatory mini-theories grounded in a narrative context. The theories are categorized into ten causal dimensions, inspired by cognitive psychology.\nWe presented our multi-stage pipeline for acquiring semi-structured causal explanations at scale from lay workers, resulting in\u02dc670K annotations in the context of everyday children's stories. We demonstrated the utility of GLUCOSE data in two ways. First, our analysis showed that GLUCOSE rules capture knowledge not available in existing resources or pre-trained models. Second, in order to evaluate how well AI models can predict GLU-COSE knowledge on novel inputs, the ultimate value of such a dataset, we defined a standalone evaluation task for predicting specific and general inference rules given a story/sentence pair and a dimension. We curated a doubly-vetted test set, developed a platform to facilitate human judgment of system outputs, and validated BLEU as a strong automated evaluation metric. We show that training on GLUCOSE data improves model performances significantly on unseen stories.\nOur results validate our hypothesis that a promising approach for imbuing machines with commonsense is to use carefully-crafted data, as in GLU-COSE, to train neural architectures that have a wide range of lexical and conceptual knowledge encoded, as in models pretrained on large corpora. Together with this paper, we release our dataset 16 and models 17 , which we hope will enable the AI research community to explore effective approaches to incorporate commonsense reasoning capabilities into various downstream tasks.\nproper temporal understanding of the stories (Figure 4). Understanding generalization is the most difficult, and the most important, aspect of our task. Assessing the prospective workers' understanding of generalization was done through curating questions demonstrating under-generalization or over-generalization. The full Qualification UI, along with all the detailed instructions that were visible to the workers, is accessible here https: //bit.ly/34Pej0N.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Main Task", "text": "The qualified workers were able to access large batches of data with no limit. The main task starts with a page like the one shown in the Figure 5. The user loops through each of the 10 dimensions of GLUCOSE data collection, in order, presented as questions. Note that the user could answer the question by simply marking the dimension as not applicable and skipping it. If they choose to answer, as shown in Figure 6, they will be presented with the structured rule slots to input their answers. The full Main GLUCOSE UI, along with all the detailed instructions that were visible to the workers, is accessible here https://bit. ly/2R8XcTt. Expert Review For work contributed through the main UI, data quality was controlled through daily monitoring of a percentage of incoming submissions and statistics on average dimensions filled out. For managing this process, we built a specialized UI for reviewing the incoming structured data. The percentage of answers reviewed by an in-house expert were used to update worker ratings. Workers enter the task with a score of \"1\", then advance to \"2\" as they become more proficient, getting a bonus increase. The top rating is \"3\". Select workers with a \"3\" rating were also moved into \"top rated\" batches that paid more per HIT and included higher bonuses and incentives. If work quality dropped, workers' ratings were adjusted accordingly. If their work was at a risk of degrading the quality of the dataset, they were disqualified from the task. 19", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Appendix C: Details on the Models", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ATOMIC-trained Model", "text": "This model is a transformer language model, specifically GPT-1 architecture, fine-tuned on ATOMIC resource. The language model is fine-tuned to generate triplet sequences such as 'PersonX goes to     The answer-entry part of the main UI. When \"Yes\" is selected for \"Your Answer\" on the main UI for GLUCOSE data collection, the workers can input answers to the dimension in question.\nthe mall <xIntent >to buy clothes'. We use the same exact model trained for . This model is only applicable to General Rule prediction. The results from this model were significantly worse than the PT-LM model, which is the worst-performing model presented in the main paper. This was expected, given the little overlap that exists between the ATOMIC dataset and the GLU-COSE knowledge, as presented in the main paper under \"Comparison to Other Resources\" Section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "K-Nearest Neighbor (KNN)", "text": "For a given test pair S, X, the KNN baseline retrieves the K most similar training instances and returns one as the prediction. It uses BERT (Devlin et al., 2019) sentence embeddings to compute cosine similarity between a candidate and each retrieved training instance. We tuned three parameters on the development set: K, min sim, and max sim. If a candidate has a similarity score above max sim, it is emitted as the prediction. Otherwise, candidates scoring below min sim are dropped, and the centroid among the remaining pool is emitted. We evaluate KNN only for general rules, since it is not meaningful to retrieve specific statements from the training set. The results from this model were significantly worse than the PT-LM model, which is the worst-performing model presented in the main paper. The performance of the KNN model highlights the importance of generalizing beyond the training data.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Pretrained Language Model (PT-LM)", "text": "We experimented with prompting the pretrained language models, specifically GPT-2, as is, for predicting GLUCOSE dimensions. Table 8 shows the list of particular templates used for decoding. We used 774M-parameter GPT-2 model, with top-K random sampling for decoding, with K = 15. The decoding for this model was done on CPU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "1S-LM and Full-LM", "text": "This model uses the exact model as with PT-LM. These models were finetuned on 8 NVIDIA Tesla V100 GPUs for 10K steps.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Enc-Dec Model", "text": "We finetuned the 770M-parameter pre-trained T5 model using the exact same hyperparameters as in (Raffel et al., 2020). We have used top-K random   sampling for decoding, with K = 15. We did the training and decoding for this model on Google TPU v3-8. We trained this model for 500k steps after pre-training, which took about 72 hours. We also experimented with non-contextual version of all the models presented in the main paper. For non-contextual models, the story S is simply re-moved from the input. The non-contextual models all underperformed their contextual counterparts. This further validates the importance of using context in making commonsense inferences.  [am, is, are, has, have, want, wants, need, needs] Table 8: Templates used for turning the ten dimensions for GLUCOSE data into natural language statements for decoding proper sequences from the pre-trained language models.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank the hundreds of amazing crowdworkers whose dedication made this work possible. We thank David Ferrucci for his valuable insights and support throughout the GLUCOSE project. We thank Andy Beck for the discussions around the GLUCOSE knowledge model and Jesse Dunietz for his discussions on the paper. We are grateful for the invaluable comments of the anonymous reviewers, Niranjan Balasubramanian, and Owen Rambow on this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Appendix A: The Knowledge Model for Collecting GLUCOSE data", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semi-structured Inference Rules", "text": "The knowledge represented in GLUCOSE is captured in the form of semi-structured inference rules that are accompanied by a specific statement that grounds the rule in the context of a specific story. Each specific statement and its corresponding general rule use the common template of antecedent connective consequent. The antecedent and consequent are each composed by filling in a few syntactic slots, namely, subject, verb, object(s), and preposition(s). In order to further shape the semantics of the acquired knowledge, some of these slots have a pre-defined list of options to choose from.\nTable 6 lists the pre-defined options for filling in the syntactic slots per GLUCOSE dimension 18 . Some of the slots allow adding a custom entry to the list of options, hence soft constraints, and some do not, hence hard constraints. Note that beyond the options listed in this table, the general rule slots across all the dimensions have pre-defined options for subject and object slots such as Someone A or Some People C .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison to Other Resources", "text": "To assess the value of the GLUCOSE dataset, we compared its coverage against the two most relevant commonsense knowledge resources: Concept-Net and ATOMIC. Table 7 shows our best-effort mapping among knowledge dimensions of GLU-COSE and relations in ConceptNet and ATOMIC.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix B: Data Collection Pipeline", "text": "To ensure obtaining our desired quality, we designed a three-stage knowledge acquisition pipeline for crowdsourcing the GLUCOSE dataset on the Amazon Mechanical Turk (Mturk): The qualification test, the main task, and the expert review. In this Section we provide more detail about each stage and its designated UI design.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Qualification Test", "text": "The qualification test contained questions testing workers' understanding in three areas: Identifying correct use of the UI slots for composing their answers (Figure 2), recognizing the right level of generalization (Figure 3), and identifying causes and effects with", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Generating coherent event schemas at scale", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Niranjan Balasubramanian; Stephen Soderland; Mausam ; Oren Etzioni"}, {"ref_id": "b1", "title": "On adversarial removal of hypothesis-only bias in natural language inference", "journal": "", "year": "2019", "authors": "Yonatan Belinkov; Adam Poliak; Stuart Shieber; Benjamin Van Durme; Alexander Rush"}, {"ref_id": "b2", "title": "COMET: Commonsense transformers for automatic knowledge graph construction", "journal": "", "year": "2019", "authors": "Antoine Bosselut; Hannah Rashkin; Maarten Sap; Chaitanya Malaviya; Asli Celikyilmaz; Yejin Choi"}, {"ref_id": "b3", "title": "Explanation scaffolds causal learning and problem solving in childhood", "journal": "Springer", "year": "2018", "authors": "T A Justin;  Busch; K Aiyana; Cristine H Willard;  Legare"}, {"ref_id": "b4", "title": "Unsupervised learning of narrative event chains", "journal": "Association for Computational Linguistics", "year": "2008", "authors": "Nathanael Chambers; Dan Jurafsky"}, {"ref_id": "b5", "title": "Unsupervised learning of narrative schemas and their participants", "journal": "", "year": "2009", "authors": "Nathanael Chambers; Dan Jurafsky"}, {"ref_id": "b6", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b7", "title": "Hierarchical neural story generation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"ref_id": "b8", "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation", "journal": "J. Artif. Int. Res", "year": "2018", "authors": "Albert Gatt; Emiel Krahmer"}, {"ref_id": "b9", "title": "Reporting bias and knowledge acquisition", "journal": "ACM", "year": "2013", "authors": "Jonathan Gordon; Benjamin Van Durme"}, {"ref_id": "b10", "title": "The relation between emotion understanding and theory of mind in children aged 3 to 8: The key role of language", "journal": "Frontiers in Psychology", "year": "2018", "authors": "Ilaria Grazzani; Veronica Ornaghi; Elisabetta Conte; Alessandro Pepe; Claudia Caprin"}, {"ref_id": "b11", "title": "Unifying human and statistical evaluation for natural language generation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tatsunori Hashimoto; Hugh Zhang; Percy Liang"}, {"ref_id": "b12", "title": "Toward a model of text comprehension and production", "journal": "Psychological review", "year": "1978", "authors": "Walter Kintsch;  Teun A Van Dijk"}, {"ref_id": "b13", "title": "Age-of-acquisition ratings for 30,000 english words", "journal": "", "year": "2012", "authors": "Victor Kuperman; Hans Stadthagen-Gonzalez; Marc Brysbaert"}, {"ref_id": "b14", "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "journal": "", "year": "2004", "authors": "Chin-Yew Lin; Franz Josef Och"}, {"ref_id": "b15", "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Chia-Wei Liu; Ryan Lowe; Iulian Serban; Mike Noseworthy; Laurent Charlin; Joelle Pineau"}, {"ref_id": "b16", "title": "The structure and function of explanations", "journal": "Trends in Cognitive Sciences", "year": "2006", "authors": "Tania Lombrozo"}, {"ref_id": "b17", "title": "Sugar for the brain: the role of glucose in physiological and pathological brain function", "journal": "Trends in neurosciences", "year": "2013", "authors": "P Mergenthaler; U Lindauer; G A Dienel; A Meisel"}, {"ref_id": "b18", "title": "Explanation in artificial intelligence: Insights from the social sciences", "journal": "Artificial Intelligence", "year": "2019", "authors": "Tim Miller"}, {"ref_id": "b19", "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "journal": "", "year": "2016", "authors": "Nasrin Mostafazadeh; Nathanael Chambers; Xiaodong He; Devi Parikh; Dhruv Batra; Lucy Vanderwende; Pushmeet Kohli; James Allen"}, {"ref_id": "b20", "title": "Why we need new evaluation metrics for NLG", "journal": "", "year": "2017", "authors": "Jekaterina Novikova; Ond\u0159ej Du\u0161ek; Amanda Cercas Curry; Verena Rieser"}, {"ref_id": "b21", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b22", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b23", "title": "Exploring the limits of transfer learning with a unified text-totext transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b24", "title": "Modeling naive psychology of characters in simple commonsense stories", "journal": "Long Papers", "year": "2018", "authors": "Hannah Rashkin; Antoine Bosselut; Maarten Sap; Kevin Knight; Yejin Choi"}, {"ref_id": "b25", "title": "Event2Mind: Commonsense inference on events, intents, and reactions", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Maarten Hannah Rashkin; Emily Sap; Noah A Allaway; Yejin Smith;  Choi"}, {"ref_id": "b26", "title": "ATOMIC: An atlas of machine commonsense for ifthen reasoning", "journal": "", "year": "2019", "authors": "Maarten Sap; Emily Ronan Le Bras; Chandra Allaway; Nicholas Bhagavatula; Hannah Lourie; Brendan Rashkin; Noah Roof; Yejin Smith;  Choi"}, {"ref_id": "b27", "title": "Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures. L. Erlbaum, Hillsdale", "journal": "", "year": "1977", "authors": "C Roger; Robert P Schank;  Abelson"}, {"ref_id": "b28", "title": "Joint learning templates and slots for event schema induction", "journal": "", "year": "2016", "authors": "Lei Sha; Sujian Li; Baobao Chang; Zhifang Sui"}, {"ref_id": "b29", "title": "Tackling the story ending biases in the story cloze test", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rishi Sharma; James Allen; Omid Bakhshandeh; Nasrin Mostafazadeh"}, {"ref_id": "b30", "title": "Conceptnet 5.5: An open multilingual graph of general knowledge", "journal": "", "year": "2017", "authors": "Robyn Speer; Joshua Chin; Catherine Havasi"}, {"ref_id": "b31", "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge", "journal": "Long and Short Papers", "year": "2019", "authors": "Alon Talmor; Jonathan Herzig; Nicholas Lourie; Jonathan Berant"}, {"ref_id": "b32", "title": "reasoning over procedural text", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Niket Tandon; Bhavana Dalvi; Keisuke Sakaguchi; Peter Clark; Antoine Bosselut"}, {"ref_id": "b33", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b34", "title": "Commonsense justification for action explanation", "journal": "", "year": "2018", "authors": "Shaohua Yang; Qiaozi Gao; Sari Sadiya; Joyce Chai"}, {"ref_id": "b35", "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference", "journal": "", "year": "2018", "authors": "Rowan Zellers; Yonatan Bisk; Roy Schwartz; Yejin Choi"}, {"ref_id": "b36", "title": "Predicting conceptnet path quality using crowdsourced assessments of naturalness", "journal": "ACM", "year": "2019", "authors": "Yilun Zhou; Steven Schockaert; Julie Shah"}, {"ref_id": "b37", "title": "The construction of situation models in narrative comprehension: An event-indexing model", "journal": "Psychological Science", "year": "1995", "authors": "Rolf A Zwaan; Mark C Langston; Arthur C Graesser"}, {"ref_id": "b38", "title": "Situation models in language comprehension and memory", "journal": "Psychological bulletin", "year": "1998", "authors": "A Rolf;  Zwaan;  Gabriel A Radvansky"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Number of rules collected for each dimension. Dimensions 1 and 6 have the most representation, while dimensions 9 and 10 are most often marked as not applicable.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Example qualification question about the correct use of the slots.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Example qualification question about the correct level of generalization.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Example qualification question about understanding causal relations between events.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: The preview page of the Main UI for GLUCOSE data collection, which can be accessed via https: //bit.ly/2R8XcTt.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "FigureFigure6: The answer-entry part of the main UI. When \"Yes\" is selected for \"Your Answer\" on the main UI for GLUCOSE data collection, the workers can input answers to the dimension in question.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "A car turned in front of him Causes/Enables Gage turned his bike SomethingA turns in front of SomethingB (that is SomeoneA's vehicle) Causes/Enables", "figure_data": "subjectverbpreposition objectsubjectverbobjectsubjectverbprepositionobjectSomeoneA turns SomethingB away from SomethingAsubjectverbobject1prepositionobject22: Emotion orGage wants safety Causes/Enables Gage turned his bikebasic humansubject verbobjectsubjectverbobjectdrive thatSomeoneA wants safety Causes/Enables SomeoneA moves away from SomethingA (that is dangerous)motivates XsubjectverbobjectsubjectverbprepositionobjectGage was close to a car Enables Gage turned his bike away from the car3: Locationsubject verb preposition objectsubjectverbobject1prepositionobject2state that enables XSomeoneA is close to SomethingA Enables SomeoneA moves away from SomethingA subject verb preposition object subject verb preposition objectGage possesses a bike Enables Gage turned his bike4: Possessionsubjectverbobjectsubjectverbobjectstate that enables XSomeoneA possesses SomethingA Enables SomeoneA moves SomethingA subject verb object subject verb object5: Other attributes enabling X: N/A (the dimension is not applicable for this example)6: Event that XGage turned his bike Causes/Enables He fell off his bikedirectly causessubjectverbobjectsubject verbobjector enablesSomeoneA turns SomethingB (that is SomeoneA's vehicle) Causes/Enables SomeoneA falls off SomethingBsubjectverbobjectsubjectverbobject7: An emotion that is caused by X: N/A8: A change insubjectverbobject1prepositionobject2subject verb object1 preposition object2location that Xresults insubjectverbprepositionobjectsubjectverbprepositionobject"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Statistics about the GLUCOSE dataset.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ceiling overlap between GLUCOSE and other resources. Omitted dimensions had no overlap.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Human evaluation scores for dimension... BLEU scores for dimension...", "figure_data": "Model123 456 7 891012345678910PT-LM 0.7 1.0 1.2 1.0 0.6 0.6 0.6 0.9 0.7 1.140.7 36.5 31.3 31.4 30.2 32.1 23.1 37.0 40.9 53.11S-LM 2.1 2.3 2.2 2.5 2.1 2.1 2.4 2.5 2.1 1.855.1 59.6 50.7 65.2 53.1 57.4 55.4 71.7 56.8 67.2Full-LM1.8 2.0 2.0 2.2 1.7 2.0 2.1 2.2 1.6 2.1 1.6 1.6 1.8 2.1 1.8 1.9 1.9 2.1 1.1 1.554.7 55.3 51.0 64.4 50.5 58.8 66.2 73.4 32.7 67.0 56.4 55.8 57.5 62.7 59.6 59.0 65.8 67.7 53.7 56.2Enc-Dec2.7 2.7 2.6 2.7 2.5* 2.6 2.7 2.8 2.2 2.5*72.5 73.9 73.8 79.3 70.5 80.2 81.1 86.6 71.7 66."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Evaluation results for GLUCOSE models. Human evaluation scores are out of 3; BLEU scores are out of 100.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "MotivatesVerb slot hard constraints: feels, wants, likes; Object slot soft constraints: curiosity, independence, competition, honor, approval, power, status, romance, success, friendship, belonging, health, safety, livelihood, happy, stressed, angered, disgusted, sad, surprised, fearful, trusting, love, obedient, amazed, disappointment, regret, worthless, aggression, optimistic. Dim 3: A location state that enables X Enables Verb slot hard constraints: am, is, are; Preposition slot hard constraints: above, across from, at, below, far from, in, in front of, inside of,near, next to, on top of, outside of. Dim 4: A possession state that enables X Enables Verb slot hard constraints: possess(es). An emotion that is caused by X Causes Verb slot hard constraints: feels, wants, likes; Object slot soft constraints: curiosity, independence, competition, honor, approval, power, status, romance, success, friendship, belonging, health, safety, livelihood, happy, stressed, angered, disgusted, sad, surprised, fearful, trusting, love, obedient, amazed, disappointment, regret, worthless, aggression, optimistic. Dim 8: A change of location that X results in Results in Verb slot hard constraints: am, is, are; Preposition slot hard constraints: above, across from, at, below, far from, in, in front of, inside of,near, next to, on top of, outside of. Dim 9: A change of possession that X results in Verb slot hard constraints: am, is, are, has, have, want, wants, need, needs.", "figure_data": "DimensionConnectiveSlot ConstraintsDim 1: An event that directly causes or en-Causes/Enables Noneables XDim 2: An emotion or basic human drive thatmotivates XDim 5: Other attribute that enables XEnablesVerb slot hard constraints: am, is, are, has, have, want, wants,need, needs.Dim 6: An event that is directly caused orCauses/Enables Noneenabled by XDim 7: Results inVerb slot hard constraints: possess(es)Dim 10: Other change in attribute that X re-Results insults in"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "The list of pre-defined options for filling in the syntactic slots per GLUCOSE dimension.", "figure_data": "Glucose ConceptNet RelATOMIC RelDims 1HasSubeventxEffect/oEffect& 6HasFirstSubeventHasLastSubeventHasPrerequisiteDim 2DesiresxAttr (\"feels\")CausesDesirexIntent (otherwise)MotivatedByGoalDim 7Same as dim2xReact/oReact (\"feels\")Dims 5DesiresxAttr/xWant& 10CausesDesireoWant"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Mappings between GLUCOSE dimensions and ConceptNet/ATOMIC relations. Concept-Net \"Causes\" applies to all GLUCOSE dimensions. Omitted GLUCOSE dimensions have no mapping in ATOMIC.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "since]+[he, she, they, it, I, you, we]+ Other attribute that enables X [am, is, are, has, have, want, wants, need, needs] Dim 6 Causes/Enables [causes, caused, results in , . This causes, . As a result] An event that is directly caused or en-", "figure_data": "DimensionConnectiveNatural Language TemplateDim 1Causes/Enables [because, since]An event that directly causes or enablesXDim 2Motivates[because, since]+ [he, she, they, I, you, we]+An emotion or basic human drive that[feels, wants, likes]motivates XDim 3Enables[because, since]+ [he, she, they, I, you, we]+A location state that enables X[is, was, were]+ [above, across from,between, at, below, far from, in, in front of,inside of,near, next to, on top of, outside of]Dim 4Enables[because, since]+[he, she, they, it, I, you, we]+A possession state that enables X[has, have]Dim 5 [because, abled by X EnablesDim 7Causes[. As a result]+ [he, she, they,I, you, we]+[feels]An emotion that is caused by XDim 8Results in[. As a result]+ [he, she, they, it, I, you, we]+A change of location that X results inbetween, [is, was, were]+ [above, across from,at, below, far from, in, in front of,inside of,near, next to, on top of, outside of]Dim 9Results in[. As a result] + [he, she, they, it, I, you, we]+A change of possession that X results in[has, have]Dim 10Results in[. As a result] + [he, she, they, it, I, you, we]+Other change in attribute that X resultsin"}], "formulas": [], "doi": "10.18653/v1/S19-1028"}