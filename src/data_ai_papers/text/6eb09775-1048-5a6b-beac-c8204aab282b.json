{"title": "Predicting the Demographics of Twitter Users from Website Traffic Data", "authors": "Aron Culotta; Nirmal Kumar; Jennifer Cutler", "pub_date": "", "abstract": "Understanding the demographics of users of online social networks has important applications for health, marketing, and public messaging. In this paper, we predict the demographics of Twitter users based on whom they follow. Whereas most prior approaches rely on a supervised learning approach, in which individual users are labeled with demographics, we instead create a distantly labeled dataset by collecting audience measurement data for 1,500 websites (e.g., 50% of visitors to gizmodo.com are estimated to have a bachelor's degree). We then fit a regression model to predict these demographics using information about the followers of each website on Twitter. The resulting average heldout correlation is .77 across six different variables (gender, age, ethnicity, education, income, and child status). We additionally validate the model on a smaller set of Twitter users labeled individually for ethnicity and gender, finding performance that is surprisingly competitive with a fully supervised approach.", "sections": [{"heading": "Introduction", "text": "Social media are increasingly being used to make inferences about the real world, with application to politics (O'Connor et al. 2010), health (Dredze 2012), and marketing (Gopinath, Thomas, and Krishnamurthi 2014). Understanding the demographic makeup of a sample of social media users is critical to further progress in this area, as it allows researchers to overcome the considerable selection bias in this uncontrolled data. Additionally, this capability will help public messaging campaigns ensure that the target demographic is being reached.\nA common approach to demographic inference is supervised classification -from a training set of annotated users, a model is fit to predict user attributes from the content of their writings (Argamon et al. 2005;Schler et al. 2006;Rao et al. 2010;Pennacchiotti and Popescu 2011;Burger et al. 2011;Rao et al. 2011;Al Zamal, Liu, and Ruths 2012). This approach has a number of limitations: collecting human annotations is costly and error-prone; many demographic variables of interest cannot easily be labeled by inspecting Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. a profile (e.g., income, education level); by restricting learning to a small set of labeled profiles, the generalizability of the classifier is limited. Additionally, most past work has focused on text as the primary source of evidence, making limited use of network evidence.\nIn this paper, we use regression to predict six demographic variables (gender, age, ethnicity, education, income, and child status) of a set of Twitter users based solely on whom they follow. Rather than using a standard supervised approach, we construct a distantly labeled dataset consisting of web traffic demographic data from Quantcast.com. By pairing web traffic data for a site with the followers of that site on Twitter.com, we fit a regression model between a set of Twitter users and their expected demographic profile.\nWith this data, we explore several questions:\nRQ1. Can the demographics of a set of Twitter users be inferred from network information alone? We find across six demographic variables an average held-out correlation of .77 between the web traffic demographics of a website and that predicted by a regression model based on the site's Twitter followers. We can learn, for example, that high-income users are likely to follow The Economist and young users are likely to follow PlayStation.", "publication_ref": ["b15", "b4", "b8", "b1", "b22", "b19", "b17", "b2", "b20", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "RQ2", "text": ". Can a regression model be extended to classify individual users? Using a hand-labeled validation set of users annotated with gender and ethnicity, we find that the regression model is competitive with a fullysupervised approach.\nRQ3. How much follower information is needed for inference? We find that the identities of only 10 followed accounts per user, chosen at random, is sufficient to achieve 90% of the accuracy obtained using 200 followed accounts.\nIn the remainder of the paper, we will first review related work, then describe the data collected from Twitter and QuantCast and the feature representation used for the task; next, we will present regression and classification results; finally, we will conclude and outline directions for future work. 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Predicting attributes of social media users is a growing area of interest, with recent work focusing on age (Schler et al. 2006;Rosenthal and McKeown 2011;Nguyen, Smith, and Ros 2011;Al Zamal, Liu, and Ruths 2012), sex (Rao et al. 2010;Burger et al. 2011;Liu and Ruths 2013), race/ethnicity (Pennacchiotti and Popescu 2011;Rao et al. 2011), and personality (Argamon et al. 2005;Schwartz et al. 2013). Other work predicts demographics from web browsing histories (Goel, Hofman, and Sirer 2012).\nThe majority of these approaches rely on hand-annotated training data, require explicit self-identification by the user, or are limited to very coarse attribute values (e.g., above or below 25-years-old).\nA related lightly supervised approach includes Chang et al. (2010), who infer user-level ethnicity using name/ethnicity distributions provided by the Census; however, that approach uses evidence from first and last names, which are often not available, and thus are more appropriate for population-level estimates. Rao et al. (2011) extend this approach to also include evidence from other linguistic features to infer gender and ethnicity of Facebook users; they evaluate on the fine-grained ethnicity classes of Nigeria and use very limited training data. More recently, Mohammady and Culotta (2014) trained an ethnicity model for Twitter using county-level supervision.\nThere have been several studies predicting populationlevel statistics from social media. Eisenstein, Smith, and Xing (2011) use geolocated tweets to predict zip-code statistics of race/ethnicity, income, and other variables using Census data; Schwartz et al. (2013) similarly predict county health statistics from Twitter. However, none of this prior work attempts to predict or evaluate at the user level.\nThe primary methodological novelties of the present work are its use of web traffic data as a form of weak supervision and its use of follower information as the primary source of evidence. Additionally, this work considers a larger set of demographic variables than prior work, and predicts a much more fine-grained set of categories (e.g., six different age brackets instead of two or three used previously).", "publication_ref": ["b22", "b21", "b13", "b0", "b19", "b2", "b10", "b17", "b20", "b1", "b23", "b7", "b3", "b20", "b12", "b5", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantcast", "text": "Quantcast.com is an audience measurement company that tracks the demographics of visitors to millions of websites. This is accomplished by using cookies to track the browsing activity of a large panel of respondents (Kamerer 2013).\nWe sampled 1,532 websites from Quantcast and downloaded statistics for six demographic variables: 25-34, 35-44, 45-54, 55-64, 65+ \u2022 Income: $0-50k, $50-100k, $100-150k, $150k+  Each variable represents the estimated percentage of visitors to a website with a given demographic.\n\u2022 Gender: Male, Female \u2022 Age: 18-24,", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Twitter", "text": "For each website collected in the previous step, we executed a script to search for its Twitter account, then manually verified it; 1,066 accounts from the original set of 1,532 were found. An assumption of this work is that the demographic profiles of followers of a website on Twitter are correlated with the demographic profiles of visitors to that website. While there are undoubtedly biases introduced here (e.g., Twitter users may skew younger than the web traffic panel), in aggregate these differences should have limited impact on the final model. For each account, we queried the Twitter REST API to sample 120 of its followers, using followers/ids request. This sample is not necessarily uniform. The Twitter API documentation states that \"At this time, results are ordered with the most recent following first -however, this ordering is subject to unannounced change and eventual consistency issues.\"\nFor each of these followers, we then collected up to 5,000 of the accounts they follow, called friends, using the friends/ids API request. Thus, for each of the original accounts from Quantcast, we have up to (120 * 5K = 600K) additional accounts that are two hops from the original account (the friend of a follower). We refer to these discovered accounts as neighbors of the original Quantcast account.\nOf course, many of these accounts will be duplicates, as two different followers will follow many of the same accounts (i.e., triadic closure) -indeed, our core assumption is that the number of such duplicates represents the strength of the similarity between the neighbors.\nFor each of the original accounts, we compute the fraction of its followers that are friends with each of its neighbors and store this in a neighbor vector. For example, suppose a Quantcast account A has two followers B and C; B follows D and E; and C follows D and F . Then the neighbor vector for A is {(D, 1), (E, .5), (F, .5)}. This suggests that A and D are closer neighbors than A and E. Figure 2 depicts this representation.\nThe resulting dataset consists of 1.7M unique neighbors of the original 1,532 accounts. To reduce dimensionality, we removed neighbors with fewer than 100 followers, leaving 46,622 unique neighbors with a total of 178M incoming links. Figure 1 plots the number of unique neighbors per account as well as the number of neighbor links per account.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Regression", "text": "For each Quantcast site, we pair its demographic variables with its neighbor vector to construct a regression problem. Thus, we attempt to predict the demographic profile of the followers of a Twitter account based on the friends of those followers.\nDue to the high dimensionality and small number of examples, we use elastic net regularization, which combines both L1 and L2 penalties. Furthermore, since each output variable consists of dependent categories (e.g., age brackets), we use a multi-task variant of elastic net to ensure that the same features are selected by the L1 regularizer for each category. We use the implementation of MultiTaskElasticNet in scikit-learn (Pedregosa and others 2011). 2 Recall that standard linear regression selects coefficients \u03b2 to minimize the squared error on a list of training instances {x i , y i } N i=1 , for feature vector x i and expected output y i .\n\u03b2 * \u2190 argmin \u03b2 N i=1 (y i \u2212 \u03b2 T x i ) 2\nLasso imposes an L1 regularizer on \u03b2, while ridge regression imposes an L2 regularizer on \u03b2. Elastic net combines 2 After tuning on a validation set for one task, we fix alpha=1e\u2212 5 and l1 ratio=0.5. both penalties:\n\u03b2 * \u2190 argmin \u03b2 1 N N i=1 (y i \u2212 \u03b2 T x i ) 2 + \u03bb 1 ||\u03b2|| 1 + \u03bb 2 ||\u03b2|| 2 2\nwhere \u03bb 1 and \u03bb 2 control the strength of L1 and L2 regularizers, respectively. The L1 regularizer encourages sparsity (i.e., many 0 values in \u03b2), while the L2 regularizer prevents \u03b2 values from becoming too large. Multi-task elastic net extends elastic net to groups of related regression problems (Obozinski, Taskar, and Jordan 2006). E.g., in our case, we would like to account for the fact that the regressions for \"No College\", \"College\", and \"Grad School\" are related; thus, we would like the sparse solutions to be similar across tasks (that is, L1 should select the same features for each task).\nLet \u03b2 (j) be the coefficients for task j, and let \u03b2 k = (\u03b2\n(1) k . . . \u03b2 (M ) k\n) T be the vector of coefficients formed by concatenating the coefficients for the kth feature across all M tasks. Then multi-task elastic net objective enforces that similar features are selected across tasks:\n\u03b2 * \u2190 argmin \u03b2 M j=1 1 N j Nj i=1 (y (j) i \u2212 \u03b2 (j)T x (j) i ) 2 + \u03bb 1 p k=1 ||\u03b2 k || 1 + \u03bb 2 ||\u03b2|| 2 2\nwhere N j is the number of instances for task j and p is the number of features.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Regression Results", "text": "We perform five-fold cross-validation and report the held-out correlation coefficient (r) between the predicted and true demographic variables. Figure 3 displays the resulting scatter plots for each of the 19 categories for 6 demographic variables.\nWe can see that overall the correlation is very strong: .77 on average, ranging from .55 for the 35-44 age bracket to .89 for Male and African American. All of these correlation coefficients are significant using a two-tailed t-test (p < 0.01), with a Bonferroni adjustment for the 19 comparisons. These results indicate that the neighbor vector provides a reliable signal of the demographics of a group of Twitter users.\nTo further examine these results, Table 1 displays the accounts with the 5 largest coefficients per class according to the fit regression model. These contain many results that match common stereotypes: sports accounts are correlated with men, video game accounts are correlated with younger people, financial news accounts are correlated with greater income, and parenting magazines are correlated with people who have children. There also appear to be some geographic effects, as California-related accounts are highly weighted for both Hispanic and Asian categories. There seems to be good city-level resolution -Los Angeles accounts (latimes, Lakers) are more strongly correlated with Hispanics, whereas San Francisco accounts (SFGate, SFist, SFWeekly) are more strongly correlated with Asians.\nFinally, we compare multi-task elastic net with the singletask variant of elastic net and ridge regression (with regularization parameters tuned as before). Table 2 shows a large Figure 3: Scatter plots of the true demographic variables from Quantcast versus those predicted from the neighbor vector, along with the held-out correlation coefficient (r).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_0"]}, {"heading": "Model", "text": "Average Correlation multi-task elastic net 0.772 elastic net 0.769 ridge 0.671 improvement from ridge to elastic net, and a more modest improvement when using the multi-task formulation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Classification", "text": "The regression results suggest that the neighbor vector of an account is highly predictive of the demographics of its followers. In this section, we provide additional validation using data manually annotated at the user level. Many of the demographic variables are difficult to label at the individual level -e.g., income or education level is rarely explicitly mentioned in either a profile or tweet. Indeed, an advantage of the approach here is that aggregate statistics are more readily available for many demographics of interest that are difficult to label at the individual level. For validation purposes, we focus on two variables that can fairly reliably be labeled for individuals: ethnicity and gender.\nThese were collected as follows: First, we used the Twit-ter Streaming API to obtain a random sample of users, filtered to the United States (using time zone and the place country code from the profile). From six days' worth of data (December 6-12, 2013), we sampled 1,000 profiles at random and categorized them by analyzing the profile, tweets, and profile image for each user. We categorized 770 Twitter profiles into one of four ethnicities (Asian, African American, Hispanic, Caucasian). Those for which ethnicity could not be determined were discarded (230/1,000; 23%). 3 The category frequency is Asian ( 22), African American (263), Hispanic (158), Caucasian (327). To estimate inter-annotator agreement, a second annotator sampled and categorized 120 users. Among users for which both annotators selected one of the four categories, 74/76 labels agreed (97%). There was some disagreement over when the category could be determined: for 21/120 labels (17.5%), one annotator indicated the category could not be determined, while the other selected a category. Gender annotation was done automatically by comparing the first name provided in the user profile with the U.S. Census list of names by gender. 4 Ambiguous names were removed.\nFor each user, we collected up to 200 of their friends using the Twitter API. We removed accounts that restricted access  to friend information; we also removed the Asian users due to the small sample size, leaving a total of 615 users. For classification, each user is represented by the identity of their friends (up to 200). Only those friend accounts contained in the 46,622 accounts used for the regression experiments were retained. Figure 4 shows the number of friends per user for each dataset.\nAs a baseline, we trained a logistic regression classifier with L2 regularization, using a binary representation of each user's friends. To repurpose the regression model to perform classification, we must modify the coefficients returned by regression. We first compute the z-score of each coefficient with respect to the other coefficients for that category value. E.g., all coefficients for the Male class are adjusted to have mean 0 and unit variance. This makes the coefficients comparable across labels. Furthermore, we set to 0 any negative coefficient. To classify each user, we then compute the sum of coefficients for each friend, and select the class with maximum value.\nClassification Results Figure 5 displays the macro-F1 value for ethnicity (three classes) and gender (two classes). The regression model is fit using only the Quantcast data, while the classification model uses three-fold crossvalidation using the labeled user accounts. We compare the regression approach with logistic regression using an increasingly larger number of labeled examples.\nFor gender, the regression model outperforms the classification approach, which is surprising given that the regression model does not have any hand-labeled profiles for training. For ethnicity, the regression approach outperforms classification until over half of the labeled data is used to fit the classification approach, after which the classification approach dominates. In general, the accuracy of the two approaches is comparable.\nSensitivity to number of friends Finally, we investigate how much information we need about a user before we can make an accurate prediction of their demographics. Whereas the previous results considered up to 200 friends of each user, we consider smaller numbers of friends to determine how the number of friends collected affects accuracy. Figure 6 displays the macro-F1 value for trials in which the number of friends per user is one of Figure 5: Classification results comparing a standard logistic regression classifier (classification), trained using cross-validation, versus the proposed approach (regression), which is fit solely on statistics from Quantcast, with no individually labeled data. , 2, 3, 4, 5, 10, 20, 30, 40, 50} (values greater than 50 did not significantly increase accuracy). The friends are sampled at random, and the results are averaged over five trials. We can see that accuracy plateaus quickly: for both tasks, the F1 score using only 10 friends is within 5% of the score using all 200 friends. This result has implications for scalability -Twitter API rate limits make it difficult to collect the complete social graph for a set of users. Additionally, this has important privacy implications; revealing even a small amount of social information also reveals a considerable amount of demographic information. Twitter users concerned about privacy may wish to disable the setting that makes friend identity information public.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "{1", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have shown that Twitter follower information provides a strong source of information for performing demographic inference. Furthermore, pairing web traffic demographic data with Twitter data provides a simple and effective way to train a demographic inference model without any annotation of individual profiles. We have validated the approach both in aggregate (by comparing with Quantcast data) and at the individual level (by comparing with handlabeled annotations), finding high accuracy in both cases. Somewhat surprisingly, the approach outperforms a fullysupervised approach for gender classification, and is competitive for ethnicity classification.\nIn the future, we will test the generalizability of this approach to new groups of Twitter users. For example, we can collect users by city or county and compare the predictions with the Census demographics from that geographic location. Additionally, we will investigate ways to combine labeled and unlabeled data using semi-supervised Figure 6: Macro F1 as the number of friends per user increases (with standard errors).\nlearning (Quadrianto et al. 2009;Ganchev et al. 2010;Mann and McCallum 2010).", "publication_ref": ["b18", "b6", "b11"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Homophily and latent attribute inference: Inferring latent attributes of twitter users from neighbors", "journal": "", "year": "2012", "authors": "Al Zamal; F Liu; W Ruths; D "}, {"ref_id": "b1", "title": "Lexical predictors of personality type", "journal": "", "year": "2005", "authors": "S Argamon; S Dhawle; M Koppel; J W Pennebaker"}, {"ref_id": "b2", "title": "Discriminating gender on twitter", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "J D Burger; J Henderson; G Kim; G Zarrella"}, {"ref_id": "b3", "title": "ePluribus: ethnicity on social networks", "journal": "", "year": "2010", "authors": "J Chang; I Rosenn; L Backstrom; C Marlow"}, {"ref_id": "b4", "title": "How social media will change public health", "journal": "IEEE Intelligent Systems", "year": "2012", "authors": "M Dredze"}, {"ref_id": "b5", "title": "Discovering sociolinguistic associations with structured sparsity", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "J Eisenstein; N A Smith; E P Xing"}, {"ref_id": "b6", "title": "Posterior regularization for structured latent variable models", "journal": "J. Mach. Learn. Res", "year": "2010", "authors": "K Ganchev; J Graca; J Gillenwater; B Taskar"}, {"ref_id": "b7", "title": "Who does what on the web: A large-scale study of browsing behavior", "journal": "", "year": "2012", "authors": "S Goel; J M Hofman; M I Sirer"}, {"ref_id": "b8", "title": "Investigating the relationship between the content of online word of mouth, advertising, and brand performance. Marketing Science", "journal": "", "year": "2014-01-10", "authors": "S Gopinath; J S Thomas; L Krishnamurthi"}, {"ref_id": "b9", "title": "Estimating online audiences: Understanding the limitations of competitive intelligence services", "journal": "First Monday", "year": "2013", "authors": "D Kamerer"}, {"ref_id": "b10", "title": "What's in a name? using first names as features for gender inference in twitter", "journal": "", "year": "2013", "authors": "W Liu; D Ruths"}, {"ref_id": "b11", "title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "journal": "J. Mach. Learn. Res", "year": "2010", "authors": "G S Mann; A Mccallum"}, {"ref_id": "b12", "title": "Using county demographics to infer attributes of twitter users", "journal": "", "year": "2014", "authors": "E Mohammady; A Culotta"}, {"ref_id": "b13", "title": "Author age prediction from text using linear regression", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "D Nguyen; N A Smith; C P Ros"}, {"ref_id": "b14", "title": "Multi-task feature selection", "journal": "Statistics Department", "year": "2006", "authors": "G Obozinski; B Taskar; M Jordan"}, {"ref_id": "b15", "title": "From tweets to polls: Linking text sentiment to public opinion time series", "journal": "ICWSM", "year": "2010", "authors": "B O'connor; R Balasubramanyan; B R Routledge; N A Smith"}, {"ref_id": "b16", "title": "Scikit-learn: Machine learning in Python", "journal": "Machine Learning Research", "year": "2011", "authors": "F Pedregosa"}, {"ref_id": "b17", "title": "A machine learning approach to twitter user classification", "journal": "ICWSM. The AAAI Press", "year": "2011", "authors": "M Pennacchiotti; A.-M Popescu"}, {"ref_id": "b18", "title": "Estimating labels from label proportions", "journal": "J. Mach. Learn. Res", "year": "2009", "authors": "N Quadrianto; A J Smola; T S Caetano; Q V Le"}, {"ref_id": "b19", "title": "Classifying latent user attributes in twitter", "journal": "ACM", "year": "2010", "authors": "D Rao; D Yarowsky; A Shreevats; M Gupta"}, {"ref_id": "b20", "title": "Hierarchical bayesian models for latent attribute detection in social media", "journal": "", "year": "2011", "authors": "D Rao; M J Paul; C Fink; D Yarowsky; T Oates; G Coppersmith"}, {"ref_id": "b21", "title": "Age prediction in blogs: A study of style, content, and online behavior in pre-and post-social media generations", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "S Rosenthal; K Mckeown"}, {"ref_id": "b22", "title": "Effects of age and gender on blogging", "journal": "", "year": "2006", "authors": "J Schler; M Koppel; S Argamon; J W Pennebaker"}, {"ref_id": "b23", "title": "Characterizing geographic variation in well-being using tweets", "journal": "", "year": "2013", "authors": "H A Schwartz; J C Eichstaedt; M L Kern; L Dziurzynski; S M Ramones; M Agrawal; A Shah; M Kosinski; D Stillwell; M E Seligman"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "\u2022Figure 1: Rank-order frequency plots of the number of neighbors per account and the number of links to all neighbors per account.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Data model. We collect QuantCast demographic data for each website, then construct a Neighbor Vector from the Twitter connections of that website, based on the proportion of the company's followers that are friends with each neighbor.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Rank-order frequency plots of the number of friends per user in each of the labeled datasets (ethnicity, gender). These friends are restricted to one of the 46,622 accounts used in the regression experiments.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Average held-out correlation across all demographic variables for three competing regression models.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Accounts with the highest estimated coefficients for each category.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 Gender: Male, Female \u2022 Age: 18-24,", "formula_coordinates": [2.0, 54.0, 609.3, 102.66, 24.0]}, {"formula_id": "formula_1", "formula_text": "\u03b2 * \u2190 argmin \u03b2 N i=1 (y i \u2212 \u03b2 T x i ) 2", "formula_coordinates": [3.0, 108.2, 621.26, 129.61, 30.32]}, {"formula_id": "formula_2", "formula_text": "\u03b2 * \u2190 argmin \u03b2 1 N N i=1 (y i \u2212 \u03b2 T x i ) 2 + \u03bb 1 ||\u03b2|| 1 + \u03bb 2 ||\u03b2|| 2 2", "formula_coordinates": [3.0, 323.8, 72.8, 229.4, 30.32]}, {"formula_id": "formula_3", "formula_text": "(1) k . . . \u03b2 (M ) k", "formula_coordinates": [3.0, 329.01, 239.32, 46.9, 14.3]}, {"formula_id": "formula_4", "formula_text": "\u03b2 * \u2190 argmin \u03b2 M j=1 1 N j Nj i=1 (y (j) i \u2212 \u03b2 (j)T x (j) i ) 2 + \u03bb 1 p k=1 ||\u03b2 k || 1 + \u03bb 2 ||\u03b2|| 2 2", "formula_coordinates": [3.0, 329.25, 288.93, 198.59, 66.65]}], "doi": ""}