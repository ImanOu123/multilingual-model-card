{"title": "Sequence classification with human attention", "authors": "Maria Barrett; Joachim Bingel; Nora Hollenstein; Marek Rei; Anders S\u00f8gaard", "pub_date": "", "abstract": "Learning attention functions requires large volumes of data, but many NLP tasks simulate human behavior, and in this paper, we show that human attention really does provide a good inductive bias on many attention functions in NLP. Specifically, we use estimated human attention derived from eyetracking corpora to regularize attention functions in recurrent neural networks. We show substantial improvements across a range of tasks, including sentiment analysis, grammatical error detection, and detection of abusive language.", "sections": [{"heading": "Introduction", "text": "When humans read a text, they do not attend to all its words (Carpenter and Just, 1983;Rayner and Duffy, 1988). For example, humans are likely to omit many function words and other words that are predictable in context and focus on less predictable content words. Moreover, when they fixate on a word, the duration of that fixation depends on a number of linguistic factors (Clifton et al., 2007;Demberg and Keller, 2008).\nSince learning good attention functions for recurrent neural networks requires large volumes of data (Zoph et al., 2016;Britz et al., 2017), and errors in attention are known to propagate to classification decisions (Alkhouli et al., 2016), we explore the idea of using human attention, as estimated from eye-tracking corpora, as an inductive bias on such attention functions. Penalizing attention functions for departing from human attention may enable us to learn better attention functions when data is limited.\nEye-trackers provide millisecond-accurate records on where humans look when they are reading, and they are becoming cheaper and more easily available by the day (San Agustin et al., 2009). In this paper, we use publicly available eye-tracking corpora, i.e., texts augmented with eye-tracking measures such as fixation duration times, and large eye-tracking corpora have appeared increasingly over the past years. Some studies suggest that the relevance of text can be inferred from the gaze pattern of the reader (Saloj\u00e4rvi et al., 2003) -even on word-level (Loboda et al., 2011).\nContributions We present a recurrent neural architecture with attention for sequence classification tasks. The architecture jointly learns its parameters and an attention function, but can alternate between supervision signals from labeled sequences (with no explicit supervision of the attention function) and from attention trajectories. This enables us to use per-word fixation durations from eye-tracking corpora to regularize attention functions for sequence classification tasks. We show such regularization leads to significant improvements across a range of tasks, including sentiment analysis, detection of abusive language, and grammatical error detection. Our implementation is made available at https://github.com/coastalcph/ Sequence_classification_with_ human_attention.", "publication_ref": ["b9", "b43", "b10", "b13", "b62", "b7", "b0", "b51", "b50", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "We present a recurrent neural architecture that jointly learns the recurrent parameters and the attention function, but can alternate between supervision signals from labeled sequences and from attention trajectories in eye-tracking corpora. The input will be a set of labeled sequences (sentences paired with discrete category labels) and a set of sequences, in which each token is associated with a scalar value representing the attention human readers devoted to this token on average.\nThe two input datasets, i.e., the target task train-ing data of sentences paired with discrete categories, and the eye-tracking corpus, need not (and will not in our experiments) overlap in any way.\nOur experimental protocol, in other words, does not require in-task eye-tracking recordings, but simply leverages information from existing, available corpora.\nBehind our approach lies the simple observation that we can correlate the token-level attention devoted by a recurrent neural network, even if trained on sentence-level signals, with any measure defined at the token level. In other words, we can compare the attention devoted by a recurrent neural network to various measures, including token-level annotation (Rei and S\u00f8gaard, 2018) and eye-tracking measures. The latter is particularly interesting as it is typically considered a measurement of human attention.\nWe go beyond this: Not only can we compare machine attention with human attention, we can also constrain or inform machine attention by human attention in various ways. In this paper, we explore this idea, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention.\nOur training method is similar to a standard approach to training multi-task architectures (Dong et al., 2015;Bingel and S\u00f8gaard, 2017), sometimes referred to as the alternating training approach (Luong et al., 2016): We randomly select a data point from our training data or the eye-tracking corpus with some (potentially equal) probability. If the data point is sampled from our training data, we predict a discrete category and use the computed loss to update our parameters. If the data point is sampled from the eye-tracking corpus, we still run the recurrent network to produce a category, but this time we only monitor the attention weights assigned to the input tokens. We then compute the minimum squared error between the normalized eye-tracking measure and the normalized attention score. In other words, in multi-task learning, we optimize each task for a fixed number of parameter updates (or mini-batches) before switching to the next task (Dong et al., 2015); in our case, we optimize for a target task (for a fixed number of updates), then improve our attention function based on human attention (for a fixed number of updates), then return to optimizing for the target task and continue iterating.", "publication_ref": ["b45", "b14", "b6", "b33", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "Our architecture is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) that encodes word representations x i into forward and backward representations, and into combined hidden states h i (of slightly lower dimensionality) at every timestep. In fact, our model is a hierarchical model whose word representations are concatenations of the output of character-level LSTMs and word embeddings, following , but we ignore the character-level part of our architecture in the equations below:\n\u2212 \u2192 h i = LST M (x i , \u2212 \u2212 \u2192 h i\u22121 )(1)\n\u2190 \u2212 h i = LST M (x i , \u2190 \u2212 \u2212 h i+1 ) (2) h i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ](3)\nh i = tanh(W h h i + b h )(4)\nThe final (reduced) hidden state is sometimes used as a sentence representation s, but we instead use attention to compute s by multiplying dynamically predicted attention weights with the hidden states for each time step. The final sentence predictions y are then computed by passing s through two more hidden layers:\ns = i a i h i (5) y = \u03c3(W y tanh(W\u1ef9s + b\u1ef9) + b y )(6)\nFrom the hidden states, we directly predict tokenlevel raw attention scores a i :\ne i = tanh(W e h i + b e )(7)\na i = W a e i + b a (8)\nWe normalize these predictions to attention weights a i :\na i = a i k a k (9)\nOur model thus combines two distinct objectives: one at the sentence level and one at the token level. The sentence-level objective is to minimize the squared error between output activations and true sentence labels y.\nL sent = j (y (j) \u2212 y (j) ) 2(10)\nThe token-level objective, similarly, is to minimize the squared error for the attention not aligning with our human attention metric.\nL tok = j t (a (j)(t) \u2212 a (j)(t) ) 2 (11)\nThese are finally combined to a weighted sum, using \u03bb (between 0 and 1) to trade off loss functions at the sentence and token levels.\nL = L sent + \u03bbL tok (12\n)\nNote again that our architecture does not require the target task data to come with eye-tracking information. We instead learn jointly to predict sentence categories and to attend to the tokens humans tend to focus on for longer. This requires a training schedule that determines when to optimize for the sentence-level classification objective, and when to optimize the machine attention at the token level. We therefore define an epoch to comprise a fixed number of batches, and sample every batch of training examples either from the target task data or from the eye-tracking corpus, as determined by a coin flip, the bias of which is tuned as a hyperparameter. Specifically, we define an epoch to consist of n batches, where n is the number of training sentences in the target task data divided by the batch size. This coin is potentially weighted with data being drawn from the auxiliary task with some probability or a decreasing probability of 1 E+1 , where E is the current epoch; see Section 4 for hyper-parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "As mentioned in the above, our architecture requires no overlap between the eye-tracking corpus and the training data for the target task. We therefore rely on publicly available eye-tracking corpora. For sentiment analysis, grammatical error detection, and hate speech detection, we use publicly available research datasets that have been used previously in the literature. All datasets were lower-cased.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Eye-tracking corpora", "text": "For our experiments, we concatenate two publicly available eye-tracking corpora, the Dundee Corpus (Kennedy et al., 2003) and the reading parts of the ZuCo Corpus (Hollenstein et al., 2018), described below. Both corpora contain eye-tracking measurements from several subjects reading the same text. For every token, we compute the mean duration of all fixations to this token as our measure of human attention, following previous work (Barrett et al., 2016a;Gonzalez-Garduno and S\u00f8gaard, 2018).\nDundee The English part of the Dundee corpus (Kennedy et al., 2003) comprises 2,368 sentences and more than 50,000 tokens. The texts were read by ten skilled, adult, native speakers. The texts are 20 newspaper articles from The Independent. The reading was self-paced and as close to natural, contextualized reading as possible for a laboratory data collection. The apparatus was a Dr Bouis Oculometer Eyetracker with a 1000 Hz monocular (right) sampling. At most five lines were shown per screen while subjects were reading.\nZuCo The ZuCo corpus (Hollenstein et al., 2018) is a combined eye-tracking and EEG dataset. It contains approximately 1,000 individual English sentences read by 12 adult, native speakers. Eye movements were recorded with the infrared video-based eye tracker EyeLink 1000 Plus at a sampling rate of 500 Hz. The sentences were presented at the same position on the screen, one at a time. Longer sentences spanned multiple lines. The subjects used a control pad to switch to the next sentence and to answer the control questions, which allowed for natural reading speed. The corpus contains both natural reading and reading in a task-solving context. For compatibility with the Dundee corpus, we only use the subset of the data, where humans were encouraged to read more naturally. This subset contains 700 sentences. This part of the Zuco corpus contains positive, negative or neutral sentences from the Stanford Sentiment Treebank (Socher et al., 2013) for passive reading, to analyze the elicitation of emotions and opinions during reading. As a control condition, the subjects sometimes had to rate the quality of the described movies; in approximately 10% of the cases. The Zuco corpus also contains instances where subjects were presented with Wikipedia sentences that contained semantic relations such as employer, award and job_title (Culotta et al., 2006). The control condition for this tasks consisted of multiple-choice questions about the content of the previous sentence; again, approximately 10% of all sentences were followed by a question.  Preprocessing of eye-tracking data Mean fixation duration (MEAN FIX DUR) is extracted from the Dundee Corpus. For Zuco, we divide total reading time per word token with the number of fixations to obtain mean fixation duration.\nThe mean fixation duration is selected empirically among gaze duration (sum of all fixations in the first pass reading of the a word) and total fixation duration, and n fixations. Then we average these numbers for all readers of the corpus to get a more robust average processing time. Eye-tracking is known to correlate with word frequency (Rayner and Duffy, 1988). We include a frequency baseline on the eye tracking text, BNC INV FREQ.\nThe word frequencies comes from the British National Corpus (BNC) frequency lists (Kilgarriff, 1995). We use log-transformed frequency per million. Before normalizing, we take the additive inverse of the frequency, such that rare words get a high value, making it comparable to gaze. MEAN FIX DUR and BNC INV FREQ are minmax-normalized to a value in the range 0-1. MEAN FIX DUR is normalized separately for the two eye tracking corpora. We expect the experimental bias -especially the fact that ZuCo contains reading of isolated sentences and Dundee contains longer texts -to influence the reading and therefore separate normalization should preserve the signal within each corpus better.", "publication_ref": ["b23", "b21", "b1", "b19", "b23", "b21", "b53", "b11", "b43", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Sentiment classification", "text": "Table 1 presents an overview of all train, development and test sets used in this paper.\nOur first task is sentence-level sentiment classification. We note that many sentiment analysis datasets contain document-level labels or include more fine-grained annotation of text spans, say phrases or words. For compatibility with our other tasks, we focus on sentence-level sentiment analysis. We use the SemEval-2013 Twitter dataset (Wilson et al., 2013;Rosenthal et al., 2015) for training and development. For test, we use a samedomain test set, the SemEval-2013 Twitter test set (SEMEVAL TWITTER POS | NEG), and an out-of-domain test set, SemEval-2013 SMS test set (SEMEVAL SMS POS | NEG). The SemEval-2013 sentiment classification task was a three-way classification task with positive, negative and neutral classes. We reduce the task to binary tasks detecting negative sentences vs. non-negative and vice versa for the positive class. Therefore the dataset size is the same for POS and NEG experiments.", "publication_ref": ["b60", "b49"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Grammatical error detection", "text": "Our second task is grammatical error detection. We use the First Certificate in English error detection dataset (FCE) (Yannakoudakis et al., 2011). This dataset contains essays written by English learners during language examinations, where any grammatical errors have been manually annotated by experts. Rei and Yannakoudakis (2016) converted the dataset for a sequence labeling task and we use their splits for training, development and testing. Similarly to Rei and S\u00f8gaard (2018), we perform sentence-level binary classification of sentences that need some editing vs. grammatically correct sentences. We do not use the tokenlevel labels for training our model.", "publication_ref": ["b61", "b46", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Hate speech detection", "text": "Our third and final task is detection of abusive language; or more specifically, hate speech detection. We use the datasets of Waseem (2016) and Waseem and Hovy (2016). The former contains 6,909 tweets; the latter 14,031 tweets. They are manually annotated for sexism and racism. In this study, sexism and racism are conflated into one category in both datasets. Both datasets are split in train, development and test splits consisting of 80%, 10% and 10% of the tweets respectively.  function regularized by information about human attention, and finally, (c) a second baseline using frequency information as a proxy for human attention and using the same regularization scheme as in our human attention model.\nHyperparameters Basic hyper-parameters such as number of hidden layers, layer size, and activation functions were following the settings of Rei and S\u00f8gaard (2018). The dimensionality of our word embedding layer was set to size 300, and we use publicly available pre-trained Glove word embeddings (Pennington et al., 2014) that we finetune during training. The dimensionality of the character embedding layer was set to 100. The recurrent layers in the character-level component have dimensionality 100; the word-level recurrent layers dimensionality 300. The dimensionality of our feed-forward layer, leading to reduced combined representations h i , is 200, and the attention layer has dimensionality 100. Three hyper-parameters, however, we tune for each architecture and for each task, by measuring sentence-level F 1 -scores on the development sets. These are: (a) learning rate, (b) \u03bb in Equation ( 12), i.e., controlling the relative importance of the attention regularization, and (c) the probability of sampling data from the eye-tracking corpus during training.\nFor all tasks and all conditions (baseline, frequency-informed baseline, and our human attention model), we perform a grid search over learning rates [ .01 .1 1. ], L att weight \u03bb values [ .2 .4 .6 .8 1. ], and probability of sampling from the eye-tracking corpus [ .125 .25 .5 1., decreasing ] -where decreasing means that the probability of sampling from the eye-tracking corpus initially is 0.5, but drops linearly for each epoch ( 1E+1 ; see 2.1. We apply the models with the best average F 1 scores over three random seeds on the validation data, to our test sets.\nInitialization Our models are randomly initialized. This leads to some variance in performance across different runs. We therefore report averages over 10 runs in our experiments below.", "publication_ref": ["b57", "b58", "b45", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Our performance metric across all our experiments is the sentence-level F 1 score. We report precision, recall and F 1 scores for all tasks in Table 2.\nOur main finding is that our human attention model, based on regularization from mean fixation durations in publicly available eye-tracking corpora, consistently outperforms the recurrent architecture with learned attention functions. The improvements over both baseline and BNC frequency are significant (p < 0.01) using bootstrapping (Calmettes et al., 2012) over all tasks, with one seed. The mean error reduction over the baseline is 4.5%. Unsurprisingly, knowing that human attention helps guide our recurrent architecture, the frequency-informed baseline is also better than the non-informed baseline across the board, but the human attention model is still significantly better across all tasks (p < 0.01). For all tasks except negative sentiment, we note that generally, most of the improvements over the learned attention baseline for the gaze-informed models, are due to improvements in recall. Precision is not worse, but we do not see any larger improvements on preci-sion either. For the negative SEMEVAL tasks, we also see larger improvements for precision.\nThe observation that improvements are primarily due to increased recall, aligns well with the hypothesis that human attention serves as an efficient regularization, preventing overfitting to surface statistical regularities that can lead the network to rely on features that are not there at test time (Globerson and Roweis, 2006), at the expense of target class precision.", "publication_ref": ["b8", "b18"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Analysis", "text": "We illustrate the differences between our baseline models and the model with gaze-informed attention by the attention weights of an example sentence. Though it is a single, cherry-picked example, it is representative of the general trends we observe in the data, when manually inspecting attention patterns. Table 3 presents a coarse visualization of the attention weights of six different models, namely our baseline architecture and the architecture with gaze-informed attention, trained on three different tasks: hate speech detection, negative sentiment classification, and error detection. The sentence is a positive hate speech example from the Waseem and Hovy (2016) development set. The words with more attention than the sentence average are bold-faced.\nFirst note that the baseline models only attend to one or two coherent text parts. This pattern was very consistent across all the sentences we examined. This pattern was not observed with gazeinformed attention.\nOur second observation is that the baseline models are more likely to attend to stop words than gaze-informed attention. This suggests that gazeinformed attention has learned to simulate human attention to some degree. We also see many differences between the jointly learned task-specific, gaze-informed attention functions.\nThe gaze-informed hate speech classifier, for example, places considerable attention on BUT, which in this case is a passive-aggressive hate speech indicator. It also gives weight to double standards and certain rules.\nThe gaze-informed sentiment classifier, on the other hand, focuses more on sorry I am not sexist which, in isolation, reads like an apologetic disclaimer. This model also gives weight to double standards and certain rules\nThe gaze-informed grammatical error detection model gives attention to standards, which is ungrammatical, because of the morphological number disagreement with its determiner a; it also gives attention to certain rules, which is disagreeing, again in number, with there's. It also gives attention to the non-word fem.\nOverall, this, in combination with our results in Table 3, suggests that the regularization effect from human attention enables our architecture to learn to better attend to the most relevant aspects of sentences for the target tasks. In other words, human attention provides the inductive bias that makes learning possible.", "publication_ref": ["b58"], "figure_ref": [], "table_ref": ["tab_5", "tab_5"]}, {"heading": "Discussion and related work", "text": "Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b).\nGaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features Goldwater, 2011, 2013;Plank, 2016;Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a;Bingel et al., 2016). We follow Gonzalez-Garduno and S\u00f8gaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations.\nFixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on open-class words that are not predictable from context, and Kliegl et al. (2004) show that a higher cognitive load results in longer fixation durations. Fixations before skipped words are shorter before short or high-frequency words and longer before long or low-frequency words in comparison with control fixations (Kliegl and Engbert, 2005). Many of these findings suggest correlations with syntactic information, and many authors have confirmed that gaze information is useful to discriminate between syntactic phenomena (Demberg and Keller, 2008;Barrett and S\u00f8gaard, 2015a,b).\nGaze data has also been used in the context of  sentiment analysis before (Mishra et al., 2017b,a). Mishra et al. (2017b) augmented a sentiment analysis system with eye-tracking features, including first fixation durations and fixation counts. They show that fixations not only have an impact in detecting sentiment, but also improve sarcasm detection. They train a convolutional neural network that learns features from both gaze and text and uses them to classify the input text (Mishra et al., 2017a). On a related note, Raudonis et al. (2013) developed a emotion recognition system from visual stimulus (not text) and showed that features such as pupil size and motion speed are relevant to accurately detect emotions from eye-tracking data. Wang et al. (2017) use variables shown to correlate with human attention, e.g. surprisal, to guide the attention for sentence representations.\nGaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and S\u00f8gaard, 2018).\nGaze has either been used as features (Barrett and S\u00f8gaard, 2015a;Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke et al., 2016;Gonzalez-Garduno and S\u00f8gaard, 2018). We are, to the best of our knowledge, the first to use gaze to inform attention functions in recurrent neural networks. Ibraheem et al. (2017), however, uses optimal attention to simulate human attention in an interactive machine translation scenario, and Britz et al. (2017) limit attention to a local context, inspired by findings in studies of human reading. Rei and S\u00f8gaard (2018) use auxiliary data to regularize attention functions in recurrent neural networks; not from psycholinguistics data, but using small amounts of task-specific, token-level annotations. While their motivation is very different from ours, technically our models are very related. In a different context, Das et al. (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context.", "publication_ref": ["b1", "b35", "b40", "b27", "b1", "b5", "b19", "b9", "b29", "b28", "b13", "b35", "b34", "b42", "b56", "b19", "b3", "b2", "b27", "b19", "b22", "b7", "b45", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Human-inspired attention functions", "text": "Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation.\nSentiment analysis While sentiment analysis is most often considered a supervised learning problem, several authors have leveraged other signals than annotated data to learn sentiment analysis models that generalize better. Felbo et al. (2017), for example, use emoji prediction to pretrain their sentiment analysis models. Mishra et al. (2018) use several auxiliary tasks, including gaze prediction, for document-level sentiment analysis. There is a lot of previous work, also, leveraging information across different sentiment analysis datasets, e.g., Liu et al. (2016).\nError detection In grammatical error detection, Rei (2017) used an unsupervised auxiliary language modeling task, which is similar in spirit to our second baseline, using frequency information as auxiliary data. Rei and Yannakoudakis (2017) go beyond this and evaluate the usefulness of many auxiliary tasks, primarily syntactic ones. They also use frequency information as an auxiliary task.\nHate speech detection In hate speech detection, many signals beyond the text are often leveraged (see Schmidt and Wiegand (2017) for an overview of the literature). Interestingly, many authors have used signals from sentiment analysis, e.g., Gitari et al. (2015), motivated by the correlation between hate speech and negative sentiment. This correlation may also explain why we see the biggest improvements with gaze-informed attention on those two tasks.\nHuman inductive bias Finally, our work relates to other work on providing better inductive biases for learning human-related tasks by observing humans (Tamuz et al., 2011;Wilson et al., 2015). We believe this is a truly exciting line of research that can help us push research horizons in many ways.", "publication_ref": ["b16", "b15", "b36", "b31", "b44", "b47", "b52", "b17", "b55", "b59"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have shown that human attention provides a useful inductive bias on machine attention in recurrent neural networks for sequence classification problems. We present an architecture that enables us to leverage human attention signals from general, publicly available eye-tracking corpora, to induce better, more robust task-specific NLP models. We evaluate our architecture and show improvements across three NLP tasks, namely sentiment analysis, grammatical error detection, and detection of abusive language. We observe that not only does human attention help models distribute their attention in a generally useful way; human attention also seems to act like a regularizer providing more robust performance across domains, and it enables better learning of task-specific attention functions through joint learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Alignment-based neural machine translation", "journal": "", "year": "2016", "authors": "Tamer Alkhouli; Gabriel Bretschner; Jan-Thorsten Peter; Mohammed Hethnawi; Andreas Guta; Hermann Ney"}, {"ref_id": "b1", "title": "Weakly supervised part-ofspeech tagging using eye-tracking data", "journal": "", "year": "2016", "authors": "Maria Barrett; Joachim Bingel; Frank Keller; Anders S\u00f8gaard"}, {"ref_id": "b2", "title": "Cross-lingual transfer of correlations between parts of speech and gaze features", "journal": "", "year": "2016", "authors": "Maria Barrett; Frank Keller; Anders S\u00f8gaard"}, {"ref_id": "b3", "title": "Reading behavior predicts syntactic categories", "journal": "", "year": "2015", "authors": "Maria Barrett; Anders S\u00f8gaard"}, {"ref_id": "b4", "title": "Using reading behavior to predict grammatical functions", "journal": "", "year": "2015", "authors": "Maria Barrett; Anders S\u00f8gaard"}, {"ref_id": "b5", "title": "Extracting token-level signals of syntactic processing from fMRI-with an application to POS induction", "journal": "", "year": "2016", "authors": "Joachim Bingel; Maria Barrett; Anders S\u00f8gaard"}, {"ref_id": "b6", "title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "journal": "", "year": "2017", "authors": "Joachim Bingel; Anders S\u00f8gaard"}, {"ref_id": "b7", "title": "Efficient attention using a fixed-size memory representation", "journal": "", "year": "2017", "authors": "Denny Britz; Melody Y Guan; Minh-Thang Luong"}, {"ref_id": "b8", "title": "Making do with what we have: use your bootstraps", "journal": "The Journal of physiology", "year": "2012", "authors": "Guillaume Calmettes; B Gordon; Sarah L Drummond;  Vowler"}, {"ref_id": "b9", "title": "What your eyes do while your mind is reading. Eye movements in reading: Perceptual and language processes", "journal": "", "year": "1983", "authors": "A Patricia; Marcel Adam Carpenter;  Just"}, {"ref_id": "b10", "title": "Eye movements in reading words and sentences", "journal": "Elsevier", "year": "2007", "authors": "Charles Clifton; Adrian Staub; Keith Rayner"}, {"ref_id": "b11", "title": "Integrating probabilistic extraction models and data mining to discover relations and patterns in text", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Aron Culotta; Andrew Mccallum; Jonathan Betz"}, {"ref_id": "b12", "title": "Human attention in visual question answering: Do humans and deep networks look at the same regions?", "journal": "Computer Vision and Image Understanding", "year": "2017", "authors": "Abhishek Das; Harsh Agrawal; Lawrence Zitnick; Devi Parikh; Dhruv Batra"}, {"ref_id": "b13", "title": "Data from eyetracking corpora as evidence for theories of syntactic processing complexity", "journal": "Cognition", "year": "2008", "authors": "Vera Demberg; Frank Keller"}, {"ref_id": "b14", "title": "Multi-task learning for multiple language translation", "journal": "", "year": "2015", "authors": "Daxiang Dong; Hua Wu; Wei He; Dianhai Yu; Haifeng Wang"}, {"ref_id": "b15", "title": "Using millions of emoji occurrences to pretrain any-domain models for detecting emotion, sentiment, and sarcasm", "journal": "", "year": "2017", "authors": "Bjarke Felbo; Alan Mislove; Anders S\u00f8gaard; Iyan Rahwan; Sune Lehmann"}, {"ref_id": "b16", "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "journal": "", "year": "2016", "authors": "Orhan Firat; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b17", "title": "A lexicon-based approach for hate speech detection", "journal": "International Journal of Multimedia and Ubiquitous Engineering", "year": "2015", "authors": "Njagi Dennis Gitari; Zhang Zuping; Hanyurwimfura Damien; Jun Long"}, {"ref_id": "b18", "title": "Nightmare at test time: robust learning by feature deletion", "journal": "", "year": "2006", "authors": "Amir Globerson; Sam Roweis"}, {"ref_id": "b19", "title": "Learning to predict readability using eye-movement data from natives and learners", "journal": "", "year": "2018", "authors": "Ana Gonzalez; - Garduno; Anders S\u00f8gaard"}, {"ref_id": "b20", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b21", "title": "ZuCo: A simultaneous EEG and eyetracking resource for natural sentence reading. Scientific data", "journal": "Under Review", "year": "2018", "authors": "Nora Hollenstein; Jonathan Rotsztejn; Marius Troendle; Andreas Pedroni; Ce Zhang; Nicolas Langer"}, {"ref_id": "b22", "title": "Learning an interactive attention policy for neural machine translation", "journal": "", "year": "2017", "authors": "Samee Ibraheem; Nicholas Altieri; John Denero"}, {"ref_id": "b23", "title": "The dundee corpus", "journal": "", "year": "2003", "authors": "Alan Kennedy; Robin Hill; Jo\u00ebl Pynte"}, {"ref_id": "b24", "title": "BNC database and word frequency lists", "journal": "", "year": "1995-12", "authors": "Adam Kilgarriff"}, {"ref_id": "b25", "title": "Looking hard: Eye tracking for detecting grammaticality of automatically compressed sentences", "journal": "", "year": "2015", "authors": "Sigrid Klerke; Anders H\u00e9ctor Mart\u00ednez Alonso;  S\u00f8gaard"}, {"ref_id": "b26", "title": "Reading metrics for estimating task efficiency with MT output", "journal": "", "year": "2015", "authors": "Sigrid Klerke; Sheila Castilho; Maria Barrett; Anders S\u00f8gaard"}, {"ref_id": "b27", "title": "Improving sentence compression by learning to predict gaze", "journal": "", "year": "2016", "authors": "Sigrid Klerke; Yoav Goldberg; Anders S\u00f8gaard"}, {"ref_id": "b28", "title": "Fixation durations before word skipping in reading", "journal": "Psychonomic Bulletin & Review", "year": "2005", "authors": "Reinhold Kliegl; Ralf Engbert"}, {"ref_id": "b29", "title": "Length, frequency, and predictability effects of words on eye movements in reading", "journal": "European Journal of Cognitive Psychology", "year": "2004", "authors": "Reinhold Kliegl; Ellen Grabner; Martin Rolfs; Ralf Engbert"}, {"ref_id": "b30", "title": "Pre-training attention mechanisms", "journal": "", "year": "2017", "authors": "Jack Lindsey"}, {"ref_id": "b31", "title": "Deep multi-task learning with shared memory", "journal": "", "year": "2016", "authors": "Pengfei Liu; Xipeng Qiu; Xuanjing Huang"}, {"ref_id": "b32", "title": "Inferring word relevance from eyemovements of readers", "journal": "ACM", "year": "2011", "authors": "D Tomasz; Peter Loboda; J\u00f6erg Brusilovsky;  Brunstein"}, {"ref_id": "b33", "title": "Multi-task sequence-to-sequence learning", "journal": "", "year": "2016", "authors": "Minh-Thang Luong; Quoc V Le; Ilya Sutskever; Oriol Vinyals; Lukasz Kaiser"}, {"ref_id": "b34", "title": "Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network", "journal": "", "year": "2017", "authors": "Abhijit Mishra; Kuntal Dey; Pushpak Bhattacharyya"}, {"ref_id": "b35", "title": "Leveraging cognitive features for sentiment analysis", "journal": "", "year": "2017", "authors": "Abhijit Mishra; Diptesh Kanojia; Seema Nagar; Kuntal Dey; Pushpak Bhattacharyya"}, {"ref_id": "b36", "title": "Cognition-cognizant sentiment analysis with multitask subjectivity summarization based on annotators' gaze behavior", "journal": "", "year": "2018", "authors": "Abhijit Mishra; Srikanth Tamilselvam; Riddhiman Dasgupta; Seema Nagar; Kuntal Dey"}, {"ref_id": "b37", "title": "Unsupervised syntactic chunking with acoustic cues: computational models for prosodic bootstrapping", "journal": "", "year": "2011", "authors": "K John; Sharon Pate;  Goldwater"}, {"ref_id": "b38", "title": "Unsupervised dependency parsing with acoustic cues", "journal": "Transactions of the Association for Computational Linguistics (TACL)", "year": "2013", "authors": "K John; Sharon Pate;  Goldwater"}, {"ref_id": "b39", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"ref_id": "b40", "title": "Keystroke dynamics as signal for shallow syntactic parsing", "journal": "", "year": "2016", "authors": "Barbara Plank"}, {"ref_id": "b41", "title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "journal": "", "year": "2016", "authors": "Barbara Plank; Yoav Goldberg; Anders S\u00f8gaard"}, {"ref_id": "b42", "title": "Andrius Vilkauskas, Agne Paulauskaite-Taraseviciene, and Gintare Kersulyte-Raudone", "journal": "Evaluation", "year": "2013", "authors": "Vidas Raudonis; Gintaras Dervinis"}, {"ref_id": "b43", "title": "On-line comprehension processes and eye movements in reading", "journal": "Academic Press", "year": "1988", "authors": "Keith Rayner; Susan A Duffy"}, {"ref_id": "b44", "title": "Semi-supervised multitask learning for sequence labeling", "journal": "", "year": "2017", "authors": "Marek Rei"}, {"ref_id": "b45", "title": "Zero-shot sequence labeling: Transferring knowledge from sentences to tokens", "journal": "", "year": "2018", "authors": "Marek Rei; Anders S\u00f8gaard"}, {"ref_id": "b46", "title": "Compositional sequence labeling models for error detection in learner writing", "journal": "", "year": "2016", "authors": "Marek Rei; Helen Yannakoudakis"}, {"ref_id": "b47", "title": "Auxiliary objectives for neural error detection models", "journal": "", "year": "2017", "authors": "Marek Rei; Helen Yannakoudakis"}, {"ref_id": "b48", "title": "Using gaze data to predict multiword expressions", "journal": "", "year": "2017", "authors": "Shiva Omid Rohanian; Victoria Taslimipoor; Le An Yaneva;  Ha"}, {"ref_id": "b49", "title": "Semeval-2015 Task 10: Sentiment analysis in Twitter", "journal": "", "year": "2015", "authors": "Sara Rosenthal; Preslav Nakov; Svetlana Kiritchenko; Saif Mohammad; Alan Ritter; Veselin Stoyanov"}, {"ref_id": "b50", "title": "Can relevance be inferred from eye movements in information retrieval", "journal": "", "year": "2003", "authors": "Jarkko Saloj\u00e4rvi; Ilpo Kojo; Jaana Simola; Samuel Kaski"}, {"ref_id": "b51", "title": "Low-cost gaze interaction: ready to deliver the promises", "journal": "ACM", "year": "2009", "authors": "Javier San Agustin; Henrik Skovsgaard; John Paulin Hansen; Dan Witzner Hansen"}, {"ref_id": "b52", "title": "A survey on hate speech detection using natural language processing", "journal": "", "year": "2017", "authors": "Anna Schmidt; Michael Wiegand"}, {"ref_id": "b53", "title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; D Christopher; Andrew Manning; Christopher Ng;  Potts"}, {"ref_id": "b54", "title": "Deep multi-task learning with low level tasks supervised at lower layers", "journal": "", "year": "2016", "authors": "Anders S\u00f8gaard; Yoav Goldberg"}, {"ref_id": "b55", "title": "Adaptively learning the crowd kernel", "journal": "", "year": "2011", "authors": "Omer Tamuz; Ce Liu; Serge Belongie; Ohad Shamir; Adam Tauman Kalai"}, {"ref_id": "b56", "title": "Learning sentence representation with guidance of human attention", "journal": "", "year": "2017", "authors": "Shaonan Wang; Jiajun Zhang; Chengqing Zong"}, {"ref_id": "b57", "title": "Are you a racist or am i seeing things? Annotator influence on hate speech detection on Twitter", "journal": "", "year": "2016", "authors": "Zeerak Waseem"}, {"ref_id": "b58", "title": "Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter", "journal": "", "year": "2016", "authors": "Zeerak Waseem; Dirk Hovy"}, {"ref_id": "b59", "title": "The human kernel", "journal": "", "year": "2015", "authors": "Andrew Wilson; Christoph Dann; Chris Lucas; Eric Xing"}, {"ref_id": "b60", "title": "Sentiment analysis in Twitter", "journal": "", "year": "2013", "authors": "Theresa Wilson; Zornitsa Kozareva; Preslav Nakov; Sara Rosenthal; Veselin Stoyanov; Alan Ritter"}, {"ref_id": "b61", "title": "A new dataset and method for automatically grading esol texts", "journal": "", "year": "2011", "authors": "Helen Yannakoudakis; Ted Briscoe; Ben Medlock"}, {"ref_id": "b62", "title": "Transfer learning for low-resource neural machine translation", "journal": "", "year": "2016", "authors": "Barret Zoph; Deniz Yuret; Jonathan May; Kevin Knight"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Overview of the tasks and datasets used.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "SEMEVAL SMS NEG 43.55 45.41 43.77 45.82 48.65 45.24 47.15 46.98 45.77 SEMEVAL SMS POS 65.79 50.81 57.08 65.92 51.04 57.45 65.46 52.95 58.50 SEMEVAL TWITTER NEG 57.39 26.87 35.70 62.50 28.66 37.78 60.52 30.67 40.23 SEMEVAL TWITTER POS 77.96 53.88 63.63 79.66 54.66 64.78 78.77 55.35 64.96 FCE 79.01 89.33 83.84 79.18 89.26 83.89 79.03 90.28 84.28 WASEEM (2016) 76.42 62.07 68.29 77.20 61.71 68.54 77.20 63.06 69.30 WASEEM AND HOVY (2016) 76.23 72.23 74.16 76.33 74.70 75.48 76.95 74.43 75.61", "figure_data": "BLBNC INV FREQMEAN FIX DURTASKPRF 1PRF 1PRF 1MEAN68.05 57.23 60.92 69.52 58.38 61.88 69.30 59.10 62.674 ExperimentsModels In our experiments, we compare threemodels: (a) a baseline model with automaticallylearned attention, (b) our model with an attention"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Sentence classification results. P(recision), R(ecall) and F 1 . Averages over 10 random seeds. The best average F 1 score per task is shown in bold.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "One sentence marked as containing sexism fromWaseem and Hovy (2016) development set. Using trained baseline (BL) and gaze model (MFD) for three tasks: error detection, sentiment classification, and hate speech detection. Words with more attention than sentence average are boldfaced.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2212 \u2192 h i = LST M (x i , \u2212 \u2212 \u2192 h i\u22121 )(1)", "formula_coordinates": [2.0, 364.76, 239.35, 160.79, 17.21]}, {"formula_id": "formula_1", "formula_text": "\u2190 \u2212 h i = LST M (x i , \u2190 \u2212 \u2212 h i+1 ) (2) h i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ](3)", "formula_coordinates": [2.0, 364.76, 266.43, 160.78, 40.02]}, {"formula_id": "formula_2", "formula_text": "h i = tanh(W h h i + b h )(4)", "formula_coordinates": [2.0, 364.78, 318.63, 160.76, 10.77]}, {"formula_id": "formula_3", "formula_text": "s = i a i h i (5) y = \u03c3(W y tanh(W\u1ef9s + b\u1ef9) + b y )(6)", "formula_coordinates": [2.0, 341.84, 447.67, 183.71, 46.51]}, {"formula_id": "formula_4", "formula_text": "e i = tanh(W e h i + b e )(7)", "formula_coordinates": [2.0, 366.34, 544.84, 159.21, 10.63]}, {"formula_id": "formula_5", "formula_text": "a i = W a e i + b a (8)", "formula_coordinates": [2.0, 381.18, 571.92, 144.37, 10.63]}, {"formula_id": "formula_6", "formula_text": "a i = a i k a k (9)", "formula_coordinates": [2.0, 389.2, 628.86, 136.35, 26.61]}, {"formula_id": "formula_7", "formula_text": "L sent = j (y (j) \u2212 y (j) ) 2(10)", "formula_coordinates": [2.0, 359.98, 743.78, 165.57, 23.67]}, {"formula_id": "formula_8", "formula_text": "L = L sent + \u03bbL tok (12", "formula_coordinates": [3.0, 138.47, 204.61, 147.25, 10.77]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [3.0, 285.72, 204.96, 4.54, 9.46]}], "doi": ""}