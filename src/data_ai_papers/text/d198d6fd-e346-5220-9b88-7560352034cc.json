{"title": "Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction", "authors": "Jeremy Reizenstein; Roman Shapovalov; Philipp Henzler; Luca Sbordone; Patrick Labatut; David Novotny", "pub_date": "2021-09-01", "abstract": "Figure 1: We introduce the Common Objects in 3D (CO3D) dataset comprising 1.5 million multi-view images of almost 19k objects from 50 MS-COCO categories annotated with accurate cameras and 3D point clouds (visualized above).", "sections": [{"heading": "Introduction", "text": "Recently, the community witnessed numerous advances in deeply learning to reconstruct category-centric 3D models. While a large variety of technical approaches was proposed [56,62,13,25,26,46,8], they are predominantly trained and benchmarked either on synthetic data [6], or on real datasets of specific object categories such as birds [62] or chairs [37]. The latter is primarily a consequence of a lack of relevant real-world datasets with 3D ground truth.\nOur main goal is therefore to collect a large-scale open real-life dataset of common objects in the wild annotated with 3D ground truth. While the latter can be collected with specialized hardware (turn-table 3D scanner, dome [29]), it is challenging to reach the scale of synthetic datasets [6] comprising thousands of instances of diverse categories.\nInstead, we devise a photogrammetric approach only requiring object-centric multi-view RGB images. Such data can be effectively gathered in huge quantities by means of crowd-sourcing \"turn-table\" videos captured with smartphones, which are nowadays a commonly owned accessory. The mature Structure-from-Motion (SfM) framework then provides 3D annotations by tracking cameras and reconstructs a dense 3D point cloud capturing the object surface.\nTo this end, we collected almost 19,000 videos of 50 MS-COCO categories with 1.5 million frames, each annotated with camera pose, where 20% of the videos are annotated with a semi-manually verified high-resolution 3D point cloud. As such, the dataset exceeds alternatives [9,1,21] in terms of number of categories and objects.\nOur work is an extension of the dataset from [26]. Here, we significantly increase the dataset size from less than 10 categories to 50 and, more importantly, conduct a humanin-the-loop check ensuring reliable accuracy of all cameras. Finally, the dataset from [26] did not contain any point cloud annotations, the examples of which are in fig. 1.\nWe also propose a novel NerFormer model that, given a small number of input source views, learns to reconstruct object categories in our dataset. NerFormer mates two of the main workhorses of machine learning and 3D computer vision: Transformers [65] and neural implicit rendering [43]. Specifically, given a set of 3D points along a rendering ray, features are sampled from known images and stacked into a tensor. The latter is in fact a ray-depth-ordered sequence of sets of sampled features which admits processing with a sequence-to-sequence Transformer. Therefore, by means of alternating feature pooling attention and ray-wise attention layers, NerFormer learns to jointly aggregate features from the source views and raymarch over them.\nImportantly, NerFormer outperforms a total of 14 baselines which leverage the most common shape representations to date. As such, our paper conducts one of the first truly large-scale evaluations of learning 3D object categories in the wild.", "publication_ref": ["b56", "b62", "b12", "b24", "b25", "b46", "b7", "b5", "b62", "b36", "b28", "b5", "b8", "b0", "b20", "b25", "b25", "b65", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "In this section we review current 3D datasets and related methods in the areas of single-image reconstruction, generative modelling and novel-view synthesis. 3D object datasets The main enabler of early works in 3D reconstruction was the synthetic ShapeNet [6] dataset. Pascal3D [72] introduced a real world dataset providing pose estimation for images, but only approximate 3D models. Choi et al. [9] provide a large set of realobject-centric RGB-D videos, however only a small subset is annotated Table 1: Common Objects in 3D compared to alternatives.\nThe \"pcl\" abbreviation stands for \"point clouds\".\nwith 3D models and cameras. Increasing the number of categories and objects, Objectron [1] contains object-centric videos, object/camera poses, point clouds, surface planes and 3D bounding boxes. Unfortunately, only a limited number of object-centric videos cover full 360 degrees. Our dataset further increases the number of categories by a factor of 5 and covers the full 360 degree range. Requiring 3D scanners, GSO [21] provides clean full 3D models including textures of real world objects. Due to the requirement of 3D scanning, it contains less objects. A detailed comparison of the aforementioned datasets is presented in tab. 1.\n3D reconstruction A vast amount of methods studied fully supervised 3D reconstruction from 2D images making use of several different representations: voxel grids [10,18], meshes [19,67], point clouds [14,73], signed distance fields, [49,2] or continuous occupancy fields [42,8,17,16]. Methods overcoming the need for 3D supervision are based on differentiable rendering allowing for comparison of 2D images rather than 3D shapes [51,63,31]. Generating images from meshes was achieved via soft rasterization in [32,40,61,30,7,36,77,20,67]. Volumetric represenations are projected to 2D via differentiable raymarching [25,15,41,43,39,55] or in a similar fashion via sphere tracing for signed distance fields [45,74]. [28] introduce differentiable point clouds. Another line of work focuses on neural rendering, i.e. neural networks are trained to approximate the rendering function [44,56]. [35,34] map pixels to object-specific derformable template shapes. [47,48] canonically align point clouds of object-categories in an unsupervised fashion, but do not reason about colour. Exploiting symmetries and reasoning about lighting [71], compose appearance, shape and lighting.\nSimilar to us, [52,75,26,60] utilize per-pixel warpconditioned embedding [26]. In contrast to our method, multi-view aggregation is handled by averaging over encodings which is prone to noise. Instead, we learn the aggregation by introducing the NerFormer module. Finally, a very recent IBRNet [68] learns to copy existing colors from known views in an IBR fashion [24,5], whereas our method can hallucinate new colors, which is crucial since we leverage far fewer source views (at most 9). Figure 2: Statistics of the Common Objects in 3D dataset reporting the total (right) and per-category (left) numbers of collected videos, and the number of videos with accurate cameras and point clouds.", "publication_ref": ["b5", "b72", "b8", "b0", "b20", "b9", "b17", "b18", "b67", "b13", "b73", "b49", "b1", "b42", "b7", "b16", "b15", "b51", "b63", "b30", "b31", "b39", "b61", "b29", "b6", "b35", "b77", "b19", "b67", "b24", "b14", "b40", "b43", "b38", "b55", "b45", "b74", "b27", "b44", "b56", "b34", "b33", "b47", "b48", "b71", "b52", "b75", "b25", "b60", "b25", "b68", "b23", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Common Objects in 3D", "text": "In this section we describe the dataset collection process. AMT video collection In order to scale the collection of object-centric videos, we crowd-sourced it on Amazon Mechanical Turk (AMT). Each AMT task asks a worker to select an object of a given category, place it on a solid surface and take a video where they keep the whole object in view while moving full circle around it. We pre-selected 50 MS-COCO [38] categories (listed in fig. 2) comprising stationary objects that are typically large enough to be reconstructed. The workers were instructed to avoid actions that would hinder the ensuing reconstruction stage such as abrupt movements leading to motion blur. Each video was reviewed to ensure that it fulfills the requirements. Generating 3D ground-truth As explained below, we use off-the-shelf software to produce object masks, camera tracking, and 3D reconstructions for each video. We then semi-automatically filter out poor reconstructions. 1) Sparse reconstruction Given the set of valid objectcentric videos, we reconstruct the extrinsic (3D location and orientation) and intrinsic (calibration) properties of the cameras that captured the videos. To this end, each video is first converted into a time-ordered sequence of images\nV = (I i | I i \u2208 R 3\u00d7H\u00d7W ) N I\ni=1 by extracting n I = 100 frames uniformly spaced in time. The frames are then fed to the COLMAP SfM pipeline [53] which annotates each image with camera projection matrices P = (P\ni | P i \u2208 R 4\u00d74 ) N I\ni=1 . Fig. 3 shows example camera tracks together with estimated sparse scene geometries.\n2) Object segmentation We segment the object in each image I i with PointRend [33], state-of-the-art instance segmentation method, resulting in a sequence of soft binary masks\nM = (M i | M i \u2208 [0, 1] H\u00d7W ) N I\ni=1 per video. Note that, while masks aid further dense reconstruction, we have not used them for camera tracking which is typically anchored to the background regions.\n3) Semantic dense reconstruction Having obtained the camera motions and segmentations, we now describe the process of annotating the captured objects with a 3D surface. We first execute the multi-view stereo (MVS) algo-rithm of COLMAP [54] to generate per-frame dense depth maps\n(D i | D i \u2208 R H\u00d7W + ) N I i=1 .\nWe then run COLMAP's point cloud fusion algorithm, which back-projects the depth values masked with M and retains the points that are consistent across frames, to get a point cloud P(V) = {x i } N P i=1 . Example dense point clouds are visualized in fig. 3. 4) Labelling reconstruction quality with Human-in-theloop. Since our reconstruction pipeline is completely automated, any of the aforementioned steps can fail, resulting in unreliable 3D annotations. We thus incorporate a semimanual check that filters inaccurate reconstructions.\nTo this end, we employ an active learning [11] pipeline which cycles between: a) manually labelling the pointcloud and camera-tracking quality; b) retraining a \"quality\" SVM classifier; and c) automatically estimating the shape/tracking quality of unlabelled videos. Details of this process are deferred to the supplementary. 5) The dataset The active-learned SVM aids the final dataset filtering step. As accurate camera annotations are crucial for the majority of recent 3D category reconstruction methods, the first filtering stage completely removes all scenes with camera tracking classified as \"inaccurate\" (18% of all videos). While all videos that pass the camera check are suitable for training, the scenes that pass both camera and point cloud checks (30% of the videos with accurate cameras) comprise the pool from which evaluation videos are selected. Note, a failure to pass the point cloud check does not entail that the corresponding scene is irreconstructible and should therefore be removed from training -instead this merely implies that the MVS method [54] failed, while other alternatives (see sec. 5) could succeed.\nFig. 2 summarizes the size of Common Objects in 3D. Reconstructing a single video took on average 1h 56 minutes with the majority of execution time spent on GPUaccelerated MVS and correspondence estimation. This amounts to the total of 43819 GPU-hours of reconstruction time distributed to a large GPU cluster.", "publication_ref": ["b37", "b53", "b32", "b54", "b10", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "Learning 3D categories in the wild", "text": "Here, we describe the problem set, give an overview of implicit shape representations, and explain our main techni- and their cameras {P src i } Nsrc i=1 which, in our case, are samples from the set of frames V v and cameras P v of a video v.\nIn order to visualize the generated shape and appearance, we differentiably render it to a target view for which only the camera parameters P tgt \u2208 R 4\u00d74 are known. This results in a render\u00ce tgt which is, during training, incentivised to match the ground truth target view I tgt .\nMethods are trained on a dataset of category-centric videos {V v } n V v=1 , which allows to exploit the regular structure of the category for better generalization to previously unseen objects. Note that, in our experiments (sec. 5), we additionally consider \"overfitting\" with n V = 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Representing common objects in 3D", "text": "In order to predict novel views, all methods need to reconstruct 3D shape in some form. The latter can be represented with a plethora of recent frameworks such as voxel grids [64,41], implicit surfaces [43,46,45], point clouds [28,69], or meshes [40,50]. Among those, implicit representations have been successfully applied to reconstructing real scenes and object categories [43,25,75], therefore we choose to use them in our pipeline. Implicit surface An implicit surface is defined as a level set S f = {x | f (x, z) = C, C \u2208 R} of a function f : R 3 \u00d7 D z \u2192 R that accepts a 3D point x \u2208 R 3 and D zdimensional latent code z \u2208 R Dz . In addition to f , which represents geometry, a second function c : R 3 \u00d7 S 2 \u00d7 D z \u2192 R 3 assigns colors c(x, r, z) to the input points x. Note that, in line with recent work [43,74], c is further conditioned on the 3D direction vector r \u2208 S 2 from which x is imaged in order to model viewpoint-dependent effects such as specularities. Finally, both functions f and c depend on the latent code z encoding geometry and appearance of the scene.\nChanges in z alter the level set of f and colors c allowing for representing different instances of object categories.\nFollowing recent successes of [55,26], we model category-specific 3D shapes with opacity functions f o . Specifically, f o assigns f o (x, z) = 0 to the unoccupied points x / \u2208 S f , and f (x , z) > 0 to surface points x \u2208 S f .\nNeural implicit surfaces Recent methods implement functions f o and c as multi-layer perceptrons (MLP) f MLP and c MLP [43,45,22,2]. They typically learn shallow specialized networks on top of the shared deep feature extractor\nf MLP : f MLP = f HEAD \u2022 f MLP and c MLP = c HEAD \u2022 f MLP .\nWe depart from representing occupancies with plain MLPs as they process input 3D points x independently, without any form of spatial reasoning, which is crucial in our case where input source views provide only a partial information about the reconstructed shape.\nPositional embedding Following [43,66], avoiding loss of detail, we pre-process the raw 3D coordinates x with a positional embedding (PE) \u03b3(x) = [sin(x), cos(x), ..., sin(2 N f x), cos(2 N f x)] \u2208 R 2N f before feeding to f MLP (x, z). While [43] was the first to demonstrate benefits of PE, in sec. 5 we also combine PE with other pre-NeRF methods, such as SRN [56] or DVR [45].\nRendering an implicit surface In order to admit imagesupervised learning, implicit surfaces are converted into an explicit representation of appearance and geometry with a rendering function r. Formally, given a target camera P tgt , the goal is to generate the target image I tgt = r(f, c, P tgt ) which depicts the scene from P tgt 's viewpoint.\nWe render opacity fields with the Emission-Absorption model (EA). EA renders the RGB value I tgt u \u2208 R 3 at a pixel u \u2208 {1, . . . , W } \u00d7 {1, ..., H}, by evaluating the opacity function f o (x, z) for an ordered point-tuple (x ru i ) NS i=1 sampled along u's projection ray r u at approximately equidistant intervals \u2206. The color    5: NerFormer learns to attend to features from source images. A ray is emitted from a target-image pixel (1st column), and its points are projected to the source views (columns 2-8) from which features are sampled. For each source feature, NerFormer predicts attention (red=high, blue=low) for aggregating to a single source embedding per point. Note how the attention model implicitly learns to pool features from nearby source views.\nI tgt u (r u , z) = NS i=1 w i (x ru i , z)c(x ru i , r u , z) is a", "publication_ref": ["b64", "b40", "b43", "b46", "b45", "b27", "b69", "b39", "b50", "b43", "b24", "b75", "b43", "b74", "b55", "b25", "b43", "b45", "b21", "b1", "b43", "b66", "b43", "b56", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "c(x ru", "text": "i , r u , z) weighted by the emission-absorption product\nw i = i\u22121 j=0 T j (1 \u2212 T i ) with T i = exp(\u2212\u2206f o (x ru i , z)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Latent shape encoding z", "text": "A crucial part of a category-centric reconstructor is the latent embedding z. Early methods [18,70,58,61,40] predicted a global scene encoding z global = \u03a6 CNN (I src ) with a deep convolutional network \u03a6 CNN that solely analyzed the colors of source image pixels. While this approach was successful for the synthetic ShapeNet dataset where shapes are rigidly aligned, it has been recently shown in [26,75] that such approach is infeasible in real world settings where objects are arbitrarily placed in the 3D space. This is because, unlike in the former case where color-based shape inference is possible since similar images of aligned scenes generate similar 3D shapes, in the latter case, similarly looking images of unaligned scenes can generate vastly different 3D.\nWarp-conditioned embedding To fix the latter, [26] proposed Warp-Conditioned Embedding (WCE): given a world coordinate point x and a source view I src with camera P src , the warp-conditioned embedding z WCE \u2208 R Dz z WCE (x, I src , P src ) = \u03a8 CNN (I src )[\u03c0 P src (x)], is formed by sampling a tensor of source image descriptors \u03a8 CNN (I src ) \u2208 R Dz\u00d7H\u00d7W at a 2D location \u03c0 P src (x).\nHere, \u03c0 P src (x) = P src [x; 1] = d u [u; 1] expresses perspective camera projection of a 3D point x to a pixel u with depth d u \u2208 R. Intuitively, since z WCE is a function of the world coordinate x, the ensuing implicit f can perceive the specific 3D location of the sampled appearance element in the world coordinates, which in turn enables f to learn invariance to rigid scene misalignment.\nIn the common case where multiple source views are given, the aggregate WCE z * WCE (x, {I src i }, {P src i }) is defined as a concatenation of the mean and standard deviation of the set of view-specific source embeddings {z WCE (x, I src i , P src )} Nsrc i=1 . Boosting baselines with WCE The vast majority of existing methods for learning 3D categories leverage global shape embeddings z global , which renders them inapplicable to our real dataset. As WCE has been designed to alleviate this critical flaw, and because of its generic nature, in this paper we endow state-of-the-art category-centric 3D reconstruction methods with WCE in order to enable them for learning category-specific models on our real dataset.\nTo this end, we complement SRN [56], NeuralVolumes [41], and the Implicit Point Cloud (discussed later) with WCE. These extensions are detailed in the supplementary.", "publication_ref": ["b17", "b70", "b58", "b61", "b39", "b25", "b75", "b25", "b56", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Attention is all you nerf", "text": "Limitations of WCE While [26] has demonstrated that the combination of NeRF and WCE (termed NeRF-WCE) leads to performance improvements, our experiments indicated that a major shortcoming of NeRF-WCE is its inability to deal with cases where parts of the 3D domain are labelled with noisy WCE. This is because NeRF's MLP f MLP independently processes each 3D point and, as such, cannot detect failures and recover from them via spatial reasoning. The 3D deconvolutions of Neural Volumes [41] are a potential solution, but we found that the method ultimately produces blurry renders due to the limited resolution of the voxel grid. The LSTM marcher of SRN [56] is capable of spatial reasoning, which is however somewhat limited due to the low-capacity of the LSTM cell. Last but not least, a fundamental flaw is that simple averaging of the sourceview WCE embeddings can suppress important features.\nOur main technical contribution aims to alleviate these issues and follows a combination of two main design guidelines: 1) We replace the f MLP with a more powerful architecture capable of spatial reasoning. 2) Instead of engineering the WCE aggregation function, we propose to learn it.", "publication_ref": ["b25", "b40", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Tgt. img.", "text": "IPC SRN [56] NV [41] IDR [74] NeRF [43] NerFormer Figure 6: Single-scene new-view synthesis on Common Objects in 3D depicting a target image from the training video (left), and corresponding synthesized view generated by IPC, SRN [56], NV [41], IDR [74], NeRF [43], and our NerFormer .\nNerFormer As a solution, we propose to leverage the popular Transformer architecture [65].\nEmploying a sequence-to-sequence model is intuitive since the set of all WCE embeddings along a projection ray is in fact a depthordered descriptor sequence along the ray dimension, and an unordered sequence along the feature pooling dimension.\nFormally, given a ray r u , we define Z ru \u2208 R NS\u00d7Nsrc\u00d7Dz as a stacking of un-aggregated WCEs of all ray-points x ru j :\nZ ru = z WCE (x ru j , I src i , P src i ) nsrc i=1 NS j=1\n.\n(1)\nThe NerFormer module f TR (Z ru ) = f HEAD TR \u2022 TE L \u2022 \u2022 \u2022 \u2022 \u2022 TE 1 (Z ru ) replaces the feature backbone f MLP (sec. 4.1) with a series of L 3D transformer modules TE l terminated by a weighted pooling head f HEAD TR (fig. 4). Here, each 3D transformer module TE l is a pairing of Transformer Encoder [65] layers TE 0 (Z) and TE 1 (Z):\nMHA d l (Z l ) = Z l = LN(MHA l (Z l , dim=d) + Z l ) (2) TE d l (Z l ) = LN(MLP l (Z l ) + Z l )(3)\nTE l (Z l ) = TE 0 l (TE 1 l (Z l )) = Z l+1 ,(4)\nMHA(Z, dim=d) is a multi-head attention layer [65] whose attention vectors span the d-th dimension of the input tensor Z, MLP is a two-layer MLP with ReLU activation, and LN is Layer Normalization [3]. Intuitively, the alternation between ray and pooling attention of TE 0 (Z) and TE 1 (Z) facilitates learning to jointly aggregate WCE features from the source views and ray-march over them respectively. Finally, f TR is terminated by a weighted pooling head f HEAD TR (Z L ) that aggregates the second dimension of Z L output by the final L-th 3D transformer module TE L :\nf HEAD TR (Z L ) = nsrc i=1 \u03c9 i (Z L )Z L [:, i, :] \u2208 R NS\u00d7Dz ,\nwhere the weights \u03c9 i \u2208 [0, 1], i \u03c9 i = 1 are output by a linear layer with softmax activation. We show \u03c9 i in fig. 5.  The final opacity and coloring functions of NerFormer are thus f o = f HEAD \u2022 f TR , c o = c HEAD \u2022 f TR respectively, which are rendered with the EA raymarcher (sec. 4.1).", "publication_ref": ["b56", "b40", "b74", "b43", "b56", "b40", "b74", "b43", "b65", "b65", "b65", "b2"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Technical details", "text": "Training minimizes, with Adam (learning rate 5\u202210 \u22124 ), a sum of the RGB MSE error I tgt \u2212\u00ce tgt 2 and the binary cross entropy between the rendered alpha maskM tgt and the ground truth mask M tgt . We iterate over batches comprising a randomly sampled target view and 1 to 9 source views of a training video until convergence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Datasets In order to evaluate and train reconstruction methods on our dataset, we split the 18,619 collected CO3D videos into 4 different sets as follows. For each of the 50 categories, we split its videos to a train and a test set in 9:1 ratio. For each video, we further define a set of frames that are removed from the training set by randomly dividing each train video in a 8:2 ratio to 80 train-known training and 20 train-unseen holdout frames. test video frames are split according to the same protocol resulting in test-known and test-unseen sets. As all base-   Evaluation protocol All evaluations focus on new RGB+D-view synthesis where a method, given a known target camera P tgt and a set of source views {I src i } nsrc i=1 , renders new target RGB view\u00ce tgt and depthD tgt . Four metrics are reported: The peak-signal-to-noise ratio (PSNR) and the LPIPS distance [76] between\u00ce tgt and the ground truth target RGB frame I tgt , the Jaccard index between the rendered/g.t. object maskM tgt /M tgt and the 1 distance depth 1 between the rendered depthD tgt and the ground truth point cloud depth render D tgt . Note that PSNR and depth 1 are only aggregated over the foreground pixels {u fg |M tgt [u fg ] = 1}. Metrics are evaluated at an image resolution of 800x800 and 400x400 pixels for single-scene (sec. 5.2) and category-specific reconstruction (sec. 5.3) respectively.", "publication_ref": ["b76"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluated methods", "text": "We implemented a total of 15 approaches that leverage the majority of common shape representation types.\nImplicit surfaces Among implicit surface estimators we selected the seminal opacity-based NeRF; and IDR [74] and DVR [45] expressing shapes as a SDF f d and render with sphere-tracing. We further benchmark SRN [56] which implements an implicit learned LSTM renderer. Importantly, following secs. 4.1 and 4.2, we evaluate the modifications SRN-\u03b3, DVR-\u03b3 that endow SRN and DVR respectively with positional embedding \u03b3. Furthermore, SRN-\u03b3-WCE, SRN-WCE, NeRF-WCE [26] complement SRN and NeRF with the Warp-conditioned Embedding [26]. Finally, we also evaluate our NerFormer method.\nVoxel grids Neural Volumes (NV) [41] represents the State of the Art among voxel grid predictors. Similar to implicit methods, we also combine NV with WCE. Point clouds In order to compare with a point cloudbased method, we devised an Implicit Point Cloud (IPC) baseline which represents shapes with a colored set of 3D points, converts the set into an implicit surface and then renders it with the EA raymarcher. We note that IPC is strongly inspired by SynSin [50,69] (see supplementary). Meshes We benchmark P3DMesh -the best-performing variant of PyTorch3D's soft mesh rasterizer from [50] inspired by Pixel2Mesh [67], which deforms an initial spherical mesh template with a fixed topology with a series of convolutions on the mesh graph.", "publication_ref": ["b74", "b45", "b56", "b25", "b25", "b40", "b50", "b69", "b50", "b67"], "figure_ref": [], "table_ref": []}, {"heading": "Single-scene reconstruction", "text": "We first task the approaches to independently reconstruct individual object videos. More specifically, given a test video, every baseline is trained to reproduce the video's test-known frames (by minimizing methodspecific loss functions such as 2 RGB error), and evaluated by comparing the renders from the given test-known camera viewpoints to the corresponding ground truth images/depths/masks. Since training on all \u223c2k test videos is prohibitively expensive (each baseline trains at least for 24 hours on a single GPU), we test on 40 randomly selected test videos (two from each of 20 random classes). Quantitative / qualitative results are in fig. 6 / Tab. 2.\nNerFormer is either the best or the second best across all metrics. NeRF+WCE is beaten by vanilla NeRF which suggests that the noisy WCE embeddings can hurt performance without NerFormer 's spatial reasoning. Interestingly, IDR's realistic, but less detailed renders win in terms of LPIPS, but are inferior in PSNR. Furthermore, we observed a large Tgt. img. ---Figure 7: Category-centric 3D reconstruction on Common Objects in 3D depicting a target image, known source images {I src i }, and a synthesized new view. The first/last 3 rows are from the train/test set (SRN+AD is not applicable to test).\ndiscrepancy between the train and test PSNR of \u03b3/WCEendowed SRN. This shows that, for the single-scene setting, increasing the model expressivity with WCE or \u03b3 can lead to overfitting to the training views.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning 3D Object Categories", "text": "Our main task is learning category-centric 3D models. In more detail, a single model is trained on all train-known frames of an object category and evaluated on 1000 randomly generated test samples from train-unseen and test-unseen sets of the category. Each train or test sample is composed of a single target image I tgt and randomly selected source views {I src } nsrc i , where n src is randomly picked from {1, 3, 5, 7, 9}. Since training a model for each of the 50 object categories is prohibitively expensive (each method takes at least 7 days to train), we chose a subset of 10 categories for evaluation. Baselines For each method we evaluate the ability to represent the shape space of training object instances by turning it into an autodecoder [4] which, in an encoder-less manner, learns a separate latent embedding z scene (sequence ID ) for each train scene as a free training parameter. Since autodecoders (abbreviated with the +AD suffix) only represent the train set, we further compare to all WCE-based methods which can additionally reconstruct test videos. Note that, as remarked in [26] and sec. 4.2, the alternative encoding z global = \u03a6 CNN (I src ) is deemed to fail due to the world coordinate ambiguity of the training SfM reconstructions. Results Quantitative and qualitative comparisons are shown in tab. 3 and fig. 7 respectively. Besides average met-rics over both test sets, we also analyze the dependence on the number of available source views, and on the difficulty of the target view. For the latter, test frames are annotated with a measure of their distance to the set of available source views, and then split into 3 different difficulty bins. Average per-bin PSNR is reported. The supplementary details the distance metric and difficulty bins.\nWhile SRN+AD has the best performance across all metrics on train-unseen; on the test-unseen set, where SRN+AD is inapplicable, our NerFormer is the best for most color metrics. Among implicit methods, the SDFbased DVR and IDR are outperformed by the opacity-based NeRF, with both DVR+WCE and IDR+WCE failing to converge. This is likely because regressing SDFs is more challenging than classifying 3D space with binary labels opaque/transparent. Finally, we observed poor performance of P3DMesh, probably due to the inability of meshes to represent complicated real geometries and textures.", "publication_ref": ["b3", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have introduced Common Objects in 3D (CO3D), a dataset of in-the-wild object-centric videos capturing 50 object categories with camera and point cloud annotations.\nWe further contributed NerFormer which is a marriage between Transformer and neural implicit rendering that can reconstruct 3D object categories from CO3D with better accuracy than a total of 14 other tested baselines.\nThe CO3D collection effort still continues at a steady pace of \u223c500 videos per week which we plan to release in the near future.  In what follows, we provide additional quantitative results (sec. A), technical details of NerFormer and the baselines (sec. B), and details of the Human-in-the-loop 3D annotation process (sec. C).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary material", "text": "Sampled features ! ! ! \" \" \u00d7 \" #!$ \u00d7 $ % Source view % & #!$ S o u rc e v ie w % ' # ! $ Sou rce vie w % ( #!$ Source view feature s \u03a8 & #!$ Linear layer D ) \u00d7 80 Multi-head att. along dim d 2-layer MLP Linear ! !\" \u00d7 ! !\" \u2192 ReLU \u2192 Linear ! !\" \u00d7 ! #$%", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Additional results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Results on all 50 categories", "text": "While tab. 3 in the main paper provides results on a subset of 10 object categories for all baselines, for completeness, in tab. I, we provide evaluation on all 50 object classes for 4 best-performing baselines according to results reported in tab. 3: NerFormer, SRN+WCE, SRN+\u03b3+WCE, and NeRF+WCE.\nSimilar to tab. 3, on the test-unseen set, NerFormer is the best in all color-based metrics, suggesting that SRN and NeRF+WCE are prone to overfitting to the training scenes. While SRN outperforms NerFormer in some cases on train-unseen, we note that the autodecoders would likely yield superior performance on train-unseen due to their ability to capture the information from all views of a training scene in the latent scene-specific encoding. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2. Convergence speed", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3. Execution speed", "text": "Tab. II contains an evaluation of execution times for all methods from tab. 2. Here, each row reports an average time to render an 800x800 pixel image on NVIDIA Tesla V100 GPU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4. Test-time autodecoder optimization", "text": "In tab. III, we provide an extension of tab. 3 containing the evaluation of the best-performing autodecoding methods at test-time. First, each method is first trained on train-known. Then, during evaluation, the latter freezes the trained weights and optimizes the input latent code for a given set of source frames from a test sequence. The latent codes are optimized with Adam until convergence, decaying the learning rate 10-fold whenever the optimization   We observed that the latent code optimization was mostly failing for NeRF and NV. On the other hand, SRN gave slightly better performance, which we attribute to the higher smoothness of the implicit function compared to the NeRF and NV (SRN contains normalization layers while the other baselines are bare MLPs interleaving linear layers and ReLUs).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5. Estimating new-view difficulty", "text": "Tab. 3 contains metrics evaluated separately for three viewpoint difficulty bins. Here, we detail the process of estimating the difficulty of a testing target view.\nCamera difficulty D Given a target camera P tgt and a set of available source views {P src i } Nsrc i=1 , the difficulty of the tar-get view D(P tgt ) \u2208 [0, 1] is quantified as the average of the two lowest distances d(P tgt , P src i ) between the target view and each of the source views.\nCamera distance d cam The distance d cam (P i , P j ) \u2208 [0, 1] between two cameras P i and P j is defined as follows. We first generate a cubical voxel grid of size 32 3 in the center of the scene with the voxel size set such that the majority of the grid is observed by all cameras in the scene. Each point x k , denoting the coordinates of the center of a cell in the voxel grid, is then projected to both cameras leading to a pair of projection rays r i k , r j k \u2208 S 2 . We then define the similarity s(\nr i k , r j k ) = \u03b4[\u03c0 Pi (x k ) \u2208 \u2126 i \u2227 \u03c0 Pj (x k ) \u2208 \u2126 j ](1 + r i k \u2022 r j k )\nas a dot product between the pair of rays weighted by an indicator that checks whether the projection of x k simultaneously lands in the rasters \u2126 i , \u2126 j \u2208 [0, W ] \u00d7 [0, H] of both cameras P i and P j . The camera distance d cam is then defined as one minus the intersection-over-union of the similarities between all pairs of rays generated by each voxel grid point x k :\nd cam (P i , P j ) = 1 \u2212 k s(r i k , r j k ) k s(r i k , r i k ) + s(r j k , r j k ) \u2212 s(r i k , r j k )\n.\nIntuitively, the camera distance is proportional to the angle between the camera heading vectors adjusted by the overlap between the voxels observed by both cameras. However, merely considering the heading vectors would not take into account the intrinsics of the cameras (focal length / principal point). We thus devised d cam which leverages angles between projection rays, which are a function of both the intrinsics and extrinsics.\nIn order to understand d cam , consider the following two examples: Two cameras observing the same set of voxels at a relative angle of 0.5\u03c0 would have d cam (P i , P j ) \u2248 2 3 , while opposite-facing cameras would yield a maximum possible d cam (P i , P j ) \u2248 1.\nCamera difficulty bins Each testing target camera P tgt is then assigned into one of 3 difficulty bins (easy, medium, hard) depending on its difficulty measure D(P tgt ). More specifically, the easy cameras satisfy 0 \u2264 D(P tgt ) < 1 6 , medium 1 6 \u2264 D(P tgt ) < 1 3 , and hard D(P tgt ) \u2265 1 3 .  Table III: Autodecoder latent optimization on test-unseen extending the results in tab. 3. Each method labelled with +AD is first trained on train-known. During evaluation the latter fixes the trained weights and optimizes the input latent code for a given set of source frames from a test sequence. For context, we also compare to NerFormer , which is not an autodecoder.", "publication_ref": ["b1", "b0", "b0"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "B. Additional technical details", "text": "In this section we provide additional details of Ner-Former and of the benchmarked baseline approaches which were outlined in sec. 5.\nsimply convert the input coordinates to positional embeddings and adjust the number of input channels of the first layer of DVR's implicit function accordingly. The released code [45] supports DVR+AD so no changes were required here. As mentioned in the paper, unfortunately, all our attempts to merge DVR with WCE lead to a non-converging model. IDR Similar to DVR, IDR [74] supports our supervisory setup by default. In order to implement IDR+AD, we append the latent code z to the positional embeddings that are input to the implicit function, and we adjust the number of input channels of the first layer of the implicit function accordingly. IDR already takes as input the positional embeddings \u03b3, so the extension IDR+\u03b3 does not apply here. As mentioned in the main paper, we could not obtain a converging version of IDR+WCE.", "publication_ref": ["b45", "b74"], "figure_ref": [], "table_ref": []}, {"heading": "B.4. SRN", "text": "Here, we first give a brief overview of the learned SRN raymarcher, followed by describing the WCE extension of SRN.\nNeural raymarching Contrasted to the explicit formulations of sphere-tracing or EA, recently, SRN [56] proposed to learn to march along the projection rays with a recurrent deep network. Similar to sphere-tracing, SRN decides at iteration t on the length of the raymarching step \u2206 t by evaluating a function at the current intersection estimate x ru t . However, instead of querying the SDF, SRN utilizes an LSTM [27] cell f LSTM (x ru t , r u , z, h t ) = (\u2206 t , h t+1 ) which is additionally conditioned on the ray direction r u and a temporal hidden state h t . In this manner, the raymarcher adapts the step-size prediction based on the past marching observations. SRN+WCE The WCE exension of SRN is straightforwardly implemented by replacing the iterative invocation of the global-encoding-conditioned implicit f LSTM (x ru t , r u , z global , h t ) of the SRN's raymarcher with the WCE-conditioned implicit\nf LSTM (x ru t , r u , z WCE (x ru t , {I src i }, {P src i }), h t )\nThis way, the learned raymarcher can \"tap\"\" into the source views during every iteration to receive a more direct triangulation signal. As apparent from tabs. 3 and I, our WCE extension of SRN provides a very strong baseline that in fact achieves the best depth prediction performance. Mask prediction The learned raymarcher of the original version of SRN does not render an alpha mask of the foreground object. In order to enable the latter, we extend the last layer of the SRN's coloring function c with an additional channel that is terminated with a sigmoid activation and represents the alpha value of the corresponding pixel u. This channel is then supervised by minimizing the DICE coefficient between its output and the ground truth segmentation masks M tgt .", "publication_ref": ["b56", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "B.5. P3DMesh", "text": "As mentioned in the paper, P3DMesh [50] deforms an initial spherical mesh template with a fixed topology with a series of convolutions on the mesh graph. As in [67], the graph convolutions accept features sampled from the source images at the 2D projections of the mesh vertices. Since P3DMesh supports conditioning only on a singlesource-view, we extend to the multi-view setting by averaging over the per-vertex features sampled from each of the source views. Furthermore, note that the implementation in [50] differentiably renders the mesh with a memoryefficient version of the Soft Rasterizer [40]. The training protocol, including the employed losses and their weighting, closely follows [50].", "publication_ref": ["b50", "b67", "b50", "b39", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "B.6. Neural Volumes (NV)", "text": "Neural Volumes [41] is a method that represents implicit surfaces as voxel grids. In what follows, we first briefly describe voxel grids, their specific implementation in NV, and its extension with warp-conditioned embedding (NV+WCE). Voxel grids While MLPs can label an arbitrary element of the 3D domain, a voxel grid can be seen as an implicit surface restricted to a subset of R 3 which is uniformly subdivided to a lattice V (z) \u2208 R R 3 of R 3 ; R \u2208 N + cuboid elements of the same size. Note that the lattice V (z) is a function (typically a 3D deconvnet) of z which allows for representing different 3D shapes. The implicit function f voxel (x, z) = \u03b6(V (z), x) is then evaluated by sampling V (z) at the corresponding world coordinate x, with a grid-sampling function \u03b6 : R R 3 \u00d7 R 3 \u2192 R dim(f ) , such as trilinear interpolation. Voxel grids also admit coloring via a volume C(z) \u2208 R 3\u00d7R 3 which can be sampled in an analogous manner. Neural Volumes A notable voxel-grid-based method is Neural Volumes [41], which proposed an improved sampling function \u03b6 warp (\u03b6(W (z), x) + x, V (z)) which refines the sampling location x with an offset vector \u03b6(W (z), x) \u2208 R 3 sampled from a warping lattice W (z) \u2208 R R 3 . Here both W and V are implemented as a 3D deconvolutional network. NV and NV+AD NV+AD is in fact the vanilla version of [41] whose 3D deconvnets V and W accept the scene-specific latent code z scene (sequence ID ) (described in sec. 5.3). The \"overfitting\" version of NV from tab. 2 is a special case of NV+AD with a single latent code. NV+WCE The WCE extension of Neural Volumes (NV+WCE) appends the WCE to the feature of each voxel after the second 3D deconvolution layer of the 3D convnets V and W . Here, the WCE of a voxel is generated by expressing the world coordinate x Vi of the center of the correspoding voxel V i and calculating the aggregate WCE z WCE (x Vi , {I src i }, {P src i }) for a set of source views {I src i } and their cameras {P src i }. Note that a similar approach has been proposed in [31]. Training All versions of NV optimize the losses from [41] with the original weights. Furthermore, we exploit the known ground truth segmentation masks and minimize the binary cross entropy between the alpha mask returned by the raymarcher of NV and the ground truth mask M tgt .", "publication_ref": ["b40", "b40", "b40", "b30", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "B.7. Implicit Point Cloud (IPC)", "text": "As mentioned in sec. 5, IPC represents shapes by converting a point cloud to an implicit function which is later rendered with EA raymarching.\nFormally, let a point cloud P(z) = {x i } Npts i=1 be an N ptssized unordered set of points, where P is a point cloud predictor (detailed later in this section) which accepts the latent code z. P(z) then admits an occupancy function f P defined as follows:\nf P (x , z) = \u03b4[ NN P(z) (x ) \u2212 x < ],\nwhere NN P(z) (x ) = arg min x\u2208P(z) x \u2212 x returns the nearest point from the point cloud P(z) to the query point x . Intuitively, f P yields zero everywhere except within an neighborhood of each point cloud point x i \u2208 P(z), where f P yields 1. As we describe later, anchoring the implicit function on the set of cloud points allows for faster and more memory-efficient EA raymarching than in the case of the neural implicit occupancy f MLP (described in sec. 4.1).\nIn order to color the implicit point cloud, we define its coloring function c IPC : c IPC (x , r, z) = c MLP (NN P(z) (x ), r, z).\nHere c IPC attaches to an arbitrary point x the response of the coloring MLP c MLP at x 's nearest point cloud neighbor NN P(z) (x ). Rendering IPC IPC is rendered efficiently with the Py-Torch3D point cloud renderer [50,69]. More specifically, given a target camera P tgt , each point from the predicted point cloud P(z) is projected to the camera plane to form a set of 2D projections {\u03c0 P tgt (x i )|x i \u2208 P(z)}. For each pixel coordinate u \u2208 {1, ..., W } \u00d7 {1, ..., H} in the rendering lattice of the target render\u00ce tgt \u2208 R 3\u00d7H\u00d7W , the renderer records the ordered set\n\u03a0 u (P(z)) = x i |x i \u2208 P(z); \u03c0 P tgt (x i ) \u2212 u \u2264 f P tgt ; d P tgt (x i ) \u2264 d P tgt (x i+1 ) ,\nof point cloud points x i \u2208 P(z) whose 2D projections \u03c0 P tgt (x i ) land within the f P tgt distance from the pixel u, and which is ordered by the depth d P tgt (x i ) of each point in the target camera P tgt . f P tgt \u2208 R is the focal length of the target camera P tgt . Intuitively, \u03a0 u (P(z)) denotes the set of point cloud points whose neighborhoods are intersected by the rendering ray r u emitted from pixel u. Note that this is an approximation: comparing the 2D camera-plane distance \u03c0 P tgt (x i ) \u2212 u to the constant f P tgt corresponds to orthographic projections of the point neighborhoods, whereas our cameras are perspective. However, the orthographic approximation is mild in our case, since the distance of the point cloud points from the camera is relatively large compared to its focal length.\nThe EA raymarching then takes the set of u's 3D points \u03a0 u (P(z)) in order to render the color\u00ce tgt u \u2208 R 3 :\nI tgt u (r u , z) = xi\u2208\u03a0 u (P(z)) w i (x i , z, u)c IPC (x i , r u , z). For IPC, the weight w i (x i , z, u) = i\u22121 j=0 T IPC j (x i , z, u) 1 \u2212 T IPC i (x i , z, u)\nis the product of emission and absorption functions with the transmission term T IPC i defined as\nT IPC i (x i , z, u) = f P (x i , z) =1 u \u2212 \u03c0 P tgt (x i ) f P tgt ,\nwhich approximately measures the amount of light transmitted through the spherical neigborhood of a point x i which intersects the projection ray r u . To demonstrate this, observe that for a pixel u intersect = \u03c0 P tgt (x i ) which coincides with the projection of the 3D point x i , the transmission T IPC i (x i , z, u intersect ) = 0, i.e. no light is transmitted through x i and the corresponding color c IPC (x i , r u intersect , z) is fully rendered. On the contrary, for a pixel u outside = \u03c0 P tgt (x i + ) outside the epsilon neighborhood, the unit transmission T IPC i (x i , z, u outside ) = 1 signifies that all light passes through and the point's color is ignored during rendering. Note that the above equation is very similar to the top-k point cloud rasterizer of SinSyn [69]. Point cloud predictor P(z) The point cloud predictor P(z) is the same for both IPC+AD and IPC+WCE. More specifically, P(z) = {x i + o MLP (x i , z)} Npts i=1 offsets a fixed set of template pointsP = {x i } Npts i=1 with an offset function o : R 3 \u00d7 R Dz \u2192 R 3 implemented as an MLP with the same architecture as f MLP . Therefore, o alters the template point cloud to match a specific shape given its latent shape code z. IPC+AD and IPC+WCE For IPC+AD, the offset function o accepts the video-specific latent code z scene (sequence ID ) described in sec. 5.3, while for IPC+WCE, o takes as input the aggregate warp-conditioned embedding z WCE (x i , {I src i }, {P src i }) evaluated at each template point x i \u2208P. Finally, the single-scene version, abbreviated simply as IPC in tab. 2, is a special case of IPC+AD with z scene (sequence ID ) := 0 set to a constant zero vector. ", "publication_ref": ["b50", "b69", "b69"], "figure_ref": [], "table_ref": []}, {"heading": "R", "text": "The average 1 depth error between the renders of the fused pointcloud P(V) into each camera Pi of a video V and the corresponding dense depth map Pi. P CL render rgb R\nThe average 1 RGB error between the renders of the fused pointcloud P(V) into each camera Pi of a video V and the corresponding frame Ii. P CL render\nIoU [0, 1]\nThe average Jaccard Index between the renders of the fused point cloud P(V) into each camera Pi of a video V and the corresponding PointRend segmentation Mi. P CL direction cover N Measures the coverage of the views of the point cloud P(V) with the number of occupied bins in the azimuth/elevation map of projection rays corresponding to each dense point cloud point xj and a camera Pi. Training All versions of IPC optimize the MSE between the rendered image\u00ce tgt and the ground truth colors I tgt . Furthermore, we make use of the ground truth segmentation masks and minimize the Chamfer distance between the set of 2D projections of the predicted point cloud points P(z), and the 2D points of the ground truth segmentation mask [36]. Note that a standard segmentation loss, such as DICE [57] or Binary Cross Entropy between the rendered alpha mask and the ground truth segmentation mask, do not apply here. This is because the gradients generated by the alpha mask renders of IPC are not well-defined and do not lead to convergence.", "publication_ref": ["b35", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "C. 3D annotations with Human-in-the-loop", "text": "In sec. 3, we outlined the process of annotating the AMTcollected videos with 3D ground truth. Here, we further detail the semi-automated process of labelling the quality of camera tracking and the 3D dense point cloud of the captured videos (Paragraph 4 in sec. 3).\nWe initialize the process by annotating an initial set of several hundreds of reconstructions with a binary label \"accurate / inaccurate\" by visually inspecting both the camera tracks (P i |P i \u2208 R 4\u00d74 ) N I i=1 and the scene point cloud P(V). From each video, we then extract various metrics that are indicative of the reconstruction quality such as a per-pixel RGB and depth error of the rendered point cloud, the number of registered cameras, final bundle adjustment energy etc. The full set of metrics is outlined in tab. IV. We then train a binary Support Vector Machine (SVM [12]) with an RBF kernel that regresses the binary label given the reconstruction metrics as input.\nAfterwards, the trained SVM classifies all previously unlabelled videos. In line with the uncertainty principle [59], we manually annotate a subset of previously unlabelled samples that are the closest to the SVM decision boundary. We further correct significant classification errors by inspecting the highest/lowest scoring samples. In this manner, we alternate between SVM training and manual annotation until 1.5k labels are collected (8 % of the whole dataset).\nIn order to validate the SVM's performance, we conduct a 5-fold cross-validation on the set of annotated videos. The cross-validation indicates that the SVM has 90% and 78% accuracy for classifying the camera tracking and point cloud quality respectively.", "publication_ref": ["b11", "b59"], "figure_ref": [], "table_ref": []}, {"heading": "B.1. NerFormer", "text": "Source image features \u03a8 The dense pixel-wise descriptor \u03a8 CNN (I src ) (sec. 4.2) of a source image I src is a stacking of 3 types of feature tensors along the channel dimension after differentiably upsampling to a common spatial resolution H \u00d7 W . The feature types are: 1) Intermediate activations extracted from the source image I src after \"layer1\", \"layer2\", and \"layer3\" layers of the ResNet34 [23] network, 2) the source segmentation mask M src , 3) the raw source image I src . Note that we separately map the output of each of the ResNet34 layers to a 32-dimensional feature with a 1x1 convolution followed by 2 normalization of the feature column at every spatial location. NerFormer architecture In fig. I we provide a more detailed visualisation of the NerFormer architecture. Rendering details Similar to NeRF [43], NerFormer optimizes loss functions for 800 randomly sampled image rays r u in each training target image. Following [43], Ner-Former implements a coarse and fine rendering network f TR . The former, given a ray r u , samples 32 points x i \u2208 r u at uniform depth intervals between predefined lower and upper depth bounds. The fine rendering network then samples 16 points on r u with importance sampling from the distribution proportional to the coarse rendering weights w i . Training details For a randomly sampled pixel u we thus render the color\u00ce tgt u and an alpha valueM tgt\nwhere the latter denotes the total amount of light absorbed by the implicit surface (M tgt u = 1 for complete absorption).\nAs noted in sec. 4.3, the optimized loss is a sum of the RGB squared error u \u00ce tgt u \u2212 I tgt u 2 and the segmenta-\n. The latter ensures that rays that do not intersect the object of interest do not terminate in the scene and vice versa. Following [43], we evaluate the losses for the fine and coarse renders and optimize their sum.", "publication_ref": ["b22", "b43", "b43", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "B.2. NeRF", "text": "We use the implementation of NeRF [43] from Py-Torch3D [50] which closely follows the original paper. Similar to NerFormer , we also add to the original losses of NeRF the BCE loss between the rendered alpha mask and the ground truth target mask. The coloring function c MLP and the opacity function f MLP have their architecture identical to the original implementation.", "publication_ref": ["b43", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "B.3. SDF methods -DVR, IDR", "text": "Here we detail the two baseline methods that represent shapes with signed distance fields (SDF). We start with introducing the SDF and a method for their rendering.\nSigned distance fields While opacity functions f o represent shapes with a measure of opaqueness of 3D spatial elements, signed distance fields f d (x, z) \u2208 R, express the signed euclidean distance to the nearest point x \u2208 S f on the implicit surface S f .\nSphere tracing (ST) While EA is the most popular method for rendering opacity fields f o , ST is its analogue for signed distance fields f d . Specifically, ST renders a pixel u by seeking the minimum of the signed distance function f d on the domain of 3D points belonging to the ray r u . ST, during its t-th iteration, refines the current estimate x ru t of the ray-surface intersection by moving \u2206 t = f d (x ru t , z) units in the direction of the projection ray: x ru t+1 = x t + \u2206 t r u . Upon convergence at time T , the rendered color\u00ce tgt u = c(x ru T , r u , z) comprises the response of the coloring function at the estimated ray-surface intersection.\nDVR natively supports our supervisory scenario and hence no alternations were required for the training protocol of both DVR+AD and DVR. In order to implement DVR+\u03b3, we", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Objectron: A large scale dataset of object-centric videos in the wild with pose annotations", "journal": "", "year": "2021", "authors": "Adel Ahmadyan; Liangkai Zhang; Jianing Wei; Artsiom Ablavatski; Matthias Grundmann"}, {"ref_id": "b1", "title": "Sal: Sign agnostic learning of shapes from raw data", "journal": "", "year": "", "authors": "Matan Atzmon; Yaron Lipman"}, {"ref_id": "b2", "title": "Layer normalization. arXiv", "journal": "", "year": "2016", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hinton"}, {"ref_id": "b3", "title": "Optimizing the latent space of generative networks", "journal": "", "year": "2017", "authors": "Piotr Bojanowski; Armand Joulin; David Lopez-Paz; Arthur Szlam"}, {"ref_id": "b4", "title": "Unstructured lumigraph rendering", "journal": "", "year": "2001", "authors": "Chris Buehler; Michael Bosse; Leonard Mcmillan; Steven Gortler; Michael Cohen"}, {"ref_id": "b5", "title": "An information-rich 3d model repository", "journal": "", "year": "2015", "authors": "X Angel; Thomas Chang; Leonidas Funkhouser; Pat Guibas; Qixing Hanrahan; Zimo Huang; Silvio Li; Manolis Savarese; Shuran Savva; Hao Song;  Su"}, {"ref_id": "b6", "title": "Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer", "journal": "", "year": "2019", "authors": "Wenzheng Chen; Jun Gao; Huan Ling; Edward J Smith; Jaakko Lehtinen; Alec Jacobson; Sanja Fidler"}, {"ref_id": "b7", "title": "Learning to predict 3d objects with an interpolation-based differentiable renderer", "journal": "", "year": "2002", "authors": "Wenzheng Chen; Huan Ling; Jun Gao; Edward Smith; Jaakko Lehtinen; Alec Jacobson; Sanja Fidler"}, {"ref_id": "b8", "title": "A large dataset of object scans", "journal": "", "year": "2016", "authors": "Sungjoon Choi; Qian-Yi Zhou; Stephen Miller; Vladlen Koltun"}, {"ref_id": "b9", "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction", "journal": "", "year": "2016", "authors": "B Christopher; Danfei Choy; Junyoung Xu; Kevin Gwak; Silvio Chen;  Savarese"}, {"ref_id": "b10", "title": "Improving generalization with active learning", "journal": "Machine learning", "year": "1994", "authors": "David Cohn; Les Atlas; Richard Ladner"}, {"ref_id": "b11", "title": "Support-vector networks", "journal": "", "year": "", "authors": "Corinna Cortes; Vladimir Vapnik"}, {"ref_id": "b12", "title": "Generating images with perceptual similarity metrics based on deep networks", "journal": "", "year": "2016", "authors": "Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b13", "title": "A point set generation network for 3d object reconstruction from a single image", "journal": "", "year": "2017", "authors": "Haoqiang Fan; Hao Su; Leonidas J Guibas"}, {"ref_id": "b14", "title": "3d shape induction from 2d views of multiple objects", "journal": "", "year": "2017", "authors": "Matheus Gadelha; Subhransu Maji; Rui Wang"}, {"ref_id": "b15", "title": "Local deep implicit functions for 3d shape", "journal": "", "year": "", "authors": "Kyle Genova; Forrester Cole; Avneesh Sud; Aaron Sarna; Thomas Funkhouser"}, {"ref_id": "b16", "title": "Learning shape templates with structured implicit functions", "journal": "", "year": "2019", "authors": "Kyle Genova; Forrester Cole; Daniel Vlasic; Aaron Sarna; T William; Thomas Freeman;  Funkhouser"}, {"ref_id": "b17", "title": "Learning a predictable and generative vector representation for objects", "journal": "", "year": "2016", "authors": "Rohit Girdhar; F David; Mikel Fouhey; Abhinav Rodriguez;  Gupta"}, {"ref_id": "b18", "title": "Mesh R-CNN", "journal": "", "year": "2019", "authors": "Georgia Gkioxari; Justin Johnson; Jitendra Malik"}, {"ref_id": "b19", "title": "Shape and viewpoint without keypoints", "journal": "", "year": "", "authors": "Shubham Goel; Angjoo Kanazawa; Jitendra Malik"}, {"ref_id": "b20", "title": "Google scanned objects", "journal": "", "year": "2002-09", "authors": " Googleresearch"}, {"ref_id": "b21", "title": "", "journal": "", "year": "2017", "authors": "David Ha; Andrew M Dai; Quoc V Le;  Hypernetworks"}, {"ref_id": "b22", "title": "Deep residual learning for image recognition", "journal": "", "year": "2015", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b23", "title": "Deep blending for free-viewpoint image-based rendering", "journal": "", "year": "2002", "authors": "Peter Hedman; Julien Philip; True Price; Jan-Michael Frahm; George Drettakis; Gabriel Brostow"}, {"ref_id": "b24", "title": "Escaping plato's cave using adversarial training: 3d shape from unstructured 2d image collections", "journal": "", "year": "2004", "authors": "Philipp Henzler; Niloy Mitra; Tobias Ritschel"}, {"ref_id": "b25", "title": "Unsupervised learning of 3d object categories from videos in the wild", "journal": "", "year": "2008", "authors": "Philipp Henzler; Jeremy Reizenstein; Patrick Labatut; Roman Shapovalov; Tobias Ritschel; Andrea Vedaldi; David Novotny"}, {"ref_id": "b26", "title": "Long short-term memory", "journal": "Neural Computation", "year": "", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b27", "title": "Unsupervised learning of shape and pose with differentiable point clouds", "journal": "", "year": "2018", "authors": "Eldar Insafutdinov; Alexey Dosovitskiy"}, {"ref_id": "b28", "title": "Panoptic studio: A massively multiview system for social interaction capture", "journal": "PAMI", "year": "2019", "authors": "Hanbyul Joo; Tomas Simon; Xulong Li; Hao Liu; Lei Tan; Lin Gui; Sean Banerjee; Timothy Godisart; Bart C Nabbe; Iain A Matthews; Takeo Kanade; Shohei Nobuhara; Yaser Sheikh"}, {"ref_id": "b29", "title": "Learning category-specific mesh reconstruction from image collections", "journal": "", "year": "2018", "authors": "Angjoo Kanazawa; Shubham Tulsiani; Alexei A Efros; Jitendra Malik"}, {"ref_id": "b30", "title": "Learning a multi-view stereo machine", "journal": "", "year": "2017", "authors": "Abhishek Kar; Christian H\u00e4ne; Jitendra Malik"}, {"ref_id": "b31", "title": "Neural 3d mesh renderer", "journal": "", "year": "2018", "authors": "Hiroharu Kato; Yoshitaka Ushiku; Tatsuya Harada"}, {"ref_id": "b32", "title": "Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering", "journal": "", "year": "2020", "authors": "Alexander Kirillov; Yuxin Wu"}, {"ref_id": "b33", "title": "Articulation-aware canonical surface mapping", "journal": "", "year": "", "authors": "Nilesh Kulkarni; Abhinav Gupta; David F Fouhey; Shubham Tulsiani"}, {"ref_id": "b34", "title": "Canonical surface mapping via geometric cycle consistency", "journal": "", "year": "2019", "authors": "Nilesh Kulkarni; Abhinav Gupta; Shubham Tulsiani"}, {"ref_id": "b35", "title": "Selfsupervised single-view 3d reconstruction via semantic consistency", "journal": "", "year": "", "authors": "Xueting Li; Sifei Liu; Kihwan Kim; Varun Shalini De Mello; Ming-Hsuan Jampani; Jan Yang;  Kautz"}, {"ref_id": "b36", "title": "Parsing IKEA Objects: Fine Pose Estimation", "journal": "", "year": "2013", "authors": "Joseph J Lim; Hamed Pirsiavash; Antonio Torralba"}, {"ref_id": "b37", "title": "Microsoft COCO: common objects in context", "journal": "", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge J Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b38", "title": "Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Proc. NIPS, 2020", "journal": "", "year": "", "authors": "Lingjie Liu; Jiatao Gu; Kyaw Zaw Lin"}, {"ref_id": "b39", "title": "Soft rasterizer: A differentiable renderer for image-based 3D reasoning. arXiv.cs, Proc. CVPR", "journal": "", "year": "2004", "authors": "Shichen Liu; Tianye Li; Weikai Chen; Hao Li"}, {"ref_id": "b40", "title": "Learning dynamic renderable volumes from images", "journal": "", "year": "", "authors": "Stephen Lombardi; Tomas Simon; Jason Saragih; Gabriel Schwartz"}, {"ref_id": "b41", "title": "", "journal": "ACM Trans. Graph", "year": "2011", "authors": ""}, {"ref_id": "b42", "title": "Occupancy networks: Learning 3d reconstruction in function space", "journal": "", "year": "2019", "authors": "Lars Mescheder; Michael Oechsle; Michael Niemeyer; Sebastian Nowozin; Andreas Geiger"}, {"ref_id": "b43", "title": "Nerf: Representing scenes as neural radiance fields for view synthesis", "journal": "", "year": "2006", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng"}, {"ref_id": "b44", "title": "HoloGAN: Unsupervised learning of 3D representations from natural images", "journal": "", "year": "1326", "authors": "Thu Nguyen-Phuoc; Chuan Li; Lucas Theis; Christian Richardt; Yong-Liang Yang"}, {"ref_id": "b45", "title": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision", "journal": "", "year": "2020", "authors": "Michael Niemeyer; Lars Mescheder; Michael Oechsle; Andreas Geiger"}, {"ref_id": "b46", "title": "Occupancy flow: 4d reconstruction by learning particle dynamics", "journal": "", "year": "2019", "authors": "Michael Niemeyer; Lars M Mescheder; Michael Oechsle; Andreas Geiger"}, {"ref_id": "b47", "title": "Learning 3d object categories by looking around them", "journal": "", "year": "2017", "authors": "David Novotny; Diane Larlus; Andrea Vedaldi"}, {"ref_id": "b48", "title": "Capturing the geometry of object categories from video supervision", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2018", "authors": "David Novotn\u00fd; Diane Larlus; Andrea Vedaldi"}, {"ref_id": "b49", "title": "Deepsdf: Learning continuous signed distance functions for shape representation", "journal": "", "year": "2019", "authors": "Jeong Joon Park; Peter Florence; Julian Straub; Richard Newcombe; Steven Lovegrove"}, {"ref_id": "b50", "title": "Accelerating 3d deep learning with pytorch3d. arXiv", "journal": "", "year": "2011", "authors": "Nikhila Ravi; Jeremy Reizenstein; David Novotny; Taylor Gordon; Wan-Yen Lo; Justin Johnson; Georgia Gkioxari"}, {"ref_id": "b51", "title": "Unsupervised learning of 3d structure from images", "journal": "", "year": "2016", "authors": "Danilo Jimenez Rezende; Ali Eslami; Shakir Mohamed; Peter Battaglia; Max Jaderberg; Nicolas Heess"}, {"ref_id": "b52", "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization", "journal": "", "year": "2019", "authors": "Shunsuke Saito; , ; Zeng Huang; Ryota Natsume; Shigeo Morishima; Angjoo Kanazawa; Hao Li"}, {"ref_id": "b53", "title": "Structure-from-motion revisited", "journal": "", "year": "2016", "authors": "Johannes Lutz Sch\u00f6nberger; Jan-Michael Frahm"}, {"ref_id": "b54", "title": "Pixelwise view selection for unstructured multi-view stereo", "journal": "", "year": "2016", "authors": "Johannes Lutz Sch\u00f6nberger; Enliang Zheng; Marc Pollefeys; Jan-Michael Frahm"}, {"ref_id": "b55", "title": "Graf: Generative radiance fields for 3d-aware image synthesis", "journal": "", "year": "", "authors": "Katja Schwarz; Yiyi Liao; Michael Niemeyer; Andreas Geiger"}, {"ref_id": "b56", "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations. CoRR, abs", "journal": "", "year": "1618", "authors": "Vincent Sitzmann; Michael Zollh\u00f6fer; Gordon Wetzstein"}, {"ref_id": "b57", "title": "Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In Deep learning in medical image analysis and multimodal learning for clinical decision support", "journal": "", "year": "2017", "authors": "H Carole; Wenqi Sudre; Tom Li; Sebastien Vercauteren; M Jorge Ourselin;  Cardoso"}, {"ref_id": "b58", "title": "Multi-view 3d models from single images with a convolutional network", "journal": "", "year": "2016", "authors": "Maxim Tatarchenko; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b59", "title": "Support vector machine active learning for image retrieval", "journal": "", "year": "2001", "authors": "Simon Tong; Edward Y Chang"}, {"ref_id": "b60", "title": "Grf: Learning a general radiance field for 3d scene representation and rendering", "journal": "arXiv", "year": "", "authors": "Alex Trevithick; Bo Yang"}, {"ref_id": "b61", "title": "Multi-view consistency as supervisory signal for learning shape and pose prediction", "journal": "", "year": "2018", "authors": "Shubham Tulsiani; Alexei A Efros; Jitendra Malik"}, {"ref_id": "b62", "title": "Learning category-specific deformable 3D models for object reconstruction", "journal": "PAMI", "year": "2017", "authors": "Shubham Tulsiani; Abhishek Kar; Joao Carreira; Jitendra Malik"}, {"ref_id": "b63", "title": "Multi-view supervision for single-view reconstruction via differentiable ray consistency", "journal": "", "year": "2017", "authors": "Shubham Tulsiani; Tinghui Zhou; Alexei A Efros; Jitendra Malik"}, {"ref_id": "b64", "title": "Multi-view supervision for single-view reconstruction via differentiable ray consistency", "journal": "", "year": "2017", "authors": "Shubham Tulsiani; Tinghui Zhou; Alexei A Efros; Jitendra Malik"}, {"ref_id": "b65", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b66", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b67", "title": "Pixel2mesh: Generating 3d mesh models from single rgb images", "journal": "", "year": "2006", "authors": "Nanyang Wang; Yinda Zhang; Zhuwen Li; Yanwei Fu; Wei Liu; Yu-Gang Jiang"}, {"ref_id": "b68", "title": "Ibrnet: Learning multi-view image-based rendering", "journal": "", "year": "", "authors": "Qianqian Wang; Zhicheng Wang; Kyle Genova; Pratul Srinivasan; Howard Zhou; Jonathan T Barron; Ricardo Martin-Brualla; Noah Snavely; Thomas Funkhouser"}, {"ref_id": "b69", "title": "Synsin: End-to-end view synthesis from a single image", "journal": "", "year": "2020", "authors": "Olivia Wiles; Georgia Gkioxari; Richard Szeliski; Justin Johnson"}, {"ref_id": "b70", "title": "Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling", "journal": "", "year": "2016", "authors": "Jiajun Wu; Chengkai Zhang; Tianfan Xue; Bill Freeman; Josh Tenenbaum"}, {"ref_id": "b71", "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild", "journal": "", "year": "", "authors": "Shangzhe Wu; Christian Rupprecht; Andrea Vedaldi"}, {"ref_id": "b72", "title": "Beyond PASCAL: A benchmark for 3D object detection in the wild", "journal": "", "year": "2014", "authors": "Yu Xiang; Roozbeh Mottaghi; Silvio Savarese"}, {"ref_id": "b73", "title": "Pointflow: 3d point cloud generation with continuous normalizing flows", "journal": "", "year": "2019", "authors": "Guandao Yang; Xun Huang; Zekun Hao; Ming-Yu Liu; Serge Belongie; Bharath Hariharan"}, {"ref_id": "b74", "title": "Multiview neural surface reconstruction by disentangling geometry and appearance", "journal": "", "year": "2007", "authors": "Lior Yariv; Yoni Kasten; Dror Moran; Meirav Galun; Matan Atzmon; Basri Ronen; Yaron Lipman"}, {"ref_id": "b75", "title": "Neural radiance fields from one or few images. Proc. ECCV", "journal": "", "year": "2004", "authors": "Alex Yu; Vickie Ye; Matthew Tancik; Angjoo Kanazawa"}, {"ref_id": "b76", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "Richard Zhang; Phillip Isola; Alexei A Efros; Eli Shechtman; Oliver Wang"}, {"ref_id": "b77", "title": "Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering", "journal": "", "year": "", "authors": "Yuxuan Zhang; Wenzheng Chen; Huan Ling; Jun Gao; Yinan Zhang; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "4", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 4 :4Figure 4: We propose NerFormer which jointly learns to pool features from source views and to raymarch by means of a series of transformers alternating between attention along the ray and pooling dimensions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "FigureFigure5: NerFormer learns to attend to features from source images. A ray is emitted from a target-image pixel (1st column), and its points are projected to the source views (columns 2-8) from which features are sampled. For each source feature, NerFormer predicts attention (red=high, blue=low) for aggregating to a single source embedding per point. Note how the attention model implicitly learns to pool features from nearby source views.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "PSNR @ # source views (c) PSNR @ target view difficulty", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure I: A detailed illustration of the architecture of NerFormer .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "FigFig. II further analyzes training convergence on the single-scene new-view synthesis task. For each method and", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "Camera tracking metrics Dense cloudManual labelaccurate /inaccurateTrainingCamera / cloudmetricsbinary classifiersRBF-SVMCamera tracking metrics Dense cloud metricsPredicted labelaccurate /inaccurateAutomatic labellingFigure 3: 3D ground truth for Common Objects in 3D was generated with active learning. Videos are first annotated withcameras and dense point clouds with COLMAP [53]. Given several reconstruction metrics and manual binary annotations(\"accurate\"/\"inaccurate\") of a representative subset of reconstructions, we train an SVM that automatically labels all videos.cal contribution: the NerFormer neural rendering model.Problem setting We tackle the task of generating a repre-sentation of appearance and geometry of an object given a small number of its observed source RGB views {I src i } Nsrc i=1Camera tracking metrics:Manual labelsaccurate/inaccurateBA energy, #points,#cameras, ...Camera metrics: BA energy, -sparse pt. cloud size, --# camerasDense cloud metrics RGB render err., #points, density, ..."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Pooling transf. enc. \"# ! Ray transf. enc. \"# % Pooling. enc. \"# ! Ray transf. enc. \"# %", "figure_data": "Sampled features $ #!NeRFormer % &'Source view ! ! \"#$"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "NeRF+WCE[26] 21.0 0.19 0.74 0.91 SRN+WCE+\u03b3 16.9 0.30 0.60 0.75", "figure_data": "methodPSNR LPIPS depth 1IoU methodPSNR LPIPS depth 1IoUNerFormer23.3 0.17 0.40 0.96 SRN[56]20.4 0.21 0.60 0.93NeRF[43]23.6 0.17 0.38 0.95 SRN+WCE15.8 0.26 0.64 0.80NV[41]22.2 0.20 0.91 0.91 SRN+\u03b316.9 0.30 0.59 0.75NV+WCE18.7 0.25 0.85 0.90 DVR[45]15.0 0.33 0.89 0.68IDR[74]18.5 0.15 0.81 0.92 DVR+\u03b315.8 0.38 0.74 0.65"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Single-scene new-view synthesis results on Common Objects in 3D comparing the baseline approaches[43,41,74,56,45,67], IPC, their variants with Warpconditioned Embeddings (+WCE) or Positional Embedding (+\u03b3), and our NerFormer (the best / 2nd best result).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ". hard easy med. hard NerFormer 17.9 0.26 0.87 0.82 17.6 0.27 0.91 0.81 19.3 19.0 18.3 17.4 15.6 18.9 18.6 18.1 17.1 15.1 18.9 15.5 14.6 18.6 14.9 14.7 SRN+WCE+\u03b3 17.6 0.24 0.28 0.89 14.4 0.27 0.40 0.81 18.0 18.0 17.8 17.6 16.8 14.6 14.5 14.6 14.5 13.9 18.0 16.8 16.0 14.7 13.6 15.1 SRN+WCE 16.6 0.26 0.31 0.87 14.6 0.27 0.36 0.82 17.0 17.0 16.7 16.4 15.8 14.9 14.8 14.8 14.6 13.9 16.9 15.7 14.5 14.9 13.7 14.8 NeRF+WCE[26] 14.3 0.27 2.14 0.72 13.8 0.27 2.23 0.70 14.3 15.0 14.9 14.7 14.2 12.6 14.5 14.4 14.2 13.8 12.1 13.6 13.6 14.4 13.0 13.0 .24 0.69 14.4 14.4 14.2 14.1 13.4 13.8 13.8 13.7 13.6 12.6 14.4 13.7 13.4 13.8 12.8 12.2 P3DMesh 17.2 0.23 0.50 0.91 12.4 0.26 2.49 0.69 17.6 17.5 17.4 17.1 16.2 12.6 12.5 12.5 12.5 12.1 17.5 16.8 16.0 12.6 11.8 13.2 .01 0.53 12.5 12.5 12.3 12.2 12.0 11.7 11.6 11.6 11.6 11.3 12.4 12.0 13.6 11.7 11.2 12.0", "figure_data": "train-unseentest-unseentrain-unseentest-unseentrain-unseen test-unseenMethod 1 easy medIPC+WCE PSNR LPIPS depth 1 IoU PSNR LPIPS depth 1 IoU 9 7 5 3 1 9 7 5 3 14.1 0.36 2.12 0.70 13.5 0.37 2NV+WCE 12.3 0.34 2.87 0.54 11.6 0.35 3SRN+\u03b3+AD 21.7 0.21 0.31 0.89 ----21.7 21.7 21.7 21.7 21.8 -----21.7 21.3 19.7---SRN[56]+AD21.20.21 0.23 0.94----21.2 21.2 21.2 21.2 21.3-----21.2 20.8 19.2---NV[41]+AD19.70.23 0.41 0.93----19.7 19.7 19.6 19.6 19.7-----19.7 19.3 17.7---NeRF+AD17.10.25 0.55 0.92----17.1 17.0 17.0 17.0 17.1-----17.1 16.9 15.8---P3DMesh[50]+AD 16.10.24 0.74 0.89----16.1 16.1 16.1 16.1 16.2-----16.1 16.0 15.1---IPC+AD14.00.37 2.20 0.69----14.4 14.3 14.1 14.0 13.3-----14.3 13.6 13.2---IDR+AD13.70.25 1.74 0.88----13.7 13.7 13.7 13.7 13.8-----13.7 13.7 14.0---DVR+\u03b3+AD7.60.29 3.24 0.09----7.6 7.6 7.5 7.5 7.7-----7.67.79.0---DVR[45]+AD7.60.29 3.01 0.06----7.6 7.6 7.5 7.5 7.7-----7.67.79.0---"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "I", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results on all 50 classes from CO3D comparing the 4 best-performing methods from tab. 3.", "figure_data": "methodtime [sec] methodtime [sec]NerFormer178.41SRN[56]1.00NeRF+WCE[26]113.82 SRN+\u03b31.19NeRF[43]23.82SRN+WCE+\u03b34.20NV[41]0.37SRN+WCE5.34NV+WCE0.41DVR[45]196.94IDR[74]69.11DVR+\u03b3204.39IPC0.15P3DMesh[50]0.09"}, {"figure_label": "II", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Average rendering time of an 800x800 pixel image comparing all methods from tab. 2.", "figure_data": "PSNR20 22 24 26 28 30 32 34NeRF NeRF+WCE NeRFormer SRN+WCE SRN SRN+WCE+ SRN+ NV NV+WCE DVR DVR+ Pixel2Mesh IDR IP IP+WCE050 100 150 200 250 300 EpochFigure II: Convergence speed on single-scene new-view-synthesis showing the mean and std. dev. over the per-scene training PSNRs for each method.objective plateaus."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "The final value of the Bundle Adjustment (BA) cost function. BAtermination {0, 1} The termination state of BA (converged/not converged). \u00b5det score [0, 1] An average over per-frame detection scores of the PointRend object detector. \u00b5perc detected [0, 100] Percentage of frames in which the category of interest is detected with PointRend. Ncameras N The number of cameras registered during BA. Nsparse pts N Number of points in the sparse point cloud. P CL render depth", "figure_data": "MetricDomain DescriptionBAfinal costR"}, {"figure_label": "IV", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "The list of SfM and point-cloud reconstruction metrics that serve as a set of features for training the active-SVM that labels camera and reconstruction quality.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "V = (I i | I i \u2208 R 3\u00d7H\u00d7W ) N I", "formula_coordinates": [3.0, 50.11, 505.35, 128.17, 11.69]}, {"formula_id": "formula_1", "formula_text": "i | P i \u2208 R 4\u00d74 ) N I", "formula_coordinates": [3.0, 50.11, 542.04, 236.25, 22.82]}, {"formula_id": "formula_2", "formula_text": "M = (M i | M i \u2208 [0, 1] H\u00d7W ) N I", "formula_coordinates": [3.0, 77.91, 615.53, 137.72, 11.13]}, {"formula_id": "formula_3", "formula_text": "(D i | D i \u2208 R H\u00d7W + ) N I i=1 .", "formula_coordinates": [3.0, 333.3, 227.39, 104.13, 13.33]}, {"formula_id": "formula_4", "formula_text": "f MLP : f MLP = f HEAD \u2022 f MLP and c MLP = c HEAD \u2022 f MLP .", "formula_coordinates": [4.0, 308.86, 393.54, 220.02, 10.95]}, {"formula_id": "formula_5", "formula_text": "I tgt u (r u , z) = NS i=1 w i (x ru i , z)c(x ru i , r u , z) is a", "formula_coordinates": [4.0, 319.38, 688.26, 225.73, 27.08]}, {"formula_id": "formula_6", "formula_text": "w i = i\u22121 j=0 T j (1 \u2212 T i ) with T i = exp(\u2212\u2206f o (x ru i , z)).", "formula_coordinates": [5.0, 50.11, 487.61, 234.9, 14.11]}, {"formula_id": "formula_7", "formula_text": "Z ru = z WCE (x ru j , I src i , P src i ) nsrc i=1 NS j=1", "formula_coordinates": [6.0, 84.96, 393.64, 163.29, 22.08]}, {"formula_id": "formula_8", "formula_text": "MHA d l (Z l ) = Z l = LN(MHA l (Z l , dim=d) + Z l ) (2) TE d l (Z l ) = LN(MLP l (Z l ) + Z l )(3)", "formula_coordinates": [6.0, 62.39, 505.1, 223.97, 29.38]}, {"formula_id": "formula_9", "formula_text": "TE l (Z l ) = TE 0 l (TE 1 l (Z l )) = Z l+1 ,(4)", "formula_coordinates": [6.0, 75.05, 537.97, 211.32, 12.77]}, {"formula_id": "formula_10", "formula_text": "f HEAD TR (Z L ) = nsrc i=1 \u03c9 i (Z L )Z L [:, i, :] \u2208 R NS\u00d7Dz ,", "formula_coordinates": [6.0, 50.11, 677.32, 208.32, 14.11]}, {"formula_id": "formula_11", "formula_text": "Sampled features ! ! ! \" \" \u00d7 \" #!$ \u00d7 $ % Source view % & #!$ S o u rc e v ie w % ' # ! $ Sou rce vie w % ( #!$ Source view feature s \u03a8 & #!$ Linear layer D ) \u00d7 80 Multi-head att. along dim d 2-layer MLP Linear ! !\" \u00d7 ! !\" \u2192 ReLU \u2192 Linear ! !\" \u00d7 ! #$%", "formula_coordinates": [9.0, 50.54, 195.49, 283.81, 132.6]}, {"formula_id": "formula_12", "formula_text": "r i k , r j k ) = \u03b4[\u03c0 Pi (x k ) \u2208 \u2126 i \u2227 \u03c0 Pj (x k ) \u2208 \u2126 j ](1 + r i k \u2022 r j k )", "formula_coordinates": [10.0, 308.86, 329.81, 236.25, 27.77]}, {"formula_id": "formula_13", "formula_text": "d cam (P i , P j ) = 1 \u2212 k s(r i k , r j k ) k s(r i k , r i k ) + s(r j k , r j k ) \u2212 s(r i k , r j k )", "formula_coordinates": [10.0, 309.45, 442.3, 231.11, 29.1]}, {"formula_id": "formula_14", "formula_text": "f LSTM (x ru t , r u , z WCE (x ru t , {I src i }, {P src i }), h t )", "formula_coordinates": [12.0, 50.11, 533.07, 182.82, 13.45]}, {"formula_id": "formula_15", "formula_text": "f P (x , z) = \u03b4[ NN P(z) (x ) \u2212 x < ],", "formula_coordinates": [13.0, 86.4, 304.42, 163.68, 9.99]}, {"formula_id": "formula_16", "formula_text": "\u03a0 u (P(z)) = x i |x i \u2208 P(z); \u03c0 P tgt (x i ) \u2212 u \u2264 f P tgt ; d P tgt (x i ) \u2264 d P tgt (x i+1 ) ,", "formula_coordinates": [13.0, 59.9, 616.75, 216.68, 27.78]}, {"formula_id": "formula_17", "formula_text": "I tgt u (r u , z) = xi\u2208\u03a0 u (P(z)) w i (x i , z, u)c IPC (x i , r u , z). For IPC, the weight w i (x i , z, u) = i\u22121 j=0 T IPC j (x i , z, u) 1 \u2212 T IPC i (x i , z, u)", "formula_coordinates": [13.0, 308.86, 227.56, 236.25, 60.56]}, {"formula_id": "formula_18", "formula_text": "T IPC i (x i , z, u) = f P (x i , z) =1 u \u2212 \u03c0 P tgt (x i ) f P tgt ,", "formula_coordinates": [13.0, 336.32, 323.12, 181.33, 30.75]}, {"formula_id": "formula_19", "formula_text": "IoU [0, 1]", "formula_coordinates": [14.0, 77.34, 203.61, 72.85, 8.86]}], "doi": ""}