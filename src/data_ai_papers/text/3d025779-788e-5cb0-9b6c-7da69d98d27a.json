{"title": "DynIBaR: Neural Dynamic Image-Based Rendering", "authors": "Zhengqi Li; Qianqian Wang; Forrester Cole; Richard Tucker; Noah Snavely; Google Research; Cornell Tech", "pub_date": "2023-04-24", "abstract": "Figure 1. Recent methods for synthesizing novel views from monocular videos of dynamic scenes-like HyperNeRF [50] and NSFF [35]struggle to render high-quality views from long videos featuring complex camera and scene motion. We present a new approach that addresses these limitations, illustrated above via an application to 6DoF video stabilization, where we apply our approach and prior methods on a 30-second, shaky video clip, and compare novel views rendered along a smoothed camera path (left). On a dynamic scenes dataset (right) [75], our approach significantly improves rendering fidelity, as indicated by synthesized images and LPIPS errors computed on pixels corresponding to moving objects (yellow numbers). Please see the supplementary video for full results.", "sections": [{"heading": "Introduction", "text": "Computer vision methods can now produce freeviewpoint renderings of static 3D scenes with spectacular quality. What about moving scenes, like those featuring people or pets? Novel view synthesis from a monocular video of a dynamic scene is a much more challenging dynamic scene reconstruction problem. Recent work has made progress towards synthesizing novel views in both space and time, thanks to new time-varying neural volumetric representations like HyperNeRF [50] and Neural Scene Flow Fields (NSFF) [35], which encode spatiotemporally varying scene content volumetrically within a coordinate-based multi-layer perceptron (MLP).\nHowever, these dynamic NeRF methods have limitations that prevent their application to casual, in-the-wild videos. Local scene flow-based methods like NSFF struggle to scale to longer input videos captured with unconstrained camera motions: the NSFF paper only claims good performance for 1-second, forward-facing videos [35]. Methods like HyperNeRF that construct a canonical model are mostly constrained to object-centric scenes with controlled camera paths, and can fail on scenes with complex object motion.\nIn this work, we present a new approach that is scalable to dynamic videos captured with 1) long time duration, 2) unbounded scenes, 3) uncontrolled camera trajectories, and 4) fast and complex object motion. Our approach retains the advantages of volumetric scene representations that can model intricate scene geometry with view-dependent effects, while significantly improving rendering fidelity for both static and dynamic scene content compared to recent methods [35,50], as illustrated in Fig. 1.\nWe take inspiration from recent methods for rendering static scenes that synthesize novel images by aggregating local image features from nearby views along epipolar lines [39,64,70]. However, scenes that are in motion violate the epipolar constraints assumed by those methods. We instead propose to aggregate multi-view image features in scene motion-adjusted ray space, which allows us to correctly reason about spatio-temporally varying geometry and appearance.\nWe also encountered many efficiency and robustness challenges in scaling up aggregation-based methods to dynamic scenes. To efficiently model scene motion across multiple views, we model this motion using motion trajectory fields that span multiple frames, represented with learned basis functions. Furthermore, to achieve temporal coherence in our dynamic scene reconstruction, we introduce a new temporal photometric loss that operates in motion-adjusted ray space. Finally, to improve the quality of novel views, we propose to factor the scene into static and dynamic components through a new IBR-based motion segmentation technique within a Bayesian learning framework.\nOn two dynamic scene benchmarks, we show that our approach can render highly detailed scene content and significantly improves upon the state-of-the-art, leading to an average reduction in LPIPS errors by over 50% both across entire scenes, as well as on regions corresponding to dynamic objects. We also show that our method can be applied to inthe-wild videos with long duration, complex scene motion, and uncontrolled camera trajectories, where prior state-ofthe-art methods fail to produce high quality renderings. We hope that our work advances the applicability of dynamic view synthesis methods to real-world videos.", "publication_ref": ["b49", "b34", "b34", "b34", "b49", "b38", "b63", "b69"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Novel view synthesis. Classic image-based rendering (IBR) methods synthesize novel views by integrating pixel information from input images [58], and can be categorized according to their dependence on explicit geometry. Light field or lumigraph rendering methods [9,21,26,32] generate new views by filtering and interpolating sampled rays, without use of explicit geometric models. To handle sparser input views, many approaches [7,14,18,23,24,26,30,52,54,55] leverage pre-computed proxy geometry such as depth maps or meshes to render novel views.\nRecently, neural representations have demonstrated highquality novel view synthesis [12, 17, 38, 40, 46, 48, 59-62, 72, 81]. In particular, Neural Radiance Fields (NeRF) [46] achieves an unprecedented level of fidelity by encoding continuous scene radiance fields within multi-layer perceptrons (MLPs). Among all methods building on NeRF, IBRNet [70] is the most relevant to our work. IBRNet combines classical IBR techniques with volume rendering to produce a generalized IBR module that can render high-quality views without per-scene optimization. Our work extends this kind of volumetric IBR framework designed for static scenes [11,64,70] to more challenging dynamic scenes. Note that our focus is on synthesizing higher-quality novel views for long videos with complex camera and object motion, rather than on generalization across scenes.\nDynamic scene view synthesis. Our work is related to geometric reconstruction of dynamic scenes from RGBD [5,15,25,47,68,83] or monocular videos [31,44,78,80]. However, depth-or mesh-based representations struggle to model complex geometry and view-dependent effects.\nMost prior work on novel view synthesis for dynamic scenes requires multiple synchronized input videos [1,3,6,27,33,63,69,76,82], limiting their real-world applicability. Some methods [8,13,22,51,71] use domain knowledge such as template models to achieve high-quality results, but are restricted to specific categories [41,56]. More recently, many works propose to synthesize novel views of dynamic scenes from a single camera. Yoon et al. [75] render novel views through explicit warping using depth maps obtained via single-view depth and multi-view stereo. However, this method fails to model complex scene geometry and to fill in realistic and consistent content at disocclusions. With advances in neural rendering, NeRF-based dynamic view synthesis methods have shown state-of-theart results [16,35,53,66,74]. Some approaches, such as Nerfies [49] and HyperNeRF [50], represent scenes using a deformation field mapping each local observation to a canonical scene representation. These deformations are conditioned on time [53] or a per-frame latent code [49,50,66], and are parameterized as translations [53,66] or rigid body motion fields [49,50]. These methods can handle long videos, but are mostly limited to object-centric scenes with relatively small object motion and controlled camera paths. Other methods represent scenes as time-varying NeRFs [19,20,35,67,74]. In particular, NSFF uses neural scene flow fields that can  Rendering via motion-adjusted multi-view feature aggregation. Given a sampled location x at time i along a target ray r, we estimate its motion trajectory, which determines the 3D correspondence of x at nearby time j \u2208 N (i), denoted xi\u2192j. Each warped point is then projected into its corresponding source view. Image features fj extracted along the projected curves are aggregated and fed to the ray transformer with time embedding \u03b3(i), producing per-sample color and density (ci, \u03c3i). The final pixel color\u0108i is then synthesized by volume rendering (ci, \u03c3i) along r. capture fast and complex 3D scene motion for in-the-wild videos [35]. However, this method only works well for short (1-2 second), forward-facing videos.", "publication_ref": ["b57", "b8", "b20", "b25", "b31", "b6", "b13", "b17", "b22", "b23", "b25", "b29", "b51", "b53", "b54", "b45", "b69", "b10", "b63", "b69", "b4", "b14", "b24", "b46", "b67", "b82", "b30", "b43", "b77", "b79", "b0", "b2", "b5", "b26", "b32", "b62", "b68", "b75", "b81", "b7", "b12", "b21", "b50", "b70", "b40", "b55", "b74", "b15", "b34", "b52", "b65", "b73", "b48", "b49", "b52", "b48", "b49", "b65", "b52", "b65", "b48", "b49", "b18", "b19", "b34", "b66", "b73", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Dynamic Image-Based Rendering", "text": "Given a monocular video of a dynamic scene with frames (I 1 , I 2 , . . . , I N ) and known camera parameters (P 1 , P 2 , . . . , P N ), our goal is to synthesize a novel viewpoint at any desired time within the video. Like many other approaches, we train per-video, first optimizing a model to reconstruct the input frames, then using this model to render novel views.\nRather than encoding 3D color and density directly in the weights of an MLP as in recent dynamic NeRF methods, we integrate classical IBR ideas into a volumetric rendering framework. Compared to explicit surfaces, volumetric representations can more readily model complex scene geometry with view-dependent effects.\nThe following sections introduce our methods for scenemotion-adjusted multi-view feature aggregation (Sec. 3.1), and enforcing temporal consistency via cross-time rendering in motion-adjusted ray space (Sec. 3.2). Our full system combines a static model and a dynamic model to produce a color at each pixel. Accurate scene factorization is achieved via segmentation masks derived from a separately trained motion segmentation module within a Bayesian learning framework (Sec. 3.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Motion-adjusted feature aggregation", "text": "We synthesize new views by aggregating features extracted from temporally nearby source views. To render an image at time i, we first identify source views I j within a temporal radius r frames of i, j \u2208 N (i) = [i \u2212 r, i + r]. For each source view, we extract a 2D feature map F i through a shared convolutional encoder network to form an input tuple {I j , P j , F j }.\nTo predict the color and density of each point sampled along a target ray r, we must aggregate source view features while accounting for scene motion. For a static scene, points along a target ray will lie along a corresponding epipolar line in a neighboring source view, hence we can aggregate potential correspondences by simply sampling along neighboring epipolar lines [64,70]. However, moving scene elements violate epipolar constraints, leading to inconsistent feature aggregation if motion is not accounted for. Hence, we perform motion-adjusted feature aggregation, as shown in Fig. 3. To determine correspondence in dynamic scenes, one straightforward idea is to estimate a scene flow field via an MLP [35] to determine a given point's motion-adjusted 3D location at a nearby time. However, this strategy is computational infeasible in a volumetric IBR framework due to recursive unrolling of the MLPs.\nMotion trajectory fields. Instead, we represent scene motion using motion trajectory fields described in terms of learned basis functions. For a given 3D point x along target ray r at time i, we encode its trajectory coefficients with an MLP G MT :\n{\u03c6 l i (x)} L l=1 = G MT (\u03b3(x), \u03b3(i)) ,(1)\nwhere \u03c6 l i \u2208 R 3 are basis coefficients (with separate coefficients for x, y, and z, using the motion basis described below) and \u03b3 denotes positional encoding. We choose L = 6 bases and 16 linearly increasing frequencies for the encoding \u03b3, based on the assumption that scene motion tends to be low frequency [80].\nWe also introduce a global learnable motion basis {h l i } L l=1 , h l i \u2208 R, spanning every time step i of the input video, which is optimized jointly with the MLP. The motion trajectory of x is then defined as \u0393 x,i (j) = L l=1 h l j \u03c6 l i (x), and thus, the relative displacement between x and its 3D correspondence x i\u2192j at time j is computed as\n\u2206 x,i (j) = \u0393 x,i (j) \u2212 \u0393 x,i (i).(2)\nWith this motion trajectory representation, finding 3D correspondences for a query point x in neighboring views requires just a single MLP query, allowing efficient multi-view feature aggregation within our volume rendering framework. We initialize the basis {h l i } L l=1 with the DCT basis as proposed by Wang et al. [67], but fine-tune it along with other components during optimization, since we observe that a fixed DCT basis can fail to model a wide range of real-world motions (see third column of Fig. 4).\nUsing the estimated motion trajectory of x at time i, we denote x's corresponding 3D point at time j as x i\u2192j = x + \u2206 x,i (j). We project each warped point x i\u2192j into its source view I j using camera parameters P j , and extract color and feature vector f j at the projected 2D pixel location. Figure 3. Temporal consistency via cross-time rendering. To enforce temporal consistency in a dynamic reconstruction, we render each frame Ii using the scene model from a nearby time j, which we call cross-time rendering. A ray r from image i is instead rendered using a curved ray ri\u2192j, i.e., r warped to time j. That is, having computed the motion-adjusted point xi\u2192j = x+\u2206x,i(j) at nearby time j from every sampled location along r, we query xi\u2192j and time j via the MLP to predict its motion trajectory \u0393x i\u2192j ,j , along which we aggregate image features f k extracted from source views within time k \u2208 N (j). The aggregated features along ri\u2192j are fed to the ray transformer with time embedding \u03b3(j) to produce per-sample color and density (cj, \u03c3j) at time j. A pixel color\u0108j\u2192i is computed by volume rendering (cj, \u03c3j), and then compared to the ground truth color Ci to form a reconstruction loss Lpho.\nThe resulting set of source features across neighbor views j is fed to a shared MLP whose output features are aggregated through weighted average pooling [70] to produce a single feature vector at each 3D sample point along ray r. A ray transformer network with time embedding \u03b3(i) then processes the sequence of aggregated features along the ray to predict per-sample colors and densities (c i , \u03c3 i ) (see Fig. 2). We then use standard NeRF volume rendering [4] to obtain a final pixel color\u0108 i (r) for the ray from this sequence of colors and densities.", "publication_ref": ["b63", "b69", "b34", "b79", "b66", "b69", "b3"], "figure_ref": ["fig_1", "fig_0"], "table_ref": []}, {"heading": "Cross-time rendering for temporal consistency", "text": "If we optimize our dynamic scene representation by com-paring\u0108 i with C i alone, the representation might overfit to the input images: it might perfectly reconstruct those views, but fail to render correct novel views. This can happen because the representation has the capacity to reconstruct completely separate models for each time instance, without utilizing or accurately reconstructing scene motion. Therefore, to recover a consistent scene with physically plausible motion, we enforce temporal coherence of the scene representation. One way to define temporal coherence in this context is that the scene at two neighboring times i and j should be consistent when taking scene motion into account [19,35,67].\nIn particular, we enforce temporal photometric consistency in our optimized representation via cross-time rendering in motion-adjusted ray space, as shown in Fig. 3. The idea is to render a view at time i but via some nearby time j, which we refer to as cross-time rendering. For each nearby time j \u2208 N (i), rather than directly using points x along ray r, we consider the points x i\u2192j along motion-adjusted ray r i\u2192j and treat them as if they lie along a ray at time j.\nSpecifically, having computed the motion-adjusted points x i\u2192j , we query the MLP to predict the coefficients of new trajectories {\u03c6 l j (x i\u2192j } L l=1 = G MT (x i\u2192j , \u03b3(j)), and use these to compute corresponding 3D points (x i\u2192j ) j\u2192k for images k in the temporal window N (j), using Eq. 2. These new 3D correspondences are then used to render a pixel color exactly as described for a \"straight\" ray r i in Sec. 3.1, except now along the curved, motion-adjusted ray r i\u2192j . That is, each point (x i\u2192j ) j\u2192k is projected into its source view I k and feature maps F k with camera parameters P k to extract an RGB color and image feature f k , and then these features are aggregated and input to the ray transformer with the time embedding \u03b3(j). The result is a sequence of colors and densities (c j , \u03c3 j ) along r i\u2192j at time j, which can be composited through volume rendering to form a color\u0108 j\u2192i .\nWe can then compare\u0108 j\u2192i (r) with the target pixel C i (r) via a motion-disocclusion-aware RGB reconstruction loss:\nL pho = r j\u2208N (i)\u0174 j\u2192i (r)\u03c1(C i (r),\u0108 j\u2192i (r)).(3)\nWe use a generalized Charbonnier loss [10] for the RGB loss \u03c1.\u0174 j\u2192i (r) is a motion disocclusion weight computed by the difference of accumulated alpha weights between time i and j to address the motion disocclusion ambiguity described by NSFF [35] (see supplement for more details). Note that when j = i, there is no scene motion-induced displacement, meaning\u0108 j\u2192i =\u0108 i and no disocclusion weights are involved (\u0174 j\u2192i = 1). We show a comparison between our method with and without enforcing temporal consistency in the first column of Fig. 4.", "publication_ref": ["b18", "b34", "b66", "b9", "b34"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Combining static and dynamic models", "text": "As observed in NSFF, synthesizing novel views using a small temporal window is insufficient to recover complete and high-quality content for static scene regions, since the contents may only be observed in spatially distant frames due to uncontrolled camera paths. Therefore, we follow the ideas of NSFF [35], and model the entire scene using two separate representations. Dynamic content (c i , \u03c3 i ) is represented with a time-varying model as above (used for cross-time rendering during optimization). Static content (c, \u03c3) is represented with a time-invariant model, which renders in the same way as the time-varying model, but aggregates multiview features without scene motion adjustment (i.e., along epipolar lines).\nThe dynamic and static predictions are combined and rendered to a single output color\u0108 full i (or\u0108 full j\u2192i during crosstime rendering) using the method for combining static and transient models of NeRF-W [45]. Each model's color and density estimates can also be rendered separately, giving color\u0108 st for static content and\u0108 dy i for dynamic content. When combining the two representations, we rewrite the photometric consistency term in Eq. 3 as a loss comparin\u011d C full j\u2192i (r) with target pixel C i (r):\nL pho = r j\u2208N (i)\u0174 j\u2192i (r)\u03c1(C i (r),\u0108 full j\u2192i (r))(4)\nImage-based motion segmentation. In our framework, we observed that without any initialization, scene factorization tends to be dominated by either the time-invariant or the time-varying representation, a phenomena also observed in recent methods [28,42]. To facilitate factorization, Gao et al. [19] initialize their system using masks from semantic segmentation, relying on the assumption that all moving objects are captured by a set of candidate semantic segmentation labels, and segmentation masks are temporally accurate. However, these assumptions do not hold in many real-world scenarios, as observed by Zhang et al. [79]. Instead, we propose a new motion segmentation module that produces segmentation masks for supervising our main two-component scene representation. Our idea is inspired by the Bayesian learning techniques proposed in recent work [45,79], but integrated into a volumetric IBR representation for dynamic videos.\nIn particular, before training our main two-component scene representation, we jointly train two lightweight models to obtain a motion segmentation mask M i for each input frame I i . We model static scene content with an IBR-Net [70] that renders a pixel colorB st using volume rendering along each ray via feature aggregation along epipolar lines from nearby source views without considering scene i (bottom). Our approach segments challenging dynamic elements such as the moving shadow, swing, and swaying bushes. motion; we model dynamic scene content with a 2D convolutional encoder-decoder network D, which predicts a 2D opacity map \u03b1 dy i , confidence map \u03b2 dy i , and RGB imageB dy i from an input frame:\nB dy i , \u03b1 dy i , \u03b2 dy i = D(I i ).(5)\nThe full reconstructed image is then composited pixelwise from the outputs of the two models:\nB full i (r) = \u03b1 dy i (r)B dy i (r) + (1 \u2212 \u03b1 dy i (r))B st (r).(6)\nTo segment moving objects, we assume the observed pixel color is uncertain in a heteroscedastic aleatoric manner, and model the observations in the video with a Cauchy distribution with time dependent confidence \u03b2 dy i . By taking the negative log-likelihood of the observations, our segmentation loss is written as a weighted reconstruction loss:\nL seg = r log \u03b2 dy i (r) + ||B full i (r) \u2212 C i (r)|| 2 \u03b2 dy i (r) .(7)\nBy optimizing the two models using Eq. 7, we obtain a motion segmentation mask M i by thresholding \u03b1 dy i at 0.5. We do not require an alpha regularization loss as in NeRF-W [45] to avoid degeneracies, since we naturally include such an inductive basis by excluding skip connections from network D, which leads D to converge more slowly than the static IBR model. We show our estimated motion segmentation masks overlaid on input images in Fig. 5.\nSupervision with segmentation masks. We initialize our main time-varying and time-invariant models with masks M i as in Omnimatte [43], by applying a reconstruction loss to renderings from the time-varying model in dynamic regions, and to renderings from the time-invariant model in static regions:\nL mask = r (1 \u2212 M i )(r)\u03c1(\u0108 st (r), C i (r)) + r M i (r)\u03c1(\u0108 dy i (r), C i (r))(8)\nWe perform morphological erosion and dilation on M i to obtain masks of dynamic and static regions respectively in order to turn off the loss near mask boundaries. We supervise the system with L mask and decay the weights by a factor of 5 for dynamic regions every 50K optimization steps.", "publication_ref": ["b34", "b44", "b27", "b41", "b18", "b78", "b44", "b78", "b69", "b44", "b42"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Regularization", "text": "As noted in prior work, monocular reconstruction of complex dynamic scenes is highly ill-posed, and using photometric consistency alone is insufficient to avoid bad local minima during optimization [19,35]. Therefore, we adopt regularization schemes used in prior work [2,35,73,75], which consist of three main parts L reg = L data + L MT + L cpt . L data is a data-driven term consisting of 1 monocular depth and optical flow consistency priors using the estimates from Zhang et al. [80] and RAFT [65]. L MT is a motion trajectory regularization term that encourages estimated trajectory fields to be cycle-consistent and spatial-temporally smooth. L cpt is a compactness prior that encourages the scene decomposition to be binary via an entropy loss, and mitigates floaters through distortion losses [2]. We refer readers to the supplement for more details.\nIn summary, the final combined loss used to optimize our main representation for space-time view synthesis is:\nL = L pho + L mask + L reg .(9)\n4. Implementation details Data. We conduct numerical evaluations on the Nvidia Dynamic Scene Dataset [75] and UCSD Dynamic Scenes Dataset [37]. Each dataset consists of eight forward-facing dynamic scenes recorded by synchronized multi-view cameras. We follow prior work [35] to derive a monocular video from each sequence, where each video contains 100\u223c250 frames. We removed frames that lack large regions of moving objects. We follow the protocol from prior work [35] that uses held-out images per time instance for evaluation. We also tested the methods on in-the-wild monocular videos, which feature more challenging camera and object motions [20]. View selection. For the time-varying, dynamic model we use a frame window radius of r = 3 for all experiments. For the time-invariant model representing static scene content, we use separate strategies for the dynamic scenes benchmarks and for in-the-wild videos. For the benchmarks, where camera viewpoints are located at discrete camera rig locations, we choose all nearby distinct viewpoints whose timestep is within 12 frames of the target time. For in-thewild videos, naively choosing the nearest source views can lead to poor reconstruction due to insufficient camera baseline. Thus, to ensure that for any rendered pixel we have sufficient source views for computing its color, we select source views from distant frames. If we wish to select N vs source views for the time-invariant model, we sub-sample every 2r max N vs frames from the input video to build a candidate pool, where for a given target time i, we only search source views within [i \u2212 r max , i + r max ] frames. We estimate r max using the method of Li et al. [34] based on SfM point covisibility and camera relative baseline. We then construct a final set of source views for the model by choosing the top N vs frames in the candidate pool that are the closest to the target view in terms of camera baseline. We set N vs = 16.\nGlobal spatial coordinate embedding With local image feature aggregation alone, it is hard to determine density accurately on non-surface or occluded surface points due to inconsistent features from different source views, as described in NeuRay [39]. Therefore, to improve global reasoning for density prediction, we append a global spatial coordinate embedding as an input to the ray transformer, in addition to the time embedding, similar to the ideas from [64]. Please see supplement for more details.\nHandling degeneracy through virtual views. Prior work [35] observed that optimization can converge to bad local minimal if camera and object motions are mostly colinear, or scene motions are too fast to track. Inspired by [36], we synthesize images at eight randomly sampled nearby viewpoints for every input time via depths estimated by [80]. During rendering, we randomly sample virtual view as additional source image. We only apply this technique to in-the-wild videos since it help avoid degenerate solutions while improving rendering quality, whereas we don't observe improvement on the benchmarks due to disentangled camera-object motions as described by [20].\nTime interpolation. Our approach also allows for time interpolation by performing scene motion-based splatting, as introduced by NSFF [35]. To render at a specified target fractional time, we predict the volumetric density and color at two nearby input times by aggregating local image features from their corresponding set of source views. The predicted color and density are then splatted and linearly blended via the scene flow derived from our motion trajectories, and weighted according to the target fractional time index.\nSetup. We estimate camera poses using COLMAP [57].\nFor each ray, we use a coarse-to-fine sampling strategy with 128 per-ray samples as in Wang et al. [70]. A separate model is trained from scratch for each scene using the Adam optimizer [29]. The network architecture for two main representations is a variant of that of IBRNet [70]. We reconstruct the entire scene in Euclidean space without special scene parameterization. Optimizing a full system on a 10 second video takes around two days using 8 Nvidia A100s, and rendering takes roughly 20 seconds for a 768 \u00d7 432 frame. We Table 2. Quantitative evaluation on the UCSD dataset [37].\nrefer readers to the supplemental material for network architectures, hyper-parameters setting, and additional details.", "publication_ref": ["b18", "b34", "b1", "b34", "b72", "b74", "b79", "b64", "b1", "b74", "b36", "b34", "b34", "b19", "b33", "b38", "b63", "b34", "b35", "b79", "b19", "b34", "b56", "b69", "b28", "b69", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Baselines and error metrics", "text": "We compare our approach to state-of-the-art monocular view synthesis methods. Specifically, we compare to three recent canonical space-based methods, Nerfies [49], and HyperNeRF [50], and to two scene flow-based methods, NSFF [35] and Dynamic View Synthesis (DVS) from Gao et al. [19]. For fair comparisons, we use the same depth, optical flow and motion segmentation masks used for our approach as inputs to other methods.\nFollowing prior work [35], we report the rendering quality of each method with three standard error metrics: peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and perceptual similarity via LPIPS [77], and calculate errors both over the entire scene (Full) and restricted to moving regions (Dynamic Only).", "publication_ref": ["b48", "b49", "b34", "b18", "b34", "b76"], "figure_ref": [], "table_ref": []}, {"heading": "Quantitative evaluation", "text": "Quantitative results on the two benchmark datasets are shown in Table 1 and Table 2. Our approach significantly improves over prior state-of-the-art methods in terms of all error metrics. Notably, our approach improves PSNR over entire scene upon the second best methods by 2dB and 4dB on each of the two datasets. Our approach also reduces LPIPS error, a major indicator of perceptual quality compared with real images [77], by over 50%. These results suggest that our framework is much more effective at recovering highly detailed scene contents. Ablation study. We conduct an ablation study on the Nvidia Dynamic Scene Dataset to validate the effectiveness of our various proposed system components. We show comparisons between our full system and variants in Tab. ", "publication_ref": ["b76"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Qualitative evaluation", "text": "Dynamic scenes dataset. We provide qualitative comparisons between our approach and three prior state-of-the-art methods [19,35,50] on the test views from two datasets in Fig. 6 and Fig. 7. Prior dynamic-NeRF methods have difficulty rendering details of moving objects, as seen in the excessively blurred dynamic content including the texture of balloons, human faces, and clothing. In contrast, our approach synthesizes photo-realistic novel views of both static and dynamic scene content and which are closest to the ground truth images.\nIn-the-wild videos. We show qualitative comparisons on in-the-wild footage of complex dynamic scenes. We show comparisons with Dynamic-NeRF based methods in Fig. 8, and Fig. 9 shows comparisons with point cloud rendering using depths [80] . Our approach synthesizes photo-realistic DVS HyperNeRF NSFF Ours Input Figure 8. Qualitative comparisons on in-the-wild videos. We show results on 10-second videos of complex dynamic scenes. The leftmost column shows the start and end frames of each video; on the right we show novel views at intermediate times rendered from our approach and prior state-of-the-art methods [19,35,50].   novel views, whereas prior dynamic-Nerf methods fail to recover high-quality details of both static and moving scene contents, such as the shirt wrinkles and the dog's fur in Fig. 8. On the other hand, explicit depth warping produces holes at regions near disocculusions and out of field of view. We refer readers to the supplementary video for full comparisons.", "publication_ref": ["b18", "b34", "b49", "b79", "b18", "b34", "b49"], "figure_ref": ["fig_3", "fig_4"], "table_ref": []}, {"heading": "Discussion and conclusion", "text": "Limitations. Our method is limited to relatively small viewpoint changes compared with methods designed for static or quasi-static scene; Our method is not able to handle small fast moving object due to incorrect initial depth and optical flow estimates (left, Fig. 10). In addition, compared to Figure 10. Limitations. Our method might fail to model moving thin objects such as moving leash (left). Our method can fail to render dynamic contents only visible in distant frames (middle). The rendered static content can be unrealistic or blank if insufficient source views feature are aggregated for a given pixel (right).\nprior dynamic NeRF methods, the synthesized views are not strictly multi-view consistent, and rendering quality of static content depends on which source views are selected (middle, Fig. 10). Our approach is also sensitive to degenerate motion patterns from in-the-wild videos, in which object and camera motion is mostly colinear, but we show heuristics to handle such cases in the supplemental. Moreover, our method is able to synthesize dynamic contents only appearing at distant time (right, Fig. 10)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion.", "text": "We presented a new approach for space-time view synthesis from a monocular video depicting a complex dynamic scene. By representing a dynamic scene within a volumetric IBR framework, our approach overcomes limitations of recent methods that cannot model long videos with complex camera and object motion. We have shown that our method can synthesize photo-realistic novel views from in-the-wild dynamic videos, and can achieve significant improvements over prior state-of-the-art methods on the dynamic scene benchmarks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements. Thanks to Andrew Liu, Richard Bowen and Lucy Chai for the fruitful discussions, and thanks to Rick Szeliski and Ricardo Martin-Brualla for helpful proofreading.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "4D visualization of dynamic events from unconstrained multi-view videos", "journal": "", "year": "2020", "authors": "Aayush Bansal; Minh Vo; Yaser Sheikh; Deva Ramanan; Srinivasa Narasimhan"}, {"ref_id": "b1", "title": "Mip-NeRF 360: Unbounded anti-aliased neural radiance fields", "journal": "", "year": "", "authors": "T Jonathan; Ben Barron; Dor Mildenhall;  Verbin; P Pratul; Peter Srinivasan;  Hedman"}, {"ref_id": "b2", "title": "X-fields: Implicit neural view-, light-and time-image interpolation", "journal": "ACM Trans. Graphics", "year": "", "authors": "Mojtaba Bemana; Karol Myszkowski; Hans-Peter Seidel; Tobias Ritschel"}, {"ref_id": "b3", "title": "Neural reflectance fields for appearance acquisition", "journal": "", "year": "2008", "authors": "Sai Bi; Zexiang Xu; Pratul P Srinivasan; Ben Mildenhall; Kalyan Sunkavalli; Milovs Havsan; Yannick Hold-Geoffroy; D Kriegman; R Ramamoorthi"}, {"ref_id": "b4", "title": "Deepdeform: Learning non-rigid RGB-D reconstruction with semi-supervised data", "journal": "", "year": "2020-06", "authors": "Aljaz Bozic; Michael Zollhofer; Christian Theobalt; Matthias Niessner"}, {"ref_id": "b5", "title": "Immersive light field video with a layered mesh representation", "journal": "ACM Trans. Graph", "year": "2020-07", "authors": "Michael Broxton; John Flynn; Ryan Overbeck; Daniel Erickson; Peter Hedman; Matthew Duvall; Jason Dourgarian; Jay Busch; Matt Whalen; Paul Debevec"}, {"ref_id": "b6", "title": "Unstructured lumigraph rendering", "journal": "", "year": "2001", "authors": "Chris Buehler; Michael Bosse; Leonard Mcmillan; Steven Gortler; Michael Cohen"}, {"ref_id": "b7", "title": "Free-viewpoint video of human actors", "journal": "ACM transactions on graphics (TOG)", "year": "2003", "authors": "Joel Carranza; Christian Theobalt; A Marcus; Hans-Peter Magnor;  Seidel"}, {"ref_id": "b8", "title": "Plenoptic sampling", "journal": "ACM Press/Addison-Wesley Publishing Co", "year": "2000", "authors": "Jin-Xiang Chai; Xin Tong; Shing-Chow Chan; Heung-Yeung Shum"}, {"ref_id": "b9", "title": "Two deterministic half-quadratic regularization algorithms for computed imaging", "journal": "IEEE", "year": "1994", "authors": "Pierre Charbonnier; Laure Blanc-Feraud; Gilles Aubert; Michel Barlaud"}, {"ref_id": "b10", "title": "MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo", "journal": "", "year": "2021", "authors": "Anpei Chen; Zexiang Xu; Fuqiang Zhao; Xiaoshuai Zhang; Fanbo Xiang; Jingyi Yu; Hao Su"}, {"ref_id": "b11", "title": "Extreme view synthesis", "journal": "", "year": "2019", "authors": "Inchang Choi; Orazio Gallo; Alejandro Troccoli; H Min; Jan Kim;  Kautz"}, {"ref_id": "b12", "title": "Performance capture from sparse multi-view video", "journal": "", "year": "2008", "authors": "Carsten Edilson De Aguiar; Christian Stoll; Naveed Theobalt; Hans-Peter Ahmed; Sebastian Seidel;  Thrun"}, {"ref_id": "b13", "title": "Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach", "journal": "", "year": "1996", "authors": " Paul E Debevec; J Camillo; Jitendra Taylor;  Malik"}, {"ref_id": "b14", "title": "Fusion4D: real-time performance capture of challenging scenes", "journal": "ACM Trans. Graphics", "year": "2016", "authors": "Mingsong Dou; Sameh Khamis; Yury Degtyarev; Philip L Davidson; Sean Ryan Fanello; Adarsh Kowdle; Sergio Orts; Christoph Rhemann; David Kim; Jonathan Taylor; Pushmeet Kohli; Vladimir Tankovich; Shahram Izadi"}, {"ref_id": "b15", "title": "Neural radiance flow for 4d view synthesis and video processing", "journal": "IEEE Computer Society", "year": "2021", "authors": "Yilun Du; Yinan Zhang; Hong-Xing Yu; Joshua B Tenenbaum; Jiajun Wu"}, {"ref_id": "b16", "title": "DeepView: View synthesis with learned gradient descent", "journal": "", "year": "2019", "authors": "John Flynn; Michael Broxton; Paul Debevec; Matthew Duvall; Graham Fyffe; Ryan Overbeck; Noah Snavely; Richard Tucker"}, {"ref_id": "b17", "title": "DeepStereo: Learning to predict new views from the world's imagery", "journal": "", "year": "2016", "authors": "John Flynn; Ivan Neulander; James Philbin; Noah Snavely"}, {"ref_id": "b18", "title": "Dynamic view synthesis from dynamic monocular video", "journal": "", "year": "2021", "authors": "Chen Gao; Ayush Saraf; Johannes Kopf; Jia-Bin Huang"}, {"ref_id": "b19", "title": "Monocular dynamic view synthesis: A reality check", "journal": "", "year": "2022", "authors": "Hang Gao; Ruilong Li; Shubham Tulsiani; Bryan Russell; Angjoo Kanazawa"}, {"ref_id": "b20", "title": "The lumigraph", "journal": "", "year": "1996", "authors": "J Steven; Radek Gortler; Richard Grzeszczuk; Michael F Szeliski;  Cohen"}, {"ref_id": "b21", "title": "The relightables: Volumetric performance capture of humans with realistic relighting", "journal": "ACM Transactions on Graphics (ToG)", "year": "2019", "authors": "Kaiwen Guo; Peter Lincoln; Philip Davidson; Jay Busch; Xueming Yu; Matt Whalen; Geoff Harvey; Sergio Orts-Escolano; Rohit Pandey; Jason Dourgarian"}, {"ref_id": "b22", "title": "Deep blending for free-viewpoint image-based rendering", "journal": "ACM Trans. Graphics", "year": "2018", "authors": "Peter Hedman; Julien Philip; True Price; Jan-Michael Frahm; George Drettakis; Gabriel Brostow"}, {"ref_id": "b23", "title": "Scalable inside-out image-based rendering", "journal": "ACM Transactions on Graphics (TOG)", "year": "2016", "authors": "Peter Hedman; Tobias Ritschel; George Drettakis; Gabriel Brostow"}, {"ref_id": "b24", "title": "VolumeDeform: Real-time volumetric non-rigid reconstruction", "journal": "", "year": "2016", "authors": "Matthias Innmann; Michael Zollh\u00f6fer; Matthias Niessner; Christian Theobalt; Marc Stamminger"}, {"ref_id": "b25", "title": "Learning-based view synthesis for light field cameras", "journal": "ACM Trans. Graphics", "year": "2016", "authors": "Ting-Chun Nima Khademi Kalantari; Ravi Wang;  Ramamoorthi"}, {"ref_id": "b26", "title": "Virtualized reality: Constructing virtual worlds from real scenes", "journal": "IEEE multimedia", "year": "1997", "authors": "Takeo Kanade; Peter Rander; P J Narayanan"}, {"ref_id": "b27", "title": "Layered neural atlases for consistent video editing", "journal": "ACM Transactions on Graphics (TOG)", "year": "2021", "authors": "Yoni Kasten; Dolev Ofri; Oliver Wang; Tali Dekel"}, {"ref_id": "b28", "title": "Adam: A method for stochastic optimization. CoRR, abs/1412", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b29", "title": "Firstperson hyper-lapse videos", "journal": "ACM Transactions on Graphics (TOG)", "year": "2014", "authors": "Johannes Kopf; F Michael; Richard Cohen;  Szeliski"}, {"ref_id": "b30", "title": "Robust consistent video depth estimation", "journal": "", "year": "2021", "authors": "Johannes Kopf; Xuejian Rong; Jia-Bin Huang"}, {"ref_id": "b31", "title": "Light field rendering", "journal": "", "year": "1996", "authors": "Marc Levoy; Pat Hanrahan"}, {"ref_id": "b32", "title": "Neural 3D video synthesis from multi-view video", "journal": "", "year": "2022", "authors": "Tianye Li; Mira Slavcheva; Michael Zollhoefer; Simon Green; Christoph Lassner; Changil Kim; Tanner Schmidt; Steven Lovegrove; Michael Goesele; Richard Newcombe"}, {"ref_id": "b33", "title": "Learning the depths of moving people by watching frozen people", "journal": "", "year": "2019", "authors": "Zhengqi Li; Tali Dekel; Forrester Cole; Richard Tucker; Noah Snavely; Ce Liu; William T Freeman"}, {"ref_id": "b34", "title": "Neural scene flow fields for space-time view synthesis of dynamic scenes", "journal": "", "year": "", "authors": "Zhengqi Li; Simon Niklaus; Noah Snavely; Oliver Wang"}, {"ref_id": "b35", "title": "Infinitenature-zero: Learning perpetual view generation of natural scenes from single images", "journal": "Springer", "year": "2022", "authors": "Zhengqi Li; Qianqian Wang; Noah Snavely; Angjoo Kanazawa"}, {"ref_id": "b36", "title": "Deep 3D mask volume for view synthesis of dynamic scenes", "journal": "", "year": "", "authors": "Kai-En Lin; Lei Xiao; Feng Liu; Guowei Yang; Ravi Ramamoorthi"}, {"ref_id": "b37", "title": "Neural sparse voxel fields", "journal": "", "year": "2020", "authors": "Lingjie Liu; Jiatao Gu; Tat-Seng Kyaw Zaw Lin; Christian Chua;  Theobalt"}, {"ref_id": "b38", "title": "Neural rays for occlusion-aware image-based rendering", "journal": "", "year": "", "authors": "Yuan Liu; Sida Peng; Lingjie Liu; Qianqian Wang; Peng Wang; Christian Theobalt; Xiaowei Zhou; Wenping Wang"}, {"ref_id": "b39", "title": "Neural volumes: Learning dynamic renderable volumes from images", "journal": "ACM Trans. Graphics", "year": "2019", "authors": "Stephen Lombardi; Tomas Simon; Jason Saragih; Gabriel Schwartz; Andreas Lehrmann; Yaser Sheikh"}, {"ref_id": "b40", "title": "SMPL: A skinned multiperson linear model", "journal": "ACM transactions on graphics (TOG)", "year": "2015", "authors": "Matthew Loper; Naureen Mahmood; Javier Romero; Gerard Pons-Moll; Michael J Black"}, {"ref_id": "b41", "title": "Layered neural rendering for retiming people in video", "journal": "ACM Trans. Graphics (SIGGRAPH Asia", "year": "", "authors": "Erika Lu; F Cole; Tali Dekel; Weidi Xie; Andrew Zisserman; D Salesin; W Freeman; M Rubinstein"}, {"ref_id": "b42", "title": "Omnimatte: Associating objects and their effects in video", "journal": "", "year": "", "authors": "Erika Lu; Forrester Cole; Tali Dekel; Andrew Zisserman; T William; Michael Freeman;  Rubinstein"}, {"ref_id": "b43", "title": "Consistent video depth estimation", "journal": "ACM Trans. Graph", "year": "2020-07", "authors": "Xuan Luo; Jia-Bin Huang; Richard Szeliski; Kevin Matzen; Johannes Kopf"}, {"ref_id": "b44", "title": "NeRF in the wild: Neural radiance fields for unconstrained photo collections", "journal": "", "year": "", "authors": "Ricardo Martin-Brualla; N Radwan; S M Mehdi; J Sajjadi; A Barron; Daniel Dosovitskiy;  Duckworth"}, {"ref_id": "b45", "title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "journal": "", "year": "", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng"}, {"ref_id": "b46", "title": "Dy-namicFusion: Reconstruction and tracking of non-rigid scenes in real-time", "journal": "", "year": "2015", "authors": "A Richard; Dieter Newcombe; Steven M Fox;  Seitz"}, {"ref_id": "b47", "title": "Differentiable volumetric rendering: Learning implicit 3D representations without 3D supervision", "journal": "", "year": "2020", "authors": "Michael Niemeyer; Lars Mescheder; Michael Oechsle; Andreas Geiger"}, {"ref_id": "b48", "title": "Nerfies: Deformable neural radiance fields", "journal": "", "year": "2021", "authors": "Keunhong Park; Utkarsh Sinha; Jonathan T Barron; Sofien Bouaziz; Dan B Goldman; M Steven; Ricardo Seitz;  Martin-Brualla"}, {"ref_id": "b49", "title": "HyperNeRF: A higherdimensional representation for topologically varying neural radiance fields", "journal": "", "year": "2021", "authors": "Keunhong Park; Utkarsh Sinha; Peter Hedman; Jonathan T Barron; Sofien Bouaziz; Dan B Goldman; Ricardo Martin-Brualla; Steven M Seitz"}, {"ref_id": "b50", "title": "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans", "journal": "", "year": "2021", "authors": "Sida Peng; Yuanqing Zhang; Yinghao Xu; Qianqian Wang; Qing Shuai; Hujun Bao; Xiaowei Zhou"}, {"ref_id": "b51", "title": "Soft 3D reconstruction for view synthesis", "journal": "ACM Trans. Graphics", "year": "2017", "authors": "Eric Penner; Li Zhang"}, {"ref_id": "b52", "title": "Neural radiance fields for dynamic scenes", "journal": "", "year": "2021", "authors": "Albert Pumarola; Enric Corona; Gerard Pons-Moll; Francesc Moreno-Noguer; . D-Nerf "}, {"ref_id": "b53", "title": "Free view synthesis", "journal": "", "year": "", "authors": "Gernot Riegler; V Koltun"}, {"ref_id": "b54", "title": "Stable view synthesis", "journal": "", "year": "2021", "authors": "Gernot Riegler; Vladlen Koltun"}, {"ref_id": "b55", "title": "PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization", "journal": "", "year": "2019", "authors": "Shunsuke Saito; Zeng Huang; Ryota Natsume; Shigeo Morishima; Angjoo Kanazawa; Hao Li"}, {"ref_id": "b56", "title": "Structurefrom-motion revisited", "journal": "", "year": "2016", "authors": "L Johannes; Jan-Michael Schonberger;  Frahm"}, {"ref_id": "b57", "title": "Review of image-based rendering techniques", "journal": "SPIE", "year": "2000", "authors": "Harry Shum; Bing Sing;  Kang"}, {"ref_id": "b58", "title": "Implicit neural representations with periodic activation functions", "journal": "", "year": "2020", "authors": "Vincent Sitzmann; Julien Martel; Alexander Bergman; David Lindell; Gordon Wetzstein"}, {"ref_id": "b59", "title": "Deepvoxels: Learning persistent 3D feature embeddings", "journal": "", "year": "2019", "authors": "Vincent Sitzmann; Justus Thies; Felix Heide; Matthias Nie\u00dfner; Gordon Wetzstein; Michael Zollhofer"}, {"ref_id": "b60", "title": "Scene representation networks: Continuous 3D-structureaware neural scene representations", "journal": "", "year": "2019", "authors": "Vincent Sitzmann; Michael Zollh\u00f6fer; Gordon Wetzstein"}, {"ref_id": "b61", "title": "Pushing the boundaries of view extrapolation with multiplane images", "journal": "", "year": "2019", "authors": "P Pratul; Richard Srinivasan; Jonathan T Tucker; Ravi Barron; Ren Ramamoorthi; Noah Ng;  Snavely"}, {"ref_id": "b62", "title": "View and time interpolation in image space", "journal": "", "year": "", "authors": "Timo Stich; Christian Linz; Georgia Albuquerque; Marcus Magnor"}, {"ref_id": "b63", "title": "Light field neural rendering", "journal": "", "year": "", "authors": "Mohammed Suhail; Carlos Esteves; Leonid Sigal; Ameesh Makadia"}, {"ref_id": "b64", "title": "RAFT: Recurrent all-pairs field transforms for optical flow", "journal": "", "year": "", "authors": "Zachary Teed; Jia Deng"}, {"ref_id": "b65", "title": "Nonrigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video", "journal": "", "year": "2021", "authors": "Edgar Tretschk; Ayush Tewari; Vladislav Golyanik; Michael Zollh\u00f6fer; Christoph Lassner; Christian Theobalt"}, {"ref_id": "b66", "title": "Neural trajectory fields for dynamic novel view synthesis", "journal": "", "year": "2021", "authors": "Chaoyang Wang; Ben Eckart; Simon Lucey; Orazio Gallo"}, {"ref_id": "b67", "title": "Neural prior for trajectory estimation", "journal": "", "year": "2022", "authors": "Chaoyang Wang; Xueqian Li; Jhony Kaesemodel Pontes; Simon Lucey"}, {"ref_id": "b68", "title": "Fourier plenoctrees for dynamic radiance field rendering in real-time", "journal": "", "year": "2022", "authors": "Liao Wang; Jiakai Zhang; Xinhang Liu; Fuqiang Zhao; Yanshun Zhang; Yingliang Zhang; Minye Wu; Jingyi Yu; Lan Xu"}, {"ref_id": "b69", "title": "Ibrnet: Learning multi-view image-based rendering", "journal": "", "year": "2021", "authors": "Qianqian Wang; Zhicheng Wang; Kyle Genova; P Pratul; Howard Srinivasan; Jonathan T Zhou; Ricardo Barron; Noah Martin-Brualla; Thomas Snavely;  Funkhouser"}, {"ref_id": "b70", "title": "Hu-manNeRF: Free-viewpoint rendering of moving people from monocular video", "journal": "", "year": "2022", "authors": "Chung-Yi Weng; Brian Curless; P Pratul; Jonathan T Srinivasan; Ira Barron;  Kemelmacher-Shlizerman"}, {"ref_id": "b71", "title": "NeX: Real-time view synthesis with neural basis expansion", "journal": "", "year": "2021", "authors": "Suttisak Wizadwongsa; Pakkapon Phongthawee; Jiraphon Yenphraphai; Supasorn Suwajanakorn"}, {"ref_id": "b72", "title": "D 2 NeRF: Self-supervised decoupling of dynamic and static objects from a monocular video", "journal": "", "year": "2022", "authors": "Tianhao Wu; Fangcheng Zhong; Andrea Tagliasacchi; Forrester Cole; Cengiz Oztireli"}, {"ref_id": "b73", "title": "Space-time neural irradiance fields for free-viewpoint video", "journal": "", "year": "2021", "authors": "Wenqi Xian; Jia-Bin Huang; Johannes Kopf; Changil Kim"}, {"ref_id": "b74", "title": "Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera", "journal": "", "year": "2020", "authors": "Jae Shin Yoon; Kihwan Kim; Orazio Gallo; Hyun Soo Park; Jan Kautz"}, {"ref_id": "b75", "title": "Editable free-viewpoint video using a layered neural representation", "journal": "ACM Transactions on Graphics (TOG)", "year": "2021", "authors": "Jiakai Zhang; Xinhang Liu; Xinyi Ye; Fuqiang Zhao; Yanshun Zhang; Minye Wu; Yingliang Zhang; Lan Xu; Jingyi Yu"}, {"ref_id": "b76", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "Richard Zhang; Phillip Isola; Alexei A Efros; Eli Shechtman; Oliver Wang"}, {"ref_id": "b77", "title": "Structure and motion from casual videos", "journal": "Springer", "year": "2022", "authors": "Zhoutong Zhang; Forrester Cole; Zhengqi Li; Michael Rubinstein; Noah Snavely; William T Freeman"}, {"ref_id": "b78", "title": "Structure and motion from casual videos", "journal": "", "year": "", "authors": "Zhoutong Zhang; Forrester Cole; Zhengqi Li; Noah Snavely; William Freeman"}, {"ref_id": "b79", "title": "Consistent depth of moving objects in video", "journal": "ACM Transactions on Graphics (TOG)", "year": "2021", "authors": "Zhoutong Zhang; Forrester Cole; Richard Tucker; T William; Tali Freeman;  Dekel"}, {"ref_id": "b80", "title": "Stereo magnification: learning view synthesis using multiplane images", "journal": "ACM Trans. Graphics", "year": "2018", "authors": "Tinghui Zhou; Richard Tucker; John Flynn; Graham Fyffe; Noah Snavely"}, {"ref_id": "b81", "title": "High-quality video view interpolation using a layered representation", "journal": "ACM Trans. Graphics", "year": "2004", "authors": " C Lawrence Zitnick; Bing Sing; Matthew Kang; Simon Uyttendaele; Richard Winder;  Szeliski"}, {"ref_id": "b82", "title": "Real-time non-rigid reconstruction using an RGB-D camera", "journal": "ACM Trans. Graphics", "year": "2014", "authors": "Michael Zollh\u00f6fer; Matthias Niessner; Shahram Izadi; Christoph Rehmann; Christopher Zach; Matthew Fisher; Chenglei Wu; Andrew Fitzgibbon; Charles Loop; Christian Theobalt"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure2. Rendering via motion-adjusted multi-view feature aggregation. Given a sampled location x at time i along a target ray r, we estimate its motion trajectory, which determines the 3D correspondence of x at nearby time j \u2208 N (i), denoted xi\u2192j. Each warped point is then projected into its corresponding source view. Image features fj extracted along the projected curves are aggregated and fed to the ray transformer with time embedding \u03b3(i), producing per-sample color and density (ci, \u03c3i). The final pixel color\u0108i is then synthesized by volume rendering (ci, \u03c3i) along r.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 .4Figure 4. Qualitative ablations. From left to right, we show rendered novel views (top) and depths (bottom) from our system (a) without enforcing temporal consistency, (b) aggregating image features with scene flow fields instead of motion trajectories, (c) representing motion trajectory with a fixed DCT basis instead of a learned one, and (d) with full configuration. Simpler configurations significantly degrade rendering quality as indicated by PSNR calculated over the regions of moving objects.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 .5Figure 5. Motion segmentation. We show full renderingB full i (top) and motion segmentation overlaid with rendered dynamic content \u03b1 dy i B dyi (bottom). Our approach segments challenging dynamic elements such as the moving shadow, swing, and swaying bushes.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 7 .7Figure 7. Qualitative comparisons on the UCSD dataset [37].", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 9 .9Figure9. From left to right, We show inputs and corresponding novel views rendered from explicit depth warping with Zhang et al.[80], and from our approach.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative evaluation on the Nvidia dataset[75].", "figure_data": "MethodsFullDynamic OnlySSIM\u2191 PSNR\u2191 LPIPS\u2193 SSIM\u2191 PSNR\u2191 LPIPS\u2193Nerfies [49]0.60920.640.2040.45517.350.258HyperNeRF [50] 0.65420.900.1820.44617.560.242DVS [19]0.92127.440.0700.77822.630.144NSFF [35]0.92728.900.0620.78323.080.159Ours0.95730.860.0270.82424.240.062"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Qualitative comparisons on the Nvidia dataset[75].", "figure_data": "Our rendered viewsDVSHyperNeRFNSFFOursGTFigure 6. MethodsFullDynamic OnlySSIM\u2191 PSNR\u2191 LPIPS\u2193 SSIM\u2191 PSNR\u2191 LPIPS\u2193Nerfies [49]0.82324.320.0960.59518.450.234HyperNeRF [50] 0.85925.100.0950.61819.260.212DVS [19]0.94330.640.0750.86626.570.096NSFF [35]0.95231.750.0340.85125.830.115Ours0.98336.470.0140.90928.010.042"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ablation study on the Nvidia Dataset. See Sec. 5.2 for detailed descriptions of each configuration.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "{\u03c6 l i (x)} L l=1 = G MT (\u03b3(x), \u03b3(i)) ,(1)", "formula_coordinates": [3.0, 360.54, 360.55, 184.57, 12.69]}, {"formula_id": "formula_1", "formula_text": "\u2206 x,i (j) = \u0393 x,i (j) \u2212 \u0393 x,i (i).(2)", "formula_coordinates": [3.0, 368.82, 530.54, 176.3, 9.65]}, {"formula_id": "formula_2", "formula_text": "L pho = r j\u2208N (i)\u0174 j\u2192i (r)\u03c1(C i (r),\u0108 j\u2192i (r)).(3)", "formula_coordinates": [4.0, 324.36, 347.75, 220.75, 20.53]}, {"formula_id": "formula_3", "formula_text": "L pho = r j\u2208N (i)\u0174 j\u2192i (r)\u03c1(C i (r),\u0108 full j\u2192i (r))(4)", "formula_coordinates": [5.0, 66.99, 394.39, 219.37, 22.38]}, {"formula_id": "formula_4", "formula_text": "B dy i , \u03b1 dy i , \u03b2 dy i = D(I i ).(5)", "formula_coordinates": [5.0, 380.02, 258.93, 165.09, 13.61]}, {"formula_id": "formula_5", "formula_text": "B full i (r) = \u03b1 dy i (r)B dy i (r) + (1 \u2212 \u03b1 dy i (r))B st (r).(6)", "formula_coordinates": [5.0, 324.77, 301.98, 220.35, 13.61]}, {"formula_id": "formula_6", "formula_text": "L seg = r log \u03b2 dy i (r) + ||B full i (r) \u2212 C i (r)|| 2 \u03b2 dy i (r) .(7)", "formula_coordinates": [5.0, 318.87, 395.84, 226.24, 27.93]}, {"formula_id": "formula_7", "formula_text": "L mask = r (1 \u2212 M i )(r)\u03c1(\u0108 st (r), C i (r)) + r M i (r)\u03c1(\u0108 dy i (r), C i (r))(8)", "formula_coordinates": [5.0, 343.84, 603.78, 201.27, 48.36]}, {"formula_id": "formula_8", "formula_text": "L = L pho + L mask + L reg .(9)", "formula_coordinates": [6.0, 116.55, 301.37, 169.81, 9.81]}], "doi": ""}