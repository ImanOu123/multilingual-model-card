{"title": "Linguistically-Informed Self-Attention for Semantic Role Labeling", "authors": "Emma Strubell; Patrick Verga; Daniel Andor; David Weiss; Andrew Mccallum", "pub_date": "", "abstract": "Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-ofspeech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on outof-domain data, nearly 10% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.", "sections": [{"heading": "Introduction", "text": "Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005;Chen et al., 2013), machine reading (Berant et al., 2014;Wang et al., 2015) and translation (Liu and Gildea, 2010;Bazrafshan and Gildea, 2013).\nThough syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993;Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015;Tan et al., 2018;He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models.\nStill, recent work (Roth and Lapata, 2016; indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it.  indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment, and show that while constrained inference using a relatively low-accuracy predicted parse can provide small improvements in SRL accuracy, providing a gold-quality parse leads to substantial gains.  incorporate syntax from a high-quality parser (Kiperwasser and Goldberg, 2016) using graph convolutional neural networks (Kipf and Welling, 2017), but like  they attain only small increases over a model with no syntactic parse, and even perform worse than a syntax-free model on out-of-domain data. These works suggest that though syntax has the potential to improve neural network SRL models, we have not yet designed an architecture which maximizes the benefits of auxiliary syntactic information.\nIn response, we propose linguistically-informed self-attention (LISA): a model that combines multi-task learning (Caruana, 1993) with stacked layers of multi-head self-attention (Vaswani et al., 2017); the model is trained to: (1) jointly predict parts of speech and predicates; (2) perform parsing; and (3) attend to syntactic parse parents, while (4) assigning semantic role labels. Whereas prior work typically requires separate models to provide linguistic analysis, including most syntaxfree neural models which still rely on external predicate detection, our model is truly end-to-end: earlier layers are trained to predict prerequisite parts-of-speech and predicates, the latter of which are supplied to later layers for scoring. Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL, we more efficiently encode each sentence only once, predict its predicates, part-of-speech tags and labeled syntactic parse, then predict the semantic roles for all predicates in the sentence in parallel. The model is trained such that, as syntactic parsing models improve, providing high-quality parses at test time will improve its performance, allowing the model to leverage updated parsing models without requiring re-training.\nIn experiments on the CoNLL-2005 and CoNLL-2012 datasets we show that our linguistically-informed models out-perform the syntax-free state-of-the-art. On CoNLL-2005 with predicted predicates and standard word embeddings, our single model out-performs the previous state-of-the-art model on the WSJ test set by 2.5 F1 points absolute. On the challenging out-of-domain Brown test set, our model improves substantially over the previous state-of-the-art by more than 3.5 F1, a nearly 10% reduction in error. On CoNLL-2012, our model gains more than 2.5 F1 absolute over the previous state-of-the-art. Our models also show improvements when using contextually-encoded word representations (Peters et al., 2018), obtaining nearly 1.0 F1 higher than the state-of-the-art on CoNLL-2005 news and more than 2.0 F1 improvement on out-of-domain text. 1 1 Our implementation in TensorFlow (Abadi et al., 2015) is available at : http://github.com/strubell/ LISA \nB-ARG0 B-V B-ARG1 I-ARG1 I-ARG1 O O B-ARG0 I-ARG0 B-V\nFigure 1: Word embeddings are input to J layers of multi-head self-attention. In layer p one attention head is trained to attend to parse parents (Figure 2). Layer r is input for a joint predicate/POS classifier. Representations from layer r corresponding to predicted predicates are passed to a bilinear operation scoring distinct predicate and role representations to produce per-token SRL predictions with respect to each predicted predicate.", "publication_ref": ["b57", "b11", "b6", "b59", "b30", "b3", "b28", "b45", "b63", "b54", "b21", "b47", "b25", "b26", "b9", "b58", "b42", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "Our goal is to design an efficient neural network model which makes use of linguistic information as effectively as possible in order to perform endto-end SRL. LISA achieves this by combining: (1) A new technique of supervising neural attention to predict syntactic dependencies with (2) multi-task learning across four related tasks. Figure 1 depicts the overall architecture of our model. The basis for our model is the Transformer encoder introduced by Vaswani et al. (2017): we transform word embeddings into contextually-encoded token representations using stacked multi-head self-attention and feedforward layers ( \u00a72.1).\nTo incorporate syntax, one self-attention head is trained to attend to each token's syntactic parent, allowing the model to use this attention head as an oracle for syntactic dependencies. We introduce this syntactically-informed self-attention (Figure 2) in more detail in \u00a72.2.\nOur model is designed for the more realistic setting in which gold predicates are not provided at test-time. Our model predicts predicates and integrates part-of-speech (POS) information into earlier layers by re-purposing representations closer to the input to predict predicate and POS tags us-I saw the sloth climbing sloth (i) \n(t = 3) MatMul: Concat + FF sloth (i+1) + A[t] parse M [t] parse A i h V i h A[t] parse A[t] parse M i 0 [t] M i [t] M i 2 [t] A i 2 [t] A i 1 [t] A i 0 [t]\nFigure 2: Syntactically-informed self-attention for the query word sloth. Attention weights A parse heavily weight the token's syntactic governor, saw, in a weighted average over the token values V parse . The other attention heads act as usual, and the attended representations from all heads are concatenated and projected through a feed-forward layer to produce the syntacticallyinformed representation for sloth.\ning hard parameter sharing ( \u00a72.3). We simplify optimization and benefit from shared statistical strength derived from highly correlated POS and predicates by treating tagging and predicate detection as a single task, performing multi-class classification into the joint Cartesian product space of POS and predicate labels. Though typical models, which re-encode the sentence for each predicate, can simplify SRL to token-wise tagging, our joint model requires a different approach to classify roles with respect to each predicate. Contextually encoded tokens are projected to distinct predicate and role embeddings ( \u00a72.4), and each predicted predicate is scored with the sequence's role representations using a bilinear model (Eqn. 6), producing per-label scores for BIO-encoded semantic role labels for each token and each semantic frame.\nThe model is trained end-to-end by maximum likelihood using stochastic gradient descent ( \u00a72.5).", "publication_ref": ["b58"], "figure_ref": [], "table_ref": []}, {"heading": "Self-attention token encoder", "text": "The basis for our model is a multi-head selfattention token encoder, recently shown to achieve state-of-the-art performance on SRL (Tan et al., 2018), and which provides a natural mechanism for incorporating syntax, as described in \u00a72.2. Our implementation replicates Vaswani et al. (2017).\nThe input to the network is a sequence X of T token representations x t . In the standard setting these token representations are initialized to pretrained word embeddings, but we also experiment with supplying pre-trained ELMo representations combined with task-specific learned parameters, which have been shown to substantially improve performance of other SRL models (Peters et al., 2018). For experiments with gold predicates, we concatenate a predicate indicator embedding p t following previous work .\nWe project 2 these input embeddings to a representation that is the same size as the output of the self-attention layers. We then add a positional encoding vector computed as a deterministic sinusoidal function of t, since the self-attention has no innate notion of token position.\nWe feed this token representation as input to a series of J residual multi-head self-attention layers with feed-forward connections. Denoting the jth self-attention layer as T (j) (\u2022), the output of that layer s (j) t , and LN (\u2022) layer normalization, the following recurrence applied to initial input c\n(p) t : s (j) t = LN (s (j 1) t + T (j) (s (j 1) t ))\n(1)\ngives our final token representations s (j)\nt . Each T (j) (\u2022) consists of: (a) multi-head self-attention and (b) a feed-forward projection.\nThe multi-head self attention consists of H attention heads, each of which learns a distinct attention function to attend to all of the tokens in the sequence. This self-attention is performed for each token for each head, and the results of the H self-attentions are concatenated to form the final self-attended representation for each token.\nSpecifically, consider the matrix S (j 1) of T token representations at layer j 1. For each attention head h, we project this matrix into distinct key, value and query representations K\n(j) h , V (j) h and Q (j)\nh of dimensions T \u21e5d k , T \u21e5d q , and T \u21e5d v , respectively. We can then multiply Q\n(j) h by K (j) h\nto obtain a T \u21e5 T matrix of attention weights A (j) h between each pair of tokens in the sentence. Following Vaswani et al. (2017) we perform scaled dot-product attention: We scale the weights by the inverse square root of their embedding dimension and normalize with the softmax function to produce a distinct distribution for each token over all the tokens in the sentence:\nA (j) h = softmax(d 0.5 k Q (j) h K (j) h T )\n(2)\nThese attention weights are then multiplied by\nV (j)\nh for each token to obtain the self-attended token representations M\n(j) h : M (j) h = A (j) h V (j) h (3) Row t of M (j)\nh , the self-attended representation for token t at layer j, is thus the weighted sum with respect to t (with weights given by A\n(j) h ) over the token representations in V (j) h .\nThe outputs of all attention heads for each token are concatenated, and this representation is passed to the feed-forward layer, which consists of two linear projections each followed by leaky ReLU activations (Maas et al., 2013). We add the output of the feed-forward to the initial representation and apply layer normalization to give the final output of self-attention layer j, as in Eqn. 1.", "publication_ref": ["b54", "b58", "b42", "b58", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Syntactically-informed self-attention", "text": "Typically, neural attention mechanisms are left on their own to learn to attend to relevant inputs. Instead, we propose training the self-attention to attend to specific tokens corresponding to the syntactic structure of the sentence as a mechanism for passing linguistic knowledge to later layers.\nSpecifically, we replace one attention head with the deep bi-affine model of Dozat and Manning (2017), trained to predict syntactic dependencies. Let A parse be the parse attention weights, at layer i. Its input is the matrix of token representations S (i 1) . As with the other attention heads, we project S (i 1) into key, value and query representations, denoted K parse , Q parse , V parse . Here the key and query projections correspond to parent and dependent representations of the tokens, and we allow their dimensions to differ from the rest of the attention heads to more closely follow the implementation of Dozat and Manning (2017). Unlike the other attention heads which use a dot product to score key-query pairs, we score the compatibility between K parse and Q parse using a bi-affine operator U heads to obtain attention weights:\nA parse = softmax(Q parse U heads K T parse ) (4)\nThese attention weights are used to compose a weighted average of the value representations V parse as in the other attention heads. We apply auxiliary supervision at this attention head to encourage it to attend to each token's parent in a syntactic dependency tree, and to encode information about the token's dependency label. Denoting the attention weight from token t to a candidate head q as A parse [t, q], we model the probability of token t having parent q as:\nP (q = head(t) | X ) = A parse [t, q]\n(5) using the attention weights A parse [t] as the distribution over possible heads for token t. We define the root token as having a self-loop. This attention head thus emits a directed graph 3 where each token's parent is the token to which the attention A parse assigns the highest weight.\nWe also predict dependency labels using perclass bi-affine operations between parent and dependent representations Q parse and K parse to produce per-label scores, with locally normalized probabilities over dependency labels y dep t given by the softmax function. We refer the reader to Dozat and Manning (2017) for more details.\nThis attention head now becomes an oracle for syntax, denoted P, providing a dependency parse to downstream layers. This model not only predicts its own dependency arcs, but allows for the injection of auxiliary parse information at test time by simply setting A parse to the parse parents produced by e.g. a state-of-the-art parser. In this way, our model can benefit from improved, external parsing models without re-training. Unlike typical multi-task models, ours maintains the ability to leverage external syntactic information.", "publication_ref": ["b16", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Multi-task learning", "text": "We also share the parameters of lower layers in our model to predict POS tags and predicates. Following , we focus on the end-toend setting, where predicates must be predicted on-the-fly. Since we also train our model to predict syntactic dependencies, it is beneficial to give the model knowledge of POS information. While much previous work employs a pipelined approach to both POS tagging for dependency parsing and predicate detection for SRL, we take a multi-task learning (MTL) approach (Caruana, 1993), sharing the parameters of earlier layers in our SRL model with a joint POS and predicate detection objective. Since POS is a strong predictor of predicates 4 and the complexity of training a multi-task model increases with the number of tasks, we combine POS tagging and predicate detection into a joint label space: For each POS tag TAG which is observed co-occurring with a predicate, we add a label of the form TAG:PREDICATE.\nSpecifically, we feed the representation s (r) t from a layer r preceding the syntacticallyinformed layer p to a linear classifier to produce per-class scores r t for token t. We compute locally-normalized probabilities using the softmax function: P (y prp t | X ) / exp(r t ), where y prp t is a label in the joint space.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Predicting semantic roles", "text": "Our final goal is to predict semantic roles for each predicate in the sequence. We score each predicate against each token in the sequence using a bilinear operation, producing per-label scores for each token for each predicate, with predicates and syntax determined by oracles V and P.\nFirst, we project each token representation s\n(J) t\nto a predicate-specific representation s pred t and a role-specific representation s role t . We then provide these representations to a bilinear transformation U for scoring. So, the role label scores s ft for the token at index t with respect to the predicate at index f (i.e. token t and frame f ) are given by:\ns ft = (s pred f ) T Us role t (6\n)\nwhich can be computed in parallel across all semantic frames in an entire minibatch. We calculate a locally normalized distribution over role labels for token t in frame f using the softmax function:\nP (y role ft | P, V, X ) / exp(s ft ). At test time, we perform constrained decoding using the Viterbi algorithm to emit valid sequences of BIO tags, using unary scores s ft and the transition probabilities given by the training data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "We maximize the sum of the likelihoods of the individual tasks. In order to maximize our model's ability to leverage syntax, during training we clamp P to the gold parse (P G ) and V to gold predicates V G when passing parse and predicate representations to later layers, whereas syntactic head prediction and joint predicate/POS prediction are conditioned only on the input sequence X . The overall objective is thus:\n1 T T X t=1 h F X f =1 log P (y role ft | P G , V G , X ) + log P (y prp t | X ) + 1 log P (head(t) | X ) + 2 log P (y dep t | P G , X ) i (7\n)\nwhere 1 and 2 are penalties on the syntactic attention loss. We train the model using Nadam (Dozat, 2016) SGD combined with the learning rate schedule in Vaswani et al. (2017). In addition to MTL, we regularize our model using dropout (Srivastava et al., 2014). We use gradient clipping to avoid exploding gradients (Bengio et al., 1994;Pascanu et al., 2013). Additional details on optimization and hyperparameters are included in Appendix A.", "publication_ref": ["b15", "b58", "b49", "b5", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Early approaches to SRL (Pradhan et al., 2005;Surdeanu et al., 2007;Johansson and Nugues, 2008;Toutanova et al., 2008) focused on developing rich sets of linguistic features as input to a linear model, often combined with complex constrained inference e.g. with an ILP (Punyakanok et al., 2008).  showed that constraints could be enforced more efficiently using a clever dynamic program for exact inference. Sutton and McCallum (2005) modeled syntactic parsing and SRL jointly, and Lewis et al. (2015) jointly modeled SRL and CCG parsing. Collobert et al. (2011) were among the first to use a neural network model for SRL, a CNN over word embeddings which failed to out-perform non-neural models. FitzGerald et al. (2015) successfully employed neural networks by embedding lexicalized features and providing them as factors in the model of .\nMore recent neural models are syntax-free. Zhou and Xu (2015),  and   (2018) jointly predict SRL spans and predicates in a model based on that of , obtaining state-of-the-art predicted predicate SRL. Concurrent to this work, Peters et al. (2018) and He et al. (2018) report significant gains on PropBank SRL by training a wide LSTM language model and using a task-specific transformation of its hidden representations (ELMo) as a deep, and computationally expensive, alternative to typical word embeddings. We find that LISA obtains further accuracy increases when provided with ELMo word representations, especially on out-of-domain data. Some work has incorporated syntax into neural models for SRL. Roth and Lapata (2016) incorporate syntax by embedding dependency paths, and similarly  encode syntax using a graph CNN over a predicted syntax tree, out-performing models without syntax on CoNLL-2009. These works are limited to incorporating partial dependency paths between tokens whereas our technique incorporates the entire parse. Additionally,  report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011;Zhang and Weiss, 2016;Hashimoto et al., 2017;Peng et al., 2017;Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (S\u00f8gaard and Goldberg, 2016;Bingel and S\u00f8gaard, 2017;Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference.\nThe question of training on gold versus predicted labels is closely related to learning to search (Daum\u00e9 III et al., 2009;Ross et al., 2011;Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011;Goldberg and Nivre, 2012;Ballesteros et al., 2016). Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989) to MTL. We leave exploration of more advanced scheduled sampling techniques to future work.", "publication_ref": ["b44", "b50", "b23", "b55", "b45", "b51", "b29", "b13", "b17", "b63", "b42", "b21", "b47", "b9", "b13", "b62", "b20", "b40", "b52", "b48", "b7", "b1", "b31", "b14", "b46", "b10", "b4", "b12", "b19", "b2", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental results", "text": "We present results on the CoNLL-2005 shared task (Carreras and M\u00e0rquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora. We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained ELMo representations with fine-tuned task-specific parameters (Peters et al., 2018) in order to best compare to prior work. Hyperparameters that resulted in the best performance on the validation set were selected via a small grid search, and models were trained for a maximum of 4 days on one TitanX GPU using early stopping on the validation set. We convert constituencies to dependencies using the Stanford head rules v3.5 (de Marneffe and Manning, 2008). A detailed description of hyperparameter settings and data pre-processing can be found in Appendix A.\nWe compare our LISA models to four strong baselines: For experiments using predicted predicates, we compare to He et al. (2018) and the ensemble model (PoE) from , as well as a version of our own self-attention model which does not incorporate syntactic information (SA). To compare to more prior work, we present additional results on CoNLL-2005 with models given gold predicates at test time. In these experiments we also compare to Tan et al. (2018), the previous state-of-the art SRL model using gold predicates and standard embeddings.\nWe demonstrate that our models benefit from injecting state-of-the-art predicted parses at test time (+D&M) by fixing the attention to parses predicted by Dozat and Manning (2017), the winner of the 2017 CoNLL shared task (Zeman et al., 2017) which we re-train using ELMo embeddings. In all cases, using these parses at test time improves performance.\nWe also evaluate our model using the gold syntactic parse at test time (+Gold), to provide an upper bound for the benefit that syntax could have for SRL using LISA. These experiments show that despite LISA's strong performance, there remains substantial room for improvement. In \u00a74.3 we perform further analysis comparing SRL models using gold and predicted parses.   ", "publication_ref": ["b8", "b43", "b41", "b42", "b36", "b21", "b54", "b16", "b61"], "figure_ref": [], "table_ref": []}, {"heading": "Semantic role labeling", "text": "Table 1 lists precision, recall and F1 on the CoNLL-2005 development and test sets using predicted predicates. For models using GloVe embeddings, our syntax-free SA model already achieves a new state-of-the-art by jointly predicting predicates, POS and SRL. LISA with its own parses performs comparably to SA, but when supplied with D&M parses LISA out-performs the previous state-of-the-art by 2.5 F1 points. On the out-ofdomain Brown test set, LISA also performs comparably to its syntax-free counterpart with its own parses, but with D&M parses LISA performs exceptionally well, more than 3.5 F1 points higher than He et al. (2018). Incorporating ELMo em-beddings improves all scores. The gap in SRL F1 between models using LISA and D&M parses is smaller due to LISA's improved parsing accuracy (see \u00a74.2), but LISA with D&M parses still achieves the highest F1: nearly 1.0 absolute F1 higher than the previous state-of-the art on WSJ, and more than 2.0 F1 higher on Brown.\nIn both settings LISA leverages domain-agnostic syntactic information rather than over-fitting to the newswire training data which leads to high performance even on out-of-domain text.\nTo compare to more prior work we also evaluate our models in the artificial setting where gold predicates are provided at test time. For fair comparison we use GloVe embeddings, provide predicate indicator embeddings on the input and reencode the sequence relative to each gold predicate. Here LISA still excels: with D&M parses, LISA out-performs the previous state-of-the-art by more than 2 F1 on both WSJ and Brown.      Here there is little difference between any of the models, with LISA models tending to perform slightly better than SA. Both parsers make mistakes on the majority of sentences (57%), difficult sentences where SA also performs the worst. These examples are likely where gold and D&M parses improve the most over other models in overall F1: Though both parsers fail to correctly parse the entire sentence, the D&M parser is less wrong (87.5 vs. 85.7 average LAS), leading to higher SRL F1 by about 1.5 average F1.\nFollowing , we next apply a series of corrections to model predictions in order to understand which error types the gold parse resolves: e.g. Fix Labels fixes labels on spans matching gold boundaries, and Merge Spans merges adjacent predicted spans into a gold span. 6 In Figure 3 we see that much of the performance gap between the gold and predicted parses is due to span boundary errors (Merge Spans, Split Spans and Fix Span Boundary), which supports the hypothesis proposed by  that incorporating syntax could be particularly helpful for resolving these errors.   that these errors are due mainly to prepositional phrase (PP) attachment mistakes. We also find this to be the case: Figure 4 shows a breakdown of split/merge corrections by phrase type. Though the number of corrections decreases substantially across phrase types, the proportion of corrections attributed to PPs remains the same (approx. 50%) even after providing the correct PP attachment to the model, indicating that PP span boundary mistakes are a fundamental difficulty for SRL.", "publication_ref": ["b21"], "figure_ref": ["fig_1", "fig_2"], "table_ref": ["tab_2"]}, {"heading": "Conclusion", "text": "We present linguistically-informed self-attention: a multi-task neural network model that effectively incorporates rich linguistic information for semantic role labeling. LISA out-performs the state-ofthe-art on two benchmark SRL datasets, including out-of-domain. Future work will explore improving LISA's parsing accuracy, developing better training techniques and adapting to more tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We are grateful to Luheng He for helpful discussions and code, Timothy Dozat for sharing his code, and to the NLP reading groups at Google and UMass and the anonymous reviewers for feedback on drafts of this work. This work was supported in part by an IBM PhD Fellowship Award to E.S., in part by the Center for Intelligent Information Retrieval, and in part by the National Science Foundation under Grant Nos. DMR-1534431 and IIS-1514053. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "journal": "", "year": "2015", "authors": "Mart\u0131n Abadi; Ashish Agarwal; Paul Barham; Eugene Brevdo; Zhifeng Chen; Craig Citro; Greg S Corrado; Andy Davis; Jeffrey Dean; Matthieu Devin"}, {"ref_id": "b1", "title": "When is multitask learning effective? semantic sequence prediction under varying data conditions", "journal": "", "year": "2017", "authors": "Alonso H\u00e9ctor Mart\u00ednez; Barbara Plank"}, {"ref_id": "b2", "title": "Training with exploration improves a greedy stack lstm parser", "journal": "", "year": "2016", "authors": "Miguel Ballesteros; Yoav Goldberg; Chris Dyer; Noah A Smith"}, {"ref_id": "b3", "title": "Semantic roles for string to tree machine translation", "journal": "", "year": "2013", "authors": "Marzieh Bazrafshan; Daniel Gildea"}, {"ref_id": "b4", "title": "Scheduled sampling for sequence prediction with recurrent neural networks", "journal": "", "year": "2015", "authors": "Samy Bengio; Oriol Vinyals; Navdeep Jaitly; Noam Shazeer"}, {"ref_id": "b5", "title": "Learning long-term dependencies with gradient descent is difficult", "journal": "IEEE Transactions on Neural Networks", "year": "1994", "authors": "Yoshua Bengio; Patrice Simard; Paolo Frasconi"}, {"ref_id": "b6", "title": "Modeling biological processes for reading comprehension", "journal": "", "year": "2014", "authors": "Jonathan Berant; Vivek Srikumar; Pei-Chun Chen; Brad Huang; Christopher D Manning; Abby Vander Linden"}, {"ref_id": "b7", "title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "journal": "", "year": "2017", "authors": "Joachim Bingel; Anders S\u00f8gaard"}, {"ref_id": "b8", "title": "Introduction to the conll-2005 shared task: Semantic role labeling", "journal": "", "year": "2005", "authors": "Xavier Carreras; Llu\u00eds M\u00e0rquez"}, {"ref_id": "b9", "title": "Multitask learning: a knowledgebased source of inductive bias", "journal": "", "year": "1993", "authors": "Rich Caruana"}, {"ref_id": "b10", "title": "Learning to search better than your teacher", "journal": "", "year": "2015", "authors": "Kai-Wei Chang; Akshay Krishnamurthy; Alekh Agarwal; Hal Daum\u00e9; Iii ; John Langford"}, {"ref_id": "b11", "title": "Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing", "journal": "", "year": "2013", "authors": "Yun-Nung Chen; William Yang Wang; Alexander I Rudnicky"}, {"ref_id": "b12", "title": "Getting the most out of transition-based dependency parsing", "journal": "", "year": "2011", "authors": "D Jinho; Martha Choi;  Palmer"}, {"ref_id": "b13", "title": "Natural language processing (almost) from scratch", "journal": "Journal of Machine Learning Research", "year": "2011-08", "authors": "Ronan Collobert; Jason Weston; L\u00e9on Bottou; Michael Karlen; Koray Kavukcuoglu; Pavel Kuksa"}, {"ref_id": "b14", "title": "Search-based structured prediction", "journal": "", "year": "2009", "authors": "Hal Daum\u00e9; Iii ; John Langford; Daniel Marcu"}, {"ref_id": "b15", "title": "Incorporating nesterov momentum into adam", "journal": "", "year": "2016", "authors": "Timothy Dozat"}, {"ref_id": "b16", "title": "Deep biaffine attention for neural dependency parsing", "journal": "", "year": "2017", "authors": "Timothy Dozat; Christopher D Manning"}, {"ref_id": "b17", "title": "Semantic role labeling with neural network factors", "journal": "", "year": "2015", "authors": "Nicholas Fitzgerald; Oscar T\u00e4ckstr\u00f6m; Kuzman Ganchev; Dipanjan Das"}, {"ref_id": "b18", "title": "Manual of information to accompany a standard corpus of presentday edited american english, for use with digital computers", "journal": "", "year": "1964", "authors": "W N Francis; H Ku\u010dera"}, {"ref_id": "b19", "title": "A dynamic oracle for arc-eager dependency parsing", "journal": "", "year": "2012", "authors": "Yoav Goldberg; Joakim Nivre"}, {"ref_id": "b20", "title": "A joint many-task model: Growing a neural network for multiple nlp tasks", "journal": "", "year": "2017", "authors": "Kazuma Hashimoto; Caiming Xiong; Yoshimasa Tsuruoka; Richard Socher"}, {"ref_id": "b21", "title": "Jointly predicting predicates and arguments in neural semantic role labeling", "journal": "", "year": "2018", "authors": "Luheng He; Kenton Lee; Omer Levy; Luke Zettlemoyer"}, {"ref_id": "b22", "title": "Deep semantic role labeling: What works and whats next", "journal": "", "year": "2017", "authors": "Luheng He; Kenton Lee; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b23", "title": "Dependency-based semantic role labeling of propbank", "journal": "", "year": "2008", "authors": "Richard Johansson; Pierre Nugues"}, {"ref_id": "b24", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "Diederik Kingma; Jimmy Ba"}, {"ref_id": "b25", "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"ref_id": "b26", "title": "Semisupervised classification with graph convolutional networks", "journal": "", "year": "2017", "authors": "N Thomas; Max Kipf;  Welling"}, {"ref_id": "b27", "title": "End-to-end neural coreference resolution", "journal": "", "year": "2017", "authors": "Kenton Lee; Luheng He; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b28", "title": "English verb classes and alternations: A preliminary investigation", "journal": "University of Chicago press", "year": "1993", "authors": "Beth Levin"}, {"ref_id": "b29", "title": "Joint A* CCG Parsing and Semantic Role Labeling", "journal": "", "year": "2015", "authors": "Mike Lewis; Luheng He; Luke Zettlemoyer"}, {"ref_id": "b30", "title": "Semantic role features for machine translation", "journal": "", "year": "2010", "authors": "Ding Liu; Daniel Gildea"}, {"ref_id": "b31", "title": "Learning structured text representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Yang Liu; Mirella Lapata"}, {"ref_id": "b32", "title": "Rectifier nonlinearities improve neural network acoustic models", "journal": "", "year": "2013", "authors": "Andrew L Maas; Awni Y Hannun; Andrew Y Ng"}, {"ref_id": "b33", "title": "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling", "journal": "", "year": "2017", "authors": "Diego Marcheggiani; Anton Frolov; Ivan Titov"}, {"ref_id": "b34", "title": "Encoding sentences with graph convolutional networks for semantic role labeling", "journal": "EMNLP", "year": "2017", "authors": "Diego Marcheggiani; Ivan Titov"}, {"ref_id": "b35", "title": "Building a large annotated corpus of English: The Penn TreeBank", "journal": "Computational Linguistics -Special issue on using large corpora: II", "year": "1993", "authors": "Mitchell P Marcus; Mary Ann Marcinkiewicz; Beatrice Santorini"}, {"ref_id": "b36", "title": "The stanford typed dependencies representation", "journal": "", "year": "2008", "authors": "Marie-Catherine De Marneffe; Christopher D Manning"}, {"ref_id": "b37", "title": "A method of solving a convex programming problem with convergence rate o(1/k 2 )", "journal": "", "year": "1983", "authors": "Yurii Nesterov"}, {"ref_id": "b38", "title": "The proposition bank: An annotated corpus of semantic roles", "journal": "Computational Linguistics", "year": "2005", "authors": "Martha Palmer; Daniel Gildea; Paul Kingsbury"}, {"ref_id": "b39", "title": "On the difficulty of training recurrent neural networks", "journal": "", "year": "2013", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio"}, {"ref_id": "b40", "title": "Deep multitask learning for semantic dependency parsing", "journal": "", "year": "2017", "authors": "Hao Peng; Sam Thomson; Noah A Smith"}, {"ref_id": "b41", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b42", "title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b43", "title": "Towards robust linguistic analysis using ontonotes", "journal": "", "year": "2006", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Hwee Tou Xue; Anders Ng; Olga Bj\u00f6rkelund; Yuchen Uryupina; Zhi Zhang;  Zhong"}, {"ref_id": "b44", "title": "Semantic role labeling using different syntactic views", "journal": "", "year": "2005", "authors": "Sameer Pradhan; Wayne Ward; Kadri Hacioglu; James Martin; Dan Jurafsky"}, {"ref_id": "b45", "title": "The importance of syntactic parsing and inference in semantic role labeling", "journal": "Computational Linguistics", "year": "2008", "authors": "Vasin Punyakanok; Dan Roth; Wen-Tau Yih"}, {"ref_id": "b46", "title": "A reduction of imitation learning and structured prediction to no-regret online learning", "journal": "", "year": "2011", "authors": "St\u00e9phane Ross; Geoffrey J Gordon; J Andrew Bagnell"}, {"ref_id": "b47", "title": "Neural semantic role labeling with dependency path embeddings", "journal": "", "year": "2016", "authors": "Michael Roth; Mirella Lapata"}, {"ref_id": "b48", "title": "Deep multi-task learning with low level tasks supervised at lower layers", "journal": "", "year": "2016", "authors": "Anders S\u00f8gaard; Yoav Goldberg"}, {"ref_id": "b49", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "Journal of machine learning research", "year": "2014", "authors": "Nitish Srivastava; Geoffrey E Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b50", "title": "Combination strategies for semantic role labeling", "journal": "Journal of Artificial Intelligence Research", "year": "2007", "authors": "Mihai Surdeanu; Llu\u00eds M\u00e0rquez; Xavier Carreras; Pere R Comas"}, {"ref_id": "b51", "title": "Joint parsing and semantic role labeling", "journal": "", "year": "2005", "authors": "Charles Sutton; Andrew Mccallum"}, {"ref_id": "b52", "title": "Frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaffold", "journal": "", "year": "2017", "authors": "Swabha Swayamdipta; Sam Thomson; Chris Dyer; Noah A Smith"}, {"ref_id": "b53", "title": "Efficient inference and structured learning for semantic role labeling", "journal": "TACL", "year": "2015", "authors": "Oscar T\u00e4ckstr\u00f6m; Kuzman Ganchev; Dipanjan Das"}, {"ref_id": "b54", "title": "Deep semantic role labeling with self-attention", "journal": "", "year": "2018", "authors": "Zhixing Tan; Mingxuan Wang; Jun Xie; Yidong Chen; Xiaodong Shi"}, {"ref_id": "b55", "title": "A global joint model for semantic role labeling", "journal": "Computational Linguistics", "year": "2008", "authors": "Kristina Toutanova; Aria Haghighi; Christopher D Manning"}, {"ref_id": "b56", "title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "journal": "", "year": "2003", "authors": "Kristina Toutanova; Dan Klein; D Christopher; Yoram Manning;  Singer"}, {"ref_id": "b57", "title": "Semi-supervised learning for spoken language understanding using semantic role labeling", "journal": "", "year": "2005", "authors": "Gokhan Tur; Dilek Hakkani-T\u00fcr; Ananlada Chotimongkol"}, {"ref_id": "b58", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b59", "title": "Machine comprehension with syntax, frames, and semantics", "journal": "", "year": "2015", "authors": "Hai Wang; Mohit Bansal; Kevin Gimpel; David Mcallester"}, {"ref_id": "b60", "title": "A learning algorithm for continually running fully recurrent neural networks", "journal": "Neural computation", "year": "1989", "authors": "R J Williams; D Zipser"}, {"ref_id": "b61", "title": "Conll 2017 shared task: Multilingual parsing from raw text to universal dependencies", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Daniel Zeman; Martin Popel; Milan Straka; Jan Hajic; Joakim Nivre; Filip Ginter; Juhani Luotolahti; Sampo Pyysalo; Slav Petrov; Martin Potthast"}, {"ref_id": "b62", "title": "Stackpropagation: Improved representation learning for syntax", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Yuan Zhang; David Weiss"}, {"ref_id": "b63", "title": "End-to-end learning of semantic role labeling using recurrent neural networks", "journal": "", "year": "2015", "authors": "Jie Zhou; Wei Xu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "all use variants of deep LSTMs with constrained decoding, while Tan et al. (2018) apply self-attention to obtain state-ofthe-art SRL with gold predicates. Like this work, He et al. (2017) present end-to-end experiments, predicting predicates using an LSTM, and He et al.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Performance of CoNLL-2005 models after performing corrections from He et al. (2017).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Percent and count of split/merge corrections performed in Figure 3, by phrase type.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Precision, recall and F1 on the CoNLL-2005 development and test sets.", "figure_data": "WSJ TestPRF1He et al. (2018) 84.283.783.9Tan et al. (2018) 84.585.284.8SA84.784.24 84.47LISA84.72 84.57 84.64+D&M86.02 86.05 86.04Brown TestPRF1He et al. (2018) 74.273.173.7Tan et al. (2018) 73.574.674.1SA73.89 72.39 73.13LISA74.77 74.32 74.55+D&M76.65 76.44 76.54"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Precision, recall and F1 on CoNLL-2005   with gold predicates.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "reports precision, recall and F1 onthe CoNLL-2012 test set. We observe perfor-mance similar to that observed on ConLL-2005:Using GloVe embeddings our SA baseline al-ready out-performs He et al. (2018) by nearly1.5 F1. With its own parses, LISA slightlyunder-performs our syntax-free model, but whenprovided with stronger D&M parses LISA out-performs the state-of-the-art by more than 2.5F1. Like CoNLL-2005, ELMo representations im-prove all models and close the F1 gap betweenmodels supplied with LISA and D&M parses. Onthis dataset ELMo also substantially narrows the"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "-96.48 94.40 LISA G 96.92 94.92 91.87 LISA E 97.80 96.28 93.65 Brown D&M E -92.56 88.52 LISA G 94.26 90.31 85.82 LISA E 95.77 93.36 88.75", "figure_data": ": Precision, recall and F1 on the CoNLL-2012 development and test sets. Italics indicatea synthetic upper bound obtained by providing agold parse at test time.difference between models with-and without syn-tactic information. This suggests that for this chal-lenging dataset, ELMo already encodes much ofthe information available in the D&M parses. Yet,higher accuracy parses could still yield improve-ments since providing gold parses increases F1 by4 points even with ELMo embeddings.4.2 Parsing, POS and predicate detectionWe first report the labeled and unlabeled attach-ment scores (LAS, UAS) of our parsing models onthe CoNLL-2005 and 2012 test sets (Table 4) withGloVe (G) and ELMo (E) embeddings. D&Machieves the best scores. Still, LISA's GloVeUAS is comparable to popular off-the-shelf de-pendency parsers such as spaCy, 5 and with ELMo"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Parsing (labeled and unlabeled attachment)  and POS accuracies attained by the models used in SRL experiments on test datasets. Subscript G denotes GloVe and E ELMo embeddings.", "figure_data": "ModelPRF1WSJHe et al. (2017) 94.5 98.5 96.4 LISA 98.9 97.9 98.4BrownHe et al. (2017) 89.3 95.7 92.4 LISA 95.5 91.9 93.7CoNLL-12 LISA99.8 94.7 97.2"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Predicate detection precision, recall and F1 on CoNLL-2005 and CoNLL-2012 test sets. embeddings comparable to the standalone D&M parser. The difference in parse accuracy between LISA G and D&M likely explains the large increase in SRL performance we see from decoding with D&M parses in that setting. In Table 5 we present predicate detection precision, recall and F1 on the CoNLL-2005 and 2012 test sets. SA and LISA with and without ELMo attain comparable scores so we report only LISA+GloVe. We compare to He et al. (2017) on CoNLL-2005, the only cited work reporting comparable predicate detection F1. LISA attains high predicate detection scores, above 97 F1, on both in-domain datasets, and out-performs He et al. (2017) by 1.5-2 F1 points even on the out-ofdomain Brown test set, suggesting that multi-task learning works well for SRL predicate detection.4.3 AnalysisFirst we assess SRL F1 on sentences divided by parse accuracy. Table6lists average SRL F1 (across sentences) for the four conditions of LISA and D&M parses being correct or not (L\u00b1, D\u00b1). Both parsers are correct on 26% of sentences.", "figure_data": "facts-figures"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Average SRL F1 on CoNLL-2005 for sentences where LISA (L) and D&M (D) parses were completely correct (+) or incorrect (-).", "figure_data": "100.097.595.0F192.5 90.0 87.5SA LISA +D&M85.0+GoldOrig.FixMoveMergeSplitFixDropAddLabelsCoreSpansSpansSpanArg.Arg.Arg.Boundary"}], "formulas": [{"formula_id": "formula_0", "formula_text": "B-ARG0 B-V B-ARG1 I-ARG1 I-ARG1 O O B-ARG0 I-ARG0 B-V", "formula_coordinates": [2.0, 370.42, 68.55, 116.74, 14.25]}, {"formula_id": "formula_1", "formula_text": "(t = 3) MatMul: Concat + FF sloth (i+1) + A[t] parse M [t] parse A i h V i h A[t] parse A[t] parse M i 0 [t] M i [t] M i 2 [t] A i 2 [t] A i 1 [t] A i 0 [t]", "formula_coordinates": [3.0, 135.47, 64.44, 104.3, 185.92]}, {"formula_id": "formula_2", "formula_text": "(p) t : s (j) t = LN (s (j 1) t + T (j) (s (j 1) t ))", "formula_coordinates": [3.0, 340.92, 391.86, 178.49, 40.77]}, {"formula_id": "formula_3", "formula_text": "(j) h , V (j) h and Q (j)", "formula_coordinates": [3.0, 307.28, 620.37, 217.77, 30.37]}, {"formula_id": "formula_4", "formula_text": "(j) h by K (j) h", "formula_coordinates": [3.0, 475.52, 652.87, 49.52, 16.57]}, {"formula_id": "formula_5", "formula_text": "A (j) h = softmax(d 0.5 k Q (j) h K (j) h T )", "formula_coordinates": [4.0, 105.83, 113.78, 150.6, 19.4]}, {"formula_id": "formula_6", "formula_text": "V (j)", "formula_coordinates": [4.0, 72.0, 156.9, 19.26, 13.79]}, {"formula_id": "formula_7", "formula_text": "(j) h : M (j) h = A (j) h V (j) h (3) Row t of M (j)", "formula_coordinates": [4.0, 72.0, 173.15, 218.27, 70.48]}, {"formula_id": "formula_8", "formula_text": "(j) h ) over the token representations in V (j) h .", "formula_coordinates": [4.0, 72.0, 258.13, 218.26, 32.82]}, {"formula_id": "formula_9", "formula_text": "A parse = softmax(Q parse U heads K T parse ) (4)", "formula_coordinates": [4.0, 84.9, 748.91, 205.37, 14.5]}, {"formula_id": "formula_10", "formula_text": "P (q = head(t) | X ) = A parse [t, q]", "formula_coordinates": [4.0, 339.35, 213.16, 154.12, 17.98]}, {"formula_id": "formula_11", "formula_text": "(J) t", "formula_coordinates": [5.0, 277.81, 388.69, 11.96, 15.94]}, {"formula_id": "formula_12", "formula_text": "s ft = (s pred f ) T Us role t (6", "formula_coordinates": [5.0, 133.93, 496.13, 152.1, 16.13]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 286.03, 499.45, 4.24, 10.36]}, {"formula_id": "formula_14", "formula_text": "1 T T X t=1 h F X f =1 log P (y role ft | P G , V G , X ) + log P (y prp t | X ) + 1 log P (head(t) | X ) + 2 log P (y dep t | P G , X ) i (7", "formula_coordinates": [5.0, 335.25, 129.7, 186.06, 96.66]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [5.0, 521.3, 208.59, 4.24, 10.36]}], "doi": ""}