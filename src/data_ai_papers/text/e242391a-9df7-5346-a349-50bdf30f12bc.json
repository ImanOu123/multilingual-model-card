{"title": "SITUATEDQA: Incorporating Extra-Linguistic Contexts into QA", "authors": "Michael J Q Zhang; Eunsol Choi", "pub_date": "", "abstract": "Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SITUATEDQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SITUATEDQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https: //situatedqa.github.io/.", "sections": [{"heading": "Introduction", "text": "Language reflects our ever-changing world; thus, the meaning of a sentence depends on a variety of extra-linguistic contexts, including when it was stated, who stated it, and where it was stated. Figure 1 depicts examples of this, where the answer to a question varies based on the temporal and geographical contexts. In this work, we study such questions by exploring the affects of temporal and geographical contexts in open-retrieval QA.\nIdeally, QA systems should provide up-to-date, localized answers to questions like those in Figure 1 that are situated in the inquirer's temporal and geographical contexts. The existing paradigm for evaluating QA systems, however, makes implicit assumptions about time and location and does not measure how well QA models adapt to new contexts. Recent work studying other NLP tasks have shown that this static paradigm misrepresents how models perform in practical settings (Osborne et al., 2014;Wang et al., 2008;Huang and Paul, 2018;Rijhwani and Preotiuc-Pietro, 2020). To address this challenge, we introduce SITUATEDQA, a dataset comprised of questions from existing openretrieval QA datasets (Kwiatkowski et al., 2019) that have been re-annotated for their their contextdependence and answers across different situations (i.e., temporal or geographical contexts). 1 As we will see in Section 3, a significant pro-  portion of information-seeking questions are sensitive to the two extra-linguistic contexts we study. We annotate 9K questions from four existing datasets (Kwiatkowski et al., 2019;Berant et al., 2013;Clark et al., 2020;Campos et al., 2016) with their temporal dependence and 2K questions for their geographical dependence. Annotators find that up to 30% of questions in existing datasets have answers that change over time, and rule-based heuristics using an NER tagger identify that about 5% of questions specify a location. We collect answers from different temporal and geographic contexts for such context-dependent questions (see Table 1).\nUsing our collected data, we evaluate whether existing open-retrieval QA systems adapt to new temporal or geographical contexts by providing answers that are up-to-date or from new locations. While it is often assumed that retrieval based systems for QA (Karpukhin et al., 2020;Guu et al., 2020) can adapt to updated facts during inference by simply updating the retrieval corpus, we find that this is not the case. State-of-the-art retrieval based systems with updated corpora are 15 percentage points less accurate on questions whose answers have been updated versus those that have remained constant since the time when their largescale training dataset was collected. We also observe that models fail to generalize to answering questions from new locations (Shankar et al., 2017), with accuracy dropping by 10 percentage points when asked to provide the answer from a rare location versus a common one.\nTo support future research developing openretrieval QA systems which situate questions within the inquirer's extra-linguistic contexts, we propose two tasks for modeling what facts change across different extra-linguistic contexts and how those facts change. We also provide fine-grained evaluations for measuring how well models adapt to new temporal and geographical contexts. We establish initial performance levels on our tasks by adapting state-of-the-art methods for open-retrieval QA (Karpukhin et al., 2020;Lewis et al., 2020a;. Finally, we provide rich analysis of existing QA systems, which suggest that benchmark construction should incorporate extralinguistic contexts to remain relevant globally and in the future.", "publication_ref": ["b33", "b53", "b17", "b45", "b23", "b23", "b0", "b7", "b2", "b19", "b15", "b49", "b19", "b26"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_1"]}, {"heading": "Definitions & Tasks", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Defining Extra-Linguistic Contexts", "text": "We begin by defining the scope of contexts studied in this work. For a given question q, we say that a i is its answer when asked in the context c i . Each context consists of a type c t i and a value c v i . We study two context types: temporal (TEMP) and geographical (GEO). TEMP defines each context value as timestamp (e.g. a date or year) where a i is the answer to q if it was asked at the time of c v i . GEO defines each context value as a geopolitical entity where a i is the answer to the q in the location c v i . See Table 1 for examples from each context type. We limit our study to valid questions, ignoring presupposition cases (Kim et al., 2021) (e.g., asking who is the CEO of Google before Google was founded).", "publication_ref": ["b21"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Situated Question Answering", "text": "Given a question q and context c i , the task is to produce the corresponding answer a i for the provided context. This task requires models to situate the question within the provided extra-linguistic context to produce an appropriate answer. Models are evaluated on exact string match with annotated answer a i and predicted answer\u00e2 i after minor normalization (Rajpurkar et al., 2016).\nBy evaluating on different sets of extra-linguistic contexts of the same question, we can measure whether models are able to generalize to new contexts. For instance, we can evaluate how models perform on commonly versus rarely asked about locations, or on questions whose answers changed recently or long ago. Furthermore, this task allows us to train systems that explicitly model how facts change across different extra-linguistic contexts.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Context-Dependent Question Identification", "text": "In context-dependent question identification, we evaluate whether models can determine whether the answer to a given question depends on its extra-linguistic context. More formally, given a question q and context type c t , models must determine whether there exists two distinct contexts values, (c v i , c v j ), with different respective answers, a i = a j . We cast this as binary classification and evaluate models on their classification accuracy, F1, precision, and recall.\nIdentifying what facts change depending on the extra-linguistic contexts is nontrivial even for human annotators that often requires extensive background knowledge on a subject. For example, determining whether the capital of Kazakhstan has changed requires specific knowledge of the nation's history. Identifying these questions has many practical applications, such as identifying suitable questions for QA systems that can only provide static answers, an idea we will explore later in Section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Collection", "text": "Here we describe the process for identifying context-dependent questions and collecting answers from alternate contexts. We split up data collection into three stages which are depicted in Figure 2: identification, {Context / Answer} collection, and validation. Additional data collection details with interface screenshots can be found in Appendix A.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Identification", "text": "We source questions from a variety of datasets for open-retrieval QA: Natural Questions (NQ-Open) (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013), TyDi-QA (Clark et al., 2020), and MS- MARCO (Campos et al., 2016). All of these datasets are in English and contain questions that are answerable by Wikipedia documents.\nTemporally dependent questions are abundant in existing datasets, as they are unambiguous during annotation and annotators simply provide the current answer at the time of collection. In contrast, geographically dependent questions are often rejected during annotation due to their ambiguity, as there is no consensus among annotators on the assumed geographical context. To correct for this scarcity, we generate geographically dependent questions by modifying existing questions from NQ-Open. We identify questions with phrases that specify a location by running an NER tagger  and remove the location entity using heuristics based on its syntactic role as identified by a dependency parser (Dozat and Manning, 2017). See Appendix A for full details.\nFor each question, we collect 3-way yes / maybe / no annotations for whether they are temporally or geographically dependent. We label questions as context dependent if at least two of annotators label so. We discard questions with split annotations and majority maybe annotations, and map the labels to a binary yes/no label. 2", "publication_ref": ["b23", "b0", "b7", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "{Context / Answer} Collection", "text": "After identifying context-dependent questions, we then move on to collecting multiple {Context / An-swer} pairs via crowdsourcing. For this part, we exclusively use questions from NQ-Open dataset. We allow crowdworkers to query an up-to-date version of English Wikipedia using the Google Search API. 3 We outline the annotation process for each context type below:\nTEMP To construct SITUATEDQA examples with temporal context, we take two steps: (1) crowd sourcing timeline for each question and (2) generating (q, c v , a) from the annotated timeline.\nCrowdworkers are asked to provide a brief timeline of answers for a given question, consisting of the current answer, the previous answer, as well as the start and end transition timestamps for each answer given as dates or years (see Figure 2). From annotated timeline and query pairs, we construct examples of (q, c v , a) pairs in one of three ways Figure 2: Data collection pipeline: Crowdworkers are first asked to identify context dependent questions. We then collect brief answer timelines for temporally dependent questions and location/answer pairs geographically dependent questions, each of which is then verified by another worker.\nwhich we refer to as Start, Sampled, and Static. We describe each of these methods below:\nStart examples simply use each answer's start transition timestamp as c v . This is intended to simulate the common scenario of asking information that is new or has recently changed. Sampled examples use a timestamp that lies between an answer's start and end transition as c v . As each answer in a timeline can result in many valid (q, c v , a) pairs, we limit the number of values of c v for each answer to a maximum of two. We uniformly sample up to two timestamps between each answers start and end transition, using each sampled timestamp to create a new (q, c v , a) triple. Static examples utilize questions that were annotated as not temporally-dependent to simulate realistic settings where context dependent and independent questions coexist. For each static question, we uniformly sample a single value of c v , resulting in one (q, c v , a) triple per static question.\nGEO We construct our evaluation datasets with crowdsourcing. For each stripped question that was annotated as geographically dependent during the identification stage, annotators were presented with the question, the original answer(s), and stripped location. Annotators first validate whether the original location and answer pair is correct for the stripped question before identifying up to two additional location and answer pairs. We then use each validated or identified pair to construct a (q, c v , a) triple for the stripped question.\nWe construct our training set via distant supervision. Using our best performing context-dependent question identification model which we introduce in Section 4, we classify unlabeled examples from the NQ-Open training set. For each example that was classified as geographically-dependent, we use its original answer, location, and stripped question to construct a (q, c v , a) triple.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quality Control", "text": "Validation The {Context / Answer} annotations collected above are reviewed by another annotator in a final validation stage. Presented with a question along with all answer timelines or location/answer pairs that have been generated by other workers, annotators are asked to validate or revise each response by marking them as correct or incorrect. We collect 2-way {Context / Answer} annotations and have one validatator for each question from the development and test sets. We collect a single {Context / Answer} annotation and skip validation for answer timelines from the training set.\nWorker Qualifications / Inter-annotator agreement Table 2 reports Fleiss's kappa for the context dependent question identification and validation stages. We find moderate to high agreement all tasks, with lower agreement on datasets with highly ambiguous queries (e.g. how much to trust friends) or questions require extensive domain knowledge (e.g., what is the cause of smog in china?). For geographically dependent question answering, questions like \"when were electric trains introduced\" had split votes from the annotators, as both geographically dependent and independent interpretations are valid. Overall, we observe high quality data with comparable agreement numbers with prior work, yet reaffirming prevalent ambiguity in open retrieval QA (Min et al., 2021).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Data Statistics / Analysis", "text": "Table 2 shows the statistics of our collected dataset. We annotated over 11K questions, and roughly 30-40% of them where identified as context-dependent. Temporally-dependent questions comprised of least  How often do temporally dependent facts change? We investigate how frequently answers change by measuring the distance between the the two most recent answers. Figure 3 depicts how long the previous answer was valid for. We observe a long tailed distribution, with a large proportion of answers changing around the one year mark.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Models", "text": "In this section, we describe our baseline models for context-dependent question identification and situated QA. Details on our learning settings and hyperparameters are in Appendix D.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Context Dependent Question Identification", "text": "As a lower bound, we provide a naive randomchoice baseline, where labels are randomly assigned to match the label distribution in the training data. We also train separate BERT-based classifiers, which encode the input question q using BERT   query for the answer for some context where c t = TEMP or c t = GEO, we append the phrase \"as of c v \" or \"in c v \", replacing c v with the context value. (Lewis et al., 2020a). 4 Both baselines are trained on NQ-Open (Kwiatkowski et al., 2019;. We use the best performing model configuration from DPR paper, but swap the retrieval corpus with an up-to-date Wikipedia dump (timestamped 2021-Feb-20). In standard settings, DPR achieves an accuracy of 41.5 and BART achieves an accuracy of 24.1 on NQ-Open. We use these baselines in three settings which we introduce below: unmodified, query modified, and query modified with finetuning.\nWe also approximate human performance on situated QA using 100 randomly sampled examples from each context type which are annotated by the authors of this paper. While our estimated human performance appear somewhat low, they are in line with agreement rates in the original Natural Questions study (Kwiatkowski et al., 2019) and are tied to the challenges of evaluating open-retrieval QA. Discrepancies are often due to ambiguous questions or equivalent answer forms (e.g., July 5, 1945 vs. 5 July 1945, The Patriots vs. New England Patriots). The latter is especially problematic for geographically dependent questions, where answers involving time expression occur much more frequently, comprising over 50% of examples. See Appendix A for more details.", "publication_ref": ["b26", "b23", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Unmodified Baseline", "text": "We evaluate openretrieval QA models on our task without any alterations. Each model is only given the question, ignoring the extra-linguistic context, and thus predicts the same answer no matter the context. For temporally-dependent questions, models are expected to output the most up-to-date answer. For geographically-dependent questions, models must assume some geographical context to produce an answer. 4 We use the pretrained model and implementation from https://github.com/shmsw25/ bart-closed-book-qa.\nQuery Modified One simple method for incorporating extra-linguistic contexts into question answering is to concatenate the context onto the question, separated by some special token. We adopt a slightly different approach that leverages what models have already learned about producing answers from specific contexts: concatenating a phrase that specifies the relevant context onto each question, transforming it into a context-independent question. The concatenation templates for each context type are described in Table 3. We find that these simple modifications mostly generate valid, fluent questions that closely resemble examples found in our models' training data. We estimate that 10% of questions in NQ-Open contain similar augmentations, inserting or concatenating a phrase that specifies the context (See Appendix B for details).\nQuery Modified w/ Finetuning We also experiment with finetuning our query modified baselines. For our retrieval based approach (DPR), we finetune separate the reader and retriever models for each context type. We finetune retriever models using gold and hard-negative passages from the retrieval results of our Query Modified DPR baseline. Likewise, we also finetune separate closed book models for each context type.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Context Dependent Question Identification", "text": "Table 4 reports our results on context dependent question identification. We find that pretrained language models perform competitively in this binary classification task, matching human agreements. Thus, it may be a useful tool for identifying context dependent questions which closed book systems are poorly suited for or identifying examples in benchmark datasets that require re-annotation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Situated Question Answering", "text": "Table 5 shows our results on the situated QA task. All models lag behind our estimated human-level performance by a significant margin, especially for temporally dependent questions. Overall, retrieval based models outperform closed book models, as in the original QA datasets (41.5 EM vs. 24.1 EM). We find that applying our query modifications to the question works well for geographically dependent questions, but see no improvement for temporally dependent questions. Finetuning   Table 6: Breakdown of errors from context dependent QA. We report EM accuracy evaluated against answers from the correct context (One) and answers from the union of all our annotated contexts (Any).\nmodels on answers from multiple extra-linguistic contexts with modified questions improved performance across the board, especially for the TEMP context type.\nAdding TEMP context to static questions distracts the model, significantly decreasing performance especially for the closed book baseline. This suggests that models are not robust to semantically equivalent edits, as observed in Ribeiro et al. (2020). Models also perform better when provided with an answer's start date as context compared to a sampled date, which falls between the transitions between answers. This gap is especially pronounced for our retrieval based model.  We further break down performance common locations by separating the two most common ones, the \"United States\" and \"India\", the \"Other\" common locations.\nDo systems performance vary based on the frequency of geographic context? Table 5 splits results on GEO questions into those from common and rare locations, many of which unseen during training. Both methods have greater difficulty with rare locations; however, the drop in performance for closed book models is significantly worse. Closed book models perform 10 percentage points worse on questions from rare locations while retrieval based models only perform 3 points worse. These findings show that retrieval based systems generalize better to new locations and are better equipped for answering geographically dependent questions. We also see that the gap in performance between common and rare locations shrinks for both models after finetuning, suggesting that explicitly modeling geographical contexts helps with generalization.  What is the assumed context for a geographically dependent question? When presented with a geographically dependent question without the context, open retrieval QA models must assume some geographical context to produce an answer. We hypothesize that models will exhibit bias toward assuming the few geographical co-texts are most frequently asked about. We test this by reporting the results of our unmodified baseline, further breaking down the results from Table 5 by individual location. Each time the unmodified baseline correctly answers a question for some location, it did so by assuming that location as its context. We report our results in In Table 7, which show that models are heavily biased toward assuming the question was asked by someone in the United States or India.\nHow often do models provide answers from the wrong context? In Table 6, we report error analysis for our situated QA baselines that shows how often models are provide the answer from the specified context versus the union of all annotated contexts. We see that models often fail to incorporate extra-linguistic contexts into the question, producing the answer from another context.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8", "tab_8", "tab_8", "tab_10"]}, {"heading": "Analysis", "text": "Can QA systems differentiate and recall previous answers to a question? To answer this, we study a new setting where we query models for the current and previous answer to a given question. Following the same steps as above for the TEMP and GEO context types, we develop a suite of retrieval-based and closed-book baselines for this new setting. Our query-modified baselines in this setting consist of prepending the word \"previously\" to the beginning of each question to query for the  previous answer. In the existing framing of QA, all questions are looking for the current answer we do no augmentations to query for the current answer. For our finetuned baselines, we train separate reader models and closed book models to query for current and previous answers. We report our results for these experiments in Table 8. We find that models are far better at producing the current answer versus past answers to a question. Finetuning, however, greatly increases performance on queries for previous answers.\nCan QA systems trained on outdated answers adapt to the present? We investigate whether QA systems that are pretrained on outdated corpora and that are trained on large-scale QA datasets containing outdated answers can adapt to answering questions situated in the present. Lewis et al. (2020b) shows that updating the corpera of retrieval based models can be effective method for updating simple facts, such as current heads of state. We test whether this extends to a broader range of temporally-dependent facts by evaluating our baselines introduced above on their ability to predict the current answer in our dataset, which are up-to-date as of Feb 2021. We split the current answers into two categories, stable where answer did not change since 2018, and updated where answer changed after 2018. We chose 2018 as a threshold as it is when Natural Questions (Kwiatkowski et al., 2019) was collected and roughly matches the timestamp of the most recent data our closed book model (Lewis et al., 2020a) was pretrained on (Feb 2019).\nTable 9 shows that, while retrieval based models were able to update some world knowledge after swapping the retrieval corpus, significant gains on updated questions only come after finetuning with newer data. This suggests that simply updating the corpora that models retrieve passages from is not sufficient to keep models up-to-date.", "publication_ref": ["b27", "b23", "b26"], "figure_ref": [], "table_ref": ["tab_12", "tab_14"]}, {"heading": "Related Work", "text": "Comparison to other QA datasets While a few prior studies mention that answers to a question can change over time , no prior work investigated the performance of existing models on temporally dependent queries and their prevalence. Addressing ambiguous questions  overlaps with our study, as ambiguity can arise from extra-linguistic contexts. Their study found 13% of examples were ambiguous due to temporal deixis. However, we found that contextdependence can co-occur with inherent ambiguities in the question which cannot be resolved with context. Thus, this figure underestimates the true proportion of temporally dependent questions. Furthermore, humans don't always consider contextdependent questions as ambiguous when there is only one answer given the context, which is normally assumed to be present time and location. Thus, ambiguities that arise due to lack of context should, therefore, be modeled separately from semantic ambiguities of the question.\nRecent work has also studied generating temporally-dependent questions with timestamped answers from temporal knowledge-bases (Chen et al., 2021;Saxena et al., 2021). Similarly, Dhingra et al. (2021) generates temporally-dependent cloze-style prompts and present a temporally aware language model to address them. These works, however, synthetically generate examples and are therefore limited in scope and diversity. In contrast, we manually annotate temporally and geographically dependent questions.\nTemporal understanding Temporal understanding in NLP has been typically studied in an intradocument setting, i.e., ordering events or finding temporal relationship between two events in the same document (Pustejovsky et al., 2003;Bethard et al., 2007;Cassidy et al., 2014;Llorens et al., 2015;. Dynamic evaluation Dynamic evaluation has been studied under the language modeling (Osborne et al., 2014;Yogatama et al., 2014), topic modeling (Wang et al., 2008) and entity linking (Rijhwani and Preotiuc-Pietro, 2020). Recent work (Lazaridou et al., 2021) studied temporal drift in the context of large scale pretrained language model, hinting reusing models from previous snapshot can cause performance decay, as we observe in our study as well.\nAdversarial, phased data collection has been proposed (Paperno et al., 2016;Zellers et al., 2018;Potts et al., 2020) to drive model development, constantly feeding models examples that current models are unable to address. We suggest collecting questions dynamically for slightly different goal, to accurately reflect the changing world by identifying temporally dependent facts from such benchmarks and continuously updating them. Building a benchmark based on a fixed snapshot (Petroni et al., 2021) can be a viable alternative.\nResearchers studied keeping knowledge sources up-to-date. Konovalov et al. (2017) looked at the constantly changing facts from the perspective of automatically extracting knowledgebase revisions. Schuster et al. (2021) presents contrastive sentences based on Wikipedia revisions, showing how entailment decision can change over time.", "publication_ref": ["b5", "b47", "b9", "b40", "b1", "b3", "b30", "b33", "b55", "b53", "b45", "b34", "b56", "b39", "b22", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion & Future Work", "text": "We present the first study of how extra-linguistic contexts affect open retrieval QA. Our study reveals that current systems fail to adapt to shifts in the temporal or geographical context. We, therefore, propose tasks and create a dataset for training and evaluating QA systems on modeling how facts change across contexts. Our dataset will support ample future work for developing models which can gracefully update its predictions based on new temporal and geographical contexts. Future research may address incorporating in temporally and geographically dependent source documents, such as news articles, or considering other extralinguistic contexts such as who is asking the question, taking individual's preferences into account.  addition to allowing annotators to search over English Wikipedia, we also provide a list of articles that were used by workers in the prior stage. In open retrieval setting, the same question may have multiple interpretations, and therefore we ask annotators to mark answers as correct if they consistent with one plausible interpretation. Figure 4, 5, 6, 7, 8 shows our annotation interface.  Table 13: Retrieval performance on situated question answering, measured by answer recall within the top 50 retrieved documents. We also report performance for partitions of the test set, was well as recall measured against the union of all answers from all annotated contexts (Any).  distribution we find in our training set annotations.\nInter Annotator Agreement Inter annotator agreement during identification phase can be found in Table 12. At the validation phase, 70% of temporal question-context-answer pairs were annotated as correct and 85% of geographical question-context-answer pairs were annotated as correct, similar to validation phase (76%) in Am-bigQA .\nAgreement with Natural Questions The answer span exact match between our collected answers and the original answers from NQ where the annotated start and end transition dates overlap with NQ's creation (2018) was around 40%, similar to the average agreement rate on NQ-Open test data is 49.2% in the original study (Kwiatkowski et al., 2019). Our manual analysis on randomly sampled 50 errors and find that only 12% of the errors can be attributed to annotation errors. 70% of other errors are due to ambiguities in the question resulting in multiple possible answers (e.g. \"Three movies made from Agatha Christie's novels\"), or the same answer being given in different forms (e.g. \"Chamberlain, Wilt\" vs. \"Wilt Chamberlain\"). The remaining errors are the result of inconsistencies in NQ annotations (18%).", "publication_ref": ["b23"], "figure_ref": ["fig_1"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "B Data Analysis", "text": "Naturally occurring query modifications In open retrieval QA datasets, questions often ask about the answer in some specific temporal or geographical context. For instance, when people want to ask about the previous answer to a questions, people often add words like \"previously\" or \"last\" in their question to specify it. We find that such questions comprise about 4.1% of NQ-Open (Kwiatkowski et al., 2019;. We also find that 5.1% of questions specify a specific point in time, as determined by whether there is a year expression in the question. Finally, we estimate the number of questions that specify a geographical context by counting number of stripped questions that were specified as geographically dependent. These questions, which have a phrase specifying the geographical context, comprise about 4.4% of NQ-Open. Our modifications closely resemble these naturally occurring questions, but can sometimes produce ungrammatical sentences, usually due verb tense agreement.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "C Retriever Analysis", "text": "Are errors in retrieval based systems due to poor retrieval? In Table 13, we report the retrieval performance from the settings in Section 5. We measure comparing against the results from Table 5 and Table 6, we see similar trends in endto-end performance reflected in our retriever performance.\nWe also explore whether the retriever model is able to adapt to updated corpora, reporting retrieval performance in Table 14. Here, we see that while retriever performance also suffers on queries with updated answers, suggesting that retriever systems are perhaps implicitly learning to situate questions within the time period of their large-scale training datasets.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_8", "tab_1"]}, {"heading": "D Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Context", "text": "Dependent Question Identification Baselines We use the pytorch-transformers (Wolf et al., 2020) library to implement our classification models. The training batch size is set to 8 and 64 for BERT-base and BERT-large, respectively. We train for 10 epochs using a learning rage of 5e-5 and 500 warmup steps and select the best preforming model measured on dev after each epoch.\nContext Dependent Question Answering Baselines We finetune our closed book baselines for 10 epochs with a batch size of 256, using the AdamW optimizer with a learning rate of 1e-5. We select the best performing model evaluated after each training epoch. We keep all other hyperparameters the same from the original implementation.\nFor our retrieval based baselines, we finetune both the retriever and reader components. The retriever models are trained using in-batch negatives plus one hard-negative passage per question. We use hard-negative and gold passages from the query modified model's predictions before finetuning. We train the retriever model for 8 epochs, using a learning rate of 1e-5, 100 warmup steps, and a batch size of 16. We then finetune our reader models for 16 epochs and a batch size of 16. We select the best model evaluated after each training epoch for both reader and retriever models, and select the best topk retrieval results to use for in k \u2208 {10, 20, 50} evaluated on the development set. All other hyperparameters from the DPR reader and retriever models are kept the same from the original work.     ", "publication_ref": ["b54"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We would like to thank Sewon Min, Raymond Mooney, and members of UT NLP group for comments and discussions. The work is partially funded by Google Faculty Awards.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Data Collection Details", "text": "Data Composition For NaturalQuestions and WebQuestions, we use the open-domain splits established in  and we only consider questions from TyDi-QA that are written in English. We randomly sampled the questions from all datasets except for NQ, where the ratio of temporally dependent questions was low (around 15%). Initial pilot tests showed that questions where the answer span is within a table in its evidence document (we refer these as NQ-Table ) are more often temporally dependent compared to questions where answers are found only in paragraphs (we refer these as NQ-Passage). To better target temporally dependent questions, we sampled heavily from NQ-Table . The source dataset statistics can be found in Table 10, and proportion of temporally dependent questions in each dataset can be found in Table 11.\nGeographically Dependent Question Generation To generate geographically dependent questions, we first run the named entity recognition tagger from  and the dependency parser from Dozat and Manning (2017) over each example and filter for those with a geopolitical entity in the question. We then delete the entity based on its syntactic role. If the entity's syntactic role is either nn or amod, we delete the entire entity and all of its descendants. If the the entity's role is pobj, the entity's parent preposition and all its descendants. Finally, if the entity's role is root or nsubj, we replace the entity with the pronoun we, deleting all determiners and conjugating any auxiliary verbs accordingly. We ignore instances where the dependency is not in one of these categories, there are multiple GPE entities, the stripped questions is has 3 tokens or less, or there is disagreement between our parser and tagger. We use the implementations of  and Dozat and Manning (2017) from AllenNLP (Gardner et al., 2017).\nCrowdsourcing Details We pay workers 0.15 USD per identification HIT and 0.40 USD per {Context / Answer} collection and validation HIT. During {Context / Answer} collection, if an annotator is unable to find the previous answer or either transition date after visiting 3 articles, they may leave out that information. In validation stage, in ", "publication_ref": ["b10", "b10", "b14"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Semantic parsing on freebase from question-answer pairs", "journal": "EMNLP", "year": "2013", "authors": "Jonathan Berant; Andrew K Chou; Roy Frostig; Percy Liang"}, {"ref_id": "b1", "title": "Timelines from text: Identification of syntactic temporal relations", "journal": "", "year": "2007", "authors": "Steven Bethard; James H Martin; Sara Klingenstein"}, {"ref_id": "b2", "title": "Ms marco: A human generated machine reading comprehension dataset", "journal": "ArXiv", "year": "2016", "authors": "Daniel Fernando Campos; T Nguyen; M Rosenberg; Xia Song; Jianfeng Gao; Saurabh Tiwary; Rangan Majumder; L Deng; Bhaskar Mitra"}, {"ref_id": "b3", "title": "An annotation framework for dense event ordering", "journal": "", "year": "2014", "authors": "T Cassidy; Bill Mcdowell; Nathanael Chambers; Steven Bethard"}, {"ref_id": "b4", "title": "Reading Wikipedia to answer opendomain questions", "journal": "", "year": "2017", "authors": "Danqi Chen; Adam Fisch; Jason Weston; Antoine Bordes"}, {"ref_id": "b5", "title": "A dataset for answering time-sensitive questions", "journal": "ArXiv", "year": "2021", "authors": "Wenhu Chen; Xinyi Wang; William Yang Wang"}, {"ref_id": "b6", "title": "Quac : Question answering in context", "journal": "", "year": "2018", "authors": "Eunsol Choi; He He; Mohit Iyyer; Mark Yatskar; Yejin Wen Tau Yih; Percy Choi; Luke Liang;  Zettlemoyer"}, {"ref_id": "b7", "title": "Tydi qa: A benchmark for informationseeking question answering in typologically diverse languages", "journal": "Transactions of the Association for Computational Linguistics (TACL)", "year": "2020", "authors": "J Clark; M Choi; Dan Collins; T Garrette; V Kwiatkowski; Jennimaria Nikolaev;  Palomaki"}, {"ref_id": "b8", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "J Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b9", "title": "Time-aware language models as temporal knowledge bases", "journal": "", "year": "2021", "authors": "Bhuwan Dhingra; Jeremy R Cole; Julian Martin Eisenschlos; D Gillick; Jacob Eisenstein; William W Cohen"}, {"ref_id": "b10", "title": "Deep biaffine attention for neural dependency parsing", "journal": "ArXiv", "year": "2017", "authors": "Timothy Dozat; Christopher D Manning"}, {"ref_id": "b11", "title": "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs", "journal": "", "year": "2019", "authors": "Dheeru Dua; Yizhong Wang; Pradeep Dasigi; Gabriel Stanovsky; Sameer Singh; Matt Gardner"}, {"ref_id": "b12", "title": "A latent variable model for geographic lexical variation", "journal": "EMNLP", "year": "2010", "authors": "Jacob Eisenstein; Brendan T O'connor; Noah A Smith; E Xing"}, {"ref_id": "b13", "title": "Entities as experts: Sparse memory access with entity supervision", "journal": "EMNLP", "year": "2020", "authors": "Thibault F\u00e9vry; Baldini Livio; Nicholas Soares; Eunsol Fitzgerald; T Choi;  Kwiatkowski"}, {"ref_id": "b14", "title": "Allennlp: A deep semantic natural language processing platform", "journal": "", "year": "2017", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke S Zettlemoyer"}, {"ref_id": "b15", "title": "REALM: Retrieval-augmented language model pre-training", "journal": "", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Ming-Wei Chang"}, {"ref_id": "b16", "title": "Yago2: A spatially and temporally enhanced knowledge base from wikipedia", "journal": "Artificial Intelligence", "year": "2013", "authors": "Johannes Hoffart; Fabian M Suchanek; Klaus Berberich; Gerhard Weikum"}, {"ref_id": "b17", "title": "Examining temporality in document classification", "journal": "", "year": "2018", "authors": "Xiaolei Huang; Michael J Paul"}, {"ref_id": "b18", "title": "Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs", "journal": "ArXiv", "year": "2020", "authors": "Jena D Hwang; Chandra Bhagavatula; Jeff Ronan Le Bras; Keisuke Da; Antoine Sakaguchi; Yejin Bosselut;  Choi"}, {"ref_id": "b19", "title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"ref_id": "b20", "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly", "journal": "", "year": "2020", "authors": "Nora Kassner; H Schutze"}, {"ref_id": "b21", "title": "Which linguist invented the lightbulb? presupposition verification for question-answering", "journal": "", "year": "2021", "authors": "Najoung Kim; Ellie Pavlick; D Burcu Karagol Ayan;  Ramachandran"}, {"ref_id": "b22", "title": "Learning to extract events from knowledge base revisions", "journal": "", "year": "2017", "authors": "A Konovalov; Benjamin Strauss; Alan Ritter; Brendan T O'connor"}, {"ref_id": "b23", "title": "Natural questions: A benchmark for question answering research", "journal": "Transactions of the Association for Computational Linguistics (TACL)", "year": "2019", "authors": "T Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur P Parikh; C Alberti; D Epstein; J Illia Polosukhin; Kenton Devlin; Kristina Lee; Llion Toutanova; Matthew Jones; Ming-Wei Kelcey; Andrew M Chang; Jakob Dai; Quoc Uszkoreit; Slav Le;  Petrov"}, {"ref_id": "b24", "title": "Tom\u00e1s Kocisk\u00fd, Susannah Young, and P. Blunsom. 2021. Pitfalls of static language modelling", "journal": "", "year": "", "authors": "A Lazaridou; Adhiguna Kuncoro; E Gribovskaya; Devang Agrawal; Adam Liska; Tayfun Terzi; Mai Gimenez; Cyprien De Masson D'autume; Sebastian Ruder; Dani Yogatama; Kris Cao"}, {"ref_id": "b25", "title": "Latent retrieval for weakly supervised open domain question answering", "journal": "", "year": "2019", "authors": "Kenton Lee; Ming-Wei Chang; Kristina Toutanova"}, {"ref_id": "b26", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b27", "title": "Retrieval-augmented generation for knowledgeintensive nlp tasks", "journal": "", "year": "2020", "authors": "Patrick Lewis; Ethan Perez; Aleksandara Piktus; F Petroni; V Karpukhin; Naman Goyal; Heinrich Kuttler; M Lewis; Wen Tau Yih; Tim Rockt\u00e4schel; Sebastian Riedel; Douwe Kiela"}, {"ref_id": "b28", "title": "Learning question classifiers", "journal": "", "year": "2002", "authors": "Xin Li; D Roth"}, {"ref_id": "b29", "title": "Temporal information extraction", "journal": "", "year": "2010", "authors": "Xiao Ling; Daniel S Weld"}, {"ref_id": "b30", "title": "Semeval-2015 task 5: Qa tempeval -evaluating temporal information understanding with question answering", "journal": "", "year": "2015", "authors": "Hector Llorens; N Chambers; N Uzzaman; James F Mostafazadeh; J Allen;  Pustejovsky"}, {"ref_id": "b31", "title": "", "journal": "", "year": "", "authors": "Sewon Min; Jordan L Boyd-Graber; C Alberti; Danqi Chen; Eunsol Choi; Michael Collins; Kelvin Guu; Hannaneh Hajishirzi; Kenton Lee; Jennimaria Palomaki; Colin Raffel; Adam Roberts; T Kwiatkowski; Patrick Lewis; Yuxiang Wu; Heinrich Kuttler; L Liu; Pasquale Minervini; Pontus Stenetorp; Sebastian Riedel; Sohee Yang; Minjoon Seo; F Gautier Izacard; Lucas Petroni; Nicola De Hosseini; E Cao; Ikuya Grave; Sonse Yamada; Masatoshi Shimaoka; Shumpei Suzuki; S Miyawaki; Ryo Sato; J Takahashi; Martin Suzuki; Martin Fajcik; Karel Docekal; P Ondrej; Hao Smrz; Y Cheng; X Shen; Pengcheng Liu; W He; Jianfeng Chen; Barlas Gao; Xilun Oguz; V Chen; Stanislav Karpukhin; Dmytro Peshterliev; M Okhonko; Sonal Schlichtkrull; Yashar Gupta;  Mehdad"}, {"ref_id": "b32", "title": "Ambigqa: Answering ambiguous open-domain questions", "journal": "EMNLP", "year": "2020", "authors": "Sewon Min; Julian Michael; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b33", "title": "Exponential reservoir sampling for streaming language models", "journal": "", "year": "2014", "authors": "Miles Osborne; Ashwin Lall; Benjamin Van Durme"}, {"ref_id": "b34", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "journal": "", "year": "2016", "authors": "Denis Paperno; Germ\u00e1n Kruszewski; Angeliki Lazaridou; Ngoc Quan Pham; Raffaella Bernardi; Sandro Pezzelle; Marco Baroni; Gemma Boleda; Raquel Fernandez"}, {"ref_id": "b35", "title": "Semi-supervised sequence tagging with bidirectional language models", "journal": "", "year": "2017", "authors": "Matthew E Peters; Waleed Ammar; Chandra Bhagavatula; R Power"}, {"ref_id": "b36", "title": "Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks", "journal": "", "year": "", "authors": "Fabio Petroni; Aleksandra Piktus; Angela Fan; Patrick Lewis; Majid Yazdani; Nicola De Cao; James Thorne; Yacine Jernite; Vladimir Karpukhin; Jean Maillard"}, {"ref_id": "b37", "title": "Language models as knowledge bases?", "journal": "EMNLP", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; A H Miller; P Lewis; A Bakhtin; Y Wu; S Riedel"}, {"ref_id": "b38", "title": "Statistical script learning with multi-argument events", "journal": "", "year": "2014", "authors": "Karl Pichotta; R Mooney"}, {"ref_id": "b39", "title": "Dynasent: A dynamic benchmark for sentiment analysis", "journal": "ArXiv", "year": "2020", "authors": "Christopher Potts; Zhengxuan Wu; A Geiger; Douwe Kiela"}, {"ref_id": "b40", "title": "Timeml: Robust specification of event and temporal expressions in text", "journal": "", "year": "2003", "authors": "J Pustejovsky; J Casta\u00f1o; R Ingria; R Saur\u00ed; R Gaizauskas; A Setzer; G Katz; Dragomir R Radev"}, {"ref_id": "b41", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b42", "title": "Know what you don't know: Unanswerable questions for squad", "journal": "", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b43", "title": "Squad: 100,000+ questions for machine comprehension of text", "journal": "EMNLP", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b44", "title": "Beyond accuracy: Behavioral testing of nlp models with checklist", "journal": "", "year": "2020", "authors": "Tongshuang Marco T\u00falio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"ref_id": "b45", "title": "Temporally-informed analysis of named entity recognition", "journal": "", "year": "2020", "authors": "Shruti Rijhwani; Daniel Preotiuc-Pietro"}, {"ref_id": "b46", "title": "How much knowledge can you pack into the parameters of a language model", "journal": "EMNLP", "year": "2020", "authors": "Adam Roberts; Colin Raffel; Noam Shazeer"}, {"ref_id": "b47", "title": "Question answering over temporal knowledge graphs", "journal": "", "year": "2021", "authors": "Apoorv Saxena; Soumen Chakrabarti; Partha P Talukdar"}, {"ref_id": "b48", "title": "Get your vitamin c! robust fact verification with contrastive evidence", "journal": "", "year": "2021", "authors": "Tal Schuster; A Fisch; R Barzilay"}, {"ref_id": "b49", "title": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world", "journal": "", "year": "2017", "authors": "S Shankar; Yoni Halpern; Eric Breck; J Atwood; J Wilson; D Sculley"}, {"ref_id": "b50", "title": "Literary event detection", "journal": "", "year": "2019", "authors": "Matthew Sims; Jong Ho Park; David Bamman"}, {"ref_id": "b51", "title": "Mul-timodal{qa}: complex question answering over text, tables and images", "journal": "", "year": "2021", "authors": "Alon Talmor; Ori Yoran; Amnon Catav; Dan Lahav; Yizhong Wang; Akari Asai; Gabriel Ilharco; Hannaneh Hajishirzi; Jonathan Berant"}, {"ref_id": "b52", "title": "Fine-grained temporal relation extraction", "journal": "", "year": "2019", "authors": "Siddharth Vashishtha; Benjamin Van Durme; A White"}, {"ref_id": "b53", "title": "Continuous time dynamic topic models", "journal": "", "year": "2008", "authors": "C Wang; D Blei; D Heckerman"}, {"ref_id": "b54", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b55", "title": "Dynamic language models for streaming text", "journal": "", "year": "2014", "authors": "Dani Yogatama; C Wang; Bryan R Routledge; Noah A Smith; E Xing"}, {"ref_id": "b56", "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "journal": "EMNLP", "year": "2018", "authors": "Rowan Zellers; Yonatan Bisk; Roy Schwartz; Yejin Choi"}, {"ref_id": "b57", "title": "Temporal common sense acquisition with minimal supervision", "journal": "", "year": "2020", "authors": "Ben Zhou; Qiang Ning; Daniel Khashabi; D Roth"}, {"ref_id": "b58", "title": "Temporal reasoning on implicit events from distant supervision", "journal": "", "year": "2021", "authors": "Ben Zhou; Kyle Richardson; Qiang Ning; Tushar Khot; Ashish Sabharwal; D Roth"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Examples of questions with answers that change depending on the temporal or geographical context.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Interface for identifying temporally and geographically dependent questions.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Interface for collecting geographical {Context / Answer} pairs. Annotators are also asked to verify the original location and answer.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure 6: Interface for searching for articles during temporal {Context / Answer} collection. The search interface is shared between temporal and geographical {Context / Answer} collection and verification stages.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Interface for viewing Wikipedia articles during the {Context / Answer} collection and verification stages.Workers select articles from the search results depicted above that they want to view.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Annotation interface for {Context / Answer} verification stages. Workers may search for the answers themselves, or view the articles used by workers in the prior stage.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Question q Context Type ct Context Value cv Answer a Who composed the music for the first Harry Potter film? ---What's the biggest country in Europe excluding Russia? ---How many seasons are there for American Horror Story?", "figure_data": "TEMPSep 18, 2019 Sep 13, 201710 9Who made the most three point shots in the NBA?TEMP2014 2005Ray Allen Reggie MillerWhen was the last time states were created?GEONigeria 1 October 1996 United States 1959Where do we rank among the world's largest cities?GEOTokyo Shanghai1st 3rd"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Examples of how questions interact with geographical and temporal context in SITUATEDQA. The first two questions are not identified as geographically nor temporally dependent.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "# q, cv, a # q, cv, a # q, cv, a Fleiss' \u03ba", "figure_data": "Identification Data{Context / Answer} DataContextTrainDevTestAgreementTrainDevTestAgreementType (ct) Fleiss' \u03ba TEMP # q % # q % # q % 4438 36 2572 32 1962 28 0.626009342327950.57GEO1149 46879 42367 370.56354813985050.56"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Dataset statistics. For our identification dataset, we report both the total number of questions (# q) and the percent of questions that are context-dependent (%). For our {Context / Answer} dataset, we report the number of unique (question, context value, answer) triples (# q, c v , a). Each context type and split's (q, c v , a) triples are collected slightly differently, see Section 3.2 for details. We also report inter-annotator agreement on context dependent question identification and {Context / Answer} validation.Figure 3: The distribution of temporally dependent questions by the duration its previous answer was true for with examples.10% of examples in all datasets we looked at, even without any filtering. We also found that examples from Natural Questions where at least one answer span is within a table in its evidence document are more often temporally-dependent. To construct our final dataset, we upsample such questions. We followed the original train, development, and test split for each dataset, which caused slight inconsistencies in the proportion of examples from each dataset across splits, simulating a domain shift.For 2.8K of those context-dependent questions (2.4K temporal, 0.5K geographical), we collected a total of 5.9K answers from alternate contexts (4.0K temporal, 1.9K geographical). From those alternate temporal context / answer pairs, we construct (q, c v , a) by sampling valid dates, creating 6K examples. The final TEMP dataset also includes 6.7K examples from temporally-independent questions. More details and statistics on individual datasets can be found in the in Appendix A.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Examples of modified queries. Text shown in bold is concatenated onto the original question. To", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "/ 44.8 31.9 / 28.3 61.7 / 64.9 42.1 / 39.4 49.7 / 48.0 42.3 / 38.1 54.2 / 62.8 47.5 / 47.4 BERTbase 94.5 / 93.5 94.2 / 92.7 88.5 / 83.1 91.2 / 87.6 86.0 / 79.6 79.0 / 67.4 90.8 / 87.6 84.5 / 76.2 BERTlarge 95.7 / 93.8 94.7 / 91.2 91.7 / 85.9 93.2 / 88.5 86.1 / 80.7 80.3 / 69.9 88.6 / 84.7 84.3 / 76.6", "figure_data": "TEMPGEOModelAccuracyPrecisionRecallF1AccuracyPrecisionRecallF1Random 45.3 Human  *  96.0 / 94.0 88.9 / 83.3100 / 100 94.1 / 90.9 88.0 / 84.0 76.5 / 72.7 86.7 / 88.9 81.2 / 80.0"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results for context-dependent question identification. Each cell report results on development / test set.", "figure_data": "QueryFine-TEMPGEOMod.tunedStatic (400) Samp. (1472) Start (923) Total Comm. (265) Rare (240) TotalRetrieval based44.2 28.8 39.816.0 15.9 17.214.2 18.5 24.919.4 18.6 23.09.1 27.5 27.92.9 22.1 25.06.1 25.0 26.5Closed Book27.2 19.5 26.015.3 12.4 16.212.9 15.7 18.316.2 14.5 18.39.4 19.2 21.54.6 9.2 11.77.1 14.5 16.8Human  *---57.0--34.0"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": "QueryFine-TEMPGEOMod.tunedOneAny OneAnyRetrieval based19.4 28.1 18.6 26.4 25.0 30.7 6.1 22.6 23.0 29.7 26.5 32.3Closed Book16.2 24.0 14.5 19.9 14.5 22.6 7.1 25.1 18.3 25.9 16.8 24.4"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results of our unmodified baseline run on GEO.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Results on training our baselines on predicting the current and previous answers to a question. Human performance is estimated using 100 randomly sampled examples annotated by the authors of this paper.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ": Results on the default open-retrieval QA set-ting where all questions are assumed to be about thepresent timestamp (Feb 2021), evaluation split by ques-tions that require updating the world state to after thelarge scale training dataset was collected, Updated, andquestions where answers didn't change since the datacollection, Stable. Our retrieval based systems use theEnglish Wikipedia dumps from 2018-12-20 and 2021-02-20 while our closed book systems are pretrained ondata from 2019-02 and earlier."}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Number of identification examples by split and source dataset.", "figure_data": "SourceTemporally DependentNoMaybe YesNQ-Passage73.3 15.611.1NQ-Table54.6 3.741.6WebQuestions 59.9 10.429.9TyDi-QA (en) 62.3 14.123.5MS-Marco60.0 26.413.6"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "", "figure_data": ": Temporal volatility distribution of raw an-notation for various datasets (numbers on the trainingsplit)."}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "", "figure_data": ": Interannotator Agreement of Identificationexamples. We compute Fleiss's kappa all exampleswhere the majority of labels (2 out of 3) are either \"Yes\"or \"No\" (first column) and over all examples (secondcolumn) \"Yes\", \"No\", or \"Maybe/Unsure\"."}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "", "figure_data": ": Retriever results on the default open-retrievalQA setting where all questions are assumed to beabout the present timestamp (Feb 2021), given as an-swer recall at top 50 on our test set. We split exam-ples based on whether the current answer started to betrue after the large scale training dataset was collected,Updated, or whether questions where answers didn'tchange since the data collection, Stable. Retrieval isrun over English Wikipedia dumps from 2018-12-20and 2021-02-20."}], "formulas": [], "doi": ""}