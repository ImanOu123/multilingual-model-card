{"title": "DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION", "authors": "Ben Poole; Ajay Jain; Jonathan T Barron; Ben Mildenhall; Google Research; U C Berkeley", "pub_date": "2022-09-29", "abstract": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors. See dreamfusion3d.github.io for a more immersive view into our 3D results.", "sections": [{"heading": "INTRODUCTION", "text": "Generative image models conditioned on text now support high-fidelity, diverse and controllable image synthesis (Nichol et al., 2022;Ramesh et al., 2021;Saharia et al., 2022;Yu et al., 2022;Saharia et al., 2021b). These quality improvements have come from large aligned image-text datasets (Schuhmann et al., 2022) and scalable generative model architectures. Diffusion models are particularly effective at learning high-quality image generators with a stable and scalable denoising objective (Ho et al., 2020;Sohl-Dickstein et al., 2015;Song et al., 2021). Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data (Chen et al., 2020;Kong et al., 2021). In this work, we develop techniques to transfer pretrained 2D image-text diffusion models to 3D object synthesis, without any 3D data (see Figure 1). Though 2D image generation is widely applicable, simulators and digital media like video games and movies demand thousands of detailed 3D assets to populate rich interactive environments. 3D assets are currently designed by hand in modeling software like Blender and Maya3D, a process requiring a great deal of time and expertise. Text-to-3D generative models could lower the barrier to entry for novices and improve the workflow of experienced artists.\n3D generative models can be trained on explicit representations of structure like voxels (Wu et al., 2016; and point clouds (Yang et al., 2019;Cai et al., 2020;Zhou et al., 2021), but the 3D data needed is relatively scarce compared to plentiful 2D images. Our approach learns 3D structure using only a 2D diffusion model trained on images, and sidesteps this issue. GANs can learn controllable 3D generators from photographs of a single object category, by placing an adversarial loss on 2D image renderings of the output 3D object or scene (Henzler et al., 2019;Nguyen-Phuoc et al., 2019;Or-El et al., 2022). Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text.\nNeural Radiance Fields, or NeRF  are an approach towards inverse rendering in which a volumetric raytracer is combined with a neural mapping from spatial coordinates to color and volumetric density. NeRF has become a critical tool for neural inverse rendering . Originally, NeRF was found to work well for \"classic\" 3D reconstruction tasks: many images of a scene are provided as input to a model, and a NeRF is optimized to recover the geometry of that specific scene, which allows for novel views of that scene from unobserved angles to be synthesized.\nan orangutan making a clay bowl on a throwing wheel* a raccoon astronaut holding his helmet \u2020 a blue jay standing on a large basket of rainbow macarons* a corgi taking a selfie* a table with dim sum on it \u2020 a lion reading the newspaper* Michelangelo style statue of dog reading news on a cellphone a tiger dressed as a doctor* a steam engine train, high resolution* a frog wearing a sweater* a humanoid robot playing the cello* Sydney opera house, aerial view \u2020 an all-utility vehicle driving across a stream \u2020 a chimpanzee dressed like Henry VIII king of England* a baby bunny sitting on top of a stack of pancakes \u2020 a sliced loaf of fresh bread a bulldozer clearing away a pile of snow* a classic Packard car* zoomed out view of Tower Bridge made out of gingerbread and candy \u2021 a robot and dinosaur playing chess, high resolution* a squirrel gesturing in front of an easel showing colorful pie charts Figure 1: DreamFusion uses a pretrained text-to-image diffusion model to generate realistic 3D models from text prompts. Rendered 3D models are presented from two views, with textureless renders and normals to the right. See dreamfusion3d.github.io for videos of these results. Symbols indicate the following prompt prefixes which we found helped to improve the quality and realism: * a DSLR photo of... \u2020 a zoomed out DSLR photo of... \u2021 a wide angle zoomed out DSLR photo of... Many 3D generative approaches have found success in incorporating NeRF-like models as a building block within a larger generative system (Schwarz et al., 2020;a;Gu et al., 2021;. One such approach is Dream Fields (Jain et al., 2022), which uses frozen image-text joint embedding models from CLIP and an optimization-based approach to train NeRFs. This work showed that pretrained 2D image-text models may be used for 3D synthesis, though 3D objects produced by this approach tend to lack realism and accuracy. CLIP has been used to guide other approaches based on voxel grids and meshes (Sanghi et al., 2022;Jetchev, 2021;.\nWe adopt a similar approach to Dream Fields, but replace CLIP with a loss derived from distillation of a 2D diffusion model. Our loss is based on probabilty density distillation, minimizing the KL divergence between a family of Gaussian distribution with shared means based on the forward process of diffusion and the score functions learned by the pretrained diffusion model. The resulting Score Distillation Sampling (SDS) method enables sampling via optimization in differentiable image parameterizations. By combining SDS with a NeRF variant tailored to this 3D generation task, DreamFusion generates high-fidelity coherent 3D objects and scenes for a diverse set of user-provided text prompts.", "publication_ref": ["b48", "b53", "b71", "b52", "b55", "b19", "b57", "b60", "b11", "b28", "b68", "b59", "b7", "b72", "b17", "b38", "b40", "b56", "b14", "b24", "b54", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "DIFFUSION MODELS AND SCORE DISTILLATION SAMPLING", "text": "Diffusion models are latent-variable generative models that learn to gradually transform a sample from a tractable noise distribution towards a data distribution (Sohl-Dickstein et al., 2015;Ho et al., 2020). Diffusion models consist of a forward process q that slowly removes structure from data x by adding noise, and a reverse process or generative model p that slowly adds structure starting from noise z t . The forward process is typically a Gaussian distribution that transitions from the previous less noisy latent at timestep t to a noisier latent at timestep t + 1. We can compute the marginal distribution of the latent variables at timestep t given an initial datapoint x by integrating out intermediate timesteps: q(z t |x) = N (\u03b1 t x, \u03c3 2 t I). The marginals integrating out the data density q(x) are q(z t ) = q(z t |x)q(x) dx, and correspond to smoothed versions of the data distribution. The coefficients \u03b1 t and \u03c3 t are chosen such that q(z t ) is close to the data density at the start of the process (\u03c3 0 \u2248 0) and close to Gaussian at the end of the forward process (\u03c3 T \u2248 1), with \u03b1 2 t = 1 \u2212 \u03c3 2 t chosen to preserve variance Song et al., 2021).\nThe generative model p is trained to slowly add structure starting from random noise p(z T ) = N (0, I) with transitions p \u03c6 (z t\u22121 |z t ). Theoretically, with enough timesteps, the optimal reverse process step is also Gaussian and related to an optimal MSE denoiser (Sohl-Dickstein et al., 2015). Transitions are typically parameterized as p \u03c6 (z t\u22121 |z t ) = q(z t\u22121 |z t , x =x \u03c6 (z t ; t)) where q(z t\u22121 |z t , x) is a posterior distribution derived from the forward process andx \u03c6 (z t ; t) is a learned approximation of the optimal denoiser. Instead of directly predictingx \u03c6 , Ho et al. (2020) trains an image-to-image U-Net \u03c6 (z t ; t) that predicts the noise content of the latent z t :\nE[x|z t ] \u2248x \u03c6 (z t ; t) = (z t \u2212 \u03c3 t \u03c6 (z t ; t)) /\u03b1 t .\nThe predicted noise can be related to a predicted score function for the smoothed density \u2207 zt log p(z t ) through Tweedie's formula (Robbins, 1992)\n: \u03c6 (z t ; t) = \u2212\u03c3 t s \u03c6 (z t ; t).\nTraining the generative model with a (weighted) evidence lower bound (ELBO) simplifies to a weighted denoising score matching objective for parameters \u03c6 (Ho et al., 2020;:\nL Diff (\u03c6, x) = E t\u223cU (0,1), \u223cN (0,I) w(t) \u03c6 (\u03b1 t x + \u03c3 t ; t) \u2212 2 2 ,(1)\nwhere w(t) is a weighting function that depends on the timestep t. Diffusion model training can thereby be viewed as either learning a latent-variable model (Sohl-Dickstein et al., 2015;Ho et al., 2020), or learning a sequence of score functions corresponding to noisier versions of the data (Vincent, 2011;Song & Ermon, 2019;Song et al., 2021). We will use p \u03c6 (z t ; t) to denote the approximate marginal distribution whose score function is given by s \u03c6 (z t ; t) = \u2212 \u03c6 (z t ; t)/\u03c3 t .\nOur work builds on text-to-image diffusion models that learn \u03c6 (z t ; t, y) conditioned on text embeddings y (Saharia et al., 2022;Ramesh et al., 2022;Nichol et al., 2022). These models use classifier-free guidance (CFG, Ho & Salimans, 2022), which jointly learns an unconditional model to enable higher quality generation via a guidance scale parameter \u03c9:\u02c6 \u03c6 (z t ; y, t) = (1 + \u03c9) \u03c6 (z t ; y, t) \u2212 \u03c9 \u03c6 (z t ; t). CFG alters the score function to prefer regions where the ratio of the conditional density to the unconditional density is large. In practice, setting \u03c9 > 0 improves sample fidelity at the cost of diversity. We use\u02c6 andp throughout to denote the guided version of the noise prediction and marginal distribution.\nUpdates parameters with SGD: Updates sample in pixel space:", "publication_ref": ["b57", "b19", "b60", "b57", "b19", "b49", "b19", "b57", "b19", "b66", "b59", "b60", "b53", "b48", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Score Distillation Sampling Ancestral Sampling", "text": "Figure 2: Comparison of 2D sampling methods from a text-to-image diffusion model with text \"a photo of a tree frog wearing a sweater.\" For score distillation sampling, as an example we use an image generator that restricts images to be symmetric by having x = (flip(\u03b8), \u03b8).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HOW CAN WE SAMPLE IN PARAMETER SPACE, NOT PIXEL SPACE?", "text": "Existing approaches for sampling from diffusion models generate a sample that is the same type and dimensionality as the observed data the model was trained on (Song et al., 2021;. Though conditional diffusion sampling enables quite a bit of flexibility (e.g. inpainting), diffusion models trained on pixels have traditionally been used to sample only pixels. We are not interested in sampling pixels; we instead want to create 3D models that look like good images when rendered from random angles. Such models can be specified as a differentiable image parameterization (DIP, Mordvintsev et al., 2018), where a differentiable generator g transforms parameters \u03b8 to create an image x = g(\u03b8). DIPs allow us to express constraints, optimize in more compact spaces (e.g. arbitrary resolution coordinate-based MLPs), or leverage more powerful optimization algorithms for traversing pixel space. For 3D, we let \u03b8 be parameters of a 3D volume and g a volumetric renderer. To learn these parameters, we require a loss function that can be applied to diffusion models.\nOur approach leverages the structure of diffusion models to enable tractable sampling via optimization -a loss function that, when minimized, yields a sample. We optimize over parameters \u03b8 such that x = g(\u03b8) looks like a sample from the frozen diffusion model. To perform this optimization, we need a differentiable loss function where plausible images have low loss, and implausible images have high loss, in a similar style to DeepDream (Mordvintsev et al., 2015). We first investigated reusing the diffusion training loss (Eqn. 1) to find modes of the learned conditional density p(x|y). While modes of generative models in high dimensions are often far from typical samples (Nalisnick et al., 2018), the multiscale nature of diffusion model training may help to avoid these pathologies. Minimizing the diffusion training loss with respect to a generated datapoint x = g(\u03b8) gives \u03b8 * = arg min \u03b8 L Diff (\u03c6, x = g(\u03b8)). In practice, we found that this loss function did not produce realistic samples even when using an identity DIP where x = \u03b8. Concurrent work from Graikos et al. (2022) shows that this method can be made to work with carefully chosen timestep schedules, but we found this objective brittle and its timestep schedules challenging to tune.\nTo understand the difficulties of this approach, consider the gradient of L Diff :\n\u2207 \u03b8 L Diff (\u03c6, x = g(\u03b8)) = E t, w(t) (\u02c6 \u03c6 (z t ; y, t) \u2212 ) Noise Residual \u2202\u02c6 \u03c6 (z t ; y, t) z t U-Net Jacobian \u2202x \u2202\u03b8 Generator Jacobian (2)\nwhere we absorb the constant \u03b1 t I = \u2202z t /\u2202x into w(t). In practice, the U-Net Jacobian term is expensive to compute (requires backpropagating through the diffusion model U-Net), and poorly conditioned for small noise levels as it is trained to approximate the scaled Hessian of the marginal density. We found that omitting the U-Net Jacobian term leads to an effective gradient for optimizing DIPs with diffusion models:\n\u2207 \u03b8 L SDS (\u03c6, x = g(\u03b8)) E t, w(t) (\u02c6 \u03c6 (z t ; y, t) \u2212 ) \u2202x \u2202\u03b8 (3)\nIntuitively, this loss perturbs x with a random amount of noise corresponding to the timestep t, and estimates an update direction that follows the score function of the diffusion model to move to a higher density region. While this gradient for learning DIPs with diffusion models may appear ad hoc, in Appendix A.4 we show that it is the gradient of a weighted probability density distillation loss (van den Oord et al., 2018) using the learned score functions from the diffusion model:\n\u2207 \u03b8 L SDS (\u03c6, x = g(\u03b8)) = \u2207 \u03b8 E t [\u03c3 t /\u03b1 t w(t)KL(q(z t |g(\u03b8); y, t) p \u03c6 (z t ; y, t))] .(4)\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" v 8 r j Q y N 5 r t 3 9 7 X D a Q p u L Z N P M T h w = \" > \nC I B q G S k R n E 9 s s D 3 V f D j l v y y t 4 Y d J b 4 U 1 I i U 9 Q 6 7 m c Q K Z 7 F k C C X z J i W 7 6 X Y z p l G w S U M i 0 F m I G X 8 l v W g Z W n C Y j D t f H z K k B 5 Y J a J d p e 1 L k I 7 V n x 0 5 i 8 1 o O 1 s Z M + y b v 9 5 I / M 9 r Z d g 9 a + c i S T O E h E 8 G d T N J U d F R L j Q S G j j K g S W M a 2 F 3 p b z P N O N o\n0 y v a E P y / J 8 + S R q X s n 5 Q r V 8 e l 6 v k 0 j g L Z J f v k k P j k l F T J J a m R O u H k g T y R F / L q P D r P z p v z P i m d c 6 Y 9 O + Q X n I 9 v W u S b V A = = < / l a t e x i t > albedo \u21e2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 i B g 5 Q m d w 3 t 0 R W B V + y 7 p / 8 K a j f M = \" > A\nA A C C n i c b V D L S g M x F M 3 4 r P U 1 6 t J N t A g u S p k p o i 6 L b l x W s A / o l J J J 0 z Y 0 k w z J H b E M X b v x V 9 y 4 U M S t X + D O v z F t Z 6 G t B 0 I O 5 9 z L v f e E s e A G P O / b W V p e W V 1 b z 2 3 k N 7 e 2 d 3 b d v f 2 6 U Y m m r E a V U L o Z E s M E l 6 w G H A R r x p q R K B S s E Q 6 v J 3 7 j n m n D l b y D U c z a E e l L 3 u O U g J U 6 7 l E A 7 A F S q X R E h B k H x a C I g 1 C J r h l F 9 k v l u O M W v J I 3 B V 4 k f k Y K K E O 1 4 3 4 F X U W T i E m g g h j T 8 r 0 Y 2 i n R w K l g 4 3 y Q G B Y T O i R 9 1 r J U k o i Z d j o 9 Z Y x P r N L F P a X t k 4 C n 6 u + O l E R m s p q t j A g M z L w 3 E f / z W g n 0 L t s p l 3 E C T N L Z o F 4 i M C g 8 y Q V 3 u W Y U x M g S Q j W 3 u 2 I 6 I J p Q s O n l b Q j + / M m L p F 4\nu + e e l 8 u 1 Z o X K V x Z F D h + g Y n S I f X a A K u k F V V E M U P a J n 9 I r e n C f n x X l 3 P m a l S 0 7 W c 4 D + w P n 8 A T P 6 m z o = < / l a t e x i t > normals n < l a t e x i t s h a 1 _ b a s e 6 4 = \" k m u W H I d x i K x W j 0 W 9 8 i g g i y X 3 a 4 4 = \"\n> A A A C A H i c b V A 9 S w N B E N 2 L X z F + R S 0 s b B a D Y B H C X R C 1 D N p Y R j A f k A t h b z N J l u z t H b t z Y j j S + F d s L B S x 9 W f Y + W / c f B S a + G D g 8 d 4 M M / O C W A q D r v v t Z F Z W 1 9 Y 3 s p u 5 r e 2 d 3 b 3 8 / k H d R I n m U O O R j H Q z Y A a k U F B D g R K a s Q Y W B h I a w f B m 4 j c e Q B s R q X s c x d A O W V + J n u A M r d T J H / k I j 5 h 2 Q R m B o 7 F f p H 7 R R 5 Z 0 8 g W 3 5 E 5 B l 4 k 3 J w U y R 7 W T / / K 7 E U 9 C U M g l M 6 b l u T G 2 U 6 Z R c A n j n J 8 Y i B k f s j 6 0 L F U s B N N O p w + M 6 a l V u r Q X a V s K 6 V T 9 P Z G y 0 J h R G N j O k O H A L H o T 8 T + v l W D v q p 0 K F S c I i s 8 W 9 R J J M a K T N G h X a O A o R 5\nY w r o W 9 l f I B 0 4 y j z S x n Q / A W X 1 4 m 9 X L J u y i V 7 8 4 L l e t 5 H F l y T E 7 I G f H I J a m Q W 1 I l N c L J m D y T V / L m P D k v z r v z M W v N O P O Z Q / I H z u c P y z a W h w = = < / l a t e x i t > density \u2327 < l a t e x i t s h a 1 _ b a s e 6 4 = \" K a I t g h l Z B + i H T 5 B E g z H e h q q Y + j A = \" > A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 B I v g q S R F 1 G P R i 8 c K 9 g P a W D a b S b t 0 s w m 7 E 7 W E / g 8 v H h T x 6 n / x 5 r 9 x 2 + a g r Q 8 G H u / N M D P P T w T X 6 D j f V m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r\n7 x + 0 d J w q B k 0 W i 1 h 1 f K p B c A l N 5 C i g k y i g k S + g 7 Y + u p 3 7 7 A Z T m s b z D c Q J e R A e S h 5 x R N N J 9 D + E J M z 2 k A Z e D S b 9 c c a r O D P Y y c X N S I T k a / f J X L 4 h Z G o F E J q j W X d d J 0 M u o Q s 4 E T E q 9 V E N C 2 Y g O o G u o p B F o L 5 t d P b F P j B L Y Y a x M S b R n 6 u + J j E Z a j y P f d E Y U h 3 r R m 4 r / e d 0 U w 0 s v 4 z J J E S S b L w p T Y W N s T y O w A 6 6 A o R g b Q p n i 5 l a b D a m i D E 1 Q J R O C u\n/ j y M m n V q u 5 5 t X Z 7 V q l f 5 X E U y R E 5 J q f E J R e k T m 5 I g z Q J I 4 o 8 k 1 f y Z j 1 a L 9 a 7 9 T F v L V j 5 z C H 5 A + v z B 0 R r k w Q = < / l a t e x i t > shading < l a t e x i t s h a 1 _ b a s e 6 4 = \" 3 L A f k N H F p v R C e P H 1 t J z g 9 r T u Z Q Y = \" > A A\nA C A 3 i c b V D L S g M x F M 3 U V 6 2 v U X e 6 C R b B h Z S Z I u q y 6 M Z l B f u A T i m Z N N O G Z j J D c k c s Q 8 G N v + L G h S J u / Q l 3 / o 2 Z d h b a e k L g c M 6 9 u T f H j w X X 4 D j f V m F p e W V 1 r b h e 2 t j c 2 t 6 x d / e a O k o U Z Q 0 a i U i 1 f a K Z 4 J I 1 g I N g 7 V g x E v q C t f z R d e a 3 7 p n S P J J 3 M I 5 Z N y Q D y Q N O C R i p Z x 9 4 w B 4 g n T 4 0 8 U 7 N C Q k M / S C l k 5 5 d d i r O F H i R u D k p o x z 1 n v 3 l 9 S O a h E w C F U T r j u v E 0 E 2 J A k 4 F m 5 S 8 R L O Y 0 B E Z s I 6 h k o R M d / P R + N g o f R x E y l w J e K r + 7 k h J q P U 4 9 E 1 l t q G e 9 z L x P 6 + T Q H D Z T b m M E 2 C S z g Y F i c A Q 4 S w Q 3 O e K U R B j Q w h V 3 O y K 6 Z A o Q s H E V j I h u P N f X i T N a s U 9 r 1 R v z 8 q 1 q z y O I j p E R + g E u e g C 1 d A N q q M G o u g R P a N X 9 G Y 9 W S / W u / U x K y\n1 Y e c 8 + + g P r 8 w e g Z Z g n < / l a t e x i t > color c rendering < l a t e x i t s h a 1 _ b a s e 6 4 = \" e V D e E O J M / M 9 x 0 z J p l X / t M y c 6 G 5 4 = \" >\nA A A C D 3 i c b V C 7 T s M w F H V 4 l v I K M L J Y V C C m K q k Q M F a w M B a J P q Q m q h z H a a 0 6 d m Q 7 S F W U P 2 D h V 1 g Y Q I i V l Y 2 / w W k z l J Y j W T 4 6 5 1 7 d e 0 + Q M K q 0 4 / x Y K 6 t r 6 x u b l a 3 q 9 s 7 u 3 r 5 9 c N h R I p W Y t L F g Q v Y C p A i j n L Q 1 1 Y z 0 E k l Q H D D S D c a 3 h d 9 9 J F J R w R / 0 J C F + j I a c R h Q j b a S B f e Y F g o V q E p s v 4 z n 0 c C g 0 n B c 9 w l g + s G t O 3 Z k C L h O 3 J D V Q o j W w v 7 1 Q 4 D Q m X G O G l O q 7 T q L 9 D E l N M S N 5 1 U s V S R A e o y H p G 8 p R T J S f T e / J 4 a l R Q h g J a R 7 X c K r O d 2 Q o V s V 2 p j J G e q Q W v U L 8 z + u n O r r 2 M 8 q T V B O O Z 4 O i l E E t Y B E O D K k k W L O J I Q h L a n a F e I Q k w t p E W D U h u I s n L 5 N O o + 5 e 1 h v 3 F 7 X m T R l H B R y D E 3 A O X H A F m u A O t E A b Y P A E X s A b e L e e r V f r w / q c l a 5 Y Z c 8 R + A P r 6 x d 1 W p 2 R < / l a t e x i t > n \u2022< l a t e x i t s h a _ b a s e = \" M n U T w k y E a Z X L l Y W + E E A d R = \" > A A A B n i c b V B N S N A E N W e t X a O X x S J K k k R V j r G C / Y A k l M l y d b M L u R C i l P O L B W + m u + W / c t j l o O B x s z z M y L M i k M u u s a + s b m X d o p + t H x x W j o b J s y W y l R I z B c C s V b K F D y b q Y J J H k n W h N / M T w b k a p H H G c T G C g R C w Y o J X Q E E k o R c g L K a c B V h W k S g o e W v o J + y P O E K m Q R j f M / N M J y A R s E k n a D P A M A g G L d U Q c J N O J m f P K X n V u n T O N W F N K + n t i A o k x y S y n Q n g C x M / E / z x v g k n Q m U c s U W i + J c U k z p H / a F o z l G N L g G l h b V s C B o Y p T K N g R v + e V V q X v K t a / e G y r g t i i R U J G L o h H r k m D J M m a R F G U v J M X s m b g L + L F r X n G L m h P y B / k D W c u R T g = = < / l a t e x i t > r\u2327 \"a DSLR photo of a peacock on a surfboard\" U-Net < l a t e x i t s h a 1 _ b a s e 6 4 = \" H H S K O Q r A 2 7 3 o n v 8 Z i k Z d P F J u e u U = \" > A A A C G n i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S x C B S l J E X V Z d K M b q W A v 0 I Q y m U 7 a o Z N J m J k I J e Q 5 3 P g q b l w o 4 k 7 c + D Z O 0 g j a + s P A x 3 / O Y c 7 5 v Y h R q S z r y 1 h Y X F p e W S 2 t l d c 3 N r e 2 z Z 3 d t g x j g U k L h y w U X Q 9 J w i g n L U U V I 9 1 I E B R 4 j H S 8 8 W V W 7 9 w T I W n I 7 9 Q k I m 6 A h p z 6 F C O l r b 5 p O y S S l I U c O p I G 0 A m Q G m H E k p u 0 m r P n J 1 Z 6 D H / 4 O j 3 q m x W r Z u W C 8 2 A X U A G F m n 3 z w x m E O A 4 I V 5 g h K X u 2 F S k 3 Q U J R z E h a d m J J I o T H a E h 6 G j k K i H S T / L Q U H m p n A P 1 Q 6 M c V z N 3 f E w k K p J w E n u 7 M V p S z t c z 8 r 9 a L l X / u J p R H s S I c T z / y Y w Z V C L O c 4 I A K g h W b a E B Y U L 0 r x C M k E F Y 6 z b I O w Z 4 9 e R 7 a 9 Z p 9 W q v f n l Q a F 0 U c J b A P D k A V 2 O A M N M A V a I I W w O A B P I E X 8 G o 8 G s / G m / E + b V 0 w i p k 9 8 E f G 5 z c y Y q D 1 < / l a t e x i t > \u270f \u21e0 N (0, I) < l a t e x i t s h a 1 _ b a s e 6 4 = \" j W j H 2 U T Q 5 s F l k B 0 + p m x P 4 j L V z 6 U = \" > A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G E 3 i H o M e v G Y g H l A s o T Z S W 8 y Z n Z 2 m Z k V Q s g X e P G g i F c / y Z t / 4 y T Z g y Y W N B R V 3 X R 3 B Y n g 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R 2 n i m G D x S J W 7 Y B q F F x i w 3 A j s J 0 o p F E g s B W M 7 m Z + 6 w m V 5 r F 8 M O M E / Y g O J A 8 5 o 8 Z K 9 Y t e s e S W 3 T n I K v E y U o I M t V 7 x q 9 u P W R q h N E x Q r T u e m x h / Q p X h T O C 0 0 E 0 1 J p S N 6 A A 7 l k o a o f Y n 8 0 O n 5 M w q f R L G y p Y 0 Z K 7 + n p j Q S O t x F N j O i J q h X v Z m 4 n 9 e J z X h j T / h M k k N S r Z Y F K a C m J j M v i Z 9 r p A Z M b a E M s X t r Y Q N q a L M 2 G w K N g R v + e V V\n0 q y U v a t y p X 5 Z q t 5 m c e T h B E 7 h H D y 4 h i r c Q w 0 a w A D h G V 7 h z X l 0 X p x 3 5 2 P R m n O y m W P 4 A + f z B 3 P P j L Y = < / l a t e x i t > + NeRF Imagen Transformer Backpropagate onto NeRF weights < l a t e x i t s h a 1 _ b a s e 6 4 = \" + 0 i 1 g I D j y A Q E 0 S B x 1 i i g 5 P / j j J o = \" > A A\nA C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d j B a h b k p S R F 0 W 3 b i s Y B / Q h D K Z T t q h k w c z N 2 I J B T f + i h s X i r j 1 J 9 z 5 N 0 7 a L L T 1 w I X D O f d y 7 z 1 e L L g C y / o 2 C k v L K 6 t r x f X S x u b W 9 o 6 5 u 9 d S U S I p a 9 J I R L L j E c U E D 1 k T O A j W i S U j g S d Y 2 x t d Z 3 7 7 n k n F o / A O x j F z A z I I u c 8 p A S 3 1 z A N H 8 Q A 7 R x g 3 K g 6 w B 0 g p C Z g k k 9 N S z y x b V W s K v E j s n J R R j k b P / H L 6 E U 0 C F g I V R K m u b c X g p k Q C p 4 J N S k 6 i W E z o i A x Y V 9 N Q L 1 J u O v 1 h g k + 0 0 s d + J H W F g K f q 7 4 m U B E q N A 0 9 3 B g S G a t 7 L x P + 8 b g L + p Z v y M E 6 A h X S 2 y E 8 E h g h n g e A + l 4 y C G G t C q O T 6 V k y H R B I K O r Y s B H v + 5 U X S q l X t 8 2 r t 9 q x c v 8 r j K K J D d I w q y E Y X q I 5 u U A M 1 E U W P 6 B m 9 o j f j y X g x 3 o 2 P W W v B y G f 2 0 R 8 Y n z / d D 5 Z l < / l a t e x i t > \u21e0 P (camera) z t , t \u21e0 U(0, 1) < l a t e x i t s h a 1 _ b a s e 6 4 = \" 0 W F U U T n Q B T f Y Q H e L C x b W i s s Y 2 e I = \" > A A A C T H i c b V D P S x t B G J 2 N V m 3 8 F d t j L 4 N B i G D D b l p Q 6 E X s p c c I R o V s W L 6 d f G s G Z 2 e X m W + F u O w f 2 E s P 3 v w r v H h Q S q G T G M G o D w Y e 7 7 1 v 5 p s X 5 0 p a 8 v 1 b r 7 a w + G F p e e V j f X V t f W O z s f X p 1 G a F E d g T m c r M e Q w W l d T Y I 0 k K z 3 O D k M Y K z + L L n x P / 7 A q N l Z k + o X G O g x Q u t E y k A H J S 1 B B h C j S K k / K 6 i m i P U 2 h l y q e a A F X 2 q p a / x 4 N d v s P D E V D 5 H A 4 x t 1 J l u q q i k E Z I 0 J q / Z v y D 0 + 7 X 5 1 T U a P p t f w r + l g Q z 0 m Q z d K P G T T j M R J G i J q H A 2 n 7 g 5 z Q o w Z A U C q t 6 W F j M Q V z C B f Y d 1 Z C i H Z T T M i q + 4 5 Q h T z L j j i Y + V V 9 O l J B a\nO 0 5 j l 5 z s b F 9 7 E / E 9 r 1 9 Q c j A o p c 4 L Q i 2 e H k o K x S n j k 2 b 5 U B o U p M a O g D D S 7 c r F C A w I c v 3 X X Q n B 6 y + / J a e d d v C t 3 T n + 3 j w 8 m t W x w r 6 w b d Z i A d t n h + w X 6 7 I e E + w 3 u 2 M P 7 N H 7 4 9 1 7 f 7 1 / T 9 G a N 5 v 5 z O Z Q W / o P N F S 0 B Q = = < / l a t e x i t > P (light) < l a t e x i t s h a 1 _ b a s e 6 4 = \" + 7 2 l z M p 1 p l 0 c P 4 f X W C n l T u A 4 2 6 g = \" >\nA A A B + H i c b V D L T g J B E J z F F + K D V Y 9 e J h I T v J B d N N E j i R e O m M g j g Q 2 Z H R q Y M P v I T K 8 R N 3 y J F w 8 a 4 9 V P 8 e b f O M A e F K y k k 0 p V d 7 q 7 / F g K j Y 7 z b e U 2 N r e 2 d / K 7 h b 3 9 g 8 O i f X T c 0 l G i O D R 5 J C P V 8 Z k G K U J o o k A J n V g B C 3 w J b X 9 y O / f b D 6 C 0 i M J 7 n M b g B W w U i q H g D I 3 U t 4 u N c g / h E V M p R m O c X f T t k l N x F q D r x M 1 I i W R o 9 O 2 v 3 i D i S Q A h c s m 0 7 r p O j F 7 K F A o u Y V b o J R p i x i d s B F 1 D Q x a A 9 t L F 4 T N 6 b p Q B H U b K V I h 0 o f 6 e S F m g 9 T T w T W f A c K x X v b n 4 n 9 d N c H j j p S K M E 4 S Q L x c N E 0 k x o v M U 6 E A o 4 C i n h j C u h L m V 8 j F T j K P J q m B C c F d f X i e t a s W 9 r F T v r k q 1 e h Z H n p y S M 1 I m L r k m N V I n D d I k n C T k m b y S N + v\nJ e r H e r Y 9 l a 8 7 K Z k 7 I H 1 i f P 6 r u k x 0 = < / l a t e x i t > MLP(\u2022; \u2713) < l a t e x i t s h a 1 _ b a s e 6 4 = \" k r i t e L 7 h t\nd S y 5 R d f W M a v S 6 Y v z z k = \" > A A A C N X i c b V B N S x x B F O z x I 5 r V J K s e c 2 l c h B X M M m M E B S 9 i L j k Y W M F V Y X s Z e n r f O I 0 9 3 U P 3 G 3 E d 5 k 9 5 8 X / k Z A 4 e D C H X / I X 0 r h t I N A U N R V U 9 + r 1 K C i U d h u G 3 Y G Z 2 b v 7 V w u L r x t L y m 7 f v m i u r p 8 6 U V k B P G G X s e c I d K K m h h x I V n B c W e J 4 o O E s u P 4 3 9 s y u w T h p 9 g q M C B j m / 0 D K V g q O X 4 u Y R Q 7 j G 6 s t R t 2 4 z M T S 4 T x l m g H y T b l C W c a w Y F E 4 q o + u Y F Z l s s 5 x j l q T V T R 3 7 L G 7 R 0 e a H P 5 G 4 2 Q o 7 4 Q T 0 J Y m m p E W m 6 M b N r 2 x o R J m D R q G 4 c / 0 o L H B Q c Y t S K K g b r H R Q c H H J L 6 D v q e Y 5 u E E 1 u b q m G 1 4 Z 0 t R Y / z T S i f r 3 R M V z 5 0 Z 5 4 p P j n d 1 z b y z + z + u X m O 4 N K q m L E k G L p 4 / S U l E 0 d F w h H U o L A t X I E y 6 s 9 L t S k X H L B f q i G 7 6 E 6 P n J L 8 n p d i f 6 2 N k + 3 m k d H E 7 r W C T v\ny T p p k 4 j s k g P y m X R J j w h y S + 7 J I / k e 3 A U P w Y / g 5 1 N 0 J p j O r J F / E P z 6 D f s T q 5 A = < / l a t e x i t >\u270f (zt|y; t) < l a t e x i t s h a 1 _ b a s e 6 4 = \" K H 6 J 7 0 0 o H H 4 R W 4 u 5 a d 5 l A 4 K l w p 8 = \" > A A A C Z H i c h V F N a 9 w w E J W d t k m 3 a e s 0 9  \nB Q o o k t g A + 1 i p 4 U W c g n t p Y c E N t B N A q v F y N p x L C J L R h q X O s Z / M r c c e + n v q H a z g X w U M i B 4 v P d m N H r K K i U d x v F V E K 4 8 e f p s d e 1 5 7 8 X 6 y 1 e v o 4 0 3 x 8 7 U V s B Y G G X s a c Y d K K l h j B I V n F Y W e J k p O M n O v 8 / 1 k 1 9 g n T T 6 J z Y V T E t + p m U u B U d P p V H L C o 4 t g 8 p J Z X S X s q q Q A 1 Z y L L K 8 v e h S p M y 3 I 2 3 2 K O 7 Q b c o Q f m N 7 e D D q B k z M D O 5 5 p g D k C + 2 R U X 7 E B 9 r s f L y x p F E / H s a L o g 9 B s g R 9 s q x R G l 2 y m R F 1 C R q F 4 s 5 N k r j C a c s t S q G g 6 7 H a Q c X F O T + D i Y e a l + C m 7 S K k j m 5 7 Z k Z z Y / 3 R S B f s 7 Y 6 W l 8 4 1 Z e a d 8 5 3 d f W 1 O / k + b 1 J h / n b Z S V z W C F t c X 5 b W i a O g 8 c T q T F g S q x g M u r P S 7 U l F w y w X 6 f + n 5 E J L 7 T 3 4 I j n e H y a f h 7 t H n / v 6 3 Z R x r Z I u 8 J w O S k C 9 k n / w g I z I m g v w J V o M o 2 A j + h u v h Z v j 2 2 h o G y 5 5 N c q f C d / 8 A F m q 3 v g = = <\nd + 5 G G 5 u H z t R W w F Q Y Z e x x x h 0 o q W G K E h U c V x Z 4 m S k 4 y k 7 f 9 P r R J 7 B O G v 0 R m w r m J T / R M p e C o 6 f S 6 A s r O L Y M K i e V 0 V 3 K q k K O W M m x y P L 2 r E u R M j + O t N m n u L 3 z q 4 9 u U Y b w G d v 3 7 y b d i I m F w X 3 P F I B 8 u 9 f + 4 + m 9 n t P m t 1 0 a D e N x v C x 6 G S Q r M C S r m q T R V 7 Y w o i 5 B o 1 D c u V k S V z h v u U U p F H Q D V j u o u D j l J z D z U P M S 3 L x d p t X R L c 8 s a G 6 s P x r p k v 1 z o u W l c 0 2 Z + c 5 + Z 3 d R 6 8 l / a b M a 8 1 f z V u q q R t D i / K K 8 V h Q N 7 a O n C 2 l B o G o 8 4 M J K v y s V\nv D l h n R h x d j o n b r q W / o X M B Y = \" > A A A C X X i c b V F N a 9 w w E J X d p E m 2 a b p t D z 3 0 I r o E N t A u d h p o I Z e Q X n J o Y Q P Z J L B a j K w d x y K y Z K R x i W v 8 J 3 t L L / 0 r k T c b y N e A 4 P H e z N P o K S 2 V d B h F 1 0 H 4 Y m X 1 5 d r 6 R u / V 5 u u t N / 2 3 7 0 6 d q a y A i T D K 2 P O U O 1 B S w w Q l K j g v L f A i V X C W X v 7 o 9 L P f Y J 0 0 + g T r E m Y F v 9 A y k 4 K j p 5 I + s p x j c 9 U m r M z l k B U c 8 z R r / r Q J U u b n k N b 7 F H f o N m U I V 9 j 8 + j l u h 0 z M D e 5 7 J g f k C 6 3 z Y F A 6 q Y x + x s p b f K b 1 z p e 7 l q Q / i E b R o u h T E C / B g C x r n P T / s r k R V Q E a h e L O T e O o x F n D L U q h o O 2 x y k H J x S W / g K m H m h f g Z s 0 i n Z Z u e 2 Z O M 2 P 9 0 U g X 7 P 2 J h h f O 1 U X q O 7 u d 3 W O t I 5 / T p h V m 3 2 e N 1 G W F o M X t R V m l K B r a R U 3 n 0 o J A V X v A h Z V + V\ny p y b r l A / y E 9 H 0 L 8 + M l P w e n u K P 4 6 2 j 3 e G x w c L u N Y J x / J J z I k M f l G D s g R G Z M J E e R f Q I K N o B f 8 D 1 f D z X D r t j U M l j P v y Y M K P 9 w A X r C 0 m g = = < / l a t e x i t > Figure 3: DreamFusion generates 3D objects from a natural language caption such as \"a DSLR photo of a peacock on a surfboard.\" The scene is represented by a Neural Radiance Field that is randomly initialized and trained from scratch for each caption. Our NeRF parameterizes volumetric density and albedo (color) with an MLP. We render the NeRF from a random camera, using normals computed from gradients of the density to shade the scene with a random lighting direction. Shading reveals geometric details that are ambiguous from a single viewpoint. To compute parameter updates, DreamFusion diffuses the rendering and reconstructs it with a (frozen) conditional Imagen model to predict the injected noise\u02c6 \u03c6 (z t |y; t). This contains structure that should improve fidelity, but is high variance. Subtracting the injected noise produces a low variance update direction stopgrad[\u02c6 \u03c6 \u2212 ] that is backpropagated through the rendering process to update the NeRF MLP parameters.\nWe name our sampling approach Score Distillation Sampling (SDS) as it is related to distillation, but uses score functions instead of densities. We refer to it as a sampler because the noise in the variational family q(z t | . . .) disappears as t \u2192 0 and the mean parameter of the variational distribution g(\u03b8) becomes the sample of interest. Our loss is easy to implement (see Fig. 8), and relatively robust to the choice of weighting w(t). Since the diffusion model directly predicts the update direction, we do not need to backpropagate through the diffusion model; the model simply acts like an efficient, frozen critic that predicts image-space edits.\nGiven the mode-seeking nature of L SDS , it may be unclear if minimizing this loss will produce good samples. In Fig. 2, we demonstrate that SDS can generate constrained images with reasonable quality. Empirically, we found that setting the guidance weight \u03c9 to a large value for classifier-free guidance improves quality (Appendix Table 9). SDS produces detail comparable to ancestral sampling, but enables new transfer learning applications because it operates in parameter space.", "publication_ref": ["b60", "b36", "b35", "b37", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "THE DREAMFUSION ALGORITHM", "text": "Now that we have demonstrated how a diffusion model can be used as a loss within a generic continuous optimization problem to generate samples, we will construct our specific algorithm that allows us to generate 3D assets from text. For the diffusion model, we use the Imagen model from Saharia et al. (2022), which has been trained to synthesize images from text. We only use the 64 \u00d7 64 base model (not the super-resolution cascade for generating higher-resolution images), and use this pretrained model as-is with no modifications. To synthesize a scene from text, we initialize a NeRF-like model with random weights, then repeatedly render views of that NeRF from random camera positions and angles, using these renderings as the input to our score distillation loss function that wraps around Imagen. As we will demonstrate, simple gradient descent with this approach eventually results in a 3D model (parameterized as a NeRF) that resembles the text. See Fig. 3 for an overview of our approach.", "publication_ref": ["b53"], "figure_ref": [], "table_ref": []}, {"heading": "NEURAL RENDERING OF A 3D MODEL", "text": "NeRF is a technique for neural inverse rendering that consists of a volumetric raytracer and a multilayer perceptron (MLP). Rendering an image from a NeRF is done by casting a ray for each pixel from a camera's center of projection through the pixel's location in the image plane and out into the world. Sampled 3D points \u00b5 along each ray are then passed through an MLP, which produces 4 scalar values as output: a volumetric density \u03c4 (how opaque the scene geometry at that 3D coordinate is) and an RGB color c. These densities and colors are then alpha-composited from the back of the ray towards the camera, producing the final rendered RGB value for the pixel:\nC = i w i c i , w i = \u03b1 i j<i (1 \u2212 \u03b1 j ) , \u03b1 i = 1 \u2212 exp (\u2212\u03c4 i \u00b5 i \u2212 \u00b5 i+1 ) ,(5)\nIn the traditional NeRF use-case we are given a dataset of input images and associated camera positions and the NeRF MLP is trained from random initialization using a mean squared error loss function between each pixel's rendered color and the corresponding ground-truth color from the input image. This yields a 3D model (parameterized by the weights of the MLP) that can produce realistic renderings from previously-unseen views. Our model is built upon mip-NeRF 360 , which is an improved version of NeRF that reduces aliasing. Though mip-NeRF 360 was originally designed for 3D reconstruction from images, its improvements are also helpful for our generative text-to-3D task (see Appendix for details).\nShading. Traditional NeRF models emit radiance, which is RGB color conditioned on the ray direction from which the 3D point is being observed. In contrast, our MLP parameterizes the color of the surface itself, which is then lit by an illumination that we control (a process commonly referred to as \"shading\"). Previous work on generative or multiview 3D reconstruction using NeRF-like models have proposed a variety of reflectance models (Bi et al., 2020;Boss et al., 2021;Pan et al., 2022). We use an RGB albedo \u03c1 (the color of the material) for each point:\n(\u03c4, \u03c1) = MLP (\u00b5; \u03b8) ,(6)\nwhere \u03c4 is volumetric density. Calculating the final shaded output color for the 3D point requires a normal vector indicating the local orientation of the object's geometry. This surface normal vector can be computed by normalizing the negative gradient of density \u03c4 with respect to the 3D coordinate \u00b5: Yariv et al., 2020;. With each normal n and material albedo \u03c1, assuming some point light source with 3D coordinate and color \u03c1 , and an ambient light color a , we render each point along the ray using diffuse reflectance (Lambert, 1760;Ramamoorthi & Hanrahan, 2001) to produce a color c for each point:\nn = \u2212\u2207 \u00b5 \u03c4 / \u2207 \u00b5 \u03c4 (\nc = \u03c1 \u2022 ( \u03c1 \u2022 max (0, n \u2022 ( \u2212 \u00b5)/ \u2212 \u00b5 ) + a ) .(7)\nWith these colors and our previously-generated densities, we approximate the volume rendering integral with the same rendering weights w i used in standard NeRF (Equation 5). As in previous work on text-to-3D generation (Hong et al., 2022;Michel et al., 2021), we find it beneficial to randomly replace the albedo color \u03c1 with white (1, 1, 1) to produce a \"textureless\" shaded output. This prevents the model from producing a degenerate solution in which scene content is drawn onto flat geometry to satisfy the text conditioning. For example, this encourages optimization to yield a 3D squirrel instead of a flat surface containing an image of a squirrel, both of which may appear identical from certain viewing angles and illumination conditions.\nScene Structure. While our method can generate some complex scenes, we find that it is helpful to only query the NeRF scene representation within a fixed bounding sphere, and use an environment map generated from a second MLP that takes positionally-encoded ray direction as input to compute a background color. We composite the rendered ray color on top of this background color using the accumulated alpha value. This prevents the NeRF model from filling up space with density very close to the camera while still allowing it to paint an appropriate color or backdrop behind the generated scene. For generating single objects instead of scenes, a reduced bounding sphere can be useful.\nGeometry regularizers. The mip-NeRF 360 model we build upon contains many other details that we omit for brevity. We include a regularization penalty on the opacity along each ray similar to Jain et al. (2022) to prevent unneccesarily filling in of empty space. To prevent pathologies in the density field where normal vectors face backwards away from the camera we use a modified version of the orientation loss proposed in Ref-NeRF . This penalty is important when including textureless shading as the density field will otherwise attempt to orient normals away from the camera so that the shading becomes darker. Full details on these regularizers and additional hyperparameters of NeRF are presented in the Appendix A.2.", "publication_ref": ["b4", "b6", "b41", "b70", "b46", "b32", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "TEXT-TO-3D SYNTHESIS", "text": "Given a pretrained text-to-image diffusion model, a differentiable image parameterization in the form of a NeRF, and a loss function whose minima are good samples, we have all the components needed for text-to-3D synthesis using no 3D data. For each text prompt, we train a randomly initialized NeRF from scratch. Each iteration of DreamFusion optimization performs the following: (1) randomly sample a camera and light, (2) render an image of the NeRF from that camera and shade with the light, (3) compute gradients of the SDS loss with respect to the NeRF parameters, (4) update the NeRF parameters using an optimizer. We detail each of these steps below, and present pseudocode in Appendix 8.\n1. Random camera and light sampling. At each iteration, a camera position is randomly sampled in spherical coordinates, with elevation angle \u03c6 cam \u2208 [\u221210\u00b0, 90\u00b0], azimuth angle \u03b8 cam \u2208 [0\u00b0, 360\u00b0], and distance from the origin in [1, 1.5]. For reference, the scene bounding sphere described previously has radius 1.4. We also sample a \"look-at\" point around the origin and an \"up\" vector, and combine these with the camera position to create a camera pose matrix. We additionally sample a focal length multiplier \u03bb focal \u2208 U(0.7, 1.35) such that the focal length is \u03bb focal w, where w = 64 is the image width in pixels. The point light position is sampled from a distribution centered around the camera position. We found using a wide range of camera locations to be critical for synthesizing coherent 3D scenes, and a wide range of camera distances helps to improve the resolution of the learned scene.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "Rendering. Given the camera pose and light position, we render the shaded NeRF model at 64 \u00d7 64 resolution as described in Section 3.1. We randomly choose between the illuminated color render, a textureless render, and a rendering of the albedo without any shading.\n3. Diffusion loss with view-dependent conditioning. Text prompts often describe canonical views of an object that are not good descriptions when sampling different views. We therefore found it beneficial to append view-dependent text to the provided input text based on the location of the randomly sampled camera. For high elevation angles \u03c6 cam > 60\u00b0, we append \"overhead view.\" For \u03c6 cam \u2264 60\u00b0, we use a weighted combination of the text embeddings for appending \"front view,\" \"side view,\" or \"back view\" depending on the value of the azimuth angle \u03b8 cam (see App. A.2 for details). We use the pretrained 64 \u00d7 64 base text-to-image model from Saharia et al. (2022). This model was trained on large-scale web-image-text data, and is conditioned on T5-XXL text embeddings (Raffel et al., 2020). We use a weighting function of w(t) = \u03c3 2 t , but found that a uniform weighting performed similarly. We sample t \u223c U(0.02, 0.98), avoiding very high and low noise levels due to numerical instabilities. For classifier-free guidance, we set \u03c9 = 100, finding that higher guidance weights give improved sample quality. This is much larger than image sampling methods, and is likely required due to the mode-seeking nature of our objective which results in oversmoothing at small guidance weights (see Appendix Table. 9). Given the rendered image and sampled timestep t, we sample noise and compute the gradient of the NeRF parameters according to Eqn. 3.", "publication_ref": ["b53", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Optimization.", "text": "Our 3D scenes are optimized on a TPUv4 machine with 4 chips. Each chip renders a separate view and evaluates the diffusion U-Net with per-device batch size of 1. We optimize for 15,000 iterations which takes around 1.5 hours. Compute time is split evenly between rendering the NeRF and evaluating the diffusion model. Parameters are optimized using the Distributed Shampoo optimizer (Anil et al., 2020). See Appendix A.2 for optimization settings.  ", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "We evaluate the ability of DreamFusion to generate coherent 3D scenes from a variety of text prompts. We compare to existing zero-shot text-to-3D generative models, identify the key components of our model that enable accurate 3D geometry, and explore the qualitative capabilities of DreamFusion such as the compositional generation shown in Figure 4. We present a large gallery of 3D assets, extended videos, and meshes at dreamfusion3d.github.io.\n3D reconstruction tasks are typically evaluated using reference-based metrics like Chamfer Distance, which compares recovered geometry to some ground truth. The view-synthesis literature often uses PSNR to compare rendered views with a held-out photograph. These reference-based metrics are difficult to apply to zero-shot text-to-3D generation, as there is no \"true\" 3D scene corresponding to our text prompts. Following Jain et al. (2022), we evaluate the CLIP R-Precision, an automated metric for the consistency of rendered images with respect to the input caption. The R-Precision is the accuracy with which CLIP (Radford et al., 2021) retrieves the correct caption among a set of distractors given a rendering of the scene. We use the 153 prompts from the object-centric COCO validation subset of Dream Fields. We also measure R-Precision on textureless renders to evaluate geometry since we found existing metrics do not capture the quality of the geometry, often yielding high values when texture is painted on flat geometry. Ablations. Fig. 6 shows CLIP R-Precision for a simplified DreamFusion ablation and progressively adds in optimization choices: a large ranges of viewpoints (ViewAug), view-dependent prompts (ViewDep), optimizing illuminated renders in addition to unlit albedo color renders (Lighting), and optimizing textureless shaded geometry images (Textureless). We measure R-Precision on the albedo render as in baselines (left), the full shaded render (middle) and the textureless render (right) to check geometric quality. Geometry significantly improves with each of these choices and full renderings improve by +12.5%. Fig. 6 shows qualitative results for the ablation. This ablation also highlights how the albedo renders can be deceiving: our base model achieves the highest score, but exhibits poor geometry (the dog has multiple heads). Recovering accurate geometry requires view-dependent prompts, illumination and textureless renders.\nAlbedo Shaded Textureless Render Type\n0.0 0.2 0.4 0.6 0.8 1.0 R-Precision Base +ViewAug (i) +ViewDep (ii) +Lighting (iii) +Textureless (iv) (i) (ii) (iii) (iv)\nFigure 6: An ablation study of DreamFusion. Left: We evaluate components of our unlit renderings on albedo, full shaded and illuminated renderings and textureless illuminated geometry using CLIP L/14 on object-centric COCO. Right: visualizations of the impact of each ablation for \"A bulldog is wearing a black pirate hat.\" on albedo (top), shaded (middle), and textureless renderings (bottom). The base method (i) without view-dependent prompts results in a multi-faced dog with flat geometry. Adding in view-dependent prompts (ii) improves geometry, but the surfaces are highly non-smooth and result in poor shaded renders. Introducing lighting (iii) improves geometry but darker areas (e.g. the hat) remain non-smooth. Rendering without color (iv) helps to smooth the geometry, but also causes some color details like the skull and crossbones to be \"carved\" into the geometry.", "publication_ref": ["b24"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "DISCUSSION", "text": "We have presented DreamFusion, an effective technique for text-to-3D synthesis for a wide range of text prompts. DreamFusion works by transferring scalable, high-quality 2D image diffusion models to the 3D domain through our use of a novel Score Distillation Sampling approach and a novel NeRF-like rendering engine. DreamFusion does not require 3D or multi-view training data, and uses only a pre-trained 2D diffusion model (trained on only 2D images) to perform 3D synthesis.\nThough DreamFusion produces compelling results and outperforms prior work on this task, it still has several limitations. SDS is not a perfect loss function when applied to image sampling, and often produces oversaturated and oversmoothed results relative to ancestral sampling. While dynamic thresholding (Saharia et al., 2022) partially ameliorates this issue when applying SDS to images, it did not resolve this issue in a NeRF context. Additionally, 2D image samples produced using SDS tend to lack diversity compared to ancestral sampling, and our 3D results exhibit few differences across random seeds. This may be fundamental to our use of reverse KL divergence, which has been previously noted to have mode-seeking properties in the context of variational inference and probability density distillation.\nDreamFusion uses the 64 \u00d7 64 Imagen model, and as such our 3D synthesized models tend to lack fine details. Using a higher-resolution diffusion model and a bigger NeRF would presumably address this, but synthesis would become impractically slow. Hopefully improvements in the efficiency of diffusion and neural rendering will enable tractable 3D synthesis at high resolution in the future.\nThe problem of 3D reconstruction from 2D observations is widely understood to be highly ill-posed, and this ambiguity has consequences in the context of 3D synthesis. Fundamentally, our task is hard for the same reason that inverse rendering is hard: there exist many possible 3D worlds that result in identical 2D images. The optimization landscape of our task is therefore highly non-convex, and many of the details of this work are designed specifically to sidestep these local minima. But despite our best efforts we still sometimes observe local minima, such as 3D reconstructions where all scene content is \"painted\" onto a single flat surface. Though the techniques presented in this work are effective, this task of \"lifting\" 2D observations into a 3D world is inherently ambiguous, and may benefit from more robust 3D priors.", "publication_ref": ["b53"], "figure_ref": [], "table_ref": []}, {"heading": "ETHICS STATEMENT", "text": "Generative models for synthesizing images carry with them several ethical concerns, and these concerns are shared by (or perhaps exacerbated in) 3D generative models such as ours.\nBecause DreamFusion uses the Imagen diffusion model as a prior, it inherits any problematic biases and limitations that Imagen may have. While Imagen's dataset was partially filtered, the LAION-400M (Schuhmann et al., 2022) subset of its data was found to contain undesirable images (Birhane et al., 2021). Imagen is also conditioned on features from a pretrained large language model, which itself may have unwanted biases. It is important to be careful about the contents of datasets that are used in text-to-image and image-to-3D models so as to not propagate hateful media.\nGenerative models, in the hands of bad actors, could be used to generate disinformation. Disinformation in the form of 3D objects may be more convincing than 2D images (though renderings of our synthesized 3D models are less realistic than the state of the art in 2D image synthesis).\nGenerative models such as ours may have the potential to displace creative workers via automation. That said, these tools may also enable growth and improve accessibility for the creative industry.", "publication_ref": ["b55", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "REPRODUCIBILITY STATEMENT", "text": "The mip-NeRF 360 model that we build upon is publicly available through the \"MultiNeRF\" code repository . While the Imagen diffusion model is not publicly available, other conditional diffusion models may produce similar results with the DreamFusion algorithm. To aid reproducibility, we have included a schematic overview of the algorithm in Figure 3, pseudocode for Score Distillation Sampling in Figure 8, hyperparameters in Appendix A. Figure 8: Pseudocode for Score Distillation Sampling with an application-specific generator that defines a differentiable mapping from parameters to images. The gradient g is computed without backpropagating through the diffusion model's U-Net. We used the stopgradient operator to express the loss, but the parameter update can also be easily computed with an explicit VJP: g = matmul(weight(t) * (-epshat t -eps), grad(x, params)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 NERF DETAILS AND TRAINING HYPERPARAMETERS", "text": "Our model builds upon mip-NeRF 360  (starting from the publicly available implementation 2022), which is an improved version of NeRF . The main modification this model makes to NeRF is in how 3D point information is passed to the NeRF MLP. In NeRF, each 3D input point is mapped to a higher dimensional space using a sinusoidal positional encoding function (Vaswani et al., 2017). In mip-NeRF, this is replaced by an integrated positional encoding that accounts for the \"width\" of the ray being rendered (based on its pixel footprint in the image plane) and the length of each interval [d i , d i+1 ] sampled along the ray . This allows each interval along a ray to be represented as a Gaussian distribution with mean \u00b5 and covariance matrix \u03a3 that approximates the interval's 3D volume.\nMip-NeRF covariance annealing. As in mip-NeRF, each mean \u00b5 is the 3D coordinate of the center of the ray interval, but unlike mip-NeRF we do not use a covariance derived from camera geometry, but instead define each \u03a3 as:\n\u03a3 = \u03bb 2 \u03a3 I 3 (8)\nWhere \u03bb \u03a3 is a scale parameter that is linearly annealed from a large value to a small value during training. Representative settings are 5 \u00d7 10 \u22122 and 2 \u00d7 10 \u22123 for the initial and final values of \u03bb \u03a3 , linearly annealed for the first 5k steps of optimization (out of 15k total). This \"coarse to fine\" annealing of a scale parameter has a similar effect as the annealing used by Park et al. (2021) but uses integrated positional encoding instead of traditional positional encoding. The underlying sinusoidal positional encoding function uses frequencies 2 0 , 2 1 , . . . , 2 L\u22121 , where we set L = 8.\nMLP architecture changes. Our NeRF MLP consists of 5 ResNet blocks (He et al., 2016) with 128 hidden units, Swish/SiLU activation (Hendrycks & Gimpel, 2016), and layer normalization (Ba et al., 2016) between blocks. We use an exp activation to produce density \u03c4 and a sigmoid activation to produce RGB albedo \u03c1.\nShading hyperparameters. For the first 1k steps of optimization we set the ambient light color a to 1 and the diffuse light color \u03c1 to 0, which effectively disables diffuse shading. For the remaining steps we set a = [0.1, 0.1, 0.1] and \u03c1 = [0.9, 0.9, 0.9] with probability 0.75, otherwise a = 1, \u03c1 = 0, i.e. we use diffuse shading 75% of the time. When shading is on ( \u03c1 > 0), we choose textureless shading (\u03c1 = 1) with probability 0.5.", "publication_ref": ["b64", "b42", "b15", "b16", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Spatial density bias.", "text": "To aid in the early stages of optimization, we add a small \"blob\" of density around the origin to the output of the MLP. This helps focus scene content at the center of the 3D coordinate space, rather than directly next to the sampled cameras. We use a Gaussian PDF to parameterize the added density:\n\u03c4 init (\u00b5) = \u03bb \u03c4 \u2022 exp \u2212 \u00b5 2 2\u03c3 2 \u03c4 .(9)\nRepresentative settings are \u03bb \u03c4 = 5 for the scale parameter and \u03c3 \u03c4 = 0.2 for the width parameter. This density is added to the \u03c4 output of the NeRF MLP.\nAdditional camera and light sampling details. Uniformly sampling camera elevation \u03c6 cam in angular space does not produce uniform samples over the surface of the sphere -the area around the pole is oversampled. We found this bias to be helpful in practice, so we sample \u03c6 cam from this biased distribution with probability 0.5, otherwise we sample from a true uniform-in-area distribution on a half-sphere. The sampled camera position is perturbed by a small uniform offset U(\u22120.1, 0.1) 3 . The \"look-at\" point is sampled from N (0, 0.2I) and the default \"up\" vector is perturbed by noise sampled from N (0, 0.02I). This noise acts as an additional augmentation, and ensures a wider diversity of viewpoints are seen during training.\nWe separately sample the direction and norm of the light position vector . To sample the direction, we sample from N (p cam , I) where p cam is the camera position (this ensures that the point light usually ends up on the same side of the object as the camera). The norm is sampled from U(0.8, 1.5), while p cam \u223c U(1.0, 1.5).\nRegularizer hyperparameters. We use the orientation loss proposed by Ref-NeRF  to encourage normal vectors of the density field to face toward the camera when they are visible (so that the camera does not observe geometry that appears to face \"backwards\" when shaded). We place a stop-gradient on the rendering weights w i , which helps prevent unintended local minima where the generated object shrinks or disappears:\nL orient = i stop grad(w i ) max(0, n i \u2022 v) 2 ,(10)\nwhere v is the direction of the ray (the viewing direction). We also apply a small regularization to the accumulated alpha value (opacity) along each ray: L opacity = ( i w i ) 2 + 0.01. This discourages optimization from unnecessarily filling in empty space, and improves foreground/background separation.\nFor orientation loss L orient , we find reasonable weights to lie in [10 \u22121 , 10 \u22123 ]. If orientation loss is too high, surfaces become oversmoothed. In most experiments, we set the weight to 10 \u22122 . This weight is annealed in starting from 10 \u22124 over the first 5k (out of 15k) steps. For accumulated alpha loss L opacity , we find reasonable weights to lie in [10 \u22123 , 5 \u00d7 10 \u22123 ].\nView-dependent prompting. We interpolate between front/side/back view prompt augmentations based on which quadrant contains the sampled azimuth \u03b8 cam . We experimented with different ways of interpolating the text embeddings, but found that simply taking the text embedding closest to the sampled azimuth worked well.\nOptimizer. We use Distributed Shampoo (Anil et al., 2020) with \u03b2 1 = 0.9, \u03b2 2 = 0.9, exponent override = 2, block size = 128, graft type = SQRT N, = 10 \u22126 , and a linear warmup of learning rate over 3000 steps from 10 \u22129 to 10 \u22124 followed by cosine decay down to 10 \u22126 . We found this long warmup period to be helpful for improving the coherence of generated geometry.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 EXPERIMENTAL SETUP", "text": "Our computation of R-Precision differs slightly from baselines. As mentioned, CLIP-based text-to-3D systems are prone to overfitting the evaluation CLIP R-Precision metric since the model used for training is similar to the evaluation model. To minimize this problem, Dream Fields (Jain et al., 2022) and CLIP-Mesh (Khalid et al., 2022) evaluate renderings at a single held out view at a 45 \u2022 elevation, higher than is seen during training (maximum 30 \u2022 ). DreamFusion evaluates at 30 \u2022 since it is not prone to this issue, but averages the metric over multiple azimuths to reduce variance. In our main results in Table 1, we evaluate all captions with 2 generation seeds unless otherwise noted.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "A.4 DERIVING THE SCORE DISTILLATION SAMPLING LOSS AND GRADIENTS", "text": "The score distillation sampling loss L SDS presented in Eqn. 4 was inspired by work on probability density distillation Ping et al., 2019;van den Oord et al., 2018). We use this loss to find modes of the score functions that are present across all noise levels in the diffusion process.\nHere we show how the gradient of this loss leads to the same update as optimizing the training loss L Diff , but without the term corresponding to the Jacobian of the diffusion U-Net. First, we consider gradients with respect to a single KL term: KL(q(z t |x = g(\u03b8)) p \u03c6 (z t |y)) = E [log q(z t |x = g(\u03b8)) \u2212 log p \u03c6 (z t |y)]\n\u2207 \u03b8 KL(q(z t |x = g(\u03b8)) p \u03c6 (z t |y)) = E \u2207 \u03b8 log q(z t |x = g(\u03b8))\n\u2212 \u2207 \u03b8 log p \u03c6 (z t |y)\n(B)(12)\nThe second term (B) can be related to\u02c6 by the chain rule, relying on s \u03c6 (z t |y) \u2248 \u2207 zt log p \u03c6 (z t |y): The first term (A) is the gradient of the entropy of the forward process with respect to the mean parameter, holding the variance fixed. Because of the fixed variance, the entropy is constant for a given t and the total gradient of (A) is 0. However, we can still write out its gradient in terms of the \"score function\" (the gradient of the log probability with respect to parameters) and path derivative (the gradient of the log probability with respect to the sample): Sticking-the-Landing (Roeder et al., 2017) shows that keeping the path derivative gradient while discarding the score function gradient can lead to reduced variance as the path derivative term can be correlated with other terms in the loss. Here, the other term (B) corresponds to a prediction of , which is certainly correlated with the RHS of equation 14. Putting these together, we can use a \"sticking-the-landing\"-style gradient of our loss by thinking of as a control variate for\u02c6 :\n\u2207\n\u2207 \u03b8 L SDS = E t,zt|x w(t)\n\u03c3 t \u03b1 t \u2207 \u03b8 KL(q(z t |x = g(\u03b8)) p \u03c6 (z t |y))\n= E t, w(t) (\u02c6 (z t |y) \u2212 ) \u2202x \u2202\u03b8 .\nIn practice, we find that including in the gradient leads to lower-variance gradients that speed up optimization and can produce better final results.\nIn related work, Graikos et al. (2022) also sample diffusion models by optimization, thereby allowing parameterized samples. Their divergence KL(h(x) p \u03c6 (x|y)) reduces to the loss E ,t \u2212\u02c6 \u03b8 (z t |y; t) 2 2 \u2212 log c(x, y). The squared error requires costly backpropagation through the diffusion model\u02c6 \u03b8 , unlike SDS. DDPM-PnP also uses an auxiliary classifier c, while we use CFG.\nA few other works have updates resembling Score Distillation Sampling for different applications. Gradients of the entropy of an implicit model have been estimated with an amortized score model at a single noise level (Lim et al., 2020), though that work does not use our control variate based on subtracting the noise from\u02c6 . SDS could also ease optimization by using multiple noise levels. GAN-like amortized samplers can be learned by minimizing the Stein discrepancy (Hu et al., 2018;Grathwohl et al., 2020), where the optimal critic resembles the difference of scores in our loss (Eqn. 3).", "publication_ref": ["b43", "b63", "b50", "b12", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "A.5 IMPACT OF SEED AND GUIDANCE WEIGHT", "text": "We find that large guidance weights are important for learning high-quality 3D models. Unlike image synthesis models that use guidance weights \u03c9 \u2208 [5, 30], DreamFusion uses weights up \u03c9 = 100, and works at even larger guidance scales without severe artifacts. This may be due to the constrained nature of the optimization problem: colors output by our MLP are bounded to [0, 1] by a sigmoid nonlinearity, whereas image samplers need clipping.\nWe also find that our method does not yield large amounts of diversity across random seeds. This is likely due to the mode-seeking properties of L SDS combined with the fact that at high noise levels, the smoothed densities may not have many distinct modes. Understanding the interplay between guidance strength, diversity, and loss functions remains an important open direction for future research. Figure 9: A 2D sweep over guidance weights and random seeds for two different prompts (\"a zoomed out DSLR photo of a robot couple fine dining\" and \"a DSLR photo of a chimpanzee dressed like Henry VIII king of England\").", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Scalable second order optimization for deep learning", "journal": "", "year": "2020", "authors": "Rohan Anil; Vineet Gupta; Tomer Koren; Kevin Regan; Yoram Singer"}, {"ref_id": "b1", "title": "Layer normalization", "journal": "", "year": "2016", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hinton"}, {"ref_id": "b2", "title": "Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields. ICCV", "journal": "", "year": "2021", "authors": "T Jonathan; Ben Barron; Matthew Mildenhall; Peter Tancik; Ricardo Hedman;  Martin-Brualla;  Srinivasan"}, {"ref_id": "b3", "title": "Unbounded anti-aliased neural radiance fields. CVPR", "journal": "", "year": "2022", "authors": "Jonathan T Barron; Ben Mildenhall; Dor Verbin; P Pratul; Peter Srinivasan;  Hedman"}, {"ref_id": "b4", "title": "Neural reflectance fields for appearance acquisition", "journal": "", "year": "2020", "authors": "Sai Bi; Zexiang Xu; Pratul Srinivasan; Ben Mildenhall; Kalyan Sunkavalli; Milo\u0161 Ha\u0161an; Yannick Hold-Geoffroy; David Kriegman; Ravi Ramamoorthi"}, {"ref_id": "b5", "title": "Multimodal datasets: misogyny, pornography, and malignant stereotypes", "journal": "", "year": "2021", "authors": "Abeba Birhane; Emmanuel Vinay Uday Prabhu;  Kahembwe"}, {"ref_id": "b6", "title": "NeRD: Neural reflectance decomposition from image collections. ICCV", "journal": "", "year": "2021", "authors": "Mark Boss; Raphael Braun; Varun Jampani; Jonathan T Barron; Ce Liu; Hendrik P A Lensch"}, {"ref_id": "b7", "title": "Learning gradient fields for shape generation", "journal": "ECCV", "year": "2020", "authors": "Ruojin Cai; Guandao Yang; Hadar Averbuch-Elor; Zekun Hao; Serge Belongie; Noah Snavely; Bharath Hariharan"}, {"ref_id": "b8", "title": "Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. arXiv", "journal": "", "year": "2021", "authors": "Eric R Chan; Connor Z Lin; Matthew A Chan; Koki Nagano; Boxiao Pan; Orazio Shalini De Mello;  Gallo"}, {"ref_id": "b9", "title": "Periodic implicit generative adversarial networks for 3d-aware image synthesis. CVPR", "journal": "", "year": "2021", "authors": "Eric R Chan; Marco Monteiro; Petr Kellnhofer; Jiajun Wu; Gordon Wetzstein;  Pi-Gan"}, {"ref_id": "b10", "title": "Text2shape: Generating shapes from natural language by learning joint embeddings", "journal": "", "year": "2018", "authors": "Kevin Chen; B Christopher; Manolis Choy;  Savva; X Angel; Thomas Chang; Silvio Funkhouser;  Savarese"}, {"ref_id": "b11", "title": "Estimating gradients for waveform generation", "journal": "", "year": "2020", "authors": "Nanxin Chen; Yu Zhang; Heiga Zen; Ron J Weiss; Mohammad Norouzi; William Chan;  Wavegrad"}, {"ref_id": "b12", "title": "Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors", "journal": "", "year": "2022", "authors": "Alexandros Graikos; Nikolay Malkin"}, {"ref_id": "b13", "title": "Learning the stein discrepancy for training and evaluating energy-based models without sampling", "journal": "", "year": "", "authors": "Will Grathwohl; Kuan-Chieh Wang; Joern-Henrik Jacobsen; David Duvenaud; Richard Zemel"}, {"ref_id": "b14", "title": "StyleNeRF: A style-based 3d-aware generator for high-resolution image synthesis", "journal": "", "year": "2021", "authors": "Jiatao Gu; Lingjie Liu; Peng Wang; Christian Theobalt"}, {"ref_id": "b15", "title": "Deep residual learning for image recognition", "journal": "CVPR", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b16", "title": "Gaussian error linear units (GELUs)", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b17", "title": "Escaping Plato's Cave: 3D shape from adversarial rendering", "journal": "ICCV", "year": "2019", "authors": "Philipp Henzler; J Niloy; Tobias Mitra;  Ritschel"}, {"ref_id": "b18", "title": "Classifier-free diffusion guidance", "journal": "", "year": "2022", "authors": "Jonathan Ho; Tim Salimans"}, {"ref_id": "b19", "title": "Denoising diffusion probabilistic models", "journal": "NeurIPS", "year": "2020", "authors": "Jonathan Ho; Ajay Jain; Pieter Abbeel"}, {"ref_id": "b20", "title": "Video diffusion models", "journal": "", "year": "2022", "authors": "Jonathan Ho; Tim Salimans; Alexey Gritsenko; William Chan; Mohammad Norouzi; David J Fleet"}, {"ref_id": "b21", "title": "AvatarCLIP: Zero-shot text-driven generation and animation of 3D avatars", "journal": "ACM Transactions on Graphics", "year": "", "authors": "Fangzhou Hong; Mingyuan Zhang; Liang Pan; Zhongang Cai; Lei Yang; Ziwei Liu"}, {"ref_id": "b22", "title": "Stein neural sampler", "journal": "", "year": "2018", "authors": "Tianyang Hu; Zixiang Chen; Hanxi Sun; Jincheng Bai; Mao Ye; Guang Cheng"}, {"ref_id": "b23", "title": "Probability distillation: A caveat and alternatives", "journal": "UAI", "year": "2019", "authors": "Chin-Wei Huang; Faruk Ahmed; Kundan Kumar; Alexandre Lacoste; Aaron C Courville"}, {"ref_id": "b24", "title": "Zero-shot text-guided object generation with dream fields", "journal": "CVPR", "year": "2022", "authors": "Ajay Jain; Ben Mildenhall; Jonathan T Barron; Pieter Abbeel; Ben Poole"}, {"ref_id": "b25", "title": "Text-controlled creation of 3d textured meshes", "journal": "", "year": "2021", "authors": "Nikolay Jetchev;  Clipmatrix"}, {"ref_id": "b26", "title": "CLIP-Mesh: Generating textured meshes from text using pretrained image-text models", "journal": "", "year": "2022", "authors": "Tianhao Nasir Mohammad Khalid; Eugene Xie; Popa Belilovsky;  Tiberiu"}, {"ref_id": "b27", "title": "Variational diffusion models", "journal": "NeurIPS", "year": "2021", "authors": "P Diederik; Tim Kingma; Ben Salimans; Jonathan Poole;  Ho"}, {"ref_id": "b28", "title": "Diffwave: A versatile diffusion model for audio synthesis", "journal": "ICLR", "year": "2021", "authors": "Zhifeng Kong; Wei Ping; Jiaji Huang; Kexin Zhao; Bryan Catanzaro"}, {"ref_id": "b29", "title": "Photometria sive de mensura et gradibus luminis, colorum et umbrae. sumptibus vidvae E. Klett, typis CP Detleffsen", "journal": "", "year": "", "authors": "Johann Heinrich; Lambert "}, {"ref_id": "b30", "title": "AR-DAE: towards unbiased neural entropy gradient estimation", "journal": "", "year": "", "authors": "Jae Hyun Lim; Aaron C Courville; Christopher J Pal; Chin-Wei Huang"}, {"ref_id": "b31", "title": "Towards implicit text-guided 3d shape generation. CVPR", "journal": "", "year": "2022", "authors": "Zhengzhe Liu; Yi Wang; Xiaojuan Qi; Chi-Wing Fu"}, {"ref_id": "b32", "title": "Text-driven neural stylization for meshes", "journal": "", "year": "2021", "authors": "Oscar Michel; Roi Bar-On; Richard Liu; Sagie Benaim; Rana Hanocka"}, {"ref_id": "b33", "title": "Representing scenes as neural radiance fields for view synthesis. ECCV", "journal": "", "year": "2020", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng;  Nerf"}, {"ref_id": "b34", "title": "MultiNeRF: A Code Release for Mip-NeRF 360, Ref-NeRF, and RawNeRF", "journal": "", "year": "2022", "authors": "Ben Mildenhall; Dor Verbin; P Pratul; Peter Srinivasan; Ricardo Hedman; Jonathan T Martin-Brualla;  Barron"}, {"ref_id": "b35", "title": "Inceptionism: Going deeper into neural networks", "journal": "", "year": "2015", "authors": "Alexander Mordvintsev; Christopher Olah; Mike Tyka"}, {"ref_id": "b36", "title": "Differentiable image parameterizations. Distill", "journal": "", "year": "2018", "authors": "Alexander Mordvintsev; Nicola Pezzotti; Ludwig Schubert; Chris Olah"}, {"ref_id": "b37", "title": "Do deep generative models know what they don't know?", "journal": "", "year": "2018", "authors": "Eric Nalisnick; Akihiro Matsukawa; Yee Whye Teh; Dilan Gorur; Balaji Lakshminarayanan"}, {"ref_id": "b38", "title": "HoloGAN: Unsupervised learning of 3d representations from natural images. ICCV", "journal": "", "year": "2019", "authors": "Thu Nguyen-Phuoc; Chuan Li; Lucas Theis; Christian Richardt; Yong-Liang Yang"}, {"ref_id": "b39", "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models", "journal": "", "year": "", "authors": "Alex Nichol; Prafulla Dhariwal; Aditya Ramesh; Pranav Shyam; Pamela Mishkin; Bob Mcgrew; Ilya Sutskever; Mark Chen"}, {"ref_id": "b40", "title": "StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. CVPR", "journal": "", "year": "2022", "authors": "Roy Or-El; Xuan Luo; Mengyi Shan; Eli Shechtman; Jeong Joon Park; Ira Kemelmacher-Shlizerman"}, {"ref_id": "b41", "title": "Gan2x: Non-lambertian inverse rendering of image gans", "journal": "", "year": "2022", "authors": "Xingang Pan; Ayush Tewari; Lingjie Liu; Christian Theobalt"}, {"ref_id": "b42", "title": "Nerfies: Deformable neural radiance fields. ICCV", "journal": "", "year": "2021", "authors": "Keunhong Park; Utkarsh Sinha; Jonathan T Barron; Sofien Bouaziz; Dan B Goldman; M Steven; Ricardo Seitz;  Martin-Brualla"}, {"ref_id": "b43", "title": "ClariNet: Parallel wave generation in end-to-end text-tospeech", "journal": "ICLR", "year": "2019", "authors": "Wei Ping; Kainan Peng; Jitong Chen"}, {"ref_id": "b44", "title": "Learning transferable visual models from natural language supervision", "journal": "", "year": "", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"ref_id": "b45", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b46", "title": "A signal-processing framework for inverse rendering", "journal": "SIG-GRAPH", "year": "2001", "authors": "Ravi Ramamoorthi; Pat Hanrahan"}, {"ref_id": "b47", "title": "Zero-shot text-to-image generation", "journal": "", "year": "", "authors": "Aditya Ramesh; Mikhail Pavlov; Gabriel Goh; Scott Gray; Chelsea Voss; Alec Radford; Mark Chen; Ilya Sutskever"}, {"ref_id": "b48", "title": "Hierarchical textconditional image generation with clip latents", "journal": "", "year": "2022", "authors": "Aditya Ramesh; Prafulla Dhariwal; Alex Nichol; Casey Chu; Mark Chen"}, {"ref_id": "b49", "title": "An Empirical Bayes Approach to Statistics", "journal": "Springer", "year": "1992", "authors": "Herbert E Robbins"}, {"ref_id": "b50", "title": "Sticking the landing: Simple, lower-variance gradient estimators for variational inference", "journal": "", "year": "2017", "authors": "Geoffrey Roeder; Yuhuai Wu; David Duvenaud"}, {"ref_id": "b51", "title": "Palette: Image-to-image diffusion models", "journal": "", "year": "2021", "authors": "Chitwan Saharia; William Chan; Huiwen Chang; Chris A Lee; Jonathan Ho; Tim Salimans; David J Fleet; Mohammad Norouzi"}, {"ref_id": "b52", "title": "Image super-resolution via iterative refinement", "journal": "", "year": "2021", "authors": "Chitwan Saharia; Jonathan Ho; William Chan; Tim Salimans; David J Fleet; Mohammad Norouzi"}, {"ref_id": "b53", "title": "Photorealistic text-to-image diffusion models with deep language understanding", "journal": "", "year": "2022", "authors": "Chitwan Saharia; William Chan; Saurabh Saxena; Lala Li; Jay Whang; Emily Denton; Seyed Kamyar Seyed;  Ghasemipour; S Sara Burcu Karagol Ayan; Rapha Gontijo Mahdavi; Tim Lopes; Jonathan Salimans;  Ho; J David; Mohammad Fleet;  Norouzi"}, {"ref_id": "b54", "title": "CLIP-Forge: Towards zero-shot text-to-shape generation", "journal": "CVPR", "year": "2022", "authors": "Aditya Sanghi; Hang Chu; Ye Joseph G Lambourne; Chin-Yi Wang; Marco Cheng;  Fumero"}, {"ref_id": "b55", "title": "LAION-5b: An open large-scale dataset for training next generation image-text models", "journal": "", "year": "2022", "authors": "Christoph Schuhmann; Romain Beaumont; W Cade; Ross Gordon; Theo Wightman; Aarush Coombes; Clayton Katta; Patrick Mullis;  Schramowski; R Srivatsa; Katherine Kundurthy; Richard Crowson; Ludwig Vencu; Robert Schmidt; Jenia Kaczmarczyk;  Jitsev"}, {"ref_id": "b56", "title": "Generative radiance fields for 3d-aware image synthesis. NeurIPS", "journal": "", "year": "2020", "authors": "Katja Schwarz; Yiyi Liao; Michael Niemeyer; Andreas Geiger;  Graf"}, {"ref_id": "b57", "title": "Deep unsupervised learning using nonequilibrium thermodynamics", "journal": "ICML", "year": "2015", "authors": "Jascha Sohl-Dickstein; Eric Weiss; Niru Maheswaranathan; Surya Ganguli"}, {"ref_id": "b58", "title": "Denoising diffusion implicit models. CoRR, abs", "journal": "", "year": "2010", "authors": "Jiaming Song; Chenlin Meng; Stefano Ermon"}, {"ref_id": "b59", "title": "Generative modeling by estimating gradients of the data distribution", "journal": "NeurIPS", "year": "2019", "authors": "Yang Song; Stefano Ermon"}, {"ref_id": "b60", "title": "Score-based generative modeling through stochastic differential equations", "journal": "ICLR", "year": "2021", "authors": "Yang Song; Jascha Sohl-Dickstein; P Diederik; Abhishek Kingma; Stefano Kumar; Ben Ermon;  Poole"}, {"ref_id": "b61", "title": "NeRV: Neural reflectance and visibility fields for relighting and view synthesis. CVPR", "journal": "", "year": "2021", "authors": "P Pratul; Boyang Srinivasan; Xiuming Deng; Matthew Zhang; Ben Tancik; Jonathan T Mildenhall;  Barron"}, {"ref_id": "b62", "title": "Advances in neural rendering", "journal": "Computer Graphics Forum", "year": "2022", "authors": "Ayush Tewari; Justus Thies; Ben Mildenhall; Pratul Srinivasan; Edgar Tretschk; Christoph Yifan; Vincent Lassner; Ricardo Sitzmann; Stephen Martin-Brualla;  Lombardi"}, {"ref_id": "b63", "title": "Belov, and Demis Hassabis. Parallel WaveNet: Fast high-fidelity speech synthesis. ICML", "journal": "", "year": "2018", "authors": "A\u00e4ron Van Den Oord; Yazhe Li; Igor Babuschkin; Karen Simonyan; Oriol Vinyals; Koray Kavukcuoglu; George Van Den Driessche; Edward Lockhart; Luis C Cobo; Florian Stimberg; Norman Casagrande; Dominik Grewe; Seb Noury"}, {"ref_id": "b64", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b65", "title": "Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR", "journal": "", "year": "2022", "authors": "Dor Verbin; Peter Hedman; Ben Mildenhall; Todd Zickler; Jonathan T Barron; Pratul P Srinivasan"}, {"ref_id": "b66", "title": "A connection between score matching and denoising autoencoders. Neural computation", "journal": "", "year": "2011", "authors": "Pascal Vincent"}, {"ref_id": "b67", "title": "CLIP-NeRF: Text-andimage driven manipulation of neural radiance fields", "journal": "CVPR", "year": "2022", "authors": "Can Wang; Menglei Chai; Mingming He; Dongdong Chen; Jing Liao"}, {"ref_id": "b68", "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "journal": "NeurIPS", "year": "2016", "authors": "Jiajun Wu; Chengkai Zhang; Tianfan Xue; T William; Joshua B Freeman;  Tenenbaum"}, {"ref_id": "b69", "title": "Pointflow: 3d point cloud generation with continuous normalizing flows", "journal": "ICCV", "year": "2019", "authors": "Guandao Yang; Xun Huang; Zekun Hao; Ming-Yu Liu; Serge Belongie; Bharath Hariharan"}, {"ref_id": "b70", "title": "Multiview neural surface reconstruction by disentangling geometry and appearance", "journal": "NeurIPS", "year": "2020", "authors": "Lior Yariv; Yoni Kasten; Dror Moran; Meirav Galun; Matan Atzmon; Basri Ronen; Yaron Lipman"}, {"ref_id": "b71", "title": "Scaling autoregressive models for content-rich text-to-image generation", "journal": "", "year": "2022", "authors": "Jiahui Yu; Yuanzhong Xu; Jing Yu Koh; Thang Luong; Gunjan Baid; Zirui Wang; Vijay Vasudevan; Alexander Ku; Yinfei Yang; Ben Burcu Karagol Ayan; Wei Hutchinson; Zarana Han; Xin Parekh; Han Li; Jason Zhang; Yonghui Baldridge;  Wu"}, {"ref_id": "b72", "title": "3D shape generation and completion through point-voxel diffusion", "journal": "ICCV", "year": "2021", "authors": "Linqi Zhou; Yilun Du; Jiajun Wu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "A A A C C n i c b V A 9 S w N B E N 3 z M 8 a v U 0 u b 1 S B Y S L g L o p Z B G 8 s I J h F y I e z t T Z L F v d t j d 0 4 M R 2 o b / 4 q N h S K 2 / g I 7 / 4 2 b j 0 K N D 5 Z 9 v D f D z L w w l c K g 5 3 0 5 c / M L i 0 v L h Z X i 6 t r 6 x q a 7 t d 0 w K t M c 6 l x J p W 9 C Z k C K B O o o U M J N q o H F o Y R m e H s x 8 p t 3 o I 1 Q y T U O U m j H r J e I r u A M r d R x 9 w K E e 8 y Z D C F S w +", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "/ l a t e x i t > \u270f (zt|y; t) \u270f < l a t e x i t s h a 1 _ b a s e 6 4 = \" S z f k I 3 8 p q v M b S T f C J I 0 E B r / R B o I = \" > A A A C b X i c h V F d a x Q x F M 2 M H 6 3 r 1 7 T i g x 9 I c C l u w S 4 z V a j Q l 6 I v P i i s 4 L a F z T J k s n c 6 o Z l k S O 6 I 0 2 H e / I W + + R d 8 8 S + Y 2 a 6 o r e C F w O G c e 0 9 u T r J K S Y d x / C 0 I r 1 y 9 d n 1 t / c b g 5 q 3 b", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "B b d c o P + g g Q 8 h u f j k y + B w d 5 y 8 G O 9 + e D k 8 e L 2 K Y 5 0 8 I k / J i C R k j x y Q t 2 R C p k S Q 7 0 E U P A g e B j / C + + H j 8 M l 5 a x i s Z u 6 R v y p 8 9 h N 5 5 r u b < / l a t e x i t >x (zt|y; t) < l a t e x i t s h a 1 _ b a s e 6 4 = \" C J b Y m", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: DreamFusion can be used to create and refine 3D scenes. Here we iteratively refine an example text prompt, while rendering each generated scene from four different viewpoints.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Qualitative comparison with baselines.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "\u2207\u03b8 log p \u03c6 (z t |y) = s \u03c6 (z t |y) \u2202z t \u2202\u03b8 = \u03b1 t s \u03c6 (z t |y)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Evaluating the coherence of DreamFusion generations with their caption using different CLIP retrieval models. We compare to the ground-truth MS-COCO images in the object-centric subset ofJain et al. (2022) as well asKhalid et al. (2022).", "figure_data": "DreamFieldsDream\u2020 Evaluated with only 1 seed per prompt. Metrics shown in parentheses may be overfit, as the sameFields (reimpl.)CLIP model is used during training and eval.MethodCLIP B/32 CLIP B/16 CLIP L/14 R-Precision \u2191CLIP-MeshColor Geo Color Geo Color GeoDream-GT Images77.1 -79.1 ---FusionDream Fields 68.3 -74.2 ---(Ours)(reimpl.) CLIP-Mesh 67.8 -75.8 -74.5  \u2020 -78.6 1.3 (99.9) (0.8) 82.9 1.4matte painting of a castle made of cheesecake surrounded by a moat made of ice creama vase with pink flowersa hamburgerDreamFusion 75.1 42.5 77.5 46.6 79.7 58.5"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "CLIP R-Precision for DreamFusion and several baselines. These include Dream Fields, CLIP-Mesh (which optimizes a mesh with CLIP), and an oracle that evaluates the original captioned image pairs in MS-COCO. We also compare against an enhanced reimplementation of Dream Fields where we use our own 3D representation (Sec. 3.1). Since this evaluation is based on CLIP, Dream Fields and CLIP-Mesh have an unfair advantage as they use CLIP during training. Despite this, DreamFusion outperforms both baselines on color images, and approaches the performance of ground truth images. While our implementation of Dream Fields performs nearly at chance when evaluating geometry (Geo) with textureless renders, DreamFusion is consistent with captions 58.5% of the time. See Appendix A.3 for more details of the experimental setup.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "2, and additional evaluation setup details in Appendix A.3. Derivations for our loss are also included in Appendix A.4.ACKNOWLEDGMENTSThank you to Mohammad Norouzi for thoughtful review of our manuscript, valuable discussions throughout this project, and help using the Imagen model. Thank you to William Chan and Chitwan Saharia for valuable discussions on Imagen and code pointers. Thank you to Kevin Murphy for ideas and feedback on our manuscript. We thank Ruiqi Gao and Durk Kingma for helpful discussions on diffusion models and the score distillation sampling loss. Thanks to Jonathan Ho, Daniel Watson, Alex Alemi, Dumitru Erhan, Abhishek Kumar, Han Zhang, David Ha, Luke Metz, Jascha Sohl-Dickstein, Ian Fischer, and Pieter Abbeel for thoughtful and valuable discussions over the course of this project. Thank you to Sarah Laszlo for help evaluating 3D models, and Rohan Anil for Distributed Shampoo tips. Thank you to Peter Hedman, Dor Verbin, Lior Yariv, Pratul Srinivasan, Christian Reiser, Garrett Tanzer, Harsh Goyal, Will McLeod, Koppany Horvath, Rodrigo Chandia, Puneet Lall, Daniel Castro Chin, Liviu Panait, Alexey Sokolov, Irina Blok, Nick Fisher, and the many other creative Googlers and artists on Twitter for the inspiring text prompt suggestions. Thank you to the Google infrastructure teams for computational support, and all the authors of open-source software packages especially JAX and NumPy that enabled this work.A APPENDIXA.1 PSEUDOCODE FOR ANCESTRAL SAMPLING AND OUR SCORE DISTILLATION SAMPLING. Pseudocode for ancestral sampling from DDPM where y is the optional conditioning signal e.g. a caption. Typically, tmax = 1 and tmin = 1/nstep. Timestep t monotonically decreases.", "figure_data": "z_t = random.normal(img_shape)for t in linspace(tmax, tmin, nstep):epshat_t = diffusion_model.epshat(z_t, y, t) # Score function evaluation.if t > tmin:eps = random.normal(img_shape)z_t = ddpm_update(z_t, epshat_t, eps) # 1 iteration, decreases noise level.x = diffusion_model.xhat(z_t, epshat_t, t_min) # Tweedie's formula: denoise the laststep.return xFigure 7: params = generator.init()opt_state = optimizer.init(params)diffusion_model = diffusion.load_model()for nstep in iterations:t = random.uniform(0., 1.)alpha_t, sigma_t = diffusion_model.get_coeffs(t)eps = random.normal(img_shape)x = generator(params, <other arguments>...) # Get an image observation.z_t = alpha_t * x + sigma_t * eps # Diffuse observation.epshat_t = diffusion_model.epshat(z_t, y, t) # Score function evaluation.g = grad(weight(t) * dot(stopgradient[epshat_t -eps], x), params)params, opt_state = optimizer.update(g, opt_state) # Update params with optimizer.return params"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "\u03b8 log q(z t |x) = \u2202 log q(z t |x) \u2202x", "figure_data": "+\u2202 log q(z t |x) \u2202z t\u2202z t \u2202x\u03b1 t\u2202x \u2202\u03b8=\u03b1 t \u03c3 t\u2212\u03b1 t \u03c3 t\u03b1 t\u2202x \u2202\u03b8= 0. (14)parameter scorepath derivative"}], "formulas": [{"formula_id": "formula_0", "formula_text": "E[x|z t ] \u2248x \u03c6 (z t ; t) = (z t \u2212 \u03c3 t \u03c6 (z t ; t)) /\u03b1 t .", "formula_coordinates": [3.0, 323.48, 499.09, 182.26, 17.29]}, {"formula_id": "formula_1", "formula_text": ": \u03c6 (z t ; t) = \u2212\u03c3 t s \u03c6 (z t ; t).", "formula_coordinates": [3.0, 281.35, 521.01, 104.86, 17.29]}, {"formula_id": "formula_2", "formula_text": "L Diff (\u03c6, x) = E t\u223cU (0,1), \u223cN (0,I) w(t) \u03c6 (\u03b1 t x + \u03c3 t ; t) \u2212 2 2 ,(1)", "formula_coordinates": [3.0, 174.64, 565.1, 329.36, 18.44]}, {"formula_id": "formula_3", "formula_text": "\u2207 \u03b8 L Diff (\u03c6, x = g(\u03b8)) = E t, w(t) (\u02c6 \u03c6 (z t ; y, t) \u2212 ) Noise Residual \u2202\u02c6 \u03c6 (z t ; y, t) z t U-Net Jacobian \u2202x \u2202\u03b8 Generator Jacobian (2)", "formula_coordinates": [4.0, 142.18, 520.92, 361.83, 37.64]}, {"formula_id": "formula_4", "formula_text": "\u2207 \u03b8 L SDS (\u03c6, x = g(\u03b8)) E t, w(t) (\u02c6 \u03c6 (z t ; y, t) \u2212 ) \u2202x \u2202\u03b8 (3)", "formula_coordinates": [4.0, 189.39, 627.71, 314.61, 23.78]}, {"formula_id": "formula_5", "formula_text": "\u2207 \u03b8 L SDS (\u03c6, x = g(\u03b8)) = \u2207 \u03b8 E t [\u03c3 t /\u03b1 t w(t)KL(q(z t |g(\u03b8); y, t) p \u03c6 (z t ; y, t))] .(4)", "formula_coordinates": [4.0, 151.66, 722.14, 352.34, 17.29]}, {"formula_id": "formula_6", "formula_text": "C I B q G S k R n E 9 s s D 3 V f D j l v y y t 4 Y d J b 4 U 1 I i U 9 Q 6 7 m c Q K Z 7 F k C C X z J i W 7 6 X Y z p l G w S U M i 0 F m I G X 8 l v W g Z W n C Y j D t f H z K k B 5 Y J a J d p e 1 L k I 7 V n x 0 5 i 8 1 o O 1 s Z M + y b v 9 5 I / M 9 r Z d g 9 a + c i S T O E h E 8 G d T N J U d F R L j Q S G j j K g S W M a 2 F 3 p b z P N O N o", "formula_coordinates": [5.0, 183.77, 180.37, 4.98, 8.64]}, {"formula_id": "formula_7", "formula_text": "A A C C n i c b V D L S g M x F M 3 4 r P U 1 6 t J N t A g u S p k p o i 6 L b l x W s A / o l J J J 0 z Y 0 k w z J H b E M X b v x V 9 y 4 U M S t X + D O v z F t Z 6 G t B 0 I O 5 9 z L v f e E s e A G P O / b W V p e W V 1 b z 2 3 k N 7 e 2 d 3 b d v f 2 6 U Y m m r E a V U L o Z E s M E l 6 w G H A R r x p q R K B S s E Q 6 v J 3 7 j n m n D l b y D U c z a E e l L 3 u O U g J U 6 7 l E A 7 A F S q X R E h B k H x a C I g 1 C J r h l F 9 k v l u O M W v J I 3 B V 4 k f k Y K K E O 1 4 3 4 F X U W T i E m g g h j T 8 r 0 Y 2 i n R w K l g 4 3 y Q G B Y T O i R 9 1 r J U k o i Z d j o 9 Z Y x P r N L F P a X t k 4 C n 6 u + O l E R m s p q t j A g M z L w 3 E f / z W g n 0 L t s p l 3 E C T N L Z o F 4 i M C g 8 y Q V 3 u W Y U x M g S Q j W 3 u 2 I 6 I J p Q s O n l b Q j + / M m L p F 4", "formula_coordinates": [5.0, 179.97, 122.68, 3.24, 5.31]}, {"formula_id": "formula_8", "formula_text": "> A A A C A H i c b V A 9 S w N B E N 2 L X z F + R S 0 s b B a D Y B H C X R C 1 D N p Y R j A f k A t h b z N J l u z t H b t z Y j j S + F d s L B S x 9 W f Y + W / c f B S a + G D g 8 d 4 M M / O C W A q D r v v t Z F Z W 1 9 Y 3 s p u 5 r e 2 d 3 b 3 8 / k H d R I n m U O O R j H Q z Y A a k U F B D g R K a s Q Y W B h I a w f B m 4 j c e Q B s R q X s c x d A O W V + J n u A M r d T J H / k I j 5 h 2 Q R m B o 7 F f p H 7 R R 5 Z 0 8 g W 3 5 E 5 B l 4 k 3 J w U y R 7 W T / / K 7 E U 9 C U M g l M 6 b l u T G 2 U 6 Z R c A n j n J 8 Y i B k f s j 6 0 L F U s B N N O p w + M 6 a l V u r Q X a V s K 6 V T 9 P Z G y 0 J h R G N j O k O H A L H o T 8 T + v l W D v q p 0 K F S c I i s 8 W 9 R J J M a K T N G h X a O A o R 5", "formula_coordinates": [5.0, 124.01, 180.37, 3.92, 5.5]}, {"formula_id": "formula_9", "formula_text": "7 x + 0 d J w q B k 0 W i 1 h 1 f K p B c A l N 5 C i g k y i g k S + g 7 Y + u p 3 7 7 A Z T m s b z D c Q J e R A e S h 5 x R N N J 9 D + E J M z 2 k A Z e D S b 9 c c a r O D P Y y c X N S I T k a / f J X L 4 h Z G o F E J q j W X d d J 0 M u o Q s 4 E T E q 9 V E N C 2 Y g O o G u o p B F o L 5 t d P b F P j B L Y Y a x M S b R n 6 u + J j E Z a j y P f d E Y U h 3 r R m 4 r / e d 0 U w 0 s v 4 z J J E S S b L w p T Y W N s T y O w A 6 6 A o R g b Q p n i 5 l a b D a m i D E 1 Q J R O C u", "formula_coordinates": [5.0, 249.43, 122.78, 2.39, 5.47]}, {"formula_id": "formula_10", "formula_text": "A C A 3 i c b V D L S g M x F M 3 U V 6 2 v U X e 6 C R b B h Z S Z I u q y 6 M Z l B f u A T i m Z N N O G Z j J D c k c s Q 8 G N v + L G h S J u / Q l 3 / o 2 Z d h b a e k L g c M 6 9 u T f H j w X X 4 D j f V m F p e W V 1 r b h e 2 t j c 2 t 6 x d / e a O k o U Z Q 0 a i U i 1 f a K Z 4 J I 1 g I N g 7 V g x E v q C t f z R d e a 3 7 p n S P J J 3 M I 5 Z N y Q D y Q N O C R i p Z x 9 4 w B 4 g n T 4 0 8 U 7 N C Q k M / S C l k 5 5 d d i r O F H i R u D k p o x z 1 n v 3 l 9 S O a h E w C F U T r j u v E 0 E 2 J A k 4 F m 5 S 8 R L O Y 0 B E Z s I 6 h k o R M d / P R + N g o f R x E y l w J e K r + 7 k h J q P U 4 9 E 1 l t q G e 9 z L x P 6 + T Q H D Z T b m M E 2 C S z g Y F i c A Q 4 S w Q 3 O e K U R B j Q w h V 3 O y K 6 Z A o Q s H E V j I h u P N f X i T N a s U 9 r 1 R v z 8 q 1 q z y O I j p E R + g E u e g C 1 d A N q q M G o u g R P a N X 9 G Y 9 W S / W u / U x K y", "formula_coordinates": [5.0, 250.22, 180.7, 17.73, 5.31]}, {"formula_id": "formula_11", "formula_text": "A A A C D 3 i c b V C 7 T s M w F H V 4 l v I K M L J Y V C C m K q k Q M F a w M B a J P q Q m q h z H a a 0 6 d m Q 7 S F W U P 2 D h V 1 g Y Q I i V l Y 2 / w W k z l J Y j W T 4 6 5 1 7 d e 0 + Q M K q 0 4 / x Y K 6 t r 6 x u b l a 3 q 9 s 7 u 3 r 5 9 c N h R I p W Y t L F g Q v Y C p A i j n L Q 1 1 Y z 0 E k l Q H D D S D c a 3 h d 9 9 J F J R w R / 0 J C F + j I a c R h Q j b a S B f e Y F g o V q E p s v 4 z n 0 c C g 0 n B c 9 w l g + s G t O 3 Z k C L h O 3 J D V Q o j W w v 7 1 Q 4 D Q m X G O G l O q 7 T q L 9 D E l N M S N 5 1 U s V S R A e o y H p G 8 p R T J S f T e / J 4 a l R Q h g J a R 7 X c K r O d 2 Q o V s V 2 p j J G e q Q W v U L 8 z + u n O r r 2 M 8 q T V B O O Z 4 O i l E E t Y B E O D K k k W L O J I Q h L a n a F e I Q k w t p E W D U h u I s n L 5 N O o + 5 e 1 h v 3 F 7 X m T R l H B R y D E 3 A O X H A F m u A O t E A b Y P A E X s A b e L e e r V f r w / q c l a 5 Y Z c 8 R + A P r 6 x d 1 W p 2 R < / l a t e x i t > n \u2022< l a t e x i t s h a _ b a s e = \" M n U T w k y E a Z X L l Y W + E E A d R = \" > A A A B n i c b V B N S N A E N W e t X a O X x S J K k k R V j r G C / Y A k l M l y d b M L u R C i l P O L B W + m u + W / c t j l o O B x s z z M y L M i k M u u s a + s b m X d o p + t H x x W j o b J s y W y l R I z B c C s V b K F D y b q Y J J H k n W h N / M T w b k a p H H G c T G C g R C w Y o J X Q E E k o R c g L K a c B V h W k S g o e W v o J + y P O E K m Q R j f M / N M J y A R s E k n a D P A M A g G L d U Q c J N O J m f P K X n V u n T O N W F N K + n t i A o k x y S y n Q n g C x M / E / z x v g k n Q m U c s U W i + J c U k z p H / a F o z l G N L g G l h b V s C B o Y p T K N g R v + e V V q X v K t a / e G y r g t i i R U J G L o h H r k m D J M m a R F G U v J M X s m b g L + L F r X n G L m h P y B / k D W c u R T g = = < / l a t e x i t > r\u2327 \"a DSLR photo of a peacock on a surfboard\" U-Net < l a t e x i t s h a 1 _ b a s e 6 4 = \" H H S K O Q r A 2 7 3 o n v 8 Z i k Z d P F J u e u U = \" > A A A C G n i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S x C B S l J E X V Z d K M b q W A v 0 I Q y m U 7 a o Z N J m J k I J e Q 5 3 P g q b l w o 4 k 7 c + D Z O 0 g j a + s P A x 3 / O Y c 7 5 v Y h R q S z r y 1 h Y X F p e W S 2 t l d c 3 N r e 2 z Z 3 d t g x j g U k L h y w U X Q 9 J w i g n L U U V I 9 1 I E B R 4 j H S 8 8 W V W 7 9 w T I W n I 7 9 Q k I m 6 A h p z 6 F C O l r b 5 p O y S S l I U c O p I G 0 A m Q G m H E k p u 0 m r P n J 1 Z 6 D H / 4 O j 3 q m x W r Z u W C 8 2 A X U A G F m n 3 z w x m E O A 4 I V 5 g h K X u 2 F S k 3 Q U J R z E h a d m J J I o T H a E h 6 G j k K i H S T / L Q U H m p n A P 1 Q 6 M c V z N 3 f E w k K p J w E n u 7 M V p S z t c z 8 r 9 a L l X / u J p R H s S I c T z / y Y w Z V C L O c 4 I A K g h W b a E B Y U L 0 r x C M k E F Y 6 z b I O w Z 4 9 e R 7 a 9 Z p 9 W q v f n l Q a F 0 U c J b A P D k A V 2 O A M N M A V a I I W w O A B P I E X 8 G o 8 G s / G m / E + b V 0 w i p k 9 8 E f G 5 z c y Y q D 1 < / l a t e x i t > \u270f \u21e0 N (0, I) < l a t e x i t s h a 1 _ b a s e 6 4 = \" j W j H 2 U T Q 5 s F l k B 0 + p m x P 4 j L V z 6 U = \" > A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G E 3 i H o M e v G Y g H l A s o T Z S W 8 y Z n Z 2 m Z k V Q s g X e P G g i F c / y Z t / 4 y T Z g y Y W N B R V 3 X R 3 B Y n g 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R 2 n i m G D x S J W 7 Y B q F F x i w 3 A j s J 0 o p F E g s B W M 7 m Z + 6 w m V 5 r F 8 M O M E / Y g O J A 8 5 o 8 Z K 9 Y t e s e S W 3 T n I K v E y U o I M t V 7 x q 9 u P W R q h N E x Q r T u e m x h / Q p X h T O C 0 0 E 0 1 J p S N 6 A A 7 l k o a o f Y n 8 0 O n 5 M w q f R L G y p Y 0 Z K 7 + n p j Q S O t x F N j O i J q h X v Z m 4 n 9 e J z X h j T / h M k k N S r Z Y F K a C m J j M v i Z 9 r p A Z M b a E M s X t r Y Q N q a L M 2 G w K N g R v + e V V", "formula_coordinates": [5.0, 161.66, 85.81, 306.96, 101.88]}, {"formula_id": "formula_12", "formula_text": "A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d j B a h b k p S R F 0 W 3 b i s Y B / Q h D K Z T t q h k w c z N 2 I J B T f + i h s X i r j 1 J 9 z 5 N 0 7 a L L T 1 w I X D O f d y 7 z 1 e L L g C y / o 2 C k v L K 6 t r x f X S x u b W 9 o 6 5 u 9 d S U S I p a 9 J I R L L j E c U E D 1 k T O A j W i S U j g S d Y 2 x t d Z 3 7 7 n k n F o / A O x j F z A z I I u c 8 p A S 3 1 z A N H 8 Q A 7 R x g 3 K g 6 w B 0 g p C Z g k k 9 N S z y x b V W s K v E j s n J R R j k b P / H L 6 E U 0 C F g I V R K m u b c X g p k Q C p 4 J N S k 6 i W E z o i A x Y V 9 N Q L 1 J u O v 1 h g k + 0 0 s d + J H W F g K f q 7 4 m U B E q N A 0 9 3 B g S G a t 7 L x P + 8 b g L + p Z v y M E 6 A h X S 2 y E 8 E h g h n g e A + l 4 y C G G t C q O T 6 V k y H R B I K O r Y s B H v + 5 U X S q l X t 8 2 r t 9 q x c v 8 r j K K J D d I w q y E Y X q I 5 u U A M 1 E U W P 6 B m 9 o j f j y X g x 3 o 2 P W W v B y G f 2 0 R 8 Y n z / d D 5 Z l < / l a t e x i t > \u21e0 P (camera) z t , t \u21e0 U(0, 1) < l a t e x i t s h a 1 _ b a s e 6 4 = \" 0 W F U U T n Q B T f Y Q H e L C x b W i s s Y 2 e I = \" > A A A C T H i c b V D P S x t B G J 2 N V m 3 8 F d t j L 4 N B i G D D b l p Q 6 E X s p c c I R o V s W L 6 d f G s G Z 2 e X m W + F u O w f 2 E s P 3 v w r v H h Q S q G T G M G o D w Y e 7 7 1 v 5 p s X 5 0 p a 8 v 1 b r 7 a w + G F p e e V j f X V t f W O z s f X p 1 G a F E d g T m c r M e Q w W l d T Y I 0 k K z 3 O D k M Y K z + L L n x P / 7 A q N l Z k + o X G O g x Q u t E y k A H J S 1 B B h C j S K k / K 6 i m i P U 2 h l y q e a A F X 2 q p a / x 4 N d v s P D E V D 5 H A 4 x t 1 J l u q q i k E Z I 0 J q / Z v y D 0 + 7 X 5 1 T U a P p t f w r + l g Q z 0 m Q z d K P G T T j M R J G i J q H A 2 n 7 g 5 z Q o w Z A U C q t 6 W F j M Q V z C B f Y d 1 Z C i H Z T T M i q + 4 5 Q h T z L j j i Y + V V 9 O l J B a", "formula_coordinates": [5.0, 280.86, 104.1, 140.53, 85.61]}, {"formula_id": "formula_13", "formula_text": "A A A B + H i c b V D L T g J B E J z F F + K D V Y 9 e J h I T v J B d N N E j i R e O m M g j g Q 2 Z H R q Y M P v I T K 8 R N 3 y J F w 8 a 4 9 V P 8 e b f O M A e F K y k k 0 p V d 7 q 7 / F g K j Y 7 z b e U 2 N r e 2 d / K 7 h b 3 9 g 8 O i f X T c 0 l G i O D R 5 J C P V 8 Z k G K U J o o k A J n V g B C 3 w J b X 9 y O / f b D 6 C 0 i M J 7 n M b g B W w U i q H g D I 3 U t 4 u N c g / h E V M p R m O c X f T t k l N x F q D r x M 1 I i W R o 9 O 2 v 3 i D i S Q A h c s m 0 7 r p O j F 7 K F A o u Y V b o J R p i x i d s B F 1 D Q x a A 9 t L F 4 T N 6 b p Q B H U b K V I h 0 o f 6 e S F m g 9 T T w T W f A c K x X v b n 4 n 9 d N c H j j p S K M E 4 S Q L x c N E 0 k x o v M U 6 E A o 4 C i n h j C u h L m V 8 j F T j K P J q m B C c F d f X i e t a s W 9 r F T v r k q 1 e h Z H n p y S M 1 I m L r k m N V I n D d I k n C T k m b y S N + v", "formula_coordinates": [5.0, 134.49, 111.72, 17.25, 6.28]}, {"formula_id": "formula_14", "formula_text": "d S y 5 R d f W M a v S 6 Y v z z k = \" > A A A C N X i c b V B N S x x B F O z x I 5 r V J K s e c 2 l c h B X M M m M E B S 9 i L j k Y W M F V Y X s Z e n r f O I 0 9 3 U P 3 G 3 E d 5 k 9 5 8 X / k Z A 4 e D C H X / I X 0 r h t I N A U N R V U 9 + r 1 K C i U d h u G 3 Y G Z 2 b v 7 V w u L r x t L y m 7 f v m i u r p 8 6 U V k B P G G X s e c I d K K m h h x I V n B c W e J 4 o O E s u P 4 3 9 s y u w T h p 9 g q M C B j m / 0 D K V g q O X 4 u Y R Q 7 j G 6 s t R t 2 4 z M T S 4 T x l m g H y T b l C W c a w Y F E 4 q o + u Y F Z l s s 5 x j l q T V T R 3 7 L G 7 R 0 e a H P 5 G 4 2 Q o 7 4 Q T 0 J Y m m p E W m 6 M b N r 2 x o R J m D R q G 4 c / 0 o L H B Q c Y t S K K g b r H R Q c H H J L 6 D v q e Y 5 u E E 1 u b q m G 1 4 Z 0 t R Y / z T S i f r 3 R M V z 5 0 Z 5 4 p P j n d 1 z b y z + z + u X m O 4 N K q m L E k G L p 4 / S U l E 0 d F w h H U o L A t X I E y 6 s 9 L t S k X H L B f q i G 7 6 E 6 P n J L 8 n p d i f 6 2 N k + 3 m k d H E 7 r W C T v", "formula_coordinates": [5.0, 154.02, 197.65, 5.91, 6.68]}, {"formula_id": "formula_15", "formula_text": "B Q o o k t g A + 1 i p 4 U W c g n t p Y c E N t B N A q v F y N p x L C J L R h q X O s Z / M r c c e + n v q H a z g X w U M i B 4 v P d m N H r K K i U d x v F V E K 4 8 e f p s d e 1 5 7 8 X 6 y 1 e v o 4 0 3 x 8 7 U V s B Y G G X s a c Y d K K l h j B I V n F Y W e J k p O M n O v 8 / 1 k 1 9 g n T T 6 J z Y V T E t + p m U u B U d P p V H L C o 4 t g 8 p J Z X S X s q q Q A 1 Z y L L K 8 v e h S p M y 3 I 2 3 2 K O 7 Q b c o Q f m N 7 e D D q B k z M D O 5 5 p g D k C + 2 R U X 7 E B 9 r s f L y x p F E / H s a L o g 9 B s g R 9 s q x R G l 2 y m R F 1 C R q F 4 s 5 N k r j C a c s t S q G g 6 7 H a Q c X F O T + D i Y e a l + C m 7 S K k j m 5 7 Z k Z z Y / 3 R S B f s 7 Y 6 W l 8 4 1 Z e a d 8 5 3 d f W 1 O / k + b 1 J h / n b Z S V z W C F t c X 5 b W i a O g 8 c T q T F g S q x g M u r P S 7 U l F w y w X 6 f + n 5 E J L 7 T 3 4 I j n e H y a f h 7 t H n / v 6 3 Z R x r Z I u 8 J w O S k C 9 k n / w g I z I m g v w J V o M o 2 A j + h u v h Z v j 2 2 h o G y 5 5 N c q f C d / 8 A F m q 3 v g = = <", "formula_coordinates": [5.0, 464.9, 206.62, 4.87, 6.5]}, {"formula_id": "formula_16", "formula_text": "d + 5 G G 5 u H z t R W w F Q Y Z e x x x h 0 o q W G K E h U c V x Z 4 m S k 4 y k 7 f 9 P r R J 7 B O G v 0 R m w r m J T / R M p e C o 6 f S 6 A s r O L Y M K i e V 0 V 3 K q k K O W M m x y P L 2 r E u R M j + O t N m n u L 3 z q 4 9 u U Y b w G d v 3 7 y b d i I m F w X 3 P F I B 8 u 9 f + 4 + m 9 n t P m t 1 0 a D e N x v C x 6 G S Q r M C S r m q T R V 7 Y w o i 5 B o 1 D c u V k S V z h v u U U p F H Q D V j u o u D j l J z D z U P M S 3 L x d p t X R L c 8 s a G 6 s P x r p k v 1 z o u W l c 0 2 Z + c 5 + Z 3 d R 6 8 l / a b M a 8 1 f z V u q q R t D i / K K 8 V h Q N 7 a O n C 2 l B o G o 8 4 M J K v y s V", "formula_coordinates": [5.0, 382.81, 206.62, 2.63, 6.5]}, {"formula_id": "formula_17", "formula_text": "v D l h n R h x d j o n b r q W / o X M B Y = \" > A A A C X X i c b V F N a 9 w w E J X d p E m 2 a b p t D z 3 0 I r o E N t A u d h p o I Z e Q X n J o Y Q P Z J L B a j K w d x y K y Z K R x i W v 8 J 3 t L L / 0 r k T c b y N e A 4 P H e z N P o K S 2 V d B h F 1 0 H 4 Y m X 1 5 d r 6 R u / V 5 u u t N / 2 3 7 0 6 d q a y A i T D K 2 P O U O 1 B S w w Q l K j g v L f A i V X C W X v 7 o 9 L P f Y J 0 0 + g T r E m Y F v 9 A y k 4 K j p 5 I + s p x j c 9 U m r M z l k B U c 8 z R r / r Q J U u b n k N b 7 F H f o N m U I V 9 j 8 + j l u h 0 z M D e 5 7 J g f k C 6 3 z Y F A 6 q Y x + x s p b f K b 1 z p e 7 l q Q / i E b R o u h T E C / B g C x r n P T / s r k R V Q E a h e L O T e O o x F n D L U q h o O 2 x y k H J x S W / g K m H m h f g Z s 0 i n Z Z u e 2 Z O M 2 P 9 0 U g X 7 P 2 J h h f O 1 U X q O 7 u d 3 W O t I 5 / T p h V m 3 2 e N 1 G W F o M X t R V m l K B r a R U 3 n 0 o J A V X v A h Z V + V", "formula_coordinates": [5.0, 464.27, 111.45, 4.87, 6.5]}, {"formula_id": "formula_18", "formula_text": "C = i w i c i , w i = \u03b1 i j<i (1 \u2212 \u03b1 j ) , \u03b1 i = 1 \u2212 exp (\u2212\u03c4 i \u00b5 i \u2212 \u00b5 i+1 ) ,(5)", "formula_coordinates": [6.0, 136.64, 121.3, 367.36, 17.29]}, {"formula_id": "formula_19", "formula_text": "(\u03c4, \u03c1) = MLP (\u00b5; \u03b8) ,(6)", "formula_coordinates": [6.0, 259.06, 301.73, 244.94, 9.96]}, {"formula_id": "formula_20", "formula_text": "n = \u2212\u2207 \u00b5 \u03c4 / \u2207 \u00b5 \u03c4 (", "formula_coordinates": [6.0, 108.0, 349.64, 92.73, 17.29]}, {"formula_id": "formula_21", "formula_text": "c = \u03c1 \u2022 ( \u03c1 \u2022 max (0, n \u2022 ( \u2212 \u00b5)/ \u2212 \u00b5 ) + a ) .(7)", "formula_coordinates": [6.0, 199.95, 397.8, 304.05, 17.29]}, {"formula_id": "formula_22", "formula_text": "0.0 0.2 0.4 0.6 0.8 1.0 R-Precision Base +ViewAug (i) +ViewDep (ii) +Lighting (iii) +Textureless (iv) (i) (ii) (iii) (iv)", "formula_coordinates": [9.0, 107.76, 85.94, 380.98, 145.06]}, {"formula_id": "formula_23", "formula_text": "\u03a3 = \u03bb 2 \u03a3 I 3 (8)", "formula_coordinates": [15.0, 284.41, 681.46, 219.59, 12.69]}, {"formula_id": "formula_24", "formula_text": "\u03c4 init (\u00b5) = \u03bb \u03c4 \u2022 exp \u2212 \u00b5 2 2\u03c3 2 \u03c4 .(9)", "formula_coordinates": [16.0, 239.34, 297.98, 264.66, 27.55]}, {"formula_id": "formula_25", "formula_text": "L orient = i stop grad(w i ) max(0, n i \u2022 v) 2 ,(10)", "formula_coordinates": [16.0, 213.97, 568.22, 290.03, 18.44]}, {"formula_id": "formula_28", "formula_text": "(B)(12)", "formula_coordinates": [17.0, 422.76, 364.77, 81.24, 24.43]}, {"formula_id": "formula_29", "formula_text": "\u2207", "formula_coordinates": [17.0, 113.74, 508.57, 8.3, 17.29]}, {"formula_id": "formula_30", "formula_text": "\u2207 \u03b8 L SDS = E t,zt|x w(t)", "formula_coordinates": [17.0, 181.91, 610.65, 96.67, 17.29]}], "doi": "10.23915/distill.00012"}