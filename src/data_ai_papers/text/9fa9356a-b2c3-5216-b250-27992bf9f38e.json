{"title": "Functional Maps: A Flexible Representation of Maps Between Shapes", "authors": "Maks Ovsjanikov; Mirela Ben-Chen; Justin Solomon; Adrian Butscher; Leonidas Guibas; \u2020 Lix; \u00c9cole Polytechnique", "pub_date": "", "abstract": "Figure 1: Horse algebra: the functional representation and map inference algorithm allow us to go beyond point-to-point maps. The source shape (top left corner) was mapped to the target shape (left) by posing descriptor-based functional constraints which do not disambiguate symmetries (i.e. without landmark constraints). By further adding correspondence constraints, we obtain a near isometric map which reverses orientation, mapping left to right (center). The representation allows for algebraic operations on shape maps, so we can subtract this map from the ambivalent map, to retrieve the orientation preserving near-isometry (right). Each column shows the first 20x20 block of the functional map representation (bottom), and the action of the map by transferring colors from the source shape to the target shape (top).", "sections": [{"heading": "Introduction", "text": "Shape matching lies at the core of many operations in geometry processing. While several solutions to rigid matching are well established, non-rigid shape matching remains difficult even when the space of deformations is limited to e.g. approximate isometries. Part of the difficulty in devising a robust and efficient non-rigid shape matching method is that unlike the rigid case, where the deformation can be represented compactly as a rotation and translation, non-rigid shape matchings are most frequently represented as pairings (correspondences) of points or regions on the two shapes. This representation makes direct map estimation and inference intractable, since the space of possible point correspondences is exponential in size. For example, isometric matching techniques try to find correspondences that preserve geodesic distances as well as possible, but such optimization problems can be shown to be an NP-hard subclass of the quadratic assignment problem [\u00c7 ela 1998]. Perhaps more importantly, this representation does not naturally support constraints such as map continuity or global consistency.\nAdditionally, in many practical situations, it is neither possible nor necessary to establish point-to-point correspondences between a pair of shapes, because of inherent shape ambiguities or because the user may only be interested in approximate alignment. Such ambiguous or approximate map inference is difficult to phrase in terms of point-to-point correspondences.\nThe majority of existing methods try to tackle these challenges by limiting their search for correspondences between a small set of landmark points and extending those to a dense set of correspondences on entire shapes during final post-processing ( [Bronstein et al. 2006;Huang et al. 2008;Lipman and Funkhouser 2009;Kin-Chung Au et al. 2010;Ovsjanikov et al. 2010;Kim et al. 2011;Tevs et al. 2011;Sahillio\u01e7lu and Yemez 2011] among many others). This strategy has also been justified theoretically, since under general conditions a small set of landmark correspondences is known to be sufficient to obtain a unique dense mapping between isometric surfaces ( [Lipman and Funkhouser 2009;Ovsjanikov et al. 2010]).\nNevertheless, although this landmark-based approach reduces the complexity of the solution space it still relies on representing shape maps as point-to-point correspondences, making it difficult to incorporate global constraints or return meaningful results when establishing point correspondences is not possible due to the presence of only coarse similarities or symmetry ambiguities.\nIn this paper we present a novel approach for inference and manipulation of maps between shapes that tries to resolve the issues above in a fundamentally different way. Rather than putting in correspondence points on the shapes, we propose to consider mappings between functions defined on the shapes. This notion of correspondence generalizes the standard point-to-point map since every pointwise correspondence induces a mapping between function spaces, while the opposite is, in general, not true. However, this generalized representation is: 1) flexible, since it allows choosing a basis for the function space on each shape and representing the mapping as a change of basis matrix and 2) well-suited for shapematching, since many natural constraints on the map become linear constraints on the functional map. As we show in the rest of this paper, our representation works especially well when combined with the eigenfunctions of the Laplace-Beltrami operator, by benefiting from their multi-scale, \"geometry-aware\" nature. This allows us, in particular, to devise a simple algorithm that achieves state-of-the art results on an isometric shape matching benchmark and at the heart of which is a single linear solve. We also demonstrate the usefulness of this representation on a number of tasks including improving existing maps, segmentation transfer and joint analysis of shape collections without establishing point-to-point correspondences.", "publication_ref": ["b3", "b8", "b15", "b14", "b20", "b13", "b28", "b23", "b15", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Both shape matching in general and non-rigid shape matching in particular are relatively well-established fields with several recent books (e.g. [Bronstein et al. 2008]) and surveys [van Kaick et al. 2011b] dedicated exclusively to this subject. It is out of scope for the current article to cover the breadth of all existing shape matching methods; we concentrate on the various classes of underlying representations for maps between pairs of shapes and indicate ways in which they are optimized for in the literature.\nAs mentioned in the introduction, the vast majority of existing shape matching methods represent a map between a pair of shapes as a point-to-point correspondence. Since it is infeasible to optimize over such correspondences directly, most methods aim to obtain a sparse set of point correspondences and extend them to dense mappings [Bronstein et al. 2006;Huang et al. 2008;Lipman and Funkhouser 2009;Kin-Chung Au et al. 2010;Ovsjanikov et al. 2010;Kim et al. 2011;Tevs et al. 2011;Sahillio\u01e7lu and Yemez 2011]. Because sparse point correspondences are inherently discrete, common ways to enforce global consistency include preservation of various quantities between pairs or sets of points, including geodesic distances [Bronstein et al. 2006;Huang et al. 2008;Sahillio\u01e7lu and Yemez 2011], various spectral quantities [Jain et al. 2007;Mateus et al. 2008;Sharma and Horaud 2010;Ovsjanikov et al. 2010], or embedding shapes into into canonical domains [Lipman and Funkhouser 2009] based on landmark correspondences, or a combination of multiple geometric and topological tests [Dubrovina and Kimmel 2011;Kin-Chung Au et al. 2010].\nA related set of techniques aims to establish shape part or segment correspondences rather than reliable point-to-point matches, e.g. [Golovinskiy and Funkhouser 2009;Xu et al. 2010;Pokrass et al. 2011;Huang et al. 2011;van Kaick et al. 2011a]. Such techniques either pre-segment the shape and try to establish part correspondences, or more recently phrase the segmentation and correspondence (and possibly labelling) in a joint optimization framework [Kalogerakis et al. 2010;Pokrass et al. 2011;Huang et al. 2011] which generally avoids the need to establish reliable pointwise correspondences. In this paper we show how segment correspondences can be used as constraints to establish high quality point matches.\nFinally, some methods optimize the deformation of one shape to align it with another, rather than optimizing the correspondences directly [Zhang et al. 2008;Yeh et al. 2010]. In the majority of cases, however, such methods still rely on point correspondences either during alignment or pre-processing as feature matches.\nWe note that some recent methods have concentrated on measuring and optimizing consistency of sets of maps [Nguyen et al. 2011;Kim et al. 2011] and showed superior performance to optimizing individual correspondences. These applications show the importance of algebraic operations on maps (averages, differences), which are challenging to do in the point-to-point correspondence domain.\nOur use of spectral quantities is also closely related to spectral embeddings [Rustamov 2007] and their application in shape matching [Jain et al. 2007;Mateus et al. 2008;Ovsjanikov et al. 2008]. However, unlike such methods our framework does not assume oneto-one correspondences between eigenfunctions of the Laplace-Beltrami operator. This difference is crucial for both removing the combinatorial complexity present in these methods (e.g. sign ambiguities, order switching) as well as achieving superior results in practice.\nOne common characteristic of all existing non-rigid shape matching methods is that they lead to difficult, non-convex, non-linear optimization problems. In this paper, we argue that this is primarily because maps between shapes are represented as point or segment correspondences, making it inherently difficult to devise map inference methods using global constraints. On the other hand, we show that by generalizing the notion of a map to include pairings of functions instead of points, map inference can be phrased as a linear system of equations. One danger of this generalization is that the solution may not correspond to a point-to-point map. We show simple regularization techniques that help avoid this possibility.\nNote that analyzing mappings through their effect on function spaces is a common theme used in various fields of mathematics. A famous example can be found in the field of Representation Theory (see for instance [Weyl 1946]). Here, one relates the different ways in which a compact Lie group (a continuous group of transformations, e.g. the group of rotations of plane) can act on itself to the induced linear action of the group on functions.", "publication_ref": ["b4", "b3", "b8", "b15", "b14", "b20", "b13", "b28", "b23", "b3", "b8", "b23", "b10", "b16", "b24", "b20", "b15", "b6", "b14", "b7", "b32", "b21", "b9", "b11", "b21", "b9", "b34", "b33", "b18", "b13", "b22", "b10", "b16", "b19", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Contributions", "text": "The key contribution of this paper is a new representation for maps between pairs of shapes as linear transformations between the corresponding function spaces. We show how this notion of a map generalizes the standard point-to-point representation and yet has the following key advantages:\n\u2022 By using the Laplace-Beltrami basis for the function space on each shape, the map can be well-approximated using a small number of basis functions and expressed simply as a matrix.\n\u2022 Most natural constraints on maps, such as descriptor preservation, landmark correspondences, part preservation and operator commutativity become linear in the functional formulation, enabling extremely efficient inference.\n\u2022 Maps in this representation can be manipulated via standard algebraic operations e.g. addition, subtraction, composition.\nLast but not least, functional maps can be useful even when they do not correspond to point-to-point maps for information or attribute transfer between shapes, shape collection analysis, and other shape processing tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Functional Map Representation", "text": "To set the stage for functional mappings as a generalization of classical point-to-point mappings, let T : M ! N be a bijective mapping between manifolds M and N (either continuous or discrete). Then, T induces a natural transformation of derived quantities, such as functions on M . To be precise, if we are given a scalar function f : M ! R then we obtain a corresponding function g : N ! R by composition, as in g = f T 1 . Let us denote this induced transformation by TF : F (M, R) ! F(N, R), where we use F (\u2022, R) to denote a generic space of real-valued functions. We call TF the functional representation of the mapping T . We now make the following two simple remarks: (y) = 0 whenever T 1 (y) 6 = a and 1 otherwise. Since T is assumed to be invertible, there is a unique point y s.t. T (a) = y. Thus, g must be an indicator function of T (a) and T (a) is the unique point y 2 N s.t. g(y) = 1.\nRemark 4.2. For any fixed bijective map T : M ! N , TF is a linear map between function spaces.\nTo see this, note\nTF (\u21b51f1 + \u21b52f2) = (\u21b51f1 + \u21b52f2) T 1 = \u21b51f1 T 1 + \u21b52f2 T 1 = \u21b51TF (f1) + \u21b52TF (f2)\n. We may paraphrase these remarks to say that knowledge of TF is equivalent to knowledge of T . And while T may be a complicated mapping between surfaces, TF acts linearly between function spaces. Now suppose that the function space of M is equipped with a basis so that any function f : M ! R can be represented as a linear combination of basis functions f = P i ai M i . Then,\nTF (f ) = TF X i ai M i ! = X i aiTF \u21e3 M i \u2318 .\nIn addition, if N is equipped with a set of basis functions { N j }, then\nTF M i = P j cij N j for some {cij} and TF (f ) = X i ai X j cij N j = X j X i aicij N j .(1)\nTherefore if we represent f as a vector of coefficients a = (a0, a1, ....ai, ...) and g = TF (f ) as a vector b = (b0, b1, ...., bi, ...), then Eq. 1 simply says: bj = P i aicij, where cij is independent of f and is completely determined by the bases and the map T . In particular cij is the j th coefficient of TF ( M i ) in the basis { N j }. Note that C has a particularly simple representation if the basis functions { N i } are orthonormal with respect to some inner product h\u2022, \u2022i, namely cij = hTF ( M i ), N j i. We conclude with the following key observation:\nRemark 4.3. The map TF can be represented as a (possibly infinite) matrix C s.t. for any function f represented as a vector of coefficients a then TF (a) = Ca.\nThis remark in combination with the previous two remarks shows that the matrix C fully encodes the original map T .  Motivated by this discussion, we now turn towards the definition of linear functional mappings that are strictly more general than functional representations of classical point-to-point mappings. The point of view that we take is to downplay the mapping T and focus our attention on the matrix C. We thus define: Definition 1. Let { M i } and { N j } be bases for F (M, R) and F (N, R), respectively. A generalized linear functional mapping TF : F (M, R) ! F(N, R) with respect to these bases is the operator defined by\nTF X i ai M i ! = X j X i aicij N j ,\nwhere cij is a possibly infinite matrix of real coefficients (subject to conditions that guarantee convergence of the sums above).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example.", "text": "As an example, consider a pair of shapes in Figure 2 with three bijective maps between then: two approximate isometries (the ground-truth map and the left-right mirror symmetric map) and one map that puts the head and tail in correspondence.\nFor each map, the point-to-point representation is shown as color correspondence while the functional representation is shown as a heat map of the matrix C0..20\u21e50..20, where we used the Laplace-Beltrami eigenfunctions as the basis for the function space on each shape. Note that the functional representations of the near-isometric maps are close to being sparse and diagonally dominant, whereas the representation of the map that associates the head with the tail is not. Also note that none of the maps is diagonal, an assumption made by previous algorithms [Jain et al. 2007;Mateus et al. 2008;Ovsjanikov et al. 2008]. For each shape with a known ground-truth pointto-point correspondence (shown as a color correspondence), we computed its functional representation and measured its accuracy in finding a point-to-point map. Note that although more eigenvalues lead to an increase in accuracy, maps that correspond to bigger deformations require more basis vectors, capturing the intuition that near-isometric maps are more compactly represented.", "publication_ref": ["b10", "b16", "b19"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Functional Representation Properties", "text": "As we have noted above, the functional representation of a pointwise bijection can be used to recover its representation as a correspondence, and is thus equivalent. Note, however, that this does not imply that the space of bijections coincides with the space of linear maps between function spaces, as the latter may include functional mappings not associated with any point-to-point correspondence.\nPerhaps the simplest example of this is a functional map D that maps every function on one shape to the constant 0 function on the other -D clearly cannot be associated with any pointwise correspondences since all such functional maps must, by definition, preserve the set of values of each function. Nevertheless, by going to this richer space of correspondences, we obtain a representation that has several key properties making it more suitable for manipulation and inference.\nIntuitively, functional maps are easy to manipulate because they can be represented as matrices and thus can benefit from standard linear algebra techniques. To make this intuition practical, however, the size of the matrices must be moderate (i.e. independent of the number of points on the shapes), and furthermore map inference should be phrased in terms of linear constraints in this representation. In the following sections we will show how to achieve these goals first by choosing the appropriate basis for the function space on each shape (Section 5.1) and then by showing how many natural constraints on the map can be phrased as linear constraints on the functional map (Section 5.3), reducing shape matching to a moderately-sized system of linear equations (Section 6).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Choice of basis", "text": "As noted above, the functional map representation is flexible in the sense that it gives us the freedom to choose the basis functions for the functional spaces of M and N . Indeed, if we choose the basis functions to be indicator functions at the vertices of the shapes, then C is simply the permutation matrix which corresponds to the original mapping. However, other choices of bases are possible, which can lead to significant reductions in representation complexity and are much better suited for continuous mappings between shapesthe desired behavior in the majority of practical applications.\nPerhaps the two most important characteristics for choosing a basis for functional maps are compactness and stability. Compactness means that most natural functions on a shape should be well approximated by using a small number of basis elements, while stability means that the space of functions spanned by all linear combinations of basis functions must be stable under small shape deformations. These two properties together ensure that we can represent the action TF using a small and robust subset of basis functions and we need only consider a finite submatrix C0..m\u21e50...n, for some moderate values of m and n, of the infinite matrix C (Definition 1). In other words, for a given function f , represented as a vector of coefficients a = (a0, a1, ....ai, ...), we would like P\nj P i aicij N j \u21e1 P n j=0 P m i=0 aicij N j ,\nfor some fixed small values of m and n.\nIn this paper, we will concentrate on shapes undergoing nearisometric deformations, for which we will use the first n Laplace-Beltrami eigenfunctions as the basis for their functional representations (where n = 100 throughout all of our experiments, independent of the number of points on the shape). This choice of basis is natural, since eigenfunctions of the Laplace-Beltrami operator are ordered from \"low frequency\" to \"higher frequency,\" meaning that they provide a natural multi-scale way to approximate functions, and as a result functional mappings, between shapes. Moreover, although individual eigenfunctions are known to be unstable under perturbations, suffering from well-known phenomena such as sign flipping and eigenfunction order changes, the space of functions spanned by the first n eigenfunctions of the Laplace-Beltrami operator can be shown to be stable under near-isometries as long as the n th and (n + 1) st eigenvalues are well separated, as shown for example in the work of [Kato 1995].\nTo illustrate the role of the choice of basis on the functional representation, we compare two widely-used discretizations of the Laplace-Beltrami operator and measure their ability to capture a ground-truth point-to-point correspondence using a fixed number n of basis functions. In particular, we consider, the cotangent weight scheme of Meyer et al. [2002] with and without area normalization (in the latter case, each vertex is assigned a uniform weight, while in the former the weight is proportional to the sum of the areas of triangles around the point). Figure 3 shows the average error induced by the functional representation for a set of pairs of deformed versions of the cat shape provided in the TOSCA [Bronstein et al. 2008] dataset. Each of these shapes contains 27.8K points, with a known ground-truth correspondence. We represented this pointwise correspondence between the cat0 shape and the others using an increasing number of eigenvectors, and for each point x computed its image as: T (x) = arg maxy TF (f )(y) where f is the indicator function at the point x on shape M . I.e. T (x) is the point where the function g = TF (f ) is maximal. The error is measured in average geodesic error units (see [Kim et al. 2011]  Figure 4: Sparsity pattern of the matrices C corresponding to two out of 4 maps shown in Figure 3. Only cells where |C| > 0.11 are shown. Note that more than 94% are not. Note also that the functional matrix for the more deformed shape cat6 is also farther from being diagonal.\nof the map for the same quality of reconstruction. This may be because this discretization is less sensitive to volume distortion (and only sensitive to non-conformal distortions). Moreover, note that only 30 40 eigenfunctions are sufficient to represent the ground truth map to a quality that is extremely close to the groundtruth point-to-point map. Since a functional representation with 40 eigenvectors implies a matrix of size 40 \u21e5 40, this means that even without exploiting its sparsity (described below), this representation has 1600 values, which is a nearly 17 times memory savings over a permutation of size 27.8K.", "publication_ref": ["b12", "b17", "b4", "b13"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Sparsity.", "text": "In addition to the multi-scale property of the functional representation with the Laplace-Beltrami eigenfunctions, we also point out that near-isometric maps induce matrices C that are nearly sparse and thus can be stored efficiently. Indeed, if the shapes M and N are isometric and T is an isometry, it is easy to see that the matrix Cij can be non-zero only if M j and N i correspond to the same eigenvalue. In particular, if all eigenvalues are non-repeating, C is a diagonal matrix. In practice, we observe that if T is only approximately an isometry, the matrix C is still close to being sparse, or funnel-shaped. Figure 4 shows the sparsity patterns of the matrices C corresponding to two of the maps shown in Figure 3. In particular, note that over 94% of the values of these matrices are below 0.1. Let us stress, however, that the functional matrix C stops being diagonal very quickly under even mild non-isometric deformations, and this effect is especially pronounced for high-frequency eigenfunctions (Figure 2 illustrates the same effect). While this poses fundamental challenges to previous spectral methods [Jain et al. 2007;Mateus et al. 2008;Ovsjanikov et al. 2008], the functional representation naturally encodes such changes.", "publication_ref": ["b10", "b16", "b19"], "figure_ref": ["fig_3", "fig_2"], "table_ref": []}, {"heading": "Continuity", "text": "Another major advantage of using the functional representation of the mapping is that it naturally handles map continuity unlike the point-to-point or segment-to-segment bijection which is inherently discrete. Here continuity means three distinct phenomena: Continuity under changes of the input function. This means that the image of a function TF (f ) = Ca varies continuously under changes of the vector of coefficients a and thus under the changes of the function for a fixed mapping C. This property is useful since in most natural settings the desired mapping is continuous.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Continuity of the image function.", "text": "The Laplace-Beltrami operator is inherently well-suited for representing smooth functions on the shapes. Thus, for any fixed number n, and any set of coefficients a, the function f = P n i=0 ai M i will be smooth. Thus, if we use a truncated functional representation matrix C0..n\u21e50...n then the image Ca of any function f will be smooth.\nContinuity of the representation. Finally, we also note that the functional representation is more amenable to numerical optimization since it is inherently continuous. That is, the matrix C can be modified continuously and still produce meaningful results. Note that there are no inherent restrictions on the matrix C to be able to establish functional correspondences. Thus, given any matrix C and any vector of coefficients a, we can interpret Ca as a functional mapping. To illustrate this, in Figure 5 we show the image of a set of three functions from a fixed source shape (shown in Figure 1) onto a target shape under a mapping matrix, interpolated between two mappings corresponding to the direct and symmetric shape matching. Note that each mapping is both meaningful and produces intuitive results.", "publication_ref": [], "figure_ref": ["fig_5", "fig_0"], "table_ref": []}, {"heading": "Linearity of constraints", "text": "Perhaps even more importantly, the functional representation is particularly well suited for map inference (i.e. constrained optimization) for the following reason: when the underlying map T (and by extension the matrix C) are unknown, many natural constraints on the map become linear constraints in its functional representation. Below we describe the most common scenarios.\nFunction preservation. Given a pair of functions f : M ! R and g : N ! R, the correspondence between f and g can be written simply as Ca = b where C is the functional representation of the map, while a and b are the representation of f and g in the chosen bases of M and N . Note that the function preservation constraint can be phrased entirely in terms of the matrix C without knowledge of the underlying correspondence T , since a and b do not depend on the map C. This is especially useful for shape matching applications where C is unknown, but could possibly be recovered by phrasing enough constraints of type Cai = bi. The function preservation constraint is quite general and includes the following as special cases.\nDescriptor preservation. If f and g are functions corresponding to point descriptors (e.g. f (x) = \uf8ff(x) where \uf8ff(x) is Gauss curvature of M at x), then the function preservation constraint simply says that descriptors are approximately preserved by the mapping. Furthermore if the point descriptors are multidimensional so that f (x) 2 R k for each x then we can phrase k scalar function preservation constraints, one for each dimension of the descriptor.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Landmark point correspondences.", "text": "If we are given landmark point correspondences T (x) = y for some known x 2 M and y 2 N (e.g. specified by the user or obtained automatically), we can phrase this knowledge as functional constraints by considering functions f and g that are e.g. distance functions to the landmarks or normally distributed functions around x and y. Indeed, the confidence with which the landmark correspondence is known can be encoded in the functional constraints very naturally (e.g. if it is only known within a certain radius).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Segment correspondences.", "text": "Similarly, if we are given correspondences between parts of shapes rather than individual points we can phrase such correspondences as functional correspondences again by either considering the indicator functions on the segments or more robust derived quantities such as the distance function.\nIn our implementation for finding functional maps between shapes, we impose a variety of functional constraints as described above.\nWe will discuss the actual choice of the functions used below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Operator Commutativity", "text": "In addition to the function preservation constraint, another class of constraints on the map that induce linear constraints on its functional representation is commutativity with respect to linear operators on M and N . That is, often M and N can be endowed with linear functional operators that we may want to preserve. A first example is a symmetry operator SF : F (M, R) ! F(M, R) which associates with every function\nf : M ! R another function SF (f ) : M ! R obtained as SF (f )(x) = f (S 1 (x))\n, where S : M ! M is some symmetry of M . A second example is the Laplace-Beltrami operator and derived operators (e.g. the heat operator), which are preserved under isometries. The operators on M and N can be quite general, however, and can represent any association of functions on the manifold. In any case, given functional operators SF and RF on M and N respectively, it may be natural to require that the functional map C commute with these operators. In particular: RF C = CSF or kRF C CSF k = 0. This constraint, despite its second order flavor, leads to linear equations in the elements of C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Regularization Constraints", "text": "Note that although we mentioned in Section 5.2 that there are no inherent constraints on the matrix C to be a functional map, this does not mean that any matrix C is associated with a point-to-point map. Indeed, while every bijective map T has a functional representation through the matrix C, the converse is not necessarily true. Thus, there may be constraints on the functional representation if it is known to be associated with a point-to-point map. Although finding such constraints seems to be a difficult open problem, a very useful observation is the following: Theorem 5.1. (1) If the basis functions are discrete and orthonormal with respect to the standard inner product, i.e.\nP x i(x) j (x) = ij , or if the underlying map T (discrete or con- tinuous) is volume preserving, i.e. \u00b5 M (x) = \u00b5 N (T (x)\n) where \u00b5 M and \u00b5 N are volume elements on M and N respectively, then the matrix C associated with the functional representation of T must be orthonormal. (2) If the underlying map T is an isometry then T commutes with the Laplace-Beltrami operator. Proof. See Appendix It follows that in most natural settings, e.g. when one expects isometries between shapes, if one is using the functional representation to obtain a point-to-point map it is most meaningful to consider orthonormal or nearly-orthonormal functional map matrices. Furthermore, it makes sense to incorporate commutation with the Laplace-Beltrami operators into the regularization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Map Inversion and Composition", "text": "A challenging task when considering point-to-point mappings between shapes is map inversion, i.e. given a map T : M ! N that is not necessarily bijective, one is required to find a meaningful version of T 1\n: N ! M . In the functional representation finding an inverse can be done simply by finding an inverse of the mapping matrix C. Moreover, because for near-isometric maps we expect this matrix to be close to diagonal (or \"funnel\" shaped as shown in Figure 4) it is reasonable to take the inverse of the approximating submatrix of C. Finally, in light of Theorem 5.1 this can be done by simply taking the transpose of C or its approximation.\nSimilarly, map composition becomes simple matrix multiplication in the functional representation. We exploit these properties when we use our representation for joint map inference on a collection of shapes in Section 8.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Functional Map Inference", "text": "As mentioned in Section 5, functional shape maps are well-suited for inference because of their continuous nature and because a large number of constraints become linear in this representation. In this section we discuss how such inference can be done in practice. For this suppose we are given a pair of discrete shapes represented as meshes, with the corresponding Laplace-Beltrami eigenfunctions. Our goal is to find the underlying functional map represented as a matrix C. The simplest way to do so is to construct a large system of linear equations, where each equation corresponds to one of the constraints mentioned above (either a functional constraint or the operator commutativity constraint) and find the best functional map by finding the matrix C that best satisfies the constraints in the least squares sense.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Conversion to Point-to-Point", "text": "As mentioned in Section 4, given a bijection T between two discrete shapes, and the basis vectors of their function spaces, the functional representation C of the map T can be obtained by solving a linear system.\nTo reconstruct the original mapping from the functional representation, however, is more challenging. The simplest method alluded to in Remark 4.1 to find a corresponding point y 2 N to a given point x 2 M would require constructing a function f : M ! R (either the indicator function, or a highly peaked Gaussian around x) obtaining its image g = TF (f ) using C and declaring y to be the point at which g(y) obtains the maximum. Such a method, however, would require O(VN VM ) operations for a pair of meshes with VN and VM vertices. Such complexity may be prohibitively expensive in practice for large meshes. To obtain a more efficient method, note that in the Laplace-Beltrami basis\nx, the delta function around a point x 2 M , has the co-\nAlgorithm 1 FUNCTIONAL MAP INFERENCE FOR MATCHING 1.\nCompute a set of descriptors for each point on M and N , and create function preservation constraints. 2. If landmark correspondences or part decomposition constraints are known, compute the function preservation constraints using those. 3. Include operator commutativity constraints for relevant linear operators on M and N (e.g. Laplace-Beltrami or symmetry). 4. Incorporate the constraints into a linear system and solve it in the least squares sense to compute the optimal C. 5. Refine the initial solution C using the iterative method in Section 6.2. 6. If point correspondences are required, obtain them using the method in Section 6.1.\nefficients: ai = M i (x)\n. This can be seen for example, since\nx = lim t!0 + k M t (x, \u2022) = lim t!0 + P 1 i=0 e t i M i (x) M i (\u2022) , where k M t (\u2022,\n\u2022) is the heat kernel at time t on the shape M . Therefore, given a matrix M of the Laplace-Beltrami eigenfunctions of M , where each column corresponds to a point and each row to an eigenfunction, one can find the image of all of the delta functions centered at points of M simply as C M . Now recall that by Plancherel's theorem, given two functions g1 and g2 both defined on N , with spectral coefficients b1 and b2, P\ni (b1i b2i) 2 = R N (g1(y) g2(y)) 2 \u00b5(y).\nThat is, the distances between the coefficient vectors is equal to the L 2 difference between the functions themselves. Therefore an efficient way to find correspondences between points is to consider for every point of C M its nearest neighbor in N . Using an efficient proximity search structure, such as a kd-tree, this procedure will require only O(VN log VN + VM log VN ) operations, giving a significant efficiency increase in practice.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Post-Processing Iterative Refinement", "text": "The observation made in Section 6.1 can also be used to refine a given matrix C to make it closer to a point-to-point map. Suppose we are given an initial estimate matrix C0 that we believe comes from a point-to-point map T . As noted in Section 6.1, theoretically C0 must be such that each column of C0 M coincides with some column of N . If we treat M and N as two point clouds with dimensionality equal to the number of eigenvalues used in the functional representation C0 then this means that C0 must align M and N . Moreover, since by Theorem 5.1 we expect the mapping matrix C0 to be orthonormal, we can phrase the problem of finding the optimal mapping matrix C as rigid alignment between M and N . Thus an iterative refinement of C0 can be obtained via:\n1. For each column x of C0 M find the closestx in N .\n2. Find the optimal orthonormal C minimizing P kCx xk.\n3. Set C0 = C and iterate until convergence.\nNote that this technique is identical to the standard Iterative Closest Point algorithm of Besl & McKay, [1992], except that it is done in the embedded functional space, rather than the standard Euclidean space. Note also that this method cannot be used on its own to obtain the optimal functional matrix C because the embedding M and N are only defined up to a sign change (or more generally an orthonormal transformation within an eigenspace). Therefore, it is essential to have a good initial estimate matrix C0. Finally, note that the output of this procedure is not only a functional matrix C but also a point-to-point correspondence given by nearest neighbor assignment between points on M and N . We will use this method to obtain good point-to-point maps when we apply these observations to devise an efficient shape matching method in Section 7.  Kim et al. [2011] andSahillioglu andYemez [2011] on two datasets: SCAPE [Anguelov et al. 2005] and TOSCA [Bronstein et al. 2008] with and without symmetric maps allowed (solid and dashed lines respectively). Note that since our method is intrinsic only symmetric (solid line) evaluation is meaningful.\nRelation to Existing Methods. Note that this refinement step is similar to existing spectral matching methods such as [Jain et al. 2007;Mateus et al. 2008;Sharma and Horaud 2010]. However, in addition to having a good initial estimate C0, our method is different since we allow \"mixing\" across eigenvectors corresponding to different eigenvalues. In addition, the functional representation allows to formulate other constraints such as operator commutativity (Section 5.4) and represent the map itself compactly.", "publication_ref": ["b2", "b13", "b23", "b0", "b4", "b10", "b16", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Shape Matching", "text": "In this section we describe a simple yet very effective method for non-rigid shape matching based on the functional representation of mappings between shapes.\nThe simplest version of the shape matching algorithm is summarized in Algorithm 1. Namely, suppose we are given two shapes M and N in their discrete (e.g. mesh) representation, and the Laplace-Beltrami eigen-decomposition, we simply compute functional constraints that correspond to descriptor and segment preservation constraints together with the operator commutativity, form a linear system of equations and solve it in the least squares sense. If necessary, we refine the solution using the method in Section 6.2 and compute the point-to-point map using the method in Section 6.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "The key ingredients necessary to implement this method in practice are the computation of the eigendecomposition of the Laplace-Beltrami operator, the descriptors used in the function preservation constraints, and a method to obtain landmark or segment correspondences. Note that our framework allows great flexibility for the choice of descriptors and correspondence constraints since they all fit into a general function preservation framework. In our implementation we have used the cotangent scheme [Meyer et al. 2002] for the Laplace-Beltrami operator on meshed surfaces. We also used the Wave Kernel Signature (WKS) and Heat Kernel Signature descriptors of [Aubry et al. 2011] and [Sun et al. 2009]. Because the method described above is fully intrinsic and does not distinguish between left and right symmetries, it is also important to resolve ambiguities using correspondence constraints. However, since point-to-point correspondences (e.g. landmark) are generally unstable and difficult to obtain without manual intervention, we have used segment correspondences instead. Towards this goal, we first pre-segment every shape using the persistence-based segmentation technique of [Skraba et al. 2010] with the WKS at a fixed Source Target 1\nTarget 2 Target 3\nFigure 7: Maps between remeshed versions of shapes from the SCAPE collection, mapping the coordinate functions from the source to the three target shapes using an inferred functional map.\nenergy value of the underlying function (we used e = 5 in all examples below). This gives a relatively stable segmentation with a small number of segments (between 3 and 7 in the shapes we examined). Given a pair of shapes, we first compute the segment correspondence constraints. For this, we first compute the set of candidate pairs of segments from the two shapes by computing segment descriptors and finding the ones likely to match. For segment descriptors we use the sum of the WKS values of the points in the segment. Given a pair of candidate segment matches s1, s2 on M and N respectively, we construct a set of functional constraints using the Heat Kernel Map [Ovsjanikov et al. 2010] based on segment correspondences. We combine these together with the Laplace-Beltrami commutativity constraint and the WKS constraints into a single linear system and solve it to find the optimal functional mapping matrix C. Finally, we refine the solution using the iterative method described in Section 6.2 and find the final dense point-to-point correspondences using the method in 6.1.", "publication_ref": ["b17", "b1", "b27", "b26", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We have evaluated our basic method for computing point-to-point correspondences on the shape matching benchmark of Kim et al. [2011] in which the authors showed state-of-the art results using their Blended Intrinsic Maps (BIM) approach. Using the correspondence evaluation, Figure 6 shows the results of our automated shape matching on two standard datasets used in the benchmark of Kim et al. [2011] on which their method reported significant improvement over prior work. In addition, we evaluated a recent shape matching method by Sahillioglu and Yemez [2011] which did not appear in the benchmark. The graphs show the percent of correspondences which have geodesic error smaller than a threshold. Note that our method shows quality improvement over Blended Intrinsic Maps on both datasets. Note also that all of the parameters for our system were fixed before running the benchmark evaluation and were therefore not optimized for benchmark performance in any way.\nAlthough the shapes in both SCAPE and TOSCA datasets have the same connectivity structure, this information is not used by our method, and is not needed for applying our algorithm. To demonstrate this, Figure 7 shows three maps computed by our method between a source and three target shapes from the SCAPE collection, all remeshed with uniform remeshing. We show the map by transferring the XYZ coordinate functions to the target shapes using the inferred functional maps. These functions are then rendered as RGB channels on the source and target shapes.\n8 Other Applications", "publication_ref": ["b13", "b13"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Improving Point-to-Point Maps", "text": "Although the method presented above achieves state-of-the art performance on an isometric shape matching benchmark, its true strength lies perhaps not in the exact optimization decisions (e.g. the descriptors or segmentation techniques used) but rather in the representation of the mapping itself. To illustrate this, we have used the functional representation to improve the quality of other isometric shape matching methods. Here, instead of computing a new point-to-point map between a pair of shapes, we assume that we are given a point-to-point map obtained by any external method and our goal is to improve it through local refinement. The functional representation is a natural choice for this task because conversion to and from this representation is very fast (Sections 4 and 6.1) while iterative refinement does not require expensive non-linear optimization techniques. To test this approach, we used the point-to-point maps provided by the methods of Kim et al. [2011] andSahillioglu andYemez [2011], and converted them into a functional representation using the basis provided by the first 100 eigenfunctions of the cotangent weight scheme [Meyer et al. 2002] of the Laplace-Beltrami operator. We then ran the iterative refinement procedure described in Section 6.2 for 20 iterations, and converted the functional representation back to a point-to-point map (Section 6.1). For models ranging in 10-50 thousand points this entire procedure took 5-15 seconds on one core of Intel Xeon 3.2 GHz on average. Figure 8 shows the improvement in the quality of the final refined maps. Note that in both datasets and both sets of input maps this simple technique achieves significant improvements in the quality of the final maps.\nFigure 9 illustrates the types of improvements provided by this iterative refinement method, For four sample pairs of meshes it shows the error of the initial mapping provided by Kim et al. [2011] and the error of the map after refinement. The color of each vertex is proportional to the distance of its match from the ground truth. As can be seen from this figure qualitatively, iterative refinement is able to recover from errors if the initial map is approximately correct.", "publication_ref": ["b13", "b23", "b17", "b13"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Shape Collections", "text": "In addition to improving maps between pairs of shapes, our representation is also beneficial for improving collections of shape maps.\nICSM Recently it has been shown, [Nguyen et al. 2011], that when given a collection of shapes and maps between them, the pairwise maps can be considerably improved by considering the context provided by the collection. The main idea is to compose maps along cycles (e.g. compose the maps L ! M , M ! N , N ! L) in the resulting graph, and compare the composite map to the identity. When the composition result on a cycle is far from the identity, it is an indication that one or more of the maps on that cycle may be faulty. The paper [Nguyen et al. 2011] presented a method (which we will refer to as ICSM -Iteratively Corrected Shape Maps) to Figure 9: Improvement of the maps provided by the method of Kim et al. [2011] using our functional representation with iterative refinement. For each pair of shapes, we show the source shape, and the errors in map of the target shape with color proportional to the distance from the ground truth. Note that iterative refinement efficiently removes spurious matches if the initial map is approximately correct.\ncompute a score for the map between every pair of shapes, as a function of the consistency of the composed maps along 3-cycles. Then a shape graph is defined, where each shape is a vertex, and an edge between shapes is weighted by the computed score. Maps are improved by replacing the original maps with composed maps along the shortest paths in this graph.\nICSM can be easily applied using the functional maps representation, without requiring conversion to point-to-point maps. For map composition we simply use matrix multiplication, i.e. mL,N = mM,N mL,M \u2318 CMN CLM , where CMN and CLM are the functional maps M ! N and L ! M respectively. Note, that if CMN and CLM are orthogonal matrices, the composition will also be orthogonal. As a measure of the distance of a functional map from the identity we use kC IkFro where I is the identity matrix.\nAlthough it is possible to convert the functional maps to point-topoint maps and run ICSM on the point-to-point maps, it is advantageous to do it directly on the functional maps, as the evaluation of distance from identity is considerably less expensive: for point-  to-point maps, one needs to compute the geodesic distance between the mapped point and the source point, whereas for functional maps we just need to compute the Frobenius norm of a matrix. Furthermore, evaluating the error of a point-to-point map will be more costly for a denser mesh, whereas the complexity remains the same for the functional map.\nTo evaluate the use of ICSM with functional maps, we generated all the maps between the first 11 meshes in the SCAPE database. As our method is fully intrinsic, some of the generated maps were symmetric (i.e. left side mapped to right side), and some were straight. Figure 10(a) shows the average geodesic error for all the pairs in the collection. The pairs which have large errors are usually pairs for which the symmetric map was chosen. Figure 10(b) shows the average geodesic error after applying ICSM to the functional maps, as described earlier. Note that the maps improved considerably, leaving only a single shape with noticeable errors.", "publication_ref": ["b18", "b18", "b13"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Map diffusion", "text": "In [Nguyen et al. 2011] map improvement was achieved by re-writing maps by compositions of other maps along shortest paths in the shape graph. However, if we use the functional map representation, we can define a more subtle map improvement, which takes into consideration additional information. Specifically, we can replace a map with a weighted average of other maps, i.e.:\u0108ij = P k w kj C kj w ik C ik , where Cij is the map between the shapes Mi and Mj, and wij is a weight which describes our confidence in the map -thus being better able to deal with the effects of noise.\nIn fact, we can construct a \"SuperMap\" for the whole collection, by creating a matrix whose (i, j)-th block is wjiCji. Computing the weighted average of maps described previously is equivalent to raising the \"SuperMap\" matrix to the power of 2. This approach can be thought of as applying heat diffusion in the space of maps, similarly to what has been done recently in [Singer and Wu 2011]. Our succinct map representation may allow for a generalization of [Singer and Wu 2011], from rotations in R 3 to general maps between shapes.\nWe have applied this simple idea to the maps generated by ICSM, computing wij from the ICSM weights, and subsequently applying the ICP post-processing described in Section 6.2. Figure 10(c) shows the average geodesic errors of the resulting maps. As can be seen in the figure, map diffusion succeeded to further reduce the error on the problematic shape where ICSM could not improve further. Figure 11 shows the percent of correspondences which have geodesic error smaller than a threshold, for the maps generated by our matching method, the maps after ICSM, the maps after ICSM and diffusion, and the maps generated by [Kim et al. 2011]. Again, we can see that applying map diffusion to the result of ICSM improves the map collection.", "publication_ref": ["b18", "b25", "b25", "b13"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Function (Segmentation) Transfer", "text": "As mentioned earlier, one of the advantages of the functional representation is that it reduces the transfer of functions across shapes to a matrix product, without resorting to establishing point-to-point correspondences. This is particularly useful since function transfer is one of the key applications of maps between shapes and obtaining point-to-point correspondences is often challenging. We illustrate the performance of this idea on the task of segmentation transfer across shapes. Here we are given a pair of shapes where one of the shapes is pre-segmented and the goal is to find a compatible segmentation of the second shape. To achieve this task we simply construct an indicator function for each of the segments on the source shape and use the functional map to transfer this indicator function. Then each point on the target map will have a set of values for each of the transferred segments. Finally, if \"hard\" clustering is required, we associate with the point the segment that produced the maximum of these values.\nFigure 12 shows this idea applied to several shapes from the TOSCA and SCAPE datasets. For each pair of shapes we show the image of the the indicator function of one of the segments as well as the final \"hard\" segmentation. Note that throughout this procedure no point-to-point correspondences were used.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusion & Future Work", "text": "In this paper we have introduced a novel representation for maps between pairs of shapes that generalizes the standard notion of a map as a pairing of points. By concentrating on finding correspondences between generic functions defined on the shapes, we devised a representation that is multiscale, compact, and amenable to efficient optimization. Perhaps the most important property of the functional representation is that many natural constraints on a map become linear, making direct optimization feasible. We have demonstrated the effectiveness of our representation both by achieving state-of-theart results on an isometric shape matching benchmark and by showing that other existing methods can benefit from this representation as a post-processing step. Finally, we have shown its effectiveness in segmentation transfer across pairs of shapes without establishing correspondences and on joint analysis of large collections of shapes.\nIn the future we will look more carefully at the optimal choice of bases for the functional representation, thus extending the scope of functional maps to more general classes of deformations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements: The authors would like to acknowledge NSF grants FODAVA 0808515, IIS 0914833 and CCF 1011228, a research grant from Google, Inc., and the support of the Max Planck Center for Visual Computing and Communications.\nFigure 12: Segmentation transfer using the functional map representation. For each pair of shapes we show 3 figures: the userprovided source segmentation of the first shape, the image of one of the indicator functions of a segment using the functional map computed with our method, and the final segmentation transfer onto the target shape. Note that point correspondences were not computed at any point during this procedure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proof of Theorem 5.1", "text": "It is well-known that isometries commute with the Laplace-Beltrami operator thus we need only concentrate on the proving the first statement. Therefore suppose that the map T is volume-preserving. This means, in particular, that \u00b5 N (T (x)) = \u00b5 M (x) 8 x 2 M , where \u00b5 is the volume element. Recall that in the LB basis: x). Note that the integrands in Ci,j and Dj,i are the same. Moreover, since T is volume preserving, the integrals themselves have to agree. It follows that C = D T . But since D is the functional representation of the inverse map T 1 we must have CD = DC = Id. Therefore, C T C = Id and C is orthonormal. The proof under the assumption of discrete orthonormality is identical.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "SCAPE: Shape completion and animation of people", "journal": "", "year": "2005", "authors": "D Anguelov; P Srinivasan; D Koller; S Thrun; J Rodgers; J Davis"}, {"ref_id": "b1", "title": "The wave kernel signature: A quantum mechanical approach to shape analysis", "journal": "", "year": "2011", "authors": "M Aubry; U Schlickewei; D Cremers"}, {"ref_id": "b2", "title": "A method for registration of 3-d shapes", "journal": "IEEE TPAMI", "year": "1992", "authors": "P J Besl; N D Mckay"}, {"ref_id": "b3", "title": "Generalized multidimensional scaling: a framework for isometry-invariant partial surface matching", "journal": "PNAS", "year": "2006", "authors": "A M Bronstein; M M Bronstein; R Kimmel"}, {"ref_id": "b4", "title": "Numerical Geometry of Non-Rigid Shapes", "journal": "Springer", "year": "2008", "authors": "A Bronstein; M Bronstein; R Kimmel"}, {"ref_id": "b5", "title": "The Quadratic Assignment Problem: Theory and Algorithms", "journal": "Kluwer Academic Publishers", "year": "1998", "authors": "\u00c7 Ela; E "}, {"ref_id": "b6", "title": "Approximately isometric shape correspondence by matching pointwise spectral features and global geodesic structures", "journal": "", "year": "2011", "authors": "A Dubrovina; R Kimmel"}, {"ref_id": "b7", "title": "Consistent segmentation of 3D models", "journal": "Computers and Graphics (Proc. SMI)", "year": "2009", "authors": "A Golovinskiy; T Funkhouser"}, {"ref_id": "b8", "title": "Non-rigid registration under isometric deformations", "journal": "CGF (Proc. SGP)", "year": "2008", "authors": "Q.-X Huang; B Adams; M Wicke; L J Guibas"}, {"ref_id": "b9", "title": "Joint shape segmentation with linear programming", "journal": "ACM TOG (Proc. SIG-GRAPH Asia)", "year": "2011", "authors": "Q Huang; V Koltun; L Guibas"}, {"ref_id": "b10", "title": "Non-rigid spectral correspondence of triangle meshes", "journal": "International Journal on Shape Modeling", "year": "2007", "authors": "V Jain; H Zhang; O Van Kaick"}, {"ref_id": "b11", "title": "Learning 3D Mesh Segmentation and Labeling", "journal": "ACM Transactions on Graphics", "year": "2010", "authors": "E Kalogerakis; A Hertzmann; K Singh"}, {"ref_id": "b12", "title": "Perturbation Theory for Linear Operators", "journal": "Springer-Verlag GmbH", "year": "1995", "authors": "T Kato"}, {"ref_id": "b13", "title": "Blended intrinsic maps", "journal": "", "year": "2011", "authors": "V G Kim; Y Lipman; T Funkhouser"}, {"ref_id": "b14", "title": "Electors voting for fast automatic shape correspondence", "journal": "Computer Graphics Forum", "year": "2010", "authors": "Kin-Chung Au; O Tai; C.-L Cohen-Or; D Zheng; Y Fu; H "}, {"ref_id": "b15", "title": "M\u00f6bius voting for surface correspondence", "journal": "", "year": "2009", "authors": "Y Lipman; T Funkhouser"}, {"ref_id": "b16", "title": "Articulated shape matching using Laplacian eigenfunctions and unsupervised point registration", "journal": "", "year": "2008", "authors": "D Mateus; R P Horaud; D Knossow; F Cuzzolin; E Boyer"}, {"ref_id": "b17", "title": "Discrete differential-geometry operators for triangulated 2-manifolds", "journal": "", "year": "2002", "authors": "M Meyer; M Desbrun; P Schr\u00f6der; A H Barr"}, {"ref_id": "b18", "title": "An optimization approach to improving collections of shape maps", "journal": "", "year": "2011", "authors": "A Nguyen; M Ben-Chen; K Welnicka; Y Ye; L Guibas"}, {"ref_id": "b19", "title": "Global intrinsic symmetries of shapes", "journal": "Comp. Graph. Forum", "year": "2008", "authors": "M Ovsjanikov; J Sun; L Guibas"}, {"ref_id": "b20", "title": "One point isometric matching with the heat kernel", "journal": "CGF", "year": "2010", "authors": "M Ovsjanikov; Q Merigot; F Memoli; L Guibas"}, {"ref_id": "b21", "title": "A correspondence-less approach to matching of deformable shapes", "journal": "", "year": "2011", "authors": "J Pokrass; A M Bronstein; M M Bronstein"}, {"ref_id": "b22", "title": "Laplace-beltrami eigenfunctions for deformation invariant shape representation", "journal": "", "year": "2007", "authors": "R M Rustamov"}, {"ref_id": "b23", "title": "Coarse-to-fine combinatorial matching for dense isometric shape correspondence", "journal": "Computer Graphics Forum", "year": "2011", "authors": "Y Sahillio\u01e7lu; Y Yemez"}, {"ref_id": "b24", "title": "Shape matching based on diffusion embedding and on mutual isometric consistency", "journal": "", "year": "2010", "authors": "A Sharma; R P Horaud"}, {"ref_id": "b25", "title": "Vector diffusion maps and the connection laplacian", "journal": "", "year": "2011", "authors": "A Singer; H Wu"}, {"ref_id": "b26", "title": "Persistence-based segmentation of deformable shapes", "journal": "", "year": "2010", "authors": "P Skraba; M Ovsjanikov; F Chazal; L Guibas"}, {"ref_id": "b27", "title": "A concise and provably informative multi-scale signature based on heat diffusion", "journal": "", "year": "2009", "authors": "J Sun; M Ovsjanikov; L Guibas"}, {"ref_id": "b28", "title": "Intrinsic shape matching by planned landmark sampling", "journal": "", "year": "2011", "authors": "A Tevs; A Berner; M Wand; I Ihrke; H.-P Seidel"}, {"ref_id": "b29", "title": "Prior knowledge for part correspondence", "journal": "", "year": "2011", "authors": "O Van Kaick; A Tagliasacchi; O Sidi; H Zhang; D Cohen-Or; L Wolf; G Hamarneh"}, {"ref_id": "b30", "title": "A survey on shape correspondence", "journal": "Computer Graphics Forum", "year": "2011", "authors": "O Van Kaick; H Zhang; G Hamarneh; D Cohen-Or"}, {"ref_id": "b31", "title": "The Classical Groups: Their Invariants and Representations", "journal": "Princeton University Press", "year": "1946", "authors": "H Weyl"}, {"ref_id": "b32", "title": "Style-content separation by anisotropic part scales", "journal": "ACM TOG (Proc. SIGGRAPH Asia)", "year": "2010", "authors": "K Xu; H Li; H Zhang; D Cohen-Or; Y Xiong; Z Cheng"}, {"ref_id": "b33", "title": "Template-based 3d model fitting using dual-domain relaxation", "journal": "IEEE Transactions on Visualization and Computer Graphics", "year": "2010", "authors": "I.-C Yeh; C.-H Lin; O Sorkine; T.-Y Lee"}, {"ref_id": "b34", "title": "Deformation-driven shape correspondence", "journal": "", "year": "2008", "authors": "H Zhang; A Sheffer;  Cohen-Or; Q Zhou; O Van Kaick; A Tagliasacchi"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Remark 4. 1 .1The original mapping T can be recovered from TF . Indeed, to recover the image T (a) of any point a on M , construct an indicator function f : M ! R, s.t. f (a) = 1 and f (x) = 0 8 x 6 = a 2 M. By construction if g = TF (f ), then g(y) = f T 1", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "left to right map (d) head to tail map", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure2: Two shapes with three maps between them, each rendered as a point-to-point mapping through color correspondence (top) and its functional representation (bottom) with colors proportional to matrix values. Note that the least isometric map in (d) leads to a less sparse functional matrix.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure3: Average mapping error vs. number of eigenvalues used in the representation. For each shape with a known ground-truth pointto-point correspondence (shown as a color correspondence), we computed its functional representation and measured its accuracy in finding a point-to-point map. Note that although more eigenvalues lead to an increase in accuracy, maps that correspond to bigger deformations require more basis vectors, capturing the intuition that near-isometric maps are more compactly represented.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 middle for illustration).Note that the eigenfunctions of the unweighted discretization of Laplace-Beltrami operator provide a more compact representation", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure5: Mapping of the three coordinate functions from the source shape shown in Figure3aonto the target shape using an interpolation between two maps C = \u21b5C1 + (1 \u21b5)C2. Note that the mapped function varies continuously under changes of the parameter \u21b5.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure6: Comparison of our method with the state-of-the-art methods ofKim et al. [2011] and Sahillioglu andYemez [2011] on two datasets: SCAPE[Anguelov et al. 2005] and TOSCA[Bronstein et al. 2008] with and without symmetric maps allowed (solid and dashed lines respectively). Note that since our method is intrinsic only symmetric (solid line) evaluation is meaningful.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Improvement of the results other methods using our representation and iterative refinement.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 :10Figure 10: (a) Average geodesic errors of maps on a collection of 11 shapes from the SCAPE benchmark. (b) Errors after running ICSM. (c) Errors after running diffusion on ICSM results.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 11 :11Figure 11: Improving a collection of shape maps using ICSM and diffusion of maps. See the text for details.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "TF (\u21b51f1 + \u21b52f2) = (\u21b51f1 + \u21b52f2) T 1 = \u21b51f1 T 1 + \u21b52f2 T 1 = \u21b51TF (f1) + \u21b52TF (f2)", "formula_coordinates": [3.0, 54.0, 344.01, 240.04, 20.0]}, {"formula_id": "formula_1", "formula_text": "TF (f ) = TF X i ai M i ! = X i aiTF \u21e3 M i \u2318 .", "formula_coordinates": [3.0, 84.6, 451.46, 178.84, 33.34]}, {"formula_id": "formula_2", "formula_text": "TF M i = P j cij N j for some {cij} and TF (f ) = X i ai X j cij N j = X j X i aicij N j .(1)", "formula_coordinates": [3.0, 71.68, 501.36, 222.36, 48.39]}, {"formula_id": "formula_3", "formula_text": "TF X i ai M i ! = X j X i aicij N j ,", "formula_coordinates": [3.0, 367.4, 504.15, 141.15, 33.34]}, {"formula_id": "formula_4", "formula_text": "j P i aicij N j \u21e1 P n j=0 P m i=0 aicij N j ,", "formula_coordinates": [4.0, 327.68, 295.89, 149.65, 21.47]}, {"formula_id": "formula_5", "formula_text": "f : M ! R another function SF (f ) : M ! R obtained as SF (f )(x) = f (S 1 (x))", "formula_coordinates": [6.0, 54.0, 300.27, 240.05, 25.88]}, {"formula_id": "formula_6", "formula_text": "P x i(x) j (x) = ij , or if the underlying map T (discrete or con- tinuous) is volume preserving, i.e. \u00b5 M (x) = \u00b5 N (T (x)", "formula_coordinates": [6.0, 54.0, 567.04, 240.05, 26.93]}, {"formula_id": "formula_7", "formula_text": "Algorithm 1 FUNCTIONAL MAP INFERENCE FOR MATCHING 1.", "formula_coordinates": [7.0, 54.0, 56.9, 230.35, 20.92]}, {"formula_id": "formula_8", "formula_text": "efficients: ai = M i (x)", "formula_coordinates": [7.0, 54.0, 216.55, 91.75, 9.89]}, {"formula_id": "formula_9", "formula_text": "x = lim t!0 + k M t (x, \u2022) = lim t!0 + P 1 i=0 e t i M i (x) M i (\u2022) , where k M t (\u2022,", "formula_coordinates": [7.0, 54.0, 221.6, 240.04, 27.58]}, {"formula_id": "formula_10", "formula_text": "i (b1i b2i) 2 = R N (g1(y) g2(y)) 2 \u00b5(y).", "formula_coordinates": [7.0, 63.73, 309.61, 163.47, 18.11]}], "doi": ""}