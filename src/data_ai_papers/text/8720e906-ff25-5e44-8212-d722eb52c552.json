{"title": "Picture: A Probabilistic Programming Language for Scene Perception", "authors": "Tejas D Kulkarni; Pushmeet Kohli; Joshua B Tenenbaum", "pub_date": "", "abstract": "Recent progress on probabilistic modeling and statistical learning, coupled with the availability of large training datasets, has led to remarkable progress in computer vision. Generative probabilistic models, or \"analysis-by-synthesis\" approaches, can capture rich scene structure but have been less widely applied than their discriminative counterparts, as they often require considerable problem-specific engineering in modeling and inference, and inference is typically seen as requiring slow, hypothesize-and-test Monte Carlo methods. Here we present Picture, a probabilistic programming language for scene understanding that allows researchers to express complex generative vision models, while automatically solving them using fast general-purpose inference machinery. Picture provides a stochastic scene language that can express generative models for arbitrary 2D/3D scenes, as well as a hierarchy of representation layers for comparing scene hypotheses with observed images by matching not simply pixels, but also more abstract features (e.g., contours, deep neural network activations). Inference can flexibly integrate advanced Monte Carlo strategies with fast bottomup data-driven methods. Thus both representations and inference strategies can build directly on progress in discriminatively trained systems to make generative vision more robust and efficient. We use Picture to write programs for 3D face analysis, 3D human pose estimation, and 3D object reconstruction -each competitive with specially engineered baselines.", "sections": [{"heading": "Introduction", "text": "Probabilistic scene understanding systems aim to produce high-probability descriptions of scenes conditioned on observed images or videos, typically either via discriminatively trained models or generative models in an \"analysis by synthesis\" framework. Discriminative approaches lend themselves to fast, bottom-up inference methods and relatively knowledge-free, data-intensive training regimes, and have been remarkably successful on many recognition problems [9,23,26,30]. Generative approaches hold out the promise of analyzing complex scenes more richly and flex-ibly [11,12,51,7,19,29,31,16,21], but have been less widely embraced for two main reasons: Inference typically depends on slower forms of approximate inference, and both model-building and inference can involve considerable problem-specific engineering to obtain robust and reliable results. These factors make it difficult to develop simple variations on state-of-the-art models, to thoroughly explore the many possible combinations of modeling, representation, and inference strategies, or to richly integrate complementary discriminative and generative modeling approaches to the same problem. More generally, to handle increasingly realistic scenes, generative approaches will have to scale not just with respect to data size but also with respect to model and scene complexity. This scaling will arguably require general-purpose frameworks to compose, extend and automatically perform inference in complex structured generative models -tools that for the most part do not yet exist.\nHere we present Picture, a probabilistic programming language that aims to provide a common representation language and inference engine suitable for a broad class of generative scene perception problems. We see probabilistic programming as key to realizing the promise of \"vision as inverse graphics\". Generative models can be represented via stochastic code that samples hypothesized scenes and generates images given those scenes. Rich deterministic and stochastic data structures can express complex 3D scenes that are difficult to manually specify. Multiple representation and inference strategies are specifically designed to address the main perceived limitations of generative approaches to vision. Instead of requiring photo-realistic generative models with pixel-level matching to images, we can compare hypothesized scenes to observations using a hierarchy of more abstract image representations such as contours, discriminatively trained part-based skeletons, or deep neural network features. Available Markov Chain Monte Carlo (MCMC) inference algorithms include not only traditional Metropolis-Hastings, but also more advanced techniques for inference in high-dimensional continuous spaces, such as elliptical slice sampling, and Hamiltonian Monte Carlo which can exploit the gradients of automatically differentiable renderers. These top-down inference approaches are integrated with bottom-up and automatically constructed data-driven Given current\nScene Language Approximate Renderer Representation Layer Scene I", "publication_ref": ["b8", "b22", "b25", "b29", "b10", "b11", "b50", "b6", "b18", "b28", "b30", "b15", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Inference Engine", "text": "Automatically produces MCMC, HMC, Elliptical Slice, Data-driven proposals  only the scene description S and image ID changes across problems. Every probabilistic graphics program f defines a stochastic procedure that generates both a scene description and all the other information needed to render an approximation IR of a given observed image ID. The program f induces a joint probability distribution on these program traces \u03c1. Every Picture program has the following components. Scene Language: Describes 2D/3D scenes and generates particular scene related trace variables S \u03c1 \u2208 \u03c1 during execution. Approximate Renderer: Produces graphics rendering IR given S \u03c1 and latents X \u03c1 for controlling the fidelity or tolerance of rendering. Representation Layer: Transforms ID or IR into a hierarchy of coarse-to-fine image representations \u03bd(ID) and \u03bd(IR) (deep neural networks [25,23], contours [8] and pixels).\n. . . qP ((S \u21e2 , X \u21e2 ) ! (S 0\u21e2 , X 0\u21e2 )) q hmc (S \u21e2 real ! S 0\u21e2 real ) q slice (S \u21e2 real ! S 0\u21e2 real ) q data ((S \u21e2 , X \u21e2 ) ! (S 0\u21e2 , X 0\u21e2 )) New (S \u21e2 , X \u21e2 ) (S 0\u21e2 , X\nComparator: During inference, IR and ID can be compared using a likelihood function or a distance metric \u03bb (as in Approximate Bayesian Computation [44]). (b) Inference Engine: Automatically produces a variety of proposals and iteratively evolves the scene hypothesis S to reach a high probability state given ID. (c):\nRepresentative random scenes drawn from probabilistic graphics programs for faces, objects, and bodies.\nproposals, which can dramatically accelerate inference by eliminating most of the \"burn in\" time of traditional samplers and enabling rapid mode-switching.\nWe demonstrate Picture on three challenging vision problems: inferring the 3D shape and detailed appearance of faces, the 3D pose of articulated human bodies, and the 3D shape of medially-symmetric objects. The vast majority of code for image modeling and inference is reusable across   3. The variables MU, PC, EV correspond to the mean shape/texture face, principal components, and eigenvectors respectively (see [36] for details). These arguments parametrize the prior on the learned shape and appearance of 3D faces. The argument VERTEX ORDER denotes the ordered list of vertices to render triangle based meshes. The observe directive constrains the program execution based on both the pixel data and CNN features. The infer directive starts the inference engine with the specified set of inference schemes (takes the program trace, a callback function CB for debugging, number of iterations and inference schemes). In this example, data-driven proposals are run for a few iterations to initialize the sampler, followed by slice sampling moves to further refine the high dimensional scene latents. these and many other tasks. We shows that Picture yields performance competitive with optimized baselines on each of these benchmark tasks.", "publication_ref": ["b24", "b22", "b7", "b43", "b35"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Picture Language", "text": "Picture descends from our earlier work on generative probabilistic graphics programming (GPGP) [31], and also incorporates insights for inference from the Helmholtz machine [17,6] and recent work on differentiable renderers [29]   We tested our approach on a held-out dataset of 2D image projections of laser-scanned faces from [36]. Our short probabilistic program is applicable to non-frontal faces and provides reasonable parses as illustrated above using only general-purpose inference machinery.\nFor quantitative metrics, refer to section 4.1.\nand informed samplers [19]. GPGP aimed to address the main challenges of generative vision by representing visual scenes as short probabilistic programs with random variables, and using a generic MCMC (single-site Metropolis-Hastings) method for inference. However, due to modeling limitations of earlier probabilistic programming languages, and the inefficiency of the Metropolis-Hastings sampler, GPGP was limited to working with low-dimensional scenes, restricted shapes, and low levels of appearance variability. Moreover, it did not support the integration of bottom-up discriminative models such as deep neural networks [23,25] for data-driven proposal learning. Our current work extends the GPGP framework in all of these directions, letting us tackle a richer set of real-world 3D vision problems.\nPicture is an imperative programming language, where expressions can take on either deterministic or stochastic values. We use the transformational compilation technique [46] to implement Picture, which is a general method of transforming arbitrary programming languages into probabilistic programming languages. Compared to earlier formulations of GPGP, Picture is dynamically compiled at run-time (JITcompilation) instead of interpreting, making program execution much faster.\nA Picture program f defines a stochastic procedure that generates both a scene description and all other information needed to render an approximation image I R for comparison with an observed image I D . The program f induces a joint probability distribution on the program trace \u03c1 = {\u03c1 i }, the set of all random choices i needed to specify the scene hypothesis S and render I R . Each random choice \u03c1 i can belong to a familiar parametric or non-parametric family of distributions, such as Multinomial, MvNormal, DiscreteUniform, Poisson, or Gaussian Process, but in being used to specify the trace of a probabilistic graphics program, their effects can be combined much more richly than is typical for random variables in traditional statistical models.\nConsider running the program in Figure 2 unconditionally (without observed data): as different \u03c1 i 's are encountered (for e.g. coeff ), random values are sampled w.r.t their underlying probability distribution and cached in the current state of the inference engine. Program execution outputs an image of a face with random shape, texture, camera and lighting parameters. Given image data I D , inference in Picture programs amounts to iteratively sampling or evolving program trace \u03c1 to a high probability state while respecting constraints imposed by the data (Figure 3). This constrained simulation can be achieved by using the observe language construct (see code in Figure 2), first proposed in Venture [32] and also used in [35,47].", "publication_ref": ["b30", "b16", "b5", "b28", "b35", "b18", "b22", "b24", "b45", "b31", "b34", "b46"], "figure_ref": ["fig_3", "fig_4", "fig_3"], "table_ref": []}, {"heading": "Architecture", "text": "In this section, we will explain the essential architectural components highlighted in Figure 1 (see Figure 4 for a summary of notation used). Scene Language: The scene language is used to describe 2D/3D visual scenes as probabilistic code. Visual scenes can be built out of several graphics primitives such as: description of 3D objects in the scene (e.g. mesh, z-map, volumetric), one or more lights, textures, and the camera information. It is important to note that scenes expressed as probabilistic code are more general than parametric prior density functions as is typical in generative vision models. The probabilistic programs we demonstrate in this paper embed ideas from computer-aided design (CAD) and nonparametric Bayesian statistics [37] to express variability in 3D shapes. Approximate Renderer (AR): Picture's AR layer takes in a scene representation trace S \u03c1 and tolerance variables X \u03c1 , and uses general-purpose graphics simulators (Blender [5] and OpenGL) to render 3D scenes. The rendering tolerance X \u03c1 defines a structured noise process over the rendering and is useful for the following purposes: (a) to make automatic inference more tractable or robust, analogous to simulated annealing (e.g. global or local blur variables in GPGP [31]), and (b) to soak up model mismatch between the true scene rendering I D and the hypothesized rendering I R . Inspired by the differentiable renderer [29], Picture also supports expressing AR's entire graphics pipeline as Picture code, enabling the language to express end-to-end differentiable generative models. Representation Layer (RL): To avoid the need for photorealistic rendering of complex scenes, which can be slow and modeling-intensive, or for pixel-wise comparison of hypothesized scenes and observed images, which can sometimes yield posteriors that are intractable for sampling-based inference, the RL supports comparison of generated and observed images in terms of a hierarchy of abstract features.  The RL can be defined as a function \u03bd which produces summary statistics given I D or I R , and may also have internal parameters \u03b8 \u03bd (e.g. weights of a deep neural net). For notational convenience, we denote \u03bd(I D ; \u03b8 \u03bd ) and \u03bd(I D ; \u03b8 \u03bd ) to be \u03bd(I D ) and \u03bd(I R ) respectively. RL produces summary statistics (features) that are used in two scenarios: (a) to compare the hypothesis I R with observed image I D during inference (RL denoted by \u03bd in this setting), and (b) as a dimensionality reduction technique for hashing learned data-driven proposals (RL denoted by \u03bd dd and its parameters \u03b8 \u03bd dd ). Picture supports a variety of summary statistic functions including raw pixels, contours [8] and supervised/unsupervised convolutional neural network (CNN) architectures [23,25]. Likelihood and Likelihood-free Comparator: Picture supports likelihood P (I D |I R ) inference in a bayesian setting. However, in the presence of black-box rendering simulators, the likelihood function is often unavailable in closed form. Given an arbitrary distance function \u03bb(\u03bd(I D ), \u03bd(I R )) (e.g. L1 error), approximate bayesian computation (ABC) [44] can be used to perform likelihood-free inference.", "publication_ref": ["b36", "b4", "b30", "b28", "b7", "b22", "b24", "b43"], "figure_ref": ["fig_2", "fig_5"], "table_ref": []}, {"heading": "Inference", "text": "We can formulate the task of image interpretation as approximately sampling mostly likely values of S \u03c1 given ob-served image I D (L stands for P (I D |I R , X \u03c1 )):\nP (S \u03c1 |I D ) \u221d P (S \u03c1 )P (X \u03c1 )\u03b4 render(S \u03c1 ,X \u03c1 ) (I R ) L dX \u03c1\nAutomatic inference in Picture programs can be especially hard due to a mix of discrete and continuous scene variables, which may be independent a priori but highly coupled in their posterior distributions (\"explaining away\"), and also because clutter, occlusion or noise can lead to local maxima of the scene posterior.\nGiven a program trace \u03c1, probabilistic inference amounts to updating (S \u03c1 , X \u03c1 ) to (S \u03c1 , X \u03c1 ) until convergence via proposal kernels q((S \u03c1 , X \u03c1 ) \u2192 q(S \u03c1 , X \u03c1 )). Let K = |{S \u03c1 }| + |{X \u03c1 }| and K = |{S \u03c1 }| + |{X \u03c1 }| be the total number of random choices in the execution before and after applying the proposal kernels q(.). Let the loglikelihoods of old and new trace be L = P (I D |I R , X) and L = P (I D |I R , X ) respectively. Let us denote the probabilities of deleted and newly created random choices created in S \u03c1 to be P (S \u03c1 del ) and P (S \u03c1 new ) respectively. Let q (S ,X )\u2192(S,X) := q((S \u03c1 , X \u03c1 ) \u2192 (S \u03c1 , X \u03c1 )) and q (S,X)\u2192(S ,X ) := q((S \u03c1 , X \u03c1 ) \u2192 (S \u03c1 , X \u03c1 )). The new trace (S \u03c1 , X \u03c1 ) can now be accepted or rejected using the acceptance ratio:\nmin 1, L P (S \u03c1 )P (X \u03c1 ) q (S ,X )\u2192(S,X) K P (S \u03c1 del ) L P (S \u03c1 )P (X \u03c1 ) q (S,X)\u2192(S ,X ) K P (S \u03c1 new )\n.\n(1)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distance Metrics and Likelihood-free Inference", "text": "The likelihood function in closed form is often unavailable when integrating top-down automatic inference with bottom-up computational elements. Moreover, this issue is exacerbated when programs use black-box rendering simulators. Approximate bayesian computation (ABC) allows Bayesian inference in likelihood-free settings, where the basic idea is to use a summary statistic function \u03bd(.), distance metric \u03bb(\u03bd(I D ), \u03bd(I R )) and tolerance variable X \u03c1 to approximately sample the posterior distribution [44].\nInference in likelihood-free settings can also be interpreted as a variant of the probabilistic approximate MCMC algorithm [44], which is similar to MCMC but with an additional tolerance parameter \u2208 X \u03c1 on the observation model. We can interpret our approach as systematically reasoning about the model error arising due to the difference of generative model's \"cartoon\" view of the world with reality. Let \u0398 be the space of all possible renderings I R that could be hypothesized and P be the error model (e.g. Gaussian). The target stationary distribution that we wish to sample can be expressed as:\nP (S \u03c1 |I D ) \u221d \u0398 P (S \u03c1 )P (\u03bd(I D )\u2212\u03bd(I R ))P (\u03bd(I R )|S \u03c1 )dI R .\nDuring inference, the updated scene S \u03c1 (assuming random choices remain unchanged, otherwise add terms relating to addition/deletion of random variables as in equation 1) can then be accepted with probability: min 1, P (\u03bd(I D ) \u2212 \u03bd(I R ))P (S \u03c1 )P (X \u03c1 ) q (S ,X )\u2192(S,X) P (\u03bd(I D ) \u2212 \u03bd(I R ))P (S \u03c1 )P (X \u03c1 ) q (S,X)\u2192(S ,X ) .", "publication_ref": ["b43", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Proposal Kernels", "text": "In this section, we will propose a variety of proposal kernels for scaling up Picture to complex 3D scenes.\nLocal and Blocked Proposals from Prior: Single site metropolis hastings moves on continuous variables and Gibbs moves on discrete variables can be useful in many cases. However, because the latent pose variables for objects in 3D scenes (e.g., positions and orientations) are often highly coupled, our inference library allows users to define arbitrary blocked proposals:\nq P ((S \u03c1 , X \u03c1 ) \u2192 (S \u03c1 , X \u03c1 )) = \u03c1 i \u2208(S \u03c1 ,X \u03c1 ) P (\u03c1 i )\nGradient Proposal: Picture inference supports automatic differentiation for a restricted class of programs (where each expression provides output and gradients w.r.t input). Therefore it is straightforward to obtain \u2207 S real \u03c1 using reverse mode automatic differentiation, where S real \u2208 S \u03c1 denotes all continuous variables. This enables us to automatically construct Hamiltonian Monte Carlo proposals [34,45] q hmc (S \u03c1 real \u2192 S \u03c1 real ) (see supplementary material for a simple example).", "publication_ref": ["b33", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Elliptical Slice Proposals:", "text": "To adaptively propose changes to a large set of latent variables at once, our inference library supports elliptical slice moves with or without adaptive step sizes (see Figure 2 for an example) [4,33]. For simplicity, assume S real \u223c N (0, \u03a3). We can generate a new sub-trace S real efficiently as follows:\nS real = \u221a 1 \u2212 \u03b1 2 S real + \u03b1\u03b8,\nwhere \u03b8 \u223c N (0, \u03a3) and \u03b1 \u223c U nif orm(\u22121, 1).", "publication_ref": ["b3", "b32"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Data-driven Proposals:", "text": "The top-down nature of MCMC inference in generative models can be slow, due to the initial \"burn-in\" period and the cost of mixing among multiple posterior modes. However, vision problems often lend themselves to much faster bottom-up inference based on data-driven proposals [19,43]. Arguably the most important inference innovation in Picture is the capacity for automatically constructing data-driven proposals by simple learning methods. Such techniques fall under the broader idea of amortizing or caching inference to improve speed and accuracy [40]. We have explored several approaches generally inspired by the Helmholtz machine [17,6], and indeed Helmholtz's own proposals, including using deep learning to construct bottom-up predictors for all or a subset of the latent scene variables in \u03c1 [49,25]. Here we focus on a simple and general-purpose memory-based approach (similar in spirit to the informed sampler [19]) that can be summarized as follows: We \"imagine\" a large set of hypothetical scenes sampled from the generative model, store the imagined latents and corresponding rendered image data in memory, and build a fast bottom-up kernel density estimate proposer that samples variants of stored graphics program traces best matching the observed image data -where these bottom-up \"matches\" are determined using the same representation layer tools we introduced earlier for comparing top-down rendered and observed images. More formally, we construct data-driven proposals q data as follows:\n(1) Specify the number of times T to forward simulate (unconditional runs) the graphics program f .\n(2) Draw T samples from f to create program traces \u03c1 t and approximate renderings I t R , where {1 \u2264 t \u2264 T }.\n(3) Specify a summary statistic function \u03bd dd with model parameters \u03b8 \u03bd dd . We can use the same representation layer tools introduced earlier to specify \u03bd dd , subject to the additional constraint that feature dimensionalities should be as small as possible to enable proposal learning and evaluation on massive datasets. (4) Fine-tune parameters \u03b8 \u03bd dd of the representation layer \u03bd dd using supervised learning to best predict program traces {\u03c1 t } T t=1 from corresponding rendered images {I t R } T t=1 . If labeled data is available for full or partial scene traces {S \u03c1 p } corresponding to actual observed images {I D }, the parameters \u03b8 \u03bd dd can also be fine-tuned further to predict these labels. (see deep convolutional inverse graphics network [25] as an alternative \u03bd dd , which works in an weakly supervised setting.) (5) Define a hash function H : \u03bd(I t R ) \u2192 h t , where h t denotes the hash value for \u03bd(I t R ). For instance, H can be defined in terms of K-nearest neighbors or a Dirichlet Process mixture model. Store triplets {\u03c1 t , \u03bd(I t R ), h t } in a database C. (6) To generate data-driven proposals for an observed image I D with hash value h D , extract all triplets {\u03c1 j , \u03bd(I j R ), h j } N j=1 that have hash value equal to h D . We can then estimate the data-driven proposal as:\nq data (S \u03c1 \u2192 S \u03c1 |C, I D ) = P density ({\u03c1 j } N j=1 ),\nwhere P density is a density estimator such as the multivariate gaussian kernel in [19]).", "publication_ref": ["b18", "b42", "b39", "b16", "b5", "b48", "b24", "b18", "b24", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Example Picture Programs", "text": "To illustrate how Picture can be applied to a wide variety of 2D and 3D computer vision problems, we present three sample applications to the core vision tasks of 3D body pose estimation, 3D reconstruction of objects and 3D face analysis. Although additional steps could be employed to improve results for any of these tasks, and there may exist better fine-tuned baselines, our goal here is to show how to solve a broad class of problems efficiently and competitively with task-specific baseline systems, using only minimal problemspecific engineering.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3D Analysis of Faces", "text": "We obtained a 3D deformable face model trained on laser scanned faces from Paysan et al [36]. After training with this dataset, the model generates a mean shape mesh and mean texture map, along with principal components and eigenvectors. A new face can be rendered by randomly choosing coefficients for the 3D model and running the program shown in Figure 2. The representation layer \u03bd in this program used the top convolutional-layer features from the ImageNet CNN model [20] as well as raw pixels. (Even better results can be obtained using the deep convolutional inverse graphics network [25] instead of the CNN.) We evaluated the program on a held-out test set of 2D projected images of 3D laser scanned data (dataset from [36]). We additionally produced a dataset of about 30 images from the held-out set with different viewpoints and lighting conditions. In Figure 3, we show qualitative results of inference runs on the dataset.\nDuring experimentation, we discovered that since the number of latent variables is large (8 sets of 100 dimensional continuous coupled variables), elliptical slice moves are significantly more efficient than Metropolis-Hastings proposals (see supplementary Figure 2 for quantitative results). We also found that adding learned data-driven proposals significantly outperforms using only the elliptical slice proposals in terms of both speed and accuracy. We trained the datadriven proposals from around 100k program traces drawn from unconditional runs. The summary statistic function \u03bd dd used were the top convolutional-layer features from the pretrained ImageNet CNN model [20]. The conditional proposal density P density was a multivariate kernel density function over cached latents with a Gaussian Kernel (0.01 bandwidth). Figure 5 shows the gains in inference from use of a mixture kernel of these data-driven proposals (0.1 probability) and elliptical slice proposals (0.9 probability), relative to a pure elliptical slice sampler.\nMany other academic researchers have used 3D deformable face models in an analysis-by-synthesis based approach [28,22,1]. However, Picture is the only system to solve this as well as many other unrelated computer vision problems using a general-purpose system. Moreover, the data-driven proposals and abstract summary statistics (top convolutional-layer activations) allow us to tackle the problem without explicitly using 2D face landmarks as compared to traditional approaches.\nWith Data-driven Proposals Without Data-driven Proposals Figure 5: The effect of adding data-driven proposals for 3D face program: A mixture of automatically learned data-driven proposals and elliptical slice proposals significantly improves speed and accuracy of inference over a pure elliptical slice sampler. We ran 50 independent chains for both approaches and show a few sample trajectories as well as the mean trajectories (in bold).", "publication_ref": ["b35", "b19", "b24", "b35", "b19", "b27", "b21", "b0"], "figure_ref": ["fig_3", "fig_4", "fig_3"], "table_ref": []}, {"heading": "3D Human Pose Estimation", "text": "We developed a Picture program for parsing 3D pose of articulated humans from single images. There has been notable work in model-based approaches [13,27] for 3D human pose estimation, which served as an inspiration for the program we describe in this section. However, in contrast to Picture, existing approaches typically require custom inference strategies and significant task-specific model engineering. The probabilistic code (see supplementary Figure 4) consists of latent variables denoting bone and joints of an articulated 3D base mesh of a body. In our probabilistic code, we use an existing base mesh of a human body, defined priors over bone location and joints, and enable the armature skin-modifier API via Picture's Blender engine API. The latent scene S \u03c1 in this program can be visualized as a tree with the root node around the center of the mesh, and consists of bone location variables, bone rotation variables and camera parameters. The representation layer \u03bd in this program uses fine-grained image contours [8] and the comparator is expressed as the probabilistic chamfer distance [41].\nWe evaluated our program on a dataset of humans performing a variety of poses, which was aggregated from KTH [39] and LabelMe [38] images with significant occlusion in the \"person sitting\"(around 50 total images). This dataset was chosen to highlight the distinctive value of a graphics model-based approach, emphasizing certain dimensions of task difficulty while minimizing others: While graphics simulators for articulated bodies can represent arbitrarily complex body configurations, they are limited with respect to fine-grained appearance (e.g., skin and clothing), and fast methods for fine-grained contour detection currently work well only in low clutter environments. We initially used only single-site MH proposals, although blocked proposals or HMC can somewhat accelerate inference.\nWe compared this approach with the discriminatively trained Deformable Parts Model (DPM) for pose estimation [48] (referred as DPM-pose), which is notably a 2D pose model. As shown in Figure 6b, images with people sitting and heavy occlusion are very hard for the discriminative  4 for the probabilistic program. We quantitatively evaluate the pose program on a dataset collected from various sources such as KTH [39], La-belMe [38] images with significant occlusion in the \"person sitting\" category and the Internet. On the given dataset, as shown in the error histogram in (a), our model is more accurate on average than just using the DPM based human pose detector [48]. The histogram shows average error for all methods considered over the entire dataset separated over each body part. model to get right -mainly due to \"missing\" observation signal -while our model-based approach can handle these reasonably if we constrain the knee parameters to bend only in natural ways in the prior. Most of our model's failure cases, as shown in Figure 6b, are in inferring the arm position; this is typically due to noisy and low quality feature maps around the arm area due to its small size.\nIn order to quantitatively compare results, we project the 3D pose obtained from our model to 2D key-points. As shown in Figure 6a, our system localizes these key-points significantly better than DPM-pose on this dataset. However, DPM-pose is a much faster bottom-up method, and we explored ways to combine its strengths with our model-based approach, by using it as the basis for learning data-driven proposals. We generated around 500k program traces by unconditionally running the body pose program. We used a pre-trained DPM pose model [48] as the function \u03bd dd , and used a similar density function P density as in the face example. As shown in Figure 7, inference using a mixture kernel of data-driven proposals (0.1 probability) and singlesite MH (0.9 probability) consistently outperformed pure With Data-driven Proposals Without Data-driven Proposals top-down MH inference in both speed and accuracy. We see this as representative of many ways that top-down inference in model-based approaches could be profitably combined with fast bottom-up methods like DPM-pose to solve richer scene parsing problems more quickly.", "publication_ref": ["b12", "b26", "b7", "b40", "b38", "b37", "b47", "b38", "b37", "b47", "b47"], "figure_ref": ["fig_5", "fig_6", "fig_5", "fig_6", "fig_6", "fig_7"], "table_ref": []}, {"heading": "3D Shape Program", "text": "Lathing and casting is a useful representation to express CAD models and inspires our approach to modeling medially-symmetric 3D objects. It is straightforward to generate random CAD object models using a probabilistic program, as shown in supplementary Figure 3. However, the distribution induced by such a program may be quite complex. Given object boundaries in B \u2208 R 2 space, we can lathe an object by taking a cross section of points (fixed for this program), defining a medial axis for the cross section and sweeping the cross section across the medial axis by continuously perturbing with respect to B. Capturing the full range of 3D shape variability in real objects will require a very large space of possible boundaries B. To this end, Picture allows flexible non-parametric priors over object profiles: here we generate B from a Gaussian Process [37] (GP). The probabilistic shape program produces an intermediate mesh of all or part of the 3D object (soft-constrained to be in the middle of the scene), which then gets rendered to an image I R by a deterministic camera re-projection function. The representation layer and the comparator used in this program were same as those used for the 3D human pose example. The proposal kernel we used during inference consisted of blocked MCMC proposals on all the coupled continuous variables as described in the supplementary material. (For more details of the program and inference summarized here, refer to supplementary Section 1.)\nWe evaluate this program on an RGB image dataset of 3D objects with large shape variability. We asked CAD experts to manually generate CAD model fits to these images in Blender, and evaluated our approach in comparison to a state-of-the-art 3D surface reconstruction algorithm from [3](SIRFS). To judge quantitative performance, we calculated two metrics: (a) Z-MAE -Shift-invariant surface mean-squared error and (b) N-MSE -mean-squared error over normals [3]. As shown in Figure 8, inference using our probabilistic shape program has a lower Z-MAE and N-MSE score than SIRFS [3], and we also obtain qualitatively better reconstruction results. However, it is important to note that SIRFS predominantly utilizes only low level shape priors such as piece-wise smoothness, in contrast to the high-level shape priors we assume, and SIRFS solves a more general and harder problem of inferring full intrinsic images (shape, illumination and reflectance). In the future, we hope to combine the best of SIRFS-style approaches and our probabilistic CAD programs to reconstruct rich 3D shape and appearance models for generic object classes, robustly and efficiently.", "publication_ref": ["b36", "b2", "b2", "b2"], "figure_ref": ["fig_4", "fig_8"], "table_ref": []}, {"heading": "Discussion", "text": "There are many promising directions for future research in probabilistic graphics programming. Introducing a dependency tracking mechanism could let us exploit the many conditional independencies in rendering for more efficient parallel inference. Automatic particle-filter based inference schemes [47,24] could extend the approach to image sequences. Better illumination [52], texture and shading models could let us work with more natural scenes. Procedural graphics techniques [2,10] would support far more complex object and scene models [50,14,7,15]. Flexible scene generator libraries will be essential in scaling up to the full range of scenes people can interpret.\nWe are also interested in extending Picture by taking insights from learning based \"analysis-by-synthesis\" approaches such as transforming auto-encoders [18], capsule networks [42] and deep convolutional inverse graphics network [25]. These models learn an implicit graphics engine in an encoder-decoder style architecture. With probabilistic programming, the space of decoders need not be restricted to neural networks and could consist of arbitrary probabilistic graphics programs with internal parameters.\nThe recent renewal of interest in inverse graphics approaches to vision has motivated a number of new modeling and inference tools. Each addresses a different facet of the general problem. Earlier formulations of probabilistic graphics programming provided compositional languages for scene modeling and a flexible template for automatic inference. Differentiable renderers make it easier to fine-tune the numerical parameters of high-dimensional scene models. Data-driven proposal schemes suggest a way to rapidly identify plausible scene elements, avoiding the slow burn-in and mixing times of top-down MCMC-based inference in generative models. Deep neural networks, deformable parts models and other discriminative learning methods can be used to automatically construct good representation layers or similarity metrics for comparing hypothesized scenes to observed images. Here we show that by integrating all of these ideas into a single probabilistic language and inference framework, it may be feasible to begin scaling up inverse graphics to a range of real-world vision problems.", "publication_ref": ["b46", "b23", "b51", "b1", "b9", "b49", "b13", "b6", "b14", "b17", "b41", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Thomas Vetter for giving us access to the Basel face model. T. Kulkarni was graciously supported by the Leventhal Fellowship. This research was supported by ONR award N000141310333, ARO MURI W911NF-13-1-2012 and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. We would like to thank Ilker Yildrim, Yura Perov, Karthik Rajagopal, Alexey Radul, Peter Battaglia, Alan Rusnak, and three anonymous reviewers for helpful feedback and discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Inverse rendering of faces with a 3d morphable model", "journal": "PAMI", "year": "2013", "authors": "O Aldrian; W A Smith"}, {"ref_id": "b1", "title": "Shapesynth: Parameterizing model collections for coupled shape exploration and synthesis", "journal": "Wiley Online Library", "year": "2014", "authors": "M Averkiou; V G Kim; Y Zheng; N J Mitra"}, {"ref_id": "b2", "title": "Shape, illumination, and reflectance from shading", "journal": "", "year": "2013", "authors": "J Barron; J Malik"}, {"ref_id": "b3", "title": "Regression and classification using gaussian process priors", "journal": "", "year": "1998", "authors": "J Bernardo; J Berger; A Dawid; A Smith"}, {"ref_id": "b4", "title": "Blender -a 3D modelling and rendering package. Blender Foundation", "journal": "", "year": "", "authors": " Blender Online Community"}, {"ref_id": "b5", "title": "The helmholtz machine", "journal": "Neural computation", "year": "1995", "authors": "P Dayan; G E Hinton; R M Neal; R S Zemel"}, {"ref_id": "b6", "title": "Understanding bayesian rooms using composite 3d object models", "journal": "IEEE", "year": "2013", "authors": "L Pero; J Bowdish; B Kermgard; E Hartley; K Barnard"}, {"ref_id": "b7", "title": "Structured forests for fast edge detection", "journal": "", "year": "2013", "authors": "P Doll\u00e1r; C L Zitnick"}, {"ref_id": "b8", "title": "Object detection with discriminatively trained partbased models", "journal": "PAMI", "year": "2010", "authors": "P F Felzenszwalb; R B Girshick; D Mcallester; D Ramanan"}, {"ref_id": "b9", "title": "Meta-representation of shape families", "journal": "", "year": "2013", "authors": "N Fish; M Averkiou; O Van Kaick; O Sorkine-Hornung; D Cohen-Or; N J Mitra"}, {"ref_id": "b10", "title": "General pattern theory-A mathematical study of regular structures", "journal": "Clarendon Press", "year": "1993", "authors": "U Grenander"}, {"ref_id": "b11", "title": "Hands: A pattern theoretic study of biological shapes", "journal": "Springer-Verlag New York, Inc", "year": "1991", "authors": "U Grenander; Y Chow; D M Keenan"}, {"ref_id": "b12", "title": "Estimating human shape and pose from a single image", "journal": "IEEE", "year": "2009", "authors": "P Guan; A Weiss; A O Balan; M J Black"}, {"ref_id": "b13", "title": "Blocks world revisited: Image understanding using qualitative geometry and mechanics", "journal": "Springer", "year": "2010", "authors": "A Gupta; A A Efros; M Hebert"}, {"ref_id": "b14", "title": "Thinking inside the box: Using appearance models and context based on room geometry", "journal": "Springer", "year": "2010", "authors": "V Hedau; D Hoiem; D Forsyth"}, {"ref_id": "b15", "title": "A fast learning algorithm for deep belief nets", "journal": "Neural computation", "year": "2006", "authors": "G Hinton; S Osindero; Y.-W Teh"}, {"ref_id": "b16", "title": "The\" wakesleep\" algorithm for unsupervised neural networks", "journal": "Science", "year": "1995", "authors": "G E Hinton; P Dayan; B J Frey; R M Neal"}, {"ref_id": "b17", "title": "Transforming auto-encoders", "journal": "Springer", "year": "2011", "authors": "G E Hinton; A Krizhevsky; S D Wang"}, {"ref_id": "b18", "title": "The informed sampler: A discriminative approach to bayesian inference in generative computer vision models", "journal": "", "year": "2014", "authors": "V Jampani; S Nowozin; M Loper; P V Gehler"}, {"ref_id": "b19", "title": "Caffe: Convolutional architecture for fast feature embedding", "journal": "ACM", "year": "2014", "authors": "Y Jia; E Shelhamer; J Donahue; S Karayev; J Long; R Girshick; S Guadarrama; T Darrell"}, {"ref_id": "b20", "title": "Context and hierarchy in a probabilistic image model", "journal": "IEEE", "year": "2006", "authors": "Y Jin; S Geman"}, {"ref_id": "b21", "title": "3d face reconstruction from a single image using a single reference face shape", "journal": "PAMI", "year": "2011", "authors": "I Kemelmacher-Shlizerman; R Basri"}, {"ref_id": "b22", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G E Hinton"}, {"ref_id": "b23", "title": "Variational particle approximations", "journal": "", "year": "2014", "authors": "T D Kulkarni; A Saeedi; S Gershman"}, {"ref_id": "b24", "title": "Deep convolutional inverse graphics network", "journal": "", "year": "2015", "authors": "T D Kulkarni; W Whitney; P Kohli; J B Tenenbaum"}, {"ref_id": "b25", "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks", "journal": "", "year": "1995", "authors": "Y Lecun; Y Bengio"}, {"ref_id": "b26", "title": "A model-based approach for estimating human 3d poses in static images", "journal": "PAMI", "year": "2006", "authors": "M W Lee; I Cohen"}, {"ref_id": "b27", "title": "State-of-the-art of 3d facial reconstruction methods for face recognition based on a single 2d training image per person", "journal": "Pattern Recognition Letters", "year": "2009", "authors": "M D Levine; Y Yu"}, {"ref_id": "b28", "title": "Opendr: An approximate differentiable renderer", "journal": "", "year": "2014", "authors": "M M Loper; M J Black"}, {"ref_id": "b29", "title": "Distinctive image features from scale-invariant keypoints", "journal": "IJCV", "year": "2004", "authors": "D G Lowe"}, {"ref_id": "b30", "title": "Approximate bayesian image interpretation using generative probabilistic graphics programs", "journal": "", "year": "2013", "authors": "V Mansinghka; T D Kulkarni; Y N Perov; J Tenenbaum"}, {"ref_id": "b31", "title": "Venture: a higherorder probabilistic programming platform with programmable inference", "journal": "", "year": "2014", "authors": "V Mansinghka; D Selsam; Y Perov"}, {"ref_id": "b32", "title": "Elliptical slice sampling", "journal": "", "year": "2009", "authors": "I Murray; R P Adams; D J Mackay"}, {"ref_id": "b33", "title": "Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo", "journal": "", "year": "2011", "authors": "R Neal"}, {"ref_id": "b34", "title": "A compilation target for probabilistic programming languages", "journal": "", "year": "2014", "authors": "B Paige; F Wood"}, {"ref_id": "b35", "title": "A 3d face model for pose and illumination invariant face recognition", "journal": "IEEE", "year": "2009", "authors": "P Paysan; R Knothe; B Amberg; S Romdhani; T Vetter"}, {"ref_id": "b36", "title": "Gaussian processes for machine learning", "journal": "", "year": "2006", "authors": "C E Rasmussen"}, {"ref_id": "b37", "title": "Labelme: a database and web-based tool for image annotation", "journal": "IJCV", "year": "2008", "authors": "B C Russell; A Torralba; K P Murphy; W T Freeman"}, {"ref_id": "b38", "title": "Recognizing human actions: a local svm approach", "journal": "", "year": "2004", "authors": "C Schuldt; I Laptev; B Caputo"}, {"ref_id": "b39", "title": "Learning stochastic inverses", "journal": "", "year": "2013", "authors": "A Stuhlm\u00fcller; J Taylor; N Goodman"}, {"ref_id": "b40", "title": "Shape context and chamfer matching in cluttered scenes", "journal": "", "year": "2003", "authors": "A Thayananthan; B Stenger; P H Torr; R Cipolla"}, {"ref_id": "b41", "title": "Optimizing Neural Networks that Generate Images", "journal": "", "year": "2014", "authors": "T Tieleman"}, {"ref_id": "b42", "title": "Image segmentation by data-driven markov chain monte carlo", "journal": "PAMI", "year": "2002", "authors": "Z Tu; S.-C Zhu"}, {"ref_id": "b43", "title": "Approximate bayesian computation (abc) gives exact results under the assumption of model error. Statistical applications in genetics and molecular biology", "journal": "", "year": "2013", "authors": "R D Wilkinson"}, {"ref_id": "b44", "title": "Nonstandard interpretations of probabilistic programs for efficient inference", "journal": "NIPS", "year": "2011", "authors": "D Wingate; N D Goodman; A Stuhlmueller; J Siskind"}, {"ref_id": "b45", "title": "Lightweight implementations of probabilistic programming languages via transformational compilation", "journal": "", "year": "2011", "authors": "D Wingate; A Stuhlmueller; N D Goodman"}, {"ref_id": "b46", "title": "A new approach to probabilistic programming inference", "journal": "", "year": "2014", "authors": "F Wood; J W Van De Meent; V Mansinghka"}, {"ref_id": "b47", "title": "Articulated pose estimation with flexible mixtures-of-parts", "journal": "", "year": "2011", "authors": "Y Yang; D Ramanan"}, {"ref_id": "b48", "title": "Tenenbaum. Efficient and robust analysis-by-synthesis in vision: A computational framework, behavioral tests, and modeling neuronal representations", "journal": "", "year": "", "authors": "I Yildirim; T D Kulkarni; W A Freiwald; J "}, {"ref_id": "b49", "title": "Panocontext: A wholeroom 3d context model for panoramic scene understanding", "journal": "Springer", "year": "2014", "authors": "Y Zhang; S Song; P Tan; J Xiao"}, {"ref_id": "b50", "title": "Image parsing via stochastic scene grammar", "journal": "", "year": "2011", "authors": "Y Zhao; S.-C Zhu"}, {"ref_id": "b51", "title": "Human face shape analysis under spherical harmonics illumination considering self occlusion", "journal": "IEEE", "year": "2013", "authors": "J Zivanov; A Forster; S Schonborn; T Vetter"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "I D ), \u232b(I R )) Likelihood or Likelihood-free Comparator or P (I D |I R , X)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Overview: (a) All models share a common template;", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Picture code illustration for 3D face analysis: Modules from Figure 1a,b are highlighted in bold. Running the program unconditionally (by removing observe's in code) produces random faces as shown in Figure 1c. Running the program conditionally (keeping observe's) on ID results in posterior inference as shown in Figure3. The variables MU, PC, EV correspond to the mean shape/texture face, principal components, and eigenvectors respectively (see[36] for details). These arguments parametrize the prior on the learned shape and appearance of 3D faces. The argument VERTEX ORDER denotes the ordered list of vertices to render triangle based meshes. The observe directive constrains the program execution based on both the pixel data and CNN features. The infer directive starts the inference engine with the specified set of inference schemes (takes the program trace, a callback function CB for debugging, number of iterations and inference schemes). In this example, data-driven proposals are run for a few iterations to initialize the sampler, followed by slice sampling moves to further refine the high dimensional scene latents.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Inference on representative faces using Picture: We", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Formal Summary: The scene S can be conceptualized as a program that describes the structure of known or unknown number of objects, texture-maps, lighting and other scene variables. The symbol T denotes the number of times the program f is executed to generate data-driven proposals (see section 3.2 for details). The rendering differentiator produces gradients of the program density with respect to continuous variables S real in the program.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Quantitative and qualitative results for 3D human pose program: Refer to supplementary Figure4for the probabilistic program. We quantitatively evaluate the pose program on a dataset collected from various sources such as KTH[39], La-belMe[38] images with significant occlusion in the \"person sitting\" category and the Internet. On the given dataset, as shown in the error histogram in (a), our model is more accurate on average than just using the DPM based human pose detector[48]. The histogram shows average error for all methods considered over the entire dataset separated over each body part.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Illustration of data-driven proposal learning for 3D human-pose program: (a) Random program traces sampled from the prior during training. The colored stick figures are the results of applying DPM pose model on the hallucinated data from the program. (b) Representative test image. (c) Visualization of the representation layer \u03bd(ID). (d) Result after inference. (e) Samples drawn from the learned bottom-up proposals conditioned on the test image are semantically close to the test image and results are fine-tuned by top-down inference to close the gap. As shown on the log-l plot, we run about 100 independent chains with and without the learned proposal. Inference with a mixture kernel of learned bottom-up proposals and single-site MH consistently outperforms baseline in terms of both speed and accuracy.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Qualitative and quantitative results of 3D object reconstruction program: Refer to supplementary Figure 3 for the probabilistic program. Top: We illustrate a typical inference trajectory of the sampler from prior to the posterior on a representative real world image. Middle: Qualitative results on representative images. Bottom: Quantitative results in comparison to[3]. For details about the scoring metrics, refer to section 4.3.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "function PROGRAM(MU, PC, EV, VERTEX_ORDER)face=Dict();shape = []; texture = [];for S in [\"shape\", \"texture\"]for p in [\"nose\", \"eyes\", \"outline\", \"lips\"]coeff = MvNormal(0,1,1,99)face[S][p] = MU[S][p]+PC[S][p]. * (coeff. * EV[S][p])endendshape=face[\"shape\"][:]; tex=face[\"texture\"][:];camera = Uniform(-1,1,1,2); light = Uniform(-1,1,1,2)# Approximate Rendererrendered_img= MeshRenderer(shape,tex,light,camera)# Representation Layerren_ftrs = getFeatures(\"CNN_Conv6\", rendered_img)# Comparator#Using Pixel as Summary Statisticsobserve(MvNormal(0,0.01), rendered_img-obs_img)#Using CNN last conv layer as Summary Statisticsobserve(MvNormal(0,10), ren_ftrs-obs_cnn)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "{ <0, 199, 20> color rgb<1.5,1.5,1.5> } camera { location <30,48,-10> angle 40 look_at <30,44,50> } object{leg-right vertices ... trans <32.7,43.6,9>} object{arm-left vertices scale 0.2 .", "figure_data": "ModulesFunctional DescriptionScene Representation S:.. rotate x*0}...object{arm-left texture}ay, April 3, 15"}], "formulas": [{"formula_id": "formula_0", "formula_text": ". . . qP ((S \u21e2 , X \u21e2 ) ! (S 0\u21e2 , X 0\u21e2 )) q hmc (S \u21e2 real ! S 0\u21e2 real ) q slice (S \u21e2 real ! S 0\u21e2 real ) q data ((S \u21e2 , X \u21e2 ) ! (S 0\u21e2 , X 0\u21e2 )) New (S \u21e2 , X \u21e2 ) (S 0\u21e2 , X", "formula_coordinates": [2.0, 51.21, 197.58, 229.76, 47.55]}, {"formula_id": "formula_1", "formula_text": "P (S \u03c1 |I D ) \u221d P (S \u03c1 )P (X \u03c1 )\u03b4 render(S \u03c1 ,X \u03c1 ) (I R ) L dX \u03c1", "formula_coordinates": [4.0, 311.99, 99.98, 229.5, 18.91]}, {"formula_id": "formula_2", "formula_text": "min 1, L P (S \u03c1 )P (X \u03c1 ) q (S ,X )\u2192(S,X) K P (S \u03c1 del ) L P (S \u03c1 )P (X \u03c1 ) q (S,X)\u2192(S ,X ) K P (S \u03c1 new )", "formula_coordinates": [4.0, 313.82, 374.75, 216.41, 27.82]}, {"formula_id": "formula_3", "formula_text": "P (S \u03c1 |I D ) \u221d \u0398 P (S \u03c1 )P (\u03bd(I D )\u2212\u03bd(I R ))P (\u03bd(I R )|S \u03c1 )dI R .", "formula_coordinates": [4.0, 308.86, 697.56, 242.69, 19.77]}, {"formula_id": "formula_4", "formula_text": "q P ((S \u03c1 , X \u03c1 ) \u2192 (S \u03c1 , X \u03c1 )) = \u03c1 i \u2208(S \u03c1 ,X \u03c1 ) P (\u03c1 i )", "formula_coordinates": [5.0, 50.11, 306.18, 203.92, 18.47]}, {"formula_id": "formula_5", "formula_text": "S real = \u221a 1 \u2212 \u03b1 2 S real + \u03b1\u03b8,", "formula_coordinates": [5.0, 50.11, 515.17, 127.52, 25.59]}, {"formula_id": "formula_6", "formula_text": "q data (S \u03c1 \u2192 S \u03c1 |C, I D ) = P density ({\u03c1 j } N j=1 ),", "formula_coordinates": [5.0, 339.68, 579.4, 184.57, 18.91]}], "doi": ""}