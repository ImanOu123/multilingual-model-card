{"title": "Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields", "authors": "Garvita Tiwari; Dimitrije Anti\u0107; Jan Eric Lenssen; Nikolaos Sarafianos; Tony Tung; Gerard Pons-Moll", "pub_date": "2022-07-27", "abstract": "We present Pose-NDF, a continuous model for plausible human poses based on neural distance fields (NDFs). Pose or motion priors are important for generating realistic new poses and for reconstructing accurate poses from noisy or partial observations. Pose-NDF learns a manifold of plausible poses as the zero level set of a neural implicit function, extending the idea of modeling implicit surfaces in 3D to the high-dimensional domain SO(3) K , where a human pose is defined by a single data point, represented by K quaternions. The resulting highdimensional implicit function can be differentiated with respect to the input poses and thus can be used to project arbitrary poses onto the manifold by using gradient descent on the set of 3-dimensional hyperspheres. In contrast to previous VAE-based human pose priors, which transform the pose space into a Gaussian distribution, we model the actual pose manifold, preserving the distances between poses. We demonstrate that Pose-NDF outperforms existing state-of-the-art methods as a prior in various downstream tasks, ranging from denoising real-world human mocap data, pose recovery from occluded data to 3D pose reconstruction from images. Furthermore, we show that it can be used to generate more diverse poses by random sampling and projection than VAE-based methods. We will release our code and pre-trained model for further research at https://virtualhumans.mpi-inf.mpg.de/posendf/.", "sections": [{"heading": "Introduction", "text": "Realistic and accurate human motion capture and generation is essential for understanding human behavior and human interaction in the scene [23,41,68,67,9]. Human motion capturing systems, like marker-based systems [37,34], IMU-based methods [23,41], or reconstruction from RGB/RGB-D data [55,22,29,70], often suffer from artifacts like skating, self-intersections and jitters and produce nonrealistic human poses, especially in the presence of noisy data and occlusion. To make the results applicable in fields like 3D scene understanding, human motion generation, or AR/VR applications, it is often required to apply exhaustive manual or automatic cleaning procedures.\nf udf = f df \u2022 f enc \u2207 \u03b8 f udf\nFig. 1. We present Pose-NDF, a neural unsigned distance field in SO(3) K , which learns the manifold of plausible poses as zero level set. We learn the distance field representation from samples of plausible (\u25a0) and unrealistic ( \u2022 ) poses (left). We encode the input pose (given as a set of quaternions) using a structural MLP f enc and predict the distance from the joint representation using an MLP f df . The gradient \u2207 \u03b8 f udf and distance value f udf (\u03b8) are used to project implausible poses onto the manifold (right).\nIn recent years, learned data priors to post-process such non-realistic human poses has become increasingly popular. Prior human pose models mainly focus on learning a joint distribution of individual joints in pose space [10] or recently in a latent space, using VAEs [49,52,66]. They have demonstrated to greatly improve the plausibility of poses after model fitting. However, VAE-based methods, such as VPoser [49] or HuMoR [52] make a Gaussian assumption on the space of possible poses, which leads to several limitations: 1) They have the tendency of producing more likely poses that lie near the mean of the computed Gaussian. Those poses however, might not be the correct ones. 2) Distances between individual human poses are not preserved in the VAE latent space. Hence, taking small steps towards the Gaussian mean might result in large steps in pose space.\n3) VAEs have been shown to fold a manifold into a Gaussian distribution [39], exposing dead regions without any data points in the outer parts of the distribution. Thus, they produce non-plausible samples that are far from the input when traversed in outer regions, as we demonstrate in our experiments.\nTo alleviate these issues, we present Pose-NDF, a human pose prior that models the full manifold of plausible poses in high-dimensional pose space. We represent the manifold as a surface, where plausible poses lie on the manifold, hence having a zero distance, and non-plausible poses lie outside of it, having a non-zero distance from the surface. We propose to learn this manifold using a high-dimensional neural field, analogously to representing 3D shapes using neural distance fields [48,13]. This formulation preserves distances between poses and allows to traverse the pose space along the negative gradient of the distance function, which points to the direction of maximum distance decrease. Using gradient descent in pose space from an initial potentially non-plausible pose, we always find the closest point on the manifold of plausible poses.\nAn overview of our method is given in Fig. 1. We formulate the problem of learning the pose manifold as a surface completion task in n-dimensional space. In order to learn a pose manifold, there are two key challenges: a) the input space is high-dimensional, and b) the input space is not Euclidean, as it is for 3-dimensional implicit surfaces [48,13]. Instead, the pose space is given as SO(3) K , in which a single pose can be represented by K elements of the rotation group SO(3), describing the orientations of joints in a human body model. To represent group elements, we opted for a quaternion representation, as they are continuous, have an easy-to-compute distance, and are subject to an efficient gradient descent algorithm. We map a given pose to a distance by applying a hierarchical implicit neural function, which encodes the pose based on the kinematic structure of the human body. We train our model using the AMASS dataset [38], where each sample from the dataset is treated as a point on the manifold. The learned neural field representation can be used to project any pose onto the manifold, similar to [13]. We leverage this property and use Pose-NDF for diverse pose generation, pose interpolation, as a pose prior for 3D pose estimation from images [49,10], and motion denoising [23,52], improving on state-of-the-art methods in all areas. In summary our contributions are:\n\u2022 A novel high-dimensional neural field representation in SO(3) K , Pose-NDF,\nwhich represents the manifold of plausible human poses. \u2022 Pose-NDF improves the state of the art in human body fitting from images by acting as a pose prior. It outperforms other human pose priors, such as VPoser [49] and the human motion prior HuMoR [52] on motion denoising. \u2022 Our method is as fast or faster than current state-of-the-art methods, is fully differentiable and the distance from the manifold can be leveraged for finding the optimal step size during optimization. \u2022 Pose-NDF generates more diverse samples than previous methods with Gaussian assumptions, which are biased towards generating more likely poses.", "publication_ref": ["b22", "b40", "b67", "b66", "b8", "b36", "b33", "b22", "b40", "b54", "b21", "b28", "b69", "b9", "b48", "b51", "b65", "b48", "b51", "b38", "b47", "b12", "b47", "b12", "b37", "b12", "b48", "b9", "b22", "b51", "b48", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Our method is a human pose prior build as neural field in high-dimensional space. Thus, we review related work in both of these areas.\nPose and Motion Priors. Human pose and motion priors are crucial for preserving the realism of models estimated from captured data [40,23,38] and to estimate human pose from images [10,49,65,14,33] and videos [32,59]. Further, they can be powerful tools for data generation. Initial work along this direction mainly focused on learning constraints for joint limits in Euler angles [17] or swing and twist representations [6,56,1], to avoid twists and bends beyond certain limits. A next iteration of methods fits a Gaussian Mixture Model (GMM) to a pose dataset and uses the GMM-based prior for downstream tasks like image-based 3D pose estimation [10,54] or registration of 3D scans [4,8,60]. Additionally, simple statistical models, such as PCA, have been proposed [47,62,57].\nWith the rise of deep learning and GANs [20], adversarial training has been used to bring predicted poses close to real poses [31,19] and for motion prediction [7]. However these are task specific models, HMR [31] models p(\u03b8|I) and requires an image I. HP-GAN [7] models p(\u03b8 t |\u03b8 t\u22121 ) and requires pose parameters \u03b8 t\u22121 for previous frame/time. Therefore they cannot be used as a prior for other tasks.\nMore recent work uses VAEs to learn pose priors [49], which can be used for generating pose samples, as prior in pose estimation, or 3d human reconstruction from images or sparse/occluded data. Some works [52,66,50] propose VAE-based human motion models. HuMoR [52] proposes to learn a distribution of possible pose transitions in motion sequences using a conditional VAE. AC-TOR [50] learns an action conditioned VAE-Transformer prior. Further work designs pose representations along the hierarchy of human skeletons [2] and uses it for character animation [5]. Concurrent work [16] learns a human pose prior using GANs and highlights the shortcomings of Gaussian assumption based models like VPoser [49]. A VPoser decoder is used as generator (mapping z \u2192 \u03b8) and an HMR [31]-like discriminator is used to train the model. As described in Sec. 1, our approach follows a different paradigm than the VAE and GAN-based methods, as we directly model the manifold of plausible poses in high-dimensional space, which leads to a distance-preserving representation.\nBefore the rise of deep learning, modeling partial pose spaces as implicit functions was common, e.g. as fields on a single shoulder joint quaternion [26] or an elbow joint quaternion, conditioned on the shoulder joint [25]. However, those ignore the real part of the quaternion, leading to ambiguities in representation, are not differentiable, and are limited to 2 joints in the human body model. In contrast, our method uses a fully differentiable neural network, which learns an implicit surface in higher dimension, taking all human joints and all four components of each quaternion into account. Neural Fields. Neural fields [13,48,42] for surface modeling have received increasing interest over the recent years. They have been used to model fields in 2D or 3D, representing images or partial differentiable equations [58,21], signed or unsigned distances from static 3D shapes [13,48,27,24], pose-conditioned distance field [53,61,43], radiance fields [44,51] and more recently for human-object [63,9] and hand-object [69] interactions. For a more detailed overview of neural fields please refer to [64]. Neural fields have recently been brought to higher dimensions to model surfaces in Euclidean spaces [45]. In this work, we apply the concept to the high-dimensional, non-Euclidean space of SO(3) K , modeling the unsigned distance to manifolds of plausible human body poses in pose space.", "publication_ref": ["b39", "b22", "b37", "b9", "b48", "b64", "b13", "b32", "b31", "b58", "b16", "b5", "b55", "b0", "b9", "b53", "b3", "b7", "b59", "b46", "b61", "b56", "b19", "b30", "b18", "b6", "b30", "b6", "b48", "b51", "b65", "b49", "b51", "b49", "b1", "b4", "b15", "b48", "b30", "b25", "b24", "b12", "b47", "b41", "b57", "b20", "b12", "b47", "b26", "b23", "b52", "b60", "b42", "b43", "b50", "b62", "b8", "b68", "b63", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "In this section, we describe our method Pose-NDF, a model for manifolds of plausible human poses based on high-dimensional neural fields. We assume that the realistic and plausible human poses lie on a manifold embedded in pose space SO(3) K , with K being the number of joints in the human body. Given a neural network f : SO(3) K \u2192 R + , which maps a pose, \u03b8 \u2208 SO(3) K to a non-negative scalar, we represent the manifold of plausible poses as the zero level set:\nS = {\u03b8 \u2208 SO(3) K | f (\u03b8) = 0},(1)\nsuch that the value of f represents the unsigned distance to the manifold, similar to neural fields-based 3D shape learning [12,13,21,48]. Without loss of generality, we use the SMPL body model [36,49], resulting in poses \u03b8 with K = 21 joints.", "publication_ref": ["b11", "b12", "b20", "b47", "b35", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Quaternions as Representation of SO(3)", "text": "A human pose is represented by 3D rotations of individual joints in the human skeleton. The 3-dimensional rotation group SO(3) has several common vector space representations that are used to describe group elements in practice. Frequently used examples are rotation matrices, axis-angle representations or unit quaternions [28]. Pose-NDF requires the representation to have specific properties: a) we aim to model a manifold, continuously embedded in pose space.\nThus, the chosen representation should be continuous in parameter space, which prohibits axis-angle representations; b) the representation should enable efficient computation of the geodesic distance between two elements; c) our algorithm requires efficient gradient descent in pose space. As described in Sec. 3.4, quaternions are subject to such a gradient descent algorithm that makes use of the efficient reprojection to SO(3) by vector normalization. In contrast, rotation matrices would require more expensive orthogonalization. Therefore, we chose unit quaternions as the best-suited SO(3) representation of joints, as they fulfill all three properties. We will use \u03b8 = {\u03b8 1 , ..., \u03b8 K } to denote the quaternions for all K joints of a pose. Each quaternion represents the rotation of a joint with respect to its parent node. Since quaternions lie on S 3 (embedded in 4-dimensional space) the full pose \u03b8 can be easily used as input for a neural network f : R 4K \u2192 R + . We define the distance d : (S 3 ) K \u00d7 (S 3 ) K \u2192 R + between two poses \u03b8 = {\u03b8 1 , ..., \u03b8 K } and\u03b8 = {\u03b8 1 , ...,\u03b8 K } as:\nd(\u03b8,\u03b8) = K i=1 w i 2 (arccos |\u03b8 \u22a4 i \u2022\u03b8 i |) 2 , (2\n)\nwhere the individual elements of summation are a metric on SO(3) [28] and w i is the weight associated with each joint based on their position in the kinematic structure of the SMPL body model (i.e. early joints in the chain have higher weights). It should be noted that the double cover property of unit quaternions, that is, the quaternions q and \u2212q represent the same SO(3) element, does not lead to additional challenges. We simply train the network to be point symmetric by applying sign flip augmentation on input quaternions.", "publication_ref": ["b27", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Hierarchical Implicit Neural Function", "text": "We represent the human pose with quaternions in local coordinate frames of the parent joint, using the kinematic structure of the SMPL body model. We treat the joints in local coordinate frame, so that continuous manipulation of a single joint corresponds to realistic motion. However, this might result in unrealistic combination of rotation of joints. The plausibility of individual joints depends on the ancestor rotations and thus needs to be conditioned on them. In order to incorporate this dependency, we use a hierarchical network f enc , which encodes the human pose based on the model structure [2,19,43], before predicting the distance based on the joint representation.\nFormally, for a given pose \u03b8 = {\u03b8 1 , ..., \u03b8 K }, where \u03b8 k is the pose for joint k, and a function \u03c4 (k), mapping the index of each joint to its parent joints index, we encode each pose using an MLP as:\nf enc 1 : (\u03b8 1 ) \u2192 v 1 f enc k : (\u03b8 k , v \u03c4 (k) ) \u2192 v k , k \u2208 {2 . . . K}(3)\nwhich takes the quaternion pose and encoded feature v \u03c4 (k) \u2208 R l of its parent joint as input and generates v k \u2208 R l , where l is the dimension of feature. We then concatenate the encoded feature for every joint to get a combined pose embedding\np = [v 1 || . . . ||v K ]\n. This embedding is processed by an MLP f df : R l\u2022K \u2192 R + , which predicts the unsigned distance for the given pose representation p. Collectively the complete model f udf (\u03b8) = (f df \u2022 f enc )(\u03b8), is termed as Pose-NDF, where f udf : SO(3) K \u2192 R + .", "publication_ref": ["b1", "b18", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Loss functions", "text": "We train the hierarchically structured neural field f udf to predict the geodesic distance to the plausible pose manifold for a given pose. The training data is given as a set D = {(\u03b8 i , d i )} 1\u2264i\u2264N , containing pairs of poses \u03b8 i and distances d i (Eq. 2). We train the network with the standard distance loss L UDF , and an Eikonal regularizer L eikonal , which encourages a unit-norm gradient for the distance field outside of the manifold [15,21]:\nL UDF = (\u03b8,d)\u2208D ||f udf (\u03b8)\u2212d \u03b8 || 2 L eikonal = (\u03b8,d)\u2208D, d\u0338 =0 (||\u2207 \u03b8 f udf (\u03b8)||\u22121) 2 , (4\n)\nMore details about training data, network architecture is provided in the supplementary material.", "publication_ref": ["b14", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Projection Algorithm", "text": "Given a trained model f udf , it can be applied to project an arbitrary pose \u03b8 to the manifold of plausible poses. We use the predicted distance f udf (\u03b8) and gradient information \u2207 \u03b8 f udf (\u03b8) to project a query pose to the manifold surface S, as was previously done in unsigned distances functions for 3D shapes [13]. In our case, given SO(3) poses, this amounts to finding:\n\u03b8 = arg min \u03b8\u2208SO(3) K d(\u03b8, S),(5)\nwhere d(\u03b8, S) is the distance (Eq. 2) of \u03b8 to the closest point in S. We find\u03b8 by applying gradient descent on the 3-sphere, using gradient information \u2207 \u03b8 f (\u03b8) and distances f (\u03b8), obtained from the implicit neural function f . One step is given as:\n\u03b8 i = \u03b8 i\u22121 \u2212 \u03b1f (\u03b8 i\u22121 )\u2207 \u03b8 f (\u03b8 i\u22121 ),(6)\nfollowed by a re-projection to the sphere (i.e. vector normalization) after several iterations. This algorithm is guaranteed to converge to local minima on the sphere, which in our case, assuming a correctly learned distance function, is the nearest point on the pose manifold.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments and Results", "text": "In this section we evaluate Pose-NDF and show the different use cases of our pose model, which include the ability to serve as a prior in denoising motion sequences or recovery from partial observations (Sec. 4.2), prior for recovering plausible poses from images (Sec. 4.3) using an optimization-based method, pose generation (Sec. 4.4) and pose interpolation (Sec. 4.5). We demonstrate that the Pose-NDF method outperforms the state-of-the-art VAE-based human pose prior methods. We also show the advantages of our distance field formulation over VAEs or Gaussian assumption models (Sec. 4.6). Before turning to the results, we explain training and implementation details of Pose-NDF in Sec. 4.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "We use the AMASS dataset [38] for training. As mentioned in Sec. 3.3, we train the network with supervision on predicted distance values, and hence we create a dataset of pose and distance pairs (\u03b8, d \u03b8 ). Since the training samples from AMASS lie on the desired manifold, d \u03b8 = 0 is assigned to all poses in the dataset. We then randomly generate negative samples with distance d \u03b8 > 0 by adding noise to AMASS poses. Please find details of data preparation in supplementary. We train our model in a multi-stage regime by varying the type of training samples used. We start our training with manifold poses \u03b8 m and non-manifold poses \u03b8 nm with a large distance to the desired manifold. Then we increase the number of non-manifold poses \u03b8 nm with a small distance in each training batch. This training scheme helps to initially learn a smooth surface and to iteratively introduce the fine details over the course of training. Our network architecture consists of one 2-layer MLP f enc with an output feature size of l = 6 for each joint, similar to [43]. Thus, the pose encoding network generates a feature vector p \u2208 R 126 . We implement the distance field network f df as a 5-layer MLP. For training, we use the softplus activation in the hidden layer and train the network end-to-end using the loss functions described in Eq. (4).", "publication_ref": ["b37", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Denoising Mocap Data", "text": "Human motion capture has been done using diverse setups ranging from RGB, RGB-D to IMU based capture systems. The data captured from these sources often produce artifacts like jitters, unnaturally rigid joints or weird bends at some joints, or positions with only partial observations. Prior work [52] improves the quality of captured motion sequences by using an optimization-based method, with the goal of recovering the captured data and preserving the realism of human poses. A robust and expressive human pose prior is key to preserve the realism of optimized poses, along with preserving the original data. Following HuMoR [52], we demonstrate the effectiveness of our pose manifold for: 1) motion denoising and 2) fitting to partial data.  We follow the same experimental setup as [52], but only deal with human poses and thus, remove the terms corresponding to human-scene contact and translation of root joint. In total, we find the pose parameters\u03b8 t at frame t as:\n\u03b8 t = arg min \u03b8 \u03bb v L v + \u03bb \u03b8 L \u03b8 + \u03bb t L t ,(7)\nwhere L v makes sure that the optimized pose is close to the observation and the temporal smoothness term L t enforces temporal consistency:\nL v = ||J (\u03b2 0 ,\u03b8 t ) \u2212 J obs || 2 2 L t = ||M (\u03b2 0 ,\u03b8 t ) \u2212 M (\u03b2 0 , \u03b8 t\u22121 )|| 2 2 . (8\n)\nHere, J (\u03b2, \u03b8) represent vertices (mocap markers) and M (\u03b2, \u03b8) represents SMPL mesh vertices for a given pose (\u03b8) and shape (\u03b2) parameters of SMPL [36]. Finally, we use Pose-NDF as a pose prior term in the optimization by minimizing the distance of the current pose from our learned manifold, L \u03b8 = f udf (\u03b8). We leverage the distance f udf (\u03b8) to get the optimal step size during optimization. We evaluate on two different settings: 1) clean mocap datasets and 2) a noisy mocap dataset. For clean mocap datasets, we use HPS [23] and the test split of AMASS [38,49]. For the noisy mocap dataset, we create random noisy sequences by adding Gaussian noise to AMASS test sequences and call it \"Noisy AMASS\". The average noise introduced in \"Noisy AMASS\" is 9.3 cm. We use a list of SMPL mesh vertices J as observation during optimization. We created the data with a fixed shape and do not optimize for shape parameters \u03b2. Instead of adding noise to joint locations, we add noise directly to the rotation of each joint. This is done for all methods to ensure a fair comparison. For HuMoR [52], we use the TestOpt optimization from the original work. VPoser does not have motion experiments, which is why we combine the latent space optimization from the original work with our optimization given in Eq. (7) to ensure that we compare against the best possible result. Specifically, we first encode the rotation matrix representation of noisy input pose \u03b8 t using the VPoser encoder as z t = f v enc (\u03b8 t ), then add random noise (\u03b5) in the latent space and reconstruct the pose b\u1ef9 \u03b8 t = f v dec (z t +\u03b5). Following [66], we observe that the temporal term in latent space yields better results than the temporal term in input pose/vertices, which we used in the VPoser experiment. The prior and temporal term for VPoserbased denoising are given as:\nL VPoser \u03b8 = ||\u03b5|| 2 L VPoser t = ||z t\u22121 \u2212\u1e91 t || 2 . (9\n)\nResults. We compare motion denoising between HuMoR [52] (TestOpt), Eq. ( 7) with VPoser prior [49], and Eq. ( 7 Table 2. Motion estimation from partial 3D observations: We compare pervertex error (in cm). It can be seen that for leg and arm/hand occlusions, Pose-NDF reconstructs the pose better than VPoser and HuMoR. For occluded shoulders, HuMoR takes the lead. We observe that results of Pose-NDF depend on the initialization of the occluded joint, as it is expected from manifold projection.\nchanges the pose and this change increases with an increasing number of frames. This is because HuMoR is a motion-based prior (conditioned on the previous pose) and, hence, over time the correction in pose accumulates and makes the output pose significantly different from the input. For the \"Noisy AMASS\" data, Pose-NDF-based optimization outperforms prior work. We visualise the denoising results in Fig. 2, and observe that the Pose-NDF-based method produces realistic and close to GT results. We further compare results of a sequence with HuMoR in Fig. 3. HuMoR results in large deviations from the input/GT, due to accumulation of correction over time. Fitting to partial data. We use the test set of AMASS and randomly create occluded poses (e.g. missing arm or legs or shoulder joint) and quantitatively compare with HuMoR [52] and VPoser [49] in Table 2. We use Eq. (7) for VPoser and Pose-NDF-based optimization. We only optimize for the occluded joints and for our model, we initialize the occluded joint pose randomly (close to 0). For HuMoR, we use the TestOpt provided in their paper. We evaluate on three different type of occlusions: 1) occluded left leg, 2) occluded left arm and 3) occluded right shoulder and upper arm. For the occluded leg case, VPoser and our prior-based method perform better. We believe this is because the majority of the poses in both AMASS training and test are upright with nearly straight legs and hence VPoser is biased towards these poses. For our method, it highly depends on initialization. Since we have used an initialization close to rest position, our optimization method generates smaller error for occluded legs but higher errors for occluded arms and shoulders, as they usually are more far away from the rest pose. For HuMoR, the motion generated is realistic and plausible, but in some cases results in large deviation from ground truth, because the correction in input pose accumulates over the time.", "publication_ref": ["b51", "b51", "b51", "b35", "b22", "b37", "b48", "b51", "b6", "b65", "b51", "b48", "b51", "b48"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "3D pose Estimation from Images", "text": "We now show that Pose-NDF can also be used as a prior in optimization-based 3D pose estimation from images [49]. We use the objective function proposed in SMPLify-X [49], see Eq. (10). Since we are working with a SMPL body only (without hands or faces), we remove the respective loss and prior terms. Thus, LSP dataset [30] High resolution LSP dataset [30] COCO dataset [35] 3DPW dataset [40] Fig. 4. 3D pose and shape estimation from in-the-wild images using Pose-NDF-based optimization method. Table 3. 3D pose and shape estimation from images using Pose-NDF, GAN-S [16] and VPoser [49] as pose prior terms in optimization-based method (left). We also use proposed prior and optimization pipeline to further improve the results of the SoTA 3D pose and shape estimation network, ExPose [14] (right).\nwe find the desired pose\u03b8 and shape\u03b2 as:\n\u03b2,\u03b8 = arg min \u03b2,\u03b8 L J + \u03bb \u03b8 L \u03b8 + \u03bb \u03b2 L \u03b2 + \u03bb \u03b1 L \u03b1 ,(10)\nwith data term L J , bending term L \u03b1 , shape regularizer L \u03b2 , and prior term L \u03b8 .\nThe data term and the bending term are given as:\nL J = i\u2208joints \u03b3 i w i \u03c1(\u03a0 K (R \u03b8 (J(\u03b2))) \u2212 J est,i ) L \u03b1 = i\u2208(elbow,knees)\nexp(\u03b8 i ), (11) where J est,i are 2D pose keypoints estimated by a SoTA 2D-pose estimation method [11], R \u03b8 transforms the joints along the kinematic tree according to the pose \u03b8, \u03a0 K represents a 3D to 2D projection with intrinsic camera parameters and \u03c1 represents a robust Geman-McClure error [18]. Further, the bending term L \u03b1 penalizes large bending near the elbow and knee joints, and the shape regularizer is given as L \u03b2 = ||\u03b2|| 2 [49]. For VPoser, the prior term is given as\nL \u03b8 = ||z|| 2 2 ,\nwhere z is the 32-dimensional latent vector of the VAE. In our model, we use L \u03b8 = f udf (\u03b8) and minimize the distance of the pose from our learned manifold using our projection algorithm. We leverage the distance information provided by our model in optimization by setting \u03bb \u03b8 = wf udf (\u03b8), where w has a fixed value. This ensures that if the pose is getting close to the manifold (i.e. f udf (\u03b8) is very small), the prior term is down-weighted, which results in faster convergence. Results. We use the EHF dataset [49] for quantitative evaluation and compare our work with the state-of-the-art priors VPoser [49] and GAN-S [16]. A Pose-NDF prior term slightly improves on the VPoser and GAN-S based optimization (Tab. 3). We observe that the neural network based model ExPose [14] outperforms all optimization-based results. However, we show that such methods can benefit from an optimization-based refinement step. We refine the ExPose output using Eq (10) with Pose-NDF as prior and compare this refinement with no-prior and other priors (Tab. 3). With no prior, the optimization objective only minimizes the joint projection loss, resulting in unrealistic poses. In contrast, GAN and Pose-NDF improve the result (qualitatively and quantitatively), generating realistic poses, while Pose-NDF outperforms the GAN prior. Finally, in Fig. 4 we show qualitative results of optimization-based 3D pose estimation on in-the-wild images from 3DPW [40], LSP [30] and MS-COCO [35] datasets.", "publication_ref": ["b48", "b48", "b29", "b29", "b34", "b15", "b48", "b13", "b10", "b10", "b17", "b48", "b48", "b48", "b15", "b13", "b39", "b29", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Pose Generation", "text": "We evaluate our model on the task of pose generation. Due to our distance field formulation, we can generate diverse poses by sampling a random point from SO(3) K and projecting it onto the manifold (Sec. 3.4). We compare the results of our model with sampling from the state-of-the-art pose prior VPoser [49], GMM [10,46] and GAN-S [16] in Fig. 5. We use Average Pairwise Distance (APD) [3], to quantify the diversity of generated poses. APD is defined as mean joint distance between all pairs of samples. We randomly sample 500 poses for each GMM, VPoser, GAN-S and Pose-NDF, which results in APD values of 48.24, 23.13, 27.52, 32.31 (in cm), respectively. We see that numerically, the GMM produces very large variance, but also results in unrealistic poses, as seen in Fig 5 (top-left). Pose-NDF generates more diverse poses than VPoser while producing only plausible poses. We also calculate the percentage of selfintersecting faces in generated poses, to evaluate one aspect of realism in poses. Pose-NDF generates poses with less self-intersecting faces (0.89%), as compared to the GAN-S (1.43%) and VPoser (2.10%).", "publication_ref": ["b48", "b9", "b45", "b15", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Pose Interpolation", "text": "Pose-NDF learns a manifold of plausible human poses, so it can be used to interpolate between two distinct poses by traversing the manifold. Specifically, for any given pose, we first project start (\u03b8 0 ) and end pose (\u03b8 T ) on our manifold using Eq (6), to get \u03b8 \u2032 0 and \u03b8 \u2032 T . We then move along the direction of \u03b8 \u2032 T from \u03b8 \u2032 0 with step size \u03c4 using Eq (12). The interpolated pose (\u03b8 t ) is again projected on the manifold to get a realistic pose (\u03b8 \u2032 t ). In the subsequent interpolation steps, we move from \u03b8 \u2032 t to \u03b8 \u2032 T , where \u03b8 \u2032 t is updated after each step.\n\u03b8 t = \u03b8 \u2032 t\u22121 + \u03c4 (\u03b8 \u2032 T \u2212 \u03b8 \u2032 t\u22121 )(12)\nResults: We compare the results of Pose-NDF with those from VPoser [49] and GAN-S [16] interpolation. For VPoser [49], we project the start and end pose into the latent space and perform linear interpolation using the latent vectors. For GAN-S [16], we use the spherical interpolation in latent space, as suggested in the work. We qualitatively evaluate the interpolation quality by calculating mean  6) and observe large jumps in VPoser interpolation. This behaviour is not observed in GAN-S and Pose-NDF based interpolation. Since the VAE learns a compact latent representation of poses, the distance between two input poses is not preserved in the latent space.", "publication_ref": ["b48", "b15", "b48", "b15"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Pose-NDF vs. Gaussian Assumption models", "text": "Prior work [52,49] uses VAE-based models as pose/motion prior, which follow a Gaussian assumption in the latent space. This has three major limitations, as mentioned in Sec. 1. Conversely, Pose-NDF learns the manifold directly in the pose-space without such assumptions and, hence, overcomes these limitations. We report the cumulative error based on deviation from the mean pose. We evaluate on AMASS Noisy (60 and 120 frames) and report cumulative error for samples with \u03c3, 2\u03c3, 3\u03c3 for both Pose-NDF and VPoser motion denoising. We obtain per-vertex error of 8.18, 8.20, 8.21 cm for Pose-NDF and 8.35, 9.11, 9.13 cm for VPoser, and 10.08, 11.38, 16.86 cm for HuMoR which reflects that VPoser and HuMoR perform well for poses close to the mean but the error increases for samples deviating from mean pose. Since the Gaussian distribution is unbounded, it produces dead regions, without any data points in these parts of distribution. Hence sampling in these regions might result in completely unrealistic poses for GMM and VPoser (Fig. 5). Lastly, since we learn the manifold in pose space, the distance between individual poses is preserved and leads to smoother interpolation compared to VPoser (see Sec. 4.5).", "publication_ref": ["b51", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We introduced a novel human pose prior model represented by a scalar neural distance field that describes a manifold of plausible poses as zero level set in SO(3) K . The method extends the idea of classic 3D shape representation using neural fields to higher the dimensions of human poses and maps quaternionbased poses to an unsigned distance value, representing the distance to the pose manifold. The resulting network can be used to project arbitrary poses to the pose manifold, opening applications in several areas. We comprehensively evaluate the performance of our model in diverse pose sampling, pose estimation from images, and motion denoising. We show that our model is able to generate poses with much more diversity than prior VAE-based works and improves state-of-the-art results in reconstruction from images and motion estimation.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Pose-conditioned joint angle limits for 3D human pose reconstruction", "journal": "CVPR", "year": "2015", "authors": "I Akhter; M J Black"}, {"ref_id": "b1", "title": "Structured prediction helps 3D human motion modelling", "journal": "ICCV", "year": "2019", "authors": "E Aksan; M Kaufmann; O Hilliges"}, {"ref_id": "b2", "title": "A stochastic conditioning scheme for diverse human motion prediction", "journal": "CVPR", "year": "2020", "authors": "S Aliakbarian; F Sadat Saleh; M Salzmann; L Petersson; S Gould"}, {"ref_id": "b3", "title": "Learning to reconstruct people in clothing from a single RGB camera", "journal": "CVPR", "year": "2019", "authors": "T Alldieck; M Magnor; B L Bhatnagar; C Theobalt; G Pons-Moll"}, {"ref_id": "b4", "title": "A hierarchy-aware pose representation for deep character animation", "journal": "", "year": "2021", "authors": "N Andreou; A Lazarou; A Aristidou; Y Chrysanthou"}, {"ref_id": "b5", "title": "Parametrization and range of motion of the ball-andsocket joint", "journal": "", "year": "2000", "authors": "P Baerlocher; R Boulic"}, {"ref_id": "b6", "title": "HP-GAN: probabilistic 3D human motion prediction via gan", "journal": "", "year": "2018", "authors": "E Barsoum; J Kender; Z Liu"}, {"ref_id": "b7", "title": "Multi-garment net: Learning to dress 3D people from images", "journal": "ICCV", "year": "2019", "authors": "B L Bhatnagar; G Tiwari; C Theobalt; G Pons-Moll"}, {"ref_id": "b8", "title": "BEHAVE: Dataset and method for tracking human object interactions", "journal": "CVPR", "year": "2022", "authors": "B L Bhatnagar; X Xie; I Petrov; C Sminchisescu; C Theobalt; G Pons-Moll"}, {"ref_id": "b9", "title": "Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image", "journal": "ECCV", "year": "2016", "authors": "F Bogo; A Kanazawa; C Lassner; P Gehler; J Romero; M J Black"}, {"ref_id": "b10", "title": "OpenPose: Realtime multi-person 2D pose estimation using part affinity fields", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2019", "authors": "Z Cao; G Hidalgo Martinez; T Simon; S Wei; Y A Sheikh"}, {"ref_id": "b11", "title": "Deep local shapes: Learning local SDF priors for detailed 3D reconstruction", "journal": "ECCV", "year": "2020", "authors": "R Chabra; J E Lenssen; E Ilg; T Schmidt; J Straub; S Lovegrove; R A Newcombe"}, {"ref_id": "b12", "title": "Neural unsigned distance fields for implicit function learning", "journal": "NeurIPS", "year": "2020", "authors": "J Chibane; A Mir; G Pons-Moll"}, {"ref_id": "b13", "title": "Monocular expressive body regression through body-driven attention", "journal": "ECCV", "year": "2020", "authors": "V Choutas; G Pavlakos; T Bolkart; D Tzionas; M J Black"}, {"ref_id": "b14", "title": "Viscosity solutions of Hamilton-Jacobi equations", "journal": "Transactions of the American mathematical society", "year": "1983", "authors": "M G Crandall; P L Lions"}, {"ref_id": "b15", "title": "Adversarial parametric pose prior", "journal": "CVPR", "year": "2022", "authors": "A Davydov; A Remizova; V Constantin; S Honari; M Salzmann; P Fua"}, {"ref_id": "b16", "title": "A joint-constraint model for human joints using signed distance-fields", "journal": "Multibody System Dynamics", "year": "2012", "authors": "M Engell-N\u00f8rreg\u00e5rd; S Niebe; K Erleben"}, {"ref_id": "b17", "title": "Statistical methods for tomographic image reconstruction", "journal": "", "year": "1987", "authors": "S Geman; D E Mcclure"}, {"ref_id": "b18", "title": "Hierarchical kinematic human mesh recovery", "journal": "ECCV", "year": "2020", "authors": "G Georgakis; R Li; S Karanam; T Chen; J Ko\u0161eck\u00e1; Z Wu"}, {"ref_id": "b19", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "I Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b20", "title": "Implicit geometric regularization for learning shapes", "journal": "", "year": "2020", "authors": "A Gropp; L Yariv; N Haim; M Atzmon; Y Lipman"}, {"ref_id": "b21", "title": "Action2Motion: Conditioned generation of 3d human motions", "journal": "ACMMM", "year": "2020", "authors": "C Guo; X Zuo; S Wang; S Zou; Q Sun; A Deng; M Gong; L Cheng"}, {"ref_id": "b22", "title": "Human POSEitioning System (HPS): 3D human pose estimation and self-localization in large scenes from bodymounted sensors", "journal": "CVPR", "year": "2021", "authors": "V Guzov; A Mir; T Sattler; G Pons-Moll"}, {"ref_id": "b23", "title": "Arch++: Animation-ready clothed human reconstruction revisited", "journal": "", "year": "2021", "authors": "T He; Y Xu; S Saito; S Soatto; T Tung"}, {"ref_id": "b24", "title": "Hierarchical implicit surface joint limits for human body tracking", "journal": "ECCV", "year": "2004", "authors": "L Herda; R Urtasun; P Fua"}, {"ref_id": "b25", "title": "Automatic determination of shoulder joint limits using quaternion field boundaries", "journal": "FG", "year": "2002", "authors": "L Herda; R Urtasun; A Hanson"}, {"ref_id": "b26", "title": "Arch: Animatable reconstruction of clothed humans", "journal": "", "year": "2020", "authors": "Z Huang; Y Xu; C Lassner; H Li; T Tung"}, {"ref_id": "b27", "title": "Metrics for 3D rotations: Comparison and analysis", "journal": "J. Math. Imaging Vis", "year": "2009-10", "authors": "D Q Huynh"}, {"ref_id": "b28", "title": "A large-scale RGB-D database for arbitrary-view human action recognition", "journal": "", "year": "2018", "authors": "Y Ji; F Xu; Y Yang; F Shen; H T Shen; W S Zheng"}, {"ref_id": "b29", "title": "Clustered pose and nonlinear appearance models for human pose estimation", "journal": "BMVC", "year": "2010", "authors": "S Johnson; M Everingham"}, {"ref_id": "b30", "title": "End-to-end recovery of human shape and pose", "journal": "CVPR", "year": "2018", "authors": "A Kanazawa; M J Black; D W Jacobs; J Malik"}, {"ref_id": "b31", "title": "Vibe: Video inference for human body pose and shape estimation", "journal": "CVPR", "year": "2020", "authors": "M Kocabas; N Athanasiou; M J Black"}, {"ref_id": "b32", "title": "Learning to reconstruct 3D human pose and shape via model-fitting in the loop", "journal": "ICCV", "year": "2019", "authors": "N Kolotouros; G Pavlakos; M J Black; K Daniilidis"}, {"ref_id": "b33", "title": "The kit bimanual manipulation dataset", "journal": "", "year": "2021", "authors": "F Krebs; A Meixner; I Patzer; T Asfour"}, {"ref_id": "b34", "title": "Microsoft COCO: Common Objects in Context", "journal": "", "year": "2014", "authors": "T Y Lin; M Maire; S Belongie; L Bourdev; R Girshick; J Hays; P Perona; D Ramanan; C L Zitnick; P Doll\u00e1r"}, {"ref_id": "b35", "title": "SMPL: A skinned multi-person linear model", "journal": "ACM Trans. Graphics (Proc. SIGGRAPH Asia)", "year": "2015", "authors": "M Loper; N Mahmood; J Romero; G Pons-Moll; M J Black"}, {"ref_id": "b36", "title": "MoSh: Motion and shape capture from sparse markers", "journal": "", "year": "2014", "authors": "M M Loper; N Mahmood; M J Black"}, {"ref_id": "b37", "title": "AMASS: Archive of motion capture as surface shapes", "journal": "ICCV", "year": "2019", "authors": "N Mahmood; N Ghorbani; N F Troje; G Pons-Moll; M J Black"}, {"ref_id": "b38", "title": "Adversarial autoencoders", "journal": "ICLR", "year": "2016", "authors": "A Makhzani; J Shlens; N Jaitly; I Goodfellow"}, {"ref_id": "b39", "title": "Recovering accurate 3d human pose in the wild using imus and a moving camera", "journal": "ECCV", "year": "2018", "authors": "T Von Marcard; R Henschel; M Black; B Rosenhahn; G Pons-Moll"}, {"ref_id": "b40", "title": "Recovering accurate 3D human pose in the wild using IMUs and a moving camera", "journal": "ECCV", "year": "2018", "authors": "T Von Marcard; R Henschel; M J Black; B Rosenhahn; G Pons-Moll"}, {"ref_id": "b41", "title": "Occupancy networks: Learning 3D reconstruction in function space", "journal": "CVPR", "year": "2019", "authors": "L Mescheder; M Oechsle; M Niemeyer; S Nowozin; A Geiger"}, {"ref_id": "b42", "title": "LEAP: Learning articulated occupancy of people", "journal": "CVPR", "year": "2021", "authors": "M Mihajlovic; Y Zhang; M J Black; S Tang"}, {"ref_id": "b43", "title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "journal": "ECCV", "year": "2020", "authors": "B Mildenhall; P P Srinivasan; M Tancik; J T Barron; R Ramamoorthi; R Ng"}, {"ref_id": "b44", "title": "Neural implicit surfaces in higher dimension", "journal": "", "year": "2022", "authors": "T Novello; V Da Silva; H Lopes; G Schardong; L Schirmer; L Velho"}, {"ref_id": "b45", "title": "Neural body fitting: Unifying deep learning and model based human pose and shape estimation", "journal": "", "year": "2018", "authors": "M Omran; C Lassner; G Pons-Moll; P Gehler; B Schiele"}, {"ref_id": "b46", "title": "Learning and tracking cyclic human motion", "journal": "Advances in Neural Information Processing Systems", "year": "2000", "authors": "D Ormoneit; H Sidenbladh; M Black; T Hastie"}, {"ref_id": "b47", "title": "DeepSDF: Learning continuous signed distance functions for shape representation", "journal": "CVPR", "year": "2019", "authors": "J J Park; P Florence; J Straub; R Newcombe; S Lovegrove"}, {"ref_id": "b48", "title": "Expressive body capture: 3D hands, face, and body from a single image", "journal": "CVPR", "year": "2011", "authors": "G Pavlakos; V Choutas; N Ghorbani; T Bolkart; A A A Osman; D Tzionas; M J Black"}, {"ref_id": "b49", "title": "Action-conditioned 3D human motion synthesis with transformer VAE", "journal": "ICCV", "year": "2021", "authors": "M Petrovich; M J Black; G Varol"}, {"ref_id": "b50", "title": "D-NeRF: Neural radiance fields for dynamic scenes", "journal": "CVPR", "year": "2020", "authors": "A Pumarola; E Corona; G Pons-Moll; F Moreno-Noguer"}, {"ref_id": "b51", "title": "HuMoR: 3D human motion model for robust pose estimation", "journal": "ICCV", "year": "2009", "authors": "D Rempe; T Birdal; A Hertzmann; J Yang; S Sridhar; L J Guibas"}, {"ref_id": "b52", "title": "SCANimate: Weakly supervised learning of skinned clothed avatar networks", "journal": "CVPR", "year": "2021", "authors": "S Saito; J Yang; Q Ma; M J Black"}, {"ref_id": "b53", "title": "3D human pose estimation: A review of the literature and analysis of covariates", "journal": "Computer Vision and Image Understanding", "year": "2016", "authors": "N Sarafianos; B Boteanu; B Ionescu; I A Kakadiaris"}, {"ref_id": "b54", "title": "NTU RGB+D: A large scale dataset for 3d human activity analysis", "journal": "CVPR", "year": "2016", "authors": "A Shahroudy; J Liu; T T Ng; G Wang"}, {"ref_id": "b55", "title": "A general joint component framework for realistic articulation in human characters", "journal": "", "year": "2003", "authors": "W Shao; V Ng-Thow-Hing"}, {"ref_id": "b56", "title": "Stochastic tracking of 3D human figures using 2D image motion", "journal": "ECCV", "year": "2000", "authors": "H Sidenbladh; M J Black; D Fleet"}, {"ref_id": "b57", "title": "Implicit neural representations with periodic activation functions", "journal": "NeurIPS", "year": "2020", "authors": "V Sitzmann; J N Martel; A W Bergman; D B Lindell; G Wetzstein"}, {"ref_id": "b58", "title": "Video-based reconstruction of animatable human characters", "journal": "ACM SIGGRAPH Asia", "year": "2010", "authors": "C Stoll; J Gall; E De Aguiar; S Thrun; C Theobalt"}, {"ref_id": "b59", "title": "SIZER: A dataset and model for parsing 3D clothing and learning size sensitive 3D clothing", "journal": "ECCV", "year": "2020", "authors": "G Tiwari; B L Bhatnagar; T Tung; G Pons-Moll"}, {"ref_id": "b60", "title": "Neural-GIF: Neural generalized implicit functions for animating people in clothing", "journal": "ICCV", "year": "2021", "authors": "G Tiwari; N Sarafianos; T Tung; G Pons-Moll"}, {"ref_id": "b61", "title": "3D people tracking with Gaussian process dynamical models", "journal": "CVPR", "year": "2006", "authors": "R Urtasun; D Fleet; P Fua"}, {"ref_id": "b62", "title": "Chore: Contact, human and object reconstruction from a single rgb image", "journal": "Springer", "year": "2022-10", "authors": "X Xie; B L Bhatnagar; G Pons-Moll"}, {"ref_id": "b63", "title": "Neural fields in visual computing and beyond", "journal": "", "year": "2022", "authors": "Y Xie; T Takikawa; S Saito; O Litany; S Yan; N Khan; F Tombari; J Tompkin; V Sitzmann; S Sridhar"}, {"ref_id": "b64", "title": "Denserac: Joint 3D pose and shape estimation by dense render and compare", "journal": "", "year": "2019", "authors": "Y Xu; S C Zhu; T Tung"}, {"ref_id": "b65", "title": "Learning motion priors for 4D human body capture in 3D scenes", "journal": "ICCV", "year": "2021", "authors": "S Zhang; H Zhang; F Bogo; M Pollefeys; S Tang"}, {"ref_id": "b66", "title": "PLACE: Proximity learning of articulation and contact in 3D environments", "journal": "", "year": "2020", "authors": "S Zhang; Y Zhang; Q Ma; M J Black; S Tang"}, {"ref_id": "b67", "title": "Generating 3D people in scenes without people", "journal": "CVPR", "year": "2020", "authors": "Y Zhang; M Hassan; H Neumann; M J Black; S Tang"}, {"ref_id": "b68", "title": "Toch: Spatio-temporal object correspondence to hand for motion refinement", "journal": "Springer", "year": "2022-10", "authors": "K Zhou; B L Bhatnagar; J E Lenssen; G Pons-Moll"}, {"ref_id": "b69", "title": "3D human shape reconstruction from a polarization image", "journal": "ECCV", "year": "2020", "authors": "S Zou; X Zuo; Y Qian; S Wang; C Xu; M Gong; L Cheng"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 2 .2Fig. 2. Motion denoising: We observe that Pose-NDF based motion denoising makes the pose realistic and solves small intersection issues, while VPoser and HuMoR still result in unrealistic poses.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 3 .3Fig. 3. Motion denoising:We compare the results on motion denoising using Pose-NDF and HuMoR[52] as priors with GT data and visualize every 10 th frame of a sequence. We observe that for HuMoR (top) the correction in input pose accumulates over time and makes the output pose significantly different from the GT (middle). Pose-NDF remains close to observations while correcting unrealistic poses (bottom).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 6 .6Fig. 6. Pose interpolation: We observe that VPoser-based interpolation (top) is less smooth than Pose-NDF-based pose interpolation (bottom).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "HuMoR changes the input pose significantly.", "figure_data": "DataHPS [23]AMASS [38]Noisy AMASS# frames601202406012024060120240MethodVPoser [49]4.914.163.811.521.551.478.969.139.15HuMoR [52]9.698.7310.863.213.623.6711.0417.1430.31Pose-NDF2.322.142.110.590.550.547.968.318.46Table 1. Motion denoising: We compare the per-vertex error (in cm) on mocapdata from HPS (left) and AMASS (middle) and on artificially created noisy AMASSdata (right). In all cases, Pose-NDF based motion denoising results in the least error.We also observe that in case of mocap data (HPS, AMASS), motion denoising usingPose-NDF results in very small error (small change from input), which is the desiredbehavior as these mocap poses are already realistic and hence close to our learnedVPoser manifold. On the other hand, Input HuMoROursGTVPoser HuMoR OursGT"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": ") with Pose-NDF prior in Table1. Pose-NDF achieves the lowest error in all settings. For mocap datasets like AMASS and HPS the motion is realistic, but can have small artifacts and jitter. Thus, an ideal motion/pose prior should not change the overall pose of these examples, but only fix these local artifacts. We observe that, numerically, VPoser and Pose-NDFbased optimization do not change the input pose significantly. However HuMoR", "figure_data": "DataOcc. LegOcc. Arm+handOcc. Shoulder +Upper Arm# frames601202406012024060120240MethodVPoser [49]2.532.572.548.518.528.599.989.499.48HuMoR [52]5.606.199.097.838.4410.254.755.114.95Pose-NDF2.492.512.477.818.137.987.637.896.76"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "GMM generates wrong and unrealistic poses, whereas VPoser, GAN-S and Pose-NDF generate much more realistic poses. We notice from APD, that variance of poses generated by Pose-NDF (32.31 cm) is larger than VPoser (23.13 cm) and GAN-S (27.52 cm).per-vertex distance between consecutive frames. Smaller value means smooth interpolation. We observe that Pose-NDF-based interpolation has a mean pervertex distance of 2.72 \u00b1 2.16, GAN-S has 2.71 \u00b1 2.45 and VPoser has 2.53 \u00b1 4.62, which shows that Pose-NDF and GAN-S based interpolation is smooth and the distance in input space is not entirely preserved in case of VAEs. We compare VPoser based interpolation with Pose-NDF in Fig.", "figure_data": "GMMVPoserGAN-SOursFig. 5. Pose generation:"}], "formulas": [{"formula_id": "formula_0", "formula_text": "f udf = f df \u2022 f enc \u2207 \u03b8 f udf", "formula_coordinates": [2.0, 246.73, 157.94, 209.63, 30.42]}, {"formula_id": "formula_1", "formula_text": "\u2022 A novel high-dimensional neural field representation in SO(3) K , Pose-NDF,", "formula_coordinates": [3.0, 141.74, 303.2, 338.85, 10.31]}, {"formula_id": "formula_2", "formula_text": "S = {\u03b8 \u2208 SO(3) K | f (\u03b8) = 0},(1)", "formula_coordinates": [4.0, 241.66, 612.88, 238.93, 10.81]}, {"formula_id": "formula_3", "formula_text": "d(\u03b8,\u03b8) = K i=1 w i 2 (arccos |\u03b8 \u22a4 i \u2022\u03b8 i |) 2 , (2", "formula_coordinates": [5.0, 227.79, 393.52, 248.56, 30.32]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [5.0, 476.35, 403.94, 4.24, 8.74]}, {"formula_id": "formula_5", "formula_text": "f enc 1 : (\u03b8 1 ) \u2192 v 1 f enc k : (\u03b8 k , v \u03c4 (k) ) \u2192 v k , k \u2208 {2 . . . K}(3)", "formula_coordinates": [6.0, 182.17, 158.94, 298.42, 12.69]}, {"formula_id": "formula_6", "formula_text": "p = [v 1 || . . . ||v K ]", "formula_coordinates": [6.0, 134.77, 217.03, 77.16, 9.68]}, {"formula_id": "formula_7", "formula_text": "L UDF = (\u03b8,d)\u2208D ||f udf (\u03b8)\u2212d \u03b8 || 2 L eikonal = (\u03b8,d)\u2208D, d\u0338 =0 (||\u2207 \u03b8 f udf (\u03b8)||\u22121) 2 , (4", "formula_coordinates": [6.0, 139.75, 374.44, 336.6, 22.6]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [6.0, 476.35, 376.51, 4.24, 8.74]}, {"formula_id": "formula_9", "formula_text": "\u03b8 = arg min \u03b8\u2208SO(3) K d(\u03b8, S),(5)", "formula_coordinates": [6.0, 261.71, 528.33, 218.88, 17.18]}, {"formula_id": "formula_10", "formula_text": "\u03b8 i = \u03b8 i\u22121 \u2212 \u03b1f (\u03b8 i\u22121 )\u2207 \u03b8 f (\u03b8 i\u22121 ),(6)", "formula_coordinates": [6.0, 236.18, 602.19, 244.41, 12.07]}, {"formula_id": "formula_11", "formula_text": "\u03b8 t = arg min \u03b8 \u03bb v L v + \u03bb \u03b8 L \u03b8 + \u03bb t L t ,(7)", "formula_coordinates": [8.0, 233.35, 472.96, 247.24, 21.64]}, {"formula_id": "formula_12", "formula_text": "L v = ||J (\u03b2 0 ,\u03b8 t ) \u2212 J obs || 2 2 L t = ||M (\u03b2 0 ,\u03b8 t ) \u2212 M (\u03b2 0 , \u03b8 t\u22121 )|| 2 2 . (8", "formula_coordinates": [8.0, 168.25, 534.51, 308.1, 15.66]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [8.0, 476.35, 539.55, 4.24, 8.74]}, {"formula_id": "formula_14", "formula_text": "L VPoser \u03b8 = ||\u03b5|| 2 L VPoser t = ||z t\u22121 \u2212\u1e91 t || 2 . (9", "formula_coordinates": [9.0, 214.33, 561.16, 262.02, 13.17]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [9.0, 476.35, 563.71, 4.24, 8.74]}, {"formula_id": "formula_16", "formula_text": "\u03b2,\u03b8 = arg min \u03b2,\u03b8 L J + \u03bb \u03b8 L \u03b8 + \u03bb \u03b2 L \u03b2 + \u03bb \u03b1 L \u03b1 ,(10)", "formula_coordinates": [11.0, 213.87, 384.02, 266.72, 16.63]}, {"formula_id": "formula_17", "formula_text": "L J = i\u2208joints \u03b3 i w i \u03c1(\u03a0 K (R \u03b8 (J(\u03b2))) \u2212 J est,i ) L \u03b1 = i\u2208(elbow,knees)", "formula_coordinates": [11.0, 142.35, 445.21, 276.32, 20.53]}, {"formula_id": "formula_18", "formula_text": "L \u03b8 = ||z|| 2 2 ,", "formula_coordinates": [11.0, 134.77, 546.95, 52.34, 12.2]}, {"formula_id": "formula_19", "formula_text": "\u03b8 t = \u03b8 \u2032 t\u22121 + \u03c4 (\u03b8 \u2032 T \u2212 \u03b8 \u2032 t\u22121 )(12)", "formula_coordinates": [12.0, 251.71, 587.79, 228.89, 13.03]}], "doi": ""}