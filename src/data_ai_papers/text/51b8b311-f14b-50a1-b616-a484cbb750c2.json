{"title": "Learning Mixtures of Linear Dynamical Systems", "authors": "Yanxi Chen; H Vincent Poor", "pub_date": "2022-05-25", "abstract": "We study the problem of learning a mixture of multiple linear dynamical systems (LDSs) from unlabeled short sample trajectories, each generated by one of the LDS models. Despite the wide applicability of mixture models for time-series data, learning algorithms that come with end-to-end performance guarantees are largely absent from existing literature. There are multiple sources of technical challenges, including but not limited to (1) the presence of latent variables (i.e. the unknown labels of trajectories); (2) the possibility that the sample trajectories might have lengths much smaller than the dimension d of the LDS models; and (3) the complicated temporal dependence inherent to time-series data. To tackle these challenges, we develop a two-stage meta-algorithm, which is guaranteed to efficiently recover each ground-truth LDS model up to error O( d/T ), where T is the total sample size. We validate our theoretical studies with numerical experiments, confirming the efficacy of the proposed algorithm.", "sections": [{"heading": "Introduction", "text": "Imagine that we are asked to learn multiple linear dynamical systems (LDSs) from a mixture of unlabeled sample trajectories -namely, each sample trajectory is generated by one of the LDSs of interest, but we have no idea which system it is. To set the stage and facilitate discussion, recall that in a classical LDS, one might observe a sample trajectory {x t } 0\u2264t\u2264T generated by an LDS obeying x t+1 = Ax t + w t , where A \u2208 R d\u00d7d determines the system dynamics in the noiseless case, and {w t } t\u22650 denote independent zero-mean noise vectors with covariance cov(w t ) = W 0. The mixed LDSs setting considered herein extends classical LDSs by allowing for mixed measurements as described below; see Figure 1 for a visualization of the scenario.\n\u2022 Multiple linear systems. Suppose that there are K different LDSs as represented by {(A (k) , W (k) )} 1\u2264k\u2264K , where A (k) \u2208 R d\u00d7d and W (k) \u2208 R d\u00d7d represent the state transition matrix and noise covariance matrix of the k-th LDS, respectively. Here and throughout, we shall refer to (A (k) , W (k) ) as the system matrix for the k-th LDS. We only assume that (A (k) , W (k) ) = (A ( ) , W ( ) ) for any k = , whereas A (k) = A ( ) or W (k) = W ( ) is allowed.\n\u2022 Mixed sample trajectories. We collect a total number of M unlabeled trajectories from these LDSs. More specifically, the m-th sample trajectory is drawn from one of the LDSs in the following manner: set (A, W ) to be (A (k) , W (k) ) for some 1 \u2264 k \u2264 K, and generate a trajectory of length T m obeying\nx t+1 = Ax t + w t , where the w t 's are i.i.d., E[w t ] = 0, cov(w t ) = W 0.\n(1)\nNote, however, that the label k associated with each sample trajectory is a latent variable not revealed to us, resulting in a mixture of unlabeled trajectories. The current paper focuses on the case where the length of each trajectory is somewhat short, making it infeasible to estimate the system matrix from a single trajectory.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A mixture of trajectories", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Clusters Estimated LDS models", "text": "Clustering and classification ( \" ! , % (!) ) ( \" $ , % ($) ) ( \" % , % (%) )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model estimation", "text": "Figure 1: A high-level visualization of the mixed LDSs formulation, and the algorithmic idea of combining clustering, classification, and model estimation. Here, we consider the special case where the multiple short trajectories come from the segments of a single continuous trajectory. The black dots within the continuous trajectory represent the time steps when the latent variable (i.e. the unknown label) changes.\n\u2022 Goal. The aim is to jointly learn the system matrices {(A (k) , W (k) )} 1\u2264k\u2264K from the mixture of sample trajectories. In particular, we seek to accomplish this task in a sample-efficient manner, where the total sample size is defined to be the aggregate trajectory length M m=1 T m . The mixed LDSs setting described above is motivated by many real-world scenarios where a single time-series model is insufficient to capture the complex and heterogeneous patterns in temporal data. In what follows, we single out a few potential applications, with a longer list provided in Section 4.\n\u2022 In psychology [BTBC16], researchers collect multiple time-series trajectories (e.g. depression-related symptoms over a period of time) from different patients. Fitting this data with multi-modal LDSs (instead of a single model) not only achieves better fitting performance, but also helps identify subgroups of the persons, which further inspires interpretations of the results and tailored treatments for patients from different subgroups.\n\u2022 Another example concerns an automobile sensor dataset, which consists of a single continuous (but possibly time-varying) trajectory of measurements from the sensors [HVBL17]. Through segmentation of the trajectory, clustering of the short pieces, and learning within each cluster, one can discover, and obtain meaningful interpretations for, a small number of key driving modes (such as \"driving straight\", \"slowing down\", \"turning\").\n\u2022 In robotics, a robot might collect data samples while navigating a complex environment with varying terrains (e.g. grass, sand, carpets, rocks) [BLL + 09]. With mixture modeling in place, one can hope to jointly learn different dynamical systems underlying various terrains, from a continuous yet timevarying trajectory acquired by the robot.\nWhile there is no shortage of potential applications, a mixture of linear dynamical systems is far more challenging to learn compared to the classical setting with a single LDS. In particular, the presence of the latent variables -namely, the unknown labels of the sample trajectories -significantly complicates matters. One straightforward idea is to learn a coarse model for each trajectory, followed by proper clustering of these coarse models (to be utilized for refined model estimation); however, this idea becomes infeasible in the high-dimensional setting unless all sample trajectories are sufficiently long. Another popular approach is to alternate between model estimation and clustering of trajectories, based on, say, the expectationmaximization (EM) algorithm; unfortunately, there is no theoretical support for such EM-type algorithms, and we cannot preclude the possibilities that the algorithms get stuck at undesired local optima. The lack of theoretical guarantees in prior literature motivates us to come up with algorithms that enjoy provable performance guarantees. Finally, the present paper is also inspired by the recent progress in meta-learning for mixed linear regression [KSS + 20, KSKO20], where the goal is to learn multiple linear models from a mixture of independent samples; note, however, that the temporal dependence underlying time-series data in our case poses substantial challenges and calls for the development of new algorithmic ideas.", "publication_ref": ["b7", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Main contributions", "text": "In this work, we take an important step towards guaranteed learning of mixed linear dynamical systems, focusing on algorithm design that comes with end-to-end theoretical guarantees. In particular, we propose a two-stage meta-algorithm to tackle the challenge of mixed LDSs:\n1. Coarse estimation: perform a coarse-level clustering of the unlabeled sample trajectories (assisted by dimension reduction), and compute initial model estimation for each cluster;\n2. Refinement: classify additional trajectories (and add each of them to the corresponding cluster) based on the above coarse model estimates, followed by refined model estimation with the updated clusters.\nThis two-stage meta approach, as well as the specific methods for individual steps, will be elucidated in Section 2. Encouragingly, assuming that the noise vectors {w m,t } are independent Gaussian random vectors, the proposed two-stage algorithm is not only computationally efficient, but also guaranteed to succeed in the presence of a polynomial sample size. Informally, our algorithm achieves exact clustering/classification of the sample trajectories as well as faithful model estimation, under the following conditions (with a focus on the dependency on the dimension d): (1) each short trajectory length T m is allowed to be much smaller than d; (2) the total trajectory length of each stage is linear in d (up to logarithmic factors); (3) to achieve a final model estimation error \u2192 0 (in the spectral norm), it suffices to have a total trajectory length of order d/ 2 for each LDS model. See Section 3 (in particular, Corollary 1) for the precise statements of our main results, which will also be validated numerically.\nIt is worth noting that, although we focus on mixed LDSs for concreteness, we will make clear that the proposed modular algorithm is fairly flexible and can be adapted to learning mixtures of other time-series models, as long as certain technical conditions are satisfied; see Remark 1 at the end of Section 2 for a detailed discussion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notation", "text": "Throughout this paper, vectors and matrices are represented by boldface letters. Let I d be the d \u00d7 d identity matrix. For a vector x \u2208 R d , we define (x) i \u2208 R as the i-th entry of x, and x 2 as its 2 norm; for a matrix X \u2208 R m\u00d7n , we define (X) i \u2208 R n as the transpose of the i-th row of X, and X (resp. X F ) as its spectral (resp. Frobenius) norm. For a square matrix X \u2208 R d\u00d7d , we denote its eigenvalues as {\u03bb i (X)} 1\u2264i\u2264d , and if X is symmetric, we sort its eigenvalues (which are all real) in descending order, with the maximum and minimum being \u03bb max (X) := \u03bb 1 (X) and \u03bb min (X) := \u03bb d (X), respectively; if in addition X is positive definite, we denote its condition number as \u03ba(X) := \u03bb max (X)/\u03bb min (X) \u2265 1. Let vec(\u2022) denote the vectorization of a matrix. For two matrices A, B, let A \u2297 B denote their Kronecker product. If A = [A ij ] 1\u2264i\u2264m,1\u2264j\u2264n and B = [B ij ] 1\u2264i\u2264m,1\u2264j\u2264n are of the same dimension, we denote by A, B := m i=1 n j=1 A ij B ij their inner product. Given vectors x 1 , . . . , x n \u2208 R d where n < d, let span{x i , 1 \u2264 i \u2264 N } \u2208 R d\u00d7n represent the subspace spanned by these vectors.\nThroughout this work, we always use the superscript \"(k)\" to indicate \"the k-th model\", as in A (k) and W (k) ; this is to be distinguished from the superscript \"k\" without the parentheses, which simply means the power of k. For a discrete set \u2126, we denote by |\u2126| its cardinality. Define 1(E) to be the indicator function, which takes value 1 if the event E happens, and 0 otherwise. Given a sequence of real numbers {x i } 1\u2264i\u2264N , we denote its median as median{x i , 1 \u2264 i \u2264 N }. Let a n b n and a n = O(b n ) indicate that a n \u2264 C 0 b n for all n = 1, 2, . . . , where C 0 > 0 is some universal constant; moreover, a n b n is equivalent to b n a n , and a n b n indicates that a n b n and a n b n hold simultaneously. We shall also let poly(n) denote some polynomial in n of a constant degree. For a positive integer n, we denote [n] := {1, 2, . . . , n}.\nAlgorithm 1: A two-stage algorithm for mixed LDSs 1 Input: M short trajectories {X m } 1\u2264m\u2264M (where X m = {x m,t } 0\u2264t\u2264Tm ); parameters \u03c4, G.\n// Stage 1: coarse estimation. 2 Run Algorithm 2 with {X m } m\u2208M subspace to obtain subspaces {V i , U i } 1\u2264i\u2264d .\n3 Run Algorithm 3 with {X m } m\u2208M clustering , {V i , U i } 1\u2264i\u2264d , \u03c4 , G, to obtain clusters {C k } 1\u2264k\u2264K . 4 Run Algorithm 4 with {C k } 1\u2264k\u2264K to obtain coarse models { A (k) , W (k) } 1\u2264k\u2264K .\n// Stage 2: refinement.\n5 Run Algorithm 5 with {X m } m\u2208M classification , { A (k) , W (k) } 1\u2264k\u2264K , {C k } 1\u2264k\u2264K , to update clusters {C k }.\n6 Run Algorithm 4 with {C k } 1\u2264k\u2264K to obtain refined models { A (k) , W (k) } 1\u2264k\u2264K .\n7 Output: final model estimation { A (k) , W (k) } 1\u2264k\u2264K and clusters {C k } 1\u2264k\u2264K .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms", "text": "We propose a two-stage paradigm for solving the mixed LDSs problem, as summarized in Algorithm 1. It consists of several subroutines as described in Algorithms 2-5, which will be explained in detail in the remainder of this section. Note that Algorithm 1 is stated in a modular manner, and one might replace certain subroutines by alternative schemes in order to handle different settings and model assumptions.\nBefore delving into the algorithmic details, we introduce some additional notation and assumptions that will be useful for our presentation, without much loss of generality. To begin with, we augment the notation for each sample trajectory with its trajectory index; that is, for each 1 \u2264 m \u2264 M , the m-th trajectorydenoted by X m := {x m,t } 0\u2264t\u2264Tm -starts with some initial state x m,0 \u2208 R d , and evolves according to the\nk m -th LDS for some unknown label 1 \u2264 k m \u2264 K such that x m,t+1 = A (km) x m,t + w m,t , where the w m,t 's are i.i.d., E[w m,t ] = 0, cov(w m,t ) = W (km) 0 (2) for all 0 \u2264 t \u2264 T m \u2212 1. Next, we divide the M sample trajectories {X m } 1\u2264m\u2264M in hand into three disjoint subsets M subspace , M clustering , M classification satisfying M subspace \u222a M clustering \u222a M classification = {1, 2, . . . , M },\nwhere each subset of samples will be employed to perform one subroutine. We assume that all trajectories within each subset have the same length, namely,\nT m = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 T subspace if m \u2208 M subspace , T clustering if m \u2208 M clustering , T classification if m \u2208 M classification .(3)\nFinally, we assume that K \u2264 d, so that performing subspace estimation in Algorithm 1 will be helpful (otherwise one might simply eliminate this step). The interested readers are referred to Appendix C for discussions of some potential extensions of our algorithms (e.g. adapting to the case where the trajectories within a subset have different lengths, or the case where certain parameters are a priori unknown).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary facts", "text": "We first introduce some preliminary background on the autocovariance structures and mixing property of linear dynamical systems, which form the basis of our algorithms for subspace estimation and clustering of trajectories.\nStationary covariance matrices. Consider first the basic LDS with x t+1 = Ax t + w t . If E[x t ] = 0, cov(x t ) = \u0393 and E[w t ] = 0, cov(w t ) = W , then it follows that\nE[x t+1 ] = 0 and cov(x t+1 ) = A \u2022 cov(x t ) \u2022 A + cov(w t ) = A\u0393A + W .\nUnder certain assumption on stability, this leads to the order-0 stationary autocovariance matrix \u0393(A, W ) of the LDS model (A, W ), defined as follows [KSH00]:\n\u0393(A, W ) := E x t x t |A, W = A \u2022 \u0393(A, W ) \u2022 A + W = \u221e t=0 A t W (A t ) .(4)\nFurthermore, define the order-1 stationary autocovariance matrix as\nY (A, W ) := E x t+1 x t |A, W = E (Ax t + w t )x t |A, W = A \u2022 \u0393(A, W ).(5)\nFor the mixed LDSs case (2) with K models, we abbreviate\n\u0393 (k) := \u0393(A (k) , W (k) ), Y (k) := Y (A (k) , W (k) ), 1 \u2264 k \u2264 K.(6)\nIn turn, the definitions of {\u0393 (k) , Y (k) } suggest that we can recover A (k) , W (k) from \u0393 (k) , Y (k) as follows:\nA (k) = Y (k) \u0393 (k) \u22121 , W (k) = \u0393 (k) \u2212 A (k) \u0393 (k) A (k) .(7)\nMixing. Expanding the LDS recursion x t+1 = Ax t + w t with cov(w t ) = W , we have\nx t+s = A s x t + s i=1 A i\u22121 w t+s\u2212i (8)\nfor all s \u2265 1. If A satisfies certain assumptions regarding stability and if s is larger than certain mixing time of the LDS, then the first term on the right-hand side of (8) approaches zero, while the second term is independent of the history up to time t, with covariance close to the stationary autocovariance matrix \u0393(A, W ). This suggests that, for two samples within the same trajectory that are sufficiently far apart, we can treat them as being (almost) independent of each other; this simple fact shall inspire our algorithmic design and streamline statistical analysis later on. It is noteworthy that the proposed algorithms do not require prior knowledge about the mixing times of the LDS models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Subspace estimation", "text": "Procedure. Recall that the notation (\u0393 (k\n) ) i \u2208 R d (resp. (Y (k) ) i ) represents the transpose of the i-th row of \u0393 (k) (resp. Y (k)\n) defined in (6). With this set of notation in place, let us define the following subspaces:\nV i := span (\u0393 (k) ) i , 1 \u2264 k \u2264 K , U i := span (Y (k) ) i , 1 \u2264 k \u2264 K , 1 \u2264 i \u2264 d.(9)\nIt is easily seen from the construction that each of these subspaces has rank at most K.\nAs it turns out, the collection of 2d subspaces defined in (9) provides crucial low-dimensional information about the linear dynamical systems of interest. This motivates us to develop a data-driven method to estimate these subspaces, which will in turn allow for proper dimension reduction in subsequent steps. Towards this end, we propose to employ a spectral method for subspace estimation using sample trajectories in M subspace :\n(i) divide {0, 1, . . . , T subspace } into four segments of the same size, and denote the 2nd (resp. 4th) segment\nas \u2126 1 (resp. \u2126 2 );\n(ii) for each m \u2208 M subspace , 1 \u2264 i \u2264 d, j \u2208 {1, 2}, compute h m,i,j := 1 |\u2126 j | t\u2208\u2126j (x m,t ) i x m,t , g m,i,j := 1 |\u2126 j | t\u2208\u2126j (x m,t+1 ) i x m,t , ;(10)\n(iii) for each 1 \u2264 i \u2264 d, compute the following matrices\nH i := 1 |M subspace | m\u2208M subspace h m,i,1 h m,i,2 , G i := 1 |M subspace | m\u2208M subspace g m,i,1 g m,i,2 ,(11)\nand let V i \u2208 R d\u00d7K (resp. U i ) be the top-K eigenspace of\nH i + H i (resp. G i + G i ).\nThe output {V i , U i } 1\u2264i\u2264d will then serve as our estimate of {V i , U i } 1\u2264i\u2264d . This spectral approach is summarized in Algorithm 2. \n\u2190 {N + j, 1 \u2264 j \u2264 N }, \u2126 2 \u2190 {3N + j, 1 \u2264 j \u2264 N }. 3 for (m, i, j) \u2208 M subspace \u00d7 [d] \u00d7 [2] do 4 Compute h m,i,j \u2190 1 |\u2126 j | t\u2208\u2126j (x m,t ) i x m,t , g m,i,j \u2190 1 |\u2126 j | t\u2208\u2126j (x m,t+1 ) i x m,t . 5 for i = 1, . . . , d do 6 Compute H i \u2190 1 |M subspace | m\u2208M subspace h m,i,1 h m,i,2 , G i \u2190 1 |M subspace | m\u2208M subspace g m,i,1 g m,i,2 , 7 Let V i \u2208 R d\u00d7K (resp. U i ) be the top-K eigenspace of H i + H i (resp. G i + G i ). 8 Output: subspaces {V i , U i } 1\u2264i\u2264d .\nRationale. According to the mixing property of LDS, if T subspace is larger than some appropriately defined mixing time, then each sample trajectory in M subspace will mix sufficiently and nearly reach stationarity (when constrained to t \u2208 \u2126 1 \u222a \u2126 2 ). In this case, it is easy to check that\nE h m,i,j ] \u2248 (\u0393 (km) ) i , E g m,i,j ] \u2248 (Y (km) ) i , 1 \u2264 i \u2264 d, j \u2208 {1, 2}.\nMoreover, the samples in \u2126 1 are nearly independent of those in \u2126 2 as long as the spacing between them exceeds the mixing time, and therefore,\nE H i \u2248 1 |M subspace | m\u2208M subspace (\u0393 (km) ) i (\u0393 (km) ) i = K k=1 p (k) subspace (\u0393 (k) ) i (\u0393 (k) ) i =: H i , (12a\n)\nE G i \u2248 1 |M subspace | m\u2208M subspace (Y (km) ) i (Y (km) ) i = K k=1 p (k) subspace (Y (k) ) i (Y (k) ) i =: G i ,(12b)\nwhere\np (k)\nsubspace denotes the fraction of sample trajectories generated by the k-th model, namely,\np (k) subspace := 1 |M subspace | m\u2208M subspace 1(k m = k), 1 \u2264 k \u2264 K. (13)\nAs a consequence, if T subspace and |M subspace | are both sufficiently large, then one might expect H i (resp. G i ) to be a reasonably good approximation of H i (resp. G i ), the latter of which has rank at most K and has V i (resp. U i ) as its eigenspace. All this motivates us to compute the rank-K eigenspaces of\nH i + H i and G i + G i in Algorithm 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Clustering", "text": "This step seeks to divide the sample trajectories in M clustering into K clusters (albeit not perfectly), such that the trajectories in each cluster are primarily generated by the same LDS model. We intend to achieve this by performing pairwise comparisons of the autocovariance matrices associated with the sample trajectories. ( ) . Therefore, in order to differentiate sample trajectories generated by different systems based on \u0393(A, W ) and Y (A, W ), it is important to ensure separation of (\u0393 (k) , Y (k) ) and (\u0393 ( ) , Y ( ) ) when k = l, which can be guaranteed by the following fact. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Key observation. Even though", "text": "(A (k) , W (k) ) = (A ( ) , W ( ) ), it is indeed possible that \u0393 (k) = \u0393 ( ) or Y (k) = Y\n\u2126 g,1 \u2190 (4g \u2212 3)N + j, 1 \u2264 j \u2264 N , \u2126 g,2 \u2190 (4g \u2212 1)N + j, 1 \u2264 j \u2264 N , 1 \u2264 g \u2264 G. 3 for (m, i, g, j) \u2208 M clustering \u00d7 [d] \u00d7 [G] \u00d7 [2] do 4 Compute h m,i,g,j \u2190 1 |\u2126 g,j | t\u2208\u2126g,j (x m,t ) i x m,t , g m,i,g,j \u2190 1 |\u2126 g,j | t\u2208\u2126g,j (x m,t+1 ) i x m,t .\n// Compute the similarity matrix S:\n5 for (m, n) \u2208 M clustering \u00d7 M clustering do 6 for g = 1, . . . , G do 7 Compute stat \u0393,g \u2190 d i=1 V i h m,i,g,1 \u2212 h n,i,g,1 , V i h m,i,g,2 \u2212 h n,i,g,2 , (14a) stat Y,g \u2190 d i=1 U i g m,i,g,1 \u2212 g n,i,g,1 , U i g m,i,g,2 \u2212 g n,i,g,2 ,(14b)\n8 Set S m,n \u2190 1 median stat \u0393,g , 1 \u2264 g \u2264 G + median stat Y,g , 1 \u2264 g \u2264 G \u2264 \u03c4 . 9 Divide M clustering into K clusters {C k } 1\u2264k\u2264K according to {S m,n } m,n\u2208M clustering . 10 Output: clusters {C k } 1\u2264k\u2264K .\nFact 1. If (A (k) , W (k) ) = (A ( ) , W ( ) ), then we have either \u0393 (k) = \u0393 ( ) or Y (k) = Y ( ) (or both).\nProof. We prove this by contradiction. Suppose instead that \u0393 (k) = \u0393 ( ) and Y (k) = Y ( ) , then (7) yields\nA (k) = Y (k) \u0393 (k) \u22121 = Y ( ) \u0393 ( ) \u22121 = A ( ) ,and\nW (k) = \u0393 (k) \u2212 A (k) \u0393 (k) A (k) = \u0393 ( ) \u2212 A ( ) \u0393 ( ) A ( ) = W ( ) ,\nwhich is contradictory to the assumption that (A (k) , W (k) ) = (A ( ) , W ( ) ).\nIdea. Let us compare a pair of sample trajectories {x t } 0\u2264t\u2264T clustering and {z t } 0\u2264t\u2264T clustering , where {x t } is generated by the system (A (k) , W (k) ) and {z t } by the system (A ( ) , W ( ) ). In order to determine whether k = , we propose to estimate the quantity\n\u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2\nF using the data samples {x t } and {z t }, which is expected to be small (resp. large) if k = (resp. k = ). To do so, let us divide {0, 1, . . . , T clustering } evenly into four segments, and denote by \u2126 1 (resp. \u2126 2 ) the 2nd (resp. 4th) segment, akin to what we have done in the previous step. Observe that\nU i E (x t+1 ) i x t \u2212 (z t+1 ) i z t \u2248 U i (Y (k) ) i \u2212 (Y ( ) ) i for all 1 \u2264 i \u2264 d and t \u2208 \u2126 1 \u222a \u2126 2 .\nAssuming sufficient mixing and utilizing (near) statistical independence due to sample splitting, we might resort to the following statistic:\nstat Y := d i=1 U i 1 |\u2126 1 | t\u2208\u21261 (x t+1 ) i x t \u2212 (z t+1 ) i z t , U i 1 |\u2126 2 | t\u2208\u21262 (x t+1 ) i x t \u2212 (z t+1 ) i z t ,(15)\nwhose expectation is given by\nE[stat Y ] \u2248 d i=1 U i (Y (k) ) i \u2212 (Y ( ) ) i , U i (Y (k) ) i \u2212 (Y ( ) ) i = d i=1 U i (Y (k) ) i \u2212 (Y ( ) ) i 2 2 \u2248 d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 2 = Y (k) \u2212 Y ( ) 2 F ;\nhere, the first approximation is due to the near independence between samples from \u2126 1 and those from \u2126 2 , whereas the second approximation holds if each subspace U i is sufficiently close to U i = span{(Y (j) ) i , 1 \u2264 j \u2264 K}. The purpose of utilizing {U i } is to reduce the variance of stat Y . Similarly, we can compute another statistic based on {V i } as follows:\nstat \u0393 := d i=1 V i 1 |\u2126 1 | t\u2208\u21261 (x t ) i x t \u2212 (z t ) i z t , V i 1 |\u2126 2 | t\u2208\u21262 (x t ) i x t \u2212 (z t ) i z t ,(16)\nwhich has expectation\nE[stat \u0393 ] \u2248 d i=1 V i ((\u0393 (k) ) i \u2212 (\u0393 ( ) ) i ) 2 2 \u2248 \u0393 (k) \u2212 \u0393 ( ) 2 F .\nConsequently, we shall declare k = if stat \u0393 + stat Y exceeds some appropriate threshold \u03c4 .\nProcedure. We are now positioned to describe the proposed clustering procedure. We first compute the statistics stat \u0393 and stat Y for each pair of sample trajectories in M clustering by means of the method described above, and then construct a similarity matrix S, in a way that S m,n is set to 0 if stat \u0393 + stat Y (computed for the m-th and n-th trajectories) is larger than a threshold \u03c4 , or set to 1 otherwise. In order to enhance the robustness of these statistics, we divide {0, 1, . . . , T clustering } into 4G (instead of 4) segments, compute G copies of stat \u0393 and stat Y , and take the medians of these values. Next, we apply a mainstream clustering algorithm (e.g. spectral clustering [CCF + 21]) to the similarity matrix S, and divide M clustering into K disjoint clusters {C k } 1\u2264k\u2264K . The complete clustering procedure is provided in Algorithm 3. The threshold \u03c4 shall be chosen to be on the order of min 1\u2264k< \u2264K { \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F } (which is strictly positive due to Fact 1), and it suffices to set the number of copies G to be on some logarithmic order. Our choice of these parameters will be specified in Theorem 1 momentarily.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model estimation", "text": "Suppose that we have obtained reasonably good clustering accuracy in the previous step, namely for each 1 \u2264 k \u2264 K, the cluster C k output by Algorithm 3 contains mostly trajectories generated by the same model (A (\u03c0(k)) , W (\u03c0(k)) ), with \u03c0 representing some permutation function of {1, . . . , K}. We propose to obtain a coarse model estimation and covariance estimation, as summarized in Algorithm 4. More specifically, for each k, we use the samples {{x m,t } 0\u2264t\u2264Tm } m\u2208C k to obtain an estimate of A (\u03c0(k)) by solving the following least-squares problem [SMT + 18, SR19]\nA (k) := arg min A m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t+1 \u2212 Ax m,t 2 2 ,(17)\nwhose closed-form solution is given in (18a). Next, we can use A (k) to estimate the noise vector w m,t := x m,t+1 \u2212 A (k) x m,t \u2248 w m,t , and finally estimate the noise covariance W (\u03c0(k)) with the empirical covariance of { w m,t }, as shown in (18b).\nAlgorithm 4: Least squares and covariance estimation\n1 Input: clusters {C k } 1\u2264k\u2264K . 2 for k = 1, . . . , K do 3 Compute A (k) \u2190 m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t+1 x m,t m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t x m,t\u22121\n, and (18a)\nW (k) \u2190 1 m\u2208C k T m m\u2208C k 0\u2264t\u2264Tm\u22121\nw m,t w m,t , where w m,t = x m,t+1 \u2212 A (k) x m,t . (18b)\n4 Output: estimated models { A (k) , W (k) } 1\u2264k\u2264K .\nAlgorithm 5: Classification\n1 Input: short trajectories {X m } m\u2208M classification , where X m = {x m,t } 0\u2264t\u2264Tm ; coarse models { A (k) , W (k) } 1\u2264k\u2264K ; clusters {C k } 1\u2264k\u2264K . 2 for m \u2208 M classification do 3\nInfer the label of the m-th trajectory by\nk m \u2190 arg min T m \u2022 log det( W ( ) ) + Tm\u22121 t=0 (x m,t+1 \u2212 A ( ) x m,t ) ( W ( ) ) \u22121 (x m,t+1 \u2212 A ( ) x m,t ) ,\nthen add m to cluster C km .\n4 Output: updated clusters {C k } 1\u2264k\u2264K .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Classification", "text": "Procedure. In the previous steps, we have obtained initial clusters {C k } 1\u2264k\u2264K and coarse model estimates { A (k) , W (k) } 1\u2264k\u2264K . With the assistance of additional sample trajectories {X m } m\u2208M classification , we can infer their latent labels and assign them to the corresponding clusters; the procedure is stated in Algorithm 5 and will be explained shortly. Once this is done, we can run Algorithm 4 again with the updated clusters {C k } to refine our model estimates, which is exactly the last step of Algorithm 1.\nRationale. The strategy of inferring labels in Algorithm 5 can be derived from the maximum likelihood estimator, under the assumption that the noise vectors {w m,t } follow Gaussian distributions. Note, however, that even in the absence of Gaussian assumptions, Algorithm 5 remains effective in principle. To see this, consider a short trajectory {x t } 0\u2264t\u2264T generated by model (A (k) , W (k) ), i.e. x t+1 = A (k) x t + w t where E[w t ] = 0, cov(w t ) = W (k) . Let us define the following loss function\nL(A, W ) := T \u2022 log det(W ) + T \u22121 t=0 (x t+1 \u2212 Ax t ) W \u22121 (x t+1 \u2212 Ax t ).(19)\nWith some elementary calculation, we can easily check that for any incorrect label = k, it holds that E[L(A ( ) , W ( ) ) \u2212 L(A (k) , W (k) )] > 0, with the proviso that (A (k) , W (k) ) = (A ( ) , W ( ) ) and {x t } are nondegenerate in some sense; in other words, the correct model (A (k) , W (k) ) achieves the minimal expected loss (which, due to the quadratic form of the loss function, depends solely on the first and second moments of the distributions of {w t }, as well as the initial state x 0 ). This justifies the proposed procedure for inferring unknown labels in Algorithm 5, provided that T m is large enough and that the estimated LDS models are sufficiently reliable.\nRemark 1. While this work focuses on mixtures of LDSs, we emphasize that the general principles of Algorithm 1 are applicable under much weaker assumptions. For Algorithms 2 and 3 to work, we essentially only require that each sample trajectory satisfies a certain mixing property, and that the autocovariances of different models are sufficiently separated (and hence distinguishable). As for Algorithms 4 and 5, we essentially require a well-specified parametric form of the time-series models. This observation might inspire future extensions (in theory or applications) of Algorithm 1 to much broader settings.\n3 Main results", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model assumptions", "text": "To streamline the theoretical analysis, we focus on the case where the trajectories are driven by Gaussian noise; that is, for each 1 \u2264 m \u2264 M, 0 \u2264 t \u2264 T m , the noise vector w m,t in (2) is independently generated from the Gaussian distribution N (0, W (km) ). Next, we assume for simplicity that the labels {k m } 1\u2264m\u2264M of the trajectories are pre-determined and fixed, although one might equivalently regard {k m } as being random and independent of the noise vectors {w m,t }. Moreover, while our algorithms and analysis are largely insensitive to the initial states {x m,0 } 1\u2264m\u2264M , we focus on two canonical cases for concreteness: (i) the trajectories start at zero state, or (ii) they are segments of one continuous long trajectory. This is formalized as follows:\nCase 0:\nx m,0 = 0, 1 \u2264 m \u2264 M ; (20a) Case 1: x 1,0 = 0, and x m+1,0 = x m,Tm , 1 \u2264 m \u2264 M \u2212 1.(20b)\nWe further introduce some notation for the total trajectory length: define\nT total := 1\u2264m\u2264M T m = T total,subspace + T total,clustering + T total,classification ,\nwhere\nT total,o := T o \u2022 |M o |, o \u2208 {subspace, clustering, classification}.\nAdditionally, we assume that each model occupies a non-degenerate fraction of the data; in other words, there exists some 0 < p min \u2264 1/K such that\np min \u2264 p (k) o := 1 |M o | m\u2208Mo 1(k m = k), 1 \u2264 k \u2264 K, o \u2208 {subspace, clustering, classification}.\nFinally, we make the following assumptions about the ground-truth LDS models, where we recall that the autocovariance matrices {\u0393 (k) , Y (k) } have been defined in (6). Assumption 1. The LDS models {A (k) , W (k) } 1\u2264k\u2264K satisfy the following conditions:\n1. There exist \u03ba A \u2265 1 and 0 \u2264 \u03c1 < 1 such that for any 1 \u2264 k \u2264 K,\n(A (k) ) t \u2264 \u03ba A \u2022 \u03c1 t , t = 1, 2, . . . ;(21)\n2. There exist \u0393 max \u2265 W max \u2265 W min > 0 and \u03ba w,cross \u2265 \u03ba w \u2265 1 such that for any\n1 \u2264 k \u2264 K, \u03bb max (\u0393 (k) ) \u2264 \u0393 max , W min \u2264 \u03bb min (W (k) ) \u2264 \u03bb max (W (k) ) \u2264 W max , W max W min =: \u03ba w,cross , \u03ba(W (k) ) = \u03bb max (W (k) ) \u03bb min (W (k) ) \u2264 \u03ba w ;\n3. There exist \u2206 \u0393,Y , \u2206 A,W > 0 such that for any 1 \u2264 k < \u2264 K, \n\u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2265 \u2206 2 \u0393,Y ,(22a)\nA (k) \u2212 A ( ) 2 F + W (k) \u2212 W ( ) 2 F W 2 max \u2265 \u2206 2 A,W .(22b)\n\u0393 (k) , Y (k)\nOrder-0 and order-1 stationary autocovariance matrices of the k-th LDS model\n\u03ba A , \u03c1 (A (k) ) t \u2264 \u03ba A \u2022 \u03c1 t , t = 1, 2, . . . \u2206 \u0393,Y , \u2206 A,W Model separation (22) W min , W max W min \u2264 \u03bb min (W (k) ) \u2264 \u03bb max (W (k) ) \u2264 W max , 1 \u2264 k \u2264 K \u03ba w,cross , \u03ba w \u03ba w,cross = W max /W min ; \u03ba(W (k) ) \u2264 \u03ba w , 1 \u2264 k \u2264 K \u0393 max \u0393 (k) \u2264 \u0393 max , 1 \u2264 k \u2264 K\nThe first assumption states that each state transition matrix A (k) is exponentially stable, which is a quantified version of stability and has appeared in various forms in the literature of LDS [KSH00, CHK + 18]; here, \u03ba A can be regarded as a condition number, while \u03c1 is a contraction rate. The second assumption ensures that the noise covariance matrices {W (k) } are well conditioned, and the autocovariance matrices {\u0393 (k) } are bounded. The third assumption quantifies the separation between different LDS models; in particular, the notion of \u2206 \u0393,Y (resp. \u2206 A,W ) will play a crucial role in our analysis of the clustering (resp. classification) step in Algorithm 1. It is important that we consider the separation of (\u0393 (k) , Y (k) ) versus (\u0393 ( ) , Y ( ) ) jointly, which guarantees that \u2206 \u0393,Y is always strictly positive (thanks to Fact 1), despite the possibility of \u0393 (k) = \u0393 ( ) or Y (k) = Y ( ) ; the reasoning for our definition of \u2206 A,W is similar. For the readers' convenience, we include Table 1 for a quick reference to the key notation and parameters used in our analysis. Remark 2. Since the separation parameters \u2206 A,W , \u2206 \u0393,Y in (22) are defined with respect to the Frobenius norm, we may naturally regard them as\n\u2206 A,W = \u221a d \u03b4 A,W , \u2206 \u0393,Y = \u221a d \u03b4 \u0393,Y ,(23)\nwhere \u03b4 A,W , \u03b4 \u0393,Y are the canonical separation parameter (in terms of the spectral norm). For example, consider the simple setting where K = 2, W (1) = W (2) , A (1) = 0.5I d and A (2) = (0.5 + \u03b4)I d for some canonical separation parameter \u03b4; in this case, we have\n\u2206 A,W = A (1) \u2212 A (2) F = \u03b4I d F = \u221a\nd\u03b4. This observation will be crucial for obtaining the correct dependence on the dimension d in our subsequent analysis. Remark 3. Most of the parameters in Assumption 1 come directly from the ground-truth LDS models {A (k) , W (k) }, except \u0393 max and \u2206 \u0393,Y . In fact, they can be bounded by \u0393 max W max and \u2206 \u0393,Y \u2206 A,W ; see Fact 2 in the appendix for the formal statements. However, the pre-factors in these bounds can be pessimistic in general cases, therefore we choose to preserve the parameters \u0393 max and \u2206 \u0393,Y in our analysis.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Theoretical guarantees", "text": "We are now ready to present our end-to-end performance guarantees for Algorithm 1. Recall the definition of Cases 0 and 1 in (20). Our main results for these two cases are summarized in Theorems 1 and 2 below, with proofs deferred to Section 5. This section ends with some interpretations and discussions of the results.\nTheorem 1 (Main result for Case 0). There exist positive constants {C i } 1\u2264i\u22648 such that the following holds for any fixed 0 < \u03b4 < 1/2. Consider the model (2) under the assumptions in Sections 2 and 3.1, with a focus on Case 0 (20a). Suppose that we run Algorithm 1 with parameters \u03c4, G that satisfy 1/8 < \u03c4 /\u2206 2 \u0393,Y < 3/8, G \u2265 C 1 \u03b9 1 , and data {X m } 1\u2264m\u2264M (where X m = {x m,t } 0\u2264t\u2264Tm ) that satisfies\nT subspace \u2265 C 2 \u03b9 1 1 \u2212 \u03c1 , T total,subspace \u2265 C 3 d 1 \u2212 \u03c1 \u0393 max \u221a d \u2206 \u0393,Y 4 K 2 p min 2 + 1 \u2022 \u03b9 4 1 ,(24a)\nT clustering \u2265 C 4 G 1 \u2212 \u03c1 \u0393 2 max \u03ba 2 A \u221a dK \u2206 2 \u0393,Y + 1 \u03b9 2 , T total,clustering \u2265 C 5 d\u03ba 2 w,cross p min d \u2206 2 A,W \u0393 max W min + 1 \u03b9 2 3 ,(24b)\nT classification \u2265 C 6 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 1 ,(24c)\nwhere we define the logarithmic terms \u03b9 1 := log(\nd\u03ba A T total \u03b4 ), \u03b9 2 := log ( \u0393max \u2206 \u0393,Y +2) d\u03ba A T total \u03b4 , \u03b9 3 := log( \u0393max W min d\u03ba A T total \u03b4\n). Then, with probability at least 1 \u2212 \u03b4, Algorithm 1 achieves exact clustering in Line 3 and exact classification in Line 5; moreover, there exists some permutation \u03c0 : {1, . . . , K} \u2192 {1, . . . , K} such that the final model estimation { A (k) , W (k) } 1\u2264k\u2264K in Line 6 obeys\nA (k) \u2212 A (\u03c0(k)) \u2264 C 7 d\u03ba w \u03b9 3 p min T , W (k) \u2212 W (\u03c0(k)) W (\u03c0(k)) \u2264 C 8 d\u03b9 3 p min T , 1 \u2264 k \u2264 K,(25)\nwhere T := T total,clustering + T total,classification .\nIn fact, for a sharper result, one can replace the p min T terms in the final error bounds (25) with the sample size for the \u03c0(k)-th model, namely\nT (\u03c0(k)) := p (\u03c0(k)) clustering T total,clustering + p (\u03c0(k)) classification T total,classification \u2265 p min T .\nTheorem 2 (Main result for Case 1). Consider the same setting of Theorem 1, except that we focus on Case 1 (cf. (20b)) instead. If we replace the conditions on T total,clustering and T classification in (24) with T total,clustering \u2265 C 5 d\u03ba 2 w,cross\np min d \u2206 2 A,W \u0393 max W min \u03ba 2 A + 1 \u03b9 2 3 ,(26a)\nT classification \u2265 C 6 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 1 + 1 1 \u2212 \u03c1 log(2\u03ba A ),(26b)\nthen the same high-probability performance guarantees in Theorem 1 continue to hold.\nWhile Theorems 1 and 2 guarantee that Algorithm 1 successfully learns a mixture of LDS models with a polynomial number of samples, these results involve many parameters and may be somewhat difficult to interpret. In the following corollary, we make some simplifications and focus on the most important parameters.\nCorollary 1. Consider the same setting of Theorems 1 and 2. For simplicity, suppose that the condition numbers \u03ba A , \u03ba w , \u03ba w,cross 1, and the fractions of data generated by different LDS models are balanced, namely p min 1/K. Moreover, define the canonical separation parameters \u03b4 A,W := \u2206 A,W / \u221a d and \u03b4 \u0393,Y := \u2206 \u0393,Y / \u221a d, as suggested by Remark 2. Finally, define the mixing time t mix := 1/(1 \u2212 \u03c1). Then we can rewrite the sample complexities in Theorems 1 and 2 as follows (where we hide the logarithmic terms {\u03b9 i } 1\u2264i\u22643 ): if\nT subspace t mix , T total,subspace t mix d \u0393 max K \u03b4 \u0393,Y 4 + 1 , T clustering t mix \u0393 max \u03b4 \u0393,Y 2 K d + 1 , T total,clustering Kd 1 \u03b4 2 A,W \u0393 max W min + 1 , T classification \uf8f1 \uf8f2 \uf8f3 1 d\u03b4 2 A,W + 1 for Case 0, 1 d\u03b4 2 A,W + t mix for Case 1, T total,clustering + T total,classification Kd 2 ,\nthen with high probability, Algorithm 1 achieves exact clustering in Line 3, exact classification in Line 5, and final model estimation errors\nA (k) \u2212 A (\u03c0(k)) \u2264 , W (k) \u2212 W (\u03c0(k)) W (\u03c0(k)) \u2264 , 1 \u2264 k \u2264 K\nfor some permutation \u03c0 : {1, . . . , K} \u2192 {1, . . . , K}.\nIn what follows, we briefly remark on the key implications of Corollary 1.\n\u2022 Dimension d and targeted error : (1) Our algorithms allow the T o 's to be much smaller than (and even decrease with) d.\n(2) Each T total,o shall grow linearly with d, and it takes an order of Kd/ 2 samples in total to learn K models in up to \u2192 0 errors (in the spectral norm), which is just K times the usual parametric rate (d/ 2 ) of estimating a single model.\n\u2022 Mixing time t mix : (1) T subspace and T clustering are linear in t mix , which ensures sufficient mixing and thus facilitates our algorithms for Stage 1. In contrast, T classification depends on t mix only for Case 1, with the sole and different purpose of ensuring that the states {x m,t } are bounded throughout (see Example 1 in the appendix for an explanation of why this is necessary).\n(2) While T total,subspace needs to grow linearly with t mix , this is not required for T total,clustering and T total,classification , because our methods for model estimation (i.e. least squares and covariance estimation) do not rely on mixing [SMT + 18, SR19].\n\u2022 Canonical separation parameters \u03b4 A,W , \u03b4 \u0393,Y : (1) T clustering 1/\u03b4 2 \u0393,Y guarantees exact clustering of the trajectories, while T classification 1/\u03b4 2 A,W guarantees exact classification. (2) T total,subspace 1/\u03b4 4 \u0393,Y\nleads to sufficiently accurate subspaces, while T total,clustering 1/\u03b4 2 A,W leads to accurate initial model estimation. 1 Remark 4. The step of subspace estimation is non-essential and optional; it allows for a smaller T clustering , but comes at the price of complicating the overall algorithm. For practitioners who prefer a simpler algorithm, they might simply remove this step (i.e. Line 2 of Algorithm 1), and replace the rank-K subspaces {V i , U i } with I d (i.e. no dimensionality reduction for the clustering step). The theoretical guarantees continue to hold with minor modification: in Corollary 1, one simply needs to remove the conditions on T subspace , T total,subspace , and in the condition for T clustering , replace the factor K/d (where K is due to dimensionality reduction) with d/d = 1. This is one example regarding how our modular algorithms and theoretical analysis can be easily modified and adapted to accommodate different situations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Numerical experiments", "text": "We now validate our theoretical findings with a series of numerical experiments, confirming that Algorithm 1 successfully solves the mixed LDSs problem. In these experiments, we fix d = 80, K = 4; moreover, let T subspace = 20, T clustering = 20 and T classification = 5, all of which are much smaller than d. Our experiments focus on Case 1 as defined in (20b), and we generate the labels of the sample trajectories uniformly at random. The groundtruth LDS models are generated in the following manner: A (k) = \u03c1R (k) , where \u03c1 = 0.5 and R (k) \u2208 R d\u00d7d is a random orthogonal matrix; W (k) has eigendecomposition U (k) \u039b (k) (U (k) ) , where U (k) is a random orthogonal matrix and the diagonal entries of \u039b (k) are independently drawn from the uniform distribution on [1, 2].\nOur experimental results are illustrated in Figure 2. Here, the horizontal axis represents the sample size T = T total,clustering +T total,classification for model estimation, and the vertical axis represents the estimation errors, measured by max 1\u2264k\u2264K A (k) \u2212 A (\u03c0(k)) (plotted in blue) and max 1\u2264k\u2264K W (k) \u2212 W (\u03c0(k)) / W (\u03c0(k)) (plotted in orange). The results confirm our main theoretical prediction: Algorithm 1 recovers the LDS models based on a mixture of short trajectories with length T o d, and achieves an error rate of 1/ \u221a T . In addition, we observe in our experiments that the final outputs of Algorithm 1 are robust to a small number of mis-clustered or mis-classified trajectories during the intermediate steps; this is clearly an appealing property to have, especially when the  ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Related works", "text": "Mixed linear regression and meta-learning. Our work is closely related to the recent papers [KSS + 20, KSKO20] that bridge mixed linear regression and meta-learning. In mixed linear regression [QR78], it is assumed that there exist a small number of linear models of interest; each independent data point is generated by one of these models, although the label (i.e., which model generates a data sample) is unknown. The past decade has witnessed numerous theoretical advances in this problem, e.g. [YCS14, CYC17, LL18, CLS20, KHC21, DK20, YPCR18, PM20, CMPC21, DEF + 21]. In addition, meta-learning [FAL17, HSFP20] considers the problem of jointly solving multiple supervised learning tasks, each of which has only a few samples. The key assumption is that there is a common underlying structure (or inductive bias) in these different tasks; the goal is to retrieve this structure, which can be utilized later on to learn a new task with much fewer samples. Some closely related problem formulations include few-shot learning [SSZ17, DHK + 21], transfer learning [PY09, TJJ20], multi-task learning [Bax00, MPRP16], and so on. Finally, in the setting of metalearning for mixed linear regression [KSS + 20, KSKO20], each task contains a few independent and identically distributed samples generated by a linear regression model, and the inductive bias is that there is only a small discrete set of ground-truth linear models. Our work extends this setting to time-series data (where each short trajectory can be naturally regarded as a task); some parts of our algorithmic designs have been inspired by the work of [KSS + 20, KSKO20], and we also improve upon some of their analyses. work from this extensive literature is that, we design algorithms and prove rigorous non-asymptotic sample complexities for model estimation, in the specific setting of mixture modeling that features (1) a finite set of underlying time-series models, and (2) unknown labels of the trajectories, with no probabilistic assumptions imposed on these latent variables.\nLinear dynamical systems. Linear dynamical systems (also referred to as vector autoregressive models in the statistics literature) is one of the most fundamental models in system identification and optimal control [Lju98,KDG96]. In the past few years, there has been a surge of studies, in both control and machine learning communities, about non-asymptotic theoretical analyses of various learning procedures for LDSs. This includes the basic LDS model [FTM18, SMT + 18, SR19], LDSs with control input and quadratic cost (i.e. linear-quadratic regulators) [DMM + 20, CHK + 18, JP19, FTM20, MTR19, SF20, FGKM18, MPB + 19], and LDSs with partial observation y t \u2248 Cx t (e.g. Kalman filtering or linear-quadratic-Gaussian control) [OO19, SBR19, SRD21, SOF20, TMP20, LAHA20, ZFKL21]. In particular, it was only until recently that the authors of [SMT + 18, SR19] proved sharp error bounds of ordinary least squares for estimating the state transition matrix of the basic LDS model, using a single trajectory; our analysis of Algorithm 4 is largely inspired by their techniques.", "publication_ref": ["b67", "b48", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "This section provides detailed, modular theoretical results for the performance of Algorithms 2-5, and concludes with a proof for the main theorems in Section 3.2 (i.e. the performance guarantees of Algorithm 1).\nSubspace estimation. The following theorem provides upper bounds on the errors of subspaces {V i , U i } output by Algorithm 2, assuming sufficient mixing of the short trajectories.\nTheorem 3. Consider the model (2) under the assumptions in Sections 2 and 3.1. There exist some universal constants C 1 , C 2 , C 3 > 0 such that the following holds. Suppose that we run Algorithm 2 with data {X m } m\u2208M subspace obeying\nT subspace \u2265 C 1 \u2022 t mix , T total,subspace \u2265 C 2 \u2022 t mix d \u2022 log T total d \u03b4 ,where\nt mix := 1 1 \u2212 \u03c1 \u2022 log d\u03ba A T total \u03b4 .\nThen with probability at least 1 \u2212 \u03b4, Algorithm 2 outputs {V i , U i } 1\u2264i\u2264d satisfying the following: for all 1 \u2264 k \u2264 K and\n1 \u2264 i \u2264 d, max (\u0393 (k) ) i \u2212 V i V i (\u0393 (k) ) i 2 , (Y (k) ) i \u2212 U i U i (Y (k) ) i 2 \u2264 C 3 \u2022 \u0393 max K p min 1/2 t mix d T total,subspace log 3 T total d \u03b4 1/4 .(27)\nOur proof (deferred to Appendix A.2) includes a novel perturbation analysis; the resulted error bound (27) has a 1/T 1/4 total,subspace dependence and is gap-free (i.e. independent of the eigenvalue gaps of the ground-truth low-rank matrices, which can be arbitrarily close to zero in the worst case). It is possible to adapt the existing perturbation results in [KSS + 20, KSKO20] to our setting (which we include in Lemma 3 in the appendix for completeness); however, one of them is dependent on the eigenvalue gaps, while the other one incurs a worse 1/T 1/6 total,subspace dependence. It would be interesting future work to investigate whether a gap-free bound with a 1/T 1/2 total,subspace dependence is possible.\nClustering. Our next theorem shows that Algorithm 3 achieves exact clustering of M clustering , if T clustering is sufficiently large and subspaces {V i , U i } are accurate. The proof is deferred to Appendix A.3.\nTheorem 4. Consider the model (2) under the assumptions in Sections 2 and 3.1. There exist universal constants C 1 , C 2 , c 3 > 0 such that the following holds. Suppose that we run Algorithm 3 with data {X m } m\u2208M clustering , independent subspaces {V i , U i } 1\u2264i\u2264d and parameters \u03c4 , G that satisfy the following:\n\u2022 The threshold \u03c4 obeys 1/8 < \u03c4 /\u2206 2 \u0393,Y < 3/8; \u2022 The short trajectory length T clustering = N G, where G \u2265 C 1 \u2022 log |M clustering | \u03b4 , N \u2265 C 2 \u0393 2 max \u03ba 2 A \u221a dK \u2206 2 \u0393,Y + 1 1 1 \u2212 \u03c1 log \u0393 max \u2206 \u0393,Y + 2 d\u03ba A T total \u03b4 ; \u2022 The subspaces {V i , U i } 1\u2264i\u2264d satisfy that, for all 1 \u2264 i \u2264 d and 1 \u2264 k \u2264 K, max (\u0393 (k) ) i \u2212 V i V i (\u0393 (k) ) i 2 , (Y (k) ) i \u2212 U i U i (Y (k) ) i 2 \u2264 c 3 \u2206 \u0393,Y \u221a d . (28\n)\nThen with probability at least 1 \u2212 \u03b4, Algorithm 3 achieves exact clustering: for all m 1 , m 2 \u2208 M clustering , S m1,m2 = 1 if and only if the m 1 -th and m 2 -th trajectories are generated by the same model, i.e. they have the same label k m1 = k m2 .\nLeast squares and covariance estimation. The next result controls the model estimation errors of Algorithm 4, under the assumption that every cluster is pure.\nTheorem 5. Consider the model (2) under the assumptions in Section 3.1. There exist universal constants\nC 1 , C 2 , C 3 > 0 such that the following holds. Let {C k } 1\u2264k\u2264K be subsets of M clustering \u222a M classification such that for all 1 \u2264 k \u2264 K, C k contains only short trajectories generated by model (A (k) , W (k) ), namely k m = k for all m \u2208 C k . Suppose that for all m \u2208 M clustering \u222a M classification , T m \u2265 4, and for all 1 \u2264 k \u2264 K, T (k) total := m\u2208C k T m \u2265 C 1 d\u03ba 2 w \u03b9, where \u03b9 := log \u0393 max W min d\u03ba A T total \u03b4 .\nLet A (k) , W (k) be computed by (18) in Algorithm 4. Then with probability at least 1 \u2212 \u03b4, one has\nA (k) \u2212 A (k) \u2264 C 2 d\u03ba w \u03b9 T (k) total , W (k) \u2212 W (k) W (k) \u2264 C 3 d\u03b9 T (k) total , 1 \u2264 k \u2264 K.\nOur proof (postponed to Appendix A.4) is based on the techniques of [SMT + 18, SR19], but with two major differences. First, the authors of [SMT + 18, SR19] consider the setting where ordinary least squares is applied to a single continuous trajectory generated by a single LDS model; this is not the case for our setting, and thus our proof and results are different from theirs. Second, the noise covariance matrix W is assumed to be \u03c3 2 I d in [SMT + 18, SR19], while in our case, {W (k) } 1\u2264k\u2264K are unknown and need to be estimated.\nClassification. Our last theorem shows that Algorithm 5 correctly classifies all trajectories in M classification , as long as the coarse models are sufficiently accurate and the short trajectory lengths are large enough; these conditions are slightly different for Cases 0 and 1 defined in (20). See Appendix A.5 for the proof.\nTheorem 6. Consider the model (2) under the assumptions in Section 3.1. There exist universal constants c 1 , c 2 , C 3 > 0 such that the following holds. Suppose that we run Algorithm 5 with data {X m } m\u2208M classification and independent coarse models {\nA (k) , W (k) } 1\u2264k\u2264K satisfying A (k) \u2212 A (k) \u2264 A , W (k) \u2212 W (k) \u2264 W for all k.\nThen with probability at least 1 \u2212 \u03b4, Algorithm 5 correctly classifies all trajectories in M classification , provided that For Case 0:\nA \u2264 c 1 \u2206 A,W W min \u0393 max \u03ba w,cross (d + \u03b9) , W W min \u2264 c 2 \u2022 min 1, \u2206 A,W \u03ba w,cross d , (29a\n)\nT m \u2265 C 3 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 , m \u2208 M classification ; (29b) For Case 1: A \u2264 c 1 \u2206 A,W W min \u0393 max \u03ba w,cross \u03ba 2 A (d + \u03b9) , W W min \u2264 c 2 \u2022 min 1, \u2206 A,W \u03ba w,cross d ,(29c)\nT m \u2265 C 3 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 + 1 1 \u2212 \u03c1 log(2\u03ba A ), m \u2208 M classification ,(29d)\nwhere \u03b9 := log T total \u03b4 is a logarithmic term.\nProof of Theorems 1 and 2. Our main theorems are direct implications of the above guarantees for the individual steps.\n\u2022 Stage 1 : To begin with, according to Theorem 3, if the condition (24a) on T subspace and T total,subspace hold, then the subspaces {V i , U i } output by Line 2 of Algorithm 1 are guaranteed to satisfy the error bounds (28) required by Theorem 4. This together with the condition (24b) on T clustering ensures exact clustering in Line 3.\n\u2022 Stage 2 : Based on this, if we further know that T total,clustering obeys condition (24b) for Case 0 or (26a) for Case 1, then Theorem 5 tells us that the coarse model estimation in Line 4 satisfies the error bounds (29a) or (29c) required by Theorem 6. Together with the assumption (24c) or (26b) on T classification , this guarantees exact classification in Line 5 of Algorithm1, for either Case 0 or 1. At the end, the final model estimation errors (25) follow immediately from Theorem 5.\nNote that all the statements above are high-probability guarantees; it suffices to take the union bound over these steps, so that the performance guarantees of Theorems 1 and 2 hold with probability at least 1 \u2212 \u03b4. This finishes the proof of our main theorems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "This paper has developed a theoretical and algorithmic framework for learning multiple LDS models from a mixture of short, unlabeled sample trajectories. Our key contributions include a modular two-stage metaalgorithm, as well as theoretical analysis demonstrating its computational and statistical efficiency in solving the mixed LDSs problem. We would like to invite the readers to contribute to this important topic by, say, further strengthening the theoretical analysis and algorithmic design. For example, in certain cases T clustering can be a bottleneck compared with T subspace and T classification , and thus one might hope to achieve a better dependence on T total,clustering (e.g. allowing T total,clustering d), by replacing Line 4 in Algorithm 1 with a different method (e.g. adapting the method of coarse model estimation in [KSS + 20, Algorithm 3] to our setting). As another example, from the practical perspective, it is possible that the data is a single continuous trajectory and the time steps when the underlying model changes are unknown [HVBL17, HSFP20]; in order to accommodate such a case, one might need to incorporate change-point detection into the learning process.\nMoving beyond the current setting of mixed LDSs, we remark that there are plenty of opportunities for future studies. For instance, while our methods in Stage 1 rely on the mixing property of the LDS models, it is worth exploring whether it is feasible to handle the non-mixing case [SMT + 18, SR19]. Another potential direction is to consider the robustness against outliers and adversarial noise [CKMY21, KSKO20]. One might even go further and extend the ideas (e.g. the two-stage meta-algorithm) to learning mixtures of other time-series models (potentially with model selection [WL00]), such as LDS with partial observations or nonlinear observations [MFS + 20], autoregressive-moving-average (ARMA) models, nonlinear dynamical systems [MJR20, KKL + 20, FSR20], to name a few. Ultimately, it would be of great importance to consider the case with controlled inputs, such as learning mixtures of linear-quadratic regulators, or latent Markov decision processes [KECM21] that arises in reinforcement learning.", "publication_ref": ["b90", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "A Proofs for Section 5", "text": "This section starts with some preliminaries about linear dynamical systems that will be helpful later. Then, it provides the main proofs for the theorems in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Preliminaries", "text": "Truncation of autocovariance. Recall the notation 4) and (6). We add a subscript t to represent its t-step truncation:\n\u0393 (k) = \u221e i=0 A (k) i W (k) (A (k) i ) from (\n\u0393 (k) t := t\u22121 i=0 A (k) i W (k) (A (k) i ) . (30\n)\nAlso recall the assumption of exponential stability in (21), namely (A\n(k) ) t \u2264 \u03ba A \u03c1 t . As a result, \u0393 (k) t is close to \u0393 (k) : 0 \u0393 (k) \u2212 \u0393 (k) t = \u221e i=t A (k) i W (k) (A (k) i ) = A (k) t \u0393 (k) (A (k) t ) , \u0393 (k) \u2212 \u0393 (k) t \u2264 \u0393 (k) A (k) t 2 \u2264 \u0393 (k) \u03ba 2 A \u03c1 2t \u2264 \u0393 max \u03ba 2 A \u03c1 2t . (31\n)\nMoreover, let Y (k) t := A (k) \u0393 (k) t , then Y (k) t is also close to Y (k) : Y (k) \u2212 Y (k) t \u2264 A (k) \u0393 (k) \u2212 \u0393 (k) t \u2264 \u0393 max \u03ba 3 A \u03c1 2t .(32)\n\"Independent version\" of states. Given some mixing time t mix , we define x m,t (t mix ) as the (t mix \u22121)-step approximation of x m,t :\nx m,t = x m,t (t mix ) :=\nt mix \u22122 i=0 (A (km) ) i w m,t\u2212i\u22121 \u223c N (0, \u0393 (km) t mix \u22121 ), t mix \u2264 t \u2264 T m , 1 \u2264 m \u2264 M. (33\n)\nSince x m,t consists of only the most recent noise vectors, it is independent of the history up to x m,t\u2212t mix +1 .\nOur proofs for Stage 1 will rely on this \"independent version\" { x m,t } of states {x m,t }; the basic idea is that, for an appropriately chosen t mix , a trajectory of length T can be regarded as a collection of T /t mix independent samples. We will often use the notation \u2022 to represent the \"independent version\" of other variables as well.\nBoundedness of states. The following lemma provides upper bounds for { x m,t 2 } and { x m,t 2 }. This will help to control the effects of mixing errors and model estimation errors in the analyses later.\nLemma 1 (Bounded states). Consider the model (2) under the assumptions in Sections 2 and 3.1. Fix any t mix \u2265 3. Then with probability at least 1 \u2212 \u03b4, we have\nx m,t 2 \u2264 C 0 \u0393 max (d + log(T total /\u03b4)) for all 1 \u2264 m \u2264 M, t mix \u2264 t \u2264 T m\n, where C 0 > 0 is some universal constant; moreover, for both cases of initial states defined in (20), all states {{x m,t } 0\u2264t\u2264Tm } 1\u2264m\u2264M are bounded throughout:\n\u2022 Case 0: with probability at least 1 \u2212 \u03b4, we have x m,t 2 \u2264 C 0 \u0393 max (d + log(T total /\u03b4)) for all m, t.\n\u2022 Case 1: suppose that t mix \u2265 1 1\u2212\u03c1 log( \u221a 2\u03ba A ), and T m \u2265 t mix for all m, then with probability at least 1\u2212\u03b4, we have x m,t 2 \u2264 3C 0 \u03ba A \u0393 max (d + log(T total /\u03b4)) for all m, t, and x m,t 2 \u2264 2C 0 \u0393 max (d + log(T total /\u03b4)) for all t \u2265 t mix or t = 0.\nProof. First, recall from [Ver18, Corollary 7.3.3] that, if random vector a \u223c N (0, I d ), then for all u \u2265 0, we have\nP( a 2 \u2265 2 \u221a d + u) \u2264 2 exp(\u2212cu 2 ). Since x m,t \u223c N (0, \u0393 (km) t mix \u22121 ), where \u0393 (km) t mix \u22121 \u0393 (km) \u0393 max I d , we have P( x m,t 2 \u2265 \u221a \u0393 max (2 \u221a d + u)) \u2264 2 exp(\u2212cu 2 ).\nTaking the union bound, we have P there exists m, t such that x m,t 2 \u2265 \u0393 max (2\n\u221a d + u) \u2264 M m=1 Tm t=t mix P x m,t 2 \u2265 \u0393 max (2 \u221a d + u) \u2264 2T total exp(\u2212cu 2 ) \u2264 \u03b4,\nwhere the last inequality holds if we pick u \u2265 1 c log 2T total \u03b4 . This finishes the proof of the first claim in the lemma. Next, we prove the boundedness of { x m,t 2 }.\n\u2022 Case 0 : It is easy to check that x m,t \u223c N (0, \u0393 (km) t\n), where \u0393 (km) t \u0393 (km) \u0393 max I d . The boundedness of { x m,t 2 } can be proved by a similar argument as before, which we omit for brevity.\n\u2022 Case 1 : Define \u03be m,t := x m,t \u2212 (A (km) ) t x m,0 \u223c N (0, \u0393 (km) t\n). By a similar argument as before, we have with probability at least 1 \u2212 \u03b4, \u03be m,t 2 \u2264 C 0 \u0393 max (d + log(T total /\u03b4)) for all m, t. Morever, for any\nt \u2265 t mix \u2265 1 1\u2212\u03c1 log( \u221a 2\u03ba A ), we have (A (km) ) t \u2264 \u03ba A \u03c1 t \u2264 1/2.\nWith this in place, we have\nx m+1,0 = x m,Tm = (A (km)\n) Tm x m,0 + \u03be m,Tm , and thus\nx m+1,0 \u2264 (A (km) ) Tm x m,0 2 + \u03be m,Tm 2 \u2264 1 2 x m,0 2 + C 0 \u0393 max (d + log T total \u03b4 ).\nRecall the assumption that x 1,0 = 0; by induction, we have\nx m,0 2 \u2264 2C 0 \u0393 max (d + log(T total /\u03b4))\nfor all 1 \u2264 m \u2264 M . Now, we have for all m, t,\nx m,t 2 \u2264 (A (km) ) t x m,0 2 + \u03be m,t 2 \u2264 \u03ba A x m,0 2 + C 0 \u0393 max (d + log T total \u03b4 ) \u2264 3C 0 \u03ba A \u0393 max (d + log T total \u03b4 );\nmoreover, for t \u2265 t mix , since (A (km) ) t \u2264 1/2, we obtain a better bound\nx m,t 2 \u2264 (A (km) ) t x m,0 2 + \u03be m,t 2 \u2264 1 2 x m,0 2 + C 0 \u0393 max (d + log T total \u03b4 ) \u2264 2C 0 \u0393 max (d + log T total \u03b4 ).\nThis shows the boundedness of { x m,t 2 } and completes our proof of the lemma.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Theorem 3", "text": "Theorem 3 is an immediate consequence of Lemmas 2 and 3 below. The former shows the concentration of H i , G i around the targeted low-rank matrices H i , G i , while the latter is a result of perturbation analysis.\nLemma 2. Under the setting of Theorem 3, with probability at least 1 \u2212 \u03b4, we have for all\n1 \u2264 i \u2264 d, max H i \u2212 H i , G i \u2212 G i \u0393 2 max t mix d T total,subspace log 3 T total d \u03b4 . Lemma 3. Consider the matrix M = K k=1 p (k) y (k) y (k) \u2208 R d\u00d7d , where 0 < p (k) < 1, K k=1 p (k)\n= 1, and y (k) \u2208 R d . Let M be symmetric and satisfy M \u2212 M \u2264 , and U \u2208 R d\u00d7K be the top-K eigenspace of M . Then we have\nK k=1 p (k) y (k) \u2212 U U y (k) 2 2 \u2264 2K ,(34)\nand for all 1 \u2264 k \u2264 K, it holds that\ny (k) \u2212 U U y (k) 2 \u2264 min 2K p (k) 1/2 , 2 \u03bb min (M ) y (k) 2 , \u221a 2 p (k) y (k) 2 1/3 . (35\n)\nFor our main analyses in Sections 3.2 and 5, we choose the first term on the right-hand side of (35).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2.1 Proof of Lemma 2", "text": "We first analyze the idealized case with i.i.d. samples; then we make a connection between this i.i.d. case and the actual case of mixed LDSs, by utilizing the mixing property of linear dynamical systems. We prove the result of Lemma 2 for G i \u2212 G i only, since the analysis for H i \u2212 H i is mostly the same (and simpler in fact).\nStep 1: the idealized i.i.d. case. With some abuse of notation, suppose that for all 1 \u2264 m \u2264 M , we have for some k m \u2208 {1, . . . , K},\nx m,t , z m,t\ni.i.d. \u223c N (0, \u0393 (km) ), w m,t , v m,t i.i.d. \u223c N (0, W (km) ), x m,t = A (km) x m,t + w m,t , z m,t = A (km) z m,t + v m,t , 1 \u2264 t \u2264 N,\nwhere for all 1 \u2264 k \u2264 K, it holds that W (k) , \u0393 (k) \u0393 (k) \u0393 max I d . Notice that cov(x m,t ) = A (km) \u0393 (km) (A (km) ) + W (km) A (km) \u0393 (km) (A (km) ) + W (km) = \u0393 (km) \u0393 max I d . Consider the i.i.d. version of matrix G i and its expectation G i defined as follows:\nG i := 1 M N M m=1 N t=1 x m,t i x m,t z m,t i z m,t , G i = K k=1 p (k) ( Y (k) ) i ( Y (k) ) i ,\nwhere ( Y (k) ) i is the transpose of the i-th row of Y (k) := A (k) \u0393 (k) . For this i.i.d. setting, we claim that,\nif the i.i.d. sample size M N satisfies M N d \u2022 log(M N d/\u03b4), then with probability at least 1 \u2212 \u03b4, G i \u2212 G i \u0393 2 max d M N log 3 M N d \u03b4 , 1 \u2264 i \u2264 d.(36)\nThis claim can be proved by a standard covering argument with truncation; we will provide a proof later for completeness.\nStep 2: back to the actual case of mixed LDSs. Now we turn to the analysis of G i defined in (11) versus its expectation G i defined in (12b), for the mixed LDSs setting. We first show that G i can be writte as a weighted average of some matrices, each of which can be further decomposed into an i.i.d. part (as in\nStep 1) plus a negligible mixing error term. Then we analyze each term in the decomposition, and finally put pieces together to show that G i \u2248 G i .\nStep 2.1: decomposition of the index set \u2126 1 \u00d7 \u2126 2 . Recall the definition of index sets \u2126 1 , \u2126 2 in Algorithm 2. Denote the first index of \u2126 1 (resp. \u2126 2 ) as \u03c4 1 + 1 (resp. \u03c4 2 + 1), and let \u2206 := \u03c4 2 \u2212 \u03c4 1 be their distance. Also denote N :\n= |\u2126 1 | = |\u2126 2 | T subspace . For any t \u2208 \u2126 1 and 1 \u2264 j \u2264 N , define s j (t) := Cycle(t + \u2206 + j; \u2126 2 ) = t + \u2206 + j if t + \u2206 + j \u2264 \u03c4 2 + N, t + \u2206 + j \u2212 N otherwise,\nwhere Cycle(i; \u2126) represents the cyclic indexing of value i on set \u2126. Then we have\n\u2126 1 \u00d7 \u2126 2 = (t 1 , t 2 ), t 1 \u2208 \u2126 1 , t 2 \u2208 \u2126 2 = \u222a N j=1 t, s j (t) , t \u2208 \u2126 1 .\nWe further define\nS \u03c4 := {\u03c4 1 + \u03c4 + f \u2022 t mix : f \u2265 0, \u03c4 + f \u2022 t mix \u2264 N }, 1 \u2264 \u03c4 \u2264 t mix , so that \u2126 1 = \u222a t mix \u03c4 =1 S \u03c4 .\nNotice that for each \u03c4 , the elements of S \u03c4 are at least t mix far apart. Putting together, we have\n\u2126 1 \u00d7 \u2126 2 = \u222a N j=1 \u222a t mix \u03c4 =1 t, s j (t) , t \u2208 S \u03c4 . (37\n)\nStep 2.2: decomposition of G i . In the remaining proof, we denote x m,t := x m,t+1 for notational consistency. Using the decomposition (37) of \u2126 1 \u00d7 \u2126 2 , we can rewrite G i defined in (11) as a weighted average of N t mix matrices:\nG i = 1 |M subspace | m\u2208M subspace 1 N 2 (t1,t2)\u2208\u21261\u00d7\u21262 (x m,t1 ) i x m,t1 \u2022 (x m,t2 ) i x m,t2 = N j=1 t mix \u03c4 =1 |S \u03c4 | N 2 \u2022 F i,j,\u03c4 ,\nwhere\nF i,j,\u03c4 := 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 (x m,t ) i x m,t \u2022 (x m,sj (t) ) i x m,sj (t) . (38\n)\nRecalling the definition of x m,t in (33) and \u0393\n(k) t in (30), we have\nx m,t = x m,t + (A (km) ) t mix \u22121 x m,t\u2212t mix +1 =:\u03b4m,t = x m,t + \u03b4 m,t ,\nwhere\n\u03b4 m,t 2 \u2264 (A (km) ) t mix \u22121 \u2022 x m,t\u2212t mix +1 2 \u2264 \u03ba A \u03c1 t mix \u22121 x m,t\u2212t mix +1 2 ,(39)\nand x m,t \u223c N (0, \u0393\nk) t mix \u22121 ) is independent of \u03b4 m,t . Moreover,(\nx m,t = A (km) x m,t + w m,t = x m,t + A (km) \u03b4 m,t , where x m,t := A (km) x m,t + w m,t .\nWe can rewrite x m,sj (t) = x m,sj (t) + \u03b4 m,sj (t) and x m,sj (t) = x m,sj (t) + A (km) \u03b4 m,sj (t) in a similar manner. Putting this back to (38), one has\nF i,j,\u03c4 = 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 (x m,t ) i x m,t \u2022 (x m,sj (t) ) i x m,sj (t) = 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 ( x m,t + A (km) \u03b4 m,t ) i ( x m,t + \u03b4 m,t ) \u2022 ( x m,sj (t) + A (km) \u03b4 m,sj (t) ) i ( x m,sj (t) + \u03b4 m,sj (t) ) = 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 ( x m,t ) i x m,t \u2022 ( x m,sj (t) ) i x m,sj (t) =: Fi,j,\u03c4 + \u2206 i,j,\u03c4 ,(40)\nwhere \u2206 i,j,\u03c4 contains all the {\u03b4 m,t } terms in the expansion. The key observation here is that, by our definition of index set S \u03c4 , the { x m,t } terms in F i,j,\u03c4 are independent, and thus we can utilize our earlier analysis of the i.i.d. case in Step 1 to study F i,j,\u03c4 .\nStep 2.3: analysis for each term of the decomposition. Towards showing G i \u2248 G i , we prove in the following that F i,j,\u03c4 concentrates around its expectation G i , which in term is close to G i ; moreover, the error term \u2206 i,j,\u03c4 becomes exponentially small as t mix grows. More specifically, we have the following:\n\u2022 Recall the notation Y (k) t mix \u22121 = A (k) \u0393 (k) t mix \u22121 . It holds that E[ F i,j,\u03c4 ] = G i := K k=1 p (k) (Y (k) t mix \u22121 ) i (Y (k) t mix \u22121 ) i .\nAccording to our result (36) for the i.i.d. case, for fixed j, \u03c4 , we have with probability at least 1 \u2212 \u03b4,\n1 \u2264 i \u2264 d, F i,j,\u03c4 \u2212 G i \u0393 2 max d |M subspace | \u2022 |S \u03c4 | log 3 |M subspace | \u2022 |S \u03c4 |d \u03b4 \u0393 2 max t mix d T total,subspace log 3 T total,subspace d \u03b4 . (41\n) \u2022 Recall from (32) that Y (k) \u2212 Y (k) t mix \u22121 \u2264 \u0393 max \u03ba 3 A \u03c1 2(t mix \u22121) . Therefore, (Y (k) ) i (Y (k) ) i \u2212 (Y (k) t mix \u22121 ) i (Y (k) t mix \u22121 ) i \u2264 (Y (k) ) i 2 + (Y (k) t mix \u22121 ) i 2 (Y (k) ) i \u2212 (Y (k) t mix \u22121 ) i 2 \u2264 2 Y (k) + Y (k) \u2212 Y (k) t mix \u22121 Y (k) \u2212 Y (k) t mix \u22121 \u2264 2\u0393 max \u03ba A + \u0393 max \u03ba 3 A \u03c1 2(t mix \u22121) \u2022 \u0393 max \u03ba 3 A \u03c1 2(t mix \u22121) = 2 + \u03ba 2 A \u03c1 2(t mix \u22121) \u2022 \u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) \u2264 3\u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) ,\nwhere we use the mild assumption that t mix \u2265 1 + log \u03ba A 1\u2212\u03c1 , and the fact that\nY (k) , Y (k) t mix \u22121 \u2264 \u0393 max \u03ba A . Consequently, G i \u2212 G i \u2264 K k=1 p (k) (Y (k) ) i (Y (k) ) i \u2212 (Y (k) t mix \u22121 ) i (Y (k) t mix \u22121 ) i \u2264 3\u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) .(42)\n\u2022 By Lemma 1, if t mix 1 1\u2212\u03c1 log(2\u03ba A ), then we have with probability at least 1\u2212\u03b4, all the x m,t 's and x m,t 's involved in the definition of \u2206 i,j,\u03c4 in (40) have 2 norm bounded by \u221a \u0393 max poly(d, \u03ba A , log(T total /\u03b4)). This together with the upper bound on \u03b4 m,t 2 in (39) implies that for all i, j, \u03c4 , it holds that\n\u2206 i,j,\u03c4 \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .(43)\nStep 2.4: putting pieces together. With (41), ( 42) and (43) in place and taking the union bound, we have with probability at least 1 \u2212 \u03b4, for all\n1 \u2264 i \u2264 d, G i \u2212 G i = N j=1 t mix \u03c4 =1 |S \u03c4 | N 2 \u2022 F i,j,\u03c4 \u2212 G i \u2264 max j,\u03c4 F i,j,\u03c4 \u2212 G i + G i \u2212 G i + max j,\u03c4 \u2206 i,j,\u03c4 \u0393 2 max t mix d T total,subspace log 3 T total,subspace d \u03b4 + \u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) + \u0393 2 max \u2022 poly d, \u03ba A , log dT total \u03b4 \u2022 \u03c1 t mix \u22121 \u0393 2 max t mix d T total,subspace log 3 T total d \u03b4 ,\nwhere the last inequality holds if t mix\n1 1\u2212\u03c1 log d\u03ba A T total \u03b4\n. This finishes the proof of Lemma 2.\nProof of (36). Define the truncating operator\nTrunc(x; D) := x \u2022 1(|x| \u2264 D), x \u2208 R, D \u2265 0.\nConsider the following truncated version of G i :\nG Trunc i := 1 M N M m=1 N t=1 Trunc x m,t i ; D 0 x m,t Trunc z m,t i ; D 0 z m,t\n(the truncating level D 0 will be specified later), and let E Trunc \u2022 By a standard covering argument, we have\nG Trunc i \u2212 E Trunc i = sup u,v\u2208S d\u22121 u G Trunc i \u2212 E Trunc i v \u2264 4 sup u,v\u2208N 1/8 u G Trunc i \u2212 E Trunc i v,\nwhere N 1/8 denotes the 1/8-covering of the unit sphere S d\u22121 and has cardinality |N 1/8 | \u2264 32 d . For fixed u, v \u2208 N 1/8 , one has\nu G Trunc i v = 1 M N M m=1 N t=1 Trunc x m,t i ; D 0 u x m,t \u2022 Trunc z m,t i ; D 0 v z m,t ,\nwhere (cf. [Ver18, Chapter 2] for the definitions of subgaussian norm \u2022 \u03c82 and subexponential norm\n\u2022 \u03c81 ) Trunc x m,t i ; D 0 , Trunc z m,t i ; D 0 \u2264 D 0 , u x m,t \u03c82 , v z m,t \u03c82 \u0393 max . Hence Trunc x m,t i ; D 0 u x m,t \u2022 Trunc z m,t i ; D 0 v z m,t \u03c81 D 2 0 \u0393 max ,\nand by Bernstein's inequality [Ver18, Corollary 2.8.3], we have\nP u G Trunc i \u2212 E Trunc i v \u2265 \u03c4 \u2264 2 exp \u2212c 0 M N \u03c4 D 2 0 \u0393 max 2 for all 0 \u2264 \u03c4 \u2264 D 2 0 \u0393 max .\nTaking the union bound over u, v \u2208 N 1/8 , we have with probability at least\n1 \u2212 \u03b4/2, G Trunc i \u2212 E Trunc i D 2 0 \u0393 max d + log 1 \u03b4 M N , provided that M N d + log 1 \u03b4 . \u2022 Note that G i \u2212 E Trunc i = K k=1 p (k) E xt,zt\u223cN (0, \u0393 (k) ) (x t ) i (z t ) i \u2212 Trunc (x t ) i Trunc (z t ) i x t z t ,\nwhere for each k,\nE xt,zt\u223cN (0, \u0393 (k) ) (x t ) i (z t ) i \u2212 Trunc (x t ) i Trunc (z t ) i x t z t = sup u,v\u2208S d\u22121 E xt,zt\u223cN (0, \u0393 (k) ) (x t ) i (z t ) i u x t v z t \u2022 1 \u2212 1 |(x t ) i | \u2264 D 0 , |(z t ) i | \u2264 D 0 \u2264 sup u,v\u2208S d\u22121 E (x t ) i (z t ) i u x t v z t 2 E 1 \u2212 1 |(x t ) i | \u2264 D 0 , |(z t ) i | \u2264 D 0 2 \u0393 2 max P(|N (0, \u0393 max )| > D 0 ) \u0393 2 max exp \u2212c 1 D 2 0 \u0393 max .\nFinally, notice that if the truncating level D 0 is sufficiently large, then we have G i = G Trunc i with high probability. More formally, we have shown that for fixed\n1 \u2264 i \u2264 d, if M N d + log(1/\u03b4), then P G i \u2212 G i D 2 0 \u0393 max d + log 1 \u03b4 M N + \u0393 2 max exp \u2212c 1 D 2 0 \u0393 max \u2265 1 \u2212 \u03b4 2 \u2212 m,t P |(x m,t ) i | > D 0 or |(z m,t ) i | > D 0 .\nIf we pick the truncating level D 0 \u0393 max log(M N/\u03b4), then it is easy to check that with probability at least 1 \u2212 \u03b4,\nG i \u2212 G i D 2 0 \u0393 max d + log 1 \u03b4 M N \u0393 2 max d M N log 3 M N \u03b4 .\nTaking the union bound over 1 \u2264 i \u2264 d finishes our proof of (36). Then we have\n\u039b = U M U = U M U + U \u2206U = K k=1 p (k) U y (k) y (k) U + U \u2206U , \u039b = U M U = K k=1 p (k) U y (k) y (k) U .\nSubstracting these two equations gives\nK k=1 p (k) U y (k) y (k) U \u2212 U y (k) y (k) U = \u039b \u2212 \u039b + U \u2206U .\nTaking the trace of both sides, we get\nK k=1 p (k) U y (k) 2 2 \u2212 U y (k) 2 2 = Tr \u039b \u2212 \u039b + Tr U \u2206U .\nOn the left-hand side,\nU y (k) 2 2 \u2212 U y (k) 2 2 = y (k) 2 2 \u2212 U y (k) 2 2 = y (k) \u2212 U U y (k) 2 2 \u2265 0, while on the right-hand side, Tr(\u039b \u2212\u039b) = K k=1 \u03bb k (\u039b )\u2212\u03bb k (\u039b) (i) \u2264 K \u2206 \u2264 K , Tr(U \u2206U ) \u2264 \u2206 \u2022Tr(U U ) = \u2206 \u2022Tr(I K ) \u2264 K ,\nwhere (i) follows from Weyl's inequality. Putting things together, we have proved (34), which immediately leads to the first upper bound in (35). The second upper bound in (35) follows from a simple application of Davis-Kahan's sin \u0398 theorem [DK70], and the third term is due to [KSS + 20, Lemma A.11]; we skip the details for brevity.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Proof of Theorem 4", "text": "Our proof follows the three steps below: 1. Consider the idealized i.i.d. case, and characterize the expectations and variances of the testing statistics computed by Algorithm 3;\n2. Go back to the actual case of mixed LDSs, and analyze one copy of stat \u0393,g or stat Y,g defined in (14) for some fixed 1 \u2264 g \u2264 G, by decomposing it into an i.i.d. part plus a negligible mixing error term;\nStep 1: the idealized i.i.d. case. Recall the definition of stat Y in (15) when we first introduce our method for clustering. For notational convenience, we drop the subscript in stat Y , and replace x t+1 , z t+1 with x t , z t ; then, with some elementary linear algebra, (15) can be rewritten as\nstat = d i=1 U i 1 |\u2126 1 | t\u2208\u21261 (x t ) i x t \u2212 (z t ) i z t , U i 1 |\u2126 2 | t\u2208\u21262 (x t ) i x t \u2212 (z t ) i z t = U 1 |\u2126 1 | t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) , U 1 |\u2126 2 | t\u2208\u21262 vec x t (x t ) \u2212 z t (z t )\n,\nwhere we define a large orthonormal matrix\nU := \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 U 1 0 . . . 0 0 U 2 . . . . . . . . . . . . . . . 0 0 . . . 0 U d \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u2208 R d 2 \u00d7dK ,(44)\nIn this step, we consider the idealized i.i.d. case:\nt \u2208 \u2126 1 \u222a \u2126 2 , x t i.i.d. \u223c N (0, \u0393 (k) ), w t i.i.d. \u223c N (0, W (k) ), x t = A (k) x t + w t , z t i.i.d. \u223c N (0, \u0393 (l) ), v t i.i.d. \u223c N (0, W (l) ), z t = A (l) z t + v t ,\nwhere \u0393 (k) , \u0393 (l) , W (k) , W (l) are d \u00d7 d covariance matrices, and A (k) , A (l) are d \u00d7 d state transition matrix.\nOur goal is to characterize the expectation and variance of stat in this i.i.d. case. Before we present the results, we need some additional notation. First, let {e i } 1\u2264i\u2264d be the canonical basis of R d , and define\nY (k) := A (k) \u0393 (k) , \u03a3 (k) := A (k) \u2297 I d ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 I d 2 + P ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( A (k) ) \u2297 I d ,\nwhere P \u2208 R d 2 \u00d7d 2 is a symmetric permutation matrix, whose (i, j)-th block is e j e i \u2208 R d\u00d7d , 1 \u2264 i, j \u2264 d. Let Y (l) , \u03a3 ( ) be defined similarly, with A (k) , \u0393 (k) replaced by A (l) , \u0393 (l) . Moreover, define\n\u00b5 k,l := U vec ( Y (k) \u2212 Y (l) ) \u2208 R dK ,(45a)\n\u03a3 k,l := U \u03a3 (k) + W (k) \u2297 \u0393 (k) + \u03a3 ( ) + W (l) \u2297 \u0393 (l) U \u2208 R dK\u00d7dK .(45b)\nNow we are ready to present our results for the i.i.d. case. The first lemma below gives a precise characterization of E[stat] and var(stat), in terms of \u00b5 k,l and \u03a3 k,l ; the second lemma provides some upper and lower bounds, which will be handy for our later analyses. Lemma 4. Denote N = min{|\u2126 1 |, |\u2126 2 |}. For the i.i.d. case just described, one has\nE[stat] = \u00b5 k,l 2 2 , var(stat) \u2264 1 N 2 Tr(\u03a3 2 k,l ) + 2 N \u00b5 k,l \u03a3 k,l \u00b5 k,l ,\nand the inequality becomes an equality if\n|\u2126 1 | = |\u2126 2 | = N .\nLemma 5. Consider the same setting of Lemma 4. Furthermore, suppose that\n\u0393 (k) , \u0393 (l) \u0393 max I d , W (k) , W (l) W max I d , A (k) , A (l) \u2264 \u03ba A\nfor some 0 < W max \u2264 \u0393 max and \u03ba A \u2265 1. Then the following holds true.\n\u2022 (Upper bound on expectation) It holds that\nE[stat] = \u00b5 k,l 2 2 \u2264 Y (k) \u2212 Y (l) 2 F .(46)\n\u2022 (Lower bound on expectation) If Y (k) = Y (l) and subspaces {U i } 1\u2264i\u2264d satisfy\n1 \u2264 i \u2264 d, max ( Y (k) ) i \u2212 U i U i ( Y (k) ) i 2 , ( Y (l) ) i \u2212 U i U i ( Y (l) ) i 2 \u2264 ,(47)\nfor some \u2265 0, then we have\nE[stat] = \u00b5 k,l 2 2 \u2265 Y (k) \u2212 Y (l) 2 F \u2212 4 d i=1 ( Y (k) ) i + ( Y (l) ) i 2 .\n\u2022 (Upper bound on variance) The matrix \u03a3 k,l is symmetric and satisfies\n0 \u03a3 k,l 6\u0393 2 max \u03ba 2 A I dK ;\nthis, together with the earlier upper bound (46) on \u00b5 k,l 2 2 , implies that\nvar(stat) \u2264 1 N 2 Tr(\u03a3 2 k,l ) + 2 N \u00b5 k,l \u03a3 k,l \u00b5 k,l \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N Y (k) \u2212 Y (l) 2 F .\nStep 2: one copy of stat Y,g , stat \u0393,g for a fixed g. Now we turn back to the mixed LDSs setting and prove Theorem 4. Let us focus on the testing of one pair of short trajectories {x m1,t }, {x m2,t } for some m 1 , m 2 \u2208 M clustering , m 1 = m 2 . For notational consistency, in this proof we rewrite these two trajectories as {x t } and {z t }, their labels k m1 , k m2 as k, , and the trajectory length T clustering as T , respectively. Also denote x t := x t+1 and z t := z t+1 . Recall the definition of {stat Y,g } 1\u2264g\u2264G in (14b); for now, we consider one specific element and ignore the subscript g. Recalling the definition of U \u2208 R d 2 \u00d7dK in (44), we have\nstat Y = d i=1 1 N t\u2208\u21261 U i (x t ) i x t \u2212 (z t ) i z t , 1 N t\u2208\u21262 U i (x t ) i x t \u2212 (z t ) i z t = U 1 N t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) , U 1 N t\u2208\u21262 vec x t (x t ) \u2212 z t (z t ) ,\nwhere\nN = T /4G = |\u2126 1 | = |\u2126 2 |.\nIn the following, we show how to decompose stat Y into an i.i.d. term plus a negligible mixing error term, and then analyze each component of this decomposition; finally, we put pieces together to give a characterization of stat Y , or {stat Y,g } 1\u2264g\u2264G when we put the subscript g back in at the end of this step.\nStep 2.1: decomposition of stat Y . Define S 1,\u03c41 := {t 1 +\u03c4 1 +f \u2022t mix : f \u2265 0, \u03c4 1 +f \u2022t mix \u2264 |\u2126 1 |}, where t 1 +1 is the first index of \u2126 1 , and the mixing time t mix will be specified later; define S 2,\u03c42 similarly. Note that for each \u03c4 1 , the elements of S 1,\u03c41 are at least t mix far apart; moreover, we have \u2126 1 = \u222a t mix \u03c41=1 S 1,\u03c41 , \u2126 2 = \u222a t mix \u03c42=1 S 2,\u03c42 , and thus\n1 N t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) = t mix \u03c41=1 |S 1,\u03c41 | N \u2022 1 |S 1,\u03c41 | t\u2208S 1,\u03c4 1 vec x t (x t ) \u2212 z t (z t ) , 1 N t\u2208\u21262 vec x t (x t ) \u2212 z t (z t ) = t mix \u03c42=1 |S 2,\u03c42 | N \u2022 1 |S 2,\u03c42 | t\u2208S 2,\u03c4 2 vec x t (x t ) \u2212 z t (z t ) .\nTherefore, we can rewrite stat Y as a weighted average\nstat Y = t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 stat \u03c41,\u03c42 Y ,where\nw \u03c41,\u03c42 := |S 1,\u03c41 ||S 2,\u03c42 | N 2 , t mix \u03c41=1 t mix \u03c42=1\nw \u03c41,\u03c42 = 1, and\nstat \u03c41,\u03c42 Y := U 1 |S 1,\u03c41 | t\u2208S 1,\u03c4 1 vec x t (x t ) \u2212 z t (z t ) , U 1 |S 2,\u03c42 | t\u2208S 2,\u03c4 2 vec x t (x t ) \u2212 z t (z t )\n. (48)\nWe can further decompose stat \u03c41,\u03c42 Y into an i.i.d. term plus a small error term. To do this, recalling the definition of x m,t in (33) and dropping the subscript m, we have\nx t = A (k) t mix \u22121\nx t\u2212t mix +1 =:\u03b4x,t\n+ x t = \u03b4 x,t + x t , x t = A (k) x t + w t = A (k) \u03b4 x,t + (A (k) x t + w t ) =: x t = A (k) \u03b4 x,t + x t ,\nwhere\nx t \u223c N (0, \u0393(k)\nt mix \u22121 ). Similarly, we rewrite z t = \u03b4 z,t + z t , z t = A ( ) \u03b4 z,t + z t . Plugging these into the right-hand side of (48) and expanding it, one has\nstat \u03c41,\u03c42 Y = stat \u03c41,\u03c42 Y + \u2206 \u03c41,\u03c42 Y , where stat \u03c41,\u03c42 Y := U 1 |S 1,\u03c41 | t\u2208S 1,\u03c4 1 vec x t ( x t ) \u2212 z t ( z t ) , U 1 |S 2,\u03c42 | t\u2208S 2,\u03c4 2 vec x t ( x t ) \u2212 z t ( z t ) ,\nand \u2206 \u03c41,\u03c42 Y involves {\u03b4 x,t , \u03b4 z,t } terms.\nStep 2.2: analysis of each component. First, notice that the { x t , z t } terms in the definition of stat \u03c41,\u03c42 Y are independent; this suggests that we can characterize stat \u03c41,\u03c42 Y by applying our earlier analysis for the i.i.d. case in Step 1. Second, \u2206 \u03c41,\u03c42 Y involves {\u03b4 x,t , \u03b4 z,t } terms, which in turn involve A (k) t mix \u22121 , A ( ) t mix \u22121 and thus will be exponentially small as t mix increases, thanks to Assumption 1. More formally, we have the following:\n\u2022 Applying Lemmas 4 and 5 with (\n\u0393 (k) , \u0393 (l) ) = (\u0393 (k) t mix \u22121 , \u0393 ( ) t mix \u22121 ), ( W (k) , W (l) ) = (W (k) , W ( ) ) and ( A (k) , A (l) ) = (A (k) , A ( ) ), we have E stat \u03c41,\u03c42 Y = U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 , var stat \u03c41,\u03c42 Y \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 2 F ,\nwhere N := min{|S 1,\u03c41 |, |S 2,\u03c42 |} N/t mix .\n\u2022 By Lemma 1, with probability at least 1\u2212\u03b4, all {x t , x t , z t , z t } terms involved in the definition of \u2206 \u03c41,\u03c42 Y has 2 norm bounded by \u221a \u0393 max poly(d, \u03ba A , log(T total /\u03b4)). This implies that\n\u2206 \u03c41,\u03c42 Y \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .\nStep 2.3: putting pieces together. Putting the subscript g back in, we have already shown that\nstat Y,g = t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 stat \u03c41,\u03c42 Y,g = t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 stat \u03c41,\u03c42 Y,g =: stat Y,g + t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 \u2206 \u03c41,\u03c42 Y,g =:\u2206 Y,g = stat Y,g + \u2206 Y,g ,where\nE stat Y,g = U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 , var stat Y,g \u2264 max \u03c41,\u03c42 var( stat \u03c41,\u03c42 Y,g ) \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 2 F , \u2206 Y,g \u2264 max \u03c41,\u03c42 |\u2206 \u03c41,\u03c42 Y,g | \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "So far in", "text": "Step 2, we have focused on the analysis of stat Y,g . We can easily adapt the argument to study stat \u0393,g as well; the major difference is that, in Step 2.2, we should apply Lemmas 4 and 5 with (\n\u0393 (k) , \u0393 (l) ) = (\u0393 (k) t mix \u22121 , \u0393 ( ) t mix \u22121 ), ( W (k) , W (l) ) = (0, 0), ( A (k) , A (l) ) = (I d , I d ), and subspaces {U i } replaced by {V i } instead. The final result is that, for all 1 \u2264 g \u2264 G, stat \u0393,g = stat \u0393,g + \u2206 \u0393,g , where E stat \u0393,g = V vec((\u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 ) ) 2 2 , var stat \u0393,g \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N \u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 2 F , \u2206 \u0393,g \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .\nStep 3: analysis of the medians, and final results. Recall the following standard result on the concentration of medians (or median-of-means in general) [LM19, Theorem 2].\nProposition 1 (Concentration of medians). Let X 1 , . . . X G be i.i.d. random variables with mean \u00b5 and variance \u03c3 2 . Then we have |median{X g , 1 \u2264 g \u2264 G} \u2212 \u00b5| \u2264 2\u03c3 with probability at least 1 \u2212 e \u2212c0G for some constant c 0 .\nNotice that by construction, { stat Y,g } 1\u2264g\u2264G are i.i.d. (and so are { stat \u0393,g } 1\u2264g\u2264G ). Applying Proposition 1 to our case, we know that if G log(1/\u03b4), then with probability at least 1 \u2212 \u03b4, the following holds:\n\u2022 If k = , i.e. the two trajectories are generated by the same LDS model, then\nmedian{stat \u0393,g , 1 \u2264 g \u2264 G} + median{stat Y,g , 1 \u2264 g \u2264 G} \u2264 median{ stat \u0393,g } + median{ stat Y,g } + max g |\u2206 \u0393,g | + max g |\u2206 Y,g | \u2264 2 var( stat \u0393,g ) + 2 var( stat Y,g ) + \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 ;(49)\n\u2022 On the other hand, if k = , then\nmedian{stat \u0393,g , 1 \u2264 g \u2264 G} + median{stat Y,g , 1 \u2264 g \u2264 G} \u2265 median{ stat \u0393,g } + median{ stat Y,g } \u2212 max g |\u2206 \u0393,g | + max g |\u2206 Y,g | \u2265 E[ stat \u0393,g ] + E[ stat Y,g ] \u2212 2 var( stat \u0393,g ) + var( stat Y,g ) \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 \u2265 V vec((\u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 ) ) 2 2 + U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 \u2212 C 0 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 F + Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 F \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 . (50\n)\nWe need to further simplify the result (50) for the k = case. According to (31) and (32), we have\nY (k) \u2212 Y (k) t mix \u22121 F \u2264 \u0393 max \u221a d\u03ba 3 A \u03c1 2(t mix \u22121) =: mix , \u0393 (k) \u2212 \u0393 (k) t mix \u22121 F \u2264 \u0393 max \u221a d\u03ba 2 A \u03c1 2(t mix \u22121) \u2264 mix , which implies that \u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 F \u2264 \u0393 (k) \u2212 \u0393 ( ) F + 2 mix , V vec (\u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 ) 2 2 \u2265 max V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 \u2212 2 mix , 0 2 \u2265 V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 \u2212 4 mix V vec((\u0393 (k) \u2212 \u0393 ( ) ) ) 2 \u2265 V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 \u2212 4 mix \u0393 (k) \u2212 \u0393 ( ) F .\nWe can do a similar analysis for Y (k)\nt mix \u22121 \u2212 Y ( ) t mix \u22121 F and U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 .\nMoreover, we claim (and prove later) that if the subspaces {V i , U i } satisfy the condition (28) in the theorem, then\nV vec((\u0393 (k) \u2212 \u0393 ( ) ) ) 2 2 + U vec((Y (k) \u2212 Y ( ) ) ) 2 2 \u2265 1 2 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F . (51)\nPutting these back to (50), we have for the k = case,\nmedian{stat \u0393,g , 1 \u2264 g \u2264 G} + median{stat Y,g , 1 \u2264 g \u2264 G} \u2265 V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 + U vec (Y (k) \u2212 Y ( ) ) 2 2 \u2212 4 mix \u0393 (k) \u2212 \u0393 ( ) F + Y (k) \u2212 Y ( ) F \u2212 C 0 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) \u2212 \u0393 ( ) F + Y (k) \u2212 Y ( ) F + 4 mix \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 (i) \u2265 1 2 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 0.01 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 C 1 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 (ii) \u2265 0.48 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 C 1 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F ,(52)\nwhere (i) holds if t mix 1 1\u2212\u03c1 log(( \u0393max \u2206 \u0393,Y +2)d\u03ba A ) so that mix \u2264 10 \u22123 \u2206 \u0393,Y , and (ii) holds if t mix\n1 1\u2212\u03c1 log(( \u0393max \u2206 \u0393,Y + 2) d\u03ba A T total \u03b4 ) so that \u0393 2 max \u2022 poly(d, \u03ba A , log T total \u03b4 ) \u2022 \u03c1 t mix \u22121 \u2264 10 \u22123 \u2206 2 \u0393,Y\n. Putting (49) and (52) together, we can finally check that, if it further holds that N N/t mix\n\u0393 2 max \u03ba 2 A \u221a dK \u2206 2 \u0393,Y\n+ 1, then we have with probability at least 1 \u2212 \u03b4,\nmedian{stat \u0393,g } + median{stat Y,g } \u2264 1 8 \u2206 2 \u0393,Y if k = , \u2265 3 8 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2265 3 8 \u2206 2 \u0393,Y if k = .\nThis together with our choice of testing threshold \u03c4 \u2208 [\u2206 2 \u0393,Y /8, 3\u2206 2 \u0393,Y /8] in Algorithm 3 implies correct testing of the two trajectories {x t }, {z t }. Finally, taking the union bound over all pairs of trajectories in M clustering leads to correct pairwise testing, which in turn implies exact clustering of M clustering ; this completes our proof of Theorem (4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3.1 Proof of Lemma 4", "text": "We first assume\n|\u2126 1 | = |\u2126 2 | = N for simplicity. Recall that stat = U 1 |\u2126 1 | t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) =:a , U 1 |\u2126 2 | t\u2208\u21262 vec x t (x t ) \u2212 z t (z t ) =:b = a, b ,\nwhere a, b \u2208 R dK are i.i.d., and\nE[a] = E U vec x t (x t ) \u2212 z t (z t ) = U vec ( Y (k) \u2212 Y (l) ) = \u00b5 k,l .\nTherefore, we have the expectation\nE[stat] = E[a], E[b] = \u00b5 k,l 2 2 .\nIt remains to compute the variance var(stat\n) = E[stat 2 ] \u2212 E[stat] 2\n, where\nE[stat 2 ] = E (a b) 2 = Tr E[bb ]E[aa ] = Tr E[aa ] 2 .(53)\nHere\nE[aa ] = E[a]E[a] + cov(a)\n, and since a is an empirical average of N i.i.d. random vectors, we have\ncov(a) = 1 N cov(f ), where f := U vec x t (x t ) \u2212 z t (z t ) \u2208 R dK .\nFor now, we claim that\ncov(f ) = U (\u03a3 (k) + W (k) \u2297 \u0393 (k) + \u03a3 ( ) + W (l) \u2297 \u0393 (l) )U = \u03a3 k,l ,(54)\nwhich will be proved soon later. Putting these back to (53), one has\nE[aa ] = cov(a) + E[a]E[a] = 1 N \u03a3 k,l + \u00b5 k,l \u00b5 k,l , E[aa ] 2 = 1 N 2 \u03a3 2 k,l + 1 N (\u03a3 k,l \u00b5 k,l \u00b5 k,l + \u00b5 k,l \u00b5 k,l \u03a3 k,l ) + \u00b5 k,l 2 2 \u00b5 k,l \u00b5 k,l ,\nand finally Proof of (54). For notational simplicity, we drop the subscripts t in the definition of f . Then we have f = U vec x(x ) \u2212 z(z ) , and hence\nvar(stat) = E[stat 2 ] \u2212 E[stat] 2 = Tr(E[aa ] 2 ) \u2212 \u00b5 k,l 4 2 = 1 N 2 Tr(\u03a3 2 k,l ) + 2 N \u00b5 k,l \u03a3 k,l \u00b5 k,\ncov(f ) = U cov vec x(x ) \u2212 z(z ) U = U cov vec(x(x ) ) + cov vec(z(z ) ) U ,(55)\nwhere the second equality uses the independence between (x, x ) and (z, z ).\nLet us focus on cov vec(x(x ) ) . For notational simplicity, for now we rewrite A (k) , W (k) , \u0393 (k) as A, W , \u0393. Define y := \u0393 \u22121/2 x \u223c N (0, I d ), and recall that x = Ax + w where w \u223c N (0, W ). Using the fact that vec(ABC) = (C \u2297 A)vec(B) for any matrices of compatible shape, we have\ng := vec x x = vec(xx A ) + vec(xw ) = (A \u2297 I d )vec(xx ) + vec(xw ) = (A \u2297 I d )vec(\u0393 1/2 yy \u0393 1/2 ) + vec(xw ) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )vec(yy ) =:g1 + vec(xw ) =:g2 = g 1 + g 2 .\nNote that\nE[g 2 ] = 0, E[g] = E[g 1 ], and E[g 1 g 2 ] = 0. Hence cov(g) = E[gg ] \u2212 E[g]E[g] = cov(g 1 ) + E[g 2 g 2 ].(56)\nFor the second term on the right-hand side, we have\ng 2 = vec(xw ) = \uf8ee \uf8ef \uf8f0 w 1 x . . . w d x \uf8f9 \uf8fa \uf8fb , E[g 2 g 2 ] = E [w i w j xx ] 1\u2264i,j\u2264d = W \u2297 \u0393;\nas for the first term, we claim (and prove soon later) that cov vec(yy\n) = I d 2 + P ,(57)\nwhich implies that\ncov(g 1 ) = cov (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )vec(yy ) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )cov vec(yy ) (\u0393 1/2 \u2297 \u0393 1/2 )(A \u2297 I d ) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )(I d 2 + P )(\u0393 1/2 \u2297 \u0393 1/2 )(A \u2297 I d ).\nPutting these back to (56), one has\ncov vec(x(x ) ) = cov(g) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )(I d 2 + P )(\u0393 1/2 \u2297 \u0393 1/2 )(A \u2297 I d ) + W \u2297 \u0393, which is equal to \u03a3 (k) + W (k) \u2297 \u0393 (k)\nif we return to the original notation of A (k) , W (k) , \u0393 (k) . By a similar analysis, we can show that cov vec(z(z ) ) = \u03a3 ( ) + W (l) \u2297 \u0393 (l) . Putting these back to (55) finishes our calculation of cov(f ). Finally, it remains to prove (57). Denote\nu := vec(yy ) = \uf8ee \uf8ef \uf8f0 y 1 y . . . y d y \uf8f9 \uf8fa \uf8fb . Then E[u] = vec(I d ), and thus E[u]E[u] = [e i e j ] 1\u2264i,j\u2264d . Next, consider E[uu ] = E [y i y j yy ] 1\u2264i,j\u2264d . \u2022 For i = j, E y 2 i y k y = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 3 if k = = i, 1 if k = = i, 0 if k = ,\nand hence E[y 2 i yy ] = I d + 2e i e i .\n\u2022 For i = j,\nE y i y j y k y = 1 if k = i, = j or k = j, = i, 0 otherwise,\nand hence E[y i y j yy ] = e i e j + e j e i .\nPutting together, the\n(i, j)-th d \u00d7 d block of cov(u) = E[uu ] \u2212 E[u]E[u]\nis equal to I d + e i e i if i = j, and e j e i if i = j. In other words, cov(vec(yy )) = cov(u) = I d 2 + P , where P = [e j e i ] 1\u2264i,j\u2264d is a symmetric permutation matrix; this completes our proof of (57).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3.2 Proof of Lemma 5", "text": "First, it holds that\n\u00b5 k,l 2 2 = d i=1 U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2264 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 = Y (k) \u2212 Y (l) 2 F ,\nwhich gives the upper bound on\nE[stat] = \u00b5 k,l 2 2 .\nFor the lower bound, the triangle inequality tells us that\nU i ( Y (k) ) i \u2212 ( Y (l) ) i 2 = U i U i ( Y (k) ) i \u2212 U i U i ( Y (l) ) i 2 \u2265 max ( Y (k) ) i \u2212 ( Y (l) ) i 2 \u2212 2 , 0 , which implies that U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2265 ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2212 4 ( Y (k) ) i \u2212 ( Y (l) ) i 2 , and hence \u00b5 k,l 2 2 = d i=1 U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2265 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2212 4 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 = Y (k) \u2212 Y (l) 2 F \u2212 4 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 .\nIt remains to upper bound \u03a3 k,l . Recall the definition\n\u03a3 k,l = U \u03a3 (k) + W (k) \u2297 \u0393 (k) + \u03a3 ( ) + W (l) \u2297 \u0393 (l) U .\nWe will utilize the following basic facts: (1) for square matrices A and B with eigenvalues {\u03bb i } and {\u00b5 j } respectively, their Kronecker product A\u2297B has eigenvalues {\u03bb i \u00b5 j };\n(2) For matrices A, B, C, D of compatible shapes, it holds that (A \u2297 B)(C \u2297 D) = (AC) \u2297 (BD). These imply that 0\nW (k) \u2297 \u0393 (k) W (k) \u0393 (k) I d 2 W max \u0393 max I d 2 ,and\n0 \u03a3 (k) = A (k) \u2297 I d ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 I d 2 + P ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( A (k) ) \u2297 I d 2 A (k) \u2297 I d ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( A (k) ) \u2297 I d = 2 A (k) \u2297 I d \u0393 (k) \u2297 \u0393 (k) ( A (k) ) \u2297 I d 2\u0393 2 max A (k) \u2297 I d ( A (k) ) \u2297 I d = 2\u0393 2 max A (k) ( A (k) ) \u2297 I d 2\u0393 2 max \u03ba 2 A I d 2 .\nUsing the conditions W max \u2264 \u0393 max and \u03ba A \u2265 1, we have\n\u03a3 (k) + W (k) \u2297 \u0393 (k) (W max \u0393 max + 2\u0393 2 max \u03ba 2 A )I d 2 3\u0393 2 max \u03ba 2 A I d 2 .\nWe can upper bound \u03a3 ( ) + W (l) \u2297 \u0393 (l) by the same analysis. As a result,\n\u03a3 k,l U (6\u0393 2 max \u03ba 2 A I d 2 )U = 6\u0393 2 max \u03ba 2 A U U = 6\u0393 2 max \u03ba 2\nA I dK , which finishes the proof of Lemma 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3.3 Proof of (51)", "text": "Let be the right-hand side of the condition (28) on the subspaces. Then, applying the second point of Lemma 5 (with\nY (k) = Y (k) , Y (l) = Y ( ) ) tells us that U vec (Y (k) \u2212 Y ( ) ) 2 2 \u2265 Y (k) \u2212 Y ( ) 2 F \u2212 4 d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 \u2265 Y (k) \u2212 Y ( ) 2 F \u2212 4 \u221a d Y (k) \u2212 Y ( ) F ,\nwhere the last line follows from the Cauchy-Schwarz inequality:\nd i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 \u2264 d d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 2 = \u221a d Y (k) \u2212 Y ( ) F .\nWe can lower bound V vec((\u0393 (k) \u2212 \u0393 ( ) ) ) 2 2 similarly. Putting pieces together, we have\nV vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 + U vec (Y (k) \u2212 Y ( ) ) 2 2 \u2265 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 4 \u221a d \u0393 (k) \u2212 \u0393 ( ) F + Y (k) \u2212 Y ( ) F \u2265 1 2 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F ,\nwhere the last inequality is due to the assumption \u2206 \u0393,Y / \u221a d. This completes our proof of (51).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Proof of Theorem 5", "text": "It suffices to prove the error bounds for one specific value of k, and then take the union bound over 1 \u2264 k \u2264 K. For notational convenience, in this proof we rewrite T\n(k) total , A (k) , A (k) , W (k) , W (k)\nas T, A, A, W , W , respectively. We will investigate the close-form solution A, and prepare ourselves with a self-normalized concentration bound; this will be helpful in finally proving the error bounds for A \u2212 A and W \u2212 W .\nStep 1: preparation. Recall the least-squares solution\nA = m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t+1 x m,t m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t x m,t \u22121 .\nUsing the notation\nX := \uf8ee \uf8ef \uf8ef \uf8f0 . . . x m,t . . . \uf8f9 \uf8fa \uf8fa \uf8fb 0\u2264t\u2264Tm\u22121,m\u2208C k \u2208 R T \u00d7d , X + := \uf8ee \uf8ef \uf8ef \uf8f0 . . . x m,t+1 . . . \uf8f9 \uf8fa \uf8fa \uf8fb , N := \uf8ee \uf8ef \uf8ef \uf8f0 . . . w m,t . . . \uf8f9 \uf8fa \uf8fa \uf8fb , we have X + = AX + N , A = X + X(X X) \u22121 = A + N X(X X) \u22121 , namely \u2206 A := A \u2212 A = N X(X X) \u22121 .(58)\nWe will utilize the following matrix form of self-normalized concentration [KKL + 20, Lemma C.4].\nLemma 6 (Self-normalized concentration, matrix form). Consider filtrations {F t }, and random vectors {x t , z t } satisfying x t \u2208 F t\u22121 and z t |F t\u22121 \u223c N (0, I d ). Let V \u2208 R d\u00d7d be a fixed, symmetric, positive definite matrix, and denote V T := T t=1 x t x t + V . Then with probability at least 1 \u2212 \u03b4,\n(V T ) \u22121/2 T t=1 x t z t d + log 1 \u03b4 + log det(V T ) det(V ) .\nStep 2: estimation error of A. Let us rewrite (58) as\n\u2206 A = (X X) \u22121/2 \u2022 (X X) \u22121/2 X N .(59)\nIt is obvious that X X plays a crucial role. Recall from Lemma 1 that with probability at least 1 \u2212 \u03b4, x m,t 2 \u2264 D vec for some D vec \u221a \u0393 max \u2022 poly(d, \u03ba A , log(T total /\u03b4)). Then trivially we have the upper bound\nX X D 2 vec T \u2022 I d =: V up .\nFor a lower bound, we claim (and prove later) that with probability at least 1 \u2212 \u03b4,\nX X 1 5 T \u2022 W =: V lb , provided that T \u03ba 2 w d \u2022 log \u0393 max W min \u03ba A dT total \u03b4 .(60)\nNow we are ready to control A \u2212 A = \u2206 A . From (59), we have\n\u2206 A \u2264 (X X) \u22121/2 \u2022 (X X) \u22121/2 X N .\nFirst, the lower bound (60) on X X tells us that (X X) \u22121/2 1/ T \u2022 \u03bb min (W ). Moreover, applying Lemma 6 with V = V lb , one has with probability at least 1 \u2212 \u03b4,\n(X X) \u22121/2 X N (X X + V ) \u22121/2 X N W 1/2 d + log 1 \u03b4 + log det(V up + V lb ) det(V lb ) W d \u2022 log \u0393 max W min d\u03ba A T total \u03b4 .(61)\nPutting these together, we have\n\u2206 A 1 T \u2022 \u03bb min (W ) \u2022 W d \u2022 log \u0393 max W min d\u03ba A T total \u03b4 d \u2022 \u03ba w T log \u0393 max W min d\u03ba A T total \u03b4 ,\nwhich proves our upper bound for A \u2212 A in the theorem.\nStep 3: estimation error of W . By the definition of w m,t = x m,t+1 \u2212 Ax m,t = w m,t \u2212 \u2206 A x m,t , we have\nN := \uf8ee \uf8ef \uf8ef \uf8f0 . . . w m,t . . . \uf8f9 \uf8fa \uf8fa \uf8fb \u2208 R T \u00d7d , N = N \u2212 \u2206 A X = N \u2212 N X(X X) \u22121 X = N I T \u2212 X(X X) \u22121 X .\nNotice that I T \u2212 X(X X) \u22121 X is a symmetric projection matrix. Therefore,\nW = 1 T N N = 1 T N I T \u2212 X(X X) \u22121 X N ,\nand thus\nW \u2212 W = 1 T N N \u2212 W \u2212 1 T N X(X X) \u22121 X N .(62)\nThe first term on the right-hand side can be controlled by a standard result for covariance estimation (see Proposition 2): with probability at least 1 \u2212 \u03b4,\n1 T N N \u2212 W W d + log 1 \u03b4 T .\nAs for the second term, we rewrite it as\n1 T N X(X X) \u22121 X N = 1 T (X X) \u22121/2 X N (X X) \u22121/2 X N .\nApplying our earlier self-normalized concentration bound (61), we have\n1 T N X(X X) \u22121 X N W d \u2022 log( \u0393max W min d\u03ba A T total \u03b4 ) T .\nPutting these back to (62), we have\nW \u2212 W \u2264 1 T N N \u2212 W + 1 T N X(X X) \u22121 X N W d + log 1 \u03b4 T + W d \u2022 log( \u0393max W min d\u03ba A T total \u03b4 ) T W d \u2022 log( \u0393max W min d\u03ba A T total \u03b4 ) T ,\nwhere the last inequality uses T d \u2022 log( \u0393max\nW min d\u03ba A T total \u03b4\n). This finishes our proof of Theorem 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4.1 Proof of (60)", "text": "We start with the following decomposition:\nX X = m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t x m,t m\u2208C k 1\u2264t\u2264Tm\u22121 x m,t x m,t = m\u2208C k 0\u2264t\u2264Tm\u22122 x m,t+1 x m,t+1 = m\u2208C k 0\u2264t\u2264Tm\u22122 (Ax m,t + w m,t )(Ax m,t + w m,t ) = m\u2208C k 0\u2264t\u2264Tm\u22122\nw m,t w m,t\n:=P + m\u2208C k 0\u2264t\u2264Tm\u22122\nAx m,t x m,t A + Ax m,t w m,t + w m,t x m,t A\n=:Q = P + Q.(63)\nLower bound for P . By Proposition 2, we have with probability at least 1 \u2212 \u03b4,\n1 T \u2212 |C k | P \u2212 W W d + log 1 \u03b4 T \u03bb min (W ) \u2022 I d , provided that T \u03ba 2 w (d + log 1 \u03b4 ).\nAs a result, we have\n1 T \u2212|C k | P 1 2 W , which implies P 1 4 T \u2022 W .\nLower bound for Q. Let N be an -net of the unit sphere S d\u22121 (where the value of 0 < < 1 will be specified later), and let \u03c0 be the projection onto N . Recall the standard result that |N | \u2264 (9/ ) d . Moreover, for any v \u2208 S d\u22121 , denote \u2206 v := v \u2212 \u03c0(v), which satisfies \u2206 v 2 \u2264 . Then we have\n\u03bb min (Q) = inf v\u2208S d\u22121 v Qv = inf v\u2208S d\u22121 (\u03c0(v) + \u2206 v ) Q(\u03c0(v) + \u2206 v ) \u2265 inf v\u2208S d\u22121 \u03c0(v) Q\u03c0(v) \u2212 (2 + 2 ) Q \u2265 inf v\u2208N v Qv \u2212 3 Q . (64\n)\nFor Q , we simply use a crude upper bound, based on the boundedness of { x m,t 2 } (Lemma 1): with probability at least 1 \u2212 \u03b4, one has\nQ \u2264 m\u2208C k 0\u2264t\u2264Tm\u22122 Ax m,t 2 2 + 2 Ax m,t 2 w m,t 2 \u0393 max \u2022 poly \u03ba A , d, T total , 1 \u03b4 . (65\n)\nNext, we lower bound inf v\u2208N v Qv. First, consider a fixed v \u2208 N ; denoting y m,t := Ax m,t and u m,t := W \u22121/2 w m,t \u223c N (0, I d ), we have\nv Qv = m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 2 m\u2208C k 0\u2264t\u2264Tm\u22122 v y m,t \u2022 u m,t W 1/2 v,\nwhere u m,t W 1/2 v \u223c N (0, v W v). Lemma 7 (the scalar version of self-normalized concentration) tells us that, for any fixed \u03bb > 0, with probability at least 1 \u2212 \u03b4,\nm\u2208C k 0\u2264t\u2264Tm\u22122 v y m,t \u2022 u m,t W 1/2 v \u2265 \u2212 \u221a v W v \u03bb 2 m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 1 \u03bb log 1 \u03b4 \u2265 \u2212 W \u03bb 2 m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 1 \u03bb log 1 \u03b4 .\nReplacing \u03b4 with \u03b4/(9/ ) d and taking the union bound, we have with probability at least 1 \u2212 \u03b4, for any\nv \u2208 N , v Qv \u2265 m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 \u2212 W \u03bb m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 2 \u03bb d \u2022 log 9 + log 1 \u03b4 = 1 \u2212 W \u03bb m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 \u2212 W 2 \u03bb d \u2022 log 9 + log 1 \u03b4 .\nWith the choice of \u03bb = 1/ W , this implies\nv Qv \u2265 \u22122 W (d \u2022 log 9 + log 1 \u03b4 ), for all v \u2208 N .(66)\nPutting ( 65) and (66) back to (64), we have with probability at least 1 \u2212 \u03b4,\n\u03bb min (Q) \u2265 inf v\u2208N v Qv \u2212 3 Q \u2265 \u2212C 0 W d \u2022 log 9 + log 1 \u03b4 + \u0393 max \u2022 poly \u03ba A , d, T total , 1 \u03b4\nfor some universal constant C 0 > 0.\nPutting things together. Recall the decomposition X X P + Q in (63). We have already shown that if T \u03ba 2 w (d + log 1 \u03b4 ), then with probability at least 1 \u2212 \u03b4,\nX X P + Q 1 4 T \u2022 W \u2212 C 0 W d \u2022 log 9 + log 1 \u03b4 + \u0393 max \u2022 poly \u03ba A , d, T total , 1 \u03b4 I d .\nIt is easy to check that, if we further choose\n1 poly(\u03ba A , d, T total , 1 \u03b4 , \u0393max W min ) , T \u03ba 2 w d \u2022 log \u0393 max W min \u03ba A dT total \u03b4 ,\nthen we have X X 1 5 T \u2022 W , which finishes the proof of (60).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Proof of Theorem 6", "text": "In this proof, we show the correct classification of one short trajectory {x m,t } 0\u2264t\u2264Tm (with true label k m ) for some m \u2208 M classification ; then it suffices to take the union bound to prove the correct classification of all trajectories in M classification . For notational simplicity, we drop the subscript m and rewrite {x m,t }, T m , k m as {x t }, T , k, respectively. The basic idea of this proof is to show that\nL( A ( ) , W ( ) ) > L( A (k) , W (k) )(67)\nfor any incorrect label = k, where L is the loss function defined in (19) and used by Algorithm 4 for classification. Our proof below is simply a sequence of arguments that finally transform (67) into a sufficient condition in terms of the coarse model errors A , W and short trajectory length T . Before we proceed, we record a few basic facts that will be useful later. First, the assumption W (k) \u2212 W (k) \u2264 W \u2264 0.1W min implies that \u03bb min ( W (k) ) \u2265 0.9\u03bb min (W (k) ), and W (k) is well conditioned with \u03ba( W (k) ) \u03ba(W (k) ) \u2264 \u03ba w . Morever, by Lemma 1, with probability at least 1 \u2212 \u03b4, we have for all 0 \u2264 t \u2264 T ,\n\u2022 In Case 0, x t 2 \u2264 D x \u0393 max (d + log T total \u03b4 ), provided that T 1;\n\u2022 In Case 1, x t 2 \u2264 D x \u03ba A \u0393 max (d + log T total \u03b4 ), provided that T 1 1\u2212\u03c1 log(2\u03ba A ). Now we are ready to state our proof. Throughout our analyses, we will make some intermediate claims, whose proofs will be deferred to the end of this subsection.\nStep 1: a sufficient condition for correct classification. In the following, we prove that for a fixed = k, the condition (67) holds with high probability; at the end of the proof, we simply take the union bound over = k. Using x t+1 = A (k) x t + w t where w t \u223c N (0, W (k) ), we can rewrite the loss function L as\nL(A, W ) = T \u2022 log det(W ) + T \u22121 t=0 w t W \u22121 w t + T \u22121 t=0 x t (A (k) \u2212 A) W \u22121 (A (k) \u2212 A)x t + 2 T \u22121 t=0 w t W \u22121 (A (k) \u2212 A)x t .\nAfter some basic calculation, (67) can be equivalently written as\nL( A ( ) , W ( ) ) \u2212 L( A (k) , W (k) ) = (A) + (B) \u2212 (C) > 0, where (A) := T \u2022 log det( W ( ) ) \u2212 log det( W (k) ) + T \u22121 t=0 w t ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 w t (B) := T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t + 2 T \u22121 t=0 w t ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t (C) := T \u22121 t=0 x t (A (k) \u2212 A (k) ) ( W (k) ) \u22121 (A (k) \u2212 A (k) )x t + 2 T \u22121 t=0 w t ( W (k) ) \u22121 (A (k) \u2212 A (k) )x t\nStep 2: a lower bound for (A) + (B) \u2212 (C). Intuitively, we expect that (A) + (B) should be large because the LDS models (A (k) , W (k) ) and (A ( ) , W ( ) ) are well separated, while (C) should be small if ( A (k) , W (k) ) \u2248 (A (k) , W (k) ). More formally, we claim that the following holds for some universal constants C 1 , C 2 , C 3 > 0:\n\u2022 With probability at least 1 \u2212 \u03b4,\n(A) \u2265 T \u2022 log det( W ( ) ) \u2212 log det( W (k) ) + Tr (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 \u2212 C 1 \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F log 1 \u03b4 ;(68)\n\u2022 With probability at least 1 \u2212 \u03b4,\n(B) \u2265 C 2 T A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 \u03ba w \u03ba w,cross log 1 \u03b4 ,(69)\nprovided that T \u03ba 2 w log 2 (1/\u03b4);\n\u2022 With probability at least 1 \u2212 \u03b4,\n(C) \u2264 C 3 T D 2 x A (k) \u2212 A (k) 2 W min + \u03ba w log 1 \u03b4 ,(70)\nprovided that T 1 (under Case 0) or T 1 1\u2212\u03c1 log(2\u03ba A ) (under Case 1). Putting these together, we have with probability at least 1 \u2212 \u03b4,\n(A) + (B) \u2212 (C) \u2265 C 4 \u2022 T \u2022 log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min \u2212 C 5 \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F + \u03ba w \u03ba w,cross log 1 \u03b4\nfor some universal constants C 4 , C 5 > 0, provided that T \u03ba 2 w log 2 1 \u03b4 (under Case 0), or T \u03ba 2 w log 2 1 \u03b4 + 1 1\u2212\u03c1 log(2\u03ba A ) (under Case 1). Now we have a lower bound of (A) + (B) \u2212 (C) as an order-T term minus a low-order term. Therefore, to show (A) + (B) \u2212 (C) > 0, it suffices to prove that (a) the leading factor of order-T term is positive and large, and (b) the low-order term is negligible compared with the order-T term. More specifically, under the assumption that the coarse models { A (j) , W (j) } satisfy A (j) \u2212 A (j) \u2264 A , W (j) \u2212 W (j) \u2264 W \u2264 0.1W min for all 1 \u2264 j \u2264 K, we make the following claims:\n\u2022 (Order-T term is large.) Define the leading factor D k, of the order-T term and a related parameter D k, as follows:\nD k, := log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min , D k, := 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max + A (k) \u2212 A ( ) 2 F \u2206 2 A,W \u03ba w,cross ,(71)\nwhere the last inequality follows from Assumption 1. They are related in the sense that\nD k, D k, := log det(W ( ) ) det(W (k) ) + Tr W (k) (W ( ) ) \u22121 \u2212 (W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross ,(72)\nwhere D k, is defined in the same way as D k, , except that the coarse models are replaced with the accurate ones; moreover, we have\nD k, D k, , provided that A W min D k, D 2 x , W W min D k, d .(73)\n\u2022 (Low-order term is negligible.) With (73) in place, we have\n(A) + (B) \u2212 (C) \u2265 C 6 T \u2022 D k, \u2212 C 5 \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F + \u03ba w \u03ba w,cross log 1 \u03b4 .(74)\nWe claim that if T \u03ba 5 w,cross\nD k, + 1 log 2 1 \u03b4 , then (A) + (B) \u2212 (C) \u2265 C 7 T \u2022 D k, > 0. (75\n)\nStep 3: putting things together. So far, we have proved that for a short trajectory generated by (A (k) , W (k) ) and for a fixed = k, it holds with probability at least 1\u2212\u03b4 that L( A ( ) , W ( ) ) > L( A (k) , W (k) ), provided that\nA W min D k, D 2 x , W W min \u2022 min 1, D k, d , T \uf8f1 \uf8f2 \uf8f3 \u03ba 2 w + \u03ba 5 w,cross D k, log 2 1 \u03b4 for Case 0, \u03ba 2 w + \u03ba 5 w,cross D k, log 2 1 \u03b4 + 1 1\u2212\u03c1 log(2\u03ba A ) for Case 1.\nPlugging in the relation D k, \u2206 2 A,W /\u03ba w,cross and D x \u0393 max (d + log T total \u03b4 ) (for Case 0) or D x \u03ba A \u0393 max (d + log T total \u03b4 ) (for Case 1), the above conditions become For Case 0:\nA W min \u2206 2 A,W \u0393 max \u03ba w,cross (d + log T total \u03b4 ) , W W min \u2022 min 1, \u2206 A,W \u03ba w,cross d , T \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W log 2 1 \u03b4 ;\nFor Case 1:\nA W min \u2206 2 A,W \u0393 max \u03ba w,cross \u03ba 2 A (d + log T total \u03b4 ) , W W min \u2022 min 1, \u2206 A,W \u03ba w,cross d , T \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W log 2 1 \u03b4 + 1 1 \u2212 \u03c1 log(2\u03ba A ).\nFinally, taking the union bound over all = k as well as over all trajectories in M classification finishes the proof of Theorem 6.\nA.5.1 Proof of (68).\nSince w t i.i.d.\n\u223c N (0, W (k) ), we have\nT \u22121 t=0 w t ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 w t = z M z,\nwhere z \u223c N (0, I T d ), and M \u2208 R T d\u00d7T d is a block-diagonal matrix with Q := (W (k) ) 1/2 (( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 )(W (k) ) 1/2 \u2208 R d\u00d7d as its diagonal blocks. Therefore, by the Hanson-Wright inequality [Ver18, Theorem 6.2.1], we have\nP z M z \u2212 E[z M z] \u2265 u \u2264 2 exp \u2212 c min u 2 M 2 F , u M ,\nwhere\nM 2 F = T Q 2 F , M = Q , and E[z M z] = Tr(M ) = T \u2022 Tr(Q). Choosing u \u221a T Q F log 1 \u03b4 , we have with probability at least 1 \u2212 \u03b4, |z M z \u2212 T \u2022 Tr(Q)| \u2264 C 1 \u221a T Q F log 1\n\u03b4 , which immediately leads to our lower bound (68) for (A).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5.2 Proof of (69).", "text": "Denote u t = (W (k) ) \u22121/2 w t \u223c N (0, I d ) and y t = (W (k) ) 1/2 ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t . Then we have\n(B) = T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t + 2 T \u22121 t=0 u t y t .\nBy Lemma 7, we have with probability at least 1 \u2212 \u03b4,\nT \u22121 t=0 u t y t \u2265 \u2212( \u03bb 2 T \u22121 t=0 y t 2 2 + 1 \u03bb log 2 \u03b4 ) for any fixed \u03bb > 0. This implies that (B) \u2265 T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t \u2212 \u03bb T \u22121 t=0 y t y t \u2212 2 \u03bb log 2 \u03b4 = T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 \u2212 \u03bb \u2022 ( W ( ) ) \u22121 W (k) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t \u2212 2 \u03bb log 2 \u03b4 .\nChoosing \u03bb = 0.05/(\u03ba w \u03ba w,cross ), we have \u03bb \u2022 ( W ( ) ) \u22121 W (k) ( W ( ) ) \u22121 0.1( W ( ) ) \u22121 , and thus (B) \u2265 0.9\nT \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 \u2265 0.9\u03bb min ( W ( ) ) \u22121 T \u22121 t=0 x t (A (k) \u2212 A ( ) ) (A (k) \u2212 A ( ) )x t \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 .(76)\nNow it remains to lower bound T \u22121 t=0 x t \u2206x t , where \u2206 := \u2206 A \u2206 A and \u2206 A := A (k) \u2212 A ( ) . Since \u2206 0, we have\nT \u22121 t=0 x t \u2206x t \u2265 T \u22122 t=0 x t+1 \u2206x t+1 = T \u22122 t=0 (A (k) x t + w t ) \u2206(A (k) x t + w t ) = T \u22122 t=0 w t \u2206w t (i) + T \u22122 t=0 x t A (k) \u2206A (k) x t + 2 T \u22122 t=0 w t \u2206A (k) x t (ii) .(77)\nWe can lower bound (i) by the Hanson-Wright inequality, similar to our previous proof of (68); the result is that, with probability at least 1 \u2212 \u03b4, one has\n(i) \u2265 T \u2022 Tr (W (k) ) 1/2 \u2206(W (k) ) 1/2 \u2212 C 0 \u221a T (W (k) ) 1/2 \u2206(W (k) ) 1/2 F log 1 \u03b4 .\nTo lower bound (ii), we apply Lemma 7, which shows that with probability at least 1 \u2212 \u03b4,\nT \u22122 t=0 w t \u2206A (k) x t \u2264 \u03bb 2 T \u22122 t=0 x t (A (k) ) \u2206W (k) \u2206A (k) x t + 1 \u03bb log 2 \u03b4 , for any fixed \u03bb > 0, hence (ii) \u2265 T \u22122 t=0 x t A (k) (\u2206 \u2212 \u03bb \u2022 \u2206W (k) \u2206)A (k) x t \u2212 2 \u03bb log 2 \u03b4 . Recall \u2206 = \u2206 A \u2206 A , and thus \u2206 \u2212 \u03bb \u2022 \u2206W (k) \u2206 = \u2206 A (I d \u2212 \u03bb \u2022 \u2206 A W (k) \u2206 A )\u2206 A 0 if we choose \u03bb = 1/( W (k) \u2206 A 2 ) = 1/( W (k) \u2206 ); this implies that (ii) \u2265 \u2212 2 \u03bb log 2 \u03b4 = \u22122 W (k) \u2206 log 2 \u03b4 .\nPutting these back to (77), we have\nT \u22121 t=0 x t \u2206x t \u2265 (i) + (ii) \u2265 T \u2022 Tr (W (k) ) 1/2 \u2206(W (k) ) 1/2 \u2212 C 0 \u221a T (W (k) ) 1/2 \u2206(W (k) ) 1/2 F log 1 \u03b4 \u2212 2 W (k) \u2206 log 2 \u03b4 \u2265 T \u2022 \u03bb min (W (k) )Tr(\u2206 A \u2206 A ) \u2212 C 0 \u221a T W (k) \u2206 A \u2206 A F log 1 \u03b4 \u2212 2 W (k) \u2206 A \u2206 A log 2 \u03b4 \u2265 T \u2022 \u03bb min (W (k) ) A (k) \u2212 A ( ) 2 F \u2022 1 \u2212 C 0 \u221a T \u03ba(W (k) ) log 1 \u03b4 \u2212 2 T \u03ba(W (k) ) log 2 \u03b4 \u2265 0.9T \u2022 \u03bb min (W (k) ) A (k) \u2212 A ( ) 2 F ,\nwhere the last inequality holds if T \u03ba 2 w log 2 1 \u03b4 . Going back to (76), we have\n(B) \u2265 0.9\u03bb min (( W ( ) ) \u22121 ) T \u22121 t=0 x t \u2206x t \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 \u2265 0.81T \u2022 \u03bb min (( W ( ) ) \u22121 )\u03bb min (W (k) ) A (k) \u2212 A ( ) 2 F \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 \u2265 0.7T A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 ,\nwhich finishes the proof of (69).\nA.5.3 Proof of (70).\nDenote \u2206 = A (k) \u2212 A (k) , u t = (W (k) ) \u22121/2 w t \u223c N (0, I d ) and y t = (W (k) ) 1/2 ( W (k) ) \u22121 \u2206x t . Then one has (C) = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + 2 T \u22121 t=0 w t ( W (k) ) \u22121 \u2206x t = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + 2 T t=1 u t y t .\nBy Lemma 7, we have with probability at least 1 \u2212 \u03b4,\nT \u22121 t=0 u t y t \u2264 \u03bb 2 T \u22121 t=0 y t 2 2 + 1 \u03bb log 2 \u03b4 for any fixed \u03bb > 0. This implies that (C) \u2264 T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + \u03bb T \u22121 t=0 y t y t + 2 \u03bb log 2 \u03b4 = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + \u03bb T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 W (k) ( W (k) ) \u22121 \u2206x t + 2 \u03bb log 2 \u03b4 = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 + \u03bb \u2022 ( W (k) ) \u22121 W (k) ( W (k) ) \u22121 \u2206x t + 2 \u03bb log 2 \u03b4 \u2264 1.5 1 \u03bb min (W (k) ) + \u03bb \u2022 W (k) \u03bb min (W (k) ) 2 T \u22121 t=0 x t \u2206 \u2206x t + 2 \u03bb log 2 \u03b4 .\nChoosing \u03bb 1/\u03ba w and recalling x t 2 \u2264 D x , we have (C)\n1 W min T \u2022 D 2\nx \u2206 2 + \u03ba w log 1 \u03b4 , which finishes the proof of (70).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5.4 Proof of (72).", "text": "Denote \u2206 := W (k) \u2212 W ( ) . Then the right-hand side of (72) becomes\nD k, = log det W ( ) \u2212 log det W (k) + Tr W (k) (W ( ) ) \u22121 \u2212 I d + A (k) \u2212 A ( ) 2 F \u03ba w,cross = log det W ( ) \u2212 log det(W ( ) + \u2206) + Tr (W ( ) + \u2206)(W ( ) ) \u22121 \u2212 I d + A (k) \u2212 A ( ) 2 F \u03ba w,cross = log det W ( ) \u2212 log det (W ( ) ) 1/2 I d + (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 (W ( ) ) 1/2 + Tr (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 + A (k) \u2212 A ( ) 2 F \u03ba w,cross = Tr(X) \u2212 log det(I d + X) + A (k) \u2212 A ( ) 2 F \u03ba w,cross ,\nwhere we define X := (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 . Notice that X is symmetric and satisfies\nX + I d = (W ( ) ) \u22121/2 W (k) (W ( ) ) \u22121/2 0, X \u2264 (W ( ) ) \u22121/2 2 W (k) \u2212 W ( ) \u2264 2W max W min = 2\u03ba w,cross , X 2 F = (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 2 F \u2265 \u2206 2 F W 2 max .\nTherefore, by Lemma 8, we have\nTr(X) \u2212 log det(I d + X) \u2265 X 2 F 6\u03ba w,cross \u2265 W (k) \u2212 W ( ) 2 F 6\u03ba w,cross W 2 max ,\nand thus\nD k, = Tr(X) \u2212 log det(I d + X) + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2265 W (k) \u2212 W ( ) 2 F 6\u03ba w,cross W 2 max + A (k) \u2212 A ( ) 2 F \u03ba w,cross D k, ,\nwhich finishes the proof of (72).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5.5 Proof of (73).", "text": "Recall the definition\nD k, = log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min .\nFirst, we have\nlog det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 = log det( W ( ) ) det(W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 (W (k) ) \u22121 (i) \u2212 log det( W (k) ) det(W (k) ) + Tr W (k) ( W (k) ) \u22121 \u2212 (W (k) ) \u22121 (ii)\n.\nWe can lower bound (i) by the same idea of our earlier proof for (72), except that we replace W ( ) in that proof with W ( ) ; this gives us\n(i) W (k) \u2212 W ( ) 2 F \u03ba w,cross W 2 max .\nAs for (ii), applying Lemma 8 with\nX = (W (k) ) \u22121/2 ( W (k) \u2212 W (k) )(W (k) ) \u22121/2 , one has (ii) = Tr(X) \u2212 log det(X + I d ) \u2264 X 2 F \u2264 W 2 d W 2 min .\nPutting things together, we have\nD k, = (i) \u2212 (ii) + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min \u2265 C 1 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max + A (k) \u2212 A ( ) 2 F (iii) \u2212 C 2 W 2 d W 2 min + D 2 x A 2 W min (iv)(78)\nIf W , A satisfy (73), then (iv) \u2264 c 0 D k, for some sufficiently small constant c 0 > 0. As for (iii), according to the definition of D k, in (71), there are two possible cases:\n\u2022 If 1 \u03baw,cross W (k) \u2212W ( ) 2 F W 2 max \u2265 D k,\n2 , then it is easy to check that (73) implies that\nW \u221a d W max \u2264 1 4 W (k) \u2212 W ( ) F W max ,\nand hence\n(iii) 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max \u2265 1 \u03ba w,cross W (k) \u2212 W ( ) F W max \u2212 W \u221a d W max 2 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max D k, . \u2022 On the other hand, if 1 \u03baw,cross A (k) \u2212 A ( ) 2 F \u2265 D k, 2\n, then one can check that (73) implies that\nA \u221a d \u2264 1 4 A (k) \u2212 A ( ) F ,\nand hence\n(iii) A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2265 ( A (k) \u2212 A ( ) F \u2212 A \u221a d) 2 \u03ba w,cross A (k) \u2212 A ( ) 2 F \u03ba w,cross D k, .\nIn sum, it is always guaranteed that (iii) D k, . Going back to (78), we claim that D k, \u2265 (iii) \u2212 (iv) D k, as long as W and A satisfy (73), which finishes the proof of (73).\nA.5.6 Proof of (75).\nRecall the lower bound (74) for (A) + (B) \u2212 (C). We want to show that the low-order term is dominated by the order-T term, namely\nT \u2022 D k, . First, if T \u03baw\u03baw,cross D k, log 1 \u03b4 , then \u03ba w \u03ba w,cross log 1 \u03b4 T \u2022 D k, . Next, we have (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F \u2264 W (k) \u2022 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 F = W (k) \u2022 ( W ( ) ) \u22121 ( W (k) \u2212 W ( ) )( W (k) ) \u22121 F \u2264 W (k) \u2022 ( W ( ) ) \u22121 \u2022 ( W (k) ) \u22121 \u2022 W (k) \u2212 W ( ) F + 2 \u221a d W \u2264 2W max W 2 min W (k) \u2212 W ( ) F + 2 \u221a d W = 2\u03ba w,cross W min W (k) \u2212 W ( ) F + 2 \u221a d W .\nNotice that the definition (71) of D k, implies that W (k) \u2212 W ( ) F \u2264 \u03ba w,cross W 2 max D k, , and thus\n(W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F \u03ba w,cross W min \u03ba w,cross W 2 max D k, + \u221a d W . Now it is easy to checked that if T \u03ba 5 w,cross D k, + \u03ba w,cross \u221a d W W min D k, 2 log 2 1 \u03b4 , then \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F log 1 \u03b4 T \u2022 D k, .\nDue to our assumption (73) on W , we have ( \u03b4 , which finishes the proof of (75).\n\u03baw,cross \u221a d W W min D k, ) 2 \u03ba 2 w,cross D k,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Miscellaneous results", "text": "Proposition 2. Consider a t , b t i.i.d.\n\u223c N (0, I d ), 1 \u2264 t \u2264 N , where N d + log(1/\u03b4). Then with probability at least\n1 \u2212 \u03b4, 1 N N t=1 a t a t \u2212 I d d + log 1 \u03b4 N , 1 N N t=1 a t b t d + log 1 \u03b4 N .\nThe proof follows from a standard covering argument (cf. [Ver18]), which we skip for brevity.\nLemma 7 (Self-normalized concentration, scalar version). Suppose that random vectors {u t , y t } 1\u2264t\u2264T and filtrations {F t } 0\u2264t\u2264T \u22121 satisfy F t = \u03c3(u i , 1 \u2264 i \u2264 t), y t \u2208 F t\u22121 , and u t |F t\u22121 \u223c N (0, I d ). Then for any fixed \u03bb > 0, with probability at least 1 \u2212 \u03b4, we have\nT t=1 u t y t < \u03bb 2 T t=1 y t 2 2 + 1 \u03bb log 2 \u03b4 .\nProof. Using basic properties of the Gaussian distribution, we have for all fixed \u03bb \u2208 R,\nE exp T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 = E exp T \u22121 t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2022 E exp \u03bb \u2022 u T y T \u2212 1 2 \u03bb 2 y T 2 2 |F T \u22121 \u22641 \u2264 E exp T \u22121 t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2\n.", "publication_ref": ["b88"], "figure_ref": [], "table_ref": []}, {"heading": "Continuing this expansion leads to the result E[exp(", "text": "T t=1 (\u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2\n))] \u2264 1. Now, letting z = log(2/\u03b4) and using Markov's inequality, we have\nP T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2265 z = P exp T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2265 exp(z) \u2264 exp(\u2212z) \u2022 E exp T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2264 exp(\u2212z) = \u03b4 2 .\nIn other words, with probability at least 1 \u2212 \u03b4/2, we have\nT t=1 (\u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 ) < z = log(2/\u03b4), which implies T t=1 u t y t < \u03bb 2 T t=1 y t 2 2 + 1 \u03bb log 2 \u03b4 if \u03bb > 0.\nBy a similar argument (but with \u03bb replaced by \u2212\u03bb), we have with probability at least 1 \u2212 \u03b4/2,\nT t=1 (\u2212\u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 ) < log(2/\u03b4), which implies T t=1 u t y t > \u2212( \u03bb 2 T t=1 y t 2 2 + 1 \u03bb log 2 \u03b4 ) if \u03bb > 0.\nFinally, taking the union bound finishes the proof of the lemma.\nLemma 8. Consider a symmetric matrix X \u2208 R d\u00d7d that satisfies I d + X 0. If X \u2264 B for some B \u2265 2, then we have\nTr(X) \u2212 log det(I d + X) \u2265 X 2 F 3B .\nOn the other hand, if X \u2212 1 2 I d , then we have\nTr(X) \u2212 log det(I d + X) \u2264 X 2 F .\nProof. Denote {\u03bb i } 1\u2264i\u2264d as the eigenvalues of X, which satisfies\n\u03bb i > \u22121 for all 1 \u2264 i \u2264 d. Then Tr(X) \u2212 log det(I d + X) = d i=1 \u03bb i \u2212 log d i=1 (1 + \u03bb i ) = d i=1 \u03bb i \u2212 log(1 + \u03bb i ) .\nIt can be checked (via elementary calculus) that, for all \u22121 < \u03bb \u2264 B where B \u2265 2, it holds that \u03bb\u2212log(1+\u03bb) \u2265 \u03bb 2 /(3B). Therefore,\nTr(X) \u2212 log det(I d + X) \u2265 d i=1 \u03bb 2 i 3B = X 2 F 3B ,\nwhich completes the proof of our first claim. Similarly, it can be checked that, for all \u03bb \u2265 \u22121/2, one has \u03bb \u2212 log(1 + \u03bb) \u2264 \u03bb 2 , which implies that Tr(X) \u2212 log det(\nI d + X) \u2264 d i=1 \u03bb 2 i = X 2 F ;\nthis completes the proof of our second claim.\nFact 2. In the setting of Section 3.1, it holds that \u0393 max \u2264 W max \u03ba 2 A /(1\u2212\u03c1) and \u2206 \u0393,Y \u2265 \u2206 A,W \u2022W max W min /(4\u03ba 2 A \u0393 max ). Proof. First, consider \u0393 = \u221e i=0 A i W (A i ) , where W \u2264 W max and A i \u2264 \u03ba A \u03c1 i . Then we have \u0393 \u2264\n\u221e i=0 A i 2 W \u2264 W max \u03ba 2 A \u221e i=0 \u03c1 2i \u2264 W max \u03ba 2\nA /(1 \u2212 \u03c1), which proves our upper bound for \u0393 max . Next, let us turn to \u2206 \u0393,Y . Our proof below can be viewed as a quantitative version of the earlier proof for Fact 1. We will show that, if the autocovariance matrices between the k-th and the -th models are close (in Frobenius norm), then the models themselves should also be close; our lower bound for \u2206 \u0393,Y in terms of \u2206 A,W then follows from contraposition.\nConsider two LDS models (A (k) , W (k) ) = (A ( ) , W ( ) ) and their autocovariance matrics (\u0393 (k) , Y (k) ), (\u0393 ( ) , Y ( ) ). Recall from (7) that A (k) = Y (k) \u0393 (k) \u22121 and W (k) = \u0393 (k) \u2212 A (k) \u0393 (k) A (k) = \u0393 (k) \u2212 A (k) Y (k) ; A ( ) and W ( ) can be expressed similarly.\n\u2022 First, regarding A (k) \u2212 A ( ) , one has\nA (k) \u2212 A ( ) = Y (k) \u0393 (k) \u22121 \u2212 Y ( ) \u0393 ( ) \u22121 = (Y (k) \u2212 Y ( ) )\u0393 (k) \u22121 + Y ( ) (\u0393 (k) \u22121 \u2212 \u0393 ( ) \u22121 ),\nwhere Y ( ) (\u0393 (k) \u22121 \u2212 \u0393 ( ) \u22121 ) = Y ( ) \u0393 ( ) \u22121 (\u0393 ( ) \u2212 \u0393 (k) )\u0393 (k) \u22121 = A ( ) (\u0393 ( ) \u2212 \u0393 (k) )\u0393 (k) \u22121 .\nTherefore, we have\nA (k) \u2212 A ( ) F \u2264 Y (k) \u2212 Y ( ) F \u0393 (k) \u22121 + A ( ) \u0393 ( ) \u2212 \u0393 (k) F \u0393 (k) \u22121 \u2264 1 W min Y (k) \u2212 Y ( ) F + \u03ba A W min \u0393 ( ) \u2212 \u0393 (k) F ,(79)\nwhere the last line follows from \u0393 (k) W (k) W min I d and A ( ) \u2264 \u03ba A \u03c1 \u2264 \u03ba A .\n\u2022 Next, we turn to the analysis of W (k) \u2212 W ( ) , which satisfies\nW (k) \u2212 W ( ) = (\u0393 (k) \u2212 \u0393 ( ) ) \u2212 (A (k) Y (k) \u2212 A ( ) Y ( ) ), W (k) \u2212 W ( ) F \u2264 \u0393 (k) \u2212 \u0393 ( ) F + A (k) Y (k) \u2212 A ( ) Y ( ) F .\nNotice that\nA (k) Y (k) \u2212 A ( ) Y ( ) F = A (k) (Y (k) \u2212 Y ( ) ) + (A (k) \u2212 A ( ) )Y ( ) F \u2264 A (k) (Y (k) \u2212 Y ( ) ) F + (A (k) \u2212 A ( ) )Y ( ) F \u2264 \u03ba A Y (k) \u2212 Y ( ) F + \u03ba A \u0393 max A (k) \u2212 A ( ) F ,\nwhere the last line is due to Y ( ) = A ( ) \u0393 ( ) \u2264 A ( ) \u2022 \u0393 ( ) \u2264 \u03ba A \u0393 max . Therefore, we have\nW (k) \u2212 W ( ) F \u2264 \u0393 (k) \u2212 \u0393 ( ) F + \u03ba A Y (k) \u2212 Y ( ) F + \u03ba A \u0393 max A (k) \u2212 A ( ) F .(80)\nFor notational simplify, denote \u2206 A := A (k) \u2212 A ( ) , \u2206 W := W (k) \u2212 W ( ) , \u2206 \u0393 := \u0393 (k) \u2212 \u0393 ( ) , \u2206 Y := Y (k) \u2212 Y ( ) . From (80), one has\n\u2206 A 2 F + \u2206 W 2 F W 2 max \u2264 \u2206 A 2 F + 3 W 2 max \u2206 \u0393 2 F + \u03ba 2 A \u2206 Y 2 F + \u03ba 2 A \u0393 2 max \u2206 A 2 F \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 1 + 3\u03ba 2 A \u0393 2 max W 2 max \u2206 A 2 F \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 4\u03ba 2 A \u0393 2 max W 2 max \u2206 A 2 F ,\nwhere the last line follows from \u03ba 2 A \u0393 2 max /W 2 max \u2265 1. Morever, (79) tells us that \u2206 A\n2 F \u2264 2 W 2 min \u2206 Y 2 F + 2\u03ba 2 A W 2 min \u2206 \u0393 2 F\n. Putting together, we have\n\u2206 A 2 F + \u2206 W 2 F W 2 max \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 4\u03ba 2 A \u0393 2 max W 2 max \u2206 A 2 F \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 4\u03ba 2 A \u0393 2 max W 2 max 2 W 2 min \u2206 Y 2 F + 2\u03ba 2 A W 2 min \u2206 \u0393 2 F = 3 W 2 max + 4\u03ba 2 A \u0393 2 max W 2 max 2\u03ba 2 A W 2 min \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max + 4\u03ba 2 A \u0393 2 max W 2 max 2 W 2 min \u2206 Y 2 F \u2264 11\u03ba 4 A \u0393 2 max W 2 max W 2 min \u2206 \u0393 2 F + \u2206 Y 2 F .\nIn sum, we have just shown that, if \u2206 \u0393\n2 F + \u2206 Y 2 F < \u2206 2 \u0393,Y , then \u2206 A 2 F + \u2206 W 2 F W 2 max < 11\u03ba 4 A \u0393 2 max W 2 max W 2 min \u2206 2 \u0393,Y . Equivalently (by contraposition), if \u2206 A 2 F + \u2206 W 2 F W 2 max \u2265 \u2206 2 A,W , then \u2206 \u0393 2 F + \u2206 Y 2 F \u2265 W 2 max W 2 min 11\u03ba 4 A \u0393 2 max \u2206 2 A,W\n. This proves our lower bound for \u2206 \u0393,Y in terms of \u2206 A,W . Example 1. It has been known that in our Case 1 (i.e. a single continuous trajectory), the quick switching of multiple LDS models may lead to exponentially large states, even if each individual model is stable [Lib03]. We give a quick example for completeness. Consider ", "publication_ref": ["b46"], "figure_ref": [], "table_ref": []}, {"heading": "C Extensions of Algorithm 1", "text": "Different trajectory lengths. Recall that in Section 2, we assume that all short trajectories within each subset of data M o have the same length T m = T o . If this is not the case, we can easily modify our algorithms in the following ways:\n\u2022 For subspace estimation, the easiest way to handle different T m 's is to simply truncate the trajectories in M subspace so that they have the same length T subspace = min m\u2208M subspace T m , and then apply Algorithm 2 without modification. However, this might waste many samples when some trajectories of M subspace are much longer than others; one way to resolve this is to manually divide the longer trajectories into shorter segments of comparable lengths, before doing truncation. A more refined method is to modify Algorithm 2 itself, by re-defining the index sets \u2126 1 , \u2126 2 separately for each trajectory; moreover, in the definition of H i and G i , one might consider assigning larger weights to longer trajectories, instead of using the uniform weight 1/|M subspace |.\n\u2022 For clustering (or pairwise testing) of M clustering , we can handle various T m 's similarly, by either truncating each pair of trajectories to the same length, or modifying Algorithm 3 itself (via re-defining the index sets {\u2126 g,1 , \u2126 g,2 } 1\u2264g\u2264G separately for each trajectory).\n\u2022 Our methods for model estimation and classification, namely Algorithms 4 and 5, are already adaptive to different T m 's in M clustering and M classification , and hence need no modification.\nUnknown parameters. Next, we show how to handle the case when certain parameters are unknown to the algorithms:\n\u2022 In Algorithm 2, we set the dimension of the output subspaces {V i , U i } to be K (the number of models).\nIf K is unknown, we might instead examine the eigenvalues of H i + H i and G i + G i , and pick the subspace dimension that covers most of the energy in the eigenvalues.\n\u2022 In Algorithm 3, we need to know the separation parameter \u2206 \u0393,Y (in order to choose the testing threshold \u03c4 appropriately) and the number of models K (for clustering). If either \u2206 \u0393,Y or K is unknown, we might instead try different values of threshold \u03c4 , and pick the one that (after permutation) makes the similarity matrix S block-diagonal with as few blocks as possible.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Additional experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Synthetic experiments: clustering and classification", "text": "First, we take a closer look at the performance of our clustering method (Algorithm 3) through synthetic experiments. We set the parameters d = 40, K = 2, \u03c1 = 0.5, \u03b4 = 0.12. The LDS models are generated by A (k) = (\u03c1 \u00b1 \u03b4)R and W (k) = I d , where R is a random orthogonal matrix. We let |M clustering | = 5d, and vary T clustering \u2208 [10, 60]. We run our clustering method on the dataset M clustering , either with or without the assistance of subspace estimation (Algorithm 2) and dimensionality reduction. For the former case, we use the same dataset M clustering for subspace estimation, without sample splitting, which is closer to practice; for the latter, we simply replace the subspaces {V i , U i } with I d . The numerical results are illustrated in Figure 3 (left), confirming that (1) in both cases, the clustering error decreases as T clustering increases, and\n(2) subspace estimation and dimensionality reduction significantly improve the clustering accuracy. Next, we examine the performance of our classification method (Algorithm 5) in the same setting as above. We first obtain a coarse model estimation by running Stage 1 of Algorithm 1 on the dataset M clustering , with |M clustering | = 10d and T clustering = 30. Then, we run classification on the dataset M classification , with varying T classification \u2208 [4, 50]. The numerical results are included in Figure 3 (right), showing that the classification error rapidly decreases to zero as T classification grows.", "publication_ref": [], "figure_ref": ["fig_8", "fig_8"], "table_ref": []}, {"heading": "D.2 Real-data experiments: MotionSense", "text": "To show the practical relevance of the proposed algorithms, we work with the MotionSense dataset [MCCH19]. This datasets consists of multivariate time series of dimension d = 12, collected (at a rate of 50Hz) by accelerometer and gyroscope sensors on a mobile phone while a person performs various activities, such as \"jogging\", \"walking\", \"sitting\", and so on. In our experiments, we break the data into 8-second short trajectories, and treat the human activities as latent variables. Figure 4 (left) illustrates what the data looks like. Notice that the time series do not satisfy the mixing property assumed in our theory, but are rather periodic instead.\nAs a preliminary attempt to apply our algorithms in the real world, we show that the proposed clustering method (which is one of the most crucial step in our overall approach), without any modification, works reasonably well even for this dataset. To be concrete, we apply Algorithm 3 (without dimensionality reduction, i.e. {V i , U i } are set to I d ) to a mixture of 12 \"jogging\" and 12 \"walking\" trajectories. Figure 4 (right) shows the resulted distance matrix, which is defined in the same way as Line 11 of Algorithm 3, but without thresholding. Its clear block structure confirms that, with an appropriate choice of threshold \u03c4 , Algorithm 3 will return an accurate/exact clustering of the mixed trajectories. These results are strong indication that the proposed algorithms in this work might generalize to much broader settings than what our current theory suggests, and we hope that this will inspire further extensions and applications of the proposed methods.\nFigure 4: Left: examples of \"jogging\" and \"walking\" trajectories from the MotionSense dataset. Right: the distance matrix constructed by Algorithm 3 for 12 \"jogging\" and 12 \"walking\" trajectories.", "publication_ref": ["b52"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Y. Chen is supported in part by the ARO grant W911NF-20-1-0097, the NSF grants CCF-1907661 and  IIS-1900140, and the AFOSR grant FA9550-19-1-0030. H. V. Poor is supported in part by the NSF under Grant CCF-1908308. We would like to thank Yuxin Chen and Gen Li for numerous helpful discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep explicit duration switching models for time series", "journal": "", "year": "2021", "authors": "] A F + 21; K Ansari; R Benidis; A C Kurle; H Turkmen; A J Soh; B Smola; T Wang;  Januschowski"}, {"ref_id": "b1", "title": "A two-state markov mixture model for a time series of epileptic seizure counts", "journal": "Biometrics", "year": "1991", "authors": "P S Albert"}, {"ref_id": "b2", "title": "Time-series clustering -a decade review", "journal": "Information Systems", "year": "2015", "authors": "S Aghabozorgi; A S Shirkhorshidi; T Y Wah"}, {"ref_id": "b3", "title": "A model of inductive bias learning", "journal": "Journal of Artificial Intelligence Research", "year": "2000", "authors": "J Baxter"}, {"ref_id": "b4", "title": "Sample complexity of multi-task reinforcement learning", "journal": "AUAI Press", "year": "2013", "authors": "E Brunskill; L Li"}, {"ref_id": "b5", "title": "Provably efficient learning with typed parametric models", "journal": "Journal of Machine Learning Research", "year": "2009", "authors": "E Brunskill; B R Leffler; L Li; M L Littman; N Roy"}, {"ref_id": "b6", "title": "Computation of weighted sums of rewards for concurrent mdps", "journal": "Mathematical Methods of Operations Research", "year": "2019", "authors": "P Buchholz; D Scheftelowitsch"}, {"ref_id": "b7", "title": "Clustering vector autoregressive models: Capturing qualitative differences in within-person dynamics", "journal": "Frontiers in Psychology", "year": "2016", "authors": "K Bulteel; F Tuerlinckx; A Brose; E Ceulemans"}, {"ref_id": "b8", "title": "Spectral methods for data science: A statistical perspective. Foundations and Trends\u00ae in Machine Learning", "journal": "", "year": "2021", "authors": "] Y + 21; Y Chen; J Chi; C Fan;  Ma"}, {"ref_id": "b9", "title": "Online linear quadratic control", "journal": "PMLR", "year": "2018", "authors": "] A Chk + 18; A Cohen; T Hasidim; N Koren; Y Lazic; K Mansour;  Talwar"}, {"ref_id": "b10", "title": "Kalman filtering with adversarial corruptions", "journal": "", "year": "2021", "authors": "S Chen; F Koehler; A Moitra; M Yau"}, {"ref_id": "b11", "title": "Learning mixtures of linear regressions in subexponential time via fourier moments", "journal": "", "year": "2020", "authors": "S Chen; J Li; Z Song"}, {"ref_id": "b12", "title": "Learning mixtures of low-rank models", "journal": "IEEE Transactions on Information Theory", "year": "2021", "authors": "Y Chen; C Ma; H V Poor; Y Chen"}, {"ref_id": "b13", "title": "Convex and nonconvex formulations for mixed regression with two components: Minimax optimal rates", "journal": "IEEE Transactions on Information Theory", "year": "2017", "authors": "Y Chen; X Yi; C Caramanis"}, {"ref_id": "b14", "title": "Time series clustering by a robust autoregressive metric with application to air pollution", "journal": "", "year": "2015", "authors": "P Urso; L De Giovanni; R Massari"}, {"ref_id": "b15", "title": "A Wasserstein minimax framework for mixed linear regression", "journal": "PMLR", "year": "2021-07", "authors": "T Diamandis; Y Eldar; A Fallah; F Farnia; A Ozdaglar"}, {"ref_id": "b16", "title": "Few-shot learning via learning the representation, provably", "journal": "", "year": "2021", "authors": "S S Du; W Hu; S M Kakade; J D Lee; Q Lei"}, {"ref_id": "b17", "title": "The rotation of eigenvectors by a perturbation", "journal": "iii. SIAM Journal on Numerical Analysis", "year": "1970", "authors": "C Davis; W M Kahan"}, {"ref_id": "b18", "title": "Small covers for near-zero sets of polynomials and learning latent variable models", "journal": "IEEE", "year": "2020", "authors": "I Diakonikolas; D M Kane"}, {"ref_id": "b19", "title": "On the sample complexity of the linear quadratic regulator", "journal": "Foundations of Computational Mathematics", "year": "2020", "authors": "] S + 20; H Dean; N Mania; B Matni; S Recht;  Tu"}, {"ref_id": "b20", "title": "A q-learning algorithm for discrete-time linear-quadratic control with random parameters of unknown distribution: convergence and stabilization", "journal": "", "year": "2020", "authors": "K Du; Q Meng; F Zhang"}, {"ref_id": "b21", "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "journal": "", "year": "2017", "authors": "C Finn; P Abbeel; S Levine"}, {"ref_id": "b22", "title": "Global convergence of policy gradient methods for the linear quadratic regulator", "journal": "PMLR", "year": "2018", "authors": "M Fazel; R Ge; S Kakade; M Mesbahi"}, {"ref_id": "b23", "title": "Learning nonlinear dynamical systems from a single trajectory", "journal": "PMLR", "year": "2020", "authors": "D Foster; T Sarkar; A Rakhlin"}, {"ref_id": "b24", "title": "Finite time identification in unstable linear systems", "journal": "Automatica", "year": "2018", "authors": "M K S Faradonbeh; A Tewari; G Michailidis"}, {"ref_id": "b25", "title": "On adaptive linear-quadratic regulators", "journal": "Automatica", "year": "2020", "authors": "M K S Faradonbeh; A Tewari; G Michailidis"}, {"ref_id": "b26", "title": "Variational learning for switching state-space models", "journal": "Neural computation", "year": "2000", "authors": "Z Ghahramani; G E Hinton"}, {"ref_id": "b27", "title": "Contextual markov decision processes", "journal": "", "year": "2015", "authors": "A Hallak; D Di Castro; S Mannor"}, {"ref_id": "b28", "title": "Adarl: What, where, and how to adapt in transfer reinforcement learning", "journal": "", "year": "2021", "authors": "] B Hfl + 21; F Huang; C Feng; S Lu; K Magliacane;  Zhang"}, {"ref_id": "b29", "title": "Latent bandits revisited", "journal": "Curran Associates, Inc", "year": "2020", "authors": "] J Hkz + 20; B Hong; M Kveton; Y Zaheer; A Chow; C Ahmed;  Boutilier"}, {"ref_id": "b30", "title": "Continuous meta-learning without tasks", "journal": "", "year": "2020", "authors": "J Harrison; A Sharma; C Finn; M Pavone"}, {"ref_id": "b31", "title": "Coresets for time series clustering", "journal": "", "year": "2021", "authors": "L Huang; K Sudhir; N Vishnoi"}, {"ref_id": "b32", "title": "Toeplitz inverse covariance-based clustering of multivariate time series data", "journal": "", "year": "2017", "authors": "D Hallac; S Vare; S Boyd; J Leskovec"}, {"ref_id": "b33", "title": "Sample complexity lower bounds for linear system identification", "journal": "IEEE", "year": "2019", "authors": "Y Jedra; A Proutiere"}, {"ref_id": "b34", "title": "Robust and Optimal Control", "journal": "Prentice Hall", "year": "1996", "authors": "I S Khalil; J Doyle; K Glover"}, {"ref_id": "b35", "title": "Rl for latent MDPs: Regret guarantees and a lower bound", "journal": "", "year": "2021", "authors": "J Kwon; Y Efroni; C Caramanis; S Mannor"}, {"ref_id": "b36", "title": "On the minimax optimality of the em algorithm for learning two-component mixed linear regression", "journal": "PMLR", "year": "2021", "authors": "J Kwon; N Ho; C Caramanis"}, {"ref_id": "b37", "title": "Information theoretic regret bounds for online nonlinear control", "journal": "Curran Associates, Inc", "year": "2020", "authors": "] S Kkl + 20; A Kakade; K Krishnamurthy; M Lowrey; W Ohnishi;  Sun"}, {"ref_id": "b38", "title": "Gaussian mixture vector autoregression", "journal": "Journal of Econometrics", "year": "2016", "authors": "L Kalliovirta; M Meitz; P Saikkonen"}, {"ref_id": "b39", "title": "Linear Estimation", "journal": "Prentice Hall", "year": "2000", "authors": "T Kailath; A H Sayed; B Hassibi"}, {"ref_id": "b40", "title": "Robust meta-learning for mixed linear regression with small batches", "journal": "Curran Associates, Inc", "year": "2020", "authors": "W Kong; R Somani; S Kakade; S Oh"}, {"ref_id": "b41", "title": "Meta-learning for mixed linear regression", "journal": "PMLR", "year": "2020", "authors": "] W Kss + 20; R Kong; Z Somani; S Song; S Kakade;  Oh"}, {"ref_id": "b42", "title": "Logarithmic regret bound in partially observable linear dynamical systems", "journal": "Curran Associates, Inc", "year": "2020", "authors": "S Lale; K Azizzadenesheli; B Hassibi; A Anandkumar"}, {"ref_id": "b43", "title": "Bayesian multi-task reinforcement learning", "journal": "Omnipress", "year": "2010", "authors": "A Lazaric; M Ghavamzadeh"}, {"ref_id": "b44", "title": "Pac continuous state online multitask reinforcement learning with identification", "journal": "", "year": "2016", "authors": "Y Liu; Z Guo; E Brunskill"}, {"ref_id": "b45", "title": "Clustering of time series data -a survey", "journal": "Pattern Recognition", "year": "2005", "authors": "T W Liao"}, {"ref_id": "b46", "title": "Switching in Systems and Control", "journal": "Springer Science & Business Media", "year": "2003", "authors": "D Liberzon"}, {"ref_id": "b47", "title": "Bayesian learning and inference in recurrent switching linear dynamical systems", "journal": "In Artificial Intelligence and Statistics", "year": "2017", "authors": "] S Ljm + 17; M Linderman; A Johnson; R Miller; D Adams; L Blei;  Paninski"}, {"ref_id": "b48", "title": "System identification", "journal": "Springer", "year": "1998", "authors": "L Ljung"}, {"ref_id": "b49", "title": "Learning mixtures of linear regressions with nearly optimal complexity", "journal": "", "year": "2018", "authors": "Y Li; Y Liang"}, {"ref_id": "b50", "title": "Mean estimation and regression under heavy-tailed distributions: A survey", "journal": "Foundations of Computational Mathematics", "year": "2019", "authors": "G Lugosi; S Mendelson"}, {"ref_id": "b51", "title": "Cluster of time series", "journal": "Journal of Classification", "year": "2000", "authors": "E A Maharaj"}, {"ref_id": "b52", "title": "Mobile sensor data anonymization", "journal": "ACM", "year": "2019", "authors": "M Malekzadeh; R G Clegg; A Cavallaro; H Haddadi"}, {"ref_id": "b53", "title": "Learning the linear quadratic regulator from nonlinear observations", "journal": "", "year": "2020", "authors": "Z Mhammedi; D J Foster; M Simchowitz; D Misra; W Sun; A Krishnamurthy; A Rakhlin; J Langford"}, {"ref_id": "b54", "title": "Joint learning of linear timeinvariant dynamical systems", "journal": "", "year": "2021", "authors": "A Modi; M K S Faradonbeh; A Tewari; G Michailidis"}, {"ref_id": "b55", "title": "Online control of unknown time-varying dynamical systems", "journal": "", "year": "2021", "authors": "E Minasyan; P Gradu; M Simchowitz; E Hazan"}, {"ref_id": "b56", "title": "Active learning for nonlinear system identification with guarantees", "journal": "", "year": "2020", "authors": "H Mania; M I Jordan; B Recht"}, {"ref_id": "b57", "title": "Latent bandits", "journal": "PMLR", "year": "2014", "authors": "O.-A Maillard; S Mannor"}, {"ref_id": "b58", "title": "Derivativefree methods for policy optimization: Guarantees for linear quadratic systems", "journal": "PMLR", "year": "2019", "authors": "] D Mpb + 19; A Malik; K Pananjady; K Bhatia; P Khamaru; M Bartlett;  Wainwright"}, {"ref_id": "b59", "title": "The benefit of multitask representation learning", "journal": "The Journal of Machine Learning Research", "year": "2016", "authors": "A Maurer; M Pontil; B Romera-Paredes"}, {"ref_id": "b60", "title": "Statistical analysis of economic time series via markov switching models", "journal": "Journal of Time Series Analysis", "year": "1994", "authors": "R E Mcculloch; R S Tsay"}, {"ref_id": "b61", "title": "Certainty equivalence is efficient for linear quadratic control", "journal": "", "year": "2019", "authors": "H Mania; S Tu; B Recht"}, {"ref_id": "b62", "title": "Cluster analysis of resting-state fmri time series", "journal": "Neuroimage", "year": "2009", "authors": "] A Myp + 09; Y Mezer; O Yovel; T Pasternak; Y Gorfine;  Assaf"}, {"ref_id": "b63", "title": "Non-asymptotic identification of LTI systems from a single trajectory", "journal": "IEEE", "year": "2019", "authors": "S Oymak; N Ozay"}, {"ref_id": "b64", "title": "Recovery of sparse signals from a mixture of linear samples", "journal": "PMLR", "year": "2020", "authors": "S Pal; A Mazumdar"}, {"ref_id": "b65", "title": "Cluster-and-conquer: A framework for time-series forecasting", "journal": "", "year": "2021", "authors": "R Pathak; R Sen; N Rao; N B Erichson; M I Jordan; I S Dhillon"}, {"ref_id": "b66", "title": "A survey on transfer learning", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2009", "authors": "S J Pan; Q Yang"}, {"ref_id": "b67", "title": "Estimating mixtures of normal distributions and switching regressions", "journal": "Journal of the American Statistical Association", "year": "1978", "authors": "R E Quandt; J B Ramsey"}, {"ref_id": "b68", "title": "Stable online control of linear time-varying systems", "journal": "PMLR", "year": "2021", "authors": "G Qu; Y Shi; S Lale; A Anandkumar; A Wierman"}, {"ref_id": "b69", "title": "Learning linear dynamical systems with semiparametric least squares", "journal": "PMLR", "year": "2019", "authors": "M Simchowitz; R Boczar; B Recht"}, {"ref_id": "b70", "title": "Naive exploration is optimal for online lqr", "journal": "PMLR", "year": "2020", "authors": "M Simchowitz; D Foster"}, {"ref_id": "b71", "title": "Machine learning in event-triggered control: Recent advances and open issues", "journal": "", "year": "2020", "authors": "] L Siw + 20; Z Sedghi; K Ijaz; D Witheephanich;  Pesch"}, {"ref_id": "b72", "title": "Multi-model markov decision processes", "journal": "IISE Transactions", "year": "2021", "authors": "L N Steimle; D L Kaufman; B T Denton"}, {"ref_id": "b73", "title": "A survey on markovian jump systems: modeling and design", "journal": "International Journal of Control, Automation and Systems", "year": "2015", "authors": "P Shi; F Li"}, {"ref_id": "b74", "title": "Learning without mixing: Towards a sharp analysis of linear system identification", "journal": "PMLR", "year": "2018", "authors": "M Simchowitz; H Mania; S Tu; M I Jordan; B Recht"}, {"ref_id": "b75", "title": "Finite sample system identification: Optimal rates and the role of regularization", "journal": "", "year": "2020-06", "authors": "Y Sun; S Oymak; M Fazel"}, {"ref_id": "b76", "title": "Near optimal finite time identification of arbitrary linear dynamical systems", "journal": "PMLR", "year": "2019", "authors": "T Sarkar; A Rakhlin"}, {"ref_id": "b77", "title": "Nonparametric system identification of stochastic switched linear systems", "journal": "IEEE", "year": "2019", "authors": "T Sarkar; A Rakhlin; M Dahleh"}, {"ref_id": "b78", "title": "Finite time LTI system identification", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "T Sarkar; A Rakhlin; M A Dahleh"}, {"ref_id": "b79", "title": "Event-triggered learning for linear quadratic control", "journal": "IEEE Transactions on Automatic Control", "year": "2020", "authors": "H Schluter; F Solowjow; S Trimpe"}, {"ref_id": "b80", "title": "Prototypical networks for few-shot learning", "journal": "", "year": "2017", "authors": "J Snell; K Swersky; R Zemel"}, {"ref_id": "b81", "title": "Switched Linear Systems: Control and Design", "journal": "Springer Science & Business Media", "year": "2006", "authors": "Z Sun"}, {"ref_id": "b82", "title": "Multi-task reinforcement learning with context-based representations", "journal": "PMLR", "year": "2021-07", "authors": "S Sodhani; A Zhang; J Pineau"}, {"ref_id": "b83", "title": "On the theory of transfer learning: The importance of task diversity", "journal": "", "year": "2020", "authors": "N Tripuraneni; M I Jordan; C Jin"}, {"ref_id": "b84", "title": "Sample complexity of kalman filtering for unknown systems", "journal": "PMLR", "year": "2020", "authors": "A Tsiamis; N Matni; G Pappas"}, {"ref_id": "b85", "title": "Sequential transfer in reinforcement learning with a generative model", "journal": "PMLR", "year": "2020", "authors": "A Tirinzoni; R Poiani; M Restelli"}, {"ref_id": "b86", "title": "Transfer learning for reinforcement learning domains: A survey", "journal": "Journal of Machine Learning Research", "year": "2009", "authors": "M E Taylor; P Stone"}, {"ref_id": "b87", "title": "Clustering individuals on limited features of a vector autoregressive model", "journal": "Multivariate Behavioral Research", "year": "2020", "authors": "K Takano; M Stefanovic; T Rosenkranz; T Ehring"}, {"ref_id": "b88", "title": "High-dimensional Probability: An Introduction with Applications in Data Science", "journal": "Cambridge university press", "year": "2018", "authors": "R Vershynin"}, {"ref_id": "b89", "title": "Multi-task reinforcement learning: a hierarchical Bayesian approach", "journal": "", "year": "2007", "authors": "A Wilson; A Fern; S Ray; P Tadepalli"}, {"ref_id": "b90", "title": "On a mixture autoregressive model", "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": "2000", "authors": "C S Wong; W K Li"}, {"ref_id": "b91", "title": "Alternating minimization for mixed linear regression", "journal": "", "year": "2014", "authors": "X Yi; C Caramanis; S Sanghavi"}, {"ref_id": "b92", "title": "Learning mixtures of sparse linear regressions using sparse graph codes", "journal": "IEEE Transactions on Information Theory", "year": "2018", "authors": "D Yin; R Pedarsani; Y Chen; K Ramchandran"}, {"ref_id": "b93", "title": "Sample complexity of linear quadratic gaussian (lqg) control for output feedback systems", "journal": "PMLR", "year": "2021", "authors": "Y Zheng; L Furieri; M Kamgarpour; N Li"}, {"ref_id": "b94", "title": "A brief tutorial and survey on markovian jump systems: Stability and control", "journal": "IEEE Systems, Man, and Cybernetics Magazine", "year": "2019", "authors": "P Zhao; Y Kang; Y.-B Zhao"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "We take |M subspace | = 30 d, |M clustering | = 10 d, and vary |M classification | between [0, 5000 d].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "T o 's and |M o |'s in reality are slightly smaller than what our theory requires.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure2: The model estimation errors of Algorithm 1 versus the total sample size (excluding M subspace ). Each curve is an average over 12 independent trials.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "i:=E G Trunc i be its expectation. In the following, we first show that G Trunc i concentrates around E Trunc i , and then prove that E Trunc i \u2248 G i .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "A.2. 22Proof of Lemma 3 Define \u2206 := M \u2212 M , and denote the eigendecomposition of M and M as M = U \u039b U and M = U \u039bU + U \u22a5 \u039b \u22a5 U \u22a5 , where diagonal matrix \u039b (resp. \u039b ) contains the top-K eigenvalues of M (resp. M ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "l , which completes our calculation of the variance for the case of |\u2126 1 | = |\u2126 2 | = N . For the more general case where (without loss of generality) |\u2126 1 | = N \u2264 |\u2126 2 |, we simply need to modify the equation (53) to an inequality E[stat 2 ] = Tr(E[bb ]E[aa ]) \u2264 Tr(E[aa ] 2 ), and the remaining analysis is the same. This finishes the proof of Lemma 4.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "D k, , and thus it suffices to have T ( \u03ba 5 w,cross D k, + 1) log 2 1", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "stability condition (21) with \u03c1 = 0.99 < 1 and hence t mix 1/(1 \u2212 \u03c1) = 100. Suppose that each short trajectory has only a length of 2, and the m-th (resp. (m + 1)-th) trajectory has label k m = (resp. k m+1 = k). Then x m+2,0 is equal to A (k) A ( ) x m,0 plus a mean-zero noise term, where A (k) A ( ) = 0.99 2 0.99 2 \u2022 3/2 > 1; this will cause the exponential explosion of the states.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 3 :3Figure 3: Left: mis-clustering rate versus T clustering . Right: mis-classification rate versus T classification .", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Algorithm 2: Subspace estimation 1 Input: short trajectories {X m } m\u2208M subspace , where X m = {x m,t } 0\u2264t\u2264T subspace .", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Clustering 1 Input: short trajectories {X m } m\u2208M clustering , where X m = {x m,t } 0\u2264t\u2264T clustering ; subspaces {V i , U i } 1\u2264i\u2264d ; testing threshold \u03c4 ; number of copies G.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "A list of notation and parameters. In the subscripts of M o , T o , T total,o , the symbol o takes value in {subspace, clustering, classification}. Subsets of M trajectories, {1, . . . , M } = M subspace \u222a M clustering \u222a M classification", "figure_data": "NotationExplanationdState dimensionKNumber of LDS modelsk mThe unknown label (latent variable) of the m-th trajectoryM op minA lower bound for the fraction of trajectories generated by each modelT oShort trajectory length for M oT total,oTotal trajectory length, T total,o = T o \u2022 |M o |A (k) , W (k)State transition matrix and noise covariance matrix of the k-th LDS model"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Mixtures of time-series models and trajectories. Mixture models for time series have achieved empirical success in the study of psychology [BTBC16, TSRE20], neuroscience [Alb91, MYP + 09], biology [WL00], air pollution [DDGM15], economics [MT94, Mah00, KMS16], automobile sensors [HVBL17], and many other domains. Some specific aspects of mixture models include hypothesis testing for a pair of trajectories [Mah00], or clustering of multiple trajectories [Lia05, ASW15, PSR + 21, HSV21]. In addition to mixture models, other related yet different models in the literature include time-varying systems [QSL + 21, MGSH21], systems with random parameters [DMZ20], switching systems [Sun06, SRD19, ABK + 21, MT94], switching state-space models [GH00, LJM + 17], Markovian jump systems [SL15, ZKZ19], and event-triggered systems [SIW + 20, SST20], to name just a few. There are even more related models in reinforcement learning (RL), such as latent bandit [MM14, HKZ + 20], multi-task learning [WFRT07, LG10, BL13, LGB16, SZP21]/metalearning [FAL17]/transfer-learning [TS09, HFL + 21, TPR20] for RL, typed parametric models [BLL + 09], latent Markov decision processes [KECM21, HDCM15, SKD21, BS19], and so on. What distinguishes our", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "k m -th LDS for some unknown label 1 \u2264 k m \u2264 K such that x m,t+1 = A (km) x m,t + w m,t , where the w m,t 's are i.i.d., E[w m,t ] = 0, cov(w m,t ) = W (km) 0 (2) for all 0 \u2264 t \u2264 T m \u2212 1. Next, we divide the M sample trajectories {X m } 1\u2264m\u2264M in hand into three disjoint subsets M subspace , M clustering , M classification satisfying M subspace \u222a M clustering \u222a M classification = {1, 2, . . . , M },", "formula_coordinates": [5.0, 72.0, 355.21, 468.0, 88.03]}, {"formula_id": "formula_1", "formula_text": "T m = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 T subspace if m \u2208 M subspace , T clustering if m \u2208 M clustering , T classification if m \u2208 M classification .(3)", "formula_coordinates": [5.0, 214.67, 487.35, 325.33, 41.38]}, {"formula_id": "formula_2", "formula_text": "E[x t+1 ] = 0 and cov(x t+1 ) = A \u2022 cov(x t ) \u2022 A + cov(w t ) = A\u0393A + W .", "formula_coordinates": [5.0, 130.76, 706.41, 350.48, 10.32]}, {"formula_id": "formula_3", "formula_text": "\u0393(A, W ) := E x t x t |A, W = A \u2022 \u0393(A, W ) \u2022 A + W = \u221e t=0 A t W (A t ) .(4)", "formula_coordinates": [6.0, 143.13, 102.43, 396.87, 30.2]}, {"formula_id": "formula_4", "formula_text": "Y (A, W ) := E x t+1 x t |A, W = E (Ax t + w t )x t |A, W = A \u2022 \u0393(A, W ).(5)", "formula_coordinates": [6.0, 138.93, 156.2, 401.07, 10.32]}, {"formula_id": "formula_5", "formula_text": "\u0393 (k) := \u0393(A (k) , W (k) ), Y (k) := Y (A (k) , W (k) ), 1 \u2264 k \u2264 K.(6)", "formula_coordinates": [6.0, 169.15, 190.28, 370.86, 11.37]}, {"formula_id": "formula_6", "formula_text": "A (k) = Y (k) \u0393 (k) \u22121 , W (k) = \u0393 (k) \u2212 A (k) \u0393 (k) A (k) .(7)", "formula_coordinates": [6.0, 190.62, 226.25, 349.38, 14.14]}, {"formula_id": "formula_7", "formula_text": "x t+s = A s x t + s i=1 A i\u22121 w t+s\u2212i (8)", "formula_coordinates": [6.0, 237.33, 271.58, 302.67, 30.32]}, {"formula_id": "formula_8", "formula_text": ") ) i \u2208 R d (resp. (Y (k) ) i ) represents the transpose of the i-th row of \u0393 (k) (resp. Y (k)", "formula_coordinates": [6.0, 72.0, 424.1, 468.0, 22.83]}, {"formula_id": "formula_9", "formula_text": "V i := span (\u0393 (k) ) i , 1 \u2264 k \u2264 K , U i := span (Y (k) ) i , 1 \u2264 k \u2264 K , 1 \u2264 i \u2264 d.(9)", "formula_coordinates": [6.0, 127.87, 456.29, 412.13, 12.69]}, {"formula_id": "formula_10", "formula_text": "(ii) for each m \u2208 M subspace , 1 \u2264 i \u2264 d, j \u2208 {1, 2}, compute h m,i,j := 1 |\u2126 j | t\u2208\u2126j (x m,t ) i x m,t , g m,i,j := 1 |\u2126 j | t\u2208\u2126j (x m,t+1 ) i x m,t , ;(10)", "formula_coordinates": [6.0, 78.65, 573.6, 461.36, 43.52]}, {"formula_id": "formula_11", "formula_text": "H i := 1 |M subspace | m\u2208M subspace h m,i,1 h m,i,2 , G i := 1 |M subspace | m\u2208M subspace g m,i,1 g m,i,2 ,(11)", "formula_coordinates": [6.0, 138.79, 645.03, 401.22, 27.47]}, {"formula_id": "formula_12", "formula_text": "H i + H i (resp. G i + G i ).", "formula_coordinates": [6.0, 348.72, 682.83, 120.21, 11.42]}, {"formula_id": "formula_13", "formula_text": "\u2190 {N + j, 1 \u2264 j \u2264 N }, \u2126 2 \u2190 {3N + j, 1 \u2264 j \u2264 N }. 3 for (m, i, j) \u2208 M subspace \u00d7 [d] \u00d7 [2] do 4 Compute h m,i,j \u2190 1 |\u2126 j | t\u2208\u2126j (x m,t ) i x m,t , g m,i,j \u2190 1 |\u2126 j | t\u2208\u2126j (x m,t+1 ) i x m,t . 5 for i = 1, . . . , d do 6 Compute H i \u2190 1 |M subspace | m\u2208M subspace h m,i,1 h m,i,2 , G i \u2190 1 |M subspace | m\u2208M subspace g m,i,1 g m,i,2 , 7 Let V i \u2208 R d\u00d7K (resp. U i ) be the top-K eigenspace of H i + H i (resp. G i + G i ). 8 Output: subspaces {V i , U i } 1\u2264i\u2264d .", "formula_coordinates": [7.0, 77.45, 104.89, 418.49, 175.7]}, {"formula_id": "formula_14", "formula_text": "E h m,i,j ] \u2248 (\u0393 (km) ) i , E g m,i,j ] \u2248 (Y (km) ) i , 1 \u2264 i \u2264 d, j \u2208 {1, 2}.", "formula_coordinates": [7.0, 153.93, 349.83, 304.14, 11.72]}, {"formula_id": "formula_15", "formula_text": "E H i \u2248 1 |M subspace | m\u2208M subspace (\u0393 (km) ) i (\u0393 (km) ) i = K k=1 p (k) subspace (\u0393 (k) ) i (\u0393 (k) ) i =: H i , (12a", "formula_coordinates": [7.0, 118.66, 401.9, 416.81, 31.15]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [7.0, 535.46, 411.64, 4.54, 9.96]}, {"formula_id": "formula_17", "formula_text": "E G i \u2248 1 |M subspace | m\u2208M subspace (Y (km) ) i (Y (km) ) i = K k=1 p (k) subspace (Y (k) ) i (Y (k) ) i =: G i ,(12b)", "formula_coordinates": [7.0, 119.61, 439.02, 420.39, 31.15]}, {"formula_id": "formula_18", "formula_text": "p (k)", "formula_coordinates": [7.0, 100.8, 480.31, 15.64, 11.87]}, {"formula_id": "formula_19", "formula_text": "p (k) subspace := 1 |M subspace | m\u2208M subspace 1(k m = k), 1 \u2264 k \u2264 K. (13)", "formula_coordinates": [7.0, 181.43, 502.89, 358.57, 27.47]}, {"formula_id": "formula_20", "formula_text": "H i + H i and G i + G i in Algorithm 2.", "formula_coordinates": [7.0, 72.0, 568.04, 468.0, 24.75]}, {"formula_id": "formula_21", "formula_text": "(A (k) , W (k) ) = (A ( ) , W ( ) ), it is indeed possible that \u0393 (k) = \u0393 ( ) or Y (k) = Y", "formula_coordinates": [7.0, 72.0, 675.76, 468.0, 22.27]}, {"formula_id": "formula_22", "formula_text": "\u2126 g,1 \u2190 (4g \u2212 3)N + j, 1 \u2264 j \u2264 N , \u2126 g,2 \u2190 (4g \u2212 1)N + j, 1 \u2264 j \u2264 N , 1 \u2264 g \u2264 G. 3 for (m, i, g, j) \u2208 M clustering \u00d7 [d] \u00d7 [G] \u00d7 [2] do 4 Compute h m,i,g,j \u2190 1 |\u2126 g,j | t\u2208\u2126g,j (x m,t ) i x m,t , g m,i,g,j \u2190 1 |\u2126 g,j | t\u2208\u2126g,j (x m,t+1 ) i x m,t .", "formula_coordinates": [8.0, 77.45, 142.85, 426.13, 85.53]}, {"formula_id": "formula_23", "formula_text": "5 for (m, n) \u2208 M clustering \u00d7 M clustering do 6 for g = 1, . . . , G do 7 Compute stat \u0393,g \u2190 d i=1 V i h m,i,g,1 \u2212 h n,i,g,1 , V i h m,i,g,2 \u2212 h n,i,g,2 , (14a) stat Y,g \u2190 d i=1 U i g m,i,g,1 \u2212 g n,i,g,1 , U i g m,i,g,2 \u2212 g n,i,g,2 ,(14b)", "formula_coordinates": [8.0, 77.45, 257.23, 447.61, 111.76]}, {"formula_id": "formula_24", "formula_text": "8 Set S m,n \u2190 1 median stat \u0393,g , 1 \u2264 g \u2264 G + median stat Y,g , 1 \u2264 g \u2264 G \u2264 \u03c4 . 9 Divide M clustering into K clusters {C k } 1\u2264k\u2264K according to {S m,n } m,n\u2208M clustering . 10 Output: clusters {C k } 1\u2264k\u2264K .", "formula_coordinates": [8.0, 72.94, 384.87, 370.91, 42.48]}, {"formula_id": "formula_25", "formula_text": "A (k) = Y (k) \u0393 (k) \u22121 = Y ( ) \u0393 ( ) \u22121 = A ( ) ,and", "formula_coordinates": [8.0, 173.05, 486.59, 204.88, 14.14]}, {"formula_id": "formula_26", "formula_text": "W (k) = \u0393 (k) \u2212 A (k) \u0393 (k) A (k) = \u0393 ( ) \u2212 A ( ) \u0393 ( ) A ( ) = W ( ) ,", "formula_coordinates": [8.0, 169.23, 508.65, 273.53, 10.81]}, {"formula_id": "formula_27", "formula_text": "\u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2", "formula_coordinates": [8.0, 271.75, 586.3, 137.72, 12.48]}, {"formula_id": "formula_28", "formula_text": "U i E (x t+1 ) i x t \u2212 (z t+1 ) i z t \u2248 U i (Y (k) ) i \u2212 (Y ( ) ) i for all 1 \u2264 i \u2264 d and t \u2208 \u2126 1 \u222a \u2126 2 .", "formula_coordinates": [8.0, 72.0, 642.06, 354.12, 32.68]}, {"formula_id": "formula_29", "formula_text": "stat Y := d i=1 U i 1 |\u2126 1 | t\u2208\u21261 (x t+1 ) i x t \u2212 (z t+1 ) i z t , U i 1 |\u2126 2 | t\u2208\u21262 (x t+1 ) i x t \u2212 (z t+1 ) i z t ,(15)", "formula_coordinates": [8.0, 108.53, 693.87, 431.47, 30.48]}, {"formula_id": "formula_30", "formula_text": "E[stat Y ] \u2248 d i=1 U i (Y (k) ) i \u2212 (Y ( ) ) i , U i (Y (k) ) i \u2212 (Y ( ) ) i = d i=1 U i (Y (k) ) i \u2212 (Y ( ) ) i 2 2 \u2248 d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 2 = Y (k) \u2212 Y ( ) 2 F ;", "formula_coordinates": [9.0, 113.76, 95.93, 384.48, 65.35]}, {"formula_id": "formula_31", "formula_text": "stat \u0393 := d i=1 V i 1 |\u2126 1 | t\u2208\u21261 (x t ) i x t \u2212 (z t ) i z t , V i 1 |\u2126 2 | t\u2208\u21262 (x t ) i x t \u2212 (z t ) i z t ,(16)", "formula_coordinates": [9.0, 130.66, 229.14, 409.34, 30.47]}, {"formula_id": "formula_32", "formula_text": "E[stat \u0393 ] \u2248 d i=1 V i ((\u0393 (k) ) i \u2212 (\u0393 ( ) ) i ) 2 2 \u2248 \u0393 (k) \u2212 \u0393 ( ) 2 F .", "formula_coordinates": [9.0, 181.11, 292.05, 249.79, 30.32]}, {"formula_id": "formula_33", "formula_text": "A (k) := arg min A m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t+1 \u2212 Ax m,t 2 2 ,(17)", "formula_coordinates": [9.0, 195.18, 606.42, 344.82, 22.81]}, {"formula_id": "formula_34", "formula_text": "1 Input: clusters {C k } 1\u2264k\u2264K . 2 for k = 1, . . . , K do 3 Compute A (k) \u2190 m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t+1 x m,t m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t x m,t\u22121", "formula_coordinates": [10.0, 77.45, 92.27, 352.69, 73.34]}, {"formula_id": "formula_35", "formula_text": "W (k) \u2190 1 m\u2208C k T m m\u2208C k 0\u2264t\u2264Tm\u22121", "formula_coordinates": [10.0, 121.36, 169.5, 154.07, 27.47]}, {"formula_id": "formula_36", "formula_text": "1 Input: short trajectories {X m } m\u2208M classification , where X m = {x m,t } 0\u2264t\u2264Tm ; coarse models { A (k) , W (k) } 1\u2264k\u2264K ; clusters {C k } 1\u2264k\u2264K . 2 for m \u2208 M classification do 3", "formula_coordinates": [10.0, 77.45, 262.65, 392.62, 47.34]}, {"formula_id": "formula_37", "formula_text": "k m \u2190 arg min T m \u2022 log det( W ( ) ) + Tm\u22121 t=0 (x m,t+1 \u2212 A ( ) x m,t ) ( W ( ) ) \u22121 (x m,t+1 \u2212 A ( ) x m,t ) ,", "formula_coordinates": [10.0, 107.31, 324.18, 417.7, 30.31]}, {"formula_id": "formula_38", "formula_text": "L(A, W ) := T \u2022 log det(W ) + T \u22121 t=0 (x t+1 \u2212 Ax t ) W \u22121 (x t+1 \u2212 Ax t ).(19)", "formula_coordinates": [10.0, 157.53, 584.06, 382.47, 30.2]}, {"formula_id": "formula_39", "formula_text": "x m,0 = 0, 1 \u2264 m \u2264 M ; (20a) Case 1: x 1,0 = 0, and x m+1,0 = x m,Tm , 1 \u2264 m \u2264 M \u2212 1.(20b)", "formula_coordinates": [11.0, 168.32, 303.06, 371.68, 25.26]}, {"formula_id": "formula_40", "formula_text": "T total := 1\u2264m\u2264M T m = T total,subspace + T total,clustering + T total,classification ,", "formula_coordinates": [11.0, 169.17, 363.6, 288.39, 20.42]}, {"formula_id": "formula_41", "formula_text": "T total,o := T o \u2022 |M o |, o \u2208 {subspace, clustering, classification}.", "formula_coordinates": [11.0, 189.88, 390.16, 263.07, 10.0]}, {"formula_id": "formula_42", "formula_text": "p min \u2264 p (k) o := 1 |M o | m\u2208Mo 1(k m = k), 1 \u2264 k \u2264 K, o \u2208 {subspace, clustering, classification}.", "formula_coordinates": [11.0, 105.26, 444.25, 401.49, 26.8]}, {"formula_id": "formula_43", "formula_text": "(A (k) ) t \u2264 \u03ba A \u2022 \u03c1 t , t = 1, 2, . . . ;(21)", "formula_coordinates": [11.0, 249.07, 554.36, 290.93, 11.72]}, {"formula_id": "formula_44", "formula_text": "1 \u2264 k \u2264 K, \u03bb max (\u0393 (k) ) \u2264 \u0393 max , W min \u2264 \u03bb min (W (k) ) \u2264 \u03bb max (W (k) ) \u2264 W max , W max W min =: \u03ba w,cross , \u03ba(W (k) ) = \u03bb max (W (k) ) \u03bb min (W (k) ) \u2264 \u03ba w ;", "formula_coordinates": [11.0, 179.86, 582.33, 304.72, 62.97]}, {"formula_id": "formula_45", "formula_text": "\u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2265 \u2206 2 \u0393,Y ,(22a)", "formula_coordinates": [11.0, 237.52, 681.09, 302.48, 12.69]}, {"formula_id": "formula_46", "formula_text": "A (k) \u2212 A ( ) 2 F + W (k) \u2212 W ( ) 2 F W 2 max \u2265 \u2206 2 A,W .(22b)", "formula_coordinates": [11.0, 225.16, 698.24, 314.85, 25.77]}, {"formula_id": "formula_47", "formula_text": "\u0393 (k) , Y (k)", "formula_coordinates": [12.0, 106.1, 263.89, 42.34, 10.31]}, {"formula_id": "formula_48", "formula_text": "\u03ba A , \u03c1 (A (k) ) t \u2264 \u03ba A \u2022 \u03c1 t , t = 1, 2, . . . \u2206 \u0393,Y , \u2206 A,W Model separation (22) W min , W max W min \u2264 \u03bb min (W (k) ) \u2264 \u03bb max (W (k) ) \u2264 W max , 1 \u2264 k \u2264 K \u03ba w,cross , \u03ba w \u03ba w,cross = W max /W min ; \u03ba(W (k) ) \u2264 \u03ba w , 1 \u2264 k \u2264 K \u0393 max \u0393 (k) \u2264 \u0393 max , 1 \u2264 k \u2264 K", "formula_coordinates": [12.0, 101.2, 281.32, 358.53, 80.48]}, {"formula_id": "formula_49", "formula_text": "\u2206 A,W = \u221a d \u03b4 A,W , \u2206 \u0393,Y = \u221a d \u03b4 \u0393,Y ,(23)", "formula_coordinates": [12.0, 224.65, 528.08, 315.35, 18.63]}, {"formula_id": "formula_50", "formula_text": "\u2206 A,W = A (1) \u2212 A (2) F = \u03b4I d F = \u221a", "formula_coordinates": [12.0, 322.23, 570.53, 178.93, 18.14]}, {"formula_id": "formula_51", "formula_text": "T subspace \u2265 C 2 \u03b9 1 1 \u2212 \u03c1 , T total,subspace \u2265 C 3 d 1 \u2212 \u03c1 \u0393 max \u221a d \u2206 \u0393,Y 4 K 2 p min 2 + 1 \u2022 \u03b9 4 1 ,(24a)", "formula_coordinates": [13.0, 89.06, 125.16, 450.94, 31.72]}, {"formula_id": "formula_52", "formula_text": "T clustering \u2265 C 4 G 1 \u2212 \u03c1 \u0393 2 max \u03ba 2 A \u221a dK \u2206 2 \u0393,Y + 1 \u03b9 2 , T total,clustering \u2265 C 5 d\u03ba 2 w,cross p min d \u2206 2 A,W \u0393 max W min + 1 \u03b9 2 3 ,(24b)", "formula_coordinates": [13.0, 89.06, 159.03, 450.94, 33.15]}, {"formula_id": "formula_53", "formula_text": "T classification \u2265 C 6 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 1 ,(24c)", "formula_coordinates": [13.0, 89.06, 197.24, 450.94, 27.2]}, {"formula_id": "formula_54", "formula_text": "d\u03ba A T total \u03b4 ), \u03b9 2 := log ( \u0393max \u2206 \u0393,Y +2) d\u03ba A T total \u03b4 , \u03b9 3 := log( \u0393max W min d\u03ba A T total \u03b4", "formula_coordinates": [13.0, 276.27, 233.6, 255.11, 14.32]}, {"formula_id": "formula_55", "formula_text": "A (k) \u2212 A (\u03c0(k)) \u2264 C 7 d\u03ba w \u03b9 3 p min T , W (k) \u2212 W (\u03c0(k)) W (\u03c0(k)) \u2264 C 8 d\u03b9 3 p min T , 1 \u2264 k \u2264 K,(25)", "formula_coordinates": [13.0, 133.11, 296.76, 406.9, 24.8]}, {"formula_id": "formula_56", "formula_text": "T (\u03c0(k)) := p (\u03c0(k)) clustering T total,clustering + p (\u03c0(k)) classification T total,classification \u2265 p min T .", "formula_coordinates": [13.0, 225.56, 361.76, 291.43, 14.3]}, {"formula_id": "formula_57", "formula_text": "p min d \u2206 2 A,W \u0393 max W min \u03ba 2 A + 1 \u03b9 2 3 ,(26a)", "formula_coordinates": [13.0, 277.19, 416.27, 262.81, 24.66]}, {"formula_id": "formula_58", "formula_text": "T classification \u2265 C 6 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 1 + 1 1 \u2212 \u03c1 log(2\u03ba A ),(26b)", "formula_coordinates": [13.0, 196.95, 444.65, 343.05, 27.2]}, {"formula_id": "formula_59", "formula_text": "T subspace t mix , T total,subspace t mix d \u0393 max K \u03b4 \u0393,Y 4 + 1 , T clustering t mix \u0393 max \u03b4 \u0393,Y 2 K d + 1 , T total,clustering Kd 1 \u03b4 2 A,W \u0393 max W min + 1 , T classification \uf8f1 \uf8f2 \uf8f3 1 d\u03b4 2 A,W + 1 for Case 0, 1 d\u03b4 2 A,W + t mix for Case 1, T total,clustering + T total,classification Kd 2 ,", "formula_coordinates": [13.0, 121.03, 622.06, 369.94, 102.08]}, {"formula_id": "formula_60", "formula_text": "A (k) \u2212 A (\u03c0(k)) \u2264 , W (k) \u2212 W (\u03c0(k)) W (\u03c0(k)) \u2264 , 1 \u2264 k \u2264 K", "formula_coordinates": [14.0, 178.06, 105.03, 260.14, 24.06]}, {"formula_id": "formula_61", "formula_text": "\u2022 Canonical separation parameters \u03b4 A,W , \u03b4 \u0393,Y : (1) T clustering 1/\u03b4 2 \u0393,Y guarantees exact clustering of the trajectories, while T classification 1/\u03b4 2 A,W guarantees exact classification. (2) T total,subspace 1/\u03b4 4 \u0393,Y", "formula_coordinates": [14.0, 86.95, 303.41, 453.06, 25.68]}, {"formula_id": "formula_62", "formula_text": "T subspace \u2265 C 1 \u2022 t mix , T total,subspace \u2265 C 2 \u2022 t mix d \u2022 log T total d \u03b4 ,where", "formula_coordinates": [16.0, 89.74, 426.43, 287.43, 22.31]}, {"formula_id": "formula_63", "formula_text": "t mix := 1 1 \u2212 \u03c1 \u2022 log d\u03ba A T total \u03b4 .", "formula_coordinates": [16.0, 387.35, 426.43, 134.92, 22.31]}, {"formula_id": "formula_64", "formula_text": "1 \u2264 i \u2264 d, max (\u0393 (k) ) i \u2212 V i V i (\u0393 (k) ) i 2 , (Y (k) ) i \u2212 U i U i (Y (k) ) i 2 \u2264 C 3 \u2022 \u0393 max K p min 1/2 t mix d T total,subspace log 3 T total d \u03b4 1/4 .(27)", "formula_coordinates": [16.0, 141.13, 472.76, 398.87, 68.24]}, {"formula_id": "formula_65", "formula_text": "\u2022 The threshold \u03c4 obeys 1/8 < \u03c4 /\u2206 2 \u0393,Y < 3/8; \u2022 The short trajectory length T clustering = N G, where G \u2265 C 1 \u2022 log |M clustering | \u03b4 , N \u2265 C 2 \u0393 2 max \u03ba 2 A \u221a dK \u2206 2 \u0393,Y + 1 1 1 \u2212 \u03c1 log \u0393 max \u2206 \u0393,Y + 2 d\u03ba A T total \u03b4 ; \u2022 The subspaces {V i , U i } 1\u2264i\u2264d satisfy that, for all 1 \u2264 i \u2264 d and 1 \u2264 k \u2264 K, max (\u0393 (k) ) i \u2212 V i V i (\u0393 (k) ) i 2 , (Y (k) ) i \u2212 U i U i (Y (k) ) i 2 \u2264 c 3 \u2206 \u0393,Y \u221a d . (28", "formula_coordinates": [17.0, 86.95, 73.58, 448.63, 132.73]}, {"formula_id": "formula_66", "formula_text": ")", "formula_coordinates": [17.0, 535.57, 188.72, 4.43, 9.96]}, {"formula_id": "formula_67", "formula_text": "C 1 , C 2 , C 3 > 0 such that the following holds. Let {C k } 1\u2264k\u2264K be subsets of M clustering \u222a M classification such that for all 1 \u2264 k \u2264 K, C k contains only short trajectories generated by model (A (k) , W (k) ), namely k m = k for all m \u2208 C k . Suppose that for all m \u2208 M clustering \u222a M classification , T m \u2265 4, and for all 1 \u2264 k \u2264 K, T (k) total := m\u2208C k T m \u2265 C 1 d\u03ba 2 w \u03b9, where \u03b9 := log \u0393 max W min d\u03ba A T total \u03b4 .", "formula_coordinates": [17.0, 72.0, 313.69, 468.0, 72.78]}, {"formula_id": "formula_68", "formula_text": "A (k) \u2212 A (k) \u2264 C 2 d\u03ba w \u03b9 T (k) total , W (k) \u2212 W (k) W (k) \u2264 C 3 d\u03b9 T (k) total , 1 \u2264 k \u2264 K.", "formula_coordinates": [17.0, 147.39, 422.75, 322.2, 28.79]}, {"formula_id": "formula_69", "formula_text": "A (k) , W (k) } 1\u2264k\u2264K satisfying A (k) \u2212 A (k) \u2264 A , W (k) \u2212 W (k) \u2264 W for all k.", "formula_coordinates": [17.0, 72.0, 616.75, 466.43, 22.82]}, {"formula_id": "formula_70", "formula_text": "A \u2264 c 1 \u2206 A,W W min \u0393 max \u03ba w,cross (d + \u03b9) , W W min \u2264 c 2 \u2022 min 1, \u2206 A,W \u03ba w,cross d , (29a", "formula_coordinates": [17.0, 169.87, 666.6, 365.59, 24.15]}, {"formula_id": "formula_71", "formula_text": ")", "formula_coordinates": [17.0, 535.46, 672.67, 4.54, 9.96]}, {"formula_id": "formula_72", "formula_text": "T m \u2265 C 3 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 , m \u2208 M classification ; (29b) For Case 1: A \u2264 c 1 \u2206 A,W W min \u0393 max \u03ba w,cross \u03ba 2 A (d + \u03b9) , W W min \u2264 c 2 \u2022 min 1, \u2206 A,W \u03ba w,cross d ,(29c)", "formula_coordinates": [17.0, 165.83, 695.93, 374.17, 27.2]}, {"formula_id": "formula_73", "formula_text": "T m \u2265 C 3 \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W \u03b9 2 + 1 1 \u2212 \u03c1 log(2\u03ba A ), m \u2208 M classification ,(29d)", "formula_coordinates": [18.0, 165.83, 106.0, 374.17, 27.2]}, {"formula_id": "formula_74", "formula_text": "\u0393 (k) = \u221e i=0 A (k) i W (k) (A (k) i ) from (", "formula_coordinates": [19.0, 319.24, 150.73, 173.6, 15.49]}, {"formula_id": "formula_75", "formula_text": "\u0393 (k) t := t\u22121 i=0 A (k) i W (k) (A (k) i ) . (30", "formula_coordinates": [19.0, 237.74, 186.66, 297.83, 30.32]}, {"formula_id": "formula_76", "formula_text": ")", "formula_coordinates": [19.0, 535.57, 196.4, 4.43, 9.96]}, {"formula_id": "formula_77", "formula_text": "(k) ) t \u2264 \u03ba A \u03c1 t . As a result, \u0393 (k) t is close to \u0393 (k) : 0 \u0393 (k) \u2212 \u0393 (k) t = \u221e i=t A (k) i W (k) (A (k) i ) = A (k) t \u0393 (k) (A (k) t ) , \u0393 (k) \u2212 \u0393 (k) t \u2264 \u0393 (k) A (k) t 2 \u2264 \u0393 (k) \u03ba 2 A \u03c1 2t \u2264 \u0393 max \u03ba 2 A \u03c1 2t . (31", "formula_coordinates": [19.0, 72.0, 227.25, 468.0, 80.23]}, {"formula_id": "formula_78", "formula_text": ")", "formula_coordinates": [19.0, 535.57, 296.2, 4.43, 9.96]}, {"formula_id": "formula_79", "formula_text": "Moreover, let Y (k) t := A (k) \u0393 (k) t , then Y (k) t is also close to Y (k) : Y (k) \u2212 Y (k) t \u2264 A (k) \u0393 (k) \u2212 \u0393 (k) t \u2264 \u0393 max \u03ba 3 A \u03c1 2t .(32)", "formula_coordinates": [19.0, 72.0, 317.89, 468.0, 36.77]}, {"formula_id": "formula_80", "formula_text": "t mix \u22122 i=0 (A (km) ) i w m,t\u2212i\u22121 \u223c N (0, \u0393 (km) t mix \u22121 ), t mix \u2264 t \u2264 T m , 1 \u2264 m \u2264 M. (33", "formula_coordinates": [19.0, 187.46, 402.3, 348.11, 30.45]}, {"formula_id": "formula_81", "formula_text": ")", "formula_coordinates": [19.0, 535.57, 412.17, 4.43, 9.96]}, {"formula_id": "formula_82", "formula_text": "x m,t 2 \u2264 C 0 \u0393 max (d + log(T total /\u03b4)) for all 1 \u2264 m \u2264 M, t mix \u2264 t \u2264 T m", "formula_coordinates": [19.0, 72.0, 547.24, 468.0, 22.27]}, {"formula_id": "formula_83", "formula_text": "P( a 2 \u2265 2 \u221a d + u) \u2264 2 exp(\u2212cu 2 ). Since x m,t \u223c N (0, \u0393 (km) t mix \u22121 ), where \u0393 (km) t mix \u22121 \u0393 (km) \u0393 max I d , we have P( x m,t 2 \u2265 \u221a \u0393 max (2 \u221a d + u)) \u2264 2 exp(\u2212cu 2 ).", "formula_coordinates": [19.0, 72.0, 664.79, 468.0, 32.09]}, {"formula_id": "formula_84", "formula_text": "\u221a d + u) \u2264 M m=1 Tm t=t mix P x m,t 2 \u2265 \u0393 max (2 \u221a d + u) \u2264 2T total exp(\u2212cu 2 ) \u2264 \u03b4,", "formula_coordinates": [19.0, 361.18, 702.72, 35.24, 17.71]}, {"formula_id": "formula_85", "formula_text": "t \u2265 t mix \u2265 1 1\u2212\u03c1 log( \u221a 2\u03ba A ), we have (A (km) ) t \u2264 \u03ba A \u03c1 t \u2264 1/2.", "formula_coordinates": [20.0, 96.91, 206.31, 266.18, 19.83]}, {"formula_id": "formula_86", "formula_text": "x m+1,0 = x m,Tm = (A (km)", "formula_coordinates": [20.0, 142.88, 236.29, 114.56, 11.72]}, {"formula_id": "formula_87", "formula_text": "x m+1,0 \u2264 (A (km) ) Tm x m,0 2 + \u03be m,Tm 2 \u2264 1 2 x m,0 2 + C 0 \u0393 max (d + log T total \u03b4 ).", "formula_coordinates": [20.0, 137.9, 255.84, 366.09, 22.31]}, {"formula_id": "formula_88", "formula_text": "x m,0 2 \u2264 2C 0 \u0393 max (d + log(T total /\u03b4))", "formula_coordinates": [20.0, 372.39, 289.21, 167.61, 9.68]}, {"formula_id": "formula_89", "formula_text": "x m,t 2 \u2264 (A (km) ) t x m,0 2 + \u03be m,t 2 \u2264 \u03ba A x m,0 2 + C 0 \u0393 max (d + log T total \u03b4 ) \u2264 3C 0 \u03ba A \u0393 max (d + log T total \u03b4 );", "formula_coordinates": [20.0, 149.67, 319.76, 342.55, 41.86]}, {"formula_id": "formula_90", "formula_text": "x m,t 2 \u2264 (A (km) ) t x m,0 2 + \u03be m,t 2 \u2264 1 2 x m,0 2 + C 0 \u0393 max (d + log T total \u03b4 ) \u2264 2C 0 \u0393 max (d + log T total \u03b4 ).", "formula_coordinates": [20.0, 158.21, 390.79, 325.47, 41.86]}, {"formula_id": "formula_91", "formula_text": "1 \u2264 i \u2264 d, max H i \u2212 H i , G i \u2212 G i \u0393 2 max t mix d T total,subspace log 3 T total d \u03b4 . Lemma 3. Consider the matrix M = K k=1 p (k) y (k) y (k) \u2208 R d\u00d7d , where 0 < p (k) < 1, K k=1 p (k)", "formula_coordinates": [20.0, 72.0, 522.53, 442.21, 73.95]}, {"formula_id": "formula_92", "formula_text": "K k=1 p (k) y (k) \u2212 U U y (k) 2 2 \u2264 2K ,(34)", "formula_coordinates": [20.0, 230.4, 618.04, 309.6, 30.55]}, {"formula_id": "formula_93", "formula_text": "y (k) \u2212 U U y (k) 2 \u2264 min 2K p (k) 1/2 , 2 \u03bb min (M ) y (k) 2 , \u221a 2 p (k) y (k) 2 1/3 . (35", "formula_coordinates": [20.0, 131.96, 673.65, 403.61, 26.43]}, {"formula_id": "formula_94", "formula_text": ")", "formula_coordinates": [20.0, 535.57, 682.93, 4.43, 9.96]}, {"formula_id": "formula_95", "formula_text": "i.i.d. \u223c N (0, \u0393 (km) ), w m,t , v m,t i.i.d. \u223c N (0, W (km) ), x m,t = A (km) x m,t + w m,t , z m,t = A (km) z m,t + v m,t , 1 \u2264 t \u2264 N,", "formula_coordinates": [21.0, 156.83, 188.51, 298.33, 30.25]}, {"formula_id": "formula_96", "formula_text": "G i := 1 M N M m=1 N t=1 x m,t i x m,t z m,t i z m,t , G i = K k=1 p (k) ( Y (k) ) i ( Y (k) ) i ,", "formula_coordinates": [21.0, 123.76, 279.64, 364.49, 30.55]}, {"formula_id": "formula_97", "formula_text": "if the i.i.d. sample size M N satisfies M N d \u2022 log(M N d/\u03b4), then with probability at least 1 \u2212 \u03b4, G i \u2212 G i \u0393 2 max d M N log 3 M N d \u03b4 , 1 \u2264 i \u2264 d.(36)", "formula_coordinates": [21.0, 72.0, 323.06, 468.0, 58.34]}, {"formula_id": "formula_98", "formula_text": "= |\u2126 1 | = |\u2126 2 | T subspace . For any t \u2208 \u2126 1 and 1 \u2264 j \u2264 N , define s j (t) := Cycle(t + \u2206 + j; \u2126 2 ) = t + \u2206 + j if t + \u2206 + j \u2264 \u03c4 2 + N, t + \u2206 + j \u2212 N otherwise,", "formula_coordinates": [21.0, 146.25, 528.61, 318.31, 48.76]}, {"formula_id": "formula_99", "formula_text": "\u2126 1 \u00d7 \u2126 2 = (t 1 , t 2 ), t 1 \u2208 \u2126 1 , t 2 \u2208 \u2126 2 = \u222a N j=1 t, s j (t) , t \u2208 \u2126 1 .", "formula_coordinates": [21.0, 164.72, 614.24, 282.55, 12.69]}, {"formula_id": "formula_100", "formula_text": "S \u03c4 := {\u03c4 1 + \u03c4 + f \u2022 t mix : f \u2265 0, \u03c4 + f \u2022 t mix \u2264 N }, 1 \u2264 \u03c4 \u2264 t mix , so that \u2126 1 = \u222a t mix \u03c4 =1 S \u03c4 .", "formula_coordinates": [21.0, 72.0, 662.18, 372.98, 33.08]}, {"formula_id": "formula_101", "formula_text": "\u2126 1 \u00d7 \u2126 2 = \u222a N j=1 \u222a t mix \u03c4 =1 t, s j (t) , t \u2208 S \u03c4 . (37", "formula_coordinates": [21.0, 215.76, 705.96, 319.82, 13.22]}, {"formula_id": "formula_102", "formula_text": ")", "formula_coordinates": [21.0, 535.57, 707.69, 4.43, 9.96]}, {"formula_id": "formula_103", "formula_text": "G i = 1 |M subspace | m\u2208M subspace 1 N 2 (t1,t2)\u2208\u21261\u00d7\u21262 (x m,t1 ) i x m,t1 \u2022 (x m,t2 ) i x m,t2 = N j=1 t mix \u03c4 =1 |S \u03c4 | N 2 \u2022 F i,j,\u03c4 ,", "formula_coordinates": [22.0, 96.46, 119.3, 419.08, 31.27]}, {"formula_id": "formula_104", "formula_text": "F i,j,\u03c4 := 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 (x m,t ) i x m,t \u2022 (x m,sj (t) ) i x m,sj (t) . (38", "formula_coordinates": [22.0, 144.63, 176.2, 390.94, 27.47]}, {"formula_id": "formula_105", "formula_text": ")", "formula_coordinates": [22.0, 535.57, 182.28, 4.43, 9.96]}, {"formula_id": "formula_106", "formula_text": "x m,t = x m,t + (A (km) ) t mix \u22121 x m,t\u2212t mix +1 =:\u03b4m,t = x m,t + \u03b4 m,t ,", "formula_coordinates": [22.0, 189.16, 236.37, 233.69, 25.93]}, {"formula_id": "formula_107", "formula_text": "\u03b4 m,t 2 \u2264 (A (km) ) t mix \u22121 \u2022 x m,t\u2212t mix +1 2 \u2264 \u03ba A \u03c1 t mix \u22121 x m,t\u2212t mix +1 2 ,(39)", "formula_coordinates": [22.0, 161.99, 282.83, 378.01, 12.35]}, {"formula_id": "formula_108", "formula_text": "k) t mix \u22121 ) is independent of \u03b4 m,t . Moreover,(", "formula_coordinates": [22.0, 154.72, 300.82, 175.66, 14.57]}, {"formula_id": "formula_109", "formula_text": "F i,j,\u03c4 = 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 (x m,t ) i x m,t \u2022 (x m,sj (t) ) i x m,sj (t) = 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 ( x m,t + A (km) \u03b4 m,t ) i ( x m,t + \u03b4 m,t ) \u2022 ( x m,sj (t) + A (km) \u03b4 m,sj (t) ) i ( x m,sj (t) + \u03b4 m,sj (t) ) = 1 |M subspace | \u2022 |S \u03c4 | m\u2208M subspace t\u2208S\u03c4 ( x m,t ) i x m,t \u2022 ( x m,sj (t) ) i x m,sj (t) =: Fi,j,\u03c4 + \u2206 i,j,\u03c4 ,(40)", "formula_coordinates": [22.0, 103.96, 379.14, 436.05, 126.8]}, {"formula_id": "formula_110", "formula_text": "\u2022 Recall the notation Y (k) t mix \u22121 = A (k) \u0393 (k) t mix \u22121 . It holds that E[ F i,j,\u03c4 ] = G i := K k=1 p (k) (Y (k) t mix \u22121 ) i (Y (k) t mix \u22121 ) i .", "formula_coordinates": [22.0, 86.95, 612.98, 330.13, 54.14]}, {"formula_id": "formula_111", "formula_text": "1 \u2264 i \u2264 d, F i,j,\u03c4 \u2212 G i \u0393 2 max d |M subspace | \u2022 |S \u03c4 | log 3 |M subspace | \u2022 |S \u03c4 |d \u03b4 \u0393 2 max t mix d T total,subspace log 3 T total,subspace d \u03b4 . (41", "formula_coordinates": [22.0, 156.93, 699.69, 321.85, 23.23]}, {"formula_id": "formula_112", "formula_text": ") \u2022 Recall from (32) that Y (k) \u2212 Y (k) t mix \u22121 \u2264 \u0393 max \u03ba 3 A \u03c1 2(t mix \u22121) . Therefore, (Y (k) ) i (Y (k) ) i \u2212 (Y (k) t mix \u22121 ) i (Y (k) t mix \u22121 ) i \u2264 (Y (k) ) i 2 + (Y (k) t mix \u22121 ) i 2 (Y (k) ) i \u2212 (Y (k) t mix \u22121 ) i 2 \u2264 2 Y (k) + Y (k) \u2212 Y (k) t mix \u22121 Y (k) \u2212 Y (k) t mix \u22121 \u2264 2\u0393 max \u03ba A + \u0393 max \u03ba 3 A \u03c1 2(t mix \u22121) \u2022 \u0393 max \u03ba 3 A \u03c1 2(t mix \u22121) = 2 + \u03ba 2 A \u03c1 2(t mix \u22121) \u2022 \u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) \u2264 3\u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) ,", "formula_coordinates": [23.0, 86.95, 82.79, 453.06, 146.1]}, {"formula_id": "formula_113", "formula_text": "Y (k) , Y (k) t mix \u22121 \u2264 \u0393 max \u03ba A . Consequently, G i \u2212 G i \u2264 K k=1 p (k) (Y (k) ) i (Y (k) ) i \u2212 (Y (k) t mix \u22121 ) i (Y (k) t mix \u22121 ) i \u2264 3\u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) .(42)", "formula_coordinates": [23.0, 96.91, 240.09, 443.09, 66.34]}, {"formula_id": "formula_114", "formula_text": "\u2206 i,j,\u03c4 \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .(43)", "formula_coordinates": [23.0, 220.48, 369.48, 319.52, 22.31]}, {"formula_id": "formula_115", "formula_text": "1 \u2264 i \u2264 d, G i \u2212 G i = N j=1 t mix \u03c4 =1 |S \u03c4 | N 2 \u2022 F i,j,\u03c4 \u2212 G i \u2264 max j,\u03c4 F i,j,\u03c4 \u2212 G i + G i \u2212 G i + max j,\u03c4 \u2206 i,j,\u03c4 \u0393 2 max t mix d T total,subspace log 3 T total,subspace d \u03b4 + \u0393 2 max \u03ba 4 A \u03c1 2(t mix \u22121) + \u0393 2 max \u2022 poly d, \u03ba A , log dT total \u03b4 \u2022 \u03c1 t mix \u22121 \u0393 2 max t mix d T total,subspace log 3 T total d \u03b4 ,", "formula_coordinates": [23.0, 76.98, 417.42, 468.52, 118.22]}, {"formula_id": "formula_116", "formula_text": "1 1\u2212\u03c1 log d\u03ba A T total \u03b4", "formula_coordinates": [23.0, 247.86, 549.02, 66.93, 13.7]}, {"formula_id": "formula_117", "formula_text": "Trunc(x; D) := x \u2022 1(|x| \u2264 D), x \u2208 R, D \u2265 0.", "formula_coordinates": [23.0, 196.55, 592.07, 218.9, 12.01]}, {"formula_id": "formula_118", "formula_text": "G Trunc i := 1 M N M m=1 N t=1 Trunc x m,t i ; D 0 x m,t Trunc z m,t i ; D 0 z m,t", "formula_coordinates": [23.0, 130.92, 638.16, 335.6, 30.2]}, {"formula_id": "formula_119", "formula_text": "G Trunc i \u2212 E Trunc i = sup u,v\u2208S d\u22121 u G Trunc i \u2212 E Trunc i v \u2264 4 sup u,v\u2208N 1/8 u G Trunc i \u2212 E Trunc i v,", "formula_coordinates": [24.0, 131.75, 96.54, 378.39, 19.62]}, {"formula_id": "formula_120", "formula_text": "u G Trunc i v = 1 M N M m=1 N t=1 Trunc x m,t i ; D 0 u x m,t \u2022 Trunc z m,t i ; D 0 v z m,t ,", "formula_coordinates": [24.0, 134.59, 162.65, 367.73, 30.2]}, {"formula_id": "formula_121", "formula_text": "\u2022 \u03c81 ) Trunc x m,t i ; D 0 , Trunc z m,t i ; D 0 \u2264 D 0 , u x m,t \u03c82 , v z m,t \u03c82 \u0393 max . Hence Trunc x m,t i ; D 0 u x m,t \u2022 Trunc z m,t i ; D 0 v z m,t \u03c81 D 2 0 \u0393 max ,", "formula_coordinates": [24.0, 96.91, 214.59, 414.7, 88.12]}, {"formula_id": "formula_122", "formula_text": "P u G Trunc i \u2212 E Trunc i v \u2265 \u03c4 \u2264 2 exp \u2212c 0 M N \u03c4 D 2 0 \u0393 max 2 for all 0 \u2264 \u03c4 \u2264 D 2 0 \u0393 max .", "formula_coordinates": [24.0, 96.91, 329.97, 355.33, 45.32]}, {"formula_id": "formula_123", "formula_text": "1 \u2212 \u03b4/2, G Trunc i \u2212 E Trunc i D 2 0 \u0393 max d + log 1 \u03b4 M N , provided that M N d + log 1 \u03b4 . \u2022 Note that G i \u2212 E Trunc i = K k=1 p (k) E xt,zt\u223cN (0, \u0393 (k) ) (x t ) i (z t ) i \u2212 Trunc (x t ) i Trunc (z t ) i x t z t ,", "formula_coordinates": [24.0, 86.95, 376.61, 436.55, 99.42]}, {"formula_id": "formula_124", "formula_text": "E xt,zt\u223cN (0, \u0393 (k) ) (x t ) i (z t ) i \u2212 Trunc (x t ) i Trunc (z t ) i x t z t = sup u,v\u2208S d\u22121 E xt,zt\u223cN (0, \u0393 (k) ) (x t ) i (z t ) i u x t v z t \u2022 1 \u2212 1 |(x t ) i | \u2264 D 0 , |(z t ) i | \u2264 D 0 \u2264 sup u,v\u2208S d\u22121 E (x t ) i (z t ) i u x t v z t 2 E 1 \u2212 1 |(x t ) i | \u2264 D 0 , |(z t ) i | \u2264 D 0 2 \u0393 2 max P(|N (0, \u0393 max )| > D 0 ) \u0393 2 max exp \u2212c 1 D 2 0 \u0393 max .", "formula_coordinates": [24.0, 117.23, 515.33, 388.39, 131.31]}, {"formula_id": "formula_125", "formula_text": "1 \u2264 i \u2264 d, if M N d + log(1/\u03b4), then P G i \u2212 G i D 2 0 \u0393 max d + log 1 \u03b4 M N + \u0393 2 max exp \u2212c 1 D 2 0 \u0393 max \u2265 1 \u2212 \u03b4 2 \u2212 m,t P |(x m,t ) i | > D 0 or |(z m,t ) i | > D 0 .", "formula_coordinates": [24.0, 150.56, 674.6, 337.87, 50.51]}, {"formula_id": "formula_126", "formula_text": "G i \u2212 G i D 2 0 \u0393 max d + log 1 \u03b4 M N \u0393 2 max d M N log 3 M N \u03b4 .", "formula_coordinates": [25.0, 184.17, 137.09, 248.65, 24.77]}, {"formula_id": "formula_127", "formula_text": "\u039b = U M U = U M U + U \u2206U = K k=1 p (k) U y (k) y (k) U + U \u2206U , \u039b = U M U = K k=1 p (k) U y (k) y (k) U .", "formula_coordinates": [25.0, 143.18, 253.23, 325.65, 65.73]}, {"formula_id": "formula_128", "formula_text": "K k=1 p (k) U y (k) y (k) U \u2212 U y (k) y (k) U = \u039b \u2212 \u039b + U \u2206U .", "formula_coordinates": [25.0, 161.9, 345.8, 288.19, 30.55]}, {"formula_id": "formula_129", "formula_text": "K k=1 p (k) U y (k) 2 2 \u2212 U y (k) 2 2 = Tr \u039b \u2212 \u039b + Tr U \u2206U .", "formula_coordinates": [25.0, 163.78, 403.18, 284.44, 30.55]}, {"formula_id": "formula_130", "formula_text": "U y (k) 2 2 \u2212 U y (k) 2 2 = y (k) 2 2 \u2212 U y (k) 2 2 = y (k) \u2212 U U y (k) 2 2 \u2265 0, while on the right-hand side, Tr(\u039b \u2212\u039b) = K k=1 \u03bb k (\u039b )\u2212\u03bb k (\u039b) (i) \u2264 K \u2206 \u2264 K , Tr(U \u2206U ) \u2264 \u2206 \u2022Tr(U U ) = \u2206 \u2022Tr(I K ) \u2264 K ,", "formula_coordinates": [25.0, 72.0, 459.71, 468.0, 70.71]}, {"formula_id": "formula_131", "formula_text": "stat = d i=1 U i 1 |\u2126 1 | t\u2208\u21261 (x t ) i x t \u2212 (z t ) i z t , U i 1 |\u2126 2 | t\u2208\u21262 (x t ) i x t \u2212 (z t ) i z t = U 1 |\u2126 1 | t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) , U 1 |\u2126 2 | t\u2208\u21262 vec x t (x t ) \u2212 z t (z t )", "formula_coordinates": [26.0, 117.55, 118.84, 356.75, 61.52]}, {"formula_id": "formula_132", "formula_text": "U := \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 U 1 0 . . . 0 0 U 2 . . . . . . . . . . . . . . . 0 0 . . . 0 U d \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u2208 R d 2 \u00d7dK ,(44)", "formula_coordinates": [26.0, 222.24, 209.24, 317.76, 59.69]}, {"formula_id": "formula_133", "formula_text": "t \u2208 \u2126 1 \u222a \u2126 2 , x t i.i.d. \u223c N (0, \u0393 (k) ), w t i.i.d. \u223c N (0, W (k) ), x t = A (k) x t + w t , z t i.i.d. \u223c N (0, \u0393 (l) ), v t i.i.d. \u223c N (0, W (l) ), z t = A (l) z t + v t ,", "formula_coordinates": [26.0, 140.05, 298.26, 331.9, 32.39]}, {"formula_id": "formula_134", "formula_text": "Y (k) := A (k) \u0393 (k) , \u03a3 (k) := A (k) \u2297 I d ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 I d 2 + P ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( A (k) ) \u2297 I d ,", "formula_coordinates": [26.0, 101.98, 398.15, 408.04, 31.03]}, {"formula_id": "formula_135", "formula_text": "\u00b5 k,l := U vec ( Y (k) \u2212 Y (l) ) \u2208 R dK ,(45a)", "formula_coordinates": [26.0, 158.58, 479.26, 381.42, 11.72]}, {"formula_id": "formula_136", "formula_text": "\u03a3 k,l := U \u03a3 (k) + W (k) \u2297 \u0393 (k) + \u03a3 ( ) + W (l) \u2297 \u0393 (l) U \u2208 R dK\u00d7dK .(45b)", "formula_coordinates": [26.0, 157.36, 499.74, 382.64, 11.72]}, {"formula_id": "formula_137", "formula_text": "E[stat] = \u00b5 k,l 2 2 , var(stat) \u2264 1 N 2 Tr(\u03a3 2 k,l ) + 2 N \u00b5 k,l \u03a3 k,l \u00b5 k,l ,", "formula_coordinates": [26.0, 172.88, 583.06, 266.25, 22.31]}, {"formula_id": "formula_138", "formula_text": "|\u2126 1 | = |\u2126 2 | = N .", "formula_coordinates": [26.0, 250.94, 612.84, 73.11, 10.32]}, {"formula_id": "formula_139", "formula_text": "\u0393 (k) , \u0393 (l) \u0393 max I d , W (k) , W (l) W max I d , A (k) , A (l) \u2264 \u03ba A", "formula_coordinates": [26.0, 159.18, 650.46, 293.14, 11.72]}, {"formula_id": "formula_140", "formula_text": "E[stat] = \u00b5 k,l 2 2 \u2264 Y (k) \u2212 Y (l) 2 F .(46)", "formula_coordinates": [26.0, 240.78, 711.12, 299.22, 12.69]}, {"formula_id": "formula_141", "formula_text": "1 \u2264 i \u2264 d, max ( Y (k) ) i \u2212 U i U i ( Y (k) ) i 2 , ( Y (l) ) i \u2212 U i U i ( Y (l) ) i 2 \u2264 ,(47)", "formula_coordinates": [27.0, 145.07, 96.71, 394.93, 11.72]}, {"formula_id": "formula_142", "formula_text": "E[stat] = \u00b5 k,l 2 2 \u2265 Y (k) \u2212 Y (l) 2 F \u2212 4 d i=1 ( Y (k) ) i + ( Y (l) ) i 2 .", "formula_coordinates": [27.0, 176.37, 140.7, 284.16, 30.32]}, {"formula_id": "formula_143", "formula_text": "0 \u03a3 k,l 6\u0393 2 max \u03ba 2 A I dK ;", "formula_coordinates": [27.0, 265.01, 202.64, 106.88, 12.69]}, {"formula_id": "formula_144", "formula_text": "var(stat) \u2264 1 N 2 Tr(\u03a3 2 k,l ) + 2 N \u00b5 k,l \u03a3 k,l \u00b5 k,l \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N Y (k) \u2212 Y (l) 2 F .", "formula_coordinates": [27.0, 131.05, 243.82, 374.81, 25.92]}, {"formula_id": "formula_145", "formula_text": "stat Y = d i=1 1 N t\u2208\u21261 U i (x t ) i x t \u2212 (z t ) i z t , 1 N t\u2208\u21262 U i (x t ) i x t \u2212 (z t ) i z t = U 1 N t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) , U 1 N t\u2208\u21262 vec x t (x t ) \u2212 z t (z t ) ,", "formula_coordinates": [27.0, 122.21, 364.81, 367.58, 61.52]}, {"formula_id": "formula_146", "formula_text": "N = T /4G = |\u2126 1 | = |\u2126 2 |.", "formula_coordinates": [27.0, 100.8, 437.01, 119.69, 9.65]}, {"formula_id": "formula_147", "formula_text": "1 N t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) = t mix \u03c41=1 |S 1,\u03c41 | N \u2022 1 |S 1,\u03c41 | t\u2208S 1,\u03c4 1 vec x t (x t ) \u2212 z t (z t ) , 1 N t\u2208\u21262 vec x t (x t ) \u2212 z t (z t ) = t mix \u03c42=1 |S 2,\u03c42 | N \u2022 1 |S 2,\u03c42 | t\u2208S 2,\u03c4 2 vec x t (x t ) \u2212 z t (z t ) .", "formula_coordinates": [27.0, 117.64, 563.02, 377.91, 67.26]}, {"formula_id": "formula_148", "formula_text": "stat Y = t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 stat \u03c41,\u03c42 Y ,where", "formula_coordinates": [27.0, 90.95, 658.54, 180.85, 30.33]}, {"formula_id": "formula_149", "formula_text": "w \u03c41,\u03c42 := |S 1,\u03c41 ||S 2,\u03c42 | N 2 , t mix \u03c41=1 t mix \u03c42=1", "formula_coordinates": [27.0, 88.26, 694.01, 147.94, 30.33]}, {"formula_id": "formula_150", "formula_text": "stat \u03c41,\u03c42 Y := U 1 |S 1,\u03c41 | t\u2208S 1,\u03c4 1 vec x t (x t ) \u2212 z t (z t ) , U 1 |S 2,\u03c42 | t\u2208S 2,\u03c4 2 vec x t (x t ) \u2212 z t (z t )", "formula_coordinates": [28.0, 79.86, 71.62, 414.42, 27.49]}, {"formula_id": "formula_151", "formula_text": "x t = A (k) t mix \u22121", "formula_coordinates": [28.0, 170.75, 143.52, 64.76, 14.49]}, {"formula_id": "formula_152", "formula_text": "+ x t = \u03b4 x,t + x t , x t = A (k) x t + w t = A (k) \u03b4 x,t + (A (k) x t + w t ) =: x t = A (k) \u03b4 x,t + x t ,", "formula_coordinates": [28.0, 167.95, 148.33, 276.09, 57.12]}, {"formula_id": "formula_153", "formula_text": "x t \u223c N (0, \u0393(k)", "formula_coordinates": [28.0, 101.1, 216.91, 65.53, 12.78]}, {"formula_id": "formula_154", "formula_text": "stat \u03c41,\u03c42 Y = stat \u03c41,\u03c42 Y + \u2206 \u03c41,\u03c42 Y , where stat \u03c41,\u03c42 Y := U 1 |S 1,\u03c41 | t\u2208S 1,\u03c4 1 vec x t ( x t ) \u2212 z t ( z t ) , U 1 |S 2,\u03c42 | t\u2208S 2,\u03c4 2 vec x t ( x t ) \u2212 z t ( z t ) ,", "formula_coordinates": [28.0, 94.3, 249.55, 423.39, 45.86]}, {"formula_id": "formula_155", "formula_text": "\u0393 (k) , \u0393 (l) ) = (\u0393 (k) t mix \u22121 , \u0393 ( ) t mix \u22121 ), ( W (k) , W (l) ) = (W (k) , W ( ) ) and ( A (k) , A (l) ) = (A (k) , A ( ) ), we have E stat \u03c41,\u03c42 Y = U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 , var stat \u03c41,\u03c42 Y \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 2 F ,", "formula_coordinates": [28.0, 96.91, 391.22, 443.09, 83.69]}, {"formula_id": "formula_156", "formula_text": "\u2206 \u03c41,\u03c42 Y \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .", "formula_coordinates": [28.0, 220.01, 539.83, 200.21, 22.31]}, {"formula_id": "formula_157", "formula_text": "stat Y,g = t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 stat \u03c41,\u03c42 Y,g = t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 stat \u03c41,\u03c42 Y,g =: stat Y,g + t mix \u03c41=1 t mix \u03c42=1 w \u03c41,\u03c42 \u2022 \u2206 \u03c41,\u03c42 Y,g =:\u2206 Y,g = stat Y,g + \u2206 Y,g ,where", "formula_coordinates": [28.0, 72.0, 595.45, 400.21, 103.94]}, {"formula_id": "formula_158", "formula_text": "E stat Y,g = U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 , var stat Y,g \u2264 max \u03c41,\u03c42 var( stat \u03c41,\u03c42 Y,g ) \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 2 F , \u2206 Y,g \u2264 max \u03c41,\u03c42 |\u2206 \u03c41,\u03c42 Y,g | \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .", "formula_coordinates": [28.0, 134.99, 708.05, 195.79, 16.11]}, {"formula_id": "formula_159", "formula_text": "\u0393 (k) , \u0393 (l) ) = (\u0393 (k) t mix \u22121 , \u0393 ( ) t mix \u22121 ), ( W (k) , W (l) ) = (0, 0), ( A (k) , A (l) ) = (I d , I d ), and subspaces {U i } replaced by {V i } instead. The final result is that, for all 1 \u2264 g \u2264 G, stat \u0393,g = stat \u0393,g + \u2206 \u0393,g , where E stat \u0393,g = V vec((\u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 ) ) 2 2 , var stat \u0393,g \u0393 2 max \u03ba 2 A N 2 dK + \u0393 2 max \u03ba 2 A N \u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 2 F , \u2206 \u0393,g \u2264 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 .", "formula_coordinates": [29.0, 72.0, 158.3, 468.0, 148.26]}, {"formula_id": "formula_160", "formula_text": "median{stat \u0393,g , 1 \u2264 g \u2264 G} + median{stat Y,g , 1 \u2264 g \u2264 G} \u2264 median{ stat \u0393,g } + median{ stat Y,g } + max g |\u2206 \u0393,g | + max g |\u2206 Y,g | \u2264 2 var( stat \u0393,g ) + 2 var( stat Y,g ) + \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 ;(49)", "formula_coordinates": [29.0, 149.24, 449.04, 390.76, 86.62]}, {"formula_id": "formula_161", "formula_text": "median{stat \u0393,g , 1 \u2264 g \u2264 G} + median{stat Y,g , 1 \u2264 g \u2264 G} \u2265 median{ stat \u0393,g } + median{ stat Y,g } \u2212 max g |\u2206 \u0393,g | + max g |\u2206 Y,g | \u2265 E[ stat \u0393,g ] + E[ stat Y,g ] \u2212 2 var( stat \u0393,g ) + var( stat Y,g ) \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 \u2265 V vec((\u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 ) ) 2 2 + U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 \u2212 C 0 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 F + Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 F \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 . (50", "formula_coordinates": [29.0, 96.91, 570.59, 449.2, 143.66]}, {"formula_id": "formula_162", "formula_text": ")", "formula_coordinates": [29.0, 535.57, 698.01, 4.43, 9.96]}, {"formula_id": "formula_163", "formula_text": "Y (k) \u2212 Y (k) t mix \u22121 F \u2264 \u0393 max \u221a d\u03ba 3 A \u03c1 2(t mix \u22121) =: mix , \u0393 (k) \u2212 \u0393 (k) t mix \u22121 F \u2264 \u0393 max \u221a d\u03ba 2 A \u03c1 2(t mix \u22121) \u2264 mix , which implies that \u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 F \u2264 \u0393 (k) \u2212 \u0393 ( ) F + 2 mix , V vec (\u0393 (k) t mix \u22121 \u2212 \u0393 ( ) t mix \u22121 ) 2 2 \u2265 max V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 \u2212 2 mix , 0 2 \u2265 V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 \u2212 4 mix V vec((\u0393 (k) \u2212 \u0393 ( ) ) ) 2 \u2265 V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 \u2212 4 mix \u0393 (k) \u2212 \u0393 ( ) F .", "formula_coordinates": [30.0, 72.0, 90.04, 441.0, 157.42]}, {"formula_id": "formula_164", "formula_text": "t mix \u22121 \u2212 Y ( ) t mix \u22121 F and U vec((Y (k) t mix \u22121 \u2212 Y ( ) t mix \u22121 ) ) 2 2 .", "formula_coordinates": [30.0, 222.84, 257.53, 229.46, 14.57]}, {"formula_id": "formula_165", "formula_text": "V vec((\u0393 (k) \u2212 \u0393 ( ) ) ) 2 2 + U vec((Y (k) \u2212 Y ( ) ) ) 2 2 \u2265 1 2 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F . (51)", "formula_coordinates": [30.0, 91.06, 292.6, 448.94, 22.52]}, {"formula_id": "formula_166", "formula_text": "median{stat \u0393,g , 1 \u2264 g \u2264 G} + median{stat Y,g , 1 \u2264 g \u2264 G} \u2265 V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 + U vec (Y (k) \u2212 Y ( ) ) 2 2 \u2212 4 mix \u0393 (k) \u2212 \u0393 ( ) F + Y (k) \u2212 Y ( ) F \u2212 C 0 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) \u2212 \u0393 ( ) F + Y (k) \u2212 Y ( ) F + 4 mix \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 (i) \u2265 1 2 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 0.01 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 C 1 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 \u0393 2 max \u2022 poly d, \u03ba A , log T total \u03b4 \u2022 \u03c1 t mix \u22121 (ii) \u2265 0.48 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 C 1 \u0393 2 max \u03ba 2 A \u221a dK N + \u0393 2 max \u03ba 2 A N \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F ,(52)", "formula_coordinates": [30.0, 116.08, 346.77, 423.92, 260.12]}, {"formula_id": "formula_167", "formula_text": "1 1\u2212\u03c1 log(( \u0393max \u2206 \u0393,Y + 2) d\u03ba A T total \u03b4 ) so that \u0393 2 max \u2022 poly(d, \u03ba A , log T total \u03b4 ) \u2022 \u03c1 t mix \u22121 \u2264 10 \u22123 \u2206 2 \u0393,Y", "formula_coordinates": [30.0, 72.0, 618.25, 474.41, 29.03]}, {"formula_id": "formula_168", "formula_text": "\u0393 2 max \u03ba 2 A \u221a dK \u2206 2 \u0393,Y", "formula_coordinates": [30.0, 73.2, 656.7, 42.32, 22.84]}, {"formula_id": "formula_169", "formula_text": "median{stat \u0393,g } + median{stat Y,g } \u2264 1 8 \u2206 2 \u0393,Y if k = , \u2265 3 8 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2265 3 8 \u2206 2 \u0393,Y if k = .", "formula_coordinates": [30.0, 95.88, 692.45, 419.04, 27.82]}, {"formula_id": "formula_170", "formula_text": "|\u2126 1 | = |\u2126 2 | = N for simplicity. Recall that stat = U 1 |\u2126 1 | t\u2208\u21261 vec x t (x t ) \u2212 z t (z t ) =:a , U 1 |\u2126 2 | t\u2208\u21262 vec x t (x t ) \u2212 z t (z t ) =:b = a, b ,", "formula_coordinates": [31.0, 99.08, 154.64, 413.85, 62.18]}, {"formula_id": "formula_171", "formula_text": "E[a] = E U vec x t (x t ) \u2212 z t (z t ) = U vec ( Y (k) \u2212 Y (l) ) = \u00b5 k,l .", "formula_coordinates": [31.0, 143.92, 251.35, 324.17, 11.72]}, {"formula_id": "formula_172", "formula_text": "E[stat] = E[a], E[b] = \u00b5 k,l 2 2 .", "formula_coordinates": [31.0, 236.85, 297.57, 138.31, 12.69]}, {"formula_id": "formula_173", "formula_text": ") = E[stat 2 ] \u2212 E[stat] 2", "formula_coordinates": [31.0, 260.88, 319.98, 93.88, 10.87]}, {"formula_id": "formula_174", "formula_text": "E[stat 2 ] = E (a b) 2 = Tr E[bb ]E[aa ] = Tr E[aa ] 2 .(53)", "formula_coordinates": [31.0, 178.77, 341.4, 361.23, 11.37]}, {"formula_id": "formula_175", "formula_text": "E[aa ] = E[a]E[a] + cov(a)", "formula_coordinates": [31.0, 95.45, 365.28, 126.83, 9.4]}, {"formula_id": "formula_176", "formula_text": "cov(a) = 1 N cov(f ), where f := U vec x t (x t ) \u2212 z t (z t ) \u2208 R dK .", "formula_coordinates": [31.0, 150.55, 385.26, 310.91, 22.31]}, {"formula_id": "formula_177", "formula_text": "cov(f ) = U (\u03a3 (k) + W (k) \u2297 \u0393 (k) + \u03a3 ( ) + W (l) \u2297 \u0393 (l) )U = \u03a3 k,l ,(54)", "formula_coordinates": [31.0, 162.52, 436.5, 377.49, 11.72]}, {"formula_id": "formula_178", "formula_text": "E[aa ] = cov(a) + E[a]E[a] = 1 N \u03a3 k,l + \u00b5 k,l \u00b5 k,l , E[aa ] 2 = 1 N 2 \u03a3 2 k,l + 1 N (\u03a3 k,l \u00b5 k,l \u00b5 k,l + \u00b5 k,l \u00b5 k,l \u03a3 k,l ) + \u00b5 k,l 2 2 \u00b5 k,l \u00b5 k,l ,", "formula_coordinates": [31.0, 150.68, 480.36, 310.64, 46.29]}, {"formula_id": "formula_179", "formula_text": "var(stat) = E[stat 2 ] \u2212 E[stat] 2 = Tr(E[aa ] 2 ) \u2212 \u00b5 k,l 4 2 = 1 N 2 Tr(\u03a3 2 k,l ) + 2 N \u00b5 k,l \u03a3 k,l \u00b5 k,", "formula_coordinates": [31.0, 116.29, 555.05, 373.01, 22.31]}, {"formula_id": "formula_180", "formula_text": "cov(f ) = U cov vec x(x ) \u2212 z(z ) U = U cov vec(x(x ) ) + cov vec(z(z ) ) U ,(55)", "formula_coordinates": [31.0, 108.95, 680.04, 431.06, 9.96]}, {"formula_id": "formula_181", "formula_text": "g := vec x x = vec(xx A ) + vec(xw ) = (A \u2297 I d )vec(xx ) + vec(xw ) = (A \u2297 I d )vec(\u0393 1/2 yy \u0393 1/2 ) + vec(xw ) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )vec(yy ) =:g1 + vec(xw ) =:g2 = g 1 + g 2 .", "formula_coordinates": [32.0, 136.12, 124.39, 339.77, 60.04]}, {"formula_id": "formula_182", "formula_text": "E[g 2 ] = 0, E[g] = E[g 1 ], and E[g 1 g 2 ] = 0. Hence cov(g) = E[gg ] \u2212 E[g]E[g] = cov(g 1 ) + E[g 2 g 2 ].(56)", "formula_coordinates": [32.0, 117.65, 196.71, 422.35, 32.54]}, {"formula_id": "formula_183", "formula_text": "g 2 = vec(xw ) = \uf8ee \uf8ef \uf8f0 w 1 x . . . w d x \uf8f9 \uf8fa \uf8fb , E[g 2 g 2 ] = E [w i w j xx ] 1\u2264i,j\u2264d = W \u2297 \u0393;", "formula_coordinates": [32.0, 148.08, 259.24, 315.84, 41.43]}, {"formula_id": "formula_184", "formula_text": ") = I d 2 + P ,(57)", "formula_coordinates": [32.0, 304.08, 335.95, 235.92, 10.6]}, {"formula_id": "formula_185", "formula_text": "cov(g 1 ) = cov (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )vec(yy ) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )cov vec(yy ) (\u0393 1/2 \u2297 \u0393 1/2 )(A \u2297 I d ) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )(I d 2 + P )(\u0393 1/2 \u2297 \u0393 1/2 )(A \u2297 I d ).", "formula_coordinates": [32.0, 149.79, 381.86, 312.41, 53.72]}, {"formula_id": "formula_186", "formula_text": "cov vec(x(x ) ) = cov(g) = (A \u2297 I d )(\u0393 1/2 \u2297 \u0393 1/2 )(I d 2 + P )(\u0393 1/2 \u2297 \u0393 1/2 )(A \u2297 I d ) + W \u2297 \u0393, which is equal to \u03a3 (k) + W (k) \u2297 \u0393 (k)", "formula_coordinates": [32.0, 72.0, 466.09, 443.42, 35.17]}, {"formula_id": "formula_187", "formula_text": "u := vec(yy ) = \uf8ee \uf8ef \uf8f0 y 1 y . . . y d y \uf8f9 \uf8fa \uf8fb . Then E[u] = vec(I d ), and thus E[u]E[u] = [e i e j ] 1\u2264i,j\u2264d . Next, consider E[uu ] = E [y i y j yy ] 1\u2264i,j\u2264d . \u2022 For i = j, E y 2 i y k y = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 3 if k = = i, 1 if k = = i, 0 if k = ,", "formula_coordinates": [32.0, 72.0, 548.74, 320.85, 154.41]}, {"formula_id": "formula_188", "formula_text": "E y i y j y k y = 1 if k = i, = j or k = j, = i, 0 otherwise,", "formula_coordinates": [33.0, 202.22, 88.06, 231.27, 24.31]}, {"formula_id": "formula_189", "formula_text": "(i, j)-th d \u00d7 d block of cov(u) = E[uu ] \u2212 E[u]E[u]", "formula_coordinates": [33.0, 166.83, 137.18, 219.79, 9.96]}, {"formula_id": "formula_190", "formula_text": "\u00b5 k,l 2 2 = d i=1 U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2264 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 = Y (k) \u2212 Y (l) 2 F ,", "formula_coordinates": [33.0, 122.24, 221.0, 372.51, 30.32]}, {"formula_id": "formula_191", "formula_text": "E[stat] = \u00b5 k,l 2 2 .", "formula_coordinates": [33.0, 214.54, 256.7, 75.34, 12.2]}, {"formula_id": "formula_192", "formula_text": "U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 = U i U i ( Y (k) ) i \u2212 U i U i ( Y (l) ) i 2 \u2265 max ( Y (k) ) i \u2212 ( Y (l) ) i 2 \u2212 2 , 0 , which implies that U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2265 ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2212 4 ( Y (k) ) i \u2212 ( Y (l) ) i 2 , and hence \u00b5 k,l 2 2 = d i=1 U i ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2265 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 2 \u2212 4 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 = Y (k) \u2212 Y (l) 2 F \u2212 4 d i=1 ( Y (k) ) i \u2212 ( Y (l) ) i 2 .", "formula_coordinates": [33.0, 72.0, 276.51, 456.23, 142.41]}, {"formula_id": "formula_193", "formula_text": "\u03a3 k,l = U \u03a3 (k) + W (k) \u2297 \u0393 (k) + \u03a3 ( ) + W (l) \u2297 \u0393 (l) U .", "formula_coordinates": [33.0, 182.79, 441.42, 246.42, 11.72]}, {"formula_id": "formula_194", "formula_text": "W (k) \u2297 \u0393 (k) W (k) \u0393 (k) I d 2 W max \u0393 max I d 2 ,and", "formula_coordinates": [33.0, 72.0, 500.8, 350.65, 28.33]}, {"formula_id": "formula_195", "formula_text": "0 \u03a3 (k) = A (k) \u2297 I d ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 I d 2 + P ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( A (k) ) \u2297 I d 2 A (k) \u2297 I d ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( \u0393 (k) ) 1/2 \u2297 ( \u0393 (k) ) 1/2 ( A (k) ) \u2297 I d = 2 A (k) \u2297 I d \u0393 (k) \u2297 \u0393 (k) ( A (k) ) \u2297 I d 2\u0393 2 max A (k) \u2297 I d ( A (k) ) \u2297 I d = 2\u0393 2 max A (k) ( A (k) ) \u2297 I d 2\u0393 2 max \u03ba 2 A I d 2 .", "formula_coordinates": [33.0, 95.41, 534.72, 414.74, 100.37]}, {"formula_id": "formula_196", "formula_text": "\u03a3 (k) + W (k) \u2297 \u0393 (k) (W max \u0393 max + 2\u0393 2 max \u03ba 2 A )I d 2 3\u0393 2 max \u03ba 2 A I d 2 .", "formula_coordinates": [33.0, 167.09, 658.69, 277.83, 12.69]}, {"formula_id": "formula_197", "formula_text": "\u03a3 k,l U (6\u0393 2 max \u03ba 2 A I d 2 )U = 6\u0393 2 max \u03ba 2 A U U = 6\u0393 2 max \u03ba 2", "formula_coordinates": [33.0, 176.36, 694.16, 237.08, 12.69]}, {"formula_id": "formula_198", "formula_text": "Y (k) = Y (k) , Y (l) = Y ( ) ) tells us that U vec (Y (k) \u2212 Y ( ) ) 2 2 \u2265 Y (k) \u2212 Y ( ) 2 F \u2212 4 d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 \u2265 Y (k) \u2212 Y ( ) 2 F \u2212 4 \u221a d Y (k) \u2212 Y ( ) F ,", "formula_coordinates": [34.0, 142.41, 105.2, 328.44, 67.78]}, {"formula_id": "formula_199", "formula_text": "d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 \u2264 d d i=1 (Y (k) ) i \u2212 (Y ( ) ) i 2 2 = \u221a d Y (k) \u2212 Y ( ) F .", "formula_coordinates": [34.0, 136.73, 203.48, 339.28, 30.32]}, {"formula_id": "formula_200", "formula_text": "V vec (\u0393 (k) \u2212 \u0393 ( ) ) 2 2 + U vec (Y (k) \u2212 Y ( ) ) 2 2 \u2265 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F \u2212 4 \u221a d \u0393 (k) \u2212 \u0393 ( ) F + Y (k) \u2212 Y ( ) F \u2265 1 2 \u0393 (k) \u2212 \u0393 ( ) 2 F + Y (k) \u2212 Y ( ) 2 F ,", "formula_coordinates": [34.0, 135.58, 262.59, 341.31, 63.99]}, {"formula_id": "formula_201", "formula_text": "(k) total , A (k) , A (k) , W (k) , W (k)", "formula_coordinates": [34.0, 336.75, 391.44, 118.9, 14.3]}, {"formula_id": "formula_202", "formula_text": "A = m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t+1 x m,t m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t x m,t \u22121 .", "formula_coordinates": [34.0, 158.56, 462.48, 294.89, 27.69]}, {"formula_id": "formula_203", "formula_text": "X := \uf8ee \uf8ef \uf8ef \uf8f0 . . . x m,t . . . \uf8f9 \uf8fa \uf8fa \uf8fb 0\u2264t\u2264Tm\u22121,m\u2208C k \u2208 R T \u00d7d , X + := \uf8ee \uf8ef \uf8ef \uf8f0 . . . x m,t+1 . . . \uf8f9 \uf8fa \uf8fa \uf8fb , N := \uf8ee \uf8ef \uf8ef \uf8f0 . . . w m,t . . . \uf8f9 \uf8fa \uf8fa \uf8fb , we have X + = AX + N , A = X + X(X X) \u22121 = A + N X(X X) \u22121 , namely \u2206 A := A \u2212 A = N X(X X) \u22121 .(58)", "formula_coordinates": [34.0, 72.0, 515.4, 468.0, 109.77]}, {"formula_id": "formula_204", "formula_text": "(V T ) \u22121/2 T t=1 x t z t d + log 1 \u03b4 + log det(V T ) det(V ) .", "formula_coordinates": [34.0, 191.47, 695.13, 234.59, 30.2]}, {"formula_id": "formula_205", "formula_text": "\u2206 A = (X X) \u22121/2 \u2022 (X X) \u22121/2 X N .(59)", "formula_coordinates": [35.0, 218.82, 95.0, 321.18, 12.69]}, {"formula_id": "formula_206", "formula_text": "X X D 2 vec T \u2022 I d =: V up .", "formula_coordinates": [35.0, 247.81, 150.79, 116.37, 12.69]}, {"formula_id": "formula_207", "formula_text": "X X 1 5 T \u2022 W =: V lb , provided that T \u03ba 2 w d \u2022 log \u0393 max W min \u03ba A dT total \u03b4 .(60)", "formula_coordinates": [35.0, 143.48, 195.15, 396.52, 23.23]}, {"formula_id": "formula_208", "formula_text": "\u2206 A \u2264 (X X) \u22121/2 \u2022 (X X) \u22121/2 X N .", "formula_coordinates": [35.0, 208.98, 250.77, 199.02, 11.72]}, {"formula_id": "formula_209", "formula_text": "(X X) \u22121/2 X N (X X + V ) \u22121/2 X N W 1/2 d + log 1 \u03b4 + log det(V up + V lb ) det(V lb ) W d \u2022 log \u0393 max W min d\u03ba A T total \u03b4 .(61)", "formula_coordinates": [35.0, 108.41, 311.56, 431.59, 54.76]}, {"formula_id": "formula_210", "formula_text": "\u2206 A 1 T \u2022 \u03bb min (W ) \u2022 W d \u2022 log \u0393 max W min d\u03ba A T total \u03b4 d \u2022 \u03ba w T log \u0393 max W min d\u03ba A T total \u03b4 ,", "formula_coordinates": [35.0, 112.72, 398.12, 391.55, 24.61]}, {"formula_id": "formula_211", "formula_text": "N := \uf8ee \uf8ef \uf8ef \uf8f0 . . . w m,t . . . \uf8f9 \uf8fa \uf8fa \uf8fb \u2208 R T \u00d7d , N = N \u2212 \u2206 A X = N \u2212 N X(X X) \u22121 X = N I T \u2212 X(X X) \u22121 X .", "formula_coordinates": [35.0, 123.62, 493.53, 364.76, 66.74]}, {"formula_id": "formula_212", "formula_text": "W = 1 T N N = 1 T N I T \u2212 X(X X) \u22121 X N ,", "formula_coordinates": [35.0, 194.85, 593.5, 222.3, 22.31]}, {"formula_id": "formula_213", "formula_text": "W \u2212 W = 1 T N N \u2212 W \u2212 1 T N X(X X) \u22121 X N .(62)", "formula_coordinates": [35.0, 181.14, 632.31, 358.87, 22.31]}, {"formula_id": "formula_214", "formula_text": "1 T N N \u2212 W W d + log 1 \u03b4 T .", "formula_coordinates": [35.0, 231.55, 697.26, 155.63, 24.77]}, {"formula_id": "formula_215", "formula_text": "1 T N X(X X) \u22121 X N = 1 T (X X) \u22121/2 X N (X X) \u22121/2 X N .", "formula_coordinates": [36.0, 142.78, 92.97, 327.63, 22.31]}, {"formula_id": "formula_216", "formula_text": "1 T N X(X X) \u22121 X N W d \u2022 log( \u0393max W min d\u03ba A T total \u03b4 ) T .", "formula_coordinates": [36.0, 189.66, 140.95, 239.42, 26.01]}, {"formula_id": "formula_217", "formula_text": "W \u2212 W \u2264 1 T N N \u2212 W + 1 T N X(X X) \u22121 X N W d + log 1 \u03b4 T + W d \u2022 log( \u0393max W min d\u03ba A T total \u03b4 ) T W d \u2022 log( \u0393max W min d\u03ba A T total \u03b4 ) T ,", "formula_coordinates": [36.0, 116.42, 192.16, 384.15, 54.65]}, {"formula_id": "formula_218", "formula_text": "W min d\u03ba A T total \u03b4", "formula_coordinates": [36.0, 254.97, 256.46, 48.03, 14.32]}, {"formula_id": "formula_219", "formula_text": "X X = m\u2208C k 0\u2264t\u2264Tm\u22121 x m,t x m,t m\u2208C k 1\u2264t\u2264Tm\u22121 x m,t x m,t = m\u2208C k 0\u2264t\u2264Tm\u22122 x m,t+1 x m,t+1 = m\u2208C k 0\u2264t\u2264Tm\u22122 (Ax m,t + w m,t )(Ax m,t + w m,t ) = m\u2208C k 0\u2264t\u2264Tm\u22122", "formula_coordinates": [36.0, 108.94, 323.62, 387.4, 78.07]}, {"formula_id": "formula_220", "formula_text": ":=P + m\u2208C k 0\u2264t\u2264Tm\u22122", "formula_coordinates": [36.0, 158.22, 409.76, 73.7, 35.37]}, {"formula_id": "formula_221", "formula_text": "=:Q = P + Q.(63)", "formula_coordinates": [36.0, 138.85, 453.44, 401.15, 21.96]}, {"formula_id": "formula_222", "formula_text": "1 T \u2212 |C k | P \u2212 W W d + log 1 \u03b4 T \u03bb min (W ) \u2022 I d , provided that T \u03ba 2 w (d + log 1 \u03b4 ).", "formula_coordinates": [36.0, 111.44, 513.87, 395.3, 25.69]}, {"formula_id": "formula_223", "formula_text": "1 T \u2212|C k | P 1 2 W , which implies P 1 4 T \u2022 W .", "formula_coordinates": [36.0, 164.36, 547.94, 194.02, 14.14]}, {"formula_id": "formula_224", "formula_text": "\u03bb min (Q) = inf v\u2208S d\u22121 v Qv = inf v\u2208S d\u22121 (\u03c0(v) + \u2206 v ) Q(\u03c0(v) + \u2206 v ) \u2265 inf v\u2208S d\u22121 \u03c0(v) Q\u03c0(v) \u2212 (2 + 2 ) Q \u2265 inf v\u2208N v Qv \u2212 3 Q . (64", "formula_coordinates": [36.0, 152.96, 621.64, 382.62, 36.88]}, {"formula_id": "formula_225", "formula_text": ")", "formula_coordinates": [36.0, 535.57, 642.58, 4.43, 9.96]}, {"formula_id": "formula_226", "formula_text": "Q \u2264 m\u2208C k 0\u2264t\u2264Tm\u22122 Ax m,t 2 2 + 2 Ax m,t 2 w m,t 2 \u0393 max \u2022 poly \u03ba A , d, T total , 1 \u03b4 . (65", "formula_coordinates": [36.0, 121.86, 697.23, 413.71, 27.47]}, {"formula_id": "formula_227", "formula_text": ")", "formula_coordinates": [36.0, 535.57, 703.3, 4.43, 9.96]}, {"formula_id": "formula_228", "formula_text": "v Qv = m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 2 m\u2208C k 0\u2264t\u2264Tm\u22122 v y m,t \u2022 u m,t W 1/2 v,", "formula_coordinates": [37.0, 143.66, 108.96, 324.69, 22.81]}, {"formula_id": "formula_229", "formula_text": "m\u2208C k 0\u2264t\u2264Tm\u22122 v y m,t \u2022 u m,t W 1/2 v \u2265 \u2212 \u221a v W v \u03bb 2 m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 1 \u03bb log 1 \u03b4 \u2265 \u2212 W \u03bb 2 m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 1 \u03bb log 1 \u03b4 .", "formula_coordinates": [37.0, 110.83, 173.47, 385.98, 61.96]}, {"formula_id": "formula_230", "formula_text": "v \u2208 N , v Qv \u2265 m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 \u2212 W \u03bb m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 + 2 \u03bb d \u2022 log 9 + log 1 \u03b4 = 1 \u2212 W \u03bb m\u2208C k 0\u2264t\u2264Tm\u22122 (v y m,t ) 2 \u2212 W 2 \u03bb d \u2022 log 9 + log 1 \u03b4 .", "formula_coordinates": [37.0, 72.0, 259.27, 431.89, 78.46]}, {"formula_id": "formula_231", "formula_text": "v Qv \u2265 \u22122 W (d \u2022 log 9 + log 1 \u03b4 ), for all v \u2208 N .(66)", "formula_coordinates": [37.0, 188.49, 371.19, 351.52, 22.31]}, {"formula_id": "formula_232", "formula_text": "\u03bb min (Q) \u2265 inf v\u2208N v Qv \u2212 3 Q \u2265 \u2212C 0 W d \u2022 log 9 + log 1 \u03b4 + \u0393 max \u2022 poly \u03ba A , d, T total , 1 \u03b4", "formula_coordinates": [37.0, 99.04, 423.65, 400.82, 22.31]}, {"formula_id": "formula_233", "formula_text": "X X P + Q 1 4 T \u2022 W \u2212 C 0 W d \u2022 log 9 + log 1 \u03b4 + \u0393 max \u2022 poly \u03ba A , d, T total , 1 \u03b4 I d .", "formula_coordinates": [37.0, 106.72, 516.86, 398.55, 22.31]}, {"formula_id": "formula_234", "formula_text": "1 poly(\u03ba A , d, T total , 1 \u03b4 , \u0393max W min ) , T \u03ba 2 w d \u2022 log \u0393 max W min \u03ba A dT total \u03b4 ,", "formula_coordinates": [37.0, 185.18, 570.0, 260.16, 26.71]}, {"formula_id": "formula_235", "formula_text": "L( A ( ) , W ( ) ) > L( A (k) , W (k) )(67)", "formula_coordinates": [37.0, 238.07, 711.12, 301.93, 11.37]}, {"formula_id": "formula_236", "formula_text": "L(A, W ) = T \u2022 log det(W ) + T \u22121 t=0 w t W \u22121 w t + T \u22121 t=0 x t (A (k) \u2212 A) W \u22121 (A (k) \u2212 A)x t + 2 T \u22121 t=0 w t W \u22121 (A (k) \u2212 A)x t .", "formula_coordinates": [38.0, 119.62, 291.55, 372.77, 65.03]}, {"formula_id": "formula_237", "formula_text": "L( A ( ) , W ( ) ) \u2212 L( A (k) , W (k) ) = (A) + (B) \u2212 (C) > 0, where (A) := T \u2022 log det( W ( ) ) \u2212 log det( W (k) ) + T \u22121 t=0 w t ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 w t (B) := T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t + 2 T \u22121 t=0 w t ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t (C) := T \u22121 t=0 x t (A (k) \u2212 A (k) ) ( W (k) ) \u22121 (A (k) \u2212 A (k) )x t + 2 T \u22121 t=0 w t ( W (k) ) \u22121 (A (k) \u2212 A (k) )x t", "formula_coordinates": [38.0, 76.88, 387.1, 451.79, 117.31]}, {"formula_id": "formula_238", "formula_text": "(A) \u2265 T \u2022 log det( W ( ) ) \u2212 log det( W (k) ) + Tr (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 \u2212 C 1 \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F log 1 \u03b4 ;(68)", "formula_coordinates": [38.0, 112.8, 600.14, 427.2, 44.25]}, {"formula_id": "formula_239", "formula_text": "(B) \u2265 C 2 T A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 \u03ba w \u03ba w,cross log 1 \u03b4 ,(69)", "formula_coordinates": [38.0, 213.11, 676.36, 326.89, 24.8]}, {"formula_id": "formula_240", "formula_text": "(C) \u2264 C 3 T D 2 x A (k) \u2212 A (k) 2 W min + \u03ba w log 1 \u03b4 ,(70)", "formula_coordinates": [39.0, 221.34, 96.22, 318.66, 24.8]}, {"formula_id": "formula_241", "formula_text": "(A) + (B) \u2212 (C) \u2265 C 4 \u2022 T \u2022 log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min \u2212 C 5 \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F + \u03ba w \u03ba w,cross log 1 \u03b4", "formula_coordinates": [39.0, 93.23, 174.64, 424.35, 83.88]}, {"formula_id": "formula_242", "formula_text": "D k, := log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min , D k, := 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max + A (k) \u2212 A ( ) 2 F \u2206 2 A,W \u03ba w,cross ,(71)", "formula_coordinates": [39.0, 100.69, 391.88, 439.31, 57.26]}, {"formula_id": "formula_243", "formula_text": "D k, D k, := log det(W ( ) ) det(W (k) ) + Tr W (k) (W ( ) ) \u22121 \u2212 (W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross ,(72)", "formula_coordinates": [39.0, 122.78, 479.36, 417.22, 24.8]}, {"formula_id": "formula_244", "formula_text": "D k, D k, , provided that A W min D k, D 2 x , W W min D k, d .(73)", "formula_coordinates": [39.0, 164.9, 554.54, 375.1, 24.19]}, {"formula_id": "formula_245", "formula_text": "(A) + (B) \u2212 (C) \u2265 C 6 T \u2022 D k, \u2212 C 5 \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F + \u03ba w \u03ba w,cross log 1 \u03b4 .(74)", "formula_coordinates": [39.0, 106.02, 617.5, 433.98, 37.59]}, {"formula_id": "formula_246", "formula_text": "D k, + 1 log 2 1 \u03b4 , then (A) + (B) \u2212 (C) \u2265 C 7 T \u2022 D k, > 0. (75", "formula_coordinates": [39.0, 208.47, 686.83, 327.11, 23.23]}, {"formula_id": "formula_247", "formula_text": ")", "formula_coordinates": [39.0, 535.57, 692.91, 4.43, 9.96]}, {"formula_id": "formula_248", "formula_text": "A W min D k, D 2 x , W W min \u2022 min 1, D k, d , T \uf8f1 \uf8f2 \uf8f3 \u03ba 2 w + \u03ba 5 w,cross D k, log 2 1 \u03b4 for Case 0, \u03ba 2 w + \u03ba 5 w,cross D k, log 2 1 \u03b4 + 1 1\u2212\u03c1 log(2\u03ba A ) for Case 1.", "formula_coordinates": [40.0, 76.04, 115.46, 470.29, 36.64]}, {"formula_id": "formula_249", "formula_text": "A W min \u2206 2 A,W \u0393 max \u03ba w,cross (d + log T total \u03b4 ) , W W min \u2022 min 1, \u2206 A,W \u03ba w,cross d , T \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W log 2 1 \u03b4 ;", "formula_coordinates": [40.0, 181.94, 208.77, 299.37, 58.86]}, {"formula_id": "formula_250", "formula_text": "A W min \u2206 2 A,W \u0393 max \u03ba w,cross \u03ba 2 A (d + log T total \u03b4 ) , W W min \u2022 min 1, \u2206 A,W \u03ba w,cross d , T \u03ba 2 w + \u03ba 6 w,cross \u2206 2 A,W log 2 1 \u03b4 + 1 1 \u2212 \u03c1 log(2\u03ba A ).", "formula_coordinates": [40.0, 181.94, 273.96, 311.6, 58.87]}, {"formula_id": "formula_251", "formula_text": "T \u22121 t=0 w t ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 w t = z M z,", "formula_coordinates": [40.0, 206.26, 417.6, 199.48, 30.2]}, {"formula_id": "formula_252", "formula_text": "P z M z \u2212 E[z M z] \u2265 u \u2264 2 exp \u2212 c min u 2 M 2 F , u M ,", "formula_coordinates": [40.0, 157.62, 500.8, 296.76, 26.23]}, {"formula_id": "formula_253", "formula_text": "M 2 F = T Q 2 F , M = Q , and E[z M z] = Tr(M ) = T \u2022 Tr(Q). Choosing u \u221a T Q F log 1 \u03b4 , we have with probability at least 1 \u2212 \u03b4, |z M z \u2212 T \u2022 Tr(Q)| \u2264 C 1 \u221a T Q F log 1", "formula_coordinates": [40.0, 72.0, 527.5, 468.0, 31.75]}, {"formula_id": "formula_254", "formula_text": "(B) = T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t + 2 T \u22121 t=0 u t y t .", "formula_coordinates": [40.0, 154.6, 624.97, 302.81, 30.2]}, {"formula_id": "formula_255", "formula_text": "T \u22121 t=0 u t y t \u2265 \u2212( \u03bb 2 T \u22121 t=0 y t 2 2 + 1 \u03bb log 2 \u03b4 ) for any fixed \u03bb > 0. This implies that (B) \u2265 T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t \u2212 \u03bb T \u22121 t=0 y t y t \u2212 2 \u03bb log 2 \u03b4 = T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 \u2212 \u03bb \u2022 ( W ( ) ) \u22121 W (k) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t \u2212 2 \u03bb log 2 \u03b4 .", "formula_coordinates": [40.0, 72.0, 662.64, 468.0, 62.69]}, {"formula_id": "formula_256", "formula_text": "T \u22121 t=0 x t (A (k) \u2212 A ( ) ) ( W ( ) ) \u22121 (A (k) \u2212 A ( ) )x t \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 \u2265 0.9\u03bb min ( W ( ) ) \u22121 T \u22121 t=0 x t (A (k) \u2212 A ( ) ) (A (k) \u2212 A ( ) )x t \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 .(76)", "formula_coordinates": [41.0, 139.12, 138.63, 400.88, 65.04]}, {"formula_id": "formula_257", "formula_text": "T \u22121 t=0 x t \u2206x t \u2265 T \u22122 t=0 x t+1 \u2206x t+1 = T \u22122 t=0 (A (k) x t + w t ) \u2206(A (k) x t + w t ) = T \u22122 t=0 w t \u2206w t (i) + T \u22122 t=0 x t A (k) \u2206A (k) x t + 2 T \u22122 t=0 w t \u2206A (k) x t (ii) .(77)", "formula_coordinates": [41.0, 138.74, 249.89, 401.26, 79.26]}, {"formula_id": "formula_258", "formula_text": "(i) \u2265 T \u2022 Tr (W (k) ) 1/2 \u2206(W (k) ) 1/2 \u2212 C 0 \u221a T (W (k) ) 1/2 \u2206(W (k) ) 1/2 F log 1 \u03b4 .", "formula_coordinates": [41.0, 137.74, 371.26, 336.52, 24.49]}, {"formula_id": "formula_259", "formula_text": "T \u22122 t=0 w t \u2206A (k) x t \u2264 \u03bb 2 T \u22122 t=0 x t (A (k) ) \u2206W (k) \u2206A (k) x t + 1 \u03bb log 2 \u03b4 , for any fixed \u03bb > 0, hence (ii) \u2265 T \u22122 t=0 x t A (k) (\u2206 \u2212 \u03bb \u2022 \u2206W (k) \u2206)A (k) x t \u2212 2 \u03bb log 2 \u03b4 . Recall \u2206 = \u2206 A \u2206 A , and thus \u2206 \u2212 \u03bb \u2022 \u2206W (k) \u2206 = \u2206 A (I d \u2212 \u03bb \u2022 \u2206 A W (k) \u2206 A )\u2206 A 0 if we choose \u03bb = 1/( W (k) \u2206 A 2 ) = 1/( W (k) \u2206 ); this implies that (ii) \u2265 \u2212 2 \u03bb log 2 \u03b4 = \u22122 W (k) \u2206 log 2 \u03b4 .", "formula_coordinates": [41.0, 72.0, 426.69, 468.0, 158.6]}, {"formula_id": "formula_260", "formula_text": "T \u22121 t=0 x t \u2206x t \u2265 (i) + (ii) \u2265 T \u2022 Tr (W (k) ) 1/2 \u2206(W (k) ) 1/2 \u2212 C 0 \u221a T (W (k) ) 1/2 \u2206(W (k) ) 1/2 F log 1 \u03b4 \u2212 2 W (k) \u2206 log 2 \u03b4 \u2265 T \u2022 \u03bb min (W (k) )Tr(\u2206 A \u2206 A ) \u2212 C 0 \u221a T W (k) \u2206 A \u2206 A F log 1 \u03b4 \u2212 2 W (k) \u2206 A \u2206 A log 2 \u03b4 \u2265 T \u2022 \u03bb min (W (k) ) A (k) \u2212 A ( ) 2 F \u2022 1 \u2212 C 0 \u221a T \u03ba(W (k) ) log 1 \u03b4 \u2212 2 T \u03ba(W (k) ) log 2 \u03b4 \u2265 0.9T \u2022 \u03bb min (W (k) ) A (k) \u2212 A ( ) 2 F ,", "formula_coordinates": [41.0, 72.0, 616.24, 469.9, 106.31]}, {"formula_id": "formula_261", "formula_text": "(B) \u2265 0.9\u03bb min (( W ( ) ) \u22121 ) T \u22121 t=0 x t \u2206x t \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 \u2265 0.81T \u2022 \u03bb min (( W ( ) ) \u22121 )\u03bb min (W (k) ) A (k) \u2212 A ( ) 2 F \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 \u2265 0.7T A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 40\u03ba w \u03ba w,cross log 2 \u03b4 ,", "formula_coordinates": [42.0, 141.09, 130.82, 328.62, 83.97]}, {"formula_id": "formula_262", "formula_text": "Denote \u2206 = A (k) \u2212 A (k) , u t = (W (k) ) \u22121/2 w t \u223c N (0, I d ) and y t = (W (k) ) 1/2 ( W (k) ) \u22121 \u2206x t . Then one has (C) = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + 2 T \u22121 t=0 w t ( W (k) ) \u22121 \u2206x t = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + 2 T t=1 u t y t .", "formula_coordinates": [42.0, 72.0, 270.07, 467.7, 52.51]}, {"formula_id": "formula_263", "formula_text": "T \u22121 t=0 u t y t \u2264 \u03bb 2 T \u22121 t=0 y t 2 2 + 1 \u03bb log 2 \u03b4 for any fixed \u03bb > 0. This implies that (C) \u2264 T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + \u03bb T \u22121 t=0 y t y t + 2 \u03bb log 2 \u03b4 = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 \u2206x t + \u03bb T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 W (k) ( W (k) ) \u22121 \u2206x t + 2 \u03bb log 2 \u03b4 = T \u22121 t=0 x t \u2206 ( W (k) ) \u22121 + \u03bb \u2022 ( W (k) ) \u22121 W (k) ( W (k) ) \u22121 \u2206x t + 2 \u03bb log 2 \u03b4 \u2264 1.5 1 \u03bb min (W (k) ) + \u03bb \u2022 W (k) \u03bb min (W (k) ) 2 T \u22121 t=0 x t \u2206 \u2206x t + 2 \u03bb log 2 \u03b4 .", "formula_coordinates": [42.0, 72.0, 332.66, 468.0, 169.8]}, {"formula_id": "formula_264", "formula_text": "1 W min T \u2022 D 2", "formula_coordinates": [42.0, 342.4, 512.27, 43.64, 14.1]}, {"formula_id": "formula_265", "formula_text": "D k, = log det W ( ) \u2212 log det W (k) + Tr W (k) (W ( ) ) \u22121 \u2212 I d + A (k) \u2212 A ( ) 2 F \u03ba w,cross = log det W ( ) \u2212 log det(W ( ) + \u2206) + Tr (W ( ) + \u2206)(W ( ) ) \u22121 \u2212 I d + A (k) \u2212 A ( ) 2 F \u03ba w,cross = log det W ( ) \u2212 log det (W ( ) ) 1/2 I d + (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 (W ( ) ) 1/2 + Tr (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 + A (k) \u2212 A ( ) 2 F \u03ba w,cross = Tr(X) \u2212 log det(I d + X) + A (k) \u2212 A ( ) 2 F \u03ba w,cross ,", "formula_coordinates": [42.0, 102.7, 590.5, 404.9, 134.48]}, {"formula_id": "formula_266", "formula_text": "X + I d = (W ( ) ) \u22121/2 W (k) (W ( ) ) \u22121/2 0, X \u2264 (W ( ) ) \u22121/2 2 W (k) \u2212 W ( ) \u2264 2W max W min = 2\u03ba w,cross , X 2 F = (W ( ) ) \u22121/2 \u2206(W ( ) ) \u22121/2 2 F \u2265 \u2206 2 F W 2 max .", "formula_coordinates": [43.0, 169.2, 91.2, 273.6, 67.81]}, {"formula_id": "formula_267", "formula_text": "Tr(X) \u2212 log det(I d + X) \u2265 X 2 F 6\u03ba w,cross \u2265 W (k) \u2212 W ( ) 2 F 6\u03ba w,cross W 2 max ,", "formula_coordinates": [43.0, 180.67, 181.24, 250.66, 25.77]}, {"formula_id": "formula_268", "formula_text": "D k, = Tr(X) \u2212 log det(I d + X) + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2265 W (k) \u2212 W ( ) 2 F 6\u03ba w,cross W 2 max + A (k) \u2212 A ( ) 2 F \u03ba w,cross D k, ,", "formula_coordinates": [43.0, 92.89, 227.68, 426.22, 25.77]}, {"formula_id": "formula_269", "formula_text": "D k, = log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min .", "formula_coordinates": [43.0, 89.29, 319.53, 433.42, 25.97]}, {"formula_id": "formula_270", "formula_text": "log det( W ( ) ) det( W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 = log det( W ( ) ) det(W (k) ) + Tr W (k) ( W ( ) ) \u22121 \u2212 (W (k) ) \u22121 (i) \u2212 log det( W (k) ) det(W (k) ) + Tr W (k) ( W (k) ) \u22121 \u2212 (W (k) ) \u22121 (ii)", "formula_coordinates": [43.0, 171.79, 370.81, 251.09, 117.58]}, {"formula_id": "formula_271", "formula_text": "(i) W (k) \u2212 W ( ) 2 F \u03ba w,cross W 2 max .", "formula_coordinates": [43.0, 254.56, 523.86, 102.89, 25.77]}, {"formula_id": "formula_272", "formula_text": "X = (W (k) ) \u22121/2 ( W (k) \u2212 W (k) )(W (k) ) \u22121/2 , one has (ii) = Tr(X) \u2212 log det(X + I d ) \u2264 X 2 F \u2264 W 2 d W 2 min .", "formula_coordinates": [43.0, 198.63, 555.0, 259.56, 44.0]}, {"formula_id": "formula_273", "formula_text": "D k, = (i) \u2212 (ii) + A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2212 D 2 x A (k) \u2212 A (k) 2 W min \u2265 C 1 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max + A (k) \u2212 A ( ) 2 F (iii) \u2212 C 2 W 2 d W 2 min + D 2 x A 2 W min (iv)(78)", "formula_coordinates": [43.0, 130.12, 622.12, 409.88, 69.91]}, {"formula_id": "formula_274", "formula_text": "\u2022 If 1 \u03baw,cross W (k) \u2212W ( ) 2 F W 2 max \u2265 D k,", "formula_coordinates": [44.0, 86.95, 72.35, 132.68, 17.87]}, {"formula_id": "formula_275", "formula_text": "W \u221a d W max \u2264 1 4 W (k) \u2212 W ( ) F W max ,", "formula_coordinates": [44.0, 257.2, 94.2, 125.62, 31.72]}, {"formula_id": "formula_276", "formula_text": "(iii) 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max \u2265 1 \u03ba w,cross W (k) \u2212 W ( ) F W max \u2212 W \u221a d W max 2 1 \u03ba w,cross W (k) \u2212 W ( ) 2 F W 2 max D k, . \u2022 On the other hand, if 1 \u03baw,cross A (k) \u2212 A ( ) 2 F \u2265 D k, 2", "formula_coordinates": [44.0, 86.95, 148.07, 390.94, 90.59]}, {"formula_id": "formula_277", "formula_text": "A \u221a d \u2264 1 4 A (k) \u2212 A ( ) F ,", "formula_coordinates": [44.0, 265.63, 247.6, 109.69, 24.55]}, {"formula_id": "formula_278", "formula_text": "(iii) A (k) \u2212 A ( ) 2 F \u03ba w,cross \u2265 ( A (k) \u2212 A ( ) F \u2212 A \u221a d) 2 \u03ba w,cross A (k) \u2212 A ( ) 2 F \u03ba w,cross D k, .", "formula_coordinates": [44.0, 145.82, 293.44, 345.26, 31.72]}, {"formula_id": "formula_279", "formula_text": "T \u2022 D k, . First, if T \u03baw\u03baw,cross D k, log 1 \u03b4 , then \u03ba w \u03ba w,cross log 1 \u03b4 T \u2022 D k, . Next, we have (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F \u2264 W (k) \u2022 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 F = W (k) \u2022 ( W ( ) ) \u22121 ( W (k) \u2212 W ( ) )( W (k) ) \u22121 F \u2264 W (k) \u2022 ( W ( ) ) \u22121 \u2022 ( W (k) ) \u22121 \u2022 W (k) \u2212 W ( ) F + 2 \u221a d W \u2264 2W max W 2 min W (k) \u2212 W ( ) F + 2 \u221a d W = 2\u03ba w,cross W min W (k) \u2212 W ( ) F + 2 \u221a d W .", "formula_coordinates": [44.0, 72.0, 408.2, 468.0, 132.55]}, {"formula_id": "formula_280", "formula_text": "(W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F \u03ba w,cross W min \u03ba w,cross W 2 max D k, + \u221a d W . Now it is easy to checked that if T \u03ba 5 w,cross D k, + \u03ba w,cross \u221a d W W min D k, 2 log 2 1 \u03b4 , then \u221a T (W (k) ) 1/2 ( W ( ) ) \u22121 \u2212 ( W (k) ) \u22121 (W (k) ) 1/2 F log 1 \u03b4 T \u2022 D k, .", "formula_coordinates": [44.0, 72.0, 570.44, 428.52, 106.45]}, {"formula_id": "formula_281", "formula_text": "\u03baw,cross \u221a d W W min D k, ) 2 \u03ba 2 w,cross D k,", "formula_coordinates": [44.0, 277.12, 682.56, 93.97, 21.23]}, {"formula_id": "formula_282", "formula_text": "1 \u2212 \u03b4, 1 N N t=1 a t a t \u2212 I d d + log 1 \u03b4 N , 1 N N t=1 a t b t d + log 1 \u03b4 N .", "formula_coordinates": [45.0, 94.65, 108.93, 357.43, 43.45]}, {"formula_id": "formula_283", "formula_text": "T t=1 u t y t < \u03bb 2 T t=1 y t 2 2 + 1 \u03bb log 2 \u03b4 .", "formula_coordinates": [45.0, 230.83, 222.0, 155.96, 30.2]}, {"formula_id": "formula_284", "formula_text": "E exp T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 = E exp T \u22121 t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2022 E exp \u03bb \u2022 u T y T \u2212 1 2 \u03bb 2 y T 2 2 |F T \u22121 \u22641 \u2264 E exp T \u22121 t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2", "formula_coordinates": [45.0, 118.65, 280.39, 364.25, 109.14]}, {"formula_id": "formula_285", "formula_text": "T t=1 (\u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2", "formula_coordinates": [45.0, 314.32, 398.52, 110.59, 14.56]}, {"formula_id": "formula_286", "formula_text": "P T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2265 z = P exp T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2265 exp(z) \u2264 exp(\u2212z) \u2022 E exp T t=1 \u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 \u2264 exp(\u2212z) = \u03b4 2 .", "formula_coordinates": [45.0, 79.74, 433.08, 452.52, 65.03]}, {"formula_id": "formula_287", "formula_text": "T t=1 (\u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 ) < z = log(2/\u03b4), which implies T t=1 u t y t < \u03bb 2 T t=1 y t 2 2 + 1 \u03bb log 2 \u03b4 if \u03bb > 0.", "formula_coordinates": [45.0, 72.0, 507.11, 468.0, 28.77]}, {"formula_id": "formula_288", "formula_text": "T t=1 (\u2212\u03bb \u2022 u t y t \u2212 1 2 \u03bb 2 y t 2 2 ) < log(2/\u03b4), which implies T t=1 u t y t > \u2212( \u03bb 2 T t=1 y t 2 2 + 1 \u03bb log 2 \u03b4 ) if \u03bb > 0.", "formula_coordinates": [45.0, 82.52, 535.52, 457.48, 28.77]}, {"formula_id": "formula_289", "formula_text": "Tr(X) \u2212 log det(I d + X) \u2265 X 2 F 3B .", "formula_coordinates": [45.0, 229.94, 603.74, 152.12, 23.89]}, {"formula_id": "formula_290", "formula_text": "Tr(X) \u2212 log det(I d + X) \u2264 X 2 F .", "formula_coordinates": [45.0, 231.13, 652.62, 149.73, 12.69]}, {"formula_id": "formula_291", "formula_text": "\u03bb i > \u22121 for all 1 \u2264 i \u2264 d. Then Tr(X) \u2212 log det(I d + X) = d i=1 \u03bb i \u2212 log d i=1 (1 + \u03bb i ) = d i=1 \u03bb i \u2212 log(1 + \u03bb i ) .", "formula_coordinates": [45.0, 139.91, 674.33, 355.08, 51.0]}, {"formula_id": "formula_292", "formula_text": "Tr(X) \u2212 log det(I d + X) \u2265 d i=1 \u03bb 2 i 3B = X 2 F 3B ,", "formula_coordinates": [46.0, 207.56, 98.48, 196.89, 30.32]}, {"formula_id": "formula_293", "formula_text": "I d + X) \u2264 d i=1 \u03bb 2 i = X 2 F ;", "formula_coordinates": [46.0, 283.73, 168.6, 116.95, 30.32]}, {"formula_id": "formula_294", "formula_text": "\u221e i=0 A i 2 W \u2264 W max \u03ba 2 A \u221e i=0 \u03c1 2i \u2264 W max \u03ba 2", "formula_coordinates": [46.0, 112.65, 257.44, 197.64, 14.11]}, {"formula_id": "formula_295", "formula_text": "A (k) \u2212 A ( ) = Y (k) \u0393 (k) \u22121 \u2212 Y ( ) \u0393 ( ) \u22121 = (Y (k) \u2212 Y ( ) )\u0393 (k) \u22121 + Y ( ) (\u0393 (k) \u22121 \u2212 \u0393 ( ) \u22121 ),", "formula_coordinates": [46.0, 126.24, 385.33, 384.43, 13.58]}, {"formula_id": "formula_296", "formula_text": "A (k) \u2212 A ( ) F \u2264 Y (k) \u2212 Y ( ) F \u0393 (k) \u22121 + A ( ) \u0393 ( ) \u2212 \u0393 (k) F \u0393 (k) \u22121 \u2264 1 W min Y (k) \u2212 Y ( ) F + \u03ba A W min \u0393 ( ) \u2212 \u0393 (k) F ,(79)", "formula_coordinates": [46.0, 159.42, 473.41, 380.58, 42.06]}, {"formula_id": "formula_297", "formula_text": "W (k) \u2212 W ( ) = (\u0393 (k) \u2212 \u0393 ( ) ) \u2212 (A (k) Y (k) \u2212 A ( ) Y ( ) ), W (k) \u2212 W ( ) F \u2264 \u0393 (k) \u2212 \u0393 ( ) F + A (k) Y (k) \u2212 A ( ) Y ( ) F .", "formula_coordinates": [46.0, 181.25, 568.42, 279.38, 31.02]}, {"formula_id": "formula_298", "formula_text": "A (k) Y (k) \u2212 A ( ) Y ( ) F = A (k) (Y (k) \u2212 Y ( ) ) + (A (k) \u2212 A ( ) )Y ( ) F \u2264 A (k) (Y (k) \u2212 Y ( ) ) F + (A (k) \u2212 A ( ) )Y ( ) F \u2264 \u03ba A Y (k) \u2212 Y ( ) F + \u03ba A \u0393 max A (k) \u2212 A ( ) F ,", "formula_coordinates": [46.0, 150.05, 630.58, 341.29, 47.94]}, {"formula_id": "formula_299", "formula_text": "W (k) \u2212 W ( ) F \u2264 \u0393 (k) \u2212 \u0393 ( ) F + \u03ba A Y (k) \u2212 Y ( ) F + \u03ba A \u0393 max A (k) \u2212 A ( ) F .(80)", "formula_coordinates": [46.0, 145.98, 711.12, 394.02, 11.72]}, {"formula_id": "formula_300", "formula_text": "\u2206 A 2 F + \u2206 W 2 F W 2 max \u2264 \u2206 A 2 F + 3 W 2 max \u2206 \u0393 2 F + \u03ba 2 A \u2206 Y 2 F + \u03ba 2 A \u0393 2 max \u2206 A 2 F \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 1 + 3\u03ba 2 A \u0393 2 max W 2 max \u2206 A 2 F \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 4\u03ba 2 A \u0393 2 max W 2 max \u2206 A 2 F ,", "formula_coordinates": [47.0, 142.99, 106.93, 324.55, 82.03]}, {"formula_id": "formula_301", "formula_text": "2 F \u2264 2 W 2 min \u2206 Y 2 F + 2\u03ba 2 A W 2 min \u2206 \u0393 2 F", "formula_coordinates": [47.0, 73.2, 198.14, 466.8, 34.12]}, {"formula_id": "formula_302", "formula_text": "\u2206 A 2 F + \u2206 W 2 F W 2 max \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 4\u03ba 2 A \u0393 2 max W 2 max \u2206 A 2 F \u2264 3 W 2 max \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max \u2206 Y 2 F + 4\u03ba 2 A \u0393 2 max W 2 max 2 W 2 min \u2206 Y 2 F + 2\u03ba 2 A W 2 min \u2206 \u0393 2 F = 3 W 2 max + 4\u03ba 2 A \u0393 2 max W 2 max 2\u03ba 2 A W 2 min \u2206 \u0393 2 F + 3\u03ba 2 A W 2 max + 4\u03ba 2 A \u0393 2 max W 2 max 2 W 2 min \u2206 Y 2 F \u2264 11\u03ba 4 A \u0393 2 max W 2 max W 2 min \u2206 \u0393 2 F + \u2206 Y 2 F .", "formula_coordinates": [47.0, 110.02, 241.82, 390.5, 111.6]}, {"formula_id": "formula_303", "formula_text": "2 F + \u2206 Y 2 F < \u2206 2 \u0393,Y , then \u2206 A 2 F + \u2206 W 2 F W 2 max < 11\u03ba 4 A \u0393 2 max W 2 max W 2 min \u2206 2 \u0393,Y . Equivalently (by contraposition), if \u2206 A 2 F + \u2206 W 2 F W 2 max \u2265 \u2206 2 A,W , then \u2206 \u0393 2 F + \u2206 Y 2 F \u2265 W 2 max W 2 min 11\u03ba 4 A \u0393 2 max \u2206 2 A,W", "formula_coordinates": [47.0, 72.0, 362.68, 468.0, 36.9]}], "doi": ""}