{"title": "Unsupervised Morphology Induction Using Word Embeddings", "authors": "Radu Soricut; Franz Och", "pub_date": "", "abstract": "We present a language agnostic, unsupervised method for inducing morphological transformations between words. The method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages.", "sections": [{"heading": "Introduction", "text": "Word representations obtained via neural networks (Bengio et al., 2003;Socher et al., 2011a) or specialized models (Mikolov et al., 2013a) have been used to address various natural language processing tasks (Mnih et al., 2009;Huang et al., 2014;Bansal et al., 2014). These vector representations capture various syntactic and semantic properties of natural language (Mikolov et al., 2013b). In many instances, natural language uses a small set of concepts to render a much larger set of meaning variations via morphology. We show in this paper that morphological transformations can be captured by exploiting regularities present in wordrepresentations as the ones trained using the Skip-Gram model (Mikolov et al., 2013a).\nIn contrast to previous approaches that combine morphology with vector-based word representations (Luong et al., 2013;Botha and Blunsom, 2014), we do not rely on an external morphological analyzer, such as Morfessor (Creutz and La-gus, 2007). Instead, our method automatically induces morphological rules and transformations, represented as vectors in the same embedding space.\nAt the heart of our method is the SkipGram model described in (Mikolov et al., 2013a). We further exploit the observations made by Mikolov et al (2013b), and further studied by (Levy and Goldberg, 2014;Pennington et al., 2014), regarding the regularities exhibited by such embedding spaces. These regularities have been shown to allow inferences of certain types (e.g., king is to man what queen is to woman). Such regularities also hold for certain morphological relations (e.g., car is to cars what dog is to dogs). In this paper, we show that one can exploit these regularities to model, in a principled way, prefix-and suffix-based morphology. The main contributions of this paper are as follows:\n1. provides a method by which morphological rules are learned in an unsupervised, languageagnostic fashion;\n2. provides a mechanism for applying these rules to known words (e.g., boldly is analyzed as bold+ly, while only is not);\n3. provides a mechanism for applying these rules to rare and unseen words;\nWe show that this method improves state-of-the-art performance on a word-similarity rating task using standard datasets. We also quantify the impact of our morphology treatment when using large amounts of training data (tens/hundreds of billions of words).\nThe technique we describe is capable of inducing transformations that cover both typical, regular morphological rules, such as adding suffix ed to verbs in English, as well as exceptions to such rules, such as the fact that pluralization of words that end in y require substituting it with ies. Because each such transformation is represented in the high-dimensional embedding space, it therefore captures the semantics of the change. Consequently, it allows us to build vector representations for any unseen word for which a morphological analysis is found, therefore covering an unbounded (albeit incomplete) vocabulary.\nOur empirical evaluations show that this language-agnostic technique is capable of learning morphological transformations across various language families. We present results for English, German, French, Spanish, Romanian, Arabic, and Uzbek. The results indicate that the induced morphological analysis deals successfully with sophisticated morphological variations.", "publication_ref": ["b2", "b29", "b21", "b24", "b11", "b1", "b22", "b21", "b19", "b4", "b21", "b22", "b17", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Previous Work", "text": "Many recent proposals in the literature use wordrepresentations as the basic units for tackling sentence-level tasks such as language modeling (Mnih and Hinton, 2007;Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), discriminative parsing (Collobert, 2011), as well as similar tasks involving larger units such as documents (Glorot et al., 2011;Huang et al., 2012;Le and Mikolov, 2014). The main advantage offered by these techniques is that they can be both trained in an unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words.\nPrevious attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor-phology using vector-space representations (Luong et al., 2013;Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representations for existing morphemes, and also to combine them into (possibly novel) embedding representations for words that may not have been seen at training time.\nCommon to these proposals is the fact that the morphological analysis of words is treated as an external, preprocessing-style step. This step is done using off-the-shelf analyzers such as Morfessor (Creutz and Lagus, 2007). As a result, the morphological analysis happens within a different model compared to the model in which the resulting morphemes are consequently used. In contrast, the work presented here uses the same vector-space embedding to achieve both the morphological analysis of words and to compute their representation. As a consequence, the morphological analysis can be justified in terms of the relationship between the resulting representation and other words that exhibit similar morphological properties.", "publication_ref": ["b23", "b20", "b29", "b30", "b5", "b8", "b10", "b16", "b0", "b14", "b19", "b4", "b19", "b4", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Morphology Induction using Embedding Spaces", "text": "The method we present induces morphological transformations supported by evidence in terms of regularities within a word-embedding space. We describe in this section the algorithm used to induce such transformations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Morphological Transformations", "text": "We consider two main transformation types, namely prefix and suffix substitutions. Other transformation types can also be considered, but we restrict the focus of this work to morphological phenomena that can be modeled via prefixes and suffixes. We provide first a high-level description of our algorithm, followed by details regarding the individual steps. The following steps are applied to monolingual training data over a finite vocabulary V :\n1. Extract candidate prefix/suffix rules from V 2. Train embedding space E n \u2282 R n for all words in V 3. Evaluate quality of candidate rules in E n", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generate lexicalized morphological transformations", "text": "We provide more detailed descriptions next.\nExtract candidate rules from V Starting from (w 1 , w 2 ) \u2208 V 2 , the algorithm extracts all possible prefix and suffix substitutions from w 1 to w 2 , up to a specified size 1 . We denote such substitutions using triplets of the form type:from:to. For instance, triplet suffix:ed:ing denotes the substitution of suffix ed with suffix ing; this substitution is supported by many word pairs in an English vocabulary, e.g. (bored, boring), (stopped, stopping), etc. We call these triplets candidate rules, because they form the basis of an extended set from which the algorithm extracts morphological rules.\nAt this stage, the candidate rules set contains both rules that reflect true morphology phenomena, e.g. suffix:s: (replace suffix s with the null suffix, extracted from (stops, stop), (weds, wed), etc.), or prefix:un: (replace prefix un with the null prefix, from (undone, done), etc.), but also rules that simply reflect surface-level coincidences, e.g. prefix:S: (delete S at the beginning of a word, from (Scream, cream), (Scope, cope), etc.).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Train embedding space", "text": "Using a large monolingual corpus, we train a word-embedding space E n of dimensionality n for all words in V using the SkipGram model (Mikolov et al., 2013a). For the experiments reported in this paper, we used our own implementation of this model (which varies only slightly from the publiclyavailable word2vec implementation 2 ).", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluate quality of candidate rules", "text": "The extracted candidate rules set is evaluated by using, for each proposed rule r, its support set:\nS r = {(w 1 , w 2 ) \u2208 V 2 |w 1 r \u2212\u2192 w 2 }\nThe notation w 1 r \u2192 w 2 means that rule r applies to word w 1 (e.g., for rule suffix:ed:ing, word w 1  ends with suffix ed), and the result of applying the rule to word w 1 is word w 2 . To speed up computation, we downsample the sets S r to a large-enough number of word pairs (1000 has been used in the experiments in this paper).\nWe define a generic evaluation function Ev F over paired couples in S r \u00d7S r , using a function F : R n \u00d7 R n \u2192 R, as follows:\nEv F ((w 1 , w 2 ), (w, w )) = F E (w 2 , w 1 + \u2191 d w ) (1) (w 1 , w 2 ), (w, w ) \u2208 S r , \u2191 d w = w \u2212 w\nWord-pair combinations in S r \u00d7S r are evaluated using Eq. 1 to assess the meaning-preservation property of rule r. We use as F E function rank E , the cosine-similarity rank function in E n . We can quantitatively measure the assertion \"car is to cars what dog is to dogs\", as rank E (cars, car +\u2191d dog ). We use a single threshold t 0 rank to capture meaning preservation (all the experiments in this paper use t 0 rank = 100): for each proposed rule r, we compute a hit rate based on the number of times Eq. 1 scores above t 0 rank , over the number of times it has been evaluated. In Table 1 we present some of these candidate rules and their hit rate.\nWe note that rules that are non-meaningpreserving receive low hit rates, while rules that are morphological in nature, such as suffix:ed:ing (verb change from past/participle to presentcontinuous) and suffix:y:ies (pluralization of y-ending nouns), receive high hit rates.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Generate lexicalized morphological transformations", "text": "The results in Table 1 indicate the need for creating lexicalized transformations. For instance, rule suffix:ly: (drop suffix ly, a perfectly reasonable morphological transformation in English) is evaluated to have a hit rate of 32.1%. While such transformations are desirable, we want to avoid applying them when firing without yielding meaningpreserving results (the rest of 67.9%), e.g., for wordpair (only, on). We therefore create lexicalized transformations by restricting the rule application to the vocabulary subset of V which passes the meaningpreservation criterion.\nThe algorithm also computes best direction vectors \u2191d w for each rule support set S r . It greedily selects a direction vector \u2191d w 0 that explains (based on Equation 1) the most pairs in S r . After subset S w 0 r is computed for direction vector \u2191d w 0 , it applies recursively on set S r \u2212 S w 0 r . This yields a new best direction vector \u2191d w 1 , and so on. The recursion stops when it finds a direction vector \u2191d w k that explains less than a predefined number of words (we used 10 in all the experiments from this paper).\nWe consider multiple direction vectors \u2191d w i because of the possibly-ambiguous nature of a morphological transformation.\nConsider rule suffix: :s, which can be applied to the noun walk to yield plural-noun walks; this case is modeled with a transformation like walk + \u2191d invention , since \u2191d invention =inventions\u2212invention is a direction that our procedure deems to explain well noun pluralization; it can also be applied to the verb walk to yield the 3rd-person singular form of the verb, in which case it is modeled as walk + \u2191d enlist , since \u2191d enlist =enlists\u2212enlist is a direction that our procedure deems to explain well 3rd-person singular verb forms. In that sense, our algorithm goes beyond proposing simple surface-level morphemes, with direction vectors encoding well-defined semantics for our morphological analysis.\nLexicalized rules enhanced with direction vectors are called morphological transformations. For each morphological transformation, we evaluate again how well it passes a proximity test in E n for the words it applies to. As evaluation criteria, we use two instances of Eq 1, with F E instantiated to rank E and cosine E , respectively. We apply more stringent criteria in this second pass, using thresholds on the resulting rank (t rank ) and cosine (t cosine ) values to indicate meaning preservation (we used t rank = 30 and t cosine = 0.5 in all the experiments in this paper). We present in Table 2 a sample of the results of this procedure. For instance, word create can be transformed to creates using two different transformations: suffix:te:tes:\u2191evaluate and suffix: :s:\u2191contradict, passing the meaning-preservation criteria with rank=0, co-sine=0.65, and rank=1, cosine=0.62, respectively.\nLexicalized morphological transformations over a vocabulary V have a graph-based interpretation: words represent nodes, transformations represent edges in a labeled, weighted, cyclic, directed multigraph (weights are (r, c) pairs, rank and cosine values; multiple direction vectors create multiple edges between two nodes; cycles may exist, see e.g. created\u2192create\u2192created in Table 2). We use the notation G V M orph to denote such a graph. G V M orph usually contains many strongly connected components, with components representing families of morphological variations. As an illustration, we present in Figure 1 a few strongly connected components obtained for an English embedding space (for illustration purposes, we show only a maximum of 2 directed edges between any two nodes in this multigraph, even though more may exist).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_1", "tab_3", "tab_3"]}, {"heading": "Inducing 1-to-1 Morphological Mappings", "text": "The induced graph G V M orph encodes a lot of information about words and how they relate to each other. For some applications, however, we want to normalize away morphological diversity by mapping to a canonical surface form. This amounts to selecting, from among all the candidate morphological transformations generated, specific 1-to-1 mappings. In graph terms, this means building a labeled, weighted, acyclic, directed graph D V M orph starting from G V M orph , using the nodes from G V M orph and retaining only edges that meet certain criteria.\nFor the experiments presented in Section 4, we build a directed graph D V M orph as follows:\n1. edge w 1\n(r,c) \u2192 w 2 in G V M orph is considered only if count(w 1 ) \u2264 count(w 2 ) in V ;\n2. if multiple such edges exist, chose the one with minimal rank r;\n3. if multiple such edges still exist, chose the one with the maximal cosine c.\nThe interpretation we give is word-normalization: a normalization of w to w is guaranteed to be meaning preserving (using the direction-vector semantics), and to a more frequent form. A snippet of the resulting graph D V M orph is presented in Figure 2. One notable aspect of this normalization procedure is that these are not \"traditional\" morphological mappings, with morphology-inflected words mapped to their linguistic roots. Rather, our method produces morphological mappings that favor frequency over linguistic normalization. An example of this can be seen in Figure 2, where the root form create is morphologically-explained by mapping it to the form created. This choice is purely based on our desire to favor the accuracy of the word-representations for the normal forms; different choices regarding how this pruning procedure is performed lead to different normalization procedures, including some that are more linguisticallymotivated (e.g., length-based).", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Morphological Transformations for Rare and Unknown Words", "text": "For some count threshold C, we define V C = {w \u2208 V |C \u2264 count(w)}. The method we presented up to this point induces a morphology graph D V C M orph that can be used to perform morphological analysis for any words in V C . We analyze the rest of the words we may encounter (i.e., rare words and OOVs) by mapping them directly to nodes in D V C M orph . We extract such mappings from D V C M orph using all the sequences of edges that start at nodes in the graph and end in a normal-form (i.e., nodes that have out-degree 0). The result is a set of rule sequences denoted RS. A count cutoff on the rule sequence counts is used, since low-count sequences tend to be less reliable (in the experiments reported in this paper we use a cutoff of 50). We also denote with R the set of all edges in D M orph . Using sets RS and R, we map w \u2208 V C to a node w \u2208 D V C M orph , as follows:\n1. for rule-sequences s \u2208 RS from highest-tolowest count, if w s \u2192 w and w \u2208 D V C M orph , then s is the morphological analysis for w; 2. if no s is found, do breadth-first search in D V C M orph using r \u2208 R, up to a predefined 3 depth d; for k \u2264 d, word w with w\nr 1 ...r k \u2212\u2192 w \u2208 D V C\nM orph and the highest count in V C is the morphological analysis for w.\nFor example, this procedure uses the RS sequence s=prefix : un : , suffix : ness : to perform the OOV morphological analysis unassertiveness s \u2212\u2192assertive. We perform an in-depth analysis of the performance of this procedure in Section 4.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Results", "text": "In this section, we evaluate the performance of the procedure described in Section 3. Our evaluations aim at answering several empirical questions: how  well does our method capture morphology, and how does it compare with previous approaches that use word-representations for morphology? How well does this method handle OOVs? How does the impact of morphology analysis change with training data size? We provide both qualitative and quantitative answers for each of these questions next.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quality of Morphological Analysis", "text": "We first evaluate the impact of our morphological analysis on a standard word-similarity rating task. The task measures word-level understanding by comparing the correlation between humanproduced similarity ratings for word pairs, e.g. (intraspecific, interspecies), with those produced by an algorithm. For the experiments reported here, we train SkipGram models 4 using a dimensionality of n = 500. We denote a system using only Skip-Gram model embeddings as SG. To evaluate the impact of our method, we perform morphological analysis for words below a count threshold C. For a word w \u2208 D V C M orph , we simply use the SkipGram vector-representation; for a word w \u2208 D V C M orph , we use as word-representation its mapping in D V C M orph ; we denote such a system SG+Morph. For both SG and SG+Morph systems, we compute the similarity of word-pairs using the cosine distance between the vector-representations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "We train both the SG and SG+Morph models from scratch, for all languages considered. For English, we use the Wikipedia data (Shaoul and Westbury, 2010). For German, French, and Spanish, we use the monolingual data released as part of the WMT-2013 shared task (Bojar et al., 2013). For Arabic we use the Arabic GigaWord corpus (Parker et al., 2011). For Romanian and Uzbek, we use collections of News harvested from the web and cleaned (boilerplate removed, formatting removed, encoding made consistent, etc.). All SkipGram models are trained using a count cutoff of 5 (all words with count less than the cutoff are ignored). Table 3 presents statistics on the data and vocabulary size, as well as the size of the induced morphology graphs. These numbers illustrate the richness of the morphological phenomena present in languages such as German, Romanian, Arabic, and Uzbek, compared to English.\nAs test sets, we use standard, publicly-available word-similarity datasets. Most relevant for our approach is the Stanford English Rare-Word (RW) dataset (Luong et al., 2013), consisting of 2034 word pairs with a higher degree of English morphology compared to other word-similarity datasets. We also use for English the WS353 (Finkelstein et al., 2002) and RG65 datasets (Rubenstein and Goodenough, 1965). For German, we use the Gur350 and ZG222 datasets (Zesch and Gurevych, 2006). For French we use the RG65 French version (Joubarne and Inkpen, 2011); for Spanish, Romanian, and Arabic we use their respective versions of WS353 (Hassan and Mihalcea, 2009).", "publication_ref": ["b28", "b3", "b25", "b19", "b7", "b27", "b31", "b12", "b9"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Results", "text": "We present in Table 4 the results obtained across 6 language pairs and 9 datasets, using a count threshold for SG+Morph of C = 100. We also include the results obtained by two previouslyproposed methods, LSM2013 (Luong et al., 2013) and BB2014 (Botha and Blunsom, 2014), which share some of the characteristics of our method.\nEven in the absence of any morphological treatment, our word representations are better than previously used ones. For instance, LSM2013 uses exactly the same EN Wikipedia (Shaoul and Westbury, 2010)   ment under the morphology condition). The morphological treatment used by LSM2013 also has a small effect on the words present in the English WS and RG sets; our method does not propose any separate morphological treatment for the words in these datasets, since all of them have been observed more than our C = 100 threshold in the training data (therefore have reliable representations). The SG word-representations for all the other languages (German, French, Spanish, Romanian, and Arabic) also perform well on this task, with much higher Spearman scores obtained by SG compared with the previously-reported scores.\nThe results in Table 4 also show that our morphology treatment provides consistent gains across all languages considered. For morphologically-rich languages, all datasets reflect the impact of morphology treatment. We observe significant gains between the performance of the SG and SG+Morph systems, on top of the high correlation numbers of the SG system. For German, the relatively small increase we observe is due to the fact the German noun-compounds are not covered by our morphological treatment. For French, Spanish, Romanian, and Arabic, the gains by the SG+Morph support the conclusion that our method, while completely languageagnostic, handles well the variety of morphological phenomena present in these languages.", "publication_ref": ["b19", "b4", "b28"], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Quality of Morphological Analysis for", "text": "Unknown/Rare Words\nIn this section, we quantify the accuracy of the morphological treatment for OOVs presented in Sec-tion 3.3. We assume that the statistics for unseen words (with respect to their morphological makeup) are similar with the statistics for low-frequency words. Therefore, for some relatively-low counts L and H, the set V [L,H) = V L \u2212 V H is a good proxy for the population of OOV words that we see at runtime. We evaluate OOV morphology as follows:\n1. Run the procedure for morphology induction on V L , resulting in D V L M orph ; 2. Run the procedure for morphology induction on V H , resulting in D V H M orph ;\n3. Apply OOV morphology using D V H M orph for each w \u2208 V [L,H] ; evaluate resulting w \u2192 w against reference w \u2192 w ref from D V L M orph , as normal-form(w ) \u2261 normal-form(w ref ).\nTo make the analysis more revealing, we split the entries in V [L,H) in two: type T1 entries are those that have in-degree > 0 in D V L M orph (i.e., words that have a morphological mapping in the reference graph); type T2 entries are those that have 0 in-degree in D V L M orph (i.e., words with no morphological mapping in the reference, e.g., proper-nouns in English). Note that the T1/T2 distinction reflects a recall/precision trade-off: T1-words should be morphologically analyzed, while T2-words should not; a method that over-analyses has poor performance on T2, while one that under-analyses performs poorly on T1.\nWe use the same datasets as the ones presented in Section 4.1, see   the same setup. Count L = 1000 was chosen such that D V L M orph is reliable enough to be used as reference. The accuracy results are consistently high (in the 80-90% range) for both T1-and T2-words, even for morphologically-rich languages such as Uzbek. These results indicate that our method does well at both identifying a morphological analysis when appropriate, as well as not proposing one when not justified, and therefore provides accurate morphology analysis for rare and OOV words.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Morphology and Training Data Size", "text": "We also evaluate the impact of our morphology analysis under a regime with substantially more training data. To this end, we use large collections of English and German News, harvested from the web and cleaned (boiler-plate removed, formatting removed, encoding made consistent). Statistics regarding the resulting vocabularies and the induced morphology are presented in Table 7 (vocabulary cutoffs of 400 for EN and 50 for DE). We present results using the word-similarity task using the same Stanford Rare-Word (RW) dataset for EN and RG dataset for DE, compared against the setup using only 1-2 billion training tokens. For SG+Morph, we use count thresholds of 3000 for EN and 100 for DE. The results are given in Table 5. For English, a 100x in-  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_9"]}, {"heading": "Conclusions and Future Work", "text": "We have presented an unsupervised method for morphology induction. The method derives a morphological analyzer from scratch, and only requires a monolingual corpus for training, with no additional knowledge of the language. Our evaluation shows that this method performs well across a large variety of language families, and we present here results that improve on current state-of-the-art for the morphologically-rich Stanford Rare-word dataset.\nWe acknowledge that certain languages exhibit phenomena (such as word-compounds in German) that require a more focused approach for solving them. But techniques like the ones presented here have the potential to exploit vector-based word representations successfully to address such phenomena as well.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Factored neural language models", "journal": "", "year": "2006", "authors": "Andrei Alexandrescu; Katrin Kirchhoff"}, {"ref_id": "b1", "title": "Tailoring continuous word representations for dependency parsing", "journal": "Short Papers", "year": "2014-06-22", "authors": "Mohit Bansal; Kevin Gimpel; Karen Livescu"}, {"ref_id": "b2", "title": "A neural probabilistic language model", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "Yoshua Bengio; R\u00e9jean Ducharme; Pascal Vincent; Christian Janvin"}, {"ref_id": "b3", "title": "Findings of the 2013 Workshop on Statistical Machine Translation", "journal": "Association for Computational Linguistics", "year": "2013-08", "authors": "Ond\u0159ej Bojar; Christian Buck; Chris Callison-Burch; Christian Federmann; Barry Haddow; Philipp Koehn; Christof Monz; Matt Post; Radu Soricut; Lucia Specia"}, {"ref_id": "b4", "title": "Compositional morphology for word representations and language modelling", "journal": "CoRR", "year": "2014", "authors": "Jan A Botha; Phil Blunsom"}, {"ref_id": "b5", "title": "Deep learning for efficient discriminative parsing", "journal": "", "year": "2011", "authors": "Ronan Collobert"}, {"ref_id": "b6", "title": "Unsupervised models for morpheme segmentation and morphology learning", "journal": "TSLP", "year": "2007", "authors": "Mathias Creutz; Krista Lagus"}, {"ref_id": "b7", "title": "Placing search in context: the concept revisited", "journal": "ACM Trans. Inf. Syst", "year": "2002", "authors": "Lev Finkelstein; Evgeniy Gabrilovich; Yossi Matias; Ehud Rivlin; Zach Solan; Gadi Wolfman; Eytan Ruppin"}, {"ref_id": "b8", "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "journal": "", "year": "2011", "authors": "Xavier Glorot; Antoine Bordes; Yoshua Bengio"}, {"ref_id": "b9", "title": "Cross-lingual semantic relatedness using encyclopedic knowledge", "journal": "", "year": "2009", "authors": "Samer Hassan; Rada Mihalcea"}, {"ref_id": "b10", "title": "Improving word representations via global context and multiple word prototypes", "journal": "", "year": "2012", "authors": "Eric H Huang; Richard Socher; Christopher D Manning; Andrew Y Ng"}, {"ref_id": "b11", "title": "Learning representations for weakly supervised natural language processing tasks", "journal": "Computational Linguistics", "year": "2014", "authors": "Fei Huang; Arun Ahuja; Doug Downey; Yi Yang; Yuhong Guo; Alexander Yates"}, {"ref_id": "b12", "title": "Comparison of semantic similarity for different languages using the google n-gram corpus and second-order co-occurrence measures", "journal": "", "year": "2011", "authors": "Colette Joubarne; Diana Inkpen"}, {"ref_id": "b13", "title": "Canadian Conference on Artificial Intelligence", "journal": "", "year": "", "authors": ""}, {"ref_id": "b14", "title": "Compositionally derived representations of morphologically complex words in distributional semantics", "journal": "", "year": "2013", "authors": "Angeliki Lazaridou; Marco Marelli; Roberto Zamparelli; Marco Baroni"}, {"ref_id": "b15", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b16", "title": "Distributed representations of sentences and documents", "journal": "CoRR", "year": "2014", "authors": "V Quoc; Tomas Le;  Mikolov"}, {"ref_id": "b17", "title": "Dependencybased word embeddings", "journal": "", "year": "2014", "authors": "Omer Levy; Yoav Goldberg"}, {"ref_id": "b18", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Short Papers", "year": "2014-06-22", "authors": ""}, {"ref_id": "b19", "title": "Better word representations with recursive neural networks for morphology", "journal": "", "year": "2013", "authors": "Minh-Thang Luong; Richard Socher; Christopher D Manning"}, {"ref_id": "b20", "title": "Context dependent recurrent neural network language model", "journal": "", "year": "2012", "authors": "Tomas Mikolov; Geoffrey Zweig"}, {"ref_id": "b21", "title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeff Dean"}, {"ref_id": "b22", "title": "Linguistic regularities in continuous space word representations", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Yih Wen-Tau; Geoffrey Zweig"}, {"ref_id": "b23", "title": "Three new graphical models for statistical language modelling", "journal": "", "year": "2007", "authors": "Andriy Mnih; Geoffrey E Hinton"}, {"ref_id": "b24", "title": "Improving a statistical language model through non-linear prediction", "journal": "Neurocomputing", "year": "2009", "authors": "Andriy Mnih; Zhang Yuecheng; Geoffrey E "}, {"ref_id": "b25", "title": "Arabic gigaword fifth edition ldc2011t11", "journal": "", "year": "2011", "authors": "Robert Parker; David Graff; Ke Chen; Junbo Kong; Kazuaki Maedaet"}, {"ref_id": "b26", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b27", "title": "Contextual correlates of synonymy", "journal": "Communications of the ACM", "year": "1965", "authors": "Herbert Rubenstein; John B Goodenough"}, {"ref_id": "b28", "title": "The Westbury lab Wikipedia corpus", "journal": "", "year": "2010", "authors": "Cyrus Shaoul; Chris Westbury"}, {"ref_id": "b29", "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "journal": "", "year": "2011", "authors": "Richard Socher; Eric H Huang; Jeffrey Pennington; Andrew Y Ng; Christopher D Manning"}, {"ref_id": "b30", "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "journal": "", "year": "2011", "authors": "Richard Socher; Jeffrey Pennington; Eric H Huang; Andrew Y Ng; Christopher D Manning"}, {"ref_id": "b31", "title": "Automatically creating datasets for measures of semantic relatedness", "journal": "", "year": "2006", "authors": "Torsten Zesch; Iryna Gurevych"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A few strongly connected components of a G V M orph graph for English.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: A part of a D V M orph graph, with the morphological family for the normal-form created.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Examples of lexicalized morphological transformations evaluated in E n using rank and cosine.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Statistics regarding the size of the training data and the induced morphology graphs.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013 uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "The results for all the languages are shown in Table6, with all rows using", "figure_data": "EN (RW testset)DE (RG testset)|Unmapped|Spearman \u03c1|Unmapped|Spearman \u03c1Wiki1b News120b Wiki1b News120b WMT2b News20b WMT2b News20bSG8017735.844.702062.462.1SG+Morph1041.852.00064.169.1"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison between models SG and SG+Morph at different training-data sizes.", "figure_data": "|V [1000,2000) |AccuracyLangT1T2T1T2EN 3421 10617 89.7% 89.6%DE 10778 21234 90.8% 93.1%FR 64359807 90.3% 90.4%ES 57247412 91.1% 90.3%RO 11905 9254 86.5% 85.3%AR 79135202 92.4% 69.0%UZ 11772 9027 81.3% 84.1%"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Accuracy of Rare&OOV analysis.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Statistics for large training-data sizes.crease in the training data for EN brings a 10-point increase in Spearman \u03c1 (from 35.8 to 44.7, and from 41.8 to 52.0). The morphological analysis provides substantial gains at either level of training-data size: 6 points in \u03c1 for Wiki1b (from 35.8 to 41.8), and 7.3 points for News120b EN (from 44.7 to 52.0). For German, the increase in training-data size does not bring visible improvements (perhaps due the high vocabulary cutoff), but the morphological treatment has a large impact under the large training-data condition (7 points for News20b DE, from 62.1 to 69.1).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "S r = {(w 1 , w 2 ) \u2208 V 2 |w 1 r \u2212\u2192 w 2 }", "formula_coordinates": [3.0, 108.54, 626.88, 153.72, 15.79]}, {"formula_id": "formula_1", "formula_text": "Ev F ((w 1 , w 2 ), (w, w )) = F E (w 2 , w 1 + \u2191 d w ) (1) (w 1 , w 2 ), (w, w ) \u2208 S r , \u2191 d w = w \u2212 w", "formula_coordinates": [3.0, 314.1, 414.24, 225.9, 30.65]}, {"formula_id": "formula_2", "formula_text": "(r,c) \u2192 w 2 in G V M orph is considered only if count(w 1 ) \u2264 count(w 2 ) in V ;", "formula_coordinates": [6.0, 93.82, 392.04, 204.98, 30.36]}, {"formula_id": "formula_3", "formula_text": "r 1 ...r k \u2212\u2192 w \u2208 D V C", "formula_coordinates": [6.0, 335.02, 496.05, 204.98, 28.87]}], "doi": ""}