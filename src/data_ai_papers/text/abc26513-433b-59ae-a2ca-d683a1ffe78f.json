{"title": "Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations", "authors": "Vincent Sitzmann; Michael Zollh\u00f6fer; Gordon Wetzstein", "pub_date": "", "abstract": "Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3Dstructure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3Dstructure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-toend from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model. 1   ", "sections": [{"heading": "Introduction", "text": "A major driver behind recent work on generative models has been the promise of unsupervised discovery of powerful neural scene representations, enabling downstream tasks ranging from robotic manipulation and few-shot 3D reconstruction to navigation. A key aspect of solving these tasks is understanding the three-dimensional structure of an environment. However, prior work on neural scene representations either does not or only weakly enforces 3D structure [1][2][3][4]. Multi-view geometry and projection operations are performed by a black-box neural renderer, which is expected to learn these operations from data. As a result, such approaches fail to discover 3D structure under limited training data (see Sec. 4), lack guarantees on multi-view consistency of the rendered images, and learned representations are generally not interpretable. Furthermore, these approaches lack an intuitive interface to multi-view and projective geometry important in computer graphics, and cannot easily generalize to camera intrinsic matrices and transformations that were completely unseen at training time.\nIn geometric deep learning, many classic 3D scene representations, such as voxel grids [5][6][7][8][9][10], point clouds [11][12][13][14], or meshes [15] have been integrated with end-to-end deep learning models and have led to significant progress in 3D scene understanding. However, these scene representations are discrete, limiting achievable spatial resolution, only sparsely sampling the underlying smooth surfaces of a scene, and often require explicit 3D supervision. 1 Please see supplemental video for additional results.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "arXiv:1906.01618v2 [cs.CV] 28 Jan 2020", "text": "We introduce Scene Representation Networks (SRNs), a continuous neural scene representation, along with a differentiable rendering algorithm, that model both 3D scene geometry and appearance, enforce 3D structure in a multi-view consistent manner, and naturally allow generalization of shape and appearance priors across scenes. The key idea of SRNs is to represent a scene implicitly as a continuous, differentiable function that maps a 3D world coordinate to a feature-based representation of the scene properties at that coordinate. This allows SRNs to naturally interface with established techniques of multi-view and projective geometry while operating at high spatial resolution in a memory-efficient manner. SRNs can be trained end-to-end, supervised only by a set of posed 2D images of a scene. SRNs generate high-quality images without any 2D convolutions, exclusively operating on individual pixels, which enables image generation at arbitrary resolutions. They generalize naturally to camera transformations and intrinsic parameters that were completely unseen at training time. For instance, SRNs that have only ever seen objects from a constant distance are capable of rendering close-ups of said objects flawlessly. We evaluate SRNs on a variety of challenging 3D computer vision problems, including novel view synthesis, few-shot scene reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.\nTo summarize, our approach makes the following key contributions:\n\u2022 A continuous, 3D-structure-aware neural scene representation and renderer, SRNs, that efficiently encapsulate both scene geometry and appearance. \u2022 End-to-end training of SRNs without explicit supervision in 3D space, purely from a set of posed 2D images. \u2022 We demonstrate novel view synthesis, shape and appearance interpolation, and few-shot reconstruction, as well as unsupervised discovery of a non-rigid face model, and significantly outperform baselines from recent literature.\nScope The current formulation of SRNs does not model view-and lighting-dependent effects or translucency, reconstructs shape and appearance in an entangled manner, and is non-probabilistic. Please see Sec. 5 for a discussion of future work in these directions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Our approach lies at the intersection of multiple fields. In the following, we review related work.\nGeometric Deep Learning. Geometric deep learning has explored various representations to reason about scene geometry. Discretization-based techniques use voxel grids [7,[16][17][18][19][20][21][22], octree hierarchies [23][24][25], point clouds [11,26,27], multiplane images [28], patches [29], or meshes [15,21,30,31]. Methods based on function spaces continuously represent space as the decision boundary of a learned binary classifier [32] or a continuous signed distance field [33][34][35]. While these techniques are successful at modeling geometry, they often require 3D supervision, and it is unclear how to efficiently infer and represent appearance. Our proposed method encapsulates both scene geometry and appearance, and can be trained end-to-end via learned differentiable rendering, supervised only with posed 2D images.\nNeural Scene Representations. Latent codes of autoencoders may be interpreted as a feature representation of the encoded scene. Novel views may be rendered by concatenating target pose and latent code [1] or performing view transformations directly in the latent space [4]. Generative Query Networks [2,3] introduce a probabilistic reasoning framework that models uncertainty due to incomplete observations, but both the scene representation and the renderer are oblivious to the scene's 3D structure. Some prior work infers voxel grid representations of 3D scenes from images [6,8,9] or uses them for 3D-structure-aware generative models [10,36]. Graph neural networks may similarly capture 3D structure [37]. Compositional structure may be modeled by representing scenes as programs [38]. We demonstrate that models with scene representations that ignore 3D structure fail to perform viewpoint transformations in a regime of limited (but significant) data, such as the Shapenet v2 dataset [39]. Instead of a discrete representation, which limits achievable spatial resolution and does not smoothly parameterize scene surfaces, we propose a continuous scene representation.\nNeural Image Synthesis. Deep models for 2D image and video synthesis have recently shown promising results in generating photorealistic images. Some of these approaches are based on Figure 1: Overview: at the heart of SRNs lies a continuous, 3D-aware neural scene representation, \u03a6, which represents a scene as a function that maps (x, y, z) world coordinates to a feature representation of the scene at those coordinates (see Sec. 3.1). A neural renderer \u0398, consisting of a learned ray marcher and a pixel generator, can render the scene from arbitrary novel view points (see Sec. 3.2).\n(variational) auto-encoders [40,41], generative flows [42,43], or autoregressive per-pixel models [44,45]. In particular, generative adversarial networks [46][47][48][49][50] and their conditional variants [51][52][53] have recently achieved photo-realistic single-image generation. Compositional Pattern Producing Networks [54,55] learn functions that map 2D image coordinates to color. Some approaches build on explicit spatial or perspective transformations in the networks [56][57][58]14]. Recently, following the spirit of \"vision as inverse graphics\" [59,60], deep neural networks have been applied to the task of inverting graphics engines [61][62][63][64][65]. However, these 2D generative models only learn to parameterize the manifold of 2D natural images, and struggle to generate images that are multi-view consistent, since the underlying 3D scene structure cannot be exploited.", "publication_ref": ["b17", "b18", "b19", "b20", "b21", "b22", "b23", "b24", "b25", "b26", "b27", "b28", "b20", "b29", "b30", "b31", "b32", "b33", "b34", "b35", "b36", "b37", "b38", "b39", "b40", "b41", "b42", "b43", "b44", "b45", "b46", "b47", "b48", "b49", "b50", "b51", "b52", "b53", "b54", "b55", "b56", "b57", "b58", "b59", "b60", "b61", "b62", "b63", "b64"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Formulation", "text": "Given\na training set C = {(I i , E i , K i )} N i=1\nof N tuples of images I i \u2208 R H\u00d7W \u00d73 along with their respective extrinsic E i = R|t \u2208 R 3\u00d74 and intrinsic K i \u2208 R 3\u00d73 camera matrices [66], our goal is to distill this dataset of observations into a neural scene representation \u03a6 that strictly enforces 3D structure and allows to generalize shape and appearance priors across scenes. In addition, we are interested in a rendering function \u0398 that allows us to render the scene represented by \u03a6 from arbitrary viewpoints. In the following, we first formalize \u03a6 and \u0398 and then discuss a framework for optimizing \u03a6, \u0398 for a single scene given only posed 2D images. Note that this approach does not require information about scene geometry. Additionally, we show how to learn a family of scene representations for an entire class of scenes, discovering powerful shape and appearance priors.", "publication_ref": ["b65"], "figure_ref": [], "table_ref": []}, {"heading": "Representing Scenes as Functions", "text": "Our key idea is to represent a scene as a function \u03a6 that maps a spatial location x to a feature representation v of learned scene properties at that spatial location:\n\u03a6 : R 3 \u2192 R n , x \u2192 \u03a6(x) = v.\n(1) The feature vector v may encode visual information such as surface color or reflectance, but it may also encode higher-order information, such as the signed distance of x to the closest scene surface. This continuous formulation can be interpreted as a generalization of discrete neural scene representations. Voxel grids, for instance, discretize R 3 and store features in the resulting 3D grid [5][6][7][8][9][10]. Point clouds [12][13][14] may contain points at any position in R 3 , but only sparsely sample surface properties of a scene. In contrast, \u03a6 densely models scene properties and can in theory model arbitrary spatial resolutions, as it is continuous over R 3 and can be sampled with arbitrary resolution. In practice, we represent \u03a6 as a multi-layer perceptron (MLP), and spatial resolution is thus limited by the capacity of the MLP.\nIn contrast to recent work on representing scenes as unstructured or weakly structured feature embeddings [1,4,2], \u03a6 is explicitly aware of the 3D structure of scenes, as the input to \u03a6 are world coordinates (x, y, z) \u2208 R 3 . This allows interacting with \u03a6 via the toolbox of multi-view and perspective geometry that the physical world obeys, only using learning to approximate the unknown properties of the scene itself. In Sec. 4, we show that this formulation leads to multi-view consistent novel view synthesis, data-efficient training, and a significant gain in model interpretability.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Neural Rendering", "text": "Given a scene representation \u03a6, we introduce a neural rendering algorithm \u0398, that maps a scene representation \u03a6 as well as the intrinsic K and extrinsic E camera parameters to an image I:\n\u0398 : X \u00d7 R 3\u00d74 \u00d7 R 3\u00d73 \u2192 R H\u00d7W \u00d73 , (\u03a6, E, K) \u2192 \u0398(\u03a6, E, K) = I,(2)\nwhere X is the space of all functions \u03a6.\nThe key complication in rendering a scene represented by \u03a6 is that geometry is represented implicitly. The surface of a wooden table top, for instance, is defined by the subspace of R 3 where \u03a6 undergoes a change from a feature vector representing free space to one representing wood.\nTo render a single pixel in the image observed by a virtual camera, we thus have to solve two sub-problems: (i) finding the world coordinates of the intersections of the respective camera rays with scene geometry, and (ii) mapping the feature vector v at that spatial coordinate to a color. We will first propose a neural ray marching algorithm with learned, adaptive step size to find ray intersections with scene geometry, and subsequently discuss the architecture of the pixel generator network that learns the feature-to-color mapping.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Differentiable Ray Marching Algorithm", "text": "Algorithm 1 Differentiable Ray-Marching\n1: function FINDINTERSECTION(\u03a6, K, E, (u, v)) 2: d0 \u2190 0.05\nNear plane 3:\n(h0, c0) \u2190 (0, 0) Initial state of LSTM 4:\nfor i \u2190 0 to max_iter do 5:\nxi \u2190 ru,v(di) Calculate world coordinates 6:\nvi \u2190 \u03a6(xi) Extract feature vector 7:\n(\u03b4, hi+1, ci+1) \u2190 LST M (v, hi, ci) Predict steplength using ray marching LSTM 8:\ndi+1 \u2190 di + \u03b4 Update d 9:\nreturn ru,v(dmax_iter)\nIntersection testing intuitively amounts to solving an optimization problem, where the point along each camera ray is sought that minimizes the distance to the surface of the scene. To model this problem, we parameterize the points along each ray, identified with the coordinates (u, v) of the respective pixel, with their distance d to the camera (d > 0 represents points in front of the camera):\nr u,v (d) = R T (K \u22121 u v d \u2212 t), d > 0,(3)\nwith world coordinates r u,v (d) of a point along the ray with distance d to the camera, camera intrinsics K, and camera rotation matrix R and translation vector t. For each ray, we aim to solve\narg min d s.t. r u,v (d) \u2208 \u2126, d > 0 (4)\nwhere we define the set of all points that lie on the surface of the scene as \u2126.\nHere, we take inspiration from the classic sphere tracing algorithm [67]. Sphere tracing belongs to the class of ray marching algorithms, which solve Eq. 4 by starting at a distance d init close to the camera and stepping along the ray until scene geometry is intersected. Sphere tracing is defined by a special choice of the step length: each step has a length equal to the signed distance to the closest surface point of the scene. Since this distance is only 0 on the surface of the scene, the algorithm takes non-zero steps until it has arrived at the surface, at which point no further steps are taken. Extensions of this algorithm propose heuristics to modifying the step length to speed up convergence [68]. We instead propose to learn the length of each step.\nSpecifically, we introduce a ray marching long short-term memory (RM-LSTM) [69], that maps the feature vector \u03a6(x i ) = v i at the current estimate of the ray intersection x i to the length of the next ray marching step. The algorithm is formalized in Alg. 1.\nGiven our current estimate d i , we compute world coordinates x i = r u,v (d i ) via Eq. 3. We then compute \u03a6(x i ) to obtain a feature vector v i , which we expect to encode information about nearby scene surfaces. We then compute the step length \u03b4 via the RM-LSTM as (\u03b4, h i+1 , c i+1 ) = LST M (v i , h i , c i ), where h and c are the output and cell states, and increment d i accordingly. We iterate this process for a constant number of steps. This is critical, because a dynamic termination criterion would have no guarantee for convergence in the beginning of the training, where both \u03a6 and the ray marching LSTM are initialized at random. The final step yields our estimate of the world coordinates of the intersection of the ray with scene geometry. The z-coordinates of running and final estimates of intersections in camera coordinates yield depth maps, which we denote as d i , which visualize every step of the ray marcher. This makes the ray marcher interpretable, as failures in geometry estimation show as inconsistencies in the depth map. Note that depth maps are differentiable with respect to all model parameters, but are not required for training \u03a6. Please see the supplement for a contextualization of the proposed rendering approach with classical rendering algorithms.", "publication_ref": ["b66", "b67", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "Pixel Generator Architecture", "text": "The pixel generator takes as input the 2D feature map sampled from \u03a6 at world coordinates of raysurface intersections and maps it to an estimate of the observed image. As a generator architecture, we choose a per-pixel MLP that maps a single feature vector v to a single RGB vector. This is equivalent to a convolutional neural network (CNN) with only 1 \u00d7 1 convolutions. Formulating the generator without 2D convolutions has several benefits. First, the generator will always map the same (x, y, z) coordinate to the same color value. Assuming that the ray-marching algorithm finds the correct intersection, the rendering is thus trivially multi-view consistent. This is in contrast to 2D convolutions, where the value of a single pixel depends on a neighborhood of features in the input feature map. When transforming the camera in 3D, e.g. by moving it closer to a surface, the 2D neighborhood of a feature may change. As a result, 2D convolutions come with no guarantee on multiview consistency. With our per-pixel formulation, the rendering function \u0398 operates independently on all pixels, allowing images to be generated with arbitrary resolutions and poses. On the flip side, we cannot exploit recent architectural progress in CNNs, and a per-pixel formulation requires the ray marching, the SRNs and the pixel generator to operate on the same (potentially high) resolution, requiring a significant memory budget. Please see the supplement for a discussion of this trade-off.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalizing Across Scenes", "text": "We now generalize SRNs from learning to represent a single scene to learning shape and appearance priors over several instances of a single class. Formally, we assume that we are given a set of M instance datasets D = {C j } M j=1 , where each C j consists of tuples\n{(I i , E i , K i )} N i=1 as discussed in Sec. 3.1.\nWe reason about the set of functions {\u03a6 j } M j=1 that represent instances of objects belonging to the same class. By parameterizing a specific \u03a6 j as an MLP, we can represent it with its vector of parameters \u03c6 j \u2208 R l . We assume scenes of the same class have common shape and appearance properties that can be fully characterized by a set of latent variables z \u2208 R k , k < l. Equivalently, this assumes that all parameters \u03c6 j live in a k-dimensional subspace of R l . Finally, we define a mapping\n\u03a8 : R k \u2192 R l , z j \u2192 \u03a8(z j ) = \u03c6 j (5)\nthat maps a latent vector z j to the parameters \u03c6 j of the corresponding \u03a6 j . We propose to parameterize \u03a8 as an MLP, with parameters \u03c8. This architecture was previously introduced as a Hypernetwork [70], a neural network that regresses the parameters of another neural network. We share the parameters of the rendering function \u0398 across scenes. We note that assuming a low-dimensional embedding manifold has so far mainly been empirically demonstrated for classes of single objects. Here, we similarly only demonstrate generalization over classes of single objects.\nFinding latent codes z j . To find the latent code vectors z j , we follow an auto-decoder framework [33]. For this purpose, each object instance C j is represented by its own latent code z j . The z j are free variables and are optimized jointly with the parameters of the hypernetwork \u03a8 and the neural renderer \u0398. We assume that the prior distribution over the z j is a zero-mean multivariate Gaussian with a diagonal covariance matrix. Please refer to [33] for additional details.  Figure 4: Normal maps for a selection of objects. We note that geometry is learned fully unsupervised and arises purely out of the perspective and multi-view geometry constraints on the image formation.", "publication_ref": ["b69", "b32", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Joint Optimization", "text": "To summarize, given a dataset\nD = {C j } M j=1 of instance datasets C = {(I i , E i , K i )} N i=1\n, we aim to find the parameters \u03c8 of \u03a8 that maps latent vectors z j to the parameters of the respective scene representation \u03c6 j , the parameters \u03b8 of the neural rendering function \u0398, as well as the latent codes z j themselves. We formulate this as an optimization problem with the following objective:\narg min {\u03b8,\u03c8,{zj} M j=1 } M j=1 N i=1 \u0398 \u03b8 (\u03a6 \u03a8(z j ) , E j i , K j i ) \u2212 I j i 2 2 Limg + \u03bb dep min(d j i,f inal , 0) 2 2 Ldepth + \u03bb lat z j 2 2 Llatent .(6)\nWhere L img is an 2 -loss enforcing closeness of the rendered image to ground-truth, L depth is a regularization term that accounts for the positivity constraint in Eq. 4, and L latent enforces a Gaussian prior on the z j . In the case of a single scene, this objective simplifies to solving for the parameters \u03c6 of the MLP parameterization of \u03a6 instead of the parameters \u03c8 and latent codes z j . We solve Eq. 6 with stochastic gradient descent. Note that the whole pipeline can be trained end-to-end, without requiring any (pre-)training of individual parts. In Sec. 4, we demonstrate that SRNs discover both geometry and appearance, initialized at random, without requiring prior knowledge of either scene geometry or scene scale, enabling multi-view consistent novel view synthesis.\nFew-shot reconstruction. After finding model parameters by solving Eq. 6, we may use the trained model for few-shot reconstruction of a new object instance, represented by a dataset C =\n{(I i , E i , K i )} N i=1 .\nWe fix \u03b8 as well as \u03c8, and estimate a new latent code\u1e91 by minimizin\u011d\nz = arg min z N i=1 \u0398 \u03b8 (\u03a6 \u03a8(z) , E i , K i ) \u2212 I i 2 2 + \u03bb dep min(d i,f inal , 0) 2 2 + \u03bb lat z 2 2 (7)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We train SRNs on several object classes and evaluate them for novel view synthesis and few-shot reconstruction. We further demonstrate the discovery of a non-rigid face model. Please see the supplement for a comparison on single-scene novel view synthesis performance with DeepVoxels [6].   [1] and the deterministic variant of the GQN [2], for novel view synthesis on the Shapenet v2 \"cars\" and \"chairs\" classes. We compare novel views for objects reconstructed from 50 observations in the training set (top row), two observations and a single observation (second and third row) from a test set. SRNs consistently outperforms these baselines with multi-view consistent novel views, while also reconstructing geometry. Please see the supplemental video for more comparisons, smooth camera trajectories, and reconstructed geometry. Implementation Details. Hyperparameters, computational complexity, and full network architectures for SRNs and all baselines are in the supplement. Training of the presented models takes on the order of 6 days. A single forward pass takes around 120 ms and 3 GB of GPU memory per batch item. Code and datasets are available.\nShepard-Metzler objects. We evaluate our approach on 7-element Shepard-Metzler objects in a limited-data setting. We render 15 observations of 1k objects at a resolution of 64 \u00d7 64. We train both SRNs and a deterministic variant of the Generative Query Network [2] (dGQN, please see supplement for an extended discussion). Note that the dGQN is solving a harder problem, as it is inferring the scene representation in each forward pass, while our formulation requires solving an optimization problem to find latent codes for unseen objects. We benchmark novel view reconstruction accuracy on (1) the training set and (2) few-shot reconstruction of 100 objects from a held-out test set. On the training objects, SRNs achieve almost pixel-perfect results with a PSNR of 30.41 dB. The dGQN fails to learn object shape and multi-view geometry on this limited dataset, achieving 20.85 dB. See Fig. 2 for a qualitative comparison. In a two-shot setting (see Fig. 7 for reference views), we succeed in reconstructing any part of the object that has been observed, achieving 24.36 dB, while the dGQN achieves 18.56 dB. In a one-shot setting, SRNs reconstruct an object consistent with the observed view. As expected, due to the current non-probabilistic implementation, both the dGQN and SRNs reconstruct an object resembling the mean of the hundreds of feasible objects that may have generated the observation, achieving 17.51 dB and 18.11 dB respectively. Shapenet v2. We consider the \"chair\" and \"car\" classes of Shapenet v.2 [39] with 4.5k and 2.5k model instances respectively. We disable transparencies and specularities, and train on 50 observations of each instance at a resolution of 128 \u00d7 128 pixels. Camera poses are randomly generated on a sphere with the object at the origin. We evaluate performance on (1) novel-view synthesis of objects in the training set and (2) novel-view synthesis on objects in the held-out, official Shapenet v2 test sets, reconstructed from one or two observations, as discussed in Sec. 3.4. Fig. 7 shows the sampled poses for the few-shot case. In all settings, we assemble ground-truth novel views by sampling 250 views in an Archimedean spiral around each object instance. We compare Table 1: PSNR (in dB) and SSIM of images reconstructed with our method, the deterministic variant of the GQN [2] (dGQN), the model proposed by Tatarchenko et al. [1] (TCO), and the method proposed by Worrall et al. [4] (WRL). We compare novel-view synthesis performance on objects in the training set (containing 50 images of each object), as well as reconstruction from 1 or 2 images on the held-out test set. SRNs to three baselines from recent literature. Table 1 and Fig. 6 report quantitative and qualitative results respectively. In all settings, we outperform all baselines by a wide margin. On the training set, we achieve very high visual fidelity. Generally, views are perfectly multi-view consistent, the only exception being objects with distinct, usually fine geometric detail, such as the windscreen of convertibles. None of the baselines succeed in generating multi-view consistent views. Several views per object are usually entirely degenerate. In the two-shot case, where most of the object has been seen, SRNs still reconstruct both object appearance and geometry robustly. In the single-shot case, SRNs complete unseen parts of the object in a plausible manner, demonstrating that the learned priors have truthfully captured the underlying distributions.\nSupervising parameters for non-rigid deformation. If latent parameters of the scene are known, we can condition on these parameters instead of jointly solving for latent variables z j . We generate 50 renderings each from 1000 faces sampled at random from the Basel face model [71]. Camera poses are sampled from a hemisphere in front of the face. Each face is fully defined by a 224-dimensional parameter vector, where the first 160 parameterize identity, and the last 64 dimensions control facial expression. We use a constant ambient illumination to render all faces. Conditioned on this disentangled latent space, SRNs succeed in reconstructing face geometry and appearance. After training, we animate facial expression by varying the 64 expression parameters while keeping the identity fixed, even though this specific combination of identity and expression has not been observed before. Fig. 3 shows qualitative results of this non-rigid deformation. Expressions smoothly transition from one to the other, and the reconstructed normal maps, which are directly computed from the depth maps (not shown), demonstrate that the model has learned the underlying geometry.\nGeometry reconstruction. SRNs reconstruct geometry in a fully unsupervised manner, purely out of necessity to explain observations in 3D. Fig. 4 visualizes geometry for 50-shot, single-shot, and single-scene reconstructions.\nLatent space interpolation. Our learned latent space allows meaningful interpolation of object instances. Fig. 5 shows latent space interpolation.  Failure cases. The ray marcher may \"get stuck\" in holes of surfaces or on rays that closely pass by occluders, such as commonly occur in chairs. SRNs generates a continuous surface in these cases, or will sometimes step through the surface. If objects are far away from the training distribution, SRNs may fail to reconstruct geometry and instead only match texture. In both cases, the reconstructed geometry allows us to analyze the failure, which is impossible with black-box alternatives. See Fig. 8 and the supplemental video.\nTowards representing room-scale scenes. We demonstrate reconstruction of a room-scale scene with SRNs. We train a single SRN on 500 observations of a minecraft room. The room contains multiple objects as well as four columns, such that parts of the scene are occluded in most observations. After training, the SRN enables novel view synthesis of the room. Though generated images are blurry, they are largely multi-view consistent, with artifacts due to ray marching failures only at object boundaries and thin structures. The SRN succeeds in inferring geometry and appearance of the room, reconstructing occluding columns and objects correctly, failing only on low-texture areas (where geometry is only weakly constrained) and thin tubes placed between columns. Please see the supplemental video for qualitative results.", "publication_ref": ["b38", "b70"], "figure_ref": ["fig_0", "fig_4", "fig_4", "fig_3", "fig_1", "fig_2", "fig_6"], "table_ref": []}, {"heading": "Discussion", "text": "We introduce SRNs, a 3D-structured neural scene representation that implicitly represents a scene as a continuous, differentiable function. This function maps 3D coordinates to a feature-based representation of the scene and can be trained end-to-end with a differentiable ray marcher to render the feature-based representation into a set of 2D images. SRNs do not require shape supervision and can be trained only with a set of posed 2D images. We demonstrate results for novel view synthesis, shape and appearance interpolation, and few-shot reconstruction.\nThere are several exciting avenues for future work. SRNs could be explored in a probabilistic framework [2,3], enabling sampling of feasible scenes given a set of observations. SRNs could be extended to model view-and lighting-dependent effects, translucency, and participating media. They could also be extended to other image formation models, such as computed tomography or magnetic resonance imaging. Currently, SRNs require camera intrinsic and extrinsic parameters, which can be obtained robustly via bundle-adjustment. However, as SRNs are differentiable with respect to camera parameters; future work may alternatively integrate them with learned algorithms for camera pose estimation [72]. SRNs also have exciting applications outside of vision and graphics, and future work may explore SRNs in robotic manipulation or as the world model of an independent agent. While SRNs can represent room-scale scenes (see the supplemental video), generalization across complex, cluttered 3D environments is an open problem. Recent work in meta-learning could enable generalization across scenes with weaker assumptions on the dimensionality of the underlying manifold [73]. Please see the supplemental material for further details on directions for future work.  ", "publication_ref": ["b71", "b72"], "figure_ref": [], "table_ref": []}, {"heading": "Additional Results on Neural Ray Marching", "text": "Computation of Normal Maps We found that normal maps visualize fine surface detail significantly better than depth maps (see Fig. 1), and thus only report normal maps in the main submission. We compute surface normals as the cross product of the numerical horizontal and vertical derivatives of the depth map.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Ray Marching Progress Visualization", "text": "The z-coordinates of running and final estimates of intersections in each iteration of the ray marcher in camera coordinates yield depth maps, which visualize every step of the ray marcher. Fig. 1 shows two example ray marches, along with their final normal maps.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Comparison to DeepVoxels", "text": "We compare performance in single-scene novel-view synthesis with the recently proposed DeepVoxels architecture [1] on their four synthetic objects. DeepVoxels proposes a 3D-structured neural scene representation in the form of a voxel grid of features. Multi-view and projective geometry are hard-coded into the model architecture. We further report accuracy of the same baselines as in [1]: a Pix2Pix architecture [2] that receives as input the per-pixel view direction, as well as the methods proposed by Tatarchenko et al. [3] as well as by Worrall et al. [4] and Cohen and Welling [5].\nTable 1 compares PSNR and SSIM of the proposed architecture and the baselines, averaged over all 4 scenes. We outperform the best baseline, DeepVoxels [1], by more than 3 dB. Qualitatively, DeepVoxels displays significant multi-view inconsistencies in the form of flickering artifacts, while the proposed method is almost perfectly multi-view consistent. We achieve this result with 550k parameters per model, as opposed to the DeepVoxels architecture with more than 160M free variables. However, we found that SRNs produce blurry output for some of the very high-frequency textural Figure 3: Undersampled letters on the side of the cube (ground truth images). Lines of letters are less than two pixels wide, leading to significant aliasing. Additionally, the 2D downsampling as described in [1] introduced blur that is not multi-view consistent.  [1]. With 3 orders of magnitude fewer parameters, we achieve a 3dB boost, with reduced multi-view inconsistencies. detail -this is most notable with the letters on the sides of the cube. Fig. 3 demonstrates why this is the case. Several of the high-frequency textural detail of the DeepVoxels objects are heavily undersampled. For instance, lines of letters on the sides of the cube often only occupy a single pixel. As a result, the letters alias across viewing angles. This violates one of our key assumptions, namely that the same (x, y, z) \u2208 R 3 world coordinate always maps to the same color, independent of the viewing angle. As a result, it is impossible for our model to generate these details. We note that detail that is not undersampled, such as the CVPR logo on the top of the cube, is reproduced with perfect accuracy. However, we can easily accommodate for this undersampling by using a 2D CNN renderer. This amounts to a trade-off of our guarantee of multi-view consistency discussed in Sec. 3 of the main paper with robustness to faulty training data. Fig. 2 shows the cube rendered with a U-Net based renderer -all detail is replicated truthfully.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_0"], "table_ref": []}, {"heading": "Reproducibility", "text": "In this section, we discuss steps we take to allow the community to reproduce our results. All code and datasets will be made publicly available. All models were evaluated on the test sets exactly once.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Architecture Details", "text": "Scene representation network \u03a6 In all experiments, \u03a6 is parameterized as a multi-layer perceptron (MLP) with ReLU activations, layer normalization before each nonlinearity [6], and four layers with 256 units each. In all generalization experiments in the main paper, its weights \u03c6 are the output of the hypernetwork \u03a8. In the DeepVoxels comparison (see Sec.2), where a separate \u03a6 is trained per scene, parameters of \u03c6 are directly initialized using the Kaiming Normal method [7].\nHypernetwork \u03a8 In generalization experiments, a hypernetwork \u03a8 maps a latent vector z j to the weights of the respective scene representation \u03c6 j . Each layer of \u03a6 is the output of a separate hypernetwork. Each hypernetwork is parameterized as a multi-layer perceptron with ReLU activations, layer normalization before each nonlinearity [6], and three layers (where the last layer has as many units as the respective layer of \u03a6 has weights). In the Shapenet and Shepard-Metzler experiments, where the latent codes z j have length 256, hypernetworks have 256 units per layer. In the Basel face experiment, where the latent codes z j have length 224, hypernetworks have 224 units per layer.\nWeights are initialized by the Kaiming Normal method, scaled by a factor 0.1. We empirically found this initialization to stabilize early training.\nRay marching LSTM In all experiments, the ray marching LSTM is implemented as a vanilla LSTM with a hidden state size of 16. The initial state is set to zero.\nPixel Generator In all experiments, the pixel generator is parameterized as a multi-layer perceptron with ReLU activations, layer normalization before each nonlinearity [6], and five layers with 256 units each. Weights are initialized with the Kaiming Normal method [7].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Time & Memory Complexity", "text": "Scene representation network \u03a6 \u03a6 scales as a standard MLP. Memory and runtime scale linearly in the number of queries, therefore quadratic in image resolution. Memory and runtime further scale linearly with the number of layers and quadratically with the number of units in each layer.\nHypernetwork \u03a8 \u03a8 scales as a standard MLP. Notably, the last layer of \u03a8 predicts all parameters of the scene representation \u03a6. As a result, the number of weights scales linearly in the number of weights of \u03a6, which is significant. For instance, with 256 units per layer and 4 layers, \u03a6 has approximately 2 \u00d7 10 5 parameters. In our experiments, \u03a8 is parameterized with 256 units in all hidden layers. The last layer of \u03a8 then has approximately 5 \u00d7 10 7 parameters, which is the bulk of learnable parameters in our model. Please note that \u03a8 only has to be queried once to obtain \u03a6, at which point it could be discarded, as both the pixel generation and the ray marching only need access to the predicted \u03a6.\nDifferentiable Ray Marching Memory and runtime of the differentiable ray marcher scale linearly in the number of ray marching steps and quadratically in image resolution. As it queries \u03a6 repeatedly, it also scales linearly in the same parameters as \u03a6.\nPixel Generator The pixel generator scales as a standard MLP. Memory and runtime scale linearly in the number of queries, therefore quadratic in image resolution. Memory and runtime further scale linearly with the number of layers and quadratically with the number of units in each layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shepard-Metzler objects", "text": "We modified an open-source implementation of a Shepard-Metzler renderer (https://github.com/musyoku/gqn-dataset-renderer.git) to generate meshes of Shepard-Metzler objects, which we rendered using Blender to have full control over camera intrinsic and extrinsic parameters consistent with other presented datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shapenet v2 cars", "text": "We render each object from random camera perspectives distributed on a sphere with radius 1.3 using Blender. We disabled specularities, shadows and transparencies and used environment lighting with energy 1.0. We noticed that a few cars in the dataset were not scaled optimally, and scaled their bounding box to unit length. A few meshes had faulty vertices, resulting in a faulty bounding box and subsequent scaling to a very small size. We discarded those 40 out of 2473 cars.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shapenet v2 chairs", "text": "We render each object from random camera perspectives distributed on a sphere with radius 2.0 using Blender. We disabled specularities, shadows and transparencies and used environment lighting with energy 1.0.\nFaces dataset We use the Basel Face dataset to generate meshes with different identities at random, where each parameter is sampled from a normal distribution with mean 0 and standard deviation of 0.7. For expressions, we use the blendshape model of Thies et al. [8], and sample expression parameters uniformly in (\u22120.4, 1.6).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DeepVoxels dataset", "text": "We use the dataset as presented in [1].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SRNs Training Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General details", "text": "Multi-Scale training Our per-pixel formulation naturally allows us to train in a coarse-to-fine setting, where we first train the model on downsampled images in a first stage, and then increase the resolution of images in stages. This allows larger batch sizes at the beginning of the training, which affords more independent views for each object, and is reminiscent of other coarse-to-fine approaches [9].\nSolver For all experiments, we use the ADAM solver with \u03b2 1 = 0.9, \u03b2 2 = 0.999.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation & Compute", "text": "We implement all models in PyTorch. All models were trained on single GPUs of the type RTX6000 or RTX8000.\nHyperparameter search Training hyperparameters for SRNs were found by informal search -we did not perform a systematic grid search due to the high computational cost.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Per-experiment details", "text": "For a resolution of 64 \u00d7 64, we train with a batch size of 72. Due to the memory complexity being quadratic in the image sidelength, we decrease the batch size by a factor of 4 when we double the image resolution. \u03bb depth is always set to 1 \u00d7 10 \u22123 and \u03bb latent is set to 1. The ADAM learning rate is set to 4 \u00d7 10 \u22124 if not reported otherwise.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shepard-Metzler experiment", "text": "We directly train our model on images of resolution 64 \u00d7 64 for 352 epochs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shapenet cars", "text": "We train our model in 2 stages. We first train on a resolution of 64 \u00d7 64 for 5k iterations. We then increase the resolution to 128 \u00d7 128. We train on the high resolution for 70 epochs. The ADAM learning rate is set to 5 \u00d7 10 \u22125 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shapenet chairs", "text": "We train our model in 2 stages. We first train on a resolution of 64 \u00d7 64 for 20k iterations. We then increase the resolution to 128 \u00d7 128. We train our model for 12 epochs.\nBasel face experiments We train our model in 2 stages. We first train on a resolution of 64 \u00d7 64 for 15k iterations. We then increase the resolution to 128 \u00d7 128 and train for another 5k iterations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DeepVoxels experiments", "text": "We train our model in 3 stages. We first train on a resolution of 12 \u00d7 128 with a learning rate of 4 \u00d7 10 \u22124 for 20k iterations. We then increase the resolution to 256 \u00d7 256, and lower the learning rate to 1 \u00d7 10 \u22124 and train for another 30k iterations. We then increase the resolution to 512 \u00d7 512, and lower the learning rate to 4 \u00d7 10 \u22126 and train for another 30k iterations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relationship to per-pixel autoregressive methods", "text": "With the proposed per-pixel generator, SRNs are also reminiscent of autoregressive per-pixel architectures, such as PixelCNN and PixelRNN [10,11] \nInstead, conditioned on a scene representation \u03a6, pixel values are conditionally independent, as our approach independentaly and deterministically assigns a value to each pixel. The probability of observing an image I thus simplifies to the probability of observing a scene \u03a6 under extrinsic E and intrinsic K camera parameters p(I) = p(\u03a6)p(E)p(K).\n(\n)2\nThis conditional independence of single pixels conditioned on the scene representation further motivates the per-pixel design of the rendering function \u0398.\n5 Baseline Discussions", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Deterministic Variant of GQN", "text": "Deterministic vs. Non-Deterministic Eslami et al. [12] propose a powerful probabilistic framework for modeling uncertainty in the reconstruction due to incomplete observations. However, here, we are exclusively interested in investigating the properties of the scene representation itself, and this submission discusses SRNs in a purely deterministic framework. To enable a fair comparison, we thus implement a deterministic baseline inspired by the Generative Query Network [12]. We note that the results obtained in this comparison are not necessarily representative of the performance of the unaltered Generative Query Network. We leave a formulation of SRNs in a probabilistic framework and a comparison to the unaltered GQN to future work.\nArchitecture As representation network architecture, we choose the \"Tower\" representation, and leave its architecture unaltered. However, instead of feeding the resulting scene representation r to a convolutional LSTM architecture to parameterize a density over latent variables z, we instead directly feed the scene representation r to a generator network. We use as generator a deterministic, autoregressive, skip-convolutional LSTM C, the deterministic equivalent of the generator architecture proposed in [12]. Specifically, the generator can be described by the following equations:\nInitial state (c 0 , h 0 , u 0 ) = (0, 0, 0) (3) Pre-process current canvas p l = \u03ba(u l ) (4) State update (c l+1 , h l+1 ) = C(E, r, c l , h l , p l ) (5) Canvas update u l+1 = u l + \u2206(h l+1 ) (6) Final output x = \u03b7(u L ),(7)\nwith timestep l and final timestep L, LSTM output c l and cell h l states, the canvas u l , a downsampling network \u03ba, the camera extrinsic parameters E, an upsampling network \u2206, and a 1 \u00d7 1 convolutional layer \u03b7. Consistent with [12], all up-and downsampling layers are convolutions of size 4 \u00d7 4 with stride 4. To account for the higher resolution of the Shapenet v2 car and chair images, we added a further convolutional layer / transposed convolution where necessary.\nTraining On both the cars and chairs datasets, we trained for 180, 000 iterations with a batch size of 140, taking approximately 6.5 days. For the lower-resolution Shepard-Metzler objects, we trained for 160, 000 iterations at a batch size of 192, or approximately 5 days.\nTesting For novel view synthesis on the training set, the model receives as input the 15 nearest neighbors of the novel view in terms of cosine similarity. For two-shot reconstruction, the model receives as input whichever of the two reference views is closer to the novel view in terms of cosine similarity. For one-shot reconstruction, the model receives as input the single reference view. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tatarchenko et al.", "text": "Architecture We implement the exact same architecture as described in [3], with approximately 70 \u2022 10 6 parameters.\nTraining For training, we choose the same hyperparameters as proposed in Tatarchenko et al. [3]. As we assume no knowledge of scene geometry, we do not supervise the model with a depth map.\nAs we observed the model to overfit, we stopped training early based on model performance on the held-out, official Shapenet v2 validation set.\nTesting For novel view synthesis on the training set, the model receives as input the nearest neighbor of the novel view in terms of cosine similarity. For two-shot reconstruction, the model receives as input whichever of the two reference views is closer to the novel view. Finally, for one-shot reconstruction, the model receives as input the single reference view.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Worrall et al.", "text": "Architecture Please see Fig. 6 for a visualization of the full architecture. The design choices in this architecture (nearest-neighbor upsampling, leaky ReLU activations, batch normalization) were made in accordance with Worrall et al. [4].\nTraining For training, we choose the same hyperparameters as proposed in Worrall et al. [4].\nTesting For novel view synthesis on the training set, the model receives as input the nearest neighbor of the novel view in terms of cosine similarity. For two-shot reconstruction, the model receives as input whichever of the two reference views is closer to the novel view. Finally, for one-shot reconstruction, the model receives as input the single reference view.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Differentiable Ray-Marching in the context of classical renderers", "text": "The proposed neural ray-marcher is inspired by the classic sphere tracing algorithm [13]. Sphere tracing was originally developed to render scenes represented via analytical signed distance functions.\nIt is defined by a special choice of the step length: each step has a length equal to the signed distance to the closest surface point of the scene. Since this distance is only zero on the surface of the scene, the algorithm takes non-zero steps until it has arrived at the surface, at which point no further steps are taken. A major downside of sphere-tracing is its weak convergence guarantee: Sphere tracing is only guaranteed to converge for an infinite number of steps. This is easy to see: For any fixed number of steps, we can construct a scene where a ray is parallel to a close surface (or falls through a slim tunnel) and eventually intersects a scene surface. For any constant number of steps, there exists a surface parallel to the ray that is so close that the ray will not reach the target surface. In classical sphere-tracing, this is circumvented by taking a large number of steps that generally take the intersection estimate within a small neighborhood of the scene surface -the color at this point is then simply defined as the color of the closest surface. However, this heuristic can still fail in constructed examples such as the one above. Extensions of sphere tracing propose heuristics to modifying the step length to speed up convergence [11]. The Ray-Marching LSTM instead has the ability to learn the step length. The key driver of computational and memory cost of the proposed rendering algorithm is the ray-marching itself: In every step of the ray-marcher, for every pixel, the scene representation \u03c6 is evaluated. Each evaluation of \u03c6 is a full forward pass through a multi-layer perceptron. See 3.2 for an exact analysis of memory and computational complexity of the different components.\nOther classical rendering algorithms usually follow a different approach. In modern computer graphics, scenes are often represented via explicit, discretized surface primitives -such as is the case in meshes. This allows rendering via rasterization, where scene geometry is projected onto the image plane of a virtual camera in a single step. As a result, rasterization is computationally cheap, and has allowed for real-time rendering that has approached photo-realism in computer graphics.\nHowever, the image formation model of rasterization is not appropriate to simulate physically accurate image formations that involve proper light transport, view-dependent effects, participating media, refraction, translucency etc. As a result, physics-based rendering usually uses ray-tracing algorithms, where for each pixel, a number of rays are traced from the camera via all possible paths to light sources through the scene. If the underlying scene representations are explicit, discrete representations -such as meshes -the intersection testing required is again cheap. Main drivers of computational complexity in such systems are then the number of rays that need to be traced to appropriately sample all paths to lights sources that contribute to the value of a single pixel.\nIn this context, the proposed ray-marcher can be thought of as a sphere-tracing-inspired ray-tracer for implicitly defined scene geometry. It does not currently model multi-bounce ray-tracing, but could potentially be extended in the future (see 8).\n7 Trade-offs of the Pixel Generator vs. CNN-based renderers\nAs described in the main paper, the pixel generator comes with a guarantee of multi-view consistency compared to a 2D-CNN based rendering network. On the flip side, we cannot make use of progress in the design of novel CNN architectures that save memory by introducing resolution bottlenecks and skip connections, such as the U-Net [14]. This means that the pixel generator is comparably memoryhungry, as each layer operates on the full resolution of the image to be generated. Furthermore, CNNs have empirically been demonstrated to be able to generate high-frequency image detail easily. It is unclear what the limitations of the proposed pipeline are with respect to generating high-frequency textural detail. We note that the pixel generator is not a necessary component of SRNs, and can be replaced by a classic 2D-CNN based renderer, as we demonstrate in 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Future work", "text": "Applications outside of vision. SRNs have promising applications outside of vision. Neural scene representations are a core aspect of artificial intelligence, as they allow an agent to model its environment, navigate, and plan interactions. Thus, natural applications of SRNs lie in robotic manipulation or as the world model of an independent agent.\nExtending SRNs to other image formation models. SRNs could be extended to other image formation models, such as computer tomography or magnetic resonance imaging. All that is required is a differentiable forward model of the image formation. The ray-marcher could be adapted accordingly to integrate features along a ray or to sample at pre-defined locations. For image formation models that observe scenes directly in 3D, the ray-marcher may be left out completely, and \u03c6 may be sampled directly.\nProbabilistic formulation. An interesting avenue of future work is to extend SRNs to a probabilistic model that can infer a probability distribution over feasible scenes consistent with a given set of observations. In the following, we formulate one such approach, very similar to the formulation of Kumar et al. [15], which is in turn based on the work of Eslami et al. [12]. Please note that this formulation is not experimentally verified in the context of SRNs and is described here purely to facilitate further research in this direction.\nFormally, the model can be summarized as:\nr i = M (I i , E i , K i )(8)\nr = i r i (9)\nz \u223c P \u0398 (z|r)(10)\n\u03c6 = \u03a8(z)(11)\nI = \u0398(\u03a6 \u03c6 , E, K)(12)\nWe assume that we are given a set of instance datasets D = {C j } M j=1 , where each C j consists of tuples {(I i , E i , K i )} N i=1 . For a single scene C with n observations, we first replicate and concatenate the camera pose E i and intrinsic parameters K i of each observations to the image channels of the corresponding 2D image I i . Using a learned convolutional encoder M , we encode each of the n observations to a code vector r i . These code vectors r i are then summed to form a permutationinvariant representation of the scene r. Via an autoregressive DRAW model [16], we form a probability distribution P \u03b8 that is conditioned on the code vector r and sample latent variables z. z is decoded into the parameters of a scene representation network, \u03c6, via a hypernetwork \u03a8(z) = \u03c6. Lastly, via our differentiable rendering function \u0398, we can render images I from \u03a6 \u03c6 as described in the main paper. This allows to train the full model end-to-end given only 2D images and their camera parameters. We note that the resulting optimization problem is intractable and requires the optimization of an evidence lower bound via an approximate posterior, which we do not derive hereplease refer to [15]. Similarly to [15], this formulation will lead to multi-view consistent renderings of each scene, as the scene representation \u03a6 stays constant across queries of \u0398.\nView-and lighting-dependent effects, translucency, and participating media. Another exciting direction for future work is to model further aspects of realistic scenes. One such aspect is viewand lighting dependent effects, such as specularities. For fixed lighting, the pixel generator could receive as input the direction of the camera ray in world coordinates, and could thus reason about the view-dependent color of a surface. To model simple lighting-dependent effects, the pixel generator could further receive the light ray direction as an input (assuming no occlusions). Lastly, the proposed formulation could also be extended to model multiple ray bounces in a ray-casting framework. To model translucency and participating media, the ray-marcher could be extended to sum features along a ray instead of only sampling a feature at the final intersection estimate.\nComplex 3D scenes and compositionality. While SRNs can represent room-scale scenes (see supplementary video), generalization across such complex, cluttered 3D environments is an open problem. To the best of our knowledge, is has not yet been demonstrated that low-dimensional embeddings are a feasible representation for photo-realistic, general 3D environments. Recent work in meta-learning could enable generalization across scenes without the limitation to a highly low-dimensional manifold [17].", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Single-view to multi-view: Reconstructing unseen views with a convolutional network", "journal": "", "year": "2015", "authors": "M Tatarchenko; A Dosovitskiy; T Brox"}, {"ref_id": "b1", "title": "Neural scene representation and rendering", "journal": "Science", "year": "2018", "authors": "S A Eslami; D J Rezende; F Besse; F Viola; A S Morcos; M Garnelo; A Ruderman; A A Rusu; I Danihelka; K Gregor"}, {"ref_id": "b2", "title": "Consistent jumpy predictions for videos and scenes", "journal": "", "year": "2018", "authors": "A Kumar; S A Eslami; D Rezende; M Garnelo; F Viola; E Lockhart; M Shanahan"}, {"ref_id": "b3", "title": "Interpretable transformations with encoder-decoder networks", "journal": "", "year": "2017", "authors": "D E Worrall; S J Garbin; D Turmukhambetov; G J Brostow"}, {"ref_id": "b4", "title": "Voxnet: A 3d convolutional neural network for real-time object recognition", "journal": "", "year": "2015-09", "authors": "D Maturana; S Scherer"}, {"ref_id": "b5", "title": "Deepvoxels: Learning persistent 3d feature embeddings", "journal": "", "year": "2019", "authors": "V Sitzmann; J Thies; F Heide; M Nie\u00dfner; G Wetzstein; M Zollh\u00f6fer"}, {"ref_id": "b6", "title": "Learning a multi-view stereo machine", "journal": "", "year": "2017", "authors": "A Kar; C H\u00e4ne; J Malik"}, {"ref_id": "b7", "title": "Learning spatial common sense with geometry-aware recurrent networks", "journal": "", "year": "2019", "authors": "H.-Y F Tung; R Cheng; K Fragkiadaki"}, {"ref_id": "b8", "title": "Rendernet: A deep convolutional network for differentiable rendering from 3d shapes", "journal": "", "year": "2018", "authors": "T H Nguyen-Phuoc; C Li; S Balaban; Y Yang"}, {"ref_id": "b9", "title": "Visual object networks: image generation with disentangled 3d representations", "journal": "", "year": "2018", "authors": "J.-Y Zhu; Z Zhang; C Zhang; J Wu; A Torralba; J Tenenbaum; B Freeman"}, {"ref_id": "b10", "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "journal": "", "year": "2017", "authors": "C R Qi; H Su; K Mo; L J Guibas"}, {"ref_id": "b11", "title": "Unsupervised learning of shape and pose with differentiable point clouds", "journal": "", "year": "2018", "authors": "E Insafutdinov; A Dosovitskiy"}, {"ref_id": "b12", "title": "Neural rerendering in the wild,\" Proc. CVPR", "journal": "", "year": "2019", "authors": "M Meshry; D B Goldman; S Khamis; H Hoppe; R Pandey; N Snavely; R Martin-Brualla"}, {"ref_id": "b13", "title": "Learning efficient point cloud generation for dense 3d object reconstruction", "journal": "", "year": "2018", "authors": "C.-H Lin; C Kong; S Lucey"}, {"ref_id": "b14", "title": "Learning free-form deformations for 3d object reconstruction", "journal": "CoRR", "year": "2018", "authors": "D Jack; J K Pontes; S Sridharan; C Fookes; S Shirazi; F Maire; A Eriksson"}, {"ref_id": "b15", "title": "Multi-view supervision for single-view reconstruction via differentiable ray consistency", "journal": "", "year": "", "authors": "S Tulsiani; T Zhou; A A Efros; J Malik"}, {"ref_id": "b16", "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "journal": "", "year": "2016", "authors": "J Wu; C Zhang; T Xue; W T Freeman; J B Tenenbaum"}, {"ref_id": "b17", "title": "3d shape induction from 2d views of multiple objects", "journal": "IEEE Computer Society", "year": "2017", "authors": "M Gadelha; S Maji; R Wang"}, {"ref_id": "b18", "title": "Volumetric and multi-view cnns for object classification on 3d data", "journal": "", "year": "2016", "authors": "C R Qi; H Su; M Nie\u00dfner; A Dai; M Yan; L Guibas"}, {"ref_id": "b19", "title": "Pix3d: Dataset and methods for single-image 3d shape modeling", "journal": "", "year": "2018", "authors": "X Sun; J Wu; X Zhang; Z Zhang; C Zhang; T Xue; J B Tenenbaum; W T Freeman"}, {"ref_id": "b20", "title": "Unsupervised learning of 3d structure from images", "journal": "", "year": "2016", "authors": "D Jimenez Rezende; S M A Eslami; S Mohamed; P Battaglia; M Jaderberg; N Heess"}, {"ref_id": "b21", "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction", "journal": "", "year": "2016", "authors": "C B Choy; D Xu; J Gwak; K Chen; S Savarese"}, {"ref_id": "b22", "title": "Octnet: Learning deep 3d representations at high resolutions", "journal": "", "year": "2017", "authors": "G Riegler; A O Ulusoy; A Geiger"}, {"ref_id": "b23", "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs", "journal": "", "year": "2017", "authors": "M Tatarchenko; A Dosovitskiy; T Brox"}, {"ref_id": "b24", "title": "Hierarchical surface prediction", "journal": "", "year": "2019", "authors": "C Haene; S Tulsiani; J Malik"}, {"ref_id": "b25", "title": "Learning representations and generative models for 3D point clouds", "journal": "", "year": "2018", "authors": "P Achlioptas; O Diamanti; I Mitliagkas; L Guibas"}, {"ref_id": "b26", "title": "Multi-view 3d models from single images with a convolutional network", "journal": "", "year": "2016", "authors": "M Tatarchenko; A Dosovitskiy; T Brox"}, {"ref_id": "b27", "title": "Stereo magnification: learning view synthesis using multiplane images", "journal": "ACM Trans. Graph", "year": "2018", "authors": "T Zhou; R Tucker; J Flynn; G Fyffe; N Snavely"}, {"ref_id": "b28", "title": "Atlasnet: A papier-m\u00e2ch\u00e9 approach to learning 3d surface generation", "journal": "", "year": "2018", "authors": "T Groueix; M Fisher; V G Kim; B C Russell; M Aubry"}, {"ref_id": "b29", "title": "Neural 3d mesh renderer", "journal": "", "year": "2018", "authors": "H Kato; Y Ushiku; T Harada"}, {"ref_id": "b30", "title": "Learning category-specific mesh reconstruction from image collections", "journal": "", "year": "2018", "authors": "A Kanazawa; S Tulsiani; A A Efros; J Malik"}, {"ref_id": "b31", "title": "Occupancy networks: Learning 3d reconstruction in function space", "journal": "", "year": "2019", "authors": "L Mescheder; M Oechsle; M Niemeyer; S Nowozin; A Geiger"}, {"ref_id": "b32", "title": "Deepsdf: Learning continuous signed distance functions for shape representation", "journal": "", "year": "2019", "authors": "J J Park; P Florence; J Straub; R Newcombe; S Lovegrove"}, {"ref_id": "b33", "title": "Learning shape templates with structured implicit functions", "journal": "", "year": "2019", "authors": "K Genova; F Cole; D Vlasic; A Sarna; W T Freeman; T Funkhouser"}, {"ref_id": "b34", "title": "Cvxnets: Learnable convex decomposition", "journal": "", "year": "2019", "authors": "B Deng; K Genova; S Yazdani; S Bouaziz; G Hinton; A Tagliasacchi"}, {"ref_id": "b35", "title": "Hologan: Unsupervised learning of 3d representations from natural images", "journal": "", "year": "2019", "authors": "T Nguyen-Phuoc; C Li; L Theis; C Richardt; Y Yang"}, {"ref_id": "b36", "title": "Graph element networks: adaptive, structured computation and memory", "journal": "", "year": "2019", "authors": "F Alet; A K Jeewajee; M Bauza; A Rodriguez; T Lozano-Perez; L P Kaelbling"}, {"ref_id": "b37", "title": "Learning to describe scenes with programs", "journal": "", "year": "2019", "authors": "Y Liu; Z Wu; D Ritchie; W T Freeman; J B Tenenbaum; J Wu"}, {"ref_id": "b38", "title": "Shapenet: An information-rich 3d model repository", "journal": "", "year": "2015", "authors": "A X Chang; T Funkhouser; L Guibas; P Hanrahan; Q Huang; Z Li; S Savarese; M Savva; S Song; H Su"}, {"ref_id": "b39", "title": "Reducing the dimensionality of data with neural networks", "journal": "Science", "year": "2006-07", "authors": "G E Hinton; R Salakhutdinov"}, {"ref_id": "b40", "title": "Auto-encoding variational bayes", "journal": "", "year": "2013", "authors": "D P Kingma; M Welling"}, {"ref_id": "b41", "title": "NICE: non-linear independent components estimation", "journal": "", "year": "2015", "authors": "L Dinh; D Krueger; Y Bengio"}, {"ref_id": "b42", "title": "Glow: Generative flow with invertible 1x1 convolutions", "journal": "NeurIPS", "year": "2018", "authors": "D P Kingma; P "}, {"ref_id": "b43", "title": "Conditional image generation with pixelcnn decoders", "journal": "", "year": "2016", "authors": "A V Oord; N Kalchbrenner; O Vinyals; L Espeholt; A Graves; K Kavukcuoglu"}, {"ref_id": "b44", "title": "Pixel recurrent neural networks", "journal": "", "year": "2016", "authors": "A V Oord; N Kalchbrenner; K Kavukcuoglu"}, {"ref_id": "b45", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "I J Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b46", "title": "Wasserstein generative adversarial networks", "journal": "", "year": "2017", "authors": "M Arjovsky; S Chintala; L Bottou"}, {"ref_id": "b47", "title": "Progressive growing of gans for improved quality, stability, and variation", "journal": "", "year": "2018", "authors": "T Karras; T Aila; S Laine; J Lehtinen"}, {"ref_id": "b48", "title": "Generative visual manipulation on the natural image manifold", "journal": "", "year": "2016", "authors": "J.-Y Zhu; P Kr\u00e4henb\u00fchl; E Shechtman; A A Efros"}, {"ref_id": "b49", "title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "journal": "", "year": "2016", "authors": "A Radford; L Metz; S Chintala"}, {"ref_id": "b50", "title": "Conditional generative adversarial nets", "journal": "", "year": "2014", "authors": "M Mirza; S Osindero"}, {"ref_id": "b51", "title": "Image-to-image translation with conditional adversarial networks", "journal": "", "year": "2017", "authors": "P Isola; J.-Y Zhu; T Zhou; A A Efros"}, {"ref_id": "b52", "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "journal": "", "year": "2017", "authors": "J.-Y Zhu; T Park; P Isola; A A Efros"}, {"ref_id": "b53", "title": "Compositional pattern producing networks: A novel abstraction of development", "journal": "", "year": "2007", "authors": "K O Stanley"}, {"ref_id": "b54", "title": "Differentiable image parameterizations", "journal": "Distill", "year": "2018", "authors": "A Mordvintsev; N Pezzotti; L Schubert; C Olah"}, {"ref_id": "b55", "title": "Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision", "journal": "", "year": "2016", "authors": "X Yan; J Yang; E Yumer; Y Guo; H Lee"}, {"ref_id": "b56", "title": "Spatial transformer networks", "journal": "", "year": "2015", "authors": "M Jaderberg; K Simonyan; A Zisserman; K Kavukcuoglu"}, {"ref_id": "b57", "title": "Transforming auto-encoders", "journal": "", "year": "2011", "authors": "G E Hinton; A Krizhevsky; S D Wang"}, {"ref_id": "b58", "title": "Vision as Bayesian inference: analysis by synthesis?", "journal": "Trends in Cognitive Sciences", "year": "2006", "authors": "A Yuille; D Kersten"}, {"ref_id": "b59", "title": "Analysis by synthesis: A (re-)emerging program of research for language and vision", "journal": "Biolinguistics", "year": "2010", "authors": "T Bever; D Poeppel"}, {"ref_id": "b60", "title": "Deep convolutional inverse graphics network", "journal": "", "year": "2015", "authors": "T D Kulkarni; W F Whitney; P Kohli; J Tenenbaum"}, {"ref_id": "b61", "title": "Weakly-supervised disentangling with recurrent transformations for 3d view synthesis", "journal": "", "year": "2015", "authors": "J Yang; S Reed; M.-H Yang; H Lee"}, {"ref_id": "b62", "title": "Picture: A probabilistic programming language for scene perception", "journal": "", "year": "2015", "authors": "T D Kulkarni; P Kohli; J B Tenenbaum; V K Mansinghka"}, {"ref_id": "b63", "title": "Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision", "journal": "", "year": "", "authors": "H F Tung; A W Harley; W Seto; K Fragkiadaki"}, {"ref_id": "b64", "title": "Neural face editing with intrinsic image disentangling", "journal": "", "year": "2017", "authors": "Z Shu; E Yumer; S Hadap; K Sunkavalli; E Shechtman; D Samaras"}, {"ref_id": "b65", "title": "Multiple View Geometry in Computer Vision", "journal": "Cambridge University Press", "year": "2003", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b66", "title": "Sphere tracing: A geometric method for the antialiased ray tracing of implicit surfaces", "journal": "", "year": "1996", "authors": "J C Hart"}, {"ref_id": "b67", "title": "Conditional image generation with pixelcnn decoders", "journal": "", "year": "2016", "authors": "A Van Den Oord; N Kalchbrenner; L Espeholt; O Vinyals; A Graves"}, {"ref_id": "b68", "title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "S Hochreiter; J Schmidhuber"}, {"ref_id": "b69", "title": "Proc. ICLR", "journal": "", "year": "2017", "authors": "D Ha; A Dai; Q V Le;  Hypernetworks"}, {"ref_id": "b70", "title": "A 3d face model for pose and illumination invariant face recognition", "journal": "Ieee", "year": "2009", "authors": "P Paysan; R Knothe; B Amberg; S Romdhani; T Vetter"}, {"ref_id": "b71", "title": "Ba-net: Dense bundle adjustment network", "journal": "", "year": "2019", "authors": "C Tang; P Tan"}, {"ref_id": "b72", "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "journal": "", "year": "2017", "authors": "C Finn; P Abbeel; S Levine"}, {"ref_id": "b73", "title": "Deepvoxels: Learning persistent 3d feature embeddings", "journal": "", "year": "2019", "authors": "V Sitzmann; J Thies; F Heide; M Nie\u00dfner; G Wetzstein; M Zollh\u00f6fer"}, {"ref_id": "b74", "title": "Image-to-image translation with conditional adversarial networks", "journal": "", "year": "2017", "authors": "P Isola; J.-Y Zhu; T Zhou; A A Efros"}, {"ref_id": "b75", "title": "Single-view to multi-view: Reconstructing unseen views with a convolutional network", "journal": "", "year": "2015", "authors": "M Tatarchenko; A Dosovitskiy; T Brox"}, {"ref_id": "b76", "title": "Interpretable transformations with encoder-decoder networks", "journal": "", "year": "2017", "authors": "D E Worrall; S J Garbin; D Turmukhambetov; G J Brostow"}, {"ref_id": "b77", "title": "Transformation properties of learned visual representations", "journal": "", "year": "2014", "authors": "T S Cohen; M Welling"}, {"ref_id": "b78", "title": "Layer normalization", "journal": "", "year": "2016", "authors": "J L Ba; J R Kiros; G E Hinton"}, {"ref_id": "b79", "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "journal": "", "year": "2015", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b80", "title": "Face2face: Real-time face capture and reenactment of rgb videos", "journal": "", "year": "2016", "authors": "J Thies; M Zollhofer; M Stamminger; C Theobalt; M Nie\u00dfner"}, {"ref_id": "b81", "title": "Progressive growing of gans for improved quality, stability, and variation", "journal": "", "year": "2017", "authors": "T Karras; T Aila; S Laine; J Lehtinen"}, {"ref_id": "b82", "title": "Pixel recurrent neural networks", "journal": "", "year": "2016", "authors": "A V Oord; N Kalchbrenner; K Kavukcuoglu"}, {"ref_id": "b83", "title": "Conditional image generation with pixelcnn decoders", "journal": "", "year": "2016", "authors": "A V Oord; N Kalchbrenner; O Vinyals; L Espeholt; A Graves; K Kavukcuoglu"}, {"ref_id": "b84", "title": "Neural scene representation and rendering", "journal": "Science", "year": "2018", "authors": "S A Eslami; D J Rezende; F Besse; F Viola; A S Morcos; M Garnelo; A Ruderman; A A Rusu; I Danihelka; K Gregor"}, {"ref_id": "b85", "title": "Sphere tracing: A geometric method for the antialiased ray tracing of implicit surfaces", "journal": "", "year": "1996", "authors": "J C Hart"}, {"ref_id": "b86", "title": "U-net: Convolutional networks for biomedical image segmentation", "journal": "Springer", "year": "2015", "authors": "O Ronneberger; P Fischer; T Brox"}, {"ref_id": "b87", "title": "Consistent jumpy predictions for videos and scenes", "journal": "", "year": "2018", "authors": "A Kumar; S A Eslami; D Rezende; M Garnelo; F Viola; E Lockhart; M Shanahan"}, {"ref_id": "b88", "title": "Draw: A recurrent neural network for image generation", "journal": "", "year": "2015", "authors": "K Gregor; I Danihelka; A Graves; D J Rezende; D Wierstra"}, {"ref_id": "b89", "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "journal": "", "year": "2017", "authors": "C Finn; P Abbeel; S Levine"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Shepard-Metzler object from 1k-object training set, 15 observations each. SRNs (right) outperform dGQN (left) on this small dataset.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Non-rigid animation of a face. Note that mouth movement is directly reflected in the normal maps.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Interpolating latent code vectors of cars and chairs in the Shapenet dataset while rotating the camera around the model. Features smoothly transition from one model to another. Ground Truth Tatarchenko et al. SRNs 50-shot dGQN", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: Qualitative comparison with Tatarchenko et al.[1]  and the deterministic variant of the GQN[2], for novel view synthesis on the Shapenet v2 \"cars\" and \"chairs\" classes. We compare novel views for objects reconstructed from 50 observations in the training set (top row), two observations and a single observation (second and third row) from a test set. SRNs consistently outperforms these baselines with multi-view consistent novel views, while also reconstructing geometry. Please see the supplemental video for more comparisons, smooth camera trajectories, and reconstructed geometry.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Single-(left) and two-shot (both) reference views.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Pose extrapolation.Due to the explicit 3D-aware and per-pixel formulation, SRNs naturally generalize to 3D transformations that have never been seen during training, such as camera close-ups or camera roll, even when trained only on up-right camera poses distributed on a sphere around the objects. Please see the supplemental video for examples of pose extrapolation.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Failure cases.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 1 :1Figure1: Visualizations of ray marching progress and the final normal map. Note that the uniformly colored background does not constrain the depth -as a result, the depth is unconstrained around the silhouette of the object. Since the final normal map visualizes surface detail much better, we only report the final normal map in the main document.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 2 :2Figure 2: Qualitative results on DeepVoxels objects. For each object: Left: Normal map of reconstructed geometry. Center: SRNs output. Right: Ground Truth.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 5 :5Figure5: Architecture overview: at the heart of SRNs lies a continuous, 3D-aware neural scene representation, \u03a6, which represents a scene as a function that maps (x, y, z) world coordinates to a feature representation of the scene at those coordinates. To render \u03a6, a neural ray-marcher interacts with \u03a6 via world coordinates along camera rays, parameterized via their distance d to the camera projective center. Ray Marching begins at a distance d 0 close to the camera. In each step, the scene representation network \u03a6 is queried at the current world coordinates x i . The resulting feature vector v i is fed to the Ray Marching LSTM that predicts a step length \u03b4 i+1 . The world coordinates are updated according to the new distance to the camera, d i+1 = d i + \u03b4 i+1 . This is repeated for a fixed number of iterations, n. The features at the final world coordinates v n = \u03a6(x n ) are then translated to an RGB color by the pixel generator.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": ". The key difference to autoregressive per-pixel architectures lies in the modeling of the probability p(I) of an image I \u2208 R H\u00d7W \u00d73 . PixelCNN and PixelRNN model an image as a one-dimensional sequence of pixel values I 1 , ..., I H\u00d7W , and estimate their joint distribution as p(I) = H\u00d7W i=1 p(I i |I 1 , ..., I i\u22121 ).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 6 :6Figure 6: Architecture of the baseline method proposed in Worrall et al. [4].", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "24.31 / 0.92 20.38 / 0.83 21.33 / 0.88 18.41 / 0.80 21.27 / 0.88 18.15 / 0.79 WRL [4] 24.57 / 0.93 19.16 / 0.82 22.28 / 0.90 17.20 / 0.78 22.11 / 0.90 16.89 / 0.77 dGQN [2] 22.72 / 0.90 19.61 / 0.81 22.36 / 0.89 18.79 / 0.79 21.59 / 0.87 18.19 / 0.78 SRNs 26.23 / 0.95 26.32 / 0.94 24.48 / 0.92 22.94 / 0.88 22.89 / 0.91 20.72 / 0.85", "figure_data": "50 images (training set)2 imagesSingle imageChairsCarsChairsCarsChairsCarsTCO [1]"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Architecture Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Time & Memory Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.3 Dataset Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.4 SRNs Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.4.1 General details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.4.2 Per-experiment details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Deterministic Variant of GQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 Tatarchenko et al. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.3 Worrall et al. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8", "figure_data": "Raycast progress -from top left to bottom rightScene Representation Networks: Continuous -Supplementary Material-3D-Structure-Aware Neural Scene Representations Final Normal MapVincent Sitzmann {sitzmann, zollhoefer}@cs.stanford.edu, gordon.wetzstein@stanford.edu Michael Zollh\u00f6fer Gordon Wetzstein Stanford University Raycast progress -from top left to bottom rightFinal Normal MapContents1 Additional Results on Neural Ray Marching22 Comparison to DeepVoxels23 Reproducibility46 Acknowledgements 3.1 4 Relationship to per-pixel autoregressive methods65 Baseline Discussions75.1 6 Differentiable Ray-Marching in the context of classical renderers97 Trade-offs of the Pixel Generator vs. CNN-based renderers98 Future work9Preprint. Under review."}], "formulas": [{"formula_id": "formula_0", "formula_text": "a training set C = {(I i , E i , K i )} N i=1", "formula_coordinates": [3.0, 135.02, 381.13, 146.0, 18.77]}, {"formula_id": "formula_1", "formula_text": "\u03a6 : R 3 \u2192 R n , x \u2192 \u03a6(x) = v.", "formula_coordinates": [3.0, 238.77, 538.27, 134.46, 18.91]}, {"formula_id": "formula_2", "formula_text": "\u0398 : X \u00d7 R 3\u00d74 \u00d7 R 3\u00d73 \u2192 R H\u00d7W \u00d73 , (\u03a6, E, K) \u2192 \u0398(\u03a6, E, K) = I,(2)", "formula_coordinates": [4.0, 161.8, 122.57, 342.2, 19.13]}, {"formula_id": "formula_3", "formula_text": "1: function FINDINTERSECTION(\u03a6, K, E, (u, v)) 2: d0 \u2190 0.05", "formula_coordinates": [4.0, 112.98, 318.74, 184.45, 21.09]}, {"formula_id": "formula_4", "formula_text": "di+1 \u2190 di + \u03b4 Update d 9:", "formula_coordinates": [4.0, 112.98, 388.8, 391.02, 22.71]}, {"formula_id": "formula_5", "formula_text": "r u,v (d) = R T (K \u22121 u v d \u2212 t), d > 0,(3)", "formula_coordinates": [4.0, 218.46, 479.87, 285.54, 31.78]}, {"formula_id": "formula_6", "formula_text": "arg min d s.t. r u,v (d) \u2208 \u2126, d > 0 (4)", "formula_coordinates": [4.0, 241.7, 549.59, 262.31, 30.93]}, {"formula_id": "formula_7", "formula_text": "{(I i , E i , K i )} N i=1 as discussed in Sec. 3.1.", "formula_coordinates": [5.0, 108.0, 476.71, 396.0, 22.91]}, {"formula_id": "formula_8", "formula_text": "\u03a8 : R k \u2192 R l , z j \u2192 \u03a8(z j ) = \u03c6 j (5)", "formula_coordinates": [5.0, 235.36, 570.52, 268.64, 18.91]}, {"formula_id": "formula_9", "formula_text": "D = {C j } M j=1 of instance datasets C = {(I i , E i , K i )} N i=1", "formula_coordinates": [6.0, 234.6, 378.75, 233.47, 18.77]}, {"formula_id": "formula_10", "formula_text": "arg min {\u03b8,\u03c8,{zj} M j=1 } M j=1 N i=1 \u0398 \u03b8 (\u03a6 \u03a8(z j ) , E j i , K j i ) \u2212 I j i 2 2 Limg + \u03bb dep min(d j i,f inal , 0) 2 2 Ldepth + \u03bb lat z j 2 2 Llatent .(6)", "formula_coordinates": [6.0, 115.75, 432.09, 388.26, 41.45]}, {"formula_id": "formula_11", "formula_text": "{(I i , E i , K i )} N i=1 .", "formula_coordinates": [6.0, 108.0, 598.55, 74.34, 18.77]}, {"formula_id": "formula_12", "formula_text": "z = arg min z N i=1 \u0398 \u03b8 (\u03a6 \u03a8(z) , E i , K i ) \u2212 I i 2 2 + \u03bb dep min(d i,f inal , 0) 2 2 + \u03bb lat z 2 2 (7)", "formula_coordinates": [6.0, 135.94, 619.6, 368.06, 31.18]}, {"formula_id": "formula_14", "formula_text": ")2", "formula_coordinates": [19.0, 496.34, 206.05, 7.75, 12.01]}, {"formula_id": "formula_15", "formula_text": "Initial state (c 0 , h 0 , u 0 ) = (0, 0, 0) (3) Pre-process current canvas p l = \u03ba(u l ) (4) State update (c l+1 , h l+1 ) = C(E, r, c l , h l , p l ) (5) Canvas update u l+1 = u l + \u2206(h l+1 ) (6) Final output x = \u03b7(u L ),(7)", "formula_coordinates": [19.0, 159.31, 487.45, 344.79, 68.07]}, {"formula_id": "formula_16", "formula_text": "r i = M (I i , E i , K i )(8)", "formula_coordinates": [22.0, 265.81, 235.37, 238.28, 12.45]}, {"formula_id": "formula_17", "formula_text": "z \u223c P \u0398 (z|r)(10)", "formula_coordinates": [22.0, 268.56, 277.48, 235.54, 18.78]}, {"formula_id": "formula_18", "formula_text": "\u03c6 = \u03a8(z)(11)", "formula_coordinates": [22.0, 267.69, 291.39, 236.4, 12.01]}, {"formula_id": "formula_19", "formula_text": "I = \u0398(\u03a6 \u03c6 , E, K)(12)", "formula_coordinates": [22.0, 267.47, 305.29, 236.63, 18.78]}], "doi": ""}