{"title": "Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition", "authors": "Yuchen Hu; Ruizhe Li; Chen Chen; Chengwei Qin; Qiushi Zhu; Eng Siong Chng", "pub_date": "", "abstract": "Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noiseinvariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous stateof-the-arts on visual speech recognition task 1 .", "sections": [{"heading": "Introduction", "text": "The world surrounding us involves multiple modalities, including vision, audio, text, etc., which complement each other and jointly comprise human perception (Baltru\u0161aitis et al., 2018;Zhu et al., 2021b). Audio-visual speech recognition (AVSR) leverages both audio and visual modalities to understand human speech, which provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with noise-invariant lip movement information (Sumby and Pollack, 1954). 1 Code is available at https://github.com/YUCHE N005/UniVPM. However, most existing efforts still focus on audio modality to improve noise-robustness considering its dominance in AVSR, where audio modality contains much richer information to represent speech content than visual modality (Sataloff, 1992;Ren et al., 2021). Current mainstream approaches introduce noise adaptation techniques to improve robustness 2 , inspired by robust speech recognition . Most of them leverage noise-corrupted training data to strengthen robustness (Afouras et al., 2018a;Ma et al., 2021b;Song et al., 2022), and recent works extend it to selfsupervised learning scheme (Shi et al., 2022b;Hsu and Shi, 2022). Based on that, latest works introduce speech enhancement as front-end to denoise before recognition . Despite the effectiveness, these methods are usually faced with two practical challenges. First, they require abundant labeled noisy audio-visual data for network training, which is not always available in some real-world scenarios (Lin et al., 2021;Chen et al., 2022). Second, the well-trained model may not adapt to new-coming noise scenes in practical applications 2 , resulting in less optimal model generality (Meng et al., 2017). Therefore, our research idea in this paper is leveraging visual modality to develop a general noise-robust AVSR system while without dependence on noisy training data.\nWe may gain some inspirations from human perception mechanism of noisy audio-visual speech. Neuroscience studies (Nath and Beauchamp, 2011) find that human brain will unconsciously rely more on the lip movement to understand speech under noisy conditions (a.k.a., McGurk Effect, McGurk and MacDonald, 1976). During this process, instead of directly recognizing lip movement, human brain will first transfer it to speech signal in auditory cortex for further understanding (Bourguignon et al., 2020;M\u00e9gevand et al., 2020). With prior knowledge of lip-audio mapping, human brain can restore informative clean audio from lip movement under any noisy conditions to aid in speech understanding (Bernstein et al., 2004;Aller et al., 2022).\nMotivated by above observations, we propose a universal viseme-phoneme 3 mapping approach (UniVPM) to implement modality transfer, which can restore clean audio from lip movement to enable speech recognition under any noisy conditions. We first build two universal memory banks to model all the visemes and phonemes via online balanced clustering. Based on that, an adversarial mutual information estimator is proposed to construct strong viseme-phoneme mapping, which enables final lip-to-audio modality transfer via retrieval. As a result, our system can adapt well to any testing noises while without noisy training data. Empirical results show the effectiveness of our approach. Our contributions are summarized as:\n\u2022 We present UniVPM, a general noise-robust AVSR approach investigated on visual modality, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation.\n\u2022 We build two universal banks to model all the visemes and phonemes via online balanced clustering, followed by an adversarial mutual information estimator to construct strong mapping between them, which enables modality transfer to restore clean audio from lip movement for speech recognition under any noises.\n\u2022 Our UniVPM outperforms previous state-ofthe-arts on LRS3 and LRS2 benchmarks. Ex-tensive experiments also show its superiority on visual speech recognition (VSR) task.", "publication_ref": ["b7", "b64", "b58", "b57", "b0", "b40", "b63", "b60", "b36", "b14", "b46", "b48", "b11", "b45", "b10", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Audio-Visual Speech Recognition. AVSR provides a promising solution to noise-robust speech recognition with the noise-invariant visual modality (Afouras et al., 2018a). However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task (Sataloff, 1992;Ren et al., 2021). Mainstream approaches introduce noise adaptation techniques to strengthen robustness, where most of them leverage noise-corrupted data to improve network training (Afouras et al., 2018a;Ma et al., 2021b;Pan et al., 2022;Shi et al., 2022b;Hsu and Shi, 2022), and recent works further introduce speech enhancement as front-end to denoise before recognition . Despite the effectiveness, these methods require abundant labeled noisy audio-visual training data that is not always available in some real scenarios, and they may not adapt to the new-coming noise scenes in practical applications. In this work, we investigate the visual modality to develop a general noise-robust AVSR approach while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Memory Network. Memory network (Weston et al., 2014) presents a long-term memory component that can be read from and written in with inference capability. Miller et al. (2016) introduces key-value memory structure where key memory is used to address a query and the retrieved output is obtained from value memory using the address. Since this scheme can remember selected information, it is effective for augmenting features in many tasks, including video prediction (Lee et al., 2021), cross-modal retrieval (Song et al., 2018;Chen et al., 2020a), lip reading (Kim et al., 2021a and talking face generation (Park et al., 2022). Despite the advances, the memory network is prone to overfitting when handling imbalanced distributed data, a.k.a., long tail 4 (Liu et al., 2019)  2022), lip reading (Ren et al., 2021) and lip-tospeech synthesis (Prajwal et al., 2020). Among them, cross-modal distillation is a popular technique to transfer knowledge from viseme to phoneme (Afouras et al., 2020;Zhao et al., 2020;Ren et al., 2021). Other works design specific neural networks to learn their mapping (Qu et al., 2019;Kim et al., 2021b). Recent studies introduce selfsupervised learning to capture correlations between visemes and phonemes (Qu et al., 2021;Ma et al., 2021a). Though effective, these methods are often challenged by the ambiguity of homophenes (Bear and Harvey, 2017) where one lip shape can produce different sounds. To this end, we propose an adversarial mutual information estimator to construct strict viseme-phoneme mapping with the strong distinguishing ability of adversarial learning.", "publication_ref": ["b0", "b58", "b57", "b0", "b40", "b49", "b60", "b70", "b47", "b35", "b62", "b15", "b29", "b50", "b37", "b57", "b53", "b2", "b76", "b57", "b54", "b30", "b55", "b39", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "The overall framework of proposed UniVPM is illustrated in Fig. 2. During training, we first send the input video and clean audio streams into two front-ends for processing, which generates modality sequences f v , f a \u2208 R T \u00d7D , where T is number of frames and D is embedding dimension. These frames are sent into two memory banks to model all the visemes and phonemes, using an online balanced clustering algorithm where each cluster center represents a specific viseme or phoneme. Then, we propose an adversarial mutual information estimator to construct strong mapping between corresponding visemes and phonemes. Based on that, we finally implement modality transfer via retrieval to restore clean audio from visual signals, which enables speech recognition under any testing noises.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Online Balanced Clustering", "text": "Clustering is a widely used knowledge discovery technique to partition a set of data points into homogeneous groups, which has a variety of applications such as data mining (Fayyad et al., 1996). Among them, K-Means algorithm (MacQueen, 1967) is the most well-known and popular one. However, it cannot be directly applied for our viseme and phoneme clustering due to imbalanced data distribution (see \u00a7A.4). This may challenge K-Means clustering according to uniform effect (Xiong et al., 2006). As shown in Fig. 3 (a), most cluster centers gather in the majority data class (i.e., over-fitting), leaving the minority class not well modeled.\nTo this end, we propose an Online Balanced Clustering algorithm in Alg. 1 to model all the visemes and phonemes equally from input frames. Set a random weight \u03b1 \u2208 (0, 1) 19:\nFind the nearest sample dnear to ci in Bi 20:\ndnew = dnear \u2022 \u03b1 + ci \u2022 (1 \u2212 \u03b1) 21:\nBi.append(dnew) 22:\nUpdate B accordingly 23:\nend if 24:\nend for 25: end for First, we set the number of clusters N to 40, following the amount of English phonemes (Phy, 2022). Then, we set a maximum cluster size S max (i.e., number of samples in each cluster) to control the total memory. We also initialize an empty bank B as an overall cache, as well as a list of empty banks {B 1 , B 2 , ..., B N } to cache each cluster.\nThe proposed algorithm is executed in three steps, center initialization, K-Means clustering and re-sampling. First, we collect the first few batches of data frames into B to initialize N dispersed cluster centers {c 1 , c 2 , ..., c N }, using K-Means++ algorithm (Arthur and Vassilvitskii, 2006). Second, we add the current batch data to bank B and employ vanilla K-Means algorithm to re-allocate each sample in the bank to the nearest cluster center, after that the new cluster centers would be updated. Finally, we propose a re-sampling strategy to balance the size of different clusters as well as control the total memory of bank B, by setting a threshold cluster size S thr (line 12 in Alg. 1). For those clusters with more than S thr samples (i.e., majority cluster), we perform undersampling by only maintaining the S thr nearest samples to cluster center. In contrast, for the minority clusters with less samples than threshold, we propose oversampling to interpolate a new sample between center and the nearest sample with a random weight, inspired by SMOTE algorithm (Chawla et al., 2002). In this way, as illustrated in Fig. 3 (b), the resulted clusters would be balanced-sized and separated to better represent each of the visemes and phonemes.", "publication_ref": ["b21", "b42", "b71", "b13"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Adversarial Mutual Information Estimator", "text": "After clustering visemes and phonemes in banks, we propose an Adversarial Mutual Information Estimator (AMIE) to construct strong mapping between them. Mutual Information (MI) is a commonly used measure to explore the coherence between two distributions, which is, however, historically difficult to estimate. Recently, Belghazi et al. (2018) propose a Mutual Information Neural Estimation (MINE) approach to approximate MI lower bound with neural network. Based on that, we propose an adversarial learning approach to maximize the MI between visemes and phonemes, in order to construct strict mapping between them and thus alleviate the ambiguity of homophenes.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary Theory of MINE", "text": "Mutual information measures the mutual dependency between two probability distributions,\nI(X, Y ) = x\u2208X y\u2208Y p(x, y) log p(x, y) p(x)p(y) ,(1)\nwhere p(x, y) is the joint probability distribution of X and Y , and p(x) and p(y) are the marginals. Therefore, the mutual information can be written in terms of Kullback-Leibler (KL-) divergence:\nI(X, Y ) = D KL (p(x, y) \u2225 p(x)p(y)),(2)\nwhere D KL is defined as:\nD KL (p \u2225 q) = x\u2208X p(x) log p(x) q(x) ,(3)\nFurthermore, the KL-divergence admits the Donsker-Varadhan (DV) representation (Donsker and Varadhan, 1983;Belghazi et al., 2018):\nD KL (p \u2225 q) = sup T :\u2126\u2192R E p [T ] \u2212 log(E q [e T ]),(4)\nwhere the supremum is taken over all functions T on \u2126 \u2282 R d to guarantee two finite expectations. Therefore, we have the MI lower bound:\nI(X, Y ) \u2265 I \u0398 (X, Y ),(5)\nwhere I \u0398 is the neural information measure,\nI \u0398 (X, Y ) = sup \u03b8\u2208\u0398 E p(x,y) [T \u03b8 (x, y)] \u2212 log(E p(x)p(y) [e T \u03b8 (x,y) ]),(6)\nand T \u03b8 denotes a trainable neural network.", "publication_ref": ["b19", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Proposed AMIE", "text": "Based on MINE, we propose an Adversarial Mutual Information Estimator to explore and maximize the mutual information between clustered visemes and phonemes. As illustrated in Fig. 2 and 4, given a visual sequence f v , we send each frame of it into viseme bank to find the nearest cluster center c v , which forms the viseme sequence s v \u2208 R T \u00d7D . Similarly, we obtain a phoneme sequence s a to represent audio features f a . The neural network T \u03b8 then feeds {s v , s a } to output a scalar for MI estimation, where T \u03b8 is a 3-layer classifier with output as a 1-dimensional scalar. Furthermore, since we do not concern the accurate value of MI when maximizing it, we employ Jensen-Shannon (JS) representation  to approximate KL-divergence in Eq. 4, which has been proved with more stable neural network optimization. Therefore, the mutual information between clustered visemes and phonemes is estimated as:\nI JS \u0398 (s v , s a ) = sup \u03b8\u2208\u0398 E p(sv,sa) [\u2212sp(\u2212T \u03b8 (s v , s a ))] \u2212E p(sv)p(sa) [sp(T \u03b8 (s v ,s a ))],(7)\nwheres a is the shuffle-ordered version of s a that subjects to the marginal distributions of phonemes, and sp(z) = log(1 + e z ) is the softplus function.\nAs stated in Belghazi et al. (2018), the neural network T \u03b8 can be used to estimate MI between generated data (s v , s a in our case) by directly trained on them. However, this will suffer a lot from the poor quality of generated data at early training stage. One feasible scheme (Zhu et al., 2021a)  T \u03b8 on real data (f v , f a in our case) and then estimate MI on generated data, but this suffers from the ambiguity of homophenes (see Fig. 8). To this end, we propose AMIE with adversarial learning to estimate and maximize the MI between corresponding visemes and phonemes, which can construct strict viseme-phoneme mapping without ambiguity. Inspired by GAN (Goodfellow et al., 2014), we design the AMIE as discriminator and the visemephoneme banks as generator. Based on that, the adversarial loss is defined as:", "publication_ref": ["b9", "b22"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "is to train", "text": "L GAN = L D + L G = I JS \u0398 (f v , f a ) + [\u2212I JS \u0398 (s v , s a )],(8)\nOur framework employs an adversarial learning strategy for optimization, where D and G play a two-player minimax game as detailed in Alg. 2. As a result, the estimated MI between corresponding visemes and phonemes would be maximized to construct mapping relationships. The strong distinguishing ability of adversarial learning enables strict viseme-phoneme mapping to overcome the ambiguity of homophenes, as shown in Fig. 5.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Modality Transfer", "text": "With constructed viseme-phoneme mapping, we can finally implement modality transfer to restore clean audio from lips. As shown in Fig. 4, given the visual sequence f v and clustered phoneme centers {c 1 a , c 2 a , ..., c N a }, we calculate an addressing score A i,j to indicate the probability that the i-th visual frame corresponds to the j-th phoneme cluster:\nA i,j = exp(\u27e8f i v , c j a \u27e9/\u03c4 ) N k=1 exp(\u27e8f i v , c k a \u27e9/\u03c4 ) ,(9)\nwhere \u27e8 \u2022, \u2022 \u27e9 denotes cosine similarity, \u03c4 is temperature weight. The restored clean audio frames are: \nf i a = N j=1 A i,j \u2022 c j a ,(10)\n- 4.1 - - - - - - - - - - - - - - - 1.2 AV-HuBERT (2022b)\nClean Clean 72.6 30.9 9.8 2.9 2.1 23.7 93.4 71.6 22.1 6.1 2.7 39.2 24.1 10.9 3.6 2.4 1.9 8.6 1.42 Noisy 30.0 15.2 5.9 2.7 1.9 11.1 15.9 7.5 3.9 2.4 1.9 6.3 12.1 5.  \nTM-seq2seq (2018a) - Noisy - - - - - - - - - - - - - - - - - - 8.5 Hyb-RNN (2018) - Noisy - - - - - - - - - - - - - - - - - - 7.0 LF-MMI TDNN (2020) - Clean - - - - - - - - - - - - - - - - - - 5.9 Hyb-AVSR (2021b) - Noisy - - - - - - - - - - - - - - - - - - 3.7 MoCo+w2v2 (2022) - Noisy - - - - - - - - - - - - - - - - - - 2.6\nAV-HuBERT (2022b) Clean Clean 65.2 33.6 10.9 5.6 3.8 23.8 88.2 57.8 20.6 7.5 4.0 35.6 27.3 13.3 6.7 4.0 3.4 10.9 2.57 Noisy 33.2 16.3 7.6 4.6 3.7 13.1 14.9 9.5 6.2 4.5 3.8 7.8 13.9 9.0 4.9 3.9 3.2 7.0 2.38 Noisy Clean 36.9 18.6 8.1 4.8 3.5 14.4 24.6 9.7 4.8 3.6 3.4 9.2 15.2 8.4 5.1 3.8 3.1 7.1 2.44 Noisy 32.7 14.9 6.4 4.5 3.4 12.4 9.0 5.9 3.9 3.5 3.0 5.1 12.5 6.0 4.4 3.5 3.0 5.  To supervise the quality of restored audiof a = {f i a } T i=1 , we first employ AMIE to maximize the MI betweenf a and f v , where Eq. 8 is rewritten as:\nLGAN = I JS \u0398 (fv, fa) + [\u2212I JS \u0398 (sv, sa) \u2212 I JS \u0398 (fv,fa)],(11)\nalong with a reconstruction loss L rec = \u2225f a \u2212 f a \u2225 2 to enable restoration of high-quality clean audio.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Optimization", "text": "The UniVPM is optimized in an end-to-end manner (see Alg. 2), with the final training objective as:\nL = LASR + \u03bbGAN \u2022 LGAN + \u03bbrec \u2022 Lrec + \u03bbvar \u2022 Lvar,(12)\nwhere L ASR denotes the downstream speech recognition loss. L var is a variance regularization term to disperse the clustered viseme and phoneme centers, which aims to ease their mapping construction. \u03bb GAN , \u03bb rec and \u03bb var are weighting parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets. Our experiments are conducted on two large-scale public datasets, LRS3 (Afouras et al., 2018b) and LRS2 (Chung et al., 2017). LRS3 dataset collects 433 hours of transcribed English videos from TED & TEDx talks. LRS2 contains 224 hours of video speech from BBC programs. Configurations and Baselines. The proposed Uni-VPM is implemented based on AV-HuBERT with similar configurations, which are detailed in \u00a7B.3. We also select some mainstream AVSR approaches as baselines for comparison, e.g., u-HuBERT (Hsu and Shi, 2022), and details are presented in \u00a7B.7.", "publication_ref": ["b1", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "Audio-Visual Speech Recognition. Table 1 com Clean Clean 33.2 11.7 4.3 3.1 13.1 26.0 8.5 2.9 2.0 9.9 63.5 30.4 11.0 3.9 27.2 20.1 7.0 4.7 2.5 8.6 Noisy 10.6 5.2 2.9 2.5 5.3 10.1 4.3 2.3 1.8 4.6 27.8 14.4 4.9 2.6 12.4 7.6 4.5 2.9 2.0 4.3 Noisy Clean 17.7 7.1 4.0 2.9 7.9 16.0 5.8 2.7 1.9 6.6 49.5 19.5 6.2 3.1 19.6 11.8 5.9 3.7 2.2 5.9 Noisy 10.2 4.8 2.7 2.4 5.0 9.4 4.0 2.2 1.8 4.4 23.5 13.2 4.4 2.4 10.9 7.2 4.3 2.9 1.8 4.1", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Finetuned on MUSAN Noise", "text": "AV-HuBERT (2022b)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Clean", "text": "Clean 33.2 11.7 4.3 3.1 13.1 26.0 8.5 2.9 2.0 9.9 63.5 30.4 11.0 3.9 27.2 20.1 7.0 4.7 2.5 8.6 Noisy 13.9 6.3 3.3 2.8 6.6 13.6 5.1 2.6 1.9 5.8 36.1 17.5 5.7 2.9 15.6 9.9 5.3 3.5 2.1 5.2 Noisy Clean 17.7 7.1 4.0 2.9 7.9 16.0 5.8 2.7 1.9 6.6 49.5 19.5 6.2 3.1 19.6 11.8 5.9 3.7 2.2 5.9 Noisy 13.2 5.5 3.2 2.7 6.2 12.4 4.8 2.3 1.8 5.3 33.7 16.1 5.1 2.6 14.4 9.8 5.1 3.5 1.9 5.1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "UniVPM (ours)", "text": "Clean Clean 12.8 5.3 3.1 2.7 6.0 12.1 4.9 2.3 1.7 5.3 32.8 15.8 5.0 2.8 14.1 9.5 5.0 3.6 2.1 5.1 Noisy 10.0 4.7 2.7 2.4 5.0 9.6 4.0 2.2 1.6 4.4 24.9 13.3 4.7 2.6 11.4 7.0 4.3 2.9 1.8 4.0 Noisy Clean 11.9 5.1 3.0 2.6 5.7 10.8 4.6 2.2 1.7 4.8 27.4 14.8 4.9 2.6 12.4 8.3 4.7 3.2 1.8 4.5 Noisy 9.7 4.6 2.6 2.3 4.8 9.0 3.8 2.1 1.6 4.1 22.6 12.9 4.3 2.4 10.6 6.9 4.3 2.8 1.7 3.9  the-art in various noisy as well as clean conditions. Furthermore, we can also observe similar results on LRS2 dataset as shown in Table 2. Table 3 further compares the performance of Uni-VPM with AV-HuBERT on unseen testing noises, which are sampled from DEMAND (Thiemann et al., 2013) dataset. First, when AV-HuBERT is finetuned and tested both on DEMAND noise, good WER performance can be achieved. However, if it is finetuned on MUSAN noise and then tested on unseen DEMAND noise, the performance would degrade a lot. In comparison, our UniVPM finetuned on clean data (purple shades) achieves significant improvement and surpasses the AV-HuBERT finetuned on MUSAN noise, which further verifies the strong generality of our model. Furthermore, when finetuned on MUSAN noise, our UniVPM even outperforms the AV-HuBERT finetuned on in-domain DEMAND noise, which highlights the superiority of our approach on unseen test noises. Visual Speech Recognition. To further verify the effectiveness of UniVPM, we evaluate its VSR performance by discarding the input audio modality during inference, as shown in     in representing visemes and phonemes. Based on that, our proposed online balanced clustering achieves significant improvement by modeling all the visemes and phonemes equally without overfitting, which is further shown in Fig. 5.", "publication_ref": ["b65"], "figure_ref": ["fig_7"], "table_ref": ["tab_5", "tab_6"]}, {"heading": "Ablation Study", "text": "(a) (b) (c) (d) (f) (g) (i) (j) (k) (l) (e) (h)\nPhoneme Centers Phoneme Centers Phoneme Centers Phoneme Centers", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effect of AMIE.", "text": "As presented in Table 5, AMIE plays the key role in the promising performance of UniVPM by constructing strong viseme-phoneme mapping. As a comparison, the contrastive learning baseline only provides limited improvement, and MINE performs better by maximizing the estimated MI between visemes and phonemes. Based on that, our proposed AMIE introduces JS representation to stabilize system optimization, which improves performance but still suffers from the ambiguity of homophenes. To this end, our adversarial learning approach achieves further improvement by constructing strict viseme-phoneme mapping without ambiguity, as shown in Fig. 8.\nAnalysis of Adversarial Learning. As illustrated in Eq. 11, there are two key components in adversarial learning, i.e., I(s v , s a ) that constructs visemephoneme mapping and I(f v ,f a ) that supervises the quality of restored clean audio. Results in Table 5 indicate that viseme-phoneme mapping is the most important, and the supervision on restored clean audio also improves the AVSR performance.\nAnalysis of Regularization. According to Eq. 12, L rec and L var are two auxiliary terms for regular-ization, where the former supervises the quality of restored audio, and the latter disperses clustered viseme and phoneme centers to ease their mapping construction. Both of them are proved with positive contributions to the gains of performance. Visualizations. Fig. 5 presents t-SNE visualization and confusion matrixes to further verify the effectiveness of UniVPM. First, the online clustering baseline generates gathered viseme and phoneme centers due to over-fitting, where only several majority phonemes are modeled as shown in (g). Our proposed online balanced clustering alleviates such over-fitting issue and generates separated phoneme centers, which can cover most of the real phonemes as illustrated in (h). However, we can still observe gathered viseme centers due to homophenes, and the ambiguity of viseme-phoneme mapping is also shown in (k). To this end, our proposed AMIE effectively alleviates the ambiguity of homophenes thanks to the strong distinguishing ability of adversarial learning, which constructs strict visemephoneme mapping in (l). Meanwhile, we also observe dispersed viseme centers in (c), which can distinguish the same visemes that correspond to different phonemes. In addition, real phonemes are also better modeled by clustering as shown in (i).\nEvaluation of Modality Transfer.  rect phonemes, and the proposed online balanced clustering improves the accuracy but still limited by the ambiguity of homophenes. Furthermore, our proposed AMIE significantly improves the quality of restored clean audio with strict viseme-phoneme mapping, which also yields better VSR result.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_9", "tab_9"]}, {"heading": "Conclusion", "text": "In this paper, we propose UniVPM, a general robust AVSR approach motivated from visual modality via unsupervised noise adaptation. UniVPM constructs universal viseme-phoneme mapping to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noises. Experiments on public benchmarks show that UniVPM achieves state-of-the-art under various noisy as well as clean conditions. Further analysis also verifies its effectiveness on VSR task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "We state two points of limitations and future work in this section. First, the UniVPM combines both restored clean audio and original input audio for downstream speech recognition, while without any trade-off to weight them. For example, under extremely noisy conditions the restored clean audio plays a more important role, while in less noisy scenarios the original audio may provide more valid information. Some weighting strategies to select the most effective audio information could benefit the downstream speech recognition. Second, the proposed clustering and viseme-phoneme mapping are actually unsupervised schemes, so that it could be promising to extend our UniVPM to the popular self-supervised learning framework, in order to make full use of the abundant unlabeled data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "All the data used in this paper are publicly available and are used under the following licenses: the Creative Commons BY-NC-ND 4.0 License and Creative Commons Attribution 4.0 International License, the TED Terms of Use, the YouTube's Terms of Service, and the BBC's Terms of Use. The data is collected from TED and BBC and contain thousands of speakers from a wide range of races. To protect the anonymity, only mouth area of speaker is visualized wherever used in the paper. Table 7: WER (%) of AV-HuBERT on LRS3 benchmark. \"Mode\" denotes the input modality during both finetuning and inference stages, \"PT Type\" denotes the pre-training data type, \"FT Type\" denotes the finetuning data type, and \"avg\" denotes the average performance on all SNRs.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "A Supplementary Experimental Analysis", "text": "A.1 Analysis of the Noise-Robustness of AVSR  (Afouras et al., 2018a;Ma et al., 2021b;Pan et al., 2022;Shi et al., 2022b;Hsu and Shi, 2022). As shown in Table 7, available noisy training data significantly improves the AVSR performance in different testing conditions. However, this strategy is usually faced with two practical challenges. First, it requires abundant labeled noisy audio-visual training data, which is not always available in some real-world scenarios (Meng et al., 2017;Long et al., 2017;Lin et al., 2021;Chen et al., 2022). For instance, in scenarios like theatre, it is valuable to develop a AVSR system but costly to obtain sufficient training data. Second, as it is impossible to cover all the realworld noises in training data, when some unseen noises appear in practical testing scenarios, the well-trained model may not perform well as shown in Table 3, resulting in less optimal model generality (Meng et al., 2017 ", "publication_ref": ["b0", "b40", "b49", "b60", "b46", "b38", "b36", "b14", "b46"], "figure_ref": [], "table_ref": ["tab_14", "tab_6"]}, {"heading": "A.2 Analysis of Limited In-domain Noisy Audio-Visual Data", "text": "According to \u00a71 and \u00a7A.1, the first challenge of audio modality-based robust AVSR is the limited indomain noisy audio-visual data, which leads to domain mismatch between training and testing (Meng et al., 2017;Long et al., 2017;Lin et al., 2021;Chen et al., 2020cChen et al., , 2022. Actually there are two methods of obtaining such data, i.e., collection and simulation. First, we can collect and transcribe amounts of noisy audio-visual data under real-world scenarios, but that is extremely time-consuming and laborious, and to our best knowledge there is currently no such public dataset. Second, as there is sufficient clean transcribed audio-visual data (Afouras et al., 2018b;Chung et al., 2017), we can collect indomain noise to simulate noisy audio-visual data. However, this data augmentation method can only partially alleviate but not resolve the domain mismatch problem (Zhang et al., 2022). What is worse, the in-domain noise data is also not always available in all the real-world scenarios (Meng et al., 2017;Long et al., 2017;Chen et al., 2020cChen et al., , 2022.\nAs presented in Table 1, in case of no available in-domain noise, our UniVPM achieves comparable performance to previous state-of-the-art trained on in-domain noise. When in-domain noise is available, our UniVPM directly outperforms previous state-of-the-art, which breaks out the limit of data augmentation and moves one step forward to the real noisy data training setting (i.e., oracle). In addition, Table 3 further investigates the cases with outof-domain training noise, where our UniVPM even surpasses previous state-of-the-art trained on indomain noise. As a result, our proposed approach effectively alleviates the limitation of in-domain noisy data in audio modality-based robust AVSR.", "publication_ref": ["b46", "b38", "b36", "b17", "b14", "b1", "b18", "b74", "b46", "b38", "b17", "b14"], "figure_ref": [], "table_ref": ["tab_4", "tab_6"]}, {"heading": "A.3 Analysis of UniVPM from Meta-Learning Perspective", "text": "The main idea of our proposed UniVPM can also be explained from meta-learning perspective (Raghu et al., 2019), i.e., learn how to learn. In AVSR task, considering the inherent information sufficiency of visual modality to represent speech content (Sataloff, 1992;Ren et al., 2021), the key factor of its robustness is still the informative audio modality. However, audio is usually interfered by background noise during practical inference. Therefore, the key of improving robustness is to gain sufficient knowledge from clean audio in training stage, and metalearning exactly tells AVSR how to learn from the clean audio. Motivated by this idea, we leverage clean audio-visual data to train the core modules of UniVPM, i.e., viseme and phoneme banks, where video serves as \"prompt\" and clean audio serves as \"meta\". In particular, our UniVPM learns the mapping between visemes and phonemes, which then enables modality transfer to restore clean audio against testing noises. Here the viseme-phoneme mapping defines how to learn from clean audio. Therefore, we only need video \"prompt\" during inference to access the clean audio \"meta\", which enables UniVPM to adapt to any testing noises. A.4 Analysis of Phoneme Distribution in LRS3 and LRS2 Datasets Fig. 6 presents the phoneme distribution in LRS3 and LRS2 datasets. We can observe that in both datasets, the phoneme obeys a long-tail distribution (Liu et al., 2019) with head classes including 'h# ','ih','n','l','s','ah',etc. For better visualization,Fig. 7 removes the dominant phoneme 'h#' and also presents a long-tail distribution. Therefore, the neural network trained on these data is prone to over-fitting to head phoneme classes, resulting in less satisfactory performance on tail classes. LRS3 and LRS2 are both large-scale English reading speech datasets recorded with thousands of speakers from a wide range of races, so that they can be roughly representative of the phoneme distribution of English language.", "publication_ref": ["b56", "b58", "b57", "b37"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "B Experimental Details", "text": "B.1 Datasets LRS3 6 (Afouras et al., 2018b) is currently the largest public sentence-level lip reading dataset, which contains over 400 hours of English video extracted from TED and TEDx talks on YouTube. The training data is divided into two parts: pretrain (403 hours) and trainval (30 hours), and both of them are transcribed at sentence level. The pretrain part differs from trainval in that the duration of its video clips are at a much wider range. Since there is no official development set provided, we randomly select 1,200 samples from trainval as validation set (\u223c 1 hour) for early stopping and hyper-parameter tuning. In addition, it provides a standard test set (0.9 hours) for evaluation. LRS2 7 (Chung et al., 2017) is a large-scale publicly available labeled audio-visual (A-V) datasets, which consists of 224 hours of video clips from BBC programs. The training data is divided into three parts: pretrain (195 hours), train (28 hours) and val (0.6 hours), which are all transcribed at sentence level. An official test set (0.5 hours) is provided for evaluation use. The dataset is very challenging as there are large variations in head pose, lighting conditions and genres.", "publication_ref": ["b1", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Data Preprocessing", "text": "The data preprocessing for above two datasets follows the LRS3 preprocessing steps in prior work (Shi et al., 2022a). For the audio stream, we extract the 26-dimensional log filter-bank feature at a stride of 10 ms from input raw waveform. For the video clips, we detect the 68 facial keypoints using dlib toolkit (King, 2009) and align the image frame to a reference face frame via affine transformation. Then, we convert the image frame to gray-scale and crop a 96\u00d796 region-of-interest (ROI) centered on the detected mouth. During training, we randomly crop a 88\u00d788 region from the whole ROI and flip it horizontally with a probability of 0.5. At inference time, the 88\u00d788 ROI is center cropped without horizontal flipping. To synchronize these two modalities, we stack each 4 neighboring acoustic frames to match the image frames that are sampled at 25Hz.", "publication_ref": ["b59", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Model Configurations", "text": "Front-ends. We adopt the modified ResNet-18 from prior work (Shi et al., 2022a) as visual frontend, where the first convolutional layer is replaced by a 3D convolutional layer with kernel size of 5\u00d77\u00d77. The visual feature is flattened into an 1D vector by spatial average pooling in the end. For audio front-end, we use one linear projection layer followed by layer normalization (Ba et al., 2016). UniVPM. The viseme and phoneme banks contain N = 40 clusters, following the amount of English phonemes (Phy, 2022), i.e., 39 regular phonemes and one special phoneme ' [PAD]' that indicates silence. It is worth mentioning that the actual amount of visemes is less than phonemes due to homophene phenomenon, i.e., one-to-many lipaudio mapping (Bear and Harvey, 2017), but in this work we set same number of clusters to construct a strict one-to-one viseme-phoneme mapping, as shown in Fig. 5 and Fig. 8. The cluster capacity S max in Alg. 1 is set to 20, and the temperature \u03c4 in Eq. 9 is set to 0.1. Speech Recognition. The downstream speech recognition model follows AV-HuBERT (Shi et al., 2022b) with 24 Transformer (Vaswani et al., 2017) encoder layers and 9 decoder layers, where the embedding dimension/feed-forward dimension/attention heads in each Transformer layer are set to 1024/4096/16 respectively. We use a dropout of p = 0.1 after the self-attention block within each Transformer layer, and each Transformer layer is dropped (Fan et al., 2019) at a rate of 0.1.\nThe total number of parameters in our UniVPM and AV-HuBERT baseline are 478M and 476M.", "publication_ref": ["b59", "b8", "b60", "b66", "b20"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "B.4 Data Augmentation", "text": "Following prior work (Shi et al., 2022b), we use many noise categories for data augmentation to simulate noisy training data. We select the noise categories of \"babble\", \"music\" and \"natural\" from MUSAN noise dataset (Snyder et al., 2015), and extract some \"speech\" noise samples from LRS3 dataset. For experiments on unseen testing noises (see Table 3), we also select the noise categories of \"Meeting\", \"Cafe\", \"Resto\" and \"Station\" from DEMAND noise dataset (Thiemann et al., 2013). All categories are divided into training, validation and test partitions.\nDuring training process, we randomly select one noise category and sample a noise clip from its training partition. Then, we randomly mix the sampled noise with input clean audio, at signal-to-noise ratio (SNR) of 0dB with a probability of 0.25.\nAt inference time, we evaluate our model on clean and noisy test sets respectively. Specifically, the system performance on each noise type is evaluated separately, where the testing noise clips are added at five different SNR levels: {\u221210, \u22125, 0, 5, 10}dB. At last, the testing results on different noise types and SNR levels will be averaged to obtain the final noisy WER result.", "publication_ref": ["b60", "b61", "b65"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "B.5 Training and Inference", "text": "Training. The noisy training data is synthesized by adding random noise from MUSAN (Snyder et al., 2015) or DEMAND (Thiemann et al., 2013) of 0dB at a probability of 0.25. We load the pretrained AV-HuBERT 8 for front-ends and downstream speech recognition model, and then follow its sequence-to-sequence (S2S) finetuning configurations (Shi et al., 2022b) to train our system. We use Transformer decoder to decode the encoded features into unigram-based subword units (Kudo, 2018), where the vocabulary size is set to 1000. The weighting parameters \u03bb GAN /\u03bb rec /\u03bb var in Eq. 12 are set to 0.1/0.2/0.5, respectively. The entire system is trained for 60K steps using Adam optimizer (Kingma and Ba, 2014), where the learning rate is warmed up to a peak of 0.001 for the first 20K updates and then linearly decayed. The training process takes \u223c 2.5 days on 4 NVIDIA-V100-32GB GPUs, where in comparison the AV-HuBERT finetuning takes \u223c 1.3 days on 4 NVIDIA-V100-32GB GPUs. Inference. As shown in Table 1, the testing noises \"Babble\", \"Music\" and \"Natural\" are sampled from MUSAN, and \"Speech\" is drawn from LRS3, following prior work (Shi et al., 2022b). No language model is used during inference. We employ beam search for decoding, where the beam width and length penalty are set to 50 and 1 respectively. All hyper-parameters in our systems are tuned on validation set. Since our experimental results are quite stable, a single run is performed for each reported result.", "publication_ref": ["b61", "b65", "b60", "b34", "b33", "b60"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "B.6 Details of UniVPM Optimization", "text": "As detailed in Alg. 2, we design a two-step adversarial learning strategy for UniVPM optimization, where the discriminator and generator play a twoplayer minimax game. First, we maximize L GAN to update the discriminator, where generator is detached from optimization. According to Eq. 11, maximizing the first term of L GAN increases the MI between visual and audio sequences, while maximizing the second term is actually decreasing the system to model the A-V features separately and then attentively fuse them for decoding, and uses cross-entropy based sequence-tosequence loss as training criterion.\n\u2022 TM-CTC (Afouras et al., 2018a): TM-CTC shares the same architecture with TM-seq2seq, but uses CTC loss (Graves et al., 2006) as training criterion.\n\u2022 Hyb-RNN (Petridis et al., 2018): Hyb-RNN proposes a RNN-based AVSR model with hybrid seq2seq/CTC loss (Watanabe et al., 2017), where the A-V features are encoded separately and then concatenated for decoding.\n\u2022 RNN-T (Makino et al., 2019): RNN-T adopts the popular recurrent neural network transducer (Graves, 2012) for AVSR task, where the audio and visual features are concatenated before fed into the encoder.\n\u2022 EG-seq2seq : EG-seq2seq builds a joint audio enhancement and multimodal speech recognition system based on RNN (Zhang et al., 2019), where the A-V features are concatenated before decoding.\n\u2022 LF-MMI TDNN : LF-MMI TDNN proposes a joint audio-visual speech separation and recognition system based on time-delay neural network (TDNN), where the A-V features are concatenated before fed into the recognition network.\n\u2022 Hyb-AVSR (Ma et al., 2021b): Hyb-AVSR proposes a Conformer-based (Gulati et al., 2020) AVSR system with hybrid seq2seq/CTC loss, where the A-V input streams are first encoded separately and then concatenated for decoding.\n\u2022 MoCo+w2v2 (Pan et al., 2022): MoCo+w2v2 employs self-supervised pre-trained audio and visual front-ends, i.e., wav2vec 2.0 (Baevski et al., 2020) and MoCo v2 (Chen et al., 2020b), to generate better audio-visual features for fusion and decoding.\n\u2022 AV-HuBERT (Shi et al., 2022a,b): AV-HuBERT employs self-supervised learning to capture deep A-V contextual information, where the A-V features are masked and concatenated before fed into Transformer encoder to calculate masked-prediction loss for pretraining, and sequence-to-sequence loss is then used for finetuning.\n\u2022 u-HuBERT (Hsu and Shi, 2022): u-HuBERT extends AV-HuBERT to a unified framework of audio-visual and audio-only pre-training.\n\u2022 Distill-PT (Ma et al., 2022): Distill-PT proposes a Conformer-based VSR framework with additional distillation from pre-trained ASR and VSR models.", "publication_ref": ["b0", "b24", "b51", "b69", "b43", "b23", "b75", "b40", "b49", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "C Clustering Algorithms", "text": "C.1 Uniform Effect in K-Means K-Means (MacQueen, 1967) is the most popular and successful clustering algorithm, where sample re-allocation and center renewal are executed alternatively to minimize the intra-cluster distance. However, Xiong et al. (2006) points out that K-Means algorithm tends to produce balanced clustering result, a.k.a., uniform effect. This preference seriously degrades the performance when the clusters are imbalanced-sized. The consequence is that the center of minority clusters will gradually move to the territory of majority cluster, as illustrated in Fig. 3 (a). In other words, the K-Means algorithm will be over-fitted to majority clusters, leaving the samples in minority clusters not well modeled.", "publication_ref": ["b71"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "C.2 K-Means++", "text": "The performance of K-Means clustering relies on the center initialization, where the vanilla algorithm initialize cluster centers randomly. K-Means++ (Arthur and Vassilvitskii, 2006) is an improved version with dispersed initial centers. It determines cluster centers one by one, and each newly initialized center is pushed as distant as possible to the existed centers. As a result, the K initial cluster centers would separate from each other and benefit the subsequent clustering process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Details of Online Clustering Baseline", "text": "For comparison, we build an Online Clustering algorithm as baseline. It is similar to Alg. 1 but employs a vanilla random pruning strategy, instead of re-sampling, to control the total memory of the bank. Our strategy is to randomly keep S thr samples in the cluster if its number of samples exceeds S thr . Compared to the proposed Online Balanced Clustering algorithm, this baseline also controls where the clean audio is employed for phoneme clustering and the noisy audio is used to improve the system noise-robustness. Compared to Fig. 2, there is an extra data stream of noisy audio to improve robustness. memory size but ignores the imbalanced clusters, as indicated by the dashed ellipses in Fig. 3 (a).", "publication_ref": [], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "C.4 Principles of Online Balanced Clustering", "text": "According to Alg. 1, the main idea of proposed Online Balanced Clustering is the re-sampling operation to balance cluster sizes. For majority clusters, we perform undersampling to maintain the S thr nearest samples to cluster center, so that the gathered clusters in Fig. 3 (a) can be separated. For minority clusters, we introduce oversampling to interpolate a new sample near the center, so that the minority clusters are highlighted. As a result, all the clusters are balanced-sized and separated from each other as shown in Fig. 3 (b), so that the over-fitting problem is resolved. As a result, all of the visemes and phonemes can get well represented, which enables the subsequent visemephoneme mapping construction.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research is supported by KLASS Engineering & Solutions Pte Ltd and the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No.: AISG2-100E-2023-103). The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Algorithm 2 UniVPM Optimization.\nRequire: Training data Dtrain that contains visual-audio pairs (xv, xa) and the text transcription y. The UniVPM network \u03b8 that consists of visual front-end \u03b8 vf , audio frontend \u03b8 af , viseme bank Bv, phoneme bank Ba, AMIE \u03b8AMIE and speech recognition model \u03b8ASR. Hyperparameter weights \u03bbGAN , \u03bbrec, \u03bbvar. 1: Load pre-trained AV-HuBERT for \u03b8 vf , \u03b8 af and \u03b8ASR, randomly initialize \u03b8AMIE. 2: Initialize empty banks Bv and Ba. 3: while not converged do 4:\nfor (xv, xa) \u2208 Dtrain do 5:\nFORWARD-PROPAGATION: 6: fv = \u03b8 vf (xv), fa = \u03b8 af (xa) \u25b7 front-ends 7:\nUpdate Bv and Ba according to Alg. 1 8:\nObtain viseme sequence sv from fv and Bv 9:\nObtain phoneme sequence sa from fa and Ba 10:\nGenerate restored audiofa in Eq. 9 and 10 11:\u0177 = \u03b8ASR(fv \u2295 fa \u2295fa)\n\u25b7 recognition 12:\nTRAINING OBJECTIVES: 13:\nLGAN (LD and LG) in Eq. 11 14:\nLrec = \u2225fa \u2212 fa\u22252 15: end for 23: end while MI between visemes and phonemes, as well as the MI between visual and restored audio sequences (this is opposite to our desired viseme-phoneme mapping and modality transfer). Second, we freeze discriminator and update the rest network, where minimizing L G increases the MI between visemes and phonemes, as well as MI between visual and restored audio sequences. In addition, L ASR optimizes the downstream speech recognition model, L rec supervise the quality of restored clean audio, and L var disperses the viseme and phoneme centers to ease their mapping construction. The entire system is trained in an end-to-end manner.\nIn actual experiments, to save computation cost, we update B v and B a once every 10 epochs, which has been proved with no affect on the system performance. One can refer to our attached code for more implementation details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.7 Baselines", "text": "In this section, we describe the baselines for comparison.\n\u2022 TM-seq2seq (Afouras et al., 2018a) B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nNo response.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\nNo response.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "C Did you run computational experiments?", "text": "Section 4 and Appendix A C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Appendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance. Appendix B D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\nNo response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep audio-visual speech recognition", "journal": "", "year": "2018", "authors": "Triantafyllos Afouras; Joon Son Chung; Andrew Senior; Oriol Vinyals; Andrew Zisserman"}, {"ref_id": "b1", "title": "Joon Son Chung, and Andrew Zisserman", "journal": "", "year": "2018", "authors": "Triantafyllos Afouras"}, {"ref_id": "b2", "title": "Asr is all you need: Cross-modal distillation for lip reading", "journal": "IEEE", "year": "2020", "authors": "Triantafyllos Afouras; Joon Son Chung; Andrew Zisserman"}, {"ref_id": "b3", "title": "Differential auditory and visual phase-locking are observed during audio-visual benefit and silent lip-reading for speech perception", "journal": "Journal of Neuroscience", "year": "2022", "authors": "M\u00e1t\u00e9 Aller; Heidi Solberg \u00d8kland; Lucy J Macgregor; Helen Blank; Matthew H Davis"}, {"ref_id": "b4", "title": "2006. k-means++: The advantages of careful seeding", "journal": "", "year": "", "authors": "David Arthur; Sergei Vassilvitskii"}, {"ref_id": "b5", "title": "", "journal": "", "year": "", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hin"}, {"ref_id": "b6", "title": "Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "Alexei Baevski; Yuhao Zhou"}, {"ref_id": "b7", "title": "Multimodal machine learning: A 15221 survey and taxonomy", "journal": "", "year": "2018", "authors": "Tadas Baltru\u0161aitis; Chaitanya Ahuja; Louis-Philippe Morency"}, {"ref_id": "b8", "title": "Phoneme-toviseme mappings: the good, the bad, and the ugly", "journal": "Speech Communication", "year": "2017", "authors": "L Helen; Richard Bear;  Harvey"}, {"ref_id": "b9", "title": "Mutual information neural estimation", "journal": "PMLR", "year": "2018", "authors": "Mohamed Ishmael Belghazi; Aristide Baratin; Sai Rajeshwar; Sherjil Ozair; Yoshua Bengio; Aaron Courville; Devon Hjelm"}, {"ref_id": "b10", "title": "Auditory speech detection in noise enhanced by lipreading", "journal": "Speech Communication", "year": "2004", "authors": "Lynne E Bernstein; Edward T Auer Jr; Sumiko Takayanagi"}, {"ref_id": "b11", "title": "Lip-reading enables the brain to synthesize auditory features of unknown silent speech", "journal": "Journal of Neuroscience", "year": "2020", "authors": "Mathieu Bourguignon; Martijn Baart; C Efthymia; Nicola Kapnoula;  Molinaro"}, {"ref_id": "b12", "title": "Multi-modal pretraining for automated speech recognition", "journal": "IEEE", "year": "2022", "authors": "Shalini David M Chan; Debmalya Ghosh; Bj\u00f6rn Chakrabarty;  Hoffmeister"}, {"ref_id": "b13", "title": "Smote: synthetic minority over-sampling technique", "journal": "Journal of artificial intelligence research", "year": "2002", "authors": "V Nitesh; Kevin W Chawla; Lawrence O Bowyer; W Philip Hall;  Kegelmeyer"}, {"ref_id": "b14", "title": "Noise-robust speech recognition with 10 minutes unparalleled in-domain data", "journal": "IEEE", "year": "2022", "authors": "Chen Chen; Nana Hou; Yuchen Hu; Shashank Shirol; Eng Siong Chng"}, {"ref_id": "b15", "title": "Imram: Iterative matching with recurrent attention memory for crossmodal image-text retrieval", "journal": "", "year": "2020", "authors": "Hui Chen; Guiguang Ding; Xudong Liu; Zijia Lin; Ji Liu; Jungong Han"}, {"ref_id": "b16", "title": "Ross Girshick, and Kaiming He. 2020b. Improved baselines with momentum contrastive learning", "journal": "", "year": "", "authors": "Xinlei Chen; Haoqi Fan"}, {"ref_id": "b17", "title": "Data techniques for online end-to-end speech recognition", "journal": "", "year": "2020", "authors": "Yang Chen; Weiran Wang; I-Fan Chen; Chao Wang"}, {"ref_id": "b18", "title": "Lip reading sentences in the wild", "journal": "IEEE", "year": "2017", "authors": "Joon Son Chung; Andrew Senior; Oriol Vinyals; Andrew Zisserman"}, {"ref_id": "b19", "title": "Asymptotic evaluation of certain markov process expectations for large time", "journal": "iv. Communications on Pure and Applied Mathematics", "year": "1983", "authors": "D Monroe;  Donsker;  Sr Srinivasa Varadhan"}, {"ref_id": "b20", "title": "Reducing transformer depth on demand with structured dropout", "journal": "", "year": "2019", "authors": "Angela Fan; Edouard Grave; Armand Joulin"}, {"ref_id": "b21", "title": "Advances in knowledge discovery and data mining", "journal": "American Association for Artificial Intelligence", "year": "1996", "authors": "M Usama; Gregory Fayyad; Padhraic Piatetsky-Shapiro; Ramasamy Smyth;  Uthurusamy"}, {"ref_id": "b22", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b23", "title": "Sequence transduction with recurrent neural networks", "journal": "", "year": "2012", "authors": "Alex Graves"}, {"ref_id": "b24", "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "journal": "", "year": "2006", "authors": "Alex Graves; Santiago Fern\u00e1ndez; Faustino Gomez; J\u00fcrgen Schmidhuber"}, {"ref_id": "b25", "title": "Conformer: Convolution-augmented transformer for speech recognition", "journal": "", "year": "2020", "authors": "Anmol Gulati; James Qin; Chiu Chung-Cheng; Niki Parmar; Yu Zhang; Jiahui Yu; Wei Han; Shibo Wang; Zhengdong Zhang; Yonghui Wu; Ruoming Pang"}, {"ref_id": "b26", "title": "Learning deep representations by mutual information estimation and maximization", "journal": "", "year": "2018", "authors": "Alex R Devon Hjelm; Samuel Fedorov; Karan Lavoie-Marchildon; Phil Grewal; Adam Bachman; Yoshua Trischler;  Bengio"}, {"ref_id": "b27", "title": "Visual context-driven audio feature enhancement for robust end-to-end audio-visual speech recognition", "journal": "", "year": "2022", "authors": "Joanna Hong; Minsu Kim; Yong Man Ro"}, {"ref_id": "b28", "title": "2022. u-hubert: Unified mixed-modal speech pretraining and zero-shot transfer to unlabeled modality", "journal": "", "year": "", "authors": "Wei-Ning Hsu; Bowen Shi"}, {"ref_id": "b29", "title": "Multi-modality associative bridging through memory: Speech sound recollected from face video", "journal": "", "year": "2021", "authors": "Minsu Kim; Joanna Hong; Jin Se; Yong Man Park;  Ro"}, {"ref_id": "b30", "title": "Lip to speech synthesis with visual context attentional gan", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Minsu Kim; Joanna Hong; Yong Man Ro"}, {"ref_id": "b31", "title": "Distinguishing homophenes using multi-head visualaudio memory for lip reading", "journal": "", "year": "2022", "authors": "Minsu Kim; Yong Man Jeong Hun Yeo;  Ro"}, {"ref_id": "b32", "title": "Dlib-ml: A machine learning toolkit", "journal": "The Journal of Machine Learning Research", "year": "2009", "authors": "E Davis;  King"}, {"ref_id": "b33", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b34", "title": "Subword regularization: Improving neural network translation models with multiple subword candidates", "journal": "Long Papers", "year": "2018", "authors": "Taku Kudo"}, {"ref_id": "b35", "title": "Video prediction recalling long-term motion context via memory alignment learning", "journal": "", "year": "2021", "authors": "Sangmin Lee; Dae Hwi Hak Gu Kim; Hyung-Il Choi; Yong Man Kim;  Ro"}, {"ref_id": "b36", "title": "Unsupervised noise adaptive speech enhancement by discriminator-constrained optimal transport", "journal": "", "year": "2021", "authors": "Hsin-Yi Lin; Huan-Hsin Tseng; Xugang Lu; Yu Tsao"}, {"ref_id": "b37", "title": "Large-scale long-tailed recognition in an open world", "journal": "", "year": "2019", "authors": "Ziwei Liu; Zhongqi Miao; Xiaohang Zhan; Jiayun Wang; Boqing Gong; Stella X Yu"}, {"ref_id": "b38", "title": "Domain adaptation of lattice-free mmi based tdnn models for speech recognition", "journal": "International Journal of Speech Technology", "year": "2017", "authors": "Yanhua Long; Yijie Li; Hone Ye; Hongwei Mao"}, {"ref_id": "b39", "title": "Lira: Learning visual speech representations from audio through selfsupervision", "journal": "", "year": "2021", "authors": "Pingchuan Ma; Rodrigo Mira; Stavros Petridis; W Bj\u00f6rn; Maja Schuller;  Pantic"}, {"ref_id": "b40", "title": "End-to-end audio-visual speech recognition with conformers", "journal": "IEEE", "year": "2021", "authors": "Pingchuan Ma; Stavros Petridis; Maja Pantic"}, {"ref_id": "b41", "title": "Visual speech recognition for multiple languages in the wild", "journal": "", "year": "2022", "authors": "Pingchuan Ma; Stavros Petridis; Maja Pantic"}, {"ref_id": "b42", "title": "Classification and analysis of multivariate observations", "journal": "", "year": "1967", "authors": "J Macqueen"}, {"ref_id": "b43", "title": "Recurrent neural network transducer for audio-visual speech recognition", "journal": "IEEE", "year": "2019", "authors": "Takaki Makino; Hank Liao; Yannis Assael; Brendan Shillingford; Basilio Garcia; Otavio Braga; Olivier Siohan"}, {"ref_id": "b44", "title": "Hearing lips and seeing voices", "journal": "Nature", "year": "1976", "authors": "Harry Mcgurk; John Macdonald"}, {"ref_id": "b45", "title": "Crossmodal phase reset and evoked responses provide complementary mechanisms for the influence of visual speech in auditory cortex", "journal": "Journal of Neuroscience", "year": "2020", "authors": "Pierre M\u00e9gevand; R Manuel; David M Mercier; Elana Zion Groppe; Nima Golumbic;  Mesgarani; S Michael; Charles E Beauchamp; Ashesh D Schroeder;  Mehta"}, {"ref_id": "b46", "title": "Unsupervised adaptation with domain separation networks for robust speech recognition", "journal": "IEEE", "year": "2017", "authors": "Zhong Meng; Zhuo Chen; Vadim Mazalov; Jinyu Li; Yifan Gong"}, {"ref_id": "b47", "title": "Key-value memory networks for directly reading documents", "journal": "", "year": "2016", "authors": "Alexander Miller; Adam Fisch; Jesse Dodge;  Amir-Hossein; Antoine Karimi; Jason Bordes;  Weston"}, {"ref_id": "b48", "title": "Dynamic changes in superior temporal sulcus connectivity during perception of noisy audiovisual speech", "journal": "Journal of Neuroscience", "year": "2011", "authors": "R Audrey; Michael S Nath;  Beauchamp"}, {"ref_id": "b49", "title": "Leveraging unimodal self-supervised learning for multimodal audio-visual speech recognition", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Xichen Pan; Peiyu Chen; Yichen Gong; Helong Zhou; Xinbing Wang; Zhouhan Lin"}, {"ref_id": "b50", "title": "Synctalkface: Talking face generation with precise lip-syncing via audio-lip memory", "journal": "", "year": "2022", "authors": "Jin Se; Minsu Park; Joanna Kim; Jeongsoo Hong; Yong Man Choi;  Ro"}, {"ref_id": "b51", "title": "Audio-visual speech recognition with a hybrid ctc/attention architecture", "journal": "IEEE", "year": "2018", "authors": "Stavros Petridis; Themos Stafylakis; Pingchuan Ma"}, {"ref_id": "b52", "title": "Automatic Phoneme Recognition on TIMIT Dataset with Wav2Vec 2.0. If you use this model, please cite it using these metadata", "journal": "", "year": "", "authors": ""}, {"ref_id": "b53", "title": "Learning individual speaking styles for accurate lip to speech synthesis", "journal": "", "year": "2020", "authors": "Rudrabha Kr Prajwal;  Mukhopadhyay; P Vinay; C V Namboodiri;  Jawahar"}, {"ref_id": "b54", "title": "Lipsound: Neural mel-spectrogram reconstruction for lip reading", "journal": "", "year": "2019", "authors": "Leyuan Qu; Cornelius Weber; Stefan Wermter"}, {"ref_id": "b55", "title": "Lipsound2: Self-supervised pre-training for lip-tospeech reconstruction and lip reading", "journal": "", "year": "2021", "authors": "Leyuan Qu; Cornelius Weber; Stefan Wermter"}, {"ref_id": "b56", "title": "Rapid learning or feature reuse? towards understanding the effectiveness of maml", "journal": "", "year": "2019", "authors": "Aniruddh Raghu; Maithra Raghu"}, {"ref_id": "b57", "title": "Learning from the master: Distilling cross-modal advanced knowledge for lip reading", "journal": "", "year": "2021", "authors": "Sucheng Ren; Yong Du; Jianming Lv; Guoqiang Han; Shengfeng He"}, {"ref_id": "b58", "title": "The human voice", "journal": "Scientific American", "year": "1992", "authors": "T Robert;  Sataloff"}, {"ref_id": "b59", "title": "Learning audio-visual speech representation by masked multimodal cluster prediction", "journal": "", "year": "2022", "authors": "Bowen Shi; Wei-Ning Hsu; Kushal Lakhotia; Abdelrahman Mohamed"}, {"ref_id": "b60", "title": "Robust self-supervised audio-visual speech recognition", "journal": "", "year": "2022", "authors": "Bowen Shi; Wei-Ning Hsu; Abdelrahman Mohamed"}, {"ref_id": "b61", "title": "Musan: A music, speech, and noise corpus", "journal": "", "year": "2015", "authors": "David Snyder; Guoguo Chen; Daniel Povey"}, {"ref_id": "b62", "title": "Deep memory network for cross-modal retrieval", "journal": "IEEE Transactions on Multimedia", "year": "2018", "authors": "Ge Song; Dong Wang; Xiaoyang Tan"}, {"ref_id": "b63", "title": "Multimodal sparse transformer network for audio-visual speech recognition", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "year": "2022", "authors": "Qiya Song; Bin Sun; Shutao Li"}, {"ref_id": "b64", "title": "Visual contribution to speech intelligibility in noise. The journal of the acoustical society of america", "journal": "", "year": "1954", "authors": "H William; Irwin Sumby;  Pollack"}, {"ref_id": "b65", "title": "The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings", "journal": "Acoustical Society", "year": "2013", "authors": "Joachim Thiemann; Nobutaka Ito; Emmanuel Vincent"}, {"ref_id": "b66", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b67", "title": "Complex spectral mapping for single-and multi-channel speech enhancement and robust asr", "journal": "", "year": "2020", "authors": "Zhong-Qiu Wang; Peidong Wang; Deliang Wang"}, {"ref_id": "b68", "title": "speech, and language processing", "journal": "", "year": "", "authors": ""}, {"ref_id": "b69", "title": "Hybrid ctc/attention architecture for end-to-end speech recognition", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2017", "authors": "Shinji Watanabe; Takaaki Hori; Suyoun Kim; R John; Tomoki Hershey;  Hayashi"}, {"ref_id": "b70", "title": "", "journal": "", "year": "2014", "authors": "Jason Weston; Sumit Chopra; Antoine Bordes"}, {"ref_id": "b71", "title": "K-means clustering versus validation measures: a data distribution perspective", "journal": "", "year": "2006", "authors": "Hui Xiong; Junjie Wu; Jian Chen"}, {"ref_id": "b72", "title": "Discriminative multi-modality speech recognition", "journal": "", "year": "2020", "authors": "Bo Xu; Cheng Lu; Yandong Guo; Jacob Wang"}, {"ref_id": "b73", "title": "Audio-visual recognition of overlapped speech for the lrs2 dataset", "journal": "IEEE", "year": "2020", "authors": "Jianwei Yu;  Shi-Xiong; Jian Zhang; Shahram Wu; Bo Ghorbani; Shiyin Wu; Shansong Kang; Xunying Liu; Helen Liu; Dong Meng;  Yu"}, {"ref_id": "b74", "title": "On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training", "journal": "", "year": "2022", "authors": "Jisi Zhang; Catalin Zorila; Rama Doddipatla; Jon Barker"}, {"ref_id": "b75", "title": "Eleatt-rnn: Adding attentiveness to neurons in recurrent neural networks", "journal": "IEEE Transactions on Image Processing", "year": "2019", "authors": "Pengfei Zhang; Jianru Xue; Cuiling Lan; Wenjun Zeng; Zhanning Gao; Nanning Zheng"}, {"ref_id": "b76", "title": "Hearing lips: Improving lip reading by distilling speech recognizers", "journal": "", "year": "2020", "authors": "Ya Zhao; Rui Xu; Xinchao Wang; Peng Hou"}, {"ref_id": "b77", "title": "Aihua Zheng, and Ran He. 2021a. Arbitrary talking face generation via attentional audio-visual coherence learning", "journal": "", "year": "", "authors": "Hao Zhu; Huaibo Huang; Yi Li"}, {"ref_id": "b78", "title": "Ai-Hua Zheng, and Ran He. 2021b. Deep audio-visual learning: A survey", "journal": "International Journal of Automation and Computing", "year": "", "authors": "Hao Zhu; Man-Di Luo; Rui Wang"}, {"ref_id": "b79", "title": "C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 4 and Appendix B", "journal": "", "year": "", "authors": ""}, {"ref_id": "b80", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Not applicable", "journal": "", "year": "", "authors": ""}, {"ref_id": "b81", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Illustration of noisy audio-visual speech recognition. (a) Mainstream AVSR approaches with noise adaptation. (b) Our framework constructs visemephoneme mapping for modality transfer, which restores clean audio from visual signals to enable speech recognition under any noisy conditions.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Illustration of our proposed UniVPM. (a) Training on clean audio-visual data to construct universal viseme-phoneme mapping. (b) Inference on any noisy data with restored clean audio from modality transfer.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 33Figure 3: t-SNE visualization of clustered phonemes from (a) online clustering (with random pruning to keep fixed cluster size, details are in \u00a7C.3), and (b) our proposed online balanced clustering. We randomly select six clusters for visualization, and black triangle denotes the cluster center. Dashed ellipses highlight the real phoneme classes, which are confirmed by pre-trained phoneme recognition model (Phy, 2022).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Illustration of (a) viseme-phoneme mapping via AMIE, and (b) modality transfer via retrieval.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "pares the AVSR performance of our UniVPM with prior methods on LRS3 benchmark. Under clean training data, the proposed UniVPM (in purple shades) significantly outperforms AV-HuBERT baseline, and it achieves comparable performance to the AV-HuBERT trained on noisy data, where the restored clean audio plays the key role and implements our original motivation of unsupervised noise adaptation. Based on that, available noisy training data further improves the robustness 5 , where our best results achieve new state-of-", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "LGAN w/ I(s v , s a ) 15.4 14.6 7.2 5.3 1.32 27.8 L GAN w/ I(f v ,f a ) 17.7 22.0 8.8 5.6 1.36 29.2 L GAN w/ I(s v , s a ) + I(f v ,f a ) 13.2", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :5Figure 5: Left panel: t-SNE visualization of clustered viseme and phoneme centers (ellipses highlight the undesirably gathered centers). Right panel: confusion matrix of phoneme matching and viseme-phoneme mapping. In (g)-(i), the vertical axis indicates phoneme center IDs and the horizontal axis indicates real phonemes predicted by pre-trained model (Phy, 2022), while in (j)-(l) the horizontal axis indicates viseme center IDs.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure 6: Phoneme distributions in LRS3 and LRS2 datasets. Pre-trained phoneme recognition model (Phy, 2022) is used for statistics, where speech is recognized into 44 phonemes, with 39 of them visualized in figures and another 5 special phonemes eliminated (i.e., '|', '[UNK]', '[PAD]', '<s>', '</s>').", "figure_data": ""}, {"figure_label": "78", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 :Figure 8 :78Figure 7: Phoneme distributions without 'h#'.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 9 :9Figure 9: Illustration of noisy training pipeline of UniVPM. Both clean and noisy audio are used for training,where the clean audio is employed for phoneme clustering and the noisy audio is used to improve the system noise-robustness. Compared to Fig.2, there is an extra data stream of noisy audio to improve robustness.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ", which may fail to model the minority classes well. In this work, we propose to build two memory banks via online balanced clustering to model all the visemes and phonemes equally, i.e., universal.", "figure_data": "VisualVideo streamFront-endclusteringmappingrestored clean audioFusionRecognizer\"I am happy\"transferAudio stream (clean)Front-end AudioclusteringViseme BankPhoneme Bank(a) Training on clean dataVisualFront-endVideo streamrestoredclean audioFusionRecognizer\"I am happy\"transferAudioFront-endAudio stream (noisy)(b) Inference on any noisy data"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Algorithm 1 Online Balanced Clustering.Require: Streaming data D, number of clusters N , maximum cluster size Smax. 1: Initialize an empty memory bank B and a list of empty cluster banks {B1, B2, ..., BN }.", "figure_data": "2: while len(B) \u2264 N do 3: Receive new batch data d from D 4: Append all frame samples in d to bank B 5: end while 6: Initialize a list of cluster centers {c1, c2, ..., cN } from B using K-MEANS++ Algorithm (2006) 7: for batch data d \u2208 D do 8: Append all frame samples in d to bank B 9: {B1, ..., BN } = RE-ALLOCATE(B, {c1, ..., cN }) 10: {c1, ..., cN } = RENEW-CENTERS({B1, ..., BN }) 11: Calculate average cluster size Savg = len(B)/N 12: Threshold cluster size S thr = min(Savg, Smax) 13: for i = 1, 2, ..., N do 14: if len(Bi) > S thr then \u25b7 UNDERSAMPLING 15: Maintain the S thr -nearest samples to ci in Bi 16: Update B accordingly 17: else \u25b7 OVERSAMPLING 18:"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "WER (%) of proposed UniVPM and prior works on LRS3 benchmark. \"PT Type\" / \"FT Type\" denote pre-training / finetuning data type. \"SNR\" is signal-to-noise ratio. All the noisy data containsMUSAN (2015)  noise.", "figure_data": "MethodPT TypeFT Type-10Babble, SNR (dB) = -5 0 5 10 avg-10Speech, SNR (dB) = -5 0 5 10 avgMusic + Natural, SNR (dB) = -10 -5 0 5 10 avgClean \u221e"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "WER (%) of proposed UniVPM and prior works on LRS2 benchmark.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "WER (%) on unseen testing noises with LRS3 benchmark. Testing noises \"Meeting\", \"Cafe\", \"Resto\" and \"Station\" are from DEMAND dataset (2013). Pre-training noise are from MUSAN dataset.", "figure_data": "MethodFinetune Unlabeled Labeled Mode Data (hrs) Data (hrs)WER (%)TM-seq2seq (2018a)AV-1,51958.9EG-seq2seq (2020)AV-59057.8Hyb-AVSR (2021b)AV-59043.3RNN-T (2019)AV-31,00033.6Distill-PT (2022)V1,02143831.5u-HuBERT (2022)AV2,21143327.2AV-HuBERT (2022a)AV V1,759 1,759433 43334.7 28.6+ Self-TrainingV1,75943326.9UniVPM (ours)AV1,75943326.7"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "In this case,", "figure_data": "MethodBSMN Clean VSRAV-HuBERT (2022b)23.7 39.2 10.7 6.4 1.4234.7Effectiveness of Online Balanced ClusteringMemory Network (2022)20.6 29.5 9.2 6.1 1.3932.0Online Clustering19.3 22.9 8.7 5.9 1.3731.2Online Balanced Clustering13.2 8.26.1 5.1 1.3126.7Effectiveness of AMIENone22.3 35.4 10.4 6.0 1.3931.8Contrastive Learning21.5 29.2 9.7 5.8 1.3730.1MINE (2018)18.6 20.1 8.3 5.5 1.3428.8AMIE w/o Adv. Learning17.0 17.9 7.7 5.4 1.3328.2AMIE13.2 8.26.1 5.1 1.3126.7Analysis of Adversarial Learning"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ": Ablation study. 'B', 'S', 'M', 'N' denote average-SNR results on four MUSAN noises in Table 1. \"Adv.\" denotes \"Adversarial\". The four ablations are all based on full UniVPM and independent with each other.with restored clean audio from lip movements, theproposed UniVPM significantly outperforms AV-HuBERT baseline (34.7%\u219226.7%). Although thevisual-only training and self-training strategies im-prove AV-HuBERT's results, our UniVPM still de-fines new state-of-the-art on LRS3 benchmark."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "presents the ablation study of components in UniVPM. The four parts of ablation are independent, i.e., each study is conducted where other three components are kept same as full UniVPM.", "figure_data": "t-SNE VisualizationConfusion MatrixViseme CentersPhoneme CentersPhonemesViseme CentersOnline ClusteringPhoneme CentersPhoneme CentersOnline BalancedClusteringOnline BalancedClustering + AMIE"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": "further"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Evaluation of restored clean audio in terms of phoneme match accuracy on LRS3 test set. It is calculated with predicted phonemes for restored audio and real clean audio by pre-trained model(Phy, 2022).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "89.6 43.9 11.0 3.7 49.5 102.5 93.8 63.5 24.1 10.7 58.9 58.6 35.9 13.9 5.4 2.6 23.3 1.55 Noisy 98.2 65.6 17.0 5.3 2.7 37.8 94.3 73.8 46.3 22.9 9.7 49.4 43.4 18.0 6.5 3.2 2.1 14.6 1.50 Noisy Clean 98.3 77.6 23.0 7.3 2.9 41.8 87.3 62.9 41.0 22.2 8.9 44.5 43.4 19.3 7.1 3.4 2.5 15.1 1.62 Noisy 97.5 62.3 15.7 5.1 2.6 36.6 81.7 56.2 37.3 19.0 8.3 40.5 38.7 15.1 5.7 3.1 2.3 13.0 1.60", "figure_data": "PT Type Clean Clean Clean 99.3 AV FT Mode Type -10 A Clean 72.6 30.9 9.8 Babble, SNR (dB) = -5 0 5 10 avg 2.9 2.1 23.7 93.4 71.6 22.1 6.1 Speech, SNR (dB) = -10 -5 0 5 10 2.7 39.2 24.1 10.9 3.6 2.4 1.9 8.6 Music + Natural, SNR (dB) = avg -10 -5 0 5 10 avg Noisy 30.0 15.2 5.9 2.7 1.9 11.1 15.9 7.5 3.9 2.4 1.9 6.3 12.1 5.9 3.1 2.2 1.8 5.0 Clean 39.4 14.5 5.2 2.7 2.0 12.8 18.8 5.1 3.1 2.3 1.9 6.2 11.4 5.0 2.8 2.2 1.8 4.6 Noisy Noisy 28.4 13.4 5.0 2.6 1.9 10.3 11.4 4.6 2.9 2.2 1.8 4.6 9.7 4.7 2.5 1.9 1.8 4.1Clean \u221e 1.42 1.40 1.54 1.40"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": "presents the performance of AV-HuBERTto analyze the noise-robustness of AVSR system.First, as the original motivation of AVSR, the vi-sual modality significantly improves the audio-only speech recognition performance under var-ious noisy as well as clean testing conditions, espe-cially the low-SNR environments. However, mostexisting efforts still focus on audio modality toimprove robustness considering its dominance inAVSR task. The reason is the inherent informa-tion insufficiency of visual modality to representspeech content. Mainstream approaches introducenoise adaptation techniques to strengthen robust-ness, where most of them leverage noise-corrupteddata to improve network training"}], "formulas": [{"formula_id": "formula_0", "formula_text": "dnew = dnear \u2022 \u03b1 + ci \u2022 (1 \u2212 \u03b1) 21:", "formula_coordinates": [4.0, 70.86, 317.91, 175.16, 19.45]}, {"formula_id": "formula_1", "formula_text": "I(X, Y ) = x\u2208X y\u2208Y p(x, y) log p(x, y) p(x)p(y) ,(1)", "formula_coordinates": [4.0, 312.49, 610.01, 212.66, 30.82]}, {"formula_id": "formula_2", "formula_text": "I(X, Y ) = D KL (p(x, y) \u2225 p(x)p(y)),(2)", "formula_coordinates": [4.0, 320.8, 707.28, 204.35, 20.54]}, {"formula_id": "formula_3", "formula_text": "D KL (p \u2225 q) = x\u2208X p(x) log p(x) q(x) ,(3)", "formula_coordinates": [4.0, 334.94, 746.1, 190.21, 30.82]}, {"formula_id": "formula_4", "formula_text": "D KL (p \u2225 q) = sup T :\u2126\u2192R E p [T ] \u2212 log(E q [e T ]),(4)", "formula_coordinates": [5.0, 78.78, 121.53, 211.08, 32.46]}, {"formula_id": "formula_5", "formula_text": "I(X, Y ) \u2265 I \u0398 (X, Y ),(5)", "formula_coordinates": [5.0, 126.08, 204.84, 163.79, 20.54]}, {"formula_id": "formula_6", "formula_text": "I \u0398 (X, Y ) = sup \u03b8\u2208\u0398 E p(x,y) [T \u03b8 (x, y)] \u2212 log(E p(x)p(y) [e T \u03b8 (x,y) ]),(6)", "formula_coordinates": [5.0, 100.06, 250.19, 189.81, 46.05]}, {"formula_id": "formula_7", "formula_text": "I JS \u0398 (s v , s a ) = sup \u03b8\u2208\u0398 E p(sv,sa) [\u2212sp(\u2212T \u03b8 (s v , s a ))] \u2212E p(sv)p(sa) [sp(T \u03b8 (s v ,s a ))],(7)", "formula_coordinates": [5.0, 77.74, 601.46, 212.13, 50.69]}, {"formula_id": "formula_8", "formula_text": "L GAN = L D + L G = I JS \u0398 (f v , f a ) + [\u2212I JS \u0398 (s v , s a )],(8)", "formula_coordinates": [5.0, 322.79, 389.42, 202.36, 31.15]}, {"formula_id": "formula_9", "formula_text": "A i,j = exp(\u27e8f i v , c j a \u27e9/\u03c4 ) N k=1 exp(\u27e8f i v , c k a \u27e9/\u03c4 ) ,(9)", "formula_coordinates": [5.0, 341.5, 671.54, 183.65, 38.41]}, {"formula_id": "formula_10", "formula_text": "f i a = N j=1 A i,j \u2022 c j a ,(10)", "formula_coordinates": [5.0, 369.59, 741.17, 155.56, 34.7]}, {"formula_id": "formula_11", "formula_text": "- 4.1 - - - - - - - - - - - - - - - 1.2 AV-HuBERT (2022b)", "formula_coordinates": [6.0, 76.1, 125.38, 441.16, 31.78]}, {"formula_id": "formula_12", "formula_text": "TM-seq2seq (2018a) - Noisy - - - - - - - - - - - - - - - - - - 8.5 Hyb-RNN (2018) - Noisy - - - - - - - - - - - - - - - - - - 7.0 LF-MMI TDNN (2020) - Clean - - - - - - - - - - - - - - - - - - 5.9 Hyb-AVSR (2021b) - Noisy - - - - - - - - - - - - - - - - - - 3.7 MoCo+w2v2 (2022) - Noisy - - - - - - - - - - - - - - - - - - 2.6", "formula_coordinates": [6.0, 76.09, 279.87, 441.18, 40.48]}, {"formula_id": "formula_13", "formula_text": "LGAN = I JS \u0398 (fv, fa) + [\u2212I JS \u0398 (sv, sa) \u2212 I JS \u0398 (fv,fa)],(11)", "formula_coordinates": [6.0, 73.68, 486.3, 216.18, 25.33]}, {"formula_id": "formula_14", "formula_text": "L = LASR + \u03bbGAN \u2022 LGAN + \u03bbrec \u2022 Lrec + \u03bbvar \u2022 Lvar,(12)", "formula_coordinates": [6.0, 70.86, 606.12, 219.01, 24.32]}, {"formula_id": "formula_15", "formula_text": "(a) (b) (c) (d) (f) (g) (i) (j) (k) (l) (e) (h)", "formula_coordinates": [8.0, 191.86, 100.99, 268.65, 147.48]}], "doi": "10.57967/hf/0125"}