{"title": "Probing the Need for Visual Context in Multimodal Machine Translation", "authors": "Ozan Caglayan; Pranava Madhyastha; Lucia Specia; Lo\u00efc Barrault", "pub_date": "", "abstract": "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.", "sections": [{"heading": "Introduction", "text": "Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) , MMT has so far been studied using the Multi30K dataset , a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017;.\nThe three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016;Calixto et al., 2016;Libovick\u00fd and Helcl, 2017;Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017;Ma et al., 2017;Caglayan et al., 2017a;Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016;Gr\u00f6nroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr\u00f6nroos et al. (2018) consider their multimodal gains \"modest\" and attribute the largest gain to the usage of external parallel corpora.  observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation . In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images.\nThese empirical findings seem to indicate that images are ignored by the models and hint at the fact that this is due to representation or modeling limitations. We conjecture that the most plausible reason for the linguistic dominance is that -at least in Multi30K -the source text is sufficient to perform the translation, eventually preventing the visual information from intervening in the learning process. To investigate this hypothesis, we introduce several input degradation regimes (Section 2) and revisit state-of-the-art MMT models (Section 3) to assess their behavior under degraded regimes. We further probe the visual sensitivity by deliberately feeding features from unrelated images. Our results (Section 4) show that MMT models successfully exploit the visual modality when the linguistic context is scarce, but indeed tend to be less sensitive to this modality when exposed to complete sentences.", "publication_ref": ["b32", "b14", "b4", "b6", "b23", "b18", "b7", "b24", "b2", "b25", "b19", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Input Degradation", "text": "In this section we propose several degradations to the input language modality to simulate conditions where sentences may miss crucial information. We denote a set of translation pairs by D and indicate degraded variants with subscripts. Both the training and the test sets are degraded.\nColor Deprivation. We consistently replace source words that refer to colors with a special token [v] (D C in Table 1). Our hypothesis is that a monomodal system will have to rely on sourceside contextual information and biases, while a multimodal architecture could potentially capitalize on color information extracted by exploiting the image and thus obtain better performance. This affects 3.3% and 3.1% of the words in the training and the test set, respectively.\nEntity Masking. The Flickr30K dataset, from which Multi30K is derived, has also been extended with coreference chains to tag mentions of visually depictable entities in image descriptions (Plummer et al., 2015). We use these to mask out the head nouns in the source sentences (D N in Table 1). This affects 26.2% of the words in both the training and the test set. We hypothesize that a multimodal system should heavily rely on the images to infer the missing parts.\nProgressive Masking. A progressively degraded variant D k replaces all but the first k tokens of source sentences with [v] . Unlike the color deprivation and entity masking, masking out suffixes does not guarantee systematic removal of visual context, but rather simulates an increasingly low-resource scenario. Overall, we form 16 degraded variants D k (Table 1) where k \u2208 {0, 2, . . . , 30}. We stop at D 30 since 99.8% of the sentences in Multi30K are shorter than 30 words with an average sentence length of 12 words. D 0 -where the only remaining information is the source sentence length -is an interesting case from two perspectives: a neural machine translation (NMT) model trained on it resembles a target language model, while an MMT model becomes an image captioner with access to \"expected length information\".\nVisual Sensitivity. Inspired by Elliott (2018), we experiment with incongruent decoding in order to understand how sensitive the multimodal systems are to the visual modality. This is achieved D a lady in a blue dress singing \nD C a lady in a [v] dress singing D N a [v] in a blue [v] singing D 4 a lady in a [v] [v] [v] D 2 a lady [v] [v] [v] [v] [v] D 0 [v] [v] [v] [v] [v] [v] [v]", "publication_ref": ["b27", "b13"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0"]}, {"heading": "Experimental Setup", "text": "Dataset. We conduct experiments on the English\u2192French part of Multi30K. The models are trained on the concatenation of the train and val sets (30K sentences) whereas test2016 (dev) and test2017 (test) are used for early-stopping and model evaluation, respectively. For entity masking, we revert to the default Flickr30K splits and perform the model evaluation on test2016, since test2017 is not annotated for entities. We use word-level vocabularies of 9,951 English and 11,216 French words. We use Moses (Koehn et al., 2007) scripts to lowercase, normalize and tokenize the sentences with hyphen splitting. The hyphens are stitched back prior to evaluation.\nVisual Features. We use a ResNet-50 CNN (He et al., 2016) trained on ImageNet (Deng et al., 2009) as image encoder. Prior to feature extraction, we center and standardize the images using ImageNet statistics, resize the shortest edge to 256 pixels and take a center crop of size 256x256. We extract spatial features of size 2048x8x8 from the final convolutional layer and apply L 2 normalization along the depth dimension (Caglayan et al., 2018). For the non-attentive model, we use the 2048-dimensional global average pooled version (pool5) of the above convolutional features.\nModels. Our baseline NMT is an attentive model  with a 2-layer bidirectional GRU encoder  and a 2-layer conditional GRU decoder (Sennrich et al., 2017). The second layer of the decoder receives the output of the attention layer as input.  For the MMT model, we explore the basic multimodal attention (DIRECT) (Caglayan et al., 2016) and its hierarchical (HIER) extension (Libovick\u00fd and Helcl, 2017). The former linearly projects the concatenation of textual and visual context vectors to obtain the multimodal context vector, while the latter replaces the concatenation with another attention layer. Finally, we also experiment with encoder-decoder initialization (INIT) (Calixto and Liu, 2017;Caglayan et al., 2017a) where we initialize both the encoder and the decoder using a non-linear transformation of the pool5 features.\nHyperparameters. The encoder and decoder GRUs have 400 hidden units and are initialized with 0 except the multimodal INIT system. All embeddings are 200-dimensional and the decoder embeddings are tied (Press and Wolf, 2016). A dropout of 0.4 and 0.5 is applied on source embeddings and encoder/decoder outputs, respectively (Srivastava et al., 2014). The weights are decayed with a factor of 1e\u22125. We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e\u22124 and mini-batches of 64 samples. The gradients are clipped if the total norm exceeds 1 (Pascanu et al., 2013). The training is early-stopped if dev set ME-TEOR (Denkowski and Lavie, 2014) does not improve for ten epochs. All experiments are conducted with nmtpytorch 1 (Caglayan et al., 2017b).", "publication_ref": ["b21", "b17", "b11", "b3", "b29", "b4", "b23", "b7", "b2", "b28", "b31", "b20", "b26", "b12", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We train all systems three times each with different random initialization in order to perform significance testing with multeval (Clark et al., 2011). Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics. We decode the translations with a beam size of 12.\n1 github.com/lium-lst/nmtpytorch Figure 1: Entity masking: all masked MMT models are significantly better than the masked NMT (dashed). Incongruent decoding severely worsens all systems. The vanilla NMT baseline is 75.9 2 . We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D (Table 2). The first column indicates that, although MMT models perform slightly better on average, they are not significantly better than the baseline NMT. We now introduce and discuss the results obtained under the proposed degradation schemes. Please refer to Table 5 and the appendix for qualitative examples.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": ["tab_2", "tab_3"]}, {"heading": "Color Deprivation", "text": "Unlike the inconclusive results for D, we observe that all MMT models are significantly better than NMT when color deprivation is applied (D C in Table 2). If we further focus on the subset of the test set subjected to color deprivation (247 sentences), the gain increases to 1.6 METEOR for HIER. For the latter subset, we also computed the average color accuracy per sentence and found that the attentive models are 12% better than the NMT (32.5\u219244.5) whereas the INIT model only brings 4% (32.5\u219236.5) improvement. This shows that more complex MMT models are better at integrating visual information to perform better.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Entity Masking", "text": "The gains are much more prominent with entity masking, where the degradation occurs at a larger scale: Attentive MMT models show up to 4.2 ME-TEOR improvement over NMT (Figure 1). We observed a large performance drop with incongruent decoding, suggesting that the visual modality is Czech +1.4 (\u2193 2.9) +1.7 (\u2193 3.5) +1.7 (\u2193 4.1) German +2.1 (\u2193 4.7) +2.5 (\u2193 5.9) +2.7 (\u2193 6.5) French +3.4 (\u2193 6.5) +3.9 (\u2193 9.0) +4.2 (\u2193 9.7) Table 3: Entity masking results across three languages: all MMT models perform significantly better than their NMT counterparts (p-value \u2264 0.01). The incongruence drop applies on top of the MMT score. now much more important than previously demonstrated (Elliott, 2018). A comparison of attention maps produced by the baseline and masked MMT models reveals that the attention weights are more consistent in the latter. An interesting example is given in Figure 2 where the masked MMT model attends to the correct region of the image and successfully translates a dropped word that was otherwise a spelling mistake (\"son\"\u2192\"song\").\nCzech and German. In order to understand whether the above observations are also consistent across different languages, we extend the entity masking experiments to German and Czech parts of Multi30K. Table 3 shows the gain of each MMT system with respect to the NMT model and the subsequent drop caused by incongruent decoding 3 . First, we see that the multimodal benefits clearly hold for German and Czech, although the gains are lower than for French 4 . Second, when we compute the average drop from using incongruent images across all languages, we see how conservative the INIT system is (\u2193 4.7) compared  Incongruent Dec. \u2193 6.4 \u2193 5.5 \u2193 1.4 \u2193 0.7 \u2193 0.7\nBlinding \u2193 3.9 \u2193 2.9 \u2193 0.4 \u2193 0.5 \u2193 0.3 NMT \u2193 3.7 \u2193 2.6 \u2193 0.6 \u2193 0.2 \u2193 0.3\nTable 4: The impact of incongruent decoding for progressive masking: all METEOR differences are against the DIRECT model. The blinded systems are both trained and decoded using incongruent features.\nto HIER (\u2193 6.1) and DIRECT (\u2193 6.8). This raises a follow-up question as to whether the hidden state initialization eventually loses its impact throughout the recurrence where, as a consequence, the only modality processed is the text.", "publication_ref": ["b13"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Progressive Masking", "text": "Finally, we discuss the results of the progressive masking experiments for French. Figure 3 clearly shows that as the sentences are progressively degraded, all MMT systems are able to leverage the visual modality. When the multimodal task becomes image captioning at k=0, MMT models improve over the language-model counterpart by \u223c7 METEOR. Further qualitative examples show that the systems perform surprisingly well by producing visually plausible sentences (see Table 5 and the Appendix).\nTo get a sense of the visual sensitivity, we pick the DIRECT models trained on four degraded variants and perform incongruent decoding. We notice that as the amount of linguistic information increases, the gap narrows down: the MMT system gradually becomes less perplexed by the incongruence or, put in other words, less sensitive to the visual modality (Table 4).\nSRC: an older woman in [v][v][v][v][v][v][v][v][v][v][v] NMT: une femme\u00e2g\u00e9e avec un t-shirt blanc et des lunettes de soleil est assise sur un banc (an older woman with a white t-shirt and sunglasses is sitting on a bank) MMT: une femme\u00e2g\u00e9e en maillot de bain rose est assise sur un rocher au bord de l'eau (an older woman with a pink swimsuit is sitting on a rock at the seaside) REF: une femme\u00e2g\u00e9e en bikini bronze sur un rocher au bord de l'oc\u00e9an (an older woman in bikini is tanning on a rock at the edge of the ocean)\nSRC: a young [v] in [v] holding a tennis [v]\nNMT: un jeune gar\u00e7on en bleu tenant une raquette de tennis (a young boy in blue holding a tennis racket) MMT: une jeune femme en blanc tenant une raquette de tennis REF: une jeune femme en blanc tenant une raquette de tennis (a young girl in white holding a tennis racket)  We then conduct a contrastive \"blinding\" experiment where the DIRECT models are not only fed with incongruent features at decoding time but also trained with them from scratch. The results suggest that the blinded models learn to ignore the visual modality. In fact, their performance is equivalent to NMT models.\nSRC", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_3"]}, {"heading": "Discussion and Conclusions", "text": "We presented an in-depth study on the potential contribution of images for multimodal machine translation. Specifically, we analysed the behavior of state-of-the-art MMT models under several degradation schemes in the Multi30K dataset, in order to reveal and understand the impact of textual predominance. Our results show that the models explored are able to integrate the visual modality if the available modalities are complementary rather than redundant. In the latter case, the primary modality (text) sufficient to accomplish the task. This dominance effect corroborates the seminal work of Colavita (1974) in Psychophysics where it has been demonstrated that visual stimuli dominate over the auditory stimuli when humans are asked to perform a simple audiovisual discrimination task. Our investigation using source degradation also suggests that visual grounding can increase the robustness of machine translation systems by mitigating input noise such as errors in the source text. In the future, we would like to devise models that can learn when and how to integrate multiple modalities by taking care of the complementary and redundant aspects of them in an intelligent way.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "A Qualitative Examples", "text": "In this appendix, we provide further translation examples for color deprivation (Table 6), entity masking (Table 7) and progressive masking (Table 8). Specifically for the entity masking experiments, we also give further examples to showcase the behavior of the visual attention in Figure 4 and Figure 5. (b) Entity-masked MMT Figure 5: Attention example from entity masking experiments where terrier, grass and fence are dropped from the source sentence: (a) Baseline MMT is not able to shift attention from the salient dog to the grass and fence, (b) the attention produced by the masked MMT first shifts to the background area while translating \"on lush green [v]\" then focuses on the fence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work is a follow-up on the research efforts conducted within the \"Grounded sequenceto-sequence transduction\" team of the JSALT 2018 Workshop. We would like to thank Jind\u0159ich Libovick\u00fd for contributing the hierarchical attention to nmtpytorch during the workshop. We also thank the reviewers for their valuable comments.\nOzan Caglayan and Lo\u00efc Barrault received funding from the French National Research Agency (ANR) through the CHIST-ERA M2CR project under the contract ANR-15-CHR2-0006-01. Lucia Specia and Pranava Madhyastha received funding from the MultiMT (H2020 ERC Starting Grant No. 678017) and MMVC (Newton Fund Institutional Links Grant, ID 352343575) projects.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SRC:", "text": "a girl in [v] is sitting on a bench NMT:\npink  \nNMT: un gar\u00e7on en t-shirt rouge joue dans la neige (a boy in a red t-shirt plays in the snow) MMT: un gar\u00e7on en maillot de bain rouge joue dans l'eau REF: un gar\u00e7on en maillot de bain rouge joue dans l'eau (a boy in a red swimsuit plays in the water)\nNMT: un homme boit du vin dehors sur le trottoir (a man drinks wine outside on the sidewalk) MMT: un chien boit de l'eau dehors sur l'herbe REF: un chien boit de l'eau dehors sur l'herbe (a dog drinks water outside on the grass) (a girl jumping rope on a sidewalk near a parking lot) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SRC: a child [v][v][v][v][v][v]", "text": "NMT: un enfant avec des lunettes de soleil en train de jouer au tennis (a child with sunglasses playing tennis) MMT: un enfant est debout dans un champ de fleurs (a child is standing in field of flowers) REF: un enfant dans un champ de tulipes (a child in a field of tulips)\nSRC: a jockey riding his [v][v] NMT: un jockey sur son v\u00e9lo (a jockey on his bike) MMT: un jockey sur son cheval REF: un jockey sur son cheval (a jockey on his horse)\nNMT: des filles jouent\u00e0 un jeu de cartes (girls are playing a card game) MMT: des filles jouent un match de football REF: des filles jouent un match de football (girls are playing a football match) \nNMT: des filles en t-shirts violets sont assises sur des chaises dans une salle de classe (girls in purple t-shirts are sitting on chairs in a classroom) MMT: des filles en costumes violets dansent dans une rue en ville (girls in purple costumes dance on a city street) REF: des filles agitent des drapeaux violets tandis qu'elles d\u00e9filent dans la rue (girls wave purple flags as they parade down the street) ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "journal": "Computing Research Repository", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b1", "title": "Findings of the third shared task on multimodal machine translation", "journal": "", "year": "2018", "authors": "Lo\u00efc Barrault; Fethi Bougares; Lucia Specia; Chiraag Lala; Desmond Elliott; Stella Frank"}, {"ref_id": "b2", "title": "LIUM-CVC submissions for WMT17 multimodal translation task", "journal": "", "year": "2017", "authors": "Ozan Caglayan; Walid Aransa; Adrien Bardet; Mercedes Garc\u00eda-Mart\u00ednez; Fethi Bougares; Lo\u00efc Barrault; Marc Masana; Luis Herranz; Joost Van De Weijer"}, {"ref_id": "b3", "title": "LIUM-CVC submissions for WMT18 multimodal translation task", "journal": "", "year": "2018", "authors": "Ozan Caglayan; Adrien Bardet; Fethi Bougares; Lo\u00efc Barrault; Kai Wang; Marc Masana; Luis Herranz; Joost Van De Weijer"}, {"ref_id": "b4", "title": "Multimodal attention for neural machine translation", "journal": "Computing Research Repository", "year": "2016", "authors": "Ozan Caglayan; Lo\u00efc Barrault; Fethi Bougares"}, {"ref_id": "b5", "title": "NMTPY: A flexible toolkit for advanced neural machine translation systems", "journal": "Prague Bull. Math. Linguistics", "year": "2017", "authors": "Ozan Caglayan; Mercedes Garc\u00eda-Mart\u00ednez; Adrien Bardet; Walid Aransa; Fethi Bougares; Lo\u00efc Barrault"}, {"ref_id": "b6", "title": "DCU-UvA multimodal MT system report", "journal": "", "year": "2016", "authors": "Iacer Calixto; Desmond Elliott; Stella Frank"}, {"ref_id": "b7", "title": "Incorporating global visual features into attention-based neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Iacer Calixto; Qun Liu"}, {"ref_id": "b8", "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"ref_id": "b9", "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "journal": "", "year": "2011", "authors": "Jonathan H Clark; Chris Dyer; Alon Lavie; Noah A Smith"}, {"ref_id": "b10", "title": "Human sensory dominance", "journal": "Perception & Psychophysics", "year": "1974", "authors": "Francis B Colavita"}, {"ref_id": "b11", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "J Deng; W Dong; R Socher; L Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b12", "title": "Meteor universal: Language specific translation evaluation for any target language", "journal": "", "year": "2014", "authors": "Michael Denkowski; Alon Lavie"}, {"ref_id": "b13", "title": "Adversarial evaluation of multimodal machine translation", "journal": "", "year": "2018", "authors": "Desmond Elliott"}, {"ref_id": "b14", "title": "Findings of the second shared task on multimodal machine translation and multilingual image description", "journal": "", "year": "2017", "authors": "Desmond Elliott; Stella Frank; Lo\u00efc Barrault; Fethi Bougares; Lucia Specia"}, {"ref_id": "b15", "title": "Multi30K: Multilingual englishgerman image descriptions", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Desmond Elliott; Stella Frank; Khalil Sima'an; Lucia Specia"}, {"ref_id": "b16", "title": "The MeMAD submission to the WMT18 multimodal translation task", "journal": "", "year": "2018", "authors": "Stig-Arne Gr\u00f6nroos; Benoit Huet; Mikko Kurimo; Jorma Laaksonen; Bernard Merialdo; Phu Pham; Mats Sj\u00f6berg; Umut Sulubacak; J\u00f6rg Tiedemann; Raphael Troncy; Ra\u00fal V\u00e1zquez"}, {"ref_id": "b17", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b18", "title": "CUNI system for the WMT18 multimodal translation task", "journal": "", "year": "2018", "authors": "Jind\u0159ich Helcl; Jind\u0159ich Libovick\u00fd; Dusan Varis"}, {"ref_id": "b19", "title": "Attention-based multimodal neural machine translation", "journal": "", "year": "2016", "authors": "Po-Yao Huang; Frederick Liu; Sz-Rung Shiang; Jean Oh; Chris Dyer"}, {"ref_id": "b20", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "Diederik Kingma; Jimmy Ba"}, {"ref_id": "b21", "title": "Moses: Open source toolkit for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ond\u0159ej Bojar; Alexandra Constantin; Evan Herbst"}, {"ref_id": "b22", "title": "Sheffield submissions for WMT18 multimodal translation shared task", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Chiraag Lala; Pranava Swaroop Madhyastha; Carolina Scarton; Lucia Specia"}, {"ref_id": "b23", "title": "Attention strategies for multi-source sequence-to-sequence learning", "journal": "Vancouver", "year": "2017", "authors": "Jind\u0159ich Libovick\u00fd; Jind\u0159ich Helcl"}, {"ref_id": "b24", "title": "OSU multimodal machine translation system report", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Mingbo Ma; Dapeng Li; Kai Zhao; Liang Huang"}, {"ref_id": "b25", "title": "Sheffield MultiMT: Using object posterior predictions for multimodal machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Josiah Pranava Swaroop Madhyastha; Lucia Wang;  Specia"}, {"ref_id": "b26", "title": "On the difficulty of training recurrent neural networks", "journal": "", "year": "2013", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio"}, {"ref_id": "b27", "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "journal": "", "year": "2015", "authors": "B A Plummer; L Wang; C M Cervantes; J C Caicedo; J Hockenmaier; S Lazebnik"}, {"ref_id": "b28", "title": "Using the output embedding to improve language models", "journal": "Computing Research Repository", "year": "2016", "authors": "Ofir Press; Lior Wolf"}, {"ref_id": "b29", "title": "Nematus: a toolkit for neural machine translation", "journal": "", "year": "2017", "authors": "Rico Sennrich; Orhan Firat; Kyunghyun Cho; Alexandra Birch; Barry Haddow; Julian Hitschler; Marcin Junczys-Dowmunt; Samuel L\u00e4ubli; Antonio Valerio Miceli; Jozef Barone; Maria Mokry;  Nadejde"}, {"ref_id": "b30", "title": "A shared task on multimodal machine translation and crosslingual image description", "journal": "", "year": "2016", "authors": "Lucia Specia; Stella Frank; Khalil Sima'an; Desmond Elliott"}, {"ref_id": "b31", "title": "Dropout: A simple way to prevent neural networks from overfitting", "journal": "J. Mach. Learn. Res", "year": "2014", "authors": "Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b32", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "journal": "", "year": "2014", "authors": "Peter Young; Alice Lai; Micah Hodosh; Julia Hockenmaier"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Baseline MMT (top) translates the misspelled \"son\" while the masked MMT (bottom) correctly produces \"enfant\" (child) by focusing on the image.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Multimodal gain in absolute METEOR for progressive masking: the dashed gray curve indicates the percentage of non-masked words in the training set.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": ": little girl covering her face with a [v] towel NMT: une petite fille couvrant son visage avec une serviette blanche (a little girl covering her face with a white towel) MMT: une petite fille couvrant son visage avec une serviette bleue REF: une petite fille couvrant son visage avec une serviette bleue (a little girl covering her face with a blue towel)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "An example of the proposed input degradation schemes: D is the original sentence.by explicitly violating the test-time semantic congruence across modalities. Specifically, we feed the visual features in reverse sample order to break image-sentence alignments. Consequently, a model capable of integrating the visual modality would likely deteriorate in terms of metrics.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Baseline and color-deprivation METEOR scores: bold systems are significantly different from the NMT system within the same column (p-value \u2264 0.03).", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Qualitative examples from progressive masking, entity masking and color deprivation, respectively. Underlined and bold words highlight the bad and good lexical choices. MMT is an attentive system.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Attention example from entity masking experiments: (a) Baseline MMT translates the misspelled \"son\" (song \u2192 chanson) while (b) the masked MMT achieves a correct translation ([v]\u2192 enfant) by exploiting the visual modality.", "figure_data": "a bostonunterrierdebostonterriera is running mother and on lushcourtune surm\u00e8re l&apos;et herbesa verteher greenyoung grasssong enjoying fence day white beautiful a a in front ofjeune luxuriante devant chansonprofitant uned&apos; cl\u00f4tureune blancheoutside .belle .journ\u00e9e <eos>dehors.<eos><eos>.une une terrier m\u00e8re m\u00e8re de et et boston un berger allemand court son sa court sur Figure 4: un <eos> a [v] and her young [v] enjoying a beautiful [v] outside . <eos> a boston [v] is running on lush green [v] in front white [v] . <eos> of ajeune jeune sur l&apos;chanson enfant l&apos; herbe profitant profitant verte d&apos; d&apos; luxuriante une une devant belle belle une herbe verte devant une cl\u00f4ture blanche journ\u00e9e journ\u00e9e cl\u00f4ture .dehors dehors blanche <eos>. (a) Baseline (non-masked) MMT <eos> . <eos> une jeune enfant belle journ\u00e9e (b) Entity-masked MMT . <eos> (a) Baseline (non-masked) MMT un sur l&apos; une cl\u00f4turem\u00e8re profitant dehors berger herbe blancheet d&apos; . allemand verte .son une <eos> court devant <eos>"}], "formulas": [{"formula_id": "formula_0", "formula_text": "D C a lady in a [v] dress singing D N a [v] in a blue [v] singing D 4 a lady in a [v] [v] [v] D 2 a lady [v] [v] [v] [v] [v] D 0 [v] [v] [v] [v] [v] [v] [v]", "formula_coordinates": [2.0, 308.36, 84.01, 215.58, 59.66]}, {"formula_id": "formula_1", "formula_text": "Blinding \u2193 3.9 \u2193 2.9 \u2193 0.4 \u2193 0.5 \u2193 0.3 NMT \u2193 3.7 \u2193 2.6 \u2193 0.6 \u2193 0.2 \u2193 0.3", "formula_coordinates": [4.0, 307.28, 293.02, 218.28, 21.69]}, {"formula_id": "formula_2", "formula_text": "SRC: a young [v] in [v] holding a tennis [v]", "formula_coordinates": [5.0, 162.8, 158.22, 190.3, 8.46]}, {"formula_id": "formula_3", "formula_text": "SRC", "formula_coordinates": [5.0, 162.8, 237.21, 17.57, 6.87]}], "doi": "10.1515/pralin-2017-0035"}