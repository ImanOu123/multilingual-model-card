{"title": "Sampling-Based Robust Control of Autonomous Systems with Non-Gaussian Noise *", "authors": "Thom S Badings; Alessandro Abate; Nils Jansen; David Parker; Hasan A Poonawala; Marielle Stoelinga", "pub_date": "", "abstract": "Controllers for autonomous systems that operate in safetycritical settings must account for stochastic disturbances. Such disturbances are often modeled as process noise, and common assumptions are that the underlying distributions are known and/or Gaussian. In practice, however, these assumptions may be unrealistic and can lead to poor approximations of the true noise distribution. We present a novel planning method that does not rely on any explicit representation of the noise distributions. In particular, we address the problem of computing a controller that provides probabilistic guarantees on safely reaching a target. First, we abstract the continuous system into a discrete-state model that captures noise by probabilistic transitions between states. As a key contribution, we adapt tools from the scenario approach to compute probably approximately correct (PAC) bounds on these transition probabilities, based on a finite number of samples of the noise. We capture these bounds in the transition probability intervals of a socalled interval Markov decision process (iMDP). This iMDP is robust against uncertainty in the transition probabilities, and the tightness of the probability intervals can be controlled through the number of samples. We use state-of-the-art verification techniques to provide guarantees on the iMDP, and compute a controller for which these guarantees carry over to the autonomous system. Realistic benchmarks show the practical applicability of our method, even when the iMDP has millions of states or transitions.", "sections": [{"heading": "Introduction", "text": "Consider a so-called reach-avoid problem for an unmanned aerial vehicle (UAV), where the goal is to reach a desirable region within a given time horizon, while avoiding certain unsafe regions (Baier and Katoen 2008;Clarke, Emerson, and Sistla 1986). A natural formal model for such an autonomous system is a dynamical system. The state of the system reflects the position and velocity of the UAV, and the control inputs reflect choices that may change the state over time (Kulakowski, Gardner, and Shearer 2007). The dynamical system is linear if the state transition is linear in the current state and control input. Our problem is to compute a controller, such that the state of the UAV progresses safely, without entering unsafe regions, to its goal (\u00c5str\u00f6m and Murray 2010).\nHowever, factors like turbulence and wind gusts cause uncertainty in the outcome of control inputs (Blackmore et al. 2010). We model such uncertainty as process noise, which is an additive random variable (with possibly infinitely many outcomes) in the dynamical system that affects the transition of the state. Controllers for autonomous systems that operate in safety-critical settings must account for such uncertainty.\nA common assumption to achieve computational tractability of the problem is that the process noise follows a Gaussian distribution (Park, Serpedin, and Qaraqe 2013), for example in linear-quadratic-Gaussian control (Anderson and Moore 2007). However, in realistic problems, such as the UAV operating under turbulence, this assumption yields a poor approximation of the uncertainty (Blackmore et al. 2010). Distributions may even be unknown, meaning that one cannot derive a set-bounded or stochastic representation of the noise. In this case, it is generally hard or even impossible to derive hard guarantees on the probability that a given controller ensures a safe progression of the system's state to the objective.\nIn this work, we do not require that the process noise is known. Specifically, we provide probably approximately correct (PAC) guarantees on the performance of a controller for the reach-avoid problem, where the distribution of the noise is unknown. As such, we solve the following problem: Given a linear dynamical system perturbed by additive noise of unknown distribution, compute a controller under which, with high confidence, the probability to satisfy a reach-avoid problem is above a given threshold value.\nFinite-state abstraction. The fundamental concept of our approach is to compute a finite-state abstraction of the dynamical system. We obtain such an abstract model from a partition of the continuous state space into a set of disjoint convex regions. Actions in this abstraction correspond to control inputs that induce transitions between these regions. Due to the process noise, the outcome of an action is stochastic, and every transition has a certain probability.\nProbability intervals. Since the distribution of the noise is unknown, it is not possible to compute the transition probabilities exactly. Instead, we estimate the probabilities based arXiv:2110.12662v3 [eess.SY] 13 Dec 2021 on a finite number of samples (also called scenarios) of the noise, which may be obtained from a high fidelity (blackbox) simulator or from experiments. To be robust against estimation errors in these probabilities, we adapt tools from the scenario approach (also called scenario optimization), which is a methodology to deal with stochastic convex optimization in a data-driven fashion (Campi and Garatti 2008;Garatti and Campi 2019). We compute upper and lower bounds on the transition probabilities with a desired confidence level, which we choose up front. These bounds are PAC, as they contain the true probabilities with at least this confidence level.\nInterval MDPs. We formalize our abstractions with the PAC probability bounds using so-called interval Markov decision processes (iMDPs). While regular MDPs require precise transition probabilities, iMDPs exhibit probability intervals (Givan, Leach, and Dean 2000). Policies for iMDPs have to robustly account for all possible probabilities within the intervals, and one usually provides upper and lower bounds on maximal or minimal reachability probabilities or expected rewards (Hahn et al. 2017;Puggelli et al. 2013;Wolff, Topcu, and Murray 2012). For MDPs with precise probabilities, mature tool support exists, for instance, via PRISM (Kwiatkowska, Norman, and Parker 2011). In this work, we extend the support of PRISM to iMDPs.\nIterative abstraction scheme. The tightness of the probability intervals depends on the number of noise samples. Hence, we propose an iterative abstraction scheme, to iteratively improve these intervals by using increasing sample sizes. For the resulting iMDP, we compute a robust policy that maximizes the probability to safely reach the goal states. Based on a pre-defined threshold, we decide whether this probability is unsatisfactory or satisfactory. In the former case, we collect additional samples to reduce the uncertainty in the probability intervals. If the probability is satisfactory, we use the policy to compute a controller for the dynamical system. The specified confidence level reflects the likelihood that the optimal reachability probability on the iMDP is a lower bound for the probability that the dynamical system satisfies the reach-avoid problem under this derived controller.\nContributions. Our contributions are threefold: (1) We propose a novel method to compute safe controllers for dynamical systems with unknown noise distributions. Specifically, the probability of safely reaching a target is guaranteed, with high confidence, to exceed a pre-defined threshold.\n(2) We propose a scalable refinement scheme that incrementally improves the iMDP abstraction by iteratively increasing the number of samples. (3) We apply our method to multiple realistic control problems, and benchmark against two other tools: StocHy and SReachTools. We demonstrate that the guarantees obtained for the iMDP abstraction carry over to the dynamical system of interest. Moreover, we show that using probability intervals instead of point estimates of probabilities yields significantly more robust results.", "publication_ref": ["b6", "b22", "b38", "b4", "b43", "b2", "b16", "b28", "b30", "b32", "b44", "b57", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Reachability analysis. Verification and controller synthesis for reachability in stochastic systems is an active field of research in safety-critical engineering (Abate et al. 2008;Lavaei et al. 2021). Most approaches are based on formal abstractions (Alur et al. 2000;Lahijanian, Andersson, and Belta 2015;Soudjani and Abate 2013) or work in the continuous domain directly, e.g., using Hamilton-Jacobi reachability analysis (Bansal et al. 2017;Herbert et al. 2017) or optimization (Rosolia, Singletary, and Ames 2020). Several tools exist, such as StocHy (Cauchi and Abate 2019), ProbReach (Shmarov and Zuliani 2015) and SReach-Tools (Vinod, Gleason, and Oishi 2019). However, the majority of these methods require full knowledge of the models.\nWe break away from this literature in putting forward abstractions that do not require any knowledge of the noise distribution, via the scenario approach. It has been used for the verification of Markov decision processes (MDPs) with uncertain parameters (Cubuktepe et al. 2020), albeit only for finite-state systems. SReachTools also exhibits a samplingbased method, but relies on Hoeffding's inequality to obtain confidence guarantees (Sartipizadeh et al. 2019), so the noise is still assumed to be sub-Gaussian (Boucheron, Lugosi, and Massart 2013). By contrast, the scenario approach is completely distribution-free (Campi and Garatti 2018). Moreover, SReachTools is limited to problems with convex safe sets (a restrictive assumption in many problems) and its samplingbased methods can only synthesize open-loop controllers. Further related are sampling-based feedback motion planning algorithms, such as LQR-Trees. However, sampling in LQR-Trees relates to random exploration of the state space, and not to stochastic noise affecting the dynamics as in our setting (Reist, Preiswerk, and Tedrake 2016;Tedrake 2009). Alternatives to the scenario approach. Monte Carlo methods (e.g. particle methods) can also solve stochastic reach-avoid problems (Blackmore et al. 2010;Lesser, Oishi, and Erwin 2013). These methods simulate the system via many samples of the uncertain variable (Smith 2013). Monte Carlo methods approximate stochastic problems, while our approach provides bounds with a desired confidence level.\nIn distributionally robust optimization (DRO), decisions are robust with respect to ambiguity sets of distributions (Esfahani and Kuhn 2018;Goh and Sim 2010;Wiesemann, Kuhn, and Sim 2014). While the scenario approach uses samples of the uncertain variable, DRO works on the domain of uncertainty directly, thus involving potentially complex ambiguity sets (Garatti and Campi 2019). Designing robust policies for iMDPs with known uncertainty sets was studied by Puggelli et al. (2013), andWolff, Topcu, andMurray (2012). Hybrid methods between the scenario approach and robust optimization also exist (Margellos, Goulart, and Lygeros 2014).\nPAC literature. The term PAC refers to obtaining, with high probability, a hypothesis that is a good approximation of some unknown phenomenon (Haussler 1990). PAC learning methods for discrete-state MDPs are developed in Brafman and Tennenholtz (2002), Fu and Topcu (2014), and Kearns and Singh (2002), and PAC statistical model checking for MDPs in Ashok, Kret\u00ednsk\u00fd, and Weininger (2019).\nSafe learning methods. We only briefly discuss the emerging field of safe learning (Brunke et al. 2021;Garc\u00eda and Fern\u00e1ndez 2015). Recent works use Gaussian processes for learning-based model predictive control (Hewing, Kabzan, and Zeilinger 2020;Koller et al. 2018) or reinforcement learning with safe exploration (Berkenkamp et al. 2017), and control barrier functions to reduce model uncertainty (Taylor et al. 2020). Safe learning control concerns learning unknown, deterministic system dynamics, while imposing strong assumptions on stochasticity (Fisac et al. 2019). By contrast, our problem setting is fundamentally different: we reason about stochastic noise of a completely unknown distribution.", "publication_ref": ["b0", "b41", "b1", "b40", "b52", "b7", "b34", "b48", "b20", "b50", "b55", "b23", "b49", "b12", "b17", "b46", "b54", "b41", "b51", "b25", "b31", "b56", "b28", "b44", "b57", "b57", "b42", "b33", "b14", "b27", "b36", "b3", "b15", "b29", "b35", "b37", "b8", "b53", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Foundations and Outline", "text": "A discrete probability distribution over a finite set X is a function prob : X \u2192 [0, 1] with x\u2208X prob(x) = 1. The set of all distributions over X is Dist(X ), and the cardinality of a set X is |X|. A probability density function over a random variable x conditioned on y is written as p(x|y). All vectors x \u2208 R n , n \u2208 N, are column vectors and denoted by bold letters. We use the term controller when referring to dynamical systems, while we use policy for (i)MDPs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linear dynamical systems", "text": "We consider discrete-time, continuous-state systems, where the progression of the n-dimensional state x \u2208 R n depends linearly on the current state, a control input, and a process noise term. Given a state x k at discrete time step k \u2208 N, the successor state at time k + 1 is computed as\nx k+1 = Ax k + Bu k + q k + w k ,(1)\nwhere u k \u2208 U \u2282 R p is the control input at time k, A \u2208 R n\u00d7n and B \u2208 R n\u00d7p are appropriate matrices, q k \u2208 R n is a deterministic disturbance, and w k \u2208 R n is an arbitrary additive process noise term. We consider piecewise-linear feedback controllers of the form \u03c6 : R n \u00d7 N \u2192 U, which map a state x k \u2208 R n and a time step k \u2208 N to a control input u k \u2208 U. The controller may be time-dependent, because we consider control objectives with a finite time horizon. The random variable w k \u2208 \u2206 is defined on a probability space (\u2206, D, P), with \u03c3-algebra D and probability measure P defined over D. We do not require the sample space \u2206 and probability measure P to be known explicitly. Instead, we employ a sampling-based approach, for which it suffices to have a finite number of N independent and identically distributed (i.i.d.) samples of the random variable, and to assume that its distribution is independent of time. Due to the process noise w k , the successor state x k+1 is a random variable at time k. We denote the probability density function over successor states as p w k (x k+1 |x k+1 ), wherex k+1 = Ax k + Bu k + q k is its noiseless value.\nRemark 1 (Restriction to linear systems). Our methods are theoretically amenable to nonlinear systems, albeit requiring more advanced 1-step reachability computations unrelated to our main contributions. Hence, we restrict ourselves to the linear system in Eq. (1). We discuss extensions to nonlinear systems in Sect. 6. Problem statement. We consider control objectives that are expressed as (step-bounded) reach-avoid properties. A reach-avoid property \u03d5 K\nx0 is satisfied if, starting from state x 0 at time k = 0, the system reaches a desired goal region X G \u2282 R n within a finite time horizon of K \u2208 N steps, while avoiding a critical region X C \u2282 R n . We write the probability of satisfying a reach-avoid property \u03d5 K\nx0 under a controller \u03c6 as Pr \u03c6 (\u03d5 K x0 ). We state the formal problem as follows.\nCompute a controller \u03c6 for the system in Eq. (1) that, with high confidence, guarantees that Pr \u03c6 (\u03d5 K x0 ) \u2265 \u03b7, where \u03b7 \u2208 [0, 1] is a pre-defined probability threshold.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Markov decision processes", "text": "A Markov decision process (MDP) is a tuple M = (S, Act, s I , P ) where S is a finite set of states, Act is a finite set of actions, s I is the initial state, and P : S \u00d7 Act Dist(S ) is the (partial) probabilistic transition function. We call (s, a, s ) with probability P (s, a)(s ) > 0 a transition. A deterministic (or pure) policy (Baier and Katoen 2008) for an MDP M is a function \u03c0 : S * \u2192 Act, where S * is a sequence of states. The set of all possible policies for M is denoted by \u03a0 M . Note that we leave out rewards for brevity, but our approach is directly amenable to expected reward properties (Baier and Katoen 2008).\nA probabilistic reach-avoid property Pr \u03c0 (\u03d5 K s I ) for an MDP describes the probability of reaching a set of goal states S G \u2282 S within K \u2208 N steps under policy \u03c0 \u2208 \u03a0, while avoiding a set of critical states S C \u2282 S, where S G \u2229 S C = \u2205. An optimal policy \u03c0 * \u2208 \u03a0 M for MDP M maximizes the reachability probability:\n\u03c0 * = arg max \u03c0\u2208\u03a0 M Pr \u03c0 (\u03d5 K s I ).(2)\nWe now relax the assumption that probabilities are precisely given. An interval Markov decision process (iMDP) is a tuple M I = (S, Act, s I , P) where the uncertain (partial) probabilistic transition function P : S \u00d7 Act \u00d7 S I is defined over intervals I = {[a, b] | a, b \u2208 (0, 1] and a \u2264 b}. iMDPs define sets of MDPs that vary only in their transition function. In particular, for an MDP transition function P , we write P \u2208 P if for all s, s \u2208 S and a \u2208 Act we have P (s, a)(s ) \u2208 P(s, a)(s ) and P (s, a) \u2208 Dist(S ). For iMDPs, a policy needs to be robust against all P \u2208 P. We employ value iteration to compute a policy \u03c0 * \u2208 \u03a0 M I for iMDP M I that maximizes the lower bound on the reachability probability Pr \u03c0 (\u03d5 K s I ) within horizon K:\n\u03c0 * = arg max \u03c0\u2208\u03a0 M I Pr \u03c0 (\u03d5 K s I ) = arg max \u03c0\u2208\u03a0 M I min P \u2208P Pr \u03c0 (\u03d5 K s I ). (3)\nNote that deterministic policies suffice to obtain optimal values for (i)MDPs (Puterman 1994;Puggelli et al. 2013).", "publication_ref": ["b6", "b6", "b45", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Our iterative abstraction scheme", "text": "Our proposed approach is shown in Fig. 1. We choose a fixed state-space partition and confidence level up front, and select an initial number of samples N (note that extensions to variable confidence levels or partitions are straightforward). As explained in Sect. 3 and 4, we then abstract the dynamical system as an iMDP using N samples of the noise. For the Figure 1: Our iterative approach between abstraction and verification, where N is the number of samples used for the abstraction, and \u03b7 is the threshold reachability probability. iMDP, we compute an optimal policy \u03c0 * that maximizes the probability of satisfying the given property, as per Eq. (3). If the maximum reachability probability under this policy is above the required threshold \u03b7, we compute the corresponding controller \u03c6 for the dynamical system, and terminate the scheme. If the maximum probability is unsatisfactory (i.e. below \u03b7), we obtain additional samples by increasing N by a fixed factor \u03b3 > 1. The updated iMDP has tighter probability intervals, but may also have more transitions. Since the states and actions of the iMDP are independent of N , they are only computed once, in the first iteration. In general we cannot guarantee a priori that the property is satisfiable up to the given value of \u03b7, so we also terminate the scheme after a fixed number of iterations, in which case no output is returned.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Finite-State MDP Abstraction", "text": "First, we describe how we partition the state space into a set of discrete convex regions. We then use this partition to build a finite-state abstraction of the dynamical system in Eq. (1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "State space discretization", "text": "We choose a partition R of the continuous state space R n into a set of disjoint regions that represent a bounded portion X \u2282 R n . In addition, we define a single absorbing region r a , representing R n \\X . We number the regions in R from 1 to |R|, and define a function T : R n \u2192 {1, 2, . . . , |R|, |R| + 1} that maps a continuous state x \u2208 R n to one of the regions in partition R through the index of that region, or to |R| + 1 if x \u2208 r a = R n \\X . Thus, the absorbing region r a captures the event that the continuous state leaves the bounded portion of the state space over which we plan. For convenience, we also define the inverse mapping as R i = T \u22121 (i).\nWe consider the regions in R to be n-dimensional bounded, convex polytopes. In particular, convex polytope R i is the solution set of m linear inequalities parameterized by\nM i \u2208 R m\u00d7n and b i \u2208 R m , yielding R i = x \u2208 R n | M i x \u2264 b i .\nIn addition, the following assumption allows us to translate properties for the dynamical system to properties on the iMDP abstraction: Assumption 1. The continuous goal region X G and critical region X C are aligned with the union of a subset of regions in R, i.e. X G = \u222a i\u2208I R i and X C = \u222a j\u2208J R j for index sets I, J \u2282 {1, 2, . . . , |R|}.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MDP abstraction", "text": "We formalize the dynamical system discretized under partition R as an MDP M = (S, Act, s I , P), by defining its states, actions, and transition probabilities (cf. ensuing paragraphs). We assume the initial state s I \u2208 S is known, and we capture time constraints by the bounded reach-avoid property.\nStates. The set of states is S = {s i | i = 1, . . . , |R|} \u222a {s a }, where discrete state s i represents all continuous states x k for which T (x k ) = i. Then, the MDP consists of |S| = |R| + 1 states: one for every region in partition R, plus one state s a corresponding to the absorbing region r a . State s a is a deadlock, meaning the only transition leads back to s a .\nActions. Discrete actions correspond to the execution of a control input u k \u2208 U in the dynamical system in Eq. (1). We define q \u2208 N MDP actions, so Act = {a 1 , . . . , a q }. Recall that the noiseless successor state of\nx k isx k+1 = Ax k + Bu k + q k .\nEvery action a j is associated with a fixed continuous target point d j \u2208 R n , and is defined such that its noiseless successor statex k+1 = d j . While not a restriction of our approach, we define one action for every MDP state, and choose the target point to be the center of its region.\nThe MDP must form a correct abstraction of the dynamical system. Thus, action a j only exists in an MDP state s i if, for every continuous state x k \u2208 R i , there exists a control u k \u2208 U, such thatx k+1 = d j . To impose this constraint, we define the one-step backward reachable set G(d j ):\nG(d j ) = {x \u2208 R n | d j = Ax+Bu k +q k , u k \u2208 U}. (4)\nThen, action a j exists in state s i if and only if R i \u2286 G(d j ). Note that the existence of an action in an MDP state merely implies that for every continuous state in the associated region, there exists a feasible control input that induces this transition. The following assumption asserts that the regions in R can indeed be contained in the backward reachable set.\nAssumption 2. The backward reachable set G(d j ) has a non-empty interior, which implies that matrix B is full row rank, i.e., rank(B) = n, where n = dim(x) in Eq. (1).\nFor many systems, we may group together multiple discrete time steps in Eq. (1), such that Assumption 2 holds (see Badings et al. (2021, Sect. 6) for more details). To compute the actual control u k in state x k at time k, we replace x k+1 by d j in Eq. (1) and solve for u k , yielding:\nu k = B + (d j \u2212 q k \u2212 Ax k ),(5)\nwith B + the pseudoinverse of B. It is easily verified that for every state where action a j is enabled, there exists a u k such that Eq. ( 5) holds (depending on B, it may not be unique).\nTransition probability intervals. We want to determine the probability P (s i , a l )(s j ) to transition from state s i to state s j upon choosing action a l . In the abstraction, this is equivalent to computing the cumulative density function of the distribution over the successor state x k+1 under the polytope R j associated with state s j . The probability density function p w k (x k+1 |x k+1 = d j ) captures the distribution over successor states x k+1 , which depends on the process noise w k . By denoting P w k (x k+1 \u2208 R j ) as the probability that x k+1 takes a value in discrete region R j , we write:\nP (s i , a l )(s j ) = P w k (x k+1 \u2208 R j ) = Rj p w k (x k+1 |x k+1 = d j )dx k+1 .(6)\nRecall that the probability density function p w k (\u2022) is unknown, making a direct evaluation of Eq. ( 6) impossible. Instead, we use a sampling-based approach to compute probability intervals as explained in Sect. 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampling-Based Probability Intervals", "text": "We introduce a sampling-based method to estimate the transition probabilities in Eq. ( 6), based on a finite set of N observations w\n(i) k \u2208 \u2206, i = 1, .\n. . , N of the process noise. Each sample has a unique index i = 1, . . . , N and is associated with a possible successor state x k+1 =x k+1 +w (i) k . We assume that these samples are available from experimental data or simulations, and are thus obtained at a low cost.\nAssumption 3. The noise samples w\n(i) k \u2208 \u2206, i = 1, . . . , N are i.i.d\n. elements from (\u2206, P), and are independent of time.\nDue to the samples being i.i.d., the set w\n(1) k , . . . , w (N ) k\nof N samples is a random element from the probability space \u2206 N equipped with the product probability P N .\nAs an example, we want to evaluate the probability P (s i , a l )(s j ) that state-action pair (s i , a l ) induces a transition to state s j . A naive frequentist approach to approximate the probability would be to determine the fraction of the samples leading to this transition, using the following definition. Definition 1. The cardinality N in j \u2208 {0, . . . , N } of the index set of the samples leading to x k+1 \u2208 R j is defined as\nN in j = {i \u2208 {1, . . . , N } | (x k+1 + w (i) k ) \u2208 R j } . (7)\nSimilarly, we define N out j = N \u2212 N in j as the number of samples for whichx k+1 + w\n(i) k is not contained in R j .\nNote that N in j and N out j depend on both the sample set and the action. The frequentist approach is simple, but may lead to estimates that deviate critically from their true values if the number of samples is limited (we illustrate this issue in practice in Sect. 5.1). In what follows, we introduce our method to be robust against such estimation errors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bounds for the transition probabilities", "text": "We adapt methods from the scenario approach (Campi and Garatti 2018) to compute intervals of probabilities instead of precise estimates. For every probability P (s i , a l )(s j ), we compute an upper and lower bound (i.e. an interval) that contains the true probability in Eq. (6) with a high confidence. We formalize the resulting abstraction as an iMDP, where these probability intervals enter the uncertain transition function P : S \u00d7 Act \u00d7 S I. As the intervals are PAC, this iMDP is a robust abstraction of the dynamical system.\nFirst, we introduce the concept of risk (or violation probability), which is a measure of the probability that a successor state is not in a given region (Campi and Garatti 2008).\nDefinition 2. The risk P w k (x k+1 / \u2208 R j ) that a successor state x k+1 is not in region R j is P w k (x k+1 / \u2208 R j ) = P{w k \u2208 \u2206 :x k + w k / \u2208 R j } = 1 \u2212 P w k (x k+1 \u2208 R j ).(8)\nCrucially for our approach, note that P (s i , a l )(s j ) = P w k (x k+1 \u2208 R j ). The scenario approach enables us to bound the risk that the optimal point of a so-called scenario optimization problem does not belong to a feasible setR defined by a set of constraints when we are only able to sample a subset of those constraints. By formulating this optimization problem such thatR is closely related to a region R j , we obtain upper and lower bounds on the risk over R j , and thus also on the corresponding transition probability (see Appendix A for details). Importantly, this result means that we can adapt the theory from the scenario approach to compute transition probability intervals for our abstractions.\nBased on this intuition, we state the main contribution of this section, as a non-trivial variant of Romao, Margellos, and Papachristodoulou (2020, Theorem 5), adapted for our context. Specifically, for a given transition (s i , a l , s j ) and the corresponding number of samples N out j outside of region R j (as per Def. 1), Theorem 1 returns an interval that contains P (s i , a l )(s j ) with at least a pre-defined confidence level. Theorem 1 (PAC probability intervals). For N \u2208 N samples of the noise, fix a confidence parameter \u03b2 \u2208 (0, 1). Given N out j , the transition probability P (s i , a l )(s j ) is bounded by\nP N p \u2264 P (s i , a l )(s j ) \u2264p \u2265 1 \u2212 \u03b2,(9)\nwherep = 0 if N out j = N , and otherwisep is the solution of\n\u03b2 2N = N out j i=0 N i (1 \u2212p) ipN \u2212i ,(10)\nandp = 1 if N out j = 0, and otherwisep is the solution of\n\u03b2 2N = 1 \u2212 N out j \u22121 i=0 N i (1 \u2212p) ipN \u2212i .(11)\nFor brevity, we postpone the proof and technical details to Appendix A. Theorem 1 states that with a probability of at least 1 \u2212 \u03b2, the probability P (s i , a l )(s j ) is bounded by the obtained interval. Importantly, this claim holds for any \u2206 and P, meaning that we can bound the probability in Eq. ( 6), even when the probability distribution of the noise is unknown.", "publication_ref": ["b17", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Practical use of Theorem 1", "text": "We describe how we apply Theorem 1 to compute probability intervals of the iMDPs. For every state-action pair (s i , a l ), we obtain N samples of the noise, and we determine N out j for every j \u2208 {1, . . . , |R|, |R| + 1}. Then, we invoke Theorem 1 for every possible successor state s j \u2208 S, to compute the bounds on P (s i , a l )(s j ). Fig. 2 shows this process, where every tick is a successor state x k+1 under a sample of the noise. This figure also shows point estimates of the probabilities, derived using the frequentist approach. If no samples are observed in a region, we assume that P (s i , a l )(s j ) = 0.   Let N = \u03b3N 13: end while 14: return piece-wise linear controller \u03c6 based on \u03c0 * Interestingly, our method is in practice almost as simple as the frequentist approach, but has the notable advantage that we obtain robust intervals of probabilities. Note that Eq. ( 10) and ( 11) are cumulative distribution functions of a beta distribution with parameters N out j + 1 (or N out j ) and N \u2212 N out j (or N \u2212 N out j \u2212 1), respectively (Campi and Garatti 2018), which can directly be solved numerically forp (orp). To speed up the computations at run-time, we apply a tabular approach to compute the intervals for all relevant values of N , \u03b2, and k up front. In Appendix A.2, we exemplify how the number of samples controls the tightness of the intervals.\np wk (x k+1 |x k+1 = d) \u015d x k+1 = d P wk (x k+1 \u2208 R 1 ) P wk (x k+1 \u2208 R 2 ) P wk (x k+1 \u2208 R 3 ) N", "publication_ref": ["b17"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Numerical Examples", "text": "We implement our iterative abstraction method in Python, and tailor the model checker PRISM (Kwiatkowska, Norman, and Parker 2011) for iMDPs to compute robust optimal policies. We present a pseudocode of our method in Algorithm 1. At every iteration, the obtained iMDP is fed to PRISM, which computes the optimal policy associated with the maximum reachability probability, as per Eq. (3). Our codes are available via https://gitlab.science.ru.nl/tbadings/sample-abstract, and all experiments are run on a computer with 32 3.7GHz cores and 64 GB of RAM. We report the performance of our method on: (1) a UAV motion control, (2) a building temperature regulation, and (3) a spacecraft rendezvous problem. In all benchmarks, we use Theorem 1 with \u03b2 = 0.01, and apply the iterative scheme with \u03b3 = 2, starting at N = 25, with an upper bound of 12, 800 samples.", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "UAV motion planning", "text": "We consider the reach-avoid problem for a UAV operating under turbulence, which was introduced in Sect. 1. Our goal is to compute a controller that guarantees (with high confidence) that the probability to reach a goal area, while also avoiding unsafe regions, is above a performance threshold of \u03b7 = 0.75. We consider a horizon of 64 time steps, and the problem layout is displayed in Fig. 3, with goal and critical regions shown in green and red, respectively. We model the UAV as a system of 3 double integrators (see Appendix B for details). The state x k \u2208 R 6 encodes the position and velocity components, and control inputs u k \u2208 R 3 model actuators that change the velocity. The effect of turbulence on the state causes (non-Gaussian) process noise, which we model using a Dryden gust model (B\u00f8hn et al. 2019;Dryden 1943). We compare two cases: a) a low turbulence case, and 2) a high turbulence case. We partition the state space into 25, 515 regions.\nScalability. We report the model sizes and run times in Appendix B.2. The number of iMDP states equals the size of the partition. Depending on the number of samples N , the iMDP has 9 \u2212 24 million transitions. The mean time to compute the set of iMDP actions (which is only done in the Accounting for noise matters. In Fig. 3, we show state trajectories under the optimal iMDP-based controller, under high and low turbulence (noise). Under low noise, the controller prefers the short but narrow path; under high noise, the longer but safer path is preferred. Thus, accounting for process noise is important to obtain controllers that are safe. iMDPs yield safer guarantees than MDPs. To show the importance of using robust abstractions, we compare, under high turbulence, our robust iMDP approach against a naive MDP abstraction. This MDP has the same states and actions as the iMDP, but uses precise (frequentist) probabilities. The maximum reachability probabilities (guarantees) for both methods are shown in Fig. 4. For every value of N , we apply the resulting controllers to the dynamical system in Monte Carlo simulations with 10, 000 iterations, to determine the empirical reachability probability. Fig. 4 shows that the nonrobust MDPs yield poor and unsafe performance guarantees: the actual reachability of the controller is much lower than the reachability guarantees obtained from PRISM. By contrast, our robust iMDP-based approach consistently yields safe lower bound guarantees on the actual performance of controllers. The performance threshold of Pr \u03c0 * (\u03d5 K s I ) \u2265 0.75 is guaranteed for N = 3, 200 samples and higher.", "publication_ref": ["b11", "b24"], "figure_ref": ["fig_2", "fig_2", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Building temperature regulation", "text": "Inspired by Cauchi and Abate (2018), we consider a temperature control problem for a building with two rooms, both having their own radiator and air supply. Our goal is to maximize the probability to reach a temperature of 20\u00b0C in both zones within 32 steps of 15 minutes. The state x k \u2208 R 4 of the system (see Appendix C for details) reflects the temperatures of both zones and radiators, and control inputs u k \u2208 R 4   Our approach (N=12,800) StocHy\nFigure 6: Maximum lower bound probabilities to reach the goal zone temperature of 21\u00b0C from any initial state within 64 steps, for our approach (N = 12, 800) and StocHy.\nchange the air supply and boiler temperatures in both zones.\nThe deterministic heat gain through zone walls is modeled by the disturbance q k \u2208 R 4 . The noise w k \u2208 R 4 has a Gaussian distribution (but this assumption is not required for our approach). We partition the state space into 35, 721 regions: 21 values for zone temperatures and 9 for radiator temperatures.\nMore samples means less uncertainty. In Fig. 5, we show (for fixed radiator temperatures) the maximum lower bound probabilities obtained from PRISM, to reach the goal from any initial state. The results clearly show that better reachability guarantees are obtained when more samples are used to compute the iMDP probability intervals. The higher the value of N , the lower the uncertainty in the intervals, leading to better reachability guarantees. Notably, the largest iMDP has around 200 million transitions, as reported in Appendix C.2.", "publication_ref": ["b18"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Benchmarks to other control synthesis tools", "text": "StocHy. We benchmark our method on a building temperature problem against StocHy (Cauchi and Abate 2019), a verification and synthesis tool based on formal abstractions (see Appendix D.1 for details on the setup and results). Similar to our approach, StocHy also derives robust iMDP abstractions. However, StocHy requires precise knowledge of the noise distribution, and it discretizes the control input space of the dynamical system, to obtain a finite action space. The maximum probabilities to reach the goal zone temperature from any initial state obtained for both methods are presented in Fig. 6. The obtained results are qualitatively similar, and close to the goal temperature, our lower bound reachability guarantees are slightly higher than those obtained from StocHy. However, when starting at temperatures close to the boundary (e.g. at both low radiator and zone temperature), the guarantees obtained from our approach are slightly more conservative. This is due to the fact that our approach relies on PAC guarantees on the transition probabilities, while StocHy gives straight probabilistic outcomes. While both methods yield results that are qualitatively similar, our approach is an order of magnitude faster (45 min for StocHy, vs. 3 \u2212 9 s for our approach; see Appendix D.1 and Table 1 for details).\nSReachTools. We apply our method to the spacecraft rendezvous benchmark (see Fig. 7) of SReachTools (Vinod, Gleason, and Oishi 2019), an optimization-based toolbox for probabilistic reachability problems (see Appendix D.2 for details). While we use samples to generate a model abstraction, SReachTools employs sample-based methods over the properties directly. Distinctively, SReachTools does not create abstractions (as in our case) and is thus generally faster than our method. However, its complexity is exponential in the number of samples (versus linear complexity for our method). Importantly, we derive feedback controllers, while the sampling-based methods of SReachTools compute open-loop controllers. Feedback controllers respond to state observations over time and are, therefore, more robust against strong disturbances from noise, as also shown in Fig. 7.", "publication_ref": ["b55"], "figure_ref": ["fig_6", "fig_6"], "table_ref": ["tab_3"]}, {"heading": "Concluding Remarks and Future Work", "text": "We have presented a novel sampling-based method for robust control of autonomous systems with process noise of unknown distribution. Based on a finite-state abstraction, we have shown how to compute controllers with PAC guarantees on the performance on the continuous system. Our experiments have shown that our method effectively solves realistic problems and provides safe lower bound guarantees on the performance of controllers.\nNonlinear systems. While we have focused on linear dynamical systems, as discussed in Remark 1, we wish to develop extensions to nonlinear systems. Such extensions are non-trivial and may require more involved reachability computations (Bansal et al. 2017;Chen,\u00c1brah\u00e1m, and Sankaranarayanan 2013). Specifically, the main challenge is to compute the enabled iMDP actions via the backward reachable set defined in Eq. (4), which may become non-convex under nonlinear dynamics. Note that computing the PAC probability intervals remains unchanged, as the scenario approach relies on the convexity of the target set only, and not on that of the backward reachable set. Alternatively, we may apply our method on a linearized version of the nonlinear system. However, in order to preserve guarantees, one must then account for any linearization error.\nState space discretization. The discretization of the state space influences the quality of the reachability guarantees: a more fine-grained partition yields an abstraction that is a more accurate representation of the dynamical system, but also increases the computational complexity. In the future, we plan to employ adaptive discretization schemes to automatically balance this trade-off, such as in Soudjani and Abate (2013).\nSafe exploration. Finally, we wish to incorporate other uncertainties in Eq. ( 1), such as state/control-dependent process noise, or measurement noise. Moreover, we may drop the assumption that the system matrices are precisely known, such that we must simultaneously learn about the unknown deterministic dynamics and the stochastic noise. Learning deterministic dynamics is common in safe learning control (Brunke et al. 2021), but enabling safe exploration requires strong assumptions on stochastic uncertainty. This is a challenging goal, as it conflicts with our assumption that the distribution of the process noise is completely unknown.\nR j R j (1.2) h j x (1)\nx (2)\nFigure 8: Polytope R j has a Chebyshev centerh j (note that it is not unique, since the circle can be shifted while remaining within the polytope). Polytope R j (1.2) is scaled by a factor \u03bb = 1.2 and is computed using Eq. (12).", "publication_ref": ["b7", "b21", "b52", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "A Proofs and Examples", "text": "In this appendix, we provide the proof of Theorem 1, and we exemplify how the number of noise samples affects the tightness of the obtained intervals. Before stating the actual proof, we introduce a number of technical details.\nScaled polytopes. First, let us define R j (\u03bb) as a version of polytope R j which is scaled by a factor \u03bb \u2265 0 relative to a socalled Chebyshev centerh j \u2208 R n (Boyd and Vandenberghe 2014). The resulting scaled polytope is\nR j (\u03bb) = {x \u2208 R n | M j x \u2264 \u03bb(b j +h j ) \u2212h j }. (12\n)\nNote that R j (1) = R j , and that shifting byh j ensures that R j (\u03bb 1 ) \u2282 R j (\u03bb 2 ) for every 0 \u2264 \u03bb 1 < \u03bb 2 . A visualization of the scaling for an arbitrary polytope R j is shown in Fig. 8 (in this example, the Chebyshev center is not unique, since the circle can be shifted while remaining within R j ).\nScenario optimization problem. We use the scenario approach to estimate the risk associated with the solution of the following convex scenario optimization problem with discarded samples, which has a scalar decision variable \u03bb:\nminimize \u03bb\u2208R+ \u03bb (13) subject tox k+1 + w (i) k \u2208 R j (\u03bb) \u2200i \u2208 {1, .\n. . , N }\\Q, whose optimal solution \u03bb * |Q| parameterizes polytope R j (\u03bb * |Q| ) through Eq. (12). We explicitly write the dependency of the solution on the cardinality |Q| of the index set Q, which accounts for a subset of samples whose constraints have been discarded based on the following rule: Lemma 1. The sample removal set Q \u2286 {1, . . . , N } is obtained by iteratively removing the active constraints from Eq. (13). Thus, given N samples and any two removal sets with cardinalities |Q 1 | < |Q 2 |, it holds that Q 1 \u2282 Q 2 . Moreover, any discarded sample i \u2208 Q violates the solution \u03bb * Q to Eq. (13), i.e.x k+1 + w\n(i) k / \u2208 R j (\u03bb * |Q| ), with probability one. \u22122 \u22121 0 1 2 R j (\u03bb * N out j \u22121 ) R j (\u03bb * N out j ) R j\nFigure 9: Bounding region R j = [\u22121, 1] using N = 10 successor state samples (N out j = 5). Discarding N out j = 5 samples defines the red region R j (\u03bb * N out j ) \u2286 R j , while discarding one less sample defines the blue region R j (\u03bb *\nN out j \u22121 ) \u2283 R j .\nIntuitively, the successor state of a sample associated with an active constraint is on the boundary of the optimal solution R j (\u03bb * |Q| ) of Eq. ( 13). Under the following non-accumulation assumption, an active constraint exists and is unique, as long as |Q| < N (i.e., not yet all samples have been discarded). Assumption 4. Given a noise sample w k \u2208 \u2206, the probability that the successor state x k+1 is on the boundary of any polytope R j (\u03bb) for any \u03bb \u2208 R + is zero.\nAssumption 4 implies that the solution to Eq. ( 13) is unique, and that the number of active constraints is equal to one (given N > 0 and |Q| < N ), as samples accumulate on the boundary of the polytope with probability zero.\nUnder/over-approximating regions. Recall from Def. 1 that N out j is the number of samples leading to a successor state outside of region R j . The following lemma uses N out j to define an under-or over-approximation of region R j . Lemma 2. When discarding |Q| = N out j samples as per Lemma 1, it holds that R j (\u03bb *\nN out j ) \u2286 R j . When discarding |Q| = N out j \u2212 1 samples, it holds that R j (\u03bb * N out j \u22121 ) \u2283 R j Proof.\nLemma 1 states that at every step, we may only discard the sample of the active constraint. By construction, after discarding |Q| = N out j samples, all remaining successor state samples lie within R j , so \u03bb *\nN out j \u2264 1, so R j (\u03bb * N out j ) \u2286 R j .\nWhen we discard one less sample, i.e. |Q| = N out j \u2212 1, we must have one sample outside of R j , so \u03bb *\nN out j \u22121 > 1, mean- ing that R j (\u03bb * N out j \u22121 ) \u2283 R j . This concludes the proof. Intuitively, R j (\u03bb * N out j ) contains exactly those samples in R j , while R j (\u03bb * N out j \u22121 )\nadditionally contains the sample closest outside of R j , as visualized in Fig. 9 for a 1D example.\nA.1 Proof of Theorem 1.\nThe proof of Theorem 1 is adapted from Romao, Margellos, and Papachristodoulou (2020, Theorem 5), which is based on three key assumptions: (1) the considered scenario problem belongs to the class of so-called fully-supported problems (see Campi and Garatti (2008) for a definition), (2) its solution is unique, and (3) discarded samples violate the optimal solution with probability one. In our case, requirement (2) is implied by Assumption 4, ( 3) is satisfied by Lemma 1, and the problem is fully-supported because the number of decision variables is one. Under these requirements, Romao, Margellos, and Papachristodoulou (2020, Theorem 5) states that the risk associated with an optimal solution \u03bb * |Q| to Eq. ( 13) for |Q| discarded samples satisfies the following expression:\nP N P x / \u2208 R j (\u03bb * |Q| ) \u2264 = 1 \u2212 |Q| i=0 N i i (1 \u2212 ) N \u2212i ,(14)\nwhere we omit the subscripts in P w k (\u2022) and x k+1 for brevity. Eq. ( 14) is the cumulative distribution function (CDF) of a beta distribution with parameters |Q| + 1 and N \u2212 |Q|. We denote this CDF by F |Q| ( ), where we explicitly write the dependency on |Q|. Hence, we obtain\nF |Q| ( ) = P N P x / \u2208 R j (\u03bb * |Q| ) \u2264 =\u03b2.(15)\nThus, if we discard |Q| samples, Eq. ( 15) returns the confidence probability\u03b2 \u2208 (0, 1) by which the probability of\nx k+1 / \u2208 R j (\u03bb * |Q|\n) is upper bounded by . Conversely, we can compute the value of needed to obtain a confidence probability of\u03b2, using the percent point function (PPF) G |Q| (\u03b2):\nP N P x / \u2208 R j (\u03bb * |Q| ) \u2264 G |Q| (\u03b2) =\u03b2. (16\n)\nThe PPF is the inverse of the CDF, so by definition, we have\n= G |Q| (\u03b2) = G |Q| F |Q| ( ) .(17)\nNote that P (x \u2208 R) + P (x / \u2208 R) = 1, so Eq. (15) equals\nF |Q| ( ) = P N P x \u2208 R j (\u03bb * |Q| ) \u2265 1 \u2212 =\u03b2. (18\n)\nBy defining p = 1 \u2212 , Eq. ( 17) and ( 18) are combined as\nP N 1 \u2212 G |Q| (\u03b2) \u2264 P x \u2208 R j (\u03bb * |Q| ) = 1 \u2212 |Q| i=0 N i (1 \u2212 p) i p N \u2212i =\u03b2.(19)\nIn what follows, we separately use Eq. ( 18) to prove the lower and upper bound of the probability interval in Theorem 1.\nLower bound. There are N possible values for |Q|, ranging from 0 to N \u2212 1. The case |Q| = N (i.e. all samples are discarded) is treated as a special case in Theorem 1. We fix \u03b2 = 1 \u2212 \u03b2 2N in Eq. ( 19), yielding the series of equations\nP N 1 \u2212 G0 1 \u2212 \u03b2 2N \u2264 P x \u2208 Rj(\u03bb * 0 ) =1 \u2212 \u03b2 2N . . . . . . (20\n)\nP N 1 \u2212 GN\u22121 1 \u2212 \u03b2 2N \u2264 P x \u2208 Rj(\u03bb * N \u22121 ) =1 \u2212 \u03b2 2N . Denote the event that 1 \u2212 G n 1 \u2212 \u03b2 2N \u2264 P x \u2208 R j (\u03bb * n ) for n = 0, . . . , N \u2212 1 by A n . Regardless of n, this event has a probability of P N {A n } = 1 \u2212 \u03b2 2N\n, and its complement A n of P N {A n } = \u03b2 2N . Via Boole's inequality, we know that\nP N N \u22121 i=0 A n \u2264 N \u22121 i=0 P N A n = \u03b2 2N N = \u03b2 2 . (21)\nThus, for the intersection of all events in Eq. (20) we have\nP N N \u22121 i=0 A n = 1 \u2212 P N N \u22121 i=0 A n \u2265 1 \u2212 \u03b2 2 .(22)\nAfter observing the samples at hand, we replace |Q| by the actual value of N out j (as per Def. 1), giving one of the expression in Eq. (20). The probability that this expression holds cannot be smaller than that of the intersection of all events in Eq. ( 22). Thus, we obtain\nP N p \u2264 P x \u2208 R j (\u03bb * N out j ) \u2265 1 \u2212 \u03b2 2 ,(23)\nwherep = 0 if N out j = N (which trivially holds with probability one), and\notherwisep = 1 \u2212 G N out j (1 \u2212 \u03b2 2N )\nis the solution for p to Eq. ( 19), with |Q| = N out j and\u03b2 = 1 \u2212 \u03b2 2N :\n1 \u2212 \u03b2 2N = 1 \u2212 N out j i=0 N i (1 \u2212 p) i p N \u2212i ,(24)\nwhich is equivalent to Eq. (10).\nUpper bound. Eq. ( 19) is rewritten as an upper bound as\nP N P x \u2208 R j (\u03bb * |Q| ) < 1 \u2212 G |Q| (\u03b2) = 1 \u2212\u03b2, (25\n)\nwhere again, |Q| can range from 0 to N \u2212 1. However, to obtain high-confidence guarantees on the upper bound, we now fix\u03b2 = \u03b2 2N , which yields the series of equations\nP N P x \u2208 Rj(\u03bb * 0 ) < 1 \u2212 G0 \u03b2 2N =1 \u2212 \u03b2 2N . . . . . .(26)\nP N P x \u2208 Rj(\u03bb * N \u22121 ) < 1 \u2212 GN\u22121 \u03b2 2N =1 \u2212 \u03b2 2N .\nAnalogous to the lower bound case, Boole's inequality implies that the intersection of all expressions in Eq. ( 26) has a probability of at least 1 \u2212 \u03b2 2 . After observing the samples at hand, we replace |Q| by N out j \u2212 1, yielding one of the expressions in Eq. (26). For this expression, it holds that\nP N P x \u2208 R j (\u03bb * N out j \u22121 ) \u2264p \u2265 1 \u2212 \u03b2 2 ,(27)\nwherep = 1 if N out j = 0 (which trivially holds with probability one), and otherwisep = 1 \u2212 G N out j \u22121 ( \u03b2 2N ) is the solution for p to Eq. (19), with |Q| = N out j \u2212 1 and\u03b2 = \u03b2 2N :\n\u03b2 2N = 1 \u2212 N out j \u22121 i=0 N i (1 \u2212 p) i p N \u2212i ,(28)\nwhich is equivalent to Eq. (11). Probability interval. We invoke Lemma 2, which states that R j (\u03bb *\nN out j ) \u2286 R j \u2282 R j (\u03bb * N out j \u22121 ), so we have P x \u2208 R j (\u03bb * N out j ) \u2264 P (x \u2208 R j ) = P (s i , a l )(s j ) < P x \u2208 R j (\u03bb * N out j \u22121 ) .(29)\nWe use Eq. (29) to write Eq. ( 23) and ( 27) in terms of the transition probability P (s i , a l )(s j ). Finally, by applying Boole's inequality, we combine Eq. ( 23) and ( 27) as follows:\nP N p \u2264 P (s i , a l )(s j ) P (s i , a l )(s j ) \u2264p = P N p \u2264 P (s i , a l )(s j ) \u2264p \u2265 1 \u2212 \u03b2,(30)\nwhich is equal to Eq. ( 9), so we conclude the proof.", "publication_ref": ["b13", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Example use of Theorem 1", "text": "We provide an example to illustrate how the number of samples N affects the tightness of the probability intervals returned by Theorem 1. Consider a system with a 1dimensional state x k \u2208 R, where the probability density function p w k (x k+1 |x k+1 = d l ) for a specific action a l with target point d l is given by a uniform distribution over the domain [\u22124, 4]. For a given region R j = [\u22121, 1] (which is also shown in Fig. 9), we want to evaluate the probability that x k+1 \u2208 R j , which is clearly 0.25. To this end, we apply Theorem 1 for different numbers of samples N \u2208 [25,12800], and a confidence level of \u03b2 = 0.1 or \u03b2 = 0.01. Since the obtained probability bounds are random variables through their dependence on the set of N samples of the noise, we repeat the experiment 100, 000 times for every value of N . The resulting bounds on the transition probability are shown in Fig. 10. The plot shows that the uncertainty in the transition probability is reduced by increasing the number of samples.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "B Details on UAV Benchmark", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Explicit model formulation", "text": "The 6-dimensional state vector of the UAV model is x = [p x , v x , p y , v y , p z , v z ] \u2208 R 6 , where p i and v i are the po-sition and velocity in the direction i. Every element of the vector of control inputs u = [f x , f y , f z ] \u2208 R 3 is constrained to the interval [\u22124, 4]. The discrete-time dynamics are an extension of the lower-dimensional case in Badings et al. (2021), and are written in the form of Eq. (1) as follows:\nx k+1 = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\n1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1\n\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb x k + \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 0.5 0 0 1 0 0 0 0.5 0 0 1 0 0 0 0.5 0 0 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb u k + w k , (31\n)\nwhere w k is the effect of turbulence on the continuous state, which we model using a Dryden gust model; we use a Python implementation by B\u00f8hn et al. (2019). Note that the drift term q k = 0, and is thus omitted from Eq. ( 31). Since the model in Eq. ( 31) is not fully actuated (it has only 3 controls, versus a state of dimension 6), we group every two discrete time steps together and rewrite the model as follows:\nx k+2 =\u0100x k +\u016a u k,k+1 + w k,k+1 ,(32)\nwhere\u0100 \u2208 R 6\u00d76 and\u016a \u2208 R 6\u00d76 are properly redefined matrices, and u k,k+1 \u2208 R 6 and w k,k+1 \u2208 R 6 reflect the control inputs and process noise at both time steps k and k + 1, combined in one vector. The objective of this problem is to reach the goal region, which is also depicted in Fig. 3, within 64 steps (i.e. 32 steps with the model in Eq. ( 32)). This goal region is written as the following set of continuous states (for brevity, we omit an explicit definition of the critical regions):\nX G = {x \u2208 R 6 | 11 \u2264 p x \u2264 15, 1 \u2264 p y \u2264 5, \u22127 \u2264 p z \u2264 \u22123}.(33)", "publication_ref": ["b5", "b11"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "B.2 Run times and size of model abstraction", "text": "The average run times and iMDP model sizes (over 10 runs) for all iterations of the UAV benchmark are presented in Table 1. Computing the states and actions took 15 min, but since this step is only performed in the first iteration, we omit the value from the table. The number of states (which equals the number of regions in the partition) and choices (the total number of state-action pairs) of the iMDP are independent of the value of N . By contrast, the number of transitions increases with N , because the additional noise samples may reveal more possible outcomes of a state-action pair.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "C Details on Building Temperature Regulation Benchmark", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Explicit model formulation", "text": "This model is a variant of the two-zone building automation system benchmark with stochastic dynamics in Cauchi and Abate (2018). The state vector of the model is x = [T z 1 , T z 2 , T r 1 , T r 2 ] \u2208 R 4 , reflecting both room (zone) temperatures (T z ) and both radiator temperatures (T r ). The control inputs u = [T ac 1 , T ac 2 , T boil 1 , T boil 2 ] \u2208 R 4 are the air conditioning (ac) and boiler temperatures, respectively, which are constrained within 14 \u2264 T ac \u2264 26 and 65 \u2264 T boil \u2264 85. The changes in the temperature of both zones are governed by the following thermodynamics:\nT z 1 = 1 C 1 T z 2 \u2212 T z 1 R 1,2 + T wall \u2212 T z 1 R 1,wall(34a)\n+ mC pa (T ac 1 \u2212 T z 1 ) + P out (T r 1 \u2212 T z 1 ) \u1e6a z 2 = 1 C 2 T z 1 \u2212 T z 2 R 1,2 + T wall \u2212 T z 2 R 2,wall(34b)\n+ mC pa (T ac 2 \u2212 T z 2 ) + P out (T r 2 \u2212 T z 2 ) ,\nwhere C i is the thermal capacitance of zone i, and R i,j is the resistance between zones i and j, m is the air mass flow, C pa is the specific heat capacity of air, and P out is the rated output of the radiator. Similarly, the dynamics of the radiator in room i are governed by the following equation:\nT r i = k 1 (T z i \u2212 T r i ) + k 0 w(T boil i \u2212 T r i ),(35)\nwhere k 0 and k 1 are constant parameters, and w is the water mass flow from the boiler. For the precise parameter values used, we refer to our codes, which are provided in the supplementary material. By discretizing Eq. ( 34) and ( 35) by a forward Euler method at a time resolution of 15 min, we obtain the following model in explicit form:\nx k+1 = \uf8ee \uf8ef \uf8f0 0.8425 0.0537 \u22120.0084 0.0000 0.0515 0.8435 0.0000 \u22120.0064 0.0668 0.0000 0.8971 0.0000 0.0000 0.0668 0.0000 0.8971\n\uf8f9 \uf8fa \uf8fbxk + \uf8ee \uf8ef \uf8f0\n0.0584 0.0000 0.0000 0.0000 0.0000 0.0599 0.0000 0.0000 0.0000 0.0000 0.0362 0.0000 0.0000 0.0000 0.0000 0.0362\n\uf8f9 \uf8fa \uf8fbuk(36)\n+ \uf8ee \uf8ef \uf8f0 1.2291 1.0749 0.0000 0.0000 \uf8f9 \uf8fa \uf8fb + N ( \uf8ee \uf8ef \uf8f0 0 0 0 0 \uf8f9 \uf8fa \uf8fb, \uf8ee \uf8ef \uf8f0 0.01 0 0 0 0 0.01 0 0 0 0 0.01 0 0 0 0 0.01 \uf8f9 \uf8fa \uf8fb),\nwhere N (\u00b5, \u03a3) \u2208 R n is a multivariate Gaussian random variable with mean \u00b5 \u2208 R n and covariance matrix \u03a3. The goal in this problem is to reach a temperature of 19.9\u2212 20.1\u00b0C in both zones within 32 discrete time steps of 15 min. As such, the goal region X G and critical region X C are:\nX G = {x \u2208 R 2 | 19.9 \u2264 T z 1 \u2264 20.1, 19.9 \u2264 T z 2 \u2264 20.1} (37) X C = \u2205.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Run times and size of model abstraction", "text": "The run times and iMDP model sizes for all iterations of the 2-zone building temperature regulation benchmark are presented in Table 1. Computing the states and actions took 5:25 min, but we omit this value from the table as this step is only performed in the first iteration. The discussion of model sizes is analogous to Appendix B.2 on the UAV benchmark.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "D Benchmarks against other tools D.1 Benchmark against StocHy", "text": "We provide a comparison between our approach and StocHy (Cauchi and Abate 2019) on a reduced version of the temperature regulation problem in Appendix C.1 with only one zone. Thus, the state vector becomes x = [T z , T r ] , and the control inputs are u = [T ac , T boil ] , which are constrained to 14 \u2264 T ac \u2264 28 and \u221210 \u2264 T boil \u2264 10 (note that T boil is now relative to the nominal boiler temperature of 75\u00b0C). Using the same dynamics as in Eq. ( 34) and ( 35), we obtain the following model in explicit form:\nx k+1 = 0.8820 0.0058 0.0134 0.9625 x k + 0.0584 0.0000 0.0000 0.0241\nu k + 0.9604 1.3269 + N ( 0 0 , 0.02 0 0 0.1 ).(38)\nThe objective in this problem is to reach a zone temperature of 20.9 \u2212 21.1\u00b0C within 64 discrete time steps of 15 min. As such, the goal region X G and critical region X C are:\nX G = {x \u2208 R 2 | 20.9 \u2264 T z \u2264 21.1}, X C = \u2205. (39)\nWe partition the continuous state space in a rectangular grid of 19 \u00d7 20 regions of width 0.2\u00b0C, which is centered at T z = 21\u00b0C and T r = 38\u00b0C. As such, the partition covers zone temperatures between 19.1 \u2212 22.9\u00b0C, and radiator temperatures between 36 \u2212 40\u00b0C.\nAbstraction method. Similar to our approach, StocHy generates robust iMDP abstractions of the model above. However, while our approach also works when the distribution of the noise is unknown, StocHy requires precise knowledge of the noise distribution. Another difference is that StocHy discretizes the control inputs of the dynamical system, to obtain a finite action space. An iMDP action is then associated with the execution of a specific value of the control input u k . In our approach, an iMDP action instead reflects an attempted transition to a given target point, and the actual control input u k (calculated using the control law Eq. ( 5)) depends on the precise continuous state x k .\nOur approach is faster. The run times and model sizes of our approach are reported in Table 1 under BAS (1-zone). For our approach, the time to compute the states and actions (needed only in the first iteration) is 0.1 s, and run times per iteration (excluding verification times) vary from 1.7 \u2212 7.2 s, depending on the value of N . Notably, the StocHy run times are orders of magnitude higher: the abstraction procedure of StocHy (excluding verification time) took 45:28 min, compared to 7.2 s for our approach with the maximum of N = 12800 samples (as reported in Table 1). We omit a comparison of the verification times, since StocHy does not leverage PRISM like our method does. These results provide a clear message: our approach is more general and significantly faster, and (as shown earlier in Fig. 6) generates results that are qualitatively similar to those obtained from StocHy.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "D.2 Benchmark against SReachTools", "text": "We benchmark our approach against the spacecraft rendezvous problem supplied with the MATLAB toolbox SReachTools (Vinod, Gleason, and Oishi 2019). In this problem, one spacecraft must dock with another within 8 time steps, while remaining in a target tube. The goal is to compute a controller that maximizes the probability to achieve this objective. The 4-dimensional state x = [p x , p x , v x , v y ] \u2208 R 4 describes the position and velocity in both directions. We adopt the same discrete-time dynamics as used in Vinod, Gleason, and Oishi (2019) and assume the same Gaussian noise (see the reference for details).\nThis problem is a finite-horizon reach-avoid problem with a rectangular goal region (target set), while the safe set is a convex tube, as also shown in Fig. 7. For our approach, we partition the state space in 3, 200 rectangular regions. It is fair to note that the safe set in SReachTools is smooth, while ours is not (due to our partition-based approach). While our current implementation is limited to regions as hyperrectangles and parallelepipeds, our method can in theory be used with all convex polytopic regions.\nReachability guarantees. We compare our method (with N = 25, . . . , 1600 samples of the noise) to the results obtained via the different methods in SReachTools. We only consider the particle and Voronoi methods of SReachTools, because only these methods are sampling-based like our approach (although only the Voronoi method can give confidence guarantees on the results). The reachability guarantees and run times are presented in Table 2, and simulated trajectories in Fig. 7. As expected, our reachability guarantees, as well as for the Voronoi method, are a lower bound on the average simulated performance (and are thus safe), while this is not the case for the particle method of SReachTools.\nComplexity and run time. As shown in Table 2, the grid-free methods of SReachTools are generally faster than our abstraction-based approach. However, we note that our method was designed for non-convex problems, which cannot be solved by SReachTools. Interestingly, the complexity of the particle (Lesser, Oishi, and Erwin 2013) and Voronoi partition method (Sartipizadeh et al. 2019) increases exponentially with the number of samples, because their optimization problems have a binary variable for every particle. By contrast, the complexity of our method increases only linearly with the number of samples, because it suffices to count the number of samples in every region, as discussed in Sect. 4.2. Controller type. The particle and Voronoi methods of SReachTools synthesize open-loop controllers, which means that they cannot act in response to observed disturbances of the state. Open-loop controllers do not consider any feedback, making such controllers unsafe in settings with significant  2010). By contrast, we derive piecewise linear feedback controllers. This structure is obtained, because our controllers are based on a state-based control policy that maps every continuous state within the same region to the same abstract action.\nRobustness against disturbances. To demonstrate the importance of feedback control, we test both methods under stronger disturbances, by increasing the covariance of the noise by a factor 10. As shown in Table 2, the particle method yields a low reachability probability (with the simulated performance being even lower), and the Voronoi method was not able to generate a solution at all. By contrast, our method still provides safe lower bound guarantees, which become tighter for increasing sample sizes. Our state-based controller is robust against the stronger noise, because it is able to act in response to observed disturbances, unlike the open-loop methods of SReachTools that results in a larger error in the simulated state trajectories, as also shown in Fig. 7.", "publication_ref": ["b55", "b55", "b49", "b10"], "figure_ref": ["fig_6", "fig_6", "fig_6"], "table_ref": ["tab_4", "tab_4", "tab_4"]}, {"heading": "Acknowledgments", "text": "We would like to thank Licio Romao for his helpful discussions related to the scenario approach and our main theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Probabilistic reachability and safety for controlled discrete time stochastic hybrid systems", "journal": "Automatica", "year": "2008", "authors": "A Abate; M Prandini; J Lygeros; S Sastry"}, {"ref_id": "b1", "title": "Discrete abstractions of hybrid systems", "journal": "Proceedings of the IEEE", "year": "2000", "authors": "R Alur; T A Henzinger; G Lafferriere; G J Pappas"}, {"ref_id": "b2", "title": "Optimal control: linear quadratic methods", "journal": "Courier Corporation", "year": "2007", "authors": "B D Anderson; J B Moore"}, {"ref_id": "b3", "title": "PAC Statistical Model Checking for Markov Decision Processes and Stochastic Games", "journal": "Springer", "year": "2019", "authors": "P Ashok; J Kret\u00ednsk\u00fd; M Weininger"}, {"ref_id": "b4", "title": "Feedback systems: an introduction for scientists and engineers", "journal": "Princeton university press", "year": "2010", "authors": "K J Astr\u00f6m; R M Murray"}, {"ref_id": "b5", "title": "Filter-Based Abstractions with Correctness Guarantees for Planning under Uncertainty", "journal": "CoRR", "year": "2021", "authors": "T S Badings; N Jansen; H A Poonawala; M Stoelinga"}, {"ref_id": "b6", "title": "Principles of model checking", "journal": "MIT Press", "year": "2008", "authors": "C Baier; J Katoen"}, {"ref_id": "b7", "title": "Hamilton-Jacobi reachability: A brief overview and recent advances", "journal": "IEEE", "year": "2017", "authors": "S Bansal; M Chen; S L Herbert; C J Tomlin"}, {"ref_id": "b8", "title": "Safe Model-based Reinforcement Learning with Stability Guarantees", "journal": "", "year": "2017", "authors": "F Berkenkamp; M Turchetta; A P Schoellig; A Krause"}, {"ref_id": "b9", "title": "", "journal": "", "year": "", "authors": "L Blackmore; M Ono; A Bektassov; B C Williams"}, {"ref_id": "b10", "title": "A Probabilistic Particle-Control Approximation of Chance-Constrained Stochastic Predictive Control", "journal": "IEEE Trans. Robotics", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Deep Reinforcement Learning Attitude Control of Fixed-Wing UAVs Using Proximal Policy optimization", "journal": "IEEE", "year": "2019", "authors": "E B\u00f8hn; E M Coates; S Moe; T A Johansen"}, {"ref_id": "b12", "title": "Concentration Inequalities -A Nonasymptotic Theory of Independence", "journal": "Oxford University Press", "year": "2013", "authors": "S Boucheron; G Lugosi; P Massart"}, {"ref_id": "b13", "title": "Convex Optimization", "journal": "Cambridge University Press", "year": "2014", "authors": "S P Boyd; L Vandenberghe"}, {"ref_id": "b14", "title": "R-MAX -A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", "journal": "J. Mach. Learn. Res", "year": "2002", "authors": "R I Brafman; M Tennenholtz"}, {"ref_id": "b15", "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning", "journal": "", "year": "2021", "authors": "L Brunke; M Greeff; A W Hall; Z Yuan; S Zhou; J Panerati; A P Schoellig"}, {"ref_id": "b16", "title": "The Exact Feasibility of Randomized Solutions of Uncertain Convex Programs", "journal": "SIAM J. Optim", "year": "2008", "authors": "M C Campi; S Garatti"}, {"ref_id": "b17", "title": "Introduction to the scenario approach", "journal": "", "year": "2018", "authors": "M C Campi; S Garatti"}, {"ref_id": "b18", "title": "Benchmarks for cyberphysical systems: A modular model library for building automation systems", "journal": "", "year": "2018", "authors": "N Cauchi; A Abate"}, {"ref_id": "b19", "title": "", "journal": "", "year": "", "authors": " Corr"}, {"ref_id": "b20", "title": "StocHy: Automated Verification and Synthesis of Stochastic Processes", "journal": "Springer", "year": "2019", "authors": "N Cauchi; A Abate"}, {"ref_id": "b21", "title": "Flow*: An Analyzer for Non-linear Hybrid Systems", "journal": "Springer", "year": "2013", "authors": "X Chen; E \u00c1brah\u00e1m; S Sankaranarayanan"}, {"ref_id": "b22", "title": "Automatic Verification of Finite-State Concurrent Systems Using Temporal Logic Specifications", "journal": "ACM Trans. Program. Lang. Syst", "year": "1986", "authors": "E M Clarke; E A Emerson; A P Sistla"}, {"ref_id": "b23", "title": "Scenario-Based Verification of Uncertain MDPs", "journal": "Springer", "year": "2020", "authors": "M Cubuktepe; N Jansen; S Junges; J Katoen; U Topcu"}, {"ref_id": "b24", "title": "A review of the statistical theory of turbulence", "journal": "Quarterly of Applied Mathematics", "year": "1943", "authors": "H L Dryden"}, {"ref_id": "b25", "title": "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations", "journal": "Math. Program", "year": "2018", "authors": "P M Esfahani; D Kuhn"}, {"ref_id": "b26", "title": "A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems", "journal": "IEEE Trans. Autom. Control", "year": "2019", "authors": "J F Fisac; A K Akametalu; M N Zeilinger; S Kaynama; J H Gillula; C J Tomlin"}, {"ref_id": "b27", "title": "Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints", "journal": "", "year": "2014", "authors": "J Fu; U Topcu"}, {"ref_id": "b28", "title": "Risk and complexity in scenario optimization", "journal": "Mathematical Programming", "year": "2019", "authors": "S Garatti; M Campi"}, {"ref_id": "b29", "title": "A comprehensive survey on safe reinforcement learning", "journal": "J. Mach. Learn. Res", "year": "2015", "authors": "J Garc\u00eda; F Fern\u00e1ndez"}, {"ref_id": "b30", "title": "Boundedparameter Markov decision processes", "journal": "Artif. Intell", "year": "2000", "authors": "R Givan; S M Leach; T L Dean"}, {"ref_id": "b31", "title": "Distributionally robust optimization and its tractable approximations", "journal": "Operations research", "year": "2010", "authors": "J Goh; M Sim"}, {"ref_id": "b32", "title": "Multi-objective Robust Strategy Synthesis for Interval Markov Decision Processes", "journal": "Springer", "year": "2017", "authors": "E M Hahn; V Hashemi; H Hermanns; M Lahijanian; A Turrini"}, {"ref_id": "b33", "title": "Probably Approximately Correct Learning", "journal": "AAAI Press / The MIT Press", "year": "1990", "authors": "D Haussler"}, {"ref_id": "b34", "title": "FaSTrack: A modular framework for fast and guaranteed safe motion planning", "journal": "IEEE", "year": "2017", "authors": "S L Herbert; M Chen; S Han; S Bansal; J F Fisac; C J Tomlin"}, {"ref_id": "b35", "title": "Cautious Model Predictive Control Using Gaussian Process Regression", "journal": "IEEE Trans. Control. Syst. Technol", "year": "2020", "authors": "L Hewing; J Kabzan; M N Zeilinger"}, {"ref_id": "b36", "title": "Near-Optimal Reinforcement Learning in Polynomial Time", "journal": "Mach. Learn", "year": "2002", "authors": "M J Kearns; S P Singh"}, {"ref_id": "b37", "title": "Learning-Based Model Predictive Control for Safe Exploration", "journal": "IEEE", "year": "2018", "authors": "T Koller; F Berkenkamp; M Turchetta; A Krause"}, {"ref_id": "b38", "title": "Dynamic modeling and control of engineering systems", "journal": "Cambridge University Press", "year": "2007", "authors": "B T Kulakowski; J F Gardner; J L Shearer"}, {"ref_id": "b39", "title": "PRISM 4.0: Verification of Probabilistic Real-Time Systems", "journal": "Springer", "year": "2011", "authors": "M Z Kwiatkowska; G Norman; D Parker"}, {"ref_id": "b40", "title": "Formal Verification and Synthesis for Discrete-Time Stochastic Systems", "journal": "IEEE Trans. Autom. Control", "year": "2015", "authors": "M Lahijanian; S B Andersson; C Belta"}, {"ref_id": "b41", "title": "Automated verification and synthesis of stochastic hybrid systems: A survey", "journal": "IEEE", "year": "2013", "authors": "A Lavaei; S Soudjani; A Abate; M Zamani; K Lesser; M M K Oishi; R S Erwin"}, {"ref_id": "b42", "title": "On the road between robust optimization and the scenario approach for chance constrained optimization problems", "journal": "IEEE Transactions on Automatic Control", "year": "2014", "authors": "K Margellos; P Goulart; J Lygeros"}, {"ref_id": "b43", "title": "Gaussian Assumption: The Least Favorable but the Most Useful", "journal": "IEEE Signal Process. Mag", "year": "2013", "authors": "S Park; E Serpedin; K A Qaraqe"}, {"ref_id": "b44", "title": "Polynomial-Time Verification of PCTL Properties of MDPs with Convex Uncertainties", "journal": "Springer", "year": "2013", "authors": "A Puggelli; W Li; A L Sangiovanni-Vincentelli; S A Seshia"}, {"ref_id": "b45", "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics", "journal": "Wiley", "year": "1994", "authors": "M L Puterman"}, {"ref_id": "b46", "title": "Feedbackmotion-planning with simulation-based LQR-trees", "journal": "Int. J. Robotics Res", "year": "2016", "authors": "P Reist; P Preiswerk; R Tedrake"}, {"ref_id": "b47", "title": "Tight generalization guarantees for the sampling and discarding approach to scenario optimization", "journal": "IEEE", "year": "2020", "authors": "L Romao; K Margellos; A Papachristodoulou"}, {"ref_id": "b48", "title": "Unified Multi-Rate Control: from Low Level Actuation to High Level Planning", "journal": "CoRR", "year": "2020", "authors": "U Rosolia; A Singletary; A D Ames"}, {"ref_id": "b49", "title": "Voronoi Partition-based Scenario Reduction for Fast Sampling-based Stochastic Reachability Computation of Linear Systems", "journal": "IEEE", "year": "2019", "authors": "H Sartipizadeh; A P Vinod; B A\u00e7ikmese; M Oishi"}, {"ref_id": "b50", "title": "ProbReach: verified probabilistic delta-reachability for stochastic hybrid systems", "journal": "ACM", "year": "2015", "authors": "F Shmarov; P Zuliani"}, {"ref_id": "b51", "title": "Sequential Monte Carlo methods in practice", "journal": "Springer Science & Business Media", "year": "2013", "authors": "A Smith"}, {"ref_id": "b52", "title": "Adaptive and Sequential Gridding Procedures for the Abstraction and Verification of Stochastic Processes", "journal": "SIAM J. Appl. Dyn. Syst", "year": "2013", "authors": "S E Z Soudjani; A Abate"}, {"ref_id": "b53", "title": "Learning for Safety-Critical Control with Control Barrier Functions", "journal": "PMLR", "year": "2020", "authors": "A J Taylor; A Singletary; Y Yue; A D Ames"}, {"ref_id": "b54", "title": "LQR-trees: Feedback motion planning on sparse randomized trees", "journal": "The MIT Press", "year": "2009", "authors": "R Tedrake"}, {"ref_id": "b55", "title": "SReachTools: a MATLAB stochastic reachability toolbox", "journal": "ACM", "year": "2019", "authors": "A P Vinod; J D Gleason; M M K Oishi"}, {"ref_id": "b56", "title": "Distributionally Robust Convex Optimization", "journal": "Oper. Res", "year": "2014", "authors": "W Wiesemann; D Kuhn; M Sim"}, {"ref_id": "b57", "title": "Robust control of uncertain Markov Decision Processes with temporal logic specifications", "journal": "IEEE", "year": "2012", "authors": "E M Wolff; U Topcu; R M Murray"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Bounds [p,p] on the probabilities P (s, a)(s j ) for 3 regions j \u2208 {1, 2, 3}, using N = 100 samples (black ticks) and \u03b2 = 0.01. The distribution over successor states is p w k (\u2022). Point estimate probabilities are computed as N in j /N .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: UAV problem (goal in green; obstacles in red), plus trajectories under the optimal iMDP-based controller from x 0 = [\u221214, 0, 6, 0, \u22126, 0] , under high and low turbulence.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Reachability guarantees on the iMDPs (blue) and MDPs (orange) for their respective policies, versus the resulting empirical (simulated) performance (dashed lines) on the dynamical system. Shaded areas show the standard deviation across 10 iterations. The empirical performance of the MDPs violates the guarantees; that of the iMDPs does not.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure5: Cross section (for radiator temp. of 38\u00b0C) of the maximum lower bound probabilities to reach the goal of 20\u00b0C from any initial state, for either 50 or 800 samples.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Simulated state trajectories for the spacecraft rendezvous problem, under low and high noise covariance. Our feedback controllers are more robust, as shown by the smaller error in the state trajectories over time (the Voronoi method under high covariance failed to generate a solution).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 10 :10Figure 10: Probability intervals (with standard deviations), obtained from Theorem 1 with \u03b2 = 0.01 or \u03b2 = 0.1 on a transition with a true probability of 0.25 (note the log scale).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Ax k +Bu k +q k +w k", "figure_data": "Partition RConfidence level 1 \u2212 \u03b2Property \u03d5 K sICompute robustLinear dynamical system M I = (S, Act, s I , P) Sample and abstract Increase sample size: N \u2190 \u03b3N Guarantees on iMDP Policy \u03c0  *  and Pr \u03c0  *  (\u03d5 K sI ) Continuous controller optimal policy Pr \u03c0  *  (\u03d5 K sI ) < \u03b7 Pr \u03c0  *  (\u03d5 K sI ) \u2265 \u03b7 Extract \u03c0  *  x k+1 = Abstract iMDP \u03c6 : X \u00d7 N \u2192 U Apply controller(and terminate)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Run times per iteration (which excludes the computation of states and actions) and model sizes for the UAV (under high turbulence), and 2-zone and 1-zone building temperature regulation (BAS) benchmarks.", "figure_data": "InstanceRun times (per iteration, in seconds)iMDP model sizeBenchmarkSamples NUpdate intervals Write iMDPModel checking StatesChoicesTransitionsUAV2511.429.039.225,5161,228,7499,477,795UAV5015.434.446.025,5161,228,74911,350,692UAV10018.639.351.825,5161,228,74913,108,282UAV20024.344.459.125,5161,228,74914,921,925UAV40035.450.668.525,5161,228,74917,203,829UAV80055.458.380.225,5161,228,74919,968,663UAV160092.264.990.625,5161,228,74922,419,161UAV3200164.168.694.425,5161,228,74923,691,107UAV6400312.169.895.525,5161,228,74923,958,183UAV12800611.269.895.125,5161,228,74923,969,028BAS (2-zone) 2513.973.6105.935,7221,611,16224,929,617BAS (2-zone) 5043.2107.7160.535,7221,611,16237,692,421BAS (2-zone) 10050.8153.2230.235,7221,611,16253,815,543BAS (2-zone) 20059.1205.8315.235,7221,611,16272,588,298BAS (2-zone) 40072.3260.6423.235,7221,611,16291,960,970BAS (2-zone) 80096.2316.8504.335,7221,611,162112,028,449BAS (2-zone) 1600132.6370.0614.835,7221,611,162131,844,146BAS (2-zone) 3200200.1432.9789.435,7221,611,162154,155,445BAS (2-zone) 6400331.8513.2982.235,7221,611,162182,175,229BAS (2-zone) 12800545.5601.81177.835,7221,611,162218,031,384BAS (1-zone) 251.620.061.393811,51120,494BAS (1-zone) 501.810.081.393811,51127,327BAS (1-zone) 1001.870.091.403811,51133,871BAS (1-zone) 2001.960.111.403811,51140,368BAS (1-zone) 4002.060.131.423811,51147,333BAS (1-zone) 8002.260.141.403811,51154,155BAS (1-zone) 16002.590.161.393811,51159,686BAS (1-zone) 32003.240.171.403811,51165,122BAS (1-zone) 64004.500.181.403811,51170,245BAS (1-zone) 128007.010.201.383811,51176,076"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Reachability probability, the average simulated reachability probability, and run times for our method and SReachTools, under the default (x1) and increased (x10) noise covariance.", "figure_data": "Our methodSReachToolsInitial abstraction N = 25N = 100N = 400N = 1600 ParticleVoronoiNoise covariance x1Reachability probability -0.440.810.950.980.880.85Simulated probability-1.001.001.001.000.830.86Run time (s)3.4206.9669.21411.01617.2940.7033.462Noise covariance x10Reachability probability -0.190.360.520.620.40NaNSimulated probability-0.590.630.590.650.19NaNRun time (s)3.37211.43315.90918.67125.4522.495NaNnoise levels (\u00c5str\u00f6m and Murray"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x k+1 = Ax k + Bu k + q k + w k ,(1)", "formula_coordinates": [3.0, 105.1, 369.82, 187.4, 9.68]}, {"formula_id": "formula_1", "formula_text": "\u03c0 * = arg max \u03c0\u2208\u03a0 M Pr \u03c0 (\u03d5 K s I ).(2)", "formula_coordinates": [3.0, 387.5, 387.97, 170.5, 19.44]}, {"formula_id": "formula_2", "formula_text": "\u03c0 * = arg max \u03c0\u2208\u03a0 M I Pr \u03c0 (\u03d5 K s I ) = arg max \u03c0\u2208\u03a0 M I min P \u2208P Pr \u03c0 (\u03d5 K s I ). (3)", "formula_coordinates": [3.0, 326.61, 565.09, 231.4, 21.43]}, {"formula_id": "formula_3", "formula_text": "M i \u2208 R m\u00d7n and b i \u2208 R m , yielding R i = x \u2208 R n | M i x \u2264 b i .", "formula_coordinates": [4.0, 54.0, 604.37, 238.5, 20.61]}, {"formula_id": "formula_4", "formula_text": "x k isx k+1 = Ax k + Bu k + q k .", "formula_coordinates": [4.0, 319.5, 240.08, 238.5, 20.64]}, {"formula_id": "formula_5", "formula_text": "G(d j ) = {x \u2208 R n | d j = Ax+Bu k +q k , u k \u2208 U}. (4)", "formula_coordinates": [4.0, 324.48, 366.77, 233.52, 11.72]}, {"formula_id": "formula_6", "formula_text": "u k = B + (d j \u2212 q k \u2212 Ax k ),(5)", "formula_coordinates": [4.0, 381.74, 557.61, 176.26, 11.72]}, {"formula_id": "formula_7", "formula_text": "P (s i , a l )(s j ) = P w k (x k+1 \u2208 R j ) = Rj p w k (x k+1 |x k+1 = d j )dx k+1 .(6)", "formula_coordinates": [5.0, 61.59, 84.32, 230.91, 38.07]}, {"formula_id": "formula_8", "formula_text": "(i) k \u2208 \u2206, i = 1, .", "formula_coordinates": [5.0, 116.11, 219.6, 67.93, 14.3]}, {"formula_id": "formula_9", "formula_text": "(i) k \u2208 \u2206, i = 1, . . . , N are i.i.d", "formula_coordinates": [5.0, 54.0, 281.91, 237.41, 22.82]}, {"formula_id": "formula_10", "formula_text": "(1) k , . . . , w (N ) k", "formula_coordinates": [5.0, 226.28, 309.83, 54.96, 14.3]}, {"formula_id": "formula_11", "formula_text": "N in j = {i \u2208 {1, . . . , N } | (x k+1 + w (i) k ) \u2208 R j } . (7)", "formula_coordinates": [5.0, 67.91, 431.52, 224.59, 14.3]}, {"formula_id": "formula_12", "formula_text": "(i) k is not contained in R j .", "formula_coordinates": [5.0, 170.86, 467.28, 102.95, 14.3]}, {"formula_id": "formula_13", "formula_text": "Definition 2. The risk P w k (x k+1 / \u2208 R j ) that a successor state x k+1 is not in region R j is P w k (x k+1 / \u2208 R j ) = P{w k \u2208 \u2206 :x k + w k / \u2208 R j } = 1 \u2212 P w k (x k+1 \u2208 R j ).(8)", "formula_coordinates": [5.0, 319.5, 57.09, 238.5, 51.71]}, {"formula_id": "formula_14", "formula_text": "P N p \u2264 P (s i , a l )(s j ) \u2264p \u2265 1 \u2212 \u03b2,(9)", "formula_coordinates": [5.0, 360.86, 370.42, 197.14, 19.5]}, {"formula_id": "formula_15", "formula_text": "\u03b2 2N = N out j i=0 N i (1 \u2212p) ipN \u2212i ,(10)", "formula_coordinates": [5.0, 376.36, 409.41, 181.64, 33.97]}, {"formula_id": "formula_16", "formula_text": "\u03b2 2N = 1 \u2212 N out j \u22121 i=0 N i (1 \u2212p) ipN \u2212i .(11)", "formula_coordinates": [5.0, 362.92, 466.66, 195.08, 33.97]}, {"formula_id": "formula_17", "formula_text": "p wk (x k+1 |x k+1 = d) \u015d x k+1 = d P wk (x k+1 \u2208 R 1 ) P wk (x k+1 \u2208 R 2 ) P wk (x k+1 \u2208 R 3 ) N", "formula_coordinates": [6.0, 56.98, 58.55, 345.36, 80.18]}, {"formula_id": "formula_18", "formula_text": "R j R j (1.2) h j x (1)", "formula_coordinates": [11.0, 159.22, 70.38, 39.74, 132.68]}, {"formula_id": "formula_19", "formula_text": "R j (\u03bb) = {x \u2208 R n | M j x \u2264 \u03bb(b j +h j ) \u2212h j }. (12", "formula_coordinates": [11.0, 68.37, 405.05, 219.98, 11.72]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [11.0, 288.35, 407.44, 4.15, 8.64]}, {"formula_id": "formula_21", "formula_text": "minimize \u03bb\u2208R+ \u03bb (13) subject tox k+1 + w (i) k \u2208 R j (\u03bb) \u2200i \u2208 {1, .", "formula_coordinates": [11.0, 62.9, 533.74, 229.6, 34.88]}, {"formula_id": "formula_22", "formula_text": "(i) k / \u2208 R j (\u03bb * |Q| ), with probability one. \u22122 \u22121 0 1 2 R j (\u03bb * N out j \u22121 ) R j (\u03bb * N out j ) R j", "formula_coordinates": [11.0, 145.62, 58.32, 397.67, 648.24]}, {"formula_id": "formula_23", "formula_text": "N out j \u22121 ) \u2283 R j .", "formula_coordinates": [11.0, 498.8, 197.25, 56.03, 13.36]}, {"formula_id": "formula_24", "formula_text": "N out j ) \u2286 R j . When discarding |Q| = N out j \u2212 1 samples, it holds that R j (\u03bb * N out j \u22121 ) \u2283 R j Proof.", "formula_coordinates": [11.0, 319.5, 442.54, 238.5, 44.77]}, {"formula_id": "formula_25", "formula_text": "N out j \u2264 1, so R j (\u03bb * N out j ) \u2286 R j .", "formula_coordinates": [11.0, 434.56, 511.43, 125.19, 14.94]}, {"formula_id": "formula_26", "formula_text": "N out j \u22121 > 1, mean- ing that R j (\u03bb * N out j \u22121 ) \u2283 R j . This concludes the proof. Intuitively, R j (\u03bb * N out j ) contains exactly those samples in R j , while R j (\u03bb * N out j \u22121 )", "formula_coordinates": [11.0, 319.14, 539.18, 240.51, 65.41]}, {"formula_id": "formula_27", "formula_text": "P N P x / \u2208 R j (\u03bb * |Q| ) \u2264 = 1 \u2212 |Q| i=0 N i i (1 \u2212 ) N \u2212i ,(14)", "formula_coordinates": [12.0, 90.18, 144.25, 202.32, 51.98]}, {"formula_id": "formula_28", "formula_text": "F |Q| ( ) = P N P x / \u2208 R j (\u03bb * |Q| ) \u2264 =\u03b2.(15)", "formula_coordinates": [12.0, 73.98, 267.32, 218.52, 12.94]}, {"formula_id": "formula_29", "formula_text": "x k+1 / \u2208 R j (\u03bb * |Q|", "formula_coordinates": [12.0, 54.0, 312.5, 66.19, 12.94]}, {"formula_id": "formula_30", "formula_text": "P N P x / \u2208 R j (\u03bb * |Q| ) \u2264 G |Q| (\u03b2) =\u03b2. (16", "formula_coordinates": [12.0, 89.17, 359.19, 199.18, 12.94]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [12.0, 288.35, 361.58, 4.15, 8.64]}, {"formula_id": "formula_32", "formula_text": "= G |Q| (\u03b2) = G |Q| F |Q| ( ) .(17)", "formula_coordinates": [12.0, 117.66, 399.98, 174.84, 9.96]}, {"formula_id": "formula_33", "formula_text": "F |Q| ( ) = P N P x \u2208 R j (\u03bb * |Q| ) \u2265 1 \u2212 =\u03b2. (18", "formula_coordinates": [12.0, 65.4, 436.89, 222.95, 12.94]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [12.0, 288.35, 439.28, 4.15, 8.64]}, {"formula_id": "formula_35", "formula_text": "P N 1 \u2212 G |Q| (\u03b2) \u2264 P x \u2208 R j (\u03bb * |Q| ) = 1 \u2212 |Q| i=0 N i (1 \u2212 p) i p N \u2212i =\u03b2.(19)", "formula_coordinates": [12.0, 91.7, 477.78, 200.8, 51.98]}, {"formula_id": "formula_36", "formula_text": "P N 1 \u2212 G0 1 \u2212 \u03b2 2N \u2264 P x \u2208 Rj(\u03bb * 0 ) =1 \u2212 \u03b2 2N . . . . . . (20", "formula_coordinates": [12.0, 57.13, 612.82, 231.64, 38.78]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [12.0, 288.76, 643.83, 3.73, 7.77]}, {"formula_id": "formula_38", "formula_text": "P N 1 \u2212 GN\u22121 1 \u2212 \u03b2 2N \u2264 P x \u2208 Rj(\u03bb * N \u22121 ) =1 \u2212 \u03b2 2N . Denote the event that 1 \u2212 G n 1 \u2212 \u03b2 2N \u2264 P x \u2208 R j (\u03bb * n ) for n = 0, . . . , N \u2212 1 by A n . Regardless of n, this event has a probability of P N {A n } = 1 \u2212 \u03b2 2N", "formula_coordinates": [12.0, 54.0, 54.75, 405.38, 650.09]}, {"formula_id": "formula_39", "formula_text": "P N N \u22121 i=0 A n \u2264 N \u22121 i=0 P N A n = \u03b2 2N N = \u03b2 2 . (21)", "formula_coordinates": [12.0, 332.87, 88.57, 225.13, 30.32]}, {"formula_id": "formula_40", "formula_text": "P N N \u22121 i=0 A n = 1 \u2212 P N N \u22121 i=0 A n \u2265 1 \u2212 \u03b2 2 .(22)", "formula_coordinates": [12.0, 333.47, 141.4, 224.53, 30.32]}, {"formula_id": "formula_41", "formula_text": "P N p \u2264 P x \u2208 R j (\u03bb * N out j ) \u2265 1 \u2212 \u03b2 2 ,(23)", "formula_coordinates": [12.0, 357.56, 237.86, 200.44, 24.19]}, {"formula_id": "formula_42", "formula_text": "otherwisep = 1 \u2212 G N out j (1 \u2212 \u03b2 2N )", "formula_coordinates": [12.0, 390.01, 277.46, 142.61, 14.36]}, {"formula_id": "formula_43", "formula_text": "1 \u2212 \u03b2 2N = 1 \u2212 N out j i=0 N i (1 \u2212 p) i p N \u2212i ,(24)", "formula_coordinates": [12.0, 358.01, 313.23, 199.99, 33.88]}, {"formula_id": "formula_44", "formula_text": "P N P x \u2208 R j (\u03bb * |Q| ) < 1 \u2212 G |Q| (\u03b2) = 1 \u2212\u03b2, (25", "formula_coordinates": [12.0, 329.22, 387.83, 224.63, 12.94]}, {"formula_id": "formula_45", "formula_text": ")", "formula_coordinates": [12.0, 553.85, 390.22, 4.15, 8.64]}, {"formula_id": "formula_46", "formula_text": "P N P x \u2208 Rj(\u03bb * 0 ) < 1 \u2212 G0 \u03b2 2N =1 \u2212 \u03b2 2N . . . . . .(26)", "formula_coordinates": [12.0, 331.84, 449.51, 226.16, 38.78]}, {"formula_id": "formula_47", "formula_text": "P N P x \u2208 Rj(\u03bb * N \u22121 ) < 1 \u2212 GN\u22121 \u03b2 2N =1 \u2212 \u03b2 2N .", "formula_coordinates": [12.0, 331.84, 493.43, 213.82, 19.74]}, {"formula_id": "formula_48", "formula_text": "P N P x \u2208 R j (\u03bb * N out j \u22121 ) \u2264p \u2265 1 \u2212 \u03b2 2 ,(27)", "formula_coordinates": [12.0, 344.16, 579.86, 213.84, 22.31]}, {"formula_id": "formula_49", "formula_text": "\u03b2 2N = 1 \u2212 N out j \u22121 i=0 N i (1 \u2212 p) i p N \u2212i ,(28)", "formula_coordinates": [12.0, 362.92, 655.22, 195.08, 33.88]}, {"formula_id": "formula_50", "formula_text": "N out j ) \u2286 R j \u2282 R j (\u03bb * N out j \u22121 ), so we have P x \u2208 R j (\u03bb * N out j ) \u2264 P (x \u2208 R j ) = P (s i , a l )(s j ) < P x \u2208 R j (\u03bb * N out j \u22121 ) .(29)", "formula_coordinates": [13.0, 64.51, 285.73, 227.99, 53.12]}, {"formula_id": "formula_51", "formula_text": "P N p \u2264 P (s i , a l )(s j ) P (s i , a l )(s j ) \u2264p = P N p \u2264 P (s i , a l )(s j ) \u2264p \u2265 1 \u2212 \u03b2,(30)", "formula_coordinates": [13.0, 71.45, 382.56, 221.05, 41.44]}, {"formula_id": "formula_52", "formula_text": "x k+1 = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0", "formula_coordinates": [13.0, 324.48, 118.94, 39.49, 37.94]}, {"formula_id": "formula_53", "formula_text": "\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb x k + \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 0.5 0 0 1 0 0 0 0.5 0 0 1 0 0 0 0.5 0 0 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb u k + w k , (31", "formula_coordinates": [13.0, 419.76, 118.94, 134.09, 45.08]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [13.0, 553.85, 137.21, 4.15, 8.64]}, {"formula_id": "formula_55", "formula_text": "x k+2 =\u0100x k +\u016a u k,k+1 + w k,k+1 ,(32)", "formula_coordinates": [13.0, 365.07, 258.62, 192.94, 9.68]}, {"formula_id": "formula_56", "formula_text": "X G = {x \u2208 R 6 | 11 \u2264 p x \u2264 15, 1 \u2264 p y \u2264 5, \u22127 \u2264 p z \u2264 \u22123}.(33)", "formula_coordinates": [13.0, 332.78, 384.52, 225.22, 25.67]}, {"formula_id": "formula_57", "formula_text": "T z 1 = 1 C 1 T z 2 \u2212 T z 1 R 1,2 + T wall \u2212 T z 1 R 1,wall(34a)", "formula_coordinates": [14.0, 81.4, 465.54, 211.1, 24.8]}, {"formula_id": "formula_58", "formula_text": "+ mC pa (T ac 1 \u2212 T z 1 ) + P out (T r 1 \u2212 T z 1 ) \u1e6a z 2 = 1 C 2 T z 1 \u2212 T z 2 R 1,2 + T wall \u2212 T z 2 R 2,wall(34b)", "formula_coordinates": [14.0, 84.45, 496.91, 208.05, 42.57]}, {"formula_id": "formula_59", "formula_text": "+ mC pa (T ac 2 \u2212 T z 2 ) + P out (T r 2 \u2212 T z 2 ) ,", "formula_coordinates": [14.0, 105.93, 546.04, 159.17, 12.69]}, {"formula_id": "formula_60", "formula_text": "T r i = k 1 (T z i \u2212 T r i ) + k 0 w(T boil i \u2212 T r i ),(35)", "formula_coordinates": [14.0, 93.25, 631.05, 199.25, 12.69]}, {"formula_id": "formula_61", "formula_text": "\uf8f9 \uf8fa \uf8fbxk + \uf8ee \uf8ef \uf8f0", "formula_coordinates": [14.0, 363.23, 471.49, 144.08, 63.28]}, {"formula_id": "formula_62", "formula_text": "\uf8f9 \uf8fa \uf8fbuk(36)", "formula_coordinates": [14.0, 490.98, 510.55, 67.02, 24.23]}, {"formula_id": "formula_63", "formula_text": "+ \uf8ee \uf8ef \uf8f0 1.2291 1.0749 0.0000 0.0000 \uf8f9 \uf8fa \uf8fb + N ( \uf8ee \uf8ef \uf8f0 0 0 0 0 \uf8f9 \uf8fa \uf8fb, \uf8ee \uf8ef \uf8f0 0.01 0 0 0 0 0.01 0 0 0 0 0.01 0 0 0 0 0.01 \uf8f9 \uf8fa \uf8fb),", "formula_coordinates": [14.0, 363.23, 549.5, 185.32, 33.29]}, {"formula_id": "formula_64", "formula_text": "X G = {x \u2208 R 2 | 19.9 \u2264 T z 1 \u2264 20.1, 19.9 \u2264 T z 2 \u2264 20.1} (37) X C = \u2205.", "formula_coordinates": [14.0, 364.11, 661.94, 193.89, 39.62]}, {"formula_id": "formula_65", "formula_text": "u k + 0.9604 1.3269 + N ( 0 0 , 0.02 0 0 0.1 ).(38)", "formula_coordinates": [15.0, 95.52, 289.39, 196.98, 41.84]}, {"formula_id": "formula_66", "formula_text": "X G = {x \u2208 R 2 | 20.9 \u2264 T z \u2264 21.1}, X C = \u2205. (39)", "formula_coordinates": [15.0, 64.46, 378.72, 228.04, 11.72]}], "doi": ""}