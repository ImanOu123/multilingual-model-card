{"title": "Causes and Cures for Interference in Multilingual Translation", "authors": "Uri Shaham; Maha Elbayad; Vedanuj Goswami; Omer Levy; Shruti Bhosale", "pub_date": "", "abstract": "Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancing the amount of interference between low and high resource language pairs effectively, and can lead to superior performance overall.", "sections": [{"heading": "Introduction", "text": "Multilingual machine translation models can benefit from transfer between different language pairs (synergy), but may also suffer from interference (Ha et al., 2016;Firat et al., 2016;Aharoni et al., 2019;Arivazhagan et al., 2019). While there are methods to reduce interference and achieve better performance (Wang et al., 2020a;Kreutzer et al., 2021;, such approaches are often compute intensive, and do not always work (Xin et al., 2022). In this work, we demonstrate that interference in multilingual translation largely occurs when the model is very small compared to the abundance of training data, and that the simple principled approach of enlarging the model and tuning the data sampling temperature provides a consistent solution to the interference problem that can even promote synergy.\nThis work methodically deduces the most simple ways of reducing interference in multilingual translation. We begin by inquiring what are the dominant factors that may interfere with learning to translate a particular language pair of focus s \u2192 t, in the context of learning a multilingual translation model with many different language pairs. Controlled experiments show that besides model size and number of s \u2192 t training examples, the main factor that correlates with the level of interference is the proportion of focus pair examples (s \u2192 t) observed out of the total number of examples (all language pairs) seen at each training step on average. Surprisingly, aspects like language similarity or number of translation directions have a much smaller effect.\nIn model and data scaling experiments, we observe that interference mainly occurs in extreme parameter poverty, when the language pair of focus is data-rich, but has to \"share\" a crowded parameter space with large quantities of other data. Enlarging the model to standard model sizes in machine translation literature alleviates interference and even facilitates synergy. For context, given a language pair of 15M sentence pairs that accounts for 20% of the total training data (75M), we observe severe levels of interference with 11M-and 44M-parameter transformers, but no interference when scaling the model to 176M parameters (the \"big\" model of Vaswani et al. (2017)) and significant synergy with 705M parameters. Interestingly, when the model is large enough, we find that increasing the amount of non-focus data to a certain point can further increase synergy.\nFinally, given the evidence that data sizes and ratios strongly correlate with interference, we experiment with a natural lever that controls the proportion of each dataset in the overall mix in the simplest way: sampling temperature. Indeed, we find that calibrating the distribution of language pairs via temperature can substantially reduce the amount of interference in both high-and lowresource language pairs. Our results demonstrate the importance of tuning the temperature hyperparameter in multitask training, and suggest that previously reported accounts of severe interference in multilingual translation models might stem from suboptimal hyperparameter configurations.", "publication_ref": ["b12", "b7", "b0", "b1", "b29", "b16", "b32", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Measuring Interference", "text": "We assume a common multilingual translation setup that involves L language pairs s \u2192 t, where the source is always the same language s (English), and the target language t varies (English-to-many), or vice versa (many-to-English). The overall training data is a union of these training subsets, we note their sizes by D s\u2192t . Sampling a training example x follows the distribution:\nP (x \u2208 s \u2192 t) \u221d D s\u2192t s \u2032 ,t \u2032 D s \u2032 \u2192t \u2032 1 T (1)\nWhere T is the temperature hyperparameter (Devlin et al., 2019;Arivazhagan et al., 2019). T = 1 maintains the original data proportions, 0 < T < 1 starves low resource language pairs, and T > 1 increases their representation in the training distribution. We mostly focus on the English-to-many setting in which interference is more apparent. 1 We define interference as a negative interaction between different translation directions in a multilingual translation model. It is measured for a specific translation direction s \u2192 t by the relative difference in performance (test-set cross-entropy loss) between a bilingual model trained to translate only from s to t (L bi s\u2192t ) and a multilingual counterpart that is trained to translate other additional directions (L multi s\u2192t ):\nI s\u2192t = L bi s\u2192t \u2212 L multi s\u2192t L bi s\u2192t (2)\nNegative values of I s\u2192t indicate interference, while positive values indicate synergy.", "publication_ref": ["b4", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Models We train encoder-decoder Transformer (Vaswani et al., 2017)   throughout our experiments. We use the original 2 transformer-base and transformer-big variants, as well as a smaller and a larger versions by adjusting the width of the architecture (Table 1). Tokenization We build a shared vocabulary of 64K BPE tokens with sentencepiece (Kudo and Richardson, 2018) using a sampling temperature of 5 to increase the lower resource languages' representation. We use this vocabulary for all our experiments. We also add language ID tokens to our vocabulary, which are prepended to each source and target sequence to indicate the target language (Johnson et al., 2017).\nTraining We use Fairseq (Ott et al., 2019) to train transformer models with the Adam optimizer (Kingma and Ba, 2015) for up to 100K steps, with a dropout rate of 0.1, inverse square root learning rate schedule up to a maximum of 0.004, 8K warmup steps, and a batch size of 256K tokens. We choose the best checkpoint according to the average validation loss of all language pairs.   In the experiments we describe next, we provide empirical evidence that indicate the last two factors do not actually have a significant effect on the level of interference, and can therefore be pruned away. Subsequent experiments reveal that interference is indeed a function of model size, data size, and data proportion. Most striking is the fact that, across various data settings, enlarging the model to standard sizes consistently alleviates interference and may even promote synergy.", "publication_ref": ["b28", "b17", "b14", "b23", "b15"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Does Language Similarity Matter?", "text": "Intuitively, data from languages that humans perceive as similar (e.g. languages that have some degree of mutual intelligibility, exhibit similar linguistic properties, or have shared vocabularies) should have a more positive effect on translation quality comparing to data from distinct languages (Lin et al., 2019;Wang et al., 2020b). To test this, we fix a focus language, and train trilingual models to translate from English to two languages, the focus language and an additional interfering language. We then look at interference trends as we vary the   3 provides an overview of the language similarity experiments.\nResults Figure 1a shows the interference rate for every model size when Spanish has only 118K parallel examples (left) and when using the full English-Spanish dataset (right). The variance in results somewhat correlates with language similarity when the dataset is very small, which aligns with previous work (Lin et al., 2019); French seems to help Spanish more than other languages when the model is big enough, while Chinese helps less. However, when training with the full dataset, the differences between other languages diminish for all model sizes. Concurrently, Fernandes et al. (2023) also found no significant difference for using French or Chinese as a third language combined with English-German in a very high resource  We observe similar trends when Estonian is the focus language. Figure 1b shows that when Estonian only has 118K training examples, combining with Finnish data seems to have some positive effect. However, this effect also shrinks when using all of the English-Estonian train set (only 2.2M examples, compared to the 15.2M of English-Spanish) and a model that is not too small. 8", "publication_ref": ["b20", "b30", "b20"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_6"]}, {"heading": "Does the Number of Languages Matter?", "text": "Do we get more interference when training with one interfering language pair or fourteen? We train models with varying numbers of language pairs while controlling for the overall number of interfering examples. We find that splitting the interfering data across more language pairs has a mild positive effect, which diminishes as the amount of focuslanguage data and/or model parameters scales up.   up to a fixed 15.2M examples budget, distributed as evenly as possible among the different languages. 9 We repeat these experiments when Estonian is the focus language and the interfering example budget is 6.6M. Table 4 provides an overview of these experiments.\nResults Figure 2a shows that more than one interfering language pair somewhat helps when English-Spanish has few training examples, but this effect largely disappears in the full training set and with larger models. We see similar trends for Estonian in Figure 2b, even though its full training set has only 2.2M examples. This phenomenon might be related to the fact that when the data distribution is sharp (i.e. one high resource paired with one very low resource) there is not enough incentive for the model to pay attention to the focus language's identifier token, compared to when the distribution is much more uniform. This result also corroborates similar findings for pretrained multilingual models (Conneau et al., 2020), although those experiments did not control the total quantity of data as in ours. 10", "publication_ref": ["b3"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_9"]}, {"heading": "The Impact of Model and Data Size", "text": "Seeing that language similarity and the number of interfering languages have only a limited effect on interference, we design a controlled setup to measure interference as a function of the remaining three factors: model size, focus language data size, and its proportion in the total amount of data seen during training.\nSetup We train models using all the available 15.2M English-Spanish examples, with an increasing example budget for interfering language pairs, ranging from 1/8 (1.9M) to 8 times (122M) the English-Spanish data, divided as evenly as possible between French, Czech, Russian, and Chinese. 11 To observe trends across D s\u2192t sizes, we  rerun these experiments with a quarter (3.8M) of the English-Spanish data, while keeping the ratios with the rest of the data similar. Finally, we also conduct these experiments in the many-to-English setting.\nResults  3c and 3d show that when translating into English, interference is much less of an issue, occurring only in the XS model when the total amount of training data significantly exceeds the model's capacity. Scaling up the model not only improves the absolute performance (Appendix A), but also introduces substantial gains from synergy. Our results align with trends observed on cross lingual transfer when scaling pretrained multilingual models to 3.5 and 10 billion parameters (Goyal et al., 2021).", "publication_ref": ["b11"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Tuning Interference with Temperature", "text": "In the previous sections we demonstrated that the dominant factors impacting interference are the model size, the amount of focus language pair data  3) discarding 4 low resource languages (Latvian, Lithuanian, Romanian and Hindi). When illustrating the results, we assign languages to high and low resource according to whether their relative data proportion decreases or increases when going from T = 1 to T = 2.\nD\nResults Figure 4 shows the trade-offs between the lower and higher resource languages, as defined above. First, we can see a clear trade-off for the smaller models (XS and S) from T = 1 to T = 4 in most cases. Increasing T helps promote synergy for low resource languages at the cost of increasing interference for the high resource languages. However, the larger models (M and L) clearly degrade when using T \u2265 3; in fact, values of T = 1 and T = 2 are often better for high-and low-resource language pairs than the commonlyused T = 5. These results align with recent work Xin et al. (2022) showing that tuned scalarization is key to achieving strong bilingual baselines that often outperform more complicated multitask optimization methods. 12", "publication_ref": ["b32"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Related Work", "text": "Scaling Laws in Machine Translation Previous work also looked at scaling trends of data and 12 See Table 5 in Appendix A for the results of these experiments with absolute BLEU scores. 40% 30% 20% 10% 0% 10% 20% 30% 40% Average interference in low resource: es,fi, de,et,lv,lt,ro,hi,kk,tr,gu  models sizes for machine translation. Gordon et al. (2021) proposed scaling laws in the data and model parameters and demonstrated their ability to predict the validation loss of bilingual translation models from Russian, Chinese, and German to English. Ghorbani et al. (2022) found scaling laws for different configurations for the encoder and decoder, independently varying the number of layers in each of them. Bansal et al. (2022) examined different architectures and described data size scaling laws for machine translation in a large scale for English to German and English to Chinese. While all of these works focused on the bilingual setting, we unveil trends for multilingual translation, which has increased complexity. Concurrently to our work, Fernandes et al. (2023) proposed scaling laws for multilingual machine translation, focusing on trilingual models trained on English-German with English-Chinese or French\nMultitask Methods for Multilingual Machine Translation Multitask methods have been proposed extensively to enhance the performance of multilingual translation models. Some utilize validation based signals to determine which language pairs should be prioritized throughout training, either with adaptive scheduling , gradient similarities to the validation set Wang et al. (2020a), or a multi-armed bandits model (Kreutzer et al., 2021). Zhu et al. (2021) added dedicated embedding and layer adapter modules to the Transformer, and Lin et al. (2021) suggested learning a binary mask for every model parameter and every language pair, both requiring further training after the base multilingual model converges. Li and Gong (2021) used per language gradients geometry to rescale gradients of different language pair to improve performance on low resource languages.  extended PCGrad (Yu et al., 2020) to create Gradient Vaccine, a method that attempts to deconflict different language pairs gradients by replacing them with more similar vectors in terms of cosine similarity. While the motivation for these methods is clear and intuitive, they are usually more complex and computationally expensive than the baseline. Moreover, their efficacy is often demonstrated using relatively small 13 models, while modestly increasing the model size can both strengthen the bilingual baselines and reduce the interference problem significantly.\nCritical Takes on Multitask Optimization Methods Multitask optimization methods were recently under scrutiny. Kurin et al. (2022) experimented with many of those for image classification and reinforcement learning problems, and found that none of them consistently outperformed a well tuned baseline with proper use of known regular- 13 Transformer-base or big from Vaswani et al. (2017).\nization techniques. Similarly, Xin et al. (2022) showed that despite their increased complexity, no popular multitask method was superior to a sweep over scalarization weights for a baseline trilingual translation model. This work complements this line of research by examining multilingual translation models and how can modest scale and calibrated temperature reduce problems associated with multitasking.", "publication_ref": ["b10", "b32", "b29", "b16", "b34", "b21", "b19", "b33", "b18", "b28", "b32"], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "Conclusion", "text": "This work examines the dominant factors that influence interference in multilingual machine translation. Namely, the model size, the amount of parallel data for the focus language pair, and the proportion of examples from the focus language pair with respect to the total data seen during training. While specialized multitask techniques are sometimes demonstrated on small transformer models, we find that a standard baseline model of 176M parameters reduces the interference problem significantly, and further scaling up results in synergy among the different language pairs. We further demonstrate the importance of tuning the temperature at which different language pairs are sampled during training; while existing literature largely relies on high temperatures, which indeed improve low-resource performance in parameter-poor settings, larger models benefit from a more natural distribution that reflects the raw training data. These simple strategies for addressing interference call into question the necessity and perhaps even the validity of recentlyproposed complex anti-interference methods and reaffirm the tried-and-true method of increasing model capacity to accommodate for higher data diversity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "One limitation of this work is the focus on Englishto-many and many-to-English settings, while previous studies also went beyond English-centric translation (Freitag and Firat, 2020;Fan et al., 2022). Second, we experiment with a WMT based benchmark that has a total of 15 languages and 200M training examples, when translation models were also trained on larger datasets (Aharoni et al., 2019;Arivazhagan et al., 2019;NLLB Team et al., 2022).\nWe leave questions about the amount of scale that will be required to effectively mitigate interference in massively (many-to-many, billions of parallel sequences) multilingual settings for future work.\nAdditionally, the data collected from high resource languages may be of higher quality compared to that collected from low resource languages. Further research is needed to determine the impact of low quality training data on interference and synergy. Finally, while we explore trends when scaling models width, deeper models (Ghorbani et al., 2022) might help mitigating interference even further.       The WMT data used in our experiment is a common machine translation dataset and is publicly available research purposes.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nThe usage was consistant with the artifacts intended use.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\nThe WMT data used in our experiment is a common machine translation dataset and is publicly available research purposes.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Mostly languages in section 3. Regarding the rest, adding justification from above: The WMT data used in our experiment is a common machine translation dataset and is publicly available research purposes.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. 3", "publication_ref": ["b8", "b5", "b0", "b1", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research is supported by the Yandex Initiative in Machine Learning. We thank Maor Ivgi, Yilin Yang, Jean Maillard, and Ves Stoyanov for their valuable feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A BLEU Scores", "text": "Throughout the paper we calculate interference in terms of test loss values. We additionally provide the test BLEU scores achieved by our models. We generate using beam search with 5 beams, without length penalty. We use SacreBLEU (Post, 2018) to calculate test sets BLEU (Papineni et al., 2002) scores. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.", "publication_ref": ["b25", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Language similarities", "text": "D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Massively multilingual neural machine translation", "journal": "Long and Short Papers", "year": "2019", "authors": "Roee Aharoni; Melvin Johnson; Orhan Firat"}, {"ref_id": "b1", "title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "ArXiv", "year": "2019", "authors": "N Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George F Foster; Colin Cherry; Wolfgang Macherey; Z Chen; Yonghui Wu"}, {"ref_id": "b2", "title": "Behnam Neyshabur, and Orhan Firat. 2022. Data scaling laws in NMT: The effect of noise and architecture", "journal": "PMLR", "year": "", "authors": "Yamini Bansal; Behrooz Ghorbani; Ankush Garg; Biao Zhang; Colin Cherry"}, {"ref_id": "b3", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b4", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "Beyond english-centric multilingual machine translation", "journal": "J. Mach. Learn. Res", "year": "2022", "authors": "Angela Fan; Shruti Bhosale; Holger Schwenk; Zhiyi Ma; Ahmed El-Kishky; Siddharth Goyal; Mandeep Baines; Onur Celebi; Guillaume Wenzek; Vishrav Chaudhary; Naman Goyal; Tom Birch; Vitaliy Liptchinsky; Sergey Edunov; Edouard Grave; Michael Auli; Armand Joulin"}, {"ref_id": "b6", "title": "Markus Freitag, and Orhan Firat. 2023. Scaling laws for multilingual neural machine translation", "journal": "", "year": "", "authors": "Patrick Fernandes; Behrooz Ghorbani; Xavier Garcia"}, {"ref_id": "b7", "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Orhan Firat; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b8", "title": "Complete multilingual neural machine translation", "journal": "", "year": "2020", "authors": "Markus Freitag; Orhan Firat"}, {"ref_id": "b9", "title": "Ciprian Chelba, and Colin Cherry. 2022. Scaling laws for neural machine translation", "journal": "", "year": "", "authors": "Behrooz Ghorbani; Orhan Firat; Markus Freitag; Ankur Bapna; Maxim Krikun; Xavier Garcia"}, {"ref_id": "b10", "title": "Data and parameter scaling laws for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Kevin Mitchell A Gordon; Jared Duh;  Kaplan"}, {"ref_id": "b11", "title": "Larger-scale transformers for multilingual masked language modeling", "journal": "", "year": "2021", "authors": "Naman Goyal; Jingfei Du; Myle Ott"}, {"ref_id": "b12", "title": "Toward multilingual neural machine translation with universal encoder and decoder", "journal": "", "year": "2016-01", "authors": "Thanh-Le Ha"}, {"ref_id": "b13", "title": "Adaptive scheduling for multi-task learning", "journal": "ArXiv", "year": "2019", "authors": "S\u00e9bastien Jean; Orhan Firat; Melvin Johnson"}, {"ref_id": "b14", "title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"ref_id": "b15", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b16", "title": "Bandits don't follow rules: Balancing multi-facet machine translation with multi-armed bandits", "journal": "", "year": "2021", "authors": "Julia Kreutzer; David Vilar; Artem Sokolov"}, {"ref_id": "b17", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"ref_id": "b18", "title": "In defense of the unitary scalarization for deep multi-task learning", "journal": "", "year": "2022", "authors": "Alessandro De Vitaly Kurin; Ilya Palma; Shimon Kostrikov; M. Pawan Whiteson;  Kumar"}, {"ref_id": "b19", "title": "Robust optimization for multilingual translation with imbalanced data", "journal": "", "year": "2021", "authors": "Xian Li; Hongyu Gong"}, {"ref_id": "b20", "title": "Antonios Anastasopoulos, Patrick Littell, and Graham Neubig", "journal": "", "year": "2019", "authors": "Yu-Hsiang Lin; Chian-Yu Chen; Jean Lee; Zirui Li; Yuyan Zhang; Mengzhou Xia; Shruti Rijhwani; Junxian He; Zhisong Zhang; Xuezhe Ma"}, {"ref_id": "b21", "title": "Learning language specific sub-network for multilingual machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Zehui Lin; Liwei Wu; Mingxuan Wang; Lei Li"}, {"ref_id": "b22", "title": "", "journal": "Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov", "year": "", "authors": "Marta R Nllb Team; James Costa-Juss\u00e0; Onur Cross; Maha \u00c7elebi; Kenneth Elbayad; Kevin Heafield; Elahe Heffernan; Janice Kalbassi; Daniel Lam; Jean Licht; Anna Maillard; Skyler Sun; Guillaume Wang; Al Wenzek; Bapi Youngblood; Loic Akula; Gabriel Mejia Barrault; Prangthip Gonzalez; John Hansanti; Semarley Hoffman;  Jarrett"}, {"ref_id": "b23", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b24", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b25", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b26", "title": "Using the output embedding to improve language models", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Ofir Press; Lior Wolf"}, {"ref_id": "b27", "title": "Leveraging monolingual data with self-supervision for multilingual neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Aditya Siddhant; Ankur Bapna; Yuan Cao; Orhan Firat; Mia Chen; Sneha Kudugunta; Naveen Arivazhagan; Yonghui Wu"}, {"ref_id": "b28", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b29", "title": "Balancing training for multilingual neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Xinyi Wang; Yulia Tsvetkov; Graham Neubig"}, {"ref_id": "b30", "title": "On negative interference in multilingual models: Findings and a meta-learning treatment", "journal": "", "year": "2020", "authors": "Zirui Wang; Zachary C Lipton; Yulia Tsvetkov"}, {"ref_id": "b31", "title": "Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models", "journal": "", "year": "2021-05-03", "authors": "Zirui Wang; Yulia Tsvetkov; Orhan Firat; Yuan Cao"}, {"ref_id": "b32", "title": "Do current multi-task optimization methods in deep learning even help?", "journal": "", "year": "2022", "authors": "Derrick Xin; Behrooz Ghorbani; Justin Gilmer; Ankush Garg; Orhan Firat"}, {"ref_id": "b33", "title": "Gradient surgery for multi-task learning", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Tianhe Yu; Saurabh Kumar; Abhishek Gupta; Sergey Levine; Karol Hausman; Chelsea Finn"}, {"ref_id": "b34", "title": "Counter-interference adapter for multilingual machine translation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Yaoming Zhu; Jiangtao Feng; Chengqi Zhao; Mingxuan Wang; Lei Li"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "( 1 )1Model size (2) Training data size of s \u2192 t, D s\u2192t (3) Proportion of s \u2192 t examples observed during training P (x \u2208 s \u2192 t) (4) Total number of languages L (5) Similarity between s \u2192 t and other pairs 4", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Interference of models trained with en-es (a) or en-et (b) as low resource languages (left) and using their full training sets (right) together with one other language. Positive values indicate synergy, i.e. the focus language (es/et) loss of a trilingual model is lower (better) compared to its bilingual model baseline. Similarly, negative values indicate interference.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: en-es (a) and en-et (b) test interference of models trained with es (a) or et (b) as low resource languages (left) and using their full train sets (right) together with increasing number of languages, sharing a fixed budget of training examples. Positive values indicate synergy, i.e the focus language (es/et) loss of a multilingual model is lower (better) comparing to its bilingual model baseline. Similarly, negative values indicate interference.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Interference of en-es (top) and es-en (bottom) models trained using the full 15.2M en-es train set (left), and a sample of 3.8M en-es (right). Positive values indicate synergy, i.e. en-es or es-en loss of a multilingual model is lower (better) comparing to its bilingual model baseline. Similarly, negative values indicate interference.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Average interference/synergy of high (proportion declining when incrementing T ) and low (proportion ascending when incrementing T ) resource languages of different model sizes (colors) for different training distributions (a,b,c) using T values ranging from 1 to 5 (numbers on markers). Positive values indicate synergy and negative values indicate interference.", "figure_data": ""}, {"figure_label": "b", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "( b )bModels trained with 118K (left) and 2.2M (right) en-et training examples and 6.6M training examples for non-et languages.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :5Figure 5: en-es (a) and en-et (b) test BLEU scores of models trained with es or et as low resource languages and using their full train sets together with one other en-xx pair.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure 6: en-es (a) and en-et (b) test BLEU scores of models trained with es or et as low resource languages and using their full train sets together with increasing number of languages, sharing a fixed budget of training examples.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Model sizes used in our experiments. Each model has 6 encoder and 6 decoder layers. We exclude the embeddings from the parameters count.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Languages from the WMT-based benchmark of Siddhant et al. (2020), along with the number of sentence pairs in the training set, and the source of the test set. All languages are paired with English (en)."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": Trilingual models for experiments on the im-pact of language similarity on interference. The most similar language to the focus language is noted with \u22c6.interfering language while controlling the amountof training data for each language pair.Setup We run two sets of experiments, one with Spanish (es, 15.2M parallel sentences) as the focuslanguage, and another with Estonian (et, 2.2M ex-"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Multilingual models for experiments on the impact of the number of other languages on interference. The trilingual model results are the average per focus language from Table 3. Models trained with 118K (left) and 15.2M (right) en-es training examples and 15.2M training examples for non-es languages. Models trained with 118K (left) and 2.2M (right) en-et training examples and 6.6M training examples for non-et languages.", "figure_data": "Setup We train multilingual models on English-Spanish data alongside English to 1, 4, or 14 inter-fering languages. The interfering data always sums"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "In a practical situation where both model size and multilingual data are fixed, how can one control the level of interference? Recalling Equation 1, we observe that the proportion of focus pair examples P (x \u2208 s \u2192 t) is controlled via the temperature hyperparameter T . Although previous literature has", "figure_data": "largely used a value of T = 5 following Arivazha-gan et al. (2019), our systematic experiments withdifferent temperatures across three different datadistributions and four model sizes suggest that thisvalue can be sub-optimal and induce a substantialamount of interference, especially for model sizesthat alleviate significant amounts of interference (Mand L). Conversely, tuning the temperature showsthat lower values (T = 1, 2) are typically able toreduce high-resource interference without harminglow-resource synergy in our standard multilingualtranslation setting.Setup We train models of four sizes with temper-ature ranging from 1 to 5 on three training distri-butions: (1) all available training data, (2) discard-ing 3 high resource languages (Czech, French andRussian), ("}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Test BLEU scores across four model sizes of bilingual baselines (bi) and multilingual models trained with temperature values T \u2208 [1, 5]. A1. Did you describe the limitations of your work? Section 7 A2. Did you discuss any potential risks of your work? Our work does not add new risks involving translation models A3. Do the abstract and introduction summarize the paper's main claims? B1. Did you cite the creators of artifacts you used? 3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?", "figure_data": "15861"}], "formulas": [{"formula_id": "formula_0", "formula_text": "P (x \u2208 s \u2192 t) \u221d D s\u2192t s \u2032 ,t \u2032 D s \u2032 \u2192t \u2032 1 T (1)", "formula_coordinates": [2.0, 104.66, 342.47, 185.21, 41.42]}, {"formula_id": "formula_1", "formula_text": "I s\u2192t = L bi s\u2192t \u2212 L multi s\u2192t L bi s\u2192t (2)", "formula_coordinates": [2.0, 129.76, 619.58, 160.11, 37.22]}, {"formula_id": "formula_2", "formula_text": "D", "formula_coordinates": [6.0, 306.14, 749.31, 9.03, 10.91]}], "doi": "10.18653/v1/N19-1388"}