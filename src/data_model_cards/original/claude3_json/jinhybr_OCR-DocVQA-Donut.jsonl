[
  {
    "question": "summary",
    "chain": "generation",
    "answer": "Donut is an OCR-free end-to-end Transformer model for document understanding that achieves state-of-the-art performance on various visual document understanding tasks such as document classification and information extraction. It is a self-contained model with a simple architecture consisting of a visual encoder and textual decoder, and does not rely on any OCR-related modules."
  },
  {
    "question": "description",
    "chain": "generation",
    "answer": "Model: jinhybr/OCR-DocVQA-Donut\n\nModel Architecture:\n- Donut is an end-to-end VDU (Visual Document Understanding) model for general understanding of document images.\n- It consists of a Transformer-based visual encoder and textual decoder.\n- The visual encoder converts the input document image into a set of embeddings. Swin Transformer is used as the encoder network.\n- The textual decoder maps the derived features into a sequence of subword tokens to construct the desired structured output.\n\nTraining Procedures:\n- The model is trained as a visual language model over the visual corpora (document images).\n- The objective is to minimize cross-entropy loss of next token prediction by jointly conditioning on the image and previous contexts.\n- In the application stage (fine-tuning), the model is taught to understand the document image by interpreting all downstream tasks as a JSON prediction problem.\n- The decoder is trained to generate a token sequence that can be converted into a JSON representing the desired output information.\n\nParameters:\n[More Information Needed]\n\nImportant Disclaimers:\n- Donut does not rely on any modules related to OCR functionality, but uses a visual encoder for extracting features from a given document image.\n- The model goes beyond the traditional OCR-based framework by modeling a direct mapping from a raw input image to the desired output.\n- Donut addresses issues induced by OCR-dependency, such as the cost of building an OCR system, inflexibility of OCR models on languages or types of documents, and OCR error propagation.\n\nFor more information or any questions, please contact the project organizer at [More Information Needed]."
  },
  {
    "question": "funded_by",
    "chain": "generation",
    "answer": "Based on the provided references, there is no clear information about the specific people or organizations that funded the jinhybr/OCR-DocVQA-Donut model project. The references mention the model and provide some technical details, but do not specify funding sources.\n\n[More Information Needed]\n\nTo properly answer this question for the model card, more details would need to be provided about any funding, grants, or sponsorships that supported the development of the jinhybr/OCR-DocVQA-Donut model. The project organizer should seek out and include that information if available."
  },
  {
    "question": "shared_by",
    "chain": "generation",
    "answer": "Based on the provided references, the main contributors that made the model jinhybr/OCR-DocVQA-Donut available online as a GitHub repo are:\n\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park from NAVER Corp (including Naver Clova, Naver Search, and Naver AI Lab teams).\n\nThe model code, weights, synthetic data and generator were first released on 2022-07-20.\n\nAdditional contributions:\n- @estaudere published a hands-on tutorial on the Donut model on Towards Data Science on 2022-08-05.\n- @NielsRogge made the Donut model available on Hugging Face Transformers on 2022-08-12.\n\n[More Information Needed] on the specific roles and contributions of the individual authors from NAVER Corp."
  },
  {
    "question": "model_type",
    "chain": "generation",
    "answer": "The jinhybr/OCR-DocVQA-Donut model is an end-to-end document understanding transformer model that is trained on visual corpora using self-supervised learning to generate structured text outputs from document images without relying on external OCR engines."
  },
  {
    "question": "language",
    "chain": "generation",
    "answer": "Based on the provided references, the Donut model is an OCR-free end-to-end transformer model for document understanding, but there is no specific information about the natural human language(s) it uses or processes. [More Information Needed]"
  },
  {
    "question": "license",
    "chain": "generation",
    "answer": "The model jinhybr/OCR-DocVQA-Donut is using the MIT License, as stated in the references:\n\n\"MIT license\n\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\"\n\nThe full text of the MIT License is provided in the references."
  },
  {
    "question": "finetuned_from",
    "chain": "generation",
    "answer": "Based on the information provided in the references, the model jinhybr/OCR-DocVQA-Donut is likely fine-tuned from the donut-base model, which is linked here:\n\n[`donut-base`](https://huggingface.co/naver-clova-ix/donut-base/tree/official)\n\nThe references mention that the DocVQA task was fine-tuned using the donut-base model:\n\n\"| [DocVQA Task1](https://rrc.cvc.uab.es/?ch=17) (Document VQA) |  0.78       | 67.5 | [donut-base-finetuned-docvqa](https://huggingface.co/naver-clova-ix/donut-base-finetuned-docvqa/tree/official) |\"\n\nSo the jinhybr/OCR-DocVQA-Donut model is probably a further fine-tuned version of the donut-base-finetuned-docvqa model, which itself was fine-tuned from the original donut-base model."
  },
  {
    "question": "demo_sources",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain a direct link to a demo for the specific model \"jinhybr/OCR-DocVQA-Donut\". The references mention demos for other Donut models like \"donut-base-finetuned-docvqa\" and \"donut-base-finetuned-cord-v2\", but not for the requested model. More information would be needed to determine if a demo exists for \"jinhybr/OCR-DocVQA-Donut\"."
  },
  {
    "question": "direct_use",
    "chain": "generation",
    "answer": "The model jinhybr/OCR-DocVQA-Donut can be used without fine-tuning, post-processing, or plugging into a pipeline in the following way:\n\nDonut is an end-to-end model that jointly learns visual and textual information from document images. It takes a document image as input and directly generates the desired output in a structured format, such as JSON. \n\nFor example, for document classification, Donut can generate a JSON containing the predicted class:\n\n{ \"class\":\"receipt\" }\n\nFor document visual question answering (DocVQA), Donut can take the question as a prompt and generate the answer:\n\n{ \"question\": \"what is the price of choco mochi?\", \"answer\": \"14,000\" }\n\nThis allows Donut to be used directly for inference on downstream tasks without the need for additional fine-tuning, post-processing steps like OCR, or integration into a pipeline. The uniformity of the task-solving method (generating structured output) makes Donut versatile and easy to use.\n\n[More Information Needed] for a specific code snippet demonstrating the usage.\n\nIn summary, the end-to-end nature and structured output generation capability of Donut enables it to be used directly for various document understanding tasks without relying on external components or task-specific fine-tuning."
  },
  {
    "question": "downstream_use",
    "chain": "generation",
    "answer": "The Donut model (jinhybr/OCR-DocVQA-Donut) can be fine-tuned for various visual document understanding tasks such as document classification, information extraction, and visual question answering. When fine-tuned, the model learns to generate a JSON output that represents the desired task-specific information.\n\nFor example, in the document classification task, the model is trained to generate a JSON like {\"class\": \"memo\"} to classify the document type. This allows the model to maintain a uniform task-solving approach across different downstream applications.\n\nWhen plugged into a larger ecosystem or app, Donut can be used as an OCR-free document understanding component. It can process document images directly, without relying on external OCR engines, and provide the extracted information in a structured JSON format. This makes it easier to integrate Donut into existing workflows and pipelines.\n\n[More Information Needed] for a specific code snippet demonstrating the fine-tuning or integration process.\n\nOverall, Donut's ability to handle various document understanding tasks without OCR dependencies, combined with its state-of-the-art performance and efficient inference, makes it a valuable asset for applications dealing with visual documents."
  },
  {
    "question": "out_of_scope_use",
    "chain": "generation",
    "answer": "Based on the provided references, here are some potential ways the Donut model (jinhybr/OCR-DocVQA-Donut) could foreseeably be misused and what users ought not do:\n\n- The model should not be used on sensitive, private or confidential documents without proper authorization, as it could extract information that is not intended to be shared. The references do not provide details on what safeguards are in place to protect private data.\n\n- Users should be cautious about relying on the model outputs for high-stakes decisions or applications where errors could have serious consequences. While the model achieves competitive performance, the references note it is not perfect and can make mistakes, especially on challenging inputs like handwritten text. \n\n- The model should not be represented as being an authoritative OCR tool or replacement for dedicated OCR engines in all scenarios. The references position it as an OCR-free alternative for certain visual document understanding tasks, but traditional OCR may still be needed for some use cases.\n\n- [More Information Needed] on what specific languages, document types, and domains the model was trained on and evaluated for. Users should not assume it will work for all languages and document types without further testing.\n\n- [More Information Needed] on how the model handles potential bias and fairness issues that could arise from the training data. Users should be cautious about applying it to sensitive use cases without deeper analysis.\n\nIn summary, while a promising approach, users should be thoughtful about appropriate use cases, be transparent about limitations, and not treat the model as an OCR replacement without further diligence. More details on the training data, evaluation, and intended use would help address potential misuse."
  },
  {
    "question": "bias_risks_limitations",
    "chain": "generation",
    "answer": "Based on the provided references about the OCR-DocVQA-Donut model, here are some known or foreseeable issues:\n\nTechnical limitations:\n- The model's performance heavily relies on the input resolution. Using larger input sizes leads to better accuracy, especially on tasks like DocVQA with many tiny texts, but incurs higher computational costs.\n- [More Information Needed] on how well the model handles extremely low-resource situations with very limited training data.\n\nSociotechnical limitations: \n- The model was trained on synthetic document images from SynthDoG to reduce reliance on large real document image datasets. However, [More Information Needed] on how representative SynthDoG is of real-world documents and any potential biases or gaps.\n- [More Information Needed] on the model's performance and fairness when applied to documents from diverse languages, cultures, and communities beyond the training data.\n\nPotential misunderstandings:\n- While the model achieves competitive performance without relying on external OCR engines, users should not assume it outperforms all OCR-based methods in all scenarios. Conventional OCR may still be advantageous for some use cases.\n- The model shows robustness on challenging handwritten documents, but [More Information Needed] on its limitations and failure modes on diverse handwritten inputs.\n\nForeseeable harms:\n- [More Information Needed] on risks of the model being misused for unauthorized processing of sensitive personal documents.\n- [More Information Needed] on potential negative impacts if the model underperforms on documents from certain demographics or communities.\n\nTo responsibly deploy this model, more information and analysis is needed in the areas marked [More Information Needed] to fully characterize limitations and proactively mitigate risks. Ongoing monitoring and maintenance is also recommended to identify and address any issues that may emerge post-deployment."
  },
  {
    "question": "bias_recommendations",
    "chain": "generation",
    "answer": "Based on the provided references about the Donut model for document understanding, here are some recommendations regarding foreseeable issues:\n\n1. OCR dependency: While Donut aims to be an OCR-free visual document understanding model, the references mention comparing performance to OCR-based methods. It's important to clearly communicate the extent to which Donut relies on or is independent from OCR to set appropriate expectations. [More Information Needed] on how Donut's OCR-free approach impacts accuracy, speed, and flexibility compared to OCR-dependent models.\n\n2. Language and document type flexibility: The references indicate Donut can be extended to a multi-lingual setting, but [More Information Needed] on the specific languages and scripts supported, and how performance varies across them. Similarly, more details should be provided about what document types and formats Donut handles well vs. struggles with.\n\n3. Computational cost vs. resolution: The references note that increasing input resolution improves Donut's performance, especially on tasks like DocVQA with large images and small text. However, higher resolutions incur greater computational costs. Guidance should be provided to users on recommended resolutions to balance accuracy and efficiency for different use cases.\n\n4. Limited training data: While the SynthDoG synthetic document generator is proposed to alleviate the need for large real document datasets, [More Information Needed] on the quantity and diversity of data required to train Donut effectively for different tasks. Limitations related to smaller training sets should be clearly documented.\n\n5. Responsible usage: As with any AI system, it's important to provide guidance on responsible usage of Donut, such as not relying on it for high-stakes decisions without human oversight. Potential failure modes and biases should be transparently discussed.\n\nIn summary, key issues to address are the relationship to OCR, flexibility across languages and document types, computational trade-offs, training data requirements, and responsible usage guidelines. More specific information is needed in several of these areas to comprehensively assess limitations and societal implications."
  },
  {
    "question": "training_data",
    "chain": "generation",
    "answer": "The model jinhybr/OCR-DocVQA-Donut was trained on the IIT-CDIP dataset, which contains 11M scanned English document images, as well as 0.5M synthetic document images per language (Chinese, Japanese, Korean and English) generated using SynthDoG and text from Wikipedia.\n\n[More Information Needed] for links to documentation related to data pre-processing or additional filtering."
  },
  {
    "question": "preprocessing",
    "chain": "generation",
    "answer": "Here are the details about preprocessing for the data of the model jinhybr/OCR-DocVQA-Donut, based on the provided references:\n\nTokenization:\n[More Information Needed]\n\nImage Resizing:\nThe input resolution of Donut is set to 2560\u00d71920 at the pre-training phase. In some downstream document IE experiments, such as CORD, Ticket and Business Card, smaller resolutions like 960\u00d71280 are used.\n\nOther Preprocessing:\nThe decoder is trained to generate a token sequence that can be converted into a JSON that represents the desired output information. Special tokens (e.g., [memo]) are introduced to represent certain classes, if such replacement is available in the target task.\n\n[More Information Needed] for other specific preprocessing steps."
  },
  {
    "question": "training_regime",
    "chain": "generation",
    "answer": "Based on the provided references, here are the training hyperparameters used for the jinhybr/OCR-DocVQA-Donut model:\n\n- Visual encoder: Swin-B with layer numbers and window size set as {2, 2, 14, 2} and 10 respectively\n- Decoder: First four layers of BART\n- Optimizer: Adam \n- Learning rate: Selected from 1e-5 to 1e-4, scheduled during training. Initial rate of 1e-4 for pre-training and 1e-5 to 1e-4 for fine-tuning.\n- Input resolution: 2560\u00d71920 for pre-training, controlled for downstream tasks (e.g. 1280\u00d7960 for CORD, Ticket and Business Card datasets)\n- Max decoder length: 1536\n- Precision: Half-precision (fp16) training\n- Gradient clipping: Maximum gradient norm selected from 0.05 to 1.0\n- Pre-training: 200K steps with 64 A100 GPUs and mini-batch size of 196\n- Fine-tuning: Monitored edit distance over token sequences. Took ~0.5 hours with one A100 GPU on CORD or Ticket datasets with 1280\u00d7960 input resolution.\n\n[More Information Needed] on the exact learning rate schedule, gradient clipping value, and fine-tuning steps/epochs used."
  },
  {
    "question": "speeds_sizes_times",
    "chain": "generation",
    "answer": "Here are the details about the model jinhybr/OCR-DocVQA-Donut based on the provided references:\n\nTraining throughput: [More Information Needed]\n\nTraining start time: [More Information Needed] \n\nTraining end time: [More Information Needed]\n\nCheckpoint sizes: [More Information Needed]\n\nOther relevant details:\n- Donut was trained with 8 V100 GPUs for approximately 5 days\n- The model architecture consists of:\nEncoder layers: {2,2,18,2}  \nDecoder layers: 4\n- Input image size used was 2048x1536\n- Swin window size of 8 was used\n- Pre-training data included SynthDoG datasets in English, Japanese, Korean, with 0.4M images each\n\nThe references do not provide specific information about the training throughput, start/end times, or checkpoint sizes for the jinhybr/OCR-DocVQA-Donut model. More details would be needed from the model developers to include that information."
  },
  {
    "question": "testing_data",
    "chain": "generation",
    "answer": "Based on the provided references, the model jinhybr/OCR-DocVQA-Donut was evaluated on the following datasets:\n\n1. DocVQA dataset (Reference 3 and 7): The model was tested on the DocVQA (Document Visual Question Answering) task, where given a document image and a question, the model predicts the answer by capturing both visual and textual information within the image.\n\n2. [More Information Needed] for other specific datasets or benchmarks, as they are not explicitly mentioned in the provided references. The references mention evaluating the model on \"various VDU tasks and datasets\" and \"six different datasets including both public benchmarks and private industrial service datasets\", but do not provide the names of these datasets."
  },
  {
    "question": "testing_factors",
    "chain": "generation",
    "answer": "Based on the provided references, here are some foreseeable characteristics that may influence how the OCR-DocVQA-Donut model behaves:\n\n1. Document types and complexity: The model's performance may vary across different types of documents, as suggested by the document classification task used to evaluate if the model can distinguish between document types.\n\n2. OCR system used: The model's performance and speed can be influenced by the choice of OCR engine, as conventional OCR-based methods heavily rely on the off-the-shelf OCR system.\n\n3. Size of training data: The model's performance may be affected by the size of the training dataset. However, Donut shows robust performance even in low-resource situations.\n\n4. Input resolution: The model's performance improves with larger input sizes, especially for tasks like DocVQA where images are larger with many tiny texts. However, increasing the input size also leads to higher computational costs.\n\n5. Handwritten documents: Donut shows robustness in processing handwritten documents, which are known to be challenging for conventional approaches.\n\n[More Information Needed] Regarding specific population subgroups or factors that may lead to disparities in performance, as the provided references do not contain sufficient information to address this aspect.\n\nIn summary, the OCR-DocVQA-Donut model's performance can be influenced by factors such as document types, OCR systems, training data size, input resolution, and the presence of handwritten content. However, more information is needed to assess potential disparities across population subgroups or other specific factors."
  },
  {
    "question": "testing_metrics",
    "chain": "generation",
    "answer": "Based on the provided references, the evaluation metric used for the DocVQA task and the jinhybr/OCR-DocVQA-Donut model is ANLS (Average Normalized Levenshtein Similarity). Specifically, reference 10 states:\n\n\"The evaluation metric is ANLS (Average Normalized Levenshtein Similarity) which is an edit-distancebased metric. The score on the test set is measured via the evaluation site.\"\n\nANLS measures the similarity between the predicted answer and the ground truth answer based on edit distance. \n\nThe references also mention evaluating trade-offs between accuracy and inference speed at different input resolutions. Higher resolutions tend to improve accuracy but slow down the model (reference 7). However, no specific information is provided on how different error types are traded off in the evaluation.\n\n[More Information Needed] on the details of how ANLS handles different error types and any additional metrics used to evaluate specific accuracy-speed tradeoffs for this model."
  },
  {
    "question": "results",
    "chain": "generation",
    "answer": "Based on the provided references, here are the evaluation results for the model jinhybr/OCR-DocVQA-Donut:\n\nPerformance:\n- Donut achieves competitive scores with baselines that depend on external OCR engines on the DocVQA dataset. It is robust on handwritten documents which are challenging to process.\n- Donut shows state-of-the-art performance on Document Classification among general-purpose VDU models like LayoutLM and LayoutLMv2, surpassing LayoutLMv2 accuracy while using fewer parameters and 2x faster speed. \n- On Document Visual Question Answering (DocVQA dataset), Donut shows stable performance regardless of dataset size and task complexity compared to other baselines.\n\nEfficiency: \n- Donut is 2x faster than LayoutLMv2 while using fewer parameters. OCR-based models must consider additional parameters and speed for the entire OCR framework.\n- [More Information Needed] on exact inference speed of Donut.\n\nLow-Resource Performance:\n- Donut shows robust performance when limiting the training set size of CORD dataset compared to other models.\n\n[More Information Needed] for additional factors and metrics like model size, memory usage, training time, etc. The provided references focus mainly on accuracy and some efficiency comparisons to other models."
  },
  {
    "question": "results_summary",
    "chain": "generation",
    "answer": "Based on the provided references, here is a summary of the evaluation results for the model jinhybr/OCR-DocVQA-Donut:\n\n- Donut achieves state-of-the-art performance on document classification tasks compared to general-purpose VDU models like LayoutLM and LayoutLMv2, while using fewer parameters and being 2x faster.\n\n- Donut shows stable performance regardless of dataset size and task complexity, which is significant for industry applications. \n\n- On the DocVQA dataset, Donut achieves competitive scores with baselines that rely on external OCR engines. Donut is robust on challenging handwritten documents.\n\n- Donut's performance on DocVQA improves rapidly with larger input image resolutions, at the cost of increased computation.\n\n- Across public and private datasets, Donut achieves the best scores in terms of both F1 and TED-based accuracy for extracting key information and predicting complex structures.\n\n- [More Information Needed] on specific visualization and analysis of Donut's strengths and challenges.\n\n- Donut keeps architecture uniform by generating answers with the question as a starting prompt.\n\n- Donut demonstrates strong document understanding ability across various VDU tasks, achieving state-of-the-art speed and accuracy without relying on OCR."
  },
  {
    "question": "model_examination",
    "chain": "generation",
    "answer": "Based on the provided references about the Donut model (jinhybr/OCR-DocVQA-Donut), here are some key points related to the model's explainability and interpretability:\n\n- Donut shows striking characteristics through experiments and visualization, indicating its effectiveness as a simple yet powerful approach for tasks like OCR and DocVQA.\n\n- The performance of Donut grows rapidly with larger input image sizes, especially for tasks like DocVQA where images contain many tiny texts. However, increasing input size also leads to higher computational costs.\n\n- Donut demonstrates strong understanding ability through extensive evaluation on various visual document understanding (VDU) tasks and datasets.\n\n- [More Information Needed] on specific techniques or methods used for explaining or interpreting Donut's predictions and internal workings.\n\n- [More Information Needed] on any visualizations, attribution methods, or other explainability tools applied to Donut.\n\nIn summary, while the provided references highlight Donut's effectiveness and some general characteristics related to its performance and understanding capabilities, more specific information is needed on the explainability and interpretability techniques applied to the model to provide a comprehensive answer to the question."
  },
  {
    "question": "hardware",
    "chain": "generation",
    "answer": "According to the references provided, the Donut model was pre-trained using 64 NVIDIA A100 GPUs for 200K steps with a mini-batch size of 196. The fine-tuning on specific datasets like CORD or Ticket took approximately 0.5 hours with one A100 GPU.\n\nFor the specific model jinhybr/OCR-DocVQA-Donut, [More Information Needed] on the exact hardware used for training, as it is not explicitly mentioned in the given references."
  },
  {
    "question": "software",
    "chain": "generation",
    "answer": "Based on the provided references, the model jinhybr/OCR-DocVQA-Donut is trained using the following software:\n\n- NVIDIA A100 GPUs (References 4 and 5)\n- Adam optimizer (References 1 and 4)\n- Half-precision (fp16) training\n\nThe references do not explicitly mention the deep learning framework or other specific software used for training the model. [More Information Needed] on the exact deep learning framework (e.g., PyTorch, TensorFlow) and any other software dependencies for training and deploying the model."
  },
  {
    "question": "hours_used",
    "chain": "generation",
    "answer": "Based on the references provided, the model jinhybr/OCR-DocVQA-Donut was pre-trained for 200K steps with 64 NVIDIA A100 GPUs and a mini-batch size of 196, which took about 2-3 GPU days.\n\nFor the fine-tuning phase on downstream tasks like CORD or Ticket datasets, it took approximately 0.5 hours with one A100 GPU when using an input resolution smaller than the pre-training resolution of 2560\u00d71920."
  },
  {
    "question": "cloud_provider",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not mention the specific cloud provider used for training the jinhybr/OCR-DocVQA-Donut model. More information would be needed from the model development process to determine the cloud provider, if any, that was used."
  },
  {
    "question": "co2_emitted",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain specific information about the amount of carbon emitted during the training of the model jinhybr/OCR-DocVQA-Donut. To accurately report the carbon emissions, more details would be needed such as the hardware used for training, total training time, energy consumption, and carbon intensity of the electricity used."
  },
  {
    "question": "model_specs",
    "chain": "generation",
    "answer": "Model Architecture:\nThe architecture of Donut consists of a Transformer-based visual encoder and textual decoder. The visual encoder converts the input document image into a set of embeddings using Swin Transformer. The textual decoder then maps the derived features into a sequence of subword tokens to construct the desired structured output.\n\nObjective:\nDonut is an end-to-end OCR-free visual document understanding model. It directly maps an input document image into a desired structured output format (e.g., JSON) without relying on any OCR engines or APIs. The model is trained using cross-entropy loss as the pre-training objective.\n\n[More Information Needed] on specific implementation details and hyperparameters used in the jinhybr/OCR-DocVQA-Donut model."
  },
  {
    "question": "compute_infrastructure",
    "chain": "generation",
    "answer": "Based on the provided references, here is the compute infrastructure information for the model jinhybr/OCR-DocVQA-Donut:\n\n- Pre-training:\n  - 64 NVIDIA A100 GPUs\n  - Mini-batch size of 196\n  - 200K training steps, which takes about 2-3 GPU days\n  - Half-precision (fp16) training\n  - Input resolution set to 2560\u00d71920\n\n- Fine-tuning:\n  - [More Information Needed] on the number and type of GPUs used\n  - Approximately 0.5 hours with one A100 GPU for fine-tuning on CORD or Ticket datasets\n  - Input resolutions are controlled, e.g., 960\u00d71280 used for Train Tickets and Business Card parsing tasks\n  - Half-precision (fp16) training\n\n- Inference:\n  - Speed measured on a P40 GPU, which is slower than A100\n\nAdditional notes:\n- Adam optimizer used for both pre-training and fine-tuning\n- Learning rate decreased as training progresses\n  - Pre-training initial learning rate: 1e-4\n  - Fine-tuning initial learning rate: selected from 1e-5 to 1e-4\n- Gradient clipping applied with maximum gradient norm selected from 0.05 to 1.0"
  }
]