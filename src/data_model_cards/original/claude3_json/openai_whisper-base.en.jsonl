[
  {
    "question": "summary",
    "chain": "generation",
    "answer": "openai/whisper-base.en is an English-only speech recognition model based on the Whisper architecture. It is a general-purpose model trained on a large dataset of diverse audio and can be used for English speech recognition tasks."
  },
  {
    "question": "description",
    "chain": "generation",
    "answer": "Model Details for openai/whisper-base.en:\n\nModel Architecture:\n- Encoder-decoder Transformer architecture\n- Audio is resampled to 16kHz, converted to 80-channel log-mel spectrogram\n- Encoder processes input with 2 conv layers, sinusoidal position embeddings, and transformer blocks\n- Decoder uses learned position embeddings and tied input-output token representations\n- Encoder and decoder have same width and number of transformer blocks\n\nTraining Procedures: \n- Trained on 680,000 hours of diverse labeled audio data\n- Multitask training on speech recognition, translation, language ID, voice activity detection\n- Tasks jointly represented as token sequences for decoder to predict\n- Uses special tokens as task specifiers\n- Raw text transcripts used without significant standardization\n- [More Information Needed] on other training details like optimizer, learning rate, etc.\n\nModel Parameters:\n- [More Information Needed]\n\nImportant Disclaimers:\n- Model may generate plausible but incorrect speaker names in transcripts\n- [More Information Needed] on other potential biases, limitations or risks\n\nThe Whisper-base.en model is a general-purpose English speech recognition model that is part of the larger multilingual Whisper model family. It demonstrates strong zero-shot generalization abilities that improve with model size. However, more specific details on model parameters, training hyperparameters, and comprehensive analysis of limitations are needed to complete the model card. Please reach out if you require any additional information to be included."
  },
  {
    "question": "funded_by",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base.en was developed by OpenAI. The references mention the following people and teams at OpenAI:\n\n- Nick Ryder, Will Zhuk, and Andrew Carr for the conversation that inspired the project\n- The Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure \n- Pamela Mishkin for advising the project from a policy perspective\n- Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever\n\nHowever, there is no explicit mention of the specific funding sources or organizations for this project. More information would be needed to definitively list the funders.\n\n[More Information Needed]"
  },
  {
    "question": "shared_by",
    "chain": "generation",
    "answer": "Based on the provided references, the main contributors who made the model openai/whisper-base.en available online as a GitHub repo are:\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever from OpenAI.\n\nThe references also mention thanking the Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure used in the project, as well as Pamela Mishkin for advising from a policy perspective. However, it's unclear if they were directly involved in making the model available on GitHub.\n\n[More Information Needed] on additional contributors, if any, who were specifically involved in publishing the GitHub repo."
  },
  {
    "question": "model_type",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model is a supervised encoder-decoder Transformer model trained on a large dataset of diverse audio for multilingual speech recognition, speech translation, and language identification."
  },
  {
    "question": "language",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model is an English-only speech recognition model that performs better than the multilingual base model for English speech transcription tasks."
  },
  {
    "question": "license",
    "chain": "generation",
    "answer": "Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details."
  },
  {
    "question": "finetuned_from",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base.en does not appear to be fine-tuned from another model. The references mention training a suite of models of various sizes from scratch to study the scaling properties of Whisper. There is no indication that the base model was fine-tuned from a pre-existing model.\n\n[More Information Needed] on whether openai/whisper-base.en was fine-tuned from another model. The provided references do not contain this information."
  },
  {
    "question": "demo_sources",
    "chain": "generation",
    "answer": "Here is the model card description for openai/whisper-base.en based on the provided references:\n\nModel name: openai/whisper-base.en\n\nLink to demo: [More Information Needed]\n\nThe references do not provide a direct link to a demo of the openai/whisper-base.en model. The Colab example notebook linked is for demonstrating usage on the LibriSpeech dataset in general, but is not a hosted demo of this specific model. More information would be needed to include a link to an interactive online demo of openai/whisper-base.en.\n\nPlease let me know if you need any other details filled out for the model card. As the project organizer, I'm happy to track down additional information as needed or put you in touch with the relevant team members who can provide more specifics. We're excited to publish this model to the community on Hugging Face."
  },
  {
    "question": "direct_use",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model can be used for English speech recognition without any fine-tuning, post-processing or complex pipeline. It can be used directly in Python like this:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\") \nprint(result[\"text\"])\n```\n\nThis code snippet loads the base model, transcribes the audio from the \"audio.mp3\" file, and prints out the recognized text.\n\nThe model can also be used from the command line to transcribe English audio files:\n\nwhisper audio.flac audio.mp3 audio.wav --model medium\n\nThis command will transcribe the specified audio files using the medium sized model. The default small model also works well for English transcription.\n\n[More Information Needed] on using the model for other languages, translation, or language identification. The model card focuses on its strong English speech recognition capabilities that work out-of-the-box."
  },
  {
    "question": "downstream_use",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model can be fine-tuned or integrated into a larger speech processing pipeline in several ways:\n\n1. It can perform end-to-end speech recognition, transcribing audio into text. This can be done in Python:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\") \nprint(result[\"text\"])\n```\n\n2. The model provides lower-level access through `whisper.detect_language()` and `whisper.decode()` methods for more granular control.\n\n3. Whisper aims to be a single model handling the entire pipeline, reducing the complexity of separate components like voice activity detection, speaker diarization, etc. \n\n4. [More Information Needed]\n\n5. [More Information Needed]  \n\n6. Audio can be loaded and preprocessed using utility functions:\n\n```python\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n```\n\n7. The model is trained to directly predict raw transcripts without significant standardization, simplifying the pipeline.\n\n8. [More Information Needed]\n\n9. Fine-tuning on high-quality supervised data and reinforcement learning could help reduce errors like incorrect speaker names or hallucinated text.\n\nIn summary, openai/whisper-base.en provides a powerful and flexible foundation for speech recognition tasks that can be used directly or integrated into larger systems with some additional fine-tuning and optimization."
  },
  {
    "question": "out_of_scope_use",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here are some potential ways the model openai/whisper-base.en may foreseeably be misused and what users ought not do with it:\n\n- The model may hallucinate or generate transcripts entirely unrelated to the actual audio input in some cases, especially on the first/last few words (ref 1). Users should not rely on it for applications requiring high accuracy on every single word.\n\n- The model's performance degrades significantly with noisy audio (ref 4, 5). Users should not expect it to work reliably in very noisy environments without further fine-tuning. \n\n- The model has a tendency to make incorrect guesses about speaker names when they are not inferable from the audio (ref 8). Users should not assume the model can accurately identify speakers just from voice.\n\n- [More Information Needed] on other specific ways the model may be intentionally misused, such as for generating deceptive or inappropriate content. The references focus more on robustness limitations than deliberate misuse.\n\nIn general, users should be aware of the model's limitations, not rely on it for high-stakes applications without further testing and adaptation, and use it responsibly in accordance with ethical AI principles. More targeted testing would help identify additional foreseeable misuses to call out."
  },
  {
    "question": "bias_risks_limitations",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper speech recognition model, here are some known or foreseeable issues, limitations, and potential harms:\n\nTechnical Limitations:\n1. Performance on lower-resource languages is still quite poor, likely due to the English-centric training data. Targeted efforts to increase data for rarer languages could significantly improve average speech recognition performance.\n\n2. The model struggles with certain stubborn error types in long-form transcription, such as getting stuck in repeat loops, not transcribing the first or last few words, or hallucinating unrelated transcripts. Fine-tuning on high-quality supervised data or using reinforcement learning could help reduce these errors.\n\n3. The model has a tendency to transcribe plausible but incorrect speaker names, as this information is rarely inferable from the audio alone.\n\nSociotechnical Issues and Potential Harms:\n1. [More Information Needed] The references do not directly discuss sociotechnical issues or potential harms.\n\nMisunderstandings and Unknowns:\n1. It's unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both. Further studies could help clarify this by ablating design components or comparing with existing speech recognition encoders.\n\n2. The robustness properties of Whisper have been the focus, so the potential improvements from fine-tuning on high-quality supervised data for specific domains are unknown and require further study.\n\nTo summarize, the main known issues are technical limitations related to performance on low-resource languages, certain stubborn error types, and incorrect speaker name transcription. More information is needed to assess potential sociotechnical issues and harms. Key areas for further study include understanding the relative contributions of model components and the effects of fine-tuning."
  },
  {
    "question": "bias_recommendations",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here are some recommendations regarding foreseeable issues with the openai/whisper-base.en model:\n\n1. The model's performance on lower-resource languages could be improved by increasing the amount of training data for those languages, as the current pre-training dataset is heavily biased towards English. [More Information Needed] on specific plans to source and incorporate more diverse language data.\n\n2. The model may struggle with certain types of errors in long-form transcription, such as getting stuck in repeat loops, failing to transcribe the beginning or end of audio segments, or hallucinating unrelated transcripts. Fine-tuning on high-quality supervised datasets or using reinforcement learning could help reduce these errors. [More Information Needed] on plans to address these specific error types.\n\n3. The model has a tendency to make plausible but incorrect guesses for speaker names, as this information is rarely inferable from the audio alone. [More Information Needed] on potential mitigation strategies for this issue.\n\n4. The model's performance varies widely depending on the language, with notably lower accuracy on many non-English languages. Refer to the WER/CER breakdown by language in the paper's Appendix D for more details. [More Information Needed] on plans to improve performance on the lowest-scoring languages.\n\n5. [More Information Needed] on any fine-tuning efforts or direct comparisons with other models, which could provide valuable insights into the model's strengths and weaknesses.\n\nIn summary, key areas to monitor and potentially address include language bias in training data, specific error types in long-form transcription, incorrect speaker labeling, and performance disparities across different languages. More information is needed on concrete plans to tackle these issues and further validate the model's performance."
  },
  {
    "question": "training_data",
    "chain": "generation",
    "answer": "The training data for openai/whisper-base.en consists of a large dataset of diverse audio transcripts sourced primarily from English-centric parts of the internet, with minimal data pre-processing and filtering of machine-generated transcripts. [More Information Needed] on the specifics of data pre-processing and additional filtering steps."
  },
  {
    "question": "preprocessing",
    "chain": "generation",
    "answer": "For the openai/whisper-base.en model, the preprocessing steps include:\n\n1. Text preprocessing:\n   - Minimal standardization of transcripts, relying on the model to learn the mapping between utterances and their transcribed form.\n   - Text normalization for English texts, including handling punctuation, formatting, and stylistic aspects.\n   - Removing machine-generated transcripts using heuristics to avoid learning \"transcript-ese\".\n\n2. Audio preprocessing:\n   - Breaking audio files into 30-second segments paired with the corresponding transcript subset.\n   - Training on all audio segments, including those without speech, for voice activity detection.\n   - Converting audio to log mel spectrogram representation:\n     ```python\n     mel = whisper.log_mel_spectrogram(audio).to(model.device)\n     ```\n\n3. Language detection:\n   - Using an audio language detector to ensure the spoken language matches the transcript language.\n   - Including English transcripts even if the audio language doesn't match.\n\n[More Information Needed] on specific details of tokenization and resizing/rewriting.\n\nThe model can be used for transcription within Python:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\nThe `transcribe()` method processes the audio with a sliding 30-second window and performs autoregressive sequence-to-sequence predictions on each window.\n\nLower-level access to the model is provided through `whisper.detect_language()` and `whisper.decode()` functions."
  },
  {
    "question": "training_regime",
    "chain": "generation",
    "answer": "Based on the provided references, here are the training hyperparameters for the openai/whisper-base.en model:\n\n- Optimizer: AdamW\n- Gradient clipping: Gradient norm clipping was used\n- Learning rate schedule: Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size: 256 segments\n- Training duration: Trained for 220 updates, which is between two and three passes over the dataset\n- Precision: Trained with FP16 and dynamic loss scaling\n- Other techniques used: Data parallelism across accelerators, activation checkpointing\n\n[More Information Needed] on the exact learning rate, weight decay, warmup steps, and other hyperparameters not directly mentioned in the references."
  },
  {
    "question": "speeds_sizes_times",
    "chain": "generation",
    "answer": "Based on the provided references, here are the details I can provide about the openai/whisper-base.en model:\n\nModel Architecture:\n- Encoder-decoder Transformer architecture\n- Audio is re-sampled to 16,000 Hz\n- 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds\n\nTraining:\n- Trained with AdamW optimizer and gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size of 256 segments was used\n- Trained for 2^20 updates, which is between two and three passes over the dataset\n- No data augmentation or regularization used during training\n\nPerformance:\n- The base.en model performs well for English-only applications\n- Difference in performance compared to multilingual base model becomes less significant for larger model sizes\n\n[More Information Needed] for the following:\n- Throughput\n- Start or end time of training\n- Checkpoint sizes"
  },
  {
    "question": "testing_data",
    "chain": "generation",
    "answer": "Based on the provided references, the openai/whisper-base.en model was evaluated on the following benchmarks and datasets:\n\nShort-form English-only datasets:\n- LibriSpeech test-clean and test-other splits\n- TED-LIUM 3 test split\n- Common Voice 5.1 English subset\n- Artie bias corpus (subset of Common Voice)\n- CallHome and Switchboard corpora\n- WSJ corpus\n- CORAAL interviews\n\nOther English datasets:\n- CHiME-6 binaural recordings\n- AMI-IHM and AMI-SDM1 corpora\n- Rev16 (subset of 16 files from Rev.AI's Podcast Transcription Benchmark)\n\nMultilingual benchmarks:\n- Multilingual LibriSpeech (MLS)\n- VoxPopuli\n\nThe model's performance was also broken down by language using WER/CER metrics on the Common Voice 15 and Fleurs datasets. Additional detailed results can be found in Appendix D.1, D.2, and D.4 of the referenced paper.\n\n[More Information Needed] on the specific training details and hyperparameters used for the openai/whisper-base.en model."
  },
  {
    "question": "testing_factors",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper speech recognition model, here are some key characteristics that could influence the behavior and performance of the openai/whisper-base.en model:\n\nDomain and Context:\n- Performance degrades with increasing levels of background noise, especially compared to models trained primarily on clean datasets like LibriSpeech\n- Robustness likely stems in part from having a strong decoder that is an audio conditional language model\n- Many remaining errors in long-form transcription are stubborn and non-human/perceptual in nature, like getting stuck in loops or hallucinating unrelated text\n\nPopulation Subgroups:\n- Performance on a given language is strongly predicted by the amount of training data for that language \n- Current pre-training data is very English-heavy, so the model likely performs much worse on lower-resource languages that have less than 1000 hours of training data\n- [More Information Needed] on detailed performance breakdowns and disparities across different languages, accents, demographics\n\nEvaluation:\n- Using a text normalizer in evaluation to reduce penalties for innocuous, non-semantic differences in transcripts\n- Some evaluation datasets may not accurately reflect robustness if commercial models were trained on them\n- Performance varies widely by language - a breakdown is provided for the large models on Common Voice and Fleurs datasets\n- [More Information Needed] on disaggregated evaluations across key subgroups to uncover disparities\n\nIn summary, key factors that likely influence openai/whisper-base.en's behavior are the noise level and length of the input audio, the amount of training data for the particular language, and quirks of the evaluation datasets and metrics used. More disaggregated evaluations are needed to fully characterize performance disparities across subgroups."
  },
  {
    "question": "testing_metrics",
    "chain": "generation",
    "answer": "Based on the references provided, the key metrics and considerations for evaluating the openai/whisper-base.en model are:\n\n- Word Error Rate (WER) is the primary metric typically used in speech recognition research to compare systems. However, WER has limitations as it penalizes all differences between model output and reference transcripts, including innocuous formatting and style differences. This is particularly challenging for zero-shot models like Whisper that don't observe dataset-specific transcript formats. (References 1-2)\n\n- To address WER limitations, the team developed a text normalizer to standardize transcripts before WER calculation and minimize penalization of non-semantic differences. This normalizer was iteratively refined through manual inspection. The code for the normalizer is being released to enable fair comparisons. (References 2-3, 5) \n\n- Character Error Rate (CER) is used as an additional metric for some languages.\n\n- To assess room for improvement, human performance levels were estimated by obtaining professional transcripts for a subset of the Kincaid46 dataset. Distributions of WER from Whisper, commercial services, and the best open-source model were compared. (References 10-11)\n\nIn summary, the primary metrics are WER and CER, with text normalization applied to transcripts to enable fairer comparisons in light of Whisper's zero-shot nature. Human performance levels help indicate headroom for further model improvements. [More Information Needed] on the specific WER/CER results for openai/whisper-base.en."
  },
  {
    "question": "results",
    "chain": "generation",
    "answer": "Evaluation Results of openai/whisper-base.en:\n\nPerformance Metrics:\n- Word Error Rate (WER) on English speech recognition: The model achieves close to human-level performance on English speech recognition tasks, with diminishing returns on performance improvements as model size increases beyond the base size.\n- [More Information Needed] on exact WER values for openai/whisper-base.en.\n- Performance on multilingual speech recognition, speech translation, and language identification continues to improve with larger model sizes beyond the base model.\n- Compared to a supervised model with similar performance on LibriSpeech test-clean, the zero-shot Whisper base model achieves an average 55.2% relative error reduction when evaluated on other speech recognition datasets.\n\nEvaluation Datasets:\n- LibriSpeech\n- Common Voice 15\n- Fleurs\n- [More Information Needed] on additional evaluation datasets and results.\n\nFactors Impacting Performance:\n- Dataset size: Performance improves with increasing dataset size, but shows diminishing returns beyond the 680,000 hours used for training the Whisper models.\n- Language: Performance varies widely depending on the language. WER and CER breakdowns by language are available for some datasets in the paper.\n- [More Information Needed] on other factors impacting performance.\n\nComparisons:\n- Outperforms open-source models like NVIDIA STT Conformer-CTC Large.\n- [More Information Needed] on detailed comparisons to the 4 commercial ASR services mentioned.\n\nMethodology Notes:\n- Extensive text normalization is performed before WER calculation to minimize penalization of zero-shot models like Whisper.\n- Human performance estimates were obtained on a subset of the Kincaid46 dataset to assess room for improvement.\n\n[More Information Needed] on additional evaluation details for the openai/whisper-base.en model specifically. The references provide general insights about the Whisper model family, but more details on the base.en model's evaluation are needed to comprehensively answer the question."
  },
  {
    "question": "results_summary",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here is a summary of the evaluation results for the openai/whisper-base.en model:\n\n- Performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification tasks, with some diminishing returns for English speech recognition possibly due to approaching human-level performance.\n\n- Whisper models are evaluated in a zero-shot manner on a wide set of existing speech processing datasets to test generalization across domains, tasks, and languages. \n\n- Whisper's performance varies widely by language, with performance on a language being well predicted by the amount of training data available for that language.\n\n- On low-data multilingual speech recognition benchmarks like Multilingual LibriSpeech (MLS) and VoxPopuli, Whisper's performance is reported but these benchmarks provide limited coverage of Whisper's multilingual capabilities which include training data for 75 languages.\n\n- Compared to other open-source and commercial ASR models, Whisper performs better on most datasets, especially those with uncommon words. Some commercial models may have been trained on the evaluation datasets though.\n\n[More Information Needed] on the specific performance metrics for the openai/whisper-base.en model size. The references discuss Whisper models in general but do not provide results broken down by the specific base.en model."
  },
  {
    "question": "model_examination",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here is what I can say about the explainability and interpretability of the openai/whisper-base.en model that our team has worked on:\n\nThe Whisper model's robustness seems to be partially due to its strong decoder, which is an audio conditional language model. However, it's currently unclear to what degree the benefits come from training the encoder, decoder, or both. Studying this further by ablating design components or using Whisper's decoder with other encoders could provide more interpretability insights. [More Information Needed]\n\nWhisper still makes some stubborn errors in long-form transcription that seem decidedly non-human/perceptual, like getting stuck in loops, skipping words, or hallucinating unrelated transcripts. Fine-tuning on high-quality datasets or using reinforcement learning to optimize decoding could help explain and reduce these failure modes. [More Information Needed]\n\nWhisper's performance varies significantly by language, with much lower accuracy for lower-resource languages. This is likely due to the English-centric pre-training data. Increasing training data for other languages is a clear path to improving explainability of the model's language-specific capabilities. [More Information Needed]\n\nIn summary, while the Whisper model shows impressive robustness and performance, there are still open questions around the relative contributions of different model components, stubborn error modes, and language-specific capabilities that require further research to fully explain and interpret the model's behaviors. Our team will continue to study these areas to improve the model's interpretability."
  },
  {
    "question": "hardware",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not specify the hardware type that the model openai/whisper-base.en was trained on. The references mention using data parallelism across accelerators and FP16 precision, but do not explicitly state whether GPUs, TPUs, or other hardware was used for training this specific model."
  },
  {
    "question": "software",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base.en is trained using:\n\n- Data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing\n- AdamW optimizer\n- Gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size of 256 segments\n- Trained for 2^20 updates (between two and three passes over the dataset)\n\nThe references do not explicitly mention the specific software or deep learning framework used for training the model. [More Information Needed] on the exact software stack and libraries used in the training process."
  },
  {
    "question": "hours_used",
    "chain": "generation",
    "answer": "Based on the information provided in the references, the Whisper models were trained for 2^20 updates, which is between two and three passes over the dataset. However, the exact amount of time used to train the specific model openai/whisper-base.en is not explicitly mentioned.\n\n[More Information Needed] on the precise training time for the openai/whisper-base.en model."
  },
  {
    "question": "cloud_provider",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not mention the specific cloud provider used for training the openai/whisper-base.en model. More information would be needed from the model developers or documentation to determine which cloud provider, if any, was utilized during training."
  },
  {
    "question": "co2_emitted",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain any information about the amount of carbon emitted during the training of the openai/whisper-base.en model. To answer this question, more specific information would be needed, such as the compute resources used for training, the duration of the training process, and the energy sources powering the training infrastructure."
  },
  {
    "question": "model_specs",
    "chain": "generation",
    "answer": "Model Architecture:\nThe openai/whisper-base.en model uses an encoder-decoder Transformer architecture. Key components include:\n\n- Audio is resampled to 16kHz and converted to an 80-channel log-mel spectrogram using a 25ms window and 10ms stride\n- Input is normalized to [-1, 1] with ~0 mean across the pretraining data\n- Encoder has a small 2-layer CNN stem with stride 2 in the second layer, followed by sinusoidal positional embeddings and Transformer blocks\n- Encoder and decoder have the same width and number of blocks\n- Decoder uses learned positional embeddings and tied input-output token embeddings\n- Pre-activation residual blocks are used in the Transformer\n- Final layer norm is applied to the encoder output\n\nModel Objective:\nThe openai/whisper-base.en model is trained on a multitask mixture of speech processing tasks, including:\n\n- Multilingual speech recognition \n- Speech translation\n- Spoken language identification\n- Voice activity detection\n\nThese tasks are jointly represented as a sequence of tokens to be predicted by the decoder. Special tokens are used to specify the task or classification targets. The model is trained on a large 680,000 hour dataset of transcribed audio from the internet with the goal of learning general speech representations that can be used for zero-shot transfer to downstream tasks without finetuning.\n\n[More Information Needed] on the exact pretraining data composition, training hyperparameters, and performance benchmarks for this specific base.en model variant."
  },
  {
    "question": "compute_infrastructure",
    "chain": "generation",
    "answer": "Based on the provided references, here is the available information about the compute infrastructure used for training the openai/whisper-base.en model:\n\nThe models were trained using:\n- Data parallelism across accelerators\n- FP16 precision with dynamic loss scaling\n- Activation checkpointing\n\nThe optimizer and training hyperparameters:\n- AdamW optimizer\n- Gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size of 256 segments\n- Trained for 2^20 updates (between two and three passes over the dataset)\n\n[More Information Needed] on the specific hardware (GPU/TPU models and counts) used for training.\n\nThe references do not include code blocks directly relevant to the compute infrastructure."
  }
]