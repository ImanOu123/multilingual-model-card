[
  {
    "question": "summary",
    "chain": "generation",
    "answer": "bert-base-multilingual-uncased is a pre-trained deep bidirectional representation model based on the BERT architecture. It is trained on 102 languages using the masked language modeling (MLM) objective, with 12 layers, 768 hidden units, and 12 attention heads, containing 110 million parameters in total.",
    "answer_Chinese": "bert-base-multilingual-uncased 是基于 BERT 架构的预训练深度双向表示模型。它使用掩码语言建模 (MLM) 目标对 102 种语言进行训练，具有 12 层、768 个隐藏单元和 12 个注意力头，总共包含 1.1 亿个参数。",
    "answer_Arabic": "bert-base-multilingual-uncased هو نموذج تمثيل عميق ثنائي الاتجاه تم تدريبه مسبقًا ويعتمد على بنية BERT. تم تدريبه على 102 لغة باستخدام هدف نمذجة اللغة المقنعة (MLM)، مع 12 طبقة، و768 وحدة مخفية، و12 رأس انتباه، تحتوي على 110 مليون معلمة في المجموع.",
    "answer_French": "bert-base-multilingual-uncased est un modèle de représentation bidirectionnelle profonde pré-entraîné basé sur l'architecture BERT. Il est formé sur 102 langages en utilisant l'objectif de modélisation du langage masqué (MLM), avec 12 couches, 768 unités cachées et 12 têtes d'attention, contenant 110 millions de paramètres au total.",
    "answer_Russian": "bert-base-multilingual-uncased — это предварительно обученная модель глубокого двунаправленного представления, основанная на архитектуре BERT. Он обучается на 102 языках с использованием цели моделирования языка в масках (MLM), с 12 слоями, 768 скрытыми единицами и 12 головами внимания, содержащими в общей сложности 110 миллионов параметров.",
    "answer_Japanese": "bert-base-multilingual-uncased は、BERT アーキテクチャに基づいた事前トレーニング済みの深い双方向表現モデルです。これは、マスク言語モデリング (MLM) 目標を使用して 102 の言語でトレーニングされており、12 レイヤー、768 の隠れユニット、12 のアテンション ヘッドを備えており、合計 1 億 1,000 万のパラメーターが含まれています。"
  },
  {
    "question": "description",
    "chain": "generation",
    "answer": "Model Details for bert-base-multilingual-uncased:\n\nModel Architecture:\n- Multi-layer bidirectional Transformer encoder based on the original implementation in Vaswani et al. (2017)\n- Uses bidirectional self-attention, allowing each token to attend to both left and right context\n- 12-layer, 768-hidden, 12-heads, 110M parameters\n\nTraining Procedures: \n- Pre-trained on unlabeled text using two unsupervised tasks:\n  1. Masked Language Model (MLM): Some input tokens are randomly masked and the model learns to predict the masked tokens\n  2. Next Sentence Prediction (NSP): The model learns to predict if two sentences follow each other\n- Fine-tuned for downstream tasks by plugging in task-specific inputs and outputs and fine-tuning all parameters end-to-end\n\nImportant Disclaimers:\n- Covers 102 languages\n- Uncased model\n- This is the original multilingual uncased model released, but not recommended. A newer cased version is recommended instead: \n[`BERT-Base, Multilingual Cased (New, recommended)`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)\n\n[More Information Needed] on specific pre-training data and fine-tuning procedures used for this model.\n\nFor more details, please refer to the BERT paper: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (Devlin et al., 2018).",
    "answer_Chinese": "bert-base-multilingual-uncased 的模型详细信息：\n\n模型架构：\n- 基于 Vaswani 等人的原始实现的多层双向 Transformer 编码器。 (2017)\n- 使用双向自注意力，允许每个令牌同时关注左右上下文\n- 12层，768隐藏，12头，110M参数\n\n培训程序： \n- 使用两个无监督任务对未标记文本进行预训练：\n  1. 屏蔽语言模型（MLM）：一些输入标记被随机屏蔽，模型学习预测屏蔽标记\n  2. 下一个句子预测（NSP）：模型学习预测两个句子是否相互跟随\n- 通过插入特定于任务的输入和输出并端到端微调所有参数，对下游任务进行微调\n\n重要免责声明：\n- 涵盖102种语言\n- 无壳型号\n- 这是最初发布的多语言无壳模型，但不推荐。建议使用更新的版本： \n[`BERT-Base，多语言案例（新，推荐）`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)\n\n[需要更多信息]有关该模型使用的特定预训练数据和微调程序。\n\n更多细节请参考BERT论文：《BERT：用于语言理解的深度双向变压器的预训练》（Devlin et al., 2018）。",
    "answer_Arabic": "تفاصيل النموذج لـ bert-base-multilingual-uncase:\n\nالعمارة النموذجية:\n- تشفير المحول ثنائي الاتجاه متعدد الطبقات بناءً على التنفيذ الأصلي في Vaswani et al. (2017)\n- يستخدم الاهتمام الذاتي ثنائي الاتجاه، مما يسمح لكل رمز بالاهتمام بالسياق الأيسر والأيمن\n- 12 طبقة، 768 مخفي، 12 رأس، 110 مليون معلمة\n\nإجراءات التدريب: \n- تم التدريب مسبقًا على النص غير المسمى باستخدام مهمتين غير خاضعتين للإشراف:\n  1. نموذج اللغة المقنع (MLM): يتم إخفاء بعض رموز الإدخال بشكل عشوائي ويتعلم النموذج التنبؤ بالرموز المميزة المقنعة\n  2. التنبؤ بالجمل التالية (NSP): يتعلم النموذج التنبؤ بما إذا كانت الجملتان تتبعان بعضهما البعض\n- ضبط دقيق للمهام النهائية عن طريق توصيل المدخلات والمخرجات الخاصة بالمهمة وضبط جميع المعلمات من البداية إلى النهاية\n\nإخلاء المسؤولية الهامة:\n- يغطي 102 لغة\n- نموذج غير مغلف\n- هذا هو النموذج الأصلي متعدد اللغات بدون غلاف، ولكن لا يوصى به. يوصى باستخدام إصدار أحدث بدلاً من ذلك: \n[`BERT-Base، غلاف متعدد اللغات (جديد، موصى به)`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)\n\n[يلزم مزيد من المعلومات] حول بيانات ما قبل التدريب المحددة وإجراءات الضبط الدقيقة المستخدمة لهذا النموذج.\n\nلمزيد من التفاصيل، يرجى الرجوع إلى ورقة BERT: \"BERT: التدريب المسبق للمحولات العميقة ثنائية الاتجاه لفهم اللغة\" (Devlin et al., 2018).",
    "answer_French": "Détails du modèle pour bert-base-multilingual-uncased :\n\nArchitecture du modèle :\n- Encodeur de transformateur bidirectionnel multicouche basé sur l'implémentation originale de Vaswani et al. (2017)\n- Utilise une auto-attention bidirectionnelle, permettant à chaque jeton de s'occuper du contexte gauche et droit\n- 12 couches, 768 cachées, 12 têtes, 110 millions de paramètres\n\nProcédures de formation : \n- Pré-entraîné sur du texte non étiqueté à l'aide de deux tâches non supervisées :\n  1. Modèle de langage masqué (MLM) : certains jetons d'entrée sont masqués de manière aléatoire et le modèle apprend à prédire les jetons masqués\n  2. Prédiction de la phrase suivante (NSP) : le modèle apprend à prédire si deux phrases se suivent\n- Affiné pour les tâches en aval en connectant des entrées et des sorties spécifiques aux tâches et en ajustant tous les paramètres de bout en bout\n\nAvertissements importants :\n- Couvre 102 langues\n- Modèle sans boîtier\n- Il s'agit du modèle original multilingue sans boîtier publié, mais non recommandé. Une version plus récente dans un boîtier est recommandée à la place : \n[`BERT-Base, boîtier multilingue (nouveau, recommandé)`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)\n\n[Plus d'informations nécessaires] sur les données de pré-formation spécifiques et les procédures de réglage fin utilisées pour ce modèle.\n\nPour plus de détails, veuillez vous référer à l'article BERT : « BERT : Pre-training of Deep Bidirectionnel Transformers for Language Understanding » (Devlin et al., 2018).",
    "answer_Russian": "Детали модели для bert-base-multilingual-uncased:\n\nМодельная архитектура:\n- Многоуровневый двунаправленный кодер-трансформер, основанный на оригинальной реализации Васвани и др. (2017)\n- Использует двунаправленное внимание к себе, позволяя каждому токену присутствовать как в левом, так и в правом контексте.\n- 12 слоев, 768 скрытых, 12 головок, 110M параметров\n\nПроцедуры обучения: \n- Предварительное обучение на неразмеченном тексте с использованием двух неконтролируемых задач:\n  1. Модель языка в маске (MLM). Некоторые входные токены маскируются случайным образом, и модель учится предсказывать замаскированные токены.\n  2. Прогнозирование следующего предложения (NSP). Модель учится предсказывать, следуют ли два предложения друг за другом.\n- Точная настройка для последующих задач путем подключения входов и выходов для конкретных задач и сквозной точной настройки всех параметров.\n\nВажные заявления об отказе от ответственности:\n- Охватывает 102 языка\n- Модель без корпуса\n- Это оригинальная многоязычная модель без корпуса, выпущенная, но не рекомендуемая. Вместо этого рекомендуется использовать более новую версию в корпусе: \n[`BERT-Base, многоязычный корпус (новинка, рекомендуется)`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)\n\n[Требуется дополнительная информация] о конкретных данных предварительного обучения и процедурах тонкой настройки, используемых для этой модели.\n\nДля получения более подробной информации обратитесь к документу BERT: «BERT: предварительное обучение глубоких двунаправленных преобразователей для понимания языка» (Девлин и др., 2018).",
    "answer_Japanese": "bert-base-multilingual-uncased のモデルの詳細:\n\nモデルのアーキテクチャ:\n- Vaswani らによるオリジナルの実装に基づいた多層双方向 Transformer エンコーダ。 (2017)\n- 双方向セルフアテンションを使用し、各トークンが左と右の両方のコンテキストに対応できるようにします。\n- 12 レイヤー、768 隠し、12 ヘッド、110M パラメーター\n\nトレーニング手順: \n- 2 つの教師なしタスクを使用して、ラベルのないテキストで事前トレーニングされています。\n  1. マスクされた言語モデル (MLM): 一部の入力トークンはランダムにマスクされ、モデルはマスクされたトークンを予測するように学習します。\n  2. 次の文予測 (NSP): モデルは 2 つの文が互いに続くかどうかを予測することを学習します。\n- タスク固有の入力と出力を接続し、すべてのパラメーターをエンドツーエンドで微調整することにより、下流のタスクに合わせて微調整されます。\n\n重要な免責事項:\n- 102の言語をカバー\n- ケースなしモデル\n- これはリリースされたオリジナルの多言語ケースなしモデルですが、推奨されません。代わりに、新しいケース入りバージョンをお勧めします。 \n[`BERT ベース、多言語大文字小文字あり (新規、推奨)`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)\n\nこのモデルに使用される特定の事前トレーニング データと微調整手順に関する [追加情報が必要]。\n\n詳細については、BERT の論文「BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding」(Devlin et al.、2018) を参照してください。"
  },
  {
    "question": "funded_by",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain any information about the funding sources for the BERT-Base Multilingual Uncased model. More details would be needed from the model developers or publishing organization to determine what people or organizations, if any, provided funding for this specific project.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献不包含有关 BERT-Base 多语言 Uncased 模型资金来源的任何信息。模型开发人员或发布组织需要提供更多详细信息，以确定哪些人或组织（如果有）为该特定项目提供了资金。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على أي معلومات حول مصادر التمويل لنموذج BERT-Base Multilingual Uncased. ستكون هناك حاجة إلى مزيد من التفاصيل من مطوري النماذج أو منظمة النشر لتحديد الأشخاص أو المنظمات، إن وجدت، التي قدمت التمويل لهذا المشروع المحدد.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent aucune information sur les sources de financement du modèle BERT-Base Multilingual Uncased. Plus de détails seraient nécessaires de la part des développeurs du modèle ou de l'organisation d'édition pour déterminer quelles personnes ou organisations, le cas échéant, ont financé ce projet spécifique.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат никакой информации об источниках финансирования модели BERT-Base Multilingual Uncased. Чтобы определить, какие люди или организации (если таковые имеются) предоставили финансирование для этого конкретного проекта, потребуется более подробная информация от разработчиков модели или издательской организации.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、BERT ベースの多言語 Uncased モデルの資金源に関する情報は含まれていません。この特定のプロジェクトにどのような人物や組織が資金を提供したのかを判断するには、モデル開発者または発行組織から詳細が必要になります。"
  },
  {
    "question": "shared_by",
    "chain": "generation",
    "answer": "Based on the provided references, it appears that BERT (Bidirectional Encoder Representations from Transformers) was developed by researchers at Google. The paper mentions that \"The code and pre-trained models are available at https://github.com/google-research/bert.\"\n\nHowever, the references do not specify the exact individuals who contributed to making the specific model bert-base-multilingual-uncased available online. To confidently name the contributors, [More Information Needed].",
    "answer_Chinese": "根据提供的参考资料，BERT（来自 Transformers 的双向编码器表示）似乎是由 Google 的研究人员开发的。论文提到“代码和预训练模型可在 https://github.com/google-research/bert 上获取”。\n\n然而，参考文献并没有具体说明为在线提供特定模型 bert-base-multilingual-uncased 做出贡献的具体个人。为了自信地命名贡献者，[需要更多信息]。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، يبدو أن BERT (تمثيلات التشفير ثنائية الاتجاه من المحولات) تم تطويرها بواسطة باحثين في Google. تشير الورقة إلى أن \"الكود والنماذج المدربة مسبقًا متاحة على https://github.com/google-research/bert.\"\n\nومع ذلك، فإن المراجع لا تحدد الأفراد الذين ساهموا في جعل النموذج المحدد bert-base-multilingual-uncased متاحًا عبر الإنترنت. لتسمية المساهمين بثقة، [يلزم مزيد من المعلومات].",
    "answer_French": "Sur la base des références fournies, il semble que BERT (Bidirectionnel Encoder Representations from Transformers) ait été développé par des chercheurs de Google. Le document mentionne que « le code et les modèles pré-entraînés sont disponibles sur https://github.com/google-research/bert ».\n\nCependant, les références ne précisent pas les personnes exactes qui ont contribué à rendre le modèle spécifique bert-base-multilingual-uncased disponible en ligne. Pour nommer en toute confiance les contributeurs, [Plus d'informations nécessaires].",
    "answer_Russian": "Судя по предоставленным ссылкам, кажется, что BERT (представления двунаправленного кодировщика от трансформаторов) был разработан исследователями из Google. В документе упоминается, что «Код и предварительно обученные модели доступны по адресу https://github.com/google-research/bert».\n\nОднако в ссылках не указаны конкретные люди, которые способствовали тому, чтобы конкретная модель bert-base-multilingual-uncased стала доступной в Интернете. Чтобы уверенно назвать авторов, [Необходима дополнительная информация].",
    "answer_Japanese": "提供された参考文献に基づくと、BERT (Bidirectional Encoder Representations from Transformers) は Google の研究者によって開発されたようです。この論文には、「コードと事前トレーニングされたモデルは https://github.com/google-research/bert で入手可能です」と記載されています。\n\nただし、参考文献には、特定のモデル bert-base-multilingual-uncased をオンラインで利用できるようにすることに貢献した正確な個人は特定されていません。貢献者に自信を持って名前を付けるには、[さらなる情報が必要です]。"
  },
  {
    "question": "model_type",
    "chain": "generation",
    "answer": "The bert-base-multilingual-uncased model is a deep bidirectional Transformer encoder pre-trained using the masked language modeling (MLM) objective on multilingual uncased text data.",
    "answer_Chinese": "bert-base-multilingual-uncased 模型是一种深度双向 Transformer 编码器，使用多语言无壳文本数据上的掩码语言建模 (MLM) 目标进行预训练。",
    "answer_Arabic": "يعد نموذج bert-base-multilingual-uncased عبارة عن برنامج تشفير محول ثنائي الاتجاه عميق تم تدريبه مسبقًا باستخدام هدف نمذجة اللغة المقنعة (MLM) على بيانات نصية غير مغطاة متعددة اللغات.",
    "answer_French": "Le modèle bert-base-multilingual-uncased est un encodeur Transformer bidirectionnel profond pré-entraîné à l'aide de l'objectif de modélisation de langage masqué (MLM) sur des données texte multilingues non cassées.",
    "answer_Russian": "Модель bert-base-multilingual-uncased — это глубокий двунаправленный кодер Transformer, предварительно обученный с использованием цели моделирования языка в масках (MLM) на многоязычных текстовых данных без регистра.",
    "answer_Japanese": "bert-base-multilingual-uncased モデルは、多言語の uncased テキスト データに対してマスク言語モデリング (MLM) 目標を使用して事前トレーニングされた、深い双方向の Transformer エンコーダーです。"
  },
  {
    "question": "language",
    "chain": "generation",
    "answer": "The BERT-Base, Multilingual Uncased model supports 102 languages, which were chosen because they are the top 100 languages with the largest Wikipedias.",
    "answer_Chinese": "BERT-Base 多语言 Uncased 模型支持 102 种语言，之所以选择这些语言是因为它们是维基百科最多的前 100 种语言。",
    "answer_Arabic": "يدعم نموذج BERT-Base متعدد اللغات غير المغطى 102 لغة، والتي تم اختيارها لأنها من بين أفضل 100 لغة تضم أكبر ويكيبيديا.",
    "answer_French": "Le modèle BERT-Base, Multilingual Uncased prend en charge 102 langues, qui ont été choisies car ce sont les 100 premières langues avec les plus grandes Wikipédias.",
    "answer_Russian": "Многоязычная модель без регистра BERT-Base поддерживает 102 языка, которые были выбраны потому, что они входят в число 100 лучших языков с крупнейшими Википедиями.",
    "answer_Japanese": "BERT ベースの多言語 Uncased モデルは 102 言語をサポートしています。これらの言語は、Wikipedia の数が最も多い上位 100 言語であるために選ばれました。"
  },
  {
    "question": "license",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe references provided do not contain any information about the specific license being used for the BERT-Base Multilingual Uncased model. More details would be needed from the model creators or documentation to determine the license that applies to this particular model.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料不包含有关用于 BERT-Base 多语言 Uncased 模型的特定许可证的任何信息。需要模型创建者或文档提供更多详细信息，以确定适用于该特定模型的许可证。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على أي معلومات حول الترخيص المحدد المستخدم لنموذج BERT-Base Multilingual Uncased. ستكون هناك حاجة إلى مزيد من التفاصيل من منشئي النموذج أو الوثائق لتحديد الترخيص الذي ينطبق على هذا النموذج بالتحديد.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent aucune information sur la licence spécifique utilisée pour le modèle BERT-Base Multilingual Uncased. Plus de détails seraient nécessaires de la part des créateurs du modèle ou de la documentation pour déterminer la licence qui s'applique à ce modèle particulier.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат никакой информации о конкретной лицензии, используемой для многоязычной бескорпусной модели BERT-Base. Для определения лицензии, применимой к этой конкретной модели, потребуется дополнительная информация от создателей модели или документация.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、BERT ベースの多言語アンケース モデルに使用されている特定のライセンスに関する情報は含まれていません。この特定のモデルに適用されるライセンスを決定するには、モデルの作成者またはドキュメントから詳細を確認する必要があります。"
  },
  {
    "question": "finetuned_from",
    "chain": "generation",
    "answer": "The bert-base-multilingual-uncased model is pre-trained, not fine-tuned from another model. It is a BERT model pre-trained on 102 languages using a masked language modeling (MLM) objective, as mentioned in Reference 7:\n\n\"BERT-Base, Multilingual Uncased (Orig, not recommended): 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\"\n\nThe model can be fine-tuned on downstream tasks by adding task-specific inputs and outputs, as described in References 2 and 3. However, the base model itself is not fine-tuned from another model.",
    "answer_Chinese": "bert-base-multilingual-uncased 模型是预先训练的，而不是根据其他模型进行微调的。它是使用掩码语言建模 (MLM) 目标对 102 种语言进行预训练的 BERT 模型，如参考文献 7 中所述：\n\n“BERT-Base，多语言Uncased（原始，不推荐）：102种语言，12层，768隐藏，12头，110M参数”\n\n该模型可以通过添加特定于任务的输入和输出来对下游任务进行微调，如参考文献 2 和 3 中所述。但是，基础模型本身并未根据其他模型进行微调。",
    "answer_Arabic": "تم تدريب نموذج bert-base-multilingual-uncased مسبقًا، ولم يتم ضبطه بشكل دقيق من نموذج آخر. إنه نموذج BERT تم تدريبه مسبقًا على 102 لغة باستخدام هدف نمذجة اللغة المقنعة (MLM)، كما هو مذكور في المرجع 7:\n\n\"قاعدة BERT، متعددة اللغات غير مغلفة (الأصل، غير مستحسن): 102 لغة، 12 طبقة، 768 مخفيًا، 12 رأسًا، 110 مليون معلمة\"\n\nيمكن ضبط النموذج بدقة على المهام النهائية عن طريق إضافة مدخلات ومخرجات خاصة بالمهمة، كما هو موضح في المرجعين 2 و3. ومع ذلك، لم يتم ضبط النموذج الأساسي نفسه من نموذج آخر.",
    "answer_French": "Le modèle bert-base-multilingual-uncased est pré-entraîné et non affiné à partir d'un autre modèle. Il s'agit d'un modèle BERT pré-entraîné sur 102 langages utilisant un objectif de modélisation de langage masqué (MLM), comme mentionné dans la référence 7 :\n\n\"BERT-Base, multilingue sans boîtier (Orig, non recommandé) : 102 langues, 12 couches, 768 cachées, 12 têtes, 110 millions de paramètres\"\n\nLe modèle peut être affiné sur les tâches en aval en ajoutant des entrées et des sorties spécifiques à des tâches, comme décrit dans les références 2 et 3. Cependant, le modèle de base lui-même n'est pas affiné à partir d'un autre modèle.",
    "answer_Russian": "Модель bert-base-multilingual-uncased предварительно обучена, а не настроена на основе другой модели. Это модель BERT, предварительно обученная на 102 языках с использованием цели моделирования языка в маске (MLM), как указано в ссылке 7:\n\n«База BERT, многоязычная без корпуса (оригинал, не рекомендуется): 102 языка, 12 слоев, 768 скрытых, 12 головок, 110M параметров»\n\nМодель может быть точно настроена для последующих задач путем добавления входных и выходных данных для конкретных задач, как описано в ссылках 2 и 3. Однако сама базовая модель не является точной настройкой на основе другой модели.",
    "answer_Japanese": "bert-base-multilingual-uncased モデルは事前トレーニングされており、別のモデルから微調整されていません。これは、参考資料 7 で説明されているように、マスク言語モデリング (MLM) 目標を使用して 102 言語で事前トレーニングされた BERT モデルです。\n\n「BERT ベース、多言語アンケース (Orig、推奨されません): 102 言語、12 レイヤー、768 隠し、12 ヘッド、1 億 1000 万パラメータ」\n\n参考文献 2 および 3 で説明されているように、タスク固有の入出力を追加することで、下流のタスクでモデルを微調整できます。ただし、基本モデル自体は別のモデルから微調整されません。"
  },
  {
    "question": "demo_sources",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe references provided do not contain a link to a demo of the BERT-Base Multilingual Uncased model. The references discuss the BERT model architecture, pre-training tasks, and performance on NLP benchmarks, but do not mention a specific demo for this particular pre-trained model variant.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料不包含 BERT-Base 多语言 Uncased 模型演示的链接。这些参考文献讨论了 BERT 模型架构、预训练任务和 NLP 基准性能，但没有提及此特定预训练模型变体的具体演示。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على رابط لعرض توضيحي لنموذج BERT-Base Multilingual Uncased. تناقش المراجع بنية نموذج BERT، ومهام التدريب المسبق، والأداء وفقًا لمعايير البرمجة اللغوية العصبية، ولكنها لا تذكر عرضًا توضيحيًا محددًا لهذا النوع من النماذج المدربة مسبقًا.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent pas de lien vers une démo du modèle BERT-Base Multilingual Uncased. Les références discutent de l'architecture du modèle BERT, des tâches de pré-formation et des performances sur les benchmarks NLP, mais ne mentionnent pas de démo spécifique pour cette variante particulière du modèle pré-entraîné.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат ссылки на демо-версию многоязычной бескорпусной модели BERT-Base. В ссылках обсуждается архитектура модели BERT, задачи предварительного обучения и производительность в тестах NLP, но не упоминается конкретная демонстрация для этого конкретного варианта предварительно обученной модели.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、BERT ベースの多言語アンケース化モデルのデモへのリンクが含まれていません。参考文献では、BERT モデルのアーキテクチャ、事前トレーニング タスク、NLP ベンチマークのパフォーマンスについて説明していますが、この特定の事前トレーニング モデル バリアントの具体的なデモについては言及していません。"
  },
  {
    "question": "direct_use",
    "chain": "generation",
    "answer": "The BERT model bert-base-multilingual-uncased can be used without fine-tuning for feature extraction, where fixed features are extracted from the pre-trained model. This has certain advantages over fine-tuning the entire model:\n\n1. It allows using BERT with task-specific model architectures that cannot be easily represented by a Transformer encoder architecture. \n\n2. There are certain computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nTo use BERT for feature extraction:\n\n1. Feed the input text into the pre-trained BERT model.\n\n2. Extract the token representations from one or more of the top hidden layers of the model. \n\n3. Feed these extracted representations into a task-specific model architecture for the downstream task.\n\n[More Information Needed] for a specific code snippet demonstrating feature extraction with bert-base-multilingual-uncased.\n\nThe pre-trained BERT model provides a powerful generic language understanding that can then be leveraged for specific tasks without the need for full fine-tuning. However, [More Information Needed] on comparative performance of feature extraction vs fine-tuning for bert-base-multilingual-uncased on specific tasks.",
    "answer_Chinese": "BERT 模型 bert-base-multilingual-uncased 无需微调即可用于特征提取，其中固定特征是从预训练的模型中提取的。与微调整个模型相比，这具有一定的优势：\n\n1. 它允许将 BERT 与特定于任务的模型架构一起使用，而这些模型架构无法通过 Transformer 编码器架构轻松表示。 \n\n2. 预先计算一次昂贵的训练数据表示，然后在此表示之上使用更便宜的模型运行许多实验，具有一定的计算优势。\n\n使用BERT进行特征提取：\n\n1. 将输入文本输入到预训练的 BERT 模型中。\n\n2. 从模型的一个或多个顶部隐藏层中提取标记表示。 \n\n3. 将这些提取的表示输入到下游任务的特定于任务的模型架构中。\n\n[需要更多信息] 演示使用 bert-base-multilingual-uncased 进行特征提取的特定代码片段。\n\n预训练的 BERT 模型提供了强大的通用语言理解能力，然后可以将其用于特定任务，而无需进行全面微调。然而，[需要更多信息]关于特定任务上 bert-base-multilingual-uncased 的特征提取与微调的性能比较。",
    "answer_Arabic": "يمكن استخدام نموذج BERT bert-base-multilingual-uncased دون ضبط دقيق لاستخراج الميزات، حيث يتم استخراج الميزات الثابتة من النموذج المُدرب مسبقًا. وهذا له مزايا معينة مقارنة بضبط النموذج بأكمله:\n\n1. يسمح باستخدام BERT مع بنيات نموذجية خاصة بالمهمة والتي لا يمكن تمثيلها بسهولة بواسطة بنية تشفير المحولات. \n\n2. هناك فوائد حسابية معينة لإجراء حساب مسبق لتمثيل باهظ الثمن لبيانات التدريب مرة واحدة ثم إجراء العديد من التجارب باستخدام نماذج أرخص فوق هذا التمثيل.\n\nلاستخدام BERT لاستخراج الميزات:\n\n1. أدخل نص الإدخال في نموذج BERT المُدرب مسبقًا.\n\n2. قم باستخراج تمثيلات الرمز المميز من واحدة أو أكثر من الطبقات المخفية العليا للنموذج. \n\n3. قم بتغذية هذه التمثيلات المستخرجة في بنية نموذجية خاصة بالمهمة للمهمة النهائية.\n\n[مطلوب مزيد من المعلومات] للحصول على مقتطف كود محدد يوضح استخراج الميزات باستخدام bert-base-multilingual-uncased.\n\nيوفر نموذج BERT المُدرب مسبقًا فهمًا عامًا قويًا للغة يمكن بعد ذلك الاستفادة منه في مهام محددة دون الحاجة إلى الضبط الدقيق الكامل. ومع ذلك، [يلزم مزيد من المعلومات] حول الأداء المقارن لاستخراج الميزات مقابل الضبط الدقيق لـ bert-base-multilingual-uncased في مهام محددة.",
    "answer_French": "Le modèle BERT bert-base-multilingual-uncased peut être utilisé sans réglage fin pour l'extraction de fonctionnalités, où les fonctionnalités fixes sont extraites du modèle pré-entraîné. Cela présente certains avantages par rapport au réglage fin de l'ensemble du modèle :\n\n1. Il permet d'utiliser BERT avec des architectures de modèles spécifiques à des tâches qui ne peuvent pas être facilement représentées par une architecture d'encodeur Transformer. \n\n2. Il existe certains avantages informatiques à précalculer une fois une représentation coûteuse des données d'entraînement, puis à exécuter de nombreuses expériences avec des modèles moins chers en plus de cette représentation.\n\nPour utiliser BERT pour l'extraction de fonctionnalités :\n\n1. Introduisez le texte saisi dans le modèle BERT pré-entraîné.\n\n2. Extrayez les représentations de jetons d'une ou plusieurs des couches cachées supérieures du modèle. \n\n3. Introduisez ces représentations extraites dans une architecture de modèle spécifique à la tâche pour la tâche en aval.\n\n[Plus d'informations nécessaires] pour un extrait de code spécifique illustrant l'extraction de fonctionnalités avec bert-base-multilingual-uncased.\n\nLe modèle BERT pré-entraîné fournit une puissante compréhension du langage générique qui peut ensuite être exploitée pour des tâches spécifiques sans avoir besoin d'un réglage précis. Cependant, [Plus d'informations nécessaires] sur les performances comparatives de l'extraction de fonctionnalités par rapport au réglage fin de bert-base-multilingual-uncased sur des tâches spécifiques.",
    "answer_Russian": "Модель BERT bert-base-multilingual-uncased можно использовать без тонкой настройки для извлечения признаков, когда фиксированные признаки извлекаются из предварительно обученной модели. Это имеет определенные преимущества перед точной настройкой всей модели:\n\n1. Это позволяет использовать BERT с архитектурами моделей для конкретных задач, которые невозможно легко представить с помощью архитектуры кодера Transformer. \n\n2. Существуют определенные вычислительные преимущества, если предварительно вычислить дорогостоящее представление обучающих данных один раз, а затем провести множество экспериментов с более дешевыми моделями поверх этого представления.\n\nЧтобы использовать BERT для извлечения признаков:\n\n1. Введите входной текст в предварительно обученную модель BERT.\n\n2. Извлеките представления токенов из одного или нескольких верхних скрытых слоев модели. \n\n3. Вставьте эти извлеченные представления в архитектуру модели конкретной задачи для последующей задачи.\n\n[Требуется дополнительная информация] для конкретного фрагмента кода, демонстрирующего извлечение функций с помощью bert-base-multilingual-uncased.\n\nПредварительно обученная модель BERT обеспечивает мощное общее понимание языка, которое затем можно использовать для конкретных задач без необходимости полной тонкой настройки. Тем не менее, [Требуется дополнительная информация] о сравнительной производительности извлечения функций и тонкой настройки bert-base-multilingual-uncased для конкретных задач.",
    "answer_Japanese": "BERT モデル bert-base-multilingual-uncased は、事前トレーニングされたモデルから固定された特徴が抽出される特徴抽出の微調整なしで使用できます。これには、モデル全体を微調整する場合に比べて、次のような利点があります。\n\n1. Transformer エンコーダ アーキテクチャでは簡単に表現できないタスク固有のモデル アーキテクチャで BERT を使用できるようになります。 \n\n2. トレーニング データの高価な表現を 1 回事前計算し、その後、この表現に基づいて安価なモデルを使用して多くの実験を実行することには、一定の計算上の利点があります。\n\nBERT を特徴抽出に使用するには:\n\n1. 入力テキストを事前トレーニングされた BERT モデルにフィードします。\n\n2. モデルの最上位の非表示層の 1 つ以上からトークン表現を抽出します。 \n\n3. これらの抽出された表現を、下流タスクのタスク固有のモデル アーキテクチャにフィードします。\n\nbert-base-multilingual-uncased を使用した特徴抽出を示す特定のコード スニペットについては、[詳細情報が必要です]。\n\n事前トレーニングされた BERT モデルは、完全な微調整を必要とせずに、特定のタスクに活用できる強力な汎用言語の理解を提供します。ただし、特定のタスクにおける特徴抽出と bert-base-multilingual-uncased の微調整のパフォーマンスの比較に関する [さらなる情報が必要]。"
  },
  {
    "question": "downstream_use",
    "chain": "generation",
    "answer": "The bert-base-multilingual-uncased model can be fine-tuned for a variety of downstream NLP tasks across multiple languages. Fine-tuning is straightforward, as the self-attention mechanism in BERT allows it to model tasks involving single text or text pairs by providing the appropriate inputs and outputs.\n\nFor example, the model can be fine-tuned for the XNLI dataset, which is a 15-language version of MultiNLI. The fine-tuning process involves modifying the `run_classifier.py` script to support the XNLI dataset, as shown in the code snippet:\n\n```\npython run_classifier.py \\\n  --task_name=XNLI \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$XNLI_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5 \\\n  --num_train_epochs=2.0 \\\n  --output_dir=/tmp/xnli_output/\n```\n\nThe multilingual model does not require any special consideration or API changes. However, the `BasicTokenizer` in `tokenization.py` should be updated to support Chinese character tokenization.\n\nWhen fine-tuned, the bert-base-multilingual-uncased model can be used for various sentence-level and token-level tasks across multiple languages, often outperforming task-specific architectures. The pre-trained representations reduce the need for heavily-engineered task-specific architectures.\n\n[More Information Needed] on how the model can be plugged into a larger ecosystem or app.\n\nFor more information and pre-trained models, please visit https://github.com/google-research/bert.",
    "answer_Chinese": "bert-base-multilingual-uncased 模型可以针对跨多种语言的各种下游 NLP 任务进行微调。微调非常简单，因为 BERT 中的自注意力机制允许它通过提供适当的输入和输出来对涉及单个文本或文本对的任务进行建模。\n\n例如，该模型可以针对 XNLI 数据集进行微调，该数据集是 MultiNLI 的 15 种语言版本。微调过程涉及修改“run_classifier.py”脚本以支持XNLI数据集，如代码片段所示：\n\n````\npython run_classifier.py \\\n  --task_name=XNLI \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$XNLI_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5 \\\n  --num_train_epochs=2.0 \\\n  --output_dir=/tmp/xnli_output/\n````\n\n多语言模型不需要任何特殊考虑或 API 更改。但是，应该更新“tokenization.py”中的“BasicTokenizer”以支持汉字标记化。\n\n经过微调后，bert-base-multilingual-uncased 模型可用于跨多种语言的各种句子级和标记级任务，通常优于特定于任务的架构。预先训练的表示减少了对精心设计的特定任务架构的需求。\n\n[需要更多信息]了解如何将模型插入到更大的生态系统或应用程序中。\n\n有关更多信息和预训练模型，请访问 https://github.com/google-research/bert。",
    "answer_Arabic": "يمكن ضبط نموذج bert-base-multilingual-uncased بشكل دقيق ليناسب مجموعة متنوعة من مهام البرمجة اللغوية العصبية (NLP) عبر لغات متعددة. يعد الضبط الدقيق أمرًا مباشرًا، حيث تسمح آلية الاهتمام الذاتي في BERT بنمذجة المهام التي تتضمن نصًا واحدًا أو أزواجًا نصية من خلال توفير المدخلات والمخرجات المناسبة.\n\nعلى سبيل المثال، يمكن ضبط النموذج بشكل دقيق لمجموعة بيانات XNLI، وهي نسخة مكونة من 15 لغة من MultiNLI. تتضمن عملية الضبط الدقيق تعديل البرنامج النصي `run_classifier.py` لدعم مجموعة بيانات XNLI، كما هو موضح في مقتطف الكود:\n\n```\nبيثون run_classifier.py \\\n  --task_name=XNLI \\\n  --do_train=صحيح \\\n  --do_eval=صحيح \\\n  --data_dir=$XNLI_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5 \\\n  --num_train_epochs=2.0 \\\n  --output_dir=/tmp/xnli_output/\n```\n\nلا يتطلب النموذج متعدد اللغات أي اعتبارات خاصة أو تغييرات في واجهة برمجة التطبيقات. ومع ذلك، يجب تحديث `BasicTokenizer' في `tokenization.py' لدعم ترميز الأحرف الصينية.\n\nعند ضبطه بشكل دقيق، يمكن استخدام نموذج bert-base-multilingual-uncased لمختلف المهام على مستوى الجملة وعلى مستوى الرمز المميز عبر لغات متعددة، وغالبًا ما يتفوق على البنى الخاصة بالمهمة المحددة. تعمل التمثيلات المدربة مسبقًا على تقليل الحاجة إلى بنيات خاصة بالمهام شديدة الهندسة.\n\n[مطلوب مزيد من المعلومات] حول كيفية توصيل النموذج بنظام بيئي أو تطبيق أكبر.\n\nلمزيد من المعلومات والنماذج المدربة مسبقًا، يرجى زيارة https://github.com/google-research/bert.",
    "answer_French": "Le modèle bert-base-multilingual-uncased peut être affiné pour une variété de tâches NLP en aval dans plusieurs langues. Le réglage fin est simple, car le mécanisme d'auto-attention de BERT lui permet de modéliser des tâches impliquant un texte unique ou des paires de textes en fournissant les entrées et sorties appropriées.\n\nPar exemple, le modèle peut être affiné pour l'ensemble de données XNLI, qui est une version en 15 langues de MultiNLI. Le processus de réglage fin implique de modifier le script « run_classifier.py » pour prendre en charge l'ensemble de données XNLI, comme indiqué dans l'extrait de code :\n\n```\npython run_classifier.py \\\n  --task_name=XNLI\\\n  --do_train=true \\\n  --do_eval=vrai \\\n  --data_dir=$XNLI_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5 \\\n  --num_train_epochs=2.0 \\\n  --output_dir=/tmp/xnli_output/\n```\n\nLe modèle multilingue ne nécessite aucune considération particulière ni modification de l'API. Cependant, le « BasicTokenizer » dans « tokenization.py » doit être mis à jour pour prendre en charge la tokenisation des caractères chinois.\n\nUne fois affiné, le modèle bert-base-multilingual-uncased peut être utilisé pour diverses tâches au niveau des phrases et des jetons dans plusieurs langues, surpassant souvent les architectures spécifiques aux tâches. Les représentations pré-entraînées réduisent le besoin d’architectures spécifiques à des tâches fortement conçues.\n\n[Plus d'informations nécessaires] sur la façon dont le modèle peut être connecté à un écosystème ou une application plus large.\n\nPour plus d'informations et des modèles pré-entraînés, veuillez visiter https://github.com/google-research/bert.",
    "answer_Russian": "Модель bert-base-multilingual-uncases может быть точно настроена для множества последующих задач НЛП на нескольких языках. Точная настройка проста, поскольку механизм самообслуживания в BERT позволяет моделировать задачи, включающие один текст или пары текстов, предоставляя соответствующие входные и выходные данные.\n\nНапример, модель можно точно настроить для набора данных XNLI, который представляет собой 15-языковую версию MultiNLI. Процесс тонкой настройки включает в себя изменение сценария run_classifier.py для поддержки набора данных XNLI, как показано во фрагменте кода:\n\n```\npython run_classifier.py \\\n  --task_name=XNLI \\\n  --do_train=истина \\\n  --do_eval=истина \\\n  --data_dir=$XNLI_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5 \\\n  --num_train_epochs=2.0 \\\n  --output_dir=/tmp/xnli_output/\n```\n\nМногоязычная модель не требует особого рассмотрения или изменения API. Однако `BasicTokenizer` в `tokenization.py` должен быть обновлен для поддержки токенизации китайских символов.\n\nПри точной настройке модель bert-base-multilingual-uncases может использоваться для различных задач на уровне предложений и токенов на нескольких языках, часто превосходя по производительности архитектуры для конкретных задач. Предварительно обученные представления уменьшают необходимость в тщательно продуманных архитектурах для конкретных задач.\n\n[Требуется дополнительная информация] о том, как модель можно подключить к более крупной экосистеме или приложению.\n\nДля получения дополнительной информации и предварительно обученных моделей посетите https://github.com/google-research/bert.",
    "answer_Japanese": "bert-base-multilingual-uncased モデルは、複数の言語にわたるさまざまなダウンストリーム NLP タスクに合わせて微調整できます。 BERT のセルフアテンション メカニズムにより、適切な入力と出力を提供することで、単一のテキストまたはテキストのペアを含むタスクをモデル化できるため、微調整は簡単です。\n\nたとえば、MultiNLI の 15 言語バージョンである XNLI データセットに合わせてモデルを微調整できます。微調整プロセスには、コード スニペットに示すように、XNLI データセットをサポートするように「run_classifier.py」スクリプトを変更することが含まれます。\n\n「」\npython run_classifier.py \\\n  --タスク名=XNLI \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$XNLI_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5 \\\n  --num_train_epochs=2.0 \\\n  --output_dir=/tmp/xnli_output/\n「」\n\n多言語モデルでは、特別な考慮や API の変更は必要ありません。ただし、中国語のトークン化をサポートするには、「tokenization.py」の「BasicTokenizer」を更新する必要があります。\n\n微調整すると、bert-base-multilingual-uncased モデルは、複数の言語にわたるさまざまな文レベルおよびトークンレベルのタスクに使用でき、多くの場合、タスク固有のアーキテクチャよりも優れたパフォーマンスを発揮します。事前にトレーニングされた表現により、高度に設計されたタスク固有のアーキテクチャの必要性が軽減されます。\n\nモデルをより大きなエコシステムまたはアプリにどのように組み込むことができるかについては、[さらなる情報が必要です]。\n\n詳細と事前トレーニングされたモデルについては、https://github.com/google-research/bert をご覧ください。"
  },
  {
    "question": "out_of_scope_use",
    "chain": "generation",
    "answer": "Based on the provided references, here are some potential ways the BERT-base-multilingual-uncased model could be misused and what users should avoid doing:\n\n- The model was trained on 102 languages in an uncased (case-insensitive) manner. Users should not expect it to handle casing or capitalization in a meaningful way. For better cased language handling, the cased multilingual model is recommended instead.\n\n- As a multilingual model, its performance on high-resource languages like English may be somewhat worse than a dedicated single-language model. Users seeking maximum performance for a specific language should consider using a model trained solely on that language rather than this multilingual one.\n\n- [More Information Needed] on other specific ways the model architecture or training data could lead to misuse or unintended consequences if applied incorrectly. The references do not provide enough details to speculate further.\n\nIn general, users should be cautious about applying the model to tasks or languages it was not designed or evaluated for. They should carefully test its suitability and performance before deploying it in any system that could negatively impact people if the model behaves unexpectedly. More targeted testing and analysis would be prudent to uncover additional potential misuses to warn users about.",
    "answer_Chinese": "根据提供的参考资料，以下是 BERT-base-multilingual-uncased 模型可能被滥用的一些潜在方式以及用户应避免执行的操作：\n\n- 该模型以不加大小写（不区分大小写）的方式对 102 种语言进行了训练。用户不应期望它以有意义的方式处理大小写或大写。为了更好地处理大小写语言，建议使用大小写多语言模型。\n\n- 作为多语言模型，它在英语等高资源语言上的性能可能比专用的单语言模型要差一些。寻求特定语言最佳性能的用户应考虑使用仅针对该语言训练的模型，而不是这种多语言模型。\n\n- [需要更多信息]关于模型架构或训练数据如果应用不当可能导致误用或意外后果的其他特定方式。参考文献没有提供足够的细节来进一步推测。\n\n一般来说，用户应该谨慎地将模型应用于其未设计或评估的任务或语言。在将其部署到任何系统中之前，他们应该仔细测试其适用性和性能，如果模型表现异常，可能会对人们产生负面影响。更有针对性的测试和分析将是谨慎的做法，以发现其他潜在的滥用行为并警告用户。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، فيما يلي بعض الطرق المحتملة التي يمكن من خلالها إساءة استخدام نموذج BERT-base-multilingual-uncased وما يجب على المستخدمين تجنب القيام به:\n\n- تم تدريب النموذج على 102 لغة بطريقة غير متضمنة (غير حساسة لحالة الأحرف). يجب ألا يتوقع المستخدمون أن يتعامل مع الغلاف أو الكتابة بالأحرف الكبيرة بطريقة ذات معنى. للتعامل بشكل أفضل مع لغة الحالة، يوصى باستخدام النموذج متعدد اللغات بدلاً من ذلك.\n\n- كنموذج متعدد اللغات، قد يكون أدائه في اللغات ذات الموارد العالية مثل اللغة الإنجليزية أسوأ إلى حد ما من النموذج المخصص للغة واحدة. يجب على المستخدمين الذين يسعون لتحقيق أقصى قدر من الأداء للغة معينة أن يفكروا في استخدام نموذج تم تدريبه على تلك اللغة فقط بدلاً من هذا النموذج متعدد اللغات.\n\n- [مطلوب مزيد من المعلومات] حول الطرق المحددة الأخرى التي قد تؤدي بها بنية النموذج أو بيانات التدريب إلى سوء الاستخدام أو إلى عواقب غير مقصودة إذا تم تطبيقها بشكل غير صحيح. لا تقدم المراجع تفاصيل كافية لمزيد من التكهن.\n\nبشكل عام، يجب على المستخدمين توخي الحذر بشأن تطبيق النموذج على المهام أو اللغات التي لم يتم تصميمه أو تقييمه لها. ويجب عليهم اختبار ملاءمته وأدائه بعناية قبل نشره في أي نظام يمكن أن يؤثر سلبًا على الأشخاص إذا تصرف النموذج بشكل غير متوقع. سيكون من الحكمة إجراء المزيد من الاختبارات والتحليلات المستهدفة للكشف عن إساءة الاستخدام المحتملة الإضافية لتحذير المستخدمين منها.",
    "answer_French": "Sur la base des références fournies, voici quelques façons potentielles dont le modèle BERT-base-multilingual-uncased pourrait être utilisé à mauvais escient et ce que les utilisateurs devraient éviter de faire :\n\n- Le modèle a été formé sur 102 langues sans casse (insensible à la casse). Les utilisateurs ne doivent pas s'attendre à ce qu'il gère la casse ou la majuscule de manière significative. Pour une meilleure gestion de la casse, le modèle multilingue avec casse est recommandé à la place.\n\n- En tant que modèle multilingue, ses performances sur des langues à ressources élevées comme l'anglais peuvent être légèrement pires qu'un modèle monolingue dédié. Les utilisateurs recherchant des performances maximales pour une langue spécifique devraient envisager d'utiliser un modèle formé uniquement sur cette langue plutôt que sur celle-ci multilingue.\n\n- [Plus d'informations nécessaires] sur d'autres manières spécifiques dont l'architecture du modèle ou les données de formation pourraient conduire à une mauvaise utilisation ou à des conséquences inattendues si elles sont appliquées incorrectement. Les références ne fournissent pas suffisamment de détails pour spéculer davantage.\n\nEn général, les utilisateurs doivent être prudents lorsqu'ils appliquent le modèle à des tâches ou à des langages pour lesquels il n'a pas été conçu ou évalué. Ils doivent soigneusement tester son adéquation et ses performances avant de le déployer dans tout système susceptible d’avoir un impact négatif sur les personnes si le modèle se comporte de manière inattendue. Des tests et des analyses plus ciblés seraient prudents pour découvrir d’autres abus potentiels dont il faudrait avertir les utilisateurs.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот несколько потенциальных способов неправильного использования модели BERT-base-multilingual-uncased и чего пользователям следует избегать:\n\n- Модель была обучена на 102 языках без учета регистра. Пользователям не следует ожидать, что он будет осмысленно обрабатывать регистр или заглавные буквы. Для лучшей обработки регистрированных языков вместо этого рекомендуется использовать регистрозависимую многоязычную модель.\n\n- В качестве многоязычной модели ее производительность на языках с высоким уровнем ресурсов, таких как английский, может быть несколько хуже, чем у специализированной одноязычной модели. Пользователям, которым нужна максимальная производительность для определенного языка, следует рассмотреть возможность использования модели, обученной исключительно на этом языке, а не на этой многоязычной.\n\n- [Требуется дополнительная информация] о других конкретных способах, которыми архитектура модели или обучающие данные могут привести к неправильному использованию или непредвиденным последствиям в случае неправильного применения. Ссылки не содержат достаточно подробностей для дальнейших предположений.\n\nВ общем, пользователям следует с осторожностью применять модель к задачам или языкам, для которых она не была разработана или оценена. Им следует тщательно проверить ее пригодность и производительность, прежде чем развертывать ее в любой системе, которая может негативно повлиять на людей, если модель поведет себя неожиданно. Было бы разумно провести более целенаправленное тестирование и анализ, чтобы выявить дополнительные потенциальные злоупотребления, о которых следует предупредить пользователей.",
    "answer_Japanese": "提供された参考資料に基づいて、BERT ベースの多言語大文字小文字区別なしモデルが悪用される可能性のある潜在的な方法と、ユーザーが避けるべきことをいくつか示します。\n\n- モデルは、大文字と小文字を区別しない (大文字と小文字を区別しない) 方法で 102 の言語でトレーニングされました。ユーザーは、大文字と小文字の区別や大文字の区別が意味のある方法で処理されることを期待すべきではありません。大文字と小文字を区別した言語の処理を改善するには、代わりに大文字と小文字を区別した多言語モデルを推奨します。\n\n- 多言語モデルとして、英語などの高リソース言語でのパフォーマンスは、専用の単一言語モデルよりも多少劣る可能性があります。特定の言語で最大のパフォーマンスを求めるユーザーは、この多言語モデルではなく、その言語のみでトレーニングされたモデルの使用を検討する必要があります。\n\n- モデル アーキテクチャまたはトレーニング データが正しく適用されなかった場合、誤用や予期せぬ結果につながる可能性があるその他の具体的な方法に関する [さらなる情報が必要]。参考文献には、さらに推測するのに十分な詳細が記載されていません。\n\n一般に、ユーザーはモデルが設計または評価されていないタスクや言語にモデルを適用する場合には注意する必要があります。モデルが予期せぬ動作をした場合に人々に悪影響を及ぼす可能性があるシステムに導入する前に、その適合性とパフォーマンスを慎重にテストする必要があります。ユーザーに警告するために、さらにターゲットを絞ったテストと分析を行って、さらなる潜在的な誤用を明らかにすることが賢明です。"
  },
  {
    "question": "bias_risks_limitations",
    "chain": "generation",
    "answer": "Based on the provided references about the BERT-base multilingual uncased model, here are some potential issues and limitations:\n\nTechnical limitations:\n- For high-resource languages like English, the multilingual model performs somewhat worse than a single-language model. However, maintaining many single-language models is not feasible.\n- [More Information Needed] on detailed performance metrics for the 102 languages covered by this model. It's unclear how well it works for lower-resource languages.\n\nPotential misunderstandings:\n- This is an older version of the multilingual model that is no longer recommended. A newer cased version covering 104 languages is preferred. \n- As a bidirectional model, BERT is designed for language understanding tasks, not generation. Using it for generation may lead to poor results.\n\nForeseeable sociotechnical issues:\n- [More Information Needed] With 102 languages, the model will have varying quality across languages based on training data availability. This could disadvantage some language communities.\n- Bias in the training data, especially for lower-resource languages, may get amplified by the model and harm underrepresented groups. More analysis is needed.\n- The model may struggle with language and dialect variations not well-represented in training. This could limit usability for some language communities.\n\nTo summarize, the main issues are the model being an outdated version, having technical limitations especially for generation and non-English languages, and the potential to amplify data bias against certain languages and dialects. More information and analysis would help better characterize these risks. Moving to the newer recommended version is advised.",
    "answer_Chinese": "根据提供的有关基于 BERT 的多语言 uncased 模型的参考资料，以下是一些潜在的问题和限制：\n\n技术限制：\n- 对于英语等高资源语言，多语言模型的表现比单语言模型稍差。然而，维护许多单语言模型是不可行的。\n- [需要更多信息] 该模型涵盖的 102 种语言的详细性能指标。目前尚不清楚它对于资源较低的语言的效果如何。\n\n潜在的误解：\n- 这是旧版本的多语言模型，不再推荐。首选涵盖 104 种语言的较新版本。 \n- 作为双向模型，BERT 专为语言理解任务而设计，而不是生成。使用它进行生成可能会导致不良结果。\n\n可预见的社会技术问题：\n- [需要更多信息] 该模型有 102 种语言，根据训练数据的可用性，不同语言的质量会有所不同。这可能会使某些语言社区处于不利地位。\n- 训练数据中的偏差，特别是对于资源较低的语言，可能会被模型放大并损害代表性不足的群体。需要更多分析。\n- 该模型可能会遇到训练中未充分体现的语言和方言变化。这可能会限制某些语言社区的可用性。\n\n总而言之，主要问题是该模型是过时的版本，具有技术限制，特别是对于生成语言和非英语语言，并且有可能放大针对某些语言和方言的数据偏差。更多信息和分析将有助于更好地描述这些风险。建议迁移到较新的推荐版本。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول النموذج غير المغطى متعدد اللغات القائم على BERT، إليك بعض المشكلات والقيود المحتملة:\n\nالقيود الفنية:\n- بالنسبة للغات عالية الموارد مثل اللغة الإنجليزية، يكون أداء النموذج متعدد اللغات أسوأ إلى حد ما من نموذج اللغة الواحدة. ومع ذلك، فإن الحفاظ على العديد من النماذج أحادية اللغة ليس ممكنًا.\n- [مطلوب مزيد من المعلومات] حول مقاييس الأداء التفصيلية للغات الـ 102 التي يغطيها هذا النموذج. من غير الواضح مدى نجاحه مع اللغات ذات الموارد المنخفضة.\n\nسوء الفهم المحتمل:\n- هذه نسخة قديمة من النموذج متعدد اللغات ولم يعد يوصى بها. يفضل إصدار أحدث يغطي 104 لغة. \n- كنموذج ثنائي الاتجاه، تم تصميم BERT لمهام فهم اللغة، وليس الجيل. استخدامه للتوليد قد يؤدي إلى نتائج سيئة.\n\nالقضايا الاجتماعية والفنية المتوقعة:\n- [يلزم المزيد من المعلومات] مع وجود 102 لغة، سيكون للنموذج جودة متفاوتة عبر اللغات بناءً على توفر بيانات التدريب. وهذا قد يضر ببعض المجتمعات اللغوية.\n- قد يتم تضخيم التحيز في بيانات التدريب، خاصة بالنسبة للغات ذات الموارد المنخفضة، بواسطة النموذج ويضر بالمجموعات الممثلة تمثيلا ناقصا. هناك حاجة إلى مزيد من التحليل.\n- قد يعاني النموذج من اختلافات اللغة واللهجة غير الممثلة بشكل جيد في التدريب. وهذا يمكن أن يحد من سهولة الاستخدام لبعض المجتمعات اللغوية.\n\nلتلخيص ذلك، تتمثل المشكلات الرئيسية في كون النموذج إصدارًا قديمًا، وله قيود فنية خاصة بالنسبة للجيل واللغات غير الإنجليزية، وإمكانية تضخيم تحيز البيانات ضد لغات ولهجات معينة. ومن شأن المزيد من المعلومات والتحليلات أن تساعد في توصيف هذه المخاطر بشكل أفضل. يُنصح بالانتقال إلى الإصدار الأحدث الموصى به.",
    "answer_French": "Sur la base des références fournies sur le modèle multilingue sans boîtier basé sur BERT, voici quelques problèmes et limitations potentiels :\n\nLimites techniques :\n- Pour les langues à ressources élevées comme l'anglais, le modèle multilingue est un peu moins performant qu'un modèle monolingue. Cependant, il n’est pas possible de conserver de nombreux modèles monolingues.\n- [Plus d'informations nécessaires] sur les mesures de performances détaillées pour les 102 langues couvertes par ce modèle. On ne sait pas exactement dans quelle mesure cela fonctionne pour les langues à faibles ressources.\n\nMalentendus potentiels :\n- Il s'agit d'une ancienne version du modèle multilingue qui n'est plus recommandée. Une version plus récente couvrant 104 langues est préférable. \n- En tant que modèle bidirectionnel, BERT est conçu pour les tâches de compréhension du langage, et non pour la génération. Son utilisation pour la génération peut conduire à de mauvais résultats.\n\nProblèmes sociotechniques prévisibles :\n- [Plus d'informations nécessaires] Avec 102 langues, le modèle aura une qualité variable selon les langues en fonction de la disponibilité des données de formation. Cela pourrait désavantager certaines communautés linguistiques.\n- Les biais dans les données de formation, en particulier pour les langues à faibles ressources, peuvent être amplifiés par le modèle et nuire aux groupes sous-représentés. Une analyse plus approfondie est nécessaire.\n- Le modèle peut avoir des difficultés avec les variations linguistiques et dialectales qui ne sont pas bien représentées dans la formation. Cela pourrait limiter la convivialité pour certaines communautés linguistiques.\n\nPour résumer, les principaux problèmes sont que le modèle est une version obsolète, présente des limitations techniques, en particulier pour les langues de génération et autres que l'anglais, et le potentiel d'amplifier les biais des données à l'encontre de certaines langues et dialectes. Plus d’informations et d’analyses permettraient de mieux caractériser ces risques. Il est conseillé de passer à la version recommandée la plus récente.",
    "answer_Russian": "На основе предоставленных ссылок на многоязычную бескорпусную модель на основе BERT можно выделить некоторые потенциальные проблемы и ограничения:\n\nТехнические ограничения:\n- Для языков с высоким уровнем ресурсов, таких как английский, многоязычная модель работает несколько хуже, чем одноязычная модель. Однако поддерживать множество одноязычных моделей невозможно.\n- [Требуется дополнительная информация] о подробных показателях производительности для 102 языков, охватываемых этой моделью. Неясно, насколько хорошо это работает для языков с низким уровнем ресурсов.\n\nВозможные недоразумения:\n- Это старая версия многоязычной модели, которая больше не рекомендуется. Предпочтительна более новая версия в корпусе, поддерживающая 104 языка. \n- Как двунаправленная модель, BERT предназначен для задач понимания языка, а не генерации. Использование его для генерации может привести к плохим результатам.\n\nПредвидимые социотехнические проблемы:\n- [Требуется дополнительная информация] Поскольку модель поддерживает 102 языка, ее качество будет разным в зависимости от доступности обучающих данных. Это может поставить в невыгодное положение некоторые языковые сообщества.\n- Предвзятость в данных обучения, особенно для языков с ограниченными ресурсами, может усиливаться моделью и наносить вред недостаточно представленным группам. Необходим дополнительный анализ.\n- Модель может иметь проблемы с языковыми и диалектными вариациями, которые недостаточно представлены в обучении. Это может ограничить удобство использования для некоторых языковых сообществ.\n\nПодводя итог, можно сказать, что основными проблемами являются устаревшая версия модели, наличие технических ограничений, особенно для поколений и неанглийских языков, а также потенциальная возможность усиления систематической ошибки данных в отношении определенных языков и диалектов. Дополнительная информация и анализ помогут лучше охарактеризовать эти риски. Рекомендуется перейти на более новую рекомендуемую версию.",
    "answer_Japanese": "BERT ベースの多言語大文字小文字なしモデルに関して提供されている参考資料に基づいて、潜在的な問題と制限をいくつか示します。\n\n技術的な制限:\n- 英語のような高リソース言語の場合、多言語モデルのパフォーマンスは単一言語モデルよりも若干劣ります。ただし、多数の単一言語モデルを維持することは現実的ではありません。\n- このモデルの対象となる 102 言語の詳細なパフォーマンス指標に関する [さらなる情報が必要]。低リソース言語でどの程度うまく機能するかは不明です。\n\n潜在的な誤解:\n- これは多言語モデルの古いバージョンであり、推奨されなくなりました。 104 言語をカバーする、より新しいケース入りバージョンが推奨されます。 \n- 双方向モデルとして、BERT は生成ではなく言語理解タスクのために設計されています。生成に使用すると、悪い結果が生じる可能性があります。\n\n予見される社会技術的問題:\n- [さらに詳しい情報が必要] 102 言語があるため、モデルの品質はトレーニング データの利用可能性に基づいて言語間で異なります。これは一部の言語コミュニティに不利益をもたらす可能性があります。\n- トレーニング データの偏り、特に低リソース言語の場合、モデルによって増幅され、過小評価されたグループに悪影響を与える可能性があります。さらなる分析が必要です。\n- モデルは、トレーニングで十分に表現されていない言語や方言のバリエーションに苦戦する可能性があります。これにより、一部の言語コミュニティでは使いやすさが制限される可能性があります。\n\n要約すると、主な問題は、モデルが古いバージョンであること、特に世代言語や英語以外の言語に対して技術的な制限があること、特定の言語や方言に対するデータの偏りを増幅する可能性があることです。より多くの情報と分析は、これらのリスクをより適切に特徴付けるのに役立ちます。新しい推奨バージョンに移行することをお勧めします。"
  },
  {
    "question": "bias_recommendations",
    "chain": "generation",
    "answer": "Based on the provided references, here are some recommendations regarding foreseeable issues with the BERT-base-multilingual-uncased model:\n\n1. The BERT-base-multilingual-uncased model is the original version covering 102 languages, but reference 5 indicates it is not recommended compared to the newer cased version covering 104 languages. Consider using the newer recommended version instead.\n\n2. The model uses bidirectional pre-training which is important for performance, as shown in references 1 and 7. However, [More Information Needed] on any potential issues or limitations of the bidirectional approach for this specific model.\n\n3. [More Information Needed] regarding any societal biases that may be captured in the pre-training data spanning 102 languages. Analyzing the training data for potentially problematic content would be prudent.\n\n4. [More Information Needed] on how well the model performs for lower-resource languages out of the 102 covered. Additional testing may be warranted especially for languages with less pre-training data.\n\n5. As an uncased model, it will not handle casing information which may be important in some languages or applications. The cased version may be preferable for such cases.\n\nIn summary, while a groundbreaking multilingual model, I would recommend using the newer cased version, analyzing biases in the pre-training data, and evaluating performance carefully on lower-resource languages and tasks requiring case information. More information is needed to draw firm conclusions in several areas.",
    "answer_Chinese": "根据提供的参考资料，以下是有关 BERT-base-multilingual-uncased 模型可预见问题的一些建议：\n\n1. BERT-base-multilingual-uncased 模型是覆盖 102 种语言的原始版本，但参考文献 5 表明与覆盖 104 种语言的新 cased 版本相比，不推荐使用。考虑使用较新的推荐版本。\n\n2. 该模型使用对性能非常重要的双向预训练，如参考文献 1 和 7 所示。但是，[需要更多信息]了解此特定模型的双向方法的任何潜在问题或限制。\n\n3. [需要更多信息] 关于跨 102 种语言的预训练数据中可能捕获的任何社会偏见。分析训练数据中可能存在问题的内容是谨慎的做法。\n\n4. [需要更多信息] 该模型对于所涵盖的 102 种资源较低的语言的表现如何。可能需要进行额外的测试，特别是对于预训练数据较少的语言。\n\n5. 作为一个无大小写模型，它不会处理在某些语言或应用程序中可能很重要的大小写信息。对于这种情况，带壳的版本可能更合适。\n\n总之，虽然是一个突破性的多语言模型，但我建议使用较新的案例版本，分析预训练数据中的偏差，并仔细评估资源较低的语言和需要案例信息的任务的性能。需要更多信息才能在几个领域得出明确的结论。",
    "answer_Arabic": "بناءً على المراجع المقدمة، فيما يلي بعض التوصيات المتعلقة بالمشكلات المتوقعة مع نموذج BERT متعدد اللغات غير المغطى:\n\n1. نموذج BERT الأساسي متعدد اللغات غير المغطى هو الإصدار الأصلي الذي يغطي 102 لغة، لكن المرجع 5 يشير إلى أنه غير موصى به مقارنة بالإصدار الأحدث الذي يغطي 104 لغة. فكر في استخدام الإصدار الأحدث الموصى به بدلاً من ذلك.\n\n2. يستخدم النموذج تدريبًا مسبقًا ثنائي الاتجاه وهو أمر مهم للأداء، كما هو موضح في المرجعين 1 و7. ومع ذلك، [يلزم مزيد من المعلومات] حول أي مشكلات أو قيود محتملة للنهج ثنائي الاتجاه لهذا النموذج المحدد.\n\n3. [بحاجة إلى مزيد من المعلومات] فيما يتعلق بأي تحيزات مجتمعية يمكن التقاطها في بيانات ما قبل التدريب التي تغطي 102 لغة. سيكون من الحكمة تحليل بيانات التدريب للمحتوى الذي قد يسبب مشاكل.\n\n4. [مطلوب مزيد من المعلومات] حول مدى جودة أداء النموذج للغات ذات الموارد المنخفضة من أصل 102 لغة مشمولة. قد يكون هناك ما يبرر إجراء اختبارات إضافية خاصة للغات التي تحتوي على بيانات أقل للتدريب المسبق.\n\n5. كنموذج غير مغلف، فإنه لن يتعامل مع معلومات الغلاف التي قد تكون مهمة في بعض اللغات أو التطبيقات. قد يكون الإصدار المغلف هو الأفضل لمثل هذه الحالات.\n\nباختصار، على الرغم من كونه نموذجًا رائدًا متعدد اللغات، إلا أنني أوصي باستخدام الإصدار الأحدث، وتحليل التحيزات في بيانات ما قبل التدريب، وتقييم الأداء بعناية في اللغات ذات الموارد المنخفضة والمهام التي تتطلب معلومات الحالة. هناك حاجة إلى مزيد من المعلومات لاستخلاص استنتاجات قاطعة في عدة مجالات.",
    "answer_French": "Sur la base des références fournies, voici quelques recommandations concernant les problèmes prévisibles avec le modèle BERT-base-multilingual-uncased :\n\n1. Le modèle BERT-base-multilingual-uncased est la version originale couvrant 102 langues, mais la référence 5 indique qu'il n'est pas recommandé par rapport à la version plus récente avec boîtier couvrant 104 langues. Pensez plutôt à utiliser la version recommandée la plus récente.\n\n2. Le modèle utilise une pré-formation bidirectionnelle qui est importante pour la performance, comme le montrent les références 1 et 7. Cependant, [Plus d'informations nécessaires] sur tout problème ou limitation potentiel de l'approche bidirectionnelle pour ce modèle spécifique.\n\n3. [Plus d'informations nécessaires] concernant les préjugés sociétaux pouvant être capturés dans les données préalables à la formation couvrant 102 langues. Il serait prudent d’analyser les données de formation pour détecter un contenu potentiellement problématique.\n\n4. [Plus d'informations nécessaires] sur les performances du modèle pour les langues à faibles ressources sur les 102 couvertes. Des tests supplémentaires peuvent être justifiés, en particulier pour les langues avec moins de données préalables à la formation.\n\n5. En tant que modèle sans casse, il ne gérera pas les informations de casse qui peuvent être importantes dans certaines langues ou applications. La version avec boîtier peut être préférable dans de tels cas.\n\nEn résumé, bien qu'il s'agisse d'un modèle multilingue révolutionnaire, je recommanderais d'utiliser la version casée la plus récente, d'analyser les biais dans les données de pré-formation et d'évaluer soigneusement les performances sur les langues à faibles ressources et les tâches nécessitant des informations de cas. Des informations supplémentaires sont nécessaires pour tirer des conclusions définitives dans plusieurs domaines.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот несколько рекомендаций относительно прогнозируемых проблем с моделью BERT-base-multilingual-uncaseed:\n\n1. Модель BERT-base-multilingual-lessed является исходной версией, охватывающей 102 языка, но ссылка 5 указывает, что она не рекомендуется по сравнению с более новой версией с регистром, охватывающей 104 языка. Вместо этого рассмотрите возможность использования более новой рекомендованной версии.\n\n2. В модели используется двунаправленное предварительное обучение, что важно для производительности, как показано в ссылках 1 и 7. Однако [Необходима дополнительная информация] о любых потенциальных проблемах или ограничениях двунаправленного подхода для этой конкретной модели.\n\n3. [Необходима дополнительная информация] относительно любых социальных предубеждений, которые могут быть отражены в данных предварительного обучения, охватывающих 102 языка. Было бы разумно проанализировать данные обучения на наличие потенциально проблемного контента.\n\n4. [Требуется дополнительная информация] о том, насколько хорошо модель работает для языков с меньшими ресурсами из 102 рассмотренных. Может потребоваться дополнительное тестирование, особенно для языков с меньшим количеством данных для предварительного обучения.\n\n5. Будучи моделью без регистра, она не обрабатывает информацию о регистре, которая может быть важной в некоторых языках или приложениях. В таких случаях вариант с корпусом может быть предпочтительнее.\n\nТаким образом, несмотря на то, что это новаторская многоязычная модель, я бы рекомендовал использовать более новую версию с регистром, анализировать отклонения в данных перед обучением и тщательно оценивать производительность на языках с меньшими ресурсами и задачах, требующих информации о регистре. Чтобы сделать однозначные выводы в нескольких областях, необходимо больше информации.",
    "answer_Japanese": "提供された参考資料に基づいて、BERT ベースの多言語ケースなしモデルで予見される問題に関する推奨事項をいくつか示します。\n\n1. BERT ベースの多言語大文字小文字なしモデルは、102 言語をカバーするオリジナル バージョンですが、参考資料 5 には、104 言語をカバーする新しい大文字小文字ありバージョンと比較して推奨されないことが示されています。代わりに、より新しい推奨バージョンを使用することを検討してください。\n\n2. このモデルは、参考文献 1 および 7 に示されているように、パフォーマンスにとって重要な双方向の事前トレーニングを使用しています。ただし、この特定のモデルに対する双方向アプローチの潜在的な問題や制限については [さらなる情報が必要] です。\n\n3. 102 言語にわたる事前トレーニング データに取り込まれる可能性のある社会的偏見に関する [さらなる情報が必要]。潜在的に問題のあるコンテンツのトレーニング データを分析することが賢明です。\n\n4. 対象となる 102 言語のうち、低リソース言語でモデルがどの程度うまく機能するかに関する [さらなる情報が必要]。特に事前トレーニング データが少ない言語では、追加のテストが保証される場合があります。\n\n5. 大文字と小文字を区別しないモデルとして、一部の言語やアプリケーションで重要になる可能性がある大文字と小文字の情報を処理しません。このような場合には、ケース入りバージョンの方が適している可能性があります。\n\n要約すると、画期的な多言語モデルではありますが、新しいケース付きバージョンを使用し、トレーニング前データのバイアスを分析し、リソースの少ない言語やケース情報を必要とするタスクでパフォーマンスを慎重に評価することをお勧めします。いくつかの分野で確かな結論を導き出すには、より多くの情報が必要です。"
  },
  {
    "question": "training_data",
    "chain": "generation",
    "answer": "The training data for bert-base-multilingual-uncased consists of the top 100 languages with the largest Wikipedias, with the entire Wikipedia dump for each language (excluding user and talk pages) taken as the training data. The data was weighted during pre-training data creation using exponentially smoothed weighting, with high-resource languages like English under-sampled and low-resource languages over-sampled.",
    "answer_Chinese": "bert-base-multilingual-uncased 的训练数据由维基百科数量最多的前 100 种语言组成，每种语言的整个维基百科转储（不包括用户页面和讨论页面）作为训练数据。在预训练数据创建过程中，使用指数平滑加权对数据进行加权，对英语等高资源语言进行欠采样，对低资源语言进行过采样。",
    "answer_Arabic": "تتكون بيانات التدريب الخاصة بـ bert-base-multilingual-uncased من أفضل 100 لغة مع أكبر ويكيبيديا، مع تفريغ ويكيبيديا بالكامل لكل لغة (باستثناء صفحات المستخدم والنقاش) التي يتم أخذها كبيانات تدريب. تم ترجيح البيانات أثناء إنشاء بيانات ما قبل التدريب باستخدام الترجيح السلس بشكل كبير، مع أخذ عينات من اللغات عالية الموارد مثل اللغة الإنجليزية، واللغات منخفضة الموارد ذات العينات الزائدة.",
    "answer_French": "Les données de formation pour bert-base-multilingual-uncased se composent des 100 principales langues avec les plus grands Wikipédia, avec l'intégralité du dump Wikipédia pour chaque langue (à l'exclusion des pages utilisateur et de discussion) prise comme données de formation. Les données ont été pondérées lors de la création des données de pré-formation à l'aide d'une pondération lissée de manière exponentielle, avec des langues à ressources élevées comme l'anglais sous-échantillonnées et des langues à faibles ressources sur-échantillonnées.",
    "answer_Russian": "Данные обучения для bert-base-multilingual-uncased состоят из 100 лучших языков с крупнейшими Википедиями, при этом в качестве данных обучения используется весь дамп Википедии для каждого языка (за исключением пользовательских страниц и страниц обсуждения). Данные были взвешены во время создания данных перед обучением с использованием экспоненциально сглаженного взвешивания, при этом языки с высоким содержанием ресурсов, такие как английский, были занижены, а языки с низкими ресурсами — избыточны.",
    "answer_Japanese": "bert-base-multilingual-uncased のトレーニング データは、最大規模の Wikipedia を持つ上位 100 言語で構成され、各言語の Wikipedia ダンプ全体 (ユーザー ページとトーク ページを除く) がトレーニング データとして取得されます。データは、英語などの高リソース言語はアンダーサンプリングされ、低リソース言語はオーバーサンプリングされて、指数平滑化された重み付けを使用して事前トレーニング データの作成中に重み付けされました。"
  },
  {
    "question": "preprocessing",
    "chain": "generation",
    "answer": "For the BERT base multilingual uncased model, the following preprocessing steps were applied to the training data:\n\nTokenization:\n- A shared 110k WordPiece vocabulary was used across all languages\n- For all languages except Chinese, the following recipe was applied:\n  a) Lowercasing and accent removal\n  b) Punctuation splitting\n  c) Whitespace tokenization\n- The word counts for the vocabulary were weighted the same way as the training data, so low-resource languages were upweighted by some factor\n- No language-specific markers were used to denote the input language, to enable zero-shot learning\n\nMasking:\n- 15% of all WordPiece tokens in each sequence were randomly selected for prediction\n- For the selected tokens:\n  - 80% of the time, the token was replaced with the [MASK] token\n  - 10% of the time, the token was replaced with a random token\n  - 10% of the time, the token was left unchanged\n- This masking allows the model to learn a bidirectional representation, but creates a mismatch with fine-tuning where no [MASK] tokens are used\n\n[More Information Needed] on any resizing or rewriting steps for the input data.",
    "answer_Chinese": "对于 BERT 基础多语言无箱模型，对训练数据应用了以下预处理步骤：\n\n代币化：\n- 所有语言都使用共享的 110k WordPiece 词汇表\n- 对于除中文之外的所有语言，应用以下配方：\n  a) 小写和重音去除\n  b) 标点符号分割\n  c) 空格标记化\n- 词汇的字数计数与训练数据的加权方式相同，因此资源匮乏的语言会因某些因素而增加权重\n- 没有使用特定于语言的标记来表示输入语言，以实现零样本学习\n\n掩蔽：\n- 每个序列中所有 WordPiece 标记的 15% 被随机选择用于预测\n- 对于选定的代币：\n  - 80% 的情况下，该令牌被替换为 [MASK] 令牌\n  - 10% 的情况下，令牌被替换为随机令牌\n  - 10% 的情况下，代币保持不变\n- 这种屏蔽允许模型学习双向表示，但会在不使用 [MASK] 标记的情况下通过微调产生不匹配\n\n[需要更多信息]有关输入数据的任何调整大小或重写步骤。",
    "answer_Arabic": "بالنسبة لنموذج BERT الأساسي متعدد اللغات غير المغطى، تم تطبيق خطوات المعالجة المسبقة التالية على بيانات التدريب:\n\nالترميز:\n- تم استخدام 110 ألف مفردة WordPiece مشتركة في جميع اللغات\n- لجميع اللغات باستثناء الصينية تم تطبيق الوصفة التالية:\n  أ) إزالة الحروف الصغيرة واللكنة\n  ب) تقسيم علامات الترقيم\n  ج) ترميز المسافات البيضاء\n- تم ترجيح عدد الكلمات في المفردات بنفس طريقة وزن بيانات التدريب، لذلك تم ترجيح اللغات منخفضة الموارد من خلال بعض العوامل\n- لم يتم استخدام أي علامات خاصة باللغة للإشارة إلى لغة الإدخال، لتمكين التعلم بدون إطلاق النار\n\nقناع:\n- تم اختيار 15% من جميع رموز WordPiece المميزة في كل تسلسل بشكل عشوائي للتنبؤ بها\n- بالنسبة للرموز المختارة:\n  - في 80% من الحالات، تم استبدال الرمز المميز بالرمز المميز [MASK].\n  - في 10% من الحالات، تم استبدال الرمز المميز برمز عشوائي\n  - في 10% من الحالات، يتم ترك الرمز دون تغيير\n- يسمح هذا الإخفاء للنموذج بتعلم تمثيل ثنائي الاتجاه، ولكنه يخلق عدم تطابق مع الضبط الدقيق حيث لا يتم استخدام الرموز المميزة [MASK]\n\n[مطلوب مزيد من المعلومات] حول أي خطوات لتغيير الحجم أو إعادة الكتابة للبيانات المدخلة.",
    "answer_French": "Pour le modèle multilingue non cassé de base BERT, les étapes de prétraitement suivantes ont été appliquées aux données de formation :\n\nTokenisation :\n- Un vocabulaire WordPièce partagé de 110 000 $ a été utilisé dans toutes les langues\n- Pour toutes les langues sauf le chinois, la recette suivante a été appliquée :\n  a) Suppression des minuscules et des accents\n  b) Division de la ponctuation\n  c) Tokenisation des espaces\n- Le nombre de mots pour le vocabulaire a été pondéré de la même manière que les données de formation, de sorte que les langues à faibles ressources ont été pondérées par un certain facteur.\n- Aucun marqueur spécifique à la langue n'a été utilisé pour désigner la langue d'entrée, afin de permettre un apprentissage sans tir.\n\nMasquage :\n- 15 % de tous les jetons WordPièce de chaque séquence ont été sélectionnés au hasard pour la prédiction\n- Pour les jetons sélectionnés :\n  - 80% du temps, le token était remplacé par le token [MASK]\n  - 10% du temps, le token a été remplacé par un token aléatoire\n  - 10% du temps, le token est resté inchangé\n- Ce masquage permet au modèle d'apprendre une représentation bidirectionnelle, mais crée un décalage avec le réglage fin où aucun jeton [MASK] n'est utilisé\n\n[Plus d'informations nécessaires] sur toute étape de redimensionnement ou de réécriture des données d'entrée.",
    "answer_Russian": "Для базовой многоязычной бескорпусной модели BERT к обучающим данным были применены следующие этапы предварительной обработки:\n\nТокенизация:\n- На всех языках использовался общий словарный запас WordPiece объемом 110 тыс.\n- Для всех языков, кроме китайского, применен следующий рецепт:\n  а) Удаление строчных букв и ударения\n  б) Пунктуационное разделение\n  в) Токенизация пробелов\n- Количество слов в словаре было взвешено так же, как данные обучения, поэтому языки с низкими ресурсами были повышены на какой-то коэффициент.\n- Для обозначения языка ввода не использовались маркеры, специфичные для языка, чтобы обеспечить обучение с нуля.\n\nМаскировка:\n- 15% всех токенов WordPiece в каждой последовательности были выбраны случайным образом для предсказания.\n- Для выбранных токенов:\n  - В 80% случаев токен заменялся токеном [MASK].\n  - в 10% случаев токен заменялся случайным токеном\n  - в 10% случаев токен оставался неизменным\n- Эта маскировка позволяет модели изучать двунаправленное представление, но создает несоответствие при точной настройке, когда токены [MASK] не используются.\n\n[Требуется дополнительная информация] о любых шагах изменения размера или перезаписи входных данных.",
    "answer_Japanese": "BERT ベースの多言語アンケースド モデルの場合、次の前処理ステップがトレーニング データに適用されました。\n\nトークン化:\n- 共有の 110,000 WordPiece 語彙がすべての言語で使用されました\n- 中国語を除くすべての言語には、次のレシピが適用されました。\n  a) 小文字とアクセントの削除\n  b) 句読点の分割\n  c) ホワイトスペースのトークン化\n- 語彙の単語数はトレーニング データと同じ方法で重み付けされたため、リソースの少ない言語は何らかの要因で重み付けされました。\n- ゼロショット学習を可能にするために、入力言語を示すために言語固有のマーカーは使用されませんでした。\n\nマスキング：\n- 各シーケンス内のすべての WordPiece トークンの 15% が予測用にランダムに選択されました\n- 選択したトークンの場合:\n  - 80% の確率で、トークンは [MASK] トークンに置き換えられました。\n  - 10% の確率で、トークンはランダムなトークンに置き換えられました。\n  - 10% の確率でトークンは変更されませんでした。\n- このマスキングにより、モデルは双方向表現を学習できるようになりますが、[MASK] トークンが使用されない場合の微調整では不一致が生じます。\n\n入力データのサイズ変更または書き換え手順に関する [詳細情報が必要]。"
  },
  {
    "question": "training_regime",
    "chain": "generation",
    "answer": "Based on the provided references, here are the training hyperparameters for the bert-base-multilingual-uncased model:\n\n- Trained on 102 languages\n- 12-layer, 768-hidden, 12-heads, 110M parameters\n- Trained with a batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus\n- Used Adam optimizer with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate\n- Dropout probability of 0.1 on all layers\n- Used gelu activation rather than the standard relu\n- [More Information Needed] on the exact pre-training data and corpus\n- Trained using the \"masked LM\" (MLM) and \"next sentence prediction\" (NSP) tasks\n- [More Information Needed] on the exact hardware setup and training time",
    "answer_Chinese": "根据提供的参考资料，以下是 bert-base-multilingual-uncased 模型的训练超参数：\n\n- 接受过 102 种语言的培训\n- 12层，768隐藏，12头，110M参数\n- 使用 256 个序列的批量大小（256 个序列 * 512 个标记 = 128,000 个标记/批次）进行 1,000,000 个步骤的训练，这大约是 33 亿单词语料库中的 40 个 epoch\n- 使用 Adam 优化器，学习率为 1e-4，β1 = 0.9，β2 = 0.999，L2 权重衰减为 0.01，前 10,000 个步骤的学习率预热，以及学习率的线性衰减\n- 所有层的 Dropout 概率为 0.1\n- 使用 gelu 激活而不是标准 relu\n- [需要更多信息]关于确切的预训练数据和语料库\n- 使用“masked LM”（MLM）和“下一句话预测”（NSP）任务进行训练\n- [需要更多信息] 关于确切的硬件设置和培训时间",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، فيما يلي معلمات التدريب الفائقة لنموذج bert-base-multilingual-uncased:\n\n- تدرب على 102 لغة\n- 12 طبقة، 768 مخفي، 12 رأس، 110 مليون معلمة\n- تم تدريبه بحجم دفعة يبلغ 256 تسلسلًا (256 تسلسلًا * 512 رمزًا = 128,000 رمزًا/دفعة) لـ 1,000,000 خطوة، أي ما يقرب من 40 حقبة على مجموعة الكلمات المكونة من 3.3 مليار كلمة\n- تم استخدام مُحسِّن Adam بمعدل تعلم قدره 1e-4، وβ1 = 0.9، وβ2 = 0.999، وانخفاض وزن L2 بمقدار 0.01، وتهيئة معدل التعلم خلال أول 10000 خطوة، والانحلال الخطي لمعدل التعلم\n- احتمالية التسرب 0.1 على جميع الطبقات\n- تم استخدام تنشيط gelu بدلاً من relu القياسي\n- [بحاجة إلى مزيد من المعلومات] حول البيانات والنصوص الدقيقة لما قبل التدريب\n- تم التدريب على استخدام مهام \"LM المقنعة\" (MLM) و\"التنبؤ بالجملة التالية\" (NSP)\n- [يلزم مزيد من المعلومات] حول الإعداد الدقيق للأجهزة ووقت التدريب",
    "answer_French": "Sur la base des références fournies, voici les hyperparamètres de formation pour le modèle bert-base-multilingual-uncased :\n\n- Formé sur 102 langues\n- 12 couches, 768 cachées, 12 têtes, 110 millions de paramètres\n- Formé avec une taille de lot de 256 séquences (256 séquences * 512 jetons = 128 000 jetons/lot) pour 1 000 000 d'étapes, soit environ 40 époques sur le corpus de 3,3 milliards de mots\n- Utilisation de l'optimiseur Adam avec un taux d'apprentissage de 1e-4, β1 = 0,9, β2 = 0,999, décroissance du poids L2 de 0,01, échauffement du taux d'apprentissage au cours des 10 000 premières étapes et décroissance linéaire du taux d'apprentissage\n- Probabilité d'abandon de 0,1 sur toutes les couches\n- Utilisation de l'activation gelu plutôt que du relu standard\n- [Plus d'informations nécessaires] sur les données et le corpus exacts de pré-formation\n- Formé à l'aide des tâches « LM masqué » (MLM) et « prédiction de la phrase suivante » (NSP)\n- [Plus d'informations nécessaires] sur la configuration matérielle exacte et le temps de formation",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот гиперпараметры обучения для модели bert-base-multilingual-uncased:\n\n- Обучение на 102 языках\n- 12 слоев, 768 скрытых, 12 головок, 110M параметров\n- Обучение с использованием пакета размером 256 последовательностей (256 последовательностей * 512 токенов = 128 000 токенов/пакет) за 1 000 000 шагов, что составляет примерно 40 эпох для корпуса слов объемом 3,3 миллиарда.\n- Используемый оптимизатор Адама со скоростью обучения 1e-4, β1 = 0,9, β2 = 0,999, затухание веса L2 0,01, повышение скорости обучения на первых 10 000 шагов и линейное затухание скорости обучения.\n- Вероятность отсева 0,1 на всех слоях\n- Использована активация геля вместо стандартного релу.\n- [Требуется дополнительная информация] о точных данных и корпусе перед обучением.\n- Обучение с использованием задач «Маскированный LM» (MLM) и «Предсказание следующего предложения» (NSP).\n- [Требуется дополнительная информация] о точной настройке оборудования и времени обучения.",
    "answer_Japanese": "提供された参考資料に基づいて、bert-base-multilingual-uncased モデルのトレーニング ハイパーパラメーターを次に示します。\n\n- 102の言語でトレーニング済み\n- 12 レイヤー、768 隠し、12 ヘッド、110M パラメーター\n- 256 シーケンス (256 シーケンス * 512 トークン = 128,000 トークン/バッチ) のバッチ サイズで 1,000,000 ステップでトレーニングされます。これは、33 億単語コーパスの約 40 エポックに相当します。\n- 学習率 1e-4、β1 = 0.9、β2 = 0.999、L2 重み減衰 0.01、最初の 10,000 ステップにわたる学習率ウォームアップ、および学習率の線形減衰を持つ Adam オプティマイザーを使用しました。\n- すべてのレイヤーでドロップアウト確率が 0.1\n- 標準の relu ではなく gelu アクティベーションを使用しました\n- 正確な事前トレーニング データとコーパスに関する [さらなる情報が必要]\n- 「マスクされた LM」（MLM）および「次の文の予測」（NSP）タスクを使用してトレーニングされました\n- 正確なハードウェアのセットアップとトレーニング時間に関する [追加情報が必要]"
  },
  {
    "question": "speeds_sizes_times",
    "chain": "generation",
    "answer": "Based on the provided references, here are the details I can provide about the BERT-Base Multilingual Uncased model:\n\nModel Architecture:\n- 12-layer bidirectional Transformer encoder\n- 768 hidden units\n- 12 attention heads\n- 110M parameters\n\nLanguages:\n- Covers 102 languages\n\nPerformance:\n- Performs somewhat worse than single-language models for high-resource languages like English and Chinese\n- English XNLI result is lower than the 84.2 MultiNLI baseline due to using the multilingual model instead of English-only\n- Chinese XNLI result is 74.2, compared to 77.2 for a Chinese-only BERT-Base model\n\n[More Information Needed] for:\n- Throughput\n- Start or end training time\n- Checkpoint sizes\n\nThe references do not provide specific information about these aspects of the model training process. More details would be needed from the model development logs or records to include this information in the model card.",
    "answer_Chinese": "根据提供的参考资料，我可以提供以下有关 BERT-Base 多语言 Uncased 模型的详细信息：\n\n模型架构：\n- 12层双向Transformer编码器\n- 768 个隐藏单位\n- 12个注意力头\n- 110M参数\n\n语言：\n- 涵盖102种语言\n\n表现：\n- 对于英语和中文等高资源语言，其性能比单语言模型稍差\n- 由于使用多语言模型而不是纯英语模型，英语 XNLI 结果低于 84.2 MultiNLI 基线\n- 中文 XNLI 结果为 74.2，而纯中文 BERT-Base 模型的结果为 77.2\n\n[需要更多信息]：\n- 吞吐量\n- 开始或结束训练时间\n- 检查点大小\n\n这些参考文献没有提供有关模型训练过程这些方面的具体信息。需要从模型开发日志或记录中获取更多详细信息，才能将这些信息包含在模型卡中。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك التفاصيل التي يمكنني تقديمها حول نموذج BERT-Base Multilingual Uncased:\n\nالعمارة النموذجية:\n- تشفير محول ثنائي الاتجاه مكون من 12 طبقة\n- 768 وحدة مخفية\n- 12 رأس انتباه\n- 110 مليون معلمة\n\nاللغات:\n- يغطي 102 لغة\n\nأداء:\n- الأداء أسوأ إلى حد ما من نماذج اللغة الواحدة بالنسبة للغات عالية الموارد مثل الإنجليزية والصينية\n- نتيجة اللغة الإنجليزية XNLI أقل من خط الأساس 84.2 MultiNLI بسبب استخدام النموذج متعدد اللغات بدلاً من اللغة الإنجليزية فقط\n- نتيجة XNLI الصينية هي 74.2، مقارنة بـ 77.2 لنموذج BERT-Base الصيني فقط\n\n[مطلوب مزيد من المعلومات] من أجل:\n- الإنتاجية\n- بدء أو إنهاء وقت التدريب\n- أحجام نقاط التفتيش\n\nلا توفر المراجع معلومات محددة حول هذه الجوانب من عملية التدريب النموذجية. ستكون هناك حاجة إلى مزيد من التفاصيل من سجلات أو سجلات تطوير النموذج لتضمين هذه المعلومات في بطاقة النموذج.",
    "answer_French": "Sur la base des références fournies, voici les détails que je peux fournir sur le modèle BERT-Base Multilingual Uncased :\n\nArchitecture du modèle :\n- Encodeur transformateur bidirectionnel à 12 couches\n- 768 unités cachées\n- 12 têtes d'attention\n- 110 millions de paramètres\n\nLangues :\n- Couvre 102 langues\n\nPerformance:\n- Fonctionne un peu moins bien que les modèles monolingues pour les langues à ressources élevées comme l'anglais et le chinois\n- Le résultat XNLI en anglais est inférieur à la référence 84.2 MultiNLI en raison de l'utilisation du modèle multilingue au lieu de l'anglais uniquement.\n- Le résultat XNLI chinois est de 74,2, contre 77,2 pour un modèle BERT-Base uniquement chinois\n\n[Plus d'informations nécessaires] pour :\n- Débit\n- Début ou fin du temps de formation\n- Tailles des points de contrôle\n\nLes références ne fournissent pas d’informations spécifiques sur ces aspects du processus de formation du modèle. Plus de détails seraient nécessaires dans les journaux ou enregistrements de développement du modèle pour inclure ces informations dans la fiche modèle.",
    "answer_Russian": "На основании предоставленных ссылок я могу предоставить подробную информацию о многоязычной бескорпусной модели BERT-Base:\n\nМодельная архитектура:\n- 12-слойный двунаправленный трансформаторный кодер\n- 768 скрытых юнитов\n- 12 глав внимания\n- 110M параметров\n\nЯзыки:\n- Охватывает 102 языка\n\nПроизводительность:\n- Работает несколько хуже, чем одноязычные модели для языков с высоким уровнем ресурсов, таких как английский и китайский.\n- Результат XNLI на английском языке ниже базового показателя MultiNLI 84,2 из-за использования многоязычной модели вместо модели только на английском языке.\n- Результат XNLI для китайского языка составляет 74,2 по сравнению с 77,2 для модели BERT-Base, предназначенной только для Китая.\n\n[Требуется дополнительная информация] для:\n- Пропускная способность\n- Начало или окончание тренировки\n- Размеры КПП\n\nСсылки не предоставляют конкретной информации об этих аспектах процесса обучения модели. Для включения этой информации в карточку модели потребуется дополнительная информация из журналов или записей разработки модели.",
    "answer_Japanese": "提供された参考資料に基づいて、BERT ベースの多言語 Uncased モデルについて提供できる詳細は次のとおりです。\n\nモデルのアーキテクチャ:\n- 12層双方向トランスフォーマーエンコーダー\n- 768 個の隠しユニット\n- 12 個の注目ヘッド\n- 1億1000万のパラメータ\n\n言語:\n- 102の言語をカバー\n\nパフォーマンス：\n- 英語や中国語などの高リソース言語の場合、単一言語モデルよりもパフォーマンスが若干劣ります。\n- 英語のみではなく多言語モデルを使用しているため、英語 XNLI の結果は 84.2 MultiNLI ベースラインよりも低くなります。\n- 中国の XNLI の結果は 74.2 ですが、中国専用の BERT ベース モデルの場合は 77.2 です。\n\n[さらに詳しい情報が必要]:\n- スループット\n- トレーニングの開始時間または終了時間\n- チェックポイントのサイズ\n\n参考文献では、モデル トレーニング プロセスのこれらの側面に関する具体的な情報は提供されていません。この情報をモデル カードに含めるには、モデル開発ログまたは記録からさらに詳細な情報が必要になります。"
  },
  {
    "question": "testing_data",
    "chain": "generation",
    "answer": "Based on the provided references, the BERT-base-multilingual-uncased model likely evaluates on the following benchmarks and datasets:\n\n- GLUE (General Language Understanding Evaluation) benchmark, which is a collection of diverse natural language understanding tasks.\n\n- SQuAD v1.1 and SQuAD v2.0 question answering datasets. BERT models were fine-tuned for these tasks and achieved state-of-the-art performance.\n\nHowever, the references do not provide specific details on the exact performance of the BERT-base-multilingual-uncased model on these benchmarks. The results mentioned are for BERT models in general, such as BERT-base and BERT-large.\n\n[More Information Needed] on the specific evaluation results and metrics for the BERT-base-multilingual-uncased model on the mentioned benchmarks and datasets.",
    "answer_Chinese": "根据提供的参考资料，BERT-base-multilingual-uncased 模型可能会根据以下基准和数据集进行评估：\n\n- GLUE（通用语言理解评估）基准，它是各种自然语言理解任务的集合。\n\n- SQuAD v1.1 和 SQuAD v2.0 问答数据集。 BERT 模型针对这些任务进行了微调，并实现了最先进的性能。\n\n然而，参考文献并未提供有关 BERT-base-multilingual-uncased 模型在这些基准测试上的确切性能的具体细节。提到的结果适用于一般的 BERT 模型，例如 BERT-base 和 BERT-large。\n\n[需要更多信息]关于 BERT-base-multilingual-uncased 模型在上述基准和数据集上的具体评估结果和指标。",
    "answer_Arabic": "استنادًا إلى المراجع المقدمة، من المحتمل أن يتم تقييم نموذج BERT-base-multilingual-uncased بناءً على المعايير ومجموعات البيانات التالية:\n\n- معيار GLUE (تقييم فهم اللغة العامة)، وهو عبارة عن مجموعة من المهام المتنوعة لفهم اللغة الطبيعية.\n\n- SQuAD v1.1 وSQuAD v2.0 مجموعات البيانات التي تجيب على الأسئلة. تم ضبط نماذج BERT لهذه المهام وحققت أداءً متطورًا.\n\nومع ذلك، لا تقدم المراجع تفاصيل محددة حول الأداء الدقيق لنموذج BERT متعدد اللغات غير المغطى على هذه المعايير. النتائج المذكورة تخص نماذج BERT بشكل عام، مثل BERT-base وBERT-large.\n\n[بحاجة إلى مزيد من المعلومات] حول نتائج ومقاييس التقييم المحددة لنموذج BERT متعدد اللغات غير المغطى على المعايير ومجموعات البيانات المذكورة.",
    "answer_French": "Sur la base des références fournies, le modèle BERT-base-multilingual-uncased est probablement évalué sur les références et ensembles de données suivants :\n\n- Benchmark GLUE (General Language Understanding Evaluation), qui est un ensemble de diverses tâches de compréhension du langage naturel.\n\n- Ensembles de données de réponses aux questions SQuAD v1.1 et SQuAD v2.0. Les modèles BERT ont été optimisés pour ces tâches et ont atteint des performances de pointe.\n\nCependant, les références ne fournissent pas de détails spécifiques sur les performances exactes du modèle BERT-base-multilingual-uncased sur ces tests. Les résultats mentionnés concernent les modèles BERT en général, tels que BERT-base et BERT-large.\n\n[Plus d'informations nécessaires] sur les résultats et les mesures d'évaluation spécifiques du modèle BERT-base-multilingual-uncased sur les références et ensembles de données mentionnés.",
    "answer_Russian": "Судя по предоставленным ссылкам, модель BERT-base-multilingual-uncased, вероятно, оценивается по следующим критериям и наборам данных:\n\n- Тест GLUE (Общая оценка понимания языка), который представляет собой набор разнообразных задач на понимание естественного языка.\n\n- Наборы данных для ответов на вопросы SQuAD v1.1 и SQuAD v2.0. Модели BERT были точно настроены для этих задач и достигли самых современных характеристик.\n\nОднако в ссылках не приводятся конкретные сведения о точной производительности модели BERT-base-multilingual-uncaseed в этих тестах. Упомянутые результаты относятся к моделям BERT в целом, таким как базовый BERT и большой BERT.\n\n[Требуется дополнительная информация] о конкретных результатах оценки и показателях для модели BERT-base-multilingual-uncased по упомянутым критериям и наборам данных.",
    "answer_Japanese": "提供された参照に基づいて、BERT ベースの多言語非大文字小文字モデルは、次のベンチマークとデータセットに基づいて評価される可能性があります。\n\n- GLUE (一般言語理解評価) ベンチマーク。さまざまな自然言語理解タスクの集合です。\n\n- SQuAD v1.1 および SQuAD v2.0 の質問応答データセット。 BERT モデルはこれらのタスク向けに微調整され、最先端のパフォーマンスを実現しました。\n\nただし、参考資料には、これらのベンチマークにおける BERT ベースの多言語非大文字小文字モデルの正確なパフォーマンスに関する具体的な詳細は記載されていません。記載されている結果は、BERT ベースや BERT-large などの BERT モデル全般に関するものです。\n\n言及されたベンチマークとデータセットにおける BERT ベースの多言語非ケース化モデルの特定の評価結果と指標に関する [追加情報が必要]。"
  },
  {
    "question": "testing_factors",
    "chain": "generation",
    "answer": "Based on the provided references, here are some key characteristics that may influence the behavior of the bert-base-multilingual-uncased model:\n\nLanguage Representation:\n- The model covers 102 languages, with the training data coming from the largest Wikipedias for each language. \n- However, the size of Wikipedia varies greatly by language, so low-resource languages may be under-represented in the model.\n- To balance this, exponentially smoothed weighting was used during pre-training data creation to under-sample high-resource languages like English and over-sample low-resource languages (References 2-4).\n\nTokenization and Vocabulary:\n- For languages other than English, lower casing, accent removal, punctuation splitting, and whitespace tokenization was applied.\n- Removing accent markers provides benefits by reducing vocabulary size, but may introduce some ambiguity. The strong contextual models of BERT should compensate for this.\n\nModel Size and Architecture: \n- The model has 12 layers, 768 hidden units, 12 attention heads, and 110M parameters total.\n- [More Information Needed] on how model size impacts downstream task performance for this specific model.\n\nEvaluation across Subgroups:\n- [More Information Needed] The references do not provide details on evaluating the model's performance disaggregated across different languages or subgroups. This would be important to assess to uncover any performance disparities.\n\nIn summary, the key factors that may influence the model's behavior are the representation of different languages in the training data, the tokenization/vocabulary approach used, and the model architecture. More information would be needed to comment on performance across different subgroups.",
    "answer_Chinese": "根据提供的参考资料，以下是可能影响 bert-base-multilingual-uncased 模型行为的一些关键特征：\n\n语言表示：\n- 该模型涵盖 102 种语言，训练数据来自每种语言最大的维基百科。 \n- 然而，维基百科的大小因语言而异，因此资源匮乏的语言在模型中的代表性可能不足。\n- 为了平衡这一点，在预训练数据创建期间使用指数平滑权重，对英语等高资源语言进行欠采样，对低资源语言进行过采样（参考文献 2-4）。\n\n标记化和词汇：\n- 对于英语以外的语言，应用了小写、重音去除、标点符号分割和空格标记化。\n- 删除重音标记可以减少词汇量，但可能会带来一些歧义。 BERT 强大的上下文模型应该可以弥补这一点。\n\n模型尺寸和架构： \n- 该模型有 12 层，768 个隐藏单元，12 个注意力头，总共 110M 个参数。\n- [需要更多信息]关于模型大小如何影响该特定模型的下游任务性能。\n\n跨子组的评估：\n- [需要更多信息] 参考文献未提供有关评估跨不同语言或子组分类的模型性能的详细信息。这对于评估以发现任何性能差异非常重要。\n\n总之，可能影响模型行为的关键因素是训练数据中不同语言的表示、使用的标记化/词汇方法以及模型架构。需要更多信息来评论不同小组的表现。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، فيما يلي بعض الخصائص الأساسية التي قد تؤثر على سلوك نموذج bert-base-multilingual-uncased:\n\nتمثيل اللغة:\n- يغطي النموذج 102 لغة، وتأتي بيانات التدريب من أكبر ويكيبيديا لكل لغة. \n- ومع ذلك، فإن حجم ويكيبيديا يختلف بشكل كبير حسب اللغة، لذلك قد تكون اللغات منخفضة الموارد ممثلة تمثيلا ناقصا في النموذج.\n- لتحقيق التوازن في ذلك، تم استخدام الترجيح السلس بشكل كبير أثناء إنشاء بيانات ما قبل التدريب لتقليل عينة اللغات عالية الموارد مثل اللغة الإنجليزية والإفراط في أخذ عينات من اللغات منخفضة الموارد (المراجع 2-4).\n\nالترميز والمفردات:\n- بالنسبة للغات غير الإنجليزية، تم تطبيق الأحرف الصغيرة، وإزالة علامات التمييز، وتقسيم علامات الترقيم، ورموز المسافات البيضاء.\n- إزالة علامات التشكيل توفر فوائد من خلال تقليل حجم المفردات، ولكنها قد تؤدي إلى بعض الغموض. يجب أن تعوض النماذج السياقية القوية لـ BERT عن ذلك.\n\nحجم النموذج والهندسة المعمارية: \n- يحتوي النموذج على 12 طبقة، و768 وحدة مخفية، و12 رأس انتباه، وإجمالي 110 مليون معلمة.\n- [مطلوب مزيد من المعلومات] حول كيفية تأثير حجم النموذج على أداء المهام النهائية لهذا النموذج المحدد.\n\nالتقييم عبر المجموعات الفرعية:\n- [يلزم مزيد من المعلومات] لا تقدم المراجع تفاصيل حول تقييم أداء النموذج مصنفًا عبر اللغات أو المجموعات الفرعية المختلفة. سيكون من المهم تقييم ذلك للكشف عن أي تباينات في الأداء.\n\nباختصار، العوامل الرئيسية التي قد تؤثر على سلوك النموذج هي تمثيل اللغات المختلفة في بيانات التدريب، ونهج الترميز/المفردات المستخدم، وبنية النموذج. وستكون هناك حاجة إلى مزيد من المعلومات للتعليق على الأداء عبر مجموعات فرعية مختلفة.",
    "answer_French": "Sur la base des références fournies, voici quelques caractéristiques clés qui peuvent influencer le comportement du modèle bert-base-multilingual-uncased :\n\nReprésentation linguistique :\n- Le modèle couvre 102 langues, les données de formation provenant des plus grandes Wikipédias pour chaque langue. \n- Cependant, la taille de Wikipédia varie considérablement selon la langue, de sorte que les langues à faibles ressources peuvent être sous-représentées dans le modèle.\n- Pour équilibrer cela, une pondération lissée de manière exponentielle a été utilisée lors de la création de données préalables à la formation pour sous-échantillonner les langues à ressources élevées comme l'anglais et sur-échantillonner les langues à faibles ressources (références 2 à 4).\n\nTokenisation et vocabulaire :\n- Pour les langues autres que l'anglais, la casse, la suppression des accents, la division de la ponctuation et la tokenisation des espaces ont été appliquées.\n- La suppression des marqueurs d'accent présente des avantages en réduisant la taille du vocabulaire, mais peut introduire une certaine ambiguïté. Les modèles contextuels forts du BERT devraient compenser cela.\n\nTaille et architecture du modèle : \n- Le modèle comporte 12 couches, 768 unités cachées, 12 têtes d'attention et 110 millions de paramètres au total.\n- [Plus d'informations nécessaires] sur l'impact de la taille du modèle sur les performances des tâches en aval pour ce modèle spécifique.\n\nÉvaluation dans les sous-groupes :\n- [Plus d'informations nécessaires] Les références ne fournissent pas de détails sur l'évaluation des performances du modèle ventilées selon différentes langues ou sous-groupes. Il serait important d’évaluer cela pour découvrir toute disparité de performance.\n\nEn résumé, les facteurs clés qui peuvent influencer le comportement du modèle sont la représentation de différents langages dans les données de formation, l'approche de tokenisation/vocabulaire utilisée et l'architecture du modèle. Plus d’informations seraient nécessaires pour commenter les performances des différents sous-groupes.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот некоторые ключевые характеристики, которые могут повлиять на поведение модели bert-base-multilingual-uncaseed:\n\nЯзыковое представление:\n- Модель охватывает 102 языка, данные для обучения поступают из крупнейших Википедий для каждого языка. \n- Однако размер Википедии сильно различается в зависимости от языка, поэтому языки с ограниченными ресурсами могут быть недостаточно представлены в модели.\n- Чтобы сбалансировать это, во время создания данных перед обучением использовалось экспоненциально сглаженное взвешивание, чтобы занижать выборку языков с высоким уровнем ресурсов, таких как английский, и увеличивать выборку языков с низким уровнем ресурсов (ссылки 2-4).\n\nТокенизация и словарь:\n- Для языков, отличных от английского, был применен нижний регистр, удаление акцента, разделение знаков препинания и токенизация пробелов.\n- Удаление акцентных маркеров дает преимущества за счет уменьшения размера словарного запаса, но может привести к некоторой двусмысленности. Это должны компенсировать сильные контекстуальные модели BERT.\n\nРазмер модели и архитектура: \n- Модель имеет 12 слоев, 768 скрытых блоков, 12 головок внимания и всего 110M параметров.\n- [Требуется дополнительная информация] о том, как размер модели влияет на производительность последующих задач для этой конкретной модели.\n\nОценка по подгруппам:\n- [Требуется дополнительная информация] В ссылках не приводятся подробные сведения об оценке эффективности модели с разбивкой по разным языкам или подгруппам. Это было бы важно оценить, чтобы выявить любые различия в производительности.\n\nТаким образом, ключевыми факторами, которые могут повлиять на поведение модели, являются представление разных языков в обучающих данных, используемый подход токенизации/словаря и архитектура модели. Чтобы прокомментировать эффективность работы различных подгрупп, потребуется дополнительная информация.",
    "answer_Japanese": "提供された参考資料に基づいて、bert-base-multilingual-uncased モデルの動作に影響を与える可能性のあるいくつかの主要な特性を次に示します。\n\n言語表現:\n- モデルは 102 の言語をカバーしており、トレーニング データは各言語の最大の Wikipedia から取得されています。 \n- ただし、Wikipedia のサイズは言語によって大きく異なるため、リソースの少ない言語はモデル内で過小評価される可能性があります。\n- これをバランスさせるために、事前トレーニング データの作成中に指数平滑化された重み付けが使用され、英語などの高リソース言語をアンダーサンプリングし、低リソース言語をオーバーサンプリングしました (参考文献 2-4)。\n\nトークン化と語彙:\n- 英語以外の言語の場合、小文字、アクセントの削除、句読点の分割、および空白のトークン化が適用されました。\n- アクセント マーカーを削除すると、語彙サイズが減少するという利点がありますが、曖昧さが生じる可能性があります。 BERT の強力なコンテキスト モデルがこれを補う必要があります。\n\nモデルのサイズとアーキテクチャ: \n- モデルには 12 層、768 個の隠れユニット、12 個のアテンション ヘッド、合計 1 億 1000 万個のパラメータがあります。\n- モデル サイズがこの特定のモデルの下流タスクのパフォーマンスにどのような影響を与えるかに関する [追加情報が必要]。\n\nサブグループ間の評価:\n- [さらなる情報が必要] 参考文献には、さまざまな言語またはサブグループにわたって分類されたモデルのパフォーマンスの評価に関する詳細が記載されていません。これは、パフォーマンスの差異を明らかにするために評価することが重要です。\n\n要約すると、モデルの動作に影響を与える可能性のある主な要素は、トレーニング データ内のさまざまな言語の表現、使用されるトークン化/語彙アプローチ、およびモデル アーキテクチャです。さまざまなサブグループにわたるパフォーマンスについてコメントするには、より多くの情報が必要です。"
  },
  {
    "question": "testing_metrics",
    "chain": "generation",
    "answer": "Based on the provided references, the key evaluation metrics for the bert-base-multilingual-uncased model appear to be:\n\n- F1 score on SQuAD v1.1 and v2.0 question answering tasks\n- Accuracy on GLUE benchmark tasks like MNLI \n- Accuracy on XNLI cross-lingual natural language inference dataset\n\nThe references mention these metrics being used to compare BERT's performance to prior state-of-the-art models. However, more specific information would be needed on the tradeoffs and error analysis for the multilingual uncased variant of BERT. The references focus more on the general BERT architecture rather than specifics of this multilingual model.\n\n[More Information Needed] on the particular strengths, weaknesses and tradeoffs of using the multilingual uncased model compared to other BERT variants or models. Details on its performance across different languages and error patterns would help round out the evaluation metrics section.",
    "answer_Chinese": "根据提供的参考资料，bert-base-multilingual-uncased 模型的关键评估指标如下：\n\n- SQuAD v1.1 和 v2.0 问答任务的 F1 分数\n- MNLI 等 GLUE 基准测试任务的准确性 \n- XNLI跨语言自然语言推理数据集的准确性\n\n参考文献提到这些指标用于将 BERT 的性能与之前最先进的模型进行比较。然而，关于 BERT 多语言无大小写变体的权衡和错误分析，需要更具体的信息。这些参考文献更多地关注一般的 BERT 架构，而不是这种多语言模型的细节。\n\n[需要更多信息] 与其他 BERT 变体或模型相比，使用多语言 uncased 模型的特定优点、缺点和权衡。有关其在不同语言和错误模式下的性能的详细信息将有助于完善评估指标部分。",
    "answer_Arabic": "استنادا إلى المراجع المقدمة، يبدو أن مقاييس التقييم الرئيسية لنموذج bert-base-multilingual-uncased هي:\n\n- درجة F1 في مهام الإجابة على الأسئلة SQuAD v1.1 وv2.0\n- الدقة في المهام المعيارية GLUE مثل MNLI \n- الدقة في مجموعة بيانات استدلال اللغة الطبيعية عبر اللغات XNLI\n\nتشير المراجع إلى استخدام هذه المقاييس لمقارنة أداء BERT بالنماذج الحديثة السابقة. ومع ذلك، ستكون هناك حاجة إلى معلومات أكثر تحديدًا بشأن المفاضلات وتحليل الأخطاء للمتغير غير المغطى متعدد اللغات لـ BERT. تركز المراجع بشكل أكبر على بنية BERT العامة بدلاً من التركيز على تفاصيل هذا النموذج متعدد اللغات.\n\n[بحاجة إلى مزيد من المعلومات] حول نقاط القوة والضعف والمقايضات الخاصة باستخدام النموذج غير المغطى متعدد اللغات مقارنة بمتغيرات أو نماذج BERT الأخرى. من شأن التفاصيل المتعلقة بأدائه عبر اللغات المختلفة وأنماط الأخطاء أن تساعد في استكمال قسم مقاييس التقييم.",
    "answer_French": "Sur la base des références fournies, les principales mesures d'évaluation du modèle bert-base-multilingual-uncased semblent être :\n\n- Score F1 sur les tâches de réponse aux questions SQuAD v1.1 et v2.0\n- Précision sur les tâches de référence GLUE comme MNLI \n- Précision sur l'ensemble de données d'inférence en langage naturel multilingue XNLI\n\nLes références mentionnent que ces mesures sont utilisées pour comparer les performances de BERT aux modèles de pointe antérieurs. Cependant, des informations plus spécifiques seraient nécessaires sur les compromis et l'analyse des erreurs pour la variante multilingue sans boîtier de BERT. Les références se concentrent davantage sur l'architecture générale du BERT plutôt que sur les spécificités de ce modèle multilingue.\n\n[Plus d'informations nécessaires] sur les forces, les faiblesses et les compromis particuliers de l'utilisation du modèle multilingue sans boîtier par rapport à d'autres variantes ou modèles BERT. Des détails sur ses performances dans différentes langues et modèles d’erreurs aideraient à compléter la section des mesures d’évaluation.",
    "answer_Russian": "Судя по предоставленным ссылкам, ключевыми показателями оценки модели bert-base-multilingual-uncased являются:\n\n- Оценка F1 за задания на ответы на вопросы SQuAD v1.1 и v2.0.\n- Точность при выполнении тестов GLUE, таких как MNLI. \n- Точность набора данных XNLI для межъязыкового вывода естественного языка.\n\nВ ссылках упоминается, что эти показатели используются для сравнения производительности BERT с предыдущими современными моделями. Однако потребуется более конкретная информация о компромиссах и анализе ошибок для многоязычного бескорпусного варианта BERT. Ссылки больше сосредоточены на общей архитектуре BERT, а не на особенностях этой многоязычной модели.\n\n[Требуется дополнительная информация] о конкретных сильных и слабых сторонах и недостатках использования многоязычной бескорпусной модели по сравнению с другими вариантами или моделями BERT. Подробная информация о его производительности на разных языках и шаблонах ошибок поможет завершить раздел показателей оценки.",
    "answer_Japanese": "提供された参考資料に基づくと、bert-base-multilingual-uncased モデルの主要な評価指標は次のようになります。\n\n- SQuAD v1.1 および v2.0 の質問応答タスクの F1 スコア\n- MNLI などの GLUE ベンチマーク タスクの精度 \n- XNLI 言語間自然言語推論データセットの精度\n\n参考文献には、BERT のパフォーマンスを以前の最先端のモデルと比較するためにこれらの指標が使用されていることが記載されています。ただし、BERT の多言語大文字小文字区別のバリアントのトレードオフとエラー分析については、より具体的な情報が必要です。参考資料では、この多言語モデルの詳細ではなく、一般的な BERT アーキテクチャに重点を置いています。\n\n他の BERT バリアントまたはモデルと比較した、多言語大文字小文字なしモデルを使用する場合の特定の長所、短所、トレードオフに関する [さらなる情報が必要]。さまざまな言語でのパフォーマンスとエラー パターンの詳細は、評価指標のセクションを完成させるのに役立ちます。"
  },
  {
    "question": "results",
    "chain": "generation",
    "answer": "Based on the provided references, here is the evaluation information I could find for the bert-base-multilingual-uncased model:\n\nEvaluation Dataset: XNLI (dev and test sets translated into 15 languages)\n\nEvaluation Metric: Accuracy\n\nResults:\n- English accuracy is lower than the 84.2 MultiNLI baseline because the multilingual model performs somewhat worse than an English-only model for high-resource languages.\n- Chinese accuracy is 74.2, which is 3% lower than a Chinese-only BERT-Base model at 77.2.\n\nOther Notes:\n- The model has 102 languages, 12 layers, 768 hidden units, 12 attention heads, and 110M parameters. \n- A newer recommended version is BERT-Base Multilingual Cased which covers 104 languages.\n- The training data for non-English languages was machine translated, so it's unclear how much lower accuracy is due to translation quality vs pre-trained model quality.\n\n[More Information Needed] on specific accuracy results for the other 13 languages in XNLI besides English and Chinese.",
    "answer_Chinese": "根据提供的参考资料，以下是我可以找到的 bert-base-multilingual-uncased 模型的评估信息：\n\n评估数据集：XNLI（开发和测试集翻译成 15 种语言）\n\n评估指标：准确性\n\n结果：\n- 英语准确率低于 84.2 MultiNLI 基线，因为多语言模型的性能比高资源语言的纯英语模型稍差。\n- 中文准确率为 74.2，比纯中文 BERT-Base 模型的 77.2 低 3%。\n\n其他注意事项：\n- 该模型有 102 种语言、12 个层、768 个隐藏单元、12 个注意力头和 110M 个参数。 \n- 较新的推荐版本是 BERT-Base Multilingual Cased，涵盖 104 种语言。\n- 非英语语言的训练数据是机器翻译的，因此尚不清楚翻译质量与预训练模型质量相比准确率降低了多少。\n\n[需要更多信息]除英语和中文外，XNLI 中其他 13 种语言的具体准确度结果。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك معلومات التقييم التي يمكنني العثور عليها لنموذج bert-base-multilingual-uncased:\n\nمجموعة بيانات التقييم: XNLI (مجموعات التطوير والاختبار مترجمة إلى 15 لغة)\n\nمقياس التقييم: الدقة\n\nنتائج:\n- دقة اللغة الإنجليزية أقل من خط الأساس 84.2 MultiNLI لأن أداء النموذج متعدد اللغات أسوأ إلى حد ما من نموذج اللغة الإنجليزية فقط للغات عالية الموارد.\n- الدقة الصينية هي 74.2، وهي أقل بنسبة 3% من الطراز الصيني الوحيد BERT-Base الذي يبلغ 77.2.\n\nالملاحظات الأخرى:\n- يحتوي النموذج على 102 لغة، و12 طبقة، و768 وحدة مخفية، و12 رأس انتباه، و110 مليون معلمة. \n- الإصدار الأحدث الموصى به هو BERT-Base Multilingual Cased والذي يغطي 104 لغة.\n- تمت ترجمة بيانات التدريب للغات غير الإنجليزية آليًا، لذلك من غير الواضح مدى انخفاض الدقة بسبب جودة الترجمة مقابل جودة النموذج المدرب مسبقًا.\n\n[مطلوب مزيد من المعلومات] حول نتائج الدقة المحددة للغات الـ 13 الأخرى في XNLI إلى جانب الإنجليزية والصينية.",
    "answer_French": "Sur la base des références fournies, voici les informations d'évaluation que j'ai pu trouver pour le modèle bert-base-multilingual-uncased :\n\nEnsemble de données d'évaluation : XNLI (ensembles de développement et de test traduits en 15 langues)\n\nMétrique d'évaluation : précision\n\nRésultats:\n- La précision de l'anglais est inférieure à la référence 84,2 MultiNLI car le modèle multilingue est un peu moins performant qu'un modèle uniquement anglais pour les langues à ressources élevées.\n- La précision chinoise est de 74,2, soit 3 % de moins qu'un modèle BERT-Base uniquement chinois de 77,2.\n\nAutres notes:\n- Le modèle comporte 102 langues, 12 couches, 768 unités cachées, 12 têtes d'attention et 110 millions de paramètres. \n- Une version plus récente recommandée est BERT-Base Multilingual Cased qui couvre 104 langues.\n- Les données de formation pour les langues autres que l'anglais ont été traduites automatiquement. Il est donc difficile de savoir dans quelle mesure la précision est inférieure en raison de la qualité de la traduction par rapport à la qualité du modèle pré-entraîné.\n\n[Plus d'informations nécessaires] sur les résultats de précision spécifiques pour les 13 autres langues du XNLI en plus de l'anglais et du chinois.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот оценочная информация, которую я смог найти для модели bert-base-multilingual-uncaseed:\n\nНабор оценочных данных: XNLI (наборы для разработки и тестирования переведены на 15 языков)\n\nПоказатель оценки: точность\n\nПолученные результаты:\n- Точность английского языка ниже базового уровня MultiNLI 84,2, поскольку многоязычная модель работает несколько хуже, чем модель только на английском языке для языков с высоким уровнем ресурсов.\n- Китайская точность составляет 74,2, что на 3% ниже, чем у китайской модели BERT-Base с точностью 77,2.\n\nДругие примечания:\n- Модель имеет 102 языка, 12 слоев, 768 скрытых единиц, 12 головок внимания и 110M параметров. \n- Более новая рекомендуемая версия — BERT-Base Multilingual Cased, которая поддерживает 104 языка.\n- Данные обучения для языков, отличных от английского, были переведены машиной, поэтому неясно, насколько меньшая точность обусловлена ​​качеством перевода по сравнению с качеством предварительно обученной модели.\n\n[Требуется дополнительная информация] о конкретных результатах точности для других 13 языков в XNLI, помимо английского и китайского.",
    "answer_Japanese": "提供された参考資料に基づいて、bert-base-multilingual-uncased モデルに関して見つけることができた評価情報は次のとおりです。\n\n評価データセット: XNLI (15 言語に翻訳された開発およびテスト セット)\n\n評価指標: 精度\n\n結果：\n- 高リソース言語では多言語モデルのパフォーマンスが英語のみのモデルよりも若干劣るため、英語の精度は 84.2 MultiNLI ベースラインよりも低くなります。\n- 中国の精度は 74.2 で、中国専用の BERT ベース モデルの 77.2 より 3% 低くなります。\n\nその他の注意事項:\n- モデルには 102 の言語、12 のレイヤー、768 の隠れユニット、12 のアテンション ヘッド、および 1 億 1000 万のパラメーターがあります。 \n- 新しい推奨バージョンは、104 言語をカバーする BERT-Base Multilingual Cased です。\n- 英語以外の言語のトレーニング データは機械翻訳されているため、翻訳の品質と事前トレーニングされたモデルの品質によって精度がどの程度低下するかは不明です。\n\n英語と中国語以外の XNLI の他の 13 言語の具体的な精度結果については [詳細情報が必要]。"
  },
  {
    "question": "results_summary",
    "chain": "generation",
    "answer": "Based on the provided references, here is a summary of the evaluation results for the bert-base-multilingual-uncased model:\n\nThe model was evaluated on the XNLI dataset, which is a version of MultiNLI translated into 15 languages. The training set was machine translated.\n\nResults on 6 languages:\n- English: 81.4\n- Chinese: 74.2 \n- [More Information Needed for other 4 languages]\n\nThe multilingual uncased model performs slightly worse than a Chinese-only BERT-Base model on Chinese (74.2 vs 77.2).\n\nDifferent fine-tuning approaches were compared:\n- Translate Train Uncased: 81.4 (en), 74.2 (zh), 77.3 (fr), 75.2 (es), 70.5 (de), 61.7 (ar)  \n- Translate Test Uncased: 81.4 (en), 70.1 (zh), 74.9 (fr), 74.4 (es), 70.4 (de), 62.1 (ar)\n- Zero Shot Uncased: 81.4 (en), 63.8 (zh), 74.3 (fr), 70.5 (es), 62.1 (de), 58.3 (ar)\n\nZero Shot means fine-tuning only on English and evaluating on the other languages without translation.\n\nThe original multilingual uncased model covers 102 languages with 12-layers, 768-hidden, 12-heads, and 110M parameters. A newer version covers 104 languages.\n\n[More Information Needed] on other downstream tasks like question answering.",
    "answer_Chinese": "根据提供的参考资料，以下是 bert-base-multilingual-uncased 模型的评估结果总结：\n\n该模型在 XNLI 数据集上进行评估，该数据集是翻译成 15 种语言的 MultiNLI 版本。训练集是机器翻译的。\n\n6 种语言的结果：\n- 英语：81.4\n- 中文：74.2 \n- [其他 4 种语言需要更多信息]\n\n多语言 uncased 模型在中文上的表现比纯中文 BERT-Base 模型稍差（74.2 vs 77.2）。\n\n比较了不同的微调方法：\n- 翻译 Train Uncased：81.4 (en)、74.2 (zh)、77.3 (fr)、75.2 (es)、70.5 (de)、61.7 (ar)  \n- 翻译测试无外壳：81.4 (en)、70.1 (zh)、74.9 (fr)、74.4 (es)、70.4 (de)、62.1 (ar)\n- 零射击无外壳：81.4 (en)、63.8 (zh)、74.3 (fr)、70.5 (es)、62.1 (de)、58.3 (ar)\n\n零射击意味着仅对英语进行微调，并在不翻译的情况下对其他语言进行评估。\n\n原始多语言uncased模型涵盖102种语言，12层、768个隐藏、12个头和110M参数。新版本涵盖 104 种语言。\n\n[需要更多信息]有关其他下游任务（例如问答）。",
    "answer_Arabic": "بناءً على المراجع المقدمة، فيما يلي ملخص لنتائج التقييم لنموذج bert-base-multilingual-uncased:\n\nتم تقييم النموذج على مجموعة بيانات XNLI، وهي نسخة من MultiNLI مترجمة إلى 15 لغة. تمت ترجمة مجموعة التدريب آليًا.\n\nالنتائج على 6 لغات:\n- اللغة الإنجليزية: 81.4\n- الصينية: 74.2 \n- [مزيد من المعلومات مطلوبة للغات الأربع الأخرى]\n\nأداء النموذج غير المغطى متعدد اللغات أسوأ قليلاً من نموذج BERT-Base الصيني فقط على اللغة الصينية (74.2 مقابل 77.2).\n\nوتمت مقارنة أساليب الضبط المختلفة:\n- ترجمة القطار غير المغطى: 81.4 (en)، 74.2 (zh)، 77.3 (fr)، 75.2 (es)، 70.5 (de)، 61.7 (ar)  \n- اختبار الترجمة غير المغطى: 81.4 (en)، 70.1 (zh)، 74.9 (fr)، 74.4 (es)، 70.4 (de)، 62.1 (ar)\n- صفر طلقة غير مغطاة: 81.4 (إنجليز)، 63.8 (zh)، 74.3 (فرنسا)، 70.5 (إسبانيا)، 62.1 (دي)، 58.3 (عربية)\n\nتعني Zero Shot الضبط الدقيق للغة الإنجليزية فقط والتقييم على اللغات الأخرى دون ترجمة.\n\nيغطي النموذج الأصلي غير المغطى متعدد اللغات 102 لغة مع 12 طبقة، و768 مخفيًا، و12 رأسًا، و110 مليون معلمة. إصدار أحدث يغطي 104 لغة.\n\n[يلزم مزيد من المعلومات] في المهام النهائية الأخرى مثل الإجابة على الأسئلة.",
    "answer_French": "Sur la base des références fournies, voici un résumé des résultats de l'évaluation du modèle bert-base-multilingual-uncased :\n\nLe modèle a été évalué sur l'ensemble de données XNLI, qui est une version de MultiNLI traduite en 15 langues. L’ensemble de formation a été traduit automatiquement.\n\nRésultats sur 6 langues :\n- Anglais : 81,4\n- Chinois : 74,2 \n- [Plus d'informations nécessaires pour les 4 autres langues]\n\nLe modèle multilingue sans boîtier fonctionne légèrement moins bien qu'un modèle BERT-Base uniquement chinois en chinois (74,2 contre 77,2).\n\nDifférentes approches de réglage fin ont été comparées :\n- Traduire Train Uncased : 81.4 (en), 74.2 (zh), 77.3 (fr), 75.2 (es), 70.5 (de), 61.7 (ar)  \n- Traduire Test Uncased : 81.4 (en), 70.1 (zh), 74.9 (fr), 74.4 (es), 70.4 (de), 62.1 (ar)\n- Zero Shot Uncased : 81.4 (en), 63.8 (zh), 74.3 (fr), 70.5 (es), 62.1 (de), 58.3 (ar)\n\nZero Shot signifie peaufiner uniquement l'anglais et évaluer les autres langues sans traduction.\n\nLe modèle multilingue original sans boîtier couvre 102 langues avec 12 couches, 768 cachées, 12 têtes et 110 millions de paramètres. Une version plus récente couvre 104 langues.\n\n[Plus d'informations nécessaires] sur d'autres tâches en aval telles que la réponse aux questions.",
    "answer_Russian": "На основе предоставленных ссылок ниже приводится сводка результатов оценки модели bert-base-multilingual-uncases:\n\nМодель была оценена на наборе данных XNLI, который представляет собой версию MultiNLI, переведенную на 15 языков. Обучающий набор был переведен машинно.\n\nРезультаты на 6 языках:\n- Английский: 81,4\n- Китайский: 74,2 \n- [Необходима дополнительная информация для других 4 языков]\n\nМногоязычная бескорпусная модель работает на китайском языке немного хуже, чем модель BERT-Base, предназначенная только для китайского языка (74,2 против 77,2).\n\nБыли сравнены различные подходы к тонкой настройке:\n- Translate Train Uncased: 81.4 (en), 74.2 (zh), 77.3 (fr), 75.2 (es), 70.5 (de), 61.7 (ar)  \n- Translate Test Uncased: 81.4 (en), 70.1 (zh), 74.9 (fr), 74.4 (es), 70.4 (de), 62.1 (ar)\n- Zero Shot без корпуса: 81,4 (ан), 63,8 (ж), 74,3 (фр), 70,5 (ис), 62,1 (де), 58,3 (ар)\n\nZero Shot означает доработку только на английском языке и оценку на других языках без перевода.\n\nИсходная многоязычная модель без регистра охватывает 102 языка с 12 слоями, 768 скрытыми, 12 головками и 110M параметрами. Новая версия поддерживает 104 языка.\n\n[Требуется дополнительная информация] по другим последующим задачам, таким как ответы на вопросы.",
    "answer_Japanese": "提供された参考資料に基づいて、bert-base-multilingual-uncased モデルの評価結果の概要を次に示します。\n\nモデルは、15 の言語に翻訳された MultiNLI のバージョンである XNLI データセットで評価されました。トレーニング セットは機械翻訳されました。\n\n6 つの言語に関する結果:\n- 英語: 81.4\n- 中国語: 74.2 \n- [他の 4 言語についてはさらに詳しい情報が必要です]\n\n多言語大文字小文字なしモデルのパフォーマンスは、中国語のみの BERT-Base モデルよりもわずかに劣ります (74.2 対 77.2)。\n\nさまざまな微調整アプローチを比較しました。\n- ケースなしの電車の翻訳: 81.4 (en)、74.2 (zh)、77.3 (fr)、75.2 (es)、70.5 (de)、61.7 (ar)  \n- ケースなしの翻訳テスト: 81.4 (en)、70.1 (zh)、74.9 (fr)、74.4 (es)、70.4 (de)、62.1 (ar)\n- ケースなしのゼロショット: 81.4 (en)、63.8 (zh)、74.3 (fr)、70.5 (es)、62.1 (de)、58.3 (ar)\n\nゼロショットとは、英語のみで微調整し、翻訳せずに他の言語で評価することを意味します。\n\nオリジナルの多言語大文字小文字なしモデルは、12 レイヤー、768 の非表示、12 ヘッド、および 1 億 1000 万のパラメーターで 102 の言語をカバーします。新しいバージョンでは 104 の言語がカバーされています。\n\n質問応答など、他の下流タスクに関する [追加情報が必要]。"
  },
  {
    "question": "model_examination",
    "chain": "generation",
    "answer": "Based on the provided references, here is what I can say about the explainability and interpretability of the BERT base multilingual uncased model that our team has worked on:\n\nThe BERT model uses a bidirectional masked language modeling objective during pretraining, which allows it to jointly condition on both left and right context in all layers. This bidirectionality is important for the model's performance, as demonstrated by evaluating pretraining objectives with and without the \"next sentence prediction\" task [1].\n\nBy pretraining deep bidirectional representations from unlabeled text, BERT can be fine-tuned with just one additional output layer for various downstream tasks [6]. This reduces the need for heavily-engineered task-specific architectures [8].\n\nIncreasing the model size (layers, hidden units, attention heads) generally leads to improvements on downstream tasks, even for small-scale tasks, provided the model has been sufficiently pre-trained [2, 3, 4]. Our BERT base model contains 110M parameters [5].\n\n[More Information Needed] on specific techniques used for explaining or interpreting the model's predictions and internal representations. The provided references do not cover this aspect in detail.\n\nIn summary, the bidirectional pretraining and the ability to scale to large model sizes contribute to BERT's strong performance across various NLP tasks, but more information is needed to discuss its explainability and interpretability in depth.",
    "answer_Chinese": "根据提供的参考资料，我可以说一下我们团队所研究的 BERT 基础多语言无壳模型的可解释性和可解释性：\n\nBERT 模型在预训练期间使用双向屏蔽语言建模目标，这使得它能够在所有层中联合条件左右上下文。这种双向性对于模型的性能非常重要，正如通过评估有或没有“下一句话预测”任务的预训练目标所证明的那样 [1]。\n\n通过对未标记文本的深度双向表示进行预训练，BERT 可以针对各种下游任务仅使用一个额外的输出层进行微调 [6]。这减少了对精心设计的特定任务架构的需求[8]。\n\n增加模型大小（层数、隐藏单元、注意力头）通常会导致下游任务的改进，即使是小规模任务，只要模型经过充分的预训练 [2,3,4]。我们的 BERT 基础模型包含 110M 个参数 [5]。\n\n[需要更多信息]用于解释或解释模型的预测和内部表示的特定技术。所提供的参考文献并未详细涵盖这方面。\n\n总而言之，双向预训练和扩展到大模型尺寸的能力有助于 BERT 在各种 NLP 任务中表现出色，但需要更多信息来深入讨论其可解释性和可解释性。",
    "answer_Arabic": "استنادًا إلى المراجع المقدمة، إليك ما يمكنني قوله حول إمكانية الشرح وقابلية التفسير لنموذج BERT الأساسي متعدد اللغات غير المغطى الذي عمل عليه فريقنا:\n\nيستخدم نموذج BERT هدف نمذجة لغة مقنعة ثنائي الاتجاه أثناء التدريب المسبق، مما يسمح له بالتكيف بشكل مشترك مع السياق الأيسر والأيمن في جميع الطبقات. تعتبر ثنائية الاتجاه مهمة لأداء النموذج، كما يتضح من تقييم أهداف التدريب المسبق مع أو بدون مهمة \"التنبؤ بالجمل التالية\" [1].\n\nمن خلال التدريب المسبق على التمثيلات العميقة ثنائية الاتجاه من النص غير المسمى، يمكن ضبط BERT بدقة باستخدام طبقة إخراج إضافية واحدة فقط لمختلف المهام النهائية [6]. وهذا يقلل من الحاجة إلى بنيات خاصة بالمهام شديدة الهندسة [8].\n\nتؤدي زيادة حجم النموذج (الطبقات، الوحدات المخفية، رؤوس الانتباه) بشكل عام إلى تحسينات في المهام النهائية، حتى بالنسبة للمهام صغيرة الحجم، بشرط أن يكون النموذج قد تم تدريبه مسبقًا بشكل كافٍ [2، 3، 4]. يحتوي نموذجنا الأساسي BERT على 110 مليون معلمة [5].\n\n[مطلوب مزيد من المعلومات] حول تقنيات محددة تستخدم لشرح أو تفسير تنبؤات النموذج والتمثيلات الداخلية. المراجع المقدمة لا تغطي هذا الجانب بالتفصيل.\n\nباختصار، يساهم التدريب المسبق ثنائي الاتجاه والقدرة على التوسع في أحجام النماذج الكبيرة في أداء BERT القوي عبر مهام البرمجة اللغوية العصبية المختلفة، ولكن هناك حاجة إلى مزيد من المعلومات لمناقشة قابلية الشرح وقابلية التفسير بعمق.",
    "answer_French": "Sur la base des références fournies, voici ce que je peux dire sur l'explicabilité et l'interprétabilité du modèle multilingue non casqué de base BERT sur lequel notre équipe a travaillé :\n\nLe modèle BERT utilise un objectif de modélisation de langage masqué bidirectionnel pendant la pré-formation, ce qui lui permet de conditionner conjointement le contexte gauche et droit dans toutes les couches. Cette bidirectionnalité est importante pour les performances du modèle, comme le démontre l'évaluation des objectifs de pré-entraînement avec et sans la tâche de « prédiction de la phrase suivante » [1].\n\nEn pré-entraînant des représentations bidirectionnelles profondes à partir de texte non étiqueté, BERT peut être affiné avec une seule couche de sortie supplémentaire pour diverses tâches en aval [6]. Cela réduit le besoin d’architectures spécifiques à des tâches fortement conçues [8].\n\nL'augmentation de la taille du modèle (couches, unités cachées, têtes d'attention) conduit généralement à des améliorations sur les tâches en aval, même pour les tâches à petite échelle, à condition que le modèle ait été suffisamment pré-entraîné [2, 3, 4]. Notre modèle de base BERT contient 110 millions de paramètres [5].\n\n[Plus d'informations nécessaires] sur les techniques spécifiques utilisées pour expliquer ou interpréter les prédictions et les représentations internes du modèle. Les références fournies ne couvrent pas cet aspect en détail.\n\nEn résumé, le pré-entraînement bidirectionnel et la capacité d'évoluer vers des modèles de grande taille contribuent aux solides performances de BERT dans diverses tâches de PNL, mais plus d'informations sont nécessaires pour discuter en profondeur de son explicabilité et de son interprétabilité.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот что я могу сказать об объяснимости и интерпретируемости базовой многоязычной бескорпусной модели BERT, над которой работала наша команда:\n\nМодель BERT использует цель моделирования двунаправленного языка в масках во время предварительного обучения, что позволяет ей совместно определять как левый, так и правый контекст на всех уровнях. Эта двунаправленность важна для производительности модели, о чем свидетельствует оценка целей предварительного обучения с задачей «предсказание следующего предложения» и без нее [1].\n\nПутем предварительной подготовки глубоких двунаправленных представлений из неразмеченного текста BERT можно точно настроить с помощью всего лишь одного дополнительного выходного слоя для различных последующих задач [6]. Это снижает потребность в тщательно продуманных архитектурах для конкретных задач [8].\n\nУвеличение размера модели (слои, скрытые блоки, головы внимания) обычно приводит к улучшению последующих задач, даже для небольших задач, при условии, что модель была достаточно предварительно обучена [2, 3, 4]. Наша базовая модель BERT содержит 110M параметров [5].\n\n[Требуется дополнительная информация] о конкретных методах, используемых для объяснения или интерпретации прогнозов и внутренних представлений модели. Предоставленные ссылки не освещают этот аспект подробно.\n\nТаким образом, двунаправленное предварительное обучение и способность масштабироваться до больших размеров модели способствуют высокой производительности BERT при выполнении различных задач НЛП, но для более глубокого обсуждения его объяснимости и интерпретируемости требуется больше информации.",
    "answer_Japanese": "提供された参考資料に基づいて、私たちのチームが取り組んだ BERT ベースの多言語大文字小文字なしモデルの説明可能性と解釈可能性について言えることは次のとおりです。\n\nBERT モデルは、事前トレーニング中に双方向のマスクされた言語モデリング目標を使用します。これにより、すべてのレイヤーで左と右の両方のコンテキストを共同で条件付けることができます。この双方向性は、「次の文の予測」タスクを使用した場合と使用しない場合の事前トレーニング目標を評価することで実証されているように、モデルのパフォーマンスにとって重要です [1]。\n\nラベルのないテキストから深い双方向表現を事前トレーニングすることにより、さまざまなダウンストリーム タスクに合わせて出力層を 1 つ追加するだけで BERT を微調整できます [6]。これにより、高度に設計されたタスク固有のアーキテクチャの必要性が軽減されます [8]。\n\nモデルが十分に事前トレーニングされていれば、モデル サイズ (レイヤー、隠れユニット、アテンション ヘッド) を増やすと、一般に、たとえ小規模なタスクであっても、下流のタスクの改善につながります [2、3、4]。 BERT 基本モデルには 1 億 1000 万のパラメータが含まれています [5]。\n\nモデルの予測と内部表現を説明または解釈するために使用される特定の手法に関する [さらなる情報が必要]。提供されている参考文献では、この点については詳しく説明されていません。\n\n要約すると、双方向の事前トレーニングと大規模なモデル サイズに拡張する機能が、さまざまな NLP タスクにわたる BERT の優れたパフォーマンスに貢献していますが、その説明可能性と解釈可能性について詳しく議論するには、より多くの情報が必要です。"
  },
  {
    "question": "hardware",
    "chain": "generation",
    "answer": "Based on the provided references, the model bert-base-multilingual-uncased was trained on Cloud TPUs. Specifically, the training was performed on 4 Cloud TPUs in Pod configuration, which amounts to a total of 16 TPU chips.\n\nReference:\n\"Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total).\"",
    "answer_Chinese": "根据提供的参考资料，模型 bert-base-multilingual-uncased 在 Cloud TPU 上进行了训练。具体来说，训练是在 Pod 配置中的 4 个 Cloud TPU 上进行的，总共 16 个 TPU 芯片。\n\n参考：\n“BERT BASE 的训练是在 Pod 配置中的 4 个 Cloud TPU 上进行的（总共 16 个 TPU 芯片）。”",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، تم تدريب نموذج bert-base-multilingual-uncased على وحدات TPU السحابية. على وجه التحديد، تم إجراء التدريب على 4 وحدات TPU السحابية في تكوين Pod، وهو ما يصل إلى إجمالي 16 شريحة TPU.\n\nمرجع:\n\"تم إجراء تدريب BERT BASE على 4 وحدات TPU السحابية في تكوين Pod (إجمالي 16 شريحة TPU).\"",
    "answer_French": "Sur la base des références fournies, le modèle bert-base-multilingual-uncased a été entraîné sur Cloud TPU. Plus précisément, la formation a été réalisée sur 4 Cloud TPU en configuration Pod, ce qui représente un total de 16 puces TPU.\n\nRéférence:\n\"La formation de BERT BASE a été effectuée sur 4 Cloud TPU en configuration Pod (16 puces TPU au total).\"",
    "answer_Russian": "На основе предоставленных ссылок модель bert-base-multilingual-uncased была обучена на Cloud TPU. В частности, обучение проводилось на 4 Cloud TPU в конфигурации Pod, что в общей сложности составляет 16 чипов TPU.\n\nСсылка:\n«Обучение BERT BASE проводилось на 4 облачных TPU в конфигурации Pod (всего 16 чипов TPU)».",
    "answer_Japanese": "提供された参照に基づいて、モデル bert-base-multilingual-uncased が Cloud TPU でトレーニングされました。具体的には、トレーニングはポッド構成の 4 つの Cloud TPU で実行され、合計 16 個の TPU チップになります。\n\n参照：\n「BERT BASE のトレーニングは、ポッド構成の 4 つの Cloud TPU (合計 16 個の TPU チップ) で実行されました。」"
  },
  {
    "question": "software",
    "chain": "generation",
    "answer": "Based on the provided references, the model bert-base-multilingual-uncased is likely trained using the tensor2tensor library, as mentioned in this excerpt:\n\n\"BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.\"\n\nHowever, more specific information about the exact software and version used for training bert-base-multilingual-uncased would be needed to provide a definitive answer.\n\n[More Information Needed]",
    "answer_Chinese": "根据提供的参考资料，模型 bert-base-multilingual-uncased 可能是使用 tensor2tensor 库进行训练的，如本摘录中所述：\n\n“BERT 的模型架构是一个多层双向 Transformer 编码器，基于 Vaswani 等人（2017 年）中描述的原始实现，并在 tensor2tensor 库中发布。”\n\n然而，需要有关用于训练 bert-base-multilingual-uncased 的确切软件和版本的更具体信息才能提供明确的答案。\n\n[需要更多信息]",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، من المحتمل أن يتم تدريب نموذج bert-base-multilingual-uncased باستخدام مكتبة Tensor2tensor، كما هو مذكور في هذا المقتطف:\n\n\"إن بنية نموذج BERT عبارة عن برنامج تشفير محول ثنائي الاتجاه متعدد الطبقات يعتمد على التنفيذ الأصلي الموضح في Vaswani et al. (2017) وتم إصداره في مكتبة Tensor2tensor.\"\n\nومع ذلك، ستكون هناك حاجة إلى معلومات أكثر تحديدًا حول البرنامج الدقيق والإصدار المستخدم لتدريب bert-base-multilingual-uncased لتوفير إجابة محددة.\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des références fournies, le modèle bert-base-multilingual-uncased est probablement formé à l'aide de la bibliothèque tensor2tensor, comme mentionné dans cet extrait :\n\n\"L'architecture du modèle de BERT est un encodeur Transformer bidirectionnel multicouche basé sur l'implémentation originale décrite dans Vaswani et al. (2017) et publiée dans la bibliothèque tensor2tensor.\"\n\nCependant, des informations plus spécifiques sur le logiciel exact et la version utilisée pour la formation bert-base-multilingual-uncased seraient nécessaires pour fournir une réponse définitive.\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "Судя по предоставленным ссылкам, модель bert-base-multilingual-uncased, скорее всего, обучается с использованием библиотеки tensor2tensor, как указано в этом отрывке:\n\n«Архитектура модели BERT представляет собой многоуровневый двунаправленный кодер Transformer, основанный на оригинальной реализации, описанной Васвани и др. (2017) и выпущенной в библиотеке tensor2tensor».\n\nОднако для получения окончательного ответа потребуется более конкретная информация о конкретном программном обеспечении и версии, используемой для обучения bert-base-multilingual-uncased.\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "提供された参照に基づいて、モデル bert-base-multilingual-uncased は、この抜粋で説明されているように、tensor2tensor ライブラリを使用してトレーニングされる可能性があります。\n\n「BERT のモデル アーキテクチャは、Vaswani et al. (2017) で説明され、tensor2tensor ライブラリでリリースされたオリジナルの実装に基づく多層双方向 Transformer エンコーダです。」\n\nただし、最終的な答えを得るには、bert-base-multilingual-uncased のトレーニングに使用される正確なソフトウェアとバージョンに関するより具体的な情報が必要です。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "hours_used",
    "chain": "generation",
    "answer": "Based on the information provided in the references, the training time for BERT-Base, Multilingual Uncased is not explicitly stated. The references mention that pre-training BERT-Base was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total) and each pre-training took 4 days to complete. However, it is not clear if this refers specifically to the Multilingual Uncased model or the English-only BERT-Base model.\n\nTherefore, the answer to the question \"Provide the amount of time used to train the model bert-base-multilingual-uncased\" is:\n\n[More Information Needed]",
    "answer_Chinese": "根据参考文献中提供的信息，并未明确说明 BERT-Base、Multilingual Uncased 的训练时间。参考文献提到，预训练 BERT-Base 是在 Pod 配置中的 4 个 Cloud TPU（总共 16 个 TPU 芯片）上执行的，每个预训练需要 4 天才能完成。然而，尚不清楚这是否特指多语言 Uncased 模型或纯英语 BERT-Base 模型。\n\n因此，“提供用于训练模型 bert-base-multilingual-uncased 的时间量”问题的答案是：\n\n[需要更多信息]",
    "answer_Arabic": "بناءً على المعلومات المقدمة في المراجع، لم يتم ذكر وقت التدريب لـ BERT-Base, Multilingual Uncased بشكل صريح. تشير المراجع إلى أنه تم تنفيذ تدريب BERT-Base المسبق على 4 وحدات TPU سحابية في تكوين Pod (إجمالي 16 شريحة TPU) واستغرق كل تدريب مسبق 4 أيام لإكماله. ومع ذلك، ليس من الواضح ما إذا كان هذا يشير على وجه التحديد إلى النموذج متعدد اللغات غير المغطى أو نموذج BERT-Base باللغة الإنجليزية فقط.\n\nلذلك، فإن إجابة السؤال \"أوجد مقدار الوقت المستخدم لتدريب نموذج bert-base-multilingual-uncased\" هو:\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des informations fournies dans les références, le temps de formation pour BERT-Base, Multilingual Uncased n'est pas explicitement indiqué. Les références mentionnent que la pré-formation BERT-Base a été effectuée sur 4 Cloud TPU en configuration Pod (16 puces TPU au total) et que chaque pré-formation a duré 4 jours. Cependant, il n'est pas clair si cela fait spécifiquement référence au modèle Multilingual Uncased ou au modèle BERT-Base uniquement en anglais.\n\nPar conséquent, la réponse à la question « Fournir le temps nécessaire pour entraîner le modèle bert-base-multilingual-uncased » est la suivante :\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "Судя по информации, представленной в ссылках, время обучения для BERT-Base, Multilingual Uncased явно не указано. В ссылках упоминается, что предварительное обучение BERT-Base проводилось на 4 облачных TPU в конфигурации Pod (всего 16 чипов TPU), и каждое предварительное обучение занимало 4 дня. Однако неясно, относится ли это конкретно к многоязычной модели без корпуса или к модели BERT-Base только на английском языке.\n\nСледовательно, ответ на вопрос «Укажите количество времени, затраченное на обучение модели bert-base-multilingual-uncased»:\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "参考資料で提供されている情報に基づくと、BERT ベースの多言語非ケース化のトレーニング時間は明示的に記載されていません。参考文献には、BERT-Base の事前トレーニングがポッド構成の 4 つの Cloud TPU (合計 16 個の TPU チップ) で実行され、各事前トレーニングが完了するまでに 4 日かかったことが記載されています。ただし、これが特に Multilingual Uncased モデルを指しているのか、それとも英語のみの BERT-Base モデルを指しているのかは明らかではありません。\n\nしたがって、「モデル bert-base-multilingual-uncased のトレーニングに使用した時間を教えてください」という質問に対する答えは次のようになります。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "cloud_provider",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain any information about the cloud provider used to train the BERT base multilingual uncased model. The references discuss the BERT model architecture, pre-training approach, and performance on various NLP tasks, but do not specify the computing infrastructure or cloud platform utilized for training.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料不包含有关用于训练 BERT 基础多语言无壳模型的云提供商的任何信息。这些参考文献讨论了 BERT 模型架构、预训练方法以及各种 NLP 任务的性能，但没有指定用于训练的计算基础设施或云平台。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على أي معلومات حول موفر السحابة المستخدم لتدريب النموذج غير المغطى متعدد اللغات الأساسي لـ BERT. تناقش المراجع بنية نموذج BERT، ونهج التدريب المسبق، والأداء في مهام البرمجة اللغوية العصبية المختلفة، ولكنها لا تحدد البنية التحتية للحوسبة أو النظام الأساسي السحابي المستخدم للتدريب.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent aucune information sur le fournisseur de cloud utilisé pour entraîner le modèle multilingue sans boîtier de base BERT. Les références discutent de l'architecture du modèle BERT, de l'approche de pré-formation et des performances sur diverses tâches NLP, mais ne précisent pas l'infrastructure informatique ou la plate-forme cloud utilisée pour la formation.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат никакой информации о поставщике облачных услуг, который использовался для обучения базовой многоязычной бескорпусной модели BERT. В ссылках обсуждается архитектура модели BERT, подход к предварительному обучению и производительность при выполнении различных задач НЛП, но не указывается вычислительная инфраструктура или облачная платформа, используемая для обучения.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されるリファレンスには、BERT ベースの多言語アンケース モデルのトレーニングに使用されるクラウド プロバイダーに関する情報は含まれていません。参考文献では、BERT モデルのアーキテクチャ、事前トレーニングのアプローチ、さまざまな NLP タスクのパフォーマンスについて説明していますが、トレーニングに利用されるコンピューティング インフラストラクチャやクラウド プラットフォームについては指定されていません。"
  },
  {
    "question": "co2_emitted",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain any information about the amount of carbon emitted during the training of the bert-base-multilingual-uncased model. To answer this question, more specific details would be needed, such as the hardware used for training, the duration of the training process, and the energy consumption of the training setup.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献不包含有关 bert-base-multilingual-uncased 模型训练期间碳排放量的任何信息。要回答这个问题，需要更具体的细节，例如用于训练的硬件、训练过程的持续时间以及训练设置的能耗。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على أي معلومات حول كمية الكربون المنبعثة أثناء التدريب على نموذج bert-base-multilingual-uncased. للإجابة على هذا السؤال، ستكون هناك حاجة إلى تفاصيل أكثر تحديدًا، مثل الأجهزة المستخدمة للتدريب، ومدة عملية التدريب، واستهلاك الطاقة لإعداد التدريب.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent aucune information sur la quantité de carbone émise lors de la formation du modèle bert-base-multilingual-uncased. Pour répondre à cette question, des détails plus spécifiques seraient nécessaires, tels que le matériel utilisé pour la formation, la durée du processus de formation et la consommation d'énergie de l'installation de formation.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат никакой информации о количестве углерода, выбрасываемого во время обучения модели bert-base-multilingual-uncased. Чтобы ответить на этот вопрос, потребуются более конкретные детали, такие как оборудование, используемое для обучения, продолжительность процесса обучения и энергопотребление тренировочной установки.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、bert-base-multilingual-uncased モデルのトレーニング中に排出される炭素の量に関する情報は含まれていません。この質問に答えるには、トレーニングに使用されるハードウェア、トレーニング プロセスの期間、トレーニング セットアップのエネルギー消費など、より具体的な詳細が必要になります。"
  },
  {
    "question": "model_specs",
    "chain": "generation",
    "answer": "Model Architecture:\nBERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017). The bert-base-multilingual-uncased model has the following architecture hyperparameters:\n- Number of layers (Transformer blocks): 12\n- Hidden size: 768\n- Number of self-attention heads: 12\n- Total parameters: 110M\n\nPre-training Objective:\nBERT is pre-trained using two unsupervised tasks:\n1. Masked Language Model (MLM): Some of the input tokens are randomly masked, and the objective is to predict the original vocabulary id of the masked word based only on its bidirectional context.\n2. [More Information Needed]\n\nThe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of downstream tasks.",
    "answer_Chinese": "模型架构：\nBERT 的模型架构是一个多层双向 Transformer 编码器，基于 Vaswani 等人描述的原始实现。 （2017）。 bert-base-multilingual-uncased 模型具有以下架构超参数：\n- 层数（变压器块）：12\n- 隐藏尺寸：768\n- 自注意力头数量：12\n- 总参数：110M\n\n预训练目标：\nBERT 使用两个无监督任务进行预训练：\n1. 屏蔽语言模型（MLM）：一些输入标记被随机屏蔽，目标是仅根据其双向上下文来预测被屏蔽单词的原始词汇ID。\n2. [需要更多信息]\n\n预训练的 BERT 模型只需一个额外的输出层即可进行微调，从而为各种下游任务创建最先进的模型。",
    "answer_Arabic": "العمارة النموذجية:\nإن بنية نموذج BERT عبارة عن مشفر محول ثنائي الاتجاه متعدد الطبقات يعتمد على التنفيذ الأصلي الموضح في Vaswani et al. (2017). يحتوي نموذج bert-base-multilingual-uncased على معلمات البنية الفائقة التالية:\n- عدد الطبقات (كتل المحولات): 12\n- الحجم المخفي: 768\n- عدد رؤوس الاهتمام الذاتي: 12\n- إجمالي المعلمات: 110 م\n\nهدف التدريب المسبق:\nتم تدريب BERT مسبقًا باستخدام مهمتين غير خاضعين للرقابة:\n1. نموذج اللغة المقنع (MLM): يتم إخفاء بعض رموز الإدخال بشكل عشوائي، والهدف هو التنبؤ بمعرف المفردات الأصلي للكلمة المقنعة بناءً على سياقها ثنائي الاتجاه فقط.\n2. [بحاجة إلى مزيد من المعلومات]\n\nيمكن ضبط نموذج BERT المُدرب مسبقًا بطبقة إخراج إضافية واحدة فقط لإنشاء نماذج حديثة لمجموعة واسعة من المهام النهائية.",
    "answer_French": "Architecture du modèle :\nL'architecture du modèle de BERT est un codeur de transformateur bidirectionnel multicouche basé sur l'implémentation originale décrite dans Vaswani et al. (2017). Le modèle bert-base-multilingual-uncased possède les hyperparamètres d'architecture suivants :\n- Nombre de couches (blocs transformateurs) : 12\n- Taille cachée : 768\n- Nombre de têtes d'auto-attention : 12\n-Paramètres totaux: 110M\n\nObjectif de la pré-formation :\nBERT est pré-entraîné à l'aide de deux tâches non supervisées :\n1. Modèle de langage masqué (MLM) : certains des jetons d'entrée sont masqués de manière aléatoire et l'objectif est de prédire l'identifiant de vocabulaire d'origine du mot masqué en fonction uniquement de son contexte bidirectionnel.\n2. [Plus d'informations nécessaires]\n\nLe modèle BERT pré-entraîné peut être affiné avec une seule couche de sortie supplémentaire pour créer des modèles de pointe pour un large éventail de tâches en aval.",
    "answer_Russian": "Модельная архитектура:\nАрхитектура модели BERT представляет собой многоуровневый двунаправленный преобразователь-кодер, основанный на оригинальной реализации, описанной Васвани и др. (2017). Модель bert-base-multilingual-uncased имеет следующие гиперпараметры архитектуры:\n- Количество слоев (блоков трансформаторов): 12\n- Скрытый размер: 768\n- Количество головок самообслуживания: 12\n- Общие параметры: 110М\n\nЦель предварительной подготовки:\nBERT предварительно обучается с использованием двух неконтролируемых задач:\n1. Модель маскированного языка (MLM). Некоторые входные токены замаскированы случайным образом, и цель состоит в том, чтобы предсказать исходный словарный идентификатор замаскированного слова на основе только его двунаправленного контекста.\n2. [Требуется дополнительная информация]\n\nПредварительно обученную модель BERT можно точно настроить с помощью всего лишь одного дополнительного выходного слоя для создания современных моделей для широкого спектра последующих задач.",
    "answer_Japanese": "モデルのアーキテクチャ:\nBERT のモデル アーキテクチャは、Vaswani et al. で説明されているオリジナルの実装に基づいた多層双方向 Transformer エンコーダです。 （2017年）。 bert-base-multilingual-uncased モデルには、次のアーキテクチャ ハイパーパラメータがあります。\n- 層数（トランスブロック）：12\n- 隠しサイズ: 768\n- 自力ヘッドの数: 12\n- 総パラメータ: 110M\n\n事前トレーニングの目的:\nBERT は、次の 2 つの教師なしタスクを使用して事前トレーニングされます。\n1. マスク言語モデル (MLM): 入力トークンの一部はランダムにマスクされ、その目的は、双方向コンテキストのみに基づいてマスクされた単語の元の語彙 ID を予測することです。\n2. [さらに詳しい情報が必要です]\n\n事前トレーニングされた BERT モデルは、出力層を 1 つ追加するだけで微調整でき、幅広いダウンストリーム タスク用の最先端のモデルを作成できます。"
  },
  {
    "question": "compute_infrastructure",
    "chain": "generation",
    "answer": "Based on the provided references, here is the information about the compute infrastructure used for training the bert-base-multilingual-uncased model:\n\nTraining of BERT BASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total). Each pretraining took 4 days to complete.\n\n[More Information Needed] on the specific compute infrastructure details for the multilingual uncased variant of BERT base model, as the provided reference only mentions the compute used for BERT base model in general.",
    "answer_Chinese": "根据提供的参考资料，以下是有关用于训练 bert-base-multilingual-uncased 模型的计算基础设施的信息：\n\nBERT BASE 的训练是在 Pod 配置中的 4 个 Cloud TPU 上进行的（总共 16 个 TPU 芯片）。每次预训练需要 4 天才能完成。\n\n[需要更多信息]关于 BERT 基础模型的多语言无大小写变体的特定计算基础设施细节，因为所提供的参考文献仅提及一般用于 BERT 基础模型的计算。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك المعلومات حول البنية الأساسية للحوسبة المستخدمة لتدريب نموذج bert-base-multilingual-uncased:\n\nتم إجراء تدريب BERT BASE على 4 وحدات TPU السحابية في تكوين Pod (إجمالي 16 شريحة TPU). استغرق كل تدريب مسبق 4 أيام لإكماله.\n\n[مطلوب مزيد من المعلومات] حول تفاصيل البنية التحتية للحوسبة المحددة للمتغير غير المغطى متعدد اللغات لنموذج BERT الأساسي، حيث أن المرجع المقدم لا يذكر سوى الحوسبة المستخدمة لنموذج BERT الأساسي بشكل عام.",
    "answer_French": "Sur la base des références fournies, voici les informations sur l'infrastructure de calcul utilisée pour entraîner le modèle bert-base-multilingual-uncased :\n\nLa formation de BERT BASE a été réalisée sur 4 Cloud TPU en configuration Pod (16 puces TPU au total). Chaque pré-formation a duré 4 jours.\n\n[Plus d'informations nécessaires] sur les détails spécifiques de l'infrastructure de calcul pour la variante multilingue non cassée du modèle de base BERT, car la référence fournie ne mentionne que le calcul utilisé pour le modèle de base BERT en général.",
    "answer_Russian": "На основе предоставленных ссылок ниже представлена ​​информация о вычислительной инфраструктуре, используемой для обучения модели bert-base-multilingual-uncases:\n\nОбучение BERT BASE проводилось на 4 облачных TPU в конфигурации Pod (всего 16 чипов TPU). Каждая предварительная подготовка занимала 4 дня.\n\n[Требуется дополнительная информация] о конкретных деталях вычислительной инфраструктуры для многоязычного варианта базовой модели BERT без корпуса, поскольку в предоставленной ссылке упоминаются только вычисления, используемые для базовой модели BERT в целом.",
    "answer_Japanese": "提供された参考資料に基づいて、bert-base-multilingual-uncased モデルのトレーニングに使用されるコンピューティング インフラストラクチャに関する情報を次に示します。\n\nBERT BASE のトレーニングは、ポッド構成の 4 つの Cloud TPU (合計 16 個の TPU チップ) で実行されました。各事前トレーニングが完了するまでに 4 日かかりました。\n\nBERT 基本モデルの多言語大文字小文字なしバリアントの特定のコンピューティング インフラストラクチャの詳細に関する [詳細情報が必要]。提供されている参考資料では、一般的に BERT 基本モデルに使用されるコンピューティングについてのみ言及されています。"
  }
]