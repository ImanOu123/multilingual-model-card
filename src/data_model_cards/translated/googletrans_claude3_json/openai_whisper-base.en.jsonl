[
  {
    "question": "summary",
    "chain": "generation",
    "answer": "openai/whisper-base.en is an English-only speech recognition model based on the Whisper architecture. It is a general-purpose model trained on a large dataset of diverse audio and can be used for English speech recognition tasks.",
    "answer_Chinese": "openai/whisper-base.en 是基于 Whisper 架构的纯英语语音识别模型。它是在大量不同音频数据集上训练的通用模型，可用于英语语音识别任务。",
    "answer_Arabic": "openai/whisper-base.en هو نموذج للتعرف على الكلام باللغة الإنجليزية فقط يعتمد على بنية Whisper. إنه نموذج للأغراض العامة تم تدريبه على مجموعة كبيرة من البيانات الصوتية المتنوعة ويمكن استخدامه لمهام التعرف على الكلام باللغة الإنجليزية.",
    "answer_French": "openai/whisper-base.en est un modèle de reconnaissance vocale uniquement en anglais basé sur l'architecture Whisper. Il s'agit d'un modèle à usage général formé sur un vaste ensemble de données audio diverses et peut être utilisé pour des tâches de reconnaissance vocale en anglais.",
    "answer_Russian": "openai/whisper-base.en — это модель распознавания речи только на английском языке, основанная на архитектуре Whisper. Это модель общего назначения, обученная на большом наборе данных разнообразного аудио, и ее можно использовать для задач распознавания английской речи.",
    "answer_Japanese": "openai/whisper-base.en は、Whisper アーキテクチャに基づく英語専用の音声認識モデルです。これは、さまざまな音声の大規模なデータセットでトレーニングされた汎用モデルであり、英語の音声認識タスクに使用できます。"
  },
  {
    "question": "description",
    "chain": "generation",
    "answer": "Model Details for openai/whisper-base.en:\n\nModel Architecture:\n- Encoder-decoder Transformer architecture\n- Audio is resampled to 16kHz, converted to 80-channel log-mel spectrogram\n- Encoder processes input with 2 conv layers, sinusoidal position embeddings, and transformer blocks\n- Decoder uses learned position embeddings and tied input-output token representations\n- Encoder and decoder have same width and number of transformer blocks\n\nTraining Procedures: \n- Trained on 680,000 hours of diverse labeled audio data\n- Multitask training on speech recognition, translation, language ID, voice activity detection\n- Tasks jointly represented as token sequences for decoder to predict\n- Uses special tokens as task specifiers\n- Raw text transcripts used without significant standardization\n- [More Information Needed] on other training details like optimizer, learning rate, etc.\n\nModel Parameters:\n- [More Information Needed]\n\nImportant Disclaimers:\n- Model may generate plausible but incorrect speaker names in transcripts\n- [More Information Needed] on other potential biases, limitations or risks\n\nThe Whisper-base.en model is a general-purpose English speech recognition model that is part of the larger multilingual Whisper model family. It demonstrates strong zero-shot generalization abilities that improve with model size. However, more specific details on model parameters, training hyperparameters, and comprehensive analysis of limitations are needed to complete the model card. Please reach out if you require any additional information to be included.",
    "answer_Chinese": "openai/whisper-base.en 的型号详细信息：\n\n模型架构：\n- 编码器-解码器 Transformer 架构\n- 音频重新采样至 16kHz，转换为 80 通道 log-mel 频谱图\n- 编码器使用 2 个转换层、正弦位置嵌入和变压器块处理输入\n- 解码器使用学习的位置嵌入和绑定的输入输出标记表示\n- 编码器和解码器具有相同的宽度和变压器块数量\n\n培训程序： \n- 经过 680,000 小时不同标记音频数据的训练\n- 语音识别、翻译、语言识别、语音活动检测等多任务训练\n- 任务联合表示为令牌序列，供解码器预测\n- 使用特殊标记作为任务说明符\n- 使用未经显着标准化的原始文本记录\n- [需要更多信息] 关于其他训练细节，如优化器、学习率等。\n\n型号参数：\n- [需要更多信息]\n\n重要免责声明：\n- 模型可能会在文字记录中生成看似合理但不正确的说话人姓名\n- [需要更多信息]关于其他潜在偏见、限制或风险\n\nWhisper-base.en 模型是一种通用英语语音识别模型，是更大的多语言 Whisper 模型系列的一部分。它展示了强大的零样本泛化能力，该能力随着模型大小的增加而提高。然而，需要更具体的模型参数、训练超参数以及局限性的综合分析来完成模型卡。如果您需要包含任何其他信息，请联系我们。",
    "answer_Arabic": "تفاصيل النموذج لـ openai/whisper-base.en:\n\nالعمارة النموذجية:\n- بنية محول التشفير وفك التشفير\n- يتم إعادة تشكيل الصوت إلى 16 كيلو هرتز، وتحويله إلى مخطط طيفي سجل ميل ذو 80 قناة\n- يقوم جهاز التشفير بمعالجة المدخلات بطبقتين تحويليتين، ودمجات الموضع الجيبية، وكتل المحولات\n- يستخدم جهاز فك التشفير تضمينات الموضع المستفادة وتمثيلات رمزية للإدخال والمخرجات المرتبطة\n- التشفير وفك التشفير لهما نفس العرض وعدد كتل المحولات\n\nإجراءات التدريب: \n- تم التدريب على 680.000 ساعة من البيانات الصوتية المتنوعة\n- تدريب متعدد المهام على التعرف على الكلام والترجمة ومعرف اللغة واكتشاف النشاط الصوتي\n- المهام ممثلة بشكل مشترك كتسلسلات رمزية لوحدة فك التشفير للتنبؤ بها\n- يستخدم الرموز الخاصة كمحددات المهام\n- نصوص النص الخام المستخدمة دون توحيد كبير\n- [يلزم مزيد من المعلومات] حول تفاصيل التدريب الأخرى مثل المحسن ومعدل التعلم وما إلى ذلك.\n\nمعلمات النموذج:\n- [يلزم مزيد من المعلومات]\n\nإخلاء المسؤولية الهامة:\n- قد يقوم النموذج بإنشاء أسماء متحدثين معقولة ولكنها غير صحيحة في النصوص\n- [بحاجة إلى مزيد من المعلومات] حول التحيزات أو القيود أو المخاطر المحتملة الأخرى\n\nنموذج Whisper-base.en هو نموذج للتعرف على الكلام باللغة الإنجليزية للأغراض العامة وهو جزء من عائلة نماذج Whisper الأكبر متعددة اللغات. إنه يُظهر قدرات تعميم صفرية قوية تتحسن مع حجم النموذج. ومع ذلك، هناك حاجة إلى تفاصيل أكثر تحديدًا حول معلمات النموذج ومعلمات التدريب الفائقة والتحليل الشامل للقيود لإكمال بطاقة النموذج. يرجى التواصل إذا كنت بحاجة إلى تضمين أي معلومات إضافية.",
    "answer_French": "Détails du modèle pour openai/whisper-base.en :\n\nArchitecture du modèle :\n- Architecture du transformateur codeur-décodeur\n- L'audio est rééchantillonné à 16 kHz, converti en spectrogramme log-mel à 80 canaux\n- L'encodeur traite l'entrée avec 2 couches de conversion, des intégrations de position sinusoïdale et des blocs de transformateur\n- Le décodeur utilise des intégrations de positions apprises et des représentations de jetons d'entrée-sortie liées\n- L'encodeur et le décodeur ont la même largeur et le même nombre de blocs de transformateur\n\nProcédures de formation : \n- Formé sur 680 000 heures de données audio étiquetées diverses\n- Formation multitâche sur la reconnaissance vocale, la traduction, l'identification de langue, la détection d'activité vocale\n- Tâches représentées conjointement sous forme de séquences de jetons que le décodeur doit prédire\n- Utilise des jetons spéciaux comme spécificateurs de tâches\n- Transcriptions de texte brut utilisées sans standardisation significative\n- [Plus d'informations nécessaires] sur d'autres détails de formation tels que l'optimiseur, le taux d'apprentissage, etc.\n\nParamètres du modèle :\n- [Plus d'informations nécessaires]\n\nAvertissements importants :\n- Le modèle peut générer des noms de locuteurs plausibles mais incorrects dans les transcriptions\n- [Plus d'informations nécessaires] sur d'autres biais, limitations ou risques potentiels\n\nLe modèle Whisper-base.en est un modèle de reconnaissance vocale en anglais à usage général qui fait partie de la plus grande famille de modèles multilingues Whisper. Il démontre de fortes capacités de généralisation sans tir qui s'améliorent avec la taille du modèle. Cependant, des détails plus spécifiques sur les paramètres du modèle, les hyperparamètres de formation et une analyse complète des limites sont nécessaires pour compléter la carte modèle. Veuillez nous contacter si vous avez besoin que des informations supplémentaires soient incluses.",
    "answer_Russian": "Детали модели для openai/whisper-base.en:\n\nМодельная архитектура:\n- Трансформаторная архитектура кодер-декодер\n- Звук передискретизируется до 16 кГц и преобразуется в 80-канальную логарифмическую спектрограмму.\n- Кодер обрабатывает входные данные с помощью двух сверточных слоев, встраивания синусоидальных положений и блоков преобразователей.\n- Декодер использует встраивание изученных позиций и связанные представления токенов ввода-вывода.\n- Кодер и декодер имеют одинаковую ширину и количество блоков преобразователя.\n\nПроцедуры обучения: \n- Обучение на 680 000 часов различных аудиоданных.\n- Многозадачное обучение распознаванию речи, переводу, идентификации языка, обнаружению голосовой активности.\n- Задачи, совместно представленные в виде последовательностей токенов, которые декодер может предсказать.\n- Использует специальные токены в качестве спецификаторов задач.\n- Необработанные текстовые расшифровки, используемые без значительной стандартизации.\n- [Требуется дополнительная информация] о других деталях обучения, таких как оптимизатор, скорость обучения и т. д.\n\nПараметры модели:\n- [Требуется дополнительная информация]\n\nВажные заявления об отказе от ответственности:\n- Модель может генерировать правдоподобные, но неправильные имена говорящих в стенограммах.\n- [Необходима дополнительная информация] о других потенциальных предубеждениях, ограничениях или рисках.\n\nМодель Whisper-base.en — это модель распознавания английской речи общего назначения, которая является частью более крупного семейства многоязычных моделей Whisper. Он демонстрирует сильные способности к обобщению с нулевым выстрелом, которые улучшаются с увеличением размера модели. Однако для заполнения карты модели необходимы более конкретные сведения о параметрах модели, обучающих гиперпараметрах и всесторонний анализ ограничений. Если вам потребуется дополнительная информация, свяжитесь с нами.",
    "answer_Japanese": "openai/whisper-base.en のモデルの詳細:\n\nモデルのアーキテクチャ:\n- エンコーダー/デコーダートランスフォーマーのアーキテクチャ\n- オーディオは 16kHz にリサンプリングされ、80 チャンネルのログメル スペクトログラムに変換されます。\n- エンコーダーは 2 つの conv レイヤー、正弦波位置埋め込み、およびトランスフォーマー ブロックを使用して入力を処理します\n- デコーダーは学習された位置埋め込みと関連付けられた入出力トークン表現を使用します\n- エンコーダとデコーダの幅とトランスブロックの数は同じです\n\nトレーニング手順: \n- 680,000 時間の多様なラベル付き音声データでトレーニング\n- 音声認識、翻訳、言語 ID、音声アクティビティ検出に関するマルチタスク トレーニング\n- デコーダーが予測するためのトークン シーケンスとして結合して表現されるタスク\n- タスク指定子として特別なトークンを使用します\n- 大幅な標準化を行わずに使用された生のテキストのトランスクリプト\n- オプティマイザー、学習率など、その他のトレーニングの詳細に関する [さらなる情報が必要]\n\nモデルパラメータ:\n- [さらに詳しい情報が必要です]\n\n重要な免責事項:\n- モデルはトランスクリプト内でもっともらしいが不正確な話者名を生成する可能性があります\n- その他の潜在的なバイアス、制限、またはリスクに関する [さらなる情報が必要]\n\nWhisper-base.en モデルは、より大規模な多言語 Whisper モデル ファミリの一部である汎用英語音声認識モデルです。これは、モデルのサイズとともに向上する強力なゼロショット汎化能力を示します。ただし、モデル カードを完成させるには、モデル パラメーター、トレーニング ハイパーパラメーター、および制限の包括的な分析に関するより具体的な詳細が必要です。追加情報を含める必要がある場合は、ご連絡ください。"
  },
  {
    "question": "funded_by",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base.en was developed by OpenAI. The references mention the following people and teams at OpenAI:\n\n- Nick Ryder, Will Zhuk, and Andrew Carr for the conversation that inspired the project\n- The Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure \n- Pamela Mishkin for advising the project from a policy perspective\n- Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever\n\nHowever, there is no explicit mention of the specific funding sources or organizations for this project. More information would be needed to definitively list the funders.\n\n[More Information Needed]",
    "answer_Chinese": "基于所提供的参考资料，模型 openai/whisper-base.en 由 OpenAI 开发。这些参考文献提到了 OpenAI 的以下人员和团队：\n\n- Nick Ryder、Will Zhuk 和 Andrew Carr 的对话激发了该项目的灵感\n- OpenAI 的加速和超级计算团队在软件和硬件基础设施方面的工作 \n- 帕梅拉·米什金 (Pamela Mishkin) 从政策角度为该项目提供了建议\n- Alec Radford、Jong Wook Kim、Tao Xu、Greg Brockman、Christine Mcleavey 和 Ilya Sutskever\n\n但没有明确提及该项目的具体资金来源或组织。需要更多信息来明确资助者名单。\n\n[需要更多信息]",
    "answer_Arabic": "بناءً على المراجع المتوفرة، تم تطوير النموذج openai/whisper-base.en بواسطة OpenAI. تشير المراجع إلى الأشخاص والفرق التالية في OpenAI:\n\n- نيك رايدر، وويل جوك، وأندرو كار على المحادثة التي ألهمت المشروع\n- فرق التسريع والحوسبة الفائقة في OpenAI لعملهم على البنية التحتية للبرامج والأجهزة \n- باميلا ميشكين لتقديم المشورة للمشروع من منظور سياسي\n- أليك رادفورد، وجونج ووك كيم، وتاو شو، وجريج بروكمان، وكريستين ماكليفي، وإيليا سوتسكيفر\n\nومع ذلك، لا يوجد ذكر صريح لمصادر التمويل أو المنظمات المحددة لهذا المشروع. ستكون هناك حاجة إلى مزيد من المعلومات لإدراج الممولين بشكل نهائي.\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base.en a été développé par OpenAI. Les références mentionnent les personnes et équipes suivantes chez OpenAI :\n\n- Nick Ryder, Will Zhuk et Andrew Carr pour la conversation qui a inspiré le projet\n- Les équipes Accélération et Supercomputing d'OpenAI pour leurs travaux sur l'infrastructure logicielle et matérielle \n- Pamela Mishkin pour avoir conseillé le projet d'un point de vue politique\n- Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey et Ilya Sutskever\n\nCependant, il n'y a aucune mention explicite des sources de financement ou des organisations spécifiques à ce projet. Des informations supplémentaires seraient nécessaires pour lister définitivement les bailleurs de fonds.\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "На основе предоставленных ссылок компания OpenAI разработала модель openai/whisper-base.en. В ссылках упоминаются следующие люди и команды OpenAI:\n\n- Нику Райдеру, Уиллу Жуку и Эндрю Карру за беседу, вдохновившую на создание проекта.\n- Командам ускорения и суперкомпьютеров OpenAI за работу над программной и аппаратной инфраструктурой. \n- Памеле Мишкин за консультирование проекта с точки зрения политики.\n- Алек Рэдфорд, Чон Ук Ким, Тао Сюй, Грег Брокман, Кристин Маккливи и Илья Суцкевер\n\nОднако нет явного упоминания конкретных источников финансирования или организаций для этого проекта. Для окончательного составления списка спонсоров потребуется дополнительная информация.\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "提供された参考資料に基づいて、モデル openai/whisper-base.en は OpenAI によって開発されました。参考文献には、OpenAI の次の人物とチームについて言及されています。\n\n- ニック・ライダー、ウィル・ジューク、アンドリュー・カーがプロジェクトにインスピレーションを与えた会話\n- ソフトウェアおよびハードウェア インフラストラクチャに関する作業を担当する OpenAI のアクセラレーション チームとスーパーコンピューティング チーム \n- パメラ・ミシュキン氏、政策の観点からプロジェクトに助言\n- アレック・ラドフォード、キム・ジョンウク、タオ・シュー、グレッグ・ブロックマン、クリスティン・マクリービー、イリヤ・サツキーヴァー\n\nただし、このプロジェクトの具体的な資金源や組織については明確に言及されていません。資金提供者を明確にリストするには、より多くの情報が必要となるだろう。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "shared_by",
    "chain": "generation",
    "answer": "Based on the provided references, the main contributors who made the model openai/whisper-base.en available online as a GitHub repo are:\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever from OpenAI.\n\nThe references also mention thanking the Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure used in the project, as well as Pamela Mishkin for advising from a policy perspective. However, it's unclear if they were directly involved in making the model available on GitHub.\n\n[More Information Needed] on additional contributors, if any, who were specifically involved in publishing the GitHub repo.",
    "answer_Chinese": "根据提供的参考资料，将 openai/whisper-base.en 模型作为 GitHub 存储库在线提供的主要贡献者是：\n\n来自 OpenAI 的 Alec Radford、Jong Wook Kim、Tao Xu、Greg Brockman、Christine Mcleavey 和 Ilya Sutskever。\n\n参考文献还提到感谢 OpenAI 的加速和超级计算团队在该项目中使用的软件和硬件基础设施方面所做的工作，以及 Pamela Mishkin 从政策角度提供的建议。然而，尚不清楚他们是否直接参与了在 GitHub 上提供该模型的过程。\n\n[需要更多信息] 有关专门参与发布 GitHub 存储库的其他贡献者（如果有）的信息。",
    "answer_Arabic": "بناءً على المراجع المقدمة، المساهمين الرئيسيين الذين جعلوا النموذج openai/whisper-base.en متاحًا عبر الإنترنت كمستودع GitHub هم:\n\nأليك رادفورد، وجونج ووك كيم، وتاو شو، وجريج بروكمان، وكريستين ماكليفي، وإيليا سوتسكيفر من OpenAI.\n\nتشير المراجع أيضًا إلى شكر فرق التسريع والحوسبة الفائقة في OpenAI على عملهم في البنية التحتية للبرامج والأجهزة المستخدمة في المشروع، وكذلك باميلا ميشكين لتقديم المشورة من منظور السياسة. ومع ذلك، ليس من الواضح ما إذا كانوا قد شاركوا بشكل مباشر في إتاحة النموذج على GitHub.\n\n[بحاجة إلى مزيد من المعلومات] عن المساهمين الإضافيين، إن وجدوا، الذين شاركوا بشكل خاص في نشر مستودع GitHub.",
    "answer_French": "Sur la base des références fournies, les principaux contributeurs qui ont rendu le modèle openai/whisper-base.en disponible en ligne sous forme de dépôt GitHub sont :\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey et Ilya Sutskever d'OpenAI.\n\nLes références mentionnent également leurs remerciements aux équipes d'accélération et de supercalcul d'OpenAI pour leur travail sur l'infrastructure logicielle et matérielle utilisée dans le projet, ainsi qu'à Pamela Mishkin pour ses conseils d'un point de vue politique. Cependant, on ne sait pas s'ils ont été directement impliqués dans la mise à disposition du modèle sur GitHub.\n\n[Plus d'informations nécessaires] sur les contributeurs supplémentaires, le cas échéant, qui ont été spécifiquement impliqués dans la publication du dépôt GitHub.",
    "answer_Russian": "Судя по предоставленным ссылкам, основными участниками, которые сделали модель openai/whisper-base.en доступной в Интернете в виде репозитория GitHub, являются:\n\nАлек Рэдфорд, Чон Ук Ким, Тао Сюй, Грег Брокман, Кристин Маккливи и Илья Суцкевер из OpenAI.\n\nВ ссылках также упоминается благодарность командам ускорения и суперкомпьютеров OpenAI за их работу над программной и аппаратной инфраструктурой, используемой в проекте, а также Памеле Мишкин за консультации с точки зрения политики. Однако неясно, принимали ли они непосредственное участие в размещении модели на GitHub.\n\n[Требуется дополнительная информация] о дополнительных участниках, если таковые имеются, которые непосредственно участвовали в публикации репозитория GitHub.",
    "answer_Japanese": "提供された参照に基づいて、モデル openai/whisper-base.en を GitHub リポジトリとしてオンラインで利用できるようにした主な貢献者は次のとおりです。\n\nOpenAI の Alec Radford、Jong Wook Kim、Tao Xu、Greg Brockman、Christine Mcleavey、Ilya Sutskever です。\n\n参考文献には、プロジェクトで使用されるソフトウェアとハ​​ードウェアのインフラストラクチャに関する作業について OpenAI のアクセラレーション チームとスーパーコンピューティング チームに感謝の意を表したほか、ポリシーの観点からアドバイスをくれた Pamela Mishkin 氏にも感謝の意が述べられています。ただし、GitHub でモデルを公開することに彼らが直接関与したかどうかは不明です。\n\nGitHub リポジトリの公開に特に関与した追加の寄稿者がいる場合は、[追加の情報が必要]。"
  },
  {
    "question": "model_type",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model is a supervised encoder-decoder Transformer model trained on a large dataset of diverse audio for multilingual speech recognition, speech translation, and language identification.",
    "answer_Chinese": "openai/whisper-base.en 模型是一个受监督的编码器-解码器 Transformer 模型，在各种音频的大型数据集上进行训练，用于多语言语音识别、语音翻译和语言识别。",
    "answer_Arabic": "نموذج openai/whisper-base.en هو نموذج محول تشفير وفك تشفير خاضع للإشراف تم تدريبه على مجموعة كبيرة من البيانات الصوتية المتنوعة للتعرف على الكلام متعدد اللغات وترجمة الكلام وتحديد اللغة.",
    "answer_French": "Le modèle openai/whisper-base.en est un modèle de transformateur d'encodeur-décodeur supervisé formé sur un vaste ensemble de données audio diverses pour la reconnaissance vocale multilingue, la traduction vocale et l'identification des langues.",
    "answer_Russian": "Модель openai/whisper-base.en представляет собой контролируемую модель преобразователя кодировщика-декодера, обученную на большом наборе данных разнообразного аудио для многоязычного распознавания речи, перевода речи и идентификации языка.",
    "answer_Japanese": "openai/whisper-base.en モデルは、多言語音声認識、音声翻訳、および言語識別のために、さまざまな音声の大規模なデータセットでトレーニングされた教師ありエンコーダー/デコーダー Transformer モデルです。"
  },
  {
    "question": "language",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model is an English-only speech recognition model that performs better than the multilingual base model for English speech transcription tasks.",
    "answer_Chinese": "openai/whisper-base.en 模型是一种纯英语语音识别模型，在英语语音转录任务中比多语言基础模型表现更好。",
    "answer_Arabic": "نموذج openai/whisper-base.en هو نموذج للتعرف على الكلام باللغة الإنجليزية فقط ويعمل بشكل أفضل من النموذج الأساسي متعدد اللغات لمهام نسخ الكلام باللغة الإنجليزية.",
    "answer_French": "Le modèle openai/whisper-base.en est un modèle de reconnaissance vocale uniquement en anglais qui fonctionne mieux que le modèle de base multilingue pour les tâches de transcription vocale en anglais.",
    "answer_Russian": "Модель openai/whisper-base.en — это модель распознавания речи только на английском языке, которая работает лучше, чем многоязычная базовая модель, для задач транскрипции английской речи.",
    "answer_Japanese": "openai/whisper-base.en モデルは、英語のみの音声認識モデルであり、英語音声の書き起こしタスクでは多言語基本モデルよりも優れたパフォーマンスを発揮します。"
  },
  {
    "question": "license",
    "chain": "generation",
    "answer": "Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.",
    "answer_Chinese": "Whisper 的代码和模型权重是根据 MIT 许可证发布的。有关更多详细信息，请参阅 [许可证](https://github.com/openai/whisper/blob/main/LICENSE)。",
    "answer_Arabic": "تم إصدار كود Whisper وأوزان النموذج بموجب ترخيص MIT. راجع [الترخيص](https://github.com/openai/whisper/blob/main/LICENSE) لمزيد من التفاصيل.",
    "answer_French": "Le code et les poids des modèles de Whisper sont publiés sous la licence MIT. Voir [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) pour plus de détails.",
    "answer_Russian": "Код Whisper и веса моделей выпускаются под лицензией MIT. Дополнительную информацию см. в [ЛИЦЕНЗИИ](https://github.com/openai/whisper/blob/main/LICENSE).",
    "answer_Japanese": "Whisper のコードとモデルの重みは MIT ライセンスに基づいて公開されています。詳細については、[ライセンス](https://github.com/openai/whisper/blob/main/LICENSE) を参照してください。"
  },
  {
    "question": "finetuned_from",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base.en does not appear to be fine-tuned from another model. The references mention training a suite of models of various sizes from scratch to study the scaling properties of Whisper. There is no indication that the base model was fine-tuned from a pre-existing model.\n\n[More Information Needed] on whether openai/whisper-base.en was fine-tuned from another model. The provided references do not contain this information.",
    "answer_Chinese": "根据提供的参考资料，模型 openai/whisper-base.en 似乎没有从另一个模型进行微调。参考文献提到从头开始训练一套不同大小的模型来研究 Whisper 的缩放特性。没有迹象表明基础模型是根据现有模型进行微调的。\n\n[需要更多信息] 关于 openai/whisper-base.en 是否从另一个模型进行了微调。提供的参考资料不包含此信息。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، لا يبدو أن النموذج openai/whisper-base.en تم تحسينه من نموذج آخر. تشير المراجع إلى تدريب مجموعة من النماذج بأحجام مختلفة من الصفر لدراسة خصائص قياس Whisper. ليس هناك ما يشير إلى أن النموذج الأساسي قد تم ضبطه بدقة من نموذج موجود مسبقًا.\n\n[بحاجة إلى مزيد من المعلومات] حول ما إذا كان openai/whisper-base.en قد تم ضبطه من نموذج آخر. المراجع المقدمة لا تحتوي على هذه المعلومات.",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base.en ne semble pas être affiné à partir d'un autre modèle. Les références mentionnent la formation d'une suite de modèles de différentes tailles à partir de zéro pour étudier les propriétés de mise à l'échelle de Whisper. Rien n’indique que le modèle de base ait été affiné à partir d’un modèle préexistant.\n\n[Plus d'informations nécessaires] pour savoir si openai/whisper-base.en a été affiné à partir d'un autre modèle. Les références fournies ne contiennent pas ces informations.",
    "answer_Russian": "Судя по предоставленным ссылкам, модель openai/whisper-base.en не является точной настройкой другой модели. В ссылках упоминается обучение с нуля набора моделей разного размера для изучения свойств масштабирования Whisper. Нет никаких признаков того, что базовая модель была доработана на основе ранее существовавшей модели.\n\n[Требуется дополнительная информация] о том, был ли openai/whisper-base.en точно настроен на основе другой модели. Предоставленные ссылки не содержат данной информации.",
    "answer_Japanese": "提供された参照に基づくと、モデル openai/whisper-base.en は別のモデルから微調整されているようには見えません。参考文献には、Whisper のスケーリング特性を研究するために、さまざまなサイズのモデルのスイートを最初からトレーニングすることが記載されています。基本モデルが既存のモデルから微調整された形跡はありません。\n\nopenai/whisper-base.en が別のモデルから微調整されたかどうかに関する [詳細情報が必要]。提供されている参考文献には、この情報は含まれていません。"
  },
  {
    "question": "demo_sources",
    "chain": "generation",
    "answer": "Here is the model card description for openai/whisper-base.en based on the provided references:\n\nModel name: openai/whisper-base.en\n\nLink to demo: [More Information Needed]\n\nThe references do not provide a direct link to a demo of the openai/whisper-base.en model. The Colab example notebook linked is for demonstrating usage on the LibriSpeech dataset in general, but is not a hosted demo of this specific model. More information would be needed to include a link to an interactive online demo of openai/whisper-base.en.\n\nPlease let me know if you need any other details filled out for the model card. As the project organizer, I'm happy to track down additional information as needed or put you in touch with the relevant team members who can provide more specifics. We're excited to publish this model to the community on Hugging Face.",
    "answer_Chinese": "以下是基于提供的参考资料的 openai/whisper-base.en 的模型卡描述：\n\n型号名称：openai/whisper-base.en\n\n演示链接：[需要更多信息]\n\n这些参考资料不提供 openai/whisper-base.en 模型演示的直接链接。链接的 Colab 示例笔记本用于演示 LibriSpeech 数据集的一般用法，但不是此特定模型的托管演示。需要更多信息来包含 openai/whisper-base.en 交互式在线演示的链接。\n\n如果您需要填写型号卡的任何其他详细信息，请告诉我。作为项目组织者，我很乐意根据需要追踪其他信息，或者让您与可以提供更多具体信息的相关团队成员取得联系。我们很高兴能将此模型发布到 Hugging Face 社区。",
    "answer_Arabic": "فيما يلي وصف البطاقة النموذجية لـ openai/whisper-base.en بناءً على المراجع المتوفرة:\n\nاسم الموديل: openai/whisper-base.en\n\nرابط العرض التوضيحي: [يلزم مزيد من المعلومات]\n\nلا توفر المراجع رابطًا مباشرًا لعرض توضيحي لنموذج openai/whisper-base.en. مثال دفتر ملاحظات Colab المرتبط مخصص لتوضيح الاستخدام على مجموعة بيانات LibriSpeech بشكل عام، ولكنه ليس عرضًا توضيحيًا مستضافًا لهذا النموذج المحدد. ستكون هناك حاجة إلى مزيد من المعلومات لتضمين رابط لعرض توضيحي تفاعلي عبر الإنترنت لـ openai/whisper-base.en.\n\nيرجى إعلامي إذا كنت بحاجة إلى أي تفاصيل أخرى مملوءة بنموذج البطاقة. باعتباري منظم المشروع، يسعدني تتبع المعلومات الإضافية حسب الحاجة أو توصيلك بأعضاء الفريق المعنيين الذين يمكنهم تقديم المزيد من التفاصيل. نحن متحمسون لنشر هذا النموذج للمجتمع على Hugging Face.",
    "answer_French": "Voici la description du modèle de carte pour openai/whisper-base.en basée sur les références fournies :\n\nNom du modèle : openai/whisper-base.en\n\nLien vers la démo : [Plus d'informations nécessaires]\n\nLes références ne fournissent pas de lien direct vers une démo du modèle openai/whisper-base.en. L'exemple de bloc-notes Colab lié sert à démontrer l'utilisation de l'ensemble de données LibriSpeech en général, mais n'est pas une démo hébergée de ce modèle spécifique. Plus d'informations seraient nécessaires pour inclure un lien vers une démo interactive en ligne de openai/whisper-base.en.\n\nS'il vous plaît laissez-moi savoir si vous avez besoin d'autres détails pour la carte modèle. En tant qu'organisateur du projet, je me ferai un plaisir de rechercher des informations supplémentaires si nécessaire ou de vous mettre en contact avec les membres de l'équipe concernés qui peuvent vous fournir plus de détails. Nous sommes ravis de publier ce modèle à la communauté sur Hugging Face.",
    "answer_Russian": "Вот описание карты модели для openai/whisper-base.en, основанное на предоставленных ссылках:\n\nНазвание модели: openai/whisper-base.en\n\nСсылка на демо: [Требуется дополнительная информация]\n\nСсылки не содержат прямой ссылки на демо-версию модели openai/whisper-base.en. Связанный пример блокнота Colab предназначен для демонстрации использования набора данных LibriSpeech в целом, но не является размещенной демонстрацией этой конкретной модели. Для включения ссылки на интерактивную онлайн-демонстрацию openai/whisper-base.en потребуется дополнительная информация.\n\nПожалуйста, дайте мне знать, если вам нужно заполнить какие-либо другие данные для карты модели. Как организатор проекта, я буду рад получить дополнительную информацию по мере необходимости или связать вас с соответствующими членами команды, которые могут предоставить более подробную информацию. Мы рады опубликовать эту модель для сообщества Hugging Face.",
    "answer_Japanese": "提供された参考資料に基づいた、openai/whisper-base.en のモデル カードの説明は次のとおりです。\n\nモデル名：openai/whisper-base.jp\n\nデモへのリンク: [さらに詳しい情報が必要です]\n\n参考資料には、openai/whisper-base.en モデルのデモへの直接リンクは提供されていません。リンクされている Colab サンプル ノートブックは、一般に LibriSpeech データセットの使用法をデモンストレーションするためのものですが、この特定のモデルのホストされたデモではありません。 openai/whisper-base.en のインタラクティブなオンライン デモへのリンクを含めるには、さらに詳しい情報が必要です。\n\nモデルカードにその他の詳細を記入する必要がある場合はお知らせください。プロジェクトの主催者として、必要に応じて追加情報を追跡したり、より詳細な情報を提供できる関連チームのメンバーに連絡したりさせていただきます。このモデルを Hugging Face のコミュニティに公開できることを嬉しく思います。"
  },
  {
    "question": "direct_use",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model can be used for English speech recognition without any fine-tuning, post-processing or complex pipeline. It can be used directly in Python like this:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\") \nprint(result[\"text\"])\n```\n\nThis code snippet loads the base model, transcribes the audio from the \"audio.mp3\" file, and prints out the recognized text.\n\nThe model can also be used from the command line to transcribe English audio files:\n\nwhisper audio.flac audio.mp3 audio.wav --model medium\n\nThis command will transcribe the specified audio files using the medium sized model. The default small model also works well for English transcription.\n\n[More Information Needed] on using the model for other languages, translation, or language identification. The model card focuses on its strong English speech recognition capabilities that work out-of-the-box.",
    "answer_Chinese": "openai/whisper-base.en 模型可用于英语语音识别，无需任何微调、后处理或​​复杂的管道。它可以直接在Python中使用，如下所示：\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\")\n结果 = model.transcribe(\"audio.mp3\") \n打印（结果[“文本”]）\n````\n\n此代码片段加载基本模型，转录“audio.mp3”文件中的音频，并打印出识别的文本。\n\n该模型还可以从命令行使用来转录英语音频文件：\n\n耳语音频.flac 音频.mp3 音频.wav --模型媒体\n\n此命令将使用中型模型转录指定的音频文件。默认的小模型也适用于英语转录。\n\n[需要更多信息]有关将该模型用于其他语言、翻译或语言识别的信息。该模型卡侧重于其强大的开​​箱即用的英语语音识别功能。",
    "answer_Arabic": "يمكن استخدام نموذج openai/whisper-base.en للتعرف على الكلام باللغة الإنجليزية دون أي ضبط دقيق أو معالجة لاحقة أو مسار معقد. يمكن استخدامه مباشرة في بايثون مثل هذا:\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\")\nالنتيجة = model.transcribe(\"audio.mp3\") \nطباعة (نتيجة [\"نص\"])\n```\n\nيقوم مقتطف الكود هذا بتحميل النموذج الأساسي، وينسخ الصوت من ملف \"audio.mp3\"، ويطبع النص الذي تم التعرف عليه.\n\nيمكن أيضًا استخدام النموذج من سطر الأوامر لنسخ الملفات الصوتية باللغة الإنجليزية:\n\nWhisper audio.flac audio.mp3 audio.wav - نموذج متوسط\n\nسيقوم هذا الأمر بنسخ الملفات الصوتية المحددة باستخدام النموذج المتوسط ​​الحجم. يعمل النموذج الصغير الافتراضي أيضًا بشكل جيد مع النسخ باللغة الإنجليزية.\n\n[مطلوب مزيد من المعلومات] حول استخدام النموذج للغات الأخرى أو الترجمة أو تحديد اللغة. تركز البطاقة النموذجية على قدراتها القوية في التعرف على الكلام باللغة الإنجليزية والتي تعمل خارج الصندوق.",
    "answer_French": "Le modèle openai/whisper-base.en peut être utilisé pour la reconnaissance vocale en anglais sans réglage fin, post-traitement ou pipeline complexe. Il peut être utilisé directement en Python comme ceci :\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\")\nrésultat = modèle.transcribe(\"audio.mp3\") \nimprimer(résultat[\"texte\"])\n```\n\nCet extrait de code charge le modèle de base, transcrit l'audio du fichier \"audio.mp3\" et imprime le texte reconnu.\n\nLe modèle peut également être utilisé depuis la ligne de commande pour transcrire des fichiers audio anglais :\n\nmurmurer audio.flac audio.mp3 audio.wav --model medium\n\nCette commande transcrira les fichiers audio spécifiés en utilisant le modèle de taille moyenne. Le petit modèle par défaut fonctionne également bien pour la transcription en anglais.\n\n[Plus d'informations nécessaires] sur l'utilisation du modèle pour d'autres langues, la traduction ou l'identification d'une langue. Le modèle de carte se concentre sur ses fortes capacités de reconnaissance vocale en anglais qui fonctionnent immédiatement.",
    "answer_Russian": "Модель openai/whisper-base.en можно использовать для распознавания английской речи без какой-либо тонкой настройки, постобработки или сложного конвейера. Его можно использовать непосредственно в Python следующим образом:\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\")\nрезультат = model.transcribe(\"audio.mp3\") \nпечать(результат[\"текст\"])\n```\n\nЭтот фрагмент кода загружает базовую модель, расшифровывает звук из файла audio.mp3 и распечатывает распознанный текст.\n\nМодель также можно использовать из командной строки для расшифровки английских аудиофайлов:\n\nшепот audio.flac audio.mp3 audio.wav --модель средняя\n\nЭта команда расшифровывает указанные аудиофайлы, используя модель среднего размера. Маленькая модель по умолчанию также хорошо подходит для английской транскрипции.\n\n[Требуется дополнительная информация] об использовании модели для других языков, переводе или идентификации языка. Модель карты ориентирована на сильные возможности распознавания английской речи, которые работают сразу после установки.",
    "answer_Japanese": "openai/whisper-base.en モデルは、微調整、後処理、複雑なパイプラインを必要とせずに、英語の音声認識に使用できます。次のように Python で直接使用できます。\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\")\nresult = model.transcribe(\"audio.mp3\") \nprint(結果[\"テキスト\"])\n「」\n\nこのコード スニペットは、基本モデルをロードし、「audio.mp3」ファイルから音声を転写し、認識されたテキストを出力します。\n\nこのモデルをコマンド ラインから使用して、英語の音声ファイルを書き写すこともできます。\n\nささやき audio.flac audio.mp3 audio.wav --model Medium\n\nこのコマンドは、中型モデルを使用して、指定されたオーディオ ファイルを転写します。デフォルトの小型モデルは、英語の書き起こしにも適しています。\n\n他の言語、翻訳、または言語識別用のモデルの使用に関する [詳細情報が必要]。このモデル カードは、すぐに使える強力な英語音声認識機能に焦点を当てています。"
  },
  {
    "question": "downstream_use",
    "chain": "generation",
    "answer": "The openai/whisper-base.en model can be fine-tuned or integrated into a larger speech processing pipeline in several ways:\n\n1. It can perform end-to-end speech recognition, transcribing audio into text. This can be done in Python:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\") \nprint(result[\"text\"])\n```\n\n2. The model provides lower-level access through `whisper.detect_language()` and `whisper.decode()` methods for more granular control.\n\n3. Whisper aims to be a single model handling the entire pipeline, reducing the complexity of separate components like voice activity detection, speaker diarization, etc. \n\n4. [More Information Needed]\n\n5. [More Information Needed]  \n\n6. Audio can be loaded and preprocessed using utility functions:\n\n```python\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n```\n\n7. The model is trained to directly predict raw transcripts without significant standardization, simplifying the pipeline.\n\n8. [More Information Needed]\n\n9. Fine-tuning on high-quality supervised data and reinforcement learning could help reduce errors like incorrect speaker names or hallucinated text.\n\nIn summary, openai/whisper-base.en provides a powerful and flexible foundation for speech recognition tasks that can be used directly or integrated into larger systems with some additional fine-tuning and optimization.",
    "answer_Chinese": "openai/whisper-base.en 模型可以通过多种方式进行微调或集成到更大的语音处理管道中：\n\n1.它可以进行端到端的语音识别，将音频转录为文本。这可以在 Python 中完成：\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\")\n结果 = model.transcribe(\"audio.mp3\") \n打印（结果[“文本”]）\n````\n\n2. 该模型通过“whisper.detect_language()”和“whisper.decode()”方法提供较低级别的访问，以实现更精细的控制。\n\n3. Whisper 的目标是成为处理整个管道的单一模型，降低语音活动检测、说话人分类等单独组件的复杂性。 \n\n4. [需要更多信息]\n\n5. [需要更多信息]  \n\n6. 可以使用实用函数加载和预处理音频：\n\n````蟒蛇\n音频 = 耳语.load_audio(\"音频.mp3\")\n音频 = 耳语.pad_or_trim(音频)\n````\n\n7. 该模型经过训练可以直接预测原始转录本，无需进行显着的标准化，从而简化了流程。\n\n8. [需要更多信息]\n\n9. 对高质量监督数据和强化学习进行微调可以帮助减少错误，例如不正确的说话者姓名或幻觉文本。\n\n总之，openai/whisper-base.en 为语音识别任务提供了强大而灵活的基础，可以直接使用或通过一些额外的微调和优化集成到更大的系统中。",
    "answer_Arabic": "يمكن ضبط نموذج openai/whisper-base.en أو دمجه في مسار أكبر لمعالجة الكلام بعدة طرق:\n\n1. يمكنه إجراء التعرف الشامل على الكلام، ونسخ الصوت إلى نص. يمكن القيام بذلك في بايثون:\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\")\nالنتيجة = model.transcribe(\"audio.mp3\") \nطباعة (نتيجة [\"نص\"])\n```\n\n2. يوفر النموذج وصولاً على مستوى أدنى من خلال طريقتي `whisper.detect_language()` و`whisper.decode()` لمزيد من التحكم الدقيق.\n\n3. يهدف Whisper إلى أن يكون نموذجًا واحدًا يتعامل مع المسار بأكمله، مما يقلل من تعقيد المكونات المنفصلة مثل اكتشاف النشاط الصوتي، وتسجيل المتحدث، وما إلى ذلك. \n\n4. [بحاجة إلى مزيد من المعلومات]\n\n5. [بحاجة إلى مزيد من المعلومات]  \n\n6. يمكن تحميل الصوت ومعالجته مسبقًا باستخدام وظائف الأداة المساعدة:\n\n```بيثون\nالصوت = whisper.load_audio(\"audio.mp3\")\nالصوت = whisper.pad_or_trim(الصوت)\n```\n\n7. تم تدريب النموذج على التنبؤ المباشر بالنصوص الأولية دون توحيد كبير، مما يؤدي إلى تبسيط المسار.\n\n8. [بحاجة إلى مزيد من المعلومات]\n\n9. يمكن أن يساعد الضبط الدقيق للبيانات الخاضعة للإشراف عالية الجودة والتعلم المعزز في تقليل الأخطاء مثل أسماء المتحدثين غير الصحيحة أو النصوص المهلوسة.\n\nباختصار، يوفر openai/whisper-base.en أساسًا قويًا ومرنًا لمهام التعرف على الكلام التي يمكن استخدامها مباشرة أو دمجها في أنظمة أكبر مع بعض الضبط والتحسين الإضافي.",
    "answer_French": "Le modèle openai/whisper-base.en peut être affiné ou intégré dans un pipeline de traitement vocal plus vaste de plusieurs manières :\n\n1. Il peut effectuer une reconnaissance vocale de bout en bout, transcrivant l’audio en texte. Cela peut être fait en Python :\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\")\nrésultat = modèle.transcribe(\"audio.mp3\") \nimprimer(résultat[\"texte\"])\n```\n\n2. Le modèle fournit un accès de niveau inférieur via les méthodes `whisper.detect_lingual()` et `whisper.decode()` pour un contrôle plus granulaire.\n\n3. Whisper vise à être un modèle unique gérant l'ensemble du pipeline, réduisant la complexité des composants distincts tels que la détection de l'activité vocale, la diarisation des locuteurs, etc. \n\n4. [Plus d'informations nécessaires]\n\n5. [Plus d'informations nécessaires]  \n\n6. L'audio peut être chargé et prétraité à l'aide de fonctions utilitaires :\n\n```python\naudio = murmure.load_audio(\"audio.mp3\")\naudio = murmure.pad_or_trim(audio)\n```\n\n7. Le modèle est entraîné pour prédire directement les transcriptions brutes sans standardisation significative, simplifiant ainsi le pipeline.\n\n8. [Plus d'informations nécessaires]\n\n9. Un réglage précis des données supervisées de haute qualité et un apprentissage par renforcement pourraient aider à réduire les erreurs telles que les noms incorrects des locuteurs ou les textes hallucinés.\n\nEn résumé, openai/whisper-base.en fournit une base puissante et flexible pour les tâches de reconnaissance vocale qui peuvent être utilisées directement ou intégrées dans des systèmes plus grands avec quelques réglages et optimisations supplémentaires.",
    "answer_Russian": "Модель openai/whisper-base.en можно доработать или интегрировать в более крупный конвейер обработки речи несколькими способами:\n\n1. Он может выполнять сквозное распознавание речи, транскрибируя звук в текст. Это можно сделать на Python:\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\")\nрезультат = model.transcribe(\"audio.mp3\") \nпечать(результат[\"текст\"])\n```\n\n2. Модель обеспечивает доступ более низкого уровня через методы `whisper.detect_language()` и `whisper.decode()` для более детального управления.\n\n3. Whisper стремится стать единой моделью, управляющей всем конвейером, уменьшая сложность отдельных компонентов, таких как обнаружение голосовой активности, дневникирование говорящего и т. д. \n\n4. [Требуется дополнительная информация]\n\n5. [Требуется дополнительная информация]  \n\n6. Аудио можно загрузить и предварительно обработать с помощью служебных функций:\n\n``` питон\nаудио = шепот.load_audio(\"audio.mp3\")\nаудио = шепот.pad_or_trim(аудио)\n```\n\n7. Модель обучена непосредственно прогнозировать необработанные транскрипты без значительной стандартизации, что упрощает конвейер.\n\n8. [Требуется дополнительная информация]\n\n9. Точная настройка на высококачественных контролируемых данных и обучение с подкреплением могут помочь уменьшить количество ошибок, таких как неправильные имена говорящих или галлюцинаторный текст.\n\nТаким образом, openai/whisper-base.en обеспечивает мощную и гибкую основу для задач распознавания речи, которую можно использовать напрямую или интегрировать в более крупные системы с некоторой дополнительной тонкой настройкой и оптимизацией.",
    "answer_Japanese": "openai/whisper-base.en モデルは、いくつかの方法で微調整したり、より大きな音声処理パイプラインに統合したりできます。\n\n1. エンドツーエンドの音声認識を実行し、音声をテキストに転写できます。これは Python で実行できます。\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\")\nresult = model.transcribe(\"audio.mp3\") \nprint(結果[\"テキスト\"])\n「」\n\n2. このモデルは、より詳細な制御のために `whisper.detect_ language()` および `whisper.decode()` メソッドを通じて下位レベルのアクセスを提供します。\n\n3. Whisper は、パイプライン全体を処理する単一のモデルとなり、音声アクティビティの検出、話者ダイアライゼーションなどの個別のコンポーネントの複雑さを軽減することを目指しています。 \n\n4. [さらに詳しい情報が必要]\n\n5. [さらに詳しい情報が必要です]  \n\n6. オーディオは、ユーティリティ関数を使用してロードおよび前処理できます。\n\n「」パイソン\naudio = ささやき.load_audio(\"audio.mp3\")\nオーディオ = ささやき.pad_or_trim(オーディオ)\n「」\n\n7. モデルは、大幅な標準化を行わずに生のトランスクリプトを直接予測するようにトレーニングされており、パイプラインを簡素化します。\n\n8. [さらに詳しい情報が必要です]\n\n9. 高品質の教師付きデータと強化学習を微調整すると、間違った話者名や幻覚テキストなどのエラーを減らすことができます。\n\n要約すると、openai/whisper-base.en は、直接使用することも、微調整と最適化を追加して大規模なシステムに統合することもできる、音声認識タスクのための強力で柔軟な基盤を提供します。"
  },
  {
    "question": "out_of_scope_use",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here are some potential ways the model openai/whisper-base.en may foreseeably be misused and what users ought not do with it:\n\n- The model may hallucinate or generate transcripts entirely unrelated to the actual audio input in some cases, especially on the first/last few words (ref 1). Users should not rely on it for applications requiring high accuracy on every single word.\n\n- The model's performance degrades significantly with noisy audio (ref 4, 5). Users should not expect it to work reliably in very noisy environments without further fine-tuning. \n\n- The model has a tendency to make incorrect guesses about speaker names when they are not inferable from the audio (ref 8). Users should not assume the model can accurately identify speakers just from voice.\n\n- [More Information Needed] on other specific ways the model may be intentionally misused, such as for generating deceptive or inappropriate content. The references focus more on robustness limitations than deliberate misuse.\n\nIn general, users should be aware of the model's limitations, not rely on it for high-stakes applications without further testing and adaptation, and use it responsibly in accordance with ethical AI principles. More targeted testing would help identify additional foreseeable misuses to call out.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，以下是模型 openai/whisper-base.en 可能被滥用的一些潜在方式以及用户不应该用它做什么：\n\n- 在某些情况下，模型可能会产生幻觉或生成与实际音频输入完全无关的文字记录，尤其是在第一个/最后几个单词上（参考 1）。用户不应该依赖它来实现对每个单词都要求高精度的应用程序。\n\n- 模型的性能在嘈杂的音频中显着下降（参考文献 4、5）。用户不应期望它能够在非常嘈杂的环境中可靠地工作而无需进一步微调。 \n\n- 当无法从音频中推断出说话者姓名时，模型倾向于对说话者姓名作出错误猜测（参考文献 8）。用户不应假设该模型仅通过语音即可准确识别说话者。\n\n- [需要更多信息] 该模型可能被故意滥用的其他特定方式，例如生成欺骗性或不适当的内容。这些参考文献更多地关注稳健性限制而不是故意滥用。\n\n一般来说，用户应该意识到该模型的局限性，不要在没有进一步测试和适应的情况下依赖它进行高风险应用，并根据道德人工智能原则负责任地使用它。更有针对性的测试将有助于识别其他可预见的滥用行为。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج Whisper، فيما يلي بعض الطرق المحتملة لإساءة استخدام النموذج openai/whisper-base.en وما يجب ألا يفعله المستخدمون به:\n\n- قد يهذي النموذج أو ينشئ نصوصًا لا علاقة لها تمامًا بإدخال الصوت الفعلي في بعض الحالات، خاصة في الكلمات القليلة الأولى/الأخيرة (المرجع 1). يجب ألا يعتمد المستخدمون عليه في التطبيقات التي تتطلب دقة عالية في كل كلمة.\n\n- يتدهور أداء النموذج بشكل كبير مع وجود ضوضاء في الصوت (المرجع 4، 5). يجب ألا يتوقع المستخدمون أن يعمل بشكل موثوق في البيئات الصاخبة للغاية دون إجراء المزيد من الضبط الدقيق. \n\n- يميل النموذج إلى إجراء تخمينات غير صحيحة حول أسماء المتحدثين عندما لا يمكن استنتاجها من الصوت (المرجع 8). يجب ألا يفترض المستخدمون أن النموذج يمكنه التعرف بدقة على مكبرات الصوت من الصوت فقط.\n\n- [مطلوب مزيد من المعلومات] حول الطرق المحددة الأخرى التي قد يتم بها إساءة استخدام النموذج عمدًا، مثل إنشاء محتوى خادع أو غير مناسب. تركز المراجع على قيود المتانة أكثر من سوء الاستخدام المتعمد.\n\nبشكل عام، يجب أن يكون المستخدمون على دراية بقيود النموذج، وعدم الاعتماد عليه في التطبيقات عالية المخاطر دون مزيد من الاختبار والتكيف، واستخدامه بمسؤولية وفقًا لمبادئ الذكاء الاصطناعي الأخلاقية. ومن شأن المزيد من الاختبارات المستهدفة أن تساعد في تحديد حالات إساءة الاستخدام المتوقعة الإضافية التي يجب ملاحظتها.",
    "answer_French": "Sur la base des références fournies sur le modèle Whisper, voici quelques façons potentielles dont le modèle openai/whisper-base.en pourrait être utilisé à mauvais escient et ce que les utilisateurs ne devraient pas en faire :\n\n- Le modèle peut halluciner ou générer des transcriptions totalement indépendantes de l'entrée audio réelle dans certains cas, notamment sur les premiers/derniers mots (réf 1). Les utilisateurs ne doivent pas s'y fier pour les applications nécessitant une grande précision sur chaque mot.\n\n- Les performances du modèle se dégradent considérablement en cas d'audio bruyant (réf 4, 5). Les utilisateurs ne doivent pas s’attendre à ce qu’il fonctionne de manière fiable dans des environnements très bruyants sans un réglage plus précis. \n\n- Le modèle a tendance à faire des suppositions incorrectes sur les noms des locuteurs lorsqu'ils ne peuvent pas être déduits de l'audio (réf. 8). Les utilisateurs ne doivent pas supposer que le modèle peut identifier avec précision les locuteurs uniquement à partir de la voix.\n\n- [Plus d'informations nécessaires] sur d'autres manières spécifiques dont le modèle peut être intentionnellement utilisé à mauvais escient, par exemple pour générer du contenu trompeur ou inapproprié. Les références se concentrent davantage sur les limitations de robustesse que sur une utilisation abusive délibérée.\n\nEn général, les utilisateurs doivent être conscients des limites du modèle, ne pas s'y fier pour des applications à enjeux élevés sans tests et adaptations supplémentaires, et l'utiliser de manière responsable, conformément aux principes éthiques de l'IA. Des tests plus ciblés aideraient à identifier d’autres abus prévisibles à dénoncer.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель Whisper, вот несколько потенциальных способов неправильного использования модели openai/whisper-base.en и того, что пользователям не следует с ней делать:\n\n- В некоторых случаях модель может галлюцинировать или генерировать расшифровки, совершенно не связанные с реальным аудиовходом, особенно в первых/последних нескольких словах (ссылка 1). Пользователям не следует полагаться на него в приложениях, требующих высокой точности каждого слова.\n\n- Производительность модели значительно ухудшается при шумном звуке (ссылки 4, 5). Пользователям не следует ожидать, что он будет надежно работать в очень шумной обстановке без дополнительной тонкой настройки. \n\n- Модель имеет тенденцию делать неверные предположения об именах говорящих, если их нельзя вывести из звука (ссылка 8). Пользователи не должны полагать, что модель может точно идентифицировать говорящего только по голосу.\n\n- [Требуется дополнительная информация] о других конкретных способах преднамеренного неправильного использования модели, например, для создания вводящего в заблуждение или неприемлемого контента. В ссылках больше внимания уделяется ограничениям надежности, чем преднамеренному неправильному использованию.\n\nВ целом, пользователи должны знать об ограничениях модели, не полагаться на нее для важных приложений без дальнейшего тестирования и адаптации и использовать ее ответственно в соответствии с этическими принципами ИИ. Более целенаправленное тестирование поможет выявить дополнительные предсказуемые злоупотребления, о которых следует сообщить.",
    "answer_Japanese": "Whisper モデルに関して提供されている参考資料に基づいて、openai/whisper-base.en モデルが悪用される可能性が予測される方法と、ユーザーがそれを使用してすべきでないことをいくつか示します。\n\n- モデルは、場合によっては、特に最初/最後の数単語で幻覚を示したり、実際の音声入力とはまったく関係のないトランスクリプトを生成したりすることがあります (参考 1)。ユーザーは、すべての単語に対して高い精度を必要とするアプリケーションにこれを依存しないでください。\n\n- ノイズの多いオーディオではモデルのパフォーマンスが大幅に低下します (参照 4、5)。ユーザーは、さらに微調整することなく、非常に騒がしい環境で確実に動作することを期待すべきではありません。 \n\n- このモデルは、音声からスピーカー名が推測できない場合、スピーカー名について誤った推測を行う傾向があります (参考 8)。ユーザーは、このモデルが音声だけから話者を正確に識別できると想定しないでください。\n\n- 欺瞞的または不適切なコンテンツの生成など、モデルが意図的に悪用される可能性のあるその他の具体的な方法に関する [さらなる情報が必要]。参考文献では、意図的な誤用よりも堅牢性の制限に焦点を当てています。\n\n一般に、ユーザーはモデルの限界を認識し、追加のテストや適応を行わずに一か八かのアプリケーションに依存せず、AI の倫理原則に従って責任を持って使用する必要があります。より対象を絞ったテストは、さらに予見可能な悪用を特定して指摘するのに役立ちます。"
  },
  {
    "question": "bias_risks_limitations",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper speech recognition model, here are some known or foreseeable issues, limitations, and potential harms:\n\nTechnical Limitations:\n1. Performance on lower-resource languages is still quite poor, likely due to the English-centric training data. Targeted efforts to increase data for rarer languages could significantly improve average speech recognition performance.\n\n2. The model struggles with certain stubborn error types in long-form transcription, such as getting stuck in repeat loops, not transcribing the first or last few words, or hallucinating unrelated transcripts. Fine-tuning on high-quality supervised data or using reinforcement learning could help reduce these errors.\n\n3. The model has a tendency to transcribe plausible but incorrect speaker names, as this information is rarely inferable from the audio alone.\n\nSociotechnical Issues and Potential Harms:\n1. [More Information Needed] The references do not directly discuss sociotechnical issues or potential harms.\n\nMisunderstandings and Unknowns:\n1. It's unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both. Further studies could help clarify this by ablating design components or comparing with existing speech recognition encoders.\n\n2. The robustness properties of Whisper have been the focus, so the potential improvements from fine-tuning on high-quality supervised data for specific domains are unknown and require further study.\n\nTo summarize, the main known issues are technical limitations related to performance on low-resource languages, certain stubborn error types, and incorrect speaker name transcription. More information is needed to assess potential sociotechnical issues and harms. Key areas for further study include understanding the relative contributions of model components and the effects of fine-tuning.",
    "answer_Chinese": "根据提供的有关 Whisper 语音识别模型的参考资料，以下是一些已知或可预见的问题、限制和潜在危害：\n\n技术限制：\n1. 在资源较低的语言上的性能仍然很差，可能是由于以英语为中心的训练数据。有针对性地增加稀有语言的数据可以显着提高平均语音识别性能。\n\n2. 该模型与长格式转录中的某些顽固错误类型作斗争，例如陷入重复循环、不转录第一个或最后几个单词，或者产生不相关的转录本的幻觉。对高质量监督数据进行微调或使用强化学习可以帮助减少这些错误。\n\n3. 该模型倾向于转录看似合理但不正确的说话者姓名，因为仅从音频中很难推断出此信息。\n\n社会技术问题和潜在危害：\n1. [需要更多信息] 参考文献不直接讨论社会技术问题或潜在危害。\n\n误解和未知：\n1. 目前还不清楚 Whisper 的好处在多大程度上源于对其编码器、解码器或两者的训练。进一步的研究可以通过消除设计组件或与现有的语音识别编码器进行比较来帮助澄清这一点。\n\n2. Whisper 的稳健性一直是关注的焦点，因此对特定领域的高质量监督数据进行微调的潜在改进尚不清楚，需要进一步研究。\n\n总而言之，主要的已知问题是与低资源语言性能相关的技术限制、某些顽固错误类型以及不正确的说话人姓名转录。需要更多信息来评估潜在的社会技术问题和危害。进一步研究的关键领域包括了解模型组件的相对贡献和微调的效果。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج التعرف على الكلام Whisper، إليك بعض المشكلات والقيود والأضرار المحتملة المعروفة أو المتوقعة:\n\nالقيود الفنية:\n1. لا يزال الأداء في اللغات ذات الموارد المنخفضة ضعيفًا للغاية، ويرجع ذلك على الأرجح إلى بيانات التدريب التي تركز على اللغة الإنجليزية. يمكن للجهود المستهدفة لزيادة البيانات الخاصة باللغات النادرة أن تحسن بشكل كبير متوسط ​​أداء التعرف على الكلام.\n\n2. يواجه النموذج بعض أنواع الأخطاء العنيدة في النسخ الطويل، مثل التعثر في حلقات التكرار، أو عدم نسخ الكلمات القليلة الأولى أو الأخيرة، أو هلوسة النصوص غير ذات الصلة. يمكن أن يساعد الضبط الدقيق للبيانات عالية الجودة الخاضعة للإشراف أو استخدام التعلم المعزز في تقليل هذه الأخطاء.\n\n3. يميل النموذج إلى تدوين أسماء المتحدثين المعقولة ولكن غير الصحيحة، حيث نادرًا ما يمكن استنتاج هذه المعلومات من الصوت وحده.\n\nالقضايا الاجتماعية والتقنية والأضرار المحتملة:\n1. [بحاجة إلى مزيد من المعلومات] لا تناقش المراجع بشكل مباشر القضايا الاجتماعية التقنية أو الأضرار المحتملة.\n\nسوء الفهم والمجهول:\n1. ليس من الواضح إلى أي مدى تنبع فوائد Whisper من تدريب برنامج التشفير أو وحدة فك التشفير أو كليهما. يمكن أن تساعد الدراسات الإضافية في توضيح ذلك من خلال تقليص مكونات التصميم أو مقارنتها بأجهزة تشفير التعرف على الكلام الموجودة.\n\n2. لقد كانت خصائص قوة Whisper هي محور التركيز، وبالتالي فإن التحسينات المحتملة من الضبط الدقيق للبيانات عالية الجودة الخاضعة للإشراف لمجالات محددة غير معروفة وتتطلب المزيد من الدراسة.\n\nلتلخيص ذلك، تتمثل المشكلات الرئيسية المعروفة في القيود الفنية المتعلقة بالأداء في اللغات منخفضة الموارد، وبعض أنواع الأخطاء العنيدة، والنسخ غير الصحيح لاسم المتحدث. هناك حاجة إلى مزيد من المعلومات لتقييم المشكلات والأضرار الاجتماعية التقنية المحتملة. تشمل المجالات الرئيسية لمزيد من الدراسة فهم المساهمات النسبية لمكونات النموذج وتأثيرات الضبط الدقيق.",
    "answer_French": "Sur la base des références fournies concernant le modèle de reconnaissance vocale Whisper, voici quelques problèmes, limitations et préjudices potentiels connus ou prévisibles :\n\nLimites techniques :\n1. Les performances dans les langues à faibles ressources sont encore assez médiocres, probablement en raison des données de formation centrées sur l'anglais. Des efforts ciblés visant à augmenter les données sur les langues plus rares pourraient améliorer considérablement les performances moyennes de reconnaissance vocale.\n\n2. Le modèle est confronté à certains types d'erreurs tenaces dans la transcription longue, comme rester bloqué dans des boucles répétées, ne pas transcrire les premiers ou les derniers mots, ou avoir des transcriptions hallucinantes sans rapport. Un réglage fin sur des données supervisées de haute qualité ou le recours à l’apprentissage par renforcement pourraient contribuer à réduire ces erreurs.\n\n3. Le modèle a tendance à transcrire les noms des locuteurs plausibles mais incorrects, car ces informations sont rarement déductibles à partir de l'audio seul.\n\nProblèmes sociotechniques et préjudices potentiels :\n1. [Plus d'informations nécessaires] Les références ne traitent pas directement des problèmes sociotechniques ou des préjudices potentiels.\n\nMalentendus et inconnues :\n1. On ne sait pas exactement dans quelle mesure les avantages de Whisper découlent de la formation de son encodeur, de son décodeur ou des deux. D’autres études pourraient aider à clarifier cela en supprimant les composants de conception ou en les comparant avec les encodeurs de reconnaissance vocale existants.\n\n2. Les propriétés de robustesse de Whisper ont été au centre de l'attention, de sorte que les améliorations potentielles résultant d'un réglage fin des données supervisées de haute qualité pour des domaines spécifiques sont inconnues et nécessitent une étude plus approfondie.\n\nPour résumer, les principaux problèmes connus sont les limitations techniques liées aux performances sur les langues à faibles ressources, certains types d'erreurs tenaces et la transcription incorrecte du nom du locuteur. Plus d’informations sont nécessaires pour évaluer les problèmes et préjudices sociotechniques potentiels. Les principaux domaines nécessitant une étude plus approfondie comprennent la compréhension des contributions relatives des composants du modèle et les effets du réglage fin.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель распознавания речи Whisper, ниже приведены некоторые известные или прогнозируемые проблемы, ограничения и потенциальный вред:\n\nТехнические ограничения:\n1. Результаты обучения языкам с ограниченными ресурсами по-прежнему довольно низкие, вероятно, из-за данных обучения, ориентированных на английский язык. Целенаправленные усилия по увеличению количества данных для более редких языков могут значительно улучшить среднюю производительность распознавания речи.\n\n2. Модель борется с некоторыми упрямыми типами ошибок при длинной транскрипции, такими как застревание в повторяющихся циклах, нерасшифровка первых или последних нескольких слов или галлюцинации несвязанных транскриптов. Точная настройка высококачественных контролируемых данных или использование обучения с подкреплением могут помочь уменьшить эти ошибки.\n\n3. Модель имеет тенденцию записывать правдоподобные, но неправильные имена говорящих, поскольку эту информацию редко можно вывести только по звуку.\n\nСоциотехнические проблемы и потенциальный вред:\n1. [Необходима дополнительная информация] В ссылках напрямую не обсуждаются социотехнические проблемы или потенциальный вред.\n\nНедоразумения и неизвестные:\n1. Неясно, в какой степени преимущества Whisper связаны с обучением его кодера, декодера или того и другого. Дальнейшие исследования могут помочь прояснить это, удалив компоненты конструкции или сравнив их с существующими кодировщиками распознавания речи.\n\n2. В центре внимания были свойства надежности Whisper, поэтому потенциальные улучшения от тонкой настройки высококачественных контролируемых данных для конкретных областей неизвестны и требуют дальнейшего изучения.\n\nПодводя итог, можно сказать, что основными известными проблемами являются технические ограничения, связанные с производительностью на языках с ограниченными ресурсами, некоторые устойчивые типы ошибок и неправильная транскрипция имени говорящего. Требуется больше информации для оценки потенциальных социотехнических проблем и вреда. Ключевые области для дальнейшего изучения включают понимание относительного вклада компонентов модели и эффектов тонкой настройки.",
    "answer_Japanese": "Whisper 音声認識モデルに関して提供されている参考資料に基づいて、既知または予測可能な問題、制限、および潜在的な害をいくつか示します。\n\n技術的な制限:\n1. おそらく英語中心のトレーニング データが原因で、リソースの少ない言語でのパフォーマンスは依然として非常に悪いです。より希少な言語のデータを増やすための的を絞った取り組みにより、平均的な音声認識パフォーマンスが大幅に向上する可能性があります。\n\n2. このモデルは、繰り返しループに陥る、最初または最後の数単語が転写されない、無関係な転写の幻覚など、長い形式の転写における特定の頑固なエラー タイプに対処します。高品質の教師付きデータを微調整するか、強化学習を使用すると、これらのエラーを減らすことができる可能性があります。\n\n3. この情報は音声だけからはほとんど推測できないため、このモデルには、もっともらしいが不正確な話者名を書き写す傾向があります。\n\n社会技術的問題と潜在的な危害:\n1. [さらなる情報が必要] 参考文献では、社会技術的な問題や潜在的な危害について直接議論されていません。\n\n誤解と不明点:\n1. Whisper の利点がそのエンコーダー、デコーダー、またはその両方のトレーニングからどの程度得られるかは不明です。今後の研究では、設計コンポーネントを除去したり、既存の音声認識エンコーダと比較したりすることで、これを明確にすることができます。\n\n2. Whisper の堅牢性特性が焦点となっているため、特定のドメインの高品質教師付きデータの微調整による潜在的な改善は不明であり、さらなる研究が必要です。\n\n要約すると、主な既知の問題は、低リソース言語でのパフォーマンスに関連する技術的な制限、特定の頑固なエラー タイプ、および不正確な話者名の転写です。潜在的な社会技術的問題や害を評価するには、より多くの情報が必要です。さらなる研究が必要な主な領域には、モデル コンポーネントの相対的な寄与と微調整の影響を理解することが含まれます。"
  },
  {
    "question": "bias_recommendations",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here are some recommendations regarding foreseeable issues with the openai/whisper-base.en model:\n\n1. The model's performance on lower-resource languages could be improved by increasing the amount of training data for those languages, as the current pre-training dataset is heavily biased towards English. [More Information Needed] on specific plans to source and incorporate more diverse language data.\n\n2. The model may struggle with certain types of errors in long-form transcription, such as getting stuck in repeat loops, failing to transcribe the beginning or end of audio segments, or hallucinating unrelated transcripts. Fine-tuning on high-quality supervised datasets or using reinforcement learning could help reduce these errors. [More Information Needed] on plans to address these specific error types.\n\n3. The model has a tendency to make plausible but incorrect guesses for speaker names, as this information is rarely inferable from the audio alone. [More Information Needed] on potential mitigation strategies for this issue.\n\n4. The model's performance varies widely depending on the language, with notably lower accuracy on many non-English languages. Refer to the WER/CER breakdown by language in the paper's Appendix D for more details. [More Information Needed] on plans to improve performance on the lowest-scoring languages.\n\n5. [More Information Needed] on any fine-tuning efforts or direct comparisons with other models, which could provide valuable insights into the model's strengths and weaknesses.\n\nIn summary, key areas to monitor and potentially address include language bias in training data, specific error types in long-form transcription, incorrect speaker labeling, and performance disparities across different languages. More information is needed on concrete plans to tackle these issues and further validate the model's performance.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，以下是有关 openai/whisper-base.en 模型可预见问题的一些建议：\n\n1. 由于当前的预训练数据集严重偏向英语，因此可以通过增加这些语言的训练数据量来提高模型在资源较低的语言上的性能。 [需要更多信息]关于获取和整合更多样化语言数据的具体计划。\n\n2. 该模型可能会遇到长格式转录中某些类型的错误，例如陷入重复循环、无法转录音频片段的开头或结尾，或者产生不相关的转录文本的幻觉。对高质量监督数据集进行微调或使用强化学习可以帮助减少这些错误。 [需要更多信息]有关解决这些特定错误类型的计划。\n\n3. 该模型倾向于对说话者姓名做出看似合理但不正确的猜测，因为仅从音频中很难推断出此信息。 [需要更多信息]有关此问题的潜在缓解策略。\n\n4. 该模型的性能根据语言的不同而有很大差异，在许多非英语语言上的准确率明显较低。有关更多详细信息，请参阅本文附录 D 中按语言划分的 WER/CER 细分。 [需要更多信息]有关提高得分最低语言性能的计划。\n\n5. [需要更多信息]关于任何微调工作或与其他模型的直接比较，这可以为模型的优点和缺点提供有价值的见解。\n\n总之，需要监控和潜在解决的关键领域包括训练数据中的语言偏差、长格式转录中的特定错误类型、不正确的说话人标签以及不同语言之间的性能差异。需要更多关于解决这些问题并进一步验证模型性能的具体计划的信息。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج Whisper، إليك بعض التوصيات المتعلقة بالمشكلات المتوقعة في نموذج openai/whisper-base.en:\n\n1. يمكن تحسين أداء النموذج على اللغات ذات الموارد المنخفضة من خلال زيادة كمية بيانات التدريب لتلك اللغات، حيث أن مجموعة بيانات ما قبل التدريب الحالية متحيزة بشدة نحو اللغة الإنجليزية. [مطلوب مزيد من المعلومات] حول خطط محددة للحصول على بيانات لغوية أكثر تنوعًا ودمجها.\n\n2. قد يعاني النموذج من أنواع معينة من الأخطاء في النسخ الطويل، مثل التعثر في حلقات التكرار، أو الفشل في نسخ بداية أو نهاية المقاطع الصوتية، أو الهذيان بالنصوص غير ذات الصلة. يمكن أن يساعد الضبط الدقيق لمجموعات البيانات عالية الجودة الخاضعة للإشراف أو استخدام التعلم المعزز في تقليل هذه الأخطاء. [يلزم مزيد من المعلومات] حول خطط معالجة أنواع الأخطاء المحددة هذه.\n\n3. يميل النموذج إلى تقديم تخمينات معقولة ولكنها غير صحيحة لأسماء المتحدثين، حيث نادرًا ما يمكن استنتاج هذه المعلومات من الصوت وحده. [بحاجة إلى مزيد من المعلومات] حول استراتيجيات التخفيف المحتملة لهذه المشكلة.\n\n4. يختلف أداء النموذج بشكل كبير حسب اللغة، مع دقة أقل بشكل ملحوظ في العديد من اللغات غير الإنجليزية. ارجع إلى تقسيم WER/CER حسب اللغة في الملحق د بالورقة لمزيد من التفاصيل. [مطلوب مزيد من المعلومات] حول خطط تحسين الأداء في اللغات ذات الدرجات الأقل.\n\n5. [يلزم مزيد من المعلومات] حول أي جهود ضبط أو مقارنات مباشرة مع النماذج الأخرى، والتي يمكن أن توفر رؤى قيمة حول نقاط القوة والضعف في النموذج.\n\nباختصار، تشمل المجالات الرئيسية التي يجب مراقبتها وربما معالجتها التحيز اللغوي في بيانات التدريب، وأنواع الأخطاء المحددة في النسخ الطويل، ووضع علامات غير صحيحة على المتحدثين، وفوارق الأداء بين اللغات المختلفة. هناك حاجة إلى مزيد من المعلومات حول الخطط الملموسة لمعالجة هذه المشكلات ومواصلة التحقق من صحة أداء النموذج.",
    "answer_French": "Sur la base des références fournies sur le modèle Whisper, voici quelques recommandations concernant les problèmes prévisibles avec le modèle openai/whisper-base.en :\n\n1. Les performances du modèle sur les langues à faibles ressources pourraient être améliorées en augmentant la quantité de données de formation pour ces langues, car l'ensemble de données de pré-formation actuel est fortement biaisé en faveur de l'anglais. [Plus d'informations nécessaires] sur les projets spécifiques visant à rechercher et à intégrer des données linguistiques plus diverses.\n\n2. Le modèle peut avoir des difficultés avec certains types d'erreurs dans la transcription longue, comme rester coincé dans des boucles répétées, ne pas réussir à transcrire le début ou la fin des segments audio ou avoir des transcriptions hallucinantes sans rapport. Un réglage fin sur des ensembles de données supervisés de haute qualité ou l’utilisation de l’apprentissage par renforcement pourraient contribuer à réduire ces erreurs. [Plus d'informations nécessaires] sur les plans visant à résoudre ces types d'erreurs spécifiques.\n\n3. Le modèle a tendance à faire des suppositions plausibles mais incorrectes concernant les noms des locuteurs, car ces informations sont rarement déductibles à partir du seul son. [Plus d'informations nécessaires] sur les stratégies d'atténuation potentielles pour ce problème.\n\n4. Les performances du modèle varient considérablement selon la langue, avec une précision nettement inférieure dans de nombreuses langues autres que l'anglais. Reportez-vous à la répartition WER/CER par langue dans l'annexe D du document pour plus de détails. [Plus d'informations nécessaires] sur les plans visant à améliorer les performances sur les langues les moins performantes.\n\n5. [Plus d'informations nécessaires] sur tout effort de réglage fin ou toute comparaison directe avec d'autres modèles, qui pourraient fournir des informations précieuses sur les forces et les faiblesses du modèle.\n\nEn résumé, les domaines clés à surveiller et éventuellement à résoudre incluent les biais linguistiques dans les données de formation, les types d'erreurs spécifiques dans la transcription longue, l'étiquetage incorrect des locuteurs et les disparités de performances entre les différentes langues. Plus d'informations sont nécessaires sur les plans concrets pour résoudre ces problèmes et valider davantage les performances du modèle.",
    "answer_Russian": "На основании предоставленных ссылок на модель Whisper, вот несколько рекомендаций относительно прогнозируемых проблем с моделью openai/whisper-base.en:\n\n1. Производительность модели на языках с ограниченными ресурсами можно улучшить, увеличив объем обучающих данных для этих языков, поскольку текущий набор данных для предварительного обучения сильно смещен в сторону английского языка. [Требуется дополнительная информация] о конкретных планах по получению и включению более разнообразных языковых данных.\n\n2. Модель может бороться с определенными типами ошибок в длинной транскрипции, такими как застревание в повторяющихся циклах, неспособность расшифровать начало или конец аудиосегментов или галлюцинации несвязанных транскриптов. Точная настройка высококачественных контролируемых наборов данных или использование обучения с подкреплением могут помочь уменьшить эти ошибки. [Требуется дополнительная информация] о планах по устранению этих конкретных типов ошибок.\n\n3. Модель имеет тенденцию делать правдоподобные, но неверные предположения об именах говорящих, поскольку эту информацию редко можно вывести только по звуку. [Требуется дополнительная информация] о потенциальных стратегиях смягчения последствий этой проблемы.\n\n4. Производительность модели сильно различается в зависимости от языка, при этом точность на многих неанглийских языках значительно ниже. Более подробную информацию можно найти в разбивке WER/CER по языкам в Приложении D к документу. [Требуется дополнительная информация] о планах по улучшению производительности языков с наименьшими оценками.\n\n5. [Необходима дополнительная информация] о любых попытках тонкой настройки или прямых сравнениях с другими моделями, которые могут дать ценную информацию о сильных и слабых сторонах модели.\n\nТаким образом, ключевые области, которые необходимо отслеживать и потенциально решать, включают языковую предвзятость в обучающих данных, конкретные типы ошибок в полной транскрипции, неправильную маркировку говорящих и различия в производительности на разных языках. Требуется дополнительная информация о конкретных планах решения этих проблем и дальнейшей проверки эффективности модели.",
    "answer_Japanese": "Whisper モデルに関して提供されている参考資料に基づいて、openai/whisper-base.en モデルで予見される問題に関する推奨事項をいくつか示します。\n\n1. 現在の事前トレーニング データセットは英語に大きく偏っているため、リソースの少ない言語でのモデルのパフォーマンスは、それらの言語のトレーニング データの量を増やすことで改善される可能性があります。より多様な言語データを調達し、組み込むための具体的な計画に関する [さらなる情報が必要]。\n\n2. モデルは、繰り返しループに陥る、オーディオ セグメントの最初または最後を転写できない、無関係な転写の幻覚など、長い形式の転写において特定のタイプのエラーに苦戦する可能性があります。高品質の教師ありデータセットを微調整したり、強化学習を使用したりすると、これらのエラーを減らすことができる可能性があります。これらの特定のエラー タイプに対処する計画に関する [詳細情報が必要]。\n\n3. この情報が音声だけから推測できることはほとんどないため、このモデルは、話者名についてもっともらしいが不正確な推測を行う傾向があります。この問題の潜在的な緩和戦略については [さらなる情報が必要]。\n\n4. モデルのパフォーマンスは言語によって大きく異なり、英語以外の多くの言語では精度が著しく低くなります。詳細については、この論文の付録 D にある言語別の WER/CER の内訳を参照してください。最もスコアの低い言語でのパフォーマンスを改善する計画に関する [さらなる情報が必要]。\n\n5. モデルの長所と短所についての貴重な洞察を提供する可能性のある、微調整作業や他のモデルとの直接比較に関する [さらなる情報が必要]。\n\n要約すると、監視し、対処すべき重要な領域には、トレーニング データの言語バイアス、長文書き起こしにおける特定のエラー タイプ、話者のラベル付けの誤り、言語間のパフォーマンスの差異が含まれます。これらの問題に取り組み、モデルのパフォーマンスをさらに検証するための具体的な計画については、さらなる情報が必要です。"
  },
  {
    "question": "training_data",
    "chain": "generation",
    "answer": "The training data for openai/whisper-base.en consists of a large dataset of diverse audio transcripts sourced primarily from English-centric parts of the internet, with minimal data pre-processing and filtering of machine-generated transcripts. [More Information Needed] on the specifics of data pre-processing and additional filtering steps.",
    "answer_Chinese": "openai/whisper-base.en 的训练数据由主要来自互联网以英语为中心的部分的各种音频转录本的大型数据集组成，对机器生成的转录本进行了最少的数据预处理和过滤。 [需要更多信息]有关数据预处理和其他过滤步骤的细节。",
    "answer_Arabic": "تتكون بيانات التدريب الخاصة بـ openai/whisper-base.en من مجموعة بيانات كبيرة من النصوص الصوتية المتنوعة التي يتم الحصول عليها بشكل أساسي من أجزاء الإنترنت التي تركز على اللغة الإنجليزية، مع الحد الأدنى من المعالجة المسبقة للبيانات وتصفية النصوص التي يتم إنشاؤها آليًا. [مطلوب مزيد من المعلومات] حول تفاصيل المعالجة المسبقة للبيانات وخطوات التصفية الإضافية.",
    "answer_French": "Les données de formation pour openai/whisper-base.en se composent d'un vaste ensemble de données de transcriptions audio diverses provenant principalement de parties d'Internet centrées sur l'anglais, avec un prétraitement minimal des données et un filtrage des transcriptions générées automatiquement. [Plus d'informations nécessaires] sur les spécificités du prétraitement des données et des étapes de filtrage supplémentaires.",
    "answer_Russian": "Обучающие данные для openai/whisper-base.en состоят из большого набора данных разнообразных аудиозаписей, полученных в основном из англоориентированных частей Интернета, с минимальной предварительной обработкой данных и фильтрацией машинно-генерируемых стенограмм. [Требуется дополнительная информация] об особенностях предварительной обработки данных и дополнительных этапах фильтрации.",
    "answer_Japanese": "openai/whisper-base.en のトレーニング データは、主にインターネットの英語中心部分からソースされた多様な音声トランスクリプトの大規模なデータセットで構成されており、機械生成されたトランスクリプトのデータの前処理とフィルタリングは最小限に抑えられています。データの前処理と追加のフィルタリング手順の詳細については、[追加の情報が必要]。"
  },
  {
    "question": "preprocessing",
    "chain": "generation",
    "answer": "For the openai/whisper-base.en model, the preprocessing steps include:\n\n1. Text preprocessing:\n   - Minimal standardization of transcripts, relying on the model to learn the mapping between utterances and their transcribed form.\n   - Text normalization for English texts, including handling punctuation, formatting, and stylistic aspects.\n   - Removing machine-generated transcripts using heuristics to avoid learning \"transcript-ese\".\n\n2. Audio preprocessing:\n   - Breaking audio files into 30-second segments paired with the corresponding transcript subset.\n   - Training on all audio segments, including those without speech, for voice activity detection.\n   - Converting audio to log mel spectrogram representation:\n     ```python\n     mel = whisper.log_mel_spectrogram(audio).to(model.device)\n     ```\n\n3. Language detection:\n   - Using an audio language detector to ensure the spoken language matches the transcript language.\n   - Including English transcripts even if the audio language doesn't match.\n\n[More Information Needed] on specific details of tokenization and resizing/rewriting.\n\nThe model can be used for transcription within Python:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\nThe `transcribe()` method processes the audio with a sliding 30-second window and performs autoregressive sequence-to-sequence predictions on each window.\n\nLower-level access to the model is provided through `whisper.detect_language()` and `whisper.decode()` functions.",
    "answer_Chinese": "对于openai/whisper-base.en模型，预处理步骤包括：\n\n1.文本预处理：\n   - 转录本的最小化标准化，依靠模型来学习话语与其转录形式之间的映射。\n   - 英文文本的文本规范化，包括处理标点符号、格式和风格方面。\n   - 使用启发式方法删除机器生成的转录本，以避免学习“转录本”。\n\n2. 音频预处理：\n   - 将音频文件分成 30 秒的片段，并与相应的转录子集配对。\n   - 对所有音频片段（包括没有语音的音频片段）进行训练，以进行语音活动检测。\n   - 将音频转换为对数梅尔频谱图表示：\n     ````蟒蛇\n     mel = 耳语.log_mel_spectrogram(音频).to(model.device)\n     ````\n\n3. 语言检测：\n   - 使用音频语言检测器确保口语与转录语言匹配。\n   - 包括英语成绩单，即使音频语言不匹配。\n\n[需要更多信息]有关标记化和调整大小/重写的具体细节。\n\n该模型可用于 Python 中的转录：\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\")\n结果 = model.transcribe(\"audio.mp3\")\n打印（结果[“文本”]）\n````\n\n`transcribe()` 方法使用 30 秒滑动窗口处理音频，并在每个窗口上执行自回归序列到序列预测。\n\n通过“whisper.detect_language()”和“whisper.decode()”函数提供对模型的较低级别访问。",
    "answer_Arabic": "بالنسبة لنموذج openai/whisper-base.en، تتضمن خطوات المعالجة المسبقة ما يلي:\n\n1. المعالجة المسبقة للنص:\n   - الحد الأدنى من توحيد النصوص، والاعتماد على النموذج لمعرفة الربط بين الألفاظ وشكلها المكتوب.\n   - تطبيع النص للنصوص الإنجليزية، بما في ذلك التعامل مع علامات الترقيم والتنسيق والجوانب الأسلوبية.\n   - إزالة النصوص التي تم إنشاؤها آليًا باستخدام الاستدلال لتجنب تعلم \"النسخة النصية\".\n\n2. المعالجة المسبقة للصوت:\n   - تقسيم الملفات الصوتية إلى مقاطع مدتها 30 ثانية مقترنة بالمجموعة الفرعية المقابلة للنص.\n   - التدريب على كافة المقاطع الصوتية بما فيها تلك التي لا تحتوي على كلام للكشف عن النشاط الصوتي.\n   - تحويل الصوت إلى تسجيل تمثيل الطيفي ميل:\n     ```بيثون\n     ميل = whisper.log_mel_spectrogram(audio).to(model.device)\n     ```\n\n3. كشف اللغة:\n   - استخدام كاشف اللغة الصوتية للتأكد من تطابق اللغة المنطوقة مع لغة النص.\n   - تضمين النصوص باللغة الإنجليزية حتى لو كانت اللغة الصوتية غير متطابقة.\n\n[يلزم مزيد من المعلومات] حول تفاصيل محددة حول الترميز وتغيير الحجم/إعادة الكتابة.\n\nيمكن استخدام النموذج للنسخ داخل بايثون:\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\")\nالنتيجة = model.transcribe(\"audio.mp3\")\nطباعة (نتيجة [\"نص\"])\n```\n\nتقوم طريقة `transcribe()` بمعالجة الصوت من خلال نافذة منزلقة مدتها 30 ثانية وتقوم بإجراء تنبؤات تسلسلية انحدارية تلقائية في كل نافذة.\n\nيتم توفير الوصول على المستوى الأدنى إلى النموذج من خلال الدالتين `whisper.detect_language()` و`whisper.decode()`.",
    "answer_French": "Pour le modèle openai/whisper-base.en, les étapes de prétraitement comprennent :\n\n1. Prétraitement du texte :\n   - Standardisation minimale des transcriptions, s'appuyant sur le modèle pour apprendre la correspondance entre les énoncés et leur forme transcrite.\n   - Normalisation de texte pour les textes anglais, y compris la gestion de la ponctuation, du formatage et des aspects stylistiques.\n   - Suppression des transcriptions générées automatiquement à l'aide d'heuristiques pour éviter d'apprendre le \"transcript-ese\".\n\n2. Prétraitement audio :\n   - Diviser les fichiers audio en segments de 30 secondes associés au sous-ensemble de transcription correspondant.\n   - Formation sur tous les segments audio, y compris ceux sans parole, pour la détection d'activité vocale.\n   - Conversion de l'audio en représentation du spectrogramme log mel :\n     ```python\n     mel = murmure.log_mel_spectrogram(audio).to(model.device)\n     ```\n\n3. Détection de langue :\n   - Utiliser un détecteur de langue audio pour garantir que la langue parlée correspond à la langue de transcription.\n   - Y compris les transcriptions en anglais même si la langue audio ne correspond pas.\n\n[Plus d'informations nécessaires] sur les détails spécifiques de la tokenisation et du redimensionnement/réécriture.\n\nLe modèle peut être utilisé pour la transcription dans Python :\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\")\nrésultat = modèle.transcribe(\"audio.mp3\")\nimprimer(résultat[\"texte\"])\n```\n\nLa méthode `transcribe()` traite l'audio avec une fenêtre glissante de 30 secondes et effectue des prédictions autorégressives séquence à séquence sur chaque fenêtre.\n\nL'accès de niveau inférieur au modèle est fourni via les fonctions `whisper.detect_lingual()` et `whisper.decode()`.",
    "answer_Russian": "Для модели openai/whisper-base.en этапы предварительной обработки включают в себя:\n\n1. Предварительная обработка текста:\n   - Минимальная стандартизация транскриптов, основанная на модели для изучения сопоставления между высказываниями и их транскрибируемой формой.\n   - Нормализация текста для английских текстов, включая обработку пунктуации, форматирования и стилистических аспектов.\n   - Удаление машинно-генерируемых транскриптов с использованием эвристики, чтобы избежать изучения «расшифровки стенограммы».\n\n2. Предварительная обработка звука:\n   - Разбиение аудиофайлов на 30-секундные сегменты в сочетании с соответствующим подмножеством стенограммы.\n   - Обучение на всех аудиосегментах, в том числе без речи, для определения голосовой активности.\n   - Преобразование звука в представление спектрограммы log mel:\n     ``` питон\n     mel = шепот.log_mel_spectrogram(аудио).to(model.device)\n     ```\n\n3. Распознавание языка:\n   - Использование детектора языка аудио, чтобы убедиться, что разговорный язык соответствует языку транскрипции.\n   - Включая расшифровки на английском языке, даже если язык аудио не совпадает.\n\n[Требуется дополнительная информация] о конкретных деталях токенизации и изменения размера/переписывания.\n\nМодель можно использовать для транскрипции в Python:\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\")\nрезультат = model.transcribe(\"audio.mp3\")\nпечать(результат[\"текст\"])\n```\n\nМетод «transcribe()» обрабатывает аудио с помощью скользящего 30-секундного окна и выполняет авторегрессионный последовательный прогноз для каждого окна.\n\nДоступ нижнего уровня к модели обеспечивается через функции `whisper.detect_language()` и `whisper.decode()`.",
    "answer_Japanese": "openai/whisper-base.en モデルの場合、前処理手順には次のものが含まれます。\n\n1. テキストの前処理:\n   - トランスクリプトの標準化を最小限に抑え、発話とその転写された形式の間のマッピングを学習するモデルに依存します。\n   - 句読点、書式設定、文体面の処理を含む、英語テキストのテキスト正規化。\n   - 「トランスクリプト」の学習を避けるために、ヒューリスティックを使用して機械によって生成されたトランスクリプトを削除します。\n\n2. オーディオの前処理:\n   - オーディオ ファイルを 30 秒のセグメントに分割し、対応するトランスクリプト サブセットと組み合わせます。\n   - 音声アクティビティ検出のために、音声のないセグメントを含むすべての音声セグメントのトレーニング。\n   - オーディオをログメルスペクトログラム表現に変換:\n     「」パイソン\n     mel = ささやき.log_mel_spectrogram(audio).to(model.device)\n     「」\n\n3. 言語検出:\n   - 音声言語検出器を使用して、話し言葉がトランスクリプト言語と一致することを確認します。\n   - 音声言語が一致しない場合でも、英語のトランスクリプトが含まれます。\n\nトークン化とサイズ変更/書き換えの具体的な詳細については、[さらなる情報が必要]。\n\nこのモデルは、Python 内での文字起こしに使用できます。\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\")\nresult = model.transcribe(\"audio.mp3\")\nprint(結果[\"テキスト\"])\n「」\n\n`transcribe()` メソッドは、30 秒のスライディング ウィンドウでオーディオを処理し、各ウィンドウで自己回帰シーケンス間の予測を実行します。\n\nモデルへの下位レベルのアクセスは、「whisper.detect_ language()」関数と「whisper.decode()」関数を通じて提供されます。"
  },
  {
    "question": "training_regime",
    "chain": "generation",
    "answer": "Based on the provided references, here are the training hyperparameters for the openai/whisper-base.en model:\n\n- Optimizer: AdamW\n- Gradient clipping: Gradient norm clipping was used\n- Learning rate schedule: Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size: 256 segments\n- Training duration: Trained for 220 updates, which is between two and three passes over the dataset\n- Precision: Trained with FP16 and dynamic loss scaling\n- Other techniques used: Data parallelism across accelerators, activation checkpointing\n\n[More Information Needed] on the exact learning rate, weight decay, warmup steps, and other hyperparameters not directly mentioned in the references.",
    "answer_Chinese": "根据提供的参考资料，以下是 openai/whisper-base.en 模型的训练超参数：\n\n- 优化器：AdamW\n- 梯度裁剪：使用梯度范数裁剪\n- 学习率计划：在前 2048 个更新的预热后，线性学习率衰减至零\n- 批量大小：256 段\n- 训练持续时间：训练 220 次更新，介于数据集的两到三遍之间\n- 精度：使用 FP16 和动态损失缩放进行训练\n- 使用的其他技术：跨加速器的数据并行性、激活检查点\n\n[需要更多信息]关于确切的学习率、权重衰减、预热步骤以及参考文献中未直接提及的其他超参数。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، فيما يلي معلمات التدريب الفائقة لنموذج openai/whisper-base.en:\n\n- محسن: AdamW\n- القطع المتدرج: تم استخدام القطع المعياري المتدرج\n- جدول معدل التعلم: يتراجع معدل التعلم الخطي إلى الصفر بعد عملية إحماء خلال تحديثات 2048 الأولى\n- حجم الدفعة: 256 قطعة\n- مدة التدريب: تم ​​التدريب على 220 تحديثًا، أي ما بين مرورين إلى ثلاثة تمريرات على مجموعة البيانات\n- الدقة: تم تدريبها باستخدام FP16 وقياس الخسارة الديناميكي\n- التقنيات الأخرى المستخدمة: توازي البيانات عبر المسرعات، فحص التنشيط\n\n[بحاجة إلى مزيد من المعلومات] حول معدل التعلم الدقيق، وتناقص الوزن، وخطوات الإحماء، وغيرها من المعلمات الفائقة التي لم يتم ذكرها مباشرة في المراجع.",
    "answer_French": "Sur la base des références fournies, voici les hyperparamètres de formation pour le modèle openai/whisper-base.en :\n\n- Optimiseur : AdamW\n- Découpage de dégradé : un découpage de norme de dégradé a été utilisé\n- Calendrier du taux d'apprentissage : le taux d'apprentissage linéaire diminue jusqu'à zéro après un échauffement au cours des 2048 premières mises à jour\n- Taille du lot : 256 segments\n- Durée de la formation : Formé pour 220 mises à jour, soit entre deux et trois passages sur l'ensemble de données\n- Précision : formé avec FP16 et mise à l'échelle dynamique des pertes\n- Autres techniques utilisées : Parallélisme des données entre accélérateurs, points de contrôle d'activation\n\n[Plus d'informations nécessaires] sur le taux d'apprentissage exact, la perte de poids, les étapes d'échauffement et d'autres hyperparamètres non directement mentionnés dans les références.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот гиперпараметры обучения для модели openai/whisper-base.en:\n\n- Оптимизатор: AdamW\n- Градиентное отсечение: использовалось градиентное отсечение нормы.\n- График скорости обучения: линейное снижение скорости обучения до нуля после прогрева в течение первых 2048 обновлений.\n- Размер партии: 256 сегментов.\n- Продолжительность обучения: обучение рассчитано на 220 обновлений, что составляет от двух до трех проходов по набору данных.\n- Точность: обучение с использованием FP16 и динамического масштабирования потерь.\n- Другие используемые методы: параллелизм данных между ускорителями, контрольные точки активации.\n\n[Требуется дополнительная информация] о точной скорости обучения, снижении веса, этапах разминки и других гиперпараметрах, прямо не упомянутых в ссылках.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base.en モデルのトレーニング ハイパーパラメーターを次に示します。\n\n- オプティマイザー: AdamW\n- 勾配クリッピング: 勾配ノルム クリッピングが使用されました。\n- 学習率スケジュール: 最初の 2048 アップデートのウォームアップ後に、線形学習率がゼロに減衰します。\n- バッチサイズ: 256 セグメント\n- トレーニング期間: 220 回の更新についてトレーニングされます。これは、データセット上で 2 回から 3 回のパスです。\n- 精度: FP16 と動的損失スケーリングでトレーニング済み\n- 使用されるその他の技術: アクセラレータ間でのデータ並列処理、アクティベーション チェックポイント処理\n\n正確な学習率、重み減衰、ウォームアップ ステップ、および参考文献に直接記載されていないその他のハイパーパラメータに関する [詳細情報が必要]。"
  },
  {
    "question": "speeds_sizes_times",
    "chain": "generation",
    "answer": "Based on the provided references, here are the details I can provide about the openai/whisper-base.en model:\n\nModel Architecture:\n- Encoder-decoder Transformer architecture\n- Audio is re-sampled to 16,000 Hz\n- 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds\n\nTraining:\n- Trained with AdamW optimizer and gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size of 256 segments was used\n- Trained for 2^20 updates, which is between two and three passes over the dataset\n- No data augmentation or regularization used during training\n\nPerformance:\n- The base.en model performs well for English-only applications\n- Difference in performance compared to multilingual base model becomes less significant for larger model sizes\n\n[More Information Needed] for the following:\n- Throughput\n- Start or end time of training\n- Checkpoint sizes",
    "answer_Chinese": "根据提供的参考资料，以下是我可以提供的有关 openai/whisper-base.en 模型的详细信息：\n\n模型架构：\n- 编码器-解码器 Transformer 架构\n- 音频重新采样至 16,000 Hz\n- 80 通道对数幅度 Mel 频谱图表示在 25 毫秒窗口上计算，步长为 10 毫秒\n\n训练：\n- 使用 AdamW 优化器和梯度范数裁剪进行训练\n- 在前 2048 个更新的预热后，线性学习率衰减至零\n- 使用 256 段的批量大小\n- 训练 2^20 次更新，即数据集的两到三遍之间\n- 训练期间不使用数据增强或正则化\n\n表现：\n- base.en 模型对于纯英语应用程序表现良好\n- 对于较大的模型尺寸，与多语言基础模型相比，性能差异变得不那么显着\n\n[需要更多信息]以下内容：\n- 吞吐量\n- 训练的开始或结束时间\n- 检查点大小",
    "answer_Arabic": "بناءً على المراجع المقدمة، إليك التفاصيل التي يمكنني تقديمها حول نموذج openai/whisper-base.en:\n\nالعمارة النموذجية:\n- بنية محول التشفير وفك التشفير\n- يتم إعادة أخذ عينات الصوت إلى 16000 هرتز\n- يتم حساب التمثيل الطيفي Mel ذو حجم السجل 80 قناة على نوافذ تبلغ 25 مللي ثانية مع خطوة قدرها 10 مللي ثانية\n\nتمرين:\n- تم التدريب على استخدام محسن AdamW وقص التدرج المعياري\n- انخفاض معدل التعلم الخطي إلى الصفر بعد الإحماء خلال تحديثات 2048 الأولى\n- تم استخدام حجم الدفعة 256 قطعة\n- تم التدريب على تحديثات 2^20، والتي تتراوح بين مرورين إلى ثلاثة تمريرات على مجموعة البيانات\n- لا يتم استخدام زيادة البيانات أو التنظيم أثناء التدريب\n\nأداء:\n- يقدم نموذج base.en أداءً جيدًا للتطبيقات الناطقة باللغة الإنجليزية فقط\n- الفرق في الأداء مقارنة بالنموذج الأساسي متعدد اللغات يصبح أقل أهمية بالنسبة لأحجام النماذج الأكبر\n\n[مطلوب مزيد من المعلومات] لما يلي:\n- الإنتاجية\n- وقت بداية أو نهاية التدريب\n- أحجام نقاط التفتيش",
    "answer_French": "Sur la base des références fournies, voici les détails que je peux fournir sur le modèle openai/whisper-base.en :\n\nArchitecture du modèle :\n- Architecture du transformateur codeur-décodeur\n- L'audio est rééchantillonné à 16 000 Hz\n- La représentation du spectrogramme Mel à 80 canaux de magnitude log est calculée sur des fenêtres de 25 millisecondes avec une foulée de 10 millisecondes\n\nEntraînement:\n- Formé avec l'optimiseur AdamW et le découpage des normes de gradient\n- Le taux d'apprentissage linéaire diminue jusqu'à zéro après un échauffement au cours des 2048 premières mises à jour\n- Une taille de lot de 256 segments a été utilisée\n- Formé pour 2^20 mises à jour, soit entre deux et trois passages sur l'ensemble de données\n- Aucune augmentation ou régularisation des données utilisée pendant la formation\n\nPerformance:\n- Le modèle base.en fonctionne bien pour les applications uniquement en anglais\n- La différence de performances par rapport au modèle de base multilingue devient moins significative pour les modèles de plus grande taille\n\n[Plus d'informations nécessaires] pour les éléments suivants :\n- Débit\n- Heure de début ou de fin de la formation\n- Tailles des points de contrôle",
    "answer_Russian": "На основании предоставленных ссылок я могу предоставить следующую информацию о модели openai/whisper-base.en:\n\nМодельная архитектура:\n- Трансформаторная архитектура кодер-декодер\n- Звук повторно дискретизируется до 16 000 Гц.\n- 80-канальное представление Mel-спектрограммы логарифмической величины рассчитывается в 25-миллисекундных окнах с шагом 10 миллисекунд.\n\nОбучение:\n- Обучение с использованием оптимизатора AdamW и обрезки нормы градиента.\n- Линейное снижение скорости обучения до нуля после прогрева в течение первых 2048 обновлений.\n- Использовался пакет размером 256 сегментов.\n- Обучено для 2^20 обновлений, что составляет от двух до трех проходов по набору данных.\n- Во время обучения не используется увеличение или регуляризация данных.\n\nПроизводительность:\n- Модель base.en хорошо работает для приложений только на английском языке.\n- Разница в производительности по сравнению с многоязычной базовой моделью становится менее значительной для моделей большего размера.\n\n[Требуется дополнительная информация] для следующего:\n- Пропускная способность\n- Время начала или окончания обучения\n- Размеры КПП",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base.en モデルについて提供できる詳細は次のとおりです。\n\nモデルのアーキテクチャ:\n- エンコーダー/デコーダートランスフォーマーのアーキテクチャ\n- オーディオは 16,000 Hz に再サンプリングされます\n- 80 チャネルの対数振幅メル スペクトログラム表現は、10 ミリ秒のストライドで 25 ミリ秒のウィンドウで計算されます。\n\nトレーニング：\n- AdamW オプティマイザーと勾配ノルム クリッピングを使用してトレーニング済み\n- 最初の 2048 アップデートのウォームアップ後、線形学習率がゼロに減衰します。\n- 256 セグメントのバッチ サイズが使用されました\n- データセットの 2 ～ 3 回のパスである 2^20 更新用にトレーニング済み\n- トレーニング中にデータの拡張や正則化は使用されません\n\nパフォーマンス：\n- Base.en モデルは英語のみのアプリケーションで適切にパフォーマンスを発揮します\n- 多言語基本モデルと比較したパフォーマンスの差は、モデルサイズが大きくなるとそれほど重要ではなくなります\n\n以下については[さらに詳しい情報が必要]:\n- スループット\n- トレーニングの開始時刻または終了時刻\n- チェックポイントのサイズ"
  },
  {
    "question": "testing_data",
    "chain": "generation",
    "answer": "Based on the provided references, the openai/whisper-base.en model was evaluated on the following benchmarks and datasets:\n\nShort-form English-only datasets:\n- LibriSpeech test-clean and test-other splits\n- TED-LIUM 3 test split\n- Common Voice 5.1 English subset\n- Artie bias corpus (subset of Common Voice)\n- CallHome and Switchboard corpora\n- WSJ corpus\n- CORAAL interviews\n\nOther English datasets:\n- CHiME-6 binaural recordings\n- AMI-IHM and AMI-SDM1 corpora\n- Rev16 (subset of 16 files from Rev.AI's Podcast Transcription Benchmark)\n\nMultilingual benchmarks:\n- Multilingual LibriSpeech (MLS)\n- VoxPopuli\n\nThe model's performance was also broken down by language using WER/CER metrics on the Common Voice 15 and Fleurs datasets. Additional detailed results can be found in Appendix D.1, D.2, and D.4 of the referenced paper.\n\n[More Information Needed] on the specific training details and hyperparameters used for the openai/whisper-base.en model.",
    "answer_Chinese": "根据提供的参考资料，openai/whisper-base.en 模型在以下基准和数据集上进行了评估：\n\n简短的纯英文数据集：\n- LibriSpeech 测试清理和测试其他分割\n- TED-LIUM 3 测试分组\n- Common Voice 5.1 英语子集\n- Artie 偏差语料库（Common Voice 的子集）\n- CallHome 和总机语料库\n- 华尔街日报语料库\n- CORAAL 采访\n\n其他英文数据集：\n- CHiME-6 双耳录音\n- AMI-IHM 和 AMI-SDM1 语料库\n- Rev16（来自 Rev.AI 播客转录基准的 16 个文件的子集）\n\n多语言基准：\n- 多语言 LibriSpeech (MLS)\n- 人民之声\n\n该模型的性能还使用 Common Voice 15 和 Fleurs 数据集上的 WER/CER 指标按语言进行了细分。其他详细结果可在参考论文的附录 D.1、D.2 和 D.4 中找到。\n\n[需要更多信息] 有关 openai/whisper-base.en 模型使用的具体训练细节和超参数。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، تم تقييم نموذج openai/whisper-base.en بناءً على المعايير ومجموعات البيانات التالية:\n\nمجموعات البيانات القصيرة باللغة الإنجليزية فقط:\n- LibriSpeech اختبار نظيف واختبار الانقسامات الأخرى\n- تقسيم اختبار TED-LIUM 3\n- الصوت المشترك 5.1 مجموعة فرعية باللغة الإنجليزية\n- مجموعة آرتي المتحيزة (مجموعة فرعية من الصوت العام)\n- CallHome وSwitchboard Corpora\n- وول ستريت جورنال\n- مقابلات كورال\n\nمجموعات البيانات الإنجليزية الأخرى:\n- تسجيلات CHiME-6 بكلتا الأذنين\n- AMI-IHM و AMI-SDM1\n- Rev16 (مجموعة فرعية مكونة من 16 ملفًا من معيار نسخ البودكاست الخاص بـ Rev.AI)\n\nمعايير متعددة اللغات:\n- LibriSpeech متعدد اللغات (MLS)\n- فوكس بوبولي\n\nتم أيضًا تقسيم أداء النموذج حسب اللغة باستخدام مقاييس WER/CER في مجموعات بيانات Common Voice 15 وFleurs. يمكن العثور على نتائج تفصيلية إضافية في الملحق د.1، د.2، ود.4 من الورقة المرجعية.\n\n[مطلوب مزيد من المعلومات] حول تفاصيل التدريب المحددة والمعلمات الفائقة المستخدمة في نموذج openai/whisper-base.en.",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base.en a été évalué sur les références et ensembles de données suivants :\n\nEnsembles de données abrégés uniquement en anglais :\n- Test LibriSpeech-nettoyage et test-autres divisions\n- Séparation des tests TED-LIUM 3\n- Sous-ensemble anglais Common Voice 5.1\n- Corpus Artie Biais (sous-ensemble de Common Voice)\n- Corpus CallHome et Switchboard\n-Corpus WSJ\n- Entretiens CORAAL\n\nAutres ensembles de données en anglais :\n- Enregistrements binauraux CHiME-6\n- Corpus AMI-IHM et AMI-SDM1\n- Rev16 (sous-ensemble de 16 fichiers du Podcast Transcription Benchmark de Rev.AI)\n\nBenchmarks multilingues :\n- LibriSpeech multilingue (MLS)\n- Vox populi\n\nLes performances du modèle ont également été ventilées par langue à l'aide des métriques WER/CER sur les ensembles de données Common Voice 15 et Fleurs. Des résultats détaillés supplémentaires peuvent être trouvés dans les annexes D.1, D.2 et D.4 de l'article référencé.\n\n[Plus d'informations nécessaires] sur les détails de formation spécifiques et les hyperparamètres utilisés pour le modèle openai/whisper-base.en.",
    "answer_Russian": "На основе предоставленных ссылок модель openai/whisper-base.en была оценена по следующим критериям и наборам данных:\n\nКраткие наборы данных только на английском языке:\n- LibriSpeech тест-чистка и тест-другие разделения\n- Тестовый сплит TED-LIUM 3\n- Английская подсистема Common Voice 5.1\n- Корпус предвзятости Арти (подмножество Common Voice)\n- Корпуса CallHome и Switchboard\n- Корпус WSJ\n- КОРААЛ интервью\n\nДругие наборы данных на английском языке:\n- Бинауральные записи CHiME-6\n- корпуса AMI-IHM и AMI-SDM1\n- Rev16 (подмножество из 16 файлов из теста транскрипции подкастов Rev.AI)\n\nМногоязычные тесты:\n- Многоязычная LibriSpeech (MLS)\n- ВоксПопули\n\nПроизводительность модели также была разбита по языкам с использованием показателей WER/CER в наборах данных Common Voice 15 и Fleurs. Дополнительные подробные результаты можно найти в Приложениях D.1, D.2 и D.4 к указанному документу.\n\n[Требуется дополнительная информация] о конкретных деталях обучения и гиперпараметрах, используемых для модели openai/whisper-base.en.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base.en モデルは次のベンチマークとデータセットで評価されました。\n\n英語のみの短い形式のデータセット:\n- LibriSpeech のテスト-クリーンとテスト-その他の分割\n- TED-LIUM 3 テスト分割\n- Common Voice 5.1 英語サブセット\n- アーティ バイアス コーパス (Common Voice のサブセット)\n- CallHome および Switchboard コーパス\n- WSJコーパス\n- CORALインタビュー\n\nその他の英語のデータセット:\n- CHiME-6バイノーラル録音\n- AMI-IHM および AMI-SDM1 コーパス\n- Rev16 (Rev.AI の Podcast Transcription Benchmark からの 16 ファイルのサブセット)\n\n多言語ベンチマーク:\n- 多言語LibriSpeech (MLS)\n- ヴォックスポプリ\n\nモデルのパフォーマンスは、Common Voice 15 および Fleurs データセットの WER/CER メトリクスを使用して言語別に分類されました。追加の詳細な結果は、参照論文の付録 D.1、D.2、および D.4 に記載されています。\n\nopenai/whisper-base.en モデルに使用される特定のトレーニングの詳細とハイパーパラメーターに関する [追加情報が必要]。"
  },
  {
    "question": "testing_factors",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper speech recognition model, here are some key characteristics that could influence the behavior and performance of the openai/whisper-base.en model:\n\nDomain and Context:\n- Performance degrades with increasing levels of background noise, especially compared to models trained primarily on clean datasets like LibriSpeech\n- Robustness likely stems in part from having a strong decoder that is an audio conditional language model\n- Many remaining errors in long-form transcription are stubborn and non-human/perceptual in nature, like getting stuck in loops or hallucinating unrelated text\n\nPopulation Subgroups:\n- Performance on a given language is strongly predicted by the amount of training data for that language \n- Current pre-training data is very English-heavy, so the model likely performs much worse on lower-resource languages that have less than 1000 hours of training data\n- [More Information Needed] on detailed performance breakdowns and disparities across different languages, accents, demographics\n\nEvaluation:\n- Using a text normalizer in evaluation to reduce penalties for innocuous, non-semantic differences in transcripts\n- Some evaluation datasets may not accurately reflect robustness if commercial models were trained on them\n- Performance varies widely by language - a breakdown is provided for the large models on Common Voice and Fleurs datasets\n- [More Information Needed] on disaggregated evaluations across key subgroups to uncover disparities\n\nIn summary, key factors that likely influence openai/whisper-base.en's behavior are the noise level and length of the input audio, the amount of training data for the particular language, and quirks of the evaluation datasets and metrics used. More disaggregated evaluations are needed to fully characterize performance disparities across subgroups.",
    "answer_Chinese": "根据提供的有关 Whisper 语音识别模型的参考资料，以下是一些可能影响 openai/whisper-base.en 模型的行为和性能的关键特征：\n\n领域和上下文：\n- 性能随着背景噪声水平的增加而降低，特别是与主要在 LibriSpeech 等干净数据集上训练的模型相比\n- 鲁棒性可能部分源于拥有强大的解码器，即音频条件语言模型\n- 长格式转录中的许多剩余错误本质上是顽固的和非人类/感知的，例如陷入循环或产生不相关文本的幻觉\n\n人口亚组：\n- 给定语言的性能是通过该语言的训练数据量来强烈预测的 \n- 当前的预训练数据以英语为主，因此该模型在训练数据少于 1000 小时的资源较低的语言上可能表现更差\n- [需要更多信息]关于不同语言、口音、人口统计数据之间的详细表现细分和差异\n\n评估：\n- 在评估中使用文本规范化器来减少对记录中无害、非语义差异的惩罚\n- 如果对商业模型进行训练，某些评估数据集可能无法准确反映鲁棒性\n- 性能因语言而异 - 为 Common Voice 和 Fleurs 数据集上的大型模型提供了细分\n- [需要更多信息]对关键分组进行分类评估以发现差异\n\n总之，可能影响 openai/whisper-base.en 行为的关键因素是输入音频的噪声水平和长度、特定语言的训练数据量以及所使用的评估数据集和指标的怪癖。需要进行更分类的评估，以充分描述子群体之间的绩效差异。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج التعرف على الكلام Whisper، إليك بعض الخصائص الأساسية التي يمكن أن تؤثر على سلوك وأداء نموذج openai/whisper-base.en:\n\nالمجال والسياق:\n- يتدهور الأداء مع زيادة مستويات الضوضاء في الخلفية، خاصة بالمقارنة مع النماذج التي تم تدريبها بشكل أساسي على مجموعات البيانات النظيفة مثل LibriSpeech\n- من المحتمل أن تنبع المتانة جزئيًا من وجود وحدة فك ترميز قوية تمثل نموذجًا للغة الشرطية الصوتية\n- العديد من الأخطاء المتبقية في النسخ الطويل هي أخطاء عنيدة وغير بشرية/إدراكية بطبيعتها، مثل التورط في الحلقات أو الهلوسة بنص غير ذي صلة\n\nالمجموعات الفرعية السكانية:\n- يتم التنبؤ بقوة بالأداء في لغة معينة من خلال كمية بيانات التدريب الخاصة بتلك اللغة \n- بيانات ما قبل التدريب الحالية كثيفة للغاية باللغة الإنجليزية، لذلك من المحتمل أن يكون أداء النموذج أسوأ بكثير في اللغات ذات الموارد المنخفضة التي تحتوي على أقل من 1000 ساعة من بيانات التدريب\n- [مطلوب مزيد من المعلومات] حول تفاصيل الأداء والتباينات بين اللغات واللهجات والتركيبة السكانية المختلفة\n\nتقييم:\n- استخدام أداة تسوية النص في التقييم لتقليل العقوبات المفروضة على الاختلافات غير الضارة وغير الدلالية في النصوص\n- قد لا تعكس بعض مجموعات بيانات التقييم المتانة بدقة إذا تم تدريب النماذج التجارية عليها\n- يختلف الأداء بشكل كبير حسب اللغة - يوجد تفصيل للنماذج الكبيرة في مجموعات بيانات Common Voice وFleurs\n- [يلزم المزيد من المعلومات] بشأن التقييمات المفصلة عبر المجموعات الفرعية الرئيسية للكشف عن التفاوتات\n\nباختصار، العوامل الرئيسية التي من المحتمل أن تؤثر على سلوك openai/whisper-base.en هي مستوى الضوضاء وطول الإدخال الصوتي، وكمية بيانات التدريب للغة معينة، والمراوغات في مجموعات بيانات التقييم والمقاييس المستخدمة. هناك حاجة إلى المزيد من التقييمات المفصلة لتوصيف الفوارق في الأداء بين المجموعات الفرعية بشكل كامل.",
    "answer_French": "Sur la base des références fournies sur le modèle de reconnaissance vocale Whisper, voici quelques caractéristiques clés qui pourraient influencer le comportement et les performances du modèle openai/whisper-base.en :\n\nDomaine et contexte :\n- Les performances se dégradent avec l'augmentation des niveaux de bruit de fond, en particulier par rapport aux modèles formés principalement sur des ensembles de données propres comme LibriSpeech\n- La robustesse provient probablement en partie du fait d'avoir un décodeur puissant qui est un modèle de langage conditionnel audio\n- De nombreuses erreurs restantes dans la transcription longue sont tenaces et de nature non humaine/perceptuelle, comme rester coincé dans des boucles ou avoir des hallucinations dans un texte sans rapport\n\nSous-groupes de population :\n- Les performances sur une langue donnée sont fortement prédites par la quantité de données de formation pour cette langue \n- Les données actuelles de pré-formation sont très lourdes en anglais, de sorte que le modèle fonctionne probablement bien moins bien sur les langues à faibles ressources qui ont moins de 1 000 heures de données de formation.\n- [Plus d'informations nécessaires] sur les répartitions détaillées des performances et les disparités entre différentes langues, accents et données démographiques\n\nÉvaluation:\n- Utilisation d'un normalisateur de texte en évaluation pour réduire les pénalités pour les différences inoffensives et non sémantiques dans les transcriptions\n- Certains ensembles de données d'évaluation peuvent ne pas refléter avec précision la robustesse si des modèles commerciaux ont été formés sur eux\n- Les performances varient considérablement selon la langue - une ventilation est fournie pour les grands modèles sur les ensembles de données Common Voice et Fleurs\n- [Plus d'informations nécessaires] sur les évaluations désagrégées entre les sous-groupes clés pour découvrir les disparités\n\nEn résumé, les facteurs clés susceptibles d'influencer le comportement d'openai/whisper-base.en sont le niveau de bruit et la durée de l'audio d'entrée, la quantité de données de formation pour la langue particulière et les bizarreries des ensembles de données d'évaluation et des métriques utilisées. Des évaluations plus désagrégées sont nécessaires pour caractériser pleinement les disparités de performance entre les sous-groupes.",
    "answer_Russian": "На основании предоставленных ссылок на модель распознавания речи Whisper, вот некоторые ключевые характеристики, которые могут повлиять на поведение и производительность модели openai/whisper-base.en:\n\nДомен и контекст:\n- Производительность снижается с увеличением уровня фонового шума, особенно по сравнению с моделями, обученными в основном на чистых наборах данных, таких как LibriSpeech.\n- Надежность, вероятно, частично обусловлена ​​наличием мощного декодера, который представляет собой модель условного языка аудио.\n- Многие оставшиеся ошибки в длинной транскрипции носят упорный и нечеловеческий/перцептивный характер, например, зацикливание или галлюцинации несвязанного текста.\n\nПодгруппы населения:\n- Производительность на конкретном языке во многом определяется объемом обучающих данных для этого языка. \n- Текущие данные предварительного обучения очень насыщены английским языком, поэтому модель, вероятно, работает намного хуже на языках с низкими ресурсами, которые имеют менее 1000 часов данных обучения.\n- [Требуется дополнительная информация] о подробной разбивке показателей и различиях в зависимости от языков, акцентов и демографических групп.\n\nОценка:\n- Использование нормализатора текста при оценке для уменьшения штрафов за безобидные, несемантические различия в транскриптах.\n- Некоторые наборы оценочных данных могут неточно отражать надежность, если на них обучались коммерческие модели.\n- Производительность сильно различается в зависимости от языка: для больших моделей предоставляется разбивка в наборах данных Common Voice и Fleurs.\n- [Требуется дополнительная информация] о дезагрегированных оценках по ключевым подгруппам для выявления различий\n\nТаким образом, ключевыми факторами, которые, вероятно, влияют на поведение openai/whisper-base.en, являются уровень шума и длина входного аудио, объем обучающих данных для конкретного языка, а также особенности используемых наборов оценочных данных и показателей. Для полной характеристики различий в производительности между подгруппами необходимы более дезагрегированные оценки.",
    "answer_Japanese": "Whisper 音声認識モデルに関して提供されている参考資料に基づいて、openai/whisper-base.en モデルの動作とパフォーマンスに影響を与える可能性のある主な特性をいくつか示します。\n\nドメインとコンテキスト:\n- 特に LibriSpeech のようなクリーンなデータセットで主にトレーニングされたモデルと比較して、バックグラウンド ノイズのレベルが増加するとパフォーマンスが低下します。\n- 堅牢性の一部は、オーディオ条件付き言語モデルである強力なデコーダを備えていることに起因すると考えられます。\n- 長い形式の文字起こしに残っているエラーの多くは、ループにはまり込んだり、無関係なテキストが幻聴されたりするなど、頑固で人間的/知覚的ではない性質のものです。\n\n人口サブグループ:\n- 特定の言語のパフォーマンスは、その言語のトレーニング データの量によって強く予測されます。 \n- 現在の事前トレーニング データは英語が非常に多いため、トレーニング データが 1000 時間未満の低リソース言語ではモデルのパフォーマンスが大幅に低下する可能性があります。\n- さまざまな言語、アクセント、人口統計間のパフォーマンスの詳細な内訳と格差に関する [さらなる情報が必要]\n\n評価：\n- 評価時にテキスト ノーマライザーを使用して、トランスクリプトにおける無害で意味論的な差異に対するペナルティを軽減します。\n- 一部の評価データセットで商用モデルがトレーニングされた場合、堅牢性が正確に反映されない可能性があります。\n- パフォーマンスは言語によって大きく異なります - Common Voice および Fleurs データセットの大規模モデルの内訳が提供されています\n- 格差を明らかにするための主要なサブグループにわたる細分化された評価に関する [さらなる情報が必要]\n\n要約すると、openai/whisper-base.en の動作に影響を与える可能性が高い主な要因は、入力音声のノイズ レベルと長さ、特定の言語のトレーニング データの量、使用される評価データセットとメトリクスの癖です。サブグループ間のパフォーマンスの差異を完全に特徴付けるには、より細分化された評価が必要です。"
  },
  {
    "question": "testing_metrics",
    "chain": "generation",
    "answer": "Based on the references provided, the key metrics and considerations for evaluating the openai/whisper-base.en model are:\n\n- Word Error Rate (WER) is the primary metric typically used in speech recognition research to compare systems. However, WER has limitations as it penalizes all differences between model output and reference transcripts, including innocuous formatting and style differences. This is particularly challenging for zero-shot models like Whisper that don't observe dataset-specific transcript formats. (References 1-2)\n\n- To address WER limitations, the team developed a text normalizer to standardize transcripts before WER calculation and minimize penalization of non-semantic differences. This normalizer was iteratively refined through manual inspection. The code for the normalizer is being released to enable fair comparisons. (References 2-3, 5) \n\n- Character Error Rate (CER) is used as an additional metric for some languages.\n\n- To assess room for improvement, human performance levels were estimated by obtaining professional transcripts for a subset of the Kincaid46 dataset. Distributions of WER from Whisper, commercial services, and the best open-source model were compared. (References 10-11)\n\nIn summary, the primary metrics are WER and CER, with text normalization applied to transcripts to enable fairer comparisons in light of Whisper's zero-shot nature. Human performance levels help indicate headroom for further model improvements. [More Information Needed] on the specific WER/CER results for openai/whisper-base.en.",
    "answer_Chinese": "根据提供的参考资料，评估 openai/whisper-base.en 模型的关键指标和注意事项是：\n\n- 字错误率 (WER) 是语音识别研究中通常用于比较系统的主要指标。然而，WER 也有局限性，因为它会惩罚模型输出和参考记录之间的所有差异，包括无害的格式和风格差异。对于像 Whisper 这样不观察数据集特定转录本格式的零样本模型来说，这尤其具有挑战性。 （参考文献1-2）\n\n- 为了解决 WER 限制，团队开发了一个文本规范化器，在 WER 计算之前对成绩单进行标准化，并最大限度地减少非语义差异的惩罚。该标准化器是通过手动检查迭代完善的。正在发布规范化器的代码以实现公平比较。 （参考文献2-3、5） \n\n- 字符错误率 (CER) 用作某些语言的附加指标。\n\n- 为了评估改进的空间，通过获取 Kincaid46 数据集子集的专业成绩单来估计人类表现水平。比较了 Whisper、商业服务和最佳开源模型的 WER 分布。 （参考文献10-11）\n\n总之，主要指标是 WER 和 CER，文本标准化应用于转录本，以便根据 Whisper 的零样本性质进行更公平的比较。人类表现水平有助于表明进一步改进模型的空间。 [需要更多信息] 关于 openai/whisper-base.en 的具体 WER/CER 结果。",
    "answer_Arabic": "بناءً على المراجع المقدمة، فإن المقاييس والاعتبارات الرئيسية لتقييم نموذج openai/whisper-base.en هي:\n\n- معدل خطأ الكلمات (WER) هو المقياس الأساسي المستخدم عادةً في أبحاث التعرف على الكلام لمقارنة الأنظمة. ومع ذلك، فإن WER له قيود لأنه يعاقب جميع الاختلافات بين مخرجات النموذج والنصوص المرجعية، بما في ذلك التنسيق غير الضار والاختلافات في الأسلوب. يمثل هذا تحديًا خاصًا بالنسبة للنماذج ذات اللقطة الصفرية مثل Whisper التي لا تراعي تنسيقات النصوص الخاصة بمجموعة البيانات. (المراجع 1-2)\n\n- لمعالجة قيود WER، قام الفريق بتطوير أداة تسوية النص لتوحيد النصوص قبل حساب WER وتقليل معاقبة الاختلافات غير الدلالية. تم تحسين هذا المُطبيع بشكل متكرر من خلال الفحص اليدوي. يتم إصدار كود أداة التسوية لتمكين إجراء مقارنات عادلة. (المراجع 2-3، 5) \n\n- يتم استخدام معدل خطأ الأحرف (CER) كمقياس إضافي لبعض اللغات.\n\n- لتقييم مجال التحسين، تم تقدير مستويات الأداء البشري من خلال الحصول على النصوص المهنية لمجموعة فرعية من مجموعة بيانات Kincaid46. تمت مقارنة توزيعات WER من Whisper والخدمات التجارية وأفضل نموذج مفتوح المصدر. (المراجع 10-11)\n\nباختصار، المقاييس الأساسية هي WER وCER، مع تطبيق تطبيع النص على النصوص لتمكين إجراء مقارنات أكثر عدالة في ضوء طبيعة Whisper الصفرية. تساعد مستويات الأداء البشري في الإشارة إلى وجود مجال لمزيد من التحسينات في النموذج. [مطلوب مزيد من المعلومات] حول نتائج WER/CER المحددة لـ openai/whisper-base.en.",
    "answer_French": "Sur la base des références fournies, les indicateurs et considérations clés pour évaluer le modèle openai/whisper-base.en sont :\n\n- Le taux d'erreur sur les mots (WER) est la principale mesure généralement utilisée dans la recherche sur la reconnaissance vocale pour comparer les systèmes. Cependant, WER a des limites car il pénalise toutes les différences entre la sortie du modèle et les transcriptions de référence, y compris les différences inoffensives de formatage et de style. Cela est particulièrement difficile pour les modèles sans tir comme Whisper qui n'observent pas les formats de transcription spécifiques aux ensembles de données. (Références 1-2)\n\n- Pour remédier aux limites du WER, l'équipe a développé un normalisateur de texte pour normaliser les transcriptions avant le calcul du WER et minimiser la pénalisation des différences non sémantiques. Ce normalisateur a été affiné de manière itérative par inspection manuelle. Le code du normalisateur est en cours de publication pour permettre des comparaisons équitables. (Références 2-3, 5) \n\n- Le taux d'erreur sur les caractères (CER) est utilisé comme mesure supplémentaire pour certaines langues.\n\n- Pour évaluer les possibilités d'amélioration, les niveaux de performance humaine ont été estimés en obtenant des relevés de notes professionnels pour un sous-ensemble de l'ensemble de données Kincaid46. Les distributions de WER de Whisper, des services commerciaux et du meilleur modèle open source ont été comparées. (Références 10-11)\n\nEn résumé, les principales mesures sont WER et CER, avec une normalisation du texte appliquée aux transcriptions pour permettre des comparaisons plus justes à la lumière de la nature zéro-shot de Whisper. Les niveaux de performance humaine aident à indiquer la marge pour de nouvelles améliorations du modèle. [Plus d'informations nécessaires] sur les résultats WER/CER spécifiques pour openai/whisper-base.en.",
    "answer_Russian": "На основании предоставленных ссылок ключевыми показателями и соображениями для оценки модели openai/whisper-base.en являются:\n\n- Коэффициент ошибок в словах (WER) — это основной показатель, который обычно используется в исследованиях по распознаванию речи для сравнения систем. Однако WER имеет ограничения, поскольку он игнорирует все различия между выходными данными модели и ссылочными расшифровками, включая безобидные различия в форматировании и стиле. Это особенно сложно для моделей с нулевым выстрелом, таких как Whisper, которые не соблюдают форматы расшифровок, специфичные для набора данных. (Ссылки 1-2)\n\n- Чтобы устранить ограничения WER, команда разработала нормализатор текста, который стандартизирует транскрипты перед расчетом WER и минимизирует штрафы за несемантические различия. Этот нормализатор итеративно уточнялся посредством ручной проверки. Код нормализатора выпускается для обеспечения честного сравнения. (Ссылки 2-3, 5) \n\n- Коэффициент ошибок символов (CER) используется в качестве дополнительного показателя для некоторых языков.\n\n- Чтобы оценить возможности для улучшения, уровни производительности человека оценивались путем получения профессиональных стенограмм для подмножества набора данных Kincaid46. Сравнивались дистрибутивы WER от Whisper, коммерческих сервисов и лучшая модель с открытым исходным кодом. (Ссылки 10-11)\n\nТаким образом, основными метриками являются WER и CER, при этом к расшифровкам применяется нормализация текста, чтобы обеспечить более справедливое сравнение в свете нулевой природы Whisper. Уровни производительности человека помогают указать запас для дальнейших улучшений модели. [Требуется дополнительная информация] о конкретных результатах WER/CER для openai/whisper-base.en.",
    "answer_Japanese": "提供されている参考資料に基づいて、openai/whisper-base.en モデルを評価するための主要な指標と考慮事項は次のとおりです。\n\n- 単語誤り率 (WER) は、音声認識研究でシステムを比較するために通常使用される主要な指標です。ただし、WER には、無害な書式設定やスタイルの違いを含む、モデル出力と参照トランスクリプト間のすべての違いにペナルティが課されるため、制限があります。これは、データセット固有のトランスクリプト形式を観察しない Whisper のようなゼロショット モデルの場合、特に困難です。 （参考１～２）\n\n- WER の制限に対処するために、チームは、WER の計算前にトランスクリプトを標準化し、非セマンティックな差異によるペナルティを最小限に抑えるテキスト ノーマライザーを開発しました。このノーマライザーは、手動検査を通じて繰り返し改良されました。公平な比較を可能にするために、ノーマライザーのコードがリリースされています。 （参考２～３、５） \n\n- 文字エラー率 (CER) は、一部の言語の追加指標として使用されます。\n\n- 改善の余地を評価するために、Kincaid46 データセットのサブセットの専門的な記録を取得することにより、人間のパフォーマンス レベルを推定しました。 Whisper の WER のディストリビューション、商用サービス、および最良のオープンソース モデルを比較しました。 （参考10～11）\n\n要約すると、主な指標は WER と CER であり、Whisper のゼロショットの性質を考慮して、より公平な比較を可能にするためにトランスクリプトにテキスト正規化が適用されます。人間のパフォーマンス レベルは、モデルをさらに改善する余地があることを示すのに役立ちます。 openai/whisper-base.en の特定の WER/CER 結果に関する [詳細情報が必要]。"
  },
  {
    "question": "results",
    "chain": "generation",
    "answer": "Evaluation Results of openai/whisper-base.en:\n\nPerformance Metrics:\n- Word Error Rate (WER) on English speech recognition: The model achieves close to human-level performance on English speech recognition tasks, with diminishing returns on performance improvements as model size increases beyond the base size.\n- [More Information Needed] on exact WER values for openai/whisper-base.en.\n- Performance on multilingual speech recognition, speech translation, and language identification continues to improve with larger model sizes beyond the base model.\n- Compared to a supervised model with similar performance on LibriSpeech test-clean, the zero-shot Whisper base model achieves an average 55.2% relative error reduction when evaluated on other speech recognition datasets.\n\nEvaluation Datasets:\n- LibriSpeech\n- Common Voice 15\n- Fleurs\n- [More Information Needed] on additional evaluation datasets and results.\n\nFactors Impacting Performance:\n- Dataset size: Performance improves with increasing dataset size, but shows diminishing returns beyond the 680,000 hours used for training the Whisper models.\n- Language: Performance varies widely depending on the language. WER and CER breakdowns by language are available for some datasets in the paper.\n- [More Information Needed] on other factors impacting performance.\n\nComparisons:\n- Outperforms open-source models like NVIDIA STT Conformer-CTC Large.\n- [More Information Needed] on detailed comparisons to the 4 commercial ASR services mentioned.\n\nMethodology Notes:\n- Extensive text normalization is performed before WER calculation to minimize penalization of zero-shot models like Whisper.\n- Human performance estimates were obtained on a subset of the Kincaid46 dataset to assess room for improvement.\n\n[More Information Needed] on additional evaluation details for the openai/whisper-base.en model specifically. The references provide general insights about the Whisper model family, but more details on the base.en model's evaluation are needed to comprehensively answer the question.",
    "answer_Chinese": "openai/whisper-base.en的评估结果：\n\n性能指标：\n- 英语语音识别的词错误率 (WER)：该模型在英语语音识别任务上实现了接近人类水平的性能，但随着模型大小增加到超出基本大小，性能改进的回报递减。\n- [需要更多信息] 关于 openai/whisper-base.en 的确切 WER 值。\n- 随着模型尺寸超出基本模型，多语言语音识别、语音翻译和语言识别的性能不断提高。\n- 与在 LibriSpeech test-clean 上具有相似性能的监督模型相比，在其他语音识别数据集上进行评估时，零样本 Whisper 基本模型平均相对误差降低了 55.2%。\n\n评估数据集：\n- LibriSpeech\n- 普通语音 15\n- 芙蓉\n- [需要更多信息]有关其他评估数据集和结果。\n\n影响性能的因素：\n- 数据集大小：性能随着数据集大小的增加而提高，但在用于训练 Whisper 模型的 680,000 小时之后显示出收益递减。\n- 语言：性能因语言而异。本文中的某些数据集提供了按语言划分的 WER 和 CER 细分。\n- [需要更多信息]有关影响性能的其他因素。\n\n比较：\n- 优于 NVIDIA STT Conformer-CTC Large 等开源模型。\n- [需要更多信息] 与提到的 4 种商业 ASR 服务的详细比较。\n\n方法论注释：\n- 在 WER 计算之前执行广泛的文本标准化，以最大限度地减少像 Whisper 这样的零样本模型的惩罚。\n- 在 Kincaid46 数据集的子集上获得人类表现估计，以评估改进的空间。\n\n[需要更多信息] 特别是 openai/whisper-base.en 模型的其他评估细节。这些参考资料提供了有关 Whisper 模型系列的一般见解，但需要有关 base.en 模型评估的更多详细信息才能全面回答该问题。",
    "answer_Arabic": "نتائج تقييم openai/whisper-base.en:\n\nمقاييس الأداء:\n- معدل خطأ الكلمات (WER) في التعرف على الكلام باللغة الإنجليزية: يحقق النموذج أداءً قريبًا من المستوى البشري في مهام التعرف على الكلام باللغة الإنجليزية، مع تناقص العوائد على تحسينات الأداء مع زيادة حجم النموذج إلى ما هو أبعد من الحجم الأساسي.\n- [يلزم مزيد من المعلومات] حول قيم WER الدقيقة لـ openai/whisper-base.en.\n- يستمر الأداء في التعرف على الكلام متعدد اللغات وترجمة الكلام وتحديد اللغة في التحسن مع أحجام نماذج أكبر تتجاوز النموذج الأساسي.\n- بالمقارنة مع نموذج خاضع للإشراف مع أداء مماثل في اختبار LibriSpeech النظيف، يحقق نموذج قاعدة Whisper بدون إطلاق النار متوسط ​​تقليل الخطأ النسبي بنسبة 55.2% عند تقييمه على مجموعات بيانات أخرى للتعرف على الكلام.\n\nمجموعات بيانات التقييم:\n- خطاب ليبري\n- الصوت المشترك 15\n- زهور\n- [يلزم مزيد من المعلومات] حول مجموعات بيانات التقييم الإضافية ونتائجها.\n\nالعوامل المؤثرة على الأداء:\n- حجم مجموعة البيانات: يتحسن الأداء مع زيادة حجم مجموعة البيانات، ولكنه يظهر عوائد متناقصة تتجاوز 680.000 ساعة مستخدمة لتدريب نماذج Whisper.\n- اللغة: يختلف الأداء بشكل كبير حسب اللغة. تتوفر تفاصيل WER وCER حسب اللغة لبعض مجموعات البيانات في الورقة.\n- [مطلوب مزيد من المعلومات] حول العوامل الأخرى التي تؤثر على الأداء.\n\nمقارنات:\n- يتفوق على النماذج مفتوحة المصدر مثل NVIDIA STT Conformer-CTC Large.\n- [يلزم مزيد من المعلومات] حول المقارنات التفصيلية مع خدمات ASR التجارية الأربع المذكورة.\n\nملاحظات المنهجية:\n- يتم إجراء تطبيع واسع النطاق للنص قبل حساب WER لتقليل معاقبة النماذج ذات اللقطة الصفرية مثل Whisper.\n- تم الحصول على تقديرات الأداء البشري على مجموعة فرعية من مجموعة بيانات Kincaid46 لتقييم مجال التحسين.\n\n[مطلوب مزيد من المعلومات] حول تفاصيل التقييم الإضافية لنموذج openai/whisper-base.en على وجه التحديد. توفر المراجع رؤى عامة حول عائلة نماذج Whisper، ولكن هناك حاجة إلى مزيد من التفاصيل حول تقييم نموذج base.en للإجابة بشكل شامل على السؤال.",
    "answer_French": "Résultats de l'évaluation de openai/whisper-base.en :\n\nIndicateurs de performance:\n- Taux d'erreur de mots (WER) sur la reconnaissance vocale en anglais : le modèle atteint des performances proches du niveau humain sur les tâches de reconnaissance vocale en anglais, avec des rendements décroissants sur les améliorations de performances à mesure que la taille du modèle augmente au-delà de la taille de base.\n- [Plus d'informations nécessaires] sur les valeurs exactes WER pour openai/whisper-base.en.\n- Les performances en matière de reconnaissance vocale multilingue, de traduction vocale et d'identification des langues continuent de s'améliorer avec des tailles de modèle plus grandes au-delà du modèle de base.\n- Comparé à un modèle supervisé avec des performances similaires sur le test-clean LibriSpeech, le modèle de base Zero-shot Whisper atteint une réduction d'erreur relative moyenne de 55,2 % lorsqu'il est évalué sur d'autres ensembles de données de reconnaissance vocale.\n\nEnsembles de données d'évaluation :\n- LibriDiscours\n- Voix commune 15\n-Fleurs\n- [Plus d'informations nécessaires] sur les ensembles de données et les résultats d'évaluation supplémentaires.\n\nFacteurs ayant un impact sur les performances :\n- Taille de l'ensemble de données : les performances s'améliorent avec l'augmentation de la taille de l'ensemble de données, mais affichent des rendements décroissants au-delà des 680 000 heures utilisées pour entraîner les modèles Whisper.\n- Langue : les performances varient considérablement en fonction de la langue. Les répartitions WER et CER par langue sont disponibles pour certains ensembles de données dans le document.\n- [Plus d'informations nécessaires] sur d'autres facteurs ayant un impact sur les performances.\n\nComparaisons :\n- Surpasse les modèles open source comme NVIDIA STT Conformer-CTC Large.\n- [Plus d'informations nécessaires] sur des comparaisons détaillées avec les 4 services ASR commerciaux mentionnés.\n\nNotes méthodologiques :\n- Une normalisation approfondie du texte est effectuée avant le calcul du WER pour minimiser la pénalisation des modèles sans tir comme Whisper.\n- Des estimations de la performance humaine ont été obtenues sur un sous-ensemble de l'ensemble de données Kincaid46 pour évaluer les possibilités d'amélioration.\n\n[Plus d'informations nécessaires] sur les détails d'évaluation supplémentaires pour le modèle openai/whisper-base.en en particulier. Les références fournissent des informations générales sur la famille de modèles Whisper, mais plus de détails sur l'évaluation du modèle base.en sont nécessaires pour répondre de manière exhaustive à la question.",
    "answer_Russian": "Результаты оценки openai/whisper-base.en:\n\nПоказатели эффективности:\n- Коэффициент ошибок в словах (WER) при распознавании английской речи: модель достигает производительности, близкой к человеческому уровню при выполнении задач по распознаванию английской речи, с уменьшением отдачи от улучшения производительности по мере увеличения размера модели за пределы базового размера.\n- [Требуется дополнительная информация] о точных значениях WER для openai/whisper-base.en.\n- Производительность многоязычного распознавания речи, перевода речи и идентификации языка продолжает улучшаться с увеличением размера модели, превышающей базовую.\n- По сравнению с контролируемой моделью с аналогичной производительностью в тесте LibriSpeech, базовая модель Whisper с нулевым выстрелом достигает среднего снижения относительной ошибки на 55,2% при оценке на других наборах данных распознавания речи.\n\nНаборы оценочных данных:\n- ЛибриРечь\n- Общий голос 15\n- Флер\n- [Требуется дополнительная информация] о дополнительных наборах данных и результатах оценки.\n\nФакторы, влияющие на производительность:\n- Размер набора данных: производительность улучшается с увеличением размера набора данных, но показывает уменьшающуюся отдачу после 680 000 часов, использованных для обучения моделей Whisper.\n- Язык: производительность сильно различается в зависимости от языка. Разбивка WER и CER по языкам доступна для некоторых наборов данных в документе.\n- [Требуется дополнительная информация] о других факторах, влияющих на производительность.\n\nСравнения:\n- Превосходит модели с открытым исходным кодом, такие как NVIDIA STT Conformer-CTC Large.\n- [Требуется дополнительная информация] для подробного сравнения с 4 упомянутыми коммерческими службами ASR.\n\nМетодические примечания:\n— Перед расчетом WER выполняется обширная нормализация текста, чтобы минимизировать штрафы для моделей с нулевым выстрелом, таких как Whisper.\n- Оценки производительности человека были получены на основе набора данных Kincaid46 для оценки возможностей для улучшения.\n\n[Требуется дополнительная информация] о дополнительных деталях оценки, в частности, для модели openai/whisper-base.en. Ссылки дают общее представление о семействе моделей Whisper, но для всестороннего ответа на вопрос необходимы более подробные сведения об оценке модели base.en.",
    "answer_Japanese": "openai/whisper-base.enの評価結果：\n\nパフォーマンス指標:\n- 英語音声認識の単語誤り率 (WER): このモデルは、英語音声認識タスクで人間レベルに近いパフォーマンスを達成しますが、モデル サイズが基本サイズを超えて増加すると、パフォーマンス向上の効果は減少します。\n- openai/whisper-base.en の正確な WER 値に関する [追加情報が必要]。\n- 多言語音声認識、音声翻訳、および言語識別のパフォーマンスは、基本モデルを超えてモデル サイズが大きくなるにつれて向上し続けています。\n- LibriSpeech テストクリーンで同様のパフォーマンスを持つ教師ありモデルと比較すると、ゼロショット ウィスパー基本モデルは、他の音声認識データセットで評価した場合、平均 55.2% の相対エラー削減を達成します。\n\n評価データセット:\n- リブスピーチ\n- コモンボイス15\n- フルール\n- 追加の評価データセットと結果に関する [さらなる情報が必要]。\n\nパフォーマンスに影響を与える要因:\n- データセット サイズ: データセット サイズが増加するとパフォーマンスは向上しますが、Wh​​isper モデルのトレーニングに使用される 680,000 時間を超えると効果が減少します。\n- 言語: パフォーマンスは言語によって大きく異なります。この論文の一部のデータセットについては、言語ごとの WER および CER の内訳が利用可能です。\n- パフォーマンスに影響を与えるその他の要因に関する [さらなる情報が必要]。\n\n比較:\n- NVIDIA STT Conformer-CTC Large などのオープンソース モデルよりも優れたパフォーマンスを発揮します。\n- 前述の 4 つの商用 ASR サービスとの詳細な比較に関する [さらなる情報が必要]。\n\n方法論のメモ:\n- Whisper のようなゼロショット モデルのペナルティを最小限に抑えるために、WER 計算の前に広範なテキスト正規化が実行されます。\n- 改善の余地を評価するために、Kincaid46 データセットのサブセットに対して人間のパフォーマンスの推定値が取得されました。\n\n特に openai/whisper-base.en モデルの追加評価の詳細に関する [詳細情報が必要]。参考資料は Whisper モデル ファミリに関する一般的な洞察を提供しますが、質問に包括的に答えるには、base.en モデルの評価に関する詳細が必要です。"
  },
  {
    "question": "results_summary",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here is a summary of the evaluation results for the openai/whisper-base.en model:\n\n- Performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification tasks, with some diminishing returns for English speech recognition possibly due to approaching human-level performance.\n\n- Whisper models are evaluated in a zero-shot manner on a wide set of existing speech processing datasets to test generalization across domains, tasks, and languages. \n\n- Whisper's performance varies widely by language, with performance on a language being well predicted by the amount of training data available for that language.\n\n- On low-data multilingual speech recognition benchmarks like Multilingual LibriSpeech (MLS) and VoxPopuli, Whisper's performance is reported but these benchmarks provide limited coverage of Whisper's multilingual capabilities which include training data for 75 languages.\n\n- Compared to other open-source and commercial ASR models, Whisper performs better on most datasets, especially those with uncommon words. Some commercial models may have been trained on the evaluation datasets though.\n\n[More Information Needed] on the specific performance metrics for the openai/whisper-base.en model size. The references discuss Whisper models in general but do not provide results broken down by the specific base.en model.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，以下是 openai/whisper-base.en 模型的评估结果摘要：\n\n- 多语言语音识别、语音翻译和语言识别任务的性能随着模型大小的增加而不断提高，而英语语音识别的回报可能会因接近人类水平的性能而递减。\n\n- 在大量现有语音处理数据集上以零样本方式评估 Whisper 模型，以测试跨领域、任务和语言的泛化能力。 \n\n- Whisper 的性能因语言而异，通过该语言可用的训练数据量可以很好地预测该语言的性能。\n\n- 在多语言 LibriSpeech (MLS) 和 VoxPopuli 等低数据多语言语音识别基准上，报告了 Whisper 的性能，但这些基准仅提供了 Whisper 多语言功能的有限覆盖范围，其中包括 75 种语言的训练数据。\n\n- 与其他开源和商业 ASR 模型相比，Whisper 在大多数数据集上表现更好，尤其是那些不常见单词的数据集。不过，一些商业模型可能已经在评估数据集上进行了训练。\n\n[需要更多信息] 有关 openai/whisper-base.en 模型大小的具体性能指标。这些参考文献总体上讨论了 Whisper 模型，但没有提供按特定 base.en 模型细分的结果。",
    "answer_Arabic": "بناءً على المراجع المتوفرة حول نموذج Whisper، فيما يلي ملخص لنتائج التقييم لنموذج openai/whisper-base.en:\n\n- يستمر الأداء في الزيادة مع حجم النموذج عبر التعرف على الكلام متعدد اللغات، وترجمة الكلام، ومهام تحديد اللغة، مع بعض العائدات المتناقصة للتعرف على الكلام باللغة الإنجليزية ربما بسبب اقتراب الأداء من المستوى البشري.\n\n- يتم تقييم نماذج Whisper بطريقة صفرية على مجموعة واسعة من مجموعات بيانات معالجة الكلام الحالية لاختبار التعميم عبر المجالات والمهام واللغات. \n\n- يختلف أداء Whisper بشكل كبير حسب اللغة، حيث يتم التنبؤ بالأداء على اللغة بشكل جيد من خلال كمية بيانات التدريب المتاحة لتلك اللغة.\n\n- في معايير التعرف على الكلام متعدد اللغات ذات البيانات المنخفضة مثل Multilingual LibriSpeech (MLS) وVoxPpuli، يتم الإبلاغ عن أداء Whisper ولكن هذه المعايير توفر تغطية محدودة لقدرات Whisper متعددة اللغات والتي تشمل بيانات التدريب لـ 75 لغة.\n\n- بالمقارنة مع نماذج ASR مفتوحة المصدر والتجارية الأخرى، فإن أداء Whisper أفضل في معظم مجموعات البيانات، خاصة تلك التي تحتوي على كلمات غير شائعة. ربما تم تدريب بعض النماذج التجارية على مجموعات بيانات التقييم.\n\n[مطلوب مزيد من المعلومات] حول مقاييس الأداء المحددة لحجم نموذج openai/whisper-base.en. تناقش المراجع نماذج Whisper بشكل عام ولكنها لا تقدم نتائج مقسمة حسب نموذج base.en المحدد.",
    "answer_French": "Sur la base des références fournies sur le modèle Whisper, voici un résumé des résultats de l'évaluation du modèle openai/whisper-base.en :\n\n- Les performances continuent d'augmenter avec la taille du modèle dans les tâches de reconnaissance vocale multilingue, de traduction vocale et d'identification de la langue, avec des rendements décroissants pour la reconnaissance vocale en anglais, probablement en raison d'une performance proche du niveau humain.\n\n- Les modèles Whisper sont évalués de manière zéro sur un large ensemble de données de traitement de la parole existantes pour tester la généralisation à travers les domaines, les tâches et les langues. \n\n- Les performances de Whisper varient considérablement selon la langue, les performances sur une langue étant bien prédites par la quantité de données d'entraînement disponibles pour cette langue.\n\n- Sur les tests de reconnaissance vocale multilingue à faible volume de données tels que Multilingual LibriSpeech (MLS) et VoxPopuli, les performances de Whisper sont rapportées, mais ces tests fournissent une couverture limitée des capacités multilingues de Whisper qui incluent des données de formation pour 75 langues.\n\n- Comparé à d'autres modèles ASR open source et commerciaux, Whisper fonctionne mieux sur la plupart des ensembles de données, en particulier ceux contenant des mots peu courants. Certains modèles commerciaux peuvent cependant avoir été formés sur les ensembles de données d'évaluation.\n\n[Plus d'informations nécessaires] sur les mesures de performances spécifiques pour la taille du modèle openai/whisper-base.en. Les références traitent des modèles Whisper en général mais ne fournissent pas de résultats ventilés par modèle base.en spécifique.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель Whisper, ниже приводится сводка результатов оценки модели openai/whisper-base.en:\n\n- Производительность продолжает расти с увеличением размера модели в задачах многоязычного распознавания речи, перевода речи и идентификации языка, при этом отдача от распознавания английской речи снижается, возможно, из-за приближения производительности к человеческому уровню.\n\n- Модели шепота оцениваются с нуля на широком наборе существующих наборов данных обработки речи для проверки обобщения по областям, задачам и языкам. \n\n- Производительность Whisper сильно зависит от языка, при этом производительность на языке хорошо прогнозируется по объему обучающих данных, доступных для этого языка.\n\n- В тестах многоязычного распознавания речи с низким объемом данных, таких как Multilingual LibriSpeech (MLS) и VoxPopuli, сообщается о производительности Whisper, но эти тесты обеспечивают ограниченный охват многоязычных возможностей Whisper, которые включают данные обучения для 75 языков.\n\n- По сравнению с другими моделями ASR с открытым исходным кодом и коммерческими моделями ASR, Whisper лучше работает с большинством наборов данных, особенно с редко встречающимися словами. Однако некоторые коммерческие модели могли быть обучены на наборах оценочных данных.\n\n[Требуется дополнительная информация] о конкретных показателях производительности для размера модели openai/whisper-base.en. В ссылках обсуждаются модели Whisper в целом, но не приводятся результаты с разбивкой по конкретной модели base.en.",
    "answer_Japanese": "Whisper モデルに関して提供されている参考資料に基づいて、openai/whisper-base.en モデルの評価結果の概要を以下に示します。\n\n- 多言語音声認識、音声翻訳、および言語識別タスク全体でモデルのサイズが増加するにつれてパフォーマンスは向上し続けていますが、おそらく人間レベルのパフォーマンスに近づいているため、英語音声認識の利益はいくらか減少しています。\n\n- ウィスパー モデルは、ドメイン、タスク、言語にわたる一般化をテストするために、既存の幅広い音声処理データセットに対してゼロショット方式で評価されます。 \n\n- Whisper のパフォーマンスは言語によって大きく異なり、言語でのパフォーマンスは、その言語で利用可能なトレーニング データの量によって十分に予測されます。\n\n- Multilingual LibriSpeech (MLS) や VoxPopuli などの低データの多言語音声認識ベンチマークでは、Whisper のパフォーマンスが報告されていますが、これらのベンチマークは、75 言語のトレーニング データを含む Whisper の多言語機能を限定的にカバーしています。\n\n- 他のオープンソースおよび商用 ASR モデルと比較して、Whisper はほとんどのデータセット、特に珍しい単語を含むデータセットで優れたパフォーマンスを発揮します。ただし、一部の商用モデルは評価データセットでトレーニングされている可能性があります。\n\nopenai/whisper-base.en モデル サイズの特定のパフォーマンス メトリクスに関する [詳細情報が必要]。参考文献では Whisper モデル全般について説明していますが、特定の Base.en モデルごとに分類された結果は提供されていません。"
  },
  {
    "question": "model_examination",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here is what I can say about the explainability and interpretability of the openai/whisper-base.en model that our team has worked on:\n\nThe Whisper model's robustness seems to be partially due to its strong decoder, which is an audio conditional language model. However, it's currently unclear to what degree the benefits come from training the encoder, decoder, or both. Studying this further by ablating design components or using Whisper's decoder with other encoders could provide more interpretability insights. [More Information Needed]\n\nWhisper still makes some stubborn errors in long-form transcription that seem decidedly non-human/perceptual, like getting stuck in loops, skipping words, or hallucinating unrelated transcripts. Fine-tuning on high-quality datasets or using reinforcement learning to optimize decoding could help explain and reduce these failure modes. [More Information Needed]\n\nWhisper's performance varies significantly by language, with much lower accuracy for lower-resource languages. This is likely due to the English-centric pre-training data. Increasing training data for other languages is a clear path to improving explainability of the model's language-specific capabilities. [More Information Needed]\n\nIn summary, while the Whisper model shows impressive robustness and performance, there are still open questions around the relative contributions of different model components, stubborn error modes, and language-specific capabilities that require further research to fully explain and interpret the model's behaviors. Our team will continue to study these areas to improve the model's interpretability.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，我可以说一下我们团队所研究的 openai/whisper-base.en 模型的可解释性和可解释性：\n\nWhisper 模型的稳健性似乎部分归功于其强大的解码器，它是一种音频条件语言模型。然而，目前尚不清楚训练编码器、解码器或两者的好处有多大。通过消除设计组件或将 Whisper 的解码器与其他编码器结合使用来进一步研究这一点，可以提供更多可解释性的见解。 [需要更多信息]\n\nWhisper 在长格式转录中仍然会犯一些顽固的错误，这些错误看起来显然是非人类/感知的，比如陷入循环、跳过单词或产生不相关转录的幻觉。对高质量数据集进行微调或使用强化学习来优化解码可以帮助解释和减少这些故障模式。 [需要更多信息]\n\nWhisper 的性能因语言而异，对于资源较少的语言，准确度要低得多。这可能是由于以英语为中心的预训练数据。增加其他语言的训练数据是提高模型特定语言功能可解释性的一条明确途径。 [需要更多信息]\n\n总之，虽然 Whisper 模型显示出令人印象深刻的鲁棒性和性能，但围绕不同模型组件的相对贡献、顽固错误模式和特定于语言的功能仍然存在悬而未决的问题，需要进一步研究以充分解释和解释模型的行为。我们的团队将继续研究这些领域，以提高模型的可解释性。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج Whisper، إليك ما يمكنني قوله حول إمكانية الشرح والتفسير لنموذج openai/whisper-base.en الذي عمل عليه فريقنا:\n\nيبدو أن قوة نموذج Whisper ترجع جزئيًا إلى وحدة فك التشفير القوية، وهو نموذج لغة مشروطة صوتية. ومع ذلك، ليس من الواضح حاليًا إلى أي مدى تأتي الفوائد من تدريب جهاز التشفير أو وحدة فك التشفير أو كليهما. إن دراسة هذا الأمر بشكل أكبر عن طريق تقليص مكونات التصميم أو استخدام وحدة فك تشفير Whisper مع أجهزة تشفير أخرى يمكن أن توفر المزيد من الأفكار القابلة للتفسير. [مطلوب مزيد من المعلومات]\n\nلا يزال Whisper يرتكب بعض الأخطاء العنيدة في النسخ الطويل الذي يبدو بالتأكيد غير بشري/إدراكي، مثل التورط في الحلقات، أو تخطي الكلمات، أو هلوسة النصوص غير ذات الصلة. يمكن أن يساعد الضبط الدقيق لمجموعات البيانات عالية الجودة أو استخدام التعلم المعزز لتحسين فك التشفير في تفسير أوضاع الفشل هذه والحد منها. [مطلوب مزيد من المعلومات]\n\nيختلف أداء Whisper بشكل كبير حسب اللغة، مع دقة أقل بكثير للغات ذات الموارد المنخفضة. من المحتمل أن يكون هذا بسبب بيانات ما قبل التدريب التي تركز على اللغة الإنجليزية. تعد زيادة بيانات التدريب للغات الأخرى طريقًا واضحًا لتحسين إمكانية شرح القدرات الخاصة باللغة الخاصة بالنموذج. [مطلوب مزيد من المعلومات]\n\nباختصار، في حين يُظهر نموذج Whisper قوة وأداء مثيرين للإعجاب، لا تزال هناك أسئلة مفتوحة حول المساهمات النسبية لمكونات النموذج المختلفة، وأنماط الخطأ العنيدة، والقدرات الخاصة باللغة التي تتطلب مزيدًا من البحث لشرح وتفسير سلوكيات النموذج بشكل كامل. سيواصل فريقنا دراسة هذه المجالات لتحسين إمكانية تفسير النموذج.",
    "answer_French": "Sur la base des références fournies sur le modèle Whisper, voici ce que je peux dire sur l'explicabilité et l'interprétabilité du modèle openai/whisper-base.en sur lequel notre équipe a travaillé :\n\nLa robustesse du modèle Whisper semble être en partie due à son décodeur puissant, qui est un modèle de langage audio conditionnel. Cependant, il est actuellement difficile de savoir dans quelle mesure les avantages proviennent de la formation de l'encodeur, du décodeur, ou des deux. Une étude plus approfondie en supprimant des composants de conception ou en utilisant le décodeur de Whisper avec d'autres encodeurs pourrait fournir davantage d'informations sur l'interprétabilité. [Plus d'informations nécessaires]\n\nWhisper commet encore des erreurs tenaces dans la transcription longue qui semblent résolument non humaines/perceptuelles, comme rester coincé dans des boucles, sauter des mots ou halluciner des transcriptions sans rapport. Un réglage fin sur des ensembles de données de haute qualité ou l’utilisation de l’apprentissage par renforcement pour optimiser le décodage pourraient aider à expliquer et à réduire ces modes de défaillance. [Plus d'informations nécessaires]\n\nLes performances de Whisper varient considérablement selon la langue, avec une précision beaucoup plus faible pour les langues à faibles ressources. Cela est probablement dû aux données de pré-formation centrées sur l'anglais. L'augmentation des données de formation pour d'autres langues est une voie claire pour améliorer l'explicabilité des capacités spécifiques à la langue du modèle. [Plus d'informations nécessaires]\n\nEn résumé, même si le modèle Whisper présente une robustesse et des performances impressionnantes, des questions restent ouvertes concernant les contributions relatives des différents composants du modèle, les modes d'erreur tenaces et les capacités spécifiques au langage qui nécessitent des recherches plus approfondies pour expliquer et interpréter pleinement les comportements du modèle. Notre équipe continuera d'étudier ces domaines pour améliorer l'interprétabilité du modèle.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель Whisper, вот что я могу сказать об объяснимости и интерпретируемости модели openai/whisper-base.en, над которой работала наша команда:\n\nНадежность модели Whisper, по-видимому, частично обусловлена ​​ее сильным декодером, который представляет собой модель условного языка аудио. Однако в настоящее время неясно, в какой степени преимущества будут получены от обучения кодировщика, декодера или того и другого. Дальнейшее изучение этого вопроса путем удаления компонентов конструкции или использования декодера Whisper с другими кодировщиками может дать больше информации об интерпретируемости. [Требуется дополнительная информация]\n\nWhisper по-прежнему допускает некоторые упрямые ошибки в длинной транскрипции, которые кажутся явно нечеловеческими/перцептивными, например, зацикливание, пропуск слов или галлюцинации несвязанных транскриптов. Точная настройка высококачественных наборов данных или использование обучения с подкреплением для оптимизации декодирования могут помочь объяснить и уменьшить эти виды сбоев. [Требуется дополнительная информация]\n\nПроизводительность Whisper значительно варьируется в зависимости от языка, при этом точность для языков с меньшими ресурсами гораздо ниже. Вероятно, это связано с данными предварительной подготовки, ориентированными на английский язык. Увеличение объема обучающих данных для других языков — это очевидный путь к улучшению объяснимости языковых возможностей модели. [Требуется дополнительная информация]\n\nТаким образом, хотя модель Whisper демонстрирует впечатляющую надежность и производительность, все еще остаются открытые вопросы относительно относительного вклада различных компонентов модели, устойчивых режимов ошибок и возможностей, специфичных для языка, которые требуют дальнейших исследований для полного объяснения и интерпретации поведения модели. Наша команда продолжит изучать эти области, чтобы улучшить интерпретируемость модели.",
    "answer_Japanese": "Whisper モデルに関して提供された参考資料に基づいて、私たちのチームが取り組んだ openai/whisper-base.en モデルの説明可能性と解釈可能性について言えることは次のとおりです。\n\nWhisper モデルの堅牢性は、オーディオ条件付き言語モデルである強力なデコーダーに部分的に起因しているようです。ただし、エンコーダー、デコーダー、またはその両方のトレーニングからどの程度のメリットが得られるかは現時点では不明です。設計コンポーネントをアブレーションするか、Whisper のデコーダを他のエンコーダと併用することでこれをさらに研究すると、より多くの解釈可能性に関する洞察が得られる可能性があります。 [さらに詳しい情報が必要です]\n\nWhisper は依然として、ループにはまり込んだり、単語を飛ばしたり、無関係なトランスクリプトを幻覚したりするなど、明らかに非人間的/知覚的であると思われる、長文のトランスクリプションでいくつかの頑固なエラーを犯します。高品質のデータセットを微調整したり、強化学習を使用してデコードを最適化することは、これらの障害モードを説明し、軽減するのに役立つ可能性があります。 [さらに詳しい情報が必要です]\n\nWhisper のパフォーマンスは言語によって大きく異なり、リソースの少ない言語では精度が大幅に低くなります。これはおそらく英語中心の事前トレーニング データによるものと思われます。他の言語のトレーニング データを増やすことは、モデルの言語固有の機能の説明可能性を向上させるための明確な道です。 [さらに詳しい情報が必要です]\n\n要約すると、Whisper モデルは印象的な堅牢性とパフォーマンスを示していますが、さまざまなモデル コンポーネントの相対的な寄与、頑固なエラー モード、および言語固有の機能に関してはまだ未解決の疑問があり、モデルの動作を完全に説明して解釈するにはさらなる研究が必要です。私たちのチームは、モデルの解釈可能性を向上させるために、これらの分野の研究を続けます。"
  },
  {
    "question": "hardware",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not specify the hardware type that the model openai/whisper-base.en was trained on. The references mention using data parallelism across accelerators and FP16 precision, but do not explicitly state whether GPUs, TPUs, or other hardware was used for training this specific model.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献未指定模型 openai/whisper-base.en 训练的硬件类型。这些参考文献提到使用跨加速器的数据并行性和 FP16 精度，但没有明确说明是否使用 GPU、TPU 或其他硬件来训练该特定模型。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحدد المراجع المقدمة نوع الجهاز الذي تم تدريب النموذج openai/whisper-base.en عليه. تشير المراجع إلى استخدام توازي البيانات عبر المسرعات ودقة FP16، ولكنها لا توضح صراحة ما إذا كانت وحدات معالجة الرسومات، أو وحدات معالجة الرسومات، أو الأجهزة الأخرى قد تم استخدامها لتدريب هذا النموذج المحدد.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne précisent pas le type de matériel sur lequel le modèle openai/whisper-base.en a été formé. Les références mentionnent l'utilisation du parallélisme des données entre les accélérateurs et la précision du FP16, mais n'indiquent pas explicitement si des GPU, des TPU ou un autre matériel ont été utilisés pour entraîner ce modèle spécifique.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nВ предоставленных ссылках не указан тип оборудования, на котором обучалась модель openai/whisper-base.en. В ссылках упоминается использование параллелизма данных между ускорителями и точности FP16, но не указывается явно, использовались ли графические процессоры, TPU или другое оборудование для обучения этой конкретной модели.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参照では、モデル openai/whisper-base.en がトレーニングされたハードウェア タイプは指定されていません。参考文献では、アクセラレータ間でのデータ並列処理と FP16 精度の使用について言及していますが、この特定のモデルのトレーニングに GPU、TPU、またはその他のハードウェアが使用されたかどうかは明示的に述べられていません。"
  },
  {
    "question": "software",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base.en is trained using:\n\n- Data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing\n- AdamW optimizer\n- Gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size of 256 segments\n- Trained for 2^20 updates (between two and three passes over the dataset)\n\nThe references do not explicitly mention the specific software or deep learning framework used for training the model. [More Information Needed] on the exact software stack and libraries used in the training process.",
    "answer_Chinese": "根据提供的参考资料，模型 openai/whisper-base.en 使用以下方法进行训练：\n\n- 使用具有动态损失缩放和激活检查点功能的 FP16 实现跨加速器的数据并行\n- AdamW优化器\n- 梯度范数裁剪\n- 在前 2048 个更新的预热后，线性学习率衰减至零\n- 256 段的批量大小\n- 训练 2^20 次更新（在数据集上进行两到三遍）\n\n这些参考文献没有明确提及用于训练模型的特定软件或深度学习框架。 [需要更多信息]关于培训过程中使用的确切软件堆栈和库。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، تم تدريب النموذج openai/whisper-base.en باستخدام:\n\n- توازي البيانات عبر المسرعات باستخدام FP16 مع قياس الخسارة الديناميكي وفحص التنشيط\n- محسن AdamW\n- لقطة قاعدة التدرج\n- انخفاض معدل التعلم الخطي إلى الصفر بعد الإحماء خلال تحديثات 2048 الأولى\n- حجم الدفعة 256 قطعة\n- تم التدريب على تحديثات 2^20 (ما بين مرورين وثلاثة تمريرات عبر مجموعة البيانات)\n\nلا تذكر المراجع صراحة البرنامج المحدد أو إطار التعلم العميق المستخدم لتدريب النموذج. [مطلوب مزيد من المعلومات] حول مجموعة البرامج والمكتبات المستخدمة في عملية التدريب.",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base.en est formé en utilisant :\n\n- Parallélisme des données entre les accélérateurs utilisant FP16 avec mise à l'échelle dynamique des pertes et points de contrôle d'activation\n- Optimiseur AdamW\n- Découpage de la norme de dégradé\n- Le taux d'apprentissage linéaire diminue jusqu'à zéro après un échauffement au cours des 2048 premières mises à jour\n- Taille du lot de 256 segments\n- Formé pour 2^20 mises à jour (entre deux et trois passages sur l'ensemble de données)\n\nLes références ne mentionnent pas explicitement le logiciel spécifique ou le cadre d'apprentissage en profondeur utilisé pour former le modèle. [Plus d'informations nécessaires] sur la pile logicielle exacte et les bibliothèques utilisées dans le processus de formation.",
    "answer_Russian": "На основе предоставленных ссылок модель openai/whisper-base.en обучается с использованием:\n\n- Параллелизм данных между ускорителями с использованием FP16 с динамическим масштабированием потерь и контрольными точками активации.\n- оптимизатор AdamW\n- Отсечение нормы градиента\n- Линейное снижение скорости обучения до нуля после прогрева в течение первых 2048 обновлений.\n- Размер партии 256 сегментов\n- Обучено для 2^20 обновлений (от двух до трех проходов по набору данных)\n\nВ ссылках явно не упоминается конкретное программное обеспечение или среда глубокого обучения, используемая для обучения модели. [Требуется дополнительная информация] о конкретном наборе программного обеспечения и библиотеках, используемых в процессе обучения.",
    "answer_Japanese": "提供された参照に基づいて、モデル openai/whisper-base.en は以下を使用してトレーニングされます。\n\n- 動的な損失スケーリングとアクティベーション チェックポイント機能を備えた FP16 を使用したアクセラレータ間でのデータ並列処理\n- AdamW オプティマイザー\n- 勾配ノルムクリッピング\n- 最初の 2048 アップデートのウォームアップ後、線形学習率がゼロに減衰します。\n- 256 セグメントのバッチ サイズ\n- 2^20 更新用にトレーニング済み (データセットの 2 回から 3 回のパスの間)\n\n参考文献では、モデルのトレーニングに使用される特定のソフトウェアや深層学習フレームワークについては明示的に言及されていません。トレーニング プロセスで使用される正確なソフトウェア スタックとライブラリに関する [追加情報が必要]。"
  },
  {
    "question": "hours_used",
    "chain": "generation",
    "answer": "Based on the information provided in the references, the Whisper models were trained for 2^20 updates, which is between two and three passes over the dataset. However, the exact amount of time used to train the specific model openai/whisper-base.en is not explicitly mentioned.\n\n[More Information Needed] on the precise training time for the openai/whisper-base.en model.",
    "answer_Chinese": "根据参考文献中提供的信息，Whisper 模型接受了 2^20 次更新训练，即对数据集进行两到三遍更新。然而，没有明确提及用于训练特定模型 openai/whisper-base.en 的确切时间。\n\n[需要更多信息] 关于 openai/whisper-base.en 模型的精确训练时间。",
    "answer_Arabic": "استنادًا إلى المعلومات المقدمة في المراجع، تم تدريب نماذج Whisper على تحديثات 2^20، والتي تتراوح بين مرتين وثلاث مرات عبر مجموعة البيانات. ومع ذلك، لم يتم ذكر مقدار الوقت المحدد الذي تم استخدامه لتدريب النموذج المحدد openai/whisper-base.en بشكل صريح.\n\n[بحاجة إلى مزيد من المعلومات] حول وقت التدريب الدقيق لنموذج openai/whisper-base.en.",
    "answer_French": "Sur la base des informations fournies dans les références, les modèles Whisper ont été entraînés pour 2 ^ 20 mises à jour, soit entre deux et trois passages sur l'ensemble de données. Cependant, le temps exact utilisé pour former le modèle spécifique openai/whisper-base.en n'est pas explicitement mentionné.\n\n[Plus d'informations nécessaires] sur le temps de formation précis pour le modèle openai/whisper-base.en.",
    "answer_Russian": "На основе информации, представленной в ссылках, модели Whisper были обучены для 2^20 обновлений, что составляет от двух до трех проходов по набору данных. Однако точное количество времени, затраченное на обучение конкретной модели openai/whisper-base.en, явно не упоминается.\n\n[Требуется дополнительная информация] о точном времени обучения для модели openai/whisper-base.en.",
    "answer_Japanese": "参考資料で提供される情報に基づいて、Whisper モデルは 2^20 更新、つまりデータセット上で 2 ～ 3 回のパスでトレーニングされました。ただし、特定のモデル openai/whisper-base.en のトレーニングに費やされる正確な時間については、明示的には言及されていません。\n\nopenai/whisper-base.en モデルの正確なトレーニング時間に関する [詳細情報が必要]。"
  },
  {
    "question": "cloud_provider",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not mention the specific cloud provider used for training the openai/whisper-base.en model. More information would be needed from the model developers or documentation to determine which cloud provider, if any, was utilized during training.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献没有提及用于训练 openai/whisper-base.en 模型的特定云提供商。需要模型开发人员或文档提供更多信息，以确定在培训期间使用哪个云提供商（如果有）。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تذكر المراجع المقدمة موفر السحابة المحدد المستخدم لتدريب نموذج openai/whisper-base.en. ستكون هناك حاجة إلى مزيد من المعلومات من مطوري النماذج أو الوثائق لتحديد موفر السحابة، إن وجد، الذي تم استخدامه أثناء التدريب.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne mentionnent pas le fournisseur de cloud spécifique utilisé pour former le modèle openai/whisper-base.en. Plus d'informations seraient nécessaires de la part des développeurs de modèles ou de la documentation pour déterminer quel fournisseur de cloud, le cas échéant, a été utilisé pendant la formation.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nВ предоставленных ссылках не упоминается конкретный поставщик облачных услуг, использованный для обучения модели openai/whisper-base.en. Чтобы определить, какой поставщик облачных услуг (если таковой имеется) использовался во время обучения, потребуется дополнительная информация от разработчиков модели или документация.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、openai/whisper-base.en モデルのトレーニングに使用される特定のクラウド プロバイダーについては言及されていません。トレーニング中にどのクラウド プロバイダーが使用されたか (存在する場合) を判断するには、モデル開発者またはドキュメントから詳細な情報が必要になります。"
  },
  {
    "question": "co2_emitted",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain any information about the amount of carbon emitted during the training of the openai/whisper-base.en model. To answer this question, more specific information would be needed, such as the compute resources used for training, the duration of the training process, and the energy sources powering the training infrastructure.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献不包含有关 openai/whisper-base.en 模型训练期间碳排放量的任何信息。要回答这个问题，需要更具体的信息，例如用于训练的计算资源、训练过程的持续时间以及为训练基础设施提供动力的能源。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على أي معلومات حول كمية الكربون المنبعثة أثناء التدريب على نموذج openai/whisper-base.en. للإجابة على هذا السؤال، ستكون هناك حاجة إلى معلومات أكثر تحديدًا، مثل موارد الحوسبة المستخدمة للتدريب، ومدة عملية التدريب، ومصادر الطاقة التي تعمل على تشغيل البنية التحتية للتدريب.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent aucune information sur la quantité de carbone émise lors de la formation du modèle openai/whisper-base.en. Pour répondre à cette question, des informations plus spécifiques seraient nécessaires, telles que les ressources de calcul utilisées pour la formation, la durée du processus de formation et les sources d'énergie alimentant l'infrastructure de formation.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат никакой информации о количестве углерода, выбрасываемого во время обучения модели openai/whisper-base.en. Чтобы ответить на этот вопрос, потребуется более конкретная информация, такая как вычислительные ресурсы, используемые для обучения, продолжительность процесса обучения и источники энергии, питающие инфраструктуру обучения.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、openai/whisper-base.en モデルのトレーニング中に排出される炭素の量に関する情報は含まれていません。この質問に答えるには、トレーニングに使用されるコンピューティング リソース、トレーニング プロセスの期間、トレーニング インフラストラクチャに電力を供給するエネルギー源など、より具体的な情報が必要になります。"
  },
  {
    "question": "model_specs",
    "chain": "generation",
    "answer": "Model Architecture:\nThe openai/whisper-base.en model uses an encoder-decoder Transformer architecture. Key components include:\n\n- Audio is resampled to 16kHz and converted to an 80-channel log-mel spectrogram using a 25ms window and 10ms stride\n- Input is normalized to [-1, 1] with ~0 mean across the pretraining data\n- Encoder has a small 2-layer CNN stem with stride 2 in the second layer, followed by sinusoidal positional embeddings and Transformer blocks\n- Encoder and decoder have the same width and number of blocks\n- Decoder uses learned positional embeddings and tied input-output token embeddings\n- Pre-activation residual blocks are used in the Transformer\n- Final layer norm is applied to the encoder output\n\nModel Objective:\nThe openai/whisper-base.en model is trained on a multitask mixture of speech processing tasks, including:\n\n- Multilingual speech recognition \n- Speech translation\n- Spoken language identification\n- Voice activity detection\n\nThese tasks are jointly represented as a sequence of tokens to be predicted by the decoder. Special tokens are used to specify the task or classification targets. The model is trained on a large 680,000 hour dataset of transcribed audio from the internet with the goal of learning general speech representations that can be used for zero-shot transfer to downstream tasks without finetuning.\n\n[More Information Needed] on the exact pretraining data composition, training hyperparameters, and performance benchmarks for this specific base.en model variant.",
    "answer_Chinese": "模型架构：\nopenai/whisper-base.en 模型使用编码器-解码器 Transformer 架构。关键组件包括：\n\n- 音频重新采样至 16kHz，并使用 25ms 窗口和 10ms 步幅转换为 80 通道 log-mel 频谱图\n- 输入标准化为 [-1, 1]，预训练数据的平均值约为 0\n- 编码器有一个小型 2 层 CNN 主干，第二层的步幅为 2，后面是正弦位置嵌入和 Transformer 块\n- 编码器和解码器具有相同的宽度和块数\n- 解码器使用学习的位置嵌入和绑定的输入输出令牌嵌入\n- Transformer 中使用预激活残差块\n- 最终层范数应用于编码器输出\n\n模型目标：\nopenai/whisper-base.en 模型经过语音处理任务的多任务混合训练，包括：\n\n- 多语言语音识别 \n- 语音翻译\n- 口语识别\n- 语音活动检测\n\n这些任务联合表示为由解码器预测的令牌序列。特殊标记用于指定任务或分类目标。该模型在来自互联网的 680,000 小时转录音频的大型数据集上进行训练，目标是学习通用语音表示，该表示可用于零样本传输到下游任务，而无需微调。\n\n[需要更多信息] 有关此特定 base.en 模型变体的确切预训练数据组成、训练超参数和性能基准。",
    "answer_Arabic": "العمارة النموذجية:\nيستخدم نموذج openai/whisper-base.en بنية محول التشفير وفك التشفير. تشمل المكونات الرئيسية ما يلي:\n\n- تتم إعادة تشكيل الصوت إلى 16 كيلو هرتز وتحويله إلى مخطط طيفي سجل ميل ذو 80 قناة باستخدام نافذة تبلغ 25 مللي ثانية وخطوة تبلغ 10 مللي ثانية\n- تتم تسوية الإدخال إلى [-1، 1] بمتوسط ​​~0 عبر بيانات التدريب المسبق\n- يحتوي جهاز التشفير على جذع CNN صغير مكون من طبقتين مع خطوة 2 في الطبقة الثانية، تليها التضمينات الموضعية الجيبية وكتل المحولات\n- التشفير ووحدة فك التشفير لهما نفس العرض وعدد الكتل\n- يستخدم جهاز فك التشفير التضمينات الموضعية المستفادة والتضمينات المميزة للإدخال والمخرجات\n- يتم استخدام الكتل المتبقية قبل التنشيط في المحول\n- يتم تطبيق معيار الطبقة النهائية على مخرجات التشفير\n\nهدف النموذج:\nتم تدريب نموذج openai/whisper-base.en على مزيج متعدد المهام من مهام معالجة الكلام، بما في ذلك:\n\n- التعرف على الكلام متعدد اللغات \n- ترجمة الكلام\n- تحديد اللغة المنطوقة\n- كشف النشاط الصوتي\n\nيتم تمثيل هذه المهام بشكل مشترك كسلسلة من الرموز المميزة التي سيتم التنبؤ بها بواسطة وحدة فك التشفير. يتم استخدام الرموز المميزة لتحديد أهداف المهمة أو التصنيف. تم تدريب النموذج على مجموعة بيانات كبيرة مدتها 680.000 ساعة من الصوت المكتوب من الإنترنت بهدف تعلم تمثيلات الكلام العامة التي يمكن استخدامها للنقل بدون لقطة إلى المهام النهائية دون ضبط دقيق.\n\n[مطلوب مزيد من المعلومات] حول التكوين الدقيق لبيانات التدريب المسبق، ومعلمات التدريب الفائقة، ومعايير الأداء لهذا النوع المحدد من طراز Base.en.",
    "answer_French": "Architecture du modèle :\nLe modèle openai/whisper-base.en utilise une architecture Transformer encodeur-décodeur. Les composants clés comprennent :\n\n- L'audio est rééchantillonné à 16 kHz et converti en un spectrogramme log-mel à 80 canaux en utilisant une fenêtre de 25 ms et une foulée de 10 ms\n- L'entrée est normalisée à [-1, 1] avec une moyenne de ~ 0 sur les données de pré-entraînement\n- L'encodeur a une petite tige CNN à 2 couches avec une foulée 2 dans la deuxième couche, suivie d'intégrations positionnelles sinusoïdales et de blocs transformateurs.\n- L'encodeur et le décodeur ont la même largeur et le même nombre de blocs\n- Le décodeur utilise des intégrations positionnelles apprises et des intégrations de jetons d'entrée-sortie liées\n- Des blocs résiduels de pré-activation sont utilisés dans le transformateur\n- La norme de couche finale est appliquée à la sortie du codeur\n\nObjectif du modèle :\nLe modèle openai/whisper-base.en est formé sur un mélange multitâche de tâches de traitement de la parole, notamment :\n\n- Reconnaissance vocale multilingue \n- Traduction vocale\n- Identification de la langue parlée\n- Détection d'activité vocale\n\nCes tâches sont représentées conjointement sous la forme d'une séquence de jetons à prédire par le décodeur. Des jetons spéciaux sont utilisés pour spécifier les cibles de tâche ou de classification. Le modèle est formé sur un vaste ensemble de données de 680 000 heures d'audio transcrit à partir d'Internet dans le but d'apprendre des représentations vocales générales qui peuvent être utilisées pour un transfert zéro vers des tâches en aval sans réglage fin.\n\n[Plus d'informations nécessaires] sur la composition exacte des données de pré-entraînement, les hyperparamètres d'entraînement et les références de performances pour cette variante spécifique du modèle base.en.",
    "answer_Russian": "Модельная архитектура:\nМодель openai/whisper-base.en использует архитектуру преобразователя кодировщика-декодера. Ключевые компоненты включают в себя:\n\n- Звук передискретизируется до 16 кГц и преобразуется в 80-канальную логарифмическую спектрограмму с использованием окна 25 мс и шага 10 мс.\n- Входные данные нормализуются до [-1, 1] со средним значением ~ 0 для данных перед обучением.\n- Кодер имеет небольшой двухслойный стержень CNN с шагом 2 во втором слое, за которым следуют синусоидальные позиционные вложения и блоки Transformer.\n- Кодер и декодер имеют одинаковую ширину и количество блоков.\n- Декодер использует изученные позиционные внедрения и привязанные внедрения токенов ввода-вывода.\n- В Трансформаторе используются остаточные блоки предварительной активации.\n- Норма финального слоя применяется к выходу кодера\n\nЦель модели:\nМодель openai/whisper-base.en обучена на многозадачном сочетании задач обработки речи, включая:\n\n- Многоязычное распознавание речи \n- Перевод речи\n- Идентификация разговорного языка\n- Обнаружение голосовой активности\n\nЭти задачи совместно представляются как последовательность токенов, которые должны быть предсказаны декодером. Специальные токены используются для указания цели задачи или классификации. Модель обучается на большом наборе данных транскрибированного аудио из Интернета продолжительностью 680 000 часов с целью изучения общих речевых представлений, которые можно использовать для нулевой передачи последующим задачам без точной настройки.\n\n[Требуется дополнительная информация] о точном составе данных перед обучением, гиперпараметрах обучения и тестах производительности для этого конкретного варианта модели base.en.",
    "answer_Japanese": "モデルのアーキテクチャ:\nopenai/whisper-base.en モデルは、エンコーダー/デコーダー Transformer アーキテクチャを使用します。主要なコンポーネントは次のとおりです。\n\n- オーディオは 16kHz にリサンプリングされ、25ms ウィンドウと 10ms ストライドを使用して 80 チャンネルのログメル スペクトログラムに変換されます。\n- 入力は [-1, 1] に正規化され、事前トレーニング データ全体で平均が ~0 になります。\n- エンコーダーには、2 番目の層にストライド 2 を持つ小さな 2 層 CNN ステムがあり、その後に正弦波位置埋め込みと Transformer ブロックが続きます。\n- エンコーダーとデコーダーの幅とブロック数は同じです\n- デコーダーは、学習された位置埋め込みと関連付けられた入出力トークン埋め込みを使用します。\n- 起動前の残差ブロックは Transformer で使用されます\n- 最終層ノルムがエンコーダー出力に適用されます\n\nモデルの目的:\nopenai/whisper-base.en モデルは、以下を含む音声処理タスクのマルチタスク混合物でトレーニングされます。\n\n- 多言語音声認識 \n- 音声翻訳\n- 音声言語の識別\n- 音声アクティビティの検出\n\nこれらのタスクは、デコーダーによって予測されるトークンのシーケンスとしてまとめて表現されます。特別なトークンは、タスクまたは分類のターゲットを指定するために使用されます。このモデルは、微調整なしで下流タスクへのゼロショット転送に使用できる一般的な音声表現を学習することを目的として、インターネットから書き起こされた音声の 680,000 時間の大規模なデータセットでトレーニングされます。\n\nこの特定のbase.enモデルバリアントの正確な事前トレーニングデータ構成、トレーニングハイパーパラメータ、およびパフォーマンスベンチマークに関する[詳細情報が必要]。"
  },
  {
    "question": "compute_infrastructure",
    "chain": "generation",
    "answer": "Based on the provided references, here is the available information about the compute infrastructure used for training the openai/whisper-base.en model:\n\nThe models were trained using:\n- Data parallelism across accelerators\n- FP16 precision with dynamic loss scaling\n- Activation checkpointing\n\nThe optimizer and training hyperparameters:\n- AdamW optimizer\n- Gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size of 256 segments\n- Trained for 2^20 updates (between two and three passes over the dataset)\n\n[More Information Needed] on the specific hardware (GPU/TPU models and counts) used for training.\n\nThe references do not include code blocks directly relevant to the compute infrastructure.",
    "answer_Chinese": "根据提供的参考资料，以下是有关用于训练 openai/whisper-base.en 模型的计算基础设施的可用信息：\n\n模型使用以下方法进行训练：\n- 跨加速器的数据并行性\n- 具有动态损耗缩放功能的 FP16 精度\n- 激活检查点\n\n优化器和训练超参数：\n- AdamW优化器\n- 梯度范数裁剪\n- 在前 2048 个更新的预热后，线性学习率衰减至零\n- 256 段的批量大小\n- 训练 2^20 次更新（在数据集上进行两到三遍）\n\n[需要更多信息]有关用于训练的特定硬件（GPU/TPU 模型和计数）。\n\n这些参考文献不包括与计算基础设施直接相关的代码块。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك المعلومات المتوفرة حول البنية الأساسية للحوسبة المستخدمة لتدريب نموذج openai/whisper-base.en:\n\nتم تدريب النماذج باستخدام:\n- توازي البيانات عبر المسرعات\n- دقة FP16 مع قياس الخسارة الديناميكي\n- تفعيل نقاط التفتيش\n\nالمحسن والمعلمات الفائقة للتدريب:\n- محسن AdamW\n- لقطة قاعدة التدرج\n- انخفاض معدل التعلم الخطي إلى الصفر بعد الإحماء خلال تحديثات 2048 الأولى\n- حجم الدفعة 256 قطعة\n- تم التدريب على تحديثات 2^20 (ما بين مرورين وثلاثة تمريرات عبر مجموعة البيانات)\n\n[مطلوب مزيد من المعلومات] حول الأجهزة المحددة (نماذج وأعداد وحدات معالجة الرسومات/وحدات معالجة الرسومات) المستخدمة للتدريب.\n\nلا تتضمن المراجع كتل التعليمات البرمجية ذات الصلة المباشرة بالبنية الأساسية للحوسبة.",
    "answer_French": "Sur la base des références fournies, voici les informations disponibles sur l'infrastructure de calcul utilisée pour former le modèle openai/whisper-base.en :\n\nLes modèles ont été formés en utilisant :\n- Parallélisme des données entre les accélérateurs\n- Précision FP16 avec mise à l'échelle dynamique des pertes\n- Points de contrôle d'activation\n\nLes hyperparamètres de l’optimiseur et de la formation :\n- Optimiseur AdamW\n- Découpage de la norme de dégradé\n- Le taux d'apprentissage linéaire diminue jusqu'à zéro après un échauffement au cours des 2048 premières mises à jour\n- Taille du lot de 256 segments\n- Formé pour 2^20 mises à jour (entre deux et trois passages sur l'ensemble de données)\n\n[Plus d'informations nécessaires] sur le matériel spécifique (modèles et nombres de GPU/TPU) utilisé pour la formation.\n\nLes références n'incluent pas de blocs de code directement pertinents pour l'infrastructure de calcul.",
    "answer_Russian": "На основе предоставленных ссылок приведена доступная информация о вычислительной инфраструктуре, используемой для обучения модели openai/whisper-base.en:\n\nМодели обучались с использованием:\n- Параллелизм данных между ускорителями\n- Точность FP16 с динамическим масштабированием потерь\n- Контрольная точка активации\n\nОптимизатор и обучающие гиперпараметры:\n- оптимизатор AdamW\n- Отсечение нормы градиента\n- Линейное снижение скорости обучения до нуля после прогрева в течение первых 2048 обновлений.\n- Размер партии 256 сегментов\n- Обучено для 2^20 обновлений (от двух до трех проходов по набору данных)\n\n[Требуется дополнительная информация] о конкретном оборудовании (модели и количество графических процессоров/TPU), используемом для обучения.\n\nСсылки не включают блоки кода, непосредственно относящиеся к вычислительной инфраструктуре.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base.en モデルのトレーニングに使用されるコンピューティング インフラストラクチャに関する入手可能な情報は次のとおりです。\n\nモデルは以下を使用してトレーニングされました。\n- アクセラレータ間でのデータ並列処理\n- 動的損失スケーリングを備えた FP16 精度\n- アクティベーションチェックポイント\n\nオプティマイザーとトレーニングのハイパーパラメーター:\n- AdamW オプティマイザー\n- 勾配ノルムクリッピング\n- 最初の 2048 アップデートのウォームアップ後、線形学習率がゼロに減衰します。\n- 256 セグメントのバッチ サイズ\n- 2^20 更新用にトレーニング済み (データセットの 2 回から 3 回のパスの間)\n\nトレーニングに使用される特定のハードウェア (GPU/TPU モデルと数) に関する [追加情報が必要]。\n\nリファレンスには、コンピューティング インフラストラクチャに直接関連するコード ブロックは含まれていません。"
  }
]