[
  {
    "question": "summary",
    "chain": "generation",
    "answer": "openai/whisper-base is a general-purpose speech recognition model that is trained on a large dataset of diverse audio. It is a multitasking model capable of multilingual speech recognition, speech translation, and language identification.",
    "answer_Chinese": "openai/whisper-base 是一种通用语音识别模型，在不同音频的大型数据集上进行训练。它是一个能够进行多语言语音识别、语音翻译和语言识别的多任务模型。",
    "answer_Arabic": "openai/whisper-base هو نموذج للتعرف على الكلام للأغراض العامة يتم تدريبه على مجموعة كبيرة من البيانات الصوتية المتنوعة. إنه نموذج متعدد المهام قادر على التعرف على الكلام متعدد اللغات وترجمة الكلام وتحديد اللغة.",
    "answer_French": "openai/whisper-base est un modèle de reconnaissance vocale à usage général qui est formé sur un vaste ensemble de données audio diverses. Il s'agit d'un modèle multitâche capable de reconnaissance vocale multilingue, de traduction vocale et d'identification de la langue.",
    "answer_Russian": "openai/whisper-base — это модель распознавания речи общего назначения, которая обучается на большом наборе данных разнообразного аудио. Это многозадачная модель, способная к многоязычному распознаванию речи, переводу речи и идентификации языка.",
    "answer_Japanese": "openai/whisper-base は、多様な音声の大規模なデータセットでトレーニングされた汎用音声認識モデルです。多言語音声認識、音声翻訳、言語識別が可能なマルチタスクモデルです。"
  },
  {
    "question": "description",
    "chain": "generation",
    "answer": "Model Details for openai/whisper-base:\n\nModel Architecture:\n- Encoder-decoder Transformer architecture\n- Audio is re-sampled to 16,000 Hz\n- 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10\n- Pre-activation residual blocks are used in the Transformer\n- Final layer normalization is applied to the encoder output\n- Decoder uses learned position embeddings and tied input-output token representations\n- Encoder and decoder have the same width and number of transformer blocks\n\nTraining Procedures:\n- Trained on a large dataset of 680,000 hours of labeled audio\n- No data augmentation or regularization is used, relying on dataset diversity for generalization and robustness\n- Trained for 2^20 updates, which is between two and three passes over the dataset\n- Early stopping based on validation loss is used\n\nParameters:\n[More Information Needed]\n\nImportant Disclaimers:\n- Whisper models may have a tendency to transcribe plausible but incorrect guesses for speaker names, as this information is rarely inferable from the most recent 30 seconds of audio alone\n\nAdditional Notes:\n- Whisper is a general-purpose speech recognition model trained on a diverse audio dataset\n- It is a multitasking model capable of multilingual speech recognition, speech translation, and language identification\n\nFor more details on the model architecture, please refer to Figure 1 in the provided references.",
    "answer_Chinese": "openai/whisper-base 的型号详细信息：\n\n模型架构：\n- 编码器-解码器 Transformer 架构\n- 音频重新采样至 16,000 Hz\n- 80 通道对数幅度 Mel 频谱图表示在 25 毫秒窗口上计算，步长为 10\n- Transformer 中使用预激活残差块\n- 最终层归一化应用于编码器输出\n- 解码器使用学习的位置嵌入和绑定的输入输出标记表示\n- 编码器和解码器具有相同的宽度和变压器块数量\n\n培训程序：\n- 在包含 680,000 小时标记音频的大型数据集上进行训练\n- 不使用数据增强或正则化，依靠数据集多样性来实现泛化和鲁棒性\n- 训练 2^20 次更新，即数据集的两到三遍之间\n- 使用基于验证损失的提前停止\n\n参数：\n[需要更多信息]\n\n重要免责声明：\n- 耳语模型可能倾向于转录看似合理但不正确的说话者姓名猜测，因为仅从最近 30 秒的音频中很难推断出此信息\n\n补充笔记：\n- Whisper 是一种在不同音频数据集上训练的通用语音识别模型\n- 它是一个多任务模型，能够进行多语言语音识别、语音翻译和语言识别\n\n有关模型架构的更多详细信息，请参阅所提供参考文献中的图 1。",
    "answer_Arabic": "تفاصيل النموذج لقاعدة openai/whisper:\n\nالعمارة النموذجية:\n- بنية محول التشفير وفك التشفير\n- يتم إعادة أخذ عينات الصوت إلى 16000 هرتز\n- يتم حساب التمثيل الطيفي Mel ذو حجم السجل 80 قناة على نوافذ تبلغ 25 مللي ثانية مع خطوة قدرها 10\n- يتم استخدام الكتل المتبقية قبل التنشيط في المحول\n- يتم تطبيق تسوية الطبقة النهائية على مخرجات التشفير\n- يستخدم جهاز فك التشفير تضمينات الموضع المستفادة وتمثيلات رمزية للإدخال والمخرجات المرتبطة\n- التشفير وفك التشفير لهما نفس العرض وعدد كتل المحولات\n\nإجراءات التدريب:\n- تم التدريب على مجموعة بيانات كبيرة تبلغ 680.000 ساعة من الصوت المسمى\n- لا يتم استخدام أي زيادة أو تنظيم للبيانات، والاعتماد على تنوع مجموعة البيانات للتعميم والقوة\n- تم التدريب على تحديثات 2^20، والتي تتراوح بين مرورين إلى ثلاثة تمريرات على مجموعة البيانات\n- يتم استخدام التوقف المبكر على أساس فقدان التحقق من الصحة\n\nحدود:\n[مطلوب مزيد من المعلومات]\n\nإخلاء المسؤولية الهامة:\n- قد تميل نماذج الهمس إلى نسخ التخمينات المعقولة ولكن غير الصحيحة لأسماء المتحدثين، حيث نادرًا ما يمكن استنتاج هذه المعلومات من آخر 30 ثانية من الصوت وحده\n\nملاحظات إضافية:\n- Whisper هو نموذج للتعرف على الكلام للأغراض العامة تم تدريبه على مجموعة بيانات صوتية متنوعة\n- إنه نموذج متعدد المهام قادر على التعرف على الكلام متعدد اللغات وترجمة الكلام وتحديد اللغة\n\nلمزيد من التفاصيل حول بنية النموذج، يرجى الرجوع إلى الشكل 1 في المراجع المتوفرة.",
    "answer_French": "Détails du modèle pour openai/whisper-base :\n\nArchitecture du modèle :\n- Architecture du transformateur codeur-décodeur\n- L'audio est rééchantillonné à 16 000 Hz\n- La représentation du spectrogramme Mel à 80 canaux de magnitude log est calculée sur des fenêtres de 25 millisecondes avec une foulée de 10\n- Des blocs résiduels de pré-activation sont utilisés dans le transformateur\n- La normalisation de la couche finale est appliquée à la sortie du codeur\n- Le décodeur utilise des intégrations de positions apprises et des représentations de jetons d'entrée-sortie liées\n- L'encodeur et le décodeur ont la même largeur et le même nombre de blocs de transformateur\n\nProcédures de formation :\n- Formé sur un vaste ensemble de données de 680 000 heures d'audio étiqueté\n- Aucune augmentation ou régularisation des données n'est utilisée, s'appuyant sur la diversité des ensembles de données pour la généralisation et la robustesse\n- Formé pour 2^20 mises à jour, soit entre deux et trois passages sur l'ensemble de données\n- Un arrêt anticipé basé sur la perte de validation est utilisé\n\nParamètres:\n[Plus d'informations nécessaires]\n\nAvertissements importants :\n- Les modèles Whisper peuvent avoir tendance à transcrire des suppositions plausibles mais incorrectes concernant les noms des locuteurs, car ces informations sont rarement déductibles à partir des seules 30 secondes d'audio les plus récentes.\n\nNotes complémentaires:\n- Whisper est un modèle de reconnaissance vocale à usage général formé sur un ensemble de données audio diversifié\n- Il s'agit d'un modèle multitâche capable de reconnaissance vocale multilingue, de traduction vocale et d'identification de la langue.\n\nPour plus de détails sur l'architecture du modèle, veuillez vous référer à la figure 1 dans les références fournies.",
    "answer_Russian": "Детали модели для openai/whisper-base:\n\nМодельная архитектура:\n- Трансформаторная архитектура кодер-декодер\n- Звук повторно дискретизируется до 16 000 Гц.\n- 80-канальное представление Mel-спектрограммы логарифмической величины рассчитывается в 25-миллисекундных окнах с шагом 10.\n- В Трансформаторе используются остаточные блоки предварительной активации.\n- К выходным данным кодера применяется окончательная нормализация слоя.\n- Декодер использует встраивание изученных позиций и связанные представления токенов ввода-вывода.\n- Кодер и декодер имеют одинаковую ширину и количество блоков преобразователя.\n\nПроцедуры обучения:\n- Обучение проводилось на большом наборе данных из 680 000 часов размеченного аудио.\n- Никакое увеличение или регуляризация данных не используется, полагаясь на разнообразие наборов данных для обобщения и надежности.\n- Обучено для 2^20 обновлений, что составляет от двух до трех проходов по набору данных.\n- Используется ранняя остановка на основе потери проверки.\n\nПараметры:\n[Требуется дополнительная информация]\n\nВажные заявления об отказе от ответственности:\n- Модели шепота могут иметь тенденцию расшифровывать правдоподобные, но неверные предположения об именах говорящих, поскольку эту информацию редко можно вывести только на основе последних 30 секунд звука.\n\nДополнительные замечания:\n- Whisper — это модель распознавания речи общего назначения, обученная на разнообразном наборе аудиоданных.\n- Это многозадачная модель, способная к многоязычному распознаванию речи, переводу речи и идентификации языка.\n\nБолее подробную информацию об архитектуре модели можно найти на рисунке 1 в предоставленных ссылках.",
    "answer_Japanese": "openai/whisper-base のモデル詳細:\n\nモデルのアーキテクチャ:\n- エンコーダー/デコーダートランスフォーマーのアーキテクチャ\n- オーディオは 16,000 Hz に再サンプリングされます\n- 80 チャネルの対数振幅メル スペクトログラム表現は、ストライド 10 の 25 ミリ秒ウィンドウで計算されます。\n- 起動前の残差ブロックは Transformer で使用されます\n- 最終層の正規化がエンコーダー出力に適用されます\n- デコーダーは学習された位置埋め込みと関連付けられた入出力トークン表現を使用します\n- エンコーダとデコーダのトランスブロックの幅と数は同じです\n\nトレーニング手順:\n- 680,000 時間のラベル付き音声の大規模なデータセットでトレーニング\n- データの拡張や正則化は使用されず、一般化と堅牢性のためにデータセットの多様性に依存します。\n- データセットの 2 ～ 3 回のパスである 2^20 更新用にトレーニング済み\n- 検証損失に基づく早期停止が使用されます\n\nパラメーター：\n[さらに詳しい情報が必要です]\n\n重要な免責事項:\n- ささやきモデルは、話者の名前について、もっともらしいが不正確な推測を転写する傾向がある可能性があります。これは、この情報が直近 30 秒の音声だけから推測できることはほとんどないためです。\n\nその他の注意事項：\n- Whisper は、多様な音声データセットでトレーニングされた汎用音声認識モデルです\n- 多言語の音声認識、音声翻訳、言語識別が可能なマルチタスクモデルです\n\nモデル アーキテクチャの詳細については、提供されている参考資料の図 1 を参照してください。"
  },
  {
    "question": "funded_by",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base was developed by OpenAI. The references mention the Acceleration and Supercomputing teams at OpenAI and their critical work on software and hardware infrastructure used in this project. However, there is no explicit information about the funding sources for this specific model.\n\n[More Information Needed]",
    "answer_Chinese": "基于所提供的参考资料，OpenAI 开发了 openai/whisper-base 模型。参考文献提到了 OpenAI 的加速和超级计算团队以及他们在该项目中使用的软件和硬件基础设施方面的关键工作。不过，目前还没有关于该特定模型的资金来源的明确信息。\n\n[需要更多信息]",
    "answer_Arabic": "بناءً على المراجع المتوفرة، تم تطوير نموذج openai/whisper-base بواسطة OpenAI. تشير المراجع إلى فرق التسريع والحوسبة الفائقة في OpenAI وعملهم المهم على البنية التحتية للبرامج والأجهزة المستخدمة في هذا المشروع. ومع ذلك، لا توجد معلومات واضحة حول مصادر التمويل لهذا النموذج المحدد.\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base a été développé par OpenAI. Les références mentionnent les équipes d'accélération et de supercalcul d'OpenAI et leur travail critique sur l'infrastructure logicielle et matérielle utilisée dans ce projet. Cependant, il n’existe aucune information explicite sur les sources de financement de ce modèle spécifique.\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "На основе предоставленных ссылок компания OpenAI разработала модель openai/whisper-base. В ссылках упоминаются группы ускорения и суперкомпьютеров в OpenAI и их критическая работа над программной и аппаратной инфраструктурой, используемой в этом проекте. Однако четкой информации об источниках финансирования этой конкретной модели нет.\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "提供された参考資料に基づいて、モデル openai/whisper-base は OpenAI によって開発されました。参考文献には、OpenAI のアクセラレーション チームとスーパーコンピューティング チーム、およびこのプロジェクトで使用されるソフトウェアおよびハードウェア インフラストラクチャに関する彼らの重要な取り組みについて言及されています。ただし、この特定のモデルの資金源に関する明確な情報はありません。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "shared_by",
    "chain": "generation",
    "answer": "Based on the provided references, the main contributors who made the model openai/whisper-base available online as a GitHub repo are:\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever from OpenAI.\n\nThe references also mention thanking the Acceleration and Supercomputing teams at OpenAI for their work on the software and hardware infrastructure used in the project, as well as Pamela Mishkin for advising from a policy perspective. However, it is not explicitly stated if they directly contributed to making the model available on GitHub.\n\n[More Information Needed] on additional contributors, if any, who were directly involved in publishing the model to the GitHub repository.",
    "answer_Chinese": "根据提供的参考资料，将模型 openai/whisper-base 作为 GitHub 存储库在线提供的主要贡献者是：\n\n来自 OpenAI 的 Alec Radford、Jong Wook Kim、Tao Xu、Greg Brockman、Christine Mcleavey 和 Ilya Sutskever。\n\n参考文献还提到感谢 OpenAI 的加速和超级计算团队在该项目中使用的软件和硬件基础设施方面所做的工作，以及 Pamela Mishkin 从政策角度提供的建议。然而，没有明确说明他们是否直接为在 GitHub 上提供该模型做出了贡献。\n\n[需要更多信息] 直接参与将模型发布到 GitHub 存储库的其他贡献者（如果有）。",
    "answer_Arabic": "استنادًا إلى المراجع المقدمة، فإن المساهمين الرئيسيين الذين جعلوا نموذج openai/whisper-base متاحًا عبر الإنترنت كمستودع GitHub هم:\n\nأليك رادفورد، وجونج ووك كيم، وتاو شو، وجريج بروكمان، وكريستين ماكليفي، وإيليا سوتسكيفر من OpenAI.\n\nتشير المراجع أيضًا إلى شكر فرق التسريع والحوسبة الفائقة في OpenAI على عملهم في البنية التحتية للبرامج والأجهزة المستخدمة في المشروع، وكذلك باميلا ميشكين لتقديم المشورة من منظور السياسة. ومع ذلك، لم يتم ذكر ما إذا كانوا قد ساهموا بشكل مباشر في إتاحة النموذج على GitHub.\n\n[مطلوب مزيد من المعلومات] عن المساهمين الإضافيين، إن وجدوا، الذين شاركوا بشكل مباشر في نشر النموذج في مستودع GitHub.",
    "answer_French": "Sur la base des références fournies, les principaux contributeurs qui ont rendu le modèle openai/whisper-base disponible en ligne sous forme de dépôt GitHub sont :\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey et Ilya Sutskever d'OpenAI.\n\nLes références mentionnent également leurs remerciements aux équipes d'accélération et de supercalcul d'OpenAI pour leur travail sur l'infrastructure logicielle et matérielle utilisée dans le projet, ainsi qu'à Pamela Mishkin pour ses conseils d'un point de vue politique. Cependant, il n'est pas explicitement indiqué s'ils ont directement contribué à rendre le modèle disponible sur GitHub.\n\n[Plus d'informations nécessaires] sur les contributeurs supplémentaires, le cas échéant, qui ont été directement impliqués dans la publication du modèle sur le référentiel GitHub.",
    "answer_Russian": "Судя по предоставленным ссылкам, основными участниками, которые сделали модель openai/whisper-base доступной в Интернете в виде репозитория GitHub, являются:\n\nАлек Рэдфорд, Чон Ук Ким, Тао Сюй, Грег Брокман, Кристин Маккливи и Илья Суцкевер из OpenAI.\n\nВ ссылках также упоминается благодарность командам ускорения и суперкомпьютеров OpenAI за их работу над программной и аппаратной инфраструктурой, используемой в проекте, а также Памеле Мишкин за консультации с точки зрения политики. Однако прямо не указано, способствовали ли они непосредственно тому, чтобы модель стала доступной на GitHub.\n\n[Требуется дополнительная информация] о дополнительных участниках, если таковые имеются, которые принимали непосредственное участие в публикации модели в репозитории GitHub.",
    "answer_Japanese": "提供された参考資料に基づいて、モデル openai/whisper-base を GitHub リポジトリとしてオンラインで利用できるようにした主な貢献者は次のとおりです。\n\nOpenAI の Alec Radford、Jong Wook Kim、Tao Xu、Greg Brockman、Christine Mcleavey、Ilya Sutskever です。\n\n参考文献には、プロジェクトで使用されるソフトウェアとハ​​ードウェアのインフラストラクチャに関する作業について OpenAI のアクセラレーション チームとスーパーコンピューティング チームに感謝の意を示しているほか、ポリシーの観点からアドバイスをくれた Pamela Mishkin にも感謝の意が述べられています。ただし、GitHub でモデルを利用できるようにすることに彼らが直接貢献したかどうかは明示されていません。\n\nGitHub リポジトリへのモデルの公開に直接関与した追加の寄稿者がいる場合は、[追加の情報が必要]。"
  },
  {
    "question": "model_type",
    "chain": "generation",
    "answer": "The openai/whisper-base model is a supervised encoder-decoder Transformer model trained on a large dataset of diverse audio for speech recognition, speech translation, and language identification.",
    "answer_Chinese": "openai/whisper-base 模型是一种受监督的编码器-解码器 Transformer 模型，在各种音频的大型数据集上进行训练，用于语音识别、语音翻译和语言识别。",
    "answer_Arabic": "نموذج openai/whisper-base عبارة عن نموذج محول لوحدة تشفير وفك ترميز خاضع للإشراف تم تدريبه على مجموعة كبيرة من البيانات الصوتية المتنوعة للتعرف على الكلام وترجمة الكلام وتحديد اللغة.",
    "answer_French": "Le modèle openai/whisper-base est un modèle de transformateur d'encodeur-décodeur supervisé formé sur un vaste ensemble de données audio diverses pour la reconnaissance vocale, la traduction vocale et l'identification de la langue.",
    "answer_Russian": "Модель openai/whisper-base представляет собой контролируемую модель преобразователя кодировщика-декодера, обученную на большом наборе данных разнообразного аудио для распознавания речи, перевода речи и идентификации языка.",
    "answer_Japanese": "openai/whisper-base モデルは、音声認識、音声翻訳、および言語識別のために、さまざまな音声の大規模なデータセットでトレーニングされた教師ありエンコーダー/デコーダー Transformer モデルです。"
  },
  {
    "question": "language",
    "chain": "generation",
    "answer": "The openai/whisper-base model can perform multilingual speech recognition, speech translation, and language identification across 75 languages.",
    "answer_Chinese": "openai/whisper-base 模型可以执行 75 种语言的多语言语音识别、语音翻译和语言识别。",
    "answer_Arabic": "يمكن لنموذج openai/whisper-base إجراء التعرف على الكلام متعدد اللغات وترجمة الكلام وتحديد اللغة عبر 75 لغة.",
    "answer_French": "Le modèle openai/whisper-base peut effectuer une reconnaissance vocale multilingue, une traduction vocale et une identification linguistique dans 75 langues.",
    "answer_Russian": "Модель openai/whisper-base может выполнять многоязычное распознавание речи, перевод речи и идентификацию языка на 75 языках.",
    "answer_Japanese": "openai/Whisper ベース モデルは、75 言語にわたる多言語音声認識、音声翻訳、および言語識別を実行できます。"
  },
  {
    "question": "license",
    "chain": "generation",
    "answer": "Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.",
    "answer_Chinese": "Whisper 的代码和模型权重是根据 MIT 许可证发布的。有关更多详细信息，请参阅 [许可证](https://github.com/openai/whisper/blob/main/LICENSE)。",
    "answer_Arabic": "تم إصدار كود Whisper وأوزان النموذج بموجب ترخيص MIT. راجع [الترخيص](https://github.com/openai/whisper/blob/main/LICENSE) لمزيد من التفاصيل.",
    "answer_French": "Le code et les poids des modèles de Whisper sont publiés sous la licence MIT. Voir [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) pour plus de détails.",
    "answer_Russian": "Код Whisper и веса моделей выпускаются под лицензией MIT. Дополнительную информацию см. в [ЛИЦЕНЗИИ](https://github.com/openai/whisper/blob/main/LICENSE).",
    "answer_Japanese": "Whisper のコードとモデルの重みは MIT ライセンスに基づいて公開されています。詳細については、[ライセンス](https://github.com/openai/whisper/blob/main/LICENSE) を参照してください。"
  },
  {
    "question": "finetuned_from",
    "chain": "generation",
    "answer": "Based on the provided references, the Whisper model does not appear to be fine-tuned from another model. The references describe Whisper as \"a general-purpose speech recognition model\" that is \"trained on a large dataset of diverse audio\" and leverages \"web-scale text from the internet for training.\" There is no mention of Whisper being fine-tuned from a pre-existing base model.\n\n[More Information Needed] to definitively state whether or not Whisper is fine-tuned from another model. The provided references do not contain enough information to make that determination.",
    "answer_Chinese": "根据提供的参考资料，Whisper 模型似乎并未根据其他模型进行微调。这些参考文献将 Whisper 描述为“一种通用语音识别模型”，“在不同音频的大型数据集上进行训练”，并利用“来自互联网的网络规模文本进行训练”。没有提到 Whisper 是根据现有的基础模型进行微调的。\n\n[需要更多信息] 明确说明 Whisper 是否是根据其他模型进行微调的。所提供的参考文献不包含足够的信息来做出该决定。",
    "answer_Arabic": "استناداً إلى المراجع المتوفرة، لا يبدو أن نموذج Whisper قد تم تحسينه من نموذج آخر. تصف المراجع Whisper بأنه \"نموذج للتعرف على الكلام للأغراض العامة\" والذي \"يتم تدريبه على مجموعة كبيرة من البيانات الصوتية المتنوعة\" ويستفيد من \"النص على نطاق الويب من الإنترنت للتدريب\". لا يوجد ذكر لتعديل Whisper من النموذج الأساسي الموجود مسبقًا.\n\n[بحاجة إلى مزيد من المعلومات] لتحديد ما إذا كان Whisper قد تم ضبطه بدقة من طراز آخر أم لا. لا تحتوي المراجع المقدمة على معلومات كافية لاتخاذ هذا القرار.",
    "answer_French": "Sur la base des références fournies, le modèle Whisper ne semble pas être affiné à partir d'un autre modèle. Les références décrivent Whisper comme « un modèle de reconnaissance vocale à usage général » qui est « formé sur un vaste ensemble de données audio diverses » et exploite « du texte à l'échelle du Web provenant d'Internet pour la formation ». Il n’y a aucune mention que Whisper soit affiné à partir d’un modèle de base préexistant.\n\n[Plus d'informations nécessaires] pour indiquer définitivement si Whisper est optimisé ou non à partir d'un autre modèle. Les références fournies ne contiennent pas suffisamment d’informations pour prendre cette décision.",
    "answer_Russian": "Судя по предоставленным ссылкам, модель Whisper не является точной настройкой другой модели. В ссылках Whisper описывается как «модель распознавания речи общего назначения», которая «обучена на большом наборе данных разнообразного аудио» и использует для обучения «текст в веб-масштабе из Интернета». Нет никаких упоминаний о том, что Whisper был доработан на основе уже существующей базовой модели.\n\n[Требуется дополнительная информация], чтобы окончательно определить, является ли Whisper точной настройкой из другой модели. Предоставленные ссылки не содержат достаточно информации для принятия такого решения.",
    "answer_Japanese": "提供された参照に基づくと、Whisper モデルは別のモデルから微調整されているようには見えません。参考文献では、Whisper について「多様な音声の大規模なデータセットでトレーニング」され、「トレーニング用にインターネットからの Web スケールのテキスト」を活用する「汎用音声認識モデル」と説明されています。 Whisper が既存のベースモデルから微調整されたことについては言及されていません。\n\nWhisper が別のモデルから微調整されているかどうかを明確にするには、[詳細情報が必要です]。提供された参考文献には、その判断を下すのに十分な情報が含まれていません。"
  },
  {
    "question": "demo_sources",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe references provided do not contain a direct link to a demo of the openai/whisper-base model. More information would be needed to definitively answer this question.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料不包含 openai/whisper-base 模型演示的直接链接。需要更多信息才能明确回答这个问题。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nالمراجع المقدمة لا تحتوي على رابط مباشر لعرض توضيحي لنموذج openai/whisper-base. ستكون هناك حاجة إلى مزيد من المعلومات للإجابة بشكل قاطع على هذا السؤال.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent pas de lien direct vers une démo du modèle openai/whisper-base. Plus d’informations seraient nécessaires pour répondre définitivement à cette question.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат прямых ссылок на демо-версию модели openai/whisper-base. Для окончательного ответа на этот вопрос потребуется больше информации.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、openai/whisper-base モデルのデモへの直接リンクは含まれていません。この質問に明確に答えるには、さらに詳しい情報が必要です。"
  },
  {
    "question": "direct_use",
    "chain": "generation",
    "answer": "The openai/whisper-base model can be used for speech recognition without any fine-tuning, post-processing or complex pipeline. It is designed as a general-purpose model that works well across many domains, tasks and languages out of the box.\n\nTo use the model, you can simply load it and call the transcribe() method with an audio file:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\") \nprint(result[\"text\"])\n```\n\nThis will read the audio file, process it using a sliding window, and output the transcribed text. \n\nThe model can also detect the spoken language automatically. To get more low-level control, you can use the detect_language() and decode() methods instead of transcribe().\n\nAdditionally, you can run the model from the command line to transcribe audio files:\n\nwhisper audio.flac audio.mp3 audio.wav --model base\n\nTo transcribe non-English speech, add the --language flag. And to translate the speech to English, add --task translate.\n\nSo in summary, the openai/whisper-base model is very flexible and easy to use for speech recognition tasks without requiring dataset-specific fine-tuning or complex post-processing steps. The sensible defaults make it suitable for many applications out-of-the-box.",
    "answer_Chinese": "openai/whisper-base 模型可用于语音识别，无需任何微调、后处理或​​复杂的管道。它被设计为通用模型，可以开箱即用地跨许多领域、任务和语言运行。\n\n要使用该模型，您只需加载它并使用音频文件调用 transcribe() 方法：\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\")\n结果 = model.transcribe(\"audio.mp3\") \n打印（结果[“文本”]）\n````\n\n这将读取音频文件，使用滑动窗口对其进行处理，并输出转录的文本。 \n\n该模型还可以自动检测口语。要获得更多低级控制，您可以使用 detector_language() 和decode() 方法而不是transcribe()。\n\n此外，您可以从命令行运行模型来转录音频文件：\n\n耳语音频.flac 音频.mp3 音频.wav --模型基础\n\n要转录非英语语音，请添加 --language 标志。要将演讲翻译成英语，请添加 --tasktranslate。\n\n总而言之，openai/whisper-base 模型非常灵活且易于用于语音识别任务，无需针对数据集进行微调或复杂的后处理步骤。合理的默认设置使其适合许多开箱即用的应用程序。",
    "answer_Arabic": "يمكن استخدام نموذج openai/whisper-base للتعرف على الكلام دون أي ضبط دقيق أو معالجة لاحقة أو مسار معقد. لقد تم تصميمه كنموذج للأغراض العامة يعمل بشكل جيد عبر العديد من المجالات والمهام واللغات خارج الصندوق.\n\nلاستخدام النموذج، يمكنك ببساطة تحميله واستدعاء التابع transcribe()‎ مع ملف صوتي:\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\")\nالنتيجة = model.transcribe(\"audio.mp3\") \nطباعة (نتيجة [\"نص\"])\n```\n\nسيؤدي هذا إلى قراءة الملف الصوتي ومعالجته باستخدام نافذة منزلقة وإخراج النص المكتوب. \n\nيمكن للنموذج أيضًا اكتشاف اللغة المنطوقة تلقائيًا. للحصول على مزيد من التحكم على المستوى المنخفض، يمكنك استخدام طريقتي Detect_language() وdecode() بدلاً من transcribe().\n\nبالإضافة إلى ذلك، يمكنك تشغيل النموذج من سطر الأوامر لنسخ الملفات الصوتية:\n\nWhisper audio.flac audio.mp3 audio.wav - قاعدة النموذج\n\nلنسخ الكلام غير الإنجليزي، أضف علامة --language. ولترجمة الخطاب إلى اللغة الإنجليزية، أضف --task ترجمة.\n\nباختصار، يعد نموذج openai/whisper-base مرنًا للغاية وسهل الاستخدام لمهام التعرف على الكلام دون الحاجة إلى ضبط دقيق لمجموعة بيانات معينة أو خطوات معقدة بعد المعالجة. الإعدادات الافتراضية المعقولة تجعلها مناسبة للعديد من التطبيقات خارج الصندوق.",
    "answer_French": "Le modèle openai/whisper-base peut être utilisé pour la reconnaissance vocale sans réglage fin, post-traitement ou pipeline complexe. Il est conçu comme un modèle à usage général qui fonctionne immédiatement dans de nombreux domaines, tâches et langages.\n\nPour utiliser le modèle, vous pouvez simplement le charger et appeler la méthode transcribe() avec un fichier audio :\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\")\nrésultat = modèle.transcribe(\"audio.mp3\") \nimprimer(résultat[\"texte\"])\n```\n\nCela lira le fichier audio, le traitera à l'aide d'une fenêtre coulissante et affichera le texte transcrit. \n\nLe modèle peut également détecter automatiquement la langue parlée. Pour obtenir plus de contrôle de bas niveau, vous pouvez utiliser les méthodes detector_langage() et decode() au lieu de transcribe().\n\nDe plus, vous pouvez exécuter le modèle à partir de la ligne de commande pour transcrire des fichiers audio :\n\nmurmurer audio.flac audio.mp3 audio.wav --modèle de base\n\nPour transcrire un discours non anglais, ajoutez l'indicateur --langue. Et pour traduire le discours en anglais, ajoutez --task Translate.\n\nEn résumé, le modèle openai/whisper-base est très flexible et facile à utiliser pour les tâches de reconnaissance vocale sans nécessiter un réglage précis spécifique à l'ensemble de données ni des étapes de post-traitement complexes. Les valeurs par défaut raisonnables le rendent adapté à de nombreuses applications prêtes à l'emploi.",
    "answer_Russian": "Модель openai/whisper-base можно использовать для распознавания речи без какой-либо тонкой настройки, постобработки или сложного конвейера. Он спроектирован как модель общего назначения, которая сразу же из коробки хорошо работает во многих областях, задачах и языках.\n\nЧтобы использовать модель, вы можете просто загрузить ее и вызвать метод transcribe() с аудиофайлом:\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\")\nрезультат = model.transcribe(\"audio.mp3\") \nпечать(результат[\"текст\"])\n```\n\nЭто прочитает аудиофайл, обработает его с помощью скользящего окна и выведет расшифрованный текст. \n\nМодель также может автоматически определять разговорный язык. Чтобы получить более низкоуровневый контроль, вы можете использовать методыDetect_language() и decode() вместо transcribe().\n\nКроме того, вы можете запустить модель из командной строки для расшифровки аудиофайлов:\n\nшепот audio.flac audio.mp3 audio.wav --базовая модель\n\nЧтобы транскрибировать неанглийскую речь, добавьте флаг --language. А чтобы перевести речь на английский, добавьте --task Translate.\n\nПодводя итог, можно сказать, что модель openai/whisper-base очень гибкая и простая в использовании для задач распознавания речи, не требуя тонкой настройки для конкретного набора данных или сложных шагов постобработки. Разумные настройки по умолчанию делают его подходящим для многих приложений без дополнительной настройки.",
    "answer_Japanese": "openai/whisper ベース モデルは、微調整、後処理、複雑なパイプラインを必要とせずに音声認識に使用できます。これは、多くのドメイン、タスク、言語でそのまま使用できる汎用モデルとして設計されています。\n\nモデルを使用するには、モデルをロードし、オーディオ ファイルを使用して transcribe() メソッドを呼び出すだけです。\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\")\nresult = model.transcribe(\"audio.mp3\") \nprint(結果[\"テキスト\"])\n「」\n\nこれにより、音声ファイルが読み取られ、スライディング ウィンドウを使用して処理され、文字起こしされたテキストが出力されます。 \n\nこのモデルは、話し言葉を自動的に検出することもできます。さらに低レベルの制御を行うには、transcribe() の代わりに detect_ language() メソッドと decode() メソッドを使用できます。\n\nさらに、コマンド ラインからモデルを実行してオーディオ ファイルを転写することもできます。\n\nささやき audio.flac audio.mp3 audio.wav --モデル ベース\n\n英語以外の音声を文字に起こすには、-- language フラグを追加します。スピーチを英語に翻訳するには、 --task translation を追加します。\n\n要約すると、openai/whisper ベース モデルは非常に柔軟で、データセット固有の微調整や複雑な後処理手順を必要とせずに、音声認識タスクに簡単に使用できます。適切なデフォルト設定により、すぐに使用できる多くのアプリケーションに適しています。"
  },
  {
    "question": "downstream_use",
    "chain": "generation",
    "answer": "The openai/whisper-base model can be fine-tuned or integrated into a larger system in several ways:\n\n1. Fine-tuning for improved performance on specific tasks like long-form transcription, by training on a high-quality supervised dataset and/or using reinforcement learning. This could help reduce errors like repeat loops, missing first/last words, or hallucination. [More Information Needed] for code snippet.\n\n2. Integrating into a speech processing pipeline as a single model that performs the entire pipeline, rather than just the core speech recognition. This simplifies the system architecture. The model uses a simple token format to specify tasks like transcription, translation, voice activity detection, alignment, and language identification. [More Information Needed] for code snippet.\n\n3. Using the model's lower-level `whisper.detect_language()` and `whisper.decode()` methods to incorporate it into custom applications, in addition to the higher-level `transcribe()` method. Here's an example of using the model directly in Python:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\") \nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\n4. Leveraging the model's multitasking abilities (multilingual speech recognition, speech translation, language identification) to enable rich functionality in applications. [More Information Needed] for code snippet.\n\nIn summary, the openai/whisper-base model provides flexibility to improve performance via fine-tuning, simplify speech processing pipelines, access low-level model functionality for custom use cases, and utilize its multitasking capabilities. More specific code snippets would depend on the particular integration or fine-tuning approach.",
    "answer_Chinese": "openai/whisper-base 模型可以通过多种方式进行微调或集成到更大的系统中：\n\n1. 通过在高质量监督数据集上进行训练和/或使用强化学习，进行微调以提高特定任务（例如长格式转录）的性能。这有助于减少重复循环、缺少第一个/最后一个单词或幻觉等错误。 [需要更多信息] 代码片段。\n\n2. 作为执行整个管道（而不仅仅是核心语音识别）的单个模型集成到语音处理管道中。这简化了系统架构。该模型使用简单的标记格式来指定转录、翻译、语音活动检测、对齐和语言识别等任务。 [需要更多信息] 代码片段。\n\n3. 除了较高级别的“transcribe()”方法之外，还使用模型的较低级别“whisper.detect_language()”和“whisper.decode()”方法将其合并到自定义应用程序中。下面是直接在 Python 中使用模型的示例：\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\") \n结果 = model.transcribe(\"audio.mp3\")\n打印（结果[“文本”]）\n````\n\n4. 利用模型的多任务处理能力（多语言语音识别、语音翻译、语言识别）在应用程序中实现丰富的功能。 [需要更多信息] 代码片段。\n\n总之，openai/whisper-base 模型提供了灵活性，可以通过微调提高性能、简化语音处理管道、访问自定义用例的低级模型功能以及利用其多任务处理功能。更具体的代码片段将取决于特定的集成或微调方法。",
    "answer_Arabic": "يمكن ضبط نموذج openai/whisper-base أو دمجه في نظام أكبر بعدة طرق:\n\n1. الضبط الدقيق لتحسين الأداء في مهام محددة مثل النسخ الطويل، من خلال التدريب على مجموعة بيانات عالية الجودة خاضعة للإشراف و/أو استخدام التعلم المعزز. يمكن أن يساعد هذا في تقليل الأخطاء مثل تكرار الحلقات أو فقدان الكلمات الأولى/الأخيرة أو الهلوسة. [مطلوب مزيد من المعلومات] لمقتطف الشفرة.\n\n2. التكامل في مسار معالجة الكلام كنموذج واحد ينفذ المسار بأكمله، بدلاً من مجرد التعرف على الكلام الأساسي. وهذا يبسط بنية النظام. يستخدم النموذج تنسيق رمزي بسيط لتحديد مهام مثل النسخ والترجمة واكتشاف النشاط الصوتي والمحاذاة وتحديد اللغة. [مطلوب مزيد من المعلومات] لمقتطف الشفرة.\n\n3. استخدام طريقتي `whisper.detect_language()` و`whisper.decode()` ذات المستوى الأدنى للنموذج لدمجها في تطبيقات مخصصة، بالإضافة إلى طريقة `transcribe()` ذات المستوى الأعلى. فيما يلي مثال لاستخدام النموذج مباشرة في بايثون:\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\") \nالنتيجة = model.transcribe(\"audio.mp3\")\nطباعة (نتيجة [\"نص\"])\n```\n\n4. الاستفادة من قدرات تعدد المهام للنموذج (التعرف على الكلام متعدد اللغات، وترجمة الكلام، وتحديد اللغة) لتمكين الوظائف الغنية في التطبيقات. [مطلوب مزيد من المعلومات] لمقتطف الشفرة.\n\nباختصار، يوفر نموذج openai/whisper-base المرونة لتحسين الأداء من خلال الضبط الدقيق، وتبسيط مسارات معالجة الكلام، والوصول إلى وظائف النموذج ذات المستوى المنخفض لحالات الاستخدام المخصصة، والاستفادة من قدراته في المهام المتعددة. تعتمد مقتطفات التعليمات البرمجية الأكثر تحديدًا على أسلوب التكامل أو الضبط الدقيق.",
    "answer_French": "Le modèle openai/whisper-base peut être affiné ou intégré dans un système plus vaste de plusieurs manières :\n\n1. Ajustement pour améliorer les performances sur des tâches spécifiques telles que la transcription longue, en s'entraînant sur un ensemble de données supervisé de haute qualité et/ou en utilisant l'apprentissage par renforcement. Cela pourrait aider à réduire les erreurs telles que les boucles répétées, les premiers/derniers mots manquants ou les hallucinations. [Plus d'informations nécessaires] pour l'extrait de code.\n\n2. Intégration dans un pipeline de traitement vocal en tant que modèle unique qui exécute l'ensemble du pipeline, plutôt que seulement la reconnaissance vocale de base. Cela simplifie l'architecture du système. Le modèle utilise un format de jeton simple pour spécifier des tâches telles que la transcription, la traduction, la détection de l'activité vocale, l'alignement et l'identification de la langue. [Plus d'informations nécessaires] pour l'extrait de code.\n\n3. Utiliser les méthodes `whisper.detect_lingual()` et `whisper.decode()` de niveau inférieur du modèle pour l'incorporer dans des applications personnalisées, en plus de la méthode `transcribe()` de niveau supérieur. Voici un exemple d'utilisation du modèle directement en Python :\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\") \nrésultat = modèle.transcribe(\"audio.mp3\")\nimprimer(résultat[\"texte\"])\n```\n\n4. Tirer parti des capacités multitâches du modèle (reconnaissance vocale multilingue, traduction vocale, identification de la langue) pour permettre des fonctionnalités riches dans les applications. [Plus d'informations nécessaires] pour l'extrait de code.\n\nEn résumé, le modèle openai/whisper-base offre la flexibilité nécessaire pour améliorer les performances via un réglage fin, simplifier les pipelines de traitement vocal, accéder aux fonctionnalités du modèle de bas niveau pour des cas d'utilisation personnalisés et utiliser ses capacités multitâches. Des extraits de code plus spécifiques dépendront de l’approche d’intégration ou de réglage particulière.",
    "answer_Russian": "Модель openai/whisper-base можно доработать или интегрировать в более крупную систему несколькими способами:\n\n1. Точная настройка для повышения производительности при выполнении конкретных задач, таких как расшифровка длинных форм, путем обучения на высококачественном контролируемом наборе данных и/или использования обучения с подкреплением. Это может помочь уменьшить количество ошибок, таких как повторы, пропуск первого/последнего слова или галлюцинации. [Требуется дополнительная информация] для фрагмента кода.\n\n2. Интеграция в конвейер обработки речи как единая модель, выполняющая весь конвейер, а не только основное распознавание речи. Это упрощает архитектуру системы. Модель использует простой формат токена для задания таких задач, как транскрипция, перевод, обнаружение голосовой активности, выравнивание и идентификация языка. [Требуется дополнительная информация] для фрагмента кода.\n\n3. Использование методов нижнего уровня модели `whisper.detect_language()` и `whisper.decode()` для включения ее в пользовательские приложения в дополнение к методу `transcribe()` более высокого уровня. Вот пример использования модели непосредственно в Python:\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\") \nрезультат = model.transcribe(\"audio.mp3\")\nпечать(результат[\"текст\"])\n```\n\n4. Использование многозадачных возможностей модели (многоязычное распознавание речи, перевод речи, идентификация языка) для обеспечения богатой функциональности приложений. [Требуется дополнительная информация] для фрагмента кода.\n\nТаким образом, модель openai/whisper-base обеспечивает гибкость для повышения производительности за счет тонкой настройки, упрощения конвейеров обработки речи, доступа к низкоуровневым функциям модели для пользовательских сценариев использования и использования ее многозадачных возможностей. Более конкретные фрагменты кода будут зависеть от конкретного подхода к интеграции или тонкой настройке.",
    "answer_Japanese": "openai/whisper ベース モデルは、いくつかの方法で微調整したり、より大きなシステムに統合したりできます。\n\n1. 高品質の教師ありデータセットでのトレーニングや強化学習の使用により、長文の書き起こしなどの特定のタスクのパフォーマンスを向上させるための微調整。これにより、ループの繰り返し、最初と最後の単語の欠落、幻覚などのエラーを減らすことができます。コード スニペットについては [詳細情報が必要です]。\n\n2. コアの音声認識だけではなく、パイプライン全体を実行する単一のモデルとして音声処理パイプラインに統合します。これにより、システム アーキテクチャが簡素化されます。このモデルは、単純なトークン形式を使用して、文字起こし、翻訳、音声アクティビティの検出、位置合わせ、言語識別などのタスクを指定します。コード スニペットについては [詳細情報が必要です]。\n\n3. 高レベルの `transcribe()` メソッドに加えて、モデルの下位レベルの `whisper.detect_langage()` および `whisper.decode()` メソッドを使用してカスタム アプリケーションに組み込む。 Python でモデルを直接使用する例を次に示します。\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\") \nresult = model.transcribe(\"audio.mp3\")\nprint(結果[\"テキスト\"])\n「」\n\n4. モデルのマルチタスク機能 (多言語音声認識、音声翻訳、言語識別) を活用して、アプリケーションで豊富な機能を有効にします。コード スニペットについては [詳細情報が必要です]。\n\n要約すると、openai/whisper ベース モデルは、微調整によるパフォーマンスの向上、音声処理パイプラインの簡素化、カスタム ユース ケース向けの低レベル モデル機能へのアクセス、およびそのマルチタスク機能の利用を実現する柔軟性を提供します。より具体的なコード スニペットは、特定の統合または微調整のアプローチによって異なります。"
  },
  {
    "question": "out_of_scope_use",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here are some potential ways the model could be misused and what users should avoid doing:\n\n- Users should not rely on Whisper for transcribing sensitive information like names or personal details, as the model has a tendency to make plausible but incorrect guesses for names of speakers based on its training data.\n\n- Whisper should not be used in high-stakes domains requiring near-perfect accuracy, as it still makes some stubborn errors in long-form transcription like getting stuck in loops or hallucinating unrelated text. Further work is needed to reduce these failure modes. \n\n- Users should be cautious about applying Whisper to low-resource languages, as the model's performance is still quite poor for many languages that had less training data. Increasing language-specific training data is an important direction for improving the model.\n\n- As a general-purpose model, Whisper should not be used without further testing and potential fine-tuning for specialized domains where high-quality supervised data exists, as this could yield better performance than the base model.\n\n[More Information Needed] on other specific ways the model could be misapplied or abused given its current capabilities and limitations. A deeper analysis of potential societal impacts and failure modes would be beneficial to include.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，以下是该模型可能被滥用的一些潜在方式以及用户应避免执行的操作：\n\n- 用户不应依赖 Whisper 来转录姓名或个人详细信息等敏感信息，因为该模型倾向于根据其训练数据对说话者姓名做出看似合理但不正确的猜测。\n\n- Whisper 不应用于需要近乎完美准确性的高风险领域，因为它在长格式转录中仍然会出现一些顽固错误，例如陷入循环或产生不相关文本的幻觉。需要进一步的工作来减少这些故障模式。 \n\n- 用户在将 Whisper 应用于低资源语言时应谨慎，因为对于许多训练数据较少的语言来说，该模型的性能仍然相当差。增加特定语言的训练数据是改进模型的重要方向。\n\n- 作为通用模型，在没有对存在高质量监督数据的专门领域进行进一步测试和潜在微调的情况下，不应使用 Whisper，因为这可能会产生比基本模型更好的性能。\n\n[需要更多信息]考虑到模型当前的功能和局限性，该模型可能被误用或滥用的其他具体方式。对潜在社会影响和故障模式进行更深入的分析将是有益的。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج Whisper، إليك بعض الطرق المحتملة لإساءة استخدام النموذج وما يجب على المستخدمين تجنب القيام به:\n\n- يجب ألا يعتمد المستخدمون على Whisper لنقل المعلومات الحساسة مثل الأسماء أو التفاصيل الشخصية، حيث يميل النموذج إلى تقديم تخمينات معقولة ولكنها غير صحيحة لأسماء المتحدثين بناءً على بيانات التدريب الخاصة به.\n\n- لا ينبغي استخدام Whisper في المجالات عالية المخاطر التي تتطلب دقة شبه مثالية، لأنه لا يزال يرتكب بعض الأخطاء العنيدة في النسخ الطويل مثل التورط في الحلقات أو هلوسة النص غير ذي الصلة. هناك حاجة إلى مزيد من العمل للحد من أوضاع الفشل هذه. \n\n- يجب على المستخدمين توخي الحذر بشأن تطبيق Whisper على اللغات منخفضة الموارد، حيث لا يزال أداء النموذج ضعيفًا جدًا بالنسبة للعديد من اللغات التي تحتوي على بيانات تدريب أقل. تعد زيادة بيانات التدريب الخاصة باللغة اتجاهًا مهمًا لتحسين النموذج.\n\n- كنموذج للأغراض العامة، لا ينبغي استخدام Whisper دون إجراء المزيد من الاختبارات والضبط الدقيق المحتمل للمجالات المتخصصة حيث توجد بيانات عالية الجودة خاضعة للإشراف، لأن هذا قد يؤدي إلى أداء أفضل من النموذج الأساسي.\n\n[بحاجة إلى مزيد من المعلومات] حول الطرق المحددة الأخرى التي يمكن من خلالها إساءة تطبيق النموذج أو إساءة استخدامه نظرًا لقدراته وقيوده الحالية. سيكون من المفيد تضمين تحليل أعمق للتأثيرات المجتمعية المحتملة وأنماط الفشل.",
    "answer_French": "Sur la base des références fournies sur le modèle Whisper, voici quelques façons potentielles dont le modèle pourrait être utilisé à mauvais escient et ce que les utilisateurs devraient éviter de faire :\n\n- Les utilisateurs ne doivent pas compter sur Whisper pour transcrire des informations sensibles telles que des noms ou des détails personnels, car le modèle a tendance à faire des suppositions plausibles mais incorrectes sur les noms des locuteurs en fonction de ses données de formation.\n\n- Whisper ne doit pas être utilisé dans des domaines à enjeux élevés nécessitant une précision presque parfaite, car il provoque toujours des erreurs tenaces dans la transcription longue, comme rester coincé dans des boucles ou halluciner un texte sans rapport. Des travaux supplémentaires sont nécessaires pour réduire ces modes de défaillance. \n\n- Les utilisateurs doivent être prudents lorsqu'ils appliquent Whisper à des langages à faibles ressources, car les performances du modèle sont encore assez médiocres pour de nombreux langages disposant de moins de données de formation. L’augmentation des données de formation spécifiques à une langue est une direction importante pour améliorer le modèle.\n\n- En tant que modèle à usage général, Whisper ne doit pas être utilisé sans des tests supplémentaires et des ajustements potentiels pour les domaines spécialisés où existent des données supervisées de haute qualité, car cela pourrait donner de meilleures performances que le modèle de base.\n\n[Plus d'informations nécessaires] sur d'autres façons spécifiques dont le modèle pourrait être mal appliqué ou abusé compte tenu de ses capacités et limites actuelles. Il serait utile d’inclure une analyse plus approfondie des impacts sociétaux potentiels et des modes de défaillance.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель Whisper, вот несколько потенциальных способов неправильного использования модели и того, чего пользователям следует избегать:\n\n- Пользователям не следует полагаться на Whisper для расшифровки конфиденциальной информации, такой как имена или личные данные, поскольку модель имеет тенденцию делать правдоподобные, но неверные предположения об именах говорящих на основе данных обучения.\n\n- Whisper не следует использовать в областях с высокими ставками, требующих почти идеальной точности, поскольку он по-прежнему допускает некоторые стойкие ошибки в длинной транскрипции, такие как зацикливание или галлюцинации несвязанного текста. Необходима дальнейшая работа по уменьшению этих видов отказов. \n\n- Пользователям следует с осторожностью применять Whisper к языкам с низким уровнем ресурсов, поскольку производительность модели по-прежнему довольно низкая для многих языков, у которых было меньше обучающих данных. Увеличение объемов данных обучения по конкретному языку является важным направлением улучшения модели.\n\n- В качестве модели общего назначения Whisper не следует использовать без дальнейшего тестирования и потенциальной тонкой настройки для специализированных областей, где существуют высококачественные контролируемые данные, поскольку это может обеспечить более высокую производительность, чем базовая модель.\n\n[Требуется дополнительная информация] о других конкретных способах неправильного применения или злоупотребления моделью, учитывая ее текущие возможности и ограничения. Было бы полезно включить более глубокий анализ потенциальных социальных последствий и видов неудач.",
    "answer_Japanese": "Whisper モデルに関して提供されている参考資料に基づいて、モデルが悪用される可能性のある潜在的な方法と、ユーザーが避けるべきことをいくつか示します。\n\n- このモデルは、トレーニング データに基づいて話者の名前について、もっともらしいが不正確な推測を行う傾向があるため、ユーザーは、名前や個人情報などの機密情報の文字起こしに Whisper に依存しないでください。\n\n- Whisper は、ループにはまり込んだり、無関係なテキストが幻覚を見せたりするなど、長文の転写において依然として頑固なエラーが発生するため、ほぼ完璧な精度を必要とする一か八かの分野では使用しないでください。これらの故障モードを減らすには、さらなる取り組みが必要です。 \n\n- トレーニング データが少ない多くの言語ではモデルのパフォーマンスが依然として非常に低いため、ユーザーは低リソース言語に Whisper を適用することに注意する必要があります。言語固有のトレーニング データを増やすことは、モデルを改善するための重要な方向性です。\n\n- 汎用モデルとして、Whisper は、基本モデルよりも優れたパフォーマンスが得られる可能性があるため、高品質の教師ありデータが存在する特殊なドメインに対してさらなるテストと潜在的な微調整を行わずに使用するべきではありません。\n\n現在の機能と制限を考慮すると、モデルが誤って適用または悪用される可能性があるその他の具体的な方法については、[さらなる情報が必要]。潜在的な社会的影響と障害モードについてのより深い分析を含めることは有益です。"
  },
  {
    "question": "bias_risks_limitations",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here are some known or foreseeable issues, limitations, and potential harms:\n\nTechnical Limitations:\n1. Poor performance on low-resource languages due to English-centric training data\n2. Remaining errors in long-form transcription, such as getting stuck in repeat loops, missing first/last words, or hallucinating unrelated transcripts\n3. Diminishing returns in English speech recognition performance as model size increases, possibly due to approaching human-level saturation\n\nSociotechnical Limitations:\n1. Tendency to transcribe plausible but incorrect speaker names, as this information is rarely inferable from the audio alone\n2. [More Information Needed] on potential biases or fairness issues arising from the English-centric training data\n\nPotential Misunderstandings:\n1. Users may overestimate the model's ability to accurately transcribe low-resource languages or infer speaker identity\n2. [More Information Needed] on how users might misinterpret or misuse the model's outputs\n\nForeseeable Harms:\n1. [More Information Needed] on potential privacy risks from transcribing sensitive audio data\n2. [More Information Needed] on how transcription errors or hallucinations could lead to harmful consequences in high-stakes applications\n\nTo address these issues, the references suggest:\n1. Increasing training data for low-resource languages \n2. Studying the impact of encoder/decoder design choices and fine-tuning\n3. Improving decoding strategies to reduce errors\n4. Releasing text normalization code for better comparison and out-of-distribution testing\n\nHowever, more information is needed to fully assess potential biases, misunderstandings, privacy risks, and high-stakes harms. Ongoing testing, monitoring, and stakeholder engagement will be important for responsible development and deployment.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，以下是一些已知或可预见的问题、限制和潜在危害：\n\n技术限制：\n1. 由于以英语为中心的训练数据，在资源匮乏的语言上表现不佳\n2. 长格式转录中的剩余错误，例如陷入重复循环、缺少第一个/最后一个单词或产生不相关转录的幻觉\n3. 随着模型大小的增加，英语语音识别性能的回报递减，这可能是由于接近人类水平的饱和度\n\n社会技术限制：\n1. 倾向于转录看似合理但不正确的说话者姓名，因为仅从音频中很难推断出此信息\n2. [需要更多信息]关于以英语为中心的训练数据引起的潜在偏见或公平问题\n\n潜在的误解：\n1. 用户可能高估了模型准确转录低资源语言或推断说话人身份的能力\n2. [需要更多信息]关于用户可能如何误解或滥用模型的输出\n\n可预见的危害：\n1. [需要更多信息]有关转录敏感音频数据的潜在隐私风险\n2. [需要更多信息]关于转录错误或幻觉如何在高风险应用中导致有害后果\n\n为了解决这些问题，参考文献建议：\n1. 增加资源匮乏语言的训练数据 \n2. 研究编码器/解码器设计选择和微调的影响\n3. 改进解码策略以减少错误\n4. 发布文本规范化代码以进行更好的比较和分布外测试\n\n然而，需要更多信息来充分评估潜在的偏见、误解、隐私风险和高风险伤害。持续的测试、监控和利益相关者的参与对于负责任的开发和部署非常重要。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج Whisper، إليك بعض المشكلات والقيود والأضرار المحتملة المعروفة أو المتوقعة:\n\nالقيود الفنية:\n1. ضعف الأداء في اللغات منخفضة الموارد بسبب بيانات التدريب التي تركز على اللغة الإنجليزية\n2. الأخطاء المتبقية في النسخ الطويل، مثل الوقوع في حلقات متكررة، أو فقدان الكلمات الأولى/الأخيرة، أو هلوسة النصوص غير ذات الصلة\n3. انخفاض العائدات في أداء التعرف على الكلام باللغة الإنجليزية مع زيادة حجم النموذج، ربما بسبب الاقتراب من التشبع على المستوى البشري\n\nالقيود الاجتماعية التقنية:\n1. الميل إلى تدوين أسماء المتحدثين المعقولة ولكن غير الصحيحة، حيث نادرًا ما يمكن استنتاج هذه المعلومات من الصوت وحده\n2. [بحاجة إلى مزيد من المعلومات] حول التحيزات المحتملة أو قضايا العدالة الناشئة عن بيانات التدريب التي تركز على اللغة الإنجليزية\n\nسوء الفهم المحتمل:\n1. قد يبالغ المستخدمون في تقدير قدرة النموذج على نسخ اللغات منخفضة الموارد بدقة أو استنتاج هوية المتحدث\n2. [مطلوب مزيد من المعلومات] حول كيفية إساءة تفسير المستخدمين لمخرجات النموذج أو إساءة استخدامها\n\nالأضرار المتوقعة:\n1. [مطلوب مزيد من المعلومات] حول مخاطر الخصوصية المحتملة الناتجة عن نسخ البيانات الصوتية الحساسة\n2. [بحاجة إلى مزيد من المعلومات] حول كيف يمكن أن تؤدي أخطاء النسخ أو الهلوسة إلى عواقب ضارة في التطبيقات عالية المخاطر\n\nولمعالجة هذه القضايا تقترح المراجع ما يلي:\n1. زيادة بيانات التدريب للغات منخفضة الموارد \n2. دراسة تأثير اختيارات تصميم التشفير/فك التشفير وضبطها\n3. تحسين استراتيجيات فك التشفير لتقليل الأخطاء\n4. إطلاق كود تطبيع النص لمقارنة أفضل واختبار خارج التوزيع\n\nومع ذلك، هناك حاجة إلى مزيد من المعلومات لإجراء تقييم كامل للتحيزات المحتملة وسوء الفهم ومخاطر الخصوصية والأضرار عالية المخاطر. سيكون الاختبار المستمر والمراقبة وإشراك أصحاب المصلحة أمرًا مهمًا للتطوير والنشر المسؤول.",
    "answer_French": "Sur la base des références fournies concernant le modèle Whisper, voici quelques problèmes, limitations et préjudices potentiels connus ou prévisibles :\n\nLimites techniques :\n1. Mauvaises performances sur les langues à faibles ressources en raison de données de formation centrées sur l'anglais\n2. Erreurs restantes dans la transcription longue, comme rester bloqué dans des boucles de répétition, manquer le premier/le dernier mot ou avoir des transcriptions hallucinantes sans rapport\n3. Diminution des performances de reconnaissance vocale en anglais à mesure que la taille du modèle augmente, probablement en raison d'une saturation proche du niveau humain\n\nLimites sociotechniques :\n1. Tendance à transcrire les noms des locuteurs plausibles mais incorrects, car ces informations sont rarement déductibles à partir de l'audio seul\n2. [Plus d'informations nécessaires] sur les biais potentiels ou les problèmes d'équité découlant des données de formation centrées sur l'anglais\n\nMalentendus potentiels :\n1. Les utilisateurs peuvent surestimer la capacité du modèle à transcrire avec précision des langues à faibles ressources ou à déduire l'identité du locuteur\n2. [Plus d'informations nécessaires] sur la manière dont les utilisateurs pourraient mal interpréter ou utiliser à mauvais escient les résultats du modèle\n\nDommages prévisibles :\n1. [Plus d'informations nécessaires] sur les risques potentiels pour la vie privée liés à la transcription de données audio sensibles\n2. [Plus d'informations nécessaires] sur la façon dont les erreurs de transcription ou les hallucinations pourraient entraîner des conséquences néfastes dans les candidatures à enjeux élevés\n\nPour résoudre ces problèmes, les références suggèrent :\n1. Augmenter les données de formation pour les langues à faibles ressources \n2. Étudier l’impact des choix de conception et des réglages des encodeurs/décodeurs\n3. Améliorer les stratégies de décodage pour réduire les erreurs\n4. Publication du code de normalisation du texte pour une meilleure comparaison et des tests hors distribution\n\nCependant, davantage d’informations sont nécessaires pour évaluer pleinement les biais potentiels, les malentendus, les risques pour la vie privée et les préjudices à enjeux élevés. Les tests, la surveillance et l'engagement continus des parties prenantes seront importants pour un développement et un déploiement responsables.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель Whisper, вот некоторые известные или прогнозируемые проблемы, ограничения и потенциальный вред:\n\nТехнические ограничения:\n1. Плохая успеваемость по языкам с ограниченными ресурсами из-за данных об обучении, ориентированных на английский язык.\n2. Остающиеся ошибки в длинной транскрипции, такие как застревание в повторяющихся циклах, пропуск первого/последнего слова или галлюцинации несвязанных транскриптов.\n3. Уменьшение эффективности распознавания английской речи по мере увеличения размера модели, возможно, из-за приближения к насыщению на человеческом уровне.\n\nСоциотехнические ограничения:\n1. Склонность записывать правдоподобные, но неправильные имена говорящих, поскольку эту информацию редко можно вывести только по звуку.\n2. [Требуется дополнительная информация] о потенциальных предубеждениях или проблемах справедливости, возникающих из данных об обучении, ориентированном на английский язык.\n\nВозможные недоразумения:\n1. Пользователи могут переоценить способность модели точно расшифровывать языки с ограниченными ресурсами или определять личность говорящего.\n2. [Необходима дополнительная информация] о том, как пользователи могут неправильно интерпретировать или неправильно использовать результаты модели.\n\nПредсказуемый вред:\n1. [Требуется дополнительная информация] о потенциальных рисках для конфиденциальности, связанных с расшифровкой конфиденциальных аудиоданных.\n2. [Требуется дополнительная информация] о том, как ошибки транскрипции или галлюцинации могут привести к вредным последствиям в приложениях с высокими ставками.\n\nДля решения этих проблем в ссылках предлагается:\n1. Увеличение объема данных по обучению малоресурсным языкам \n2. Изучение влияния выбора конструкции кодера/декодера и его точная настройка.\n3. Улучшение стратегий декодирования для уменьшения ошибок.\n4. Выпуск кода нормализации текста для лучшего сравнения и тестирования вне дистрибутива.\n\nОднако необходимо больше информации, чтобы полностью оценить потенциальные предубеждения, недопонимания, риски для конфиденциальности и серьезный ущерб. Постоянное тестирование, мониторинг и привлечение заинтересованных сторон будут иметь важное значение для ответственной разработки и внедрения.",
    "answer_Japanese": "Whisper モデルに関して提供されている参考資料に基づいて、既知または予測可能な問題、制限、および潜在的な害をいくつか示します。\n\n技術的な制限:\n1. 英語中心のトレーニングデータのため、リソースの少ない言語ではパフォーマンスが低い\n2. 長い形式の書き起こしに残っているエラー（繰り返しループにはまり込む、最初と最後の単語が欠落している、または無関係な書き起こしが幻覚に見えるなど）\n3. モデルサイズが大きくなるにつれて、英語音声認識パフォーマンスの利益が減少します。おそらく人間レベルの飽和に近づいているためです。\n\n社会技術的な限界:\n1. この情報が音声だけから推測できることはほとんどないため、もっともらしいが不正確な話者名を書き写す傾向\n2. 英語中心のトレーニングデータから生じる潜在的なバイアスや公平性の問題に関する[さらなる情報が必要]\n\n潜在的な誤解:\n1. ユーザーは、リソースの少ない言語を正確に転写したり、話者の身元を推測したりするモデルの能力を過大評価する可能性があります。\n2. ユーザーがモデルの出力をどのように誤解または悪用する可能性があるかに関する [さらなる情報が必要]\n\n予見可能な危害:\n1. 機密音声データの転写による潜在的なプライバシー リスクに関する [さらなる情報が必要]\n2. 転記エラーや幻覚が、一か八かの出願においてどのように有害な結果を引き起こす可能性があるかについて[さらなる情報が必要]\n\nこれらの問題に対処するために、参考文献では次のことが提案されています。\n1. リソースの少ない言語の学習データを増やす \n2. エンコーダ/デコーダ設計の選択と微調整の影響の調査\n3. エラーを減らすためのデコード戦略の改善\n4. 比較と配布外テストを改善するためのテキスト正規化コードのリリース\n\nただし、潜在的な偏見、誤解、プライバシーのリスク、一か八かの損害を完全​​に評価するには、より多くの情報が必要です。責任ある開発と導入には、継続的なテスト、監視、関係者の関与が重要です。"
  },
  {
    "question": "bias_recommendations",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper speech recognition model, here are some recommendations regarding foreseeable issues with the openai/whisper-base model:\n\n1. Performance on low-resource languages could be improved by increasing the amount of training data for those languages, as the current dataset is English-heavy. More diverse data collection is needed.\n\n2. [More Information Needed] on the specific contributions of the encoder vs. decoder to Whisper's robustness. Ablation studies or experiments with other encoders could provide insights.\n\n3. Decoding errors like looping, skipping words, or hallucination are still issues, especially on long-form transcription. Fine-tuning on high-quality datasets or using reinforcement learning techniques may help reduce these.\n\n4. [More Information Needed] on whether incorporating unsupervised pre-training or self-teaching methods used in other state-of-the-art models could further improve Whisper's performance.\n\n5. Fine-tuning Whisper on specific domains where supervised data is available is likely to boost results and enable direct comparisons to other models. More studies are needed on fine-tuning.\n\n6. The model has a tendency to make incorrect guesses about speaker names, as this information is rarely inferable from the audio alone. [More Information Needed] on mitigation strategies.\n\n7. While performance continues to scale with model size in most tasks, diminishing returns are seen for English ASR, possibly due to saturation as human-level performance is approached. Analyzing scaling trends can inform efficient model sizing.\n\n8. Whisper aims to be a fully-featured recognition system, but some components like voice activity detection, speaker diarization and inverse text normalization may still need to be handled separately. [More Information Needed] on Whisper's capabilities in these areas and opportunities for a more integrated system.\n\nIn summary, key issues include low-resource language performance, decoding errors, analyzing the impact of different architectural components and training techniques, studying fine-tuning behavior, mitigating incorrect speaker labeling, model scaling efficiency, and expanding Whisper's capabilities as an integrated speech recognition system. Collecting more diverse data, conducting ablation studies, and exploring techniques used in other state-of-the-art models could help address these issues.",
    "answer_Chinese": "根据提供的有关 Whisper 语音识别模型的参考资料，以下是有关 openai/whisper-base 模型可预见问题的一些建议：\n\n1. 由于当前数据集以英语为主，因此可以通过增加这些语言的训练数据量来提高这些语言的性能。需要更多样化的数据收集。\n\n2. [需要更多信息] 编码器与解码器对 Whisper 鲁棒性的具体贡献。消融研究或其他编码器的实验可以提供见解。\n\n3. 诸如循环、跳词或幻觉之类的解码错误仍然是问题，特别是在长格式转录方面。对高质量数据集进行微调或使用强化学习技术可能有助于减少这些问题。\n\n4. [需要更多信息] 结合其他最先进模型中使用的无监督预训练或自学方法是否可以进一步提高 Whisper 的性能。\n\n5. 在有监督数据的特定领域对 Whisper 进行微调可能会提高结果并能够与其他模型进行直接比较。需要更多的微调研究。\n\n6. 该模型倾向于对说话者姓名做出错误的猜测，因为仅从音频中很难推断出此信息。 [需要更多信息]有关缓解策略。\n\n7. 虽然在大多数任务中，性能随着模型大小的变化而不断扩大，但英语 ASR 的回报却在递减，这可能是由于接近人类水平性能时的饱和所致。分析缩放趋势可以为有效的模型大小调整提供信息。\n\n8. Whisper 的目标是成为一个功能齐全的识别系统，但语音活动检测、说话人二值化和逆文本标准化等一些组件可能仍需要单独处理。 [需要更多信息] 关于 Whisper 在这些领域的能力以及更加集成的系统的机会。\n\n总之，关键问题包括低资源语言性能、解码错误、分析不同架构组件和训练技术的影响、研究微调行为、减少不正确的说话人标记、模型扩展效率以及扩展 Whisper 作为集成语音识别的功能系统。收集更多样化的数据、进行消融研究以及探索其他最先进模型中使用的技术可以帮助解决这些问题。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج التعرف على الكلام Whisper، إليك بعض التوصيات المتعلقة بالمشكلات المتوقعة في نموذج openai/whisper-base:\n\n1. يمكن تحسين الأداء في اللغات منخفضة الموارد من خلال زيادة كمية بيانات التدريب الخاصة بتلك اللغات، نظرًا لأن مجموعة البيانات الحالية مثقلة باللغة الإنجليزية. هناك حاجة إلى جمع بيانات أكثر تنوعًا.\n\n2. [مطلوب مزيد من المعلومات] حول المساهمات المحددة لبرنامج التشفير مقابل وحدة فك التشفير في قوة Whisper. يمكن لدراسات الاستئصال أو التجارب مع أجهزة التشفير الأخرى أن توفر رؤى ثاقبة.\n\n3. لا تزال أخطاء فك التشفير مثل التكرار أو تخطي الكلمات أو الهلوسة تمثل مشكلات، خاصة في النسخ الطويلة. قد يساعد الضبط الدقيق لمجموعات البيانات عالية الجودة أو استخدام تقنيات التعلم المعزز في تقليل ذلك.\n\n4. [بحاجة إلى مزيد من المعلومات] حول ما إذا كان دمج أساليب التدريب المسبق أو التدريس الذاتي غير الخاضعة للرقابة المستخدمة في النماذج الحديثة الأخرى يمكن أن يزيد من تحسين أداء Whisper.\n\n5. من المرجح أن يؤدي ضبط Whisper في مجالات محددة حيث تتوفر البيانات الخاضعة للإشراف إلى تعزيز النتائج وتمكين المقارنات المباشرة مع النماذج الأخرى. هناك حاجة إلى مزيد من الدراسات حول الضبط الدقيق.\n\n6. يميل النموذج إلى إجراء تخمينات غير صحيحة حول أسماء المتحدثين، حيث نادرًا ما يتم استنتاج هذه المعلومات من الصوت وحده. [بحاجة إلى مزيد من المعلومات] حول استراتيجيات التخفيف.\n\n7. بينما يستمر الأداء في التوسع مع حجم النموذج في معظم المهام، يتم ملاحظة انخفاض العائدات بالنسبة لـ ASR باللغة الإنجليزية، ربما بسبب التشبع مع اقتراب الأداء من المستوى البشري. يمكن أن يؤدي تحليل اتجاهات القياس إلى تحديد الحجم الفعال للنموذج.\n\n8. يهدف Whisper إلى أن يكون نظامًا للتعرف على كامل الميزات، ولكن قد لا تزال هناك حاجة إلى التعامل مع بعض المكونات مثل اكتشاف النشاط الصوتي وتدوين المتحدث وتطبيع النص العكسي بشكل منفصل. [بحاجة إلى مزيد من المعلومات] حول قدرات Whisper في هذه المجالات والفرص المتاحة لنظام أكثر تكاملاً.\n\nباختصار، تشمل المشكلات الرئيسية أداء اللغة منخفض الموارد، وأخطاء فك التشفير، وتحليل تأثير المكونات المعمارية المختلفة وتقنيات التدريب، ودراسة سلوك الضبط الدقيق، وتخفيف التصنيف غير الصحيح للمتحدثين، وكفاءة قياس النموذج، وتوسيع قدرات Whisper كأداة متكاملة للتعرف على الكلام. نظام. إن جمع المزيد من البيانات المتنوعة، وإجراء دراسات الاستئصال، واستكشاف التقنيات المستخدمة في النماذج الحديثة الأخرى يمكن أن يساعد في معالجة هذه المشكلات.",
    "answer_French": "Sur la base des références fournies sur le modèle de reconnaissance vocale Whisper, voici quelques recommandations concernant les problèmes prévisibles avec le modèle openai/whisper-base :\n\n1. Les performances dans les langues à faibles ressources pourraient être améliorées en augmentant la quantité de données de formation pour ces langues, car l'ensemble de données actuel est fortement axé sur l'anglais. Une collecte de données plus diversifiée est nécessaire.\n\n2. [Plus d'informations nécessaires] sur les contributions spécifiques de l'encodeur par rapport au décodeur à la robustesse de Whisper. Des études d’ablation ou des expériences avec d’autres codeurs pourraient fournir des informations.\n\n3. Les erreurs de décodage telles que les boucles, les sauts de mots ou les hallucinations restent des problèmes, en particulier dans le cas des transcriptions longues. Un réglage fin sur des ensembles de données de haute qualité ou l’utilisation de techniques d’apprentissage par renforcement peuvent contribuer à les réduire.\n\n4. [Plus d'informations nécessaires] sur la question de savoir si l'intégration de méthodes de pré-formation ou d'auto-apprentissage non supervisées utilisées dans d'autres modèles de pointe pourraient améliorer encore les performances de Whisper.\n\n5. Affiner Whisper sur des domaines spécifiques où des données supervisées sont disponibles est susceptible d'améliorer les résultats et de permettre des comparaisons directes avec d'autres modèles. Des études supplémentaires sont nécessaires pour un réglage précis.\n\n6. Le modèle a tendance à faire des suppositions incorrectes sur les noms des locuteurs, car ces informations sont rarement déductibles à partir du seul son. [Plus d'informations nécessaires] sur les stratégies d'atténuation.\n\n7. Bien que les performances continuent d'évoluer avec la taille du modèle dans la plupart des tâches, des rendements décroissants sont observés pour l'ASR en anglais, probablement en raison de la saturation à mesure que l'on se rapproche des performances au niveau humain. L'analyse des tendances de mise à l'échelle peut éclairer un dimensionnement efficace du modèle.\n\n8. Whisper vise à être un système de reconnaissance complet, mais certains composants tels que la détection de l'activité vocale, la diarisation du locuteur et la normalisation inverse du texte devront peut-être encore être traités séparément. [Plus d'informations nécessaires] sur les capacités de Whisper dans ces domaines et les opportunités pour un système plus intégré.\n\nEn résumé, les problèmes clés incluent les performances linguistiques à faibles ressources, les erreurs de décodage, l'analyse de l'impact des différents composants architecturaux et techniques de formation, l'étude du comportement de réglage fin, l'atténuation de l'étiquetage incorrect des locuteurs, l'efficacité de la mise à l'échelle du modèle et l'extension des capacités de Whisper en tant que système de reconnaissance vocale intégré. système. La collecte de données plus diverses, la réalisation d'études sur l'ablation et l'exploration des techniques utilisées dans d'autres modèles de pointe pourraient aider à résoudre ces problèmes.",
    "answer_Russian": "На основе предоставленных ссылок на модель распознавания речи Whisper, вот несколько рекомендаций относительно предсказуемых проблем с моделью openai/whisper-base:\n\n1. Производительность языков с ограниченными ресурсами можно улучшить, увеличив объем обучающих данных для этих языков, поскольку текущий набор данных насыщен английским языком. Необходим более разнообразный сбор данных.\n\n2. [Требуется дополнительная информация] о конкретном вкладе кодера и декодера в надежность Whisper. Исследования абляции или эксперименты с другими кодировщиками могут дать ценную информацию.\n\n3. Ошибки декодирования, такие как зацикливание, пропуск слов или галлюцинации, по-прежнему остаются проблемой, особенно при транскрипции длинных форм. Точная настройка высококачественных наборов данных или использование методов обучения с подкреплением могут помочь уменьшить их.\n\n4. [Требуется дополнительная информация] о том, может ли включение неконтролируемых методов предварительного обучения или самообучения, используемых в других современных моделях, еще больше улучшить производительность Whisper.\n\n5. Точная настройка Whisper в конкретных областях, где доступны контролируемые данные, вероятно, улучшит результаты и позволит проводить прямые сравнения с другими моделями. Необходимы дополнительные исследования по точной настройке.\n\n6. Модель имеет тенденцию делать неверные предположения об именах говорящих, поскольку эту информацию редко можно вывести только по звуку. [Требуется дополнительная информация] о стратегиях смягчения последствий.\n\n7. Хотя в большинстве задач производительность продолжает расти вместе с размером модели, для английского ASR наблюдается уменьшение отдачи, возможно, из-за насыщения по мере приближения к производительности человеческого уровня. Анализ тенденций масштабирования может помочь в эффективном выборе размера модели.\n\n8. Whisper призван стать полнофункциональной системой распознавания, но некоторые компоненты, такие как обнаружение голосовой активности, дневникирование говорящего и инверсная нормализация текста, возможно, все же придется обрабатывать отдельно. [Требуется дополнительная информация] о возможностях Whisper в этих областях и возможностях создания более интегрированной системы.\n\nТаким образом, ключевые проблемы включают производительность языка с низким уровнем ресурсов, ошибки декодирования, анализ влияния различных архитектурных компонентов и методов обучения, изучение точной настройки поведения, смягчение неправильной маркировки говорящих, эффективность масштабирования модели и расширение возможностей Whisper как интегрированного средства распознавания речи. система. Сбор более разнообразных данных, проведение исследований абляции и изучение методов, используемых в других современных моделях, могут помочь решить эти проблемы.",
    "answer_Japanese": "Whisper 音声認識モデルに関して提供されている参考資料に基づいて、openai/Whisper-Base モデルで予見される問題に関する推奨事項をいくつか示します。\n\n1. 現在のデータセットは英語が多いため、リソースの少ない言語でのパフォーマンスは、それらの言語のトレーニング データの量を増やすことで改善できる可能性があります。より多様なデータ収集が必要です。\n\n2. Whisper の堅牢性に対するエンコーダとデコーダの具体的な貢献に関する [さらなる情報が必要]。アブレーションの研究や他のエンコーダを使用した実験により、洞察が得られる可能性があります。\n\n3. ループ、単語のスキップ、幻覚などのデコードエラーは、特に長い形式の文字起こしの場合、依然として問題です。高品質のデータセットを微調整するか、強化学習手法を使用すると、これらを軽減できる可能性があります。\n\n4. 他の最先端のモデルで使用されている教師なし事前トレーニングまたは自己学習方法を組み込むことで、Whisper のパフォーマンスをさらに向上できるかどうかに関する [さらなる情報が必要]。\n\n5. 教師付きデータが利用可能な特定のドメインで Whisper を微調整すると、結果が向上し、他のモデルとの直接比較が可能になる可能性があります。微調整についてはさらなる研究が必要です。\n\n6. この情報は音声だけからはほとんど推測できないため、このモデルは話者名について誤った推測をする傾向があります。緩和戦略に関する[さらなる情報が必要]。\n\n7. ほとんどのタスクではパフォーマンスがモデル サイズに応じて拡大し続けますが、英語 ASR では利益の減少が見られます。これは、おそらく人間レベルのパフォーマンスに近づくにつれて飽和したためと考えられます。スケーリングの傾向を分析すると、効率的なモデルのサイジングを行うことができます。\n\n8. Whisper は完全な機能を備えた認識システムを目指していますが、音声アクティビティ検出、話者ダイアライゼーション、逆テキスト正規化などの一部のコンポーネントは依然として個別に処理する必要がある場合があります。これらの分野における Whisper の機能と、より統合されたシステムの機会に関する [さらなる情報が必要]。\n\n要約すると、重要な問題には、低リソース言語のパフォーマンス、エラーのデコード、さまざまなアーキテクチャ コンポーネントとトレーニング手法の影響の分析、動作の微調整の研究、不正確な話者ラベル付けの軽減、モデルのスケーリング効率、および統合音声認識としての Whisper の機能の拡張が含まれます。システム。より多様なデータを収集し、アブレーション研究を実施し、他の最先端のモデルで使用されている技術を調査することは、これらの問題の解決に役立つ可能性があります。"
  },
  {
    "question": "training_data",
    "chain": "generation",
    "answer": "The training data for the openai/whisper-base model consists of a large dataset of 680,000 hours of diverse and multilingual audio data collected from the internet. The data underwent minimal pre-processing and filtering to remove low-quality and machine-generated transcripts, and to ensure the spoken language matches the transcript language.\n\n[More Information Needed] for links to documentation related to data pre-processing or additional filtering.",
    "answer_Chinese": "openai/whisper-base 模型的训练数据由从互联网收集的 680,000 小时多样化和多语言音频数据的大型数据集组成。数据经过最少的预处理和过滤，以删除低质量和机器生成的转录本，并确保口语与转录本语言匹配。\n\n[需要更多信息] 获取与数据预处理或附加过滤相关的文档链接。",
    "answer_Arabic": "تتكون بيانات التدريب الخاصة بنموذج openai/whisper-base من مجموعة بيانات كبيرة تبلغ 680.000 ساعة من البيانات الصوتية المتنوعة والمتعددة اللغات التي تم جمعها من الإنترنت. خضعت البيانات للحد الأدنى من المعالجة المسبقة والتصفية لإزالة النصوص منخفضة الجودة والمولدة آليًا، وللتأكد من تطابق اللغة المنطوقة مع لغة النص.\n\n[يلزم مزيد من المعلومات] للحصول على روابط للوثائق المتعلقة بالمعالجة المسبقة للبيانات أو التصفية الإضافية.",
    "answer_French": "Les données de formation pour le modèle openai/whisper-base consistent en un vaste ensemble de données de 680 000 heures de données audio diverses et multilingues collectées sur Internet. Les données ont subi un prétraitement et un filtrage minimes pour supprimer les transcriptions de mauvaise qualité et générées automatiquement, et pour garantir que la langue parlée correspond à la langue de la transcription.\n\n[Plus d'informations nécessaires] pour des liens vers la documentation relative au prétraitement des données ou au filtrage supplémentaire.",
    "answer_Russian": "Данные обучения для модели openai/whisper-base состоят из большого набора данных объемом 680 000 часов разнообразных и многоязычных аудиоданных, собранных из Интернета. Данные прошли минимальную предварительную обработку и фильтрацию для удаления некачественных и машинных расшифровок, а также для обеспечения соответствия разговорного языка языку расшифровки.\n\n[Требуется дополнительная информация] для ссылок на документацию, связанную с предварительной обработкой данных или дополнительной фильтрацией.",
    "answer_Japanese": "openai/ウィスパーベース モデルのトレーニング データは、インターネットから収集された 680,000 時間の多様な多言語音声データの大規模なデータセットで構成されています。データには最小限の前処理とフィルタリングが行われ、低品質の機械生成トランスクリプトが削除され、話し言葉がトランスクリプト言語と一致することが確認されます。\n\nデータの前処理または追加のフィルタリングに関連するドキュメントへのリンクについては、[詳細情報が必要] を参照してください。"
  },
  {
    "question": "preprocessing",
    "chain": "generation",
    "answer": "Here are the details about preprocessing for the openai/whisper-base model, based on the provided references:\n\nTokenization:\n[More Information Needed]\n\nAudio Preprocessing:\n- All audio is re-sampled to 16,000 Hz\n- An 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds\n\nText Preprocessing:\n- Whisper models are trained to predict the raw text of transcripts without any significant standardization, relying on the expressiveness of sequence-to-sequence models to learn the mapping between utterances and their transcribed form\n- For English texts, the following normalization steps are performed to standardize the text into a consistent form:\n  [More Information Needed]\n- The text normalizer is released to allow for easy comparison and to help others study the performance of speech recognition systems in out-of-distribution settings\n\nData Filtering:\n- An audio language detector, fine-tuned on VoxLingua107, is used to ensure that the spoken language matches the language of the transcript according to CLD2. If the languages do not match, the (audio, transcript) pair is not included as a speech recognition training example, with an exception for English transcripts\n- Audio files are broken into 30-second segments paired with the corresponding transcript subset\n- Several automated filtering techniques were developed to address subpar transcripts in the raw dataset",
    "answer_Chinese": "以下是基于提供的参考文献的 openai/whisper-base 模型预处理的详细信息：\n\n代币化：\n[需要更多信息]\n\n音频预处理：\n- 所有音频均重新采样至 16,000 Hz\n- 80 通道对数幅度 Mel 频谱图表示在 25 毫秒窗口上计算，步长为 10 毫秒\n\n文本预处理：\n- Whisper 模型经过训练可以在没有任何显着标准化的情况下预测转录本的原始文本，依靠序列到序列模型的表达能力来学习话语与其转录形式之间的映射\n- 对于英文文本，执行以下规范化步骤，将文本标准化为一致的形式：\n  [需要更多信息]\n- 发布文本规范化器是为了方便比较并帮助其他人研究语音识别系统在分布外设置中的性能\n\n数据过滤：\n- 在 VoxLingua107 上进行微调的音频语言检测器用于确保口语与根据 CLD2 的文字记录的语言相匹配。如果语言不匹配，则（音频，转录本）对不会包含在语音识别训练示例中，但英语转录本除外\n- 音频文件被分成 30 秒的片段，并与相应的转录子集配对\n- 开发了几种自动过滤技术来解决原始数据集中的低于标准的转录本",
    "answer_Arabic": "فيما يلي التفاصيل حول المعالجة المسبقة لنموذج openai/whisper-base، استنادًا إلى المراجع المتوفرة:\n\nالترميز:\n[مطلوب مزيد من المعلومات]\n\nالمعالجة المسبقة للصوت:\n- يتم إعادة أخذ عينات الصوت بالكامل إلى 16000 هرتز\n- يتم حساب التمثيل الطيفي Mel ذو حجم السجل 80 قناة على نوافذ تبلغ 25 مللي ثانية مع خطوة قدرها 10 مللي ثانية\n\nالمعالجة المسبقة للنص:\n- يتم تدريب نماذج Whisper على التنبؤ بالنص الخام للنصوص دون أي توحيد مهم، وذلك بالاعتماد على تعبير نماذج التسلسل إلى التسلسل لتعلم الربط بين الكلام وشكله المكتوب\n- بالنسبة للنصوص الإنجليزية، يتم تنفيذ خطوات التطبيع التالية لتوحيد النص في شكل متسق:\n  [مطلوب مزيد من المعلومات]\n- تم إصدار أداة تسوية النص للسماح بإجراء مقارنة سهلة ولمساعدة الآخرين على دراسة أداء أنظمة التعرف على الكلام في إعدادات خارج التوزيع\n\nتصفية البيانات:\n- يتم استخدام كاشف اللغة الصوتية، المضبوط بدقة على VoxLingua107، للتأكد من تطابق اللغة المنطوقة مع لغة النص وفقًا لـ CLD2. إذا كانت اللغات غير متطابقة، فلن يتم تضمين زوج (الصوت والنص) كمثال للتدريب على التعرف على الكلام، باستثناء النصوص الإنجليزية\n- يتم تقسيم الملفات الصوتية إلى أجزاء مدتها 30 ثانية مقترنة بالمجموعة الفرعية للنص المقابلة\n- تم تطوير العديد من تقنيات التصفية الآلية لمعالجة النصوص الفرعية في مجموعة البيانات الأولية",
    "answer_French": "Voici les détails sur le prétraitement pour le modèle openai/whisper-base, basés sur les références fournies :\n\nTokenisation :\n[Plus d'informations nécessaires]\n\nPrétraitement audio :\n- Tout l'audio est rééchantillonné à 16 000 Hz\n- Une représentation du spectrogramme Mel de log-magnitude à 80 canaux est calculée sur des fenêtres de 25 millisecondes avec une foulée de 10 millisecondes\n\nPrétraitement du texte :\n- Les modèles chuchotés sont entraînés pour prédire le texte brut des transcriptions sans aucune standardisation significative, en s'appuyant sur l'expressivité des modèles séquence à séquence pour apprendre la correspondance entre les énoncés et leur forme transcrite.\n- Pour les textes anglais, les étapes de normalisation suivantes sont effectuées pour normaliser le texte sous une forme cohérente :\n  [Plus d'informations nécessaires]\n- Le normaliseur de texte est publié pour permettre une comparaison facile et pour aider les autres à étudier les performances des systèmes de reconnaissance vocale dans des contextes hors distribution.\n\nFiltrage des données :\n- Un détecteur de langue audio, affiné sur VoxLingua107, est utilisé pour s'assurer que la langue parlée correspond à la langue de la transcription selon CLD2. Si les langues ne correspondent pas, la paire (audio, transcription) n'est pas incluse comme exemple de formation à la reconnaissance vocale, à l'exception des transcriptions en anglais.\n- Les fichiers audio sont divisés en segments de 30 secondes associés au sous-ensemble de transcription correspondant\n- Plusieurs techniques de filtrage automatisées ont été développées pour traiter les transcriptions inférieures à la moyenne dans l'ensemble de données brutes",
    "answer_Russian": "Вот подробности предварительной обработки модели openai/whisper-base, основанные на предоставленных ссылках:\n\nТокенизация:\n[Требуется дополнительная информация]\n\nПредварительная обработка звука:\n- Весь звук повторно дискретизируется до 16 000 Гц.\n- 80-канальное представление Mel-спектрограммы логарифмической величины рассчитывается в 25-миллисекундных окнах с шагом 10 миллисекунд.\n\nПредварительная обработка текста:\n- Модели шепота обучены предсказывать необработанный текст транскриптов без какой-либо значительной стандартизации, полагаясь на выразительность моделей последовательностей для изучения сопоставления между высказываниями и их транскрибируемой формой.\n- Для английских текстов выполняются следующие шаги нормализации для приведения текста в единообразную форму:\n  [Требуется дополнительная информация]\n- Выпущен нормализатор текста, который упрощает сравнение и помогает другим изучить производительность систем распознавания речи в условиях отсутствия распространения.\n\nФильтрация данных:\n- Детектор языка аудио, настроенный на VoxLingua107, используется для обеспечения соответствия разговорного языка языку расшифровки в соответствии с CLD2. Если языки не совпадают, пара (аудио, стенограмма) не включается в качестве примера обучения распознаванию речи, за исключением транскриптов на английском языке.\n- Аудиофайлы разбиты на 30-секундные сегменты в сочетании с соответствующим подмножеством стенограммы.\n- Было разработано несколько методов автоматической фильтрации для устранения некачественных расшифровок в наборе необработанных данных.",
    "answer_Japanese": "提供された参考資料に基づいた、openai/whisper-base モデルの前処理の詳細は次のとおりです。\n\nトークン化:\n[さらに詳しい情報が必要です]\n\nオーディオの前処理:\n- すべてのオーディオは 16,000 Hz に再サンプリングされます\n- 80 チャネルの対数振幅メル スペクトログラム表現は、10 ミリ秒のストライドで 25 ミリ秒のウィンドウで計算されます。\n\nテキストの前処理:\n- ウィスパー モデルは、大幅な標準化を行わずにトランスクリプトの生のテキストを予測するようにトレーニングされ、シーケンスツーシーケンス モデルの表現力に依存して、発話とその転写された形式の間のマッピングを学習します。\n- 英語のテキストの場合、テキストを一貫した形式に標準化するために次の正規化手順が実行されます。\n  [さらに詳しい情報が必要です]\n- テキスト ノーマライザーがリリースされ、比較が容易になり、配布外の設定で音声認識システムのパフォーマンスを他の人が研究できるようになりました。\n\nデータフィルタリング:\n- VoxLingua107 で微調整された音声言語検出器を使用して、話し言葉が CLD2 に従ってトランスクリプトの言語と一致することを確認します。言語が一致しない場合、英語のトランスクリプトを除き、(音声、トランスクリプト) ペアは音声認識トレーニングの例として含まれません。\n- オーディオ ファイルは、対応するトランスクリプト サブセットと組み合わせられた 30 秒のセグメントに分割されます。\n- 生のデータセット内の標準以下のトランスクリプトに対処するために、いくつかの自動フィルタリング技術が開発されました"
  },
  {
    "question": "training_regime",
    "chain": "generation",
    "answer": "Based on the provided references, here are the training hyperparameters for the openai/whisper-base model:\n\n- Data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing was used for training\n- Optimizer: AdamW \n- Gradient norm clipping was used\n- Learning rate schedule: Linear learning rate decay to zero after a warmup over the first 2048 updates\n- Batch size: 256 segments\n- Number of training updates: 2^20 updates, which is between two and three passes over the dataset\n- No data augmentation or regularization was used, relying on the diversity of the large dataset to encourage generalization and robustness\n\n[More Information Needed] on the exact values used for:\n- Learning rate \n- Weight decay\n- Warmup steps\n- Gradient clipping threshold\n- Activation checkpointing configuration\n\nThe model was briefly fine-tuned on a subset of transcripts that do not include speaker annotations to remove the behavior of incorrectly guessing speaker names.",
    "answer_Chinese": "根据提供的参考资料，以下是 openai/whisper-base 模型的训练超参数：\n\n- 使用 FP16 的加速器之间的数据并行性以及动态损失缩放和激活检查点用于训练\n- 优化器：AdamW \n- 使用梯度范数裁剪\n- 学习率计划：在前 2048 个更新的预热后，线性学习率衰减至零\n- 批量大小：256 段\n- 训练更新次数：2^20 次更新，介于数据集的两到三遍之间\n- 没有使用数据增强或正则化，依靠大数据集的多样性来鼓励泛化和鲁棒性\n\n[需要更多信息]有关用于以下用途的确切值：\n- 学习率 \n- 重量衰减\n- 热身步骤\n- 梯度裁剪阈值\n- 激活检查点配置\n\n该模型对不包含说话者注释的记录子集进行了简要微调，以消除错误猜测说话者姓名的行为。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك معلمات التدريب الفائقة لنموذج openai/whisper-base:\n\n- تم استخدام توازي البيانات عبر المسرعات باستخدام FP16 مع قياس الخسارة الديناميكي وفحص التنشيط للتدريب\n- محسن: AdamW \n- تم استخدام القطع المعياري المتدرج\n- جدول معدل التعلم: يتراجع معدل التعلم الخطي إلى الصفر بعد عملية إحماء خلال تحديثات 2048 الأولى\n- حجم الدفعة: 256 قطعة\n- عدد تحديثات التدريب: 2^20 تحديث، وهو ما بين مرتين وثلاثة تمريرات على مجموعة البيانات\n- لم يتم استخدام أي زيادة أو تنظيم للبيانات، والاعتماد على تنوع مجموعة البيانات الكبيرة لتشجيع التعميم والقوة\n\n[يلزم مزيد من المعلومات] حول القيم الدقيقة المستخدمة من أجل:\n- معدل التعليم \n- تسوس الوزن\n- خطوات الإحماء\n- عتبة القطع التدرج\n- تكوين نقاط التفتيش التنشيط\n\nتم ضبط النموذج لفترة وجيزة على مجموعة فرعية من النصوص التي لا تتضمن تعليقات توضيحية للمتحدث لإزالة سلوك تخمين أسماء المتحدثين بشكل غير صحيح.",
    "answer_French": "Sur la base des références fournies, voici les hyperparamètres de formation pour le modèle openai/whisper-base :\n\n- Le parallélisme des données entre les accélérateurs utilisant FP16 avec mise à l'échelle dynamique des pertes et points de contrôle d'activation a été utilisé pour la formation\n- Optimiseur : AdamW \n- Un découpage de la norme de dégradé a été utilisé\n- Calendrier du taux d'apprentissage : le taux d'apprentissage linéaire diminue jusqu'à zéro après un échauffement au cours des 2048 premières mises à jour\n- Taille du lot : 256 segments\n- Nombre de mises à jour de formation : 2^20 mises à jour, soit entre deux et trois passages sur l'ensemble de données\n- Aucune augmentation ou régularisation des données n'a été utilisée, s'appuyant sur la diversité du grand ensemble de données pour encourager la généralisation et la robustesse\n\n[Plus d'informations nécessaires] sur les valeurs exactes utilisées pour :\n- Taux d'apprentissage \n- Perte de poids\n- Étapes d'échauffement\n- Seuil d'écrêtage du dégradé\n- Configuration des points de contrôle d'activation\n\nLe modèle a été brièvement affiné sur un sous-ensemble de transcriptions qui n'incluent pas d'annotations des locuteurs afin de supprimer le comportement consistant à deviner incorrectement les noms des locuteurs.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот гиперпараметры обучения для модели openai/whisper-base:\n\n- Для обучения использовался параллелизм данных между ускорителями с использованием FP16 с динамическим масштабированием потерь и контрольными точками активации.\n- Оптимизатор: AdamW \n- Использовалось отсечение градиентной нормы.\n- График скорости обучения: линейное снижение скорости обучения до нуля после прогрева в течение первых 2048 обновлений.\n- Размер партии: 256 сегментов.\n- Количество обучающих обновлений: 2^20 обновлений, что составляет от двух до трех проходов по набору данных.\n- Никакое увеличение или регуляризация данных не использовалось, поскольку разнообразие большого набора данных способствует обобщению и надежности.\n\n[Требуется дополнительная информация] о точных значениях, используемых для:\n- Скорость обучения \n- Снижение веса\n- Этапы разминки\n- Порог отсечения градиента\n- Конфигурация контрольной точки активации.\n\nМодель была кратко доработана для подмножества стенограмм, которые не включают аннотации говорящего, чтобы исключить поведение, связанное с неправильным угадыванием имен говорящих.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base モデルのトレーニング ハイパーパラメーターを次に示します。\n\n- 動的な損失スケーリングとアクティベーション チェックポイント機能を備えた FP16 を使用したアクセラレータ間のデータ並列処理がトレーニングに使用されました\n- オプティマイザー: AdamW \n- 勾配ノルムクリッピングが使用されました\n- 学習率スケジュール: 最初の 2048 アップデートのウォームアップ後に、線形学習率がゼロに減衰します。\n- バッチサイズ: 256 セグメント\n- トレーニング更新の数: 2^20 更新。これはデータセット上の 2 回から 3 回のパスの間です。\n- 一般化と堅牢性を促進するために大規模なデータセットの多様性に依存し、データの拡張や正則化は使用されませんでした。\n\n以下に使用される正確な値に関する [追加情報が必要]:\n- 学習率 \n- 体重の減少\n- ウォームアップ手順\n- グラデーションクリッピング閾値\n- アクティベーションチェックポイント設定\n\nこのモデルは、話者名を誤って推測する動作を除去するために、話者の注釈を含まないトランスクリプトのサブセットに対して簡単に微調整されました。"
  },
  {
    "question": "speeds_sizes_times",
    "chain": "generation",
    "answer": "Based on the provided references, here are the details I can provide about the openai/whisper-base model:\n\nModel size: The base model is part of a suite of models of various sizes trained to study the scaling properties of Whisper. However, the exact number of parameters for the base model is [More Information Needed].\n\nTraining:\n- Models were trained with AdamW optimizer and gradient norm clipping.\n- A linear learning rate decay to zero was used after a warmup over the first 2048 updates. \n- Batch size of 256 segments was used.\n- Models were trained for 2^20 updates, which is between two and three passes over the dataset.\n- No data augmentation or regularization was used during training.\n\n[More Information Needed] on specific throughput, start/end times, and checkpoint sizes for the base model.\n\nThe model architecture is an encoder-decoder Transformer. Audio is resampled to 16kHz and converted to an 80-channel log-mel spectrogram using 25ms windows and 10ms stride.",
    "answer_Chinese": "根据提供的参考资料，以下是我可以提供的有关 openai/whisper-base 模型的详细信息：\n\n模型大小：基础模型是一组不同大小的模型的一部分，这些模型经过训练来研究 Whisper 的缩放特性。然而，基本模型的确切参数数量是[需要更多信息]。\n\n训练：\n- 使用 AdamW 优化器和梯度范数裁剪来训练模型。\n- 在前 2048 次更新的预热后，使用了线性学习率衰减到零。 \n- 使用 256 个段的批量大小。\n- 模型经过 2^20 次更新训练，即对数据集进行两到三遍更新。\n- 训练期间未使用数据增强或正则化。\n\n[需要更多信息]有关基本模型的特定吞吐量、开始/结束时间和检查点大小。\n\n模型架构是一个编码器-解码器 Transformer。音频被重新采样至 16kHz，并使用 25ms 窗口和 10ms 步幅转换为 80 通道 log-mel 频谱图。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك التفاصيل التي يمكنني تقديمها حول نموذج openai/whisper-base:\n\nحجم النموذج: النموذج الأساسي هو جزء من مجموعة نماذج بأحجام مختلفة تم تدريبها لدراسة خصائص قياس Whisper. ومع ذلك، فإن العدد الدقيق لمعلمات النموذج الأساسي هو [بحاجة إلى مزيد من المعلومات].\n\nتمرين:\n- تم تدريب النماذج باستخدام أداة تحسين AdamW وقص التدرج المعياري.\n- تم استخدام انخفاض معدل التعلم الخطي إلى الصفر بعد عملية الإحماء خلال تحديثات 2048 الأولى. \n- تم استخدام حجم الدفعة 256 قطعة.\n- تم تدريب النماذج على تحديثات 2^20، والتي تتراوح بين مرتين وثلاث مرات عبر مجموعة البيانات.\n- لم يتم استخدام أي زيادة أو تنظيم للبيانات أثناء التدريب.\n\n[مطلوب مزيد من المعلومات] حول الإنتاجية المحددة وأوقات البدء/الانتهاء وأحجام نقاط التفتيش للنموذج الأساسي.\n\nبنية النموذج عبارة عن محول تشفير وفك تشفير. تتم إعادة تشكيل الصوت إلى 16 كيلو هرتز وتحويله إلى مخطط طيفي لتسجيل الدخول ذو 80 قناة باستخدام نوافذ تبلغ 25 مللي ثانية وخطوة تبلغ 10 مللي ثانية.",
    "answer_French": "Sur la base des références fournies, voici les détails que je peux fournir sur le modèle openai/whisper-base :\n\nTaille du modèle : le modèle de base fait partie d'une suite de modèles de différentes tailles formés pour étudier les propriétés de mise à l'échelle de Whisper. Cependant, le nombre exact de paramètres pour le modèle de base est [Plus d'informations nécessaires].\n\nEntraînement:\n- Les modèles ont été formés avec l'optimiseur AdamW et le découpage des normes de gradient.\n- Une décroissance linéaire du taux d'apprentissage jusqu'à zéro a été utilisée après un échauffement sur les 2048 premières mises à jour. \n- Une taille de lot de 256 segments a été utilisée.\n- Les modèles ont été entraînés pour 2 ^ 20 mises à jour, soit entre deux et trois passages sur l'ensemble de données.\n- Aucune augmentation ou régularisation des données n'a été utilisée pendant la formation.\n\n[Plus d'informations nécessaires] sur le débit spécifique, les heures de début/fin et les tailles des points de contrôle pour le modèle de base.\n\nL'architecture du modèle est un transformateur codeur-décodeur. L'audio est rééchantillonné à 16 kHz et converti en un spectrogramme log-mel à 80 canaux en utilisant des fenêtres de 25 ms et une foulée de 10 ms.",
    "answer_Russian": "На основе предоставленных ссылок я могу предоставить следующую информацию о модели openai/whisper-base:\n\nРазмер модели: базовая модель является частью набора моделей разного размера, предназначенных для изучения масштабируемых свойств Whisper. Однако точное количество параметров базовой модели равно [Необходима дополнительная информация].\n\nОбучение:\n- Модели были обучены с помощью оптимизатора AdamW и ограничения нормы градиента.\n- Линейное снижение скорости обучения до нуля использовалось после прогрева в течение первых 2048 обновлений. \n- Использовался пакет размером 256 сегментов.\n— Модели были обучены для 2^20 обновлений, что составляет от двух до трех проходов по набору данных.\n- Во время обучения не использовалось увеличение или регуляризация данных.\n\n[Требуется дополнительная информация] о конкретной пропускной способности, времени начала/окончания и размерах контрольных точек для базовой модели.\n\nАрхитектура модели представляет собой преобразователь кодер-декодер. Звук передискретизируется до 16 кГц и преобразуется в 80-канальную логарифмическую спектрограмму с использованием окон 25 мс и шага 10 мс.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base モデルについて提供できる詳細は次のとおりです。\n\nモデル サイズ: 基本モデルは、Whisper のスケーリング特性を研究するためにトレーニングされたさまざまなサイズのモデル スイートの一部です。ただし、ベース モデルのパラメータの正確な数は [追加情報が必要] です。\n\nトレーニング：\n- モデルは、AdamW オプティマイザーと勾配ノルム クリッピングを使用してトレーニングされました。\n- 最初の 2048 アップデートでのウォームアップ後に、ゼロまでの線形学習率減衰が使用されました。 \n- 256 セグメントのバッチ サイズが使用されました。\n- モデルは 2^20 更新用にトレーニングされました。これは、データセット全体で 2 回から 3 回のパスです。\n- トレーニング中にデータの拡張や正則化は使用されませんでした。\n\n基本モデルの特定のスループット、開始/終了時間、チェックポイント サイズに関する [詳細情報が必要]。\n\nモデル アーキテクチャは、エンコーダー/デコーダー トランスフォーマーです。オーディオは 16kHz にリサンプリングされ、25ms ウィンドウと 10ms ストライドを使用して 80 チャネルのログメル スペクトログラムに変換されます。"
  },
  {
    "question": "testing_data",
    "chain": "generation",
    "answer": "Based on the provided references, the openai/whisper-base model was evaluated on the following benchmarks and datasets:\n\nShort-form English-only datasets:\n- LibriSpeech (test-clean and test-other splits)\n- TED-LIUM 3 (test split)\n- Common Voice 5.1 (English subset)\n- Artie bias corpus (subset of Common Voice)\n- CallHome and Switchboard (from LDC2002S09 and LDC2002T43)\n- WSJ (from LDC93S6B and LDC94S13B, preprocessed using s5 recipe)\n- CORAAL (231 interviews, preprocessed using FairSpeech project script)\n- Rev16 (subset of 16 files from Rev.AI's Podcast Transcription Benchmark)\n\nOther datasets:\n- CHiME-6 (preprocessed from CHiME-5 using s5 track1 recipe stage 0)\n- AMI-IHM and AMI-SDM1 (preprocessed using s5b recipe stages 0 and 2)\n- Common Voice 15 (used for language-specific performance breakdown)\n- Fleurs (used for language-specific performance breakdown)\n\nThe model was also compared against open-source models like NVIDIA STT Conformer-CTC Large and 4 commercial ASR services on various datasets. Detailed results can be found in the paper.\n\n[More Information Needed] on the specific datasets used for the commercial ASR comparison and the \"Meanwhile\" dataset mentioned.",
    "answer_Chinese": "根据提供的参考资料，openai/whisper-base 模型在以下基准和数据集上进行了评估：\n\n简短的纯英文数据集：\n- LibriSpeech（测试干净和测试其他分割）\n- TED-LIUM 3（测试分组）\n- Common Voice 5.1（英语子集）\n- Artie 偏差语料库（Common Voice 的子集）\n- CallHome 和总机（来自 LDC2002S09 和 LDC2002T43）\n- WSJ（来自 LDC93S6B 和 LDC94S13B，使用 s5 配方进行预处理）\n- CORAAL（231 次采访，使用 FairSpeech 项目脚本进行预处理）\n- Rev16（来自 Rev.AI 播客转录基准的 16 个文件的子集）\n\n其他数据集：\n- CHiME-6（使用 s5 track1 配方阶段 0 从 ChiME-5 进行预处理）\n- AMI-IHM 和 AMI-SDM1（使用 s5b 配方阶段 0 和 2 进行预处理）\n- Common Voice 15（用于特定语言的性能细分）\n- Fleurs（用于特定语言的性能细分）\n\n该模型还与各种数据集上的 NVIDIA STT Conformer-CTC Large 等开源模型和 4 个商业 ASR 服务进行了比较。详细结果可以在论文中找到。\n\n[需要更多信息] 用于商业 ASR 比较的特定数据集和提到的“同时”数据集。",
    "answer_Arabic": "بناءً على المراجع المقدمة، تم تقييم نموذج openai/whisper-base بناءً على المعايير ومجموعات البيانات التالية:\n\nمجموعات البيانات القصيرة باللغة الإنجليزية فقط:\n- LibriSpeech (اختبار الانقسامات النظيفة واختبار الأخرى)\n- TED-LIUM 3 (تقسيم الاختبار)\n- الصوت المشترك 5.1 (مجموعة فرعية باللغة الإنجليزية)\n- مجموعة آرتي المتحيزة (مجموعة فرعية من الصوت العام)\n- CallHome وSwitchboard (من LDC2002S09 وLDC2002T43)\n- WSJ (من LDC93S6B وLDC94S13B، تمت معالجته مسبقًا باستخدام وصفة s5)\n- CORAAL (231 مقابلة، تمت معالجتها مسبقًا باستخدام نص مشروع FairSpeech)\n- Rev16 (مجموعة فرعية مكونة من 16 ملفًا من معيار نسخ البودكاست الخاص بـ Rev.AI)\n\nمجموعات البيانات الأخرى:\n- CHiME-6 (معالج مسبقًا من CHiME-5 باستخدام مرحلة وصفة s5 Track1 0)\n- AMI-IHM وAMI-SDM1 (تمت معالجتهما مسبقًا باستخدام مرحلتي وصفة s5b 0 و2)\n- الصوت العام 15 (يستخدم لتحليل الأداء الخاص باللغة)\n- Fleurs (تستخدم لتحليل الأداء الخاص باللغة)\n\nتمت مقارنة النموذج أيضًا بنماذج مفتوحة المصدر مثل NVIDIA STT Conformer-CTC Large و4 خدمات ASR تجارية في مجموعات بيانات مختلفة. يمكن العثور على النتائج التفصيلية في الورقة.\n\n[يلزم مزيد من المعلومات] حول مجموعات البيانات المحددة المستخدمة لمقارنة ASR التجارية ومجموعة البيانات \"في هذه الأثناء\" المذكورة.",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base a été évalué sur les références et ensembles de données suivants :\n\nEnsembles de données abrégés uniquement en anglais :\n- LibriSpeech (test-clean et test-autres fractionnements)\n- TED-LIUM 3 (test fractionné)\n- Common Voice 5.1 (sous-ensemble anglais)\n- Corpus Artie Biais (sous-ensemble de Common Voice)\n- CallHome et Standard (depuis LDC2002S09 et LDC2002T43)\n- WSJ (de LDC93S6B et LDC94S13B, prétraité à l'aide de la recette s5)\n- CORAAL (231 entretiens, prétraités à l'aide du script du projet FairSpeech)\n- Rev16 (sous-ensemble de 16 fichiers du Podcast Transcription Benchmark de Rev.AI)\n\nAutres ensembles de données :\n- CHiME-6 (prétraité à partir de CHiME-5 en utilisant l'étape 0 de la recette s5 track1)\n- AMI-IHM et AMI-SDM1 (prétraités à l'aide des étapes 0 et 2 de la recette s5b)\n- Common Voice 15 (utilisé pour la répartition des performances spécifiques à une langue)\n- Fleurs (utilisé pour la répartition des performances spécifiques à la langue)\n\nLe modèle a également été comparé à des modèles open source tels que NVIDIA STT Conformer-CTC Large et 4 services ASR commerciaux sur divers ensembles de données. Les résultats détaillés peuvent être trouvés dans l’article.\n\n[Plus d'informations nécessaires] sur les ensembles de données spécifiques utilisés pour la comparaison ASR commerciale et l'ensemble de données « Pendant ce temps » mentionné.",
    "answer_Russian": "На основе предоставленных ссылок модель openai/whisper-base была оценена по следующим критериям и наборам данных:\n\nКраткие наборы данных только на английском языке:\n- LibriSpeech (разделение «тест-чистка» и «тест-другие»)\n- ТЭД-ЛИУМ 3 (тестовый сплит)\n- Common Voice 5.1 (английское подмножество)\n- Корпус предвзятости Арти (подмножество Common Voice)\n- CallHome и коммутатор (из LDC2002S09 и LDC2002T43)\n- WSJ (из LDC93S6B и LDC94S13B, предварительно обработано с использованием рецепта s5)\n- CORAAL (231 интервью, предварительно обработанные с использованием сценария проекта FairSpeech)\n- Rev16 (подмножество из 16 файлов из теста транскрипции подкастов Rev.AI)\n\nДругие наборы данных:\n- CHiME-6 (предварительно обработано из CHiME-5 с использованием рецепта s5 track1, этап 0)\n- AMI-IHM и AMI-SDM1 (предварительно обработанные с использованием этапов рецепта s5b 0 и 2)\n- Common Voice 15 (используется для разбивки производительности в зависимости от языка)\n- Fleurs (используется для разбивки производительности в зависимости от языка)\n\nМодель также сравнивалась с моделями с открытым исходным кодом, такими как NVIDIA STT Conformer-CTC Large и 4 коммерческими сервисами ASR, на различных наборах данных. Подробные результаты можно найти в статье.\n\n[Требуется дополнительная информация] о конкретных наборах данных, используемых для коммерческого сравнения ASR, и упомянутом наборе данных «Между тем».",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper ベース モデルは次のベンチマークとデータセットで評価されました。\n\n英語のみの短い形式のデータセット:\n- LibriSpeech (テスト-クリーンおよびテスト-その他の分割)\n- TED-LIUM 3 (テスト分割)\n- Common Voice 5.1 (英語サブセット)\n- アーティ バイアス コーパス (Common Voice のサブセット)\n- CallHome およびスイッチボード (LDC2002S09 および LDC2002T43 より)\n- WSJ (LDC93S6B および LDC94S13B から、s5 レシピを使用して前処理)\n- CORAAL (231 件のインタビュー、FairSpeech プロジェクト スクリプトを使用して前処理)\n- Rev16 (Rev.AI の Podcast Transcription Benchmark からの 16 ファイルのサブセット)\n\nその他のデータセット:\n- CHiME-6 (s5 track1レシピステージ0を使用してCHiME-5から前処理)\n- AMI-IHM および AMI-SDM1 (s5b レシピ ステージ 0 および 2 を使用して前処理)\n- Common Voice 15 (言語固有のパフォーマンスの内訳に使用)\n- Fleurs (言語固有のパフォーマンスの内訳に使用)\n\nこのモデルは、さまざまなデータセット上の NVIDIA STT Conformer-CTC Large などのオープンソース モデルや 4 つの商用 ASR サービスとも比較されました。詳細な結果は論文でご覧いただけます。\n\n商用 ASR 比較に使用される特定のデータセットと言及された「Meanwhile」データセットに関する [さらなる情報が必要]。"
  },
  {
    "question": "testing_factors",
    "chain": "generation",
    "answer": "Based on the provided references about the openai/whisper-base model, here are some key foreseeable characteristics that may influence the model's behavior and performance:\n\n1. Language and amount of training data: The model's performance on a given language is strongly correlated with the amount of training data available for that language. The current pre-training dataset is English-heavy, so the model likely performs better on English compared to lower-resource languages.\n\n2. Domain and context: The model may struggle with domain-specific terminology or contexts that are underrepresented in the training data. Fine-tuning on high-quality supervised datasets from specific domains could help improve performance.\n\n3. Audio quality and noise: The model's performance may degrade when dealing with noisy audio or audio from challenging environments like pubs or restaurants. Testing the model's robustness to white noise and pub noise additions could reveal disparities in performance.\n\n4. Speaker demographics: [More Information Needed] The references do not directly address performance disparities across speaker demographics like age, gender, or accent.\n\n5. Transcript formatting: The model's zero-shot performance may be impacted by variations in transcript formatting across datasets, as it does not observe dataset-specific formatting examples during training.\n\n6. Evaluation metrics: Standard evaluation metrics like WER may not always correlate well with human judgment, especially for zero-shot models. Developing better evaluation metrics is an active research area.\n\nTo uncover potential performance disparities, the model should be evaluated on a diverse range of datasets covering different languages, domains, audio qualities, and speaker demographics. Disaggregated evaluation results would provide a clearer picture of the model's strengths and weaknesses.",
    "answer_Chinese": "根据提供的有关 openai/whisper-base 模型的参考资料，以下是一些可能影响模型行为和性能的关键可预见特征：\n\n1. 语言和训练数据量：模型在给定语言上的性能与该语言可用的训练数据量密切相关。当前的预训练数据集以英语为主，因此与资源较低的语言相比，该模型在英语上的表现可能更好。\n\n2. 领域和上下文：模型可能会遇到训练数据中代表性不足的特定领域术语或上下文。对特定领域的高质量监督数据集进行微调有助于提高性能。\n\n3. 音频质量和噪声：在处理嘈杂的音频或来自酒吧或餐馆等具有挑战性的环境的音频时，模型的性能可能会下降。测试模型对白噪声和酒吧噪声添加的鲁棒性可以揭示性能差异。\n\n4. 演讲者人口统计数据：[需要更多信息] 参考资料并未直接解决不同演讲者人口统计数据（如年龄、性别或口音）之间的表现差异。\n\n5. 转录本格式：模型的零样本性能可能会受到数据集中转录本格式变化的影响，因为它在训练期间不会观察数据集特定的格式示例。\n\n6. 评估指标：像 WER 这样的标准评估指标可能并不总是与人类判断很好地相关，特别是对于零样本模型。开发更好的评估指标是一个活跃的研究领域。\n\n为了发现潜在的性能差异，应该在涵盖不同语言、领域、音频质量和说话者人口统计的各种数据集上评估模型。分类评估结果可以更清楚地了解模型的优点和缺点。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج openai/whisper-base، إليك بعض الخصائص الرئيسية المتوقعة التي قد تؤثر على سلوك النموذج وأدائه:\n\n1. اللغة وكمية بيانات التدريب: يرتبط أداء النموذج على لغة معينة ارتباطًا وثيقًا بكمية بيانات التدريب المتاحة لتلك اللغة. مجموعة بيانات ما قبل التدريب الحالية كثيفة اللغة الإنجليزية، لذلك من المحتمل أن يكون أداء النموذج أفضل في اللغة الإنجليزية مقارنة باللغات منخفضة الموارد.\n\n2. المجال والسياق: قد يعاني النموذج من المصطلحات أو السياقات الخاصة بالمجال التي لا يتم تمثيلها بشكل كافٍ في بيانات التدريب. يمكن أن يساعد الضبط الدقيق لمجموعات البيانات عالية الجودة الخاضعة للإشراف من مجالات محددة في تحسين الأداء.\n\n3. جودة الصوت والضوضاء: قد ينخفض ​​أداء النموذج عند التعامل مع الصوت الصاخب أو الصوت الصادر من البيئات الصعبة مثل الحانات أو المطاعم. اختبار متانة النموذج للضوضاء البيضاء وإضافات ضوضاء الحانة يمكن أن يكشف عن تباينات في الأداء.\n\n4. التركيبة السكانية للمتحدثين: [بحاجة إلى مزيد من المعلومات] لا تتناول المراجع بشكل مباشر الفوارق في الأداء بين التركيبة السكانية للمتحدثين مثل العمر أو الجنس أو اللهجة.\n\n5. تنسيق النص: قد يتأثر أداء النموذج الصفري بالاختلافات في تنسيق النص عبر مجموعات البيانات، لأنه لا يلاحظ أمثلة التنسيق الخاصة بمجموعة البيانات أثناء التدريب.\n\n6. مقاييس التقييم: قد لا تتوافق مقاييس التقييم القياسية مثل WER دائمًا بشكل جيد مع الحكم البشري، خاصة بالنسبة للنماذج الصفرية. يعد تطوير مقاييس تقييم أفضل مجالًا بحثيًا نشطًا.\n\nللكشف عن التباينات المحتملة في الأداء، يجب تقييم النموذج على مجموعة متنوعة من مجموعات البيانات التي تغطي لغات ومجالات وخصائص صوتية مختلفة والتركيبة السكانية للمتحدثين. ومن شأن نتائج التقييم المفصلة أن توفر صورة أوضح لنقاط القوة والضعف في النموذج.",
    "answer_French": "Sur la base des références fournies sur le modèle openai/whisper-base, voici quelques caractéristiques clés prévisibles qui peuvent influencer le comportement et les performances du modèle :\n\n1. Langue et quantité de données de formation : les performances du modèle sur une langue donnée sont fortement corrélées à la quantité de données de formation disponibles pour cette langue. L'ensemble de données de pré-formation actuel contient beaucoup d'anglais, de sorte que le modèle fonctionne probablement mieux en anglais que dans les langues à faibles ressources.\n\n2. Domaine et contexte : le modèle peut avoir des difficultés avec une terminologie ou des contextes spécifiques au domaine qui sont sous-représentés dans les données de formation. Un réglage précis des ensembles de données supervisés de haute qualité provenant de domaines spécifiques pourrait contribuer à améliorer les performances.\n\n3. Qualité audio et bruit : les performances du modèle peuvent se dégrader lorsqu'il est confronté à un son bruyant ou à un son provenant d'environnements difficiles comme les pubs ou les restaurants. Tester la robustesse du modèle aux ajouts de bruit blanc et de bruit de pub pourrait révéler des disparités de performances.\n\n4. Données démographiques des locuteurs : [Plus d'informations nécessaires] Les références ne traitent pas directement des disparités de performances selon les données démographiques des locuteurs, telles que l'âge, le sexe ou l'accent.\n\n5. Formatage de la transcription : les performances zéro du modèle peuvent être affectées par les variations du formatage de la transcription entre les ensembles de données, car il n'observe pas d'exemples de formatage spécifiques à l'ensemble de données pendant la formation.\n\n6. Métriques d'évaluation : les mesures d'évaluation standard telles que WER peuvent ne pas toujours être bien corrélées avec le jugement humain, en particulier pour les modèles sans tir. Le développement de meilleurs paramètres d’évaluation est un domaine de recherche actif.\n\nPour découvrir les disparités potentielles de performances, le modèle doit être évalué sur un large éventail d'ensembles de données couvrant différentes langues, domaines, qualités audio et données démographiques des locuteurs. Les résultats de l'évaluation ventilés fourniraient une image plus claire des forces et des faiblesses du modèle.",
    "answer_Russian": "На основе предоставленных ссылок на модель openai/whisper-base, вот некоторые ключевые прогнозируемые характеристики, которые могут повлиять на поведение и производительность модели:\n\n1. Язык и объем обучающих данных. Производительность модели на конкретном языке сильно коррелирует с объемом обучающих данных, доступных для этого языка. Текущий набор данных для предварительного обучения насыщен английским языком, поэтому модель, вероятно, работает лучше на английском языке по сравнению с языками с меньшими ресурсами.\n\n2. Область и контекст. Модель может иметь проблемы с терминологией или контекстами, специфичными для предметной области, которые недостаточно представлены в обучающих данных. Точная настройка высококачественных контролируемых наборов данных из конкретных доменов может помочь повысить производительность.\n\n3. Качество звука и шум. Производительность модели может ухудшиться при работе с шумным звуком или звуком в сложных условиях, например в пабах или ресторанах. Тестирование устойчивости модели к белому шуму и шуму паба может выявить различия в производительности.\n\n4. Демографические данные говорящих: [Необходима дополнительная информация] Ссылки не касаются напрямую различий в производительности среди говорящих по демографическим характеристикам, таким как возраст, пол или акцент.\n\n5. Форматирование расшифровки. На нулевую производительность модели могут влиять различия в форматировании расшифровки в разных наборах данных, поскольку во время обучения она не учитывает примеры форматирования, специфичные для набора данных.\n\n6. Метрики оценки. Стандартные метрики оценки, такие как WER, не всегда хорошо коррелируют с человеческим суждением, особенно для моделей с нулевым выстрелом. Разработка более эффективных показателей оценки является активной областью исследований.\n\nЧтобы выявить потенциальные различия в производительности, модель следует оценить на широком спектре наборов данных, охватывающих разные языки, домены, качество звука и демографические характеристики говорящих. Результаты дезагрегированной оценки дадут более четкое представление о сильных и слабых сторонах модели.",
    "answer_Japanese": "openai/whisper-base モデルに関して提供されている参考資料に基づいて、モデルの動作とパフォーマンスに影響を与える可能性があるいくつかの主要な予測可能な特性を次に示します。\n\n1. 言語とトレーニング データの量: 特定の言語でのモデルのパフォーマンスは、その言語で利用可能なトレーニング データの量と強く相関します。現在の事前トレーニング データセットは英語が多く含まれているため、モデルはリソースの少ない言語に比べて英語でのパフォーマンスが向上すると考えられます。\n\n2. ドメインとコンテキスト: モデルは、トレーニング データで過小評価されているドメイン固有の用語やコンテキストに苦戦する可能性があります。特定のドメインからの高品質の教師付きデータセットを微調整すると、パフォーマンスの向上に役立つ可能性があります。\n\n3. オーディオ品質とノイズ: ノイズの多い音声や、パブやレストランなどの困難な環境からの音声を扱う場合、モデルのパフォーマンスが低下する可能性があります。ホワイト ノイズやパブ ノイズの追加に対するモデルの堅牢性をテストすると、パフォーマンスの差が明らかになる可能性があります。\n\n4. 話者人口統計: [さらなる情報が必要] 参考文献は、年齢、性別、アクセントなどの話者人口統計間のパフォーマンスの格差について直接言及していません。\n\n5. トランスクリプトの書式設定: モデルのゼロショット パフォーマンスは、トレーニング中にデータセット固有の書式設定の例を観察しないため、データセット間のトランスクリプトの書式設定の変動によって影響を受ける可能性があります。\n\n6. 評価指標: WER などの標準的な評価指標は、特にゼロショット モデルの場合、人間の判断と必ずしも適切に相関するとは限りません。より良い評価指標の開発は、活発な研究分野です。\n\n潜在的なパフォーマンスの差異を明らかにするには、さまざまな言語、ドメイン、オーディオ品質、話者人口統計をカバーするさまざまなデータセットでモデルを評価する必要があります。細分化された評価結果により、モデルの長所と短所がより明確にわかります。"
  },
  {
    "question": "testing_metrics",
    "chain": "generation",
    "answer": "Based on the references provided, the key metrics and considerations for evaluating the openai/whisper-base model are:\n\n1. Word Error Rate (WER) is the primary metric, but it has limitations as it penalizes innocuous differences in transcript style. To mitigate this, extensive text standardization is done before calculating WER, especially for zero-shot models like Whisper. (References 1-3)\n\n2. Character Error Rate (CER) is used for some languages, particularly in the Common Voice and Fleurs datasets.\n\n3. BLEU scores are also calculated in some cases, as mentioned in Reference 9, but details are in the appendix of the full paper. [More Information Needed]\n\n4. To assess how close Whisper's performance is to human level, professional human transcripts were obtained for a subset of the Kincaid46 dataset and compared to Whisper and other models.\n\n5. Whisper's performance is compared to open-source models like NVIDIA STT Conformer-CTC Large as well as commercial ASR services.\n\nIn summary, WER is the primary metric, supplemented by CER for certain languages, with efforts made to standardize the text to make the metrics more meaningful. Human-level performance and comparisons to other open-source and commercial models provide additional context for Whisper's performance.",
    "answer_Chinese": "根据提供的参考资料，评估 openai/whisper-base 模型的关键指标和注意事项是：\n\n1. 单词错误率 (WER) 是主要指标，但它有局限性，因为它会惩罚抄本风格中无害的差异。为了缓解这种情况，在计算 WER 之前需要进行广泛的文本标准化，特别是对于像 Whisper 这样的零样本模型。 （参考文献1-3）\n\n2. 字符错误率 (CER) 用于某些语言，特别是在 Common Voice 和 Fleurs 数据集中。\n\n3. 在某些情况下也会计算BLEU分数，如参考文献9中提到的，但详细信息在全文的附录中。 [需要更多信息]\n\n4. 为了评估 Whisper 的性能与人类水平的接近程度，我们获得了 Kincaid46 数据集的子集的专业人类转录本，并与 Whisper 和其他模型进行了比较。\n\n5. Whisper 的性能与 NVIDIA STT Conformer-CTC Large 等开源模型以及商业 ASR 服务进行了比较。\n\n总之，WER 是主要指标，某些语言的 CER 为补充，并努力标准化文本以使指标更有意义。人类水平的性能以及与其他开源和商业模型的比较为 Whisper 的性能提供了额外的背景。",
    "answer_Arabic": "بناءً على المراجع المقدمة، فإن المقاييس والاعتبارات الرئيسية لتقييم نموذج openai/whisper-base هي:\n\n1. معدل خطأ الكلمات (WER) هو المقياس الأساسي، ولكن له قيود لأنه يعاقب الاختلافات الحميدة في نمط النص. للتخفيف من ذلك، يتم إجراء توحيد شامل للنص قبل حساب WER، خاصة بالنسبة للنماذج ذات اللقطة الصفرية مثل Whisper. (المراجع 1-3)\n\n2. يُستخدم معدل خطأ الأحرف (CER) في بعض اللغات، خاصة في مجموعات بيانات Common Voice وFleurs.\n\n3. يتم أيضًا حساب درجات BLEU في بعض الحالات، كما هو مذكور في المرجع 9، ولكن التفاصيل موجودة في ملحق الورقة الكاملة. [مطلوب مزيد من المعلومات]\n\n4. لتقييم مدى قرب أداء Whisper من المستوى البشري، تم الحصول على نصوص بشرية احترافية لمجموعة فرعية من مجموعة بيانات Kincaid46 ومقارنتها بـ Whisper والنماذج الأخرى.\n\n5. تتم مقارنة أداء Whisper بالنماذج مفتوحة المصدر مثل NVIDIA STT Conformer-CTC Large بالإضافة إلى خدمات ASR التجارية.\n\nباختصار، WER هو المقياس الأساسي، الذي تكمله CER لبعض اللغات، مع الجهود المبذولة لتوحيد النص لجعل المقاييس أكثر فائدة. يوفر الأداء على المستوى البشري والمقارنات مع النماذج الأخرى مفتوحة المصدر والتجارية سياقًا إضافيًا لأداء Whisper.",
    "answer_French": "Sur la base des références fournies, les mesures et considérations clés pour évaluer le modèle openai/whisper-base sont :\n\n1. Le taux d’erreur sur les mots (WER) est la mesure principale, mais il présente des limites car il pénalise les différences inoffensives dans le style de transcription. Pour atténuer cela, une normalisation approfondie du texte est effectuée avant de calculer le WER, en particulier pour les modèles sans tir comme Whisper. (Références 1-3)\n\n2. Le taux d'erreur sur les caractères (CER) est utilisé pour certaines langues, notamment dans les ensembles de données Common Voice et Fleurs.\n\n3. Les scores BLEU sont également calculés dans certains cas, comme mentionné dans la référence 9, mais les détails figurent en annexe du document complet. [Plus d'informations nécessaires]\n\n4. Pour évaluer à quel point les performances de Whisper sont proches du niveau humain, des transcriptions humaines professionnelles ont été obtenues pour un sous-ensemble de l'ensemble de données Kincaid46 et comparées à Whisper et à d'autres modèles.\n\n5. Les performances de Whisper sont comparées à celles de modèles open source tels que NVIDIA STT Conformer-CTC Large ainsi qu'aux services ASR commerciaux.\n\nEn résumé, le WER est la métrique principale, complétée par le CER pour certaines langues, avec des efforts déployés pour standardiser le texte afin de rendre les métriques plus significatives. Les performances au niveau humain et les comparaisons avec d'autres modèles open source et commerciaux fournissent un contexte supplémentaire pour les performances de Whisper.",
    "answer_Russian": "На основе предоставленных ссылок ключевыми показателями и соображениями для оценки модели openai/Whisper-base являются:\n\n1. Частота ошибок в словах (WER) является основным показателем, но у него есть ограничения, поскольку он наказывает за безобидные различия в стиле стенограммы. Чтобы смягчить это, перед расчетом WER выполняется обширная стандартизация текста, особенно для моделей с нулевым выстрелом, таких как Whisper. (Ссылки 1-3)\n\n2. Коэффициент ошибок символов (CER) используется для некоторых языков, особенно в наборах данных Common Voice и Fleurs.\n\n3. В некоторых случаях также рассчитываются баллы BLEU, как указано в справке 9, но подробности приведены в приложении к полной версии документа. [Требуется дополнительная информация]\n\n4. Чтобы оценить, насколько производительность Whisper близка к человеческому уровню, были получены профессиональные человеческие транскрипты для подмножества набора данных Kincaid46 и сравнены с Whisper и другими моделями.\n\n5. Производительность Whisper сравнивается с моделями с открытым исходным кодом, такими как NVIDIA STT Conformer-CTC Large, а также с коммерческими сервисами ASR.\n\nТаким образом, WER является основным показателем, дополненным CER для некоторых языков, при этом прилагаются усилия по стандартизации текста, чтобы сделать показатели более значимыми. Производительность на человеческом уровне и сравнение с другими моделями с открытым исходным кодом и коммерческими моделями обеспечивают дополнительный контекст для производительности Whisper.",
    "answer_Japanese": "提供されている参考資料に基づいて、openai/whisper-base モデルを評価するための主要な指標と考慮事項は次のとおりです。\n\n1. Word Error Rate (WER) が主要な指標ですが、トランスクリプトのスタイルにおける無害な違いにペナルティを課すため、制限があります。これを軽減するために、特に Whisper のようなゼロショット モデルの場合、WER を計算する前に広範なテキストの標準化が行われます。 （参考１～３）\n\n2. 文字誤り率 (CER) は、一部の言語、特に Common Voice および Fleurs データセットで使用されます。\n\n3. 参考文献 9 に記載されているように、場合によっては BLEU スコアも計算されますが、詳細は論文全文の付録に記載されています。 [さらに詳しい情報が必要です]\n\n4. Whisper のパフォーマンスが人間のレベルにどの程度近いかを評価するために、Kincaid46 データセットのサブセットに対して専門的な人間の転写結果が取得され、Whisper および他のモデルと比較されました。\n\n5. Whisper のパフォーマンスは、NVIDIA STT Conformer-CTC Large などのオープンソース モデルや商用 ASR サービスと比較されます。\n\n要約すると、WER が主要な指標であり、特定の言語では CER によって補完され、指標をより意味のあるものにするためにテキストを標準化する取り組みが行われています。人間レベルのパフォーマンスと、他のオープンソースおよび商用モデルとの比較により、Whisper のパフォーマンスに関する追加のコンテキストが提供されます。"
  },
  {
    "question": "results",
    "chain": "generation",
    "answer": "Evaluation Results of openai/whisper-base:\n\nPerformance (Accuracy):\n- English speech recognition performance is close to human-level and shows diminishing returns with increasing model size.\n- Performance continues to increase with model size for multilingual speech recognition, speech translation, and language identification.\n- Achieves an average 55.2% relative error reduction compared to a supervised LibriSpeech model with similar performance when evaluated on other speech recognition datasets.\n- [More Information Needed] for exact accuracy metrics.\n\nPerformance (WER/CER):\n- Performance varies widely depending on the language.\n- [More Information Needed] for exact WER/CER metrics for openai/whisper-base. References only provide metrics for large-v3 and large-v2 models.\n\nPerformance (Language Identification):\n- Underperforms supervised SOTA on the Fleurs dataset by 13.6%.\n- Achieves 80.3% accuracy on the 82 overlapping languages in the Fleurs dataset.\n\nRobustness:\n- Designed to be a single robust model that works reliably across domains, tasks, and languages without dataset-specific fine-tuning.\n\nScalability:\n- Trained on one of the largest ever datasets in supervised speech recognition with 680,000 hours of labeled audio.\n- Performance scales with dataset size, but shows diminishing returns from 54,000 to 680,000 hours.\n\n[More Information Needed] for Efficiency, Fairness, and Interpretability.",
    "answer_Chinese": "openai/whisper-base的评测结果：\n\n性能（准确度）：\n- 英语语音识别性能接近人类水平，但随着模型大小的增加，收益递减。\n- 多语言语音识别、语音翻译和语言识别的性能随着模型大小的增加而不断提高。\n- 在其他语音识别数据集上进行评估时，与具有类似性能的监督 LibriSpeech 模型相比，平均相对误差减少了 55.2%。\n- [需要更多信息]以获得准确的准确度指标。\n\n性能（WER/CER）：\n- 性能因语言而异。\n- [需要更多信息] 了解 openai/whisper-base 的确切 WER/CER 指标。参考仅提供大型 v3 和大型 v2 模型的指标。\n\n性能（语言识别）：\n- 在 Fleurs 数据集上的监督 SOTA 表现较差 13.6%。\n- Fleurs 数据集中 82 种重叠语言的准确率达到 80.3%。\n\n鲁棒性：\n- 设计为单一强大的模型，可以跨领域、任务和语言可靠地工作，无需针对数据集进行微调。\n\n可扩展性：\n- 在监督语音识别领域有史以来最大的数据集之一上进行了训练，其中包含 680,000 小时的标记音频。\n- 性能随数据集大小而变化，但显示出从 54,000 小时到 680,000 小时的收益递减。\n\n[需要更多信息]以提高效率、公平性和可解释性。",
    "answer_Arabic": "نتائج تقييم قاعدة openai/whisper:\n\nالأداء (الدقة):\n- أداء التعرف على الكلام باللغة الإنجليزية قريب من المستوى البشري ويظهر عوائد متناقصة مع زيادة حجم النموذج.\n- يستمر الأداء في الزيادة مع حجم النموذج للتعرف على الكلام متعدد اللغات، وترجمة الكلام، وتحديد اللغة.\n- يحقق متوسط ​​تقليل الأخطاء النسبية بنسبة 55.2% مقارنة بنموذج LibriSpeech الخاضع للإشراف مع أداء مماثل عند تقييمه على مجموعات بيانات أخرى للتعرف على الكلام.\n- [يلزم مزيد من المعلومات] للحصول على مقاييس الدقة الدقيقة.\n\nالأداء (WER/CER):\n- يختلف الأداء بشكل كبير حسب اللغة.\n- [يلزم مزيد من المعلومات] للحصول على مقاييس WER/CER الدقيقة لقاعدة openai/whisper. توفر المراجع فقط مقاييس للنماذج الكبيرة v3 والكبيرة v2.\n\nالأداء (تحديد اللغة):\n- كان أداء SOTA الخاضع للإشراف في مجموعة بيانات Fleurs أقل بنسبة 13.6%.\n- يحقق دقة بنسبة 80.3% في 82 لغة متداخلة في مجموعة بيانات Fleurs.\n\nالمتانة:\n- تم تصميمه ليكون نموذجًا واحدًا قويًا يعمل بشكل موثوق عبر المجالات والمهام واللغات دون إجراء ضبط دقيق لمجموعة بيانات محددة.\n\nقابلية التوسع:\n- تم التدريب على واحدة من أكبر مجموعات البيانات على الإطلاق في مجال التعرف على الكلام الخاضع للإشراف مع 680.000 ساعة من الصوت المسمى.\n- مقاييس الأداء بحجم مجموعة البيانات، ولكنها تظهر عوائد متناقصة من 54000 إلى 680000 ساعة.\n\n[يلزم المزيد من المعلومات] لتحقيق الكفاءة والإنصاف وقابلية التفسير.",
    "answer_French": "Résultats de l'évaluation d'openai/whisper-base :\n\nPerformances (précision) :\n- Les performances de reconnaissance vocale en anglais sont proches du niveau humain et montrent des rendements décroissants avec l'augmentation de la taille du modèle.\n- Les performances continuent d'augmenter avec la taille du modèle pour la reconnaissance vocale multilingue, la traduction vocale et l'identification des langues.\n- Permet d'obtenir une réduction d'erreur relative moyenne de 55,2 % par rapport à un modèle LibriSpeech supervisé avec des performances similaires lorsqu'il est évalué sur d'autres ensembles de données de reconnaissance vocale.\n- [Plus d'informations nécessaires] pour des mesures de précision exactes.\n\nPerformances (WER/CER) :\n- Les performances varient considérablement en fonction de la langue.\n- [Plus d'informations nécessaires] pour les métriques WER/CER exactes pour openai/whisper-base. Les références fournissent uniquement des métriques pour les modèles large-v3 et large-v2.\n\nPerformance (identification de la langue) :\n- Sous-performe le SOTA supervisé sur l'ensemble de données Fleurs de 13,6 %.\n- Atteint une précision de 80,3 % sur les 82 langues qui se chevauchent dans l'ensemble de données Fleurs.\n\nRobustesse :\n- Conçu pour être un modèle unique et robuste qui fonctionne de manière fiable dans tous les domaines, tâches et langages sans ajustement spécifique à l'ensemble de données.\n\nÉvolutivité :\n- Formé sur l'un des plus grands ensembles de données jamais créés en matière de reconnaissance vocale supervisée avec 680 000 heures d'audio étiqueté.\n- Les performances évoluent en fonction de la taille de l'ensemble de données, mais affichent des rendements décroissants de 54 000 à 680 000 heures.\n\n[Plus d'informations nécessaires] pour l'efficacité, l'équité et l'interprétabilité.",
    "answer_Russian": "Результаты оценки openai/whisper-base:\n\nПроизводительность (точность):\n- Производительность распознавания английской речи близка к человеческому уровню и демонстрирует снижение эффективности с увеличением размера модели.\n- Производительность продолжает расти с увеличением размера модели для многоязычного распознавания речи, перевода речи и идентификации языка.\n- Достигает среднего снижения относительной ошибки на 55,2% по сравнению с контролируемой моделью LibriSpeech с аналогичной производительностью при оценке на других наборах данных распознавания речи.\n- [Требуется дополнительная информация] для точных показателей точности.\n\nПроизводительность (WER/CER):\n- Производительность сильно варьируется в зависимости от языка.\n- [Требуется дополнительная информация] для точных показателей WER/CER для openai/whisper-base. В ссылках представлены метрики только для моделей big-v3 и big-v2.\n\nПроизводительность (идентификация языка):\n- Отстает от контролируемого SOTA на наборе данных Fleurs на 13,6%.\n- Достигается точность 80,3% на 82 перекрывающихся языках в наборе данных Fleurs.\n\nПрочность:\n- Создана как единая надежная модель, которая надежно работает в разных областях, задачах и языках без тонкой настройки для конкретного набора данных.\n\nМасштабируемость:\n- Прошел обучение на одном из крупнейших в истории наборов данных по контролируемому распознаванию речи с 680 000 часов размеченного аудио.\n- Производительность увеличивается в зависимости от размера набора данных, но показывает уменьшающуюся отдачу от 54 000 до 680 000 часов.\n\n[Требуется дополнительная информация] для эффективности, справедливости и интерпретируемости.",
    "answer_Japanese": "openai/whisper-baseの評価結果：\n\n性能 (精度):\n- 英語の音声認識パフォーマンスは人間のレベルに近く、モデルのサイズが大きくなるにつれて効果が減少することがわかります。\n- 多言語音声認識、音声翻訳、言語識別のモデル サイズに応じてパフォーマンスが向上し続けています。\n- 他の音声認識データセットで評価した場合、同様のパフォーマンスを持つ教師付き LibriSpeech モデルと比較して、平均 55.2% の相対誤差削減を達成します。\n- 正確な精度のメトリクスについては [追加情報が必要]。\n\nパフォーマンス (WER/CER):\n- パフォーマンスは言語によって大きく異なります。\n- openai/whisper-base の正確な WER/CER メトリクスについては [追加情報が必要]。リファレンスでは、大規模 v3 モデルおよび大規模 v2 モデルのメトリクスのみが提供されます。\n\nパフォーマンス (言語識別):\n- Fleurs データセットの教師あり SOTA のパフォーマンスを 13.6% も下回ります。\n- Fleurs データセット内の重複する 82 言語で 80.3% の精度を達成。\n\n堅牢性:\n- データセット固有の微調整を行わなくても、ドメイン、タスク、言語間で確実に動作する単一の堅牢なモデルになるように設計されています。\n\nスケーラビリティ:\n- 680,000 時間のラベル付き音声を含む、教師あり音声認識における史上最大のデータセットの 1 つでトレーニングされました。\n- パフォーマンスはデータセットのサイズに応じて変化しますが、54,000 時間から 680,000 時間に利益が減少します。\n\n効率、公平性、解釈可能性のために[さらに情報が必要]。"
  },
  {
    "question": "results_summary",
    "chain": "generation",
    "answer": "Based on the provided references, here is a summary of the evaluation results for the openai/whisper-base model:\n\n- Whisper's performance varies widely depending on the language. WER (word error rate) and CER (character error rate) metrics are used to evaluate performance on datasets like Common Voice 15 and Fleurs. Full results can be found in the paper's Appendix D.1, D.2 and D.4.\n\n- With the exception of English speech recognition, Whisper's performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification. English performance may be saturating as it approaches human-level. \n\n- Whisper performs better than compared models on most datasets, especially those with uncommon words, even though some commercial systems may have been trained on the public datasets.\n\n- To quantify how close Whisper is to human-level performance, professional human transcripts were obtained for 25 recordings from the Kincaid46 dataset. [More Information Needed] on the results of this comparison.\n\n- Common errors include truncation, hallucination, and struggling with the first/last few words. Fine-tuning on high-quality data or using reinforcement learning could help reduce these.\n\n- [More Information Needed] for code examples, as no code blocks were provided in the references.\n\nIn summary, Whisper shows strong zero-shot generalization across languages, with performance scaling with model size, but still has room for improvement especially on low-resource languages and in reducing specific error types.",
    "answer_Chinese": "根据提供的参考资料，以下是 openai/whisper-base 模型的评估结果摘要：\n\n- Whisper 的性能因语言而异。 WER（单词错误率）和 ​​CER（字符错误率）指标用于评估 Common Voice 15 和 Fleurs 等数据集的性能。完整的结果可以在论文的附录 D.1、D.2 和 D.4 中找到。\n\n- 除英语语音识别外，Whisper 的性能在多语言语音识别、语音翻译和语言识别方面随着模型大小的增加而不断提高。英语表现在接近人类水平时可能会饱和。 \n\n- Whisper 在大多数数据集上的表现优于比较模型，尤其是那些不常见单词的数据集，即使某些商业系统可能已经在公共数据集上进行了训练。\n\n- 为了量化 Whisper 与人类水平表现的接近程度，我们从 Kincaid46 数据集中获取了 25 个录音的专业人类转录本。 [需要更多信息] 关于此比较的结果。\n\n- 常见错误包括截断、幻觉以及难以理解第一个/最后几个单词。对高质量数据进行微调或使用强化学习可以帮助减少这些问题。\n\n- [需要更多信息] 获取代码示例，因为参考文献中未提供代码块。\n\n总之，Whisper 显示了跨语言的强大零样本泛化能力，性能随模型大小而扩展，但仍然有改进的空间，特别是在低资源语言和减少特定错误类型方面。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، فيما يلي ملخص لنتائج التقييم لنموذج openai/whisper-base:\n\n- يختلف أداء Whisper بشكل كبير حسب اللغة. تُستخدم مقاييس WER (معدل خطأ الكلمات) وCER (معدل خطأ الأحرف) لتقييم الأداء في مجموعات البيانات مثل Common Voice 15 وFleurs. يمكن العثور على النتائج الكاملة في ملحق الورقة د.1، د.2، د.4.\n\n- باستثناء التعرف على الكلام باللغة الإنجليزية، يستمر أداء Whisper في الزيادة مع حجم النموذج عبر التعرف على الكلام متعدد اللغات، وترجمة الكلام، وتحديد اللغة. قد يكون أداء اللغة الإنجليزية مشبعًا عندما يقترب من المستوى البشري. \n\n- أداء Whisper أفضل من النماذج المقارنة في معظم مجموعات البيانات، خاصة تلك التي تحتوي على كلمات غير شائعة، على الرغم من أن بعض الأنظمة التجارية ربما تم تدريبها على مجموعات البيانات العامة.\n\n- لتحديد مدى قرب Whisper من الأداء على المستوى البشري، تم الحصول على نصوص بشرية احترافية لـ 25 تسجيلاً من مجموعة بيانات Kincaid46. [بحاجة إلى مزيد من المعلومات] حول نتائج هذه المقارنة.\n\n- الأخطاء الشائعة تشمل الاقتطاع، والهلوسة، والصراع مع الكلمات القليلة الأولى والأخيرة. يمكن أن يساعد ضبط البيانات عالية الجودة أو استخدام التعلم المعزز في تقليل هذه المخاطر.\n\n- [يلزم مزيد من المعلومات] للحصول على أمثلة التعليمات البرمجية، حيث لم يتم توفير كتل التعليمات البرمجية في المراجع.\n\nباختصار، يُظهر Whisper تعميمًا قويًا صفريًا عبر اللغات، مع قياس الأداء بحجم النموذج، ولكن لا يزال لديه مجال للتحسين خاصة في اللغات منخفضة الموارد وفي تقليل أنواع أخطاء محددة.",
    "answer_French": "Sur la base des références fournies, voici un résumé des résultats de l'évaluation du modèle openai/whisper-base :\n\n- Les performances de Whisper varient considérablement selon la langue. Les mesures WER (taux d'erreur de mots) et CER (taux d'erreur de caractères) sont utilisées pour évaluer les performances sur des ensembles de données tels que Common Voice 15 et Fleurs. Les résultats complets peuvent être consultés dans les annexes D.1, D.2 et D.4 du document.\n\n- À l'exception de la reconnaissance vocale en anglais, les performances de Whisper continuent d'augmenter avec la taille du modèle en matière de reconnaissance vocale multilingue, de traduction vocale et d'identification de la langue. La performance en anglais peut être saturée à mesure qu’elle se rapproche du niveau humain. \n\n- Whisper fonctionne mieux que les modèles comparés sur la plupart des ensembles de données, en particulier ceux contenant des mots peu courants, même si certains systèmes commerciaux peuvent avoir été formés sur les ensembles de données publics.\n\n- Pour quantifier à quel point Whisper est proche des performances au niveau humain, des transcriptions humaines professionnelles ont été obtenues pour 25 enregistrements de l'ensemble de données Kincaid46. [Plus d'informations nécessaires] sur les résultats de cette comparaison.\n\n- Les erreurs courantes incluent la troncature, les hallucinations et les difficultés avec les premiers/derniers mots. Un réglage fin sur des données de haute qualité ou le recours à l’apprentissage par renforcement pourraient contribuer à les réduire.\n\n- [Plus d'informations nécessaires] pour les exemples de code, car aucun bloc de code n'a été fourni dans les références.\n\nEn résumé, Whisper présente une forte généralisation sans tir dans tous les langages, avec une mise à l'échelle des performances en fonction de la taille du modèle, mais il peut encore être amélioré, en particulier sur les langages à faibles ressources, et dans la réduction des types d'erreurs spécifiques.",
    "answer_Russian": "На основе предоставленных ссылок ниже приводится краткое изложение результатов оценки модели openai/whisper-base:\n\n- Производительность Whisper сильно различается в зависимости от языка. Метрики WER (частота ошибок в словах) и CER (частота ошибок в символах) используются для оценки производительности таких наборов данных, как Common Voice 15 и Fleurs. Полные результаты можно найти в Приложениях D.1, D.2 и D.4 к документу.\n\n- За исключением распознавания английской речи, производительность Whisper продолжает расти с увеличением размера модели в области многоязычного распознавания речи, перевода речи и идентификации языка. Производительность английского языка может быть насыщенной, поскольку она приближается к человеческому уровню. \n\n- Whisper работает лучше, чем сравниваемые модели, на большинстве наборов данных, особенно с редко встречающимися словами, хотя некоторые коммерческие системы могли быть обучены на общедоступных наборах данных.\n\n- Чтобы количественно оценить, насколько Whisper близок к производительности человеческого уровня, были получены профессиональные человеческие транскрипты для 25 записей из набора данных Kincaid46. [Требуется дополнительная информация] о результатах этого сравнения.\n\n- Распространенные ошибки включают усечение, галлюцинации и трудности с первыми/последними словами. Точная настройка на высококачественных данных или использование обучения с подкреплением могут помочь уменьшить их.\n\n- [Требуется дополнительная информация] для примеров кода, поскольку в ссылках не указаны блоки кода.\n\nПодводя итог, Whisper демонстрирует сильную нулевую генерализацию между языками с масштабированием производительности в зависимости от размера модели, но все еще имеет возможности для улучшения, особенно на языках с низким уровнем ресурсов, и в уменьшении количества конкретных типов ошибок.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base モデルの評価結果の概要を以下に示します。\n\n- Whisper のパフォーマンスは言語によって大きく異なります。 WER (単語誤り率) および CER (文字誤り率) メトリクスは、Common Voice 15 や Fleurs などのデータセットのパフォーマンスを評価するために使用されます。完全な結果は、この論文の付録 D.1、D.2、および D.4 に記載されています。\n\n- 英語の音声認識を除いて、Whisper のパフォーマンスは、多言語音声認識、音声翻訳、および言語識別全体にわたってモデル サイズとともに向上し続けています。英語のパフォーマンスは人間のレベルに近づき飽和しつつあるのかもしれません。 \n\n- 一部の商用システムが公開データセットでトレーニングされている場合でも、Whisper はほとんどのデータセット、特に一般的ではない単語を含むデータセットで比較モデルよりも優れたパフォーマンスを発揮します。\n\n- Whisper が人間レベルのパフォーマンスにどの程度近づいているかを定量化するために、Kincaid46 データセットから 25 件の録音について専門的な人間のトランスクリプトを取得しました。この比較の結果については [さらに詳しい情報が必要です]。\n\n- よくある間違いには、切り捨て、幻覚、最初と最後のいくつかの単語を理解するのに苦労するなどがあります。高品質のデータを微調整するか、強化学習を使用すると、これらを軽減できる可能性があります。\n\n- リファレンスにはコード ブロックが提供されていないため、コード例については [詳細情報が必要]。\n\n要約すると、Whisper は、モデル サイズに応じてパフォーマンスが拡張され、言語間で強力なゼロショット一般化を示していますが、特に低リソース言語や特定のエラー タイプの削減においてはまだ改善の余地があります。"
  },
  {
    "question": "model_examination",
    "chain": "generation",
    "answer": "Based on the provided references about the Whisper model, here is an experimental section on explainability and interpretability for the openai/whisper-base model:\n\nThe openai/whisper-base model is a robust speech recognition system that aims to generalize well across domains, tasks, and languages without the need for dataset-specific fine-tuning. To better understand the model's behavior and decision-making process, several areas could be explored:\n\n1. Ablation studies: Investigating the contributions of the encoder and decoder components to the model's robustness by training variations such as a decoder-less CTC model or combining the Whisper encoder with existing speech recognition encoders like wav2vec 2.0. [More Information Needed]\n\n2. Error analysis: Examining the types of errors made by the model, such as perception-related errors, repetition loops, hallucinations, and incorrect speaker name predictions. This analysis could help identify areas for improvement in the model's decoding strategies and training data. [More Information Needed]\n\n3. Language-specific performance: Analyzing the relationship between the amount of training data for each language and the model's performance in that language. This could provide insights into the model's generalization capabilities and highlight the need for more diverse training data. [More Information Needed]\n\n4. Fine-tuning impact: Studying the effects of fine-tuning the model on high-quality supervised datasets for specific domains or tasks. This could help understand the model's adaptability and potential for further improvement in specialized applications. [More Information Needed]\n\n5. Interpretability techniques: Applying interpretability methods such as attention visualization, saliency maps, or feature attribution to understand which parts of the input audio the model focuses on when making predictions. [More Information Needed]\n\nBy exploring these areas, we aim to gain a deeper understanding of the openai/whisper-base model's inner workings, identify its strengths and weaknesses, and discover potential avenues for improvement in terms of robustness, generalization, and interpretability.",
    "answer_Chinese": "根据提供的有关 Whisper 模型的参考资料，以下是关于 openai/whisper-base 模型的可解释性和可解释性的实验部分：\n\nopenai/whisper-base 模型是一个强大的语音识别系统，旨在跨领域、任务和语言进行良好泛化，而不需要针对数据集进行微调。为了更好地理解模型的行为和决策过程，可以探索几个领域：\n\n1. 消融研究：通过训练无解码器 CTC 模型等变体或将 Whisper 编码器与现有语音识别编码器（如 wav2vec 2.0）相结合，研究编码器和解码器组件对模型鲁棒性的贡献。 [需要更多信息]\n\n2. 错误分析：检查模型所犯错误的类型，例如与感知相关的错误、重复循环、幻觉和不正确的说话人姓名预测。此分析可以帮助确定模型解码策略和训练数据中需要改进的领域。 [需要更多信息]\n\n3.特定语言的性能：分析每种语言的训练数据量与模型在该语言中的性能之间的关系。这可以深入了解模型的泛化能力，并强调对更多样化的训练数据的需求。 [需要更多信息]\n\n4.微调影响：研究微调模型对特定领域或任务的高质量监督数据集的影响。这可以帮助了解模型的适应性以及在专业应用中进一步改进的潜力。 [需要更多信息]\n\n5. 可解释性技术：应用注意力可视化、显着性图或特征归因等可解释性方法来了解模型在进行预测时关注输入音频的哪些部分。 [需要更多信息]\n\n通过探索这些领域，我们的目标是更深入地了解 openai/whisper-base 模型的内部工作原理，确定其优点和缺点，并发现在鲁棒性、泛化性和可解释性方面改进的潜在途径。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج Whisper، إليك قسم تجريبي حول إمكانية الشرح وقابلية التفسير لنموذج openai/whisper-base:\n\nيعد نموذج openai/whisper-base نظامًا قويًا للتعرف على الكلام يهدف إلى التعميم بشكل جيد عبر المجالات والمهام واللغات دون الحاجة إلى ضبط دقيق لمجموعة بيانات محددة. لفهم سلوك النموذج وعملية اتخاذ القرار بشكل أفضل، يمكن استكشاف عدة مجالات:\n\n1. دراسات الاجتثاث: التحقيق في مساهمات مكونات التشفير ووحدة فك التشفير في قوة النموذج من خلال تدريب الاختلافات مثل نموذج CTC بدون وحدة فك ترميز أو الجمع بين أداة تشفير Whisper وأجهزة تشفير التعرف على الكلام الموجودة مثل wav2vec 2.0. [مطلوب مزيد من المعلومات]\n\n2. تحليل الأخطاء: دراسة أنواع الأخطاء التي يحدثها النموذج، مثل الأخطاء المتعلقة بالإدراك، وحلقات التكرار، والهلوسة، والتنبؤات غير الصحيحة لاسم المتحدث. يمكن أن يساعد هذا التحليل في تحديد مجالات التحسين في استراتيجيات فك تشفير النموذج وبيانات التدريب. [مطلوب مزيد من المعلومات]\n\n3. الأداء الخاص باللغة: تحليل العلاقة بين كمية بيانات التدريب لكل لغة وأداء النموذج في تلك اللغة. يمكن أن يوفر هذا نظرة ثاقبة لقدرات تعميم النموذج ويسلط الضوء على الحاجة إلى بيانات تدريب أكثر تنوعًا. [مطلوب مزيد من المعلومات]\n\n4. تأثير الضبط الدقيق: دراسة تأثيرات الضبط الدقيق للنموذج على مجموعات البيانات عالية الجودة الخاضعة للإشراف لمجالات أو مهام محددة. يمكن أن يساعد هذا في فهم قدرة النموذج على التكيف وإمكانية إجراء المزيد من التحسين في التطبيقات المتخصصة. [مطلوب مزيد من المعلومات]\n\n5. تقنيات التفسير: تطبيق أساليب التفسير مثل تصور الانتباه، أو خرائط البروز، أو إسناد الميزة لفهم أجزاء الصوت المدخلة التي يركز عليها النموذج عند إجراء التنبؤات. [مطلوب مزيد من المعلومات]\n\nمن خلال استكشاف هذه المجالات، نهدف إلى الحصول على فهم أعمق لطريقة العمل الداخلية لنموذج openai/whisper-base، وتحديد نقاط القوة والضعف فيه، واكتشاف السبل المحتملة للتحسين من حيث القوة والتعميم وقابلية التفسير.",
    "answer_French": "Sur la base des références fournies sur le modèle Whisper, voici une section expérimentale sur l'explicabilité et l'interprétabilité du modèle openai/whisper-base :\n\nLe modèle openai/whisper-base est un système de reconnaissance vocale robuste qui vise à bien se généraliser à travers les domaines, les tâches et les langues sans avoir besoin d'un réglage précis spécifique à un ensemble de données. Pour mieux comprendre le comportement du modèle et le processus de prise de décision, plusieurs domaines pourraient être explorés :\n\n1. Études d'ablation : étudier les contributions des composants d'encodeur et de décodeur à la robustesse du modèle en entraînant des variantes telles qu'un modèle CTC sans décodeur ou en combinant l'encodeur Whisper avec des encodeurs de reconnaissance vocale existants comme wav2vec 2.0. [Plus d'informations nécessaires]\n\n2. Analyse des erreurs : examiner les types d'erreurs commises par le modèle, telles que les erreurs liées à la perception, les boucles de répétition, les hallucinations et les prédictions incorrectes du nom du locuteur. Cette analyse pourrait aider à identifier les domaines à améliorer dans les stratégies de décodage et les données de formation du modèle. [Plus d'informations nécessaires]\n\n3. Performances spécifiques à la langue : analyser la relation entre la quantité de données de formation pour chaque langue et les performances du modèle dans cette langue. Cela pourrait donner un aperçu des capacités de généralisation du modèle et mettre en évidence la nécessité de données de formation plus diversifiées. [Plus d'informations nécessaires]\n\n4. Impact du réglage fin : étudier les effets du réglage fin du modèle sur des ensembles de données supervisés de haute qualité pour des domaines ou des tâches spécifiques. Cela pourrait aider à comprendre l’adaptabilité du modèle et son potentiel d’amélioration dans les applications spécialisées. [Plus d'informations nécessaires]\n\n5. Techniques d'interprétabilité : application de méthodes d'interprétabilité telles que la visualisation de l'attention, les cartes de saillance ou l'attribution de fonctionnalités pour comprendre sur quelles parties de l'audio d'entrée le modèle se concentre lors de la réalisation de prédictions. [Plus d'informations nécessaires]\n\nEn explorant ces domaines, nous visons à acquérir une compréhension plus approfondie du fonctionnement interne du modèle openai/whisper-base, à identifier ses forces et ses faiblesses et à découvrir des pistes d'amélioration potentielles en termes de robustesse, de généralisation et d'interprétabilité.",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель Whisper, вот экспериментальный раздел, посвященный объяснимости и интерпретируемости модели openai/whisper-base:\n\nМодель openai/whisper-base — это надежная система распознавания речи, целью которой является хорошее обобщение по областям, задачам и языкам без необходимости тонкой настройки для конкретного набора данных. Чтобы лучше понять поведение модели и процесс принятия решений, можно изучить несколько областей:\n\n1. Исследования абляции: изучение вклада компонентов кодера и декодера в надежность модели путем обучения таких вариантов, как модель CTC без декодера или объединение кодера Whisper с существующими кодировщиками распознавания речи, такими как wav2vec 2.0. [Требуется дополнительная информация]\n\n2. Анализ ошибок: изучение типов ошибок, допущенных моделью, таких как ошибки, связанные с восприятием, циклы повторения, галлюцинации и неверные предсказания имени говорящего. Этот анализ может помочь определить области для улучшения стратегий декодирования модели и обучающих данных. [Требуется дополнительная информация]\n\n3. Производительность на конкретном языке: анализ взаимосвязи между объемом обучающих данных для каждого языка и производительностью модели на этом языке. Это может дать представление о возможностях модели по обобщению и подчеркнуть необходимость в более разнообразных обучающих данных. [Требуется дополнительная информация]\n\n4. Влияние тонкой настройки: изучение влияния тонкой настройки модели на высококачественные контролируемые наборы данных для конкретных областей или задач. Это может помочь понять адаптивность модели и потенциал для дальнейшего улучшения в специализированных приложениях. [Требуется дополнительная информация]\n\n5. Методы интерпретации: применение методов интерпретации, таких как визуализация внимания, карты значимости или атрибуция функций, чтобы понять, на каких частях входного аудио модель фокусируется при составлении прогнозов. [Требуется дополнительная информация]\n\nИсследуя эти области, мы стремимся получить более глубокое понимание внутренней работы модели openai/Whisper-base, выявить ее сильные и слабые стороны, а также обнаружить потенциальные пути улучшения с точки зрения надежности, обобщения и интерпретируемости.",
    "answer_Japanese": "Whisper モデルに関して提供されている参考資料に基づいて、openai/Whisper-base モデルの説明可能性と解釈可能性に関する実験セクションを以下に示します。\n\nopenai/whisper-base モデルは、データセット固有の微調整を必要とせずに、ドメイン、タスク、言語全体で適切に一般化することを目的とした堅牢な音声認識システムです。モデルの動作と意思決定プロセスをより深く理解するために、いくつかの領域を調査することができます。\n\n1. アブレーション研究: デコーダのない CTC モデルなどのバリエーションをトレーニングしたり、Whisper エンコーダを wav2vec 2.0 などの既存の音声認識エンコーダと組み合わせたりすることにより、モデルの堅牢性に対するエンコーダおよびデコーダ コンポーネントの寄与を調査します。 [さらに詳しい情報が必要です]\n\n2. エラー分析: 知覚関連のエラー、反復ループ、幻覚、不正確な話者名の予測など、モデルによって発生したエラーの種類を調査します。この分析は、モデルのデコード戦略とトレーニング データにおける改善の余地がある領域を特定するのに役立ちます。 [さらに詳しい情報が必要です]\n\n3. 言語固有のパフォーマンス: 各言語のトレーニング データの量とその言語でのモデルのパフォーマンスとの関係を分析します。これにより、モデルの一般化機能についての洞察が得られ、より多様なトレーニング データの必要性が強調される可能性があります。 [さらに詳しい情報が必要です]\n\n4. 微調整の影響: 特定のドメインまたはタスクに対する高品質の教師付きデータセットに対するモデルの微調整の影響を研究します。これは、モデルの適応性と、特殊なアプリケーションにおけるさらなる改善の可能性を理解するのに役立ちます。 [さらに詳しい情報が必要です]\n\n5. 解釈可能性の手法: アテンションの視覚化、顕著性マップ、または特徴の属性などの解釈可能性の手法を適用して、予測を行うときにモデルが入力音声のどの部分に焦点を当てているかを理解します。 [さらに詳しい情報が必要です]\n\nこれらの領域を調査することで、openai/Whisper-Base モデルの内部動作をより深く理解し、その長所と短所を特定し、堅牢性、一般化、解釈可能性の点で潜在的な改善の道を発見することを目指しています。"
  },
  {
    "question": "hardware",
    "chain": "generation",
    "answer": "Based on the provided references, the model openai/whisper-base was trained using:\n\nData parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing.\n\nHowever, the specific hardware type (e.g., GPU, TPU) is not explicitly mentioned. Therefore, for the hardware type used for training openai/whisper-base:\n\n[More Information Needed]",
    "answer_Chinese": "根据提供的参考资料，模型 openai/whisper-base 使用以下方法进行训练：\n\n使用具有动态损失缩放和激活检查点功能的 FP16 实现跨加速器的数据并行性。\n\n但是，没有明确提及具体的硬件类型（例如 GPU、TPU）。因此，对于用于训练 openai/whisper-base 的硬件类型：\n\n[需要更多信息]",
    "answer_Arabic": "بناءً على المراجع المتوفرة، تم تدريب نموذج openai/whisper-base باستخدام:\n\nتوازي البيانات عبر المسرعات باستخدام FP16 مع قياس الخسارة الديناميكي وفحص التنشيط.\n\nومع ذلك، لم يتم ذكر نوع الأجهزة المحدد (على سبيل المثال، GPU، TPU) بشكل صريح. لذلك، بالنسبة لنوع الأجهزة المستخدمة لتدريب openai/whisper-base:\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des références fournies, le modèle openai/whisper-base a été formé en utilisant :\n\nParallélisme des données entre les accélérateurs utilisant FP16 avec mise à l'échelle dynamique des pertes et points de contrôle d'activation.\n\nCependant, le type de matériel spécifique (par exemple GPU, TPU) n'est pas explicitement mentionné. Par conséquent, pour le type de matériel utilisé pour la formation openai/whisper-base :\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "На основе предоставленных ссылок модель openai/whisper-base была обучена с использованием:\n\nПараллелизм данных между ускорителями с использованием FP16 с динамическим масштабированием потерь и контрольными точками активации.\n\nОднако конкретный тип оборудования (например, графический процессор, TPU) явно не упоминается. Следовательно, для типа оборудования, используемого для обучения openai/whisper-base:\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "提供された参照に基づいて、モデル openai/whisper-base は以下を使用してトレーニングされました。\n\n動的な損失スケーリングとアクティベーション チェックポイント機能を備えた FP16 を使用した、アクセラレータ間のデータ並列処理。\n\nただし、特定のハードウェア タイプ (GPU、TPU など) については明示的には言及されていません。したがって、openai/whisper-base のトレーニングに使用されるハードウェア タイプは次のようになります。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "software",
    "chain": "generation",
    "answer": "The model openai/whisper-base is trained using Python, as evidenced by the code snippets in the references:\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\nand\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\n```\n\nThese code examples demonstrate loading and using the Whisper model in Python.",
    "answer_Chinese": "模型 openai/whisper-base 是使用 Python 进行训练的，参考文献中的代码片段证明了这一点：\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\")\n结果 = model.transcribe(\"audio.mp3\")\n打印（结果[“文本”]）\n````\n\n和\n\n````蟒蛇\n进口耳语\n\n模型 = 耳语.load_model(\"基础\")\n````\n\n这些代码示例演示了在 Python 中加载和使用 Whisper 模型。",
    "answer_Arabic": "يتم تدريب نموذج openai/whisper-base باستخدام Python، كما يتضح من مقتطفات التعليمات البرمجية في المراجع:\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\")\nالنتيجة = model.transcribe(\"audio.mp3\")\nطباعة (نتيجة [\"نص\"])\n```\n\nو\n\n```بيثون\nهمس الاستيراد\n\nالنموذج = Whisper.load_model(\"base\")\n```\n\nتوضح أمثلة التعليمات البرمجية هذه تحميل واستخدام نموذج Whisper في Python.",
    "answer_French": "Le modèle openai/whisper-base est entraîné à l'aide de Python, comme en témoignent les extraits de code dans les références :\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\")\nrésultat = modèle.transcribe(\"audio.mp3\")\nimprimer(résultat[\"texte\"])\n```\n\net\n\n```python\nimporter un murmure\n\nmodèle = murmure.load_model(\"base\")\n```\n\nCes exemples de code démontrent le chargement et l'utilisation du modèle Whisper en Python.",
    "answer_Russian": "Модель openai/whisper-base обучается с использованием Python, о чем свидетельствуют фрагменты кода в ссылках:\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\")\nрезультат = model.transcribe(\"audio.mp3\")\nпечать(результат[\"текст\"])\n```\n\nи\n\n``` питон\nимпортный шепот\n\nмодель = шепот.load_model(\"база\")\n```\n\nЭти примеры кода демонстрируют загрузку и использование модели Whisper в Python.",
    "answer_Japanese": "モデル openai/whisper-base は、参考文献のコード スニペットで明らかなように、Python を使用してトレーニングされています。\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\")\nresult = model.transcribe(\"audio.mp3\")\nprint(結果[\"テキスト\"])\n「」\n\nそして\n\n「」パイソン\n輸入ささやき\n\nモデル = ささやき.load_model(\"ベース\")\n「」\n\nこれらのコード例は、Python での Whisper モデルの読み込みと使用を示しています。"
  },
  {
    "question": "hours_used",
    "chain": "generation",
    "answer": "Based on the information provided in the references, the Whisper models were trained for 2^20 updates, which is between two and three passes over the dataset. However, the specific training time for the openai/whisper-base model is not explicitly mentioned.\n\n[More Information Needed] on the exact training time for the openai/whisper-base model.",
    "answer_Chinese": "根据参考文献中提供的信息，Whisper 模型接受了 2^20 次更新训练，即对数据集进行两到三遍更新。但并未明确提及openai/whisper-base模型的具体训练时间。\n\n[需要更多信息] 关于 openai/whisper-base 模型的确切训练时间。",
    "answer_Arabic": "استنادًا إلى المعلومات المقدمة في المراجع، تم تدريب نماذج Whisper على تحديثات 2^20، والتي تتراوح بين مرتين وثلاث مرات عبر مجموعة البيانات. ومع ذلك، لم يتم ذكر وقت التدريب المحدد لنموذج openai/whisper-base بشكل صريح.\n\n[يلزم مزيد من المعلومات] حول وقت التدريب المحدد لنموذج openai/whisper-base.",
    "answer_French": "Sur la base des informations fournies dans les références, les modèles Whisper ont été entraînés pour 2 ^ 20 mises à jour, soit entre deux et trois passages sur l'ensemble de données. Cependant, le temps de formation spécifique pour le modèle openai/whisper-base n'est pas explicitement mentionné.\n\n[Plus d'informations nécessaires] sur le temps de formation exact pour le modèle openai/Whisper-base.",
    "answer_Russian": "На основе информации, представленной в ссылках, модели Whisper были обучены для 2^20 обновлений, что составляет от двух до трех проходов по набору данных. Однако конкретное время обучения для модели openai/whisper-base явно не упоминается.\n\n[Требуется дополнительная информация] о точном времени обучения для модели openai/whisper-base.",
    "answer_Japanese": "参考資料で提供される情報に基づいて、Whisper モデルは 2^20 更新、つまりデータセット上で 2 ～ 3 回のパスでトレーニングされました。ただし、openai/Whisper ベース モデルの具体的なトレーニング時間については明示的には言及されていません。\n\nopenai/whisper-base モデルの正確なトレーニング時間に関する [詳細情報が必要]。"
  },
  {
    "question": "cloud_provider",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not mention the specific cloud provider used for training the openai/whisper-base model. More information would be needed to determine which cloud provider, if any, was utilized during the model's training process.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献没有提及用于训练 openai/whisper-base 模型的特定云提供商。需要更多信息来确定模型训练过程中使用的云提供商（如果有）。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تذكر المراجع المقدمة موفر السحابة المحدد المستخدم لتدريب نموذج openai/whisper-base. ستكون هناك حاجة إلى مزيد من المعلومات لتحديد موفر السحابة، إن وجد، الذي تم استخدامه أثناء عملية التدريب على النموذج.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne mentionnent pas le fournisseur de cloud spécifique utilisé pour former le modèle openai/whisper-base. Plus d'informations seraient nécessaires pour déterminer quel fournisseur de cloud, le cas échéant, a été utilisé pendant le processus de formation du modèle.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nВ предоставленных ссылках не упоминается конкретный поставщик облачных услуг, используемый для обучения модели openai/whisper-base. Потребуется дополнительная информация, чтобы определить, какой поставщик облачных услуг (если таковой имеется) использовался в процессе обучения модели.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、openai/whisper-base モデルのトレーニングに使用される特定のクラウド プロバイダーについては言及されていません。モデルのトレーニング プロセス中にどのクラウド プロバイダーが使用されたか (存在する場合) を判断するには、より多くの情報が必要になります。"
  },
  {
    "question": "co2_emitted",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain any information about the amount of carbon emitted during the training of the openai/whisper-base model. To answer this question, more specific details would be needed, such as the hardware used for training, the energy consumption of the training process, and the carbon intensity of the electricity used.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献不包含有关 openai/whisper-base 模型训练期间碳排放量的任何信息。要回答这个问题，需要更具体的细节，例如训练所用的硬件、训练过程的能耗以及所用电力的碳强度。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على أي معلومات حول كمية الكربون المنبعثة أثناء تدريب نموذج openai/whisper-base. للإجابة على هذا السؤال، ستكون هناك حاجة إلى تفاصيل أكثر تحديدًا، مثل الأجهزة المستخدمة للتدريب، واستهلاك الطاقة في عملية التدريب، وكثافة الكربون في الكهرباء المستخدمة.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent aucune information sur la quantité de carbone émise lors de la formation du modèle openai/whisper-base. Pour répondre à cette question, des détails plus précis seraient nécessaires, tels que le matériel utilisé pour la formation, la consommation énergétique du processus de formation et l'intensité carbone de l'électricité utilisée.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат никакой информации о количестве углерода, выбрасываемого во время обучения модели openai/whisper-base. Чтобы ответить на этот вопрос, потребуются более конкретные детали, такие как оборудование, используемое для обучения, энергопотребление тренировочного процесса и углеродоемкость используемой электроэнергии.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、openai/Whisper-Base モデルのトレーニング中に放出される炭素の量に関する情報は含まれていません。この質問に答えるには、トレーニングに使用されるハードウェア、トレーニング プロセスのエネルギー消費量、使用される電力の炭素強度など、より具体的な詳細が必要になります。"
  },
  {
    "question": "model_specs",
    "chain": "generation",
    "answer": "Model Architecture:\n- Encoder-decoder Transformer architecture\n- Audio is re-sampled to 16,000 Hz\n- 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds\n- Input is globally scaled to be between -1 and 1 with approximately zero mean across the pre-training dataset\n- Encoder processes input with a small stem consisting of two convolution layers (filter width of 3, GELU activation, second layer has stride of two)\n- Sinusoidal position embeddings are added to the output of the stem\n- Encoder Transformer blocks are applied, using pre-activation residual blocks\n- Final layer normalization is applied to the encoder output\n- Decoder uses learned position embeddings and tied input-output token representations\n- Encoder and decoder have the same width and number of transformer blocks\n\nModel Objective:\nThe Whisper model is trained on various speech processing tasks, including:\n- Multilingual speech recognition\n- Speech translation\n- Spoken language identification\n- Voice activity detection\n\nThese tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.\n\nThe model is trained on a large dataset of diverse audio and aims to be a general-purpose speech recognition model that can perform multiple tasks without the need for fine-tuning.",
    "answer_Chinese": "模型架构：\n- 编码器-解码器 Transformer 架构\n- 音频重新采样至 16,000 Hz\n- 80 通道对数幅度 Mel 频谱图表示在 25 毫秒窗口上计算，步长为 10 毫秒\n- 输入全局缩放至 -1 到 1 之间，预训练数据集中的均值近似为零\n- 编码器使用由两个卷积层组成的小茎处理输入（滤波器宽度为 3，GELU 激活，第二层的步长为 2）\n- 正弦位置嵌入被添加到茎的输出中\n- 使用预激活残差块应用编码器变压器块\n- 最终层归一化应用于编码器输出\n- 解码器使用学习的位置嵌入和绑定的输入输出标记表示\n- 编码器和解码器具有相同的宽度和变压器块数量\n\n模型目标：\nWhisper 模型经过各种语音处理任务的训练，包括：\n- 多语言语音识别\n- 语音翻译\n- 口语识别\n- 语音活动检测\n\n这些任务联合表示为由解码器预测的标记序列，从而允许单个模型取代传统语音处理管道的许多阶段。多任务训练格式使用一组特殊标记作为任务说明符或分类目标。\n\n该模型在不同音频的大型数据集上进行训练，旨在成为一种通用语音识别模型，无需微调即可执行多项任务。",
    "answer_Arabic": "العمارة النموذجية:\n- بنية محول التشفير وفك التشفير\n- يتم إعادة أخذ عينات الصوت إلى 16000 هرتز\n- يتم حساب التمثيل الطيفي Mel ذو حجم السجل 80 قناة على نوافذ تبلغ 25 مللي ثانية مع خطوة قدرها 10 مللي ثانية\n- يتم قياس المدخلات عالميًا لتكون بين -1 و1 بمتوسط ​​صفر تقريبًا عبر مجموعة بيانات ما قبل التدريب\n- يقوم المشفر بمعالجة المدخلات باستخدام جذع صغير يتكون من طبقتين ملتويتين (عرض المرشح 3، تنشيط GELU، الطبقة الثانية بها خطوة من اثنتين)\n- تضاف التضمينات الموضعية الجيبية إلى مخرج الجذع\n- يتم تطبيق كتل محول التشفير باستخدام الكتل المتبقية قبل التنشيط\n- يتم تطبيق تسوية الطبقة النهائية على مخرجات التشفير\n- يستخدم جهاز فك التشفير تضمينات الموضع المستفادة وتمثيلات رمزية للإدخال والمخرجات المرتبطة\n- التشفير وفك التشفير لهما نفس العرض وعدد كتل المحولات\n\nهدف النموذج:\nيتم تدريب نموذج Whisper على مهام معالجة الكلام المختلفة، بما في ذلك:\n- التعرف على الكلام متعدد اللغات\n- ترجمة الكلام\n- تحديد اللغة المنطوقة\n- كشف النشاط الصوتي\n\nيتم تمثيل هذه المهام بشكل مشترك كسلسلة من الرموز المميزة التي سيتم التنبؤ بها بواسطة وحدة فك التشفير، مما يسمح لنموذج واحد باستبدال العديد من مراحل خط أنابيب معالجة الكلام التقليدي. يستخدم تنسيق التدريب على المهام المتعددة مجموعة من الرموز المميزة التي تعمل كمحددات المهام أو أهداف التصنيف.\n\nتم تدريب النموذج على مجموعة كبيرة من البيانات الصوتية المتنوعة ويهدف إلى أن يكون نموذجًا للتعرف على الكلام للأغراض العامة يمكنه أداء مهام متعددة دون الحاجة إلى الضبط الدقيق.",
    "answer_French": "Architecture du modèle :\n- Architecture du transformateur codeur-décodeur\n- L'audio est rééchantillonné à 16 000 Hz\n- La représentation du spectrogramme Mel à 80 canaux de magnitude log est calculée sur des fenêtres de 25 millisecondes avec une foulée de 10 millisecondes\n- L'entrée est globalement mise à l'échelle pour être comprise entre -1 et 1 avec une moyenne approximativement nulle sur l'ensemble de données de pré-entraînement\n- L'encodeur traite l'entrée avec une petite tige composée de deux couches de convolution (largeur de filtre de 3, activation GELU, la deuxième couche a une foulée de deux)\n- Des intégrations de position sinusoïdale sont ajoutées à la sortie de la tige\n- Les blocs de transformateur d'encodeur sont appliqués, en utilisant des blocs résiduels de pré-activation\n- La normalisation de la couche finale est appliquée à la sortie du codeur\n- Le décodeur utilise des intégrations de positions apprises et des représentations de jetons d'entrée-sortie liées\n- L'encodeur et le décodeur ont la même largeur et le même nombre de blocs de transformateur\n\nObjectif du modèle :\nLe modèle Whisper est formé sur diverses tâches de traitement de la parole, notamment :\n- Reconnaissance vocale multilingue\n- Traduction vocale\n- Identification de la langue parlée\n- Détection d'activité vocale\n\nCes tâches sont représentées conjointement sous la forme d'une séquence de jetons devant être prédits par le décodeur, permettant à un modèle unique de remplacer de nombreuses étapes d'un pipeline de traitement vocal traditionnel. Le format de formation multitâche utilise un ensemble de jetons spéciaux qui servent de spécificateurs de tâches ou de cibles de classification.\n\nLe modèle est formé sur un vaste ensemble de données audio diverses et vise à être un modèle de reconnaissance vocale à usage général capable d'effectuer plusieurs tâches sans avoir besoin de réglages précis.",
    "answer_Russian": "Модельная архитектура:\n- Трансформаторная архитектура кодер-декодер\n- Звук повторно дискретизируется до 16 000 Гц.\n- 80-канальное представление Mel-спектрограммы логарифмической величины рассчитывается в 25-миллисекундных окнах с шагом 10 миллисекунд.\n- Входные данные глобально масштабируются в диапазоне от -1 до 1 с примерно нулевым средним значением для набора данных перед обучением.\n- Кодер обрабатывает входные данные с помощью небольшого стержня, состоящего из двух слоев свертки (ширина фильтра 3, активация GELU, шаг второго слоя равен двум)\n- К выходным данным штока добавлены вложения синусоидального положения.\n- Применяются блоки Encoder Transformer с использованием остаточных блоков предварительной активации.\n- К выходным данным кодера применяется окончательная нормализация слоя.\n- Декодер использует встраивание изученных позиций и связанные представления токенов ввода-вывода.\n- Кодер и декодер имеют одинаковую ширину и количество блоков преобразователя.\n\nЦель модели:\nМодель Whisper обучена решению различных задач по обработке речи, в том числе:\n- Многоязычное распознавание речи\n- Перевод речи\n- Идентификация разговорного языка\n- Обнаружение голосовой активности\n\nЭти задачи совместно представлены в виде последовательности токенов, которые должны быть предсказаны декодером, что позволяет одной модели заменить многие этапы традиционного конвейера обработки речи. В формате многозадачного обучения используется набор специальных токенов, которые служат спецификаторами задач или целями классификации.\n\nМодель обучена на большом наборе данных разнообразного аудио и призвана стать моделью распознавания речи общего назначения, которая может выполнять несколько задач без необходимости тонкой настройки.",
    "answer_Japanese": "モデルのアーキテクチャ:\n- エンコーダー/デコーダートランスフォーマーのアーキテクチャ\n- オーディオは 16,000 Hz に再サンプリングされます\n- 80 チャネルの対数振幅メル スペクトログラム表現は、10 ミリ秒のストライドで 25 ミリ秒のウィンドウで計算されます。\n- 入力は -1 から 1 の間になるようにグローバルにスケーリングされ、事前トレーニング データセット全体の平均はほぼゼロになります\n- エンコーダーは、2 つの畳み込み層で構成される小さなステムで入力を処理します (フィルター幅 3、GELU アクティブ化、2 番目の層のストライドは 2)\n- 正弦波位置の埋め込みがステムの出力に追加されます\n- Encoder Transformer ブロックは、アクティベーション前の残差ブロックを使用して適用されます。\n- 最終層の正規化がエンコーダー出力に適用されます\n- デコーダーは学習された位置埋め込みと関連付けられた入出力トークン表現を使用します\n- エンコーダとデコーダのトランスブロックの幅と数は同じです\n\nモデルの目的:\nWhisper モデルは、次のようなさまざまな音声処理タスクでトレーニングされます。\n- 多言語音声認識\n- 音声翻訳\n- 音声言語の識別\n- 音声アクティビティの検出\n\nこれらのタスクは、デコーダによって予測されるトークンのシーケンスとして結合して表現されるため、単一のモデルで従来の音声処理パイプラインの多くのステージを置き換えることができます。マルチタスク トレーニング形式では、タスク指定子または分類ターゲットとして機能する特別なトークンのセットが使用されます。\n\nこのモデルは、多様な音声の大規模なデータセットでトレーニングされ、微調整を必要とせずに複数のタスクを実行できる汎用音声認識モデルを目指しています。"
  },
  {
    "question": "compute_infrastructure",
    "chain": "generation",
    "answer": "Based on the provided references, here is the information about the compute infrastructure used for training the openai/whisper-base model:\n\nThe models were trained using:\n- Data parallelism across accelerators\n- FP16 precision with dynamic loss scaling\n- Activation checkpointing\n\nThe model was trained for 2^20 updates, which is between two and three passes over the dataset of 680,000 hours.\n\nBatch size: 256\n\nOptimizer:\n- AdamW optimizer\n- Gradient norm clipping\n- Linear learning rate decay to zero after a warmup over the first 2048 updates\n\n[More Information Needed] on the exact compute infrastructure details such as number and type of accelerators used, training time, etc.",
    "answer_Chinese": "根据提供的参考资料，以下是有关用于训练 openai/whisper-base 模型的计算基础设施的信息：\n\n模型使用以下方法进行训练：\n- 跨加速器的数据并行性\n- 具有动态损耗缩放功能的 FP16 精度\n- 激活检查点\n\n该模型接受了 2^20 次更新训练，即对 680,000 小时的数据集进行了两到三遍。\n\n批量大小：256\n\n优化器：\n- AdamW优化器\n- 梯度范数裁剪\n- 在前 2048 个更新的预热后，线性学习率衰减至零\n\n[需要更多信息]有关确切的计算基础设施详细信息，例如所使用的加速器的数量和类型、训练时间等。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك المعلومات حول البنية الأساسية للحوسبة المستخدمة لتدريب نموذج openai/whisper-base:\n\nتم تدريب النماذج باستخدام:\n- توازي البيانات عبر المسرعات\n- دقة FP16 مع قياس الخسارة الديناميكي\n- تفعيل نقاط التفتيش\n\nتم تدريب النموذج على تحديثات 2^20، أي ما بين مرورين إلى ثلاثة تمريرات على مجموعة البيانات البالغة 680,000 ساعة.\n\nحجم الدفعة: 256\n\nمحسن:\n- محسن AdamW\n- لقطة قاعدة التدرج\n- انخفاض معدل التعلم الخطي إلى الصفر بعد الإحماء خلال تحديثات 2048 الأولى\n\n[مطلوب مزيد من المعلومات] حول تفاصيل البنية التحتية للحوسبة الدقيقة مثل عدد ونوع المسرعات المستخدمة ووقت التدريب وما إلى ذلك.",
    "answer_French": "Sur la base des références fournies, voici les informations sur l'infrastructure de calcul utilisée pour entraîner le modèle openai/whisper-base :\n\nLes modèles ont été formés en utilisant :\n- Parallélisme des données entre les accélérateurs\n- Précision FP16 avec mise à l'échelle dynamique des pertes\n- Points de contrôle d'activation\n\nLe modèle a été entraîné pour 2 ^ 20 mises à jour, soit entre deux et trois passes sur l'ensemble de données de 680 000 heures.\n\nTaille du lot : 256\n\nOptimiseur :\n- Optimiseur AdamW\n- Découpage de la norme de dégradé\n- Le taux d'apprentissage linéaire diminue jusqu'à zéro après un échauffement au cours des 2048 premières mises à jour\n\n[Plus d'informations nécessaires] sur les détails exacts de l'infrastructure de calcul tels que le nombre et le type d'accélérateurs utilisés, la durée de la formation, etc.",
    "answer_Russian": "На основании предоставленных ссылок ниже представлена ​​информация о вычислительной инфраструктуре, используемой для обучения модели openai/whisper-base:\n\nМодели обучались с использованием:\n- Параллелизм данных между ускорителями\n- Точность FP16 с динамическим масштабированием потерь\n- Контрольная точка активации\n\nМодель была обучена для 2^20 обновлений, что составляет от двух до трех проходов по набору данных объемом 680 000 часов.\n\nРазмер пакета: 256\n\nОптимизатор:\n- оптимизатор AdamW\n- Отсечение нормы градиента\n- Линейное снижение скорости обучения до нуля после прогрева в течение первых 2048 обновлений.\n\n[Требуется дополнительная информация] о точных деталях вычислительной инфраструктуры, таких как количество и тип используемых ускорителей, время обучения и т. д.",
    "answer_Japanese": "提供された参考資料に基づいて、openai/whisper-base モデルのトレーニングに使用されるコンピューティング インフラストラクチャに関する情報を次に示します。\n\nモデルは以下を使用してトレーニングされました。\n- アクセラレータ間でのデータ並列処理\n- 動的損失スケーリングを備えた FP16 精度\n- アクティベーションチェックポイント\n\nモデルは 2^20 更新用にトレーニングされました。これは、680,000 時間のデータセットに対して 2 ～ 3 回のパスに相当します。\n\nバッチサイズ: 256\n\nオプティマイザ：\n- AdamW オプティマイザー\n- 勾配ノルムクリッピング\n- 最初の 2048 アップデートのウォームアップ後、線形学習率がゼロに減衰します。\n\n使用するアクセラレータの数と種類、トレーニング時間など、コンピューティング インフラストラクチャの正確な詳細に関する [追加情報が必要]"
  }
]