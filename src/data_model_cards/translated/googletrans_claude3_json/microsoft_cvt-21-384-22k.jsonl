[
  {
    "question": "summary",
    "chain": "generation",
    "answer": "The microsoft/cvt-21-384-22k model is a Convolutional vision Transformer (CvT) that combines the benefits of convolutional neural networks (CNNs) and Transformers for computer vision tasks. It achieves state-of-the-art performance on ImageNet-1k with fewer parameters and lower FLOPs compared to other Vision Transformers and ResNets, and maintains performance gains when pretrained on the larger ImageNet-22k dataset and fine-tuned on downstream tasks.",
    "answer_Chinese": "microsoft/cvt-21-384-22k 模型是一种卷积视觉 Transformer (CvT)，它结合了卷积神经网络 (CNN) 和 Transformer 在计算机视觉任务中的优点。与其他 Vision Transformer 和 ResNet 相比，它在 ImageNet-1k 上以更少的参数和更低的 FLOP 实现了最先进的性能，并且在更大的 ImageNet-22k 数据集上进行预训练并在下游任务上进行微调时保持了性能增益。",
    "answer_Arabic": "نموذج microsoft/cvt-21-384-22k عبارة عن محول رؤية تلافيفي (CvT) يجمع بين فوائد الشبكات العصبية التلافيفية (CNN) والمحولات لمهام رؤية الكمبيوتر. إنه يحقق أداءً متطورًا على ImageNet-1k مع معلمات أقل وFLOPs أقل مقارنةً بمحولات Vision وResNets الأخرى، ويحافظ على مكاسب الأداء عند تدريبه مسبقًا على مجموعة بيانات ImageNet-22k الأكبر وضبطه بدقة على المهام النهائية.",
    "answer_French": "Le modèle Microsoft/cvt-21-384-22k est un transformateur de vision convolutive (CvT) qui combine les avantages des réseaux neuronaux convolutifs (CNN) et des transformateurs pour les tâches de vision par ordinateur. Il atteint des performances de pointe sur ImageNet-1k avec moins de paramètres et des FLOP inférieurs par rapport aux autres Vision Transformers et ResNets, et maintient les gains de performances lorsqu'il est pré-entraîné sur le plus grand ensemble de données ImageNet-22k et affiné sur les tâches en aval.",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k представляет собой преобразователь сверточного зрения (CvT), который сочетает в себе преимущества сверточных нейронных сетей (CNN) и преобразователей для задач компьютерного зрения. Он обеспечивает высочайшую производительность на ImageNet-1k с меньшим количеством параметров и меньшими FLOP по сравнению с другими Vision Transformers и ResNet, а также сохраняет прирост производительности при предварительном обучении на более крупном наборе данных ImageNet-22k и точной настройке для последующих задач.",
    "answer_Japanese": "microsoft/cvt-21-384-22k モデルは、畳み込みニューラル ネットワーク (CNN) とコンピューター ビジョン タスク用のトランスフォーマーの利点を組み合わせた畳み込みビジョン トランスフォーマー (CvT) です。他のビジョン トランスフォーマーや ResNet と比較して、少ないパラメーターと低い FLOP で ImageNet-1k 上で最先端のパフォーマンスを実現し、大規模な ImageNet-22k データセットで事前トレーニングし、ダウンストリーム タスクで微調整した場合でもパフォーマンスの向上を維持します。"
  },
  {
    "question": "description",
    "chain": "generation",
    "answer": "Here are the basic details about the model microsoft/cvt-21-384-22k:\n\nModel Architecture:\n- Uses a Convolutional vision Transformer (CvT) architecture \n- Employs a multi-stage hierarchy design with 3 stages total\n- Each stage has a Convolutional Token Embedding layer followed by Convolutional Transformer Blocks\n- Convolutional Token Embedding is implemented as a convolution with overlapping patches that reshapes tokens to a 2D spatial grid\n- Convolutional Projection uses depth-wise separable convolutions for query, key, and value embeddings in the Transformer blocks\n- Progressive downsampling of sequence length and increase in feature dimension across stages\n- Position embeddings are not used\n\nTraining:\n- Pre-trained on the ImageNet-22k dataset\n- [More Information Needed] on other training details \n\nParameters:\n- This CvT-21 model likely has around 31.54M parameters based on the details provided\n\nImportant Disclaimers:\n[More Information Needed]\n\nPlease reach out if you need any other details to complete the model card. I'm happy to track down additional information from the team.",
    "answer_Chinese": "以下是有关型号 microsoft/cvt-21-384-22k 的基本详细信息：\n\n模型架构：\n- 使用卷积视觉 Transformer (CvT) 架构 \n- 采用多级层次结构设计，共3级\n- 每个阶段都有一个卷积令牌嵌入层，后面是卷积变压器块\n- 卷积令牌嵌入被实现为具有重叠补丁的卷积，将令牌重塑为 2D 空间网格\n- 卷积投影使用深度可分离卷积在 Transformer 块中进行查询、键和值嵌入\n- 序列长度的渐进下采样和跨阶段特征维度的增加\n- 不使用位置嵌入\n\n训练：\n- 在 ImageNet-22k 数据集上进行预训练\n- [需要更多信息]有关其他培训细节 \n\n参数：\n- 根据提供的详细信息，此 CvT-21 模型可能有大约 3154 万个参数\n\n重要免责声明：\n[需要更多信息]\n\n如果您需要任何其他详细信息来完成模型卡，请联系我们。我很高兴从团队中找到更多信息。",
    "answer_Arabic": "فيما يلي التفاصيل الأساسية حول الطراز microsoft/cvt-21-384-22k:\n\nالعمارة النموذجية:\n- يستخدم بنية محول الرؤية التلافيفية (CvT). \n- يستخدم تصميم هرمي متعدد المراحل بإجمالي 3 مراحل\n- تحتوي كل مرحلة على طبقة تضمين رمزية تلافيفية متبوعة بكتل محولات تلافيفية\n- يتم تنفيذ تضمين الرمز المميز كالتفاف مع تصحيحات متداخلة تعيد تشكيل الرموز المميزة إلى شبكة مكانية ثنائية الأبعاد\n- يستخدم الإسقاط التلافيفي التفافات قابلة للفصل من حيث العمق للاستعلام والمفتاح ودمج القيمة في كتل المحولات\n- الاختزال التدريجي لطول التسلسل وزيادة أبعاد الميزة عبر المراحل\n- لا يتم استخدام تضمينات الموضع\n\nتمرين:\n- تم تدريبه مسبقًا على مجموعة بيانات ImageNet-22k\n- [يلزم مزيد من المعلومات] حول تفاصيل التدريب الأخرى \n\nحدود:\n- من المحتمل أن يحتوي طراز CvT-21 على حوالي 31.54 مليون معلمة بناءً على التفاصيل المقدمة\n\nإخلاء المسؤولية الهامة:\n[مطلوب مزيد من المعلومات]\n\nيُرجى التواصل معنا إذا كنت بحاجة إلى أي تفاصيل أخرى لإكمال بطاقة النموذج. يسعدني تتبع المعلومات الإضافية من الفريق.",
    "answer_French": "Voici les détails de base sur le modèle Microsoft/cvt-21-384-22k :\n\nArchitecture du modèle :\n- Utilise une architecture de transformateur de vision convolutive (CvT) \n- Utilise une conception hiérarchique à plusieurs étapes avec 3 étapes au total\n- Chaque étape comporte une couche d'intégration de jetons convolutifs suivie de blocs de transformateurs convolutifs\n- L'intégration convolutive de jetons est implémentée sous forme de convolution avec des correctifs superposés qui remodèlent les jetons en une grille spatiale 2D.\n- La projection convolutive utilise des convolutions séparables en profondeur pour les intégrations de requêtes, de clés et de valeurs dans les blocs Transformer.\n- Sous-échantillonnage progressif de la longueur de la séquence et augmentation de la dimension des fonctionnalités à travers les étapes\n- Les intégrations de position ne sont pas utilisées\n\nEntraînement:\n- Pré-entraîné sur le jeu de données ImageNet-22k\n- [Plus d'informations nécessaires] sur d'autres détails de la formation \n\nParamètres:\n- Ce modèle CvT-21 a probablement environ 31,54 millions de paramètres sur la base des détails fournis\n\nAvertissements importants :\n[Plus d'informations nécessaires]\n\nVeuillez nous contacter si vous avez besoin d'autres détails pour compléter la carte modèle. Je suis heureux de retrouver des informations supplémentaires auprès de l’équipe.",
    "answer_Russian": "Вот основные сведения о модели microsoft/cvt-21-384-22k:\n\nМодельная архитектура:\n- Использует архитектуру преобразователя сверточного видения (CvT). \n- Использует многоступенчатую иерархию, состоящую всего из 3 этапов.\n- Каждый этап имеет слой внедрения сверточных токенов, за которым следуют блоки сверточных преобразователей.\n- Встраивание сверточных токенов реализовано как свертка с перекрывающимися патчами, которая преобразует токены в двумерную пространственную сетку.\n- Сверточная проекция использует разделяемые по глубине свертки для встраивания запросов, ключей и значений в блоки Transformer.\n- Прогрессивное уменьшение длины последовательности и увеличение размера объекта на разных этапах.\n- Вложения позиций не используются.\n\nОбучение:\n- Предварительное обучение на наборе данных ImageNet-22k.\n- [Требуется дополнительная информация] о других деталях обучения. \n\nПараметры:\n- Судя по предоставленной информации, эта модель CvT-21, вероятно, имеет около 31,54 млн параметров.\n\nВажные заявления об отказе от ответственности:\n[Требуется дополнительная информация]\n\nЕсли вам нужны какие-либо другие данные для заполнения карты модели, свяжитесь с нами. Я буду рад получить дополнительную информацию от команды.",
    "answer_Japanese": "Microsoft/cvt-21-384-22k モデルの基本的な詳細は次のとおりです。\n\nモデルのアーキテクチャ:\n- Convolutional Vision Transformer (CvT) アーキテクチャを使用 \n・合計3ステージの多段階層設計を採用\n- 各ステージには、畳み込みトークン埋め込み層とそれに続く畳み込み変換ブロックがあります。\n- 畳み込みトークン埋め込みは、トークンを 2D 空間グリッドに再形成するオーバーラップするパッチを備えた畳み込みとして実装されます。\n- 畳み込み射影は、Transformer ブロック内のクエリ、キー、および値の埋め込みに深さ方向に分離可能な畳み込みを使用します。\n- シーケンス長の段階的なダウンサンプリングとステージ全体にわたる特徴次元の増加\n- 位置の埋め込みは使用されません\n\nトレーニング：\n- ImageNet-22k データセットで事前トレーニング済み\n- その他のトレーニングの詳細に関する [さらなる情報が必要] \n\nパラメーター：\n- この CvT-21 モデルには、提供された詳細に基づくと、約 3,154 万のパラメータがある可能性があります\n\n重要な免責事項:\n[さらに詳しい情報が必要です]\n\nモデル カードを完成させるためにその他の詳細が必要な場合は、お問い合わせください。チームからの追加情報を追跡できることを嬉しく思います。"
  },
  {
    "question": "funded_by",
    "chain": "generation",
    "answer": "Based on the provided references, there is no explicit mention of the people or organizations that fund the microsoft/cvt-21-384-22k model project. The references discuss technical details of the model architecture and performance, but do not specify funding sources.\n\n[More Information Needed]",
    "answer_Chinese": "根据提供的参考资料，没有明确提及资助 microsoft/cvt-21-384-22k 模型项目的人员或组织。这些参考文献讨论了模型架构和性能的技术细节，但没有指定资金来源。\n\n[需要更多信息]",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، لا يوجد أي ذكر صريح للأشخاص أو المؤسسات التي تمول المشروع النموذجي microsoft/cvt-21-384-22k. تناقش المراجع التفاصيل الفنية لبنية النموذج والأداء، ولكنها لا تحدد مصادر التمويل.\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des références fournies, il n'y a aucune mention explicite des personnes ou des organisations qui financent le projet modèle Microsoft/cvt-21-384-22k. Les références discutent des détails techniques de l'architecture et des performances du modèle, mais ne précisent pas les sources de financement.\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "Судя по предоставленным ссылкам, нет явного упоминания людей или организаций, финансирующих модельный проект microsoft/cvt-21-384-22k. В ссылках обсуждаются технические детали архитектуры и производительности модели, но не указываются источники финансирования.\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "提供された参考文献に基づくと、microsoft/cvt-21-384-22k モデル プロジェクトに資金提供している人物や組織については明示的に言及されていません。参考文献では、モデルのアーキテクチャとパフォーマンスの技術的な詳細について説明していますが、資金源は特定されていません。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "shared_by",
    "chain": "generation",
    "answer": "Based on the references provided, the main contributors to the microsoft/cvt-21-384-22k model are:\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang from Microsoft.\n\nThey authored the paper \"Cvt: Introducing convolutions to vision transformers\" which introduced the CvT model architecture.\n\nThe references mention that code will be released at https://github.com/leoxiaobin/CvT, but more information would be needed to confirm if that GitHub repo specifically contains the microsoft/cvt-21-384-22k model weights and code.\n\nThe references also note that the project has adopted the Microsoft Open Source Code of Conduct and welcomes outside contributions under a Contributor License Agreement, but [More Information Needed] on if any external contributors helped make this specific model variant available online.",
    "answer_Chinese": "根据提供的参考资料，microsoft/cvt-21-384-22k 模型的主要贡献者是：\n\n来自 Microsoft 的 Haiping Wu、Bin Shaw、Noel Codella、Mengchen Liu、Xiyang Dai、Lu Yuan 和 Lei Zhang。\n\n他们撰写了论文“Cvt：将卷积引入视觉变换器”，其中介绍了 CvT 模型架构。\n\n参考文献提到代码将在 https://github.com/leoxiaobin/CvT 发布，但需要更多信息来确认 GitHub 存储库是否特别包含 microsoft/cvt-21-384-22k 模型权重和代码。\n\n参考资料还指出，该项目已采用 Microsoft 开源行为准则，并欢迎根据贡献者许可协议进行外部贡献，但 [需要更多信息] 是否有任何外部贡献者帮助在线提供此特定模型变体。",
    "answer_Arabic": "استنادًا إلى المراجع المقدمة، فإن المساهمين الرئيسيين في نموذج Microsoft/cvt-21-384-22k هم:\n\nHaiping Wu، وBin Xiao، وNoel Codella، وMengchen Liu، وXiyang Dai، وLu Yuan، وLei Zhang من Microsoft.\n\nقاموا بتأليف ورقة بعنوان \"Cvt: تقديم التلافيف لمحولات الرؤية\" والتي قدمت بنية نموذج CvT.\n\nتشير المراجع إلى أنه سيتم إصدار الكود على https://github.com/leoxiaobin/CvT، ولكن ستكون هناك حاجة إلى مزيد من المعلومات لتأكيد ما إذا كان مستودع GitHub يحتوي على أوزان نموذج microsoft/cvt-21-384-22k ورمزه على وجه التحديد.\n\nتشير المراجع أيضًا إلى أن المشروع قد اعتمد قواعد سلوك Microsoft مفتوح المصدر ويرحب بالمساهمات الخارجية بموجب اتفاقية ترخيص المساهم، ولكن [يلزم مزيد من المعلومات] حول ما إذا كان أي مساهمين خارجيين قد ساعدوا في إتاحة هذا النموذج المحدد عبر الإنترنت.",
    "answer_French": "Sur la base des références fournies, les principaux contributeurs au modèle Microsoft/cvt-21-384-22k sont :\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan et Lei Zhang de Microsoft.\n\nIls sont les auteurs de l'article « Cvt : Introducing convolutions to vision transformers » qui présente l'architecture du modèle CvT.\n\nLes références mentionnent que le code sera publié sur https://github.com/leoxiaobin/CvT, mais plus d'informations seraient nécessaires pour confirmer si ce dépôt GitHub contient spécifiquement les poids et le code du modèle Microsoft/cvt-21-384-22k.\n\nLes références notent également que le projet a adopté le code de conduite Microsoft Open Source et accueille les contributions extérieures dans le cadre d'un accord de licence de contributeur, mais [Plus d'informations nécessaires] indiquent si des contributeurs externes ont contribué à rendre cette variante de modèle spécifique disponible en ligne.",
    "answer_Russian": "Судя по предоставленным ссылкам, основными участниками модели microsoft/cvt-21-384-22k являются:\n\nХайпин Ву, Бинь Сяо, Ноэль Коделла, Мэнчен Лю, Сиянг Дай, Лу Юань и Лэй Чжан из Microsoft.\n\nОни написали статью «Cvt: введение сверток в преобразователи зрения», в которой представили архитектуру модели CvT.\n\nВ ссылках упоминается, что код будет опубликован по адресу https://github.com/leoxiaobin/CvT, но потребуется дополнительная информация, чтобы подтвердить, содержит ли этот репозиторий GitHub веса и код модели microsoft/cvt-21-384-22k.\n\nВ ссылках также отмечается, что проект принял Кодекс поведения Microsoft с открытым исходным кодом и приветствует участие внешних участников в соответствии с Лицензионным соглашением с участником, но [Необходима дополнительная информация] о том, помогли ли какие-либо внешние участники сделать этот конкретный вариант модели доступным в Интернете.",
    "answer_Japanese": "提供されている参考資料に基づくと、microsoft/cvt-21-384-22k モデルの主な貢献者は次のとおりです。\n\nMicrosoft の Haiping Wu、Bin Xiao、Noel Codella、Mengchen Liu、Xiyang Dai、Lu Yuan、Lei Zhang です。\n\n彼らは、CvT モデル アーキテクチャを紹介した論文「Cvt: ビジョン トランスフォーマーへの畳み込みの導入」を執筆しました。\n\n参考文献には、コードが https://github.com/leoxiaobin/CvT でリリースされると記載されていますが、その GitHub リポジトリに Microsoft/cvt-21-384-22k モデルの重みとコードが具体的に含まれているかどうかを確認するには、より多くの情報が必要です。\n\n参考資料には、プロジェクトが Microsoft オープンソース行動規範を採用しており、コントリビューター ライセンス契約に基づいて外部からの貢献を歓迎しているが、この特定のモデル バリアントをオンラインで利用できるようにするのに外部貢献者が協力したかどうかについては [さらなる情報が必要] であることも記載されています。"
  },
  {
    "question": "model_type",
    "chain": "generation",
    "answer": "The model microsoft/cvt-21-384-22k is a Convolutional vision Transformer (CvT) that is pretrained on the ImageNet-22k dataset using supervised learning for the computer vision modality.",
    "answer_Chinese": "模型 microsoft/cvt-21-384-22k 是一个卷积视觉 Transformer (CvT)，它使用计算机视觉模态的监督学习在 ImageNet-22k 数据集上进行预训练。",
    "answer_Arabic": "نموذج microsoft/cvt-21-384-22k عبارة عن محول رؤية تلافيفي (CvT) تم تدريبه مسبقًا على مجموعة بيانات ImageNet-22k باستخدام التعلم الخاضع للإشراف لطريقة رؤية الكمبيوتر.",
    "answer_French": "Le modèle Microsoft/cvt-21-384-22k est un transformateur de vision convolutionnelle (CvT) pré-entraîné sur l'ensemble de données ImageNet-22k à l'aide d'un apprentissage supervisé pour la modalité de vision par ordinateur.",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k представляет собой преобразователь сверточного зрения (CvT), предварительно обученный на наборе данных ImageNet-22k с использованием контролируемого обучения для модальности компьютерного зрения.",
    "answer_Japanese": "モデル microsoft/cvt-21-384-22k は、コンピューター ビジョン モダリティの教師あり学習を使用して ImageNet-22k データセットで事前トレーニングされた畳み込みビジョン トランスフォーマー (CvT) です。"
  },
  {
    "question": "language",
    "chain": "generation",
    "answer": "The model microsoft/cvt-21-384-22k is a computer vision model that processes image data, not natural human language. [More Information Needed] on the specific natural language capabilities of this model.",
    "answer_Chinese": "模型 microsoft/cvt-21-384-22k 是处理图像数据而不是自然人类语言的计算机视觉模型。 [需要更多信息] 该模型的特定自然语言功能。",
    "answer_Arabic": "النموذج microsoft/cvt-21-384-22k هو نموذج رؤية حاسوبية يعالج بيانات الصورة، وليس لغة الإنسان الطبيعية. [يلزم مزيد من المعلومات] حول إمكانيات اللغة الطبيعية المحددة لهذا النموذج.",
    "answer_French": "Le modèle Microsoft/cvt-21-384-22k est un modèle de vision par ordinateur qui traite les données d'image, et non le langage humain naturel. [Plus d'informations nécessaires] sur les capacités spécifiques de langage naturel de ce modèle.",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k — это модель компьютерного зрения, которая обрабатывает данные изображения, а не естественный человеческий язык. [Требуется дополнительная информация] о конкретных возможностях этой модели на естественном языке.",
    "answer_Japanese": "モデル microsoft/cvt-21-384-22k は、人間の自然言語ではなく画像データを処理するコンピューター ビジョン モデルです。このモデルの特定の自然言語機能については [さらに詳しい情報が必要です]。"
  },
  {
    "question": "license",
    "chain": "generation",
    "answer": "The model microsoft/cvt-21-384-22k uses the Microsoft Open Source Code of Conduct license. The link to the license is:\n\nhttps://opensource.microsoft.com/codeofconduct/\n\nThis information can be found in Reference 2:\n\n\"This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\"",
    "answer_Chinese": "型号 microsoft/cvt-21-384-22k 使用 Microsoft 开源行为准则许可证。许可证的链接是：\n\nhttps://opensource.microsoft.com/codeofconduct/\n\n此信息可在参考文献 2 中找到：\n\n“该项目已采用 [Microsoft 开源行为准则](https://opensource.microsoft.com/codeofconduct/)。”",
    "answer_Arabic": "يستخدم النموذج microsoft/cvt-21-384-22k ترخيص قواعد سلوك Microsoft مفتوح المصدر. رابط الترخيص هو :\n\nhttps://opensource.microsoft.com/codeofconduct/\n\nيمكن العثور على هذه المعلومات في المرجع 2:\n\n\"لقد اعتمد هذا المشروع [قواعد سلوك Microsoft مفتوحة المصدر](https://opensource.microsoft.com/codeofconduct/).\"",
    "answer_French": "Le modèle microsoft/cvt-21-384-22k utilise la licence Microsoft Open Source Code of Conduct. Le lien vers la licence est :\n\nhttps://opensource.microsoft.com/codeofconduct/\n\nCes informations peuvent être trouvées dans la référence 2 :\n\n\"Ce projet a adopté le [Code de conduite Microsoft Open Source](https://opensource.microsoft.com/codeofconduct/).\"",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k использует лицензию Microsoft с открытым исходным кодом. Ссылка на лицензию:\n\nhttps://opensource.microsoft.com/codeofconduct/\n\nЭту информацию можно найти в ссылке 2:\n\n«В этом проекте принят [Кодекс поведения Microsoft с открытым исходным кодом](https://opensource.microsoft.com/codeofconduct/)».",
    "answer_Japanese": "モデル microsoft/cvt-21-384-22k は、Microsoft オープンソース行動規範ライセンスを使用します。ライセンスへのリンクは次のとおりです。\n\nhttps://opensource.microsoft.com/codeofconduct/\n\nこの情報は参考資料 2 にあります。\n\n「このプロジェクトは [Microsoft オープンソース行動規範](https://opensource.microsoft.com/codeofconduct/) を採用しています。」"
  },
  {
    "question": "finetuned_from",
    "chain": "generation",
    "answer": "Based on the provided references, the model microsoft/cvt-21-384-22k is pretrained on the ImageNet-22k dataset, as mentioned in this excerpt:\n\n\"Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set.\"\n\nHowever, there is no explicit mention of the model being fine-tuned from another base model. Therefore, for the specific question:\n\n[More Information Needed]\n\nTo definitively state whether microsoft/cvt-21-384-22k is fine-tuned from another model, more information would be required beyond what is provided in the given references.",
    "answer_Chinese": "根据提供的参考资料，模型 microsoft/cvt-21-384-22k 在 ImageNet-22k 数据集上进行了预训练，如本摘录中所述：\n\n“我们的 CvT-W24 在 ImageNet-22k 上进行预训练，在 ImageNet-1k 验证集上获得了 87.7% 的 top-1 准确率。”\n\n然而，没有明确提及该模型是从另一个基础模型进行微调的。因此，对于具体问题：\n\n[需要更多信息]\n\n为了明确说明 microsoft/cvt-21-384-22k 是否是从另一个模型进行微调的，除了给定参考文献中提供的信息之外，还需要更多信息。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، تم تدريب النموذج microsoft/cvt-21-384-22k مسبقًا على مجموعة بيانات ImageNet-22k، كما هو مذكور في هذا المقتطف:\n\n\"تم تدريب CvT-W24 الخاص بنا مسبقًا على ImageNet-22k، وحصل على دقة من الدرجة الأولى بنسبة 87.7% في مجموعة قيم ImageNet-1k.\"\n\nومع ذلك، لا يوجد ذكر صريح للنموذج الذي تم تحسينه من نموذج أساسي آخر. لذلك بالنسبة للسؤال المحدد:\n\n[مطلوب مزيد من المعلومات]\n\nلتحديد ما إذا كان microsoft/cvt-21-384-22k تم ضبطه بشكل نهائي من طراز آخر، ستكون هناك حاجة إلى مزيد من المعلومات بما يتجاوز ما تم توفيره في المراجع المحددة.",
    "answer_French": "Sur la base des références fournies, le modèle Microsoft/cvt-21-384-22k est pré-entraîné sur l'ensemble de données ImageNet-22k, comme mentionné dans cet extrait :\n\n\"Pré-entraîné sur ImageNet-22k, notre CvT-W24 obtient une précision top 1 de 87,7 % sur l'ensemble de valeurs ImageNet-1k.\"\n\nCependant, il n’y a aucune mention explicite que le modèle soit affiné à partir d’un autre modèle de base. Par conséquent, pour la question spécifique :\n\n[Plus d'informations nécessaires]\n\nPour déterminer avec certitude si Microsoft/cvt-21-384-22k est optimisé à partir d'un autre modèle, davantage d'informations seraient nécessaires au-delà de ce qui est fourni dans les références données.",
    "answer_Russian": "На основе предоставленных ссылок модель microsoft/cvt-21-384-22k предварительно обучена на наборе данных ImageNet-22k, как указано в этом отрывке:\n\n«Предварительно обученный на ImageNet-22k, наш CvT-W24 достигает точности 87,7% на наборе значений ImageNet-1k».\n\nОднако нет явного упоминания о доработке модели на основе другой базовой модели. Поэтому по конкретному вопросу:\n\n[Требуется дополнительная информация]\n\nЧтобы окончательно определить, является ли microsoft/cvt-21-384-22k точной настройкой другой модели, потребуется дополнительная информация, помимо той, которая представлена ​​в данных ссылках.",
    "answer_Japanese": "この抜粋で説明されているように、提供された参照に基づいて、モデル Microsoft/cvt-21-384-22k は ImageNet-22k データセットで事前トレーニングされます。\n\n「ImageNet-22k で事前トレーニングされた当社の CvT-W24 は、ImageNet-1k val set で 87.7% というトップ 1 の精度を獲得しました。」\n\nただし、モデルが別のベースモデルから微調整されることについては明示的に言及されていません。したがって、具体的な質問については次のようになります。\n\n[さらに詳しい情報が必要です]\n\nmicrosoft/cvt-21-384-22k が別のモデルから微調整されているかどうかを明確にするには、指定された参考資料に記載されている以上の情報が必要になります。"
  },
  {
    "question": "demo_sources",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nBased on the provided references, there is no information about a demo link for the model microsoft/cvt-21-384-22k. The references mention details about the model architecture, training, and results, but do not include a link to an online demo. More information would be needed from the model developers or documentation to determine if a demo link exists.",
    "answer_Chinese": "[需要更多信息]\n\n根据提供的参考资料，没有有关模型 microsoft/cvt-21-384-22k 的演示链接的信息。这些参考文献提到了有关模型架构、训练和结果的详细信息，但不包含在线演示的链接。需要模型开发人员或文档提供更多信息来确定演示链接是否存在。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nبناءً على المراجع المتوفرة، لا توجد معلومات حول رابط تجريبي للطراز microsoft/cvt-21-384-22k. تذكر المراجع تفاصيل حول بنية النموذج والتدريب والنتائج، ولكنها لا تتضمن رابطًا لعرض توضيحي عبر الإنترنت. ستكون هناك حاجة إلى مزيد من المعلومات من مطوري النماذج أو الوثائق لتحديد ما إذا كان هناك رابط تجريبي.",
    "answer_French": "[Plus d'informations nécessaires]\n\nSur la base des références fournies, il n'existe aucune information sur un lien de démonstration pour le modèle Microsoft/cvt-21-384-22k. Les références mentionnent des détails sur l'architecture du modèle, la formation et les résultats, mais n'incluent pas de lien vers une démo en ligne. Plus d'informations seraient nécessaires de la part des développeurs de modèles ou de la documentation pour déterminer si un lien de démonstration existe.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nСудя по предоставленным ссылкам, информация о демо-ссылке для модели microsoft/cvt-21-384-22k отсутствует. В ссылках упоминаются подробности об архитектуре модели, обучении и результатах, но нет ссылки на онлайн-демонстрацию. Чтобы определить, существует ли демонстрационная ссылка, потребуется дополнительная информация от разработчиков модели или документации.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考情報に基づくと、モデル Microsoft/cvt-21-384-22k のデモ リンクに関する情報はありません。参考資料にはモデルのアーキテクチャ、トレーニング、結果についての詳細が記載されていますが、オンライン デモへのリンクは含まれていません。デモ リンクが存在するかどうかを確認するには、モデル開発者またはドキュメントから詳細な情報が必要になります。"
  },
  {
    "question": "direct_use",
    "chain": "generation",
    "answer": "The model microsoft/cvt-21-384-22k can be used for image classification tasks without requiring fine-tuning, post-processing, or plugging into a pipeline. It has been pre-trained on the large-scale ImageNet-22k dataset.\n\nTo use the model for inference:\n\n1. Install PyTorch and TorchVision if not already installed. Install other dependencies using:\n   ``` sh\n   python -m pip install -r requirements.txt --user -q\n   ```\n\n2. Run the following command to perform inference (testing) with the pre-trained model:\n   ``` sh\n   bash run.sh -t test --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n   ```\n   Replace `${PRETRAINED_MODLE_FILE}` with the path to the pre-trained model file.\n\n[More Information Needed] on the specific input format and how to obtain the classification results.\n\nThe model achieves competitive performance on various downstream tasks even without fine-tuning, thanks to its pre-training on the large-scale ImageNet-22k dataset. It obtains a top-1 accuracy of 87.7% on the ImageNet-1k validation set.\n\nFor any further assistance or information, please feel free to reach out to the model development team.",
    "answer_Chinese": "模型 microsoft/cvt-21-384-22k 可用于图像分类任务，无需微调、后处理或​​插入管道。它已经在大规模 ImageNet-22k 数据集上进行了预训练。\n\n使用模型进行推理：\n\n1. 如果尚未安装，请安装 PyTorch 和 TorchVision。使用以下命令安装其他依赖项：\n   ````嘘\n   python -m pip install -r requests.txt --user -q\n   ````\n\n2. 运行以下命令，使用预训练模型进行推理（测试）：\n   ````嘘\n   bash run.sh -t test --cfg Experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n   ````\n   将 `${PRETRAINED_MODLE_FILE}` 替换为预训练模型文件的路径。\n\n[需要更多信息]关于具体的输入格式以及如何获得分类结果。\n\n由于在大规模 ImageNet-22k 数据集上进行了预训练，该模型即使无需微调，也能在各种下游任务上实现具有竞争力的性能。它在 ImageNet-1k 验证集上获得了 87.7% 的 top-1 准确率。\n\n如需任何进一步的帮助或信息，请随时联系模型开发团队。",
    "answer_Arabic": "يمكن استخدام النموذج microsoft/cvt-21-384-22k لمهام تصنيف الصور دون الحاجة إلى الضبط الدقيق أو المعالجة اللاحقة أو التوصيل بخط أنابيب. لقد تم تدريبه مسبقًا على مجموعة بيانات ImageNet-22k واسعة النطاق.\n\nلاستخدام النموذج للاستدلال:\n\n1. قم بتثبيت PyTorch وTorchVision إذا لم يكن مثبتًا بالفعل. تثبيت التبعيات الأخرى باستخدام:\n   ``` ش\n   python -m pip install -r require.txt --user -q\n   ```\n\n2. قم بتشغيل الأمر التالي لإجراء الاستدلال (الاختبار) باستخدام النموذج المُدرب مسبقًا:\n   ``` ش\n   اختبار bash run.sh -t --cfg experiences/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n   ```\n   استبدل `${PRETRAINED_MODLE_FILE}` بالمسار إلى ملف النموذج المُدرب مسبقًا.\n\n[يلزم مزيد من المعلومات] حول تنسيق الإدخال المحدد وكيفية الحصول على نتائج التصنيف.\n\nيحقق النموذج أداءً تنافسيًا في العديد من المهام النهائية حتى بدون الضبط الدقيق، وذلك بفضل تدريبه المسبق على مجموعة بيانات ImageNet-22k واسعة النطاق. حصلت على دقة من أعلى 1 بنسبة 87.7% في مجموعة التحقق من صحة ImageNet-1k.\n\nللحصول على أي مساعدة أو معلومات إضافية، لا تتردد في التواصل مع فريق تطوير النموذج.",
    "answer_French": "Le modèle Microsoft/cvt-21-384-22k peut être utilisé pour des tâches de classification d'images sans nécessiter de réglage fin, de post-traitement ou de connexion à un pipeline. Il a été pré-entraîné sur l'ensemble de données à grande échelle ImageNet-22k.\n\nPour utiliser le modèle à des fins d'inférence :\n\n1. Installez PyTorch et TorchVision s'ils ne sont pas déjà installés. Installez d'autres dépendances en utilisant :\n   ``` merde\n   python -m pip install -r exigences.txt --user -q\n   ```\n\n2. Exécutez la commande suivante pour effectuer une inférence (test) avec le modèle pré-entraîné :\n   ``` merde\n   bash run.sh -t test --cfg experimental/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n   ```\n   Remplacez `${PRETRAINED_MODLE_FILE}` par le chemin d'accès au fichier de modèle pré-entraîné.\n\n[Plus d'informations nécessaires] sur le format d'entrée spécifique et comment obtenir les résultats de classification.\n\nLe modèle atteint des performances compétitives sur diverses tâches en aval, même sans réglage fin, grâce à sa pré-formation sur l'ensemble de données à grande échelle ImageNet-22k. Il obtient une précision top 1 de 87,7 % sur l’ensemble de validation ImageNet-1k.\n\nPour toute assistance ou information supplémentaire, n'hésitez pas à contacter l'équipe de développement de modèles.",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k можно использовать для задач классификации изображений, не требуя тонкой настройки, постобработки или подключения к конвейеру. Он был предварительно обучен на крупномасштабном наборе данных ImageNet-22k.\n\nЧтобы использовать модель для вывода:\n\n1. Установите PyTorch и TorchVision, если они еще не установлены. Установите другие зависимости, используя:\n   ``` ш\n   python -m pip install -r требования.txt --user -q\n   ```\n\n2. Запустите следующую команду, чтобы выполнить вывод (тестирование) с использованием предварительно обученной модели:\n   ``` ш\n   bash run.sh -t test --cfg эксперименты/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n   ```\n   Замените `${PRETRAINED_MODLE_FILE}` на путь к файлу предварительно обученной модели.\n\n[Требуется дополнительная информация] о конкретном входном формате и о том, как получить результаты классификации.\n\nМодель достигает конкурентоспособной производительности при выполнении различных последующих задач даже без тонкой настройки благодаря предварительному обучению на крупномасштабном наборе данных ImageNet-22k. Он достигает первой точности 87,7% на проверочном наборе ImageNet-1k.\n\nДля получения дополнительной помощи или информации обращайтесь к команде разработчиков модели.",
    "answer_Japanese": "モデル Microsoft/cvt-21-384-22k は、微調整、後処理、パイプラインへの接続を必要とせずに、画像分類タスクに使用できます。大規模な ImageNet-22k データセットで事前トレーニングされています。\n\nモデルを推論に使用するには:\n\n1. PyTorch と TorchVision がまだインストールされていない場合はインストールします。以下を使用して他の依存関係をインストールします。\n   「」し\n   python -m pip install -rrequirements.txt --user -q\n   「」\n\n2. 次のコマンドを実行して、事前トレーニングされたモデルを使用して推論 (テスト) を実行します。\n   「」し\n   bash run.sh -t test --cfg Experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n   「」\n   `${PRETRAINED_MODLE_FILE}` を事前トレーニングされたモデル ファイルへのパスに置き換えます。\n\n具体的な入力形式と分類結果の取得方法については、[詳細情報が必要です]。\n\nこのモデルは、大規模な ImageNet-22k データセットでの事前トレーニングのおかげで、微調整を行わなくても、さまざまな下流タスクで競争力のあるパフォーマンスを実現します。 ImageNet-1k 検証セットでは 87.7% というトップ 1 の精度が得られます。\n\nさらにサポートや情報が必要な場合は、お気軽にモデル開発チームにお問い合わせください。"
  },
  {
    "question": "downstream_use",
    "chain": "generation",
    "answer": "The microsoft/cvt-21-384-22k model can be fine-tuned on various downstream tasks to achieve state-of-the-art performance. As mentioned in the references, when pre-trained on the larger ImageNet-22k dataset and fine-tuned, CvT-W24 obtains top results across all considered downstream tasks, surpassing even much larger models like BiT-R152x4.\n\nTo fine-tune the model on a specific task, you can use the provided scripts and configuration files. For example, to train the model, you can run:\n\n```sh\nbash run.sh -g 8 -t train --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml\n```\n\nAnd to test the fine-tuned model:\n\n```sh\nbash run.sh -t test --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n```\n\nThe model's strong transfer learning capabilities when pre-trained on a large dataset like ImageNet-22k make it suitable to plug into larger ecosystems or applications that require high accuracy on downstream computer vision tasks.\n\n[More Information Needed] on specific larger ecosystem or app integrations.\n\nThe references also highlight that the model achieves its performance efficiently, with fewer parameters and FLOPs compared to alternatives. This makes it more feasible to deploy in resource-constrained settings.\n\nFor further questions or to update this model card, please contact the project organizer.",
    "answer_Chinese": "microsoft/cvt-21-384-22k 模型可以针对各种下游任务进行微调，以实现最先进的性能。正如参考文献中提到的，当在更大的 ImageNet-22k 数据集上进行预训练并进行微调时，CvT-W24 在所有考虑的下游任务中获得了最佳结果，甚至超越了 BiT-R152x4 等更大的模型。\n\n要针对特定​​任务微调模型，您可以使用提供的脚本和配置文件。例如，要训练模型，您可以运行：\n\n````嘘\nbash run.sh -g 8 -t train --cfg Experiments/imagenet/cvt/cvt-13-224x224.yaml\n````\n\n并测试微调后的模型：\n\n````嘘\nbash run.sh -t test --cfg Experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n````\n\n该模型在 ImageNet-22k 等大型数据集上进行预训练时具有强大的迁移学习功能，使其适合插入需要下游计算机视觉任务高精度的更大生态系统或应用程序。\n\n[需要更多信息]关于特定的更大的生态系统或应用程序集成。\n\n这些参考文献还强调，与替代方案相比，该模型可以通过更少的参数和更少的失败次数有效地实现其性能。这使得在资源有限的环境中部署更加可行。\n\n如有其他问题或更新此模型卡，请联系项目组织者。",
    "answer_Arabic": "يمكن ضبط الطراز microsoft/cvt-21-384-22k بشكل دقيق على العديد من المهام النهائية لتحقيق أداء متطور. كما هو مذكور في المراجع، عند تدريب CvT-W24 مسبقًا على مجموعة بيانات ImageNet-22k الأكبر حجمًا وضبطه بدقة، يحصل على أفضل النتائج عبر جميع المهام النهائية التي تم اعتبارها، متجاوزًا حتى النماذج الأكبر حجمًا مثل BiT-R152x4.\n\nلضبط النموذج في مهمة محددة، يمكنك استخدام البرامج النصية وملفات التكوين المتوفرة. على سبيل المثال، لتدريب النموذج، يمكنك تشغيل:\n\n```ش\nbash run.sh -g 8 -t Train --cfg experiences/imagenet/cvt/cvt-13-224x224.yaml\n```\n\nولاختبار النموذج الدقيق:\n\n```ش\nاختبار bash run.sh -t --cfg experiences/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n```\n\nإن إمكانات تعلم النقل القوية للنموذج عند تدريبه مسبقًا على مجموعة بيانات كبيرة مثل ImageNet-22k تجعله مناسبًا للتوصيل بأنظمة بيئية أكبر أو تطبيقات تتطلب دقة عالية في مهام رؤية الكمبيوتر النهائية.\n\n[مطلوب مزيد من المعلومات] حول عمليات تكامل النظام البيئي أو التطبيقات الأكبر حجمًا.\n\nوتسلط المراجع الضوء أيضًا على أن النموذج يحقق أدائه بكفاءة، مع عدد أقل من المعلمات وFLOPs مقارنة بالبدائل. وهذا يجعل النشر في إعدادات محدودة الموارد أكثر جدوى.\n\nلمزيد من الأسئلة أو لتحديث هذه البطاقة النموذجية، يرجى الاتصال بمنظم المشروع.",
    "answer_French": "Le modèle Microsoft/cvt-21-384-22k peut être affiné sur diverses tâches en aval pour obtenir des performances de pointe. Comme mentionné dans les références, lorsqu'il est pré-entraîné sur le plus grand ensemble de données ImageNet-22k et affiné, le CvT-W24 obtient les meilleurs résultats dans toutes les tâches en aval considérées, surpassant même des modèles beaucoup plus grands comme le BiT-R152x4.\n\nPour affiner le modèle sur une tâche spécifique, vous pouvez utiliser les scripts et fichiers de configuration fournis. Par exemple, pour entraîner le modèle, vous pouvez exécuter :\n\n``` merde\nbash run.sh -g 8 -t train --cfg experimental/imagenet/cvt/cvt-13-224x224.yaml\n```\n\nEt pour tester le modèle affiné :\n\n``` merde\nbash run.sh -t test --cfg experimental/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n```\n\nLes fortes capacités d'apprentissage par transfert du modèle lorsqu'il est pré-entraîné sur un grand ensemble de données comme ImageNet-22k le rendent approprié pour se connecter à des écosystèmes ou à des applications plus vastes qui nécessitent une grande précision sur les tâches de vision par ordinateur en aval.\n\n[Plus d'informations nécessaires] sur des intégrations d'écosystèmes ou d'applications plus larges spécifiques.\n\nLes références soulignent également que le modèle atteint ses performances de manière efficace, avec moins de paramètres et de FLOP par rapport aux alternatives. Cela rend le déploiement plus réalisable dans des environnements aux ressources limitées.\n\nPour d’autres questions ou pour mettre à jour cette fiche modèle, veuillez contacter l’organisateur du projet.",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k можно настроить для решения различных последующих задач для достижения высочайшей производительности. Как упоминалось в ссылках, при предварительном обучении на более крупном наборе данных ImageNet-22k и точной настройке CvT-W24 получает лучшие результаты по всем рассматриваемым последующим задачам, превосходя даже гораздо более крупные модели, такие как BiT-R152x4.\n\nДля тонкой настройки модели под конкретную задачу вы можете использовать предоставленные скрипты и файлы конфигурации. Например, чтобы обучить модель, вы можете запустить:\n\n```ш\nbash run.sh -g 8 -t train --cfg эксперименты/imagenet/cvt/cvt-13-224x224.yaml\n```\n\nИ чтобы протестировать точно настроенную модель:\n\n```ш\nbash run.sh -t test --cfg эксперименты/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n```\n\nШирокие возможности модели по переносу обучения при предварительном обучении на большом наборе данных, таком как ImageNet-22k, делают ее подходящей для подключения к более крупным экосистемам или приложениям, требующим высокой точности при выполнении последующих задач компьютерного зрения.\n\n[Требуется дополнительная информация] о конкретной более крупной экосистеме или интеграции приложений.\n\nВ ссылках также подчеркивается, что модель эффективно достигает своей производительности с меньшим количеством параметров и FLOP по сравнению с альтернативами. Это делает более целесообразным развертывание в условиях ограниченных ресурсов.\n\nЕсли у вас есть дополнительные вопросы или чтобы обновить эту карточку модели, свяжитесь с организатором проекта.",
    "answer_Japanese": "Microsoft/cvt-21-384-22k モデルは、さまざまなダウンストリーム タスクで微調整して、最先端のパフォーマンスを実現できます。参考文献で述べられているように、より大きな ImageNet-22k データセットで事前トレーニングされ、微調整された場合、CvT-W24 は、考慮されたすべての下流タスクにわたって最高の結果を取得し、BiT-R152x4 のようなはるかに大きなモデルさえも上回ります。\n\n特定のタスクでモデルを微調整するには、提供されたスクリプトと構成ファイルを使用できます。たとえば、モデルをトレーニングするには、次を実行できます。\n\n「し」\nbash run.sh -g 8 -t train --cfg実験/imagenet/cvt/cvt-13-224x224.yaml\n「」\n\nそして、微調整されたモデルをテストするには、次のようにします。\n\n「し」\nbash run.sh -t test --cfg Experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n「」\n\nImageNet-22k のような大規模なデータセットで事前トレーニングされたモデルの強力な転移学習機能により、下流のコンピューター ビジョン タスクで高精度を必要とする大規模なエコシステムやアプリケーションへのプラグインに適しています。\n\n特定のより大規模なエコシステムまたはアプリの統合に関する [追加情報が必要]。\n\n参考資料では、このモデルが代替モデルと比較してパラメーターと FLOP が少なく、効率的にパフォーマンスを達成していることも強調しています。これにより、リソースに制約のある設定での展開がより現実的になります。\n\nさらに質問がある場合、またはこのモデル カードを更新するには、プロジェクト主催者にお問い合わせください。"
  },
  {
    "question": "out_of_scope_use",
    "chain": "generation",
    "answer": "Based on the provided references, here are some potential ways the microsoft/cvt-21-384-22k model could be misused and what users ought not do:\n\n- The model should not be used in a way that violates Microsoft's Trademark & Brand Guidelines, causes confusion, or implies Microsoft sponsorship without permission. Users must follow the policies for any third-party trademarks or logos used with the model.\n\n- Users should adhere to the Microsoft Open Source Code of Conduct when using or contributing to this model. Misuse would include harassment, discrimination, or other unethical conduct.\n\n- The model should not be applied to tasks or domains it was not designed for or validated on, as performance may be unpredictable. It is intended for image classification and related visual tasks, not other unrelated applications. [References 6-8]\n\n- When fine-tuning the pre-trained model on downstream tasks, users should be aware of potential issues like bias, fairness, and responsible usage. Applying it to sensitive use cases without proper diligence would be inadvisable.\n\n- [More Information Needed] on other specific failure modes, vulnerabilities, or misuse cases of the model architecture and training data. The references do not provide enough direct information to responsibly comment further.\n\nIn general, users ought to employ the model in an ethical manner, understand its limitations, and not assume it will perform accurately for all possible inputs and applications. Misuse would involve violating licenses/terms, ignoring conduct guidelines, and irresponsible deployment without considering consequences.",
    "answer_Chinese": "根据提供的参考资料，以下是 microsoft/cvt-21-384-22k 模型可能被滥用的一些潜在方式以及用户不应该执行的操作：\n\n- 该模型的使用方式不应违反 Microsoft 的商标和品牌指南、引起混淆或未经许可暗示 Microsoft 赞助。用户必须遵守模型所使用的任何第三方商标或徽标的政策。\n\n- 用户在使用或贡献此模型时应遵守 Microsoft 开源行为准则。滥用包括骚扰、歧视或其他不道德行为。\n\n- 该模型不应应用于其未设计或验证的任务或领域，因为性能可能不可预测。它适用于图像分类和相关的视觉任务，而不是其他不相关的应用程序。 [参考文献6-8]\n\n- 在对下游任务的预训练模型进行微调时，用户应该意识到潜在的问题，例如偏见、公平性和负责任的使用。在没有适当调查的情况下将其应用于敏感用例是不可取的。\n\n- [需要更多信息]有关模型架构和训练数据的其他特定故障模式、漏洞或误用案例。参考文献没有提供足够的直接信息来负责任地进一步发表评论。\n\n一般来说，用户应该以合乎道德的方式使用该模型，了解其局限性，而不是假设它对于所有可能的输入和应用程序都能准确执行。滥用将涉及违反许可证/条款、忽视行为准则以及不考虑后果的不负责任的部署。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، فيما يلي بعض الطرق المحتملة لإساءة استخدام نموذج microsoft/cvt-21-384-22k وما لا يجب على المستخدمين فعله:\n\n- لا ينبغي استخدام النموذج بطريقة تنتهك إرشادات العلامة التجارية والعلامات التجارية الخاصة بشركة Microsoft، أو تسبب ارتباكًا، أو تشير ضمنًا إلى رعاية Microsoft دون إذن. يجب على المستخدمين اتباع السياسات الخاصة بأي علامات تجارية أو شعارات تابعة لجهات خارجية مستخدمة مع النموذج.\n\n- يجب على المستخدمين الالتزام بقواعد سلوك Microsoft Open Source عند استخدام هذا النموذج أو المساهمة فيه. قد تشمل إساءة الاستخدام التحرش أو التمييز أو أي سلوك غير أخلاقي آخر.\n\n- لا ينبغي تطبيق النموذج على المهام أو المجالات التي لم يتم تصميمه لها أو التحقق من صحتها، حيث قد لا يمكن التنبؤ بالأداء. إنه مخصص لتصنيف الصور والمهام المرئية ذات الصلة، وليس للتطبيقات الأخرى غير ذات الصلة. [المراجع 6-8]\n\n- عند ضبط النموذج الذي تم تدريبه مسبقًا على المهام النهائية، يجب أن يكون المستخدمون على دراية بالمشكلات المحتملة مثل التحيز والعدالة والاستخدام المسؤول. ومن غير المستحسن تطبيقه على حالات الاستخدام الحساسة دون بذل العناية المناسبة.\n\n- [مطلوب مزيد من المعلومات] حول أوضاع الفشل أو نقاط الضعف أو حالات سوء الاستخدام المحددة الأخرى لبنية النموذج وبيانات التدريب. لا توفر المراجع معلومات مباشرة كافية للتعليق بشكل مسؤول.\n\nبشكل عام، يجب على المستخدمين استخدام النموذج بطريقة أخلاقية، وفهم حدوده، وعدم افتراض أنه سيعمل بدقة لجميع المدخلات والتطبيقات الممكنة. قد تنطوي إساءة الاستخدام على انتهاك التراخيص/الشروط، وتجاهل إرشادات السلوك، والنشر غير المسؤول دون النظر في العواقب.",
    "answer_French": "Sur la base des références fournies, voici quelques façons potentielles dont le modèle Microsoft/cvt-21-384-22k pourrait être utilisé à mauvais escient et ce que les utilisateurs ne devraient pas faire :\n\n- Le modèle ne doit pas être utilisé d'une manière qui viole les directives de Microsoft en matière de marques et de marques, crée de la confusion ou implique le parrainage de Microsoft sans autorisation. Les utilisateurs doivent suivre les politiques relatives aux marques ou logos tiers utilisés avec le modèle.\n\n- Les utilisateurs doivent adhérer au code de conduite Microsoft Open Source lorsqu'ils utilisent ou contribuent à ce modèle. Une utilisation abusive comprendrait le harcèlement, la discrimination ou toute autre conduite contraire à l'éthique.\n\n- Le modèle ne doit pas être appliqué à des tâches ou à des domaines pour lesquels il n'a pas été conçu ou validé, car les performances peuvent être imprévisibles. Il est destiné à la classification d'images et aux tâches visuelles associées, et non à d'autres applications non liées. [Références 6-8]\n\n- Lors du réglage fin du modèle pré-entraîné sur les tâches en aval, les utilisateurs doivent être conscients des problèmes potentiels tels que les biais, l'équité et l'utilisation responsable. L’appliquer à des cas d’utilisation sensibles sans une diligence appropriée serait déconseillé.\n\n- [Plus d'informations nécessaires] sur d'autres modes de défaillance spécifiques, vulnérabilités ou cas d'utilisation abusive de l'architecture du modèle et des données de formation. Les références ne fournissent pas suffisamment d’informations directes pour pouvoir commenter davantage de manière responsable.\n\nEn général, les utilisateurs doivent utiliser le modèle de manière éthique, comprendre ses limites et ne pas supposer qu'il fonctionnera avec précision pour toutes les entrées et applications possibles. Une utilisation abusive impliquerait la violation des licences/conditions, l’ignorance des directives de conduite et un déploiement irresponsable sans tenir compte des conséquences.",
    "answer_Russian": "Основываясь на предоставленных ссылках, вот несколько потенциальных способов неправильного использования модели microsoft/cvt-21-384-22k и того, чего пользователям не следует делать:\n\n- Модель не должна использоваться таким образом, чтобы нарушать правила Microsoft по товарным знакам и брендам, вызывать путаницу или подразумевать спонсорство Microsoft без разрешения. Пользователи должны соблюдать политику в отношении любых сторонних товарных знаков или логотипов, используемых с моделью.\n\n- Пользователи должны соблюдать Кодекс поведения Microsoft с открытым исходным кодом при использовании этой модели или внесении в нее вклада. Неправомерное использование будет включать притеснение, дискриминацию или другое неэтичное поведение.\n\n— Модель не следует применять к задачам или областям, для которых она не была разработана или проверена, поскольку производительность может быть непредсказуемой. Он предназначен для классификации изображений и связанных с ним визуальных задач, а не для других несвязанных приложений. [Ссылки 6-8]\n\n- При точной настройке предварительно обученной модели для выполнения последующих задач пользователи должны знать о потенциальных проблемах, таких как предвзятость, справедливость и ответственное использование. Применять его к деликатным случаям использования без должной осмотрительности было бы нецелесообразно.\n\n- [Требуется дополнительная информация] о других конкретных режимах сбоя, уязвимостях или случаях неправильного использования архитектуры модели и данных обучения. Ссылки не предоставляют достаточно прямой информации для дальнейших ответственных комментариев.\n\nВ общем, пользователи должны использовать модель этично, понимать ее ограничения и не предполагать, что она будет работать точно для всех возможных входных данных и приложений. Неправильное использование повлечет за собой нарушение лицензий/условий, игнорирование правил поведения и безответственное развертывание без учета последствий.",
    "answer_Japanese": "提供された参考資料に基づいて、microsoft/cvt-21-384-22k モデルが悪用される可能性のある方法とユーザーがすべきでないことをいくつか示します。\n\n- このモデルは、Microsoft の商標およびブランド ガイドラインに違反したり、混乱を引き起こしたり、許可なく Microsoft のスポンサーシップを示唆したりする方法で使用してはなりません。ユーザーは、モデルで使用されるサードパーティの商標またはロゴに関するポリシーに従う必要があります。\n\n- ユーザーは、このモデルを使用または貢献する場合、Microsoft オープンソース行動規範に従う必要があります。悪用には、嫌がらせ、差別、その他の非倫理的な行為が含まれます。\n\n- モデルは、パフォーマンスが予測できない可能性があるため、設計または検証されていないタスクやドメインには適用しないでください。これは、画像分類および関連する視覚タスクを目的としており、他の無関係なアプリケーションではありません。 【参考6～8】\n\n- 下流タスクで事前トレーニングされたモデルを微調整する場合、ユーザーはバイアス、公平性、責任ある使用などの潜在的な問題に注意する必要があります。適切な注意を払わずにこれを機密性の高いユースケースに適用することはお勧めできません。\n\n- モデル アーキテクチャとトレーニング データの他の特定の障害モード、脆弱性、または誤用ケースに関する [さらなる情報が必要]。参考文献には、責任を持ってさらにコメントできるほどの直接的な情報が提供されていません。\n\n一般に、ユーザーは倫理的な方法でモデルを使用し、その制限を理解し、考えられるすべての入力およびアプリケーションに対してモデルが正確に実行されると想定すべきではありません。悪用には、ライセンス/規約への違反、行動ガイドラインの無視、結果を考慮しない無責任な展開が含まれます。"
  },
  {
    "question": "bias_risks_limitations",
    "chain": "generation",
    "answer": "Based on the provided references about the CvT (Convolutional vision Transformer) model, here are some potential issues and limitations:\n\nTechnical limitations:\n- The model relies on large-scale pretraining on the ImageNet-22k dataset. Performance on tasks with significantly different data distributions may be limited. [More Information Needed] on robustness to distribution shift.\n- Computational cost and memory usage can still be relatively high, especially for larger model variants like CvT-W24. This may limit practical deployment.\n\nForeseeable misuse and harms: \n- As a powerful visual recognition model, CvT could potentially be used for harmful surveillance applications or to generate deepfakes. Responsible deployment practices are important.\n- [More Information Needed] on fairness and bias testing of the model across different demographics. Vision models can reflect biases in training data.\n\nSociotechnical considerations:\n- Reliance on large proprietary datasets like ImageNet-22k for pretraining raises questions about representation and inclusivity. [More Information Needed] on the diversity and consent of data subjects.\n- As CvT advances the state-of-the-art in vision models, it may contribute to job displacement in fields like medical image analysis or moderation. Considering social impacts is important.\n\nTo mitigate these issues, the model card should clearly document the intended use cases, limitations, and potential misuse. Transparency around training data and testing for fairness and robustness is also recommended. Ongoing monitoring and responsible deployment practices will be key.",
    "answer_Chinese": "根据提供的有关 CvT（卷积视觉变换器）模型的参考资料，以下是一些潜在的问题和限制：\n\n技术限制：\n- 该模型依赖于 ImageNet-22k 数据集上的大规模预训练。数据分布显着不同的任务的性能可能会受到限制。 [需要更多信息]关于分布转变的稳健性。\n- 计算成本和内存使用量仍然相对较高，特别是对于 CvT-W24 等较大的模型变体。这可能会限制实际部署。\n\n可预见的误用和危害： \n- 作为一种强大的视觉识别模型，CvT 可能用于有害的监控应用或生成深度伪造品。负责任的部署实践很重要。\n- [需要更多信息]关于不同人口统计模型的公平性和偏见测试。视觉模型可以反映训练数据中的偏差。\n\n社会技术考虑因素：\n- 依赖 ImageNet-22k 等大型专有数据集进行预训练引发了有关代表性和包容性的问题。 [需要更多信息]关于数据主体的多样性和同意。\n- 随着 CvT 推动视觉模型的发展，它可能会导致医学图像分析或审核等领域的工作岗位流失。考虑社会影响很重要。\n\n为了缓解这些问题，模型卡应清楚地记录预期的用例、限制和潜在的误用。还建议围绕训练数据和公平性和稳健性测试保持透明度。持续监控和负责任的部署实践将是关键。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج CvT (محول الرؤية التلافيفي)، إليك بعض المشكلات والقيود المحتملة:\n\nالقيود الفنية:\n- يعتمد النموذج على تدريب مسبق واسع النطاق على مجموعة بيانات ImageNet-22k. قد يكون الأداء في المهام ذات توزيعات البيانات المختلفة بشكل كبير محدودًا. [بحاجة إلى مزيد من المعلومات] حول متانة تحول التوزيع.\n- من الممكن أن تظل التكلفة الحسابية واستخدام الذاكرة مرتفعًا نسبيًا، خاصة بالنسبة للنماذج الأكبر حجمًا مثل CvT-W24. وهذا قد يحد من النشر العملي.\n\nسوء الاستخدام والأضرار المتوقعة: \n- باعتباره نموذجًا قويًا للتعرف البصري، يمكن استخدام CvT لتطبيقات المراقبة الضارة أو لإنشاء مقاطع مزيفة عميقة. ممارسات النشر المسؤولة مهمة.\n- [بحاجة إلى مزيد من المعلومات] حول اختبار العدالة والتحيز للنموذج عبر التركيبة السكانية المختلفة. يمكن أن تعكس نماذج الرؤية التحيزات في بيانات التدريب.\n\nالاعتبارات الاجتماعية التقنية:\n- الاعتماد على مجموعات البيانات الكبيرة الخاصة مثل ImageNet-22k للتدريب المسبق يثير تساؤلات حول التمثيل والشمولية. [يلزم مزيد من المعلومات] حول تنوع موضوعات البيانات وموافقتها.\n- نظرًا لأن CvT تعمل على تطوير أحدث نماذج الرؤية، فقد تساهم في إزاحة الوظائف في مجالات مثل تحليل الصور الطبية أو الإشراف عليها. من المهم مراعاة التأثيرات الاجتماعية.\n\nوللتخفيف من حدة هذه المشكلات، يجب أن توثق البطاقة النموذجية بوضوح حالات الاستخدام المقصودة والقيود وسوء الاستخدام المحتمل. يوصى أيضًا بالشفافية حول بيانات التدريب واختبار العدالة والقوة. ستكون المراقبة المستمرة وممارسات النشر المسؤولة أمرًا أساسيًا.",
    "answer_French": "Sur la base des références fournies sur le modèle CvT (Convolutional vision Transformer), voici quelques problèmes et limitations potentiels :\n\nLimites techniques :\n- Le modèle s'appuie sur un pré-entraînement à grande échelle sur l'ensemble de données ImageNet-22k. Les performances sur des tâches avec des distributions de données très différentes peuvent être limitées. [Plus d'informations nécessaires] sur la robustesse du changement de distribution.\n- Le coût de calcul et l'utilisation de la mémoire peuvent encore être relativement élevés, en particulier pour les variantes de modèles plus grandes comme le CvT-W24. Cela peut limiter le déploiement pratique.\n\nUtilisation abusive et préjudices prévisibles : \n- En tant que modèle de reconnaissance visuelle puissant, le CvT pourrait potentiellement être utilisé pour des applications de surveillance nuisibles ou pour générer des deepfakes. Des pratiques de déploiement responsables sont importantes.\n- [Plus d'informations nécessaires] sur les tests d'équité et de biais du modèle sur différentes données démographiques. Les modèles de vision peuvent refléter des biais dans les données d'entraînement.\n\nConsidérations sociotechniques :\n- Le recours à de grands ensembles de données propriétaires comme ImageNet-22k pour la pré-formation soulève des questions sur la représentation et l'inclusivité. [Plus d'informations nécessaires] sur la diversité et le consentement des personnes concernées.\n- À mesure que le CvT fait progresser l'état de l'art en matière de modèles de vision, il peut contribuer au déplacement d'emplois dans des domaines tels que l'analyse ou la modération d'images médicales. Il est important de prendre en compte les impacts sociaux.\n\nPour atténuer ces problèmes, la fiche modèle doit clairement documenter les cas d'utilisation prévus, les limitations et les utilisations abusives potentielles. La transparence autour des données de formation et des tests d’équité et de robustesse est également recommandée. Une surveillance continue et des pratiques de déploiement responsables seront essentielles.",
    "answer_Russian": "На основе предоставленных ссылок на модель CvT (трансформатор сверточного видения) можно выделить некоторые потенциальные проблемы и ограничения:\n\nТехнические ограничения:\n- Модель основана на крупномасштабном предварительном обучении на наборе данных ImageNet-22k. Производительность задач со значительно отличающимся распределением данных может быть ограничена. [Требуется дополнительная информация] об устойчивости к изменению распределения.\n- Затраты на вычисления и использование памяти все еще могут быть относительно высокими, особенно для более крупных вариантов модели, таких как CvT-W24. Это может ограничить практическое развертывание.\n\nПредвидимые злоупотребления и вред: \n- Будучи мощной моделью визуального распознавания, CvT потенциально может использоваться для вредоносных приложений наблюдения или для создания дипфейков. Ответственное развертывание имеет важное значение.\n- [Требуется дополнительная информация] о тестировании модели на справедливость и предвзятость среди различных демографических групп. Модели видения могут отражать отклонения в данных обучения.\n\nСоциотехнические соображения:\n- Использование больших собственных наборов данных, таких как ImageNet-22k, для предварительного обучения поднимает вопросы о репрезентативности и инклюзивности. [Требуется дополнительная информация] о разнообразии и согласии субъектов данных.\n- Поскольку CvT совершенствует новейшие модели зрительного зрения, это может способствовать сокращению рабочих мест в таких областях, как анализ медицинских изображений или модерация. Важно учитывать социальные последствия.\n\nЧтобы смягчить эти проблемы, карточка модели должна четко документировать предполагаемые варианты использования, ограничения и потенциальное неправильное использование. Также рекомендуется обеспечить прозрачность данных обучения и тестирования на предмет справедливости и надежности. Ключевое значение будут иметь постоянный мониторинг и ответственное внедрение.",
    "answer_Japanese": "CvT (畳み込みビジョン トランスフォーマー) モデルに関して提供されている参考資料に基づいて、潜在的な問題と制限をいくつか示します。\n\n技術的な制限:\n- モデルは、ImageNet-22k データセットでの大規模な事前トレーニングに依存しています。データ分布が大きく異なるタスクのパフォーマンスは制限される可能性があります。配布シフトに対する堅牢性については[さらなる情報が必要]。\n- 特に CvT-W24 のような大規模なモデルのバリアントでは、計算コストとメモリ使用量が依然として比較的高くなる可能性があります。これにより、実際の展開が制限される可能性があります。\n\n予見可能な誤用と危害: \n- 強力な視覚認識モデルとして、CvT は有害な監視アプリケーションやディープフェイクの生成に使用される可能性があります。責任を持って導入を実践することが重要です。\n- さまざまな人口統計にわたるモデルの公平性とバイアスのテストに関する [さらなる情報が必要]。ビジョン モデルはトレーニング データのバイアスを反映する可能性があります。\n\n社会技術的考慮事項:\n- 事前トレーニングのために ImageNet-22k のような大規模な独自のデータセットに依存すると、表現と包括性について疑問が生じます。データ主体の多様性と同意に関する[さらなる情報が必要]。\n- CvT は最先端の視覚モデルを進歩させるため、医療画像分析やモデレーションなどの分野での離職につながる可能性があります。社会的影響を考慮することが重要です。\n\nこれらの問題を軽減するには、モデル カードに、意図された使用例、制限事項、および潜在的な誤用を明確に文書化する必要があります。トレーニング データと公平性と堅牢性のテストに関する透明性も推奨されます。継続的なモニタリングと責任ある導入実践が鍵となります。"
  },
  {
    "question": "bias_recommendations",
    "chain": "generation",
    "answer": "Based on the provided references about the microsoft/cvt-21-384-22k model, here are some recommendations regarding foreseeable issues:\n\nSocietal Impact:\n[More Information Needed] The references do not discuss potential societal impacts or risks of deploying this model. More analysis is needed on how the model's capabilities could be misused or lead to unintended negative consequences.\n\nBias and Fairness: \n[More Information Needed] There are no details provided on what data the model was trained on and whether bias and fairness evaluations were conducted. The training data and model outputs should be analyzed for potential biases before deployment.\n\nPrivacy:\n[More Information Needed] Information is lacking on privacy considerations during model training and inference. Details should be provided on whether the model was trained on any sensitive or personally identifiable information.\n\nRobustness:\nThe references mention the model achieves strong performance on downstream tasks even with lower parameters and FLOPs compared to other models. However, [More Information Needed] on how robust the model is to adversarial attacks, distribution shift, and outlier inputs. Thorough testing of failure modes and robustness is recommended.\n\nEnvironmental Impact:\nThe model is described as having a more efficient design with fewer parameters and lower compute (FLOPs) than other vision transformers. This suggests a lower environmental footprint for training and deployment. However, [More Information Needed] to quantify the specific carbon emissions and energy usage.\n\nI would recommend conducting more thorough testing and analysis to fill in these information gaps before releasing the model. Let me know if you need any other assistance!",
    "answer_Chinese": "根据提供的有关 microsoft/cvt-21-384-22k 模型的参考资料，以下是有关可预见问题的一些建议：\n\n社会影响：\n[需要更多信息] 参考文献未讨论部署此模型的潜在社会影响或风险。需要对模型的功能如何被滥用或导致意想不到的负面后果进行更多分析。\n\n偏见和公平： \n[需要更多信息] 没有提供有关模型训练数据以及是否进行偏见和公平评估的详细信息。在部署之前应分析训练数据和模型输出的潜在偏差。\n\n隐私：\n[需要更多信息] 缺乏关于模型训练和推理过程中隐私考虑的信息。应详细说明模型是否接受过任何敏感或个人身份信息的培训。\n\n鲁棒性：\n参考文献提到，与其他模型相比，即使参数和失败率较低，该模型也能在下游任务上实现出色的性能。然而，[需要更多信息]关于该模型对对抗性攻击、分布变化和异常输入的鲁棒性。建议对故障模式和稳健性进行彻底测试。\n\n对环境造成的影响：\n与其他视觉转换器相比，该模型具有更高效的设计、更少的参数和更低的计算量 (FLOP)。这表明训练和部署的环境足迹较低。然而，[需要更多信息]来量化具体的碳排放和能源使用。\n\n我建议在发布模型之前进行更彻底的测试和分析，以填补这些信息空白。如果您需要任何其他帮助，请告诉我！",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول طراز microsoft/cvt-21-384-22k، إليك بعض التوصيات المتعلقة بالمشكلات المتوقعة:\n\nالتأثير المجتمعي:\n[بحاجة إلى مزيد من المعلومات] لا تناقش المراجع التأثيرات المجتمعية المحتملة أو مخاطر نشر هذا النموذج. هناك حاجة إلى مزيد من التحليل حول كيفية إساءة استخدام قدرات النموذج أو أن تؤدي إلى عواقب سلبية غير مقصودة.\n\nالتحيز والعدالة: \n[بحاجة إلى مزيد من المعلومات] لا توجد تفاصيل حول البيانات التي تم تدريب النموذج عليها وما إذا كان قد تم إجراء تقييمات التحيز والعدالة. ينبغي تحليل بيانات التدريب ومخرجات النموذج بحثًا عن التحيزات المحتملة قبل النشر.\n\nخصوصية:\n[بحاجة إلى مزيد من المعلومات] المعلومات غير متوفرة حول اعتبارات الخصوصية أثناء التدريب النموذجي والاستدلال. يجب تقديم تفاصيل حول ما إذا كان النموذج قد تم تدريبه على أي معلومات حساسة أو معلومات تعريف شخصية.\n\nالمتانة:\nتشير المراجع إلى أن النموذج يحقق أداءً قويًا في المهام النهائية حتى مع انخفاض المعلمات وFLOPs مقارنة بالنماذج الأخرى. ومع ذلك، [بحاجة إلى مزيد من المعلومات] حول مدى قوة النموذج في مواجهة الهجمات العدائية، وتحول التوزيع، والمدخلات الخارجية. يوصى بإجراء اختبار شامل لأنماط الفشل والمتانة.\n\nتأثير بيئي:\nيوصف النموذج بأنه يتمتع بتصميم أكثر كفاءة مع معلمات أقل وحساب أقل (FLOPs) من محولات الرؤية الأخرى. ويشير هذا إلى بصمة بيئية أقل للتدريب والنشر. ومع ذلك، [يلزم مزيد من المعلومات] لتحديد كمية انبعاثات الكربون المحددة واستخدام الطاقة.\n\nأوصي بإجراء اختبارات وتحليلات أكثر شمولاً لملء فجوات المعلومات هذه قبل إصدار النموذج. اسمحوا لي أن أعرف إذا كنت بحاجة إلى أي مساعدة أخرى!",
    "answer_French": "Sur la base des références fournies concernant le modèle Microsoft/cvt-21-384-22k, voici quelques recommandations concernant les problèmes prévisibles :\n\nImpact sociétal :\n[Plus d'informations nécessaires] Les références ne discutent pas des impacts sociétaux potentiels ou des risques liés au déploiement de ce modèle. Une analyse plus approfondie est nécessaire sur la manière dont les capacités du modèle pourraient être utilisées à mauvais escient ou entraîner des conséquences négatives involontaires.\n\nBiais et équité : \n[Plus d'informations nécessaires] Aucun détail n'est fourni sur les données sur lesquelles le modèle a été formé et si des évaluations de biais et d'équité ont été effectuées. Les données de formation et les résultats du modèle doivent être analysés pour détecter tout biais potentiel avant le déploiement.\n\nConfidentialité:\n[Plus d'informations nécessaires] Il manque des informations sur les considérations en matière de confidentialité lors de la formation et de l'inférence du modèle. Des détails doivent être fournis indiquant si le modèle a été formé sur des informations sensibles ou personnellement identifiables.\n\nRobustesse :\nLes références mentionnent que le modèle atteint de solides performances sur les tâches en aval, même avec des paramètres et des FLOP inférieurs par rapport aux autres modèles. Cependant, [Plus d'informations nécessaires] sur la robustesse du modèle face aux attaques contradictoires, au changement de distribution et aux entrées aberrantes. Des tests approfondis des modes de défaillance et de la robustesse sont recommandés.\n\nImpact environnemental:\nLe modèle est décrit comme ayant une conception plus efficace avec moins de paramètres et un calcul inférieur (FLOP) que les autres transformateurs de vision. Cela suggère une empreinte environnementale plus faible pour la formation et le déploiement. Cependant, [Plus d'informations nécessaires] pour quantifier les émissions de carbone spécifiques et la consommation d'énergie.\n\nJe recommanderais d'effectuer des tests et des analyses plus approfondis pour combler ces lacunes d'informations avant de publier le modèle. Faites-moi savoir si vous avez besoin d'une autre aide !",
    "answer_Russian": "Основываясь на предоставленных ссылках на модель microsoft/cvt-21-384-22k, вот несколько рекомендаций относительно возможных проблем:\n\nСоциальное воздействие:\n[Требуется дополнительная информация] В ссылках не обсуждаются потенциальные социальные последствия или риски внедрения этой модели. Необходим дополнительный анализ того, как возможности модели могут быть использованы неправильно или привести к непредвиденным негативным последствиям.\n\nПредвзятость и справедливость: \n[Требуется дополнительная информация] Не предоставлено никаких подробностей о том, на каких данных была обучена модель и проводились ли оценки предвзятости и справедливости. Перед развертыванием данные обучения и результаты модели следует проанализировать на предмет потенциальных ошибок.\n\nКонфиденциальность:\n[Требуется дополнительная информация] Отсутствует информация о соображениях конфиденциальности во время обучения модели и вывода. Должна быть предоставлена ​​подробная информация о том, была ли модель обучена использованию какой-либо конфиденциальной или личной информации.\n\nПрочность:\nВ ссылках упоминается, что модель обеспечивает высокую производительность при выполнении последующих задач даже при более низких параметрах и FLOP по сравнению с другими моделями. Однако [необходимо больше информации] о том, насколько устойчива модель к состязательным атакам, сдвигу распределения и посторонним входным данным. Рекомендуется тщательное тестирование видов отказов и надежности.\n\nВоздействие на окружающую среду:\nМодель описывается как имеющая более эффективную конструкцию с меньшим количеством параметров и меньшими вычислительными затратами (FLOP), чем другие преобразователи машинного зрения. Это предполагает меньшее воздействие на окружающую среду при обучении и развертывании. Однако [необходимо больше информации] для количественной оценки удельных выбросов углерода и энергопотребления.\n\nЯ бы рекомендовал провести более тщательное тестирование и анализ, чтобы заполнить эти информационные пробелы, прежде чем выпускать модель. Дайте мне знать, если вам понадобится дополнительная помощь!",
    "answer_Japanese": "Microsoft/cvt-21-384-22k モデルに関して提供されている参考資料に基づいて、予見される問題に関する推奨事項をいくつか示します。\n\n社会的影響:\n[さらなる情報が必要] 参考文献では、このモデルの導入による潜在的な社会的影響やリスクについては説明されていません。モデルの機能がどのように悪用されたり、意図しないマイナスの結果を引き起こしたりする可能性があるかについては、さらなる分析が必要です。\n\nバイアスと公平性: \n[さらなる情報が必要] モデルがどのようなデータでトレーニングされたか、バイアスと公平性の評価が行われたかどうかについては詳細が提供されていません。トレーニング データとモデルの出力は、展開前に潜在的なバイアスを分析する必要があります。\n\nプライバシー：\n[さらなる情報が必要] モデルのトレーニングおよび推論中のプライバシーの考慮事項に関する情報が不足しています。モデルが機密情報や個人を特定できる情報に基づいてトレーニングされたかどうかについて、詳細を提供する必要があります。\n\n堅牢性:\n参考文献には、このモデルが他のモデルと比較してパラメーターと FLOP が低い場合でも、ダウンストリーム タスクで強力なパフォーマンスを達成すると記載されています。ただし、敵対的な攻撃、分布の変化、異常値の入力に対してモデルがどの程度堅牢であるかについては[さらなる情報が必要]です。故障モードと堅牢性を徹底的にテストすることをお勧めします。\n\n環境への影響:\nこのモデルは、他のビジョン トランスフォーマーよりもパラメーターが少なく、計算量 (FLOP) が少ない、より効率的な設計になっていると説明されています。これは、トレーニングと導入のための環境フットプリントが低いことを示唆しています。ただし、具体的な炭素排出量とエネルギー使用量を定量化するには[さらなる情報が必要]。\n\nモデルをリリースする前に、より徹底的なテストと分析を実施して、これらの情報のギャップを埋めることをお勧めします。他にサポートが必要な場合はお知らせください。"
  },
  {
    "question": "training_data",
    "chain": "generation",
    "answer": "The model microsoft/cvt-21-384-22k is pre-trained on the ImageNet-22k dataset, which contains 14.2 million images across 22k classes. The model is then fine-tuned on the ImageNet-1k dataset to obtain a top-1 accuracy of 84.9% on the ImageNet-1k validation set at 384x384 resolution.\n\nTo prepare the ImageNet dataset for training, the following folder structure should be used:\n\n```sh\n|-DATASET\n  |-imagenet\n    |-train\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-class2\n    | | |-img3.jpg\n    | | |-...\n    | |-class3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-class2\n      | |-img6.jpg\n      | |-...\n      |-class3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\n[More Information Needed] on any additional data pre-processing or filtering steps used.",
    "answer_Chinese": "模型 microsoft/cvt-21-384-22k 在 ImageNet-22k 数据集上进行预训练，该数据集包含 22k 类的 1420 万张图像。然后在 ImageNet-1k 数据集上对模型进行微调，以在 384x384 分辨率的 ImageNet-1k 验证集上获得 84.9% 的 top-1 准确率。\n\n要准备用于训练的 ImageNet 数据集，应使用以下文件夹结构：\n\n````嘘\n|-数据集\n  |-imagenet\n    |-火车\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-2 级\n    | | |-img3.jpg\n    | | |-...\n    | |-3级\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-2 级\n      | |-img6.jpg\n      | |-...\n      |-3级\n      | |-img7.jpg\n      | |-...\n      |-...\n````\n\n[需要更多信息]有关使用的任何其他数据预处理或过滤步骤。",
    "answer_Arabic": "تم تدريب النموذج microsoft/cvt-21-384-22k مسبقًا على مجموعة بيانات ImageNet-22k، التي تحتوي على 14.2 مليون صورة عبر فئات 22 ألفًا. يتم بعد ذلك ضبط النموذج على مجموعة بيانات ImageNet-1k للحصول على دقة من الدرجة الأولى بنسبة 84.9% في مجموعة التحقق من صحة ImageNet-1k بدقة 384x384.\n\nلإعداد مجموعة بيانات ImageNet للتدريب، يجب استخدام بنية المجلد التالية:\n\n```ش\n|-مجموعة البيانات\n  |-imagenet\n    |-القطار\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-class2\n    | | |-img3.jpg\n    | | |-...\n    | |-class3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-فال\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-class2\n      | |-img6.jpg\n      | |-...\n      |-class3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\n[يلزم مزيد من المعلومات] حول أي خطوات إضافية للمعالجة المسبقة أو التصفية للبيانات المستخدمة.",
    "answer_French": "Le modèle Microsoft/cvt-21-384-22k est pré-entraîné sur l'ensemble de données ImageNet-22k, qui contient 14,2 millions d'images dans 22 000 classes. Le modèle est ensuite affiné sur l'ensemble de données ImageNet-1k pour obtenir une précision top-1 de 84,9 % sur l'ensemble de validation ImageNet-1k à une résolution de 384 x 384.\n\nPour préparer l'ensemble de données ImageNet pour la formation, la structure de dossiers suivante doit être utilisée :\n\n``` merde\n|-JEU DE DONNÉES\n  |-imagenet\n    |-train\n    | |-classe1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-classe2\n    | | |-img3.jpg\n    | | |-...\n    | |-classe3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-classe1\n      | |-img5.jpg\n      | |-...\n      |-classe2\n      | |-img6.jpg\n      | |-...\n      |-classe3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\n[Plus d'informations nécessaires] sur toute étape supplémentaire de prétraitement ou de filtrage des données utilisée.",
    "answer_Russian": "Модель microsoft/cvt-21-384-22k предварительно обучена на наборе данных ImageNet-22k, который содержит 14,2 миллиона изображений в 22 тысячах классов. Затем модель подвергается точной настройке на наборе данных ImageNet-1k, чтобы получить высшую точность 84,9% на проверочном наборе ImageNet-1k с разрешением 384x384.\n\nЧтобы подготовить набор данных ImageNet к обучению, следует использовать следующую структуру папок:\n\n```ш\n|-НАБОР ДАННЫХ\n  |-изображение\n    |-поезд\n    | |-класс1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-класс2\n    | | |-img3.jpg\n    | | |-...\n    | |-класс3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-вал\n      |-класс1\n      | |-img5.jpg\n      | |-...\n      |-класс2\n      | |-img6.jpg\n      | |-...\n      |-класс3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\n[Требуется дополнительная информация] о любых дополнительных этапах предварительной обработки или фильтрации данных.",
    "answer_Japanese": "モデル microsoft/cvt-21-384-22k は、22k クラスにわたる 1,420 万個の画像を含む ImageNet-22k データセットで事前トレーニングされています。次に、モデルは ImageNet-1k データセットで微調整され、解像度 384x384 の ImageNet-1k 検証セットで 84.9% というトップ 1 の精度が得られます。\n\nImageNet データセットをトレーニング用に準備するには、次のフォルダー構造を使用する必要があります。\n\n「し」\n|-データセット\n  |-イメージネット\n    |-電車\n    | |-クラス1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-クラス2\n    | | |-img3.jpg\n    | | |-...\n    | |-クラス3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-クラス1\n      | |-img5.jpg\n      | |-...\n      |-クラス2\n      | |-img6.jpg\n      | |-...\n      |-クラス3\n      | |-img7.jpg\n      | |-...\n      |-...\n「」\n\n使用される追加のデータ前処理またはフィルタリング手順に関する [追加情報が必要]。"
  },
  {
    "question": "preprocessing",
    "chain": "generation",
    "answer": "Here are the details about the preprocessing for the data of the model microsoft/cvt-21-384-22k:\n\nResizing:\nThe input images are resized to a resolution of 384x384 pixels, as indicated by the model name \"cvt-21-384-22k\".\n\nTokenization:\nThe model uses a Convolutional Token Embedding layer to convert the input image into a sequence of tokens. This layer adjusts the token feature dimension and the number of tokens at each stage by varying parameters of the convolution operation. The specific details are:\n\n``` sh\n1. x q/k/v i = Flatten (Conv2d (Reshape2D(x i ), s)) ,(2)\nwhere x q/k/v i is the token input for Q/K/V matrices at layer i, x i is the unperturbed token prior to the Convolutional Projection, Conv2d is a depth-wise separable convolution [5] implemented by: Depth-wise Conv2d → BatchNorm2d → Point-wise Conv2d, and s refers to the convolution kernel size.\n```\n\nThe convolution operation in the Convolutional Token Embedding layer aims to model local spatial contexts, from low-level edges to higher order semantic primitives, over a multi-stage hierarchy approach, similar to CNNs.\n\nData Preparation:\nThe training data should be prepared in the following format:\n\n``` sh\n|-DATASET\n  |-imagenet\n    |-train\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-class2\n    | | |-img3.jpg\n    | | |-...\n    | |-class3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-class2\n      | |-img6.jpg\n      | |-...\n      |-class3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\nThe model is pretrained on the ImageNet-22k dataset and fine-tuned on downstream tasks.\n\n[More Information Needed] about any additional preprocessing steps or data augmentation techniques used during training.",
    "answer_Chinese": "以下是有关模型 microsoft/cvt-21-384-22k 数据预处理的详细信息：\n\n调整大小：\n输入图像的大小调整为 384x384 像素的分辨率，如模型名称“cvt-21-384-22k”所示。\n\n代币化：\n该模型使用卷积令牌嵌入层将输入图像转换为令牌序列。该层通过改变卷积运算的参数来调整每个阶段的标记特征维度和标记数量。具体细节是：\n\n````嘘\n1. x q/k/v i = 展平 (Conv2d (Reshape2D(xi ), s)) ,(2)\n其中 x q/k/v i 是第 i 层 Q/K/V 矩阵的标记输入，x i 是卷积投影之前未受干扰的标记，Conv2d 是深度方向可分离卷积 [5]，实现方式为： Depth-wise Conv2d → BatchNorm2d → Point-wise Conv2d，s 指卷积核大小。\n````\n\n卷积令牌嵌入层中的卷积运算旨在通过类似于 CNN 的多级层次结构方法对局部空间上下文进行建模，从低级边缘到高阶语义基元。\n\n数据准备：\n训练数据应按以下格式准备：\n\n````嘘\n|-数据集\n  |-imagenet\n    |-火车\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-2 级\n    | | |-img3.jpg\n    | | |-...\n    | |-3级\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-2 级\n      | |-img6.jpg\n      | |-...\n      |-3级\n      | |-img7.jpg\n      | |-...\n      |-...\n````\n\n该模型在 ImageNet-22k 数据集上进行预训练，并针对下游任务进行微调。\n\n[需要更多信息]有关训练期间使用的任何其他预处理步骤或数据增强技术。",
    "answer_Arabic": "فيما يلي تفاصيل حول المعالجة المسبقة لبيانات النموذج microsoft/cvt-21-384-22k:\n\nتغيير الحجم:\nيتم تغيير حجم الصور المدخلة إلى دقة 384 × 384 بكسل، كما هو موضح في اسم الطراز \"cvt-21-384-22k\".\n\nالترميز:\nيستخدم النموذج طبقة تضمين الرمز المميز لتحويل الصورة المدخلة إلى سلسلة من الرموز المميزة. تقوم هذه الطبقة بضبط بُعد ميزة الرمز المميز وعدد الرموز المميزة في كل مرحلة عن طريق تغيير معلمات عملية الالتفاف. التفاصيل المحددة هي:\n\n``` ش\n1. x q/k/v i = مسطح (Conv2d (Reshape2D(x i ), s)) ,(2)\nحيث x q/k/v i هو إدخال الرمز المميز لمصفوفات Q/K/V في الطبقة i، x i هو الرمز المميز غير المضطرب قبل الإسقاط التلافيفي، Conv2d عبارة عن التفاف قابل للفصل من حيث العمق [5] يتم تنفيذه بواسطة: Depth-wise Conv2d → BatchNorm2d → Point-wise Conv2d، ويشير s إلى حجم نواة الالتواء.\n```\n\nتهدف عملية الالتواء في طبقة تضمين الرمز المميز إلى نمذجة السياقات المكانية المحلية، من الحواف ذات المستوى المنخفض إلى البدائيات الدلالية ذات الترتيب الأعلى، عبر نهج تسلسل هرمي متعدد المراحل، على غرار شبكات CNN.\n\nإعداد البيانات:\nيجب إعداد بيانات التدريب بالتنسيق التالي:\n\n``` ش\n|-مجموعة البيانات\n  |-imagenet\n    |-القطار\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-class2\n    | | |-img3.jpg\n    | | |-...\n    | |-class3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-فال\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-class2\n      | |-img6.jpg\n      | |-...\n      |-class3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\nتم تدريب النموذج مسبقًا على مجموعة بيانات ImageNet-22k وتم ضبطه بدقة على المهام النهائية.\n\n[يلزم مزيد من المعلومات] حول أي خطوات معالجة مسبقة إضافية أو تقنيات زيادة البيانات المستخدمة أثناء التدريب.",
    "answer_French": "Voici les détails sur le prétraitement des données du modèle microsoft/cvt-21-384-22k :\n\nRedimensionnement :\nLes images d'entrée sont redimensionnées à une résolution de 384 x 384 pixels, comme indiqué par le nom du modèle « cvt-21-384-22k ».\n\nTokenisation :\nLe modèle utilise une couche d'intégration de jetons convolutifs pour convertir l'image d'entrée en une séquence de jetons. Cette couche ajuste la dimension des caractéristiques du jeton et le nombre de jetons à chaque étape en faisant varier les paramètres de l'opération de convolution. Les détails spécifiques sont :\n\n``` merde\n1. x q/k/v i = Aplatir (Conv2d (Reshape2D(x i ), s)) ,(2)\noù x q/k/v i est l'entrée du jeton pour les matrices Q/K/V de la couche i, x i est le jeton non perturbé avant la projection convolutive, Conv2d est une convolution séparable en profondeur [5] implémentée par : Conv2d en profondeur → BatchNorm2d → Point-wise Conv2d, et s fait référence à la taille du noyau de convolution.\n```\n\nL'opération de convolution dans la couche Convolutional Token Embedding vise à modéliser les contextes spatiaux locaux, des bords de bas niveau aux primitives sémantiques d'ordre supérieur, selon une approche hiérarchique à plusieurs étages, similaire aux CNN.\n\nPréparation des données:\nLes données de formation doivent être préparées dans le format suivant :\n\n``` merde\n|-JEU DE DONNÉES\n  |-imagenet\n    |-train\n    | |-classe1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-classe2\n    | | |-img3.jpg\n    | | |-...\n    | |-classe3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-classe1\n      | |-img5.jpg\n      | |-...\n      |-classe2\n      | |-img6.jpg\n      | |-...\n      |-classe3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\nLe modèle est pré-entraîné sur l'ensemble de données ImageNet-22k et affiné sur les tâches en aval.\n\n[Plus d'informations nécessaires] sur les étapes de prétraitement supplémentaires ou les techniques d'augmentation des données utilisées pendant la formation.",
    "answer_Russian": "Вот подробности предварительной обработки данных модели microsoft/cvt-21-384-22k:\n\nИзменение размера:\nРазмер входных изображений изменяется до разрешения 384x384 пикселей, на что указывает название модели «cvt-21-384-22k».\n\nТокенизация:\nМодель использует слой внедрения сверточных токенов для преобразования входного изображения в последовательность токенов. Этот слой регулирует размер объекта токена и количество токенов на каждом этапе путем изменения параметров операции свертки. Конкретные детали:\n\n``` ш\n1. x q/k/v i = Flatten (Conv2d (Reshape2D(x i), s)) ,(2)\nгде x q/k/v i — входной токен для матриц Q/K/V на слое i, x i — невозмущенный токен до сверточной проекции, Conv2d — это отделимая по глубине свертка [5], реализованная с помощью: Conv2d по глубине → BatchNorm2d → Поточечный Conv2d, а s относится к размеру ядра свертки.\n```\n\nОперация свертки на уровне внедрения сверточных токенов направлена ​​на моделирование локальных пространственных контекстов, от ребер низкого уровня до семантических примитивов более высокого порядка, с помощью многоступенчатого иерархического подхода, аналогичного CNN.\n\nПодготовка данных:\nДанные обучения должны быть подготовлены в следующем формате:\n\n``` ш\n|-НАБОР ДАННЫХ\n  |-изображение\n    |-поезд\n    | |-класс1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-класс2\n    | | |-img3.jpg\n    | | |-...\n    | |-класс3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-вал\n      |-класс1\n      | |-img5.jpg\n      | |-...\n      |-класс2\n      | |-img6.jpg\n      | |-...\n      |-класс3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\nМодель предварительно обучена на наборе данных ImageNet-22k и точно настроена для последующих задач.\n\n[Требуется дополнительная информация] о любых дополнительных шагах предварительной обработки или методах увеличения данных, используемых во время обучения.",
    "answer_Japanese": "モデル Microsoft/cvt-21-384-22k のデータの前処理の詳細は次のとおりです。\n\nサイズ変更:\nモデル名「cvt-21-384-22k」で示されているように、入力イメージは 384x384 ピクセルの解像度にサイズ変更されます。\n\nトークン化:\nこのモデルは、畳み込みトークン埋め込み層を使用して、入力画像をトークンのシーケンスに変換します。この層は、畳み込み演算のパラメーターを変更することで、各段階でのトークン特徴の次元とトークンの数を調整します。具体的な詳細は次のとおりです。\n\n「」し\n1. x q/k/v i = 平坦化 (Conv2d (Reshape2D(x i ), s)) ,(2)\nここで、x q/k/v i は層 i の Q/K/V 行列のトークン入力、x i は畳み込み投影前の摂動のないトークン、Conv2d は深さ方向の分離可能な畳み込み [5] で実装されます。 Depth-wise Conv2d → BatchNorm2d → Point-wise Conv2d、s はコンボリューション カーネル サイズを指します。\n「」\n\n畳み込みトークン埋め込み層の畳み込み演算は、CNN と同様に、多段階の階層アプローチを通じて、低レベルのエッジから高次のセマンティック プリミティブまで、ローカルの空間コンテキストをモデル化することを目的としています。\n\nデータの準備:\nトレーニング データは次の形式で準備する必要があります。\n\n「」し\n|-データセット\n  |-イメージネット\n    |-電車\n    | |-クラス1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-クラス2\n    | | |-img3.jpg\n    | | |-...\n    | |-クラス3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-クラス1\n      | |-img5.jpg\n      | |-...\n      |-クラス2\n      | |-img6.jpg\n      | |-...\n      |-クラス3\n      | |-img7.jpg\n      | |-...\n      |-...\n「」\n\nモデルは ImageNet-22k データセットで事前トレーニングされ、ダウンストリーム タスクで微調整されます。\n\nトレーニング中に使用される追加の前処理手順やデータ拡張手法に関する [追加情報が必要]。"
  },
  {
    "question": "training_regime",
    "chain": "generation",
    "answer": "Based on the provided references, here are the training hyperparameters for the model microsoft/cvt-21-384-22k:\n\nModel Architecture:\n- CvT-21 model with 21 Transformer Blocks in total\n- Input image resolution: 384x384\n- 32M parameters\n- 24.9 GFLOPs computational cost\n\nDataset:\n- Pre-trained on ImageNet-22k dataset\n\n[More Information Needed] for the following hyperparameters:\n- Learning rate schedule\n- Optimizer \n- Batch size\n- Number of training epochs\n- Data augmentation techniques used\n- Regularization methods (e.g., weight decay, dropout)\n- Loss function\n\nThe references do not provide the specific values for these hyperparameters. More details would be needed from the model developers or the training code to fill in this information in the model card.",
    "answer_Chinese": "根据提供的参考资料，以下是模型 microsoft/cvt-21-384-22k 的训练超参数：\n\n模型架构：\n- CvT-21 模型共有 21 个变压器块\n- 输入图像分辨率：384x384\n- 32M参数\n- 24.9 GFLOPs 计算成本\n\n数据集：\n- 在 ImageNet-22k 数据集上进行预训练\n\n[需要更多信息]以下超参数：\n- 学习率表\n- 优化器 \n- 批量大小\n- 训练纪元数\n- 使用的数据增强技术\n- 正则化方法（例如权重衰减、dropout）\n- 损失函数\n\n参考文献没有提供这些超参数的具体值。模型开发人员或培训代码需要更多详细信息才能在模型卡中填写此信息。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك معلمات التدريب الفائقة للنموذج microsoft/cvt-21-384-22k:\n\nالعمارة النموذجية:\n- طراز CvT-21 بإجمالي 21 كتلة محولة\n- دقة صورة الإدخال: 384x384\n- 32 مليون معلمة\n- 24.9 تكلفة حسابية لـ GFLOPs\n\nمجموعة البيانات:\n- تم تدريبه مسبقًا على مجموعة بيانات ImageNet-22k\n\n[مطلوب مزيد من المعلومات] للمعلمات الفائقة التالية:\n- جدول معدل التعلم\n- محسن \n- حجم الدفعة\n- عدد فترات التدريب\n- تقنيات زيادة البيانات المستخدمة\n- طرق التنظيم (على سبيل المثال، تسوس الوزن، التسرب)\n- فقدان وظيفة\n\nلا توفر المراجع القيم المحددة لهذه المعلمات الفائقة. ستكون هناك حاجة إلى مزيد من التفاصيل من مطوري النماذج أو رمز التدريب لملء هذه المعلومات في بطاقة النموذج.",
    "answer_French": "Sur la base des références fournies, voici les hyperparamètres de formation pour le modèle Microsoft/cvt-21-384-22k :\n\nArchitecture du modèle :\n- Modèle CvT-21 avec 21 blocs transformateurs au total\n- Résolution de l'image d'entrée : 384x384\n- 32 millions de paramètres\n- Coût de calcul de 24,9 GFLOP\n\nBase de données:\n- Pré-formé sur le jeu de données ImageNet-22k\n\n[Plus d'informations nécessaires] pour les hyperparamètres suivants :\n- Grille tarifaire d'apprentissage\n- Optimiseur \n- Taille du lot\n- Nombre d'époques de formation\n- Techniques d'augmentation des données utilisées\n- Méthodes de régularisation (par exemple, perte de poids, abandon)\n- Fonction de perte\n\nLes références ne fournissent pas les valeurs spécifiques de ces hyperparamètres. Plus de détails seraient nécessaires de la part des développeurs du modèle ou du code de formation pour remplir ces informations dans la fiche modèle.",
    "answer_Russian": "На основе предоставленных ссылок приведены гиперпараметры обучения для модели microsoft/cvt-21-384-22k:\n\nМодельная архитектура:\n- Модель CvT-21 всего с 21 трансформаторным блоком\n- Разрешение входного изображения: 384x384\n- 32 млн параметров\n- Вычислительные затраты 24,9 GFLOPs\n\nНабор данных:\n- Предварительное обучение на наборе данных ImageNet-22k.\n\n[Требуется дополнительная информация] для следующих гиперпараметров:\n- График обучения\n- Оптимизатор \n- Размер партии\n- Количество эпох обучения\n- Используемые методы увеличения данных\n- Методы регуляризации (например, снижение веса, отсев)\n- Функция потери\n\nВ ссылках не указаны конкретные значения для этих гиперпараметров. Для заполнения этой информации в карточке модели потребуется дополнительная информация от разработчиков модели или код обучения.",
    "answer_Japanese": "提供された参照に基づいて、モデル Microsoft/cvt-21-384-22k のトレーニング ハイパーパラメーターを次に示します。\n\nモデルのアーキテクチャ:\n- 合計21個のトランスブロックを備えたCvT-21モデル\n- 入力画像解像度: 384x384\n- 32M パラメータ\n- 24.9 GFLOP の計算コスト\n\nデータセット:\n- ImageNet-22k データセットで事前トレーニング済み\n\n次のハイパーパラメータについては [追加情報が必要]:\n- 学習速度スケジュール\n- オプティマイザー \n- バッチサイズ\n- トレーニング エポックの数\n- 使用されるデータ拡張技術\n- 正則化方法 (重み減衰、ドロップアウトなど)\n- 損失関数\n\nリファレンスでは、これらのハイパーパラメータの具体的な値は提供されていません。この情報をモデル カードに記入するには、モデル開発者またはトレーニング コードから詳細が必要になります。"
  },
  {
    "question": "speeds_sizes_times",
    "chain": "generation",
    "answer": "Here are the details about the model microsoft/cvt-21-384-22k based on the provided references:\n\nModel Architecture:\n- Uses Convolutional Projection layers in the Transformer blocks, which is a generalization of the original Transformer block design\n- Key and value projections are subsampled using convolutions with stride 2, while query projection uses stride 1\n- Depth-wise separable convolutions are used for computational efficiency\n\nModel Parameters and Compute:\n- Resolution: 384x384 \n- Parameters: 32M\n- GFLOPs: 24.9\n\nPerformance:\n- Top-1 Accuracy on ImageNet-1k: 84.9%\n\nTraining Data:\n- Pre-trained on ImageNet-22k dataset\n\n[More Information Needed] for:\n- Throughput \n- Training start and end times\n- Checkpoint sizes\n\nThe code for the model is not provided in the references, so [More Information Needed] for the model implementation code as well.\n\nIn summary, the microsoft/cvt-21-384-22k model is a Convolutional vision Transformer with 21 blocks total, 32M parameters, and achieves 84.9% top-1 accuracy on ImageNet-1k when pre-trained on ImageNet-22k at 384x384 resolution. More details would be needed to fully characterize the model training and resulting artifacts.",
    "answer_Chinese": "以下是根据提供的参考资料有关模型 microsoft/cvt-21-384-22k 的详细信息：\n\n模型架构：\n- 在 Transformer 块中使用卷积投影层，这是原始 Transformer 块设计的概括\n- 键和值投影使用步幅为 2 的卷积进行二次采样，而查询投影使用步幅 1\n- 使用深度可分离卷积来提高计算效率\n\n模型参数及计算：\n- 分辨率：384x384 \n- 参数：32M\n- GFLOPs：24.9\n\n表现：\n- ImageNet-1k 上的 Top-1 准确率：84.9%\n\n训练数据：\n- 在 ImageNet-22k 数据集上进行预训练\n\n[需要更多信息]：\n- 吞吐量 \n- 训练开始和结束时间\n- 检查点大小\n\n参考文献中未提供模型的代码，因此[需要更多信息]模型实现代码。\n\n总之，microsoft/cvt-21-384-22k 模型是一个卷积视觉 Transformer，总共有 21 个块、32M 个参数，在 ImageNet-22k 上以 384x384 分辨率进行预训练时，在 ImageNet-1k 上实现了 84.9% 的 top-1 准确率。需要更多细节来充分描述模型训练和生成的工件的特征。",
    "answer_Arabic": "فيما يلي التفاصيل حول الطراز microsoft/cvt-21-384-22k بناءً على المراجع المتوفرة:\n\nالعمارة النموذجية:\n- يستخدم طبقات الإسقاط التلافيفية في كتل المحولات، وهو تعميم لتصميم كتلة المحولات الأصلي\n- يتم أخذ عينات فرعية من إسقاطات المفتاح والقيمة باستخدام الالتفافات مع الخطوة 2، بينما يستخدم إسقاط الاستعلام الخطوة 1\n- تُستخدم التلافيفات القابلة للفصل من حيث العمق لتحقيق الكفاءة الحسابية\n\nمعلمات النموذج والحساب:\n- الدقة: 384x384 \n- المعلمات: 32 م\n- GFLOPS: 24.9\n\nأداء:\n- الدقة الأولى على ImageNet-1k: 84.9%\n\nبيانات التدريب:\n- تم تدريبه مسبقًا على مجموعة بيانات ImageNet-22k\n\n[مطلوب مزيد من المعلومات] من أجل:\n- الإنتاجية \n- مواعيد بداية ونهاية التدريب\n- أحجام نقاط التفتيش\n\nلم يتم توفير رمز النموذج في المراجع، لذلك [يلزم مزيد من المعلومات] لرمز تنفيذ النموذج أيضًا.\n\nباختصار، نموذج microsoft/cvt-21-384-22k عبارة عن محول رؤية تلافيفي بإجمالي 21 كتلة، و32 مليون معلمة، ويحقق دقة من الدرجة الأولى بنسبة 84.9% على ImageNet-1k عند تدريبه مسبقًا على ImageNet-22k بدقة 384x384 . ستكون هناك حاجة إلى مزيد من التفاصيل لتوصيف التدريب النموذجي والتحف الناتجة بشكل كامل.",
    "answer_French": "Voici les détails sur le modèle microsoft/cvt-21-384-22k basés sur les références fournies :\n\nArchitecture du modèle :\n- Utilise des couches de projection convolutive dans les blocs Transformer, ce qui est une généralisation de la conception originale du bloc Transformer\n- Les projections de clé et de valeur sont sous-échantillonnées à l'aide de convolutions avec le pas 2, tandis que la projection de requête utilise le pas 1\n- Des convolutions séparables en profondeur sont utilisées pour l'efficacité du calcul\n\nParamètres du modèle et calcul :\n- Résolution : 384x384 \n- Paramètres : 32M\n- GFLOP : 24,9\n\nPerformance:\n- Précision Top-1 sur ImageNet-1k : 84,9 %\n\nDonnées d'entraînement:\n- Pré-formé sur le jeu de données ImageNet-22k\n\n[Plus d'informations nécessaires] pour :\n- Débit \n- Heures de début et de fin des formations\n- Tailles des points de contrôle\n\nLe code du modèle n'est pas fourni dans les références, donc [Plus d'informations nécessaires] pour le code d'implémentation du modèle également.\n\nEn résumé, le modèle Microsoft/cvt-21-384-22k est un transformateur de vision convolutive avec 21 blocs au total, 32 millions de paramètres et atteint une précision top 1 de 84,9 % sur ImageNet-1k lorsqu'il est pré-entraîné sur ImageNet-22k à une résolution de 384 x 384. . Plus de détails seraient nécessaires pour caractériser pleinement la formation du modèle et les artefacts qui en résultent.",
    "answer_Russian": "Вот подробности о модели microsoft/cvt-21-384-22k на основе предоставленных ссылок:\n\nМодельная архитектура:\n- Использует слои сверточной проекции в блоках Transformer, которые являются обобщением исходной конструкции блока Transformer.\n- Проекции ключей и значений формируются с использованием сверток с шагом 2, тогда как проекция запроса использует шаг 1.\n- Для повышения эффективности вычислений используются отделимые по глубине свертки.\n\nПараметры модели и расчет:\n- Разрешение: 384x384 \n- Параметры: 32М\n- ГФЛОПС: 24,9\n\nПроизводительность:\n- Точность топ-1 на ImageNet-1k: 84,9%\n\nДанные обучения:\n- Предварительное обучение на наборе данных ImageNet-22k.\n\n[Требуется дополнительная информация] для:\n- Пропускная способность \n- Время начала и окончания обучения\n- Размеры КПП\n\nКод модели не указан в ссылках, поэтому [Необходима дополнительная информация] и для кода реализации модели.\n\nТаким образом, модель microsoft/cvt-21-384-22k представляет собой преобразователь сверточного зрения с общим количеством 21 блока, 32M параметров и достигает точности Top-1 84,9% на ImageNet-1k при предварительном обучении на ImageNet-22k с разрешением 384x384. . Чтобы полностью охарактеризовать обучение модели и полученные артефакты, потребуется больше деталей.",
    "answer_Japanese": "提供された参考資料に基づいたモデル Microsoft/cvt-21-384-22k の詳細は次のとおりです。\n\nモデルのアーキテクチャ:\n- Transformer ブロックで畳み込み射影レイヤーを使用します。これは、元の Transformer ブロック設計を一般化したものです。\n- キーと値の投影はストライド 2 の畳み込みを使用してサブサンプリングされますが、クエリ投影はストライド 1 を使用します。\n- 計算効率を高めるために、深さ方向に分離可能な畳み込みが使用されます\n\nパラメータのモデル化と計算:\n- 解像度: 384x384 \n- パラメータ: 32M\n- GFLOP: 24.9\n\nパフォーマンス：\n- ImageNet-1k でトップ 1 の精度: 84.9%\n\nトレーニングデータ:\n- ImageNet-22k データセットで事前トレーニング済み\n\n[さらに詳しい情報が必要]:\n- スループット \n- トレーニングの開始時間と終了時間\n- チェックポイントのサイズ\n\nモデルのコードはリファレンスに記載されていないため、モデルの実装コードについても [追加情報が必要] です。\n\n要約すると、microsoft/cvt-21-384-22k モデルは、合計 21 ブロック、3,200 万のパラメーターを備えた畳み込みビジョン Transformer であり、384x384 解像度の ImageNet-22k で事前トレーニングした場合、ImageNet-1k で 84.9% のトップ 1 精度を達成します。 。モデルのトレーニングとその結果として得られるアーティファクトを完全に特徴付けるには、さらに詳細な情報が必要になります。"
  },
  {
    "question": "testing_data",
    "chain": "generation",
    "answer": "Based on the provided references, the model microsoft/cvt-21-384-22k was evaluated on the following benchmarks and datasets:\n\n1. ImageNet-1k (also known as ILSVRC 2012): A large-scale image classification dataset with 1.3M images and 1k classes.\n\n2. ImageNet-22k: A superset of ImageNet-1k with 22k classes and 14M images, used for pre-training the model.\n\n3. ImageNet Real: [More Information Needed]\n\n4. ImageNet V2 matched frequency: [More Information Needed]\n\n5. Downstream tasks (fine-tuned from ImageNet-22k pre-trained model):\n   - CIFAR-10\n   - CIFAR-100\n   - Oxford-IIIT-Pet\n   - Oxford-IIIT-Flower\n\nThe model was pre-trained on the larger ImageNet-22k dataset and then fine-tuned and evaluated on the ImageNet-1k validation set, achieving a top-1 accuracy of 87.7%.",
    "answer_Chinese": "根据提供的参考，模型 microsoft/cvt-21-384-22k 在以下基准和数据集上进行了评估：\n\n1. ImageNet-1k（也称为 ILSVRC 2012）：具有 130 万张图像和 1k 个类别的大规模图像分类数据集。\n\n2. ImageNet-22k：ImageNet-1k的超集，具有22k类和14M图像，用于预训练模型。\n\n3. ImageNet Real：[需要更多信息]\n\n4. ImageNet V2 匹配频率：[需要更多信息]\n\n5.下游任务（根据ImageNet-22k预训练模型进行微调）：\n   - CIFAR-10\n   - CIFAR-100\n   - 牛津-IIIT-宠物\n   - 牛津-IIIT-花\n\n该模型在较大的 ImageNet-22k 数据集上进行了预训练，然后在 ImageNet-1k 验证集上进行了微调和评估，实现了 87.7% 的 top-1 准确率。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، تم تقييم النموذج microsoft/cvt-21-384-22k بناءً على المعايير ومجموعات البيانات التالية:\n\n1. ImageNet-1k (المعروف أيضًا باسم ILSVRC 2012): مجموعة بيانات واسعة النطاق لتصنيف الصور تحتوي على 1.3 مليون صورة وفئة 1k.\n\n2. ImageNet-22k: مجموعة شاملة من ImageNet-1k تحتوي على فئات 22 كيلو و14 مليون صورة، تُستخدم للتدريب المسبق للنموذج.\n\n3. إيماج نت ريال: [بحاجة إلى مزيد من المعلومات]\n\n4. التردد المطابق لـ ImageNet V2: [بحاجة إلى مزيد من المعلومات]\n\n5. المهام النهائية (المضبوطة بدقة من نموذج ImageNet-22k المُدرب مسبقًا):\n   - سيفار-10\n   - سيفار-100\n   - أكسفورد-IIIT-Pet\n   - زهرة أكسفورد-IIIT\n\nتم تدريب النموذج مسبقًا على مجموعة بيانات ImageNet-22k الأكبر ثم تم ضبطه وتقييمه على مجموعة التحقق من صحة ImageNet-1k، مما حقق دقة من الدرجة الأولى بنسبة 87.7%.",
    "answer_French": "Sur la base des références fournies, le modèle Microsoft/cvt-21-384-22k a été évalué sur les références et ensembles de données suivants :\n\n1. ImageNet-1k (également connu sous le nom d'ILSVRC 2012) : un ensemble de données de classification d'images à grande échelle avec 1,3 million d'images et 1 000 classes.\n\n2. ImageNet-22k : un surensemble d'ImageNet-1k avec 22 000 classes et 14 millions d'images, utilisé pour la pré-entraînement du modèle.\n\n3. ImageNet Real : [Plus d'informations nécessaires]\n\n4. Fréquence adaptée à ImageNet V2 : [Plus d'informations nécessaires]\n\n5. Tâches en aval (affinées à partir du modèle pré-entraîné ImageNet-22k) :\n   -CIFAR-10\n   -CIFAR-100\n   - Oxford-IIIT-Animal\n   - Oxford-IIIT-Fleur\n\nLe modèle a été pré-entraîné sur le plus grand ensemble de données ImageNet-22k, puis affiné et évalué sur l'ensemble de validation ImageNet-1k, atteignant une précision top 1 de 87,7 %.",
    "answer_Russian": "На основании предоставленных ссылок модель microsoft/cvt-21-384-22k оценивалась по следующим критериям и наборам данных:\n\n1. ImageNet-1k (также известный как ILSVRC 2012): крупномасштабный набор данных для классификации изображений с 1,3 млн изображений и 1 тыс. классов.\n\n2. ImageNet-22k: расширенный набор ImageNet-1k с 22 тысячами классов и 14 миллионами изображений, используемый для предварительного обучения модели.\n\n3. ImageNet Real: [Необходима дополнительная информация]\n\n4. Соответствующая частота ImageNet V2: [Необходима дополнительная информация]\n\n5. Последующие задачи (тонкая настройка на основе предварительно обученной модели ImageNet-22k):\n   - СИФАР-10\n   - СИФАР-100\n   - Оксфорд-IIIT-Пет\n   - Оксфорд-IIIT-Цветок\n\nМодель была предварительно обучена на более крупном наборе данных ImageNet-22k, а затем точно настроена и оценена на проверочном наборе ImageNet-1k, достигнув точности высшего уровня 87,7%.",
    "answer_Japanese": "提供された参照に基づいて、モデル Microsoft/cvt-21-384-22k は次のベンチマークとデータセットで評価されました。\n\n1. ImageNet-1k (ILSVRC 2012 としても知られる): 130 万個の画像と 1,000 個のクラスを含む大規模な画像分類データセット。\n\n2. ImageNet-22k: 22k クラスと 14M イメージを含む ImageNet-1k のスーパーセットで、モデルの事前トレーニングに使用されます。\n\n3. ImageNet Real: [さらに詳しい情報が必要です]\n\n4. ImageNet V2 の一致した周波数: [さらに詳しい情報が必要です]\n\n5. ダウンストリーム タスク (ImageNet-22k の事前トレーニング済みモデルから微調整):\n   - CIFAR-10\n   - CIFAR-100\n   - オックスフォード-IIIT-ペット\n   - オックスフォード-IIIT-フラワー\n\nモデルは、より大規模な ImageNet-22k データセットで事前トレーニングされた後、ImageNet-1k 検証セットで微調整および評価され、87.7% というトップ 1 の精度を達成しました。"
  },
  {
    "question": "testing_factors",
    "chain": "generation",
    "answer": "Based on the provided references about the microsoft/cvt-21-384-22k model, here are some foreseeable characteristics that may influence the model's behavior:\n\nDomain and Context:\n- The model was pre-trained on the large-scale ImageNet-22k dataset and fine-tuned on various downstream tasks. This suggests it is well-suited for general image classification tasks, but performance on more specialized domains would need further evaluation.\n\nPopulation Subgroups:\n[More Information Needed] - The references do not provide information on how the model performs across different population subgroups. Disaggregated evaluation across factors like demographics, geographies, etc. would be needed to uncover any performance disparities.\n\nOther Notable Characteristics:\n- The model incorporates convolutional projections and token embeddings, allowing it to model local spatial relationships without positional embeddings (References 3-6, 8). This simplifies the architecture for variable input resolutions common in vision tasks.\n- Despite having fewer parameters and lower computational cost than some other models, it achieved state-of-the-art performance on ImageNet-1k. However, computational efficiency on downstream tasks is not discussed.\n- Ablation studies validated the architecture design choices, but more details on the specific experiments would help assess robustness.\n\nIn summary, the microsoft/cvt-21-384-22k model shows promise for general image classification, with architecture innovations to efficiently handle vision tasks. However, more disaggregated evaluation is needed to characterize performance across different subgroups and specialized domains. The provided information focuses more on architecture than behavior across diverse contexts.",
    "answer_Chinese": "根据提供的有关 microsoft/cvt-21-384-22k 模型的参考资料，以下是一些可能影响模型行为的可预见特征：\n\n领域和上下文：\n- 该模型在大规模 ImageNet-22k 数据集上进行了预训练，并针对各种下游任务进行了微调。这表明它非常适合一般图像分类任务，但在更专业领域的性能需要进一步评估。\n\n人口亚组：\n[需要更多信息] - 参考文献未提供有关模型如何在不同人群亚组中执行的信息。需要对人口统计、地理位置等因素进行分类评估，以发现任何绩效差异。\n\n其他显着特征：\n- 该模型结合了卷积投影和标记嵌入，使其能够在没有位置嵌入的情况下对局部空间关系进行建模（参考文献 3-6、8）。这简化了视觉任务中常见的可变输入分辨率的架构。\n- 尽管比其他一些模型具有更少的参数和更低的计算成本，但它在 ImageNet-1k 上实现了最先进的性能。然而，没有讨论下游任务的计算效率。\n- 消融研究验证了架构设计选择，但具体实验的更多细节将有助于评估稳健性。\n\n总之，microsoft/cvt-21-384-22k 模型显示了通用图像分类的前景，其架构创新可有效处理视觉任务。然而，需要更分类的评估来描述不同子群体和专业领域的绩效特征。所提供的信息更多地关注架构而不是跨不同环境的行为。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة حول نموذج microsoft/cvt-21-384-22k، إليك بعض الخصائص المتوقعة التي قد تؤثر على سلوك النموذج:\n\nالمجال والسياق:\n- تم تدريب النموذج مسبقًا على مجموعة بيانات ImageNet-22k واسعة النطاق وتم ضبطه على العديد من المهام النهائية. يشير هذا إلى أنه مناسب تمامًا لمهام تصنيف الصور العامة، لكن الأداء في المجالات الأكثر تخصصًا سيحتاج إلى مزيد من التقييم.\n\nالمجموعات الفرعية السكانية:\n[مطلوب مزيد من المعلومات] - لا توفر المراجع معلومات حول كيفية أداء النموذج عبر مجموعات فرعية مختلفة من السكان. ستكون هناك حاجة إلى تقييم مصنف عبر عوامل مثل التركيبة السكانية والجغرافية وما إلى ذلك للكشف عن أي فوارق في الأداء.\n\nالخصائص البارزة الأخرى:\n- يشتمل النموذج على إسقاطات تلافيفية وتضمينات رمزية، مما يسمح له بنمذجة العلاقات المكانية المحلية دون تضمينات موضعية (المراجع 3-6، 8). يؤدي هذا إلى تبسيط بنية دقة الإدخال المتغيرة الشائعة في مهام الرؤية.\n- على الرغم من وجود معلمات أقل وتكلفة حسابية أقل من بعض النماذج الأخرى، فقد حقق أداءً متطورًا على ImageNet-1k. ومع ذلك، لا تتم مناقشة الكفاءة الحسابية في المهام النهائية.\n- أثبتت دراسات الاستئصال صحة اختيارات التصميم المعماري، ولكن المزيد من التفاصيل حول التجارب المحددة من شأنها أن تساعد في تقييم المتانة.\n\nباختصار، يُظهر نموذج microsoft/cvt-21-384-22k وعدًا بالتصنيف العام للصور، مع ابتكارات معمارية للتعامل بكفاءة مع مهام الرؤية. ومع ذلك، هناك حاجة إلى تقييم أكثر تفصيلاً لوصف الأداء عبر مجموعات فرعية مختلفة ومجالات متخصصة. تركز المعلومات المقدمة على الهندسة المعمارية أكثر من السلوك عبر سياقات متنوعة.",
    "answer_French": "Sur la base des références fournies concernant le modèle Microsoft/cvt-21-384-22k, voici quelques caractéristiques prévisibles qui peuvent influencer le comportement du modèle :\n\nDomaine et contexte :\n- Le modèle a été pré-entraîné sur l'ensemble de données à grande échelle ImageNet-22k et affiné sur diverses tâches en aval. Cela suggère qu’il est bien adapté aux tâches générales de classification d’images, mais que ses performances dans des domaines plus spécialisés nécessiteraient une évaluation plus approfondie.\n\nSous-groupes de population :\n[Plus d'informations nécessaires] - Les références ne fournissent pas d'informations sur les performances du modèle dans différents sous-groupes de population. Une évaluation ventilée selon des facteurs tels que les données démographiques, les zones géographiques, etc. serait nécessaire pour découvrir toute disparité de performance.\n\nAutres caractéristiques notables :\n- Le modèle intègre des projections convolutives et des intégrations de jetons, lui permettant de modéliser des relations spatiales locales sans intégrations positionnelles (Références 3-6, 8). Cela simplifie l'architecture des résolutions d'entrée variables courantes dans les tâches de vision.\n- Malgré moins de paramètres et un coût de calcul inférieur à celui de certains autres modèles, il a atteint des performances de pointe sur ImageNet-1k. Cependant, l’efficacité des calculs sur les tâches en aval n’est pas discutée.\n- Des études d'ablation ont validé les choix de conception de l'architecture, mais plus de détails sur les expériences spécifiques permettraient d'évaluer la robustesse.\n\nEn résumé, le modèle Microsoft/cvt-21-384-22k s'avère prometteur pour la classification générale des images, avec des innovations architecturales pour gérer efficacement les tâches de vision. Cependant, une évaluation plus désagrégée est nécessaire pour caractériser les performances des différents sous-groupes et domaines spécialisés. Les informations fournies se concentrent davantage sur l’architecture que sur le comportement dans divers contextes.",
    "answer_Russian": "На основе предоставленных ссылок на модель microsoft/cvt-21-384-22k можно выделить некоторые прогнозируемые характеристики, которые могут повлиять на поведение модели:\n\nДомен и контекст:\n- Модель была предварительно обучена на крупномасштабном наборе данных ImageNet-22k и точно настроена для выполнения различных последующих задач. Это говорит о том, что он хорошо подходит для общих задач классификации изображений, но производительность в более специализированных областях потребует дальнейшей оценки.\n\nПодгруппы населения:\n[Требуется дополнительная информация] — ссылки не предоставляют информацию о том, как модель работает в различных подгруппах населения. Для выявления любых различий в производительности потребуется дезагрегированная оценка по таким факторам, как демография, географическое положение и т. д.\n\nДругие примечательные характеристики:\n- Модель включает в себя сверточные проекции и вложения токенов, что позволяет моделировать локальные пространственные отношения без позиционных вложений (ссылки 3-6, 8). Это упрощает архитектуру для переменных входных разрешений, часто встречающихся в задачах машинного зрения.\n- Несмотря на меньшее количество параметров и меньшие вычислительные затраты, чем у некоторых других моделей, он обеспечивает высочайшую производительность на ImageNet-1k. Однако эффективность вычислений в последующих задачах не обсуждается.\n- Исследования абляции подтвердили правильность выбора архитектуры, но более подробная информация о конкретных экспериментах поможет оценить надежность.\n\nПодводя итог, можно сказать, что модель microsoft/cvt-21-384-22k перспективна для общей классификации изображений, а архитектурные инновации позволяют эффективно решать задачи машинного зрения. Однако необходима более дезагрегированная оценка, чтобы охарактеризовать эффективность работы в различных подгруппах и специализированных областях. Предоставленная информация больше фокусируется на архитектуре, чем на поведении в различных контекстах.",
    "answer_Japanese": "Microsoft/cvt-21-384-22k モデルに関して提供されている参考資料に基づいて、モデルの動作に影響を与える可能性があるいくつかの予測可能な特性を次に示します。\n\nドメインとコンテキスト:\n- モデルは大規模な ImageNet-22k データセットで事前トレーニングされ、さまざまな下流タスクで微調整されました。これは、一般的な画像分類タスクには適しているが、より特殊なドメインでのパフォーマンスについてはさらなる評価が必要であることを示唆しています。\n\n人口サブグループ:\n[さらなる情報が必要] - 参考文献には、さまざまな集団サブグループ間でモデルがどのように機能するかについての情報が提供されていません。パフォーマンスの差異を明らかにするには、人口統計や地域などの要素にまたがる細分化された評価が必要になります。\n\nその他の注目すべき特徴:\n- このモデルには畳み込み射影とトークン埋め込みが組み込まれており、位置埋め込みなしで局所的な空間関係をモデル化できます (参考文献 3-6、8)。これにより、視覚タスクで一般的な可変入力解像度のアーキテクチャが簡素化されます。\n- 他のモデルよりもパラメータが少なく、計算コストが低いにもかかわらず、ImageNet-1k で最先端のパフォーマンスを達成しました。ただし、下流タスクの計算効率については議論されていません。\n- アブレーション研究によりアーキテクチャ設計の選択が検証されましたが、特定の実験の詳細は堅牢性の評価に役立ちます。\n\n要約すると、microsoft/cvt-21-384-22k モデルは、視覚タスクを効率的に処理するためのアーキテクチャの革新を備えた、一般的な画像分類に有望であることを示しています。ただし、さまざまなサブグループや特殊なドメイン全体のパフォーマンスを特徴付けるには、より細分化された評価が必要です。提供される情報は、さまざまなコンテキストにわたる動作よりもアーキテクチャに重点を置いています。"
  },
  {
    "question": "testing_metrics",
    "chain": "generation",
    "answer": "Based on the provided references, the main metrics used for evaluating the microsoft/cvt-21-384-22k model are:\n\n1. Top-1 accuracy on the ImageNet-1k validation set, after pre-training on ImageNet-22k.\n\n2. Performance on downstream tasks such as CIFAR-10/100, Oxford-IIIT-Pet, and Oxford-IIIT-Flower, after fine-tuning the model pre-trained on ImageNet-22k.\n\nThe references do not provide specific information about the tradeoffs between different errors or the exact metrics used for the downstream tasks. [More Information Needed] regarding the specific error types and their tradeoffs, as well as the evaluation metrics for the mentioned downstream tasks.",
    "answer_Chinese": "根据提供的参考资料，用于评估 microsoft/cvt-21-384-22k 模型的主要指标包括：\n\n1. 在 ImageNet-22k 上进行预训练后，在 ImageNet-1k 验证集上达到 Top-1 准确率。\n\n2. 对在 ImageNet-22k 上预训练的模型进行微调后，在 CIFAR-10/100、Oxford-IIIT-Pet 和 Oxford-IIIT-Flower 等下游任务上的性能。\n\n这些参考文献没有提供有关不同错误之间权衡或用于下游任务的确切指标的具体信息。 [需要更多信息]关于特定错误类型及其权衡，以及上述下游任务的评估指标。",
    "answer_Arabic": "بناءً على المراجع المتوفرة، فإن المقاييس الرئيسية المستخدمة لتقييم نموذج Microsoft/cvt-21-384-22k هي:\n\n1. أعلى دقة في مجموعة التحقق من صحة ImageNet-1k، بعد التدريب المسبق على ImageNet-22k.\n\n2. الأداء في المهام النهائية مثل CIFAR-10/100، وOxford-IIIT-Pet، وOxford-IIIT-Flower، بعد ضبط النموذج الذي تم تدريبه مسبقًا على ImageNet-22k.\n\nلا توفر المراجع معلومات محددة حول المفاضلات بين الأخطاء المختلفة أو المقاييس الدقيقة المستخدمة للمهام النهائية. [مطلوب مزيد من المعلومات] فيما يتعلق بأنواع الأخطاء المحددة ومقايضاتها، بالإضافة إلى مقاييس التقييم للمهام النهائية المذكورة.",
    "answer_French": "Sur la base des références fournies, les principales mesures utilisées pour évaluer le modèle Microsoft/cvt-21-384-22k sont :\n\n1. Précision Top-1 sur l'ensemble de validation ImageNet-1k, après pré-formation sur ImageNet-22k.\n\n2. Performances sur les tâches en aval telles que CIFAR-10/100, Oxford-IIIT-Pet et Oxford-IIIT-Flower, après avoir affiné le modèle pré-entraîné sur ImageNet-22k.\n\nLes références ne fournissent pas d'informations spécifiques sur les compromis entre les différentes erreurs ou les métriques exactes utilisées pour les tâches en aval. [Plus d'informations nécessaires] concernant les types d'erreurs spécifiques et leurs compromis, ainsi que les mesures d'évaluation pour les tâches en aval mentionnées.",
    "answer_Russian": "На основе предоставленных ссылок основными метриками, используемыми для оценки модели microsoft/cvt-21-384-22k, являются:\n\n1. Точность высшего уровня 1 на проверочном наборе ImageNet-1k после предварительного обучения на ImageNet-22k.\n\n2. Производительность при выполнении последующих задач, таких как CIFAR-10/100, Oxford-IIIT-Pet и Oxford-IIIT-Flower, после тонкой настройки модели, предварительно обученной на ImageNet-22k.\n\nСсылки не предоставляют конкретной информации о компромиссах между различными ошибками или точных показателях, используемых для последующих задач. [Требуется дополнительная информация] относительно конкретных типов ошибок и их компромиссов, а также показателей оценки для упомянутых последующих задач.",
    "answer_Japanese": "提供されている参考資料に基づいて、microsoft/cvt-21-384-22k モデルの評価に使用される主な指標は次のとおりです。\n\n1. ImageNet-22k での事前トレーニング後、ImageNet-1k 検証セットでトップ 1 の精度。\n\n2. ImageNet-22k で事前トレーニングされたモデルを微調整した後の、CIFAR-10/100、Oxford-IIIT-Pet、Oxford-IIIT-Fflower などのダウンストリーム タスクのパフォーマンス。\n\nこれらのリファレンスには、さまざまなエラー間のトレードオフや、ダウンストリーム タスクに使用される正確なメトリクスに関する具体的な情報は提供されていません。特定のエラー タイプとそのトレードオフ、および前述の下流タスクの評価指標に関する [さらなる情報が必要]。"
  },
  {
    "question": "results",
    "chain": "generation",
    "answer": "Based on the provided references, here is the evaluation information I could find for the model microsoft/cvt-21-384-22k:\n\nPerformance:\n- Achieves 87.7% top-1 accuracy on ImageNet-1k validation set when pre-trained on ImageNet-22k and fine-tuned with 384x384 input size\n- Outperforms other Transformer-based models like DeiT-B with higher accuracy while using 63% fewer parameters and 60% fewer FLOPs\n- Obtains the best performance across various downstream tasks compared to models like BiT-R152x4, even with 3x fewer parameters\n\nEfficiency: \n- Uses fewer parameters and FLOPs compared to other Transformer-based models while achieving higher accuracy\n\nModel Size:\n- CvT-21 has 31.54M parameters\n\nTraining:\n- Trained with AdamW optimizer, weight decay 0.1, initial learning rate 0.02, total batch size 2048 for 300 epochs\n- Uses cosine learning rate decay scheduler\n- Employs same data augmentation and regularization as ViT\n- Trained on 224x224 input size images by default\n\n[More Information Needed] for other factors like Intended Use, Limitations, and Potential Risks.",
    "answer_Chinese": "根据提供的参考资料，以下是我可以找到的模型 microsoft/cvt-21-384-22k 的评估信息：\n\n表现：\n- 在 ImageNet-22k 上进行预训练并使用 384x384 输入大小进行微调时，在 ImageNet-1k 验证集上实现 87.7% 的 top-1 准确率\n- 优于 DeiT-B 等其他基于 Transformer 的模型，精度更高，同时使用的参数减少了 63%，失败次数减少了 60%\n- 与 BiT-R152x4 等模型相比，即使参数减少了 3 倍，也能在各种下游任务中获得最佳性能\n\n效率： \n- 与其他基于 Transformer 的模型相比，使用更少的参数和 FLOPs，同时实现更高的精度\n\n型号尺寸：\n- CvT-21有31.54M参数\n\n训练：\n- 使用 AdamW 优化器进行训练，权重衰减 0.1，初始学习率 0.02，总批量大小 2048，持续 300 个 epoch\n- 使用余弦学习率衰减调度器\n- 采用与 ViT 相同的数据增强和正则化\n- 默认情况下在 224x224 输入尺寸图像上进行训练\n\n[需要更多信息]了解其他因素，例如预期用途、限制和潜在风险。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك معلومات التقييم التي يمكنني العثور عليها للطراز microsoft/cvt-21-384-22k:\n\nأداء:\n- يحقق دقة من أعلى 1 بنسبة 87.7% في مجموعة التحقق من صحة ImageNet-1k عند تدريبه مسبقًا على ImageNet-22k وضبطه بدقة بحجم إدخال 384 × 384\n- يتفوق على النماذج الأخرى المعتمدة على المحولات مثل DeiT-B بدقة أعلى مع استخدام معلمات أقل بنسبة 63% وFLOPs أقل بنسبة 60%\n- يحصل على أفضل أداء عبر مختلف المهام النهائية مقارنة بنماذج مثل BiT-R152x4، حتى مع معلمات أقل بمقدار 3 مرات\n\nكفاءة: \n- يستخدم عددًا أقل من المعلمات وFLOPs مقارنة بالنماذج الأخرى المستندة إلى المحولات مع تحقيق دقة أعلى\n\nحجم النموذج:\n- يحتوي CvT-21 على 31.54 مليون معلمة\n\nتمرين:\n- تم التدريب باستخدام مُحسِّن AdamW، تناقص الوزن 0.1، معدل التعلم الأولي 0.02، إجمالي حجم الدفعة 2048 لمدة 300 حقبة\n- يستخدم جدولة تسوس معدل التعلم جيب التمام\n- يستخدم نفس زيادة البيانات وتنظيمها مثل ViT\n- تم التدريب على الصور بحجم إدخال 224 × 224 بشكل افتراضي\n\n[يلزم مزيد من المعلومات] لعوامل أخرى مثل الاستخدام المقصود والقيود والمخاطر المحتملة.",
    "answer_French": "Sur la base des références fournies, voici les informations d'évaluation que j'ai pu trouver pour le modèle microsoft/cvt-21-384-22k :\n\nPerformance:\n- Atteint une précision top 1 de 87,7 % sur l'ensemble de validation ImageNet-1k lorsqu'il est pré-entraîné sur ImageNet-22k et affiné avec une taille d'entrée de 384 x 384.\n- Surpasse les autres modèles basés sur Transformer comme DeiT-B avec une plus grande précision tout en utilisant 63 % de paramètres en moins et 60 % de FLOP en moins\n- Obtient les meilleures performances sur diverses tâches en aval par rapport aux modèles comme le BiT-R152x4, même avec 3 fois moins de paramètres\n\nEfficacité: \n- Utilise moins de paramètres et de FLOP par rapport aux autres modèles basés sur Transformer tout en obtenant une plus grande précision\n\nTaille du modèle :\n- CvT-21 a 31,54 millions de paramètres\n\nEntraînement:\n- Formé avec l'optimiseur AdamW, perte de poids 0,1, taux d'apprentissage initial 0,02, taille totale du lot 2048 pour 300 époques\n- Utilise le planificateur de décroissance du taux d'apprentissage du cosinus\n- Utilise la même augmentation et régularisation des données que ViT\n- Formé sur des images de taille d'entrée 224x224 par défaut\n\n[Plus d'informations nécessaires] pour d'autres facteurs tels que l'utilisation prévue, les limitations et les risques potentiels.",
    "answer_Russian": "Основываясь на предоставленных ссылках, я смог найти оценочную информацию для модели microsoft/cvt-21-384-22k:\n\nПроизводительность:\n- Достигает 87,7% точности высшего уровня 1 на проверочном наборе ImageNet-1k при предварительном обучении на ImageNet-22k и точной настройке с входным размером 384x384.\n- Превосходит другие модели на основе трансформатора, такие как DeiT-B, с более высокой точностью, используя при этом на 63 % меньше параметров и на 60 % меньше операций FLOP.\n- Обеспечивает лучшую производительность при выполнении различных последующих задач по сравнению с такими моделями, как BiT-R152x4, даже при в 3 раза меньшем количестве параметров.\n\nЭффективность: \n- Использует меньше параметров и FLOP по сравнению с другими моделями на основе трансформатора, обеспечивая при этом более высокую точность.\n\nРазмер модели:\n- CvT-21 имеет 31,54М параметров\n\nОбучение:\n- Обучение с помощью оптимизатора AdamW, затухание веса 0,1, начальная скорость обучения 0,02, общий размер пакета 2048 для 300 эпох.\n- Использует планировщик затухания скорости обучения косинуса.\n- Использует то же увеличение и регуляризацию данных, что и ViT.\n- По умолчанию обучение ведется на изображениях входного размера 224x224.\n\n[Требуется дополнительная информация] для других факторов, таких как предполагаемое использование, ограничения и потенциальные риски.",
    "answer_Japanese": "提供された参考資料に基づいて、モデル Microsoft/cvt-21-384-22k について見つけた評価情報は次のとおりです。\n\nパフォーマンス：\n- ImageNet-22k で事前トレーニングされ、384x384 入力サイズで微調整された場合、ImageNet-1k 検証セットで 87.7% のトップ 1 精度を達成\n- 63% 少ないパラメータと 60% 少ない FLOP を使用しながら、DeiT-B などの他の Transformer ベースのモデルよりも高い精度で優れたパフォーマンスを発揮します\n- BiT-R152x4 などのモデルと比較して、パラメータが 3 分の 1 であっても、さまざまなダウンストリーム タスクにわたって最高のパフォーマンスを実現\n\n効率： \n- 他の Transformer ベースのモデルと比較して、使用するパラメーターと FLOP が少なく、より高い精度を実現します\n\nモデルサイズ:\n- CvT-21には3154万のパラメータがあります\n\nトレーニング：\n- AdamW オプティマイザーでトレーニング、重み減衰 0.1、初期学習率 0.02、300 エポックの合計バッチ サイズ 2048\n- コサイン学習率減衰スケジューラを使用\n- ViT と同じデータ拡張と正則化を採用\n- デフォルトで 224x224 の入力サイズの画像でトレーニングされます\n\n使用目的、制限事項、潜在的なリスクなどのその他の要素については、[さらなる情報が必要]。"
  },
  {
    "question": "results_summary",
    "chain": "generation",
    "answer": "Based on the provided references, here is a summary of the evaluation results for the model microsoft/cvt-21-384-22k:\n\n- CvT-21 obtains an 82.5% ImageNet Top-1 accuracy, which is 0.5% higher than DeiT-B with a 63% reduction in parameters and 60% fewer FLOPs.\n\n- When pre-trained on ImageNet-22k and fine-tuned on downstream tasks, the CvT-W24 model achieves the best performance across all considered tasks, even outperforming the larger BiT-R152x4 model which has more than 3 times the number of parameters. (References 6 and 8)\n\n- Pre-trained on ImageNet-22k, CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k validation set.\n\n- [More Information Needed] on the specific evaluation results of the CvT-21-384-22k model, as the provided references focus more on the CvT-13, CvT-21, and CvT-W24 models.\n\nThe references also highlight that the proposed Convolutional Token Embedding and Convolutional Projection components contribute to the effectiveness of the CvT architecture, and that positional encoding can be safely removed in this model.",
    "answer_Chinese": "根据提供的参考资料，以下是模型 microsoft/cvt-21-384-22k 的评估结果摘要：\n\n- CvT-21 获得了 82.5% 的 ImageNet Top-1 准确率，比 DeiT-B 高 0.5%，参数减少了 63%，FLOP 减少了 60%。\n\n- 在 ImageNet-22k 上进行预训练并在下游任务上进行微调时，CvT-W24 模型在所有考虑的任务中实现了最佳性能，甚至优于参数数量超过 3 倍的较大 BiT-R152x4 模型。 （参考文献6和8）\n\n- 在 ImageNet-22k 上进行预训练，CvT-W24 在 ImageNet-1k 验证集上获得了 87.7% 的 top-1 准确率。\n\n- [需要更多信息]关于CvT-21-384-22k模型的具体评估结果，因为提供的参考更多地关注CvT-13、CvT-21和CvT-W24模型。\n\n这些参考文献还强调，所提出的卷积令牌嵌入和卷积投影组件有助于 CvT 架构的有效性，并且可以在此模型中安全地删除位置编码。",
    "answer_Arabic": "وبناء على المراجع المتوفرة إليكم ملخص نتائج التقييم للنموذج microsoft/cvt-21-384-22k:\n\n- حصل CvT-21 على دقة ImageNet Top-1 بنسبة 82.5%، وهي أعلى بنسبة 0.5% من DeiT-B مع انخفاض بنسبة 63% في المعلمات و60% أقل من عمليات FLOP.\n\n- عند تدريبه مسبقًا على ImageNet-22k وضبطه بدقة على المهام النهائية، يحقق نموذج CvT-W24 أفضل أداء عبر جميع المهام التي تم النظر فيها، حتى أنه يتفوق على نموذج BiT-R152x4 الأكبر الذي يحتوي على أكثر من 3 أضعاف عدد المعلمات. (المرجعان 6 و 8)\n\n- حصل CvT-W24، الذي تم تدريبه مسبقًا على ImageNet-22k، على دقة من الدرجة الأولى بنسبة 87.7% في مجموعة التحقق من صحة ImageNet-1k.\n\n- [بحاجة إلى مزيد من المعلومات] حول نتائج التقييم المحددة لنموذج CvT-21-384-22k، حيث تركز المراجع المقدمة بشكل أكبر على نماذج CvT-13، وCvT-21، وCvT-W24.\n\nتسلط المراجع أيضًا الضوء على أن مكونات تضمين الرمز التلافيفي والإسقاط التلافيفي المقترحة تساهم في فعالية بنية CvT، ويمكن إزالة التشفير الموضعي بأمان في هذا النموذج.",
    "answer_French": "Sur la base des références fournies, voici un résumé des résultats de l'évaluation pour le modèle Microsoft/cvt-21-384-22k :\n\n- CvT-21 obtient une précision ImageNet Top-1 de 82,5 %, soit 0,5 % de plus que DeiT-B avec une réduction de 63 % des paramètres et 60 % de FLOP en moins.\n\n- Lorsqu'il est pré-entraîné sur ImageNet-22k et affiné sur les tâches en aval, le modèle CvT-W24 atteint les meilleures performances sur toutes les tâches considérées, surpassant même le plus grand modèle BiT-R152x4 qui possède plus de 3 fois le nombre de paramètres. (Références 6 et 8)\n\n- Pré-entraîné sur ImageNet-22k, CvT-W24 obtient une précision top-1 de 87,7 % sur l'ensemble de validation ImageNet-1k.\n\n- [Plus d'informations nécessaires] sur les résultats d'évaluation spécifiques du modèle CvT-21-384-22k, car les références fournies se concentrent davantage sur les modèles CvT-13, CvT-21 et CvT-W24.\n\nLes références soulignent également que les composants proposés pour l'intégration de jetons convolutifs et la projection convolutive contribuent à l'efficacité de l'architecture CvT et que le codage positionnel peut être supprimé en toute sécurité dans ce modèle.",
    "answer_Russian": "На основании предоставленных ссылок приводим сводку результатов оценки модели microsoft/cvt-21-384-22k:\n\n- CvT-21 имеет точность ImageNet Top-1 82,5%, что на 0,5% выше, чем у DeiT-B, при уменьшении параметров на 63% и меньшем количестве FLOP на 60%.\n\n- При предварительном обучении на ImageNet-22k и точной настройке для последующих задач модель CvT-W24 достигает наилучшей производительности во всех рассмотренных задачах, даже превосходя более крупную модель BiT-R152x4, которая имеет более чем в 3 раза больше параметров. (Ссылки 6 и 8)\n\n- Предварительно обученный на ImageNet-22k, CvT-W24 достигает первой точности 87,7% на проверочном наборе ImageNet-1k.\n\n- [Требуется дополнительная информация] о конкретных результатах оценки модели CvT-21-384-22k, поскольку предоставленные ссылки больше ориентированы на модели CvT-13, CvT-21 и CvT-W24.\n\nВ ссылках также подчеркивается, что предлагаемые компоненты Convolutional Token Embedding и Convolutional Projection способствуют эффективности архитектуры CvT и что позиционное кодирование можно безопасно удалить в этой модели.",
    "answer_Japanese": "提供された参考資料に基づいて、モデル Microsoft/cvt-21-384-22k の評価結果の概要を次に示します。\n\n- CvT-21 は、82.5% の ImageNet Top-1 精度を獲得します。これは、DeiT-B より 0.5% 高く、パラメーターが 63% 削減され、FLOP が 60% 減少します。\n\n- ImageNet-22k で事前トレーニングされ、ダウンストリーム タスクで微調整された場合、CvT-W24 モデルは、考慮されたすべてのタスクにわたって最高のパフォーマンスを達成し、パラメーター数が 3 倍を超える大型の BiT-R152x4 モデルをも上回ります。 (参考6、8)\n\n- ImageNet-22k で事前トレーニングされた CvT-W24 は、ImageNet-1k 検証セットで 87.7% というトップ 1 の精度を獲得します。\n\n- 提供されている参考資料は CvT-13、CvT-21、および CvT-W24 モデルに重点を置いているため、CvT-21-384-22k モデルの特定の評価結果に関する [さらなる情報が必要]。\n\nこの参考文献では、提案されている畳み込みトークン埋め込みコンポーネントと畳み込み射影コンポーネントが CvT アーキテクチャの有効性に貢献していること、およびこのモデルでは位置エンコーディングを安全に削除できることも強調しています。"
  },
  {
    "question": "model_examination",
    "chain": "generation",
    "answer": "Here is my attempt at answering the question about explainability/interpretability for the microsoft/cvt-21-384-22k model based on the provided references:\n\nThe CvT (Convolutional vision Transformer) architecture introduced in this model enables better interpretability compared to standard Vision Transformers in a few key ways:\n\n1. By replacing position-wise linear projections with convolutional projections and using a Convolutional Token Embedding, the model can capture local spatial relationships without relying on positional embeddings. This simplifies the architecture and makes the learned features more spatially grounded and interpretable.\n\n2. Incorporating a pyramid structure with convolutions allows the model to learn multi-scale feature maps, similar to CNNs. Fusing local neighborhood information through the convolutions is important for performance and interpretability of the learned representations.  \n\n3. [More Information Needed] on specific explainability/interpretability analysis or visualizations of the learned features and attentions in the CvT-21 model. The references do not appear to include examples of this.\n\n4. [More Information Needed] on how the design choices and performance of CvT-21 on downstream tasks may relate to its explainability/interpretability properties compared to other vision transformer models like ViT. More direct comparisons would help characterize the benefits of the CvT architecture.\n\nIn summary, the convolutional design elements in CvT-21 take inspiration from CNNs to make the learned visual features more locally grounded and multi-scale compared to ViT. However, more specific interpretability analysis of the model is needed to make strong claims. The provided references focus more on the architecture design and performance rather than explainability.",
    "answer_Chinese": "这是我根据提供的参考文献尝试回答有关 microsoft/cvt-21-384-22k 模型的可解释性/可解释性的问题：\n\n与标准视觉变压器相比，该模型中引入的 CvT（卷积视觉变压器）架构在几个关键方面实现了更好的可解释性：\n\n1. 通过用卷积投影替换位置线性投影并使用卷积令牌嵌入，该模型可以在不依赖位置嵌入的情况下捕获局部空间关系。这简化了架构，并使学习到的特征更加具有空间基础和可解释性。\n\n2. 将金字塔结构与卷积相结合，使模型能够学习多尺度特征图，类似于 CNN。通过卷积融合局部邻域信息对于学习表示的性能和可解释性非常重要。  \n\n3. [需要更多信息] 关于 CvT-21 模型中学习特征和注意力的具体可解释性/可解释性分析或可视化。参考文献似乎没有包含这方面的示例。\n\n4. [需要更多信息] 与 ViT 等其他视觉转换器模型相比，CvT-21 在下游任务上的设计选择和性能可能与其可解释性/可解释性属性相关。更直接的比较将有助于描述 CvT 架构的优势。\n\n总之，与 ViT 相比，CvT-21 中的卷积设计元素从 CNN 中汲取灵感，使学习到的视觉特征更加局部化和多尺度。然而，需要对该模型进行更具体的可解释性分析才能做出强有力的主张。提供的参考资料更多地关注架构设计和性能，而不是可解释性。",
    "answer_Arabic": "إليك محاولتي للإجابة على سؤال حول إمكانية الشرح/قابلية التفسير لنموذج microsoft/cvt-21-384-22k استنادًا إلى المراجع المتوفرة:\n\nتتيح بنية CvT (محول الرؤية التلافيفية) المقدمة في هذا النموذج إمكانية تفسير أفضل مقارنة بمحولات الرؤية القياسية بعدة طرق رئيسية:\n\n1. من خلال استبدال الإسقاطات الخطية المتعلقة بالموضع بإسقاطات تلافيفية واستخدام تضمين الرمز التلافيفي، يمكن للنموذج التقاط العلاقات المكانية المحلية دون الاعتماد على التضمينات الموضعية. يؤدي هذا إلى تبسيط البنية ويجعل الميزات المستفادة أكثر ترسيخًا من الناحية المكانية وقابلة للتفسير.\n\n2. إن دمج بنية هرمية مع التلافيفات يسمح للنموذج بتعلم خرائط المعالم متعددة المقاييس، على غرار شبكات CNN. يعد دمج معلومات الحي المحلي من خلال التلافيف أمرًا مهمًا لأداء وتفسير التمثيلات المستفادة.  \n\n3. [بحاجة إلى مزيد من المعلومات] حول التحليل المحدد لقابلية الشرح/قابلية التفسير أو تصورات الميزات المستفادة والاهتمامات في نموذج CvT-21. ولا يبدو أن المراجع تتضمن أمثلة على ذلك.\n\n4. [مطلوب مزيد من المعلومات] حول كيفية ارتباط اختيارات التصميم وأداء CvT-21 في المهام النهائية بخصائص قابلية الشرح/التفسير مقارنة بنماذج محولات الرؤية الأخرى مثل ViT. من شأن المزيد من المقارنات المباشرة أن تساعد في تحديد فوائد بنية CvT.\n\nباختصار، تستلهم عناصر التصميم التلافيفي في CvT-21 من شبكات CNN لجعل الميزات المرئية المستفادة أكثر ترسيخًا محليًا ومتعددة النطاق مقارنةً بـ ViT. ومع ذلك، هناك حاجة إلى تحليل تفسيري أكثر تحديدًا للنموذج لتقديم ادعاءات قوية. تركز المراجع المقدمة بشكل أكبر على التصميم والأداء المعماري بدلاً من الشرح.",
    "answer_French": "Voici ma tentative de répondre à la question sur l'explicabilité/interprétabilité du modèle Microsoft/cvt-21-384-22k sur la base des références fournies :\n\nL'architecture CvT (Convolutional vision Transformer) introduite dans ce modèle permet une meilleure interprétabilité par rapport aux Vision Transformers standard de plusieurs manières clés :\n\n1. En remplaçant les projections linéaires en fonction de la position par des projections convolutives et en utilisant une intégration de jetons convolutive, le modèle peut capturer les relations spatiales locales sans s'appuyer sur des intégrations positionnelles. Cela simplifie l'architecture et rend les fonctionnalités apprises plus spatialement ancrées et interprétables.\n\n2. L'intégration d'une structure pyramidale avec des convolutions permet au modèle d'apprendre des cartes de caractéristiques multi-échelles, similaires aux CNN. La fusion des informations du quartier local à travers les circonvolutions est importante pour la performance et l'interprétabilité des représentations apprises.  \n\n3. [Plus d'informations nécessaires] sur l'analyse d'explicabilité/interprétabilité spécifique ou les visualisations des fonctionnalités et attentions apprises dans le modèle CvT-21. Les références ne semblent pas inclure d'exemples de cela.\n\n4. [Plus d'informations nécessaires] sur la façon dont les choix de conception et les performances du CvT-21 sur les tâches en aval peuvent être liés à ses propriétés d'explicabilité/interprétabilité par rapport à d'autres modèles de transformateur de vision comme ViT. Des comparaisons plus directes aideraient à caractériser les avantages de l’architecture CvT.\n\nEn résumé, les éléments de conception convolutionnelle de CvT-21 s'inspirent des CNN pour rendre les caractéristiques visuelles apprises plus localement et à plusieurs échelles par rapport à ViT. Cependant, une analyse plus spécifique de l’interprétabilité du modèle est nécessaire pour formuler des affirmations solides. Les références fournies se concentrent davantage sur la conception et les performances de l'architecture plutôt que sur l'explicabilité.",
    "answer_Russian": "Вот моя попытка ответить на вопрос об объяснимости/интерпретируемости модели microsoft/cvt-21-384-22k на основе предоставленных ссылок:\n\nАрхитектура CvT (сверточный преобразователь видения), представленная в этой модели, обеспечивает лучшую интерпретируемость по сравнению со стандартными преобразователями видения несколькими ключевыми способами:\n\n1. Заменяя позиционные линейные проекции сверточными проекциями и используя встраивание сверточных токенов, модель может фиксировать локальные пространственные отношения, не полагаясь на позиционные встраивания. Это упрощает архитектуру и делает изученные функции более пространственно обоснованными и интерпретируемыми.\n\n2. Включение пирамидальной структуры со свертками позволяет модели изучать многомасштабные карты объектов, аналогичные CNN. Объединение информации о локальных окрестностях посредством сверток важно для производительности и интерпретируемости изученных представлений.  \n\n3. [Необходима дополнительная информация] по конкретному анализу объяснимости/интерпретируемости или визуализации изученных особенностей и внимания в модели CvT-21. Ссылки, по-видимому, не содержат примеров этого.\n\n4. [Необходима дополнительная информация] о том, как выбор конструкции и производительность CvT-21 при выполнении последующих задач могут быть связаны с ее свойствами объяснимости/интерпретируемости по сравнению с другими моделями преобразователей зрения, такими как ViT. Более прямые сравнения помогут охарактеризовать преимущества архитектуры CvT.\n\nТаким образом, элементы сверточного дизайна в CvT-21 черпают вдохновение из CNN, чтобы сделать изученные визуальные функции более локальными и многомасштабными по сравнению с ViT. Однако для того, чтобы сделать серьезные заявления, необходим более конкретный анализ интерпретируемости модели. Предоставленные ссылки больше ориентированы на дизайн архитектуры и производительность, а не на объяснимость.",
    "answer_Japanese": "以下は、提供された参考資料に基づいて、microsoft/cvt-21-384-22k モデルの説明可能性/解釈可能性に関する質問に答える私の試みです。\n\nこのモデルに導入された CvT (畳み込みビジョン トランスフォーマー) アーキテクチャにより、いくつかの重要な点で標準のビジョン トランスフォーマーと比較して解釈性が向上します。\n\n1. 位置に関する線形投影を畳み込み投影に置き換え、畳み込みトークン埋め込みを使用することにより、モデルは位置埋め込みに依存せずに局所的な空間関係をキャプチャできます。これにより、アーキテクチャが簡素化され、学習された特徴がより空間的に根拠があり、解釈しやすくなります。\n\n2. 畳み込みを伴うピラミッド構造を組み込むことで、CNN と同様に、モデルがマルチスケールの特徴マップを学習できるようになります。畳み込みを通じてローカル近傍情報を融合することは、学習された表現のパフォーマンスと解釈可能性にとって重要です。  \n\n3. CvT-21 モデルで学習された特徴と注意事項の特定の説明可能性/解釈可能性分析または視覚化に関する [さらなる情報が必要]。参考文献にはこの例は含まれていないようです。\n\n4. 下流タスクにおける CvT-21 の設計選択とパフォーマンスが、ViT などの他のビジョン トランスフォーマー モデルと比較してその説明可能性/解釈可能性の特性にどのように関連するかについての [さらなる情報が必要]。より直接的な比較は、CvT アーキテクチャの利点を特徴付けるのに役立ちます。\n\n要約すると、CvT-21 の畳み込み設計要素は CNN からインスピレーションを得て、学習された視覚特徴を ViT と比較してより局所的に根拠のあるマルチスケールにします。ただし、強力な主張をするには、モデルのより具体的な解釈可能性分析が必要です。提供されている参考資料は、説明可能性よりもアーキテクチャ設計とパフォーマンスに重点を置いています。"
  },
  {
    "question": "hardware",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain information about the specific hardware used to train the microsoft/cvt-21-384-22k model. More details would be needed from the model developers or training logs to determine the hardware type utilized during training.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料不包含有关用于训练 microsoft/cvt-21-384-22k 模型的特定硬件的信息。需要模型开发人员或培训日志提供更多详细信息，以确定培训期间使用的硬件类型。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على معلومات حول الأجهزة المحددة المستخدمة لتدريب طراز microsoft/cvt-21-384-22k. ستكون هناك حاجة إلى مزيد من التفاصيل من مطوري النماذج أو سجلات التدريب لتحديد نوع الأجهزة المستخدمة أثناء التدريب.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent pas d'informations sur le matériel spécifique utilisé pour former le modèle Microsoft/cvt-21-384-22k. Plus de détails seraient nécessaires auprès des développeurs de modèles ou des journaux de formation pour déterminer le type de matériel utilisé pendant la formation.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат информации о конкретном оборудовании, используемом для обучения модели microsoft/cvt-21-384-22k. Для определения типа оборудования, используемого во время обучения, потребуется дополнительная информация от разработчиков модели или журналов обучения.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、microsoft/cvt-21-384-22k モデルのトレーニングに使用される特定のハードウェアに関する情報は含まれていません。トレーニング中に使用されるハードウェアの種類を決定するには、モデル開発者またはトレーニング ログから詳細が必要になります。"
  },
  {
    "question": "software",
    "chain": "generation",
    "answer": "Based on the provided references, the model microsoft/cvt-21-384-22k is likely trained using PyTorch, as evidenced by this excerpt:\n\n\"The code is developed and tested using pytorch 1.7.1. Other versions of pytorch are not fully tested.\"\n\nHowever, more specific information about the exact software and versions used to train this particular model would be needed to provide a definitive answer. The references mention some software dependencies and a requirements.txt file, but do not confirm the exact software stack used for microsoft/cvt-21-384-22k.\n\n[More Information Needed]",
    "answer_Chinese": "根据提供的参考资料，模型 microsoft/cvt-21-384-22k 可能是使用 PyTorch 进行训练的，如以下摘录所示：\n\n“该代码是使用 pytorch 1.7.1 开发和测试的。其他版本的 pytorch 尚未经过充分测试。”\n\n然而，需要有关用于训练该特定模型的确切软件和版本的更具体信息才能提供明确的答案。这些参考文献提到了一些软件依赖项和requirements.txt 文件，但没有确认用于microsoft/cvt-21-384-22k 的确切软件堆栈。\n\n[需要更多信息]",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، من المحتمل أن يتم تدريب النموذج microsoft/cvt-21-384-22k باستخدام PyTorch، كما يتضح من هذا المقتطف:\n\n\"تم تطوير الكود واختباره باستخدام pytorch 1.7.1. ولم يتم اختبار الإصدارات الأخرى من pytorch بشكل كامل.\"\n\nومع ذلك، ستكون هناك حاجة إلى معلومات أكثر تحديدًا حول البرامج والإصدارات الدقيقة المستخدمة لتدريب هذا النموذج المحدد لتقديم إجابة محددة. تشير المراجع إلى بعض تبعيات البرامج وملف require.txt، ولكنها لا تؤكد مجموعة البرامج الدقيقة المستخدمة لـ microsoft/cvt-21-384-22k.\n\n[مطلوب مزيد من المعلومات]",
    "answer_French": "Sur la base des références fournies, le modèle microsoft/cvt-21-384-22k est probablement formé à l'aide de PyTorch, comme en témoigne cet extrait :\n\n\"Le code est développé et testé à l'aide de pytorch 1.7.1. Les autres versions de pytorch ne sont pas entièrement testées.\"\n\nCependant, des informations plus spécifiques sur le logiciel exact et les versions utilisées pour entraîner ce modèle particulier seraient nécessaires pour fournir une réponse définitive. Les références mentionnent certaines dépendances logicielles et un fichier Requirements.txt, mais ne confirment pas la pile logicielle exacte utilisée pour Microsoft/cvt-21-384-22k.\n\n[Plus d'informations nécessaires]",
    "answer_Russian": "Судя по предоставленным ссылкам, модель microsoft/cvt-21-384-22k, скорее всего, обучена с использованием PyTorch, о чем свидетельствует этот отрывок:\n\n«Код разработан и протестирован с использованием pytorch 1.7.1. Другие версии pytorch не протестированы полностью».\n\nОднако для получения окончательного ответа потребуется более конкретная информация о конкретном программном обеспечении и версиях, используемых для обучения этой конкретной модели. В ссылках упоминаются некоторые программные зависимости и файл require.txt, но не подтверждается точный программный стек, используемый для microsoft/cvt-21-384-22k.\n\n[Требуется дополнительная информация]",
    "answer_Japanese": "提供された参照に基づくと、次の抜粋で明らかなように、モデル Microsoft/cvt-21-384-22k は PyTorch を使用してトレーニングされた可能性があります。\n\n「コードは pytorch 1.7.1 を使用して開発およびテストされています。pytorch の他のバージョンは完全にはテストされていません。」\n\nただし、最終的な答えを得るには、この特定のモデルのトレーニングに使用された正確なソフトウェアとバージョンに関するより具体的な情報が必要です。参考資料には、いくつかのソフトウェアの依存関係とrequirements.txtファイルについて言及されていますが、microsoft/cvt-21-384-22kに使用される正確なソフトウェアスタックは確認されていません。\n\n[さらに詳しい情報が必要です]"
  },
  {
    "question": "hours_used",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain information about the training time for the microsoft/cvt-21-384-22k model. More details would be needed from the model developers or documentation to determine how long this specific model took to train.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料不包含有关 microsoft/cvt-21-384-22k 模型的训练时间的信息。模型开发人员或文档需要提供更多详细信息，以确定该特定模型的训练时间。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على معلومات حول وقت التدريب لنموذج microsoft/cvt-21-384-22k. ستكون هناك حاجة إلى مزيد من التفاصيل من مطوري النماذج أو الوثائق لتحديد المدة التي يستغرقها تدريب هذا النموذج المحدد.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent pas d'informations sur le temps de formation pour le modèle Microsoft/cvt-21-384-22k. Plus de détails seraient nécessaires de la part des développeurs du modèle ou de la documentation pour déterminer combien de temps il a fallu à ce modèle spécifique pour s'entraîner.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат информации о времени обучения модели microsoft/cvt-21-384-22k. Чтобы определить, сколько времени потребовалось для обучения этой конкретной модели, потребуется дополнительная информация от разработчиков модели или документация.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、microsoft/cvt-21-384-22k モデルのトレーニング時間に関する情報は含まれていません。この特定のモデルのトレーニングにどれくらいの時間がかかるかを判断するには、モデル開発者またはドキュメントから詳細を得る必要があります。"
  },
  {
    "question": "cloud_provider",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not mention the specific cloud provider used for training the microsoft/cvt-21-384-22k model. More information would be needed from the model developers or training logs to determine the cloud provider used.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考资料未提及用于训练 microsoft/cvt-21-384-22k 模型的特定云提供商。需要来自模型开发人员或培训日志的更多信息来确定所使用的云提供商。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تذكر المراجع المقدمة موفر السحابة المحدد المستخدم لتدريب نموذج microsoft/cvt-21-384-22k. ستكون هناك حاجة إلى مزيد من المعلومات من مطوري النماذج أو سجلات التدريب لتحديد موفر السحابة المستخدم.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne mentionnent pas le fournisseur de cloud spécifique utilisé pour former le modèle Microsoft/cvt-21-384-22k. Plus d'informations seraient nécessaires auprès des développeurs de modèles ou des journaux de formation pour déterminer le fournisseur de cloud utilisé.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nВ предоставленных ссылках не упоминается конкретный поставщик облачных услуг, использованный для обучения модели microsoft/cvt-21-384-22k. Для определения используемого поставщика облачных услуг потребуется дополнительная информация от разработчиков модели или журналы обучения.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、microsoft/cvt-21-384-22k モデルのトレーニングに使用される特定のクラウド プロバイダーについては言及されていません。使用されるクラウド プロバイダーを決定するには、モデル開発者またはトレーニング ログから詳細な情報が必要になります。"
  },
  {
    "question": "co2_emitted",
    "chain": "generation",
    "answer": "[More Information Needed]\n\nThe provided references do not contain specific information about the amount of carbon emitted during the training of the microsoft/cvt-21-384-22k model. To accurately report the carbon emissions, additional details would be needed such as the hardware used for training, total training time, energy consumption, and carbon intensity of the electricity grid.",
    "answer_Chinese": "[需要更多信息]\n\n提供的参考文献不包含有关 microsoft/cvt-21-384-22k 模型训练期间碳排放量的具体信息。为了准确报告碳排放量，需要额外的详细信息，例如用于培训的硬件、总培训时间、能源消耗和电网的碳强度。",
    "answer_Arabic": "[مطلوب مزيد من المعلومات]\n\nلا تحتوي المراجع المقدمة على معلومات محددة حول كمية الكربون المنبعثة أثناء التدريب على طراز microsoft/cvt-21-384-22k. للإبلاغ بدقة عن انبعاثات الكربون، ستكون هناك حاجة إلى تفاصيل إضافية مثل الأجهزة المستخدمة للتدريب، وإجمالي وقت التدريب، واستهلاك الطاقة، وكثافة الكربون في شبكة الكهرباء.",
    "answer_French": "[Plus d'informations nécessaires]\n\nLes références fournies ne contiennent pas d'informations spécifiques sur la quantité de carbone émise lors de la formation du modèle Microsoft/cvt-21-384-22k. Pour déclarer avec précision les émissions de carbone, des détails supplémentaires seraient nécessaires, tels que le matériel utilisé pour la formation, la durée totale de la formation, la consommation d'énergie et l'intensité carbone du réseau électrique.",
    "answer_Russian": "[Требуется дополнительная информация]\n\nПредоставленные ссылки не содержат конкретной информации о количестве углерода, выделяемого при обучении модели microsoft/cvt-21-384-22k. Чтобы точно сообщить о выбросах углекислого газа, потребуются дополнительные сведения, такие как оборудование, используемое для обучения, общее время обучения, энергопотребление и углеродоемкость электросети.",
    "answer_Japanese": "[さらに詳しい情報が必要です]\n\n提供されている参考資料には、microsoft/cvt-21-384-22k モデルのトレーニング中に排出される炭素の量に関する具体的な情報は含まれていません。炭素排出量を正確に報告するには、トレーニングに使用したハードウェア、総トレーニング時間、エネルギー消費量、電力網の炭素強度などの追加の詳細が必要になります。"
  },
  {
    "question": "model_specs",
    "chain": "generation",
    "answer": "Model Architecture:\nThe Convolutional vision Transformer (CvT) architecture introduces two convolution-based operations into the Vision Transformer architecture:\n1. Convolutional Token Embedding: The input image (or 2D reshaped token maps) are subjected to the Convolutional Token Embedding layer, which is implemented as a convolution with overlapping patches with tokens reshaped to the 2D spatial grid. This allows each stage to progressively reduce the number of tokens while increasing the width of the tokens, achieving spatial downsampling and increased richness of representation.\n2. Convolutional Projection: In the Convolutional Transformer Block, a depth-wise separable convolution operation, referred to as Convolutional Projection, is applied for query, key, and value embeddings. This replaces the original position-wise linear projection for Multi-Head Self-Attention (MHSA).\n\nThe CvT architecture employs a multi-stage hierarchy design borrowed from CNNs, with three stages in total. Each stage consists of the Convolutional Token Embedding followed by a stack of Convolutional Transformer Blocks.\n\nThe specific configuration for the microsoft/cvt-21-384-22k model is CvT-21, which has 31.54M parameters.\n\nModel Objective:\nThe objective of the CvT model is to achieve state-of-the-art performance on image classification tasks, such as ImageNet-1k, while having fewer parameters and lower FLOPs compared to other Vision Transformers and ResNets. The model aims to introduce the desirable properties of CNNs to the Vision Transformer architecture (i.e., shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e., dynamic attention, global context, and better generalization).\n\nThe CvT-W24 model, when pretrained on the larger ImageNet-22k dataset and fine-tuned on ImageNet-1k, obtains a top-1 accuracy of 87.7% on the ImageNet-1k validation set.\n\n[More Information Needed] on the specific training details and hyperparameters used for the microsoft/cvt-21-384-22k model.",
    "answer_Chinese": "模型架构：\n卷积视觉 Transformer (CvT) 架构在 Vision Transformer 架构中引入了两种基于卷积的运算：\n1. 卷积令牌嵌入：输入图像（或二维重塑令牌图）受到卷积令牌嵌入层的影响，该层被实现为具有重叠补丁的卷积，其中令牌被重塑为二维空间网格。这允许每个阶段逐步减少标记的数量，同时增加标记的宽度，实现空间下采样并增加表示的丰富性。\n2. 卷积投影：在卷积变换器块中，深度可分离的卷积运算（称为卷积投影）应用于查询、键和值嵌入。这取代了多头自注意力（MHSA）的原始位置线性投影。\n\nCvT架构采用了借鉴CNN的多级层次结构设计，总共三个阶段。每个阶段都由卷积令牌嵌入和卷积变压器块堆栈组成。\n\nmicrosoft/cvt-21-384-22k模型的具体配置是CvT-21，有31.54M参数。\n\n模型目标：\nCvT 模型的目标是在图像分类任务（例如 ImageNet-1k）上实现最先进的性能，同时与其他 Vision Transformer 和 ResNet 相比具有更少的参数和更低的 FLOP。该模型旨在将 CNN 的理想特性引入 Vision Transformer 架构（即平移、尺度和失真不变性），同时保持 Transformer 的优点（即动态注意力、全局上下文和更好的泛化能力）。\n\nCvT-W24 模型在较大的 ImageNet-22k 数据集上进行预训练并在 ImageNet-1k 上进行微调时，在 ImageNet-1k 验证集上获得了 87.7% 的 top-1 准确率。\n\n[需要更多信息] 有关 microsoft/cvt-21-384-22k 模型使用的具体训练细节和超参数。",
    "answer_Arabic": "العمارة النموذجية:\nتقدم بنية محول الرؤية التلافيفي (CvT) عمليتين قائمتين على التلافيف في بنية محول الرؤية:\n1. تضمين الرمز التلافيفي: تخضع صورة الإدخال (أو خرائط الرمز المميز المعاد تشكيلها ثنائية الأبعاد) لطبقة تضمين الرمز التلافيفي، والتي يتم تنفيذها كالتفاف مع بقع متداخلة مع الرموز المميزة المعاد تشكيلها إلى الشبكة المكانية ثنائية الأبعاد. يتيح ذلك لكل مرحلة تقليل عدد الرموز المميزة تدريجيًا مع زيادة عرض الرموز المميزة، وتحقيق الاختزال المكاني وزيادة ثراء التمثيل.\n2. الإسقاط التلافيفي: في كتلة المحولات التلافيفية، يتم تطبيق عملية تلافيفية قابلة للفصل من حيث العمق، يشار إليها باسم الإسقاط التلافيفي، على عمليات تضمين الاستعلام والمفتاح والقيمة. يحل هذا محل الإسقاط الخطي الأصلي للموضع من أجل الانتباه الذاتي متعدد الرؤوس (MHSA).\n\nتستخدم بنية CvT تصميمًا هرميًا متعدد المراحل مأخوذًا من شبكات CNN، مع ثلاث مراحل إجمالاً. تتكون كل مرحلة من تضمين الرمز التلافيفي متبوعًا بمجموعة من كتل المحولات التلافيفية.\n\nالتكوين المحدد لنموذج microsoft/cvt-21-384-22k هو CvT-21، الذي يحتوي على 31.54 مليون معلمة.\n\nهدف النموذج:\nالهدف من نموذج CvT هو تحقيق أداء متطور في مهام تصنيف الصور، مثل ImageNet-1k، مع وجود معلمات أقل وFLOPs أقل مقارنة بمحولات الرؤية وشبكات ResNets الأخرى. يهدف النموذج إلى تقديم الخصائص المرغوبة لشبكات CNN في بنية Vision Transformer (أي التحول والحجم وثبات التشويه) مع الحفاظ على مزايا المحولات (أي الاهتمام الديناميكي والسياق العالمي والتعميم الأفضل).\n\nيحصل نموذج CvT-W24، عند تدريبه مسبقًا على مجموعة بيانات ImageNet-22k الأكبر حجمًا وضبطه بدقة على ImageNet-1k، على دقة من أعلى 1 بنسبة 87.7% في مجموعة التحقق من صحة ImageNet-1k.\n\n[مطلوب مزيد من المعلومات] حول تفاصيل التدريب المحددة والمعلمات الفائقة المستخدمة لنموذج microsoft/cvt-21-384-22k.",
    "answer_French": "Architecture du modèle :\nL'architecture Convolutional Vision Transformer (CvT) introduit deux opérations basées sur la convolution dans l'architecture Vision Transformer :\n1. Incorporation de jetons convolutifs : l'image d'entrée (ou les cartes de jetons remodelées en 2D) sont soumises à la couche d'incorporation de jetons convolutifs, qui est implémentée sous la forme d'une convolution avec des correctifs superposés avec des jetons remodelés selon la grille spatiale 2D. Cela permet à chaque étape de réduire progressivement le nombre de jetons tout en augmentant la largeur des jetons, obtenant ainsi un sous-échantillonnage spatial et une richesse de représentation accrue.\n2. Projection convolutive : dans le bloc transformateur convolutif, une opération de convolution séparable en profondeur, appelée projection convolutive, est appliquée pour les intégrations de requêtes, de clés et de valeurs. Cela remplace la projection linéaire originale par position pour l'auto-attention multi-têtes (MHSA).\n\nL'architecture CvT utilise une conception hiérarchique à plusieurs étapes empruntée aux CNN, avec trois étapes au total. Chaque étape consiste en l'intégration de jetons convolutifs suivi d'une pile de blocs de transformateur convolutifs.\n\nLa configuration spécifique pour le modèle Microsoft/cvt-21-384-22k est CvT-21, qui possède 31,54 millions de paramètres.\n\nObjectif du modèle :\nL'objectif du modèle CvT est d'atteindre des performances de pointe sur les tâches de classification d'images, telles que ImageNet-1k, tout en ayant moins de paramètres et des FLOP inférieurs par rapport aux autres transformateurs de vision et ResNets. Le modèle vise à introduire les propriétés souhaitables des CNN dans l'architecture Vision Transformer (c'est-à-dire l'invariance de décalage, d'échelle et de distorsion) tout en conservant les mérites des Transformers (c'est-à-dire l'attention dynamique, le contexte global et une meilleure généralisation).\n\nLe modèle CvT-W24, lorsqu'il est pré-entraîné sur le plus grand ensemble de données ImageNet-22k et affiné sur ImageNet-1k, obtient une précision top 1 de 87,7 % sur l'ensemble de validation ImageNet-1k.\n\n[Plus d'informations nécessaires] sur les détails de formation spécifiques et les hyperparamètres utilisés pour le modèle Microsoft/cvt-21-384-22k.",
    "answer_Russian": "Модельная архитектура:\nАрхитектура Convolutional Vision Transformer (CvT) вводит в архитектуру Vision Transformer две операции на основе свертки:\n1. Встраивание сверточных токенов. Входное изображение (или двумерные карты токенов измененной формы) подвергается воздействию слоя встраивания сверточных токенов, который реализуется как свертка с перекрывающимися участками с токенами, преобразованными в двумерную пространственную сетку. Это позволяет на каждом этапе постепенно уменьшать количество токенов, одновременно увеличивая ширину токенов, достигая пространственного понижения разрешения и увеличения богатства представления.\n2. Сверточная проекция. В блоке сверточного преобразователя для встраивания запроса, ключа и значения применяется разделяемая по глубине операция свертки, называемая сверточной проекцией. Это заменяет исходную позиционную линейную проекцию для многоголового самообслуживания (MHSA).\n\nВ архитектуре CvT используется многоступенчатая иерархия, заимствованная у CNN, всего три этапа. Каждый этап состоит из внедрения сверточного токена, за которым следует стек блоков сверточного преобразователя.\n\nКонкретной конфигурацией модели microsoft/cvt-21-384-22k является CvT-21, которая имеет 31,54M параметров.\n\nЦель модели:\nЦелью модели CvT является достижение самой современной производительности в задачах классификации изображений, таких как ImageNet-1k, при меньшем количестве параметров и меньших FLOP по сравнению с другими Vision Transformers и ResNet. Модель направлена ​​на то, чтобы привнести желаемые свойства CNN в архитектуру Vision Transformer (т. Е. Инвариантность сдвига, масштаба и искажений), сохраняя при этом достоинства Transformers (т. Е. Динамическое внимание, глобальный контекст и лучшее обобщение).\n\nМодель CvT-W24, предварительно обученная на более крупном наборе данных ImageNet-22k и точно настроенная на ImageNet-1k, обеспечивает высшую точность 87,7% на проверочном наборе ImageNet-1k.\n\n[Требуется дополнительная информация] о конкретных деталях обучения и гиперпараметрах, используемых для модели microsoft/cvt-21-384-22k.",
    "answer_Japanese": "モデルのアーキテクチャ:\n畳み込みビジョン トランスフォーマー (CvT) アーキテクチャでは、ビジョン トランスフォーマー アーキテクチャに 2 つの畳み込みベースの演算が導入されています。\n1. 畳み込みトークン埋め込み: 入力画像 (または 2D 再整形トークン マップ) は、2D 空間グリッドに再整形されたトークンを含むオーバーラップ パッチによる畳み込みとして実装される畳み込みトークン埋め込みレイヤーの対象となります。これにより、各ステージでトークンの幅を増やしながらトークンの数を段階的に減らすことができ、空間的なダウンサンプリングと表現の豊かさの向上が実現します。\n2. 畳み込み投影:畳み込み変換ブロックでは、畳み込み投影と呼ばれる深さ方向の分離可能な畳み込み演算がクエリ、キー、および値の埋め込みに適用されます。これは、マルチヘッド セルフ アテンション (MHSA) の元の位置に関する線形投影を置き換えます。\n\nCvT アーキテクチャは、CNN から借用した多段階階層設計を採用しており、合計 3 つの段階があります。各ステージは、畳み込みトークンの埋め込みと、その後に続く畳み込み変換ブロックのスタックで構成されます。\n\nMicrosoft/cvt-21-384-22k モデルの具体的な構成は CvT-21 で、これには 3154 万個のパラメータがあります。\n\nモデルの目的:\nCvT モデルの目的は、他の Vision Transformer や ResNet と比べてパラメータが少なく、FLOP が低い一方で、ImageNet-1k などの画像分類タスクで最先端のパフォーマンスを達成することです。このモデルは、Transformer の利点 (つまり、動的な注意、グローバル コンテキスト、より優れた一般化) を維持しながら、CNN の望ましい特性 (つまり、シフト、スケール、歪みの不変性) を Vision Transformer アーキテクチャに導入することを目的としています。\n\nCvT-W24 モデルは、より大きな ImageNet-22k データセットで事前トレーニングされ、ImageNet-1k で微調整された場合、ImageNet-1k 検証セットで 87.7% というトップ 1 の精度を獲得します。\n\nMicrosoft/cvt-21-384-22k モデルに使用される特定のトレーニングの詳細とハイパーパラメーターに関する [詳細情報が必要]。"
  },
  {
    "question": "compute_infrastructure",
    "chain": "generation",
    "answer": "Based on the provided references, here is the available information about the compute infrastructure for the model microsoft/cvt-21-384-22k:\n\n- The model was trained using the AdamW optimizer with a weight decay of 0.1 for CvT-21.\n- The models were trained with an initial learning rate of 0.02 and a total batch size of 2048 for 300 epochs, using a cosine learning rate decay scheduler.\n- The input size for training the ImageNet models was 224 × 224, unless otherwise stated.\n\n[More Information Needed] about the specific hardware (GPU/TPU) and the number of devices used for training the model microsoft/cvt-21-384-22k.",
    "answer_Chinese": "根据提供的参考资料，以下是有关模型 microsoft/cvt-21-384-22k 的计算基础架构的可用信息：\n\n- 该模型使用 AdamW 优化器进行训练，CvT-21 的权重衰减为 0.1。\n- 使用余弦学习率衰减调度程序，对模型进行训练，初始学习率为 0.02，总批量大小为 2048，持续 300 个时期。\n- 除非另有说明，训练 ImageNet 模型的输入大小为 224 × 224。\n\n[需要更多信息]有关用于训练模型的特定硬件（GPU/TPU）和设备数量 microsoft/cvt-21-384-22k。",
    "answer_Arabic": "استنادًا إلى المراجع المتوفرة، إليك المعلومات المتوفرة حول البنية الأساسية للحوسبة للطراز microsoft/cvt-21-384-22k:\n\n- تم تدريب النموذج باستخدام محسن AdamW مع اضمحلال وزن قدره 0.1 لـ CvT-21.\n- تم تدريب النماذج بمعدل تعلم أولي قدره 0.02 وحجم دفعة إجمالي قدره 2048 لمدة 300 عصر، باستخدام جدولة تضاؤل ​​معدل التعلم جيب التمام.\n- كان حجم الإدخال لتدريب نماذج ImageNet 224 × 224، ما لم يُنص على خلاف ذلك.\n\n[مطلوب مزيد من المعلومات] حول الأجهزة المحددة (GPU/TPU) وعدد الأجهزة المستخدمة لتدريب الطراز microsoft/cvt-21-384-22k.",
    "answer_French": "Sur la base des références fournies, voici les informations disponibles sur l'infrastructure de calcul pour le modèle Microsoft/cvt-21-384-22k :\n\n- Le modèle a été entraîné à l'aide de l'optimiseur AdamW avec une décroissance de poids de 0,1 pour CvT-21.\n- Les modèles ont été formés avec un taux d'apprentissage initial de 0,02 et une taille totale de lot de 2048 pour 300 époques, en utilisant un planificateur de décroissance du taux d'apprentissage du cosinus.\n- La taille d'entrée pour la formation des modèles ImageNet était de 224 × 224, sauf indication contraire.\n\n[Plus d'informations nécessaires] sur le matériel spécifique (GPU/TPU) et le nombre d'appareils utilisés pour la formation du modèle Microsoft/cvt-21-384-22k.",
    "answer_Russian": "На основании предоставленных ссылок приведена доступная информация о вычислительной инфраструктуре для модели microsoft/cvt-21-384-22k:\n\n- Модель обучена с использованием оптимизатора AdamW со снижением веса 0,1 для CvT-21.\n- Модели были обучены с начальной скоростью обучения 0,02 и общим размером пакета 2048 для 300 эпох с использованием косинусного планировщика затухания скорости обучения.\n- Размер входных данных для обучения моделей ImageNet составлял 224 × 224, если не указано иное.\n\n[Требуется дополнительная информация] о конкретном оборудовании (GPU/TPU) и количестве устройств, используемых для обучения модели microsoft/cvt-21-384-22k.",
    "answer_Japanese": "提供された参考資料に基づいて、microsoft/cvt-21-384-22k モデルのコンピューティング インフラストラクチャに関する入手可能な情報は次のとおりです。\n\n- モデルは、CvT-21 の重み減衰 0.1 で AdamW オプティマイザーを使用してトレーニングされました。\n- モデルは、コサイン学習率減衰スケジューラーを使用して、初期学習率 0.02、合計バッチ サイズ 2048 で 300 エポックでトレーニングされました。\n- 特に明記されていない限り、ImageNet モデルのトレーニングの入力サイズは 224 × 224 でした。\n\nモデルのトレーニングに使用される特定のハードウェア (GPU/TPU) とデバイスの数に関する [詳細情報が必要] Microsoft/cvt-21-384-22k。"
  }
]