{"text": "Hi, this is Elena and I'm going to be presenting our work, Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling.\n", "text_Chinese": "大家好，我是埃琳娜，我将介绍我们的工作，检测西班牙语中的未同化借词：带注释的语料库和建模方法。", "text_Arabic": "مرحبًا، هذه إيلينا وسأقدم عملنا، اكتشاف الاقتراضات غير المستوعبة باللغة الإسبانية: مجموعة مشروحة ومناهج للنمذجة.", "text_French": "Bonjour, ici Elena et je vais présenter notre travail, Détection des emprunts non assimilés en espagnol : un corpus annoté et des approches de modélisation.", "text_Japanese": "こんにちは、エレナです。私はこれから私たちの研究「スペイン語の未同化借用の検出: 注釈付きコーパスとモデリングへのアプローチ」を発表します。", "text_Russian": "Привет, это Елена, и я собираюсь представить нашу работу «Обнаружение неассимилированных заимствований в испанском языке: аннотированный корпус и подходы к моделированию»."}
{"text": "So we're going to be covering what lexical borrowing is, the task that we proposed, the dataset that we have released and some models that we explored.\n", "text_Chinese": "因此，我们将介绍什么是词汇借用、我们提出的任务、我们发布的数据集以及我们探索的一些模型。", "text_Arabic": "لذا، سنغطي ماهية الاقتراض المعجمي، والمهمة التي اقترحناها، ومجموعة البيانات التي أصدرناها وبعض النماذج التي استكشفناها.", "text_French": "Nous allons donc aborder ce qu'est l'emprunt lexical, la tâche que nous avons proposée, l'ensemble de données que nous avons publié et certains modèles que nous avons explorés.", "text_Japanese": "そこで、語彙借用とは何か、私たちが提案したタスク、私たちがリリースしたデータセット、そして私たちが調査したいくつかのモデルについて説明します。", "text_Russian": "Итак, мы собираемся рассказать, что такое лексическое заимствование, задачу, которую мы предложили, набор данных, который мы выпустили, и некоторые модели, которые мы исследовали."}
{"text": "But to begin with, what is lexical borrowing and why it matters as an NLP task?\n", "text_Chinese": "但首先，什么是词汇借用以及为什么它作为 NLP 任务很重要？", "text_Arabic": "ولكن في البداية، ما هو الاقتراض المعجمي ولماذا هو مهم كمهمة البرمجة اللغوية العصبية؟", "text_French": "Mais pour commencer, qu’est-ce que l’emprunt lexical et pourquoi est-il important en tant que tâche de PNL ?", "text_Japanese": "しかし、そもそも、語彙借用とは何でしょうか?また、なぜそれが NLP タスクとして重要なのでしょうか?", "text_Russian": "Но для начала, что такое лексическое заимствование и почему оно важно для задачи НЛП?"}
{"text": "Well, lexical borrowing is basically the incorporation of words from one language into another language.\n", "text_Chinese": "嗯，词汇借用基本上是将一种语言的单词合并到另一种语言中。", "text_Arabic": "حسنًا، الاقتراض المعجمي هو في الأساس دمج كلمات من لغة ما إلى لغة أخرى.", "text_French": "Eh bien, l’emprunt lexical consiste essentiellement à incorporer des mots d’une langue dans une autre langue.", "text_Japanese": "そうですね、語彙借用とは基本的に、ある言語の単語を別の言語に組み込むことです。", "text_Russian": "Что ж, лексическое заимствование — это, по сути, включение слов из одного языка в другой язык."}
{"text": "For instance, in Spanish we use words that come from English.\n", "text_Chinese": "例如，在西班牙语中，我们使用来自英语的单词。", "text_Arabic": "على سبيل المثال، في اللغة الإسبانية نستخدم الكلمات التي تأتي من اللغة الإنجليزية.", "text_French": "Par exemple, en espagnol, nous utilisons des mots qui viennent de l’anglais.", "text_Japanese": "たとえば、スペイン語では英語から来た単語を使用します。", "text_Russian": "Например, в испанском мы используем слова, пришедшие из английского."}
{"text": "And here you have a few examples, words such as podcast, app, and online crowdfunding, all these are English words that we sometimes use in Spanish.\n", "text_Chinese": "这里有一些例子，诸如播客、应用程序和在线众筹等单词，所有这些都是我们有时在西班牙语中使用的英语单词。", "text_Arabic": "وهنا لديك بعض الأمثلة، كلمات مثل البودكاست والتطبيق والتمويل الجماعي عبر الإنترنت، كلها كلمات إنجليزية نستخدمها أحيانًا باللغة الإسبانية.", "text_French": "Et voici quelques exemples, des mots comme podcast, application et financement participatif en ligne, ce sont tous des mots anglais que nous utilisons parfois en espagnol.", "text_Japanese": "ポッドキャスト、アプリ、オンライン クラウドファンディングなどの単語の例をいくつか示します。これらはすべて、スペイン語で時々使用される英語の単語です。", "text_Russian": "И вот несколько примеров: такие слова, как подкаст, приложение и онлайн-краудфандинг, все это английские слова, которые мы иногда используем в испанском языке."}
{"text": "Lexical borrowing is a type of linguistic borrowing um which is basically reproducing in one language patterns of other languages.\n", "text_Chinese": "词汇借用是一种语言借用，基本上是在一种语言模式中再现其他语言。", "text_Arabic": "الاقتراض المعجمي هو نوع من الاقتراض اللغوي الذي يتكاثر بشكل أساسي في لغة واحدة أنماط اللغات الأخرى.", "text_French": "L'emprunt lexical est un type d'emprunt linguistique qui reproduit essentiellement dans une langue des modèles d'autres langues.", "text_Japanese": "語彙借用は、基本的にある言語のパターンを他の言語で再現する言語借用の一種です。", "text_Russian": "Лексическое заимствование – это тип лингвистического заимствования, который в основном воспроизводит в одном языке образцы других языков."}
{"text": "And borrowing and code switching have sometimes been compared and described as a continuum, code switching being ah the thing that bilinguals do where they mix two languages at the same time.\n", "text_Chinese": "借用和语码转换有时被比较并描述为一个连续体，语码转换是双语者同时混合两种语言时所做的事情。", "text_Arabic": "وقد تمت في بعض الأحيان مقارنة الاقتراض وتبديل التعليمات البرمجية ووصفهما على أنهما سلسلة متصلة، حيث أن تبديل التعليمات البرمجية هو الشيء الذي يفعله ثنائيو اللغة حيث يمزجون لغتين في نفس الوقت.", "text_French": "Et l'emprunt et le changement de code ont parfois été comparés et décrits comme un continuum, le changement de code étant ce que font les bilingues lorsqu'ils mélangent deux langues en même temps.", "text_Japanese": "また、借用とコード スイッチングは、連続体として比較され説明されることがあります。コード スイッチングは、ああ、バイリンガルが 2 つの言語を同時に混合するときに行うことです。", "text_Russian": "А заимствование и переключение кода иногда сравнивают и описывают как континуум, причем переключение кода — это то, что делают билингвы, когда они смешивают два языка одновременно."}
{"text": "There are however some differences between lexical borrowing and code-switching.\n", "text_Chinese": "然而，词汇借用和语码转换之间存在一些差异。", "text_Arabic": "ومع ذلك، هناك بعض الاختلافات بين الاقتراض المعجمي وتبديل التعليمات البرمجية.", "text_French": "Il existe cependant quelques différences entre l'emprunt lexical et le changement de code.", "text_Japanese": "ただし、語彙借用とコードスイッチングの間にはいくつかの違いがあります。", "text_Russian": "Однако существуют некоторые различия между лексическим заимствованием и переключением кода."}
{"text": "We're going to be focusing on lexical borrowing.\n", "text_Chinese": "我们将重点关注词汇借用。", "text_Arabic": "سنركز على الاقتراض المعجمي.", "text_French": "Nous allons nous concentrer sur les emprunts lexicaux.", "text_Japanese": "ここでは語彙借用に焦点を当てていきます。", "text_Russian": "Мы собираемся сосредоточиться на лексических заимствованиях."}
{"text": "Code switching is something that is done by bilinguals and by definition the code switches are not integrated into any of the languages in use, whereas lexical borrowing is something that is also done by monolinguals.\n", "text_Chinese": "语码转换是双语者完成的事情，根据定义，语码转换不会集成到所使用的任何语言中，而词汇借用也是单语者完成的事情。", "text_Arabic": "تبديل التعليمات البرمجية هو شيء يتم إجراؤه بواسطة ثنائيي اللغة، وبحكم التعريف، لا يتم دمج محولات التعليمات البرمجية في أي من اللغات المستخدمة، في حين أن الاقتراض المعجمي هو شيء يتم إجراؤه أيضًا بواسطة أحاديي اللغة.", "text_French": "Le changement de code est quelque chose qui est effectué par les bilingues et, par définition, les changements de code ne sont intégrés dans aucune des langues utilisées, alors que l'emprunt lexical est également quelque chose qui est effectué par les monolingues.", "text_Japanese": "コードスイッチングはバイリンガルによって行われるものであり、定義上、コードスイッチは使用中のどの言語にも統合されませんが、語彙借用もモノリンガルによって行われます。", "text_Russian": "Переключение кода — это то, что делают билингвы, и по определению переключатели кода не интегрированы ни в один из используемых языков, тогда как лексическое заимствование — это то, что также делают монолингвы."}
{"text": "The borrowings will comply with the grammar of the recipient language.\n", "text_Chinese": "借用的内容将符合接收语言的语法。", "text_Arabic": "سوف تتوافق الاقتراضات مع قواعد اللغة المتلقية.", "text_French": "Les emprunts respecteront la grammaire de la langue destinataire.", "text_Japanese": "借用は受信者の言語の文法に準拠します。", "text_Russian": "Заимствования будут соответствовать грамматике языка получателя."}
{"text": "And borrowings can eventually be integrated into the recipient language.\n", "text_Chinese": "借用的内容最终可以融入接收者的语言中。", "text_Arabic": "ويمكن في النهاية دمج الاقتراضات في اللغة المتلقية.", "text_French": "Et les emprunts peuvent éventuellement être intégrés dans la langue du destinataire.", "text_Japanese": "そして、借用は最終的に受信者の言語に統合される可能性があります。", "text_Russian": "А заимствования со временем могут быть интегрированы в язык-реципиент."}
{"text": "So why is borrowing an interesting phenomenon?\n", "text_Chinese": "那么为什么借贷是一个有趣的现象呢？", "text_Arabic": "فلماذا يعتبر الاقتراض ظاهرة مثيرة للاهتمام؟", "text_French": "Alors pourquoi l’emprunt est-il un phénomène intéressant ?", "text_Japanese": "では、なぜ借用は興味深い現象なのでしょうか?", "text_Russian": "Так почему же заимствование является интересным явлением?"}
{"text": "Well, from the point of view of linguistics, borrowing is a manifestation of of how languages change and how they interact.\n", "text_Chinese": "那么，从语言学的角度来看，借用是语言如何变化、如何相互作用的表现。", "text_Arabic": "حسنًا، من وجهة نظر علم اللغة، يعد الاقتراض مظهرًا لكيفية تغير اللغات وكيفية تفاعلها.", "text_French": "Eh bien, du point de vue de la linguistique, l’emprunt est une manifestation de la façon dont les langues changent et dont elles interagissent.", "text_Japanese": "言語学の観点から見ると、借用は言語がどのように変化し、言語がどのように相互作用するかを示しています。", "text_Russian": "Что ж, с точки зрения лингвистики заимствование — это проявление того, как изменяются языки и как они взаимодействуют."}
{"text": "And also lexical borrowings are a source of new words.\n", "text_Chinese": "词汇借用也是新词的来源。", "text_Arabic": "وكذلك الاقتراضات المعجمية هي مصدر للكلمات الجديدة.", "text_French": "Et aussi les emprunts lexicaux sont source de nouveaux mots.", "text_Japanese": "また、語彙の借用は新しい単語の源でもあります。", "text_Russian": "А также лексические заимствования являются источником новых слов."}
{"text": "Here you have some examples of lexical borrowings that have been incorporated into the Spanish language as new words.\n", "text_Chinese": "这里有一些词汇借用的例子，它们已作为新词并入西班牙语。", "text_Arabic": "هنا لديك بعض الأمثلة على الاقتراضات المعجمية التي تم دمجها في اللغة الإسبانية ككلمات جديدة.", "text_French": "Vous trouverez ici quelques exemples d'emprunts lexicaux qui ont été incorporés à la langue espagnole en tant que nouveaux mots.", "text_Japanese": "ここでは、新しい単語としてスペイン語に組み込まれた語彙借用の例をいくつか示します。", "text_Russian": "Здесь вы найдете несколько примеров лексических заимствований, которые вошли в испанский язык как новые слова."}
{"text": "In terms of NLP ah borrowings are a common source of out-of-vocabulary words.\n", "text_Chinese": "就 NLP 而言，ah 借用是词汇表外单词的常见来源。", "text_Arabic": "فيما يتعلق بالبرمجة اللغوية العصبية، تعد الاقتراضات مصدرًا شائعًا للكلمات خارج المفردات.", "text_French": "En termes de PNL, les emprunts ah sont une source courante de mots hors vocabulaire.", "text_Japanese": "NLP の観点から言えば、借用は語彙外の単語の一般的な原因です。", "text_Russian": "С точки зрения НЛП, заимствования являются распространенным источником слов, выходящих за рамки словарного запаса."}
{"text": "And in fact, automatically detecting lexical borrowings ah has proven to be useful for NLP downstream tasks such as parsing, text-to-speech synthesis or machine translation.\n", "text_Chinese": "事实上，自动检测词汇借用 ah 已被证明对于 NLP 下游任务（例如解析、文本到语音合成或机器翻译）非常有用。", "text_Arabic": "وفي الواقع، أثبت الكشف التلقائي عن الاقتراضات المعجمية أنه مفيد لمهام البرمجة اللغوية العصبية (NLP) مثل التحليل أو تحويل النص إلى كلام أو الترجمة الآلية.", "text_French": "Et en fait, la détection automatique des emprunts lexicaux s'est avérée utile pour les tâches en aval de la PNL telles que l'analyse syntaxique, la synthèse texte-parole ou la traduction automatique.", "text_Japanese": "実際、語彙借用の自動検出は、解析、音声合成、機械翻訳などの NLP の下流タスクに役立つことが証明されています。", "text_Russian": "Фактически, автоматическое обнаружение лексических заимствований оказалось полезным для последующих задач НЛП, таких как синтаксический анализ, синтез речи или машинный перевод."}
{"text": "There has been a growing interest in the influence of English on other languages ah particularly ah related to English lexical borrowings, borrowings which sometimes have been called Anglicisms.\n", "text_Chinese": "人们对英语对其他语言的影响越来越感兴趣，尤其是与英语词汇借用有关的影响，这些借用有时被称为英语主义。", "text_Arabic": "كان هناك اهتمام متزايد بتأثير اللغة الإنجليزية على اللغات الأخرى، لا سيما فيما يتعلق بالاستعارات المعجمية الإنجليزية، وهي الاقتراضات التي تسمى أحيانًا بالانجليزية.", "text_French": "Il y a eu un intérêt croissant pour l'influence de l'anglais sur d'autres langues, en particulier en ce qui concerne les emprunts lexicaux anglais, emprunts qui ont parfois été appelés anglicismes.", "text_Japanese": "英語が他の言語に与える影響、特に英語の語彙の借用、英国主義と呼ばれることもある借用に関連するものについての関心が高まっています。", "text_Russian": "Растет интерес к влиянию английского языка на другие языки, особенно в отношении английских лексических заимствований, заимствований, которые иногда называют англицизмами."}
{"text": "And here, you have some examples of ah work on automatic detection of borrowings in ah some of these languages.\n", "text_Chinese": "在这里，您有一些关于自动检测某些语言中的借用的工作的示例。", "text_Arabic": "وهنا، لديك بعض الأمثلة على العمل على الكشف التلقائي عن الاقتراضات في بعض هذه اللغات.", "text_French": "Et ici, vous avez quelques exemples de travaux sur la détection automatique des emprunts dans certaines de ces langues.", "text_Japanese": "ここでは、これらの言語の一部での借用の自動検出に関するいくつかの例を示します。", "text_Russian": "И здесь у вас есть несколько примеров работы по автоматическому обнаружению заимствований в некоторых из этих языков."}
{"text": "So the task that we propose is to detect unassimilated lexical borrowings in Spanish newswire.\n", "text_Chinese": "因此，我们提出的任务是检测西班牙语新闻专线中未同化的词汇借用。", "text_Arabic": "لذا فإن المهمة التي نقترحها هي اكتشاف الاقتراضات المعجمية غير المستوعبة في وكالة الأنباء الإسبانية.", "text_French": "La tâche que nous proposons est donc de détecter les emprunts lexicaux non assimilés dans les médias espagnols.", "text_Japanese": "したがって、私たちが提案するタスクは、スペイン語のニュースワイヤーで同化されていない語彙の借用を検出することです。", "text_Russian": "Поэтому задача, которую мы предлагаем, — выявить неассимилированные лексические заимствования в испанской ленте новостей."}
{"text": "Which means that we are interested in extracting ah words borrowed from other languages that are being used in Spanish newspapers but that have not been integrated or assimilated into the recipient language.\n", "text_Chinese": "这意味着我们有兴趣提取从其他语言借用的 ah 单词，这些单词在西班牙语报纸中使用，但尚未被整合或同化到接收语言中。", "text_Arabic": "مما يعني أننا مهتمون باستخراج الكلمات المستعارة من اللغات الأخرى والتي يتم استخدامها في الصحف الإسبانية ولكن لم يتم دمجها أو استيعابها في اللغة المتلقية.", "text_French": "Cela signifie que nous souhaitons extraire tous les mots empruntés à d'autres langues qui sont utilisés dans les journaux espagnols mais qui n'ont pas été intégrés ou assimilés dans la langue du destinataire.", "text_Japanese": "つまり、スペイン語の新聞で使用されているものの、受信者の言語に統合または同化されていない、他の言語から借用した単語を抽出することに興味があるということです。", "text_Russian": "Это означает, что мы заинтересованы в извлечении слов, заимствованных из других языков, которые используются в испанских газетах, но не были интегрированы или ассимилированы в язык получателя."}
{"text": "So not yet integrated into Spanish.\n", "text_Chinese": "所以还没有融入西班牙语。", "text_Arabic": "لذلك لم يتم دمجها بعد في اللغة الإسبانية.", "text_French": "Donc pas encore intégré à l'espagnol.", "text_Japanese": "したがって、まだスペイン語に統合されていません。", "text_Russian": "Так что еще не интегрирован в испанский."}
{"text": "Here you have an example.\n", "text_Chinese": "这里有一个例子。", "text_Arabic": "هنا لديك مثال.", "text_French": "Ici vous avez un exemple.", "text_Japanese": "ここに例があります。", "text_Russian": "Вот вам пример."}
{"text": "This is a sentence in Spanish: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork.\n", "text_Chinese": "这是西班牙语中的一句话：Las prendas bestsellers se estampan con motivosflores,animal print o retales Tipo patchwork。", "text_Arabic": "هذه جملة باللغة الإسبانية: الأكثر مبيعًا Las prendas se estampan con motivos Florales، أو طباعة حيوانية، أو retales Tipo patchwork.", "text_French": "Voici une phrase en espagnol : Las prendas best-sellers se estampan con motivos florales, animal print o retales tipo patchwork.", "text_Japanese": "これはスペイン語の文です：Las prendas bestsellers se estampan con motivos Flores、animal print o retalestipo patchwork。", "text_Russian": "Это предложение на испанском языке: Бестселлеры Las prendas se estampan con motivos flowers, Animal Print или Retales Tipo Patchwork."}
{"text": "Um, and as you can see, there are three spans of texts which are actually English words like bestseller, animal print and patchwork.\n", "text_Chinese": "嗯，正如你所看到的，有三段文本实际上是英语单词，比如 bestseller、animal print 和 patchwork。", "text_Arabic": "وكما ترون، هناك ثلاثة أجزاء من النصوص وهي في الواقع كلمات إنجليزية مثل الكتب الأكثر مبيعًا وطباعة الحيوانات والرقعة.", "text_French": "Euh, et comme vous pouvez le voir, il y a trois tranches de textes qui sont en fait des mots anglais comme best-seller, imprimé animal et patchwork.", "text_Japanese": "ご覧のとおり、ベストセラー、アニマル プリント、パッチワークなど、実際には英語の単語が 3 つのスパンで含まれています。", "text_Russian": "Хм, и, как вы можете видеть, есть три фрагмента текста, которые на самом деле представляют собой английские слова, такие как «бестселлер», «животный принт» и «пэчворк»."}
{"text": "These are the type of spans that we are interested in extracting and detecting.\n", "text_Chinese": "这些是我们有兴趣提取和检测的跨度类型。", "text_Arabic": "هذه هي أنواع الامتدادات التي نهتم باستخراجها وكشفها.", "text_French": "C’est le type de travées que nous souhaitons extraire et détecter.", "text_Japanese": "これらは、抽出して検出する必要があるタイプのスパンです。", "text_Russian": "Это тот тип промежутков, который нас интересует в извлечении и обнаружении."}
{"text": "There has been previous word on Anglicism detection ah which consists consisted of a CRF model for Anglicism detection on Spanish Newswire.\n", "text_Chinese": "之前有关于英语主义检测的消息，它由西班牙新闻专线上用于英语主义检测的 CRF 模型组成。", "text_Arabic": "كانت هناك كلمة سابقة حول اكتشاف الانجليكانية والتي تتكون من نموذج CRF للكشف عن الانجليكانية على Newswire الاسبانية.", "text_French": "Il y a eu des rumeurs sur la détection de l'anglicisme, qui consistaient en un modèle CRF pour la détection de l'anglicisme sur le fil de presse espagnol.", "text_Japanese": "スペインのニュースワイヤーで英国主義検出のための CRF モデルで構成される英国主義検出に関する以前の記事がありました。", "text_Russian": "Ранее сообщалось об обнаружении англицизма, которое состояло из модели CRF для обнаружения англицизма в испанской новостной ленте."}
{"text": "This model achieved an F1 score of eighty six.\n", "text_Chinese": "该模型获得了 86 分的 F1 分数。", "text_Arabic": "حقق هذا النموذج درجة F1 ستة وثمانين.", "text_French": "Ce modèle a obtenu un score F1 de quatre-vingt-six.", "text_Japanese": "このモデルは F1 スコア 86 を達成しました。", "text_Russian": "Эта модель получила восемьдесят шесть баллов в Формуле-1."}
{"text": "But there were some limitations both um in the dataset and the modeling approach.\n", "text_Chinese": "但数据集和建模方法都存在一些限制。", "text_Arabic": "ولكن كانت هناك بعض القيود في مجموعة البيانات ونهج النمذجة.", "text_French": "Mais il y avait certaines limites à la fois dans l’ensemble de données et dans l’approche de modélisation.", "text_Japanese": "しかし、データセットとモデリングのアプローチの両方にいくつかの制限がありました。", "text_Russian": "Но были некоторые ограничения как в наборе данных, так и в подходе к моделированию."}
{"text": "So the dataset focused exclusively on one source of news, consisted only of headlines.\n", "text_Chinese": "因此，该数据集仅关注一种新闻来源，仅包含头条新闻。", "text_Arabic": "لذا، ركزت مجموعة البيانات حصريًا على مصدر واحد للأخبار، وتتكون فقط من العناوين الرئيسية.", "text_French": "Ainsi, l’ensemble de données se concentrait exclusivement sur une source d’information et ne comprenait que des titres.", "text_Japanese": "したがって、データセットは 1 つのニュース ソースのみに焦点を当て、見出しのみで構成されていました。", "text_Russian": "Таким образом, набор данных, ориентированный исключительно на один источник новостей, состоял только из заголовков."}
{"text": "And also there was an overlap in the borrowings that appear in the training set and the test set.\n", "text_Chinese": "而且训练集和测试集中出现的借用也存在重叠。", "text_Arabic": "وأيضا كان هناك تداخل في الاقتراضات التي تظهر في مجموعة التدريب ومجموعة الاختبار.", "text_French": "Et il y avait également un chevauchement dans les emprunts qui apparaissent dans l’ensemble de formation et dans l’ensemble de test.", "text_Japanese": "また、トレーニング セットとテスト セットに出現する借用には重複がありました。", "text_Russian": "А также произошло перекрытие заимствований, которые появляются в обучающем наборе и тестовом наборе."}
{"text": "This prevented the assessment of whether the modeling approach could actually generalize to previously unseen borrowings.\n", "text_Chinese": "这阻碍了对建模方法是否实际上可以推广到以前未见过的借用的评估。", "text_Arabic": "وقد حال هذا دون تقييم ما إذا كان نهج النمذجة يمكن أن يعمم بالفعل على القروض غير المرئية من قبل.", "text_French": "Cela a empêché d’évaluer si l’approche de modélisation pouvait réellement se généraliser à des emprunts inédits.", "text_Japanese": "これにより、モデル化アプローチが実際にこれまで目に見えなかった借入に一般化できるかどうかを評価することができなくなりました。", "text_Russian": "Это помешало оценить, может ли подход к моделированию действительно обобщаться на ранее не встречавшиеся заимствования."}
{"text": "So what we aim is to tackle some of these limitations in the task.\n", "text_Chinese": "因此，我们的目标是解决任务中的一些限制。", "text_Arabic": "لذا فإن ما نهدف إليه هو معالجة بعض هذه القيود في المهمة.", "text_French": "Notre objectif est donc de remédier à certaines de ces limites dans la tâche.", "text_Japanese": "したがって、私たちが目指しているのは、タスクにおけるこれらの制限のいくつかに対処することです。", "text_Russian": "Поэтому наша цель — устранить некоторые из этих ограничений в задаче."}
{"text": "So to begin we, to begin with, we created a new dataset.\n", "text_Chinese": "首先，我们创建了一个新的数据集。", "text_Arabic": "لذا، في البداية، قمنا بإنشاء مجموعة بيانات جديدة.", "text_French": "Donc, pour commencer, nous avons créé un nouvel ensemble de données.", "text_Japanese": "そこでまず、新しいデータセットを作成しました。", "text_Russian": "Итак, для начала мы создали новый набор данных."}
{"text": "Ah the aim at a new dataset that was annotated with lexical borrowings and the aim was to create a test set that was as difficult as possible.\n", "text_Chinese": "啊，目标是一个用词汇借用注释的新数据集，目的是创建一个尽可能困难的测试集。", "text_Arabic": "آه كان الهدف هو مجموعة بيانات جديدة تم شرحها باستخدام استعارات معجمية وكان الهدف هو إنشاء مجموعة اختبار صعبة قدر الإمكان.", "text_French": "Ah le but était d'avoir un nouvel ensemble de données annoté avec des emprunts lexicaux et le but était de créer un ensemble de tests aussi difficile que possible.", "text_Japanese": "ああ、語彙借用で注釈が付けられた新しいデータセットが目的であり、可能な限り難しいテスト セットを作成することが目的でした。", "text_Russian": "Ах, цель заключалась в создании нового набора данных, который был бы снабжен лексическими заимствованиями, и целью было создать как можно более сложный тестовый набор."}
{"text": "So there would be minimal overlap in words and topics between the training set and test set.\n", "text_Chinese": "因此，训练集和测试集之间的单词和主题重叠最小。", "text_Arabic": "لذلك سيكون هناك الحد الأدنى من التداخل في الكلمات والموضوعات بين مجموعة التدريب ومجموعة الاختبار.", "text_French": "Il y aurait donc un chevauchement minimal de mots et de sujets entre l'ensemble de formation et l'ensemble de test.", "text_Japanese": "したがって、トレーニング セットとテスト セットの間で単語やトピックの重複は最小限になります。", "text_Russian": "Таким образом, между обучающим и тестовым наборами будет минимальное совпадение слов и тем."}
{"text": "And as a result, well, the test set comes from sources and dates that we're not seeing in the training set.\n", "text_Chinese": "因此，测试集来自我们在训练集中看不到的来源和日期。", "text_Arabic": "ونتيجة لذلك، حسنًا، تأتي مجموعة الاختبار من مصادر وتواريخ لا نراها في مجموعة التدريب.", "text_French": "Et par conséquent, l'ensemble de tests provient de sources et de dates que nous ne voyons pas dans l'ensemble de formation.", "text_Japanese": "その結果、テスト セットは、トレーニング セットでは見られないソースと日付から取得されています。", "text_Russian": "И в результате тестовый набор формируется из источников и дат, которых мы не видим в обучающем наборе."}
{"text": "Here you can see that there's no overlap in the in the time.\n", "text_Chinese": "在这里您可以看到时间没有重叠。", "text_Arabic": "هنا يمكنك أن ترى أنه لا يوجد تداخل في الوقت.", "text_French": "Ici, vous pouvez voir qu'il n'y a pas de chevauchement dans le temps.", "text_Japanese": "ここでは、時間に重複がないことがわかります。", "text_Russian": "Здесь вы можете видеть, что во времени нет совпадений."}
{"text": "It's also, the test set is also very borrowing-dense.\n", "text_Chinese": "而且，测试集的借用也非常密集。", "text_Arabic": "إنها أيضًا مجموعة الاختبار كثيفة الاقتراض أيضًا.", "text_French": "C'est aussi que l'ensemble de tests est également très dense en emprunts.", "text_Japanese": "また、テスト セットには借用が非常に多くあります。", "text_Russian": "Кроме того, набор тестов также очень насыщен заимствованиями."}
{"text": "Just to give you some numbers, if the training set contains six borrowings per each thousand tokens, the test set contained twenty borrowings per each thousand tokens.\n", "text_Chinese": "只是为了给您一些数字，如果训练集每千个令牌包含 6 个借用，则测试集每千个令牌包含 20 个借用。", "text_Arabic": "فقط لإعطائك بعض الأرقام، إذا كانت مجموعة التدريب تحتوي على ستة استعارات لكل ألف رمز، فإن مجموعة الاختبار تحتوي على عشرين استعارة لكل ألف رمز.", "text_French": "Juste pour vous donner quelques chiffres, si l'ensemble de formation contient six emprunts pour mille jetons, l'ensemble de test contenait vingt emprunts pour mille jetons.", "text_Japanese": "数字をいくつか挙げると、トレーニング セットに 1,000 トークンごとに 6 つの借入が含まれる場合、テスト セットには 1,000 トークンごとに 20 の借入が含まれます。", "text_Russian": "Просто чтобы дать вам некоторые цифры: если обучающий набор содержит шесть заимствований на каждую тысячу токенов, тестовый набор содержит двадцать заимствований на каждую тысячу токенов."}
{"text": "The test set contained as many out of vocabulary words as possible.\n", "text_Chinese": "测试集包含尽可能多的词汇表之外的单词。", "text_Arabic": "تحتوي مجموعة الاختبار على أكبر عدد ممكن من الكلمات من المفردات.", "text_French": "L'ensemble de tests contenait autant de mots hors vocabulaire que possible.", "text_Japanese": "テストセットには、語彙外の単語ができるだけ多く含まれていました。", "text_Russian": "Тестовый набор содержал как можно больше слов из словаря."}
{"text": "In fact, ninety two percent of the borrowings in the test set are OOV.\n", "text_Chinese": "事实上，测试集中 92% 的借用都是 OOV。", "text_Arabic": "في الواقع، اثنان وتسعون بالمائة من الاقتراضات في مجموعة الاختبار هي قيمة إجمالية (OOV).", "text_French": "En fait, quatre-vingt-douze pour cent des emprunts de l’ensemble de test sont des OOV.", "text_Japanese": "実際、テスト セット内の借入の 92% は OOV です。", "text_Russian": "Фактически, девяносто два процента заимствований в тестовом наборе являются OOV."}
{"text": "So, they were not seen during training.\n", "text_Chinese": "所以，在训练的时候，他们并没有被看到。", "text_Arabic": "لذلك، لم يتم رؤيتهم أثناء التدريب.", "text_French": "Ils n’ont donc pas été vus pendant l’entraînement.", "text_Japanese": "そのため、訓練中に彼らの姿は見られませんでした。", "text_Russian": "Так что во время тренировки их не видели."}
{"text": "And the corpus consisted basically of a collection of texts that came from different sources of Spanish newspapers.\n", "text_Chinese": "该语料库基本上由来自西班牙报纸不同来源的文本集合组成。", "text_Arabic": "وتتكون المجموعة بشكل أساسي من مجموعة من النصوص التي جاءت من مصادر مختلفة للصحف الإسبانية.", "text_French": "Et le corpus consistait essentiellement en une collection de textes provenant de différentes sources de journaux espagnols.", "text_Japanese": "そして、コーパスは基本的に、スペインの新聞のさまざまな情報源から得たテキストのコレクションで構成されていました。", "text_Russian": "А корпус состоял в основном из набора текстов, взятых из разных источников испанских газет."}
{"text": "And ah it was annotated by hand ah using two tags.\n", "text_Chinese": "啊，它是使用两个标签手工注释的。", "text_Arabic": "وقد تم شرحه يدويًا باستخدام علامتين.", "text_French": "Et ah, c'était annoté à la main, ah, en utilisant deux balises.", "text_Japanese": "そして、2 つのタグを使用して手作業で注釈が付けられました。", "text_Russian": "И ах, это было аннотировано вручную с использованием двух тегов."}
{"text": "One for English lexical borrowings which is the majority of lexical borrowings in Spanish, and then the label other for borrowings from other languages.\n", "text_Chinese": "一个用于英语词汇借用，这是西班牙语词汇借用的大部分，然后另一个标签用于从其他语言借用。", "text_Arabic": "واحد للاستعارات المعجمية الإنجليزية التي تمثل غالبية الاقتراضات المعجمية باللغة الإسبانية، ثم تسمية أخرى للاستعارات من اللغات الأخرى.", "text_French": "Un pour les emprunts lexicaux anglais qui représentent la majorité des emprunts lexicaux en espagnol, puis le label autre pour les emprunts à d'autres langues.", "text_Japanese": "1 つはスペイン語の語彙借用の大部分である英語の語彙借用用で、次に他の言語からの借用用のラベルです。", "text_Russian": "Один для английских лексических заимствований, которые составляют большинство лексических заимствований в испанском языке, а затем ярлык для заимствований из других языков."}
{"text": "We use CONLL formats and we used BIO encoding so that we could encode ah single token borrowings such as app or multi token borrowings such as machine learning.\n", "text_Chinese": "我们使用 CONLL 格式并使用 BIO 编码，以便我们可以对单令牌借用（例如应用程序）或多令牌借用（例如机器学习）进行编码。", "text_Arabic": "نحن نستخدم تنسيقات CONLL واستخدمنا تشفير BIO حتى نتمكن من تشفير قروض الرمز المميز الفردي مثل التطبيق أو قروض الرموز المميزة المتعددة مثل التعلم الآلي.", "text_French": "Nous utilisons les formats CONLL et nous avons utilisé l'encodage BIO afin de pouvoir encoder des emprunts à jeton unique tels que des applications ou des emprunts à jetons multiples tels que l'apprentissage automatique.", "text_Japanese": "私たちは CONLL 形式を使用し、BIO エンコーディングを使用して、アプリなどの単一トークンの借用、または機械学習などの複数のトークンの借用をエンコードできるようにしました。", "text_Russian": "Мы используем форматы CONLL и кодировку BIO, чтобы можно было кодировать заимствования отдельных токенов, например приложения, или заимствования нескольких токенов, например машинное обучение."}
{"text": "These are the numbers of the corpus.\n", "text_Chinese": "这些是语料库的编号。", "text_Arabic": "هذه هي أرقام الجسم.", "text_French": "Ce sont les numéros du corpus.", "text_Japanese": "これらはコーパスの番号です。", "text_Russian": "Это номера корпуса."}
{"text": "As you can see, it amounts to roughly three hundred seventy thousand tokens.\n", "text_Chinese": "正如你所看到的，大约有三十七万代币。", "text_Arabic": "وكما ترون، فإن المبلغ يصل إلى ما يقرب من ثلاثمائة وسبعين ألف رمز.", "text_French": "Comme vous pouvez le constater, cela représente environ trois cent soixante-dix mille jetons.", "text_Japanese": "ご覧のとおり、約 37 万トークンに相当します。", "text_Russian": "Как видите, это примерно триста семьдесят тысяч токенов."}
{"text": "And here you have the number of spans that were labeled as English and the spans that were labeled as other borrowings and how many of them were unique.\n", "text_Chinese": "这里有被标记为英语的跨度的数量和被标记为其他借用的跨度的数量，以及其中有多少是独特的。", "text_Arabic": "وهنا لديك عدد الامتدادات التي تم تصنيفها على أنها باللغة الإنجليزية والامتدادات التي تم تصنيفها على أنها مستعارة أخرى وعدد الامتدادات الفريدة منها.", "text_French": "Et ici vous avez le nombre de travées qui ont été étiquetées comme anglaises et les travées qui ont été étiquetées comme autres emprunts et combien d'entre elles étaient uniques.", "text_Japanese": "ここには、英語としてラベル付けされたスパンの数と、その他の借用としてラベル付けされたスパンの数と、そのうちの一意のスパンの数が表示されます。", "text_Russian": "А вот количество спанов, которые были помечены как английские, и спанов, которые были помечены как другие заимствования, и сколько из них были уникальными."}
{"text": "And here you have a couple of examples of the of the set of the dataset.\n", "text_Chinese": "这里有几个数据集的示例。", "text_Arabic": "وهنا لديك بضعة أمثلة لمجموعة البيانات.", "text_French": "Et ici vous avez quelques exemples de l’ensemble de l’ensemble de données.", "text_Japanese": "ここには、データセットの例がいくつかあります。", "text_Russian": "И здесь у вас есть пара примеров набора данных."}
{"text": "As you can see for instance here, we have ah in the first example, we have the borrowing batch cooking which is a multi word borrowing.\n", "text_Chinese": "正如你在这里看到的，我们在第一个例子中有啊，我们有借用批量烹饪，这是一个多词借用。", "text_Arabic": "كما ترون على سبيل المثال هنا، لدينا آه في المثال الأول، لدينا عملية الاقتراض المجمعة وهي عبارة عن استعارة متعددة الكلمات.", "text_French": "Comme vous pouvez le voir par exemple ici, nous avons ah dans le premier exemple, nous avons l'emprunt batch cooking qui est un emprunt de plusieurs mots.", "text_Japanese": "たとえばここでわかるように、最初の例には、複数の単語の借用である借用バッチ クッキングがあります。", "text_Russian": "Как вы можете видеть, например, здесь, в первом примере у нас есть заимствование пакетного приготовления, которое представляет собой заимствование из нескольких слов."}
{"text": "And we have annotated it using the BIO um encode.\n", "text_Chinese": "我们使用 BIO um 编码对其进行了注释。", "text_Arabic": "وقد قمنا بتعليقه باستخدام تشفير BIO um.", "text_French": "Et nous l'avons annoté en utilisant l'encodage BIO um.", "text_Japanese": "そして、BIO um エンコードを使用して注釈を付けました。", "text_Russian": "И мы аннотировали это, используя код BIO um."}
{"text": "So the BIO was used for words in Spanish so not for words that were not borrowed.\n", "text_Chinese": "因此，BIO 用于西班牙语单词，而不是非借用单词。", "text_Arabic": "لذلك تم استخدام BIO للكلمات باللغة الإسبانية، وليس للكلمات التي لم يتم استعارتها.", "text_French": "Ainsi, le BIO a été utilisé pour des mots en espagnol, donc pas pour des mots qui n'étaient pas empruntés.", "text_Japanese": "つまり、BIO はスペイン語の単語に使用され、借用されていない単語には使用されませんでした。", "text_Russian": "Таким образом, BIO использовался для слов на испанском языке, а не для слов, которые не были заимствованы."}
{"text": "And here in this second example, you have benching and crash which are also labeled as borrowings from English.\n", "text_Chinese": "在第二个例子中，你有 benching 和 crash ，它们也被标记为从英语借用的。", "text_Arabic": "وهنا في هذا المثال الثاني، لديك مقاعد البدلاء والاصطدام والتي تم تصنيفها أيضًا على أنها مستعارة من اللغة الإنجليزية.", "text_French": "Et ici, dans ce deuxième exemple, vous avez benching et crash qui sont également qualifiés d'emprunts à l'anglais.", "text_Japanese": "そして、この 2 番目の例には、英語からの借用としてラベル付けされているベンチングとクラッシュがあります。", "text_Russian": "И здесь, во втором примере, есть жим и крэш, которые также помечены как заимствования из английского языка."}
{"text": "So, once we had the dataset, we explored several models for the task of extracting and detecting these lexical borrowings.\n", "text_Chinese": "因此，一旦我们有了数据集，我们就探索了几种用于提取和检测这些词汇借用任务的模型。", "text_Arabic": "لذلك، بمجرد حصولنا على مجموعة البيانات، استكشفنا عدة نماذج لمهمة استخراج وكشف هذه الاقتراضات المعجمية.", "text_French": "Ainsi, une fois l’ensemble de données en possession, nous avons exploré plusieurs modèles pour extraire et détecter ces emprunts lexicaux.", "text_Japanese": "そこで、データセットを取得したら、これらの語彙借用を抽出して検出するタスク用にいくつかのモデルを検討しました。", "text_Russian": "Итак, получив набор данных, мы исследовали несколько моделей для извлечения и обнаружения этих лексических заимствований."}
{"text": "The first one that we tried was the conditional random field model.\n", "text_Chinese": "我们尝试的第一个模型是条件随机场模型。", "text_Arabic": "أول نموذج جربناه كان نموذج المجال العشوائي المشروط.", "text_French": "Le premier que nous avons essayé était le modèle de champ aléatoire conditionnel.", "text_Japanese": "私たちが最初に試したのは、条件付きランダム フィールド モデルでした。", "text_Russian": "Первой, которую мы опробовали, была модель условного случайного поля."}
{"text": "Ah, this was the model that had been used on previous work.\n", "text_Chinese": "啊，这是之前作品中用过的模型。", "text_Arabic": "اه، هذا هو النموذج الذي تم استخدامه في العمل السابق.", "text_French": "Ah, c'était le modèle qui avait été utilisé lors de travaux précédents.", "text_Japanese": "あ、これは前作で使われていたモデルですね。", "text_Russian": "А, это была модель, которая использовалась в предыдущей работе."}
{"text": "And we used the same handcrafted features from that from those from that work.\n", "text_Chinese": "我们使用了与该作品相同的手工制作功能。", "text_Arabic": "واستخدمنا نفس الميزات المصنوعة يدويًا من تلك الموجودة في هذا العمل.", "text_French": "Et nous avons utilisé les mêmes fonctionnalités artisanales que celles de ce travail.", "text_Japanese": "そして、その作品のものと同じ手作りの特徴を使用しました。", "text_Russian": "И мы использовали те же элементы ручной работы, что и в этой работе."}
{"text": "As you can see, these are the features.\n", "text_Chinese": "如您所见，这些是功能。", "text_Arabic": "كما ترون، هذه هي الميزات.", "text_French": "Comme vous pouvez le voir, ce sont les fonctionnalités.", "text_Japanese": "ご覧のとおり、これらが特徴です。", "text_Russian": "Как видите, это особенности."}
{"text": "These are binary features such as the word or the token in upper case?\n", "text_Chinese": "这些是二进制特征，例如大写的单词或标记？", "text_Arabic": "هل هذه ميزات ثنائية مثل الكلمة أو الرمز المميز بالأحرف الكبيرة؟", "text_French": "Ce sont des fonctionnalités binaires comme le mot ou le jeton en majuscule ?", "text_Japanese": "これらは単語や大文字のトークンなどのバイナリ特徴です。", "text_Russian": "Это двоичные функции, такие как слово или токен в верхнем регистре?"}
{"text": "Is it title titlecase?\n", "text_Chinese": "是标题标题吗？", "text_Arabic": "هل هو عنوان العنوان؟", "text_French": "Est-ce que c'est le titre du titre ?", "text_Japanese": "タイトルタイトルケースですか？", "text_Russian": "Это титульный регистр?"}
{"text": "Is it a quotation mark?\n", "text_Chinese": "是引号吗？", "text_Arabic": "هل هي علامة اقتباس؟", "text_French": "Est-ce un guillemet ?", "text_Japanese": "引用符ですか?", "text_Russian": "Это кавычка?"}
{"text": "Things like that, which are the type of features that one would expect in a named entity recognition task.\n", "text_Chinese": "类似的事情，这是人们在命名实体识别任务中所期望的特征类型。", "text_Arabic": "أشياء من هذا القبيل، وهي نوع الميزات التي يتوقعها المرء في مهمة التعرف على الكيانات المسماة.", "text_French": "Des choses comme ça, qui sont le type de fonctionnalités auxquelles on peut s'attendre dans une tâche de reconnaissance d'entité nommée.", "text_Japanese": "このようなものは、固有表現認識タスクで期待されるタイプの機能です。", "text_Russian": "Подобные вещи являются типом функций, которые можно ожидать от задачи распознавания именованного объекта."}
{"text": "These are the results that we got.\n", "text_Chinese": "这些是我们得到的结果。", "text_Arabic": "هذه هي النتائج التي حصلنا عليها.", "text_French": "Ce sont les résultats que nous avons obtenus.", "text_Japanese": "これらが得られた結果です。", "text_Russian": "Вот какие результаты мы получили."}
{"text": "We obtain fifty five F1 score using the the CRF model with handcrafted features.\n", "text_Chinese": "我们使用带有手工特征的 CRF 模型获得了 55 个 F1 分数。", "text_Arabic": "حصلنا على خمسة وخمسين درجة F1 باستخدام نموذج CRF مع الميزات المصنوعة يدويًا.", "text_French": "Nous obtenons cinquante-cinq scores F1 en utilisant le modèle CRF avec des fonctionnalités artisanales.", "text_Japanese": "手作りの特徴を備えた CRF モデルを使用して、55 の F1 スコアを取得しました。", "text_Russian": "Мы получили пятьдесят пять очков Формулы-1, используя модель CRF с функциями, созданными вручную."}
{"text": "Which is a huge different difference um compared to the reported F1 score of eighty six, which was the result obtained with the same CRF model, same features but on a different dataset also for Spanish lexical borrowing detection.\n", "text_Chinese": "与报告的 86 分 F1 分数相比，这是一个巨大的差异，这是使用相同的 CRF 模型、相同的特征但在不同的数据集上获得的结果，也用于西班牙语词汇借用检测。", "text_Arabic": "وهو فرق كبير ومختلف مقارنةً بدرجة F1 المُبلغ عنها والتي تبلغ ستة وثمانين، والتي كانت النتيجة التي تم الحصول عليها باستخدام نفس نموذج CRF، ونفس الميزات ولكن على مجموعة بيانات مختلفة أيضًا لاكتشاف الاقتراض المعجمي الإسباني.", "text_French": "Ce qui représente une énorme différence par rapport au score F1 rapporté de quatre-vingt-six, qui était le résultat obtenu avec le même modèle CRF, les mêmes caractéristiques mais sur un ensemble de données différent également pour la détection des emprunts lexicaux espagnols.", "text_Japanese": "これは、報告された F1 スコア 86 と比較すると、大きな違いです。これは、同じ CRF モデル、同じ特徴を使用して、スペイン語の語彙借用検出についても異なるデータセットで得られた結果です。", "text_Russian": "Это огромная разница по сравнению с заявленным показателем F1 в восемьдесят шесть, который был результатом, полученным с помощью той же модели CRF, с теми же функциями, но с другим набором данных, а также для обнаружения испанских лексических заимствований."}
{"text": "So, this proves that the dataset that we created is more difficult and that we needed to explore more sophisticated models for these tasks.\n", "text_Chinese": "因此，这证明我们创建的数据集更加困难，我们需要为这些任务探索更复杂的模型。", "text_Arabic": "لذا، فهذا يثبت أن مجموعة البيانات التي أنشأناها أكثر صعوبة وأننا بحاجة إلى استكشاف نماذج أكثر تعقيدًا لهذه المهام.", "text_French": "Cela prouve donc que l’ensemble de données que nous avons créé est plus difficile et que nous devions explorer des modèles plus sophistiqués pour ces tâches.", "text_Japanese": "したがって、これは、私たちが作成したデータセットがより困難であり、これらのタスクにはより洗練されたモデルを探索する必要があることを証明しています。", "text_Russian": "Итак, это доказывает, что созданный нами набор данных более сложен и что нам необходимо изучить более сложные модели для этих задач."}
{"text": "So, we tested two transformer based models.\n", "text_Chinese": "因此，我们测试了两种基于变压器的模型。", "text_Arabic": "لذلك، قمنا باختبار نموذجين يعتمدان على المحولات.", "text_French": "Nous avons donc testé deux modèles basés sur un transformateur.", "text_Japanese": "そこで、2 つのトランスベースのモデルをテストしました。", "text_Russian": "Итак, мы протестировали две модели на базе трансформатора."}
{"text": "We used BETO which is a monolingual BERT model trained for Spanish and also multilingual BERT.\n", "text_Chinese": "我们使用 BETO，它是一个针对西班牙语训练的单语言 BERT 模型，也是多语言 BERT 模型。", "text_Arabic": "استخدمنا BETO وهو نموذج BERT أحادي اللغة تم تدريبه للغة الإسبانية وأيضًا نموذج BERT متعدد اللغات.", "text_French": "Nous avons utilisé BETO qui est un modèle BERT monolingue formé pour l'espagnol et également un BERT multilingue.", "text_Japanese": "スペイン語用にトレーニングされた単言語 BERT モデルである BETO と、多言語 BERT を使用しました。", "text_Russian": "Мы использовали BETO, одноязычную модель BERT, подготовленную для испанского языка, а также многоязычную модель BERT."}
{"text": "Both models we use them through the transformers library by HuggingFace.\n", "text_Chinese": "我们通过 HuggingFace 的 Transformer 库使用这两个模型。", "text_Arabic": "كلا النموذجين نستخدمهما من خلال مكتبة المحولات بواسطة HuggingFace.", "text_French": "Nous utilisons les deux modèles via la bibliothèque de transformateurs de HuggingFace.", "text_Japanese": "どちらのモデルも、HuggingFace のトランスフォーマー ライブラリを通じて使用します。", "text_Russian": "Обе модели мы используем через библиотеку трансформеров HuggingFace."}
{"text": "These are the results that we got.\n", "text_Chinese": "这些是我们得到的结果。", "text_Arabic": "هذه هي النتائج التي حصلنا عليها.", "text_French": "Ce sont les résultats que nous avons obtenus.", "text_Japanese": "これらが得られた結果です。", "text_Russian": "Вот какие результаты мы получили."}
{"text": "As you can see, multilingual BERT performs better than BETO both on the development set and on the test set and across all metrics.\n", "text_Chinese": "正如您所看到的，多语言 BERT 在开发集、测试集以及所有指标上都比 BETO 表现更好。", "text_Arabic": "كما ترون، فإن أداء BERT متعدد اللغات أفضل من BETO سواء في مجموعة التطوير أو في مجموعة الاختبار أو عبر جميع المقاييس.", "text_French": "Comme vous pouvez le constater, le BERT multilingue fonctionne mieux que BETO à la fois sur l'ensemble de développement et sur l'ensemble de test et sur toutes les métriques.", "text_Japanese": "ご覧のとおり、多言語 BERT は、開発セット、テスト セット、およびすべての指標において BETO よりも優れたパフォーマンスを示します。", "text_Russian": "Как видите, многоязычный BERT работает лучше, чем BETO, как на наборе разработки, так и на тестовом наборе, а также по всем показателям."}
{"text": "Just so we have ah an idea to compare, the CRF model obtained an eighty two.\n", "text_Chinese": "就这样我们有啊一个想法来比较，CRF模型得到了八十二。", "text_Arabic": "فقط لكي تكون لدينا فكرة للمقارنة، حصل نموذج CRF على اثنين وثمانين.", "text_French": "Juste pour avoir une idée de comparaison, le modèle CRF a obtenu un quatre-vingt-deux.", "text_Japanese": "比較するアイデアがあるので、CRF モデルは 82 を取得しました。", "text_Russian": "Просто чтобы у нас была идея для сравнения: модель CRF получила восемьдесят два."}
{"text": "The CRF model obtained a fifty five obtained a fifty five F1 score, whereas the multilingual BERT obtained eighty two, which is a big difference.\n", "text_Chinese": "CRF 模型获得了 55 分，F1 分数为 55 分，而多语言 BERT 获得了 82 分，这是一个很大的差异。", "text_Arabic": "حصل نموذج CRF على درجة خمسة وخمسين وحصل على خمسة وخمسين درجة F1، في حين حصل نموذج BERT متعدد اللغات على اثنين وثمانين، وهو فرق كبير.", "text_French": "Le modèle CRF a obtenu un score F1 de cinquante-cinq, tandis que le BERT multilingue a obtenu quatre-vingt-deux, ce qui représente une grande différence.", "text_Japanese": "CRF モデルは 55 の F1 スコアを獲得しましたが、多言語 BERT は 82 を獲得しました。これは大きな違いです。", "text_Russian": "Модель CRF получила пятьдесят пять баллов F1, тогда как многоязычная модель BERT получила восемьдесят два балла, что является большой разницей."}
{"text": "So, once that we had those results, we asked ourselves another question which is, could we find a BiLSTM-CRF model, feed it with different types of embeddings, embeddings that encode different types of linguistic information and perform outperform the results obtained by transformer based models?\n", "text_Chinese": "因此，一旦我们得到了这些结果，我们就问自己另一个问题，那就是，我们能否找到一个 BiLSTM-CRF 模型，为其提供不同类型的嵌入，这些嵌入编码不同类型的语言信息，并且性能优于 Transformer 获得的结果基于模型？", "text_Arabic": "لذا، بمجرد حصولنا على هذه النتائج، سألنا أنفسنا سؤالًا آخر وهو، هل يمكننا العثور على نموذج BiLSTM-CRF، وتغذيته بأنواع مختلفة من التضمينات، التضمينات التي تشفر أنواعًا مختلفة من المعلومات اللغوية وتتفوق في الأداء على النتائج التي تم الحصول عليها بواسطة المحول النماذج القائمة؟", "text_French": "Donc, une fois que nous avons eu ces résultats, nous nous sommes posés une autre question : pourrions-nous trouver un modèle BiLSTM-CRF, l'alimenter avec différents types d'intégrations, des intégrations qui codent différents types d'informations linguistiques et dont les performances surpassent les résultats obtenus par le transformateur. modèles basés sur ?", "text_Japanese": "そこで、これらの結果が得られた後、私たちは別の質問を自問しました。それは、BiLSTM-CRF モデルを見つけて、それにさまざまなタイプのエンベディングをフィードし、さまざまなタイプの言語情報をエンコードするエンベディングを実行して、transformer によって得られた結果を上回るパフォーマンスを発揮できるかどうかです。ベースのモデル?", "text_Russian": "Итак, как только мы получили эти результаты, мы задали себе еще один вопрос: можем ли мы найти модель BiLSTM-CRF, снабдить ее различными типами вложений, встраивания, которые кодируют разные типы лингвистической информации и по производительности превосходят результаты, полученные преобразователем. основанные модели?"}
{"text": "So in order to do so, we ran some preliminary experiments, we we run this by BiLSTM-CRF model using flare library.\n", "text_Chinese": "因此，为了做到这一点，我们进行了一些初步实验，我们使用 Flare 库通过 BiLSTM-CRF 模型运行此实验。", "text_Arabic": "ومن أجل القيام بذلك، أجرينا بعض التجارب الأولية، وقمنا بذلك باستخدام نموذج BiLSTM-CRF باستخدام مكتبة التوهج.", "text_French": "Donc, pour ce faire, nous avons mené quelques expériences préliminaires, nous les exécutons avec le modèle BiLSTM-CRF en utilisant la bibliothèque Flare.", "text_Japanese": "そのために、フレア ライブラリを使用して BiLSTM-CRF モデルによってこれを実行するいくつかの予備実験を実行しました。", "text_Russian": "Для этого мы провели несколько предварительных экспериментов, используя модель BiLSTM-CRF с использованием библиотеки Flare."}
{"text": "And we tried experimented with different type of embeddings like transformer-based but also fast-text, character embeddings, and so on.\n", "text_Chinese": "我们尝试了不同类型的嵌入，例如基于变压器的嵌入，以及快速文本、字符嵌入等。", "text_Arabic": "وقد حاولنا تجربة أنواع مختلفة من عمليات التضمين مثل المستندة إلى المحولات ولكن أيضًا النص السريع وتضمين الأحرف وما إلى ذلك.", "text_French": "Et nous avons essayé d'expérimenter différents types d'intégrations telles que les intégrations basées sur un transformateur mais aussi le texte rapide, les intégrations de caractères, etc.", "text_Japanese": "そして、トランスフォーマーベースだけでなく、高速テキスト、文字埋め込みなど、さまざまなタイプの埋め込みを試してみました。", "text_Russian": "И мы пробовали экспериментировать с различными типами встраивания, такими как преобразование, а также быстрый текст, встраивание символов и так далее."}
{"text": "What we found out was that transformer-based embeddings performed better than non contextualized embeddings, that the combination of English BERT and Spanish BETO embeddings outperform multilingual BERT embeddings.\n", "text_Chinese": "我们发现，基于 Transformer 的嵌入比非上下文化嵌入表现更好，英语 BERT 和西班牙语 BETO 嵌入的组合优于多语言 BERT 嵌入。", "text_Arabic": "ما اكتشفناه هو أن عمليات التضمين المستندة إلى المحولات كانت أفضل من عمليات التضمين غير السياقية، وأن الجمع بين عمليات التضمين الإنجليزية BERT والتضمين BETO الإسبانية يتفوق على عمليات التضمين BERT متعددة اللغات.", "text_French": "Ce que nous avons découvert, c'est que les intégrations basées sur des transformateurs fonctionnaient mieux que les intégrations non contextualisées, que la combinaison des intégrations BERT anglaises et espagnoles BETO surpassait les intégrations BERT multilingues.", "text_Japanese": "私たちが発見したのは、トランスフォーマーベースのエンベディングが非コンテキスト化エンベディングよりもパフォーマンスが優れていること、英語 BERT とスペイン語 BETO エンベディングの組み合わせが多言語 BERT エンベディングよりもパフォーマンスが高いことです。", "text_Russian": "Мы обнаружили, что встраивания на основе преобразователей работают лучше, чем неконтекстуализированные встраивания, что комбинация английских вложений BERT и испанских вложений BETO превосходит многоязычные встраивания BERT."}
{"text": "And that BPE embeddings produced better F1 and character embeddings produce better recall.\n", "text_Chinese": "BPE 嵌入产生了更好的 F1，字符嵌入产生了更好的召回率。", "text_Arabic": "وأن عمليات تضمين BPE أنتجت F1 أفضل وأن عمليات تضمين الأحرف تنتج استدعاء أفضل.", "text_French": "Et que les intégrations BPE ont produit une meilleure F1 et que les intégrations de personnages produisent un meilleur rappel.", "text_Japanese": "そして、BPE 埋め込みはより良い F1 を生成し、文字埋め込みはより良い再現率を生成します。", "text_Russian": "И что встраивания BPE обеспечивают лучший F1, а встраивания символов обеспечивают лучший отзыв."}
{"text": "With that in mind, these were the best performing results that we got.\n", "text_Chinese": "考虑到这一点，这些是我们获得的最佳表现结果。", "text_Arabic": "ومع أخذ ذلك في الاعتبار، كانت هذه أفضل النتائج التي حصلنا عليها من حيث الأداء.", "text_French": "Dans cet esprit, ce sont les résultats les plus performants que nous ayons obtenus.", "text_Japanese": "それを念頭に置くと、これらは私たちが得た最高のパフォーマンスの結果でした。", "text_Russian": "Учитывая это, это были лучшие результаты, которые мы получили."}
{"text": "Both models were BiLSTM-CRF models using flare.\n", "text_Chinese": "两个模型都是使用耀斑的 BiLSTM-CRF 模型。", "text_Arabic": "كلا النموذجين كانا من نماذج BiLSTM-CRF باستخدام التوهج.", "text_French": "Les deux modèles étaient des modèles BiLSTM-CRF utilisant une fusée éclairante.", "text_Japanese": "どちらのモデルもフレアを使用した BiLSTM-CRF モデルでした。", "text_Russian": "Обе модели представляли собой модели BiLSTM-CRF, использующие блики."}
{"text": "One was fed with BETO and BERT embeddings and BPE, and the other one BETO and BERT embeddings and BPE and also character embeddings.\n", "text_Chinese": "一个包含 BETO 和 BERT 嵌入以及 BPE，另一个包含 BETO 和 BERT 嵌入以及 BPE 和字符嵌入。", "text_Arabic": "تم تغذية أحدهما بتضمينات BETO وBERT وBPE، والآخر بتضمينات BETO وBERT وBPE وأيضًا بتضمينات الأحرف.", "text_French": "L'un était alimenté par les intégrations BETO et BERT et BPE, et l'autre par les intégrations BETO et BERT et BPE ainsi que par les intégrations de caractères.", "text_Japanese": "1 つは BETO および BERT エンベディングと BPE を供給され、もう 1 つは BETO および BERT エンベディングと BPE、さらに文字エンベディングを供給されました。", "text_Russian": "В один были вложены вложения BETO и BERT и BPE, а в другой — вложения BETO и BERT и BPE, а также вложения символов."}
{"text": "This last one was the one that produced the highest F1 score on the test set, although the highest score on the development set was obtained by the one without character embeddings.\n", "text_Chinese": "最后一个是在测试集上产生最高 F1 分数的一个，尽管开发集上的最高分数是由没有字符嵌入的那个获得的。", "text_Arabic": "كانت هذه الأخيرة هي التي أنتجت أعلى درجة F1 في مجموعة الاختبار، على الرغم من أن أعلى الدرجات في مجموعة التطوير تم الحصول عليها من خلال المجموعة التي لا تحتوي على تضمينات شخصية.", "text_French": "Ce dernier était celui qui a produit le score F1 le plus élevé sur l'ensemble de test, bien que le score le plus élevé sur l'ensemble de développement ait été obtenu par celui sans intégration de personnages.", "text_Japanese": "この最後のものは、テスト セットで最高の F1 スコアを生成したものですが、開発セットでは文字埋め込みのないセットで最高のスコアが得られました。", "text_Russian": "Этот последний из них дал наивысший балл F1 в тестовом наборе, хотя самый высокий балл в наборе разработки был получен без встраивания символов."}
{"text": "Just ah to bear in mind that the best result that we got with multilingual BERT obtained an F1 of seventy six on the development set and eighty two on the test set.\n", "text_Chinese": "请记住，我们使用多语言 BERT 获得的最佳结果在开发集上获得了 76 的 F1，在测试集上获得了 82。", "text_Arabic": "فقط آه، لنضع في اعتبارنا أن أفضل نتيجة حصلنا عليها مع BERT متعدد اللغات حصلت على F1 من ستة وسبعين في مجموعة التطوير واثنين وثمانين في مجموعة الاختبار.", "text_French": "Juste ah, il faut garder à l'esprit que le meilleur résultat que nous avons obtenu avec le BERT multilingue a obtenu un F1 de soixante-seize sur l'ensemble de développement et de quatre-vingt-deux sur l'ensemble de test.", "text_Japanese": "多言語 BERT で得られた最高の結果は、開発セットで 76 の F1、テスト セットで 82 という F1 を獲得したことを覚えておいてください。", "text_Russian": "Просто надо иметь в виду, что лучший результат, который мы получили с помощью многоязычного BERT, составил F1, равный семидесяти шести на тестовом наборе и восьмидесяти двум на тестовом наборе."}
{"text": "So this is an improvement compared to those results.\n", "text_Chinese": "因此，与这些结果相比，这是一个改进。", "text_Arabic": "لذلك يعد هذا تحسنا مقارنة بتلك النتائج.", "text_French": "C’est donc une amélioration par rapport à ces résultats.", "text_Japanese": "したがって、これらの結果と比較すると、これは改善です。", "text_Russian": "Так что это улучшение по сравнению с теми результатами."}
{"text": "Finally, we asked ourselves another question which was can lexical borrowing detection be framed as transfer learning from language identification in code switching?\n", "text_Chinese": "最后，我们问自己另一个问题，即词汇借用检测是否可以被定义为代码转换中语言识别的迁移学习？", "text_Arabic": "أخيرًا، سألنا أنفسنا سؤالًا آخر وهو هل يمكن تأطير اكتشاف الاقتراض المعجمي على أنه نقل التعلم من تحديد اللغة في تبديل التعليمات البرمجية؟", "text_French": "Enfin, nous nous sommes posés une autre question : la détection des emprunts lexicaux peut-elle être considérée comme un apprentissage par transfert à partir de l'identification de la langue dans le changement de code ?", "text_Japanese": "最後に、私たちはもう 1 つの質問を自問しました。それは、字句借用の検出を、コード スイッチングにおける言語識別からの転移学習として組み立てることができるかということでした。", "text_Russian": "Наконец, мы задали себе еще один вопрос: можно ли выявление лексических заимствований рассматривать как передачу обучения от идентификации языка при переключении кода?"}
{"text": "So, we run the same BiLSTM-CRF model that we had run using flare, but instead of using these unadapted transformer-based BETO and BERT embeddings, we used code switch embeddings.\n", "text_Chinese": "因此，我们运行与使用 Flare 运行相同的 BiLSTM-CRF 模型，但我们没有使用这些未经调整的基于 Transformer 的 BETO 和 BERT 嵌入，而是使用代码切换嵌入。", "text_Arabic": "لذلك، قمنا بتشغيل نفس نموذج BiLSTM-CRF الذي قمنا بتشغيله باستخدام التوهج، ولكن بدلاً من استخدام تضمينات BETO وBERT غير المعدلة القائمة على المحولات، استخدمنا تضمينات تبديل التعليمات البرمجية.", "text_French": "Nous exécutons donc le même modèle BiLSTM-CRF que nous avions exécuté avec Flare, mais au lieu d'utiliser ces intégrations BETO et BERT basées sur des transformateurs inadaptées, nous avons utilisé des intégrations de commutateurs de code.", "text_Japanese": "そこで、フレアを使用して実行したのと同じ BiLSTM-CRF モデルを実行しますが、これらの未適応のトランスベースの BETO および BERT 埋め込みを使用する代わりに、コード スイッチ 埋め込みを使用しました。", "text_Russian": "Итак, мы запускаем ту же модель BiLSTM-CRF, которую мы запускали с использованием Flare, но вместо использования этих неадаптированных вложений BETO и BERT на основе трансформатора мы использовали встраивания кодового переключения."}
{"text": "What are code switch embeddings?\n", "text_Chinese": "什么是代码开关嵌入？", "text_Arabic": "ما هي تضمينات تبديل التعليمات البرمجية؟", "text_French": "Que sont les intégrations de commutateurs de code ?", "text_Japanese": "コードスイッチ埋め込みとは何ですか?", "text_Russian": "Что такое встраивания кодовых переключателей?"}
{"text": "Well these are um embeddings that are have been fine tuned transformer-based embeddings that have been pretrained for language identification on the Spanish English section of the LinCE code switching dataset.\n", "text_Chinese": "嗯，这些嵌入是经过微调的基于变压器的嵌入，已针对 LinCE 代码交换数据集的西班牙语英语部分的语言识别进行了预训练。", "text_Arabic": "حسنًا، هذه هي التضمينات التي تم ضبطها بشكل دقيق على التضمينات المستندة إلى المحولات والتي تم تدريبها مسبقًا لتحديد اللغة في قسم اللغة الإنجليزية الإسبانية في مجموعة بيانات تبديل كود LinCE.", "text_French": "Eh bien, ce sont des intégrations qui ont été affinées, basées sur un transformateur, qui ont été pré-entraînées pour l'identification de la langue dans la section anglais espagnol de l'ensemble de données de commutation de code LinCE.", "text_Japanese": "これらは、LinCE コード スイッチング データセットのスペイン語英語セクションで言語識別用に事前トレーニングされた、微調整されたトランスフォーマー ベースの埋め込みです。", "text_Russian": "Ну, это встраивания, которые представляют собой точно настроенные встраивания на основе преобразователей, которые были предварительно обучены для идентификации языка в испанско-английском разделе набора данных переключения кода LinCE."}
{"text": "LinCE is a dataset on code switching that has a section on Spanish English, Spanish English code switching.\n", "text_Chinese": "LinCE 是一个关于语码转换的数据集，其中有一个关于西班牙语英语、西班牙语英语语码转换的部分。", "text_Arabic": "LinCE عبارة عن مجموعة بيانات حول تبديل التعليمات البرمجية والتي تحتوي على قسم حول تبديل التعليمات البرمجية باللغة الإنجليزية الإسبانية والإنجليزية الإسبانية.", "text_French": "LinCE est un ensemble de données sur la commutation de code qui comporte une section sur la commutation de code espagnol-anglais et espagnol-anglais.", "text_Japanese": "LinCE はコード スイッチングに関するデータセットで、スペイン語英語、スペイン語英語コード スイッチングに関するセクションがあります。", "text_Russian": "LinCE — это набор данных по переключению кодов, в котором есть раздел, посвященный испанскому английскому и переключению кодов на испанском английском языке."}
{"text": "So we fed our BiLSTM-CRF with code switch embeddings and optionally character embeddings, BPE embeddings and so on.\n", "text_Chinese": "因此，我们为 BiLSTM-CRF 提供了代码开关嵌入和可选的字符嵌入、BPE 嵌入等。", "text_Arabic": "لذلك قمنا بتغذية BiLSTM-CRF الخاص بنا بتضمينات تبديل التعليمات البرمجية وتضمينات الأحرف بشكل اختياري، وتضمينات BPE وما إلى ذلك.", "text_French": "Nous avons donc alimenté notre BiLSTM-CRF avec des intégrations de commutateurs de code et éventuellement des intégrations de caractères, des intégrations BPE, etc.", "text_Japanese": "そこで、BiLSTM-CRF にコード スイッチ エンベ​​ディングと、オプションで文字エンベディング、BPE エンベディングなどを供給しました。", "text_Russian": "Поэтому мы снабдили наш BiLSTM-CRF встраиванием кодового переключателя и, при необходимости, встраиванием символов, встраиванием BPE и так далее."}
{"text": "The best result that we got was eighty four point twenty two, which is the highest across all the models that we tried on the test set.\n", "text_Chinese": "我们得到的最好结果是八十四点二十二分，这是我们在测试集上尝试的所有模型中最高的。", "text_Arabic": "أفضل نتيجة حصلنا عليها كانت 84.22، وهي الأعلى بين جميع النماذج التي جربناها في مجموعة الاختبار.", "text_French": "Le meilleur résultat que nous avons obtenu était de quatre-vingt-quatre virgule vingt-deux, ce qui est le plus élevé de tous les modèles que nous avons essayés sur l'ensemble de test.", "text_Japanese": "得られた最良の結果は 84.22 で、これはテスト セットで試したすべてのモデルの中で最高でした。", "text_Russian": "Лучший результат, который мы получили, — восемьдесят четыре целых двадцать два, что является самым высоким показателем среди всех моделей, которые мы опробовали на тестовом наборе."}
{"text": "Although the best result F1 score that we got on the development set, which was seventy nine, was lower than the best result obtained by the BiLSTM-CRF fed with unadapted embeddings.\n", "text_Chinese": "尽管我们在开发集上获得的最佳结果 F1 分数为 79，但低于使用未适应嵌入的 BiLSTM-CRF 获得的最佳结果。", "text_Arabic": "على الرغم من أن أفضل نتيجة في الفورمولا 1 حصلنا عليها في مجموعة التطوير، والتي كانت تسعة وسبعين، كانت أقل من أفضل نتيجة حصل عليها BiLSTM-CRF التي تم تغذيتها بتضمينات غير معدلة.", "text_French": "Bien que le meilleur résultat F1 que nous ayons obtenu sur l'ensemble de développement, qui était de soixante-dix-neuf, était inférieur au meilleur résultat obtenu par le BiLSTM-CRF alimenté avec des intégrations inadaptées.", "text_Japanese": "開発セットで得られた最良の結果である F1 スコアは 79 でしたが、適応されていない埋め込みを使用した BiLSTM-CRF によって得られた最良の結果よりも低かったです。", "text_Russian": "Хотя лучший результат F1, который мы получили на наборе разработки, составивший семьдесят девять, был ниже, чем лучший результат, полученный BiLSTM-CRF с неадаптированными вложениями."}
{"text": "So, some conclusions from our work.\n", "text_Chinese": "所以，我们的工作得出了一些结论。", "text_Arabic": "لذلك، بعض الاستنتاجات من عملنا.", "text_French": "Voilà donc quelques conclusions de notre travail.", "text_Japanese": "さて、私たちの研究から得られたいくつかの結論です。", "text_Russian": "Итак, некоторые выводы из нашей работы."}
{"text": "We have ah we have produced a new dataset of Spanish newswire that is annotated with unassimilated lexical borrowings.\n", "text_Chinese": "我们已经制作了一个新的西班牙新闻专线数据集，该数据集用未同化的词汇借用进行了注释。", "text_Arabic": "لقد أنتجنا مجموعة بيانات جديدة من وكالات الأنباء الإسبانية الموضحة باستعارات معجمية غير مستوعبة.", "text_French": "Nous avons produit un nouvel ensemble de données de fil de presse espagnol annoté avec des emprunts lexicaux non assimilés.", "text_Japanese": "ああ、同化されていない語彙借用で注釈が付けられたスペイン語ニュースワイヤーの新しいデータセットを作成しました。", "text_Russian": "У нас есть новый набор данных испанской ленты новостей, который снабжен неассимилированными лексическими заимствованиями."}
{"text": "This dataset is more borrowing dense and OOV-rich than previous resources.\n", "text_Chinese": "该数据集比以前的资源更加借用密集且 OOV 丰富。", "text_Arabic": "تعد مجموعة البيانات هذه أكثر كثافة في الاقتراض وغنية بـ OOV من الموارد السابقة.", "text_French": "Cet ensemble de données est plus dense en emprunts et plus riche en OOV que les ressources précédentes.", "text_Japanese": "このデータセットは、以前のリソースよりも借用密度が高く、OOV が豊富です。", "text_Russian": "Этот набор данных более плотный и богат OOV, чем предыдущие ресурсы."}
{"text": "We have explored four types of models for lexical borrowing detection.\n", "text_Chinese": "我们探索了四种类型的词汇借用检测模型。", "text_Arabic": "لقد استكشفنا أربعة أنواع من النماذج للكشف عن الاقتراض المعجمي.", "text_French": "Nous avons exploré quatre types de modèles pour la détection des emprunts lexicaux.", "text_Japanese": "語彙借用検出のための 4 種類のモデルを検討してきました。", "text_Russian": "Мы исследовали четыре типа моделей обнаружения лексических заимствований."}
{"text": "Um. In terms of error analysis, well, recall was a weak point for all models.\n", "text_Chinese": "嗯。就错误分析而言，召回率是所有模型的弱点。", "text_Arabic": "أم. فيما يتعلق بتحليل الأخطاء، حسنًا، كان الاستدعاء نقطة ضعف لجميع النماذج.", "text_French": "Euh. En termes d’analyse des erreurs, le rappel était un point faible pour tous les modèles.", "text_Japanese": "うーん。エラー分析の観点から言えば、すべてのモデルでリコールが弱点でした。", "text_Russian": "Хм. Что касается анализа ошибок, отзыв был слабым местом для всех моделей."}
{"text": "Ah, as you can see here, some frequent false negatives include uppercase borrowings, words that exist in both English and Spanish, for instance.\n", "text_Chinese": "啊，正如您在这里看到的，一些常见的漏报包括大写借用，例如英语和西班牙语中都存在的单词。", "text_Arabic": "آه، كما ترون هنا، بعض السلبيات الكاذبة المتكررة تتضمن استعارات بأحرف كبيرة، كلمات موجودة باللغتين الإنجليزية والإسبانية، على سبيل المثال.", "text_French": "Ah, comme vous pouvez le voir ici, certains faux négatifs fréquents incluent des emprunts de majuscules, des mots qui existent en anglais et en espagnol, par exemple.", "text_Japanese": "ああ、ここでわかるように、頻繁に発生する誤検出には、大文字の借用、たとえば英語とスペイン語の両方に存在する単語が含まれます。", "text_Russian": "Ах, как вы можете видеть здесь, некоторые частые ложноотрицательные результаты включают заимствования в верхнем регистре, например, слова, которые существуют как в английском, так и в испанском языках."}
{"text": "Also interestingly, BPE embeddings seem to improve F1 score.\n", "text_Chinese": "同样有趣的是，BPE 嵌入似乎提高了 F1 分数。", "text_Arabic": "ومن المثير للاهتمام أيضًا أن عمليات تضمين BPE تعمل على تحسين نتيجة F1.", "text_French": "Il est également intéressant de noter que les intégrations BPE semblent améliorer le score F1.", "text_Japanese": "また興味深いことに、BPE 埋め込みにより F1 スコアが向上するようです。", "text_Russian": "Также интересно, что встраивание BPE, похоже, улучшает оценку F1."}
{"text": "And character embedding seem to improve recall.\n", "text_Chinese": "字符嵌入似乎可以提高记忆力。", "text_Arabic": "ويبدو أن تضمين الأحرف يؤدي إلى تحسين عملية التذكر.", "text_French": "Et l'intégration de personnages semble améliorer le rappel.", "text_Japanese": "また、文字の埋め込みにより再現率が向上するようです。", "text_Russian": "А встраивание символов, похоже, улучшает запоминаемость."}
{"text": "Which ah it's an interesting finding that perhaps we can explore on future work.\n", "text_Chinese": "这是一个有趣的发现，也许我们可以在未来的工作中进行探索。", "text_Arabic": "وهي نتيجة مثيرة للاهتمام ربما يمكننا استكشافها في العمل المستقبلي.", "text_French": "C'est une découverte intéressante que nous pourrons peut-être explorer lors de travaux futurs.", "text_Japanese": "ああ、これは興味深い発見で、今後の研究で検討できるかもしれません。", "text_Russian": "Это интересное открытие, которое, возможно, мы сможем изучить в будущих работах."}
{"text": "Um. Well, this is everything that I have.\n", "text_Chinese": "嗯。嗯，这就是我所拥有的一切。", "text_Arabic": "أم. حسنا، هذا هو كل ما لدي.", "text_French": "Euh. Eh bien, c'est tout ce que j'ai.", "text_Japanese": "うーん。まあ、これが私が持っているすべてです。", "text_Russian": "Хм. Ну, это все, что у меня есть."}
{"text": "Thank you so much for listening.\n", "text_Chinese": "非常感谢您的聆听。", "text_Arabic": "شكرا جزيلا لكم على الإستماع.", "text_French": "Merci beaucoup d'avoir écouté.", "text_Japanese": "ご清聴ありがとうございました。", "text_Russian": "Большое спасибо за то, что выслушали."}
{"text": "My name is Antoine.\n", "text_Chinese": "我叫安托万。", "text_Arabic": "اسمي أنطوان.", "text_French": "Je m'appelle Antoine.", "text_Japanese": "私の名前はアントワーヌです。", "text_Russian": "Меня зовут Антуан."}
{"text": "I'm a PhD student at the University of Massachusetts Amherst.\n", "text_Chinese": "我是马萨诸塞大学阿默斯特分校的博士生。", "text_Arabic": "أنا طالب دكتوراه في جامعة ماساتشوستس أمهرست.", "text_French": "Je suis doctorant à l'Université du Massachusetts Amherst.", "text_Japanese": "私はマサチューセッツ大学アマースト校の博士課程の学生です。", "text_Russian": "Я аспирант Массачусетского университета в Амхерсте."}
{"text": "I am presenting our paper KinyaBERT: a Morphology-aware Kinyarwanda Language Model.\n", "text_Chinese": "我正在展示我们的论文 KinyaBERT：一种形态学感知的卢旺达语语言模型。", "text_Arabic": "أقدم ورقتنا KinyaBERT: نموذج لغة كينيارواندا المدرك للمورفولوجيا.", "text_French": "Je présente notre article KinyaBERT : un modèle linguistique kinyarwanda sensible à la morphologie.", "text_Japanese": "私は論文「KinyaBERT: a Morphology-aware Kinyarwanda Language Model」を発表します。", "text_Russian": "Я представляю нашу статью KinyaBERT: модель языка киньяруанда с учетом морфологии."}
{"text": "Today, I'll talk about the motivation for this research.\n", "text_Chinese": "今天我就来说说这项研究的动机。", "text_Arabic": "اليوم سأتحدث عن الدافع لهذا البحث.", "text_French": "Aujourd'hui, je vais parler de la motivation de cette recherche.", "text_Japanese": "今日はこの研究の動機についてお話します。", "text_Russian": "Сегодня я расскажу о мотивации этого исследования."}
{"text": "Then I'll present KinyaBERT model architecture in detail.\n", "text_Chinese": "然后我将详细介绍 KinyaBERT 模型架构。", "text_Arabic": "ثم سأقدم بنية نموذج KinyaBERT بالتفصيل.", "text_French": "Ensuite, je présenterai en détail l'architecture du modèle KinyaBERT.", "text_Japanese": "次に、KinyaBERT モデル アーキテクチャを詳しく説明します。", "text_Russian": "Затем я подробно представлю архитектуру модели KinyaBERT."}
{"text": "I'll then talk about our experimental results, then finish with some conclusions.\n", "text_Chinese": "然后我将讨论我们的实验结果，然后得出一些结论。", "text_Arabic": "سأتحدث بعد ذلك عن نتائجنا التجريبية، ثم أنهي ببعض الاستنتاجات.", "text_French": "Je parlerai ensuite de nos résultats expérimentaux, puis terminerai par quelques conclusions.", "text_Japanese": "次に実験結果について話してから、いくつかの結論を述べて終わります。", "text_Russian": "Затем я расскажу о результатах наших экспериментов и закончу некоторыми выводами."}
{"text": "We all know that recent natural language processing advances have been made possible by the use of pretrained language models such as BERT.\n", "text_Chinese": "我们都知道，最近自然语言处理的进步是通过使用 BERT 等预训练语言模型实现的。", "text_Arabic": "نعلم جميعًا أن التطورات الحديثة في معالجة اللغة الطبيعية أصبحت ممكنة بفضل استخدام نماذج اللغة المدربة مسبقًا مثل BERT.", "text_French": "Nous savons tous que les progrès récents en matière de traitement du langage naturel ont été rendus possibles grâce à l'utilisation de modèles linguistiques pré-entraînés tels que BERT.", "text_Japanese": "最近の自然言語処理の進歩は、BERT などの事前トレーニング済み言語モデルの使用によって可能になったことは誰もが知っています。", "text_Russian": "Мы все знаем, что недавние достижения в области обработки естественного языка стали возможными благодаря использованию предварительно обученных языковых моделей, таких как BERT."}
{"text": "However, there are still a number of limitations.\n", "text_Chinese": "然而，仍然存在一些限制。", "text_Arabic": "ومع ذلك، لا يزال هناك عدد من القيود.", "text_French": "Cependant, il existe encore un certain nombre de limites.", "text_Japanese": "ただし、まだ多くの制限があります。", "text_Russian": "Однако по-прежнему существует ряд ограничений."}
{"text": "Due to the complex morphology that is expressed by most morphologically rich languages, the ubiquitous byte pair encoding tokenization algorithm that I used cannot extract the exact subword lexical units, meaning the morphemes, which are needed for effective representation.\n", "text_Chinese": "由于大多数形态丰富的语言所表达的复杂形态，我使用的普遍存在的字节对编码标记化算法无法提取准确的子词词汇单元，即有效表示所需的语素。", "text_Arabic": "نظرًا للتشكل المعقد الذي يتم التعبير عنه في معظم اللغات الغنية شكليًا، فإن خوارزمية ترميز زوج البايت المنتشرة في كل مكان والتي استخدمتها لا يمكنها استخراج الوحدات المعجمية الدقيقة للكلمات الفرعية، أي المورفيمات، اللازمة للتمثيل الفعال.", "text_French": "En raison de la morphologie complexe exprimée par la plupart des langues morphologiquement riches, l'algorithme de tokenisation de codage par paires d'octets omniprésent que j'ai utilisé ne peut pas extraire les unités lexicales exactes des sous-mots, c'est-à-dire les morphèmes, qui sont nécessaires à une représentation efficace.", "text_Japanese": "形態学的に豊富なほとんどの言語では複雑な形態が表現されるため、私が使用したユビキタス バイト ペア エンコーディングのトークン化アルゴリズムでは、効果的な表現に必要な正確なサブワードの語彙単位、つまり形態素を抽出できません。", "text_Russian": "Из-за сложной морфологии, которая выражается большинством морфологически богатых языков, вездесущий алгоритм токенизации кодирования пар байтов, который я использовал, не может извлечь точные лексические единицы подслова, то есть морфемы, которые необходимы для эффективного представления."}
{"text": "For example, here we have three Kinyarwanda words that have several morphemes in them, but the BPE algorithms cannot extract them.\n", "text_Chinese": "例如，这里有三个卢旺达语单词，其中包含多个语素，但 BPE 算法无法提取它们。", "text_Arabic": "على سبيل المثال، لدينا هنا ثلاث كلمات بلغة كينيارواندا تحتوي على العديد من المقاطع الصرفية، لكن خوارزميات BPE لا يمكنها استخراجها.", "text_French": "Par exemple, nous avons ici trois mots kinyarwanda qui contiennent plusieurs morphèmes, mais les algorithmes du BPE ne peuvent pas les extraire.", "text_Japanese": "たとえば、ここには複数の形態素を含む 3 つのルワンダ語の単語がありますが、BPE アルゴリズムはそれらを抽出できません。", "text_Russian": "Например, здесь у нас есть три слова киньяруанда, в которых есть несколько морфем, но алгоритмы BPE не могут их извлечь."}
{"text": "This is because some morphological rules produce different surface forms that hide the exact lexical information, and BPE, which is solely based on the surface forms, does not have access to this lexical model.\n", "text_Chinese": "这是因为某些形态规则会产生不同的表面形式，隐藏了确切的词汇信息，而仅基于表面形式的 BPE 无法访问该词汇模型。", "text_Arabic": "وذلك لأن بعض القواعد المورفولوجية تنتج أشكالًا سطحية مختلفة تخفي المعلومات المعجمية الدقيقة، ولا يستطيع BPE، الذي يعتمد فقط على الأشكال السطحية، الوصول إلى هذا النموذج المعجمي.", "text_French": "En effet, certaines règles morphologiques produisent des formes de surface différentes qui cachent l'information lexicale exacte, et le BPE, qui se base uniquement sur les formes de surface, n'a pas accès à ce modèle lexical.", "text_Japanese": "これは、一部の形態論的規則が正確な語彙情報を隠す異なる表面形式を生成し、表面形式のみに基づく BPE がこの語彙モデルにアクセスできないためです。", "text_Russian": "Это связано с тем, что некоторые морфологические правила создают различные поверхностные формы, которые скрывают точную лексическую информацию, а BPE, основанный исключительно на поверхностных формах, не имеет доступа к этой лексической модели."}
{"text": "The second challenge is that even if one had access to an oracle morphological analyzer, replacing BPE tokens with morphemes is not enough to express the morphological compositionality.\n", "text_Chinese": "第二个挑战是，即使能够使用预言机形态分析器，用语素替换 BPE 标记也不足以表达形态组合性​​。", "text_Arabic": "التحدي الثاني هو أنه حتى لو كان لدى المرء إمكانية الوصول إلى محلل مورفولوجي أوراكل، فإن استبدال رموز BPE المميزة بالمورفيمات لا يكفي للتعبير عن التركيب المورفولوجي.", "text_French": "Le deuxième défi est que même si l’on avait accès à un analyseur morphologique Oracle, remplacer les jetons BPE par des morphèmes ne suffit pas pour exprimer la composition morphologique.", "text_Japanese": "2 番目の課題は、たとえ Oracle 形態素アナライザーにアクセスできたとしても、BPE トークンを形態素に置き換えるだけでは形態学的構成性を表現するには十分ではないということです。", "text_Russian": "Вторая проблема заключается в том, что даже если бы у вас был доступ к морфологическому анализатору Oracle, замены токенов BPE морфемами недостаточно для выражения морфологической композиционности."}
{"text": "A third gap in the research is that new pretrained language models are most often evaluated on high resource languages.\n", "text_Chinese": "研究中的第三个差距是新的预训练语言模型通常是在高资源语言上进行评估的。", "text_Arabic": "الفجوة الثالثة في البحث هي أن نماذج اللغة الجديدة المدربة مسبقًا يتم تقييمها غالبًا على لغات ذات موارد عالية.", "text_French": "Une troisième lacune dans la recherche est que les nouveaux modèles linguistiques pré-entraînés sont le plus souvent évalués sur des langages à ressources élevées.", "text_Japanese": "研究における 3 番目のギャップは、新しい事前トレーニング済み言語モデルは、ほとんどの場合、高リソース言語で評価されることです。", "text_Russian": "Третий пробел в исследовании заключается в том, что новые предварительно обученные языковые модели чаще всего оцениваются на языках с высокими ресурсами."}
{"text": "And we need to assess their applicability on low resources and diverse languages as well.\n", "text_Chinese": "我们还需要评估它们在资源匮乏和语言多样的情况下的适用性。", "text_Arabic": "ونحن بحاجة إلى تقييم إمكانية تطبيقها على الموارد المنخفضة واللغات المتنوعة أيضًا.", "text_French": "Et nous devons également évaluer leur applicabilité sur des ressources limitées et dans des langues diverses.", "text_Japanese": "そして、リソースが少なく、多様な言語でもそれらの適用可能性を評価する必要があります。", "text_Russian": "И нам также необходимо оценить их применимость на ограниченных ресурсах и на разных языках."}
{"text": "Therefore, we present KinyaBERT, which is a simple but effective adaptation of the BERT architecture that is meant to more effectively handle morphologically rich languages.\n", "text_Chinese": "因此，我们提出了 KinyaBERT，它是 BERT 架构的简单但有效的改编，旨在更有效地处理形态丰富的语言。", "text_Arabic": "لذلك، نقدم KinyaBERT، وهو تعديل بسيط ولكنه فعال لبنية BERT التي تهدف إلى التعامل بشكل أكثر فعالية مع اللغات الغنية من الناحية الشكلية.", "text_French": "Par conséquent, nous présentons KinyaBERT, qui est une adaptation simple mais efficace de l'architecture BERT destinée à gérer plus efficacement les langues morphologiquement riches.", "text_Japanese": "したがって、形態素豊かな言語をより効果的に処理することを目的とした BERT アーキテクチャのシンプルだが効果的な適応である KinyaBERT を紹介します。", "text_Russian": "Поэтому мы представляем KinyaBERT, который представляет собой простую, но эффективную адаптацию архитектуры BERT, предназначенную для более эффективной работы с морфологически богатыми языками."}
{"text": "We evaluate KinyaBERT on Kinyarwanda, a low resource morphologically rich language, which is spoken by more than twelve million people across Eastern and Central Africa.\n", "text_Chinese": "我们在基尼亚卢旺达语上评估 KinyaBERT，这是一种资源匮乏、形态丰富的语言，东部和中部非洲有超过 1200 万人使用该语言。", "text_Arabic": "نقوم بتقييم لغة كينيا بيرت على لغة كينيارواندا، وهي لغة منخفضة الموارد وغنية من الناحية الشكلية، والتي يتحدث بها أكثر من اثني عشر مليون شخص في جميع أنحاء شرق ووسط أفريقيا.", "text_French": "Nous évaluons le KinyaBERT sur le kinyarwanda, une langue morphologiquement riche en ressources, qui est parlée par plus de douze millions de personnes en Afrique orientale et centrale.", "text_Japanese": "私たちは、資源が少ない形態学的に豊富な言語であるルワンダ語で KinyaBERT を評価します。この言語は東アフリカと中央アフリカの 1,200 万人以上が話しています。", "text_Russian": "Мы оцениваем KinyaBERT на киньяруанде, морфологически богатом языке с ограниченными ресурсами, на котором говорят более двенадцати миллионов человек в Восточной и Центральной Африке."}
{"text": "The input to the model is either a sentence or a document.\n", "text_Chinese": "模型的输入可以是句子或文档。", "text_Arabic": "الإدخال إلى النموذج إما جملة أو مستند.", "text_French": "L'entrée du modèle est soit une phrase, soit un document.", "text_Japanese": "モデルへの入力は文またはドキュメントのいずれかです。", "text_Russian": "Входными данными для модели является либо предложение, либо документ."}
{"text": "For example here, we have John twarahamubonye biradutangaza, which means we were surprised to find John there.\n", "text_Chinese": "例如，这里有 John twarhamubonye biradutangaza，这意味着我们很惊讶地发现约翰在那里。", "text_Arabic": "على سبيل المثال، لدينا هنا جون توارهاموبوني بيرادوتانغازا، مما يعني أننا فوجئنا بالعثور على جون هناك.", "text_French": "Par exemple ici, nous avons John twarahamubonye biradutangaza, ce qui signifie que nous avons été surpris de trouver John là-bas.", "text_Japanese": "たとえば、ここには John twarahamubonye biradutangaza があります。これは、そこでジョンを見つけて驚いたことを意味します。", "text_Russian": "Например, здесь у нас есть John twarahamubonye biradutangaza, что означает, что мы были удивлены, обнаружив там Джона."}
{"text": "As you can see, Kinyarwanda words contains several morphemes that contain different information in them.\n", "text_Chinese": "正如您所看到的，基尼亚卢旺达语单词包含多个语素，其中包含不同的信息。", "text_Arabic": "كما ترون، تحتوي الكلمات الكينيارواندية على العديد من المقاطع التي تحتوي على معلومات مختلفة فيها.", "text_French": "Comme vous pouvez le constater, les mots kinyarwanda contiennent plusieurs morphèmes qui contiennent des informations différentes.", "text_Japanese": "ご覧のとおり、ルワンダ語の単語には、異なる情報を含むいくつかの形態素が含まれています。", "text_Russian": "Как видите, слова киньяруанда содержат несколько морфем, несущих в себе различную информацию."}
{"text": "Therefore, in our model, we pass this sentence or a document to a morphological analyzer.\n", "text_Chinese": "因此，在我们的模型中，我们将这个句子或文档传递给形态分析器。", "text_Arabic": "لذلك، في نموذجنا، نقوم بتمرير هذه الجملة أو الوثيقة إلى محلل صرفي.", "text_French": "Ainsi, dans notre modèle, nous transmettons cette phrase ou ce document à un analyseur morphologique.", "text_Japanese": "したがって、私たちのモデルでは、この文または文書を形態素解析器に渡します。", "text_Russian": "Поэтому в нашей модели мы передаем это предложение или документ морфологическому анализатору."}
{"text": "Which then generates morphemes contained in each of the words.\n", "text_Chinese": "然后生成每个单词中包含的语素。", "text_Arabic": "والذي يقوم بعد ذلك بإنشاء المورفيمات الموجودة في كل كلمة.", "text_French": "Ce qui génère alors des morphèmes contenus dans chacun des mots.", "text_Japanese": "次に、各単語に含まれる形態素を生成します。", "text_Russian": "Который затем генерирует морфемы, содержащиеся в каждом из слов."}
{"text": "The morphemes usually are made of the stem and zero or more affixes.\n", "text_Chinese": "语素通常由词干和零个或多个词缀组成。", "text_Arabic": "تتكون المورفيمات عادة من الجذع وصفر أو أكثر من اللواحق.", "text_French": "Les morphèmes sont généralement constitués du radical et de zéro ou plusieurs affixes.", "text_Japanese": "形態素は通常、語幹と 0 個以上の接辞で構成されます。", "text_Russian": "Морфемы обычно состоят из основы и нуля или более аффиксов."}
{"text": "The affixes may indicate tense, aspect, subject or object in verbs, and more often relates to the Bantu noun class for subjects and objects.\n", "text_Chinese": "词缀可以表示动词中的时态、体、主语或宾语，并且更常见地与班图语名词类的主语和宾语相关。", "text_Arabic": "قد تشير اللواحق إلى زمن أو جانب أو موضوع أو مفعول به في الأفعال، وغالبًا ما ترتبط بفئة اسم البانتو للمواضيع والأشياء.", "text_French": "Les affixes peuvent indiquer le temps, l'aspect, le sujet ou l'objet dans les verbes, et se rapportent le plus souvent à la classe nominale bantoue pour les sujets et les objets.", "text_Japanese": "接辞は動詞の時制、側面、主語または目的語を示すことがあり、多くの場合、主語と目的語のバンツー名詞クラスに関連します。", "text_Russian": "Аффиксы могут обозначать время, вид, подлежащее или объект в глаголах и чаще всего относятся к классу существительных банту для субъектов и объектов."}
{"text": "The morphological analyzer also produces a part of speech tag for each of the words.\n", "text_Chinese": "形态分析器还为每个单词生成词性标签。", "text_Arabic": "يقوم المحلل الصرفي أيضًا بإنتاج جزء من علامة الكلام لكل كلمة.", "text_French": "L'analyseur morphologique produit également une balise de partie de discours pour chacun des mots.", "text_Japanese": "形態素解析器は、各単語の品詞タグも生成します。", "text_Russian": "Морфологический анализатор также выдает часть речевого тега для каждого из слов."}
{"text": "After this step, we make embeddings for the spee- for the part of speech tags.\n", "text_Chinese": "在此步骤之后，我们为词性标记的 spee 进行嵌入。", "text_Arabic": "بعد هذه الخطوة نقوم بعمل التضمينات للجزء الخاص بعلامات الكلام.", "text_French": "Après cette étape, nous réalisons des intégrations pour la parole et pour la partie des balises vocales.", "text_Japanese": "このステップの後、品詞タグの速度の埋め込みを作成します。", "text_Russian": "После этого шага мы делаем встраивания для скоростной части речевых тегов."}
{"text": "Embeddings for the affixes.\n", "text_Chinese": "词缀的嵌入。", "text_Arabic": "التضمينات للملحقات.", "text_French": "Intégrations pour les affixes.", "text_Japanese": "接辞の埋め込み。", "text_Russian": "Вложения для аффиксов."}
{"text": "And embeddings for the stem.\n", "text_Chinese": "以及茎的嵌入。", "text_Arabic": "والتضمينات للجذع.", "text_French": "Et des intégrations pour la tige.", "text_Japanese": "そしてステムの埋め込み。", "text_Russian": "И закладные для штока."}
{"text": "These are the morphology level, these are the morphology level embeddings.\n", "text_Chinese": "这些是形态级别，这些是形态级别嵌入。", "text_Arabic": "هذه هي المستويات المورفولوجية، وهذه هي التضمينات على المستوى المورفولوجي.", "text_French": "Ce sont le niveau morphologique, ce sont les plongements au niveau morphologique.", "text_Japanese": "これらは形態レベルであり、形態レベルの埋め込みです。", "text_Russian": "Это уровень морфологии, это вложения уровня морфологии."}
{"text": "We then pass these embeddings through a morphology encoder, which is a small transformer encoder that is applied to each word independently.\n", "text_Chinese": "然后，我们将这些嵌入通过形态编码器，这是一个独立应用于每个单词的小型变压器编码器。", "text_Arabic": "نقوم بعد ذلك بتمرير هذه التضمينات من خلال مشفر مورفولوجي، وهو عبارة عن مشفر محول صغير يتم تطبيقه على كل كلمة بشكل مستقل.", "text_French": "Nous transmettons ensuite ces intégrations via un encodeur de morphologie, qui est un petit encodeur de transformateur appliqué à chaque mot indépendamment.", "text_Japanese": "次に、これらの埋め込みを形態エンコーダに渡します。形態エンコーダは、各単語に個別に適用される小さな変換エンコーダです。", "text_Russian": "Затем мы пропускаем эти внедрения через морфологический кодер, который представляет собой небольшой преобразователь-кодер, который применяется к каждому слову независимо."}
{"text": "The output of the are the vectors that are contextualized with the morphological information at each word.\n", "text_Chinese": "的输出是与每个单词的形态信息相关的向量。", "text_Arabic": "مخرجات هي المتجهات التي يتم وضعها في سياق المعلومات المورفولوجية في كل كلمة.", "text_French": "La sortie du sont les vecteurs qui sont contextualisés avec les informations morphologiques de chaque mot.", "text_Japanese": "の出力は、各単語の形態学的情報でコンテキスト化されたベクトルです。", "text_Russian": "Результатом работы являются векторы, контекстуализированные морфологической информацией каждого слова."}
{"text": "Now, we perform composition where the morphological embeddings corresponding to part of speech and stem are concatenated together.\n", "text_Chinese": "现在，我们进行组合，其中对应于词性和词干的形态嵌入被连接在一起。", "text_Arabic": "الآن، نقوم بالتركيب حيث يتم ربط التضمينات المورفولوجية المقابلة لجزء من الكلام والجذع معًا.", "text_French": "Maintenant, nous effectuons une composition où les plongements morphologiques correspondant à une partie du discours et au radical sont concaténés ensemble.", "text_Japanese": "ここで、品詞と語幹に対応する形態素埋め込みを連結する合成を行います。", "text_Russian": "Теперь мы выполняем композицию, в которой морфологические вложения, соответствующие части речи и основе, объединяются вместе."}
{"text": "We further concat we further concatenate them with another stem embedding at the sentence level.\n", "text_Chinese": "我们进一步将它们与句子级别的另一个词干嵌入进一步连接起来。", "text_Arabic": "نقوم أيضًا بربطها مع تضمين جذع آخر على مستوى الجملة.", "text_French": "Nous les concaténons en outre avec un autre radical intégré au niveau de la phrase.", "text_Japanese": "さらにそれらを文レベルで埋め込まれた別の語幹と連結します。", "text_Russian": "Далее мы объединяем их с помощью еще одного встраивания основы на уровне предложения."}
{"text": "Then we form an input to the main sentence or document encoder.\n", "text_Chinese": "然后我们形成主句子或文档编码器的输入。", "text_Arabic": "ثم نقوم بتكوين مدخلات للجملة الرئيسية أو برنامج تشفير المستندات.", "text_French": "Ensuite, nous formons une entrée dans l'encodeur de phrase principale ou de document.", "text_Japanese": "次に、メインセンテンスまたはドキュメントエンコーダへの入力を形成します。", "text_Russian": "Затем формируем вход в основное предложение или кодировщик документа."}
{"text": "The final output are contextualized embeddings that can be used for downstream NLP tasks.\n", "text_Chinese": "最终输出是可用于下游 NLP 任务的上下文嵌入。", "text_Arabic": "الناتج النهائي عبارة عن عمليات تضمين سياقية يمكن استخدامها لمهام البرمجة اللغوية العصبية (NLP) النهائية.", "text_French": "Le résultat final sont des intégrations contextualisées qui peuvent être utilisées pour les tâches NLP en aval.", "text_Japanese": "最終出力は、下流の NLP タスクに使用できるコンテキスト化された埋め込みです。", "text_Russian": "Конечным результатом являются контекстуализированные внедрения, которые можно использовать для последующих задач НЛП."}
{"text": "For a morphological analyzer, we use finite state two level morphology principles with custom implementation that is tailored to the Kinyarwanda language.\n", "text_Chinese": "对于形态分析器，我们使用有限状态二级形态学原理以及针对基尼亚卢旺达语定制的自定义实现。", "text_Arabic": "بالنسبة للمحلل المورفولوجي، نستخدم مبادئ مورفولوجية ذات مستويين للحالة المحدودة مع تنفيذ مخصص مصمم خصيصًا للغة الكينيارواندا.", "text_French": "Pour un analyseur morphologique, nous utilisons des principes de morphologie à deux niveaux à états finis avec une implémentation personnalisée adaptée à la langue kinyarwanda.", "text_Japanese": "形態素解析器の場合、有限状態 2 レベルの形態素原理とルワンダ語に合わせたカスタム実装を使用します。", "text_Russian": "Для морфологического анализатора мы используем принципы двухуровневой морфологии с конечным состоянием с индивидуальной реализацией, адаптированной к языку киньяруанда."}
{"text": "We effectively model the morphology of all Kinyarwanda words, including verbals, nouns, demonstrative and possessive pronouns, numerals, and others.\n", "text_Chinese": "我们有效地模拟了所有卢旺达语单词的形态，包括动词、名词、指示代词和所有格代词、数字等。", "text_Arabic": "نحن نصمم بشكل فعال مورفولوجية جميع الكلمات الكينيارواندية، بما في ذلك الأفعال والأسماء وضمائر الإشارة والملكية والأرقام وغيرها.", "text_French": "Nous modélisons efficacement la morphologie de tous les mots kinyarwanda, y compris les mots verbaux, les noms, les pronoms démonstratifs et possessifs, les chiffres et autres.", "text_Japanese": "私たちは、動詞、名詞、指示代名詞、所有代名詞、数字などを含むすべてのルワンダ語の単語の形態を効果的にモデル化します。", "text_Russian": "Мы эффективно моделируем морфологию всех слов киньяруанда, включая глаголы, существительные, указательные и притяжательные местоимения, числительные и другие."}
{"text": "We use an unsupervised part of speech tagging algorithm.\n", "text_Chinese": "我们使用无监督的词性标记算法。", "text_Arabic": "نحن نستخدم جزءًا غير خاضع للرقابة من خوارزمية وضع علامات على الكلام.", "text_French": "Nous utilisons un algorithme de marquage vocal non supervisé.", "text_Japanese": "教師なし品詞タグ付けアルゴリズムを使用します。", "text_Russian": "Мы используем неконтролируемую часть алгоритма речевой маркировки."}
{"text": "A first order factored model is used to account for morphology probability, basically the probability that is assigned by the morphological analyzer.\n", "text_Chinese": "一阶因子模型用于解释形态概率，基本上是由形态分析器分配的概率。", "text_Arabic": "يتم استخدام نموذج عامل من الدرجة الأولى لحساب احتمالية الشكل، وهو في الأساس الاحتمال الذي يتم تعيينه بواسطة المحلل المورفولوجي.", "text_French": "Un modèle factorisé de premier ordre est utilisé pour tenir compte de la probabilité morphologique, essentiellement la probabilité attribuée par l'analyseur morphologique.", "text_Japanese": "一次因数分解モデルは、形態学的確率、基本的には形態素解析器によって割り当てられる確率を考慮するために使用されます。", "text_Russian": "Факторизованная модель первого порядка используется для учета вероятности морфологии, то есть вероятности, присваиваемой морфологическим анализатором."}
{"text": "We also take into consideration the part of speech tag precedence as well as the syntactic agreements that are present in the in the input words.\n", "text_Chinese": "我们还考虑词性标签优先级以及输入单词中存在的句法协议。", "text_Arabic": "نحن نأخذ في الاعتبار أيضًا جزء أسبقية علامة الكلام بالإضافة إلى الاتفاقيات النحوية الموجودة في كلمات الإدخال.", "text_French": "Nous prenons également en compte la priorité des balises de partie du discours ainsi que les accords syntaxiques présents dans les mots d'entrée.", "text_Japanese": "また、品詞タグの優先順位や、入力単語に存在する構文上の一致も考慮されます。", "text_Russian": "Мы также принимаем во внимание часть приоритета речевых тегов, а также синтаксические соглашения, присутствующие во входных словах."}
{"text": "The part of speech tagger uses a bidi bidirectional inference which improves upon the more often used Viterbi algorithm for decoding.\n", "text_Chinese": "词性标注器使用双向推理，该推理改进了更常用的维特比解码算法。", "text_Arabic": "يستخدم جزء أداة تمييز الكلام استدلالًا ثنائي الاتجاه يعمل على تحسين خوارزمية Viterbi الأكثر استخدامًا لفك التشفير.", "text_French": "Le marqueur de parties de discours utilise une inférence bidirectionnelle bidi qui améliore l'algorithme de Viterbi le plus souvent utilisé pour le décodage.", "text_Japanese": "品詞タグ付け機能は、デコードによく使用されるビタビ アルゴリズムを改良した双方向推論を使用します。", "text_Russian": "Часть речевого тегера использует двунаправленный вывод двунаправленного текста, который улучшает более часто используемый алгоритм Витерби для декодирования."}
{"text": "A few remarks here for positional encoding.\n", "text_Chinese": "这里有一些关于位置编码的注释。", "text_Arabic": "بعض الملاحظات هنا للتشفير الموضعي.", "text_French": "Quelques remarques ici pour l'encodage positionnel.", "text_Japanese": "ここで、位置エンコーディングについていくつか説明します。", "text_Russian": "Несколько замечаний по поводу позиционного кодирования."}
{"text": "One, the morphology encoder does not use any positional encoding.\n", "text_Chinese": "第一，形态编码器不使用任何位置编码。", "text_Arabic": "أولاً، لا يستخدم التشفير المورفولوجي أي تشفير موضعي.", "text_French": "Premièrement, l’encodeur morphologique n’utilise aucun codage de position.", "text_Japanese": "1 つ目は、形態エンコーダーは位置エンコーディングを使用しません。", "text_Russian": "Во-первых, морфологический кодер не использует никакого позиционного кодирования."}
{"text": "This is because each of the morphemes occupies a known slot in the morphological model.\n", "text_Chinese": "这是因为每个语素在形态模型中占据一个已知的位置。", "text_Arabic": "وذلك لأن كل شكل من الأشكال يحتل مكانًا معروفًا في النموذج المورفولوجي.", "text_French": "En effet, chacun des morphèmes occupe une case connue dans le modèle morphologique.", "text_Japanese": "これは、各形態素が形態学的モデル内の既知のスロットを占有するためです。", "text_Russian": "Это связано с тем, что каждая из морфем занимает известный слот в морфологической модели."}
{"text": "Therefore, positional information is inherent when the morphemes are given.\n", "text_Chinese": "因此，当给出语素时，位置信息是固有的。", "text_Arabic": "ولذلك، فإن المعلومات الموضعية تكون متأصلة عند إعطاء المورفيمات.", "text_French": "Par conséquent, les informations de position sont inhérentes lorsque les morphèmes sont donnés.", "text_Japanese": "したがって、形態素を与える際に位置情報が内在することになる。", "text_Russian": "Следовательно, позиционная информация присуща при передаче морфем."}
{"text": "Second, the sentence encoder uses the so-called untied relative positional embeddings, which have been recently published at ICLR conference.\n", "text_Chinese": "其次，句子编码器使用所谓的无限制相对位置嵌入，该嵌入最近在 ICLR 会议上发布。", "text_Arabic": "ثانيًا، يستخدم برنامج تشفير الجملة ما يسمى بالتضمين الموضعي النسبي غير المقيد، والذي تم نشره مؤخرًا في مؤتمر ICLR.", "text_French": "Deuxièmement, l'encodeur de phrase utilise ce que l'on appelle les intégrations positionnelles relatives non liées, qui ont été récemment publiées lors de la conférence ICLR.", "text_Japanese": "第 2 に、文エンコーダは、ICLR カンファレンスで最近発表された、いわゆるアンタイド相対位置埋め込みを使用します。", "text_Russian": "Во-вторых, кодер предложений использует так называемые несвязанные относительные позиционные вложения, которые были недавно опубликованы на конференции ICLR."}
{"text": "This positional embeddings essentially disentangles positional correlations from token to token attention computation.\n", "text_Chinese": "这种位置嵌入本质上解开了令牌到令牌注意力计算之间的位置相关性。", "text_Arabic": "تعمل هذه التضمينات الموضعية بشكل أساسي على فصل الارتباطات الموضعية من حساب الانتباه المميز إلى الرمز المميز.", "text_French": "Cette intégration positionnelle démêle essentiellement les corrélations de position du calcul de l'attention jeton à jeton.", "text_Japanese": "この位置埋め込みは本質的に、トークン間のアテンション計算からの位置相関を解きほぐします。", "text_Russian": "Такое позиционное встраивание существенно распутывает позиционные корреляции от вычисления внимания к токену."}
{"text": "Similar to BERT, we use a masked language model pre-training objective.\n", "text_Chinese": "与 BERT 类似，我们使用屏蔽语言模型预训练目标。", "text_Arabic": "على غرار BERT، نستخدم هدف التدريب المسبق لنموذج اللغة المقنع.", "text_French": "Semblable à BERT, nous utilisons un objectif de pré-formation de modèle de langage masqué.", "text_Japanese": "BERT と同様に、マスクされた言語モデルの事前トレーニング目標を使用します。", "text_Russian": "Подобно BERT, мы используем цель предварительного обучения замаскированной языковой модели."}
{"text": "Essentially we have to predict both the stem and the affixes that are associated with the words.\n", "text_Chinese": "本质上，我们必须预测与单词相关的词干和词缀。", "text_Arabic": "في الأساس علينا أن نتنبأ بكل من الجذع واللواحق المرتبطة بالكلمات.", "text_French": "Essentiellement, nous devons prédire à la fois le radical et les affixes associés aux mots.", "text_Japanese": "基本的に、単語に関連付けられた語幹と接辞の両方を予測する必要があります。", "text_Russian": "По сути, нам нужно предсказать как основу, так и аффиксы, связанные со словами."}
{"text": "During pre-training, fifteen percent of all words are considered for prediction, of which eighty percent are masked, ten percent are swapped with random words, and ten percent are left unchanged.\n", "text_Chinese": "在预训练期间，所有单词的 15% 被考虑用于预测，其中 80% 被屏蔽，10% 与随机单词交换，10% 保持不变。", "text_Arabic": "أثناء التدريب المسبق، يتم أخذ خمسة عشر بالمائة من جميع الكلمات في الاعتبار للتنبؤ، حيث يتم إخفاء ثمانين بالمائة منها، ويتم تبديل عشرة بالمائة بكلمات عشوائية، ويتم ترك عشرة بالمائة دون تغيير.", "text_French": "Au cours de la pré-formation, quinze pour cent de tous les mots sont pris en compte pour la prédiction, dont quatre-vingts pour cent sont masqués, dix pour cent sont échangés avec des mots aléatoires et dix pour cent restent inchangés.", "text_Japanese": "事前トレーニング中、全単語の 15% が予測の対象とされ、そのうち 80% がマスクされ、10% がランダムな単語と交換され、10% は変更されません。", "text_Russian": "Во время предварительного обучения пятнадцать процентов всех слов рассматриваются для прогнозирования, из которых восемьдесят процентов маскируются, десять процентов заменяются случайными словами и десять процентов остаются неизменными."}
{"text": "For affix prediction, we face some multi label classification problem.\n", "text_Chinese": "对于词缀预测，我们面临一些多标签分类问题。", "text_Arabic": "بالنسبة للتنبؤ بالملحقات، نواجه مشكلة في التصنيف المتعدد العلامات.", "text_French": "Pour la prédiction des affixes, nous sommes confrontés à un problème de classification multi-étiquettes.", "text_Japanese": "接辞予測では、複数ラベル分類の問題に直面します。", "text_Russian": "При прогнозировании аффиксов мы сталкиваемся с проблемой классификации нескольких меток."}
{"text": "For this, we either group together affixes into a fixed number of sets and predict the set as a class label.\n", "text_Chinese": "为此，我们要么将词缀组合成固定数量的集合，并将该集合预测为类标签。", "text_Arabic": "لهذا، إما أن نقوم بتجميع اللواحق معًا في عدد ثابت من المجموعات والتنبؤ بالمجموعة كتسمية فئة.", "text_French": "Pour cela, soit nous regroupons les affixes en un nombre fixe d’ensembles et prédisons l’ensemble sous forme d’étiquette de classe.", "text_Japanese": "このために、接辞を固定数のセットにグループ化し、そのセットをクラス ラベルとして予測します。", "text_Russian": "Для этого мы либо группируем аффиксы в фиксированное количество наборов и прогнозируем этот набор как метку класса."}
{"text": "The other option is to predict the affix probability vector.\n", "text_Chinese": "另一种选择是预测词缀概率向量。", "text_Arabic": "الخيار الآخر هو التنبؤ بمتجه احتمالية التثبيت.", "text_French": "L'autre option consiste à prédire le vecteur de probabilité d'affixe.", "text_Japanese": "もう 1 つのオプションは、接辞確率ベクトルを予測することです。", "text_Russian": "Другой вариант — спрогнозировать вектор вероятности аффикса."}
{"text": "We evaluate both of these approaches in our experiments.\n", "text_Chinese": "我们在实验中评估这两种方法。", "text_Arabic": "نقوم بتقييم كلا النهجين في تجاربنا.", "text_French": "Nous évaluons ces deux approches dans nos expériences.", "text_Japanese": "私たちは実験でこれらのアプローチの両方を評価します。", "text_Russian": "В наших экспериментах мы оцениваем оба этих подхода."}
{"text": "We pre-train KinyaBERT on about two and half gigabytes of Kinyarwanda text, and compare it to three baseline models.\n", "text_Chinese": "我们在大约 2.5 GB 的卢旺达语文本上预训练 KinyaBERT，并将其与三个基线模型进行比较。", "text_Arabic": "لقد قمنا بتدريب لغة KinyaBERT مسبقًا على حوالي 2.5 جيجا بايت من نص لغة كينيارواندا، وقارنناها بثلاثة نماذج أساسية.", "text_French": "Nous pré-entraînons KinyaBERT sur environ deux gigaoctets et demi de texte kinyarwanda et le comparons à trois modèles de base.", "text_Japanese": "約 2.5 ギガバイトのルワンダ語テキストで KinyaBERT を事前トレーニングし、それを 3 つのベースライン モデルと比較します。", "text_Russian": "Мы предварительно обучаем KinyaBERT примерно на двух с половиной гигабайтах текста киньяруанда и сравниваем его с тремя базовыми моделями."}
{"text": "One is a multilingual model called XLM-R, that is trained on a large text corpora that is made of multiple languages.\n", "text_Chinese": "一种是名为 XLM-R 的多语言模型，它是在由多种语言组成的大型文本语料库上进行训练的。", "text_Arabic": "أحدهما هو نموذج متعدد اللغات يسمى XLM-R، والذي يتم تدريبه على مجموعة نصية كبيرة مكونة من لغات متعددة.", "text_French": "L’un est un modèle multilingue appelé XLM-R, qui est formé sur un grand corpus de textes composé de plusieurs langues.", "text_Japanese": "1 つは、XLM-R と呼ばれる多言語モデルで、複数の言語で構成される大規模なテキスト コーパスでトレーニングされます。", "text_Russian": "Одна из них — многоязычная модель под названием XLM-R, которая обучается на больших текстовых корпусах, состоящих из нескольких языков."}
{"text": "The other two baselines are pretrained on the same Kinyarwanda text using either the byte pair encoding algorithm or using morphological analysis without using the two tier transformer encoder architecture.\n", "text_Chinese": "其他两个基线使用字节对编码算法或使用形态分析在相同的基尼亚卢旺达语文本上进行预训练，而不使用两层变压器编码器架构。", "text_Arabic": "يتم تدريب خطي الأساس الآخرين مسبقًا على نفس نص كينيارواندا باستخدام إما خوارزمية تشفير زوج البايت أو استخدام التحليل المورفولوجي دون استخدام بنية تشفير المحولات ذات المستويين.", "text_French": "Les deux autres lignes de base sont pré-entraînées sur le même texte kinyarwanda en utilisant soit l'algorithme de codage par paires d'octets, soit en utilisant l'analyse morphologique sans utiliser l'architecture d'encodeur de transformateur à deux niveaux.", "text_Japanese": "他の 2 つのベースラインは、バイト ペア エンコード アルゴリズムを使用するか、2 層トランスフォーマー エンコーダー アーキテクチャを使用せずに形態素解析を使用して、同じルワンダ語テキストで事前トレーニングされます。", "text_Russian": "Две другие базовые линии предварительно обучаются на одном и том же тексте киньяруанда с использованием либо алгоритма кодирования пар байтов, либо с использованием морфологического анализа без использования архитектуры двухуровневого преобразователя кодера."}
{"text": "All models are configured in the base architecture, which is about between a hundred and a hundred and ten million parameters, with Kinyarwanda with KinyaBERT using the least number of parameters.\n", "text_Chinese": "所有模型都在基础架构中配置，大约有一百一亿个参数，其中 Kinyarwanda 和 KinyaBERT 使用的参数数量最少。", "text_Arabic": "يتم تكوين جميع النماذج في البنية الأساسية، والتي تتراوح ما بين مائة إلى مائة وعشرة ملايين من المعلمات، مع استخدام لغة كينيارواندا وKinyaBERT لأقل عدد من المعلمات.", "text_French": "Tous les modèles sont configurés dans l'architecture de base, qui comprend entre cent et cent dix millions de paramètres, le kinyarwanda et le kinyaBERT utilisant le moins de paramètres.", "text_Japanese": "すべてのモデルは基本アーキテクチャで構成されており、約 100 万から 1100 万のパラメータが含まれますが、KinyaBERT を使用したルワンダでは最も少ない数のパラメータが使用されます。", "text_Russian": "Все модели настроены в базовой архитектуре, которая содержит от ста до ста десяти миллионов параметров, при этом киньяруанда и киньяBERT используют наименьшее количество параметров."}
{"text": "All models except the multilingual are pretrained for thirty two thousand gradient updates with a batch size of two thousand five hundred and sixty sequences in each batch.\n", "text_Chinese": "除多语言模型外，所有模型都经过预训练，可进行 32000 次梯度更新，每批的批量大小为 2560 个序列。", "text_Arabic": "يتم تدريب جميع النماذج، باستثناء الطراز متعدد اللغات، مسبقًا على اثنين وثلاثين ألف تحديث متدرج بحجم دفعة يبلغ ألفين وخمسمائة وستين تسلسلًا في كل دفعة.", "text_French": "Tous les modèles, à l'exception du multilingue, sont pré-entraînés pour trente-deux mille mises à jour de gradient avec une taille de lot de deux mille cinq cent soixante séquences dans chaque lot.", "text_Japanese": "多言語モデルを除くすべてのモデルは、各バッチで 2,560 シーケンスのバッチ サイズで 32,000 の勾配更新用に事前トレーニングされています。", "text_Russian": "Все модели, кроме многоязычной, предварительно обучены тридцати двум тысячам обновлений градиента с размером пакета в две тысячи пятьсот шестьдесят последовательностей в каждом пакете."}
{"text": "We evaluate the pretrained models on three sets of tasks.\n", "text_Chinese": "我们在三组任务上评估预训练模型。", "text_Arabic": "نقوم بتقييم النماذج المدربة مسبقًا على ثلاث مجموعات من المهام.", "text_French": "Nous évaluons les modèles pré-entraînés sur trois ensembles de tâches.", "text_Japanese": "3 セットのタスクで事前トレーニングされたモデルを評価します。", "text_Russian": "Мы оцениваем предварительно обученные модели по трем наборам задач."}
{"text": "One is the GLUE benchmark which has often been used for evaluating the effectiveness of pretrained language models.\n", "text_Chinese": "其中之一是 GLUE 基准，通常用于评估预训练语言模型的有效性。", "text_Arabic": "أحدهما هو معيار GLUE الذي غالبًا ما يستخدم لتقييم فعالية نماذج اللغة المدربة مسبقًا.", "text_French": "L'un d'eux est le benchmark GLUE qui a souvent été utilisé pour évaluer l'efficacité des modèles linguistiques pré-entraînés.", "text_Japanese": "1 つは、事前トレーニングされた言語モデルの有効性を評価するためによく使用される GLUE ベンチマークです。", "text_Russian": "Одним из них является тест GLUE, который часто используется для оценки эффективности предварительно обученных языковых моделей."}
{"text": "We obtain our GLUE benchmark data by translating the original benchmark data into Kinyarwanda using Google Translate.\n", "text_Chinese": "我们通过使用谷歌翻译将原始基准数据翻译成卢旺达语来获得 GLUE 基准数据。", "text_Arabic": "نحصل على بيانات قياس الأداء GLUE الخاصة بنا عن طريق ترجمة البيانات المعيارية الأصلية إلى لغة كينيارواندا باستخدام الترجمة من Google.", "text_French": "Nous obtenons nos données de référence GLUE en traduisant les données de référence originales en kinyarwanda à l'aide de Google Translate.", "text_Japanese": "GLUE ベンチマーク データは、Google 翻訳を使用して元のベンチマーク データをルワンダ語に翻訳して取得します。", "text_Russian": "Мы получаем данные тестов GLUE, переводя исходные данные тестов на язык киньяруанда с помощью Google Translate."}
{"text": "The second task is Kinyarwanda named entity recognition benchmark, which is a high quality dataset that was annotated by trained native speakers.\n", "text_Chinese": "第二个任务是 Kinyarwanda 命名的实体识别基准，这是一个由训练有素的母语人士注释的高质量数据集。", "text_Arabic": "المهمة الثانية هي معيار التعرف على الكيانات المسمى بلغة كينيارواندا، وهو عبارة عن مجموعة بيانات عالية الجودة تم شرحها بواسطة متحدثين أصليين مدربين.", "text_French": "La deuxième tâche est le kinyarwanda nommé référentiel de reconnaissance d'entités, qui est un ensemble de données de haute qualité annoté par des locuteurs natifs formés.", "text_Japanese": "2 番目のタスクは、ルワンダ語固有表現認識ベンチマークです。これは、訓練されたネイティブ スピーカーによって注釈が付けられた高品質のデータセットです。", "text_Russian": "Вторая задача — это тест распознавания объектов, названный киньяруандой, который представляет собой высококачественный набор данных, аннотированный обученными носителями языка."}
{"text": "The third one is a news categorization task where we pull news articles from several websites and collecting their categorization tags that were assigned by the authors and then essentially trying to predict the same, the the same categories.\n", "text_Chinese": "第三个是新闻分类任务，我们从多个网站提取新闻文章并收集作者分配的分类标签，然后基本上尝试预测相同的类别。", "text_Arabic": "المهمة الثالثة هي مهمة تصنيف الأخبار حيث نقوم بسحب المقالات الإخبارية من عدة مواقع ويب وجمع علامات التصنيف الخاصة بها التي تم تعيينها من قبل المؤلفين ثم نحاول بشكل أساسي التنبؤ بنفس الفئات.", "text_French": "La troisième est une tâche de catégorisation des actualités dans laquelle nous extrayons des articles d'actualité de plusieurs sites Web et collectons leurs balises de catégorisation qui ont été attribuées par les auteurs, puis essayons essentiellement de prédire les mêmes, les mêmes catégories.", "text_Japanese": "3 つ目はニュース分類タスクです。このタスクでは、いくつかの Web サイトからニュース記事を取得し、著者によって割り当てられた分類タグを収集し、基本的に同じカテゴリを予測しようとします。", "text_Russian": "Третий — это задача категоризации новостей, когда мы извлекаем новостные статьи с нескольких веб-сайтов и собираем их теги категоризации, назначенные авторами, а затем, по сути, пытаемся предсказать одни и те же категории."}
{"text": "And now we go to the results.\n", "text_Chinese": "现在我们来看结果。", "text_Arabic": "والآن ننتقل إلى النتائج.", "text_French": "Et maintenant, passons aux résultats.", "text_Japanese": "それでは結果に移ります。", "text_Russian": "А теперь переходим к результатам."}
{"text": "For the GLUE benchmark, we find that KinyaBERT consistently outperforms baseline models.\n", "text_Chinese": "对于 GLUE 基准测试，我们发现 KinyaBERT 始终优于基线模型。", "text_Arabic": "بالنسبة لمعيار GLUE، نجد أن KinyaBERT يتفوق باستمرار على النماذج الأساسية.", "text_French": "Pour le benchmark GLUE, nous constatons que KinyaBERT surpasse systématiquement les modèles de base.", "text_Japanese": "GLUE ベンチマークでは、KinyaBERT が常にベースライン モデルを上回るパフォーマンスを示していることがわかります。", "text_Russian": "В тесте GLUE мы обнаружили, что KinyaBERT постоянно превосходит базовые модели."}
{"text": "Here we show the average performance for ten finetuning runs.\n", "text_Chinese": "在这里，我们展示了十次微调运行的平均性能。", "text_Arabic": "نعرض هنا متوسط ​​الأداء لعشر عمليات ضبط دقيقة.", "text_French": "Nous montrons ici les performances moyennes pour dix cycles de réglage fin.", "text_Japanese": "ここでは、10 回の微調整実行の平均パフォーマンスを示します。", "text_Russian": "Здесь мы показываем среднюю производительность для десяти прогонов точной настройки."}
{"text": "We also run a user evaluation of the translations that are produced by Google Translate.\n", "text_Chinese": "我们还对谷歌翻译生成的翻译进行了用户评估。", "text_Arabic": "نقوم أيضًا بإجراء تقييم للمستخدم للترجمات التي تنتجها خدمة الترجمة من Google.", "text_French": "Nous effectuons également une évaluation des utilisateurs des traductions produites par Google Translate.", "text_Japanese": "また、Google 翻訳によって生成された翻訳のユーザー評価も実施します。", "text_Russian": "Мы также проводим пользовательскую оценку переводов, выполненных с помощью Google Translate."}
{"text": "Essentially, user users rated about six thousand examples, assigning scores on a scale from one to four, assessing the quality of the translations.\n", "text_Chinese": "本质上，用户对大约六千个示例进行评分，从一到四的范围内打分，评估翻译的质量。", "text_Arabic": "في الأساس، قام المستخدمون بتقييم حوالي ستة آلاف مثال، وتعيين الدرجات على مقياس من واحد إلى أربعة، وتقييم جودة الترجمات.", "text_French": "Essentiellement, les utilisateurs ont évalué environ six mille exemples, en attribuant des notes sur une échelle de un à quatre, évaluant la qualité des traductions.", "text_Japanese": "基本的に、ユーザーは約6,000の例文を評価し、1から4のスケールでスコアを割り当て、翻訳の品質を評価しました。", "text_Russian": "По сути пользователи оценили около шести тысяч примеров, выставив оценки по шкале от одного до четырех, оценивая качество переводов."}
{"text": "The result is that many translations were noisy.\n", "text_Chinese": "结果是很多翻译都是有噪音的。", "text_Arabic": "والنتيجة هي أن العديد من الترجمات كانت صاخبة.", "text_French": "Le résultat est que de nombreuses traductions étaient bruyantes.", "text_Japanese": "その結果、多くの翻訳にノイズが発生しました。", "text_Russian": "В результате многие переводы были зашумлены."}
{"text": "But, all models had to cope with the same translation noise, and the relative performance between the models is still important to notice.\n", "text_Chinese": "但是，所有模型都必须应对相同的平移噪声，并且模型之间的相对性能仍然值得注意。", "text_Arabic": "ولكن، كان على جميع النماذج أن تتعامل مع نفس ضجيج الترجمة، ولا يزال من المهم ملاحظة الأداء النسبي بين النماذج.", "text_French": "Mais tous les modèles ont dû faire face au même bruit de traduction, et les performances relatives entre les modèles sont toujours importantes à noter.", "text_Japanese": "ただし、すべてのモデルは同じ変換ノイズに対処する必要があり、モデル間の相対的なパフォーマンスに注目することが依然として重要です。", "text_Russian": "Но все модели должны были справляться с одним и тем же шумом перевода, и по-прежнему важно учитывать относительную производительность между моделями."}
{"text": "For the named entity recognition task, we also find that KinyaBERT gives the best performance with the affix distribution regression variant performing best.\n", "text_Chinese": "对于命名实体识别任务，我们还发现 KinyaBERT 提供了最佳性能，其中词缀分布回归变体表现最佳。", "text_Arabic": "بالنسبة لمهمة التعرف على الكيانات المسماة، نجد أيضًا أن KinyaBERT يقدم أفضل أداء مع أداء متغير انحدار توزيع الملحق بشكل أفضل.", "text_French": "Pour la tâche de reconnaissance d'entités nommées, nous constatons également que KinyaBERT donne les meilleures performances, la variante de régression de distribution d'affixes étant la plus performante.", "text_Japanese": "固有表現認識タスクでは、接辞分布回帰バリアントのパフォーマンスが最高であり、KinyaBERT が最高のパフォーマンスを提供することもわかりました。", "text_Russian": "Для задачи распознавания именованного объекта мы также обнаружили, что KinyaBERT обеспечивает наилучшую производительность, а вариант регрессии распределения аффиксов работает лучше всего."}
{"text": "These results are also averages of ten finetuning runs.\n", "text_Chinese": "这些结果也是十次微调运行的平均值。", "text_Arabic": "هذه النتائج هي أيضًا متوسطات لعشر عمليات ضبط دقيقة.", "text_French": "Ces résultats sont également des moyennes de dix cycles de réglage fin.", "text_Japanese": "これらの結果は、10 回の微調整実行の平均でもあります。", "text_Russian": "Эти результаты также являются средними значениями десяти прогонов точной настройки."}
{"text": "For the news categorization task, we find mixed results.\n", "text_Chinese": "对于新闻分类任务，我们发现结果好坏参半。", "text_Arabic": "بالنسبة لمهمة تصنيف الأخبار، نجد نتائج مختلطة.", "text_French": "Pour la tâche de catégorisation des actualités, nous trouvons des résultats mitigés.", "text_Japanese": "ニュースの分類タスクでは、さまざまな結果が得られます。", "text_Russian": "Для задачи категоризации новостей мы получили неоднозначные результаты."}
{"text": "Previous work on text classification for Kinyarwanda had found that simple keyword detection is mostly enough for solving this specific task.\n", "text_Chinese": "之前有关基尼亚卢旺达语文本分类的工作发现，简单的关键字检测足以解决这一特定任务。", "text_Arabic": "وقد وجد العمل السابق على تصنيف النص للغة الكينيارواندا أن الكشف البسيط عن الكلمات الرئيسية يكفي في الغالب لحل هذه المهمة المحددة.", "text_French": "Des travaux antérieurs sur la classification de textes en kinyarwanda avaient montré qu'une simple détection de mots-clés était largement suffisante pour résoudre cette tâche spécifique.", "text_Japanese": "ルワンダのテキスト分類に関する以前の研究では、この特定のタスクを解決するには単純なキーワード検出でほとんど十分であることが判明しました。", "text_Russian": "Предыдущая работа по классификации текста для киньяруанда показала, что простого обнаружения ключевых слов в большинстве случаев достаточно для решения этой конкретной задачи."}
{"text": "Therefore, there is less gain from using pretrained language models.\n", "text_Chinese": "因此，使用预训练语言模型的收益较少。", "text_Arabic": "لذلك، هناك مكاسب أقل من استخدام نماذج اللغة المدربة مسبقًا.", "text_French": "Par conséquent, l’utilisation de modèles linguistiques pré-entraînés présente moins d’avantages.", "text_Japanese": "したがって、事前トレーニングされた言語モデルを使用しても得られる効果はほとんどありません。", "text_Russian": "Следовательно, использование предварительно обученных языковых моделей дает меньшую выгоду."}
{"text": "On this particular task of news categorization.\n", "text_Chinese": "关于新闻分类这一特殊任务。", "text_Arabic": "في هذه المهمة المحددة لتصنيف الأخبار.", "text_French": "Sur cette tâche particulière de catégorisation des informations.", "text_Japanese": "ニュースの分類というこの特定のタスクについて。", "text_Russian": "Об этой конкретной задаче категоризации новостей."}
{"text": "We also conducted an ablation study to see if there are alternative structures that improve performance.\n", "text_Chinese": "我们还进行了一项消融研究，看看是否有替代结构可以提高性能。", "text_Arabic": "لقد أجرينا أيضًا دراسة استئصال لمعرفة ما إذا كانت هناك هياكل بديلة تعمل على تحسين الأداء.", "text_French": "Nous avons également mené une étude d'ablation pour voir s'il existe des structures alternatives qui améliorent les performances.", "text_Japanese": "また、パフォーマンスを向上させる代替構造があるかどうかを確認するためにアブレーション研究も実施しました。", "text_Russian": "Мы также провели исследование абляции, чтобы выяснить, существуют ли альтернативные структуры, улучшающие производительность."}
{"text": "For the GLUE benchmark, we find that using affix sets consistently performs better, while affix probability regression objective yields the best performance on named entity recognition.\n", "text_Chinese": "对于 GLUE 基准，我们发现使用词缀集始终表现更好，而词缀概率回归目标在命名实体识别上产生最佳性能。", "text_Arabic": "بالنسبة لمعيار GLUE، نجد أن استخدام مجموعات الملحقات يؤدي باستمرار إلى أداء أفضل، في حين أن هدف انحدار احتمالية الملحقات يحقق أفضل أداء في التعرف على الكيانات المسماة.", "text_French": "Pour le benchmark GLUE, nous constatons que l'utilisation d'ensembles d'affixes donne systématiquement de meilleurs résultats, tandis que l'objectif de régression de probabilité d'affixe donne les meilleures performances en matière de reconnaissance d'entités nommées.", "text_Japanese": "GLUE ベンチマークでは、接辞セットを使用すると一貫してパフォーマンスが向上し、固有表現認識では接辞確率回帰目標が最高のパフォーマンスをもたらすことがわかりました。", "text_Russian": "В тесте GLUE мы обнаружили, что использование наборов аффиксов стабильно работает лучше, а цель регрессии вероятности аффиксов дает лучшую производительность при распознавании именованных объектов."}
{"text": "Also by looking at the low scores for finetuning, we find that KinyaBERT has better convergence in most cases.\n", "text_Chinese": "另外，通过观察微调的低分，我们发现 KinyaBERT 在大多数情况下具有更好的收敛性。", "text_Arabic": "ومن خلال النظر أيضًا إلى الدرجات المنخفضة للضبط الدقيق، نجد أن KinyaBERT لديها تقارب أفضل في معظم الحالات.", "text_French": "En examinant également les faibles scores de réglage fin, nous constatons que KinyaBERT a une meilleure convergence dans la plupart des cas.", "text_Japanese": "また、微調整の低いスコアを見ると、ほとんどの場合、KinyaBERT の方が収束が優れていることがわかります。", "text_Russian": "Кроме того, глядя на низкие оценки за тонкую настройку, мы обнаруживаем, что KinyaBERT в большинстве случаев имеет лучшую сходимость."}
{"text": "So to conclude, this work has demonstrated the effectiveness of explicitly using morphological information in pretrained language models.\n", "text_Chinese": "总而言之，这项工作证明了在预训练语言模型中显式使用形态信息的有效性。", "text_Arabic": "في الختام، أثبت هذا العمل فعالية الاستخدام الصريح للمعلومات الصرفية في نماذج اللغة المدربة مسبقًا.", "text_French": "En conclusion, ce travail a démontré l’efficacité de l’utilisation explicite des informations morphologiques dans des modèles linguistiques pré-entraînés.", "text_Japanese": "結論として、この研究は、事前学習済み言語モデルで形態素情報を明示的に使用することの有効性を実証しました。", "text_Russian": "Итак, в заключение, эта работа продемонстрировала эффективность явного использования морфологической информации в предварительно обученных языковых моделях."}
{"text": "The proposed two tier transformer encoder architecture enables capturing morphological complexity morphological compositionality, which is an important aspect of morphologically rich languages.\n", "text_Chinese": "所提出的两层变压器编码器架构能够捕获形态复杂性和形态组合性​​，这是形态丰富的语言的一个重要方面。", "text_Arabic": "تتيح بنية تشفير المحولات المقترحة ذات المستويين التقاط التعقيد المورفولوجي والتركيب المورفولوجي، وهو جانب مهم من اللغات الغنية شكليا.", "text_French": "L'architecture d'encodeur de transformateur à deux niveaux proposée permet de capturer la complexité morphologique de la composition morphologique, qui est un aspect important des langues morphologiquement riches.", "text_Japanese": "提案された 2 層トランスフォーマー エンコーダ アーキテクチャにより、形態学的に豊富な言語の重要な側面である形態学的複雑性、形態学的構成性を捉えることが可能になります。", "text_Russian": "Предлагаемая двухуровневая архитектура преобразователя кодирования позволяет улавливать морфологическую сложность, морфологическую композиционность, которая является важным аспектом морфологически богатых языков."}
{"text": "These findings should motivate further research into morphology aware language pretrained language models.\n", "text_Chinese": "这些发现应该会激发对形态感知语言预训练语言模型的进一步研究。", "text_Arabic": "يجب أن تحفز هذه النتائج على إجراء مزيد من البحث في نماذج اللغة المدربة مسبقًا على علم التشكل.", "text_French": "Ces résultats devraient motiver des recherches plus approfondies sur les modèles linguistiques pré-entraînés sensibles à la morphologie.", "text_Japanese": "これらの発見は、形態学を意識した言語の事前訓練済み言語モデルに関するさらなる研究の動機となるはずです。", "text_Russian": "Эти результаты должны мотивировать дальнейшие исследования предварительно обученных языковых моделей с учетом морфологии."}
{"text": "Hello, my name is Michał Pietruszka and it is my pleasure to present to you the paper titled Sparsifying Transformer Models with Trainable Representation Pooling.\n", "text_Chinese": "大家好，我的名字是 Michał Pietruszka，我很高兴向您展示题为“使用可训练表示池的稀疏化变压器模型”的论文。", "text_Arabic": "مرحبًا، اسمي Michał Pietruszka ويسعدني أن أقدم لكم الورقة التي تحمل عنوان Sparsifying Transformer Models with Trainable Representation Pooling.", "text_French": "Bonjour, je m'appelle Michał Pietruszka et j'ai le plaisir de vous présenter l'article intitulé Sparsifying Transformer Models with Trainable Representation Pooling.", "text_Japanese": "こんにちは、私の名前は Michał Pietruszka です。トレーニング可能な表現プーリングによるスパース化トランスフォーマー モデルというタイトルの論文を発表できることを光栄に思います。", "text_Russian": "Здравствуйте, меня зовут Михал Петрушка, и я рад представить вам статью под названием «Разреженные модели трансформаторов с помощью обучаемого пула представлений»."}
{"text": "A work done at Applica AI in cooperation with Lukasz Borchmann and Lukasz Garncarek.\n", "text_Chinese": "Applica AI 与 Lukasz Borchmann 和 Lukasz Garncarek 合作完成的工作。", "text_Arabic": "عمل تم إنجازه في Applica AI بالتعاون مع Lukasz Borchmann وLukasz Garncarek.", "text_French": "Un travail réalisé chez Applica AI en coopération avec Lukasz Borchmann et Lukasz Garncarek.", "text_Japanese": "Appplica AI で Lukasz Borchmann および Lukasz Garncarek と協力して行われた研究。", "text_Russian": "Работа, выполненная в Applica AI в сотрудничестве с Лукашем Борхманном и Лукашом Гарнчареком."}
{"text": "Let me start with the problems our work targets.\n", "text_Chinese": "我先说一下我们工作中存在的问题。", "text_Arabic": "اسمحوا لي أن أبدأ بالمشاكل التي يستهدفها عملنا.", "text_French": "Permettez-moi de commencer par les problèmes ciblés par notre travail.", "text_Japanese": "私たちの仕事が対象とする問題から始めましょう。", "text_Russian": "Позвольте мне начать с проблем, над которыми мы работаем."}
{"text": "Our method works well for the cases where long inputs are considered.\n", "text_Chinese": "我们的方法适用于考虑长输入的情况。", "text_Arabic": "تعمل طريقتنا بشكل جيد في الحالات التي يتم فيها أخذ المدخلات الطويلة في الاعتبار.", "text_French": "Notre méthode fonctionne bien pour les cas où des entrées longues sont considérées.", "text_Japanese": "私たちの方法は、長い入力が考慮される場合にうまく機能します。", "text_Russian": "Наш метод хорошо работает в случаях, когда рассматриваются длинные входные данные."}
{"text": "Roughly speaking, it is meant for the task orders and input of over two thousand tokens and the targets are shorter than the provided inputs.\n", "text_Chinese": "粗略地说，它是针对超过两千个令牌的任务订单和输入，并且目标比提供的输入短。", "text_Arabic": "بشكل تقريبي، فهو مخصص لأوامر المهام وإدخال أكثر من ألفي رمز مميز والأهداف أقصر من المدخلات المقدمة.", "text_French": "En gros, il est destiné aux commandes de tâches et à la saisie de plus de deux mille jetons et les cibles sont plus courtes que les entrées fournies.", "text_Japanese": "大まかに言うと、タスクの指示と 2,000 トークンを超える入力を対象としており、ターゲットは提供された入力よりも短いです。", "text_Russian": "Грубо говоря, он рассчитан на задачи и ввод более двух тысяч токенов, а цели короче предоставленных входов."}
{"text": "This has some specific applications in NLP.\n", "text_Chinese": "这在 NLP 中有一些特定的应用。", "text_Arabic": "هذا له بعض التطبيقات المحددة في البرمجة اللغوية العصبية.", "text_French": "Cela a des applications spécifiques en PNL.", "text_Japanese": "これには、NLP での特定の応用例がいくつかあります。", "text_Russian": "Это имеет некоторые конкретные применения в НЛП."}
{"text": "For example, one can imagine that given a long document, there's a need to summarize it, classify, answer the question about it, extract information or some key phrases.\n", "text_Chinese": "例如，可以想象，给定一份很长的文档，需要对其进行总结、分类、回答有关它的问题、提取信息或一些关键短语。", "text_Arabic": "على سبيل المثال، يمكن للمرء أن يتخيل أنه بالنظر إلى مستند طويل، هناك حاجة إلى تلخيصه وتصنيفه والإجابة على السؤال المتعلق به واستخراج المعلومات أو بعض العبارات الرئيسية.", "text_French": "Par exemple, on peut imaginer que face à un document long, il soit nécessaire de le résumer, de le classer, de répondre à la question le concernant, d'en extraire des informations ou des phrases clés.", "text_Japanese": "たとえば、長い文書が与えられた場合、それを要約し、分類し、それに関する質問に答え、情報やいくつかのキーフレーズを抽出する必要があると想像できます。", "text_Russian": "Например, можно представить, что, имея длинный документ, необходимо его резюмировать, классифицировать, ответить на вопрос о нем, извлечь информацию или какие-то ключевые фразы."}
{"text": "Let me recall the vanilla transformer and our and its issue of its attention complexity that depends on the square of the input line.\n", "text_Chinese": "让我回想一下普通变压器和我们的注意力复杂度问题，该复杂度取决于输入线的平方。", "text_Arabic": "اسمحوا لي أن أذكر محول الفانيليا وقضيتنا الخاصة بتعقيد الانتباه الذي يعتمد على مربع خط الإدخال.", "text_French": "Permettez-moi de rappeler le transformateur vanille et notre problème de complexité d'attention qui dépend du carré de la ligne d'entrée.", "text_Japanese": "バニラトランスと、入力ラインの二乗に依存する注意の複雑さの問題を思い出してください。", "text_Russian": "Позвольте мне вспомнить ванильный трансформатор и нашу с ним проблему сложности его внимания, которая зависит от квадрата входной строки."}
{"text": "In the vanilla transformer, with full attention connectivity, relations of each token to every other token have to be calculated.\n", "text_Chinese": "在普通 Transformer 中，通过充分的注意力连接，必须计算每个标记与其他标记的关系。", "text_Arabic": "في محول الفانيليا، مع اتصال الاهتمام الكامل، يجب حساب علاقات كل رمز مميز بكل رمز مميز آخر.", "text_French": "Dans le transformateur Vanilla, avec une connectivité d'attention totale, les relations de chaque jeton avec tous les autres jetons doivent être calculées.", "text_Japanese": "バニラトランスフォーマーでは、完全な注意接続を使用して、各トークンと他のすべてのトークンの関係を計算する必要があります。", "text_Russian": "В ванильном преобразователе с возможностью подключения полного внимания необходимо рассчитать отношения каждого токена с каждым другим токеном."}
{"text": "The computational complexity of attention, this depends on the number of layers l, sequence length n, another sequence length, and the dimensionality of representations.\n", "text_Chinese": "注意力的计算复杂度，这取决于层数 l、序列长度 n、另一个序列长度以及表示的维数。", "text_Arabic": "التعقيد الحسابي للانتباه، يعتمد على عدد الطبقات l، وطول التسلسل n، وطول تسلسل آخر، وأبعاد التمثيلات.", "text_French": "La complexité informatique de l'attention dépend du nombre de couches l, de la longueur de séquence n, d'une autre longueur de séquence et de la dimensionnalité des représentations.", "text_Japanese": "注意すべき計算の複雑さは、層の数 l、シーケンスの長さ n、別のシーケンスの長さ、および表現の次元に依存します。", "text_Russian": "Вычислительная сложность внимания зависит от количества слоев l, длины последовательности n, длины другой последовательности и размерности представлений."}
{"text": "Similarly, in the decoder's cross attention, to this picture on the right side, the only difference here is that the target tokens are attending to the input tokens in this case.\n", "text_Chinese": "类似地，在解码器的交叉注意力中，对于右侧的这张图片，这里唯一的区别是目标标记在这种情况下关注输入标记。", "text_Arabic": "وبالمثل، في انتباه وحدة فك التشفير، إلى هذه الصورة على الجانب الأيمن، فإن الاختلاف الوحيد هنا هو أن الرموز المميزة المستهدفة تهتم برموز الإدخال في هذه الحالة.", "text_French": "De même, dans l'attention croisée du décodeur, sur cette image du côté droit, la seule différence ici est que les jetons cibles s'occupent des jetons d'entrée dans ce cas.", "text_Japanese": "同様に、右側の図に対するデコーダのクロス アテンションでは、唯一の違いは、この場合ターゲット トークンが入力トークンに注目していることです。", "text_Russian": "Аналогично, при перекрестном внимании декодера к этому изображению справа, единственная разница здесь состоит в том, что в этом случае целевые токены обслуживают входные токены."}
{"text": "Which can be seen also in this formula.\n", "text_Chinese": "从这个公式中也可以看出这一点。", "text_Arabic": "والذي يمكن رؤيته أيضًا في هذه الصيغة.", "text_French": "Ce qui se voit également dans cette formule.", "text_Japanese": "それはこの式からもわかります。", "text_Russian": "Что можно увидеть и в этой формуле."}
{"text": "The BLEU score represents relations that have to be calculated.\n", "text_Chinese": "BLEU 分数表示必须计算的关系。", "text_Arabic": "تمثل درجة BLEU العلاقات التي يجب حسابها.", "text_French": "Le score BLEU représente les relations qui doivent être calculées.", "text_Japanese": "BLEU スコアは、計算する必要がある関係を表します。", "text_Russian": "Оценка BLEU представляет отношения, которые необходимо рассчитать."}
{"text": "In case of the full attention, we need to calculate every relations within the input sequence.\n", "text_Chinese": "在充分注意的情况下，我们需要计算输入序列中的每个关系。", "text_Arabic": "في حالة الاهتمام الكامل، نحتاج إلى حساب كل العلاقات ضمن تسلسل الإدخال.", "text_French": "En cas de pleine attention, nous devons calculer toutes les relations au sein de la séquence d'entrée.", "text_Japanese": "完全な注意の場合、入力シーケンス内のすべての関係を計算する必要があります。", "text_Russian": "В случае полного внимания нам необходимо вычислить все отношения внутри входной последовательности."}
{"text": "Now, we see what happens when we have a blockwise encoder that works by limiting the tokens connectivity so that they can only see other nearby tokens.\n", "text_Chinese": "现在，我们看看当我们有一个块式编码器时会发生什么，该编码器通过限制令牌连接性来工作，以便它们只能看到其他附近的令牌。", "text_Arabic": "الآن، نرى ما يحدث عندما يكون لدينا برنامج تشفير يعمل عن طريق الحد من اتصال الرموز المميزة بحيث يمكنهم فقط رؤية الرموز المميزة الأخرى القريبة.", "text_French": "Maintenant, nous voyons ce qui se passe lorsque nous avons un encodeur par blocs qui fonctionne en limitant la connectivité des jetons afin qu'ils ne puissent voir que les autres jetons à proximité.", "text_Japanese": "ここで、近くにある他のトークンのみを認識できるようにトークンの接続を制限することで機能するブロック単位のエンコーダーがある場合に何が起こるかを見てみましょう。", "text_Russian": "Теперь мы видим, что происходит, когда у нас есть блочный кодер, который ограничивает возможности подключения токенов, чтобы они могли видеть только другие близлежащие токены."}
{"text": "The text is read in chunks which can drastically reduce the number of computations on the encoder side, but does not improve the decoder's cross attention as every input token is passed to the decoder anyway.\n", "text_Chinese": "文本以块的形式读取，这可以大大减少编码器端的计算数量，但不会提高解码器的交叉注意力，因为每个输入令牌无论如何都会传递到解码器。", "text_Arabic": "تتم قراءة النص في أجزاء مما يمكن أن يقلل بشكل كبير من عدد العمليات الحسابية على جانب جهاز التشفير، ولكنه لا يحسن الانتباه المتبادل لوحدة فك التشفير حيث يتم تمرير كل رمز إدخال إلى وحدة فك التشفير على أي حال.", "text_French": "Le texte est lu en morceaux, ce qui peut réduire considérablement le nombre de calculs du côté du codeur, mais n'améliore pas l'attention croisée du décodeur puisque chaque jeton d'entrée est de toute façon transmis au décodeur.", "text_Japanese": "テキストはチャンクで読み取られるため、エンコーダ側の計算数は大幅に削減できますが、いずれにしてもすべての入力トークンがデコーダに渡されるため、デコーダのクロス アテンションは改善されません。", "text_Russian": "Текст читается порциями, что может значительно сократить количество вычислений на стороне кодера, но не улучшает перекрестное внимание декодера, поскольку каждый входной токен все равно передается декодеру."}
{"text": "This method is often referred to as fusion in decoder.\n", "text_Chinese": "这种方法通常被称为解码器中的融合。", "text_Arabic": "غالبًا ما يشار إلى هذه الطريقة باسم الانصهار في وحدة فك التشفير.", "text_French": "Cette méthode est souvent appelée fusion dans le décodeur.", "text_Japanese": "この方法は、デコーダにおけるフュージョンと呼ばれることがよくあります。", "text_Russian": "Этот метод часто называют слиянием в декодере."}
{"text": "The improvement here can be interpreted as changing one of the dependencies of n to another constant m representing the block size.\n", "text_Chinese": "这里的改进可以解释为将 n 的依赖关系之一更改为表示块大小的另一个常数 m。", "text_Arabic": "يمكن تفسير التحسين هنا على أنه تغيير إحدى تبعيات n إلى ثابت آخر m يمثل حجم الكتلة.", "text_French": "L'amélioration ici peut être interprétée comme le changement de l'une des dépendances de n en une autre constante m représentant la taille du bloc.", "text_Japanese": "ここでの改善は、n の依存関係の 1 つをブロック サイズを表す別の定数 m に変更することと解釈できます。", "text_Russian": "Улучшение здесь можно интерпретировать как изменение одной из зависимостей n на другую константу m, представляющую размер блока."}
{"text": "Our key observation is that most tokens are irrelevant for a wide variety of tasks and can be almost completely disregarded. This is exemplified on the slide.\n", "text_Chinese": "我们的主要观察结果是，大多数令牌与各种任务无关，几乎可以完全忽略。幻灯片上对此进行了举例说明。", "text_Arabic": "ملاحظتنا الرئيسية هي أن معظم الرموز المميزة ليست ذات صلة بمجموعة واسعة من المهام ويمكن تجاهلها تمامًا تقريبًا. ويتجلى هذا على الشريحة.", "text_French": "Notre principale observation est que la plupart des jetons ne sont pas pertinents pour une grande variété de tâches et peuvent être presque totalement ignorés. Ceci est illustré sur la diapositive.", "text_Japanese": "私たちの重要な観察は、ほとんどのトークンはさまざまなタスクには無関係であり、ほぼ完全に無視できるということです。これはスライドに例示されています。", "text_Russian": "Наше ключевое наблюдение заключается в том, что большинство токенов не имеют отношения к широкому спектру задач и ими можно практически полностью пренебречь. Пример этого представлен на слайде."}
{"text": "The only parts of the inputs are relevant to the desired output.\n", "text_Chinese": "输入的唯一部分与所需的输出相关。", "text_Arabic": "الأجزاء الوحيدة من المدخلات ذات الصلة بالمخرجات المطلوبة.", "text_French": "Les seules parties des entrées sont pertinentes pour la sortie souhaitée.", "text_Japanese": "入力の一部のみが目的の出力に関連します。", "text_Russian": "Только части входных данных имеют отношение к желаемому результату."}
{"text": "For example.\n", "text_Chinese": "例如。", "text_Arabic": "على سبيل المثال.", "text_French": "Par exemple.", "text_Japanese": "例えば。", "text_Russian": "Например."}
{"text": "One can read an article once marking the most important parts with a highlighter, and then produce a summary based on this part from the middle stage only.\n", "text_Chinese": "读完一篇文章，先用荧光笔标记出最重要的部分，然后只从中间阶段就根据这部分进行总结。", "text_Arabic": "يمكن للمرء قراءة مقال مرة واحدة مع تحديد الأجزاء الأكثر أهمية بقلم تمييز، ومن ثم إنتاج ملخص بناءً على هذا الجزء من المرحلة المتوسطة فقط.", "text_French": "On peut lire un article une fois en marquant les parties les plus importantes avec un surligneur, puis produire un résumé basé sur cette partie uniquement à partir de l'étape intermédiaire.", "text_Japanese": "記事を一度読み、最も重要な部分を蛍光ペンでマーキングし、途中からはその部分だけを基に要約を作成することができます。", "text_Russian": "Можно прочитать статью, один раз отметив наиболее важные части маркером, а затем уже на основе этой части составить резюме, начиная с среднего этапа."}
{"text": "The cost of highlighting and deciding if the current token is essential to produce the summary is thus cheap and depends only on the token's representation.\n", "text_Chinese": "因此，突出显示并确定当前标记是否对于生成摘要至关重要的成本很便宜，并且仅取决于标记的表示。", "text_Arabic": "وبالتالي فإن تكلفة تسليط الضوء على الرمز المميز الحالي وتحديد ما إذا كان ضروريًا لإنتاج الملخص رخيصة وتعتمد فقط على تمثيل الرمز المميز.", "text_French": "Le coût de mise en évidence et de décision si le jeton actuel est essentiel pour produire le résumé est donc peu coûteux et dépend uniquement de la représentation du jeton.", "text_Japanese": "したがって、強調表示し、現在のトークンが要約を生成するために不可欠であるかどうかを決定するコストは安く、トークンの表現のみに依存します。", "text_Russian": "Таким образом, стоимость выделения и принятия решения о том, необходим ли текущий токен для создания сводки, невелика и зависит только от представления токена."}
{"text": "The pooling of the highlighted tokens is possible.\n", "text_Chinese": "突出显示的代币的池化是可能的。", "text_Arabic": "من الممكن تجميع الرموز المميزة.", "text_French": "La mutualisation des tokens mis en avant est possible.", "text_Japanese": "強調表示されたトークンのプールが可能です。", "text_Russian": "Возможно объединение выделенных токенов."}
{"text": "Thanks to our top k operator and its cost is negligible.\n", "text_Chinese": "感谢我们的 top k 运营商，其成本可以忽略不计。", "text_Arabic": "بفضل أفضل مشغل k لدينا وتكلفته لا تذكر.", "text_French": "Merci à notre opérateur top k et son coût est négligeable.", "text_Japanese": "トップ k のオペレータのおかげで、そのコストは無視できます。", "text_Russian": "Спасибо нашему топ-к оператору и стоимость его незначительна."}
{"text": "The cost of producing a summary from a shortened input is also much lower than in the vanilla model when the whole input is considered.\n", "text_Chinese": "当考虑整个输入时，从缩短的输入生成摘要的成本也比普通模型低得多。", "text_Arabic": "كما أن تكلفة إنتاج ملخص من مدخلات مختصرة هي أيضًا أقل بكثير مما هي عليه في نموذج الفانيليا عند أخذ المدخلات بأكملها في الاعتبار.", "text_French": "Le coût de production d’un résumé à partir d’une entrée raccourcie est également bien inférieur à celui du modèle vanille lorsque l’ensemble de l’entrée est pris en compte.", "text_Japanese": "短縮された入力から要約を生成するコストも、入力全体を考慮した場合、バニラ モデルよりもはるかに低くなります。", "text_Russian": "Стоимость создания сводки на основе сокращенных входных данных также намного ниже, чем в стандартной модели, когда рассматриваются все входные данные."}
{"text": "But here's a question.\n", "text_Chinese": "但这里有一个问题。", "text_Arabic": "ولكن هنا سؤال.", "text_French": "Mais voici une question.", "text_Japanese": "しかし、ここで質問があります。", "text_Russian": "Но вот вопрос."}
{"text": "How to select important tokens and backpropagate gradients to that selection?\n", "text_Chinese": "如何选择重要的标记并将梯度反向传播到该选择？", "text_Arabic": "كيفية تحديد الرموز المهمة والتدرجات العكسية لهذا التحديد؟", "text_French": "Comment sélectionner des jetons importants et rétropropager les dégradés vers cette sélection ?", "text_Japanese": "重要なトークンを選択し、その選択に対して勾配を逆伝播する方法は?", "text_Russian": "Как выбрать важные токены и применить обратное распространение градиентов к этому выбору?"}
{"text": "The essential underlying problem that we solve is to propose the trainable selection mechanism.\n", "text_Chinese": "我们解决的根本问题是提出可训练的选择机制。", "text_Arabic": "المشكلة الأساسية الأساسية التي نحلها هي اقتراح آلية الاختيار القابلة للتدريب.", "text_French": "Le problème sous-jacent essentiel que nous résolvons est de proposer le mécanisme de sélection entraînable.", "text_Japanese": "私たちが解決する本質的な根本的な問題は、訓練可能な選択メカニズムを提案することです。", "text_Russian": "Основная основная проблема, которую мы решаем, — предложить обучаемый механизм отбора."}
{"text": "One that can allow for gradient to be back propagated during the training so that the network can learn to select the most important tokens.\n", "text_Chinese": "一种可以允许梯度在训练期间反向传播的方法，以便网络可以学习选择最重要的标记。", "text_Arabic": "واحدة يمكن أن تسمح بإعادة نشر التدرج أثناء التدريب حتى تتمكن الشبكة من تعلم كيفية اختيار الرموز المميزة الأكثر أهمية.", "text_French": "Celui qui peut permettre au gradient de se propager à nouveau pendant la formation afin que le réseau puisse apprendre à sélectionner les jetons les plus importants.", "text_Japanese": "これは、ネットワークが最も重要なトークンの選択を学習できるように、トレーニング中に勾配を逆伝播できるようにするものです。", "text_Russian": "Тот, который может обеспечить обратное распространение градиента во время обучения, чтобы сеть могла научиться выбирать наиболее важные токены."}
{"text": "More precisely\n", "text_Chinese": "更确切地说", "text_Arabic": "أكثر دقة", "text_French": "Plus précisément", "text_Japanese": "より正確に", "text_Russian": "Точнее"}
{"text": "Given some embeddings underscore obtained from a simple linear layer, the task is to return the highest scoring embeddings. First, the sequence is permuted and pairs are prepared so that the higher scoring vector is taken with the lower scoring one.\n", "text_Chinese": "给定从简单线性层获得的一些嵌入下划线，任务是返回得分最高的嵌入。首先，对序列进行排列并准备对，以便将得分较高的向量与得分较低的向量一起采用。", "text_Arabic": "بالنظر إلى الشرطة السفلية لبعض التضمينات التي تم الحصول عليها من طبقة خطية بسيطة، فإن المهمة هي إرجاع التضمينات ذات أعلى الدرجات. أولاً، يتم تبديل التسلسل وإعداد الأزواج بحيث يتم أخذ ناقل الدرجات الأعلى مع ناقل الدرجات الأقل.", "text_French": "Étant donné le soulignement de certains plongements obtenus à partir d'une simple couche linéaire, la tâche consiste à renvoyer les plongements ayant le score le plus élevé. Tout d’abord, la séquence est permutée et les paires sont préparées de manière à ce que le vecteur de score le plus élevé soit pris avec celui qui a le score le plus faible.", "text_Japanese": "単純な線形層から取得されたいくつかの埋め込みアンダースコアが与えられた場合、タスクは最高スコアの埋め込みを返すことです。まず、シーケンスが並べ替えられ、スコアの高いベクトルがスコアの低いベクトルと取られるようにペアが準備されます。", "text_Russian": "Учитывая, что некоторые вложения подчеркивания получены из простого линейного слоя, задача состоит в том, чтобы вернуть вложения с наивысшей оценкой. Сначала последовательность переставляется и подготавливаются пары так, чтобы вектор с более высокой оценкой брался вместе с вектором с более низкой оценкой."}
{"text": "Next, weights are calculated using boosted softmax over scores.\n", "text_Chinese": "接下来，使用基于分数的 boosted softmax 来计算权重。", "text_Arabic": "بعد ذلك، يتم حساب الأوزان باستخدام softmax المعزز على الدرجات.", "text_French": "Ensuite, les poids sont calculés en utilisant le softmax amélioré sur les scores.", "text_Japanese": "次に、スコアに対するブーストされたソフトマックスを使用して重みが計算されます。", "text_Russian": "Затем веса рассчитываются с использованием повышения softmax по баллам."}
{"text": "After each tournament round, new vectors and scores are composed as a linear combination of those pairs with the obtained weights.\n", "text_Chinese": "每轮比赛结束后，新的向量和分数将由这些对与所获得的权重的线性组合组成。", "text_Arabic": "بعد كل جولة من جولات البطولة، يتم تكوين المتجهات والنتائج الجديدة كمجموعة خطية من تلك الأزواج مع الأوزان التي تم الحصول عليها.", "text_French": "Après chaque tour de tournoi, de nouveaux vecteurs et scores sont composés comme une combinaison linéaire de ces paires avec les poids obtenus.", "text_Japanese": "各トーナメント ラウンドの後、新しいベクトルとスコアが、取得された重みを備えたそれらのペアの線形結合として構成されます。", "text_Russian": "После каждого раунда турнира новые векторы и баллы составляются как линейная комбинация этих пар с полученными весами."}
{"text": "So in short, we combine them linearly by performing a softmax over their scores.\n", "text_Chinese": "简而言之，我们通过对它们的分数执行 softmax 来线性组合它们。", "text_Arabic": "باختصار، نحن نجمعهم خطيًا عن طريق إجراء تأثير softmax على درجاتهم.", "text_French": "Bref, on les combine linéairement en effectuant un softmax sur leurs scores.", "text_Japanese": "つまり、スコアに対してソフトマックスを実行することで、それらを線形に結合します。", "text_Russian": "Короче говоря, мы объединяем их линейно, выполняя softmax по их оценкам."}
{"text": "And while combining two tokens, some noise can be produces produced.\n", "text_Chinese": "并且在组合两个令牌时，可能会产生一些噪音。", "text_Arabic": "وأثناء الجمع بين رمزين، يمكن إنتاج بعض الضوضاء.", "text_French": "Et en combinant deux jetons, du bruit peut être produit.", "text_Japanese": "2 つのトークンを組み合わせると、ノイズが発生する可能性があります。", "text_Russian": "И при объединении двух токенов может возникнуть некоторый шум."}
{"text": "But it also allows the gradients to be propagated to all input embeddings.\n", "text_Chinese": "但它也允许将梯度传播到所有输入嵌入。", "text_Arabic": "ولكنه يسمح أيضًا بنشر التدرجات إلى جميع عمليات تضمين الإدخال.", "text_French": "Mais cela permet également aux dégradés d'être propagés à tous les intégrations d'entrée.", "text_Japanese": "ただし、勾配をすべての入力埋め込みに伝播することもできます。", "text_Russian": "Но это также позволяет распространять градиенты на все входные внедрения."}
{"text": "In short, a trainable top k we propose is based on performing a tournament like soft selection at each step.\n", "text_Chinese": "简而言之，我们提出的可训练的 top k 是基于在每一步执行类似软选择的锦标赛。", "text_Arabic": "باختصار، يعتمد الجزء العلوي القابل للتدريب الذي نقترحه على أداء دورة مثل الاختيار البسيط في كل خطوة.", "text_French": "En bref, un top k entraînable que nous proposons est basé sur la réalisation d'un tournoi de type sélection douce à chaque étape.", "text_Japanese": "つまり、私たちが提案するトレーニング可能なトップ k は、各ステップでのソフト選択のようなトーナメントの実行に基づいています。", "text_Russian": "Короче говоря, предлагаемый нами обучаемый топ k основан на проведении турнира, подобного мягкому отбору, на каждом этапе."}
{"text": "And from a different perspective, the representation pooling follows the encoder layer.\n", "text_Chinese": "从不同的角度来看，表示池遵循编码器层。", "text_Arabic": "ومن منظور مختلف، يتبع تجميع التمثيل طبقة التشفير.", "text_French": "Et d’un point de vue différent, le regroupement de représentations suit la couche d’encodeur.", "text_Japanese": "また、別の観点から見ると、表現プーリングはエンコーダー層に続きます。", "text_Russian": "С другой стороны, пул представлений следует за уровнем кодера."}
{"text": "First, each representation is scored and then only those with the highest scores are passed to the next layer.\n", "text_Chinese": "首先，对每个表示进行评分，然后只有得分最高的表示才会传递到下一层。", "text_Arabic": "أولاً، يتم تسجيل كل تمثيل ثم يتم تمرير فقط أولئك الذين حصلوا على أعلى الدرجات إلى الطبقة التالية.", "text_French": "Tout d’abord, chaque représentation est notée, puis seules celles ayant les scores les plus élevés sont transmises à la couche suivante.", "text_Japanese": "まず、各表現にスコアが付けられ、最も高いスコアを持つものだけが次の層に渡されます。", "text_Russian": "Сначала оценивается каждое представление, а затем на следующий уровень передаются только те, у кого самые высокие оценки."}
{"text": "Encoding can be performed as in standard transformer architecture on the full length input.\n", "text_Chinese": "可以像标准变压器架构一样对全长输入执行编码。", "text_Arabic": "يمكن تنفيذ التشفير كما هو الحال في بنية المحولات القياسية على المدخلات كاملة الطول.", "text_French": "Le codage peut être effectué comme dans une architecture de transformateur standard sur l'entrée pleine longueur.", "text_Japanese": "エンコードは、全長入力に対して標準のトランス アーキテクチャと同様に実行できます。", "text_Russian": "Кодирование может выполняться, как в стандартной архитектуре трансформатора, на полноразмерном входе."}
{"text": "It is however possible to process text in blocks of fixed length of fixed length and globally select the best representation.\n", "text_Chinese": "然而，可以以固定长度的块处理文本并全局选择最佳表示。", "text_Arabic": "ومع ذلك، فمن الممكن معالجة النص في كتل ذات طول ثابت وطول ثابت واختيار أفضل تمثيل عالميًا.", "text_French": "Il est cependant possible de traiter du texte par blocs de longueur fixe et de sélectionner globalement la meilleure représentation.", "text_Japanese": "ただし、テキストを固定長のブロックで処理し、最適な表現をグローバルに選択することは可能です。", "text_Russian": "Однако можно обрабатывать текст блоками фиксированной длины и глобально выбирать лучшее представление."}
{"text": "Here is an example of the representation pooling introduced after the encoder.\n", "text_Chinese": "这是编码器之后引入的表示池的示例。", "text_Arabic": "فيما يلي مثال على تجميع التمثيل الذي تم تقديمه بعد برنامج التشفير.", "text_French": "Voici un exemple de pooling de représentation introduit après l'encodeur.", "text_Japanese": "以下は、エンコーダーの後に導入された表現プーリングの例です。", "text_Russian": "Вот пример пула представлений, представленного после кодировщика."}
{"text": "This directly influenced the cause of cross attention, which depends not on the input length N, but the constant K, representing the pooled length.\n", "text_Chinese": "这直接影响了交叉注意力的原因，交叉注意力不取决于输入长度N，而是取决于表示池化长度的常数K。", "text_Arabic": "أثر هذا بشكل مباشر على سبب الانتباه المتقاطع، والذي لا يعتمد على طول الإدخال N، ولكن على الثابت K، الذي يمثل الطول المجمع.", "text_French": "Cela a directement influencé la cause de l’attention croisée, qui ne dépend pas de la longueur d’entrée N, mais de la constante K, représentant la longueur regroupée.", "text_Japanese": "これは、入力長 N ではなく、プールされた長さを表す定数 K に依存するクロス アテンションの原因に直接影響しました。", "text_Russian": "Это напрямую повлияло на причину перекрестного внимания, которая зависит не от входной длины N, а от константы K, представляющей объединенную длину."}
{"text": "This constant informs how many representations are selected and passed to the decoder.\n", "text_Chinese": "该常量告知有多少表示被选择并传递给解码器。", "text_Arabic": "يُعلم هذا الثابت عدد التمثيلات التي تم تحديدها وتمريرها إلى وحدة فك التشفير.", "text_French": "Cette constante indique combien de représentations sont sélectionnées et transmises au décodeur.", "text_Japanese": "この定数は、選択されてデコーダに渡される表現の数を通知します。", "text_Russian": "Эта константа сообщает, сколько представлений выбрано и передано в декодер."}
{"text": "Producing a summary from a shorter text is significantly cheaper than previous solution.\n", "text_Chinese": "从较短的文本生成摘要比以前的解决方案要便宜得多。", "text_Arabic": "يعد إنتاج ملخص من نص أقصر أرخص بكثير من الحل السابق.", "text_French": "Produire un résumé à partir d’un texte plus court est nettement moins cher que la solution précédente.", "text_Japanese": "短いテキストから要約を作成することは、以前のソリューションよりも大幅にコストがかかります。", "text_Russian": "Создание резюме из более короткого текста значительно дешевле, чем предыдущее решение."}
{"text": "As the sequence length can be shortened by a large factor.\n", "text_Chinese": "由于序列长度可以缩短很大的倍数。", "text_Arabic": "حيث يمكن تقصير طول التسلسل بعامل كبير.", "text_French": "Comme la longueur de la séquence peut être considérablement raccourcie.", "text_Japanese": "シーケンスの長さを大幅に短縮できるため。", "text_Russian": "Поскольку длина последовательности может быть сокращена в большой раз."}
{"text": "For example, we successfully used k of sixteen or even sixty times four or even sixty four times smaller than the value of n in our experiments.\n", "text_Chinese": "例如，在我们的实验中，我们成功地使用了比n值小十六倍甚至六十倍四倍甚至六十四倍的k。", "text_Arabic": "على سبيل المثال، نجحنا في استخدام k من ستة عشر أو حتى ستين مرة أربعة أو حتى أربعة وستين مرة أصغر من قيمة n في تجاربنا.", "text_French": "Par exemple, nous avons utilisé avec succès un k de seize, voire soixante fois quatre, voire soixante-quatre fois inférieur à la valeur de n dans nos expériences.", "text_Japanese": "たとえば、実験では、n の値の 16 倍、さらには 60 倍、さらには 64 倍小さい k を使用することに成功しました。", "text_Russian": "Например, в наших экспериментах мы успешно использовали k, в шестнадцать или даже в шестьдесят раз в четыре или даже в шестьдесят четыре раза меньший, чем значение n."}
{"text": "Please note that the beneficial impact of blockwise encoding and self attention is sustained.\n", "text_Chinese": "请注意，分块编码和自注意力的有益影响是持续的。", "text_Arabic": "يرجى ملاحظة أن التأثير المفيد للتشفير الكتلي والاهتمام الذاتي مستمر.", "text_French": "Veuillez noter que l’impact bénéfique du codage par blocs et de l’attention personnelle est durable.", "text_Japanese": "ブロック単位のエンコーディングとセルフアテンションの有益な効果は持続することに注意してください。", "text_Russian": "Обратите внимание, что благотворное влияние блочного кодирования и самоконтроля сохраняется."}
{"text": "Remember that the computational cost of attention depend on the square of the input length.\n", "text_Chinese": "请记住，注意力的计算成本取决于输入长度的平方。", "text_Arabic": "تذكر أن التكلفة الحسابية للانتباه تعتمد على مربع طول الإدخال.", "text_French": "N'oubliez pas que le coût de calcul de l'attention dépend du carré de la longueur d'entrée.", "text_Japanese": "アテンションの計算コストは​​入力長の 2 乗に依存することに注意してください。", "text_Russian": "Помните, что вычислительная стоимость внимания зависит от квадрата входной длины."}
{"text": "Reducing it the input earlier during the encoding process can significantly lower the costs.\n", "text_Chinese": "在编码过程中尽早减少输入可以显着降低成本。", "text_Arabic": "يمكن أن يؤدي تقليل المدخلات مبكرًا أثناء عملية الترميز إلى خفض التكاليف بشكل كبير.", "text_French": "Réduire l'entrée plus tôt pendant le processus d'encodage peut réduire considérablement les coûts.", "text_Japanese": "エンコードプロセスの早い段階で入力を減らすと、コストを大幅に削減できます。", "text_Russian": "Уменьшение ввода на более раннем этапе процесса кодирования может значительно снизить затраты."}
{"text": "For the pyramidion model, we narrowed down the size of the representation on the output of each of each chosen layer, leading to the exponential reduction of computational cost as the encoding proceeds.\n", "text_Chinese": "对于金字塔模型，我们缩小了每个所选层输出的表示大小，从而随着编码的进行，计算成本呈指数级减少。", "text_Arabic": "بالنسبة للنموذج الهرمي، قمنا بتضييق حجم التمثيل على مخرجات كل طبقة مختارة، مما أدى إلى انخفاض كبير في التكلفة الحسابية مع استمرار عملية التشفير.", "text_French": "Pour le modèle pyramidion, nous avons réduit la taille de la représentation sur la sortie de chacune des couches choisies, conduisant à une réduction exponentielle du coût de calcul au fur et à mesure de l'encodage.", "text_Japanese": "ピラミディオン モデルでは、選択した各レイヤーの出力の表現サイズを絞り込み、エンコードが進むにつれて計算コストが指数関数的に削減されました。", "text_Russian": "Для модели пирамидиона мы сузили размер представления на выходе каждого выбранного слоя, что привело к экспоненциальному снижению вычислительных затрат по мере продолжения кодирования."}
{"text": "As you can see, the total computational cost of a full encoder here is less than two times the cost of the full-sized first layer.\n", "text_Chinese": "正如您所看到的，这里完整编码器的总计算成本不到全尺寸第一层成本的两倍。", "text_Arabic": "كما ترون، فإن التكلفة الحسابية الإجمالية لجهاز التشفير الكامل هنا أقل من ضعف تكلفة الطبقة الأولى كاملة الحجم.", "text_French": "Comme vous pouvez le constater, le coût de calcul total d'un encodeur complet est ici inférieur à deux fois le coût de la première couche de taille réelle.", "text_Japanese": "ご覧のとおり、ここでのフル エンコーダーの合計計算コストは​​、フルサイズの最初のレイヤーのコストの 2 倍未満です。", "text_Russian": "Как видите, общие вычислительные затраты полного кодера здесь менее чем в два раза превышают стоимость полноразмерного первого слоя."}
{"text": "When pooling is introduced earlier, the sum of all purple squares is thus bounded to a constant, not dependent on the number of layers l.\n", "text_Chinese": "当早期引入池化时，所有紫色方块的总和因此限制为一个常数，而不依赖于层数 l。", "text_Arabic": "عندما تم تقديم التجميع سابقًا، فإن مجموع كل المربعات الأرجوانية يرتبط بثابت، ولا يعتمد على عدد الطبقات l.", "text_French": "Lorsque le regroupement est introduit plus tôt, la somme de tous les carrés violets est ainsi limitée à une constante, indépendante du nombre de couches l.", "text_Japanese": "プーリングが以前に導入された場合、すべての紫色の正方形の合計は、層 l の数に依存せず、定数に制限されます。", "text_Russian": "Таким образом, когда объединение вводится ранее, сумма всех фиолетовых квадратов ограничивается константой, не зависящей от количества слоев l."}
{"text": "But on the constant c, which can be influenced by the placing of the pooling layers within the network.\n", "text_Chinese": "但对于常数 c，它可能会受到网络中池化层放置的影响。", "text_Arabic": "ولكن على الثابت c، والذي يمكن أن يتأثر بوضع طبقات التجميع داخل الشبكة.", "text_French": "Mais sur la constante c, qui peut être influencée par la disposition des couches de pooling au sein du réseau.", "text_Japanese": "ただし、定数 c については、ネットワーク内のプーリング層の配置によって影響を受ける可能性があります。", "text_Russian": "А вот на константу c, на которую может влиять размещение слоев пула внутри сети."}
{"text": "Our improvements were benchmarked on eight thousand tokens long inputs.\n", "text_Chinese": "我们的改进以八千个代币长输入为基准。", "text_Arabic": "تم قياس التحسينات التي أجريناها على مدخلات طويلة تبلغ ثمانية آلاف رمز.", "text_French": "Nos améliorations ont été évaluées sur des entrées longues de huit mille jetons.", "text_Japanese": "私たちの改善は、8,000 トークンの長さの入力でベンチマークされました。", "text_Russian": "Наши улучшения были проверены на входных данных длиной в восемь тысяч токенов."}
{"text": "And the figure shows that when pooling is engaged, the best scalability for the network's depth is achieved.\n", "text_Chinese": "该图显示，当使用池化时，可以实现网络深度的最佳可扩展性。", "text_Arabic": "ويوضح الشكل أنه عند استخدام التجميع، يتم تحقيق أفضل قابلية للتوسعة لعمق الشبكة.", "text_French": "Et la figure montre que lorsque le pooling est activé, la meilleure évolutivité pour la profondeur du réseau est obtenue.", "text_Japanese": "この図は、プーリングを使用すると、ネットワークの深さに対して最適なスケーラビリティが達成されることを示しています。", "text_Russian": "И на рисунке видно, что при использовании пула достигается лучшая масштабируемость по глубине сети."}
{"text": "Here one can note that training the pyramidion of twenty four layers can be cheaper than training a two layer vanilla transformer on such long inputs.\n", "text_Chinese": "在这里，人们可以注意到，在如此长的输入上训练二十四层的金字塔可能比训练两层普通变压器更便宜。", "text_Arabic": "هنا يمكن ملاحظة أن تدريب الهرم المكون من أربع وعشرين طبقة يمكن أن يكون أرخص من تدريب محول الفانيليا المكون من طبقتين على مثل هذه المدخلات الطويلة.", "text_French": "Ici, on peut noter que la formation du pyramidion de vingt-quatre couches peut être moins chère que la formation d'un transformateur vanille à deux couches sur des entrées aussi longues.", "text_Japanese": "ここで、24 層のピラミディオンをトレーニングする方が、そのような長い入力で 2 層のバニラ トランスフォーマーをトレーニングするよりも安価になる可能性があることがわかります。", "text_Russian": "Здесь можно отметить, что обучение пирамидиона из двадцати четырех слоев может быть дешевле, чем обучение двухслойного ванильного трансформатора на таких длинных входах."}
{"text": "Not to mention how easily vanilla transformer can go out of memory for such a long input.\n", "text_Chinese": "更不用说对于这么长的输入，香草变压器很容易出现内存不足的情况。", "text_Arabic": "ناهيك عن مدى سهولة نفاد محول الفانيليا من الذاكرة لمثل هذا الإدخال الطويل.", "text_French": "Sans parler de la facilité avec laquelle le transformateur Vanilla peut manquer de mémoire pour une entrée aussi longue.", "text_Japanese": "バニラのトランスフォーマーがこれほど長い入力を行うと簡単にメモリ不足になることは言うまでもありません。", "text_Russian": "Не говоря уже о том, как легко ванильный трансформатор может потерять память при таком длинном вводе."}
{"text": "The qual quality qual qualitative comparison of our trend pyramidion to other baseline is performed on the long document summarization task, or given the body of an article from arXiv or PubMed, the task is to generate its abstract.\n", "text_Chinese": "我们的趋势金字塔与其他基线的定性比较是在长文档摘要任务上执行的，或者给定来自 arXiv 或 PubMed 的文章正文，任务是生成其摘要。", "text_Arabic": "يتم إجراء المقارنة النوعية النوعية لهرم الاتجاه الخاص بنا مع خط الأساس الآخر في مهمة تلخيص المستند الطويل، أو بالنظر إلى نص مقال من arXiv أو PubMed، فإن المهمة هي إنشاء ملخصه.", "text_French": "La comparaison qualitative et qualitative de notre pyramidion de tendances avec d'autres références est effectuée sur la tâche de résumé de document long, ou étant donné le corps d'un article d'arXiv ou de PubMed, la tâche consiste à générer son résumé.", "text_Japanese": "トレンドピラミディオンと他のベースラインとの質の高い、定性的な比較は、長い文書の要約タスクで実行されます。または、arXiv または PubMed からの論文の本文が与えられた場合、タスクはその要約を生成します。", "text_Russian": "Качественное качественное сравнение нашей пирамиды тенденций с другими базовыми показателями выполняется для задачи обобщения длинного документа или, учитывая текст статьи из arXiv или PubMed, задача состоит в том, чтобы создать ее реферат."}
{"text": "Thus, one can see blockwise, which is our baseline, performs on the level of the re, recent state-of-the-art models, while the pyramidion retains or improves the performance of this competitive baseline.\n", "text_Chinese": "因此，我们可以看到 blockwise（我们的基线）在最新的最先进模型的水平上执行，而金字塔保留或提高了这一竞争基线的性能。", "text_Arabic": "وبالتالي، يمكن للمرء أن يرى أن اتجاه المجموعة، وهو خط الأساس لدينا، يؤدي على مستوى النماذج الحديثة الحديثة، في حين يحتفظ الهرم بأداء خط الأساس التنافسي هذا أو يحسنه.", "text_French": "Ainsi, on peut voir que le blockwise, qui est notre baseline, fonctionne au niveau des modèles récents de pointe, tandis que le pyramidion conserve ou améliore les performances de cette baseline compétitive.", "text_Japanese": "したがって、私たちのベースラインであるブロックワイズは、最近の最先端モデルのレベルでパフォーマンスを発揮する一方、ピラミディオンはこの競争力のあるベースラインのパフォーマンスを維持または向上させていることがわかります。", "text_Russian": "Таким образом, можно видеть, что блочный режим, который является нашим базовым уровнем, работает на уровне последних современных моделей, в то время как пирамидион сохраняет или улучшает производительность этого конкурентоспособного базового уровня."}
{"text": "At the same time, our model is eighty percent faster to train and over four hundred fifty percent faster at inference when compared to the blockwise baseline.\n", "text_Chinese": "同时，与分块基线相比，我们的模型训练速度快了 80%，推理速度快了 450% 以上。", "text_Arabic": "وفي الوقت نفسه، يعد نموذجنا أسرع بنسبة ثمانين بالمائة في التدريب وأسرع بأكثر من أربعمائة وخمسين بالمائة في الاستدلال عند مقارنته بخط الأساس في اتجاه الكتلة.", "text_French": "Dans le même temps, notre modèle est quatre-vingt pour cent plus rapide à former et plus de quatre cent cinquante pour cent plus rapide en termes d'inférence par rapport à la ligne de base par blocs.", "text_Japanese": "同時に、ブロックごとのベースラインと比較すると、私たちのモデルはトレーニングが 80% 高速になり、推論が 450% 以上高速になります。", "text_Russian": "В то же время наша модель обучается на восемьдесят процентов быстрее и на четыреста пятьдесят процентов быстрее при выводе по сравнению с блочной базовой моделью."}
{"text": "Both models have much lower parameter counts and were trained from scratch on the chosen tasks.\n", "text_Chinese": "这两个模型的参数数量要少得多，并且都是针对所选任务从头开始训练的。", "text_Arabic": "يحتوي كلا النموذجين على عدد أقل بكثير من المعلمات وتم تدريبهما من البداية على المهام المختارة.", "text_French": "Les deux modèles ont un nombre de paramètres beaucoup plus faible et ont été formés à partir de zéro sur les tâches choisies.", "text_Japanese": "どちらのモデルもパラメーター数がはるかに少なく、選択されたタスクでゼロからトレーニングされました。", "text_Russian": "Обе модели имеют гораздо меньшее количество параметров и были обучены с нуля выбранным задачам."}
{"text": "Previous approaches to to achieve a similar performance had to use more parameters and leverage pretrained foundation foundational models and additional language pretraining objective to achieve similar performance.\n", "text_Chinese": "以前实现类似性能的方法必须使用更多参数并利用预训练的基础模型和额外的语言预训练目标来实现类似的性能。", "text_Arabic": "كان على الأساليب السابقة لتحقيق أداء مماثل استخدام المزيد من المعلمات والاستفادة من النماذج التأسيسية الأساسية المدربة مسبقًا وهدف التدريب المسبق للغة الإضافية لتحقيق أداء مماثل.", "text_French": "Les approches précédentes pour obtenir des performances similaires devaient utiliser davantage de paramètres et exploiter des modèles fondamentaux de base pré-entraînés et un objectif de pré-formation linguistique supplémentaire pour obtenir des performances similaires.", "text_Japanese": "同様のパフォーマンスを達成するための以前のアプローチでは、より多くのパラメーターを使用し、事前トレーニングされた基礎基本モデルと追加の言語事前トレーニング目標を活用して、同様のパフォーマンスを達成する必要がありました。", "text_Russian": "Предыдущие подходы к достижению аналогичной производительности должны были использовать больше параметров и использовать предварительно обученные базовые базовые модели, а также дополнительную цель предварительной языковой подготовки для достижения аналогичной производительности."}
{"text": "We invite you to read our full paper and use our GitHub code.\n", "text_Chinese": "我们邀请您阅读我们的全文并使用我们的 GitHub 代码。", "text_Arabic": "نحن ندعوك لقراءة ورقتنا الكاملة واستخدام كود GitHub الخاص بنا.", "text_French": "Nous vous invitons à lire notre article complet et à utiliser notre code GitHub.", "text_Japanese": "ぜひ論文全文を読んで、GitHub コードを使用してください。", "text_Russian": "Мы приглашаем вас прочитать полную версию нашей статьи и использовать наш код GitHub."}
{"text": "Thank you for watching.\n", "text_Chinese": "感谢您的观看。", "text_Arabic": "شكرا لمشاهدتك.", "text_French": "Merci d'avoir regardé.", "text_Japanese": "ご清覧ありがとうございました。", "text_Russian": "Спасибо за просмотр."}
{"text": "Hello, this is Jiawei Zhou from Harvard University.\n", "text_Chinese": "大家好，我是哈佛大学的周嘉伟。", "text_Arabic": "مرحبًا، أنا جياوي تشو من جامعة هارفارد.", "text_French": "Bonjour, voici Jiawei Zhou de l'Université Harvard.", "text_Japanese": "こんにちは、ハーバード大学の周佳偉です。", "text_Russian": "Здравствуйте, это Цзявэй Чжоу из Гарвардского университета."}
{"text": "I am very glad to present our work on Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue.\n", "text_Chinese": "我很高兴介绍我们在面向任务的对话中减少延迟的在线语义解析方面的工作。", "text_Arabic": "يسعدني جدًا أن أقدم عملنا في التحليل الدلالي عبر الإنترنت لتقليل زمن الوصول في الحوار الموجه نحو المهام.", "text_French": "Je suis très heureux de présenter notre travail sur l'analyse sémantique en ligne pour la réduction de la latence dans le dialogue orienté tâches.", "text_Japanese": "タスク指向対話におけるレイテンシー削減のためのオンライン セマンティック解析に関する私たちの取り組みを紹介できることを大変うれしく思います。", "text_Russian": "Я очень рад представить нашу работу по онлайн-семантическому анализу для сокращения задержек в ориентированном на задачу диалоге."}
{"text": "This is joint work with Jason, Michael, Anthony and Sam from Microsoft Semantic Machines.\n", "text_Chinese": "这是与 Microsoft Semantic Machines 的 Jason、Michael、Anthony 和 Sam 的合作成果。", "text_Arabic": "هذا عمل مشترك مع جيسون ومايكل وأنتوني وسام من شركة Microsoft Semantic Machines.", "text_French": "Il s'agit d'un travail conjoint avec Jason, Michael, Anthony et Sam de Microsoft Semantic Machines.", "text_Japanese": "これは、Microsoft Semantic Machines の Jason、Michael、Anthony、Sam との共同作業です。", "text_Russian": "Это совместная работа с Джейсоном, Майклом, Энтони и Сэмом из Microsoft Semantic Machines."}
{"text": "In task-oriented dialogue, a user interacts with the system that handles requests from user utterances usually in speaking.\n", "text_Chinese": "在面向任务的对话中，用户与系统进行交互，系统通常通过说话来处理用户话语的请求。", "text_Arabic": "في الحوار الموجه نحو المهام، يتفاعل المستخدم مع النظام الذي يتعامل مع الطلبات الواردة من أقوال المستخدم عادة في التحدث.", "text_French": "Dans un dialogue axé sur les tâches, un utilisateur interagit avec le système qui gère les demandes des énoncés de l'utilisateur, généralement oralement.", "text_Japanese": "タスク指向の対話では、ユーザーは、通常は話し言葉でユーザーの発話からのリクエストを処理するシステムと対話します。", "text_Russian": "В ориентированном на задачу диалоге пользователь взаимодействует с системой, которая обрабатывает запросы от высказываний пользователя, обычно при разговоре."}
{"text": "From the finish of the user utterance to the system response there is often a noticeable delay.\n", "text_Chinese": "从用户话语结束到系统响应通常存在明显的延迟。", "text_Arabic": "من نهاية كلام المستخدم إلى استجابة النظام غالبًا ما يكون هناك تأخير ملحوظ.", "text_French": "Entre la fin de l'énoncé de l'utilisateur et la réponse du système, il y a souvent un délai notable.", "text_Japanese": "ユーザーの発話が終了してからシステムの応答までに、顕著な遅延が生じることがよくあります。", "text_Russian": "От окончания высказывания пользователя до ответа системы часто наблюдается заметная задержка."}
{"text": "Under the hood, the user utterance is translated into an executable program.\n", "text_Chinese": "在幕后，用户话语被翻译成可执行程序。", "text_Arabic": "تحت الغطاء، تتم ترجمة كلام المستخدم إلى برنامج قابل للتنفيذ.", "text_French": "Sous le capot, l'énoncé de l'utilisateur est traduit en programme exécutable.", "text_Japanese": "内部では、ユーザーの発話が実行可能プログラムに変換されます。", "text_Russian": "Под капотом высказывание пользователя преобразуется в исполняемую программу."}
{"text": "Which is then executed so that the system can respond properly.\n", "text_Chinese": "然后执行该命令以便系统能够正确响应。", "text_Arabic": "والذي يتم تنفيذه بعد ذلك حتى يتمكن النظام من الاستجابة بشكل صحيح.", "text_French": "Qui est ensuite exécuté pour que le système puisse répondre correctement.", "text_Japanese": "これは、システムが適切に応答できるように実行されます。", "text_Russian": "Который затем выполняется, чтобы система могла отреагировать должным образом."}
{"text": "Because the program is represented as a semantic graph that outlines the computation, where node represents a function invocation and its children are the arguments.\n", "text_Chinese": "因为程序被表示为概述计算的语义图，其中节点表示函数调用，其子级是参数。", "text_Arabic": "لأن البرنامج يتم تمثيله كرسم بياني دلالي يوضح العملية الحسابية، حيث تمثل العقدة استدعاء دالة وأطفالها هم الوسيطات.", "text_French": "Parce que le programme est représenté comme un graphe sémantique qui décrit le calcul, où le nœud représente une invocation de fonction et ses enfants sont les arguments.", "text_Japanese": "プログラムは計算の概要を示すセマンティック グラフとして表現されるため、ノードは関数の呼び出しを表し、その子は引数を表します。", "text_Russian": "Потому что программа представлена ​​в виде семантического графа, описывающего вычисления, где узел представляет вызов функции, а его дочерние элементы — аргументы."}
{"text": "The great nodes mark instantaneous operations, but the others are slow to execute.\n", "text_Chinese": "伟大的节点标志着瞬时操作，但其他节点执行速度很慢。", "text_Arabic": "تشير العقد الكبيرة إلى عمليات فورية، لكن العقد الأخرى بطيئة في التنفيذ.", "text_French": "Les grands nœuds marquent les opérations instantanées, mais les autres sont lents à s'exécuter.", "text_Japanese": "優れたノードは瞬時の操作を示しますが、他のノードは実行に時間がかかります。", "text_Russian": "Большие узлы отмечают мгновенные операции, но остальные выполняются медленно."}
{"text": "The simple example here we show, these programs can often be more complicated graphs beyond the tree structures.\n", "text_Chinese": "我们在这里展示的简单示例，这些程序通常可以是超出树结构的更复杂的图形。", "text_Arabic": "المثال البسيط الذي نعرضه هنا هو أن هذه البرامج غالبًا ما تكون عبارة عن رسوم بيانية أكثر تعقيدًا تتجاوز الهياكل الشجرية.", "text_French": "Dans l'exemple simple que nous montrons ici, ces programmes peuvent souvent être des graphiques plus compliqués au-delà des structures arborescentes.", "text_Japanese": "ここで示す簡単な例では、これらのプログラムは多くの場合、ツリー構造を超えたより複雑なグラフになることがあります。", "text_Russian": "На простом примере, который мы показываем, эти программы часто могут представлять собой более сложные графы, выходящие за рамки древовидных структур."}
{"text": "In this talk, we ask the question, can we start generating the program and executing it before the user even finishes the utterance so that the faster response can be achieved by the system?\n", "text_Chinese": "在本次演讲中，我们提出一个问题，我们是否可以在用户说完之前就开始生成程序并执行它，以便系统能够实现更快的响应？", "text_Arabic": "في هذا الحديث نطرح السؤال، هل يمكننا البدء في إنشاء البرنامج وتنفيذه قبل أن ينتهي المستخدم من النطق حتى يتمكن النظام من تحقيق استجابة أسرع؟", "text_French": "Dans cet exposé, nous posons la question : pouvons-nous commencer à générer le programme et à l'exécuter avant même que l'utilisateur ait terminé l'énoncé afin que le système puisse obtenir une réponse plus rapide ?", "text_Japanese": "この講演では、システムがより高速な応答を達成できるように、ユーザーが発話を終える前にプログラムの生成と実行を開始できるか、という質問をします。", "text_Russian": "В этом докладе мы задаем вопрос: можем ли мы начать генерировать программу и выполнять ее еще до того, как пользователь закончит произнести фразу, чтобы система могла добиться более быстрого ответа?"}
{"text": "This is the online prediction and decision problem.\n", "text_Chinese": "这就是在线预测和决策问题。", "text_Arabic": "هذه هي مشكلة التنبؤ والقرار عبر الإنترنت.", "text_French": "C’est le problème de la prédiction et de la décision en ligne.", "text_Japanese": "これはオンライン予測と決定の問題です。", "text_Russian": "Это проблема онлайн-прогнозирования и принятия решений."}
{"text": "There are a lot of others in this realm.\n", "text_Chinese": "这个领域还有很多其他人。", "text_Arabic": "هناك الكثير من الآخرين في هذا المجال.", "text_French": "Il y en a bien d’autres dans ce domaine.", "text_Japanese": "この領域には他にもたくさんあります。", "text_Russian": "В этом мире есть много других."}
{"text": "Examples include simultaneous translation where a live interpreter translates one language to another in real time, smart text auto completion to guess the user intent, and Uber pool where the drivers are sent to where they might be needed based on the predicted demand.\n", "text_Chinese": "例如，现场口译员将一种语言实时翻译成另一种语言的同声翻译、猜测用户意图的智能文本自动完成功能，以及根据预测需求将司机发送到可能需要的地方的 Uber 服务池。", "text_Arabic": "تشمل الأمثلة الترجمة الفورية حيث يقوم مترجم فوري بترجمة لغة إلى أخرى في الوقت الفعلي، والإكمال التلقائي للنص الذكي لتخمين نية المستخدم، وتجمع أوبر حيث يتم إرسال السائقين إلى حيث قد تكون هناك حاجة إليهم بناءً على الطلب المتوقع.", "text_French": "Les exemples incluent la traduction simultanée où un interprète en direct traduit une langue dans une autre en temps réel, la saisie automatique intelligente du texte pour deviner l'intention de l'utilisateur et le pool Uber où les chauffeurs sont envoyés là où ils pourraient être nécessaires en fonction de la demande prévue.", "text_Japanese": "例としては、ライブ通訳者がリアルタイムである言語を別の言語に翻訳する同時翻訳、ユーザーの意図を推測するスマート テキスト オート コンプリート、予測された需要に基づいてドライバーが必要とされる場所に派遣される Uber プールなどがあります。", "text_Russian": "Примеры включают синхронный перевод, когда живой переводчик переводит один язык на другой в режиме реального времени, автоматическое заполнение интеллектуального текста, чтобы угадать намерения пользователя, и пул Uber, где водители отправляются туда, где они могут понадобиться в зависимости от прогнозируемого спроса."}
{"text": "All of these scenarios have one thing in common.\n", "text_Chinese": "所有这些场景都有一个共同点。", "text_Arabic": "كل هذه السيناريوهات تشترك في شيء واحد.", "text_French": "Tous ces scénarios ont un point commun.", "text_Japanese": "これらすべてのシナリオには 1 つの共通点があります。", "text_Russian": "Все эти сценарии имеют одну общую черту."}
{"text": "That is, it is beneficial to make decisions before seeing all the input.\n", "text_Chinese": "也就是说，在看到所有输入之前做出决定是有益的。", "text_Arabic": "وهذا يعني أنه من المفيد اتخاذ القرارات قبل رؤية جميع المدخلات.", "text_French": "Autrement dit, il est avantageux de prendre des décisions avant de voir toutes les informations reçues.", "text_Japanese": "つまり、すべての入力を確認する前に決定を下すことが有益です。", "text_Russian": "То есть полезно принимать решения до того, как вы увидите все исходные данные."}
{"text": "In our case, we are going to deal with online semantic parsing, which could be expected to be challenging as we have to guess what the user might say.\n", "text_Chinese": "在我们的例子中，我们将处理在线语义解析，这可能会具有挑战性，因为我们必须猜测用户可能会说什么。", "text_Arabic": "في حالتنا، سنتعامل مع التحليل الدلالي عبر الإنترنت، والذي من المتوقع أن يكون صعبًا حيث يتعين علينا تخمين ما قد يقوله المستخدم.", "text_French": "Dans notre cas, nous allons traiter de l’analyse sémantique en ligne, ce qui pourrait s’avérer difficile car nous devons deviner ce que l’utilisateur pourrait dire.", "text_Japanese": "私たちの場合は、オンラインのセマンティック解析を扱いますが、ユーザーが何を言うかを推測する必要があるため、困難になることが予想されます。", "text_Russian": "В нашем случае мы будем иметь дело с онлайн-семантическим анализом, который, как можно ожидать, будет сложной задачей, поскольку нам нужно угадать, что может сказать пользователь."}
{"text": "And it is also underexplored with no formal evaluation metric.\n", "text_Chinese": "而且它还没有得到充分探索，没有正式的评估指标。", "text_Arabic": "كما أنها غير مستكشفة بالكامل مع عدم وجود مقياس تقييم رسمي.", "text_French": "Et il est également sous-exploré sans aucune mesure d’évaluation formelle.", "text_Japanese": "また、正式な評価指標もなく、十分に研究されていません。", "text_Russian": "Кроме того, он недостаточно изучен и не имеет формальных показателей оценки."}
{"text": "First, let's look at how an ordinary system works.\n", "text_Chinese": "首先，让我们看看普通系统是如何工作的。", "text_Arabic": "أولاً، دعونا نلقي نظرة على كيفية عمل النظام العادي.", "text_French": "Voyons d’abord comment fonctionne un système ordinaire.", "text_Japanese": "まず、通常のシステムがどのように動作するかを見てみましょう。", "text_Russian": "Для начала давайте посмотрим, как работает обычная система."}
{"text": "It is operating offline by parsing to the program only at the end of the user utterance.\n", "text_Chinese": "它仅在用户话语结束时解析到程序来离线操作。", "text_Arabic": "إنه يعمل دون اتصال بالإنترنت عن طريق التحليل للبرنامج فقط في نهاية كلام المستخدم.", "text_French": "Il fonctionne hors ligne en analysant le programme uniquement à la fin de l'énoncé de l'utilisateur.", "text_Japanese": "ユーザーの発話の終了時にのみプログラムに解析することでオフラインで動作します。", "text_Russian": "Он работает в автономном режиме, анализируя программу только в конце высказывания пользователя."}
{"text": "Here, the character graph is predicted after seeing all the information.\n", "text_Chinese": "这里，在看到所有信息后预测人物图。", "text_Arabic": "هنا، يتم توقع الرسم البياني للشخصية بعد رؤية جميع المعلومات.", "text_French": "Ici, le graphique des personnages est prédit après avoir vu toutes les informations.", "text_Japanese": "ここでは、すべての情報を見た上でキャラクターグラフを予測します。", "text_Russian": "Здесь граф персонажей прогнозируется после просмотра всей информации."}
{"text": "In contrast, we are proposing an online system that compares at every utterance prefix.\n", "text_Chinese": "相比之下，我们提出了一个在线系统，可以对每个话语前缀进行比较。", "text_Arabic": "في المقابل، نحن نقترح نظامًا عبر الإنترنت يقارن في كل بادئة كلام.", "text_French": "En revanche, nous proposons un système en ligne qui compare chaque préfixe d'énoncé.", "text_Japanese": "対照的に、私たちは発話の接頭語ごとに比較するオンライン システムを提案しています。", "text_Russian": "Напротив, мы предлагаем онлайн-систему, которая сравнивает каждый префикс высказывания."}
{"text": "For example, each time we see a new token, we predict a new graph.\n", "text_Chinese": "例如，每次我们看到一个新的标记，我们都会预测一个新的图表。", "text_Arabic": "على سبيل المثال، في كل مرة نرى رمزًا مميزًا جديدًا، نتوقع رسمًا بيانيًا جديدًا.", "text_French": "Par exemple, chaque fois que nous voyons un nouveau jeton, nous prédisons un nouveau graphique.", "text_Japanese": "たとえば、新しいトークンを見つけるたびに、新しいグラフを予測します。", "text_Russian": "Например, каждый раз, когда мы видим новый токен, мы прогнозируем новый график."}
{"text": "Notice that there could be errors.\n", "text_Chinese": "请注意，可能会出现错误。", "text_Arabic": "لاحظ أنه قد تكون هناك أخطاء.", "text_French": "Notez qu'il peut y avoir des erreurs.", "text_Japanese": "エラーが発生する可能性があることに注意してください。", "text_Russian": "Обратите внимание, что могут быть ошибки."}
{"text": "At the position of at the pool party with Barack Obama, we got a graph with the right nodes on the person and the event subject, but guess the wrong timing information.\n", "text_Chinese": "在与巴拉克·奥巴马 (Barack Obama) 一起参加泳池派对时，我们得到了一张图表，其中包含人物和事件主题的正确节点，但猜测了错误的时间信息。", "text_Arabic": "في موقع حفلة البلياردو مع باراك أوباما، حصلنا على رسم بياني يحتوي على العقد الصحيحة للشخص وموضوع الحدث، ولكن خمن معلومات التوقيت الخاطئة.", "text_French": "Lors d'une soirée piscine avec Barack Obama, nous avons obtenu un graphique avec les bons nœuds sur la personne et le sujet de l'événement, mais devinez les mauvaises informations de timing.", "text_Japanese": "バラク・オバマとのプールパーティーの位置では、人物とイベントの主題に関する正しいノードを含むグラフを取得しましたが、間違ったタイミング情報を推測しました。", "text_Russian": "В позиции на вечеринке у бассейна с Бараком Обамой мы получили график с правильными узлами по человеку и теме события, но угадали неправильную информацию о времени."}
{"text": "This process goes on until we receive the full user utterance.\n", "text_Chinese": "这个过程一直持续到我们收到完整的用户话语为止。", "text_Arabic": "تستمر هذه العملية حتى نتلقى كلام المستخدم الكامل.", "text_French": "Ce processus se poursuit jusqu'à ce que nous recevions l'énoncé complet de l'utilisateur.", "text_Japanese": "このプロセスは、ユーザーの発話全体を受信するまで続きます。", "text_Russian": "Этот процесс продолжается до тех пор, пока мы не получим полное высказывание пользователя."}
{"text": "How would this affect the execution timeline in the offline system?\n", "text_Chinese": "这将如何影响离线系统中的执行时间线？", "text_Arabic": "كيف سيؤثر ذلك على الجدول الزمني للتنفيذ في النظام غير المتصل بالإنترنت؟", "text_French": "Comment cela affecterait-il le calendrier d’exécution dans le système hors ligne ?", "text_Japanese": "これはオフライン システムでの実行タイムラインにどのような影響を与えるでしょうか?", "text_Russian": "Как это повлияет на сроки выполнения в автономной системе?"}
{"text": "We'll get the program graph at the end so that the system can start execution at this point.\n", "text_Chinese": "最后我们会得到程序图，这样系统就可以在此时开始执行。", "text_Arabic": "سنحصل على الرسم البياني للبرنامج في النهاية حتى يتمكن النظام من بدء التنفيذ عند هذه النقطة.", "text_French": "Nous obtiendrons le graphique du programme à la fin afin que le système puisse commencer l'exécution à ce stade.", "text_Japanese": "システムがこの時点で実行を開始できるように、最後にプログラム グラフを取得します。", "text_Russian": "В конце мы получим граф программы, чтобы система могла начать выполнение с этого момента."}
{"text": "Remember that the great nodes are fast operations, so we only consider the execution timeline of the colored slow functions.\n", "text_Chinese": "请记住，伟大的节点是快速操作，因此我们只考虑彩色慢速函数的执行时间线。", "text_Arabic": "تذكر أن العقد الكبيرة هي عمليات سريعة، لذلك نأخذ في الاعتبار فقط الجدول الزمني لتنفيذ الوظائف البطيئة الملونة.", "text_French": "N'oubliez pas que les grands nœuds sont des opérations rapides, nous ne considérons donc que la chronologie d'exécution des fonctions lentes colorées.", "text_Japanese": "優れたノードは高速な操作であるため、色付きの遅い関数の実行タイムラインのみを考慮することに注意してください。", "text_Russian": "Помните, что большие узлы — это быстрые операции, поэтому мы рассматриваем только временную шкалу выполнения цветных медленных функций."}
{"text": "First, these two find person functions can be executed in parallel, highlighted in white from the pink box as they have no dependency on other functions.\n", "text_Chinese": "首先，这两个查找人员功能可以并行执行，粉红色框中以白色突出显示，因为它们不依赖于其他功能。", "text_Arabic": "أولاً، يمكن تنفيذ وظيفتي البحث عن شخص بالتوازي، ويتم تمييزهما باللون الأبيض من المربع الوردي حيث لا يعتمدان على وظائف أخرى.", "text_French": "Premièrement, ces deux fonctions de recherche de personne peuvent être exécutées en parallèle, surlignées en blanc dans la case rose, car elles ne dépendent d'autres fonctions.", "text_Japanese": "まず、これら 2 つの find person 関数は、他の関数に依存していないため、ピンクのボックスで白で強調表示されており、並行して実行できます。", "text_Russian": "Во-первых, эти две функции поиска человека могут выполняться параллельно и выделены белым цветом в розовом поле, поскольку они не зависят от других функций."}
{"text": "Next, the node create event can then get executed after obtaining results from lower level nodes and then the top function yield so the whole program is finished.\n", "text_Chinese": "接下来，从下层节点获取结果后，即可执行节点创建事件，然后顶层函数yield，整个程序结束。", "text_Arabic": "بعد ذلك، يمكن بعد ذلك تنفيذ حدث إنشاء العقدة بعد الحصول على نتائج من العقد ذات المستوى الأدنى ومن ثم إنتاج الوظيفة العليا بحيث يتم الانتهاء من البرنامج بأكمله.", "text_French": "Ensuite, l'événement de création de nœud peut ensuite être exécuté après avoir obtenu les résultats des nœuds de niveau inférieur, puis la fonction supérieure produit afin que l'ensemble du programme soit terminé.", "text_Japanese": "次に、下位レベルのノードから結果を取得した後にノード作成イベントが実行され、最上位の関数が生成されるため、プログラム全体が終了します。", "text_Russian": "Затем событие создания узла может быть выполнено после получения результатов от узлов нижнего уровня, а затем выход верхней функции, так что вся программа завершена."}
{"text": "The execution process is strict, restricted to the program dependency structure where some operations cannot be parallelized which induces a noticeable delay.\n", "text_Chinese": "执行过程是严格的，受限于程序依赖结构，其中某些操作无法并行化，从而导致明显的延迟。", "text_Arabic": "عملية التنفيذ صارمة، وتقتصر على بنية تبعية البرنامج حيث لا يمكن موازنة بعض العمليات مما يؤدي إلى تأخير ملحوظ.", "text_French": "Le processus d'exécution est strict, restreint à la structure de dépendances du programme où certaines opérations ne peuvent pas être parallélisées ce qui induit un retard notable.", "text_Japanese": "実行プロセスは厳密で、プログラムの依存関係構造に制限されており、一部の操作は並列化できず、顕著な遅延が発生します。", "text_Russian": "Процесс выполнения является строгим, ограничен структурой зависимостей программы, где некоторые операции не могут быть распараллелены, что приводит к заметной задержке."}
{"text": "In our online system, where we predict as we go, the program execution can start earlier.\n", "text_Chinese": "在我们的在线系统中，我们可以随时预测，程序执行可以更早开始。", "text_Arabic": "في نظامنا عبر الإنترنت، حيث نتوقع بينما نمضي قدمًا، يمكن أن يبدأ تنفيذ البرنامج مبكرًا.", "text_French": "Dans notre système en ligne, où nous prédisons au fur et à mesure, l'exécution du programme peut commencer plus tôt.", "text_Japanese": "私たちのオンライン システムでは、進行中に予測を行うため、プログラムの実行をより早く開始できます。", "text_Russian": "В нашей онлайн-системе, где мы прогнозируем по ходу дела, выполнение программы может начаться раньше."}
{"text": "Here, at the prefix after Obama we predict confidently that the find person function should be in the program, but the rest may contain errors as they are grayed out.\n", "text_Chinese": "在这里，在奥巴马之后的前缀处，我们自信地预测查找人员功能应该在程序中，但其余部分可能包含错误，因为它们呈灰色。", "text_Arabic": "هنا، في البادئة بعد أوباما، نتوقع بثقة أن وظيفة البحث عن شخص يجب أن تكون موجودة في البرنامج، ولكن الباقي قد يحتوي على أخطاء لأنها تظهر باللون الرمادي.", "text_French": "Ici, au préfixe après Obama, nous prédisons avec confiance que la fonction de recherche de personne devrait être dans le programme, mais le reste peut contenir des erreurs car elles sont grisées.", "text_Japanese": "ここで、Obama の後の接頭辞では、人物検索機能がプログラムに含まれるはずであると自信を持って予測していますが、残りの部分はグレー表示されているため、エラーが含まれている可能性があります。", "text_Russian": "Здесь в префиксе после Обамы мы уверенно прогнозируем, что функция поиска человека должна быть в программе, но остальные могут содержать ошибки, поскольку они выделены серым цветом."}
{"text": "The execution of the node can be immediately started as a step.\n", "text_Chinese": "节点的执行可以作为一个步骤立即开始。", "text_Arabic": "يمكن البدء في تنفيذ العقدة على الفور كخطوة.", "text_French": "L'exécution du nœud peut être immédiatement lancée sous forme d'étape.", "text_Japanese": "ノードの実行はステップとしてすぐに開始できます。", "text_Russian": "Выполнение узла можно сразу начать как шаг."}
{"text": "Then, with more tokens, we predict a totally new graph, but part of it has already being executed.\n", "text_Chinese": "然后，用更多的令牌，我们预测一个全新的图，但它的一部分已经被执行。", "text_Arabic": "بعد ذلك، مع المزيد من الرموز المميزة، نتوقع رسمًا بيانيًا جديدًا تمامًا، ولكن تم تنفيذ جزء منه بالفعل.", "text_French": "Ensuite, avec plus de jetons, nous prédisons un graphe totalement nouveau, mais une partie de celui-ci a déjà été exécutée.", "text_Japanese": "次に、より多くのトークンを使用して、まったく新しいグラフを予測しますが、その一部はすでに実行されています。", "text_Russian": "Затем, используя больше токенов, мы прогнозируем совершенно новый граф, но часть его уже выполняется."}
{"text": "So, we only need to consider the rest of the nodes that we are confident about as well.\n", "text_Chinese": "因此，我们只需要考虑我们有信心的其余节点。", "text_Arabic": "لذلك، نحتاج فقط إلى النظر في بقية العقد التي نحن واثقون منها أيضًا.", "text_French": "Il nous suffit donc de considérer également le reste des nœuds sur lesquels nous avons confiance.", "text_Japanese": "したがって、同様に信頼できる残りのノードのみを考慮する必要があります。", "text_Russian": "Итак, нам нужно рассмотреть только остальные узлы, в которых мы уверены."}
{"text": "Here, another find person can be executed in parallel.\n", "text_Chinese": "这里，另一个find person可以并行执行。", "text_Arabic": "هنا، يمكن تنفيذ عملية البحث عن شخص آخر بالتوازي.", "text_French": "Ici, une autre personne de recherche peut être exécutée en parallèle.", "text_Japanese": "ここで、別の検索者を並行して実行できます。", "text_Russian": "Здесь параллельно может быть выполнен еще один поиск человека."}
{"text": "Again, we may have wrong predictions.\n", "text_Chinese": "同样，我们可能会有错误的预测。", "text_Arabic": "مرة أخرى، ربما تكون لدينا توقعات خاطئة.", "text_French": "Encore une fois, nous pouvons faire de fausses prédictions.", "text_Japanese": "繰り返しますが、私たちは間違った予測をするかもしれません。", "text_Russian": "Опять же, у нас могут быть неверные прогнозы."}
{"text": "With more text, we have more ability to make it right.\n", "text_Chinese": "文字越多，我们就越有能力做出正确的决定。", "text_Arabic": "مع المزيد من النص، لدينا المزيد من القدرة على تصحيح الأمر.", "text_French": "Avec plus de texte, nous avons plus de capacité à le corriger.", "text_Japanese": "テキストが増えると、より正確に表現できるようになります。", "text_Russian": "Чем больше текста, тем больше у нас возможностей сделать его правильно."}
{"text": "Such as the event time here where AM is also anticipated correctly.\n", "text_Chinese": "例如这里的事件时间，AM 也被正确预测。", "text_Arabic": "مثل وقت الحدث هنا حيث يتم أيضًا توقع AM بشكل صحيح.", "text_French": "Comme l'heure de l'événement ici où AM est également anticipé correctement.", "text_Japanese": "ここでのイベント時間など、AM も正しく予測されます。", "text_Russian": "Например, время события здесь, где AM также прогнозируется правильно."}
{"text": "Then, we can start executing the rest following the program dependency structure.\n", "text_Chinese": "然后，我们就可以按照程序依赖结构开始执行剩下的部分了。", "text_Arabic": "بعد ذلك، يمكننا البدء في تنفيذ الباقي باتباع هيكل تبعية البرنامج.", "text_French": "Ensuite, nous pouvons commencer à exécuter le reste en suivant la structure de dépendance du programme.", "text_Japanese": "その後、プログラムの依存関係構造に従って残りの実行を開始できます。", "text_Russian": "Затем мы можем начать выполнение остальных, следуя структуре зависимостей программы."}
{"text": "By overlapping the execution timeline with the utterance timeline, we save a big amount of time.\n", "text_Chinese": "通过将执行时间线与话语时间线重叠，我们节省了大量时间。", "text_Arabic": "ومن خلال تداخل الجدول الزمني للتنفيذ مع الجدول الزمني للكلام، فإننا نوفر قدرًا كبيرًا من الوقت.", "text_French": "En superposant la chronologie d’exécution avec la chronologie d’énoncé, nous gagnons beaucoup de temps.", "text_Japanese": "実行タイムラインを発話タイムラインと重ねることで、時間を大幅に節約できます。", "text_Russian": "Перекрывая временную шкалу выполнения и временную шкалу высказывания, мы экономим большое количество времени."}
{"text": "So we proposed the task of online semantic parsing.\n", "text_Chinese": "所以我们提出了在线语义解析的任务。", "text_Arabic": "لذلك اقترحنا مهمة التحليل الدلالي عبر الإنترنت.", "text_French": "Nous avons donc proposé la tâche d'analyse sémantique en ligne.", "text_Japanese": "そこで私たちはオンライン意味解析というタスクを提案しました。", "text_Russian": "Поэтому мы предложили задачу онлайн-семантического разбора."}
{"text": "One underlying assumption is that the execution time dominates the model prediction time.\n", "text_Chinese": "一个基本假设是执行时间主导模型预测时间。", "text_Arabic": "أحد الافتراضات الأساسية هو أن وقت التنفيذ يهيمن على وقت التنبؤ بالنموذج.", "text_French": "Une hypothèse sous-jacente est que le temps d’exécution domine le temps de prédiction du modèle.", "text_Japanese": "根底にある仮定の 1 つは、実行時間がモデルの予測時間の大部分を占めるということです。", "text_Russian": "Одно из основных предположений заключается в том, что время выполнения доминирует над временем прогнозирования модели."}
{"text": "So we could only gain time by predicting earlier.\n", "text_Chinese": "所以我们只能通过提前预测来赢得时间。", "text_Arabic": "لذلك لا يمكننا كسب الوقت إلا من خلال التنبؤ مبكرًا.", "text_French": "Nous ne pourrions donc gagner du temps qu’en prévoyant plus tôt.", "text_Japanese": "したがって、より早く予測することによってのみ時間を稼ぐことができました。", "text_Russian": "Таким образом, мы могли только выиграть время, предсказывая заранее."}
{"text": "Another assumption is that as the prediction and execution happen in the background, that it is not visible to users.\n", "text_Chinese": "另一个假设是，由于预测和执行在后台发生，因此用户不可见。", "text_Arabic": "الافتراض الآخر هو أنه بما أن التنبؤ والتنفيذ يحدث في الخلفية، فإنه غير مرئي للمستخدمين.", "text_French": "Une autre hypothèse est que, comme la prédiction et l’exécution se produisent en arrière-plan, elles ne sont pas visibles pour les utilisateurs.", "text_Japanese": "もう 1 つの仮定は、予測と実行はバックグラウンドで行われるため、ユーザーには表示されないということです。", "text_Russian": "Другое предположение состоит в том, что прогнозирование и выполнение происходят в фоновом режиме и не видны пользователям."}
{"text": "It is not necessary to maintain a consistent parsing history.\n", "text_Chinese": "没有必要维护一致的解析历史记录。", "text_Arabic": "ليس من الضروري الحفاظ على سجل تحليل ثابت.", "text_French": "Il n'est pas nécessaire de conserver un historique d'analyse cohérent.", "text_Japanese": "一貫した解析履歴を維持する必要はありません。", "text_Russian": "Нет необходимости поддерживать последовательную историю синтаксического анализа."}
{"text": "So, we reparse from scratch after each token.\n", "text_Chinese": "因此，我们在每个标记之后从头开始重新解析。", "text_Arabic": "لذلك، نقوم بإعادة التوزيع من الصفر بعد كل رمز مميز.", "text_French": "Nous analysons donc à partir de zéro après chaque jeton.", "text_Japanese": "したがって、各トークンの後に最初から再解析します。", "text_Russian": "Итак, мы выполняем повторную обработку с нуля после каждого токена."}
{"text": "In particular, we propose a two step approach.\n", "text_Chinese": "特别是，我们提出了一个两步走的方法。", "text_Arabic": "وعلى وجه الخصوص، نقترح نهجا من خطوتين.", "text_French": "En particulier, nous proposons une approche en deux étapes.", "text_Japanese": "特に、2 段階のアプローチを提案します。", "text_Russian": "В частности, мы предлагаем двухэтапный подход."}
{"text": "A proposed step that predicts a graph with complete structure and a select step that selects the nodes that are worth executing at this time.\n", "text_Chinese": "一个预测具有完整结构的图的建议步骤和一个选择此时值得执行的节点的选择步骤。", "text_Arabic": "خطوة مقترحة تتنبأ برسم بياني ببنية كاملة وخطوة محددة تحدد العقد التي تستحق التنفيذ في هذا الوقت.", "text_French": "Une étape proposée qui prédit un graphique avec une structure complète et une étape de sélection qui sélectionne les nœuds qui valent la peine d'être exécutés à ce moment-là.", "text_Japanese": "完全な構造を持つグラフを予測する提案されたステップと、現時点で実行する価値のあるノードを選択する選択ステップ。", "text_Russian": "Предлагаемый шаг, который прогнозирует граф с полной структурой, и шаг выбора, который выбирает узлы, которые стоит выполнить в данный момент."}
{"text": "We had two variants of the proposed method.\n", "text_Chinese": "我们对所提出的方法有两种变体。", "text_Arabic": "كان لدينا نوعان مختلفان من الطريقة المقترحة.", "text_French": "Nous avions deux variantes de la méthode proposée.", "text_Japanese": "提案された方法には 2 つのバリエーションがありました。", "text_Russian": "У нас было два варианта предлагаемого метода."}
{"text": "First approach combines a language model completion with full utterance to graph parsing.\n", "text_Chinese": "第一种方法将语言模型完成与完整话语结合起来进行图解析。", "text_Arabic": "يجمع النهج الأول بين إكمال نموذج اللغة والنطق الكامل لتحليل الرسم البياني.", "text_French": "La première approche combine la complétion d'un modèle de langage avec un énoncé complet pour l'analyse graphique.", "text_Japanese": "最初のアプローチでは、言語モデルの補完と完全な発話を組み合わせてグラフ解析します。", "text_Russian": "Первый подход сочетает в себе завершение языковой модели с полным анализом графа."}
{"text": "In particular, the prefix after Obama is first completed through a finetuned BART language model and then translated into a program with full offline parser.\n", "text_Chinese": "特别是，Obama 后面的前缀首先通过微调的 BART 语言模型完成，然后翻译成具有完整离线解析器的程序。", "text_Arabic": "على وجه الخصوص، يتم إكمال البادئة بعد أوباما لأول مرة من خلال نموذج لغة BART المضبوط ثم ترجمتها إلى برنامج مع محلل كامل غير متصل بالإنترنت.", "text_French": "En particulier, le préfixe après Obama est d'abord complété via un modèle de langage BART affiné, puis traduit dans un programme avec un analyseur complet hors ligne.", "text_Japanese": "特に、オバマ後の接頭辞は、まず微調整された BART 言語モデルを通じて完成され、次に完全なオフライン パーサーを使用してプログラムに変換されます。", "text_Russian": "В частности, приставка после Обамы сначала дорабатывается посредством доработанной языковой модели BART, а затем переводится в программу с полноценным автономным парсером."}
{"text": "The second approach directly predicts the program from user utterance prefixes.\n", "text_Chinese": "第二种方法直接根据用户话语前缀预测程序。", "text_Arabic": "النهج الثاني يتنبأ مباشرة بالبرنامج من بادئات كلام المستخدم.", "text_French": "La deuxième approche prédit directement le programme à partir des préfixes d’énoncés des utilisateurs.", "text_Japanese": "2 番目のアプローチは、ユーザーの発話プレフィックスからプログラムを直接予測します。", "text_Russian": "Второй подход напрямую прогнозирует программу на основе префиксов высказываний пользователя."}
{"text": "This is achieved by training a single online parser to translate to the goal graph from each prefix.\n", "text_Chinese": "这是通过训练单个在线解析器将每个前缀转换为目标图来​​实现的。", "text_Arabic": "يتم تحقيق ذلك من خلال تدريب محلل واحد عبر الإنترنت للترجمة إلى الرسم البياني للهدف من كل بادئة.", "text_French": "Ceci est réalisé en entraînant un seul analyseur en ligne à traduire en graphique d'objectifs à partir de chaque préfixe.", "text_Japanese": "これは、単一のオンライン パーサーをトレーニングして各プレフィックスから目標グラフに変換することで実現されます。", "text_Russian": "Это достигается за счет обучения одного онлайн-парсера преобразованию в целевой граф каждого префикса."}
{"text": "This facilitates the model to learn the right anticipation.\n", "text_Chinese": "这有助于模型学习正确的预期。", "text_Arabic": "وهذا يسهل النموذج لمعرفة التوقع الصحيح.", "text_French": "Cela permet au modèle d’apprendre la bonne anticipation.", "text_Japanese": "これにより、モデルが正しい予測を学習しやすくなります。", "text_Russian": "Это помогает модели научиться правильному ожиданию."}
{"text": "In a bit more detail, how do we generate these graphs?\n", "text_Chinese": "更详细地说，我们如何生成这些图表？", "text_Arabic": "بمزيد من التفاصيل، كيف يمكننا إنشاء هذه الرسوم البيانية؟", "text_French": "De manière un peu plus détaillée, comment générons-nous ces graphiques ?", "text_Japanese": "もう少し詳しく言うと、これらのグラフはどのように生成するのでしょうか?", "text_Russian": "Более подробно, как мы генерируем эти графики?"}
{"text": "We formulate the problem by generating a serial version of the graph.\n", "text_Chinese": "我们通过生成图表的串行版本来表述问题。", "text_Arabic": "نقوم بصياغة المشكلة عن طريق إنشاء نسخة تسلسلية من الرسم البياني.", "text_French": "Nous formulons le problème en générant une version série du graphique.", "text_Japanese": "グラフのシリアル バージョンを生成することで問題を定式化します。", "text_Russian": "Сформулируем задачу, создав последовательную версию графа."}
{"text": "Each node or edge is represented by an action.\n", "text_Chinese": "每个节点或边都由一个动作表示。", "text_Arabic": "يتم تمثيل كل عقدة أو حافة بإجراء.", "text_French": "Chaque nœud ou bord est représenté par une action.", "text_Japanese": "各ノードまたはエッジはアクションによって表されます。", "text_Russian": "Каждый узел или ребро представлен действием."}
{"text": "Here, we start from the first node.\n", "text_Chinese": "在这里，我们从第一个节点开始。", "text_Arabic": "وهنا نبدأ من العقدة الأولى.", "text_French": "Ici, nous partons du premier nœud.", "text_Japanese": "ここでは、最初のノードから始めます。", "text_Russian": "Здесь мы начинаем с первого узла."}
{"text": "The number below records the absolute index in action history.\n", "text_Chinese": "下面的数字记录了动作历史中的绝对索引。", "text_Arabic": "الرقم أدناه يسجل المؤشر المطلق في تاريخ العمل.", "text_French": "Le nombre ci-dessous enregistre l'indice absolu dans l'historique des actions.", "text_Japanese": "以下の番号は、アクション履歴の絶対インデックスを記録します。", "text_Russian": "Число ниже записывает абсолютный индекс в истории действий."}
{"text": "Then, we got the second node.\n", "text_Chinese": "然后，我们得到了第二个节点。", "text_Arabic": "ثم حصلنا على العقدة الثانية.", "text_French": "Ensuite, nous avons obtenu le deuxième nœud.", "text_Japanese": "次に、2 番目のノードを取得しました。", "text_Russian": "Затем мы получили второй узел."}
{"text": "Next, is the edge between them.\n", "text_Chinese": "接下来，是它们之间的边缘。", "text_Arabic": "التالي هو الحافة بينهما.", "text_French": "Ensuite, c'est la frontière entre eux.", "text_Japanese": "次に、それらの間のエッジです。", "text_Russian": "Далее идет грань между ними."}
{"text": "It contains the pointer to the index of the previous node and the edge label.\n", "text_Chinese": "它包含指向前一个节点索引和边标签的指针。", "text_Arabic": "يحتوي على مؤشر فهرس العقدة السابقة وتسمية الحافة.", "text_French": "Il contient le pointeur vers l'index du nœud précédent et l'étiquette du bord.", "text_Japanese": "これには、前のノードのインデックスへのポインタとエッジ ラベルが含まれます。", "text_Russian": "Он содержит указатель на индекс предыдущего узла и метку ребра."}
{"text": "Zero here means connecting the most recent node with the node generated by the zeroth action and next node next edge.\n", "text_Chinese": "这里的零意味着将最近的节点与第零个动作生成的节点和下一个节点的下一条边连接起来。", "text_Arabic": "الصفر هنا يعني ربط العقدة الأحدث بالعقدة التي تم إنشاؤها بواسطة الإجراء الصفري والحافة التالية للعقدة التالية.", "text_French": "Zéro signifie ici connecter le nœud le plus récent au nœud généré par la dixième action et le nœud suivant au bord suivant.", "text_Japanese": "ここでのゼロは、最新のノードを、0 番目のアクションによって生成されたノードと次のノード、次のエッジに接続することを意味します。", "text_Russian": "Ноль здесь означает соединение самого последнего узла с узлом, созданным нулевым действием, и следующим узлом, следующим ребром."}
{"text": "This process goes on until we generate the full graph.\n", "text_Chinese": "这个过程一直持续到我们生成完整的图。", "text_Arabic": "تستمر هذه العملية حتى نقوم بإنشاء الرسم البياني الكامل.", "text_French": "Ce processus se poursuit jusqu'à ce que nous générions le graphique complet.", "text_Japanese": "このプロセスは、完全なグラフが生成されるまで続きます。", "text_Russian": "Этот процесс продолжается до тех пор, пока мы не сгенерируем полный граф."}
{"text": "The underlying model is based on transformer with self pointing mechanism similar to a previous transition based parser.\n", "text_Chinese": "底层模型基于具有自指向机制的变压器，类似于之前的基于转换的解析器。", "text_Arabic": "يعتمد النموذج الأساسي على محول مزود بآلية التوجيه الذاتي المشابهة للمحلل اللغوي السابق القائم على الانتقال.", "text_French": "Le modèle sous-jacent est basé sur un transformateur avec un mécanisme de pointage automatique similaire à un analyseur précédent basé sur une transition.", "text_Japanese": "基礎となるモデルは、以前の遷移ベースのパーサーと同様の自己ポインティング メカニズムを備えたトランスフォーマーに基づいています。", "text_Russian": "Базовая модель основана на преобразователе с механизмом самоуказания, аналогичным предыдущему анализатору на основе переходов."}
{"text": "After generating a complete graph, we obtained the action level probabilities that correspond to different parts of the graph.\n", "text_Chinese": "生成完整的图后，我们获得了对应于图的不同部分的动作级别概率。", "text_Arabic": "بعد إنشاء رسم بياني كامل، حصلنا على احتمالات مستوى الإجراء التي تتوافق مع أجزاء مختلفة من الرسم البياني.", "text_French": "Après avoir généré un graphique complet, nous avons obtenu les probabilités de niveau d'action qui correspondent aux différentes parties du graphique.", "text_Japanese": "完全なグラフを生成した後、グラフのさまざまな部分に対応するアクション レベルの確率を取得しました。", "text_Russian": "После создания полного графа мы получили вероятности уровня действий, соответствующие различным частям графа."}
{"text": "We select confidence subgraphs based on the thresholding heuristic to be executed.\n", "text_Chinese": "我们根据要执行的阈值启发式选择置信子图。", "text_Arabic": "نختار الرسوم البيانية الفرعية للثقة بناءً على العتبة الإرشادية التي سيتم تنفيذها.", "text_French": "Nous sélectionnons des sous-graphes de confiance en fonction de l'heuristique de seuil à exécuter.", "text_Japanese": "実行するしきい値ヒューリスティックに基づいて信頼サブグラフを選択します。", "text_Russian": "Мы выбираем подграфы достоверности на основе эвристики порогового значения, которая будет выполнена."}
{"text": "Later on, we're going to vary the threshold to achieve different tradeoffs between the latency reduction and the execution cost.\n", "text_Chinese": "稍后，我们将改变阈值以实现延迟减少和执行成本之间的不同权衡。", "text_Arabic": "لاحقًا، سنقوم بتغيير الحد الأدنى لتحقيق مقايضات مختلفة بين تقليل زمن الوصول وتكلفة التنفيذ.", "text_French": "Plus tard, nous allons faire varier le seuil pour obtenir différents compromis entre la réduction de la latence et le coût d'exécution.", "text_Japanese": "後で、しきい値を変更して、レイテンシの削減と実行コストの間のさまざまなトレードオフを実現します。", "text_Russian": "Позже мы собираемся изменить порог, чтобы добиться различных компромиссов между уменьшением задержки и стоимостью выполнения."}
{"text": "For formal evaluation of the online methods, we propose final latency reduction or FLR metric.\n", "text_Chinese": "为了对在线方法进行正式评估，我们提出了最终的延迟减少或 FLR 指标。", "text_Arabic": "لإجراء تقييم رسمي للطرق عبر الإنترنت، نقترح تقليل زمن الوصول النهائي أو قياس FLR.", "text_French": "Pour une évaluation formelle des méthodes en ligne, nous proposons une réduction de latence finale ou métrique FLR.", "text_Japanese": "オンライン手法の正式な評価のために、最終的な遅延短縮または FLR メトリックを提案します。", "text_Russian": "Для формальной оценки онлайн-методов мы предлагаем окончательную метрику сокращения задержки или FLR."}
{"text": "Here's a recap of how an offline system finishes the execution timeline.\n", "text_Chinese": "以下是离线系统如何完成执行时间线的回顾。", "text_Arabic": "فيما يلي ملخص لكيفية قيام النظام غير المتصل بالإنترنت بإنهاء الجدول الزمني للتنفيذ.", "text_French": "Voici un récapitulatif de la façon dont un système hors ligne termine la chronologie d'exécution.", "text_Japanese": "ここでは、オフライン システムがどのように実行タイムラインを完了するかを要約します。", "text_Russian": "Вот краткий обзор того, как автономная система завершает временную шкалу выполнения."}
{"text": "In online systems, execution overlaps with the utterance timeline, so it ends earlier.\n", "text_Chinese": "在在线系统中，执行与话语时间线重叠，因此它会更早结束。", "text_Arabic": "في الأنظمة عبر الإنترنت، يتداخل التنفيذ مع الجدول الزمني للكلام، لذلك ينتهي في وقت مبكر.", "text_French": "Dans les systèmes en ligne, l’exécution chevauche la chronologie de l’énoncé, elle se termine donc plus tôt.", "text_Japanese": "オンライン システムでは、実行が発話タイムラインと重なるため、より早く終了します。", "text_Russian": "В онлайн-системах исполнение совпадает с временной шкалой высказывания, поэтому оно заканчивается раньше."}
{"text": "FLR is defined as the reduction time compared to the offline system, marked by the end of the execution.\n", "text_Chinese": "FLR定义为与离线系统相比减少的时间，以执行结束为​​标志。", "text_Arabic": "يتم تعريف FLR على أنه وقت التخفيض مقارنة بالنظام غير المتصل بالإنترنت، والذي يتميز بنهاية التنفيذ.", "text_French": "FLR est défini comme le temps de réduction par rapport au système hors ligne, marqué par la fin de l'exécution.", "text_Japanese": "FLR は、実行の終了によってマークされる、オフライン システムと比較した短縮時間として定義されます。", "text_Russian": "FLR определяется как время сокращения по сравнению с автономной системой, отмеченное окончанием выполнения."}
{"text": "We conduct experiments on two large conversational semantic parsing datasets, SMCalFlow and TreeDST.\n", "text_Chinese": "我们在两个大型会话语义解析数据集 SMCalFlow 和 TreeDST 上进行了实验。", "text_Arabic": "نحن نجري تجارب على مجموعتي بيانات كبيرتين للتحليل الدلالي للمحادثة، SMCalFlow وTreeDST.", "text_French": "Nous menons des expériences sur deux grands ensembles de données d'analyse sémantique conversationnelle, SMCalFlow et TreeDST.", "text_Japanese": "私たちは、SMCalFlow と TreeDST という 2 つの大規模な会話型セマンティック解析データセットで実験を実施します。", "text_Russian": "Мы проводим эксперименты с двумя большими наборами данных семантического анализа диалогов: SMCalFlow и TreeDST."}
{"text": "Our graph based parser when operating offline, achieves state-of-the-art performance on parsing on both datasets.\n", "text_Chinese": "我们基于图的解析器在离线运行时，在解析两个数据集时实现了最先进的性能。", "text_Arabic": "يحقق المحلل اللغوي المعتمد على الرسم البياني لدينا، عند العمل دون الاتصال بالإنترنت، أداءً متطورًا في التحليل على مجموعتي البيانات.", "text_French": "Notre analyseur basé sur des graphiques, lorsqu'il fonctionne hors ligne, atteint des performances de pointe en matière d'analyse sur les deux ensembles de données.", "text_Japanese": "当社のグラフベースのパーサーは、オフラインで動作する場合、両方のデータセットの解析で最先端のパフォーマンスを実現します。", "text_Russian": "Наш анализатор на основе графов при работе в автономном режиме обеспечивает высочайшую производительность при анализе обоих наборов данных."}
{"text": "The LM complete model also achieves nontrivial BLEU gain compared with the simple baseline of node completion.\n", "text_Chinese": "与节点完成的简单基线相比，LM 完整模型还实现了不平凡的 BLEU 增益。", "text_Arabic": "يحقق نموذج LM الكامل أيضًا مكاسب BLEU غير تافهة مقارنة بالخط الأساسي البسيط لإكمال العقدة.", "text_French": "Le modèle complet LM permet également d'obtenir un gain BLEU non trivial par rapport à la simple ligne de base d'achèvement des nœuds.", "text_Japanese": "LM 完全モデルは、ノード完了の単純なベースラインと比較して、自明ではない BLEU ゲインも達成します。", "text_Russian": "Полная модель LM также обеспечивает нетривиальный выигрыш BLEU по сравнению с простой базовой моделью завершения узла."}
{"text": "Now, let's look at the prediction accuracy of our prefix to graph parser.\n", "text_Chinese": "现在，让我们看看图解析器前缀的预测准确性。", "text_Arabic": "الآن، دعونا نلقي نظرة على دقة التنبؤ لبادئة محلل الرسم البياني لدينا.", "text_French": "Examinons maintenant la précision de la prédiction de notre préfixe à l'analyseur graphique.", "text_Japanese": "次に、グラフ パーサーへのプレフィックスの予測精度を見てみましょう。", "text_Russian": "Теперь давайте посмотрим на точность прогнозирования нашего префикса для анализатора графов."}
{"text": "We test the match F1 score of graph tuples between the generation and the go graph in validation data in y axis for each prefix length in x axis represented by percentages.\n", "text_Chinese": "我们在 y 轴的验证数据中测试生成图和 go 图之间图元组的匹配 F1 分数，以百分比表示 x 轴中的每个前缀长度。", "text_Arabic": "نقوم باختبار درجة المطابقة F1 لمجموعات الرسم البياني بين التوليد والرسم البياني go في بيانات التحقق من الصحة في المحور y لكل طول بادئة في المحور x ممثلاً بالنسب المئوية.", "text_French": "Nous testons le score de correspondance F1 des tuples de graphiques entre la génération et le graphique go dans les données de validation sur l'axe y pour chaque longueur de préfixe sur l'axe x représentée par des pourcentages.", "text_Japanese": "パーセンテージで表される x 軸の各プレフィックス長について、y 軸の検証データの世代グラフと go グラフの間のグラフ タプルの一致 F1 スコアをテストします。", "text_Russian": "Мы проверяем соответствие оценки F1 кортежей графов между генерацией и графиком перехода в данных проверки по оси Y для каждой длины префикса по оси X, представленной в процентах."}
{"text": "Each of these curves represents a different model with the only difference in training data.\n", "text_Chinese": "每条曲线都代表不同的模型，唯一的区别在于训练数据。", "text_Arabic": "يمثل كل من هذه المنحنيات نموذجًا مختلفًا مع الاختلاف الوحيد في بيانات التدريب.", "text_French": "Chacune de ces courbes représente un modèle différent avec la seule différence dans les données d'entraînement.", "text_Japanese": "これらの各曲線は、トレーニング データのみが異なる異なるモデルを表しています。", "text_Russian": "Каждая из этих кривых представляет собой отдельную модель с единственной разницей в обучающих данных."}
{"text": "The bottom curve is the offline parser, and we mix in prefix data in different lengths to transition the model to an online parser.\n", "text_Chinese": "底部曲线是离线解析器，我们混合不同长度的前缀数据，将模型转换为在线解析器。", "text_Arabic": "المنحنى السفلي هو المحلل اللغوي غير المتصل بالإنترنت، ونحن نمزج بيانات البادئة بأطوال مختلفة لنقل النموذج إلى محلل عبر الإنترنت.", "text_French": "La courbe inférieure est l'analyseur hors ligne, et nous mélangeons des données de préfixe de différentes longueurs pour faire passer le modèle à un analyseur en ligne.", "text_Japanese": "下の曲線はオフライン パーサーであり、さまざまな長さのプレフィックス データを混合して、モデルをオンライン パーサーに移行します。", "text_Russian": "Нижняя кривая — это автономный анализатор, и мы смешиваем префиксные данные разной длины, чтобы перевести модель в онлайн-анализатор."}
{"text": "For example, the legend prefix eighty percent plus means the model is trained with prefix data with prefix length larger than eighty percent of the full utterance length.\n", "text_Chinese": "例如，图例前缀百分之八十加意味着模型是使用前缀长度大于完整话语长度百分之八十的前缀数据进行训练的。", "text_Arabic": "على سبيل المثال، تعني بادئة وسيلة الإيضاح ثمانين بالمائة زائد أن النموذج قد تم تدريبه باستخدام بيانات بادئة بطول بادئة أكبر من ثمانين بالمائة من طول الكلام الكامل.", "text_French": "Par exemple, le préfixe de légende quatre-vingts pour cent plus signifie que le modèle est entraîné avec des données de préfixe dont la longueur de préfixe est supérieure à quatre-vingts pour cent de la longueur totale de l'énoncé.", "text_Japanese": "たとえば、凡例のプレフィックス 80% プラスは、発話全体の長さの 80% を超えるプレフィックス長のプレフィックス データを使用してモデルがトレーニングされていることを意味します。", "text_Russian": "Например, префикс легенды восемьдесят процентов плюс означает, что модель обучается с использованием данных префикса с длиной префикса, превышающей восемьдесят процентов от полной длины высказывания."}
{"text": "The upper left corner is the desired area.\n", "text_Chinese": "左上角是所需的区域。", "text_Arabic": "الزاوية اليسرى العليا هي المنطقة المطلوبة.", "text_French": "Le coin supérieur gauche est la zone souhaitée.", "text_Japanese": "左上隅が目的の領域です。", "text_Russian": "Левый верхний угол – это искомая область."}
{"text": "As we can see, the offline parser in black curve is not doing well on the prefix data.\n", "text_Chinese": "正如我们所看到的，黑色曲线中的离线解析器在前缀数据上表现不佳。", "text_Arabic": "كما نرى، فإن المحلل اللغوي غير المتصل بالمنحنى الأسود لا يعمل بشكل جيد على بيانات البادئة.", "text_French": "Comme nous pouvons le constater, l’analyseur hors ligne en courbe noire ne fonctionne pas bien sur les données de préfixe.", "text_Japanese": "ご覧のとおり、黒い曲線のオフライン パーサーはプレフィックス データに対して適切に機能していません。", "text_Russian": "Как мы видим, автономный парсер (черная кривая) не очень хорошо справляется с данными префикса."}
{"text": "As we're mixing more prefixes in training, the curve is lifting upper and left, performing better on all the prefix lengths.\n", "text_Chinese": "当我们在训练中混合更多前缀时，曲线向左上方提升，在所有前缀长度上表现更好。", "text_Arabic": "نظرًا لأننا نقوم بدمج المزيد من البادئات في التدريب، فإن المنحنى يرتفع إلى الأعلى واليسار، مما يؤدي إلى أداء أفضل على جميع أطوال البادئات.", "text_French": "Au fur et à mesure que nous mélangeons davantage de préfixes lors de l'entraînement, la courbe s'élève en haut et à gauche, ce qui donne de meilleurs résultats sur toutes les longueurs de préfixes.", "text_Japanese": "トレーニングでより多くのプレフィックスを混合すると、曲線は上と左に上昇し、すべてのプレフィックス長でパフォーマンスが向上します。", "text_Russian": "По мере того, как мы смешиваем больше префиксов во время обучения, кривая поднимается вверх и влево, обеспечивая лучшую производительность для всех длин префиксов."}
{"text": "However, the full utterance parsing performance is not affected in the upper right dot.\n", "text_Chinese": "然而，右上点的完整话语解析性能不受影响。", "text_Arabic": "ومع ذلك، لا يتأثر أداء تحليل الكلام الكامل في النقطة العلوية اليمنى.", "text_French": "Cependant, les performances d'analyse complète de l'énoncé ne sont pas affectées dans le point supérieur droit.", "text_Japanese": "ただし、右上のドットでは、完全な発話解析パフォーマンスは影響を受けません。", "text_Russian": "Однако это не влияет на полную производительность синтаксического анализа высказываний в верхней правой точке."}
{"text": "Based on these strong results, how much latency do we reduce?\n", "text_Chinese": "基于这些强有力的结果，我们可以减少多少延迟？", "text_Arabic": "بناءً على هذه النتائج القوية، ما مقدار زمن الوصول الذي يمكننا تقليله؟", "text_French": "Sur la base de ces excellents résultats, quel degré de latence pouvons-nous réduire ?", "text_Japanese": "これらの優れた結果に基づいて、レイテンシはどのくらい短縮されるでしょうか?", "text_Russian": "Основываясь на этих впечатляющих результатах, насколько мы можем сократить задержку?"}
{"text": "We measure the time by the number of source tokens and simulate different function execution times.\n", "text_Chinese": "我们通过源令牌的数量来测量时间并模拟不同的函数执行时间。", "text_Arabic": "نحن نقيس الوقت بعدد الرموز المصدرية ونحاكي أوقات تنفيذ الوظائف المختلفة.", "text_French": "Nous mesurons le temps par le nombre de jetons sources et simulons différents temps d'exécution de fonctions.", "text_Japanese": "ソース トークンの数によって時間を測定し、さまざまな関数の実行時間をシミュレートします。", "text_Russian": "Мы измеряем время по количеству исходных токенов и моделируем различное время выполнения функций."}
{"text": "The curves show the tradeoff between the FLR metric and the execution cost, measured by the number of excessive function costs that are not correct.\n", "text_Chinese": "曲线显示了 FLR 指标和执行成本之间的权衡，执行成本通过不正确的过多函数成本的数量来衡量。", "text_Arabic": "تُظهر المنحنيات المفاضلة بين مقياس FLR وتكلفة التنفيذ، والتي يتم قياسها بعدد تكاليف الوظائف المفرطة غير الصحيحة.", "text_French": "Les courbes montrent le compromis entre la métrique FLR et le coût d'exécution, mesuré par le nombre de coûts de fonction excessifs qui ne sont pas corrects.", "text_Japanese": "曲線は、FLR メトリックと、正しくない過剰な関数コストの数によって測定される実行コストとの間のトレードオフを示しています。", "text_Russian": "Кривые показывают компромисс между метрикой FLR и стоимостью выполнения, измеряемой количеством чрезмерных затрат функций, которые являются неправильными."}
{"text": "This is achieved by varying the subgraph selection threshold.\n", "text_Chinese": "这是通过改变子图选择阈值来实现的。", "text_Arabic": "يتم تحقيق ذلك عن طريق تغيير عتبة اختيار الرسم البياني الفرعي.", "text_French": "Ceci est réalisé en faisant varier le seuil de sélection du sous-graphe.", "text_Japanese": "これは、サブグラフ選択のしきい値を変更することによって実現されます。", "text_Russian": "Это достигается путем изменения порога выбора подграфа."}
{"text": "A higher threshold selects fewer functions of mistake, but obtains a smaller FLR, whereas the lower threshold more aggressively selects and executes programs.\n", "text_Chinese": "较高的阈值选择错误的函数较少，但获得较小的FLR，而较低的阈值则更积极地选择和执行程序。", "text_Arabic": "تختار العتبة الأعلى عددًا أقل من وظائف الخطأ، ولكنها تحصل على FLR أصغر، في حين أن العتبة السفلية تختار البرامج وتنفذها بشكل أكثر قوة.", "text_French": "Un seuil plus élevé sélectionne moins de fonctions d'erreur, mais obtient un FLR plus petit, tandis que le seuil inférieur sélectionne et exécute les programmes de manière plus agressive.", "text_Japanese": "しきい値を高くすると、誤った関数が選択されることは少なくなりますが、得られる FLR は小さくなります。一方、しきい値を低くすると、プログラムがより積極的に選択されて実行されます。", "text_Russian": "Более высокий порог выбирает меньше функций ошибки, но получает меньший FLR, тогда как более низкий порог более агрессивно выбирает и выполняет программы."}
{"text": "We compare the two approaches we propose and a baseline that does nothing but directly applying the offline parser for online use.\n", "text_Chinese": "我们比较了我们提出的两种方法和一个除了直接应用离线解析器以供在线使用之外什么也不做的基线。", "text_Arabic": "نحن نقارن بين الطريقتين اللتين نقترحهما وخط الأساس الذي لا يفعل شيئًا سوى تطبيق المحلل اللغوي بشكل مباشر للاستخدام عبر الإنترنت.", "text_French": "Nous comparons les deux approches que nous proposons et une base de référence qui ne fait rien d'autre que d'appliquer directement l'analyseur hors ligne pour une utilisation en ligne.", "text_Japanese": "私たちが提案する 2 つのアプローチと、オフライン パーサーをオンラインで使用するために直接適用するだけのベースラインを比較します。", "text_Russian": "Мы сравниваем два подхода, которые мы предлагаем, и базовый вариант, который ничего не делает, кроме непосредственного применения автономного парсера для онлайн-использования."}
{"text": "The upper left region is has the best FLR and cost tradeoff.\n", "text_Chinese": "左上方区域具有最佳的 FOR 和成本权衡。", "text_Arabic": "المنطقة العلوية اليسرى لديها أفضل FOR ومقايضة التكلفة.", "text_French": "La région supérieure gauche présente le meilleur compromis FOR et coût.", "text_Japanese": "左上の領域は、FOR とコストのトレードオフが最も優れています。", "text_Russian": "Верхняя левая область имеет лучший компромисс между ценой и стоимостью."}
{"text": "We see both of our methods beat the baseline by a large margin, and they perform more similarly on TreeDST.\n", "text_Chinese": "我们看到我们的两种方法都大大超过了基线，并且它们在 TreeDST 上的表现更加相似。", "text_Arabic": "نرى أن كلتا الطريقتين تتفوقان على خط الأساس بهامش كبير، كما أنهما يؤديان بشكل أكثر تشابهًا على TreeDST.", "text_French": "Nous constatons que nos deux méthodes dépassent largement la ligne de base et qu'elles fonctionnent de manière plus similaire sur TreeDST.", "text_Japanese": "どちらの方法もベースラインを大幅に上回っており、TreeDST では同様のパフォーマンスを示しています。", "text_Russian": "Мы видим, что оба наших метода значительно превосходят базовый уровень, и они работают более похоже на TreeDST."}
{"text": "While individual function execution is faster, there tends to be more run executions and lower latency reduction room.\n", "text_Chinese": "虽然单个函数执行速度更快，但往往会有更多的运行执行和更低的延迟减少空间。", "text_Arabic": "على الرغم من أن تنفيذ الوظائف الفردية يكون أسرع، إلا أنه يميل إلى أن يكون هناك المزيد من عمليات تنفيذ التشغيل ومساحة أقل لتقليل زمن الوصول.", "text_French": "Bien que l'exécution de fonctions individuelles soit plus rapide, il y a généralement plus d'exécutions et une marge de réduction de la latence plus faible.", "text_Japanese": "個々の関数の実行は高速になりますが、実行回数が多くなり、レイテンシー削減の余地が小さくなる傾向があります。", "text_Russian": "Хотя выполнение отдельных функций происходит быстрее, обычно выполняется больше запусков и меньше возможностей для сокращения задержек."}
{"text": "When individual function execution is slower, there is more room for FLR improvement.\n", "text_Chinese": "当单个函数执行速度较慢时，FLR 的改进空间就更大。", "text_Arabic": "عندما يكون تنفيذ الوظيفة الفردية أبطأ، يكون هناك مجال أكبر لتحسين FLR.", "text_French": "Lorsque l’exécution d’une fonction individuelle est plus lente, il y a plus de place pour l’amélioration du FLR.", "text_Japanese": "個々の関数の実行が遅い場合、FLR を改善する余地がより多くなります。", "text_Russian": "Когда выполнение отдельных функций происходит медленнее, появляется больше возможностей для улучшения FLR."}
{"text": "Our two approaches achieve better performance in different cost cost regions.\n", "text_Chinese": "我们的两种方法在不同的成本成本区域中实现了更好的性能。", "text_Arabic": "يحقق منهجانا أداءً أفضل في مناطق التكلفة المختلفة.", "text_French": "Nos deux approches permettent d'obtenir de meilleures performances dans différentes régions de coûts.", "text_Japanese": "私たちの 2 つのアプローチは、さまざまなコスト領域でより優れたパフォーマンスを実現します。", "text_Russian": "Наши два подхода обеспечивают лучшую производительность в разных регионах затрат."}
{"text": "Overall, we achieve thirty to sixty three percent relative latency reduction depending on execution time and allowed cost.\n", "text_Chinese": "总体而言，我们实现了 30% 到 63% 的相对延迟减少，具体取决于执行时间和允许的成本。", "text_Arabic": "بشكل عام، نحقق تقليلًا نسبيًا لزمن الاستجابة بنسبة تتراوح بين ثلاثين وثلاثة وستين بالمائة اعتمادًا على وقت التنفيذ والتكلفة المسموح بها.", "text_French": "Dans l'ensemble, nous obtenons une réduction de la latence relative de trente à soixante-trois pour cent en fonction du temps d'exécution et du coût autorisé.", "text_Japanese": "全体として、実行時間と許容されるコストに応じて、相対的なレイテンシーの 30 ～ 63% の削減を達成します。", "text_Russian": "В целом мы достигаем снижения относительной задержки на тридцать-шестьдесят три процента в зависимости от времени выполнения и допустимой стоимости."}
{"text": "Finally, we have a breakdown of average latency reduction in tokens for each type of the function node when the allowed cost is three run executions.\n", "text_Chinese": "最后，当允许的成本为三次运行执行时，我们对每种类型的功能节点的平均延迟减少的令牌进行了细分。", "text_Arabic": "أخيرًا، لدينا تحليل لمتوسط ​​تقليل زمن الوصول في الرموز المميزة لكل نوع من عقدة الوظيفة عندما تكون التكلفة المسموح بها هي ثلاث عمليات تنفيذ.", "text_French": "Enfin, nous avons une répartition de la réduction moyenne de la latence des jetons pour chaque type de nœud de fonction lorsque le coût autorisé est de trois exécutions.", "text_Japanese": "最後に、許容コストが 3 回の実行である場合の、関数ノードのタイプごとのトークンの平均レイテンシー削減の内訳を示します。", "text_Russian": "Наконец, у нас есть разбивка по снижению средней задержки в токенах для каждого типа функционального узла, когда разрешенная стоимость равна трем выполнениям."}
{"text": "As we can see, there are gains all over the board.\n", "text_Chinese": "正如我们所看到的，各方面都有所收获。", "text_Arabic": "وكما نرى، هناك مكاسب في جميع المجالات.", "text_French": "Comme nous pouvons le constater, il y a des gains partout.", "text_Japanese": "ご覧のとおり、全面的に利益が得られています。", "text_Russian": "Как мы видим, успехи есть по всем направлениям."}
{"text": "There are also some functions on which we gain impressive latency reduction where the red bar is much longer, such as find manager and recipient.\n", "text_Chinese": "还有一些功能，我们获得了令人印象深刻的延迟减少，其中红色条更长，例如查找经理和收件人。", "text_Arabic": "هناك أيضًا بعض الوظائف التي نحصل فيها على تقليل زمن الوصول بشكل مذهل حيث يكون الشريط الأحمر أطول بكثير، مثل مدير البحث والمستلم.", "text_French": "Il existe également certaines fonctions sur lesquelles on obtient une réduction de latence impressionnante où la barre rouge est beaucoup plus longue, comme find manager et destinataire.", "text_Japanese": "マネージャーや受信者の検索など、赤色のバーがはるかに長い機能では、レイテンシーが大幅に短縮されます。", "text_Russian": "Есть также некоторые функции, в которых мы добились впечатляющего снижения задержки, где красная полоса намного длиннее, например поиск менеджера и получателя."}
{"text": "These are low level functions that do not have much dependency on others.\n", "text_Chinese": "这些是低级函数，对其他函数没有太多依赖性。", "text_Arabic": "هذه وظائف منخفضة المستوى ولا تعتمد كثيرًا على الآخرين.", "text_French": "Ce sont des fonctions de bas niveau qui ne dépendent pas beaucoup des autres.", "text_Japanese": "これらは、他への依存性があまりない低レベルの関数です。", "text_Russian": "Это функции низкого уровня, которые не сильно зависят от других."}
{"text": "In conclusion, we proposed online semantic parsing as new task to explore with the rigorous latency reduction metric.\n", "text_Chinese": "总之，我们提出在线语义解析作为新任务，以严格的延迟减少指标进行探索。", "text_Arabic": "في الختام، اقترحنا التحليل الدلالي عبر الإنترنت كمهمة جديدة لاستكشافها باستخدام المقياس الصارم لتقليل زمن الوصول.", "text_French": "En conclusion, nous avons proposé l’analyse sémantique en ligne comme nouvelle tâche à explorer avec la métrique rigoureuse de réduction de latence.", "text_Japanese": "結論として、私たちは、厳密なレイテンシ削減メトリックを使用して探索する新しいタスクとして、オンライン セマンティック解析を提案しました。", "text_Russian": "В заключение мы предложили онлайн-семантический анализ в качестве новой задачи для изучения с помощью строгой метрики сокращения задержки."}
{"text": "With a strong graph based semantic parser, we achieve relatively good latency reduction either through our pipeline approach with LM completion and a full parser or directly through a learned parser on the prefixes.\n", "text_Chinese": "借助强大的基于图的语义解析器，我们可以通过具有 LM 完成和完整解析器的管道方法或直接通过前缀上的学习解析器实现相对较好的延迟减少。", "text_Arabic": "باستخدام محلل دلالي قوي قائم على الرسم البياني، نحقق تقليلًا جيدًا نسبيًا لوقت الاستجابة إما من خلال نهج خط الأنابيب الخاص بنا مع إكمال LM ومحلل كامل أو مباشرة من خلال محلل متعلم على البادئات.", "text_French": "Avec un analyseur sémantique puissant basé sur des graphes, nous obtenons une réduction de latence relativement bonne, soit grâce à notre approche pipeline avec complétion LM et un analyseur complet, soit directement via un analyseur appris sur les préfixes.", "text_Japanese": "強力なグラフ ベースのセマンティック パーサーを使用すると、LM 補完と完全なパーサーを使用したパイプライン アプローチを通じて、またはプレフィックスで学習したパーサーを直接介して、比較的良好なレイテンシの削減を実現します。", "text_Russian": "Благодаря мощному семантическому анализатору на основе графов мы достигаем относительно хорошего снижения задержки либо с помощью нашего конвейерного подхода с завершением LM и полным анализатором, либо напрямую с помощью обученного анализатора префиксов."}
{"text": "Moreover, our approach can be a general framework and can be applied to other executable semantic representations in different domains.\n", "text_Chinese": "此外，我们的方法可以是一个通用框架，并且可以应用于不同领域中的其他可执行语义表示。", "text_Arabic": "علاوة على ذلك، يمكن أن يكون نهجنا إطارًا عامًا ويمكن تطبيقه على تمثيلات دلالية أخرى قابلة للتنفيذ في مجالات مختلفة.", "text_French": "De plus, notre approche peut constituer un cadre général et peut être appliquée à d’autres représentations sémantiques exécutables dans différents domaines.", "text_Japanese": "さらに、私たちのアプローチは一般的なフレームワークとなり、さまざまなドメインの他の実行可能な意味表現にも適用できます。", "text_Russian": "Более того, наш подход может представлять собой общую основу и применяться к другим исполняемым семантическим представлениям в разных областях."}
{"text": "Future works could explore smarter prediction and execution integration method.\n", "text_Chinese": "未来的工作可以探索更智能的预测和执行集成方法。", "text_Arabic": "يمكن للأعمال المستقبلية استكشاف طريقة أكثر ذكاءً للتنبؤ وتكامل التنفيذ.", "text_French": "Les travaux futurs pourraient explorer une méthode d’intégration de prédiction et d’exécution plus intelligente.", "text_Japanese": "今後の取り組みでは、よりスマートな予測と実行の統合方法を検討する可能性があります。", "text_Russian": "В будущих работах можно будет изучить более разумный метод интеграции прогнозирования и выполнения."}
{"text": "Thanks for your listening.\n", "text_Chinese": "感谢您的聆听。", "text_Arabic": "شكرا لاستماعك.", "text_French": "Merci pour votre écoute.", "text_Japanese": "ご清聴ありがとうございました。", "text_Russian": "Спасибо за ваше внимание."}
{"text": "Hi.\n", "text_Chinese": "你好。", "text_Arabic": "أهلاً.", "text_French": "Salut.", "text_Japanese": "こんにちは。", "text_Russian": "Привет."}
{"text": "I'm going to discuss our work on generating retrieval augmented counterfactuals for question answering tasks.\n", "text_Chinese": "我将讨论我们为问答任务生成检索增强反事实的工作。", "text_Arabic": "سأقوم بمناقشة عملنا في إنشاء حقائق استرجاعية مُعززة مضادة لمهام الإجابة على الأسئلة.", "text_French": "Je vais discuter de notre travail sur la génération de contrefactuels augmentés de récupération pour les tâches de réponse aux questions.", "text_Japanese": "質問応答タスク用の検索拡張反事実を生成する取り組みについて説明します。", "text_Russian": "Я собираюсь обсудить нашу работу по созданию дополненных поиском контрфактов для задач ответа на вопросы."}
{"text": "This is work done during my internship at Google Research, where I was mentored by Matthew Lamm and Ian Tenney.\n", "text_Chinese": "这是我在 Google Research 实习期间完成的工作，在那里我得到了 Matthew Lamm 和 Ian Tenney 的指导。", "text_Arabic": "تم إنجاز هذا العمل أثناء فترة تدريبي في Google Research، حيث تم إرشادي من قبل ماثيو لام وإيان تيني.", "text_French": "Il s'agit d'un travail effectué lors de mon stage chez Google Research, où j'ai été encadré par Matthew Lamm et Ian Tenney.", "text_Japanese": "これは、Google Research でのインターンシップ中にマシュー ラムとイアン テニーの指導を受けた研究です。", "text_Russian": "Эта работа была проделана во время моей стажировки в Google Research, где меня обучали Мэтью Ламм и Ян Тенни."}
{"text": "To motivate the task, let me begin by defining a counterfactual.\n", "text_Chinese": "为了激励这项任务，让我首先定义一个反事实。", "text_Arabic": "لتحفيز المهمة، اسمحوا لي أن أبدأ بتعريف الواقع المخالف.", "text_French": "Pour motiver la tâche, permettez-moi de commencer par définir un contrefactuel.", "text_Japanese": "このタスクを動機付けるために、反事実を定義することから始めましょう。", "text_Russian": "Чтобы мотивировать задачу, позвольте мне начать с определения контрфакта."}
{"text": "In this work, we define a counterfactual as a perturbation of the input text that differs in some meaningful controlled way from the original text.\n", "text_Chinese": "在这项工作中，我们将反事实定义为输入文本的扰动，其在某种有意义的受控方式上与原始文本不同。", "text_Arabic": "في هذا العمل، قمنا بتعريف الواقع المخالف على أنه اضطراب في النص المدخل يختلف بطريقة ذات معنى ومتحكم فيها عن النص الأصلي.", "text_French": "Dans ce travail, nous définissons un contrefactuel comme une perturbation du texte d'entrée qui diffère d'une manière significative et contrôlée du texte original.", "text_Japanese": "この研究では、反事実を、意味のある制御された方法で元のテキストとは異なる入力テキストの摂動として定義します。", "text_Russian": "В этой работе мы определяем контрфактуал как возмущение входного текста, которое каким-то значимым контролируемым образом отличается от исходного текста."}
{"text": "And allows us to reason about the changes in the outcome or the task label.\n", "text_Chinese": "并允许我们推断结果或任务标签的变化。", "text_Arabic": "ويتيح لنا التفكير في التغييرات في النتيجة أو تسمية المهمة.", "text_French": "Et nous permet de raisonner sur les changements dans le résultat ou dans l’étiquette de la tâche.", "text_Japanese": "そして、結果やタスクのラベルの変化について推論することができます。", "text_Russian": "И позволяет нам рассуждать об изменениях в результате или метке задачи."}
{"text": "For instance, changing the words fascinating to captivating or expected to mind-numbing changes the sentiment for this movie review.\n", "text_Chinese": "例如，将“迷人”一词改为“迷人”或“预期”改为“头脑麻木”会改变这篇影评的情绪。", "text_Arabic": "على سبيل المثال، تغيير الكلمات الرائعة إلى الآسرة أو المتوقع إلى الذهول يغير الشعور تجاه مراجعة الفيلم.", "text_French": "Par exemple, changer les mots fascinant en captivant ou attendu en abrutissant change le sentiment de cette critique de film.", "text_Japanese": "たとえば、「魅力的」という言葉を「魅惑的な」または「予想外」に変えると、この映画レビューの感情が変わります。", "text_Russian": "Например, изменение слов «захватывающий» на «захватывающий» или «ожидаемый отупляющий» меняет настроение этого обзора фильма."}
{"text": "Similarly, adding the qualifier women's to the question changes the answer to the question in the example below.\n", "text_Chinese": "同样，在问题中添加限定词“女性”会更改以下示例中问题的答案。", "text_Arabic": "وبالمثل، فإن إضافة المؤهل \"سيدات\" إلى السؤال يؤدي إلى تغيير إجابة السؤال في المثال أدناه.", "text_French": "De même, l'ajout du qualificatif « femmes » à la question modifie la réponse à la question dans l'exemple ci-dessous.", "text_Japanese": "同様に、質問に修飾語「women's」を追加すると、以下の例の質問の答えが変わります。", "text_Russian": "Аналогично, добавление к вопросу уточнения «женщины» меняет ответ на вопрос в приведенном ниже примере."}
{"text": "Humans are typically robust to such perturbations compared to NLP models trained on the task.\n", "text_Chinese": "与针对该任务训练的 NLP 模型相比，人类通常对此类扰动具有鲁棒性。", "text_Arabic": "عادة ما يكون البشر أقوياء في مواجهة مثل هذه الاضطرابات مقارنة بنماذج البرمجة اللغوية العصبية المدربة على هذه المهمة.", "text_French": "Les humains sont généralement robustes à de telles perturbations par rapport aux modèles PNL formés à cette tâche.", "text_Japanese": "人間は通常、タスクでトレーニングされた NLP モデルと比較して、そのような摂動に対して堅牢です。", "text_Russian": "Люди обычно устойчивы к таким возмущениям по сравнению с моделями НЛП, обученными этой задаче."}
{"text": "Why is that?\n", "text_Chinese": "这是为什么？", "text_Arabic": "لماذا هذا؟", "text_French": "Pourquoi donc?", "text_Japanese": "何故ですか？", "text_Russian": "Почему это?"}
{"text": "The dataset may be sampled with systematic biases that lead to a simple decision boundary that is violated by the counterfactual.\n", "text_Chinese": "数据集的采样可能带有系统偏差，从而导致简单的决策边界被反事实所违反。", "text_Arabic": "قد يتم أخذ عينات من مجموعة البيانات بتحيزات منهجية تؤدي إلى حدود قرار بسيطة تنتهكها الحقيقة المغايرة.", "text_French": "L'ensemble de données peut être échantillonné avec des biais systématiques qui conduisent à une simple limite de décision qui est violée par le contrefactuel.", "text_Japanese": "データセットは、反事実によって違反される単純な決定境界につながる体系的なバイアスを伴ってサンプリングされる可能性があります。", "text_Russian": "Набор данных может быть выбран с систематическими отклонениями, которые приводят к простой границе принятия решения, которая нарушается контрфактическими данными."}
{"text": "As shown in this 2D classification problem.\n", "text_Chinese": "如该二维分类问题所示。", "text_Arabic": "كما هو موضح في مشكلة التصنيف ثنائية الأبعاد هذه.", "text_French": "Comme le montre ce problème de classification 2D.", "text_Japanese": "この 2D 分類問題に示されているように。", "text_Russian": "Как показано в этой задаче двумерной классификации."}
{"text": "My work has found that adding counterfactual examples to the training data can make the model robust to such perturbations.\n", "text_Chinese": "我的工作发现，在训练数据中添加反事实示例可以使模型对此类扰动具有鲁棒性。", "text_Arabic": "لقد وجد عملي أن إضافة أمثلة مناقضة للواقع إلى بيانات التدريب يمكن أن يجعل النموذج قويًا في مواجهة مثل هذه الاضطرابات.", "text_French": "Mes travaux ont montré que l'ajout d'exemples contrefactuels aux données d'entraînement peut rendre le modèle robuste à de telles perturbations.", "text_Japanese": "私の研究では、反事実の例をトレーニング データに追加すると、モデルがそのような摂動に対して堅牢になることがわかりました。", "text_Russian": "Моя работа показала, что добавление контрфактических примеров к обучающим данным может сделать модель устойчивой к таким возмущениям."}
{"text": "So, if counterfactuals are valuable, how can we generate them?\n", "text_Chinese": "那么，如果反事实很有价值，我们如何生成它们呢？", "text_Arabic": "لذا، إذا كانت الحقائق المضادة ذات قيمة، فكيف يمكننا توليدها؟", "text_French": "Alors, si les contrefactuels sont précieux, comment pouvons-nous les générer ?", "text_Japanese": "では、反事実に価値があるとしたら、どうやって反事実を生成できるのでしょうか?", "text_Russian": "Итак, если контрфакты ценны, как мы можем их генерировать?"}
{"text": "This task is especially hard for NLP because here are three examples from three different NLP tasks.\n", "text_Chinese": "这项任务对于 NLP 来说尤其困难，因为这里有来自三个不同 NLP 任务的三个示例。", "text_Arabic": "هذه المهمة صعبة بشكل خاص على البرمجة اللغوية العصبية لأن فيما يلي ثلاثة أمثلة من ثلاث مهام مختلفة في البرمجة اللغوية العصبية.", "text_French": "Cette tâche est particulièrement difficile pour la PNL car voici trois exemples issus de trois tâches PNL différentes.", "text_Japanese": "ここでは 3 つの異なる NLP タスクからの 3 つの例を示しているため、このタスクは NLP にとって特に困難です。", "text_Russian": "Эта задача особенно сложна для НЛП, поскольку вот три примера из трех разных задач НЛП."}
{"text": "As you can see, examples that violate the decision boundary between outcomes need to be very carefully crafted by perturbing some attributes of the text that are underlined here.\n", "text_Chinese": "正如您所看到的，违反结果之间决策边界的示例需要通过扰乱此处下划线的文本的某些属性来精心设计。", "text_Arabic": "كما ترون، فإن الأمثلة التي تنتهك حدود القرار بين النتائج تحتاج إلى صياغة بعناية فائقة عن طريق إزعاج بعض سمات النص التي تم وضع خط تحتها هنا.", "text_French": "Comme vous pouvez le constater, les exemples qui violent la frontière décisionnelle entre les résultats doivent être élaborés avec beaucoup de soin en perturbant certains attributs du texte soulignés ici.", "text_Japanese": "ご覧のとおり、結果間の決定境界に違反する例は、ここで下線が引かれているテキストのいくつかの属性を混乱させることによって、非常に慎重に作成する必要があります。", "text_Russian": "Как видите, примеры, нарушающие границу принятия решений между результатами, должны быть очень тщательно составлены, искажая некоторые атрибуты текста, подчеркнутые здесь."}
{"text": "This could be done by human annotation, but this is expensive and biased.\n", "text_Chinese": "这可以通过人工注释来完成，但这既昂贵又存在偏差。", "text_Arabic": "يمكن أن يتم ذلك عن طريق الشرح البشري، لكن هذا مكلف ومتحيز.", "text_French": "Cela pourrait être fait par annotation humaine, mais cela serait coûteux et biaisé.", "text_Japanese": "これは人間による注釈によって行うこともできますが、コストがかかり、偏りが生じます。", "text_Russian": "Это можно было бы сделать с помощью человеческих аннотаций, но это дорого и необъективно."}
{"text": "Some prior work has focused on using syntax trees or semantic role labeling.\n", "text_Chinese": "之前的一些工作主要集中在使用语法树或语义角色标签。", "text_Arabic": "ركزت بعض الأعمال السابقة على استخدام أشجار بناء الجملة أو تصنيف الأدوار الدلالية.", "text_French": "Certains travaux antérieurs se sont concentrés sur l'utilisation d'arbres syntaxiques ou d'étiquetage de rôles sémantiques.", "text_Japanese": "これまでの研究の中には、構文ツリーまたは意味論的な役割のラベル付けの使用に焦点を当てたものもあります。", "text_Russian": "Некоторые предыдущие работы были сосредоточены на использовании синтаксических деревьев или разметке семантических ролей."}
{"text": "But the set of perturbations generated by these techniques are limited by the semantic framework.\n", "text_Chinese": "但这些技术产生的扰动集受到语义框架的限制。", "text_Arabic": "لكن مجموعة الاضطرابات الناتجة عن هذه التقنيات محدودة بالإطار الدلالي.", "text_French": "Mais l’ensemble des perturbations générées par ces techniques est limité par le cadre sémantique.", "text_Japanese": "ただし、これらの手法によって生成される一連の摂動は、意味論的なフレームワークによって制限されます。", "text_Russian": "Но набор возмущений, генерируемых этими методами, ограничен семантическими рамками."}
{"text": "More recent work has used masked language models to fill in masked portions of the text to change labels.\n", "text_Chinese": "最近的工作使用屏蔽语言模型来填充文本的屏蔽部分以更改标签。", "text_Arabic": "استخدم العمل الأحدث نماذج اللغة المقنعة لملء الأجزاء المقنعة من النص لتغيير التسميات.", "text_French": "Des travaux plus récents ont utilisé des modèles de langage masqué pour remplir des parties masquées du texte afin de modifier les étiquettes.", "text_Japanese": "最近の研究では、マスクされた言語モデルを使用して、テキストのマスクされた部分を埋めてラベルを変更しています。", "text_Russian": "В более поздних работах использовались замаскированные языковые модели для заполнения замаскированных частей текста и изменения меток."}
{"text": "But finding what parts of the text to perturb can be challenging.\n", "text_Chinese": "但找出文本的哪些部分需要干扰可能具有挑战性。", "text_Arabic": "لكن العثور على أجزاء النص التي قد تسبب الاضطراب قد يكون أمرًا صعبًا.", "text_French": "Mais trouver quelles parties du texte perturber peut être un défi.", "text_Japanese": "しかし、テキストのどの部分を混乱させるかを見つけるのは難しい場合があります。", "text_Russian": "Но найти, какие части текста следует исказить, может быть непросто."}
{"text": "There are more challenges to generating counterfactuals for question answering specifically.\n", "text_Chinese": "生成专门用于问答的反事实存在更多挑战。", "text_Arabic": "هناك المزيد من التحديات لتوليد حقائق مضادة للإجابة على الأسئلة على وجه التحديد.", "text_French": "Il est plus difficile de générer des contrefactuels pour répondre spécifiquement aux questions.", "text_Japanese": "特に質問に答えるために反事実を生成するには、さらに多くの課題があります。", "text_Russian": "Есть больше проблем с созданием контрфактов, конкретно для ответов на вопросы."}
{"text": "This task requires background knowledge.\n", "text_Chinese": "这项任务需要背景知识。", "text_Arabic": "تتطلب هذه المهمة معرفة أساسية.", "text_French": "Cette tâche nécessite des connaissances de base.", "text_Japanese": "このタスクには背景知識が必要です。", "text_Russian": "Эта задача требует базовых знаний."}
{"text": "For instance, to perturb the original question is Indiana Jones Temple of Doom a prequel?\n", "text_Chinese": "例如，扰乱最初的问题是印第安纳琼斯末日神庙是前传吗？", "text_Arabic": "على سبيل المثال، لإثارة السؤال الأصلي، هل معبد إنديانا جونز للموت هو مقدمة مسبقة؟", "text_French": "Par exemple, pour perturber la question initiale, Indiana Jones Temple of Doom est-il une préquelle ?", "text_Japanese": "たとえば、最初の質問を混乱させるために、『インディ・ジョーンズ テンプル・オブ・ドゥーム』は前日譚ですか?", "text_Russian": "Например, чтобы нарушить первоначальный вопрос, является ли Храм Судьбы Индианы Джонса приквелом?"}
{"text": "We need to be aware of the other movies in the franchise to get to a question like is Indiana Jones Raiders of the Lost Ark a prequel?\n", "text_Chinese": "我们需要了解该系列中的其他电影才能回答诸如《夺宝奇兵：夺宝奇兵》是前传之类的问题吗？", "text_Arabic": "يجب أن نكون على دراية بالأفلام الأخرى في السلسلة لنصل إلى سؤال مثل: هل إنديانا جونز رايدرز أوف ذا لوست آرك هي مقدمة مسبقة؟", "text_French": "Nous devons être conscients des autres films de la franchise pour répondre à une question telle que Indiana Jones Les Aventuriers de l'Arche Perdue est-il une préquelle ?", "text_Japanese": "「インディ・ジョーンズ レイダース 失われたアーク《聖櫃》は前日譚なのか？」という質問にたどり着くには、シリーズ内の他の映画について知っておく必要があります。", "text_Russian": "Нам нужно знать о других фильмах франшизы, чтобы ответить на вопрос, является ли «Индиана Джонс: В поисках утраченного ковчега» приквелом?"}
{"text": "Furthermore, random perturbations can lead to questions that are not answerable with the available evidence or have false premises.\n", "text_Chinese": "此外，随机扰动可能会导致现有证据无法回答的问题或具有错误前提的问题。", "text_Arabic": "علاوة على ذلك، يمكن أن تؤدي الاضطرابات العشوائية إلى أسئلة لا يمكن الإجابة عليها بالأدلة المتاحة أو تحتوي على مقدمات خاطئة.", "text_French": "De plus, des perturbations aléatoires peuvent conduire à des questions auxquelles il n’est pas possible de répondre avec les preuves disponibles ou qui reposent sur de fausses prémisses.", "text_Japanese": "さらに、ランダムな混乱により、入手可能な証拠では答えられない疑問や、誤った前提が含まれる疑問が生じる可能性があります。", "text_Russian": "Более того, случайные возмущения могут привести к вопросам, на которые невозможно ответить с помощью имеющихся доказательств или которые содержат ложные предпосылки."}
{"text": "Moreover, some question perturbations can lead to significant semantic drift from the original input.\n", "text_Chinese": "此外，某些问题扰动可能会导致与原始输入的语义发生重大偏差。", "text_Arabic": "علاوة على ذلك، يمكن أن تؤدي بعض الاضطرابات في الأسئلة إلى انحراف دلالي كبير عن المدخلات الأصلية.", "text_French": "De plus, certaines perturbations des questions peuvent conduire à une dérive sémantique significative par rapport à l’entrée originale.", "text_Japanese": "さらに、一部の質問の混乱により、元の入力からの意味上の大幅なずれが生じる可能性があります。", "text_Russian": "Более того, некоторые отклонения от вопросов могут привести к значительному семантическому отклонению от исходного ввода."}
{"text": "For instance, this question is Indiana Jones practicing child slavery in Temple of Doom?\n", "text_Chinese": "例如，这个问题是《印第安纳·琼斯》在《末日神殿》中实行儿童奴役吗？", "text_Arabic": "على سبيل المثال، هذا السؤال هل يمارس إنديانا جونز استعباد الأطفال في معبد الموت؟", "text_French": "Par exemple, cette question est-ce qu'Indiana Jones pratique l'esclavage des enfants dans le Temple maudit ?", "text_Japanese": "たとえば、この質問は、インディ・ジョーンズが運命の神殿で児童奴隷制を実践していることですか?", "text_Russian": "Например, вопрос: практикует ли Индиана Джонс детское рабство в «Храме Судьбы»?"}
{"text": "We propose a very simple yet effective technique called retrieve generate filter or RGF, to tackle counterfactual perturbations of questions, and also aims to tackle all the other aforementioned challenges.\n", "text_Chinese": "我们提出了一种非常简单但有效的技术，称为检索生成过滤器或 RGF，来解决问题的反事实扰动，并且还旨在解决所有其他上述挑战。", "text_Arabic": "نقترح تقنية بسيطة للغاية لكنها فعالة تسمى استرداد توليد المرشح أو RGF، لمعالجة الاضطرابات غير الواقعية للأسئلة، وتهدف أيضًا إلى معالجة جميع التحديات الأخرى المذكورة أعلاه.", "text_French": "Nous proposons une technique très simple mais efficace appelée retrieve generate filter ou RGF, pour lutter contre les perturbations contrefactuelles des questions, et vise également à relever tous les autres défis susmentionnés.", "text_Japanese": "私たちは、質問の反事実的摂動に取り組むために、検索生成フィルター (RGF) と呼ばれる非常にシンプルだが効果的な手法を提案します。また、前述の他のすべての課題に取り組むことも目的としています。", "text_Russian": "Мы предлагаем очень простой, но эффективный метод, называемый фильтром извлечения генерации или RGF, для решения контрфактических искажений вопросов, а также направлен на решение всех других вышеупомянутых проблем."}
{"text": "The core intuition behind RGF is that the necessary background information that is needed to generate perturbations may be present in the near misses made by a question answering model.\n", "text_Chinese": "RGF 背后的核心直觉是，生成扰动所需的必要背景信息可能存在于问答模型的未遂事件中。", "text_Arabic": "الحدس الأساسي وراء RGF هو أن المعلومات الأساسية الضرورية اللازمة لتوليد الاضطرابات قد تكون موجودة في الأخطاء الوشيكة التي يحدثها نموذج الإجابة على الأسئلة.", "text_French": "L'intuition fondamentale derrière RGF est que les informations de base nécessaires pour générer des perturbations peuvent être présentes dans les quasi-accidents réalisés par un modèle de questions-réponses.", "text_Japanese": "RGF の背後にある中心となる直感は、摂動を生成するために必要な背景情報が、質問応答モデルによって行われるニアミスの中に存在する可能性があるということです。", "text_Russian": "Основная идея RGF заключается в том, что необходимая исходная информация, необходимая для генерации возмущений, может присутствовать в промахах, допускаемых моделью ответов на вопросы."}
{"text": "For instance, the state-of-the-art model REALM produces the following top k answers to the question who is the captain of the Richmond Football Club?\n", "text_Chinese": "例如，对于谁是里士满足球俱乐部的队长？这个问题，最先进的模型 REALM 会生成以下前 k 个答案。", "text_Arabic": "على سبيل المثال، ينتج نموذج REALM المتطور أفضل الإجابات التالية على السؤال من هو قائد نادي ريتشموند لكرة القدم؟", "text_French": "Par exemple, le modèle de pointe REALM produit les k réponses suivantes à la question qui est le capitaine du Richmond Football Club ?", "text_Japanese": "たとえば、最先端のモデル REALM は、リッチモンド フットボール クラブのキャプテンは誰ですか? という質問に対して次の上位 k 個の回答を生成します。", "text_Russian": "Например, современная модель REALM дает следующие k лучших ответов на вопрос, кто является капитаном футбольного клуба Ричмонда?"}
{"text": "While it does recover the original reference passage and answer Trent Cotchin as the top most choice.\n", "text_Chinese": "虽然它确实恢复了原始参考段落并回答 Trent Cotchin 作为最重要的选择。", "text_Arabic": "في حين أنه يستعيد المقطع المرجعي الأصلي ويجيب على Trent Cotchin باعتباره الخيار الأفضل.", "text_French": "Bien qu'il récupère le passage de référence original et réponde à Trent Cotchin comme étant le premier choix.", "text_Japanese": "ただし、元の参照箇所は復元されており、トレント・コッチンが最上位の選択肢として回答されています。", "text_Russian": "Хотя он восстанавливает исходный отрывок и называет Трента Котчина лучшим выбором."}
{"text": "It also retrieves additional passages and answers which can be used to guide question perturbation.\n", "text_Chinese": "它还检索可用于指导问题扰动的附加段落和答案。", "text_Arabic": "كما أنه يسترد مقاطع وإجابات إضافية يمكن استخدامها لتوجيه اضطراب السؤال.", "text_French": "Il récupère également des passages et des réponses supplémentaires qui peuvent être utilisés pour guider la perturbation des questions.", "text_Japanese": "また、質問の混乱を導くために使用できる追加の文章と回答も取得します。", "text_Russian": "Он также извлекает дополнительные отрывки и ответы, которые можно использовать для устранения возмущений в вопросах."}
{"text": "For instance, it recovers two more answers corresponding to the captains of the reserve team and the women's team of the same club, and this can lead to interesting edits.\n", "text_Chinese": "例如，它恢复了与同一俱乐部的预备队和女队队长相对应的另外两个答案，这可能会导致有趣的编辑。", "text_Arabic": "على سبيل المثال، فإنه يستعيد إجابتين أخريين يتوافقان مع قائدي الفريق الاحتياطي وفريق السيدات في نفس النادي، ويمكن أن يؤدي هذا إلى تعديلات مثيرة للاهتمام.", "text_French": "Par exemple, il récupère deux réponses supplémentaires correspondant aux capitaines de l'équipe réserve et de l'équipe féminine du même club, ce qui peut conduire à des modifications intéressantes.", "text_Japanese": "たとえば、同じクラブの予備チームと女子チームのキャプテンに対応するさらに 2 つの回答が回収され、興味深い編集につながる可能性があります。", "text_Russian": "Например, он восстанавливает еще два ответа, соответствующие капитанам резервной команды и женской команды того же клуба, что может привести к интересным правкам."}
{"text": "To summarize, RGF first retrieves top k most relevant answers and contexts which don't match the reference answer in context.\n", "text_Chinese": "总而言之，RGF 首先检索前 k 个最相关的答案和上下文，这些答案和上下文与上下文中的参考答案不匹配。", "text_Arabic": "للتلخيص، يقوم RGF أولاً باسترداد الإجابات والسياقات الأكثر صلة بالموضوع والتي لا تتطابق مع الإجابة المرجعية في السياق.", "text_French": "Pour résumer, RGF récupère d'abord les k réponses et contextes les plus pertinents qui ne correspondent pas à la réponse de référence dans son contexte.", "text_Japanese": "要約すると、RGF はまず、コンテキスト内の参照回答と一致しない上位 k 個の最も関連性の高い回答とコンテキストを取得します。", "text_Russian": "Подводя итог, RGF сначала извлекает k самых релевантных ответов и контекстов, которые не соответствуют эталонному ответу по контексту."}
{"text": "Following this step, the question generation model conditions on these alternate answers to generate a question that corresponds to them.\n", "text_Chinese": "在此步骤之后，问题生成模型根据这些替代答案生成与它们相对应的问题。", "text_Arabic": "بعد هذه الخطوة، يشترط نموذج توليد الأسئلة على هذه الإجابات البديلة إنشاء سؤال يتوافق معها.", "text_French": "Suite à cette étape, le modèle de génération de questions conditionne ces réponses alternatives pour générer une question qui leur correspond.", "text_Japanese": "このステップに続いて、質問生成モデルは、これらの代替回答を条件として、それらに対応する質問を生成します。", "text_Russian": "После этого шага модель генерации вопросов учитывает эти альтернативные ответы, чтобы сгенерировать соответствующий им вопрос."}
{"text": "And finally, we can filter the generated questions based on minimality or based on the type of semantic perturbation we are interested in introducing.\n", "text_Chinese": "最后，我们可以根据极简性或根据我们感兴趣引入的语义扰动类型来过滤生成的问题。", "text_Arabic": "وأخيرًا، يمكننا تصفية الأسئلة المولدة بناءً على الحد الأدنى أو بناءً على نوع الاضطراب الدلالي الذي نهتم بتقديمه.", "text_French": "Et enfin, nous pouvons filtrer les questions générées en fonction de la minimalité ou en fonction du type de perturbation sémantique que nous souhaitons introduire.", "text_Japanese": "そして最後に、最小性に基づいて、または導入したい意味論的摂動の種類に基づいて、生成された質問をフィルタリングできます。", "text_Russian": "И, наконец, мы можем фильтровать сгенерированные вопросы на основе минимальности или типа семантического возмущения, которое мы хотим ввести."}
{"text": "Going over each step in greater detail for retrieval, we use a retrieve then read model like REALM that takes as input the original question, and a large corpus like Wikipedia.\n", "text_Chinese": "为了更详细地检查检索的每个步骤，我们使用像 REALM 这样的检索然后读取模型，它以原始问题作为输入，以及像维基百科这样的大型语料库。", "text_Arabic": "من خلال مراجعة كل خطوة بمزيد من التفصيل للاسترجاع، نستخدم نموذج الاسترداد ثم القراءة مثل REALM الذي يأخذ السؤال الأصلي كمدخل، ومجموعة كبيرة مثل ويكيبيديا.", "text_French": "En examinant chaque étape plus en détail pour la récupération, nous utilisons un modèle de récupération puis de lecture comme REALM qui prend en entrée la question d'origine, et un vaste corpus comme Wikipédia.", "text_Japanese": "検索の各ステップをより詳細に検討し、元の質問を入力として受け取る REALM のような検索してから読み取るモデルと、Wikipedia のような大規模なコーパスを使用します。", "text_Russian": "Более подробно рассматривая каждый шаг для поиска, мы используем модель «получение и чтение», такую ​​как REALM, которая принимает в качестве входных данных исходный вопрос, и большой корпус, такой как Википедия."}
{"text": "It consists of two modules.\n", "text_Chinese": "它由两个模块组成。", "text_Arabic": "وهو يتألف من وحدتين.", "text_French": "Il se compose de deux modules.", "text_Japanese": "2 つのモジュールで構成されます。", "text_Russian": "Он состоит из двух модулей."}
{"text": "The retriever module performs similarity search over a dense index of passages to retrieve the top k most relevant passages to the question.\n", "text_Chinese": "检索器模块对密集的段落索引执行相似性搜索，以检索与问题最相关的前 k 个段落。", "text_Arabic": "تقوم وحدة المسترد بإجراء بحث عن التشابه عبر فهرس كثيف من المقاطع لاسترداد المقاطع الأكثر صلة بالسؤال.", "text_French": "Le module de récupération effectue une recherche de similarité sur un index dense de passages pour récupérer les k passages les plus pertinents pour la question.", "text_Japanese": "検索モジュールは、パッセージの密なインデックスに対して類似性検索を実行し、質問に最も関連する上位 k 個のパッセージを取得します。", "text_Russian": "Модуль извлечения выполняет поиск по сходству по плотному индексу отрывков, чтобы получить k верхних отрывков, наиболее подходящих для вопроса."}
{"text": "And a reader module then extracts a span from each passage as a potential answer.\n", "text_Chinese": "然后，阅读器模块从每个段落中提取一个跨度作为潜在的答案。", "text_Arabic": "وتقوم وحدة القارئ بعد ذلك باستخراج مسافة من كل مقطع كإجابة محتملة.", "text_French": "Et un module de lecture extrait ensuite une partie de chaque passage comme réponse potentielle.", "text_Japanese": "そして、リーダー モジュールが各パッセージから潜在的な回答としてスパンを抽出します。", "text_Russian": "Затем модуль чтения извлекает из каждого отрывка интервал в качестве потенциального ответа."}
{"text": "REALM retrieves the gold passage and answer in most cases.\n", "text_Chinese": "大多数情况下，REALM 会检索黄金通道并给出答案。", "text_Arabic": "يقوم RELM باسترداد المقطع الذهبي والإجابة في معظم الحالات.", "text_French": "REALM récupère le passage en or et la réponse dans la plupart des cas.", "text_Japanese": "REALM は、ほとんどの場合、ゴールド パッセージと応答を取得します。", "text_Russian": "В большинстве случаев REALM получает золотой проход и ответ."}
{"text": "However, in this work, we are more interested in the answers and context that it retrieves further down the line.\n", "text_Chinese": "然而，在这项工作中，我们更感兴趣的是它进一步检索到的答案和上下文。", "text_Arabic": "ومع ذلك، في هذا العمل، نحن مهتمون أكثر بالإجابات والسياق الذي يسترجعه في المستقبل.", "text_French": "Cependant, dans ce travail, nous nous intéressons davantage aux réponses et au contexte qu’il récupère plus tard.", "text_Japanese": "ただし、この作業では、さらに後で得られる答えとコンテキストに興味があります。", "text_Russian": "Однако в этой работе нас больше интересуют ответы и контекст, которые он получает в дальнейшем."}
{"text": "In the next step, question generation, we use these alternate answers and contexts to regenerate new questions that correspond to these alternatives.\n", "text_Chinese": "在下一步（问题生成）中，我们使用这些替代答案和上下文来重新生成与这些替代方案相对应的新问题。", "text_Arabic": "في الخطوة التالية، وهي إنشاء الأسئلة، نستخدم هذه الإجابات والسياقات البديلة لإعادة إنشاء أسئلة جديدة تتوافق مع هذه البدائل.", "text_French": "Dans l'étape suivante, la génération de questions, nous utilisons ces réponses et contextes alternatifs pour régénérer de nouvelles questions qui correspondent à ces alternatives.", "text_Japanese": "次のステップである質問の生成では、これらの代替回答とコンテキストを使用して、これらの代替に対応する新しい質問を再生成します。", "text_Russian": "На следующем этапе — генерации вопросов — мы используем эти альтернативные ответы и контексты для регенерации новых вопросов, соответствующих этим альтернативам."}
{"text": "Question generation model is a pre trained text-to-text transformer that is fine-tuned on the NQ data to generate a question for an answer that's marked in context.\n", "text_Chinese": "问题生成模型是一个预先训练的文本到文本转换器，它在 NQ 数据上进行微调，以生成在上下文中标记的答案的问题。", "text_Arabic": "نموذج إنشاء الأسئلة هو عبارة عن محول نص إلى نص تم تدريبه مسبقًا ويتم ضبطه بدقة على بيانات NQ لإنشاء سؤال لإجابة تم تحديدها في السياق.", "text_French": "Le modèle de génération de questions est un transformateur texte-texte pré-entraîné qui est affiné sur les données NQ pour générer une question pour une réponse marquée en contexte.", "text_Japanese": "質問生成モデルは、NQ データに基づいて微調整され、コンテキスト内でマークされた回答に対する質問を生成する、事前トレーニングされたテキストからテキストへの変換器です。", "text_Russian": "Модель генерации вопросов — это предварительно обученный преобразователь текста в текст, который точно настраивается на данных NQ для генерации вопроса для ответа, помеченного в контексте."}
{"text": "During inference we supply the question generation model, the alternative answer and context that we retrieved in the previous step.\n", "text_Chinese": "在推理过程中，我们提供问题生成模型、我们在上一步中检索到的替代答案和上下文。", "text_Arabic": "أثناء الاستدلال، نقوم بتوفير نموذج توليد الأسئلة والإجابة البديلة والسياق الذي استرجعناه في الخطوة السابقة.", "text_French": "Lors de l'inférence, nous fournissons le modèle de génération de questions, la réponse alternative et le contexte que nous avons récupérés à l'étape précédente.", "text_Japanese": "推論中に、質問生成モデル、前のステップで取得した代替回答およびコンテキストを提供します。", "text_Russian": "Во время вывода мы предоставляем модель генерации вопросов, альтернативный ответ и контекст, которые мы получили на предыдущем шаге."}
{"text": "For example, for the query who is the captain of the Richmond Football Club? REALM retrieves passages about the club's women's team, captained by Jess Kennedy, and the question generation model generates the query who captained Richmond Football Club's first ever women's team?\n", "text_Chinese": "例如，对于查询谁是里士满足球俱乐部的队长？ REALM 检索有关由杰西·肯尼迪 (Jess Kennedy) 担任队长的俱乐部女队的段落，问题生成模型生成查询谁担任里士满足球俱乐部有史以来第一支女队的队长？", "text_Arabic": "على سبيل المثال، بالنسبة للاستعلام من هو قائد نادي ريتشموند لكرة القدم؟ يسترد REALM مقاطع حول الفريق النسائي للنادي، بقيادة جيس كينيدي، ويقوم نموذج توليد الأسئلة بإنشاء الاستعلام من الذي قاد أول فريق نسائي على الإطلاق لنادي ريتشموند لكرة القدم؟", "text_French": "Par exemple, pour la requête qui est le capitaine du Richmond Football Club ? REALM récupère des passages sur l'équipe féminine du club, dirigée par Jess Kennedy, et le modèle de génération de questions génère la question qui a dirigé la toute première équipe féminine du Richmond Football Club ?", "text_Japanese": "たとえば、「リッチモンド フットボール クラブのキャプテンは誰ですか?」というクエリの場合です。 REALM は、ジェス ケネディがキャプテンを務めたクラブの女子チームに関する文章を取得し、質問生成モデルは「リッチモンド フットボール クラブ初の女子チームのキャプテンを務めたのは誰ですか?」というクエリを生成します。", "text_Russian": "Например, на вопрос, кто является капитаном футбольного клуба Ричмонд? REALM извлекает отрывки о женской команде клуба, капитаном которой является Джесс Кеннеди, а модель генерации вопросов генерирует вопрос, кто был капитаном первой в истории женской команды футбольного клуба Ричмонд?"}
{"text": "Which has a specific semantic perturbation.\n", "text_Chinese": "其中具有特定的语义扰动。", "text_Arabic": "والتي لها اضطراب دلالي محدد.", "text_French": "Ce qui a une perturbation sémantique spécifique.", "text_Japanese": "特定の意味上の混乱があります。", "text_Russian": "Который имеет специфическое смысловое возмущение."}
{"text": "In a similar fashion, we also get queries like who captained Richmond's VFL Reserve team?\n", "text_Chinese": "以类似的方式，我们也会收到诸如里士满 VFL 预备队队长是谁？", "text_Arabic": "بطريقة مماثلة، نحصل أيضًا على استفسارات مثل من قاد فريق Richmond's VFL Reserve؟", "text_French": "De la même manière, nous recevons également des questions telles que : qui était le capitaine de l'équipe de réserve VFL de Richmond ?", "text_Japanese": "同様に、リッチモンドの VFL リザーブ チームのキャプテンを務めたのは誰ですか? といった質問も受けます。", "text_Russian": "Аналогичным образом мы также получаем вопросы, например, кто был капитаном резервной команды ВФЛ Ричмонда?"}
{"text": "Or who did graham negate in the grand final last year?\n", "text_Chinese": "或者去年格雷厄姆在总决赛中否定了谁？", "text_Arabic": "أو من الذي أبطله جراهام في النهائي الكبير العام الماضي؟", "text_French": "Ou qui Graham a-t-il annulé lors de la grande finale l’année dernière ?", "text_Japanese": "あるいは昨年のグランドファイナルでグラハムが誰を打ち消したか?", "text_Russian": "Или кого Грэм отыграл в гранд-финале прошлого года?"}
{"text": "Finally, we filter out a subset of the generated queries based on some desired characteristics.\n", "text_Chinese": "最后，我们根据一些所需的特征过滤掉生成的查询的子集。", "text_Arabic": "وأخيرًا، نقوم بتصفية مجموعة فرعية من الاستعلامات التي تم إنشاؤها بناءً على بعض الخصائص المطلوبة.", "text_French": "Enfin, nous filtrons un sous-ensemble des requêtes générées en fonction de certaines caractéristiques souhaitées.", "text_Japanese": "最後に、必要な特性に基づいて、生成されたクエリのサブセットをフィルターで除外します。", "text_Russian": "Наконец, мы отфильтровываем подмножество сгенерированных запросов на основе некоторых желаемых характеристик."}
{"text": "As motivated earlier, we would like to ensure that the new question is still semantically close to the original.\n", "text_Chinese": "正如之前所提出的，我们希望确保新问题在语义上仍然接近原始问题。", "text_Arabic": "كما حفزنا سابقًا، نود التأكد من أن السؤال الجديد لا يزال قريبًا لغويًا من السؤال الأصلي.", "text_French": "Comme motivé précédemment, nous souhaitons nous assurer que la nouvelle question est toujours sémantiquement proche de l'originale.", "text_Japanese": "先ほどの動機と同様に、新しい質問が意味的に元の質問に近いことを確認したいと考えています。", "text_Russian": "Как мотивировалось ранее, мы хотели бы гарантировать, что новый вопрос по-прежнему семантически близок к оригиналу."}
{"text": "For filtering techniques that doesn't require additional supervision, we simply retain new questions that have a small token label edit distance from the original question.\n", "text_Chinese": "对于不需要额外监督的过滤技术，我们只需保留与原始问题具有较小标记标签编辑距离的新问题。", "text_Arabic": "بالنسبة لتقنيات التصفية التي لا تتطلب إشرافًا إضافيًا، فإننا ببساطة نحتفظ بالأسئلة الجديدة التي لها مسافة تحرير صغيرة للعلامة المميزة من السؤال الأصلي.", "text_French": "Pour les techniques de filtrage qui ne nécessitent pas de supervision supplémentaire, nous conservons simplement les nouvelles questions qui ont une petite distance de modification d'étiquette symbolique par rapport à la question d'origine.", "text_Japanese": "追加の監視を必要としないフィルタリング手法の場合、元の質問からトークン ラベルの編集距離が小さい新しい質問を単に保持します。", "text_Russian": "Для методов фильтрации, не требующих дополнительного контроля, мы просто сохраняем новые вопросы, которые имеют небольшое расстояние редактирования метки токена от исходного вопроса."}
{"text": "For example, we remove the question who did graham negate in the grand final last year?\n", "text_Chinese": "例如，我们删除了去年总决赛中格雷厄姆否定了谁的问题？", "text_Arabic": "على سبيل المثال، نزيل السؤال من الذي نفاه جراهام في النهائي الكبير العام الماضي؟", "text_French": "Par exemple, nous supprimons la question : qui Graham a-t-il nié lors de la grande finale l'année dernière ?", "text_Japanese": "たとえば、昨年のグランドファイナルでグラハムが誰を否定したかという質問を削除します。", "text_Russian": "Например, мы снимаем вопрос, кого Грэм отрицал в гранд-финале прошлого года?"}
{"text": "Because it has a longer edit distance from the original question.\n", "text_Chinese": "因为它与原问题的编辑距离较长。", "text_Arabic": "لأنه يحتوي على مسافة تحرير أطول من السؤال الأصلي.", "text_French": "Parce qu'il y a une distance d'édition plus longue par rapport à la question d'origine.", "text_Japanese": "元の質問からの編集距離が長いためです。", "text_Russian": "Потому что расстояние редактирования от исходного вопроса больше."}
{"text": "In our experiments, we demonstrate that this simple heuristic can be used to augment and queue training data.\n", "text_Chinese": "在我们的实验中，我们证明了这种简单的启发式方法可用于增强和排队训练数据。", "text_Arabic": "في تجاربنا، أثبتنا أن هذا الاستدلال البسيط يمكن استخدامه لزيادة بيانات التدريب ووضعها في قائمة الانتظار.", "text_French": "Dans nos expériences, nous démontrons que cette heuristique simple peut être utilisée pour augmenter et mettre en file d'attente les données d'entraînement.", "text_Japanese": "私たちの実験では、この単純なヒューリスティックを使用してトレーニング データを拡張し、キューに入れることができることを実証しました。", "text_Russian": "В наших экспериментах мы показываем, что эту простую эвристику можно использовать для увеличения и постановки в очередь обучающих данных."}
{"text": "We also experiment with a filtering strategy that is based on the type of semantic perturbation.\n", "text_Chinese": "我们还尝试了一种基于语义扰动类型的过滤策略。", "text_Arabic": "نقوم أيضًا بتجربة استراتيجية التصفية التي تعتمد على نوع الاضطراب الدلالي.", "text_French": "Nous expérimentons également une stratégie de filtrage basée sur le type de perturbation sémantique.", "text_Japanese": "また、意味論的摂動のタイプに基づいたフィルタリング戦略も実験します。", "text_Russian": "Мы также экспериментируем со стратегией фильтрации, основанной на типе семантического возмущения."}
{"text": "To this end, we use a general purpose query decomposition framework called QED.\n", "text_Chinese": "为此，我们使用称为 QED 的通用查询分解框架。", "text_Arabic": "ولتحقيق هذه الغاية، نستخدم إطار تحليل الاستعلام للأغراض العامة يسمى QED.", "text_French": "À cette fin, nous utilisons un framework de décomposition de requêtes à usage général appelé QED.", "text_Japanese": "この目的のために、QED と呼ばれる汎用クエリ分解フレームワークを使用します。", "text_Russian": "С этой целью мы используем инфраструктуру декомпозиции запросов общего назначения, называемую QED."}
{"text": "QED identifies two parts to the question, a predicate and a reference.\n", "text_Chinese": "QED 确定了问题的两个部分：谓语和指称。", "text_Arabic": "يحدد QED جزأين للسؤال، المسند والمرجع.", "text_French": "QED identifie deux parties de la question, un prédicat et une référence.", "text_Japanese": "QED は、質問の 2 つの部分、述語と参照を識別します。", "text_Russian": "QED выделяет две части вопроса: предикат и ссылку."}
{"text": "References are noun phrases in the question that correspond to entities in the context.\n", "text_Chinese": "参考文献是问题中与上下文中的实体相对应的名词短语。", "text_Arabic": "المراجع هي عبارات اسمية في السؤال تتوافق مع الكيانات في السياق.", "text_French": "Les références sont des expressions nominales dans la question qui correspondent à des entités dans le contexte.", "text_Japanese": "参照は、文脈内のエンティティに対応する質問内の名詞句です。", "text_Russian": "Ссылки — это именные фразы в вопросе, которые соответствуют объектам в контексте."}
{"text": "A predicate is basically the remaining portion of the question.\n", "text_Chinese": "谓词基本上是问题的剩余部分。", "text_Arabic": "المسند هو في الأساس الجزء المتبقي من السؤال.", "text_French": "Un prédicat est essentiellement la partie restante de la question.", "text_Japanese": "述語は基本的に質問の残りの部分です。", "text_Russian": "Предикат — это, по сути, оставшаяся часть вопроса."}
{"text": "For example, we are able to decompose the query who captained Richmond's first ever women's team into two references: Richmond Football Club women's team and the predicate who captained X.\n", "text_Chinese": "例如，我们可以将查询 who Captain Richmond's First Women's team 分解为两个引用：Richmond Football Club Women's team 和谓词 who Captained X。", "text_Arabic": "على سبيل المثال، نحن قادرون على تحليل الاستعلام الذي قاد فريق ريتشموند الأول للسيدات على الإطلاق إلى مرجعين: فريق ريتشموند لكرة القدم للسيدات والمسند الذي قاد كابتن X.", "text_French": "Par exemple, nous sommes en mesure de décomposer la requête qui a dirigé la toute première équipe féminine de Richmond en deux références : l'équipe féminine du Richmond Football Club et le prédicat qui a dirigé X.", "text_Japanese": "たとえば、リッチモンド初の女子チームのキャプテンを誰が務めたかというクエリを、リッチモンド フットボール クラブ女子チームと述語誰が X をキャプテンしたかという 2 つの参照に分解できます。", "text_Russian": "Например, мы можем разложить запрос о том, кто был капитаном первой женской команды Ричмонда, на две ссылки: женская команда футбольного клуба Ричмонда и предикат, который был капитаном X."}
{"text": "A model trained on reference predicate annotations for NQ gives us this question decomposition.\n", "text_Chinese": "在 NQ 的参考谓词注释上训练的模型为我们提供了这个问题分解。", "text_Arabic": "نموذج تم تدريبه على التعليقات التوضيحية المرجعية لـ NQ يمنحنا تحليل الأسئلة هذا.", "text_French": "Un modèle entraîné sur les annotations de prédicats de référence pour NQ nous donne cette décomposition de questions.", "text_Japanese": "NQ の参照述語アノテーションでトレーニングされたモデルにより、この質問の分解が得られます。", "text_Russian": "Модель, обученная на аннотациях ссылочных предикатов для NQ, дает нам декомпозицию этого вопроса."}
{"text": "Decomposing both the original and generated question based on QED allows us to categorize our generated counterfactuals for evaluation.\n", "text_Chinese": "基于 QED 分解原始问题和生成问题使我们能够对生成的反事实进行分类以进行评估。", "text_Arabic": "إن تحليل كل من السؤال الأصلي والسؤال الذي تم إنشاؤه بناءً على QED يسمح لنا بتصنيف الحقائق المضادة التي تم إنشاؤها للتقييم.", "text_French": "La décomposition de la question originale et de la question générée sur la base de QED nous permet de catégoriser nos contrefactuels générés pour évaluation.", "text_Japanese": "QED に基づいて元の質問と生成された質問の両方を分解することで、生成された反事実を評価のために分類することができます。", "text_Russian": "Разложение исходного и сгенерированного вопроса на основе QED позволяет нам классифицировать сгенерированные контрфакты для оценки."}
{"text": "Specifically, we obtain two groups of questions.\n", "text_Chinese": "具体来说，我们得到两组问题。", "text_Arabic": "على وجه التحديد، نحصل على مجموعتين من الأسئلة.", "text_French": "Plus précisément, nous obtenons deux groupes de questions.", "text_Japanese": "具体的には、2 つのグループの質問が得られます。", "text_Russian": "В частности, мы получаем две группы вопросов."}
{"text": "Those that undergo a reference change while retaining predicates, and those that undergo a predicate change and optionally add references.\n", "text_Chinese": "那些经历引用更改同时保留谓词的人，以及那些经历谓词更改并可选择添加引用的人。", "text_Arabic": "تلك التي تخضع لتغيير مرجع مع الاحتفاظ بالمسندات، وتلك التي تخضع لتغيير المسند وإضافة مراجع بشكل اختياري.", "text_French": "Ceux qui subissent un changement de référence tout en conservant les prédicats, et ceux qui subissent un changement de prédicat et ajoutent éventuellement des références.", "text_Japanese": "述語を保持したまま参照が変更されるものと、述語が変更され、オプションで参照が追加されるものです。", "text_Russian": "Те, которые претерпевают изменение ссылки с сохранением предикатов, и те, которые претерпевают изменение предиката и при необходимости добавляют ссылки."}
{"text": "For instance, who captained Richmond's VFL reserve team is a reference change?\n", "text_Chinese": "例如，谁担任里士满 VFL 预备队队长是一个参考变更？", "text_Arabic": "على سبيل المثال، من الذي قاد فريق ريتشموند الاحتياطي في VFL هو تغيير مرجعي؟", "text_French": "Par exemple, qui a dirigé l’équipe réserve VFL de Richmond est un changement de référence ?", "text_Japanese": "たとえば、リッチモンドの VFL リザーブチームのキャプテンを務めたのは誰ですか?", "text_Russian": "Например, показательным изменением является вопрос о том, кто был капитаном резервной команды Ричмонда ВФЛ?"}
{"text": "While, who wears number nine for the club is a predicate change.\n", "text_Chinese": "然而，谁为俱乐部穿9号球衣是一个谓词变化。", "text_Arabic": "بينما من يرتدي الرقم تسعة للنادي هو تغيير المسند.", "text_French": "Tandis que savoir qui porte le numéro neuf pour le club est un changement de prédicat.", "text_Japanese": "一方、クラブの背番号9を誰がつけるかは、前提条件の変化である。", "text_Russian": "А вот то, кто будет носить девятый номер за клуб, – это предикатное изменение."}
{"text": "We now evaluate the effectiveness of RGF perturbations when augmented to training data.\n", "text_Chinese": "我们现在评估 RGF 扰动增强到训练数据时的有效性。", "text_Arabic": "نقوم الآن بتقييم فعالية اضطرابات RGF عند زيادتها إلى بيانات التدريب.", "text_French": "Nous évaluons maintenant l'efficacité des perturbations RGF lorsqu'elles sont ajoutées aux données d'entraînement.", "text_Japanese": "次に、トレーニング データに拡張した場合の RGF 摂動の有効性を評価します。", "text_Russian": "Теперь мы оцениваем эффективность возмущений RGF при дополнении к обучающим данным."}
{"text": "So, to effectively evaluate the effectiveness of counterfactual augmentation in particular, we experiment with two strong data augmentation baselines.\n", "text_Chinese": "因此，为了有效评估反事实增强的有效性，我们使用两个强大的数据增强基线进行实验。", "text_Arabic": "لذلك، لتقييم فعالية التعزيز المضاد للواقع على وجه الخصوص، قمنا بتجربة خطين أساسيين قويين لزيادة البيانات.", "text_French": "Ainsi, pour évaluer efficacement l’efficacité de l’augmentation contrefactuelle en particulier, nous expérimentons deux bases de référence solides pour l’augmentation des données.", "text_Japanese": "したがって、特に反事実増強の有効性を効果的に評価するために、2 つの強力なデータ増強ベースラインを実験します。", "text_Russian": "Итак, чтобы эффективно оценить эффективность контрфактического увеличения, в частности, мы экспериментируем с двумя сильными базовыми показателями увеличения данных."}
{"text": "The first baseline, called random answer and question generation, adds data that has no relation with the original question.\n", "text_Chinese": "第一个基线称为随机答案和问题生成，添加与原始问题无关的数据。", "text_Arabic": "يضيف خط الأساس الأول، الذي يسمى الإجابة العشوائية وتوليد الأسئلة، بيانات ليس لها علاقة بالسؤال الأصلي.", "text_French": "La première ligne de base, appelée génération de réponses et de questions aléatoires, ajoute des données qui n'ont aucun rapport avec la question d'origine.", "text_Japanese": "最初のベースラインはランダムな回答と質問の生成と呼ばれ、元の質問と無関係なデータを追加します。", "text_Russian": "Первая базовая линия, называемая генерацией случайных ответов и вопросов, добавляет данные, не имеющие никакого отношения к исходному вопросу."}
{"text": "That is, passages and answers are simply randomly sampled from Wikipedia.\n", "text_Chinese": "也就是说，段落和答案只是从维基百科中随机抽取的。", "text_Arabic": "وهذا يعني أن المقاطع والإجابات يتم أخذ عينات منها بشكل عشوائي من ويكيبيديا.", "text_French": "Autrement dit, les passages et les réponses sont simplement échantillonnés au hasard sur Wikipédia.", "text_Japanese": "つまり、文章と答えはウィキペディアからランダムに抽出されたものです。", "text_Russian": "То есть отрывки и ответы просто случайным образом выбираются из Википедии."}
{"text": "This baseline basically adds more data that looks like NQ.\n", "text_Chinese": "这个基线基本上添加了更多看起来像 NQ 的数据。", "text_Arabic": "يضيف خط الأساس هذا بشكل أساسي المزيد من البيانات التي تشبه NQ.", "text_French": "Cette référence ajoute essentiellement plus de données qui ressemblent à NQ.", "text_Japanese": "このベースラインは基本的に、NQ に似たデータを追加します。", "text_Russian": "Этот базовый уровень по сути добавляет больше данных, похожих на NQ."}
{"text": "With the second baseline gold answer and question generation, we specifically update the retrieval portion of our method.\n", "text_Chinese": "通过第二个基线黄金答案和问题生成，我们专门更新了我们方法的检索部分。", "text_Arabic": "مع الإجابة الذهبية الأساسية الثانية وتوليد الأسئلة، نقوم على وجه التحديد بتحديث جزء الاسترجاع من طريقتنا.", "text_French": "Avec la deuxième génération de réponses et de questions de base, nous mettons spécifiquement à jour la partie récupération de notre méthode.", "text_Japanese": "2 番目のベースラインのゴールドアンサーと質問の生成では、特にメソッドの検索部分を更新します。", "text_Russian": "При генерации второго базового золотого ответа и вопроса мы специально обновляем поисковую часть нашего метода."}
{"text": "Here, alternate answers are just chosen from the same passage that contained the gold answer.\n", "text_Chinese": "在这里，备用答案只是从包含黄金答案的同一段落中选择的。", "text_Arabic": "هنا، يتم اختيار الإجابات البديلة من نفس المقطع الذي يحتوي على الإجابة الذهبية.", "text_French": "Ici, les réponses alternatives sont simplement choisies dans le même passage qui contenait la réponse en or.", "text_Japanese": "ここでは、代替回答は、ゴールドアンサーが含まれていたのと同じ文章から選択されるだけです。", "text_Russian": "Здесь альтернативные ответы просто выбраны из того же отрывка, в котором содержится золотой ответ."}
{"text": "How base how do the baselines and RGF ah augmentation perform on reading comprehension where the model has access to question and context?\n", "text_Chinese": "在模型可以访问问题和上下文的情况下，基线和 RGF ah 增强在阅读理解方面的表现如何？", "text_Arabic": "ما مدى تأثير خطوط الأساس وزيادة RGF آه على فهم القراءة حيث يكون للنموذج إمكانية الوصول إلى السؤال والسياق؟", "text_French": "Comment se basent sur les performances des lignes de base et de l'augmentation RGF ah sur la compréhension en lecture où le modèle a accès à la question et au contexte ?", "text_Japanese": "モデルが質問やコンテキストにアクセスできる場合、ベースラインと RGF ah 拡張は読解力にどのように基づいて実行されますか?", "text_Russian": "Как базовые линии и дополнение RGF влияют на понимание прочитанного, когда модель имеет доступ к вопросам и контексту?"}
{"text": "We experiment with six out of domain datasets and present results here, where data is the training data is doubled in augmentation.\n", "text_Chinese": "我们使用六个域外数据集进行实验，并在此展示结果，其中数据是训练数据在增强中加倍。", "text_Arabic": "لقد قمنا بتجربة ست مجموعات بيانات خارج النطاق ونقدم النتائج هنا، حيث تكون البيانات هي بيانات التدريب التي يتم مضاعفة زيادتها.", "text_French": "Nous expérimentons six ensembles de données hors domaine et présentons ici les résultats, où les données sont les données de formation doublées en augmentation.", "text_Japanese": "6 つのドメイン外データセットで実験し、その結果をここに示します。ここで、データはトレーニング データであり、拡張では 2 倍になります。", "text_Russian": "Мы экспериментируем с шестью наборами данных из предметной области и представляем здесь результаты, где данные представляют собой обучающие данные, увеличенные вдвое."}
{"text": "We find that both data augmentation baselines are not able to improve our domain generalization.\n", "text_Chinese": "我们发现这两个数据增强基线都无法提高我们的领域泛化能力。", "text_Arabic": "نجد أن كلا خطي الأساس لزيادة البيانات غير قادرين على تحسين تعميم مجالنا.", "text_French": "Nous constatons que les deux bases d’augmentation des données ne sont pas en mesure d’améliorer la généralisation de notre domaine.", "text_Japanese": "どちらのデータ拡張ベースラインもドメインの一般化を改善できないことがわかりました。", "text_Russian": "Мы обнаружили, что оба базовых варианта увеличения данных не способны улучшить обобщение нашей предметной области."}
{"text": "In fact, an ensemble of six models trained on the original data seems to be the most competitive baseline.\n", "text_Chinese": "事实上，在原始数据上训练的六个模型的集合似乎是最具竞争力的基线。", "text_Arabic": "في الواقع، يبدو أن مجموعة من ستة نماذج تم تدريبها على البيانات الأصلية هي خط الأساس الأكثر تنافسية.", "text_French": "En fait, un ensemble de six modèles formés sur les données originales semble constituer la base de référence la plus compétitive.", "text_Japanese": "実際、元のデータでトレーニングされた 6 つのモデルのアンサンブルが、最も競争力のあるベースラインであると思われます。", "text_Russian": "Фактически, ансамбль из шести моделей, обученных на исходных данных, кажется наиболее конкурентоспособной базой."}
{"text": "Comparing against that baseline, we find that RGF counterfactuals are able to improve out of domain performance while maintaining in domain performance.\n", "text_Chinese": "与该基线相比，我们发现 RGF 反事实能够提高域外性能，同时保持域内性能。", "text_Arabic": "بالمقارنة مع خط الأساس هذا، نجد أن الحقائق المضادة لـ RGF قادرة على تحسين الأداء خارج المجال مع الحفاظ على أداء المجال.", "text_French": "En comparant avec cette base de référence, nous constatons que les contrefactuels RGF sont capables d'améliorer les performances hors domaine tout en maintenant les performances dans le domaine.", "text_Japanese": "そのベースラインと比較すると、RGF 反事実はドメイン内のパフォーマンスを維持しながらドメイン外のパフォーマンスを向上させることができることがわかります。", "text_Russian": "Сравнивая этот базовый уровень, мы обнаруживаем, что альтернативные варианты RGF способны улучшить производительность за пределами предметной области, сохраняя при этом производительность в предметной области."}
{"text": "This suggests that filling in the reasoning gaps of the model via counterfactual augmentation is more effective than adding more data from the training distribution.\n", "text_Chinese": "这表明通过反事实增强来填补模型的推理空白比从训练分布中添加更多数据更有效。", "text_Arabic": "يشير هذا إلى أن سد فجوات الاستدلال في النموذج من خلال التعزيز المضاد للواقع هو أكثر فعالية من إضافة المزيد من البيانات من توزيع التدريب.", "text_French": "Cela suggère que combler les lacunes de raisonnement du modèle via une augmentation contrefactuelle est plus efficace que d'ajouter davantage de données provenant de la distribution de formation.", "text_Japanese": "これは、トレーニング分布からさらにデータを追加するよりも、反事実の拡張によってモデルの推論ギャップを埋める方が効果的であることを示唆しています。", "text_Russian": "Это говорит о том, что заполнение пробелов в рассуждениях модели посредством контрфактического дополнения более эффективно, чем добавление дополнительных данных из обучающего распределения."}
{"text": "Furthermore, we find that using retrieval to sample alternative outcomes or answers is important for effective CDA.\n", "text_Chinese": "此外，我们发现使用检索来采样替代结果或答案对于有效的 CDA 很重要。", "text_Arabic": "علاوة على ذلك، نجد أن استخدام الاسترجاع لأخذ عينات من النتائج أو الإجابات البديلة أمر مهم لـ CDA الفعال.", "text_French": "En outre, nous constatons que le recours à la récupération pour échantillonner des résultats ou des réponses alternatives est important pour une ADC efficace.", "text_Japanese": "さらに、効果的な CDA には検索を使用して代替の結果や回答をサンプリングすることが重要であることがわかりました。", "text_Russian": "Кроме того, мы обнаружили, что использование поиска для выборки альтернативных результатов или ответов важно для эффективного CDA."}
{"text": "We also experiment with open domain QA setting where the model only sees the question and once again we evaluate on four out of domain datasets.\n", "text_Chinese": "我们还尝试了开放域 QA 设置，其中模型仅看到问题，并且我们再次对四个域外数据集进行评估。", "text_Arabic": "نقوم أيضًا بتجربة إعداد ضمان الجودة في المجال المفتوح حيث يرى النموذج السؤال فقط ونقوم مرة أخرى بالتقييم على أربع مجموعات بيانات خارج المجال.", "text_French": "Nous expérimentons également un paramètre d'assurance qualité en domaine ouvert où le modèle ne voit que la question et, une fois de plus, nous évaluons sur quatre ensembles de données hors domaine.", "text_Japanese": "また、モデルが質問のみを参照するオープン ドメイン QA 設定も実験し、再度ドメイン外の 4 つのデータセットで評価します。", "text_Russian": "Мы также экспериментируем с настройкой контроля качества открытого домена, когда модель видит только вопрос, и еще раз мы оцениваем четыре набора данных из домена."}
{"text": "We find that baseline models are not as effective for out of domain generalization.\n", "text_Chinese": "我们发现基线模型对于域外泛化并不那么有效。", "text_Arabic": "لقد وجدنا أن النماذج الأساسية ليست فعالة للتعميم خارج المجال.", "text_French": "Nous constatons que les modèles de base ne sont pas aussi efficaces pour la généralisation hors domaine.", "text_Japanese": "ベースライン モデルは領域外の一般化にはそれほど効果的ではないことがわかりました。", "text_Russian": "Мы обнаружили, что базовые модели не так эффективны для обобщения за пределами предметной области."}
{"text": "However, data augmentation with RGF shows more significant improvements.\n", "text_Chinese": "然而，使用 RGF 进行数据增强显示出更显着的改进。", "text_Arabic": "ومع ذلك، فإن زيادة البيانات باستخدام RGF تظهر تحسينات أكثر أهمية.", "text_French": "Cependant, l’augmentation des données avec RGF montre des améliorations plus significatives.", "text_Japanese": "ただし、RGF を使用したデータ拡張では、さらに大幅な改善が見られます。", "text_Russian": "Однако увеличение данных с помощью RGF показывает более значительные улучшения."}
{"text": "We even improve in the in domain NQ dataset.\n", "text_Chinese": "我们甚至改进了域内 NQ 数据集。", "text_Arabic": "لقد قمنا أيضًا بتحسين مجموعة بيانات NQ في المجال.", "text_French": "Nous améliorons même l'ensemble de données NQ du domaine.", "text_Japanese": "ドメイン内 NQ データセットでも改善されています。", "text_Russian": "Мы даже совершенствуемся в наборе данных NQ."}
{"text": "We hypothesized that the counterfactual data augmentation aids the model in learning better query encodings for very similar queries.\n", "text_Chinese": "我们假设反事实数据增强有助于模型为非常相似的查询学习更好的查询编码。", "text_Arabic": "لقد افترضنا أن زيادة البيانات المغايرة للواقع تساعد النموذج في تعلم ترميزات استعلام أفضل للاستعلامات المشابهة جدًا.", "text_French": "Nous avons émis l'hypothèse que l'augmentation des données contrefactuelles aide le modèle à apprendre de meilleurs codages de requêtes pour des requêtes très similaires.", "text_Japanese": "私たちは、反事実データの拡張により、モデルが非常に類似したクエリに対してより適切なクエリ エンコーディングを学習するのに役立つと仮説を立てました。", "text_Russian": "Мы предположили, что контрфактическое увеличение данных помогает модели лучше изучить кодировки запросов для очень похожих запросов."}
{"text": "Finally, we also evaluate on the model's ability to improve consistency in the local neighborhood of the original question.\n", "text_Chinese": "最后，我们还评估了模型提高原始问题局部邻域一致性的能力。", "text_Arabic": "أخيرًا، نقوم أيضًا بتقييم قدرة النموذج على تحسين الاتساق في الحي المحلي للسؤال الأصلي.", "text_French": "Enfin, nous évaluons également la capacité du modèle à améliorer la cohérence dans le voisinage local de la question initiale.", "text_Japanese": "最後に、元の質問の局所的な近傍における一貫性を向上させるモデルの能力についても評価します。", "text_Russian": "Наконец, мы также оцениваем способность модели улучшать согласованность в локальной окрестности исходного вопроса."}
{"text": "Consistency measures the proportion of questions correctly answered by the model where both the original and the counterfactual query are correctly answered.\n", "text_Chinese": "一致性衡量模型正确回答的问题的比例，其中原始查询和反事实查询均得到正确回答。", "text_Arabic": "يقيس الاتساق نسبة الأسئلة التي تم الرد عليها بشكل صحيح بواسطة النموذج حيث تمت الإجابة بشكل صحيح على كل من الاستعلام الأصلي والاستعلام المخالف.", "text_French": "La cohérence mesure la proportion de questions répondues correctement par le modèle où la requête originale et la requête contrefactuelle reçoivent une réponse correcte.", "text_Japanese": "一貫性は、元のクエリと反事実のクエリの両方が正しく回答される、モデルによって正しく回答される質問の割合を測定します。", "text_Russian": "Согласованность измеряет долю вопросов, на которые модель правильно ответила, при этом были даны правильные ответы как на исходный, так и на контрфактический запрос."}
{"text": "This explicitly helps us to measure the model's robustness to small perturbations in the neighborhood of the original input.\n", "text_Chinese": "这明确地帮助我们测量模型对原始输入附近的小扰动的鲁棒性。", "text_Arabic": "وهذا يساعدنا بشكل واضح على قياس متانة النموذج في مواجهة الاضطرابات الصغيرة في جوار المدخلات الأصلية.", "text_French": "Cela nous aide explicitement à mesurer la robustesse du modèle aux petites perturbations au voisinage de l'entrée d'origine.", "text_Japanese": "これは、元の入力付近の小さな摂動に対するモデルのロバスト性を測定するのに明らかに役立ちます。", "text_Russian": "Это явно помогает нам измерить устойчивость модели к небольшим возмущениям вблизи исходных входных данных."}
{"text": "We experiment with five datasets which contain pairs of questions that are semantically close to each other.\n", "text_Chinese": "我们使用五个数据集进行实验，其中包含语义上彼此接近的问题对。", "text_Arabic": "قمنا بتجربة خمس مجموعات بيانات تحتوي على أزواج من الأسئلة القريبة من بعضها البعض لغويًا.", "text_French": "Nous expérimentons cinq ensembles de données contenant des paires de questions sémantiquement proches les unes des autres.", "text_Japanese": "意味的に互いに近い質問のペアを含む 5 つのデータセットを実験します。", "text_Russian": "Мы экспериментируем с пятью наборами данных, которые содержат пары вопросов, семантически близких друг другу."}
{"text": "Apart from the three datasets AQA, AmbigQA and QUOREF-Contrast set that are already available, we also evaluate on RGF counterfactuals that are paired with original NQ questions based on whether they underwent a predicate change or reference change.\n", "text_Chinese": "除了已经可用的三个数据集 AQA、AmbigQA 和 QUOREF-Contrast 之外，我们还根据原始 NQ 问题是否经历了谓词更改或参考更改来评估与原始 NQ 问题配对的 RGF 反事实。", "text_Arabic": "بصرف النظر عن مجموعات البيانات الثلاث AQA وAmbigQA وQUOREF-Contrast المتوفرة بالفعل، فإننا نقوم أيضًا بتقييم الحقائق المضادة لـ RGF المقترنة بأسئلة NQ الأصلية بناءً على ما إذا كانت قد خضعت لتغيير أصلي أو تغيير مرجعي.", "text_French": "Outre les trois ensembles de données AQA, AmbigQA et QUOREF-Contrast déjà disponibles, nous évaluons également les contrefactuels RGF associés aux questions NQ originales selon qu'ils ont subi un changement de prédicat ou un changement de référence.", "text_Japanese": "すでに利用可能な 3 つのデータセット AQA、AmbigQA、および QUOREF-Contrast セットとは別に、述語変更または参照変更が行われたかどうかに基づいて、元の NQ 質問とペアになった RGF 反事実にも基づいて評価します。", "text_Russian": "Помимо трех наборов данных AQA, AmbigQA и набора QUOREF-Contrast, которые уже доступны, мы также оцениваем контрфакты RGF, которые сочетаются с исходными вопросами NQ, на основе того, претерпели ли они изменение предиката или изменение ссылки."}
{"text": "These subsets were annotated in-house to eliminate noise and are provided as a resource.\n", "text_Chinese": "这些子集在内部进行了注释，以消除噪音，并作为资源提供。", "text_Arabic": "تم شرح هذه المجموعات الفرعية داخليًا للتخلص من الضوضاء وتوفيرها كمورد.", "text_French": "Ces sous-ensembles ont été annotés en interne pour éliminer le bruit et sont fournis à titre de ressource.", "text_Japanese": "これらのサブセットはノイズを除去するために社内で注釈が付けられ、リソースとして提供されています。", "text_Russian": "Эти подмножества были аннотированы собственными силами для устранения шума и предоставлены в качестве ресурса."}
{"text": "All baselines are unable to significantly improve consistency with the ensemble model improving consistency by a small margin.\n", "text_Chinese": "所有基线都无法显着提高与集成模型的一致性，从而小幅提高一致性。", "text_Arabic": "جميع خطوط الأساس غير قادرة على تحسين الاتساق بشكل كبير مع نموذج المجموعة الذي يعمل على تحسين الاتساق بهامش صغير.", "text_French": "Toutes les lignes de base ne sont pas en mesure d'améliorer de manière significative la cohérence avec le modèle d'ensemble, améliorant légèrement la cohérence.", "text_Japanese": "すべてのベースラインでは一貫性を大幅に向上させることはできませんが、アンサンブル モデルの一貫性はわずかに向上します。", "text_Russian": "Все базовые уровни не могут значительно улучшить согласованность, при этом ансамблевая модель улучшает согласованность с небольшим отрывом."}
{"text": "However, RGF counterfactual augmentation has impressive gains in consistency both on prior datasets and the two subsets we curated for reference and predicate perturbations.\n", "text_Chinese": "然而，RGF 反事实增强在先前数据集以及我们为参考和谓词扰动策划的两个子集上的一致性方面都取得了令人印象深刻的进步。", "text_Arabic": "ومع ذلك، فإن زيادة RGF المغايرة للواقع لها مكاسب مثيرة للإعجاب في الاتساق سواء في مجموعات البيانات السابقة أو المجموعتين الفرعيتين اللتين قمنا برعايةهما كمرجع والاضطرابات المسندة.", "text_French": "Cependant, l’augmentation contrefactuelle RGF présente des gains impressionnants en termes de cohérence à la fois sur les ensembles de données antérieurs et sur les deux sous-ensembles que nous avons sélectionnés pour les perturbations de référence et de prédicat.", "text_Japanese": "ただし、RGF 反事実拡張では、以前のデータセットと、参照摂動および述語摂動のために厳選した 2 つのサブセットの両方で一貫性が大幅に向上しました。", "text_Russian": "Тем не менее, контрфактическое дополнение RGF дает впечатляющий выигрыш в согласованности как на предыдущих наборах данных, так и на двух подмножествах, которые мы курировали для эталонных и предикатных возмущений."}
{"text": "Note that the augmented RGF data is not biased by perturbation type, only the evaluation sets are.\n", "text_Chinese": "请注意，增强 RGF 数据不会因扰动类型而产生偏差，只有评估集会产生偏差。", "text_Arabic": "لاحظ أن بيانات RGF المعززة ليست متحيزة حسب نوع الاضطراب، بل مجموعات التقييم فقط هي التي تكون متحيزة.", "text_French": "Notez que les données RGF augmentées ne sont pas biaisées par le type de perturbation, seuls les ensembles d'évaluation le sont.", "text_Japanese": "拡張 RGF データには摂動タイプによるバイアスがなく、評価セットのみにバイアスがかかることに注意してください。", "text_Russian": "Обратите внимание, что расширенные данные RGF не подвержены смещению в зависимости от типа возмущения, а только наборы оценок."}
{"text": "In fact, a qualitative inspection of the kinds of counterfactuals generated show that the generated questions contain several diverse perturbations.\n", "text_Chinese": "事实上，对生成的反事实类型的定性检查表明，生成的问题包含几种不同的扰动。", "text_Arabic": "في الواقع، يُظهر الفحص النوعي لأنواع الحقائق المضادة التي تم إنشاؤها أن الأسئلة المولدة تحتوي على العديد من الاضطرابات المتنوعة.", "text_French": "En fait, une inspection qualitative des types de contrefactuels générés montre que les questions générées contiennent plusieurs perturbations diverses.", "text_Japanese": "実際、生成された反事実の種類を定性的に検査すると、生成された質問にはいくつかの多様な摂動が含まれていることがわかります。", "text_Russian": "Фактически, качественный анализ видов сгенерированных контрфактов показывает, что сгенерированные вопросы содержат несколько различных искажений."}
{"text": "For instance, this original question on the population of Walnut Grove, Minnesota is perturbed along different dimensions like town, state, country, and along different predicates like location, poverty, number of schools.\n", "text_Chinese": "例如，关于明尼苏达州核桃格罗夫人口的原始问题在城镇、州、国家等不同维度以及位置、贫困、学校数量等不同谓词上受到干扰。", "text_Arabic": "على سبيل المثال، هذا السؤال الأصلي حول سكان وولنت جروف، مينيسوتا، مضطرب على طول أبعاد مختلفة مثل المدينة والولاية والبلد، وعلى أساس مسندات مختلفة مثل الموقع والفقر وعدد المدارس.", "text_French": "Par exemple, cette question originale sur la population de Walnut Grove, dans le Minnesota, est perturbée selon différentes dimensions comme la ville, l'État, le pays, et selon différents prédicats comme l'emplacement, la pauvreté, le nombre d'écoles.", "text_Japanese": "たとえば、ミネソタ州ウォルナットグローブの人口に関するこの最初の質問は、町、州、国などのさまざまな次元に沿って、また場所、貧困、学校の数などのさまざまな述語に沿って混乱しています。", "text_Russian": "Например, этот первоначальный вопрос о населении Уолнат-Гроув, штат Миннесота, нарушен по разным параметрам, таким как город, штат, страна, а также по различным предикатам, таким как местоположение, бедность, количество школ."}
{"text": "Audio of perturbations are context specific.\n", "text_Chinese": "扰动的音频是特定于上下文的。", "text_Arabic": "صوت الاضطرابات محدد بالسياق.", "text_French": "L'audio des perturbations est spécifique au contexte.", "text_Japanese": "摂動の音声はコンテキスト固有です。", "text_Russian": "Звук возмущений зависит от контекста."}
{"text": "For example, for this other question about the Wimbledon ah singles tournament, the perturbation is along type of game, type of tournament, or the game outcome.\n", "text_Chinese": "例如，对于有关温布尔登单打锦标赛的另一个问题，扰动是沿着游戏类型、锦标赛类型或游戏结果。", "text_Arabic": "على سبيل المثال، بالنسبة لهذا السؤال الآخر حول بطولة ويمبلدون الفردية، يكون الاضطراب على طول نوع اللعبة أو نوع البطولة أو نتيجة المباراة.", "text_French": "Par exemple, pour cette autre question sur le tournoi en simple de Wimbledon, la perturbation dépend du type de jeu, du type de tournoi ou du résultat du jeu.", "text_Japanese": "たとえば、ウィンブルドンのシングルス トーナメントに関するこの別の質問の場合、混乱は試合の種類、トーナメントの種類、または試合の結果に沿ったものです。", "text_Russian": "Например, в другом вопросе об Уимблдонском турнире в одиночном разряде возмущения связаны с типом игры, типом турнира или ее результатом."}
{"text": "Final takeaways; we tackle the task of counterfactual data augmentation and perturbations for information seeking queries and tackle its unique challenges via a reversal of the generation approach, over generate using near misses of the model and filter based on perturbation type or minimality.\n", "text_Chinese": "最后要点；我们解决反事实数据增强和信息搜索查询扰动的任务，并通过生成方法的逆转、使用模型的未遂事件进行过度生成以及基于扰动类型或极小性的过滤器来解决其独特的挑战。", "text_Arabic": "الوجبات السريعة النهائية. نحن نتعامل مع مهمة زيادة البيانات المغايرة للواقع والاضطرابات لاستفسارات البحث عن المعلومات ومعالجة تحدياتها الفريدة من خلال عكس نهج التوليد، والتوليد الزائد باستخدام الأخطاء الوشيكة للنموذج والتصفية بناءً على نوع الاضطراب أو الحد الأدنى.", "text_French": "Points à retenir : nous abordons la tâche d'augmentation des données contrefactuelles et de perturbations pour les requêtes de recherche d'informations et abordons ses défis uniques via une inversion de l'approche de génération, une surgénération en utilisant des quasi-accidents du modèle et un filtre basé sur le type de perturbation ou la minimalité.", "text_Japanese": "最後の要点;私たちは、情報探索クエリに対する反事実データの増強と摂動のタスクに取り組み、生成アプローチの逆転を通じて、モデルのニアミスを使用して過剰生成し、摂動のタイプまたは最小性に基づいてフィルタリングすることで、その独特の課題に取り組みます。", "text_Russian": "Заключительные выводы; мы решаем задачу контрфактического увеличения и искажения данных для запросов на поиск информации и решаем ее уникальные проблемы посредством обращения к подходу генерации, сверхгенерации с использованием близких промахов модели и фильтра на основе типа возмущения или минимальности."}
{"text": "We find that this technique requires no additional supervision and the examples are labeled for augmentation.\n", "text_Chinese": "我们发现这种技术不需要额外的监督，并且示例被标记为增强。", "text_Arabic": "نجد أن هذه التقنية لا تتطلب أي إشراف إضافي وتم تصنيف الأمثلة للزيادة.", "text_French": "Nous constatons que cette technique ne nécessite aucune supervision supplémentaire et que les exemples sont étiquetés pour une augmentation.", "text_Japanese": "この手法には追加の監督が必要なく、例には拡張用のラベルが付いていることがわかりました。", "text_Russian": "Мы обнаружили, что этот метод не требует дополнительного контроля, и примеры помечены для расширения."}
{"text": "Augmentation improves out of domain generalization and neighborhood consistency.\n", "text_Chinese": "增强提高了域外泛化和邻域一致性。", "text_Arabic": "تعمل عملية التعزيز على تحسين التعميم خارج النطاق واتساق الحي.", "text_French": "L'augmentation s'améliore hors de la généralisation du domaine et de la cohérence du quartier.", "text_Japanese": "拡張により、領域外の一般化と近傍の一貫性が向上します。", "text_Russian": "Расширение улучшается за счет обобщения предметной области и согласованности окрестностей."}
{"text": "And we find that RGF counterfactuals are semantically diverse without introducing bias during augmentation.\n", "text_Chinese": "我们发现 RGF 反事实在语义上是多样化的，并且在增强过程中不会引入偏差。", "text_Arabic": "ونجد أن الحقائق المضادة لـ RGF متنوعة لغويًا دون تقديم تحيز أثناء التكبير.", "text_French": "Et nous constatons que les contrefactuels RGF sont sémantiquement divers sans introduire de biais lors de l’augmentation.", "text_Japanese": "そして、RGF の反事実は、拡張中にバイアスを導入することなく意味的に多様であることがわかりました。", "text_Russian": "И мы обнаруживаем, что контрфакты RGF семантически разнообразны, не внося предвзятости во время дополнения."}
{"text": "Thank you.\n", "text_Chinese": "谢谢。", "text_Arabic": "شكرًا لك.", "text_French": "Merci.", "text_Japanese": "ありがとう。", "text_Russian": "Спасибо."}
