{"text": "Hi, this is Elena and I'm going to be presenting our work, Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling.\n", "text_Chinese": "嗨, 这是 Elena, 我将介绍我们的工作, 西班牙语中未同化借用检测 : 附注的语料库和建模方法", "text_Arabic": "مرحبا, أنا إيلينا وسأقدم عملنا, الكشف عن الاقتراض غير المتساوي في اللغة الإسبانية: مجموعة من الملاحظات والمقاربات للنمذجة.", "text_French": "Bonjour, c'est Elena et je vais présenter notre travail, Détection des emprunts non assimilés en espagnol : un corpus annoté et des approches de modélisation.", "text_Japanese": "こんにちはエレナです私は私たちの研究スペイン語で同化されていない借り物を検出する:注釈されたコーパスとモデリングへのアプローチを発表します", "text_Russian": "Привет, это Елена, и я собираюсь представить нашу работу, \"Обнаружение не ассимилированных заимствований на испанском языке: аннотированный корпус и подходы к моделированию\"."}
{"text": "So we're going to be covering what lexical borrowing is, the task that we proposed, the dataset that we have released and some models that we explored.\n", "text_Chinese": "我们将介绍什么是词汇借用, 我们提出的任务, 我们发布的数据集, 以及我们探索的一些模型", "text_Arabic": "لذا سنقوم بتغطية ما هو الاقتراض اللفظي, المهمة التي اقترحناها, مجموعة البيانات التي أصدرناها وبعض النماذج التي استكشفناها.", "text_French": "Nous allons donc couvrir ce qu'est l'emprunt lexical, la tâche que nous avons proposée, l'ensemble de données que nous avons publié et certains modèles que nous avons explorés.", "text_Japanese": "語借り入れとは何か提案したタスクリリースしたデータセットそして探求したモデルについて説明します", "text_Russian": "Мы расскажем о том, что такое лексическое заимствование, о задаче, которую мы предложили, о наборе данных, который мы опубликовали, и о некоторых моделях, которые мы исследовали."}
{"text": "But to begin with, what is lexical borrowing and why it matters as an NLP task?\n", "text_Chinese": "首先, 什么是词汇借用, 为什么它对 NLP 任务很重要?", "text_Arabic": "ولكن بداية, ما هو الاقتراض اللفظي ولماذا هو مهم كمهمة في NLP?", "text_French": "Mais pour commencer, qu'est-ce que l'emprunt lexical et pourquoi est-il important comme tâche de PNL?", "text_Japanese": "しかし最初に語借り入れとは何ですかそしてNLPのタスクとしてなぜ重要なのか?", "text_Russian": "Но для начала, что такое лексическое заимствование и почему оно важно в задаче НЛП?"}
{"text": "Well, lexical borrowing is basically the incorporation of words from one language into another language.\n", "text_Chinese": "词汇借用基本上是将一个语言的单词融入到另一个语言中.", "text_Arabic": "حسنا, الاقتراض اللفظي هو أساسا دمج الكلمات من لغة إلى لغة أخرى.", "text_French": "L'emprunt lexical est essentiellement l'incorporation de mots d'une langue dans une autre langue.", "text_Japanese": "語借用とは基本的に一言語から別の言語に単語を組み込むことです", "text_Russian": "Лексическое заимствование - это в основном включение слов из одного языка в другой."}
{"text": "For instance, in Spanish we use words that come from English.\n", "text_Chinese": "例如,在西班牙语中,我们使用来自英语的单词.", "text_Arabic": "على سبيل المثال، في الإسبانية نستخدم كلمات تأتي من الإنجليزية.", "text_French": "Par exemple, en espagnol, nous utilisons des mots qui viennent de l'anglais.", "text_Japanese": "例えば スペイン語では 英語から来た言葉を使います", "text_Russian": "Например, в испанском языке мы используем слова, взятые из английского."}
{"text": "And here you have a few examples, words such as podcast, app, and online crowdfunding, all these are English words that we sometimes use in Spanish.\n", "text_Chinese": "这里有几个例子, 像播客, 应用程序, 和在线众筹这样的词, 这些都是我们有时在西班牙语中使用的英语词", "text_Arabic": "وهنا لدينا بعض الأمثلة, كلمات مثل البودكاست, التطبيق, والتمويل الجماعي عبر الإنترنت, كل هذه كلمات إنجليزية نستخدمها أحيانا في الإسبانية.", "text_French": "Et voici quelques exemples, des mots comme podcast, app, et crowdfunding en ligne, tous ces mots sont anglais que nous utilisons parfois en espagnol.", "text_Japanese": "ポッドキャストアプリオンラインクラウドファンディングなどの言葉はこれらはすべてスペイン語で時々使用する英語の言葉です", "text_Russian": "Вот несколько примеров: такие слова, как подкаст, приложение и онлайн краудфандинг,  это английские слова, которые мы иногда используем в испанском языке."}
{"text": "Lexical borrowing is a type of linguistic borrowing um which is basically reproducing in one language patterns of other languages.\n", "text_Chinese": "词汇借用是一种语言借用,基本上是在一种语言中复制其他语言的模式.", "text_Arabic": "الاقتراض اللفظي هو نوع من الاقتراض اللغوي الذي يتم في الأساس إعادة إنتاج أنماط لغات أخرى في لغة واحدة.", "text_French": "L'emprunt lexicale est un type d'emprunt linguistique qui consiste essentiellement à reproduire dans une langue des modèles d'autres langues.", "text_Japanese": "語借用は,基本的に他の言語のパターンを1つの言語で再現する言語学的な借用の一種です.", "text_Russian": "Лексическое заимствование - это тип лингвистического заимствования, который в основном воспроизводит в одном языке образцы других языков."}
{"text": "And borrowing and code switching have sometimes been compared and described as a continuum, code switching being ah the thing that bilinguals do where they mix two languages at the same time.\n", "text_Chinese": "借用和代码交换有时被比较并描述为一个连续体, 代码交换是双语者同时混合两种语言的做法", "text_Arabic": "وقد تم مقارنة الاقتراض وتبديل الشفرة في بعض الأحيان ووصفها بأنها متواصلة, وتبديل الشفرة هو الشيء الذي يقوم به ثنائي اللغة حيث يخلطون لغتين في نفس الوقت.", "text_French": "Et l'emprunt et le changement de code ont parfois été comparés et décrits comme un continuum, le changement de code étant ah la chose que font les bilingues où ils mélangent deux langues en même temps.", "text_Japanese": "借用とコード切り替えは時々比較され連続として説明されていますコード切り替えは同時に2つの言語を混ぜる双語の人がするものです", "text_Russian": "И заимствование и переключение кодов иногда сравнивают и описывают как континуум, переключение кодов  это то, что делают двуязычные, когда они смешивают два языка одновременно."}
{"text": "There are however some differences between lexical borrowing and code-switching.\n", "text_Chinese": "然而,词汇借用和代码切换之间存在一些差异.", "text_Arabic": "ومع ذلك، هناك بعض الاختلافات بين الاقتراض اللفظي وتبديل الشفرة.", "text_French": "Il existe cependant certaines différences entre l'emprunt lexicale et la commutation de code.", "text_Japanese": "しかし,語借用とコード切り替えにはいくつかの違いがあります.", "text_Russian": "Однако существуют некоторые различия между лексическим заимствованием и переключением кодов."}
{"text": "We're going to be focusing on lexical borrowing.\n", "text_Chinese": "我们将专注于词汇借用.", "text_Arabic": "سنركز على الاقتراض اللفظي.", "text_French": "Nous allons nous concentrer sur les emprunts lexicales.", "text_Japanese": "語の借用に焦点を当てます", "text_Russian": "Мы собираемся сосредоточиться на лексическом заимствовании."}
{"text": "Code switching is something that is done by bilinguals and by definition the code switches are not integrated into any of the languages in use, whereas lexical borrowing is something that is also done by monolinguals.\n", "text_Chinese": "代码切换是由双语人士进行的,并且根据定义,代码切换不会被整合到任何使用的语言中,而词汇借用也是单语人士进行的.", "text_Arabic": "التبديل الشفوي هو شيء يتم من قبل ثنائي اللغة وبالتعريف لا يتم دمج التبديلات الشفويه في أي من اللغات المستخدمة، في حين أن الاقتراض اللفظي هو شيء يتم أيضا من قبل أحادي اللغة.", "text_French": "La commutation de code est quelque chose qui est fait par les bilingues et par définition les commutateurs de code ne sont intégrés dans aucune des langues utilisées, alors que l'emprunt lexicale est quelque chose qui est également fait par les monolingues.", "text_Japanese": "コード・スイッチングは 両言語で行われるもので 定義上 コード・スイッチは 使われている言語に統合されていませんが 語の借り入れは 単語で行われるものです", "text_Russian": "Переключение кодов - это то, что делают двуязычные люди, и по определению переключения кодов не интегрированы ни в один из используемых языков, в то время как лексическое заимствование - это то, что также делают одноязычные люди."}
{"text": "The borrowings will comply with the grammar of the recipient language.\n", "text_Chinese": "借款将符合接受语言的语法.", "text_Arabic": "يجب أن تتوافق القروض مع قواعد اللغة في اللغة المتلقية.", "text_French": "Les emprunts seront conformes à la grammaire de la langue destinataire.", "text_Japanese": "借り物は受信言語の文法に準拠する.", "text_Russian": "Заимствования будут соответствовать грамматике языка получателя."}
{"text": "And borrowings can eventually be integrated into the recipient language.\n", "text_Chinese": "借用文字最终可以被整合到接受语言中.", "text_Arabic": "ويمكن دمج الاقتراضات في نهاية المطاف في لغة المتلقي.", "text_French": "Et les emprunts peuvent éventuellement être intégrés dans la langue destinataire.", "text_Japanese": "そして借り物は最終的に 受信言語に統合されます", "text_Russian": "И заимствования в конечном итоге могут быть интегрированы в язык получателя."}
{"text": "So why is borrowing an interesting phenomenon?\n", "text_Chinese": "那么为什么借贷是一个有趣的现象?", "text_Arabic": "لماذا الاقتراض ظاهرة مثيرة للاهتمام?", "text_French": "Alors pourquoi l'emprunt est-il un phénomène intéressant?", "text_Japanese": "では借りることはなぜ興味深い現象なのか?", "text_Russian": "Почему же заимствование является интересным явлением?"}
{"text": "Well, from the point of view of linguistics, borrowing is a manifestation of of how languages change and how they interact.\n", "text_Chinese": "从语言学的角度来看, 借用是语言如何变化和如何相互作用的表现", "text_Arabic": "حسنا, من وجهة نظر اللغويات, الاقتراض هو مظهر لكيفية تغير اللغات وكيفية تفاعلها.", "text_French": "Du point de vue de la linguistique, l'emprunt est une manifestation de la façon dont les langues changent et comment elles interagissent.", "text_Japanese": "言語学的な観点から見ると 借り物は 言語がどのように変化し どのように相互作用するかという現れです", "text_Russian": "С точки зрения лингвистики, заимствование - это проявление того, как языки меняются и как они взаимодействуют."}
{"text": "And also lexical borrowings are a source of new words.\n", "text_Chinese": "词汇借用也是新词的来源.", "text_Arabic": "كما أن الاقتراضات اللفظية هي مصدر لكلمات جديدة.", "text_French": "Et aussi les emprunts lexicales sont une source de nouveaux mots.", "text_Japanese": "また語借用は新しい単語の源泉でもある", "text_Russian": "Лексические заимствования также являются источником новых слов."}
{"text": "Here you have some examples of lexical borrowings that have been incorporated into the Spanish language as new words.\n", "text_Chinese": "这里有一些词汇借用例子,", "text_Arabic": "هنا لديك بعض الأمثلة على الاقتراضات اللفظية التي تم دمجها في اللغة الإسبانية ككلمات جديدة.", "text_French": "Voici quelques exemples d'emprunts lexicales qui ont été incorporés dans la langue espagnole en tant que nouveaux mots.", "text_Japanese": "ここでは新しい単語としてスペイン語に組み込まれた語借用の例をいくつか紹介します", "text_Russian": "Вот несколько примеров лексических заимствований, которые были включены в испанский язык в качестве новых слов."}
{"text": "In terms of NLP ah borrowings are a common source of out-of-vocabulary words.\n", "text_Chinese": "在NLP方面, 借用是词汇之外的常见词源.", "text_Arabic": "من حيث NLP ah الاقتراضات هي مصدر شائع للكلمات خارج المفردات.", "text_French": "En termes de PNL, les emprunts ah sont une source courante de mots hors vocabulaire.", "text_Japanese": "NLPの観点から言えば 借用は 語外の単語の よくある源です", "text_Russian": "С точки зрения НЛП, заимствования являются распространенным источником слов, не входящих в словарный запас."}
{"text": "And in fact, automatically detecting lexical borrowings ah has proven to be useful for NLP downstream tasks such as parsing, text-to-speech synthesis or machine translation.\n", "text_Chinese": "事实上,自动检测词汇借用已经被证明对NLP下游任务有用,如解析,文本转语音合成或机器翻译.", "text_Arabic": "وفي الواقع, فقد ثبت أن الكشف التلقائي عن الاقتراضات المفردة مفيد لمهام NLP مثل التحليل, أو التوليف النص إلى الكلام أو الترجمة الآلية.", "text_French": "Et en fait, la détection automatique des emprunts lexicales s'est avérée utile pour les tâches en aval de la PNL telles que l'analyse, la synthèse texte-parole ou la traduction automatique.", "text_Japanese": "実際語借用を自動的に検出することは解析テキスト・スピーチ合成機械翻訳などのNLPのダウンストリームタスクに有用であることが証明されています", "text_Russian": "И на самом деле, автоматическое обнаружение лексических заимствований оказалось полезным для NLP-задач, таких как анализ, синтез текста в речь или машинный перевод."}
{"text": "There has been a growing interest in the influence of English on other languages ah particularly ah related to English lexical borrowings, borrowings which sometimes have been called Anglicisms.\n", "text_Chinese": "人们越来越感兴趣英语对其他语言的影响, 特别是与英语词汇借用有关的借用,", "text_Arabic": "كان هناك اهتمام متزايد بتأثير اللغة الإنجليزية على اللغات الأخرى، وخاصة فيما يتعلق بالاقتراضات اللفظية الإنجليزية، والتي تسمى أحيانًا الإنكليزية.", "text_French": "Il y a eu un intérêt croissant pour l'influence de l'anglais sur d'autres langues, en particulier en ce qui concerne les emprunts lexicales anglais, emprunts qui ont parfois été appelés anglicismes.", "text_Japanese": "英語の他の言語への影響に 関心が高まってきています 特に英語の語の借用に関連する 借用は時々 イングランド語と呼ばれています", "text_Russian": "Возрастает интерес к влиянию английского языка на другие языки, особенно связанному с английскими лексическими заимствованиями, заимствованиями, которые иногда называют англицизмами."}
{"text": "And here, you have some examples of ah work on automatic detection of borrowings in ah some of these languages.\n", "text_Chinese": "这里有一些例子, 用于自动检测这些语言中的借用", "text_Arabic": "وهنا، لديك بعض الأمثلة على العمل على الكشف التلقائي عن الاقتراض في بعض هذه اللغات.", "text_French": "Et ici, vous avez quelques exemples de travail sur la détection automatique des emprunts dans certaines de ces langues.", "text_Japanese": "ここではこれらの言語のいくつかで借り物の自動検出に関するいくつかの例があります", "text_Russian": "И здесь у вас есть несколько примеров работы по автоматическому обнаружению заимствований на некоторых из этих языков."}
{"text": "So the task that we propose is to detect unassimilated lexical borrowings in Spanish newswire.\n", "text_Chinese": "所以我们提议的任务是检测西班牙新闻电报中未同化词汇借用.", "text_Arabic": "لذا فإن المهمة التي نقترحها هي الكشف عن الاقتراضات المفردية غير المستوحاة في الأخبار الإسبانية.", "text_French": "La tâche que nous proposons est de détecter les emprunts lexicales non assimilés dans les journaux espagnols.", "text_Japanese": "提案する課題は スペイン語のニュースワイヤーの 吸収されていない語の借用を検出することです", "text_Russian": "Задача, которую мы предлагаем, состоит в обнаружении не ассимилированных лексических заимствований в испанских новостях."}
{"text": "Which means that we are interested in extracting ah words borrowed from other languages that are being used in Spanish newspapers but that have not been integrated or assimilated into the recipient language.\n", "text_Chinese": "这意味着我们有兴趣提取来自其他语言的借用词, 这些词在西班牙报纸中使用, 但尚未被整合或同化到接受语言中.", "text_Arabic": "وهذا يعني أننا مهتمون باستخراج كلمات مستعارة من لغات أخرى تستخدم في الصحف الإسبانية ولكن لم يتم دمجها أو استيعابها في اللغة المستقبلة.", "text_French": "Ce qui signifie que nous sommes intéressés à extraire des mots empruntés à d'autres langues qui sont utilisés dans les journaux espagnols mais qui n'ont pas été intégrés ou assimilés dans la langue destinataire.", "text_Japanese": "スペイン語の新聞で使われているが 受信言語に統合されない 言語に同化されない 他の言語から借りた言葉を 抽出することに興味があるのです", "text_Russian": "Это означает, что мы заинтересованы в извлечении слов, заимствованных из других языков, которые используются в испанских газетах, но не были интегрированы или ассимилированы в язык-получатель."}
{"text": "So not yet integrated into Spanish.\n", "text_Chinese": "所以还没有融入西班牙语.", "text_Arabic": "لذلك لم يتم دمجها بعد في الإسبانية.", "text_French": "Donc pas encore intégré à l'espagnol.", "text_Japanese": "スペイン語に統合されていないのです", "text_Russian": "Так что еще не интегрирован в испанский."}
{"text": "Here you have an example.\n", "text_Chinese": "这里有一个例子.", "text_Arabic": "إليك مثال.", "text_French": "Voici un exemple.", "text_Japanese": "ここに例があります", "text_Russian": "Вот вам пример."}
{"text": "This is a sentence in Spanish: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork.\n", "text_Chinese": "这是西班牙语的一句话: 畅销书的印刷品以花卉为动机, 动物印花或复制品类的拼接.", "text_Arabic": "هذه جملة باللغة الإسبانية: \"الكتب الأكثر مبيعا تمثلت في النواحي الزهورية، الطباعة الحيوانية أو القصص التى تشبه النصوص\".", "text_French": "Voici une phrase en espagnol : Les best-sellers se estampan con motivos florales, animal print o retales tipo patchwork.", "text_Japanese": "これはスペイン語の文です ベストセラーの作品は 花のモチーフで印刷され 動物のプリントや ペッチワークのようなレタレです", "text_Russian": "Это предложение на испанском языке: \"Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork\"."}
{"text": "Um, and as you can see, there are three spans of texts which are actually English words like bestseller, animal print and patchwork.\n", "text_Chinese": "嗯, 正如你所看到的, 有三段文字实际上是英语单词, 比如bestseller, animal print和patchwork", "text_Arabic": "وكما ترون, هناك ثلاث فترات من النصوص التي هي في الواقع كلمات إنجليزية مثل الأكثر مبيعا, طباعة الحيوانات و patchwork.", "text_French": "Et comme vous pouvez le voir, il y a trois textes qui sont en fait des mots anglais comme bestseller, animal print et patchwork.", "text_Japanese": "よく見えるように本文には3つのスパンがあり実際にはベストセラー動物のプリントパッチワークなどの英語の単語です", "text_Russian": "Как видите, в тексте три слога, которые на самом деле являются английскими словами, такими как бестселлер, животный принт и патчворк."}
{"text": "These are the type of spans that we are interested in extracting and detecting.\n", "text_Chinese": "这些是我们感兴趣的抽取和检测的跨度类型.", "text_Arabic": "هذه هي نوع الفترات التي نحن مهتمون في استخراج والكشف عنها.", "text_French": "Ce sont le type de portées que nous sommes intéressés à extraire et à détecter.", "text_Japanese": "これらは我々が抽出し検出することに興味を持っているスパンタイプです", "text_Russian": "Это тот тип пролётов, который мы заинтересованы в извлечении и обнаружении."}
{"text": "There has been previous word on Anglicism detection ah which consists consisted of a CRF model for Anglicism detection on Spanish Newswire.\n", "text_Chinese": "在西班牙新闻网上, 已经有过关于英语检测的说法,", "text_Arabic": "كان هناك كلمة سابقة عن الكشف عن الانجليكية التي تتكون من نموذج CRF للكشف عن الانجليكية على Spanish Newswire.", "text_French": "Il y a eu un mot précédent sur la détection de l'anglicisme ah qui consiste consistait en un modèle CRF pour la détection de l'anglicisme sur Spanish Newswire.", "text_Japanese": "スペイン語のニュースワイヤーの英語化検出のためのCRFモデルで構成されている.", "text_Russian": "Было предыдущее слово о обнаружении англицизма, которое состоит из модели CRF для обнаружения англицизма на испанском Newswire."}
{"text": "This model achieved an F1 score of eighty six.\n", "text_Chinese": "这款车型获得了F1得分86分.", "text_Arabic": "حقق هذا النموذج درجة F1 من ستة وثمانين.", "text_French": "Ce modèle a obtenu un score F1 de quatre-vingt-six.", "text_Japanese": "このモデルはF1スコア86を達成した.", "text_Russian": "Эта модель достигла балла F1 в восемьдесят шесть."}
{"text": "But there were some limitations both um in the dataset and the modeling approach.\n", "text_Chinese": "但是在数据集和建模方法中都有一些局限性.", "text_Arabic": "ولكن كان هناك بعض القيود في كل من مجموعة البيانات ونهج النمذجة.", "text_French": "Mais il y avait des limitations à la fois dans l'ensemble de données et dans l'approche de modélisation.", "text_Japanese": "しかしデータセットとモデリングアプローチの両方にいくつかの制限がありました", "text_Russian": "Но были некоторые ограничения как в наборе данных, так и в подходе к моделированию."}
{"text": "So the dataset focused exclusively on one source of news, consisted only of headlines.\n", "text_Chinese": "所以数据集只关注一个新闻来源, 仅包括头条新闻", "text_Arabic": "لذا ركزت مجموعة البيانات حصريًا على مصدر واحد من الأخبار, تتكون فقط من العناوين الرئيسية.", "text_French": "Donc l'ensemble de données se concentrait exclusivement sur une source d'information, ne comprenant que des titres.", "text_Japanese": "データセットは ニュース源に専念し タイトルだけを集めました", "text_Russian": "Таким образом, набор данных был сосредоточен исключительно на одном источнике новостей, состоял только из заголовков."}
{"text": "And also there was an overlap in the borrowings that appear in the training set and the test set.\n", "text_Chinese": "还有在训练集和测试集中出现的借款中存在重叠.", "text_Arabic": "وكان هناك أيضا تداخل في الاقتراضات التي تظهر في مجموعة التدريب ومجموعة الاختبار.", "text_French": "Et il y avait aussi un chevauchement dans les emprunts qui apparaissent dans l'ensemble d'entraînement et l'ensemble de test.", "text_Japanese": "またトレーニングセットとテストセットに表示される借り物に重複がありました", "text_Russian": "Также было совпадение в заимствованиях, которые появляются в наборе тренировок и наборе тестов."}
{"text": "This prevented the assessment of whether the modeling approach could actually generalize to previously unseen borrowings.\n", "text_Chinese": "这使得无法评估建模方法是否实际上可以推广到以前未见的借款.", "text_Arabic": "هذا منع تقييم ما إذا كان نهج النمذجة يمكن فعلاً تعميمه على الاقتراضات غير المرئية سابقًا.", "text_French": "Cela a empêché d'évaluer si l'approche de modélisation pouvait réellement se généraliser à des emprunts inédits.", "text_Japanese": "これは,モデリングアプローチが実際に以前に見られなかった借入に一般化できるかどうかを評価することを妨げました.", "text_Russian": "Это помешало оценить, может ли подход к моделированию фактически обобщить ранее невиданные заимствования."}
{"text": "So what we aim is to tackle some of these limitations in the task.\n", "text_Chinese": "所以我们的目标是解决这些任务的局限性", "text_Arabic": "لذا ما نهدف إليه هو معالجة بعض هذه القيود في المهمة.", "text_French": "Notre objectif est donc de nous attaquer à certaines de ces limitations.", "text_Japanese": "したがって私たちの目標はこのタスクのいくつかの制限に対処することです", "text_Russian": "Поэтому мы стремимся устранить некоторые из этих ограничений."}
{"text": "So to begin we, to begin with, we created a new dataset.\n", "text_Chinese": "首先, 我们创建了一个新的数据集", "text_Arabic": "لذلك, في البداية, قمنا بإنشاء مجموعة بيانات جديدة.", "text_French": "Donc pour commencer, nous avons créé un nouvel ensemble de données.", "text_Japanese": "まず新しいデータセットを作りました", "text_Russian": "Для начала мы создали новый набор данных."}
{"text": "Ah the aim at a new dataset that was annotated with lexical borrowings and the aim was to create a test set that was as difficult as possible.\n", "text_Chinese": "目标是创建一个尽可能困难的测试集.", "text_Arabic": "كان الهدف هو مجموعة بيانات جديدة مشمولة بمقترضات لغوية وكان الهدف هو إنشاء مجموعة اختبارات صعبة قدر الإمكان.", "text_French": "Ah, le but était d'obtenir un nouvel ensemble de données annoté avec des emprunts lexicaux et le but était de créer un ensemble de tests aussi difficile que possible.", "text_Japanese": "目的は語借用で注釈された新しいデータセットであり目的は可能な限り困難なテストセットを作成することでした", "text_Russian": "Целью было создание нового набора данных, который был аннотирован лексическими заимствованиями, и целью было создать набор тестов, которые были как можно сложнее."}
{"text": "So there would be minimal overlap in words and topics between the training set and test set.\n", "text_Chinese": "因此,训练集和测试集之间的单词和主题重叠率将最小.", "text_Arabic": "لذلك سيكون هناك الحد الأدنى من التداخل في الكلمات والمواضيع بين مجموعة التدريب ومجموعة الاختبار.", "text_French": "Il y aurait donc un chevauchement minimal de mots et de sujets entre l'ensemble d'entraînement et l'ensemble de tests.", "text_Japanese": "訓練セットとテストセットの間には 単語とトピックの重複が 少なくありません", "text_Russian": "Таким образом, будет минимальное перекрытие слов и тем между набором тренировок и набором тестов."}
{"text": "And as a result, well, the test set comes from sources and dates that we're not seeing in the training set.\n", "text_Chinese": "结果, 测试集来自于我们没有在训练集中看到的来源和日期", "text_Arabic": "ونتيجة لذلك, مجموعة الاختبارات تأتي من مصادر وتاريخات لا نراها في مجموعة التدريب.", "text_French": "Et en conséquence, le jeu de tests provient de sources et de dates que nous ne voyons pas dans le jeu d'entraînement.", "text_Japanese": "結果としてテストセットはトレーニングセットでは見えないソースと日付から来ています", "text_Russian": "В результате, набор тестов исходит из источников и дат, которых мы не видим в наборе тренировок."}
{"text": "Here you can see that there's no overlap in the in the time.\n", "text_Chinese": "在这里你可以看到没有时间的重叠.", "text_Arabic": "هنا يمكنك أن ترى أنه لا يوجد تداخل في الوقت.", "text_French": "Ici vous pouvez voir qu'il n'y a pas de chevauchement dans le temps.", "text_Japanese": "ここでは時間に重なり合うことはないことがわかります", "text_Russian": "Здесь вы можете видеть, что нет никакого перекрытия во времени."}
{"text": "It's also, the test set is also very borrowing-dense.\n", "text_Chinese": "测试集也非常密集的借用.", "text_Arabic": "كما أن مجموعة الاختبارات كثيفة جداً من الاقتراض.", "text_French": "C'est aussi, le jeu de tests est aussi très dense en emprunts.", "text_Japanese": "またテストセットも非常に借り入れが密集しています", "text_Russian": "Это также, набор тестов также очень заимствованный."}
{"text": "Just to give you some numbers, if the training set contains six borrowings per each thousand tokens, the test set contained twenty borrowings per each thousand tokens.\n", "text_Chinese": "给大家一些数字,如果训练集每千个代币包含六个借款,测试集每千个代币包含二十个借款.", "text_Arabic": "فقط لإعطائك بعض الأرقام, إذا كانت مجموعة التدريب تحتوي على ستة اقتراضات لكل ألف توكن, فإن مجموعة الاختبار تحتوي على عشرين اقتراض لكل ألف توكن.", "text_French": "Juste pour vous donner quelques chiffres, si l'ensemble d'entraînement contient six emprunts pour chaque millier de jetons, l'ensemble de test contenait vingt emprunts pour chaque millier de jetons.", "text_Japanese": "テストセットには千トークンあたり20の借り入れが含まれていました テストセットには千トークンあたり20の借り入れが含まれていました", "text_Russian": "Просто чтобы дать вам некоторые цифры, если учебный набор содержит шесть заимствований на каждую тысячу токенов, тестовый набор содержал двадцать заимствований на каждую тысячу токенов."}
{"text": "The test set contained as many out of vocabulary words as possible.\n", "text_Chinese": "测试套件包含尽可能多的词汇词.", "text_Arabic": "مجموعة الاختبار تحتوي على أكبر عدد ممكن من الكلمات خارج المفردات.", "text_French": "Le jeu de tests contenait autant de mots hors du vocabulaire que possible.", "text_Japanese": "テストセットにはできるだけ多くの語のない単語が含まれていました", "text_Russian": "Тест содержал как можно больше слов, не относящихся к словарному запасу."}
{"text": "In fact, ninety two percent of the borrowings in the test set are OOV.\n", "text_Chinese": "事实上,测试中的九十二%的借款是OOV.", "text_Arabic": "في الواقع, 92% من الاقتراضات في مجموعة الاختبار هي OOV.", "text_French": "En fait, 92% des emprunts dans le test sont OOV.", "text_Japanese": "実際テストセットの借入の92%はOOVです", "text_Russian": "На самом деле, 92% заимствований в тесте являются OOV."}
{"text": "So, they were not seen during training.\n", "text_Chinese": "所以,他们在训练期间没有被看到.", "text_Arabic": "لذا، لم يُشاهدوا أثناء التدريب.", "text_French": "Donc, ils n'ont pas été vus pendant l'entraînement.", "text_Japanese": "訓練中に見られなかった", "text_Russian": "Значит, их не видели во время тренировок."}
{"text": "And the corpus consisted basically of a collection of texts that came from different sources of Spanish newspapers.\n", "text_Chinese": "书集基本上是由来自西班牙报纸的不同来源的文本收集而成.", "text_Arabic": "وكان المجموع يتكون أساسا من مجموعة من النصوص التي جاءت من مصادر مختلفة من الصحف الإسبانية.", "text_French": "Et le corpus consistait essentiellement en une collection de textes provenant de différentes sources de journaux espagnols.", "text_Japanese": "コルパスは基本的に スペインの新聞の 異なる情報源から集められたテキストで構成されていました", "text_Russian": "Корпус состоял в основном из сборника текстов из разных источников испанских газет."}
{"text": "And ah it was annotated by hand ah using two tags.\n", "text_Chinese": "它是手工注释的,使用两个标签.", "text_Arabic": "وقد تم التعليق عليها باليد باستخدام علامة.", "text_French": "Et il a été annoté à la main en utilisant deux étiquettes.", "text_Japanese": "2つのタグを使用して手書きで注釈されました.", "text_Russian": "И он был аннотирован вручную с помощью двух меток."}
{"text": "One for English lexical borrowings which is the majority of lexical borrowings in Spanish, and then the label other for borrowings from other languages.\n", "text_Chinese": "一个是英语词汇借用,这是西班牙语的大部分词汇借用,", "text_Arabic": "واحد للعبور اللفظي الإنجليزي الذي هو غالبية الاقتراض اللفظي في الإسبانية، ثم التسمية الأخرى للاقتراض من اللغات الأخرى.", "text_French": "Un pour les emprunts lexicales anglais qui sont la majorité des emprunts lexicales en espagnol, et puis l'étiquette autre pour les emprunts d'autres langues.", "text_Japanese": "スペイン語の語借用の大部分である英語の語借用のための1つ,そして他の言語からの借用のための他のラベル.", "text_Russian": "Один для английских лексических заимствований, которые являются большинством лексических заимствований в испанском языке, а затем другой для заимствований из других языков."}
{"text": "We use CONLL formats and we used BIO encoding so that we could encode ah single token borrowings such as app or multi token borrowings such as machine learning.\n", "text_Chinese": "我们使用 CONLL 格式,并使用 BIO 编码,以便我们能够编码单个令牌借用,如应用程序,或多令牌借用,如机器学习", "text_Arabic": "نحن نستخدم تنسيقات CONLL ونستخدم ترميز BIO حتى نتمكن من ترميز اقتراضات رمزية واحدة مثل التطبيق أو اقتراضات متعددة الرموز مثل التعلم الآلي.", "text_French": "Nous utilisons des formats CONLL et nous avons utilisé l'encodage BIO afin de pouvoir encoder des emprunts d'un seul jeton tels que des applications ou des emprunts multi-tokens tels que l'apprentissage automatique.", "text_Japanese": "アプリのような単一トークン借入や マシンラーニングのようなマルチトークン借入を 暗号化できるようにしました", "text_Russian": "Мы используем форматы CONLL и BIO кодирование, чтобы мы могли кодировать заимствования одного токена, такие как приложение, или заимствования нескольких токенов, такие как машинное обучение."}
{"text": "These are the numbers of the corpus.\n", "text_Chinese": "这些是库布斯的数字.", "text_Arabic": "هذه هي أرقام المجموع.", "text_French": "Ce sont les numéros du corpus.", "text_Japanese": "コルパスの番号です", "text_Russian": "Это номера корпуса."}
{"text": "As you can see, it amounts to roughly three hundred seventy thousand tokens.\n", "text_Chinese": "正如你所看到的,它大约有37万个代币.", "text_Arabic": "كما ترون، يبلغ إجماليها حوالي ثلاثمائة وسبعين ألف رمز.", "text_French": "Comme vous pouvez le voir, il s'agit d'environ trois cent soixante-dix mille jetons.", "text_Japanese": "ご覧の通り,およそ37万トークンです.", "text_Russian": "Как вы видите, это примерно 370 тысяч токенов."}
{"text": "And here you have the number of spans that were labeled as English and the spans that were labeled as other borrowings and how many of them were unique.\n", "text_Chinese": "这里有标记为英语的跨度和标记为其他借用的跨度,", "text_Arabic": "وهنا لدينا عدد الفترات التي تم تصنيفها على أنها إنجليزية والفترات التي تم تصنيفها على أنها اقتراضات أخرى وكم منها كانت فريدة من نوعها.", "text_French": "Et ici vous avez le nombre de spans qui ont été étiquetés comme anglais et les spans qui ont été étiquetés comme d'autres emprunts et combien d'entre eux étaient uniques.", "text_Japanese": "英語と他の借用とラベル付けされたスパンとそのうちのどれがユニークだったか", "text_Russian": "Здесь вы видите количество пробегов, которые были помечены как английские, и пробеги, которые были помечены как другие заимствования, и сколько из них были уникальными."}
{"text": "And here you have a couple of examples of the of the set of the dataset.\n", "text_Chinese": "这里有几个数据集的例子", "text_Arabic": "وهنا لديك بضعة أمثلة على مجموعة من مجموعة البيانات.", "text_French": "Et ici vous avez quelques exemples de l'ensemble de l'ensemble de données.", "text_Japanese": "データセットのセットの例をいくつか示します.", "text_Russian": "И здесь у вас есть несколько примеров набора набора данных."}
{"text": "As you can see for instance here, we have ah in the first example, we have the borrowing batch cooking which is a multi word borrowing.\n", "text_Chinese": "正如你所看到的,在第一个例子中,我们有借用批次烹饪,这是一个多词借用.", "text_Arabic": "كما ترون هنا على سبيل المثال, لدينا ah في المثال الأول, لدينا الطهي بالمجموعة المستعارة وهو استعارة متعددة الكلمات.", "text_French": "Comme vous pouvez le voir ici, nous avons ah dans le premier exemple, nous avons l'emprunt de cuisson en lots qui est un emprunt de plusieurs mots.", "text_Japanese": "例えば最初の例では多語借用である借用バッチクッキングがあります", "text_Russian": "Как вы можете видеть, например, здесь, у нас есть ah в первом примере, у нас есть заимствование партии приготовления, которая является многословным заимствованием."}
{"text": "And we have annotated it using the BIO um encode.\n", "text_Chinese": "我们使用BIO um编码进行注释", "text_Arabic": "وقد قمنا بتدوينها باستخدام شفرة BIO um.", "text_French": "Et nous l'avons annoté en utilisant le codage BIO um.", "text_Japanese": "BIO umエンコードを使用して注釈しました.", "text_Russian": "И мы аннотировали его, используя BIO um кодирование."}
{"text": "So the BIO was used for words in Spanish so not for words that were not borrowed.\n", "text_Chinese": "所以BIO被用于西班牙语的单词, 而不是未借用的单词.", "text_Arabic": "لذا تم استخدام BIO للكلمات باللغة الإسبانية وليس للكلمات التي لم يتم استعارتها.", "text_French": "Donc le BIO a été utilisé pour les mots en espagnol donc pas pour les mots qui n'ont pas été empruntés.", "text_Japanese": "BIOはスペイン語の単語に使われました 借用されていない単語には使われませんでした", "text_Russian": "Итак, BIO использовался для слов на испанском языке, а не для слов, которые не были заимствованы."}
{"text": "And here in this second example, you have benching and crash which are also labeled as borrowings from English.\n", "text_Chinese": "在这个第二个例子中, 还有benching和crash, 它们也被标记为从英语借来的.", "text_Arabic": "وهنا في هذا المثال الثاني, لديك \"بنشينغ\" و \"كراش\" والتي تم تصنيفها أيضا على أنها اقتراضات من اللغة الإنجليزية.", "text_French": "Et ici dans ce deuxième exemple, vous avez benching et crash qui sont également étiquetés comme emprunts de l'anglais.", "text_Japanese": "そしてこの2つ目の例ではベンチングとクラッシュも英語から借用されたと表示されています", "text_Russian": "И здесь, во втором примере, у вас есть \"benching\" и \"crash\", которые также помечены как заимствования из английского."}
{"text": "So, once we had the dataset, we explored several models for the task of extracting and detecting these lexical borrowings.\n", "text_Chinese": "所以, 一旦我们有了数据集, 我们就探索了几个模型来提取和检测这些词汇借用", "text_Arabic": "لذا, بمجرد أن حصلنا على مجموعة البيانات, استكشفنا عدة نماذج لمهمة استخراج واكتشاف هذه الاقتراضات المفردية.", "text_French": "Donc, une fois que nous avons eu l'ensemble de données, nous avons exploré plusieurs modèles pour l'extraction et la détection de ces emprunts lexicales.", "text_Japanese": "データセットを手に入れた後これらの語借用を抽出し検出するためのいくつかのモデルを探索しました", "text_Russian": "Итак, когда у нас был набор данных, мы исследовали несколько моделей для извлечения и обнаружения этих лексических заимствований."}
{"text": "The first one that we tried was the conditional random field model.\n", "text_Chinese": "我们首先尝试的是条件随机场模型.", "text_Arabic": "أول واحد جربناه كان نموذج المجال العشوائي المشروط.", "text_French": "Le premier que nous avons essayé était le modèle de champ aléatoire conditionnel.", "text_Japanese": "条件付きランダムフィールドモデルです 条件付きランダムフィールドモデルです", "text_Russian": "Первым, что мы попробовали, была условная случайная модель поля."}
{"text": "Ah, this was the model that had been used on previous work.\n", "text_Chinese": "这是以前工作中使用的模型.", "text_Arabic": "آه، هذا كان النموذج الذي تم استخدامه في العمل السابق.", "text_French": "C'était le modèle utilisé dans les travaux précédents.", "text_Japanese": "これは以前の作品で使われていたモデルです", "text_Russian": "Это была модель, которая использовалась в предыдущих работах."}
{"text": "And we used the same handcrafted features from that from those from that work.\n", "text_Chinese": "我们使用了来自那件作品的手工制作的相同特征", "text_Arabic": "واستخدمنا نفس الميزات المصنوعة يدويًا من تلك من تلك الأعمال.", "text_French": "Et nous avons utilisé les mêmes caractéristiques artisanales de celles de cette œuvre.", "text_Japanese": "そしてその作品の手作りの特徴と同じものを使用しました", "text_Russian": "И мы использовали те же самые ручные элементы, что и в этой работе."}
{"text": "As you can see, these are the features.\n", "text_Chinese": "正如你所看到的,这些是特点.", "text_Arabic": "كما ترون، هذه هي الميزات.", "text_French": "Comme vous pouvez le voir, ce sont les caractéristiques.", "text_Japanese": "ご覧の通りこれらは特徴です", "text_Russian": "Как вы видите, вот особенности."}
{"text": "These are binary features such as the word or the token in upper case?\n", "text_Chinese": "这些是二进制特征,如大写的单词或令牌?", "text_Arabic": "هذه هي الميزات الثنائية مثل الكلمة أو الرمز في الحروف الكبيرة؟", "text_French": "Ce sont des caractéristiques binaires telles que le mot ou le jeton en majuscules?", "text_Japanese": "これは単語やトークンなどの二進法の特徴です", "text_Russian": "Это двоичные функции, такие как слово или жетон в больших буквах?"}
{"text": "Is it title titlecase?\n", "text_Chinese": "这是标题标题案件吗?", "text_Arabic": "هل هي قضية الملكية؟", "text_French": "C'est une affaire de titre?", "text_Japanese": "タイトル・タイトルの訴訟ですか?", "text_Russian": "Это дело о титуле?"}
{"text": "Is it a quotation mark?\n", "text_Chinese": "这是一个引号吗?", "text_Arabic": "هل هي علامة اقتباس؟", "text_French": "C'est une guillotine?", "text_Japanese": "引用符ですか?", "text_Russian": "Это в кавычках?"}
{"text": "Things like that, which are the type of features that one would expect in a named entity recognition task.\n", "text_Chinese": "这类东西是人们在命名实体识别任务中期望的功能类型.", "text_Arabic": "أشياء مثل ذلك، والتي هي نوع الميزات التي يتوقعها المرء في مهمة التعرف على الكيان المسمى.", "text_French": "Des choses comme ça, qui sont le type de caractéristiques que l'on s'attendrait dans une tâche de reconnaissance d'entité nommée.", "text_Japanese": "名前付きのエンティティの認識タスクで期待できる機能です.", "text_Russian": "Такие вещи, которые являются типом функций, которые можно ожидать в задаче распознавания названных сущностей."}
{"text": "These are the results that we got.\n", "text_Chinese": "这是我们得到的结果", "text_Arabic": "هذه هي النتائج التي حصلنا عليها.", "text_French": "Voici les résultats que nous avons obtenus.", "text_Japanese": "これが得られた結果です", "text_Russian": "Вот результаты, которые мы получили."}
{"text": "We obtain fifty five F1 score using the the CRF model with handcrafted features.\n", "text_Chinese": "我们使用手工制作的CRF模型获得55F1分数.", "text_Arabic": "نحصل على خمسة وخمسين درجة F1 باستخدام نموذج CRF مع ميزات مصنوعة يدويًا.", "text_French": "Nous obtenons cinquante-cinq points F1 en utilisant le modèle CRF avec des caractéristiques artisanales.", "text_Japanese": "手作りの特徴を持つCRFモデルを使用して55F1スコアを取得します.", "text_Russian": "Мы получаем пятьдесят пять баллов F1, используя модель CRF с ручными функциями."}
{"text": "Which is a huge different difference um compared to the reported F1 score of eighty six, which was the result obtained with the same CRF model, same features but on a different dataset also for Spanish lexical borrowing detection.\n", "text_Chinese": "与报告的 F1 分数 86 相比,这是用相同的 CRF 模型获得的结果,相同的特征,但是在不同的数据集上,也用于西班牙语词汇借用检测.", "text_Arabic": "وهو فرق كبير مقارنة بنتيجة F1 المبلغ عنها التي تبلغ 86، والتي كانت النتيجة التي تم الحصول عليها بنفس نموذج CRF، نفس الميزات ولكن على مجموعة بيانات مختلفة أيضا للكشف عن الاقتراض الاسباني.", "text_French": "Ce qui est une différence énorme par rapport au score F1 de 86 rapporté, qui était le résultat obtenu avec le même modèle CRF, les mêmes caractéristiques mais sur un ensemble de données différent également pour la détection des emprunts lexicales espagnols.", "text_Japanese": "同じCRFモデルで得られた結果です 同じ特徴ですが スペイン語の語借用検出にも異なるデータセットです", "text_Russian": "Это огромная разница по сравнению с оценой F1, которая равнялась 86 баллам, полученной с помощью той же модели CRF, с теми же характеристиками, но с другим набором данных, также для обнаружения испанских лексических заимствований."}
{"text": "So, this proves that the dataset that we created is more difficult and that we needed to explore more sophisticated models for these tasks.\n", "text_Chinese": "这证明了我们创建的数据集更困难, 我们需要探索更复杂的模型来完成这些任务", "text_Arabic": "لذلك, هذا يثبت أن مجموعة البيانات التي أنشأناها أكثر صعوبة وأننا بحاجة لاستكشاف نماذج أكثر تطورا لهذه المهام.", "text_French": "Cela prouve que l'ensemble de données que nous avons créé est plus difficile et que nous devions explorer des modèles plus sophistiqués pour ces tâches.", "text_Japanese": "これは我々が作成したデータセットが より困難であり これらのタスクのために より洗練されたモデルを 探求する必要があることを証明しています", "text_Russian": "Это доказывает, что созданный нами набор данных сложнее и что нам нужно было исследовать более сложные модели для этих задач."}
{"text": "So, we tested two transformer based models.\n", "text_Chinese": "所以, 我们测试了两个基于变压器的模型", "text_Arabic": "لذا, اختبرنا نموذجين قائمين على المحولات.", "text_French": "Nous avons testé deux modèles basés sur des transformateurs.", "text_Japanese": "2つのトランスフォーマーベースのモデルをテストしました", "text_Russian": "Мы протестировали две модели, основанные на трансформаторах."}
{"text": "We used BETO which is a monolingual BERT model trained for Spanish and also multilingual BERT.\n", "text_Chinese": "我们使用了 BETO, 这是一种单语的 BERT 模型, 训练为西班牙语, 还有多语的 BERT", "text_Arabic": "استخدمنا BETO وهو نموذج BERT أحادي اللغة مدرب على اللغة الإسبانية وأيضا BERT متعدد اللغات.", "text_French": "Nous avons utilisé BETO qui est un modèle BERT monolingue formé pour l'espagnol et aussi BERT multilingue.", "text_Japanese": "BETOはスペイン語に訓練された 単語のBERTモデルであり 多言語のBERTモデルでもあります", "text_Russian": "Мы использовали BETO, который является одноязычной моделью BERT, обученной для испанского языка, а также многоязычной BERT."}
{"text": "Both models we use them through the transformers library by HuggingFace.\n", "text_Chinese": "这两种模型都通过HuggingFace的变压器库使用.", "text_Arabic": "كلا النماذج نستخدمها من خلال مكتبة التحويل من قبل HuggingFace.", "text_French": "Les deux modèles sont utilisés dans la bibliothèque de transformateurs de HuggingFace.", "text_Japanese": "ハギングフェイスによるトランスフォーマーライブラリを通じて使用します.", "text_Russian": "Обе модели мы используем через библиотеку трансформаторов HuggingFace."}
{"text": "These are the results that we got.\n", "text_Chinese": "这是我们得到的结果", "text_Arabic": "هذه هي النتائج التي حصلنا عليها.", "text_French": "Voici les résultats que nous avons obtenus.", "text_Japanese": "これが得られた結果です", "text_Russian": "Вот результаты, которые мы получили."}
{"text": "As you can see, multilingual BERT performs better than BETO both on the development set and on the test set and across all metrics.\n", "text_Chinese": "正如你所看到的,多语言的BERT在开发集和测试集以及所有指标上都比BETO表现更好.", "text_Arabic": "كما ترون، يعمل BERT متعدد اللغات بشكل أفضل من BETO على كل من مجموعة التطوير ومجموعة الاختبار وعلى جميع المقاييس.", "text_French": "Comme vous pouvez le voir, le BERT multilingue fonctionne mieux que BETO à la fois sur le jeu de développement et sur le jeu de test et sur toutes les mesures.", "text_Japanese": "開発セットとテストセットの両方で BETOよりも優れていることがわかります", "text_Russian": "Как вы можете видеть, многоязычный BERT работает лучше, чем BETO как на наборе разработки, так и на наборе тестов и по всем показателям."}
{"text": "Just so we have ah an idea to compare, the CRF model obtained an eighty two.\n", "text_Chinese": "为了让我们有比较的想法,CRF模型获得了82分.", "text_Arabic": "فقط حتى يكون لدينا فكرة للمقارنة، حصل نموذج CRF على 82.", "text_French": "Juste pour que nous ayons une idée à comparer, le modèle CRF a obtenu un 82.", "text_Japanese": "CRFモデルは82を得ました.", "text_Russian": "Просто чтобы мы имели идею для сравнения, модель CRF получила восемьдесят два."}
{"text": "The CRF model obtained a fifty five obtained a fifty five F1 score, whereas the multilingual BERT obtained eighty two, which is a big difference.\n", "text_Chinese": "CRF 模型获得 55 个 F1 得分, 而多语种 BERT 获得了 82 个, 这是一大区别", "text_Arabic": "حصل نموذج CRF على خمسة وخمسين حصل على خمسة وخمسين درجة F1، في حين حصل BERT متعدد اللغات على اثنين وثمانين، وهو فرق كبير.", "text_French": "Le modèle CRF a obtenu un score de cinquante-cinq F1, tandis que le modèle BERT multilingue a obtenu quatre-vingt-deux, ce qui est une grande différence.", "text_Japanese": "CRFモデルは55のF1スコアを得ましたが多言語BERTは82のスコアを得ましたこれは大きな違いです", "text_Russian": "Модель CRF получила пятьдесят пять баллов F1, в то время как многоязычная BERT получила восемьдесят два, что является большой разницей."}
{"text": "So, once that we had those results, we asked ourselves another question which is, could we find a BiLSTM-CRF model, feed it with different types of embeddings, embeddings that encode different types of linguistic information and perform outperform the results obtained by transformer based models?\n", "text_Chinese": "所以, 一旦我们有了这些结果, 我们又问自己另一个问题, 那就是, 我们能找到一个 BiLSTM-CRF 模型, 用不同类型的嵌入, 嵌入来编码不同类型的语言信息, 并且表现出超出基于变压器的模型所获得的结果吗?", "text_Arabic": "لذا, بمجرد أن حصلنا على هذه النتائج, سألنا أنفسنا سؤال آخر وهو, هل يمكننا العثور على نموذج BiLSTM-CRF, وإطعامه بأنواع مختلفة من التضمينات, التضمينات التي ترميز أنواع مختلفة من المعلومات اللغوية وتفوق النتائج التي تم الحصول عليها من قبل النماذج القائمة على المحولات?", "text_French": "Donc, une fois que nous avons eu ces résultats, nous nous sommes posé une autre question qui est, pourrions-nous trouver un modèle BiLSTM-CRF, le nourrir avec différents types d'incorporations, des incorporations qui encodent différents types d'informations linguistiques et qui surpassent les résultats obtenus par les modèles basés sur des transformateurs?", "text_Japanese": "すると結果が得られたら私たちは別の質問を自分にしましたそれはBiLSTM-CRFモデルを見つけ異なるタイプの埋め込みを入し異なるタイプの言語情報をエンコードし変換器ベースのモデルで得られた結果を上回る性能を備えた埋め込みを入することができますか?", "text_Russian": "Итак, получив результаты, мы задались другим вопросом: можем ли мы найти модель BiLSTM-CRF, накормить её различными типами встраиваний, встраиваний, которые кодируют различные типы лингвистической информации и превосходят результаты, полученные на базе трансформаторных моделей?"}
{"text": "So in order to do so, we ran some preliminary experiments, we we run this by BiLSTM-CRF model using flare library.\n", "text_Chinese": "为了做到这一点,我们进行了一些初步实验,我们通过使用闪光库的 BiLSTM-CRF 模型来运行它", "text_Arabic": "لذلك من أجل القيام بذلك، قمنا ببعض التجارب الأولية، قمنا بتشغيل هذا من خلال نموذج BiLSTM-CRF باستخدام مكتبة الوميض.", "text_French": "Pour ce faire, nous avons fait quelques expériences préliminaires, nous avons utilisé le modèle BiLSTM-CRF en utilisant la bibliothèque de flare.", "text_Japanese": "BiLSTM-CRFモデルで フレアライブラリを使用して実行しました", "text_Russian": "Для этого мы провели предварительные эксперименты, мы запустили модель BiLSTM-CRF, используя библиотеку флеер."}
{"text": "And we tried experimented with different type of embeddings like transformer-based but also fast-text, character embeddings, and so on.\n", "text_Chinese": "我们尝试了不同类型的嵌入,比如基于变压器的,但也包括快文本,字符嵌入等等.", "text_Arabic": "وحاولنا تجربة أنواع مختلفة من التضمينات مثل التحويل القائم ولكن أيضا النص السريع, التضمينات الشخصية, وهلم جرا.", "text_French": "Et nous avons essayé d'expérimenter avec différents types d'incorporations comme basées sur des transformateurs mais aussi sur du texte rapide, des incorporations de caractères, et ainsi de suite.", "text_Japanese": "変換器ベースの組み込みや ファストテキストキャラクター組み込みなどを試みました", "text_Russian": "Мы экспериментировали с различными типами встраивания, как на основе трансформаторов, но также с быстрым текстом, встраиванием символов и так далее."}
{"text": "What we found out was that transformer-based embeddings performed better than non contextualized embeddings, that the combination of English BERT and Spanish BETO embeddings outperform multilingual BERT embeddings.\n", "text_Chinese": "我们发现, 基于变压器的嵌入比非上下文化的嵌入更好, 英语 BERT 和西班牙语 BETO 嵌入的组合优于多语言的 BERT 嵌入", "text_Arabic": "ما وجدناه هو أن التضمينات القائمة على المحول كانت أفضل من التضمينات غير السياقية, أن مزيج من التضمينات الإنجليزية BERT والإسبانية BETO تفوق التضمينات متعددة اللغات BERT.", "text_French": "Nous avons découvert que les intégrations basées sur transformateur fonctionnaient mieux que les intégrations non contextualisées, que la combinaison des intégrations BERT anglaises et BETO espagnoles surpassait les intégrations BERT multilingues.", "text_Japanese": "英語のBERTとスペイン語のBETOの組み合わせが 多言語のBERTの組み合わせを上回るということです", "text_Russian": "Мы обнаружили, что встраивания на основе трансформаторов работают лучше, чем неконтекстуализированные встраивания, что комбинация английских встраиваний BERT и испанских встраиваний BETO превосходит многоязычные встраивания BERT."}
{"text": "And that BPE embeddings produced better F1 and character embeddings produce better recall.\n", "text_Chinese": "而且, BPE 嵌入产生了更好的 F1, 而字符嵌入产生了更好的回忆", "text_Arabic": "وأن تضمينات BPE تنتج F1 أفضل وتضمينات الأحرف تنتج تذكرا أفضل.", "text_French": "Et que les intégrations de BPE produisaient de meilleurs F1 et les intégrations de caractères produisaient de meilleurs souvenirs.", "text_Japanese": "BPE埋め込みはより良いF1を生み出し 文字埋め込みはより良い記憶を生み出します", "text_Russian": "И что встраивание в БПЭ производит лучшую F1, а встраивание в символы производит лучшую память."}
{"text": "With that in mind, these were the best performing results that we got.\n", "text_Chinese": "考虑到这一点, 这些是我们得到的最佳表现结果", "text_Arabic": "مع وضع ذلك في الاعتبار, كانت هذه أفضل النتائج التي حصلنا عليها.", "text_French": "Dans cet esprit, ce sont les meilleurs résultats que nous ayons obtenus.", "text_Japanese": "これらを念頭に置いてこれらは私たちが得た最高のパフォーマンス結果でした", "text_Russian": "Учитывая это, это были лучшие результаты, которые мы получили."}
{"text": "Both models were BiLSTM-CRF models using flare.\n", "text_Chinese": "这两种模型都是使用闪光的BiLSTM-CRF模型.", "text_Arabic": "كلا النماذج كانت نماذج BiLSTM-CRF باستخدام الاندلاع.", "text_French": "Les deux modèles étaient des modèles BiLSTM-CRF utilisant des éclairs.", "text_Japanese": "両方のモデルも,フレアを使用したBiLSTM-CRFモデルでした.", "text_Russian": "Обе модели были BiLSTM-CRF моделями с использованием факела."}
{"text": "One was fed with BETO and BERT embeddings and BPE, and the other one BETO and BERT embeddings and BPE and also character embeddings.\n", "text_Chinese": "一个是BETO和BERT嵌入和BPE,另一个是BETO和BERT嵌入和BPE,还有字符嵌入.", "text_Arabic": "تم تغذية واحد مع تضمينات BETO و BERT و BPE، والآخر تضمينات BETO و BERT و BPE وأيضا تضمينات الأحرف.", "text_French": "L'un a été alimenté avec des intégrations BETO et BERT et BPE, et l'autre avec des intégrations BETO et BERT et BPE et également des intégrations de caractères.", "text_Japanese": "もう一つはBETOとBERTの埋め込みとBPEと 別のBETOとBERTの埋め込みとBPEと 文字埋め込みも", "text_Russian": "Один был насыщен встраиваниями BETO и BERT и BPE, а другой встраиваниями BETO и BERT и BPE, а также встраиваниями символов."}
{"text": "This last one was the one that produced the highest F1 score on the test set, although the highest score on the development set was obtained by the one without character embeddings.\n", "text_Chinese": "最后一个是测试集上F1得分最高的,尽管在开发集上得分最高的是没有字符嵌入的.", "text_Arabic": "كان هذا الأخير هو الذي أنتج أعلى درجة F1 في مجموعة الاختبار ، على الرغم من أن أعلى درجة في مجموعة التطوير تم الحصول عليها من قبل واحد بدون تضمينات الشخصيات.", "text_French": "Ce dernier a été celui qui a produit le score F1 le plus élevé sur l'ensemble de tests, bien que le score le plus élevé sur l'ensemble de développement ait été obtenu par celui sans incrustations de caractères.", "text_Japanese": "この最後のものは,テストセットで最高F1スコアを生み出したものでしたが,開発セットで最高スコアは,キャラクター埋め込みのないものによって得られました.", "text_Russian": "Последний был тем, который дал самый высокий балл F1 на тестовом наборе, хотя самый высокий балл на наборе разработки был получен тем, который не имел встроенных символов."}
{"text": "Just ah to bear in mind that the best result that we got with multilingual BERT obtained an F1 of seventy six on the development set and eighty two on the test set.\n", "text_Chinese": "记住,我们用多语言BERT获得的最佳结果是开发集的F1为76和测试集的82.", "text_Arabic": "فقط لأضع في اعتبارنا أن أفضل نتيجة حصلنا عليها مع BERT متعددة اللغات حصلت على F1 من ستة وسبعين على مجموعة التطوير واثمانين على مجموعة الاختبار.", "text_French": "Juste pour garder à l'esprit que le meilleur résultat que nous avons obtenu avec le BERT multilingue a obtenu un F1 de soixante-seize sur le jeu de développement et quatre-vingt-deux sur le jeu de test.", "text_Japanese": "多言語BERTで得た最高の結果は 開発セットで76のF1と テストセットで82のF1でした", "text_Russian": "Просто имейте в виду, что лучший результат, который мы получили с многоязычным BERT, получил F1 в семьдесят шесть на наборе разработки и восемьдесят два на наборе тестирования."}
{"text": "So this is an improvement compared to those results.\n", "text_Chinese": "这与那些结果相比是进步", "text_Arabic": "لذا فهذا تحسن مقارنةً بتلك النتائج.", "text_French": "C'est donc une amélioration par rapport à ces résultats.", "text_Japanese": "これはそれらの結果と比較して改善です", "text_Russian": "Это улучшение по сравнению с этими результатами."}
{"text": "Finally, we asked ourselves another question which was can lexical borrowing detection be framed as transfer learning from language identification in code switching?\n", "text_Chinese": "最后,我们问自己另一个问题,就是词汇借用检测是否可以被视为从代码切换中的语言识别转移学习?", "text_Arabic": "وأخيرا, سألنا أنفسنا سؤال آخر وهو هل يمكن تحديد الاقتراض اللفظي أن يكون بمثابة تعلم نقل من تحديد اللغة في تبديل الشفرة?", "text_French": "Enfin, nous nous sommes posé une autre question : la détection des emprunts lexicales peut-elle être encadrée comme un apprentissage de transfert à partir de l'identification linguistique dans le changement de code?", "text_Japanese": "最後に私たちは別の質問をしました語借用検出はコード切り替えにおける言語識別から学習を転送することとして組み込むことができるか", "text_Russian": "Наконец, мы задали себе другой вопрос: можно ли обнаружение лексического заимствования рассматривать как перенос обучения от идентификации языка в переключении кодов?"}
{"text": "So, we run the same BiLSTM-CRF model that we had run using flare, but instead of using these unadapted transformer-based BETO and BERT embeddings, we used code switch embeddings.\n", "text_Chinese": "所以, 我们运行了相同的 BiLSTM-CRF 模型, 我们使用了flare, 但不是使用这些不适应的基于变压器的 BETO 和 BERT 嵌入, 我们使用了代码交换器嵌入", "text_Arabic": "لذلك، قمنا بتشغيل نفس نموذج BiLSTM-CRF الذي قمنا بتشغيله باستخدام الفلاير، ولكن بدلاً من استخدام هذه التضمينات BETO و BERT القائمة على المحولات غير المكيفة، استخدمنا تضمينات تبديل الشفرة.", "text_French": "Donc, nous avons exécuté le même modèle BiLSTM-CRF que nous avions exécuté en utilisant flare, mais au lieu d'utiliser ces intégrations BETO et BERT basées sur transformateur non adaptées, nous avons utilisé des intégrations de commutateur de code.", "text_Japanese": "変換器ベースの BETOとBERTの埋め込みを使う代わりに コードスイッチの埋め込みを使いました", "text_Russian": "Итак, мы запустили ту же модель BiLSTM-CRF, что и с помощью flare, но вместо использования этих не адаптированных встроенных BETO и BERT на основе трансформаторов мы использовали встроенные переключатели кода."}
{"text": "What are code switch embeddings?\n", "text_Chinese": "代码交换器嵌入是什么?", "text_Arabic": "ما هي تدمج مفاتيح التبديل؟", "text_French": "Quelles sont les intégrations de commutateur de code?", "text_Japanese": "コードスイッチの埋め込みとは何ですか?", "text_Russian": "Что такое встраивание переключателя кода?"}
{"text": "Well these are um embeddings that are have been fine tuned transformer-based embeddings that have been pretrained for language identification on the Spanish English section of the LinCE code switching dataset.\n", "text_Chinese": "这些是经过微调的基于变压器的嵌入, 已经预训练在LINCE代码切换数据集的西班牙语-英语部分进行语言识别.", "text_Arabic": "حسنا هذه هي um تضمينات التي تم ضبطها بدقة تضمينات قائمة على المحول التي تم تدريبها مسبقا لتحديد اللغة على القسم الإسباني الإنجليزي من مجموعة بيانات تبديل شفرة LinCE.", "text_French": "Eh bien, ce sont des embeddings qui ont été ajustés à base de transformateurs qui ont été préentraînés pour l'identification de la langue sur la section espagnole-anglais de l'ensemble de données de commutation de code LinCE.", "text_Japanese": "LinCEコード切り替えデータセットのスペイン語・英語セクションで 言語識別のために予備訓練された 変換器ベースの埋め込みです", "text_Russian": "Это встраивания, которые были тонко настроены на основе трансформаторов, которые были предварительно обучены для идентификации языка в испанском-английском разделе набора данных переключения кода LinCE."}
{"text": "LinCE is a dataset on code switching that has a section on Spanish English, Spanish English code switching.\n", "text_Chinese": "LinCE 是一个关于代码切换的数据集,其中有一个关于西班牙语英语,西班牙语英语代码切换的部分.", "text_Arabic": "لينسي هو مجموعة بيانات حول تبديل الشفرة التي تحتوي على قسم حول تبديل الشفرة الإسبانية الإنجليزية والإسبانية الإنجليزية.", "text_French": "LinCE est un ensemble de données sur la commutation de code qui a une section sur l'anglais espagnol, la commutation de code espagnol anglais.", "text_Japanese": "LinCEはコード切り替えに関するデータセットで,スペイン語・英語,スペイン語・英語のコード切り替えに関するセクションがあります.", "text_Russian": "LinCE - это набор данных о переключении кодов, который имеет раздел о переключении кодов с испанского на английский, с испанского на английский."}
{"text": "So we fed our BiLSTM-CRF with code switch embeddings and optionally character embeddings, BPE embeddings and so on.\n", "text_Chinese": "所以我们用代码交换器嵌入和可选的字符嵌入,BPE嵌入等等来喂养我们的BiLSTM-CRF.", "text_Arabic": "لذلك قمنا بتغذية BiLSTM-CRF مع تضمينات تبديل الشفرة وتضمينات الأحرف الاختيارية، وتضمينات BPE وهلم جرا.", "text_French": "Nous avons donc alimenté notre BiLSTM-CRF avec des intégrations de commutateur de code et éventuellement des intégrations de caractères, des intégrations BPE et ainsi de suite.", "text_Japanese": "コードスイッチの埋め込みと 選択的に文字埋め込み BPE埋め込みなどを 埋め込みました", "text_Russian": "Поэтому мы накормили наш BiLSTM-CRF встраиваниями переключателей кодов и, по желанию, встраиваниями символов, встраиваниями BPE и так далее."}
{"text": "The best result that we got was eighty four point twenty two, which is the highest across all the models that we tried on the test set.\n", "text_Chinese": "我们得到的最佳结果是八十四点二十二, 这是我们测试的所有模型中最高的", "text_Arabic": "أفضل نتيجة حصلنا عليها كانت 84.22, وهي أعلى نتيجة بين جميع النماذج التي جربناها في مجموعة الاختبار.", "text_French": "Le meilleur résultat que nous avons obtenu était de 84,22, ce qui est le plus élevé de tous les modèles que nous avons essayés sur le jeu de test.", "text_Japanese": "最高の結果は84点22でした これはテストセットで試したすべてのモデルで最も高い結果です", "text_Russian": "Лучший результат, который мы получили, был 84,22, что является самым высоким среди всех моделей, которые мы попробовали на тестовом наборе."}
{"text": "Although the best result F1 score that we got on the development set, which was seventy nine, was lower than the best result obtained by the BiLSTM-CRF fed with unadapted embeddings.\n", "text_Chinese": "虽然我们在开发集上获得的最佳结果F1分数是79分, 但低于使用未适应的嵌入式的BiLSTM-CRF获得的最佳结果.", "text_Arabic": "على الرغم من أن أفضل نتيجة F1 التي حصلنا عليها على مجموعة التطوير ، والتي كانت تسعة وسبعين ، كانت أقل من أفضل نتيجة تم الحصول عليها من قبل BiLSTM-CRF التي تم تغذيتها مع التضمينات غير المكيفة.", "text_French": "Bien que le meilleur résultat F1 que nous avons obtenu sur le jeu de développement, qui était de soixante-dix-neuf, était inférieur au meilleur résultat obtenu par le BiLSTM-CRF alimenté avec des intégrations non adaptées.", "text_Japanese": "開発セットで得た最高の結果F1スコアは79でしたが,適応されていない埋め込みで供給されたBiLSTM-CRFで得られた最高の結果よりも低かったです.", "text_Russian": "Хотя лучший результат F1, который мы получили на наборе разработки, который был семьдесят девять, был ниже, чем лучший результат, полученный BiLSTM-CRF, кормленный неадаптированными встраиваниями."}
{"text": "So, some conclusions from our work.\n", "text_Chinese": "所以, 我们的工作得出了一些结论", "text_Arabic": "إذا, بعض الاستنتاجات من عملنا.", "text_French": "Donc, quelques conclusions de notre travail.", "text_Japanese": "結果をいくつか紹介します", "text_Russian": "Итак, некоторые выводы из нашей работы."}
{"text": "We have ah we have produced a new dataset of Spanish newswire that is annotated with unassimilated lexical borrowings.\n", "text_Chinese": "我们已经制作了一套新的西班牙新闻新闻数据集, 并用未同化词汇借用来注释", "text_Arabic": "لقد قمنا بإنتاج مجموعة بيانات جديدة من الأخبار الإسبانية التي تم تعليقها مع الاقتراضات المفردية غير المتماثلة.", "text_French": "Nous avons produit un nouvel ensemble de données de nouvelles espagnoles annotées avec des emprunts lexicales non assimilés.", "text_Japanese": "スペイン語のニュースワイヤーの新しいデータセットを生成しました それは同化されていない語の借用で注釈されています", "text_Russian": "Мы создали новый набор данных испанских новостей, который аннотирован не ассимилированными лексическими заимствованиями."}
{"text": "This dataset is more borrowing dense and OOV-rich than previous resources.\n", "text_Chinese": "这种数据集比以前的资源更密集,更丰富.", "text_Arabic": "هذه مجموعة البيانات أكثر كثافة الاقتراض وغنية بالOOV من الموارد السابقة.", "text_French": "Cet ensemble de données est plus dense en emprunts et plus riche en OOV que les ressources précédentes.", "text_Japanese": "このデータセットは,以前のリソースよりも借り入れが密集し,OOVが豊富です.", "text_Russian": "Этот набор данных более насыщен заимствованиями и богат OOV, чем предыдущие ресурсы."}
{"text": "We have explored four types of models for lexical borrowing detection.\n", "text_Chinese": "我们探索了四种类型的词汇借用检测模型.", "text_Arabic": "لقد استكشفنا أربعة أنواع من النماذج للكشف عن الاقتراض اللفظي.", "text_French": "Nous avons exploré quatre types de modèles pour la détection des emprunts lexicaux.", "text_Japanese": "語借用の検出のための4つのタイプのモデルを検討しました.", "text_Russian": "Мы исследовали четыре типа моделей для обнаружения лексического заимствования."}
{"text": "Um. In terms of error analysis, well, recall was a weak point for all models.\n", "text_Chinese": "在错误分析方面, 回忆是所有模型的一个弱点.", "text_Arabic": "من حيث تحليل الأخطاء, حسنا, كان التذكير نقطة ضعيفة لجميع النماذج.", "text_French": "En termes d'analyse d'erreur, le rappel était un point faible pour tous les modèles.", "text_Japanese": "エラー分析の面では 記憶はすべてのモデルにとって 弱点でした", "text_Russian": "Что касается анализа ошибок, то вспоминание было слабым местом для всех моделей."}
{"text": "Ah, as you can see here, some frequent false negatives include uppercase borrowings, words that exist in both English and Spanish, for instance.\n", "text_Chinese": "啊, 正如你所看到的, 一些常见的假负数包括大写借用, 例如, 既有英语和西班牙语的词", "text_Arabic": "آه, كما ترون هنا, بعض السلبيات الزائفة الشائعة تتضمن اقتراضات من حروف كبيرة, كلمات موجودة في كل من الإنجليزية والإسبانية, على سبيل المثال.", "text_French": "Comme vous pouvez le voir ici, certains faux négatifs fréquents incluent des emprunts en majuscules, des mots qui existent en anglais et en espagnol, par exemple.", "text_Japanese": "ご覧の通りよくある偽否定には英語とスペイン語の両方に存在する単語である大文字の借用が含まれています", "text_Russian": "Как вы видите, некоторые частые ложные отрицательные результаты включают заимствования с заглавными буквами, слова, которые существуют как в английском, так и в испанском языках."}
{"text": "Also interestingly, BPE embeddings seem to improve F1 score.\n", "text_Chinese": "值得注意的是,BPE嵌入似乎可以提高F1分数.", "text_Arabic": "ومن المثير للاهتمام أيضًا أن تضمينات BPE تبدو أنها تحسن درجة F1.", "text_French": "Il est également intéressant de noter que les intégrations BPE semblent améliorer le score F1.", "text_Japanese": "また,興味深いことに,BPE埋め込みはF1スコアを改善するようです.", "text_Russian": "Также интересно, что встраивание BPE, кажется, улучшает оценку F1."}
{"text": "And character embedding seem to improve recall.\n", "text_Chinese": "嵌入字符似乎可以改善回忆.", "text_Arabic": "ويبدو أن تضمين الشخصيات يحسن التذكير.", "text_French": "Et l'intégration de caractères semble améliorer le souvenir.", "text_Japanese": "そしてキャラクターを埋め込むと 記憶が良くなるようです", "text_Russian": "И встраивание символов, кажется, улучшает запоминание."}
{"text": "Which ah it's an interesting finding that perhaps we can explore on future work.\n", "text_Chinese": "这是一项有趣的发现, 我们也许可以在未来的工作中探索", "text_Arabic": "وهو اكتشاف مثير للاهتمام ربما يمكننا استكشافه في أعمالنا المستقبلية.", "text_French": "C'est une découverte intéressante que nous pourrons peut-être explorer dans de futurs travaux.", "text_Japanese": "興味深い発見で 将来の研究で 探求できるかもしれません", "text_Russian": "Это интересное открытие, которое мы, возможно, сможем изучить в будущей работе."}
{"text": "Um. Well, this is everything that I have.\n", "text_Chinese": "嗯,这是我所拥有的一切", "text_Arabic": "حسناً، هذا كل ما أملكه.", "text_French": "Eh bien, c'est tout ce que j'ai.", "text_Japanese": "これは私が持っているすべてです", "text_Russian": "Ну, это всё, что у меня есть."}
{"text": "Thank you so much for listening.\n", "text_Chinese": "非常感谢您的聆听", "text_Arabic": "شكراً جزيلاً على الاستماع.", "text_French": "Merci beaucoup de m'écouter.", "text_Japanese": "聞いてくれてありがとう", "text_Russian": "Большое спасибо, что выслушали."}
{"text": "My name is Antoine.\n", "text_Chinese": "我的名字是安东尼.", "text_Arabic": "اسمي (أنطوان)", "text_French": "Je m'appelle Antoine.", "text_Japanese": "私の名前はアントワインです", "text_Russian": "Меня зовут Антуан."}
{"text": "I'm a PhD student at the University of Massachusetts Amherst.\n", "text_Chinese": "我是马萨诸塞大学阿默斯特分校的博士生", "text_Arabic": "أنا طالب دكتوراه في جامعة ماساتشوستس أمهيرست.", "text_French": "Je suis doctorant à l'Université du Massachusetts Amherst.", "text_Japanese": "私はマサチューセッツ・アムハースト大学で 博士課程を修了しています", "text_Russian": "Я аспирант в Университете Массачусетса Амхерст."}
{"text": "I am presenting our paper KinyaBERT: a Morphology-aware Kinyarwanda Language Model.\n", "text_Chinese": "我将介绍我们的论文 KinyaBERT:一个以形态为基础的 Kinyarwanda 语言模型", "text_Arabic": "أقدم ورقة KinyaBERT: نموذج لغة Kinyarwanda مدركًا للتركيب.", "text_French": "Je présente notre article KinyaBERT: un modèle de langue kinyarwanda conscient de la morphologie.", "text_Japanese": "KinyaBERT:形態学に意識したキニアルワンダ語モデルを発表します.", "text_Russian": "Я представляю нашу работу KinyaBERT: модель языка киньяруанда, осведомленная о морфологии."}
{"text": "Today, I'll talk about the motivation for this research.\n", "text_Chinese": "今天, 我将谈谈这项研究的动机", "text_Arabic": "اليوم, سأتكلم عن الدافع لهذا البحث.", "text_French": "Aujourd'hui, je vais vous parler de la motivation de cette recherche.", "text_Japanese": "今日は この研究の動機について話します", "text_Russian": "Сегодня я расскажу о мотивах этого исследования."}
{"text": "Then I'll present KinyaBERT model architecture in detail.\n", "text_Chinese": "然后我会详细介绍KinyaBERT模型架构.", "text_Arabic": "ثم سأقدم بنية نموذج KinyaBERT بالتفصيل.", "text_French": "Ensuite, je vais présenter l'architecture du modèle KinyaBERT en détail.", "text_Japanese": "KinyaBERTのモデルアーキテクチャを 詳細に紹介します", "text_Russian": "Затем я подробно представлю архитектуру модели KinyaBERT."}
{"text": "I'll then talk about our experimental results, then finish with some conclusions.\n", "text_Chinese": "然后我将谈谈我们的实验结果, 然后以一些结论结束", "text_Arabic": "ثم سأتكلم عن نتائج تجاربنا, ثم أنهي ببعض الاستنتاجات.", "text_French": "Je vais ensuite parler de nos résultats expérimentaux, puis terminer avec quelques conclusions.", "text_Japanese": "結果について話し 結論を出します", "text_Russian": "Затем я расскажу о наших экспериментальных результатах, а затем закончу с некоторыми выводами."}
{"text": "We all know that recent natural language processing advances have been made possible by the use of pretrained language models such as BERT.\n", "text_Chinese": "我们都知道, 最近自然语言处理的进步是通过使用预训练的语言模型, 比如 BERT", "text_Arabic": "كلنا نعلم أن التقدم الأخير في معالجة اللغة الطبيعية أصبح ممكنا باستخدام نماذج اللغة المدربة مسبقا مثل BERT.", "text_French": "Nous savons tous que les récentes avancées dans le traitement du langage naturel ont été rendues possibles par l'utilisation de modèles de langage préentraînés tels que BERT.", "text_Japanese": "BERTのような 予備訓練された言語モデルを使用することで 可能になったことが分かっています", "text_Russian": "Мы все знаем, что последние достижения в обработке естественного языка стали возможными благодаря использованию предварительно обученных языковых моделей, таких как BERT."}
{"text": "However, there are still a number of limitations.\n", "text_Chinese": "然而,仍然有一些限制.", "text_Arabic": "ومع ذلك، لا يزال هناك عدد من القيود.", "text_French": "Cependant, il existe encore un certain nombre de limitations.", "text_Japanese": "しかし,まだいくつかの制限があります.", "text_Russian": "Однако все еще существует ряд ограничений."}
{"text": "Due to the complex morphology that is expressed by most morphologically rich languages, the ubiquitous byte pair encoding tokenization algorithm that I used cannot extract the exact subword lexical units, meaning the morphemes, which are needed for effective representation.\n", "text_Chinese": "由于大多数形态丰富的语言表达的复杂形态, 我使用的无处不在的字节对编码标记算法无法提取确切的子词词汇单位, 即形态,", "text_Arabic": "بسبب الشكل المعقد الذي يتم التعبير عنه من قبل معظم اللغات الغنية من الناحية الشكلية، فإن خوارزمية ترميز زوج البايت المنتشرة في كل مكان التي استخدمتها لا يمكنها استخراج الوحدات اللفظية الدقيقة لللفظ الفرعي، أي المورفيمات، التي هي ضرورية للتمثيل الفعال.", "text_French": "En raison de la morphologie complexe exprimée par la plupart des langues morphologiquement riches, l'algorithme de codage de paires d'octets omniprésent que j'ai utilisé ne peut pas extraire les unités lexicales exactes des sous-mots, c'est-à-dire les morphèmes, qui sont nécessaires pour une représentation efficace.", "text_Japanese": "複雑な形態学が 表現されている言語の多くは 形態学的に豊富な言語です 使われているバイットペアエンコーディングの トークン化アルゴリズムは 効果的な表現に必要な 語単位を抽出できません", "text_Russian": "Из-за сложной морфологии, которая выражается большинством морфологически богатых языков, повсеместный алгоритм кодирования байтовых пар, который я использовал, не может извлечь точные подсловные лексические единицы, то есть морфемы, которые необходимы для эффективного представления."}
{"text": "For example, here we have three Kinyarwanda words that have several morphemes in them, but the BPE algorithms cannot extract them.\n", "text_Chinese": "例如, 这里有三个基尼亚鲁旺达语的词, 它们中包含了几个形态, 但 BPE 算法无法提取它们", "text_Arabic": "على سبيل المثال, هنا لدينا ثلاث كلمات كينيارواندا تحتوي على عدة أشكال, لكن خوارزميات BPE لا تستطيع استخراجها.", "text_French": "Par exemple, ici nous avons trois mots Kinyarwanda qui ont plusieurs morphèmes en eux, mais les algorithmes BPE ne peuvent pas les extraire.", "text_Japanese": "例えばここにはキニアルワンダ語の3つの単語がありそれらにはいくつかの形態があるがBPEアルゴリズムはそれらを抽出できません", "text_Russian": "Например, здесь у нас есть три слова киньяруанда, которые содержат несколько морфемов, но алгоритмы BPE не могут их извлечь."}
{"text": "This is because some morphological rules produce different surface forms that hide the exact lexical information, and BPE, which is solely based on the surface forms, does not have access to this lexical model.\n", "text_Chinese": "这是因为一些形态规则产生不同的表面形式,隐藏了确切的词汇信息,而仅基于表面形式的BPE无法访问这个词汇模型.", "text_Arabic": "ويرجع ذلك إلى أن بعض القواعد المورفولوجية تنتج أشكالًا سطحية مختلفة تخفي المعلومات اللغوية الدقيقة، والبي بي، التي تعتمد فقط على أشكال السطح، لا تملك إمكانية الوصول إلى هذا النموذج اللغوية.", "text_French": "C'est parce que certaines règles morphologiques produisent différentes formes de surface qui cachent l'information lexicale exacte, et BPE, qui est uniquement basée sur les formes de surface, n'a pas accès à ce modèle lexical.", "text_Japanese": "これは,いくつかの形態学的規則が正確な語情報を隠す異なる表層形状を生成するためであり,表面形状のみに基づくBPEは,この語モデルにアクセスできません.", "text_Russian": "Это связано с тем, что некоторые морфологические правила производят различные поверхностные формы, которые скрывают точную лексическую информацию, а BPE, которая основана исключительно на поверхностных формах, не имеет доступа к этой лексической модели."}
{"text": "The second challenge is that even if one had access to an oracle morphological analyzer, replacing BPE tokens with morphemes is not enough to express the morphological compositionality.\n", "text_Chinese": "第二个挑战是,即使有人可以使用一个神话形态分析器,用形态符号取代BPE符号也不足以表达形态组成性.", "text_Arabic": "التحدي الثاني هو أنه حتى لو كان لدى المرء إمكانية الوصول إلى محلل مورفولوجي أوراكل، فإن استبدال رموز BPE بالمورفيمات ليس كافياً للتعبير عن التكوين المورفولوجي.", "text_French": "Le deuxième défi est que même si l'on avait accès à un analyseur morphologique oracle, remplacer les jetons BPE par des morphèmes n'est pas suffisant pour exprimer la composition morphologique.", "text_Japanese": "2つ目の課題は,オラクル形態分析器にアクセスしても,BPEトークンを形態で置き換えることは,形態組成性を表現するのに十分ではないことです.", "text_Russian": "Вторая проблема заключается в том, что даже если бы у кого-то был доступ к морфологическому анализатору оракула, замены токенов BPE морфемами недостаточно для выражения морфологической композиционности."}
{"text": "A third gap in the research is that new pretrained language models are most often evaluated on high resource languages.\n", "text_Chinese": "研究中的第三个差距是,新的预训练语言模型最常被评估在高资源语言上.", "text_Arabic": "ثغرة ثالثة في البحث هي أن نماذج اللغة الجديدة المدربة مسبقاً يتم تقييمها في أغلب الأحيان على لغات عالية الموارد.", "text_French": "Une troisième lacune dans la recherche est que les nouveaux modèles linguistiques préentraînés sont le plus souvent évalués sur des langues à haute ressource.", "text_Japanese": "研究の3つ目の欠は,新しい予備訓練された言語モデルが,最も頻繁に高リソース言語で評価されることです.", "text_Russian": "Третий пробел в исследованиях заключается в том, что новые предварительно обученные языковые модели чаще всего оцениваются на языках с высоким уровнем ресурсов."}
{"text": "And we need to assess their applicability on low resources and diverse languages as well.\n", "text_Chinese": "我们还需要评估它们在低资源和多样化语言中的适用性", "text_Arabic": "ونحن بحاجة إلى تقييم قابلية تطبيقها على موارد منخفضة ولغات متنوعة أيضا.", "text_French": "Et nous devons évaluer leur applicabilité sur des ressources faibles et des langues diverses aussi bien.", "text_Japanese": "低リソースで 様々な言語でも 適用可能性を評価する必要があります", "text_Russian": "И нам нужно оценить их применимость и на низких ресурсах и различных языках."}
{"text": "Therefore, we present KinyaBERT, which is a simple but effective adaptation of the BERT architecture that is meant to more effectively handle morphologically rich languages.\n", "text_Chinese": "因此,我们介绍了KinyaBERT,这是BERT架构的简单但有效的适应,旨在更有效地处理形态丰富的语言.", "text_Arabic": "لذلك، نقدم KinyaBERT، وهو تكييف بسيط ولكن فعال لهيكل BERT الذي يهدف إلى التعامل بشكل أكثر فعالية مع اللغات الغنية مورفولوجيا.", "text_French": "Par conséquent, nous présentons KinyaBERT, qui est une adaptation simple mais efficace de l'architecture BERT qui est destinée à gérer plus efficacement les langues morphologiquement riches.", "text_Japanese": "したがって,KinyaBERTを紹介します. これはBERTアーキテクチャのシンプルで効果的な適応であり,形態学的に豊かな言語をより効果的に扱うことを目的としています.", "text_Russian": "Поэтому мы представляем KinyaBERT, который является простой, но эффективной адаптацией архитектуры BERT, которая предназначена для более эффективной обработки морфологически богатых языков."}
{"text": "We evaluate KinyaBERT on Kinyarwanda, a low resource morphologically rich language, which is spoken by more than twelve million people across Eastern and Central Africa.\n", "text_Chinese": "我们对基尼亚文达语进行了评估,", "text_Arabic": "نحن نقيّم KinyaBERT على Kinyarwanda، لغة غنية من الناحية المورفولوجية منخفضة الموارد، والتي يتحدث بها أكثر من اثني عشر مليون شخص في جميع أنحاء شرق ووسط أفريقيا.", "text_French": "Nous évaluons KinyaBERT sur le kinyarwanda, une langue riche en ressources morphologiques, parlée par plus de douze millions de personnes en Afrique de l'Est et du Centre.", "text_Japanese": "私たちはKinyaBERTをKinyarwandaで評価しました 低資源の形態学的に豊かな言語で 東アフリカと中央アフリカで1200万人以上が話しています", "text_Russian": "Мы оцениваем KinyaBERT на киньяруанде, морфологически богатом языке с низкими ресурсами, на котором говорят более двенадцати миллионов человек в Восточной и Центральной Африке."}
{"text": "The input to the model is either a sentence or a document.\n", "text_Chinese": "模型输入要么是句子,要么是文档.", "text_Arabic": "مدخل النموذج هو إما جملة أو وثيقة.", "text_French": "L'entrée du modèle est soit une phrase, soit un document.", "text_Japanese": "モデルの入力は文句か文書です.", "text_Russian": "Вход в модель представляет собой предложение или документ."}
{"text": "For example here, we have John twarahamubonye biradutangaza, which means we were surprised to find John there.\n", "text_Chinese": "例如, 这里有 John twarahamubonye biradutangaza, 这意味着我们很惊讶地发现 John 在那里", "text_Arabic": "على سبيل المثال هنا, لدينا جون twarahamubonye biradutangaza, مما يعني أننا فوجئنا بالعثور على جون هناك.", "text_French": "Par exemple, ici, nous avons John twarahamubonye biradutangaza, ce qui signifie que nous avons été surpris de trouver John là.", "text_Japanese": "例えばここにはジョン・タラハムブネ・ビラドゥータンガザがありそれはジョンがそこにいたことに驚いたという意味です", "text_Russian": "Например, здесь у нас есть Джон twarahamubonye biradutangaza, что означает, что мы были удивлены, обнаружив Джона там."}
{"text": "As you can see, Kinyarwanda words contains several morphemes that contain different information in them.\n", "text_Chinese": "正如你所看到的,基尼亚鲁旺达语的单词包含了几个形态,", "text_Arabic": "كما ترون، تحتوي كلمات كينيارواندا على عدة أشكال تحتوي على معلومات مختلفة.", "text_French": "Comme vous pouvez le voir, les mots kinyarwanda contiennent plusieurs morphèmes qui contiennent des informations différentes.", "text_Japanese": "キニアルワンダ語の単語には 異なる情報を含む 幾つかの形容詞が含まれています", "text_Russian": "Как видите, слова киньяруанда содержат несколько морфемов, которые содержат в себе разную информацию."}
{"text": "Therefore, in our model, we pass this sentence or a document to a morphological analyzer.\n", "text_Chinese": "因此,在我们的模型中,我们将这个句子或文档传递给一个形态分析器.", "text_Arabic": "لذلك, في نموذجنا, نمرر هذه الجملة أو وثيقة إلى محلل مورفولوجي.", "text_French": "Par conséquent, dans notre modèle, nous passons cette phrase ou un document à un analyseur morphologique.", "text_Japanese": "したがって私たちのモデルではこの文句または文書を形態分析器に渡します", "text_Russian": "Поэтому в нашей модели мы передаем это предложение или документ морфологическому анализатору."}
{"text": "Which then generates morphemes contained in each of the words.\n", "text_Chinese": "然后生成每个单词中包含的形态.", "text_Arabic": "والتي بعد ذلك تولد المورفيمات الواردة في كل كلمة.", "text_French": "Ce qui génère ensuite des morphèmes contenus dans chacun des mots.", "text_Japanese": "それぞれの単語に含まれる形態を生成します", "text_Russian": "Что затем генерирует морфемы, содержащиеся в каждом из слов."}
{"text": "The morphemes usually are made of the stem and zero or more affixes.\n", "text_Chinese": "形态通常由茎和零或更多的附加词组成.", "text_Arabic": "عادة ما تتكون المورفيمات من الجذع و صفر أو أكثر من الملحقات.", "text_French": "Les morphèmes sont généralement constitués de la tige et de zéro ou plusieurs affixes.", "text_Japanese": "モルフェムは通常,幹とゼロまたはそれ以上のアフィックスで構成されます.", "text_Russian": "Морфемы обычно состоят из стебля и нуля или более аффикса."}
{"text": "The affixes may indicate tense, aspect, subject or object in verbs, and more often relates to the Bantu noun class for subjects and objects.\n", "text_Chinese": "附词可能在动词中表示时态,方面,主题或对象,并且更常见地与班图名词类的主题和对象有关.", "text_Arabic": "قد تشير الملحقات إلى التوقيت أو الجانب أو الموضوع أو الكائن في الأفعال ، وغالباً ما تتعلق بفئة الأسماء البانتو للمواضيع والأشياء.", "text_French": "Les affixes peuvent indiquer le temps, l'aspect, le sujet ou l'objet dans les verbes, et plus souvent se rapportent à la classe de noms bantou pour les sujets et les objets.", "text_Japanese": "アフィクスは,動詞のテンス,側面,主題またはオブジェクトを示すことができ,よりしばしば,主題とオブジェクトのためのバンツ名詞クラスに関連しています.", "text_Russian": "Аффиксы могут указывать на время, аспект, предмет или объект в глаголах, и чаще относятся к классу существительных банту для предметов и объектов."}
{"text": "The morphological analyzer also produces a part of speech tag for each of the words.\n", "text_Chinese": "形态分析器还为每个单词产生了一部分语音标签.", "text_Arabic": "ينتج المحلل المورفولوجي أيضًا جزءًا من علامة الكلام لكل كلمة.", "text_French": "L'analyseur morphologique produit également une partie de l'étiquette de parole pour chacun des mots.", "text_Japanese": "形態分析器はまた,各単語のスピーチタグの一部を生成します.", "text_Russian": "Морфологический анализатор также производит часть речевого тега для каждого из слов."}
{"text": "After this step, we make embeddings for the spee- for the part of speech tags.\n", "text_Chinese": "在这一步之后,我们为演讲制作嵌入式 - - 对于演讲标签的一部分.", "text_Arabic": "بعد هذه الخطوة، نقوم بتضمين جزء من علامات الكلام.", "text_French": "Après cette étape, nous faisons des intégrations pour le discours - pour la partie des balises de discours.", "text_Japanese": "このステップの後スピーチの部分のスピーチタグの埋め込みを作ります", "text_Russian": "После этого шага мы делаем встраивания для речи - для части речевых тегов."}
{"text": "Embeddings for the affixes.\n", "text_Chinese": "附加词的嵌入.", "text_Arabic": "تضمينات للملحقات.", "text_French": "Les intégrations pour les affixes.", "text_Japanese": "アフィックスへの埋め込み", "text_Russian": "Встраивания для аффикса."}
{"text": "And embeddings for the stem.\n", "text_Chinese": "还有树干的嵌入物.", "text_Arabic": "و تدمج الجذع.", "text_French": "Et des insertions pour la tige.", "text_Japanese": "幹の埋め込みも", "text_Russian": "И встраивания для стебля."}
{"text": "These are the morphology level, these are the morphology level embeddings.\n", "text_Chinese": "这些是形态层面,这些是形态层面嵌入.", "text_Arabic": "هذه هي مستوى المورفولوجيا، وهذه هي مستويات المورفولوجيا.", "text_French": "Ce sont les niveaux de morphologie, ce sont les niveaux de morphologie.", "text_Japanese": "これらは形態レベルです これらは形態レベル埋め込みです", "text_Russian": "Это уровень морфологии, это уровень встраивания морфологии."}
{"text": "We then pass these embeddings through a morphology encoder, which is a small transformer encoder that is applied to each word independently.\n", "text_Chinese": "然后我们把这些嵌入通过一个形态编码器, 这是一个小变换器编码器,", "text_Arabic": "ثم نمرر هذه التضمينات من خلال مكوّن الشكل, وهو مكوّن تحويل صغير يتم تطبيقه على كل كلمة بشكل مستقل.", "text_French": "Nous passons ensuite ces intégrations à travers un encodeur de morphologie, qui est un petit encodeur de transformateur qui est appliqué à chaque mot indépendamment.", "text_Japanese": "それからこれらの埋め込みを 形態エンコーダーに渡します 形態エンコーダーは小さな変換エンコーダーで 単語ごとに独立して適用されます", "text_Russian": "Затем мы пропускаем эти встраивания через морфологический кодер, который представляет собой небольшой трансформаторный кодер, который применяется к каждому слову независимо."}
{"text": "The output of the are the vectors that are contextualized with the morphological information at each word.\n", "text_Chinese": "输出的是每个单词的形态信息的矢量.", "text_Arabic": "المخرج من هي المتجهات التي هي سياق مع المعلومات المورفولوجية في كل كلمة.", "text_French": "La sortie des vecteurs sont contextualisés avec l'information morphologique de chaque mot.", "text_Japanese": "単語の形態情報と コンテキスト化されたベクトルです", "text_Russian": "Выходными данными являются векторы, которые контекстуализируются с морфологической информацией в каждом слове."}
{"text": "Now, we perform composition where the morphological embeddings corresponding to part of speech and stem are concatenated together.\n", "text_Chinese": "现在, 我们进行组合, 模拟与语音和语根相应的形态嵌入, 并相连", "text_Arabic": "الآن, نقوم بتأليف حيث يتم ربط التضمينات المورفولوجية التي تتوافق مع جزء من الكلام والجذع معاً.", "text_French": "Maintenant, nous effectuons une composition où les intégrations morphologiques correspondant à une partie de la parole et du tronc sont concaténées.", "text_Japanese": "言語の一部と幹に相当する形態学的埋め込みが 連結されている構成を実行します", "text_Russian": "Теперь мы выполняем композицию, где морфологические встраивания, соответствующие части речи и ствола, соединены вместе."}
{"text": "We further concat we further concatenate them with another stem embedding at the sentence level.\n", "text_Chinese": "我们进一步将它们与另一个干嵌入在句子级别上.", "text_Arabic": "نحن نربطهم أكثر مع غصن آخر يضمن على مستوى الجملة.", "text_French": "Nous les concaténons davantage avec un autre tronc intégré au niveau de la phrase.", "text_Japanese": "句のレベルで別の幹を埋め込むことで 組み合わせます", "text_Russian": "Мы далее соединяем их с другим стеблем, встроенным на уровне предложения."}
{"text": "Then we form an input to the main sentence or document encoder.\n", "text_Chinese": "然后我们形成一个输入到主句子或文档编码器.", "text_Arabic": "ثم نقوم بتشكيل مدخل للجملة الرئيسية أو مكوّن المستند.", "text_French": "Ensuite, nous formons une entrée dans la phrase principale ou l'encodeur de document.", "text_Japanese": "主要な文句やドキュメントエンコーダーに 入力します", "text_Russian": "Затем мы формируем вход в главное предложение или кодировщик документов."}
{"text": "The final output are contextualized embeddings that can be used for downstream NLP tasks.\n", "text_Chinese": "最终的输出是可以用于下游NLP任务的上下文嵌入.", "text_Arabic": "النتيجة النهائية هي التضمينات السياقية التي يمكن استخدامها لمهام NLP في الممر السفلي.", "text_French": "Le résultat final est des intégrations contextualisées qui peuvent être utilisées pour les tâches de PNL en aval.", "text_Japanese": "最終的な出力は,下流のNLPタスクに使用できるコンテキスト化された埋め込みです.", "text_Russian": "Окончательным результатом являются контекстуализированные встраивания, которые могут использоваться для последующих задач NLP."}
{"text": "For a morphological analyzer, we use finite state two level morphology principles with custom implementation that is tailored to the Kinyarwanda language.\n", "text_Chinese": "对于形态分析器,我们使用有限状态两级形态学原则,并使用定制实现,该实现适合基尼亚鲁旺达语言.", "text_Arabic": "بالنسبة لمحلل المورفولوجي، نستخدم مبادئ المورفولوجيا ذات المستوىين المحدود مع تنفيذ مخصص مصمم خصيصًا للغة الكينيارواندا.", "text_French": "Pour un analyseur morphologique, nous utilisons des principes de morphologie à deux niveaux à état fini avec une mise en œuvre personnalisée adaptée à la langue kinyarwanda.", "text_Japanese": "形態分析器ではキニアルワンダ語に合わせたカスタム実装で有限状態2レベル形態原理を使用します", "text_Russian": "Для морфологического анализатора мы используем принципы морфологии двух уровней с конечным состоянием с пользовательской реализацией, адаптированной к языку киньяруанда."}
{"text": "We effectively model the morphology of all Kinyarwanda words, including verbals, nouns, demonstrative and possessive pronouns, numerals, and others.\n", "text_Chinese": "我们有效地模拟了所有基尼亚鲁万达语单词的形态, 包括词,名词,示意和占有代词,数字和其他.", "text_Arabic": "نحن نعمل على نمذجة المورفولوجيا لجميع الكلمات الكينيارواندا, بما في ذلك الكلمات, الأسماء, الضمائر الدلالية والملكية, الأرقام, وغيرها.", "text_French": "Nous modélisons efficacement la morphologie de tous les mots Kinyarwanda, y compris les verbaux, les noms, les pronoms démonstratifs et possessifs, les chiffres et autres.", "text_Japanese": "言葉,名詞,示唆的,所有的代名詞,数字など,すべてのキニアルワンダ語の形態を効果的にモデル化します.", "text_Russian": "Мы эффективно моделируем морфологию всех слов киньяруанда, включая вербалы, существительные, демонстративные и обладательные местоимения, цифры и другие."}
{"text": "We use an unsupervised part of speech tagging algorithm.\n", "text_Chinese": "我们使用一个无监督的语音标记算法部分.", "text_Arabic": "نستخدم جزء غير خاضع للإشراف من خوارزمية تسجيل الكلام.", "text_French": "Nous utilisons une partie non supervisée de l'algorithme de marquage de la parole.", "text_Japanese": "私たちはスピーチタグングアルゴリズムの 監督されていない部分を使用します", "text_Russian": "Мы используем неконтролируемую часть алгоритма отслеживания речи."}
{"text": "A first order factored model is used to account for morphology probability, basically the probability that is assigned by the morphological analyzer.\n", "text_Chinese": "一级分解模型用于解释形态概率,基本上是形态分析器分配的概率.", "text_Arabic": "يتم استخدام نموذج مسبب من الدرجة الأولى لمراعاة احتمال التشكيل، أساسا الاحتمال الذي يتم تعيينه من قبل المحلل التشكيلي.", "text_French": "Un modèle factorisé de premier ordre est utilisé pour prendre en compte la probabilité de morphologie, essentiellement la probabilité attribuée par l'analyseur morphologique.", "text_Japanese": "形態学的確率を説明するために,第一次因子化モデルが使用されます.基本的に形態学的分析器によって割り当てられる確率です.", "text_Russian": "Модель первого порядка используется для учета вероятности морфологии, в основном вероятности, которая назначается морфологическим анализатором."}
{"text": "We also take into consideration the part of speech tag precedence as well as the syntactic agreements that are present in the in the input words.\n", "text_Chinese": "我们还考虑了语音标签优先级的部分以及输入单词中存在的语法协议.", "text_Arabic": "نأخذ أيضًا في الاعتبار جزء من أولوية علامة الكلام وكذلك الاتفاقات التجميلية الموجودة في كلمات المدخلات.", "text_French": "Nous prenons également en compte la partie de la prééminence des balises de parole ainsi que les accords syntaxiques présents dans les mots d'entrée.", "text_Japanese": "語音タグの優先順位と 入力ワードに存在する構文の合意も考慮します", "text_Russian": "Мы также принимаем во внимание часть речи, маркирующей приоритет, а также синтаксические соглашения, которые присутствуют в входных словах."}
{"text": "The part of speech tagger uses a bidi bidirectional inference which improves upon the more often used Viterbi algorithm for decoding.\n", "text_Chinese": "语音标签器的一部分使用双向推断,这改进了更常用的Viterbi算法进行解码.", "text_Arabic": "يستخدم جزء من علامة الكلام استنتاجًا ثنائي الاتجاه الذي يحسن خوارزمية فيتربي الأكثر استخدامًا لفك التشفير.", "text_French": "La partie de l'étiquetteur de parole utilise une inférence bidi bidirectionnelle qui améliore l'algorithme Viterbi le plus souvent utilisé pour le décodage.", "text_Japanese": "スピーチタグの部分は,より頻繁に使用されるViterbiアルゴリズムを改善したbidi双方向推論を使用しています.", "text_Russian": "Часть теггера речи использует двунаправленный вывод биди, который улучшает более часто используемый алгоритм Витерби для декодирования."}
{"text": "A few remarks here for positional encoding.\n", "text_Chinese": "这里有一些关于位置编码的评论.", "text_Arabic": "بعض الملاحظات هنا للتشفير الموضعي.", "text_French": "Quelques remarques ici pour l'encodage de position.", "text_Japanese": "位置エンコーディングについてここではいくつかの注釈を述べます", "text_Russian": "Несколько замечаний по позиционному кодированию."}
{"text": "One, the morphology encoder does not use any positional encoding.\n", "text_Chinese": "一,形态编码器不使用任何位置编码.", "text_Arabic": "أولاً، لا يستخدم مشفر التشكيل أي تشفير موقعي.", "text_French": "Premièrement, l'encodeur de morphologie n'utilise aucun encodage de position.", "text_Japanese": "1つ目は 形態エンコーダーは 位置エンコーディングを使用しないことです", "text_Russian": "Во-первых, морфологический кодировщик не использует никакого позиционного кодирования."}
{"text": "This is because each of the morphemes occupies a known slot in the morphological model.\n", "text_Chinese": "这是因为每个形态都占据了形态模型中的已知位置.", "text_Arabic": "ويرجع ذلك إلى أن كل من المورفيمات تحتل مكانًا معروفًا في النموذج المورفولوجي.", "text_French": "C'est parce que chacun des morphèmes occupe une place connue dans le modèle morphologique.", "text_Japanese": "これは,モルフェムがそれぞれ形態学的モデルで既知のスロットを占めているからです.", "text_Russian": "Это связано с тем, что каждая из морфем занимает известное место в морфологической модели."}
{"text": "Therefore, positional information is inherent when the morphemes are given.\n", "text_Chinese": "因此,当给出形态时,位置信息是固有的.", "text_Arabic": "وبالتالي، فإن المعلومات الموضعية متأصلة عندما يتم إعطاء المورفيمات.", "text_French": "Par conséquent, l'information de position est inhérente lorsque les morphèmes sont donnés.", "text_Japanese": "したがって,モルフェムが与えられるとき,位置情報は固有である.", "text_Russian": "Следовательно, позиционная информация присуща, когда даны морфемы."}
{"text": "Second, the sentence encoder uses the so-called untied relative positional embeddings, which have been recently published at ICLR conference.\n", "text_Chinese": "第二,句子编码器使用所谓的无关相对位置嵌入,", "text_Arabic": "ثانياً، يستخدم مشفر الجملة ما يسمى التضمينات الموضعية النسبية غير المرتبطة، والتي تم نشرها مؤخرًا في مؤتمر ICLR.", "text_French": "Deuxièmement, le codeur de phrase utilise les soi-disant incorporations positionnelles relatives non liées, qui ont été récemment publiées à la conférence du CICR.", "text_Japanese": "第二に文エンコーダーは最近ICLR会議で発表されたいわゆる無相対位置埋め込みを使用します", "text_Russian": "Во-вторых, кодировщик предложений использует так называемые несвязанные относительные позиционные встраивания, которые были недавно опубликованы на конференции ICLR."}
{"text": "This positional embeddings essentially disentangles positional correlations from token to token attention computation.\n", "text_Chinese": "这种位置嵌入基本上从令牌到令牌的注意力计算中解开了位置相关性.", "text_Arabic": "هذا التضمين الموضعي في الأساس يفصل بين الارتباطات الموضعية من حساب الاهتمام من رمز إلى رمز.", "text_French": "Ces intégrations positionnelles démêlent essentiellement les corrélations positionnelles du calcul de l'attention de jeton à jeton.", "text_Japanese": "この位置埋め込みは基本的にトークンからトークンへの注意計算の位置相関を解き明かします", "text_Russian": "Это позиционное встраивание по существу распутывает позиционные корреляции от токена к токену вычисления внимания."}
{"text": "Similar to BERT, we use a masked language model pre-training objective.\n", "text_Chinese": "类似于BERT,我们使用一个掩盖的语言模型预训练目标.", "text_Arabic": "على غرار BERT، نستخدم نموذج لغة مقنعة قبل الهدف التدريبي.", "text_French": "Semblable à BERT, nous utilisons un modèle de langage masqué pour l'entraînement préalable.", "text_Japanese": "BERTと同様に私たちはマスクされた言語モデルをトレーニング前の目的として使用します", "text_Russian": "Подобно BERT, мы используем маскированную языковую модель для предварительной подготовки целей."}
{"text": "Essentially we have to predict both the stem and the affixes that are associated with the words.\n", "text_Chinese": "基本上,我们必须预测与单词相关的根和附加词.", "text_Arabic": "أساسا علينا أن نتنبأ بكل من الجذع والملحقات المرتبطة بالكلمات.", "text_French": "Essentiellement, nous devons prédire à la fois la tige et les affixes qui sont associés aux mots.", "text_Japanese": "基本的に私たちは言葉と関連付けられている幹と付属語の両方を予測する必要があります", "text_Russian": "По сути, мы должны предсказать как ствол, так и аффиксы, которые связаны со словами."}
{"text": "During pre-training, fifteen percent of all words are considered for prediction, of which eighty percent are masked, ten percent are swapped with random words, and ten percent are left unchanged.\n", "text_Chinese": "在预训练期间, 15% 的词语被考虑进行预测, 其中 80% 被掩盖, 10% 被随机词语交换, 10% 保持不变", "text_Arabic": "خلال التدريب المسبق, خمسة عشر في المئة من جميع الكلمات يتم النظر فيها للتنبؤ, منها ثمانون في المئة مقنعة, عشرة في المئة يتم استبدالها بكلمات عشوائية, وعشرة في المئة تبقى دون تغيير.", "text_French": "Pendant la pré-formation, quinze pour cent de tous les mots sont pris en compte pour la prédiction, dont quatre-vingts pour cent sont masqués, dix pour cent sont échangés avec des mots aléatoires, et dix pour cent sont laissés inchangés.", "text_Japanese": "予備訓練では すべての単語の15%が 予測に考慮され そのうち80%は 隠され 10%はランダムな単語で交換され 10%は 変更されません", "text_Russian": "Во время предварительной подготовки 15% всех слов рассматриваются для предсказания, из которых 80% замаскированы, 10% заменены случайными словами, а 10% остаются неизменными."}
{"text": "For affix prediction, we face some multi label classification problem.\n", "text_Chinese": "对于附加词预测,我们面临一些多标签分类问题.", "text_Arabic": "للتنبؤ بالملحقات، نواجه بعض مشاكل تصنيف العلامات المتعددة.", "text_French": "Pour la prédiction des affixes, nous sommes confrontés à un problème de classification à plusieurs étiquettes.", "text_Japanese": "アフィックス予測では,マルチラベル分類の問題に直面します.", "text_Russian": "Для прогнозирования аффикса мы сталкиваемся с проблемой классификации с несколькими ярлыками."}
{"text": "For this, we either group together affixes into a fixed number of sets and predict the set as a class label.\n", "text_Chinese": "为此,我们要么将附加符分组成固定数量的集合,并预测该集合为类标签.", "text_Arabic": "لهذا، إما نقوم بتجميع الملحقات في عدد ثابت من المجموعات ونتوقع المجموعة كملصق فئة.", "text_French": "Pour cela, nous regroupons les affixes en un nombre fixe d'ensembles et prédisons l'ensemble comme une étiquette de classe.", "text_Japanese": "セットの固定数にアフィックスをグループ化し セットをクラスラベルとして予測します", "text_Russian": "Для этого мы либо группируем аффиксы в фиксированное количество наборов, либо предсказываем набор как ярлык класса."}
{"text": "The other option is to predict the affix probability vector.\n", "text_Chinese": "另一个选择是预测附加概率向量.", "text_Arabic": "الخيار الآخر هو التنبؤ بمتجه احتمال الملحق.", "text_French": "L'autre option est de prédire le vecteur de probabilité d'affix.", "text_Japanese": "もう一つの選択肢はアフィックス確率ベクトルを予測することです", "text_Russian": "Другой вариант - предсказать вектор вероятности аффикса."}
{"text": "We evaluate both of these approaches in our experiments.\n", "text_Chinese": "我们在实验中评估了这两种方法.", "text_Arabic": "نحن نقيّم كلا هذين النهج في تجاربنا.", "text_French": "Nous évaluons ces deux approches dans nos expériences.", "text_Japanese": "この2つのアプローチを実験で評価します", "text_Russian": "Мы оцениваем оба этих подхода в наших экспериментах."}
{"text": "We pre-train KinyaBERT on about two and half gigabytes of Kinyarwanda text, and compare it to three baseline models.\n", "text_Chinese": "我们预先训练了 KinyaBERT, 约有 2.5 吉巴的 Kinyarwanda 文字, 并与三个基线模型进行比较", "text_Arabic": "قمنا بتدريب KinyaBERT مسبقا على حوالي اثنين ونصف غيغابايت من نص Kinyarwanda, ومقارنتها بثلاثة نماذج أساسية.", "text_French": "Nous avons pré-entraîné KinyaBERT sur environ deux gigaoctets et demi de texte Kinyarwanda, et l'avons comparé à trois modèles de base.", "text_Japanese": "KinyaBERTを2ギガバイト半のキニアルワンダ語のテキストで 予備訓練し 3つのベースラインモデルと比較しました", "text_Russian": "Мы предварительно обучили KinyaBERT примерно 2,5 гигабайта текста на языке киньяруанда и сравнили его с тремя базовыми моделями."}
{"text": "One is a multilingual model called XLM-R, that is trained on a large text corpora that is made of multiple languages.\n", "text_Chinese": "一个是多语言模型, 叫做 XLM-R, 它被训练在由多种语言组成的大量文本上", "text_Arabic": "أحدها نموذج متعدد اللغات يسمى XLM-R, والذي يتم تدريبه على مجموعة كبيرة من النصوص التي تتكون من لغات متعددة.", "text_French": "L'un est un modèle multilingue appelé XLM-R, qui est formé sur un grand corpus de texte composé de plusieurs langues.", "text_Japanese": "1つはXLM-Rと呼ばれる多言語モデルで 複数の言語で構成された 大規模なテキストコルポラで訓練されています", "text_Russian": "Одна из них  многоязычная модель под названием XLM-R, которая обучается на большом текстовом корпусе, состоящем из нескольких языков."}
{"text": "The other two baselines are pretrained on the same Kinyarwanda text using either the byte pair encoding algorithm or using morphological analysis without using the two tier transformer encoder architecture.\n", "text_Chinese": "其他两个基线在相同的基尼亚卢旺达文本上进行预训练,使用字节对编码算法或使用形态分析,而不使用两层变压器编码器架构.", "text_Arabic": "يتم تدريب خطين أساسيين آخرين على نفس نص Kinyarwanda باستخدام خوارزمية ترميز زوج البايت أو باستخدام التحليل المورفولوجي دون استخدام بنية ترميز المحول ذات المستويين.", "text_French": "Les deux autres lignes de base sont préentrainées sur le même texte kinyarwanda en utilisant soit l'algorithme d'encodage par paire d'octets, soit l'analyse morphologique sans utiliser l'architecture d'encodeur de transformateur à deux niveaux.", "text_Japanese": "他の2つのベースラインは,バイットペアエンコーディングアルゴリズムを使用して,または2層トランスフォーマーエンコーダーアーキテクチャを使用せずに形態分析を使用して,同じキニアワンダテキストで事前訓練されます.", "text_Russian": "Остальные две базовые линии предварительно обучаются на том же тексте киньяруанда с использованием алгоритма кодирования байтовых пар или с использованием морфологического анализа без использования архитектуры кодера двухслойного трансформатора."}
{"text": "All models are configured in the base architecture, which is about between a hundred and a hundred and ten million parameters, with Kinyarwanda with KinyaBERT using the least number of parameters.\n", "text_Chinese": "所有模型都配置在基本架构中,大约在100到100到100万个参数之间,Kinyarwanda和KinyaBERT使用最少的参数.", "text_Arabic": "يتم تكوين جميع النماذج في الهندسة المعمارية الأساسية ، والتي تتراوح بين مائة ومائة وعشرة ملايين معيار ، مع Kinyarwanda مع KinyaBERT باستخدام أقل عدد من المعايير.", "text_French": "Tous les modèles sont configurés dans l'architecture de base, qui est d'environ entre cent et cent et dix millions de paramètres, avec Kinyarwanda avec KinyaBERT utilisant le moins de paramètres.", "text_Japanese": "すべてのモデルが基本アーキテクチャで構成されています. これは約100から100から1000万パラメータの間で,キニアルワンダとキニアBERTはパラメータの数が最も少ない.", "text_Russian": "Все модели настроены в базовой архитектуре, которая составляет примерно от ста до ста до десяти миллионов параметров, причем Kinyarwanda с KinyaBERT использует наименьшее количество параметров."}
{"text": "All models except the multilingual are pretrained for thirty two thousand gradient updates with a batch size of two thousand five hundred and sixty sequences in each batch.\n", "text_Chinese": "除了多语言模型之外,所有模型都经过预训练,进行三万两千次梯度更新,每批次有两千五百六十个序列.", "text_Arabic": "يتم تدريب جميع النماذج باستثناء متعددة اللغات مسبقًا على اثنين وثلاثين ألف تحديث من التدرج بحجم دفعة من ألفين وخمسمائة وستين تسلسل في كل دفعة.", "text_French": "Tous les modèles, à l'exception du multilingue, sont préentraînés pour trente-deux mille mises à jour de gradient avec une taille de lot de deux mille cinq cent soixante séquences dans chaque lot.", "text_Japanese": "多言語モデルを除くすべてのモデルが各バッチのバッチサイズで2560シーケンスで32,000のグラディエントアップデートのために事前訓練されています", "text_Russian": "Все модели, за исключением многоязычной, предварительно обучаются для тридцати двух тысяч обновлений градиента с размером партии из двух тысяч пятисот шестидесяти последовательностей в каждой партии."}
{"text": "We evaluate the pretrained models on three sets of tasks.\n", "text_Chinese": "我们对预先训练的模型进行评估,", "text_Arabic": "نقوم بتقييم النماذج المدربة مسبقاً على ثلاث مجموعات من المهام.", "text_French": "Nous évaluons les modèles préentraînés sur trois ensembles de tâches.", "text_Japanese": "3つのタスクで 予備訓練されたモデルを評価します", "text_Russian": "Мы оцениваем предварительно обученные модели по трем наборам задач."}
{"text": "One is the GLUE benchmark which has often been used for evaluating the effectiveness of pretrained language models.\n", "text_Chinese": "一个是GLUE基准,它经常用于评估预训练语言模型的有效性.", "text_Arabic": "أحدها هو معيار GLUE الذي تم استخدامه في كثير من الأحيان لتقييم فعالية نماذج اللغة المدربة مسبقًا.", "text_French": "L'un d'eux est le critère GLUE, qui a souvent été utilisé pour évaluer l'efficacité des modèles linguistiques préentraînés.", "text_Japanese": "一つは,事前訓練された言語モデルの有効性を評価するためにしばしば使用されているGLUEベンチマークです.", "text_Russian": "Одним из них является эталон GLUE, который часто используется для оценки эффективности предварительно обученных языковых моделей."}
{"text": "We obtain our GLUE benchmark data by translating the original benchmark data into Kinyarwanda using Google Translate.\n", "text_Chinese": "我们通过使用谷歌翻译将原始基准数据翻译成基尼亚卢旺达语来获得GLUE基准数据.", "text_Arabic": "نحصل على بيانات مرجعية GLUE من خلال ترجمة بيانات مرجعية أصلية إلى لغة Kinyarwanda باستخدام Google Translate.", "text_French": "Nous obtenons nos données de référence GLUE en traduisant les données de référence originales en kinyarwanda à l'aide de Google Translate.", "text_Japanese": "GLUEのベンチマークデータはGoogle Translateを使用してオリジナルのベンチマークデータをキニヤルワンダ語に翻訳することで得られます", "text_Russian": "Мы получаем наши данные GLUE путем перевода оригинальных данных на киньяруанда с помощью Google Translate."}
{"text": "The second task is Kinyarwanda named entity recognition benchmark, which is a high quality dataset that was annotated by trained native speakers.\n", "text_Chinese": "第二个任务是基尼亚卢旺达语命名实体识别基准,这是一个由训练有素的母语人士注释的高质量数据集.", "text_Arabic": "المهمة الثانية هي معيار التعرف على الكيانات التي تحمل أسماء كينيارواندا، وهو مجموعة بيانات عالية الجودة تم تعليقها من قبل المتحدثين الأصليين المدربين.", "text_French": "La deuxième tâche est le référentiel de reconnaissance des entités nommées Kinyarwanda, qui est un ensemble de données de haute qualité annoté par des locuteurs natifs formés.", "text_Japanese": "2つ目のタスクは キニアルワンダ語の名前の認知基準です 訓練されたネイティブスピーカーによって解説された 高品質のデータセットです", "text_Russian": "Вторая задача - это эталон признания названий в киньяруанском языке, который представляет собой высококачественный набор данных, который был аннотирован обученными носителями языка."}
{"text": "The third one is a news categorization task where we pull news articles from several websites and collecting their categorization tags that were assigned by the authors and then essentially trying to predict the same, the the same categories.\n", "text_Chinese": "第三个是新闻分类任务, 我们从几个网站上提取新闻文章, 收集作者分配的分类标签, 然后基本上试图预测相同的类别", "text_Arabic": "والثالث هو مهمة تصنيف الأخبار حيث نخرج مقالات أخبار من عدة مواقع وجمع علامات تصنيفها التي تم تعيينها من قبل المؤلفين ثم في الأساس محاولة التنبؤ بنفس, نفس الفئات.", "text_French": "La troisième est une tâche de catégorisation des nouvelles où nous tirons des articles de plusieurs sites web et recueillons leurs étiquettes de catégorisation qui ont été assignées par les auteurs et ensuite essayons essentiellement de prédire la même chose, les mêmes catégories.", "text_Japanese": "3つ目は ニュース分類の作業で 複数のウェブサイトから ニュース記事を集め 著者によって割り当てられた 分類タグを集め 基本的に同じカテゴリーを 予測しようとします", "text_Russian": "Третий - это задача по классификации новостей, где мы выбираем новостные статьи с нескольких сайтов и собираем их теги классификации, которые были присвоены авторами, и затем, по сути, пытаемся предсказать то же самое, те же самые категории."}
{"text": "And now we go to the results.\n", "text_Chinese": "现在我们来看看结果.", "text_Arabic": "والآن نذهب إلى النتائج.", "text_French": "Et maintenant les résultats.", "text_Japanese": "そして今結果をみていきましょう", "text_Russian": "А теперь переходим к результатам."}
{"text": "For the GLUE benchmark, we find that KinyaBERT consistently outperforms baseline models.\n", "text_Chinese": "对于GLUE基准,我们发现KinyaBERT始终优于基线模型.", "text_Arabic": "بالنسبة لمقياس GLUE، نجد أن KinyaBERT يتفوق باستمرار على نماذج خط الأساس.", "text_French": "Pour le benchmark GLUE, nous constatons que KinyaBERT surpasse constamment les modèles de base.", "text_Japanese": "GLUEベンチマークでは,KinyaBERTがベースラインモデルを一貫して上回っていることがわかりました.", "text_Russian": "Для бенчмарка GLUE мы обнаруживаем, что KinyaBERT постоянно превосходит базовые модели."}
{"text": "Here we show the average performance for ten finetuning runs.\n", "text_Chinese": "这里我们显示了十次微调运行的平均性能.", "text_Arabic": "هنا نعرض متوسط الأداء لعشرة عمليات ضبط دقيقة.", "text_French": "Ici, nous montrons la performance moyenne pour dix essais de réglage fin.", "text_Japanese": "ここでは10回のフィニチュリングの平均パフォーマンスを示しています.", "text_Russian": "Здесь мы показываем среднюю производительность за десять пробегов тонкой настройки."}
{"text": "We also run a user evaluation of the translations that are produced by Google Translate.\n", "text_Chinese": "我们还对谷歌翻译所制作的翻译进行用户评估.", "text_Arabic": "كما نقوم بتقييم المستخدم للترجمات التي تنتجها Google Translate.", "text_French": "Nous effectuons également une évaluation des utilisateurs des traductions produites par Google Translate.", "text_Japanese": "Google Translateによって生成された翻訳のユーザー評価も実行します.", "text_Russian": "Мы также проводим пользовательскую оценку переводов, которые производятся Google Translate."}
{"text": "Essentially, user users rated about six thousand examples, assigning scores on a scale from one to four, assessing the quality of the translations.\n", "text_Chinese": "用户基本上对大约六千个例子进行了评分,", "text_Arabic": "في الأساس، قام المستخدمون بتقييم حوالي ستة آلاف مثال، مع منح درجات على مقياس من واحد إلى أربعة، لتقييم جودة الترجمة.", "text_French": "Essentiellement, les utilisateurs ont évalué environ six mille exemples, attribuant des notes sur une échelle de un à quatre, évaluant la qualité des traductions.", "text_Japanese": "基本的に ユーザーユーザーは 約6千の例を評価し 1から4までのスケールで 評価し 翻訳の質を評価しました", "text_Russian": "По сути, пользователи оценивали около шести тысяч примеров, присваивая оценки по шкале от одного до четырех, оценивая качество переводов."}
{"text": "The result is that many translations were noisy.\n", "text_Chinese": "结果是许多翻译都很嘈杂.", "text_Arabic": "النتيجة هي أن العديد من الترجمات كانت صاخبة.", "text_French": "Le résultat est que de nombreuses traductions étaient bruyantes.", "text_Japanese": "その結果,多くの翻訳ががしいことになりました.", "text_Russian": "В результате многие переводы были шумными."}
{"text": "But, all models had to cope with the same translation noise, and the relative performance between the models is still important to notice.\n", "text_Chinese": "但是,所有模型都必须应对相同的翻译噪音, 模型之间的相对性能仍然很重要.", "text_Arabic": "ولكن، كان على جميع النماذج التعامل مع نفس ضوضاء الترجمة، والأداء النسبي بين النماذج لا يزال من المهم ملاحظة.", "text_French": "Mais, tous les modèles devaient faire face au même bruit de traduction, et la performance relative entre les modèles est toujours importante à remarquer.", "text_Japanese": "しかし,すべてのモデルが同じ翻訳ノイズに対処しなければならず,モデル間の相対的なパフォーマンスはまだ注目することが重要です.", "text_Russian": "Но всем моделям приходилось справляться с одинаковым шумом перевода, и относительная производительность между моделями по-прежнему важна для замечания."}
{"text": "For the named entity recognition task, we also find that KinyaBERT gives the best performance with the affix distribution regression variant performing best.\n", "text_Chinese": "对于命名实体识别任务,我们还发现KinyaBERT提供了最好的性能,而附加分布回归变体表现最好.", "text_Arabic": "بالنسبة لمهمة التعرف على الكيان المسمى ، نجد أيضًا أن KinyaBERT يعطي أفضل أداء مع أفضل أداء في متغير تراجع توزيع الملحق.", "text_French": "Pour la tâche de reconnaissance d'entités nommées, nous constatons également que KinyaBERT donne les meilleures performances avec la variante de régression de la distribution d'affixes ayant les meilleures performances.", "text_Japanese": "名前付きエンティティの認識タスクでは,KinyaBERTが最高のパフォーマンスを与え,アフィックス分布回帰変数が最も良いパフォーマンスを与えることがわかりました.", "text_Russian": "Для задачи распознавания названных сущностей мы также находим, что KinyaBERT дает лучшую производительность с вариантом регрессии распределения аффикса, который работает лучше всего."}
{"text": "These results are also averages of ten finetuning runs.\n", "text_Chinese": "这些结果也是十次微调运行的平均值.", "text_Arabic": "هذه النتائج هي أيضا متوسط عشرة جولات ضبط دقيقة.", "text_French": "Ces résultats sont également des moyennes de dix essais de réglage fin.", "text_Japanese": "これらの結果は10回のフィニチュリングの平均値でもあります.", "text_Russian": "Эти результаты также являются средними показателями десяти пробегов тонкой настройки."}
{"text": "For the news categorization task, we find mixed results.\n", "text_Chinese": "对于新闻分类任务, 我们发现了不同的结果.", "text_Arabic": "بالنسبة لمهمة تصنيف الأخبار، نجد نتائج مختلطة.", "text_French": "Pour la tâche de catégorisation des nouvelles, nous trouvons des résultats mitigés.", "text_Japanese": "ニュース分類のタスクでは 結果は混じり合っています", "text_Russian": "Для задачи категоризации новостей мы находим смешанные результаты."}
{"text": "Previous work on text classification for Kinyarwanda had found that simple keyword detection is mostly enough for solving this specific task.\n", "text_Chinese": "以前关于基尼亚卢旺达语文本分类的工作发现,简单的关键字检测通常足以解决这一特定任务.", "text_Arabic": "وجد العمل السابق على تصنيف النص لكينيارواندا أن الكشف البسيط عن الكلمات الرئيسية يكفي في الغالب لحل هذه المهمة المحددة.", "text_French": "Des travaux antérieurs sur la classification du texte pour le kinyarwanda avaient montré que la simple détection de mots-clés suffit généralement à résoudre cette tâche spécifique.", "text_Japanese": "キニアルワンダ語のテキスト分類に関する以前の研究では,単純なキーワード検出がこの特定のタスクを解決するのにほとんど十分であることが判明しました.", "text_Russian": "Предыдущая работа по классификации текста для киньяруанды показала, что простого обнаружения ключевых слов в основном достаточно для решения этой конкретной задачи."}
{"text": "Therefore, there is less gain from using pretrained language models.\n", "text_Chinese": "因此,使用预先训练的语言模型所获得的收益较少.", "text_Arabic": "وبالتالي، هناك ربح أقل من استخدام نماذج اللغة المدربة مسبقًا.", "text_French": "Par conséquent, il y a moins de gain à utiliser des modèles de langage préentraînés.", "text_Japanese": "従って,事前訓練された言語モデルを使用することで得られる利益は少ない.", "text_Russian": "Поэтому использование предварительно обученных языковых моделей приносит меньшую выгоду."}
{"text": "On this particular task of news categorization.\n", "text_Chinese": "在这个特殊的新闻分类任务上.", "text_Arabic": "على هذه المهمة الخاصة بتصنيف الأخبار.", "text_French": "Sur cette tâche particulière de catégorisation des nouvelles.", "text_Japanese": "このニュース分類の特定の課題について", "text_Russian": "На этой конкретной задаче категоризации новостей."}
{"text": "We also conducted an ablation study to see if there are alternative structures that improve performance.\n", "text_Chinese": "我们还进行了消灭研究, 看看是否有替代结构可以提高性能.", "text_Arabic": "كما أجرينا دراسة للانتقاص لنرى ما إذا كانت هناك هياكل بديلة تحسن الأداء.", "text_French": "Nous avons également mené une étude d'ablation pour voir s'il y avait des structures alternatives qui améliorent les performances.", "text_Japanese": "性能を向上させる代替構造があるかどうかを調べるために 摘出研究も行いました", "text_Russian": "Мы также провели исследование абляции, чтобы увидеть, есть ли альтернативные структуры, которые улучшают производительность."}
{"text": "For the GLUE benchmark, we find that using affix sets consistently performs better, while affix probability regression objective yields the best performance on named entity recognition.\n", "text_Chinese": "对于GLUE基准,我们发现使用附加组的性能始终更好,而附加概率回归目标在命名实体识别上产生最佳性能.", "text_Arabic": "بالنسبة لمقياس GLUE ، نجد أن استخدام مجموعات الملحقات يؤدي بشكل أفضل باستمرار ، في حين أن هدف تراجع احتمالية الملحقات يعطي أفضل أداء في التعرف على الكيان المسمى.", "text_French": "Pour le benchmark GLUE, nous constatons que l'utilisation d'ensembles d'affixes est systématiquement plus performante, tandis que l'objectif de régression de probabilité d'affixes produit les meilleures performances sur la reconnaissance d'entités nommées.", "text_Japanese": "GLUEベンチマークでは,アフィックスセットを使用すると一貫してより良いパフォーマンスを得ることがわかりますが,アフィックス確率回帰目標は,名前のエンティティ認識で最高のパフォーマンスを得ます.", "text_Russian": "Для бенчмарка GLUE мы обнаруживаем, что использование наборов аффикса постоянно работает лучше, в то время как цель регрессии вероятности аффикса дает лучшие результаты при распознавании названных сущностей."}
{"text": "Also by looking at the low scores for finetuning, we find that KinyaBERT has better convergence in most cases.\n", "text_Chinese": "此外,通过查看微调的低分数,我们发现KinyaBERT在大多数情况下具有更好的收<unk>.", "text_Arabic": "أيضًا من خلال النظر إلى الدرجات المنخفضة للتعديل الدقيق ، نجد أن KinyaBERT لديه تقارب أفضل في معظم الحالات.", "text_French": "En regardant également les scores faibles pour le réglage fin, nous constatons que KinyaBERT a une meilleure convergence dans la plupart des cas.", "text_Japanese": "また微調整の低いスコアを見るとKinyaBERTはほとんどの場合より良い収束性を有していることがわかります", "text_Russian": "Также, глядя на низкие баллы для тонкой настройки, мы находим, что KinyaBERT имеет лучшую конвергенцию в большинстве случаев."}
{"text": "So to conclude, this work has demonstrated the effectiveness of explicitly using morphological information in pretrained language models.\n", "text_Chinese": "总之,这项工作证明了在预训练的语言模型中明确使用形态信息的有效性.", "text_Arabic": "لذلك في الختام، أظهر هذا العمل فعالية استخدام المعلومات المورفولوجية بشكل صريح في نماذج اللغة المدربة مسبقا.", "text_French": "Pour conclure, ce travail a démontré l'efficacité d'utiliser explicitement l'information morphologique dans des modèles de langage préentraînés.", "text_Japanese": "結論として この研究は 訓練済みの言語モデルで 形態学的な情報を 明確に使用する効果を 示しました", "text_Russian": "В заключение, эта работа продемонстрировала эффективность явного использования морфологической информации в предварительно обученных языковых моделях."}
{"text": "The proposed two tier transformer encoder architecture enables capturing morphological complexity morphological compositionality, which is an important aspect of morphologically rich languages.\n", "text_Chinese": "提议的两层变压器编码器架构能够捕捉形态复杂性和形态组成性,这是形态丰富语言的重要方面.", "text_Arabic": "يسمح بنية رمز التحويل المزدوجة المقترحة بالاستيلاء على التعقيد المورفولوجي والتكوين المورفولوجي ، وهو جانب مهم من اللغات الغنية مورفولوجياً.", "text_French": "L'architecture d'encodeur de transformateur à deux niveaux proposée permet de capturer la complexité morphologique et la composition morphologique, ce qui est un aspect important des langues morphologiquement riches.", "text_Japanese": "提案された2層トランスフォーマーエンコーダーアーキテクチャは,形態学的に豊かな言語の重要な側面である形態学的な複雑性や形態学的な構成性を捉えることができます.", "text_Russian": "Предлагаемая двухуровневая архитектура трансформаторного кодера позволяет улавливать морфологическую сложность и морфологическую композиционность, что является важным аспектом морфологически богатых языков."}
{"text": "These findings should motivate further research into morphology aware language pretrained language models.\n", "text_Chinese": "这些发现应该激励进一步研究形态意识语言预训练语言模型.", "text_Arabic": "يجب أن تحفز هذه النتائج المزيد من البحوث على نماذج اللغة المدربة مسبقًا على اللغة الواعية بالتشكيل.", "text_French": "Ces résultats devraient motiver de nouvelles recherches sur les modèles de langage préentraînés conscients de la morphologie.", "text_Japanese": "これらの発見は,形態学に意識した言語の予備訓練された言語モデルに関するさらなる研究を促進するべきです.", "text_Russian": "Эти результаты должны побудить к дальнейшим исследованиям морфологически осведомленных языковых моделей с предварительной подготовкой."}
{"text": "Hello, my name is Michał Pietruszka and it is my pleasure to present to you the paper titled Sparsifying Transformer Models with Trainable Representation Pooling.\n", "text_Chinese": "您好,我的名字是 Michał Pietruszka,我很高兴向您展示一篇题为可训练的表示池的散射变换模型的论文", "text_Arabic": "مرحبا، اسمي ميشيل بيتروشكا، ويسعدني أن أقدم لكم ورقة بعنوان نماذج التحويل المتفجر مع تجميع التمثيل القابل للتدريب.", "text_French": "Bonjour, mon nom est Michał Pietruszka et c'est avec plaisir que je vous présente l'article intitulé \"Modèles de transformateurs dispersifs avec pooling de représentation entrainable\".", "text_Japanese": "こんにちは私の名前はミカール・ピエトロスカですそして私は訓練可能な表現プーリングを用いたスパルシフィング・トランスフォーマー・モデルという論文をご紹介するのが嬉しいです", "text_Russian": "Здравствуйте, меня зовут Михаль Пиетрушка и я с удовольствием представляю вам статью под названием \"Спарсификационные модели трансформаторов с обучаемым объединением представлений\"."}
{"text": "A work done at Applica AI in cooperation with Lukasz Borchmann and Lukasz Garncarek.\n", "text_Chinese": "这是在Applica AI与Lukasz Borchmann和Lukasz Garncarek合作完成的工作.", "text_Arabic": "عمل تم في Applica AI بالتعاون مع لوكاس بورشمان و لوكاس غارنكارك.", "text_French": "Un travail réalisé par Applica AI en coopération avec Lukasz Borchmann et Lukasz Garncarek.", "text_Japanese": "アプリキュアAIでルカシュ・ボルクマンとルカシュ・ガーンカレックと協力して行われた作業です", "text_Russian": "Работа, выполненная в Applica AI в сотрудничестве с Лукашем Борхманом и Лукашем Гарнкареком."}
{"text": "Let me start with the problems our work targets.\n", "text_Chinese": "让我从我们工作的目标问题开始", "text_Arabic": "دعوني أبدأ بالمشاكل التي يستهدفها عملنا.", "text_French": "Permettez-moi de commencer par les problèmes visés par notre travail.", "text_Japanese": "まずは私たちの仕事の目標である問題から始めましょう", "text_Russian": "Позвольте мне начать с проблем, на которые нацелена наша работа."}
{"text": "Our method works well for the cases where long inputs are considered.\n", "text_Chinese": "我们的方法适用于考虑长输入的案例.", "text_Arabic": "تعمل طريقتنا بشكل جيد للحالات التي يتم فيها النظر في المدخلات الطويلة.", "text_French": "Notre méthode fonctionne bien pour les cas où de longues entrées sont considérées.", "text_Japanese": "長い入力が考慮される場合には,私たちの方法はうまく機能します.", "text_Russian": "Наш метод хорошо работает для случаев, когда рассматриваются длинные входы."}
{"text": "Roughly speaking, it is meant for the task orders and input of over two thousand tokens and the targets are shorter than the provided inputs.\n", "text_Chinese": "大致来说,它用于任务命令和超过两千个令牌的输入,目标比提供的输入更短.", "text_Arabic": "بشكل عام، فإنه يهدف إلى أوامر المهام وإدخال أكثر من ألفين من الرموز والأهداف أقصر من المدخلات المقدمة.", "text_French": "En gros, il est destiné aux ordres de tâche et à l'entrée de plus de deux mille jetons et les cibles sont plus courtes que les entrées fournies.", "text_Japanese": "大体それは2千以上のトークンのタスクオーダーと入力のために意図されターゲットは提供された入力よりも短い", "text_Russian": "Грубо говоря, он предназначен для заказов задач и ввода более двух тысяч токенов, а цели короче, чем предоставленные входы."}
{"text": "This has some specific applications in NLP.\n", "text_Chinese": "这在NLP中有一些特定的应用.", "text_Arabic": "هذا له بعض التطبيقات المحددة في NLP.", "text_French": "Cela a des applications spécifiques dans la PNL.", "text_Japanese": "これはNLPに特有の応用があります", "text_Russian": "Это имеет некоторые конкретные применения в НЛП."}
{"text": "For example, one can imagine that given a long document, there's a need to summarize it, classify, answer the question about it, extract information or some key phrases.\n", "text_Chinese": "例如, 可以想象, 给定一个长文档, 有需要总结它, 分类它, 回答有关它的问题, 提取信息或一些关键短语", "text_Arabic": "على سبيل المثال, يمكن للمرء أن يتخيل أنه نظرا لوثيقة طويلة, هناك حاجة لتلخيصها, تصنيفها, الإجابة على السؤال عنها, استخراج المعلومات أو بعض العبارات الرئيسية.", "text_French": "Par exemple, on peut imaginer qu'un long document, il y a un besoin de le résumer, de le classer, de répondre à la question à ce sujet, d'extraire des informations ou des phrases clés.", "text_Japanese": "例えば 長い文書を 概要化し 分類し 質問に答える 情報やキーフレーズを 抽出する必要があると 想像できます", "text_Russian": "Например, можно представить, что при наличии длинного документа, есть необходимость обобщить его, классифицировать, ответить на вопрос о нем, извлечь информацию или некоторые ключевые фразы."}
{"text": "Let me recall the vanilla transformer and our and its issue of its attention complexity that depends on the square of the input line.\n", "text_Chinese": "让我回顾一下瓦尼拉变压器和我们的注意力复杂性问题,它取决于输入线的平方.", "text_Arabic": "دعوني أذكر المحول الفانيليا ونا وناقش تعقيد الانتباه الذي يعتمد على مربع خط الإدخال.", "text_French": "Laissez-moi rappeler le transformateur vanille et notre et son problème de sa complexité d'attention qui dépend du carré de la ligne d'entrée.", "text_Japanese": "入力線の正方形に依存する注意の複雑さの問題です. 注意の複雑さは,入力線の正方形に依存する.", "text_Russian": "Позвольте мне вспомнить ванильный трансформатор и наш и его вопрос о его сложности внимания, который зависит от квадрата входной линии."}
{"text": "In the vanilla transformer, with full attention connectivity, relations of each token to every other token have to be calculated.\n", "text_Chinese": "在香草变压器中,在全注意力连接下,每个令牌与每个其他令牌的关系必须被计算出来.", "text_Arabic": "في محول الفانيليا، مع اتصال الاهتمام الكامل، يجب حساب علاقات كل رمز إلى كل رمز آخر.", "text_French": "Dans le transformateur vanille, avec une connectivité d'attention complète, les relations de chaque jeton à chaque autre jeton doivent être calculées.", "text_Japanese": "バニラトランスフォーマーでは完全な注意接続で各トークンの関係を他のトークンと計算する必要があります", "text_Russian": "В ванильном трансформаторе, с полной связностью внимания, отношения каждого токена к каждому другому токену должны быть рассчитаны."}
{"text": "The computational complexity of attention, this depends on the number of layers l, sequence length n, another sequence length, and the dimensionality of representations.\n", "text_Chinese": "注意的计算复杂性取决于层数l,序列长度n,另一个序列长度和表示的维度.", "text_Arabic": "التعقيد الحسابي للانتباه، هذا يعتمد على عدد الطبقات l، طول التسلسل n، طول التسلسل الآخر، وأبعاد التمثيلات.", "text_French": "La complexité informatique de l'attention, cela dépend du nombre de couches l, de la longueur de séquence n, d'une autre longueur de séquence et de la dimensionnalité des représentations.", "text_Japanese": "注意の計算複雑さは,レイヤーの数l,シーケンスの長さn,別のシーケンスの長さ,および表現の次元性に依存します.", "text_Russian": "Вычислительная сложность внимания зависит от количества слоев l, длины последовательности n, другой длины последовательности и размерности представлений."}
{"text": "Similarly, in the decoder's cross attention, to this picture on the right side, the only difference here is that the target tokens are attending to the input tokens in this case.\n", "text_Chinese": "同样地, 在解码器的交叉注意力, 与右边的图片相比, 唯一的区别是, 目标令牌在这种情况下是与输入令牌相配合的", "text_Arabic": "وبالمثل, في تركيز المبرمج, إلى هذه الصورة على الجانب الأيمن, الفرق الوحيد هنا هو أن الرموز المستهدفة تلبي الرموز المدخلة في هذه الحالة.", "text_French": "De même, dans l'attention croisée du décodeur, à cette image sur le côté droit, la seule différence ici est que les jetons cibles assistent aux jetons d'entrée dans ce cas.", "text_Japanese": "同じようにデコーダーのクロスアタックでは右側の画像と唯一の違いはターゲットトークンが入力トークンに注意を払っていることです", "text_Russian": "Аналогично, в перекрестном внимании декодера, на этой картинке справа, единственное различие здесь в том, что целевые токены в этом случае обслуживают входные токены."}
{"text": "Which can be seen also in this formula.\n", "text_Chinese": "这也可以在这个公式中看到.", "text_Arabic": "والتي يمكن رؤيتها أيضا في هذه الصيغة.", "text_French": "Ce qui peut être vu aussi dans cette formule.", "text_Japanese": "この式でも見ることができます.", "text_Russian": "Что можно увидеть и в этой формуле."}
{"text": "The BLEU score represents relations that have to be calculated.\n", "text_Chinese": "BLEU 得分表示必须计算的关系", "text_Arabic": "تمثل درجة BLEU العلاقات التي يجب حسابها.", "text_French": "Le score BLEU représente des relations qui doivent être calculées.", "text_Japanese": "BLEUスコアは計算する必要のある関係を表します.", "text_Russian": "Оценка BLEU представляет собой отношения, которые должны быть рассчитаны."}
{"text": "In case of the full attention, we need to calculate every relations within the input sequence.\n", "text_Chinese": "在完全注意的情况下,我们需要计算输入序列中的每个关系.", "text_Arabic": "في حالة الانتباه الكامل، نحتاج إلى حساب كل علاقات داخل تسلسل الإدخال.", "text_French": "Dans le cas de l'attention totale, nous devons calculer toutes les relations dans la séquence d'entrée.", "text_Japanese": "入力シーケンス内のすべての関係を計算する必要があります.", "text_Russian": "В случае полного внимания, нам нужно вычислить все отношения в последовательности ввода."}
{"text": "Now, we see what happens when we have a blockwise encoder that works by limiting the tokens connectivity so that they can only see other nearby tokens.\n", "text_Chinese": "现在, 我们看到当我们有一个块向编码器时会发生什么, 它通过限制令牌连接来工作, 这样他们只能看到附近的其他令牌", "text_Arabic": "الآن, نرى ما يحدث عندما يكون لدينا مدوّن في اتجاه الكتلة يعمل عن طريق الحد من اتصال الرموز بحيث يمكنهم رؤية الرموز القريبة فقط.", "text_French": "Maintenant, nous voyons ce qui se passe quand nous avons un encodeur bloc qui fonctionne en limitant la connectivité des jetons de sorte qu'ils ne peuvent voir que d'autres jetons à proximité.", "text_Japanese": "近くのトークンしか見えないように トークンの接続性を制限します ブロック方向のエンコーダーが作成されると どうなるか見てみましょう", "text_Russian": "Теперь мы видим, что происходит, когда у нас есть блок-кодер, который работает, ограничивая соединение токенов, чтобы они могли видеть только другие близлежащие токены."}
{"text": "The text is read in chunks which can drastically reduce the number of computations on the encoder side, but does not improve the decoder's cross attention as every input token is passed to the decoder anyway.\n", "text_Chinese": "文字是分块读取的,这可以大大减少编码器侧的计算数量,但不会提高解码器的交叉注意力,因为每个输入令牌都会传递给解码器.", "text_Arabic": "يتم قراءة النص في أجزاء مما يمكن أن يقلل بشكل كبير من عدد الحسابات على جانب المكشف ، ولكنه لا يحسن الاهتمام المتقاطع للمكشف حيث يتم تمرير كل رمز إدخال إلى مكشف الكود على أي حال.", "text_French": "Le texte est lu en morceaux, ce qui peut réduire considérablement le nombre de calculs du côté de l'encodeur, mais n'améliore pas l'attention croisée du décodeur, car chaque jeton d'entrée est de toute façon transmis au décodeur.", "text_Japanese": "テキストはブロックで読み取られ,エンコーダー側の計算数を大幅に減らすことができますが,すべての入力トークンがデコーダーに送られるため,デコーダーのクロスアタネスを改善しません.", "text_Russian": "Текст читается в кусках, что может резко уменьшить количество вычислений на стороне кодера, но не улучшает перекрестное внимание декодера, поскольку каждый входной токен в любом случае передается декодеру."}
{"text": "This method is often referred to as fusion in decoder.\n", "text_Chinese": "这种方法通常被称为解码器中的融合.", "text_Arabic": "غالبًا ما يشار إلى هذه الطريقة باسم الاندماج في جهاز فك التشفير.", "text_French": "Cette méthode est souvent appelée fusion dans le décodeur.", "text_Japanese": "この方法はしばしばデコーダーの融合と呼ばれます.", "text_Russian": "Этот метод часто называют слиянием в декодере."}
{"text": "The improvement here can be interpreted as changing one of the dependencies of n to another constant m representing the block size.\n", "text_Chinese": "这里改进可以被解释为将n的依赖性之一改变为另一个常数m,代表块大小.", "text_Arabic": "يمكن تفسير التحسن هنا على أنه تغيير أحد التبعيات لـ n إلى ثابت آخر m يمثل حجم الكتلة.", "text_French": "L'amélioration ici peut être interprétée comme le changement d'une des dépendances de n à une autre constante m représentant la taille du bloc.", "text_Japanese": "この改善は,nの依存関係の1つをブロックサイズを表す別の定数mに変更することとして解釈できます.", "text_Russian": "Улучшение здесь может быть интерпретировано как изменение одной из зависимостей n на другую постоянную m, представляющую размер блока."}
{"text": "Our key observation is that most tokens are irrelevant for a wide variety of tasks and can be almost completely disregarded. This is exemplified on the slide.\n", "text_Chinese": "我们的主要观察是大多数令牌对于各种各样的任务来说都不相关,几乎可以完全忽略.", "text_Arabic": "ملاحظتنا الرئيسية هي أن معظم الرموز غير ذات صلة لمجموعة واسعة من المهام ويمكن تجاهلها بالكامل تقريبًا.", "text_French": "Notre observation clé est que la plupart des jetons sont sans importance pour une grande variété de tâches et peuvent être presque complètement ignorés. Ceci est illustré sur la diapositive.", "text_Japanese": "主要な観察はほとんどのトークンは様々なタスクに無関係でありほぼ完全に無視することができるということですこれはスライドで例示されています", "text_Russian": "Наше ключевое наблюдение заключается в том, что большинство токенов не имеют значения для широкого спектра задач и могут быть почти полностью проигнорированы. Это показано на слайде."}
{"text": "The only parts of the inputs are relevant to the desired output.\n", "text_Chinese": "只有输入的部分与所需输出相关.", "text_Arabic": "الأجزاء الوحيدة من المدخلات ذات صلة بالنواتج المرجوة.", "text_French": "Seules les parties des intrants sont pertinentes pour la sortie souhaitée.", "text_Japanese": "必要な出力には,入力の部分だけが関連しています.", "text_Russian": "Единственные части входов относятся к желаемому выходу."}
{"text": "For example.\n", "text_Chinese": "例如", "text_Arabic": "على سبيل المثال.", "text_French": "Par exemple.", "text_Japanese": "例えば", "text_Russian": "Например."}
{"text": "One can read an article once marking the most important parts with a highlighter, and then produce a summary based on this part from the middle stage only.\n", "text_Chinese": "一个人可以读一篇文章一次,用高光标记最重要的部分,然后根据中间阶段的这个部分编写一个总结.", "text_Arabic": "يمكن للمرء أن يقرأ مقالًا مرة واحدة مع تحديد أهم الأجزاء بعلامة تمييز ، ثم ينتج ملخصًا يعتمد على هذا الجزء من المرحلة الوسطى فقط.", "text_French": "On peut lire un article une fois en marquant les parties les plus importantes avec un surligneur, puis produire un résumé basé sur cette partie à partir de la phase intermédiaire seulement.", "text_Japanese": "一度記事を読んで 最も重要な部分をハイライトでマークし, 中間段階からこの部分のみをベースに要約を作成できます.", "text_Russian": "Можно прочитать статью один раз, отметив наиболее важные части с помощью подсвечника, а затем составить резюме, основанное только на этой части с средней стадии."}
{"text": "The cost of highlighting and deciding if the current token is essential to produce the summary is thus cheap and depends only on the token's representation.\n", "text_Chinese": "因此,突出和决定当前令牌是否对生成摘要至关重要的成本是廉价的,并且仅取决于令牌的表示.", "text_Arabic": "وبالتالي فإن تكلفة تسليط الضوء والبت في ما إذا كانت الرمز الحالي ضروري لإنتاج الملخص رخيصة وتعتمد فقط على تمثيل الرمز.", "text_French": "Le coût de la mise en évidence et de la décision de savoir si le jeton actuel est essentiel pour produire le résumé est donc bon marché et ne dépend que de la représentation du jeton.", "text_Japanese": "概要を作成するために現在のトークンが不可欠であるかどうかを強調し,決定するコストは,トークンの表現にのみ依存し,安価です.", "text_Russian": "Таким образом, стоимость выделения и принятия решения о том, является ли текущий токен необходимым для создания резюме, дешева и зависит только от представления токена."}
{"text": "The pooling of the highlighted tokens is possible.\n", "text_Chinese": "突出显示的令牌可以组合在一起.", "text_Arabic": "تجميع الرموز المميزة ممكن.", "text_French": "La mise en commun des jetons mis en évidence est possible.", "text_Japanese": "ハイライトされたトークンのプール化は可能です.", "text_Russian": "Объединение выделенных токенов возможно."}
{"text": "Thanks to our top k operator and its cost is negligible.\n", "text_Chinese": "多亏了我们的顶部k操作员,它的成本是微不足道的.", "text_Arabic": "بفضل مشغل k الأعلى لدينا وتكلفته لا تُذكر.", "text_French": "Grâce à notre opérateur top k et son coût est négligeable.", "text_Japanese": "そのコストは無視できるものです.", "text_Russian": "Благодаря нашему топ-оператору k и его стоимость незначительна."}
{"text": "The cost of producing a summary from a shortened input is also much lower than in the vanilla model when the whole input is considered.\n", "text_Chinese": "从缩短的输入中制作摘要的成本也比在整个输入中考虑的瓦尼拉模型要低得多.", "text_Arabic": "تكلفة إنتاج ملخص من مدخلات مختصرة هي أيضا أقل بكثير من النموذج الفانيليا عندما يتم النظر في المدخلات بأكملها.", "text_French": "Le coût de production d'un résumé à partir d'un input raccourci est également beaucoup plus faible que dans le modèle vanille lorsque l'ensemble des intrants est pris en compte.", "text_Japanese": "短縮された入力から要約を作成するコストも,全体の入力が考慮されるときのバニラモデルよりもはるかに低い.", "text_Russian": "Стоимость подготовки резюме из сокращенного ввода также намного ниже, чем в ванильной модели, когда рассматривается весь вход."}
{"text": "But here's a question.\n", "text_Chinese": "但有一个问题", "text_Arabic": "ولكن إليك سؤال.", "text_French": "Mais voici une question.", "text_Japanese": "しかし質問があります", "text_Russian": "Но вот вопрос."}
{"text": "How to select important tokens and backpropagate gradients to that selection?\n", "text_Chinese": "如何选择重要的令牌并向该选择反向传播梯度?", "text_Arabic": "كيف نختار الرموز المهمة وننتقل إلى الخلف إلى هذا الاختيار؟", "text_French": "Comment sélectionner des jetons importants et rétropropager les gradients à cette sélection?", "text_Japanese": "重要なトークンを選択し,その選択にグラデイントをバックプロパゲートする方法?", "text_Russian": "Как выбрать важные токены и обратно распространить градиенты к этому выбору?"}
{"text": "The essential underlying problem that we solve is to propose the trainable selection mechanism.\n", "text_Chinese": "我们解决的基本基本问题是提出可训练的选择机制.", "text_Arabic": "المشكلة الأساسية الأساسية التي نحلها هي اقتراح آلية الاختيار القابلة للتدريب.", "text_French": "Le problème fondamental que nous résolvons est de proposer le mécanisme de sélection entrainable.", "text_Japanese": "私たちが解決する基本的な問題は,訓練可能な選択メカニズムを提案することです.", "text_Russian": "Основная проблема, которую мы решаем, заключается в том, чтобы предложить обучаемый механизм отбора."}
{"text": "One that can allow for gradient to be back propagated during the training so that the network can learn to select the most important tokens.\n", "text_Chinese": "一个可以允许梯度在训练期间被反向传播,以便网络可以学习选择最重要的代币.", "text_Arabic": "واحد يمكن أن يسمح للتدرج أن يتم نشر مرة أخرى خلال التدريب بحيث يمكن للشبكة أن تتعلم لاختيار أهم الرموز.", "text_French": "Un qui peut permettre au gradient d'être propagé pendant l'entraînement afin que le réseau puisse apprendre à sélectionner les jetons les plus importants.", "text_Japanese": "ネットワークが最も重要なトークンを選択することを学ぶことができます. ネットワークは最も重要なトークンを選択することができます.", "text_Russian": "Один, который может позволить градиенту быть обратно распространенным во время обучения, чтобы сеть могла научиться выбирать самые важные токены."}
{"text": "More precisely\n", "text_Chinese": "更准确地说", "text_Arabic": "بدقة أكبر", "text_French": "Plus précisément", "text_Japanese": "より正確に", "text_Russian": "Точнее"}
{"text": "Given some embeddings underscore obtained from a simple linear layer, the task is to return the highest scoring embeddings. First, the sequence is permuted and pairs are prepared so that the higher scoring vector is taken with the lower scoring one.\n", "text_Chinese": "给定一些嵌入从一个简单的线性层获得的下标,任务是返回得分最高的嵌入. 首先,序列被排列并准备对,以便以得分较高的向量与得分较低的向量.", "text_Arabic": "بالنظر إلى بعض التضمينات التي تم الحصول عليها من طبقة خطية بسيطة ، فإن المهمة هي إرجاع التضمينات ذات أعلى درجة. أولاً ، يتم تبديل التسلسل وإعداد الأزواج بحيث يتم أخذ متجه التقييم الأعلى مع المتجه التقييم الأدنى.", "text_French": "Étant donné que certains embeddings soulignent obtenus à partir d'un simple calque linéaire, la tâche consiste à renvoyer les embeddings ayant le score le plus élevé. Tout d'abord, la séquence est permutée et des paires sont préparées de sorte que le vecteur ayant le score le plus élevé est pris avec le vecteur ayant le score le plus bas.", "text_Japanese": "単純な線形層から得られたいくつかの埋め込みの下線を考えると,最も高いスコアの埋め込みを返すのがタスクです.まず,配列を交替し,より高いスコアのベクトルと低いスコアのベクトルを取るようにペアを準備します.", "text_Russian": "Учитывая некоторые встраивания, полученные из простого линейного слоя, задача состоит в том, чтобы вернуть встраивания с самым высоким баллом. Сначала последовательность пермутируется и пары подготавливаются так, чтобы вектор с более высоким баллом был взят с вектором с более низким баллом."}
{"text": "Next, weights are calculated using boosted softmax over scores.\n", "text_Chinese": "接下来,使用增强的软最大分数计算权重.", "text_Arabic": "بعد ذلك ، يتم حساب الأوزان باستخدام softmax المعزز على الدرجات.", "text_French": "Ensuite, les poids sont calculés en utilisant softmax boosté sur les scores.", "text_Japanese": "次に,スコアを上昇したソフトマックスを使用して重量を計算します.", "text_Russian": "Далее веса рассчитываются с использованием усиленного softmax над баллами."}
{"text": "After each tournament round, new vectors and scores are composed as a linear combination of those pairs with the obtained weights.\n", "text_Chinese": "在每个比赛轮之后,新的向量和分数被组成为这些对的线性组合,并获得权重.", "text_Arabic": "بعد كل جولة من دورات البطولة، يتم تكوين متجهات جديدة ودرجات كتركيبة خطية من تلك الأزواج مع الأوزان المتحصل عليها.", "text_French": "Après chaque tour de tournoi, de nouveaux vecteurs et scores sont composés comme une combinaison linéaire de ces paires avec les poids obtenus.", "text_Japanese": "各トーナメントラウンドの後,新しいベクトルとスコアは,得られた重量とこれらのペアの線形組み合わせとして構成されます.", "text_Russian": "После каждого раунда турнира новые векторы и баллы составляются как линейная комбинация этих пар с полученными весами."}
{"text": "So in short, we combine them linearly by performing a softmax over their scores.\n", "text_Chinese": "简而言之,我们通过对他们的分数进行软最大值来线性地组合它们.", "text_Arabic": "باختصار, نقوم بدمجهم بشكل خطي عن طريق إجراء تقييم لينة على نتائجهم.", "text_French": "Donc, en bref, nous les combinons linéairement en effectuant un softmax sur leurs scores.", "text_Japanese": "簡単に言えばスコアの上にソフトマックスを実行することで線形で組み合わせます", "text_Russian": "Короче говоря, мы объединяем их линейно, выполняя softmax над их результатами."}
{"text": "And while combining two tokens, some noise can be produces produced.\n", "text_Chinese": "在组合两个令牌时,可能会产生一些噪音.", "text_Arabic": "وبينما يجمع بين توكنين، يمكن إنتاج بعض الضوضاء.", "text_French": "Et tout en combinant deux jetons, un certain bruit peut être produit.", "text_Japanese": "2つのトークンを組み合わせると 音が生じます", "text_Russian": "И при сочетании двух токенов может быть произведен некоторый шум."}
{"text": "But it also allows the gradients to be propagated to all input embeddings.\n", "text_Chinese": "但它也允许梯度传播到所有输入嵌入.", "text_Arabic": "لكنه يسمح أيضًا بنشر التدرج إلى جميع تضمينات المدخلات.", "text_French": "Mais il permet également aux gradients d'être propagés à tous les empilements d'entrée.", "text_Japanese": "しかしそれはまたグラディエントをすべての入力埋め込みに伝播することを可能にします", "text_Russian": "Но это также позволяет градиентам распространяться на все входные встраивания."}
{"text": "In short, a trainable top k we propose is based on performing a tournament like soft selection at each step.\n", "text_Chinese": "简而言之,我们提出的可训练的顶部k是基于在每个步骤上执行像软选择的比赛.", "text_Arabic": "باختصار، قائمة k الأعلى القابلة للتدريب التي نقترحها تعتمد على أداء بطولة مثل الاختيار الناعم في كل خطوة.", "text_French": "En bref, un top k entrainable que nous proposons est basé sur l'exécution d'un tournoi comme une sélection douce à chaque étape.", "text_Japanese": "簡単に言えばトレーニング可能なトップkは各ステップでソフトセレクションのようなトーナメントを実行することに基づいています", "text_Russian": "Короче говоря, тренируемый топ k, который мы предлагаем, основан на проведении турнира, подобного мягкому отбору на каждом этапе."}
{"text": "And from a different perspective, the representation pooling follows the encoder layer.\n", "text_Chinese": "从不同的角度来看,表示池跟随编码层.", "text_Arabic": "ومن منظور مختلف، فإن تجميع التمثيل يتبع طبقة المكوّن.", "text_French": "Et d'un point de vue différent, le pooling de représentation suit la couche d'encodeur.", "text_Japanese": "別の視点から見ると 表現プールはエンコーダー層に沿っています", "text_Russian": "И с другой точки зрения, объединение представлений следует за слоем кодера."}
{"text": "First, each representation is scored and then only those with the highest scores are passed to the next layer.\n", "text_Chinese": "首先,每个表示都得到分数,然后只有得分最高的表示才会通过到下一层.", "text_Arabic": "أولاً، يتم تقييم كل تمثيل ثم يتم تمرير أولئك الذين لديهم أعلى درجات فقط إلى الطبقة التالية.", "text_French": "Tout d'abord, chaque représentation est notée et ensuite seulement ceux avec les scores les plus élevés sont passés à la couche suivante.", "text_Japanese": "まず,各表現がスコアされ,最高スコアのものだけが次の層に渡されます.", "text_Russian": "Сначала каждое представление оценивается, а затем только те, которые имеют самые высокие баллы, переходят на следующий уровень."}
{"text": "Encoding can be performed as in standard transformer architecture on the full length input.\n", "text_Chinese": "在全长输入上可以像标准变压器架构一样执行编码.", "text_Arabic": "يمكن إجراء التشفير كما هو الحال في بنية المحول القياسية على مدخل الطول الكامل.", "text_French": "L'encodage peut être effectué comme dans l'architecture de transformateur standard sur l'entrée de longueur complète.", "text_Japanese": "エンコーディングは,標準のトランスフォーマーアーキテクチャのように,全長入力で実行できます.", "text_Russian": "Кодирование может быть выполнено как в стандартной архитектуре трансформатора на входе полной длины."}
{"text": "It is however possible to process text in blocks of fixed length of fixed length and globally select the best representation.\n", "text_Chinese": "然而,可以将文本处理为固定长度的固定长度的块,并全面选择最佳表示.", "text_Arabic": "ومع ذلك، من الممكن معالجة النص في كتل ذات طول ثابت و اختيار أفضل تمثيل بشكل عالمي.", "text_French": "Il est cependant possible de traiter le texte en blocs de longueur fixe de longueur fixe et de sélectionner globalement la meilleure représentation.", "text_Japanese": "しかし,固定長のブロックでテキストを処理し,全体的に最適な表現を選択することができます.", "text_Russian": "Тем не менее, можно обрабатывать текст в блоках фиксированной длины и в целом выбирать лучшее представление."}
{"text": "Here is an example of the representation pooling introduced after the encoder.\n", "text_Chinese": "这里是一个例子,在编码器之后引入的表示池.", "text_Arabic": "إليك مثال على تجميع التمثيل المقدم بعد المكوّن.", "text_French": "Voici un exemple de regroupement de représentation introduit après l'encodeur.", "text_Japanese": "ここでは,エンコーダー後に導入された表現プールの例を示します.", "text_Russian": "Вот пример объединения представлений, введенный после кодера."}
{"text": "This directly influenced the cause of cross attention, which depends not on the input length N, but the constant K, representing the pooled length.\n", "text_Chinese": "这直接影响了交叉注意力的原因,它不是取决于输入长度N,而是代表聚合长度的常数K.", "text_Arabic": "هذا أثر بشكل مباشر على سبب الاهتمام المتقاطع ، الذي لا يعتمد على طول المدخل N ، ولكن ثابت K ، الذي يمثل الطول المجمع.", "text_French": "Cela a directement influencé la cause de l'attention croisée, qui ne dépend pas de la longueur d'entrée N, mais de la constante K, représentant la longueur regroupée.", "text_Japanese": "これは,入力長Nではなく,集合長を表す定数Kに依存するクロスアタックの原因に直接影響を与えました.", "text_Russian": "Это непосредственно повлияло на причину перекрестного внимания, которое зависит не от входной длины N, а от константы K, представляющей объединенную длину."}
{"text": "This constant informs how many representations are selected and passed to the decoder.\n", "text_Chinese": "这个常数告诉多少表示被选择并传递给解码器.", "text_Arabic": "يخبر هذا الثابت عدد التمثيلات التي يتم اختيارها وتمريرها إلى جهاز فك التشفير.", "text_French": "Cette constante indique le nombre de représentations sélectionnées et transmises au décodeur.", "text_Japanese": "この定数が,どれだけの表現が選択され,デコーダーに渡されるかを伝えます.", "text_Russian": "Эта константа сообщает, сколько представлений выбирается и передается декодеру."}
{"text": "Producing a summary from a shorter text is significantly cheaper than previous solution.\n", "text_Chinese": "从较短的文本中制作摘要比以前的解决方案便宜得多.", "text_Arabic": "إنتاج ملخص من نص أقصر أرخص بكثير من الحل السابق.", "text_French": "La production d'un résumé à partir d'un texte plus court est nettement moins chère que la solution précédente.", "text_Japanese": "短いテキストから要約を作成することは,以前の解決策よりも大幅に安価です.", "text_Russian": "Составление резюме из более короткого текста значительно дешевле, чем предыдущее решение."}
{"text": "As the sequence length can be shortened by a large factor.\n", "text_Chinese": "因为序列长度可以缩短一个大的因子.", "text_Arabic": "حيث يمكن تقصير طول التسلسل بمقدار عامل كبير.", "text_French": "Comme la longueur de la séquence peut être raccourcie d'un facteur important.", "text_Japanese": "配列の長さは大きな因子で短縮できます", "text_Russian": "Поскольку длина последовательности может быть сокращена большим фактором."}
{"text": "For example, we successfully used k of sixteen or even sixty times four or even sixty four times smaller than the value of n in our experiments.\n", "text_Chinese": "例如,我们在实验中成功地使用了 k 的十六,甚至是六十乘以四,甚至是六十四倍小于 n 的值", "text_Arabic": "على سبيل المثال، استخدمنا بنجاح k من ستة عشر أو حتى ستين مرة أربعة أو حتى أربعة وستين مرة أصغر من قيمة n في تجاربنا.", "text_French": "Par exemple, nous avons utilisé avec succès k de seize ou même soixante fois quatre ou même soixante-quatre fois plus petit que la valeur de n dans nos expériences.", "text_Japanese": "例えば私たちは実験でnの値より16倍または64倍小さいkを成功裏に使用しました", "text_Russian": "Например, мы успешно использовали k из шестнадцати или даже шестьдесят раз четыре или даже шестьдесят четыре раза меньше, чем значение n в наших экспериментах."}
{"text": "Please note that the beneficial impact of blockwise encoding and self attention is sustained.\n", "text_Chinese": "请注意,块向编码和自我注意的有益影响是持续的.", "text_Arabic": "يرجى ملاحظة أن التأثير المفيد للتشفير في اتجاه الكتلة والاهتمام الذاتي مستمر.", "text_French": "Veuillez noter que l'impact bénéfique de l'encodage bloc et de l'attention personnelle est soutenu.", "text_Japanese": "ブロック方向のエンコーディングと自己注意の有益な影響は持続していることに注意してください.", "text_Russian": "Обратите внимание, что благотворное влияние блокового кодирования и самовнимания сохраняется."}
{"text": "Remember that the computational cost of attention depend on the square of the input length.\n", "text_Chinese": "记住,注意力的计算成本取决于输入长度的平方.", "text_Arabic": "تذكر أن التكلفة الحسابية للانتباه تعتمد على مربع طول المدخلات.", "text_French": "N'oubliez pas que le coût de calcul de l'attention dépend du carré de la longueur d'entrée.", "text_Japanese": "注意の計算コストは 入力長の正方形に依存することを覚えておいてください", "text_Russian": "Помните, что вычислительная стоимость внимания зависит от квадрата длины ввода."}
{"text": "Reducing it the input earlier during the encoding process can significantly lower the costs.\n", "text_Chinese": "在编码过程的早期减少输入可以显著降低成本.", "text_Arabic": "يمكن تقليل المدخلات في وقت سابق خلال عملية التشفير بشكل كبير لتخفيض التكاليف.", "text_French": "La réduction de l'entrée plus tôt pendant le processus d'encodage peut réduire considérablement les coûts.", "text_Japanese": "暗号化プロセスの初期に入力量を減らすことで 費用を大幅に削減できます", "text_Russian": "Сокращение ввода ранее в процессе кодирования может значительно снизить затраты."}
{"text": "For the pyramidion model, we narrowed down the size of the representation on the output of each of each chosen layer, leading to the exponential reduction of computational cost as the encoding proceeds.\n", "text_Chinese": "对于金字塔模型, 我们缩小了每个选定的层输出的表示大小,", "text_Arabic": "بالنسبة لنموذج الهرم، قمنا بتضييق حجم التمثيل على مخرجات كل طبقة مختارة، مما أدى إلى التقليل الأسطي للتكلفة الحسابية مع تقدم التشفير.", "text_French": "Pour le modèle pyramidion, nous avons réduit la taille de la représentation sur la sortie de chaque couche choisie, conduisant à la réduction exponentielle du coût de calcul au fur et à mesure que l'encodage se poursuit.", "text_Japanese": "ピラミディオンモデルの場合選択した各層の出力による表現のサイズを狭めエンコーディングが進むにつれて計算コストが指数関数的に減少します", "text_Russian": "Для модели пирамидиона мы сузили размер представления на выходе каждого из каждого выбранного слоя, что приводит к экспоненциальному сокращению вычислительных затрат по мере продолжения кодирования."}
{"text": "As you can see, the total computational cost of a full encoder here is less than two times the cost of the full-sized first layer.\n", "text_Chinese": "正如你所看到的, 一个完整的编码器的计算成本是第一层的两倍以下", "text_Arabic": "كما ترون, إجمالي تكلفة الحوسبة للمشفر الكامل هنا أقل من ضعف تكلفة الطبقة الأولى كاملة الحجم.", "text_French": "Comme vous pouvez le voir, le coût total de calcul d'un encodeur complet ici est moins de deux fois le coût de la première couche pleine taille.", "text_Japanese": "ご覧の通りここで完全なエンコーダーの総計算コストは完全なサイズの最初のレイヤーのコストの2倍未満です", "text_Russian": "Как вы видите, общая вычислительная стоимость полного кодера здесь меньше, чем в два раза меньше, чем стоимость первого слоя полного размера."}
{"text": "When pooling is introduced earlier, the sum of all purple squares is thus bounded to a constant, not dependent on the number of layers l.\n", "text_Chinese": "当提前引入集合时,所有紫色方块的总和将被限制在恒定值,不依赖于层数", "text_Arabic": "عند إدخال التجميع في وقت سابق ، يتم تحديد مجموع جميع المربعات الأرجوانية إلى ثابت ، لا يعتمد على عدد الطبقات l.", "text_French": "Lorsque le regroupement est introduit plus tôt, la somme de tous les carrés violets est ainsi limitée à une constante, indépendante du nombre de couches l.", "text_Japanese": "プールが早期に導入されると,すべての紫の正方形の和は,レイヤーの数lに依存しない定数に制限されます.", "text_Russian": "Когда объединение вводится раньше, сумма всех фиолетовых квадратов ограничивается постоянной, не зависящей от количества слоев l."}
{"text": "But on the constant c, which can be influenced by the placing of the pooling layers within the network.\n", "text_Chinese": "但是在常数c上,它可以受到网络中聚合层的位置的影响.", "text_Arabic": "ولكن على الثابت c, والذي يمكن أن يتأثر بوضع طبقات التجميع داخل الشبكة.", "text_French": "Mais sur la constante c, qui peut être influencée par le placement des couches de pooling dans le réseau.", "text_Japanese": "ネットワーク内のプール層の配置によって 影響を受けることができます", "text_Russian": "Но на постоянную c, на которую может влиять расположение слоев объединения в сети."}
{"text": "Our improvements were benchmarked on eight thousand tokens long inputs.\n", "text_Chinese": "我们的改进是基于八千个代币长的输入.", "text_Arabic": "تم تقييم تحسيناتنا على مدخلات طولها ثمانية آلاف رمز.", "text_French": "Nos améliorations ont été comparées sur des entrées de huit mille jetons.", "text_Japanese": "私たちの改善は 8000トークン長の入力でベンチマークされました", "text_Russian": "Наши улучшения были сопоставлены с входами длиной восемь тысяч токенов."}
{"text": "And the figure shows that when pooling is engaged, the best scalability for the network's depth is achieved.\n", "text_Chinese": "图显示,当集群参与时, 网络深度的最佳可扩展性就实现了", "text_Arabic": "ويظهر الشكل أنه عندما يتم استخدام التجميع، يتم تحقيق أفضل قابلية للتوسع لعمق الشبكة.", "text_French": "Et la figure montre que lorsque le pooling est engagé, la meilleure évolutivité pour la profondeur du réseau est atteinte.", "text_Japanese": "図はプールが使用されるとネットワークの深さの最良のスケーラビリティが達成されることを示しています", "text_Russian": "И рисунок показывает, что при объединении достигается лучшая масштабируемость для глубины сети."}
{"text": "Here one can note that training the pyramidion of twenty four layers can be cheaper than training a two layer vanilla transformer on such long inputs.\n", "text_Chinese": "这里可以注意到,训练二十四层的金字塔比训练两层的香草变压器更便宜.", "text_Arabic": "هنا يمكن للمرء أن يلاحظ أن تدريب الهرم من أربعة وعشرين طبقة يمكن أن يكون أرخص من تدريب محول فانيلا من طبقتين على مثل هذه المدخلات الطويلة.", "text_French": "Ici, on peut noter que l'entraînement du pyramidion de vingt-quatre couches peut être moins cher que l'entraînement d'un transformateur vanille à deux couches sur de telles entrées longues.", "text_Japanese": "ここでは,24層のピラミディオンを訓練することは,このような長い入力で2層のバニラ変圧器を訓練するよりも安価であることに注意することができます.", "text_Russian": "Здесь можно заметить, что обучение пирамидиона из двадцати четырех слоев может быть дешевле, чем обучение двухслойного ванильного трансформатора на таких длинных входах."}
{"text": "Not to mention how easily vanilla transformer can go out of memory for such a long input.\n", "text_Chinese": "更不用说瓦尼拉变压器在这么长的输入中有多容易失去内存.", "text_Arabic": "ناهيك عن مدى سهولة تحويل الفانيليا يمكن أن يخرج من الذاكرة لمثل هذا المدخل الطويل.", "text_French": "Sans parler de la facilité avec laquelle le transformateur vanille peut se retrouver hors mémoire pour une entrée aussi longue.", "text_Japanese": "バニラ変換器が 記憶から 簡単に切り出されることを 言うまでもありません", "text_Russian": "Не говоря уже о том, как легко ванильный трансформатор может выйти из памяти для такого длительного ввода."}
{"text": "The qual quality qual qualitative comparison of our trend pyramidion to other baseline is performed on the long document summarization task, or given the body of an article from arXiv or PubMed, the task is to generate its abstract.\n", "text_Chinese": "在长文档总结任务上进行我们的趋势金字塔与其他基线的质量比较,或者给出来自arXiv或PubMed的文章的正文,任务是生成其摘要.", "text_Arabic": "تتم مقارنة نوعية هرم الاتجاه لدينا مع خط الأساس الآخر على مهمة تلخيص الوثيقة الطويلة، أو نظرا لجسم مقالة من arXiv أو PubMed، فإن المهمة هي توليد ملخصها.", "text_French": "La comparaison qualitative de notre pyramide de tendance à une autre base de référence est effectuée sur la tâche de résumé de long document, ou étant donné le corps d'un article d'arXiv ou PubMed, la tâche est de générer son résumé.", "text_Japanese": "質の質の質の質の比較は,私たちのトレンドピラミディオンと他のベースラインに長文書の要約タスクで実行されます,またはarXivまたはPubMedからの記事の本体を与えられ,タスクはその抽象を生成することです.", "text_Russian": "Качественное качественное сравнение нашей тренд-пирамиды с другой базовой линией выполняется на задаче резюме длинного документа, или, учитывая тело статьи из arXiv или PubMed, задача состоит в том, чтобы создать ее резюме."}
{"text": "Thus, one can see blockwise, which is our baseline, performs on the level of the re, recent state-of-the-art models, while the pyramidion retains or improves the performance of this competitive baseline.\n", "text_Chinese": "因此,我们可以看到,我们的基线在最近的最先进模型水平上表现良好,而金字塔则保持或改善了竞争基线的表现.", "text_Arabic": "وبالتالي، يمكن للمرء أن يرى أن الكتلة، التي هي خط الأساس لدينا، تؤدي على مستوى نماذج أحدث أحدث، في حين أن الهرم يحتفظ أو يحسن أداء هذا الخط الأساسي التنافسي.", "text_French": "Ainsi, on peut voir blockwise, qui est notre base, fonctionne au niveau des modèles ré, récents de pointe, tandis que le pyramidion conserve ou améliore la performance de cette base compétitive.", "text_Japanese": "したがって,私たちのベースラインであるブロックワイズが,最近の最先端モデルのレベルで動作するのを確認できます. 一方,ピラミディオンは,この競争力のあるベースラインの性能を維持または改善します.", "text_Russian": "Таким образом, можно увидеть, что блок, который является нашей базовой линией, работает на уровне последних современных моделей, в то время как пирамида сохраняет или улучшает производительность этой конкурентной базовой линии."}
{"text": "At the same time, our model is eighty percent faster to train and over four hundred fifty percent faster at inference when compared to the blockwise baseline.\n", "text_Chinese": "同时, 我们的模型训练速度要快 80%, 与基线的块向相比, 推断速度要快 450%", "text_Arabic": "في نفس الوقت, نموذجنا أسرع بنسبة ثمانين في المئة في التدريب وأكثر من 450 في المئة في الاستنتاج بالمقارنة مع خط الأساس في اتجاه الكتلة.", "text_French": "En même temps, notre modèle est 80% plus rapide à entraîner et plus de 450% plus rapide à inférer par rapport à la ligne de base en bloc.", "text_Japanese": "同時に私たちのモデルはブロック方向のベースラインと比較するとトレーニングが80%速く推論が450%以上速い", "text_Russian": "В то же время, наша модель на 80% быстрее тренируется и на 450% быстрее делает выводы по сравнению с базовой линией в направлении блока."}
{"text": "Both models have much lower parameter counts and were trained from scratch on the chosen tasks.\n", "text_Chinese": "这两种模型都有更低的参数数目,并且在选定的任务上从头开始训练.", "text_Arabic": "كلا النماذج لديها عدد من المعلمات أقل بكثير وتم تدريبها من الصفر على المهام المختارة.", "text_French": "Les deux modèles ont des nombres de paramètres beaucoup plus faibles et ont été formés à partir de zéro sur les tâches choisies.", "text_Japanese": "両方のモデルもパラメータ数がはるかに低く,選択されたタスクでゼロからトレーニングされました.", "text_Russian": "Обе модели имеют гораздо более низкий уровень параметров и были обучены с нуля на выбранных задачах."}
{"text": "Previous approaches to to achieve a similar performance had to use more parameters and leverage pretrained foundation foundational models and additional language pretraining objective to achieve similar performance.\n", "text_Chinese": "以前的方法是使用更多的参数,利用预训练的基础基础模型和额外的语言预训练目标来实现类似的性能.", "text_Arabic": "كان على النهج السابقة لتحقيق أداء مماثل استخدام المزيد من المعلمات والاستفادة من النماذج الأساسية المدربة مسبقاً وهدف التدريب المسبق للغة الإضافي لتحقيق أداء مماثل.", "text_French": "Les approches précédentes pour atteindre des performances similaires devaient utiliser plus de paramètres et tirer parti de modèles fondamentaux pré-formés et d'objectifs supplémentaires de pré-formation linguistique pour atteindre des performances similaires.", "text_Japanese": "以前のアプローチでは同様のパフォーマンスを達成するためにより多くのパラメータを使用し予備訓練された基礎基礎モデルと追加の言語予備訓練の目標を活用する必要がありました同様のパフォーマンスを達成するために", "text_Russian": "Предыдущие подходы для достижения аналогичной производительности должны были использовать больше параметров и использовать предварительно подготовленные фундаментальные модели и дополнительную цель предварительной языковой подготовки для достижения аналогичной производительности."}
{"text": "We invite you to read our full paper and use our GitHub code.\n", "text_Chinese": "我们邀请您阅读我们的完整论文并使用我们的GitHub代码.", "text_Arabic": "ندعوكم لقراءة ورقتنا الكاملة واستخدام شفرة GitHub.", "text_French": "Nous vous invitons à lire notre article complet et à utiliser notre code GitHub.", "text_Japanese": "完全な論文を読んで GitHubのコードを使用してください", "text_Russian": "Мы приглашаем вас прочитать нашу полную статью и использовать наш код на GitHub."}
{"text": "Thank you for watching.\n", "text_Chinese": "谢谢您观看.", "text_Arabic": "شكراً لمشاهدتكم", "text_French": "Merci d'avoir regardé.", "text_Japanese": "観てくれてありがとう", "text_Russian": "Спасибо, что смотрели."}
{"text": "Hello, this is Jiawei Zhou from Harvard University.\n", "text_Chinese": "您好,我是哈佛大学的贾伟周", "text_Arabic": "مرحبا، أنا جياوي تشو من جامعة هارفارد.", "text_French": "Hello, ici Jiawei Zhou de l'université de Harvard.", "text_Japanese": "ハロハーバード大学のJiawei Zhouです", "text_Russian": "Здравствуйте, это Цзявэй Чжоу из Гарвардского университета."}
{"text": "I am very glad to present our work on Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue.\n", "text_Chinese": "我很高兴能够介绍我们关于在线语义解析以减少任务导向对话的延迟的工作.", "text_Arabic": "يسعدني جدًا أن أقدم عملنا على تحليل الدلالات عبر الإنترنت لتقليل التأخير في الحوار الموجه نحو المهام.", "text_French": "Je suis très heureux de présenter notre travail sur l'analyse sémantique en ligne pour la réduction de la latence dans le dialogue axé sur les tâches.", "text_Japanese": "課題指向の対話における遅延削減のためのオンラインセマンティックパーシングに関する私たちの作業を紹介できてとても嬉しいです.", "text_Russian": "Я очень рад представить нашу работу по онлайн-семантическому анализу для снижения задержки в задаче-ориентированном диалоге."}
{"text": "This is joint work with Jason, Michael, Anthony and Sam from Microsoft Semantic Machines.\n", "text_Chinese": "这是与杰森,迈克尔,安东尼和萨姆的联合工作,", "text_Arabic": "هذا عمل مشترك مع جيسون ومايكل وأنتوني وسام من مايكروسوفت للآلات الدلالية.", "text_French": "C'est un travail conjoint avec Jason, Michael, Anthony et Sam de Microsoft Semantic Machines.", "text_Japanese": "これはマイクロソフト・セマンティック・マシンの ジェイソン,マイケル,アンソニー,サムとの共同研究です", "text_Russian": "Это совместная работа с Джейсоном, Майклом, Энтони и Сэмом из Microsoft Semantic Machines."}
{"text": "In task-oriented dialogue, a user interacts with the system that handles requests from user utterances usually in speaking.\n", "text_Chinese": "在面向任务的对话中,用户与处理用户言论请求的系统互动,通常是在说话中.", "text_Arabic": "في الحوار الموجه نحو المهام، يتفاعل المستخدم مع النظام الذي يتعامل مع الطلبات من كلمات المستخدم عادة في الكلام.", "text_French": "Dans le dialogue orienté sur les tâches, un utilisateur interagit avec le système qui traite les demandes des déclarations de l'utilisateur généralement en parlant.", "text_Japanese": "タスク指向の対話では,ユーザーは通常話しでユーザーの発言からの要求を処理するシステムと相互作用します.", "text_Russian": "В задаче-ориентированном диалоге пользователь взаимодействует с системой, которая обрабатывает запросы от высказываний пользователя, обычно в речи."}
{"text": "From the finish of the user utterance to the system response there is often a noticeable delay.\n", "text_Chinese": "从用户的表达结束到系统响应之间通常会有明显的延迟.", "text_Arabic": "من نهاية كلام المستخدم إلى استجابة النظام غالبًا ما يكون هناك تأخير ملحوظ.", "text_French": "De la fin de l'énoncé de l'utilisateur à la réponse du système, il y a souvent un retard notable.", "text_Japanese": "ユーザーの発言の終了からシステムの応答まで,しばしば著な遅延があります.", "text_Russian": "От окончания высказывания пользователя до ответа системы часто наблюдается заметная задержка."}
{"text": "Under the hood, the user utterance is translated into an executable program.\n", "text_Chinese": "在引擎盖下,用户的言论被翻译成可执行的程序.", "text_Arabic": "تحت الغطاء، يتم ترجمة كلام المستخدم إلى برنامج قابل للتنفيذ.", "text_French": "Sous le capot, l'énoncé de l'utilisateur est traduit en un programme exécutable.", "text_Japanese": "ハウドの下では ユーザーの発言が実行可能なプログラムに翻訳されます", "text_Russian": "Под капотом, высказывание пользователя переводится в исполняемую программу."}
{"text": "Which is then executed so that the system can respond properly.\n", "text_Chinese": "然后执行,以便系统能够正确响应.", "text_Arabic": "ثم يتم تنفيذها حتى يتمكن النظام من الاستجابة بشكل صحيح.", "text_French": "Qui est ensuite exécuté pour que le système puisse répondre correctement.", "text_Japanese": "システムが適切に反応できるように実行されます.", "text_Russian": "Который затем выполняется, чтобы система могла правильно реагировать."}
{"text": "Because the program is represented as a semantic graph that outlines the computation, where node represents a function invocation and its children are the arguments.\n", "text_Chinese": "因为程序被表示为一个语义图,它概述了计算,其中节点代表一个函数调用,它的子是参数.", "text_Arabic": "لأن البرنامج يمثل كرسوم بيانية دلالة تحدد الحساب، حيث تمثل العقدة استدعاء الدالة وأطفالها هي الحجج.", "text_French": "Parce que le programme est représenté comme un graphe sémantique qui décrit le calcul, où le nœud représente une invocation de fonction et ses enfants sont les arguments.", "text_Japanese": "ノードは関数呼び出しを表し,その子は引数です. 関数呼び出しは関数呼び出しを表します.", "text_Russian": "Потому что программа представлена как семантический график, который описывает вычисление, где узел представляет собой вызов функции, а его дети - аргументы."}
{"text": "The great nodes mark instantaneous operations, but the others are slow to execute.\n", "text_Chinese": "大节点标记即时操作, 但其他节点执行速度缓慢.", "text_Arabic": "العقد الكبيرة تشير إلى العمليات الفورية، ولكن الأخرى بطيئة في التنفيذ.", "text_French": "Les grands nœuds marquent les opérations instantanées, mais les autres sont lents à exécuter.", "text_Japanese": "大きなノードは即時操作をマークしますが 他のノードは実行が遅いです", "text_Russian": "Большие узлы отмечают мгновенные операции, но другие медленно выполняются."}
{"text": "The simple example here we show, these programs can often be more complicated graphs beyond the tree structures.\n", "text_Chinese": "在这里的简单例子中,这些程序通常可以是树结构之外的更复杂的图形.", "text_Arabic": "المثال البسيط هنا الذي نريه، هذه البرامج يمكن أن تكون في كثير من الأحيان رسومات أكثر تعقيدا وراء هياكل الشجرة.", "text_French": "L'exemple simple que nous montrons ici, ces programmes peuvent souvent être des graphiques plus compliqués au-delà des structures d'arbres.", "text_Japanese": "これらのプログラムはしばしば木構造を超えた複雑なグラフになることができます.", "text_Russian": "Простой пример здесь мы показываем, эти программы часто могут быть более сложными графами за пределами деревянных структур."}
{"text": "In this talk, we ask the question, can we start generating the program and executing it before the user even finishes the utterance so that the faster response can be achieved by the system?\n", "text_Chinese": "在这篇演讲中, 我们问一个问题, 我们能否在用户完成表达之前就开始生成程序并执行它, 这样系统就能实现更快的响应?", "text_Arabic": "في هذه المحادثة, نسأل السؤال, هل يمكننا البدء في إنشاء البرنامج وتنفيذه قبل أن ينتهي المستخدم من الكلام حتى يمكن أن يتحقق الاستجابة الأسرع من قبل النظام?", "text_French": "Dans cette conférence, nous posons la question, pouvons-nous commencer à générer le programme et l'exécuter avant même que l'utilisateur ait terminé l'expression afin que la réponse la plus rapide puisse être obtenue par le système?", "text_Japanese": "ユーザが発言を終わらせる前に プログラムを生成し 実行することができますか? システムがより速い反応を 達成できるようにします", "text_Russian": "В этом выступлении мы задаем вопрос: можем ли мы начать генерировать программу и выполнять ее до того, как пользователь закончит высказывание, чтобы система могла быстрее реагировать?"}
{"text": "This is the online prediction and decision problem.\n", "text_Chinese": "这是在线预测和决策问题.", "text_Arabic": "هذه هي مشكلة التنبؤ والقرار عبر الإنترنت.", "text_French": "C'est le problème de prédiction et de décision en ligne.", "text_Japanese": "これはオンラインの予測と決定問題です", "text_Russian": "Это проблема онлайн-предсказания и принятия решения."}
{"text": "There are a lot of others in this realm.\n", "text_Chinese": "这个世界还有很多其他的人.", "text_Arabic": "هناك الكثير من الآخرين في هذا العالم", "text_French": "Il y en a beaucoup d'autres dans ce royaume.", "text_Japanese": "この世界には他にもたくさんいる", "text_Russian": "В этом мире их много."}
{"text": "Examples include simultaneous translation where a live interpreter translates one language to another in real time, smart text auto completion to guess the user intent, and Uber pool where the drivers are sent to where they might be needed based on the predicted demand.\n", "text_Chinese": "例子包括同步翻译,即现场翻译者实时将一种语言翻译成另一种语言,智能文本自动完成以猜测用户的意图,以及Uber池,根据预测的需求,将司机发送到可能需要的地方.", "text_Arabic": "وتشمل الأمثلة الترجمة المتزامنة حيث يترجم مترجم مباشر لغة إلى أخرى في الوقت الفعلي، والإكمال التلقائي للنص الذكي لتخمين نية المستخدم، ومجمع أوبر حيث يتم إرسال السائقين إلى حيث قد يكونون مطلوبين بناءً على الطلب المتوقع.", "text_French": "Les exemples incluent la traduction simultanée où un interprète en direct traduit d'une langue à une autre en temps réel, la complétion automatique de texte intelligent pour deviner l'intention de l'utilisateur, et le pool Uber où les chauffeurs sont envoyés là où ils pourraient être nécessaires en fonction de la demande prévue.", "text_Japanese": "例としてはライブ通訳がリアルタイムで1つの言語を別の言語に翻訳する同時に翻訳ユーザーの意図を推測するためのスマートテキストの自動完了予測された需要に基づいてドライバーが必要とされる場所に送られるウーバープールなどがあります", "text_Russian": "Примеры включают синхронный перевод, где живой переводчик переводит один язык на другой в режиме реального времени, автоматическое заполнение умного текста, чтобы угадать намерение пользователя, и пул Uber, где водители отправляются туда, где они могут быть необходимы в зависимости от прогнозируемого спроса."}
{"text": "All of these scenarios have one thing in common.\n", "text_Chinese": "所有这些场景都有一个共同点.", "text_Arabic": "كل هذه السيناريوهات لها شيء واحد مشترك.", "text_French": "Tous ces scénarios ont une chose en commun.", "text_Japanese": "これらのシナリオには共通点があります", "text_Russian": "Все эти сценарии имеют одну общую черту."}
{"text": "That is, it is beneficial to make decisions before seeing all the input.\n", "text_Chinese": "也就是说,在看到所有投入之前做出决定是有益的.", "text_Arabic": "أي أنه من المفيد اتخاذ القرارات قبل رؤية كل المدخلات.", "text_French": "C'est-à-dire qu'il est bénéfique de prendre des décisions avant de voir tous les éléments.", "text_Japanese": "つまりすべての入力を見る前に決定を下すことは有益です", "text_Russian": "То есть, полезно принимать решения до того, как увидеть все входы."}
{"text": "In our case, we are going to deal with online semantic parsing, which could be expected to be challenging as we have to guess what the user might say.\n", "text_Chinese": "在我们的例子中,我们将处理在线语义解析,", "text_Arabic": "في حالتنا، سنتعامل مع التحليل الدلالي عبر الإنترنت، والذي يمكن توقع أنه سيكون صعبًا لأننا يجب أن نخمن ما قد يقوله المستخدم.", "text_French": "Dans notre cas, nous allons traiter de l'analyse sémantique en ligne, qui pourrait être difficile car nous devons deviner ce que l'utilisateur pourrait dire.", "text_Japanese": "ユーザが何を言うか推測する必要があるので 難しいでしょう ユーザが何を言うか推測する必要があります ユーザが何を言うか推測する必要があります", "text_Russian": "В нашем случае мы собираемся иметь дело с онлайн-семантическим анализом, который, как можно ожидать, будет сложным, поскольку мы должны догадаться, что может сказать пользователь."}
{"text": "And it is also underexplored with no formal evaluation metric.\n", "text_Chinese": "而且它也没有被充分探索, 没有正式的评估指标", "text_Arabic": "كما أنه غير مستكشف بشكل كاف مع عدم وجود مقياس تقييم رسمي.", "text_French": "Et il est également sous-exploré sans mesure d'évaluation formelle.", "text_Japanese": "正式な評価基準もなく 探索が不足しています", "text_Russian": "И он также недостаточно изучен без официальной метрики оценки."}
{"text": "First, let's look at how an ordinary system works.\n", "text_Chinese": "首先,让我们看看一个普通系统是如何工作的.", "text_Arabic": "أولاً، دعونا ننظر إلى كيفية عمل نظام عادي.", "text_French": "D'abord, regardons comment fonctionne un système ordinaire.", "text_Japanese": "まず普通のシステムがどのように機能するか見てみましょう", "text_Russian": "Сначала давайте посмотрим, как работает обычная система."}
{"text": "It is operating offline by parsing to the program only at the end of the user utterance.\n", "text_Chinese": "它通过在用户语句结束时对程序进行解析来离线操作.", "text_Arabic": "إنه يعمل خارج الإنترنت عن طريق تحليل البرنامج فقط في نهاية كلام المستخدم.", "text_French": "Il fonctionne hors ligne en analysant le programme uniquement à la fin de l'énoncé de l'utilisateur.", "text_Japanese": "ユーザの発言の最後にプログラムに解析することでオフラインで動作します.", "text_Russian": "Он работает в автономном режиме, анализируя программу только в конце высказывания пользователя."}
{"text": "Here, the character graph is predicted after seeing all the information.\n", "text_Chinese": "在这里,在看到所有信息后, 字符图被预测.", "text_Arabic": "هنا، يتم التنبؤ بمخطط الحروف بعد رؤية كل المعلومات.", "text_French": "Ici, le graphique de caractères est prédit après avoir vu toutes les informations.", "text_Japanese": "ここではすべての情報を見た後に文字グラフが予測されます", "text_Russian": "Здесь график символов предсказывается после просмотра всей информации."}
{"text": "In contrast, we are proposing an online system that compares at every utterance prefix.\n", "text_Chinese": "相比之下,我们提议一个在线系统,", "text_Arabic": "على النقيض من ذلك، نحن نقترح نظامًا عبر الإنترنت يقارن في كل مقدمة للحديث.", "text_French": "En revanche, nous proposons un système en ligne qui compare à chaque préfixe d'énonciation.", "text_Japanese": "対照的に私たちはオンラインシステムを提案していますこれはすべての発言の前置詞を比較します", "text_Russian": "В отличие от этого, мы предлагаем онлайн-систему, которая сравнивает при каждом префиксе высказывания."}
{"text": "For example, each time we see a new token, we predict a new graph.\n", "text_Chinese": "例如,每次我们看到一个新的令牌, 我们预测一个新的图形.", "text_Arabic": "على سبيل المثال، في كل مرة نرى رمز جديد، نتوقع رسم بياني جديد.", "text_French": "Par exemple, chaque fois que nous voyons un nouveau jeton, nous prévoyons un nouveau graphique.", "text_Japanese": "例えば 新しいトークンを見るたびに 新しいグラフを予測します", "text_Russian": "Например, каждый раз, когда мы видим новый жетон, мы предсказываем новый график."}
{"text": "Notice that there could be errors.\n", "text_Chinese": "注意可能有错误.", "text_Arabic": "لاحظ أنه قد تكون هناك أخطاء.", "text_French": "Notez qu'il peut y avoir des erreurs.", "text_Japanese": "間違いがある可能性があることに注意してください.", "text_Russian": "Обратите внимание, что могут быть ошибки."}
{"text": "At the position of at the pool party with Barack Obama, we got a graph with the right nodes on the person and the event subject, but guess the wrong timing information.\n", "text_Chinese": "在巴拉克·奥巴马的游泳池派对的位置, 我们得到了一个图, 显示了对的人和事件主题的正确节点, 但猜猜错了时间信息", "text_Arabic": "في موقع حفلة المسبح مع باراك أوباما, حصلنا على الرسم البياني مع العقد الصحيحة على الشخص وموضوع الحدث, ولكن تخمين المعلومات الزمنية الخاطئة.", "text_French": "À la position de la fête à la piscine avec Barack Obama, nous avons un graphique avec les bons nœuds sur la personne et le sujet de l'événement, mais devinez les mauvaises informations de timing.", "text_Japanese": "バラク・オバマのプールパーティーの位置では人とイベントの主題の正しいノードを持つグラフを得ましたが間違ったタイミング情報を推測します", "text_Russian": "На месте вечеринки у бассейна с Бараком Обамой, мы получили график с правильными узлами на человека и субъекта события, но угадайте неправильную информацию о времени."}
{"text": "This process goes on until we receive the full user utterance.\n", "text_Chinese": "这种过程持续到我们收到完整的用户表达.", "text_Arabic": "تستمر هذه العملية حتى نتلقى كلمة المستخدم الكاملة.", "text_French": "Ce processus se poursuit jusqu'à ce que nous recevions l'énoncé complet de l'utilisateur.", "text_Japanese": "このプロセスはユーザーからの完全な発言を受け取るまで続きます", "text_Russian": "Этот процесс продолжается, пока мы не получим полное высказывание пользователя."}
{"text": "How would this affect the execution timeline in the offline system?\n", "text_Chinese": "这会如何影响线下系统的执行时间表?", "text_Arabic": "كيف سيؤثر هذا على جدول الوقت للتنفيذ في النظام غير المباشر؟", "text_French": "Comment cela affecterait-il le calendrier d'exécution dans le système hors ligne?", "text_Japanese": "オフラインシステムの実行タイムラインに 影響するのでしょうか?", "text_Russian": "Как это повлияет на график исполнения в оффлайн-системе?"}
{"text": "We'll get the program graph at the end so that the system can start execution at this point.\n", "text_Chinese": "我们将在最后得到程序图,以便系统可以在此点开始执行.", "text_Arabic": "سنحصل على مخطط البرنامج في النهاية حتى يتمكن النظام من بدء التنفيذ في هذه النقطة.", "text_French": "Nous obtiendrons le graphique du programme à la fin afin que le système puisse commencer l'exécution à ce stade.", "text_Japanese": "システムが実行を開始できるようにプログラムグラフを最後に取得します", "text_Russian": "Мы получим график программы в конце, чтобы система могла начать выполнение в этой точке."}
{"text": "Remember that the great nodes are fast operations, so we only consider the execution timeline of the colored slow functions.\n", "text_Chinese": "记住,伟大的节点是快速的操作,所以我们只考虑彩色慢函数的执行时间线.", "text_Arabic": "تذكر أن العقد الكبيرة هي عمليات سريعة، لذلك نحن نعتبر فقط خط الوقت لتنفيذ الوظائف البطيئة الملونة.", "text_French": "N'oubliez pas que les grands nœuds sont des opérations rapides, donc nous ne considérons que la ligne de temps d'exécution des fonctions lentes colorées.", "text_Japanese": "記憶してください 偉大なノードは高速操作です したがって私たちは色の遅い関数の実行タイムラインのみを考慮します", "text_Russian": "Помните, что большие узлы - это быстрые операции, поэтому мы рассматриваем только временную линию выполнения цветных медленных функций."}
{"text": "First, these two find person functions can be executed in parallel, highlighted in white from the pink box as they have no dependency on other functions.\n", "text_Chinese": "首先,这两个找到人的函数可以并行执行,从粉红色框中以白色突出显示,因为它们不依赖于其他函数.", "text_Arabic": "أولاً، يمكن تنفيذ هاتين الوظيفتين في موازاة، مميزات باللون الأبيض من الصندوق الوردي لأنها لا تعتمد على وظائف أخرى.", "text_French": "Tout d'abord, ces deux fonctions de recherche de personne peuvent être exécutées en parallèle, en blanc dans la boîte rose car elles ne dépendent pas d'autres fonctions.", "text_Japanese": "まずこれらの2つのfind person関数は他の関数に依存しないためピンクのボックスから白で強調された並行して実行できます", "text_Russian": "Во-первых, эти две функции поиска человека могут выполняться параллельно, выделены белым цветом из розового ящика, поскольку они не зависят от других функций."}
{"text": "Next, the node create event can then get executed after obtaining results from lower level nodes and then the top function yield so the whole program is finished.\n", "text_Chinese": "接下来,节点创建事件可以在从低级节点获得结果后执行,然后顶部函数产生,因此整个程序已经完成.", "text_Arabic": "بعد ذلك، يمكن تنفيذ حدث إنشاء العقدة بعد الحصول على نتائج من العقدة من المستوى الأدنى ثم إنتاج الدالة العليا حتى ينتهي البرنامج بأكمله.", "text_French": "Ensuite, l'événement de création de nœud peut être exécuté après avoir obtenu les résultats des nœuds de niveau inférieur, puis le rendement de la fonction supérieure, de sorte que l'ensemble du programme est terminé.", "text_Japanese": "次に,ノードの作成イベントは,下位ノードの結果を取得して実行され,トップ関数が生成され,プログラム全体が終了します.", "text_Russian": "Далее, событие создания узла может быть выполнено после получения результатов от узлов более низкого уровня, а затем верхняя функция дает, поэтому вся программа закончена."}
{"text": "The execution process is strict, restricted to the program dependency structure where some operations cannot be parallelized which induces a noticeable delay.\n", "text_Chinese": "执行过程严格,仅限于程序依赖结构,其中一些操作不能并行,从而导致明显的延迟.", "text_Arabic": "عملية التنفيذ صارمة، مقيدة ببنية اعتماد البرنامج حيث لا يمكن موازي بعض العمليات مما يؤدي إلى تأخير ملحوظ.", "text_French": "Le processus d'exécution est strict, limité à la structure de dépendance du programme où certaines opérations ne peuvent pas être parallélisées, ce qui induit un retard notable.", "text_Japanese": "実行プロセスは厳格で,プログラム依存性構造に制限され,いくつかの操作が並列化されないため,著な遅延が発生します.", "text_Russian": "Процесс выполнения строг, ограничен структурой зависимости программы, где некоторые операции не могут быть параллелизированы, что вызывает заметную задержку."}
{"text": "In our online system, where we predict as we go, the program execution can start earlier.\n", "text_Chinese": "在我们的在线系统中, 我们可以随时预测, 程序执行可以更早开始", "text_Arabic": "في نظامنا عبر الإنترنت, حيث نتوقع أثناء المضي قدما, يمكن تنفيذ البرنامج أن يبدأ في وقت سابق.", "text_French": "Dans notre système en ligne, où nous prévoyons au fur et à mesure, l'exécution du programme peut commencer plus tôt.", "text_Japanese": "オンラインシステムでは 予測を進めると プログラムの実行が早く始まります", "text_Russian": "В нашей онлайн-системе, где мы предсказываем, как мы идем, выполнение программы может начаться раньше."}
{"text": "Here, at the prefix after Obama we predict confidently that the find person function should be in the program, but the rest may contain errors as they are grayed out.\n", "text_Chinese": "在这里,在奥巴马后的前<unk>中,我们自信地预测找到人的函数应该在程序中,但其余的可能包含错误,因为它们是灰色的.", "text_Arabic": "هنا، في الوصلة السابقة بعد أوباما نتنبأ بثقة بأن وظيفة العثور على شخص يجب أن تكون في البرنامج، ولكن الباقي قد يحتوي على أخطاء لأنها رمادية.", "text_French": "Ici, au préfixe après Obama, nous prédisons avec confiance que la fonction find person devrait être dans le programme, mais le reste peut contenir des erreurs car elles sont grisées.", "text_Japanese": "プログラムに含まれるべきですが,残りの部分は灰色に塗られているので,エラーが含まれる可能性があります.", "text_Russian": "Здесь, в префиксе после Обамы, мы уверенно предсказываем, что функция поиска человека должна быть в программе, но остальные могут содержать ошибки, поскольку они вычеркнуты."}
{"text": "The execution of the node can be immediately started as a step.\n", "text_Chinese": "节点的执行可以立即作为一步开始.", "text_Arabic": "يمكن بدء تنفيذ العقدة على الفور كخطوة.", "text_French": "L'exécution du nœud peut être immédiatement démarrée en tant qu'étape.", "text_Japanese": "ノードの実行はすぐにステップとして開始できます.", "text_Russian": "Исполнение узла может быть немедленно начато в качестве шага."}
{"text": "Then, with more tokens, we predict a totally new graph, but part of it has already being executed.\n", "text_Chinese": "然后,用更多的令牌, 我们预测一个完全新的图, 但部分已经被执行.", "text_Arabic": "ثم، مع المزيد من الرموز، نتوقع رسم بياني جديد تماما، ولكن جزء منه قد تم تنفيذه بالفعل.", "text_French": "Puis, avec plus de jetons, nous prédisons un graphique totalement nouveau, mais une partie de celui-ci a déjà été exécutée.", "text_Japanese": "そしてより多くのトークンで私たちは全く新しいグラフを予測しますがその一部はすでに実行されています", "text_Russian": "Затем, с большим количеством токенов, мы предсказываем совершенно новый график, но часть его уже выполнена."}
{"text": "So, we only need to consider the rest of the nodes that we are confident about as well.\n", "text_Chinese": "所以,我们只需要考虑我们有信心的其他节点.", "text_Arabic": "لذلك، نحن بحاجة فقط للنظر في بقية العقد التي نحن واثقون عنها كذلك.", "text_French": "Donc, nous n'avons qu'à considérer le reste des nœuds dont nous sommes sûrs.", "text_Japanese": "確実に確認できるノードの残りを 考慮する必要があります", "text_Russian": "Так что нам нужно рассмотреть только остальные узлы, в которых мы уверены."}
{"text": "Here, another find person can be executed in parallel.\n", "text_Chinese": "在这里,另一个发现的人可以同时被处决.", "text_Arabic": "هنا، يمكن تنفيذ شخص آخر في الوقت نفسه.", "text_French": "Ici, une autre personne trouvée peut être exécutée en parallèle.", "text_Japanese": "ここでは,別の発見者が並行して処刑されることができます.", "text_Russian": "Здесь, другой человек может быть казнен параллельно."}
{"text": "Again, we may have wrong predictions.\n", "text_Chinese": "我们可能有错误的预测.", "text_Arabic": "مرة أخرى، قد يكون لدينا تنبؤات خاطئة.", "text_French": "Encore une fois, nous pouvons avoir de mauvaises prédictions.", "text_Japanese": "また私たちは間違った予測を持っているかもしれません", "text_Russian": "Опять же, у нас могут быть неправильные предсказания."}
{"text": "With more text, we have more ability to make it right.\n", "text_Chinese": "有更多的文字,我们有更多的能力来把它做对.", "text_Arabic": "مع المزيد من النص، لدينا المزيد من القدرة على تصحيح الأمر.", "text_French": "Avec plus de texte, nous avons plus de capacité à le corriger.", "text_Japanese": "テキストが多いと 正しく書く能力が増えます", "text_Russian": "С большим количеством текста у нас больше возможностей исправить это."}
{"text": "Such as the event time here where AM is also anticipated correctly.\n", "text_Chinese": "例如这里的事件时间, AM 也被正确预测.", "text_Arabic": "مثل وقت الحدث هنا حيث يتم توقع AM أيضًا بشكل صحيح.", "text_French": "Comme l'heure de l'événement ici où AM est également anticipé correctement.", "text_Japanese": "AMも正しく予測されています. AMも正しく予測されています.", "text_Russian": "Например, время события здесь, где AM также правильно ожидается."}
{"text": "Then, we can start executing the rest following the program dependency structure.\n", "text_Chinese": "然后,我们可以开始执行其余的程序依靠结构.", "text_Arabic": "ثم، يمكننا البدء في تنفيذ الباقي باتباع بنية اعتماد البرنامج.", "text_French": "Ensuite, nous pouvons commencer à exécuter le reste en suivant la structure de dépendance du programme.", "text_Japanese": "プログラムの依存性構造に従って残りを実行し始めます", "text_Russian": "Затем мы можем начать выполнять остальные действия, следуя структуре зависимости программы."}
{"text": "By overlapping the execution timeline with the utterance timeline, we save a big amount of time.\n", "text_Chinese": "通过将执行时间线与表达时间线重叠, 我们节省了大量的时间.", "text_Arabic": "من خلال تداخل خط الوقت للتنفيذ مع خط الوقت للحديث، نضمن كمية كبيرة من الوقت.", "text_French": "En chevauchant la chronologie de l'exécution avec la chronologie de l'énoncé, nous économisons beaucoup de temps.", "text_Japanese": "実行タイムラインと発言タイムラインを重なって 膨大な時間を節約します", "text_Russian": "Перекрывая временную шкалу исполнения с временной шкалой высказывания, мы экономим много времени."}
{"text": "So we proposed the task of online semantic parsing.\n", "text_Chinese": "所以我们提出了一个在线语义解析的任务", "text_Arabic": "لذلك اقترحنا مهمة التحليل الدلالي عبر الإنترنت.", "text_French": "Nous avons donc proposé la tâche de l'analyse sémantique en ligne.", "text_Japanese": "オンラインの意味解析の課題を提案しました", "text_Russian": "Поэтому мы предложили задачу онлайн-семантического анализа."}
{"text": "One underlying assumption is that the execution time dominates the model prediction time.\n", "text_Chinese": "一个基本的假设是执行时间主导模型预测时间.", "text_Arabic": "أحد الافتراضات الأساسية هو أن وقت التنفيذ يهيمن على وقت التنبؤ بالنموذج.", "text_French": "Une hypothèse sous-jacente est que le temps d'exécution domine le temps de prédiction du modèle.", "text_Japanese": "基本的な仮定の1つは,実行時間がモデル予測時間を支配することです.", "text_Russian": "Одно из основных предположений заключается в том, что время выполнения доминирует над временем прогнозирования модели."}
{"text": "So we could only gain time by predicting earlier.\n", "text_Chinese": "所以我们只能通过提前预测来获得时间.", "text_Arabic": "لذا يمكننا فقط كسب الوقت من خلال التنبؤ في وقت مبكر.", "text_French": "Nous ne pouvions gagner du temps qu'en prédisant plus tôt.", "text_Japanese": "予測を早くすることで 時間を稼ぐしかありませんでした", "text_Russian": "Поэтому мы могли выиграть время, предсказав раньше."}
{"text": "Another assumption is that as the prediction and execution happen in the background, that it is not visible to users.\n", "text_Chinese": "另一个假设是,由于预测和执行发生在后台,因此用户看不见.", "text_Arabic": "افتراض آخر هو أنه نظرًا لأن التنبؤ والتنفيذ يحدثان في الخلفية ، فإنه لا يكون مرئيًا للمستخدمين.", "text_French": "Une autre hypothèse est que la prédiction et l'exécution se produisent en arrière-plan, ce qui ne est pas visible pour les utilisateurs.", "text_Japanese": "もう一つの仮定は 予測と実行が 背景で起こるので ユーザーには見えないことです", "text_Russian": "Другое предположение заключается в том, что, поскольку прогнозирование и выполнение происходят в фоновом режиме, они не видны пользователям."}
{"text": "It is not necessary to maintain a consistent parsing history.\n", "text_Chinese": "不需要保持一致的解析历史.", "text_Arabic": "ليس من الضروري الحفاظ على تاريخ تحليل متسق.", "text_French": "Il n'est pas nécessaire de maintenir un historique d'analyse cohérent.", "text_Japanese": "一貫した解析履歴を維持する必要はありません.", "text_Russian": "Не обязательно поддерживать последовательную историю анализа."}
{"text": "So, we reparse from scratch after each token.\n", "text_Chinese": "所以,在每个令牌之后,我们从头开始重新分配.", "text_Arabic": "لذا، نعيد توزيع الرموز من الصفر بعد كل رمز.", "text_French": "Donc, nous repartageons à partir de zéro après chaque jeton.", "text_Japanese": "すべてのトークンの後ゼロから再配分します", "text_Russian": "Итак, мы перерабатываем с нуля после каждого жетона."}
{"text": "In particular, we propose a two step approach.\n", "text_Chinese": "特别是,我们建议采取两步的方法.", "text_Arabic": "على وجه الخصوص، نقترح نهجا من خطوتين.", "text_French": "En particulier, nous proposons une approche en deux étapes.", "text_Japanese": "特に私たちは2段階のアプローチを提案します", "text_Russian": "В частности, мы предлагаем двухэтапный подход."}
{"text": "A proposed step that predicts a graph with complete structure and a select step that selects the nodes that are worth executing at this time.\n", "text_Chinese": "一个提议的步骤预测一个完整结构的图形,一个选择步骤选择此时值得执行的节点.", "text_Arabic": "خطوة مقترحة تتنبأ بجراف مع بنية كاملة وخطوة اختيار تختار العقد التي تستحق التنفيذ في هذا الوقت.", "text_French": "Une étape proposée qui prédit un graphique avec une structure complète et une étape de sélection qui sélectionne les nœuds qui valent la peine d'être exécutés à ce moment-là.", "text_Japanese": "完全な構造のグラフを予測する提案されたステップと,この時点で実行する価値のあるノードを選択する選択ステップです.", "text_Russian": "Предлагаемый шаг, который предсказывает график с полной структурой, и выборный шаг, который выбирает узлы, которые стоит выполнить в это время."}
{"text": "We had two variants of the proposed method.\n", "text_Chinese": "我们有两种变体的方法.", "text_Arabic": "كان لدينا نوعان من الطريقة المقترحة.", "text_French": "Nous avions deux variantes de la méthode proposée.", "text_Japanese": "提案された方法には2つの変形がありました", "text_Russian": "У нас было два варианта предложенного метода."}
{"text": "First approach combines a language model completion with full utterance to graph parsing.\n", "text_Chinese": "第一个方法将语言模型完成与图形解析的完整表达相结合.", "text_Arabic": "النهج الأول يجمع بين إكمال نموذج اللغة مع التعبير الكامل لتحليل الرسم البياني.", "text_French": "La première approche combine l'achèvement d'un modèle de langage avec une énonciation complète pour l'analyse graphique.", "text_Japanese": "最初のアプローチは,言語モデルを完了し,グラフ解析に完全な発言を組み合わせます.", "text_Russian": "Первый подход сочетает в себе завершение языковой модели с полным высказыванием для анализа графов."}
{"text": "In particular, the prefix after Obama is first completed through a finetuned BART language model and then translated into a program with full offline parser.\n", "text_Chinese": "特别是,奥巴马之后的前<unk>首先通过精细调整的BART语言模型完成,然后转换成一个具有完全离线解析器的程序.", "text_Arabic": "على وجه الخصوص، تم الانتهاء من الوصلة السابقة بعد أوباما أولاً من خلال نموذج لغة BART المرن ثم ترجم إلى برنامج مع تحليل كامل خارج الإنترنت.", "text_French": "En particulier, le préfixe après Obama est d'abord complété par un modèle de langage BART finement ajusté, puis traduit dans un programme avec un analyseur entièrement hors ligne.", "text_Japanese": "特にオバマの後の前頭文字はまず細化されたBART言語モデルを通じて完了しその後完全なオフライン解析器を持つプログラムに翻訳されます", "text_Russian": "В частности, префикс после Обамы сначала завершается с помощью тонкой языковой модели BART, а затем переводится в программу с полным оффлайн-парсером."}
{"text": "The second approach directly predicts the program from user utterance prefixes.\n", "text_Chinese": "第二种方法直接从用户的语音前<unk>中预测程序.", "text_Arabic": "النهج الثاني يتوقع البرنامج مباشرة من مقدمات التعبير المستخدم.", "text_French": "La deuxième approche prédit directement le programme à partir des préfixes d'expression de l'utilisateur.", "text_Japanese": "2番目のアプローチは,ユーザーの発言の前置詞からプログラムを直接予測します.", "text_Russian": "Второй подход непосредственно предсказывает программу из префиксов высказывания пользователя."}
{"text": "This is achieved by training a single online parser to translate to the goal graph from each prefix.\n", "text_Chinese": "通过训练单个在线解析器来将每个前<unk>转换为目标图来实现这一点.", "text_Arabic": "يتم تحقيق ذلك عن طريق تدريب تحليل واحد عبر الإنترنت لترجمة إلى مخطط الهدف من كل مقدمة.", "text_French": "Cela est réalisé en formant un seul analyseur en ligne pour traduire le graphique d'objectif de chaque préfixe.", "text_Japanese": "これは,単一のオンラインパサーを訓練して,各プレフィックスからゴールグラフに翻訳することで達成されます.", "text_Russian": "Это достигается путем обучения одного онлайн-парсера для перевода на график цели из каждого префикса."}
{"text": "This facilitates the model to learn the right anticipation.\n", "text_Chinese": "这有助于模型学习正确的预测.", "text_Arabic": "هذا يسهل على النموذج تعلم التوقع الصحيح.", "text_French": "Cela facilite le modèle pour apprendre la bonne anticipation.", "text_Japanese": "これはモデルが正しい予測を 学ぶことを容易にします", "text_Russian": "Это облегчает модели изучение правильного ожидания."}
{"text": "In a bit more detail, how do we generate these graphs?\n", "text_Chinese": "详细地说,我们如何生成这些图形?", "text_Arabic": "بتفاصيل أكثر قليلا, كيف نخلق هذه الرسوم البيانية?", "text_French": "Un peu plus en détail, comment générer ces graphiques?", "text_Japanese": "少し詳しく言えばどのようにしてこれらのグラフを生成するのですか?", "text_Russian": "Более подробно, как мы создаем эти графики?"}
{"text": "We formulate the problem by generating a serial version of the graph.\n", "text_Chinese": "我们通过生成图的串行版本来制定问题.", "text_Arabic": "نقوم بتصنيف المشكلة عن طريق إنشاء نسخة متسلسلة من الرسم البياني.", "text_French": "Nous formulons le problème en générant une version en série du graphique.", "text_Japanese": "グラフのシリアルバージョンを生成することで問題を形成します.", "text_Russian": "Мы формулируем проблему, генерируя серийную версию графика."}
{"text": "Each node or edge is represented by an action.\n", "text_Chinese": "每个节点或边缘都由一个动作表示.", "text_Arabic": "يتم تمثيل كل عقدة أو حافة بواسطة إجراء.", "text_French": "Chaque nœud ou bord est représenté par une action.", "text_Japanese": "各ノードまたはエッジはアクションで表されます.", "text_Russian": "Каждый узел или край представлен действием."}
{"text": "Here, we start from the first node.\n", "text_Chinese": "这里,我们从第一个节点开始.", "text_Arabic": "هنا، نبدأ من العقدة الأولى.", "text_French": "Ici, nous commençons par le premier nœud.", "text_Japanese": "ここでは最初のノードから始める", "text_Russian": "Здесь мы начинаем с первого узла."}
{"text": "The number below records the absolute index in action history.\n", "text_Chinese": "下面的数字记录了行动历史中的绝对指数.", "text_Arabic": "الرقم أدناه يسجل المؤشر المطلق في تاريخ العمل.", "text_French": "Le nombre ci-dessous enregistre l'indice absolu dans l'histoire de l'action.", "text_Japanese": "下の数字は,アクション歴史における絶対指数を記録しています.", "text_Russian": "Нижеприведенное число записывает абсолютный индекс в истории действий."}
{"text": "Then, we got the second node.\n", "text_Chinese": "然后,我们得到第二个节点.", "text_Arabic": "ثم، حصلنا على العقدة الثانية.", "text_French": "Puis, nous avons le deuxième nœud.", "text_Japanese": "次に2番目のノードが得られます", "text_Russian": "Затем у нас есть второй узел."}
{"text": "Next, is the edge between them.\n", "text_Chinese": "接下来,是它们之间的边缘.", "text_Arabic": "التالي، هو الحافة بينهما.", "text_French": "Ensuite, il y a le bord entre eux.", "text_Japanese": "次はそれらの間のです", "text_Russian": "Далее, край между ними."}
{"text": "It contains the pointer to the index of the previous node and the edge label.\n", "text_Chinese": "它包含了前一个节点的索引和边缘标签的指针.", "text_Arabic": "يحتوي على المؤشر إلى مؤشر العقدة السابقة وعلامة الحافة.", "text_French": "Il contient le pointeur de l'index du nœud précédent et l'étiquette de bord.", "text_Japanese": "前のノードのインデックスとエッジラベルへのポインタが含まれています.", "text_Russian": "Он содержит указатель на индекс предыдущего узла и этикетку края."}
{"text": "Zero here means connecting the most recent node with the node generated by the zeroth action and next node next edge.\n", "text_Chinese": "这里的零意味着连接最近的节点与由零行动生成的节点和下一个节点下一个边缘.", "text_Arabic": "الصفر هنا يعني ربط أحدث عقدة مع العقدة الناتجة عن العمل الصفر والعقدة التالية.", "text_French": "Zéro ici signifie connecter le nœud le plus récent avec le nœud généré par l'action zéro et le nœud suivant la prochaine arête.", "text_Japanese": "ゼロは最も最近のノードとゼロのアクションによって生成されたノードと次のノードのエッジを接続することを意味します", "text_Russian": "Нуль здесь означает соединение самого последнего узла с узлом, созданным нулевым действием, и следующим узлом следующего края."}
{"text": "This process goes on until we generate the full graph.\n", "text_Chinese": "这种过程持续到我们生成完整的图形.", "text_Arabic": "تستمر هذه العملية حتى ننتج الرسم البياني الكامل.", "text_French": "Ce processus se poursuit jusqu'à ce que nous générions le graphique complet.", "text_Japanese": "このプロセスは完全なグラフを生成するまで続きます", "text_Russian": "Этот процесс продолжается до тех пор, пока мы не получим полный график."}
{"text": "The underlying model is based on transformer with self pointing mechanism similar to a previous transition based parser.\n", "text_Chinese": "底层模型基于具有自我指向机制的变压器,类似于以前基于过渡的解析器.", "text_Arabic": "يعتمد النموذج الأساسي على محول مع آلية الإشارة الذاتية مماثلة للمحلل القائم على الانتقال السابق.", "text_French": "Le modèle sous-jacent est basé sur un transformateur avec un mécanisme d'auto-pointage similaire à un analyseur basé sur une transition précédente.", "text_Japanese": "基礎となるモデルは,以前の移行ベースのパサーに似た自己指向メカニズムを持つトランスフォーマーに基づいています.", "text_Russian": "Основная модель основана на трансформаторе с механизмом самонаправленности, аналогичным предыдущему анализу, основанному на переходе."}
{"text": "After generating a complete graph, we obtained the action level probabilities that correspond to different parts of the graph.\n", "text_Chinese": "在生成完整的图后,我们获得了对应于图的不同部分的行动级别概率.", "text_Arabic": "بعد إنشاء الرسم البياني الكامل، حصلنا على احتمالات مستوى العمل التي تتوافق مع أجزاء مختلفة من الرسم البياني.", "text_French": "Après avoir généré un graphique complet, nous avons obtenu les probabilités de niveau d'action qui correspondent à différentes parties du graphique.", "text_Japanese": "グラフの異なる部分に対応するアクションレベル確率を得ました. グラフの異なる部分に対応するアクションレベル確率を得ました.", "text_Russian": "После создания полного графика мы получили вероятности уровня действия, которые соответствуют различным частям графика."}
{"text": "We select confidence subgraphs based on the thresholding heuristic to be executed.\n", "text_Chinese": "我们根据要执行的值启发式来选择信任子图", "text_Arabic": "نختار الجرافات الفرعية للثقة بناءً على حساب العتبة الذي سيتم تنفيذه.", "text_French": "Nous sélectionnons des sous-graphes de confiance basés sur l'heuristique de seuil à exécuter.", "text_Japanese": "実行するスローホール・ヒューリスティックに基づいて 信頼性サブグラフを選択します", "text_Russian": "Мы выбираем подграфы доверия на основе эвристики порога, которая должна быть выполнена."}
{"text": "Later on, we're going to vary the threshold to achieve different tradeoffs between the latency reduction and the execution cost.\n", "text_Chinese": "后来,我们将改变<unk>值,以实现延迟减少和执行成本之间的不同权衡.", "text_Arabic": "في وقت لاحق، سنقوم بتغيير العتبة لتحقيق مقايضات مختلفة بين تقليل التأخير وتكلفة التنفيذ.", "text_French": "Plus tard, nous allons varier le seuil pour atteindre différents compromis entre la réduction de la latence et le coût d'exécution.", "text_Japanese": "遅延の削減と実行コストの間の異なるトレードオフを達成するために私たちは後にスリングスを変える予定です", "text_Russian": "Позже мы будем изменять порог, чтобы достичь различных компромиссов между сокращением задержки и стоимостью выполнения."}
{"text": "For formal evaluation of the online methods, we propose final latency reduction or FLR metric.\n", "text_Chinese": "为了正式评估在线方法,我们建议最终延迟减少或FLR指标.", "text_Arabic": "للتقييم الرسمي للطرق عبر الإنترنت، نقترح تخفيض التأخير النهائي أو مقياس FLR.", "text_French": "Pour l'évaluation formelle des méthodes en ligne, nous proposons la réduction finale de la latence ou la métrique FLR.", "text_Japanese": "オンライン方法の正式な評価のために,我々は最終的な遅延削減またはFLRメトリックを提案します.", "text_Russian": "Для формальной оценки онлайн-методов мы предлагаем конечное сокращение латентности или FLR-метрику."}
{"text": "Here's a recap of how an offline system finishes the execution timeline.\n", "text_Chinese": "这是离线系统如何完成执行时间表的概述.", "text_Arabic": "إليك ملخص لكيفية إكمال نظام خارج الإنترنت لخطوط التنفيذ.", "text_French": "Voici un récapitulatif de la façon dont un système hors ligne termine la chronologie d'exécution.", "text_Japanese": "オフラインシステムが実行タイムラインを完了する方法の概要です.", "text_Russian": "Вот краткое описание того, как оффлайн-система завершает временную линию исполнения."}
{"text": "In online systems, execution overlaps with the utterance timeline, so it ends earlier.\n", "text_Chinese": "在在线系统中,执行与发音时间线重叠,所以它结束得更早.", "text_Arabic": "في الأنظمة عبر الإنترنت، يتداخل التنفيذ مع خط الوقت للحديث، لذلك ينتهي في وقت مبكر.", "text_French": "Dans les systèmes en ligne, l'exécution se chevauche avec la chronologie de l'énoncé, donc elle se termine plus tôt.", "text_Japanese": "オンラインシステムでは 実行が発言のタイムラインと重なり 早期に終了します", "text_Russian": "В онлайн-системах исполнение перекрывается с временной линией высказывания, поэтому оно заканчивается раньше."}
{"text": "FLR is defined as the reduction time compared to the offline system, marked by the end of the execution.\n", "text_Chinese": "FLR被定义为与离线系统相比的缩短时间,由执行结束标记.", "text_Arabic": "يتم تعريف FLR على أنه وقت التخفيض مقارنة بالنظام خارج الإنترنت ، والذي يتميز بنهاية التنفيذ.", "text_French": "FLR est défini comme le temps de réduction par rapport au système hors ligne, marqué par la fin de l'exécution.", "text_Japanese": "FLRは,オフラインシステムと比較して,実行の終了によってマークされた短縮時間として定義されます.", "text_Russian": "FLR определяется как время сокращения по сравнению с автономной системой, отмеченное концом выполнения."}
{"text": "We conduct experiments on two large conversational semantic parsing datasets, SMCalFlow and TreeDST.\n", "text_Chinese": "我们对两个大型对话语义解析数据集进行实验,", "text_Arabic": "نقوم بإجراء تجارب على مجموعتين كبيرتين من بيانات التفسير الدلالي المحادثي، SMCalFlow و TreeDST.", "text_French": "Nous menons des expériences sur deux grands ensembles de données d'analyse sémantique conversationnelle, SMCalFlow et TreeDST.", "text_Japanese": "SMCalFlowとTreeDSTという 2つの大きな会話セマンティックパースデータセットで 実験を行っています", "text_Russian": "Мы проводим эксперименты с двумя большими наборами данных семантического анализа разговора, SMCalFlow и TreeDST."}
{"text": "Our graph based parser when operating offline, achieves state-of-the-art performance on parsing on both datasets.\n", "text_Chinese": "我们的基于图形的解析器在离线操作时,在两个数据集的解析中实现了最先进的性能.", "text_Arabic": "يقوم تحليلنا القائم على الرسم البياني عند التشغيل خارج الإنترنت، بتحقيق أداء متطور في تحليل كل من مجموعات البيانات.", "text_French": "Notre analyseur basé sur des graphiques, lorsqu'il fonctionne hors ligne, atteint des performances de pointe sur l'analyse des deux ensembles de données.", "text_Japanese": "グラフベースのパースは オフラインで動作すると 両方のデータセットを パースする最先端のパフォーマンスを実現します", "text_Russian": "Наш графический парсер при работе в автономном режиме достигает передовой производительности при анализе обоих наборов данных."}
{"text": "The LM complete model also achieves nontrivial BLEU gain compared with the simple baseline of node completion.\n", "text_Chinese": "与节点完成的简单基线相比,LM完整模型还实现了非微不足道的BLEU收益.", "text_Arabic": "يحقق نموذج LM الكامل أيضًا مكاسب BLEU غير تافهة مقارنة بالخط الأساسي البسيط لإكمال العقدة.", "text_French": "Le modèle complet LM obtient également un gain BLEU non trivial par rapport à la simple ligne de base de l'achèvement du nœud.", "text_Japanese": "LM完全モデルでは,ノード完了の単純なベースラインと比較して,非軽微なBLEUの獲得も達成します.", "text_Russian": "Полная модель LM также достигает нетривиального увеличения BLEU по сравнению с простой базовой линией завершения узла."}
{"text": "Now, let's look at the prediction accuracy of our prefix to graph parser.\n", "text_Chinese": "现在,让我们看看我们的图解析器的前<unk>的预测准确性.", "text_Arabic": "الآن، دعونا ننظر إلى دقة التنبؤ من مقدمة لدينا إلى تحليل الرسم البياني.", "text_French": "Maintenant, regardons la précision de prédiction de notre préfixe pour l'analyseur de graphique.", "text_Japanese": "グラフ解析器の前置詞の予測精度を 調べてみましょう", "text_Russian": "Теперь давайте посмотрим на точность предсказания нашего префикса для анализатора графов."}
{"text": "We test the match F1 score of graph tuples between the generation and the go graph in validation data in y axis for each prefix length in x axis represented by percentages.\n", "text_Chinese": "我们测试生成和走图之间的图形组的匹配F1分数,在y轴中验证数据中,为每个前<unk>长度在x轴中以百分比表示.", "text_Arabic": "نختبر درجة F1 للمطابقة بين مجموعات الرسم البياني بين الجيل و الرسم البياني في بيانات التحقق في محور y لكل طول مقدم في محور x ممثلاً بالنسب المئوية.", "text_French": "Nous testons le score F1 de correspondance des tuples de graphique entre la génération et le graphique de départ dans les données de validation dans l'axe y pour chaque longueur de préfixe dans l'axe x représentée par des pourcentages.", "text_Japanese": "グラフタップルのマッチF1スコアを生成とゴーグラフの間の検証データでy軸でX軸の各プレフィックス長さをパーセントで表すようにテストします.", "text_Russian": "Мы тестируем соответствие F1 баллов графских наборов между генерацией и графиком go в данных проверки в оси y для каждой длины префикса в оси x, представленной процентами."}
{"text": "Each of these curves represents a different model with the only difference in training data.\n", "text_Chinese": "每个曲线都代表一个不同的模型, 唯一的区别是在训练数据.", "text_Arabic": "كل من هذه المنحنيات تمثل نموذجًا مختلفًا مع الفرق الوحيد في بيانات التدريب.", "text_French": "Chacune de ces courbes représente un modèle différent avec la seule différence dans les données de formation.", "text_Japanese": "これらの曲線のそれぞれは,訓練データでの唯一の違いを持つ異なるモデルを表しています.", "text_Russian": "Каждая из этих кривых представляет собой другую модель с единственным различием в данных обучения."}
{"text": "The bottom curve is the offline parser, and we mix in prefix data in different lengths to transition the model to an online parser.\n", "text_Chinese": "下面的曲线是离线解析器, 我们混合不同长度的前<unk>数据将模型转换为在线解析器.", "text_Arabic": "المنحنى السفلي هو تحليل غير متصل، ونخلط في بيانات المبدئي في أطوال مختلفة لتحويل النموذج إلى تحليل عبر الإنترنت.", "text_French": "La courbe inférieure est l'analyseur hors ligne, et nous mélangeons des données de préfixe de différentes longueurs pour faire la transition du modèle vers un analyseur en ligne.", "text_Japanese": "オフライン解析器です オフライン解析器にモデルを移行させるために 異なる長さのプレフィックスデータを混ぜます", "text_Russian": "Нижняя кривая - это оффлайн-парсер, и мы смешиваем данные префикса в разных длинах, чтобы перевести модель в онлайн-парсер."}
{"text": "For example, the legend prefix eighty percent plus means the model is trained with prefix data with prefix length larger than eighty percent of the full utterance length.\n", "text_Chinese": "例如,传奇前<unk>八成加意味着模型是用前<unk>数据训练的,前<unk>长度大于整个语句长度的八成.", "text_Arabic": "على سبيل المثال، يعني مقدمة الأسطورة ثمانين في المائة زائد أن النموذج تم تدريبه ببيانات مقدمة مع طول مقدمة أكبر من ثمانين في المائة من طول الكلام الكامل.", "text_French": "Par exemple, le préfixe de légende quatre-vingts pour cent plus signifie que le modèle est entraîné avec des données de préfixe avec une longueur de préfixe supérieure à quatre-vingts pour cent de la longueur complète de l'énoncé.", "text_Japanese": "例えば,レジェンドプレフィックス八割プラスとは,モデルがプレフィックスデータでトレーニングされ,プレフィックスの長さは,全発言長の八割以上です.", "text_Russian": "Например, префикс легенды восемьдесят процентов плюс означает, что модель обучена с помощью данных префикса с длиной префикса, превышающей восемьдесят процентов от полной длины высказывания."}
{"text": "The upper left corner is the desired area.\n", "text_Chinese": "左上角是所需的区域.", "text_Arabic": "الزاوية اليسرى العليا هي المنطقة المرغوب فيها.", "text_French": "Le coin supérieur gauche est la zone souhaitée.", "text_Japanese": "左上隅は望ましい領域です", "text_Russian": "В левом верхнем углу - желаемая область."}
{"text": "As we can see, the offline parser in black curve is not doing well on the prefix data.\n", "text_Chinese": "正如我们所看到的,黑色曲线中的离线解析器在前<unk>数据上做得不好.", "text_Arabic": "كما نري، فإن المحلل غير متصل في المنحنى الأسود لا يعمل بشكل جيد على بيانات الوصفة.", "text_French": "Comme nous pouvons le voir, l'analyseur hors ligne dans la courbe noire ne fonctionne pas bien sur les données de préfixe.", "text_Japanese": "黒い曲線のオフライン解析器は プレフィックスデータでうまく機能していません", "text_Russian": "Как мы видим, оффлайн-парсер в черной кривой не справляется с данными префикса."}
{"text": "As we're mixing more prefixes in training, the curve is lifting upper and left, performing better on all the prefix lengths.\n", "text_Chinese": "随着我们在训练中混合更多的前<unk>, 曲线在上方和左边提升, 在所有前<unk>长度上表现更好.", "text_Arabic": "مع خلطنا المزيد من المبدعات في التدريب, فإن المنحنى يرتفع أعلى و يسار, ويؤدي بشكل أفضل على جميع أطوال المبدعات.", "text_French": "En mélangeant plus de préfixes dans l'entraînement, la courbe s'élève en haut et à gauche, offrant de meilleures performances sur toutes les longueurs de préfixes.", "text_Japanese": "訓練中により多くの前置詞を混ぜるにつれ 曲線は上部と左側を上げ すべての前置詞の長さでより良いパフォーマンスを発揮します", "text_Russian": "По мере того, как мы смешиваем больше префикса в тренировке, кривая поднимается вверх и влево, лучше выполняя на всех длинах префикса."}
{"text": "However, the full utterance parsing performance is not affected in the upper right dot.\n", "text_Chinese": "然而,在右上角的点中,完整的语句解析性能不会受到影响.", "text_Arabic": "ومع ذلك، فإن أداء تحليل الكلام الكامل لا يتأثر في النقطة اليمنى العليا.", "text_French": "Cependant, la performance d'analyse complète de l'énoncé n'est pas affectée dans le point supérieur droit.", "text_Japanese": "しかし,右上部のドットでは,完全な発言解析性能は影響を受けません.", "text_Russian": "Однако полная производительность анализа высказывания не влияет на верхнюю правую точку."}
{"text": "Based on these strong results, how much latency do we reduce?\n", "text_Chinese": "根据这些强有力的结果, 我们能减少多少延迟?", "text_Arabic": "بناءً على هذه النتائج القوية, كم من الوقت المتأخر نقلله?", "text_French": "Sur la base de ces résultats solides, combien de latence réduisons-nous?", "text_Japanese": "これらの強力な結果に基づいてどのくらいの遅延を減らしますか?", "text_Russian": "На основе этих сильных результатов, насколько мы сокращаем латентность?"}
{"text": "We measure the time by the number of source tokens and simulate different function execution times.\n", "text_Chinese": "我们通过源令牌的数量来测量时间,并模拟不同的函数执行时间.", "text_Arabic": "نقيس الوقت من خلال عدد الرموز المصدرة ونحاكي أوقات تنفيذ الوظائف المختلفة.", "text_French": "Nous mesurons le temps par le nombre de jetons source et simulons différents temps d'exécution de fonctions.", "text_Japanese": "異なる関数実行時間をシミュレートします. 関数実行時間をシミュレートします.", "text_Russian": "Мы измеряем время по количеству токенов-источников и имитируем различные времена выполнения функций."}
{"text": "The curves show the tradeoff between the FLR metric and the execution cost, measured by the number of excessive function costs that are not correct.\n", "text_Chinese": "曲线显示了FLR指标和执行成本之间的权衡,以不正确的过度功能成本的数量来衡量.", "text_Arabic": "تظهر المنحنيات التوازن بين مقياس FLR وتكلفة التنفيذ ، والتي تقاس بعدد تكاليف الوظيفة المفرطة التي ليست صحيحة.", "text_French": "Les courbes montrent le compromis entre la métrique FLR et le coût d'exécution, mesuré par le nombre de coûts de fonction excessifs qui ne sont pas corrects.", "text_Japanese": "曲線は,FLRメトリックと実行コストの間のトレードオフを示しており,正しくない過な機能コストの数で測定されています.", "text_Russian": "Кривые показывают компромисс между метрикой FLR и стоимостью исполнения, измеряемую количеством чрезмерных функционных затрат, которые не являются правильными."}
{"text": "This is achieved by varying the subgraph selection threshold.\n", "text_Chinese": "这是通过改变子图选择<unk>值来实现的.", "text_Arabic": "يتم تحقيق ذلك عن طريق تغيير عتبة اختيار الصفات الفرعية.", "text_French": "Cela est réalisé en variant le seuil de sélection des sous-graphes.", "text_Japanese": "これは,サブグラフ選択の値を変えることによって達成されます.", "text_Russian": "Это достигается путем изменения порога выбора подграфа."}
{"text": "A higher threshold selects fewer functions of mistake, but obtains a smaller FLR, whereas the lower threshold more aggressively selects and executes programs.\n", "text_Chinese": "一个较高的<unk>值选择更少的错误函数,但获得更小的FLR,而较低的<unk>值更积极地选择和执行程序.", "text_Arabic": "تختار العتبة العالية عددًا أقل من وظائف الخطأ ، ولكنها تحصل على FLR أصغر ، في حين أن العتبة السفلى تختار وتنفذ البرامج بشكل أكثر عدوانية.", "text_French": "Un seuil plus élevé sélectionne moins de fonctions d'erreur, mais obtient un FLR plus petit, tandis que le seuil inférieur sélectionne et exécute plus agressivement les programmes.", "text_Japanese": "より高いスレッシュは,より少ないエラー関数を選択しますが,より小さなFLRを取得しますが,より低いスレッシュは,より積極的にプログラムを選択し,実行します.", "text_Russian": "Более высокий порог выбирает меньше функций ошибки, но получает меньший FLR, в то время как более низкий порог более агрессивно выбирает и выполняет программы."}
{"text": "We compare the two approaches we propose and a baseline that does nothing but directly applying the offline parser for online use.\n", "text_Chinese": "我们比较了我们提出的两种方法和一个基线,除了直接应用离线解析器来在线使用,", "text_Arabic": "نقارن النهجين اللذين نقترحهما وخطوط الأساس التي لا تفعل شيئا سوى تطبيق مفصل الإنترنت بشكل مباشر للاستخدام عبر الإنترنت.", "text_French": "Nous comparons les deux approches que nous proposons et une base de référence qui ne fait rien d'autre que d'appliquer directement l'analyseur hors ligne pour une utilisation en ligne.", "text_Japanese": "オフラインパサーをオンラインで使用する以外何もしないベースラインを比較します.", "text_Russian": "Мы сравниваем два подхода, которые мы предлагаем, и базовую линию, которая ничего не делает, кроме как непосредственно применяет оффлайн-парсер для онлайн-использования."}
{"text": "The upper left region is has the best FLR and cost tradeoff.\n", "text_Chinese": "左上区域是FLR和成本权衡的最佳区域.", "text_Arabic": "المنطقة اليسرى العليا لديها أفضل FLR والتفاوض على التكلفة.", "text_French": "La région en haut à gauche a le meilleur FLR et le meilleur compromis de coût.", "text_Japanese": "左上部の地域は FLRとコストのトレードオフが最も良いです", "text_Russian": "Верхний левый регион имеет лучший FLR и компромисс затрат."}
{"text": "We see both of our methods beat the baseline by a large margin, and they perform more similarly on TreeDST.\n", "text_Chinese": "我们看到我们的两种方法都大大超过了基线,", "text_Arabic": "نرى أن كلتا الطريقتين تفوق الخط الأساسي بفارق كبير، وتؤدي بشكل مشابه على TreeDST.", "text_French": "Nous voyons que nos deux méthodes dépassent la ligne de base d'une grande marge, et elles fonctionnent plus similairement sur TreeDST.", "text_Japanese": "TreeDSTではより類似したパフォーマンスを示しています. TreeDSTではより類似したパフォーマンスを示しています.", "text_Russian": "Мы видим, что оба наших метода превзошли исходную линию с большим отрывом, и они работают более похоже на TreeDST."}
{"text": "While individual function execution is faster, there tends to be more run executions and lower latency reduction room.\n", "text_Chinese": "虽然单个函数的执行速度更快,但往往有更多的运行执行和更低的延迟减少空间.", "text_Arabic": "في حين أن تنفيذ الوظيفة الفردية أسرع، هناك ميل إلى المزيد من عمليات التنفيذ وانخفاض غرفة تقليل التأخير.", "text_French": "Alors que l'exécution de fonctions individuelles est plus rapide, il y a tendance à avoir plus d'exécutions et moins de latence.", "text_Japanese": "個々の関数実行が速くなる一方で,実行実行が増え,レイテンシー削減の余地が減る傾向があります.", "text_Russian": "В то время как выполнение индивидуальной функции быстрее, имеет тенденцию к большему количеству выполнений и меньшему пространству для сокращения задержки."}
{"text": "When individual function execution is slower, there is more room for FLR improvement.\n", "text_Chinese": "当单个功能的执行速度较慢时, FLR 改进的空间就更多了", "text_Arabic": "عندما يكون تنفيذ الوظيفة الفردية أبطأ، يكون هناك مجال أكبر لتحسين FLR.", "text_French": "Lorsque l'exécution de la fonction individuelle est plus lente, il y a plus de place pour l'amélioration de la FLR.", "text_Japanese": "個々の機能の実行が遅くなると,FLRの改善に多くの余地があります.", "text_Russian": "Когда выполнение отдельных функций медленнее, есть больше возможностей для улучшения FLR."}
{"text": "Our two approaches achieve better performance in different cost cost regions.\n", "text_Chinese": "我们的两种方法在不同的成本成本区域实现了更好的性能.", "text_Arabic": "نهجنا يحقق أداء أفضل في مناطق تكلفة مختلفة.", "text_French": "Nos deux approches permettent d'obtenir de meilleures performances dans différentes régions de coût.", "text_Japanese": "私たちの2つのアプローチは,異なるコストコスト地域でより良いパフォーマンスを達成します.", "text_Russian": "Наши два подхода достигают лучшей производительности в разных регионах затрат."}
{"text": "Overall, we achieve thirty to sixty three percent relative latency reduction depending on execution time and allowed cost.\n", "text_Chinese": "总体而言,我们实现了30%到63%的相对延迟减少,", "text_Arabic": "بشكل عام، نحقق انخفاض في التأخير النسبي بنسبة ثلاثين إلى ثلاثة وستين في المائة اعتمادًا على وقت التنفيذ والتكلفة المسموح بها.", "text_French": "Dans l'ensemble, nous atteignons une réduction relative de la latence de trente à soixante-trois pour cent en fonction du temps d'exécution et du coût autorisé.", "text_Japanese": "執行時間と許容されるコストに応じて全体的に我々は30〜63%の相対的な遅延の削減を達成します", "text_Russian": "В целом, мы достигаем тридцати до шестьдесят трех процентов относительного сокращения задержки в зависимости от времени выполнения и разрешенной стоимости."}
{"text": "Finally, we have a breakdown of average latency reduction in tokens for each type of the function node when the allowed cost is three run executions.\n", "text_Chinese": "最后,我们有一个平均延迟减少的分解在令牌的每个类型的函数节点当允许的成本是三个运行执行.", "text_Arabic": "وأخيرا، لدينا تقسيم متوسط انخفاض التأخير في الرموز لكل نوع من عقدة الوظيفة عندما تكون التكلفة المسموح بها ثلاثة عمليات تنفيذ.", "text_French": "Enfin, nous avons une ventilation de la réduction moyenne de la latence dans les jetons pour chaque type de nœud de fonction lorsque le coût autorisé est de trois exécutions.", "text_Japanese": "最後に許可されたコストが3回の実行である場合各タイプの関数ノードのトークンの平均遅延削減の分解があります", "text_Russian": "Наконец, у нас есть разбивка среднего сокращения задержки в токенах для каждого типа узла функции, когда допустимая стоимость составляет три выполнения."}
{"text": "As we can see, there are gains all over the board.\n", "text_Chinese": "我们可以看到,在所有方面都有收益.", "text_Arabic": "كما نري، هناك مكاسب في كل المجالات.", "text_French": "Comme nous pouvons le voir, il y a des gains partout.", "text_Japanese": "見ての通り すべての分野で 利益が得られています", "text_Russian": "Как мы видим, есть успехи по всем направлениям."}
{"text": "There are also some functions on which we gain impressive latency reduction where the red bar is much longer, such as find manager and recipient.\n", "text_Chinese": "还有一些功能,我们可以显著减少延迟,红色条条更长,比如找到管理员和接收者.", "text_Arabic": "هناك أيضا بعض الوظائف التي نحصل على تخفيض التأخير المذهل حيث الشريط الأحمر أطول بكثير، مثل العثور على مدير والمستلم.", "text_French": "Il y a aussi des fonctions sur lesquelles nous obtenons une réduction impressionnante de la latence où la barre rouge est beaucoup plus longue, comme trouver le gestionnaire et le destinataire.", "text_Japanese": "赤いバーがずっと長いところで 印象的なレイテンシー削減を得る機能もあります 例えば マネージャーと受信者を検索する", "text_Russian": "Есть также некоторые функции, на которых мы получаем впечатляющее сокращение задержки, где красная панель намного длиннее, например, найти менеджера и получателя."}
{"text": "These are low level functions that do not have much dependency on others.\n", "text_Chinese": "这些是低级别的功能,不依赖于其他功能.", "text_Arabic": "هذه وظائف منخفضة المستوى التي لا تعتمد كثيراً على الآخرين.", "text_French": "Il s'agit de fonctions de bas niveau qui ne dépendent pas beaucoup des autres.", "text_Japanese": "これらは他者への依存度があまりない低レベル機能です", "text_Russian": "Это функции низкого уровня, которые не имеют большой зависимости от других."}
{"text": "In conclusion, we proposed online semantic parsing as new task to explore with the rigorous latency reduction metric.\n", "text_Chinese": "最后,我们提议在线语义解析作为一项新的任务,", "text_Arabic": "في الختام، اقترحنا التحليل الدلالي عبر الإنترنت كمهمة جديدة لاستكشافها مع مقياس الحد من التأخير الصارم.", "text_French": "En conclusion, nous avons proposé l'analyse sémantique en ligne comme nouvelle tâche à explorer avec la métrique rigoureuse de réduction de la latence.", "text_Japanese": "結論として厳格なレイテンシー削減メトリックで探索する新しいタスクとしてオンラインセマンティックパーシングを提案しました", "text_Russian": "В заключение мы предложили онлайн-семантический анализ в качестве новой задачи для изучения с помощью строгой метрики сокращения задержки."}
{"text": "With a strong graph based semantic parser, we achieve relatively good latency reduction either through our pipeline approach with LM completion and a full parser or directly through a learned parser on the prefixes.\n", "text_Chinese": "通过强大的基于图形的语义解析器,我们通过我们的管道方法实现相对好的延迟减少,通过LM完成和完整的解析器,或直接通过先<unk>上的学习解析器.", "text_Arabic": "مع تحليل معنوي قوي قائم على الرسم البياني، نحقق انخفاض الكمون الجيد نسبيًا إما من خلال نهج خط الأنابيب مع إكمال LM ومحلل كامل أو مباشرة من خلال تحليل تعلم على المبدعات.", "text_French": "Avec un analyseur sémantique basé sur un graphique fort, nous atteignons une réduction de latence relativement bonne soit par notre approche de pipeline avec l'achèvement de LM et un analyseur complet, soit directement par un analyseur appris sur les préfixes.", "text_Japanese": "強力なグラフベースのセマンティックパーサーを使用すると,LM完了とフルパーサーのパイプラインアプローチまたはプレフィックス上の学習パーサーを直接使用して,比較的良いレイテンシー削減を達成します.", "text_Russian": "С сильным семантическим парсером, основанным на графах, мы достигаем относительно хорошего снижения латентности либо с помощью нашего подхода к трубопроводу с завершением LM и полным парсером, либо непосредственно с помощью изученного парсера на префиксах."}
{"text": "Moreover, our approach can be a general framework and can be applied to other executable semantic representations in different domains.\n", "text_Chinese": "此外,我们的方法可以是一个一般框架,可以应用于不同领域的其他可执行语义表示.", "text_Arabic": "علاوة على ذلك ، يمكن أن يكون نهجنا إطارًا عامًا ويمكن تطبيقه على تمثيلات دلالية أخرى قابلة للتنفيذ في مجالات مختلفة.", "text_French": "En outre, notre approche peut être un cadre général et peut être appliquée à d'autres représentations sémantiques exécutables dans différents domaines.", "text_Japanese": "さらに,私たちのアプローチは一般的なフレームワークであり,異なるドメインの他の実行可能なセマンティック表現に適用できます.", "text_Russian": "Кроме того, наш подход может быть общей основой и может применяться к другим исполняемым семантическим представлениям в разных областях."}
{"text": "Future works could explore smarter prediction and execution integration method.\n", "text_Chinese": "未来的工作可以探索更智能的预测和执行整合方法.", "text_Arabic": "يمكن للأعمال المستقبلية استكشاف طريقة تكامل التنبؤ والتنفيذ الأكثر ذكاءً.", "text_French": "Les travaux futurs pourraient explorer une méthode d'intégration de prévision et d'exécution plus intelligente.", "text_Japanese": "将来の作業はよりスマートな予測と実行統合方法を探索することができます", "text_Russian": "Будущие работы могут изучить более умный метод интеграции прогнозирования и исполнения."}
{"text": "Thanks for your listening.\n", "text_Chinese": "谢谢您的聆听", "text_Arabic": "شكراً على استماعك.", "text_French": "Merci d'avoir écouté.", "text_Japanese": "聞いてくれてありがとう", "text_Russian": "Спасибо, что выслушали."}
{"text": "Hi.\n", "text_Chinese": "你好,你好", "text_Arabic": "مرحباً.", "text_French": "- Je vous salue.", "text_Japanese": "こんにちは", "text_Russian": "Привет. Привет."}
{"text": "I'm going to discuss our work on generating retrieval augmented counterfactuals for question answering tasks.\n", "text_Chinese": "我将讨论我们对生成检索增强对应事实的研究,", "text_Arabic": "سأناقش عملنا على توليد الاسترداد المعزز للمواضيع المضادة لمهام الإجابة على الأسئلة.", "text_French": "Je vais discuter de notre travail sur la génération de contre-faits augmentés de récupération pour les tâches de réponse aux questions.", "text_Japanese": "質問答弁のタスクのための 検索拡張対照の生成に関する私たちの作業について 議論します", "text_Russian": "Я собираюсь обсудить нашу работу по созданию расширенных контрфактуальных данных для задач ответов на вопросы."}
{"text": "This is work done during my internship at Google Research, where I was mentored by Matthew Lamm and Ian Tenney.\n", "text_Chinese": "这是我在谷歌研究的实习期间完成的工作, 在那里我受到了马修·拉姆和伊恩·特尼的指导", "text_Arabic": "هذا عمل تم خلال التدريب في جوجل ريسرچ, حيث كنت مرشدا من قبل ماثيو لام وإيان تيني.", "text_French": "C'est un travail réalisé pendant mon stage chez Google Research, où j'ai été encadré par Matthew Lamm et Ian Tenney.", "text_Japanese": "これは私がマシュー・ラムとイアン・テニーによって指導されたGoogleリサーチでのインターンシップ中に行われた作業です", "text_Russian": "Это работа, проделанная во время моей стажировки в Google Research, где меня наставляли Мэтью Лэмм и Иан Тенни."}
{"text": "To motivate the task, let me begin by defining a counterfactual.\n", "text_Chinese": "为了激励这项任务,让我首先定义一个反事实.", "text_Arabic": "لتحفيز المهمة، دعوني أبدأ بتعريف الواقع المضاد.", "text_French": "Pour motiver la tâche, laissez-moi commencer par définir un contrefactuel.", "text_Japanese": "課題を動機づけるために 対照的な定義から始めましょう", "text_Russian": "Чтобы мотивировать задачу, позвольте мне начать с определения контрфактического."}
{"text": "In this work, we define a counterfactual as a perturbation of the input text that differs in some meaningful controlled way from the original text.\n", "text_Chinese": "在本文中,我们定义反事实是输入文本的扰动,它以某种有意义的控制方式与原始文本不同.", "text_Arabic": "في هذا العمل، نحدد الواقع المضاد على أنه اضطراب في النص الإدخالي يختلف بطريقة مفيدة ومتحكمة عن النص الأصلي.", "text_French": "Dans ce travail, nous définissons un contrefactuel comme une perturbation du texte d'entrée qui diffère d'une manière contrôlée significative du texte original.", "text_Japanese": "この研究ではオリジナルのテキストと意味のある制御された方法で異なる入力テキストの乱れとして反事実を定義します", "text_Russian": "В этой работе мы определяем контрфактуальный как возмущение входного текста, которое отличается каким-то значимым контролируемым образом от оригинального текста."}
{"text": "And allows us to reason about the changes in the outcome or the task label.\n", "text_Chinese": "让我们可以推理结果或任务标签的变化.", "text_Arabic": "ويسمح لنا بالتفكير حول التغييرات في النتيجة أو تسمية المهمة.", "text_French": "Et nous permet de raisonner sur les changements dans le résultat ou l'étiquette de la tâche.", "text_Japanese": "結果やタスクのラベルの変化について 推論できるようにします", "text_Russian": "И позволяет нам рассуждать об изменениях в результате или на ярлыке задачи."}
{"text": "For instance, changing the words fascinating to captivating or expected to mind-numbing changes the sentiment for this movie review.\n", "text_Chinese": "例如, 将迷人改为迷人, 或预期改为麻木, 改变了对这部电影评论的看法", "text_Arabic": "على سبيل المثال, تغيير الكلمات المذهلة إلى مغرية أو المتوقعة إلى مخيفة العقل يغير الشعور لهذا الاستعراض السينمائي.", "text_French": "Par exemple, changer les mots fascinant en captivant ou attendu en étouffant change le sentiment pour cette critique de film.", "text_Japanese": "例えば魅力的な言葉を魅力的な言葉にまたは期待される言葉を麻的な言葉に変えるとこの映画レビューに対する感情が変わります", "text_Russian": "Например, изменение слов \"завораживающий\" на \"завораживающий\" или \"ожидаемый\" на \"обезболивающий\" меняет настроение к этому кинорецензии."}
{"text": "Similarly, adding the qualifier women's to the question changes the answer to the question in the example below.\n", "text_Chinese": "同样,在下面的例子中,将\"妇女\"添加到问题中会改变问题的答案.", "text_Arabic": "وبالمثل، فإن إضافة مؤهل النساء إلى السؤال يغير الإجابة على السؤال في المثال أدناه.", "text_French": "De même, l'ajout du qualificatif femmes à la question modifie la réponse à la question dans l'exemple ci-dessous.", "text_Japanese": "同様に,質問に\"女性\"という条件を追加すると,下の例の質問の答えが変わります.", "text_Russian": "Аналогичным образом, добавление квалификатора \"женщины\" к вопросу меняет ответ на вопрос в примере ниже."}
{"text": "Humans are typically robust to such perturbations compared to NLP models trained on the task.\n", "text_Chinese": "与训练过NLP模型相比,人类通常对这种干扰有较强的抵抗力.", "text_Arabic": "البشر عادة ما يكونون قويين تجاه مثل هذه الاضطرابات مقارنة بنماذج NLP المدربة على المهمة.", "text_French": "Les humains sont généralement robustes à de telles perturbations par rapport aux modèles de PNL formés à la tâche.", "text_Japanese": "NLPモデルと比較して 人間は通常 このような混乱に強固です", "text_Russian": "Люди, как правило, устойчивы к таким возмущениям по сравнению с NLP-моделями, обученными на эту задачу."}
{"text": "Why is that?\n", "text_Chinese": "这是为什么?", "text_Arabic": "لماذا هذا؟", "text_French": "Pourquoi c'est comme ça?", "text_Japanese": "なぜそうなったの?", "text_Russian": "Почему так?"}
{"text": "The dataset may be sampled with systematic biases that lead to a simple decision boundary that is violated by the counterfactual.\n", "text_Chinese": "数据集可以以系统偏差进行抽样,从而导致简单的决策边界被反事实侵犯.", "text_Arabic": "يمكن أخذ عينة من مجموعة البيانات مع التحيزات المنهجية التي تؤدي إلى حدود قرار بسيطة يتم انتهاكها من قبل الواقع المضاد.", "text_French": "L'ensemble de données peut être échantillonné avec des biais systématiques qui conduisent à une simple limite de décision qui est violée par le contrefactuel.", "text_Japanese": "データセットは,反事実によって侵害される単純な決定境界につながる体系的なバイアスでサンプリングされる可能性があります.", "text_Russian": "Набор данных может быть отобран с систематическими предвзятостями, которые приводят к простой границе решения, которая нарушается контрафактуальной."}
{"text": "As shown in this 2D classification problem.\n", "text_Chinese": "正如这个2D分类问题所示.", "text_Arabic": "كما هو مبين في هذه مشكلة التصنيف ثنائية الأبعاد.", "text_French": "Comme le montre ce problème de classification 2D.", "text_Japanese": "この2D分類問題で示されているように", "text_Russian": "Как показано в этой 2D-задаче классификации."}
{"text": "My work has found that adding counterfactual examples to the training data can make the model robust to such perturbations.\n", "text_Chinese": "我的研究发现, 在训练数据中添加假设例子可以使模型对这种干扰有较强的抵抗力", "text_Arabic": "وجد عملي أن إضافة أمثلة غير واقعية إلى بيانات التدريب يمكن أن تجعل النموذج قويًا لمثل هذه الاضطرابات.", "text_French": "Mon travail a révélé que l'ajout d'exemples contrefactuels aux données d'entraînement peut rendre le modèle robuste à de telles perturbations.", "text_Japanese": "私の研究ではトレーニングデータに反事実の例を追加することでモデルをそのような混乱に強固にすることができることがわかりました", "text_Russian": "Моя работа показала, что добавление контрфактических примеров к данным обучения может сделать модель устойчивой к таким возмущениям."}
{"text": "So, if counterfactuals are valuable, how can we generate them?\n", "text_Chinese": "所以, 如果反事实是有价值的, 我们怎么能产生它们呢?", "text_Arabic": "إذا, إذا كانت الحقائق المضادة قيمة, كيف يمكننا توليدها?", "text_French": "Donc, si les contrefactuels sont précieux, comment pouvons-nous les générer?", "text_Japanese": "ではもし反事実が価値があるならどのようにしてそれを生成できるのでしょうか?", "text_Russian": "Итак, если контрфактуалы ценны, как мы можем их генерировать?"}
{"text": "This task is especially hard for NLP because here are three examples from three different NLP tasks.\n", "text_Chinese": "这个任务对 NLP 来说特别困难,因为这里有三个不同的 NLP 任务的三个例子", "text_Arabic": "هذه المهمة صعبة بشكل خاص بالنسبة لل NLP لأن هنا ثلاثة أمثلة من ثلاث مهام مختلفة لل NLP.", "text_French": "Cette tâche est particulièrement difficile pour la PNL car voici trois exemples de trois tâches différentes de la PNL.", "text_Japanese": "このタスクはNLPにとって特に難しいです なぜならNLPの3つの異なるタスクから3つの例がここにあります", "text_Russian": "Эта задача особенно трудна для НЛП, потому что вот три примера из трех разных задач НЛП."}
{"text": "As you can see, examples that violate the decision boundary between outcomes need to be very carefully crafted by perturbing some attributes of the text that are underlined here.\n", "text_Chinese": "正如你所看到的,违反结果之间的决策界限的例子需要非常仔细地设计,", "text_Arabic": "كما ترون، الأمثلة التي تنتهك حدود القرار بين النتائج يجب أن تكون صناعية بعناية كبيرة عن طريق إزعاج بعض سمات النص التي تم تأكيدها هنا.", "text_French": "Comme vous pouvez le voir, les exemples qui violent la limite de décision entre les résultats doivent être très soigneusement conçus en perturbant certains attributs du texte qui sont soulignés ici.", "text_Japanese": "結果間の決定境界線を 違反する例は ここで強調されたテキストのいくつかの属性を 乱すことで 慎重に作成する必要があります", "text_Russian": "Как вы можете видеть, примеры, которые нарушают границу решения между результатами, должны быть очень тщательно разработаны, нарушая некоторые атрибуты текста, которые здесь подчеркнуты."}
{"text": "This could be done by human annotation, but this is expensive and biased.\n", "text_Chinese": "这可以通过人类注释来完成, 但这很昂贵,", "text_Arabic": "يمكن القيام بذلك عن طريق التعليق البشري، ولكن هذا مكلف ومحيز.", "text_French": "Cela pourrait être fait par annotation humaine, mais c'est coûteux et biaisé.", "text_Japanese": "これは人間の注釈で行うことができますが これは高価で偏見があります", "text_Russian": "Это можно сделать с помощью человеческих аннотаций, но это дорого и предвзято."}
{"text": "Some prior work has focused on using syntax trees or semantic role labeling.\n", "text_Chinese": "一些以前的工作重点是使用语法树或语义角色标签.", "text_Arabic": "ركز بعض الأعمال السابقة على استخدام أشجار التركيب أو تسمية الأدوار الدلالية.", "text_French": "Certains travaux antérieurs se sont concentrés sur l'utilisation d'arbres de syntaxe ou d'étiquettes de rôles sémantiques.", "text_Japanese": "以前の研究では構文ツリーや意味の役割ラベルを用いることに焦点を当ててきました", "text_Russian": "Некоторые предыдущие работы были сосредоточены на использовании синтаксических деревьев или семантического маркирования ролей."}
{"text": "But the set of perturbations generated by these techniques are limited by the semantic framework.\n", "text_Chinese": "但是这些技术产生的扰动由语义框架限制.", "text_Arabic": "لكن مجموعة الاضطرابات التي تولدها هذه التقنيات محدودة من قبل الإطار الدلالي.", "text_French": "Mais l'ensemble des perturbations générées par ces techniques est limité par le cadre sémantique.", "text_Japanese": "しかし,これらの技術によって生成される乱れのセットは,意味論的フレームワークによって制限されています.", "text_Russian": "Но набор возмущений, генерируемых этими методами, ограничен семантической структурой."}
{"text": "More recent work has used masked language models to fill in masked portions of the text to change labels.\n", "text_Chinese": "最近的工作使用了掩盖的语言模型来填写文本的掩盖部分以更改标签.", "text_Arabic": "استخدمت أعمال أحدث نماذج لغة مقنعة لملء أجزاء مقنعة من النص لتغيير التسميات.", "text_French": "Des travaux plus récents ont utilisé des modèles de langage masqué pour remplir des parties masquées du texte pour changer d'étiquette.", "text_Japanese": "最近の研究ではラベルを変更するためにテキストのマスクされた部分を埋めるためにマスクされた言語モデルを使用しています", "text_Russian": "В более поздних работах использовались маскированные языковые модели для заполнения маскированных частей текста для изменения этикеток."}
{"text": "But finding what parts of the text to perturb can be challenging.\n", "text_Chinese": "但找到要扰乱的文本部分可能是一个挑战.", "text_Arabic": "لكن العثور على أجزاء من النص لتزعج يمكن أن يكون تحديا.", "text_French": "Mais trouver les parties du texte à perturber peut être difficile.", "text_Japanese": "しかしテキストのどの部分を乱すかを探すことは困難です", "text_Russian": "Но найти, какие части текста следует нарушить, может быть непросто."}
{"text": "There are more challenges to generating counterfactuals for question answering specifically.\n", "text_Chinese": "具体来说,为回答问题产生反事实存在更多挑战.", "text_Arabic": "هناك المزيد من التحديات لتوليد الحقائق المضادة للإجابة على الأسئلة على وجه التحديد.", "text_French": "Il y a plus de défis à générer des contre-faits pour répondre spécifiquement aux questions.", "text_Japanese": "具体的に質問に答えるために 対照的事実を生成するには より多くの課題があります", "text_Russian": "Существует больше проблем с созданием контрфактуальных данных для ответов на вопросы."}
{"text": "This task requires background knowledge.\n", "text_Chinese": "这项任务需要背景知识.", "text_Arabic": "هذه المهمة تتطلب معرفة خلفية.", "text_French": "Cette tâche nécessite des connaissances de base.", "text_Japanese": "この作業には背景知識が必要です.", "text_Russian": "Эта задача требует знаний о прошлом."}
{"text": "For instance, to perturb the original question is Indiana Jones Temple of Doom a prequel?\n", "text_Chinese": "例如,为了扰乱最初的问题, 印第安纳·<unk>斯的毁灭之寺是一个前传吗?", "text_Arabic": "على سبيل المثال، لإثارة السؤال الأصلي هل معبد إنديانا جونز للدم هو سابقة؟", "text_French": "Par exemple, pour perturber la question originale, le Temple de la Damnation d'Indiana Jones est-il une préquelle?", "text_Japanese": "例えばインディアナ・ジョーンズ・テンプル・オブ・ドゥームは前編ですか?", "text_Russian": "Например, чтобы нарушить первоначальный вопрос, является ли \"Храм гибели Индианы Джонса\" приквелом?"}
{"text": "We need to be aware of the other movies in the franchise to get to a question like is Indiana Jones Raiders of the Lost Ark a prequel?\n", "text_Chinese": "我们需要了解该系列的其他电影才能回答一个问题,比如印第安纳·琼斯失踪方舟的追逐者是前传吗?", "text_Arabic": "نحتاج أن نكون على دراية بالأفلام الأخرى في السلسلة للوصول إلى سؤال مثل هل Indiana Jones Raiders of the Lost Ark هو سابقة؟", "text_French": "Nous devons être conscients des autres films de la franchise pour arriver à une question comme est-ce que Indiana Jones Raiders of the Lost Ark est une préquelle?", "text_Japanese": "インディアナ・ジョーンズ 失われた箱舟のレイダーズは 前編ですか?", "text_Russian": "Нам нужно знать о других фильмах франшизы, чтобы получить ответ на вопрос, например, \"Индиана Джонс: Похитители потерянного ковчега\" - это приквел?"}
{"text": "Furthermore, random perturbations can lead to questions that are not answerable with the available evidence or have false premises.\n", "text_Chinese": "此外,随机扰动可能会导致与现有证据无法回答的问题或有错误的前提.", "text_Arabic": "علاوة على ذلك ، يمكن أن تؤدي الاضطرابات العشوائية إلى أسئلة لا يمكن الإجابة عليها باستخدام الأدلة المتاحة أو تحتوي على فرضية خاطئة.", "text_French": "En outre, les perturbations aléatoires peuvent conduire à des questions auxquelles il n'y a pas de réponse avec les preuves disponibles ou qui ont de fausses prémisses.", "text_Japanese": "さらに ランダムな混乱は 利用可能な証拠で答えられない質問や 誤った前提をもたらす可能性があります", "text_Russian": "Кроме того, случайные возмущения могут привести к вопросам, на которые нельзя ответить с помощью имеющихся доказательств или которые имеют ложные предпосылки."}
{"text": "Moreover, some question perturbations can lead to significant semantic drift from the original input.\n", "text_Chinese": "此外,一些问题扰动可能会导致与原始输入的语义偏离.", "text_Arabic": "علاوة على ذلك ، يمكن أن تؤدي بعض اضطرابات الأسئلة إلى انحراف معنوي كبير عن المدخلات الأصلية.", "text_French": "En outre, certaines perturbations de questions peuvent entraîner une dérive sémantique significative par rapport à l'entrée d'origine.", "text_Japanese": "さらに,いくつかの質問の混乱は,元の入力から大幅なセマンティックドリフトにつながる可能性があります.", "text_Russian": "Кроме того, некоторые нарушения вопросов могут привести к значительному семантическому отклонению от первоначального ввода."}
{"text": "For instance, this question is Indiana Jones practicing child slavery in Temple of Doom?\n", "text_Chinese": "例如,这个问题是印第安纳·<unk>斯在\"毁灭之寺\"中实行儿童奴隶制吗?", "text_Arabic": "على سبيل المثال، هذا السؤال هو أن إنديانا جونز يمارس عبودية الأطفال في معبد الموت؟", "text_French": "Par exemple, cette question est-ce que Indiana Jones pratique l'esclavage des enfants dans le Temple du Destin?", "text_Japanese": "例えばこの質問はインディアナ・ジョーンズがTemple of Doomで児童奴制を実践しているか?", "text_Russian": "Например, вопрос: \"Индиана Джонс практикует детское рабство в Храме Судного дня?\""}
{"text": "We propose a very simple yet effective technique called retrieve generate filter or RGF, to tackle counterfactual perturbations of questions, and also aims to tackle all the other aforementioned challenges.\n", "text_Chinese": "我们提出一种非常简单但有效的技术,称为检索生成过<unk>器或RGF,用于解决问题的假设扰动,", "text_Arabic": "نقترح تقنية بسيطة جدا ولكنها فعالة تسمى تصفية استعادة توليد أو RGF، لمعالجة الاضطرابات المضادة للحقائق من الأسئلة، ويهدف أيضا إلى معالجة جميع التحديات الأخرى المذكورة أعلاه.", "text_French": "Nous proposons une technique très simple mais efficace appelée filtre de génération de récupération ou RGF, pour s'attaquer aux perturbations contrefactuelles des questions, et vise également à s'attaquer à tous les autres défis susmentionnés.", "text_Japanese": "質問の反事実的混乱に対処するためにRGFと呼ばれる非常にシンプルで効果的な技術を提案し他のすべての前述の課題に対処することを目的としています", "text_Russian": "Мы предлагаем очень простой, но эффективный метод, называемый фильтром извлечения и генерации или RGF, для решения контрфактических возмущений вопросов, а также направлен на решение всех других вышеупомянутых проблем."}
{"text": "The core intuition behind RGF is that the necessary background information that is needed to generate perturbations may be present in the near misses made by a question answering model.\n", "text_Chinese": "RGF背后的核心直觉是,产生扰动所需的必要背景信息可能存在于一个回答问题模型所做的近错中.", "text_Arabic": "الحدس الأساسي وراء RGF هو أن المعلومات الأساسية اللازمة التي تحتاج إلى توليد الاضطرابات قد تكون موجودة في الأخطاء القريبة التي يقوم بها نموذج الإجابة على السؤال.", "text_French": "L'intuition de base derrière RGF est que les informations de fond nécessaires pour générer des perturbations peuvent être présentes dans les quasi-échecs effectués par un modèle de réponse à une question.", "text_Japanese": "RGFの背後にある核心的な直観は,乱れを生み出すために必要な背景情報は,質問回答モデルによって作られたほぼミスに存在する可能性があるということです.", "text_Russian": "Основная интуиция RGF заключается в том, что необходимая фоновая информация, необходимая для создания возмущений, может присутствовать в почти неудачных случаях, сделанных моделью ответа на вопрос."}
{"text": "For instance, the state-of-the-art model REALM produces the following top k answers to the question who is the captain of the Richmond Football Club?\n", "text_Chinese": "例如,最先进的模型REALM对 Richmond 足球俱乐部的队长是谁的问题产生了以下顶级的 k 答案?", "text_Arabic": "على سبيل المثال، نموذج REALM المتطور ينتج أفضل k إجابات التالية على السؤال من هو قائد نادي ريتشموند لكرة القدم؟", "text_French": "Par exemple, le modèle de pointe REALM produit les k réponses suivantes à la question qui est le capitaine du Richmond Football Club?", "text_Japanese": "例えば最先端のモデルREALMはリッチモンドフットボールクラブのキャプテンである誰?という質問に次のトップkの答えを生成します", "text_Russian": "Например, современная модель REALM дает следующие ответы на вопрос: кто капитан футбольного клуба Ричмонда?"}
{"text": "While it does recover the original reference passage and answer Trent Cotchin as the top most choice.\n", "text_Chinese": "虽然它确实恢复了原始参考段落,并回答了特伦特·科钦作为最受欢迎的选择.", "text_Arabic": "بينما يستعيد المقطع المرجعي الأصلي ويجيب ترينت كوتشين كأفضل خيار.", "text_French": "Alors qu'il récupère le passage de référence original et répond Trent Cotchin comme le meilleur choix.", "text_Japanese": "トレント・コッチンが一番の選択肢として答えます. トレント・コッチンが一番の選択肢として答えます.", "text_Russian": "В то время как он восстанавливает оригинальный отрывок ссылки и ответить Трент Кочин как самый лучший выбор."}
{"text": "It also retrieves additional passages and answers which can be used to guide question perturbation.\n", "text_Chinese": "它还可以检索额外的段落和答案,", "text_Arabic": "كما يسترد المقاطع والإجابات الإضافية التي يمكن استخدامها لتوجيه اضطراب السؤال.", "text_French": "Il récupère également des passages et des réponses supplémentaires qui peuvent être utilisés pour guider la perturbation des questions.", "text_Japanese": "また,質問の乱れを導くのに使用できる追加のパスと答えを回収します.", "text_Russian": "Он также извлекает дополнительные отрывки и ответы, которые могут быть использованы для руководства возмущением вопроса."}
{"text": "For instance, it recovers two more answers corresponding to the captains of the reserve team and the women's team of the same club, and this can lead to interesting edits.\n", "text_Chinese": "例如,它还恢复了两个与同一俱乐部的后备队和女子队的队长相对应的答案,", "text_Arabic": "على سبيل المثال، يسترد إجابات أخرى تتوافق مع كابتنات الفريق الاحتياطي وفريق النساء من نفس النادي، وهذا يمكن أن يؤدي إلى تحريرات مثيرة للاهتمام.", "text_French": "Par exemple, il récupère deux réponses supplémentaires correspondant aux capitaines de l'équipe de réserve et de l'équipe féminine du même club, ce qui peut conduire à des modifications intéressantes.", "text_Japanese": "例えば 予備チームと同じクラブの女子チームのキャプテンに 対応する2つの答えを回復し 興味深い編集につながります", "text_Russian": "Например, он восстанавливает еще два ответа, соответствующие капитанам резервной команды и женской команды того же клуба, и это может привести к интересным редакциям."}
{"text": "To summarize, RGF first retrieves top k most relevant answers and contexts which don't match the reference answer in context.\n", "text_Chinese": "总结起来,RGF首先检索顶部k个最相关的答案和上下文,这些答案与上下文的参考答案不匹配.", "text_Arabic": "لتلخيص، RGF أولا يسترد أعلى k الأكثر أهمية الإجابات والسياقات التي لا تتطابق مع الإجابة المرجعية في السياق.", "text_French": "Pour résumer, RGF récupère d'abord les k premières réponses et contextes les plus pertinents qui ne correspondent pas à la réponse de référence dans le contexte.", "text_Japanese": "概要として,RGFはまず,コンテキストで参照回答と一致しないトップkの最も関連する答えと文脈を回収します.", "text_Russian": "Подводя итог, RGF сначала извлекает топ k наиболее релевантных ответов и контекстов, которые не соответствуют справочному ответу в контексте."}
{"text": "Following this step, the question generation model conditions on these alternate answers to generate a question that corresponds to them.\n", "text_Chinese": "在这一步骤之后,问题生成模型对这些替代答案进行条件,以生成与它们对应的问题.", "text_Arabic": "بعد هذه الخطوة، يفرض نموذج توليد الأسئلة شروطًا على هذه الإجابات البديلة لتوليد سؤال يتوافق معها.", "text_French": "Après cette étape, le modèle de génération de questions conditionne ces réponses alternatives pour générer une question qui leur correspond.", "text_Japanese": "このステップの後,質問生成モデルはこれらの代替回答を条件にして,それらに対応する質問を生成します.", "text_Russian": "После этого шага модель генерации вопросов обусловливает эти альтернативные ответы, чтобы сгенерировать вопрос, который им соответствует."}
{"text": "And finally, we can filter the generated questions based on minimality or based on the type of semantic perturbation we are interested in introducing.\n", "text_Chinese": "最后,我们可以根据最小性或基于我们感兴趣的语义干扰类型来过<unk>生成的问题.", "text_Arabic": "وأخيرا، يمكننا تصفية الأسئلة الناتجة على أساس الحد الأدنى أو على أساس نوع الاضطراب الدلالي الذي نود تقديمه.", "text_French": "Et enfin, nous pouvons filtrer les questions générées en fonction de la minimalité ou en fonction du type de perturbation sémantique que nous sommes intéressés à introduire.", "text_Japanese": "最後に最小限性に基づいてまたは導入したいセマンティック・パーバーテーションのタイプに基づいて生成された質問をフィルターすることができます", "text_Russian": "И, наконец, мы можем фильтровать сгенерированные вопросы на основе минимальности или на основе типа семантического возмущения, которое мы заинтересованы в введении."}
{"text": "Going over each step in greater detail for retrieval, we use a retrieve then read model like REALM that takes as input the original question, and a large corpus like Wikipedia.\n", "text_Chinese": "我们使用像 REALM 这样的检索然后阅读模型,", "text_Arabic": "نذهب إلى كل خطوة بتفاصيل أكبر للانتشال، نستخدم نموذج استرداد ثم قراءة مثل REALM الذي يأخذ كمدخل السؤال الأصلي، ومجموع كبير مثل ويكيبيديا.", "text_French": "En passant en revue chaque étape plus en détail pour la récupération, nous utilisons un modèle de récupération puis de lecture comme REALM qui prend comme entrée la question originale, et un grand corpus comme Wikipedia.", "text_Japanese": "REALMのような入力として元の質問とウィキペディアのような大きなコーパスを取る", "text_Russian": "Рассматривая каждый шаг более подробно для извлечения, мы используем модель извлечения и затем чтения, такую как REALM, которая принимает в качестве ввода первоначальный вопрос и большой корпус, такой как Википедия."}
{"text": "It consists of two modules.\n", "text_Chinese": "它由两个模块组成.", "text_Arabic": "يتكون من وحدتين.", "text_French": "Il se compose de deux modules.", "text_Japanese": "2つのモジュールで構成されています.", "text_Russian": "Он состоит из двух модулей."}
{"text": "The retriever module performs similarity search over a dense index of passages to retrieve the top k most relevant passages to the question.\n", "text_Chinese": "检索模块在密集的段落索引上执行相似性搜索,以检索对问题最相关的顶部k段落.", "text_Arabic": "تقوم وحدة الاسترداد بالبحث عن التشابه على فهرس كثيف من المقاطع لاسترداد المقاطع k الأكثر أهمية للسؤال.", "text_French": "Le module de récupération effectue une recherche de similitude sur un index dense de passages pour récupérer les k premiers passages les plus pertinents pour la question.", "text_Japanese": "検索モジュールは,質問に最も関連するトップkのパスワードを検索するために,パスワードの密度の高いインデックス上で類似性検索を行います.", "text_Russian": "Модуль извлекателя выполняет поиск сходства по плотному индексу отрывков, чтобы извлечь верхние k наиболее релевантные отрывки к вопросу."}
{"text": "And a reader module then extracts a span from each passage as a potential answer.\n", "text_Chinese": "然后一个读者模块从每个段落中提取一个跨度作为潜在的答案.", "text_Arabic": "ثم تستخرج وحدة القارئ فترة من كل ممر كإجابة محتملة.", "text_French": "Et un module de lecteur extrait ensuite une portée de chaque passage comme une réponse potentielle.", "text_Japanese": "読み取りモジュールは それぞれのパスから スパンを抽出し 潜在的な答えとして表示します", "text_Russian": "И модуль-читатель затем извлекает промежуток из каждого отрывка в качестве потенциального ответа."}
{"text": "REALM retrieves the gold passage and answer in most cases.\n", "text_Chinese": "在大多数情况下, REALM 检索黄金通道和答案", "text_Arabic": "يستعيد REALM الممر الذهبي ويجيب في معظم الحالات.", "text_French": "REALM récupère le passage d'or et répond dans la plupart des cas.", "text_Japanese": "REALMはほとんどの場合ゴールドパスと答えを取り戻します", "text_Russian": "REALM извлекает золотой проход и ответ в большинстве случаев."}
{"text": "However, in this work, we are more interested in the answers and context that it retrieves further down the line.\n", "text_Chinese": "然而,在这项工作中, 我们更感兴趣的是它进一步恢复的答案和背景.", "text_Arabic": "ومع ذلك، في هذا العمل، نحن مهتمون أكثر في الإجابات والسياق الذي يسترده في وقت لاحق.", "text_French": "Cependant, dans ce travail, nous sommes plus intéressés par les réponses et le contexte qu'il récupère plus loin dans la ligne.", "text_Japanese": "しかし この研究では 答えと文脈に より興味があります 答えと文脈は 線を下に回収します", "text_Russian": "Однако в этой работе нас больше интересуют ответы и контекст, которые она извлекает в дальнейшем."}
{"text": "In the next step, question generation, we use these alternate answers and contexts to regenerate new questions that correspond to these alternatives.\n", "text_Chinese": "在下一步, 问题生成, 我们使用这些替代答案和背景来再生与这些替代答案对应的新问题", "text_Arabic": "في الخطوة التالية, إنشاء الأسئلة, نستخدم هذه الإجابات البديلة والسياقات لتجديد أسئلة جديدة تتوافق مع هذه البدائل.", "text_French": "Dans l'étape suivante, la génération de questions, nous utilisons ces réponses et contextes alternatifs pour régénérer de nouvelles questions qui correspondent à ces alternatives.", "text_Japanese": "次のステップは 質問の生成です これらの代替の答えと文脈を使って これらの代替に対応する 新しい質問を再生します", "text_Russian": "На следующем этапе, генерации вопросов, мы используем эти альтернативные ответы и контексты, чтобы регенерировать новые вопросы, которые соответствуют этим альтернативам."}
{"text": "Question generation model is a pre trained text-to-text transformer that is fine-tuned on the NQ data to generate a question for an answer that's marked in context.\n", "text_Chinese": "问题生成模型是一个预先训练的文本到文本转换器,", "text_Arabic": "نموذج توليد الأسئلة هو محول نص إلى نص مدرب مسبقًا يتم ضبطه بدقة على بيانات NQ لتوليد سؤال لإجابة تم وضع علامة عليها في السياق.", "text_French": "Le modèle de génération de questions est un transformateur texte-à-texte pré-entraîné qui est réglé sur les données NQ pour générer une question pour une réponse qui est marquée dans le contexte.", "text_Japanese": "NQデータを微調整して 問い合わせの答えを生成します 問い合わせの答えは 問い合わせの答えを生成します", "text_Russian": "Модель генерации вопросов - это предварительно обученный преобразователь текста в текст, который настраивается на данные NQ, чтобы генерировать вопрос для ответа, который отмечен в контексте."}
{"text": "During inference we supply the question generation model, the alternative answer and context that we retrieved in the previous step.\n", "text_Chinese": "在推断过程中,我们提供问题生成模型,替代答案和上一步检索的上下文.", "text_Arabic": "خلال الاستنتاج نقدم نموذج توليد السؤال، والإجابة البديلة والسياق الذي استردناه في الخطوة السابقة.", "text_French": "Pendant l'inférence, nous fournissons le modèle de génération de questions, la réponse alternative et le contexte que nous avons récupérés à l'étape précédente.", "text_Japanese": "推論の過程で質問生成モデル前回のステップで回収した代替答えと文脈を提供します", "text_Russian": "Во время вывода мы предоставляем модель генерации вопроса, альтернативный ответ и контекст, которые мы получили на предыдущем этапе."}
{"text": "For example, for the query who is the captain of the Richmond Football Club? REALM retrieves passages about the club's women's team, captained by Jess Kennedy, and the question generation model generates the query who captained Richmond Football Club's first ever women's team?\n", "text_Chinese": "例如,对于 Richmond Football Club 的队长是谁? REALM 检索了关于俱乐部女子队的段落,由 Jess Kennedy 担任队长,并且问题生成模型生成了 Richmond Football Club 第一支女子队的队长是谁?", "text_Arabic": "على سبيل المثال ، بالنسبة للطلب من هو قائد نادي ريتشموند لكرة القدم؟ يستعيد REALM المقاطع حول فريق النادي النسائي ، الذي يقوده جيس كينيدي ، ونموذج توليد الأسئلة يولد الاستعلام الذي يقود أول فريق نسائي في نادي ريتشموند لكرة القدم؟", "text_French": "Par exemple, pour la requête qui est le capitaine du Richmond Football Club? REALM récupère des passages sur l'équipe féminine du club, commandée par Jess Kennedy, et le modèle de génération de questions génère la requête qui a commandé la première équipe féminine du Richmond Football Club?", "text_Japanese": "例えばリッチモンド・フットボール・クラブのキャプテンが誰ですか?REALMはジェス・ケネディがキャプテンを務めるクラブの女子チームに関するパスワードを検索し質問生成モデルはリッチモンド・フットボール・クラブの最初の女子チームをキャプテンを務めた人物を生成します", "text_Russian": "Например, для запроса \"Кто капитан футбольного клуба Ричмонда?\" REALM извлекает отрывки о женской команде клуба, капитаном которой является Джесс Кеннеди, и модель генерации вопросов генерирует запрос \"Кто был капитаном первой женской команды футбольного клуба Ричмонда?\""}
{"text": "Which has a specific semantic perturbation.\n", "text_Chinese": "它具有特定的语义扰动.", "text_Arabic": "والذي لديه اضطراب معنوي محدد.", "text_French": "Qui a une perturbation sémantique spécifique.", "text_Japanese": "特定の意味の混乱があります", "text_Russian": "Который имеет специфическое семантическое возмущение."}
{"text": "In a similar fashion, we also get queries like who captained Richmond's VFL Reserve team?\n", "text_Chinese": "同样地,我们也收到一些查询,比如谁是里士满的VFL预备队队长?", "text_Arabic": "وبطريقة مماثلة، نحصل أيضًا على استفسارات مثل من كان قائد فريق ريتشموند الاحتياطي في VFL؟", "text_French": "De la même manière, nous recevons aussi des questions comme qui était le capitaine de l'équipe de réserve de Richmond?", "text_Japanese": "同じようにリッチモンドのVFLリザーブチームのキャプテンだった人なども質問を受けています", "text_Russian": "Аналогичным образом, мы также получаем запросы, такие как кто был капитаном резервной команды Ричмонда?"}
{"text": "Or who did graham negate in the grand final last year?\n", "text_Chinese": "或者格雷厄姆去年在决赛中否定了谁?", "text_Arabic": "أو من كان غراهام ينفي في النهائي الكبير العام الماضي؟", "text_French": "Ou qui Graham a nié dans la grande finale l'année dernière?", "text_Japanese": "それとも昨年のグレアムがグランドファイナルで否定したのは誰ですか?", "text_Russian": "Или кого Грехам отрицал в грандиозном финале в прошлом году?"}
{"text": "Finally, we filter out a subset of the generated queries based on some desired characteristics.\n", "text_Chinese": "最后,我们根据一些所需的特征过滤出生成的查询子集.", "text_Arabic": "وأخيراً، نقوم بتصفية مجموعة فرعية من الاستعلامات الناتجة بناءً على بعض الخصائص المرغوبة.", "text_French": "Enfin, nous filtrons un sous-ensemble des requêtes générées en fonction de certaines caractéristiques souhaitées.", "text_Japanese": "最後にいくつかの望ましい特徴に基づいて生成されたクエリのサブセットをフィルターします", "text_Russian": "Наконец, мы отфильтровываем подмножество сгенерированных запросов на основе некоторых желаемых характеристик."}
{"text": "As motivated earlier, we would like to ensure that the new question is still semantically close to the original.\n", "text_Chinese": "正如前面所述,我们希望确保新问题在语义上仍然与原始问题接近.", "text_Arabic": "كما هو موضح سابقاً، نود أن نتأكد من أن السؤال الجديد لا يزال قريبًا من الناحية الدلالية من السؤال الأصلي.", "text_French": "Comme nous l'avons expliqué plus haut, nous aimerions nous assurer que la nouvelle question reste sémantiquement proche de l'original.", "text_Japanese": "前述の通り,新しい質問が語的にオリジナルに近いことを確実にしたいと思います.", "text_Russian": "Как было мотивировано ранее, мы хотели бы убедиться, что новый вопрос по-прежнему семантически близок к оригиналу."}
{"text": "For filtering techniques that doesn't require additional supervision, we simply retain new questions that have a small token label edit distance from the original question.\n", "text_Chinese": "对于不需要额外的监督的过<unk>技术, 我们只需保留与原始问题有小标记标签编辑距离的新问题.", "text_Arabic": "بالنسبة لتقنيات التصفية التي لا تتطلب إشرافًا إضافيًا، ببساطة نحتفظ بالأسئلة الجديدة التي لديها مسافة تحرير علامة رمزية صغيرة من السؤال الأصلي.", "text_French": "Pour les techniques de filtrage qui ne nécessitent pas de supervision supplémentaire, nous conservons simplement de nouvelles questions qui ont une petite distance d'édition d'étiquette de jeton par rapport à la question originale.", "text_Japanese": "追加の監督を必要としないフィルタリングテクニックでは,元の質問から小さなトークンラベル編集距離を持つ新しい質問を保持します.", "text_Russian": "Для методов фильтрации, которые не требуют дополнительного надзора, мы просто сохраняем новые вопросы, которые имеют небольшое расстояние от оригинального вопроса."}
{"text": "For example, we remove the question who did graham negate in the grand final last year?\n", "text_Chinese": "例如,我们删除了去年格雷厄姆在决赛中否定了谁的问题?", "text_Arabic": "على سبيل المثال، نزيل السؤال الذي قام به (غراهام) في النهائي الكبير العام الماضي؟", "text_French": "Par exemple, nous supprimons la question qui a fait Graham négation dans la grande finale l'année dernière?", "text_Japanese": "例えば昨年のグランドファイナルでグラハム・ネガートが誰だったかという質問を削除します", "text_Russian": "Например, мы удаляем вопрос, кто проиграл Грэму в финале в прошлом году?"}
{"text": "Because it has a longer edit distance from the original question.\n", "text_Chinese": "因为它与原始问题有更长的编辑距离.", "text_Arabic": "لأنه لديه مسافة تحرير أطول من السؤال الأصلي.", "text_French": "Parce qu'il a une plus longue distance de modification de la question originale.", "text_Japanese": "なぜなら元の質問から編集距離が長いからです", "text_Russian": "Потому что у него большее расстояние от первоначального вопроса."}
{"text": "In our experiments, we demonstrate that this simple heuristic can be used to augment and queue training data.\n", "text_Chinese": "在我们的实验中,我们证明了这种简单的启发式可以用来增强和排队训练数据.", "text_Arabic": "في تجاربنا، نثبت أن هذه الاستشعارية البسيطة يمكن استخدامها لزيادة بيانات التدريب وترتيبها في طابور.", "text_French": "Dans nos expériences, nous démontrons que cette heuristique simple peut être utilisée pour augmenter et mettre en file d'attente les données d'entraînement.", "text_Japanese": "このシンプルなヒューリスティックが 訓練データを拡張し 列に並べるのに使えることを実証しました", "text_Russian": "В наших экспериментах мы демонстрируем, что эта простая эвристика может быть использована для увеличения и очереди тренировочных данных."}
{"text": "We also experiment with a filtering strategy that is based on the type of semantic perturbation.\n", "text_Chinese": "我们还试验了一种基于语义扰动类型的过<unk>策略.", "text_Arabic": "نجرّب أيضًا استراتيجية تصفية تستند إلى نوع الاضطراب الدلالي.", "text_French": "Nous expérimentons également une stratégie de filtrage basée sur le type de perturbation sémantique.", "text_Japanese": "また意味の混乱の種類に基づくフィルタリング戦略を実験しています", "text_Russian": "Мы также экспериментируем со стратегией фильтрации, основанной на типе семантического возмущения."}
{"text": "To this end, we use a general purpose query decomposition framework called QED.\n", "text_Chinese": "为此,我们使用一个名为QED的通用查询分解框架.", "text_Arabic": "لهذا الغرض، نستخدم إطار تفكيك استفسارات الغرض العام يسمى QED.", "text_French": "À cette fin, nous utilisons un cadre de décomposition de requête à usage général appelé QED.", "text_Japanese": "この目的のためにQEDと呼ばれる一般的な目的のクエリ分解フレームワークを使用します", "text_Russian": "С этой целью мы используем общую структуру декомпозиции запросов под названием QED."}
{"text": "QED identifies two parts to the question, a predicate and a reference.\n", "text_Chinese": "QED 确定了问题中的两个部分,一个前提和一个引用.", "text_Arabic": "يحدد QED جزأين للسؤال، عبارة وإشارة.", "text_French": "QED identifie deux parties à la question, un prédicat et une référence.", "text_Japanese": "QEDは質問の2つの部分,予言と参照を特定します.", "text_Russian": "QED идентифицирует две части вопроса, предикат и ссылку."}
{"text": "References are noun phrases in the question that correspond to entities in the context.\n", "text_Chinese": "引用是问题中的名词短语,与上下文中的实体相对应.", "text_Arabic": "المراجع هي عبارات اسمية في السؤال تتوافق مع الكيانات في السياق.", "text_French": "Les références sont des expressions de noms dans la question qui correspondent à des entités dans le contexte.", "text_Japanese": "参照は文脈のエンティティに対応する質問の名詞フレーズです", "text_Russian": "Ссылки - это существительные фразы в вопросе, которые соответствуют сущностям в контексте."}
{"text": "A predicate is basically the remaining portion of the question.\n", "text_Chinese": "一个前提基本上是问题的剩余部分.", "text_Arabic": "المعلنة هي في الأساس الجزء المتبقي من السؤال.", "text_French": "Un prédicat est essentiellement la partie restante de la question.", "text_Japanese": "問いかけの残りの部分です. 問いかけの残りの部分です.", "text_Russian": "Предикат - это, по сути, оставшаяся часть вопроса."}
{"text": "For example, we are able to decompose the query who captained Richmond's first ever women's team into two references: Richmond Football Club women's team and the predicate who captained X.\n", "text_Chinese": "例如,我们能够将查询谁是里士满第一个女子队的队长分解为两个引用:里士满足球俱乐部女子队和谁是X队长的前提.", "text_Arabic": "على سبيل المثال، نحن قادرين على تفكيك الاستعلام الذي كان قائد أول فريق نسائي في ريتشموند إلى مرجعين: فريق ريتشموند لكرة القدم النسائي والمقولة التي كانت قائد X.", "text_French": "Par exemple, nous sommes en mesure de décomposer la requête qui a été capitaine de la première équipe féminine de Richmond en deux références: l'équipe féminine du Richmond Football Club et le prédicat qui a été capitaine de X.", "text_Japanese": "例えばリッチモンド初の女子チームのキャプテンを2つの参照に分解することができますリッチモンドフットボールクラブ女子チームとキャプテンをXにした予測です", "text_Russian": "Например, мы можем разложить запрос, кто был капитаном первой женской команды Ричмонда, на две ссылки: женская команда футбольного клуба Ричмонда и предикат, который был капитаном X."}
{"text": "A model trained on reference predicate annotations for NQ gives us this question decomposition.\n", "text_Chinese": "一个基于NQ的参考前提注释训练的模型给了我们这个问题分解.", "text_Arabic": "نموذج مدرب على ملاحظات المعلنة المرجعية لـ NQ يعطينا هذا التفكيك السؤال.", "text_French": "Un modèle formé sur des annotations de prédicats de référence pour NQ nous donne cette décomposition de la question.", "text_Japanese": "NQの参照前提アノテーションで訓練されたモデルではこの質問分解が得られます", "text_Russian": "Модель, обученная аннотациям ссылочных предикатов для NQ, дает нам эту декомпозицию вопросов."}
{"text": "Decomposing both the original and generated question based on QED allows us to categorize our generated counterfactuals for evaluation.\n", "text_Chinese": "根据QED分解原始和生成的问题, 允许我们对生成的反事实进行分类以进行评估.", "text_Arabic": "يسمح لنا تفكيك كل من السؤال الأصلي والسؤال الناتج على أساس QED بتصنيف حقائقنا المضادة الناتجة للتقييم.", "text_French": "La décomposition à la fois de la question originale et de la question générée sur la base de QED nous permet de catégoriser nos contre-faits générés pour l'évaluation.", "text_Japanese": "QEDに基づいて元の質問と生成された質問の両方を分解すると,評価のために生成された反事実を分類することができます.", "text_Russian": "Разложение как первоначального, так и сгенерированного вопроса на основе QED позволяет нам классифицировать наши сгенерированные контрфактуалы для оценки."}
{"text": "Specifically, we obtain two groups of questions.\n", "text_Chinese": "具体来说,我们得到两组问题.", "text_Arabic": "على وجه التحديد، نحصل على مجموعتين من الأسئلة.", "text_French": "Plus précisément, nous obtenons deux groupes de questions.", "text_Japanese": "具体的には2つのグループの質問を得ます", "text_Russian": "В частности, мы получаем две группы вопросов."}
{"text": "Those that undergo a reference change while retaining predicates, and those that undergo a predicate change and optionally add references.\n", "text_Chinese": "那些在保留前言的同时经历引用变化的人,以及那些经历前言变化并可选地添加引用的人.", "text_Arabic": "تلك التي تخضع لتغيير مرجعي مع الاحتفاظ بالقوليات، وتلك التي تخضع لتغيير قولية وتضيف مرجعية اختيارية.", "text_French": "Ceux qui subissent un changement de référence tout en conservant des prédicats, et ceux qui subissent un changement de prédicat et ajoutent facultativement des références.", "text_Japanese": "引用を変更しながら引用を保持するものと,引用を変更し,選択的に引用を追加するもの.", "text_Russian": "Те, которые подвергаются изменению ссылки, сохраняя предикаты, и те, которые подвергаются изменению предиката и необязательно добавляют ссылки."}
{"text": "For instance, who captained Richmond's VFL reserve team is a reference change?\n", "text_Chinese": "例如,谁是里士满的VFL预备队队长是一个参考变化?", "text_Arabic": "على سبيل المثال، من كان قائد فريق ريتشموند الاحتياطي في فيفل هو تغيير مرجعي؟", "text_French": "Par exemple, qui a été le capitaine de l'équipe de réserve de Richmond est un changement de référence?", "text_Japanese": "例えばリッチモンドのVFL予備チームのキャプテンが参照変更である人は?", "text_Russian": "Например, кто был капитаном резервной команды Ричмонда в VFL?"}
{"text": "While, who wears number nine for the club is a predicate change.\n", "text_Chinese": "而谁在俱乐部里穿九号是前提变化.", "text_Arabic": "بينما، من يرتدي الرقم تسعة للنادي هو تغيير المقال.", "text_French": "Alors que, qui porte le numéro neuf pour le club est un changement de prédicat.", "text_Japanese": "クラブで9番を着る人は 予言の変化です", "text_Russian": "В то время как, кто носит номер девять для клуба является изменением предиката."}
{"text": "We now evaluate the effectiveness of RGF perturbations when augmented to training data.\n", "text_Chinese": "我们现在评估RGF扰动的有效性,", "text_Arabic": "نحن الآن نقيّم فعالية اضطرابات RGF عند زيادتها إلى بيانات التدريب.", "text_French": "Nous évaluons maintenant l'efficacité des perturbations RGF lorsqu'elles sont augmentées aux données d'entraînement.", "text_Japanese": "RGFの乱れの有効性を 評価します", "text_Russian": "Теперь мы оцениваем эффективность возмущений RGF при добавлении к данным обучения."}
{"text": "So, to effectively evaluate the effectiveness of counterfactual augmentation in particular, we experiment with two strong data augmentation baselines.\n", "text_Chinese": "因此,为了有效地评估假设增强的有效性, 我们使用两个强大的数据增强基线进行实验.", "text_Arabic": "لذلك، لتقييم فعالية التوسع في الحقائق المضادة على وجه الخصوص، نجرب خطين أساسيين قويين لتوسيع البيانات.", "text_French": "Donc, pour évaluer efficacement l'efficacité de l'augmentation contrefactuelle en particulier, nous expérimentons avec deux lignes de base d'augmentation de données fortes.", "text_Japanese": "具体的に対照的な拡張の有効性を 効果的に評価するために 2つの強力なデータ拡張ベースラインを実験します", "text_Russian": "Таким образом, чтобы эффективно оценить эффективность контрфактического увеличения, мы экспериментируем с двумя сильными базовыми линиями увеличения данных."}
{"text": "The first baseline, called random answer and question generation, adds data that has no relation with the original question.\n", "text_Chinese": "第一个基线,称为随机答案和问题生成, 添加与原始问题无关的数据.", "text_Arabic": "خط الأساس الأول، يسمى إجابات عشوائية وتوليد الأسئلة، يضيف بيانات لا علاقة لها بالسؤال الأصلي.", "text_French": "La première ligne de base, appelée génération de réponses et de questions aléatoires, ajoute des données qui n'ont aucun lien avec la question originale.", "text_Japanese": "最初のベースラインは ランダム回答と質問生成と呼ばれ オリジナルの質問と関係のないデータを追加します", "text_Russian": "Первая исходная линия, называемая генерацией случайных ответов и вопросов, добавляет данные, которые не имеют отношения к первоначальному вопросу."}
{"text": "That is, passages and answers are simply randomly sampled from Wikipedia.\n", "text_Chinese": "也就是说,段落和答案只是随机从维基百科中抽取的.", "text_Arabic": "أي أن المقاطع والإجابات يتم اختبارها عشوائياً من ويكيبيديا.", "text_French": "C'est-à-dire que les passages et les réponses sont simplement échantillonnés au hasard de Wikipedia.", "text_Japanese": "つまり文章と答えは単にウィキペディアからランダムにサンプリングされます", "text_Russian": "То есть, отрывки и ответы просто случайно отобраны из Википедии."}
{"text": "This baseline basically adds more data that looks like NQ.\n", "text_Chinese": "这个基线基本上添加了更多的数据,看起来像NQ.", "text_Arabic": "هذا الخط الأساسي يضيف أساسا المزيد من البيانات التي تبدو مثل NQ.", "text_French": "Cette ligne de base ajoute essentiellement plus de données qui ressemblent à NQ.", "text_Japanese": "このベースラインは基本的にNQに似たデータを追加します.", "text_Russian": "Эта исходная линия в основном добавляет больше данных, которые выглядят как NQ."}
{"text": "With the second baseline gold answer and question generation, we specifically update the retrieval portion of our method.\n", "text_Chinese": "通过第二个基线黄金答案和问题生成, 我们特别更新了方法的检索部分.", "text_Arabic": "مع الجواب الذهبي الأساسي الثاني وتوليد الأسئلة، نقوم بتحديث جزء الاسترداد من طريقتنا على وجه التحديد.", "text_French": "Avec la deuxième réponse d'or de base et la génération de questions, nous mettons spécifiquement à jour la partie de récupération de notre méthode.", "text_Japanese": "2番目のベースラインのゴールデン回答と質問生成で私たちは特に方法の検索部分を更新します", "text_Russian": "С вторым базовым золотым ответом и генерацией вопросов мы специально обновляем часть поиска нашего метода."}
{"text": "Here, alternate answers are just chosen from the same passage that contained the gold answer.\n", "text_Chinese": "在这里, 替代答案只是从包含黄金答案的同一段落中选择.", "text_Arabic": "هنا، يتم اختيار إجابات بديلة من نفس المقطع الذي يحتوي على الإجابة الذهبية.", "text_French": "Ici, les réponses alternatives sont simplement choisies dans le même passage qui contenait la réponse d'or.", "text_Japanese": "ここでは 選択肢の答えは 黄金の答えが含まれていた 同じ段落から選ばれています", "text_Russian": "Здесь альтернативные ответы просто выбираются из того же отрывка, который содержал золотой ответ."}
{"text": "How base how do the baselines and RGF ah augmentation perform on reading comprehension where the model has access to question and context?\n", "text_Chinese": "模型可以访问问题和上下文的阅读理解率如何?", "text_Arabic": "كيف تقوم خطوط الأساس وزيادة RGF على فهم القراءة حيث يمكن للنموذج الوصول إلى السؤال والسياق؟", "text_French": "Comment les lignes de base et l'augmentation RGF se comportent-elles en termes de compréhension de la lecture lorsque le modèle a accès à la question et au contexte?", "text_Japanese": "モデルが質問と文脈にアクセスできる場合ベースラインとRGFの増強はどのように読解能力をベースにしていますか?", "text_Russian": "Как основываются базовые линии и RGF ah augmentation на понимании чтения, где модель имеет доступ к вопросу и контексту?"}
{"text": "We experiment with six out of domain datasets and present results here, where data is the training data is doubled in augmentation.\n", "text_Chinese": "我们用六个域外数据集进行实验,并在这里展示结果,其中数据是训练数据的倍增.", "text_Arabic": "نجرّب ستة مجموعات بيانات خارج المجال ونقدم النتائج هنا، حيث يتم مضاعفة بيانات التدريب في التوسيع.", "text_French": "Nous expérimentons avec six ensembles de données hors domaine et présentons les résultats ici, où les données sont les données d'entraînement sont doublées en augmentation.", "text_Japanese": "6つのドメインデータセットで実験し 結果をここに提示します データはトレーニングデータの倍増です", "text_Russian": "Мы экспериментируем с шестью наборами данных из домена и представляем результаты здесь, где данные - данные обучения удваиваются в увеличении."}
{"text": "We find that both data augmentation baselines are not able to improve our domain generalization.\n", "text_Chinese": "我们发现两个数据增强基线都无法改善我们的域概括.", "text_Arabic": "نجد أن كلا خط الأساسين لزيادة البيانات غير قادرين على تحسين تعميم المجال لدينا.", "text_French": "Nous constatons que les deux lignes de base d'augmentation des données ne sont pas en mesure d'améliorer notre généralisation de domaine.", "text_Japanese": "データ増強のベースラインはどちらも ドメインの一般化を改善できないことがわかりました", "text_Russian": "Мы обнаруживаем, что обе базовые линии увеличения данных не способны улучшить наше обобщение домена."}
{"text": "In fact, an ensemble of six models trained on the original data seems to be the most competitive baseline.\n", "text_Chinese": "事实上,一个由六个模型组成的集合, 训练在原始数据上, 似乎是最具竞争力的基线.", "text_Arabic": "في الواقع، مجموعة من ستة نماذج مدربة على البيانات الأصلية يبدو أن يكون خط الأساس الأكثر تنافسية.", "text_French": "En fait, un ensemble de six modèles formés sur les données originales semble être la base de référence la plus compétitive.", "text_Japanese": "実際オリジナルのデータで訓練された6つのモデルが最も競争力のあるベースラインであるように見えます", "text_Russian": "На самом деле, ансамбль из шести моделей, обученных на оригинальных данных, кажется, является наиболее конкурентоспособной базовой линией."}
{"text": "Comparing against that baseline, we find that RGF counterfactuals are able to improve out of domain performance while maintaining in domain performance.\n", "text_Chinese": "与该基线进行比较,我们发现RGF反事实能够在保持域内性能的同时改善域外性能.", "text_Arabic": "بالمقارنة مع خط الأساس هذا، نجد أن RGF counterfactuals قادرة على تحسين خارج أداء المجال مع الحفاظ على أداء المجال.", "text_French": "En comparant avec cette ligne de base, nous constatons que les contre-factuels RGF sont capables d'améliorer les performances hors domaine tout en maintenant les performances dans le domaine.", "text_Japanese": "そのベースラインと比較すると,RGFの対照は,ドメインのパフォーマンスを維持しながら,ドメインのパフォーマンスを改善することができることがわかります.", "text_Russian": "Сравнивая с этой базовой линией, мы обнаруживаем, что контрфактуалы RGF способны улучшить производительность вне домена при сохранении производительности в домене."}
{"text": "This suggests that filling in the reasoning gaps of the model via counterfactual augmentation is more effective than adding more data from the training distribution.\n", "text_Chinese": "这表明通过对比增强来填补模型中的推理空白比从训练分布中添加更多数据更有效.", "text_Arabic": "هذا يشير إلى أن ملء فجوات التفكير في النموذج عن طريق التوسع الواقعي المضاد هو أكثر فعالية من إضافة المزيد من البيانات من توزيع التدريب.", "text_French": "Cela suggère que le remplissage des lacunes de raisonnement du modèle par augmentation contrefactuelle est plus efficace que l'ajout de plus de données de la distribution de formation.", "text_Japanese": "これは,対照的増強を介してモデルの推論のギャップを埋めることが,トレーニング分布からより多くのデータを追加するよりも効果的であることを示唆しています.", "text_Russian": "Это говорит о том, что заполнение разрывов в рассуждениях модели с помощью контрфактического увеличения более эффективно, чем добавление большего количества данных из распределения обучения."}
{"text": "Furthermore, we find that using retrieval to sample alternative outcomes or answers is important for effective CDA.\n", "text_Chinese": "此外,我们发现使用检索来抽样替代结果或答案对于有效的CDA很重要.", "text_Arabic": "علاوة على ذلك ، نجد أن استخدام الاسترداد لعينة النتائج أو الإجابات البديلة مهم للتقييم التعاوني الفعال.", "text_French": "En outre, nous constatons que l'utilisation de la récupération pour échantillonner des résultats ou des réponses alternatives est importante pour une ADC efficace.", "text_Japanese": "さらに代替結果や答えのサンプリングのためにリクエストを使用することは効果的なCDAにとって重要であることがわかりました", "text_Russian": "Кроме того, мы обнаруживаем, что использование извлечения для выборки альтернативных результатов или ответов важно для эффективной CDA."}
{"text": "We also experiment with open domain QA setting where the model only sees the question and once again we evaluate on four out of domain datasets.\n", "text_Chinese": "我们还尝试了开放域QA设置,其中模型只看到问题,并且再次评估四个域外数据集.", "text_Arabic": "نجرّب أيضًا إعدادًا مفتوحًا لضبط الجودة حيث يرى النموذج السؤال فقط ومرة أخرى نقوم بالتقييم على أربع مجموعات بيانات خارج النطاق.", "text_French": "Nous expérimentons également avec un réglage QA de domaine ouvert où le modèle ne voit que la question et une fois de plus nous évaluons sur quatre ensembles de données hors domaine.", "text_Japanese": "モデルが質問のみを見るオープンドメインQA設定も実験し再び4つのドメイン外データセットを評価します", "text_Russian": "Мы также экспериментируем с настройкой QA открытого домена, где модель видит только вопрос, и снова мы оцениваем на четырех наборах данных вне домена."}
{"text": "We find that baseline models are not as effective for out of domain generalization.\n", "text_Chinese": "我们发现基线模型对于域外概括来说并不那么有效.", "text_Arabic": "نجد أن نماذج الخط الأساسي ليست فعالة مثلها في التعميم خارج المجال.", "text_French": "Nous trouvons que les modèles de base ne sont pas aussi efficaces pour la généralisation hors domaine.", "text_Japanese": "ベースラインモデルは 領域外の一般化には 効果的ではないことがわかりました", "text_Russian": "Мы обнаружили, что базовые модели не так эффективны для обобщения вне домена."}
{"text": "However, data augmentation with RGF shows more significant improvements.\n", "text_Chinese": "然而,使用RGF的数据增强显示了更显著的改善.", "text_Arabic": "ومع ذلك، فإن زيادة البيانات مع RGF تظهر تحسينات أكثر أهمية.", "text_French": "Cependant, l'augmentation des données avec RGF montre des améliorations plus significatives.", "text_Japanese": "しかし,RGFによるデータ増強は,より重要な改善を示しています.", "text_Russian": "Однако увеличение данных с помощью RGF показывает более значительные улучшения."}
{"text": "We even improve in the in domain NQ dataset.\n", "text_Chinese": "我们甚至在域内NQ数据集中有所改进.", "text_Arabic": "حتى أننا تحسننا في مجموعة بيانات NQ في المجال.", "text_French": "Nous améliorons même dans le jeu de données NQ dans le domaine.", "text_Japanese": "ドメイン内のNQデータセットでさえ改善しました", "text_Russian": "Мы даже улучшили набор данных NQ в домене."}
{"text": "We hypothesized that the counterfactual data augmentation aids the model in learning better query encodings for very similar queries.\n", "text_Chinese": "我们假设对比数据增强有助于模型学习非常相似的查询编码.", "text_Arabic": "افترضنا أن زيادة البيانات المضادة تساعد النموذج في تعلم ترميزات استفسارات أفضل للاستفسارات المتشابهة للغاية.", "text_French": "Nous avons émis l'hypothèse que l'augmentation des données contrefactuelles aide le modèle à apprendre de meilleurs codages de requêtes pour des requêtes très similaires.", "text_Japanese": "対照的なデータ増強は非常に類似したクエリのより良いクエリエンコーディングを学習するモデルを支援すると仮定しました", "text_Russian": "Мы предположили, что увеличение контрфактических данных помогает модели в изучении лучшего кодирования запросов для очень похожих запросов."}
{"text": "Finally, we also evaluate on the model's ability to improve consistency in the local neighborhood of the original question.\n", "text_Chinese": "最后,我们还评估模型在原始问题本地邻近地区提高一致性的能力.", "text_Arabic": "وأخيراً، نقوم أيضًا بتقييم قدرة النموذج على تحسين الاتساق في الحي المحلي للسؤال الأصلي.", "text_French": "Enfin, nous évaluons également la capacité du modèle à améliorer la cohérence dans le voisinage local de la question originale.", "text_Japanese": "最後に当初の質問のローカルな近隣で一貫性を向上させるモデルの能力についても評価します", "text_Russian": "Наконец, мы также оцениваем способность модели улучшать согласованность в местном районе первоначального вопроса."}
{"text": "Consistency measures the proportion of questions correctly answered by the model where both the original and the counterfactual query are correctly answered.\n", "text_Chinese": "一致性衡量了模型正确回答的问题的比例,其中原始查询和反事实查询都正确回答.", "text_Arabic": "يقياس الاتساق نسبة الأسئلة التي يجيب عليها النموذج بشكل صحيح حيث يتم الإجابة بشكل صحيح على كل من الاستعلام الأصلي والواقعي المضاد.", "text_French": "La cohérence mesure la proportion de questions répondues correctement par le modèle où la requête originale et la requête contrefactuelle sont répondues correctement.", "text_Japanese": "一貫性は,元の質問と反事実の質問の両方が正しく回答された場合,モデルによって正しく回答された質問の割合を測定します.", "text_Russian": "Согласованность измеряет долю вопросов, правильно отвеченных моделью, когда как первоначальный, так и контрфактический запрос правильно отвечены."}
{"text": "This explicitly helps us to measure the model's robustness to small perturbations in the neighborhood of the original input.\n", "text_Chinese": "这明确帮助我们测量模型对原始输入附近小扰动的坚固性.", "text_Arabic": "هذا يساعدنا بشكل صريح على قياس قوة النموذج على الاضطرابات الصغيرة في حي المدخل الأصلي.", "text_French": "Cela nous aide explicitement à mesurer la robustesse du modèle à de petites perturbations dans le voisinage de l'entrée originale.", "text_Japanese": "これは,オリジナルの入力の近隣の小さな乱れに対するモデルの堅牢性を測定するのに明示的に役立ちます.", "text_Russian": "Это явно помогает нам измерить надежность модели к небольшим возмущениям в окрестностях первоначального ввода."}
{"text": "We experiment with five datasets which contain pairs of questions that are semantically close to each other.\n", "text_Chinese": "我们用五个数据集进行实验, 这些数据集包含了两个语义上彼此接近的问题.", "text_Arabic": "نجرّب خمس مجموعات بيانات تحتوي على أزواج من الأسئلة القريبة من بعضها البعض من الناحية الدلالية.", "text_French": "Nous expérimentons avec cinq ensembles de données qui contiennent des paires de questions qui sont sémantiquement proches les unes des autres.", "text_Japanese": "意味的に近い質問のペアを含む 5つのデータセットで実験します", "text_Russian": "Мы экспериментируем с пятью наборами данных, которые содержат пары вопросов, которые семантически близки друг к другу."}
{"text": "Apart from the three datasets AQA, AmbigQA and QUOREF-Contrast set that are already available, we also evaluate on RGF counterfactuals that are paired with original NQ questions based on whether they underwent a predicate change or reference change.\n", "text_Chinese": "除了已经可用的三组数据集AQA,AmbigQA和QUOREF-Contrast集之外,我们还根据它们是否经历了前提变化或参考变化来评估与原始NQ问题相配对的RGF反事实.", "text_Arabic": "بصرف النظر عن مجموعات البيانات الثلاث AQA و AmbigQA و QUOREF-Contrast التي هي متاحة بالفعل ، فإننا نقوم أيضًا بتقييم على RGF counterfactuals التي يتم مزجها مع أسئلة NQ الأصلية بناءً على ما إذا كانت قد خضعت لتغيير المقال أو تغيير المرجع.", "text_French": "Outre les trois ensembles de données AQA, AmbigQA et QUOREF-Contrast déjà disponibles, nous évaluons également les contre-factuels RGF qui sont jumelés aux questions NQ originales en fonction du fait qu'elles ont subi un changement de prédicat ou un changement de référence.", "text_Japanese": "既に利用可能な3つのデータセットであるAQA,AmbigQA,QUOREF-Contrastセットに加えて,元のNQ質問とペアリングされたRGFのカウンターファクタールも,前提変更または参照変更を受けたかどうかに基づいて評価します.", "text_Russian": "Помимо трех наборов данных AQA, AmbigQA и QUOREF-Contrast, которые уже доступны, мы также оцениваем контрфактуалы RGF, которые сочетаются с оригинальными вопросами NQ на основе того, подверглись ли они изменению предиката или изменению ссылки."}
{"text": "These subsets were annotated in-house to eliminate noise and are provided as a resource.\n", "text_Chinese": "这些子集是内部注释的,以消除噪音,并作为资源提供.", "text_Arabic": "تم شرح هذه المجموعات الفرعية داخليًا للقضاء على الضوضاء ويتم توفيرها كمورد.", "text_French": "Ces sous-ensembles ont été annotés en interne pour éliminer le bruit et sont fournis comme ressource.", "text_Japanese": "これらのサブセットはノイズを排除するために社内で注釈されリソースとして提供されています", "text_Russian": "Эти подмножества были аннотированы внутри компании для устранения шума и предоставляются в качестве ресурса."}
{"text": "All baselines are unable to significantly improve consistency with the ensemble model improving consistency by a small margin.\n", "text_Chinese": "所有基线都无法显著提高与整体模型的一致性,而整体模型提高了一致性只有小幅度.", "text_Arabic": "لا تستطيع جميع خطوط الأساس تحسين الاتساق بشكل كبير مع نموذج المجموعة الذي يحسن الاتساق بفارق صغير.", "text_French": "Toutes les lignes de base ne sont pas en mesure d'améliorer de manière significative la cohérence avec le modèle d'ensemble, améliorant la cohérence d'une petite marge.", "text_Japanese": "すべてのベースラインは,アンサンブルモデルとの整合性を大幅に改善することができず,整合性をわずかな差で改善します.", "text_Russian": "Все исходные линии не могут значительно улучшить согласованность с моделью ансамбля, улучшая согласованность с небольшим отрывом."}
{"text": "However, RGF counterfactual augmentation has impressive gains in consistency both on prior datasets and the two subsets we curated for reference and predicate perturbations.\n", "text_Chinese": "然而,RGF反事实增强在以前的数据集和我们为参考和前提扰动整理的两个子集的一致性方面取得了令人印象深刻的收益.", "text_Arabic": "ومع ذلك، فإن التوسع التنازلي لـ RGF لديه مكاسب مثيرة للإعجاب في الاتساق على كل من مجموعات البيانات السابقة والمجموعات الفرعية التي قمنا بتدبيرها للمراجع والاضطرابات المسبقة.", "text_French": "Cependant, l'augmentation contrefactuelle du RGF a des gains impressionnants en cohérence à la fois sur les ensembles de données précédents et sur les deux sous-ensembles que nous avons sélectionnés pour les perturbations de référence et de prédicat.", "text_Japanese": "しかし,RGFの反事実増強は,以前のデータセットと,リファレンスとプレディケートの乱れのために私たちがキュレーションした2つのサブセットの両方で,一貫性で印象的な利益を得ています.", "text_Russian": "Тем не менее, контрфактическое увеличение RGF имеет впечатляющие достижения в согласованности как на предыдущих наборах данных, так и на двух подмножествах, которые мы curated для справочных и предикатных возмущений."}
{"text": "Note that the augmented RGF data is not biased by perturbation type, only the evaluation sets are.\n", "text_Chinese": "请注意,增强的RGF数据不受扰动类型的偏差,只有评估集是.", "text_Arabic": "لاحظ أن بيانات RGF المعززة ليست منحازة حسب نوع الاضطراب ، فقط مجموعات التقييم.", "text_French": "Notez que les données RGF augmentées ne sont pas biaisées par le type de perturbation, seules les séries d'évaluation le sont.", "text_Japanese": "拡張されたRGFデータは,障害タイプによって偏向していないことに注意してください.評価セットのみです.", "text_Russian": "Обратите внимание, что расширенные данные RGF не подвержены смещению по типу возмущения, только наборы оценки."}
{"text": "In fact, a qualitative inspection of the kinds of counterfactuals generated show that the generated questions contain several diverse perturbations.\n", "text_Chinese": "事实上,对生成的反事实的质量检查表明,生成的问题包含了几种不同的扰动.", "text_Arabic": "في الواقع، فإن الفحص النوعي لأنواع الحقائق المضادة الناتجة يظهر أن الأسئلة الناتجة تحتوي على العديد من الاضطرابات المتنوعة.", "text_French": "En fait, une inspection qualitative des types de contrefactuels générés montre que les questions générées contiennent plusieurs perturbations diverses.", "text_Japanese": "実際生成された反事実の種類の質的な検査は生成された質問にはいくつかの異なる混乱が含まれていることを示しています", "text_Russian": "Фактически, качественная проверка типов генерируемых контрфактуальных данных показывает, что генерируемые вопросы содержат несколько различных возмущений."}
{"text": "For instance, this original question on the population of Walnut Grove, Minnesota is perturbed along different dimensions like town, state, country, and along different predicates like location, poverty, number of schools.\n", "text_Chinese": "例如, 这个关于明尼苏达州沃尔纳特格罗夫的人口的原始问题, 是根据不同的维度, 比如城市, 州, 国家, 以及不同的预测, 比如位置, 贫困, 学校数量, 进行调节的", "text_Arabic": "على سبيل المثال, هذا السؤال الأصلي عن عدد سكان وولنت غروف, مينيسوتا, متقلب على طول أبعاد مختلفة مثل المدينة, الولاية, البلد, وعلى طول محددات مختلفة مثل الموقع, الفقر, عدد المدارس.", "text_French": "Par exemple, cette question originale sur la population de Walnut Grove, Minnesota est perturbée selon différentes dimensions comme la ville, l'état, le pays, et selon différents prédicats comme l'emplacement, la pauvreté, le nombre d'écoles.", "text_Japanese": "例えばミネソタ州ウォルナットグローブの人口に関するこの元の質問は町州国などの異なる次元と場所貧困学校の数などの異なる前提に沿って混乱しています", "text_Russian": "Например, этот первоначальный вопрос о населении Уолнат-Гроув, штат Миннесота, разбросан по различным измерениям, таким как город, штат, страна, и по различным предикатам, таким как местоположение, бедность, количество школ."}
{"text": "Audio of perturbations are context specific.\n", "text_Chinese": "扰动的声音是特定于环境的.", "text_Arabic": "الصوت من الاضطرابات هو محدد السياق.", "text_French": "L'audio des perturbations est spécifique au contexte.", "text_Japanese": "乱音は文脈に固有です", "text_Russian": "Звук возмущений зависит от контекста."}
{"text": "For example, for this other question about the Wimbledon ah singles tournament, the perturbation is along type of game, type of tournament, or the game outcome.\n", "text_Chinese": "例如,对于关于温布尔登单打比赛的另一个问题, 扰动是沿着比赛类型, 比赛类型, 或游戏结果.", "text_Arabic": "على سبيل المثال, بالنسبة لهذا السؤال الآخر حول بطولة ويمبلدون للأفراد, الاضطراب هو على طول نوع اللعبة, نوع البطولة, أو نتيجة اللعبة.", "text_French": "Par exemple, pour cette autre question sur le tournoi de Wimbledon, la perturbation est le type de jeu, le type de tournoi ou le résultat du jeu.", "text_Japanese": "例えばウィンブルドン・ア・シングルス・トーナメントに関するこの他の質問では混乱はゲームの種類トーナメントの種類またはゲームの結果に沿っています", "text_Russian": "Например, для этого другого вопроса о турнире в одиночку на Уимблдоне, возмущение находится вдоль типа игры, типа турнира или результата игры."}
{"text": "Final takeaways; we tackle the task of counterfactual data augmentation and perturbations for information seeking queries and tackle its unique challenges via a reversal of the generation approach, over generate using near misses of the model and filter based on perturbation type or minimality.\n", "text_Chinese": "最后要点;我们解决了对比数据增强和扰动的任务,以寻求信息查询,并通过逆转生成方法解决其独特的挑战,使用模型的近错和基于扰动类型或最小度的过滤器进行过度生成.", "text_Arabic": "الملاحظات النهائية؛ نتعامل مع مهمة زيادة البيانات المضادة والاضطرابات لطلبات البحث عن المعلومات والتعامل مع تحدياتها الفريدة من خلال عكس نهج التوليد، والتوليد الزائد باستخدام الأخطاء القريبة من النموذج والتصفية على أساس نوع الاضطراب أو الحد الأدنى.", "text_French": "Derniers conseils; nous nous attaquons à la tâche de l'augmentation et des perturbations contrefactuelles des données pour les requêtes de recherche d'informations et nous nous attaquons à ses défis uniques par une inversion de l'approche de génération, une surgénération en utilisant des erreurs proches du modèle et un filtre basé sur le type de perturbation ou la minimalité.", "text_Japanese": "最後のテイクアウェイ;我々は情報を求めるクエリのための対照的なデータ増強と乱れの課題に取り組んで,生成アプローチの逆転を通じて,モデルとフィルターの近くのミスを使用して生成し,乱れのタイプまたは最小限に基づいて解決します.", "text_Russian": "Заключительные выводы; мы решаем задачу контрфактического увеличения данных и возмущений для запросов по поиску информации и решаем его уникальные проблемы с помощью обратного подхода к генерации, сверхгенерирования с использованием близких ошибок модели и фильтра, основанного на типе возмущения или минимальности."}
{"text": "We find that this technique requires no additional supervision and the examples are labeled for augmentation.\n", "text_Chinese": "我们发现这种技术不需要额外的监督,并且这些示例被标记为增强.", "text_Arabic": "نجد أن هذه التقنية لا تتطلب إشرافًا إضافيًا ويتم تسمية الأمثلة للتكبير.", "text_French": "Nous constatons que cette technique ne nécessite pas de supervision supplémentaire et que les exemples sont étiquetés pour l'augmentation.", "text_Japanese": "この技術は追加の監督を必要とせず,例は拡張のためにラベル付けされています.", "text_Russian": "Мы обнаружили, что этот метод не требует дополнительного надзора, и примеры маркированы для увеличения."}
{"text": "Augmentation improves out of domain generalization and neighborhood consistency.\n", "text_Chinese": "增强可以通过域泛化和邻近一致性来改善.", "text_Arabic": "التوسيع يحسن من تعميم المجال واتساق الحي.", "text_French": "L'augmentation s'améliore hors de la généralisation du domaine et de la cohérence du voisinage.", "text_Japanese": "拡張はドメインの一般化と近隣の一貫性から改善されます.", "text_Russian": "Увеличение улучшается за счет обобщения домена и согласованности соседства."}
{"text": "And we find that RGF counterfactuals are semantically diverse without introducing bias during augmentation.\n", "text_Chinese": "我们发现RGF反事实在语义上是不同的,", "text_Arabic": "ونجد أن RGF counterfactuals متنوعة من الناحية الدلالية دون إدخال التحيز أثناء التوسع.", "text_French": "Et nous trouvons que les contre-factuels RGF sont sémantiquement divers sans introduire de biais pendant l'augmentation.", "text_Japanese": "RGFの反事実は 拡張の際に偏見を導入することなく 意味的に多様であることがわかりました", "text_Russian": "И мы обнаруживаем, что контрфактуалы RGF семантически разнообразны без введения предвзятости во время увеличения."}
{"text": "Thank you.\n", "text_Chinese": "谢谢你.", "text_Arabic": "شكراً لك.", "text_French": "Je vous remercie.", "text_Japanese": "ありがとうございました", "text_Russian": "- Спасибо. - Спасибо."}
