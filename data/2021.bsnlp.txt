{'en': 'HerBERT : Efficiently Pretrained Transformer-based Language Model for Polish', 'pt': 'HerBERT: modelo de linguagem baseado em transformador eficientemente pré-treinado para polonês', 'ar': 'هربرت: نموذج اللغة البولندية المعتمد على المحولات بكفاءة', 'fr': 'HerBert\xa0: modèle de langage préentraîné efficace basé sur un transformateur pour le polonais', 'es': 'HerBert: modelo de lenguaje para polaco basado en transformadores preentrenado de manera eficiente', 'ja': 'HerBERT ：ポーランド語のための効率的な事前訓練された変圧器ベースの言語モデル', 'zh': 'HerBERT:变压器波兰语高效豫训言模', 'hi': 'HerBERT: पोलिश के लिए कुशलतापूर्वक Pretrained ट्रांसफॉर्मर-आधारित भाषा मॉडल', 'ru': 'HerBERT: Эффективно обученная трансформерная языковая модель для польского языка', 'ga': 'HerBERT: Múnla Teanga Bunaithe ar Chlaochladán atá Réamhthraenáilte go hÉifeachtach don Pholainnis', 'ka': 'Name', 'hu': 'HerBERT: Hatékonyan előkészített transzformátor alapú nyelvi modell lengyel számára', 'it': 'HerBERT: Modello linguistico efficace basato su trasformatori per il polacco', 'el': 'Αποτελεσματικά προσχεδιασμένο μοντέλο γλώσσας βασισμένο στον μετασχηματιστή για τα πολωνικά', 'lt': 'HerBERT: veiksmingai iš anksto parengtas kalbos modelis lenkų kalba', 'kk': 'HerBERT: Польша тіл үлгісінің түрлендіргіш негізінде әсер етеді', 'mk': 'HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish', 'ms': 'HerBERT: Model Bahasa Berasas-Transformer Berlatih Efisif untuk Polandia', 'ml': 'ഹെര്\u200dബെര്\u200dട്ട്: പോളിഷിനുള്ള പരിശീലിപ്പിക്കപ്പെട്ട ട ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d ഭാഷ മോഡല്\u200d', 'mt': 'HerBERT: Mudell tal-Lingwa bbażat fuq it-Trasformer imħarreġ b’mod Effiċjenti minn qabel għall-Pollakk', 'no': 'HerBERT: Effektivt transformert språk- modell for polsk', 'mn': 'HerBERT: Польшийн хэл загварын төлөвлөгчийн төлөвлөгч', 'pl': 'HerBERT: Wydajnie wstępnie przeszkolony model językowy oparty na transformatorze dla języka polskiego', 'ro': 'HerBERT: Model lingvistic eficient pretrainat bazat pe transformator pentru poloneză', 'sr': 'HerBERT: Učinjeno pretvaranje jezičkog modela na transformaciji za poljski', 'si': 'HerBERT: පෝලිෂ් වෙනුවෙන් ප්\u200dරතික්\u200dරීයාත්මක වෙනස් භාෂා මොඩල්', 'so': 'HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish', 'sv': 'HerBERT: Effektivt förutbestämd transformatorbaserad språkmodell för polska', 'ta': 'ஹெர்பெர்ட்: வெற்றிகரமாக பயிற்சி மாற்றியமைக்கப்பட்ட மொழி மாதிரி', 'ur': 'HerBERT: پالیش کے لئے عمدہ طور پر تغییر دینے والی زبان موڈل', 'vi': 'HerBERT: Hiệu quả giả sử ngôn ngữ biến hình cho người Ba Lan.', 'uz': 'Comment', 'hr': 'HerBERT: Učinjeno pretvaranje jezičkog modela na transformatoru za poljski', 'bg': 'Ефективно подготвен трансформаторен езиков модел за полски език', 'de': 'HerBERT: Effizient vortrainiertes Transformer-basiertes Sprachmodell für Polnisch', 'id': 'HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish', 'fa': 'HerBERT: مدل زبان بوسیله تغییر دهنده برای لهستان به طور فعال تغییر داده شده', 'nl': 'HerBERT: Efficiënt voorgetraind Transformer-gebaseerd taalmodel voor Pools', 'da': 'HerBERT: Effektivt prætrænet transformatorbaseret sprogmodel til polsk', 'tr': 'HerBERT: Polonyça için güçlü şekilde örneklendirilmiş Dil Modeli', 'sq': 'HerBERT: Model gjuhësh me bazë në Transformer të parastërvitur efikasisht për polak', 'sw': 'HerBERT: Utamaduni wa lugha inayojifunza kwa ufanisi', 'hy': 'ՀերԲԵրթ. Ավելի արդյունավետ նախապատրաստված լեզվի մոդել պոլանդական լեզվի համար', 'am': 'አርቢስቴል: በተግባር የተtrained Transformer-based ቋንቋ Model for Polish', 'az': 'HerBERT: Polonyalı üçün ehtiyacı olaraq Transformer-tabanlı Dil Modeli', 'bs': 'HerBERT: Učinjeno pretvaranje jezičkog modela na transformatoru za poljski', 'bn': 'হার্বেরেট: পোলিশের জন্য প্রশিক্ষিত ভিত্তিক ভিত্তিক ভাষার মডেল', 'cs': 'HerBERT: Efektivně předtrénovaný jazykový model založený na transformátoru pro polštinu', 'et': 'HerBERT: tõhusalt eeltreenitud transformaatoril põhinev keelemudel poola keele jaoks', 'ca': 'HerBERT: Model de llenguatge efectivament pré-entrenat en polc', 'fi': 'HerBERT: Tehokkaasti koulutettu muuntajapohjainen kielimalli puolalle', 'ko': '허버트: 고효율 예비 훈련의 변환기 기반 폴란드어 언어 모델', 'af': 'HerBERT: Effektief Gevorderde Transformeerder- gebaseerde Taal Model vir PoolsName', 'jv': 'HerBERT: Efeffectly Click Transformer-supported Language model for polar', 'ha': '@ item Spelling dictionary', 'he': 'הרברט: דוגמא לשפה המבוססת על טרנספורטרים מתאמנת בצורה יעילה לפולנית', 'sk': 'HerBERT: Učinkovito predtreniran jezikovni model za poljščino na podlagi transformatorjev', 'bo': 'HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish'}
{'en': 'BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language. A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERT-based language model   HerBERT   is trained. This model achieves state-of-the-art results on multiple downstream tasks.', 'ar': 'تُستخدم النماذج المستندة إلى BERT حاليًا لحل جميع مهام معالجة اللغة الطبيعية (NLP) تقريبًا وغالبًا ما تحقق نتائج متطورة. لذلك ، يجري مجتمع البرمجة اللغوية العصبية (NLP) بحثًا مكثفًا حول فهم هذه النماذج ، ولكن قبل كل شيء في تصميم إجراءات تدريب فعالة وكفؤة. تم إجراء العديد من دراسات الاجتثاث التي تبحث في كيفية تدريب النماذج المشابهة لـ BERT ، لكن الغالبية العظمى منها تتعلق فقط باللغة الإنجليزية. لا يجب أن يكون إجراء التدريب المصمم للغة الإنجليزية عالميًا وقابل للتطبيق على لغات أخرى مختلفة بشكل خاص من حيث التصنيف. لذلك ، تقدم هذه الورقة أول دراسة استئصال تركز على اللغة البولندية ، والتي ، على عكس اللغة الإنجليزية المعزولة ، هي لغة اندماجية. نحن نصمم ونقيم بدقة إجراء ما قبل التدريب لنقل المعرفة من النماذج متعددة اللغات إلى النماذج المستندة إلى BERT أحادية اللغة. بالإضافة إلى تهيئة النموذج متعدد اللغات ، يتم أيضًا استكشاف العوامل الأخرى التي من المحتمل أن تؤثر على التدريب المسبق ، مثل هدف التدريب ، وحجم المجموعة ، و BPE-Dropout ، وطول فترة ما قبل التدريب. بناءً على الإجراء المقترح ، تم تدريب نموذج اللغة البولندي المستند إلى BERT - HerBERT. يحقق هذا النموذج أحدث النتائج في العديد من المهام النهائية.', 'es': 'Los modelos basados en BERT se utilizan actualmente para resolver casi todas las tareas de procesamiento del lenguaje natural (NLP) y, en la mayoría de los casos, logran resultados de vanguardia. Por lo tanto, la comunidad de PNL lleva a cabo una investigación exhaustiva sobre la comprensión de estos modelos, pero sobre todo sobre el diseño de procedimientos de capacitación efectivos y eficientes. Se han llevado a cabo varios estudios de ablación que investigan cómo entrenar modelos similares a los de Bert, pero la gran mayoría de ellos se refieren únicamente al idioma inglés. Un procedimiento de formación diseñado para el inglés no tiene que ser universal y aplicable a otros idiomas especialmente diferentes tipológicamente. Por lo tanto, este artículo presenta el primer estudio de ablación centrado en el polaco, que, a diferencia del inglés que lo aísla, es un idioma fusional. Diseñamos y evaluamos minuciosamente un procedimiento de preentrenamiento para transferir conocimientos de modelos basados en BERT multilingües a monolingües. Además de la inicialización del modelo multilingüe, también se exploran otros factores que pueden influir en el preentrenamiento, como el objetivo del entrenamiento, el tamaño del corpus, el abandono de BPE y la duración del preentrenamiento. Basado en el procedimiento propuesto, se capacita a un modelo lingüístico basado en el polaco Bert, HerBert. Este modelo logra resultados de vanguardia en múltiples tareas posteriores.', 'fr': "Les modèles basés sur BERT sont actuellement utilisés pour résoudre presque toutes les tâches de traitement du langage naturel (NLP) et permettent le plus souvent d'obtenir des résultats de pointe. Par conséquent, la communauté de la PNL mène des recherches approfondies sur la compréhension de ces modèles, mais surtout sur la conception de procédures de formation efficaces et efficientes. Plusieurs études d'ablation visant à déterminer comment entraîner des modèles de type Bert ont été réalisées, mais la grande majorité d'entre elles ne concernaient que la langue anglaise. Une procédure de formation conçue pour l'anglais n'a pas à être universelle et applicable à d'autres langues particulièrement différentes du point de vue typologique. Par conséquent, cet article présente la première étude d'ablation axée sur le polonais, qui, contrairement à la langue anglaise isolante, est une langue fusionnelle. Nous concevons et évaluons en profondeur une procédure de pré-formation pour le transfert de connaissances depuis des modèles multilingues vers des modèles monolingues basés sur BERT. En plus de l'initialisation du modèle multilingue, d'autres facteurs susceptibles d'influencer la préformation sont également explorés, à savoir l'objectif de la formation, la taille du corps, l'abandon du BPE et la durée de la pré-formation. Sur la base de la procédure proposée, un modèle linguistique polonais basé sur BERT — HerBERT — est formé. Ce modèle permet d'obtenir des résultats de pointe sur de multiples tâches en aval.", 'pt': 'Modelos baseados em BERT são usados atualmente para resolver quase todas as tarefas de Processamento de Linguagem Natural (NLP) e na maioria das vezes alcançam resultados de última geração. Portanto, a comunidade de PNL realiza uma extensa pesquisa para entender esses modelos, mas sobretudo para projetar procedimentos de treinamento eficazes e eficientes. Vários estudos de ablação investigando como treinar modelos do tipo BERT foram realizados, mas a grande maioria deles dizia respeito apenas à língua inglesa. Um procedimento de treinamento projetado para o inglês não precisa ser universal e aplicável a outros idiomas especialmente tipologicamente diferentes. Portanto, este artigo apresenta o primeiro estudo de ablação focado no polonês, que, diferentemente da isolante língua inglesa, é uma língua fusional. Projetamos e avaliamos minuciosamente um procedimento de pré-treinamento de transferência de conhecimento de modelos baseados em BERT multilíngue para monolíngue. Além da inicialização do modelo multilíngue, outros fatores que possivelmente influenciam o pré-treinamento também são explorados, ou seja, objetivo do treinamento, tamanho do corpus, BPE-Dropout e duração do pré-treinamento. Com base no procedimento proposto, um modelo de linguagem baseado em BERT polonês – HerBERT – é treinado. Este modelo alcança resultados de última geração em várias tarefas downstream.', 'ja': 'BERTベースのモデルは、現在、ほぼすべての自然言語処理（ NLP ）タスクを解決するために使用されており、ほとんどの場合、最先端の結果を達成します。 したがって、NLPコミュニティは、これらのモデルを理解するための広範な研究を行っているが、何よりも効果的で効率的なトレーニング手順を設計するための研究を行っている。 BERT様モデルの訓練方法を調べるいくつかのアブレーション研究が行われているが、その大部分は英語のみに関わるものであった。 英語のために設計されたトレーニング手順は、普遍的である必要はなく、特に類型的に異なる他の言語に適用される必要はありません。 そこで本稿では、孤立する英語とは異なり融合言語であるポーランド語を中心とした最初のアブレーション研究を紹介する。 私たちは、多言語からBERTベースの単一言語モデルへの知識の転送の事前トレーニング手順を設計し、徹底的に評価します。 多言語モデルの初期化に加えて、トレーニングの目的、コーパスサイズ、BPEドロップアウト、およびトレーニング前の長さなど、トレーニング前に影響を与える可能性のある他の要因も検討されています。 提案された手順に基づいて、ポーランドのBERTベースの言語モデル– HerBERT -がトレーニングされます。 このモデルは、複数のダウンストリームタスクで最先端の結果を達成します。', 'zh': '盖BERT之所以决者,几于自然语言(NLP),而最常得先进者也。 是以NLP社区博究其体,而莫若设高效之培训序。 既行几项消融研究,讲习类BERT,而绝大多数及英语。 为英语计者培训不必通用,而适于异类之言。 故本文首注于波兰语之消融研究,与孤立之英语异,波兰语为融合之语。 设意穷评BERT移知于单语BERT预培训程。 多言模初始化之外,讨论其他,曰教的,曰语料库大小,曰BPE-Dropout 预训练。 基于程序,练于波兰BERT言模HerBERT。 当于下流最先进。', 'ru': 'Основанные на BERT модели в настоящее время используются для решения почти всех задач обработки естественного языка (NLP) и чаще всего достигают самых современных результатов. Поэтому сообщество NLP проводит обширные исследования по пониманию этих моделей, но прежде всего по разработке эффективных и действенных процедур обучения. Было проведено несколько абляционных исследований по изучению того, как обучать BERT-подобные модели, но подавляющее большинство из них касалось только английского языка. Процедура обучения, разработанная для английского языка, не обязательно должна быть универсальной и применимой к другим особенно типологически различным языкам. Поэтому в данной работе представлено первое исследование абляции, посвященное польскому языку, который, в отличие от изолирующего английского языка, является фьюжн-языком. Мы разрабатываем и тщательно оцениваем процедуру предварительного обучения для передачи знаний от многоязычных к одноязычным моделям на основе BERT. В дополнение к многоязычной инициализации модели также изучаются другие факторы, которые, возможно, влияют на предварительное обучение, например, цель обучения, размер корпуса, BPE-Dropout и продолжительность предварительного обучения. На основе предлагаемой процедуры проводится обучение польской языковой модели на основе BERT – HerBERT. Эта модель достигает самых современных результатов при выполнении нескольких последующих задач.', 'hi': 'BERT-आधारित मॉडल वर्तमान में लगभग सभी प्राकृतिक भाषा प्रसंस्करण (एनएलपी) कार्यों को हल करने के लिए उपयोग किए जाते हैं और अक्सर अत्याधुनिक परिणाम प्राप्त करते हैं। इसलिए, एनएलपी समुदाय इन मॉडलों को समझने पर व्यापक शोध करता है, लेकिन प्रभावी और कुशल प्रशिक्षण प्रक्रियाओं को डिजाइन करने पर सबसे ऊपर। BERT-जैसे मॉडल को प्रशिक्षित करने के तरीके की जांच करने वाले कई एब्लेशन अध्ययन किए गए हैं, लेकिन उनमें से अधिकांश केवल अंग्रेजी भाषा से संबंधित हैं। अंग्रेजी के लिए डिज़ाइन की गई एक प्रशिक्षण प्रक्रिया सार्वभौमिक नहीं होनी चाहिए और अन्य विशेष रूप से टाइपोलॉजिकल रूप से विभिन्न भाषाओं पर लागू होती है। इसलिए, यह पेपर पोलिश पर केंद्रित पहला एब्लेशन अध्ययन प्रस्तुत करता है, जो अलग-थलग अंग्रेजी भाषा के विपरीत, एक संलयन भाषा है। हम बहुभाषी से मोनोलिंगुअल BERT-आधारित मॉडल में ज्ञान को स्थानांतरित करने की एक पूर्व-प्रशिक्षण प्रक्रिया को डिजाइन और पूरी तरह से मूल्यांकन करते हैं। बहुभाषी मॉडल आरंभीकरण के अलावा, अन्य कारक जो संभवतः प्रीट्रेनिंग को प्रभावित करते हैं, उनका भी पता लगाया जाता है, यानी प्रशिक्षण उद्देश्य, कॉर्पस आकार, बीपीई-ड्रॉपआउट, और प्रीट्रेनिंग लंबाई। प्रस्तावित प्रक्रिया के आधार पर, एक पोलिश BERT-आधारित भाषा मॉडल - HerBERT - को प्रशिक्षित किया जाता है। यह मॉडल कई डाउनस्ट्रीम कार्यों पर अत्याधुनिक परिणाम प्राप्त करता है।', 'ga': 'Úsáidtear samhlacha bunaithe ar BERT faoi láthair chun beagnach gach tasc Próiseála Teanga Nádúrtha (NLP) a réiteach agus is minic a ghnóthaítear torthaí den scoth. Mar sin, déanann an pobal NLP taighde fairsing ar thuiscint na múnlaí seo, ach thar aon rud eile ar dhearadh nósanna imeachta oiliúna éifeachtacha agus éifeachtach. Rinneadh go leor staidéar eisiblithe ag fiosrú conas samhlacha ar nós BERT a thraenáil, ach bhain a bhformhór mór díobh leis an mBéarla amháin. Ní gá go mbeadh nós imeachta oiliúna atá deartha don Bhéarla uilíoch agus infheidhme i dteangacha eile atá difriúil go háirithe ó thaobh na clódóireachta de. Dá bhrí sin, cuireann an páipéar seo i láthair an chéad staidéar ablation atá dírithe ar an bPolainnis, atá, murab ionann agus an Béarla iargúlta, ina teanga chomhleá. Dearaimid agus déanaimid measúnú críochnúil ar nós imeachta réamhoiliúint chun eolas a aistriú ó mhúnlaí ilteangacha go samhlacha aonteangacha atá bunaithe ar CRET. Chomh maith le samhlacha ilteangacha a thosú, déantar iniúchadh freisin ar fhachtóirí eile a d’fhéadfadh tionchar a bheith acu ar réamhoiliúint, i.e. cuspóir oiliúna, méid an chorpais, BPE-Dropout, agus fad na réamhoiliúint. Bunaithe ar an nós imeachta atá beartaithe, cuirtear oiliúint ar mhúnla teanga Polainnis bunaithe ar BERT – HerBERT. Baineann an tsamhail seo torthaí den scoth amach ar thascanna iolracha iartheachtacha.', 'ka': 'BERT-დაბათი მოდელები ახლა გამოიყენება პირდაპირადი ენერგიის პროცესი (NLP) პარამეტრებისთვის და უფრო მეტი წარმოიდგინოთ წარმოიდგინების შესახებ. ამიტომ, NLP-ის საზოგადოება უფრო დიდი შესწავლობა ამ მოდელების შესახებ, მაგრამ უფრო მეტი ეფექტიური და ეფექტიური შესწავლობის პროცესტების რამდენიმე აბლაციის კვლევები, როგორ BERT-ის მოდელების გასწავლება, მაგრამ უფრო დიდი მათგანი მხოლოდ ანგლისური ენაზე დარწმუნდა. ანგლისთვის განაზღვრებული განაზღვრების პროცესი არ უნდა იყოს სხვა განსაკუთრებულად ტიპოლოგიურად განსხვავებული ენებისთვის უნდა იყოს. ამიტომ, ეს წიგნი აჩვენებს პირველი შესაძლებლობა სწავლა პოლიქური წიგნის განმავლობაში, რომელიც, განმავლობაში, ინგლისური языკის განმავლობაში, არის ფუნციონიურ ჩვენ განვიყენებთ და უფრო უფრო გავაკეთებთ მრავალენგური მოდელებისგან მონოლენგური BERT-ის მუშაობის მოდელებისთვის პროცემი. მრავალენგური მოდელის თნციალიზაციაზე დამატებით, სხვა ფაქტორები, რომელიც შეიძლება შეიძლება შეიძლება გააკეთება, მაგალითად განაკეთებული მიზეზი, კორპუსს ზომა, BPE-Dropout და განაკეთ პროცექტის მიზეზით, პოლიქური ბერტიკური ენის მოდელი - HerBERT - სტრუნქცია. ეს მოდელის შესაძლებელია მრავალ კონტრომენტის შესახებ.', 'hu': 'A BERT-alapú modelleket jelenleg szinte minden Natural Language Processing (NLP) feladat megoldására használják, és leggyakrabban korszerű eredményeket érnek el. Ezért az NLP közösség kiterjedt kutatást folytat e modellek megértésére, de mindenekelőtt hatékony és hatékony képzési eljárások kidolgozására. Számos ablációs tanulmányt végeztek a BERT-szerű modellek képzésének vizsgálatára, de ezek túlnyomó többsége csak az angol nyelvet érintette. Az angol nyelvre tervezett képzési eljárásnak nem kell univerzálisnak lennie, és más különösen tipológiailag eltérő nyelvekre is alkalmazhatónak kell lennie. Ezért ez a tanulmány bemutatja az első ablációs tanulmányt, amely a lengyel nyelvre összpontosít, amely az elszigetelő angol nyelvtől eltérően fúziós nyelv. Tervezünk és alaposan értékeljük a tudás többnyelvű BERT-alapú modellekről történő átadásának előkészítési eljárását. A többnyelvű modell inicializálása mellett egyéb tényezőket is feltárnak, amelyek esetleg befolyásolják a képzést, például az edzési célt, a corpus méretét, a BPE-Dropout-ot és a képzési hosszát. A javasolt eljárás alapján egy lengyel BERT-alapú nyelvi modell - HerBERT - képzésre kerül. Ez a modell több downstream feladatnál is korszerű eredményeket ér el.', 'el': 'Τα μοντέλα που βασίζονται σήμερα χρησιμοποιούνται για την επίλυση σχεδόν όλων των εργασιών επεξεργασίας φυσικής γλώσσας και συνήθως επιτυγχάνουν αποτελέσματα τελευταίας τεχνολογίας. Ως εκ τούτου, η κοινότητα διεξάγει εκτεταμένη έρευνα για την κατανόηση αυτών των μοντέλων, αλλά κυρίως για τον σχεδιασμό αποτελεσματικών και αποτελεσματικών διαδικασιών κατάρτισης. Έχουν πραγματοποιηθεί αρκετές μελέτες αφαίρεσης που διερευνούν τον τρόπο εκπαίδευσης μοντέλων που μοιάζουν με BERT, αλλά η συντριπτική πλειοψηφία από αυτές αφορούσε μόνο την αγγλική γλώσσα. Μια εκπαιδευτική διαδικασία σχεδιασμένη για τα αγγλικά δεν χρειάζεται να είναι καθολική και εφαρμόσιμη σε άλλες ειδικά τυπολογικά διαφορετικές γλώσσες. Ως εκ τούτου, η παρούσα εργασία παρουσιάζει την πρώτη μελέτη αφαίρεσης που επικεντρώνεται στην πολωνική, η οποία, σε αντίθεση με την απομονωμένη αγγλική γλώσσα, είναι μια συγχωνευτική γλώσσα. Σχεδιάζουμε και αξιολογούμε διεξοδικά μια διαδικασία προετοιμασίας για τη μεταφορά γνώσεων από πολυγλωσσικά σε μονογλωσσικά μοντέλα. Εκτός από την πολύγλωσση αρχικοποίηση μοντέλων, διερευνώνται και άλλοι παράγοντες που ενδεχομένως επηρεάζουν την προεπεξεργασία, δηλαδή ο στόχος εκπαίδευσης, το μέγεθος του σώματος, η πτώση και το μήκος προεπεξεργασίας. Με βάση την προτεινόμενη διαδικασία, εκπαιδεύεται ένα μοντέλο γλώσσας βασισμένο στην πολωνική BERT. Αυτό το μοντέλο επιτυγχάνει αποτελέσματα τελευταίας τεχνολογίας σε πολλαπλές μεταγενέστερες εργασίες.', 'it': "I modelli basati su BERT sono attualmente utilizzati per risolvere quasi tutte le attività di Natural Language Processing (NLP) e il più delle volte raggiungono risultati all'avanguardia. Pertanto, la comunità PNL conduce una vasta ricerca sulla comprensione di questi modelli, ma soprattutto sulla progettazione di procedure di formazione efficaci ed efficienti. Sono stati condotti diversi studi di ablazione per studiare come formare modelli simili a BERT, ma la stragrande maggioranza di essi riguardava solo la lingua inglese. Una procedura di formazione pensata per l'inglese non deve essere universale e applicabile ad altre lingue particolarmente tipologicamente diverse. Pertanto, questo articolo presenta il primo studio di ablazione incentrato sul polacco, che, a differenza della lingua inglese isolante, è una lingua fusionale. Progettiamo e valutiamo accuratamente una procedura di pre-formazione per trasferire le conoscenze da modelli multilingue a monolingue basati su BERT. Oltre all'inizializzazione del modello multilingue, vengono esplorati anche altri fattori che possono influenzare il pre-training, come obiettivo di allenamento, dimensione del corpo, BPE-Dropout e lunghezza di pre-training. Sulla base della procedura proposta, viene formato un modello linguistico polacco basato su BERT - HerBERT. Questo modello raggiunge risultati all'avanguardia su più attività a valle.", 'kk': 'BERT- негіздеген үлгілер барлық Түзіндік тіл процессорының (NLP) тапсырмаларын шешу үшін қолданылады және көпшілік- орындық нәтижелерін жеткізеді. Сондықтан NLP коммуникасы осы үлгілерді түсінуге көптеген зерттеулерді жұмыс істейді, бірақ бұл үлгілерді ефективті және эффективті оқыту процедурлерін құр BERT сияқты моделдерді қалай оқыту үшін бірнеше жұмыс істеу зерттеулері жұмыс істеді, бірақ олардың көпшілігі тек ағылшын тіліне қатысты. Ағылшын тіліне құрылған оқыту процедуры әбденепсіздік және басқаларға әбденепсіздік түрлі тілдерге қолдану керек емес. Сондықтан бұл қағаз бірінші қосулы зерттеулерді Польша тіліне назар аударады. Бұл ағылшын тілінің айырмашылығында бірінші қосулы зерттеулері - бірінші тіл. Біз бірнеше тілдерден білім мәліметін бірнеше тілдерге ауыстыру және бірнеше тілдерге негізделген моделдерді құрамыз. Көптілік үлгі инициализациялау үлгісінің қосымша, басқа факторлар, көпшілік көпшілігіне әсер ететін, мысалы, оқыту мақсаты, корпус өлшемі, BPE-Dropout және қалқын ұзындығы Келтірілген процедура негізінде Польша BERT тіл үлгісі - HerBERT - оқылған. Бұл үлгі бірнеше бағытталған тапсырмалардың күйінің нәтижесін жеткізеді.', 'lt': 'Šiuo metu BERT grindžiami modeliai naudojami beveik visoms gamtinių kalbų apdorojimo užduotims išspręsti ir dažniausiai pasiekti naujausius rezultatus. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures.  Buvo atlikti keli abliacijos tyrimai, kuriuose buvo tiriama, kaip treniruoti BERT panašius modelius, tačiau didžioji jų dalis buvo susijusi tik su anglų kalba. Mokymo procedūra, skirta anglų kalbai, neturi būti universali ir taikoma kitoms ypač tipologiškai skirtingoms kalboms. Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language.  Mes parengiame ir nuodugniai vertiname išankstinio mokymo procedūrą, skirtą žinių perdavimui iš daugiakalbių į vienkalbius BERT grindžiamus modelius. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  Remiantis siūloma procedūra, mokomas Lenkijos BERT pagrįstas kalbų model is – HerBERT. Šiuo modeliu pasiekti pažangiausi rezultatai keliose tolesnėse veiklos srityse.', 'mk': 'Моделите базирани на БЕРТ моментно се користат за решавање на речиси сите задачи за процес на природен јазик (НЛП) и најчесто постигнуваат најдобри резултати. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures.  Неколку студии за аблација кои истражуваат како да се обучуваат модели слични на БЕРТ се спроведени, но големиот дел од нив се однесуваа само на англискиот јазик. Процедурата за обука дизајнирана за англиски не мора да биде универзална и да се примени на други, особено типологички различни јазици. Затоа, овој весник ја претставува првата студија за аблација фокусирана на полски, кој, за разлика од изолирачкиот англиски јазик, е фузионален јазик. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models.  Покрај инцијализацијата на мултијазичкиот модел, се истражуваат и други фактори кои можеби влијаат на претренирањето, т.е. објективната обука, големината на корпусот, БПЕ-дропут и должината на претренирањето. Според предложената процедура, се обучува полски јазички модел - ХЕРБЕРТ - базиран на БЕРТ. This model achieves state-of-the-art results on multiple downstream tasks.', 'ms': 'Model berasaskan BERT kini digunakan untuk menyelesaikan hampir semua tugas Proses Bahasa Alami (NLP) dan paling sering mencapai keputusan-state-of-the-art. Oleh itu, komuniti NLP melaksanakan kajian luas mengenai memahami model ini, tetapi terutama mengenai merancang prosedur latihan yang efektif dan efisien. Beberapa kajian ablasi menyelidiki bagaimana untuk melatih model seperti BERT telah dilakukan, tetapi kebanyakan mereka hanya berkaitan bahasa Inggeris. A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages.  Oleh itu, kertas ini memperkenalkan kajian ablasi pertama yang fokus pada bahasa Polandia, yang, tidak seperti bahasa Inggeris yang mengisolasi, adalah bahasa fusional. Kami merancang dan meneliti secara teliti prosedur pembelajaran untuk memindahkan pengetahuan dari model berbilang bahasa ke model berdasarkan BERT. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  Based on the proposed procedure, a Polish BERT-based language model - HerBERT - is trained.  Model ini mencapai keputusan state-of-the-art pada beberapa tugas turun.', 'ml': 'BERT-അടിസ്ഥാനമായ മോഡലുകള്\u200d ഇപ്പോള്\u200d ഉപയോഗിക്കുന്നു എല്ലാ സ്വാഭാവിക ഭാഷ പ്രവര്\u200dത്തനങ്ങളും (NLP) പ്രവര്\u200dത്തനങ്ങള്\u200dക്കും പരിഹരിക്കുവാനും,  അതുകൊണ്ട്, NLP സമൂഹം ഈ മോഡലുകളെ മനസ്സിലാക്കുന്നതിനെക്കുറിച്ച് വിശാലമായ ഗവേഷണം നടത്തുന്നു, പക്ഷെ എല്ലാവരെക്കാളും സ ബെര്\u200dട്ടി പോലുള്ള മോഡലുകള്\u200d എങ്ങനെ പരിശീലിപ്പിക്കുന്നതെന്ന് അന്വേഷിക്കുന്ന കുറച്ച് ആഗ്രഹിക്കുന്നു ഇംഗ്ലീഷിന് വേണ്ടി നിര്\u200dമ്മിക്കപ്പെട്ട ഒരു പരിശീലന പ്രക്രിയയില്\u200d പ്രത്യേകിച്ച് വ്യത്യസ്ത ഭാഷകള്\u200dക്ക്  അതുകൊണ്ട്, ഈ പത്രത്തില്\u200d ആദ്യത്തെ പോളിഷിനെ ശ്രദ്ധിക്കുന്ന ആദ്യത്തെ തെളിയിക്കുന്ന പഠനം കാണിക്കുന്നു. അത് ഇംഗ് നമ്മള്\u200d ഡിസൈന്\u200d ചെയ്യുകയും പൂര്\u200dണ്ണമായും പരിഗണിക്കുകയും ചെയ്യുന്നു. പല ഭാഷകളില്\u200d നിന്നും ബെര്\u200dട്ടി അടിസ്ഥാനമായ മോഡലുകളിലേക്ക പല ഭാഷകങ്ങളുടെ മോഡല്\u200d തുടങ്ങുന്നതിനു ശേഷം, മഴ പ്രഭാവിക്കുന്ന മറ്റു കാരണങ്ങളും പരിശോധിക്കുന്നു, പരിശീലനത്തിന്റെ ലക്ഷ്യം, കോര്\u200dപ്പുസ് വലിപ്പം, ബ പ്രൊദ്ദേശിക്കപ്പെട്ട പ്രക്രിയയെ അടിസ്ഥാനമാക്കി ഒരു പോളിഷ് ബെര്\u200dട്ടി ഭാഷ മോഡല്\u200d - ഹെര്\u200dബെര്\u200dട്ടി - പരിശീലന ഈ മോഡല്\u200d ഒരുപാട് പ്രവര്\u200dത്തനങ്ങളില്\u200d നില്\u200dക്കുന്ന സ്ഥാനത്തിന്റെ ഫലങ്ങള്\u200d പ്രാപിക്കുന്നു.', 'mt': 'Bħalissa jintużaw mudelli bbażati fuq il-BERT biex jissolvew kważi l-kompiti kollha tal-ipproċessar tal-lingwi naturali (NLP) u l-aktar spiss jiksbu riżultati l-aktar avvanzati. Għalhekk, il-komunità NLP twettaq riċerka estensiva dwar il-fehim ta’ dawn il-mudelli, iżda fuq kollox dwar it-tfassil ta’ proċeduri ta’ taħriġ effettivi u effiċjenti. Twettqu diversi studji dwar l-abblazzjoni li jinvestigaw kif jitħarrġu mudelli simili għall-BERT, iżda l-maġġoranza l-kbira tagħhom ikkonċernaw biss il-lingwa Ingliża. Proċedura ta’ taħriġ imfassla għall-Ingliż m’għandhiex għalfejn tkun universali u applikabbli għal lingwi oħra speċjalment tipikament differenti. Għalhekk, dan id-dokument jippreżenta l-ewwel studju ta’ ablazzjoni ffukat fuq il-Pollakk, li, għall-kuntrarju tal-lingwa Ingliża iżolata, huwa lingwa fużjonali. Aħna niddisinjaw u jevalwaw bir-reqqa proċedura ta’ taħriġ minn qabel tat-trasferiment tal-għarfien minn mudelli multilingwi għal mudelli monolingwi bbażati fuq BERT. Minbarra l-inizjalizzazzjoni tal-mudell multilingwi, huma esplorati wkoll fatturi oħra li possibilment jinfluwenzaw it-taħriġ minn qabel, jiġifieri l-objettiv tat-taħriġ, id-daqs tal-korpus, il-BPE-Dropout, u t-tul tat-taħriġ minn qabel. Abbażi tal-proċedura proposta, huwa mħarreġ mudell lingwistiku Pollakk ibbażat fuq il-BERT - HerBERT. Dan il-mudell jikseb riżultati l-aktar avvanzati fuq diversi kompiti downstream.', 'no': 'BERT-baserte modeller vert brukte for å løysa nesten alle naturspråk- prosesseringar (NLP) og mest ofte oppnår resultatet av kunsten. NLP-samfunnet gjer derfor utvida forskning om å forstå desse modelane, men over alt om å designa effektive og effektive treningsprosedyrar. Fleire ablasjonssøker som undersøker korleis BERT-liknande modeller skal trenjast, men dei store fleste av dei er berre henta til engelske språk. Ein opplæringsprosedyr som er utvikla for engelsk må ikkje vera universell og tilgjengeleg til andre spesielt typologisk ulike språk. Denne papiret viser derfor den første aktiveringsstudien fokusert på polsk, som, i motsetjing av den isolerende engelske språket, er ein fusjonspråk. Vi designerer og utviklar ein prøveprosedyr for å overføra kunnskap frå fleire språk til monospråk BERT-baserte modeller. I tillegg til å starta fleirspråk modeller vert også utforska andre faktorer som kanskje påvirkar treinging, t.d. treingingsmål, korpusstorleik, BPE-Dropout og treingingslengd. Basert på den foreslåtte prosedyren er ein polsk språk- modell - HerBERT- basert på BERT- utlært. Denne modellen gjer tilstanden til kunsten til fleire nedstrekkoppgåver.', 'pl': 'Modele oparte na BERT są obecnie wykorzystywane do rozwiązywania prawie wszystkich zadań przetwarzania języka naturalnego (NLP) i najczęściej osiągają najnowocześniejsze wyniki. Dlatego też społeczność NLP prowadzi szeroko zakrojone badania nad zrozumieniem tych modeli, ale przede wszystkim nad projektowaniem skutecznych i efektywnych procedur szkoleniowych. Przeprowadzono kilka badań ablacyjnych badających, jak trenować modele podobne do BERT, ale zdecydowana większość z nich dotyczyła tylko języka angielskiego. Procedura szkoleniowa przeznaczona dla angielskiego nie musi być uniwersalna i stosowana do innych, szczególnie typologicznie różnych języków. W niniejszym artykule przedstawiono zatem pierwsze badanie ablacji skupione na języku polskim, który, w przeciwieństwie do izolującego języka angielskiego, jest językiem fuzyjnym. Projektujemy i dokładnie oceniamy procedurę wstępnego szkolenia przekazywania wiedzy z wielojęzycznych do jednojęzycznych modeli opartych na BERT. Oprócz inicjalizacji modelu wielojęzycznego badane są również inne czynniki, które mogą wpływać na trening wstępny, tj. cel treningu, wielkość ciała, BPE-Dropout i długość treningu wstępnego. W oparciu o proponowaną procedurę przeszkolony jest polski model językowy oparty na BERT.HerBERT. Model ten uzyskuje najnowocześniejsze wyniki w wielu dalszych zadaniach.', 'ro': 'Modelele bazate pe BERT sunt utilizate în prezent pentru rezolvarea aproape a tuturor sarcinilor de procesare a limbajului natural (PNL) și, cel mai adesea, obțin rezultate de ultimă generație. Prin urmare, comunitatea PNL efectuează cercetări ample privind înțelegerea acestor modele, dar mai presus de toate privind conceperea unor proceduri eficiente și eficiente de formare. Au fost efectuate mai multe studii de ablație care au investigat modul de instruire a modelelor asemănătoare BERT, dar marea majoritate a acestora au vizat doar limba engleză. O procedură de formare concepută pentru limba engleză nu trebuie să fie universală și aplicabilă altor limbi deosebit de diferite din punct de vedere tipologic. Prin urmare, această lucrare prezintă primul studiu de ablație axat pe poloneză, care, spre deosebire de limba engleză izolatoare, este o limbă fuzională. Proiectăm și evaluăm în detaliu o procedură de pregătire a transferului cunoștințelor de la modele multilingve la modele monolingve bazate pe BERT. Pe lângă inițializarea modelului multilingv, sunt explorați și alți factori care pot influența pregătirea, cum ar fi obiectivul de formare, dimensiunea corpului, BPE-Dropout și lungimea pregătirii. Pe baza procedurii propuse, este instruit un model lingvistic polonez bazat pe BERT - HerBERT. Acest model obține rezultate de ultimă generație pentru mai multe sarcini în aval.', 'sr': 'BERT-bazirani modeli trenutno se koriste za rješavanje skoro svih zadataka prirodnog procesa jezika (NLP) i najčešće postižu rezultate umjetnosti. Stoga zajednica NLP vodi široko istraživanje o razumijevanju tih modela, ali iznad svega o dizajniranju efikasnih i efikasnih postupaka obuke. Nekoliko ispitivanja ablacije koje istražuju kako trenirati modele kao BERT provedene su, ali većina njih je zabrinuta samo engleski jezik. Procedura obuke dizajnirana za engleski ne mora biti univerzalna i primjenjiva na druge posebno tipološki različite jezike. Stoga, ovaj papir predstavlja prvu studiju aktivacije usredotočenu na poljski, koji je, za razliku od izolacijskog engleskog jezika, kombinacijski jezik. Mi dizajniramo i temeljno procjenjujemo pretvaranje procedure prebacivanja znanja iz multijezičkih na monojezičke modele na BERT-u. Pored inicijalizacije multijezičkih modela, istražuju se i drugi faktori koji mogu uticati na pretkivanje, to je cilj obuke, veličine korpusa, BPE-Dropout i dužina pretkivanja. Na temelju predložene procedure, obučen je poljski model jezika na BERT-u HerBERT-u. Ovaj model postiže rezultate umjetnosti na višestrukim poslovima.', 'si': 'BERT- අධාරිත මොඩේල් දැනටමත් ප්\u200dරයෝජනය සම්පූර්ණ භාෂාව ප්\u200dරක්\u200dරියාපනය (NLP) වැඩ කරන්න ප්\u200dරයෝජනය කරන්න භාවිත වි ඉතින්, NLP සමාජයෙන් මේ මොඩල් තේරුම් ගන්න විශාල පරීක්ෂණය කරනවා, ඒත් ඔක්කොම වඩා ප්\u200dරයෝජනය සහ ප්\u200dරයෝජනය බෙර්ට් වගේ මොඩේල් එක්ක කොහොමද කියලා පරීක්ෂණය කරලා තියෙන්නේ, ඒත් ඔවුන්ගේ ගොඩක් අධාර්යයෙන් ඉංග්\u200dරීසි භ ඉංග්\u200dරීසිය සඳහා සැලසුම් කරපු ප්\u200dරධානයක් විශේෂයෙන් වෙනස් භාෂාවට විශේෂයෙන් වෙනස් භා ඉතින්, මේ පැත්තේ පොලිෂ් වලින් ප්\u200dරධාන අධ්\u200dයානයක් පෙන්වනවා, ඒක, ඉංග්\u200dරීසි භාෂාවට වෙනස් වෙනුවෙන් ඉන අපි සිද්ධානය කරනවා සහ සම්පූර්ණයෙන් අවශ්\u200dය කරනවා වගේම භාෂාවයෙන් ගොඩක් භාෂාවක් BERT-අධාරිත මොඩේල්  ගොඩක් භාෂාවික මොඩල් පටන් ගන්න, අනිත් විශේෂයෙන් ප්\u200dරීට්\u200dරීන් ගන්න පුළුවන් විදිහට පරීක්ෂණය කරනවා, ඉතින් ප්\u200dරීට්\u200dරීන් අක්ෂ පොලිෂ් බෙර්ට් අධාරිත භාෂාවක් නිර්මාණයක් හෙර්බෙර්ට් වලින් ප්\u200dරධානය කරලා තියෙනවා. මේ මොඩල් එක්ක තත්වයේ කාර්යාත්මක ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරති', 'so': 'Tusaale-asalka BERT waxaa loo isticmaalaa in lagu xalliyo shaqooyinka arimaha afka asalka ah oo dhan (NLP) iyo inta badan waxay gaadhaan arimaha farshaxanka. Sidaas darteed bulshada NLP waxay sameeyaan baaritaan aad u badan oo ku saabsan waxgarashada modelladan, laakiin wax walba ka sarreeya ku saabsan sawirada waxbarashada oo faa’iido leh iyo shaqeynta. Waxbarashada dalbashada iskuulaadka waxaa lagu baaraandegay sida loo baro Tusaale u eg BERT, laakiin inta badan oo ka mid ah waxay ka mid yihiin afka Ingiriiska oo keliya. Ruqsada waxbarashada ingiriisiga loo qoray uma baahna in uu noqdo caalami ah oo uu ku habboon luuqado kale oo si gaar ah u kala duwan. Sidaa darteed warqaddaas waxay soo bandhigtaa barashada koowaad ee uu ku kalsoonaaday Boolis, taas oo aan kala duwan afka ingiriisiga ee kala duwan, waa luqad fudud. Waxaynu qornaynaa oo si buuxda ah u qiimeynaynaa tartanka hore oo aqoonta looga wareejiyo luuqadaha kala duduwan ilaa modellada bilowga BERT. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  Sida lagu saleynayo tartanka la soo jeeday, waxaa lagu tababariyaa model afka BERT ee Boolish BERT- HerBERT. This model achieves state-of-the-art results on multiple downstream tasks.', 'sv': 'BERT-baserade modeller används för närvarande för att lösa nästan alla Natural Language Processing (NLP) uppgifter och uppnår oftast toppmoderna resultat. Därför bedriver NLP-gemenskapen omfattande forskning om att förstå dessa modeller, men framför allt om att utforma effektiva och effektiva utbildningsförfaranden. Flera ablationsstudier som undersökte hur man tränar BERT-liknande modeller har genomförts, men de allra flesta av dem gällde endast engelska språket. Ett utbildningsförfarande som utformats för engelska behöver inte vara universellt och tillämpligt på andra särskilt typologiskt olika språk. Därför presenterar denna uppsats den första ablationsstudien inriktad på polska, som till skillnad från det isolerande engelska språket är ett fusionsspråk. Vi utformar och utvärderar grundligt ett förberedande förfarande för överföring av kunskap från flerspråkiga till enspråkiga BERT-baserade modeller. Förutom flerspråkig modellinitiering utforskas även andra faktorer som eventuellt påverkar förberedelseprocessen, t.ex. träningsmål, korpusstorlek, BPE-Dropout och förberedelselängd. Baserat på det föreslagna förfarandet utbildas en polsk BERT-baserad språkmodell - HerBERT. Denna modell ger toppmoderna resultat på flera nedströmsuppgifter.', 'ta': 'BERT- அடிப்படையான மாதிரிகள் தற்போது கிட்டத்தட்ட அனைத்து இயல்பான மொழி செயல்பாடு எனவே, NLP சமூகத்தில் இந்த மாதிரிகளை புரிந்து கொள்ள விரிவான ஆராய்ச்சி செய்கிறது, ஆனால் எல்லாவற்றிற்கும் மேலாக செயல்பட BERT-போன்ற மாதிரிகளை எப்படி பயிற்சி செய்யப்பட்டுள்ளது என்பதை அறிவிக்கும் பல உறுப்பு படிப்புகள், ஆனால் அவர்களில் பெரும்பாலா ஆங்கிலத்திற்கான வடிவமைக்கப்பட்ட ஒரு பயிற்சி செயல்பாடு பொதுவான மற்றும் சிறப்பான வழக்கமான மொழிகளுக்கு பயன்பட ஆகையால், இந்த காகிதத்தில் முதல் உறுதிப்படுத்தல் ஆய்வு போலிஷ் மீது கவனம் செலுத்தப்பட்டது, அது தனிப்படுத்தல் ஆங்கிலத்தை போ நாம் வடிவமைக்க மற்றும் முழுமையாக மதிப்பிடுகிறோம் பல மொழிகளிலிருந்து பிரெட் அடிப்படையில் இருந்து அறிவை மாற்றும் முறைம In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  பரிந்துரைக்கப்பட்ட செயல்பாட்டின் அடிப்படையில், போலிஷ் BERT- அடிப்படையான மொழி மாதிரி ஹெர்பெர்ட் - பயிற்சிய இந்த மாதிரி பல்வேறு கீழ் நீர் பணிகளின் நிலையில்- கலை முடிவுகளை பெறுகிறது.', 'ur': 'BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-art results. لہٰذا NLP کمونٹی نے ان موڈلوں کو سمجھنے کے بارے میں بڑی تحقیق کی، لیکن سب سے زیادہ عمدہ اور عمدہ تدریس پردازیوں کی طراحی پر۔ بہت سی مطالعہ پڑھنے کی تحقیق کرتی ہے کہ BERT جیسی موڈل کیسے تطالق کرنا ہے، لیکن ان میں سے اکثر اکثر صرف انگلیسی زبان کا مطالعہ ہے. انگلیسی کے لئے طراحی کی تدریس پردازی نہیں کرنا چاہیے اور ان کے لئے مخصوصاً ٹائیپولوژیکی مختلف زبانوں پر موجود ہونا چاہیے۔ لہٰذا، یہ کاغذ پہلی آبریس تحقیقات کو پولیش پر تمرکز کر رہا ہے، جو انگلیسی زبان کے بغیر ایک متفرق زبان ہے۔ ہم نے بہت زبان سے علم کو ایک زبان BERT-based موڈل کی طرف لے جانے کے لئے ایک دکھانے والی طراحی اور کامل قدرت کی۔ بہت سی زبان موڈل کی آغاز کی تعداد کے علاوہ، دوسرے فکتوروں کو بھی اثر دیتے ہیں جن کی ممکن ہے پررینڈینگ کی تاثیر کرتی ہے، یعنی تدریس موضوع، کورپوس کی اندازہ، BPE-ڈروپوٹ، اور پررینڈینگ کی مدت. پیشنهاد کی پردازی پر، ایک پولیش BERT-based زبان موڈل - HerBERT - کی تعلیم کی جاتی ہے. یہ موڈل بہت سی ڈونسٹریم کے کاموں پر موجود ہے.', 'mn': 'BERT-д суурилсан загварууд ойролцоогоор Байгалийн хэл Процессорын (NLP) ажлыг шийдэхэд хэрэглэгддэг ба ихэнхдээ урлагийн үр дүн гаргадаг. Тиймээс NLP-ын нийгэм эдгээр загваруудыг ойлгохын тулд маш их судалгаа хийдэг. Гэхдээ үүнээс илүү үр дүнтэй, үр дүнтэй сургалтын процедурыг зохион байгуулахын тулд. БЕРТ шиг загваруудыг хэрхэн суралцах талаар судалгааны олон судалгаа хийгдсэн. Гэхдээ ихэнх нь зөвхөн Англи хэлний тухай асуудаг. Англи хэлний төлөө зохион сургалтын процедур бусад хэл дээр ерөнхийлөгч, ялангуяа типтологийн өөр хэл дээр хэрэглэх шаардлагагүй. Тиймээс энэ цаас Польшад анхны хүчирхийллийн судалгааг илэрхийлж байна. Энэ нь англи хэлний хувьд нэг хэл юм. Бид олон хэлний мэдлэгийг БЕРТ-ын ганц хэлний загвар руу шилжүүлэх боломжтой арга загварыг зохион бүтээж, бүрэн үнэлдэг. Ихэнх хэл загварын эхлүүлэлтийн нэмэлт нь, эргүүлэлтийн нөлөөлдөг бусад хүчин зүйлс ч мөн судалж байна, яг л сургалтын зорилго, корпус хэмжээ, BPE-Dropout болон эргүүлэлтийн урт. Хэрвээ зөвлөсөн процедур дээр Польшийн Берт суурилсан хэл загвар - HerBERT - сургалт хийгддэг. Энэ загвар нь олон доорх үйл ажиллагааны төвшин урлагийн үр дүнг гаргадаг.', 'uz': "BERT asosida modellar hozirda hamma Natalik tillar jarayonlarini (NLP) ishlab chiqarish uchun ishlatiladi va ko'pchilik barcha sanalar natijasi natijasi natijasi natijasida bajarish natijalarini bajarish uchun ishlatiladi. Shunday qilib, NLP jamiyati bu modellarni tushunish uchun juda katta taʼminot boshlaydi, lekin hamma narsalardan tasdiqli va effektiv trening vazifalarini yaratish uchun. BERT kabi modellarni qanday o'rganishni o'rganishni o'rganish bir nechta o'rganishni o'rganish mumkin, lekin ularning ko'pchiligi ingliz tilidan faqat eng tilni anglatadi. Ingliz tili uchun yaratilgan taʼminlovchi dastur butunlay va boshqa odatda boshqa tillar uchun qoʻllanilmaydi kerak. Hullas, bu qogʻoz birinchi tashkilotni Polish tiliga aniqlaydi. Bu ingliz tildan teng'ilgan tilni o'zgartiradi. Biz bir so'zlar tildan monolik BERT asosiy modellarga o'zgartirish xizmatni o'zgartirish jarayonni qiymatimiz. Ko'pchilik modelni ishga tushirish qo'shish bilan boshqa sabablar o'zgarishga ega bo'lishi mumkin. Masalan, trening obʼekt, corpus oʻlchami, BPE-chiqarish va o'zgarishni o'rganadi. Aniqlanadigan vazifa asosida, Polish BERT asosiy tili modeli - HerBERT - o'rganilgan. Name", 'vi': 'Các mô hình nền Berlin được sử dụng để giải quyết gần như mọi công việc sản xuất ngôn ngữ tự nhiên (NLP) và hầu hết là đạt được kết quả tối tân. Do đó, cộng đồng Njala thực hiện nhiều nghiên cứu về hiểu những mô hình này, nhưng trên hết là thiết kế những thủ tục đào tạo hiệu quả và hiệu quả. Đã có nhiều nghiên cứu cắt bỏ về cách huấn luyện các mô hình giống loại BERT, nhưng hầu hết họ chỉ liên quan đến ngôn ngữ Anh. Một quy trình đào tạo cho người Anh không cần phải phổ biến và áp dụng cho những ngôn ngữ khác nhau đặc biệt theo tiêu chuẩn. Vì vậy, tờ giấy này trình bày nghiên cứu xóa đầu tiên tập trung vào người Ba Lan, mà, không giống như cách biệt ngôn ngữ Anh, là một ngôn ngữ liên tục. Chúng tôi thiết kế và đánh giá kỹ một thủ tục sản xuất để truyền kiến thức từ đa dạng sang các mô-đun thiếu sót ngôn ngữ. Ngoài việc khởi tạo mẫu đa dạng, cũng được tìm hiểu thêm các yếu tố có thể ảnh hưởng đến việc sản xuất trước, ví dụ như mục tiêu huấn luyện, kích thước thực tế, mất dạng BPE và độ dài trước. Dựa trên thủ tục được đề xuất, một mô hình ngôn ngữ Ba Lan dựa trên BERT được huấn luyện. Mô hình này đạt được kết quả hiện đại về nhiều công việc xuôi dòng.', 'bg': 'В момента базираните модели се използват за решаване на почти всички задачи по обработка на естествения език и най-често постигат най-съвременни резултати. Затова общността на НЛП провежда обширни изследвания за разбирането на тези модели, но преди всичко за проектиране на ефективни и ефективни процедури за обучение. Проведени са няколко аблационни проучвания, изследващи как да се обучават модели, подобни на BERT, но по-голямата част от тях засягат само английския език. Процедурата за обучение, предназначена за английски език, не трябва да бъде универсална и приложима за други особено типологично различни езици. Ето защо настоящата статия представя първото аблационно изследване, фокусирано върху полския, който, за разлика от изолиращия английски език, е фузионен език. Ние проектираме и задълбочено оценяваме процедура за предварително обучение за прехвърляне на знания от многоезични към едноезични модели базирани на БРТ. В допълнение към инициализацията на многоезичния модел се изследват и други фактори, които евентуално влияят на предтренирането, т.е. целта на обучението, размера на корпуса, отпадането и дължината на предтренирането. Въз основа на предложената процедура се обучава полски езиков модел - Херберт. Този модел постига най-съвременни резултати при множество задачи надолу по веригата.', 'da': 'BERT-baserede modeller bruges i øjeblikket til at løse næsten alle Natural Language Processing (NLP) opgaver og opnå oftest state-of-the-art resultater. NLP-fællesskabet udfører derfor omfattende forskning i forståelsen af disse modeller, men frem for alt i udformningen af effektive og effektive uddannelsesprocedurer. Der er gennemført adskillige ablationsundersøgelser, der undersøgte, hvordan man uddanner BERT-lignende modeller, men langt de fleste af dem vedrørte kun det engelske sprog. En uddannelsesprocedure til engelsk behøver ikke at være universel og gælde for andre særligt typologisk forskellige sprog. Derfor præsenterer denne artikel det første ablationsstudie fokuseret på polsk, som i modsætning til det isolerende engelske sprog er et fusionssprog. Vi designer og evaluerer grundigt en forududdannelsesprocedure for overførsel af viden fra flersprogede til ensprogede BERT-baserede modeller. Ud over flersproget model initialisering undersøges også andre faktorer, der muligvis påvirker pre-training, f.eks. træningsmål, korpusstørrelse, BPE-Dropout og pre-training længde. På grundlag af den foreslåede procedure trænes en polsk BERT-baseret sprogmodel - HerBERT. Denne model opnår topmoderne resultater på flere downstream-opgaver.', 'hr': 'Modeli na BERT-u trenutno se koriste za rješavanje skoro svih zadataka prirodnog procesa jezika (NLP) i najčešće postignu rezultate umjetnosti. Stoga zajednica NLP vodi široko istraživanje o razumijevanju tih modela, ali iznad svega o dizajniranju učinkovitih i učinkovitih postupka obuke. Provedeno je nekoliko ispitivanja ablacije koje istražuju kako obučavati modele slične BERT, ali većina njih je zabrinuta samo engleski jezik. Procedura obuke dizajnirana za engleski ne mora biti univerzalna i primjenjiva na druge posebno tipološki različite jezike. Stoga, ovaj papir predstavlja prvo proučavanje aktivacije usredotočeno na poljski, što je, za razliku od izolacijskog engleskog jezika, kombinacijski jezik. Mi dizajniramo i temeljno procjenjujemo proceduru pretvaranja premještaja znanja iz multijezičkih na monojezičke modele na osnovu BERT-a. Uz početak multijezičkih modela, istražuju se i drugi faktori koji su vjerojatno utjecali na pretkivanje, tj. cilj obuke, veličine korpusa, BPE-Dropout i dužina pretkivanja. Na temelju predloženog postupaka obučen je poljski model jezika na BERT-u HerBERT-u. Ovaj model postiže rezultate umjetnosti na višestrukim zadatkima.', 'nl': 'BERT-gebaseerde modellen worden momenteel gebruikt voor het oplossen van bijna alle Natural Language Processing (NLP) taken en bereiken meestal state-of-the-art resultaten. Daarom doet de NLP gemeenschap uitgebreid onderzoek naar het begrijpen van deze modellen, maar vooral naar het ontwerpen van effectieve en efficiënte trainingsprocedures. Er zijn verscheidene ablatiestudies uitgevoerd naar het trainen van BERT-achtige modellen, maar de overgrote meerderheid ervan betrof alleen de Engelse taal. Een opleidingsprocedure voor Engels hoeft niet universeel te zijn en van toepassing te zijn op andere, vooral typologisch verschillende talen. Daarom presenteert dit artikel de eerste ablatiestudie gericht op Pools, dat, in tegenstelling tot de isolerende Engelse taal, een fusionele taal is. We ontwerpen en evalueren grondig een pretraining procedure voor het overbrengen van kennis van meertalige naar eentalige BERT-gebaseerde modellen. Naast meertalige modellinitialisatie worden ook andere factoren onderzocht die pretraining mogelijk beïnvloeden, zoals trainingsdoel, corpusgrootte, BPE-Dropout en pretraining lengte. Op basis van de voorgestelde procedure wordt een Pools BERT-gebaseerd taalmodel HerBERT getraind. Dit model bereikt state-of-the-art resultaten op meerdere downstream taken.', 'de': 'BERT-basierte Modelle werden derzeit zur Lösung nahezu aller Aufgaben der Natural Language Processing (NLP) eingesetzt und erzielen meist State-of-the-Art Ergebnisse. Daher forscht die NLP-Community intensiv zum Verständnis dieser Modelle, vor allem aber zur Gestaltung effektiver und effizienter Trainingsverfahren. Es wurden mehrere Ablationsstudien durchgeführt, in denen untersucht wurde, wie BERT-ähnliche Modelle trainiert werden können, die überwiegende Mehrheit betraf jedoch nur die englische Sprache. Ein auf Englisch konzipiertes Schulungsverfahren muss nicht universell sein und auf andere besonders typologisch unterschiedliche Sprachen anwendbar sein. Daher stellt diese Arbeit die erste Ablationsstudie vor, die sich auf Polnisch konzentriert, das im Gegensatz zur isolierenden englischen Sprache eine Fusionssprache ist. Wir entwerfen und evaluieren ein Vortrainingsverfahren zur Wissensübertragung von mehrsprachigen auf einsprachige BERT-basierte Modelle. Neben der mehrsprachigen Modellinitialisierung werden auch andere Faktoren untersucht, die das Pretraining beeinflussen könnten, wie Trainingsziel, Korpusgröße, BPE-Dropout und Vorbereitungslänge. Basierend auf dem vorgeschlagenen Verfahren wird ein polnisches BERT-basiertes Sprachmodell HerBERT. geschult. Dieses Modell erzielt aktuelle Ergebnisse bei mehreren nachgelagerten Aufgaben.', 'id': 'BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results.  Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures.  Beberapa studi ablasi yang menyelidiki bagaimana melatih model seperti BERT telah dilakukan, tapi kebanyakan mereka hanya berkaitan dengan bahasa Inggris. A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages.  Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language.  Kami merancang dan meneliti secara teliti prosedur pelatihan untuk memindahkan pengetahuan dari model berbeda bahasa ke model berdasarkan BERT. Selain inisialisasi model berbagai bahasa, faktor lain yang mungkin mempengaruhi pretraining juga dieksplorasi, i.e. objektif latihan, ukuran corpus, BPE-Dropout, dan panjang pretraining. Berdasarkan prosedur yang diusulkan, model bahasa berbasis BERT Polandia - HerBERT - dilatih. Model ini mencapai hasil terbaik dalam beberapa tugas turun.', 'fa': 'مدل\u200cهای بنیاد BERT در حال حاضر برای حل تقریباً تمام کار\u200cهای پردازش زبان طبیعی (NLP) استفاده می\u200cشوند و اغلب به نتیجه\u200cهای حالت هنر رسیده می\u200cشوند. بنابراین، جامعه NLP تحقیقات زیادی در مورد درک این مدلها انجام می دهد، ولی بیشتر از همه در طراحی پروژه\u200cهای آموزش موثرتی و موثرتی انجام می دهد. تحقیقات چندین تحقیقات فعالیت در مورد تحقیقات چگونه مدل\u200cهای مانند BERT را آموزش می\u200cدهند، ولی بیشتر آنها فقط زبان انگلیسی را نگران می\u200cدارند. یک روش آموزش برای انگلیسی طراحی شده نیازی نیست که برای دیگران مخصوصاً زبانهای متفاوتی جهانی باشد. بنابراین، این کاغذ اولین مطالعه فعالیت را روی لهستان نشان می دهد، که در اختیار زبان انگلیسی، یک زبان فعالیت است. ما طراحی می کنیم و دقیقاً یک روش پیش\u200cگیری از انتقال علم از بسیاری زبان به مدل\u200cهای متحدی BERT را ارزیابی می\u200cکنیم. علاوه بر آغاز مدل زیادی زبان، faktورهای دیگر که احتمالاً تأثیر تأثیر تأثیر تأثیر تأثیر تأثیر می\u200cکنند، یعنی هدف آموزش، اندازه کورپوس، BPE-Dropout و طول تأثیر تأثیر تأثیر می بر اساس روش پیشنهاد، یک مدل زبان بوسیله BERT لهستان HerBERT آموزش داده می شود. این مدل نتیجه\u200cهای وضعیت هنری در کارهای زیادی پایین\u200cترین رسیده است.', 'ko': '버트 기반 모델은 현재 거의 모든 자연 언어 처리 (NLP) 임무를 해결하는 데 사용되고 있으며, 통상적으로 가장 선진적인 결과를 얻을 수 있다.따라서 NLP 커뮤니티는 이러한 모델을 이해하는 데 광범위한 연구를 진행했지만 가장 중요한 것은 효과적인 교육 프로그램을 설계하는 것이다.이미 몇 가지 연구를 전개하여 버트와 유사한 모델을 어떻게 훈련하는지 조사했지만, 그 중 절대 다수는 영어에만 관련되어 있다.영어를 위한 교육 프로그램이 반드시 통용되는 것도 아니고 다른 유형의 다른 언어에도 적용되는 것도 아니다.따라서 본고는 처음으로 폴란드어에 대해 연구를 진행했는데 고립된 영어와 달리 폴란드어는 융합된 언어이다.Dell은 다중 언어 모델에서 BERT 기반의 단일 언어 모델로 지식을 이전하기 위한 교육 전 절차를 설계하고 전면적으로 평가했습니다.다중 언어 모델의 초기화 외에 예훈련에 영향을 미칠 수 있는 다른 요소, 즉 훈련 목표, 어료 라이브러리 크기, BPE 퇴출과 예훈련 길이도 연구했다.이를 바탕으로 폴란드어 버트를 바탕으로 한 언어 모델인 허버트를 훈련시켰다.이 모델은 여러 하류 임무에서 가장 선진적인 결과를 실현했다.', 'tr': 'BERT tabanly nusgalary häzirki bärde tiz dil işlemlerini çözmek üçin ullanylýar we köplenç täze-de-sungatyň netijesini çözmek üçin ullanylýar. Şol sebäpli, NLP jemgyýeti bu nusgalary düşünmekde örän uly barlag edýär, ýöne hemme zadynda etkinji we täsirli okuw prosellerini tasarlamakda. BERT ýaly nusgalary öwrenmek üçin birnäçe täsirlenme öwrenmeleri barlap, ýöne olaryň köp bölegi diňe iňlisçe dilinde alada edýär. Iňlisçe tasarlanýan eğitim prosedüri be ýleki dillere uniwersal we ýagdaýynda tapawutlar gerek däl. Şol sebäpli, bu kagyz ilkinji gezek polonyça üns berilen ýagdaýy ukyplary görkezýär. Iňlisçe sözleri ýaly bir birleşik dildir. Biz çoxlu dillerden BERT tabanlı modellerinden bilimi almak üçin önlenmek ve düzgün bir şekilde değerlendiriyoruz. Birnäçe dil nusgasyny başlatmak üçin, önünlik etmäge mümkin başga faktörler keşfedildi, meselâ, okuw maksady, korpus ululygy, BPE-Dropout we önünlik uzyny. Suggestiň procedesine görä, Polonça BERT tabanly dil nusgasyna görä - HerBERT - okuwçylýar. Bu nusga birnäçe aşak täzeliklerde möhüm-sanat netijesini ýetip bilýär.', 'sw': 'Mfano wa msingi wa BERT kwa sasa unatumiwa ili kutatua kazi zote za Utarabu (NLP) na mara nyingi hupata matokeo ya sanaa. Kwa hiyo jumuiya ya NLP inafanya utafiti mkubwa wa kuelewa mifano hii, lakini zaidi ya yote kuhusu kutengeneza mbinu za mafunzo yenye ufanisi na ufanisi. Tafiti kadhaa za mabomu zinazochunguza namna ya kufundisha mifano kama BERT imefanywa, lakini wengi wao walihusu lugha ya Kiingereza tu. Utaratibu wa mafunzo uliobuniwa kwa Kiingereza hauhitaji kuwa wa kawaida na kutumika kwa lugha nyingine hususani kwa kawaida. Kwa hiyo, karatasi hii inaonyesha utafiti wa kwanza wa mwanzo wa kuboresha ulioanganisha na Wapole, ambao, tofauti na lugha ya Kiingereza inayotenganisha, ni lugha ya uchochezi. Tunaweza kutengeneza na kutathmini kwa kiasi kikubwa utaratibu wa kuhamisha maarifa kutoka lugha mbalimbali hadi mifano yenye lugha ya BERT. Zaidi ya kuanzishwa kwa mifano ya lugha mbalimbali, sababu nyingine ambazo zinaweza kuathiri kutengeneza mvua pia zinatafuta, yaani lengo la mafunzo, ukubwa wa makampuni, Utoaji wa BPE, na kuondoa kwa matumizi ya mvua. Kutokana na utaratibu huu unapendekezwa, muundo wa lugha yenye asili ya Polish BERT - HerBERT - umefundishwa. Mfano huu unafanikiwa matokeo ya hali ya sanaa katika kazi mbalimbali za mito.', 'sq': 'BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results.  Prandaj, komuniteti i NLP kryen kërkime të gjera mbi kuptimin e këtyre modeleve, por mbi të gjitha mbi dizajnimin e procedurave të trajnimit të efektshme dhe të efektshme. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language.  A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages.  Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language.  We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models.  In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  Bazuar në procedurën e propozuar, është trajnuar një model gjuhësor polak me bazë në BERT - HerBERT. Ky model arrin rezultate më të larta në disa detyra më poshtë.', 'am': 'BERT-based models are currently used for solving all natural ቋንቋ processes (NLP) and most often the state of the art results. ስለዚህ የNLP ማኅበረሰብ እነዚህን ምሳሌዎች በማስተዋል ብዙ ምርመራ ይሠራል፣ ነገር ግን በሁሉም ላይ ጥቅም እና በጥቅም ማስተካከል ነው፡፡ BERT-ምሳሌ ምሳሌዎችን እንዴት እንዲያስተምር በብዙ አካባቢዎች ተማርከዋል፤ ነገር ግን አብዛኞቻቸው በንግሊዝኛ ቋንቋ ብቻ ነው፡፡ እንግሊዘኛ የተለየ የትምህርት ፕሮጀክት ለሌሎች በተለየ በተለያዩ ቋንቋዎች አቀማሚ እና ልዩ ልዩ ቋንቋዎች መሆኑን አያስፈልግም፡፡ ስለዚህም ይህ ገጽ የፊተኛውን የአደባቂውን ትምህርት በፖሊስ ላይ ያስተካክላል፤ ከኢንጂልኛ ቋንቋ በተለየ አንደኛ ቋንቋ ነው፡፡ ከቋንቋ ቋንቋዎች ጀምሮ በሀይላንቋዊ BERT-ተመሳሳይ ወደሚደረገው ምሳሌዎችን ለመለወጥ የዝግጅት ሥርዓት እናስተዋልታለን፡፡ In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  በተፈቀደው ሥርዓት ላይ፣ የፖሊሽ BERT-based የቋንቋ ሞዴል - ሄርቢERT - የተማረ ነው፡፡ ይህ ምሳሌ የወንዝ አካባቢ ስራዎችን በብዛት የልዩ አርእስት ፍሬዎችን ያገኛል፡፡', 'hy': 'BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results.  Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures.  Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language.  Անգլերենի համար նախագծված ուսուցման գործընթացը ստիպված չէ լինել համաշխարհային և կիրառելի այլ հատկապես տիպոլոգիապես տարբեր լեզուների համար: Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language.  We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models.  In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length.  Հաշվի առնելով առաջարկած գործընթացի վրա, վարժեցվում է մի պոլանդական BERT-ի հիմնված լեզվի մոդել՝ ՀերBERT-ը: Այս մոդելը հասնում է ամենաբարձր արդյունքների բազմաթիվ հետագա խնդիրների վրա:', 'az': 'BERT tabanlı modellər həmin vaxtda növbənöv dil işləməsi (NLP) işlərini çəkmək üçün istifadə edilir və çox çox-çox sanat sonuçlarını başa çıxarır. Buna görə də NLP cəmiyyəti bu modelləri anlamaq haqqında geniş araştırma edər, amma üstündə də effektiv və effektiv təhsil prosedürlərini dizayn edir. BERT kimi modelləri təhsil etməyi təhsil edən bir neçə ablasiya təhsil edildi, amma onların əksəriyyəti yalnız İngilizə dilindən endirildi. İngilizce üçün müəyyən edilmiş təhsil prosedürü universel və başqa dillərə istisna olmaqla təhsil edilməli deyildir. Beləliklə, bu kağıt Polonyca tərəfindən təsirli ilk fəaliyyət təhsil edilməsini göstərir. Bu, İngilizə dilindən ayrılmasına bənzər, birləşdirilmiş dildir. Biz elmi çoxlu dildən monodil BERT tabanlı modellərə göndərmək və tamamilə təmizlənmək prosedüsini tasarlayırıq. Çoxlu dil modellərin başlançılığı ilə birlikdə, pretraining təsirlərinə mümkün olan başqa faktörlər də keşif edilir, məsələn təhsil məqsədili, korpus böyüklüyü, BPE-Dropout və pretraining uzunluğu. Önülləşdirilmiş prosedüyə görə Polonyca BERT tabanlı dil modeli - HerBERT - təhsil edilir. Bu model çoxlu aşağılıq işlərində müəyyən edilmiş sanat sonuçlarına yetişir.', 'bn': 'বেরেট ভিত্তিক মডেল বর্তমানে প্রায় সকল স্বাভাবিক ভাষা প্রক্রিয়ার (এনএলপি) কাজ সমাধানের জন্য ব্যবহার করা হয় এবং প্রায়শ শিল্পের ফলাফল প Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures.  বেরেট-এর মত মডেল কিভাবে প্রশিক্ষণ প্রদান করা হয়, কিন্তু তাদের বেশীরভাগ বেশীরভাগ ইংরেজি ভাষায় উদ্বিগ্ন। ইংরেজীর জন্য ডিজাইন করা একটি প্রশিক্ষণ প্রক্রিয়া বিশেষ করে বিভিন্ন ভাষায় অন্যান্য ভাষার জন্য সাধারণ এবং প্রয়োজনীয় তাই এই পত্রিকাটি প্রথম আগুনের গবেষণা পোলিশের দিকে মনোযোগ দিচ্ছে, যা ইংরেজি ভাষার ভাষার মতো বিচ্ছিন্ন ভাষার ভাষায় আমরা ডিজাইন এবং পরিপূর্ণ মূল্যায়ন করি বহুভাষা ভাষা থেকে বিবেরেট ভিত্তিক মডেল থেকে জ্ঞান পরিবর্তন করার প্রক্রিয়া। বহুভাষার মডেল শুরু করার পরিবর্তে অন্যান্য কারণ যে বৃষ্টির প্রভাব ফেলতে পারে তা অনুসন্ধান করা হয়, যেমন প্রশিক্ষণের উদ্দেশ্য, কোর্পাসের আকার, বিপেই-ড প্রস্তাবিত প্রক্রিয়ার ভিত্তিতে পোলিশ ভিত্তিক ভাষার মডেল - হার্বেরেট - প্রশিক্ষণ প্রদান করা হয়েছে। এই মডেলটি বেশ কয়েকটি নিচে নদীর কাজে রাষ্ট্র-শিল্পের ফলাফল অর্জন করে।', 'bs': 'Modeli na BERT-u trenutno se koriste za rješavanje skoro svih zadataka prirodnog procesa jezika (NLP) i najčešće postignu rezultate umjetnosti. Stoga zajednica NLP vodi široko istraživanje o razumijevanju tih modela, ali iznad svega o dizajniranju efikasnih i efikasnih postupaka obuke. Provedeno je nekoliko ispitivanja ablacije koje istražuju kako obučavati modele slične BERT-om, ali većina njih je zabrinuta samo engleski jezik. Procedura obuke dizajnirana za engleski ne mora biti univerzalna i primjenjiva na druge posebno tipološki različite jezike. Stoga, ovaj papir predstavlja prvo proučavanje aktivacije usredotočeno na poljski, što je, za razliku od izolacijskog engleskog jezika, kombinacijski jezik. Mi dizajniramo i temeljno procjenjujemo pretvaranje procedure prebacivanja znanja iz multijezičkih na monojezičke modele na BERT-u. Osim inicijalizacije multijezičkih modela, istražuju se i drugi faktori koji su vjerojatno utjecali na pretkivanje, tj. cilj obuke, veličine korpusa, BPE-Dropout i dužina pretkivanja. Na temelju predloženog postupaka, obučen je poljski model jezika na BERT-u HerBERT-u. Ovaj model postiže rezultate umjetnosti na višestrukim poslovima.', 'et': 'BERT-põhiseid mudeleid kasutatakse praegu peaaegu kõigi looduskeele töötlemise (NLP) ülesannete lahendamiseks ja kõige sagedamini saavutatakse tipptasemel tulemusi. Seetõttu viib uue tööprogrammi kogukond läbi ulatuslikke uuringuid nende mudelite mõistmiseks, kuid eelkõige tõhusate ja tõhusate koolitusprotseduuride väljatöötamiseks. On läbi viidud mitmeid ablatsiooniuuringuid, milles uuritakse BERT-sarnaste mudelite koolitamist, kuid enamik neist puudutas ainult inglise keelt. Inglise keele jaoks mõeldud koolitusprotseduur ei pea olema universaalne ja kohaldatav teistele eriti tüpoloogiliselt erinevatele keeltele. Seetõttu esitatakse käesolevas töös esimene ablatsiooniuuring, mis keskendub poola keelele, mis erinevalt isoleerivast inglise keelest on fusioonikeel. Projekteerime ja hindame põhjalikult koolituseelset protseduuri teadmiste ülekandmiseks mitmekeelsetelt BERT-põhistele mudelitele. Lisaks mitmekeelsele mudeli initsialiseerimisele uuritakse ka teisi tegureid, mis võivad mõjutada eeltreeningut, nt koolituse eesmärki, korpuse suurust, BPE-loobumist ja eeltreeningu pikkust. Kavandatud menetluse alusel koolitatakse poola BERT-põhist keelemudelit - HerBERT. Selle mudeliga saavutatakse tipptasemel tulemusi mitme järgmise etapi ülesande puhul.', 'cs': 'Modely založené na BERT se v současné době používají pro řešení téměř všech úloh zpracování přirozeného jazyka (NLP) a nejčastěji dosahují nejmodernějších výsledků. Proto NLP komunita provádí rozsáhlý výzkum na porozumění těmto modelům, ale především na navrhování efektivních a efektivních tréninkových postupů. Bylo provedeno několik ablačních studií zkoumajících, jak trénovat modely podobné BERT, ale drtivá většina z nich se týkala pouze anglického jazyka. Výcvikový postup určený pro angličtinu nemusí být univerzální a použitelný pro jiné obzvláště typologicky odlišné jazyky. Příspěvek proto představuje první ablační studii zaměřené na polštinu, která je na rozdíl od izolačního anglického jazyka fúzičním jazykem. Navrhujeme a důkladně zhodnotíme postup předškolení přenosu znalostí z vícejazyčných na jednojjazyčné modely založené na BERT. Kromě inicializace vícejazyčného modelu jsou zkoumány i další faktory, které možná ovlivňují předtrénink, tj. cíl tréninku, velikost korpusu, BPE-Dropout a délka předtréninku. Na základě navrženého postupu je vyškolen polský jazykový model založený na BERT.HerBERT. Tento model dosahuje nejmodernějších výsledků při více následných úkolech.', 'ca': "Actualment s'utilitzen models basats en BERT per resoldre gairebé totes les tasques de processament de llenguatges naturals (NLP) i aconsegueixen resultats més avançats. Així doncs, la comunitat del NLP fa una investigació extensa sobre la comprensió d'aquests models, però sobretot sobre el disseny de procediments eficients i eficients de formació. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language.  A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages.  Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language.  dissenyem i evaluem completament un procediment de pré-capacitació de transferència de coneixements de models multilingües a models monolingües basats en BERT. A més de l'inicialització del model multilingüe, també s'estudien altres factors que possiblement influeixen en la pré-capacitació, és a dir, l'objectiu d'entrenament, la mida del corpus, la BPE-Dropout i la llargada de pré-capacitació. Sobre la base del procediment proposat, un model de llenguatge polac basat en BERT, HerBERT, està capacitat. Aquest model aconsegueix resultats més avançats en múltiples tasques avall.", 'af': "BERT-gebaseerde modele word huidiglik gebruik vir die oplossing van byna alle Natuurlike Taal Verwerking (NLP) opdragte en mees dikwels bereik state-of-the-art resultate. Daarom, die NLP-gemeenskap bestuur uitbreidige ondersoek op hierdie modele te verstaan, maar bo almal op die ontwerp van effektief en effektief ondersoek prosesse. Verskeie ablasie studies wat ondersoek hoe om BERT-soos modele te oefen is uitgevoer, maar die groot meeste van hulle het slegs die Engelstaal betrokke. 'n Onderwysing Proceduur ontwerp vir Engels moet nie universele en toewenbaar wees vir ander spesifieke tipologiese verskillende tale nie. Daarom, hierdie papier stel die eerste ablasie studie wat op Poolse fokus is, wat, ongelukkig van die isolating Engelske taal, is 'n fusionele taal. Ons ontwerp en ondersteun 'n voortrekende prosedure van die oordraging van kennis van multilinglike na monolinglike BERT-gebaseerde modele. In addition to multilingual model initialization, other factors that possibly affect pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout and pretraining length. Op die voorgestelde prosedure is 'n Poolse BERT-gebaseerde taal model - HerBERT - onderwerp. Hierdie model bereik status-of-the-art resultate op veelvuldige onderstreem opdragte.", 'fi': 'BERT-pohjaisia malleja käytetään tällä hetkellä lähes kaikkien Natural Language Processing (NLP) -tehtävien ratkaisemiseen ja useimmiten saavutetaan huippuluokan tuloksia. Siksi NLP-yhteisö tekee laajaa tutkimusta näiden mallien ymmärtämisestä, mutta ennen kaikkea tehokkaiden ja tehokkaiden koulutusmenetelmien suunnittelusta. BERT-mallien kouluttamiseen liittyviä ablaatiotutkimuksia on tehty, mutta suurin osa niistä koski vain englannin kieltä. Englannille suunnitellun koulutusmenettelyn ei tarvitse olla yleismaailmallinen eikä sitä tarvitse soveltaa muihin erityisen typologisesti erilaisiin kieliin. Tämän vuoksi tässä työssä esitellään ensimmäinen ablaatiotutkimus, joka keskittyi puolaan, joka eristävästä englannin kielestä poiketen on fuusiokieli. Suunnittelemme ja arvioimme perusteellisesti esikoulutusprosessin tiedon siirtämiseksi monikielisistä BERT-malleista monikielisiin malleihin. Monikielisen mallin alustamisen lisäksi tutkitaan myös muita tekijöitä, jotka mahdollisesti vaikuttavat esikoulutukseen, kuten koulutustavoitetta, korpusen kokoa, BPE-pudotusta ja esikoulutuksen pituutta. Ehdotetun menettelyn perusteella koulutetaan puolalainen BERT-kielimalli eli HerBERT. Tällä mallilla saavutetaan huippuluokan tulokset useissa jatko-vaiheen tehtävissä.', 'jv': 'Language Kaya, komunitas NLP gunakake supaya perusahaan kanggo ngerasakno model iki, njuk kuwi supaya segala ngono nggawe barang layang lan ijol-ijolan. Ana sing perusahaan karo akeh pisan neng sampeyan ingkang dipatensi kapan banget nggawe model BERT koyo nggawe, njuk ingkang ibut akeh podho sing apik dhéwé kesempatan kanggo langgambar ingkang. Suara perusahaan sing dibutuhke kanggo ingkang ora bisa dianggap universe lan sakjane kanggo langgar sing itlanjut Kasempatan. Kaya, pepuluh iki mudheng perusahaan kanggo ngerasakno sing sumulaki nang polih, sing bingki karo alih ingkang sapa-sapa ingkang, kuwi lengkang sampeyan. Awakdhéwé nggawe lan akeh luwih basa perusahaan nggawe akeh luwih dumateng multilanggar sampek lan model singular BERT-sampek. politenessoffpolite"), and when there is a change ("assertivepoliteness NgaNgawe Perintah sing dibutuhke batasan, model sing basa Perancis BERT kuwi nggunakake boten - HerBERT - terus ampungan. Monday', 'he': 'דוגמנים מבוססים על BERT משתמשים כרגע לפתור כמעט את כל משימות תהליך שפת טבעית (NLP) ולפעמים קרובות להשיג תוצאות חדשות. לכן, קהילת NLP מבצעת מחקר רחב על הבנה של הדוגמנים האלה, אך מעל הכל על עיצוב תהליכים אימונים יעילים ויעילים. עשו כמה מחקרי ניתוח לחקור איך לאמן דוגמנים דומים דומים כמו BERT, אבל רובם העצום דאג רק לשפה האנגלית. תהליך אימון מעוצב לאנגלית לא צריך להיות אוניברסלי ולא מתאים לשפה אחרת במיוחד טיפולוגית שונה. Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language.  אנו מעצבים ובערכים בצורה יסודית תהליך מתקדם מאימון של העברת ידע ממדוגמנים רבות לשונות לבוססים על BERT. בנוסף לאתחילת מודל רב-שפתי, גורמים אחרים שאולי משפיעים על שימוש לפני שימוש חוקרים גם, כלומר אובייקטיבי אימון, גודל הקורפוס, BPE-Dropout, ואורך שימוש לפני שימוש. בהתבסס על ההליך המוצע, מודל שפה פולני מבוסס על BERT - הרברט - מאומן. המודל הזה משיג תוצאות חדשות במספר משימות מתקדמות.', 'sk': 'Modeli, ki temeljijo na BERT, se trenutno uporabljajo za reševanje skoraj vseh nalog obdelave naravnega jezika in najpogosteje dosegajo najsodobnejše rezultate. Zato skupnost NLP izvaja obsežne raziskave o razumevanju teh modelov, predvsem pa o oblikovanju učinkovitih in učinkovitih postopkov usposabljanja. Izvedenih je bilo več študij ablacije, ki so preučevale usposabljanje modelov, podobnih BERT-u, vendar se je velika večina nanašala le na angleški jezik. Postopek usposabljanja, namenjen za angleščino, ni treba biti univerzalen in uporabljati za druge zlasti tipološko različne jezike. Zato je v prispevku predstavljena prva ablacijska študija, osredotočena na poljščino, ki je za razliko od izolacijskega angleškega jezika fuzijski jezik. Načrtujemo in temeljito ocenjujemo postopek predusposabljanja prenosa znanja iz večjezičnih na enojezične modele BERT. Poleg inicializacije večjezičnega modela so raziskani tudi drugi dejavniki, ki morda vplivajo na predtreninge, npr. cilj usposabljanja, velikost korpusa, BPE-opustitev in dolžina predtreninga. Na podlagi predlaganega postopka se usposablja poljski jezikovni model BERT – HerBERT. Ta model dosega najsodobnejše rezultate pri več nadaljnjih nalogah.', 'ha': "Ana amfani da misãlai da aka ƙayyade BERT yanzu a yi amfani da yin solar aiki na taki duk aikin Jalali na Natural (NLP) kuma ana sami ƙarami masu babban-state-of-the-art. Saboda haka, jumuin NLP yana tafiyar research mai fassara a kan fahimtar waɗannan misãlai, kuma amma a kan dukkan kashfiya masu tsari da masu amfani da fassara. Babu'a karatun littattafai masu yin amfani da misãlai kamar BERT, kuma amma mafi yawansu suna cikin harshen Ingiriya kawai. Rufiyar wa zaman shawara wanda aka designe wa Ingiriya ba ya kamata a zama mai amfani da su da wasu harshen dabam'a mai ƙayyadadde. Saboda haka, wannan karatun na gaurar da ta farkon karatun na kullinta wanda aka yi makini a kan Polish, wanda, bã da misãlin harshen Ingirinsa da ɗabi'a, yana da wata harshe na zafi. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models.  Ga bayan faratarwa na misalin mulki-lingui, da wasu fakta masu yiwuwa sun yi amfani da raina, za'a nuna, misali, abun mai amfani, girmar nau'in rubutu, da tsawo na fitarwa. Basan da aka buɗa da shirin da aka yi niyyar da, an sanar da wata misalin harshen na Polish BERT - HerBERT - an daidaita. Wannan motel yana sãmun fassarar-sanar da masu yawa a kan aikin bayani-ƙarami.", 'bo': 'BERT་ལ་གཞི་བརྟེན་པའི་མིག་དཔེ་གཟུགས་དེ་ལྟར་རང་བཞིན་པའི་སྐད་ཡིག་ལས་སྦྱོར་ཀྱི་བྱ་བ་མང་ཆེ་ཤོས་ཡོད་པ། དེར་བརྟེན། NLP ཚོགས་སྡེར་གྱིས་མ་དཔེ་གཞི་འདི་དག་གི་སྐོར་ལྟ་རྟོག་པའི་ཆ་འཕྲིན་ཡོད་ཚད་ལྟར་ཞིབ་བྱེད་ཀྱི་ཡོད། BERT ལྟ་བུའི་དཔེ་དབྱིབས་བཟོས་མིན་དབྱེ་རིགས་ལ་ལྟ་བུ་བཏུབ་པ་ཡིན་ནའང་མང་ཆེ་ཤོས་ཀྱིས་དབྱིན་ཡིག་སྐད་ཡིག་ལས་གནོད་ དབྱིན་ཡིག་གི་ལ་སྒྲིག་ཐད་གཟུགས་འགོད་བྱས་པའི་སྒྲིག་འགོད་བྱས་མི་དགོས་པས། ཁྱད་དུ་དབྱིན དེར་བརྟེན། ཤོག་བྱང་འདིས་ཡིག་ཆ་དང་པོ་ཞིག་ལ་བློ་གཏོང་ནི་ལྕགས་རིགས་དང་པོ་ཞིག་སྟོན་པ་ཡིན། We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. སྐད་ཡིག སྔོན་འཛིན་བྱས་པའི་སྒེར་གྱི་ཐབས་ལམ་ལ་གཞི་རྟེན་དེ་ སྔོན་གྱི་BERT་གཞི་བརྟེན་པའི་སྐད་རིགས་མིང་ - HerBERT ། གཙོ་ར མ་དབྱིབས་འདིས་གནས་སྟངས་ལ་རང་ཉིད་སྒྱུ་རྩལ་གྱི་གནད་དོན་མང་པོ་ཞིག་ཡོད་པ'}
{'en': 'Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company’s Reputation', 'ar': 'الكشف عن الرسائل غير الملائمة حول الموضوعات الحساسة التي قد تضر بسمعة الشركة', 'fr': "Détecter les messages inappropriés sur des sujets sensibles susceptibles de nuire à la réputation d'une entreprise", 'pt': 'Detectando mensagens impróprias sobre tópicos sensíveis que podem prejudicar a reputação de uma empresa', 'es': 'Detección de mensajes inapropiados sobre temas delicados que podrían dañar la reputación de una empresa', 'zh': '检损公司声誉敏感主题之失消息', 'ja': '会社の評判を損なう可能性のある機密性の高いトピックに関する不適切なメッセージの検出', 'hi': 'संवेदनशील विषयों पर अनुचित संदेशों का पता लगाना जो किसी कंपनी की प्रतिष्ठा को नुकसान पहुंचा सकते हैं', 'ru': 'Обнаружение неподобающих сообщений на чувствительные темы, которые могут нанести вред репутации компании', 'ga': "Teachtaireachtaí Míchuí ar Thopaicí Íogaire a d'fhéadfadh Dochar a Dhéanamh ar Chlú Cuideachta a Bhrath", 'ka': 'შესაძლებელია შეტყობინება სენსიტიგური თემების შესახებ, რომელიც კომპონიაციის რეპუტაცია შესაძლებელია', 'el': 'Ανίχνευση ακατάλληλων μηνυμάτων σε ευαίσθητα θέματα που θα μπορούσαν να βλάψουν τη φήμη μιας εταιρείας', 'hu': 'Nem megfelelő üzenetek észlelése érzékeny témákon, amelyek árthatják a vállalat hírnevét', 'it': "Rilevare messaggi inappropriati su argomenti sensibili che potrebbero danneggiare la reputazione di un'azienda", 'kk': 'Компанияның қайталауын қайталау мүмкін болатын сезімді тақырыптарды таңдау', 'mk': 'Детектирање на несоодветни пораки за чувствителни теми кои би можеле да ја оштетат репутацијата на компанијата', 'lt': 'Nustatyti netinkamus pranešimus dėl jautrių temų, galinčių pakenkti bendrovės reputacijai', 'ml': 'ഒരു കമ്പനിയുടെ പുതുക്കം കണക്കാക്കുവാന്\u200d കഴിയുന്ന സെന്\u200dസിറ്റിവിലുള്ള സന്ദേശങ്ങള്\u200d തിരിച്ചറിയുന്നു', 'ms': "Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company's Reputation", 'mt': 'Is-Sejbien ta’ Messaġġi Inxierqa dwar Toġġetti Sensittivi li jistgħu jagħmlu ħsara lir-Reputazzjoni ta’ Kumpanija', 'pl': 'Wykrywanie niewłaściwych wiadomości dotyczących wrażliwych tematów, które mogą zaszkodzić reputacji firmy', 'no': 'Oppdagar inavhengige meldingar på følsområde som kunne avslutta gjentakinga av eit firma', 'mn': 'Компанийг дахин давтах боломжтой мэдрэмжгүй сэдвийн тухай буруу захиаг олж мэдэх', 'ro': 'Detectarea mesajelor inadecvate pe subiecte sensibile care ar putea dăuna reputației unei companii', 'si': 'සංවේදනය විදිහට අයුක්\u200dරිය පණිවිඩය හොයාගන්න පුළුවන් විදිහට සම්පූර්ණ විදිහට පණිවිඩය', 'sr': 'Otkrivanje neprikladnih poruka o osetljivim temama koje bi mogle da Harm ponavljanje kompanije', 'so': "Finding Inappropriate Messages on Sensitive Topics that could Harm a Company's Reputation", 'sv': 'Att upptäcka olämpliga meddelanden om känsliga ämnen som kan skada ett företags rykte', 'ta': 'ஒரு நிறுவனத்தின் மீண்டும் கண்டுபிடிக்க முடியும் உணர்ந்த தலைப்புகளில் சரியான செய்திகளை கண்டறிதல்', 'ur': 'Sensitive Topics پر غیر مناسب پیغام پتہ لیا جاتا ہے جو ایک کمپنی کا دوبارہ کرسکتا ہے', 'uz': 'Name', 'vi': 'Phát hiện các tin nhắn không phù hợp về các chủ đề nhạy cảm có thể ảnh hưởng đến danh tiếng của công ty', 'da': 'Opdagelse af upassende meddelelser om følsomme emner, der kan skade en virksomheds omdømme', 'bg': 'Откриване на неподходящи съобщения по чувствителни теми, които могат да навредят на репутацията на компанията', 'de': 'Erkennung unangemessener Nachrichten zu sensiblen Themen, die dem Ruf eines Unternehmens schaden könnten', 'nl': 'Het detecteren van ongepaste berichten over gevoelige onderwerpen die de reputatie van een bedrijf kunnen schaden', 'hr': 'Otkrivanje neprikladnih poruka o osjetljivim temama koje bi mogle zaustaviti ponavljanje kompanije', 'id': 'Mengeteksi pesan yang tidak sesuai pada Topik Sensitif yang bisa merusak reputasi sebuah perusahaan', 'fa': 'پیام\u200cهای غیرمناسب در مورد موضوع احساساتی که می\u200cتواند تکرار شرکت را تحقیق کند', 'sw': 'Kugundua ujumbe usio na uhakika kwenye mada yenye maana ambayo inaweza kuharibu Reputation of Company', 'tr': 'Kompaniýanyň ýene gaýşartmaky mümkin edýän hassas mesajlary a ňladylýar', 'af': "Ongeldige boodskappe ontdek op Sensitiewe onderwerpe wat kan 'n maatskappy se herhaal Harm", 'sq': 'Duke zbuluar mesazhe të papërshtatshme mbi temë të ndjeshme që mund të dëmtojnë reputacionin e një kompanie', 'am': 'መልዕክቶች', 'ko': '회사의 명예를 해칠 수 있는 민감한 화제의 부당한 정보를 검측하다', 'hy': 'Հետաքրքիր թեմաների մասին անհարմար հաղորդագրություններ հայտնաբերելը, որոնք կարող են վնասել ընկերության համբավը', 'bs': 'Otkrivanje neprikladnih poruka o osjetljivim temama koje bi mogle da harmira ponavljanje kompanije', 'ca': "Detectar missatges inadaptats sobre temes sensibles que podrien perjudicar la reputació d'una empresa", 'cs': 'Odhalování nevhodných zpráv o citlivých tématech, které by mohly poškodit reputaci společnosti', 'bn': 'সেন্সিটিভ বিষয়ের ব্যাপারে অযোগ্য বার্তা সনাক্ত করা হচ্ছে যা কোম্পানির পুনরুত্থান হার্ম করতে পারে', 'az': 'Ňěirketin yenil…ônm…ôsini Haram ed…ô bil…ôc…ôk hissl√ľ m…ôs …ôl…ôl…ôr bar…ôsind…ô uyńüun ismarńĪŇülar keŇüif edilir', 'et': 'Ebasobivate sõnumite tuvastamine tundlikel teemadel, mis võivad kahjustada ettevõtte mainet', 'fi': 'Epäasiallisten viestien havaitseminen arkaluonteisista aiheista, jotka voivat vahingoittaa yrityksen mainetta', 'he': 'גילוי הודעות לא הולמות על נושאים רגישים שיכולים לפגוע במוניטין של חברה', 'jv': 'Ngawe Perintah Mesah Gak-Pasang Sensitive Tema kang Could harm Ngubah Kompèni', 'ha': 'KCharselect unicode block name', 'sk': 'Odkrivanje neprimernih sporočil o občutljivih temah, ki lahko škodujejo ugledu podjetja', 'bo': 'མཁའ་དབུལ་གྱི་བསྐྱར་ལེན་བཟོ་བྱེད་པའི་གནད་དོན་ཐོག་ཏུ་ཚོར་བ་སྐྱེལ་མེད་པའི་འཕྲིན་དོན་རྟོགས'}
{'en': 'Not all topics are equally flammable in terms of toxicity : a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness. The core of inappropriateness is that it can harm the reputation of a speaker. This is different from toxicity in two respects : (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two datasets for Russian : a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this data.', 'pt': 'Nem todos os tópicos são igualmente “inflamáveis” em termos de toxicidade: uma discussão calma sobre tartarugas ou pesca alimenta diálogos tóxicos inapropriados com menos frequência do que uma discussão sobre política ou minorias sexuais. Definimos um conjunto de tópicos sensíveis que podem gerar mensagens inapropriadas e tóxicas e descrevemos a metodologia de coleta e rotulagem de um conjunto de dados para adequação. Embora a toxicidade em dados gerados pelo usuário seja bem estudada, nosso objetivo é definir uma noção mais refinada de inadequação. O cerne da inadequação é que pode prejudicar a reputação de um orador. Isso é diferente da toxicidade em dois aspectos: (i) a inadequação está relacionada ao tópico e (ii) a mensagem inadequada não é tóxica, mas ainda é inaceitável. Coletamos e liberamos dois conjuntos de dados para russo: um conjunto de dados rotulado por tópico e um conjunto de dados rotulado de adequação. Também lançamos modelos de classificação pré-treinados com base nesses dados.', 'ar': 'ليست كل الموضوعات "قابلة للاشتعال" بنفس القدر من حيث السمية: فالنقاش الهادئ للسلاحف أو صيد الأسماك غالبًا ما يغذي الحوارات السامة غير الملائمة أكثر من مناقشة السياسة أو الأقليات الجنسية. نحدد مجموعة من الموضوعات الحساسة التي يمكن أن تسفر عن رسائل غير ملائمة وسامة ونصف منهجية جمع مجموعة بيانات وتصنيفها للتأكد من ملاءمتها. بينما تمت دراسة السمية في البيانات التي ينشئها المستخدم جيدًا ، فإننا نهدف إلى تحديد مفهوم أكثر دقة عن عدم الملاءمة. جوهر عدم الملاءمة هو أنه يمكن أن يضر بسمعة المتحدث. هذا يختلف عن السمية من ناحيتين: (1) عدم الملاءمة مرتبط بالموضوع ، و (2) الرسالة غير الملائمة ليست سامة ولكنها لا تزال غير مقبولة. نقوم بجمع وإصدار مجموعتي بيانات للغة الروسية: مجموعة بيانات معنونة بالموضوع ومجموعة بيانات معنونة الملاءمة. نقوم أيضًا بإصدار نماذج تصنيف مُدرَّبة مسبقًا ومُدرَّبة على هذه البيانات.', 'es': 'No todos los temas son igualmente «inflamables» en términos de toxicidad: una discusión tranquila sobre las tortugas o la pesca con menos frecuencia alimenta diálogos tóxicos inapropiados que una discusión sobre política o minorías sexuales. Definimos un conjunto de temas delicados que pueden generar mensajes inapropiados y tóxicos y describimos la metodología de recopilación y etiquetado de un conjunto de datos para su adecuación. Si bien la toxicidad en los datos generados por los usuarios está bien estudiada, nuestro objetivo es definir una noción más precisa de lo inapropiado. El núcleo de lo inapropiado es que puede dañar la reputación de un orador. Esto es diferente de la toxicidad en dos aspectos: (i) lo inapropiado está relacionado con el tema y (ii) un mensaje inapropiado no es tóxico pero sigue siendo inaceptable. Recopilamos y publicamos dos conjuntos de datos para el ruso: un conjunto de datos etiquetado por tema y un conjunto de datos etiquetado como apropiado. También publicamos modelos de clasificación previamente entrenados y capacitados en estos datos.', 'fr': "Tous les sujets ne sont pas aussi «\xa0inflammables\xa0» en termes de toxicité\xa0: une discussion calme sur les tortues ou la pêche alimente moins souvent des dialogues toxiques inappropriés qu'une discussion sur la politique ou les minorités sexuelles. Nous définissons un ensemble de sujets sensibles pouvant générer des messages inappropriés et toxiques et décrivons la méthodologie de collecte et d'étiquetage d'un ensemble de données pour en vérifier la pertinence. Bien que la toxicité des données générées par les utilisateurs soit bien étudiée, nous visons à définir une notion plus fine d'inadéquation. Le cœur de l'inadéquation est qu'elle peut nuire à la réputation d'un orateur. Cela diffère de la toxicité à deux égards\xa0: (i) le caractère inapproprié est lié au sujet, et (ii) un message inapproprié n'est pas toxique mais reste inacceptable. Nous collectons et publions deux ensembles de données pour le russe\xa0: un jeu de données étiqueté par sujet et un ensemble de données étiqueté par pertinence. Nous publions également des modèles de classification pré-entraînés formés sur ces données.", 'ja': 'すべてのトピックが毒性の観点から同等に「燃えやすい」わけではありません。カメや釣りに関する冷静な議論は、政治や性的マイノリティに関する議論よりも不適切な毒性のある対話を助長することが少なくありません。私たちは、不適切で有毒なメッセージを生み出す可能性のある機密性の高いトピックのセットを定義し、適切性のためのデータセットの収集とラベル付けの方法論を説明します。ユーザーが生成したデータの毒性は十分に研究されていますが、私たちはより細かい不適切性の概念を定義することを目指しています。不適切さの核心は、スピーカーの評判を傷つける可能性があることです。これは、（ ｉ ）不適切性はトピックに関連しており、（ ｉ ｉ ）不適切なメッセージは毒性ではないが、依然として許容されないという２つの点で毒性とは異なる。トピックラベル付きデータセットと適切性ラベル付きデータセットの2つのロシア語データセットを収集してリリースします。また、このデータについてトレーニングを受けた事前トレーニング済みの分類モデルもリリースします。', 'zh': '夫毒性言之,非一言同易燃也:比于政或性少数群体之论,海龟渔之论,往往不长非其毒也。 我们定义了一朋敏感主题,这些主题会生有不当和有毒的信息,并描述了收标记数据集以使其适性的方法。 虽用户生数之毒而得之善,吾之所以定义者细粒度非其名也。 不当者,其可损言者之誉也。 此与毒性两异:(i)不当性与主题相关,及(ii)非其信无毒,然犹不可受也。 下二俄语数集:主题标数集,适性标数集。 又发此数训练之预训练分类模型。', 'hi': 'विषाक्तता के मामले में सभी विषय समान रूप से "ज्वलनशील" नहीं हैं: कछुओं या मछली पकड़ने की एक शांत चर्चा कम बार राजनीति या यौन अल्पसंख्यकों की चर्चा की तुलना में अनुचित विषाक्त संवादों को ईंधन देती है। हम संवेदनशील विषयों के एक सेट को परिभाषित करते हैं जो अनुचित और विषाक्त संदेश उत्पन्न कर सकते हैं और उपयुक्तता के लिए डेटासेट को इकट्ठा करने और लेबल करने की पद्धति का वर्णन कर सकते हैं। जबकि उपयोगकर्ता-जनित डेटा में विषाक्तता का अच्छी तरह से अध्ययन किया जाता है, हम अनुचितता की अधिक बारीक धारणा को परिभाषित करने का लक्ष्य रखते हैं। अनुचितता का मूल यह है कि यह एक वक्ता की प्रतिष्ठा को नुकसान पहुंचा सकता है। यह दो मामलों में विषाक्तता से अलग है: (i) अनुचितता विषय से संबंधित है, और (ii) अनुचित संदेश विषाक्त नहीं है लेकिन अभी भी अस्वीकार्य है। हम रूसी के लिए दो डेटासेट एकत्र और जारी करते हैं: एक विषय-लेबल डेटासेट और एक उपयुक्तता-लेबल डेटासेट। हम इस डेटा पर प्रशिक्षित पूर्व-प्रशिक्षित वर्गीकरण मॉडल भी जारी करते हैं।', 'ru': 'Не все темы одинаково «воспламеняются» с точки зрения токсичности: спокойное обсуждение черепах или рыболовства реже ведет к неуместному токсичному диалогу, чем обсуждение политики или сексуальных меньшинств. Мы определяем ряд деликатных тем, которые могут привести к появлению неуместных и токсичных сообщений, и описываем методологию сбора и маркировки набора данных на предмет их пригодности. Хотя токсичность данных, получаемых пользователями, хорошо изучена, мы стремимся определить более детальное понятие неуместности. Суть неуместности заключается в том, что она может нанести ущерб репутации оратора. Это отличается от токсичности в двух отношениях: i) неадекватность связана с темой, и ii) неадекватное сообщение не является токсичным, но все еще неприемлемым. Мы собираем и выпускаем два набора данных для русского языка: набор данных по теме и набор данных по пригодности. Мы также выпускаем предварительно подготовленные модели классификации, обученные этим данным.', 'ga': 'Níl gach ábhar chomh “inadhainte” céanna i dtéarmaí tocsaineachta: is lú go minic a spreagann plé socair ar thurtair nó ar iascaireacht le hidirphlé míchuí tocsaineacha ná le plé ar an bpolaitíocht nó ar mhionlaigh ghnéis. Sainímid sraith ábhar íogair ar féidir leo teachtaireachtaí míchuí agus tocsaineacha a thabhairt agus déanaimid cur síos ar an modheolaíocht chun tacar sonraí a bhailiú agus a lipéadú le haghaidh oiriúnachta. Cé go bhfuil dea-staidéar déanta ar thocsaineacht i sonraí a ghintear ag úsáideoirí, tá sé mar aidhm againn nóisean mí-oiriúnachta níos míne a shainiú. Is é croílár na míchuí ná go bhféadfadh sé dochar a dhéanamh do cháil cainteoir. Tá sé seo difriúil ó thocsaineacht ar dhá bhealach: (i) baineann míchuí leis an ábhar, agus (ii) níl an teachtaireacht mhíchuí tocsaineach ach fós do-ghlactha. Bailímid agus scaoilimid dhá thacar sonraí don Rúisis: tacar sonraí lipéadaithe topaicí agus tacar sonraí lipéadaithe oiriúnachta. Eisímid freisin samhlacha aicmithe réamhoilte atá oilte ar na sonraí seo.', 'ka': 'არა ყველა თემები ტოქსიტების შესახებ განსაზღვრებით არიან განსაზღვრებულია: საკუთარი საკუთარი განსაზღვრებით ან საკუთარი საკუთარი განსაზღვრებით არიან მოსაზღვრებული ტოქსიკური დიალოგიები, ვიდ ჩვენ განსაზღვრებთ სენტიგური ტემების ნახვა, რომელიც შეუძლებელია დაახლოებით და ტოქსიკური შეტყობინებების შესახებ და დაახლოებით მონაცემების შესახებ. თუმცა მომხმარებლის შექმნა მონაცემების ტოქსიტურობა კარგი სწავლია, ჩვენ მინდომით უფრო უფრო სწავლილი მონაცემების განსაზღვრება. ნვოპაგთლნჲ ვ, ფვ მჲზვ ეა ოჲგპვეთ პვოსრაუთწრა ნა დჲგჲპთრვლწ. ეს ტოქსიტიდან განსხვავებულია ორი განსაზღვრებით: i) შესაძლებელობა ტემიკითხულია, და ii) შესაძლებელი შეტყობინება არ არის ტოქსიური, მაგრამ არ შესაძლებელია. ჩვენ პროსიათვის ორი მონაცემების კონფიგურაციას შევყენებთ და გამოუშაოთ: საქმე დანაცემებული მონაცემების კონფიგურაცია და მონაცემების კონფიგ ჩვენ ასევე უფრო განვითარებით კლასიფიკაციის მოდელები, რომლებიც ამ მონაცემებზე განვითარებით.', 'el': 'Δεν είναι όλα τα θέματα εξίσου "εύφλεκτα" όσον αφορά την τοξικότητα: μια ήρεμη συζήτηση για τις χελώνες ή την αλιεία τροφοδοτεί λιγότερο συχνά ακατάλληλους τοξικούς διαλόγους από μια συζήτηση για την πολιτική ή τις σεξουαλικές μειονότητες. Καθορίζουμε ένα σύνολο ευαίσθητων θεμάτων που μπορούν να αποφέρουν ακατάλληλα και τοξικά μηνύματα και περιγράφουμε τη μεθοδολογία συλλογής και επισήμανσης ενός συνόλου δεδομένων για την καταλληλότητα. Ενώ η τοξικότητα στα δεδομένα που παράγονται από τους χρήστες είναι καλά μελετημένη, στοχεύουμε στον καθορισμό μιας πιο λεπτόκοκκης έννοιας της ακατάλληλης. Ο πυρήνας της ακατάλληλης είναι ότι μπορεί να βλάψει τη φήμη ενός ομιλητή. Αυτό διαφέρει από την τοξικότητα από δύο απόψεις: (i) η ακατάλληλη συμπεριφορά σχετίζεται με το θέμα και (ii) το ακατάλληλο μήνυμα δεν είναι τοξικό αλλά ακόμα απαράδεκτο. Συλλέγουμε και απελευθερώνουμε δύο σύνολα δεδομένων για τα ρωσικά: ένα σύνολο δεδομένων με ετικέτα θέματος και ένα σύνολο δεδομένων με ετικέτα καταλληλότητας. Επίσης, απελευθερώνουμε προ-εκπαιδευμένα μοντέλα ταξινόμησης εκπαιδευμένα σε αυτά τα δεδομένα.', 'hu': 'Nem minden téma ugyanolyan "gyúlékony" a toxicitás tekintetében: a teknősökről vagy a halászatról folytatott nyugodt viták ritkábban táplálják a nem megfelelő mérgező párbeszédeket, mint a politikáról vagy a szexuális kisebbségekről folytatott viták. Olyan érzékeny témákat határozunk meg, amelyek nem megfelelő és mérgező üzeneteket hozhatnak létre, és leírjuk az adatkészlet megfelelőségének és címkézésének módszerét. Bár a felhasználók által generált adatok toxicitását jól tanulmányozzák, célunk a helytelenség finomabb szemcséjű fogalmának meghatározása. A helytelenség lényege, hogy árthat egy felszólaló hírnevének. Ez két tekintetben különbözik a toxicitástól: (i) a helytelenség témával kapcsolatos, és (ii) a helytelen üzenet nem mérgező, de még mindig elfogadhatatlan. Két adatkészletet gyűjtünk és bocsátunk ki orosz számára: egy témakörrel ellátott adatkészletet és egy megfelelőséggel ellátott adatkészletet. Ezen adatokra képzett osztályozási modelleket is kiadunk.', 'it': 'Non tutti gli argomenti sono ugualmente "infiammabili" in termini di tossicità: una discussione calma sulle tartarughe o la pesca alimenta meno spesso dialoghi tossici inappropriati di una discussione sulla politica o sulle minoranze sessuali. Definiamo una serie di argomenti sensibili che possono generare messaggi inappropriati e tossici e descriviamo la metodologia di raccolta e etichettatura di un set di dati per l\'adeguatezza. Mentre la tossicità nei dati generati dagli utenti è ben studiata, miriamo a definire una nozione più fine di inappropriatezza. Il nocciolo dell\'inadeguatezza è che può danneggiare la reputazione di un oratore. Ciò è diverso dalla tossicità per due aspetti: (i) l\'inadeguatezza è legata all\'argomento e (ii) il messaggio inappropriato non è tossico ma comunque inaccettabile. Raccogliamo e rilasciamo due set di dati per il russo: un set di dati con etichetta tematica e un set di dati con etichetta appropriata. Rilasciamo anche modelli di classificazione pre-addestrati addestrati su questi dati.', 'kk': 'Барлық тақырыптар тоғыздық қасиетінде "жарқынышты" емес: саясат не сексуалдық әлпеттердің талқылауынан тынышты айтылу немесе балық қасиеттердің әдімгі диалогтарының қасиеттері болмайды. Біз жарамсыз және тәуелсіз хаттарды жібере алатын сезімді нақыштар жиынын анықтап, мәлімет үшін деректер жиынын жинау және белгілеу методологиясын анықтаймыз. Пайдаланушылардың құрылған деректерінің токсичілігі жақсы зерттеулі болғанда, біз бұл жағдайда жарамсыз ойын анықтау керек. Ол сөйлейтінің репутациясына қалдыру мүмкін емес. Бұл екі сәйкестікте токсичіліктен айырмашылық: i) мүмкіндіктілікті тақырыпты қатынасыз, және ii) мүмкіндікті хат токсичілікті емес, бірақ әлі қабылдамайды. Орус үшін екі деректер жинағын жинап, шығару: нақышты жарлық деректер жинағы және мәліметті жарлық деректер жинағы. Мұндай-ақ бұл деректерге оқылған алдын- оқылған классификациялау үлгілерін тастаймыз.', 'lt': 'Ne visos temos yra vienodai „degios“ toksiškumo požiūriu: rami diskusija dėl vėžlių arba žvejyba dažniau skatina netinkamus toksinius dialogus nei diskusija dėl politinių ar seksualinių mažumų. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness.  Nors toksiškumas naudotojų gautuose duomenise yra gerai ištirtas, mes siekiame apibrėžti tikslesnę netinkamumo sąvoką. Netinkamumo esmė yra ta, kad jis gali pakenkti kalbėtojo reputacijai. Tai skiriasi nuo toksinio poveikio dviem aspektais: i) netinkamumas yra susijęs su tema, ir ii) netinkamas pranešimas nėra toksinis, bet vis dar nepriimtinas. We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset.  We also release pre-trained classification models trained on this data.', 'ms': "Bukan semua topik sama-sama 'terbakar' dalam terma toksik: diskusi tenang penyu atau memancing kurang sering menghasilkan dialog toksik yang tidak sesuai daripada diskusi politik atau minoriti seksual. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness.  Sementara toksiciti dalam data yang dijana oleh pengguna dipelajari dengan baik, kita bertujuan untuk menentukan pemikiran yang lebih sempurna tentang ketidaksesuaian. Kerana ketidaksesuaian adalah bahawa ia boleh merugikan reputasi seorang pembicara. Ini berbeza dari toksik dalam dua aspek: (i) ketidaksesuaian berkaitan dengan topik, dan (ii) mesej tidak sesuai tidak beracun tetapi masih tidak dapat diterima. Kami mengumpulkan dan melepaskan dua set data untuk Rusia: set data yang ditabel topik dan set data yang ditabel keperluan. Kami juga melepaskan model klasifikasi terlatih terlatih pada data ini.", 'ml': "പ്രധാനപ്പെട്ട വിഷയത്തെക്കുറിച്ച് എല്ലാ വിഷയങ്ങളും ഒരുപോലെയല്ല 'അഗ്നിബിള്\u200d' അല്ല: കറലുകളെക്കുറിച്ചോ മീനിനെക്കുറിച്ചോ ശാന്തമായി സംസാരിക്ക ഒരു സെന്റിസിറ്റീവ് പ്രമേയങ്ങള്\u200d നാം വിശദീകരിക്കുന്നു. അത് അസാധുവായ സന്ദേശങ്ങള്\u200d കൊടുക്കുവാനും പാടില്ലാത്ത വിവരങ്ങള്\u200d കൊടുക്കുകയും ചെ ഉപയോക്താവ് സൃഷ്ടിക്കപ്പെട്ട വിവരങ്ങളിലെ വിദ്യാഭ്യാസം നന്നായി പഠിക്കുമ്പോള്\u200d നമ്മള്\u200d നിര്\u200dണ്ണയിക്കുന്നത് അപരിച പ്രസംഗിക്കുന്നവന്റെ വിശ്വാസത്തിന്റെ കൂട്ടത്തില്\u200d അതിന് വേദന വരുത്താന്\u200d സാധിക്കും. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable.  റഷ്യന്റെ രണ്ട് ഡാറ്റാസറ്റ് സെറ്റ് ചേര്\u200dക്കുകയും ചെയ്യുന്നു: ഒരു പ്രമേയത്തിലുള്ള ഡാറ്റാസേറ്റും ഒരു അനുവാദമായ ഈ വിവരങ്ങളില്\u200d പഠിപ്പിക്കപ്പെട്ടിരിക്കുന്ന ക്ലാസ്ഫിക്കല്\u200d മോഡലുകള്\u200d ഞങ്ങള്\u200d പുറത്ത് വിടുന്", 'mt': 'Mhux is-suġġetti kollha huma ugwalment “fjammabbli” f’termini ta’ tossiċità: diskussjoni kalma dwar il-fekruna jew is-sajd inqas spiss tikkawża djalogi tossiċi mhux xierqa minn diskussjoni dwar il-politika jew il-minoranzi sesswali. Aħna niddefinixxu sett ta’ suġġetti sensittivi li jistgħu jagħtu messaġġi mhux xierqa u tossiċi u niddeskrivu l-metodoloġija tal-ġbir u t-tikkettar ta’ sett ta’ dejta għall-adegwatezza. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness.  Il-qalba tal-inadegwatezza hija li tista’ tagħmel ħsara lir-reputazzjoni ta’ kelliem. Dan huwa different i mit-tossiċità f’żewġ aspetti: (i) in-nuqqas ta’ adattament huwa relatat mas-suġġett, u (ii) messaġġ mhux xieraq mhuwiex tossiku i żda għadu mhux aċċettabbli. Aħna nġabru u nħarġu żewġ settijiet ta’ dejta għar-Russu: sett ta’ dejta ttikkettat b’suġġett u sett ta’ dejta ttikkettat b’adegwatezza. Aħna nirrilaxxaw ukoll mudelli ta’ klassifikazzjoni mħarrġa minn qabel imħarrġa fuq din id-dejta.', 'mn': 'Бүх сэдвүүд нь хохиромжтой байдлын тухай адилхан "гэрэлтэй" гэдэг биш: салхи эсвэл загасны тухай ярилцлага нь улс төрийн эсвэл хүйсийн цөөнхийн тухай ярилцлагаас илүү ихэвчлэн хохиромжтой ярилцлага юм. Бид зөвхөн зөвхөн хохиромжгүй, хохиромжгүй захирагдал гаргаж, өгөгдлийн санг цуглуулж, тэмдэглэх аргыг тодорхойлж чадна. Хэрэв хэрэглэгчдийн гаргасан мэдээллийн хохирол сайн судалж байгаа ч бид илүү сайхан тарианы ойлголтыг тодорхойлж чадахгүй. Үнэндээ буруутгагүй байдлын үндсэн зүйл бол илтгэгчийн нэр хүндийг нөлөөлж чадна. Энэ нь хоёр хамааралтай хохиромжтой холбоотой холбоотой: i) зөвхөн зөвхөн сэдэв холбоотой, ii) зөвхөн зөвхөн зөвхөн зөвхөн хохиромжтой биш, гэхдээ хүлээн зөвшөөрөгдөхгүй. Бид хоёр өгөгдлийн санг Оросын хувьд цуглуулж, гаргаж өгөгдлийн санг: сэдвээр нэрлэгдсэн өгөгдлийн санг, зөвхөн нэрлэгдсэн өгөгдлийн санг. Мөн бид энэ өгөгдлийн талаар сургалтын өмнө сургалтын ангилалын загваруудыг илгээнэ.', 'pl': 'Nie wszystkie tematy są równie "łatwopalne" pod względem toksyczności: spokojna dyskusja na temat żółwi lub rybołówstwa rzadziej podsyca nieodpowiednie toksyczne dialogi niż dyskusja na temat polityki czy mniejszości seksualnych. Definiujemy zestaw wrażliwych tematów, które mogą dać niewłaściwe i toksyczne komunikaty oraz opisujemy metodologię gromadzenia i etykietowania zbioru danych pod kątem stosowności. Chociaż toksyczność w danych generowanych przez użytkowników jest dobrze zbadana, staramy się zdefiniować bardziej precyzyjne pojęcie niewłaściwości. Podstawą niewłaściwości jest to, że może ona zaszkodzić reputacji mówcy. Różni się to od toksyczności pod dwoma względami: (i) niewłaściwość jest związana z tematem, a (ii) niewłaściwa wiadomość nie jest toksyczna, ale nadal nie do przyjęcia. Gromadzimy i udostępniamy dwa zbiory danych dla rosyjskiego: zbiór danych o etykiecie tematycznej i zbiór danych o etykiecie odpowiedniości. Udostępniamy również wstępnie przeszkolone modele klasyfikacji przeszkolone na podstawie tych danych.', 'no': 'Ikkje alle emne er likevel «flammable» i forhold av toksikitet: ein stille diskusjon om kortler eller fisking mindre ofte brenner inappropørte toksiske dialogar enn ein diskusjon om politikk eller seksuele minoritet. Vi definerer eit sett av følsomsiktige emner som kan gje inapprette og toksiske meldingar og beskrive metodologien for å samla og merke ein dataset for tilgjengelighet. Selv om toksikitet i brukaren genererte data er godt studiert, må vi definera ein meir fynnekorn notisjon om ikkje tilgjengeleg. Kjarten av inappriktighet er at han kan skade reputasjonen av eit taler. Dette er forskjellig frå toksikitet i to område: i) ikkje tilgjengeleg er temarelatert, og ii) ikkje tilgjengeleg melding er toksisk, men fortsatt ikkje tilgjengeleg. Vi samler og slettar to datasett for russisk: eit datasett med temamerket og eit datasett med merkeligheten. Vi gjev også opplærte klassifikasjonsmodular som trengte på denne data.', 'mk': "Not all topics are equally 'flammable' in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities.  We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness.  While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness.  The core of inappropriateness is that it can harm the reputation of a speaker.  This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable.  We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset.  Исто така, објавуваме и предобучени класификациски модели обучени на овие податоци.", 'sr': 'Nije sve teme jednako "plamen" u smislu toksičnosti: mirna diskusija o kornjacima ili ribarstvu manje često gori neprikladne toksične dijaloge nego diskusija o politici ili seksualnim manjinama. Definiramo set osjetljivih tema koje mogu da donose neprikladne i toksične poruke i opisujemo metodologiju skupljanja i označavanja set a podataka za pristojnost. Dok je toksičnost u podacima proizvedenim korisnicima dobro proučena, ciljamo da definišemo fino zrno mišljenje o neprikladnosti. Jezgrada neprikladnosti je da može štetiti reputaciju govornika. Ovo je drugačije od toksičnosti u dva pogleda: i) neprikladnost je povezana sa temom, i ii) neprikladna poruka nije toksična, ali ipak neprihvatljiva. Skupljamo i oslobodimo dva seta podataka za rusku: seta podataka označena na temi i seta podataka označena za pristojnost. Takođe oslobodimo predobučene klasifikacijske modele obučene na ovim podacima.', 'ro': 'Nu toate subiectele sunt la fel de "inflamabile" în ceea ce privește toxicitatea: o discuție calmă despre țestoase sau pescuitul alimentează mai puțin adesea dialoguri toxice inadecvate decât o discuție despre politică sau minorități sexuale. Definim un set de subiecte sensibile care pot genera mesaje inadecvate și toxice și descriem metodologia de colectare și etichetare a unui set de date pentru adecvare. În timp ce toxicitatea datelor generate de utilizatori este bine studiată, ne propunem să definim o noțiune mai fină de inadecvatitudine. Nucleul inadecvat este că poate dăuna reputației unui vorbitor. Acest lucru diferă de toxicitate în două privințe: (i) inadecvarea este legată de subiect și (ii) mesajul inadecvat nu este toxic, dar totuși inacceptabil. Colectăm și lansăm două seturi de date pentru limba rusă: un set de date etichetat cu subiect și un set de date etichetat cu adecvarea. De asemenea, lansăm modele de clasificare pre-instruite instruite pe aceste date.', 'si': "නෑ හැම ප්\u200dරශ්නයක්ම සිද්ධ විදිහට සමාන්\u200dය 'flammeble' තියෙන්නේ: සංසුන් ප්\u200dරශ්නයක් නැති විදිහට මුළු ප්\u200dරශ්නයක් නැති ප්\u200dරශ්නයක් ව අපි සංවේදනය විදිහට සංවේදනය කරන්න පුළුවන් වගේ සංවේදනය සංවේදනය සහ විෂ්\u200dය පණිවිදිහට පිළිගන්න පුළුවන් වගේ සංවේද පාවිච්චි නිර්මාණය කරපු දත්තේ හොඳින් අධ්\u200dයානය කරලා තියෙනවා නමුත්, අපි අදහස් කරනවා වැඩිය හොඳින් ප්\u200dරතිකාරය අවුලක් නැහැ කිරීමක් තමයි ඒකට පුළුවන් කතා කරපු කෙනෙක්ගේ ප්\u200dරමාණය අපරාධ කරන්න පුළුවන්. මේක ප්\u200dරශ්නයක් දෙන්නෙක් වලින් වෙනස් වෙන්නේ: (i) විශ්නයක් නැති විදියට සම්බන්ධ වෙන්න පුළුවන්, ඒ (i) විදියට සම්බන්ධ අපි රුසියානු වෙනුවෙන් දත්ත සෙට් දෙකක් සම්බන්ධ කරනවා: ප්\u200dරශ්නයක් ලේබල් කරලා තොරතුරු සෙට් සහ අවශ්\u200dයතා අපි මේ තොරතුරු ගැන පුහුණුවන් ප්\u200dරධානය කරලා තියෙන්න පුළුවන් පරීක්ෂණ පරික්ෂණ මොඩ", 'so': 'Madaxaha dhamaantoodna ma ahan isku mid ah oo ku saabsan xumaatooyinka: sheekeysi xasilloon oo kala sheekeysan dhaqdhaqaaq ama kalluumeysi inta badan oo aan istaahilin is-baaraandegista siyaasada ama qofka ka tirsan jinsiga ah. Waxaannu qornaa maadooyin jilicsan oo ay soo saari karaan warqado aan habboonayn iyo caqli xumayn, waxaana qori karnaa qaababka soo ururista iyo ku sawirida sawirada macluumaadka. Inta lagu baranayo macluumaadka isticmaalayaasha, waxaan ku qoraynaa fikrada aan ku filnayn. Oo xukunka aan la filnayn waa in kan hadalka ku hadlaa magaciisa wax yeeli kara. Tani waa mid ka duwan dhibaatada labada dhinac: (i) mid aan istaahilin waa mid la xiriira mada, oo (ii) farriin aan ku filnaynna ma khatar laakiin weli la aqbali karo. Waxaynu soo urursannaa oo u soo bixinaynaa laba koosaar oo macluumaad ah oo Ruush ah: sawir macluumaad lagu magacaabay iyo sawir macluumaad ku habboon. Sidoo kale waxaynu bixinaa samooyinka tababarida horay loo tababaray oo lagu baray macluumaadkan.', 'sv': 'Alla ämnen är inte lika "brandfarliga" när det gäller toxicitet: en lugn diskussion om sköldpaddor eller fiske ger mindre ofta upphov till olämpliga giftiga dialoger än en diskussion om politik eller sexuella minoriteter. Vi definierar en uppsättning känsliga ämnen som kan ge olämpliga och giftiga budskap och beskriver metoden för att samla in och märka ett dataset för lämplighet. Medan toxicitet i användargenererade data är väl studerad, strävar vi efter att definiera en mer finkornig uppfattning om olämplighet. Kärnan i olämplighet är att det kan skada en talares rykte. Detta skiljer sig från toxicitet i två avseenden: i) olämplighet är ämnesrelaterad, och ii) olämpligt budskap är inte giftigt men ändå oacceptabelt. Vi samlar in och släpper två dataset för ryska: ett ämnesmärkt dataset och ett lämplighetsmärkt dataset. Vi lanserar även förkränade klassificeringsmodeller som är utbildade på dessa data.', 'ta': 'புவியுற்றத்தின் விஷயத்தில் அனைத்து தலைப்புகளும் சமமாக இல்லை: குளிர்கள் அல்லது மீன் பற்றிய ஒரு அமைதியான விவாதம் அல்லது பெரும்பாலாவது தீவிரமான உரையாடல்கள We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness.  பயனர் உருவாக்கப்பட்ட தரவுகளில் புவியுற்றத்தை நன்றாக படிக்கப்பட்டுள்ளது போது, நாம் ஒரு நன்றாக பிடிக்கப்படாத சிந்தனையை வரையறுக பேசுபவரின் பெயரை தீங்க முடியும். இது இரண்டு பகுதிகளில் துன்பத்தை விட்டும் வேறுபட்டுள்ளது: (i) தேவையில்லாத தலைப்பு தொடர்பு உள்ளது, இன்னும் (ii) சரியான செய்தியில் துன்பத் நாங்கள் ரஷ்யனுக்கு இரண்டு தரவு அமைப்புகளை சேகரித்து வெளியிடுகிறோம்: ஒரு தலைப்பு குறிப்பிட்ட தகவல் அமைப்பு மற்றும இந்த தரவுகளில் பயிற்சிக்கப்பட்ட முன் பயிற்சி வகுப்பு மாதிரிகளை வெளியிடுகிறோம்.', 'ur': 'تمام موضوع سمندی کے معاملہ میں برابر نہیں ہیں: ٹورٹل یا مچھلی کے معاملہ میں آرام کی بحث ہے کہ سیاست یا جنسی ذرہ ذرہ کی بحث سے زیادہ مطابق غیر مطابق سمندی دائولوں کے مطابق مطابق ہے. ہم ایک مجموعہ حساس موضوع کی تعریف کرتے ہیں جو غلط اور جسمی پیغام پہنچا سکتے ہیں اور مطابق کے لئے ایک ڈاٹ سٹ کے مطابق جمع کرنے اور لیبل کرنے کے مطابق مطابق کا مطابق بیان کرسکتے ہیں. اگرچہ کارساز کے پیدا کئے ہوئے ڈیٹے میں سمیت بہتر تحقیق کی جاتی ہے، ہم اس کا ارادہ کررہے ہیں کہ بہت اچھی دانے کا اندازہ مقرر کریں۔ ناپسندیدہ بات کا ذریعہ یہ ہے کہ یہ ایک ایسے شخص کی نسبت نقصان پہنچا سکتا ہے۔ یہ دو قسموں میں جسم سے مختلف ہے: i) ناپاکیزگی موضوع سے متعلق ہے، اور ii) ناپاکیزہ پیغام جسم نہیں ہے لیکن اب بھی قبول نہیں کرسکتا۔ ہم روسی کے لئے دو ڈیٹ سٹ جمع کریں اور آزاد کریں: ایک ٹیٹ پر لابل کیا گیا ڈیٹ سٹ اور ایک اچھی طرح لابل کیا گیا ڈیٹ سٹ۔ ہم نے اس ڈیٹے پر آموزش کی پیش آموزش کی کلاسیک موڈل کو بھی چھوڑ دیا۔', 'uz': "Hamma mavzular toʻgʻri darajada teng bo'lgan narsa emas: turtler yoki kallik haqida xavfsiz talabatlar ko'p narsa qo'shilik yoki sexlemalar haqida gapirishdan foydalanmaydi. Biz juda yetarli mavzularni ajratib, notoʻgʻri va toksisk xabarlarni chiqarishi mumkin va muhimiy maʼlumotlar toʻplarini ajratish va tahrirlash usulini ajratish mumkin. Foydalanuvchi maʼlumotdagi toʻgʻri taʼminlovchi bo'lganda, biz notoʻgʻri emas deb o'ylaymiz. Mavjud emas, gapiruvchi sifatini qo'shish mumkin. Bu ikki darajada o'xshash bo'lgan boshqa ma'lum: (i) mavzu bilan bogʻ'liq va (ii) notoʻgʻri xabar toʻgʻri emas lekin qabul qilmaydi. Biz Ruscha uchun ikkita maʼlumotlar tarkibini olib tashlamiz. Mavzu maʼlumotlar tarkibini va muhimiy maʼlumotlar tarkibi. Biz bu maʼlumot haqida o'rganilgan o'rganishdan oldin classification modellarini chiqaramiz.", 'vi': "Không phải tất cả các chủ đề đều'dễ cháy nổ'về độ độc tố. Một cuộc thảo luận bình tĩnh về rùa hay câu cá ít khi cung cấp cho cuộc đối thoại độc hại không thích hợp hơn là một cuộc thảo luận về chính trị hay thiểu số tình dục. Chúng tôi xác định một loạt các chủ đề nhạy cảm có thể cung cấp thông điệp không thích đáng và độc hại và mô tả phương pháp thu thập và mô tả một bộ dữ liệu về sự thích hợp. Trong khi độc tính trong dữ liệu người dùng được nghiên cứu kỹ, chúng tôi hướng tới xác định một khái niệm thiếu phù hợp hơn. Cốt lõi của sự thiếu chính là nó có thể làm tổn hại đến danh tiếng của người nói. Đi ều này khác với độc tố trong hai khía cạnh: i) không phù hợp là liên quan đến chủ đề, và II) thông điệp không đúng là độc hại, nhưng vẫn không thể chấp nhận. Chúng tôi thu thập và phát hành hai bộ dữ liệu cho Nga: một tập tin có nhãn về chủ đề và một tập tin có nhãn hợp lý. Chúng tôi cũng phát hành các mẫu phân loại được đào tạo trên dữ liệu này.", 'da': 'Ikke alle emner er lige så brændbare med hensyn til toksicitet: En rolig diskussion om skildpadder eller fiskeri giver mindre ofte anledning til upassende giftige dialoger end en diskussion om politik eller seksuelle mindretal. Vi definerer et sæt følsomme emner, der kan give upassende og giftige budskaber, og beskriver metoden til at indsamle og mærke et datasæt for hensigtsmæssighed. Mens toksicitet i brugergenererede data er velundersøgt, sigter vi mod at definere en mere finkornet begreb om upassende. Kernen i uhensigtsmæssighed er, at det kan skade en talers omdømme. Dette adskiller sig fra toksicitet i to henseender: i) upassende er emnerelateret, og ii) upassende budskaber er ikke giftige, men stadig uacceptable. Vi indsamler og frigiver to datasæt til russisk: et emnemærket datasæt og et passende datasæt. Vi udgiver også prætrænede klassifikationsmodeller, der er trænet på disse data.', 'bg': 'Не всички теми са еднакво "запалими" по отношение на токсичността: спокойната дискусия за костенурки или риболов по-рядко подхранва неподходящи токсични диалози, отколкото дискусия за политика или сексуални малцинства. Определяме набор от чувствителни теми, които могат да доведат до неподходящи и токсични съобщения и описваме методологията за събиране и етикетиране на набор от данни за целесъобразност. Въпреки че токсичността в генерираните от потребителите данни е добре проучена, ние се стремим да определим по-фино понятие за неподходящо. Ядрото на неподходящото е, че може да навреди на репутацията на оратор. Това се различава от токсичността в две отношения: i) неподходящото е свързано с темата и ii) неподходящото съобщение не е токсично, но все пак е неприемливо. Събираме и пускаме два набора от данни за руски език: набор от данни, обозначен с тема, и набор от данни, обозначен с целесъобразност. Ние също така пускаме предварително обучени модели за класификация, обучени по тези данни.', 'nl': "Niet alle onderwerpen zijn even 'ontvlambaar' in termen van toxiciteit: een kalme discussie over schildpadden of visserij voedt minder vaak ongepaste toxische dialogen dan een discussie over politiek of seksuele minderheden. We definiëren een reeks gevoelige onderwerpen die ongepaste en giftige berichten kunnen opleveren en beschrijven de methodologie voor het verzamelen en labelen van een dataset op geschiktheid. Hoewel toxiciteit in door gebruikers gegenereerde gegevens goed bestudeerd is, streven we ernaar een meer gedetailleerd begrip van ongepast te definiëren. De kern van ongepast gedrag is dat het de reputatie van een spreker kan schaden. Dit verschilt in twee opzichten van toxiciteit: (i) ongepast is thematisch gerelateerd en (ii) ongepast bericht is niet giftig maar nog steeds onaanvaardbaar. We verzamelen en vrijgeven twee datasets voor het Russisch: een onderwerp-gelabelde dataset en een gepastheid-gelabelde dataset. We geven ook vooraf getrainde classificatiemodellen uit die op deze gegevens zijn getraind.", 'de': 'Nicht alle Themen sind in Bezug auf Toxizität gleich "brennbar": Eine ruhige Diskussion über Schildkröten oder Fischen schürt seltener unangemessene toxische Dialoge als eine Diskussion über Politik oder sexuelle Minderheiten. Wir definieren eine Reihe sensibler Themen, die unangemessene und toxische Botschaften hervorbringen können, und beschreiben die Methodik zur Sammlung und Kennzeichnung eines Datensatzes auf Angemessenheit. Während die Toxizität in nutzergenerierten Daten gut untersucht ist, streben wir an, einen detaillierteren Begriff von Unangemessenheit zu definieren. Der Kern der Unangemessenheit ist, dass sie dem Ruf eines Sprechers schaden kann. Dies unterscheidet sich von der Toxizität in zweierlei Hinsicht: (i) Unangemessenheit ist themenbezogen und (ii) unangemessene Botschaft ist nicht giftig, aber immer noch inakzeptabel. Wir sammeln und veröffentlichen zwei Datensätze für Russisch: einen thematisch gekennzeichneten Datensatz und einen angemessen gekennzeichneten Datensatz. Wir veröffentlichen auch vortrainierte Klassifizierungsmodelle, die auf diesen Daten trainiert werden.', 'ko': "독성으로 말하자면 모든 화제가 똑같은 '인화성' 은 아니다. 정치나 성소수자들의 토론에 비해 거북이를 냉정하게 논의하거나 물고기를 적게 잡으면 부적절한 유독 대화를 불러일으킨다.우리는 부적절하고 유해한 정보를 초래할 수 있는 민감한 주제를 정의했고 데이터 집합을 수집하고 표시하는 방법을 설명했다.사용자가 생성한 데이터 중의 독성은 이미 좋은 연구를 받았지만 우리의 목표는 더욱 세밀한 입도의 부적절한 개념을 정의하는 것이다.부당한 행위의 핵심은 강연자의 명예를 손상시키는 것이다.이것은 두 가지 측면에서 독성과 다르다. (i) 부적절함은 화제와 관련이 있고, (ii) 부적절한 정보는 독성이 없지만 여전히 받아들일 수 없다.우리는 두 개의 러시아어 데이터 집합을 수집하고 발표했다. 하나는 주제 표시의 데이터 집합이고 하나는 적당한 표시의 데이터 집합이다.우리는 또 이러한 데이터를 바탕으로 하는 예비 훈련 분류 모델을 발표했다.", 'id': "Not all topics are equally 'flammable' in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities.  We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness.  Sementara toksicitas dalam data yang dihasilkan oleh pengguna dipelajari dengan baik, kita bermaksud mendefinisikan ide yang lebih baik tentang ketidaksesuaian. Intinya tidak sesuai adalah bahwa itu bisa merugikan reputasi seorang pembicara. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable.  Kami mengumpulkan dan melepaskan dua set data untuk Rusia: sebuah set data yang ditabel topik dan sebuah set data yang ditabel keperluan. Kami juga melepaskan model klasifikasi terlatih terlatih pada data ini.", 'fa': 'همه موضوع به همان اندازه "آتش سوزان" در مورد سمی نیستند: یک بحث آرامش درباره کورک ها یا ماهیگیری کمتر اغلب به عنوان بحث سیاست یا کمترین گروهی جنسی به سوزان می رسد. ما یک مجموعه از موضوع حساس را تعریف می\u200cکنیم که می\u200cتواند پیغام\u200cهای غیرقابل و سمی را بدست آورد و روش\u200cشناسی جمع کردن و برچسب کردن یک مجموعه داده برای مناسب توصیف می\u200cکنیم. در حالی که سمی در اطلاعات تولید شده از کاربر خوب مطالعه می\u200cشود، ما هدف داریم که یک فکری پاکیزه\u200cتر از غیر مناسب تعریف کنیم. core of inappropriate is that it can harm the reputation of a speaker. این از سمی در دو جهت متفاوت است: i) غیرقابل استفاده از موضوع است و ii) پیام غیرقابل استفاده از سمی نیست ولی هنوز غیرقابل قبول است. ما دو مجموعه داده برای روسیه جمع می\u200cکنیم و آزاد می\u200cکنیم: یک مجموعه داده\u200cهای مورد علامت و یک مجموعه داده\u200cهای مورد مناسب. ما همچنین مدلهای مختصات پیش آموزش آموزش داده شده\u200cایم که روی این داده آموزش داده شده\u200cاند.', 'sw': 'Sio mada zote ni sawa na ‘moto’ kwa sababu ya ukosoaji: mjadala wa utulivu wa mapinduzi au uvuvu mara nyingi hugusa mazungumzo yasiyo ya kisaikolojia kuliko mjadala wa siasa au wachache wa kijinsia. Tunaweza kufafanua mfululizo wa mada yenye hisia ambazo zinaweza kusambaza ujumbe usio sahihi na kutosha na kuelezea njia ya kukusanya na kutuma taarifa kwa ajili ya usawa. Wakati ukosefu wa taarifa zinazozaliwa kwa watumiaji unasomwa vizuri, tunakusudia kuelezea dhana nzuri inayofanikiwa isiyo sahihi. Msingi wa kutokuwa na usawa ni kwamba unaweza kuharibu sifa ya mzungumzaji. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable.  Tunakusanya na kuachia seti mbili za taarifa kwa ajili ya Urusi: seti ya taarifa zilizotajwa mada na seti ya taarifa inayopendekezwa. Pia tunaachia mitindo ya usafiri wa zamani yanayofundishwa kwenye taarifa hizi.', 'af': "Nie alle onderwerpe is gelyk 'flammebaar' in bedryf van toksisiteit:  'n kalm diskusie van kortels of visse minder dikwels brei onpasmaak toksiske dialoog as 'n diskusie van politiek of seksuele minoriteit. Ons definieer 'n stel van sensitiewe onderwerpe wat onpasmaak en toxiese boodskappe kan gee en die metodologie van versameling en etiketting van 'n datastel vir toepassing beskrywe. Alhoewel die toksiciteit in gebruiker genereerde data goed onderwyser is, is ons doel om 'n meer fyn-koring notie van onverwendigheid te definieer. Die kern van onregverdigheid is dat dit kan skade die reputasie van 'n sprekker. Hierdie is verskillend van toksiciteit in twee respekte: i) onapprop is onderwerp-verwante, en ii) onapprop boodskap is nie toksic nie, maar nog steeds onaanvaarbaar. Ons versamel en verlos twee datastel vir Russies: â\x80\x99n onderwerp-etiketeerde datastel en â\x80\x99n toepassing-etiketeerde datastel. Ons verlos ook voor-opgelei klasifikasie-modele wat op hierdie data opgelei is.", 'am': 'ሁሉም ጉዳዮች በጥቃት፣ የጥላቅ ውይይት፣ የፖለቲካ ወይም የዝሙት አነስተኛ ታዋቂዎች ከመወያየት ይልቅ የማይፈቅድ የስክሲ ማነጋገር አይደሉም፡፡ የማይፈለግ እና የስክሲ መልዕክቶች የሚያሳልፍ እና የዳታዎችን ማሰብሰብ እና ማሳየት የሚችሉትን አካሄድ እናሳውቃለን፡፡ በተጠቃሚ አካባቢ ዳታዎችን በመልካም ሲተማርከው፣ ደግሞም የማይገባውን አሳብ ማረጋገጥ እናስፈልጋለን፡፡ የማይገባው አውታር የሚናገረውን ዝና ይጐዳል። ይህ በሁለቱ ክፍሎች የሚለይ ነው:(i) የማይፈለግ ጉዳዩ ነው፣ (ii) የማይሰራ መልእክት ስክራት አይደለም ነገር ግን የማይቀበል ነው፡፡ We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset.  እንደዚህ አዳራዎች የተማሩት የፊተኛውን ክፍለ ሥርዓት ምሳሌዎችን እናስቀራለን፡፡', 'tr': 'Hemme temalar toksiýa görä bir ýaramaz däl: turunlar ýa-da balyklar hakynda köplenç gürrüňsiz bir ýaramaz däldir. syýasaty ýa-da cinsel azyklary diýmek üçin ýeterli toksiýa däldir. Biz ýaramaz we toksik mesajlary edip biljek hassas mesajlaryň bir toparyny tanaýarýarys we özüniň derejesi üçin bir dataset etiketleyebilir Ullançylar tarapyndan zehirlenen maglumatlarda gowy öwrenmeli bolsa, biz iň ýigrenç bir düşünjäni tanlamak üçin amaçlandyrys. Uýarsyzlygyň çekiminiň esasy çykyşlygyň ünsüni uryp biler. Bu ikinji görnüşde toksik täsirinden üýtgeşik (i) meňzeşligi meňzeşdir, we (ii) ýerleşmiş mesaj toksik däl, emma entägem kabul edilmez. Biz Rusça üçin iki sany sany gurlaýarys we çykarýarys: bir tema etilen veri setir we nädogry etilen veri setir. Biz hem öňünden eğitilen klasifikasyon nusgalaryny bu maglumatlarda çykarýarys.', 'sq': 'Jo të gjitha çështjet janë njëlloj "të djegur" në lidhje me toksicitetin: një diskutim i qetë i bretkosave apo peshkimit më pak shpesh nxit dialoge të papërshtatshme toksike sesa një diskutim i politikës apo pakicave seksuale. Ne përcaktojmë një sërë temësh të ndjeshme që mund të japin mesazhe të papërshtatshme dhe toksike dhe të përshkruajmë metodologjinë e mbledhjes dhe etiketimit të një sërë të dhënash për përshtatjen. Ndërsa toksiciteti në të dhënat e gjeneruara nga përdoruesit është studiuar mirë, ne synojmë të përcaktojmë një koncept më të hollë të papërshtatshmërisë. Qendra e papërshtatshmërisë është se mund të dëmtojë reputacionin e një folësi. Kjo është ndryshe nga toksiciteti në dy aspekte: (i) papërshtatshmëria është e lidhur me temën dhe (ii) mesazhi i papërshtatshëm nuk është toksik por ende i papërshtatshëm. Ne mbledhim dhe lëshojmë dy grupe të dhënash për rusët: një grup të dhënash me etiketë tematike dhe një grup të dhënash me etiketë të përshtatshmërisë. Ne gjithashtu lëshojmë modele klasifikimi të paratrajnuar të trajnuar në këto të dhëna.', 'hy': 'Ոչ բոլոր թեմաները նույնքան "վառելի" չեն թունավորության տեսանկյունից: Կաշտանակների հանգիստ քննարկումները կամ ձկնորսությունը ավելի հաճախ պայքարում են անհամապատասխան թունավոր հաղորդակցություններ, քան քաղաքականության կամ սեռական փոքրությունների քննար Մենք սահմանում ենք մի շարք զգայուն թեմաներ, որոնք կարող են անհարմար և թունավոր հաղորդագրություններ տալ և նկարագրել տվյալների համակարգի հավաքելու և նշանակելու մեթոդոլոգիան: Մինչդեռ օգտագործողների կողմից ստեղծված տվյալների թունավորությունը լավ ուսումնասիրում է, մենք նպատակում ենք սահմանել ավելի գեղեցիկ անհարմարության գաղափար: Անպատասխանատվության հիմքն այն է, որ այն կարող է վնասել խոսնակի համբավը: Սա տարբերվում է թունավորությունից երկու հարցում. i) անհարմարությունը թեմայի հետ կապված է, և i) անհարմար հաղորդագրությունը թունավոր չէ, բայց դեռևս անընդունելի է: Մենք հավաքում և հրապարակում ենք երկու տվյալների համակարգ Ռուսաստանի համար. թեմայով պիտակված տվյալների համակարգ և համապատասխանության պիտակված տվյալների համակարգ: We also release pre-trained classification models trained on this data.', 'bn': 'ব্যথার মাধ্যমে সকল বিষয়গুলো একই রকম নয়: টার্লল বা মাছ নিয়ে শান্তিপূর্ণ আলোচনা রাজনৈতিক বা যৌন সংখ্যালঘুদের আলোচনার চেয়ে প্রায়শই ব্যথাযথ ব্যথা নেই। আমরা একটি সংবেদনশীল বিষয় নির্ধারণ করি যা যথেষ্ট এবং ব্যস্ত বার্তা উৎপাদন করতে পারে এবং যে কোনো তথ্য সংগ্রহ করা ও লেবেলিং করতে পারে তা বর্ণনা করি। ব্যবহারকারীদের উৎপাদন করা তথ্যের ব্যাক্তিগত তথ্য ভালো গবেষণা করা হচ্ছে, আমরা উদ্দেশ্যের উদ্দেশ্য হচ্ছি যে অযথাযথ ধারণা নির্ধ The core of inappropriateness is that it can harm the reputation of a speaker.  এই বিষয়টি দুই ভাগের মধ্যে বিদ্রুপ থেকে আলাদা: (i) অবৈধ বিষয়ের সাথে সম্পর্কিত, আর (ii) অবৈধ বার্তা ব্যস্ত নয় কিন্তু এখনো গ্রহণযোগ্য নয়। আমরা রুশের জন্য দুটি ডাটাসেট সংগ্রহ করি এবং মুক্তি প্রদান করি: একটি বিষয়টি লেবেল করা ডাটাসেট এবং একটি মূল্যবান ডাটাসেট। We also release pre-trained classification models trained on this data.', 'bs': "Nije sve teme jednako 'plamen' u smislu toksičnosti: smirena diskusija o kornjacima ili ribarstvu manje često gori neprikladne toksične dijaloge nego o diskusiji o politici ili seksualnim manjinama. Definiramo set osjetljivih tema koje mogu donijeti neprikladne i toksične poruke i opisati metodologiju skupljanja i označavanja set a podataka za pristojnost. Dok je toksičnost u podacima od korisnika dobro proučena, ciljamo definirati fino zrno mišljenje o neprikladnosti. Jezgro neprikladnosti je da može štetiti reputaciju govornika. To je drugačije od toksičnosti u dva pogleda: i) neprikladnost je povezana s temom, a ii) neprikladna poruka nije toksična, ali još uvijek neprihvatljiva. Skupljamo i objavljujemo dve datasete za ruske: setu podataka označenog teme i setu podataka označenog pristojnosti. Takođe objavljujemo predobučene modele klasifikacije obučene na ovim podacima.", 'ca': 'No tots els temes són igualment "inflamables" en termes de toxicitat: una discussió calm a sobre tortuetes o pesca menys sovint alimenta diàlegs tòxics inadequats que una discussió sobre política o minories sexuals. Definim un conjunt de temes sensibles que poden donar missatges inadequats i tòxics i descriure la metodologia de recollir i etiquetar un conjunt de dades per l\'apropiació. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness.  El nucli de l\'inadequació és que pot perjudicar la reputació d\'un orador. Això és diferent de la toxicitat en dos aspectes: i) la inadequació està relacionada amb el tema, i ii) el missatge inadequat no és tòxic però encara inacceptable. Recollim i publicam dos conjunts de dades per russos: un conjunt de dades etiquetat amb tema i un conjunt de dades etiquetat amb apropiació. We also release pre-trained classification models trained on this data.', 'cs': 'Ne všechna témata jsou stejně "hořlavá" z hlediska toxicity: klidná diskuse o želvách nebo rybolovu méně často podněcuje nevhodné toxické dialogy než diskuse o politice nebo sexuálních menšinách. Definujeme soubor citlivých témat, která mohou přinášet nevhodné a toxické zprávy a popisujeme metodiku shromažďování a označování datové sady pro vhodnost. Zatímco toxicita v uživatelsky generovaných datech je dobře studována, snažíme se definovat jemnější pojem nevhodnosti. Jádrem nevhodnosti je, že může poškodit pověst řečníka. To se liší od toxicity ve dvou ohledech: (i) nevhodnost souvisí s tématem a (ii) nevhodná zpráva není toxická, ale stále nepřijatelná. Shromažďujeme a uvolňujeme dva datové sady pro ruský jazyk: datovou sadu označenou tématem a datovou sadu označenou vhodností. Vydáváme také předškolené klasifikační modely trénované na těchto údajích.', 'fi': 'Kaikki aiheet eivät ole myrkyllisyyden kannalta yhtä "syttyviä": rauhallinen keskustelu kilpikonnista tai kalastuksesta ruokkii vähemmän sopimatonta myrkyllistä vuoropuhelua kuin keskustelu politiikasta tai seksuaalisista vähemmistöistä. Määrittelemme joukon arkaluonteisia aiheita, jotka voivat tuottaa sopimattomia ja myrkyllisiä viestejä, ja kuvaamme menetelmän kerätä ja merkitä aineiston tarkoituksenmukaisuutta. Vaikka käyttäjien tuottaman tiedon myrkyllisyyttä on tutkittu hyvin, pyrimme määrittelemään hienojakoisemman käsitteen sopimattomuudesta. Asianvastaisuuden ydin on se, että se voi vahingoittaa puhujan mainetta. Tämä eroaa myrkyllisyydestä kahdella tavalla: i) sopimattomuus liittyy aiheeseen ja ii) sopimaton viesti ei ole myrkyllinen, mutta sitä ei voida hyväksyä. Keräämme ja julkaisemme venäjänkieliselle aineistolle kaksi aineistoa: aihemerkitty aineisto ja tarkoituksenmukaisuusmerkitty aineisto. Julkaisemme myös ennalta koulutettuja luokittelumalleja, jotka on koulutettu näistä tiedoista.', 'et': 'Mitte kõik teemad ei ole mürgisuse mõttes võrdselt "tuleohtlikud": rahulik arutelu kilpkonnast või kalapüügist tekitab vähem ebasobivaid mürgiseid dialooge kui arutelu poliitika või seksuaalvähemuste üle. Määratleme kogumi tundlikke teemasid, mis võivad anda sobimatuid ja mürgiseid sõnumeid, ning kirjeldame andmekogumi asjakohasuse kogumise ja märgistamise metoodikat. Kuigi mürgisust kasutajate loodud andmetes on hästi uuritud, on meie eesmärk määratleda täpsem mõiste sobimatusest. Sobimatuse tuum on see, et see võib kahjustada kõneleja mainet. See erineb toksilisusest kahes aspektis: i) sobimatus on teemaga seotud ja ii) sobimatu teade ei ole toksiline, kuid siiski vastuvõetamatu. Kogume ja avaldame vene keele jaoks kaks andmekogumit: teemamärgistusega andmekogumit ja asjakohasusemärgistusega andmekogumit. Samuti anname välja eelnevalt koolitatud klassifitseerimismudelid, mis on koolitatud nende andmete põhjal.', 'az': "Bütün məsələlər zehirlik haqqında eyni olaraq 'yandırıcı' deyildir: çörpələr və balıqlar barəsində daha az sıxıntılı müzakirə edir, siyasi və cinsi azınlıqların müzakirə etməsindən çox uyğun zehirli müzakirə edir. Biz uyğunluq və zehirli mesajlar təbliğ edə biləcək hisslə məsələləri təyin edirik və yaxşılıq üçün verilən qurma və etiketlərin metodiklərini təsdiqləyirik. İstifadəçilərin təhsil edilmiş verilər yaxşı təhsil edilirsə də, biz daha yaxşı təhsil edilməyən fikrini tanımmaq istəyirik. Uğursuzluq məqsədilə danışanların şəninə zərər verə biləcəyini düşünürlər. Bu iki tərəfdə toxunmadan fərqli: i) uyğunluq məsələdən əlaqədir, ii) uyğunluq məsələn zehirli deyildir, amma hala qəbul edilməz. Biz Rus üçün iki veri qurğunu toplayırıq: məlumat etiketli veri qurğusu və etiketli veri qurğusu. Biz də bu məlumatlarda təhsil edilmiş öncə təhsil edilmiş klasifikasyon modellərini yayındırırıq.", 'hr': 'Nije sve teme jednako "plamen" u smislu toksičnosti: mirna diskusija o kornjačama ili ribarstvu manje često gori neprikladne toksične dijaloge nego diskusija o politici ili seksualnim manjinama. Definiramo niz osjetljivih tema koji mogu donijeti neprikladne i toksične poruke i opisati metodologiju prikupljanja i označavanja podataka za pristojnost. Iako je toksičnost u podacima od korisnika dobra ispitivanja, ciljamo definirati fino zrno mišljenje o neprikladnosti. Željeznica neprikladnosti je da može štetiti reputaciju govornika. To je drugačije od toksičnosti u dva pogleda: i) neprikladnost je povezana s temom, a ii) neprikladna poruka nije toksična, ali još uvijek neprihvatljiva. Skupljamo i objavljujemo dvije podatke za ruske: komplet podataka označen na temi i komplet podataka označen za pristojnost. Također objavljujemo predobučene modele klasifikacije obučene na ovim podacima.', 'ha': 'Babu duk madaidaici daidai da "Firasa" a cikin mazaɓa na haske: wata magana mai aminci na magana na turɓãya ko kifi da yawa zajeyen akwatin bayani na\'urar zafi ba da daidai ko kuma zafi masu ƙaranci. Munã ƙayyade wasu madaidaita masu aikin da za\'a iya fitar da manzannin da ba\'a daidai ba kuma za\'a bayyana hanyoyi wa tãrayyar da kuma za\'a lissafa wani matsayin da za\'a yi daidaita. Idan tozarci na cikin data wanda aka ƙiƙira shi mai amfani da shi aka jarraba shi mai kyau, sai za\'a ƙayyade wani zafi mai kyau wa\'anar gaske ba. The core of inappropriateness is that it can harm the reputation of a speaker.  Wannan yana rarrabe daga aikin giya guda biyu:(i) Babbanci na da maɓalli, kuma (ii) bai zama mai zartar da ba, kuma amma ba za a karɓi ba. Tuna samun kuma Muke saka kodi biyu masu tsari ga Ruushi: wani matsayi wanda aka yi wa maɓalli da kuma wani tsari da aka rubũta ta daidaita. Tuna sakar da misãlai masu da aka yi wa zaman tsari a kan wannan data.', 'sk': 'Vse teme niso enako "vnetljive" glede strupenosti: mirna razprava o želvah ali ribolovu manj pogosto spodbuja neprimerne strupene dialoge kot razprava o politiki ali spolnih manjšinah. Opredelimo nabor občutljivih tem, ki lahko prinesejo neprimerna in strupena sporočila ter opišemo metodologijo zbiranja in označevanja nabora podatkov za ustreznost. Čeprav je strupenost v podatkih, ki jih ustvarijo uporabniki, dobro proučena, si prizadevamo opredeliti bolj drobnozrnat pojem neustreznosti. Jedro neustreznosti je, da lahko škoduje ugledu govornika. To se razlikuje od strupenosti v dveh pogledih: (i) neustreznost je povezana s temi in (ii) neustrezno sporočilo ni strupeno, vendar še vedno nesprejemljivo. Zbiramo in izdajamo dva nabora podatkov za ruski jezik: nabor podatkov z oznako tematike in nabor podatkov z oznako ustreznosti. Izdajamo tudi vnaprej usposobljene modele klasifikacije, usposobljene na podlagi teh podatkov.', 'jv': 'Gak dhéwé tema sing berarti \'anyar\' sing dikarepaké karo toksisané: nganggep langgar sampeyan kanggo ngilangno dolanan sing isiné, or a tau nggawe dialog toksiso sing gak bener kanggo nganggep politik apa menyang siku. Awak dhéwé ngerwih akeh sing ngerasakno iki luwih akeh gambar ngono akeh dumadhi iki bakal terus nggawe gerakno ngono nggawe akeh dhèwèké beraksi barêng-barêng. layers-action Bilih sing ora adalah kuwi nganggep kuwi ora bisa nggawe pernik obah-obahan. Iki dumadhi kanggo toksisan iki sakjane: i) kabèh dumadhi aceh dumadhi, lan (i i) kabèh dumadhi sing ora ngerasakno, sane ora ngerasakno. string" in "context_BAR_stringLink Awakdhéwé éntuk Program-model sing ditambahak asakno, dadi, ingkang diangkat.', 'he': 'לא כל הנושאים "דלקים" במונחים של רעילות באותה מידה: דיון רגוע על צבים או דייג פחות לעתים קרובות דופק דיאלוגים רעילים לא הולמים מאשר דיון על פוליטיקה או מינויות מיניות. אנחנו מגדירים קבוצה של נושאים רגישים שיכולים להביא הודעות לא הולמות ורעילות ולתיאר את המטדולוגיה של אסוף וטיקוט קבוצת נתונים לתאימה. בזמן שהרעילות בנתונים שנgenerו ע"י המשתמשים נלמדה היטב, אנו מתכוונים להגדיר רעיון לא מתאים יותר. הליבה של הלא הולמת היא שזה יכול לפגוע במוניטין של רמקול. זה שונה מהרעילות בשני מבחינים: (i) לא מתאימה היא קשורה לנושא, ו (ii) מסר לא מתאים הוא לא רעיל אבל עדיין לא מקובל. אנחנו אוספים ושחררים שני קבוצות נתונים לרוסי: קבוצת נתונים עם תווית נושא ומקבוצת נתונים עם תווית. אנחנו גם משחררים מודלים מסווג מאומנים מראש מאומנים על הנתונים האלה.', 'bo': "གནད་དོན་འདི་ཆ་མཉམ་ཞིག་མིན་ན་'flammable'དང་བློ་གཏོང་ཆེན་' སླེབས་ཚུལ་དང་། We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. སྤྱོད་མཁན་གྱི་བྱ་ཚུལ་ཞིབ་འཇུག་པའི་ཆ་འཕྲིན་དེ་ལ་ཕལ་ཆེར་བསླབ་སྟེ། ང་ཚོས་དམིགས་ཡུལ་དུ་བསམ་བློ་གཏོང་ནི་ཕལ་ཆེར ང་ཚོའི་འཇིག་སྣོད་མི་དགོས་མཁན་དེ་ཁོང་ཚོའི་ཆེད་དུ་ཉེན་ཁ་ཞིག་ཡིན་ཏེ། འདི་ལ་གནད་དོན་གཉིས་ཀྱི་གསལ་བ་གཉིས་ལས་ཁྱད་པར་འདྲ་ཡིན། We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset. ང་ཚོས་ཀྱང་གསལ་གྲངས་སྔོན་འཛིན་གྱི་དབྱེ་རིམ་དཔེ་དབྱེ་བ་གྲངས་ཀ་གསལ་བཤད་ཀྱི་ཡོད།"}
{'en': 'RuSentEval : Linguistic Source, Encoder Force !', 'ar': 'RuSentEval: مصدر لغوي ، قوة التشفير!', 'fr': "RusEnteVal\xa0: Source linguistique, force d'encodeur\xa0!", 'pt': 'RuSentEval: fonte linguística, força do codificador!', 'es': 'RusentEval: ¡Fuente lingüística, fuerza codificadora!', 'ja': 'RuSentEval:言語ソース、エンコーダフォース！', 'zh': 'RuSentEval:语言源,编码器力!', 'hi': 'RuSentEval: भाषाई स्रोत, एनकोडर बल!', 'ru': 'RuSentEval: Лингвистический источник, Сила кодировщика!', 'ga': 'RuSentEval: Foinse Teangeolaíoch, Fórsa Ionchódóra!', 'ka': 'ლთნდსთჟრთფვნ თჱრჲპ, კჲევპჟკა ჟთლა!', 'hu': 'RuSentEval: Nyelvi forrás, kódoló erő!', 'el': 'Γλωσσολογική Πηγή, Δύναμη Κωδικοποιητή!', 'it': 'RuSentEval: Linguistic Source, Encoder Force!', 'kk': 'RuSentEval: Linguistic Source, Encoder Force!', 'mk': 'RuSentEval: Linguistic Source, Encoder Force!', 'ml': 'റൂസെന്\u200dറ് എവാല്\u200d: ലിങ്ഗിസ്റ്റിക്ക് സ്രോതസ്സ്, എന്\u200dകോഡെര്\u200d ഫോര്\u200dസ്!', 'lt': 'RuSentEval: Linguistic Source, Encoder Force!', 'mn': 'RuSentEval: Linguistic Source, Encoder Force!', 'mt': 'RuSentEval: Sors Lingwistiku, Forza tal-Kodifikatur!', 'pl': 'RuSentEval: Źródło językowe, siła kodowania!', 'ro': 'RuSentEval: Sursă lingvistică, Forța Encoder!', 'no': 'RuSentEval: Linguistic Source, Encoder Force!', 'so': 'RuSentEval: Linguistic Source, Encoder Force!', 'si': 'RuSenteval: ලින්ග්යුස්ටික් ප්\u200dරදේශය, එන්කෝඩර් බලය!', 'ms': 'RuSentEval: Sumber Bahasa, Pasukan Pengenkod!', 'ur': 'روسینٹ ایولی: لینگویسٹ سورس، انکوڈر فورس!', 'sr': 'Lingistički izvor, koderska snaga!', 'sv': 'RuSentEval: Linguistic Source, Encoder Force!', 'ta': 'RuSentEval: Linguistic Source, Encoder Force!', 'uz': 'Eval: Linguistic Source, Encoder Force!', 'vi': 'Nguồn ngôn ngữ, Mã hóa.', 'bg': 'RuSentEval: Linguistic Source, Encoder Force!', 'hr': 'Lingistički izvor, koderska snaga!', 'nl': 'RuSentEval: Taalkundige Bron, Encoder Force!', 'da': 'RuSentEval: Linguistic Source, Encoder Force!', 'id': 'RuSentEval: Sumber Bahasa, Pasukan Pengenkodar!', 'sw': 'RuSentEval: Chanzo cha Kilinguistic, Vikosi vya Kufungua!', 'de': 'RuSentEval: Sprachliche Quelle, Encoder Force!', 'ko': '언어 출처, 인코딩 역량!', 'fa': 'روسنتEval: منبع لینگیستیک، نیروی رمزگذاری!', 'af': 'RuSentEval: Linguistic Bron, Encoder Force!', 'sq': 'RuSentEval: Burim gjuhësor, forca e koduesit!', 'tr': 'Comment', 'bn': 'রুসেন্ট ইভাল: লিঙ্গিস্টিক সূত্র, এনকোডার বাহিনী!', 'am': 'Eval: Linguistic source, Encoder Force!', 'hy': 'RuSentewal. լեզվաբանական աղբյուր, կոդավոր ուժ:', 'bs': 'Lingistički izvor, koderska snaga!', 'ca': 'RuSentEval: Font lingüística, Força del codificador!', 'cs': 'RuSentEval: Jazykový zdroj, síla kódování!', 'fi': 'RuSentEval: Linguistic Source, Encoder Force!', 'et': 'RuSentEval: Linguistic Source, Encoder Force!', 'az': 'RuSentEval: Linguistic Source, Encoder Force!', 'jv': 'Language', 'ha': '@ info: status', 'sk': 'RuSentEval: Linguistic Source, Encoder Force!', 'he': 'RuSentEval: מקור לינגיסטי, כוח קודד!', 'bo': 'RuSentEval: སྐད་རིགས་འདྲ་བྱུང་མཁན་དང་ཨིན་ཀོ་ཌིར་བྱེད་ཀྱི་ཡོད།'}
{'en': 'The success of pre-trained transformer language models has brought a great deal of interest on how these models work, and what they learn about language. However, prior research in the field is mainly devoted to English, and little is known regarding other languages. To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for Russian, including ones that have not been explored yet. We apply a combination of complementary probing methods to explore the distribution of various linguistic properties in five multilingual transformers for two typologically contrasting languages   Russian and English. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences.', 'ar': 'أثار نجاح نماذج لغة المحولات المدربة مسبقًا قدرًا كبيرًا من الاهتمام بكيفية عمل هذه النماذج ، وماذا تعلموه عن اللغة. ومع ذلك ، فإن الأبحاث السابقة في هذا المجال مكرسة بشكل أساسي للغة الإنجليزية ، ولا يُعرف سوى القليل عن اللغات الأخرى. تحقيقًا لهذه الغاية ، نقدم RuSentEval ، وهي مجموعة محسّنة من 14 مهمة فحص للروسية ، بما في ذلك المهام التي لم يتم استكشافها بعد. نحن نطبق مجموعة من طرق الفحص التكميلية لاستكشاف توزيع الخصائص اللغوية المختلفة في خمسة محولات متعددة اللغات لغتين متناقضتين نمطياً - الروسية والإنجليزية. تقدم نتائجنا نتائج مثيرة للاهتمام تتعارض مع الفهم الشائع لكيفية تمثيل المعرفة اللغوية ، وتوضح أن بعض الخصائص يتم تعلمها بطريقة مماثلة على الرغم من الاختلافات اللغوية.', 'fr': "Le succès des modèles de langage de transformateur préformés a suscité un vif intérêt sur le fonctionnement de ces modèles et sur ce qu'ils apprennent sur la langue. Cependant, les recherches antérieures dans ce domaine sont principalement consacrées à l'anglais et on sait peu de choses sur les autres langues. À cette fin, nous présentons RusEnteVal, un ensemble amélioré de 14 tâches de sondage pour le russe, y compris celles qui n'ont pas encore été explorées. Nous appliquons une combinaison de méthodes de sondage complémentaires pour explorer la distribution de diverses propriétés linguistiques dans cinq transformateurs multilingues pour deux langues typologiquement opposées\xa0: le russe et l'anglais. Nos résultats fournissent des résultats intrigants qui contredisent la compréhension commune de la représentation des connaissances linguistiques et démontrent que certaines propriétés sont apprises de la même manière malgré les différences linguistiques.", 'es': 'El éxito de los modelos lingüísticos transformadores previamente entrenados ha despertado un gran interés sobre cómo funcionan estos modelos y qué aprenden sobre el lenguaje. Sin embargo, la investigación previa en el campo se dedica principalmente al inglés, y se sabe poco sobre otros idiomas. Con este fin, presentamos RusEnTeval, un conjunto mejorado de 14 tareas de sondeo para el ruso, incluidas las que aún no se han explorado. Aplicamos una combinación de métodos de sondeo complementarios para explorar la distribución de varias propiedades lingüísticas en cinco transformadores multilingües para dos idiomas que contrastan tipológicamente: el ruso y el inglés. Nuestros resultados proporcionan hallazgos interesantes que contradicen la comprensión común de cómo se representa el conocimiento lingüístico y demuestran que algunas propiedades se aprenden de manera similar a pesar de las diferencias lingüísticas.', 'pt': 'O sucesso de modelos de linguagem de transformador pré-treinados trouxe muito interesse sobre como esses modelos funcionam e o que eles aprendem sobre linguagem. No entanto, pesquisas anteriores na área são principalmente dedicadas ao inglês, e pouco se sabe sobre outros idiomas. Para isso, apresentamos o RuSentEval, um conjunto aprimorado de 14 tarefas de sondagem para russo, incluindo aquelas que ainda não foram exploradas. Aplicamos uma combinação de métodos de sondagem complementares para explorar a distribuição de várias propriedades linguísticas em cinco transformadores multilíngues para dois idiomas tipologicamente contrastantes – russo e inglês. Nossos resultados fornecem descobertas intrigantes que contradizem o entendimento comum de como o conhecimento linguístico é representado e demonstram que algumas propriedades são aprendidas de maneira semelhante, apesar das diferenças linguísticas.', 'ja': '事前に訓練された変圧器言語モデルの成功は、これらのモデルがどのように機能し、言語について何を学ぶかに大きな関心をもたらしました。しかし、この分野での先行研究は主に英語に捧げられており、他の言語に関してはほとんど知られていない。この目的のために、RuSentEvalを紹介します。RuSentEvalは、まだ探索されていないものを含む、ロシア語の14の探索タスクの強化されたセットです。私たちは、ロシア語と英語という2つの類型的に対照的な言語のための5つの多言語変換器におけるさまざまな言語特性の分布を探るために、補完的な探索方法の組み合わせを適用します。私たちの結果は、言語学的知識がどのように表現されるかについての共通理解と矛盾する興味深い発見を提供し、言語の違いにもかかわらず、いくつかの特性が同様の方法で学習されることを示しています。', 'zh': '先训变形金刚言成,及其大乐。 然该领之先,究主于英语,于他言之甚少。 为言RuSentEval者,14俄语探事也。 合互补以索二比 ( 俄语与英语 ) 五多言转换器言语之分也。 吾道有趣,与言语相违共识,虽有言语差异,性犹以类学也。', 'hi': 'पूर्व-प्रशिक्षित ट्रांसफॉर्मर भाषा मॉडल की सफलता ने इन मॉडलों के काम करने के तरीके पर बहुत रुचि लाई है, और वे भाषा के बारे में क्या सीखते हैं। हालांकि, क्षेत्र में पूर्व अनुसंधान मुख्य रूप से अंग्रेजी के लिए समर्पित है, और अन्य भाषाओं के बारे में बहुत कम जाना जाता है। इस अंत तक, हम RuSentEval, रूसी के लिए 14 जांच कार्यों का एक बढ़ाया सेट पेश करते हैं, जिसमें वे भी शामिल हैं जिन्हें अभी तक खोजा नहीं गया है। हम दो टाइपोलॉजिकल रूप से विपरीत भाषाओं - रूसी और अंग्रेजी के लिए पांच बहुभाषी ट्रांसफॉर्मर में विभिन्न भाषाई गुणों के वितरण का पता लगाने के लिए पूरक जांच विधियों का एक संयोजन लागू करते हैं। हमारे परिणाम पेचीदा निष्कर्ष प्रदान करते हैं जो भाषाई ज्ञान का प्रतिनिधित्व करने के तरीके की सामान्य समझ के विपरीत हैं, और यह प्रदर्शित करते हैं कि भाषा के मतभेदों के बावजूद कुछ गुणों को समान तरीके से सीखा जाता है।', 'ru': 'Успех предварительно обученных моделей языков трансформаторов вызвал большой интерес к тому, как работают эти модели и что они узнают о языке. Однако предыдущие исследования в этой области в основном посвящены английскому языку, и мало что известно о других языках. С этой целью мы представляем RuSentEval - расширенный набор из 14 зондирующих задач для российских, в том числе еще не исследованных. Мы применяем комбинацию комплементарных методов зондирования для исследования распределения различных лингвистических свойств в пяти многоязычных трансформаторах для двух типологически контрастных языков – русского и английского. Наши результаты дают интригующие выводы, которые противоречат общему пониманию того, как языковые знания представлены, и демонстрируют, что некоторые свойства изучаются аналогичным образом, несмотря на языковые различия.', 'ga': 'Chuir rath na múnlaí teanga claochladán réamhoilte go mór an-suim sa chaoi a n-oibríonn na samhlacha seo, agus ar an méid a fhoghlaimíonn siad faoin teanga. Mar sin féin, tá an taighde a rinneadh cheana sa réimse dírithe go príomha ar an mBéarla, agus is beag atá ar eolas faoi theangacha eile. Chuige sin, tugaimid isteach RuSentEval, sraith fheabhsaithe de 14 thasc iniúchta don Rúisis, lena n-áirítear cinn nach bhfuil iniúchadh déanta orthu go fóill. Cuirimid meascán de mhodhanna comhlántacha taiscéalaíochta i bhfeidhm chun iniúchadh a dhéanamh ar dháileadh airíonna teangeolaíocha éagsúla i gcúig chlaochladán ilteangach do dhá theanga atá codarsnachta ó thaobh na clódóireachta de – Rúisis agus Béarla. Soláthraíonn ár gcuid torthaí torthaí suimiúla a thagann salach ar an gcomhthuiscint ar an gcaoi a léirítear eolas teangeolaíoch, agus a léiríonn go bhfoghlaimítear airíonna áirithe ar an mbealach céanna in ainneoin na ndifríochtaí teanga.', 'ka': 'პრეტრანსტრინსტრიქტური ენის მოდელების წარმატება ძალიან ინტერესტის შესახებ როგორ ეს მოდელები მუშაობენ, და როგორ ისინი ვისწავლებენ ენის შესახებ. მაგრამ, პირველი პასუხში ინგლისური კონფიგურაცია უფრო მნიშვნელოვანია, და სხვა ენების შესახებ პატარა უცნობია. ამ მიზეზით, ჩვენ RuSentEval-ს ჩვენ გავიყენებთ, რომელიც 14 პროცესური მოწყობილობა დავამუშავებული საქმედები, რომელიც უკვე არ განსხვავებულია. ჩვენ კომპლენტერიური პრობენტის კომპლენტერიური პრობენტის კომპლენტერიური პრობენტის კომპლენტების კომპლენტების კომპლენტების კომპლენტების კომპლენტებ ჩვენი წარმოდგენების შესაძლებლობები გააკეთება, რომლებიც საერთო სხვადასხვა ინგლიგური ცოდნიერების განსაკუთრებების განსაკუთრებების განსაკუთრებების განსაკუთრებების განსაკუთრებების განსაკ', 'hu': 'Az előre képzett transzformátor nyelvi modellek sikere nagy érdeklődést keltett arra, hogy ezek a modellek hogyan működnek és mit tanulnak a nyelvről. Azonban a területen végzett korábbi kutatások elsősorban az angol nyelvről szólnak, és más nyelvekről keveset tudunk. Ebből a célból bemutatjuk a RuSentEval-t, amely egy továbbfejlesztett 14 szondázási feladatot tartalmaz orosz számára, beleértve azokat is, amelyeket még nem fedeztek fel. Kiegészítő mérési módszerek kombinációját alkalmazzuk a különböző nyelvi tulajdonságok eloszlásának vizsgálatára öt többnyelvű transzformátorban két tipológiailag kontrasztos nyelven - orosz és angol. Eredményeink érdekes megállapításokat szolgáltatnak, amelyek ellentmondanak a nyelvtudás ábrázolásának közös megértésével, és azt mutatják, hogy bizonyos tulajdonságok hasonló módon tanulhatók a nyelvi különbségek ellenére.', 'el': 'Η επιτυχία των προ-εκπαιδευμένων μοντέλων γλώσσας μετασχηματιστών έφερε μεγάλο ενδιαφέρον για το πώς λειτουργούν αυτά τα μοντέλα και τι μαθαίνουν για τη γλώσσα. Ωστόσο, η προηγούμενη έρευνα στον τομέα είναι κυρίως αφιερωμένη στα αγγλικά, και λίγα είναι γνωστά σχετικά με άλλες γλώσσες. Για το σκοπό αυτό, εισάγουμε το ένα ενισχυμένο σύνολο τεσσάρων εργασιών ανίχνευσης για τη ρωσική, συμπεριλαμβανομένων εκείνων που δεν έχουν διερευνηθεί ακόμα. Εφαρμόζουμε έναν συνδυασμό συμπληρωματικών μεθόδων ανίχνευσης για να διερευνήσουμε την κατανομή διαφόρων γλωσσικών ιδιοτήτων σε πέντε πολύγλωσσους μετασχηματιστές για δύο τυπολογικά αντίθετες γλώσσες, τη ρωσική και την αγγλική. Τα αποτελέσματά μας παρέχουν συναρπαστικά ευρήματα που έρχονται σε αντίθεση με την κοινή κατανόηση του πώς αναπαρίσταται η γλωσσική γνώση και αποδεικνύουν ότι ορισμένες ιδιότητες μαθαίνονται με παρόμοιο τρόπο παρά τις γλωσσικές διαφορές.', 'it': "Il successo dei modelli linguistici pre-formati dei trasformatori ha suscitato grande interesse sul funzionamento di questi modelli e su ciò che imparano sulla lingua. Tuttavia, le ricerche precedenti nel campo sono principalmente dedicate all'inglese, e poco si sa per quanto riguarda le altre lingue. A tal fine, presentiamo RuSentEval, una serie migliorata di 14 compiti di ricerca per il russo, inclusi quelli che non sono stati ancora esplorati. Applichiamo una combinazione di metodi di sondaggio complementari per esplorare la distribuzione di varie proprietà linguistiche in cinque trasformatori multilingui per due lingue tipologicamente contrastanti - russo e inglese. I nostri risultati forniscono risultati intriganti che contraddicono la comprensione comune di come la conoscenza linguistica è rappresentata, e dimostrano che alcune proprietà vengono apprese in modo simile nonostante le differenze linguistiche.", 'lt': 'Iš anksto parengtų kalbų transformatorių modelių sėkmė sukėlė didelį susidomėjimą, kaip šie modeliai veikia ir ką jie mokosi apie kalbą. Tačiau ankstesni moksliniai tyrimai šioje srityje daugiausia skiriami anglų kalbai, o apie kitas kalbas mažai žinoma. Šiuo tikslu pristatysime RuSentEval, sustiprintą 14 rusų tyrimo užduočių rinkinį, įskaitant tuos, kurie dar nebuvo ištirti. Taikome papildomų tyrimo metodų derinį, kad ištirtume įvairių kalbinių savybių pasiskirstymą penkiose daugiakalbėse transformatoriuose dviem tipologiškai kontrastinėmis kalbomis - rusų ir anglų. Mūsų rezultatai suteikia įdomių išvadų, prieštaraujančių bendram supratimui apie kalbinių žinių atstovavimą, ir rodo, kad kai kurios savybės mokomos panašiai, nepaisant kalbų skirtumų.', 'mk': 'Успехот на предобучените јазички модели на трансформатори доведе голем интерес за тоа како овие модели функционираат и за тоа што тие научуваат за јазикот. Сепак, претходното истражување на теренот е посветено главно на англиски, а малку е познато во врска со другите јазици. За ова, го претставуваме RuSentEval, засилен сет на 14 истражувачки задачи за Русите, вклучително и оние кои сé уште не се истражувани. Ние аплицираме комбинација на комплементарни методи на истражување за истражување на дистрибуцијата на различни јазични сопствености во пет мултијазични трансформатори за два типологички контрастни јазици - руски и англиски. Нашите резултати обезбедуваат интересни откритија кои се спротивставуваат на заедничкото разбирање како е претставено јазичкото знаење и покажуваат дека некои имоти се научуваат на сличен начин и покрај разликите во јазикот.', 'kk': 'Бұл үлгілер қалай жұмыс істейді және олардың тіл туралы оқыту үшін бірнеше қызықты түсіндіреді. Бірақ бұл өрістің алдындағы зерттеулері негізінде ағылшын тіліне аударылады, және басқа тілдер туралы білмейді. Бұл үшін біз RuSentEval дегенді түсіндіреміз. Осылық үшін 14 сынақ тапсырмаларын түсіндіреміз, сондай-ақ тек зерттелмеген. Біз бес тілді көптілік түрлендірушілерде екі типтологиялық қарсы тілдерге - руссия және ағылшын тілдер үшін түрлендірушілердің түрлендіру әдістерін қолданамыз. Біздің нәтижелеріміз лингвистикалық білімдердің қалай түсініктеріне қарсы түсініктеріне қарсы болып, кейбір қасиеттер тілдердің айырмашылығына қарамастырып, ұқсас түрде оқылған.', 'ms': 'Keberjalan model bahasa pengubah terlatih telah membawa banyak minat tentang bagaimana model ini berfungsi, dan apa yang mereka belajar tentang bahasa. Namun, kajian sebelumnya di lapangan adalah terutama dedikasi kepada bahasa Inggeris, dan sedikit diketahui mengenai bahasa lain. Untuk tujuan ini, kami memperkenalkan RuSentEval, satu set meningkat 14 tugas penyelidikan untuk Rusia, termasuk tugas yang belum dikenalpasti lagi. Kami melaksanakan kombinasi kaedah penyelidikan tambahan untuk mengeksplorasi distribusi beberapa sifat bahasa dalam lima pengubah berbilang bahasa untuk dua bahasa yang bertentangan tipologi - Rusia dan Inggeris. Hasil kami memberikan penemuan menarik yang bertentangan dengan pemahaman umum bagaimana pengetahuan bahasa diwakili, dan menunjukkan bahawa beberapa ciri-ciri belajar dengan cara yang sama walaupun perbezaan bahasa.', 'ml': 'മുമ്പ് പരിശീലന മാറ്റങ്ങളുടെ ഭാഷ മോഡലുകളുടെ വിജയം ഈ മോഡലുകള്\u200d എങ്ങനെ ജോലി ചെയ്യുന്നുവെന്നും ഭാഷ കുറിച്ചും പഠിക എന്നാലും പ്രദേശത്തിലെ പഠനത്തിനു മുമ്പ് പ്രധാനമായി ഇംഗ്ലീഷിലേക്ക് പ്രത്യേകിച്ചിരിക്കുന്നു. മറ ഈ അവസാനത്തിനു വേണ്ടി ഞങ്ങള്\u200d റൂസെന്\u200dറ് എവാലിനെ പരിചയപ്പെടുത്തുന്നു. റഷ്യന്\u200dറെ ഒരു 14 കൂട്ടം മെച്ചപ്പെടുത്തിയ ജോലികള്\u200d, ഇതു അഞ്ച് മള്\u200dട്ടില്\u200d മാറ്റങ്ങളില്\u200d വിതരണം ചെയ്യുന്നതിന് വേണ്ടി നമ്മള്\u200d ഒരു കൂട്ടിച്ചേര്\u200dക്കുന്ന കണക്ടറി പരീക്ഷിക്കുന്ന രീതികള്\u200d പ്രയോഗി നമ്മുടെ ഫലങ്ങള്\u200d അത്ഭുതപ്പെടുത്തുന്ന കണ്ടുപിടികള്\u200d കൊണ്ട് വരുന്നു. ഭാഷ വ്യത്യാസം എങ്ങനെയാണ് പ്രതിനിധിക്കപ്പെടുന്നതെന്നും ഭാഷ വ്യത്', 'mt': 'The success of pre-trained transformer language models has brought a great deal of interest on how these models work, and what they learn about language.  Madankollu, ir-riċerka preċedenti fil-qasam hija ddedikata prinċipalment għall-Ingliż, u ftit huwa magħruf dwar lingwi oħra. Għal dan il-għan, aħna nintroduċu RuSentEval, sett imtejjeb ta’ 14-il kompitu ta’ sondaġġ għar-Russu, inklużi dawk li għadhom ma ġewx esplorati. Aħna napplikaw taħlita ta’ metodi komplementari ta’ sondaġġ biex tiġi esplorata d-distribuzzjoni ta’ karatteristiċi lingwistiċi varji f’ħames trasformaturi multilingwi għal żewġ lingwi tipoloġikament kuntrastanti - ir-Russu u l-Ingliż. Ir-riżultati tagħna jipprovdu sejbiet intriganti li jikkontradixxu l-fehim komuni ta’ kif l-għarfien lingwistiku huwa rappreżentat, u juru li xi karatteristiċi jitgħallmu b’mod simili minkejja d-differenzi lingwistiċi.', 'mn': 'Өмнөх сургалтын өөрчлөлтийн хэл загварын амжилт нь эдгээр загваруудын хэрхэн ажилладаг талаар, хэлний талаар юу сурах талаар маш их сонирхолтой болсон. Гэхдээ салбарын өмнөх судалгаа англи хэлний хувьд ихэвчлэн англи хэлний хувьд зориулагддаг. Өөр хэлний хувьд бага ч мэддэг. Энэ төгсгөлд бид RuSentEval-г Оросын 14 судалгааны даалгавар дээр танилцуулсан. Харин одоо судалгаагүй хүмүүс ч мөн адил. Бид хэл хэлний өөрчлөлтийг таван хэлний шилжүүлэгчид хоёр типтологийн эсрэг хэл болон Англи хэлний хуваарьт судалгааны нэмэлт судалгааны аргыг ашиглаж байна. Бидний үр дүнд хэлний мэдлэг хэрхэн илэрхийлж байгааг харуулж, хэлний ялгаатай ч зарим өөрчлөлт нь адилхан аргаар суралцдаг гэдгийг харуулж байна.', 'ro': 'Succesul modelelor lingvistice pre-instruite a adus un mare interes asupra modului în care funcționează aceste modele și a ceea ce învață despre limbă. Cu toate acestea, cercetarea anterioară în domeniu este dedicată în principal engleză, și puțin se știe despre alte limbi. În acest scop, vă prezentăm RuSentEval, un set îmbunătățit de 14 sarcini de sondare pentru ruși, inclusiv cele care nu au fost explorate încă. Aplicăm o combinație de metode complementare de sondare pentru a explora distribuția diferitelor proprietăți lingvistice în cinci transformatoare multilingve pentru două limbi contrastante tipologic - rusă și engleză. Rezultatele noastre oferă descoperiri interesante care contrazic înțelegerea comună a modului în care sunt reprezentate cunoștințele lingvistice și demonstrează că unele proprietăți sunt învățate în mod similar în ciuda diferențelor lingvistice.', 'pl': 'Sukces wstępnie przeszkolonych modeli językowych transformatorów wzbudził duże zainteresowanie tym, jak te modele działają i czego się uczą o języku. Jednak wcześniejsze badania w tej dziedzinie poświęcone są głównie angielskiemu, a niewiele wiadomo o innych językach. W tym celu wprowadzamy RuSentEval, rozszerzony zestaw czternastu zadań sondujących dla Rosji, w tym te, które jeszcze nie zostały zbadane. Stosujemy kombinację uzupełniających się metod sondowania, aby zbadać rozkład różnych właściwości językowych w pięciu wielojęzycznych transformatorach dla dwóch typologicznie kontrastujących języków: rosyjskiego i angielskiego. Nasze wyniki dostarczają intrygujących odkryć, które sprzecznie z powszechnym zrozumieniem, jak reprezentowana jest wiedza językowa i pokazują, że niektóre właściwości uczą się w podobny sposób pomimo różnic językowych.', 'no': 'Det første transformeringsspråk-modellen har ført mykje interesse på korleis disse modelane fungerer, og kva dei lærer om språk. Førre forskning i feltet er imidlertid spesifisert til engelsk, og lite er kjent om andre språk. I denne slutten introdusere vi RuSentEval, eit forbetra sett av 14 proberingsoppgåver for russisk, inkludert dei som ikkje er utforska enno. Vi bruker ein kombinasjon av komplementære proberingsmetodar for å utforske distribusjonen av ulike lingviske eigenskapar i fem fleirspråk transformerande for to typologisk kontrastspråk – russisk og engelsk. Resultatet våre gjev interessante oppdagar som mottrykkjer den felles forståelse av korleis lingviske kunnskap vert representert, og demonstrerer at nokre eigenskapar er lært på ein liknande måte, selv om språk-forskjeller.', 'sr': 'Uspjeh predobučenih transformacijskih jezičkih modela dovodio je mnogo interesa o tome kako ovi modeli funkcionišu i o tome šta nauče o jeziku. Međutim, prethodno istraživanje na terenu je uglavnom posvećeno engleskom, a malo je poznato u vezi drugih jezika. Za taj cilj predstavljamo RuSentEval, povećan set od 14 zadataka za istraživanje Rusa, uključujući one koje još nisu istraživali. Primjenjujemo kombinaciju komplementarnih metoda istraživanja distribucije različitih jezičkih vlasništva u pet multijezičkih transformatora za dva tipološki kontrastveni jezika - ruski i engleski. Naši rezultati pružaju zanimljive nalaze koje se suprotstavljaju zajedničkom razumijevanju kako je jezičko znanje predstavljeno, i pokazuju da su neke vlasništvo naučene na sličan način uprkos jezičkim razlikama.', 'si': 'මුලින් ප්\u200dරශ්නයක් වෙන්න පුළුවන් භාෂා මොඩේල් වැඩ කරන්න පුළුවන් විදියට ගොඩක් ආශ්වාසයක් තියෙනවා මේ මො නමුත්, කලින් පරීක්ෂණයක් තියෙන්නේ ඉංග්\u200dරීසි වලට ප්\u200dරධාන විශ්වාස කරනවා, ඒ වගේම අනිත් භාෂ මේ අවසානයෙන්, අපි රුසෙන්ට් එව්ල්ව ප්\u200dරදානය කරනවා, රුසියානුවෙන් පරීක්ෂණාකරණ වැඩ 14ක් විශ්වාස කරනවා, තවම පර අපි විවිධ භාෂාවික විශේෂතාවක් පහත් භාෂාවික විශේෂ කරන්න සම්බන්ධ විශේෂ විශේෂයක් සම්බන්ධ කරනවා වගේ භාෂ අපේ ප්\u200dරතිචාර දේවල් ප්\u200dරශ්නයක් තියෙනවා කියලා භාෂාවික දැනගන්නේ කොහොමද කියලා ප්\u200dරශ්නයක් කරනවා කියලා, සමහර විශේෂතා', 'sv': 'Framgången med färdigutbildade transformatorspråkmodeller har väckt stort intresse för hur dessa modeller fungerar och vad de lär sig om språk. Tidigare forskning inom området ägnas dock främst åt engelska, och lite är känt om andra språk. För detta ändamål introducerar vi RuSentEval, en utökad uppsättning av 14 sonderingsuppgifter för ryska, inklusive sådana som ännu inte har utforskats. Vi använder en kombination av kompletterande sondmetoder för att utforska fördelningen av olika språkliga egenskaper i fem flerspråkiga transformatorer för två typologiskt kontrasterande språk - ryska och engelska. Våra resultat ger spännande fynd som motsäger den gemensamma förståelsen av hur språklig kunskap representeras och visar att vissa egenskaper lärs på liknande sätt trots språkskillnaderna.', 'so': 'Liibadii samooyinka isbedelka afka hore ee horay lagu tababariyey waxay keentay xiiso badan oo ku saabsan sida modelladan u shaqeeyaan iyo waxa ay ku bartaan luuqada. Si kastaba ha ahaatee baaritaanka hore ee duurka waxaa loogu talogalay ingiriiska, wax yarna waxaa lagu yaqaanaa luuqado kale. Taas darteed waxaannu soo bandhignaa RuSentEval oo ah 14 shaqooyin aad u kordhisan oo Ruush ah, kuwaas oo ah kuwo aan weli la baadhay. Waxaannu u dalbannaa qalabka imtixaanka iskuulka ah si aan ugu baarayno qaybinta hantida luuqadaha kala duduwan shan isbedelka oo luuqadaha kala duduwan laba luuqadood oo si caadi ah u kala duwan luqada- Ruush iyo Ingiriis. Sababyadayada waxaa bixiya arimo xiiseysan oo ka gees ah garashada caadiga ah sida aqoonta luuqada looga jeedo, waxayna muujiyaan in xoolaha qaarkood lagu barto si isku mid ah, haba yeeshee kala duwan luuqada.', 'ur': 'پہلے تدریس کی تغییر کی زبان مدل کی موفقیت نے بہت زیادہ علاقه پیدا کی ہے کہ یہ مدل کس طرح کام کرتے ہیں اور ان کی زبان کے بارے میں کیا سیکھتے ہیں۔ لیکن مکان میں پہلے کی تحقیقات انگلیسی کے لئے مطابق ہے، اور بہت ہی کم لوگ دوسری زبانوں کے بارے میں جانتے ہیں۔ اس کے لئے ہم روسین ٹیوال کو معرفی کرتے ہیں، روسی کے لئے 14 پرینڈنگ کام کا ایک بڑھایا مجموعہ ہے، جو اب تک اچھی طرح نہیں کی گئی ہیں۔ ہم پیچھے زبان کی مختلف اختلافات کا تقسیم کرنے کے لئے پینچ زبان کی مختلف تبدیل کرنے والوں کے لئے دو ٹیپولوژیکی زبانوں کے مقابلہ میں استعمال کرتے ہیں - روسی اور انگلیسی زبانوں کے لئے۔ ہمارے نتیجے ان باتوں کی مخالفت کرتی ہیں جو زبان علم کی تعریف کیسی ہے اور دکھاتے ہیں کہ بعض اختلاف کی تعریف ایسی طریقہ سے سیکھی جاتی ہیں۔', 'ta': 'முன் பயிற்சி மொழி மாற்ற மாதிரிகளின் வெற்றியம் இந்த மாதிரிகள் எப்படி வேலை செய்கிறார்கள் என்பதைப் பற்றியும் மொழிய ஆனால், புலத்தில் முன்னால் ஆராய்ச்சி முக்கியமாக ஆங்கிலத்திற்கு தேவைப்படுகிறது, மற்றும் மற்ற மொழிகள இந்த முடிவிற்கு, நாம் ரூசென்ட்வெல், ஒரு 14 மேம்படுத்தப்பட்ட பணிகளை அறிமுகப்படுத்துகிறோம், இன்னும் தீர்வு செய்யப்படவில்லை  ஐந்து மொழி மாற்றங்களில் பங்கிடும் பல மொழி மாற்றங்களில் இரண்டு வழக்கமான மொழிகளுக்கு ருஷ்யன் மற்றும் ஆங்கிலத்திற்கான மொழிகளுக்கு நாம்  எங்கள் முடிவுகள் மொழி வித்தியாசமான மொழியின் வித்தியாசமான அறிவு எவ்வாறு குறிப்பிடுகிறது என்று பொதுவான தெரிவுகளை கொடுக்கும் மொ', 'uz': "Taʼminlovchi o'zgartirish modellari muvaffaqiyatli bu modellar qanday ishlaydigan va ular tilning haqida o'rganishlari haqida juda qiziqarli bo'lgan. Lekin maydondagi birinchi o'rganishdan avval ingliz tilga tayyorlangan va boshqa tillar haqida qisqa bilmagan. Shunday qilib, biz RuSentEval orqali ko'proq RusentEval orqali ko'proq orqali ko'proq orqali ko'proq qilamiz, va bu yerda o'ylab topilmagan narsalarni ko'rsamiz. Biz bir necha tildagi o'zgarishlarda ikkita odatda o'zgarishni qo'llab-qo'llash uchun bir komplimentar tizimni qo'llayapmiz. Ruscha va ingliz tilida har hil tillarda har xil xossalarni ajratish uchun. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences.", 'vi': 'Sự thành công của các mô hình ngôn ngữ biến đổi được huấn luyện đã khiến cho nhiều người quan tâm đến cách làm việc của các mô hình này, và những gì họ học về ngôn ngữ. Tuy nhiên, nghiên cứu trước trong lĩnh vực này chủ yếu dành cho tiếng Anh, và ít được biết về các ngôn ngữ khác. Để đạt được mục đích, chúng tôi xin giới thiệu RuffEvl, một loạt các nhiệm vụ thăm dò phát triển cho người Nga, bao gồm những nhiệm vụ chưa được khám phá. Chúng tôi sử dụng một loạt các phương pháp thăm dò bổ sung để khám phá sự phân phối các đặc tính ngôn ngữ khác nhau trong năm máy biến đổi đa dạng cho hai ngôn ngữ khác nhau (kiểu Nga) và Anh. Những kết quả của chúng tôi đưa ra những kết quả hấp dẫn trái với sự hiểu chung về cách đại diện kiến thức ngôn ngữ, và chứng minh rằng một số tài sản được học theo một cách tương tự, bất chấp sự khác biệt ngôn ngữ.', 'bg': 'Успехът на предварително обучените трансформаторни езикови модели донесе голям интерес за това как работят тези модели и какво научават за езика. Въпреки това, предишните изследвания в областта са посветени главно на английски, и малко е известно по отношение на други езици. За тази цел представяме разширен набор от 14 пробни задачи за руски език, включително и такива, които все още не са проучени. Прилагаме комбинация от допълващи се методи за изследване на разпределението на различни езикови свойства в пет многоезични трансформатора за два типологично контрастиращи езика - руски и английски. Нашите резултати предоставят интригуващи открития, които противоречат на общото разбиране за това как езиковото знание е представено и демонстрират, че някои свойства се научават по подобен начин въпреки езиковите различия.', 'da': 'Succesen med forududdannede transformersprogsmodeller har bragt stor interesse for, hvordan disse modeller fungerer, og hvad de lærer om sprog. Tidligere forskning på området er imidlertid hovedsagelig dedikeret til engelsk, og der vides lidt om andre sprog. Til dette formål introducerer vi RuSentEval, et forbedret sæt af 14 sondeopgaver for russisk, herunder dem, der endnu ikke er blevet udforsket. Vi anvender en kombination af supplerende sondemetoder til at undersøge fordelingen af forskellige sproglige egenskaber i fem flersprogede transformatorer til to typologisk kontrasterende sprog - russisk og engelsk. Vores resultater giver spændende resultater, der modsiger den fælles forståelse af, hvordan sproglig viden repræsenteres, og viser, at nogle egenskaber læres på samme måde trods sprogforskellene.', 'de': 'Der Erfolg von vortrainierten Transformator-Sprachmodellen hat großes Interesse daran geweckt, wie diese Modelle funktionieren und was sie über Sprache lernen. Bisherige Forschung auf diesem Gebiet widmet sich jedoch hauptsächlich dem Englischen, und über andere Sprachen ist wenig bekannt. Zu diesem Zweck stellen wir RuSentEval vor, ein erweitertes Set von 14-Sondierungsaufgaben für Russisch, einschließlich solcher, die noch nicht erforscht wurden. Wir verwenden eine Kombination komplementärer Sondierungsmethoden, um die Verteilung verschiedener linguistischer Eigenschaften in fünf mehrsprachigen Transformatoren für zwei typologisch gegensätzliche Sprachen – Russisch und Englisch – zu untersuchen. Unsere Ergebnisse liefern faszinierende Erkenntnisse, die dem allgemeinen Verständnis der Repräsentation von sprachlichem Wissen widersprechen und zeigen, dass einige Eigenschaften trotz der sprachlichen Unterschiede in ähnlicher Weise erlernt werden.', 'nl': 'Het succes van voorgetrainde transformatortaalmodellen heeft veel belangstelling gewekt voor hoe deze modellen werken en wat ze leren over taal. Eerder onderzoek op dit gebied is echter voornamelijk gewijd aan het Engels, en er is weinig bekend over andere talen. Daartoe introduceren we RuSentEval, een uitgebreide set van 14-sondetaken voor Russisch, inclusief taken die nog niet zijn onderzocht. We passen een combinatie van complementaire sonderingsmethoden toe om de verdeling van verschillende linguïstische eigenschappen in vijf meertalige transformatoren te onderzoeken voor twee typologisch contrasterende talen: Russisch en Engels. Onze resultaten leveren intrigerende bevindingen op die in tegenspraak zijn met het algemeen begrip van de manier waarop taalkennis wordt vertegenwoordigd, en tonen aan dat sommige eigenschappen ondanks de taalverschillen op dezelfde manier worden geleerd.', 'ko': '미리 훈련된transformer 언어 모델의 성공은 이러한 모델이 어떻게 작동하는지, 그리고 언어에 대한 이해에 큰 흥미를 가지게 했다.그러나 그동안 이 분야의 연구는 주로 영어에 집중됐지만 다른 언어에 대한 연구는 드물었다.이를 위해 러시아인을 위한 증강형 14개 탐사 임무인 루센트 에벌을 소개합니다. 아직 탐지하지 않은 임무를 포함합니다.우리는 상호 보완 탐지 방법을 결합하여 러시아어와 영어라는 두 가지 서로 다른 유형의 언어의 다섯 가지 다언어 변형금강 중의 각종 언어 속성의 분포를 탐색했다.우리의 연구 결과는 언어 지식이 어떻게 표현하는지에 대한 공감대와 모순되는 흥미로운 발견을 제공했고 언어 차이가 존재하지만 일부 속성은 유사한 방식으로 학습된 것으로 나타났다.', 'fa': 'موفقیت مدلهای زبان تغییر دهنده پیش آموزش داده شده بسیار علاقه ای به چگونه این مدلها کار می کنند، و آنچه در مورد زبان یاد می گیرند. با این حال، تحقیقات قبلی در زمینه در اصل به انگلیسی تعلق دارد و در مورد زبانهای دیگر کمی شناخته می شود. برای این قسمت، ما روسنت Eval را معرفی می کنیم، یک مجموعه بیشتر از 14 کار تحقیق برای روسیه، شامل کسانی که هنوز تحقیق نشده اند. ما یک ترکیب از روش تحقیقات اضافه برای تحقیق توزیع گونه\u200cهای زبان\u200cشناسی در پنج تغییردهنده\u200cهای متعدد زبان\u200cشناسی برای دو زبان\u200cهای متفاوت\u200cشناسی نوع\u200cشناسی - روسیه و انگلیسی استفاده می\u200cکنیم. نتیجه\u200cهایمان نتیجه\u200cهای جالبی را پیشنهاد می\u200cدهند که با وجود تفاوت زبان\u200cهای مشترک درباره\u200cی دانش زبان\u200cشناسی چگونه نمایش داده می\u200cشود، و نشان می\u200cدهند که بعضی از ویژه\u200cها با وجود تفاوت زبان\u200cها به', 'id': 'Sukses dari model bahasa transformer yang terlatih telah membawa banyak minat tentang bagaimana model ini bekerja, dan apa yang mereka belajar tentang bahasa. Namun, penelitian sebelumnya di lapangan ini terutama didedikasikan kepada bahasa Inggris, dan sedikit dikenal mengenai bahasa lain. Untuk tujuan ini, kami memperkenalkan RuSentEval, sebuah set meningkat 14 tugas penyelidikan untuk Rusia, termasuk tugas yang belum dikeksplorasi. Kami menerapkan kombinasi metode pengujian komplementari untuk mengeksplorasi distribusi berbagai properti bahasa dalam lima transformator berbagai bahasa untuk dua bahasa tipologis yang bertentangan - Rusia dan Inggris. Hasil kami menyediakan penemuan menarik yang bertentangan dengan pemahaman umum tentang bagaimana pengetahuan bahasa diwakili, dan menunjukkan bahwa beberapa properti belajar dengan cara yang sama meskipun perbedaan bahasa.', 'hr': 'Uspjeh predobučenih transformacijskih jezičkih modela dovodio je mnogo interesa o tome kako ovi modeli rade i o tome što uče o jeziku. Međutim, prije istraživanja na terenu je uglavnom posvećena engleskom, a malo je poznato u vezi drugih jezika. Za taj cilj predstavljamo RuSentEval, poboljšan set od 14 zadataka za istraživanje Rusa, uključujući one koje još nisu istraživali. Primjenjujemo kombinaciju dodatnih metoda ispitivanja za istraživanje distribucije različitih jezičkih vlasništva u pet multijezičkih transformatora za dva tipološki suprotnog jezika - ruski i engleski. Naši rezultati pružaju zanimljive nalaze koje se suprotstavljaju zajedničkom razumijevanju kako se predstavljaju jezički znanje, i pokazuju da su neki vlasnici učeni na sličan način uprkos jezičkim razlikama.', 'tr': "Öň bilim öňünden üýtgetmeli dil nusgalarynyň başarnygy bu nusgalaryň nähili işleýändigini we dilleri barada öwrenenlerini örän gyzyklandyrdy. Ýöne bu sahada öňki araştyrmalar iñlis diline adatdyr we başga diller barada az tanalýar. Şonuň üçin RuSentEval'i tanyşdyrýarys, we şu wagt öňünde hiç gözlenmedikleri Rus üçin 14 sany barlamak işi bilen tanyşdyryldyk. Biz beş dilli çeşitli diller üçin diňleýän dillerde, Rusça we Iňlisçe çeşitli diller üçin täsirleýän bir süpürler metodlaryny gözlemek üçin üýtgedik. Biziň netijelerimiz dil bilgileriniň nähili täze bir şekilde öwrenmeli bolandygyny we dil üýtgeşiklerine görä birnäçe häsiýetlerin bir şekilde öwrenmeli diýip täsirleýär.", 'am': 'የቀድሞው የቋንቋ ምሳሌዎች የደረሰለት ስኬት የዚህ ምሳሌዎች እንዴት እንደሚሠራ እና ስለ ቋንቋ የሚማሩትን እጅግ የሚጠቅምበት ማድረግ አመጣላቸው፡፡ ነገር ግን የሜዳ ምርመራ አብዛኛውን ለእንግሊዝኛ የተጠቃሚ ነው፤ ለሌሎች ቋንቋዎች ግን ጥቂት ነው፡፡ ለዚህ ምክንያት፣ ገና ያልተመረመረውን የሩስታንትEval 14 ተጨማሪ ስራዎችን እናስታውቃለን፡፡ በአምስት በብዙ ቋንቋ ለውጦች ላይ ለሁለት ተቃውሞ ቋንቋዎች - ሩሽና እንግሊዘኛ ለመከላከል የሚደረገውን አካባቢ ፈተናዎችን ለመከላከል እናደርጋለን፡፡ ፍሬዎቻችን የቋንቋ እውቀት እንዴት እንደምታሳየው የቋንቋ እውቀት የሚቃወሙትን እና አንዳንዶቹ ባለሥርዓቶች በተለያዩ ልዩነት ቢተማሩ በሚያሳየው ግንኙነት ያሳያል፡፡', 'sw': 'Mafanikio ya mifano ya mabadiliko ya lugha ya zamani yamesababisha maslahi mengi ya namna mifano hii inavyofanya kazi, na kile wanachojifunza kuhusu lugha. Hata hivyo, utafiti wa zamani umejikita kwa Kiingereza, na ni kidogo sana unafahamika kuhusu lugha nyingine. Kwa mwisho huu, tunautambulisha RuSentEval, mfululizo wa shughuli 14 zinazoonyesha Urusi, ikiwa ni pamoja na wale ambao bado hawajadiliwa. Tunatumia muunganiko wa njia za kuchunguza za utaratibu wa lugha mbalimbali katika mabadiliko matano ya lugha kwa lugha mbili zinazotofautiana na lugha mbili - Urusi na Kiingereza. Matokeo yetu yanaleta matokeo ya kusisimua kwamba kinyume na uelewa wa namna maarifa ya lugha inavyowakilishwa, na kuonyesha kwamba baadhi ya utajiri hujifunza kwa namna sawa na pamoja na tofauti za lugha.', 'hy': 'Նախապատրաստված լեզվի վերափոխողների մոդելների հաջողությունը շատ հետաքրքրություն է բերել այն մասին, թե ինչպես են այս մոդելները աշխատում և ինչ են նրանք սովորում լեզվի մասին: However, prior research in the field is mainly devoted to English, and little is known regarding other languages.  Այս նպատակով, մենք ներկայացնում ենք RuSentewal-ը, որն ունի 14 ուսումնասիրություններ Ռուսաստանի համար, ներառյալ այն, որոնք դեռ չեն ուսումնասիրել: Մենք կիրառում ենք բազմաթիվ լեզվաբանական հատկությունների բաշխման ուսումնասիրելու համար բազմաթիվ լեզվաբանական փոխակերպողների հինգ տարբեր լեզվով երկու տիպոլոգիապես հակադրող լեզուների՝ ռուսերենի և անգլերենի համար համադրված մեթո Մեր արդյունքները ներկայացնում են հետաքրքիր եզրակացություններ, որոնք հակադրում են լեզվաբանական գիտելիքների ներկայացման ընդհանուր հասկացությունը և ցույց են տալիս, որ որոշ հատկություններ սովորվում են նման կերպ, չնայած լեզվաբանական տարբերություններին', 'bn': 'প্রথম প্রশিক্ষিত ভাষা পরিবর্তন মডেলের সফলতা এই মডেল কিভাবে কাজ করে এবং ভাষার ব্যাপারে তারা কি শিখে তা নিয়ে বেশী আগ্রহী  তবে ক্ষেত্রের পূর্বে গবেষণা মূলত ইংরেজীতে বিশেষ করা হয়েছে এবং অন্যান্য ভাষার ব্যাপারে খুব কম পরিচিত। এই পর্যন্ত আমরা রুসেন্ট ইভালের সাথে পরিচয় করিয়ে দিচ্ছি, রাশিয়ার ১৪টি বাড়িয়ে দেয়া কাজ, যাদের মধ্যে রয়েছে যাদের এখনো সন্ধান করা হয়ন আমরা পাঁচ ভাষার বিভিন্ন ভাষার বিভিন্ন ভাষার বৈশিষ্ট্য বিতরণের ব্যাপারটি ব্যবহার করি যাতে দুটি সাধারণ ভাষায় বিপরীত ভাষার জন্য দুটি ভাষার ব আমাদের ফলাফল তৈরি করেছে কৌতূহলজনক পরিস্থিতি যা ভাষাগত জ্ঞান কিভাবে প্রতিনিধিত্ব করা হয়েছে তার বিরুদ্ধে সাধারণ বুঝতে পারে এবং তা প্রদর্শন', 'af': "Die sukses van voorafgevorderde transformeerde taal modele het 'n groot belang gebring oor hoe hierdie modele werk en wat hulle leer oor taal. Maar vooraf ondersoek in die veld is heeltemal aan Engels besluit, en klein is bekend aangaande ander tale. Op hierdie einde, introduseer ons RuSentEval, 'n verbeterde stel van 14 probeertaak vir Russiese, insluitend die wat nog nie uitgevoer is nie. Ons het 'n kombinasie van komplementare probeermetodes aanwend om die verspreiding van verskeie lingvisse eienskappe in vyf multitaalske transformeerders vir twee tipologiese kontrastende tale - Russiese en Engels te exploreer. Ons resultate verskaf intriguerende onderstellings wat teen die gemeenskap verstanding van hoe lingvisse kennis verteenwoordig is, en wys dat sommige eienskappe op 'n gelyke manier geleer word, behalwe die taal verskille.", 'sq': 'Suksesi i modeleve të gjuhës transformuese të paratrajnuar ka sjellë shumë interes në mënyrën se si funksionojnë këto modele dhe atë që mësojnë për gjuhën. Megjithatë, kërkimi i mëparshëm në fushë është kryesisht i përkushtuar anglisht dhe pak është e njohur lidhur me gjuhët e tjera. Për këtë qëllim, ne prezantojmë RuSentEval, një grup të përmirësuar prej 14 detyrash sondazhi për rusët, duke përfshirë ato që nuk janë eksploruar ende. Ne aplikojmë një kombinim të metodave komplementare të vëzhgimit për të eksploruar shpërndarjen e pronave të ndryshme gjuhësore në pesë transformues shumëgjuhës për dy gjuhë tipologjikisht kontrastuese - ruse dhe angleze. Rezultatet tona ofrojnë gjetje intriguese që kundërshtojnë kuptimin e përbashkët të sesi përfaqësohet njohuria gjuhësore dhe demonstrojnë se disa prona mësohen në një mënyrë të ngjashme pavarësisht nga dallimet gjuhësore.', 'cs': 'Úspěch předškolených transformátorových jazykových modelů přinesl velký zájem o to, jak tyto modely fungují a co se naučí o jazyce. Předchozí výzkum v této oblasti se však věnuje především angličtině a o ostatních jazycích je známo málo. Za tímto účelem představujeme RuSentEval, rozšířený soubor čtrnácti sondovacích úkolů pro Rusko, včetně těch, které dosud nebyly prozkoumány. Používáme kombinaci doplňkových sondačních metod k prozkoumání distribuce různých jazykových vlastností v pěti vícejazyčných transformátorech pro dva typologicky kontrastní jazyky – ruštinu a angličtinu. Naše výsledky poskytují zajímavé poznatky, které jsou v rozporu s běžným chápáním jazykových znalostí reprezentovány, a ukazují, že některé vlastnosti jsou navzdory jazykovým rozdílům učeny podobným způsobem.', 'az': "Əvvəlcə təhsil edilmiş transformer dil modellərin başarısızlığı bu modellərin necə işlədiyini və dillərin öyrəndiklərini çox maraqlaşdırdı. Ancaq sahədə əvvəlki araştırmalar ingilis dilinə məxsus edilir və digər dillər haqqında az bilinir. Buna görə biz RuSentEval'i təşkil edirik, Rus üçün daha çox təşkil edilməmiş 14 nəfər təşkil etdik. Biz müxtəlif dil xüsusiyyətlərini beş dildə çoxlu transformatçılar üçün iki tipolojik müxtəlif dillərə - Rus və İngilizce dillərinə təşkil etmək üçün komplementar prob metodlarının birləşdiririk. Bizim sonuçlarımız dil bilgisinin necə təşkil edildiyini və bəzi özelliklərin dil fərqliyinə baxmayaraq bənzər bir yolla öyrəndiyini göstərir.", 'et': 'Eelnevalt koolitatud transformaatorkeelemudelite edu on toonud suurt huvi selle kohta, kuidas need mudelid töötavad ja mida nad keele kohta õpivad. Kuid varasemad uuringud valdkonnas on pühendatud peamiselt inglise keele ja vähe on teada teiste keelte kohta. Sel eesmärgil tutvustame RuSentEvali, mis koosneb 14 vene keele proovimisülesandest, sealhulgas neist, mida pole veel uuritud. Rakendame täiendavate proovimeetodite kombinatsiooni, et uurida erinevate keeleliste omaduste jaotumist viies mitmekeelses transformaatoris kahe tüpoloogiliselt kontrastse keele - vene ja inglise keele jaoks. Meie tulemused annavad intrigeerivaid tulemusi, mis on vastuolus ühise arusaamaga, kuidas keeleteadmisi esindatakse, ning näitavad, et mõningaid omadusi õpitakse sarnaselt vaatamata keelelistele erinevustele.', 'fi': 'Esikoulutettujen muuntajien kielimallien menestys on herättänyt suurta kiinnostusta siitä, miten nämä mallit toimivat ja mitä ne oppivat kielestä. Aiempi tutkimus alalla on kuitenkin omistettu pääasiassa englannille, ja muista kielistä tiedetään vähän. Tässä tarkoituksessa esittelemme RuSentEvalin, joka on parannettu joukko 14 luotaustehtävää venäjälle, mukaan lukien ne, joita ei ole vielä tutkittu. Sovellamme täydentävien luotausmenetelmien yhdistelmää tutkiaksemme erilaisten kielellisten ominaisuuksien jakautumista viidessä monikielisessä muuntajassa kahdelle typologisesti vastakkaiselle kielelle - venäjälle ja englannille. Tuloksemme tarjoavat kiehtovia havaintoja, jotka ovat ristiriidassa yleisen ymmärryksen kielellisen tiedon edustuksesta ja osoittavat, että joitain ominaisuuksia opitaan samalla tavalla kielieroista huolimatta.', 'bs': 'Uspjeh predobučenih transformacijskih jezičkih modela dovodio je dosta interesa o tome kako ovi modeli rade, i o tome što oni uče o jeziku. Međutim, prije istraživanja na terenu je uglavnom posvećena engleskom, a malo je poznato u vezi drugih jezika. Za taj cilj predstavljamo RuSentEval, povećan set od 14 zadataka za istraživanje ruskih, uključujući one koje još nisu istraživali. Primjenjujemo kombinaciju dodatnih metoda ispitivanja za istraživanje distribucije različitih jezičkih vlasništva u pet multijezičkih transformatora za dva tipološki kontrastnog jezika - ruski i engleski. Naši rezultati pružaju zanimljive nalaze koje se suprotstavljaju zajedničkom razumijevanju kako je jezičko znanje predstavljeno, i pokazuju da su neki vlasnici naučeni na sličan način uprkos jezičkim razlikama.', 'ca': "The success of pre-trained transformer language models has brought a great deal of interest on how these models work, and what they learn about language.  However, prior research in the field is mainly devoted to English, and little is known regarding other languages.  To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for Russian, including ones that have not been explored yet.  Aplicam una combinació de mètodes complementars d'investigació per explorar la distribució de diverses propietats lingüístices en cinc transformadors multilingües per dues llengües tipològicament contrastants - russos i anglès. Els nostres resultats proporcionen descobriments intrigants que contradicten la comprensió comú de com es representa el coneixement lingüístic, i demostren que algunes propietats s'aprenen d'una manera similar malgrat les diferències lingüístices.", 'jv': 'Rasané model sing beranduwé, sampeyan ingkang sampeyan luwih dumateng kuwi kesempatan tentang karo ngono kuwi model iki ngono ngono kuwi jenis sira nggawe barang kelas. Nanging, sabanjur-sabanjuré sak ing sakjane kanggo ingkang, lan akeh cilik diangkat barang langgar. Nambah iki, kita nganggep nyengke RSentDebal, 14 awak dhéwé nggawe perbudhakan kanggo Russisa, sisan sing ora bisa kejahake. Awak dhéwé éntuk karo perusahaan karo perusahaan langkung sampeyan karo nganggep nggawe layakno karo hal-sampeyan ingkang sampeyan luwih Rejalaké awak dhéwé nggawe barang kelas kuwi bagasara karo paké kesempatan karo akeh langkung sapa-aké awak dhéwé, lan ngomongke tindakan karo perusahaan karo perusahaan sing dipunangé karo koyo cah-cah sabané, mengko perusahaan langkung sampey', 'he': 'הצלחה של דוגמני שפת משתנים מאומנים מראש הביאה הרבה עניין על איך הדוגמנים הללו עובדים, ומה הם לומדים על שפה. עם זאת, מחקר קודם בשטח מוקדש בעיקר לאנגלית, ומעט ידוע בנוגע לשפות אחרות. למטרה זו, אנחנו מציגים את RuSentEval, קבוצה משופרת של 14 משימות חקירה לרוסי, כולל אלה שלא נחקרו עדיין. אנו משתמשים בשילוב של שיטות חקירה תוספות כדי לחקור את ההפצה של תכונות שפתיים שונות בחמישה משתנים רבות שפתיים לשתי שפות שונות טיפולוגית - רוסית ואנגלית. התוצאות שלנו מספקות מציאות מעניינות שמתנגדות להבנה המשותפת של כיצד מייצג ידע שפתי, ולהראות שמספר תכונות נלמדות באופן דומה למרות ההבדלים בשפה.', 'sk': 'Uspeh vnaprej usposobljenih transformatorskih jezikovnih modelov je prinesel veliko zanimanja za delovanje teh modelov in kaj se naučijo o jeziku. Vendar pa so predhodne raziskave na tem področju predvsem namenjene angleščini, o drugih jezikih pa je malo znano. V ta namen predstavljamo RuSentEval, izboljšan nabor 14 nalog sondiranja za ruski jezik, vključno s tistimi, ki še niso bile raziskane. Uporabljamo kombinacijo dopolnilnih merilnih metod za raziskovanje porazdelitve različnih jezikovnih lastnosti v petih večjezičnih transformatorjih za dva tipološko kontrastna jezika - ruski in angleški. Naši rezultati ponujajo zanimive ugotovitve, ki nasprotujejo skupnemu razumevanju, kako je jezikovno znanje predstavljeno, in kažejo, da se nekatere lastnosti kljub jezikovnim razlikam naučijo na podoben način.', 'ha': "Haƙĩƙa, masu cin nasara da misãlai masu motsi da aka tsare ta gabãni ya zo da amfani mai yawa a kan jinsi misãlai ke aiki, da abin da suke karanta game da harshen. A lokacin da, kafin da za'a yi amfani da yin kawaici a cikin birnin, yana da amfani kaɗan a kan harshen dabam. Ga wannan, Munã ƙara RucentEal, mai ƙaranci matsayin 14 na taskõki na Ruushi, da waɗannan da ba a riga ba. Munã amfani da komai da shiryoyin jarraba masu kamfata ko kuma don su sami raba-rabo masu cikin littafan lingui shan cikin shifottori masu mulki-lingui, wa'ura da Ingiriya biyu masu motsi a cikin harshen-rubuci. MataimakinMu na bãyar da fassaran mai fasahawa da ke motsi ga fahimcin da ke da jinsi ilimi na lugha, kuma ya nuna cewa, za'a sanar da wasu properties da misãlin, kuma kõ da sãɓãnin harshe.", 'bo': 'སྔོན་གྱིས་བསླབ་པའི་སྔོན་གྱིས་འགྱུར་བ་ཅན་གྱི་སྐད་རིགས་དཔེ་གཞི་འདི་དག་གིས་ཇི་ལྟར་བྱེད་སྣང་ཆེན་ཤུགས་ཀྱི་ཡོད། འོན་ཀྱང་། མ་ཟད། རང་གི་སྒེར་གྱི་འཚོལ་ཞིབ་འདི་ལ་དབྱིན་ཡིག་དང་། སྐད་རིགས་གཞན་ཞིག་ལ་ཆ་ཁ་ཤས་མེད། འདི་ལྟར། ང་ཚོས་RuSentEval ལ་ངོ་སྤྲོད་བྱས་པ་ཡིན། རྒྱ་ནག་གི་དྲ་རྒྱ་ལྟར་ལྟ་ཞིབ་བྱས་པའི་ལས་ཀ་ཆ་༡༤་ཞིག ང་ཚོས་སྐད ང་ཚོའི་མཐོང་སྣང་ཚུལ་མང་པོ་ཞིག་ཡིན་པའི་སྐད་རིགས་ཤེས་ཀྱི་གསལ་བཤད་ལ་མཐོང་མི་རྣམས་དང་། སྐད་རིགས་ལ་ཁྱད་པར་ལ་རྐྱེན་བྱས་པར་ལ'}
{'en': 'Exploratory Analysis of News Sentiment Using Subgroup Discovery', 'ar': 'التحليل الاستكشافي لمشاعر الأخبار باستخدام اكتشاف المجموعة الفرعية', 'pt': 'Análise exploratória do sentimento de notícias usando a descoberta de subgrupo', 'fr': "Analyse exploratoire du sentiment des actualités à l'aide de la découverte de sous-groupes", 'es': 'Análisis exploratorio de la opinión de las noticias mediante el descubrimiento de subgrupos', 'ja': 'サブグループ発見を用いたニュース感情の探求的分析', 'zh': '用子组见新闻情探索性析', 'hi': 'उपसमूह डिस्कवरी का उपयोग कर समाचार भावना का अन्वेषणात्मक विश्लेषण', 'ru': 'Поисковый анализ настроений новостей с использованием подгруппы Discovery', 'ga': 'Anailís Taiscéalaíoch ar Mheintiteas Nuachta ag Úsáid Fionnachtain Foghrúpa', 'ka': 'ინფორმაციის სისტემისტის გამოყენება საბეჯგუფის Discovery', 'el': 'Εξερευνητική ανάλυση του συναισθήματος ειδήσεων χρησιμοποιώντας την ανακάλυψη υποομάδας', 'hu': 'A hírek hangulatának feltáró elemzése alcsoport felfedezésével', 'kk': 'Жаңалықтардың сезімдігін зерттеу талдауы', 'mk': 'Истражувачка анализа на чувството за вести користејќи откритие на подгрупата', 'it': 'Analisi esplorativa del sentimento delle notizie utilizzando la scoperta del sottogruppo', 'lt': 'Naujų jausmų tiriamoji analizė naudojant pogrupio atradimą', 'mn': 'Хөгжлийн мэдээллийн мэдрэгчийн судалгааны шинжилгээ', 'ml': 'സബ്ഗ്രൂപ്പ് ഡിസ്ക്രവറി ഉപയോഗിക്കുന്ന വാര്\u200dത്ത സെന്റിമെന്\u200dറിന്\u200dറെ വിശ്വാസം', 'mt': 'Analiżi esploratorja tas-Sentiment tal-Aħbarijiet bl-użu tal-Iskopri tas-Sottogrupp', 'pl': 'Analiza rozpoznawcza sentymentu wiadomości przy użyciu odkrycia podgrup', 'ro': 'Analiza exploratorie a sentimentelor de stiri folosind descoperirea subgrupului', 'sr': 'Eksploraciona analiza novosti Sentimenta koristeći otkrivanje podgrupe', 'ms': 'Analisi Penjelasan Pengesanan Berita Menggunakan Penemuan Subkumpulan', 'si': 'වාර්තාවක් විශේෂ විශ්ලේෂණය සබ්ග්\u200dරූපය හොයාගන්න භාවිත කරන්න', 'no': 'Comment', 'sv': 'Explorerande analys av nyhetskänslor med hjälp av undergrupp Discovery', 'so': 'Analyse of News Sentiment using Subgroup Discovery', 'ta': 'துணை குழுவின் கண்டுபிடிப்பை பயன்படுத்தி செய்தி மேலாளரின் விளக்கம்', 'ur': 'نیویس سنٹیمینٹ کی تحقیق کا تحقیق تحقیق', 'uz': 'Name', 'vi': 'Khám phá bản tin tình cảm thông qua cách tìm hiểu', 'bg': 'Изследователски анализ на сентимента на новините чрез откриване на подгрупа', 'da': 'Eksperimenterende analyse af nyhedsfølelser ved hjælp af undergruppeopdagelse', 'hr': 'Istraživačka analiza vijesti Sentimenta koristeći otkriće podskupine', 'nl': 'Verkennende Analyse van Nieuwsentiment met behulp van Subgroup Discovery', 'de': 'Explorative Analyse von Nachrichten Sentiment unter Verwendung von Subgroup Discovery', 'ko': '서브그룹을 바탕으로 발견된 뉴스 정서 탐색적 분석', 'id': 'Analisi Explorator Sentiment Berita Menggunakan Penemuan Subgrup', 'fa': 'تحلیل تحقیقات حسابگروهی از استفاده از پیدا کردن زیر گروه', 'tr': 'Täzelikler Sentimentynyň Exploratory Analysis of Sentiment Using Subgroup Discovery', 'af': 'Ondersoektog analyseer van Nuus Sentiment gebruik Subgroup Discovery', 'sq': 'Analiza eksploruese e ndjenjave të lajmeve duke përdorur zbulimin e nëngrupit', 'sw': 'Uchambuzi wa Seneti ya Habari kwa kutumia Ugunduzi wa Vikundi', 'am': 'አቀማመጥ', 'az': 'Subgroup Discovery', 'bn': 'সাবগ্রুপ ডিস্ক্রিভারি ব্যবহার করে সংবাদ সেন্টাইমেন্টের এক্সপ্লোরেটরি বিশ্লেষণ', 'bs': 'Eksploraciona analiza novosti Sentimenta koristeći otkrivanje podgrupe', 'cs': 'Průzkumná analýza novinkového sentimentu pomocí zjištění podskupiny', 'hy': 'Նորությունների զգացմունքի հետազոտական վերլուծությունը օգտագործելով ենթախմբի բացահայտությունը', 'et': 'Uudiste tunnete uurimisanalüüs allrühma avastamise abil', 'fi': 'Uutisten tunteiden eksploratiivinen analyysi alaryhmän löytöä käyttäen', 'ca': 'Anàlisi exploratori del sentiment de notícies utilitzant la descoberta de subgrups', 'he': 'ניתוח חוקרי של תחושת חדשות באמצעות גילוי תחת קבוצה', 'ha': 'Analyze of News sensitive uses Subgroup DiscDiscDiscover', 'sk': 'Raziskovalna analiza čustva novic z uporabo odkrivanja podskupine', 'bo': 'ཆ་འཕྲིན་ཡོད་པའི་བརྡ་སྟོན་ཞིབ་བྱེད་སྟངས་མཁན་དབྱེ་ཞིབ་བྱེད་པ', 'jv': 'Sentiment Sentiment Ngawe Diskutrolan Subgroup'}
{'en': 'In this study, we present an exploratory analysis of a Slovenian news corpus, in which we investigate the association between named entities and sentiment in the news. We propose a methodology that combines Named Entity Recognition and Subgroup Discovery-a descriptive rule learning technique for identifying groups of examples that share the same class label (sentiment) and pattern (features-Named Entities). The approach is used to induce the positive and negative sentiment class rules that reveal interesting patterns related to different Slovenian and international politicians, organizations, and locations.', 'ar': 'في هذه الدراسة ، نقدم تحليلًا استكشافيًا لمجموعة الأخبار السلوفينية ، حيث نقوم بالتحقيق في الارتباط بين الكيانات المسماة والمشاعر في الأخبار. نقترح منهجية تجمع بين التعرف على الكيانات المسماة واكتشاف المجموعة الفرعية - وهي تقنية وصفية لتعلم القواعد لتحديد مجموعات الأمثلة التي تشترك في نفس تسمية الفئة (المشاعر) والنمط (الميزات - الكيانات المسماة). يتم استخدام النهج للحث على قواعد فئة المشاعر الإيجابية والسلبية التي تكشف عن أنماط مثيرة للاهتمام تتعلق بمختلف السياسيين والمنظمات والمواقع السلوفينية والدولية.', 'fr': "Dans cette étude, nous présentons une analyse exploratoire d'un corpus d'actualités slovène, dans laquelle nous étudions l'association entre les entités nommées et le sentiment dans les actualités. Nous proposons une méthodologie qui combine la reconnaissance d'entités nommées et la découverte de sous-groupes - une technique d'apprentissage de règles descriptives permettant d'identifier des groupes d'exemples partageant le même label de classe (sentiment) et le même modèle (fonctionnalités - entités nommées). L'approche est utilisée pour induire les règles de classe de sentiment positif et négatif qui révèlent des modèles intéressants liés à différents politiciens, organisations et lieux slovènes et internationaux.", 'pt': 'Neste estudo, apresentamos uma análise exploratória de um corpus de notícias esloveno, no qual investigamos a associação entre entidades nomeadas e sentimento nas notícias. Propomos uma metodologia que combina Reconhecimento de Entidades Nomeadas e Descoberta de Subgrupos - uma técnica descritiva de aprendizado de regras para identificar grupos de exemplos que compartilham o mesmo rótulo de classe (sentimento) e padrão (características - Entidades Nomeadas). A abordagem é usada para induzir as regras de classe de sentimento positivo e negativo que revelam padrões interessantes relacionados a diferentes políticos, organizações e locais eslovenos e internacionais.', 'es': 'En este estudio, presentamos un análisis exploratorio de un corpus de noticias esloveno, en el que investigamos la asociación entre las entidades nombradas y el sentimiento en las noticias. Proponemos una metodología que combina el reconocimiento de entidades con nombre y el descubrimiento de subgrupos, una técnica de aprendizaje de reglas descriptivas para identificar grupos de ejemplos que comparten la misma etiqueta de clase (sentimiento) y patrón (características: entidades con nombre). El enfoque se utiliza para inducir las reglas de clase de sentimiento positivo y negativo que revelan patrones interesantes relacionados con diferentes políticos, organizaciones y lugares eslovenos e internacionales.', 'ja': 'この研究では、スロベニアのニュースコーパスの探索的分析を提示し、名前付きエンティティとニュース内の感情との関連性を調査します。名前付きエンティティ認識とサブグループディスカバリーを組み合わせた方法論を提案します。これは、同じクラスラベル（感情）とパターン（特徴-名前付きエンティティ）を共有する例のグループを識別するための記述的なルール学習テクニックです。このアプローチは、さまざまなスロベニアと国際的な政治家、組織、場所に関連する興味深いパターンを明らかにする肯定的な感情クラスルールと否定的な感情クラスルールを誘導するために使用されます。', 'zh': '于此论之,吾对斯洛文尼亚新闻语料库为探索性析之,其间名实与新闻情相关。 立将名实体识与子组相合之法 - 一描述性法则学术,以识共享同类(情)模式(特征 - 名实体)之示例组。 其法以诱消极情绪类,揭示异斯洛文尼亚国际政治家,结地相关。', 'ru': 'В этом исследовании мы представляем поисковый анализ новостного корпуса Словении, в котором исследуем связь между названными сущностями и настроениями в новостях. Мы предлагаем методологию, которая объединяет распознавание именованных сущностей и обнаружение подгрупп - описательный метод обучения правилам для выявления групп примеров, которые имеют одинаковую метку класса (настроение) и шаблон (особенности - именованные сущности). Подход используется для стимулирования позитивных и негативных правил класса настроений, которые раскрывают интересные закономерности, связанные с различными словенскими и международными политиками, организациями и локациями.', 'hi': 'इस अध्ययन में, हम एक स्लोवेनियाई समाचार कॉर्पस का एक अन्वेषणात्मक विश्लेषण प्रस्तुत करते हैं, जिसमें हम समाचार में नामित संस्थाओं और भावनाओं के बीच संबंध की जांच करते हैं। हम एक ऐसी पद्धति का प्रस्ताव करते हैं जो नामित एंटिटी पहचान और उपसमूह डिस्कवरी को जोड़ती है - उदाहरणों के समूहों की पहचान करने के लिए एक वर्णनात्मक नियम सीखने की तकनीक जो एक ही वर्ग लेबल (भावना) और पैटर्न (विशेषताएं - नामित निकाय) साझा करती है। इस दृष्टिकोण का उपयोग सकारात्मक और नकारात्मक भावना वर्ग नियमों को प्रेरित करने के लिए किया जाता है जो विभिन्न स्लोवेनियाई और अंतरराष्ट्रीय राजनेताओं, संगठनों और स्थानों से संबंधित दिलचस्प पैटर्न को प्रकट करते हैं।', 'ga': 'Sa staidéar seo, cuirimid i láthair anailís thaiscéalaíoch ar chorpas nuachta Slóivéineach, ina ndéanaimid imscrúdú ar an mbaint atá idir eintitis ainmnithe agus meon sa nuacht. Molaimid modheolaíocht a chomhcheanglaíonn Aitheantas Aonán Ainmnithe agus Fionnachtain Foghrúpaí - teicníocht foghlama tuairisciúla rialacha chun grúpaí samplaí a aithint a roinneann an lipéad ranga céanna (meonachán) agus an patrún céanna (gnéithe - Aonáin Ainmnithe). Baintear úsáid as an gcur chuige seo chun na rialacha aicme dearfacha agus diúltacha a chothú a nochtann patrúin suimiúla a bhaineann le polaiteoirí, eagraíochtaí agus láithreacha éagsúla Slóivéineacha agus idirnáisiúnta.', 'ka': 'ამ კვლევაში ჩვენ სლოვანიის ნუტური კორპუსის განსხვავებელი ანალიზია, რომელიც ჩვენ განსხვავებთ ახალგაზრულებში სახელსახულების და სენტიმენტების ახალგაზრულებში ახალგაზრულ ჩვენ შეგიძლიათ მეტოლოგია, რომელიც სახელი ინტერტის განაცნობა და სახელიჯგუფის განაცნობა - განაცნობის მეტოლოგია მაგალითა ჯგუფის განაცნობისთვის, რომელიც იგივე კლასის ნიშანი (სინტერმენტი) და ნიშანი პროგორმაცია გამოყენებულია პოლიტიკური და განსაკუთრებული სენტიმენტის კლასის წესები, რომლებიც განსაკუთრებულია ინტერესური შაბლონები, რომლებიც განსხვავებული სლოვინეთის და', 'el': 'Στην παρούσα μελέτη, παρουσιάζουμε μια διερευνητική ανάλυση ενός σλοβενικού ειδησεογραφικού σώματος, στο οποίο διερευνούμε τη συσχέτιση μεταξύ των ονομαζόμενων οντοτήτων και του συναισθήματος στις ειδήσεις. Προτείνουμε μια μεθοδολογία που συνδυάζει την Αναγνώριση Οντότητας και την Αποκάλυψη Υποομάδας, δημιουργώντας μια περιγραφική τεχνική εκμάθησης κανόνων για τον προσδιορισμό ομάδων παραδειγμάτων που μοιράζονται την ίδια ετικέτα τάξης (συναίσθημα) και μοτίβο (χαρακτηριστικά των Οντότητες). Η προσέγγιση χρησιμοποιείται για να προκαλέσει τους κανόνες της τάξης θετικών και αρνητικών συναισθημάτων που αποκαλύπτουν ενδιαφέροντα μοτίβα που σχετίζονται με διαφορετικούς σλοβένιους και διεθνείς πολιτικούς, οργανισμούς και τοποθεσίες.', 'hu': 'Ebben a tanulmányban egy szlovén hírcsoport feltáró elemzését mutatjuk be, amelyben megvizsgáljuk a megnevezett entitások és az érzelmek közötti kapcsolatot a hírekben. Javasolunk egy olyan módszertant, amely kombinálja a Nevezett entitások felismerését és az alcsoport felfedezését - egy leíró szabályok tanulási technikáját, amely azonosítja a példacsoportokat, amelyek ugyanazon osztálycímkével (sentiment) és mintával (funkciók - Nevezett entitások). A megközelítést arra használják, hogy indukálják a pozitív és negatív érzelmi osztály szabályait, amelyek érdekes mintákat mutatnak fel a különböző szlovén és nemzetközi politikusokhoz, szervezetekhez és helyszínekhez kapcsolódóan.', 'it': "In questo studio presentiamo un'analisi esplorativa di un corpus di notizie sloveno, in cui indaghiamo l'associazione tra entità nominate e sentiment nelle notizie. Proponiamo una metodologia che combina Named Entity Recognition e Subgroup Discovery - una tecnica descrittiva di apprendimento delle regole per identificare gruppi di esempi che condividono la stessa etichetta di classe (sentiment) e modello (caratteristiche - Named Entities). L'approccio viene utilizzato per indurre le regole positive e negative della classe sentiment che rivelano modelli interessanti legati a diversi politici, organizzazioni e luoghi sloveni e internazionali.", 'kk': 'Бұл зерттеулерде біз Словенияның жаңалық корпусының зерттеулерді анализ көрсеткіздік. Бұл жерде жаңалықтардың атаулары мен сезімдер арасындағы байланыстығын зерттейміз. Біз аталған нысандарды анықтау және ішкі топтарды табу - бір класс белгісін (сезімі) мен үлгісін ортақтастыратын мәселелердің топтарды анықтау үшін түсінікті ережелердің оқыту техникасын ұсындық. Бұл жағдай әртүрлі Словения және халықаралық саясатшылар, организациялар және жергіліктерге қатынасыз қызықты үлгілерді көрсету үшін қолданылады.', 'lt': 'Šiame tyrime pristatome tiriamąją Slovėnijos naujienų korpuso analizę, kurioje tiriame pavadintų subjektų asociaciją ir jausmą naujienose. Siūlome metodiką, kuri derintų pavadintų subjektų pripažinimą ir pogrupių atradimą – aprašomąją taisyklių mokymosi metodiką, skirtą nustatyti pavyzdžių grupes, turinčias tą pačią klasės etiketę (jausmą) ir modelį (savybes – pavadintus subjektus). Šis metodas naudojamas skatinant teigiamų ir neigiamų jausmų klasės taisykles, kurios atskleidžia įdomius modelius, susijusius su skirtingais Slovėnijos ir tarptautiniais politikais, organizacijomis ir vietovėmis.', 'mk': 'Во оваа студија, претставуваме експлоаторна анализа на словенечкиот новински корпус, во кој ја истражуваме асоцијацијата помеѓу именуваните ентитети и чувствата во вестите. Предложуваме методологија која ја комбинира препознавањето на именуваните ентитети и откритието на подгрупите - описна техника за учење на правила за идентификување на групи примери кои ја делат истата класа етикета (чувство) и шаблон (карактеристики - именувани ентитети). Пристапот се користи за индукција на позитивните и негативните правила на класата на чувства кои откриваат интересни шеми поврзани со различните словенечки и меѓународни политичари, организации и локации.', 'ms': 'Dalam kajian ini, kami memperkenalkan analisis eksploratori dari korpus berita Slovenia, di mana kami menyelidiki persatuan antara entiti bernama dan perasaan dalam berita. Kami cadangkan metodologi yang menggabungkan Pengenalan Entiti bernama dan Penemuan Subkumpulan - teknik pembelajaran peraturan yang menjelaskan untuk mengenalpasti kumpulan contoh yang berkongsi label kelas (perasaan) dan corak (ciri - Entiti bernama). Pendekatan ini digunakan untuk mendorong peraturan kelas perasaan positif dan negatif yang mengungkapkan corak menarik berkaitan dengan politik, organisasi dan lokasi Slovenia dan antarabangsa yang berbeza.', 'ml': 'ഈ പഠനത്തില്\u200d സ്ലോവേനിയന്\u200d വാര്\u200dത്തകളുടെ കോര്\u200dപ്പുസിന്\u200dറെ ഒരു പരിശോധന വിശദീകരണവും ഞങ്ങള്\u200d കൊണ്ടുവരുന്നു. അതില്\u200d വാര്\u200dത്തകളില്\u200d പേരുള പേരിട്ട എന്റിറ്റി തിരിച്ചറിയുന്നതിനെയും സബ്ഗ്രൂപ്പിനെയും കൂട്ടിച്ചേര്\u200dക്കുന്ന ഒരു മാറ്റിത്തീര്\u200dക്കുന്നു. ഒരു വിശദീകരിക്കുന്ന നിയമവിവരങ്ങള്\u200d  വ്യത്യസ്ത സ്ലോവേനിയയുടെയും അന്താരാഷ്ട്രീയ രാഷ്ട്രീയീയക്കാരുടെയും സംഘടനകളുടെയും സ്ഥലങ്ങളുടെയും ബന്ധപ്പെട്ടിരിക്കുന', 'mt': 'F’dan l-istudju, qed nippreżentaw analiżi esploratorja ta’ korpus tal-a ħbarijiet Sloven, li fih ninvestigaw l-assoċjazzjoni bejn entitajiet imsemmija u s-sentiment fl-aħbarijiet. Aħna nipproponu metodoloġija li tgħaqqad ir-Rikonoxximent tal-Entità Ismija u l-Iskopri tas-Sottogrupp - teknika deskrittiva tat-tagħlim tar-regoli għall-identifikazzjoni ta’ gruppi ta’ eżempji li jaqsmu l-istess tikketta tal-klassi (sentiment) u mudell (karatteristiċi - Entitajiet Ismija). L-approċċ jintuża biex jinduċi regoli pożittivi u negattivi tal-klassi tas-sentimenti li jiżvelaw xejriet interessanti relatati ma’ politiki, organizzazzjonijiet u postijiet differenti Sloveni u internazzjonali.', 'mn': 'Энэ судалгаанд бид Словенийн мэдээллийн корпусын судалгааны шинжилгээг үзүүлнэ. Энэ нь мэдээллийн нэр санаа, сэтгэл хөдлөлийн хоорондын холбоотой холбоотой байдлыг судалж байна. Бид нэрлэгдсэн Entity Recognition and Subgroup Discovery-г нэгтгэдэг методологийг санал болгож байна. Энэ арга нь олон Словени, улсын улс төрчид, байгууллагууд, байгууллагууд, газрын холбоотой сонирхолтой дүрслэлүүдтэй холбоотой эерэг болон сөрөг сэтгэл хөдлөлийн дүрслэлүүдийг үүсгэх', 'no': 'I denne studien presenterer vi eit utforskingssanalyse av ein Slovensk nyhetskorpus, der vi undersøker assosiasjonen mellom namnet entitet og sentiment i nyhetene. Vi foreslår ein metodologi som kombinerer namnet entitetskjenning og undergruppe- oppdaging - ein skildring av regel- læringsteknikk for å identifisera grupper av eksemplar som deler den same klassesmerkelappen (sentiment) og mønster (funksjonar – namnet entiteter). Tilnærminga vert brukt for å indusera de positivne og negativne sentimentklassesreglane som viser interessante mønsterer relaterte til ulike slovenske og internasjonale politikere, organisasjonar og stader.', 'pl': 'W niniejszym opracowaniu przedstawiamy analizę eksploracyjną słoweńskiego korpusu informacyjnego, w której badamy związek między wymienionymi podmiotami a sentymentami w wiadomościach. Proponujemy metodologię łączącą rozpoznawanie nazwanych podmiotów i odkrywanie podgrup z opisową techniką uczenia się reguł służącą identyfikacji grup przykładów, które mają tę samą etykietę klasy (sentyment) i wzór (cechy nazwanych podmiotów). Podejście to służy do wywoływania pozytywnych i negatywnych reguł klasowych sentymentów, które ujawniają ciekawe wzorce związane z różnymi słoweńskimi i międzynarodowymi politykami, organizacjami i lokalizacjami.', 'ro': 'În acest studiu prezentăm o analiză exploratorie a unui corpus de știri sloven, în care investigăm asocierea dintre entitățile denumite și sentimentul din știri. Propunem o metodologie care combină recunoașterea entităților denumite și descoperirea subgrupurilor - o tehnică descriptivă de învățare a regulilor pentru identificarea grupurilor de exemple care împărtășesc aceeași etichetă de clasă (sentiment) și model (caracteristici - entități denumite). Abordarea este folosită pentru a induce regulile clasei sentimentelor pozitive și negative care dezvăluie modele interesante legate de diferiți politicieni, organizații și locații slovene și internaționale.', 'sr': 'U ovoj studiji predstavljamo istraživačku analizu slovenskog vestničkog korpusa, u kojem istražujemo asocijaciju između imenovanih entiteta i osjećanja u vestima. Mi predlažemo metodologiju koja kombinira priznanje imenovanih entiteta i otkrivanje podgrupa - tehniku učenja pravila za identifikaciju grupa primjera koji deli istu klasu etiketu (osjećaj) i obrazac (karakteristike - imenovane entitate). Pristup se koristi kako bi izazvao pravila klase pozitivnih i negativnih sentimenta koji otkrivaju zanimljive obrasce povezane sa različitim slovenskim i međunarodnim političarima, organizacijama i lokacijama.', 'si': 'මේ පරීක්ෂණයේ අපි ස්ලෝවෙනියානු වාර්තාව කොර්පුස් ගැන පරීක්ෂණ විශ්ලේෂණයක් පෙන්වනවා, ඒ වගේම අපි පරීක්ෂණය කරනවා න Name අනුවෙන් විදිහට ස්ලෝවෙනියානු සහ ජාතික රාජ්\u200dයාධිකාරය, සංවිධාන, සහ තැන් සම්බන්ධ විදිහට සම්බන්ධ විදිහ', 'so': "Waxbarashadan waxaynu keenaynaa baaritaan baaritaan oo ka mid ah koobasha wararka Slovenian, kaas oo aynu baaritaan ururka u dhexeeya hay'adaha la magacaabay iyo fikrada warka. Waxaan soo bandhignaynaa qaab ku qoran aqoonsiga Entity and Discovery Subgroup - waa qoraal waxbarasho oo ka mid ah koox tusaale ah oo isku mid ah calaamada (sentiment) iyo tusaale (xos - magac Entities). Dhaqdhaqaaqa waxaa loo isticmaalaa in la sameeyo qaynuunnada farsamada positive iyo negative, kuwaas oo muujiya qaabab xiiso leh oo la xiriira dhaqdhaqaaq kala duduwan Slovenian iyo siyaasadeeda caalamiga ah, ururada iyo meelo ay joogaan.", 'sv': 'I denna studie presenterar vi en explorativ analys av en slovensk nyhetskorpus, där vi undersöker sambandet mellan namngivna entiteter och sentiment i nyheterna. Vi föreslår en metod som kombinerar Named Entity Recognition och Undergroup Discovery - en beskrivande regelinlärningsteknik för att identifiera grupper av exempel som delar samma klassetikett (sentiment) och mönster (funktioner - Named Entities). Metoden används för att framkalla positiva och negativa sentimentklassregler som avslöjar intressanta mönster relaterade till olika slovenska och internationella politiker, organisationer och platser.', 'ta': 'இந்த ஆராய்ச்சியில், நாம் ஒரு ஸ்லோவெனிய செய்தி கார்ப்ஸின் ஒரு ஆய்வு ஆராய்ச்சியை கொண்டு வருகிறோம், அதில் நாம் பெயரிடப்பட்ட நாம் ஒரு முறைமையை தொடர்பு கொள்கிறோம் பெயரிடப்பட்ட பொருள் அடையாளம் மற்றும் துணை குழு கண்டுபிடிப்பது - ஒரு விவரிப்பு விதியாசம் கற்றுக் கொள்ளும் தொழில்நு நேர்மறை மற்றும் எதிர்மறை உணர்வு வகுப்பு விதிகளை முறைப்படுத்த பயன்படுத்தப்படும். இது வேறு சுவாரஸ்யமான மாதிரிகளை வெளிப்படுத்தும்', 'ur': 'اس مطالعہ میں ہم اسلووین کی خبر کورپوس کی ایک تحقیق تحقیق دیتے ہیں جس میں ہم نے اخبار میں نام داروں اور احساساتوں کے درمیان شرکت کی تحقیق کی ہے. ہم ایک طریقہ پیشنهاد کرتے ہیں جس نے نام کی Entity Recognition اور Subgroup Discovery کو ترکیب کیا ہے - ایک توصیف قانون تعلیم کی تکنیک ہے مثالوں کے گروہوں کو پہچان کرنے کے لئے جو ایک کلاس لیبل (احساس) اور پٹرن کے ساتھ ملتے ہیں۔ یہ طریقہ مثبت اور منفی احساسات کلاس کے قانون کے ذریعے استعمال کیا جاتا ہے جو مختلف سلووین اور بین المللی سیاستمداروں، سازمانوں اور جگہ کے معاملہ میں علاقمند پٹرنے والے نمونے ظاہر کرتے ہیں.', 'vi': 'Trong nghiên cứu này, chúng tôi đưa ra một nghiên cứu thăm dò về một tập đoàn thông tin người Xlôven, trong đó chúng tôi điều tra sự liên kết giữa các thực thể có tên và cảm xúc trên tin tức. Chúng tôi đề xuất một phương pháp kết hợp Named Entity recognition và Subgroup Disco... a descripive quy leaving technique for identification groups of exampIe that share the same class Nhãn (ủy mị) and pattern (tính năng tên "Enties". Cách tiếp cận được dùng để tạo ra các quy tắc xã hội tình cảm tích cực và tiêu cực, tiết lộ những mô hình thú vị liên quan đến các chính trị gia, tổ chức và địa điểm của người Xlôven và quốc tế.', 'uz': "Bu tadqida, vi Slovenian news corpusning aniqlarini ko'rganamiz. Bu yerda xabarlarda nomlangan obʼektlar va hissiyotlar orasidagi aloqalarni o'rganamiz. Biz nomli tizimni Tasdiqlash va tub guruhni qidirish usulini birlashtirish imkoniyatini taʼrif qilamiz - bitta sinf (sentiment) va shaklni (xususiyatlar - nomli tizimlar) bilan bogʻlash mumkin. Bu usulni boshqa Sloveniya va international siyosatkorlar, 组织lar, mahallar bilan bog'liqchi qiziqarli shakllarni ishga tushirish uchun ishlatiladi.", 'bg': 'В това проучване представяме изследователски анализ на словенски новинарски корпус, в който изследваме връзката между названите субекти и настроението в новините. Предлагаме методология, която съчетава разпознаване на наименовани субекти и откриване на подгрупи - описателна техника за учене на правила за идентифициране на групи примери, които споделят един и същ клас етикет (сентимент) и модел (характеристики - наименовани субекти). Подходът се използва за индуциране на позитивните и отрицателните класове на сантименталните правила, които разкриват интересни модели, свързани с различни словенски и международни политици, организации и локации.', 'hr': 'U ovom ispitivanju predstavljamo istraživačku analizu slovenskog vijestinskog korpusa, u kojem istražujemo asocijaciju između imenovanih entitata i osjećanja u vijestima. Predlažemo metodologiju koja kombinira priznanje imenovanih podskupina i otkrivanje podskupina - tehniku učenja pravila za identifikaciju skupina primjera koji dijele istu klasu etiketu (osjećaj) i obrazac (karakteristika imenovanih podskupina). Pristup se koristi kako bi izazvao pravila klase pozitivnih i negativnih osjećaja koje otkrivaju zanimljive obrasce povezane sa različitim slovenskim i međunarodnim političarima, organizacijama i lokacijama.', 'nl': 'In deze studie presenteren we een verkennende analyse van een Sloveens nieuwscorpus, waarin we de associatie onderzoeken tussen genoemde entiteiten en sentiment in het nieuws. We stellen een methodologie voor die Named Entity Recognition en Subgroup Discovery combineert met een beschrijvende rule learning techniek voor het identificeren van groepen voorbeelden die hetzelfde klassenlabel (sentiment) en patroon (features bij Named Entities) delen. De benadering wordt gebruikt om de regels van positieve en negatieve sentimenten op te wekken die interessante patronen onthullen die verband houden met verschillende Sloveense en internationale politici, organisaties en locaties.', 'da': 'I dette studie præsenterer vi en sonderende analyse af et slovensk nyhedskorpus, hvor vi undersøger sammenhængen mellem navngivne enheder og sentiment i nyhederne. Vi foreslår en metode, der kombinerer navngivet enhedsgenkendelse og undergruppeopdagelse - en beskrivende regellæringsteknik til identificering af grupper af eksempler, der deler den samme klasse etiket (sentiment) og mønster (funktioner - navngivne enheder). Tilgangen bruges til at fremkalde de positive og negative sentiment klasse regler, der afslører interessante mønstre relateret til forskellige slovenske og internationale politikere, organisationer og steder.', 'ko': '이 연구에서 우리는 슬로베니아 뉴스 자료 라이브러리에 대해 탐색적 분석을 하고 명명 실체와 뉴스 중의 정서 간의 관련을 연구했다.우리는 명명 실체 식별과 하위 그룹을 결합시켜 발견하는 방법을 제시했다. 이것은 묘사적 규칙 학습 기술로 같은 종류의 라벨(감정)과 모델(특징-명명 실체)을 가진 예시 그룹을 식별하는 데 사용된다.이 방법은 적극적이고 소극적인 계급 규칙을 귀납하고 서로 다른 슬로베니아와 국제 정치인, 조직과 장소와 관련된 흥미로운 모델을 제시하는 데 쓰인다.', 'id': 'Dalam studi ini, kami mempersembahkan analisis esploratori dari sebuah korpus berita Slovenia, di mana kami menyelidiki asosiasi antara entitas bernama dan sentimen dalam berita. Kami mengusulkan metodologi yang menggabungkan Pengenalan Entitas bernama dan Penemuan Subkelompok - teknik penelitian aturan deskriptif untuk mengidentifikasi kelompok contoh yang berbagi label kelas (sentimen) dan pola (fitur - Entitas bernama). The approach is used to induce the positive and negative sentiment class rules that reveal interesting patterns related to different Slovenian and international politicians, organizations, and locations.', 'de': 'In dieser Studie präsentieren wir eine explorative Analyse eines slowenischen Nachrichtenkorpus, in dem wir den Zusammenhang zwischen benannten Entitäten und Sentiment in den Nachrichten untersuchen. Wir schlagen eine Methodik vor, die Named Entity Recognition und Subgroup Discovery mit einer deskriptiven Regellerntechnik kombiniert, um Beispielgruppen zu identifizieren, die dieselbe Klassenbezeichnung (Sentiment) und dasselbe Muster (Features bei Named Entities) teilen. Der Ansatz wird verwendet, um die positiven und negativen Sentiment-Klassenregeln zu induzieren, die interessante Muster in Bezug auf verschiedene slowenische und internationale Politiker, Organisationen und Standorte offenbaren.', 'sw': 'Katika utafiti huu, tunaweka uchambuzi wa kituo cha habari cha Slovenia, ambacho tunachunguza uhusiano kati ya vyombo vinavyoitwa na hisia katika habari. Tunazipendekeza mbinu inayounganisha utambulisho wa Intaneti na Ugunduzi wa Kikundi cha Kusini - teknolojia ya kujifunza kwa maelezo ya maelezo kwa kutambua makundi ya mifano yanayoshirikisha alama hiyo (hisia) na mtindo (utambulisho - Vituo vinavyoitwa). Hatua hii inatumiwa kuendesha sheria za tabaka chanya na hasi za hisia zinazoonyesha mitindo ya kusisimua yanayohusiana na wanasiasa tofauti na wa kimataifa, mashirika, na maeneo.', 'tr': "Bu okuwda, Sloveniýanyň täzelikler korpusynyň aralygy çykyşlygyny görkeýäris. Şol ýerde biz kellämizde adly zatlar we duýgular arasyndaky baglaşlygyny barlaýarys. Biz Adlanan Entity Recognition we Subgroup Discovery'y birleştirip bilen bir methodologi tekniýetlendirdik. -Aynı sınıf etiketini we şartlary bilen beýleki milletleri tanap üçin bir tassyklam tekniýeti tekniýetlendirdik. Bu ýagdaý pozitif we negatif duýgular klasynyň düzgünlerini seretmek üçin ullanylýar. Diňe bir Sloweniýa we Halkara syýasatçylar, organizasyýatlar we ýerler bilen ilginç nusgalary ýaly düzgünlere seredenler.", 'af': "In hierdie studie voorsien ons 'n uitsoek analisie van 'n Sloveniese nuuskorpus, waarin ons die vergadering ondersoek tussen genaamde entiteite en sentimente in die nuus. Ons voorstel 'n metodologie wat gemeenskap genoem Entiteit-herkening en Subgroep-Discovery - 'n beskrywende reël-leer-tekniks vir identifiseer groepe van voorbeelde wat dieselfde klas etiket (sentiment) en patroon (funksies - Gegenoem Entiteit) deel. Die toegang is gebruik om die positiewe en negatiewe sentiment klas reëls te induseer wat interessante patrone openbaar wat verwante is met verskillende slowense en internasionale politieke, organisasies en liggings.", 'fa': 'در این مطالعه، ما یک تحلیل تحقیقاتی از یک شرکت اخبار اسلوونی را پیشنهاد می\u200cکنیم، که در آن مشارکت بین شرکت\u200cهای نامیده و احساسات در اخبار تحقیق می\u200cکنیم. ما روش\u200cشناسی را پیشنهاد می\u200cکنیم که شناسایی واحد\u200cهای نامیده و آشنایی زیر گروه\u200cها را ترکیب می\u200cکند - تکنیک یادگیری قانون توصیف برای شناسایی گروهی از مثالها که نقاط یک کلاس (احساسات) و الگو (ویژه\u200cهای نامیده) را شریک می\u200cکنند. این دستور برای تحریک قوانین کلاس احساسات مثبت و منفی استفاده می\u200cشود که الگوهای جالبی رابطه به سیاستمداران، سازمان\u200cها و محل\u200cهای بین المللی و اسلوونی متفاوت را نشان می\u200cدهد.', 'am': 'በዚህ ትምህርት፣ የስሎፎንያ ዜና ኮርፓስ የጦማሪያን አስተያየት እናቀርባለን፤ በዚህም ወሬ በተባለው አካባቢዎች እና ስሜት መካከል የተባለውን ግንኙነት እናሳውቃለን፡፡ እና የስሜት ማውቀት እና የቡድን አቀማመጥ እና ትምህርት መግለጫ የሥርዓት ትምህርት መግለጫ እና በአንድ ደረጃ ምልክት (እድሜት) እና ምሳሌዎችን ማሳየት የሚችሉትን ምሳሌዎች ለመግለጥ እና ምሳሌዎችን (ስሜት) እና ምሳሌዎችን (ስሜት - ስሜት Entities) እና መግለጫ እናደርጋለን። የስሎቪያን እና አቀፍ ፖለቲካውያን፣ ድርጅቶች እና ቦታዎች ጋር የሚጠያየቁ የስሎቪያን እና የዓለምአቀፍ ፖለቲካውያን እና የስልፎች ሥርዓቶችን ለመግለጥ ይደረጋል፡፡', 'hy': 'Այս ուսումնասիրության ընթացքում մենք ներկայացնում ենք Սլովենիայի լրատվական կորպոսի ուսումնասիրությունը, որտեղ մենք ուսումնասիրում ենք կոչված էակների և լրատվական զգացմունքների կապը: Մենք առաջարկում ենք մի մեթոդոլոգիա, որը համադրում է անվանումների ճանաչելը և ենթախմբի բացահայտումը՝ նկարագրական կանոնների ուսումնասիրության մեթոդ, որպեսզի բացահայտենք օրինակների խմբեր, որոնք կիսում են նույն դասարանի պիտակ (զգացմունք) և կաղապար (հատ Այս մոտեցումը օգտագործվում է դրական և բացասական զգացմունքների դասարանի կանոնների առաջացման համար, որոնք բացահայտում են հետաքրքիր կաղապարներ, որոնք կապված են տարբեր սլովենյան և միջազգային քաղաքական գործիչների, կազմակերպությունների', 'az': "Bu təhsil içində, Sloveniya haber korpusunun keşif analizisini göstərdik. Biz bu təhsil içində adlı hisslər və hisslər arasındakı şəkildə xəbər verəcəyik. Biz Adlı Entity Recognition və Subgroup Discovery'u birləşdirən bir metodoloji təklif edirik - aynı sınıf etiketini və örneklərini paylaşan nümunələrin qruplarını tanıtmaq üçün müəyyən bir hökm öyrənmək teknikini təklif edirik. Bu tərzim pozitif və negatif hisslər sınıf qanunlarını təşkil etmək üçün istifadə edilir ki, müxtəlif Sloveniya, uluslararası siyasətçilərə, organizasiyalara və yerlərə bağlı olan ilginç örtükləri göstərər.", 'sq': 'Në këtë studim, ne paraqesim një analizë eksploruese të një korpusi slloven të lajmeve, në të cilën ne hetojmë shoqërimin midis njësive të quajtura dhe ndjenjave në lajme. Ne propozojmë një metodologi që kombinon njohjen e emëruar të njësisë dhe zbulimin e nëngrupeve - një teknikë përshkrimtare të mësimit të rregullave për identifikimin e grupeve të shembujve që ndajnë të njëjtin etiketë klase (ndjesi) dhe modelin (karakteristika - njësi të emëruar). Përqasja përdoret për të nxitur rregullat pozitive dhe negative të klasës së ndjenjave që zbulojnë modele interesante lidhur me politikanët, organizatat dhe vendet e ndryshme sllovenë dhe ndërkombëtare.', 'bn': 'In this study, we present an exploratory analysis of a Slovenian news corpus, in which we investigate the association between named entities and sentiment in the news.  আমরা একটি পদ্ধতি প্রস্তাব করি যা নামের এন্টিটি স্বীকৃতি এবং সাবগ্রুপ ডিস্কোরির সাথে সংযুক্ত করে - একটি বিস্তারিত শিক্ষা প্রযুক্তি যা একই ধরনের ক্লাসের লেবেল এই পদ্ধতি ব্যবহার করা হয়েছে ভালো এবং নেতিবাচক ক্লাসের নিয়ম উৎপাদনের জন্য যা বিভিন্ন স্লোভেনিয়ান এবং আন্তর্জাতিক রাজনীতিবিদ, সংস', 'bs': 'U ovom istraživanju predstavljamo istraživačku analizu slovenskog novinskog korpusa, u kojem istražujemo asocijaciju između imenovanih entitata i osjećanja u vijestima. Predlažemo metodologiju koja kombinira prepoznavanje imenovanih entiteta i otkrivanje podgrupa - tehniku učenja pravila za identifikaciju grupa primjera koji dijele istog etiketa klase (osjećaj) i obrazac (karakteristike imenovanih entiteta). Pristup se koristi kako bi izazvao pravila klase pozitivnih i negativnih osjećanja koje otkrivaju zanimljive obrasce povezane sa različitim slovenskim i međunarodnim političarima, organizacijama i lokacijama.', 'ca': "En aquest estudi, presentem una an àlisi exploratoria d'un corps esloveny de notícies, en el qual investigam l'associació entre entitats anomenades i sentiments a les notícies. Proposem una metodologia que combina la reconeixement d'entitats anomenades i la descoberta de subgrups - una tècnica descriptiva d'aprenentatge de regles per identificar grups d'exemples que comparteixen la mateixa etiqueta de classe (sentiment) i patró (característiques - Entitats anomenades). L'enfocament s'utilitza per induir les normes positives i negatives de classe de sentiments que revelen patrons interessants relacionats amb diferents polítics, organitzacions i llocs eslovens i internacionals.", 'cs': 'V této práci představujeme průzkumnou analýzu slovinského zpravodajského korpusu, ve které zkoumáme souvislost mezi jmenovanými subjekty a sentimentem ve zprávách. Navrhujeme metodiku kombinující rozpoznávání pojmenovaných entit a zjištění podskupin s popisnou technikou učení pravidel pro identifikaci skupin příkladů, které sdílejí stejný popis třídy (sentiment) a vzor (funkce pojmenovaných entit). Tento přístup se používá k navození pozitivních a negativních pravidel třídy sentimentů, která odhalují zajímavé vzorce vztahující se k různým slovinským a mezinárodním politikům, organizacím a lokalitám.', 'et': 'Käesolevas uuringus esitame Sloveenia uudistekorpuse uuriva analüüsi, milles uurime seost nimetatud üksuste ja uudiste tundete vahel. Pakume välja metoodika, mis ühendab nimelise üksuse tunnustamise ja alamrühma avastamise - kirjeldava reeglite õppemeetodi näidete rühmade tuvastamiseks, mis jagavad sama klassi sildi (sentiment) ja mustrit (funktsioonid - nimelised üksused). Lähenemist kasutatakse positiivsete ja negatiivsete sentimentaalsete klassireeglite esilekutsumiseks, mis paljastavad huvitavaid mustreid, mis on seotud erinevate Sloveenia ja rahvusvaheliste poliitikute, organisatsioonide ja asukohtadega.', 'fi': 'Tässä tutkimuksessa esitämme slovenialaisen uutiskorpusen eksploratiivisen analyysin, jossa tutkimme nimettyjen entiteettien ja uutisten tunteiden välistä yhteyttä. Ehdotamme metodologiaa, jossa yhdistetään Nimettyjen entiteettien tunnistus ja Alaryhmien etsintä - kuvaileva sääntöoppimistekniikka, jolla tunnistetaan esimerkkiryhmiä, joilla on sama luokkamerkki (tunne) ja kuvio (ominaisuudet - Nimetyt entiteetit). Lähestymistapaa käytetään indusoimaan positiivisia ja negatiivisia tunteiden luokkasääntöjä, jotka paljastavat mielenkiintoisia malleja liittyen eri slovenialaisiin ja kansainvälisiin poliitikkoihin, järjestöihin ja paikkoihin.', 'sk': 'V študiji predstavljamo raziskovalno analizo slovenskega korpusa novic, v kateri raziskujemo povezavo med imenovanimi entitetami in čustvom v novicah. Predlagamo metodologijo, ki združuje prepoznavanje imenovanih subjektov in odkrivanje podskupin – opisno tehniko učenja pravil za identifikacijo skupin primerov, ki imajo enako oznako razreda (sentiment) in vzorec (značilnosti – imenovani subjekti). S tem pristopom spodbujamo pravila pozitivnega in negativnega sentimentalnega razreda, ki razkrivajo zanimive vzorce, povezane z različnimi slovenskimi in mednarodnimi politiki, organizacijami in lokacijami.', 'jv': 'Nang ujaran iki, awak dhéwé éntukno karo perusahaan resmi sak sabanjuré karo perusahaan anyar iki, ning keujaran nyong nggawe resmi an a karo perusahaan anyar karo perusahaan karo winih. Awak dhéwé ngerasah sistem sing dibenakno Entty nggawe Kemerdekaan Karo Subgroup Diskutêr - teknik sing oleh nggawe geraksi karo nggawe Kemerdekaan sistem sing berarti Perintah sing berarti ketahanan karo bener Awak dhéwé nggunakake sing ngerasakno akeh basa lan kelas kuwi mau sing gawe nguasakno sing nyimpen winih sing karo pergambar obah-obahan kanggo nguasakno politik sing gawe slowenya karo perusahaan, sampeyan lan sak duluran.', 'he': 'במחקר הזה, אנו מציגים ניתוח חוקרי של קורפוס חדשות סלובני, שבה אנו חוקרים את האיגוד בין ישויות בשם ומרגשות בחדשות. אנו מציעים מטודולוגיה שמשולבת זיהוי איכות בשם וגילוי תחת קבוצה - טכניקת לימוד חוקים תיאורית לזהות קבוצות של דוגמאות שחולקות את אותו תווית כיתה (רגשות) ודפוס (תכונות - איכויות בשמות). הגישה משתמשת כדי לגרום לחוקי שיעור רגשות חיוביים ושליליים שמגלים דפוסים מעניינים שקשורים לפוליטיקאים, ארגונים ומיקומים סלובינים ובינלאומיים שונים.', 'ha': "Daga wannan lõkaci, Munã zo da wani anayya mai ƙidãya na jama'a na Sslovonian, a cikinsa tuna ƙidãya da shirin da ke tsakanin abubuwa da hisia a cikin lãbãran. Tuna buɗe wata hanyoyi wanda ke haɗa sunan Entity Recognition da Subgroup Discover - an canza wa karatun wa masu bayyana kunnuwa na tsari ga jama'a na misãlai da ke raba cikin alama guda (hisia'a) da shirin (misalin - suna). Ana amfani da hanyarwa zuwa ta gabatar da sharĩ'a masu marubuci na fassarar hisani da negative, waɗanda ke nuna wa misãlai masu son sha'awa da masu husũma na daban-daban Sslovoniyan da masu marubũta na kasancẽwa.", 'bo': 'འདི་ལྟ་བུའི་ནང་དུ་ང་ཚོར་སྐྱེན་ཡོད་པའི་སྒོང་ཡིག་གི་བརྡ་སྤྱི་ཚོགས་ཀྱི་དཔྱད་ཞིག་འཚོལ་ཞིབ་བྱས་པ་དང་། གསར་འགོད་ནང་གི་དབུལ་དང་ We propose a methodology that combines Named Entity Recognition and Subgroup Discovery - a descriptive rule learning technique for identifying groups of examples that share the same class label (sentiment) and pattern (features - Named Entities). འདི་ལྟ་བུའི་ཐབས་ལམ་དེ་སྤྱད་ནས་མཐུན་རྐྱེན་དང་་སྐྱུན་རྐྱེན་གྱི་ཐབས་ལམ་ལུགས་གཅིག'}
{'en': 'Creating an Aligned Russian Text Simplification Dataset from Language Learner Data', 'ar': 'إنشاء مجموعة بيانات تبسيط النص الروسي المحاذاة من بيانات متعلم اللغة', 'fr': "Création d'un jeu de données de simplification de texte russe aligné à partir des données de l'", 'pt': 'Criando um conjunto de dados de simplificação de texto russo alinhado a partir de dados do aluno de idiomas', 'es': 'Creación de un conjunto de datos de simplificación de texto en ruso alineado a partir de datos', 'ja': '言語学習者データから整列したロシア語テキスト簡略化データセットを作成する', 'zh': '自语言学者数据创对齐俄语文本简化数据集', 'ru': 'Создание согласованного набора данных об упрощении русского текста на основе данных об изучающих язык лицах', 'hi': 'भाषा शिक्षार्थी डेटा से एक संरेखित रूसी पाठ सरलीकरण डेटासेट बनाना', 'ga': 'Tacar Sonraí Simplithe Téacs Rúisis Ailínithe a Chruthú ó Shonraí Foghlaimeora Teanga', 'hu': 'Egy összehangolt orosz szöveg egyszerűsítési adatkészlet létrehozása nyelvtanulói adatokból', 'ka': 'Name', 'el': 'Δημιουργία ενός ευθυγραμμισμένου συνόλου δεδομένων απλοποίησης ρωσικού κειμένου από δεδομένα εκμάθησης γλωσσών', 'it': 'Creazione di un set di dati di semplificazione del testo russo allineato dai dati degli studenti di lingue', 'kk': 'Тілді оқу деректерінен түрлендірілген руссия мәтінді қарапайым деректер қорын құру', 'lt': 'Sukurti suderintą rusų teksto supaprastinimo duomenų rinkinį iš kalbų mokytojų duomenų', 'mk': 'Креирање на податок за распоредено руско текстово едноставување од податоци за учење јазици', 'ms': 'Mencipta Set Data Simplifikasi Teks Rusia Berjajaran dari Data Pelajar Bahasa', 'ml': 'Language Leader Data', 'no': 'Opprettar ein justert russisk tekst- forenklingsdata frå språk- læringsdata', 'mt': 'Il-ħolqien ta’ Sett ta’ Dejta Alinjat ta’ Simplifikazzjoni tat-Test Russu minn Dejta ta’ Tagħlim tal-Lingwi', 'mn': 'Холбоо сурагчийн өгөгдлийн нэвтрүүлэгч Оросын Текст Хялбарлалтын өгөгдлийг бүтээх', 'pl': 'Tworzenie wyrównanego zestawu danych dotyczących uproszczenia tekstu rosyjskiego z danych uczących się języka', 'si': 'Name', 'so': 'Muujinta macluumaadka barbaarinta luqada', 'ro': 'Crearea unui set de date de simplificare a textului rusesc aliniat din datele învățătorilor de limbi străine', 'sr': 'Stvaranje podataka o pojednostavljivanju ruskog teksta iz podataka učitelja jezika', 'sv': 'Skapa ett anpassat dataset för förenklad rysk text från språkinlärningsdata', 'ur': 'Name', 'ta': 'Name', 'uz': 'Name', 'vi': 'Tạo ra một bộ nhớ đơn giản văn bản Nga hợp pháp từ dữ liệu Leaner', 'bg': 'Създаване на подравнен набор от данни за опростяване на руски текст от данни на изучаващия езици', 'da': 'Oprettelse af et tilpasset datasæt til forenkling af russisk tekst fra sproglærerdata', 'hr': 'Stvaranje podataka o jednostavljivanju ruskog teksta iz podataka učitelja jezika', 'de': 'Erstellen eines ausgerichteten russischen Textvereinfachungsdatensatzes aus Daten von Sprachenlernern', 'ko': '언어 학습자 데이터에서 정렬된 러시아어 텍스트 간소화 데이터 집합 만들기', 'fa': 'ایجاد داده\u200cهای ساده\u200cسازی متن روسیه\u200cای از داده\u200cهای یادآوری زبان', 'nl': 'Een uitgelijnde Russische tekstvereenvoudigingsdataset maken op basis van gegevens van taalleerders', 'tr': 'Dil öwrendirici Maglumatdan çykylýan Russiýa Metin Besitlendirmek Maglumaty Bejer', 'af': 'Name', 'id': 'Membuat Set Data Simplifikasi Teks Rusia Beraliran dari Data Pelajar Bahasa', 'sq': 'Krijimi i një baze të dhënash të rregulluara ruse për thjeshtimin e tekstit nga të dhënat e mësuesve të gjuhës', 'am': 'ፋይል sን መክፈት አልቻለም፦ %s፦ %s', 'hy': 'Հաճեցված ռուսական տեքստի պարզաբանության տվյալներ ստեղծելը լեզվի սովորողի տվյալներից', 'sw': 'Creating an Aligned Russian Text Simplification Dataset from Language Learner Data', 'bn': 'ভাষা শিক্ষার তথ্য থেকে রাশিয়ান টেক্সট সাধারণ তথ্য তৈরি করা হচ্ছে', 'bs': 'Stvaranje podataka o jednostavljivanju ruskog teksta iz podataka učitelja jezika', 'cs': 'Vytvoření zarovnané datové sady pro zjednodušení ruského textu z dat učitele jazyka', 'et': 'Joondatud vene teksti lihtsustamise andmekogumi loomine keeleõppija andmetest', 'az': 'Dil √Ėyr…ôn…ôn M…ôlumatdan Q…ôrcl…ôndiril…ôn Rus Metin YapńĪŇümasńĪ M…ôlumatńĪ QurulmasńĪ', 'ca': "Crear un conjunt de dades allinjats de simplificació del text russo a partir de dades d'aprenentatge de llenguatges", 'fi': 'Tasatun venäjänkielisen tekstin yksinkertaistamistietosarjan luominen kielioppijan tiedoista', 'jv': 'politenessoffpolite"), and when there is a change ("assertivepoliteness', 'ha': 'KCharselect unicode block name', 'he': 'יצירת ערכת נתונים של פשטות טקסט רוסית מסוימת מידע למלמד שפות', 'sk': 'Ustvarjanje poravnanega nabora podatkov za poenostavitev ruskega besedila iz podatkov učencev jezika', 'bo': 'སྐད་རིགས་སློབ་པའི་ཆ་འཕྲིན་ནས་གོ་རྒྱུན་སྒྲིག་ཡོད་པའི་ཡིག་གེ་སྟངས་འཛུགས་ཀྱི་ཆ་འཕྲིན་ཞིབ་གསར་བཟོ་བ'}
{'en': 'Parallel language corpora where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for English and Simple English, but many other languages lack such data. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of Russian literature texts adapted for learners of Russian as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.', 'ar': 'يمكن استخدام مجموعة اللغات الموازية حيث يتم محاذاة النصوص العادية مع إصداراتها المبسطة في كل من معالجة اللغة الطبيعية والدراسات اللغوية النظرية. إنها ضرورية لمهمة التبسيط التلقائي للنص ، ولكنها يمكن أن توفر أيضًا رؤى قيمة حول الخصائص التي تجعل النصوص أكثر سهولة وتكشف عن الاستراتيجيات التي يستخدمها الخبراء البشريون لتبسيط النصوص. يوجد اليوم عدد قليل من مجموعات البيانات المتوازية للغة الإنجليزية والإنجليزية البسيطة ، لكن العديد من اللغات الأخرى تفتقر إلى مثل هذه البيانات. في هذه الورقة ، نصف عملنا على إنشاء مجموعة بيانات روسية بسيطة متناسقة تتكون من نصوص الأدب الروسي التي تم تكييفها لمتعلمي اللغة الروسية كلغة أجنبية. ستكون هذه أول مجموعة بيانات موازية في هذا المجال ، وواحدة من أولى مجموعات البيانات الروسية البسيطة بشكل عام.', 'es': 'Los corpus lingüísticos paralelos en los que los textos regulares están alineados con sus versiones simplificadas se pueden utilizar tanto en el procesamiento del lenguaje natural como en los estudios lingüísticos teóricos. Son esenciales para la tarea de simplificación automática de textos, pero también pueden proporcionar información valiosa sobre las características que hacen que los textos sean más accesibles y revelan estrategias que los expertos humanos utilizan para simplificar los textos. Hoy en día, existen algunos conjuntos de datos paralelos para inglés e inglés simple, pero muchos otros idiomas carecen de esos datos. En este artículo describimos nuestro trabajo en la creación de un conjunto de datos alineado ruso-ruso simple compuesto por textos de literatura rusa adaptados para estudiantes de ruso como lengua extranjera. Este será el primer conjunto de datos paralelo en este dominio y uno de los primeros conjuntos de datos rusos simples en general.', 'fr': "Les corpus en langage parallèle où les textes réguliers sont alignés sur leurs versions simplifiées peuvent être utilisés à la fois dans le traitement du langage naturel et dans les études linguistiques théoriques. Ils sont essentiels à la tâche de simplification automatique du texte, mais peuvent également fournir des informations précieuses sur les caractéristiques qui rendent les textes plus accessibles et révèlent les stratégies utilisées par les experts humains pour simplifier les textes. Aujourd'hui, il existe quelques ensembles de données parallèles pour l'anglais et l'anglais simple, mais de nombreuses autres langues ne disposent pas de telles données. Dans cet article, nous décrivons notre travail sur la création d'un jeu de données russe-russe simplifié aligné composé de textes littéraires russes adaptés aux apprenants du russe en tant que langue étrangère. Il s'agira du premier jeu de données parallèle dans ce domaine, et de l'un des premiers ensembles de données russes simples en général.", 'ja': '自然言語処理と理論言語学的研究の両方で、通常のテキストがその簡略化されたバージョンと整列している並列言語コーパスを使用することができます。これらは、自動テキスト簡素化のタスクに不可欠ですが、テキストをよりアクセスしやすくする特徴についての貴重な洞察を提供し、テキストを簡素化するために人間の専門家が使用する戦略を明らかにすることもできます。今日、英語と単純英語の並列データセットはいくつか存在しますが、他の多くの言語はそのようなデータを欠いています。この論文では、外国語としてのロシア語の学習者に適応したロシア語の文献テキストで構成される整列したロシア語-単純ロシア語データセットの作成に関する私たちの仕事について説明します。これは、このドメインで最初の並列データセットであり、一般的に最初のシンプルなロシア語データセットの1つになります。', 'zh': '平行语言语料库,其常文本与其简化版本相保,可施于自然语言理语言学。 其于文本简化之务至重,然亦可以为使文本更易访问之见,而揭人伦专家以简文本之策。 今英语简英语有并行数集,而他语多阙其数。 本文之中,述我创对齐之俄语 - 俄语略数集之事,宜数集由宜俄语为外语学者俄语文学文本。 此将领域之首并行数据集,亦一简俄语数集之一也。', 'hi': 'समानांतर भाषा निगम जहां नियमित ग्रंथों को उनके सरलीकृत संस्करणों के साथ संरेखित किया जाता है, का उपयोग प्राकृतिक भाषा प्रसंस्करण और सैद्धांतिक भाषाई अध्ययन दोनों में किया जा सकता है। वे स्वचालित पाठ सरलीकरण के कार्य के लिए आवश्यक हैं, लेकिन उन विशेषताओं में मूल्यवान अंतर्दृष्टि भी प्रदान कर सकते हैं जो ग्रंथों को अधिक सुलभ बनाते हैं और उन रणनीतियों को प्रकट करते हैं जो मानव विशेषज्ञ ग्रंथों को सरल बनाने के लिए उपयोग करते हैं। आज, अंग्रेजी और सरल अंग्रेजी के लिए कुछ समानांतर डेटासेट मौजूद हैं, लेकिन कई अन्य भाषाओं में इस तरह के डेटा की कमी है। इस पेपर में हम एक विदेशी भाषा के रूप में रूसी के शिक्षार्थियों के लिए अनुकूलित रूसी साहित्य ग्रंथों से बना एक संरेखित रूसी-सरल रूसी डेटासेट बनाने पर हमारे काम का वर्णन करते हैं। यह इस डोमेन में पहला समानांतर डेटासेट होगा, और सामान्य रूप से पहले सरल रूसी डेटासेट में से एक होगा।', 'pt': 'Corpora de linguagem paralela onde textos regulares são alinhados com suas versões simplificadas podem ser usados tanto em processamento de linguagem natural quanto em estudos linguísticos teóricos. Eles são essenciais para a tarefa de simplificação automática de texto, mas também podem fornecer informações valiosas sobre as características que tornam os textos mais acessíveis e revelar estratégias que especialistas humanos usam para simplificar textos. Hoje, existem alguns conjuntos de dados paralelos para inglês e inglês simples, mas muitos outros idiomas não possuem esses dados. Neste artigo, descrevemos nosso trabalho na criação de um conjunto de dados alinhado russo-russo simples composto de textos de literatura russa adaptados para alunos de russo como língua estrangeira. Este será o primeiro conjunto de dados paralelo neste domínio e um dos primeiros conjuntos de dados russos simples em geral.', 'ru': 'Параллельные языковые корпуса, где обычные тексты согласованы с их упрощенными версиями, могут использоваться как в обработке естественного языка, так и в теоретических лингвистических исследованиях. Они имеют важное значение для задачи автоматического упрощения текста, но могут также дать ценное представление о характеристиках, которые делают тексты более доступными и раскрывают стратегии, которые эксперты-человеки используют для упрощения текстов. Сегодня существует несколько параллельных наборов данных для английского языка и простого английского языка, но многим другим языкам не хватает таких данных. В данной работе мы описываем нашу работу по созданию согласованного русско-простого русского набора данных, состоящего из текстов русской литературы, адаптированных для изучающих русский язык как иностранный. Это будет первый параллельный набор данных в этой области, и один из первых Простых русских наборов данных в целом.', 'ga': 'Is féidir corpora teanga comhthreomhar ina bhfuil téacsanna rialta ailínithe lena leaganacha simplithe a úsáid i bpróiseáil teanga nádúrtha agus i staidéir theoiriciúil theangeolaíocha. Tá siad riachtanach chun téacs a shimpliú go huathoibríoch, ach féadann siad léargais luachmhara a sholáthar freisin ar na saintréithe a dhéanann rochtain níos fearr ar théacsanna agus a nochtann straitéisí a úsáideann saineolaithe daonna chun téacsanna a shimpliú. Sa lá atá inniu ann, tá cúpla tacar sonraí comhthreomhara ann don Bhéarla agus do Bhéarla Simplí, ach tá easpa sonraí dá leithéid ag go leor teangacha eile. Sa pháipéar seo déanaimid cur síos ar ár gcuid oibre ar thacar sonraí ailínithe Rúisis-Simplí na Rúise a chruthú comhdhéanta de théacsanna litríochta na Rúise arna gcur in oiriúint d’fhoghlaimeoirí na Rúise mar theanga iasachta. Beidh sé seo ar an gcéad tacar sonraí comhthreomhara sa bhfearann seo, agus ar cheann de na chéad thacair sonraí Simplí Rúisise go ginearálta.', 'el': 'Παράλληλα σώματα γλωσσών όπου τα κανονικά κείμενα ευθυγραμμίζονται με τις απλοποιημένες εκδόσεις τους μπορούν να χρησιμοποιηθούν τόσο στην επεξεργασία φυσικής γλώσσας όσο και στις θεωρητικές γλωσσικές σπουδές. Είναι ουσιώδεις για το έργο της αυτόματης απλούστευσης κειμένου, αλλά μπορούν επίσης να παρέχουν πολύτιμες πληροφορίες σχετικά με τα χαρακτηριστικά που καθιστούν τα κείμενα πιο προσβάσιμα και αποκαλύπτουν στρατηγικές που χρησιμοποιούν οι ανθρώπινοι εμπειρογνώμονες για την απλοποίηση των κειμένων. Σήμερα, υπάρχουν μερικά παράλληλα σύνολα δεδομένων για τα Αγγλικά και τα Απλά Αγγλικά, αλλά πολλές άλλες γλώσσες δεν έχουν τέτοια δεδομένα. Στην παρούσα εργασία περιγράφουμε την εργασία μας για τη δημιουργία ενός ευθυγραμμισμένου Russisch-Απλού Ρωσικού συνόλου δεδομένων που αποτελείται από κείμενα ρωσικής λογοτεχνίας προσαρμοσμένα για μαθητές της ρωσικής ως ξένης γλώσσας. Αυτό θα είναι το πρώτο παράλληλο σύνολο δεδομένων σε αυτόν τον τομέα, και ένα από τα πρώτα απλά ρωσικά σύνολα δεδομένων γενικά.', 'hu': 'A párhuzamos nyelvi korpuszok, ahol a rendszeres szövegeket egyszerűsített változatukhoz igazítják, mind a természetes nyelvi feldolgozásban, mind az elméleti nyelvi tanulmányokban használhatók. Elengedhetetlenek az automatikus szövegegyszerűsítés feladatához, de értékes betekintést nyújthatnak azokba a jellemzőkbe, amelyek hozzáférhetővé teszik a szövegeket, és felfedik azokat a stratégiákat, amelyeket az emberi szakértők a szövegek egyszerűsítésére használnak. Ma már létezik néhány párhuzamos adatkészlet az angol és az egyszerű angol számára, de sok más nyelv hiányzik ilyen adatok. Ebben a tanulmányban bemutatjuk az orosz-egyszerű orosz adatkészlet létrehozására irányuló munkánkat, amely orosz irodalmi szövegekből áll, idegen nyelvű tanulók számára. Ez lesz az első párhuzamos adatkészlet ezen a területen, és általában az egyik első Simple orosz adatkészlet.', 'ka': 'Parallel language corpora, where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. ისინი ავტომატური ტექსტის განხორციელების დავალებისთვის უფრო მნიშვნელოვანია, მაგრამ შეიძლება ასევე უფრო მნიშვნელოვანი ინფორმაციების შესახებ, რომლებიც ტექსტის უფრო accessible და გაახსნა სტ დღეს ინგლისური და განსაკუთრებული ანგლისური მონაცემების რამდენიმე პარალელური მონაცემები არსებობს, მაგრამ ბევრი სხვა ენები არსებობს ასეთი მო ამ დოკუნში ჩვენ აღწერეთ ჩვენი სამუშაო, რომელიც პროსია ლიტერატურის ტექსტის შექმნა როგორც გარეშე ენაზიდან გადასწავლებელებისთვის, როგორც პროსია-უკეთესი სი ეს იქნება პირველი პარალელური მონაცემები დემომინში, და ერთი პირველი პირველი პირველი პირველი პირველური მონაცემების საერთო.', 'mk': 'Паралелна јазичка корпора каде регуларните тексти се усогласени со нивните поедноставени верзии може да се користи и во природното обработување јазик, и во теоретските јазични студии. Тие се суштински за задачата на автоматското поедноставување на текстот, но исто така можат да обезбедат вредни информации за карактеристиките кои ги прават текстите попристапни и откриваат стратегии кои човечките експерти ги користат за поедноставување на текстите. Денес постојат неколку паралелни податоци за англиски и едноставен англиски, но многу други јазици немаат такви податоци. Во овој весник ја опишуваме нашата работа за создавање на уредно руско-едноставно руско податоци составено од руски литературни тексти адаптирани за учениците на руски како странски јазик. Ова ќе биде првиот паралелен податок во овој домен, и еден од првите едноставни руски податоци генерално.', 'it': "I corpi linguistici paralleli in cui i testi regolari sono allineati con le loro versioni semplificate possono essere utilizzati sia nell'elaborazione del linguaggio naturale che negli studi linguistici teorici. Sono essenziali per il compito di semplificazione automatica del testo, ma possono anche fornire preziose informazioni sulle caratteristiche che rendono i testi più accessibili e rivelano strategie che gli esperti umani utilizzano per semplificare i testi. Oggi esistono alcuni set di dati paralleli per l'inglese e l'inglese semplice, ma molte altre lingue mancano di tali dati. In questo articolo descriviamo il nostro lavoro sulla creazione di un dataset russo-semplice allineato composto da testi di letteratura russa adattati per gli studenti del russo come lingua straniera. Questo sarà il primo dataset parallelo in questo dominio e uno dei primi dataset Simple Russian in generale.", 'lt': 'Paralelinė kalba corpora, kai reguliarius tekstus suderina su supaprastintomis jų versijomis, gali būti naudojama ir natūraliam kalbų apdorojimui, ir teoriniams kalbų tyrimams. Jos labai svarbios automatiniam teksto supaprastinimui atlikti, tačiau taip pat gali suteikti vertingą supratimą apie tekstų prieinamumą užtikrinančias savybes ir atskleisti strategijas, kurias žmogaus ekspertai taiko tekstams supaprastinti. Šiandien egzistuoja keletas lygiagrečių anglų ir paprasto anglų duomenų rinkinių, tačiau tokių duomenų trūksta daugeliui kitų kalbų. Šiame dokumente apibūdiname savo darbą kuriant suderintą Rusijos ir paprasto Rusijos duomenų rinkinį, sudarytą iš Rusijos literatūros tekstų, pritaikytų Rusijos kaip užsienio kalbos mokytojams. Tai bus pirmasis lygiagretus duomenų rinkinys šioje srityje ir vienas iš pirmųjų paprastų Rusijos duomenų rinkinių apskritai.', 'kk': 'Параллельді тіл корпорасы, қалыпты мәтіндері өзінің қарапайым нұсқаларымен теоретикалық тілдерді өңдеу және теоретикалық лингвистикалық зерттеулерде қолданылады. Олар автоматты мәтін қарапайымдастыру тапсырмасына маңызды, бірақ мәтіндерді қарапайымдастыру үшін мәтіндерді қолданатын стратегияларды көмектесу үшін мәтіндерді қарапайым көмектесетін қасиеттерге қара Бүгін ағылшын және қарапайым ағылшын тілдер үшін бірнеше параллел деректер қорлары бар, бірақ басқа тілдер бұл деректер жоқ. Бұл қағазда біз жұмысты Орус-Қарапайым Орус деректер жиынын құру үшін, Орус литературасының оқушыларына сыртқы тіл ретінде адаптацияланған мәтіндерінен құрылған. Бұл домендегі бірінші параллель деректер жиыны және бірінші Қарапайым Орус деректер жиындарының бірі болады.', 'ml': 'സ്വാഭാവികമായ ഭാഷയുടെ പ്രക്രിയഭാഷ കോര്\u200dപ്പോരിയില്\u200d സാധാരണമായ പദാവലികള്\u200d അവരുടെ സാധാരണമായ പതിപ്പുകളോടൊപ്പം ചേര്\u200dക്കുന്ന പ They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts.  ഇന്ന് ഇംഗ്ലീഷും ഇംഗ്ലീഷും സാധാരണ ഡാറ്റാസറ്റുകള്\u200d നിലവിലുണ്ട്, പക്ഷെ മറ്റു പല ഭാഷകള്\u200dക്കും ഇതുപോലുള്ള ഡാ ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d ഞങ്ങളുടെ ജോലി വിശദീകരിക്കുന്നു. റഷ്യന്\u200d സാഹിത്രത്തിലെ പഠിക്കുന്നവര്\u200dക്ക് വിദേശ ഭാഷയായി മാറ്റപ്പെട്ട റുഷ ഇതായിരിക്കും ആദ്യത്തെ പാരല്\u200d ഡാറ്റാല്\u200d സെറ്റ്, ആദ്യത്തെ സാധാരണ റഷ്യന്\u200d ഡാറ്റാസറ്റുകളില്\u200d ഒന്ന്.', 'mt': 'Il-korpra tal-lingwa parallela fejn it-testi regolari huma allinjati mal-verżjonijiet simplifikati tagħhom jistgħu jintużaw kemm fl-ipproċessar tal-lingwa naturali kif ukoll fl-istudji lingwistiċi teoretiċi. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts.  Illum, jeżistu ftit settijiet ta’ dejta paralleli għall-Ingliż u l-Ingliż Simplu, iżda ħafna lingwi oħra m’għandhomx tali dejta. F’dan id-dokument niddeskrivu x-xogħol tagħna fuq il-ħolqien ta’ sett ta’ dejta Russu-Russu Simplu allinjat magħmul minn testi tal-letteratura Russa adattati għal dawk li jitgħallmu tar-Russu bħala lingwa barranija. Dan se jkun l-ewwel sett ta’ dejta parallel f’dan id-dominju, u wieħed mill-ewwel settijiet ta’ dejta sempliċi Russi b’mod ġenerali.', 'ms': 'Korpora bahasa paralel dimana teks biasa disesuaikan dengan versi mudah mereka boleh digunakan dalam kedua-dua pemprosesan bahasa alam dan kajian bahasa teori. Mereka penting untuk tugas pemudahan teks automatik, tetapi juga boleh menyediakan pandangan berharga ke dalam ciri-ciri yang membuat teks lebih capai dan mengungkap strategi yang ahli manusia gunakan untuk mempermudahkan teks. Hari ini, terdapat beberapa set data selari untuk Bahasa Inggeris dan Bahasa Inggeris Mudah, tetapi banyak bahasa lain kekurangan data seperti itu. Dalam kertas ini kami menggambarkan kerja kami untuk mencipta set data Rusia-Rusia sederhana yang disesuaikan terdiri daripada teks literatur Rusia disesuaikan untuk pelajar bahasa Rusia sebagai bahasa asing. Ini akan menjadi set data selari pertama dalam domain ini, dan salah satu set data Rusia sederhana pertama secara umum.', 'mn': 'Байгалийн хэл процесс болон теоретикийн хэлний судалгаанд энгийн текстүүдтэй холбогдож болно. Тэд автоматжуулалтын текст хялбарлах үйл ажилд чухал, гэхдээ мөн мөн текст илүү ашиглах болон хүний мэргэжилтнүүд текст хялбарлахад ашигладаг стратегийг илэрхийлж чадна. Өнөөдөр Англи болон энгийн Англи хэл дээр хэдэн параллел өгөгдлийн сангууд байдаг. Гэхдээ өөр олон хэл ийм өгөгдлийн сангууд байхгүй. Энэ цаасан дээр бид Орос-энгийн Орос өгөгдлийн санг бүтээхэд бидний ажлыг Орос-ын сурагчдын хувьд гадаад хэл болгон зохицуулагдсан зохиол бичсэн. Энэ бол энэ холбооны анхны параллел өгөгдлийн суурь ба анхны Энгийн Орос өгөгдлийн суурь юм.', 'no': 'Parallel språkkkorpora der regulære tekstar er alignert med sine forenkle versjonar kan brukast i både naturspråkkhandsaming og teoretiske språksstudiar. Dei er viktige for oppgåva av automatisk tekstforenkling, men kan også gje verdilege innsyningar i karakteristika som gjer tekstar meir tilgjengelege og opna strategiar som menneske ekspertar brukar for å forenkla tekstar. I dag finst nokre parallelle dataset for engelsk og enkelt engelsk, men mange andre språk manglar slike data. I denne papiret beskriver vi arbeidet vårt på å laga eit samsvara russisk enkelt russisk dataset som er saman av russisk literaturtekst som er tilpassa til lærarar av russisk som eit eksternt språk. Dette vil vera den første parallelle datasettet i denne domenet, og ein av dei første enkle russiske datasettene generelt.', 'pl': 'Równoległe korpusy językowe, w których zwykłe teksty są dostosowane do ich uproszczonych wersji, mogą być wykorzystywane zarówno w przetwarzaniu języka naturalnego, jak i teoretycznych studiach językowych. Są one niezbędne dla zadania automatycznego uproszczenia tekstu, ale mogą również dać cenny wgląd w cechy, które sprawiają, że teksty są bardziej dostępne i ujawniają strategie, które eksperci ludzi stosują w celu uproszczenia tekstów. Obecnie istnieje kilka równoległych zbiorów danych dla angielskiego i prostego angielskiego, ale w wielu innych językach brakuje takich danych. W niniejszym artykule opisujemy nasze prace nad stworzeniem skoordynowanego rosyjsko-prostego zbioru danych złożonego z tekstów literatury rosyjskiej dostosowanych dla uczących się języka rosyjskiego jako obcego. Będzie to pierwszy równoległy zbiór danych w tej dziedzinie i jeden z pierwszych zbiorów danych prostych rosyjskich w ogóle.', 'sr': 'Paralelna jezička korpora u kojoj se redovni teksti usklađuju sa njihovim pojednostavljenim verzijama mogu koristiti u prirodnoj obradi jezika i teoretičkim jezičkim studijama. Oni su ključni za zadatak automatskog pojednostavljanja teksta, ali mogu takođe pružiti vrijedne uvide u karakteristike koje čine tekste dostupnijim i otkrivaju strategije koje ljudski stručnjaci koriste za jednostavljanje teksta. Danas postoji nekoliko paralelnih podataka za engleski i jednostavan engleski, ali mnogi drugi jezici nedostaju takvi podaci. U ovom papiru opisujemo naš rad o stvaranju usklađenog ruskog jednostavnog ruskog podataka, sastavljenog od ruskog teksta literatura prilagođenih za učenike ruskog kao stranog jezika. Ovo će biti prva paralelna seta podataka u ovom domenu, i jedna od prvih jednostavnih ruskih podataka općenito.', 'ro': 'Corporele lingvistice paralele în care textele regulate sunt aliniate cu versiunile lor simplificate pot fi utilizate atât în procesarea limbajului natural, cât și în studiile lingvistice teoretice. Ele sunt esențiale pentru sarcina simplificării automate a textului, dar pot oferi, de asemenea, informații valoroase asupra caracteristicilor care fac textele mai accesibile și dezvăluie strategiile pe care experții umani le folosesc pentru a simplifica textele. Astăzi, există câteva seturi de date paralele pentru engleză și engleză simplă, dar multe alte limbi lipsesc astfel de date. În această lucrare descriem lucrarea noastră privind crearea unui set de date ruso-rusă simplă aliniat compus din texte de literatură rusă adaptate cursanților de limbă rusă ca limbă străină. Acesta va fi primul set de date paralel din acest domeniu și unul dintre primele seturi de date Simple Russian în general.', 'si': 'සාමාන්\u200dය භාෂාව කොර්පෝරා තියෙන්නේ සාමාන්\u200dය පාළුවන් සංවිධානය කරලා තියෙන්නේ ඔවුන්ගේ සාමාන්\u200dය සංවිධ ඔවුන් ස්වයංක්\u200dරිය පාළුවන් සාමාන්\u200dය විශේෂණය සඳහා වැඩක් වෙනුවෙන් අවශ්\u200dයයි, නමුත් පාළුවන් විශේෂතාවට වඩා විශේෂතාවක් ස අද, ඉංග්\u200dරීසි සහ සාමාන්\u200dය ඉංග්\u200dරීසි වලට සමාන්\u200dය දත්ත සෙට් කිහිපයක් තියෙනවා, ඒත් අනික් භාෂා මේ පත්තරේ අපි අපේ වැඩේ විස්තර කරනවා රුසියානු සාමාන්\u200dය රුසියානු දත්ත සෙට් හදන්න රුසියානු සාක්ෂික පාළුවන්ගේ ලිපිය මේක තමයි මේ ඩෝමේන් එකේ පළමු සාමාන්\u200dය දත්ත සෙට්, සහ පළමු සාමාන්\u200dය රුසියානු දත්ත සෙට් එකක්.', 'so': 'Shirkadaha luuqada kala duduwan ee ku qoran qoraalka caadiga ah oo ay ku qoran yihiin warqadooda sahlan, waxaa lagu isticmaali karaa baaraandegista luqada dabiicadda ah iyo waxbarashada luuqadaha cilmiga ah ee fikradda. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts.  Maanta, waxaa jira dhawr kooban macluumaad oo lambarka ah oo loogu talagalay Ingiriis iyo Ingiriis fudud, laakiin luuqado kale oo kala duduwan ayaa u baahan macluumaadkaas oo kale. Qoraalkan waxaynu ku qoraynaa shaqadeeda ku saabsan abuuridda shabakada Ruushka ee fudud ee Ruushka oo ka mid ah qoraalka warqada Ruushka oo loogu beddelay barashada Ruushka oo af qalaad ah. Tani waxay noqon doontaa macluumaadka ugu horeeya ee lambarka ah ee gurigan, mid ka mid ah kooxaha ugu horeeya ee Ruushka ee ugu fudud.', 'sv': 'Parallellspråkskorpor där vanliga texter anpassas till sina förenklade versioner kan användas både i naturspråksbehandling och i teoretiska språkstudier. De är väsentliga för uppgiften att förenkla texten automatiskt, men kan också ge värdefulla insikter i de egenskaper som gör texter mer tillgängliga och avslöjar strategier som mänskliga experter använder för att förenkla texter. Idag finns det ett fåtal parallella datauppsättningar för engelska och enkel engelska, men många andra språk saknar sådana data. I denna uppsats beskriver vi vårt arbete med att skapa ett anpassat rysk-enkel rysk dataset bestående av ryska litteraturtexter anpassade för elever av ryska som främmande språk. Detta kommer att vara den första parallella datauppsättningen inom denna domän, och en av de första Simple Russian datauppsättningarna i allmänhet.', 'ta': 'வழக்கமான உரைகள் தங்களுடைய எளிதாக்கப்பட்ட பதிப்புகளுடன் இணைக்கப்பட்ட மொழி நிறுவனத்தில் இயற்கையான மொழி செயல்படுத்தல் மற்றும் தி தானியங்கி உரை எளிதாக்கத்தின் செயலுக்கு அவர்களுக்கு முக்கியமானது, ஆனால் உரைகளை மிகவும் அணுகலாக்கும் மற்றும் தெளிவாக்கும் திட்டங்களை மனித சிறப்பா இன்று, ஆங்கிலம் மற்றும் எளிய ஆங்கிலத்திற்கும் சில இணைய தரவுத்தளங்கள் இருக்கிறது, ஆனால் பல மொழிகளுக்கு இது போன இந்த காகிதத்தில் நாம் எங்கள் வேலையை விளக்குகிறோம் ஒரு ரஷ்ஷிய- எளிதான ரஷிய தகவல் அமைப்பை உருவாக்கும் பொழுது ருஷ்யன் நிரல் மொ இந்த களத்தில் முதல் இணைப்பு தகவல் அமைப்பு, மற்றும் முதல் எளிய ரஷ்ய தரவுத்தளங்களில் ஒன்று.', 'ur': 'Parallel language corpora where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. یہ آٹوٹی ٹیکسٹ سفالت کے کام کے لئے ضرورت ہیں، لیکن وہ اس شخصیت کے بارے میں بھی ارزش دار بصیرت دے سکتے ہیں جو ٹیکسٹ کو زیادہ دسترسی حاصل کرتا ہے اور اس طرح استراتژی ظاہر کرتا ہے جن کو انسان کے متخصص متخصص کو متخصص سا آج، انگلیسی اور ساده انگلیسی کے لئے چند سارل ڈیٹ سٹ موجود ہیں، لیکن بہت سے دوسری زبانیں ایسی ڈیٹ کے لائق ہیں. ہم اس کاغذ میں اپنے کام کو روسی-ساده روسی ڈاٹ سٹ بنانے کے بارے میں بیان کرتے ہیں جو روسی لکھائی پڑھنے والوں کے لئے ایک خارجی زبان کے طور پر مطابق کیا گیا ہے۔ یہ اس ڈومین میں پہلی parallel ڈاٹ سٹ ہوگا، اور سب سے پہلی ساده روسی ڈاٹ سٹ میں سے ایک ہے.', 'uz': "@ info: whatsthis Ular avtomatik matn soddalashtirish vazifasi muhim, ammo bu matnlarni qo'llashga qo'llash mumkin va textlarni qo'llash uchun qo'llanmalar qo'llanmalar qo'llash uchun ishlatiladigan strategiyalarni koʻrsatish mumkin. Bugun ingliz va oddiy ingliz tili uchun bir necha parallel maʼlumot setlari mavjud, lekin boshqa tillar bu maʼlumot yoʻq. Bu hujjatda biz Ruscha tili tili o'rganishga bir tashkilgan Ruscha-Similar Ruscha haqida o'rganishga o'rganishga qo'yilgan rasm maʼlumotlar tarkibini yaratishda ishni anglatamiz. Bu domenadagi birinchi parallel maʼlumot tarkibi va birinchi oddiy Ruscha maʼlumotlar toʻplami.", 'vi': 'Có thể sử dụng ngôn ngữ song song song, nếu văn bản đều trùng khớp với phiên bản đơn giản của chúng, cho cả việc xử lý ngôn ngữ tự nhiên lẫn việc lý thuyết. Chúng là những điều cần thiết cho nhiệm vụ đơn giản văn bản tự động, nhưng cũng có thể cung cấp những hiểu biết giá trị về các đặc điểm giúp văn bản dễ tiếp cận hơn và tiết lộ các chiến lược mà chuyên gia dùng để đơn giản văn bản. Ngày nay, có vài bộ dữ liệu song song cho Anh Quốc và Anh Đơn giản, nhưng nhiều ngôn ngữ khác thiếu dữ liệu như vậy. Trong tờ giấy này chúng tôi mô tả công việc của chúng tôi nhằm tạo ra một tập tin liên kết giữa Nga và Nga, gồm các văn bản văn học của Nga, thích nghi với học sinh người Nga như ngôn ngữ ngoại quốc. Đây sẽ là bộ dữ liệu song song đầu tiên trong lãnh vực này, và một trong những tập tin mẫu đầu tiên của Nga.', 'nl': 'Parallele taalcorpora waar reguliere teksten worden uitgelijnd met hun vereenvoudigde versies kunnen worden gebruikt in zowel natuurlijke taalverwerking als theoretische taalstudies. Ze zijn essentieel voor de taak van automatische tekstvereenvoudiging, maar kunnen ook waardevolle inzichten geven in de kenmerken die teksten toegankelijker maken en strategieën onthullen die menselijke experts gebruiken om teksten te vereenvoudigen. Tegenwoordig bestaan er een paar parallelle datasets voor Engels en Eenvoudig Engels, maar veel andere talen missen dergelijke gegevens. In dit artikel beschrijven we ons werk aan het creëren van een uitgelijnde Russisch-Eenvoudige Russische dataset samengesteld uit Russische literatuurteksten aangepast voor studenten van Russisch als vreemde taal. Dit zal de eerste parallelle dataset in dit domein zijn, en een van de eerste Simple Russian datasets in het algemeen.', 'da': 'Parallelsprogkorpora, hvor almindelige tekster er justeret til deres forenklede versioner, kan anvendes både i naturlig sprogbehandling og teoretiske sprogstudier. De er afgørende for opgaven med automatisk tekstforenkling, men kan også give værdifuld indsigt i de egenskaber, der gør tekster mere tilgængelige og afslører strategier, som menneskelige eksperter bruger til at forenkle tekster. I dag findes der et par parallelle datasæt for engelsk og simpelt engelsk, men mange andre sprog mangler sådanne data. I denne artikel beskriver vi vores arbejde med at skabe et tilpasset russisk-simpelt russisk datasæt bestående af russisk litteratur tekster tilpasset til elever i russisk som fremmedsprog. Dette vil være det første parallelle datasæt på dette område, og et af de første Simple Russian datasæt generelt.', 'hr': 'Paralelna jezička korpora u kojoj se redovni teksti usklađuju s njihovim pojednostavljenim verzijama mogu koristiti u prirodnoj obradi jezika i teoretičkim jezičkim ispitivanjima. Oni su ključni za zadatak automatskog pojednostavljanja teksta, ali mogu također pružiti vrijedne uvide u karakteristike koje čine tekste dostupnijim i otkrivaju strategije koje ljudski stručnjaci koriste za jednostavljanje teksta. Danas postoji nekoliko paralelnih podataka za engleski i jednostavan engleski, ali mnogi drugi jezici nedostaju takvi podaci. U ovom papiru opisujemo naš rad o stvaranju usklađenog ruskog jednostavnog ruskog podataka, sastavljenog od ruskog teksta literacije prilagođenih za učenike ruskog kao stranog jezika. Ovo će biti prva paralelna podataka u ovom domenu, a jedan od prvih jednostavnih ruskih podataka općenito.', 'bg': 'Паралелни езикови корпуси, в които редовните текстове са съгласувани с техните опростени версии, могат да се използват както в обработката на естествения език, така и в теоретичните езикови изследвания. Те са от съществено значение за задачата за автоматично опростяване на текста, но могат също така да осигурят ценна представа за характеристиките, които правят текстовете по-достъпни и разкриват стратегии, които човешките експерти използват за опростяване на текстовете. Днес съществуват няколко паралелни набора от данни за английски и прост английски, но много други езици нямат такива данни. В настоящата статия описваме нашата работа по създаването на подравнен руски-прост руски набор от данни, съставен от текстове на руската литература, адаптирани за учащи руски като чужд език. Това ще бъде първият паралелен набор от данни в тази област и един от първите прости руски набори от данни като цяло.', 'ko': '평행 언어 자료 라이브러리의 일반적인 텍스트는 간략한 버전과 일치하여 자연 언어 처리와 이론 언어학 연구에 사용할 수 있다.그것들은 자동 텍스트 간소화 작업에 매우 중요하지만, 텍스트를 더욱 쉽게 접근할 수 있도록 하는 특징을 이해하고, 인류 전문가들이 텍스트를 간소화하는 전략을 제시할 수 있는 가치 있는 견해를 제공할 수도 있다.오늘날 영어와 단순영어는 평행 데이터 집합이 있지만, 많은 다른 언어들은 이런 데이터가 부족하다.본고에서 우리는 통일된 러시아어 단순 러시아어 데이터 집합을 만드는 작업을 묘사했다. 이 데이터 집합은 러시아어를 외국어 학습자로서 적합한 러시아어 문학 텍스트로 구성된다.이것은 이 분야의 첫 번째 병행 데이터 집합이자 첫 번째 간단한 러시아 데이터 집합이 될 것이다.', 'id': 'Korpora bahasa paralel di mana teks biasa disesuaikan dengan versi sederhana mereka dapat digunakan dalam proses bahasa alam dan studi bahasa teori. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts.  Hari ini, ada beberapa set data paralel untuk Bahasa Inggris dan Bahasa Inggris sederhana, tetapi banyak bahasa lain kekurangan data seperti itu. Dalam kertas ini kami menjelaskan pekerjaan kami untuk menciptakan set data Rusia-Rusia sederhana yang disesuaikan dari teks literatur Rusia yang disesuaikan untuk pelajar bahasa Rusia sebagai bahasa asing. Ini akan menjadi set data paralel pertama di domain ini, dan salah satu set data Rusia sederhana pertama secara umum.', 'de': 'Parallele Sprachkorpora, in denen reguläre Texte auf ihre vereinfachten Versionen ausgerichtet sind, können sowohl in der natürlichen Sprachverarbeitung als auch in der theoretischen Sprachwissenschaft verwendet werden. Sie sind essentiell für die Aufgabe der automatischen Textvereinfachung, können aber auch wertvolle Einblicke in die Merkmale liefern, die Texte zugänglicher machen und Strategien aufzeigen, mit denen menschliche Experten Texte vereinfachen. Heute gibt es einige parallele Datensätze für Englisch und Simple English, aber vielen anderen Sprachen fehlen solche Daten. In diesem Beitrag beschreiben wir unsere Arbeit an der Erstellung eines ausgerichteten Russisch-Simple Russian Datensatzes bestehend aus russischen Literaturtexten, die für Lernende von Russisch als Fremdsprache angepasst sind. Dies wird der erste parallele Datensatz in dieser Domäne und einer der ersten Simple Russian Datensätze im Allgemeinen sein.', 'fa': 'شرکت زبان پاراللی جایی که متن های معمولی با نسخه های ساده\u200cشده\u200cشان در پروسه\u200cهای زبان طبیعی و تحقیقات زبان نظریه استفاده می\u200cشود. آنها برای کار ساده\u200cسازی متن خودکار ضروری دارند، ولی می\u200cتوانند به ویژگی\u200cهایی که متن\u200cها را دسترسی\u200cتر می\u200cدهند و استراتژی\u200cهایی را که متخصص\u200cها برای ساده\u200cسازی متن\u200cها استفاده می\u200cکنند، برای استفاده از متن\u200cهای انسان استفاده می\u200cکنند،  امروز، چند مجموعه داده\u200cهای متفاوتی برای انگلیسی و انگلیسی ساده وجود دارد، ولی بسیاری از زبانهای دیگر چنین داده\u200cها را لازم دارند. در این کاغذ ما کارمون را توصیف می\u200cکنیم که یک مجموعه داده\u200cهای روسیه و ساده روسیه\u200cای متصل شده از متن\u200cهای ادبیات روسیه برای دانش آموزان روسیه به عنوان زبان خارجی متصل شده است. این اولین مجموعه داده\u200cهای پارالی در این دامنه است، و یکی از اولین مجموعه داده\u200cهای ساده روسیه در general.', 'af': "Parallele taal korpora waar gewone teks gelyk word met hulle eenvoudige weergawes kan gebruik word in beide natuurlike taal verwerking en teorieese lingvisse studie. Hulle is noodsaaklik vir die taak van outomatiese teks vereenvoudiging, maar kan ook waardelike insigs verskaf binne die karakteristieke wat teks meer toeganklik maak en strategies wat menslike eksperte gebruik word om teks te vereenvoudig. Vandag bestaan daar 'n paar parallele datastel vir Engels en eenvoudige Engels, maar baie ander tale het sodanige data ontbreek. In hierdie papier beskrywe ons werk op die skep van 'n gelyk Russiese-Eenvoudige Russiese datastel wat gemaak is van Russiese literateertekste wat vir die leerners van Russies as 'n vreemde taal aanpas is. Hierdie sal wees die eerste parallele datastel in hierdie domein, en een van die eerste Eenvoudige Russiese datastel in algemeen.", 'tr': 'Parallel dil korporasy, munuň düzgün metinleri diňe eşitlendirilen wersiýalary hem tebigy dil işlemesinde hem teoretik dil işlemesinde ullanýar. Olar otomatik metin esaslaşdyrylmagyň görevi üçin wajypdyr, ýöne metinleri ýeterli ýerleşdirjek we adamlaryň mektuplary ýeňil etmek üçin ulanylan strategiýalaryny açyp bilýärler. Bu gün iňlisçe we basit iňlisçe üçin birnäçe parallel veri setirleri bar, ýöne başga dilleriň beýleki maglumatlary ýok. Bu kagyzda biz işimizi Rus-Besitleri bilen guralýan, Rus edebiýatyýasynyň öwrenmeleri üçin daşary dil hökmünde ýazylýan, çyzgyly bir Rus dili döretmäge tassyklaýarys. Bu bu domyň ilkinji parallel veri setidi, we ilkinji Basit Rus veri setidiniň birisi bolar.', 'sw': 'Kampuni ya lugha ya Parallel ambapo maandiko ya kawaida yanawekwa pamoja na toleo lao rahisi inaweza kutumika katika utafiti wa lugha za asili na utafiti wa lugha za kiuchunguzi. Ni muhimu kwa jukumu la urahisi wa maandishi ya kujitegemea, lakini pia wanaweza kutoa mitazamo ya thamani katika mahusiano yanayofanya maandishi yanayoweza kupatikana zaidi na kuonyesha mikakati ambayo wataalam wanaotumia kutumia ujumbe rahisi. Leo, kuna seti chache zilizofanana kwa Kiingereza na Kiingereza rahisi, lakini lugha nyingine nyingine hazina taarifa kama hizo. Katika gazeti hili tunaelezea kazi zetu kuhusu kutengeneza seti ya data nyepesi ya Urusi na Urusi zilizotengenezwa na maandishi ya fasihi ya Kirusi yaliyobadilishwa kwa wanafunzi wa Urusi kama lugha ya kigeni. Hii itakuwa takwimu za kwanza za za usambazaji katika eneo hili, na moja ya seti za taarifa za kwanza za za Urusi kwa ujumla.', 'sq': 'Korpora e gjuhës paralele ku tekstet e rregullta janë të përshtatshme me versionet e tyre të thjeshta mund të përdoren si në procesimin natyror të gjuhës, ashtu edhe në studimet teorike gjuhësore. Ato janë thelbësore për detyrën e thjeshtimit automatik të tekstit, por mund të ofrojnë gjithashtu kuptime të vlefshme në karakteristikat që i bëjnë tekstet më të pranueshme dhe zbulojnë strategji që ekspertët njerëzorë përdorin për të thjeshtuar tekstet. Sot, ekzistojnë disa të dhëna paralele për anglisht dhe anglisht të thjeshtë, por shumë gjuhë të tjera mungojnë të dhëna të tilla. Në këtë letër ne përshkruajmë punën tonë për krijimin e një grupi të dhënash ruso-ruse të thjeshtë të përbërë nga tekstet e letërsisë ruse të përshtatura për mësuesit e rusit si një gjuhë e huaj. Ky do të jetë kompleti i parë paralel i të dhënave në këtë domeni dhe një nga kompletet e parë të të dhënave të thjeshta ruse në përgjithësi.', 'am': 'የፍጥረት ቋንቋ ማቀናቀል እና የtheoretical ቋንቋ ትምህርት ውስጥ የተቀናቀሉ ጽሑፎች በተቀራረቡበት የቋንቋ ኮርፖርት የሚጠቀሙበት፡፡ ደግሞም ጽሑፎችን ለማቀናቀል ለራሳቸው የጽሑፍ ማቀናቀል ያስፈልጋል፡፡ ዛሬ እንግሊዝኛ እና ቀላል እንግሊዝኛ እና ቀላል አካላቢ ዳታተሮች አሉ ግን ሌሎች ቋንቋዎች እንዲህ ያለ ዳታ አይጎድሉም፡፡ በዚህ ፕሮግራም የራሽኛ ቋንቋ ለማስተማሪዎች የራሽኛ የራሽያ-ቀላል የራሲ የጽሑፎችን የረሽሽኛ የቋንቋ ጽሑፎችን በመፍጠር ላይ ሥራችንን እናሳውቃለን፡፡ ይሄ የዚህ ዶሜን መጀመሪያ ተሳያፊ ዳታዎችን እና የመጀመሪያው ቀላል የሩሽኛ ዳታተሮች አንዱ ነው፡፡', 'hy': '钥铡謤榨宅斋 乍 謪眨湛铡眨崭謤债榨宅 斩铡謬 闸斩铡寨铡斩 宅榨咋站斋 站榨謤铡沾辗铡寨崭謧沾, 斩铡謬 湛榨战铡寨铡斩 宅榨咋站铡闸铡斩铡寨铡斩 崭謧战崭謧沾斩铡战斋謤崭謧诈盏崭謧斩斩榨謤崭謧沾: 源謤铡斩謩 寨铡謤謬崭謤 榨斩 铡站湛崭沾铡湛斋寨 湛榨謩战湛斋 蘸铡謤咋榨謥沾铡斩 窄斩栅謤斋 瞻铡沾铡謤, 闸铡盏謥 寨铡謤崭詹 榨斩 斩铡謬 铡謤摘榨謩铡站崭謤 炸斩寨铡宅崭謧沾斩榨謤 湛铡宅 铡盏斩 瞻铡湛寨铡斩斋辗斩榨謤斋斩, 崭謤崭斩謩 寨栅铡謤毡斩榨斩 湛榨謩战湛斩榨謤炸 铡站榨宅斋 瞻铡战铡斩榨宅斋 謬 闸铡謥铡瞻铡盏湛榨斩 铡盏斩 占铡咋沾铡站铡謤崭謧诈盏崭謧斩斩榨謤炸, 崭謤崭斩謩 沾铡謤栅寨铡盏斋斩 沾铡战斩铡眨榨湛斩榨謤炸 謪眨湛铡眨崭謤债崭謧沾 榨斩 湛榨 员盏战謪謤 眨崭盏崭謧诈盏崭謧斩 崭謧斩斋 沾斋 謩铡斩斋 咋崭謧眨铡瞻榨占 湛站盏铡宅斩榨謤 铡斩眨宅榨謤榨斩斋 謬 蘸铡謤咋 铡斩眨宅榨謤榨斩斋 瞻铡沾铡謤, 闸铡盏謥 辗铡湛 铡盏宅 宅榨咋崭謧斩榨謤 铡盏栅蘸斋战斋 湛站盏铡宅斩榨謤 展崭謧斩榨斩: 员盏战 诈詹诈斋 沾榨栈 沾榨斩謩 斩寨铡謤铡眨謤崭謧沾 榨斩謩 沾榨謤 铡辗窄铡湛铡斩謩炸 諏崭謧战铡战湛铡斩斋-諍铡謤咋 諏崭謧战铡战湛铡斩斋 湛站盏铡宅斩榨謤斋 瞻铡沾铡寨铡謤眨 战湛榨詹债榨宅崭謧 瞻铡沾铡謤, 崭謤炸 寨铡咋沾站铡债 乍 諏崭謧战铡战湛铡斩斋 眨謤铡寨铡斩崭謧诈盏铡斩 湛榨謩战湛榨謤斋謥, 崭謤崭斩謩 瞻铡沾铡蘸铡湛铡战窄铡斩崭謧沾 榨斩 諏崭謧战铡战湛铡斩斋 崭謧战铡斩崭詹斩榨謤斋 瞻铡沾铡謤 崭謤蘸榨战 謪湛铡謤 宅榨咋崭謧 諐铡 寨宅斋斩斋 铡盏战 崭宅崭謤湛崭謧沾 铡占铡栈斋斩 咋崭謧眨铡瞻榨占 湛站盏铡宅斩榨謤斋 瞻铡沾铡寨铡謤眨炸, 謬 炸斩栅瞻铡斩崭謧謤 铡占沾铡沾闸 諏崭謧战铡战湛铡斩斋 铡占铡栈斋斩 蘸铡謤咋 湛站盏铡宅斩榨謤斋 瞻铡沾铡寨铡謤眨炸:', 'az': 'Parallel dil korporası, həmçin in düzgün metinlərin basitləşdirilmiş versiyaları ilə istifadə ediləcəyi təbiətli dil işləməsində və teoretik dil işləməsində də istifadə edilə bilər. Onlar avtomatik metin basitləşdirməsi üçün vacib olarlar, amma insanların məktubları basitləşdirmək üçün istifadə edən məktubları daha faydalandırmaq və təhsil etmək üçün insanların istifadə etdiyi stratejikləri də göstərə bilərlər. Bu gün İngilizce və Basit İngilizce üçün bir neçə paralel veri qurğuları var, amma başqa dillərin çoxu bu məlumatları yoxdur. Bu kağızda biz işimizi Rus-Basit Rus veri setini yaratmaq üçün Rus edebiyatı öyrənənənlərə daxil edilən mətnlərdən oluşdurduq. Bu, bu domandaki ilk paralel veri quruluğu və ilk basit Rus veri quruluğundan biridir.', 'bn': 'প্রাকৃতিক ভাষা প্রক্রিয়া এবং থিওরেটিক্যাল ভাষার গবেষণায় স্বাভাবিক ভাষার সংস্করণের সাথে নিয়মিত লেখা যোগ করা যায়। স্বয়ংক্রিয়ভাবে টেক্সট সুস্কৃতির কাজের জন্য তারা গুরুত্বপূর্ণ, কিন্তু লেখাগুলোর ব্যাপারে মূল্যবান দৃষ্টিভঙ্গি প্রদান করতে পারে যা ল আজকে ইংরেজি এবং সাধারণ ইংরেজীর জন্য কয়েকটি প্যারালেল ডাটাসেট রয়েছে, কিন্তু অনেক ভাষা এরকম তথ্যের অভাব রয়েছে। এই কাগজটিতে আমরা আমাদের কাজের বর্ণনা করি যে রাশিয়ান সাহিত্যের লেখাগুলো বিদেশী ভাষা হিসেবে রুশ শ শিক্ষার্থীদের জন্য তৈরি করা একট This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.', 'et': 'Paralleelseid keelekorporeid, kus tavalised tekstid on kooskõlas lihtsustatud versioonidega, saab kasutada nii looduskeele töötlemisel kui ka teoreetilistes keeleteadustes. Need on olulised teksti automaatse lihtsustamise ülesandeks, kuid võivad anda ka väärtuslikku ülevaadet omadustest, mis muudavad teksti kättesaadavamaks, ja paljastada strateegiad, mida inimeksperdid kasutavad teksti lihtsustamiseks. Tänapäeval on olemas mõned paralleelsed andmekogumid inglise ja lihtsa inglise kohta, kuid paljudes teistes keeltes puuduvad sellised andmed. Käesolevas töös kirjeldame oma tööd vene keele kui võõrkeela õppijatele kohandatud vene kirjanduse tekstidest koosneva ühtlustatud vene-lihtsa vene andmekogumi loomisel. See on esimene paralleelne andmekogum selles valdkonnas ja üks esimesi lihtvene andmekogumeid üldiselt.', 'ca': "El corpora de llenguatges parallels on els textos regulars estan allinjats amb les seves versions simplificades es pot utilitzar tant en el processament natural de llenguatges com en estudis lingüístics teòrics. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts.  Avui, hi ha alguns conjunts de dades parallels per anglès i anglès senzill, però moltes altres llengües no tenen aquestes dades. En aquest article descrivim la nostra feina per crear un conjunt de dades russo-senzill russo compost de textos de literatura russa adaptats als alumnes del russo com a llengua estrangera. Aquest serà el primer conjunt de dades paral·lels d'aquest domini, i un dels primers conjunts de dades senzills russos en general.", 'cs': 'Paralelní jazykové korpusy, kde jsou běžné texty sladěny s jejich zjednodušenými verzemi, mohou být použity jak v zpracování přirozeného jazyka, tak v teoretických jazykových studiích. Jsou nezbytné pro úkol automatického zjednodušení textu, ale mohou také poskytnout cenný pohled na charakteristiky, které zpřístupní texty a odhalují strategie, které lidští odborníci používají ke zjednodušení textů. Dnes existuje několik paralelních datových sad pro angličtinu a jednoduchou angličtinu, ale mnoho dalších jazyků taková data chybí. V tomto článku popisujeme naši práci na vytvoření sladěného rusko-jednoduchého datového souboru složeného z ruských literaturních textů přizpůsobených studentům ruštiny jako cizího jazyka. Jedná se o první paralelní datovou sadu v této doméně a obecně o jeden z prvních jednoduchých ruských datových sad.', 'bs': 'Paralelna jezička korpora u kojoj se redovni teksti usklađuju sa njihovim jednostavnim verzijama mogu koristiti u prirodnim jezičkim procesima i teorijskim jezičkim studijama. Oni su ključni za zadatak automatskog pojednostavljanja teksta, ali mogu također pružiti vrijedne uvide u karakteristike koje čine tekste dostupnijim i otkrivaju strategije koje ljudski stručnjaci koriste za jednostavljanje teksta. Danas postoji nekoliko paralelnih podataka za engleski i jednostavan engleski, ali mnogi drugi jezici nedostaju takvi podaci. U ovom papiru opisujemo naš rad o stvaranju usklađenog ruskog jednostavnog ruskog podataka, sastavljenog od ruskog teksta literacije prilagođenih za učenike ruskog kao stranog jezika. Ovo će biti prva paralelna seta podataka u ovom domenu, i jedna od prvih jednostavnih ruskih podataka općenito.', 'fi': 'Rinnakkaisia kielikorpusia, joissa säännölliset tekstit ovat linjassa yksinkertaistettujen versioiden kanssa, voidaan käyttää sekä luonnollisen kielen käsittelyssä että teoreettisessa kielentutkimuksessa. Ne ovat olennaisia automaattisen tekstin yksinkertaistamisen kannalta, mutta ne voivat myös tarjota arvokasta tietoa ominaisuuksista, jotka tekevät teksteistä helpommin saatavilla, ja paljastaa strategioita, joita ihmiset käyttävät tekstien yksinkertaistamiseen. Nykyään englannin ja yksinkertaisen englannin osalta on olemassa muutamia rinnakkaisia tietokokonaisuuksia, mutta monilla muilla kielillä tällaisia tietoja ei ole. Tässä artikkelissa kuvailemme työtämme luoda venäjän kielen oppijoille sovelletuista venäjän kirjallisuuden teksteistä koostuva yhtenäinen venäjä-yksinkertainen venäjän aineisto. Tämä on ensimmäinen rinnakkaisaineisto tällä alalla ja yksi ensimmäisistä yksinkertaisista venäläisistä aineistoista yleisesti.', 'ha': "Shirin harshen fanel, inda aka haɗa misalin littãfin daidai da sigogi masu sauki, za'a iya amfani da su cikin masu amfani da fassarar harshen asimi da littafan littafi. Suna da muhimu wa aikin rubutun farat ɗaya, kuma amma yana iya ƙayyade gannai masu inganci cikin sifatida, da za'a sami matsayin su da sauri da matsayin da za'a iya sauƙi da kuma su bayyana takiyaikin da masu fitarwa na mutane ke amfani da dõmin a sauƙi matsayin. Daga yau, there masu sami kaɗan da daidaita data masu daidaita wa Ingiriya da Ingiriya mai sauƙi, kuma amma wasu harshe daban na ƙari da wannan data ba. A cikin wannan karatun, Munã bayyana aikinmu na samun ka sami wani danne na Ruushi-Similar Ruushi wanda aka sammentar da littafan rubuci na rususi da aka adage zuwa masu sanar da Ruushi kamar harshe na ajabu. Wannan zai kasance na farkon daidaita data na daidaita cikin wannan Domen, kuma gudan na farkon tsari na Similar Ruusi na jumla.", 'sk': 'Vzporedne jezikovne korpuse, kjer so redna besedila usklajena s poenostavljenimi različicami, se lahko uporabljajo tako v obdelavi naravnega jezika kot v teoretičnem jezikovnem študiju. So bistvenega pomena za nalogo samodejne poenostavitve besedila, lahko pa tudi zagotovijo dragocene vpoglede v značilnosti, zaradi katerih je besedila dostopnejša, in razkrijejo strategije, ki jih človeški strokovnjaki uporabljajo za poenostavitev besedil. Danes obstaja nekaj vzporednih naborov podatkov za angleščino in enostavno angleščino, vendar mnogi drugi jeziki nimajo takih podatkov. V prispevku opisujemo naše delo pri ustvarjanju usklajenega rusko-preprostega ruskega nabora podatkov, sestavljenega iz besedil ruske literature, prilagojenih učencem ruskega jezika kot tujega jezika. To bo prvi vzporedni nabor podatkov na tem področju in eden od prvih enostavnih ruskih naborov podatkov na splošno.', 'he': 'טקסטים רגילים מתאימים לגרסאות הפשוטות שלהם יכולים להשתמש גם בעבודת שפת טבעית וגם במחקרים שפתיים תיאורטיים. הן חיוניות למשימה של הפשטות טקסט אוטומטית, אך הן יכולות גם לספק תובנות ערובות לתחומים שהופכים לטקסטים גישיים יותר ולחשוף אסטרטגיות שהמומחים האנושיים משתמשים כדי לפשט טקסטים. היום, יש כמה קבוצות מידע מקבילות לאנגלית ופשוטה אנגלית, אבל הרבה שפות אחרות חסרות מידע כזה. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of Russian literature texts adapted for learners of Russian as a foreign language.  זו תהיה קבוצת נתונים מקבילה הראשונה בתחום הזה, ואחד מהקבוצות נתונים הרוסיות הפשוטות הראשונות באופן כללי.', 'jv': 'Sampeyan kelas barang Awak dhéwé sing klêngé kanggo nggawe gambar teka-seneng automatik, lan uga iso nggawe barang kanggo ngerasakno perintahaan sing nggawe textin mesthi luwih apik lan mbukakipun striketungsing sing sing paling-perintahaan sing bisa nguasai nggawe textin. user Nang pepulan iki, kéné rambarang nggawe gerapakan ning arep nggawe dataset Rusi-Sampeyan Rusi sing paling berapakan karo texti basa Rusi sing apik kanggo kelas barang resmi. This will be the first parameters in this domain, and one of the first Simple Rusian dataset in General.', 'bo': 'Parallel language corpora where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. རང་འགུལ་གྱི་ཡིག་གེ་ཆེ་སྤྲོད་ཀྱི་བྱ་འགུལ་ལ་དགོས་པ་ཡིན། ཡིག་གི་ཁྱད་ཆོས་ཀྱི་ཁྱད་ཆོས་ཉིད་རྟོགས་ཐུབ་པའི་ཁྱད་ཆོས་རྣམས་རང་འགུལ་གྱི་ཡིག དེ་རིང་ལ། དབྱིན་ཡིག་དང་སྟབས་བདེ་བའི་དབྱིན་ཡིག་ཆ་ལས་ཀྱང་ཉུང་ཅིག་ཡོད་པ་རེད། ང་ཚོས་ཤོག་བུ་འདིའི་ནང་དུ་ང་ཚོའི་ལས་འགན་སྐོར་ལ་ང་ཚོའི་རྩོམ་པ་ཞིག་གིས་སྒྲིག་རྩོམ་སྒྲིག་གི་ རུ་ཤོག་བྱང་ཆོས་སྟབས་ འདི་ནི་domain འདིའི་ནང་དུ་ཆ་དང་པོ་སྒྲིག་གནད་སྡུད་གཞི་རྩིས་ཐོག་དང་གཅིག་ནི་སྤྱིར་བཏང་བ་རྒྱ་ནག་གི་གནད་སྡུད་གཞི་'}
{'en': 'Multilingual Named Entity Recognition and Matching Using BERT and Dedupe for Slavic Languages', 'pt': 'Reconhecimento e correspondência de entidades nomeadas multilíngues usando BERT e desduplicação para idiomas eslavos', 'ar': 'التعرف على الكيان متعدد اللغات ومطابقته باستخدام BERT و Dedupe للغات السلافية', 'es': 'Reconocimiento y correspondencia multilingüe de entidades nombradas mediante BERT y deduplicación para lenguas eslavas', 'fr': "Reconnaissance et mise en correspondance d'entités nommées multilingues à l'aide du BERT et de la déduplication pour les langues slaves", 'ja': 'スラブ語のためのBERTとDEDUPEを使用した多言語の名前付きエンティティ認識とマッチング', 'ru': 'Многоязычное распознавание и сопоставление именованных сущностей с использованием BERT и Dedupe для славянских языков', 'zh': '用 BERT 重复数斯拉夫言语多言名实体', 'hi': 'बहुभाषी नामित एंटिटी पहचान और स्लाव भाषाओं के लिए BERT और Dedupe का उपयोग करके मिलान', 'ga': 'Aithint agus Meaitseáil Aonán Ainmnithe Ilteangach ag Úsáid BERT agus Dedupe do Theangacha Slavacha', 'ka': 'მრავალენგიური სახელი ინტერტის განახლება და შესაბამისი გამოყენება BERT და Dedupe სლავიური ენებისთვის', 'hu': 'Többnyelvű elnevezett entitások felismerése és összeegyeztetése a BERT és a Dedupe használatával szláv nyelveken', 'el': 'Αναγνώριση και αντιστοίχιση πολλών γλωσσών Οντότητας με τη χρήση του και του για τις Σλαβικές Γλώσσες', 'kk': 'Бірнеше тілді аталған нысандарды анықтау және сәйкес келетін BERT және Slavic тілдер үшін қалдыру', 'it': 'Riconoscimento e abbinamento multilingue delle entità nominate utilizzando BERT e Dedupe per le lingue slave', 'lt': 'Multilingual Named Entity Recognition and Matching Using BERT and Dedupe for Slavic Languages', 'ml': 'ബെര്\u200dടി ഉപയോഗിക്കുന്നതും സ്ലാവിക് ഭാഷകള്\u200dക്കുള്ള പേരുള്ള പല ഭാഷ', 'mn': 'Олон хэл нэрлэгдсэн Entity Recognition and Matching Use BERT and Dedupe for Slavic Languages', 'no': 'Fleirspråkskjenning og samsvar med BERT og Dedupe for slaviske språk', 'pl': 'Rozpoznawanie i dopasowanie podmiotów wielojęzycznych przy użyciu BERT i Dedupe dla języków słowiańskich', 'mk': 'Повеќејазично препознавање и совпаѓање на ентитетите со име BERT и Dedupe за рапски јазици', 'ro': 'Recunoașterea și potrivirea multilingvă a entităților denumite utilizând BERT și Dedupe pentru limbile slave', 'ms': 'Pengenalan dan Persamaan Entiti bernama berbilang Menggunakan BERT dan Dedupe untuk Bahasa Slavic', 'mt': 'Ir-Rikonoxximent u t-Tqabbil Multilingwi ta’ Entità Ismija bl-Użu ta’ BERT u Dedupe għal-Lingwi Sklavi', 'sr': 'Prepoznavanje i odgovaranje višejezičkih imenovanih entiteta korištenje BERT i Dedupe za slavicske jezike', 'si': 'ගොඩක් භාෂාවක් නම් නම් ඉන්තිත් අඳුරගන්න සහ සම්බන්ධය BERT සහ ස්ලාවික් භාෂාවට ප්\u200dරයෝජනය කරන්න', 'sv': 'Identifiering och matchning av flerspråkiga namngivna enheter med hjälp av BERT och Dedupe för slaviska språk', 'ur': 'Multilingual Named Entity Recognition and Matching using BERT and Dedupe for Slavic Languages', 'so': 'Isticmaalka BERT iyo Dedupe for Slavic Languages', 'ta': 'Name', 'uz': 'Name', 'vi': 'Phát ngôn ngữ rộng có tên các loài, nhận dạng và kết hợp bằng giao thức thiếu niên cho ngôn ngữ Slavic', 'bg': 'Многоезично разпознаване и съвпадение на имената на субекти с помощта на BERT и Dedupe за славянски езици', 'nl': 'Meertalige naamsbekendheid en matching met behulp van BERT en Dedupe voor Slavische talen', 'hr': 'Prepoznavanje i odgovaranje višejezičkih područja s korištenjem BERT i Dedupe za slavne jezike', 'da': 'Genkendelse og matchning af flersprogede navngivne enheder ved hjælp af BERT og Dedupe til slaviske sprog', 'ko': 'BERT 및 데이터 중복 제거를 사용한 슬라브어 다중 언어 명명 엔티티 인식 및 일치', 'de': 'Mehrsprachige Named Entity Erkennung und Abgleich mit BERT und Dedupe für slawische Sprachen', 'fa': 'شناسایی و هماهنگی از استفاده از BERT و Dedupe برای زبانهای اسلاویک', 'id': 'Pengenalan dan Perpadanan Entitas bernama berbilang Menggunakan BERT dan Dedupe untuk Bahasa Slav', 'sw': 'Utumiaji wa BERT na Ubaguzi wa Lugha za Kislavic', 'tr': 'Birnäçe Diller', 'af': 'Veelvuldige genoem eenheid herken en ooreenstemmende gebruik BERT en Dedupe vir Slavic Taal', 'sq': 'Njohja dhe përputhja e njësisë me emër të shumëgjuhës duke përdorur BERT dhe Dedupe për gjuhët sllavë', 'am': 'የቋንቋ ቋንቋዎች ስም Entity Recognition and Matching using BERT and Dedupe for Slavic ቋንቋዎች', 'hy': 'Բազլեզու անվանված միավորների ճանաչելը և համապատասխանումը, օգտագործելով BER և Dedwe-ը սլավիկ լեզուների համար', 'az': 'Çoxlu dil Adlı Entity Recognition and Matching Use BERT and Dedupe for Slavic Languages', 'bn': 'Name', 'bs': 'Prepoznavanje i odgovaranje višejezičkog imenovanog entiteta korištenje BERT i Dedupe za slavne jezike', 'cs': 'Vícejazyčné rozpoznávání a porovnávání jmenovaných entit pomocí BERT a Dedupe pro slovanské jazyky', 'ca': "Recognició i comparació multilingües d'entitats anomenades utilitzant BERT i Dedupe per a llengües esclaves", 'et': 'Mitmekeelse nimega üksuste tuvastamine ja sobitamine slaavi keelte BERT- ja Dedupe-i abil', 'fi': 'Monikielinen nimetty kokonaisuus tunnistus ja täsmäytys käyttäen BERT ja Dedupe slaavilaisille kielille', 'sk': 'Večjezično prepoznavanje imenovanih subjektov in ujemanje z BERT in Dedupe za slovanske jezike', 'ha': 'KCharselect unicode block name', 'jv': 'Layout', 'he': 'זיהוי ויתואם של יחידות בשמות רבות בשמו בשימוש של BERT ו Dedupe לשפות סלאביות', 'bo': 'སྐད་རིགས་མིང་ཡོད་པའི་ཨིན་ཚན་དང་མཐུན་སྒྲིག་using BERT and Dedupe for Slavic Languages'}
{'en': 'This paper describes the University of Ljubljana (UL FRI) Group’s submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data. We perform training iteratively and on the concatenated data of previously available NER datasets. For the normalization task we use Stanza lemmatizer, while for entity matching we implemented a baseline using the Dedupe library. The performance of evaluations suggests that multi-source settings outperform less-resourced approaches. The best NER models achieve 0.91 F-score on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. In multi-lingual NER setting we achieve F-scores of 0.82 and 0.74.', 'pt': 'Este artigo descreve as submissões do Grupo da Universidade de Ljubljana (UL FRI) para a tarefa compartilhada no Workshop Balto-Slavic Natural Language Processing (BSNLP) 2021. Experimentamos vários modelos baseados em BERT, pré-treinados em dados multilíngues, croata-esloveno-inglês e somente esloveno. Realizamos o treinamento iterativamente e nos dados concatenados de conjuntos de dados NER disponíveis anteriormente. Para a tarefa de normalização, usamos o lematizador Stanza, enquanto para correspondência de entidade implementamos uma linha de base usando a biblioteca Dedupe. O desempenho das avaliações sugere que as configurações de várias fontes superam as abordagens com menos recursos. Os melhores modelos NER atingem 0,91 F-score em divisões de dados de treinamento esloveno, enquanto a melhor submissão oficial alcançou F-scores de 0,84 e 0,78 para correspondência parcial relaxada e configurações estritas, respectivamente. Na configuração NER multilíngue, alcançamos F-scores de 0,82 e 0,74.', 'ar': 'تصف هذه الورقة عمليات إرسال مجموعة جامعة ليوبليانا (UL FRI) إلى المهمة المشتركة في ورشة عمل معالجة اللغة الطبيعية Balto-Slavic (BSNLP) 2021. نجرب مع نماذج متعددة تستند إلى BERT ، مدربة مسبقًا على بيانات متعددة اللغات والكرواتية-السلوفينية-الإنجليزية والسلوفينية فقط. نقوم بالتدريب بشكل متكرر وعلى البيانات المتسلسلة لمجموعات بيانات NER المتوفرة سابقًا. بالنسبة لمهمة التطبيع ، نستخدم Stanza lemmatizer ، بينما بالنسبة للكيان المطابق ، قمنا بتنفيذ خط أساسي باستخدام مكتبة Dedupe. يشير أداء التقييمات إلى أن الإعدادات متعددة المصادر تتفوق في الأداء على الأساليب منخفضة الموارد. تحقق أفضل نماذج NER 0.91 درجة F على تقسيمات بيانات التدريب السلوفيني بينما حقق أفضل تقديم رسمي درجات F تبلغ 0.84 و 0.78 للمطابقة الجزئية المريحة والإعدادات الصارمة ، على التوالي. في إعداد NER متعدد اللغات ، نحقق درجات F تبلغ 0.82 و 0.74.', 'fr': "Cet article décrit les soumissions du groupe de l'Université de Ljubljana (UL FRI) à la tâche partagée lors de l'atelier Balto-Slavic Natural Language Processing (BSNLP) 2021. Nous expérimentons plusieurs modèles basés sur BERT, préformés avec des données multilingues, croate-slovène-anglais et uniquement slovène. Nous effectuons la formation de manière itérative et sur les données concaténées d'ensembles de données NER précédemment disponibles. Pour la tâche de normalisation, nous utilisons le lemmatiseur Stanza, tandis que pour la correspondance des entités, nous avons implémenté une ligne de base à l'aide de la bibliothèque de déduplication. La performance des évaluations suggère que les environnements à sources multiples surpassent les approches moins dotées de ressources. Les meilleurs modèles NER obtiennent un score F de 0,91 sur les divisions de données d'entraînement slovènes tandis que la meilleure soumission officielle a obtenu des scores F de 0,84 et 0,78 pour une correspondance partielle assouplie et des paramètres stricts, respectivement. Dans un environnement NER multilingue, nous obtenons des scores F de 0,82 et 0,74.", 'es': 'Este artículo describe las presentaciones del Grupo de la Universidad de Ljubljana (UL FRI) a la tarea compartida en el taller Balto-Slavic Natural Language Processing (BSNLP) 2021. Experimentamos con múltiples modelos basados en BERT, previamente entrenados en datos multilingües, croata-esloveno-inglés y solo esloveno. Realizamos el entrenamiento de forma iterativa y sobre los datos concatenados de los conjuntos de datos NER previamente disponibles. Para la tarea de normalización utilizamos el lematizador Stanza, mientras que para la coincidencia de entidades implementamos una línea base utilizando la biblioteca Dedupe. El desempeño de las evaluaciones sugiere que los entornos de múltiples fuentes superan a los enfoques con menos recursos. Los mejores modelos NER alcanzaron una puntuación F de 0,91 en las divisiones de datos de entrenamiento eslovenos, mientras que la mejor presentación oficial obtuvo puntuaciones F de 0,84 y 0,78 para ajustes de emparejamiento parcial relajados y estrictos, respectivamente. En un entorno de NER multilingüe, logramos puntuaciones F de 0.82 y 0.74.', 'ja': '本稿では、2021年のバルト・スラブ自然言語処理（ BSNLP ）ワークショップにおける共有タスクへのリュブリャナ大学（ UL FRI ）グループの提出について説明します。複数のBERTベースのモデルを実験し、多言語、クロアチア語、スロベニア語、英語、スロベニア語のみのデータで事前にトレーニングしています。私たちは、以前に利用可能なNERデータセットの連結されたデータに対して反復的にトレーニングを実行します。標準化タスクではStanza lemmatizerを使用し、エンティティマッチングではDedupeライブラリを使用してベースラインを実装しました。評価のパフォーマンスは、マルチソース設定がリソースの少ないアプローチよりも優れていることを示唆しています。最良のNERモデルは、スロベニアのトレーニングデータ分割で0.91 Fスコアを達成し、最良の公式提出物は、リラックスした部分マッチングと厳格な設定でそれぞれ0.84および0.78 Fスコアを達成しました。多言語NER設定では、0.82と0.74のFスコアを達成しました。', 'zh': '本文引卢布尔雅大学(UL FRI)小组在Balto-Slavic Natural Language Processing(BSNLP)2021年研讨会上同务。 余试数BERT,多语言,克罗地亚语 - 斯洛文尼亚语 - 英语与仅斯洛文尼亚语数预训练之。 以迭代前所用 NER 数集数行之。 其于规范化也,吾用 Stanza 词形还原器,而于实体之匹,吾用重数删库基线矣。 论绩效明,多源设优于资源少之术。 最佳NER模在斯洛文尼亚练数分得0.91 F分,而最佳官方提交模形在弛缓者,与严设下分得0.840.78F分。 多言NER置中,F得分为0.82与0.74。', 'hi': 'यह पेपर बाल्टो-स्लाव प्राकृतिक भाषा प्रसंस्करण (BSNLP) 2021 कार्यशाला में साझा कार्य के लिए जुब्लजाना विश्वविद्यालय (UL FRI) समूह की प्रस्तुतियों का वर्णन करता है। हम कई BERT-आधारित मॉडलों के साथ प्रयोग करते हैं, जो बहुभाषी, क्रोएशियाई-स्लोवेन-अंग्रेजी और स्लोवेन-केवल डेटा में पूर्व-प्रशिक्षित हैं। हम प्रशिक्षण पुनरावृत्ति और पहले से उपलब्ध एनईआर डेटासेट के concatenated डेटा पर प्रदर्शन करते हैं। सामान्यीकरण कार्य के लिए हम Stanza lemmatizer का उपयोग करते हैं, जबकि एंटिटी मिलान के लिए हमने Dedupe लाइब्रेरी का उपयोग करके एक बेसलाइन लागू की। मूल्यांकन के प्रदर्शन से पता चलता है कि बहु-स्रोत सेटिंग्स कम-संसाधन वाले दृष्टिकोणों को मात देती हैं। सर्वश्रेष्ठ एनईआर मॉडल स्लोवेन प्रशिक्षण डेटा विभाजन पर 0.91 एफ-स्कोर प्राप्त करते हैं, जबकि सर्वश्रेष्ठ आधिकारिक सबमिशन ने क्रमशः आराम से आंशिक मिलान और सख्त सेटिंग्स के लिए 0.84 और 0.78 के एफ-स्कोर प्राप्त किए। बहुभाषी एनईआर सेटिंग में हम 0.82 और 0.74 के एफ-स्कोर प्राप्त करते हैं।', 'ru': 'В этой статье описываются материалы, представленные группой Университета Любляны (UL FRI) для совместной задачи на семинаре по обработке балто-славянских естественных языков (BSNLP) 2021 года. Мы экспериментируем с несколькими моделями на основе BERT, предварительно обученными многоязычным, хорватско-словенско-английским и только словенским данным. Мы проводим обучение итеративно и на объединенных данных ранее доступных наборов данных NER. Для задачи нормализации мы используем лематизатор Stanza, в то время как для сопоставления сущностей мы реализовали базовую линию с помощью библиотеки Dedupe. Результаты оценок свидетельствуют о том, что условия использования различных источников превосходят возможности применения менее обеспеченных ресурсами подходов. Лучшие модели NER достигают 0,91 F-балла на словенском разделении данных обучения, в то время как лучшее официальное представление достигло F-балла 0,84 и 0,78 для расслабленного частичного соответствия и строгих настроек, соответственно. В многоязычной настройке NER мы получаем F-показатели 0,82 и 0,74.', 'ga': 'Déanann an páipéar seo cur síos ar aighneachtaí Ghrúpa Ollscoil Liúibleána (UL FRI) maidir leis an tasc roinnte ag Ceardlann Próiseála Teanga Nádúrtha Balto-Slavacha (BSNLP) 2021. Déanaimid trialacha le samhlacha iolracha bunaithe ar BERT, réamh-oilte i sonraí ilteangacha, Cróitis-Slóivéinis-Béarla agus Slóivéinis amháin. Déanaimid oiliúint atriallach agus ar shonraí comh-chomhtháithe na dtacar sonraí NER a bhí ar fáil roimhe seo. Don tasc normalaithe bainimid úsáid as Stanza lemmatizer, agus le haghaidh meaitseáil aonáin chuireamar bonnlíne i bhfeidhm ag baint úsáide as leabharlann Dedupe. Tugann feidhmíocht na meastóireachtaí le fios go n-éiríonn le suíomhanna ilfhoinse cur chuige nach bhfuil mórán acmhainní acu. Baineann na samhlacha NER is fearr amach 0.91 scór-F ar scoilteanna sonraí oiliúna na Slóivéine agus ghnóthaigh an aighneacht oifigiúil is fearr scóir F de 0.84 agus 0.78 maidir le comhoiriúnú páirteach réchúiseach agus socruithe dochta, faoi seach. I socrú NER ilteangach bainimid amach scóir F de 0.82 agus 0.74.', 'el': 'Η παρούσα εργασία περιγράφει τις υποβολές της ομάδας του Πανεπιστημίου της Λιουμπλιάνα (UL FRI) στο κοινό έργο του εργαστηρίου επεξεργασίας βαλτοσλαβικής φυσικής γλώσσας (BSNLP) 2021. Πειραματιζόμαστε με πολλαπλά μοντέλα βασισμένα στο BERT, προεκπαίδευτα σε πολυγλωσσικά, κροατικά-σλοβενικά-αγγλικά και σλοβενικά δεδομένα μόνο. Πραγματοποιούμε την εκπαίδευση επαναληπτικά και με τα συνδεδεμένα δεδομένα των προηγουμένως διαθέσιμων συνόλων δεδομένων. Για την εργασία ομαλοποίησης χρησιμοποιούμε λεμmatisator Stanza, ενώ για την αντιστοίχιση οντοτήτων εφαρμόσαμε μια γραμμή βάσης χρησιμοποιώντας τη βιβλιοθήκη Dedupe. Η απόδοση των αξιολογήσεων υποδηλώνει ότι οι ρυθμίσεις πολλαπλών πηγών ξεπερνούν τις προσεγγίσεις με λιγότερους πόρους. Τα καλύτερα μοντέλα επιτυγχάνουν 0.91 σκορ στα σλοβενικά δεδομένα προπόνησης ενώ η καλύτερη επίσημη υποβολή πέτυχε σκορ 0.84 και 0.78 για χαλαρό μερικό ταίριασμα και αυστηρές ρυθμίσεις αντίστοιχα. Σε πολυγλωσσικό περιβάλλον επιτυγχάνουμε βαθμολογίες Φ 0.82 και 0.74.', 'hu': 'Ez a tanulmány bemutatja a Ljubljanai Egyetem (UL FRI) Csoport beadványait a Balto-Szláv Természetes Nyelvfeldolgozás (BSNLP) 2021 Workshop közös feladatára. Több BERT alapú modellel kísérletezünk, amelyeket előzetesen többnyelvű, horvát-szlovén-angol és csak szlovén nyelvű adatokban képzettek. A képzéseket iteratívan és korábban rendelkezésre álló NER adatkészletek összekapcsolt adataival végezzük. A normalizációs feladathoz Stanza lemmatizert használunk, míg entitásjavításhoz a Dedupe könyvtár segítségével egy alapvonalat hajtottunk végre. Az értékelések teljesítménye arra utal, hogy a többforrásból származó beállítások felülmúlják a kevésbé erőforrásokkal rendelkező megközelítéseket. A legjobb NER modellek 0,91 F pontszámot értek el a szlovén edzési adatok elosztásán, míg a legjobb hivatalos beadványok 0,84 és 0,78 F pontszámot értek el a lazított részleges mérkőzés és a szigorú beállítások mellett. Többnyelvű NER beállításban 0,82 és 0,74 F pontszámot érünk el.', 'it': "Questo articolo descrive i contributi del gruppo dell'Università di Lubiana (UL FRI) al compito condiviso al workshop Balto-Slavic Natural Language Processing (BSNLP) 2021. Sperimentiamo più modelli basati su BERT, pre-formati in dati multilingui, croato-sloveno-inglese e sloveno-only. Eseguiamo la formazione iterativamente e sui dati concatenati di set di dati NER precedentemente disponibili. Per l'attività di normalizzazione usiamo lemmatizer Stanza, mentre per l'entity matching abbiamo implementato una linea di base utilizzando la libreria Dedupe. Le prestazioni delle valutazioni suggeriscono che le impostazioni multi-source superano gli approcci meno risorse. I migliori modelli NER raggiungono un punteggio F di 0,91 sulle divisioni dei dati di allenamento sloveni, mentre la migliore presentazione ufficiale ha ottenuto punteggi F di 0,84 e 0,78 rispettivamente per una partial matching rilassata e impostazioni rigide. Nell'impostazione NER multilingue otteniamo punteggi F di 0,82 e 0,74.", 'kk': 'Бұл қағаз 2021 жыл Balto- Slavic Natural Language Processing (BSNLP) жұмысының ортақтастырылған тапсырмаға Ljubljana университетінің (UL FRI) тобының жұмысын таңдайды. Біз бірнеше BERT негіздеген үлгілерді тәжірибедік, бірнеше тілді, Хорват- Словен- ағылшын және Словен- тек деректерінде алдында оқылған. Біз кейінгі NER деректер жиындарының біріктірілген деректерін қайталап жұмыс істейміз. Біз Станза лимматизаторын нормализациялау тапсырмасына қолданамыз, біз Dedupe жиынын қолдану үшін негізгі жолды орындадық. Бірнеше көзі баптаулар көп көзі баптауларының ресурстардың артықшылығын шешуіне болады. Ең жақсы NER үлгілері Словен оқыту деректерінің бөлігінде 0,91 F- нәтижесін жеткізеді, бірақ ең жақсы оқыту үшін 0,84 және 0,78 F- нәтижелерін жеткізеді. Көп тілдік NER параметрлерінде, 0,82 және 0,74 F- нөмірлерін жеткіземіз.', 'lt': 'Šiame dokumente aprašomi Ljubljanos universiteto (UL FRI) grupės pareiškimai dėl bendros užduoties Balto ir Slavo gamtos kalbų apdorojimo (BSNLP) 2021 m. seminare. Eksperimentuojame su daugeliu BERT pagrįstų modelių, parengtų daugiakalbių, kroatijos, slovėnų, anglų ir tik slovėnų duomenimis. Atliekame mokymus kartotinai ir sutrumpintais anksčiau turimų NER duomenų rinkinių duomenimis. Normalizacijos uždaviniui naudojame Stanza lemmatizatorį, o subjekto atitikimui mes įdiegėme bazę naudojant Dedupe biblioteką. Vertinimų rezultatai rodo, kad daugialypiai nustatymai viršija mažiau išteklių turinčius metodus. Geriausi NER modeliai pasiekia 0,91 F balas Slovėnijos mokymo duomenų pasidalijimuose, o geriausias oficialus pateikimas pasiekė atitinkamai 0,84 ir 0,78 F balas susilpninus dalinį derinimą ir griežtus nustatymus. Daugiakalbėje NER aplinkoje pasiekti F rezultatai yra 0,82 ir 0,74.', 'mk': 'Овој весник ги опишува поднесувањата на Групата на Универзитетот во Љубљана (УЛ ФРИ) на заедничката задача на работилницата за балто-славскиот процес на природен јазик (БСНП) 2021 година. Експериментираме со повеќе модели базирани на БЕРТ, предобучени во мултијазични, хрватско-словенско-англиски и само словенски податоци. We perform training iteratively and on the concatenated data of previously available NER datasets.  За нормализациската задача го користиме Лемматизаторот Станза, додека за одговарање на ентитетите спроведовме основна задача користејќи ја библиотеката Дедуп. Извршувањето на проценките покажува дека поставувањата од мултиизвори ги надминуваат пристапите со помалку ресурси. Најдобрите НЕР модели постигнаа 0,91 Ф-оценка за поделбата на податоци за словенечка обука, додека најдоброто официјално поднесување постигна F-оценка од 0,84 и 0,78 за релаксирано парцијално спојување и строги поставувања, односно. Во мултијазичното поставување на НЕР постигнуваме ф-оценки од 0,82 и 0,74.', 'ml': 'ഈ പത്രത്തില്\u200d ലൂജുബ്ലുജാന യൂണിവേഴ്സിറ്റിയിലെ (UL FRI) ഗ്രൂപ്പിന്\u200dറെ പങ്കുചേര്\u200dന്ന ജോലിക്ക് വിവരിക്കുന്നു. ബാള്\u200dട്ടോ- സ്ലാവിക് സ്വാഭ ബെര്\u200dട്ടി അടിസ്ഥാനമായിട്ടുള്ള പല മോഡലുകളുമായി ഞങ്ങള്\u200d പരീക്ഷിക്കുന്നു, പല ഭാഷകളില്\u200d, ക്രോവേഷ്യന്\u200d- സ്ലോവെന്\u200d - ഇംഗ്ലീഷ് മാത നമ്മള്\u200d പരിശീലിക്കുന്നത് വ്യത്യസ്തമായി പ്രവര്\u200dത്തിപ്പിക്കുന്നു. മുമ്പ് ലഭ്യമായ നെആര്\u200d ഡാറ്റാസറ്റു സാധാരണ പ്രവര്\u200dത്തനത്തിനായി ഞങ്ങള്\u200d സ്റ്റാന്\u200dസാ ലെമാറ്ററിനെ ഉപയോഗിക്കുന്നു. സാധാരണ വസ്തുത പൊരുതുന്ന വസ്തുക്കള്\u200d വിലാസങ്ങളുടെ പ്രവര്\u200dത്തനപ്പെടുത്തിയിരിക്കുന്നു പല-സോര്\u200dസ്സുകളുടെ സജ്ജീകരണങ്ങള്\u200d കുറച്ച് വിഭവങ The best NER models achieve 0.91 F-score on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively.  പല ഭാഷകങ്ങളുടെ നെയറിന്റെ സജ്ജീകരണങ്ങളില്\u200d നമ്മള്\u200d എഫ് സ്കോര്\u200d പ്രാപിക്കുന്നു. 0.82, 0.74.', 'mt': 'Dan id-dokument jiddeskrivi s-sottomissjonijiet tal-Grupp tal-Università ta’ Ljubljana (UL FRI) għall-kompitu kondiviż fil-Workshop tal-2021 dwar l-Ipproċessar tal-Lingwi Naturali tal-Balto-Slav (BSNLP). Aħna ninsperimentaw b’mudelli multipli bbażati fuq BERT, imħarrġa minn qabel f’dejta multilingwi, Kroata-Slovena-Ingliża u Slovena biss. Aħna nagħmlu taħriġ b’mod iterattiv u dwar id-dejta konġunta ta’ settijiet ta’ dejta NER disponibbli qabel. Għall-kompitu tan-normalizzazzjoni a ħna nużaw il-lemmatizzatur Stanza, filwaqt li għall-paragun tal-entità implimentajna linja bażi bl-użu tal-librerija Dedupe. Il-prestazzjoni tal-evalwazzjonijiet tissuġġerixxi li l-konfigurazzjonijiet b’diversi sorsi jaqbżu l-approċċi b’inqas riżorsi. L-aħjar mudelli NER jiksbu punteġġ F ta’ 0.91 fuq il-qsim tad-dejta tat-taħriġ Sloven filwaqt li l-aħjar sottomissjoni uffiċjali kisbet punteġġi F ta’ 0.84 u 0.78 għal tqabbil parzjali rilassat u settings stretti, rispettivament. F’ambjent NER multilingwi inkisbu punteġġi F ta’ 0.82 u 0.74.', 'ms': 'Kertas ini menggambarkan penghantaran Kumpulan Universiti Ljubljana (UL FRI) kepada tugas terkongsi di Pekerjaan Balto-Slavic Natural Language Processing (BSNLP) 2021. Kami eksperimen dengan beberapa model berasaskan BERT, dilatih-dilatih dalam data berbilang-bahasa, Croatian-Slovene-English dan Slovene-only. Kami melatih secara berulang-ulang dan pada data yang dikumpulkan dari set data NER yang terdapat sebelumnya. Untuk tugas normalisasi kami menggunakan lemmatizer Stanza, sementara untuk entiti yang sepadan kami melaksanakan dasar menggunakan perpustakaan Dedupe. Performasi penilaian menyarankan tetapan sumber berbilang melampaui pendekatan yang kurang sumber. Model NER terbaik mencapai skor-F 0.91 pada pemisahan data latihan Slovene sementara penghantaran rasmi terbaik mencapai skor-F 0.84 dan 0.78 untuk persamaan bahagian yang santai dan tetapan ketat, berdasarkan. Dalam tetapan NER berbilang bahasa kita mencapai skor F 0.82 dan 0.74.', 'ka': 'ეს დომენტი აღწერა ლუბლიანის (UL FRI) სუნივერსის გასაკეთება 2021 წლის სამუშაო დასახლებელი საქაღალდე ბალტო-სლავისური თავისუფალური ენის პროცესი (BSNLP). ჩვენ ვქცევით ბერტის მრავალური მოდელებით, მრავალური, სლოვატიური და სლოვანიური მხოლოდ მონაცემებით. ჩვენ განვიყენებთ ინტერეტიურად და მხოლოდ მომხმარებული NER მონაცემების მონაცემებზე. ნორმალიზაციის დავალებისთვის, ჩვენ სტანზა ლემეტიზერის გამოყენებთ, როცა ელემენტის შესაბამისი განსხვავებაში, ჩვენ დავამყენეთ ფესური ხაზი, რომელიც De განსაზღვრებების შესაძლებლობა იტყვებს, რომ მრავალ წიგნის პარამეტრები უფრო ცოტა რესურსურსურსურსურსური შესაძლებლობა. ყველაზე საუკეთესი NER მოდელები 0,91 F-score სლოვინის განაკლების მონაცემების გაყოფილი, როცა ყველაზე საუკეთესი ოფიციალური მონაცემები გააკეთებულია 0,84 და 0,78 F-scores განაკეთებული ფართოლური დამატების მრავალენგიური NER-ის შენახვედში ჩვენ მივიღეთ F-scores 0.82 და 0.74.', 'no': 'Denne papiret beskriver innstillingane av gruppa i Universiteten Ljubljana (UL FRI) til den delte oppgåva i Balto-Slavic Natural Language Processing (BSNLP) 2021 Arbeidshop. Vi eksperimenterer med fleire BERT-baserte modeller, før-trent på fleire språk, kroatsk-slovensk-engelsk og slovensk-berre data. Vi utfører trening gjentaktiv og på samsvarte data av førre tilgjengelege NER- datasett. For normaliseringsplassen bruker vi Stanza- lemmatisering, mens vi implementerte ein grunnlinje med Dedupe- biblioteket for einingar som passar. Utviklinga av evalueringa tyder på at fleire kjeldeinnstillingar utfører mindre ressurserte tilnærmingar. Den beste NER-modellen oppnår 0,91 F-poeng på Slovenske opplæringsdata, mens den beste offisielle opplæringa oppnådd F-poeng med 0,84 og 0,78 for relakserte delvis treff og streke innstillingar. I fleire språk NER-innstillingar oppnår vi F-poeng med 0,82 og 0,74.', 'mn': 'Энэ цаас Ljubljana (UL FRI) сургуулийн Балто-Славик Байгалийн хэл Процессорын (BSNLP) 2021 оны ажлын хуваалцааны ажлыг тайлбарладаг. Бид олон BERT-д суурилсан загваруудыг туршиж, олон хэл, Хорват-Словен-Англи болон Словен зөвхөн өгөгдлийн талаар суралцагдсан. Бид өмнө нь NER өгөгдлийн сангийн тодорхойлолт дээр дахин дахин сургалт хийдэг. Бид Стэнза лимматизаторыг ашиглаж байгаа хэмжээний даалгаврын тулд бид Dedupe номын санг ашиглаж суурь шугам хийсэн. Баталгааны үйлдэл нь олон эх үүсвэрийн төлөвлөгөөс бага боломжтой арга барилгыг илэрхийлж чадна. Хамгийн сайн НЭР загварууд Словенийн сургалтын өгөгдлийн хуваагдах 0.91 F-тоо гарч ирсэн. Хамгийн сайн официйн сургалтын хуваагдах нь 0.84 болон 0.78-той F-тоо гарч ирсэн юм. Ихэнх хэлний NER хэлбэрээр бид 0.82, 0.74 F-тоо гаргадаг.', 'ro': 'Această lucrare descrie observațiile Grupului Universității din Ljubljana (UL FRI) la sarcina comună la Atelierul Balto-Slavic Natural Language Processing (BSNLP) 2021. Experimentăm mai multe modele bazate pe BERT, pre-instruite în mai multe limbi, croată-slovenă-engleză și doar slovenă. Realizăm instruire iterativă și pe baza datelor concatenate ale seturilor de date NER disponibile anterior. Pentru sarcina de normalizare folosim lemmatizer Stanza, în timp ce pentru potrivirea entităților am implementat o bază de referință folosind biblioteca Dedupe. Performanța evaluărilor sugerează că setările multi-sursă depășesc abordările cu resurse mai puține. Cele mai bune modele NER obțin scorul F de 0,91 pe divizarea datelor de antrenament slovene, în timp ce cea mai bună depunere oficială a obținut scoruri F de 0,84 și 0,78 pentru potrivirea parțială relaxată și, respectiv, setările stricte. În setările NER multilingve obținem scoruri F de 0,82 și 0,74.', 'sr': 'Ovaj papir opisuje podatke grupe Univerziteta Ljubljane (UL FRI) na zajednički zadatak na Balto-Slavičkom prirodnom jeziku procesu (BSNLP) 2021. radionici. Eksperimentiramo sa višestrukim modelima baziranim na BERT-u, predobučenim na višejezičkim, hrvatskim-slovenim-engleskim i slovenim podacima. Ponovno obavljamo obuku i povezane podatke ranije dostupnih podataka NER-a. Za normalizacijski zadatak koristimo limmatizaciju Stanze, dok smo za podudaranje entiteta proveli početnu liniju koristeći biblioteku Dedupe. Izvrsnost procjene ukazuje na to da više izvora postavljanja iznosi manje resursnog pristupa. Najbolji modeli NER postignu 0,91 F-rezultat na sloveničkim podacima obuke, dok je najbolja službena predstava postigla F-rezultate 0,84 i 0,78 za opuštene parcijalne odgovarajuće i stroge nastave. U mnogim jezičkim postavljanjima NER postižemo F-rezultate od 0,82 i 0,74.', 'si': 'මේ පත්තුව ලිජුබ්ලානාගේ විශ්වාසිත්තාව (UT FRI) කණ්ඩායමේ සම්පූර්ණ විශ්වාසිකයේ බාල්ටෝස්ලාවික් ස්වාභික භාෂාව ප අපි ගොඩක් BERT-අධාරිත මොඩල් එක්ක පරීක්ෂණය කරනවා, ගොඩක් භාෂාවක්, ක්\u200dරෝටියාන්-ස්ලෝවේන්-ඉංග්\u200dරීසි සහ ස්ල අපි ප්\u200dරශ්නයක් ආයෙත් ප්\u200dරශ්නයක් කරනවා ඒ වගේම කලින් පුළුවන් NER දත්ත සේට් ගැන සම්බන්ධ දත්තේ  සාමාන්\u200dය වැඩසටහන් අපි ස්ටැන්සා ලෙම්මේසර් වැඩසටහන් පාවිච්චි කරනවා, අපි ඩිඩුප් ලායිබරිය ප්\u200dරයෝජනය කරන්න විශ්වාසයේ ප්\u200dරභාව ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙනවා කියලා ගොඩක් ප්\u200dරභාව සැකසුම්  හොඳම NER මොඩල් 0.91 F-score ස්ලෝවෙන් ප්\u200dරේක්ෂණය දත්ත ස්පිල්ට් වෙනුවෙන් හොඳම අධ්\u200dයාත්මක ප්\u200dරේක්ෂණය 0.84 සහ 0.78 වෙනුවෙන් F-scores ලැබුනා ස්ලෝව බොහොම භාෂාවික NER සැකසුමේ අපිට 0.82 සහ 0.74 විශේෂ F-ස්කෝර් ලැබෙනවා.', 'so': 'Warqadan wuxuu ku qoran Jaamacadda Ljubljana (UL FRI) submissions to the task shared at the Balto-Slavic-Slavic Processing Natural Language (BSNLP) 2021 workshop. Waxaan imtixaamaynaa tusaalooyin badan oo BERT ku saleysan, waxbarasho hore oo ku qoran luuqado badan, Croatian-Slovene-Ingiriis iyo Slovene-kaliya. Waxaynu si rasmi ah u sameynaa waxbarashada iyo macluumaadka la xiriiray ee horay loo soo bandhigay NER-da. Shaqada caadiga ah ee Stanza waxaynu isticmaalnaa lemmatizer, isla markaasna waxaynu u sameynnay shabakadda ee lagu isticmaalayo maktabadda Dedupe. Dhaqanka qiimeynta waxaa loola jeedaa in heerarka badan ay sameeyaan qaabooyin ka yar oo ay ka shaqeeyaan. Tilmaamaha ugu wanaagsan ee NER waxay gaadhaan goobaha waxbarashada ee Slovene ku qoran 0.91 F-score, marka loo soo dhiibo rasmiga ugu wanaagsan waxay gaadhay tusaale ahaan F-score 0.84 iyo 0.78 si loo qabsado qeyb u eg iyo habbooyin adag. Xarunta NER ee luuqadaha badan ayaannu gaadhnaa F-scoro 0.82 iyo 0.74.', 'sv': 'Denna uppsats beskriver University of Ljubljana (UL FRI) Groups bidrag till den gemensamma uppgiften vid Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. Vi experimenterar med flera BERT-baserade modeller, förberedda i flerspråkiga, kroatiska-slovenska-engelska och slovenska-endast data. Vi utför utbildning iterativt och på sammanlänkade data från tidigare tillgängliga NER-dataset. För normaliseringsuppgiften använder vi Stanza lemmatizer, medan för entitetsmatchning implementerade vi en baslinje med Dedupe-biblioteket. Utvärderingarnas resultat tyder på att inställningar med flera källor överträffar metoder med mindre resurser. De bästa NER-modellerna uppnår 0,91 F-poäng på slovenska träningsdata splits medan de bästa officiella inlämningarna uppnådde F-poäng på 0,84 respektive 0,78 för avslappnad partiell matchning respektive strikta inställningar. I flerspråkig NER-inställning uppnår vi F-poäng på 0,82 och 0,74.', 'pl': 'Niniejszy artykuł opisuje zgłoszenia Grupy Uniwersytetu w Lublanie (UL FRI) do wspólnego zadania podczas warsztatów Balto-Slavich Natural Language Processing (BSNLP) 2021. Eksperymentujemy z wieloma modelami opartymi na BERT, wstępnie przeszkolonymi w wielojęzycznych, chorwackio-słoweńsko-angielskich i słoweńskich danych. Szkolenia przeprowadzamy iteracyjnie i na połączonych danych dotychczas dostępnych zestawów danych NER. Do zadania normalizacyjnego używamy lemmatyzatora Stanza, natomiast do dopasowywania entity implementowaliśmy bazę bazową przy użyciu biblioteki Dedupe. Wyniki ocen sugerują, że ustawienia wielu źródeł przewyższają podejścia mniej zasobów. Najlepsze modele NER osiągają 0,91 F-wynik na podziałach danych treningowych słoweńskich, podczas gdy najlepsze oficjalne zgłoszenia osiągnęły F-wyniki 0.84 i 0.78, odpowiednio dla luźnego dopasowania częściowego i ścisłych ustawień. W wielojęzycznym ustawieniu NER osiągamy wyniki F 0.82 i 0.74.', 'ta': 'இந்த தாள் லுஜுப்லாஜானா கல்லூரியில் (UL FRI) குழுவின் கூட்டத்திற்கு பந்து- ஸ்லாவிக் இயல்பான மொழி செயல்பாடு (BSNLP) 2021 வேலையில் பங்கிடப்பட பல மொழிகளில், க்ரோவியன்- ஸ்லோவென்- ஆங்கிலம் மற்றும் ஸ்லோவென்ன் மட்டும் தகவல்களில் முன்பயிற்சி செய்யப்பட்டுள்ளோம். நாங்கள் பயிற்சி உருவாக்கத்தில் செயல்படுத்துகிறோம் மற்றும் முன்பு கிடைக்கும் NER தரவு அமைப்பு நாங்கள் ஸ்டான்சா லிமாட்டரை பயன்படுத்தும் செயல்பாட்டிற்கு, பொருந்தும் உடன் நாங்கள் Dedupe நூலகத்தை பயன்படுத்தினோம். மதிப்பீடுகளின் செயல்கூறு பல- மூல அமைப்புகள் குறைந்த மூலங்களிலிருந்து செயல்படுத்துகிறது. சிறந்த NER மாதிரிகள் 0. 91 F- score பெறுகிறது ஸ்லோவென்ன் பயிற்சி தரவு பிரிவில் பெறுகிறது, சிறந்த அரசியல் பரிமாற்றம் 0. 84 மற்றும் 0. 78 பாகம் பொருத்தும் மற பல மொழி புதிய அமைப்பில் நாம் 0.82 மற்றும் 0.74 புள்ளிகளை பெறுகிறோம்.', 'ur': "This paper describes the University of Ljubljana (UL FRI) Group's submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. ہم بہت سے BERT بنیادی موڈل کے ساتھ آزمائش کرتے ہیں، بہت سی زبان، کروٹی-اسلووین-انگلیسی اور صرف اسلووین-ڈاٹ میں پہلے آموزش کی جاتی ہیں. ہم اس سے پہلے نیر ڈیٹسٹ کے مطابق تطابق کرتے ہیں اور اس سے پہلے موجود نیر ڈیٹسٹ کے مطابق مطابق دکھائے جاتے ہیں۔ ہم نے استنسا لیمٹیزر کے لئے استعمال کیا، حالانکہ ہم نے دیڈوپ لیبری کے مطابق ایک بنسس لین کو استعمال کیا۔ مطالعہ کے عملکرد کی نشان دیتا ہے کہ بہت سی سورس سیٹیوں کم رسسورس کی طریقے سے کام لیتے ہیں۔ سب سے بہترین NER موڈلیوں نے اسلووین ترینس ڈیٹوں پر 0.91 F-اسکور پائی، حالانکہ سب سے بہترین رسمی رسمی رسمی رسمی نے 0.84 اور 0.78 کی F-اسکور پیدا کی تھی، آہستہ حصہ مطابق اور تنگ سیٹیوں کے لئے۔ ہم 0.82 اور 0.74 کے F-Scores کو پہنچ رہے ہیں.", 'uz': "This paper describes the University of Ljubljana (UL FRI) Group's submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop.  Biz bir nechta BERT asosiy modellar bilan tajriba qilamiz, bir necha tillar, Croatian- Sloven- Ingliz va Sloven faqat maʼlumot bilan bir nechta o'rganishni o'rganamiz. Biz oldingi maʼlumot sahifadagi taʼminlovchi soʻzni bajaramiz. @ info: whatsthis Qiymatlar bajarishini bir necha manba- manba moslamalari qoʻshish mumkin. Yaxshi NER modellari Sloven taʼminlovchi maʼlumotlar splittida 0. 91 F score tugatadi, va eng yaxshi xabarlarni olib tashlash uchun eng yuqori rasm joʻnatish uchun 0. 84 va 0. 78 qismi qisqa oʻxshash va katta moslamalar uchun F- scori yetardi. Koʻp tillar uchun NER moslamalarida, biz 0.82 va 0.74 foydalanamiz.", 'vi': 'Bài viết này mô tả s ự phát biểu của đại học Ljubljana (Oul FRI) của tập đoàn về các nhiệm vụ được chia sẻ tại Xưởng điều chế ngôn ngữ tự nhiên Balto-Slavic (BSNLP) 2021. Chúng tôi thử nghiệm với nhiều mẫu cây Berlin, được đào tạo trước nhiều ngôn ngữ, người Xlôven-Anh và người Xlôven-Anh. Chúng tôi thực hiện huấn luyện theo phương pháp số liệu của người máy sẵn sàng. Để làm nhiệm vụ bình thường, chúng tôi dùng Stanza lemmatizer, trong khi Đối với thực thể khớp, chúng tôi thực hiện một đường hầm bằng thư viện Deduhai. Kết quả đánh giá cho thấy thiết lập đa nguồn cung cấp thiếu nguồn lực. Những người mẫu tối cao nhất đạt được 0.91 F-điểm trên dữ liệu đào tạo Slovenia phân mảnh trong khi lần đệ tốt nhất chính thức đã được ghi điểm F-điểm của 0.84 và 0.78 với việc xác thực thư giãn và thiết lập chặt chẽ. Trong chế độ tối đa ngôn ngữ rộng chúng ta đạt điểm F trong 0.82 và 0.74.', 'da': 'Denne artikel beskriver University of Ljubljana (UL FRI) Gruppens indlæg til den fælles opgave på Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. Vi eksperimenterer med flere BERT-baserede modeller, der er forududdannet i flersprogede, kroatisk-slovensk-engelsk og slovensk-kun data. Vi udfører træning iterativt og på sammenhængende data fra tidligere tilgængelige NER datasæt. Til normaliseringsopgaven bruger vi Stanza lemmatizer, mens vi implementerede en basislinje ved hjælp af Dedupe-biblioteket. Evalueringernes resultater tyder på, at indstillinger med flere kilder overgår tilgange med mindre ressourcer. De bedste NER modeller opnår 0,91 F-score på slovenske træningsdata split, mens den bedste officielle indsendelse opnåede F-score på henholdsvis 0,84 og 0,78 for afslappet delmatch og strenge indstillinger. I flersproget NER indstilling opnår vi F-scorer på 0,82 og 0,74.', 'bg': 'Настоящата статия описва предложенията на групата на Университета в Любляна (ФРИ) по споделената задача на Балто-славянската работна среща за обработка на естествен език (БНЛП) 2021 г. Експериментираме с множество базирани модели, предварително обучени в многоезични, хърватско-словенско-английски и само словенски данни. Ние провеждаме обучение итеративно и върху конкатенираните данни от досега налични набори от данни. За задачата за нормализиране използваме лематизатор Станза, а за съвпадение на обекти внедрихме базова линия с помощта на библиотеката Дедупе. Изпълнението на оценките предполага, че настройките на няколко източника превъзхождат подходите с по-малко ресурси. Най-добрите модели постигат 0,91 F-score при разделянето на данните за обучение в Словения, докато най-добрите официални резултати постигат F-score от 0,84 и 0,78 за спокойно частично съвпадение и строги настройки, съответно. При многоезична настройка постигаме резултати от 0,82 и 0,74.', 'hr': 'Ovaj papir opisuje podatke grupe Univerziteta Ljubljane (UL FRI) na zajednički zadatak na Balto-Slavic Natural Language Processing Workshop 2021. Eksperimentiramo s višestrukim modelima na BERT-u, predobučenim na višejezičkim, hrvatskim-slovenim-engleskim i slovenim podacima. Ponovno vježbamo podatke o prethodnim dostupnim podacima NER-a. Za normalizacijski zadatak koristimo limmatizator Stanza, dok smo za podudaranje entiteta proveli početnu liniju koristeći knjižnicu Dedupe. Proizvodnja procjena ukazuje na to da više izvora postavljanja iznosi manje resursnih pristupa. Najbolji modeli NER postignu 0,91 F rezultata na sloveničkim podacima obuke dok je najbolja službena predstava postigla F-rezultate 0,84 i 0,78 za opuštene djelomične odgovarajuće i stroge nastave. U višejezičkom postavljanju NER postižemo F-rezultate 0,82 i 0,74.', 'de': 'Dieser Beitrag beschreibt die Beiträge der Gruppe der Universität Ljubljana (UL FRI) zur gemeinsamen Aufgabe im Workshop Balto-Slavische Natural Language Processing (BSNLP) 2021. Wir experimentieren mit mehreren BERT-basierten Modellen, die in mehrsprachigen, kroatisch-slowenisch-englischen und slowenischen Daten vortrainiert sind. Wir führen das Training iterativ und auf den verketteten Daten bereits vorhandener NER-Datensätze durch. Für die Normalisierungsaufgabe verwenden wir Stanza Lemmatizer, während wir für Entity Matching eine Baseline mit der Dedupe Bibliothek implementiert haben. Die Ergebnisse der Auswertungen deuten darauf hin, dass Multi-Source-Einstellungen weniger ressourcenschonende Ansätze übertreffen. Die besten NER-Modelle erzielen 0.91 F-Score bei slowenischen Trainingsdatensplits, während die beste offizielle Einreichung F-Scores von 0.84 und 0.78 für entspanntes Teilmatching bzw. strenge Einstellungen erzielte. Im mehrsprachigen NER-Setting erreichen wir F-Scores von 0.82 und 0.74.', 'nl': 'Dit artikel beschrijft de inzendingen van de Universiteit van Ljubljana (UL FRI) Groep voor de gezamenlijke taak tijdens de Balto-Slavische Natural Language Processing (BSNLP) 2021 Workshop. We experimenteren met meerdere BERT-gebaseerde modellen, voorgetraind in meertalige, Kroatisch-Sloveens-Engels en alleen Sloveens-gegevens. We voeren iteratief trainingen uit op de samengevoegde data van eerder beschikbare NER datasets. Voor de normalisatie taak gebruiken we Stanza lemmatizer, terwijl we voor entiteiten matching een baseline geïmplementeerd hebben met behulp van de Dedupe bibliotheek. De prestaties van evaluaties suggereren dat multi-source-instellingen beter presteren dan benaderingen met minder middelen. De beste NER-modellen behalen 0.91 F-score op Sloveense trainingsdata splits, terwijl de beste officiële inzending F-scores behaalde van 0.84 en 0.78 voor ontspannen gedeeltelijke matching en strikte instellingen, respectievelijk. In meertalige NER setting bereiken we F-scores van 0.82 en 0.74.', 'fa': 'این کاغذ تحویل گروهی دانشگاه Ljubljana (UL FRI) را به کار مشترک در پردازش زبان طبیعی Balto-Slavic (BSNLP) 2021 توصیف می\u200cکند. ما با مدل های متعدد بنیاد BERT آزمایش می کنیم، پیش آموزش با داده های متعدد زبان، کروات-اسلووین-انگلیسی و فقط اسلووین. ما دوباره آموزش می\u200cکنیم و روی داده\u200cهای متصل شده از مجموعه\u200cهای داده\u200cهای NER پیش موجود است. برای وظیفه\u200cی عادی استفاده می\u200cکنیم از لیماتیزر استانزا، در حالی که برای اتحادیه\u200cای که با استفاده از کتابخانه\u200cی Dedupe پیدا می\u200cکنند، یک خط بنیادی را از استفاده می\u200cکنیم. فعالیت ارزیابی پیشنهاد می\u200cدهد که تنظیمات متعدد منبع بیشتر از نزدیک\u200cهای کمتری را انجام می\u200cدهد. بهترین مدل NER 0.91 امتیاز F در اطلاعات آموزش اسلووین می رسد، در حالی که بهترین فرستادن رسمی F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. در تنظیمات NER چند زبان می توانیم امتیاز F 0.82 و 0.74 را به دست یابیم.', 'id': 'Kertas ini menjelaskan pengiriman Grup Universitas Ljubljana (UL FRI) ke tugas yang sama di Workshop 2021 Balto-Slavic Natural Language Processing (BSNLP). We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data.  Kami melatih secara iteratif dan pada data kongkatenasi dari set data NER yang sebelumnya tersedia. Untuk tugas normalisasi kami menggunakan lemmatizer Stanza, sementara untuk entitas yang cocok kami implementasi dasar menggunakan perpustakaan Dedupe. Performasi evaluasi menunjukkan bahwa pengaturan multisumber melebihi pendekatan yang kurang sumber daya. Model NER terbaik mencapai skor F 0,91 pada pemisahan data pelatihan Slovene sementara pengiriman resmi terbaik mencapai skor F 0,84 dan 0,78 untuk persamaan parsial yang santai dan pengaturan yang ketat, respektif. Dalam pengaturan NER berbilang bahasa kita mencapai skor F 0,82 dan 0,74.', 'tr': 'Bu kagyz Ljubljana Uniwersitetiniň (UL FRI) 2021-nji Çalışma bellenilýär. Biz BERT-dan birnäçe nusgalar bilen synanyşýarys, birnäçe dil öňünde öňünde eğitilýän, horwatça-slowença-iňlis we slowença-diňe maglumatlar bilen synanyşýarys. Biz iň öňki NER veri setirleriniň konkretli maglumatlaryny tekrar edip ýöredýäris. Biz Stanza lemmatizer zadyny ullanýarys, bir eşleşen entitet üçin Dedupe kitaphanesini ulanan basit haty implemente etdik. Ýardamlar çykyşyrlygyny birnäçe-çeşme düzümlerniň kaynakly golaýlardan daşyrýandygyny maslahat berýär. Iň gowy NER modelleri Sloweniň eğitim maglumatynda 0.91 F अंश berilýär we olaryň iň gowy resmi süýtgetmesi 0.84 we 0.78 diýilip dynç duşuşygy üçin ýeterdi. Birnäçe dilli NER düzümlerinde 0.82 we 0.74 düzümlerini ýetip bilýäris.', 'sw': 'Gazeti hili linaelezea ujumbe wa Chuo Kikuu cha Ljubljana (UL FRI) wa kikundi hicho kinachoshirikishwa katika Warsha ya Utarabu wa Lugha ya Kibalto-Slavic (BSNLP) 2021. Tunajaribu kwa mifano mingi yenye BERT, mafunzo ya awali katika lugha mbalimbali, takwimu za KiCroatia-Slovene-Kiingereza na Slovene-pekee. Tunafanya mafunzo kwa kiasi kikubwa na katika taarifa zilizotengenezwa za taarifa zilizopatikana hapo awali za NER. Kwa jukumu la kawaida tunatumia mpiga mfumo wa Stanza, wakati kwa ajili ya vyombo vinavyochanganya tulitekeleza msingi kwa kutumia maktaba ya Dedupe. Utendaji wa tafiti unaonyesha kuwa kituo cha vyanzo vingi vinavyofanya hatua zisizo rasilimali. Mradi bora zaidi wa NER hufanikiwa vipande 0.91 F katika maeneo ya data ya mafunzo ya Slovenia wakati ujumbe bora wa rasmi ulifikia vipindi vya F.84 na 0.78 kwa ajili ya mapigano na vikwazo vikali. Katika mazingira ya lugha mbalimbali tunapata F-score za 0.82 na 0.74.', 'am': 'ይህ ገጽ ለዩንቨርስቲ ዩንቨርስቲ (UL FRI) በBalto-Slavic-Natural ቋንቋ ፕሮጀክት (BSNLP) 2021 workshop ላይ የተካፈሉትን ሥራ የሚያሳውቀው የቡድን አዋጅ ነው፡፡ በብዙ ቋንቋዎች፣ ክሮሽያዊ-ስሎቪኛ-እንግሊዘኛ እና ስሎቪን ብቻ ዳታዎችን በተለየን ብዙዎችን BERT-ተመሳሳይን እናሞክራለን፡፡ በአሁኑ ጊዜ የNER ዳታ ሰርቨሮች በተለየ ማህበረሰብ እናደርጋለን፡፡ ለተቀናቀል ስራ Stanza lemmatizer ን እናጠቃለን፡፡ የመድረክ ግንኙነት የብዙ-source ምርጫዎች ከክፍለ ሀብት የጎዳና ደረጃዎች እንዲፈጸም ያሳያል፡፡ የተሻለሙ የNER ዓይነቶች በስሎቪን ትምህርት ዳታ ስፋት ላይ 0.91 F-score ያገኛሉ፡፡ በብዙ ቋንቋ ውስጥ የF-score 0.82 እና 0.74 አግኝተናል፡፡', 'af': "Hierdie papier beskryf die Universiteit van Ljubljana (UL FRI) groep se onderskrywings na die gedeelde taak by die Balto- Slavic Natuurlike Taal Prosessering (BSNLP) 2021 Werkshop. Ons eksperimenteer met veelvuldige BERT-gebaseerde modele, vooraf-opgelei in multi-tale, Kroatiese-Slovene-Engels en slegs-Slovene-data. Ons uitvoer onderwerp iteratief en op die samelekteerde data van voorheen beskikbaar NER datastelle. Vir die normaliseerde taak gebruik ons Stanza lemmatiseerder, terwyl vir entiteit wat ooreenstem het, het ons 'n basislien geïmplementeer met die Dedupe biblioteek. Die prestasie van evaluasies stel voorstel dat multibron instellings minder-hulpbronne toegang uitvoer. Die beste NER-modelles bereik 0.91 F-telling op Sloweniese onderwerking data splitter terwyl die beste offisiele onderwerking F-poeiers van 0.84 en 0.78 bereik het vir verlasse deel ooreenstemming en strikte instellings. In multi-tale NER-instelling het ons F-telling van 0.82 en 0.74 bereik.", 'sq': "This paper describes the University of Ljubljana (UL FRI) Group's submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop.  Ne eksperimentojmë me modele të shumta me bazë në BERT, të paratrajnuar në të dhëna shumëgjuhësore, kroate-sllovene-angleze dhe vetëm sllovene. Ne bëjmë stërvitje përsëritur dhe në të dhënat e shkurtra të të dhënave të disponueshme më parë NER. Për detyrën e normalizimit ne përdorim limmatizimin Stanza, ndërsa për njësinë që përputhet ne zbatuam një bazë duke përdorur bibliotekën Dedupe. Performanca e vlerësimeve sugjeron se rregullimet me shumë burime tejkalojnë qasjet me më pak burime. Modelet më të mira të NER arrijnë 0.91 pikë F në ndarjet e të dhënave të trajnimit slloven ndërsa paraqitja më e mirë zyrtare arriti rezultate F prej 0.84 dhe 0.78 respektivisht për përputhje të qetë të pjesshme dhe rregullime të ashpra. Në përcaktimin e NER shumëgjuhës arrijmë rezultate F prej 0.82 dhe 0.74.", 'ko': '본고는 발토슬라브자연언어처리(BSNLP) 2021 세미나에서 루블랴나대학(UL FRI)팀이 제출한 공유 임무를 기술한다.우리는 여러 개의 버트 기반 모델을 사용하여 실험을 진행했는데, 이 모델들은 다양한 언어, 크로아티아어 - 슬로베니아어 - 영어와 순수한 슬로베니아어 데이터로 미리 훈련된 것이다.우리는 이전에 사용할 수 있었던 NER 데이터 집합의 연결 데이터를 교체하고 훈련했다.규범화 작업에 대해 우리는 Stanza lemmatizer를 사용하고 실체가 일치하는 것에 대해 우리는 데이터 중복 제거 라이브러리를 사용하여 기선을 실현했다.평가 결과 다원적 환경이 자원이 적은 방법보다 낫다는 것이 밝혀졌다.베스트 NER 모델은 슬로베니아 훈련 데이터 분할에서 F 점수가 0.91로 가장 높았고, 베스트 공식적으로 제출된 느슨한 부분 매칭과 엄격한 설정에서 F 점수는 각각 0.84와 0.78이었다.다국어 환경에서 우리의 F 점수는 각각 0.82와 0.74이다.', 'az': 'Bu kańüńĪt, Ljubljana Universitetinin (UL FRI) Grubunun 2021-ci ńįŇü Hopu Balto-Slavic Natural Language Processing (BSNLP) Ňü…ôkild…ô paylaŇüńĪlan iŇül…ôr…ô t…ôsdiql…ônm…ôsini t…ôsdiql…ôyir. Biz √ßoxlu BERT tabanlńĪ modell…ôrl…ô imtahana √ß…ôkirik, √ßoxlu dil, HńĪrvat-Sloven-ńįngilizce v…ô Sloven-d…ôn …ôvv…ôl t…ôhsil edilmiŇüik. Biz h…ôm√ßinin …ôvv…ôlki NER veril…ônl…ôrin m√ľ…ôyy…ôn edilmiŇü m…ôlumatlarńĪnńĪ t…ôhsil edirik. Biz Stanza limmatizeri istifad…ô edirik, √ß√ľnki eŇüitm…ôk √ľ√ß√ľn, Dedupe k√ľt√ľphan…ôsini istifad…ô ed…ôr…ôk baz √ßizgi t…ôyin etdik. QńĪymetl…ôrin t…ôcr√ľb…ôsi √ßoxlu m…ônb…ô ayarlarńĪ √ßoxlu m…ônb…ô ayarlarńĪnńĪn √ßoxluńüundan daha az m…ônb…ô t…ôr…ôfind…ôn istifad…ô edilm…ôsini iddia edir. ∆Źn yaxŇüńĪ NER modell…ôri Slovenin t…ôhsil m…ôlumatlarńĪnda 0,91 F-score par√ßalanńĪr, lakin …ôn yaxŇüńĪ resmi t…ôhsil m√ľ…ôyy…ôn edilm…ôsi 0,84 v…ô 0,78 f…ôrqli par√ßalńĪq m√ľ…ôyy…ôn edilm…ôsi v…ô d√ľzg√ľn t…ôhsil m√ľ…ôyy…ôn edilm…ôsi √ľ√ß√ľn F-scores tapńĪlńĪr. √áoxlu dilli NER ayarlarńĪnda F-scores 0,82 v…ô 0,74 √ßatdńĪrńĪrńĪq.', 'bs': 'Ovaj papir opisuje podatke grupe Univerziteta Ljubljane (UL FRI) na zajednički zadatak na Balto-Slavic Natural Language Processing Workshop 2021. Mi eksperimentiramo sa višestrukim modelima baziranim na BERT-u, predobučenim na višejezičkim, hrvatskim-slovenim-engleskim i slovenim podacima. Ponovno obavljamo obuku i povezane podatke ranije dostupnih podataka NER-a. Za normalizacijski zadatak koristimo limuzizator Stanza, dok smo za podudaranje entiteta proveli početnu liniju koristeći biblioteku Dedupe. Izvrsnost procjena ukazuje na to da multiizvorske nastave iznose manje resursnog pristupa. Najbolji modeli NER postignu 0,91 F-rezultat na slovenskim podacima o obuci, dok je najbolja službena predstava postigla F-rezultate 0,84 i 0,78 za opuštene djelomične odgovarajuće i stroge nastave. U višejezičkom postavku NER postižemo F-rezultate 0,82 i 0,74.', 'cs': 'Tento článek popisuje příspěvek skupiny Univerzity v Lublani (UL FRI) ke společnému úkolu na workshopu Balto-Slovic Natural Language Processing (BSNLP) 2021. Experimentujeme s několika modely založenými na BERT, předškolenými na vícejazyčných, chorvatsko-slovinsko-anglických a slovinských datech. Školení provádíme iterativně a na řetězených datech dříve dostupných datových sad NER. Pro normalizační úlohu používáme Stanza lemmatizer, zatímco pro porovnání entit jsme implementovali směrnici pomocí knihovny Dedupe. Výkon hodnocení naznačuje, že nastavení více zdrojů překonává přístupy s méně zdrojů. Nejlepší NER modely dosahují 0,91 F skóre na slovinských tréninkových datových rozděleních, zatímco nejlepší oficiální podání dosáhlo F skóre 0,84 a 0,78 pro uvolněné částečné shodování a přísné nastavení. Ve vícejazyčném NER nastavení dosahujeme F-skóre 0.82 a 0.74.', 'ca': "Aquest article descriu les presentacions del grup de la Universitat de Ljubljana (UL FRI) a la tasca compartida a l'atelier 2021 de Procesament de Llingues Natural s (BSNLP) de Balto-esclavic. Experimentem amb múltiples models basats en BERT, pré-entrenats en dades multilingües, croata-eslovena-anglès i sols eslovena. Ens entrenem repetidament i en les dades concatenades dels conjunts de dades NER disponibles abans. Per a la tasca de normalització utilitzem el lemmatitzador Stanza, mentre per a la combinació d'entitats vam implementar una base de referencia utilitzant la biblioteca Dedupe. El rendiment de les evaluacions suggereix que les configuracions de múltiples fonts superen els enfocaments amb menys recursos. Els millors models NER aconsegueixen una puntuació F de 0,91 en divisiós de dades d'entrenament slovenès mentre la millor presentació oficial aconsegueix una puntuació F de 0,84 i 0,78 per a ajustes relaxats parcials i estrictes, respectivament. En el contexte NER multilingüe obtenim puntuacions F de 0,82 i 0,74.", 'hy': "This paper describes the University of Ljubljana (UL FRI) Group's submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop.  We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data.  Մենք կրկնօրինակ վարժեցնում ենք նախկինում հասանելի ՆԵՌ տվյալների կոնկրետ տվյալների վրա: Նորմալիզացիայի առաջադրանքի համար մենք օգտագործում ենք Սթանզա լեմմատիզերը, մինչդեռ միավորների համար մենք կիրառեցինք հիմնական հիմքը օգտագործելով DeDope գրադարան: Արժեքների արդյունքը ցույց է տալիս, որ բազմաաղբյուրների սահմանումները գերազանցում են ավելի քիչ ռեսուրսներ ունեցող մոտեցումները: Լավագույն ՆԵՌ մոդելները հասնում են 0.91 Ֆ-գնահատականի Սլովենի ուսումնասիրության տվյալների բաժանման վրա, մինչդեռ լավագույն պաշտոնական ներկայացումը հասնում է 0.84 և Բազլեզու ՆԵՌ-ի միջոցով մենք հասնում ենք F-գնահատականների 0.82 և 0.74:", 'et': 'Käesolevas artiklis kirjeldatakse Ljubljana Ülikooli (UL FRI) rühma ettepanekuid Balto-slaavi looduskeele töötlemise (BSNLP) 2021. aasta seminaril ühisele ülesandele. Me eksperimenteerime mitmete BERT-põhiste mudelitega, mis on eelnevalt koolitatud mitmekeelsete, horvaadi-sloveeni-inglise ja ainult sloveeni andmetega. Me teeme treeninguid iteratiivselt ja varem kättesaadavate NER andmekogumite ühendatud andmetel. Normaliseerimisülesandeks kasutame Stanza lemmatisaatorit, samas olemi sobitamiseks rakendasime baasjoone Dedupe teegi abil. Hindamiste tulemuslikkus näitab, et mitut allikat hõlmavad seaded on vähem ressurssidega lähenemisviisidest paremad. Parimad NER mudelid saavutavad Sloveenia treeninguandmete jagamisel 0,91 F-skoori ning parim ametlik esitus saavutas vastavalt F-skoori 0,84 ja 0,78 lõdvestunud osalise sobitamise ja rangete seadistuste puhul. Mitmekeelses NER seadistuses saavutame F-skoorid 0,82 ja 0,74.', 'bn': 'এই পত্রিকাটি লুজুব্লজানা বিশ্ববিদ্যালয়ের (UL FRI) গ্রুপ বাল্টো-স্লাভিক স্বাভাবিক ভাষার প্রক্রিয়া (বিএসএনএলপি) ২০২১ ওয়ার্কশার্কে শ We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data.  আমরা প্রশিক্ষণ বৈষম্যকভাবে প্রশিক্ষণ করি এবং পূর্ববর্তী প্রাপ্ত NER ডাটাসেট সম্পর্কে যে তথ্য তৈরি করা হয়েছে  স্বাভাবিক কাজের জন্য আমরা স্ট্যানজা লেম্যাম্যাটাজার ব্যবহার করি, যখন বস্তুর সাথে মিলিত হওয়ার জন্য আমরা ডেডুপ লাইব্রেরীর মূল্যায়নের প্রক্রিয়া পরামর্শ প্রদান করেছে যে বহুসূত্রের বৈশিষ্ট্য কম সম্পদের ক্ষেত্রে প্রয়োগ করা হয়েছে। স্লোভেনের প্রশিক্ষণের তথ্য স্প্লেটে সেরা সেরা NER মডেল ০. 91 F-স্কোর অর্জন করে যায়, যখন সর্বোচ্চ সরকারি প্রদান কর্মকর্তারা শান্তিপূর্ণ অংশ এবং কঠিন সংশো মাল্টিভাষার নেয়ার সেটিং থেকে আমরা ০. ৮২ এবং ০. ৭৪ স্কোর অর্জন করি।', 'fi': 'Tässä artikkelissa kuvataan Ljubljanan yliopiston (UL FRI) ryhmän ehdotuksia Baltoslavian luonnonkielen käsittelyn (BSNLP) 2021 työpajan yhteiseen tehtävään. Kokeilemme useita BERT-pohjaisia malleja, jotka on koulutettu monikieliseksi, kroatia-sloveeni-englanti ja vain sloveeni-dataksi. Harjoittelemme iteratiivisesti ja aiemmin saatavilla olevien NER-aineistojen yhdistettyjen tietojen pohjalta. Normalisointitehtävässä käytämme Stanza-lemmatisaattoria, kun taas entiteetin täsmäyttämisessä toteutimme perusaikataulun Dedupe-kirjaston avulla. Arviointien tulokset viittaavat siihen, että usean lähteen asetukset ovat vähemmän resurssoituja lähestymistapoja parempia. Parhaat NER-mallit saavuttavat 0,91 F-pisteen Slovenian harjoitustietojen jaossa, kun taas paras virallinen syöttö saavutti F-pisteet 0,84 ja 0,78 rennossa osittaisessa vastaavuudessa ja tiukkoissa asetuksissa. Monikielisessä NER-ympäristössä saavutamme F-pisteet 0,82 ja 0,74.', 'sk': 'V prispevku so opisani prispevki Skupine Univerze v Ljubljani (UL FRI) za skupno nalogo na baltoslovanski delavnici obdelave naravnega jezika 2021. Eksperimentiramo z več modeli BERT, ki temeljijo na večjezičnih, hrvaško-slovensko-angleških in samo slovenskih podatkih. Trening opravljamo iterativno in na podlagi povezanih podatkov prej razpoložljivih naborov podatkov NER. Za opravilo normalizacije uporabljamo lemmatizer Stanza, za ujemanje entitet pa smo izvedli osnovni načrt z uporabo knjižnice Dedupe. Izvedba ocenjevanja kaže, da nastavitve več virov presegajo pristope z manj virov. Najboljši NER modeli dosegajo 0,91 F-score na slovenskih delih podatkov o treningu, najboljši uradni oddaji pa so dosegli F-score 0,84 oziroma 0,78 za sproščeno delno ujemanje oziroma stroge nastavitve. Pri večjezičnem NER-u dosegamo F-rezultate 0,82 in 0,74.', 'ha': "Wannan karatun describes the University of Ljublajana (UL FR) Group's Submitions to the share job at the Balto-Slamic Natural language Surgering (BSNLP) 2021 workworkspace. Ko jarraba da misãlai masu baka BERT masu yawa, wanda aka yi wa kwanza a cikin multi-linguin, na-Kroati-slovone-Ingiriya da-slovone-kawai. Tuna tafiyar da mafarin aiki mai daidaita da kuma kan da aka sami da data na samun samun da ke samu'a samu'in da ake samu da NER. @ action: button Farawa ga ƙidãyayyuta yana gaya cewa tsarin masu yawa masu tsẽtare zafi-resource masu ƙaranci. Babbar masu motsi na NER za'a sami 0.91 F-score kan tsarin data na Training na Slofen a lokacin da mafi kyautar da aikin da aka samu F-score 0.84 da 0.78 for relaxed rabo da daidaita mai tsanani. Ga tsarin NER masu yawa masu samun F-score na 0.82 da 0.74.", 'jv': "Ngerti iki rambarang nggawe kelompok Universite of LJularana Awak dhéwé éntuk karo akeh model sing basa nang BERT, dadi luwih-luwih banter, krutat-slowene-Inggris lan slowene-barang. section Sampeyan ono Menu item to Open 'Search for Open Files' dialog Monday Nanging akeh-akeh luwih-luwih 'NeR' setunggal dipunangé awak dhéwé nggawe barang F-kalih 0.32 lan 0.75", 'he': 'העיתון הזה מתאר את ההעבדות של קבוצת אוניברסיטת ליובליאנה (UL FRI) למשימה המשותפת במעבד השפה הטבעית בלטו-סלאבי (BSNLP) 2021. אנחנו מנסים עם דוגמנים רבים מבוססים על BERT, מאומנים מראש במידע רב-שפתי, קרוטי-סלובני-אנגלי ורק סלובני. אנו מבצעים אימונים באופן חוזר ועל הנתונים הקצרים של קבוצות נתונים NER זמינות קודם לכן. עבור משימה הנורמליזציה אנו משתמשים בסטנזה למטיזר, בעוד עבור היחידות מתאימות אנו הפעילו בסיסית בשימוש בספרייה Dedupe. ביצועים של הערכות מציעים שסדרות ממקורים רבים מעליפות גישות פחות משאבים. הדוגמנים הטובים ביותר של NER משיגים 0.91 נקודות F על חלקי נתוני האימון הסלובליני בזמן שהשליחה הרשמית הטובה ביותר השיגה נקודות F של 0.84 ו-0.78 עבור התאמה חלקית מרגיעה ומוסדות קשות, בהתאם. במסגרת NER רב-שפותית אנו משיגים נקודות F של 0.82 ו-0.74.', 'bo': 'ཤོག་བུ་འདིས་ཡིག་ཆ་གིས་Ljubljana (UL FRI) ཚོ་ཁག་གི་ཆ་འཕྲིན་མཐུན་གྱི་ལས་འགུལ་ལ་མཉམ་དུ་ཡོད་པ་Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop སྦྲེལ་བ་རེད། ང་ཚོས་BERT་ལ་ཡོད་པའི་སྔོན་སྒྲིག་གི་མིག་གཟུགས་རིས་བློ་གཏོང་བ་ཡིན་ནའང་། སྡོར་ཤེས་སྐད་ཀྱི་སྔོན་སྒྲིག་འགོད་བྱས་ ང་ཚོས་སྔོན་གྱི་NER གནད་སྡུད་ཚན་གྱི་མཐའ་སྡུད་གྲངས་སྒྲིག་ཆ་སྒྲིག་ཚོའི་གྲངས་སྒྲིག་ཐལ་རིམ། ང་ཚོའི་རྒྱུན་ལྡན་གྱི་ལས་འགུལ་ལ་ཞིབ་པ་དང་ཨ་རིའི་ནང་འདུག རྗེས་འབྲས་འབོར་བ་ཡིན་ཚད་ལ་ཐོག་མའི་སྒྲིག་འགོད་མང་པོ་ཞིག་གིས་མཐུན་རྐྱེན་ཐབས་མེད་པར། The best NER models achieve 0.91 F-score on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. སྐད་རིགས་མང་ཙམ་གྱི་སྒྲིག་འགོད་ནང་དུ་ང་ཚོས་F-scores of 0.82 and 0.74་ཐུབ་ཀྱི་ཡོད།'}
{'en': 'Benchmarking Pre-trained Language Models for Multilingual NER : TraSpaS at the BSNLP2021 Shared Task', 'pt': 'Benchmarking de modelos de linguagem pré-treinados para NER multilíngue: TraSpaS na tarefa compartilhada BSNLP2021', 'ar': 'المقارنة المعيارية لنماذج اللغة المدربة مسبقًا لـ NER متعدد اللغات: TraSpaS في المهمة المشتركة BSNLP2021', 'es': 'Evaluación comparativa de modelos lingüísticos preentrenados para NER multilingüe: TRASPAS en la tarea compartida BSNLP2021', 'fr': 'Benchmarking des modèles linguistiques pré-entraînés pour le NER multilingue\xa0: TrasPas à la tâche partagée BSNLP2021', 'ja': '多言語NERのための事前トレーニング済み言語モデルのベンチマーク： BSNLP 2021共有タスクでのTraSpaS', 'ru': 'Сравнительный анализ предварительно подготовленных языковых моделей для многоязычного NER: TraSpaS в общей задаче BSNLP2021', 'zh': '多言 NER 预训语体,BSNLP2021 共事 TraSpaS', 'hi': 'बहुभाषी एनईआर के लिए पूर्व-प्रशिक्षित भाषा मॉडल बेंचमार्किंग: BSNLP2021 साझा कार्य में TraSpaS', 'ga': 'Samhailtí Teanga Réamhoilte a Thagarmharcáil le haghaidh NER Ilteangach: TraSpaS ag Tasc Comhroinnte BSNLP2021', 'hu': 'Benchmarking Előképzett nyelvi modellek többnyelvű NER számára: TraSpaS a BSNL2021 megosztott feladaton', 'it': 'Benchmarking Modelli linguistici pre-formati per NER multilingue: TraSpaS al BSNL2021 Shared Task', 'ka': 'მრავალენგური NER- სთვის საწყისი მუშაობის მარტივის მოდელები: TraSpaS BSNLP2021 საზოგადოებული მომუშაობაში', 'kk': 'Көп тілді NER үшін алдын- ала оқылған тіл үлгілері: BSNLP2021 ортақ тапсырмасында TraSpaS', 'el': 'Αξιολόγηση Προεκπαιδευμένων Γλωσσικών Μοντέλων για Πολυγλωσσικούς ΝΕR: στο Κοινό Καθήκον του BSNLP2021', 'ml': 'മുന്\u200dട്രെയിന്\u200dറ് പരിശീലന ഭാഷ മോഡലുകള്\u200d ബെന്\u200dമെങ്കിങ് ചെയ്യുന്നു: BSNLP2021 പങ്കെടുത്ത ജോലിയിലെ ട്രാസ്പാസ്', 'ms': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'mk': 'Параметри на преобучени јазични модели за мултијазични NER: TraSpaS на заедничката задача BSNLP2021', 'mn': 'Олон хэлний NER-ын өмнөх сургалтын хэл загваруудын банкмаркинг: TraSpaS at the BSNLP2021 Shared Task', 'lt': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'no': 'Benchmarking før- treng språk- modeller for fleirspråk NER: TraSpaS på BSNLP2021 delt oppgåve', 'sr': 'Preobučeni jezički modeli za višejezički NER: TraSpaS na BSNLP2021 zajedničkom zadatku', 'mt': 'Benchmarking Pre-Trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'pl': 'Porównanie wstępnie przeszkolonych modeli językowych dla wielojęzycznego NER: TraSpaS na wspólnym zadaniu BSNLP2021', 'ro': 'Benchmarking Modele lingvistice pre-instruite pentru NER multilingv: TraSpaS la sarcina partajată BSNL2021', 'ta': 'பல மொழி NER க்கான முன் பயிற்சி மொழி மாதிரிகளை பென்பாக்குதல்: BSNLP2021 பகிர்ந்த பணியில் TraSpaS', 'ur': 'بہت سی زبان NER کے لئے پیش آموزش کی زبان موڈل بنچ مارکینگ: BSNLP2021 شریک ٹاکس پر TraSpaS', 'si': 'බෙන්ච්මාර්ක් කරන්න පුරුද්ගලික භාෂාවක් නිර්මාණය NER වෙනුවෙන් ප්\u200dරධාන භාෂාවක් නිර්මාණය: TraspaS BSNLP202', 'so': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'sv': 'Benchmarking Förtränade språkmodeller för flerspråkiga NER: TraSpaS vid BSNL2021 Shared Task', 'vi': 'Đối tượng chuẩn bị ngôn ngữ đa ngôn ngữ:', 'uz': 'Bir nechta tili NER uchun pre-trained Til Models: TraSpaS at BSNLP2021 Shared Vazifa', 'nl': 'Benchmarking Vooropgeleide Taalmodellen voor meertalige NER: TraSpaS op de gezamenlijke taak BSNLP2021', 'bg': 'Предварително обучени езикови модели за многоезичен НЕР: Споделена задача', 'hr': 'Preobučeni jezički modeli za višejezički NER: TraSpaS na BSNLP2021 zajedničkom zadatku', 'de': 'Benchmarking vortrainierter Sprachmodelle für mehrsprachige NER: TraSpaS bei der gemeinsamen Aufgabe BSNLP2021', 'da': 'Benchmarking Forududdannede sprogmodeller til flersprogede NER: TraSpaS ved BSNL2021 Shared Task', 'ko': 'BSNLP2021 공유 퀘스트에서 다중 언어 NER:TRASPA 테스트를 위한 사전 훈련 언어 모델', 'fa': 'نمونه\u200cهای زبان پیش آموزش برای NER Multilingual: TraSpaS در کار مشترک BSNLP2021', 'sw': 'Mradi wa lugha zilizofundishwa mbele wa mafunzo ya lugha kwa ajili ya NERE ya lugha nyingi: TraSpaS kwenye kazi ya BSNLP2021', 'tr': 'Çoklu diller NER üçin öňlerden eğlenen Diller nusgalary: TraSpaS BSNLP2021 Paýlaşylyş Görevinde', 'am': 'benchmarking', 'id': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'sq': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'hy': 'Բազլեզու ՆԵՌ-ի համար նախապատրաստված լեզվի մոդելների համեմատական նշաններ՝', 'az': 'Çoxlu dilli NER üçün öyrənmiş dil modelləri: BSNLP2021 paylaşılmış iş işlərdə TraSpaS', 'af': 'Benchmarking Vorige- oefening Taal Modelle vir Multilingual NER: TraSpas na die BSNLP2021 Gedeelde Opdrag', 'bn': 'বহুভাষী নেরার জন্য পূর্ব প্রশিক্ষিত ভাষা মডেল ব্যাঙ্কিং করা হচ্ছে: BSNLP2021 শেয়ার করা কাজের ট্রাস্প্যাস', 'ca': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task', 'cs': 'Srovnávání předškolených jazykových modelů pro vícejazyčné NER: TraSpaS na BSNLP2021 Shared Task', 'et': 'Eelkoolitud keelemudelite võrdlusanalüüs mitmekeelse NER-i jaoks: TraSpaS BSNLP2021 jagatud ülesandes', 'bs': 'Preobučeni jezički modeli za višejezički NER: TraSpaS na BSNLP2021 zajedničkom zadatku', 'fi': 'Esikoulutetut kielimallit monikieliselle NER:lle: TraSpaS BSNLP2021 Shared Task', 'sk': 'Primerjalna analiza Predusposobljenih jezikovnih modelov za večjezični NER: TraSpaS na BSNLP2021 Shared Task', 'ha': 'KCharselect unicode block name', 'he': 'מודלים לשפה מאומנים מראש למשפטים רבים NER: TraSpaS במשימה משותפת BSNLP2021', 'jv': 'Bench-marking Preview-Tried Language Modes for Multilanguage NeR: TraSpaS at the GSNLP2020 1 shared task', 'bo': 'Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task'}
{'en': 'In this paper we describe TraSpaS, a submission to the third shared task on named entity recognition hosted as part of the Balto-Slavic Natural Language Processing (BSNLP) Workshop. In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits : character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data. Our code is available at.https://github.com/NaiveNeuron/slavner-2021.', 'fr': "Dans cet article, nous décrivons TrAsPas, une soumission à la troisième tâche partagée sur la reconnaissance d'entités nommées organisée dans le cadre de l'atelier Balto-Slavic Natural Language Processing (BSNLP). Dans ce document, nous évaluons divers modèles de langage pré-entraînés sur la tâche NER à l'aide de trois boîtes à outils de PNL open source\xa0: un modèle de langage au niveau du caractère avec Stanza, des modèles de style BERT spécifiques à une langue avec SpaCy et un XLM-R compatible avec Adapter avec Trankit. Nos résultats montrent que les modèles basés sur Trankit ont surpassé ceux basés sur les deux autres boîtes à outils, même lorsqu'ils ont été formés sur de petites quantités de données. Notre code est disponible à l'adresse < https://github.com/NaiveNeuron/slavner-2021 >.", 'es': 'En este artículo describimos las TRASPAS, una presentación a la tercera tarea compartida sobre el reconocimiento de entidades nombradas organizada como parte del taller de procesamiento del lenguaje natural balto-eslavo (BSNLP). En él evaluamos varios modelos de lenguaje previamente entrenados en la tarea NER utilizando tres kits de herramientas de PNL de código abierto: modelo de lenguaje a nivel de caracteres con Stanza, modelos de estilo BERT específicos del idioma con SpAcy y XLM-R habilitado para Adapter con Trankit. Nuestros resultados muestran que los modelos basados en Trankit superaron a los basados en los otros dos conjuntos de herramientas, incluso cuando se entrenaron con cantidades más pequeñas de datos. Nuestro código está disponible en < https://github.com/NaiveNeuron/slavner-2021 >.', 'pt': 'Neste artigo descrevemos o TraSpaS, uma submissão para a terceira tarefa compartilhada no reconhecimento de entidade nomeada hospedada como parte do Workshop Balto-Slavic Natural Language Processing (BSNLP). Nele, avaliamos vários modelos de linguagem pré-treinados na tarefa NER usando três kits de ferramentas NLP de código aberto: modelo de linguagem de nível de caractere com Stanza, modelos de estilo BERT específicos de linguagem com SpaCy e XLM-R habilitado para adaptador com Trankit. Nossos resultados mostram que os modelos baseados em Trankit superaram aqueles baseados nos outros dois kits de ferramentas, mesmo quando treinados em quantidades menores de dados. Nosso código está disponível em <https://github.com/NaiveNeuron/slavner-2021>.', 'zh': '本文述TraSpaS,是为Balto-Slavic自然语言处(BSNLP)研讨会之一而托管名实之三也。 三开源NLP工具包评估NER诸预训语模:带Stanza字符级言语模形,用SpaCy特定于语言BERT风格带Trankit之适配器启用之XLM-R。 吾之的结果表明,Trankit之于二工具包,虽少数据量习亦如之。 吾代码可<https://github.com/NaiveNeuron/slavner-2021>。', 'ja': 'この論文では、バルト・スラブ自然言語処理（ BSNLP ）ワークショップの一環として開催された、名前付きエンティティ認識に関する3番目の共有タスクへの提出物であるTraSpaSについて説明します。その中で、私たちは3つのオープンソースのNLPツールキットを使用して、NERタスク上のさまざまな事前にトレーニングされた言語モデルを評価します： Stanzaを使用した文字レベルの言語モデル、SpaCyを使用した言語固有のBERTスタイルモデル、およびTrankitを使用したアダプター対応のXLM - R。私たちの結果は、Trankitベースのモデルが、少量のデータで訓練された場合でも、他の2つのツールキットに基づいたモデルよりも優れていることを示しています。コードはで入手でき<https://github.com/NaiveNeuron/slavner-2021>ます。', 'ar': 'في هذه الورقة ، نصف TraSpaS ، وهو تقديم للمهمة المشتركة الثالثة حول التعرف على الكيانات المسماة المستضافة كجزء من ورشة Balto-Slavic Natural Language Processing (BSNLP). نقوم فيه بتقييم العديد من نماذج اللغة المدربة مسبقًا على مهمة NER باستخدام ثلاث مجموعات من أدوات البرمجة اللغوية العصبية مفتوحة المصدر: نموذج لغة على مستوى الحرف مع Stanza ، ونماذج على غرار BERT خاصة باللغة مع SpaCy و XLM-R ممكّن للمحول مع Trankit. تظهر نتائجنا أن النماذج المستندة إلى Trankit تفوقت في الأداء على تلك القائمة على مجموعتي الأدوات الأخريين ، حتى عند التدريب على كميات أقل من البيانات. الكود الخاص بنا متاح على <https://github.com/NaiveNeuron/slavner-2021>.', 'hi': 'इस पेपर में हम TraSpaS का वर्णन करते हैं, जो बाल्टो-स्लाव प्राकृतिक भाषा प्रसंस्करण (BSNLP) कार्यशाला के हिस्से के रूप में होस्ट की गई नामित इकाई मान्यता पर तीसरे साझा कार्य के लिए एक सबमिशन है। इसमें हम तीन ओपन-सोर्स एनएलपी टूलकिट का उपयोग करके एनईआर कार्य पर विभिन्न पूर्व-प्रशिक्षित भाषा मॉडल का मूल्यांकन करते हैं: छंद के साथ चरित्र स्तर का भाषा मॉडल, स्पैसी के साथ भाषा-विशिष्ट बर्ट-शैली मॉडल और ट्रैंकिट के साथ एडाप्टर-सक्षम एक्सएलएम-आर। हमारे परिणाम बताते हैं कि Trankit-आधारित मॉडल ने अन्य दो टूलकिट के आधार पर उन लोगों को पछाड़ दिया, भले ही डेटा की छोटी मात्रा पर प्रशिक्षित किया गया हो। हमारा कोड <https://github.com/NaiveNeuron/slavner-2021> पर उपलब्ध है।', 'ru': 'В этой статье мы описываем TraSpaS, представление к третьей общей задаче по распознаванию именованных сущностей, проведенной в рамках семинара по обработке балто-славянских естественных языков (BSNLP). В нем мы оцениваем различные предварительно обученные языковые модели по задаче NER, используя три инструментария NLP с открытым исходным кодом: языковая модель на уровне символов с Stanza, языковые модели в стиле BERT с SpaCy и XLM-R с поддержкой адаптера с Trankit. Наши результаты показывают, что модели, основанные на Trankit, превосходят модели, основанные на двух других инструментах, даже если они обучены меньшим объемам данных. Наш код доступен по адресу<https://github.com/NaiveNeuron/slavner-2021>.', 'ga': 'Sa pháipéar seo déanaimid cur síos ar TraSpaS, aighneacht don tríú tasc roinnte maidir le haitheantas aonáin ainmnithe a óstáiltear mar chuid de Cheardlann Próiseála Teanga Nádúrtha Balto-Slavacha (BSNLP). In sé déanaimid measúnú ar mhúnlaí teanga réamh-oilte éagsúla ar thasc NER ag baint úsáide as trí fhoireann uirlisí foinse oscailte NLP: múnla teanga ar leibhéal na gcarachtar le Stanza, samhlacha teanga-shonracha ar stíl BERT le SpaCy agus XLM-R le cumas Adapter le Trankit. Léiríonn ár dtorthaí gur sháraigh na samhlacha Trankit na cinn a bhí bunaithe ar an dá fhoireann uirlisí eile, fiú nuair a cuireadh oiliúint orthu ar mhéideanna níos lú sonraí. Tá ár gcód ar fáil ag <https://github.com/NaiveNeuron/slavner-2021>.', 'hu': 'Jelen tanulmányban bemutatjuk a TraSpaS-t, amely a Balto-szláv Természetes Nyelvfeldolgozás (BSNLP) Workshop részeként megvalósított nevezett entitások felismerésére vonatkozó harmadik megosztott feladat benyújtását. Ebben különböző, előre képzett nyelvi modelleket értékelünk a NER feladathoz három nyílt forráskódú NLP eszköztárral: karakterszintű nyelvi modellek Stanzával, nyelvspecifikus BERT stílusú modellek SpaCy-vel és Adapter-kompatibilis XLM-R Trankit-tel. Eredményeink azt mutatják, hogy a Trankit alapú modellek a másik két eszközkészleten alapuló modellek teljesítményét felülmúlták, még akkor is, ha kisebb mennyiségű adatokra képzettek. A kódunk elérhető a < https://github.com/NaiveNeuron/slavner-2021 >.', 'el': 'Στην παρούσα εργασία περιγράφουμε το TraSpaS, μια υποβολή στην τρίτη κοινή εργασία σχετικά με την αναγνώριση ονομασίας οντότητας που φιλοξενείται στο πλαίσιο του εργαστηρίου επεξεργασίας βαλτοσλαβικής φυσικής γλώσσας (BSNLP). Σε αυτό αξιολογούμε διάφορα προ-εκπαιδευμένα γλωσσικά μοντέλα για την εργασία χρησιμοποιώντας τρία κιτ εργαλείων ανοιχτού κώδικα: μοντέλο γλώσσας επιπέδου χαρακτήρων με τη Στάνζα, μοντέλα τύπου τύπου BERT με SpaCy και με δυνατότητα προσαρμογής XLM-R με Trankit. Τα αποτελέσματά μας δείχνουν ότι τα μοντέλα που βασίζονται στο Trankit ξεπερνούν αυτά που βασίζονται στα άλλα δύο κιτ εργαλείων, ακόμα και όταν εκπαιδεύονται σε μικρότερες ποσότητες δεδομένων. Ο κωδικός μας είναι διαθέσιμος στο < https://github.com/NaiveNeuron/slavner-2021 >.', 'kk': 'Бұл қағазда TraSpaS дегенді түсіндіредік. Бұл үшінші ортақтастырылған тапсырманы Balto- Slavic Natural Language Processing (BSNLP) жұмысының бір бөлігі деп аталатын нысандарды анықтау. Осында NER тапсырмасындағы көзі ашық NLP құралдарын қолдану үшін әртүрлі тіл үлгілерін бағалаймыз: Stanza деңгейіндегі таңбалар тілінің үлгісі, SpaCy мен Adapter қолданылған XLM- R үлгілерімен белгілер үлгілерімен BERT- стил Біздің нәтижелеріміз Транкит негіздеген үлгілер басқа екі құралдар құралдарына негізделген үлгілерді өзгертетінін көрсетеді. Кіші мәліметтерді бақылағанда да. Біздің кодмыз < https://github.com/NaiveNeuron/slavner-2021 >.', 'it': "In questo articolo descriviamo TraSpaS, una presentazione al terzo compito condiviso sul riconoscimento di entità nominative ospitato come parte del Balto-Slavic Natural Language Processing (BSNLP) Workshop. In esso valutiamo vari modelli linguistici pre-addestrati sul compito NER utilizzando tre toolkit NLP open source: modello linguistico a livello di carattere con Stanza, modelli in stile BERT specifici per il linguaggio con SpaCy e XLM-R con Trankit abilitato per l'adattatore. I nostri risultati mostrano che i modelli basati su Trankit hanno superato quelli basati sugli altri due toolkit, anche se addestrati su piccole quantità di dati. Il nostro codice è disponibile all'indirizzo < https://github.com/NaiveNeuron/slavner-2021 >.", 'ms': 'Dalam kertas ini kami menggambarkan TraSpaS, penghantaran ke tugas terkongsi ketiga mengenai pengenalan entiti bernama yang diterima sebagai sebahagian dari Kerja kerja Balto-Slavic Natural Language Processing (BSNLP). Di dalamnya kami menilai pelbagai model bahasa pra-dilatih pada tugas NER menggunakan tiga kumpulan alat NLP sumber terbuka: model bahasa aras aksara dengan Stanza, model gaya BERT-spesifik bahasa dengan SpaCy dan XLM-R dengan Trankit dibenarkan-Adapter. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data.  Kod kami tersedia di < https://github.com/NaiveNeuron/slavner-2021 >.', 'ml': 'ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d ട്രാസ്പാസിനെ വിവരിക്കുന്നു. ബാല്\u200dട്ടോ-സ്ലാവിക് നാറല്\u200d ഭാഷ പ്രക്രിയശ്ചിത്രത്തിന്\u200dറെ (BSNLP) വര്\u200dക്കിഷനില്\u200d ഒരു ഭാഗമ നെആര്\u200d ജോലിയില്\u200d നിന്നും മുമ്പ് പരിശീലന മാതൃകങ്ങള്\u200d മൂന്നു തുറന്ന NLP ഉപകരണങ്ങള്\u200d ഉപയോഗിച്ച് നാം ഇതില്\u200d വ്യത്യസ്ത മാതൃകങ്ങള്\u200d വിലയിച്ചുകൊടുക്കുന്നു. സ്പാന്\u200dസായിയോടൊപ നമ്മുടെ ഫലങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നത് ട്രാങ്കിക്റ്റിന്\u200dറെ അടിസ്ഥാനത്തുള്ള മോഡലുകള്\u200d മറ്റു രണ്ട് ഉപകരണങ്ങള്\u200d അടിസ് ഞങ്ങളുടെ കോഡ് ലഭ്യമല്ല < https://github.com/NaiveNeuron/slavner-2021 >.', 'mt': 'F’dan id-dokument niddeskrivu TraSpaS, sottomissjoni għat-tielet kompitu kondiviż dwar ir-rikonoxximent tal-entità msejħa ospitat bħala parti mill-Workshop tal-Proċess tal-Lingwi Naturali tal-Balto-Slavi (BSNLP). Fih jevalwaw diversi mudelli lingwistiċi mħarrġa minn qabel dwar il-kompitu NER bl-użu ta’ tliet settijiet ta’ għodod NLP open-source: mudell lingwistiku fil-livell tal-karattri b’Stanza, mudelli tal-istil BERT speċifiċi għall-lingwa b’SpaCy u XLM-R b’Trankit b’Adapter. Ir-riżultati tagħna juru li l-mudelli bbażati fuq it-Trankit qabżu dawk ibbażati fuq iż-żewġ settijiet l-oħra ta’ għodod, anki meta mħarrġa fuq ammonti iżgħar ta’ dejta. Il-kodiċi tagħna huwa disponibbli fuq < https://github.com/NaiveNeuron/slavner-2021 >.', 'lt': 'Šiame dokumente apibūdiname TraSpaS, trečią bendrą užduotį dėl vardinio subjekto pripažinimo, kuris buvo surengtas kaip Balto slavo gamtos kalbų apdorojimo (BSNLP) seminaro dalis. Šiame dokumente vertiname įvairius iš anksto parengtus kalbos modelius NER užduotyje, naudojant tris atviro kodo NLP įrankių rinkinius: ženklų lygio kalbos model į su Stanza, kalbos specifinius BERT modelius su SpaCy ir pritaikytu XLM-R su Trankit. Mūsų rezultatai rodo, kad Trankite pagrįsti modeliai viršijo tuos, kurie buvo pagrįsti kitais dviem priemonių rinkiniais, net jei jie buvo apmokyti mažesniais duomenimis. Mūsų kodas pateikiamas < https://github.com/NaiveNeuron/slavner-2021 >.', 'mn': 'Энэ цаасан дээр бид TraSpaS-г тайлбарлаж байна. Балто-Славийн Байгалийн Холбоо Процессорын (BSNLP) ажлын нэг хэсэг болгон нэрлэгдсэн бүтэц хүлээн зөвшөөрөгдсөн гурав дахь хуваалтын ажил. Үүнд бид NER ажил дээрх олон сургалтын өмнө сургалтын хэл загварыг 3 нээлттэй эх үүсвэрийн NLP хэрэгсэл хэрэгсэл ашиглан дүрслэж байна: Стэнцатай харьцаа хэл загварын загвар, SpaCy болон Адаптер болон Trankit-тай XLM-R хэлбэртэй BERT-хэлб Манай үр дүнд Транкит суурилсан загварууд бусад хоёр хэрэгсэл хэрэгсэл дээр бага хэмжээний мэдээлэл дээр сургалтын тулд бага зэрэг хийсэн юм. Бидний код https://github.com/NaiveNeuron/slavner-2021 >.', 'ka': 'ამ დომენტში ჩვენ განახსენებთ TraSpaS, სამუშაო გაყოფილი სამუშაო გაყოფილი სამუშაო რაქაღაზე, რომელიც სახელი განახსენებული ინტერტიკის განახსენებაზე, რომელიც ჰოსტირებულია Balto- ჩვენ განვითარებთ განსხვავებული წავლის მოდელები NER დავალების გამოყენებაში სამი გახსნილი NLP ხელსაწყობილობის გამოყენებით: სიმბოლობის წავლის მოდელი Stanza-ს, ენის განსხვავებული BERT-სტილის მოდელები SpaCy და აეპტერტის შესა ჩვენი წარმოდგენები აჩვენებენ, რომ ტრანკიტის ბაზეული მოდელები გავამუშავებენ ესხვა ორი ხელსაწყოთა ხელსაწყოთა ხელსაწყოთან, მაგრამ როდესაც ცოტა მონაცემებ ჩვენი კოდის შესაძლებელია < https://github.com/NaiveNeuron/slavner-2021 - ეა.', 'no': 'I denne papiret beskriver vi TraSpaS, ei oppføring til den tredje delte oppgåva om gjenkjenning av namnet entitet vert vert som del av arbeidsområdet for Balto- Slavic Natural Language Processing (BSNLP). I det er det vi evaluerer forskjellige språk- modeller på NER- oppgåva med tre open-source NLP- verktøylinjer: teiknspråk- modeller med Stanza, språk- spesifikke BERT- stilmodeller med SpaCy og Adapter- aktivert XLM- R med Trankit. Resultatet våre viser at trankit-baserte modelane utførte dei baserte på dei andre to verktøylinjene, sjølv når det trengte på mindre mengda data. Koden vårt er tilgjengeleg på < https://github.com/NaiveNeuron/slavner-2021 >', 'ro': 'În această lucrare descriem TraSpaS, o depunere la a treia sarcină comună privind recunoașterea entităților denumite găzduită în cadrul Workshop-ului Balto-Slavic Natural Language Processing (BSNLP). În cadrul acestuia evaluăm diferite modele lingvistice pre-instruite cu privire la sarcina NER folosind trei seturi de instrumente NLP cu sursă deschisă: model de limbaj la nivel de caracter cu Stanza, modele de stil BERT specifice limbajului cu SpaCy și XLM-R cu Trankit activat cu adaptor. Rezultatele noastre arată că modelele bazate pe Trankit au depășit cele bazate pe celelalte două seturi de instrumente, chiar și atunci când sunt instruite pe cantități mai mici de date. Codul nostru este disponibil la < https://github.com/NaiveNeuron/slavner-2021 >.', 'mk': 'Во овој весник го опишуваме Траспас, поднесување на третата заедничка задача за препознавање на именуваниот ентитет, домаќин како дел од работилницата за балто-славски природен јазик процес (БСНЛП). Во неа ги проценуваме различните предобучени јазички модели на задачата НЕР користејќи три набори на алатки на отворен код НЛП: јазички модел на ниво на карактери со Станза, јазички модели на стил БЕРТ со SpaCy и Адаптер-овозможени XLM-R со Trankit. Нашите резултати покажуваат дека моделите базирани на Транките ги надминаа оние базирани на другите две алатки, дури и кога се обучуваат на помали количини на податоци. Нашиот код е достапен на < https://github.com/NaiveNeuron/slavner-2021 >.', 'si': 'මේ පත්තරේ අපි TraspaS විස්තර කරනවා, තුන්වෙනි භාෂාව ප්\u200dරවේශනය (BSNLP) කාර්යාලයේ භාෂාවේ කොටසක් විදියට පත්තර කරලා තියෙන තුන්වෙ අපි ඒකෙන් විවිධ ප්\u200dරධානය කරපු භාෂාව මොඩේල් අනුවෙන් NER වැඩේ තියෙන්නේ විවිධ ප්\u200dරධානය කරපු භාෂාව මොඩේල් තුනක් භාවිතා කරන්න: ස්ටැන්සා සමග ස් අපේ ප්\u200dරතිචාරය පෙන්වන්නේ ට්\u200dරැන්කිට් අධිරූපයේ මොඩේල් එකේ අනිත් උපකරණ දෙකක් අධිරූපය තියෙනවා කියලා. අපේ කෝඩ් තියෙන්නේ <at> https://github.com/NaiveNeuron/slavner-2021 >', 'pl': 'W niniejszym artykule opisujemy TraSpaS, zgłoszenie do trzeciego wspólnego zadania dotyczącego rozpoznawania nazwanych podmiotów prowadzonego w ramach Warsztatu Balto-Slavic Natural Language Processing (BSNLP). W nim oceniamy różne wstępnie przeszkolone modele językowe dotyczące zadania NER przy użyciu trzech open-source narzędzi NLP: model językowy na poziomie znaków z Stanzą, model językowy BERT w stylu SpaCy oraz XLM-R z Trankit. Nasze wyniki pokazują, że modele oparte na Trankit przewyższają te oparte na pozostałych dwóch zestawach narzędzi, nawet gdy są przeszkolone na mniejszych ilościach danych. Nasz kod jest dostępny pod adresem < https://github.com/NaiveNeuron/slavner-2021 >.', 'sv': 'I denna uppsats beskriver vi TraSpaS, en inlämning till den tredje delade uppgiften om namngiven entitetsigenkänning som en del av Balto-Slavic Natural Language Processing (BSNLP) Workshop. I den utvärderar vi olika förklädda språkmodeller på NER-uppgiften med hjälp av tre NLP-verktygssatser med öppen källkod: teckennivå språkmodell med Stanza, språkspecifika BERT-modeller med SpaCy och Adapter-aktiverad XLM-R med Trankit. Våra resultat visar att de Trankit-baserade modellerna presterade bättre än de andra två verktygssatserna, även när de tränas på mindre datamängder. Vår kod finns tillgänglig på < https://github.com/NaiveNeuron/slavner-2021 >.', 'sr': 'U ovom papiru opisujemo TraSpaS, podnošenje trećem zajedničkom zadatku o priznanju entiteta domaćina kao deo radionice Balto-Slavičkog prirodnog jezika (BSNLP). U njemu procjenjujemo različite pre-obučene jezičke modele na NER zadatku koristeći tri otvorena izvora NLP alata: model jezika na nivou karaktera sa Stanzom, modeli za jezik specifični BERT stil sa Spacyjem i adapterom omogućeni XLM-R sa Trankitom. Naši rezultati pokazuju da su modeli koji su bazirani na Trankitu nadmašili one na temelju ostalih dva alata, čak i kada su obučeni na manje količine podataka. Naš kod je dostupan na < https://github.com/NaiveNeuron/slavner-2021 >', 'ur': 'اس کاغذ میں ہم ٹراسپاس کو توصیح دیتے ہیں، ایک تیسرے شریک کام کے ذریعہ ایک ایستی شناسایی کے ذریعہ جو Balto-Slavic Natural Language Processing (BSNLP) کارشاپ کے حصہ میں حاضر ہے۔ اس میں ہم نے NER ٹاکس پر مختلف پیش آموزش کی زبان نمڈلوں کا ارزش کرلیا تھا تین اوپن سورس NLP تولیک کیٹوں کے مطابق: کراتر سطح کی زبان نمڈل استنزا کے ساتھ، زبان-مختلف BERT-استیل نمڈلوں کے ساتھ اسپاسی اور اڈپٹر-فعال XLM-R کے ہمارے نتیجے دکھاتے ہیں کہ ٹرنکیٹ کی بنیادی موڈل ان لوگوں کو دوسرے دو تولیک کیٹوں پر بنیاد رکھتے ہیں، اگرچہ چھوٹے اندازے پر آموزش کی جاتی ہیں۔ ہمارا کوڈ <پر موجود ہے https://github.com/NaiveNeuron/slavner-2021 >', 'so': 'Kanu warqaddan waxaan ku qornaa TraSpaS, warqad saddexaad oo la qaybsan yahay oo lagu magacaabay aqoonsiga entity ee loo hoggaamiyey qeyb ka mid ah warqada shaqaalaha baaraandegista afka asalka (BSNLP) ee Balto-Slavic. Markaas waxaynu ku qiimeynaynaa noocyo af kala duduwan oo ku qoran shaqada NER oo isticmaalaya saddex nooc oo furan oo NLP toolkit: qoraalka heerka afka ah ee Stanza, models-specific BERT-style with SpaCy iyo Adapter-enabled XLM-R with Trankit. Midhahayaga waxaa tusinaya in tusaalayaasha ku saleysan labada qalabka kale, xataa marka lagu tababaray tiro yar oo macluumaad ah. Our code is available at < https://github.com/NaiveNeuron/slavner-2021 >.', 'ta': 'இந்த காகிதத்தில் நாம் ட்ராஸ்பாஸ் விவரிக்கிறோம், மூன்றாம் பகிர்ந்த பணிக்கு, பெயர் பொருள் அடையாளம் பால்டோ- ஸ்லாவிக் இயல்பான மொழி செயல்பா இதில் நாம் முன்பயிற்சி மொழி மாதிரி மாதிரிகளை மூன்று திறந்த NLP கருவிப்பொருள் பயன்படுத்தி NER பணியில் மதிப்பிடுகிறோம்: எழுத்து நிலை மொழி மாதிரி மாதிரி ஸ்டான்சா எங்கள் முடிவுகள் தெரியும் டான்க்கிட் அடிப்படையிலான மாதிரிகள் மற்ற இரண்டு கருவிப்பொறிகளை அடிப்படையில் செய்தார் எங்கள் குறியீடு உள்ளது < https://github.com/NaiveNeuron/slavner-2021 >.', 'uz': "Bu sahifani biz TraSpaS (BSNLP) workshopasi (Balto-Slavic Natural Tilning jarayonlarining qismi) deb nomli narsa bilan uchinchi boʻlishilgan vazifani yaratishimiz mumkin. Bu yerda biz uchta ochiq manba NLP asboblar yordamida oldin o'rganilgan tillar modellarini qiymatimiz: Stanza bilan character level modeli, SpaCy va Adapter ishlatadigan XLM-R va Trankit bilan foydalanishimiz mumkin. Bizning natijalarimiz esa Trankit asosida modellari boshqa ikki asboblar asosida bajaradi. Agar bir kichkina maʼlumot sohasida o'rganilganda ko'ra ko'proq o'rganadi. Kodlash usuli <da mavjud https://github.com/NaiveNeuron/slavner-2021 >.", 'vi': 'Trong tờ giấy này, chúng tôi mô tả TraSpaS, một đơn trình bày nhiệm vụ chia sẻ thứ ba về việc nhận dạng thực thể tên được tổ chức như một phần của xưởng sản xuất ngôn ngữ tự nhiên Balto-Slavic (BSNLP). Trong đó chúng tôi đánh giá các mô hình ngôn ngữ đã được đào tạo về phi vụ NER, sử dụng ba bản công cụ Nchọc dò nguồn mở: mô hình ngôn ngữ với Stanza, mô hình đặc trưng của thiếu sót ngôn ngữ với SpaCy và Adaptar-invite XLM-R với Trankit. Kết quả của chúng tôi cho thấy các mô hình dựa trên Trankit đã hoàn thành kết quả dựa trên hai bộ dụng cụ khác, thậm chí khi được huấn luyện về lượng dữ liệu nhỏ hơn. Mật mã của chúng tôi có sẵn ở https://github.com/NaiveNeuron/slavner-2021 -̀.', 'bg': 'В настоящата статия описваме предаване на третата споделена задача за разпознаване на наименовани субекти, организирана като част от балтославянската работилница за обработка на естествени езици (БНЛП). В него оценяваме различни предварително обучени езикови модели по задачата с помощта на три инструмента с отворен код за НЛП: езиков модел на ниво символ със Станза, езикови специфични модели в стил с SpaCy и адаптер-активиран XLM-R с Trankit. Нашите резултати показват, че моделите, базирани на Транкит, са по-добри от тези, базирани на другите два инструментариума, дори когато са обучени на по-малки количества данни. Нашият код е достъпен на < https://github.com/NaiveNeuron/slavner-2021 >.', 'da': 'I denne artikel beskriver vi TraSpaS, en indsendelse til den tredje delte opgave om navngivne entiteter anerkendelse hostet som en del af Balto-Slavic Natural Language Processing (BSNLP) Workshop. I den evaluerer vi forskellige prætrænede sprogmodeller på NER-opgaven ved hjælp af tre open source NLP-værktøjssæt: tegn niveau sprogmodel med Stanza, sprogspecifikke BERT-modeller med SpaCy og Adapter-aktiveret XLM-R med Trankit. Vores resultater viser, at de Trankit-baserede modeller klarede bedre end dem baseret på de to andre værktøjssæt, selv når de trænede på mindre mængder data. Vores kode er tilgængelig på < https://github.com/NaiveNeuron/slavner-2021 >.', 'nl': 'In dit artikel beschrijven we TraSpaS, een inzending aan de derde gedeelde taak over naamsbekendheid georganiseerd als onderdeel van de Balto-Slavische Natural Language Processing (BSNLP) Workshop. Hierin evalueren we verschillende voorgetrainde taalmodellen op de NER taak met behulp van drie open-source NLP toolkits: karakterniveau taalmodel met Stanza, taalspecifieke BERT-stijl modellen met SpaCy en Adapter-enabled XLM-R met Trankit. Onze resultaten tonen aan dat de op Trankit gebaseerde modellen beter presteerden dan die op de andere twee toolkits, zelfs als ze getraind waren op kleinere hoeveelheden gegevens. Onze code is beschikbaar op < https://github.com/NaiveNeuron/slavner-2021 >.', 'de': 'In diesem Beitrag beschreiben wir TraSpaS, eine Einreichung zur dritten gemeinsamen Aufgabe zur Erkennung benannter Entitäten, die im Rahmen des Balto-Slavic Natural Language Processing (BSNLP) Workshops durchgeführt wird. Darin evaluieren wir verschiedene vortrainierte Sprachmodelle zur NER-Aufgabe mit drei Open-Source-NLP-Toolkits: Zeichenebene-Sprachmodell mit Stanza, sprachspezifische BERT-Modelle mit SpaCy und adaptergestütztes XLM-R mit Trankit. Unsere Ergebnisse zeigen, dass die Trankit-basierten Modelle diejenigen übertrafen, die auf den anderen beiden Toolkits basieren, selbst wenn sie auf kleineren Datenmengen trainiert wurden. Unser Code ist verfügbar unter < https://github.com/NaiveNeuron/slavner-2021 >.', 'id': 'Dalam kertas ini kami menggambarkan TraSpaS, pengiriman ke tugas berbagi ketiga mengenai pengenalan entitas bernama diterima sebagai bagian dari Workshop Balto-Slavic Natural Language Processing (BSNLP). Di dalamnya kami mengevaluasi berbagai model bahasa yang sudah dilatih pada tugas NER menggunakan tiga set alat NLP sumber terbuka: model bahasa tingkat karakter dengan Stanza, model gaya BERT spesifik bahasa dengan SpaCy dan XLM-R dengan Trankit yang diaktifkan oleh Adapter. Hasil kami menunjukkan bahwa model berdasarkan Trankit melampaui batas pada dua toolkit lainnya, bahkan ketika dilatih pada jumlah data yang lebih kecil. Kode kami tersedia di < https://github.com/NaiveNeuron/slavner-2021 >.', 'fa': 'در این کاغذ ما تراسپاس را توصیف می\u200cکنیم، یک تسلیم به کار سوم مشترک در مورد شناسایی entity نامیده شده به عنوان بخشی از کارگاه پروسیس زبان طبیعی Balto-Slavic (BSNLP) است. در آن ما مدل های پیش آموزش زبانی مختلف را در کار NER با استفاده از سه ابزار NLP منبع باز ارزیابی می کنیم: مدل زبانی سطح شخصیت با استنزا، مدل های سبک BERT خاص زبان با اسپاسی و adapter-enabled XLM-R با Trankit. نتیجه\u200cهای ما نشان می\u200cدهند که مدل\u200cهای بنیاد ترانکیت بر اساس دو ابزار دیگر، حتی هنگامی که در مقدار کوچک داده آموزش داده می\u200cشود، بیشتر از آن را انجام داد. کد ما در < https://github.com/NaiveNeuron/slavner-2021 .', 'hr': 'U ovom papiru opisujemo TraSpaS, podatak trećem zajedničkom zadatku o priznanju imena entiteta domaćin kao dio Balto-Slavic Natural Language Processing workshop (BSNLP). U njemu procjenjujemo različite predobučene jezičke modele na zadatku NER-a koristeći tri otvorenog izvora NLP alata: model jezika na razini karaktera sa Stanzom, modeli za jezik-specifični BERT-stil s SpaCyom i adapterom omogućeni XLM-R s Trankitom. Naši rezultati pokazuju da su modeli bazirani na Trankitu nadmašili one na temelju ostalih dva alata, čak i kada su obučeni na manje količine podataka. Naš kod je dostupan na < https://github.com/NaiveNeuron/slavner-2021 >', 'tr': "Bu kagyzda TraSpaS'i ýazylýarys, Balto-Slawiýa Doňky Dil Prozesasynda (BSNLP) işleýän bir zat üçünji beýleki zada gönderilýär Bu ýerde NER täblisinde üç açyk-çeşme NLP esbap çykyşlarynyň ullanyşynda görkezilen dil nusgalaryny: Stanza bilen karakterleriň derejesi nusgalary, dilleriň beýleki BERT-stili nusgalary Spasy we Trenkit bilen eliň üstine gelen XLM-R nusgalaryny ulanýarys. Biziň netijelerimiz çykyş tabanly nusgalaryň beýleki iki çykyş barada daýanýan nusgalaryň üstüne çykandygyny görkez. Biziň ködimiz bar <at> https://github.com/NaiveNeuron/slavner-2021 >", 'sq': 'Në këtë letër ne përshkruajmë TraSpaS, një paraqitje në detyrën e tretë të përbashkët mbi njohjen e emëruar të njësisë pritur si pjesë e Workshop për Procesimin e Gjuhave Natyrore Balto-Sllavike (BSNLP). Në të vlerësojmë modele të ndryshme gjuhësh të stërvitura para-të në detyrën NER duke përdorur tre paketa mjetesh NLP me burim të hapur: model gjuhësh të nivelit të karakterit me Stanza, modele të stilit BERT specifik gjuhësh me SpaCy dhe XLM-R me Trankit të aktivizuar nga Adapter. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data.  Kodi ynë është në dispozicion në < https://github.com/NaiveNeuron/slavner-2021 >.', 'af': "In hierdie papier beskryf ons TraSpas, 'n onderwerp na die derde gedeelde taak op genaamde entiteit herken bediener as deel van die Balto- Slavic Natuurlike Taal Prosessering (BSNLP) Werkshop. In dit is ons evalueer verskillende voor-opgelei taal modele op die NER taak met drie open-bron NLP nutsbalke: karaktervlak taal model met Stanza, taal-spesifieke BERT-styl modele met SpaCy en Adapter-geaktiveer XLM-R met Trankit. Ons resultate wys dat die Trankit-gebaseerde modele uitgevoer het wat op die ander twee nutsbalke gebaseer het, selfs wanneer op kleiner hoeveelheid data opgelei is. Ons kode is beskikbaar by < https://github.com/NaiveNeuron/slavner-2021 >", 'ko': '본고에서 우리는 TraSpaS를 묘사했는데 이것은 발토 슬라브 자연언어처리(BSNLP) 세미나의 일부로서 주최하는 세 번째 명칭 실체 식별 공유 임무의 제출이다.이 책에서 우리는 세 개의 소스 NLP 도구 패키지를 사용하여 NER 작업에서 미리 훈련된 언어 모델을 평가했다. 그것이 바로 바이트가 있는 문자급 언어 모델, 스페이스가 있는 특정한 언어가 있는 버트 스타일 모델, Trankit가 있는 지원 어댑터가 있는 XLM-R이다.우리의 결과에 따르면 Trankit을 바탕으로 하는 모델은 다른 두 개의 도구 패키지를 바탕으로 하는 모델보다 비교적 적은 데이터량의 상황에서도 낫다.우리의 코드는<https://github.com/NaiveNeuron/slavner-2021>.', 'am': 'በዚህ ገጾች ላይ ቴርስፓንን እናሳውቃለን፡፡ በዚህ ውስጥ ሦስት ክፈት የNLP መሣሪያዎች በተጠቃሚ የቋንቋ ሞዴላዎችን በNER አድራሻ ላይ እናስተምር: የቃላት ደረጃ ቋንቋ ሞዴል ከስታንዛ ጋር, የቋንቋ-የተለየ BERT-ዓይነት በስፓስኮ እና አዳapter-enabled XLM-R ጋር Trankit. ፍሬዎቻችን የቴርንኪት መሠረት ምሳሌዎች ሌሎችን ሁለት የቱልባር መሣሪያዎች በመሠረት ያሳዩታል፡፡ ኮዶችን በ < https://github.com/NaiveNeuron/slavner-2021 >', 'sw': 'Katika karatasi hii tunaielezea TraSpaS, ujumbe wa jukumu la tatu lililosambazwa kwa jina la kutambua entity ulioandaliwa kama sehemu ya Warsha ya Utarabu wa Lugha ya Balto-Slavic (BSNLP). Katika hiyo tunatathmini mifano mbalimbali ya lugha zilizofunzwa kabla katika kazi ya NER kwa kutumia vifaa vitatu vya wazi vya NLP: modeli ya lugha ya wahusika na Stanza, mifano maalum ya lugha ya BERT yenye SpaCy na Adapter-R yenye uwezekano wa XLM-R na Trankit. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data.  Utawala wetu unapatikana kwenye < https://github.com/NaiveNeuron/slavner-2021 >.', 'bs': 'U ovom papiru opisujemo TraSpaS, podatak trećem zajedničkom zadatku o priznanju entiteta domaćina kao dio Balto-Slavic Natural Language Processing workshop (BSNLP). U njemu procjenjujemo različite predobučene jezičke modele na NER zadatku koristeći tri otvorena izvora NLP alata: model jezika na nivou karaktera sa Stanzom, modeli za jezik specifični BERT stil sa SpaCy i adapterom omogućeni XLM-R sa Trankitom. Naši rezultati pokazuju da su modeli na trankitu iznosili one na temelju ostalih dva alata, čak i kada su obučeni na manjim količinama podataka. Naš kod je dostupan na < https://github.com/NaiveNeuron/slavner-2021 - Da.', 'az': "Bu kağızda TraSpaS'i, Balto-Slavic Təbiq Dil İşləməsi (BSNLP) İş Hopunun bir parças ı olaraq qonaqlanmış ünvanların adı ilə üçüncü paylaşılmış işinə göndərdik. İçində NER işində üç açıq-kaynak NLP araç çubuqlarını kullanarak müxtəlif öyrənmiş dil modellərini təmin edirik: Stanza ilə karakter səviyyəsi dili modeli, SpaCy və Adapter-enabled XLM-R ilə birlikdə BERT-stili modellərini təmin edirik. Sonuçlarımız göstərir ki, Trankit tabanlı modellərin digər iki araç çubuğuna dayanan modellərin daha böyüklüyünü göstərmişdir, hətta daha kiçik məlumatlar haqqında təhsil edilmişdir. Kodumuz < https://github.com/NaiveNeuron/slavner-2021 >", 'hy': 'Այս թղթի մեջ մենք նկարագրում ենք "Տրասպաս"-ը, երրորդ ընդհանուր հանձնարարության ներկայացումը անվանելի անհատականության ճանաչելու մասին, որը կազմակերպել է Բալտո-Սլավիական բնական լեզվի մշակույթի (ԲՍԼՊ) մասը: Դրա մեջ մենք գնահատում ենք տարբեր նախապատրաստված լեզվի մոդելներ, որոնք վերաբերվում են ՆԵՌ խնդրի վրա, օգտագործելով երեք բաց կոդը բաց կոդը ՆԵՌ գործիքների շարք. Ստենզայի լեզվի մոդելը բնորոշ մակարդակի մակարդակի հետ, լեզվի մասնավոր BER-ոճ Մեր արդյունքները ցույց են տալիս, որ Թրանկտին հիմնված մոդելները գերազանցեցին երկու այլ գործիքների շարքերի վրա հիմնված մոդելները, նույնիսկ այն ժամանակ, երբ պատրաստվում էին ավելի փոքր տվյալների վրա: Our code is available at < https://github.com/NaiveNeuron/slavner-2021 ]:', 'bn': 'এই কাগজটিতে আমরা ট্রাস্প্যাসের বর্ণনা করছি, যা বাল্টো-স্লাভিক ন্যাশনাল ভাষার প্রক্রিয়া (বিএসএনএলপি) ওয়ার্কশার্কের অংশ হিসেবে নামে একটি ত স্প্যানিক এবং এডাপ্টার-সক্রিয় XLM-R দ্বারা ক্যারেক্টান্জার স্ট্যান্জার সাথে তিন উন্মুক্ত সোর্স এনএলপি টুলিকিট ব্যবহার করে নির্দিষ্ট ভাষার মডেল বিভিন্ন ভা আমাদের ফলাফল দেখা যাচ্ছে যে ট্রাঙ্কিট ভিত্তিক মডেল অন্য দুই টুলবিকেটের ভিত্তিক ভিত্তিক করেছে, এমনকি যখন ছোট তথ্যের পরিমাণ আমাদের কোড পাওয়া যাচ্ছে < https://github.com/NaiveNeuron/slavner-2021 >', 'ca': "En aquest article descrivim TraSpaS, una presentació a la tercera tasca compartida sobre el reconeixement d'entitats anomenades, aconseguida com part de l'atelier de processament de llenguatges naturals (BSNLP). In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits: character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit.  Els nostres resultats demostren que els models basats en Trankit superaven els basats en les altres dues eines, fins i tot quan es van entrenar en quantitats més petites de dades. El nostre codi està disponible a < https://github.com/NaiveNeuron/slavner-2021 >.", 'cs': 'V tomto článku popisujeme TraSpaS, předložení třetího sdíleného úkolu o rozpoznávání jmenovaných entit hostovaného v rámci Balto-Slavic Natural Language Processing (BSNLP) Workshop. V něm hodnotíme různé předškolené jazykové modely na úlohu NER pomocí tří open-source NLP toolkits: jazykový model na úrovni znaků se Stanzou, jazykově specifické BERT modely se SpaCy a adaptérový XLM-R s Trankit. Naše výsledky ukazují, že modely založené na Trankitu předčily ty založené na dalších dvou sadách nástrojů, i když jsou trénovány na menším množství dat. Náš kód je k dispozici na < https://github.com/NaiveNeuron/slavner-2021 >.', 'et': 'Käesolevas töös kirjeldame TraSpaS-i, kolmanda jagatud ülesande täitmist nimetatud üksuste tunnustamisel, mida korraldatakse Balto-Slaavi looduskeele töötlemise (BSNLP) töötluse raames. Selles hindame erinevaid eelõpetatud keelemudeleid NER ülesandel, kasutades kolme avatud lähtekoodiga NLP tööriistakomplekti: märgitaseme keelemudel Stanza, keelespetsiifilistel BERT-stiilis mudelitel SpaCy ja Adapter-toega XLM-R Trankitiga. Meie tulemused näitavad, et Trankitil põhinevad mudelid ületasid ülejäänud kahel tööriistakomplektil põhinevaid mudeleid, isegi kui neid on koolitatud väiksemate andmekogustega. Meie kood on kättesaadav aadressil < https://github.com/NaiveNeuron/slavner-2021 >.', 'fi': 'Tässä artikkelissa kuvailemme TraSpaS:ää, joka on kolmas jaettu tehtävä nimettyjen entiteettien tunnistamisesta osana Baltoslavian Natural Language Processing (BSNLP) -työpajaa. Arvioimme NER-tehtävään liittyviä esikoulutettuja kielimalleja kolmella avoimen lähdekoodin NLP-työkalupakilla: merkkitason kielimalli Stanzalla, kielikohtaiset BERT-tyyliset mallit SpaCyllä ja Adapter-yhteensopiva XLM-R Trankitillä. Tuloksemme osoittavat, että Trankit-pohjaiset mallit suoriutuivat kahdesta muusta työkalupakista paremmin, vaikka niitä olisi koulutettu pienemmillä tietomäärillä. Koodimme on saatavilla osoitteessa < https://github.com/NaiveNeuron/slavner-2021 >.', 'ha': 'In this paper we describe TraSpaS, a submission to the third shared task on named entity recognition hosted as part of the Balto-Slavic Natural Language Processing (BSNLP) Workshop.  Daga shi, Munã ƙaddara misãlai masu yin zaman-haƙin harshen zaman tsari a kan aikin NER da ke amfani da shiryoyin kayan aiki uku masu buɗe-source NLP: misãlin harshen daraja da Stanza, misãlai-masu ƙayyade BERT-style na spacy da Adafter-an-fara XLM-R da Tranket. MatamayinMu na nũna cewa, misalin a kan Trankt na samar da su a kan zanen aiki biyu, ko kuma idan an yi tattalin ko ƙarami masu ƙaranci na data. Cikakken mu yana da a < https://github.com/NaiveNeuron/slavner-2021 >', 'sk': 'V prispevku opisujemo TraSpaS, predložitev tretje skupne naloge o prepoznavanju imenovanih entitet, ki je gostila v okviru balto-slovanske delavnice obdelave naravnega jezika (BSNLP). V njem smo ocenili različne vnaprej usposobljene jezikovne modele za nalogo NER z uporabo treh odprtokodnih orodij NLP: jezikovni model na ravni znakov s Stanzo, jezikovno specifični modeli BERT s SpaCy in Adapter-omogočen XLM-R s Trankitom. Naši rezultati kažejo, da so modeli, ki temeljijo na Trankitu, presegli tiste, ki temeljijo na drugih dveh orodjih, tudi če so usposobljeni na manjših količinah podatkov. Naša koda je na voljo na < https://github.com/NaiveNeuron/slavner-2021 >.', 'he': 'בעיתון הזה אנחנו מתארים את טראספאס, ההעברה למשימה השלישית המשותפת על זיהוי ישויות בשם מארח כחלק מהעבדה של תהליך השפה הטבעית בלטו-סלאבי (BSNLP). בתוכו אנו מעריכים מודלים שונים של שפה מאומנים מראש על המשימה של NER בשימוש שלושה כלי NLP מקור פתוח: מודל שפת רמת האופים עם סטנזה, מודלים בסגנון BERT ספציפי לשפה עם SpaCy ואפשר המתאים XLM-R עם Trankit. התוצאות שלנו מראות שהדוגמנים מבוססים בטרנקיט עברו את אלה מבוססים על שני כלים האחרים, אפילו כשהתאמנו על כמויות קטנות יותר של נתונים. הקוד שלנו זמין ב < https://github.com/NaiveNeuron/slavner-2021 >.', 'jv': 'Nan pepulan iki kita nambah TraSpaS,miturunggu telu operasi wis gawe ngupakan anyari tentang karo perusahaan Balto-Slave Nanging kuwi, kita deweke nggawe model sing sampeyan luwih bantuan karo NeR task nggo gambar telu open-source NLP Tulkit: model caratar languasi nggo Stazza, languai-pernik model sing nggo BERT-style nggo SpaCi lan Adver-enabled XLM-R nggo Trangkat. Rejalaké awak dhéwé ngerasakno ngono model sing basa Tran sampek dudu supoyo barang iki alat, lho ngono terus-terus kuwi alat sing bisa barang langgar sampek dadi. Coverage https://github.com/NaiveNeuron/slavner-2021 >', 'bo': 'ང་ཚོས་ཤོག་བུ་འདིའི་ནང་དུ་TraSpaS་ལ་གསལ་བཤད་པ་ཞིག་འཇུག་གི་བྱ་རིམ་གཞན་གྱི་ནང་དུ་ཕན་ཐོགས་གཙོ་རིམ་དང་མིང་ཁང་པོ་ཞིག་གི་རྒྱལ་ཁབ་Balto- In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits: character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit. ང་ཚོའི་མཐོང་སྣང་ཚུལ་མང་པོ་ཞིག་གིས་སྐྱེས་པའི་མིག་དཔེ་གཟུགས་རིས་ལག་ཆས་གཞན་གཉིས་ཀྱི་རྩིས་གཞི་བྱས་པ་ནི་ ང་ཚོའི་ཨང་རྟགས་འདི་< https://github.com/NaiveNeuron/slavner-2021 >'}
{'en': 'Named Entity Recognition and Linking Augmented with Large-Scale Structured Data', 'es': 'Reconocimiento de entidades nombradas y vinculación aumentada con datos estructurados a gran escala', 'fr': "Reconnaissance et liaison d'entités nommées augmentées avec des données structurées à grande échelle", 'ar': 'التعرف على الكيان المحدد وربطه مع البيانات المنظمة واسعة النطاق', 'pt': 'Reconhecimento de Entidade Nomeada e Vinculação Aumentada com Dados Estruturados em Grande Escala', 'ja': '大規模構造化データで拡張された名前付きエンティティ認識とリンク', 'hi': 'नामित एंटिटी पहचान और बड़े पैमाने पर संरचित डेटा के साथ संवर्धित लिंक करना', 'ru': 'Распознавание и связывание именованных сущностей, дополненных крупномасштабными структурированными данными', 'zh': '以大结构化数增名实识链接', 'ga': 'Aithint Aonáin Ainmnithe agus Nascáil Méadaithe le Sonraí Struchtúrtha ar Mhórscála', 'ka': 'სახელი ინტერტის განაცნობა და დაკავშირება აგგენტირებული დიდი- სკენ სტრუქტურაციული მონაცემებით', 'el': 'Αναγνώριση Οντότητας και Σύνδεση Ενοποιημένη με Δομημένα Δεδομένα μεγάλης κλίμακας', 'hu': 'Megnevezett szervezetek felismerése és összekapcsolása nagyméretű strukturált adatokkal', 'it': 'Riconoscimento delle entità nominate e collegamento potenziato con dati strutturati su larga scala', 'lt': 'Pavadintas subjektas pripažįstamas ir susietas su didelio masto struktūriniais duomenimis', 'mk': 'Признавање и поврзување на именувани ентитети зголемено со структурни податоци на голем степен', 'kk': 'Аталған нысандарды анықтау мен сілтемелеу үлкен- масштабтау құрылған деректермен көбейтілген', 'ml': 'പേരിട്ട എന്റിറ്റിറ്റി തിരിച്ചറിയുന്നതും ലിങ്ങിങ് അഗ്നിങ്ങ് ആഗ്രഹിക്കുന്നതും വലിയ വിവരങ്ങളുമായ', 'ms': 'Pengenalan dan Pautan Entiti bernama Ditambah dengan Data Struktur Skala Besar', 'mt': 'Named Entity Recognition and Linking Augmented with Large-Scale Structured Data', 'mn': 'Нэрэг Entity Recognition and Linking Augmented with Large-Scale Structured Data', 'no': 'Namn på gjenkjenning og lenking av einhetar augmentert med stor skala strukturerte data', 'pl': 'Rozpoznawanie nazwanych podmiotów i powiązanie rozszerzone o dużą skalę danych strukturalnych', 'ro': 'Recunoașterea entităților denumite și legătura augmentată cu datele structurate la scară largă', 'sr': 'Prepoznavanje i povezivanje imenovanih podataka povećane sa velikom skalom strukturovanim podacima', 'so': 'Magaca Aqoonsiga iyo Linking Augmented with Large-Scale Structured Data', 'si': 'Name', 'sv': 'Namngivn enhetsigenkänning och sammankoppling förstärkt med storskalig strukturerad data', 'ta': 'பெயரிடப்பட்ட பெரிய அளவு உருவாக்கப்பட்ட தரவுடன் தொடர்பு மற்றும் இணைப்பு உருவாக்கப்பட்டது', 'ur': 'بڑے-اسکیل ساخترڈ ڈاٹا کے ساتھ نام دار انٹیٹی پتچار اور لینک', 'vi': 'Nhận dạng và liên kết với dữ liệu cấu trúc diện diện lớn', 'uz': 'Xabarlar roĘ»yxati', 'hr': 'Prepoznavanje i povezivanje imenovanih podataka povećane s velikom skalom strukturovanim podacima', 'da': 'Anerkendelse af navngivne enheder og sammenkædning forbedret med strukturerede data i stor skala', 'de': 'Named Entity Erkennung und Verknüpfung mit strukturierten Daten im großen Maßstab', 'id': 'Pengenalan dan Penghubungan Entitas bernama Ditambah dengan Data Struktur Skala Besar', 'ko': '대규모 구조화된 데이터 기반의 명명 실체 식별과 링크', 'fa': 'شناسایی و ارتباط واحد نامیده شده با داده\u200cهای ساخته\u200cشده\u200cی مقیاس بزرگ', 'tr': 'Ullakan Resim Namaýyş we Baglaýyş', 'af': 'Genaamde Eenheidherkening en Linking Gegroot met Groot- Skaal Struktuurde Data', 'bg': 'Разпознаване и свързване на наименовани субекти с разширени структурирани данни', 'nl': 'Erkenning van naamloze entiteiten en koppeling uitgebreid met grootschalige gestructureerde gegevens', 'sq': 'Njohja dhe lidhja e njësisë së quajtur rritet me të dhëna të strukturuara në shkallë të madhe', 'sw': 'Tambulisho la Ujumbe na Kuunganisha', 'am': 'ምርጫዎች', 'hy': 'Հանվանված միավորների ճանաչելը և կապը աճեցված է մեծ մասի կառուցվածքային տվյալներով', 'ca': "Recognició i enllaç d'entitats anomenades augmentats amb dades estructuradas a gran escala", 'bn': 'Name', 'bs': 'Prepoznavanje i povezivanje imenovanih podataka povećane sa velikom skalom strukturovanim podacima', 'az': 'Ən böyük ölçülü Yapılmış Verilə Adlı Entity Recognition və Linking', 'cs': 'Rozpoznávání jmenovaných entit a propojení rozsáhlých strukturovaných dat', 'et': 'Nimetatud üksuse tunnustamine ja seostamine laiaulatuslike struktureeritud andmetega', 'fi': 'Nimettyjen yksiköiden tunnistaminen ja linkittäminen laajamittaiseen strukturoituun dataan', 'ha': 'Media controller element', 'sk': 'Prepoznavanje imenovanega subjekta in povezovanje z obsežnimi strukturiranimi podatki', 'he': 'זיהוי יחידות בשם וקשר מוגדל עם נתונים מבוססים במערכת גדולה', 'bo': 'Named Entity Recognition and Linking Augmented with Large-Scale Structured Data', 'jv': 'Named Entty Learning and Linking AGmented with big-scale structural data'}
{'en': 'In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the analysis of Named Entities in multilingual Web documents in Slavic languages with rich inflection. Our solution takes advantage of large collections of both unstructured and structured documents. The former serve as data for unsupervised training of language models and embeddings of lexical units. The latter refers to Wikipedia and its structured counterpart-Wikidata, our source of lemmatization rules, and real-world entities. With the aid of those resources, our system could recognize, normalize and link entities, while being trained with only small amounts of labeled data.', 'es': 'En este artículo describimos nuestras presentaciones a la segunda y tercera tareas compartidas de SLAVner realizadas en BSNLP 2019 y BSNLP 2021, respectivamente. Las tareas se centraron en el análisis de entidades nombradas en documentos web multilingües en idiomas eslavos con una gran inflexión. Nuestra solución aprovecha las grandes colecciones de documentos estructurados y no estructurados. Los primeros sirven como datos para el entrenamiento no supervisado de modelos lingüísticos e incrustaciones de unidades léxicas. Este último se refiere a Wikipedia y su contraparte estructurada, Wikidata, nuestra fuente de reglas de lematización y entidades del mundo real. Con la ayuda de esos recursos, nuestro sistema podría reconocer, normalizar y vincular entidades, mientras se capacitaba con solo pequeñas cantidades de datos etiquetados.', 'pt': 'Neste artigo descrevemos nossas submissões para as 2ª e 3ª Tarefas Compartilhadas SlavNER realizadas no BSNLP 2019 e BSNLP 2021, respectivamente. As tarefas se concentraram na análise de Entidades Nomeadas em documentos Web multilíngues em idiomas eslavos com rica inflexão. Nossa solução tira proveito de grandes coleções de documentos estruturados e não estruturados. Os primeiros servem como dados para treinamento não supervisionado de modelos de linguagem e incorporações de unidades lexicais. O último refere-se à Wikipedia e sua contraparte estruturada - Wikidata, nossa fonte de regras de lematização e entidades do mundo real. Com a ajuda desses recursos, nosso sistema pode reconhecer, normalizar e vincular entidades, sendo treinado apenas com pequenas quantidades de dados rotulados.', 'ar': 'في هذه الورقة ، نصف عمليات إرسالنا إلى مهام SlavNER المشتركة الثانية والثالثة التي عقدت في BSNLP 2019 و BSNLP 2021 ، على التوالي. ركزت المهام على تحليل الكيانات المسماة في مستندات الويب متعددة اللغات باللغات السلافية مع انعكاسات غنية. يستفيد حلنا من المجموعات الكبيرة من المستندات غير المهيكلة وغير المهيكلة. الأول بمثابة بيانات للتدريب غير الخاضع للرقابة للنماذج اللغوية وحفلات الزفاف للوحدات المعجمية. يشير الأخير إلى ويكيبيديا ونظيرتها المنظمة - ويكي بيانات ، مصدر قواعد اللماتة لدينا ، وكيانات العالم الحقيقي. بمساعدة هذه الموارد ، يمكن لنظامنا التعرف على الكيانات وتطبيعها وربطها ، بينما يتم تدريبه باستخدام كميات صغيرة فقط من البيانات المصنفة.', 'ja': '本稿では、BSNLP 2019およびBSNLP 2021で開催された第2回および第3回SlavNER共有タスクへの提出について説明します。タスクは、多言語ウェブドキュメントの名前付きエンティティの分析に焦点を当て、豊富なインフレーションを持つスラブ語で行われました。当社のソリューションは、構造化されていないドキュメントと構造化されたドキュメントの両方の大規模なコレクションを利用しています。前者は、言語モデルの監督されていないトレーニングや語彙単位の埋め込みのためのデータとなる。後者は、ウィキペディアとその構造化された相手方であるウィキデータ、私たちのレマティゼーションルールのソース、そして現実世界の実体を指します。これらのリソースの助けを借りて、わずかなラベル付きデータのみで訓練されながら、当社のシステムはエンティティを認識し、正規化し、リンクすることができます。', 'fr': "Dans cet article, nous décrivons nos soumissions aux 2e et 3e tâches partagées Slavner tenues respectivement au BSNLP 2019 et au BSNLP 2021. Les tâches se sont concentrées sur l'analyse des entités nommées dans des documents Web multilingues en langues slaves avec une forte inflexion. Notre solution tire parti de vastes collections de documents structurés et non structurés. Les premiers servent de données pour la formation non supervisée de modèles linguistiques et l'intégration d'unités lexicales. Ce dernier fait référence à Wikipédia et à son homologue structuré - Wikidata, notre source de règles de lemmatisation, et des entités du monde réel. À l'aide de ces ressources, notre système pourrait reconnaître, normaliser et relier des entités, tout en étant formé avec de petites quantités de données étiquetées.", 'zh': '本文,我们分别在BSNLP 2019和BSNLP 2021上向第二和第三届SlavNER共享提交的材料。 侧重于析多变词形斯拉夫语多言 Web 文档名实体。 吾解决方案因非结构化与结构化文档大合。 前者以语言模形无监督训练及词法单元嵌之数。 后者,维基百科及其结构化之应物 - 维基数,吾词形还其本末,及世界之实体也。 借此资源,吾统可以识规范化链接实体,兼以少量标数训练之。', 'ru': 'В этой статье мы описываем наши представления на 2-й и 3-й совместные задачи SlavNER, проведенные на BSNLP 2019 и BSNLP 2021 соответственно. Задачи были сосредоточены на анализе именованных субъектов в многоязычных веб-документах на славянских языках с богатым перегибом. Наше решение использует большие коллекции как неструктурированных, так и структурированных документов. Первые служат в качестве данных для неконтролируемого обучения языковым моделям и вложениям лексических единиц. Последнее относится к Википедии и ее структурированному аналогу - Викидата, нашему источнику правил лемматизации и реальным сущностям. С помощью этих ресурсов наша система может распознавать, нормализовывать и связывать сущности, одновременно обучаясь лишь небольшому объему маркированных данных.', 'hi': 'इस पेपर में हम क्रमशः BSNLP 2019 और BSNLP 2021 में आयोजित दूसरे और तीसरे स्लावनर साझा कार्यों के लिए हमारी प्रस्तुतियों का वर्णन करते हैं। कार्य समृद्ध मोड़ के साथ स्लाव भाषाओं में बहुभाषी वेब दस्तावेजों में नामित संस्थाओं के विश्लेषण पर केंद्रित थे। हमारा समाधान असंरचित और संरचित दोनों दस्तावेजों के बड़े संग्रह का लाभ उठाता है। पूर्व भाषा मॉडल और लेक्सिकल इकाइयों के एम्बेडिंग के असुरक्षित प्रशिक्षण के लिए डेटा के रूप में कार्य करता है। उत्तरार्द्ध विकिपीडिया और इसके संरचित समकक्ष को संदर्भित करता है - विकिडेटा, लेमैटाइजेशन नियमों का हमारा स्रोत, और वास्तविक दुनिया की संस्थाएं। उन संसाधनों की सहायता से, हमारा सिस्टम केवल लेबल किए गए डेटा की केवल थोड़ी मात्रा के साथ प्रशिक्षित होने के दौरान संस्थाओं को पहचान, सामान्य और लिंक कर सकता है।', 'ga': 'Sa pháipéar seo déanaimid cur síos ar ár n-aighneachtaí chuig an 2ú agus an 3ú Tasc Comhroinnte SlavNER a tionóladh ag BSNLP 2019 agus BSNLP 2021, faoi seach. Dhírigh na tascanna ar anailís a dhéanamh ar Aonáin Ainmnithe i gcáipéisí ilteangacha Gréasáin i dteangacha Slavacha le hinfhillteacht shaibhir. Baineann ár réiteach leas as bailiúcháin mhóra de dhoiciméid neamhstruchtúrtha agus struchtúrtha. Feidhmíonn na chéad cheann díobh mar shonraí d’oiliúint gan mhaoirseacht ar shamhlacha teanga agus le leabú aonad foclóireachta. Tagraíonn an dara ceann do Vicipéid agus a mhacasamhail struchtúrtha - Wikidata, ár bhfoinse rialacha lemméadaithe, agus eintitis fhíorshaoil. Le cúnamh ó na hacmhainní sin, d’fhéadfadh ár gcóras eintitis a aithint, a normalú agus a nascadh, agus gan ach líon beag sonraí lipéadaithe a bheith oilte orthu.', 'el': 'Στην παρούσα εργασία περιγράφουμε τις υποβολές μας στις 2ες και 3ες Κοινές Εργασίες που διεξάγονται στο πλαίσιο των BSNLP 2019 και BSNLP 2021 αντίστοιχα. Οι εργασίες επικεντρώθηκαν στην ανάλυση ονομάτων οντοτήτων σε πολύγλωσσα έγγραφα σε σλαβικές γλώσσες με πλούσια κλίση. Η λύση μας επωφελείται από μεγάλες συλλογές τόσο άτακτων όσο και δομημένων εγγράφων. Τα πρώτα χρησιμεύουν ως δεδομένα για την εκπαίδευση χωρίς επίβλεψη γλωσσικών μοντέλων και ενσωμάτωσης λεξικών μονάδων. Το τελευταίο αναφέρεται στη Βικιπαίδεια και το δομημένο αντίστοιχο της με τα Βικιντεδομένα, την πηγή των κανόνων λεμματοποίησης μας, και τις οντότητες του πραγματικού κόσμου. Με τη βοήθεια αυτών των πόρων, το σύστημά μας θα μπορούσε να αναγνωρίσει, να ομαλοποιήσει και να συνδέσει οντότητες, ενώ παράλληλα εκπαιδεύεται με μικρές μόνο ποσότητες επισημασμένων δεδομένων.', 'hu': 'Ebben a tanulmányban ismertetjük a BSNLP 2019-es és BSNLP 2021-es második és harmadik SlavNER megosztott feladatokra tett beadványainkat. A feladatok középpontjában a szláv nyelvű, többnyelvű webes dokumentumokban található Nevezett entitások elemzése volt. Megoldásunk kihasználja a strukturálatlan és strukturált dokumentumok nagy gyűjteményét. Az előbbiek adatként szolgálnak a nyelvi modellek felügyelet nélküli képzéséhez és a lexikai egységek beágyazásához. Ez utóbbi a Wikipédiára és annak strukturált megfelelőjére utal, a Wikidata-ra, a lemmatizációs szabályok forrására és a valós világ entitásaira. Ezeknek az erőforrásoknak a segítségével rendszerünk felismerheti, normalizálhatja és összekapcsolhatja az entitásokat, miközben csak kis mennyiségű címkézett adatokkal képzett.', 'ka': 'ამ დოკუნეში ჩვენ ჩვენი წარმოდგენების შეტყობინება 2 და 3 SlavNER დაყობინებული დავალებებით BSNLP 2019 და BSNLP 2021-ში. მისამართები მრავალური ვებ დოკუმენტების ანალიზაციისთვის კონუქტურებულია, რომელიც ბედნიერი ინფლექციაში. ჩვენი პასუხი გამოიყენება დიდი კოლექციების, რომელიც არ შექმნა და სტრუქტურული დოკუმენტების. პირველი იმუშაობენ როგორც მონაცემები ენის მოდელების და ლექსიკალური ერთეულების შემწყვებისთვის განაკეთებას. შემდეგ ვიკიპედია და მისი სტრუქტურაციული კონტრუქტურაცია - ვიკიდეტატი, ჩვენი ლემეტიზაციის წესების გამოსახულება და რეალური მსოფლიოს ინტერტიკების ამ რესურსების დახმარებით, ჩვენი სისტემა შეიძლება აღმოჩნოთ, ნორმალიზოთ და დაკავშიროთ ინტერტიები, როცა მხოლოდ მალკი რაოდენობით მარტივი მონაცემებ', 'lt': 'Šiame dokumente apibūdiname savo pastabas 2-ajam ir 3-ajam SlavNER bendroms užduotims, įvykdytoms atitinkamai 2019 m. BSNLP ir 2021 m. BSNLP. Įgyvendinant užduotis daugiakalbių interneto dokumentų slavų kalbomis, turinčių daug įtakos, pavarduotų subjektų analizė. Mūsų sprendimas naudojasi dideliais nesustruktūruotų ir struktūruotų dokumentų rinkiniais. Pirmieji duomenys naudojami kaip nenorėtam kalbų modelių mokymui ir tekstinių vienetų įterpimui. Pastaruoju atveju kalbama apie Wikipedia ir jos struktūrinę partnerę - Wikidata, mūsų lemmatizacijos taisyklių šaltinį ir realiojo pasaulio subjektus. Pagal šiuos išteklius mūsų sistema galėtų pripažinti, normalizuoti ir susieti subjektus, tuo pat metu mokydama tik nedideliu kiekiu pažymėtų duomenų.', 'it': "In questo articolo descriviamo i nostri contributi al 2° e 3° SlavNER Shared Task tenutosi rispettivamente a BSNLP 2019 e BSNLP 2021. I compiti si sono concentrati sull'analisi di Entità Nomate in documenti Web multilingue in lingue slave con ricca inflessione. La nostra soluzione sfrutta ampie collezioni di documenti non strutturati e strutturati. I primi servono da dati per la formazione non supervisionata di modelli linguistici e incorporazioni di unità lessicali. Quest'ultimo si riferisce a Wikipedia e alla sua controparte strutturata - Wikidata, la nostra fonte di regole di lemmatizzazione, e entità del mondo reale. Con l'aiuto di queste risorse, il nostro sistema potrebbe riconoscere, normalizzare e collegare le entità, pur essendo addestrato con solo piccole quantità di dati etichettati.", 'kk': 'Бұл қағазда біз біздің 2- мен 3- ші SlavNER ортақтастырылған тапсырмаларды BSNLP 2019 және BSNLP 2021 жылы ортақтастыратын тапсырмаларды таңдаймыз. Тапсырмалар бірнеше тілді Веб құжаттарындағы аталған нысандардың анализациясына көп тілдері бос инфлекциясы бар. Біздің шешіміміз құрылмаған және құрылмаған құжаттардың үлкен жинақтарын ашықтайды. Бұрынғы тіл үлгілерін және лексикалық бірліктерін ендіру үшін деректер ретінде жұмыс істейді. Соңғылар Википедия және оның құрылған партнері - Викидеректері, біздің лимматизациялау ережелеріміздің көзі және шын әлемдегі бөліктеріміз. Бұл ресурстардың көмегімен, жүйеміз жарлық деректерді тек кішкентай деңгейінде оқылған болғанда, нысандарды анықтай, нормализациялау және сілтемелеу мүмкін болады.', 'ml': 'ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d രണ്ടാമത്തേതും മൂന്നാമത്തേതും സ്ലാവ്നെര്\u200d പങ്കുചേര്\u200dത്ത ജോലികള്\u200dക്ക് കീഴ്പെടുത്തിയിരിക്കുന്നു. The tasks focused on the analysis of Named Entities in multilingual Web documents in Slavic languages with rich inflection.  നമ്മുടെ തീരുമാനം നിര്\u200dമ്മിക്കപ്പെട്ട രണ്ടു രേഖകളുടെയും വലിയ കൂട്ടത്തില്\u200d ഉപയോഗിക്കുന്നു. സൂക്ഷിക്കാത്ത ഭാഷ മോഡലുകളുടെ പരിശീലനത്തിനും ലെക്സിക്കല്\u200d യൂണിറ്റുകളുടെ പൂര്\u200dവ്വികന്\u200d ഡേറ്റായി സേവിക അവസാനം വിക്കിപിഡിയയെക്കുറിച്ചും അതിന്റെ അടിസ്ഥാനമായ പിന്തുണയെക്കുറിച്ചും വിക്കിതാതയെക്കുറിച്ചും നമ്മുടെ ലി ആ വിഭവങ്ങളുടെ സഹായം കൊണ്ട്, നമ്മുടെ സിസ്റ്റത്തിന് സാധാരണ, സ്വാഭാവീകരിക്കുകയും ബന്ധിപ്പിക്കുകയും ചെയ്യാന്\u200d കഴിയും, ചെ', 'ms': 'In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively.  Tugas berfokus pada analisis Entiti bernama dalam dokumen web berbilang bahasa dalam bahasa Slavic dengan pengaruh yang kaya. Solusi kami mengambil keuntungan dari koleksi besar kedua-dua dokumen tidak struktur dan struktur. Pertama berkhidmat sebagai data untuk latihan tidak diawasi bagi model bahasa dan penyelesaian unit leksikal. The latter refers to Wikipedia and its structured counterpart - Wikidata, our source of lemmatization rules, and real-world entities.  Dengan bantuan sumber tersebut, sistem kita boleh mengenali, normalisasi dan paut entiti, sementara dilatih dengan jumlah kecil data yang ditabel.', 'mk': 'Во овој документ ги опишуваме нашите поднесувања на втората и третата заедничка задача на SlavNER одржана на BSNLP 2019 и BSNLP 2021, односно. Овие задачи се фокусираа на анализата на именуваните ентитети во мултијазичните веб-документи на славски јазици со богато влијание. Нашето решение ги искористува големите колекции на неструктурирани и структурирани документи. Првите служат како податоци за ненадгледувано обука на јазичките модели и вградување на лексикалните единици. The latter refers to Wikipedia and its structured counterpart - Wikidata, our source of lemmatization rules, and real-world entities.  Со помош на овие ресурси, нашиот систем би можел да ги препознае, нормализира и поврзува ентитетите, додека би бил обучен со само мали количини обележани податоци.', 'no': 'I denne papiret beskriver vi våre tillegg til den andre og tredje SlavNER delte oppgåva som er held i BSNLP 2019 og BSNLP 2021. Oppgåvene fokuserte på analysen av namne einingar i fleirspråk nettdokument i slaviske språk med ryg infleksjon. Løysinga vårt brukar fordel av store samlingar av både ikkje strukturerte og strukturerte dokument. Den tidlegare kaller som data for ikkje-oppretta opplæring av språk-modeller og innbygging av leksiske einingar. Dei siste refererer til Wikipedia og sine strukturerte mottaler – Wikidata, våre kjelde til lemmatisasjonsregular og verdens einingar. Med hjelp av desse ressursane kan systemet vårt gjenkjenne, normalisere og lenke opplysningar, mens det er trengd med berre små mengda merkelige data.', 'mt': 'F’dan id-dokument niddeskrivu s-sottomissjonijiet tagħna għat-tieni u t-tielet Kompiti Kondiviżi SlavNER li saru fil-BSNLP 2019 u l-BSNLP 2021 rispettivament. Il-kompiti ffukaw fuq l-analiżi ta’ Entitajiet Ismija f’dokumenti multilingwi tal-Internet fil-lingwi Slavi b’inflezzjoni rikka. Is-soluzzjoni tagħna tieħu vantaġġ minn kollezzjonijiet kbar ta’ dokumenti kemm mhux strutturati kif ukoll strutturati. Dan ta’ qabel iservi bħala dejta għal taħriġ mhux sorveljat ta’ mudelli lingwistiċi u inkorporazzjonijiet ta’ unitajiet lexiċi. Dan tal-aħħar jirreferi għall-Wikipedia u l-kontroparti strutturata tagħha - Wikidata, is-sors tagħna tar-regoli tal-limmatizzazzjoni, u entitajiet tad-dinja reali. Bl-għajnuna ta’ dawk ir-riżorsi, is-sistema tagħna tista’ tirrikonoxxi, tinnormalizza u tgħaqqad entitajiet, filwaqt li tkun imħarrġa b’ammonti żgħar biss ta’ dejta ttikkettata.', 'ro': 'În această lucrare descriem înscrierile noastre la cele de-a doua și a treia sarcini partajate SlavNER desfășurate la BSNLP 2019 și respectiv BSNLP 2021. Sarcinile s-au axat pe analiza entităților denumite în documente web multilingve în limbi slave cu inflexiune bogată. Soluția noastră profită de colecții mari de documente nestructurate și structurate. Primele servesc ca date pentru formarea nesupravegheată a modelelor lingvistice și încorporarea unităților lexicale. Aceasta din urmă se referă la Wikipedia și omologul său structurat - Wikidata, sursa noastră de reguli de lemmatizare, și entitățile din lumea reală. Cu ajutorul acestor resurse, sistemul nostru ar putea recunoaște, normaliza și lega entitățile, fiind instruit cu doar cantități mici de date etichetate.', 'pl': 'W niniejszym artykule opisujemy nasze zgłoszenia do II i III zadań wspólnych SlavNER odbywających się odpowiednio w BSNLP 2019 i BSNLP 2021. Zadania koncentrowały się na analizie Nazwanych Podmiotów w wielojęzycznych dokumentach internetowych w językach słowiańskich o bogatym przekształceniu. Nasze rozwiązanie wykorzystuje duże zbiory zarówno nieustrukturyzowanych, jak i ustrukturyzowanych dokumentów. Te pierwsze służą jako dane do szkolenia modeli językowych bez nadzoru i osadzania jednostek leksykalnych. Ta ostatnia odnosi się do Wikipedii i jej ustrukturyzowanego odpowiednika do Wikidanych, naszego źródła reguł lemmatyzacji oraz podmiotów rzeczywistych. Z pomocą tych zasobów nasz system mógł rozpoznawać, normalizować i łączyć podmioty, jednocześnie być szkolony z niewielkimi ilościami etykietowanych danych.', 'so': "Warqadan ayaannu ku qoraynaa hoos u dhigistayada shaqaalaha loo sharciyey 2aad iyo 3rd SlavNER ee loo qabtay BSNLP 2019 iyo BSNLP 2021. Shaqooyinku waxay ku qoran yihiin baaritaanka magaca Entities oo ku qoran dukumentiyada internetka oo luuqadaha kala duduwan oo Slavic ku qoran taariikh. Xaruntayadu wuxuu faa'iido u helaa ururo badan oo labada dukumenti aan la dhisay iyo dhisay. The former serve as data for unsupervised training of language models and embeddings of lexical units.  Qofkii dambe waxaa loola jeedaa Wikipedia iyo saaxiibkeedii la dhisay - Wikidata, noogu soo jeeday sharciyadeenna la sameeyo iyo waxyaabaha caalamiga ah. Sida uu caawiyo lacagahaas, nidaamkayagu wuxuu aqoonsan karaa, caadi ahaan iyo isku xiri karaa, marka lagu baranayo macluumaad yar oo la qoray oo keliya.", 'sv': 'I denna uppsats beskriver vi våra bidrag till den 2:a och 3:e SlavNER Shared Tasks som hölls på BSNLP 2019 respektive BSNLP 2021. Uppgifterna fokuserade på analys av namngivna enheter i flerspråkiga webbdokument på slaviska språk med rik böjning. Vår lösning utnyttjar stora samlingar av både ostrukturerade och strukturerade dokument. De förstnämnda fungerar som data för oövervakad utbildning av språkmodeller och inbäddningar av lexikala enheter. Den senare hänvisar till Wikipedia och dess strukturerade motsvarighet - Wikidata, vår källa till lemmatiseringsregler, och verkliga enheter. Med hjälp av dessa resurser kunde vårt system känna igen, normalisera och länka enheter, samtidigt som det tränas med endast små mängder märkta data.', 'ta': 'இந்த காகிதத்தில் நாம் BSNLP 2019 மற்றும் BSNLP 2021 பிரித்து இரண்டாவது மற்றும் மூன்றாவது அடிமையில் பங்கிடப்பட்ட பணிகளை விவரிக்கிறோம். பல மொழியில் இணைய ஆவணங்களில் உள்ள பெயரிடப்பட்ட உள்ளீடுகளை ஆராய்வு செய்யும் பணிகளின் மீது கவனம் செலுத்தப்படுகிறது. Our solution takes advantage of large collections of both unstructured and structured documents.  @ info பின்வரும் விக்கிபிடியா மற்றும் அதன் அமைப்பு எதிராளி - விகிடிட்டா அந்த வளங்களின் உதவியால், எங்கள் கணினியை புரிந்து கொள்ள முடியும், இயல்பாக்கி மற்றும் இணைப்பு பொருள்', 'mn': 'Энэ цаасан дээр бид BSNLP 2019 болон BSNLP 2021 онд зориулсан 2, 3-р SlavNER хуваалцааны ажлыг тайлбарлаж байна. Олон хэл хэлний Веб баримтуудад нэрлэгдсэн Объектуудын шинжилгээнд анхаарлаа хандуулсан. Бидний шийдэл нь бүтээгдэхүүн болон бүтээгдэхүүний том цуглуулалтыг ашигладаг. Өмнөх нь хэл загварын загвар болон лексикийн нэгжүүдийн сургалтын тусламжгүй сургалтын мэдээллүүд юм. Хамгийн сүүлчийн нь Википедиа, түүний бүтээгдэхүүний хамтрагч, Wikidata, бидний лимматизацийн дүрмийн эх үүсвэр болон бодит ертөнцийн бүтээлүүд юм. Эдгээр нөөцийн тусламжтай тусламжтайгаар бидний систем нэг зүйлсийг ойлгож, нормализаж, холбогдож чадна. Гэхдээ зөвхөн жижиг хэмжээний тэмдэглэгдсэн мэдээллээр сургалтын тулд', 'sr': 'U ovom papiru opisujemo svoje podatke na druge i treće delove SlavNER, održane na BSNLP 2019 i BSNLP 2021. Poslovi su se fokusirali na analizu imenovanih subjekta u multijezičkim web dokumentima na slovičkim jezicima sa bogatom uticajem. Naše rješenje koristi velike kolekcije neostrukturnih i strukturnih dokumenta. Bivši služe kao podaci za neodređenu obuku jezičkih modela i uključenje leksičkih jedinica. Posljednji se odnosi na Vikipediju i njegovu strukturiranu kolegu, Wikidata, naš izvor pravila lematizacije i pravog sveta. Uz pomoć tih resursa, naš sistem bi mogao prepoznati, normalizirati i povezati entitete, dok je obučen samo sa malim količinama označenih podataka.', 'ur': 'اس کاغذ میں ہم نے اپنے مضبوطی کو دوسرے اور تیسرے SlavNER شریک کاموں کی توصیح دی ہے جو BSNLP 2019 اور BSNLP 2021 میں حاضر ہوئے ہیں۔ ان کاموں نے اسلاویک زبانوں میں بہت سی زبان ویب دکھانوں میں نام رکھے ہوئے ایٹینٹیوں کی تحلیل پر تمرکز کی۔ ہمارا حل بہت بڑی جماعتیں اور ساختہ دفتروں کے مطابق فائدہ اٹھاتا ہے۔ پہلے لوگ زبان مدلکوں اور لکسیکل یونیٹوں کی آموزش کے لئے ناپابندی کی آموزش کے لئے ڈیٹا بناتے ہیں. آخری ویکیپیڈیا اور اس کی ساختہ کنٹرپارت کی طرف متوجہ ہوتی ہے - ویکیڈا، ہمارے لیمٹیزی قانون کے سراسر، اور حقیقی دنیا کی ایٹنیٹیوں کی طرف متوجہ ہوتی ہے. ان رسولوں کی مدد کے ساتھ، ہماری سیستم نے ایستیموں کو پہچان لیا، عامل کر لیا اور لینک کر لیا، حالانکہ صرف چھوٹے ذریعے سے آموزش کی جاتی تھی۔', 'si': 'මේ පත්තරේ අපි දෙවෙනි හා තුන්වෙනි SlavNER සමාගත වැඩ කරන්නේ BSNLP 2019 වලින් BSNLP 2021 වලින්. Name අපේ විස්තරය ප්\u200dරයෝජනය කරනවා ලොකු සංග්රහනය සහ සංවිධානය නැති ලොකු ලොකු සම්බන්ධයක්. මුලින් භාෂා මොඩේල් සහ ලෙක්සිකල් යුනිටිකල් සංවිධානය සඳහා ප්\u200dරශ්නයක් නොපුරුද්ධ වෙන්න අන්තිම විකිපිඩියාව සහ එයාගේ සංවිධානය සම්බන්ධය - විකිඩේටා, අපේ ලෙම්මේටිස් නීතිය, සහ ඇත්ත ලෝකයේ සංවිධා අපේ පද්ධතියට ප්\u200dරශ්නයක් තියෙන්න පුළුවන්, සාමාන්\u200dය සහ සම්බන්ධයක් තියෙන්න පුළුවන්, පොඩි ප්\u200dරශ්නයක් ලේබ', 'uz': "In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively.  Name Bizning qiymiz quyidagi va quyidagi hujjatlarning katta toʻplamlaridan foydalanadi. Keyingi birinchi xil tilning modellari va leksikal birliklarni ishlab chiqarish uchun maʼlumot sifatida ishlaydi. Keyingi paytda Wikipedia va uning quyidagi kompaniyasi - Wikidata, bizning taqdimot qoidalarimizning manbasi va haqiqiqiy dunyo to ʻplamlari. Bu rasmlar yordam bilan bizning tizimimiz oddiy narsalarni aniqlash, aniqlash va bogʻlash tizimlarini aniqlash mumkin, ammo faqat bir kichkina maʼlumot bilan o'rganish mumkin.", 'vi': 'Trong tờ giấy này, chúng tôi mô tả những kiến nghị của chúng tôi... dành cho Chiến dịch chia sẻ thứ hai và thứ ba... được tổ chức tại BSNLP 2009 và BSNLP 2021. Các công việc tập trung vào phân tích các tổ chức có tên trong các tài liệu mạng đa dạng trong ngôn ngữ Slavic và giàu mềm. Cách giải quyết của chúng tôi tận dụng những bộ sưu tập lớn của những tài liệu không xây dựng và cơ cấu. Đầu tiên là dữ liệu cho việc huấn luyện không giám sát các mô hình ngôn ngữ và sự ghép nối của các đơn vị ngôn ngữ. Cái sau là Wikipedia và đối tác cấu trúc của Wikipedia, nguồn của các quy tắc chia sẻ của chúng tôi, và các thực thể ngoài đời. Với sự hỗ trợ của các nguồn tài nguyên đó, hệ thống của chúng ta có thể nhận ra, làm bình thường và kết nối thực thể, trong khi được huấn luyện chỉ với một lượng nhỏ dữ liệu dán nhãn.', 'bg': 'В настоящата статия описваме нашите предложения за 2-ри и 3-ти Споделени задачи на СлавНЕР, проведени съответно на БНЛП 2019 и БНЛП 2021. Задачите бяха фокусирани върху анализа на имената в многоезични уеб документи на славянски езици с богата флексия. Нашето решение се възползва от големи колекции както от неструктурирани, така и от структурирани документи. Първите служат като данни за необуздано обучение на езикови модели и вграждане на лексикални единици. Последният се отнася до Уикипедия и нейния структуриран аналог - Уикиданни, нашия източник на правила за лематизация и същности от реалния свят. С помощта на тези ресурси нашата система може да разпознава, нормализира и свързва обекти, като същевременно се обучава само с малки количества етикетирани данни.', 'da': 'I denne artikel beskriver vi vores indlæg til 2. og 3. SlavNER Shared Tasks afholdt på henholdsvis BSNLP 2019 og BSNLP 2021. Opgaverne fokuserede på analyse af navngivne enheder i flersprogede webdokumenter på slaviske sprog med rig bøjning. Vores løsning udnytter store samlinger af både ustrukturerede og strukturerede dokumenter. Førstnævnte tjener som data til uautoriseret uddannelse af sprogmodeller og indlejringer af leksikalske enheder. Sidstnævnte refererer til Wikipedia og dets strukturerede modstykke - Wikidata, vores kilde til lemmatiseringsregler, og virkelige enheder. Ved hjælp af disse ressourcer kunne vores system genkende, normalisere og forbinde enheder, samtidig med at det kun blev uddannet med små mængder mærkede data.', 'nl': 'In dit artikel beschrijven we onze inzendingen aan respectievelijk de 2e en 3e SlavNER Shared Tasks gehouden bij BSNLP 2019 en BSNLP 2021. De taken waren gericht op de analyse van Named Entities in meertalige webdocumenten in Slavische talen met rijke buiging. Onze oplossing maakt gebruik van grote collecties van zowel ongestructureerde als gestructureerde documenten. De eerste dienen als data voor het trainen van taalmodellen zonder toezicht en het inbedden van lexicale eenheden. Dit laatste verwijst naar Wikipedia en de gestructureerde tegenhanger van Wikidata, onze bron van lemmatisatieregels, en echte entiteiten. Met behulp van deze middelen kon ons systeem entiteiten herkennen, normaliseren en koppelen, terwijl het getraind werd met slechts kleine hoeveelheden gelabelde gegevens.', 'hr': 'U ovom papiru opisujemo svoje podatke 2. i 3. SlavNER zajedničkim zadatkima održanim na BSNLP 2019. i BSNLP 2021. godini. Zadataci su usredotočeni na analizu imenovanih subjekta u multijezičkim web dokumentima na slavičkim jezicima s bogatom uticajem. Naše rješenje koristi velike kolekcije neostrukturovanih i strukturovanih dokumenta. Bivši služe kao podaci za neodređenu obuku jezičkih modela i uključenje leksičkih jedinica. Posljednji se odnosi na Wikipedia i njegovu strukturiranu kolegu - Wikidata, naše izvor pravila limmatizacije i pravog svijeta. Uz pomoć tih resursa, naš sustav bi mogao prepoznati, normalizirati i povezati entitete, dok je obučen samo sa malim količinama označenih podataka.', 'de': 'In diesem Beitrag beschreiben wir unsere Beiträge zu den 2nd und 3rd SlavNER Shared Tasks bei BSNLP 2019 bzw. BSNLP 2021. Die Aufgaben konzentrierten sich auf die Analyse von Named Entities in mehrsprachigen Webdokumenten in slawischen Sprachen mit reicher Flexion. Unsere Lösung nutzt große Sammlungen unstrukturierter und strukturierter Dokumente. Erstere dienen als Daten für das unbeaufsichtigte Training von Sprachmodellen und Einbettungen von lexikalischen Einheiten. Letzteres bezieht sich auf Wikipedia und ihr strukturiertes Gegenstück zu Wikidata, unserer Quelle für Lemmatisierungsregeln, und reale Entitäten. Mit Hilfe dieser Ressourcen konnte unser System Entitäten erkennen, normalisieren und verknüpfen, während es mit nur geringen Mengen markierter Daten trainiert wurde.', 'id': 'In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively.  Tugas fokus pada analisis Entitas bernama dalam dokumen web berbilang bahasa dalam bahasa Slavic dengan pengaruh yang kaya. Our solution takes advantage of large collections of both unstructured and structured documents.  Yang pertama melayani sebagai data untuk pelatihan tidak diawasi dari model bahasa dan embedding unit leksik. Yang terakhir merujuk Wikipedia dan counterpart strukturnya - Wikidata, sumber peraturan lemmatisasi kita, dan entitas dunia nyata. Dengan bantuan sumber daya tersebut, sistem kita bisa mengenali, normalisasi dan menghubungkan entitas, sementara dilatih dengan jumlah kecil data yang ditabel.', 'sw': 'Katika karatasi hii tunaelezea maombi yetu kwa kazi za pili na tatu za SlavNER zilizoshirikishwa kwenye BSNLP 2019 na BSNLP 2021. Mipango hiyo ilijikita kwenye uchambuzi wa Vituo vya Mitandao ya Jinai katika nyaraka za mtandao wa lugha mbalimbali kwa lugha za Kislavic wenye ushawishi tajiri. suluhisho letu linatumia mkusanyiko mkubwa wa nyaraka hizo zilizojengwa na zilizojengwa. Wazazi wa zamani huhudumia kama taarifa kwa mafunzo yasiyoeleweka katika mifano ya lugha na maeneo ya viungo vya lexico. Matokeo ya pili yanamaanisha Wikipedia na mpinzani wake ulioandaliwa - Wikidata, chanzo cha sheria zetu za unyanyasaji, na vyombo halisi vya dunia. Kwa msaada wa rasilimali hizo, mfumo wetu unaweza kutambua, kuboresha na kuunganisha vifaa vyetu, wakati wakifundishwa kwa kiasi kidogo tu cha taarifa zilizowekwa.', 'tr': 'Bu kagyzda biziň gönderişimizi BSNLP 2019 we BSNLP 2021-de 2-nji we 3-nji SlavNER paýlaşdyrylýar. Mazmunlar Slawiýa dillerinde birnäçe dilli web senediň ady birnäçe görnöşi bilen üns berildi. Biziň çözümüz döredilmedik we düzensiz senediň uly ýygnamalarynyň üstüne bir faydasy bar. Öňki dil nusgalary we leksiýal nusgalary tarapyndan saklanmaýan eğitim üçin maglumatlar hökmünde işleýärler. Soňki Vikipediýa we onuň düzümlenmiş karmaşgalaryna söz edýär - Wikidata, lemmatizaçy kurallarymyzyň çeşmesi we dünýädäki maksadlarymyz. Bu çeşmeleriň kömegi bilen sistemamyz diňe kiçi görkezilen maglumatlar bilen tanap, normallaşdyryp we baglaşdyryp biler.', 'sq': 'Në këtë letër ne përshkruajmë paraqitjet tona ndaj detyrave të ndara të SlavNER të dytë dhe të tretë mbajtur respektivisht në BSNLP 2019 dhe BSNLP 2021. Detyrat u përqëndruan në analizën e njësive të emëruara në dokumentet shumëgjuhësore të internetit në gjuhët sllave me ndikim të pasur. Zgjidhja jonë përfiton nga koleksionet e mëdha të dokumenteve të pa strukturuar dhe të strukturuar. E para shërben si të dhëna për trajnimin e pazgjidhur të modeleve gjuhësore dhe përfshirjeve të njësive lexike. Kjo e fundit referohet në Wikipedia dhe homologun e saj të strukturuar - Wikidata, burimi ynë i rregullave të limmatizimit dhe njësitë e botës reale. Me ndihmën e këtyre burimeve, sistemi ynë mund të njohë, normalizojë dhe lidhë njësitë, ndërsa të trajnohet me vetëm sasi të vogla të dhënash të etiketuara.', 'af': 'In hierdie papier beskrywe ons onderskrywings na die 2de en 3de SlavNER Gedeelde Opdragte wat gehou het by BSNLP 2019 en BSNLP 2021, respectively. Die taak gefokus is op die analiseer van Naam Entiteit in veelvuldige Web dokumente in Slavic tale met ryk infleksie. Ons oplossing neem voordeel van groot versameling van beide ongestruktureerde en struktureerde dokumente. Die vorige dien as data vir ononderwerpende oefening van taal modele en inbêring van leksiese eenhede. Die laaste verwys na Wikipedia en sy struktureerde kunstenaar - Wikidata, ons bron van lemmatisasie reëls en regte wêreld entiteite. Met die hulp van daardie hulpbronne kan ons stelsel herken, normaliseer en skakel entiteite, terwyl onderwerp word met slegs klein hoeveelheid etiketeerde data.', 'am': 'በዚህ ገጽ በBSNLP 2019 እና BSNLP 2021 የተደረገውን ለሁለተኛው እና ለሦስተኛው ባርነት ስራዎችን እናሳውቃለን፡፡ ስራዎቹ በተለየ ቋንቋ ቋንቋዎች ውስጥ የስሜት ኢንተርቶችን በሚያሳየው ሰነዱን በመፍጠር ላይ ያሳያል፡፡ መፍትረታችን የተመሠረቱና የተመሠረቱት ሰነዶች የሚጠቅመው ትልቅ ሰብስብ ነው፡፡ የቀድሞው የቋንቋ ምሳሌዎች እና የሊክሲካዊ ዩንቨርስቲዎች ማህበረሰብ ዳታዎችን ለመጠበቅ ይጠቅማል፡፡ ኋለኛይቱ Wikipedia እና የተመሠረተውን ተቃዋሚ - Wikidata፣ የግንኙነታችንን ሕግ እና እውነተኛ የዓለም አካላት ምንጭ ነው፡፡ በእነዚህ ሀብት እርዳታ፣ ሲስተካከላችን ማወቅ፣ ማስተካከል እና አካሄዱን ማቀናቀል ይችላል፣ በተጨማሪው የዳታ ብዛት ብቻ ሲያስተማሩ፡፡', 'hy': 'Այս թղթի մեջ մենք նկարագրում ենք մեր ներկայացումները 2-րդ և 3-րդ SlavNetR-ի ընդհանուր հանձնարարություններին, որոնք կատարվում են 2019 թվականին ԲՍԼՊ-ում, և 2021 թվականին ԲՍԼՊ-ում: Այս խնդիրները կենտրոնացել էին անվանված միավորների վերլուծության վրա բազլեզու վեբ փաստաթղթերում սլավիական լեզուներում, որոնք հարուստ ազդեցություն ունեն: Մեր լուծումը օգտագործում է ոչ կառուցվածքային, ոչ էլ կառուցվածքային փաստաթղթերի մեծ հավաքածուներ: Առաջինները ծառայում են որպես տվյալներ լեզվի մոդելների և լեքսիկական միավորների անվերահսկված ուսուցման համար: Վերջինը վերաբերում է Վիքիփեդիային և դրա կառուցվածքավորված հակառակը՝ Վիքիդատային, մեր լեմմատիզացիայի կանոնների աղբյուրը և իրական աշխարհի միավորներին: Այս ռեսուրսների օգնությամբ մեր համակարգը կարող էր ճանաչել, նորմալիզեցնել և կապել կազմակերպություններ, մինչդեռ ուսուցանում էր միայն փոքր քանակությամբ պիտակուցված տվյալներ:', 'az': 'Bu kańüńĪtda bizim g√∂nd…ôril…ôrimizi BSNLP 2019-d…ô v…ô BSNLP 2021-d…ô olan 2-ci v…ô 3-ci SlavNER paylaŇüńĪlmńĪŇü iŇül…ôr…ô t…ôsdiql…ôyirik. G√∂revl…ôr Slavic dill…ôrind…ô √ßoxlu dilli Web belgelerind…ô AdlńĪ Entit…ôl…ôrin analizi il…ô √ßoxlu t…ôŇükil edilmiŇüdir. Bizim √ß…ôtinlikl…ôrimiz m√ľ…ôyy…ôn edilm…ômiŇü v…ô m√ľ…ôyy…ôn edilm…ômiŇü bel…ôl…ôrin b√∂y√ľk koleksiyonlarńĪndan faydalanńĪr. ∆Źvv…ôlki m…ôlumatlar dil modell…ôrinin v…ô leksik birlikl…ôrinin istifad…ô edilm…ôsi √ľ√ß√ľn m…ôlumatlar olar. Sonuncusu Wikipedia v…ô onun qurulmuŇü komparta - Wikidata, limmatizasyon kurallarńĪnńĪn m…ônb…ôsi v…ô h…ôqiq…ôt d√ľnya entit…ôl…ôrin…ô danńĪŇüńĪr. Bu resurslarńĪn k√∂m…ôyi il…ô sistemimiz yalnńĪz ki√ßik qiym…ôtl…ôrl…ô t…ôhsil edil…ôn m…ôlumatlarńĪ tanńĪyabilir, normalize v…ô bańülayńĪr.', 'bs': 'U ovom papiru opisujemo svoje podatke 2. i 3. SlavNER zajedničkim zadatkima održanim na BSNLP 2019. i BSNLP 2021. godini. Poslovi su se fokusirali na analizu imenovanih subjekta u multijezičkim web dokumentima na slavičkim jezicima s bogatom uticajem. Naše rješenje koristi velike kolekcije neostrukturovanih i strukturovanih dokumenta. Bivši služe kao podaci za neodređenu obuku jezičkih modela i uključenje leksičkih jedinica. Posljednji se odnosi na Wikipedia i njegovu strukturiranu kolegu - Wikidata, naše izvor pravila limmatizacije i pravog svijeta. Uz pomoć tih resursa, naš sistem bi mogao prepoznati, normalizirati i povezati entitete, dok je obučen samo sa malim količinama označenih podataka.', 'ca': "En aquest paper descrivim les nostres presentacions a la segona i tercera tasca compartida d'esclaviner a BSNLP 2019 i BSNLP 2021, respectivament. The tasks focused on the analysis of Named Entities in multilingual Web documents in Slavic languages with rich inflection.  La nostra solució aprofita grans col·leccions de documents no estructurats i estructurats. The former serve as data for unsupervised training of language models and embeddings of lexical units.  Aquesta última es refereix a Wikipedia i la seva homologue estructurada - Wikidata, la nostra font de regles de limmatització, i entitats del món real. Amb l'ajuda d'aquests recursos, el nostre sistema podria reconèixer, normalitzar i enllaçar entitats, mentre estaven entrenats amb només petites quantitats de dades etiquetades.", 'bn': 'এই পত্রিকায় আমরা বিএসএনএলপি ২০১৯ এবং বিএসএনএলপি ২০১২-এ অনুষ্ঠিত দ্বিতীয় এবং তৃতীয় স্লাভনের কাজের প্রতি আমাদের প্রতিক্রিয়া বর্ণনা  স্লাভিক ভাষায় অনেক ভাষায় নামের ওয়েব ডকুমেন্ট বিশ্লেষণের উপর মনোযোগ দিয়েছে যা সমৃদ্ধ প্রভাব দিয়েছে। Our solution takes advantage of large collections of both unstructured and structured documents.  ভাষার মডেল এবং লেক্সিক্যাল ইউনিটের প্রশিক্ষণের জন্য প্রাক্তন তথ্য হিসেবে সাহায্য করা হয়েছে। পরবর্তীতে উইকিপিডিয়া এবং তার কাঠামো প্রতিপক্ষ - উইকিডাতা, আমাদের লিমেশন নিয়ম এবং বাস্তবতার উৎস। এই সম্পদের সাহায্যের মাধ্যমে আমাদের সিস্টেম স্বাভাবিক, স্বাভাবিক এবং লিঙ্ক বস্তুগুলো স্বীকার করতে পারে, যখন শুধুমাত্', 'cs': 'V tomto článku popisujeme naše příspěvky na druhém a třetím SlavNER Shared Tasks konaném na BSNLP 2019 a BSNLP 2021. Úkoly byly zaměřeny na analýzu jmenovaných entit ve vícejazyčných webových dokumentech ve slovanských jazycích s bohatou flexí. Naše řešení využívá velkých sbírek nestrukturovaných i strukturovaných dokumentů. První slouží jako data pro bez dozoru trénink jazykových modelů a vkládání lexikálních jednotek. Druhý odkazuje na Wikipedii a její strukturovaný protějšek Wikidata, náš zdroj lemmatizačních pravidel, a entity reálného světa. S pomocí těchto zdrojů byl náš systém schopen rozpoznat, normalizovat a propojit entity, zatímco byl trénován pouze s malým množstvím označených dat.', 'fi': 'Tässä artikkelissa kuvailemme ehdotuksia 2. ja 3. SlavNER Shared Tasks -tapahtumaan BSNLP 2019 ja BSNLP 2021. Tehtävät keskittyivät nimettyjen yhteisöjen analysointiin monikielisissä web-dokumenteissa slaavilaisilla kielillä, joissa on runsaasti taipumusta. Ratkaisumme hyödyntää suuria kokoelmia sekä strukturoimattomista että strukturoiduista asiakirjoista. Ensimmäiset toimivat aineistona kielimallien valvomattomaan koulutukseen ja sanastoyksiköiden sulauttamiseen. Jälkimmäinen viittaa Wikipediaan ja sen strukturoituun vastineeseen – Wikidataan, lemmatisaatiosääntöjemme lähteeseen ja reaalimaailman entiteetteihin. Näiden resurssien avulla järjestelmämme pystyi tunnistamaan, normalisoimaan ja linkittämään kokonaisuuksia samalla kun sitä koulutettiin vain pienillä määrillä merkittyä dataa.', 'fa': 'در این کاغذ ما تحویل\u200cهای خود را به وظیفه\u200cهای مشترک دوم و سوم SlavNER در BSNLP 2019 و BSNLP 2021 توصیف می\u200cکنیم. وظیفه\u200cها روی تحلیل شرکت\u200cهای نامیده\u200cشده در مدارک وب\u200cهای زیادی زبان\u200cهای اسلاویک با تأثیر ثروت تمرکز می\u200cکردند. راه حل ما از جمع\u200cآوری بزرگ\u200cهای سند\u200cهای ساخته و ساخته\u200cشده استفاده می\u200cکند. سابق به عنوان اطلاعات برای آموزش غیرقابل تحویل مدل زبان و ابتدایی از واحدهای زبانی خدمت می کنند. آخرین به ویکیپیدیا و همکاری ساخته شده\u200cاش، ویکیدادا، منبع قوانین لیماتیزی ما و ساخته\u200cهای دنیای واقعی اشاره می\u200cکند. با کمک این منابع، سیستم ما می\u200cتواند entities را شناسایی، عادت و ارتباط کند، در حالی که فقط با مقدار کوچک داده\u200cهای نقاشی آموزش می\u200cشوند.', 'et': 'Käesolevas töös kirjeldame oma ettepanekuid BSNLP 2019 ja BSNLP 2021. aasta teisele ja kolmandale SlavNERi jagatud ülesandele. Ülesanded keskendusid nimetatud isikute analüüsile mitmekeelsetes veebidokumentides slaavi keeltes rikkaliku painduvusega. Meie lahendus kasutab ära nii struktureerimata kui ka struktureerimata dokumentide suuri kogumeid. Esimesed on andmed keelemudelite järelevalveta koolitamiseks ja leksikaalüksuste manustamiseks. Viimane viitab Vikipeediale ja selle struktureeritud kolleegile - Wikiandmele, meie lemmatiseerimisreeglite allikale ja reaalmaailma olemitele. Nende ressursside abil suutis meie süsteem tuvastada, normaliseerida ja siduda olemeid, olles koolitatud ainult väikeste märgistatud andmetega.', 'ko': '본문에서 2019년과 2021년에 각각 열리는 제2회와 제3회 슬라브나 공유 임무에 대한 제출을 묘사했다.이러한 임무는 슬라브어 다국어 인터넷 문서에서 풍부한 굴절 변화를 가진 명명 실체를 분석하는 데 중심을 두었다.우리의 해결 방안은 대량의 비구조화와 구조화 문서를 이용했다.전자는 언어 모델로서의 무감독 훈련과 어휘 단위로 삽입된 데이터이다.후자는 위키백과와 그 구조화 대응물인 위키데이터, 우리의 레몬화 규칙의 출처와 현실 세계의 실체를 가리킨다.이러한 자원의 도움 아래 우리 시스템은 실체를 식별하고 규범화하며 링크할 수 있으며 소량의 표기 데이터만 사용하여 훈련을 할 수 있다.', 'sk': 'V tem prispevku opisujemo naše prispevke k 2. in 3. skupni nalogi SlavNER, ki sta potekala na BSNLP 2019 oziroma BSNLP 2021. Naloge so se osredotočile na analizo imenovanih subjektov v večjezičnih spletnih dokumentih v slovanskih jezikih z bogato infleksijo. Naša rešitev izkorišča velike zbirke nestrukturiranih in strukturiranih dokumentov. Prvi služijo kot podatki za nenadzorovano usposabljanje jezikovnih modelov in vgradnjo leksikalnih enot. Slednji se nanaša na Wikipedijo in njeno strukturirano nasprotno – Wikipodatke, naš vir pravil za lemmatizacijo in entitete realnega sveta. S pomočjo teh virov je naš sistem lahko prepoznal, normaliziral in povezal subjekte, medtem ko je bil usposobljen samo z majhnimi količinami označenih podatkov.', 'ha': "A cikin wannan takarda, Munã describe musuluntu zuwa al'amarin na 2 da 3 na SlavNER Shared Tayyinin BSNLP 2019 da BSNLP 2021. Kayan aikin da aka fokus a kan Ana yin Ana cikin takardar wasu takardar Web na'ura masu cikin Slawi da kifi. Cikakcinmu na amfani da dukkan takardar dukansu masu da aka samar da kuma aka baka. The former serve as data for unsupervised training of language models and embeddings of lexical units.  Ganda na amfani da Wikimedia da wanda aka samar da shi mai haɗi - Wikidata, yanzu'in rubutu na mutilation, da abubuwa masu cikin duniya. Ga taimakon waɗannan resource, na iya gane, na'asarmu, yana iya zama mai amfani da abubuwa, da kuma a sanar da su da yawan ƙarami kaɗan na rubutu.", 'he': 'In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively.  המשימות התמקדו בניתוח של ישויות בשמות במסמכים אינטרנטים רבות שפות בשפות סלאביות עם השפעה עשירה. הפתרון שלנו מנצל אוסף גדול של מסמכים לא מבושלים ומבנים. הקודמים משתמשים בתור נתונים לאימונים ללא השגחה של דוגמנים שפותיים ומערכות יחידות לקסיות. האחרונה מתייחסת לוויקיפדיה והשותפה המבנה שלה, ויקידאטה, מקור חוקי הלימטיזציה שלנו, ואנשים בעולם האמיתי. עם עזרת המשאבים האלה, המערכת שלנו יכולה להכיר, לנורמליז ולקשר יחידות, בזמן שהיא מתאמנת עם כמויות קטנות בלבד של נתונים מסומנים.', 'bo': 'ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོའི་འཇུག དམིགས་ཡུལ་འདིས་སྒེར་གྱི་སྐད་ཡིག ང་ཚོའི་ཐབས་ཤེས་ཀྱི་ཆ་འཕྲིན་ཡིག་ཆ་ལས་སྒྲིག་མེད་པའི་ཡིག་ཆ་གི་བསྡུར་ཚང་ཆེ་བའི་གྲངས་སུ་ལག་ལེན སྔོན་པ་དེ་ནི་སྐད་ཡིག་ཆ་མིང་དཔེ་དབྱིབས་དང་སྒྲིག་ཆ་སྦྱར་བའི་གནས་ཚུལ་གྱི་ཆ་འཕྲིན་ཡིག་ཆ་ལ་སྤྱོད་པ། འཛམ་གླིང་འདིས་Wikipedia་དང་དེའི་སྒྲིག་བཀོད་པའི་རྩ་འབྲེལ་པོ་གཉིས་ཀྱིས་མངོན་གསལ་བྱེད་ཀྱི་ཡོད། With the aid of those resources, our system could recognize, normalize and link entities, while being trained with only small amounts of labeled data.', 'jv': 'Nang pepulan iki kita dadi tanggal nggawe aturan mrasai tanggal gawe tanggal 2 lan 3 SlaVNeR Joined tasks oleh nang GSNLP 2011 lan GSNLP 2020 1, responsibno. Ombudhakan mengko ana karo akeh basa ning Named Entients ning dokumen web multi langkang Slawit karo akeh luwih apik. Relative Laptop" and "Desktop Pak oleh banjur kelompok-pakan YouTube lan video sak dituruti paketekak - Wikdatah, kelompok sami perusahaan lematirasyon, lan sami cah-cah dunyane kuwi mesthi. Ngkang luwih kanggo pernik-pernik nganggo, sistem awak dhéwé iso dianteksi, normal karo nganggo entis, sampek kudu kelangan karo nganggo sistem sing gawe barang beraksi dadi sing bisa etiket awak dhéwé.'}
{'en': 'Slav-NER : the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages', 'ar': 'Slav-NER: التحدي الثالث عبر اللغات على التعرف على الكيانات المسماة وتطبيعها وتصنيفها وربطها عبر اللغات السلافية', 'es': 'Slav-ner: el tercer desafío multilingüe sobre reconocimiento, normalización, clasificación y vinculación de entidades nombradas en lenguas eslavas', 'pt': 'Slav-NER: o 3º desafio multilíngue sobre reconhecimento, normalização, classificação e vinculação de entidades nomeadas em idiomas eslavos', 'fr': 'Slav-ner\xa0: le 3e défi multilingue sur la reconnaissance, la normalisation, la classification et la liaison des entités nommées dans les langues slaves', 'ja': 'スラヴ語- NER ：スラヴ語全体での名前付きエンティティの認識、正規化、分類、およびリンクに関する第3のクロスリンガルチャレンジ', 'zh': 'Slav-NER曰:跨斯拉夫言名实之知、规范化、类链接之三也', 'hi': 'स्लाव-एनईआर: मान्यता, सामान्यीकरण, वर्गीकरण, और स्लाव भाषाओं में नामित संस्थाओं को जोड़ने पर तीसरी क्रॉस-भाषाई चुनौती', 'ru': 'Slav-NER: 3-й межъязыковой вызов по распознаванию, нормализации, классификации и привязке именованных сущностей к славянским языкам', 'ga': 'Slav-NER: an 3ú Dúshlán Trastheangach ar Aithint, Normalú, Aicmiú, agus Nascadh Aonáin Ainmnithe thar Theangacha Slavacha', 'el': 'Σλάβα-ΝΕρ: η 3η διαγώνια πρόκληση για την αναγνώριση, την ομαλοποίηση, την ταξινόμηση και τη σύνδεση ονομάτων οντοτήτων σε όλες τις σλαβικές γλώσσες', 'ka': 'Slav-NER: მესამე კრესენგური განსაზღვრება სილავიური ენების განაცნობის, ნორმალიზაციის, კლასიფიკაციის და სახელსახულებული ელემენტების დაკავშირება', 'hu': 'Slav-NER: 3. nyelvű kihívás a nevezett entitások felismeréséről, normalizálásáról, osztályozásáról és összekapcsolásáról szláv nyelvek között', 'it': 'Slav-NER: la terza sfida multilingue sul riconoscimento, la normalizzazione, la classificazione e il collegamento delle entità nominate attraverso le lingue slave', 'kk': 'Slav-NER: Slavic тілдерінде аталған нысандарды түсініп, нормализациялау, классификациялау және аталған нысандардың сілтемесі', 'mk': 'Slav-NER: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages', 'ml': 'അടിമകള്\u200d- നെര്\u200d: സ്ലാവിക് ഭാഷകളില്\u200d മൂന്നാമത്തെ ക്രോസ്- ഭാഷ തിരിച്ചറിയുന്നതിനെപ്പറ്റി, നോര്\u200dമാലിഷനേഷന്\u200d, ക്ലാസിഷനേഷന്\u200d, പേരുള്', 'lt': 'Slav-NER: trečiasis tarpkalbinis uždavinys pripažinti, normalizuoti, klasifikuoti ir sujungti pavadintus subjektus tarp slavo kalbų', 'ms': 'Slav-NER: cabaran 3rd Cross-language on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages', 'mn': 'Slav-NER: Slavic Languages-д нэрлэгдсэн элементүүдийн холбоотой 3 давхар хэлний зорилго', 'no': 'Slav-NER: den tredje krysspråksutfordringen på gjenkjenning, normalisering, klassifikasjon og lenking av namnet einingar over slaviske språk', 'pl': 'Słowiański-NER: trzecie wyzwanie wielojęzyczne w zakresie rozpoznawania, normalizacji, klasyfikacji i łączenia nazwanych podmiotów w językach słowiańskich', 'mt': 'Slav-NER: it-tielet Sfida Translingwi dwar ir-Rikonoxximent, in-Normalizzazzjoni, il-Klassifikazzjoni, u r-Rabta ta’ Entitajiet Ismija bejn il-Lingwi Slavi', 'si': 'Slav-NER: තුන්වෙනි ක්\u200dරොස් භාෂාවික චාලන්ජින්, සාමාන්\u200dය, ක්ලාසිෆේෂන්, සහ ස්ලාවික් භාෂාවික වලින් නම් කරපු අය', 'so': 'Slav-NER: The 3rd Cross-language Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic languages', 'ro': 'Slav-NER: a treia provocare translingvistică privind recunoașterea, normalizarea, clasificarea și conectarea entităților numite în limbile slave', 'sr': 'Slav-NER: treći krstojezički izazov za priznanje, Normalizaciju, klasifikaciju i povezanje imenovanih subjekta preko slavičkih jezika', 'ta': 'அடிமை- NER: மூன்றாவது கிராஸ்- மொழி சவால் அடையாளம், சார்ந்தது, வகைப்படுத்தல், மற்றும் ஸ்லாவிக் மொழிகள் முழுவதும் பெயர் உள்ளீடுகள் இணை', 'ur': 'Slav-NER: سلویک زبانوں کے درمیان نام داروں کی نسبت تیسری کروس زبان کی تصادف کی', 'sv': 'Slav-NER: den tredje tvärspråkiga utmaningen om erkännande, normalisering, klassificering och koppling av namngivna enheter över slaviska språk', 'uz': 'Name', 'vi': 'Chủ nô-cái: thử thách ngôn ngữ học thứ ba về nhận dạng, hoá học, phân loại, liên kết các đơn vị có tên qua các ngôn ngữ Slavic', 'hr': 'Slav-NER: treći krstojezički izazov o priznanju, normalizaciji, klasifikaciji i povezivanju imenovanih subjekta na slavičkim jezicima', 'bg': 'Слав-НЕР: третото междуезично предизвикателство за разпознаване, нормализиране, класификация и свързване на имена сред славянските езици', 'nl': 'Slav-NER: de 3e meertalige uitdaging op het gebied van herkenning, normalisatie, classificatie en koppeling van benoemde entiteiten in Slavische talen', 'da': 'Slav-NER: den tredje tværsprogede udfordring om anerkendelse, normalisering, klassificering og sammenkædning af navngivne enheder på tværs af slaviske sprog', 'de': 'Slav-NER: Die dritte sprachübergreifende Herausforderung zur Erkennung, Normalisierung, Klassifizierung und Verknüpfung benannter Entitäten in slawischen Sprachen', 'ko': '슬라브어: 슬라브어 명명 실체에 대한 식별, 규범화, 분류와 링크의 세 번째 크로스 언어 도전', 'id': 'Slav-NER: tantangan 3rd Cross-language on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages', 'fa': 'Slav-NER: 3rd Cross-language challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic languages', 'sw': 'Mtumwa-NER: Mgogoro wa lugha ya tatu kuhusu Kutambua, Udhalilishaji, Uwekezaji, na Uunganishaji wa Makasiliano yaliyoitwa katika lugha za Kislavic', 'tr': 'Slav-NER: Slavyçe dillerden tanyş, normallaşdyrma, klasifikasy we Adlanylar guramlaryň arasynda gatlanma', 'af': 'Slav-NER: die 3de Kruis-tale uitdrukking oor herkening, Normaliseering, Klassifikasie en Linking van Naamde Eenheidse oor Slaviese tale', 'sq': 'Slav-NER: Sfida e tretë ndërgjuhësore mbi njohjen, normalizimin, klasifikimin dhe lidhjen e njësive të emëruara nëpër gjuhët sllave', 'am': 'ባሪያ-NER: ሦስተኛው Cross-ቋንቋ-ቋንቋ Challenge on Recognition, Normalization, Classification and Linking of Named Entities across Slavic ቋንቋዎች', 'bn': 'Slav-NER: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages', 'hy': 'Սլավ-ՆԵՌ: Երրորդ խաչլեզվային մարտահրավերը սլավիկ լեզուներում', 'az': 'Slav-NER: Slavi√ß dill…ôrin arasńĪnda ńįsiml…ônmiŇü Entit…ôl…ôrin 3-ci C…ôrc Dili ńįsiml…ônm…ôsi, Normalizasyon, Klasifikasyon v…ô ńįliŇüim', 'bs': 'Slav-NER: treći krstojezički izazov o priznanju, normalizaciji, klasifikaciji i povezivanju imenovanih subjekta na slavičkim jezicima', 'cs': 'Slovan-NER: třetí křížová výzva na rozpoznávání, normalizaci, klasifikaci a propojení jmenovaných entit napříč slovanskými jazyky', 'et': 'Slav-NER: kolmas keeleülene väljakutse nimeliste üksuste tunnustamise, normaliseerimise, klassifitseerimise ja sidumise kohta slaavikeeltes', 'ca': "Esclav-NER: el tercer repte translingüístic en reconeixement, normalització, classificació i enllaç d'entitats anomenades entre llengües esclaves", 'fi': 'Slav-NER: kolmas monikielinen haaste nimettyjen entiteettien tunnistamisesta, normalisoinnista, luokittelusta ja linkittämisestä slaavilaisten kielten välillä', 'jv': 'Slav-NOR: the third Krot-language', 'sk': 'Slav-NER: 3. medjezični izziv o prepoznavanju, normalizaciji, klasifikaciji in povezovanju imenovanih subjektov v slovanskih jezikih', 'bo': 'Slav-NER: སྔོན་ཤུགས་དང་། སྤྱིར་བཏང་ནི་དང་། སྤྱིར་བཏང་ནི་དང་། སྡེར་གྱིས་བཏང་ཡོད་པའི་སྒེར་གྱི་ཁྱད་ཆ་ལ་བརྗོད་པ་ཚོའི་ནང་དུ་འབྲེལ་བ', 'he': 'Slav-NER: האתגר הצלבי-שפוי השלישי על זיהוי, נורמליזציה, קליזציה, וקשר של ישויות בשמות על שפות סלאביות', 'ha': 'KCharselect unicode block name'}
{'en': 'This paper describes Slav-NER : the 3rd Multilingual Named Entity Challenge in Slavic languages. The tasks involve recognizing mentions of named entities in Web documents, normalization of the names, and cross-lingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten teams participated in the competition. Performance for the named entity recognition task reached 90 % F-measure, much higher than reported in the first edition of the Challenge. Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. Detailed valuation information is available on the shared task web page.', 'ar': 'تصف هذه الورقة Slav-NER: التحدي الثالث للكيان متعدد اللغات في اللغات السلافية. تتضمن المهام التعرف على إشارات الكيانات المسماة في مستندات الويب ، وتسوية الأسماء ، والربط عبر اللغات. يغطي التحدي ست لغات وخمسة أنواع من الكيانات ، ويتم تنظيمه كجزء من ورشة العمل الثامنة لمعالجة اللغة الطبيعية Balto-Slavic ، والموجودة في مكان واحد مع مؤتمر EACL 2021. شاركت عشرة فرق في المنافسة. وصل أداء مهمة التعرف على الكيان المحدد إلى 90٪ من مقياس F ، وهو أعلى بكثير مما تم الإبلاغ عنه في الإصدار الأول من التحدي. غطت سبع فرق جميع اللغات الست ، وشاركت خمس فرق في مهمة ربط الكيانات متعددة اللغات. تتوفر معلومات التقييم التفصيلية على صفحة ويب المهمة المشتركة.', 'fr': "Cet article décrit Slav-ner\xa0: the 3rd Multilingual Named Entity Challenge en langues slaves. Les tâches consistent à reconnaître les mentions d'entités nommées dans les documents Web, à normaliser les noms et à créer des liens multilingues. Le Challenge couvre six langues et cinq types d'entités, et est organisé dans le cadre du 8e atelier de traitement du langage naturel balto-slave, co-localisé avec la conférence EACL 2021. Dix équipes ont participé à la compétition. Les performances de la tâche de reconnaissance d'entité nommée ont atteint 90\xa0% de la mesure F, bien plus que ce qui était indiqué dans la première édition du Challenge. Sept équipes ont couvert les six langues, et cinq équipes ont participé à la tâche de liaison d'entités multilingues. Des informations détaillées sur l'évaluation sont disponibles sur la page Web des tâches partagées.", 'pt': 'Este artigo descreve o Slav-NER: o 3º Multilingual Named Entity Challenge em línguas eslavas. As tarefas envolvem o reconhecimento de menções de entidades nomeadas em documentos da Web, a normalização dos nomes e a vinculação entre idiomas. O Desafio abrange seis idiomas e cinco tipos de entidades e é organizado como parte do 8º Workshop de Processamento de Linguagem Natural Balto-eslava, co-localizado com a Conferência EACL 2021. Dez equipes participaram da competição. O desempenho para a tarefa de reconhecimento da entidade nomeada atingiu 90% de F-measure, muito superior ao reportado na primeira edição do Desafio. Sete equipes cobriram todos os seis idiomas e cinco equipes participaram da tarefa de vinculação de entidades multilíngues. Informações detalhadas de avaliação estão disponíveis na página da Web de tarefas compartilhadas.', 'es': 'Este artículo describe Slav-ner: the 3rd Multilingual Named Entity Challenge en lenguas eslavas. Las tareas incluyen el reconocimiento de menciones de entidades con nombre en los documentos web, la normalización de los nombres y la vinculación entre idiomas. El Desafío abarca seis idiomas y cinco tipos de entidades, y se organiza como parte del 8º Taller de Procesamiento del Lenguaje Natural Balto-eslavo, ubicado junto con la Conferencia EACL 2021. Diez equipos participaron en la competición. El rendimiento de la tarea de reconocimiento de la entidad nombrada alcanzó el 90% de la medida F, muy superior al reportado en la primera edición del Desafío. Siete equipos cubrieron los seis idiomas y cinco equipos participaron en la tarea de vinculación de entidades multilingües. La información de valoración detallada está disponible en la página web de tareas compartidas.', 'ja': '本稿では、Slav - NER:スラブ諸言語における第3の多言語名エンティティチャレンジについて説明します。タスクには、Webドキュメント内の名前付きエンティティのメンションの認識、名前の正規化、およびクロスリンガルリンクが含まれます。このチャレンジは、6つの言語と5つのエンティティタイプを対象としており、EACL 2021カンファレンスに併設されている第8回バルト・スラブ語自然言語処理ワークショップの一部として開催されています。競技には10チームが参加した。名前付きエンティティ認識タスクのパフォーマンスは、Fメジャーの90 ％に達し、チャレンジの初版で報告されたものよりもはるかに高くなりました。7チームが6つの言語すべてをカバーし、5チームがクロスリンガルエンティティリンクのタスクに参加しました。詳細な評価情報は、共有タスクのウェブページで入手できます。', 'zh': '本文述斯拉夫-NER:斯拉夫言三多言名实体。 凡此诸事, Web 文档名实、名规范化、语言链接。 凡挑战赛涵盖六言五体,第8届巴尔托 - 斯拉夫自然语言治研讨会之一,与EACL 2021会议同期举。 十队较赛。 名实知事者,90%F量直,远高于第一版挑战赛中以闻。 七小组涵盖凡六语,五小组与跨语体链接事。 详评信息可于共事网页上得之。', 'ru': 'В этой статье описывается Slav-NER: the 3rd Multilingual Named Entity Challenge in Slavic languages. Задачи включают распознавание упоминаний именованных сущностей в веб-документах, нормализацию имен и кросс-лингвистическую привязку. Вызов охватывает шесть языков и пять типов образований и организуется в рамках восьмого семинара по обработке балто-славянских естественных языков, который проводится совместно с Конференцией EACL 2021. В соревнованиях приняли участие 10 команд. Производительность именованной задачи распознавания сущностей достигла 90% F-измерения, что намного выше, чем сообщалось в первом издании Challenge. Семь групп охватили все шесть языков, и пять групп приняли участие в выполнении задачи по установлению связей между языковыми подразделениями. Подробная информация об оценке доступна на веб-странице общей задачи.', 'hi': 'यह पेपर स्लाव-एनईआर का वर्णन करता है: स्लाव भाषाओं में तीसरा बहुभाषी नामित एंटिटी चैलेंज। कार्यों में वेब दस्तावेज़ों में नामित निकायों के उल्लेखों को पहचानना, नामों का सामान्यीकरण और क्रॉस-लिंगुअल लिंकिंग शामिल है। चुनौती छह भाषाओं और पांच इकाई प्रकारों को कवर करती है, और 8 वीं बाल्टो-स्लाव प्राकृतिक भाषा प्रसंस्करण कार्यशाला के हिस्से के रूप में आयोजित की जाती है, जो ईएसीएल 2021 सम्मेलन के साथ सह-स्थित है। प्रतियोगिता में दस टीमों ने भाग लिया। नामित एंटिटी पहचान कार्य के लिए प्रदर्शन 90% एफ-माप तक पहुंच गया, जो चुनौती के पहले संस्करण में रिपोर्ट की तुलना में बहुत अधिक है। सात टीमों ने सभी छह भाषाओं को कवर किया, और पांच टीमों ने क्रॉस-लिंगुअल इकाई को जोड़ने वाले कार्य में भाग लिया। विस्तृत मूल्यांकन जानकारी साझा किए गए कार्य वेब पेज पर उपलब्ध है.', 'ga': 'Déanann an páipéar seo cur síos ar Slav-NER: an 3ú Dúshlán Aonán Ainmnithe Ilteangach i dteangacha Slavacha. Is éard atá i gceist leis na tascanna ná tagairtí d’eintitis ainmnithe i gcáipéisí Gréasáin a aithint, ainmneacha a normalú, agus nascadh tras-teangach. Clúdaíonn an Dúshlán sé theanga agus cúig chineál eintiteas, agus eagraítear é mar chuid den 8ú Ceardlann Próiseála Teanga Nádúrtha Balto-Slavacha, atá comhlonnaithe le Comhdháil EACL 2021. Ghlac deich bhfoireann páirt sa chomórtas. Shroich an fheidhmíocht don tasc aitheantais aonáin ainmnithe 90% F-tomhais, i bhfad níos airde ná mar a tuairiscíodh sa chéad eagrán den Dúshlán. Chlúdaigh seacht bhfoireann na sé theanga ar fad, agus ghlac cúig fhoireann páirt sa tasc nasctha aonán tras-teangach. Tá faisnéis mhionsonraithe luachála ar fáil ar leathanach gréasáin na dtascanna comhroinnte.', 'ka': 'ამ წიგნის აღწერა Slav-NER: მესამე მრავალენგური სახელ ინტერტიკური გამოსახულება სლავიქური ენაში. პარამეტრები აქვს საბოლოო დოკუმენტებში სახელსახულებული ინტერტიების მოცნობა, სახელსახულებების ნორმალიზაცია და მრავალური სიგრძნობა. გამოსახულება ექვსი ენები და ხუთი ინტერტიკური ტიპები აკეთებულია, და აკეთებულია 8-ი ბალტო-სლავისური თავისუფალური ენის პროცესი სამუშაო, რომელიც EACL 2021-ის კონფერენცი ევჟვრ რთოა ჟვ ნაევლთჳა გ კჲნკპვნუთწრა. სახელი ინტერტის განაცნობის რაოდენობა 90% F-მაზეტის გასაღებისთვის, უფრო მეტი, ვიდრე შეცდომა გასაღების პირველი რედაციაში. Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. განსაზღვრებული მნიშვნელობის ინფორმაცია სამუშაო საქაღალდე გვერდის ხელსაწყებელია.', 'el': 'Η παρούσα εργασία περιγράφει τη Σλάβα-ΝΕρ: την 3η πρόκληση πολυγλωσσικής ονομαστικής οντότητας στις σλαβικές γλώσσες. Οι εργασίες περιλαμβάνουν την αναγνώριση αναφορών ονομάτων οντοτήτων σε έγγραφα ιστού, την ομαλοποίηση των ονομάτων και τη διασύνδεση γλωσσών. Η Πρόκληση καλύπτει έξι γλώσσες και πέντε τύπους οντοτήτων, και διοργανώνεται στο πλαίσιο του 8ου εργαστηρίου επεξεργασίας βαλτοσλαβικής φυσικής γλώσσας, που βρίσκεται σε συνεργασία με το συνέδριο. Δέκα ομάδες συμμετείχαν στον διαγωνισμό. Η απόδοση για την εργασία αναγνώρισης οντότητας που ονομάζεται έφτασε στο 90% μέτρο F, πολύ υψηλότερη από εκείνη που αναφέρθηκε στην πρώτη έκδοση της Προκλήσεως. Επτά ομάδες κάλυψαν και τις έξι γλώσσες και πέντε ομάδες συμμετείχαν στην εργασία διασύνδεσης γλωσσικών οντοτήτων. Λεπτομερείς πληροφορίες αποτίμησης είναι διαθέσιμες στην ιστοσελίδα κοινής εργασίας.', 'hu': 'Ez a tanulmány a Slav-NER: a 3. többnyelvű elnevezett entitás kihívását ismerteti szláv nyelveken. A feladatok magukban foglalják a nevezett entitások webes dokumentumokban történő említéseinek felismerését, a nevek normalizálását és a nyelvek közötti kapcsolódást. A kihívás hat nyelvre és öt entitástípusra terjed ki, és a 8. Balto-szláv Természetes Nyelvfeldolgozó Workshop részeként kerül megrendezésre, amely az EACL 2021 konferenciájával együtt található. Tíz csapat vett részt a versenyen. A megnevezett entitás felismerési feladat teljesítménye elérte a 90%-os F-mértéket, ami sokkal magasabb, mint a Kihívás első kiadásában jelentett. Mind a hat nyelvet hét csapat fedte le, és öt csapat vett részt a többnyelvű entitás összekapcsolásában. Részletes értékelési információk a megosztott feladat weboldalán találhatók.', 'it': "Questo articolo descrive Slav-NER: la terza sfida multilingue delle entità denominate nelle lingue slave. I compiti includono il riconoscimento delle menzioni di entità nominate nei documenti Web, la normalizzazione dei nomi e il collegamento cross-lingual. La Sfida copre sei lingue e cinque tipi di entità ed è organizzata nell'ambito dell'ottavo workshop di elaborazione del linguaggio naturale Balto-slavo, co-situato con la Conferenza EACL 2021. Dieci squadre hanno partecipato alla competizione. Le prestazioni per l'attività di riconoscimento dell'entità nominata hanno raggiunto il 90% F-measure, molto più alto di quanto riportato nella prima edizione della Challenge. Sette squadre hanno coperto tutte e sei le lingue e cinque squadre hanno partecipato al compito di collegamento tra entità multilingue. Informazioni dettagliate sulla valutazione sono disponibili sulla pagina web delle attività condivise.", 'kk': 'Бұл қағаз Slav- NER: слав тілдерінде үшінші көп тілді аталған нысандар шақыруы. Тапсырмалар Веб құжаттарында аталған нысандарды анықтау, атауларды нормализациялау және тілдерді көптеген сілтемелерді түсініп тұрады. Бұл шақыру алты тілдер мен бес нысандар түрін таңдайды, және 8- ші Балто- Славиттік Түзіндік Түзіндік процессорының жұмысының бөлігі болып, EACL 2021 конференциясында орналастырылады. Он команда бұл конкурсқа қатысушы болды. Аталған нысандарды анықтау тапсырмасының орындалуы 90% F- өлшеміне жетті, Challenge алғашқы шығармасындағы хабарламадан артық. Жеті топ барлық алты тілдерді басып, бес топ тапсырманы қосып тұрған тілдерді қосып тұрған. Ортақ тапсырмалар веб- парақтағы егжей- тегжейі мәндер мәліметі бар.', 'ml': 'ഈ പത്രത്തില്\u200d സ്ലാവിക് ഭാഷകളിലെ മൂന്നാമത്തെ പല ഭാഷകളുടെ പേരുള്ള എന്റിറ്റി ചാലന്\u200dജ് വിവരിക്കുന്നു. വെബ് രേഖകളില്\u200d പേരുള്ള വസ്തുക്കളുടെ പേരുകള്\u200d തിരിച്ചറിയുന്നതും, പേരുകളുടെ സാധാരണ വ്യക്തിപരമാക്കുന്നതും, ക്രിസ്ലിങ ചാലഞ്ചെന്\u200dജ് ആറു ഭാഷകളും അഞ്ചു വസ്തുക്കളുടെ രീതികളും പൂര്\u200dത്തിയാക്കുന്നു. എക്സില്\u200d 2021 കോണ്\u200dഫറന്\u200dസിനോടൊപ്പം സ്ഥാപിക്കുന്ന എട്ടാം ബല്\u200dട്ടോ പത്തു ടീം പ്രതിയോഗിച്ചു. പേരിട്ട വസ്തുവിന്റെ തിരിച്ചറിയുന്ന ജോലി ഏഴ് ടീം എല്ലാ ആറു ഭാഷകളും മൂടിയിരുന്നു. അഞ്ചു ടീം ക്രിസ്ലിങ്ങ് ലിങ്ങ് ചെയ്യുന്ന സാധാരണയില്\u200d പങ്കുചേ പങ്കാളിയുള്ള ജോലി വെബ് താളില്\u200d വിശദീകരിച്ച വില വിവരങ്ങള്\u200d ലഭ്യമാകുന്നു.', 'mt': 'Dan id-dokument jiddeskrivi Slav-NER: it-tielet Sfida ta’ Entità b’Ismijiet Multilingwi fil-lingwi Slavi. Il-kompiti jinvolvu r-rikonoxximent ta’ menzjonijiet ta’ entitajiet imsemmija fid-dokumenti tal-internet, in-normalizzazzjoni tal-ismijiet, u r-rabta bejn il-lingwi. L-Isfida tkopri sitt lingwi u ħames tipi ta’ entitajiet, u hija organizzata bħala parti mit-tmien Ħidma ta’ Proċessar tal-Lingwi Naturali Balto-Slavi, li tinsab flimkien mal-Konferenza EACL 2021. Għaxar timijiet ipparteċipaw fil-kompetizzjoni. Il-prestazzjoni għall-kompitu ta’ rikonoxximent tal-entità msemmi laħqet il-miżura F ta’ 90 %, ferm ogħla minn dik irrappurtata fl-ewwel edizzjoni tal-Isfida. Seba’ timijiet koprew is-sitt lingwi kollha, u ħames timijiet ipparteċipaw fil-kompitu ta’ rabta bejn l-entitajiet lingwistiċi. Informazzjoni dettaljata dwar il-valutazzjoni hija disponibbli fuq il-paġna web tal-kompiti kondiviżi.', 'ms': 'Kertas ini menggambarkan Slav-NER: cabaran Entiti bernama berbilang ketiga dalam bahasa Slav. The tasks involve recognizing mentions of named entities in Web documents, normalization of the names, and cross-lingual linking.  Challenge meliputi enam bahasa dan lima jenis entiti, dan disorganisasi sebagai sebahagian dari Ke-8 Kerja Proses Bahasa Alami Balto-Slavic, ditempatkan bersama dengan Persidangan EACL 2021. Ten teams participated in the competition.  Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge.  Tujuh pasukan menutupi semua enam bahasa, dan lima pasukan berpartisipasi dalam tugas menyambung entiti saling bahasa. Maklumat penghargaan terperinci tersedia pada halaman web tugas berkongsi.', 'mk': 'Овој весник го опишува Slav-NER: третиот предизвик на мнографски именуван ентитет на славски јазици. Овие задачи вклучуваат признавање на спомените на именуваните ентитети во веб-документите, нормализација на имињата и прекујазични поврзувања. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference.  Ten teams participated in the competition.  Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge.  Седум тимови ги покриваа сите шест јазици и пет тимови учествуваа во прекујазичниот ентитет кој ја поврзува задачата. Detailed valuation information is available on the shared task web page.', 'lt': 'This paper describes Slav-NER: the 3rd Multilingual Named Entity Challenge in Slavic languages.  Užduotys apima vardinių subjektų pripažinimą interneto dokumentuose, vardų normalizavimą ir tarpkalbinį ryšį. Iššūkis apima šešias kalbas ir penkis subjektų tipus ir organizuojamas kaip 8-ojo Balto-Slavo gamtos kalbų apdorojimo seminaro dalis, kuris bendradarbiauja su EACL 2021 konferencija. Ten teams participated in the competition.  Pavardyto subjekto pripažinimo užduoties rezultatai pasiekė 90 % F priemonę, gerokai didesnę nei nurodyta pirmame iššūkio leidinyje. Septynios komandos apimo visas šešias kalbas ir penkios komandos dalyvavo tarpkalbiniame subjekte, susiejančiame užduotį. Išsami vertinimo informacija pateikiama bendrame užduoties interneto puslapyje.', 'mn': 'Энэ цаас Slav-NER: Славийн хэлний 3-р олон хэлний нэрлэгдсэн Entity Challenge гэдгийг тайлбарладаг. Энэ үйл ажиллагаанд Веб баримтуудад нэрлэгдсэн бүтээлүүдийг таних, нэрийг нормализах, олон хэлний холбоотой байдал юм. Шалгаан нь зургаан хэл, таван нэгж төрлийн хэлбэрийг агуулдаг. Энэ нь EACL 2021 оны Конференцийн 8-р Балто-Славийн Байгалийн Холбоо Процессийн Их Сургуулийн нэг хэсэг болсон. 10 баг өрсөлдөөнд оролцсон. Нэгдсэн нэр хүлээн зөвшөөрөгдсөн ажил нь F-хэмжээгээс 90%-аас илүү өндөр байлаа. 7 баг бүх зургаан хэл, 5 баг олон хэл бүрийн ажлыг холбоотой ажилд оролцсон. Тодорхой үнэ цэнэтэй мэдээллийг хуваалцах ажлын веб хуудас дээр гаргаж байна.', 'no': 'Denne papiret beskriver Slav- NER: den tredje fleirspråksforskjellen med namnet entitet i slaviske språk. Oppgåvene involverer gjenkjenning av namn på einingar i nettdokument, normalisering av namn og krysspråk lenking. Challenge covers six languages and five entities types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten grupper delta i konkurransen. Funksjonen for oppgåva med namnet entitetskjenning fikk 90% F- mål, mykje høgare enn rapportert i den første utgåva av Challenge. Sju grupper dekket alle seks språk, og fem grupper delta i den krysspråksteininga som lenkjer oppgåva. Detaljerte verdisinformasjon er tilgjengeleg på den delte oppgåveside.', 'pl': 'Niniejszy artykuł opisuje Słowiański-NER: Trzecie Wielojęzyczne Wyzwanie Nazwanych Entit w językach słowiańskich. Zadania obejmują rozpoznawanie wzmianek nazwanych podmiotów w dokumentach internetowych, normalizację nazw oraz łączenie między językami. Wyzwanie obejmuje sześć języków i pięć rodzajów podmiotów i jest organizowane w ramach VIII Warsztatu Przetwarzania Języka Naturalnego Balto-Słowiańskiego, współorganizowanego z Konferencją EACL 2021. W zawodach uczestniczyło dziesięć drużyn. Wydajność wymienionego zadania rozpoznawania podmiotów osiągnęła 90% F-miary, znacznie wyższa niż zgłoszono w pierwszej edycji Wyzwania. Siedem zespołów obejmowało wszystkie sześć języków, a pięć zespołów uczestniczyło w zadaniu łączącym podmioty między językami. Szczegółowe informacje o wycenie są dostępne na stronie internetowej zadań współdzielonych.', 'si': 'මේ පත්තුව Slav-NER විස්තර කරනවා: 3වෙනි භාෂාවක් නම් කිරීමේ ඉන්තිත් ප්\u200dරශ්නයක් ස්ලාවික් භාෂාවක වෙබ් විස්තාරයේදී, නම් සාමාන්\u200dයය සහ ක්\u200dරීස් භාෂාවික සම්බන්ධයේදී කිරීමට සම්බන්ධ කරනවා. The challenge Covers 6 language and five units types, and is Organised as part of the 8th Balto-Slavic Native language processing Workshop, co-located with the EACL 2021 Conferenment. කණ්ඩායම් දහයක් තර්ජනයේ සම්බන්ධ වුනා. නම් කියලා තියෙන අයිති අඳුරණු වැඩේ 90% F- මාර්ගයක් ලැබුණා, චාල්යාන්ජ් පළමු ප්\u200dරතිචාරයේ ප්\u200dරතිච කණ්ඩායම් හත් කණ්ඩායම් හැම භාෂාවක් හයක්ම කළා, සහ කණ්ඩායම් පහක් කණ්ඩායම් විශාල භාෂාව විස්තර අවශ්\u200dය තොරතුරු භාගයක් වෙබ පිටුවේ තොරතුරු ලැබෙනවා.', 'ro': 'Această lucrare descrie Slav-NER: a treia provocare multilingvă a entităților numite în limbile slave. Sarcinile implică recunoașterea mențiunilor entităților denumite în documentele Web, normalizarea numelor și legătura între limbi. Provocarea acoperă șase limbi și cinci tipuri de entități și este organizată în cadrul celui de-al 8-lea atelier de prelucrare a limbilor naturale Balto-slave, co-situat cu Conferința EACL 2021. Zece echipe au participat la competiţie. Performanța pentru sarcina de recunoaștere a entității numită a atins 90% măsură F, mult mai mare decât s-a raportat în prima ediție a Provocării. Șapte echipe au acoperit toate cele șase limbi, iar cinci echipe au participat la sarcina de legătură a entităților interlingve. Informații detaliate de evaluare sunt disponibile pe pagina web a activităților partajate.', 'ta': 'இந்த தாள் Slav- NER விளக்குகிறது: மூன்றாம் பல மொழிகள் பெயரிடப்பட்ட முதல் சவால்ஜ் ஸ்லாவிக் மொழிகளில். பணிகள் இணைய ஆவணங்களில் பெயரிடப்பட்ட பொருள்களின் குறிப்புகளை அறியும், பெயர்களின் வழக்கம், மற்றும் மொழி இணைப்புகள். The Challenge covers six languages and five entities types, and is organized as part of the 8th Balto- Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. பத்து குழுக்கள் போட்டியில் பங்கிட்டனர். Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge.  ஏழு குழுக்கள் எல்லா ஆறு மொழிகளையும் மூடப்பட்டுள்ளன, மற்றும் ஐந்து குழுக்கள் குறுக்கும் மொழியில் இணைப் பகிர்ந்த பணியின் இணைய பக்கத்தில் விவரமான மதிப்பு தகவல் கிடைக்கும்.', 'sv': 'Denna uppsats beskriver Slav-NER: den tredje flerspråkiga namngivna entitetsutmaningen i slaviska språk. Uppgifterna innefattar att känna igen omnämnanden av namngivna entiteter i webbdokument, normalisering av namnen och korspråkig länkning. Utmaningen omfattar sex språk och fem entitetstyper, och organiseras som en del av den 8:e Balto-slaviska Natural Language Processing Workshop, i samarbete med EACL 2021-konferensen. Tio lag deltog i tävlingen. Prestationen för den namngivna entitetsidentifieringsuppgiften nådde 90% F-mått, mycket högre än vad som rapporterades i den första utgåvan av Utmaningen. Sju team täckte alla sex språk och fem team deltog i den tvärspråkiga enhetssammanlänkningsuppgiften. Detaljerad värderingsinformation finns tillgänglig på webbplatsen för delade uppgifter.', 'sr': 'Ovaj papir opisuje Slav-NER: treći izazov višejezičkog imenovanog entiteta na slavičkim jezicima. Ovi zadaci uključuju priznanje spomena imenovanih entiteta u web dokumentima, normalizaciju imena i preko jezika povezanja. Izazov pokriva šest jezika i pet tipa entiteta, i organizuje se kao deo 8. Balto-Slavičkog radionice za proces prirodnog jezika, zajedno sa EACL 2021. konferencijom. Deset tima su sudjelovali u takmičenju. Proizvodnja zadataka priznanja imena entiteta postigla je 90% F-mjere, mnogo više nego što je prijavljena u prvom izdanju izazova. Sedam tima pokrivalo je sve šest jezika, a pet timova sudjelovalo u međujezičkom entitetu povezivanju zadataka. Detaljne informacije o ocjeni su dostupne na zajedničkoj web stranici zadataka.', 'so': 'Kanu wuxuu ku qoran yahay Slav-NER: luqada saddexaad oo afka Slavic lagu magacaabay Entity Challenge. Shaqooyinka waxaa ka mid ah aqoonsashada macluumaadka lagu magacaabay bogga dukumentiyada internetka, qaabilsiinta magacyada iyo xiriirka luuqadaha kala duwan. Challenge waxay qarisaa lix luuqadood iyo shan noocyo oo kaliya, waxaana loo qabanqaabiyaa warqada Processing Luqada asalka ah ee 8aad ee Balto-Slavic, kaas oo la xiriira shirka EACL 2021. Toban koox ayaa ka qayb galay competition. Shaqada aqoonsiga ee qofka la magacaabay wuxuu gaadhay 90% F oo ka sarreeya warqada ugu horeeya ee Challenge. Toddoba koox oo ku qoran lix luuqadood oo dhan, shan koox ayaa ka qayb galay shaqada iskuulka xiriira luuqadaha. Bogga shabakadda shaqada waxaad ka heleysaa macluumaadka qiimaha gaarka ah.', 'ur': 'This paper describes Slav-NER: the third Multilingual Named Entity Challenge in Slavic languages. ان کاموں میں ویب دفتروں میں نام داروں کے ذکر پہچان جانے کے شامل ہے، ناموں کی تعبیر، اور کل زبان کی تعبیر. چالینگ چھ زبانیں اور پانچ ایٹنی ٹیپ پورش کرتی ہے، اور آٹھم بالٹو-سلویک طبیعی زبان پردازی کارشاپ کی حصہ کے طور پر سازمان کی گئی ہے، جو EACL 2021 کنفرانس کے ساتھ ہے. دس تیموں مقابلہ میں شریک ہوئے۔ انٹیٹی شناسایی کا کام 90% F-measure تک پہنچ گیا ہے، چالینگ کے پہلی ایڈیٹ میں گزارے ہوئے سے بہت زیادہ بلند ہے۔ سات تیموں نے تمام چھ زبانوں پر پورا کیا اور پانچ تیموں نے کروس زبان کی ایک ٹیٹی میں مشارکت کی۔ سفارشی ٹاکس ویب صفحے پر detailed valuation information available on the shared task web page.', 'uz': "Бу саҳифа Slav-NER: Slavic tilida 3 chi ko'pchilik tili nomli Entity Challenge (Slavic tilida) Name Name 10 guruhi rivojlanishga ega bo'lgan. Name Boʻsh guruh barcha 6 tillar bilan qaragan va besh guruhga bogʻ'liq tashkilotga ega bo'lgan qismlarga ega bo'lgan. @ info", 'vi': 'Bài viết này mô tả Nam chủ nô-NER: Con thách thức gọi nhiều ngôn ngữ thứ ba trên ngôn ngữ Slavic. Nhiệm vụ bao gồm việc nhận ra các thực thể có tên trong các tài liệu Mạng, việc bình thường các tên, và kết nối ngôn ngữ khác nhau. The Challeng bao gồm sáu ngôn ngữ và năm loại thực thể, và is organized as part of the 8th Balto-Slavic Natural Language Proxử lý Ngôn ngữ, coogn-located with the EAM 2021 Convphiền. Mười đội tham gia cuộc thi. Thành quả cho nhiệm vụ nhận dạng thực thể được đặt tên đã đạt tới 97-F-thước đo, còn cao hơn nhiều so với báo cáo trong lần thử thách đầu tiên. Bảy đội bao gồm cả sáu ngôn ngữ, và năm đội đã tham gia vào nhiệm vụ liên kết ngôn ngữ khác nhau. Chi tiết đánh giá được cung cấp trên trang web chia sẻ.', 'da': 'Denne artikel beskriver Slav-NER: den 3. flersprogede navngivne enhedsudfordring på slaviske sprog. Opgaverne involverer anerkendelse af navngivne enheder i webdokumenter, normalisering af navnene og tværsproget sammenkædning. Udfordringen dækker seks sprog og fem enhedstyper, og er organiseret som en del af den 8. Balto-Slavic Natural Language Processing Workshop, der er placeret sammen med EACL 2021 konferencen. Ti hold deltog i konkurrencen. Præstationen for den navngivne enhedsgenkendelse opgave nåede 90% F-måling, meget højere end rapporteret i første udgave af udfordringen. Syv hold dækkede alle seks sprog, og fem hold deltog i den tværsprogede enhedsforbindelsesopgave. Detaljerede værdiansættelsesoplysninger er tilgængelige på websiden for delte opgaver.', 'nl': 'Dit artikel beschrijft Slav-NER: de 3e meertalige Named Entity Challenge in Slavische talen. De taken omvatten het herkennen van vermeldingen van benoemde entiteiten in webdocumenten, normaliseren van de namen en cross-lingual linken. De Challenge bestrijkt zes talen en vijf entiteitentypes, en wordt georganiseerd als onderdeel van de 8e Balto-Slavische Natural Language Processing Workshop, co-located met de EACL 2021 Conference. Tien teams namen deel aan de wedstrijd. De prestaties voor de benoemde entiteitsherkenningstak bereikten 90% F-maat, veel hoger dan in de eerste editie van de Challenge. Zeven teams behandelden alle zes talen en vijf teams namen deel aan de cross-lingual entity linking taak. Gedetailleerde waarderingsinformatie is beschikbaar op de gedeelde taak webpagina.', 'hr': 'Ovaj papir opisuje Slav-NER: treći izazov višejezičkog imenovanog entiteta na slavičkim jezicima. Zapovijedi uključuju priznanje spomena imenovanih subjekta u web dokumentima, normalizaciju imena i međujezičkog povezanja. Izazov pokriva šest jezika i pet tipa entiteta, a organiziran je u sklopu 8. Balto-Slavičkog radionice za proces prirodnog jezika, zajedno s EACL 2021. konferencijom. Deset tima sudjelovalo je u takmičenju. Učinnost zadatka za priznanje imena entiteta postigla je 90% F mjere, mnogo više nego što je prijavljena u prvom izdanju izazova. Sedam tima pokrivalo je sve šest jezika, a pet tima sudjelovalo u međujezičkom entitetu povezivanju zadataka. Detaljne informacije o ocjenama dostupne su na zajedničkoj web stranici zadataka.', 'bg': 'Статията описва Слав-НЕР: третото многоезично предизвикателство с имена на същества в славянски езици. Задачите включват разпознаване на споменавания на имена обекти в уеб документи, нормализиране на имената и междуезично свързване. Предизвикателството обхваща шест езика и пет типа субекти и се организира като част от 8-ия балтославянски семинар за обработка на естествени езици, съвместно с конференцията на ЕАКЛ 2021. Десет отбора участваха в състезанието. Изпълнението на задачата за разпознаване на наименованите субекти достигна 90% мярка, много по-високо от отчетеното в първото издание на Предизвикателството. Седем екипа обхванаха всичките шест езика, а пет екипа участваха в задачата за свързване на междуезични единици. Подробна информация за оценка е достъпна на уеб страницата на споделената задача.', 'de': 'Diese Arbeit beschreibt Slav-NER: die 3rd Multilingual Named Entity Challenge in slawischen Sprachen. Die Aufgaben umfassen das Erkennen von Erwähnungen benannter Entitäten in Webdokumenten, die Normalisierung der Namen und die sprachübergreifende Verknüpfung. Die Challenge umfasst sechs Sprachen und fünf Entitätentypen und wird im Rahmen des achten baltoslavischen Workshops zur Verarbeitung natürlicher Sprache organisiert, der gemeinsam mit der EACL 2021 Konferenz stattfindet. Zehn Teams nahmen am Wettbewerb teil. Die Leistung der benannten Entitätenerkennungsaufgabe erreichte 90% F-Maß, viel höher als in der ersten Ausgabe der Challenge berichtet. Sieben Teams deckten alle sechs Sprachen ab, und fünf Teams nahmen an der sprachübergreifenden Entitätenverknüpfung teil. Detaillierte Bewertungsinformationen finden Sie auf der gemeinsamen Aufgabenseite.', 'ko': "본고는 슬라브어: 슬라브어 중의 세 번째 다중 언어 명명 실체의 도전을 묘사한다.이러한 작업에는 웹 문서에서 언급된 명명된 엔티티 식별, 이름 사양화, 언어 간 링크 식별 등이 포함됩니다.이번 도전은 6개 언어와 5개 실체 유형을 아우르는 제8회 발토-슬라브 자연 언어 처리 세미나의 일환으로 EACL 2021 콘퍼런스와 함께 개최된다.열 팀이 시합에 참가했다.네이밍 엔티티 인식 임무의 성능은'챌린지'1판의 보고서보다 훨씬 높은 90%의 F-measure에 달했다.7개 팀은 모든 6개 언어를 포함하고 5개 팀은 크로스 언어 실체 링크 임무에 참여했다.공유 작업 홈페이지에서 상세한 견적 정보를 제공하였다.", 'fa': 'این کاغذ Slav-NER را توصیف می\u200cکند: سومین چالش Entity نامیده شده در زبانهای اسلاویک. این وظیفه شامل تشخیص یادآوری از عنوان\u200cهای نامیده در سند\u200cهای وب، تعریف نام\u200cها و ارتباط\u200cهای متوسط زبان است. چالش شش زبان و پنج نوع اجتماعی را پوشش می\u200cدهد، و به عنوان بخشی از کارگاه\u200cهای پرداخت زبان طبیعی بالتو-اسلاویک، همکاری با کنفرانس EACL 2021 سازمان می\u200cشود. ده تیم در مسابقه شرکت کردند. عملکرد واژه شناسایی entity نامیده به اندازه ۹۰ درصد F رسید، بیشتر از گزارش در اولین نسخه چالج. هفت تیم همه شش زبان را پوشش دادند و پنج تیم در یک شرکت متوسط زبان مشارکت کردند. اطلاعات ارزش جزئیات در صفحه وب کار مشترک دسترسی دارد.', 'sw': "Gazeti hili linaelezea Slav-NER: Lugha ya tatu inayoitwa Ujumbe katika lugha za Kislavic. Kazi hizi zinahusisha kutambua majina ya vifaa vinavyoitwa katika nyaraka za mtandaoni, utaratibu wa majina, na viungo vya lugha. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference.  Timu kumi walishiriki katika mashindano. Performance for the name's recognition entity reached 90% F, higher than reported in the first edition of Challenge. Timu saba zilichukua lugha zote sita, na timu tano walishiriki katika kazi ya kuunganisha lugha. Taarifa zilizoelezwa za thamani zinapatikana kwenye tovuti ya kazi zinazoshirikishwa.", 'id': 'Kertas ini menggambarkan Slav-NER: tantangan Entitas bernama berbilang bahasa ketiga dalam bahasa Slav. Tugas-tugas melibatkan mengenali sebutan entitas bernama dalam dokumen web, normalisasi nama-nama, dan hubungan saling bahasa. Tantangan meliputi enam bahasa dan lima jenis entitas, dan terorganisir sebagai bagian dari Workshop 8 Balto-Slavic Natural Language Processing Workshop, terletak bersama dengan Konferensi EACL 2021. Ten teams participated in the competition.  Performasi untuk tugas pengenalan entitas bernama mencapai 90% ukuran F, jauh lebih tinggi dari yang dilaporkan di edisi pertama tantangan. Tujuh tim menutupi semua enam bahasa, dan lima tim berpartisipasi dalam entitas saling bahasa menghubungkan tugas. Informasi penghargaan terperinci tersedia di halaman web tugas berbagi.', 'af': 'Hierdie papier beskryf Slav- NER: die 3de Multilingual Genaamde Eenheidspeletjie in Slavic tale. Die opdragte involveer herkening van bepaalde entiteite in webdokumente, normalisering van die name en kruistale koppeling. Die uitdrukking bedek ses tale en vyf entiteite tipes, en is organiseer as deel van die 8de Balto-Slavic Natuurlike Taal Prosessering Werkshop, wat saamgeplaas met die EACL 2021 Konferensie. tien teams het in die mededing gedeel. Performasie vir die genaamde entiteiterkening taak het 90% F-maat bereik, baie hoër as rapporteer in die eerste uitsig van die Challenge. Sewe teams het al ses tale bedek, en vyf teams het gedeel in die kruistale entiteit wat die taak verbind het. Gedetaileerde waarde inligting is beskikbaar op die gedeelde taak web bladsy.', 'am': 'ይህ ገጽ ስልፍ-NER: ሦስተኛው ብልልቋንቋ የስሎቪ ቋንቋ የተባለው የEntity Challenge ይናገራል፡፡ ስራዎቹ የዌብ ሰነዶች፣ የስሞች ማቀናቀል እና የቋንቋ-ቋንቋ ግንኙነት የሚያስታውቁ አካባቢዎች እና ማቀናቀል ነው፡፡ ቻልጋኑ ስድስት ቋንቋዎች እና አምስት አካባቢዎች ዓይነቶች ይሸፍናል፣ እናም ከEACL 2021 ጉባኤ ጋር የተገኘ 8 ባላት-ስልጣዊ ቋንቋ ፕሮጀክት ክፍል ክፍል ይደረጋል፡፡ አሥር ቡድን ተቃውሞ ነበር፡፡ የመጀመሪያ ክፍል ውስጥ ካለው ክፍል የበለጠ ክፍል በ90 በመቶ F መስመር ደረሰ፡፡ ሰባት ቡድን ስድስት ቋንቋዎች ሁሉ ይሸፍናሉ፥ አምስቱም የቋንቋ አካባቢ አካባቢ ተካክሎ ነበር፡፡ የኩነቶች መረጃ', 'tr': 'Bu kagyz Slav-NER: Slawiýa dilinde üçünji köp dilli adylan bir zady çykarýar Meýdançalar Web senediň adlanylaryň habarlarynyň aýdyşyny tanamakda, adlary diňe bir dil baglaýyşyny diňleýär. Zorlamak alty diller we bäş sanat tipi bar we 8-nji Balto-Slawiýa Dogaty Diller işleýän Workshop bolan we EACL 2021 konferensiýasynda ýerleşýän. On topar ýaryşykda goşuldy. Adynyň barlag tanamasynyň görevi 90% F-ölçüsi ýokary, Challenge birinji edişinde rapor edilýän zadyndan köp ýokary boldy. Yedi topar ähli alty dilleri ýapýardy we beş topar çyz dil baglaşdyryşynda gatnaşdy. Details', 'sq': 'Ky dokument përshkruan Slav-NER: sfidën e tretë të njësisë me emër shumëgjuhës në gjuhët sllavë. Detyrat përfshijnë njohjen e përmendimeve të njësive të emëruara në dokumentet e internetit, normalizimin e emrave dhe lidhjen ndërgjuhësore. Sfida mbulon gjashtë gjuhë dhe pesë lloje njësie dhe është organizuar si pjesë e Workshop të 8-të Balto-Sllav për Procesimin e gjuhëve natyrore, bashkëpunuar me konferencën e EACL 2021. Dhjetë ekipe morën pjesë në konkurs. Performanca për detyrën e emëruar të njohjes së njësisë arriti 90% masë F, shumë më të lartë se raportuar në edicionin e parë të sfidës. Shtatë ekipe mbuluan të gjashtë gjuhët dhe pesë ekipe morën pjesë në detyrën e lidhjes ndërgjuhësore të njësisë. Informacioni i detajuar i vlerësimit është në dispozicion në faqen web të detyrave të përbashkëta.', 'bn': 'এই পত্রিকাটি স্লাভিক ভাষায় তৃতীয় মাল্টিভাষার নামের এন্টিটি চ্যালেঞ্জ বর্ণনা করেছে। এই কাজগুলোর মধ্যে ওয়েব ডকুমেন্টে, নামের স্বাভাবিকভাবে স্বীকৃতি এবং স্বাভাবিক ভাষার লিংকের উল্লেখ রয়েছে। চ্যালেঞ্জ ছয়টি ভাষা এবং পাঁচটি বস্তু ধরনের কাভার প্রদান করে, এবং আয়োজন করা হয় ৮ম বাল্টো-স্লাভিক স্বাভাবিক ভাষার প্রক্রিয়া কর্মশালার অংশ হিসেবে। দশ দল এই প্রতিযোগিতায় অংশগ্রহণ করেছে। নামের বস্তুর স্বীকৃতি কাজের জন্য ৯০% এফ মাপ পৌঁছেছে, চ্যালেঞ্জের প্রথম সংস্করণে রিপোর্ট করা হয়েছে। সপ্তাহ দল সকল ছয় ভাষায় ঢেকে দেয়া হয়েছে এবং পাঁচটি দল ক্রিশ ভাষাভাষায় লিঙ্কিং প্রতিষ্ঠানে অংশগ্রহণ করেছে। Detailed valuation information is available on the shared task web page.', 'hy': 'Այս հոդվածը նկարագրում է Slav-NOR-ը՝ երրորդ բազլեզու անվանումների մարտահրավերը սլավական լեզուներում: Այս խնդիրները ներառում են համացանցի փաստաթղթերում անվանված միավորների հայտարարման, անունների նորմալիզացիայի և լեզվի միջև կապերի ճանաչելը: Խնդիրը ներառում է վեց լեզու և հինգ անհատականության տեսակ և կազմակերպվում է որպես Բալտո-Սլավիական բնական լեզվի մշակույթի 8-րդ աշխատասենյակի մաս, որը համատեղվում է 2021 թվականի ԱՄԿ-ի կոնֆերանսի հետ: Տաս թիմ մասնակցել է մրցակցության մեջ: Նման անհատականության ճանաչության առաջադրանքի արդյունքը հասավ F-չափի 90 տոկոսին, շատ ավելի բարձր, քան հայտարարվեց մարտահրավերի առաջին հրատարակում: Յոթ թիմերը ծախսեցին բոլոր վեց լեզուները, և հինգ թիմերը մասնակցեցին լեզվային առանձնահատվածներին, որոնք կապում են խնդիրը: Մասնավոր գնահատման ինֆորմացիան հասանելի է ընդհանուր աշխատանքի վեբ էջի վրա:', 'az': 'Bu kağıt Slav-NER: Slaviq dillərində üçüncü dilli İsmim Entity Challenge tərzini təsdiqləyir. Görevlərin adlı nömrələri Web belələrində tanınmağı, adların normalizasyonu və çox dil bağlaması barəsindədir. Challenge altı dil və beş entity türünü örtür, və 8. Balto-Slaviq Təbiətli Dil Prozesiyası İş Hopunun bir parças ı olaraq, EACL 2021 konferensiyası ilə birlikdə yerləşdirilmişdir. On takım yarışmağa katıldı. Adlı entitə tanıması işi 90% F ölçüsünə çatdı, Challenge ilk edityonundan daha yüksək. Yedi dəstə bütün altı dillərə daxil olmuş, beş dəstə də çoxlu dil birlikdə işləri birlikdə iştirak etmişdilər. Bölüşdürülən qiymətlər məlumatı paylaşılan işlər web sayfasında faydalanır.', 'ca': "Aquest article descriu l'esclav-NER: el tercer repte d'entitats anomenades multilingües en llengües esclaves. Les tasques consisteixen en reconèixer mencions d'entitats anomenades en documents web, normalitzar els noms i enllaçar-se entre llengües. El repte abarca sis llengües i cinc tipus d'entitats, i està organitzat com part de la 8ª ateliera de processament de llengües naturals baltoslàvics, col·locada amb la conferència EACL 2021. 10 equips van participar a la competició. El rendiment de la tasca de reconeixement de l'entitat anomenada va arribar al 90% a mesura F, molt més alta que la que es va reportar a la primera edició del repte. Sete equips van cobrir totes les sis llengües, i cinc equips van participar en la tasca interlingüística d'unes entitats. En la pàgina web compartida hi ha informació detallada sobre la valoració.", 'cs': 'Tento článek popisuje Slav-NER: 3rd Multilingual Named Entity Challenge ve slovanských jazycích. Úkoly zahrnují rozpoznávání zmínek pojmenovaných entit ve webových dokumentech, normalizaci názvů a křížové propojení. Soutěž se týká šesti jazyků a pěti typů entit a je organizována jako součást osmého workshopu zpracování baltslovanského přírodního jazyka, který se spolupořádá s konferencí EACL 2021. Soutěže se zúčastnilo deset týmů. Výkon pro pojmenovanou úlohu rozpoznávání entity dosáhl 90% F-měřítka, mnohem vyšší, než bylo uvedeno v prvním vydání výzvy. Sedm týmů pokrývalo všech šest jazyků a pět týmů se podílelo na úkolu propojení entit mezi jazyky. Podrobné informace o ocenění jsou k dispozici na webové stránce sdíleného úkolu.', 'bs': 'Ovaj papir opisuje Slav-NER: treći izazov višejezičkog imenovanog entiteta na slavičkim jezicima. Ovi zadaci uključuju priznanje spomena imenovanih entiteta u web dokumentima, normalizaciju imena i međujezičkog povezanja. Izazov pokriva šest jezika i pet tipa entiteta, te je organizovan kao dio 8. Balto-Slavic Natural Language Processing Workshop, zajedno sa EACL 2021. konferencijom. Deset tima su sudjelovali u takmičenju. Proizvodnja zadataka priznavanja imena entiteta postigla je 90% F-mjere, mnogo više nego što je prijavljena u prvom izdanju izazova. Sedam tima pokrivalo je sve šest jezika, a pet tima sudjelovalo u međujezičkom entitetu povezanom zadatku. Detaljne informacije o ocjeni su dostupne na zajedničkoj web stranici zadataka.', 'fi': 'Tässä artikkelissa kuvataan Slav-NER: kolmas monikielinen nimetty entiteetti haaste slaavilaisilla kielillä. Tehtäviin kuuluu nimettyjen entiteettien tunnistaminen WWW-dokumenteissa, nimien normalisointi ja monikielinen linkitys. Haaste kattaa kuusi kieltä ja viisi entiteettityyppiä, ja se järjestetään osana 8. baltoslavian luonnonkielen käsittelytyöpajaa, joka sijaitsee yhdessä EACL 2021 -konferenssin kanssa. Kilpailuun osallistui kymmenen joukkuetta. Nimetyn entiteettitunnistustehtävän suorituskyky saavutti 90% F-mittarin, mikä on paljon korkeampi kuin Challengen ensimmäisessä painoksessa ilmoitettiin. Seitsemän tiimiä kattoi kaikki kuusi kieltä ja viisi tiimiä osallistui monikieliseen kokonaisuuksien linkitystehtävään. Yksityiskohtaiset arvostustiedot löytyvät jaetun tehtävän verkkosivulta.', 'et': 'Käesolevas artiklis kirjeldatakse Slav-NER-i kolmandat mitmekeelset nimelise olemuse väljakutset slaavi keeltes. Ülesanded hõlmavad nimeliste üksuste märkimise tuvastamist veebidokumentides, nimede normaliseerimist ja keeleülest linkimist. Väljakutse hõlmab kuut keelt ja viit olenditüüpi ning seda korraldatakse osana 8. balto-slaavi looduskeele töötlemise seminarist, mis asub koos EACL 2021 konverentsiga. Võistlusel osales kümme meeskonda. Nimetatud üksuse tunnustamise ülesande tulemuslikkus ulatus 90% F-mõõtmeni, mis on palju suurem kui väljakutse esimeses väljaandes. Seitse meeskonda hõlmasid kõiki kuut keelt ja viis meeskonda osales keeleülese üksuse sidumise ülesandes. Üksikasjalik hindamisteave on saadaval jagatud ülesande veebilehel.', 'jv': "Slav-NeR Awak dhéwé éntuk akèh nggawé éntukno baléné karo pakem nganggo web dokumen, Normal kuwi jeneng, lan basa-seneng pisan bangsane. Ngulang Sampeyan sing wis rampun ning patimbang. Menu item to Open 'Search for Open Files' dialog Jute-Jute sing nganggo akeh basa ning sesih, lan lima taman minggu nggawe bagian podho karo perusahaan-ingkang sampeyan. Detays Validity", 'he': 'העיתון הזה מתאר Slav-NER: האתגר של איכות בשם רבות שלישי בשפות סלאביות. המשימות כוללות זיהוי זכרות של ישויות בשמות במסמכים האינטרנטים, נורמליזציה של השמות, וקשר בין שפתיים. האתגר מכסה שש שפות וחמש סוגי יחידות, והוא מאורגן כחלק מהעבדת השפה הטבעית בלטו-סלאבית השמינית, שמוקמת יחד עם איגוד EACL 2021. עשר קבוצות השתתפו בתחרות. ההופעה של משימת זיהוי היחידות בשם הגיעה למידה F של 90%, הרבה יותר גבוהה ממה שדווח בהוצאה הראשונה של האתגר. שבע קבוצות כיסו את כל ששת השפות, וחמישה קבוצות השתתפו במשימת הקשר בין יחידת השפות. Detailed valuation information is available on the shared task web page.', 'ha': "@ item license (short name) Kayan aiki na ƙunsa da su gane sunan abubuwa cikin takardun aiki na Web, mai normal sunayen, da kuma linki na tsakanin-harshen. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference.  Team gõma sun yi shirin ta. Tsarin wa aikin sunan shaidar abun ya kai 90% F-ƙaddara, mafi girma daga da aka yi furuci da shi a farkon editin Challenge. Ga jama'a bakwai sun rufe duk harshen sita, kuma jama'a shan sun yi shirin cikin aikin shaidar da ke haɗa. Ana iya da information na kimar da aka ƙayyade kan bangon aiki da aka raba shi.", 'sk': 'V prispevku je opisan Slav-NER: 3. večjezični izziv imenovanih entitet v slovanskih jezikih. Opravila vključujejo prepoznavanje omenjenih entitet v spletnih dokumentih, normalizacijo imen in medjezično povezovanje. Izziv pokriva šest jezikov in pet vrst entitet, organiziran pa je v okviru 8. baltoslovanske delavnice obdelave naravnega jezika, ki se nahaja skupaj s konferenco EACL 2021. Na tekmovanju je sodelovalo deset ekip. Uspešnost naloge prepoznavanja imenovanega subjekta je dosegla 90% F-merila, kar je precej višja od poročanja v prvi izdaji izziva. Sedem ekip je pokrilo vseh šest jezikov, pet ekip pa je sodelovalo pri nalogi povezovanja medjezičnih entitet. Podrobne informacije o vrednotenju so na voljo na spletni strani opravila v skupni rabi.', 'bo': 'ཤོག་བྱང་འདིས་Slav-NER：སྐད་རིགས་ཀྱི་མིང་བཏགས་ཡོད་པའི་ཆ་འཕྲིན་གྱི་སྒེར་གྱི་ནང་དུ་བཏོན་གཏོང་། The tasks involve recognizing mentions of named entities in Web documents, normalization of the names, and cross-lingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. ཚོ་ཁག་བཅུ་ཐམ་གྱི་ཕྱོགས་སྐོར་གྱི་ནང་དུ་བྱ་སྤྱོད་བྱས་པ་ཡིན། མིང་ཡོད་པའི་ཞུགས་འབྲེལ་གྱི་ལས་འགན་སྒེར་གྱི་འཛུལ་སྤྱོད་ཀྱི་ལས་འགན་ལྡན་སྒོ་ཕྱོགས་དང་པོ་ཞིག་གི་རྐྱེན་སྐྱེས་པ་ད Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. རིམ་ཐང་གསལ་བཤད་བྱས་པའི་གནས་ཚུལ་མང་ཙམ་སྤྱོད་པའི་བྱ་འགུལ་གྱི་ཤོག་བྱང་ཐོག་ཡོད་པ'}
