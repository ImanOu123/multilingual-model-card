{'en': 'Learning to Generate Multiple Style Transfer Outputs for an Input Sentence', 'ar': 'تعلم إنشاء مخرجات نقل نمط متعددة لجملة إدخال', 'es': 'Aprender a generar múltiples salidas de transferencia de estilo para una oración de entrada', 'fr': "Apprendre à générer plusieurs sorties de transfert de style pour une phrase d'entrée", 'pt': 'Aprendendo a gerar saídas de transferência de vários estilos para uma frase de entrada', 'ja': '入力文の複数のスタイル転送出力を生成する方法を学ぶ', 'zh': '学为输句生多样者传输输', 'ru': 'Обучение генерации нескольких выходов передачи стиля для входного предложения', 'hi': 'एक इनपुट वाक्य के लिए एकाधिक शैली स्थानांतरण आउटपुट जनरेट करने के लिए सीखना', 'ga': 'Foghlaim conas Aschur Aistrithe Ilstíl a Ghiniúint le haghaidh Pianbhreithe Ionchuir', 'ka': 'მეტი სტილის გადატანსტის გამოსახულების შექმნა', 'hu': 'Több stílusátviteli kimenet generálásának tanulása beviteli mondathoz', 'el': 'Μάθετε να δημιουργείτε πολλαπλές εξόδους μεταφοράς στυλ για μια πρόταση εισόδου', 'it': 'Imparare a generare più uscite di trasferimento di stile per una frase di input', 'mk': 'Научи да генерира повеќе излези за трансфер на стил за влезна реченица', 'kk': 'Кіріс сөз үшін бірнеше стилді тасымалдау шығысын құру', 'ms': 'Belajar untuk Jana Output Pemindahan Gaya Berbilang untuk Hukuman Input', 'ml': 'ഒരു ഇന്\u200dപുട്ട് ശിക്ഷയ്ക്കായി പല ശൈലി മാറ്റുന്നതിനുള്ള ഫലങ്ങള്\u200d ഉണ്ടാക്കാന്\u200d പഠിക്കുന്നു', 'lt': 'Mokymasis generuoti kelių stilių perkėlimo rezultatus įvedimo sakinyje', 'mt': 'Learning to Generate Multiple Style Transfer Outputs for an Input Sentence', 'mn': 'Ингээд өгүүлбэрийн олон хэлбэрийн дамжуулалт үүсгэх суралцах', 'no': 'Læring for å laga fleire stiloverføringar for ein innsetsetning', 'pl': 'Nauka generowania wielu wyjść transferu stylów dla zdania wejściowego', 'ro': 'Învățarea de a genera mai multe ieșiri de transfer de stil pentru o propoziție de intrare', 'sr': 'Naučenje da proizvede izveštaj više stila za rečenicu za ulaz', 'si': 'ඇතුළු වාර්තාවක් වාර්තාවක් නිර්මාණය කරන්න ඉගෙනගන්න', 'so': 'Learning to Generate Multiple Style Transfer Outputs for an Input Sentence', 'ta': 'உள்ளீட்டு வாக்கியத்திற்கான பல பாணி மாற்றும் வெளியீடுகளை உருவாக்க கற்றுக்கொள்ளும்', 'sv': 'L채r dig att generera flera format철verf철ringsutg책ngar f철r en inmatningsmening', 'ur': 'ایک اینپیٹ سنٹنس کے لئے بہت سی استیل ترنسفور آئٹپوٹ پیدا کرنے کے لئے سیکھ رہی ہے', 'uz': 'Name', 'vi': 'Học để tạo ra ảnh truyền tải nhiều kiểu cho một câu nhập', 'da': 'Lære at generere flere typografioverførselsudgange for en input sætning', 'nl': 'Leren om meerdere stijl overdracht outputs te genereren voor een invoerzin', 'hr': 'Naučenje stvaranja izvješća prijenosa višestrukog stila za uvod kazne', 'id': 'Belajar untuk menghasilkan Output Transfer Gaya Berbilang untuk Sentensi Input', 'bg': 'Научете се да генерирате множество стилови трансферни изходи за входно изречение', 'fa': 'یاد گرفتن برای تولید انتقال متعدد استیل برای یک کلمه وارد', 'sw': 'Kujifunza kutengeneza matokeo ya Mitandao mingi ya Uhamiaji kwa ajili ya Hukumu ya Kuingiza', 'de': 'Lernen, mehrere Style Transfer Outputs für einen Eingabesatz zu generieren', 'ko': '입력 문장을 위한 여러 스타일 변환 출력 학습', 'af': "Leer na Genereer Veelvuldige Styl Oordrag Uitset vir' n Invoer Uitset", 'sq': 'Mësimi për të gjeneruar daljet e transferimit të stilit të shumtë për një dënim të hyrjes', 'hy': 'Սովորել ստեղծել բազմաթիվ ոճի փոխանցման արտադրանքներ մուտքային նախադասության համար', 'am': 'Multiple Style Transfer Output for an Input Sentence', 'tr': 'Girdi sözleri üçin Birden Stil Transfer Çikgini Bejer öwrenmek', 'bn': 'ইনপুটের শাস্তির জন্য অনেক ধরনের স্টাইল পরিবর্তন আউটপুট তৈরি করতে শিখা হচ্ছে', 'az': 'Girdi S칬z칲 칲칞칲n 칞oxlu Styl Transfer 칂캼qt캼lar캼n캼 Olmaq 칬yr톛nm톛k', 'bs': 'Naučenje stvaranja izveštaja prijenosa više stila za uvod kazne', 'ca': "Aprendre a generar productes de transferència d'estils múltiples per a una frase d'entrada", 'cs': 'Naučit se generovat více stylových přenosových výstupů pro vstupní větu', 'fi': 'Oppiminen luomaan useita tyylisiirtolähtöjä syöttölauseelle', 'et': 'Sisendlause mitme stiili ülekande väljundite genereerimise õppimine', 'jv': 'FindOK', 'ha': 'KCharselect unicode block name', 'sk': 'Učenje ustvarjanja več slogovnih prenosov za vhodni stavek', 'he': 'ללמוד ליצור יציאות מעברת סגנון רבות למשפט כניסה', 'bo': 'ནང་འཇུག་ཚིག་མང་བོའི་བཟོ་རྣམ་གནས་སོར་མང་བོའི་གནས་ཚུལ་གསར་བསྐྲུན་འབད་ནི་ཤེས་འཇུག་བྱེད་པ'}
{'en': 'Text style transfer refers to the task of rephrasing a given text in a different style. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a  delta distribution , and thus their  models  can not generate different style transfer results for a given input text. To address the limitation, we propose a one-to-many text style transfer framework. In contrast to prior works that learn a  one-to-one mapping  that converts an input sentence to one output sentence, our approach learns a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content. This is achieved by applying  adversarial training  with a latent decomposition scheme. Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content. We then combine the  content code  with the  style code  for generating a style transfer output. By combining the same  content code  with a different  style code , we generate a different style transfer output. Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach.', 'pt': 'A transferência de estilo de texto refere-se à tarefa de reformular um determinado texto em um estilo diferente. Embora vários métodos tenham sido propostos para avançar no estado da arte, eles geralmente assumem que a saída de transferência segue uma distribuição delta e, portanto, seus modelos não podem gerar resultados de transferência de estilo diferentes para um determinado texto de entrada. Para resolver a limitação, propomos uma estrutura de transferência de estilo de texto um-para-muitos. Em contraste com trabalhos anteriores que aprendem um mapeamento um para um que converte uma sentença de entrada em uma sentença de saída, nossa abordagem aprende um mapeamento um para muitos que pode converter uma sentença de entrada em várias sentenças de saída diferentes, preservando a entrada contente. Isto é conseguido através da aplicação de treinamento adversário com um esquema de decomposição latente. Especificamente, decompomos a representação latente da sentença de entrada em um código de estilo que captura a variação do estilo da linguagem e um código de conteúdo que codifica o conteúdo independente do estilo da linguagem. Em seguida, combinamos o código de conteúdo com o código de estilo para gerar uma saída de transferência de estilo. Ao combinar o mesmo código de conteúdo com um código de estilo diferente, geramos uma saída de transferência de estilo diferente. Amplos resultados experimentais com comparações com várias abordagens de transferência de estilo de texto em vários conjuntos de dados públicos usando um conjunto diversificado de métricas de desempenho validam a eficácia da abordagem proposta.', 'ar': 'يشير نقل نمط النص إلى مهمة إعادة صياغة نص معين بأسلوب مختلف. في حين تم اقتراح طرق مختلفة لتطوير حالة الفن ، فإنها غالبًا ما تفترض أن إخراج النقل يتبع توزيع دلتا ، وبالتالي لا يمكن لنماذجها إنشاء نتائج نقل نمط مختلفة لنص إدخال معين. لمعالجة القيد ، نقترح إطار عمل لنقل نمط نص واحد إلى متعدد. على عكس الأعمال السابقة التي تتعلم تعيين واحد لواحد يحول جملة إدخال إلى جملة إخراج واحدة ، يتعلم نهجنا تعيين واحد إلى متعدد يمكنه تحويل جملة إدخال إلى عدة جمل مخرجات مختلفة ، مع الحفاظ على الإدخال المحتوى. يتم تحقيق ذلك من خلال تطبيق التدريب العدائي مع مخطط التحلل الكامن. على وجه التحديد ، نقوم بتحليل التمثيل الكامن لجملة الإدخال إلى رمز نمط يلتقط تنوع نمط اللغة وكود المحتوى الذي يشفر المحتوى المستقل عن نمط اللغة. نقوم بعد ذلك بدمج كود المحتوى مع كود النمط لتوليد إخراج نقل النمط. من خلال دمج نفس كود المحتوى مع كود نمط مختلف ، نقوم بإنشاء إخراج نقل نمط مختلف. نتائج تجريبية واسعة النطاق مع مقارنات للعديد من مناهج نقل نمط النص على مجموعات بيانات عامة متعددة باستخدام مجموعة متنوعة من مقاييس الأداء للتحقق من فعالية النهج المقترح.', 'es': 'La transferencia de estilo de texto se refiere a la tarea de reformular un texto determinado en un estilo diferente. Si bien se han propuesto varios métodos para avanzar en el estado de la técnica, a menudo suponen que la salida de la transferencia sigue una distribución delta y, por lo tanto, sus modelos no pueden generar resultados de transferencia de estilos diferentes para un texto de entrada dado. Para abordar esta limitación, proponemos un marco de transferencia de estilos de texto de uno a varios. A diferencia de los trabajos anteriores que aprenden un mapeo uno a uno que convierte una oración de entrada en una oración de salida, nuestro enfoque aprende un mapeo de uno a muchos que puede convertir una oración de entrada en varias oraciones de salida diferentes, al tiempo que preserva el contenido de entrada. Esto se logra mediante la aplicación de un entrenamiento contradictorio con un esquema de descomposición latente. Específicamente, descomponemos la representación latente de la oración introducida en un código de estilo que capture la variación del estilo del idioma y un código de contenido que codifique el contenido independiente del estilo del idioma. A continuación, combinamos el código de contenido con el código de estilo para generar una salida de transferencia de estilo. Al combinar el mismo código de contenido con un código de estilo diferente, generamos una salida de transferencia de estilo diferente. Los amplios resultados experimentales con comparaciones con varios enfoques de transferencia de estilos de texto en múltiples conjuntos de datos públicos que utilizan un conjunto diverso de métricas de rendimiento validan la eficacia del enfoque propuesto.', 'fr': "Le transfert de style de texte consiste à reformuler un texte donné dans un style différent. Bien que diverses méthodes aient été proposées pour faire progresser l'état de la technique, elles supposent souvent que la sortie de transfert suit une distribution delta, et leurs modèles ne peuvent donc pas générer de résultats de transfert de style différents pour un texte d'entrée donné. Pour remédier à cette limitation, nous proposons un cadre de transfert de style de texte un-à-plusieurs. Contrairement aux travaux antérieurs qui apprennent un mappage un-à-un qui convertit une phrase d'entrée en une phrase de sortie, notre approche apprend un mappage un-à-plusieurs qui peut convertir une phrase d'entrée en plusieurs phrases de sortie différentes, tout en préservant le contenu d'entrée. Ceci est réalisé en appliquant un entraînement contradictoire avec un schéma de décomposition latent. Plus précisément, nous décomposons la représentation latente de la phrase d'entrée en un code de style qui capture la variation de style de langue et un code de contenu qui code le contenu indépendant du style de langue. Nous combinons ensuite le code de contenu avec le code de style pour générer une sortie de transfert de style. En combinant le même code de contenu avec un code de style différent, nous générons une sortie de transfert de style différente. Des résultats expérimentaux approfondis avec des comparaisons avec plusieurs approches de transfert de style de texte sur plusieurs ensembles de données publics utilisant un ensemble diversifié de mesures de performance valident l'efficacité de l'approche proposée.", 'hi': 'पाठ्य शैली अंतरण किसी दिए गए पाठ को एक अलग शैली में पुन: प्रस्तुत करने के कार्य को निर्दिष्ट करता है | जबकि कला की स्थिति को आगे बढ़ाने के लिए विभिन्न तरीकों का प्रस्ताव किया गया है, वे अक्सर मानते हैं कि स्थानांतरण आउटपुट डेल्टा वितरण का अनुसरण करता है, और इस प्रकार उनके मॉडल किसी दिए गए इनपुट पाठ के लिए अलग-अलग शैली हस्तांतरण परिणाम उत्पन्न नहीं कर सकते हैं। सीमा को संबोधित करने के लिए, हम एक-से-कई पाठ शैली स्थानांतरण ढांचे का प्रस्ताव करते हैं। पहले के कार्यों के विपरीत जो एक-से-एक मैपिंग सीखते हैं जो एक इनपुट वाक्य को एक आउटपुट वाक्य में परिवर्तित करता है, हमारा दृष्टिकोण एक-से-कई मैपिंग सीखता है जो इनपुट सामग्री को संरक्षित करते हुए एक इनपुट वाक्य को कई अलग-अलग आउटपुट वाक्यों में परिवर्तित कर सकता है। यह एक अव्यक्त अपघटन योजना के साथ प्रतिकूल प्रशिक्षण को लागू करके प्राप्त किया जाता है। विशेष रूप से, हम इनपुट वाक्य के अव्यक्त प्रतिनिधित्व को एक शैली कोड में विघटित करते हैं जो भाषा शैली भिन्नता और एक सामग्री कोड को कैप्चर करता है जो भाषा शैली-स्वतंत्र सामग्री को एन्कोड करता है। फिर हम एक शैली हस्तांतरण आउटपुट उत्पन्न करने के लिए शैली कोड के साथ सामग्री कोड को संयोजित करते हैं। एक अलग शैली कोड के साथ एक ही सामग्री कोड के संयोजन से, हम एक अलग शैली हस्तांतरण आउटपुट उत्पन्न करते हैं। प्रदर्शन मैट्रिक्स के एक विविध सेट का उपयोग करके कई सार्वजनिक डेटासेट पर कई पाठ शैली हस्तांतरण दृष्टिकोणों की तुलना के साथ व्यापक प्रयोगात्मक परिणाम प्रस्तावित दृष्टिकोण की प्रभावशीलता को मान्य करते हैं।', 'zh': '文本式传输指以不同式改定文本。 虽已建百术以进先进之术,然常设传输输循delta布,故其模形不得为给定输文本生异式传输。 为解此限,作双文本式传输框架。 与前学将输入句转换为一个输出句子的一对一映射,我们的方法学了一对多映射,可将输入句子转换为多不同的输句,同时留输入。 此所以潜于分解者也。 具体来说,将输句之潜解为获言之式变体代码与编码语式无关者代码。 然后合代码合式代码以成式传输输。 因将同义代码与式代码合,吾生为异式传输输。 用群性能指标,博实验与数公集上文本传输校之,验其有效性。', 'ja': 'テキストスタイルの転送とは、指定されたテキストを別のスタイルで言い換える作業を指します。 先端技術を進歩させるための様々な方法が提案されているが、それらは多くの場合、転送出力がデルタ分布に従うと仮定しているため、それらのモデルは、所与の入力テキストに対して異なるスタイル転送結果を生成することができない。 この制限に対処するために、1対多のテキストスタイル転送フレームワークを提案します。 入力文を1つの出力文に変換する1対1のマッピングを学習する先行研究とは対照的に、我々のアプローチは、入力内容を保持しながら、入力文を複数の異なる出力文に変換できる1対多のマッピングを学習する。 これは、潜在的な分解スキームを用いた対抗訓練を適用することによって達成される。 具体的には、入力文の潜在表現を、言語スタイルのバリエーションを取り込むスタイルコードと、言語スタイルに依存しないコンテンツを符号化するコンテンツコードに分解する。 次に、コンテンツコードとスタイルコードを組み合わせて、スタイル転送出力を生成します。 同じコンテンツコードと異なるスタイルコードを組み合わせることで、異なるスタイル転送出力を生成します。 多様なパフォーマンスメトリクスのセットを使用して、複数のパブリックデータセット上のいくつかのテキストスタイル転送アプローチと比較した広範な実験結果は、提案されたアプローチの有効性を検証します。', 'ru': 'Передача стиля текста относится к задаче перефразирования данного текста в другом стиле. Хотя были предложены различные способы для продвижения уровня техники, они часто предполагают, что выход передачи следует распределению дельта, и, следовательно, их модели не могут генерировать различные результаты передачи стиля для данного входного текста. Чтобы устранить это ограничение, мы предлагаем фреймворк переноса текста от одного ко многим. В отличие от предыдущих работ, которые изучают однозначное отображение, которое преобразует входное предложение в одно выходное предложение, наш подход изучает однозначное отображение, которое может преобразовать входное предложение в несколько различных выходных предложений, сохраняя при этом входное содержимое. Это достигается путем применения состязательной тренировки с латентной схемой разложения. В частности, мы декомпозируем скрытое представление входного предложения на код стиля, который фиксирует вариацию стиля языка, и код контента, который кодирует содержимое, не зависящее от стиля языка. Затем мы объединяем код контента с кодом стиля для генерации вывода передачи стиля. Объединяя один и тот же код контента с другим кодом стиля, мы генерируем другой выход передачи стиля. Обширные экспериментальные результаты со сравнением с несколькими подходами к переносу стиля текста на нескольких публичных наборах данных с использованием разнообразного набора показателей эффективности подтверждают эффективность предлагаемого подхода.', 'ga': 'Tagraíonn aistriú stíl téacs don tasc a bhaineann le téacs áirithe a athfhrású i stíl dhifriúil. Cé go bhfuil modhanna éagsúla molta chun an úrscothacht a chur chun cinn, glacann siad leis go minic go leanann an t-aschur aistrithe dáileadh deilt, agus mar sin ní féidir lena samhlacha torthaí aistrithe stíleanna éagsúla a ghiniúint do théacs ionchuir ar leith. Chun aghaidh a thabhairt ar an teorannú, molaimid creat aistrithe stíl téacs aon-ar-go leor. I gcodarsnacht le saothair roimhe seo a fhoghlaimíonn léarscáiliú duine-le-duine a thiontaíonn abairt ionchuir go habairt aschuir amháin, foghlaimíonn ár gcur chuige léarscáiliú aon-ar-go leor a fhéadfaidh abairt ionchuir a thiontú go il-abairtí aschuir éagsúla, agus an t-ionchur á chaomhnú. ábhar. Baintear é seo amach trí oiliúint sáraíochta a chur i bhfeidhm le scéim um dhianscaoileadh folaigh. Go sonrach, díchumaimid léiriú folaigh na habairte ionchuir chuig cód stíle a ghlacann an éagsúlacht stíle teanga agus cód ábhair a ionchódaíonn an t-ábhar atá neamhspleách ar stíl teanga. Ansin cuirimid an cód ábhair le chéile leis an gcód stíle chun aschur aistrithe stíle a ghiniúint. Tríd an gcód ábhar céanna a chomhcheangal le cód stíl dhifriúil, gineann muid aschur aistrithe stíl éagsúil. Déanann torthaí turgnamhacha fairsinge mar aon le comparáidí le go leor cineálacha cur chuige aistrithe stíl téacs ar il-thacair sonraí poiblí ag baint úsáide as sraith éagsúil de mhéadracht feidhmíochta bailíochtú ar éifeachtacht an chur chuige atá beartaithe.', 'hu': 'A szövegstílusátvitel egy adott szöveg más stílusban történő átfogalmazásának feladata. Bár különböző módszereket javasoltak a technika állapotának előmozdítására, gyakran feltételezik, hogy az átviteli kimenet delta eloszlást követ, így modelleik nem tudnak eltérő stílusátviteli eredményeket generálni egy adott bemeneti szöveghez. A korlátozás kezelése érdekében egy szövegstílus átviteli keretrendszert javasolunk. A korábbi munkákkal ellentétben, amelyek egy bemeneti mondatot egy kimeneti mondattá alakítanak át, a megközelítésünk egy egy-a-sokhoz kapcsolódó leképezést tanul meg, amely egy bemeneti mondatot több különböző kimeneti mondattá alakíthatja át, miközben megőrzi a bemeneti tartalmat. Ezt úgy érhetjük el, hogy ellenséges képzést alkalmazunk egy látens bomlási sémával. Konkrétan a bemeneti mondat látens ábrázolását egy stíluskóddal bontjuk le, amely rögzíti a nyelvstílus variációját, és egy tartalomkóddal, amely kódolja a nyelvstílusfüggetlen tartalmat. Ezután kombináljuk a tartalomkódot a stíluskóddal a stílusátviteli kimenet létrehozásához. Ugyanazt a tartalomkódot egy másik stíluskóddal kombinálva más stílusátviteli kimenetet generálunk. A különböző teljesítménymutatók segítségével több nyilvános adatkészleten több szövegstílusátviteli megközelítéssel való összehasonlítással végzett kiterjedt kísérleti eredmények igazolják a javasolt megközelítés hatékonyságát.', 'ka': 'ტექსტის სტილის გადატვირთვა განსხვავებული სტილის ტექსტის გადატვირთვას. მაგრამ განსხვავებული მეტოვები იყო, რომ ხელოვნების სტატის სტატის წინასწარმოდგინოთ, ისინი უფრო მეტი გადავიწყებენ, რომ ტრანსტრინტის გამოსახულების შემდეგ დელტა გაყოფილი, და ამიტო დასაწყებლად ტექსტის სტილის გადატანსტის ფრამეტრის მივეღებთ. პირველი სამუშაოდ, რომელიც ერთი-ერთი სამუშაო შესწავლა, რომელიც ერთი გამოყენებული სამუშაო წესების შეცვლა, ჩვენი წესები ერთი-ერთი სამუშაო შესწავლა, რომელიც შეუძლია გადატანა წესების შემდეგ გან ეს მიიღება, რომელიც განსაკუთრებული სქემებით განაკუთრებულია. განსაკუთრებულია, ჩვენ ჩვენ გავაკეთებთ შეტყობინებული სიტყვას სტილის ფორმაციას, რომელიც სახელის სტილის განცვლების და შემდგომარების კოდის, რომელიც ენის სტილის განსაკუთრებული შე შემდეგ ჩვენ სტილის გატანსტრიქციის გამოყენება სტილის კოდის შესაძლებელად შემყვებით. განსხვავებული სტილის კოდის ერთადერთი შემდგომარების კოდის შეერთებით, ჩვენ განსხვავებული სტილის გადატანსტის გამოყენება. განსაზღვრებული ექსპერიმენტიური შედეგი, რამდენიმე ტექსტური სტილის გადატანსტის გადასვლებაზე, რამდენიმე ადამიანის მონაცემების განსაზღვრებული მონაცემების განსაზღვრებული', 'el': 'Η μεταφορά στυλ κειμένου αναφέρεται στην εργασία της αναδιάταξης ενός συγκεκριμένου κειμένου σε διαφορετικό στυλ. Ενώ έχουν προταθεί διάφορες μέθοδοι για να προωθήσουν την κατάσταση της τεχνολογίας, συχνά υποθέτουν ότι η παραγωγή μεταφοράς ακολουθεί κατανομή δέλτα, και επομένως τα μοντέλα τους δεν μπορούν να δημιουργήσουν διαφορετικά αποτελέσματα μεταφοράς στυλ για ένα δεδομένο κείμενο εισαγωγής. Για να αντιμετωπιστεί ο περιορισμός, προτείνουμε ένα πλαίσιο μεταφοράς στυλ κειμένου ένα προς πολλά. Σε αντίθεση με προηγούμενες εργασίες που μαθαίνουν μια αντιστοίχιση ένα προς ένα που μετατρέπει μια πρόταση εισόδου σε μια πρόταση εξόδου, η προσέγγισή μας μαθαίνει μια αντιστοίχιση ένα προς πολλά που μπορεί να μετατρέψει μια πρόταση εισόδου σε πολλές διαφορετικές προτάσεις εξόδου, διατηρώντας παράλληλα το περιεχόμενο εισόδου. Αυτό επιτυγχάνεται με την εφαρμογή αντιτιθέμενης εκπαίδευσης με ένα λανθάνον σχέδιο αποσύνθεσης. Συγκεκριμένα, αποσυντίθεται η λανθάνουσα αναπαράσταση της πρότασης εισόδου σε έναν κώδικα στυλ που συλλαμβάνει την παραλλαγή στυλ γλώσσας και έναν κώδικα περιεχομένου που κωδικοποιεί το περιεχόμενο που εξαρτάται από το στυλ γλώσσας. Στη συνέχεια συνδυάζουμε τον κώδικα περιεχομένου με τον κωδικό στυλ για τη δημιουργία μιας εξόδου μεταφοράς στυλ. Συνδυάζοντας τον ίδιο κώδικα περιεχομένου με διαφορετικό κώδικα στυλ, δημιουργούμε μια διαφορετική έξοδο μεταφοράς στυλ. Εκτεταμένα πειραματικά αποτελέσματα με συγκρίσεις με διάφορες προσεγγίσεις μεταφοράς στυλ κειμένου σε πολλαπλά δημόσια σύνολα δεδομένων χρησιμοποιώντας ένα διαφορετικό σύνολο μετρήσεων απόδοσης επικυρώνουν την αποτελεσματικότητα της προτεινόμενης προσέγγισης.', 'it': "Il trasferimento di stile di testo si riferisce al compito di riformulare un dato testo in uno stile diverso. Mentre diversi metodi sono stati proposti per avanzare lo stato dell'arte, spesso assumono che l'output di trasferimento segua una distribuzione delta, e quindi i loro modelli non possono generare risultati di trasferimento di stile diversi per un dato testo di input. Per affrontare la limitazione, proponiamo un framework di trasferimento di stile di testo one-to-many. A differenza dei lavori precedenti che imparano una mappatura one-to-one che converte una frase di input in una frase di output, il nostro approccio impara una mappatura one-to-many che può convertire una frase di input in più frasi di output diverse, preservando il contenuto di input. Ciò si ottiene applicando l'addestramento avversario con uno schema di decomposizione latente. Nello specifico, scomposiamo la rappresentazione latente della frase di input in un codice stile che cattura la variazione dello stile della lingua e un codice contenuto che codifica il contenuto indipendente dallo stile della lingua. Combiniamo quindi il codice contenuto con il codice stile per generare un output di trasferimento stile. Combinando lo stesso codice di contenuto con un codice di stile diverso, generiamo un output di trasferimento di stile diverso. Ampi risultati sperimentali con confronti con diversi approcci di trasferimento di stili di testo su più set di dati pubblici utilizzando un insieme diversificato di metriche di performance convalidano l'efficacia dell'approccio proposto.", 'kk': 'Мәтін стилінің аударуы басқа стилінде келтірілген мәтінді қайта фраграциялау тапсырмасына сәйкес береді. Әртүрлі әдістер суреттің күйін өзгерту үшін ұсынылған болса, олар көбінде алмастыру шығысын делта үлестіріміне келеді деп ойлайды, сондықтан олардың үлгілері келтірілген мәтін үшін басқа стиль Шектерді шектеу үшін бір-бір мәтін стилінің аудару фреймісін таңдаймыз. Алдыңғы жұмыстың қарсы бір- бірінен бір сұлбаны бір шығыс сөйлемеге аударатын сұлбаны үйренетін жұмыстың қарсы бір- бірінен бір сұлбаға аударатын сұлбаны бірнеше түрлі шығыс сөйлемелеріне аударатын сұлбаларды бірнеше тү Бұл келесі бөліс сұлбамен қарсы оқыту үшін жеткізіледі. Ескерту үшін, біз тіл стилінің айнымалылығын, тіл стилінен тәуелді мазмұнын кодтамыз. Содан кейін стиль тасымалдау шығысын құру үшін мазмұның кодын стиль кодымен біріктіреміз. Бұл мазмұның кодын басқа стиль кодымен біріктіріп, біз басқа стиль тасымалдау шығысын құрамыз. Көпшілік деректер қорларында бірнеше мәтін стилінен салыстырылған эксперименталдық нәтижелері, көпшілік деректер қорларындағы көпшілікті метрикалық әрекеттердің әсерін тексеру әрекет', 'ml': 'പദാവലി സ്റ്റൈല്\u200d മാറ്റുന്നതു് വ്യത്യസ്ത രീതിയില്\u200d ഒരു പദാവലി പുതുക്കുന്നതിന്റെ ജോലി കാര്യത്തിന്റെ അവസ്ഥ മുന്\u200dകൂട്ടാന്\u200d വ്യത്യസ്ത രീതികള്\u200d പ്രൊദ്ദേശിച്ചിരിക്കുമ്പോള്\u200d, മാറ്റപ്പെടുത്തുന്നത് ഡെല്\u200dട്ടാ വിതരണം പിന്ന അതിരുകള്\u200d വിശദീകരിക്കാന്\u200d ഞങ്ങള്\u200d ഒരു ടെക്സ്റ്റ് സ്റ്റൈല്\u200d ട്രാന്\u200dസ്റ്റാര്\u200d ഫ്രെയിമ്പ് ചെയ്യും. ഒരു ഇന്\u200dപുട്ട് വാക്കിലേക്ക് ഒരു വാക്ക് മാറ്റുന്ന ഒരു മാപ്പിങ്ങ് പഠിക്കുന്ന പ്രവര്\u200dത്തനങ്ങള്\u200dക്ക് മുമ്പുള്ള പ്രവര്\u200dത്തനങ്ങള്\u200dക്ക് വിരോധമായി നമ്മുടെ സമ്മാനം  പ്രതിരോധ പരിശീലനം പ്രയോഗിക്കുന്നതിനാല്\u200d ഇത് നേടുന്നതാണ്. പ്രത്യേകിച്ച്, ഞങ്ങള്\u200d ഇന്\u200dപുട്ട് വാക്കിന്റെ അവസാനത്തെ പ്രദര്\u200dശിപ്പിക്കുന്നത് ഭാഷ ശൈലിയുടെ മാറ്റങ്ങളും ഭാഷ ശൈലിയുടെ സ്വാതന്ത്ര് പിന്നീട് സ്റ്റൈല്\u200d മാറ്റുന്നതിനുള്ള സ്റ്റൈല്\u200d കോഡുകള്\u200d സൃഷ്ടിക്കുന്നതിനുള്ള കൂട്ടിയിട ഒരു വ്യത്യസ്ത്രീയ കോഡുമായി ഒരേ ഉള്ളിലുള്ള കോഡ് കൂട്ടിചേര്\u200dക്കുന്നതിനാല്\u200d വ്യത്യസ്ത സ്റ്റൈല്\u200d  പ്രൊദ്ദേശിക്കപ്പെട്ട പ്രവര്\u200dത്തിപ്പിക്കുന്ന മെറ്റിക്കുകള്\u200d ഉപയോഗിച്ച് പല പൊതുവിന്റെ ഡാറ്റാസറ്റുകളിലേക്കുള്ള വിവിധ പര', 'lt': 'Teksto stiliaus perkėlimas reiškia užduotį pakeisti tam tikrą tekstą kitokiu stiliu. Nors buvo pasiūlyti skirtingi metodai pažangiajai pažangai pasiekti, jie dažnai daro prielaidą, kad perdavimo rezultatas vyksta po delta paskirstymo, todėl jų modeliai negali gauti skirtingų stilių perdavimo rezultatų tam tikram įvesties tekstui. Siekiant išspręsti šį apribojimą, siūlome vieno ar kelių teksto stilių perdavimo sistemą. Priešingai nei ankstesni darbai, kurie išmoko kartografiją vienam į vieną, kuri paverčia įvestą sakinį į vieną išėjimo sakinį, mūsų metodas išmoko kartografiją vienam į daugelį, kuri gali pakeisti įvestą sakinį į kelis skirtingus išėjimo sakinius, kartu išsaugodama įvestą turinį. Tai pasiekta taikant prieštaringą mokymą taikant latentinio skaidymo sistemą. Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content.  We then combine the content code with the style code for generating a style transfer output.  By combining the same content code with a different style code, we generate a different style transfer output.  Išsamūs eksperimentiniai rezultatai, palyginti su keliais kelių viešųjų duomenų rinkinių teksto būdo perdavimo metodais, naudojant įvairius veiklos rodiklius, patvirtina siūlomo metodo veiksmingumą.', 'mt': 'It-trasferiment tal-istil tat-test jirreferi għall-kompitu li jiġi mfassal mill-ġdid test partikolari fi stil differenti. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a delta distribution, and thus their models cannot generate different style transfer results for a given input text.  Biex nindirizzaw il-limitazzjoni, nipproponu qafas ta’ trasferiment ta’ stil ta’ test wieħed għal ħafna. In contrast to prior works that learn a one-to-one mapping that converts an input sentence to one output sentence, our approach learns a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content.  Dan jinkiseb bl-applikazzjoni ta’ taħriġ avversarju bi skema ta’ dekompożizzjoni moħbija. B’mod speċifiku, niddekomponu r-rappreżentazzjoni moħbija tas-sentenza ta’ input għal kodiċi ta’ stil li jaqbad il-varjazzjoni fl-istil tal-lingwa u kodiċi tal-kontenut li jikkodifika l-kontenut indipendenti mill-istil tal-lingwa. Imbagħad nikkombinaw il-kodiċi tal-kontenut mal-kodiċi tal-istil għall-ġenerazzjoni ta’ output ta’ trasferiment tal-istil. Billi nikkombinaw l-istess kodiċi tal-kontenut ma’ kodiċi ta’ stil differenti, niġġeneraw output ta’ trasferiment ta’ stil differenti. Riżultati esperimentali estensivi b’paraguni ma’ diversi approċċi ta’ trasferiment tal-istil tat-test fuq settijiet ta’ dejta pubbliċi multipli bl-użu ta’ sett varjat ta’ metriċi tal-prestazzjoni jivvalidaw l-effettività tal-approċċ propost.', 'mk': 'Пренесувањето на текстови стили се однесува на задачата за рефразирање на даден текст во различен стил. И покрај тоа што се предложени различни методи за унапредување на техничката состојба, тие честопати претпоставуваат дека излезот на трансферот следи делта дистрибуција, и така нивните модели не можат да генерираат различни резултати на трансферот на стил За да се реши ограничувањето, предлагаме рамка за трансфер на текст стил од еден до многу. За разлика од претходните дела кои научија мапирање од една до една кое ја претвора вводната реченица во една излезна реченица, нашиот пристап научи мапирање од една до многу кое може да ја претвори вводната реченица во повеќе различни излезни реченици, при што ја зачувува вводната содржина. Ова се постигнува со апликација на непријателска обука со тајна шема на декомпозиција. Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content.  Потоа го комбинираме кодот на содржината со кодот на стил за генерирање на излез на трансфер на стил. Со комбинација на истиот код на содржина со различен код на стил, генерираме различна излез на трансфер на стил. Експерименталните експериментални резултати со споредби со неколку пристапи за трансфер на текст на неколку јавни податоци со користење на различни метрики на перформанса ја потврдуваат ефикасноста на предложениот пристап.', 'ms': 'Pemindahan gaya teks rujuk ke tugas untuk mengubah frasa teks yang diberi dalam gaya yang berbeza. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a delta distribution, and thus their models cannot generate different style transfer results for a given input text.  Untuk mengatasi keterangan, kami melamar kerangka pemindahan gaya teks satu-ke-banyak. Sebaliknya dengan kerja sebelumnya yang belajar petakan satu-ke-satu yang menukar kalimat input ke satu kalimat output, pendekatan kita belajar petakan satu-ke-banyak yang boleh menukar kalimat input ke kalimat output berbeza berbilang, sementara menyimpan kandungan input. Ini dicapai dengan melaksanakan latihan lawan dengan skema pecahan tertutup. Secara khusus, kita pecahkan perwakilan tertutup bagi kalimat input ke kod gaya yang menangkap variasi gaya bahasa dan kod kandungan yang mengekod kandungan yang bebas gaya bahasa. Kemudian kita menggabungkan kod kandungan dengan kod gaya untuk menghasilkan output pemindahan gaya. Dengan menggabungkan kod kandungan yang sama dengan kod gaya yang berbeza, kami menghasilkan output pemindahan gaya yang berbeza. Keputusan eksperimen luas dengan perbandingan dengan beberapa pendekatan pemindahan gaya teks pada set data awam berbilang menggunakan set berbeza metrik prestasi sahkan efektiviti pendekatan yang diusulkan.', 'mn': 'Текст хэлбэрийн шилжүүлэлт өөр хэлбэрээр өгөгдсөн текст дахин шилжүүлэх үйлдэлтэй холбоотой. Хэдийгээр олон арга зам урлагийн байдлыг хөгжүүлэхэд санал өгсөн ч, тэд ихэвчлэн шилжүүлэлтийн үр дүнг делта хуваарилалтыг дагадаг гэж боддог. Тиймээс тэдний загварууд өгсөн өгөгдлийн текст дээр өөр хэлбэрийн шилж Хязгаарлалтын тухай бодохын тулд бид текст хэлбэрээс олон хэлбэрээр шилжүүлэх хэлбэрийг санал болно. Өмнөх ажлын эсрэг нэг-нэгээс нэг-нэгээс нэг өгүүлбэрийг нэг өгүүлбэрт шилжүүлдэг газрын зураг суралцаж, бидний арга нь нэг-нэгээс олон газрын зураг суралцаж, өгүүлбэрийг олон янз бүрийн өгүүлбэрт шилжүүлж чадна. Энэ нь эсрэг дасгал сургалтыг сүүлийн хуваагдах схемээр ашиглаж байна. Ялангуяа бид хэлний хэлбэрийн өөрчлөлтийг, хэлний хэлбэрээс хамааралгүй соёлыг кодлох хэлбэрийн үеийн дараагийн үзүүлэлтийг хэлбэрээр хувааж байна. Дараа нь бид хэлбэрийн шилжүүлэлтийн үр дүнг бүтээх хэлбэрийн кодыг нэгтгэнэ. Өөр хэлбэртэй адилхан соёлын кодыг нэгтгэхэд бид өөр хэлбэрийн шилжүүлэлтийн гаралтай бүтээж байна. Төрсөн туршилтын үр дүнг олон нийтийн өгөгдлийн сангийн хэлбэртэй харьцуулах хэд хэдэн текст хэлбэртэй харьцуулах арга барилгуудтай харьцуулсан.', 'pl': 'Transfer stylu tekstu odnosi się do zadania zmiany sformułowania danego tekstu w innym stylu. Podczas gdy proponowano różne metody, aby poprawić stan techniki, często zakładają, że wyjście transferu następuje po rozkładzie delta, a zatem ich modele nie mogą generować różnych wyników transferu stylu dla danego tekstu wejściowego. Aby zaradzić temu ograniczeniu, proponujemy framework transferu stylu tekstu jeden do wielu. W przeciwieństwie do poprzednich prac, które uczą się mapowania jeden do jeden, które konwertuje zdanie wejściowe na jedno zdanie wyjściowe, nasze podejście uczy się mapowania jeden do wielu, które może konwertować zdanie wejściowe na wiele różnych zdań wyjściowych, przy jednoczesnym zachowaniu treści wejściowej. Osiąga się to poprzez zastosowanie szkolenia przeciwnego z ukrytym schematem rozkładu. W szczególności rozkładamy ukrytą reprezentację zdania wejściowego na kod stylu, który przechwytuje wariancję stylu językowego i kod treści, który koduje treść niezależną od stylu językowego. Następnie łączymy kod treści z kodem stylu w celu generowania wyjścia transferu stylu. Łącząc ten sam kod treści z innym kodem stylu, generujemy inny wyjście transferu stylu. Obszerne wyniki eksperymentalne z porównaniami z kilkoma metodami transferu stylów tekstowych na wielu publicznych zbiorach danych z wykorzystaniem zróżnicowanego zestawu wskaźników wydajności potwierdzają skuteczność proponowanego podejścia.', 'ro': 'Transferul stilului de text se referă la sarcina reformulării unui text dat într-un stil diferit. Deși s-au propus diferite metode pentru a avansa starea tehnologiei, ele presupun adesea că ieșirea de transfer urmează o distribuție delta, și astfel modelele lor nu pot genera rezultate diferite de transfer de stil pentru un anumit text de intrare. Pentru a aborda limitarea, propunem un cadru de transfer de stil text unu-la-mulți. Spre deosebire de lucrările anterioare care învață o mapare unu-la-unu care convertește o propoziție de intrare într-o propoziție de ieșire, abordarea noastră învață o mapare unu-la-mulți care poate converti o propoziție de intrare în mai multe propoziții de ieșire diferite, păstrând în același timp conținutul de intrare. Acest lucru se realizează prin aplicarea instruirii adversare cu o schemă de descompunere latentă. Mai exact, descompunem reprezentarea latentă a propoziției de intrare într-un cod de stil care capturează variația stilului lingvistic și un cod de conținut care codează conținutul independent de stilul lingvistic. Apoi combinăm codul de conținut cu codul de stil pentru generarea unei ieșiri de transfer de stil. Combinând același cod de conținut cu un cod de stil diferit, generăm o ieșire de transfer de stil diferită. Rezultatele experimentale extinse cu comparații cu mai multe abordări de transfer de stil text pe mai multe seturi de date publice, utilizând un set divers de metrici de performanță, validează eficiența abordării propuse.', 'no': 'Overføring av tekststil refererer til oppgåva for å gjenoppretta tekst i ein annan stil. Mens forskjellige metodar er foreslått for å forbetra kunststanden, antar dei ofte at overføringseininga følgjer ein delta- distribusjon, og dermed kan modelane sine ikkje laga ulike stiloverføringsfeila for eit gitt inndata- tekst. For å handtera grensesnittet, foreslår vi eit rammeverk for overføring av tekststil til ein til mange. I contrast to previous works that learn a one to one mapping that converts an input sentence to one output sentence, our approach lears a one to many mapping that can convert an input sentence to multiple different output sentences while preserving the input content. Dette er oppnådd ved å bruka oppositær trening med eit latent dekomposisjonsskjema. Vi dekomprimerer den latente representasjonen av innsetninga til ein stilkode som inneheld variasjonen av språkstil og innhaldskoden som koderer innhaldet uavhengig med språkstil. Vi kombinerer så innhaldskoden med stilkoden for å laga ein stiloverføringskode. Ved å kombinere den same innhaldskoden med ein annan stilkode, lager vi ein annan utskrift av stiloverføring. Ekstra eksperimentelt resultat med sammenlikningar med fleire tilnærmingar for overføring av tekststil på fleire offentlige datasett ved å bruka ein ulike sett av utviklingsmeteriske effektivitet for den foreslåde tilnærminga.', 'so': 'Isticmaalka qoraalka waxaa loola jeedaa shaqada ku cusboonaysiinta qoraalka la siiyey qaab kala duwan. Inta lagu talo galay in uu horumariyo xaalada farshaxanka, waxay marar badan u malaynayaan in midhaha wareejinta uu raaco qaybsiga delta, sidaas darteed modelalkooda ma samayn karo resulto u beddelista qaab kala duduwan oo laga soo diro qoraalka lagu qoray. Si aan u baaraandegiso xadhigga, waxaynu horumarinaynaa qoraal aad u beddelin karto. Shuqullada hore oo ka hor barta hal-hal sawir oo ku beddelaya hal xalad ka soo baxa, dhaqdhaqaalahayagu wuxuu baran karaa hal-ilaa-badan sawir lagu beddeli karo xarumo input si uu ugu beddelo sentenceyo kala duduwan oo soo baxa, marka uu ilaaliyo waxyaabaha input. Taas waxaa lagu sameyn karaa codsashada waxbarashada cadaawayaasha ah qorshaha dhaqaalaha ee ugu dambeeya. Si gaar ah, waxaynu u qeyb ka dhignaa nooca ugu dambeeya qaababka lagu soo qorayo qoraalka qoraalka ah oo qabsada beddelka qaababka luqada iyo kooxda waxyaabaha ku jira oo ku qoran waxyaabaha ku qoran qoran qoraalka oo aan xor ahayn. Markaas waxaynu koobnaynaa kooxda waxyaabaha ku jira codsiga qaababka ah si aan u sameyno qalabka wareejinta. Marka aad isku darsameyso kaarka waxyaabaha isku mid ah, waxaynu sameynaa midhaha wareejinta qaab kala duwan. Midhaha jirrabka ah ee badan ee isbarbardhigga qaababka qoraalka ah ee kala duduwan ayaa lagu soo bandhigayaa samooyin badan oo lagu isticmaalayo metrito kala duduwan oo sameynta shaqaalaha ah ee loo talagalay.', 'sr': 'Prebacivanje stila teksta se odnosi na zadatak ponovnog prehramiranja određenog teksta u drugom stilu. Iako su predloženi različiti metodi da napreduju stanje umjetnosti, često pretpostavljaju da izlaz prijenosa slijedi distribuciju delte, i tako njihovi modeli ne mogu stvoriti različite rezultate prijenosa stila za određeni tekst. Da bi se riješili ograničenja, predlažemo okvir prebacivanja teksta na jedan do mnogo stila. U suprotnosti sa prethodnim radovima koji nauče mapiranje jedne na jednu koja pretvara ulaznu rečenicu u jednu izlaznu rečenicu, naš pristup nauči mapiranje jedne na mnoge koje može pretvoriti ulaznu rečenicu na višestruke različite izlazne rečenice, dok čuva sadržaj ulazne rečenice. To je postignuto primjenom neprijateljskog treninga sa latentnom šemom raskidanja. Posebno, raspoređujemo latentnu predstavu rečenice za ulazak kod stila koji uključuje varijaciju jezika stila i kodiranje sadržaja koji kodira sadržaj nezavisnog jezika stila. Onda ćemo kombinirati sadržaj kod stila za proizvodnju izveštaja stila. Kombinujući isti sadržaj kod sa različitim širom stila, stvaramo drugačiji izlaz transfer stila. Eksperimentalni rezultati sa usporedbama sa nekoliko pristupa premeštaju teksta stila na višestrukim javnim podacima koristeći različite sete metrika za provedbu potvrđuje učinkovitost predloženog pristupa.', 'si': 'පාළ ස්ථානය සංවර්තනය වෙනස් ස්ථානයෙන් දෙන්න පාළුවක් ආපහු ප්\u200dරතික්\u200dරියා කරන වැඩයට පණි විවිධ විධානයක් ප්\u200dරශ්නයක් විදිහට කල්ලාගේ ස්ථිතිය ප්\u200dරධානය කරන්න ප්\u200dරශ්නයක් තියෙනවා නමුත්, ඔවුන් සාමාන්\u200dය ප්\u200dරශ්නයක සීමාව සම්බන්ධ කරන්න, අපි පාළ ස්ථානයක් එක්කෙන් ගොඩක් පාළ ස්ථානයක් ප්\u200dරයෝජනය කරනවා. මුලින් වැඩට ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ඉගෙන ගන්න පුළුවන් ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ප්\u200dරති මේක ප්\u200dරශ්නයක් විරෝද්ධ ප්\u200dරශ්නයක් ලේටින් විස්තරණය සැකසුම් සඳහා විරෝද්ධ ප්\u200dරශ්නයක විශේෂයෙන්, අපි භාෂාව වෙනස් විකල්පයක් සහ භාෂාව ස්ථාවිත සාමාවිත සංකේතයක් සංකේතය කරන භාෂාව ස්ථාවිත විකල් ඊට පස්සේ අපි සාමාන්\u200dය කෝඩ් එක්ක ස්ටේල් කෝඩ් එක්ක සම්බන්ධ කරනවා ශේලි ප්\u200dරවේශනය ප්\u200dරවේශන වෙනස් විදියට ස්ථානය කෝඩ් එක්ක සම්බන්ධ කරලා අපි වෙනස් විදියට ස්ථානය ප්\u200dරවර්තනයක් සිද්ධ විශේෂ පරීක්ෂණ ප්\u200dරතිචාර ප්\u200dරතිචාර ප්\u200dරතිචාර පරීක්ෂණය සමඟ පාළුවන් විශේෂ ස්ථානය සඳහා පාළුවන් ස්ථා', 'sv': 'Textformatöverföring avser uppgiften att omformulera en viss text i en annan stil. Även om olika metoder har föreslagits för att avancera den senaste tekniken, antar de ofta att överföringsutgången följer en deltafördelning, och därför kan deras modeller inte generera olika formatöverföringsresultat för en given inmatningstext. För att ta itu med begränsningen föreslår vi ett one-to-many textformatöverföringsramverk. Till skillnad från tidigare arbeten som lär sig en en-till-en-mappning som omvandlar en inmatningspunkt till en utmatningspunkt, lär vårt tillvägagångssätt en en-till-många-mappning som kan konvertera en inmatningspunkt till flera olika utmatningspunkter, samtidigt som indatainnehållet bevaras. Detta uppnås genom att tillämpa motstridig träning med ett latent nedbrytningsschema. Specifikt bryter vi ned den latenta representationen av inmatningssatsen till en stilkod som fångar språkstilsvariationen och en innehållskod som kodar språkstiloberoende innehåll. Vi kombinerar sedan innehållskoden med stilkoden för att generera en formatöverföringsutmatning. Genom att kombinera samma innehållskod med en annan stilkod genererar vi en annan formatöverföringsutmatning. Omfattande experimentella resultat med jämförelser med flera metoder för överföring av textstilar på flera offentliga datauppsättningar med hjälp av en mängd olika prestandamätningar bekräftar effektiviteten av den föreslagna metoden.', 'ur': 'Text style transfer refers to the task of rephrasing a given text in a different style. اگرچہ مختلف طریقے پیشنهاد کیے گئے ہیں کہ هنر کی حالت پیشرفت کریں، انہوں نے اکثر سمجھ لیا ہے کہ انتقال اجلام ڈیلٹا تقسیم کے پیچھے چلتا ہے، اور اسی طرح ان کے مدلے مختلف طریقے انتقال نتیجے پیدا نہیں کر سکتے جنہیں ایک مقر محدودیت کے بارے میں ہم ایک سے بہت سے ٹیکسٹ استیل ٹرانسٹر فرمیک پیشنهاد کرتے ہیں۔ پہلے کے کاموں کے مقابلہ میں جو ایک سے ایک مکاپینگ کی سیکھ رہے ہیں جو ایک اینٹ ویٹ ویٹ میں ایک اینٹ ویٹ ویٹ ویٹ کی طرف تبدیل کرتی ہے، ہماری طریقہ ایک سے بہت سی مکاپینگ کی سیکھ رہی ہے جو ایک اینٹ ویٹ ویٹ ویٹ کو بہت سی متفاوت ویٹ ویٹ یہ کامیاب ہوا ہے لٹینٹ ڈھیٹ ڈھیٹ ڈھیٹ سیکھ کے ساتھ مخالفت کی تعلیم کے ذریعہ۔ خاص طور پر، ہم اینٹ ویل کی لاٹینٹ روشنی کو ایک استیل کیڈ میں تقسیم کرتے ہیں جو زبان کی طرح تغییرات اور ایک منصوبہ کیڈ کو پکڑتا ہے جو زبان کی استیل ناخفی منصوبات کا کوڈ کرتا ہے. پھر ہم نے سائل ٹرانسٹر آوٹ پیٹ پیدا کرنے کے لئے سائل کوڈ کو استیل کوڈ کے ساتھ ترکیب کیا ہے. ایک دوسرے شیل کڈ کے ساتھ ایک دوسرے سائل کڈ کو جوڑ کر، ہم ایک دوسرے شیل ترنسیٹ آوٹ پیدا کرتے ہیں. بہت سی آزمائش کے نتیجے کے ساتھ بہت سی ٹیکسٹ سٹیل ترنسفور کے مطابق بہت سی ٹیکسٹ سٹیلوں پر بہت سی ڈیٹ سٹیلوں کے مطابق مختلف فعالیت متریک کے مطابق استعمال کرتے ہیں۔', 'ta': 'உரை பாணி மாற்றுதல் கொடுக்கப்பட்ட உரையை வேறு பாணியில் மறுமாற்றும் பணிக்கு குறிப்பிடுகிறது. கலையின் நிலையை முன்னேற்ற பல முறைகள் பரிந்துரைக்கப்பட்ட போது, அவர்கள் பெரும்பாலும் மாற்றும் வெளியீடு டெல்டா பகிர்ந்தளிக்கும் பின்பற வரம்பை முகவரிப்பதற்கு, நாம் ஒரு முதலில் இருந்து பல உரை பாணியை மாற்றும் சட்டத்தை பரிந்துரைக்கிறோம். ஒரு வெளியீட்டு வாக்கியத்திற்கு ஒரு வெளியீட்டு வாக்கியத்தை மாற்றும் ஒரு முன்னால் ஒரு வரைபடத்தில் கற்றுக் கொள்ளும் முன்னால், எங்கள் முன்னால் ஒரு வரைபடத்தில்  இது ஒரு சமீபத்தில் குறைந்த திட்டத்துடன் எதிரிய பயிற்சியை பயன்படுத்துவதால் அடையப்படுகிறது. குறிப்பிட்டு, நாம் உள்ளீட்டு வாக்கியத்தின் தற்பொதுவான பிரதிநிர்வாக்கத்தை ஒரு பாணி குறியீட்டை பிடிக்கும் மொழி பாணி மாறியை  பின்னர் நாம் பாணி மாற்றும் வெளியீட்டை உருவாக்குவதற்கான உள்ளடக்க குறியீட்டை பாணி குறியீட் ஒரே உள்ளடக்க குறியீட்டை வேறு பாணி குறியீட்டுடன் இணைத்தால், வேறு பாணி மாற்றும் வெளியீட்டை உருவா விரிவான சோதனை முடிவு', 'uz': 'Name Sananing holatini oldinga turli usullar talab qilingan paytda, ular odatda uzatish natijasi delta tarqatishga keladi, shunday qilib modellari koʻrsatilgan matn uchun turli transfer natijalarini yaratib boʻlmaydi. Chegarani tasdiqlash uchun biz bir necha matn uslub almashtirish freymini davom etimiz. Name Name Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content.  Keyin biz uslub transfer natijasini yaratish uchun tarkib kodi bilan birlashtiramiz. @ info: whatsthis Name', 'vi': 'Chuyển đổi kiểu văn bản đề cập đến việc lặp lại văn bản đã đưa ra theo kiểu khác. Trong khi các phương pháp khác nhau được đề xuất để nâng cao trạng thái nghệ thuật, họ thường giả sử kết quả chuyển nhượng theo một phân phối châu thổ, và các mô hình của họ không thể tạo ra kết quả chuyển đổi kiểu cách cho một văn bản nhập. Để giải quyết giới hạn, chúng tôi đề xuất một đến nhiều phương pháp chuyển nhượng văn bản. Tương đối với các tác phẩm trước đã học được một bản đồ một đến một bản đồ có thể chuyển đổi một câu từ nhập thành một câu kết xuất, cách tiếp cận của chúng ta học một đến nhiều bản đồ có thể chuyển đổi một câu vào nhiều câu khác nhau, trong khi bảo vệ nội dung nhập. Điều này được thực hiện bằng cách huấn luyện đối nghịch với một phương pháp phân hủy chậm. Cụ thể, chúng ta phân hủy các mô tả tiềm năng của câu nhập thành một mã kiểu dáng nắm bắt các thay đổi kiểu ngôn ngữ và một mã hóa nội dung mã hóa nội dung tự lập ngôn ngữ. Chúng ta kết hợp mã nội dung với mã kiểu dáng để tạo ra một xuất chuyển kiểu dáng. Bằng cách kết hợp cùng một mã nội dung với một mã kiểu khác, chúng tôi tạo ra một kết xuất chuyển kiểu khác. Kết quả thử nghiệm rộng lớn với so sánh với nhiều phương pháp chuyển đổi kiểu văn bản trên nhiều bộ dữ liệu công cộng dùng một số bộ đo trình mô phỏng hiệu quả xác nhận hiệu quả của phương pháp yêu cầu.', 'nl': 'Tekststijloverdracht verwijst naar de taak van het herformuleren van een bepaalde tekst in een andere stijl. Hoewel er verschillende methoden zijn voorgesteld om de stand van de techniek te verbeteren, gaan ze er vaak van uit dat de overdracht output een delta verdeling volgt, en hun modellen dus geen verschillende stijltransferresultaten kunnen genereren voor een bepaalde invoertekst. Om de beperking aan te pakken, stellen we een één-op-veel tekststijl transfer framework voor. In tegenstelling tot eerdere werken die een één-op-één toewijzing leren die een invoerzin omzet naar één uitvoerzin, leert onze aanpak een één-op-veel toewijzing die een invoerzin kan converteren naar meerdere verschillende uitvoerzinnen, terwijl de invoerinhoud behouden blijft. Dit wordt bereikt door het toepassen van tegenstrijdige training met een latent decompositieschema. Met name decomponeren we de latente weergave van de invoerzin in een stijlcode die de taalstijlvariatie vastlegt en een inhoudscode die de taalstijlvariatie codeert. Vervolgens combineren we de inhoudscode met de stijlcode voor het genereren van een style transfer output. Door dezelfde inhoudscode te combineren met een andere stijlcode, genereren we een andere stijlcode. Uitgebreide experimentele resultaten met vergelijkingen met verschillende benaderingen van tekststijl overdracht op meerdere publieke datasets met behulp van een diverse set prestatiemetrics valideren de effectiviteit van de voorgestelde aanpak.', 'hr': 'Prebacivanje stila teksta se odnosi na zadatak rephrazivanja određenog teksta u drugom stilu. Iako su predloženi različite metode za napredak stanja umjetnosti, često pretpostavljaju da izlaz prijenosa slijedi distribuciju delte, te tako njihovi modeli ne mogu stvoriti različite rezultate prijenosa stila za određeni tekst. Da bi se riješili ograničenja, predlažemo okvir prijenosa teksta od jednog do mnogih stila. U suprotnosti s prethodnim radovima koji nauče mapiranje jedne na jednu koja pretvara ulaznu rečenicu u jednu izlaznu rečenicu, naš pristup nauči mapiranje jedne na mnoge koje može pretvoriti ulaznu rečenicu na višestruke različite izlazne rečenice, dok čuva sadržaj ulazne rečenice. To se postigne primjenom neprijateljskog treninga s latentnom rasključenjem. Posebno, raspoređujemo latentnu predstavu uvodne rečenice kod stila koji uključuje varijaciju jezika stila i sadržaj koda koji kodira sadržaj neovisnog jezika stila. Onda ćemo kombinirati sadržaj kod stila za proizvodnju izlaza prijenosa stila. Kombinirajući isti sadržaj kod sa različitim širom stila, stvaramo različit izlaz prijenosa stila. Prošireni eksperimentalni rezultati s usporedbama s nekoliko pristupa premještaju teksta na višestrukim javnim podacima koristeći različite skupine provedbe metrika potvrđujući učinkovitost predloženog pristupa.', 'da': 'Overførsel af tekststil refererer til opgaven med at omformulere en given tekst i en anden stil. Selvom forskellige metoder er blevet foreslået for at fremme den nye teknologi, antager de ofte, at overførselsudgangen følger en delta fordeling, og dermed deres modeller ikke kan generere forskellige stiloverførselsresultater for en given input tekst. For at løse begrænsningen foreslår vi en én-til-mange teksttypeoverførselsramme. I modsætning til tidligere værker, der lærer en en-til-en-kortlægning, der konverterer en inputsætning til en outputsætning, lærer vores tilgang en en-til-mange kortlægning, der kan konvertere en inputsætning til flere forskellige outputsætninger, samtidig med at inputindholdet bevares. Dette opnås ved at anvende modstridende træning med en latent nedbrydningsordning. Specielt nedbryder vi den latente repræsentation af indtastningen til en typografikode, der fanger variationen i sprogstilen, og en indholdskode, der koder det sprogstiluafhængige indhold. Vi kombinerer derefter indholdskoden med typografikoden for at generere en typografioverførsel output. Ved at kombinere den samme indholdskode med en anden typografikode genererer vi en anden typografioverførsel output. Omfattende eksperimentelle resultater med sammenligninger med flere tilgange til overførsel af tekststile på flere offentlige datasæt ved hjælp af et forskelligt sæt præstationsmålinger validerer effektiviteten af den foreslåede tilgang.', 'id': 'Transfer gaya teks merujuk tugas mengubah kata teks yang diberikan dalam gaya yang berbeda. Sementara berbagai metode telah diusulkan untuk melanjutkan state of the art, mereka sering menganggap output transfer mengikuti distribusi delta, dan oleh itu model mereka tidak dapat menghasilkan hasil transfer gaya berbeda untuk teks input tertentu. Untuk mengatasi batasan, kami mengusulkan rangkaian transfer gaya teks satu-ke-banyak. Berbeda dengan pekerjaan sebelumnya yang belajar peta satu ke satu yang mengubah kalimat input ke satu kalimat output, pendekatan kita belajar peta satu ke banyak yang dapat mengubah kalimat input ke multiple kalimat output yang berbeda, sementara menyimpan isi input. Ini dicapai dengan menerapkan pelatihan lawan dengan skema dekomposisi latent. Secara spesifik, kita merusak represensi latent dari kalimat masukan ke kode gaya yang menangkap variasi gaya bahasa dan kode konten yang mengkode konten yang bebas gaya bahasa. Kemudian kita menggabungkan kode isi dengan kode gaya untuk menghasilkan output transfer gaya. Dengan menggabungkan kode konten yang sama dengan kode gaya yang berbeda, kami menghasilkan output transfer gaya yang berbeda. Hasil eksperimen yang luas dengan perbandingan dengan beberapa pendekatan transfer gaya teks pada beberapa set data publik menggunakan set metrik prestasi yang berbeda menentukan efektivitas pendekatan yang diusulkan.', 'bg': 'Прехвърлянето на текстов стил се отнася до задачата за преформулиране на даден текст в различен стил. Въпреки че са предложени различни методи за напредване в съвременните технологии, те често приемат, че трансферният изход следва делта разпределение и по този начин техните модели не могат да генерират различни резултати от трансфер на стил за даден входен текст. За да се справим с ограничението, предлагаме рамка за прехвърляне на текстов стил от един към много. За разлика от предишни работи, които научават едно към едно картографиране, което преобразува входно изречение в едно изходно изречение, нашият подход научава едно към много картографиране, което може да преобразува входно изречение в няколко различни изходни изречения, като същевременно запазва входното съдържание. Това се постига чрез прилагане на съперническо обучение с латентна схема на разлагане. По-конкретно, ние разлагаме латентното представяне на входното изречение на стил код, който улавя вариацията на езиковия стил и код на съдържание, който кодира съдържанието, независимо от езиковия стил. След това комбинираме кода на съдържанието с кода на стила за генериране на изход за трансфер на стил. Като комбинираме един и същ код на съдържание с различен стил код, генерираме различен стил трансфер изход. Обширни експериментални резултати с сравнения с няколко подхода за трансфер на текстов стил върху множество обществени набори от данни, използващи разнообразен набор от показатели за ефективност, потвърждават ефективността на предложения подход.', 'de': 'Textstilübertragung bezieht sich auf die Aufgabe, einen bestimmten Text in einem anderen Stil umzuformulieren. Während verschiedene Methoden vorgeschlagen wurden, um den Stand der Technik voranzutreiben, gehen sie oft davon aus, dass die Transferausgabe einer Delta-Verteilung folgt, und ihre Modelle können daher keine unterschiedlichen Stilergebnisse für einen gegebenen Eingabetext generieren. Um dieser Einschränkung entgegenzuwirken, schlagen wir ein one-to-many Textstil Transfer Framework vor. Im Gegensatz zu früheren Arbeiten, die eine Eins-zu-Eins-Zuordnung erlernen, die einen Eingangssatz in einen Ausgangssatz konvertiert, lernt unser Ansatz eine Eins-zu-Viele-Zuordnung, die einen Eingangssatz in mehrere verschiedene Ausgangssätze konvertieren kann, während der Eingangsinhalt beibehalten wird. Dies wird durch das Anwenden von adversarialem Training mit einem latenten Zerlegungsschema erreicht. Insbesondere zerlegen wir die latente Darstellung des Eingabesatzes in einen Stilcode, der die Sprachstilvariation erfasst, und einen Inhaltscode, der den sprachstilunabhängigen Inhalt kodiert. Anschließend kombinieren wir den Content Code mit dem Style Code, um eine Style Transfer Ausgabe zu generieren. Durch die Kombination desselben Inhaltscodes mit einem anderen Style Code generieren wir eine andere Style Transfer Ausgabe. Umfangreiche experimentelle Ergebnisse mit Vergleichen zu mehreren Textstilübertragungsansätzen auf mehreren öffentlichen Datensätzen unter Verwendung verschiedener Leistungsmetriken bestätigen die Wirksamkeit des vorgeschlagenen Ansatzes.', 'ko': '텍스트 스타일 전환은 서로 다른 스타일로 주어진 텍스트를 재해석하는 임무를 말한다.이미 각종 방법을 제시하여 기술 수준을 향상시켰지만 전송 출력은 증량 분포에 따라 이루어진다고 가정하기 때문에 그들의 모델은 주어진 입력 텍스트에 서로 다른 양식 전송 결과를 생성할 수 없다.이러한 한계를 해결하기 위해 우리는 한 쌍의 다중 텍스트 스타일 전환 구조를 제시했다.입력문장을 출력문장의 일대일로 전환하는 것을 배운 이전과 달리 입력문장을 여러 출력문장의 일대일로 전환하고 입력내용을 보존하는 방법을 배웁니다.잠재적 해체 방안을 이용한 대항 훈련을 통해 이뤄진 것이다.구체적으로 말하면 우리는 문장의 잠재적 표시를 언어 스타일 변화를 포획하는 스타일 코드와 언어 스타일과 무관한 내용을 인코딩하는 내용 코드로 분해할 것이다.그리고 우리는 내용 코드와 스타일 코드를 결합시켜 스타일 변환 출력을 생성할 것이다.같은 내용 코드와 다른 스타일 코드를 조합함으로써 우리는 서로 다른 스타일 변환 출력을 생성했다.여러 개의 공공 데이터 집합에서 서로 다른 성능 지표를 사용하여 몇 가지 텍스트 스타일의 전송 방법과 비교한 결과 대량의 실험 결과가 이 방법의 유효성을 검증했다.', 'fa': 'Name در حالی که روش\u200cهای مختلف پیشنهاد داده شده است تا وضعیت هنر را پیشرفت کند، آنها اغلب می\u200cپندارند که نتیجه انتقال انتقال از یک توزیع دلتا را پیروی می\u200cکند، و بنابراین مدل\u200cهایشان نتیجه انتقال طرح مختلف برای متن وارد داده نمی\u200c برای حل محدودیت، ما یک چهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهار در مقابل کارهای قبلی که نقشه\u200cای از یک به یک نقشه یاد می\u200cگیرند که جمله\u200cی ورودی را به یک جمله\u200cی خروجی تبدیل می\u200cکند، دستور ما نقشه\u200cای از یک به بسیاری یاد می\u200cگیرد که می\u200cتواند جمله\u200cی ورودی را به جمله\u200cهای متفاوت تبدیل کند، در حالی که محتوای ورودی را این موضوع با استفاده از آموزش دشمنی با یک نقشه جدا کردن latent موفق می شود. به طور خاصی، ما نمایش تاریخی از جمله ورودی را به یک کد سبک که تغییرات سبک زبان و یک کد محتوای محتوای مستقل زبان را دریافت می\u200cکنیم، تقسیم می\u200cکنیم. سپس ما کد محتویات را با کد سبک برای تولید یک انتقال سبک ترکیب می کنیم. با ترکیب یک کد محتوای مشابه با یک کد سبک متفاوتی، یک انتقال سبک متفاوتی را ایجاد می\u200cکنیم. نتیجه\u200cهای آزمایشی گسترده با مقایسه\u200cها با چندین دستور انتقال متن بر مجموعه\u200cهای داده\u200cهای عمومی با استفاده از مجموعه\u200cی متریک عملکرد تاثیر\u200cپذیری از دستور پیشنهاد.', 'sw': 'Uhamiaji wa mtindo wa maandishi unamaanisha kazi ya kurudisha ujumbe uliopewa katika mtindo tofauti. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a delta distribution, and thus their models cannot generate different style transfer results for a given input text.  Ili kukabiliana na vizuizi, tunapendekeza mfumo wa usafirishaji wa maandishi moja hadi nyingi. Tofauti na kazi za awali ambazo zinajifunza ramani moja kwa moja ambazo hugeuza sentensi ya input kwenye hukumu moja ya output, mbinu yetu inajifunza ramani moja kwa moja na nyingi ambazo inaweza kubadilisha sentensi ya input ili hukumu mbalimbali za matokeo, wakati wa kulinda maudhui ya input. Hii imefanikiwa kwa kutumia mafunzo ya upinzani kwa mpango wa hivi karibuni wa kupunguza mabomu. Kwa ujumla, tunapinga uwakilishi wa hivi karibuni wa hukumu ya input kwa sheria ya mtindo ambao unachukua mabadiliko ya mtindo wa lugha na kanuni ya maudhui yanayojumuisha maudhui yanayojitegemea lugha. Kisha tunaunganisha kodi ya maudhui na kanuni ya mtindo kwa kutengeneza matokeo ya usafirishaji wa mtindo. Kwa kuunganisha kodi hilo la maudhui kwa kutumia kanuni tofauti ya mtindo, tunatengeneza matokeo tofauti ya uhamishaji wa mtindo. Matokeo mengi ya majaribio yanayolinganisha na namna kadhaa za usafirishaji wa simu za maandishi kwenye seti mbalimbali za taarifa za umma kwa kutumia mitindo mbalimbali ya utendaji yanayothibitisha ufanisi wa mbinu hiyo ya pendekezo.', 'am': 'text-tool-action የፊደል ቅርጽ አካባቢ ጥያቄ ለመቀናኘት የልዩ ሥርዓቶች በተዘጋጀ ጊዜ የtransfer ውጤት የዴልታ ክፍል እንዲከተል ያስባል፤ ስለዚህም ሞዴሎቻቸው በተሰጠው የጥያቄ ጽሑፍ የተለየ የዓይነት ለውጦ ፍሬዎችን ማቅረብ አይችሉም። To address the limitation, we propose a one-to-many text style transfer framework.  ለአንድ-ለአንድ-ወደ-አንድ ክፍል የሚማር ክፍተቶችን ወደ አንድ ውጤት የሚለውጥ ክፍተት የሚተካክሉ ክፍተቶችን ይማራሉ፤ የግንኙነታችን ግንኙነት በመጠበቅ የሚችለውን የውስጥ ክፍል ለመለወጥ ይችላል፡፡ ይህ በተቃዋሚ ትምህርት በመጠቀም በአሁኑ የውጤት አካባቢ ፕሮግራም ነው፡፡ በተለየ ጊዜ የቋንቋውን የዘይት መለወጥ እና የቋንቋውን ዘይት-ነፃ ሆኖ የሚይዘውን የሆኑን ኮድ አካባቢ እናደርጋለን፡፡ በኋላም የሆኑን ኮድ በዓይነቱ የዘይት ስፋት ውጤትን ለመፍጠር እናስገባለን፡፡ በተለየ የዓይነት ጽሑፍ በተለየ የዓይነት ጽሑፍ በመቀላቀል እናደርጋለን፡፡ የሥርዓት ፈተና ፍሬዎች በተለያዩ የሥርዓት ሥርዓት የጥያቄ ጥያቄ በሁለት የህዝብ ዳታዎችን በመቀናቀል ይደረጋሉ፡፡', 'sq': 'Transferimi i stilit të tekstit referohet në detyrën e rifreshimit të një teksti të dhënë në një stil tjetër. Ndërsa janë propozuar metoda të ndryshme për të përparuar gjendjen e teknologjisë, a to shpesh supozojnë se dalja e transferit ndjek një shpërndarje delta dhe kështu modelet e tyre nuk mund të gjenerojnë rezultate të ndryshme të transferimit të stilit për një tekst të caktuar të hyrjes. Për të trajtuar kufizimin, ne propozojmë një kuadër të transferimit të stilit të tekstit një-në-shumë. Në kundërshtim me punët e mëparshme që mësojnë një hartim nga një në një që konverton një fjalim të hyrjes në një fjalim të daljes, qasja jonë mëson një hartim nga një në shumë që mund të konvertojë një fjalim të hyrjes në shumë fjalime të ndryshme të daljes, duke ruajtur përmbajtjen e hyrjes. Kjo arrihet duke aplikuar trajnimin kundërshtar me një skemë dekompozimi të fshehtë. Veçanërisht, ne shkëputemi përfaqësimin e fshehtë të fjalës së hyrjes në një kod stili që kapon variacionin e stilit gjuhësor dhe një kod përmbajtjeje që kodon përmbajtjen e pavarur nga stili gjuhësor. Pastaj kombinojmë kodin e përmbajtjes me kodin e stilit për të krijuar një dalje transferimi stili. Duke kombinuar të njëjtin kod përmbajtjeje me një kod stili të ndryshëm, ne gjenerojmë një dalje tjetër stili transferimi. Rezultatet eksperimentale të gjera me krahasime me disa metoda të transferimit të stilit të tekstit në grupe të dhënash publike të shumta duke përdorur një grup të ndryshëm metrike performance validojnë efektshmërinë e metodës së propozuar.', 'tr': 'Metin biçimi terjime edilen metin beýleki biçiminde gaýd etmek üçin täblisaňa gürrüň edýär. Çeşitli yöntemler sungatyň durumyny öňden geçirmek üçin teklip edildi welin, köplenç süýşinden dolanan delta döwletlerini çaklaýarlar we bu şekilde modelleriniň berilen girdi metin üçin başga dürli biçimler netijesini döredip bilmeýärler. Hatlary çykmak üçin, bir-birden üçin bir metin biçiminden geçirmek üçin teklip teklip et. Öňki işlerden öňki bir-bire-birine giriş sözlerini bir çizgi sözlere üýtgeden bir haritasy öwrenip, biziň metodamyz giriş sözlerini birnäçe çeşit sözlere üýtgeden, giriş mazmunlaryny sagda tutup bilen bir-bire-bire haritasy öwrenip biler. Bu ýer täsirli eğitim goýulýar. Adatça, girdi sözleriniň soňky suratyny dil biçiminden üýtgewişini we dil biçiminden mahsusyny kodlayan bir şekilde çykarýarys. Sonra daşary çykyş çykyşyny bejermek üçin mod kody bilen birleştirip bileris. Aynı mazmunlar ködini başga bir stil ködleri bilen birleşdirerek, başga bir biçim aktarışyny bejeririz. Çeşitli metin biçimleriniň karşılaştyrylygy bilen örän metin biçimleriniň golaýatyny görkezilýän çylşyrymlyk netijesi görkezilýän.', 'hy': 'Տեքստի ոճի փոխանցումը նշանակում է հատուկ տեքստի փոխարինել այլ ոճով: Մինչդեռ տարբեր մեթոդներ են առաջարկվել արվեստի զարգացման համար, նրանք հաճախ ենթադրում են, որ տեղափոխման արտադրությունը հետևում է դելտա տարածմանը, և այսպես նրանց մոդելները չեն կարող ստեղծել տարբեր ոճի տեղափոխման արդյունքներ որոշ ներմու Այս սահմանափակումների լուծման համար մենք առաջարկում ենք մեկ-շատերին տեքստի ոճի փոխանցման շրջանակ: Ի հակադրություն նախորդ աշխատանքներին, որոնք սովորում են քարտեզագրություն, որը փոխակերպում է մուտքային նախադասությունը մեկ արտադրյալ նախադասությամբ, մեր մոտեցումը սովորում է քարտեզագրություն, որը կարող է մուտքային նախադասությունը փոխակերպել բազմաթիվ տարբեր արտադրյալ նախադա Սա հասնում է օգտագործելով հակառակ վարժությունները թաքնված բաժանման ծրագրով: Հատկապես, մենք բաժանում ենք ներմուծի նախադասության թաքնված ներկայացումը ոճի կոդի, որը ներառում է լեզվի ոճի տարբերությունը և պարունակության կոդի, որը կոդավորում է լեզվի ոճի անկախ պարունակությունը: Հետո մենք համադրում ենք պարունակության կոդը ոճի կոդի հետ ոճի փոխանցման արդյունքի ստեղծման համար: Համադրելով նույն պարունակության կոդը այլ ոճի կոդի հետ, մենք ստեղծում ենք այլ ոճի փոխանցման արդյունք: Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach.', 'az': 'Metin şəkili tərəfindən başqa bir şəkildə verilən mətn tərəfindən yenidən təkrarlanması işin ə müvəffəq edir. Müxtəlif metodlar sanatın durumunu ilerləşdirmək üçün təbliğ edilmiş halda, onlar əksəriyyətli hərəkət çıxımından delta dağıtıcının ardınca gəldiklərini iddia edirlər. Beləliklə onların modelləri bənzər stil dağıtma sonuçlarını verilən girdi metinə yaratmaz. Sınırları çəkmək üçün, bir-çox mətn tərəfindən çox tərəfindən təklif edirik. Əvvəlki işlərdən əvvəl bir-birinə qarşılıq cümləsini bir çıxış cümləsinə döndərir, bizim yolumuz bir-çox xəritə öyrənir ki, giriş cümləsini çoxlu müxtəlif çıxış cümləsinə döndərir, giriş məlumatını qoruyarkən. Bu, latent parçalandırma taslağıyla düşmənçilik təhsilini uygulamaq üçün başarılır. Özellikle, biz girdi cümləsinin latent ifadəsini dil stili dəyişikliklərini və dil stili bağımsız məlumatını kodlayan məlumat kodunu çəkirik. Sonra məlumat kodu istil kodu ilə bir istil istifadə çıxışı yaratmaq üçün birləşdiririk. Aynı məlumat kodu başqa bir stil kodu ilə birləşdirərək, başqa bir stil tərəfindən istifadə edirik. Müxtəlif performans metriklərinin tərzini təsdiqlədiyi metodların etkinliğini təsdiqlədiyi kimi çoxlu mətn stilinin istifadə edilməsi ilə çoxlu məlumat tərzlərinin tərəfindən keçirilməsi ilə çoxlu tərzlərin istifadə edilməsi üçün genişliyi tə', 'af': "Teks styl oordrag verwys na die taak van herhaal 'n gegewe teks in' n ander styl. Alhoewel verskillende metodes is voorgestel om die staat van die kuns te vordering, aanvaar hulle dikwels die oordrag uitvoer volg 'n delta verspreiding, en dus kan hulle modele nie verskillende styl oordrag resultate genereer vir 'n gegewe invoer teks nie. Om die beperking te adres, voorstel ons 'n een na veel teks-styl oordrag raamwerk. In contrast to previous works that learn a one-to-one mapping that converts an input sentence to one output sentence, our approach lears a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content. Hierdie word bereik deur die toepassing van teëstandige onderwerking met 'n latente aflossing skema te doen. Spesifieke, ons dekomprimeer die latente voorstelling van die invoer setsing na 'n styl kode wat die taal styl veranderinge en 'n inhoud kode wat die taal styl-onafhanklike inhoud enkodeer. Ons kombinieer dan die inhoud kode met die styl kode vir genereer van 'n styl oordrag uitset. Deur die selfde inhoud kode met 'n ander styl kode te kombinerer, genereer ons 'n ander styl oordrag uitset. Ekstensiewe eksperimentale resultate met vergelykings met verskeie teks-styl oordrag toegang op veelvuldige publieke datastelle gebruik 'n verskeie stel van prestasie-metries waardig effektiviteit van die voorgestelde toegang.", 'bs': 'Prebacivanje stila teksta se odnosi na zadatak ponovnog prehramanja određenog teksta u drugom stilu. Iako su predloženi različiti metodi da napreduju stanje umjetnosti, oni često pretpostavljaju da izlaz prijenosa slijedi distribuciju delte, i tako njihovi modeli ne mogu stvoriti različite rezultate prijenosa stila za određeni tekst unosa. Da bi se riješili ograničenja, predlažemo okvir prebacivanja teksta na jedan do mnogo stila. U suprotnosti sa prethodnim radovima koji nauče mapiranje jedne na jednu koja pretvara ulaznu rečenicu u jednu izlaznu rečenicu, naš pristup nauči mapiranje jedne na mnoge koje može pretvoriti ulaznu rečenicu na višestruke različite izlazne rečenice, dok čuva sadržaj ulazne rečenice. To se postigne primjenom neprijateljskog treninga sa latentnom šemom rasporedanja. Posebno, rasporedimo latentnu predstavu uvodne rečenice kod stila koji uključuje varijaciju jezika stila i sadržaj koda koji kodira sadržaj nezavisnog jezika stila. Onda ćemo kombinirati sadržaj kod stila za proizvodnju izlaza stila. Kombinujući isti sadržaj kod sa različitim širom stila, stvaramo drugačiji izlaz transfer stila. Eksperimentalni rezultati sa usporedbama s nekoliko pristupa premještaju teksta na višestrukim javnim podacima koristeći različite skupine metrika za provedbu potvrđuju učinkovitost predloženog pristupa.', 'ca': "Text style transfer refers to the task of rephrasing a given text in a different style.  Mentre s'han proposat diversos mètodes per avançar l'estat de l'art, sovint suposen que la producció de transfer ència segueix una distribució delta, i així els seus models no poden generar diferents resultats de transferència d'estil per un text d'entrada determinat. Per abordar la limitació, proposem un marc de transfer ència d'estil de text d'un a molts. Contrariament a treballs anteriors que aprenen un mapatge d'una a una que converteix una frase d'entrada en una frase d'entrada, el nostre enfocament aprenen un mapatge d'una a moltes que pot convertir una frase d'entrada en múltiples frases d'entrada diferents mentre conservem el contingut d'entrada. Això es pot aconseguir aplicant formació adversaria amb un esquema de descomposició latent. Concretament, descomponem la representació latent de la frase d'entrada en un codi d'estil que captura la variació de l'estil lingüístic i un codi de contingut que codifica el contingut independent de l'estil lingüístic. Llavors combinam el codi de contingut amb el codi d'estil per generar una sortida de transfer ència d'estil. Combinant el mateix codi de contingut amb un codi d'estil diferent, generem una sortida de transfer ència d'estil diferent. Els resultats experimentals extensos amb comparacions amb diversos enfocaments de transfer ència d'estils de text en múltiples conjunts de dades públices utilitzant un conjunt divers de mètriques de rendiment validen l'eficacia de l'enfocament proposat.", 'et': 'Teksti laadi ülekanne viitab antud teksti ümbersõnastamise ülesandele teises laadis. Kuigi tehnika arendamiseks on välja pakutud erinevaid meetodeid, eeldavad nad sageli, et ülekandeväljund järgib deltajaotust ja seega ei saa nende mudelid anda erinevaid stiiliülekande tulemusi antud sisendteksti jaoks. Piirangu lahendamiseks pakume välja ühest mitmele tekstistiilis ülekande raamistiku. Erinevalt varasematest töödest, mis õpivad üks-ühele kaardistamist, mis teisendab sisendlause üheks väljundvlauseks, õpib meie lähenemisviis üks-paljudele kaardistamist, mis suudab teisendada sisendlause mitmeks erinevaks väljundvlauseks, säilitades sisendsisu. See saavutatakse vastandliku koolituse rakendamisega latentse lagunemise skeemiga. Täpsemalt lagundame sisendlause latentse esituse laadikoodiks, mis tabab keelelaadi variatsiooni, ja sisukoodiks, mis kodeerib keelelaadist sõltumatut sisu. Seejärel kombineerime sisukoodi stiiliülekande väljundi genereerimiseks stiilikoodiga. Kombineerides sama sisukoodi erineva stiilikoodiga, genereerime erineva stiiliülekande väljundi. Kavandatud lähenemisviisi tõhusust kinnitavad ulatuslikud eksperimentaalsed tulemused koos võrdlustega mitmete avalike andmekogumite tekstistiili edastamise meetoditega, kasutades mitmesuguseid tulemuslikkuse mõõdikuid.', 'bn': 'টেক্সট স্টাইল পরিবর্তন অন্য ধরনের ধরনে একটি টেক্সট পুনরায় প্রতিরোধ করার কাজের কথা উল্লেখ করে। যদিও শিল্পের অবস্থা উন্নত করার প্রস্তাবিত বিভিন্ন পদ্ধতি প্রস্তাব করা হয়েছে, তারা প্রায়শই মনে করে যে পরিবর্তন আউটপুট ডেল্টা বিতরণের অনুসরণ করে, আর তাই ত সীমাবদ্ধতা নিয়ে কথা বলার জন্য, আমরা এক-থেকে অনেক টেক্সট স্টাইল পরিবর্তনের ফ্রেম প্রস্তাব করি। আউটপুটের বাক্যে একটি ইনপুটের বাক্য পরিবর্তন করা একটি মানচিত্র শিখতে পারে এর আগের কাজের বিপরীতে, আমাদের ক্ষেত্রে একটি মানচিত্র শিখতে পারে যা ইনপুটের বাক্য বিভিন্ন আউ This is achieved by applying adversarial training with a latent decomposition scheme.  বিশেষ করে, আমরা ইনপুটের সাম্প্রতিক প্রতিনিধিত্বের সাম্প্রতিক প্রতিনিধিত্বের সাথে একটি স্টাইল কোডের সাথে যা ভাষার স্টাইল পরিবর্তন এবং একটি  তারপর আমরা স্টাইল পরিবর্তনের আউটপুট তৈরি করার জন্য বিষয়বস্তু কোডের সাথে সংযুক্ত করি। একটি ভিন্ন ধরনের স্টাইল কোড সহযোগিতায় একই বিষয়বস্তু কোডের মাধ্যমে আমরা ভিন্ন ধরনের স্টাইল পরিবর্তন আউটপুট ত প্রস্তাবিত পদ্ধতির বিভিন্ন ধরনের কার্যকর মেট্রিক ব্যবহার করে বেশ কিছু পাবলিক ডাটাসেটের তুলনায় বেশ কিছু টেক্সট স্টাইল পরীক্ষার ফলা', 'cs': 'Přenos textového stylu odkazuje na úkol přeformulovat daný text v jiném stylu. Zatímco byly navrženy různé metody pro posouzení stavu techniky, často předpokládají, že přenosový výstup následuje delta distribuce, a proto jejich modely nemohou generovat různé výsledky přenosu stylu pro daný vstupní text. Pro řešení tohoto omezení navrhujeme rámec pro přenos textových stylů jeden na mnoho. Na rozdíl od předchozích prací, které se naučí mapování jednoho na jednoho, které převádí vstupní větu na jednu výstupní větu, se náš přístup naučí mapování jednoho na mnoho, které může převést vstupní větu na více různých výstupních vět, při zachování vstupního obsahu. Toho je dosaženo aplikací adversariálního tréninku s latentním schématem rozkladu. Konkrétně rozložíme latentní reprezentaci vstupní věty na kód stylu, který zachycuje variaci jazykového stylu a kód obsahu, který kóduje obsah nezávislý na jazykovém stylu. Následně kombinujeme kód obsahu s kódem stylu pro generování výstupu přenosu stylu. Kombinací stejného kódu obsahu s jiným kódem stylu vygenerujeme jiný výstup přenosu stylu. Rozsáhlé experimentální výsledky se srovnáním s několika přístupy k přenosu textových stylů na více veřejných datových sadách s využitím různých výkonnostních metrik ověřují efektivitu navrhovaného přístupu.', 'fi': 'Tekstityylisiirrolla tarkoitetaan tehtävää muotoilla tietty teksti uudelleen eri tyyliin. Vaikka tekniikan nykytilan edistämiseksi on ehdotettu erilaisia menetelmiä, ne usein olettavat siirtotuotoksen noudattavan delta-jakaumaa, joten niiden mallit eivät voi tuottaa erilaisia tyylisiirtotuloksia tietylle syöttötekstille. Rajoituksen korjaamiseksi ehdotamme yhdestä moneen -tyyppistä tekstinsiirtokehystä. Toisin kuin aikaisemmissa teoksissa, joissa opitaan yksi-yhteen-kartoitus, joka muuntaa syöttölauseen yhdeksi tuloslauseeksi, lähestymistapamme oppii yhden-moneen-kartoituksen, joka voi muuntaa syöttölauseen useiksi eri tuloslauseiksi säilyttäen syötteen sisällön. Tämä saavutetaan soveltamalla vastustajakoulutusta piilevällä hajoamisjärjestelmällä. Määritämme syöttölauseen piilevän esityksen tyylikoodiksi, joka tallentaa kielityylin vaihtelun, ja sisältökoodiksi, joka koodaa kielityylistä riippumattoman sisällön. Yhdistämme sitten sisältökoodin tyylikoodiin luodaksemme tyylinsiirtotuotoksen. Yhdistämällä saman sisältökoodin eri tyylikoodiin luomme erilaisen tyylinsiirtotuotoksen. Laajat kokeelliset tulokset, joissa vertaillaan useita tekstityylien siirtomenetelmiä useissa julkisissa aineistoissa käyttäen erilaisia suorituskykymittareita, vahvistavat ehdotetun lähestymistavan tehokkuutta.', 'ha': "Transform style Kuma a lokacin da aka buɗe wasu hanyoyin ayuka dõmin a gabatar da halin sanar da shi, sai su yi zato idan idan ana saka shirin ayuka ya bi raba tsakanin déta, don haka kuma misalinsu ba za'a iya ƙiƙiro matsalar transfer daban-daban zuwa matsayin injistan. To, don mu buɗa wata firam mai transfer matsayin zuwa-ɗaya. Tsarin aiki da ke gaba da aiki da za'a sanar da wani-zuwa-ramani wanda ya mayar da gefen cikin shirin ayuka zuwa filin aiki guda, aikin kananmu yana amfani da zanen-zuwa-yawa wanda zai iya mayar da cewa ɗin cikin shirin ayuka masu yawa, da kuma a tsare maɓallin input an. Wannan anã cika da shi da za'a yi amfani da wa'adin na motsi da wani na'urar wa'adadin na dama. Aka ƙayyade, muna koɗa irin gayarwa na ƙarani da aka shigar da shi zuwa wani kode na salon wanda ke riƙe variant na salon harshen da wani kodi na ƙunsa da tsarin-salon da ba'a hura ba. Sa'an nan kuma muna haɗa kodi na cikin kayan ƙunsa da kodi na salon da za'a ƙãga wani hoto mai shige. Ga mu haɗa kodi guda da kodi mai daban-style, za'a ƙiƙiro wata sabo mai amfani da shiryuwa. Mataimakin jarrabãwa masu yawa da tsarin misãlai masu sami da misãlai mai amfani da shiryoyin mutane masu yawa", 'sk': 'Prenos sloga besedila se nanaša na nalogo preoblikovanja določenega besedila v drugem slogu. Medtem ko so bile predlagane različne metode za napredek v stanju tehnike, pogosto domnevajo, da prenosni izhod sledi delta porazdelitvi, zato njihovi modeli ne morejo ustvariti različnih rezultatov prenosa sloga za dano vhodno besedilo. Za odpravo omejitve predlagamo okvir za prenos besedilnega sloga enega na več. V nasprotju s prejšnjimi deli, ki se učijo kartiranja ena proti ena, ki pretvori vhodni stavek v en izhodni stavek, se naš pristop nauči kartiranja ena proti več, ki lahko pretvori vhodni stavek v več različnih izhodnih stavkov, hkrati pa ohrani vhodno vsebino. To se doseže z uporabo kontradikcijskega usposabljanja s latentno shemo razgradnje. Natančneje, latentno predstavitev vhodnega stavka razdelimo na slogovno kodo, ki zajema različice sloga jezika, in vsebinsko kodo, ki kodira vsebino, neodvisno od sloga jezika. Nato združimo kodo vsebine s kodo sloga za ustvarjanje izhoda za prenos sloga. Z združevanjem iste vsebinske kode z drugačno slogovno kodo ustvarimo drugačen izhod prenosa sloga. Obsežni eksperimentalni rezultati s primerjavami z več pristopi prenosa besedilnih slogov na več javnih naborih podatkov z uporabo različnih meril uspešnosti potrjujejo učinkovitost predlaganega pristopa.', 'he': 'העברת סגנון טקסט מתייחסת למשימה של שינוי טקסט מסוים בסגנון אחר. למרות שהצעו שיטות שונות כדי להתקדם במצב האמנות, הם לעתים קרובות מניחים שהתוצאה של ההעברה עוקבת לפיצוץ דלתא, ולכן הדוגמנים שלהם לא יכולים ליצור תוצאות שונות של העברה בסגנון לטקסט כניסה מסוים. כדי להתמודד עם הגבלה, אנו מציעים מסגרת העברה בסגנון טקסט אחד-להרבה. בניגוד לעבודות קודמות שלמדות מפה אחד לאחד שמהפכה משפט כניסה למשפט יציאה אחד, הגישה שלנו לומדת מפה אחד לרבים שיכולה להפוך משפט כניסה למספר משפטים יציאה שונים, בזמן לשמור על תוכן כניסה. זה מושג באמצעות שימוש אימון יריבי עם תכנית פירוק מוסתרת. במיוחד, אנו פוצצים את היציגה הסתורית של המשפט הכניסה לקוד סגנון שמתפס את שונות הסגנון השפה וקוד התוכן שמקודד את התוכן עצמאי בסגנון השפה. ואז אנחנו משלבים את קוד התוכן עם קוד הסגנון לייצור תוצאה מעברת סגנון. By combining the same content code with a different style code, we generate a different style transfer output.  Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach.', 'jv': 'structural navigation politenessoffpolite"), and when there is a change ("assertivepoliteness structural navigation drawable-action Punika dipoleh nggawe nguasai aturan pakan karo akeh luwih nguasai winih. string" in "context_BAR_stringLink Name politenessoffpolite"), and when there is a change ("assertivepoliteness AllProgressBar', 'bo': 'ཡི་གེའི་བཟོ་རྣམ་གྱི་སྐྱེལ While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a delta distribution, and thus their models cannot generate different style transfer results for a given input text. ཚད་ལྟར་བཤད་ནི་ང་ཚོས་ཡིག་བཟོས་གཞུང་གཅིག་ལས་མང་པོ་ཞིག་གི་སྐྱེལ་འདྲེན་བྱས་ཆོག་ཡིན། In contrast to prior works that learn a one-to-one mapping that converts an input sentence to one output sentence, our approach learns a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content. འདི་ལ་མཐའ་མཇུག་གི་བསྡུས་བཤེར་གྱི་ཐབས་ལམ་ཞིག་གིས་གནད་དོན་ངལ་སྒྲིག་འགོད་བྱེད་ཀྱི་ཡོད། དམིགས་འཛུགས་ཀྱིས། ང་ཚོས་སྐད་ཡིག་བཟོས་འགྱུར་བ་དང་སྐད་རིགས་ཁོངས་ཀྱི་ནང་དོན་ཡིག་གི་ཚིག འུ་ཚོས་དབྱིབས་སྐྱེལ་འདྲེན་བྱེད་པར་ནང་དོན་ཀྱི་ཨང་རིམ་དང་བཟོ་བཅོས་བྱེད་ཀྱི་རྣམ་པ By combining the same content code with a different style code, we generate a different style transfer output. Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach.'}
{'en': 'Automatically Ranked Russian Paraphrase Corpus for Text Generation R ussian Paraphrase Corpus for Text Generation', 'ar': 'تم تصنيف مجموعة إعادة صياغة النص الروسية تلقائيًا لتوليد النص', 'pt': 'Corpus de paráfrases russas classificadas automaticamente para geração de texto', 'fr': 'Corpus de paraphrase russe classé automatiquement pour la génération de texte', 'es': 'Corpus de paráfrasis ruso clasificado automáticamente para la generación de texto', 'ja': 'テキスト生成のためのロシア語パラフレーズコーパスの自動ランク付け', 'zh': '文本自名俄语释义语料库', 'hi': 'पाठ जनरेशन के लिए स्वचालित रूप से रैंक रूसी व्याख्या कॉर्पस', 'ru': 'Автоматически ранжированный русский перефразированный корпус для генерации текста', 'ga': 'Corpas Athróg na Rúise atá Rangaithe go Uathoibríoch do Ghiniúint Téacs', 'ka': 'ტექსტის შექმნა', 'hu': 'Automatikusan rangsorolt orosz parafrasz Corpus szöveggeneráláshoz', 'el': 'Αυτόματα ταξινομημένο ρωσικό σώμα παραφράσεων για τη δημιουργία κειμένου', 'kk': 'Мәтін құру үшін автоматты түрде орыс парафрасы', 'it': 'Corpo di parafrasi russo classificato automaticamente per la generazione di testo', 'lt': 'Automatically Ranked Russian Paraphrase Corpus for Text Generation', 'mk': 'Автоматски рангиран руски парафразен корпус за генерација на текст', 'ms': 'Korpus Parafrasa Rusia Dijajar secara Automatik untuk Jenerasi Teks', 'ml': 'പദാവലി ജനിപ്പിക്കുന്നതിനുള്ള റഷ്യന്\u200d പാരാപ്പസ് കോര്\u200dപ്സ് സ്വയമായി റാങ്കിച്ചു', 'mt': 'Korp tal-Parafrażi Russa b’Klassifika Awtomatika għall-Ġenerazzjoni tat-Test', 'mn': 'Текст үүсгэх үеийн автоматаар Российн Paraphrase Corpus', 'no': 'Automatisk rask russisk parafrase- korpus for tekstgenerering', 'pl': 'Automatycznie rankingowany rosyjski korpus parafrazy dla generowania tekstu', 'ro': 'Clasat automat în limba rusă Parafrază Corpus pentru generarea textului', 'si': 'ස්වයංක්\u200dරමයෙන්ම රුසියානු ප්\u200dරතිචාර ප්\u200dරතිචාර කර්පුස්', 'so': 'Horumarinta baaraandegista Ruushka', 'sr': 'Автоматично почињена русна парафраза корпус за тексту генерацију', 'ta': 'Name', 'sv': 'Automatiskt rankad rysk parafraskorpus för textgenerering', 'ur': 'روسی پارافریز کورپوس کو متن پیدائش کے لئے آٹوٹی سے رنگ کیا گیا ہے', 'uz': 'Name', 'vi': 'Dùng cụm từ kiểu Nga tự động tạo ra văn bản', 'bg': 'Автоматично класиран руски парафразен корпус за генериране на текст', 'da': 'Automatisk klassificeret russisk parafrasekorpus til tekstgenerering', 'nl': 'Automatisch gerangschikt Russisch Parafrase Corpus voor Tekst Generatie', 'hr': 'Automatski pokrenut ruski parafraza korpus za generaciju teksta', 'id': 'Korpus Parafrasa Rusia Berbaris Otomatis untuk Generasi Teks', 'de': 'Automatisch ranked Russian Paraphrase Corpus für die Textgenerierung', 'ko': '텍스트 생성에 사용되는 자동 정렬 러시아어 해석 자료 라이브러리', 'fa': 'کرپوس پارافریز روسیه برای تولید متن خودکار', 'sw': 'Kikosi cha Paraphrase cha Urusi kwa ajili ya Uzalishaji wa Maandishi', 'tr': 'Metin Jeşirmek üçin Otomatik Aýratyn Parafraz Buýruky', 'af': 'Automaties Rank Russiese Parafrase Korpus vir Teks Genereering', 'sq': 'Automatically Ranged Russian Paraphrase Corpus for Text Generation', 'hy': 'Comment', 'am': 'ፖርት', 'bn': 'টেক্সট প্রজন্মের জন্য স্বয়ংক্রিয়ভাবে রাশিয়ান প্যারাপ্রাপ্ত কোর্পুস রেঙ্ক করা হয়েছে', 'az': 'M톛tn M톛xluqat캼', 'bs': 'Automatski pokrenut ruski parafraza korpus za generaciju teksta', 'ca': 'Automàticament classificat Russian Paraphrase Corpus per a generar text', 'cs': 'Automaticky hodnocený ruský parafrázový korpus pro generování textu', 'et': 'Automaatne vene parafraasikorpus teksti genereerimiseks', 'fi': 'Automaattisesti luokiteltu Venäjän parafraasikorpus tekstin tuottamista varten', 'jv': 'Sample rate', 'sk': 'Samodejno razvrščen ruski parafrazni korpus za ustvarjanje besedila', 'he': 'גופוס פראפרזיה רוסי מורכב באופן אוטומטי לייצור טקסט', 'ha': 'KCharselect unicode block name', 'bo': 'རང་འགུལ་གྱིས་ཡི་གེའི་བཟོ་བརྗོད་ལ་སྔོན་ཤུགས་ཀྱི་སྐོར་གཟུགས་རིས་རང་འགུལ་གྱིས་རྐྱང་པ'}
{'en': 'The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first  corpus  of such type in Russian computational linguistics. Existing manually annotated paraphrase datasets for Russian are limited to small-sized ParaPhraser corpus and ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and plagiarism detection, sentence similarity and relatedness estimation, etc. Due to size restrictions, these  datasets  can hardly be applied in end-to-end text generation solutions. Meanwhile,  paraphrase generation  requires a large amount of training data. In our study we propose a solution to the problem : we collect, rank and evaluate a new publicly available headline paraphrase corpus (ParaPhraser Plus), and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.', 'ar': 'تركز المقالة على التطوير التلقائي وتصنيف مجموعة كبيرة من المواد لتوليد إعادة الصياغة الروسية والتي تثبت أنها أول مجموعة من هذا النوع في اللغويات الحاسوبية الروسية. تقتصر مجموعات بيانات إعادة الصياغة الحالية المعلقة يدويًا للغة الروسية على مجموعة ParaPhraser و ParaPlag صغيرة الحجم مناسبة لمجموعة من مهام البرمجة اللغوية العصبية ، مثل إعادة الصياغة واكتشاف الانتحال وتشابه الجملة وتقدير الارتباط ، وما إلى ذلك نظرًا لقيود الحجم ، يمكن لمجموعات البيانات هذه بالكاد يتم تطبيقها في حلول إنشاء النص من طرف إلى طرف. وفي الوقت نفسه ، يتطلب إنشاء إعادة الصياغة قدرًا كبيرًا من بيانات التدريب. في دراستنا ، نقترح حلاً للمشكلة: نقوم بجمع وتصنيف وتقييم عنوان جديد متاح للجمهور يعاد صياغة النص (ParaPhraser Plus) ، ثم نجري تجارب إنشاء نص مع التقييم اليدوي على المجموعات المصنفة تلقائيًا باستخدام بنية المحولات العالمية.', 'fr': "L'article se concentre sur le développement automatique et le classement d'un vaste corpus pour la génération de paraphrases russes qui s'avère être le premier corpus de ce type en linguistique computationnelle russe. Les ensembles de données de paraphrase annotés manuellement existants pour le russe sont limités aux corpus ParaPhraser et ParaPLAG de petite taille qui conviennent à un ensemble de tâches de PNL, telles que la détection de paraphrase et de plagiat, la similarité de phrases et l'estimation de la parenté, etc. En raison des restrictions de taille, ces ensembles de données peuvent difficilement être appliquées dans des solutions de génération de texte de bout en bout. Pendant ce temps, la génération de paraphrases nécessite une grande quantité de données d'entraînement. Dans notre étude, nous proposons une solution au problème\xa0: nous collectons, classons et évaluons un nouveau corpus de paraphrase de titre accessible au public (ParaPhraser Plus), puis nous réalisons des expériences de génération de texte avec évaluation manuelle de corpus classés automatiquement à l'aide de l'architecture Universal Transformer.", 'pt': 'O artigo está focado no desenvolvimento automático e classificação de um grande corpus para geração de paráfrases russas que prova ser o primeiro corpus desse tipo na linguística computacional russa. Os conjuntos de dados de paráfrases anotados manualmente para russo são limitados a corpus ParaPhraser e ParaPlag de tamanho pequeno, que são adequados para um conjunto de tarefas de PNL, como detecção de paráfrase e plágio, estimativa de similaridade e parentesco de frases, etc. Devido a restrições de tamanho, esses conjuntos de dados podem dificilmente pode ser aplicado em soluções de geração de texto de ponta a ponta. Enquanto isso, a geração de paráfrases requer uma grande quantidade de dados de treinamento. Em nosso estudo, propomos uma solução para o problema: coletamos, classificamos e avaliamos um novo corpus de paráfrase de título disponível publicamente (ParaPhraser Plus) e, em seguida, realizamos experimentos de geração de texto com avaliação manual em corpora classificados automaticamente usando a arquitetura Universal Transformer.', 'es': 'El artículo se centra en el desarrollo automático y la clasificación de un gran corpus para la generación de paráfrasis rusa que resulta ser el primer corpus de este tipo en la lingüística computacional rusa. Los conjuntos de datos de paráfrasis anotados manualmente existentes para ruso se limitan a un corpus ParaFraser y ParaPlag de pequeño tamaño, que son adecuados para un conjunto de tareas de PNL, como la detección de paráfrasis y plagio, la estimación de similitud y relación de oraciones, etc. Debido a las restricciones de tamaño, estos conjuntos de datos difícilmente pueden aplicado en soluciones integrales de generación de textos. Mientras tanto, la generación de paráfrasis requiere una gran cantidad de datos de entrenamiento. En nuestro estudio proponemos una solución al problema: recopilamos, clasificamos y evaluamos un nuevo corpus de paráfrasis de titulares disponible públicamente (ParaPhraser Plus), y luego realizamos experimentos de generación de texto con evaluación manual en corpus clasificados automáticamente utilizando la arquitectura Transformer Universal.', 'ja': 'この記事では、ロシアのパラフレーズ生成のための大規模なコーパスの自動開発とランク付けに焦点を当てています。これは、ロシアの計算言語学におけるこのようなタイプのコーパスの最初のものであることが証明されています。ロシア語用の既存の手動注釈付きパラフレーズデータセットは、パラフレーズおよび剽窃の検出、文の類似性および関連性の推定などの一連のNLPタスクに適した小さなParaPhraserコーパスおよびParaPlagに限定されます。サイズ制限のため、これらのデータセットはエンドツーエンドのテキスト生成ソリューションにはほとんど適用できません。一方、パラフレーズ生成には大量のトレーニングデータが必要です。私たちの研究では、問題の解決策を提案します。新しい公開されている見出しパラフレーズコーパス（ ParaPhraser Plus ）を収集、ランク付けして評価し、ユニバーサル変換アーキテクチャを使用して自動的にランク付けされたコーパスで手動で評価するテキスト生成実験を行います。', 'hi': 'लेख रूसी व्याख्या पीढ़ी के लिए एक बड़े कॉर्पस के स्वचालित विकास और रैंकिंग पर केंद्रित है जो रूसी कम्प्यूटेशनल भाषाविज्ञान में इस तरह के प्रकार का पहला कॉर्पस साबित होता है। रूसी के लिए मौजूदा मैन्युअल रूप से एनोटेट किए गए पैराफ्रेज डेटासेट छोटे आकार के पैराफ्रेजर कॉर्पस और पैराप्लैग तक सीमित हैं जो एनएलपी कार्यों के एक सेट के लिए उपयुक्त हैं, जैसे कि व्याख्या और साहित्यिक चोरी का पता लगाना, वाक्य समानता और संबंधितता अनुमान, आदि। आकार प्रतिबंधों के कारण, इन डेटासेट को शायद ही एंड-टू-एंड टेक्स्ट जनरेशन समाधानों में लागू किया जा सकता है। इस बीच, व्याख्या पीढ़ी को बड़ी मात्रा में प्रशिक्षण डेटा की आवश्यकता होती है। हमारे अध्ययन में हम समस्या के समाधान का प्रस्ताव करते हैं: हम एक नए सार्वजनिक रूप से उपलब्ध शीर्षक व्याख्या कॉर्पस (पैराफ्रेजर प्लस) को इकट्ठा करते हैं, रैंक करते हैं और मूल्यांकन करते हैं, और फिर यूनिवर्सल ट्रांसफॉर्मर आर्किटेक्चर का उपयोग करके स्वचालित रूप से रैंक किए गए कॉर्पोरेट पर मैनुअल मूल्यांकन के साथ पाठ पीढ़ी के प्रयोग करते हैं।', 'zh': '本文之重,俄语释义生大语料库自开排名,当语料库证俄语算语言学中第一语料库。 今俄语手动注释义数集止于一组NLP小释义语料库、PraPlag,如释义、抄检,句相似性与相关性料等。 以大小限之,其数难用于端端文本解决方案。 同时,释义生须大练数。 臣等考之,解决方案:臣等收拾排名与评估一新公可标题释义语料库(ParaPhraser Plus),然后用通用变压器架构自行之语料库手动估本生实验。', 'ru': 'Статья посвящена автоматическому развитию и ранжированию большого корпуса для генерации русских парафраз, который оказался первым корпусом такого типа в российской вычислительной лингвистике. Существующие ручные аннотированные наборы данных парафраз для русского языка ограничены малогабаритным корпусом ParaPhraser и ParaPlag, которые подходят для набора задач NLP, таких как обнаружение парафраз и плагиата, оценка сходства и родства предложений и т. д. Из-за ограничений по размеру эти наборы данных вряд ли могут быть применены в комплексных решениях для генерации текста. Между тем, генерация перефразов требует большого объема обучающих данных. В нашем исследовании мы предлагаем решение проблемы: мы собираем, ранжируем и оцениваем новый общедоступный корпус парафраз (ParaPhraser Plus), а затем проводим эксперименты по генерации текста с ручной оценкой на автоматически ранжируемых телах с использованием архитектуры Universal Transformer.', 'ga': 'Tá an t-alt dírithe ar fhorbairt uathoibríoch agus rangú a dhéanamh ar chorpas mór chun athinsint na Rúise a ghiniúint, arb é an chéad chorpas dá leithéid i dteangeolaíocht ríomhaireachtúil na Rúise é. Tá tacair sonraí athinsinte anótáilte láimhe reatha don Rúisis teoranta do chorpas ParaPhraser beagmhéide agus ParaPlag atá oiriúnach do thacar tascanna NLP, mar athinsint agus brath bradaíl, cosúlacht abairtí agus meastachán gaolmhar, etc. Mar gheall ar shrianta méide, is féidir leis na tacair sonraí seo ar éigean a chur i bhfeidhm i réitigh giniúna téacs ceann go ceann. Idir an dá linn, teastaíonn cuid mhór sonraí oiliúna chun athinsint a ghiniúint. Inár staidéar molaimid réiteach ar an bhfadhb: bailímid, déanaimid rangú agus luacháil ar chorpas ceannlíne nua atá ar fáil go poiblí (ParaPhraser Plus), agus ansin déanaimid turgnaimh giniúna téacs le meastóireacht láimhe ar chorpas rangaithe go huathoibríoch ag baint úsáide as ailtireacht an Trasfhoirmeora Uilíoch.', 'el': 'Το άρθρο επικεντρώνεται στην αυτόματη ανάπτυξη και ταξινόμηση ενός μεγάλου σώματος για τη ρωσική παράφραση, το οποίο αποδεικνύεται ότι είναι το πρώτο σώμα αυτού του τύπου στη ρωσική υπολογιστική γλωσσολογία. Τα υπάρχοντα χειροκίνητα σχολιασμένα σύνολα δεδομένων παραφράσεων για τα ρωσικά περιορίζονται σε μικρό μέγεθος σώμα και τα οποία είναι κατάλληλα για ένα σύνολο εργασιών όπως η ανίχνευση παραφράσεων και λογοκλοπής, η εκτίμηση ομοιότητας και συσχέτισης των προτάσεων, κλπ. Λόγω περιορισμών μεγέθους, αυτά τα σύνολα δεδομένων δύσκολα μπορούν να εφαρμοστούν σε ολοκληρωμένες λύσεις δημιουργίας κειμένου. Εν τω μεταξύ, η παραγωγή παραφράσεων απαιτεί μεγάλο ποσό δεδομένων κατάρτισης. Στη μελέτη μας προτείνουμε μια λύση στο πρόβλημα: συλλέγουμε, ταξινομούμε και αξιολογούμε ένα νέο δημοσίως διαθέσιμο σώμα παραφράσεων τίτλου (και στη συνέχεια διεξάγουμε πειράματα δημιουργίας κειμένου με χειρωνακτική αξιολόγηση σε αυτόματα ταξινομημένα σώματα χρησιμοποιώντας την αρχιτεκτονική του μετασχηματιστή.', 'hu': 'A cikk egy olyan nagy korpusz automatikus fejlesztésére és rangsorolására összpontosít, amely az orosz számítástechnikai nyelvészet első ilyen típusú korpusza. A meglévő, manuálisan jegyzetelt parafráz-adatkészletek orosz nyelvre korlátozódnak a kis méretű ParaPhraser corpusra és ParaPlagra, amelyek alkalmasak egy sor NLP feladatra, mint például parafráz- és plagizációs felismerés, mondat hasonlóság és kapcsolattartás becslése stb. A méretkorlátozások miatt ezek az adatkészletek aligha alkalmazhatók végpontos szöveggenerálási megoldásokban. Eközben a parafráz generálása nagy mennyiségű edzési adatot igényel. Tanulmányunkban a probléma megoldását javasoljuk: összegyűjtjük, rangsoroljuk és értékeljük egy új, nyilvánosan elérhető parafráz korpuszt (ParaPhraser Plus), majd kézi értékeléssel végezzük el a szöveggenerációs kísérleteket automatikusan rangsorolt korpuszokon az Universal Transformer architektúrával.', 'ka': 'მარტიკაში ასეთი ტიპის პირველი კორპუსი როგორც კომპუტაციალური ლენგლისტიკში იქნება. პარაფრესური კოპპუზერი და ParaPlag, რომლებიც შესაძლებელია NLP დავალებისთვის, როგორც პარაფრესური და პლაგიარიზმის განახლება, სიტყვების განახლება და შესაძლებელობის განახლება, ანუ. ზომის განახლებისთვის, ეს მონაცემების კონფიგურაციის განახლებაში არაფერად შეიძლება დააყენებ საშუალოდ, პარაფრაზეზის განვითარება მნიშვნელოვანი მონაცემები იჭირდება. ჩვენი სწავლაში პრობლემაზე გავაკეთებთ გარეშე: ჩვენ კომპერაციას, რენეციას და გავაკეთებთ ახალი სახელსაწყოთა სახელსაწყოთა სახელსაწყოთა პარაფრაზერ პლუს, და შემდეგ ტექსტის წარმოიქმნა ექსპერიმენტები,', 'it': "L'articolo è incentrato sullo sviluppo automatico e la classificazione di un corpus di grandi dimensioni per la generazione di parafrasi russa che si rivela il primo corpus di questo tipo nella linguistica computazionale russa. Gli attuali set di dati di parafrasi annotati manualmente per il russo sono limitati a corpus ParaPhraser e ParaPlag di piccole dimensioni che sono adatti per una serie di attività NLP, come il rilevamento di parafrasi e plagio, la stima della somiglianza delle frasi e della relazione, ecc A causa delle restrizioni di dimensione, questi set di dati difficilmente possono essere applicati in soluzioni end-to-end di generazione di testo. Nel frattempo, la generazione di parafrasi richiede una grande quantità di dati di allenamento. Nel nostro studio proponiamo una soluzione al problema: raccogliamo, classifichiamo e valutiamo un nuovo corpus di parafrasi pubblicamente disponibile (ParaPhraser Plus), e poi eseguiamo esperimenti di generazione di testo con valutazione manuale su corpi classificati automaticamente utilizzando l'architettura Universal Transformer.", 'kk': 'Мақаланың автоматты түрде өзгерту және россияның парафразын құру үшін үлкен корпус көмегімен көмектесілген. Бұл россияның компьютерлік лингвистикалық тілдерінің алғашқы корпус болуы Бар қолмен белгіленген парафраз деректер жиындары Руссия үшін кішкентай өлшемі ParaPhraser corpus және ParaPlag деген NLP тапсырмалар жиынына шектелген, мысалы парафраз және плагитаризм анықтауы, сөздердің ұқсастығы және қатынастығы, т. б. өлшемі шектеуден себебі бұл деректер жиындары мәтін құр Осы уақытта парафраның құрылуы үлкен оқыту деректерін талап етеді. Біздің зерттеулерімізде мәселеге шешім келтіріп тұрмыз: біз жаңа жалпы айдарының парафразы корпус (ParaPhraser Plus) дегенді жинап, реттеу және оқыту үшін, содан кейін Universal Transformer архитектурасын қолданып, автоматты түрде орнатылған корпора', 'lt': 'Šiame straipsnyje daugiausia dėmesio skiriama didelės Rusijos parafrazės kartai skirto korpuso automatiniam kūrimui ir klasifikavimui, kuris pasirodo kaip pirmasis tokio tipo korpusas Rusijos kompiuterinėje kalboje. Esami rankiniu būdu pažymėti rusų parafrazių duomenų rinkiniai apsiriboja mažais ParaPhraser korpus ir ParaPlag, kurie yra tinkami NLP užduotims, pvz., parafrazių ir plagiarizmo nustatymui, sakinių panašumui ir sąsajų vertinimui ir t. t. Dėl dydžio apribojimų šie duomenų rinkiniai vargu ar gali būti taikomi galutiniame teksto kūrimo sprendime. Tuo tarpu parafrazių kūrimui reikia daug mokymo duomenų. Mūsų tyrime siūlome išspręsti šią problem ą: renkame, klasifikuojame ir vertiname naują viešai prieinamą antraštinę parafrazės korpusą (ParaPhraser Plus), o vėliau atliksime teksto generacijos eksperimentus, atliekant rankinį automatinio klasifikavimo korpusą naudojant Universal Transformer architektūrą.', 'mk': 'Написот се фокусира на автоматски развој и рангирање на голем труп за руската генерација на парафрази, кој се покажува дека е првиот труп на ваков тип во руската компјутативна јазика. Постојаните рачно анотирани парафразни датотеки за руски се ограничени на мали парафразни корпуси и параПлаг кои се соодветни за множина задачи на НЛП, како што се парафрази и детекција на плагијаризам, сличност на реченици и проценка на поврзаност итн. Поради ограничувањата на големината, овие датотеки едвај можат да се применат во решенијата за генерација на тек Во меѓувреме, генерацијата на парафрази бара голема количина податоци за обука. Во нашата студија предложуваме решение на проблемот: собираме, рангираме и оценуваме нов јавно достапен заглавен парафразен корпус (ParaPhraser Plus), а потоа спроведуваме експерименти за генерација на текст со рачна оценка на автоматски рангиран корпус користејќи ја универзалната трансформна архитектура.', 'ml': 'റഷ്യന്\u200d പാരാഫ്രേസ് തലമുറയ്ക്കുള്ള ഒരു വലിയ കോര്\u200dപ്പസിന്\u200dറെ സ്വയം വികസിക്കുകയും ചെയ്യുന്നതിന്\u200dറെ പേരിലാണ് ഈ ലേഖനം റുഷ്യന്\u200d കണക്കു രുഷ്യന്റെ കൈയ്യില്\u200d നിലവിലുള്ള പാരാഫ്രസ് കോര്\u200dപ്പസും പാരാപ്പ്ലാഗിന്റെയും കൈയ്യില്\u200d ക്രമപ്പെടുത്തിയിരിക്കുന്ന പാരാഫ്രസ് ഡാറ്റാസറ്റേറ്റുകള്\u200d പരിധികളാണ്. പാരാഫ്രസ് ജോലികള്\u200dക്കും പ്രയോഗിക്കാന്\u200d  Meanwhile, paraphrase generation requires a large amount of training data.  ഞങ്ങളുടെ പഠനത്തില്\u200d ഞങ്ങള്\u200d പ്രശ്നത്തിന് ഒരു പരിഹാരം പ്രായണം ചെയ്യുന്നു: ഞങ്ങള്\u200d ഒരു പുതിയ തലക്കെട്ടില്\u200d ലഭ്യമായ ഹെഡ്ലൈന്\u200d പാരാഫ്രസ് കോര്\u200dപ്പുസിനെ ശേഖരിക്കുകയും, പിന്നീട്', 'ms': 'Artikel ini fokus pada pembangunan automatik dan rangkaian mayat besar untuk generasi parafrasa Rusia yang membuktikan menjadi mayat pertama jenis dalam bahasa komputasi Rusia. Set data parafrasa yang ada secara manual yang dicatat untuk Rusia terbatas kepada mayat ParaPhraser kecil dan ParaPlag yang sesuai untuk set tugas NLP, seperti pengesan parafrasa dan plagiarism, persamaan kalimat dan penghargaan hubungan, dll. Kerana batasan saiz, set data ini hampir tidak boleh dilaksanakan dalam penyelesaian generasi teks akhir-akhir. Sementara itu, generasi parafrasa memerlukan sejumlah besar data latihan. Dalam kajian kami kami cadangkan penyelesaian untuk masalah ini: kami mengumpulkan, mengumpulkan dan menilai tubuh parafrasa utama baru yang tersedia secara awam (ParaPhraser Plus), dan kemudian melakukan eksperimen generasi teks dengan penilaian manual pada tubuh yang berturut secara automatik menggunakan arkitektur Universal Transformer.', 'mt': 'L-artikolu huwa ffukat fuq l-iżvilupp awtomatiku u l-klassifikazzjoni ta’ korpus kbir għall-ġenerazzjoni ta’ parafrażi Russa li juri li huwa l-ewwel korpus ta’ tali tip fil-lingwistika komputattiva Russa. Settijiet ta’ dejta eżistenti ta’ parafrażijiet annotati manwalment għar-Russu huma limitati għal ParaPhraser corpus u ParaPlag ta’ daqs żgħir li huma adatti għal sett ta’ kompiti NLP, bħall-identifikazzjoni ta’ parafrażijiet u plaġjariżmu, similarità tas-sentenza u stima tar-relazzjoni, eċċ. Minħabba restrizzjonijiet tad-daqs, dawn is-settijiet ta’ dejta bilkemm jistgħu jiġu applikati f’soluzzjonijiet ta’ ġenerazzjoni tat-test minn tarf sa tarf. Sadanittant, il-ġenerazzjoni tal-parafrażi teħtieġ ammont kbir ta’ dejta ta’ taħriġ. Fl-istudju tagħna nipproponu soluzzjoni għall-problem a: niġbru, nikklassifikaw u nivvalutaw korpus ġdid ta’ parafrażi prinċipali disponibbli għall-pubbliku (ParaPhraser Plus), u mbagħad nagħmlu esperimenti ta’ ġenerazzjoni tat-test b’evalwazzjoni manwali fuq korpura kklassifikata awtomatikament bl-użu tal-arkitettura tat-Trasformer Universali.', 'pl': 'Artykuł skupia się na automatycznym opracowaniu i rankingu dużego korpusu do generowania rosyjskich parafraz, który okazuje się pierwszym tego typu korpusem w rosyjskiej językoznawstwie obliczeniowej. Istniejące ręcznie annotowane zestawy danych parafraz dla języka rosyjskiego ograniczają się do małych korpusów ParaPhraser i ParaPlag, które są odpowiednie do zestawu zadań NLP, takich jak wykrywanie parafraz i plagiatu, podobieństwo zdań i szacowanie powiązaności itp. Ze względu na ograniczenia wielkości zbiory danych nie mogą być stosowane w kompleksowych rozwiązaniach generowania tekstu. Tymczasem generowanie parafraz wymaga dużej ilości danych treningowych. W naszym opracowaniu proponujemy rozwiązanie tego problemu: zbieramy, rankingujemy i oceniamy nowy publicznie dostępny korpus parafrazy nagłówkowych (ParaPhraser Plus), a następnie przeprowadzamy eksperymenty generowania tekstu z ręczną oceną korpusów automatycznie rankingowanych za pomocą architektury Universal Transformer.', 'ro': 'Articolul se concentrează pe dezvoltarea automată și clasificarea unui corpus mare pentru generarea de parafraze rusești, care se dovedește a fi primul corpus de acest tip în lingvistica computațională rusă. Seturile de date existente cu parafrază adnotată manual pentru limba rusă sunt limitate la corpul ParaPhraser și ParaPlag de dimensiuni mici, care sunt potrivite pentru un set de sarcini PNL, cum ar fi detectarea parafrazei și plagiatului, estimarea similarității frazelor și relației etc. Din cauza restricțiilor de dimensiune, aceste seturi de date nu pot fi aplicate în soluții end-to-end de generare de text. Între timp, generarea parafrazei necesită o cantitate mare de date de antrenament. În studiul nostru propunem o soluție la problemă: colectăm, clasificăm și evaluăm un nou corpus de parafrază disponibil public (ParaPhraser Plus), apoi efectuăm experimente de generare de text cu evaluare manuală pe corpuri clasificate automat folosind arhitectura Universal Transformer.', 'no': 'Artikkelen er fokusert på automatisk utvikling og rangering av ein stor korpus for russisk parafrase-generering, som viser at det er den første korpusen av slik typen i russisk computational linguistics. Det eksisterande manuelt merkte parafrasiske datasett for russisk er begrenset til små storleik ParaPhraser corpus og ParaPlag som er passande for eit sett av NLP- oppgåver, slik som parafrase og plagiarism- oppdaging, setningsstyrke og forholdsstyrke osv. Denne datasetta kan vanskeleg brukast i løysingane for å laga tekst til slutt- til- slutt. I mellomtida krev eit stor mengd opplæringsdata for parafrasar. I studien vårt foreslår vi eit løsning til problemet: vi samler, rankerer og evaluerer eit nytt offentlig tilgjengeleg parafrase-korpus (ParaPhraser Plus), og så utfører tekstgenerasjonseksperimenter med manuelt evaluering på automatisk rankerert korpora med Universet Transformeringsarkitektur.', 'mn': 'Энэ баримт Российн тооцоололтын хэлний анхны корпус гэдгийг баталж байдаг. Орос улсын барилга барилгасан парафрейз өгөгдлийн сангууд нь жижиг хэмжээний ParaPhraser corpus болон ParaPlag-д хязгаарлагддаг. Энэ нь NLP даалгавруудын нэг хэмжээнд зөв бөгөөд парафрейзим, plagiarism нээлт, өгүүлбэр төстэй, харилцааны тооцоолол, т.б. хэмжээний шаардлагатай учраас, энэ өгөгдлийн сангууд Гэвч энэ үед хэлбэрийн үеэр маш их суралцах өгөгдлийн шаардлагатай. Бидний судалгаанд асуудлын шийдэл гаргаж байна: бид олон нийтэд ашиглаж байгаа дараа нь "ParaPhraser Plus" (ParaPhraser Plus)-ын дараа олон нийтэд ашиглаж байгаа дараа нь "Корпора"-ын хувьд автоматаар хуваалцаж байгаа "Корпора"-ын хувьд автоматаар хуваалца', 'sr': 'Članak je fokusiran na automatski razvoj i ranking velikog korpusa za rusku generaciju parafraze koji dokazuje da je prvi korpus takvog tipa na ruskom računalnom jeziku. Postoje rukopisane podatke o parafrazi za rusku ograničene su na male veličine ParaPhraser corpus i ParaPlag koji su odgovarajući za skup zadataka NLP-a, kao što su parafraza i otkrivanje plagijarizma, sličnost rečenica i procjena veze, itd. Zbog ograničenja veličine, te podatke se jedva mogu primjenjivati u rješenjima generacije teksta na kraju do kraja. U međuvremenu, generacija parafraza zahteva veliku količinu podataka o obuci. U našoj studiji predlažemo rješenje problem a: skupljamo, rankiramo i procjenjujemo novu javno dostupnu parafrazu korpusa (ParaPhraser Plus), a zatim izvršavamo eksperimente za generaciju teksta sa ručnom procjenom na automatski rankiranom korpusu koristeći univerzalnu arhitekturu transformera.', 'so': 'The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first corpus of such type in Russian computational linguistics.  Ruush waxaa ku qoran sawirada macluumaadka ku saabsan si qof ah oo uu qof ahaan u isticmaalo baaritaanka ka mid ah baaritaanka ParaPhraser corpus iyo ParaPlag oo u eg shuqullada NLP, tusaale ahaan baaritaanka paraphraska iyo plagiariska, isku mid iyo qiimeynta isku mid ah iyo qiimeynta xiriirka, sababtoo ah xadhiidka tirada, sawiradaas waxaa si adag looga codsan karaa dhammaadka ugu dambeeya xafiisyada qoraalka. Wakhtigaas waxaa loo baahan yahay macluumaad badan oo waxbarasho ah. Waxbarashada waxaynu horumarinaynaa xal ka qabsashada dhibaatada: waxaynu soo ururinnaa, baaritaanno, waxaana qiimeynaynaa qof cusub oo caadiga ah oo la isticmaalayo baaritaanka jaamacadda.', 'sv': 'Artikeln fokuserar på automatisk utveckling och rangordning av en stor korpus för rysk parafrasgeneration som visar sig vara den första korpus av sådan typ i rysk datorlingvistik. Befintliga manuellt kommenterade parafrasdatauppsättningar för ryska är begränsade till små ParaPhraser corpus och ParaPlag som är lämpliga för en uppsättning NLP-uppgifter, såsom parafrasdetektering och plagiatdetektering, meningsskillnader och uppskattning av samband, etc. På grund av storleksbegränsningar kan dessa datauppsättningar knappast tillämpas i slutgiltiga textgenereringslösningar. Samtidigt kräver parafrasgenerering en stor mängd träningsdata. I vår studie föreslår vi en lösning på problemet: vi samlar in, rangordnar och utvärderar en ny allmänt tillgänglig parafraskorpus (ParaPhraser Plus), och utför sedan textgenereringsexperiment med manuell utvärdering på automatiskt rangordnade korpus med hjälp av Universal Transformer-arkitekturen.', 'si': 'ලේඛනය ස්වයංක්\u200dරීය විකාසය සහ ලොකු කොර්පස් එක්ක රුසියානු ප්\u200dරභාව ප්\u200dරමාණය සඳහා ස්වයංක්\u200dරීය විකාශයේ පළමුවෙනි කොර්ප Exiting handly annuted paraPhraser Corpus and ParaPlag data sets for Russian are Limit to Little-Sized ParaPhraser Corpus and ParaPlag that are suited for a set of NLP Jobs, e.g. paraPhrase and Plagirism Detection, Sentence SimLarity and erltedness Estination, etc. Due to size Restricions, this data sets can Hardly be Application in end-to-end text generation options. අතර වෙලාවෙන්, පැරැෆ්\u200dරේස් ප්\u200dරසිද්ධානයට ලොකු ප්\u200dරශ්නයක් තොරතුරු අවශ්\u200dයයි. අපේ පරීක්ෂණයේ අපි ප්\u200dරශ්නයක් ගැන විස්තර කරන්න ප්\u200dරශ්නයක් තියෙනවා: අපි ප්\u200dරශ්නයක් සම්බන්ධ කරනවා, ප්\u200dරශ්නයක් සම්බන්ධ කරනවා, පස්සේ ප්\u200dරශ්නයක් ප්\u200dරශ', 'ta': 'இந்த கட்டுரை தன்னியக்கமாக உருவாக்கம் மற்றும் ஒரு பெரிய கார்புஸ் தலைமுறையில் கவனம் செலுத்தப்படுகிறது ருஷ்யன் கணக்கீட்டு மொழிகளில் உள்ள மொழ ருஷியனுக்கான தற்போதைய கைமுறையாக வெளிப்படுத்தப்பட்ட சொற்றொடர் தகவல் அமைப்புகள் சிறிய அளவான பாராPhraser குறியீடு மற்றும் ParaPlag வரையறுக்கப்பட்டுள்ளன, அது NLP செயல்களுக்கு பொருத்தமானது, சொற்றொடர் மற்றும் பிளாஜிரியல் கண இந்த நேரத்தில், சொற்றொடர் உருவாக்கி பயிற்சி தரவு பெரிய தேவைப்படுகிறது. எங்கள் ஆராய்ச்சியில் நாம் பிரச்சனைக்கான தீர்வை பரிந்துரைக்கிறோம்: நாங்கள் புதிய பொது கிடைக்கும் தலைப்பு சொற்றொடர் கோப்புகளை (ParaPhraser Plus) சேகரிக்கிறோம். பிறகு உரை உர', 'ur': 'یہ لکھا روسی پارافریز کی نسل کے لئے ایک بڑے کورپوس کی آٹوٹی توسعہ اور رنگ پر تمرکز کیا گیا ہے جو روسی کمپیوٹریسی زبان شناسی میں اس طرح کی پہلی کورپوس ہے۔ روسی کے لئے موجود پارافریز ڈیٹ سٹ چھوٹے سایز پارافریزر کورپوس اور پاراپلگ کے لئے محدود ہیں جو NLP ٹاکس کے لئے مناسب ہیں، جیسے پارافریز اور پیغاریسم ڈیٹ شناسایی، sentence similarity اور relatedness estimation، اور اگلے. اندازے کے باعث یہ ڈیٹ سٹ ٹاکس نسل کے حلوں میں بہت کم لازم کر سکتے ہیں. یہاں تک کہ پارافریز کی نسل بہت زیادہ تعلیم دادہ کی ضرورت ہے. ہمارے مطالعہ میں ہم مشکل کے لئے ایک حل پیشنهاد کرتے ہیں: ہم ایک نئی جمع کریں، رقم کریں اور ایک نئی عموماً موجود سرلین پارافریزہ کورپوس (ParaPhraser Plus) کا ارزش کریں، اور پھر ایک ٹیکسٹ نسل کی آزمائش کریں، جہانولوسٹ ترنسفور معماری کے مطالعہ سے اپنے ساتھ رق', 'vi': 'Bài báo được tập trung vào việc phát triển tự động và xếp hạng một tập thể lớn cho thế hệ liệt thuật của Nga, thứ mà có thể là tập thể đầu tiên của loại đó trong ngôn ngữ tổng tính Nga. Hiện các tập tin ảnh được đánh vần bằng tay cho Nga chỉ giới hạn cho tập đoàn ParaPhi Kiểu và ParaPlag a những chuyên mục thích hợp cho một nhóm công việc lập tư, như liệt giải và phát hiện mức độ, câu nói giống nhau và ước tính liên quan, v. Vì giới hạn kích thước, các tập tin này hầu như không thể áp dụng trong các biện pháp sản xuất văn bản cuối cùng. Trong khi đó, phân tích tâm lý cần một lượng lớn dữ liệu đào tạo. Trong nghiên cứu của chúng tôi đề xuất một giải pháp cho vấn đề: chúng tôi sưu tập, cấp bậc và đánh giá một tập thể hình ảnh siêu nhiên mới có sẵn công khai (Paranyser Plus), và sau đó thực hiện thí nghiệm sản xuất văn bản với đánh giá bằng tay về người hạ đẳng tự động sử dụng kiến trúc transformer Universal.', 'uz': "Ushbu maqola Ruscha paraphras generalida katta kopus taʼminlovchi avtomatik taʼminlovchi va tarqatishga foydalanadi. Bu birinchi Ruscha kompyuterlar tillaridagi birinchi harpus bo'lishi mumkin. Ruscha uchun qoʻlbola qoʻlbola taʼminlovchi paraphrase maʼlumotlari moslamalari chegara boʻlgan ParaPhraser corpus va ParaPlag, balki NLP vazifalar, masalan paraphras va plagiarism aniqlash, sentence oʻxshash va bogʻliq qiymatiga oʻxshash boʻlishi mumkin. Bu maʼlumotlar tartiblarning oʻlchamini oʻzgartirish uchun bu maʼlumotlar tuzuvlari tuzuvchi matn yozuvlariga qoʻllanmaydi. Bu paytda paraphrase generalida taʼminlovchi katta maʼlumot kerak. Biz o'qituvchimizda muammolar uchun xabar qilamiz: biz yangi barcha sarlavhasi paraphrase (ParaPhraser Plus) qiymatimiz va keyin matn generalining imtiyozlarini avtomatik tarjima qilish bilan avtomatik tarjima qilish uchun qoʻlbola qilamiz, Universal Transfer arkitektikasi yordamida.", 'da': 'Artiklen fokuserer på automatisk udvikling og rangering af et stort korpus til russisk parafrasegeneration, som viser sig at være det første korpus af denne type i russisk beregningslingvistik. Eksisterende manuelt kommenterede parafrase datasæt til russisk er begrænset til små størrelser ParaPhraser corpus og ParaPlag, som er egnede til et sæt NLP-opgaver, såsom parafrase og plagiatdetektion, sætningens lighed og relaterethed estimering osv. På grund af størrelsesbegrænsninger kan disse datasæt næppe anvendes i end-to-end tekstgenereringsløsninger. I mellemtiden kræver parafrasegenerering en stor mængde træningsdata. I vores undersøgelse foreslår vi en løsning på problemet: Vi indsamler, rangerer og evaluerer et nyt offentligt tilgængeligt overskriftsprophrasekorpus (ParaPhraser Plus), og derefter udfører tekstgenereringseksperimenter med manuel evaluering på automatisk rangerede corpora ved hjælp af Universal Transformer-arkitekturen.', 'nl': "Het artikel richt zich op de automatische ontwikkeling en rangschikking van een groot corpus voor het genereren van Russische parafrases, dat het eerste corpus van dit type in de Russische computerlinguïstiek blijkt te zijn. Bestaande handmatig geannoteerde parafrase datasets voor Russisch zijn beperkt tot kleine ParaPhraser corpus en ParaPlag die geschikt zijn voor een reeks NLP taken, zoals parafrase en plagiaat detectie, zinsgelijkenis en relatedness schatting, enz. Vanwege de grootte beperkingen, kunnen deze datasets nauwelijks worden toegepast in end-to-end tekstgeneratie oplossingen. Ondertussen vereist het genereren van parafrases een grote hoeveelheid trainingsgegevens. In ons onderzoek stellen we een oplossing voor het probleem voor: we verzamelen, rangschikken en evalueren een nieuw publiek beschikbaar headline parafrase corpus (ParaPhraser Plus), en voeren vervolgens experimenten met tekstgeneratie uit met handmatige evaluatie op automatisch gerangschikte corpora's met behulp van de Universal Transformer architectuur.", 'bg': 'Статията е фокусирана върху автоматичното разработване и класиране на голям корпус за генериране на руски парафрази, който се оказва първият такъв корпус в руската компютърна лингвистика. Съществуващите ръчно анотирани парафразни набори данни за руски език са ограничени до малки по размер корпус и които са подходящи за набор от задачи от НЛП, като откриване на парафрази и плагиатство, сходство на изречения и оценка на връзката и др. Поради ограниченията по размера тези набори от данни трудно могат да бъдат приложени в решения за генериране на текст от край до край. Междувременно генерирането на парафрази изисква голямо количество данни за обучение. В нашето проучване предлагаме решение на проблема: събираме, класираме и оценяваме нов публично достъпен корпус на парафразата на заглавията (и след това извършваме експерименти за генериране на текст с ръчна оценка на автоматично класирани корпуси, използвайки архитектурата на универсалния трансформатор.', 'hr': 'Članak je usredotočen na automatski razvoj i ranking velikog korpusa za rusku generaciju parafraze koji dokazuje da je prvi korpus takvog tipa na ruskom računalnom jeziku. Postoje ručno označene parafraze podatke za rusku ograničene su na male veličine ParaPhraser corpus i ParaPlag koji su odgovarajući za skup zadataka NLP-a, poput parafraze i otkrivanja plagijarizma, sličnosti rečenica i procjene odnosa itd. Zbog ograničenja veličine, te podatke se jedva mogu primjenjivati u rješenjima generacije teksta na kraju do kraja. U međuvremenu, generacija parafraza zahtijeva veliku količinu podataka o obuci. U našem ispitivanju predlažemo rješenje problem a: skupljamo, rankiramo i procjenjujemo novu javno dostupnu parafrazu korpusa (ParaPhraser Plus), a zatim izvršimo eksperimente za generaciju teksta s ručnom procjenom na automatski rankiranom korpusu koristeći univerzalnu arhitekturu transformera.', 'de': 'Der Artikel konzentriert sich auf die automatische Entwicklung und Rangfolge eines großen Korpus zur Generierung russischer Paraphrasen, der sich als erster Korpus dieser Art in der russischen Computerlinguistik erweist. Vorhandene manuell annotierte Paraphrasen-Datensätze für Russisch sind auf kleine ParaPhraser-Korpus und ParaPlag beschränkt, die für eine Reihe von NLP-Aufgaben geeignet sind, wie Paraphrasen- und Plagiatserkennung, Satzähnlichkeits- und Relatedness-Schätzungen usw. Aufgrund von Größenbeschränkungen können diese Datensätze kaum in End-to-End-Textgenerierungslösungen angewendet werden. Gleichzeitig erfordert die Paraphrasenerzeugung eine große Menge an Trainingsdaten. In unserer Studie schlagen wir eine Lösung für das Problem vor: Wir sammeln, ranken und evaluieren ein neues öffentlich zugängliches Headline-Paraphrasen-Korpus (ParaPhraser Plus) und führen dann Textgenerierungsexperimente mit manueller Auswertung auf automatisch gerankten Korpora mit Hilfe der Universal Transformer-Architektur durch.', 'id': 'Artikel ini fokus pada pembangunan otomatis dan rangkaian mayat besar untuk generasi parafrasa Rusia yang membuktikan menjadi mayat pertama dari jenis tersebut dalam bahasa komputasi Rusia. Seting data parafrasa yang ada secara manual yang dicatat untuk Rusia terbatas pada ukuran kecil ParaPhraser corpus dan ParaPlag yang cocok untuk set tugas NLP, seperti parafrasa dan deteksi plagiarisme, persamaan kalimat dan perhitungan hubungan, dll. Karena batasan ukuran, set data ini tidak dapat diaplikasikan dalam solusi generasi teks akhir-akhir. Sementara itu, generasi parafrasa membutuhkan jumlah besar data latihan. Dalam penelitian kami kami mengusulkan solusi untuk masalah ini: kami mengumpulkan, mempertimbangkan dan mengevaluasi sebuah parafrasa headline corpus baru yang tersedia secara publik (ParaPhraser Plus), dan kemudian melakukan eksperimen generasi teks dengan evaluasi manual pada corpora yang secara otomatis berturut-turut menggunakan arsitektur Universal Transformer.', 'ko': '본고의 중점은 러시아어 해석이 대형 어료 라이브러리를 생성하는 자동 개발과 정렬이다. 이것은 러시아어 계산언어학에서 처음으로 이런 어료 라이브러리이다.기존의 러시아어 인공 주석 해석 데이터 집합은 소형 해석 자료 라이브러리와 해석에만 한정되어 해석과 표절 검출, 문장의 유사성과 관련성 평가 등 NLP 임무에 적용된다. 크기 제한으로 인해 이러한 데이터 집합은 끝에서 끝까지 텍스트 생성 해결 방안에 응용하기 어렵다.동시에 해석의 생성은 대량의 훈련 데이터를 필요로 한다.우리의 연구에서 우리는 새로운 공개된 제목 해석 자료 라이브러리(Reporthraser Plus)를 수집, 배열, 평가한 다음에 Universal Transformer 체계 구조를 이용하여 자동으로 배열된 자료 라이브러리에 대해 수동 평가를 하고 텍스트 생성 실험을 하는 해결 방안을 제시했다.', 'fa': 'این مقاله روی توسعه اتوماتیک و رقابت یک کورپوس بزرگ برای نسل پارافریز روسیه تمرکز شده است که ثابت می\u200cکند اولین کورپوس این نوع در زبان\u200cهای کامپیوتری روسیه است. مجموعه\u200cهای داده\u200cهای پارافریز برای روسیه\u200cها به اندازه\u200cهای کوچک پارافریزر و پاراپلگ محدود می\u200cشوند که برای مجموعه کار NLP مناسب هستند، مثل پارافریز و شناسایی پاراگریز، شبیه\u200cسازی جمله\u200cها و ارزش ارتباطی، و غیر از آن. به دلیل محدودیت اندازه\u200cها، این مجموعه\u200cهای داده\u200cها به سختی در راه\u200cحل\u200cهای نسل متن به پایان در ضمن، نسل پارافراز نیاز به مقدار زیادی اطلاعات آموزش دارد. در مطالعه\u200cمون یک راه حل برای مشکل پیشنهاد می\u200cکنیم: ما یک پارافریزر Plus سر عنوان جدیدی را جمع می\u200cکنیم، درجه می\u200cگیریم و ارزش می\u200cدهیم که از طریق معماری تبدیل کننده جهانی استفاده می\u200cکند.', 'sw': 'The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first corpus of such type in Russian computational linguistics.  Maketi ya taarifa za upinzani zinazoonekana kwa mkono kwa Urusi yamepungua vifaa vikubwa vya ParaPhraser na ParaPlag ambavyo vinavyofaa kwa ajili ya mfululizo wa kazi za NLP, kama vile upunguzi wa maneno na upunguzi wa kitaalam, hukumu inayofanana na hisia za mahusiano, etc. Kutokana na vikwazo vya ukubwa, vikwazo hivi vya taarifa vinaweza kutumika vigumu katika ufumbuzi wa kizazi cha teknolojia cha mwisho. Wakati huo huo, kizazi cha upinzani kinahitaji takwimu kubwa za mafunzo. Katika utafiti wetu tunapendekeza suluhisho la tatizo: tunakusanya, vyeo na kutathmini makampuni mpya yanayopatikana hadharani (ParaPhraser Plus), na kisha kutekeleza majaribio ya kizazi cha maandishi kwa kutumia makampuni yanayotokana na vifaa vya kibinafsi kwa kutumia ujenzi wa Kimataifa.', 'sq': 'Artikli është përqëndruar në zhvillimin automatik dhe renditjen e një trupi të madh për gjeneratën e parafrazave ruse që provon të jetë trupi i parë i një lloji të tillë në gjuhën kompjuterike ruse. Të dhënat ekzistuese të parafrazave të anotuara manualisht për rusët janë të kufizuara në trupin ParaPhraser të vogël dhe ParaPlag të cilat janë të përshtatshme për një sërë detyrash NLP, të tilla si parafraza dhe zbulimi plagiarismi, ngjashmëria e fjalëve dhe vlerësimi i lidhjes, etj. Për shkak të kufizimeve të madhësisë, këto sëra të dhënash nuk mund të aplikohen në zgjidhjet e gjenerimit të tekstit nga fundi Ndërkohë, gjenerimi i parafrazave kërkon një sasi të madhe të dhënash treinimi. Në studimin tonë propozojmë një zgjidhje për problemin: ne mbledhim, rendisim dhe vlerësojmë një kufom të ri parafrazë të disponueshme publikisht (ParaPhraser Plus) dhe pastaj kryejmë eksperimente të gjenerimit të tekstit me vlerësim manual mbi kufomë të renditur automatikisht duke përdorur arkitekturën Universal Transformer.', 'af': "Die artikel is fokus op outomatiese ontwikkeling en rangering van 'n groot korpus vir Russiese parafrase generasie wat bevestig dat die eerste korpus van sodanige tipe in Russiese rekenaasjonale lingwisies is. Die bestaande hand aanmerkte parafrase datastel vir Russies is beperk na klein grootte ParaFraser corpus en ParaPlag wat is geskik vir 'n stel van NLP opdragte, soos parafrase en plagiarisme opdekking, sentence gelykenis en verwantigheid estimatie, ensfh. Dus grootte beperking, kan hierdie datastel skaars toewend word in einde- na- einde teks generasie oplossing. In middeltyd, parafrase generasie benodig 'n groot hoeveelheid onderwerp data. In ons studie voorstel ons 'n oplossing vir die probleem: ons versamel, rank en evalueer 'n nuwe openlik beskikbaar hooflyn paraphrase corpus (ParaPhraser Plus), en dan uitvoer teks generasie eksperimente met hand evaluering op outomatiese rangeerde korpora met die Universele Transformer-arkitektuur.", 'am': 'የኢንተርኔቱ ጽሑፉ የራሱ አካባቢ አፍሪካ ትውልድ ለራሱ የኮርፓስ ትውልድ መግለጫ ነው፡፡ የራሱ አካባቢ አካል እና የNLP ሥርዓቶች እንዲያስፈልጉ፣ paraphrase እና plagiarism ማግኘት፣ sentence ብጤናዊ እና ግንኙነት ቁጥር እና ግንኙነት ቁጥር እንደሆነ እነዚህ ዳታ-sets በመጨረሻ-መጨረሻ የጽሑፍ ትውልድ መፍትሄዎችን በሚያስቀምጡ በጭንቅ ይደረጋሉ፡፡ በዚያን ጊዜም የፓርላፊል ትውልድ ብዙ ትምህርት ዳታ ያስፈልጋል። በተማርነው ውስጥ ጉዳዩን መፍትሄ እናስፈልጋለን፡፡ አዲስ የህዝብ አዲስ የደረጃ የፊደል አርእስት ካርፓስ (ፓራፊር Plus) እናቆማለን፡፡', 'tr': 'Makal Rus parafraz döwletleri üçin awtomatik ösümäne we uly korpusyň bu şekilde Rus kompýuter lingwistiklerinde birinji köpüsi bolandygyny kanıtlaýar. Öň bar parafraz sanlary Russiýa üçin örän kiçi ölçü ParaPhraser corpus we ParaPlag-a süýtgedilýär. Bu parafraz we plagiýarism deteksi, sözleriň meňzeşliki we baglanylyk deňlenmesi üçin bir NLP zady üçin mümkin däldir. Ululyk mümkinçilikleri sebäpli, bu veri düzümleri metin netijesinde çözümlerde ýazmakda mümkin edýär. Bu arada, parafraz döredişi uly şekilde okuwçylyk maglumaty gerek. Biziň araşdyrymyzda mesele üçin çözüm teklip edip otyrýarys: biz jemgyýet, derejä we jemgyýet berilýän başlyg kellämiz parafraze korpusu (ParaPhraser Plus) we soňra Uniwersal Transformer arhitektegi ullanýän şekilde metin döredilişi deneyleri çykarýarys.', 'hy': 'Այս հոդվածը կենտրոնացված է մեծ մարմինի ավտոմատիկ զարգացման և դասակարգման վրա Ռուսաստանի պարաֆրեզի սերունդի համար, որը պարզվում է որպես այս տեսակի առաջին մարմինը Ռուսաստանի հաշվարկման լեզվաբանության մեջ: Ռուսաստանի համար գոյություն ունի ձեռքով գրված պարաֆրեզների տվյալներ սահմանափակված են փոքր չափերի պարաֆրեզերի մարմնի և պարաՊլագի միջոցով, որոնք համապատասխանում են մի շարք ՆԼՊ-ի առաջադրանքների համար, ինչպիսիք են պարաֆրեզները և պլագարիզմի հայտնաբերումը, նախադասությունների նմանությունը և կապվածությունը, և այլն: Քանի որ չափերի Միևնույն ժամանակ, պարաֆրեզների ստեղծումը պահանջում է մեծ քանակությամբ ուսուցման տվյալներ: In our study we propose a solution to the problem: we collect, rank and evaluate a new publicly available headline paraphrase corpus (ParaPhraser Plus), and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.', 'az': 'Bu məktub Rus parafraza nəsili üçün böyük bir korpusun otomatik gelişməsinə və səviyyətinə tərəf göstərildi ki, Rus kompjuterlərin dilində böyük ilk korpus olduğunu kanıtladı. Əvvəlki məlumatlar, parafraz və plagiarism tanıması, cümlələr bənzəriliyi və bağlılıq hesablaması üçün küçük ölçülü ParaPhraser corpus və ParaPlag işlərinə uyğun olan NLP işləri üçün qısılır. Bu məlumatlar böyüklük çətinliklərə görə bu məlumatlar məlumatların sonlarına gələn çətinliklərdə çox az uyğulanar. Bu arada, parafraz nəsili böyük təhsil məlumatlarına ehtiyac edir. Bizim araşdırmamızda problem üçün çözüm təbliğ edirik: biz dünya Transformer arhitektarını istifadə edərək yeni a çıq-aşkar başlıq çözümlü parafraz korpusu (ParaPhraser Plus) toplayırıq, sonra özünüzə dərəcəli korpora görə mətn nəsilindən təcrübə edərik.', 'bs': 'Članak je usredotočen na automatski razvoj i ranking velikog korpusa za rusku generaciju parafraze koji dokazuje da je prvi korpus takvog tipa na ruskom računalnom jeziku. Postojeće rukopisane podatke o parafrazi za rusku ograničene su na male veličine ParaPhraser corpus i ParaPlag koji su odgovarajući za skup zadataka NLP-a, poput parafraze i otkrivanja plagijarizma, sličnosti rečenica i procjene odnošnosti itd. Zbog ograničenja veličine, te podatke se jedva mogu primjenjivati na rješenja generacije teksta na kraju. U međuvremenu, generacija parafraza zahtijeva veliku količinu podataka o obuci. U našoj studiji predlažemo rješenje problem a: skupljamo, rankiramo i procjenjujemo novu javno dostupnu parafrazu korpusa (ParaPhraser Plus), a zatim izvršimo eksperimente za generaciju teksta sa ručnom procjenom o automatski rankiranom korpusu koristeći univerzalnu arhitekturu transformera.', 'ca': "L'article es centra en el desenvolupament automàtic i la classificació d'un gran cos per a la generació de parafrases russes que es demostra ser el primer cos d'aquest tipus en la lingüística computacional russa. Els conjunts de dades de parafrases anotats manualment en russos estan limitats a petits ParaPhraser corpus i ParaPlag que són apropiats per a un conjunt de tasques NLP, com la detecció de parafrases i plagiarisme, la similitud de frases i estimació de relació, etc. Gràcies a restriccions de tamany, aquests conjunts de dades gairebé no es poden aplicar en solucions de generació de text final a final. Mentrestant, la generació de parafrases requereix una gran quantitat de dades d'entrenament. En el nostre estudi proposem una solució al problem a: recollim, classifiquem i evaluem un nou cos de parafrases de títol (ParaPhraser Plus), i després fem experiments de generació de text amb una evaluació manual sobre corpores de classe automàtica utilitzant l'arquitectura Universal Transformer.", 'bn': 'এই প্রবন্ধে স্বয়ংক্রিয়ভাবে উন্নয়ন এবং রাশিয়ার প্রজন্মের জন্য বিশাল প্যারাফ্রেস প্রজন্মের জন্য একটি বিশাল কোর্পাসের দিকে মনোযোগ দেয়া  রাশিয়ার জন্য বিদ্যমান প্যারাফ্রাসার কোর্পাস এবং প্যারাপ্লাগের সীমাবদ্ধতা সীমাবদ্ধ, যারা এনএলপি কাজের জন্য যথাযথ, যেমন প্যারাফ্রেস এবং প্লাজিয়ারিম আবিষ্কার, শাস্তি একই ধরনের এবং সংশ্লিষ্ট সংশ্লিষ এদিকে, প্যারাফ্রেস প্রজন্মের বিশাল পরিমাণ প্রশিক্ষণের তথ্য প্রয়োজন। আমাদের গবেষণায় আমরা সমস্যার সমাধানের প্রস্তাব করছি: আমরা একটি নতুন প্রকাশ্য বিভিন্ন শিরোনামের প্যারাফ্রেস কোর্পাস (প্যারাফ্রাসার প্লাস) সংগ্রহ করি এবং পরে বিশ্ববিদ্যালয়ের পরি', 'cs': 'Článek je zaměřen na automatický vývoj a hodnocení velkého korpusu pro generaci ruských parafrází, který se ukáže jako první korpus tohoto typu v ruské výpočetní lingvistice. Existující manuálně anotované parafrázové datové sady pro ruštinu jsou omezeny na malý korpus ParaPhraser a ParaPlag, které jsou vhodné pro sadu úloh NLP, jako je detekce parafrází a plagiátů, odhad podobnosti vět a souvislosti atd. Vzhledem k omezení velikosti lze tyto datové sady sotva aplikovat v řešeních generování textu end-to-end. Generování parafráz zatím vyžaduje velké množství tréninkových dat. V naší studii navrhujeme řešení tohoto problému: shromažďujeme, hodnotíme a vyhodnocujeme nový veřejně dostupný korpus parafráze nadpisů (ParaPhraser Plus) a následně provádíme experimenty generace textů s manuálním vyhodnocením na automaticky hodnocených korpusech pomocí architektury Universal Transformer.', 'et': "Artiklis keskendutakse suure korpuse automaatsele arendamisele ja järjestamisele vene parafraaside genereerimiseks, mis osutub vene arvutuslingvistikas esimeseks sellist tüüpi korpuseks. Olemasolevad käsitsi annoteeritud parafraaside andmekogumid vene keeles piirduvad väikese suurusega ParaPhraser korpuse ja ParaPlag'iga, mis sobivad mitmesuguste NLP-ülesannete jaoks, nagu parafraaside ja plagiaadi tuvastamine, lausete sarnasuse ja seotuse hindamine jne. Suuruse piirangute tõttu ei saa neid andmekogumeid peaaegu lõpuni teksti genereerimise lahendustes rakendada. Samal ajal nõuab parafraasi genereerimine suurt hulka koolitusandmeid. Oma uuringus pakume välja lahenduse probleemile: kogume, järjestame ja hindame uut avalikult kättesaadavat pealkirjade parafraasikorpust (ParaPhraser Plus) ning seejärel teostame tekstigeneratsiooni katseid käsitsi hindamisega automaatselt järjestatud korpustele, kasutades Universal Transformer arhitektuuri.", 'fi': 'Artikkeli keskittyy venäjänkielisen parafraasien tuottamiseen tarkoitetun suuren korpusen automaattiseen kehittämiseen ja luokitteluun, joka osoittautuu ensimmäiseksi tämäntyyppiseksi korpuseksi venäjänkielisessä laskennallisessa kielitieteessä. Olemassa olevat manuaalisesti merkityt parafraasiaineistot venäjänkielisille kielille rajoittuvat pienikokoisiin ParaPhraser- ja ParaPlag-korpusiin, jotka soveltuvat moniin NLP-tehtäviin, kuten parafraasien ja plagioinnin havaitsemiseen, lauseiden samankaltaisuuden ja sukulaisuuden arviointiin jne. Kokorajoitusten vuoksi näitä tietoaineistoja tuskin voi käyttää lopusta päähän -tekstintuotantoratkaisuissa. Samaan aikaan parafraasien luominen vaatii suuren määrän harjoitusdataa. Työssämme ehdotamme ratkaisua ongelmaan: keräämme, sijoitamme ja arvioimme uuden julkisesti saatavilla olevan otsikon parafraasikorpusen (ParaPhraser Plus) ja suoritamme sitten tekstinkehityskokeita manuaalisella arvioinnilla automaattisesti sijoitetuilla korpusilla käyttäen Universal Transformer -arkkitehtuuria.', 'jv': 'Artiklé wis dipontrolan ngono nggawe sistem otomatik lan uga sawian karo perusahaan anyar tentang kanggo nganggo perusahaan kuwi mau sing bisa nguasai perusahaan karo perusahaan sing koyo ngono perusahaan liya sistem Rusi. echoH e l l o space w o r l d periodHelloworldHello world Slackfree Anyone', 'ha': "Makala na fokus a kan kwamfyuta farat developer da rankin wata mai girma wa dangi na rubutu na rususi, wanda yana jarraba ta zama farkon nau'i na wannan cikin linguistics na Ruushi. Ana ƙayyade data na fassarar da hannun da aka faɗa ɗa wa Ruushi, ana ƙayyade zuwa ƙarami-sized ParaPhraser nau'in da ParaPlag da ke daidai wa wasu aikin NLP, kamar gane na fassarar da plagiiki, kalma da daidaita da kuma ana ƙayyade masu husũma da kuma tsarin haɗi da hannun rubutu. Dukan ƙarshen girma, za'a iya amfani da wannan tsari ba cikin masu ƙari-zuwa-ƙari matsalan. A lokacin da kizaɓen parameter ya ƙayyade data mai girma wa zaman amfani. Daga karatunmu, Munã buɗa masu sola wa matabbata: Munã samun, daraja kuma munã qiimar wata shekara guda da ake iya samar rubutun sarrafa (ParaPhraser Add), kuma a bayan haka, mu sami jarrabi matsayin da manual da za'a iya ƙayyade makampuni da ke amfani da littãfin na Transformer Universal.", 'he': "המאמר ממוקד בפיתוח אוטומטי והדרגות של גופה גדולה לדור פראפרזיה רוסית שמוכיח להיות גופה ראשונה של סוג כזה בלשון מחשבי רוסי. קבוצות נתונים של פראפרסאר קטנות ורופרסאר פלאג שנמצאים באופן ידני מוגבלות לקבוצה של משימות NLP, כמו פראפרסם וגילוי פלג'יאריזם, דמיון משפטים והעריכה של קשר וכו. בגלל הגבלות בגודל, קבוצות נתונים אלה בקושי יכולות להפעיל בפתרונות של דורת טקסט בסוף לסוף. בינתיים, דור פראפרזים דורש כמות גדולה של נתוני אימון. In our study we propose a solution to the problem: we collect, rank and evaluate a new publicly available headline paraphrase corpus (ParaPhraser Plus), and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.", 'sk': 'Prispevek se osredotoča na avtomatski razvoj in razvrščanje velikega korpusa za ustvarjanje ruskih parafraz, ki se izkaže za prvi korpus te vrste v ruskem računalniškem jezikoslovju. Obstoječi ročno označeni zbirki podatkov parafraze za ruski jezik so omejeni na majhne velikosti korpusa ParaPhraser in ParaPlag, ki so primerni za niz nalog NLP, kot so odkrivanje parafraze in plagiatorstva, podobnost stavkov in ocena povezanosti itd. Zaradi omejitev velikosti teh zbirk podatkov težko uporabljati v rešitvah ustvarjanja besedila od konca do konca. Medtem pa generiranje parafraze zahteva veliko količino podatkov o usposabljanju. V naši študiji predlagamo rešitev problema: zberemo, razvrstimo in ocenimo nov javno dostopen parafrazni korpus naslovov (ParaPhraser Plus), nato pa izvajamo poskuse generiranja besedila z ročnim vrednotenjem na samodejno razvrščenih korpusih z uporabo arhitekture univerzalnega transformatorja.', 'bo': 'འདི་གྲངས་དུ་རང་འགུལ་གྱིས་ཡར་རྒྱས་འགྲོ་བ་དང་སྤྱད་པར་ཨ་རུ་སྐད་ཆ་ཆེ་བའི་དཔུད་ཁང་ཞིག་ཡིན་པ་ལྟ་བུའི་ནང་གི་རིགས་རྩིས་རྩིས་འཁོ Existing manually annotated paraphrase datasets for Russian are limited to small-sized ParaPhraser corpus and ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and plagiarism detection, sentence similarity and relatedness estimation, etc. Due to size restrictions, these datasets can hardly be applied in end-to-end text generation solutions. དུས་བརྟན་པར། ཚིག་ཕུང་གི་མི་རབས་པར་སྨན་གྱི་ཚད་ཆེ་བ་དགོས་པ་དེ་རེད། ང་ཚོའི་ལས་འཚོལ་ཞིབ་གིས་དཀའ་ངལ་གཅིག་གམ་གཤིས་ཀྱི་ཐབས་ལམ་ཞིག་སྟོན་པ་ཡིན།'}
{'en': 'Distill, Adapt, Distill : Training Small, In-Domain Models for Neural Machine Translation', 'ar': 'التقطير والتكيف والتقطير: تدريب النماذج الصغيرة في المجال للترجمة الآلية العصبية', 'pt': 'Distill, Adapt, Distill: treinamento de modelos pequenos no domínio para tradução automática neural', 'fr': 'Distiller, Adapter, Distiller\xa0: formation de petits modèles dans le domaine pour la traduction automatique neuronale', 'es': 'Destilar, adaptar, destilar: entrenamiento de modelos pequeños en el dominio para la traducción automática neuronal', 'ja': '蒸留、適応、蒸留：神経機械翻訳のための小さなトレーニングインドメインモデル', 'ru': 'Дистилляция, адаптация, дистилляция: Обучение небольших внутридоменных моделей для нейронного машинного перевода', 'zh': '蒸馏、改编、提炼:练于神经机器翻译小域', 'hi': 'Distill, Adapt, Distill: न्यूरल मशीन अनुवाद के लिए छोटे, इन-डोमेन मॉडल प्रशिक्षण', 'ga': 'Driogadh, Oiriúnú, Driogadh: Múnlaí Beaga In-Fearainn a Thraenáil le haghaidh Aistriúcháin Néar-Mhuineacháin', 'el': 'Αποστάγματα, Προσαρμογή, Αποστάγματα: Εκπαίδευση μικρών μοντέλων για Νευρική Μηχανική Μετάφραση', 'ka': 'Name', 'hu': 'Desztillálás, adaptálás, desztillálás: Kis, tartományon belüli modellek kiképzése Neurális gépi fordításhoz', 'kk': 'Шағын, адаптер, шектеу: Шағын оқыту, нейрондық машинаны аудару үшін домен үлгілері', 'it': 'Distilla, Adatta, Distilla: Formazione di piccoli modelli di dominio per la traduzione automatica neurale', 'ms': 'Distill, Adapt, Distill: Latihan Model Kecil, Dalam Domain untuk Terjemahan Mesin Neural', 'mt': 'Distill, Adapt, Distill: Taħriġ Mudelli Żgħar, Domeniċi għat-Traduzzjoni ta’ Magni Newrali', 'lt': 'Distill, Adapt, Distill: mokymas mažiems, domeniniams modeliams, skirtiems versti nervines mašinas', 'ml': 'ഡിസ്റ്റില്\u200d, അഡാപ്റ്റ്, ഡിസ്റ്റില്\u200d: ചെറിയ പരിശീലിപ്പിക്കുന്നത്, ഡൊമെയിന്\u200d മോഡലുകള്\u200d, ന്യൂറല്\u200d മെഷീന്\u200d പരിഭ', 'mk': 'Distill, Adapt, Distill: Тренирање на мали, внатрешни модели за превод на неврални машини', 'mn': 'Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation', 'sr': 'Distil, Adapt, Distil: obuka mala, model u domenu za neurološki prevod mašine', 'si': 'Name', 'so': 'Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation', 'no': 'Distil, Adapt, Distil: Treng Liten, Inndomene- modeller for Neural Machine Translation', 'ro': 'Distill, Adapt, Distill: Instruirea modelelor mici în domeniu pentru traducerea automată neurală', 'pl': 'Distill, Adapt, Distill: Szkolenie małych, wewnętrznych modeli neuronowego tłumaczenia maszynowego', 'sv': 'Destillera, anpassa, destillera: Utbildning av små domänmodeller för neural maskinöversättning', 'ta': 'டிஸ்டில், செயல்படுத்து, திருப்பு: சிறிய பயிற்சி, டொமைன் மாதிரிகளுக்கான புதிய இயந்திரம் மொழிபெயர்ப்பு', 'ur': 'Distill, Adapt, Distill: Training Small, In- Domain Models for Neural Machine Translation', 'uz': 'Name', 'vi': 'Trình diễn, Thích nghi, Trình diễn: Thông tin nhỏ, Ở miền cho phiên dịch máy thần kinh', 'bg': 'Дестилиране, адаптиране, дестилиране: обучение на малки, вътрешни модели за неврален машинен превод', 'da': 'Destiller, tilpasser, Destiller: Træning af små, in-domæne modeller til neural maskinoversættelse', 'nl': 'Distill, Adapt, Distill: training van kleine, in-domein modellen voor neuronale machinevertaling', 'hr': 'Distil, Adapt, Distil: obuka mala, model u domenu za neurološki prevod strojeva', 'id': 'Distill, Adapt, Distill: Latihan Model Kecil, Dalam Domain untuk Translation Mesin Neural', 'de': 'Distill, Adapt, Distill: Training kleiner, interner Modelle für neuronale maschinelle Übersetzung', 'ko': '추출, 조정, 추출: 신경기계 번역에 사용되는 소형 역내 모형 훈련', 'fa': 'Distill, Adapt, Distill: Training Small, In Domain Models for Neural Machine Translation', 'tr': 'Saýlaw, Topleme, Saýlaw: Ewezam Kiçi, In-Domain Models for Neural Machine Translation', 'af': 'Name', 'sw': 'Utafiti, Adapt, Disbeli: Mafunzo madogo, Katika Modeli za Kidomen kwa Tafsiri ya Mashine ya Kiurali', 'sq': 'Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation', 'hy': 'Դիստալ, Ադապտալ, Դիստալ. Նյարդային մեքենայի թարգմանման փոքր, բնական մոդելներ վարժեցնում', 'am': 'District, Adapt, District: Training Small, In-Domain Models for Neural Machine translation', 'az': 'Distil, Adapt, Distil: Kiçik təhsil, In-Domain Models for Neural Machine Translation', 'bn': 'ডিস্টিল, আড্যাপ্ট, ডিস্টোর: ছোট প্রশিক্ষণ, ডোমেইন মোডেল, নিউরাল মেশিন অনুবাদের জন্য', 'ca': 'Distill, Adapt, Distill: Formació de petits models de traducció de màquines neuronals', 'bs': 'Distil, Adapt, Distil: obuka mala, model u domenu za neurološki prevod mašine', 'cs': 'Distill, Adapt, Distill: Školení malých, In-Domain modelů pro neuronový strojový překlad', 'et': 'Destill, Adapt, Destill: väikeste, domeenisiseste mudelite koolitamine neuraalse masintõlke jaoks', 'fi': 'Distill, Adapt, Distill: Pienten, Domain-mallien koulutus neurokonekääntämiseen', 'jv': 'Distil, Adv, Distil: Learning small, In-domain model for Neral Masine translation', 'sk': 'Destill, Adapt, Destill: Usposabljanje majhnih, domenskih modelov za nevralno strojno prevajanje', 'ha': '@ info: whatsthis', 'bo': 'Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation', 'he': 'Distill, Adapt, Distill:'}
{'en': 'We explore best practices for training small, memory efficient  machine translation models  with sequence-level knowledge distillation in the domain adaptation setting. While both  domain adaptation  and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in  machine translation  (on three language pairs with three domains each) suggest distilling twice for best performance : once using general-domain data and again using in-domain data with an adapted teacher.', 'ar': 'نستكشف أفضل الممارسات لتدريب نماذج الترجمة الآلية الصغيرة والفعالة للذاكرة مع تقطير المعرفة على مستوى التسلسل في إعداد تكييف المجال. بينما يتم استخدام كل من تكيف المجال وتقطير المعرفة على نطاق واسع ، إلا أن تفاعلهما لا يزال غير مفهوم. تقترح نتائجنا التجريبية واسعة النطاق في الترجمة الآلية (على ثلاثة أزواج لغوية مع ثلاثة مجالات لكل منها) الاستخلاص مرتين للحصول على أفضل أداء: مرة واحدة باستخدام بيانات المجال العام ومرة أخرى باستخدام بيانات في المجال مع معلم مُكيف.', 'es': 'Exploramos las mejores prácticas para entrenar modelos de traducción automática pequeños y eficientes en memoria con destilación de conocimiento a nivel de secuencia en el entorno de adaptación de dominio. Si bien tanto la adaptación de dominios como la destilación del conocimiento se utilizan ampliamente, su interacción sigue siendo poco entendida. Nuestros resultados empíricos a gran escala en traducción automática (en tres pares de idiomas con tres dominios cada uno) sugieren destilar dos veces para obtener el mejor rendimiento: una vez usar datos de dominio general y otra vez usar datos de dominio con un profesor adaptado.', 'pt': 'Exploramos as melhores práticas para treinar modelos de tradução de máquina pequenos e eficientes em memória com destilação de conhecimento em nível de sequência na configuração de adaptação de domínio. Embora tanto a adaptação de domínio quanto a destilação do conhecimento sejam amplamente utilizadas, sua interação permanece pouco compreendida. Nossos resultados empíricos em grande escala em tradução automática (em três pares de idiomas com três domínios cada) sugerem destilar duas vezes para obter melhor desempenho: uma vez usando dados de domínio geral e novamente usando dados de domínio com um professor adaptado.', 'fr': "Nous explorons les meilleures pratiques pour former de petits modèles de traduction automatique économes en mémoire avec distillation des connaissances au niveau de la séquence dans le cadre de l'adaptation de domaine. Bien que l'adaptation au domaine et la distillation des connaissances soient toutes deux largement utilisées, leur interaction reste peu comprise. Nos résultats empiriques à grande échelle en traduction automatique (sur trois paires de langues avec trois domaines chacun) suggèrent de distiller deux fois pour obtenir les meilleures performances\xa0: une fois en utilisant des données du domaine général et une fois de plus en utilisant des données internes au domaine avec un enseignant adapté.", 'ja': '私たちは、ドメイン適応設定でシーケンスレベルの知識蒸留を使用して、小さくてメモリ効率の良い機械翻訳モデルをトレーニングするためのベストプラクティスを探ります。ドメイン適応と知識蒸留の両方が広く使用されていますが、それらの相互作用はほとんど理解されていません。機械翻訳における大規模な実証結果（ 3つの言語ペアで、それぞれ3つのドメイン）は、最高のパフォーマンスのために2回蒸留することを示唆しています。1回は一般ドメインデータを使用し、もう1回は適応された教師とのドメイン内データを使用します。', 'hi': 'हम डोमेन अनुकूलन सेटिंग में अनुक्रम-स्तरीय ज्ञान आसवन के साथ छोटे, स्मृति कुशल मशीन अनुवाद मॉडल के प्रशिक्षण के लिए सर्वोत्तम प्रथाओं का पता लगाते हैं। जबकि डोमेन अनुकूलन और ज्ञान आसवन दोनों का व्यापक रूप से उपयोग किया जाता है, उनकी बातचीत बहुत कम समझ में आती है। मशीन अनुवाद में हमारे बड़े पैमाने पर अनुभवजन्य परिणाम (प्रत्येक तीन डोमेन के साथ तीन भाषा जोड़े पर) सर्वोत्तम प्रदर्शन के लिए दो बार डिस्टिलिंग का सुझाव देते हैं: एक बार सामान्य-डोमेन डेटा का उपयोग करना और फिर से एक अनुकूलित शिक्षक के साथ इन-डोमेन डेटा का उपयोग करना।', 'zh': '求诸域中序列级练小内存效率高机器翻译形之最。 虽应知博用,其用犹鲜为人知。 大实机器翻译中(三言对上,每言三域)再炼以获得最佳性:一用常数,一用域中数。。', 'ru': 'Мы изучаем лучшие практики для обучения небольших, эффективных в памяти моделей машинного перевода с дистилляцией знаний на уровне последовательности в условиях адаптации домена. Хотя как адаптация домена, так и дистилляция знаний широко используются, их взаимодействие остается малопонятным. Наши крупномасштабные эмпирические результаты в машинном переводе (на трех языковых парах с тремя доменами каждая) предлагают дистилляцию дважды для лучшей производительности: один раз с использованием данных в общей области и еще раз с использованием данных в области с адаптированным учителем.', 'ga': 'Déanaimid iniúchadh ar dhea-chleachtais maidir le hoiliúint a chur ar mhúnlaí beaga aistriúcháin inneall-éifeachtacha cuimhne le driogadh eolais ar leibhéal seicheamh sa suíomh oiriúnaithe fearainn. Cé go n-úsáidtear go forleathan oiriúnú fearainn agus driogadh eolais, is beag a thuigtear go fóill a n-idirghníomhaíocht. Molann ár dtorthaí eimpíreacha ar scála mór in aistriúchán meaisín (ar thrí phéire teanga le trí fhearann an ceann) driogadh a dhéanamh faoi dhó chun an fheidhmíocht is fearr a bhaint amach: úsáid a bhaint as sonraí fearainn ghinearálta uair amháin agus arís ag baint úsáide as sonraí in-fearainn le múinteoir oiriúnaithe.', 'ka': 'ჩვენ ვაკვირდებით ყველაზე საუკეთესო პრექტიკები მანქანის შეცვლის მანქანის შეცვლის მოდელებისთვის მანქანის შეცვლელებისთვის, რომელიც კოდენური მეცნიერები თუმცა ორივე დიომინური ადაპტიკაცია და ცნობიერების დისტლილაცია ძალიან გამოყენებულია, მათი ინტერფექცია უფრო ცოტა. ჩვენი დიდი მაგალითად ემპერიკალური შედეგი მანქანის გაგრძელებაში (სამი ენაზე სამი დიომენზე ყოველ) აჩვენებს სხვადასხვა ორჯერ უკეთესი მუშაობისთვის: ერთხელ საერთო დიომენის მონაცე', 'hu': 'Feltárjuk a legjobb gyakorlatokat a kis, memória-hatékony gépi fordítási modellek képzéséhez szekvencia szintű tudás desztillációval a domain adaptációs beállításokban. Bár mind a domain adaptációt, mind a tudás desztillációját széles körben használják, kölcsönhatásuk továbbra is kevéssé érthető. A gépi fordításban szerzett nagyszabású empirikus eredményeink (három nyelvpár, egyenként három domain) azt sugallják, hogy kétszer lepároljuk a legjobb teljesítmény érdekében: egyszer általános domain adatokat használunk, majd ismét domain adatokat használunk egy adaptált tanárral.', 'el': 'Ερευνούμε βέλτιστες πρακτικές για την εκπαίδευση μικρών μοντέλων μηχανικής μετάφρασης αποδοτικών στη μνήμη με απόσταξη γνώσεων σε επίπεδο ακολουθίας στη ρύθμιση προσαρμογής τομέα. Ενώ τόσο η προσαρμογή του τομέα όσο και η απόσταξη της γνώσης χρησιμοποιούνται ευρέως, η αλληλεπίδρασή τους παραμένει ελάχιστα κατανοητή. Τα ευρείας κλίμακας εμπειρικά μας αποτελέσματα στη μηχανική μετάφραση (σε τρία ζεύγη γλωσσών με τρεις τομείς ο καθένας) προτείνουν την απόσταξη δύο φορές για την καλύτερη απόδοση: μία φορά χρησιμοποιώντας δεδομένα γενικού τομέα και πάλι χρησιμοποιώντας δεδομένα εντός του τομέα με προσαρμοσμένο δάσκαλο.', 'it': "Esploriamo le best practice per la formazione di piccoli modelli di traduzione automatica efficienti in memoria con distillazione di conoscenze a livello sequenziale nell'impostazione di adattamento del dominio. Mentre sia l'adattamento del dominio che la distillazione della conoscenza sono ampiamente utilizzati, la loro interazione rimane poco compresa. I nostri risultati empirici su larga scala nella traduzione automatica (su tre coppie linguistiche con tre domini ciascuno) suggeriscono di distillare due volte per ottenere le migliori prestazioni: una volta utilizzando dati di dominio generale e di nuovo utilizzando dati di dominio interno con un insegnante adattato.", 'lt': 'Mes tiriame geriausią praktiką mokymui mažų, atminties efektyvumo mašin ų vertimo modelių su sekos lygio žinių distiliavimu adaptacijos srityje. Nors ir pritaikymas prie sričių, ir žinių distiliavimas plačiai naudojami, jų sąveika vis dar mažai suprantama. Mūsų didelio masto empiriniai rezultatai vertimui mašinomis (po trijų kalbų poros su kiekviena trimis sritimis) siūlo du kartus distiliuoti geriausius rezultatus: vieną kartą naudojant bendruosius domeno duomenis ir kitą kartą naudojant domeno duomenis su pritaikytu mokytoju.', 'mk': 'Истражуваме најдобри практики за обука на мали, сеќавачки ефикасни машински преведувачки модели со дестилација на степен на знаење во поставувањето на адаптација на доменот. While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood.  Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.', 'ms': 'We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting.  Sementara penyesuaian domain dan pengusiran pengetahuan digunakan secara luas, interaksi mereka masih sedikit dipahami. Keputusan empirik skala besar kami dalam terjemahan mesin (pada tiga pasangan bahasa dengan tiga domain masing-masing) menyarankan membeli dua kali untuk prestasi terbaik: sekali menggunakan data domain umum dan sekali lagi menggunakan data domain dengan guru yang sesuai.', 'ml': 'We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting.  ഡൊമെയിന്\u200dറെ പ്രശ്നവും പരിജ്ഞാനവും വ്യത്യസ്തമായി ഉപയോഗിക്കപ്പെടുമ്പോള്\u200d അവരുടെ ഇടപെടല്\u200d കുറച്ച് മനസ്സി മെഷിന്\u200d പരിഭാഷക്കുറിച്ച് നമ്മുടെ വലിയ പ്രതിഫലങ്ങള്\u200d', 'mt': 'We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting.  Filwaqt li kemm l-adattament tad-dominju kif ukoll id-distillazzjoni tal-għarfien jintużaw b’mod wiesa’, l-interazzjoni tagħhom għadha ftit li xejn mifhuma. Ir-riżultati empiriċi fuq skala kbira tagħna fit-traduzzjoni bil-magna (fuq tliet pari lingwistiċi bi tliet dominji kull wieħed) jissuġġerixxu d-distillazzjoni darbtejn għall-aħjar prestazzjoni: darba tuża dejta tad-dominju ġenerali u mill-ġdid tuża dejta fid-dominju ma’ għalliem adattat.', 'pl': 'Badamy najlepsze praktyki w zakresie szkolenia małych, pamięciowych modeli tłumaczenia maszynowego z destylacją wiedzy na poziomie sekwencji w ustawieniach adaptacji domeny. Chociaż zarówno adaptacja domeny, jak i destylacja wiedzy są szeroko stosowane, ich interakcja pozostaje mało zrozumiana. Nasze wielkoskalowe wyniki empiryczne w zakresie tłumaczenia maszynowego (na trzech parach językowych z każdą z trzech domen) sugerują dwukrotną destylację dla najlepszej wydajności: raz wykorzystując dane ogólne domeny i ponownie wykorzystując dane wewnątrz domeny z dostosowanym nauczycielem.', 'ro': 'Explorăm cele mai bune practici pentru instruirea modelelor de traducere automată mici, eficiente din memorie, cu distilarea cunoștințelor la nivel de secvență în cadrul adaptării domeniului. În timp ce atât adaptarea domeniului, cât și distilarea cunoștințelor sunt utilizate pe scară largă, interacțiunea lor rămâne puțin înțeleasă. Rezultatele noastre empirice la scară largă în traducerea automată (pe trei perechi de limbi cu trei domenii fiecare) sugerează distilarea de două ori pentru a obține cea mai bună performanță: o dată utilizarea datelor domeniului general și din nou utilizarea datelor din domeniu cu un profesor adaptat.', 'kk': 'Біз кішкентай, жадындағы компьютердің аудару үлгілерін келесі деңгейіндегі білім деңгейіндегі дәрежелерді өзгерту үшін ең жақсы әрекеттерді зерттейміз. Доменнің адаптациясы мен білім дистилациясы көп қолданылатын болса, олардың әрі әрі білімсіздігі білмейді. Біздің үлкен масштабы импирикалық компьютердің аудармасының нәтижесі (әрбір үш тіл екі доменге үш доменге арналған) ең жақсы істеу үшін екі рет бөлімдеу үшін таңдайды: жалпы доменнің деректерін қо', 'mn': 'Бид жижиг, санамсаргүй машины хөрөнгө дасгал хөдөлгөөний сайн дасгал дасгал судалж байна. Хэрэв хүрээлэн буй адилтгал болон мэдлэг хуваалцах хоёр нь ихэвчлэн хэрэглэгддэг ч тэдний харилцаа бага зэрэг ойлгомжтой. Бидний том хэмжээний эзэмшигийн үр дүн нь машины хөгжүүлэлт (гурван хэл хоорондоо бүр гурван хэлбэртэй холбоотой холбоотой) сайн үйл ажиллагаанд хоёр дахин ялгаатай санал болгож байна: ерөнхий хэлбэрийн өгөгдлийг ашигла', 'no': 'Vi undersøker beste praksis for å undersøke små, minneeffektiv maskineomsetjingsmodular med distillasjon av kjenningsnivå i domenetilpassingsinnstillinga. Selv om både domeneadaptasjon og kjenningsdistillasjon er vast brukt, vil interaksjonen deres bli litt forstått. Våre store forskjellige empiriske resultat i maskineomsetjinga (på tre språkopar med tre domene kvar) foreslår å distilere to gonger for best utvikling: ein gong brukar generell domenedata og ein gong brukar inndomenedata med ein tilpassa lærar.', 'so': 'Waxaynu baaraannaa habooyinka ugu wanaagsan ee lagu baro qalabka turjumidda maskaxda ee faa’iidada ah ee xasuusta, oo lagu kala soocaa aqoonta heerka dabarka ee habka habboonaanta. Inta lagu isticmaalo isbedelashada deegaanka iyo kala duwanaanta aqoonta, xiriirkooda weli wax yar ayaa la garanayaa. Midhahayaga aad u weyn ee turjumista machine (ku qoran saddex luqadood oo saddex domain each) waxaa laga jeedaa in lagu kala sooco laba jeer si aad u samayso, marka lagu isticmaalo macallimiin macallin adag.', 'sv': 'Vi unders철ker b채sta praxis f철r utbildning av sm책 minneseffektiva maskin철vers채ttningsmodeller med kunskapsdestillation p책 sekvensniv책 i dom채nanpassningsinst채llningen. 횆ven om b책de dom채nanpassning och kunskapsdestillation anv채nds ofta, 채r deras interaktion fortfarande lite f철rst책dd. V책ra storskaliga empiriska resultat inom maskin철vers채ttning (p책 tre spr책kpar med tre dom채ner vardera) f철resl책r att man destillerar tv책 g책nger f철r b채sta resultat: en g책ng anv채nder allm채nna dom채ndata och igen anv채nder dom채ndata med en anpassad l채rare.', 'ta': 'நாம் சிறிய, நினைவு தேவையான இயந்திரம் மொழிபெயர்ப்பு மாதிரிகளை பயிற்சி செய்ய சிறந்த செயல்பாடுகளை தேடுகிறோம் களம் ஒத இருவரும் களம் ஒதுக்கும் அறிவு பிரிப்பு பயன்படுத்தப்பட்டுள்ளது போது, அவைகளின் இடைவெளி செயல்பாடு சிறிது பு கணினி மொழிபெயர்ப்பில் எங்கள் பெரிய அளவு விளைவுகள் (ஒவ்வொரு மூன்று மொழி ஜோடி ஒவ்வொரு மூன்று களங்களிலும்) சிறந்த செயல்பாட்டிற்கு இருமுறை பிரித்த', 'sr': 'Istražujemo najbolje prakse za obuku malih, uspomenih modela prevoda mašina sa destilacijom znanja na nivou sekvence u postavljanju adaptacije domena. Iako se i domena adaptacija i destilacija znanja široko koriste, njihova interakcija ostaje malo razumjena. Naši veliki empirički rezultati u prevodu mašine (na tri jezičke pare sa tri domena svaki) predlažu da se dvaput destilacija za najbolje izvedbe: jednom koristeći podatke o općem domenu i ponovo koristeći podatke u domenu sa prilagođenim nastavnikom.', 'ur': 'ہم چھوٹے، مہمانی کے مطالعہ کے لئے بہترین مطالعہ مہمانی ترجمہ موڈل کے لئے اچھے مطالعہ کے مطالعہ کا تحقیق کرتے ہیں، جن کے مطالعہ میں سطح-سطح علم کے متفرق ہے۔ حالانکہ دونوں ڈومین کی تعمیر اور علم کی تفریق وسیع طور پر استعمال کی جاتی ہے، ان کی تعامل کم سمجھ رہی ہے. ہمارے بڑے اسکیل امپریکل نتیجے ماشین کی ترجمہ میں (تین زبان جوڑوں پر ہر تین ڈومین کے ساتھ) بہترین عملکرد کے لئے دوبارہ جدائی کرنا مشورہ کرتا ہے: ایک بار عمومی ڈومین ڈاٹ کا استعمال کرتا ہے اور دوبارہ ڈومین ڈاٹ کے ساتھ ایک', 'si': 'අපි පුංචි ප්\u200dරධානය සඳහා හොඳ ප්\u200dරශ්නයක් පරීක්ෂණය කරනවා, මතකයේ ප්\u200dරශ්නයක් පරීක්ෂණය සඳහා පරීක්ෂණ-ත ඩොමේන් සැකසුම් සහ දන්න විශේෂණය ප්\u200dරමාණය කරලා තියෙනවා නමුත් ඔවුන්ගේ සම්බන්ධය ප්\u200dරමාණය ප්\u200dර අපේ ලොකු ප්\u200dරමාණයක් ප්\u200dරතිචාරයක් පද්ධතිය පද්ධතිය පද්ධතිය (භාෂාව තුනක් පද්ධතියෙන් හැම භාෂාව තුනක් තුනක් තියෙන්න) හොඳම ප්\u200d', 'uz': "Biz domen adapterining moslamalarida kichkina, xotira foydalanish modellarini o'rganish uchun eng yaxshi narsalarni o'rganamiz. Agar икковларининг доменни ўзгартириш ва илм ўзгартириш жиҳатидан фойдаланаётганида, ularning interaction кам тушунилади. Umumiy natijalarimiz mashina tarjima qilish natijalarimiz (har uchta tilda uchta domen bo'lgan uchta tilda) yaxshi bajarish uchun ikki marta ajratish imkoniyatlarini anglatadi: bir marta umumiy domen maʼlumotdan foydalanish va doim tahrirchi bilan ishlatish mumkin.", 'vi': 'Chúng tôi tìm hiểu kỹ thuật tốt nhất để huấn luyện các mô hình dịch cỗ máy nhỏ, hiệu quả trí nhớ, với việc chưng cất kiến thức ở trình độ phân tử miền. Trong khi cả việc thích nghi và chưng cất tri thức được phổ biến, giao tiếp của chúng vẫn chưa rõ lắm. Những kết quả của bản dịch cơ bản trên diện rộng (trên ba cặp ngôn ngữ với ba miền mỗi loại) đề nghị chưng cất hai lần cho hiệu suất tốt nhất: một lần sử dụng dữ liệu miền chung và một lần nữa sử dụng dữ liệu nội bộ với một giáo viên thích nghi.', 'bg': 'Проучваме най-добрите практики за обучение на малки модели с ефективно използване на паметта машинен превод с дестилация на знания на ниво последователност в настройката за адаптация на домейна. Въпреки че както адаптацията в областта, така и дестилацията на знанието са широко използвани, тяхното взаимодействие остава малко разбрано. Нашите широкомащабни емпирични резултати в машинния превод (на три езикови двойки с три домейна всяка) предполагат дестилиране на два пъти за най-добра производителност: веднъж използване на общи данни и отново използване на вътрешни данни с адаптиран учител.', 'nl': 'We onderzoeken best practices voor het trainen van kleine, geheugenefficiënte machinevertaalmodellen met kennisdistillatie op sequentieniveau in de domeinadaptatie setting. Hoewel zowel domeinadaptatie als kennisdestillatie veel gebruikt worden, blijft hun interactie weinig bekend. Onze grootschalige empirische resultaten in machinevertaling (op drie taalparen met elk drie domeinen) suggereren twee keer te distilleren voor de beste prestaties: één keer gebruik maken van algemene domeingegevens en opnieuw gebruik maken van in-domein data met een aangepaste docent.', 'hr': 'Istražujemo najbolje prakse za obuku malih, učinkovitih modela prevoda mašina s destilacijom znanja na nivou sekvencije u postavljanju adaptacije domena. Iako se i domena adaptacija i destilacija znanja široko koriste, njihova interakcija ostaje malo razumjena. Naši veliki empirički rezultati u prevodu strojeva (na tri jezičke pare sa tri domena svaki) sugeriraju destilaciju dvaput za najbolje učinkovito: jednom koristeći podatke općeg domena i ponovo koristeći podatke u domenu s prilagođenim učiteljem.', 'da': 'Vi undersøger bedste praksis til uddannelse af små, hukommelseseffektive maskinoversættelsesmodeller med videndedestillation på sekvensniveau i domænetilpasningsindstillingen. Mens både domænetilpasning og videndedestillation er udbredt, er deres interaktion stadig lidt forstået. Vores store empiriske resultater inden for maskinoversættelse (på tre sprogpar med tre domæner hver) foreslår at destillere to gange for at opnå den bedste ydeevne: én gang bruger generelle domænedata og igen bruger in-domænedata med en tilpasset lærer.', 'de': 'Wir erforschen Best Practices für das Training kleiner, speichereffizienter maschineller Übersetzungsmodelle mit Sequenz-Level-Wissensdestillation in der Domänenanpassung. Während sowohl Domänenanpassung als auch Wissensdestillation weit verbreitet sind, bleibt ihre Wechselwirkung wenig verstanden. Unsere umfangreichen empirischen Ergebnisse in der maschinellen Übersetzung (auf drei Sprachpaaren mit jeweils drei Domänen) legen nahe, zweimal zu destillieren, um die beste Leistung zu erzielen: einmal mit allgemeinen Domänendaten und wieder mit In-Domänendaten mit einem angepassten Lehrer.', 'id': 'Kami mengeksplorasi praktek terbaik untuk melatih model terjemahan mesin kecil, efisien memori dengan distillasi pengetahuan tingkat urutan dalam setting adaptasi domain. Sementara adaptasi domain dan distillasi pengetahuan digunakan secara luas, interaksi mereka masih sedikit dipahami. Hasil empirik skala besar kami dalam terjemahan mesin (pada tiga pasangan bahasa dengan tiga domain masing-masing) menyarankan mendistil dua kali untuk prestasi terbaik: sekali menggunakan data domain umum dan lagi menggunakan data domain dengan guru yang disesuaikan.', 'ko': '우리는 분야 적응 환경에서 서열급 지식 추출을 통해 소형, 메모리가 효율적인 기계 번역 모델을 훈련하는 가장 좋은 실천을 모색했다.영역 적응과 지식 추출이 널리 사용되고 있지만 그 상호작용은 아직 알려지지 않았다.우리는 기계 번역 분야의 대규모 실증 결과(세 쌍의 언어, 한 쌍의 언어는 세 개의 영역이 있다)에 의하면 최상의 성능을 얻기 위해서는 두 번의 추출이 필요하다. 한 번은 일반 영역 데이터를 사용하고 다른 한 번은 영역 내 데이터와 적응된 교사를 사용해야 한다.', 'fa': 'ما بهترین تجربه\u200cها را برای آموزش مدل\u200cهای ترجمه\u200cهای ماشین کوچک و حافظه\u200cای با جدایی\u200cهای دانش\u200cهای سطح طبقه\u200cای در تنظیم تغییرات دومین تحقیق می\u200cکنیم. در حالی که هر دو تغییر و تغییر علم از وسیع استفاده می\u200cشود، تعامل آنها کمی درک می\u200cشود. نتایج امپراتیک بزرگ ما در ترجمه ماشین (در سه جفت زبان با سه دامنه هر یک) دو بار برای بهترین عملکرد پیشنهاد می\u200cدهد: یک بار از داده\u200cهای دامنه\u200cهای عمومی و دوباره از داده\u200cهای دامنه\u200cی دامنه با یک معلم adapted استفاده می\u200cکند.', 'sw': 'Tunafanya mbinu nzuri kwa ajili ya mafunzo madogo, miradi yenye ufanisi wa mashine ya kutafsiri kwa kumbukumbu yenye utofauti wa maarifa kwa kiwango kikubwa katika mazingira ya kuboresha ndani. Wakati mabadiliko ya ndani na utofauti wa maarifa yanatumika kwa kiasi kikubwa, mazingira yao bado hayaelewi vibaya. Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.', 'af': "Ons ondersoek die beste praksies vir onderwerp van klein, geheue effektief masjien vertaling modele met sekwensievlak kennis versteling in die domein aanpassing instelling. Alhoewel albei domein-adaptasie en kennis-destilasie vaste gebruik word, bly hulle interaksie klein verstaan. Ons groot-skaal empiriese resultate in masjien vertaling (op drie taal paar met drie domeine elkeen) voorstel om twee maal te verskaf vir beste prestasie: een keer te gebruik algemene-domein data en weer te gebruik in-domein data met 'n aanpasbare onderwyser.", 'sq': 'Ne eksplorojmë praktikat më të mira për trajnimin e modeleve të vogla të përkthimit të makinave të efektshëm në kujtesë me distillacionin e njohurive në nivelin e sekuencës në rregullimin e përshtatjes në domeni. Ndërsa adaptimi dhe distilimi i njohurive janë përdorur gjerësisht, ndërveprimi i tyre mbetet pak i kuptuar. Rezultatet tona empirike në shkallë të madhe në përkthimin e makinave (në tre çifte gjuhësh me tre domene secili) sugjerojnë distilimin dy herë për performancën më të mirë: një herë duke përdorur të dhënat e përgjithshme të domenit dhe përsëri duke përdorur të dhënat në domeni me një mësues të përshtatshëm.', 'tr': 'Biz kiçi, hatyra etkinlik maşynyň terjime modellerini domain üýtgetmesinde düzümler bilen tapawutlamak üçin iň gowy praktikleri keşfetýäris. Hem domena adaptasy hem bilim tapawutlanmasy köpüräk ulanylýan bolsa, olaryň etkileşimi az düşünýändir. Biziň uly ölçekli empirik netijelerimiz maşynyň terjimelerinde (her üç dil çift üçin üç domena bilen) iň gowy başarylyk üçin iki gezek tapawutlandyrmagy maslahat berýär: bir gezek umumy domena maglumaty ullanýan we ýene-de domena maglumaty ullanýan.', 'am': 'ትንሽ፣ ማስታወሻ የበለጠ የሞክራዊ ትርጉም ሞዴላዎችን በዲሞናዊ አካባቢ እናሳውቃለን፡፡ የዶሜን አካባቢ እና እውቀት መግለጫ በተስፋው ሲጠቀም፣ የግንኙነታቸው ግንኙነት ጥቂት ሳይታወቅ ነው፡፡ የመረጃ ትርጉም ውጤታችን (በሦስት ቋንቋ ሁለት በሦስት ዲሜን ላይ) ሁለት ጊዜ ለጥሩ ማስታወቂያውን ለመለየት ይገልጻል፡፡ አንድ ጊዜ በጠቅላላ-ዶሜን ዳታ እና በዲሞናዊ ዳታዎችን በመጠቀም እና በተጠቃሚ አስተማሪ ጋር ነው፡፡', 'az': 'Biz küçük, yada salmaq faydalı maşın çevirici modellərinin ən yaxşı praksisini keşfetirik, domain adaptasiyonu qurğuluşdurulmasında sequence-level elm destilasyonu ilə. Hər iki domena uyğunlaşdırma və elm destilyası geniş istifadə edilirsə də, onların əlaqəsi az anlaşılır. Bizim böyük ölçülü empirik məlumatlarımız maşına çevirilməsi (üç dil çift ilə hər biri üç domena ilə üç domena ilə üç domena ilə) ən yaxşı performans üçün iki dəfə ayrılmağı təmin edir: bir dəfə genel domena məlumatlarını və bir dəfə domena məlumatlarını təmin edi', 'hy': 'Մենք ուսումնասիրում ենք փոքրիկ, հիշողության արդյունավետ մեքենային թարգմանման մոդելներ ուսումնասիրելու լավագույն գործողությունները հաջորդականության մակարդակի գիտելիքների դիսլիլացիայի միջոցով բնագավառի ադա Մինչդեռ երկու բնագավառի ադապտացիան և գիտելիքների դիսլիլացիան լայնորեն օգտագործվում են, նրանց փոխազդեցությունը դեռևս քիչ հասկանալի է: Մեր մեծ էմպրիկական արդյունքները մեքենային թարգմանման մեջ (յուրաքանչյուր երեք լեզվի զույգով երեք բնագավառներով) առաջարկում են երկու անգամ դիստել լավագույն արդյունքների համար: մեկ անգամ օգտագործելով ընդհանուր բնագավառների տվյալներ և կրկին օգ', 'bn': 'আমরা স্মৃতি কার্যকর মেশিন অনুবাদ মডেল প্রশিক্ষণের জন্য শ্রেষ্ঠ কার্যকলাপ খুঁজে বের করি ডোমেনের অ্যাডাপমেনের সেটিংসে স যদিও দুই ডোমেইনের আপডেট এবং জ্ঞানের বিভাগ ব্যবহার করা হয়, তাদের মধ্যে যোগাযোগ বুঝতে পারে না। মেশিন অনুবাদের (প্রত্যেক ভাষায় তিন ভাষার জোড়ায় তিন ডোমেনের মধ্যে) আমাদের বিশাল সামাজিক ফলাফলের পরামর্শ দেয়া হয়েছে: একবার সাধারণ ডোমেইনের তথ্য ব্', 'bs': 'Istražujemo najbolje prakse za obuku malih, učinkovitih modela prevoda mašina sa destilacijom znanja na nivou sekvence u postavljanju adaptacije domena. Iako se i domena adaptacija i destilacija znanja široko koriste, njihova interakcija ostaje malo razumjena. Naši veliki empirički rezultati u prevodu mašine (na tri jezička parova sa tri domena svaki) sugeriraju destilaciju dvaput za najbolje učinkovito: jednom korištenje podataka općeg domena i ponovo korištenje podataka u domenu s prilagođenim učiteljem.', 'ca': "Exploram les millors pràctiques per formar petits models de traducció de màquines eficients en la memòria amb distillació de coneixements a nivell secuencial en el contexte d'adaptació al domini. Mentre que l'adaptació de dominis i la destilació del coneixement són generalment utilitzats, la seva interacció encara no s'entend gaire. Els nostres resultats empírics a gran escala en la traducció màquina (en tres parelles de llenguatges amb tres dominis cada) suggereixen distilar el doble per a obtenir el millor rendiment: una vegada utilitzant dades de domini general i una altra vegada utilitzant dades en domini amb un professor adaptat.", 'cs': 'Prozkoumáme osvědčené postupy pro výcvik malých, paměťově efektivních strojových modelů překladu s destilací znalostí na úrovni sekvence v nastavení adaptace domény. Zatímco jak doménová adaptace, tak destilace znalostí jsou široce používány, jejich interakce zůstává málo pochopena. Naše rozsáhlé empirické výsledky v oblasti strojového překladu (na třech jazykových párech se třemi doménami) navrhují destilaci dvakrát pro nejlepší výkon: jednou použití obecných doménových dat a opět použití interdoménových dat s adaptovaným učitelem.', 'et': 'Uurime parimaid tavasid väikeste mälutõhusate masintõlkemudelite koolitamiseks järjestuse tasemel teadmiste destilleerimisega valdkonna kohandamise seadetes. Kuigi nii valdkonna kohandamist kui ka teadmiste destilleerimist kasutatakse laialdaselt, on nende vastastikune mõju endiselt vähe mõistetav. Meie suuremahulised empiirilised tulemused masintõlke valdkonnas (kolmes keelepaaris, igaühes kolmes valdkonnas) soovitavad destilleerida kaks korda parima tulemuslikkuse saavutamiseks: üks kord kasutades ülddomeeni andmeid ja uuesti kohandatud õpetajaga domeenisiseseid andmeid.', 'fi': 'Tutkimme parhaita käytäntöjä pienten, muistitehokkaiden konekäännösmallien kouluttamiseen sekvenssitason tietotislauksella toimialueen mukauttamisasetuksessa. Vaikka sekä toimialojen sopeutumista että tietämyksen tislausta käytetään laajalti, niiden vuorovaikutusta ymmärretään edelleen vähän. Laajamittaiset empiiriset tulokset konekääntämisestä (kolmella kieliparilla, joissa kussakin on kolme verkkotunnusta) viittaavat tislaukseen kaksi kertaa parhaan suorituskyvyn saavuttamiseksi: kerran käytetään yleistä verkkotunnusta ja taas käytetään verkkotunnuksen sisäistä dataa mukautetun opettajan kanssa.', 'ha': "Tuna karatun mafiya kyaun aikin da za'a yi amfani da masu ƙarami, masu karatun fassarori masu amfani da kwamfyutan aikin tuna da rarrabe zane-zane-duffai cikin muhimmin adaptori. Kuma a lokacin da dukansu, za'a yi amfani da yin adadi da rarrabin ilmi, sai aikin su bada yi fahimta ba. Matarinmu masu ƙaranci mai girma na fassarar mashine (a cikin misalin harshen sau biyu da sau biyu dukkan kowansa) yana shauri a rarraba su sau biyu dõmin mafi kyaun aikin aiki: idan an yi amfani da data na jumla-Domen da kuma ana yi amfani da data na cikin-Domen da wani mai adadi.", 'jv': 'We istrage all right prakses for testing small, memberspace effectual device translation modes with sec-evel knowingUnit in the domain modification settings. Speaking We big-scale empircal output in device translation', 'sk': 'Raziskujemo najboljše prakse za usposabljanje majhnih, pomnilniško učinkovitih modelov strojnega prevajanja z destilacijo znanja na ravni zaporedja v nastavitvi prilagajanja domene. Medtem ko se tako prilagajanje področja kot destilacija znanja široko uporabljata, je njuno medsebojno delovanje še vedno malo razumljeno. Naši obsežni empirični rezultati strojnega prevajanja (na treh jezikovnih parih s tremi domenami) kažejo, da se za najboljšo učinkovitost destilira dvakrat: enkrat z uporabo splošnih domenskih podatkov in ponovno z uporabo notranjih podatkov s prilagojenim učiteljem.', 'he': 'We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting.  While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood.  התוצאות האמפריות הגדולות שלנו בתרגום מכונות (בשלושה זוגות שפות עם שלושה שפות לכל אחד) מציעות למייבש פעמיים עבור ההופעה הטובה ביותר: פעם בשימוש בנתונים של שפות כלליות ושוב בשימוש בנתונים בתחום עם מורה מתאים.', 'bo': 'We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting. དེ་འདྲ་རྒྱ་སྟངས་འཛིན་གྱི་གྲོས་མཐུན་འགྱུར་བ་དང་ཤེས་པའི་དབྱེ་བ་གཉིས་ཀྱིས་ཕལ་ཆེན་ཐུབ་ཀྱི་ཡོད་པ་དང་ཁ Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.'}
{'en': 'The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task ADAPT  System Description for the  STAPLE  2020  E nglish-to- P ortuguese Translation Task', 'ar': 'وصف نظام ADAPT لمهمة الترجمة من الإنجليزية إلى البرتغالية STAPLE 2020', 'es': 'Descripción del sistema ADAPT para la tarea de traducción del inglés al portugués STAPLE 2020', 'fr': "Description du système ADAPT pour la tâche de traduction de l'anglais vers le portugais STAPLE 2020", 'pt': 'A descrição do sistema ADAPT para a tarefa de tradução de inglês para português do STAPLE 2020', 'ja': 'ステープル2020英語からポルトガル語への翻訳タスクのADAPTシステム説明', 'zh': 'STAPLE 2020 英语至葡萄牙语译者 ADAPT 统言之', 'hi': 'STAPLE 2020 अंग्रेज़ी-से-पुर्तगाली अनुवाद कार्य के लिए ADAPT सिस्टम विवरण', 'ru': 'Описание системы ADAPT для задачи перевода с английского НА португальский 2020 Staple', 'ga': 'Cur síos ar an gCóras ADAPT don Tasc Aistriúcháin Béarla-go-Portaingéilis STAPLE 2020', 'el': 'Η περιγραφή του συστήματος για το μεταφραστικό έργο Αγγλικά-Πορτογαλικά', 'hu': 'ADAPT rendszer leírása a STAPLE 2020 angol-portugál fordítási feladathoz', 'lt': 'STAPLE 2020 vertimo anglų kalba ir portugalų kalba užduoties ADAPT sistemos aprašymas', 'kk': 'ADAPT жүйесінің STAPLE 2020 ағылшын тілінен португалық аудару тапсырмасының сипаттамасы', 'mk': 'АДАПТ системски опис за задачата STAPLE 2020 за англиски на португалски превод', 'ka': 'ADAPT სისტემის გამოსახულება STAPLE 2020-ის ინგლისურ-პორგუტებური განსახულების დავალებისთვის', 'ms': 'The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task', 'mt': 'Id-Deskrizzjoni tas-Sistema ADAPT għall-Kompitu tat-Traduzzjoni bl-Ingliż għall-Portugiż STAPLE 2020', 'mn': 'STAPLE 2020-н Англи-болон Португалийн хөгжлийн ажлын ADAPT системийн тодорхойлолт', 'it': 'Descrizione del sistema ADAPT per il compito di traduzione inglese-portoghese STAPLE 2020', 'ml': 'STAPLE 2020 ഇംഗ്ലീഷ്- ലേക്ക്- പോര്\u200dട്ടുഗീഷ് പരിഭാഷ ടാസ്കിനുള്ള ADAPT സിസ്റ്റം വിശദീകരണം', 'pl': 'Opis systemu ADAPT dla zadania tłumaczeniowego STAPLE 2020 z języka angielskiego na portugalski', 'sr': 'Описане АДАПТ Система за задач превода на английски до португалски до STAPLE 2020', 'so': 'Tilmaamaha nidaamka ADAPT ee STAPLE 2020 Ingiriis-to-Portuguese Task of Translation', 'ro': 'Descrierea sistemului ADAPT pentru sarcina de traducere din engleză în portugheză STAPLE 2020', 'sv': 'Beskrivning av ADAPT-systemet för översättningsuppgiften från engelska till portugisiska STAPLE 2020', 'ta': 'STAPLE 2020 ஆங்கிலம்- மொழிபெயர்ப்பு பணிக்கான ADAPT அமைப்பு விவரம்', 'si': 'STApleE 2020යි ඉංග්\u200dරීසියාවෙන් පොර්තුතියාවට පද්ධතිය විවෘතිය', 'ur': 'STAPLE 2020 کے لئے ADAPT سیسٹم سپٹم کا سپٹر انگلیسی سے پورتوگل ترجمہ ٹاکس کے لئے', 'no': 'ADAPT-systemskildring for STAPLE 2020-engelsk-til-portugisisk omsetjingsprogrammet', 'uz': 'STAPLE 2020 uchun ADAPT tizim taĘĽrifi', 'vi': 'Mô tả hệ thống ADAP cho STAPLE 2020 Dịch vụ tiếng Anh-Bồ Đào Nha', 'hr': 'Opis ADAPT sustava za zadatak za prevod engleskog do portugalca STAPLE 2020', 'nl': 'De ADAPT systeembeschrijving voor de STAPLE 2020 vertaaltaak Engels-naar-Portugees', 'bg': 'Описание на системата АДАПТ за задачата за превод от английски на португалски език', 'id': 'Deskripsi Sistem ADAPT untuk Tugas Terjemahan Bahasa Inggris ke Portugis STAPLE 2020', 'de': 'Die ADAPT Systembeschreibung für die STAPLE 2020 Englisch-Portugiesisch Übersetzungsaufgabe', 'fa': 'Description of the ADAPT System for the STAPLE 2020 English-to-Portuguese Translation Task', 'ko': '영어에서 포르투갈어 번역 임무에 대한 사용자 적응 시스템 설명', 'af': 'Die ADAPT Stelsel Beskrywing vir die STAPLE 2020 Engels-na-Portugese Vertaling Taak', 'tr': 'STAPLE 2020 Iňlisçe-we-Portugalça terjime göresiniň ADAPT Sistemi Wasp', 'am': 'የADAPT ስርዓት መግለጫ ለSTAPLE 2020 እንግሊዝኛ-to-ፖርቱጋልኛ ትርጉም ስራ', 'sq': 'Përshkrimi i sistemit ADAPT për detyrën STAPLE 2020 të përkthimit anglez-portugal', 'az': 'STAPLE 2020 캻ngiliz톛-Portugalca T톛rc칲m톛 G칬revi 칲칞칲n ADAPT Sistemi T톛rc칲m톛', 'bn': 'STAPLE ২০২০ ইংরেজী থেকে পর্তুগীজ অনুবাদ কাজের জন্য ADAPT সিস্টেম বিবরণ', 'bs': 'Opis ADAPT sustava za zadatak za prevod engleskog do portugesa STAPLE 2020', 'da': 'Beskrivelsen af ADAPT-systemet for STAPLE 2020 engelsk-til-portugisisk oversættelsesopgave', 'cs': 'Popis systému ADAPT pro překladatelskou úlohu STAPLE 2020 z angličtiny do portugalštiny', 'et': 'ADAPT süsteemi kirjeldus STAPLE 2020 inglise-portugali tõlke ülesandeks', 'sw': 'Maelezo ya Mfumo wa ADAPT kwa ajili ya STAPLE 2020 Kiingereza-hadi Kireno', 'fi': 'ADAPT-järjestelmän kuvaus STAPLE 2020 englannin ja portugalin kääntämistehtävästä', 'ca': 'The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task', 'hy': 'The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task', 'jv': 'PINDAR Sistem Keterangan kanggo STAPLE 2020 Inggal-kanggo-Putuk Terjamahan task', 'sk': 'Opis sistema ADAPT za prevajalsko nalogo STAPLE 2020 iz angleščine v portugalščino', 'he': 'Description of the ADAPT System for the STAPLE 2020 English-to-Portuguese Translation Task', 'ha': 'Description', 'bo': 'ADAPT མ་ལག་གི་འགྲེལ་བཤད་ཀྱི་STAPLE 2020་ཡི་དབྱིན་ཡིག་ལས་པོར་ཊུས་སྐད་ཡིག་གནང་།'}
{'en': 'This paper describes the ADAPT Centre’s submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task. In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences). We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple n-best translations. Our experiments show that adding the aforementioned techniques to the  baseline  yields an excellent performance in the English-to-Portuguese translation task.', 'ar': 'تصف هذه الورقة تقديم مركز ADAPT إلى STAPLE (الترجمة الفورية وإعادة الصياغة لتعليم اللغة) 2020 ، وهي مهمة مشتركة لورشة العمل الرابعة حول التوليد العصبي والترجمة (WNGT) ، لمهمة الترجمة من الإنجليزية إلى البرتغالية. في هذه المهمة المشتركة ، طُلب من المشاركين إنتاج مجموعات عالية التغطية من الترجمات المعقولة بناءً على مطالبات باللغة الإنجليزية (جمل مصدر الإدخال). نقدم نماذج الترجمة الآلية من الإنجليزية إلى البرتغالية (MT) التي تم إنشاؤها باستخدام استراتيجيات مختلفة ، على سبيل المثال اختيار البيانات والجمل ، الترجمة الآلية أحادية اللغة لإنشاء ترجمات بديلة ، والجمع بين أفضل الترجمات المتعددة. تظهر تجاربنا أن إضافة التقنيات المذكورة أعلاه إلى خط الأساس ينتج عنه أداء ممتاز في مهمة الترجمة من الإنجليزية إلى البرتغالية.', 'pt': 'Este artigo descreve a submissão do Centro ADAPT ao STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, uma tarefa compartilhada do 4th Workshop on Neural Generation and Translation (WNGT), para a tarefa de tradução de inglês para português. Nesta tarefa compartilhada, os participantes foram solicitados a produzir conjuntos de alta cobertura de traduções plausíveis, dadas as instruções em inglês (frases de origem de entrada). Apresentamos nossos modelos de tradução automática (TA) de inglês para português que foram construídos aplicando várias estratégias, por exemplo. seleção de dados e frases, MT monolíngue para gerar traduções alternativas e combinar várias n melhores traduções. Nossos experimentos mostram que a adição das técnicas mencionadas à linha de base resulta em um excelente desempenho na tarefa de tradução de inglês para português.', 'es': 'Este artículo describe la presentación del Centro ADAPT a STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, una tarea compartida del 4º Workshop on Neural Generation and Translation (WNGT), para la tarea de traducción del inglés al portugués. En esta tarea compartida, se pidió a los participantes que produjeran conjuntos de traducciones plausibles de alta cobertura dadas las indicaciones en inglés (oraciones fuente de entrada). Presentamos nuestros modelos de traducción automática (MT) del inglés al portugués que se crearon aplicando diversas estrategias, por ejemplo, la selección de datos y frases, la traducción automática monolingüe para generar traducciones alternativas y la combinación de múltiples n-mejores traducciones. Nuestros experimentos muestran que la adición de las técnicas antes mencionadas a la línea de base produce un rendimiento excelente en la tarea de traducción del inglés al portugués.', 'fr': "Cet article décrit la soumission du Centre ADAPT à STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, une tâche partagée du 4e atelier sur la génération neuronale et la traduction (WNGT), pour la tâche de traduction de l'anglais vers le portugais. Dans cette tâche partagée, les participants ont été invités à produire des ensembles de traductions plausibles à couverture élevée en fonction des invites en anglais (phrases sources d'entrée). Nous présentons nos modèles de traduction automatique (TA) de l'anglais vers le portugais qui ont été conçus en appliquant diverses stratégies, par exemple la sélection de données et de phrases, la TA monolingue pour générer des traductions alternatives et la combinaison de plusieurs traductions n-best. Nos expériences montrent que l'ajout des techniques susmentionnées à la base de référence permet d'obtenir d'excellentes performances dans la tâche de traduction de l'anglais vers le portugais.", 'ja': '本稿では、英語からポルトガル語への翻訳タスクのための第4回ニューラルジェネレーションと翻訳ワークショップ（ WNGT ）の共有タスクであるSTAPLE （同時翻訳と言語教育のためのパラフレーズ） 2020へのADAPTセンターの提出について説明する。この共有タスクでは、参加者は、英語のプロンプト（入力ソース文）を与えられた妥当な翻訳のカバレッジの高いセットを作成するように求められました。私たちは、データと文章の選択、代替翻訳を生成するためのモノリンガルMT、複数の最良の翻訳を組み合わせるなど、さまざまな戦略を適用して構築された英語からポルトガル語への機械翻訳（ MT ）モデルを提示します。私たちの実験では、前述の技術をベースラインに追加することで、英語からポルトガル語への翻訳タスクで優れたパフォーマンスが得られることが示されています。', 'zh': '本文言ADAPT心向2020年STAPLE(言教同声传译与释义)交者,此第四届神经生成与译研讨会(WNGT)之同务,施于英语葡萄牙语之译也。 当此之务,参与者据英语言(输源句)制高覆盖率之理翻译集。 吾展吾英语于葡萄牙语机器翻译(MT),宜用诸策,如数句相择,以成代译之单语机器翻译,及合数n-best译。 吾实验明加此术于基线英语葡萄牙语译之性也。', 'ru': 'В этой статье описывается подача Центра ADAPT на Staple (синхронный перевод и перефразирование для языкового образования) 2020, совместная задача 4-го семинара по нейронному генерации и переводу (WNGT), для задачи перевода с английского на португальский. В этой совместной задаче участников попросили подготовить высокопокрытые наборы правдоподобных переводов с учетом подсказок на английском языке (предложения источника ввода). Мы представляем наши модели машинного перевода с английского на португальский (MT), которые были построены с применением различных стратегий, например, выбор данных и предложений, одноязычный MT для создания альтернативных переводов и сочетание нескольких n-лучших переводов. Наши эксперименты показывают, что добавление вышеупомянутых методик к базовой линии дает отличную производительность в задаче перевода с английского на португальский язык.', 'hi': 'यह पेपर STAPLE (भाषा शिक्षा के लिए एक साथ अनुवाद और अनुवाद) 2020 के लिए ADAPT केंद्र के प्रस्तुतीकरण का वर्णन करता है, जो अंग्रेजी-से-पुर्तगाली अनुवाद कार्य के लिए तंत्रिका पीढ़ी और अनुवाद (WNGT) पर 4 वीं कार्यशाला का एक साझा कार्य है। इस साझा कार्य में, प्रतिभागियों को अंग्रेजी संकेतों (इनपुट स्रोत वाक्यों) को देखते हुए प्रशंसनीय अनुवादों के उच्च-कवरेज सेट का उत्पादन करने के लिए कहा गया था। हम अपने अंग्रेजी-से-पुर्तगाली मशीन अनुवाद (एमटी) मॉडल प्रस्तुत करते हैं जो विभिन्न रणनीतियों को लागू करते हुए बनाए गए थे, उदाहरण के लिए डेटा और वाक्य चयन, वैकल्पिक अनुवाद उत्पन्न करने के लिए मोनोलिंगुअल एमटी, और कई एन-बेस्ट अनुवादों के संयोजन। हमारे प्रयोगों से पता चलता है कि आधार रेखा में उपर्युक्त तकनीकों को जोड़ने से अंग्रेजी-से-पुर्तगाली अनुवाद कार्य में एक उत्कृष्ट प्रदर्शन होता है।', 'ga': 'Déanann an páipéar seo cur síos ar aighneacht an Ionaid ADAPT do STAPLE (Aistriúchán Comhuaineach agus Athinsint don Oideachas Teanga) 2020, tasc comhroinnte den 4ú Ceardlann ar Ghiniúint Néarach agus Aistriúcháin (WNGT), don tasc aistriúcháin Béarla-go-Portaingéilis. Sa tasc roinnte seo, iarradh ar na rannpháirtithe tacair ardchlúdaigh d’aistriúcháin sochreidte a chur ar fáil nuair a tugadh leideanna Béarla (abairtí foinse ionchuir). Cuirimid i láthair ár múnlaí aistriúchán meaisín Béarla-go-Portaingéilis (MT) a tógadh ag cur straitéisí éagsúla i bhfeidhm, m.sh. roghnú sonraí agus abairtí, MT aonteangach chun aistriúcháin mhalartacha a ghiniúint, agus aistriúcháin iolracha n-is fearr a chomhcheangal. Léiríonn ár dturgnaimh go n-éiríonn thar barr leis an tasc aistriúcháin Béarla-go-Portaingéilis trí na teicníochtaí thuasluaite a chur leis an mbunlíne.', 'ka': 'ამ დოკუმენტის აღწერა ADAPT ცენტრის STAPLE-ზე (სიტყველური თავსწორება და პარაფრაზაცია ენის განათლებისთვის) 2020-ში, 4-ი სამუშაო სამუშაო სამუშაო პროგრალური განათლების და განაცვლის (WNGT) სამუშა ამ საზოგადომი დავალებში, მოწყობინებელი მოთხოვნილია, რომ უფრო დიდი საზოგადომი განახლებელი განახლებების ნაწილად გამოიყენება ანგლისური საზოგადოებების (შე ჩვენ ჩვენი ანგლისური-პორგუტებური მაქანის გარგულისხმების (MT) მოდელები, რომლებიც შექმნა განსხვავებული სტრატიგიების გამოყენებას, მაგალითად მონაცემები და სიტყვების ამორჩევა, მონოლენგური MT ჩვენი ექსპერიმენტები გამოჩვენება, რომ დამატება წინასწორეული ტექნექციების დამატება ბაზილინში იქნება საკეთესო გამოყენება ინგლისურად-პორგუტებურად გან', 'it': "Questo articolo descrive la presentazione del Centro ADAPT a STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, un compito condiviso del 4° Workshop sulla Generazione Neurale e Traduzione (WNGT), per il compito di traduzione dall'inglese al portoghese. In questo compito condiviso, ai partecipanti è stato chiesto di produrre serie di traduzioni plausibili ad alta copertura con prompt in inglese (frasi sorgente di input). Presentiamo i nostri modelli di traduzione automatica inglese-portoghese (MT) costruiti applicando varie strategie, ad esempio la selezione di dati e frasi, MT monolingue per generare traduzioni alternative e combinando più traduzioni n-best. I nostri esperimenti dimostrano che l'aggiunta delle tecniche di cui sopra alla base produce un'eccellente prestazione nel compito di traduzione inglese-portoghese.", 'kk': 'Бұл қағаз 2020 жылы STAPLE орталығына ADAPT орталығын (тіл білім беру және парафразы үшін бірнеше аудару және тіл білім беру үшін) бөлек тапсырманы, ағылшын- және португалық аудару тапсырмасының 4- ші жұмысының ортақтаст Бұл ортақтастырылған тапсырмада қатысушыларды ағылшынша сұраулар (кіріс көзі сұраулар) жоғары мәлімет жиындарын жасауға сұралды. Біз ағылшын тілінен португалық компьютердің аудармаларын (MT) үлгілерімізді таңдаймыз. Мысалы, деректер мен сөйлемелерді таңдау үшін, альтернативті аудармаларды құру үшін монолингілік MT үлгілерін таңдаймы Біздің тәжірибеміз алдыңғы техникаларды негізгі жолда қосу үшін ағылшын және Португалия аудармалардың тапсырмасында жақсы істеу үшін көрсетеді.', 'mk': 'Овој весник го опишува поднесувањето на АДАПТ центарот на STAPLE (Симултанео преведување и парафраза за јазичко образование) 2020, заедничка задача на четвртиот работилник за неурална генерација и преведување (WNGT), за задачата на англиско-португалско преведување. Во оваа заедничка задача, учесниците беа побарани да создадат големи сети веројатни преведувања со англиски информации (реченици од изворот на внесување). Ги претставуваме нашите модели на англиско-португалски машински превод (МТ) кои беа изградени при апликација на различни стратегии, како на пример избор на податоци и реченици, монојазичен МТ за генерирање алтернативни преводи и комбинација на повеќе n-најдобри Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Portuguese translation task.', 'hu': 'Ez a tanulmány bemutatja az ADAPT Központ beadványát az angol-portugál fordítási feladat céljából a STAPLE (Simultanean Translation and Paraphrase for Language Education) 2020-ra, amely a 4. Neural Generation and Translation Workshop (WNGT) közös feladata. Ebben a megosztott feladatban a résztvevőket arra kérték, hogy készítsenek nagy lefedettségű, valószínű fordításokat angol utasításokkal (beviteli forráskód mondatokkal). Bemutatjuk angol-portugál gépi fordítási (MT) modelleinket, amelyeket különböző stratégiák alkalmazásával építettünk, például adat- és mondatválasztás, egynyelvű MT alternatív fordítások létrehozására és több n-best fordítás kombinálására. Kísérleteink azt mutatják, hogy a fent említett technikák hozzáadása kiváló teljesítményt eredményez az angol-portugál fordítási feladatban.', 'ms': 'Kertas ini menggambarkan penghantaran Pusat ADAPT kepada STAPLE (Translation Simultaneous and Paraphrase for Language Education) 2020, tugas terkongsi Ke-4 Workshop on Neural Generation and Translation (WNGT), untuk tugas terjemahan Inggeris-Portugis. Dalam tugas terkongsi ini, peserta diminta untuk menghasilkan set penyamaran tinggi bagi terjemahan yang boleh diterima dengan maklumat bahasa Inggeris (kalimat sumber input). Kami memperkenalkan model terjemahan mesin bahasa Inggeris ke Portugis (MT) yang dibina melaksanakan berbagai strategi, cth. pemilihan data dan kalimat, MT monobahasa untuk menghasilkan terjemahan alternatif, dan menggabungkan pelbagai n-terbaik terjemahan. Eksperimen kami menunjukkan bahawa menambah teknik yang terdahulu ke dasar asas memberikan prestasi yang baik dalam tugas terjemahan bahasa Inggeris ke Portugis.', 'mt': "Dan id-dokument jiddeskrivi s-sottomissjoni taċ-Ċentru ADAPT lil STAPLE (Traduzzjoni Simultanja u Parafrażi għall-Edukazzjoni tal-Lingwi) 2020, kompitu kondiviż tar-Raba' Ħidma dwar il-Ġenerazzjoni u t-Traduzzjoni Newrali (WNGT), għall-kompitu ta' traduzzjoni Ingliż-Portugiż. F’dan il-kompitu kondiviż, il-parteċipanti ntalbu jipproduċu settijiet ta’ kopertura għolja ta’ traduzzjonijiet plawżibbli mogħtija bil-pronti Ingliż (sentenzi tas-sors ta’ input). Aħna nippreżentaw il-mudelli tagħna tat-traduzzjoni bil-magni Ingliż-Portugiż (MT) li nbnew bl-applikazzjoni ta’ diversi strateġiji, pereżempju l-g ħażla tad-dejta u tas-sentenzi, MT monolingwi għall-ġenerazzjoni ta’ traduzzjonijiet alternattivi, u l-kombinazzjoni ta’ diversi n-aħjar traduzzjonijiet. L-esperimenti tagħna juru li ż-żieda tat-tekniki msemmija hawn fuq mal-linja bażi twassal għal prestazzjoni eċċellenti fil-kompitu tat-traduzzjoni Ingliż-Portugiż.", 'el': 'Η παρούσα εργασία περιγράφει την υποβολή του Κέντρου στο ΣτάPLE (Ταυτόχρονη Μετάφραση και Παράφραση για την Εκπαίδευση της Γλώσσας) 2020, ένα κοινό έργο του 4ου Εργαστηρίου Νευρικής Γενιάς και Μετάφρασης (WNGT), για το μεταφραστικό έργο Αγγλικά-Πορτογαλικά. Σε αυτό το κοινό έργο, ζητήθηκε από τους συμμετέχοντες να παράγουν υψηλής κάλυψης σύνολα αξιόπιστων μεταφράσεων με αγγλικές προτροπές (προτάσεις πηγής εισαγωγής). Παρουσιάζουμε τα μοντέλα μηχανικής μετάφρασης (ΜΤ) που κατασκευάστηκαν εφαρμόζοντας διάφορες στρατηγικές, π.χ. επιλογή δεδομένων και προτάσεων, μονόγλωσσο ΜΤ για τη δημιουργία εναλλακτικών μεταφράσεων και συνδυάζοντας πολλαπλές και καλύτερες μεταφράσεις. Τα πειράματά μας δείχνουν ότι η προσθήκη των προαναφερθέντων τεχνικών στη βάση αποδίδει εξαιρετική απόδοση στην εργασία της αγγλικής-πορτογαλικής μετάφρασης.', 'no': 'Denne papiret beskriver at ADAPT-senteret sender til STAPLE (Simulert omsetjing og parafrase for språk utdanning) 2020. Dette er ein delt oppgåve av den 4. arbeidsområdet på neurogenerering og omsetjing (WNGT) for omsetjinga til engelsk til portugisk. I denne delt oppgåva vert deltakarane spurd om å produsera høg dekkefølgje av tilgjengelege omsetjingar i engelske spørsmål (inndatakjeldesetningar). Vi presenterer våre engelsk- til- portugisisk maskinsomsetjingsmodular (MT) som ble bygd med forskjellige strategiar, f.eks. data og setningsverking, monolingual MT for å laga alternativ omsetjingar, og kombinere fleire n- best e omsetjingar. Eksperimentane våre viser at å leggja til dei foregåande teknikka til baselinja gir eit utrolig utvikling i omsetjinga engelsk til portugisk.', 'lt': "This paper describes the ADAPT Centre's submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task.  Šioje bendroje užduotyje dalyviai buvo paprašyti parengti didelės apimties patikimų vertimų rinkinius, pateiktus anglų kalba (įvesties šaltinio sakiniai). Mes pristatome savo anglų ir portugalų vertimo mašinomis (MT) modelius, kurie buvo sukurti taikant įvairias strategijas, pvz., duomenų ir sakinių atranką, vienkalbį MT alternatyviems vertimams kurti ir įvairių n geriausių vertimų derinimą. Mūsų eksperimentai rodo, kad pirmiau minėtų metodų įtraukimas į pradinę vertimo dieną duoda puikių rezultatų anglų kalba į portugalų kalbą.", 'pl': 'Niniejszy artykuł opisuje zgłoszenie Centrum ADAPT do STAPLE (Simultane Translation and Parafraze for Language Education) 2020, wspólnego zadania IV Warsztatu Generacji Neuralnej i Tłumaczenia (WNGT), dla zadania tłumaczenia angielsko-portugalskiego. W ramach tego wspólnego zadania uczestnicy zostali poproszeni o stworzenie wysokiego zasięgu zestawów wiarygodnych tłumaczeń z angielskimi podpowiedziami (zdania źródłowe wejścia). Przedstawiamy nasze modele tłumaczeń maszynowych (MT) z języka angielskiego na portugalski, które zostały zbudowane w oparciu o różne strategie, np. wybór danych i zdań, jednojęzyczny MT do generowania alternatywnych tłumaczeń oraz łączenie wielu n-best tłumaczeń. Nasze eksperymenty pokazują, że dodanie wyżej wymienionych technik do bazy danych daje doskonałą wydajność w zadaniu tłumaczenia angielsko-portugalskiego.', 'sr': 'Ovaj papir opisuje podnošenje ADAPT Centra STAPLE (istovremeno prevod i parafraza za obrazovanje jezika) 2020. godine, zajednički zadatak četvrtog radionice o neuronskoj generaciji i prevodi (WNGT), za posao prevoda engleskog i portugalskog jezika. U ovom zajedničkom zadatku, sudionici su zamolili da proizvode visoke prekrivene sete uvjerljivih prevoda s obzirom na engleski promet (rečenice iz izvora). Predstavljamo naše modele prevoda engleskog na portugalsku mašinu (MT) koji su izgrađeni primjenjujući različite strategije, npr. podatke i izbor rečenica, monojezički MT za stvaranje alternativnih prevoda i kombinaciju višestrukih n-najboljih prevoda. Naši eksperimenti pokazuju da dodavanje navedenih tehnika na početnu liniju pruža odličan izvor u zadatku prevođenja engleskog na portugalski.', 'ro': 'Această lucrare descrie depunerea Centrului ADAPT la STAPLE (Traducere simultană și parafrază pentru educația lingvistică) 2020, o sarcină comună a celui de-al patrulea atelier de lucru privind generarea și traducerea neurală (WNGT), pentru sarcina de traducere din engleză în portugheză. În această sarcină comună, participanții au fost rugați să producă seturi de traduceri plauzibile cu solicitări în limba engleză (propoziții sursă de introducere). Vă prezentăm modelele noastre de traducere automată din engleză în portugheză (MT) care au fost construite aplicând diverse strategii, de exemplu selectarea datelor și propozițiilor, MT monolingv pentru generarea traducerilor alternative și combinând mai multe n-cele mai bune traduceri. Experimentele noastre arată că adăugarea tehnicilor menționate mai sus la bază produce o performanță excelentă în sarcina de traducere din engleză în portugheză.', 'so': 'Kanu waa qoraal ku qoran xarunta ADAPT uu u dhiibo STAPLE (Simultaneous Translation and Paraphrase for Education of Language) 2020, kaas oo ah shaqo qayb la mid ah 4. Workshop on Neural Generation and Translation (WNGT), kaas oo ah task of Ingiriis-to-Portuguese translation. Shaqadan la qaybsaday waxaa la weyddiiyey in dadka ka qeybqaaday ay soo saaraan koobab aad u badan oo turjumaadyo la micnexeynayo oo la siiyay qoraalka ingiriisiga (xarumaha ingiriisiga). Waxaynu soo bandhignaa turjumista machine-afka Ingiriiska-ilaa-Burtuqiis (MT) modellada loo dhisay inay codsadaan strategiyo kala duduwan, tusaale ahaan data iyo doorashada sentenceka, MT-ka muuqashada tarjumaadyo kala duduwan, waxaana lagu soo bandhigayaa tarjumaadyo kala duduwan. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Portuguese translation task.', 'sv': 'Denna uppsats beskriver ADAPT-centrumets bidrag till STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, en gemensam uppgift för den fjärde workshopen om neural generation och översättning (WNGT), för översättningsuppgiften engelsk-portugisiska. I denna delade uppgift ombads deltagarna att producera högtäckande uppsättningar av rimliga översättningar med engelska uppmaningar (inmatningskällmeningar). Vi presenterar våra engelsk-portugisiska maskinöversättningsmodeller (MT) som byggts utifrån olika strategier, t.ex. data- och meningsval, enspråkig MT för att generera alternativa översättningar och kombinera flera n-bästa översättningar. Våra experiment visar att tillägg av de ovannämnda teknikerna till baslinjen ger en utmärkt prestanda i översättningsuppgiften från engelska till portugisiska.', 'ta': "This paper describes the ADAPT Centre's submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task.  இந்த பகிர்ந்த செயலில் பங்கீட்டாளர்கள் ஆங்கிலத்தின் விருப்பங்கள் கொடுக்கப்பட்ட உயர்ந்த மொழிபெயர்ப்புகளை உருவாக்க க @ info எங்கள் சோதனைகள் முன்னோக்கப்பட்ட தொழில்நுட்பத்தை அடிப்படையில் சேர்த்து ஒரு சிறந்த செயல்பாடு ஆங்கிலத்தில் இருந்து போ", 'ml': 'ഈ പത്രത്തില്\u200d എഡിഎപ്പിറ്റ് സെന്\u200dറിന്\u200dറെ സന്ദേശം സ്റ്റാപ്ലിന് വിശദീകരിക്കുന്നു (സാമാതൃക പരിഭാഷവും ഭാഷ വിദ്യാഭ്യാസത്തിനുവേണ്ടി പാരാപ്രസ്താവിക്കുന്ന) 2020, നെ ഈ പങ്കാളിയുള്ള ജോലിയില്\u200d പങ്കാളികളോട് ആജ്ഞാപിക്കപ്പെട്ടിരുന്നു. ഇംഗ്ലീഷ് പ്രോപ്റ്റുകള്\u200d നല്\u200dകിയിരിക്കുന് നമ്മുടെ ഇംഗ്ലീഷില്\u200d നിന്നും പോര്\u200dട്ടുഗീഷിലെ മെഷിന്\u200d പരിഭാഷ (എംടി) മോഡലുകള്\u200d നാം കാണിച്ചുകൊടുക്കുന്നു. വ്യത്യസ്ത ഘടനകള്\u200d പ്രയോഗിക്കുന്നു. ഉദാഹരണത്തി നമ്മുടെ പരീക്ഷണങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നത് മുമ്പ് പറഞ്ഞ സാങ്കേതികവിദ്യകള്\u200d കൂട്ടിച്ചേര്\u200dക്കുന്നത് ഇംഗ്', 'mn': 'Энэ цаас "ADAPT Центр" STAPLE-д (Англи-болон Португалийн хөгжлийн ажлын 4-р ажлын хуваалцааны ажлыг) тайлбарладаг. Энэ хуваалцааны ажил дээр оролцогчдыг Англи хэлний хувьд (орхих эх үүсвэрийн өгүүлбэрүүдийг) өндөр хэлбэртэй хэлбэрүүдийг бий болгохыг хүсч байсан. Бид Англи-аас Португалийн машины хөрөнгө оруулалт (MT) загварыг тайлбарлаж, олон стратегийг ашиглаж, жишээ нь өгөгдлийн, өгөгдлийн сонголт, өөр нэг хэлний хөрөнгө оруулалт гаргаж, n-хамгийн сайн хөрөнгө оруулалт Бидний туршилтууд суурь шугам руу дээр өнгөрсөн техникуудыг нэмэх нь Англи-болон Португалийн орчуулалтын ажлын гайхалтай үйл ажиллагааг гаргадаг.', 'si': 'මේ පැත්තේ ADAPT මධ්\u200dයස්ථානයේ STApleE (සිමුල්ටනිය අභිවාදය සහ භාෂා ශික්ෂණය සඳහා පැරැෆ්\u200dරේස් සඳහා භාෂා ශික්ෂණය) 2020යි, 4වෙනි වැඩසටහන් සඳහා  මේ වැදගත් වැඩේදී, අංග්\u200dරීසි ප්\u200dරොම්ප්ට් (ඇතුළු මුළු වාක්ය) ගොඩක් හොඳ ආවරණ සෙට් නිර්මාණය කරන්න අහලා ත අපි අපේ ඉංග්\u200dරීසි වල පොර්තුතුජින් මැෂින් වාර්තාව (MT) නිර්මාණය කරන්න පුළුවන් විවිධ ප්\u200dරයෝජනය සහ වාර්තාව තෝරණය, වෙනස් භාෂාව අපේ පරීක්ෂණය පෙන්වන්නේ මුලින් ඉදිරියට පොර්තුතික වාර්තාවෙන් ඉංග්\u200dරීසි වලින් පොර්තුතික වාර්තාවෙන් ඉතු', 'ur': 'اس کاغذ نے STAPLE (زبان تعلیم کے لئے سیمالٹ ترجمہ اور پارافریز) کے لئے ADAPT سنٹر کی مزید بیان کی ہے، نورول پیدائش اور ترجمہ (WNGT) کے چوتھے ویرشاپ کا مشترک کام ہے، انگلیسی سے پورٹوګیز ترجمہ کے کام کے لئے۔ اس مشترک کام میں مشترک لوگوں سے پوچھا گیا تھا کہ انگلیسی پورٹ (انگلیسی سورج جماعتیں) کو اچھی پورٹ کریں۔ ہم اپنے انگلیسی سے پورٹوګیسی مشین ترجمہ (MT) موڈل کو پیش کرتے ہیں جو مختلف استراتژی کے مطابق بنائے گئے ہیں، جیسے ڈیٹا اور جماعت انتخاب کرنے کے لئے، ایک زبان مٹی کے مطابق مختلف ترجمہ پیدا کرنے کے لئے اور بہت سے n-بہترین ترجمہ پیدا کرنے ہماری آزمائش دکھاتی ہے کہ ابتدال کی تکنیک جمع کرنے کی وجہ سے انگلیسی سے پورچوٹی ترجمہ کے کام میں بہترین فعالیت حاصل کرتی ہے.', 'uz': "Name Bu ishlatilgan vazifani o'rganishlar ingliz soʻzlari (kiritish манбаъ soʻzlari) ga oʻrnatilgan eng yuqori tarjima tarjimalarning moslamalarini yaratish talab qilindi. Biz ingliz tilidagi portugalchaga tarjima qilish modellarimizni (MT) present qilamiz. Bu bir xil strategiya qoʻllaniladigan, masalan, ma'lumot va sentence tanlash uchun, boshqa tarjima tarjima qilish uchun monolingual MT'ni yaratish va bir necha va yaxshi tarjima qilish uchun bir nechta tarjima qilamiz. Bizning tajribalarimiz esa birinchi taʼminlovchi tekniklarni asboblar tarjima vazifani qoʻshish uchun ingliz-portugalcha tahrirlash vazifani ajoyib bajaradi.", 'vi': 'Tờ giấy này mô tả bản tin Trung tâm ADAT đã nộp cho STAPLE (thoại đồng thời và cụm từ Paracụm từ cho nền giáo dục ngôn ngữ) 2020, nhiệm vụ chia s ẻ của bốn xưởng làm việc về ngắt thần kinh (WNGT), cho nhiệm vụ dịch chuyển Anh-tới-Bồ Đào Nha. Trong nhiệm vụ chia sẻ này, các diễn viên đã được yêu cầu sản xuất những bộ dịch giọng hợp lý được đưa ra lời nhắc tiếng Anh (câu đầu từ nhập). Chún g tôi giới thiệu các mô hình dịch cỗ máy từ Anh-tơ-Bồ Đào Nha (MTV) được xây dựng dựa trên các chiến lược khác nhau, ví dụ như chọn dữ liệu và câu, kênh kênh n g ôn ngữ để tạo ra bản dịch thay thế, và kết hợp dịch n-best nhiều lần. Những thí nghiệm của chúng tôi cho thấy việc thêm những kỹ thuật này vào cơ sở cơ sở cơ bản sẽ thành công mỹ mãn trong nhiệm vụ dịch chuyển Anh-Bồ Đào Nha.', 'bg': 'Настоящата статия описва представянето на Центъра АДАПТ в съвместна задача на Четвъртия семинар по невронно генериране и превод (УНГТ), за задачата превод от английски на португалски език. В тази споделена задача участниците бяха помолени да изготвят серии от правдоподобни преводи на английски език (входни изречения). Представяме нашите модели на машинен превод от английски на португалски език (МТ), които са изградени като прилагат различни стратегии, например подбор на данни и изречения, едноезичен МТ за генериране на алтернативни преводи и комбиниране на множество n-best преводи. Нашите експерименти показват, че добавянето на гореспоменатите техники към базовата база дава отлично представяне в задачата за превод от английски на португалски.', 'da': 'Denne artikel beskriver ADAPT-centrets indsendelse til STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, en fælles opgave for 4. Workshop om Neural Generation and Translation (WNGT), til den engelsk-portugisiske oversættelsesopgave. I denne fælles opgave blev deltagerne bedt om at producere sæt med høj dækning af plausible oversættelser givet engelske prompts (input source sætninger). Vi præsenterer vores engelsk-portugisisk maskinoversættelsesmodeller (MT), der er bygget med forskellige strategier, f.eks. data- og sætningsvalg, ensproget MT til generering af alternative oversættelser og kombinering af flere n-bedste oversættelser. Vores eksperimenter viser, at tilføjelse af ovennævnte teknikker til baseline giver en fremragende præstation i den engelsk-portugisiske oversættelsesopgave.', 'hr': 'Ovaj papir opisuje podnošenje ADAPT Centra STAPLE (istodobno prevod i parafraza za obrazovanje jezika) 2020. godine, zajednički zadatak četvrtog radionice o neuronskoj generaciji i prevodi (WNGT), za zadatak prevoda engleskog i portugalskog jezika. U ovom zajedničkom zadatku, sudionici su zamolili da proizvode visoko pokrivene sete uvjerljivih prevoda s obzirom na engleski zahtjevi (rečenice iz izvora). Predstavljamo naše modele prevoda strojeva engleskog na portugalski (MT) koji su izgrađeni primjenjujući različite strategije, na primjer odabir podataka i rečenica, monojezički MT za stvaranje alternativnih prevoda i kombinaciju višestrukih n-najboljih prevoda. Naši eksperimenti pokazuju da dodavanje navedenih tehnika na početnu liniju pruža odličan učinak u zadatku prevođenja engleskog na portugalski.', 'de': 'Dieser Beitrag beschreibt die Einreichung des ADAPT Zentrums an STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, eine gemeinsame Aufgabe des Vierten Workshops zur Neuralen Generation und Übersetzung (WNGT), für die Übersetzungsaufgabe Englisch-Portugiesisch. In dieser gemeinsamen Aufgabe wurden die Teilnehmer gebeten, hochdeckende Sätze plausibler Übersetzungen mit englischen Eingabeaufforderungen (Eingabequellensätze) zu erstellen. Wir stellen unsere maschinellen Übersetzungsmodelle (MT) von Englisch nach Portugiesisch vor, die unter Anwendung verschiedener Strategien entwickelt wurden, z.B. Daten- und Satzauswahl, einsprachige MT zur Generierung alternativer Übersetzungen und die Kombination mehrerer n-best Übersetzungen. Unsere Experimente zeigen, dass die Hinzufügung der oben genannten Techniken zu einer hervorragenden Leistung in der Übersetzungsaufgabe Englisch-Portugiesisch führt.', 'nl': 'Dit artikel beschrijft de inzending van het ADAPT Centre aan STAPLE (Simultane Translation and Paraphrase for Language Education) 2020, een gedeelde taak van de 4e Workshop on Neural Generation and Translation (WNGT), voor de vertaaltaak Engels-Portugees. In deze gezamenlijke taak werden de deelnemers gevraagd om veelomvattende sets van plausibele vertalingen te produceren met Engelse prompts (input source zinnen). We presenteren onze Engels-naar-Portugees machine translation (MT) modellen die zijn gebouwd met behulp van verschillende strategieën, zoals data- en zinnenselectie, eentalige MT voor het genereren van alternatieve vertalingen en het combineren van meerdere n-beste vertalingen. Onze experimenten tonen aan dat het toevoegen van de bovengenoemde technieken aan de basislijn een uitstekende prestatie levert in de vertaaltaak Engels-Portugees.', 'fa': 'این کاغذ تحویل مرکز ADAPT به STAPLE (ترجمه\u200cهای یکسان و پارافریز برای تعلیم زبان) ۲۰۰۲ را توصیف می\u200cکند، یک کار مشترک در چهارمین کارگاه روی نسل\u200cهای عصبی و ترجمه\u200cهای عصبی (WNGT) برای کار ترجمه\u200cهای انگلیسی به پورتوژیک. در این وظیفه مشترک، شرکتگران را خواسته می\u200cشد که مجموعه\u200cهای پرداخت بالا از ترجمه\u200cهای قابل توجه کنند که به انگلیسی اجازه داده می\u200cشود (جمله\u200cهای منبع ورودی). ما مدل\u200cهای ترجمه\u200cهای ماشین انگلیسی به پورتوژیک (MT) خود را پیشنهاد می\u200cکنیم که با استراتژی\u200cهای مختلف ساخته شده\u200cاند، مثلا داده\u200cها و انتخاب جمله\u200cها، MT تنها زبان برای تولید ترجمه\u200cهای متفاوتی و ترجمه\u200cهای n-بهترین را ترجمه می\u200cکنی آزمایشات ما نشان می دهند که اضافه کردن تکنیک\u200cهای پیش\u200cبینی به خط پایین یک عملکرد فوق العاده در کار ترجمه\u200cی انگلیسی به پورتوژیک می\u200cدهد.', 'id': 'Kertas ini menggambarkan pengiriman Pusat ADAPT ke STAPLE (Translation Simultaneous and Paraphrase for Language Education) 2020, tugas yang sama dari Workshop Keempat tentang Generasi Neural dan Translation (WNGT), untuk tugas terjemahan Inggris-Portugis. Dalam tugas berbagi ini, para peserta diminta untuk menghasilkan set penyamaran tinggi dari terjemahan yang dapat dipercaya diberikan petunjuk bahasa Inggris (kalimat sumber input). Kami memperkenalkan model mesin terjemahan bahasa Inggris ke Portugis (MT) yang dibangun melaksanakan berbagai strategi, misalnya seleksi data dan kalimat, MT monobahasa untuk menghasilkan terjemahan alternatif, dan menggabungkan banyak n-terbaik terjemahan. Eksperimen kami menunjukkan bahwa menambahkan teknik-teknik yang sebelumnya ke dasar dasar memberikan prestasi yang luar biasa dalam tugas terjemahan bahasa Inggris ke Portugis.', 'ko': '본고는 ADAPT센터가 Stype(언어교육 동시통역과 의역) 2020에 제출한 보고서를 기술하는데, 이는 제4회 신경생성과 번역 세미나(WNGT)의 공통 임무로 영어에서 포르투갈어로 번역하는 데 쓰인다.이 공유 임무에서 참가자들은 영어 알림(소스 입력 문장)을 제공하는 상황에서 높은 범위의 합리적인 번역집을 제공해야 한다.우리는 우리의 영어부터 포르투갈어기계번역(MT)모델을 보여 주었다. 이 모델은 각종 전략을 응용하여 구축된 것이다. 예를 들어 데이터와 문장 선택, 대체 번역을 생성하는 단어기계번역, 여러 n-best번역을 조합한 것이다.우리의 실험은 영어에서 포르투갈어로 번역하는 임무에서 상술한 기술을 기선에 첨가하면 뛰어난 성능을 낼 수 있음을 나타냈다.', 'af': "Hierdie papier beskrywe die ADAPT Sentrum se onderskrywing na STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, 'n gedeelde taak van die 4de Werkshop op Neural Generation and Translation (WNGT), vir die Engels-na-Portugees vertaling taak. In hierdie gedeelde taak is die deelnimmers gevra om hoë- dekkeningsstelle van verplaasbare vertalings gegee Engelske voorskrifte te produseer (invoer bron setings). Ons stel ons Engels-na-Portugese masjien vertaling (MT) modele wat gebou word met verskeie strategies, bv. data e n setkeuse, monolinglike MT om alternatiewe vertalings te genereer en veelvuldige n-best e vertalings te kombinerer. Ons eksperimente wys dat toevoeg van die voorgeskryfde teknike aan die basislien gee 'n uiteindelike prestasie in die Engels-na-Portugese vertalingstaak.", 'tr': "Bu kagyz ADAPT Merkeziniň STAPLE'a (Taýýarlan terjime we Paraphrase for Language Education) 2020-nji ýylda , iňlisçe-we-Portugalça terjime göreviniň 4-nji sany bellenilýär. Bu beýleki täblisaň içinde, iştirakçilerden beýleki ýazgyş terjime edilen iňlis dilinde (giriş çeşme sözleri) döretmäge soryldy. Biz Iňlisçe-Portugalça maşynyň terjimelerimizi (MT) nusgalarymyzy g örkeýäris. Çe şitli strategiýalary üçin uygulanan, meselâ data we sözlemler saýlamak, başga terjimeleri döretmek üçin monodil MT we birnäçe n-iň gowy terjimeleri birleşdirmek üçin. Biziň deneylerimiz öňünden geçen tehnikalary beýleki çyzgyna eklemek iňlisçe-we portugalça terjime etmegi täsirinde örän gowy täsirleýär.", 'am': 'ይህ ገጽ ለSTAPLE (ለቋንቋ ትርጓሜ እና ለቋንቋ ትምህርት ትርጓሜ እና ፓራፕራሲ) ለኢንጂልኛ-ወደ ፖርቱጋሊኛ ትርጓሜ ስራ በ4ኛ workshop (WNGT) የተካፈለ ጉዳይ ይናገራል፡፡ በዚህ በተካፈሉት ስራ ተጋሪዎቹ የኢንግሊዝኛ ግንኙነት (የኢንጂል ምንጭ ክፍሎች) የተሰጡ የደረጃ ክፍተቶችን መፍጠር ይጠይቃሉ፡፡ እንግሊዝኛ-ወደ ፖርቱጋዊው መሣሪያን ትርጉም (MT) ሞዴላዎችን እናቀርባታለን፤ በተለይ ትርጉም ለመፍጠር፣ ዳታ እና የፍርድ ምርጫ፣ የተለየ ልዩ ትርጉም ለመፍጠር እና በብዙ n-መልካም ትርጓሜዎችን ለማቀናቀል ተዘጋጅተናል፡፡ ፈተናዎቻችን አስቀድሞ በተወሰነ ጥናት ለመጨመር እንግሊዝኛ-ወደ ፖርቱጋዊ ትርጉም አድራሻ የተሻለ ስርዓት እንዲያሳየው ነው፡፡', 'sq': 'Ky dokument përshkruan paraqitjen e Qendrës ADAPT ndaj STAPLE (Translation Simultaneous and Paraphrase for Language Education) 2020, një detyrë e përbashkët të Workshop të katërt mbi Gjenerimin dhe Translation Neural (WNGT), për detyrën e përkthimit anglez-portugal. Në këtë detyrë të përbashkët, pjesëmarrësit u kërkuan të prodhojnë sërat e mbulimit të lartë të përkthimeve të besueshme të dhëna në gjuhën angleze. Ne prezantojmë modelet tona të përkthimit të makinave anglisht-portugal (MT) që u ndërtuan duke aplikuar strategji të ndryshme, për shembull zgjedhjen e të dhënave dhe fjalëve, MT monogjuhës për krijimin e përkthimeve alternative dhe kombinimin e përkthimeve të shumta n-më të mira. Eksperimentet tona tregojnë se shtimi i teknikave të përmendura në bazë jep një shfaqje të shkëlqyer në detyrën e përkthimit anglez-portugal.', 'az': 'Bu kağıt ABAPT Merkezinin STAPLE-ə (Tövrat təhsilinə bənzəyən və dil təhsilinə bənzəyən Paraphrase for Language Education) 2020-nin 4. Workshop on Neural Generation and Translation (WNGT) və İngilizə-to-Portuguese çeviri işinin paylaşılmış vəzifəsini təsdiq edir. Bu paylaşılmış işdə, iştirakçilərə İngilizce təbliğlərini (giriş mənbə cümlələri) təhsil etmək üçün yüksək pərdə çəkilir. İngilizce-Portugalca maşın çevirimi (MT) modellərimizi g östəririk ki, müxtəlif stratejilər, məlumatlar və cümlələr seçimi, alternatif çeviriləri yaratmaq üçün monodilli MT və n-ən yaxşı çeviriləri birləşdirir. Bizim təcrübələrimiz belə göstərir ki, əvvəlkilərin təcrübələrini baz çətinliklərə əlavə etmək İngilizə-Portugalca çeviriş işində gözəl bir performans yaradır.', 'bn': "This paper describes the ADAPT Centre's submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task.  এই শেয়ার কর্মসূচীতে অংশগ্রহণকারীদের অনুবাদের উচ্চ কাভারেজের সেট তৈরি করার আহ্বান জানানো হয়েছিল ইংরেজি অনুবাদের ( আমরা আমাদের ইংরেজী থেকে পর্তুগীজ মেশিন অনুবাদ উপস্থাপন করছি (এমটি) মডেল যা বিভিন্ন কৌশল প্রয়োগ করা হয়েছে, যেমন ডাটা এবং বিকল্প অনুবাদ তৈরি করার জন্য, মোনোলিভাল এমট আমাদের পরীক্ষা দেখাচ্ছে যে পূর্ববর্তী উল্লেখিত প্রযুক্তিগুলো বেসেলাইনে যোগ করে ইংরেজি থেকে পর্তুগীজ অনুবাদ কর্ম", 'sw': 'Gazeti hili linaelezea ujumbe wa kituo cha ADAPT kwa ajili ya STAPLE (Tafsiri na Paraphrase kwa ajili ya Elimu ya Lugha) 2020, ambacho ni jukumu la nne la Warsha ya Uzazi na Tafsiri ya Neural (WNGT), kwa ajili ya kazi ya kutafsiri kwa Kiingereza-hadi-Kireno. In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences).  Tunawasilisha mifano yetu ya kutafsiri mashine ya Kiingereza-hadi-Kireno (MT) ambayo yalijengwa kutumia mbinu mbalimbali, kwa mfano data na uteuzi wa sentensi, MT kwa ajili ya kutengeneza tafsiri mbadala, na kuunganisha tafsiri mbadala nyingine na bora zaidi. Majaribio yetu yanaonyesha kuwa kuongeza mbinu hizo zilizotajwa awali kwenye msingi wa msingi zinaleta utendaji mzuri katika kazi ya tafsiri ya Kiingereza na Kireno.', 'bs': 'Ovaj papir opisuje podatke ADAPT Centra STAPLE (istovremeno prevod i parafraza za obrazovanje jezika) 2020. godine, zajednički zadatak četvrtog radionice o neuronskoj generaciji i prevodi (WNGT), za zadatak prevoda engleskog i portugalskog jezika. U ovom zajedničkom zadatku, sudionici su zamolili da proizvode visoko prekrivene sete uvjerljivih prevoda s obzirom na engleske promene (izreke iz izvora). Predstavljamo naše modele prevoda engleskog na portugalsku mašinu (MT), koje su izgrađene, primjenjujući različite strategije, na primjer odabir podataka i rečenica, monojezički MT za stvaranje alternativnih prevoda i kombinaciju višestrukih n-najboljih prevoda. Naši eksperimenti pokazuju da dodavanje navedenih tehnika na početnu liniju pruža odličan izvor u zadatku prevođenja engleskog i portugalskog jezika.', 'cs': 'Tento příspěvek popisuje předložení střediska ADAPT ke STAPLE (Simultane Translation and Parafráze for Language Education) 2020, společnému úkolu čtvrtého workshopu o nervové generaci a překladu (WNGT), pro překladatelskou úlohu z angličtiny do portugalštiny. V rámci tohoto sdíleného úkolu byli účastníci požádáni, aby vytvořili vysoce pokryté sady věrohodných překladů s anglickými výzvami (vstupní zdrojové věty). Představujeme naše modely strojového překladu (MT) z angličtiny do portugalštiny, které byly vytvořeny s využitím různých strategií, např. výběr dat a vět, jednojjazyčné MT pro generování alternativních překladů a kombinace několika n-best překladů. Naše experimenty ukazují, že přidání výše zmíněných technik k základnímu principu přináší vynikající výkon v anglicko-portugalském překladu.', 'et': 'Käesolevas dokumendis kirjeldatakse ADAPT-keskuse esitatud tööd inglise-portugali tõlkeülesandeks inglise-portugali tõlkeülesandele STAPLE (Simultaneo Translation and Paraphrase for Language Education) 2020 (Simultaneo Translation and Language Education) ühisele ülesandele. Selles ühises ülesandes paluti osalejatel koostada kõrge katvusega tõlgete komplekt inglise keeles (sisendite lähtelaused). Esitleme inglise-portugali masintõlke mudeleid, mis on ehitatud erinevate strateegiate abil, nt andmete ja lausete valimisel, ühekeelsel MT alternatiivsete tõlkete genereerimisel ja mitme n-parima tõlke kombineerimisel. Meie eksperimendid näitavad, et eespool nimetatud tehnikate lisamine lähtetasemele annab suurepärase tulemuse inglise-portugali tõlketöös.', 'hy': 'Այս հոդվածը նկարագրում է ADAPT-ի կենտրոնի ներկայացումը STAPLE-ին 2020-ին, որը հանդիսանում է անգլերեն-պորտուգալերեն թարգմանման աշխատանքի 4-րդ աշխատասենյակի համար: In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences).  Մենք ներկայացնում ենք մեր անգլերեն-պորտուգալերեն մեքենայի թարգմանման (MT) մոդելները, որոնք կառուցվել են օգտագործելով տարբեր ռազմավարություններ, օրինակ տվյալներ և նախադասություններ ընտրելու, միալեզվով MT-ը այլընտրանքային թարգմանություններ ստեղ Մեր փորձարկումները ցույց են տալիս, որ նախորդ տեխնիկաների ավելացումը հիմնական հիմքի մեջ հանգեցնում է հրաշալի արդյունք անգլերեն-պորտուգալերեն թարգմանման խնդրում:', 'fi': 'T채ss채 artikkelissa kuvataan ADAPT-keskuksen hakemusta STAPLE (Simultaneo Translation and Paraphrase for Language Education) 2020 -ohjelmaan, joka on nelj채s hermojen synty채 ja k채채nt채mist채 k채sittelev채 ty철paja (WNGT) yhteinen teht채v채 englannin-portugalin k채채nn철steht채v채채n. T채ss채 yhteisess채 teht채v채ss채 osallistujia pyydettiin tuottamaan laaja joukko uskottavia k채채nn철ksi채 englanninkielisten kehotusten (sy철tteen l채hdelauseet) perusteella. Esittelemme englannin-portugalin konek채채nn철smallimme, jotka on rakennettu soveltamalla erilaisia strategioita, kuten datan ja lauseiden valintaa, monikielist채 MT vaihtoehtoisten k채채nn철sten tuottamiseen ja yhdist채m채ll채 useita n-best k채채nn철ksi채. Kokeemme osoittavat, ett채 edell채 mainittujen tekniikoiden lis채채minen l채ht철tilanteeseen tuottaa erinomaisen suorituskyvyn englannin-portugalin k채채nn철sty철ss채.', 'ca': "Aquest paper descriu la presentació del Centre ADAPT a STAPLE (Translation Simultaneous and Paraphrase for Language Education) 2020, una tasca compartida de la 4ª Workshop on Neural Generation and Translation (WNGT), per la tasca de traducció anglès-portuguès. En aquesta tasca compartida, els participants van demanar que produíssin conjunts de traduccions plausibles de gran cobertura donant preguntes d'anglès (frases de fonts d'entrada). Presentam els nostres models de traducció màquina anglès-portuguès (MT) que van ser construïts aplicant diverses estratègies, per exemple selecció de dades i frases, MT monolingüe per generar traduccions alternatives i combinant múltiples n-millors traduccions. Els nostres experiments demostren que afegir les tècniques mencionades a la base produeix un excel·lent rendiment en la tasca de traducció anglesa-portuguesa.", 'jv': 'Pamir iki rambarang nggawe tarjamahan DEApT center kang STAPLE (Simultaneous translation and ParaFraz for Language Learning) 2020, nggawe gerakan tanggal 4th Workspace na Generation and translation (WNGT), kanggo kelas terjamahan Inggris-to-portugis. Where am I Awak dhéwé nambah model sing itoleh ingkang karo Putuk karo ingkang sampeyan (MT) sing paling nggawe barang nggawe sistem sing sampeyan nggawe sistem dadi karo perangkat sapar kang dadi lan ujar-ujar, MT sing oleh akeh terjamahan Alternate, lan tambah nggawe ujaran sistem sing dibutuhke sing apik n- Awak dhéwé éntuk sing nyengkuyé nambah teknik sing paling apik dhéwé kanggo nggawe barang apik dhéwé kuwi nggawe aturan sing apik dhéwé ning tarjamahan Inggris-karo-portugis.', 'sk': 'V prispevku je opisana predložitev centra ADAPT za prevajanje iz angleščine v portugalščino v STAPLE (Simultaneo Translation and Paraphrase for Language Education) 2020, skupna naloga 4. delavnice o nevralni generaciji in prevajanju (WNGT). V tej skupni nalogi so bili udeleženci pozvani, naj pripravijo visoko pokrite nabore verjetnih prevodov na podlagi angleških pozivov (vhodnih izvornih stavkov). Predstavljamo naše modele strojnega prevajanja angleško-portugalščino (MT), ki so bili zgrajeni z uporabo različnih strategij, npr. izbiro podatkov in stavkov, enojezičnega MT za ustvarjanje alternativnih prevodov in kombinacijo več n-best prevodov. Naši eksperimenti kažejo, da dodajanje omenjenih tehnik v osnovno vrednost prinaša odlično uspešnost pri prevajanju angleščine v portugalščino.', 'he': 'העיתון הזה מתאר את ההעברה של מרכז ADAPT ל STAPLE (Translation Simultaneous and Paraphrase for Language Education) 2020, משימה משותפת של Workshop הרביעי על הגנרציה והתרגום נוירואלים (WNGT), עבור משימה התרגום אנגלית-לפורטוגזית. במשימה המשותפת הזו, ביקשו מהמשתתפים לייצר קבוצות מכסה גבוהה של תרגומות אמיתיות שנתנו נקודות אנגלית (משפטי מקור כניסה). We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple n-best translations.  הניסויים שלנו מראים שההוספת הטכניקות הללו לבסיס נותנת ביצוע מצוין במשימת התרגום אנגלית-לפורטוגזית.', 'ha': "Wannan karatun na describes the ADPAT Centre's Submit to StaPALE (Similarly Translate and Parafrase for education in Lugha) 2020, a share job of the fourth workspace on Neural Creation and Translate (WNGT), for the English-to-Portugal translation job. In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences).  Tuna halatar da misãlai masu tarjiwa na Ingiriya-zuwa-Portugueski (MT) waɗanda aka gina su shirya wa masu motsi masu motsi, misali, data da cewa, misali MT da za'a ƙiƙira fassarori masu motsi da wasu fassarori, kuma masu haɗi tarjifani masu yawa na-mafi kyau. Kayan jarrabõyinmu, za'a nũna cewa, a ƙara technical da aka taɓa wa zaman zuwa bakin, yana fitar da wani mai kyau a cikin aikin fassarar Ingiriya-zuwa-Portugueski.", 'bo': "This paper describes the ADAPT Centre's submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task. མཉམ་སྤྱོད་པའི་བྱ་འགུལ་འདིའི་ནང་དུ་ཞུགས་བྱས་པ་ཚོར་རྣམས་མཐོ་རིམ་པ་སྦྲེལ་མཐུད་བཏང་བ་ཡིན་པའི་སྐད་འགའ We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple n-best translations. ང་ཚོའི་ལས་འཚོལ་བྱ་ཚིག་དག་གི་རྨང་གཞིའི་ཐོག་ལས་སྔོན་འཕགས་རིས་ཐབས་ལམ་ལ་ཁ་སྣོན་མི་འདྲ་བ་དེ་གིས་དབྱིན་ཡུལ་ལས་"}
{'en': 'Efficient and High-Quality  Neural Machine Translation  with OpenNMT O pen NMT', 'ar': 'ترجمة آلية عصبية عالية الجودة مع OpenNMT', 'pt': 'Tradução automática neural eficiente e de alta qualidade com OpenNMT', 'es': 'Traducción automática neuronal eficiente y de alta calidad con OpenNMT', 'fr': 'Traduction automatique neuronale efficace et de haute qualité avec OpenNMT', 'ja': 'OpenNMTによる効率的で高品質なニューラル・マシン・トランスレーション', 'zh': '用 OpenNMT 成高效、高质量之神经机器翻译', 'hi': 'OpenNMT के साथ कुशल और उच्च गुणवत्ता तंत्रिका मशीन अनुवाद', 'ru': 'Эффективный и высококачественный нейронный машинный перевод с OpenNMT', 'ga': 'Aistriúchán Meaisín Néarach Éifeachtúil agus Ardcháilíochta le OpenNMT', 'hu': 'Hatékony és kiváló minőségű neurális gépi fordítás OpenNMT segítségével', 'el': 'Αποτελεσματική και υψηλής ποιότητας νευρωνική μηχανική μετάφραση με OpenNMT', 'ka': 'Name', 'it': 'Traduzione automatica neurale efficiente e di alta qualità con OpenNMT', 'kk': 'OpenNMT- мен жұмыс және жоғары сапалы нейралы машинаны аудару', 'lt': 'Veiksmingas ir aukštos kokybės neurologinis vertimas su OpenNMT', 'mt': 'Efficient and High-Quality Neural Machine Translation with OpenNMT', 'ms': 'Efficient and High-Quality Neural Machine Translation with OpenNMT', 'ml': 'Efficient and High-Quality Neural Machine Translation with OpenNMT', 'pl': 'Wydajne i wysokiej jakości tłumaczenie maszynowe neuronowe dzięki OpenNMT', 'ro': 'Traducere automată neurală eficientă și de înaltă calitate cu OpenNMT', 'no': 'Effektiv og høg kvalitet Neuralmaskinsomsetjing med OpenNMT', 'si': 'OpenNMT එක්ක ප්\u200dරයෝජනය සහ උත්සත් ප්\u200dරතිශේෂතාවක් න්\u200dයූරල් මැෂින් පරිවර්තනය', 'so': 'Efficient and High-Quality Neural Machine Translation with OpenNMT', 'mn': 'OpenNMT-тай үр дүнтэй, өндөр чанартай мэдрэлийн машин хөрөнгө', 'sv': 'Effektiv och högkvalitativ neural maskinöversättning med OpenNMT', 'ta': 'Name', 'sr': 'Efikasna i visokokvalitetna Neuralna mašina prevoda sa OpenNMT-om', 'ur': 'OpenNMT کے ساتھ فعال اور عالی کیفیت نیورال ماشین ترجمہ', 'mk': 'Name', 'vi': 'Dịch sang hình thần kinh hiệu quả và chất lượng cao với OpenNMT', 'uz': 'Name', 'bg': 'Ефективен и висококачествен неврален машинен превод с OpenNMT', 'hr': 'Efikasna i visokokvalitetna neurološka prevoda stroja s OpenNMT-om', 'nl': 'Efficiënte en hoogwaardige neuronale machinevertaling met OpenNMT', 'ko': 'OpenNMT 기반의 고효율 고품질 신경 기계 번역', 'da': 'Effektiv og højkvalitets neural maskinoversættelse med OpenNMT', 'sw': 'Tafsiri yenye ufanisi na ubora wa Mashine ya Neural kwa OpenNMT', 'tr': 'OpenNMT bilen ýeterlik we ýokary kaliwatly Näral Maşynyň terjimesi', 'de': 'Effiziente und hochwertige neuronale maschinelle Übersetzung mit OpenNMT', 'id': 'Efficient and High-Quality Neural Machine Translation with OpenNMT', 'fa': 'ترجمه ماشین عصبی با OpenNMT فعالیت و کیفیت بالا', 'am': 'ትርጉም', 'sq': 'Përkthimi i efektshëm dhe i kualitetit të lartë i makinës nervore me OpenNMT', 'af': 'Effektiewe en Hoë- Kwaliteit Neurale Masjien Vertaling met OpenNMT', 'hy': 'Ավելի արդյունավետ և բարձր որակի նյարդային մեքենայի թարգմանություն OpenNMT-ով', 'bn': 'OpenNMT এর সাথে কার্যকর এবং উচ্চমান নিউরেল মেশিন অনুবাদ', 'az': 'OpenNMT ilə effektiv və yüksək kaliteli nöral maşın çeviricisi', 'ca': "Una traducció eficient i d'alta qualitat de màquina neuronal amb OpenNMT", 'cs': 'Efektivní a vysoce kvalitní neuronový strojový překlad s OpenNMT', 'et': 'Tõhus ja kvaliteetne neuraalne masintõlge OpenNMT-ga', 'fi': 'Tehokas ja laadukas neurokonekäännös OpenNMT:llä', 'bs': 'Efikasna i visokokvalitetna Neuralna mašina prevoda sa OpenNMT-om', 'sk': 'Učinkovit in visokokakovosten nevralni strojni prevod z OpenNMT', 'ha': 'Translate game da Open NMT', 'jv': 'Suasai lan Kapasai sawar-kalitas Neral Manual Terjamahan karo Open NMT', 'he': 'Efficient and High-Quality Neural Machine Translation with OpenNMT', 'bo': 'OpenNMT དང་སྤྱིར་བཏང་ནུས་དང་མཐོ་རིམ་པོ Neural Machine་ལུས་ཚོགས་པ'}
{'en': 'This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional  optimizations  and  parallelization techniques , we create small, efficient, and high-quality neural machine translation models.', 'fr': "Cet article décrit les soumissions OpenNMT pour la tâche partagée d'efficacité WNGT 2020. Nous explorons la formation et l'accélération de modèles Transformer de différentes tailles qui sont formés dans une configuration enseignant-étudiant. Nous présentons également un moteur d'inférence C++ personnalisé et optimisé qui permet un décodage rapide du processeur et du processeur graphique avec peu de dépendances. En combinant des optimisations supplémentaires et des techniques de parallélisation, nous créons des modèles de traduction automatique neuronale petits, efficaces et de haute qualité.", 'ar': 'تصف هذه الورقة عمليات تقديم OpenNMT إلى مهمة كفاءة WNGT 2020 المشتركة. نستكشف تدريب وتسريع نماذج Transformer بأحجام مختلفة يتم تدريبها في إعداد المعلم والطالب. نقدم أيضًا محركًا مخصصًا ومحسّنًا للاستدلال C ++ يتيح فك تشفير وحدة المعالجة المركزية ووحدة معالجة الرسومات بسرعة مع القليل من التبعيات. من خلال الجمع بين التحسينات الإضافية وتقنيات الموازاة ، نقوم بإنشاء نماذج ترجمة آلية عصبية صغيرة وفعالة وعالية الجودة.', 'es': 'Este documento describe las presentaciones de OpenNMT a la tarea compartida de eficiencia de WNGT 2020. Exploramos el entrenamiento y la aceleración de modelos Transformer con varios tamaños que se entrenan en una configuración de profesor-alumno. También presentamos un motor de inferencia de C++ personalizado y optimizado que permite una decodificación rápida de CPU y GPU con pocas dependencias. Al combinar optimizaciones adicionales y técnicas de paralelización, creamos modelos de traducción automática neuronal pequeños, eficientes y de alta calidad.', 'pt': 'Este artigo descreve os envios do OpenNMT para a tarefa compartilhada de eficiência do WNGT 2020. Exploramos o treinamento e a aceleração de modelos Transformer com vários tamanhos que são treinados em uma configuração de professor-aluno. Também apresentamos um mecanismo de inferência C++ personalizado e otimizado que permite decodificação rápida de CPU e GPU com poucas dependências. Ao combinar otimizações adicionais e técnicas de paralelização, criamos modelos de tradução automática neural pequenos, eficientes e de alta qualidade.', 'ja': 'この論文では、WNGT 2020効率共有タスクへのOpenNMTサブミッションについて説明します。私たちは、教師と学生のセットアップでトレーニングされたさまざまなサイズのトランスフォーマーモデルのトレーニングとアクセラレーションを探ります。また、依存関係の少ない高速CPUおよびGPUデコードを可能にするカスタムおよび最適化されたC ++推論エンジンも提供しています。追加の最適化と並列化技術を組み合わせることで、小さく効率的で高品質なニューラル機械翻訳モデルを作成します。', 'zh': '本文述OpenNMTWNGT 2020效率共其事。 寻师生所设尺寸之变形金刚,以速其形。 给一自定义、优化C++推理引擎,宜引擎扶速 CPU GPU 解码,殆无所赖。 合优化并化,创小巧,高效高质量神经机器翻译形。', 'hi': 'यह पेपर WNGT 2020 दक्षता साझा कार्य के लिए OpenNMT सबमिशन का वर्णन करता है। हम विभिन्न आकारों के साथ ट्रांसफॉर्मर मॉडल के प्रशिक्षण और त्वरण का पता लगाते हैं जो शिक्षक-छात्र सेटअप में प्रशिक्षित होते हैं। हम एक कस्टम और अनुकूलित सी ++ अनुमान इंजन भी प्रस्तुत करते हैं जो कुछ निर्भरताओं के साथ तेजी से सीपीयू और जीपीयू डिकोडिंग को सक्षम बनाता है। अतिरिक्त अनुकूलन और समानांतरीकरण तकनीकों के संयोजन से, हम छोटे, कुशल और उच्च गुणवत्ता वाले तंत्रिका मशीन अनुवाद मॉडल बनाते हैं।', 'ru': 'В этом документе описываются представления OpenNMT для совместной задачи эффективности WNGT 2020. Мы изучаем обучение и ускорение моделей трансформаторов с различными размерами, которые обучены в конфигурации преподаватель-студент. Мы также представляем пользовательский и оптимизированный механизм вывода C++, который позволяет быстро декодировать процессор и графический процессор с небольшим количеством зависимостей. Объединяя дополнительные оптимизации и методы параллелизации, мы создаем небольшие, эффективные и высококачественные модели нейронного машинного перевода.', 'ga': 'Déanann an páipéar seo cur síos ar na haighneachtaí OpenNMT do thasc roinnte éifeachtúlachta WNGT 2020. Déanaimid iniúchadh ar oiliúint agus ar luasghéarú samhlacha Trasfhoirmeora de mhéideanna éagsúla atá oilte i socrú múinteoir-mac léinn. Cuirimid i láthair freisin inneall tátail C++ saincheaptha agus optamaithe a chumasaíonn díchódú tapa LAP agus GPU gan mórán spleáchais. Trí leas iomlán a bhaint breise agus teicnící comhthreomhaireachta a chur le chéile, cruthaímid samhlacha néaraistriúcháin meaisín néaracha beaga, éifeachtacha agus ardcháilíochta.', 'ka': 'ეს დოკუნტი აღწერს OpenNMT-ის გადასტანება WNGT 2020 ეფექტიურობის გაყოფილი რაოდენობაში. ჩვენ განვიკეთებთ ტრანფორმენტერის მოდელების განსწავლებას და დაწყვეტილებას, რომლებიც განსწავლება სტრანფორმენტერის მოდელების განსწავლებაში. ჩვენ ასევე დავიწყებთ C++ ინფრენციის მოწყობილობა, რომელიც ძალიან სიჩქარე CPU და GPU-ს გამოყენებას, რამდენიმე დასახლოებით. დამატებული ოპტიმიზაციები და პარალელიზაციის ტექნოლოგიების შეერთებით, ჩვენ შევქმნით პატარა, ეფექტიური და მაღალი კანოლიტური ნეირალური მანქანის გადაწყ', 'hu': 'Ez a tanulmány bemutatja az OpenNMT beadványait a WNGT 2020 hatékonysági megosztott feladathoz. Feltárjuk a különböző méretű Transformer modellek képzését és gyorsítását, amelyeket tanár-diák beállításban képzettek. Bemutatunk egy egyéni és optimalizált C++ következtető motort is, amely lehetővé teszi a gyors CPU és GPU dekódolást kevés függőséggel. További optimalizálások és párhuzamos technikák kombinációjával kis, hatékony és kiváló minőségű neurális gépi fordítási modelleket hozunk létre.', 'el': 'Η παρούσα εργασία περιγράφει τις υποβολές του OpenNMT στο κοινό έργο αποδοτικότητας WNGT 2020. Ερευνούμε την εκπαίδευση και την επιτάχυνση των μοντέλων με διάφορα μεγέθη που εκπαιδεύονται σε μια εγκατάσταση δασκάλου-μαθητή. Παρουσιάζουμε επίσης μια προσαρμοσμένη και βελτιστοποιημένη μηχανή συμπερασμάτων που επιτρέπει γρήγορη αποκωδικοποίηση CPU και GPU με λίγες εξαρτήσεις. Συνδυάζοντας πρόσθετες βελτιστοποιήσεις και τεχνικές παραλληλισμού, δημιουργούμε μικρά, αποδοτικά και υψηλής ποιότητας μοντέλα νευρολογικής μηχανικής μετάφρασης.', 'it': "Questo articolo descrive le presentazioni OpenNMT al compito condiviso di efficienza WNGT 2020. Esploriamo la formazione e l'accelerazione di modelli Transformer di varie dimensioni che sono formati in un setup insegnante-studente. Presentiamo anche un motore di inferenza C++ personalizzato e ottimizzato che consente una decodifica veloce di CPU e GPU con poche dipendenze. Combinando ulteriori ottimizzazioni e tecniche di parallelizzazione, creiamo modelli di traduzione automatica neurale piccoli, efficienti e di alta qualità.", 'kk': 'Бұл қағаз OpenNMT жіберілген тапсырманы WNGT 2020 жылдамдығына ортақтастыратын тапсырманы таңдайды. Біз мұғалім- студенттердің баптауларында оқылған түрлі өлшемдерді түрлі түрлі түрлі түрлі түрлердің оқытуын және жылдамдығын зерттейміз. Сонымен қатар, C++ бағыттау тетігін таңдаймыз. Бұл процессорды және GPU бағыттылығымен жылдам кодтамасын мүмкіндік береді. Қосымша оптимизацияларды және параллелизациялау технологияларды біріктіріп, кішкентай, эффективні және жоғары сапатты невралдық компьютердің аудару моделдерін құрамыз.', 'lt': 'Šiame dokumente aprašomos OpenNMT pateiktos bendros užduoties „WNGT 2020“ efektyvumas. Mes tiriame įvairių dydžių Transformer modelių mokymą ir paspartinimą, kurie mokomi mokytojų ir student ų struktūroje. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.', 'ms': 'This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task.  We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup.  We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.', 'ml': 'ഈ പത്രത്തില്\u200d WNGT 2020 പ്രവര്\u200dത്തനത്തില്\u200d പങ്കുചേര്\u200dത്ത പ്രവര്\u200dത്തനത്തിനുള്ള ഓപ്പണ്\u200dനെഎംടി കീഴ്പെടുത്തി ട്രാന്\u200dസ്ഫോര്\u200d മോഡലുകളുടെ പരിശീലനവും വേഗതയും പരിശീലിക്കുന്നു. വ്യത്യസ്തമായ വലിപ്പം പഠിപ്പിക്കപ്പെട്ട ഒരു ട സിപിയുവിന്റെയും ജിപിയുവിന്റെയും ആശ്രയിക്കുന്നതിന്റെയും പ്രാപ്തികമായി സിപിയുവിന്റെയും ജിപിയുവിന്റെയും കോഡിങ കൂടുതല്\u200d ഉപാധീകരണങ്ങളും പാരലേഷന്\u200d സാങ്കേതികവിദ്യകളും കൂട്ടിചേര്\u200dക്കുന്നതിനാല്\u200d, ചെറിയ, സാധ്യതയും ഉയര്\u200dന്ന പ്രധാനപൂര', 'mn': 'Энэ цаас WNGT 2020 оны үр дүнтэй ажиллагаанд OpenNMT-ын хэвлэлийг тайлбарладаг. Бид багш оюутнуудын сургуульд суралцагдсан олон хэмжээтэй Трансфер загварын сургалт болон хурдан судалж байна. Мөн бид хурдан CPU болон GPU-г хэдэн хамааралтай хурдан шилжүүлэх боломжтой C++ инференцийн хөтөлбөрийг илтгэдэг. Эсвэл нэмэлт санаачлалын болон параллелизацийн технологийг нэгтгэхэд бид жижиг, үр дүнтэй, өндөр чанартай мэдрэлийн мэдрэлийн дамжуулагч загварыг бий болгодог.', 'no': 'Denne papiret beskriver OpenNMT-tillegga til delt oppgåve i WNGT 2020. Vi utforskar opplæring og akselerasjon av transformeringsmodeller med forskjellige storleik som er trent i oppsettet av lærarar-studenter. Vi presenterer også ein sjølvvald og optimalisert C++ infeksjonsmotor som slår på rask CPU og GPU-koding med få avhengighet. Ved å kombinere fleire optimaliseringar og paralleliseringssteknikk, lager vi små, effektiv og høg kvalitetsverktøymodeller.', 'ro': 'Această lucrare descrie propunerile OpenNMT la sarcina comună de eficiență WNGT 2020. Explorăm pregătirea și accelerarea modelelor Transformer cu diferite dimensiuni, care sunt instruite într-o configurație profesor-elev. De asemenea, prezentăm un motor de inferență C++ personalizat și optimizat care permite decodarea rapidă a procesorului și GPU cu câteva dependențe. Combinând optimizări suplimentare și tehnici de paralelizare, creăm modele mici, eficiente și de înaltă calitate de traducere automată neurală.', 'pl': 'Niniejszy artykuł opisuje zgłoszenia OpenNMT do wspólnego zadania efektywności WNGT 2020. Badamy szkolenia i przyspieszanie modeli Transformera o różnych rozmiarach, które są szkolone w konfiguracji nauczyciela-ucznia. Prezentujemy również niestandardowy i zoptymalizowany silnik inferencji C++ umożliwiający szybkie dekodowanie procesorów i GPU przy niewielkich zależnościach. Łącząc dodatkowe optymalizacje i techniki paralelizacji, tworzymy małe, wydajne i wysokiej jakości modele tłumaczenia maszynowego neuronowego.', 'sr': 'Ovaj papir opisuje podatke OpenNMT za zajednički zadatak učinkovitosti WNGT 2020. Istražujemo obuku i ubrzanje modela transformera sa različitim veličinama koje su obučene u nastavnici i studentima. Također predstavljamo obični i optimizirani C++ inferens motor koji omogućava brzu dekodiranje CPU i GPU sa nekoliko ovisnosti. Kombinacijom dodatnih optimizacija i parallelizacijskih tehnika, stvaramo male, efikasne i visoke kvalitetne modele prevoda neuralnih mašina.', 'si': 'මේ පත්තුව OpenNMT පිළිබඳය WNGT 2020ක් විශේෂතාවක් විතරයි. අපි පරීක්ෂණය සහ ප්\u200dරවේශනය කරනවා විවිධ ප්\u200dරමාණයක් තියෙන විවිධ ප්\u200dරමාණයක් තියෙන්නේ ගුරුත්රිය-විද්\u200d අපි ප්\u200dරමාණය සහ හුද්ධතාවක් තියෙන්නේ C++ ප්\u200dරමාණයක් ඉංජින් එක්ක, ඉක්මනින් CPU සහ GPU විශේෂතාවක් සඳහා ප අපි පොඩි විශේෂතාවක් සහ සමාන්\u200dය විශේෂතාවක් සම්බන්ධ කරනවා, අපි පොඩි විශේෂතාවක් සහ උත්සත් විශේෂතාවක් න', 'so': "Kanu warqaddan waxaa ku qoran warqada aad u soo dhiibtay WNGT 2020 waxyaabaha la qaybiyey. Waxaynu baaraynaa waxbarashada iyo isticmaalka sameynta tusaalaha turjumista ee kala duduwan, kuwaas oo lagu baran karo tababarka waxbarashada. Sidoo kale waxaynu soo bandhignaa mashiinka caadi ah oo loo bedelay C++ infer, kaas oo awoodda u dhigi kara saxda CPU iyo GPU inay ku xiran tahay wax yar oo ku filan. Si aan kuugu soo ururiyo qalabka faa'iidada dheeraadka ah iyo qalabka isbedelka, waxaynu sameynaa noocyo yar, faa’iido leh iyo sameynta turjumidda maskaxda neurada ah oo sare.", 'sv': 'Denna uppsats beskriver OpenNMT:s bidrag till den delade uppgiften WNGT 2020 effektivitet. Vi utforskar utbildning och acceleration av Transformer modeller i olika storlekar som utbildas i en lärare-elev setup. Vi presenterar också en anpassad och optimerad C++ inferensmotor som möjliggör snabb CPU- och GPU-avkodning med få beroenden. Genom att kombinera ytterligare optimeringar och parallelliseringstekniker skapar vi små, effektiva och högkvalitativa neurala maskinöversättningsmodeller.', 'ta': 'This paper describes the OpenNMT submissions to the WNGT 2020 performance shared task. மாற்றும் மாதிரி மாதிரிகளின் பயிற்சி மற்றும் வேக்கத்தை நாங்கள் தேடுகிறோம் மற்றும் ஒரு ஆசிரியர் மாணவர் அமைப சிபியூ மற்றும் ஜிபியூ மற்றும் சில சார்புகளுடன் குறியீட்டை செயல்படுத்தும் வேகமான குறியீடு கூடுதல் தேர்வுகள் மற்றும் ஒப்பீட்டு தொழில்நுட்பத்தை இணைத்து, சிறிய, துவாக்கி, மற்றும் உயர்தரமான புதிய மொழிமாற்று', 'mt': 'Dan id-dokument jiddeskrivi s-sottomissjonijiet tal-OpenNMT għall-kompitu kondiviż tal-effiċjenza tad-WNGT 2020. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup.  We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.', 'mk': 'Овој документ ги опишува поднесувањата на OpenNMT на заедничката задача за ефикасност на WNGT 2020. Истражуваме обука и забрзување на трансформските модели со различни големини кои се обучени во поставување учител-студент. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  Со комбинација на дополнителни оптимизации и паралелизациски техники, создаваме мали, ефикасни и висококвалитетни модели за превод на невропските машини.', 'ur': 'This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. ہم تدریس اور تدریس کرنے والی موڈل کی تدریس کرتے ہیں جو مختلف اندازے کے ساتھ تدریس کئے جاتے ہیں جو ایک استاد-طالب استعمال میں تدریس کئے جاتے ہیں. ہم نے ایک طریقہ اور optimized C++ inference انجینٹ کو بھی پیش کیا ہے جو بہت کم اعتباری کے ساتھ سریع CPU اور GPU ڈیکوڈ کرتا ہے۔ اور اضافہ optimizations اور parallelization techniques کو جمع کر، ہم چھوٹے، فعالیت اور بلند کیفیت نیورال ماشین ترجمہ موڈل بناتے ہیں.', 'vi': 'Tờ giấy này mô tả công việc chia sẻ hiệu quả OpenNMT cho WNGT 2020. Chúng tôi tìm hiểu sự huấn luyện và gia tốc của các mô hình transformer với các kích thước khác nhau được rèn luyện trong một sự thiết lập giáo viên-sinh viên. Chúng tôi cũng giới thiệu một động cơ nhận ra C+có khả năng giải mã CPU và GPU nhanh cùng với vài phụ thuộc. Bằng cách kết hợp cải thiện thêm và các kỹ thuật đồng song, chúng tôi tạo ra các mô hình dịch thiết bị thần kinh chất lượng cao.', 'uz': "Name Va biz o'qituvchi o'quvchi o'quvchi o'quvchi o'quvchida o'rganilgan ko'plab ko'plab o'rganishni o'rganamiz. Biz ham foydalanuvchi C++ kamaytirish mashina hozir qilamiz, bu CPU va GPU bilan bir necha ishlatadigan narsalar bilan foydalanadi. Qoʻshimcha optimiz va paralleliz teknikalarni birlashtirish orqali biz kichkina, effektiv va yuqori darajadagi neural tarjima modellarini yaramiz.", 'bg': 'Настоящата статия описва предложенията на OpenNMT към споделената задача за ефективност на WNGT 2020. Ние изследваме обучението и ускоряването на трансформаторни модели с различни размери, които се обучават в настройка учител-студент. Представяме и персонализиран и оптимизиран двигател за заключения, който позволява бързо декодиране на процесора и графичния процесор с малко зависимости. Чрез комбиниране на допълнителни оптимизации и паралелизационни техники създаваме малки, ефективни и висококачествени модели на невронен машинен превод.', 'da': "Dette dokument beskriver OpenNMT's indlæg til WNGT 2020 effektivitet delt opgave. Vi undersøger træning og acceleration af Transformer modeller med forskellige størrelser, der er trænet i en lærer-studerende setup. Vi præsenterer også en brugerdefineret og optimeret C ++ inference motor, der muliggør hurtig CPU og GPU afkodning med få afhængigheder. Ved at kombinere yderligere optimeringer og paralleliseringsteknikker skaber vi små, effektive og højkvalitets neurale maskinoversættelsesmodeller.", 'hr': 'Ovaj papir opisuje podatke OpenNMT-a zajedničkom zadatku učinkovitosti WNGT 2020. Istražujemo obuku i ubrzanje modela transformera sa različitim veličinama koje su obučene u nastavnom nastavniku i učeniku. Također predstavljamo obični i optimizirani C++ inferencijski motor koji omogućava brzu dekodiranje CPU i GPU sa nekoliko ovisnosti. Kombinacijom dodatnih optimizacija i parallelizacijskih tehnika stvaramo male, učinkovite i visoke kvalitetne modele prevoda neuralnih strojeva.', 'de': 'Dieses Papier beschreibt die OpenNMT-Einreichungen zur gemeinsamen Aufgabe WNGT 2020 Effizienz. Wir erforschen Training und Beschleunigung von Transformer-Modellen mit verschiedenen Größen, die in einem Lehrer-Schüler-Setup trainiert werden. Wir präsentieren auch eine benutzerdefinierte und optimierte C++ Inference Engine, die eine schnelle CPU- und GPU-Dekodierung mit wenigen Abhängigkeiten ermöglicht. Durch die Kombination zusätzlicher Optimierungen und Parallelisierungstechniken erstellen wir kleine, effiziente und qualitativ hochwertige neuronale maschinelle Übersetzungsmodelle.', 'ko': '본고는 OpenNMT가 WNGT 2020 효율 공유 임무에 제출한 상황을 묘사한다.우리는 교사와 학생의 교육에서 교육된 각종 사이즈의 변압기 모형의 교육과 가속화를 탐색했다.우리는 또한 맞춤형 C++ 추리 엔진을 제공했는데, 이 엔진은 빠른 CPU와 GPU 디코딩을 지원하고 의존성이 매우 적다.추가 최적화와 병행화 기술을 결합시켜 우리는 소형, 효율, 고품질의 신경기계 번역 모델을 만들었다.', 'sw': 'Gazeti hili linaelezea ujumbe wa OpenNMT kwa ufanisi wa WNGT 2020 uliosambazwa na kazi hiyo. Tunafahamu mafunzo na kuongezeka kwa mifano ya zamani yenye ukubwa wa a in a mbalimbali ambazo hufundishwa katika taasisi ya wanafunzi. Pia tunaweka mbinu yenye matumaini na ubora wa CPU na GPU inayowezesha kupanuka kwa kasi kwa matumaini machache. Kwa kuunganisha mbinu za matumaini na usambazaji, tunatengeneza mifano madogo, yenye ufanisi na mifano ya tafsiri ya mashine yenye ubora.', 'id': 'This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task.  We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup.  We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.', 'fa': 'این کاغذ تحویل OpenNMT را به کار مشترک موثیت WNGT 2020 توصیف می\u200cکند. ما آموزش و سرعت مدل تغییر دهنده را با اندازه\u200cهای مختلف تحقیق می\u200cکنیم که در یک ساختمان دانش آموزان آموزش داده می\u200cشوند. ما همچنین یک موتور آلودگی سی++ سفارشی و optimized C++ را پیشنهاد می\u200cکنیم که به سرعت CPU و GPU دستگاه\u200cبندی با کمی بستگی توانایی می\u200cدهد. با ترکیب فناوری\u200cهای اضافه\u200cای و تکنیک\u200cهای parallelization، ما مدل\u200cهای ترکیب ماشین\u200cهای عصبی کوچک، موثرت و کیفیت بالا ایجاد می\u200cکنیم.', 'tr': 'Bu kagyz OpenNMT sahypalaryny WNGT 2020-nji buýruklyk bilen paýlaşýar Mugallym-okuwçylar guruldygynda okuw edilen çykyş bilen Transformer nusgalarynyň okuwçylygyny we paýlaşmagyny gözleýäris. Biz hem şahsy we optimized C++ hasaplanjak motoryny görkezip, bu da hızlı CPU we GPU ködlemelerini azaltýar. Ekstra optimizasy we parallellaşdyrma teknikleri birleşerek kiçi, etkinlik we ýokary kaliteli näyral maşynyň terjime modellerini bejerýäris', 'af': "Hierdie papier beskryf die OpenNMT-onderskrywings aan die WNGT 2020-effektiviteit gedeelde taak. Ons onderwerp onderwerp en versnelling van Transformer-modelles met verskeie grootte wat onderwerp word in 'n onderwerp-studente instelling. Ons het ook 'n pasmaak en optimaliseerde C++ inferensie masjien voorgeskryf wat vinnige CPU en GPU-dekodering met paar afhanklikhede aktiveer. Deur addisionele optimalisasies en parallelisering teknike te kombinerer, skep ons klein, effektief en hoë-kwaliteit neurale masjien vertalingsmodelle.", 'sq': 'Ky dokument përshkruan paraqitjet e OpenNMT ndaj detyrës së përbashkët të efektshmërisë së WNGT 2020. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup.  We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.', 'am': 'ይህ ፕሮግራም የOpenNMT ጥያቄ በ2020 ክፍተት የተካፈለውን ስራ ለWNGT ጥያቄ ይናገራል፡፡ ተማሪ-ተማሪ ተማሪ ክፍል በሚያስተማሩበት ብዙዎች የተማሪዎችን ትምህርት ማድረግ እና ድርጅትን እንፈልጋለን፡፡ የCPU እና የGPU ድጋፍ በጥቂት ተደጋጋፊዎች የሚችል የCPU እና የGPU ድጋፍ የሚያስፈልገውን የC++ ማስታወቂያውን እናቀርባለን፡፡ በተጨማሪው ምርጫዎች እና ተሳያፊዎችን በመቀላቀል፣ ታናሽ፣ አዋቂ እና ከፍተኛ የናውሬው መሣሪያን ትርጉም ሞዴላዎችን እናፈጥራለን፡፡', 'hy': 'Այս հոդվածը նկարագրում է OpenNMT-ի ներկայացումները ՀՆԳT 2020-ի արդյունավետության ընդհանուր խնդիրը: We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup.  Մենք նաև ներկայացնում ենք հնարավոր և օպտիմացված C++ եզրակացության շարժիչ, որը հնարավորություն է տալիս արագ պրոցեսոր և GPU-ի կոդավորումը քիչ կախվածություններով: Համակարգելով ավելին օպտիմացման և զուգահեռացման տեխնիկաներ, մենք ստեղծում ենք փոքր, արդյունավետ և բարձր որակի նյարդային մեքենայի թարգմանման մոդելներ:', 'nl': 'Dit document beschrijft de OpenNMT inzendingen aan de WNGT 2020 efficiency shared task. We onderzoeken training en versnelling van Transformer modellen met verschillende maten die zijn getraind in een leraar-student setup. We presenteren ook een aangepaste en geoptimaliseerde C++ inference engine die snelle CPU en GPU decodering mogelijk maakt met weinig afhankelijkheden. Door aanvullende optimalisaties en parallellisatietechnieken te combineren, creëren we kleine, efficiënte en hoogwaardige neurale machinevertaalmodellen.', 'bn': 'এই পত্রিকা ওপেনএমটি উইনজিটি ২০২০ এর কার্যক্রম শেয়ার করার কাজের প্রতি প্রদত্ত প্রদান করেছে। আমরা ট্রান্সফ্রান্সফার মডেলের প্রশিক্ষণ এবং ত্বরান্বিত করি বিভিন্ন আকারের মাধ্যমে যা শিক্ষক-ছাত্রী সংস্থা আমরা একটি স্বনির্বাচিত এবং অপ্রাসঙ্গিক সি+ ইঞ্জিন উপস্থাপন করি যা দ্রুত সিপিউ এবং জিপিউ-এর নির্ভর করে কোডিং সক্রিয় করে। অতিরিক্ত অপারেশন এবং প্যারালেজশন কৌশল সম্মিলনের মাধ্যমে আমরা ছোট, দক্ষতা এবং উচ্চমান নিউরেল মেশিন অনুবাদ মডেল তৈরি করি।', 'bs': 'Ovaj papir opisuje podatke OpenNMT-a zajedničkom zadatku učinkovitosti WNGT 2020. Istražujemo obuku i ubrzanje modela transformera sa različitim veličinama koje su obučene u nastavnom nastavniku. Također predstavljamo obični i optimizirani C++ inferencijski motor koji omogućava brzu dekodiranje CPU i GPU sa nekoliko ovisnosti. Kombinujući dodatne optimizacije i parallelizacijske tehnike, stvaramo male, efikasne i visoke kvalitetne modele prevoda neuralnih strojeva.', 'cs': 'Tento článek popisuje příspěvek OpenNMT pro sdílený úkol efektivity WNGT 2020. Prozkoumáme trénink a zrychlení modelů Transformer různých velikostí, které jsou trénovány v nastavení učitele-student. Představujeme také vlastní a optimalizovaný C++ inferenční engine, který umožňuje rychlé dekódování procesorů a GPU s několika závislostmi. Kombinací dalších optimalizací a paralelizačních technik vytváříme malé, efektivní a vysoce kvalitní modely neuronového strojového překladu.', 'et': 'Käesolevas dokumendis kirjeldatakse OpenNMT esitusi WNGT 2020 tõhususe jagatud ülesande kohta. Uurime erineva suurusega Transformeri mudelite koolitamist ja kiirendamist, mida koolitatakse õpetaja-õpilase seadistuses. Esitleme ka kohandatud ja optimeeritud C++ järeldusmootorit, mis võimaldab kiiret protsessori ja GPU dekodeerimist vähese sõltuvusega. Kombineerides täiendavaid optimeerimisi ja paralleelsusmeetodeid, loome väikeseid, tõhusaid ja kvaliteetseid neuromasintõlke mudeleid.', 'fi': 'Tässä artikkelissa kuvataan OpenNMT:n ehdotuksia WNGT 2020:n tehokkuuteen jaettuun tehtävään. Tutkimme eri kokoisten Transformer-mallien koulutusta ja kiihdytystä, joita koulutetaan opettaja-oppilas-kokoonpanossa. Esittelemme myös räätälöidyn ja optimoidun C++-päättelymoottorin, joka mahdollistaa nopean CPU- ja GPU-dekoodauksen muutamalla riippuvuudella. Yhdistämällä lisäoptimointeja ja rinnastustekniikoita luomme pieniä, tehokkaita ja laadukkaita neurokonekäännösmalleja.', 'az': 'Bu kağıt, WNGT 2020 faydallığı paylaşdığı işlərə OpenNMT təbliğlərini təsdiqləyir. Biz müəllimlər-öğrencilər qurulmasında təhsil edilən müxtəlif ölçülərlə Transformer modellerinin təhsil və hızlandırmasını keşfetirik. Biz həmçinin C++ infeksyon motorunu da müəyyən və optimizləndirdik ki, cürbəcür bağlılıqlarla hızlı CPU və GPU kodlamasını qabilleştirir. Əlavə optimizasiyaları və parallelizasyon tekniklərini birləşdirərək kiçik, etkili və yüksək kaliteli nöral maşına çevirmə modellerini yaradırıq.', 'ca': "Aquest paper descriu les presentacions d'OpenNMT a la tasca compartida d'eficiència WNGT 2020. Exploram l'entrenament i l'acceleració de models de Transformer amb diferents dimensions que s'entrenen en una configuració professor-estudiant. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies.  Combinant més optimitzacions i tècniques de parallelització, creem models de traducció neural de petita qualitat, eficients i d'alta qualitat.", 'jv': 'Perintah iki dadi ono nggawe ngeremusi Open NMT kanggo wong njaluk gerakan WNGT 2020 Awak dhéwé éntuk nglanggar aturan karo akeh perusahaan model Transformer sampeyan karo hal-hal sing nyimpen kuwi nggawe aturan guru mesthi. Awak dhéwé éntuk sistem perusahaan lan basa perusahaan C++-pakan sing bisa nguasakno ujaran Ngawe jenis-jenis sistem Optimisasi tambah karo teknik Paralelisasi', 'ha': "Wannan avir describes the Open NMT elements to the WNGT 2020 functionin shari. Tuna karatun mafarin da gaskatawa wa misãlai na Transformer da ko da huɗu masu yin wa'anar-macalli. Tuna gabatar da wata shirin da aka fi kyauta a kan CPU da GPU masu iya amfani da masu ƙaranci kaɗan. Ga ku haɗa wasu masu kwamfyuta da kuma masu daidaita, muna halitta misãlai masu ƙarami, mai amfani da kuma masu fassarar fassarori na masu nau'in neural.", 'sk': 'V tem članku so opisani prispevki OpenNMT k skupni nalogi učinkovitosti WNGT 2020. Raziskujemo usposabljanje in pospeševanje transformatorskih modelov različnih velikosti, ki so usposobljeni v nastavitvi učitelj-učenec. Predstavljamo tudi optimiziran motor za sklepanje C++ po meri, ki omogoča hitro dekodiranje CPU in GPU z malo odvisnosti. Z združevanjem dodatnih optimizacij in tehnik paralelizacije ustvarjamo majhne, učinkovite in visokokakovostne modele nevralnega strojnega prevajanja.', 'he': 'העיתון הזה מתאר את ההעברות של OpenNMT למשימה משותפת יעילות WNGT 2020. אנחנו חוקרים אימונים ומאיצים של מודלים טרנספורטרים עם גודלים שונים שאומנים בתכנית מורה-סטודנטים. אנחנו גם מציגים מנוע ההנחה C++ מתאים ומאופטימי שמאפשר פיתוח מהיר ולפיתוח GPU עם כמה תלויות. על ידי שילוב אופטימיזציות נוספות וטכניקות שיתוף, אנחנו יוצרים דוגמנים קטנים, יעילים ומכונות גבוהה של התרגום של מכונות עצביות.', 'bo': 'ཤོག་བྱང་འདིས་OpenNMT་གི་གནད་སྡུད་དུ་འཇུག་སའི་ལས་འགན་སྐྱོང་བྱེད་ཀྱི་བྱ་རིམ་ལ་སྔར་སྒྲིག ང་ཚོས་སློབ་གྲྭར་དང་མགྱོགས་རྩོལ་མོལ་ཁག་ཅིག་གི་ཚད་གཞི་སྒྲིག་འཛུགས་ཀྱི་ནང་གི་གྲངས་ཚད་མ་འདྲ་བ་དང་ལྟ་སྟངས་གནང We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient and high-quality neural machine translation models.'}
{'en': 'Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task E dinburgh’s Submissions to the 2020 Machine Translation Efficiency Task', 'ar': 'تقديمات إدنبرة لمهمة كفاءة الترجمة الآلية لعام 2020', 'fr': "Soumissions d'Edimbourg pour la tâche d'efficacité de la traduction automatique 2020", 'pt': 'Submissões de Edimburgo para a Tarefa de Eficiência da Tradução Automática de 2020', 'es': 'Presentaciones de Edimburgo para la tarea de eficiencia de la traducción automática de 2020', 'ja': '2020年の機械翻訳効率化タスクへのエディンバラの提出', 'zh': '爱丁堡提交 2020 年机器翻译效率任', 'hi': '2020 मशीन अनुवाद दक्षता कार्य के लिए एडिनबर्ग की प्रस्तुतियाँ', 'ru': 'Материалы Эдинбурга по задаче «Эффективность машинного перевода 2020»', 'ga': 'Aighneachtaí Dhún Éideann don Tasc Éifeachtúlachta um Aistriú Meaisín 2020', 'ka': 'ექინდონის მისამართები 2020-ის მაქინის გადაწყვეტილების ეფექტურება', 'el': 'Υποβολές του Εδιμβούργου στο έργο αποδοτικότητας μηχανικής μετάφρασης 2020', 'hu': 'Edinburgh beadványai a 2020-as gépi fordítás hatékonysági feladathoz', 'kk': 'Эдинбург 2020 жылы машинаны аудару мүмкіндігінің тапсырмасы', 'it': 'Contributi di Edimburgo al compito 2020 di efficienza della traduzione automatica', 'lt': 'Edinburgo pasiūlymai dėl 2020 m. mašinų vertimo efektyvumo užduoties', 'mk': 'Предлозите на Единбург на задачата за ефикасност на преведувањето на машините во 2020 година', 'ml': 'എഡിന്\u200dബര്\u200dഗിന്\u200dറെ അബ്രിമിഷന്\u200d 2020 മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നതിന്\u200dറെ പ്രവര്\u200dത്തനങ്ങള്\u200d', 'ms': "Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task", 'mn': 'Эдинбургийн 2020 оны Машин хөрөнгө оруулах чадварын даалгавар', 'mt': "Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task", 'no': "Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task", 'ro': 'Transmiterile de la Edinburgh la sarcina 2020 privind eficiența traducerii automate', 'pl': 'Zgłoszenia Edynburga do zadania związanego z efektywnością tłumaczenia maszynowego 2020', 'sr': 'Edinburgovi podaci na zadatak za prevod mašine 2020.', 'si': 'ඩිවන්බින්ග්රින්ග් සාමිෂ්ණයෝ 2020වාර්ථාන පරිවර්තනය සක්\u200dරියාත්මක කාර්ය', 'so': 'Edinburgh Submissions to the 2020 machine Translation Effective Task', 'ta': '2020 இயந்திரத்தின் மொழிபெயர்ப்பு விளைவுகள்', 'sv': 'Edinburghs bidrag till 2020-uppgiften för effektivitet i maskinöversättning', 'ur': '۲۰۰۲ ماشین ترجمہ فعالیت ٹاکس کے لئے ادینڈینبور کے سرماشین', 'uz': 'Name', 'vi': 'Sự đệ trình của Edinburgh vào Nhiệm vụ Độ khẩn máy 2020', 'bg': 'Докладите на Единбург за задачата за ефективност на машинния превод през 2020 г.', 'nl': 'Edinburghs inzendingen aan de 2020 Machine Translation Efficiency Task', 'da': 'Edinburghs indlæg til opgaven om effektiv maskinoversættelse i 2020', 'hr': 'Edinburgovi podaci na zadatak za praćenje strojeva 2020.', 'ko': '에든버러 2020년 기계번역 효율 임무 제출', 'de': 'Edinburghs Einreichungen zur 2020-Aufgabe zur Effizienz maschineller Übersetzung', 'fa': 'مسئله\u200cهای ادینبورگ برای تاثیر تغییرات ماشین ۲۰۰۲', 'sw': 'Mipango ya Edinburgh kwenye kazi ya Tafsiri ya Mashiniki 2020', 'af': 'Edinburgh se Submissions na die 2020 Masjien Vertaling Efficiency Task', 'sq': 'Paraqitjet e Edinburgut në detyrën 2020 për efektshmërinë e përkthimit të makinave', 'am': "Edinburgh's Submissions to the 2020 machine Translation Effective Task", 'hy': 'Էդինբուրգի ներկայացումները 2020 թվականի մեքենային թարգմանման արդյունավետության խնդիրը', 'az': "Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task", 'bn': '২০২০০ মেশিন অনুবাদের কাজে এডিনবার্গের সাবমিশন', 'id': "Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task", 'tr': "Edinburg's Submissions to the 2020 Machine Translation Efficiency Task", 'bs': 'Edinburgovi podaci na zadatak za praćenje mašine 2020.', 'ca': "Les Submissions d'Edimburgue a la tasca d'eficiència en la traducció màquina del 2020", 'cs': 'Edinburghské příspěvky k úkolu efektivity strojového překladu 2020', 'et': 'Edinburghi ettepanekud 2020. aasta masintõlke tõhususe ülesandele', 'fi': 'Edinburghin ehdotukset vuoden 2020 konekäännöksen tehokkuustehtävään', 'jv': 'Sub-misi nang Edgar 2020 Majin Terjamahan Efeffisisi Job', 'ha': 'KCharselect unicode block name', 'sk': 'Edinburgovi prispevki k nalogi učinkovitosti strojnega prevajanja za leto 2020', 'bo': "Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task", 'he': 'הגישות של אדינבורג למשימה של יעילות התרגום של מכונות 2020'}
{'en': 'We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task :  single-core CPU ,  multi-core CPU , and  GPU . At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On  GPUs , we used 16-bit floating-point tensor cores. On  CPUs , we customized 8-bit quantization and  multiple processes  with affinity for the  multi-core setting . To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.', 'ar': 'شاركنا في جميع مسارات ورشة العمل الخاصة بالجيل العصبي والترجمة 2020 Efficiency Shared Task: وحدة المعالجة المركزية أحادية النواة ، ووحدة المعالجة المركزية متعددة النواة ، ووحدة معالجة الرسومات. على مستوى النموذج ، نستخدم تدريب المعلم والطالب مع مجموعة متنوعة من أحجام الطلاب ، وربطة عنق وأحيانًا طبقات ، ونستخدم وحدة أبسط بسيطة متكررة ، ونقدم تقليم الرأس. في وحدات معالجة الرسومات ، استخدمنا نوى موتر ذات فاصلة عائمة 16 بت. على وحدات المعالجة المركزية (CPU) ، قمنا بتخصيص 8 بت تكميم وعمليات متعددة مع تقارب لإعداد متعدد النواة. لتقليل حجم النموذج ، جربنا تكميمًا لسجل 4 بت ولكننا استخدمنا عوامات في وقت التشغيل. في المهمة المشتركة ، كانت معظم عمليات الإرسال لدينا مثالية باريتو فيما يتعلق بالمفاضلة بين الوقت والجودة.', 'fr': "Nous avons participé à toutes les pistes de l'atelier sur la tâche partagée d'efficacité de génération et de traduction neuronales 2020\xa0: processeur monocœur, processeur multicœur et processeur graphique. Au niveau du modèle, nous utilisons la formation enseignant-étudiant avec différentes tailles d'étudiants, des incrustations de cravate et parfois des couches, nous utilisons l'unité récurrente simple plus simple et nous introduisons l'élagage de la tête. Sur les GPU, nous avons utilisé des cœurs tenseurs en virgule flottante 16 bits. Sur les processeurs, nous avons personnalisé la quantification 8 bits et plusieurs processus avec une affinité pour le réglage multicœur. Pour réduire la taille du modèle, nous avons expérimenté la quantification logarithmique sur 4 bits, mais nous avons utilisé des nombres flottants lors de l'exécution. Dans la tâche partagée, la plupart de nos soumissions étaient optimales au sens de Pareto en termes de compromis entre le temps et la qualité.", 'es': 'Participamos en todos los temas del Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: CPU de un solo núcleo, CPU multinúcleo y GPU. A nivel de modelo, utilizamos la capacitación de maestros y estudiantes con una variedad de tamaños de estudiantes, incrustaciones de ataduras y, a veces, capas, utilizamos la Unidad Recurrente Simple Más Simple e introducimos la poda de cabezas. En las GPU, utilizamos núcleos tensoriales de punto flotante de 16 bits. En las CPU, personalizamos la cuantificación de 8 bits y los procesos múltiples con afinidad por la configuración multinúcleo. Para reducir el tamaño del modelo, experimentamos con la cuantificación logarítmica de 4 bits, pero utilizamos flotantes en tiempo de ejecución. En la tarea compartida, la mayoría de nuestras presentaciones fueron óptimas en términos de Pareto con respecto a la compensación entre tiempo y calidad.', 'pt': 'Participamos de todas as trilhas do Workshop sobre Geração Neural e Tarefa Compartilhada de Eficiência de Tradução 2020: CPU single-core, CPU multi-core e GPU. No nível do modelo, usamos o treinamento professor-aluno com uma variedade de tamanhos de alunos, incorporações de gravata e, às vezes, camadas, usamos a Unidade Recorrente Simples Mais Simples e introduzimos a poda de cabeça. Em GPUs, usamos núcleos tensores de ponto flutuante de 16 bits. Em CPUs, personalizamos a quantização de 8 bits e vários processos com afinidade para a configuração multi-core. Para reduzir o tamanho do modelo, experimentamos a quantização de log de 4 bits, mas usamos floats em tempo de execução. Na tarefa compartilhada, a maioria de nossas submissões foram ótimas de Pareto em relação ao compromisso entre tempo e qualidade.', 'ja': '私たちは、シングルコアCPU、マルチコアCPU、およびGPUという、ニューラルジェネレーションと翻訳に関するワークショップ2020効率共有タスクのすべてのトラックに参加しました。モデルレベルでは、さまざまな生徒のサイズの教師と生徒のトレーニング、タイの埋め込み、時にはレイヤー、シンプルなリカレントユニットを使用し、頭の刈り込みを導入します。GPUでは、16ビットの浮動小数点テンソルコアを使用しました。CPUでは、マルチコア設定に親和性のある8ビット量子化と複数のプロセスをカスタマイズしました。モデルサイズを小さくするために、4ビットのログ量子化を実験しましたが、実行時にフロートを使用します。共有タスクでは、パレートの提出物のほとんどは、時間と品質のトレードオフに関して最適でした。', 'zh': '2020年神经成转研讨会效率共事分会场单核CPU多核CPUGPU。 凡模形等级,用诸生规模师生培训,绑扎嵌之,或为层层,用更简循环单元,引入头剪。 于 GPU ,我用 16 位浮点张量心。 于 CPU 之上,自定义 8 位量化多进,与多核设有关联性。 为小大,试 4 位日志量化,浮数于行。 凡所共任,多为帕累托最,时质之权衡。', 'hi': 'हमने न्यूरल जनरेशन और अनुवाद 2020 दक्षता साझा कार्य पर कार्यशाला के सभी पटरियों में भाग लिया: एकल-कोर सीपीयू, मल्टी-कोर सीपीयू और जीपीयू। मॉडल स्तर पर, हम विभिन्न प्रकार के छात्र आकारों के साथ शिक्षक-छात्र प्रशिक्षण का उपयोग करते हैं, एम्बेडिंग और कभी-कभी परतों को बांधते हैं, सरल सरल आवर्तक इकाई का उपयोग करते हैं, और सिर की छंटाई का परिचय देते हैं। जीपीयू पर, हमने 16-बिट फ्लोटिंग-पॉइंट टेंसर कोर का उपयोग किया। सीपीयू पर, हमने मल्टी-कोर सेटिंग के लिए आत्मीयता के साथ 8-बिट परिमाणीकरण और कई प्रक्रियाओं को अनुकूलित किया। मॉडल आकार को कम करने के लिए, हमने 4-बिट लॉग परिमाणीकरण के साथ प्रयोग किया लेकिन रनटाइम पर फ्लोट का उपयोग करें। साझा कार्य में, हमारे अधिकांश सबमिशन समय और गुणवत्ता के बीच व्यापार-बंद के संबंध में पारेटो इष्टतम थे।', 'ru': 'Мы участвовали во всех направлениях Семинара по нейрогенерации и совместному решению задачи «Эффективность 2020»: одноядерный ЦП, многоядерный ЦП и графический процессор. На уровне модели мы используем обучение учителя-студента с различными размерами учащихся, вставками и иногда слоями, используем упрощенную простую рекуррентную единицу и вводим обрезку головы. На графических процессорах мы использовали 16-битные тензорные ядра с плавающей запятой. На процессорах мы настроили 8-битное квантование и несколько процессов с аффинностью к многоядерным настройкам. Чтобы уменьшить размер модели, мы экспериментировали с 4-битным логарифмическим квантованием, но использовали плавающие значения во время выполнения. В совместной задаче большинство наших представлений были оптимальными по Парето с точки зрения компромисса между временем и качеством.', 'ga': 'Ghlacamar páirt i ngach rian den Cheardlann ar Thasc Comhroinnte Éifeachtúlachta Giniúint Néar agus Aistriúcháin 2020: LAP aon-lárnach, LAP il-lárnach, agus GPU. Ag leibhéal an mhúnla, bainimid úsáid as oiliúint múinteoirí-mac léinn le méideanna éagsúla mac léinn, leabaithe ceangail agus uaireanta sraitheanna, úsáidimid an tAonad Athfhillteach Níos Simplí, agus tugtar isteach bearradh cinn. Ar GPUanna, d’úsáideamar croíleacáin tensor snámhphointe 16-giotán. Ar LAPanna, rinneamar cainníochtú 8-giotán agus próisis iolracha a shaincheapadh le cleamhnas don suíomh il-lárnach. Chun méid an mhúnla a laghdú, rinneamar turgnamh le cainníochtú loga 4-giotán ach úsáidimid snámháin ag am rite. Sa tasc roinnte, bhí an chuid is mó dár n-aighneachtaí Pareto optamach maidir leis an gcomhbhabhtáil idir am agus cáilíocht.', 'hu': 'Részt vettünk a 2020-as Neural Generation and Translation Efficiency Shared Task Workshop összes pályáján: egymagos processzor, többmagos processzor és GPU. Modellszinten tanár-diák képzést használunk különböző diákméretekkel, kötőbeágyazásokkal és néha rétegekkel, az Egyszerűbb Egyszerű Ismétlődő Egységet használjuk, és bevezetjük a fejmetszést. GPU-kon 16 bites lebegőpontos tenzormagokat használtunk. A processzorokon testre szabtuk a 8 bites kvantizálást és a több folyamatot, amelyek a többmagos beállításhoz hasonlóak. A modell méretének csökkentése érdekében 4 bites log kvantizálással kísérleteztünk, de futási időben float-okat használtunk. A közös feladat során a beadványok többsége Pareto optimális volt az idő és a minőség közötti kompromisszum tekintetében.', 'el': 'Συμμετείχαμε σε όλα τα κομμάτια του Εργαστηρίου Νευρικής Παραγωγής και Μετάφρασης 2020 Κοινή Εργασία: μονοπυρήνων CPU, πολυπυρήνων CPU και GPU. Στο επίπεδο μοντέλου, χρησιμοποιούμε εκπαίδευση δασκάλου-μαθητή με ποικίλα μεγέθη μαθητών, ενσωμάτωση δεσμών και μερικές φορές στρώματα, χρησιμοποιούμε την απλούστερη απλή επαναλαμβανόμενη μονάδα και εισάγουμε κλάδεμα κεφαλής. Σε GPU, χρησιμοποιήσαμε 16bits κυμαινόμενου σημείου πυρήνες Tensor. Στους επεξεργαστές, προσαρμόσαμε 8-κβαντισμό και πολλαπλάσιες διαδικασίες με συγγένεια για τη ρύθμιση πολλαπλών πυρήνων. Για να μειώσουμε το μέγεθος του μοντέλου, πειραματιστήκαμε με 4-bit κβαντισμό καταγραφής, αλλά χρησιμοποιούμε επιπλέες κατά τη διάρκεια εκτέλεσης. Στο κοινό έργο, οι περισσότερες από τις υποβολές μας ήταν βέλτιστες όσον αφορά το συμβιβασμό μεταξύ χρόνου και ποιότητας.', 'ka': 'ჩვენ მივიღეთ ყველა სამუშაო სამუშაო ნეიროლური განვითარება და განვითარება 2020 ეფექციენტის სამუშაო სამუშაო პარამეტრებში: ერთ-core CPU, მრავალ-core CPU და GPU. მოდელური დონეში, ჩვენ სტუდენტის სტუდენტის სტუდენტის სტუდენტის განმავლობას გამოყენებთ სტუდენტის განმავლობას, დაკავშირებას და ზოგჯერ სტუდენტის განმავლობას, გამოყ GPUs-ში ჩვენ გამოყენეთ 16-ბიტური კონტაქტის ტენსორის კონტაქტი. პროცესების შესახებ, ჩვენ 8- ბიტის კვანტიზაციას და მრავალ პროცესების განმავლობაში, რამდენიმე კვანტიზაციას დავაკეთებდით. მოდელური ზომის შემცირებისთვის, ჩვენ 4- ბიტური ლოგური კვანტიზაციით ექსპერიმენტირებდით, მაგრამ გამოყენეთ წარმოდგენების დროს. ოჲგვფვრჲ ჲრ ნაქთრვ ოპჲეყლზვნთწ ბვქვ ოაპვრჲ ჲორთმალნთ ჟ გყჱდლვზეანვრჲ ნა რპყდგარა между გპვმვრჲ თ კალთრვრჲ.', 'it': "Abbiamo partecipato a tutte le tracce del Workshop sulla Generazione Neurale e Translation 2020 Efficiency Shared Task: CPU single-core, CPU multi-core e GPU. A livello di modello, utilizziamo la formazione insegnante-studente con una varietà di dimensioni studentesche, incorporazioni di cravatte e talvolta strati, usiamo l'unità ricorrente semplice più semplice e introduciamo la potatura della testa. Sulle GPU, abbiamo utilizzato core tensori a 16 bit a virgola mobile. Sulle CPU, abbiamo personalizzato la quantizzazione a 8 bit e più processi con affinità per l'impostazione multi-core. Per ridurre le dimensioni del modello, abbiamo sperimentato la quantizzazione di log a 4 bit ma abbiamo utilizzato float al runtime. Nel compito condiviso, la maggior parte delle nostre proposte sono state Pareto ottimali nel rispetto del compromesso tra tempo e qualità.", 'lt': 'Dalyvavome visuose seminaro „Neural in ės generacijos ir 2020 m. vertimo efektyvumo klausimais“ etapuose: vienas pagrindinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis Modelio lygmeniu mes naudojame mokytojų ir student ų mokymą su įvairiais studentų dydžiais, sąsajų įdėjimais ir kartais sluoksniais, naudojame paprastesnį paprastesnį pakartotinį vienetą ir įdiegiame galvos smulkinimą. GPU naudojome 16 bit ų svyravimo taško tempimo jėgas. CPU, mes pritaikėme 8 bit ų kiekybinį nustatymą ir kelis procesus su afinitetu daugiašaliam nustatymui. Siekdami sumažinti modelio dydį, eksperimentavome su 4 bit ų log kiekybiškumu, bet naudojame plaukiojimo metu. Bendroje užduotyje dauguma mūsų pareiškimų buvo Pareto optimalus laikas ir kokybė.', 'kk': 'Біз 2020 жылы нейралық жасау және аудару және аудару жұмысының барлық жолдарына қатысушы болдық: бір негізгі процессор, бірнеше негізгі процессор және GPU. Үлгі деңгейінде мұғалім- студенттердің оқытуын көптеген студенттердің өлшемі, көптеген ендіру және кейбірде қабаттарды қолданып, Қарапайым қайталану бірлігін қолдану және басының көптеген GPУ үшін 16- биттік жылжымалы нүктелген тензер тензерін қолдандық. Процессорларда біз 8- бит квантизациясын және бірнеше процестерді бірнеше негізгі параметрлердің көпшілігімен өзгертдік. Үлгі өлшемін азайту үшін 4- биттік журналды квантизациялау арқылы тәжірибедік, бірақ жегу уақытында жылжымыз. Ортақ тапсырманың көпшілігіміз уақыт мен сапа арасындағы тәжірибесіне қатынасыз керек болды.', 'mk': 'Учествувавме на сите траги од работилницата за неурална генерација и превод 2020 ефикасност споделена задача: еднојадрен процесор, мултијадрен процесор и GPU. На моделно ниво, користиме обука на учител-студент со различни студентски големини, врзувања и понекогаш слоеви, користиме едноставна единица за повторно повторно, и воведуваме кревање на главата. На GPU, користевме 16-битни тензорски јадра. На процесорите, ја прилагодивме квантизацијата од 8 бити и многуте процеси со афинитет за поставувањето на многуте јадра. За да ја намалиме големината на моделот, експериментиравме со квантизација на логот од 4 бити, но употребувавме пливачки на време на бегство. Во заедничката задача, повеќето од нашите предлози беа Парето оптимални во однос на разликата помеѓу времето и квалитетот.', 'ms': 'Kami berpartisipasi dalam semua trek Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Pada GPU, kami menggunakan inti tensor titik-terapung 16-bit. Pada CPU, kami menyesuaikan kuantisasi 8-bit dan proses berbilang dengan afini untuk tetapan berbilang-inti. Untuk mengurangi saiz model, kami eksperimen dengan kuantisasi log 4-bit tetapi guna float pada masa berjalan. Dalam tugas berkongsi, kebanyakan penghantaran kami adalah Pareto optimal dengan menghormati perdagangan antara masa dan kualiti.', 'mt': 'Parteċipajna fil-binarji kollha tal-Workshop dwar il-Ġenerazzjoni Newrali u t-Traduzzjoni 2020 Kompitu Konġunt dwar l-Effiċjenza: CPU uniku, CPU multi ċentrali, u GPU. Fil-livell tal-mudell, a ħna nużaw taħriġ bejn l-għalliema u l-istudenti b’varjetà ta’ daqsijiet tal-istudenti, inkorporazzjonijiet tal-irbit u xi kultant saffi, nużaw l-Unit à Simpli u Rikorrenti, u nintroduċu l-pruning tar-ras. Fuq GPUs, użajna ċentri tat-tensur b’punt floating ta’ 16-il bit. Fuq is-CPUs, aħna addattajna kwantifikazzjoni ta’ 8 bits u proċessi multipli b’affinità għall-issettjar multi-core. Biex tnaqqas id-daqs tal-mudell, esperimentajna b’kwantifikazzjoni ta’ logaritmu ta’ 4 bits iżda użajna floats waqt ir-runtime. Fil-kompitu kondiviż, il-biċċa l-kbira tas-sottomissjonijiet tagħna kienu Pareto ottimali fir-rigward tal-kompromess bejn iż-żmien u l-kwalità.', 'ml': 'ഞങ്ങള്\u200d നെയുറല്\u200d ജനറനും പരിഭാഷവും 2020 സാധാരണ പങ്കാളികള്\u200d പങ്കുചേര്\u200dക്കുന്ന പണിയില്\u200d പങ്കാളികളില്\u200d പങ്കാളികളായി പങ്കുചേര്\u200dത്തിരിക്കുന്നു. ഒര മോഡല്\u200d നിലയില്\u200d, ഞങ്ങള്\u200d വിദ്യാര്\u200dത്ഥികളുടെ വലിപ്പം ഉപയോഗിക്കുന്നു, വിദ്യാര്\u200dത്ഥികളുടെ വലിപ്പമുള്ള പരിശീലനം ഉപയോഗിക്കുന്നു, ചിലപ്പോള്\u200d തട്ട ജിപിയുസില്\u200d ഞങ്ങള്\u200d 16- ബിറ്റ് നീലുന്ന ടെന്\u200dസര്\u200d കോര്\u200d ഉപയോഗിച്ചു. സിപിയുസില്\u200d ഞങ്ങള്\u200d 8 ബിറ്റ് വ്യവസ്ഥയും പല പ്രക്രിയകളും കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d സംവിധാ മോഡലിന്റെ വലിപ്പം കുറയ്ക്കാന്\u200d ഞങ്ങള്\u200d 4- ബിറ്റ് ലോഗിന്റെ പരീക്ഷണത്തില്\u200d പരീക്ഷിച്ചിരിക്കുന്നു. പക്ഷെ റ പങ്കാളിയുള്ള ജോലിയില്\u200d, നമ്മുടെ കീഴ്പ്പെടുത്തുന്നവരില്\u200d മിക്കവാറും സമയവും ഗുണവും തമ്മിലുള്ള വ്യാപാര്\u200dത്', 'mn': 'Бид 2020 оны мэдрэлийн бүтээлч, хөгжлийн үр дүнтэй хуваалцах үйл ажиллагаанд оролцсон. Нэг төвөгтэй CPU, олон төвөгтэй CPU, GPU. Загварын түвшинд бид багш нарын оюутнуудын сургалтын сургалтыг олон төрлийн хэмжээтэй хэрэглэдэг. Заримдаа холбоотой холбоотой холбоотой, заримдаа давхар холбоотой холбоотой холбоотой. Энгийн энгийн давхар GPUs дээр бид 16-бит floating point tensor cores ашигласан. Процессорын хувьд бид 8-бит хэмжээст болон олон төвөгтэй олон төвөгтэй процессүүдийг хувьд зөвшөөрсөн. Загварын хэмжээг багасгахын тулд бид 4-бит лог квантизацийг туршиж, гэхдээ ажиллах цаг хугацаанд хөдөлгөөн ашиглаж байна. Бидний хуваалцах ажлын ихэнх нь хугацаа болон сайн чанарын хоорондох худалдааны тухай Парето зөвхөн эерэг байсан.', 'no': 'Vi delta i alle spor av arbeidsområdet på Neuralgenerasjon og omsetjing 2020 delt delt effektivt delt oppgåve: enkelkjerne CPU, fleire kjerneCPU og GPU. På modellnivået bruker vi lærer-studenttrening med forskjellige studentstorleik, knyttingar og noen ganger lag, bruker den enklare gjentakingseininga og introduserer hodeprinning. På GPUs brukte vi 16- bits flytande- punkttensorer. På prosessarar har vi tilpassa 8- bit kvantisering og fleire prosessar med affinitet for multikjerneinnstillingane. For å redusera modellstorleiken, eksperimenterte vi med 4- bit loggkvantisering, men bruk flyttar ved køyringsdato. I den delte oppgåva var dei fleste av våre søknader Pareto optimalt med respekt av utviklinga mellom tid og kvalitet.', 'pl': 'Uczestniczyliśmy we wszystkich utworach Warsztatu Generacji Neuralnej i Translation 2020 Efektywność Wspólnego Zadania: procesor jednorzeniowy, procesor wielorzeniowy i GPU. Na poziomie modelu stosujemy szkolenia nauczyciela-ucznia z różnymi rozmiarami uczniów, osadzeniami wiązań, a czasem warstwami, używamy prostszej jednostki powtarzającej i wprowadzamy przycinanie głowy. Na procesorach graficznych używaliśmy 16-bitowych rdzeni tensorów zmiennoprzecinkowych. Na procesorach dostosowaliśmy 8-bitową kwantyzację i wiele procesów z powinowactwem do ustawienia wielorzeniowego. Aby zmniejszyć rozmiar modelu, eksperymentowaliśmy z 4-bitową kwantyzacją logów, ale używaliśmy floatów w czasie uruchomienia. W ramach wspólnego zadania większość naszych zgłoszeń była optymalna w Pareto pod względem kompromisu między czasem a jakością.', 'ro': 'Am participat la toate piesele Workshop-ului privind generarea neurală și traducerea eficienței 2020 Sarcină partajată: procesor single-core, procesor multi-core și GPU. La nivelul modelului, folosim instruirea profesor-elev cu o varietate de dimensiuni de elev, încorporări de cravată și uneori straturi, folosim Unitatea Recurentă Simplă și introducem tăierea capului. Pe GPU-uri, am folosit nuclee tensoare cu punct plutitor pe 16 biți. Pe procesoare, am personalizat cuantificarea pe 8 biți și procese multiple cu afinitate pentru setarea multi-core. Pentru a reduce dimensiunea modelului, am experimentat cuantificarea logului pe 4 biți, dar am folosit flotoare la rulare. În sarcina partajată, majoritatea depunerilor noastre au fost Pareto optime în ceea ce privește compromisul dintre timp și calitate.', 'sr': 'Učestvovali smo u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, multi core CPU i GPU. Na nivou model a, koristimo trening učitelja i učitelja sa raznim veličinama studenata, vezama i ponekad slojevima, koristimo jednostavniju jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16 bitnih tenzorskih koraka. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese sa afinitetom za višecore postavljanje. Da bismo smanjili veličinu modela, eksperimentirali smo sa kvantizacijom 4-bitnih dnevnika, ali koristili smo plove u provoznom vremenu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvaliteta.', 'si': 'අපි සියළුම ප්\u200dරවේශනය සහ පරිවර්තනය සම්ප්\u200dරවේශනයේ සියළුම ප්\u200dරවේශකයේ සියළුම ප්\u200dරවේශකයේ සම්පූර්ණය කරලා: එක-කෝර් CPU මොඩල් ස්ථානයේදී, අපි ගුරුවර්ති-විද්\u200dයාණික ප්\u200dරශ්නයක් පාවිච්චි කරනවා, විද්\u200dයාණික ප්\u200dරශ්නයක් සමග විද්\u200dයාණික ප්\u200dරශ GPUSට, අපි 16-බිට් ප්\u200dරවාහනය ප්\u200dරවාහනය කරලා තියෙන්නේ. CPU වලින්, අපි 8- බිට් ක්වාන්තිකරණය සහ ගොඩක් පරීක්ෂණය සඳහා ගොඩක් ක්\u200dරියාත්මක සැකසුම් වෙනුවෙන්. මොඩල් ප්\u200dරමාණය අඩු කරන්න, අපි 4- බිට් ලොග් ක්වාන්ටිස් එක්ක පරීක්ෂණය කරලා තියෙන්නේ නමුත් රන්ටිම් ව සමාගත වැඩේ ඉන්නේ, අපේ ගොඩක් පිළිගන්නේ පැරෙටෝ විශේෂය සහ කුළුතිය අතර ව්\u200dයාපාරයක් ගැන ගෞරවයෙන', 'so': 'We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU.  Iskuulka tusaale ahaan waxaynu isticmaalnaa waxbarashada waxbarashada ardayda oo kala duduwan, qashinka xidhan iyo qasnada qaarkood, waxaynu isticmaalnaa qaybta soo deganaanshaha ee fudud, waxaana soo bandhigaynaa caqliga madaxa. GPUs, waxaynu isticmaalnay 16-bit oo ku socota barta tensor. CPUs, waxaynu u isticmaalnay qiyaastii 8 bit oo kala duduwan, waxaana la xiriirnay kooxaha kala duduwan. Si aan u hooseeyo tirada modellka, waxaan ku tijaabiyey qiyaastii qoriga 4-bit, laakiin waxaynu isticmaalnaa goobta lagu soconayo. Shaqada la qaybsan, inta badan ka soo dhiibeyno waxay ahaayeen Pareto mid aad u wanaagsan inay ka heshiiyaan ganacsiga u dhexeeya wakhtiga iyo qiimo.', 'sv': 'Vi deltog i alla spår av Workshopen om Neural Generation and Translation 2020 Efficiency Shared Task: enkärnig processor, flerkärnig processor och GPU. På modellnivå använder vi lärar-elevutbildning med olika elevstorlekar, slipsbäddning och ibland lager, använder Simpler Simple Recurrent Unit och introducerar huvudbeskärning. På GPU:er använde vi 16-bitars flytande tensorkärnor. På processorer anpassade vi 8-bitars kvantisering och flera processer med affinitet för flerkärnig inställning. För att minska modellstorleken experimenterade vi med 4-bitars loggkvantisering men använde floats vid körning. I den delade uppgiften var de flesta av våra bidrag Pareto optimala med hänsyn till avvägningen mellan tid och kvalitet.', 'ta': '2020 மொழிபெயர்ப்புகள் பங்கிடப்பட்ட பணியில் நாங்கள் நெயுரல் உருவாக்குதல் மற்றும் மொழிபெயர்ப்புகளின் அனைத்து தடங்களிலும் பங்கிடப்பட்டோம்: ஒ At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  ஜிபியூஸில், நாங்கள் 16 பிட்டு மிதவை புள்ளி டென்சார் கோர்களை பயன்படுத்தினோம். சிபியூஸ் மீது, நாம் 8 பிட் அளவு மற்றும் பல உறுப்பு அமைப்புகளுக்கு தொடர்புடன் பல செயல்களை தனிப்பயனாக்கினோம். மாதிரி அளவை குறைக்க, நாம் 4- பிட் பதிவு மதிப்புடன் பரிசோதித்தோம் ஆனால் ஓடும் நேரத்தில் மிதவைகளை பயன்படுத்து. பங்கிடப்பட்ட பணியில், எங்கள் பெரும்பாலானவர்கள் பார்டோ விருப்பத்தேர்வாக இருந்தனர் நேரம் மற்றும் தரம் இடையே வணி', 'ur': 'ہم نے نورول پیدائش اور ترجمہ 2020 کے تمام ٹراکیوں میں شامل ہوا: single-core CPU, multicore CPU اور GPU. نمڈل سطح میں، ہم استاد-استاد کی تعلیم کے مطابق مختلف استاد کے ساتھ استعمال کرتے ہیں، ٹائی ابڈینگ اور کبھی لہروں کے مطابق، ساده ساده دوبارہ یونیٹ کے مطابق استعمال کرتے ہیں، اور سر پرینگ کو معلوم کرتے ہیں. جی پی یوس پر ہم 16 بیٹ فلانٹ پوینٹ ٹینسٹر کور استعمال کرتے تھے۔ CPUs پر ہم نے 8-bit quantization اور بہت سی پروسسوں کو مثبت کے ساتھ مطابق کیا۔ ماڈل کی اندازہ کم کرنے کے لئے، ہم نے 4-بیٹ لاگ کوانتیزی کے ساتھ آزمائش کی لیکن رونٹ زمانہ میں فلاٹ استعمال کریں۔ مشترک کام میں، ہماری اکثریت مسلمانوں کو وقت اور کیفیت کے درمیان تجارت کے معاملہ سے پاریٹا بہترین تھا.', 'uz': "2020 Tafsilotlar bilan bir necha CPU, bir nechta CPU, va GPU va bir nechta tarjima qilgan Workshopining hamma yo'plalariga ega bo'lgan. Model darajada, biz o'qituvchi o'qituvchi o'quvchi o'quvchi o'quvchi o'quvchi o'quvchi o'rganishdan foydalanamiz, o'zlarining ko'plab o'quvchilari sizlari bilan boshlanamiz, va ba'zida qachon qachon qatlamlar bilan boshlanamiz, od GPUs'da, biz 16-bit floating-point tensorning tugmalaridan foydalanamiz. CPU bilan 8- bit qiymatni moslash va bir nechta vazifalarni bir nechta boshqarish. Modelning oʻlchamini kamaytirish uchun biz 4- bitta logning tizimini aniqlashni istadik lekin ishga tushganda floatsiyalarni ishlatish. Bizning ko'pchiligimizning qismlarimiz vaqt va сифат орасидаги гуруҳларни муҳим қилиш учун Pareto.", 'vi': 'Chúng tôi đã tham gia vào mọi dấu vết của "Workshop in Neural generation and translation 2020 efficity shared Task: single-core CPU, đa-core CPU, và GPU. Ở mức mô hình, chúng tôi dùng khóa giáo viên-sinh viên với nhiều kích thước sinh viên, sự ghép thắt cà vạt và đôi khi nhiều lớp, dùng đơn giản đơn vị phục hồi, và giới thiệu việc tỉa đầu. Trên GPU, chúng tôi dùng lõi tensor ở tư khoản 16-cắn. Trên CPU, chúng tôi đã tùy chỉnh lượng 8-cắn và nhiều tiến trình có sự đồng thuận với thiết lập đa lõi. Để giảm cỡ mô hình, chúng tôi thí nghiệm với phân lượng 4-cắn bản ghi, nhưng sử dụng nổi ở thời gian chạy. Trong công việc chia sẻ, hầu hết tài liệu của chúng tôi là "Pareto" tối ưu với tôn trọng sự trao đổi giữa thời gian và chất lượng.', 'bg': 'Участвахме във всички песни на семинара за генериране и превод на неврони 2020 Споделена задача за ефективност: едноядрен процесор, многоядрен процесор и графичен процесор. На ниво модел използваме обучение учител-студент с различни размери на учениците, вграждания на вратовръзки и понякога слоеве, използваме по-простата обикновена повтаряща се единица и въвеждаме подрязване на главата. На GPU използвахме 16-битови тензорни ядра с плаваща точка. На процесорите персонализирахме 8-битова квантизация и множество процеси с афинитет към многоядрената настройка. За да намалим размера на модела, експериментирахме с 4-битова логова квантизация, но използвахме плувки по време на изпълнение. В споделената задача повечето от нашите предложения бяха оптимални по отношение на компромиса между време и качество.', 'da': "Vi deltog i alle spor af Workshoppen om Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU og GPU. På modelniveau bruger vi lærer-studerende træning med en række forskellige studerende størrelser, bindeindlejringer og undertiden lag, bruger Simpler Simple Recurrent Unit og introducerer hovedbeskæring. På GPU'er brugte vi 16-bit floating-point tensor kerner. På CPU'er tilpassede vi 8-bit kvantisering og flere processer med affinitet til multi-core indstillingen. For at reducere modelstørrelsen eksperimenterede vi med 4-bit logkvantisering, men bruger floats ved kørselstid. I den fælles opgave var de fleste af vores indlæg Pareto optimale med hensyn til afstemningen mellem tid og kvalitet.", 'hr': 'Učinili smo se u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, višecore CPU i GPU. Na razini model a, koristimo trening učitelja i učitelja s raznim veličinama učenika, ugrađenjem vezama i ponekad slojevima, koristimo jednostavniju jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16-bit plivajućih tenzorskih kabla. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese s afinitetom za višecore postavku. Da bismo smanjili veličinu modela, eksperimentirali smo s kvantizacijom 4-bit dnevnika, ali koristili smo plove u provozu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvalitete.', 'nl': "We hebben deelgenomen aan alle tracks van de Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU en GPU. Op modelniveau maken we gebruik van leraar-student training met een verscheidenheid aan studentengrootten, bindbindingen en soms lagen, gebruiken we de Simpler Simple Recurrent Unit en introduceren we hoofdsnijden. Op GPU's gebruikten we 16-bit floating-point tensor cores. Op CPU's hebben we 8-bits kwantisatie en meerdere processen aangepast met affiniteit voor de multi-core instelling. Om de modelgrootte te verkleinen, experimenteerden we met 4-bits logquantisatie, maar gebruikten floats tijdens runtime. In de gedeelde taak waren de meeste van onze inzendingen Pareto optimaal met respect voor de afweging tussen tijd en kwaliteit.", 'de': 'Wir haben an allen Tracks des Workshops zur Neuralen Generation und Übersetzung 2020 Efficiency Shared Task teilgenommen: Single-Core CPU, Multi-Core CPU und GPU. Auf Modellebene verwenden wir Lehrer-Schüler-Training mit einer Vielzahl von Schülergrößen, Bindeeinbettungen und manchmal Schichten, verwenden die Simpler Simple Recurrent Unit und führen Kopfschnitt ein. Auf GPUs verwendeten wir 16-Bit Gleitkomma-Tensorkerne. Auf CPUs haben wir 8-Bit-Quantisierung und mehrere Prozesse mit Affinität für die Multi-Core-Einstellung angepasst. Um die Modellgröße zu reduzieren, experimentierten wir mit 4-Bit Log Quantisierung, verwenden aber Floats zur Laufzeit. In der gemeinsamen Aufgabe waren die meisten unserer Einreichungen Pareto optimal hinsichtlich des Kompromisses zwischen Zeit und Qualität.', 'id': 'Kami berpartisipasi dalam semua jejak Workshop tentang Generasi Neural dan Translation 2020 Efisiensi Bersama Tugas: CPU satu-inti, CPU multi-inti, dan GPU. Pada tingkat model, kami menggunakan pelatihan guru-siswa dengan berbagai ukuran siswa, dasi embedding dan kadang-kadang lapisan, menggunakan Simpler Simple Recurrent Unit, dan memperkenalkan pemotong kepala. Pada GPU, kami menggunakan inti tensor titik berenang 16 bit. Pada CPU, kami menyesuaikan kuantisasi 8 bit dan proses berbilang dengan afinitas untuk seting multi-core. Untuk mengurangi ukuran model, kami eksperimen dengan kuantisasi log 4-bit tetapi menggunakan float pada waktu berjalan. Dalam tugas bersama, kebanyakan pengiriman kami adalah Pareto optimal dengan menghormati perdagangan antara waktu dan kualitas.', 'fa': 'ما در تمام رده\u200cهای کارگاه روی نسل\u200cهای عصبی و ترجمه\u200cهای فعالیت\u200cهای مشترک در سال ۲۰۰۲ شرکت کردیم: CPU single-core, CPU multicore, and GPU. در سطح مدل، ما از آموزش آموزش آموزش دانش آموزش استفاده می کنیم با اندازه های مختلف دانش آموزشی، وسیله\u200cهای قالب و گاهی طبقه\u200cها، از واحد ساده\u200cترین دوباره استفاده می\u200cکنیم، و سر را معرفی می\u200cکنیم. روی جی پی یوس، ما از ۱۶ بیت تنسور نقطه شناورن استفاده کردیم. در CPUs، ما 8 بیت کوانتیزی و فرایند چندین را با تعادل برای تنظیم مجموعه هسته\u200cهای زیادی تنظیم کردیم. برای کاهش اندازه مدل، ما با کوانتیزی چهار بیت لیگ آزمایش کردیم ولی از پرواز در زمان چرخش استفاده کردیم. در وظیفه مشترک، بیشتر تسلیم\u200cهای ما پارتو بهترین بود با احترام تجارت بین زمان و کیفیت.', 'sw': 'Tumeshiriki katika tafiti zote za Warsha ya Uzalishaji na Tafsiri ya Neurali 2020 Kushirikishwa na kazi: CPU moja kwa moja, CPU na GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Kwenye GPUs, tulitumia viungo vya viungo vya sekta 16 vya ndege. Kwenye CPU, tulitumia kiasi cha takwimu 8 na michakato mengi yenye uhusiano wa mazingira mengi. Ili kupunguza ukubwa wa mifano, tulijaribu kwa kiasi kikubwa cha log 4-bit lakini tunatumia mafua wakati wa runtime. Katika jukumu lililoshirikishwa, maoni mengi yetu yalikuwa bora zaidi ya Pareto kwa kuheshimu biashara kati ya muda na kiwango.', 'ko': '우리는 신경 생성 및 번역 2020 효율 공유 작업 세미나의 모든 궤도인 단핵 CPU, 다핵 CPU와 GPU에 참가했다.모델 차원에서 우리는 교사-학생 교육을 사용하는데 각종 학생 규모, 넥타이를 포함하고 때로는 차원도 있다. 더욱 간단한 귀속 단원을 사용하고 머리 커팅을 도입한다.GPU에서 우리는 16비트 부동 소수점 장량 핵을 사용한다.CPU에서는 멀티 코어 설정을 위한 8비트 계량화 및 멀티 프로세스를 맞춤형으로 구성했습니다.모델의 크기를 줄이기 위해서, 우리는 4개의 로그 양화를 시도했지만, 실행할 때 부동점을 사용했다.공유 작업 중 시간과 품질 사이의 균형을 보면, 우리의 대다수 제출은 파르토리코가 가장 좋다.', 'tr': "Biz Nural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, we GPU'yň ähli hatlaryna goşuldyk. Model derejesinde mugallymlary okuwçylar üçin birnäçe topar ölçüsi, baglaýyşlar we käwagt katlar bilen ulanýarys, Kiçiräk ýeterlik Birlikden ullan we kellämizi süýtgetmek üçin ullanýarys. GPU'da 16 bit ýüzlik noktalar esnesörlerini ulandyk CPU-lerde, multi-core düzenlemek üçin 8-bit küntatiýasyny we köp prosesleri bejerdik. Model ölçüsini azaltmak üçin 4-bit küçümseme ile denedik ama eserdeki çizgileri kullandık. Paýlaşan zadyň köpüsi wagt we kwalitet arasyndaky syýahat alyp Pareto optimaldyr.", 'am': 'በኔural Generation እና ትርጓሜ 2020 ተርጓሚዎች የስራ ትርጓሜ ሰርቨርስቲ ሁሉ ተጋርተናል: አንድ-core CPU፣ ብዙ-core CPU እና GPU. በሞዴል ደረጃው፣ አስተማሪ-ተማሪ ትምህርት ትምህርት በተለይ መጠን፣ እየቆረጥ እና አንዳንድ ጊዜ ደረጃዎች፣ ቀላል የቀረበውን ተማሪ እኩል እናስታውቃለን፡፡ በGPU ላይ 16 ቢትር እየነጥፍ ቆንጆር እየቆረጥን ነበር፡፡ በCPU ላይ 8 ቢትር መጠን እና በብዙ አካባቢ ማህበረሰብ ጋር አብዛኛዎችን ፍጥረቶች አስቀምጠን፡፡ የሞዴል መጠን ለማሳነስ፣ 4 ቢትር የሎግ ማሰናከል ፈተናል ግን በሮን ጊዜ መፍጠርን ተጠቀምን፡፡ In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.', 'af': "Ons het gedeel in alle snitte van die Werkshop op Neurale Generasie en Vertaling 2020 Gedeelde Opdrag: enkelcore CPU, multi core CPU en GPU. Op die model vlak gebruik ons onderwysers-studente onderwyseling met 'n verskillende studente grootte, tie inbêding en soms laag, gebruik ons die Eenvoudige Herhaalde Eenheid en introduseer hoof aandrukking. Op GPUs gebruik ons 16- bit floating- point tensor cores. Op CPUs, ons pasmaak 8- bit quantisasie en veelvuldige prosesse met affinity vir die multi core instelling. Om model grootte te verminder, het ons eksperimenteer met 4- bit log quantiseering, maar gebruik vliewe op looptyd. In die gedeelde taak was die meeste van ons ondersoek Pareto optimal met respek van die handel tussen tyd en kwaliteit.", 'hy': "Մենք մասնակցեցինք նյարդային սերունդի և 2020-ի թարգմանման արդյունավետության բոլոր գործընթացներին' մեկ հիմնական պրոցեբույսը, բազմահիմնական պրոցեբույսը և GPU-ը: Մոդելի մակարդակում մենք օգտագործում ենք ուսուցիչներ-ուսանողներ ուսուցիչներ ուսանողների տարբեր չափսերով, կապերով և երբեմն շերտերով, օգտագործում ենք պարզ կրկնվող միավորը և ներկայացնում գլխի կտրտումը: GPU-ում մենք օգտագործեցինք 16-բիթ լողացող կետի տենսորի հիմքեր: Համակարգչային համակարգերի վրա մենք պատրաստեցինք 8-բիտի քանակությամբ և բազմաթիվ գործընթացներով, որոնք բազմահիմնական սահմանափակում են աֆինիտիվություն: Մոդելի չափսի կրճատելու համար մենք փորձեցինք 4-բիտ լոգ քվանտիզացիայի միջոցով, բայց օգտագործեցինք լոգակներ ընթացքում: Մեր ընդհանուր խնդրի մեծամասնությունը Pareto-ն օպտիմալ էր ժամանակի և որակի միջև հակամարտության հարցում:", 'sq': 'Ne morëm pjesë në të gjitha gjurmët e Workshop mbi Gjenerimin Neural dhe Efikasitetin e Përkthimit 2020 Detyrën e Përbashkët: CPU me një qendër, CPU me shumë qendër dhe GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Në GPU, kemi përdorur 16-bit pikë-fluturimi në bazë të tensorit. Në CPU, ne personalizuam kuantizimin 8-bit dhe proceset e shumëfishtë me afinitet për përcaktimin e shumëbërthamës. Për të reduktuar madhësinë e modelit, ne eksperimentuam me kuantizim logaritmi 4-bit por përdorim fluturime në kohën e funksionimit. Në detyrën e përbashkët, shumica e paraqitjeve tona ishin Pareto optimale me respektin e kompromisit midis kohës dhe cilësisë.', 'bn': 'আমরা নিউরাল জেনারেশন এবং অনুবাদের ২০২০ টি প্রভাব শেয়ার করার কার্যকর কর্মশালায় অংশগ্রহণ করেছি: একক মূল সিপিউ, বহুমূল সিপিউ এবং জিপিউ। মডেল পর্যায়ে আমরা শিক্ষক-ছাত্রের প্রশিক্ষণ ব্যবহার করি বিভিন্ন ধরনের ছাত্রের আকার, বাড়ি বাঁধা এবং মাঝে মাঝে মাঝে মাঝে মাঝে সাধারণ পুনরাবার ইউনিট জিপিউসে আমরা ১৬ বিট ফ্লানিং পয়েন্ট ট টেনসার কোর ব্যবহার করেছিলাম। সিপিউসে আমরা ৮ বিটের পরিমাণ ব্যবহার করেছি এবং অনেক প্রক্রিয়ার সাথে মাল্টিক মূল সেটের সাথে সাথে যোগাযোগ করেছি। মডেলের আকার কমানোর জন্য আমরা ৪ বিট লগের পরীক্ষা করেছি কিন্তু রান্টাইমে ফ্লোটগুলো ব্যবহার করেছি। শেয়ার কর্মসূচীতে আমাদের বেশীরভাগ উপস্থাপন ছিল প্যারেটো অপেক্ষায়, সময় এবং মানের মধ্যে ব্যবসায়িক সম্মানের', 'az': 'Biz Nural Generation and Translation 2020 İşçin in Bütün İşçinin İşçilərinə daxil olduq: single-core CPU, multi-core CPU və GPU. Model səviyyəsində, müəllimlər-öğrencilər təhsilini müxtəlif ölçülərlə istifadə edirik, körpüsü və bəzi səviyyələr istifadə edirik, sadəcə olaraq təhsil təhsilini istifadə edirik və başlığı təhsil edirik. GPUlar üzerində 16-bit yüksək nöqtələr tenzeri kullandıq. CPUlar barəsində, çoxlu-core ayarlarının bağlılığı ilə 8-bit kvantifikasyonu və çoxlu prosesləri müəyyən etdik. Model böyüklüyünü azaltmaq üçün 4-bit log kvantifikasiyası ilə imtahana çəkdik, amma hərəkət vaxtında floats kullandıq. Bu paylaşdığımız işdə, müsəlmanlarımızın çoxu zaman və keyfiyyəti arasındakı ticarət haqqında Pareto optimal idi.', 'bs': 'Učestvovali smo u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, multi core CPU i GPU. Na razini model a, koristimo trening učitelja i učitelja sa raznim veličinama učenika, ugrađenjem vezama i ponekad slojevima, koristimo Jednostavnu Jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16-bit plivajućih tenzora. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese sa afinitetom za višecore postavku. Da bismo smanjili veličinu modela, eksperimentirali smo sa kvantizacijom 4-bit dnevnika, ali koristili smo plove u provozu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvalitete.', 'cs': 'Účastnili jsme se všech stop workshopu o neuronové generaci a překladu 2020 Efficiency Shared Task: jednojádrový CPU, vícejádrový CPU a GPU. Na úrovni modelu používáme školení učitelů-studentů s různými velikostmi studentů, vkládáním vazeb a někdy vrstvami, používáme jednodušší jednoduché opakované jednotky a zavádíme prořezávání hlavy. Na GPU jsme použili 16bitová tenzorová jádra s plovoucím bodem. U procesorů jsme přizpůsobili 8bitovou kvantizaci a více procesů s afinitou pro vícejádrové nastavení. Pro snížení velikosti modelu jsme experimentovali s 4-bitovou kvantizací log, ale používali jsme plováky za runtime. Ve společném úkolu byla většina našich podání Pareto optimální s ohledem na kompromis mezi časem a kvalitou.', 'et': 'Osalesime 2020. aasta neurogeneratsiooni ja tõlkimise töötoa kõikidel radadel: ühetuumaline protsessor, mitmetuumaline protsessor ja GPU. Mudeli tasandil kasutame õpetaja-õpilase koolitust erinevate õpilaste suuruste, lipsude manustamise ja mõnikord kihtidega, kasutame lihtsamat lihtsat korduvat üksust ja tutvustame pea lõikamist. GPU-del kasutasime 16-bitist ujuvpunktiga tensorsüdamikku. Protsessoritel kohandasime 8-bitist kvantiseerimist ja mitut protsessi mitme tuumaga seadistusega. Mudeli suuruse vähendamiseks katsetasime 4-bitist logikvantiseerimist, kuid kasutasime käivitusajal ujuvuid. Ühise ülesande puhul olid enamik meie ettepanekuid Pareto optimaalsed aja ja kvaliteedi vahelise kompromisse osas.', 'fi': 'Osallistuimme kaikkiin hermojen tuottamista ja kﾃ､ﾃ､ntﾃ､mistﾃ､ kﾃ､sittelevﾃ､n tyﾃｶpajan kappaleisiin 2020 Efficiency Shared Task: yksiytiminen suoritin, moniytiminen suoritin ja grafiikkasuoritin. Mallitasolla kﾃ､ytﾃ､mme opettaja-opiskelijakoulutusta, jossa on erilaisia opiskelijakokoja, solmioita ja joskus kerroksia, kﾃ､ytﾃ､mme Simpler Simple Toistuva yksikkﾃｶ ja esittelemme pﾃ､ﾃ､n karsimista. Grafiikkanﾃ､ytﾃｶnohjaimilla kﾃ､ytimme 16-bittisiﾃ､ kelluvapistetensoriytimiﾃ､. Prosessoreilla rﾃ､ﾃ､tﾃ､lﾃｶimme 8-bittistﾃ､ kvantisointia ja useita prosesseja moniytimisiin asetuksiin. Mallin koon pienentﾃ､miseksi kokeilimme 4-bittistﾃ､ lokikivantointia, mutta kﾃ､ytﾃ､mme floatseja ajoaikana. Yhteisessﾃ､ tehtﾃ､vﾃ､ssﾃ､ suurin osa ehdotuksistamme oli Pareto-optimaalista ajan ja laadun vﾃ､lisen kompromissin suhteen.', 'ca': "Vam participar en totes les pistes de la Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU i GPU. A nivell model, fem servir entrenament professor-estudiant amb una varietat de mida d'estudiants, enganxats i de vegades capes, fem servir l'Unitat Simple Recurrent, i introduïm pruning del cap. En GPU, vam utilitzar núcles de tensor flutuants de 16 bits. En els CPU, vam personalitzar la quantificació de 8 bits i múltiples processos amb afinitat a l'ajustament multinucli. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime.  En la tasca compartida, la majoria de les nostres proposicionseren Pareto óptims en relació amb el compromís entre temps i qualitat.", 'jv': 'Awak dhéwé wis ngubah barêng-barêng ning arep Workspace nang Generation Neral lan translation 2020 Efeffectness Joined tasks: single-coral Nang kudu model, kita gambar nggawe aturan guru kelas karo akeh pisan umut, ditambah barang nggawe layang lan sampek Layer, kuwi iso nggawe Simple Jejaring section Laptop" and "Desktop Nang ngomongke karo hal-hal nganggo kesempatan, akeh sing paling awak dhéwé Peretono sing paling nggawe ngupakan negoro kejahatan ning terakhir lan kaliwat', 'ha': '@ info: whatsthis At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Ga GPU, mun yi amfani da karatun tsumarni na guda na guda. Ga CPU, mun ƙayyade lissafar 8-bitan sami masu yawa da masu hushi wa daidaita mulki-nufi. Ga mu ƙara girmar shirin ayuka, sai muka jarraba tsarin logogin 4-bits kuma amma, mun yi amfani da floatsu idan na yi tafiya. Kuma a cikin aikin da aka raba, mafi yawanku da musuluntu sun kasance Parato mafificiya game da cinikin fara tsakanin lokaci da sifa.', 'sk': 'Sodelovali smo na vseh skladbah delavnice o nevralni generaciji in prevajanju 2020 Efficiency Shared Task: enojedrni procesor, večjedrni procesor in GPU. Na ravni modela uporabljamo usposabljanje učiteljev-učencev z različnimi velikostmi študentov, vgradnjo kravat in včasih plastmi, uporabljamo enoto enostavnejše ponavljajoče se enote in uvajamo obrezovanje glave. Na GPU smo uporabili 16-bitna tenzorska jedra s plavajočo točko. Na procesih smo prilagodili 8-bitno kvantizacijo in več procesov z afiniteto za večjedrno nastavitev. Za zmanjšanje velikosti modela smo eksperimentirali s 4-bitno kvantizacijo log, vendar uporabljali plovce v času delovanja. V skupni nalogi je bila večina naših prispevkov Pareto optimalna glede kompromisa med časom in kakovostjo.', 'he': 'השתתפנו בכל העקבות של הלימודים על דנרציה נוירולית ותרגום 2020 יעילות משימה משותפת: ברמה של המודל, אנו משתמשים באימונים מורים-סטודנטים עם מגוון של גודלים סטודנטים, קישורים קשרים ולפעמים שכבות, להשתמש ביחידה חדשה פשוטה יותר, ולהציג שיקוי ראש. על GPUs, השתמשנו בגרעני טנסור נקודת צף 16-ביט. על CPUs, אנחנו מתאימים 8-ביט קוונטיזציה ומספר תהליכים עם affinity לסטה multi-core. כדי להפחית את גודל המודל, ניסונו עם קיוונטיזציה של לוג 4-ביטים אבל להשתמש בציפורים בזמן הריצה. במשימה המשותפת, רוב ההצעות שלנו היו Pareto אופטימליים בכבוד הסחר בין זמן לאיכות.', 'bo': 'We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. མ་དབུལ་གྱི་ཚད་ལྟར། ང་ཚོས་སློབ་ཆེན་པོ་སྣ་ཚོགས་ལས་སྦྱོར་བྱེད་སྐབས་ཡིག་ཆ་རྐྱེན་བྱས་ནས། ང་ཚོས་GPUདུ་16-bit floating-point tensor cores་སྤྱོད་པ་ཡིན། On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. རྣམ་གྲངས་ཚད་དམའ་དགོས་བྱས་ན། ང་ཚོས་དྲན་ཐིག་གི་ཚད་ལྡན་བཞིན་པའི་བརྡ་སྟོན་བྱས་མིན་འདུག དབྱེ་སྤྱོད་མཁན་གྱི་ལས་འགུལ་དུ་མང་ཆེ་བ་ཡིན་ན་ང་ཚོའི་བསམ་འཆར་པ་ནི་དུས་ཚོད་དང་ཁྱད་པར་བརྗོད་སྤེལ་གྱི་'}
{'en': 'Improving Document-Level Neural Machine Translation with Domain Adaptation', 'ar': 'تحسين الترجمة الآلية العصبية على مستوى المستند مع تكييف المجال', 'fr': "Améliorer la traduction automatique neuronale au niveau du document grâce à l'adaptation de domaine", 'es': 'Mejora de la traducción automática neuronal a nivel de documento con adaptación de dominios', 'pt': 'Melhorando a tradução automática neural em nível de documento com adaptação de domínio', 'ja': 'ドメイン適応による文書レベルの神経機械翻訳の改善', 'zh': '因域适配改进文档级神经机器翻译', 'ru': 'Улучшение нейронного машинного перевода на уровне документа с адаптацией домена', 'hi': 'डोमेन अनुकूलन के साथ दस्तावेज़-स्तर तंत्रिका मशीन अनुवाद में सुधार', 'ga': 'Feabhas a chur ar Aistriú Inneall Néarach ar Leibhéal Doiciméid le hoiriúnú Fearainn', 'ka': 'დოკუმენტის დოკუმენტის ნეიროლური მაქინის გადატყვება დომინის ადაპტაციაზე', 'el': 'Βελτίωση της νευρωνικής μηχανικής μετάφρασης σε επίπεδο εγγράφου με προσαρμογή τομέα', 'hu': 'A dokumentum szintű idegszintű gépi fordítás javítása a domain adaptációval', 'lt': 'Dokumentų lygio nervinių mašinų vertimo tobulinimas pritaikant domeną', 'mk': 'Improving Document-Level Neural Machine Translation with Domain Adaptation', 'it': "Miglioramento della traduzione automatica neurale a livello di documento con l'adattamento del dominio", 'kk': 'Домен адаптациясы менен құжат деңгейіндегі нейралы машинаның аударуын жақсарту', 'ms': 'Perbaikan Penerjemahan Mesin Neural Aras Dokumen dengan Penyesuaian Domain', 'mn': 'Документын түвшинд мэдрэлийн машин хөгжүүлэлтийг сайжруулах', 'ml': 'ഡൊമെയിന്\u200d അഡാപ്റ്റേഷനുമായി രേഖ- നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നു', 'mt': 'Titjib fit-Traduzzjoni tal-Magna Newrali fil-Livell tad-Dokument bl-Adattament tad-Domain', 'no': 'Forbetra omsetjing av dokumentnivå på neuralmaskin med domenetilpassing', 'si': 'ඩොමේන් අනුවර්තනය සඳහා ලේඛන- තත්වය න්\u200dයූරාල මේෂන් අවවාදය වැඩ කරන්න', 'ro': 'Îmbunătățirea traducerii automate neurale la nivel de document cu adaptarea domeniului', 'pl': 'Poprawa neuronowego tłumaczenia maszynowego na poziomie dokumentów dzięki adaptacji domeny', 'so': 'Horumarinta dukumenti-Level Neural Mashine Turjumidda Domain Adaptation', 'sv': 'Förbättra neural maskinöversättning på dokumentnivå med domänanpassning', 'ta': 'ஆவண- மட்டத்தை மொழிபெயர்ப்பு டொமைன் மொழிபெயர்ப்புடன் மேம்படுத்துகிறது', 'ur': 'ڈومین اڈپٹیٹ کے ساتھ دکھانے-سطح نیورال ماشین کا ترجمہ بہتر کر رہا ہے', 'sr': 'Poboljšanje prevoda neuroloških mašina na nivou dokumenta sa adaptacijom domena', 'uz': 'Name', 'vi': 'Thay đổi máy thần kinh cấp Tài liệu với sửa lỗi miền', 'bg': 'Подобряване на невралния машинен превод на ниво документ с адаптация на домейна', 'nl': 'Verbetering van neuronale machinevertaling op documentniveau met domeinaanpassing', 'da': 'Forbedring af dokumentniveau neural maskinoversættelse med domænetilpasning', 'hr': 'Poboljšanje prijevoza neuroloških strojeva na nivou dokumenta s prilagodbom domena', 'id': 'Menembangkan Penerjemahan Mesin Neural Tingkat Dokumen dengan Adaptasi Domain', 'de': 'Verbesserung der neuronalen maschinellen Übersetzung auf Dokumentenebene mit Domänenanpassung', 'fa': 'بهتر ترجمه ماشین عصبی سطح سند', 'ko': '영역 기반 자동 적응 문서급 신경기계 번역 개선', 'sw': 'Kuboresha Tafsiri ya Mashine ya Kikaribia ya Makala na Kupitia Huduma', 'af': 'Verbeter dokument- vlak neurale masjien vertaling met domein aanpassing', 'sq': 'Duke përmirësuar përkthimin e makinës nervore me nivelin e dokumentit me adaptimin e domenit', 'tr': 'Sened derejesi Nural Maşynyň Çeviri Aýratma bilen gowylaşdyr', 'hy': 'Փաստողի-մակարդակի նյարդային մեքենայի թարգմանման բարելավումը դաշտի հարմարեցման միջոցով', 'am': 'ሰነድ-ደረጃን በ ዶሜን አድaptation ትርጉም', 'az': 'D칬k칲man-s톛viyy톛si N칬ral Makinel톛rin T톛rc칲m칲n칲 Domena Adfiksiyas캼 il톛 캻cat Et', 'bn': 'Improving Document-Level Neural Machine Translation with Domain Adaptation', 'ca': "millorar la traducció de màquines neuronals a nivell documental amb l'adaptació del domini", 'bs': 'Poboljšanje prevoda neuroloških strojeva na nivou dokumenta sa adaptacijom domena', 'cs': 'Zlepšení neuronového strojového překladu na úrovni dokumentů pomocí adaptace domény', 'et': 'Dokumenditaseme neuromasintõlke parandamine domeeni kohandamisega', 'fi': 'Asiakirjatason neurokäännöksen parantaminen verkkotunnuksen mukauttamisella', 'ha': '@ action', 'jv': 'politenessoffpolite"), and when there is a change ("assertive', 'sk': 'Izboljšanje živčnega strojnega prevajanja na ravni dokumentov s prilagajanjem domen', 'he': 'Improving Document-Level Neural Machine Translation with Domain Adaptation', 'bo': 'དྲ་ཁྲོད་ཀྱིས་ཡིག་ཆ་སྐྱེལ་ཚད་དང་སྐྱེལ་གྱི་མ་ལག་གི་ཆ་ཚུལ་ཡར་རྒྱས་གཏོང་བ'}
{'en': 'Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture  contextual information  from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed  systems  by exploiting limited in-domain data. This paper presents FJWU’s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe  systems  according to the testing domain.', 'ar': 'أظهرت الدراسات الحديثة أنه يمكن تحسين جودة الترجمة لأنظمة NMT من خلال توفير معلومات سياقية على مستوى المستند. بشكل عام ، يتم توسيع نماذج NMT المستندة إلى الجملة لالتقاط المعلومات السياقية من مجموعة وثائق على مستوى واسع النطاق يصعب الحصول عليها. من ناحية أخرى ، يعد تكييف المجال بتكييف مكونات الأنظمة المطورة بالفعل من خلال استغلال البيانات المحدودة في المجال. تقدم هذه الورقة تقديم نظام FJWU في WNGT ، وقد شاركنا على وجه التحديد في مهمة MT على مستوى المستند للترجمة الألمانية-الإنجليزية. يعتمد نظامنا على نموذج Transformer الواعي بالسياق والذي تم تطويره على رأس بنية NMT الأصلية من خلال دمج المعلومات السياقية باستخدام شبكات الانتباه. تُظهر نتائجنا التجريبية تقديم جمل سابقة لأن السياق يحسن بدرجة كبيرة درجة BLEU مقارنة بخط أساس NMT القوي. درسنا أيضًا تأثير تكييف المجال على ترجمة المستندات وتمكنا من تحسين النتائج من خلال تكييف الأنظمة وفقًا لمجال الاختبار.', 'pt': 'Estudos recentes mostraram que a qualidade da tradução de sistemas NMT pode ser melhorada fornecendo informações contextuais em nível de documento. Em geral, os modelos NMT baseados em sentenças são estendidos para capturar informações contextuais de corpora em nível de documento de grande escala que são difíceis de adquirir. A adaptação de domínio, por outro lado, promete adaptar componentes de sistemas já desenvolvidos, explorando dados limitados no domínio. Este artigo apresenta a submissão do sistema da FJWU no WNGT, participamos especificamente da tarefa de MT em nível de documento para tradução Alemão-Inglês. Nosso sistema é baseado no modelo Transformer sensível ao contexto desenvolvido em cima da arquitetura NMT original, integrando informações contextuais usando redes de atenção. Nossos resultados experimentais mostram que fornecer frases anteriores como contexto melhora significativamente a pontuação BLEU em comparação com uma linha de base NMT forte. Também estudamos o impacto da adaptação de domínio na tradução em nível de documento e conseguimos melhorar os resultados adaptando os sistemas de acordo com o domínio de teste.', 'fr': "Des études récentes ont montré que la qualité de traduction des systèmes NMT peut être améliorée en fournissant des informations contextuelles au niveau du document. En général, les modèles NMT basés sur des phrases sont étendus pour capturer des informations contextuelles à partir de corpus de documents à grande échelle qui sont difficiles à acquérir. L'adaptation de domaine, quant à elle, promet d'adapter des composants de systèmes déjà développés en exploitant des données internes limitées au domaine. Cet article présente la soumission du système de FJWU à WNGT, nous avons spécifiquement participé à la tâche de traduction automatique au niveau du document pour la traduction allemand-anglais. Notre système est basé sur un modèle de transformateur sensible au contexte développé sur l'architecture NMT originale en intégrant des informations contextuelles à l'aide de réseaux d'attention. Nos résultats expérimentaux montrent que le fait de fournir des phrases précédentes en tant que contexte améliore considérablement le score BLEU par rapport à une base NMT solide. Nous avons également étudié l'impact de l'adaptation des domaines sur la traduction au niveau du document et avons pu améliorer les résultats en adaptant les systèmes en fonction du domaine de test.", 'es': 'Estudios recientes han demostrado que la calidad de la traducción de los sistemas de NMT puede mejorarse proporcionando información contextual a nivel de documento. En general, los modelos de NMT basados en oraciones se extienden para capturar información contextual de corpus a gran escala a nivel de documento que son difíciles de adquirir. La adaptación de dominio, por otro lado, promete adaptar componentes de sistemas ya desarrollados mediante la explotación de datos limitados en el dominio. Este documento presenta el envío del sistema de FJWU en WNGT, participamos específicamente en la tarea MT a nivel de documento para la traducción alemán-inglés. Nuestro sistema se basa en el modelo Transformer sensible al contexto desarrollado sobre la arquitectura NMT original mediante la integración de información contextual mediante redes de atención. Nuestros resultados experimentales muestran que proporcionar oraciones previas como contexto mejora significativamente la puntuación BLEU en comparación con una base sólida de NMT. También estudiamos el impacto de la adaptación de dominios en la traducción a nivel de documento y pudimos mejorar los resultados adaptando los sistemas de acuerdo con el dominio de prueba.', 'ja': '最近の研究では、文書レベルの文脈情報を提供することで、NMTシステムの翻訳品質を向上させることができることが示されている。 一般に、文ベースのNMTモデルは、取得が困難な大規模なドキュメントレベルのコーラからコンテキスト情報を取り込むために拡張される。 一方、ドメインの適応は、限られたドメイン内データを利用することによって、既に開発されたシステムのコンポーネントを適応させることを約束します。 本稿では、WNGTにおけるFJWUのシステムサブミッションを紹介し、特にドイツ語-英語翻訳の文書レベルのMTタスクに参加しました。 当社のシステムは、注意喚起ネットワークを使用してコンテキスト情報を統合することにより、オリジナルのNMTアーキテクチャの上に開発されたコンテキスト認識型トランスフォーマーモデルに基づいています。 我々の実験結果は、コンテキストが強力なNMTベースラインと比較してBLEUスコアを有意に改善するため、前の文章を提供することを示す。 また、ドキュメントレベルの翻訳におけるドメイン適応の影響を研究し、テストドメインに応じてシステムを適応させることで結果を改善することができました。', 'zh': '近者研明,文档上下文信息,可以崇 NMT 统之译。 大抵句之 NMT ,广为难得之大文档语料库获上下文信。 其一,域应承诺因有限之域内数以应已发之统组件。 本文FJWU在WNGT系统提交,专参德语 - 英语翻译文档级MT务。 统者,上下文之Transformer也,始NMT架构之上,因意网络上下文。 吾实验结果表明,比于强NMT基线,供前句以为上下文显增BLEU分。 又考域名改编文档译,得以试领调统改善。', 'ru': 'Недавние исследования показали, что качество перевода систем NMT может быть улучшено путем предоставления контекстной информации на уровне документа. В целом, модели НМП, основанные на предложениях, расширяются для получения контекстуальной информации от крупномасштабных корпусов на уровне документов, которые трудно получить. Адаптация домена, с другой стороны, обещает адаптацию компонентов уже разработанных систем за счет использования ограниченных внутридоменных данных. В этой статье представлена подача системы FJWU на WNGT, мы специально участвовали в задании на уровне документа MT для перевода с немецкого на английский. Наша система основана на контекстно-зависимой модели трансформатора, разработанной на основе оригинальной архитектуры NMT путем интеграции контекстной информации с использованием сетей внимания. Наши экспериментальные результаты показывают, что предоставление предыдущих предложений по мере того, как контекст значительно улучшает балл BLEU по сравнению с сильным базовым уровнем НБ. Мы также изучили влияние адаптации домена на трансляцию на уровне документа и смогли улучшить результаты, адаптировав системы в соответствии с областью тестирования.', 'hi': 'हाल के अध्ययनों से पता चला है कि एनएमटी सिस्टम की अनुवाद गुणवत्ता को दस्तावेज़-स्तर की प्रासंगिक जानकारी प्रदान करके सुधारा जा सकता है। सामान्य रूप से वाक्य-आधारित एनएमटी मॉडल को बड़े पैमाने पर दस्तावेज़-स्तर के कॉर्पोरेट से प्रासंगिक जानकारी को कैप्चर करने के लिए विस्तारित किया जाता है, जिसे प्राप्त करना मुश्किल होता है। दूसरी ओर डोमेन अनुकूलन सीमित इन-डोमेन डेटा का शोषण करके पहले से ही विकसित सिस्टम के घटकों को अनुकूलित करने का वादा करता है। यह पेपर WNGT में FJWU के सिस्टम सबमिशन को प्रस्तुत करता है, हमने विशेष रूप से जर्मन-अंग्रेजी अनुवाद के लिए दस्तावेज़ स्तर एमटी कार्य में भाग लिया। हमारी प्रणाली संदर्भ-जागरूक ट्रांसफॉर्मर मॉडल पर आधारित है जो ध्यान नेटवर्क का उपयोग करके प्रासंगिक जानकारी को एकीकृत करके मूल एनएमटी आर्किटेक्चर के शीर्ष पर विकसित किया गया है। हमारे प्रयोगात्मक परिणाम पिछले वाक्यों को संदर्भ के रूप में प्रदान करते हुए दिखाते हैं जो एक मजबूत एनएमटी बेसलाइन की तुलना में BLEU स्कोर में काफी सुधार करता है। हमने दस्तावेज़ स्तर के अनुवाद पर डोमेन अनुकूलन के प्रभाव का भी अध्ययन किया और परीक्षण डोमेन के अनुसार सिस्टम को अनुकूलित करके परिणामों में सुधार करने में सक्षम थे।', 'ga': 'Léiríodh i staidéir le déanaí gur féidir cáilíocht aistriúcháin na gcóras NMT a fheabhsú trí fhaisnéis chomhthéacsúil ag leibhéal doiciméad a sholáthar. Go ginearálta leathnaítear samhlacha NMT bunaithe ar phianbhreith chun faisnéis chomhthéacsúil a ghabháil ó chorpora ar scála mór ar leibhéal doiciméad atá deacair a fháil. Ar an taobh eile de réir oiriúnú fearainn, táthar ag súil go ndéanfar comhpháirteanna de chórais atá forbartha cheana féin a oiriúnú trí shonraí teoranta san fhearann a shaothrú. Cuireann an páipéar seo aighneacht chóras FJWU i láthair ag WNGT, ghlacamar go sonrach páirt i tasc MT leibhéal Doiciméad le haghaidh aistriúchán Gearmáinis-Béarla. Tá ár gcóras bunaithe ar mhúnla Trasfhoirmeora atá feasach ar an gcomhthéacs a forbraíodh ar bharr na hailtireachta bunaidh NMT trí fhaisnéis chomhthéacsúil a chomhtháthú ag baint úsáide as líonraí aird. Léiríonn ár dtorthaí turgnamhacha má chuirtear abairtí roimhe seo ar fáil mar go gcuireann comhthéacs feabhas suntasach ar scór BLEU i gcomparáid le bunlíne láidir NMT. Rinneamar staidéar freisin ar an tionchar a bhíonn ag oiriúnú fearainn ar aistriúchán ar leibhéal doiciméad agus bhíomar in ann torthaí a fheabhsú trí na córais a oiriúnú de réir an fhearainn tástála.', 'ka': 'NMT სისტემის თავისუფალი კონტექსტური ინფორმაციის შესაძლებელია დაახლოებით დოკუმენტის დოკუმენტის კონტექსტური ინფორმაციას. ყველაფერი სიტყვების განსაზღვრებული NMT მოდელები გაფართებულია, რომ კონტექსტური ინფორმაცია დიდი დოკუმენტის კოპორაციის განსაზღვრებისთვის, რომელიც ძალიან დომენის ადაპტიფიკაცია მეორე მხარეს გვეუბნება, რომ კომპონტენტები მხოლოდ განვითარებული სისტემების ადაპტიფიკაცია, ექსპონტიკური მონაცემე ამ დოკუმენტის შესახებ FJWU-ს სისტემის შესახებ WNGT-ში, ჩვენ განსაკუთრებით დავწევეთ დოკუმენტის MT რაოდენობაში გერმანული-ანგლისური შესახებ. ჩვენი სისტემა კონტექსტურის შესახებ რენსტრუმენტერის მოდელი, რომელიც განვითარებულია ორიგინალური NMT აქტიქტიქტურის ზედან, ინტერქსტური ინფორმა ჩვენი ექსპერიმენტიური შედეგები გამოჩვენება წინასწარმოდგენების წინასწარმოდგენება, როგორც კონტექსტური მნიშვნელოვანია BLEU წინასწარმოდგენება,  ჩვენ ასევე შევსწავლოთ დემომინის აკაპრაციაციის შედეგი დოკუმენტის ტრანლაციონის დონეზე და შეგვიძლია გაუკეთესოთ შედეგი სისტემის აკაპრატირება სექ', 'el': 'Πρόσφατες μελέτες έχουν δείξει ότι η ποιότητα της μετάφρασης των συστημάτων NMT μπορεί να βελτιωθεί με την παροχή πληροφοριών περιβάλλοντος σε επίπεδο εγγράφων. Γενικά, τα μοντέλα που βασίζονται σε προτάσεις επεκτείνονται για να συλλάβουν πληροφορίες περιβάλλοντος από μεγάλα σώματα σε επίπεδο εγγράφων που είναι δύσκολο να αποκτηθούν. Η προσαρμογή του τομέα από την άλλη πλευρά υπόσχεται προσαρμογή στοιχείων ήδη αναπτυγμένων συστημάτων με την αξιοποίηση περιορισμένων δεδομένων εντός του τομέα. Αυτή η εργασία παρουσιάζει την υποβολή του συστήματος στο WNGT, συμμετέχαμε συγκεκριμένα στην εργασία ΜΤ επιπέδου εγγράφου για γερμανική-αγγλική μετάφραση. Το σύστημά μας βασίζεται στο μοντέλο μετασχηματιστή που αναπτύχθηκε πάνω στην αρχική αρχιτεκτονική ενσωματώνοντας πληροφορίες περιβάλλοντος χρησιμοποιώντας δίκτυα προσοχής. Τα πειραματικά μας αποτελέσματα δείχνουν ότι η παροχή προηγούμενων προτάσεων ως πλαίσιο βελτιώνει σημαντικά την βαθμολογία σε σύγκριση με μια ισχυρή βάση NMT. Μελετήσαμε επίσης τον αντίκτυπο της προσαρμογής του τομέα στη μετάφραση του εγγράφου και μπορέσαμε να βελτιώσουμε τα αποτελέσματα προσαρμόζοντας τα συστήματα σύμφωνα με τον τομέα δοκιμής.', 'hu': 'A közelmúltbeli tanulmányok kimutatták, hogy az NMT-rendszerek fordítási minősége javítható dokumentumszintű kontextuális információkkal. Általában a mondat alapú NMT modelleket kiterjesztik, hogy kontextuális információkat rögzítsenek nagyméretű dokumentumszintű korpuszokból, amelyeket nehéz beszerezni. A domain adaptáció másrészt a korlátozott domain adatok kihasználásával ígéri a már kifejlesztett rendszerek összetevőinek adaptálását. Ez a tanulmány bemutatja az FJWU rendszer benyújtását a WNGT-nél, kifejezetten részt vettünk a dokumentum szintű MT feladatban a német-angol fordításhoz. Rendszerünk az eredeti NMT architektúra tetején fejlesztett kontextustudatos Transformer modellre épül, figyelemfelkeltő hálózatok segítségével integrálva a kontextuális információkat. Kísérleti eredményeink azt mutatják, hogy korábbi mondatokat adtunk, mivel a kontextus jelentősen javítja a BLEU pontszámot az erős NMT kiinduláshoz képest. Tanulmányoztuk továbbá a domain adaptáció dokumentumszintű fordításra gyakorolt hatását, és a rendszerek tesztelési tartományhoz igazításával javítottuk az eredményeket.', 'it': "Studi recenti hanno dimostrato che la qualità della traduzione dei sistemi NMT può essere migliorata fornendo informazioni contestuali a livello di documento. In generale, i modelli NMT basati su frasi sono estesi per acquisire informazioni contestuali da corpora a livello di documento su larga scala che sono difficili da acquisire. L'adattamento del dominio promette invece di adattare componenti di sistemi già sviluppati sfruttando dati interni limitati. Questo articolo presenta la presentazione del sistema di FJWU a WNGT, abbiamo partecipato specificamente al compito MT a livello di documento per la traduzione tedesco-inglese. Il nostro sistema si basa sul modello Transformer contestuale sviluppato sulla base dell'architettura NMT originale integrando informazioni contestuali utilizzando reti di attenzione. I nostri risultati sperimentali mostrano che fornire frasi precedenti come contesto migliora significativamente il punteggio BLEU rispetto a una base NMT forte. Abbiamo anche studiato l'impatto dell'adattamento del dominio sulla traduzione a livello di documento e siamo stati in grado di migliorare i risultati adattando i sistemi in base al dominio di prova.", 'lt': 'Neseniai atlikti tyrimai parodė, kad NMT sistemų vertimo kokybę galima pagerinti teikiant dokumentų lygmens kontekstinę informaciją. Paprastai NMT modeliai, pagrįsti sakiniais, išplečiami siekiant surinkti kontekstinę informaciją iš didelio masto dokumentų lygmens korporacijos, kurią sunku įgyti. Kita vertus, prisitaikymas prie srities žada prisitaikyti jau sukurtų sistemų komponentus naudojant ribotus srities duomenis. Šiame dokumente pateikiamas FJWU sistemos pristatymas WNGT, mes konkrečiai dalyvavome dokumentų lygmens MT užduotyje vokiečių ir anglų vertimui. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks.  Mūsų eksperimentiniai rezultatai rodo, kad ankstesnių sakinių pateikimas, nes aplinkybės gerokai pagerina BLEU rezultatą, palyginti su stipria NMT pradine verte. Taip pat išnagrinėjome srities pritaikymo poveikį dokumentų vertimui ir sugebėjome pagerinti rezultatus pritaikydami sistemas pagal bandymų sritį.', 'kk': 'Жуырдағы зерттеулерде NMT жүйелердің аудармалардың сапасы құжат деңгейіндегі контексті мәліметті беріп жақсартылады. Ең жалпы сұрақ негіздеген NMT үлгілері құжат деңгейіндегі корпорасынан контексті мәліметті алу үшін кеңейтіледі. Басқа жағында доменнің адаптациясы доменнің шектелген деректерін қолдану үшін қолданылған жүйелердің компоненттерін адаптациялауға мүмкіндік береді. Бұл қағаз FJWU жүйесінің WNGT- ке жіберілімін көрсетеді. Біз неміс- ағылшын тілінде MT құжат деңгейінде қатынасыз. Біздің жүйеміз контексті түрлендіру үлгісіне негізделген NMT архитектурасының үстінен жасалған, тәртіпсіздік мәліметтерді желілерімен біріктіру үшін. Біздің тәжірибелік нәтижеміз алдыңғы сөйлемелерді көрсету үшін контексті BLEU нәтижесін күшті NMT негізгі жолымен салыстыру үшін жақсы жасайды. Құжат деңгейінде домен адаптациясының әсерін зерттедік. Жүйелерді сынақтар доменіне сәйкес қолдану үшін нәтижелерді жақсартуға болады.', 'ms': 'Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information.  Secara umum, model NMT berasaskan kalimat dilambangkan untuk menangkap maklumat kontekstual dari korpra aras dokumen skala besar yang sukar untuk dicapai. Penyesuaian domain pada sisi lain berjanji menyesuaikan komponen sistem yang telah dikembangkan dengan mengeksploitasi data dalam domain yang terbatas. Kertas ini memperkenalkan penghantaran sistem FJWU di WNGT, kami secara khusus berpartisipasi dalam tugas MT aras Dokumen untuk terjemahan bahasa Jerman-Inggeris. Sistem kita berdasarkan model Transformer yang sedar-konteks yang dikembangkan di atas arkitektur NMT asal dengan mengintegrasikan maklumat kontekstual menggunakan rangkaian perhatian. Hasil percubaan kami menunjukkan menyediakan kalimat terdahulu sebagai konteks meningkatkan dengan signifikan skor BLEU dibandingkan dengan dasar NMT yang kuat. Kami juga mempelajari kesan penyesuaian domain pada penerjemahan aras dokumen dan mampu memperbaiki keputusan dengan menyesuaikan sistem mengikut domain ujian.', 'ml': 'അടുത്ത വിദ്യാഭാഷണങ്ങള്\u200d കാണിച്ചിരിക്കുന്നുവെങ്കില്\u200d NMT സിസ്റ്റമുള്ള വിവരങ്ങള്\u200d മെച്ചപ്പെടുത്തുന പൊതുവായി വാക്ക് അടിസ്ഥാനമാക്കിയ NMT മോഡലുകള്\u200d പിടിക്കാന്\u200d വിശാലമായി വിശാലമാക്കിയിരിക്കുന്നു. അത് ലഭ്യമാക്കാന്\u200d  ഡൊമെയിന്\u200d ഡേറ്റാ ഉപയോഗിക്കുന്നതിനുള്ള സിസ്റ്റമുള്ള ഘടകങ്ങള്\u200d മാറ്റുന്നതിനുള്ള വാക്കുകള്\u200d മറ്റേ ഭാഗത്തുള്ള ഈ പത്രത്തില്\u200d WNGT-ല്\u200d FJWU സിസ്റ്റത്തിന്റെ സന്ദേശം കൊടുക്കുന്നു. ഞങ്ങള്\u200d പ്രത്യേകിച്ച് ജര്\u200dമ്മന്\u200d- ഇംഗ്ലീഷ് പരിഭാഷയ് നമ്മുടെ സിസ്റ്റത്തിന്റെ അടിസ്ഥാനത്താണ് പരിശോധിക്കുന്നത്- പരിശോധിയുള്ള ട്രാന്\u200dസ്ഫോര്\u200dമാറ്റര്\u200d മോഡല്\u200d ആദ്യമായ NMT നമ്മുടെ പരീക്ഷണ ഫലങ്ങള്\u200d മുമ്പുള്ള വാക്കുകള്\u200d നല്\u200dകുന്നത് കാണിച്ചുകൊണ്ടിരിക്കുന്നു. കോണ്\u200dട്ടെക്സ്റ്റെക്സ്റ്റ ടെസ്റ്റ് ഡോമെന്\u200d പ്രകൃതിയുടെ പ്രഭാവം ഞങ്ങള്\u200d ഡൊമെന്\u200d മാറ്റുന്നതിന്റെ പ്രഭാവം പഠിച്ചിട്ടുണ്ടായിരുന്നു.', 'mt': 'Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information.  B’mod ġenerali, il-mudelli NMT ibbażati fuq sentenzi huma estiżi biex jaqbdu informazzjoni kuntestwali minn korporazzjonijiet fuq skala kbira ta’ dokumenti li huma diffiċli biex jinkisbu. L-adattament tad-dominju min-naħa l-oħra jippromwovi l-adattament tal-komponenti tas-sistemi diġà żviluppati billi jisfrutta dejta limitata fid-dominju. Dan id-dokument jippreżenta s-sottomissjoni tas-sistema tal-FJWU fid-WNGT, ipparteċipajna speċifikament fil-kompitu MT fil-livell tad-Dokument għat-traduzzjoni Ġermaniża-Ingliż. Is-sistema tagħna hija bbażata fuq mudell Transformer li huwa konxju mill-kuntest żviluppat fuq l-arkitettura oriġinali tal-NMT billi tintegra informazzjoni kuntestwali permezz ta’ netwerks ta’ attenzjoni. Ir-riżultati sperimentali tagħna juru li jipprovdu sentenzi preċedenti bħala kuntest itejjeb b’mod sinifikanti l-punteġġ BLEU meta mqabbel ma’ linja bażi qawwija ta’ NMT. Studjajna wkoll l-impatt ta’ l-adattament tad-dominju fuq it-traduzzjoni fil-livell tad-dokumenti u rnexxielna jtejbu r-riżultati billi tadattaw is-sistemi skond id-dominju ta’ l-ittestjar.', 'no': 'Nyleg har studiar vist at omsetjingskvaliteten av NMT- systemer kan forbetrast ved å gi kontekstinformasjon om dokumentnivå. Generelt setningsbaserte NMT-modeller er utvida for å henta kontekstinformasjon frå stor stor dokumentnivå-korpora som er vanskeleg å henta. Domeneadaptasjon på den andre siden foreslår å tilpassa komponentar av allereie utvikla systemet ved å bruka begrenset i domenedata. Denne papiret viser FJWU s in systemoppføring på WNGT, og vi spesifikke delta i dokumentnivå MT-oppgåve for tysk-engelsk omsetjing. Sistemet vårt er basert på kontekstvare transformeringsmodellen utvikla øvre på originale NMT-arkitektur ved å integrera kontekstinformasjon med oppmerksnettverk. Eksperimentale resultatet våre viser å gjera førre setningar som konteksten betydelig forbetrar BLEU-poeng som sammenlignet med ein sterk NMT-baseline. Vi studierte også effekten av domeneadaptasjonen på dokumentnivået translationand var i stand til å forbetra resultat ved å tilpassa systemet etter testdomenet.', 'mn': 'Саяхан судалгаагаар NMT системийн орчуулалтын чанар баримтын түвшинд орчин үеийн мэдээллийг өгөхөд сайжруулж чадна гэдгийг харуулсан. Ерөнхийдөө өгүүлбэрт суурилсан NMT загвар нь том хэмжээний баримт түвшинд хүргэх хэцүү корпоратаас орчин үеийн мэдээллийг авахад нэмэгдүүлдэг. Нөгөө талаас холбоотой адилтгал нь аль хэдийн хөгжигдсэн системийн компонентүүдийг зохицуулах гэсэн үг. Хязгаарлагдсан мэдээллийг ашиглаж байна. Энэ цаас FJWU-ын системийн WNGT-д зохион байгуулалт гаргадаг. Бид онцгой хэлбэрээр Герман-Англи хэлний хөрөнгө оруулалтын MT даалгаварт оролцсон. Бидний систем нөхцөл ухаантай Трансформатчийн загвар дээр анхны NMT архитектурын дээд хөгжсөн байдаг. Бидний туршилтын үр дүнд өмнөх өгүүлбэрийг харуулж байгааг харуулж байна. БЛЕУ-ын оноо нь хүчирхэг НMT суурь шугамнаас харьцуулахад чухал сайжруулдаг. Бид мөн баримтын түвшинд зохион байгуулалтын нөлөөлөлийг судалж, шалгалтын орон зайд системүүдийг сайжруулж чадна.', 'pl': 'Ostatnie badania wykazały, że jakość tłumaczeń systemów NMT można poprawić poprzez dostarczanie informacji kontekstowych na poziomie dokumentu. Ogólnie rzecz biorąc, modele NMT oparte na zdaniach są rozszerzane o przechwytywanie informacji kontekstowych z dużych korpusów na poziomie dokumentów, które są trudne do zdobycia. Z drugiej strony adaptacja domeny obiecuje adaptację komponentów już opracowanych systemów poprzez wykorzystanie ograniczonych danych wewnątrz domeny. W niniejszym artykule przedstawiono zgłoszenie systemu FJWU w WNGT, w szczególności uczestniczyliśmy w zadaniu MT na poziomie dokumentu dla tłumaczenia niemiecko-angielskiego. Nasz system oparty jest na kontekstowym modelu Transformera opracowanym na oryginalnej architekturze NMT poprzez integrację informacji kontekstowych za pomocą sieci uwagi. Nasze wyniki eksperymentalne pokazują, że podanie poprzednich zdań jako kontekstu znacząco poprawia wynik BLEU w porównaniu do silnej bazy NMT. Badaliśmy również wpływ adaptacji domeny na tłumaczenie dokumentów i udało nam się poprawić wyniki poprzez dostosowanie systemów do domeny testowej.', 'ro': 'Studii recente au arătat că calitatea traducerii sistemelor NMT poate fi îmbunătățită prin furnizarea de informații contextuale la nivel de document. În general, modelele NMT bazate pe propoziții sunt extinse pentru a capta informații contextuale de la corporații la nivel de documente la scară largă, care sunt dificil de dobândit. Adaptarea domeniului promite, pe de altă parte, adaptarea componentelor sistemelor deja dezvoltate prin exploatarea datelor interne limitate. Această lucrare prezintă depunerea sistemului FJWU la WNGT, am participat în mod specific la sarcina MT la nivel de document pentru traducerea germană-engleză. Sistemul nostru se bazează pe modelul Transformer conștient de context dezvoltat pe partea de sus a arhitecturii originale NMT prin integrarea informațiilor contextuale folosind rețelele de atenție. Rezultatele noastre experimentale arată furnizarea de propoziții anterioare, deoarece contextul îmbunătățește semnificativ scorul BLEU în comparație cu o bază puternică NMT. De asemenea, am studiat impactul adaptării domeniului asupra traducerii la nivel de document și am reușit să îmbunătățim rezultatele adaptând sistemele în funcție de domeniul de testare.', 'sr': 'Nedavno ispitivanje pokazalo je da se kvalitet prevođenja NMT-a može poboljšati pružanjem kontekstnih informacija na nivou dokumenta. Generalni modeli NMT-a na osnovu rečenice su prošireni kako bi uhvatili kontekstualne informacije iz korporacije na velikoj razini dokumenta koji su teško dobiti. Domena adaptacija, s druge strane, obećava prilagodbu komponenta veæ razvijenih sustava koristeći ograničene podatke u domenu. Ovaj papir predstavlja podnošenje FJWU-ovog sistema na WNGT-u, posebno smo sudjelovali u MT zadatku na nivou dokumenta za njemački-engleski prevod. Naš sistem je zasnovan na modelu transformera koji je svestan konteksta razvijen na vrhu originalne arhitekture NMT integracijom kontekstualne informacije koristeći pažnju mreže. Naši eksperimentalni rezultati pokazuju pružanje prethodnih rečenica kao kontekst značajno poboljšava rezultat BLEU u usporedbi sa jakom početnom linijom NMT-a. Takođe smo proučavali uticaj adaptacije domena na nivo translationacije dokumenta, uspeli su poboljšati rezultate prilagođenjem sustava prema domenu testiranja.', 'si': 'අලුත් පරීක්ෂණය පෙන්වන්නේ NMT පද්ධතියේ වාර්ථාව කුළුවත් විශේෂ විශේෂ විශේෂ විශේෂ වි සාමාන්\u200dය වාක්ය සඳහා අධාරිත NMT මොඩේල් විසින් විසින් විසින් විසින් විසින් ලොකු විසින් ලොකු ලැබුණු ලැබුණ අනිත් අත්තරයේ ඩොමේන් සම්පූර්ණය ප්\u200dරශ්නයක් තියෙනවා දැනටමත් විකාශ කරපු පද්ධතියේ සම්පූර්ණය ස මේ පැත්තේ FJWU ගේ පද්ධතිය WNGT වල පිළිගන්න පුළුවන්, අපි විශේෂයෙන් විශේෂයෙන් විශේෂයෙන් ලේඛනය MT වැඩේ ජ අපේ පද්ධතිය පද්ධතිය පරිස්ථාපනය අවස්ථාවක් පරිවර්තනය විසින් ප්\u200dරධාන නිර්මාණකය නිර්මාණය කරලා තියෙන අපේ පරීක්ෂණ ප්\u200dරතිචාර ප්\u200dරතිචාරයක් පෙන්වන්න පුළුවන් වාක්ය ප්\u200dරතිචාරයක් වෙන්න පුළුවන් විදිහට BL අපි පරීක්ෂණ ඩෝමින් වලට පරීක්ෂණය සඳහා පරීක්ෂණයේ පරීක්ෂණයේ පද්ධතියේ පරීක්ෂණය සඳහා පද්ධතියේ ප්\u200dරතිපති', 'sv': 'Nyligen genomförda studier har visat att översättningskvaliteten hos NMT-system kan förbättras genom att tillhandahålla kontextuell information på dokumentnivå. I allmänhet utökas meningsbaserade NMT-modeller för att fånga in kontextuell information från storskaliga dokument-nivå corpora som är svåra att förvärva. Domänanpassning lovar å andra sidan anpassning av komponenter i redan utvecklade system genom att utnyttja begränsade domändata. Denna uppsats presenterar FJWU:s systeminlämning på WNGT, vi deltog specifikt i dokumentnivå MT-uppgift för tysk-engelsk översättning. Vårt system bygger på kontextmedveten Transformer-modell som utvecklats ovanpå den ursprungliga NMT-arkitekturen genom att integrera kontextuell information med hjälp av uppmärksamhetsnätverk. Våra experimentella resultat visar att det finns tidigare meningar som kontext avsevärt förbättrar BLEU-poängen jämfört med en stark NMT-baseline. Vi studerade även hur domänanpassningen påverkar översättningen på dokumentnivå och kunde förbättra resultaten genom att anpassa systemen efter testdomänen.', 'so': 'Waxbarashada soo dhowaaday waxay tuseen in lagu kordhiyo qiimaha turjumista nidaamka NMT waxaa lagu hagi karaa macluumaad ku qoran qoraalka. In general sentence-based NMT models are extended to capture contextual information from large-scale document-level corpora which are difficult to acquire.  Tilmaamaha internetka ee kale waxaa lagu ballanqaadaa in la beddelo alaabta nidaamka horay loo horumariyey si ay u bedeshaan isticmaalka macluumaadka gudaha. Kanu warqaddan wuxuu soo bandhigaa nidaamka FJWU ee WNGT, si gaar ah ayaannu ka qayb galnay shaqada dokumenta MT ee turjumista Jarmalka-Ingiriis. nidaamkayagu wuxuu ku saleysan yahay model ku qoran qoraal-aqoon ah oo ku qoran taariikhda asalka ah ee NMT, si uu u qabsado macluumaadka xilliga ah isticmaalo shabakadaha maandooriyaha. Imtixaankayada waxaa muuqda in lagu siiyo hadallada hore, taas oo ah mid aad u hagaajiya qiimaha BLEU-da, barbardhiga qoriga hoose ee NMT. Waxaannu sidoo kale baranay saamaynta beddelinta deegaanka ku saabsan heerka wargeyska, waxaana awoodnay in lagu beddelo nidaamka si waafaqsan jirrabka.', 'ur': 'اچھے مطالعے دکھائے گئے ہیں کہ NMT سیستموں کی ترجمہ کیفیت سنوار کر دکھائی-سطح کی کنٹکسٹ معلومات کے ذریعہ بہتر کر سکتی ہے. عمومی جماعت بنیادی NMT موڈل میں بڑے سطح سند سطح کرپورے سے کنٹکسٹی معلومات کو پکڑنے کے لئے پھیلایا جاتا ہے جو حاصل کرنے کے لئے مشکل ہے. دوسرے ہاتھوں سے ڈومین ادامه کا وعدہ ہے کہ پہلے سے ڈومین ڈیٹ میں محدود ہونے کے ذریعہ سے ڈومین ڈیٹوں کی اجزاء اضافہ کریں۔ یہ کاغذ WNGT میں FJWU کا سیستم تحویل پیش کرتا ہے، ہم مخصوصا دکھانے کی سطح MT کا تابع جرمن-انگلیسی تحویل کے لئے مشارک کرتے ہیں. ہمارا سیستم کانٹس جانے والی ترنسفور موڈل پر بنیاد ہے جو اصلی NMT معماری کے اوپر پھیلائی گئی ہے، توجه نیٹ ورک کے مطابق کنٹکسٹیل معلومات کو متصل کر رہی ہے. ہمارے آزمائش نتیجے پہلے کلمات کو پیش کرنے کے لئے دکھائے جاتے ہیں جیسے کنٹنسیٹ بلیوس اسکور کو ایک طاقتور NMT بنسٹلین کے مقابلہ میں اضافہ کرتا ہے۔ ہم نے بھی ڈومین اپنا اضافہ سند سطح کے ترجمن پر پڑھا تھا اور اس کے ذریعہ سیستموں کو آزمائش ڈومین کے مطابق اضافہ کرنے کے ذریعہ نتیجے اضافہ کر سکے تھے.', 'ta': 'அண்மையில் படிப்பு பொதுவாக வாக்கு அடிப்படையிலான NMT மாதிரிகள் பெரிய அளவு ஆவண- நிலைக்குறிப்பிலிருந்து தற்போதைய தகவலை பிடிக்க வேண்டும்.  முன்பே உருவாக்கப்பட்ட அமைப்புகளின் பொருள்களை மாற்றுதல் மற்றொரு பக்கத்தில் களம் தரவை பயன்படுத்தினால் தளம் ஒதுக்குதல். @ info Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks.  எங்கள் சோதனையின் முடிவுகள் முந்தைய வாக்கியங்களை வழங்குவதை காட்டுகிறது என்றால் பிலியு மதிப்பை மிகவும் மேம்படுத்து மேலும் நாங்கள் கணினி தளத்தை மாற்றியமைப்பதின் தாக்கத்தை கண்டறிந்தோம் ஆவண மட்டத்தில் மொழிபெயர்ப்பில் மாற்றி முடிவுகளை மே', 'mk': 'Неодамнешните студии покажаа дека квалитетот на преводот на НМТ системите може да се подобри со обезбедување контекстуални информации на ниво на документи. Генерално, моделите на НМТ базирани на реченици се проширени за заземање контекстуални информации од големо ниво на документ кое е тешко да се набави. Од друга страна, адаптацијата на доменот ветува адаптација на компонентите на веќе развиените системи со искористување на ограничени податоци во доменот. Овој весник го претставува системот на FJWU на WNGT, специфично учествувавме во MT задачата на нивото на документот за германско-англиски превод. Нашиот систем се базира на контекстно свесен трансформен модел развиен врз оригиналната архитектура на НМТ со интеграција на контекстни информации користејќи мрежи на внимание. Нашите експериментални резултати покажуваат дека обезбедувањето претходни реченици како контекст значително го подобрува резултатот на БЛЕУ во споредба со силната основа на НМТ. Исто така, го проучувавме влијанието на адаптацијата на доменот врз преведувањето на нивото на документите и успеавме да ги подобриме резултатите со адаптацијата на системите според доменот на тестирање.', 'uz': "Yaqinda ishlatilgan taʼminlovlar koʻrsatilgan, hujjat- darajasi muvaffaqiyatli maʼlumot yordamida tarjima tizimning quality imkoniyatini oshirish mumkin. Name Name Bu hujjat WNGT'da FJWU tizimini koʻrsatiladi. Biz faqat German- Ingliz tilidan tarjima qilish uchun hujjat darajaga MT vazifasiga murojaat qildik. Bizning tizimmiz muhim taʼminlovchi Transformer modeli asl NMT architektsiyagi asl asosida yaratilgan muvaffaqiyatlar tarmoqlari bilan birlashtirish mumkin. Bizning tajriba natijalarimiz oldingi so'zlarni qo'shishni ko'rsatish mumkin, muktadha bir xoog NMT asosiy satrlari kabi BLEU scorini o'zgartiradi. Biz dokument darajasini tahrirlash natijasini o'rgangan va tizimni sinov domen davomida o'zgartirishni o'rganish mumkin.", 'vi': 'Những nghiên cứu gần đây đã cho thấy chất lượng dịch chuyển của hệ thống NMT có thể cải thiện bằng cách cung cấp thông tin ngữ cảnh bằng tài liệu. Thông thường, các mô hình NMT dựa trên bản án được mở rộng để thu thập các thông tin ngữ cảnh từ cơ thể trên diện mạo lớn mà rất khó có thể lấy được. Thay vào đó, việc chỉnh sửa miền hứa hẹn sửa các thành phần của hệ thống đã được phát triển bằng cách khai thác dữ liệu nội bộ. Tờ giấy này trình bày bài đệ hệ thống của FJWU tại WNGT, chúng tôi đã tham gia đặc biệt vào công việc PT cấp Tài liệu cho dịch tiếng Đức-Anh. Hệ thống của chúng tôi dựa trên một mô hình biến hình có nhận diện ngữ cảnh được phát triển trên cấu trúc NMT gốc bằng cách nhập thông tin ngữ cảnh bằng cách dùng mạng tập trung. Các kết quả thử nghiệm của chúng tôi cho thấy cung cấp các câu trước như ngữ cảnh cải thiện đáng kể tỉ số lượng NIST so với một cơ sở không quân NMT mạnh. Chúng tôi cũng nghiên cứu tác động của việc chỉnh sửa miền lên nền văn bản thông dịch và đã có khả năng cải thiện kết quả bằng cách sửa hệ thống theo dạng kiểm tra.', 'bg': 'Последните проучвания показват, че качеството на превода на системите за НМТ може да бъде подобрено чрез предоставяне на контекстуална информация на ниво документ. Като цяло моделите, базирани на изречения, се разширяват, за да улавят контекстуална информация от големи корпуси на ниво документи, които са трудни за придобиване. Адаптацията на домейна от друга страна обещава адаптиране на компонентите на вече разработени системи чрез използване на ограничени данни в домейна. Настоящата статия представя подаването на системата на ФБУ във ВНГТ, ние специално участвахме в задачата за превод на немски-английски език. Нашата система се основава на контекст-съзнателен трансформаторен модел, разработен върху оригиналната архитектура чрез интегриране на контекстуална информация, използвайки мрежи за внимание. Нашите експериментални резултати показват предоставянето на предишни изречения, тъй като контекстът значително подобрява резултата в сравнение със силната база на НМТ. Проучихме и въздействието на адаптацията на домейна върху превода на ниво документи и успяхме да подобрим резултатите чрез адаптиране на системите според домейна на тестване.', 'hr': 'Nedavno ispitivanje pokazalo je da se kvalitet prevoda NMT sustava može poboljšati pružanjem kontekstnih informacija na razini dokumenta. Općenito se šire modeli NMT-a na osnovu rečenice kako bi uhvatili kontekstne informacije iz korporacije na velikoj razini dokumenta koji su teško dobiti. Domena adaptacija, s druge strane, obećava prilagodbu komponenta već razvijenih sustava koristeći ograničene podatke u domenu. Ovaj papir predstavlja podatke FJWU-ovog sustava na WNGT-u, posebno smo učestvovali u MT zadatku na razini dokumenta za njemački-engleski prevod. Naš sustav je temeljen na modelu transformera koji je svjestan konteksta razvijen na vrhu originalne arhitekture NMT-a integriranjem kontekstskih informacija koristeći mreže pažnje. Naši eksperimentalni rezultati pokazuju pružanje prethodnih rečenica kao kontekst značajno poboljšava rezultat BLEU u usporedbi s jakom početnom linijom NMT-a. Također smo proučavali utjecaj adaptacije domena na razinu dokumenta, koji su bili u mogućnosti poboljšati rezultate prilagođenjem sustava prema domenu testiranja.', 'nl': "Recente studies hebben aangetoond dat de vertaalkwaliteit van NMT-systemen verbeterd kan worden door contextuele informatie op documentniveau te verstrekken. Over het algemeen worden op zinnen gebaseerde NMT-modellen uitgebreid om contextuele informatie vast te leggen van grootschalige corpora's op documentniveau die moeilijk te verkrijgen zijn. Domeinaanpassing daarentegen belooft het aanpassen van componenten van reeds ontwikkelde systemen door gebruik te maken van beperkte in-domain data. Deze paper presenteert FJWU's systeem indiening bij WNGT, we hebben specifiek deelgenomen aan Document level MT taak voor Duits-Engels vertaling. Ons systeem is gebaseerd op contextbewust Transformer model ontwikkeld bovenop de originele NMT architectuur door contextuele informatie te integreren met behulp van aandachtsnetwerken. Onze experimentele resultaten tonen aan dat het verstrekken van eerdere zinnen als context de BLEU score aanzienlijk verbetert in vergelijking met een sterke NMT baseline. Ook bestudeerden we de impact van domeinaanpassing op documentniveau en konden we de resultaten verbeteren door de systemen aan te passen aan het testdomein.", 'da': "Nylige undersøgelser har vist, at oversættelseskvaliteten af NMT-systemer kan forbedres ved at levere kontekstuel information på dokumentniveau. Generelt udvides sætningsbaserede NMT-modeller til at indfange kontekstuelle oplysninger fra store dokumentniveauer, som er vanskelige at erhverve. Domænetilpasning lover derimod at tilpasse komponenter i allerede udviklede systemer ved at udnytte begrænsede data inden for domænet. Denne artikel præsenterer FJWU's systemindsendelse hos WNGT, vi deltog specifikt i Dokumentniveau MT opgave for tysk-engelsk oversættelse. Vores system er baseret på kontekstbevidst Transformer model udviklet oven på original NMT arkitektur ved at integrere kontekstuel information ved hjælp af opmærksomhedsnetværk. Vores eksperimentelle resultater viser at give tidligere sætninger, da kontekst signifikant forbedrer BLEU scoren sammenlignet med en stærk NMT baseline. Vi undersøgte også effekten af domænetilpasning på dokumentniveau oversættelse og kunne forbedre resultaterne ved at tilpasse systemerne til testdomænet.", 'de': 'Neuere Studien haben gezeigt, dass die Übersetzungsqualität von NMT-Systemen verbessert werden kann, indem kontextbezogene Informationen auf Dokumentenebene bereitgestellt werden. Im Allgemeinen werden satzbasierte NMT-Modelle erweitert, um kontextbezogene Informationen aus großen Korpora auf Dokumentenebene zu erfassen, die schwer zu erfassen sind. Domänenanpassung verspricht hingegen die Anpassung von Komponenten bereits entwickelter Systeme durch die Nutzung begrenzter In-Domain-Daten. Dieses Papier stellt die Systemeinreichung der FJWU bei WNGT vor, wir haben speziell an der Dokumentenebene MT-Aufgabe für Deutsch-Englisch-Übersetzungen teilgenommen. Unser System basiert auf einem kontextbezogenen Transformer-Modell, das auf der ursprünglichen NMT-Architektur entwickelt wurde, indem kontextbezogene Informationen mithilfe von Aufmerksamkeitsnetzwerken integriert wurden. Unsere experimentellen Ergebnisse zeigen, dass die Bereitstellung vorheriger Sätze als Kontext den BLEU-Score im Vergleich zu einer starken NMT-Baseline signifikant verbessert. Außerdem untersuchten wir den Einfluss der Domänenanpassung auf die Übersetzung auf Dokumentenebene und konnten die Ergebnisse durch Anpassung der Systeme an die Testdomäne verbessern.', 'ko': '최근 연구에 따르면 문서급 상하문 정보를 제공함으로써 NMT 시스템의 번역 질을 향상시킬 수 있다.일반적으로 문장 기반의 NMT 모델은 얻기 어려운 대규모 문서급 자료 라이브러리에서 상하문 정보를 얻기 위해 확장된다.다른 한편, 도메인 어댑터는 유한한 도메인 내 데이터를 이용하여 개발된 시스템의 구성 요소를 어댑터할 것을 약속한다.본고는 FJWU가 WNGT에 제출한 시스템을 소개하고 우리는 독일어-영어 번역의 문서급 기계 번역 임무에 전문적으로 참여했다.우리 시스템은 상하문 감지 변환기 모델을 바탕으로 한다. 이 모델은 원시 NMT 구조를 바탕으로 개발된 것으로 주의 네트워크를 이용하여 상하문 정보를 통합시킨다.우리의 실험 결과는 강력한 NMT 기선에 비해 이전 문장을 위아래 문장으로 삼아 BLEU 점수를 현저히 높였다는 것을 보여준다.우리는 또한 역조정이 문서급 번역에 미치는 영향을 연구했고 테스트역조정 시스템에 따라 결과를 개선할 수 있었다.', 'id': 'Studi baru-baru ini menunjukkan bahwa kualitas terjemahan sistem NMT dapat diperbaiki dengan menyediakan informasi kontekstual tingkat dokumen. Secara umum model NMT berdasarkan kalimat diperluaskan untuk menangkap informasi kontekstual dari skala besar dokumen-level corpora yang sulit diperoleh. Adaptasi domain dari sisi lain berjanji untuk mengadaptasi komponen dari sistem yang sudah dikembangkan dengan mengeksploitasi data dalam domain terbatas. Kertas ini menunjukkan pengiriman sistem FJWU di WNGT, kami secara khusus berpartisipasi dalam tugas MT tingkat Dokumen untuk terjemahan bahasa Jerman-Inggris. Sistem kami berdasarkan model Transformer yang menyadari konteks yang dikembangkan di atas arsitektur NMT asli dengan mengintegrasi informasi kontekstual menggunakan jaringan perhatian. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline.  Kami juga mempelajari dampak adaptasi domain pada penerjemah tingkat dokumen dan mampu memperbaiki hasil dengan mengadaptasi sistem menurut domain tes.', 'fa': 'تحقیقات اخیرا نشان داده\u200cاند که کیفیت ترجمه سیستم NMT می\u200cتواند با دادن اطلاعات متوسط سطح سند بهتر شود. در حالی که مدل NMT بر اساس جمله عمومی برای capture of contextual information from large-scale document-level corporation that are difficult to acquire. تغییرات دامنی از طرف دیگر قول می\u200cدهد که بخش\u200cهای سیستم\u200cهای پیش از این توسعه\u200cشده\u200cاند با استفاده از داده\u200cهای محدود در دامنی محدود باشند. این کاغذ تحویل سیستم FJWU را در WNGT نشان می دهد، ما مخصوصا در سطح سند MT برای ترجمه آلمانی و انگلیسی شرکت کردیم. سیستم ما بر روی مدل تغییر دهنده آگاهی متصل شده است که بر بالای معماری اصلی NMT توسط تغییر اطلاعات متصل با توجه به شبکه های توجه توجه است. نتیجه آزمایشی ما نشان می دهد که جمله های قبلی را به عنوان محیط زیادی در مقایسه با یک خط پایین NMT قوی بهتر می کند. ما همچنین تاثیر تغییرات دومین بر سطح تغییرات سند را مطالعه کردیم و توانستیم نتیجه\u200cها را با توجه به سیستم\u200cهای تغییرات بر اساس دومین آزمایش بهتر کنیم.', 'sw': 'Tafiti za hivi karibuni zimeonyesha kwamba ubora wa tafsiri wa mfumo wa NMT unaweza kuboreshwa kwa kutoa taarifa za kiwango cha nyaraka. Kwa ujumla, mifano ya NMT yenye msingi wa hukumu imeenea kupata taarifa za kisasa kutoka makampuni makubwa ya dokumentari ambayo ni vigumu kupata. Upatikanaji wa intaneti kwa upande mwingine unaahidi kubadilisha vifaa vya mifumo tayari vilivyoendelea kwa kutumia taarifa chache za ndani. Gazeti hili linatoa taarifa za mfumo wa FJWU kwenye WNGT, hasa tulishiriki katika juhudi la MT kwa ajili ya kutafsiri Kijerumani-Kiingereza. Mfumo wetu unategemea muundo wa Transformer unaoeleweka juu ya ujenzi wa asili wa NMT kwa kuunganisha taarifa za kisasa kwa kutumia mitandao ya ufuatiliaji. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline.  Pia tulisoma madhara ya kubadilishwa kwa ndani kwa kiwango cha kutafsiri nyaraka na tuliweza kuboresha matokeo kwa kubadilisha mifumo kwa mujibu wa majaribio.', 'sq': 'Studimet e fundit kanë treguar se cilësia e përkthimit e sistemeve NMT mund të përmirësohet duke ofruar informacion kontekstual në nivel të dokumentit. Në përgjithësi modelet NMT bazuar në fjalë janë zgjeruar për të kapur informacion kontekstual nga korporata në shkallë të madhe dokumentesh që janë të vështira për të fituar. Përpërshtatja e domenit nga ana tjetër premton përshtatjen e komponenteve të sistemeve tashmë të zhvilluar duke shfrytëzuar të dhënat e kufizuara në domeni. Ky dokument paraqet paraqitjen e sistemit të FJWU në WNGT, ne veçanërisht morëm pjesë në detyrën e nivelit të dokumentit MT për përkthimin gjerman-anglez. Sistemi ynë është bazuar në modelin Transformer të ndërgjegjshëm në kontekst të zhvilluar mbi arkitekturën origjinale të NMT duke integruar informacionin kontekstual duke përdorur rrjetet e vëmendjes. Rezultatet tona eksperimentale tregojnë dhënien e fjalëve të mëparshme si konteksti përmirëson ndjeshëm rezultatin BLEU krahasuar me një bazë të fortë NMT. Ne studiuam gjithashtu ndikimin e përshtatjes së domenisë në përkthimin e nivelit të dokumentit dhe ishim në gjendje të përmirësonim rezultatet duke përshtatur sistemet sipas domenisë së testimit.', 'af': "Onlangse studie het vertoon dat vertaling kwaliteit van NMT stelsels kan verbeter word deur die verskaffing van dokumentvlak contextual informasie. In algemeen setgebaseerde NMT-modele word uitgebrei om contextual inligting te vang van groot-skaal dokumentvlak-korpora wat moeilik is om te kry. Domein-aanpassing op die ander hand belowe om komponente van reeds ontwikkelde stelsels te adapteer deur die gebruik van beperkte in-domein-data te gebruik. Hierdie papier stel FJWU se stelsel onderwerp op WNGT, ons het spesifieke gedeel in Dokumentvlak MT taak vir Duits-Engels vertaling. Ons stelsel is gebaseer op konteks-bewyse Transformer model ontwikkel op bo van oorspronklike NMT-arkitektuur deur konteks-inligting te integreer deur aandagnetwerke te gebruik. Ons eksperimentele resultate vertoon voorige setings as konteks betekeurig verbeter die BLEU-poarte soos vergelyk met 'n sterk NMT-basislien. Ons het ook die invloek van domein-aanpassing op dokumentvlak-translationaal ondersoek dat resultate kan verbeter deur die stelsels aanpassing volgens die toets domein.", 'tr': 'Ýakynda öňki araştyrmalar NMT sistemleriniň terjime kwaliteti sened derejesini däpli bolup biljekdigini görkezilýär. Umumy sözleriň tabanly NMT nusgalary uly ölçekli sened derejesinden çykarmak üçin duşuşygy gollaýar. Başga taraftan domena adaptasy eýýädäki gelişmiş sistemleriň hatlaryny domain maglumatynda ulanarak adalatmak söz berýär. Bu kagyz FJWU sisteminiň WNGT-da s üýşikligini görkezýär. Biz diňe-diňe Sened derejesinde MT täbligini Alman-iňlisçe terjime etmäge goşuldyk. Biziň sistemimiz kontekst bilen tanyş transformer nusgasyna daýanýar, özüniň NMT arhitekturyň üstünde düzüldir. Kontekst maglumaty aňlary ulanyp birleşdirmek üçin. Biziň denedik netijelerimiz öňki sözleriň önümlerini saýlamagyny görkezýär we olaryň konteksti BLEU netijesini güýçli NMT baseliniň derejesine görä has gowurar. Biz hem sened seviyesinde domen adaptasyonuň etkisini çözdik. Çözümler sistemlerini testi domenya görä üstlenerek netijeleri geliştirebilirdik.', 'am': 'የአሁኑን ትርጓሜዎች የNMT ስርዓት ጥያቄ የሥልጣን መረጃዎችን በመጠቀም ይችላል፡፡ በቁጥጥር የNMT ሞዴላዎችን ለማግኘት የሚችሉትን የሰነድ-ደረጃ ኮርፖራ ከትልቅ ቁጥጥር የመረጃ መረጃ ለመያዝ ይዘረጋሉ፡፡ የዶሜን አቀማመጥ በተጨማሪው የድምፅ መረጃዎችን በመጠቀም የድምፅ ተቃውሞ ሲስተካከሉ የሚጠቅሙ ተስፋ ሰጥተዋል፡፡ ይህም ፕሮግራም የFJWU ስርዓት በWNGT ጥያቄ ያቀርባል፤ በተለየንም የሰነድ ደረጃን MT ስራ ለጀርመን-እንግሊዘኛ ትርጓሜ አጋራረብን። ስርዓታችን የግንኙነት ታዋቂው የTransformer model በመጀመሪያው NMT የመሠረታዊ አካባቢ ላይ በመቀነስ በጥያቄ መረጃ በመጠቀም በመጠቀም ነው፡፡ ፈተና ፍሬዎቻችን አስቀድሞ የውይይት ቃላት እንዲያሳየው የBLEU score እንደ ጠንካራ NMT መደበኛ መስመር እንዲያበዛው ያሳያል፡፡ We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.', 'hy': 'Վերջերս ուսումնասիրությունները ցույց են տվել, որ NMT համակարգերի թարգմանման որակը կարող է բարելավվել, ապահովելով փաստաթղթի մակարդակի կոնտեքստոնալ տեղեկատվություն: Ընդհանուր առմամբ, նախադասություններով հիմնված NMT մոդելները ընդլայնվում են այնպես, որ հնչեն կոնտեքստոնալ տեղեկատվություն փաստաթղթի մակարդակից, որը դժվար է ձեռք բերել: Մյուս կողմից բնագավառի ադապտացիան խոստանում է արդեն զարգացած համակարգերի բաղադրիչները ադապտացնել՝ օգտագործելով բնագավառի սահմանափակ տվյալներ: Այս աշխատանքը ներկայացնում է FJW-ի համակարգի ներկայացումը ՀՆԳԹ-ում, մենք մասնակցեցինք փաստաթղթի մակարդակի MT խնդիրներին գերմաներեն-անգլերեն թարգմանման համար: Մեր համակարգը հիմնված է կոնտեքստ-գիտակցած տրանֆորմային մոդելի վրա, որը զարգացել է հիմնական NMT ճարտարապետության վրա ինտեգրելով կոնտեքստ ինֆորմացիան ուշադրության ցանցերով: Մեր փորձարկման արդյունքները ցույց են տալիս, որ նախորդ նախադասությունները տրամադրելը որպես կոնտեքստ նշանակալի բարելավում է ԲԼԵՎ-ի գնահատականը, համեմատած ՆՄԹ-ի ուժեղ հի We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.', 'az': "NMT sistemlərinin tercümə keyfiyetini belə göstərdilər ki, dökümət seviyyəsi müxtəlif məlumatları təmin edərək daha yaxşılaşdırıla bilər. Cümlələrə dayanan NMT modelləri böyük ölçülü döküm-seviyyəti korporasından müxtəlif məlumatları almaq üçün genişlənir. Digər tərəfindən domena uyğunlaşdırması çox mükəmməl sistemlərin komponentlərini dəyişdirən domena məlumatlarını istifadə edərək və vəd edir. Bu kağıt FJWU sisteminin WNGT'də təsdiqlənməsini göstərir. Biz məs ələn Alman-İngilizce tercüməsi üçün Döküman seviyesi MT işin ə katıldıq. Bizim sistemimiz kontekst-bilikli Transformer modeli NMT arhitektürünün üstündə təşkil edilmiş, məlumat ağlarını istifadə edərək müxtəlif məlumatları birləşdirir. Müxtəlif sonuçlarımız əvvəlki sözləri təmin edir ki, məlumatlar BLEU nöqtəsini güclü NMT səviyyəsinə salıb çox yaxşılaşdırır. Biz də dökümət seviyyətinin dəyişikliklərinin təsirini təhsil etdik və sistemləri sınama doməsinə uyğunlaşdırmaq üçün sonuçlarını daha yaxşılaşdırmaq bacarırdıq.", 'bn': "সাম্প্রতিক গবেষণা প্রদর্শন করেছে যে নথিপত্র-স্তরের তথ্য প্রদান করার মাধ্যমে NMT সিস্টেমের অনুবাদের মান উন্নতি প সাধারণত বাক্য ভিত্তিক NMT মডেল ব্যাপক বিশাল নথিপত্র-স্তরের তথ্য গ্রহণ করার জন্য প্রসারিত হয়েছে যা পাওয়ার কঠিন। অন্য দিকে ডোমেইন অ্যাডাপেটশন প্রতিশ্রুতি দিয়েছে যে ডোমেইনের তথ্য ব্যবহার করে ইতোমধ্যে উন্নয়নশীল সিস্টেমের অ্যাপা This paper presents FJWU's system submission at WNGT, we specifically participated in Document level MT task for German-English translation.  আমাদের সিস্টেম মনোযোগ নেটওয়ার্ক ব্যবহারের মাধ্যমে প্রকৃত NMT স্থাপত্যের উপর উন্নয়নের উপর ভিত্তিতে রয়েছে। আমাদের পরীক্ষার ফলাফল দেখা যাচ্ছে পূর্ববর্তী শাস্তি প্রদান করা যেহেতু প্রেক্ষিতে বিলুউ স্কোরের তুলনায় এক শক্তিশালী  এছাড়াও আমরা ডোমেইনের প্রভাব গবেষণা করেছি ডকুমেন স্তরের অনুবাদের উপর এবং পরীক্ষামূলক ডোমেইন অনুবাদ অনুবাদের মাধ্যমে সিস্টেমের সাথ", 'bs': 'Nedavno ispitivanje pokazalo je da se kvalitet prevoda NMT-a može poboljšati pružanjem kontekstnih informacija na nivou dokumenta. Uopće se modeli NMT-a na osnovu rečenice šire kako bi uhvatili kontekstualne informacije iz korporacije na velikoj razini dokumenta koji su teško dobiti. Domena adaptacija, s druge strane, obećava prilagodbu komponenta već razvijenih sustava koristeći ograničene podatke u domenu. Ovaj papir predstavlja podnošenje FJWU-ovog sustava na WNGT-u, posebno smo učestvovali u MT zadatku na nivou dokumenta za prevod njemačkog i engleskog jezika. Naš sistem je baziran na modelu transformera svjesnog konteksta razvijenom na vrhu originalne arhitekture NMT integracijom kontekstnih informacija koristeći mreže pažnje. Naši eksperimentalni rezultati pokazuju pružanje prethodnih rečenica kao kontekst značajno poboljšava rezultat BLEU u usporedbi sa jakom početnom linijom NMT-a. Također smo proučavali utjecaj adaptacije domena na nivo translationacije dokumenta, uspjeli su poboljšati rezultate prilagođenjem sustava prema domenu testiranja.', 'ca': "Estudis recents han demostrat que la qualitat de traducció dels sistemes NMT pot millorar proporcionant informació contextual a nivell de documents. En general, els models NMT basats en frases s'estenen a captar informació contextual de corpores a gran escala de documents que són difícils d'adquirir. L'adaptació al domini, d'altra banda, promet adaptar els components de sistemes ja desenvolupats explotant dades limitades en domini. Aquest paper presenta la presentació del sistema de la FJWU a WNGT, i vam participar específicament en la tasca MT de nivell documental per traducció alemanya-anglès. El nostre sistema està basat en un model Transformer conscient del context desenvolupat a sobre de l'arquitectura original del NMT integrant la informació contextual fent servir xarxes d'atenció. Els nostres resultats experimentals mostren que proporcionar frases anteriors com el context millora significativament la puntuació BLEU en comparació amb una forta línia de referència NMT. També vam estudiar l'impacte de l'adaptació del domini en la traducció a nivell documental i vam poder millorar els resultats adaptant els sistemes segons el domini de prova.", 'cs': 'Nedávné studie ukázaly, že kvalitu překladu NMT systémů lze zlepšit poskytováním kontextových informací na úrovni dokumentů. Obecně jsou NMT modely založené na větách rozšířeny tak, aby zachytily kontextové informace z velkých korpusů na úrovni dokumentů, které jsou obtížně získatelné. Adaptace domény naopak slibuje adaptaci komponent již vyvinutých systémů využitím omezených dat v doméně. Tento příspěvek prezentuje systémové předložení FJWU na WNGT, konkrétně jsme se podíleli na úloze MT na úrovni dokumentů pro německo-anglický překlad. Náš systém je založen na kontextovém modelu Transformer vyvinutém na původní NMT architektuře integrací kontextových informací pomocí pozornostních sítí. Naše experimentální výsledky ukazují, že poskytnutí předchozích vět jako kontextu výrazně zlepšuje BLEU skóre ve srovnání se silnou NMT základní linií. Dále jsme studovali vliv adaptace domény na překlad dokumentů a byli schopni zlepšit výsledky adaptací systémů podle testovací domény.', 'et': 'Hiljutised uuringud on näidanud, et NMT süsteemide tõlkekvaliteeti saab parandada dokumenditasemel kontekstiteabe pakkumisega. Üldiselt laiendatakse lausepõhiseid NMT mudeleid, et koguda kontekstiteavet suuremahulistest dokumenditasemel korpustest, mida on raske omandada. Domeeni kohandamine lubab aga kohandada juba arendatud süsteemide komponente piiratud domeenisiseste andmete kasutamisega. Käesolev töö tutvustab FJWU süsteemi esitamist WNGT, me osalesime konkreetselt dokumendi taseme MT ülesandes saksa-inglise tõlkimiseks. Meie süsteem põhineb kontekstiteadlikul Transformeri mudelil, mis on välja töötatud algse NMT arhitektuuri kõrval, integreerides kontekstiteavet tähelepanuvõrgustike abil. Meie eksperimentaalsed tulemused näitavad eelnevate lausete esitamist, sest kontekst parandab oluliselt BLEU skoori võrreldes tugeva NMT algtasemega. Samuti uurisime domeeni kohandamise mõju dokumenditasemel tõlkimisele ning suutsime parandada tulemusi, kohandades süsteeme vastavalt testimise domeenile.', 'fi': 'Viimeaikaiset tutkimukset ovat osoittaneet, että NMT-järjestelmien käännöslaatua voidaan parantaa tarjoamalla dokumenttitason asiayhteydellistä tietoa. Yleensä lausepohjaisia NMT-malleja laajennetaan ottamaan kontekstitietoa suurista dokumenttitason korpusista, joita on vaikea hankkia. Toisaalta verkkotunnuksen mukauttaminen lupaa jo kehitettyjen järjestelmien osien mukauttamista hyödyntämällä rajallista verkkotunnuksen sisäistä dataa. Tämä artikkeli esittelee FJWU:n järjestelmän toimittamista WNGT:ssä, olemme erityisesti osallistuneet dokumenttitason MT-tehtävään saksa-englanti käännös. Järjestelmämme perustuu kontekstitietoiseen Transformer-malliin, joka on kehitetty alkuperäisen NMT-arkkitehtuurin päälle integroimalla kontekstitietoa huomioverkostojen avulla. Kokeelliset tulokset osoittavat, että aiempien lauseiden antaminen kontekstissa parantaa merkittävästi BLEU-pisteitä verrattuna vahvaan NMT-lähtötilanteeseen. Tutkimme myös toimialueen mukauttamisen vaikutusta dokumenttitason kääntämiseen ja pystyimme parantamaan tuloksia mukauttamalla järjestelmiä testaustoimialueen mukaan.', 'jv': 'text-tool-action General domain modification on the second hand umuses Adjusting komputers of before expanded sistems by expanding limits in-domain data. Gambar iki nambah Sistem menehi FjWU nang WNGT, kita ngomong ngerasakno ning video MT kanggo tarjamahan German-englisi. Sistem awak dhéwé digawesi sistem sing katêpakan karo context-awak Transformer model sing ditambah dumadhi supra architecture NMT sing digawe informasi contextual gambar aturan tambah. Rejalaké sing paling-perintasi puntuan nyenggawe sawasar kelas telu nggawe barang langgar tarjamahan kanggo masara batar NMT sing nggawe barang nggawe We awso yated the effect of domain modification on document scale', 'ha': "Wata fitina na da yanzu suka nuna cewa tsarin fassarar na NMT na ƙari da za'a samar da shi da kuma a bãyar da information masu cikin takardar-daraja. Kuma a ƙunsa da misãlai na NMT wanda aka bingeda shi na gefen maganar, dõmin su kãma maɓallin takardar-daraja mai girma, da kuma masu ƙunci da za'a sami. Shiryarwa Domen da ke kan wani na guda, yana yi wa'adi da za'a cika compositi na'urar da aka riga suka buɗe, da kuma ya yi amfani da data masu tsari cikin-Domin. Wannan takardan na bãyar da shirin FJWU's system da WNGT, mun yi shirin da ƙayyade aikin Hukunan Aiki na MT wa translation na Jarman-Ingiriya. @ info: whatsthis Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline.  Haƙĩƙa, Mun karanta matsayin adaptarin Domen a kan fassarar takarda na rubutu kuma an iya iya ƙara matsalar da su sake adadin na'urar da tsarin da aka jarraba.", 'sk': 'Nedavne študije so pokazale, da je kakovost prevajanja sistemov NMT mogoče izboljšati z zagotavljanjem kontekstualnih informacij na ravni dokumenta. Na splošno so modeli NMT, ki temeljijo na stavkih, razširjeni tako, da zajemajo kontekstne informacije iz velikih korpusov na ravni dokumentov, ki jih je težko pridobiti. Po drugi strani pa obljublja prilagoditev komponent že razvitih sistemov z izkoriščanjem omejenih podatkov na področju. Ta prispevek predstavlja oddajo sistema FJWU na WNGT, posebej smo sodelovali pri nalogi MT na ravni dokumentov za nemško-angleško prevajanje. Naš sistem temelji na kontekstnem transformatorskem modelu, ki je bil razvit na vrhu originalne arhitekture NMT z integracijo kontekstnih informacij z uporabo mrež pozornosti. Naši eksperimentalni rezultati kažejo, da se predhodni stavki zagotavljajo, saj kontekst bistveno izboljša rezultat BLEU v primerjavi z močno izhodiščno vrednostjo NMT. Preučili smo tudi vpliv prilagajanja domenskih področij na prevajanje na ravni dokumentov in izboljšali rezultate s prilagajanjem sistemov glede na domeno testiranja.', 'he': 'Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information.  בדרך כלל מודלים NMT מבוססים על משפטים מתרחשים כדי לתפוס מידע קונטוקטואלי מרמה מסמכים גדולה שקשה להשיג. מצד שני, ההסתגלות למשטרה מבטיחה להסתגל חלקים של מערכות כבר מפותחות על ידי ניצול נתונים מוגבלים בתחום. העיתון הזה מציג את ההעברה של FJWU במערכת WNGT, השתתפנו במיוחד במשימה MT רמת מסמכים לתרגום גרמני-אנגלי. המערכת שלנו מבוססת על מודל טרנספורר מודע לקונקסט שפותח על גבי הארכיטקטורה המקורית של NMT על ידי האינטרגציה מידע הקונקסט באמצעות רשתות תשומת לב. תוצאות הניסויים שלנו מראות שסיפקנו משפטים קודמים בתור הקשר משפר באופן משמעותי את נקודת BLEU בהשוואה לבסיס NMT חזק. למדנו גם את ההשפעה של התאמה לתחום על רמת המסמכים והצלחנו לשפר את התוצאות על ידי התאמה של המערכות לפי התחום הבדיקה.', 'bo': 'NMT་མ་ལག་གི་གནས་ཚུལ་གསར་བསྐྲུན་ཡོད་པའི་ལྟ་བུ་ནི་ཡིག་ཆའི་གནས་ཚུལ་རིམ་གནས་ཚུལ་དང་བསྒྲགས་ཡར་རྒྱས་ཡོད་པ སྤྱིར་བཏང་བའི་ཚིག་གྲངས་གཞི་ཡོད་པའི་NMT དཔེ་གཞི་ཚོགས་དེ་ཚོར་ཡུལ Domain adaptation on the other hand promises adapting components of already developed systems by exploiting limited in-domain data. ཤོག ང་ཚོའི་མ་ལག ང་ཚོའི་experimental results་ནི་སྔོན་གྱི་ཚིག་རྟགས་ལ་འགྱུར་བའི་ཚིག་རྟགས་ལ་ཉེན་ཁ་ཡུལ་གྱི་ཚད་ཉེན་ཡོད། We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.'}
