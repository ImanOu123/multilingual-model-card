{'en': 'Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora', 'pt': 'Aprendendo incorporações e alinhamentos de palavras multilíngues contextualizados para idiomas de recursos extremamente baixos usando corpora paralelos', 'es': 'Aprendizaje de incrustaciones y alineaciones de palabras multilingües contextualizadas para idiomas de recursos extremadamente bajos mediante corpus paralelos', 'ar': 'تعلم تضمين الكلمات في سياق اللغات والمحاذاة للغات منخفضة الموارد للغاية باستخدام Parallel Corpora', 'fr': "Apprentissage d'intégrations et d'alignements de mots multilingues contextualisés pour des langues à ressources extrêmement faibles à l'aide de corpus parallèles", 'ja': 'パラレルコーラを使用した、非常に低いリソース言語のためのコンテキスト化されたクロスリンガルワード埋め込みとアライメントの学習', 'hi': 'समानांतर कॉर्पोरेट का उपयोग करके अत्यंत कम संसाधन भाषाओं के लिए प्रासंगिक क्रॉस-भाषी वर्ड एम्बेडिंग और संरेखण सीखना', 'zh': '用并行语料库学极低资源语言语境化跨语言词嵌对齐', 'ru': 'Изучение контекстуализированных кросс-лингвистических вложений и выравниваний слов для чрезвычайно малоресурсных языков с использованием параллельных корпусов', 'ga': 'Foghlaim Comhthéacsaithe Trasteangacha Focal Leabaithe agus Ailínithe do Theangacha Fíor-Ísealcmhainne Úsáid Corpora Comhthreomhar', 'hu': 'Kontextualizált, többnyelvű szóbeágyazások és igazítások a rendkívül alacsony erőforrású nyelvek számára Parallel Corpora használatával', 'el': 'Διάγλωσσες ενσωμάτωσης λέξεων και ευθυγράμμισης για γλώσσες με εξαιρετικά χαμηλούς πόρους χρησιμοποιώντας παράλληλο σώμα', 'ka': 'Name', 'kk': 'Төмен төмен ресурс тілдері үшін контекстуализацияланған көп тілдер ендіру және түрлендіру', 'it': 'Incorporazioni e allineamenti di parole cross-linguali contestualizzati per lingue a bassissimo contenuto di risorse utilizzando Parallel Corpora', 'lt': 'Mokymasis kontekstiniu tarpkalbiniu žodžių įterpimu ir labai mažų išteklių kalbų suderinimu naudojant paralelinę korporą', 'mk': 'Учење на контекстни меѓујазични вградувања на зборови и израмнувања за јазици со многу ниски ресурси користејќи паралелна корпора', 'ms': 'Mempelajari Penjelmaan dan Jajaran Perkataan Selasa-Bahasa Berkonteks untuk Bahasa Sumber Terrendah-Terdahulu Menggunakan Korpora Paralel', 'ml': 'ക്രോസ്- ഭാഷ വാക്കുകള്\u200d എംബെഡിങ്ങുകള്\u200dക്കും ചേര്\u200dന്നുകൊണ്ടിരിക്കുന്ന ഭാഷകള്\u200d', 'mt': 'It-Tagħlim tal-Intrapriżi u l-Allinjamenti tal-Kliem Kontekstwali Translingwi għal Lingwi b’Riżorsi Extremement Baxxi bl-Użu ta’ Korpora Paralela', 'mn': 'Сургууль хэлний олон хэлний хэлний нэгтгэл болон маш бага нөөцийн хэлнүүд ашиглаж', 'pl': 'Uczenie się kontekstualizowanych wielojęzycznych osadzeń i wyrównań słowa dla języków bardzo niskich zasobów przy użyciu równoległego korpusu', 'no': 'Læring av kontekstualiserte krysspråk- tekstinnbygging og justering for ekstremt låg ressursspråk ved bruk av parallelle korpora', 'ro': 'Încorporări și alinieri de cuvinte interlingve contextualizate pentru limbi cu resurse extrem de scăzute utilizând Parallel Corpora', 'so': 'Learning Limited Cross-language Word Embeddings and Alignments for Extremely Low-Resource Languages using Parallel Corpora', 'sv': 'Inlärning av kontextualiserade korspråkiga ordinbäddningar och justeringar för språk med extremt låga resurser med hjälp av Parallel Corpora', 'ta': 'Name', 'ur': 'Name', 'sr': 'Učenje kontekstualiziranog krstojezičkog uključenja riječi i ispravljanja za ekstremno niske jezike resursa koristeći paralelnu korporu', 'si': 'Name', 'vi': 'Học tương ứng chữ thập ngôn ngữ và liên kết cho ngôn ngữ cực thấp sử dụng ngôn ngữ đồng song song', 'uz': 'Name', 'nl': 'Leren Contextualiseerde Cross-lingual Word Embeddings en uitlijningen voor extreem lage resource talen met behulp van parallel corpora', 'hr': 'Učenje kontekstualiziranog krstojezičkog uključenja riječi i ispravljanja za ekstremno niske jezike resursa koristeći paralelnu korporu', 'de': 'Lernen kontextualisierter sprachübergreifender Word-Einbettungen und -Ausrichtungen für extrem ressourcenarme Sprachen mit parallelem Korpora', 'bg': 'Обучение на контекстуализирани междуезични словесни вграждания и подравнявания за езици с изключително ниски ресурси, използващи паралелни корпоратори', 'da': 'Indlæring af kontekstualiserede tværsprogede ordindlejringer og justeringer til ekstremt lav ressource sprog ved hjælp af Parallel Corpora', 'ko': '평행 어료 라이브러리로 극저 자원 언어의 언어 환경화 다중 언어 단어의 삽입과 정렬을 학습하다', 'fa': 'یادگیری کلمات متوسط زبان و تنظیم برای زبانهای زیادی کم منبع استفاده از زبان پارالی', 'tr': 'Kontekstualiziран Çapraz Diller bilen Ullanyş we çykyşlar', 'id': 'Mempelajari Bentuk Kata Bersaling-Bahasa Berkontekstualisasi dan Penjajaran untuk Bahasa Sumber Sangat Terrendah Menggunakan Korpora Paralel', 'af': 'Onderleer Konteksualiseerde Kruistale Woord Inbetering en Oplyn vir Ekstra Lae Hulpbron Taal gebruik Parallele Korpora', 'hy': 'Օգտագործելով զուգահեռ կորպորա', 'am': 'የኩነቶች ቋንቋዎች ቃላት አቀማመጥ', 'sw': 'Kujifunza maneno ya lugha yenye lugha ya Kushindwa', 'bn': 'প্যারালেল কোর্পোরা ব্যবহার করে বেশীরভাগ কম-সম্পদ ভাষার জন্য বিদ্যমান ক্রস-ভাষা শব্দ বিভিন্ন বিভিন্ন বিভিন্ন ভাষা এবং স্থিতির', 'bs': 'Naučenje kontekstualiziranog krstojezičkog uključenja riječi i ispravljanja za ekstremno niske jezike resursa koristeći paralelnu korporu', 'ca': 'Aprendre integracions i alliniaments de paraules translingües contextuals per llengües extremadament baixos amb recursos utilitzant la corpora parallela', 'sq': 'Mësimi i lidhjeve dhe rregullimeve të fjalëve ndërgjuhësore të kontekstuara për gjuhët me burime ekstremisht të ulëta duke përdorur korporën paralele', 'cs': 'Učení kontextualizované vkládání a zarovnání slov pro jazyky s extrémně nízkými zdroji pomocí paralelního korpusu', 'fi': 'Oppiminen kontekstualisoituja monikielisiä sanaupotuksia ja linjauksia erittäin vähän resursseja käyttäville kielille rinnakkaisella korporalla', 'et': 'Õppimine kontekstualiseeritud keeleüleste sõnade manustamine ja joondus äärmiselt vähese ressursiga keeltele paralleelse korpuse abil', 'az': 'Ən aşağı Kaynaklı Dillər üçün müxtəlif Xərcli Kelimi İfadələri və İfadələri Öyrənmək', 'sk': 'Učenje kontekstualiziranih večjezičnih besednih vdelav in poravnave za jezike z izjemno nizkimi viri z uporabo vzporednih korpusov', 'jv': 'Learn contextual', 'ha': 'KCharselect unicode block name', 'bo': 'སྐད་རིགས་འདིའི་ནང་དུ་ཕན་ཚུན་ཐོག', 'he': 'Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora'}
{'en': 'We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains ', 'pt': 'Propomos uma nova abordagem para o aprendizado de embeddings de palavras multilíngues contextualizados com base em um pequeno corpus paralelo (por exemplo, algumas centenas de pares de frases). Nosso método obtém embeddings de palavras por meio de um modelo codificador-decodificador LSTM que traduz e reconstrói simultaneamente uma sentença de entrada. Por meio do compartilhamento de parâmetros de modelo entre diferentes idiomas, nosso modelo treina em conjunto as incorporações de palavras em um espaço comum de vários idiomas. Também propomos combinar a incorporação de palavras e subpalavras para fazer uso de semelhanças ortográficas em diferentes idiomas. Baseamos nossos experimentos em dados do mundo real de línguas ameaçadas, como Yongning Na, Shipibo-Konibo e Griko. Nossos experimentos em tarefas de indução de léxico bilíngue e alinhamento de palavras mostram que nosso modelo supera os métodos existentes por uma grande margem para a maioria dos pares de idiomas. Esses resultados demonstram que, ao contrário da crença comum, um modelo de tradução codificador-decodificador é benéfico para aprender representações multilíngues, mesmo em condições de recursos extremamente baixos. Além disso, nosso modelo também funciona bem em condições de muitos recursos, alcançando desempenho de última geração em uma tarefa de alinhamento de palavras alemão-inglês.', 'fr': "Nous proposons une nouvelle approche pour l'apprentissage des intégrations de mots multilingues contextualisées sur la base d'un petit corpus parallèle (par exemple quelques centaines de paires de phrases). Notre méthode permet d'obtenir des intégrations de mots via un modèle codeur-décodeur LSTM qui traduit et reconstruit simultanément une phrase d'entrée. En partageant les paramètres du modèle entre différentes langues, notre modèle entraîne conjointement les intégrations de mots dans un espace multilingue commun. Nous proposons également de combiner des intégrations de mots et de sous-mots pour utiliser les similitudes orthographiques entre les différentes langues. Nous basons nos expériences sur des données réelles provenant de langues menacées, à savoir le Yongning Na, le Shipibo-Konibo et le Griko. Nos expériences sur les tâches d'introduction de lexiques bilingues et d'alignement de mots montrent que notre modèle surpasse largement les méthodes existantes pour la plupart des paires de langues. Ces résultats démontrent que, contrairement à la croyance populaire, un modèle de traduction encodeur-décodeur est bénéfique pour l'apprentissage de représentations multilingues même dans des conditions de ressources extrêmement faibles. De plus, notre modèle fonctionne également bien dans des conditions de ressources élevées, obtenant des performances de pointe sur une tâche d'alignement de mots allemand-anglais.", 'ar': 'نقترح نهجًا جديدًا لتعلم الزخارف السياقية للكلمات عبر اللغات بناءً على مجموعة موازية صغيرة (على سبيل المثال بضع مئات من أزواج الجمل). تحصل طريقتنا على تضمين الكلمات عبر نموذج وحدة فك ترميز LSTM الذي يترجم ويعيد بناء جملة إدخال في نفس الوقت. من خلال مشاركة معلمات النموذج بين اللغات المختلفة ، يقوم نموذجنا بشكل مشترك بتدريب كلمة حفلات الزفاف في مساحة مشتركة متعددة اللغات. نقترح أيضًا الجمع بين تضمين الكلمات والكلمات الفرعية للاستفادة من أوجه التشابه الإملائي عبر اللغات المختلفة. نبني تجاربنا على بيانات واقعية من اللغات المهددة بالانقراض ، وهي Yongning Na و Shipibo-Konibo و Griko. تُظهر تجاربنا على تحريض المعجم ثنائي اللغة ومهام محاذاة الكلمات أن نموذجنا يتفوق في الأداء على الأساليب الحالية بهامش كبير لمعظم أزواج اللغات. توضح هذه النتائج ، على عكس الاعتقاد الشائع ، أن نموذج ترجمة وحدة فك التشفير والتشفير مفيد لتعلم التمثيلات عبر اللغات حتى في ظروف الموارد المنخفضة للغاية. علاوة على ذلك ، يعمل نموذجنا أيضًا بشكل جيد في ظروف الموارد العالية ، مما يحقق أداءً متطورًا في مهمة محاذاة الكلمات الألمانية الإنجليزية.', 'es': 'Proponemos un nuevo enfoque para aprender incrustaciones de palabras contextualizadas en varios idiomas basado en un pequeño corpus paralelo (por ejemplo, unos pocos cientos de pares de oraciones). Nuestro método obtiene incrustaciones de palabras a través de un modelo de codificador-decodificador LSTM que traduce y reconstruye simultáneamente una oración de entrada. Al compartir los parámetros del modelo entre diferentes idiomas, nuestro modelo entrena conjuntamente las incrustaciones de palabras en un espacio multilingüe común. También proponemos combinar incrustaciones de palabras y subpalabras para hacer uso de similitudes ortográficas en diferentes idiomas. Basamos nuestros experimentos en datos del mundo real de idiomas en peligro de extinción, a saber, Yongning Na, Shipibo-Konibo y Griko. Nuestros experimentos sobre inducción de léxico bilingüe y tareas de alineación de palabras muestran que nuestro modelo supera a los métodos existentes por un amplio margen para la mayoría de las combinaciones de idiomas. Estos resultados demuestran que, contrariamente a la creencia común, un modelo de traducción codificador-decodificador es beneficioso para aprender representaciones multilingües incluso en condiciones de recursos extremadamente bajos. Además, nuestro modelo también funciona bien en condiciones de altos recursos, logrando un rendimiento de vanguardia en una tarea de alineación de palabras alemán-inglés.', 'zh': '立小并行语料库(如数百句对)语境化跨语言词嵌新法。 吾法因LSTM编码器 - 解码器取词嵌之,当同时译与重建输句。 异言共模参数,同跨语空单词。 又议销单词与子词合,以用异言之正字法相似性。 吾实验基于濒危言之真实世界数,即Yongning Na,Shipibo-KoniboGriko也。 双语词典归单词齐之实验,多言于言,大于术。 此结果表明与普论相反,编码器-解码器转换模型虽资源极低,利于学语。 高资运良,德语 - 英语单词齐先进。', 'hi': 'हम एक छोटे से समानांतर कॉर्पस (जैसे कुछ सौ वाक्य जोड़े) के आधार पर प्रासंगिक क्रॉस-लिंगुअल शब्द एम्बेडिंग सीखने के लिए एक नए दृष्टिकोण का प्रस्ताव करते हैं। हमारी विधि एक LSTM एन्कोडर-डिकोडर मॉडल के माध्यम से शब्द एम्बेडिंग प्राप्त करती है जो एक साथ एक इनपुट वाक्य का अनुवाद और पुनर्निर्माण करती है। विभिन्न भाषाओं के बीच मॉडल मापदंडों को साझा करने के माध्यम से, हमारा मॉडल संयुक्त रूप से एक सामान्य क्रॉस-लिंगुअल स्पेस में एम्बेडिंग शब्द को प्रशिक्षित करता है। हम विभिन्न भाषाओं में ऑर्थोग्राफिक समानताओं का उपयोग करने के लिए शब्द और उप-शब्द एम्बेडिंग को संयोजित करने का भी प्रस्ताव करते हैं। हम लुप्तप्राय भाषाओं से वास्तविक दुनिया के डेटा पर अपने प्रयोगों को आधार बनाते हैं, अर्थात् योंगिंग ना, शिपिबो-कोनिबो और ग्रिको। द्विभाषी शब्दकोश प्रेरण और शब्द संरेखण कार्यों पर हमारे प्रयोगों से पता चलता है कि हमारा मॉडल अधिकांश भाषा जोड़े के लिए एक बड़े मार्जिन से मौजूदा तरीकों को मात देता है। इन परिणामों से पता चलता है कि, आम धारणा के विपरीत, एक एन्कोडर-डिकोडर अनुवाद मॉडल बेहद कम संसाधन स्थितियों में भी क्रॉस-लिंगुअल प्रतिनिधित्व सीखने के लिए फायदेमंद है। इसके अलावा, हमारा मॉडल उच्च-संसाधन स्थितियों पर भी अच्छी तरह से काम करता है, एक जर्मन-अंग्रेजी शब्द-संरेखण कार्य पर अत्याधुनिक प्रदर्शन प्राप्त करता है।', 'ja': '私たちは、小さな並列コーパス（例えば、数百の文のペア）に基づいて、文脈化されたクロスリンガルワード埋め込みを学習するための新しいアプローチを提案します。 私たちの方法は、入力文を同時に翻訳および再構築するLSTMエンコーダデコーダモデルを介してワード埋め込みを取得します。 異なる言語間でモデルパラメータを共有することにより、私たちのモデルは共通のクロスリンガル空間で単語埋め込みを共同でトレーニングします。 また、異なる言語間の正書法の類似性を利用するために、単語とサブワードの埋め込みを組み合わせることも提案しています。 私たちの実験は、永寧語、Shipibo - Konibo、Grikoなどの絶滅危惧言語の現実のデータに基づいています。 私たちのバイリンガル辞書誘導とワードアライメントタスクの実験は、ほとんどの言語ペアで、私たちのモデルが既存の方法を大幅に上回っていることを示しています。 これらの結果は、一般的な信念に反して、エンコーダ-デコーダ翻訳モデルが、極めて低いリソース条件であっても、クロスリンガル表現を学習するのに有益であることを示している。 さらに、当社のモデルは、ドイツ語と英語のワードアライメントタスクで最先端のパフォーマンスを達成する、高いリソース条件でもうまく機能します。', 'ru': 'Мы предлагаем новый подход к изучению контекстуализированных кросс-лингвистических вложений слов на основе небольшого параллельного корпуса (например, нескольких сотен пар предложений). Наш метод получает вложения слов с помощью модели LSTM encoder-decoder, которая одновременно переводит и реконструирует входное предложение. Благодаря обмену параметрами модели между различными языками, наша модель совместно обучает встраивания слов в общем межъязыковом пространстве. Мы также предлагаем объединить вложения слов и подсловов, чтобы использовать орфографическое сходство между различными языками. Мы основываем наши эксперименты на реальных данных из языков, находящихся под угрозой исчезновения, а именно: Yongning Na, Shipibo-Konibo и Griko. Наши эксперименты по индукции двуязычного лексикона и задачам выравнивания слов показывают, что наша модель превосходит существующие методы по большому запасу для большинства языковых пар. Эти результаты демонстрируют, что, вопреки общепринятому мнению, модель перевода кодировщик-декодер полезна для изучения кросс-лингвистических представлений даже в условиях крайне низкого ресурса. Кроме того, наша модель также хорошо работает в условиях высоких ресурсов, достигая самой современной производительности в немецко-английской задаче выравнивания слов.', 'ga': 'Molaimid cur chuige nua chun leabú focal tras-teanga comhthéacsaithe a fhoghlaim bunaithe ar chorpas beag comhthreomhar (m.sh. cúpla céad péire abairtí). Faigheann ár modh leabaithe focal trí mhúnla ionchódóra-díchódóra LSTM a aistríonn agus a athchruthaíonn abairt ionchuir ag an am céanna. Trí pharaiméadair mhúnla a roinnt i measc teangacha éagsúla, déanann ár múnla comhthraenáil ar na leabaithe focal i spás coiteann trasteangach. Tá sé beartaithe againn freisin leabú focal agus fofhocal a chur le chéile chun úsáid a bhaint as cosúlachtaí ortagrafacha trasna teangacha éagsúla. Bunaimid ár dturgnaimh ar shonraí ón bhfíorshaol ó theangacha atá i mbaol, is iad sin Yongning Na, Shipibo-Konibo, agus Griko. Léiríonn ár dturgnaimh ar ionduchtú foclóir dátheangach agus tascanna ailínithe focal go sáraíonn ár samhail na modhanna atá ann cheana féin go mór i gcás fhormhór na bpéirí teanga. Léiríonn na torthaí seo, contrártha le creideamh coitianta, go bhfuil samhail aistriúcháin ionchódóra-dhíchódóra tairbheach chun léirithe trasteangacha a fhoghlaim fiú i gcoinníollacha acmhainní an-íseal. Ina theannta sin, oibríonn ár múnla go maith freisin ar choinníollacha ard-acmhainní, ag baint amach feidhmíocht den scoth ar thasc ailíniú focal Gearmáinis-Béarla.', 'el': 'Προτείνουμε μια νέα προσέγγιση για την εκμάθηση πλαισιωμένων γλωσσικών ενσωμάτωσης λέξεων με βάση ένα μικρό παράλληλο σώμα (π.χ. μερικές εκατοντάδες ζεύγη προτάσεων). Η μέθοδος μας λαμβάνει ενσωμάτωση λέξεων μέσω ενός μοντέλου κωδικοποιητή-αποκωδικοποιητή που ταυτόχρονα μεταφράζει και αναπαριστά μια πρόταση εισόδου. Μέσω της κοινής χρήσης παραμέτρων μοντέλων μεταξύ διαφορετικών γλωσσών, το μοντέλο μας εκπαιδεύει από κοινού τις ενσωματώσεις λέξεων σε έναν κοινό διασταυρούμενο χώρο. Προτείνουμε επίσης να συνδυάσουμε την ενσωμάτωση λέξεων και υπολέξεων για να χρησιμοποιήσουμε ορθογραφικές ομοιότητες σε διαφορετικές γλώσσες. Βασίζουμε τα πειράματά μας σε δεδομένα πραγματικού κόσμου από απειλούμενες γλώσσες, δηλαδή Γιόνγκνινγκ Να, Σίμπιμπο-Κόνιμπο και Γκρίκο. Τα πειράματά μας σχετικά με τη δίγλωσση επαγωγή λεξικού και τις εργασίες ευθυγράμμισης λέξεων δείχνουν ότι το μοντέλο μας ξεπερνά τις υπάρχουσες μεθόδους κατά μεγάλο περιθώριο για τα περισσότερα γλωσσικά ζεύγη. Τα αποτελέσματα αυτά αποδεικνύουν ότι, αντίθετα με τις κοινές πεποιθήσεις, ένα μοντέλο μετάφρασης κωδικοποιητή-αποκωδικοποιητή είναι ευεργετικό για την εκμάθηση γλωσσικών αναπαραστάσεων ακόμη και σε εξαιρετικά χαμηλές συνθήκες πόρων. Επιπλέον, το μοντέλο μας λειτουργεί επίσης καλά σε συνθήκες υψηλής περιεκτικότητας σε πόρους, επιτυγχάνοντας επιδόσεις τελευταίας τεχνολογίας σε μια εργασία ευθυγράμμισης λέξεων γερμανικά-αγγλικά.', 'hu': 'Új megközelítést javasolunk a kontextuális, többnyelvű szóbeágyazások tanulására egy kis párhuzamos korpuszon alapuló (például néhány száz mondatpár). Módszerünk egy LSTM kódoló-dekódoló modell segítségével szóbeágyazást nyer, amely egyidejűleg lefordítja és rekonstruálja a bemeneti mondatot. A modellparaméterek különböző nyelvek közötti megosztásával modellünk közösen tanítja a beágyazásokat egy közös, többnyelvű térbe. Javasoljuk továbbá, hogy kombináljuk a szó- és alszóbeágyazásokat, hogy kihasználjuk a különböző nyelvek ortográfiai hasonlóságait. Kísérleteinket a veszélyeztetett nyelvekből származó valós világi adatokra alapozzuk, nevezetesen Yongning Na, Shipibo-Konibo és Griko. Kétnyelvű lexikon indukciós és szóigazítási feladatokkal végzett kísérleteink azt mutatják, hogy modellünk a legtöbb nyelvpár esetében nagy mértékben felülmúlja a meglévő módszereket. Ezek az eredmények azt mutatják, hogy az általános meggyőződéssel ellentétben egy kódoló-dekódoló fordítási modell előnyös a nyelvek közötti reprezentációk tanulására még rendkívül alacsony erőforrás-igényű körülmények között is. Ezen túlmenően modellünk jól működik nagy erőforrásokkal rendelkező körülmények között is, és korszerű teljesítményt ér el egy német-angol szóigazítási feladatban.', 'it': "Proponiamo un nuovo approccio per l'apprendimento di incorporazioni di parole cross-linguali contestualizzate basate su un piccolo corpus parallelo (ad esempio poche centinaia di coppie di frasi). Il nostro metodo ottiene incorporazioni di parole tramite un modello di encoder-decoder LSTM che traduce e ricostruisce simultaneamente una frase di input. Attraverso la condivisione dei parametri del modello tra diverse lingue, il nostro modello allena congiuntamente la parola embeddings in uno spazio comune cross-lingual. Proponiamo anche di combinare le incorporazioni di parole e sottoparole per fare uso di somiglianze ortografiche in diverse lingue. Basamo i nostri esperimenti su dati del mondo reale provenienti da lingue in via di estinzione, vale a dire Yongning Na, Shipibo-Konibo e Griko. I nostri esperimenti sull'induzione bilingue del lessico e sulle attività di allineamento delle parole mostrano che il nostro modello supera i metodi esistenti con un ampio margine per la maggior parte delle coppie linguistiche. Questi risultati dimostrano che, contrariamente alla credenza comune, un modello di traduzione codificatore-decodificatore è utile per l'apprendimento di rappresentazioni cross-lingual anche in condizioni estremamente basse di risorse. Inoltre, il nostro modello funziona bene anche in condizioni di elevata risorsa, ottenendo prestazioni all'avanguardia in un compito di allineamento delle parole tedesco-inglese.", 'lt': 'Siūlome naują požiūrį į mokymąsi kontekstiniu tarpkalbiniu žodžių įterpimu, grindžiamu mažu lygiagrečiu korpusu (pvz., keliomis šimtomis sakinių poromis). Mūsų metodas gauna žodžių įdėjimą naudojant LSTM kodavimo kodavimo model į, kuris vienu metu verta ir atstato įvedimo sakinį. Mūsų modelis bendrai moko žodžius įterpti į bendrą tarpkalbinę erdvę. Mes taip pat siūlome derinti žodžių ir paražodžių įterpimus, kad būtų naudojami ortografiniai panašumai įvairiose kalbose. We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko.  Mūsų dvikalbių leksikonų indukcijos ir žodžių derinimo užduočių eksperimentai rodo, kad mūsų modelis daugeliui kalbų porų yra didesnis už esamus metodus. Šie rezultatai rodo, kad, priešingai nei bendras įsitikinimas, koduotojo ir koduotojo vertimo model is yra naudingas mokymuisi tarpkalbiniais atstovavimais net ir labai mažomis išteklių sąlygomis. Be to, mūsų modelis taip pat gerai veikia didelių išteklių sąlygomis, siekdamas pažangiausių rezultatų, atliekant vokiečių ir anglų kalbų derinimo užduotį.', 'kk': 'Біз жаңа тәртіпті тілдерді көптеген тілдерді ендіру үшін жаңа тәртібін таңдаймыз (мысалы, бірнеше жүз сұрақ екі) деген кішкентай параллель корпус негізінде (мысалы,  Біздің әдіміміз LSTM кодер- декодер үлгісімен сөзді ендіру үлгісін бірдей аударып, келтірілген сөзді қайта құру үлгісін алады. Басқа тілдер арасындағы үлгі параметрлерін ортақтастыру арқылы, біздің үлгіміз сөздерді біріктіріп, біріктіріп, біріктіріп, біріктіріп, біріктіріп, біріктір Сонымен қатар біз сөздерді және ішкі сөздерді басқа тілдерде ортографикалық ұқсастықтарын қолдану үшін біріктіруге ұсынамыз. Біз өзіміздің тәжірибелерімізді қауіпсіздік тілдерінің шын әлемдегі деректеріне негізделік: Yongning Na, Shipibo-Konibo және Griko. Екі тілі лексикалық индукциясы және сөздерді түзету тапсырмаларындағы тәжірибелеріміз үлгілеріміздің көпшілігіне үлкен шегі арқылы бар әдістерімізді көрсетеді Бұл нәтижелер жалпы сенімдерге қарсы кодерлеу үлгісін аудару үлгісі көп тілді түсініктерді оқытуға мүмкіндік береді. Сонымен қатар, біздің үлгіміз біздің көп ресурс шарттарында да жақсы жұмыс істейді, неміс-ағылшын сөздерді түзету тапсырмасын жеткізу үшін.', 'mk': 'Ние предложуваме нов пристап за учење контекстуални меѓујазични зборови вклучени врз основа на мал паралелен корпус (на пример неколку стотици парови реченици). Нашиот метод добива вложување на зборови преку модел на LSTM кодер-декодер кој истовремено преведува и реконструира вложена реченица. Со споделување на параметрите на моделот меѓу различните јазици, нашиот модел заеднички ги тренира зборовите вградени во заедничкиот крстојазичен простор. Ние, исто така, предложуваме да се комбинираат зборови и подзборови вложувања за да се искористат ортографските сличности во различни јазици. Ние ги базираме нашите експерименти на податоците од реалниот свет од загрозени јазици, имено Јонгнинг На, Шипибо-Конибо и Грико. Нашите експерименти за индукција на двојјазични лексикони и задачи за подредување на зборовите покажуваат дека нашиот модел ги надминува постоечките методи со голема маргина за повеќето јазички парови. Овие резултати покажуваат дека, спротивно на заедничкото верување, моделот на превод на кодер-декодер е корисен за учењето на меѓујазичните претставувања дури и во екстремно ниски ресурси. Покрај тоа, нашиот модел, исто така, функционира добро на условите со високи ресурси, постигнувајќи најсовремена резултат во врска со задачата на германско-англиско обврзување на зборовите.', 'ms': 'We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs).  Kaedah kami memperoleh penyembelihan perkataan melalui model penyekod-penyekod LSTM yang secara bersamaan menerjemahkan dan membina semula kalimat input. Melalui berkongsi parameter model antara bahasa yang berbeza, model kita bersama-sama melatih perkataan penyampaian dalam ruang salib bahasa umum. Kami juga mengusulkan untuk menggabungkan penyambungan perkataan dan subperkataan untuk menggunakan persamaan ortografik melalui bahasa yang berbeza. Kami mendasarkan eksperimen kami pada data dunia nyata dari bahasa yang mengancam, iaitu Yongning Na, Shipibo-Konibo, dan Griko. Eksperimen kami pada induksi leksikon bilingual dan tugas penyesuaian perkataan menunjukkan bahawa model kami melampaui kaedah sedia ada dengan margin besar untuk kebanyakan pasangan bahasa. Hasil-hasil ini menunjukkan bahawa, melawan kepercayaan umum, model terjemahan-pengekod adalah bermanfaat untuk belajar perwakilan saling bahasa walaupun dalam keadaan sumber yang sangat rendah. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.', 'ml': 'ഒരു ചെറിയ പാരാളല്\u200d കോര്\u200dപ്പുസിന്\u200dറെ അടിസ്ഥാനത്തില്\u200d പഠിപ്പിക്കുന്ന വാക്കുകള്\u200d പഠിപ്പിക്കുന്നതിനുള്ള ഒരു പുതിയ മാര്\u200dഗ്ഗമ നമ്മുടെ രീതിയില്\u200d ഒരു LSTM എന്\u200dകോഡെര്\u200d ഡെക്കോഡെര്\u200d മോഡലിലൂടെ വാക്കുകള്\u200d അകത്തേക്ക് വരുന്നുണ്ട്. അത് ഒരു ഇന്\u200dപുട്ട് വാക്ക് പുനര്\u200d വ്യത്യസ്ത ഭാഷകളില്\u200d പങ്കുചേര്\u200dക്കുന്ന മോഡല്\u200d പരാമീറ്ററുകള്\u200d മുഖേന നമ്മുടെ മോഡല്\u200d ഒരു സാധാരണ ക്രിസ്ലിങ്ങ് സ്പെയിനി വ്യത്യസ്ത ഭാഷകളിലുള്ള വാക്കും വാക്കുകളും സബ്\u200cവോര്\u200dഡുകളും കൂട്ടിചേര്\u200dക്കാന്\u200d ഞങ്ങള്\u200d പ്രാര്\u200dത്ഥിക്കുന്നു. നമ്മുടെ യഥാര്\u200dത്ഥ ലോക വിവരങ്ങളില്\u200d നിന്നും അപകടത്തിലുള്ള ഭാഷകളില്\u200d നിന്നും നമ്മുടെ പരീക്ഷണങ്ങള്\u200d അടിസ്ഥാനമാക്കുന്നു. യ രണ്ടു ഭാഷ ലെക്സിക്ഷന്\u200d വിനിമയത്തിന്റെയും വാക്കുകളുടെയും പ്രവര്\u200dത്തനങ്ങളില്\u200d നമ്മുടെ പരീക്ഷണങ്ങള്\u200d കാണിച്ചു തരുന്നു നമ്മുടെ മ ഈ ഫലങ്ങള്\u200d കാണിക്കുന്നു, സാധാരണ വിശ്വാസത്തിന് വിരോധമാണെങ്കില്\u200d, ഒരു കോഡോര്\u200d ഡെക്കോഡേര്\u200d പരിഭാഷ മോഡല്\u200d, ക്രിസ്ലിങ്ക് പ്രതിനിധികള്\u200d പഠ Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.', 'ka': 'ჩვენ შეგიძლიათ ახალი პროგრამა, რომელიც კონტექსტუალურად გავისწავლოთ კრისტუალური სიტყვები, რომელიც მარტივი პარალელური კორპუსზე ბაზიან (მაგალითად, რამდენიმე ჩვენი მეთოდი მიიღება სიტყვების შებრუნება LSTM კოდირების მოდელის გამოყენებით, რომელიც ერთადერთად გარგუნება და გამოყენება შებრუნებული სიტყვები. ჩვენი მოდელური პარამეტრები განსხვავებული ენების შორის გაყოფილი შემთხვევაში ჩვენი მოდელური ერთადერთად გაგრძნობა სიტყვების გაყოფილი სიტყვები ერთადერთი კრისი ენგურ ჩვენ შეგიძლიათ, რომ სიტყვები და სუბსიტყვები დავყენებული სიტყვები სუბსიტყვების გამოყენება ორტოგრაფიკური განსხვავებული ენების განმავლობაში. ჩვენ ექსპერიმენტებით რეალური მსოფლიოს მონაცემებით სხვადასხვა ენებიდან, ანუ Yongning Na, Shipibo-Konibo და Griko. ჩვენი ექსპერიმენტები ორიენგური ლექსიკონის ინდექციის და სიტყვების განსაზღვრების მოქმედების შესახებ, რომ ჩვენი მოდელი უფრო დიდი მარტინზე გავაკეთება მსგავსი ეს წარმოდგენები აჩვენებენ, რომ საერთო წარმოდგენისთვის, კოდირების გამოყენების მოდელედ უფრო საფრთხოველია კრესიენგური გამოსახულებებისთვის. დამატებით, ჩვენი მოდელი კიდევ მუშაობს მაღალი რესურსების შესახებ, რომელიც გერმანული-ანგლისური სიტყვების შესახებ გავაკეთება.', 'mt': 'Aħna nipproponu approċċ ġdid g ħat-tagħlim ta’ inkorporazzjonijiet ta’ kliem translingwi kuntestwalizzati bbażati fuq korpus parallel żgħir (e ż. ftit mijiet ta’ pari ta’ sentenzi). Il-metodu tagħna jikseb inkorporazzjonijiet tal-kliem permezz ta’ mudell ta’ kodifikatur-dekoder LSTM li fl-istess ħin jittraduċi u jirrikostruċi sentenza ta’ input. Permezz tal-kondiviżjoni tal-parametri mudell fost il-lingwi differenti, il-mudell tagħna jitħarreġ b’mod konġunt il-kelma inkorporazzjoni fi spazju translingwi komuni. Aħna qed nipproponu wkoll li nikkombinaw l-inkorporazzjonijiet tal-kelma u tas-subkelma biex nużaw similaritajiet ortografiċi f’lingwi differenti. Aħna nibbażaw l-esperimenti tagħna fuq dejta tad-dinja reali minn lingwi mhedda, jiġifieri Yongning Na, Shipibo-Konibo, u Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs.  Dawn ir-riżultati juru li, kuntrarjament għat-twemmin komuni, mudell ta’ traduzzjoni ta’ kodifikatur-dekoder huwa ta’ benefiċċju għat-tagħlim ta’ rappreżentazzjonijiet translingwi anke f’kundizzjonijiet estremament baxxi ta’ riżorsi. Barra minn hekk, il-mudell tagħna jaħdem tajjeb ukoll fuq kundizzjonijiet ta’ riżorsi għoljin, u jikseb prestazzjoni l-aktar avvanzata fuq kompitu ta’ allinjament tal-kliem Ġermaniż-Ingliż.', 'ro': 'Propunem o nouă abordare pentru învățarea încorporărilor de cuvinte cross-lingvistice contextualizate bazate pe un corpus paralel mic (de exemplu, câteva sute de perechi de propoziții). Metoda noastră obține încorporări de cuvinte prin intermediul unui model de codificator-decodor LSTM care traduce și reconstruiește simultan o propoziție de intrare. Prin partajarea parametrilor modelului între diferite limbi, modelul nostru instruiește împreună încorporarea cuvântului într-un spațiu comun cross-lingvistic. De asemenea, propunem să combinăm încorporările de cuvinte și subcuvinte pentru a utiliza asemănările ortografice în diferite limbi. Ne bazăm experimentele pe date din lumea reală din limbi pe cale de dispariție, și anume Yongning Na, Shipibo-Konibo și Griko. Experimentele noastre privind inducția lexiconului bilingv și sarcinile de aliniere a cuvintelor arată că modelul nostru depășește metodele existente cu o marjă mare pentru majoritatea perechilor de limbi. Aceste rezultate demonstrează că, contrar convingerii comune, un model de traducere codificator-decodor este benefic pentru învățarea reprezentărilor interlingve chiar și în condiții extrem de scăzute de resurse. În plus, modelul nostru funcționează bine și în condiții cu resurse ridicate, obținând performanțe de ultimă generație în cadrul unei sarcini de aliniere a cuvintelor german-engleză.', 'pl': 'Proponujemy nowe podejście do nauki kontekstualizowanych wielojęzycznych osadzeń słów opartych na małym równoległym korpusie (np. kilkaset par zdań). Nasza metoda uzyskuje osadzenia słów za pomocą modelu kodera-dekodera LSTM, który jednocześnie tłumaczy i rekonstruuje zdanie wejściowe. Poprzez udostępnianie parametrów modelu między różnymi językami, nasz model wspólnie szkoli osadzenia słów we wspólnej przestrzeni wielojęzycznej. Proponujemy również połączenie osadzeń słowa i podsłów w celu wykorzystania podobieństw ortograficznych w różnych językach. Nasze eksperymenty opieramy na danych z rzeczywistych języków zagrożonych zagrożeniem, a mianowicie Yongning Na, Shipibo-Konibo i Griko. Nasze eksperymenty dotyczące indukcji dwujęzycznego leksykonu i zadań wyrównywania słów pokazują, że nasz model przewyższa istniejące metody o duży margines dla większości par językowych. Wyniki te pokazują, że wbrew powszechnemu przekonaniu, model tłumaczenia kodera-dekodera jest korzystny dla nauki reprezentacji wielojęzycznych nawet w warunkach bardzo niskiego zasobu. Ponadto nasz model sprawdza się również w warunkach o wysokich zasobach, osiągając najnowocześniejsze osiągnięcia w zadaniu dostosowania słów niemiecko-angielskich.', 'no': 'Vi foreslår ein ny tilnærming for å læra kontekstualisert krysspråk ordinnbygging basert på ein liten parallell korpus (f.eks. noen hundre setningar). Metoden vårt får ordinnbygging via ein LSTM-koderingsmodell som samtidig translaterer og gjenoppretter ein innsetjing. Med å dele modeller mellom ulike språk, treng vårt modell saman ordinnbygging i eit vanleg krysspråksplass. Vi foreslår også å kombinere ord- og underordinnbygging for å bruka ortografiske liknande på ulike språk. Vi baserer eksperimentene våre på verkeleg data frå vanskelege språk, t.d. Yongning Na, Shipibo-Konibo og Griko. Eksperimentane våre på bilinguelt tekstinduksjon og ordjusteringsoppgåver viser at modellen vårt utfører eksisterande metoder med ein stor margin for dei fleste språkparar. Desse resultatene viser at, mot vanlege tror, er eit koderingsmodell nyttig for å lære krysspråk representasjonar sjølv i ekstremt låg ressursforhold. I tillegg arbeider modellen vårt også godt på høg-ressursforhold, og når det gjer tilstand-av-kunsten på eit tysk-engelsk ordjusteringsoppgåve.', 'so': "Waxaan soo jeedaynaa qaab cusub oo barashada hadalka afka kala duwan e e la isku daray oo ku saleysan korpus yar (tusaale ahaan dhawr boqol xilli). Metalkeenu wuxuu heli karaa hadal ku qoran qoraal-cod-coder-LSTM, kaasoo isla marka lagu turjumo oo cusboonaysiiya ereyga input. Tusaale'yadeenu wuxuu si wada jir ah ugu tababarayaa hadalka ku qoran goobta luuqada oo dhan. Sidoo kale waxaan soo jeedaynaa in aan isku daryeelano hadal iyo hadal hoose ah si aan u isticmaalno siman luuqado kala duwan. Imtixaanadeena ayaannu ku qornaa macluumaadka caalamiga ah ee laga helaa luuqadaha halista ah, kuwaas oo ah Yongning Na, Shipibo-Konibo iyo Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs.  Sababtaasi waxay muuqataa in, in ka gees ah rumaysadka caadiga ah, model turjumaadda codcodcodsiga ayaa faa’iido u leh in la barto wakiilada luuqadaha kala duwan xittaa haddii ay ku jirto xaaladaha aad u hoos-hooseeya. Sidoo kale modellkayaga wuxuu si wanaagsan ugu shaqeeyaa shuruudaha sare ee nololeed, wuxuuna gaadhaa shaqo la isbedelo qoraalka afka Jarmalka-Ingiriiska.", 'si': 'අපි අළුත් ප්\u200dරවේශනයක් ඉගෙන ගන්න පුළුවන් විදිහට ප්\u200dරවේශනය කරන්න පුළුවන් භාෂාවික වචනයක් ප්\u200dරවේශනය කරනවා (උදාහරණ අපේ විධානය LSTM සංකේතකය- ඩිකෝඩර් මොඩල් එක්කෙන් පරිවර්තනය කරනවා ඒ වගේම ඇතුළත් වාක්යයක් නිර්මාණය කරනවා. වෙනස් භාෂාවල් අතර වෙනස් භාෂාවල් එක්ක සාමාන්\u200dය භාෂාවල් වචනය සමාන්\u200dය භාෂාවක් වචනය සමාන්\u200dය විදි අපි වෙනස් භාෂාවට වචනය සහ උපවචනය සම්බන්ඩින්ග් එකතු කරන්න ප්\u200dරයෝජනය කරනවා. අපි අපේ පරීක්ෂණය අනතුරු භාෂාවල් වලින් ඇත්ත ලෝකයේ තොරතුරු පරීක්ෂණය අධාරණය කරනවා, ජොන්ගින්නිංග්නිං අපේ පරීක්ෂණය දෙවල් භාෂාවක් ලෙක්සිකෝන් සහ වචන සම්බන්ධ වැඩක් පෙන්වන්නේ අපේ මොඩේල් ඉතින් විදියට බොහොම භාෂ මේ ප්\u200dරතිචාරයක් පෙන්වන්නේ සාමාන්\u200dය විශ්වාසයෙන් විරුද්ධ විශ්වාස කරන්න, සංකේතකය-ඩිකෝඩර් අවශ්\u200dයාත්මක මදුල් තවත්, අපේ මොඩල් හොඳට වැඩ කරන්නේ උත්සත්වයක් තියෙන්නේ, ජර්මාන්-ඉංග්\u200dරීසි වචනය සම්පූර්ණ වැඩක් වෙනුවෙන් ජා', 'sv': 'Vi föreslår ett nytt tillvägagångssätt för att lära sig kontextuella korsspråkliga ordinbäddningar baserade på en liten parallell korpus (t.ex. några hundra meningspar). Vår metod erhåller ordinbäddningar via en LSTM encoder-dekoder modell som samtidigt översätter och rekonstruerar en inmatningssats. Genom att dela modellparametrar mellan olika språk tränar vår modell gemensamt ordet inbäddningar i ett gemensamt tvärspråkligt utrymme. Vi föreslår också att kombinera ord och underord inbäddningar för att utnyttja ortografiska likheter över olika språk. Vi baserar våra experiment på verkliga data från utrotningshotade språk, nämligen Yongning Na, Shipibo-Konibo och Griko. Våra experiment med tvåspråkig lexikoninduktion och ordjustering visar att vår modell överträffar befintliga metoder med stor marginal för de flesta språkpar. Dessa resultat visar att, i motsats till vad som sägs, en encoder-dekoder översättningsmodell är fördelaktig för att lära sig tvärspråkliga representationer även under extremt låga resursförhållanden. Dessutom fungerar vår modell bra även på resurskrävande förhållanden och uppnår toppmodern prestanda på en tysk-engelsk ordjusteringsuppgift.', 'mn': 'Бид бага зэрэг параллел корпус дээр суралцах шаардлагатай олон хэлний үгийг суралцах шинэ арга зам санал өгдөг. Бидний арга нь LSTM кодлогч-декодер загварын аргаар үгийг нэмж авдаг. Энэ нь одоогоор орлуулж, орлуулж өгүүлбэрийг дахин бүтээж байна. Өөр хэл дээр загварын параметрлүүдийг хуваалцах аргаар бидний загвар нь нийтлэг хэлний орон зайд нэмэгдүүлэх үгийг сургаж байна. Мөн бид өөр хэл дээр ортографик тэнцүү байдлыг ашиглах үг болон суб-үг цуглуулах гэсэн санал өгдөг. Бид жинхэнэ ертөнцийн мэдээлэл дээр зориулсан хэл дээр хийсэн туршилтуудыг Yongning Na, Shipibo-Konibo, Griko гэдэг. Бидний хоёр хэл хэлний хэлбэрийн үйлдвэрлэлийн туршилт болон үг хэлбэрийн даалгаврын үйлдвэрлэлүүд бидний загвар ихэнх хэл хоёрын хувьд маш том загвараар үйлдвэрлэхийг харуулдаг. Эдгээр үр дүн нь ерөнхий итгэлтэй эсрэг коддогч хэлний илтгэл суралцах нь маш бага нөхцөл байдлаар ч бага байдаг. Мөн бидний загвар нь Герман-Англи хэлний үгийг дамжуулах ажил дээр өндөр баялаг нөхцөл байдалд сайн ажилладаг.', 'ta': 'நாம் ஒரு சிறிய இணைப்பு கார்புஸ் அடிப்படையில் கற்றுக்கொள்ள ஒரு புதிய முறைமையை பரிந்துரைக்கிறோம். Our method obtains word embeddings via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence.  மாதிரி அளபுருக்களை வேறு மொழிகளில் பகிர்ந்து கொள்ள வழியாக, எங்கள் மாதிரி ஒரு பொதுவான மொழியின் இடைவெளியில் உள்ள சொல நாம் வார்த்தையும் துணை வார்த்தையும் உள்ளடக்கங்களையும் ஒன்று சேர்க்க பரிந்துரைக்கிறோம் வேறு மொழிகளில்  நாங்கள் உண்மையான உலக தரவில் இருந்து நம்முடைய சோதனைகளை அடிப்படையிடுகிறோம், யாங்கிங் நா இரண்டு மொழி லெக்சியன் தொகுப்பு மற்றும் வார்த்தை ஒழுங்குப்படுத்தும் செயல்கள் பற்றி எங்கள் பரிசோதனைகள் பெரும்பாலான மொழி ஜோ இந்த முடிவுகள் பொதுவான நம்பிக்கைக்கும் எதிராக குறியீட்டு மொழிமாற்ற மாதிரி மாதிரியில் மிகவும் குறைந்த மூலத்தின் நிலைமைகளில அதற்கும், எங்கள் மாதிரி அதிக மூலத்தில் நன்றாக வேலை செய்கிறது, ஒரு ஜெர்மன்- ஆங்கிலம் சொல்ல- ஒழுங்குபடுத்தும் செயல்பாட்டில் கலை நி', 'ur': 'ہم نے ایک نوی طریقہ پیش کرتا ہے کہ ایک چھوٹی parallel corpus پر بنیاد رکھی ہوئی مختلف زبان کی زبان کی تعلیم کے لئے ایک نئی طریقہ کی۔ ہمارا طریقہ ایک LSTM Encoder-Decoder موڈل کے ذریعے لفظ ایمبڈینگ لکھتا ہے جو ایک دفعہ کو ترجمہ کرتا ہے اور ایک ایمبٹ جماعت کو دوبارہ بناتا ہے. مختلف زبانوں کے درمیان موڈل پارامیٹر شریک کرنے کے ذریعہ، ہمارا موڈل ایک دوسرے کے ساتھ کلمات کے مطابق کرس زبان کی جگہ میں استعمال کرتا ہے. ہم نے بھی کلمات اور سوبرویڈ امبرڈینگ کو مختلف زبانوں میں استعمال کرنے کے لئے پیشنهاد کرتا ہے۔ ہم نے اپنی آزمائش کو خطرناک زبانوں کی حقیقی دنیا کے ذریعے بنیاد رکھا ہے، یعنی یونگینگ نا، شیپیبو-کونیبو اور گریکو۔ ہماری آزمائش دو زبان کی لکسیون اینڈیکشن اور کلمات الکیٹ ٹاکس کے بارے میں دکھاتی ہے کہ ہماری مدل بہت سے زبان جوڑوں کے لئے ایک بڑی مرز سے موجود طریقے سے کام کرتا ہے۔ یہ نتیجے دکھاتے ہیں کہ عام ایمان کے مقابلہ میں ایک کوڈر-ڈیکوڈر کی ترجمہ موڈل کرسی زبان کی تعلیمات کے لئے فائدہ اٹھاتے ہیں اگرچہ بہت کم منبع شرایط میں بھی۔ اور اس کے علاوہ، ہمارا مدل بھی بہتر کام کرتا ہے، ایک جرمن-انگلیسی کلمات-تعمیر کے کام پر،', 'sr': 'Predlažemo novi pristup učenju kontekstualiziranih međujezičkih reči ugrađenih na osnovu malog paralelnog korpusa (npr. par stotina rečenica). Naša metoda dobija reč uključenja putem LSTM kodera-dekodera koji istovremeno prevodi i rekonstruiše rečenicu za ulazak. Kroz dijeljenje modelnih parametara među različitim jezicima, naš model zajedno obučava reč ugrađenja u zajedničkom krstojezičkom prostoru. Takoðe predlažemo da kombiniramo reèi i podreèi da bi iskoristili ortografske sliènosti na razlièitim jezicima. Mi baziramo eksperimente na podacima stvarnog svijeta iz ugroženih jezika, a to je Yongning Na, Shipibo-Konibo i Griko. Naši eksperimenti o indukciji dvojezičkih leksikona i zadatkima o poravnanju reči pokazuju da naš model iznosi postojeće metode velikom marginom za većinu jezičkih parova. Ovi rezultati pokazuju da, suprotno zajedničkom verovanju, model prevođenja kodera-dekodera je korisno za učenje cross-jezičkih predstavljanja čak i u izuzetno niskim uslovima resursa. Nadalje, naš model takođe dobro radi na uvjetima visokog resursa, postižeći nastup umjetnosti na zadatku za poravnanje riječi na njemačkom i engleskom jeziku.', 'uz': "Biz bir yangi tilni o'rganishni o'rganish uchun yangi usulni o'rganamiz, balki kichkina parametrlar (masalan, bir necha soʻzlar). Bizning usuli, LSTM kodlash modeli orqali bir xil tarjima qiladi va kiritish imkoniyatini qayta ishga tushirish mumkin. Model parametrlarini boshqa tillarda boʻlishish orqali, modelimiz bir necha tillarda bir necha tildagi so'zlarni birlashtirishni o'rganadi. Biz yana boshqa tillarda ortografik tilidan foydalanish uchun so'zlar va tub so'zlarni birlashtirishni talab qilamiz. Biz haqiqiqiy dunyo haqida xavfsiz tillarimizni yongning Nan, Shipibo-Konibo va Griko kabi imtiyozni bajaramiz. Bizning ikkita tillar leksikaning tashkilotlarimiz va so'zning taqlilotlarimizni ko'rsatadi, modelimiz ko'proq tilning ko'pchiligi qo'llari uchun katta harfni bajaradi. Bu natijalar ko'rsatadi, umumiy ishni o'zgartirish modeli ko'paytirish mumkin. Kodkoder tarjima modeli o'rganish imkoniyatini juda juda kichkina manbalar holatida o'rganish uchun foydalanadi. Ko'pchilik, bizning modelimiz yuqori resource holatida yaxshi ishlaydi, yuqori so'z-Ingliz so'zlarini birlashtirish vazifasini bajaradi.", 'vi': 'Chúng tôi đề nghị một phương pháp mới để học ngữ pháp hình thành từ ngữ khác nhau dựa trên một tập thể song song nhỏ (v.d. vài trăm cặp câu). Theo phương pháp của chúng tôi có được sự nhúng từ qua một mô hình mã hóa LSD, có thể đồng thời dịch chuyển và tái tạo một bản nhập. Bằng cách chia sẻ các thông số mô hình cho các ngôn ngữ khác nhau, mẫu của chúng ta sẽ luyện tập các từ ngữ rộng. Chúng tôi cũng muốn kết hợp từ và chữ phụ để sử dụng những điểm tương đồng ở các ngôn ngữ khác nhau. Chúng tôi thí nghiệm dựa trên các dữ liệu trên thế giới thực từ các ngôn ngữ bị đe dọa, là Yonging Na, Shipibe-Konibe, và Griko. Những thí nghiệm về ngôn ngữ ngữ dẫn dẫn dẫn và các công việc định dạng từ cho thấy mẫu của chúng ta hoàn thành các phương pháp tồn tại với một khoảng cách lớn cho các cặp ngôn ngữ. Những kết quả này chứng minh, trái với niềm tin chung, một mô hình dịch mã hóa có lợi cho việc học các biểu hiện ngôn ngữ, ngay cả trong trường hợp cực thấp. Hơn nữa, mô hình của chúng tôi cũng hoạt động tốt trong những điều kiện có nguồn cao, đạt được những thành tựu hiện đại trong một nhiệm vụ chỉnh Từ-Anh-Đức.', 'da': 'Vi foreslår en ny tilgang til at lære kontekstualiserede tværsprogede ordindlejringer baseret på et lille parallelt korpus (f.eks. et par hundrede sætningspar). Vores metode opnår ordindlejringer via en LSTM encoder-dekoder model, der samtidig oversætter og rekonstruerer en input sætning. Ved at dele modelparametre mellem forskellige sprog træner vores model i fællesskab ordet embeddings i et fælles tværsproget rum. Vi foreslår også at kombinere ord og underord indlejringer for at gøre brug af ortografiske ligheder på tværs af forskellige sprog. Vi baserer vores eksperimenter på data fra den virkelige verden fra truede sprog, nemlig Yongning Na, Shipibo-Konibo og Griko. Vores eksperimenter med tosproget lexikoninduktion og ordjusteringsopgaver viser, at vores model overgår eksisterende metoder med stor margin for de fleste sprogpar. Disse resultater viser, at en encoder-dekoder-oversættelsesmodel, i modsætning til almindelig tro, er gavnlig for læring af tværsprogede repræsentationer selv under ekstremt lave ressourceforhold. Desuden fungerer vores model også godt på forhold med høj ressource og opnår state-of-the-art performance på en tysk-engelsk ordjusteringsopgave.', 'nl': 'We stellen een nieuwe aanpak voor voor het leren van contextualiseerde meertalige woordinbeddingen op basis van een klein parallel corpus (bijvoorbeeld een paar honderd zinnenparen). Onze methode verkrijgt woord embeddings via een LSTM encoder-decoder model dat tegelijkertijd een invoerzin vertaalt en reconstrueert. Door modelparameters tussen verschillende talen te delen, traint ons model gezamenlijk de woordinbeddingen in een gemeenschappelijke meertalige ruimte. We stellen ook voor om woord en subwoord embeddings te combineren om gebruik te maken van orthografische overeenkomsten in verschillende talen. We baseren onze experimenten op real-world data van bedreigde talen, namelijk Yongning Na, Shipibo-Konibo en Griko. Onze experimenten met tweetalige lexicon inductie en woorduitlijningstaken tonen aan dat ons model bestaande methoden met een grote marge overtreft voor de meeste taalparen. Deze resultaten tonen aan dat, in tegenstelling tot de algemene overtuiging, een encoder-decoder vertaalmodel gunstig is voor het leren van meertalige representaties, zelfs in extreem lage resources omstandigheden. Bovendien werkt ons model ook goed op hoge resources omstandigheden, waardoor state-of-the-art prestaties worden behaald bij een Duits-Engels woorduitlijningstaak.', 'hr': 'Predlažemo novi pristup učenju kontekstualiziranih međujezičkih riječi ugrađenih na temelju malog paralelnog korpusa (npr. par stotina rečenica). Naša metoda dobija reč ugrađenja putem LSTM kodera-dekodera koji istovremeno prevodi i rekonstruira ulaznu rečenicu. Kroz dijeljenje modelnih parametara među različitim jezicima, naš model zajedno obučava riječ ugrađenja u zajedničkom međujezičkom prostoru. Također predlažemo kombinirati riječi i podriječje za uključenje ortografske sličnosti na različitim jezicima. Na ši eksperimenti baziramo na podacima stvarnog svijeta iz ugroženih jezika, a to je Yongning Na, Shipibo-Konibo i Griko. Naši eksperimenti o indukciji dvojezičkih leksikona i zadatkima o poravnanju riječi pokazuju da naš model iznosi postojeće metode velikom marginom za većinu jezičkih parova. Ovi rezultati pokazuju da, suprotno zajedničkom vjerovanju, model prevođenja kodera-dekodera koristi za učenje cross-jezičkih predstavljanja čak i u ekstremno niskim uvjetima resursa. Osim toga, naš model također dobro radi na uvjetima visokog resursa, postizajući izvršnost države umjetnosti na zadatku za njemačko-englesko poravnanje riječi.', 'de': 'Wir schlagen einen neuen Ansatz für das Erlernen kontextualisierter sprachübergreifender Worteinbettungen vor, der auf einem kleinen parallelen Korpus (z.B. ein paar hundert Satzpaare) basiert. Unsere Methode erhält Worteinbettungen über ein LSTM Encoder-Decoder-Modell, das gleichzeitig einen Eingabesatz übersetzt und rekonstruiert. Durch die gemeinsame Nutzung von Modellparametern zwischen verschiedenen Sprachen trainiert unser Modell gemeinsam die Worteinbettungen in einem gemeinsamen sprachübergreifenden Raum. Wir schlagen auch vor, Wort- und Unterwort-Einbettungen zu kombinieren, um orthographische Ähnlichkeiten zwischen verschiedenen Sprachen zu nutzen. Unsere Experimente basieren auf realen Daten aus gefährdeten Sprachen, nämlich Yongning Na, Shipibo-Konibo und Griko. Unsere Experimente zur zweisprachigen Lexikoninduktion und Wortausrichtung zeigen, dass unser Modell bestehende Methoden bei den meisten Sprachpaaren deutlich übertrifft. Diese Ergebnisse zeigen, dass ein Encoder-Decoder-Übersetzungsmodell entgegen allgemeiner Überzeugung auch unter extrem ressourcenarmen Bedingungen vorteilhaft für das Erlernen von sprachübergreifenden Repräsentationen ist. Darüber hinaus funktioniert unser Modell auch unter ressourcenintensiven Bedingungen gut und erreicht so den neuesten Stand der Technik bei einer deutsch-englischen Wortausrichtungsaufgabe.', 'ko': '우리는 소형 평행 어료 라이브러리(예를 들어 수백 개의 문장 대조)를 바탕으로 하는 언어 환경화 다중 언어 단어를 학습에 삽입하는 새로운 방법을 제시했다.우리의 방법은 LSTM 인코더 - 디코더 모형을 통해 단어 삽입을 얻을 수 있으며, 이 모형은 입력 문장을 동시에 번역하고 재구성합니다.서로 다른 언어 사이에서 모델 파라미터를 공유함으로써 우리의 모델은 하나의 공통된 크로스 언어 공간에서 단어 삽입을 연합하여 훈련한다.우리는 또 서로 다른 언어의 맞춤법 유사성을 이용하기 위해 단어와 하위 단어를 결합시켜 삽입하는 것을 권장한다.우리의 실험은 멸종 위기 언어인 영녕나어, 히피보 코니보어, 그리코어에서 나온 진실한 데이터를 바탕으로 한다.우리가 이중 언어 어휘 귀납과 단어 정렬 임무에 대한 실험에서 대부분의 언어가 옳고 우리의 모델은 기존의 방법보다 매우 큰 우위를 가진다는 것을 알 수 있다.이러한 결과는 사람들이 보편적으로 생각하는 것과 반대로 인코딩-디코딩 번역 모델은 크로스 언어 표현을 배우는 데 매우 낮은 자원 조건에서도 유리하다는 것을 보여준다.그 밖에 우리 모델은 높은 자원 조건에서도 잘 작동하고 독일어-영어 단어 정렬 임무에서 가장 선진적인 성능을 실현했다.', 'sw': 'Tunazipendekeza mbinu mpya ya kujifunza maneno ya lugha yanayohusiana na utaratibu unaoandaliwa kwa kutumia viungo vidogo vinavyofanana (kama vile viwili chache vya hukumu). Utawala wetu unapata maneno yanayotumiwa kupitia mtindo wa kodi wa LSTM ambao kwa wakati huo hutafsiri na kujenga tena hukumu ya kuingia. Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space.  Pia tunapendekeza kuunganisha maneno na maneno ya upinzani ili kutengeneza matumizi ya aina tofauti za kifografia katika lugha mbalimbali. Tunaweza kutumia majaribio yetu kuhusu takwimu halisi za lugha zinazohatarisha, yaani Yongning Na, Shipibo-Konibo na Griko. Majaribio yetu kuhusu uzalishaji wa lexico wa lugha mbili na kazi za upasuaji wa maneno yanaonyesha kuwa mtindo wetu unafanya mbinu zilizopo kwa njia kubwa kwa ajili ya wanaume wengi wa lugha. Matokeo haya yanaonyesha kuwa, kinyume na imani ya kawaida, modeli ya kutafsiri kwa ajili ya kujifunza uwakilishi wa lugha tofauti hata katika mazingira ya rasilimali yenye chini sana. Zaidi ya hayo, mwelekeo wetu pia unafanya kazi vizuri katika mazingira ya rasilimali ya juu, kutekeleza hali ya mambo ya sanaa katika kazi ya kujitengeneza maneno ya Kijerumani na Kiingereza.', 'bg': 'Предлагаме нов подход за изучаване на контекстуализирани междуезични вграждания на думи въз основа на малък паралелен корпус (например няколко стотин двойки изречения). Нашият метод получава вграждане на думи чрез модел кодер-декодер, който едновременно превежда и възстановява входно изречение. Чрез споделяне на параметри на модела между различните езици нашият модел съвместно обучава вграждането на думата в общо междуезично пространство. Предлагаме също да се комбинират вграждания на думи и поддуми, за да се използват ортографски прилики между различните езици. Ние базираме нашите експерименти на реални данни от застрашени езици, а именно Йонгнинг На, Шибибо-Конибо и Грико. Нашите експерименти с двуезични лексиконни индукции и задачи по подравняване на думите показват, че нашият модел надминава съществуващите методи с голяма разлика за повечето езикови двойки. Тези резултати показват, че, противно на общото убеждение, моделът на превод на кодер-декодер е полезен за изучаването на междуезични представяния дори при условия с изключително нисък ресурс. Освен това нашият модел работи добре и при условия с висок ресурс, постигайки най-съвременни резултати при задача за подравняване на думите на немски-английски език.', 'id': 'Kami mengusulkan pendekatan baru untuk belajar kontekstualisasi saling bahasa embedding berdasarkan corpus paralel kecil (contohnya beberapa ratus pasangan kalimat). Metode kami memperoleh pembangunan kata melalui model LSTM encoder-decoder yang secara bersamaan menerjemahkan dan membangun ulang kalimat masukan. Melalui berbagi parameter model antara bahasa yang berbeda, model kita bersama-sama melatih kata embedding dalam ruang salib bahasa umum. Kami juga mengusulkan untuk menggabungkan pembangunan kata dan subkata untuk menggunakan persamaan ortografik di berbagai bahasa. Kami mendasarkan eksperimen kami pada data dunia nyata dari bahasa yang berbahaya, yaitu Yongning Na, Shipibo-Konibo, dan Griko. Eksperimen kami pada induksi leksikon bilingual dan tugas penyesuaian kata menunjukkan bahwa model kami melampaui metode yang ada dengan margin besar untuk kebanyakan pasangan bahasa. Hasil-hasil ini menunjukkan bahwa, bertentangan dengan keyakinan umum, model terjemahan kode-dekoder berguna untuk belajar representation saling bahasa bahkan dalam kondisi sumber daya yang sangat rendah. Selain itu, model kami juga bekerja dengan baik pada kondisi sumber daya tinggi, mencapai prestasi state-of-the-art pada tugas penjajaran kata Jerman-Inggris.', 'af': "Ons voorstel 'n nuwe toegang vir die leer van kontekstualiseerde kruistale woord inbêdings gebaseer op 'n klein parallel e korpus (bv. 'n paar honderd setpaar). Ons metode kry woord inbêdings deur 'n LSTM enkoder- dekoder model wat simultaan vertaling en herkonstrukteer 'n invoer seting. Deur die deel van model parameters tussen verskillende tale, ons model saamgevoer die woord inbêring in 'n gemeenskaplike kruistale spasie. Ons voorstel ook om woord en subwoord inbêding te kombinerer om te gebruik van orthografiese gelykenisse oor verskillende tale. Ons basiseer ons eksperimente op regte wêreld data van gevaardige tale, naamlik Yongning Na, Shipibo-Konibo en Griko. Ons eksperimente op tweedelingse leksikone induksie en woord-aligning-taak vertoon dat ons model uitvoer bestaande metodes deur 'n groot marge vir meeste taal paar. Hierdie resultate wys dat, teenoor gemeenskaplike geloof,  'n enkoder-dekoder-vertalingsmodel is nuttig vir die leer van kruistale voorstellings selfs in ekstrem lae hulpbron voorwaardes. Ons model werk ook goed op hoë-hulpbronne voorwaardes, tot staat-van-kuns-prestasie op 'n Duitse-Engels woord-alignment taak te bereik.", 'tr': 'Öwrenmek üçin täze bir gol teklip berýäris kiçi parallel korpusa daýanýar.Näçe ýüz sözlem çift Biziň yöntemimiz LSTM ködleme-ködleme modeli bilen kelime ködlemeleri alır we bir giriş sözlemi täzeden inşa edir. Farklı diller arasynda nusgala parametralaryny paylaşdyrmak üçin, biziň nusgymyz ýerleşýän kelimelerimizi umumy çat diller arasynda hatda ýerleşýär. Biz hem sözleri we ilat sözlerimizi farklı dillerde ortografyk meňzeşliklerini ulanmak üçin birleştirmek teklif edip otyrýarys. Biz deneylerimizi çykyş dillerden, yanylşyk edilen Yongning Na, Shipibo-Konibo we Griko dillerden çykyp bilýäris. Biziň iki dilli leksiýa indukýa we söz gabdalyk uzmanlarymyzyň örän köp dil çiftleri üçin meýdançalarymyz täze bir gabdalyk bilen çykarýandygyny görkez. Bu netijeler orta ynamynyň tersine, koder-koder nusgasy çykyş dillerini çarpyş-dil täsirlerde hem gaty az resurslar şertlerde öwrenmek üçin bir faydaly däldir. Munuň üçin biziň modelimiz hem ýokary resurslarda gowy işleýär, Alman-iňlisçe söz gabdalyk täblisasynyň ýerine ýetirýär.', 'hy': "Մենք առաջարկում ենք նոր մոտեցում, որպեսզի սովորենք կոնտեքստիալ լեզվով բառերի ներդրումներ' հիմնված փոքր զուգահեռ կորպոսի վրա (օրինակ մի քանի հարյուր նախադասությունների զուգավոր): Մեր մեթոդը ստանում է բառերի ներգրավումներ LSMT-ի կոդեր-կոդեր մոդելի միջոցով, որը միաժամանակ թարգմանում է և վերակառուցնում մուտքային նախադասությունը: Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space.  Մենք նաև առաջարկում ենք միավորել բառեր և ենթաբառեր, որպեսզի տարբեր լեզուներում օգտագործենք օրտոգրաֆիկ նմանությունները: Մենք հիմնում ենք մեր փորձերը վտանգված լեզուների իրական աշխարհի տվյալների վրա, հատկապես Յոնգ Նայի, Շիպիբո-Կոնիբոյի և Գրիկոյի միջոցով: Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs.  Այս արդյունքները ցույց են տալիս, որ, հակառակ ընդհանուր համոզմունքներին, կոդեր-կոդեր թարգմանման մոդելը օգտակար է լեզվի միջև սովորելու համար նույնիսկ շատ ցածր ռեսուրսների պայմաններում: Ավելին, մեր մոդելը նաև լավ է աշխատում բարձր ռեսուրսների պայմանների վրա, հասնելով բարձրագույն արտադրողություններին գերմաներեն-անգլերեն բառերի համապատասխանման խնդրի վրա:", 'fa': 'ما پیشنهاد می\u200cکنیم روش جدید برای یادگیری کلمه\u200cهای متوسط زبان بر اساس یک کورپوس متوسط کوچک (مثلا چند صد جفت جمله). روش ما از طریق یک مدل رمز\u200cدهنده\u200cکننده LSTM استفاده می\u200cکند که همزمان یک جمله ورودی را ترجمه می\u200cکند و بازسازی می\u200cکند. از طریق تقسیم پارامترهای مدل بین زبانهای متفاوت، مدل ما به همراه کلمه\u200cای که در فضای متفاوت زبان\u200cهای مشترک وارد می\u200cشود را آموزش می\u200cدهد. ما همچنین پیشنهاد می\u200cکنیم که کلمه\u200cها و زیر کلمه\u200cها را ترکیب کنیم تا از شبیه\u200cهای orthographic در زبان\u200cهای مختلف استفاده کنیم. ما آزمایشات خود را بر داده های دنیای واقعی از زبانهای خطرناک بنیاد می دهیم، یعنی یونگینگ نا، شیپیبو-کونیبو و گریکو. آزمایش\u200cهای ما در مورد تولید زبان\u200cهای دوزخ و وظیفه\u200cهای تولید کلمه\u200cها نشان می\u200cدهند که مدل ما با یک مرز بزرگ برای بیشتر جفت\u200cهای زبان از روش\u200cهای موجود استفاده می\u200cکند. این نتیجه ها نشان می دهند که بر خلاف اعتقاد مشترک یک مدل ترجمه\u200cدهنده\u200cی کودکور برای یادگیری نمایش\u200cدهنده\u200cهای زیادی زبان حتی در شرایط بسیار کم منابع است. علاوه بر این، مدل ما هم در شرایط زیادی منابع کار می کند، به رسیدن اجرای وضعیت هنری در یک کار تعمیر کلمات آلمان و انگلیسی کار می کند.', 'az': 'Biz küçük paralel korpus üzərində müxtəlif çoxlu dilli sözləri öyrənmək üçün yeni bir yol təklif edirik. Bizim metodumuz LSTM kodlayıcı modeli vasitəsilə sözləri inkişaf edir və bir giriş cümləsini yenidən inşa edir. Müxtəlif dillər arasında modellərin parametrlərini paylaşdırmaq vasitəsilə, modellərimiz birlikdə dillərin çox-çox uzaqlaşdırılmasını tələb edir. Biz də sözləri və ilahi sözləri birləşdirməyi təklif edirik ki, ortografik bənzərini fərqli dillərdə istifadə etmək üçün. Biz təhlükəsizli dillərdən, yani Yongning Na, Shipibo-Konibo və Griko təşviyyələrimizi gerçek dünyanın məlumatlarına dayandırırıq. Bizim iki dil leksikon induksyonu və söz düzəltməsi işlərimizin modellərimizin çox dil çiftləri üçün böyük bir margin ilə mövcuddur. Bu sonuçlar ortaq inanmaların qarşısında, koder-dekoder çevirici modeli çox dil göstəriciləri öyrənmək üçün faydalı olduğunu göstərər. Daha sonra, modellərimiz də yüksək ressurs şartlarında yaxşı işlər edir, Alman-İngilizce söz-düzəltmə işini başa düşər.', 'bn': 'একটি ছোট্ট প্যারালেল কোর্পাসের উপর ভিত্তিতে প্রতিযোগিতার ক্ষেত্রে বিভিন্ন ভাষায় বিভিন্ন বিভিন্ন ভাষায় বিভিন্ন ভাষার কথা শ আমাদের পদ্ধতি একটি LSTM এনকোডার-ডেকোডার মডেলের মাধ্যমে শব্দ প্রবেশ করা যায় যা একই সাথে অনুবাদ করে এবং একটি ইনপুটের বাক্য পুনর্গঠন করে। ভিন্ন ভাষার মাধ্যমে মডেল প্যারামিটার শেয়ার করে আমাদের মডেল একত্রিত করে একটি সাধারণ ক্রস-ভাষায় শব্দের প্রশিক্ষণ দেয়। আমরা একই সাথে প্রস্তাব করছি যে শব্দ এবং সাবওয়ার্ড বিভিন্ন ভাষায় ব্যবহার করার জন্য তৈরি করা যায়। We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko.  আমাদের দুই ভাষা লেক্সিকোর শিল্প এবং শব্দ প্রতিষ্ঠানের কাজের পরীক্ষা দেখাচ্ছে যে আমাদের মডেল বিশাল ভাষার জোড়ার জন্য বিশ এই ফলাফল দেখাচ্ছে যে সাধারণ বিশ্বাসের বিরুদ্ধে একটি এনকোডার-ডেকোডার অনুবাদ মডেল ক্রিভাষাভাষী প্রতিনিধিত্ব শিখতে পারে এমনকি অনেক কম সম্ এছাড়াও, আমাদের মডেল উচ্চ মূল্য পরিস্থিতিতে ভালো কাজ করে, জার্মান-ইংরেজি শব্দ-সামান্য কাজের উপর শিল্পের রাষ্ট্রের প্রদর্শনে', 'bs': 'Predlažemo novi pristup učenju kontekstualiziranih međujezičkih reči ugrađenih na osnovu malog paralelnog korpusa (npr. par stotina rečenica). Naša metoda dobija ugrađenje riječi putem LSTM kodera-dekodera koji istovremeno prevodi i rekonstruira ulaznu rečenicu. Kroz dijeljenje modelnih parametara među različitim jezicima, naš model zajedno obučava riječ ugrađenja u zajedničkom međujezičkom prostoru. Također predlažemo da kombiniramo riječi i podriječje za uključenje korištenja ortografskih sličnosti na različitim jezicima. Mi baziramo eksperimente na podacima stvarnog svijeta iz ugroženih jezika, a to je Yongning Na, Shipibo-Konibo i Griko. Naši eksperimenti o indukciji dvojezičkih leksiona i zadatkima o poravnanju riječi pokazuju da naš model iznosi postojeće metode velikom marginom za većinu jezičkih parova. Ovi rezultati pokazuju da, suprotno zajedničkom vjerovanju, model prevođenja kodera-dekodera je korisno za učenje cross-jezičkih predstavljanja čak i u ekstremno niskim uvjetima resursa. Osim toga, naš model također dobro radi na uvjetima visokog resursa, ostvarivanju izvedbe umjetnosti na njemačkom-engleskom zadatku o poravnanju riječi.', 'ca': "Proposem un nou enfocament per aprendre integracions de paraules translingües contextualitzades basades en un petit cos paral·lel (per exemple unes quantes centenars de parelles de frases). El nostre mètode obté incorporacions de paraules a través d'un model de codificador-decodificador LSTM que tradueix i reconstruit simultàniament una frase d'entrada. Amb compartir paràmetres de model entre les diferents llengües, el nostre model treina conjuntament la paraula incorporació en un espai translingüístic comú. També proposem combinar integracions de paraules i subparaules per utilitzar similituds ortogràfiques a diferents llengües. Basem els nostres experiments en dades del món real de llengües en perill, a saber Yongning Na, Shipibo-Konibo i Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs.  Aquests resultats demostren que, contrariament a la creença comú, un model de traducció codificador-codificador és beneficiós per aprendre representacions translingües fins i tot en condicions extremadament baixes de recursos. A més, el nostre model també funciona bé en condicions d'alta quantitat de recursos, aconseguint un desempeny més avançat en una tasca d'alliniament de paraules alemany-anglès.", 'et': 'Pakume välja uue lähenemisviisi kontekstiseeritud keeleüleste sõnade manustamiseks, mis põhinevad väikesel paralleelsel korpusel (nt mõnesada lausepaari). Meie meetod saab sõna manustamise LSTM kodeerija-dekooderi mudeli kaudu, mis samaaegselt tõlgib ja rekonstrueerib sisendlause. Mudeliparameetrite jagamise kaudu erinevate keelte vahel treenib mudel ühiselt sõna manustamist ühisesse keeleülesesse ruumi. Samuti teeme ettepaneku kombineerida sõna ja alamsõna manustamist, et kasutada ortograafilisi sarnasusi erinevates keeltes. Meie eksperimendid põhinevad reaalsetel andmetel ohustatud keeltest, nimelt Yongning Na, Shipibo-Konibo ja Griko. Meie eksperimendid kakskeelse leksikoni induktsiooni ja sõna joondamise ülesannetega näitavad, et meie mudel on enamiku keelepaaride puhul suurem kui olemasolevad meetodid. Need tulemused näitavad, et vastupidiselt üldisele veendumusele on kodeerija-dekooder tõlkemudel kasulik keeleüleste esitluste õppimisel isegi väga vähese ressursi tingimustes. Lisaks töötab meie mudel hästi ka suurte ressursside tingimustes, saavutades kaasaegse jõudluse saksa-inglise sõna ühtlustamise ülesandel.', 'sq': 'Ne propozojmë një qasje të re për mësimin e përfshirjeve të fjalëve kontekstuale ndërgjuhësore bazuar në një korpus të vogël paralel (për shembull disa qindra çifte fjalësh). Metoda jonë merr përfshirje fjalësh nëpërmjet një modeli LSTM kodues-dekoder që përkthen dhe rindërton njëkohësisht një fjalim të hyrjes. Nëpërmjet ndarjes së parametrave të modelit midis gjuhëve të ndryshme, modeli ynë trajnon së bashku fjalën përfshirje në një hapësirë të përbashkët ndërgjuhësore. Ne propozojmë gjithashtu të kombinojmë përfshirje fjalësh dhe nënfjalësh për të përdorur ngjashmëritë ortografike nëpërmjet gjuhëve të ndryshme. We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko.  Eksperimentet tona mbi induksionin e lexikonëve dygjuhës dhe detyrat e përshtatjes së fjalëve tregojnë se modeli ynë mbizotëron metodat ekzistuese me një margin të madh për shumicën e çifteve gjuhësore. Këto rezultate demonstrojnë se, në kundërshtim me besimin e përbashkët, një model përkthimi kodifikues-dekoder është i dobishëm për mësimin e përfaqësimeve ndërgjuhësore edhe në kushte ekstremisht të ulëta të burimeve. Përveç kësaj, modeli ynë punon gjithashtu mirë në kushtet e burimeve të larta, duke arritur shfaqjen më të lartë në një detyrë gjermano-angleze të përshtatjes së fjalëve.', 'am': 'የቋንቋ ቋንቋ ቃላትን ለመማር አዲስ ሥርዓት እናስባለን፡፡ የግንኙነታችን ቃላትን በመስጠት በመተርጓም እና የድምፅ ውይይትን በመሠረት የሆኑት የLSTM ኮድ-ዴድድ ሞዴል አግኝቷል፡፡ በተለያዩ ቋንቋዎች መካከል የሞዴል parameters በመጋጠም ሞዴሌያችን በተለየ የቋንቋ ቋንቋ ቋንቋ ቦታ ቃላችንን በመጠቀም ያስተምራል፡፡ እናም ቃልን እና ዝርዝር ቃላትን በመቀላቀል በተለያዩ ቋንቋዎች ላይ የፖሮግራፊ ብጤቶች ለመጠቀም እናስጀምር፡፡ የቅርብ ዓለምን ዳታዎችን፣ ኢዮንግን ና፣ ሺፒቦ-ኮንቢባ እና ጋሪኮ የሚሉት የቋንቋዎች መፍትሄታችንን እናሳድጋለን፡፡ በሁለት ቋንቋ ሊክሲን እና ቃሎችን ማቀናቀል ስራዎችን ሞዴሌያችን ለብዙዎቹ ቋንቋዎች ሁለት ዓይነቶች የተለየ የሆኑን ሥርዓት የሚያሳየው ነው፡፡ እነዚህ ፍሬዎች በተቃዋሚ እምነት ቢተቃወሙት የencoder-decoder ትርጉም ሞዴል በክፍለ ዝቅተኛ የቋንቋ መልዕክቶች ማወቅ ጥቅም ነው፡፡ ከዚህም በላይ ሞዴሌያችን ደግሞ ከፍተኛ resource ጉዳዮች ላይ በጀርመን-እንግሊዘኛ ቃላት-ተቃራኒ ስራ ላይ የየልዩ አርእስት ግንኙነትን ለማግኘት ይሠራል፡፡', 'fi': 'Ehdotamme uutta lähestymistapaa kontekstualisoitujen monikielisten sanaupotusten oppimiseen pienen rinnakkaisen korpusen (esim. muutaman sadan lauseparin) pohjalta. Menetelmämme tuottaa sanaupotuksia LSTM-kooderi-dekooderimallin avulla, joka samanaikaisesti kääntää ja rekonstruoi syöttölauseen. Mallimme kouluttaa yhdessä sanaupotuksia yhteiseen monikieliseen tilaan jakamalla malliparametrit eri kielten kesken. Ehdotamme myös, että sana- ja alasanaupotukset yhdistetään käyttämällä ortografisia yhtäläisyyksiä eri kielillä. Kokeet pohjautuvat uhanalaisten kielten eli Yongning Na:n, Shipibo-Konibon ja Grikon todellisiin tietoihin. Kokeemme kaksikielisestä sanaston induktiosta ja sanakohdistustehtävistä osoittavat, että mallimme suoriutuu useimmissa kielipareissa huomattavasti nykyisiä menetelmiä paremmin. Tulokset osoittavat, että toisin kuin yleisesti uskotaan, kooderi-dekooderi-käännösmalli on hyödyllinen monikielisten representaatioiden oppimisessa myös erittäin vähävaraisissa olosuhteissa. Lisäksi mallimme toimii hyvin myös korkean resurssin olosuhteissa ja saavuttaa huipputason suorituskyky saksa-englanti sanalinjauksen tehtävässä.', 'cs': 'Navrhujeme nový přístup k učení kontextualizovaných vícejazyčných vložení slov založený na malém paralelním korpusu (např. několik set větových párů). Naše metoda získává vložení slov prostřednictvím LSTM kodéru-dekodéru modelu, který současně překládá a rekonstruuje vstupní větu. Prostřednictvím sdílení parametrů modelu mezi různými jazyky náš model společně trénuje vkládání slov do společného vícejazyčného prostoru. Navrhujeme také kombinovat vložení slov a podslov pro využití ortografických podobností napříč různými jazyky. Naše experimenty jsou založeny na reálných datech ohrožených jazyků, konkrétně Yongning Na, Shipibo-Konibo a Griko. Naše experimenty na indukci dvojjazyčného lexikonu a úlohách zarovnávání slov ukazují, že náš model výrazně překonává existující metody pro většinu jazykových párů. Tyto výsledky ukazují, že na rozdíl od běžného přesvědčení je překladový model kodéru-dekodéru přínosný pro učení se vícejazyčných reprezentací i v extrémně nízkých prostředcích. Kromě toho náš model dobře funguje i v podmínkách s vysokými zdroji a dosahuje nejmodernějších výkonů při německo-anglickém vyrovnávání slov.', 'sk': 'Predlagamo nov pristop za učenje kontekstualiziranih večjezičnih besednih vdelav, ki temelji na majhnem vzporednem korpusu (npr. nekaj sto parov stavkov). Naša metoda pridobi vgradnjo besed preko modela LSTM kodirnika-dekodirnika, ki hkrati prevaja in rekonstruira vhodni stavek. Z delitvijo parametrov modela med različnimi jeziki naš model skupaj usposablja vgradnjo besed v skupni medjezični prostor. Predlagamo tudi kombinacijo vgradnje besed in podbesed, da bi izkoristili ortografske podobnosti v različnih jezikih. Naše poskuse temeljijo na resničnih podatkih iz ogroženih jezikov, in sicer Yongning Na, Shipibo-Konibo in Griko. Naši eksperimenti z dvojezično indukcijo leksikona in nalogami poravnave besed kažejo, da naš model za večino jezikovnih parov precej presega obstoječe metode. Ti rezultati kažejo, da je v nasprotju s splošnim prepričanjem model prevajanja kodirnika-dekodirnika koristna za učenje medjezičnih reprezentacij tudi v izjemno nizkih razmerah. Poleg tega naš model dobro deluje tudi pri pogojih z visokimi viri in dosega najsodobnejše rezultate pri nemško-angleški nalogi usklajevanja besed.', 'ha': "Tuna goyya da wani hanyoyi na sanar da maganar cikin harshen da aka taɓa da shi a kan karatun kaɗan (misali sauri ɗari). Tsarin Mu na sami maganar da aka shigar da shi a cikin wani misali na LSSM kode-koda wanda ke fassara da shi sami guda, kuma ya canza wani cewa na shiga. Yi yin shirin parameteri na motel a cikin harshen daban-daban, misalinmu yana karanta maganar da aka shigar da shi cikin filin mai ɗabi'a-lugha. Kayya, Munã buɗa su haɗa maganar da sauri da wanda aka shigar da shi dõmin su yi amfani da misalin ortofogi cikin wasu harshe dabam. Tuna baka jarrabayanmu a kan data masu cikin duniya da ke cikin harshen da aka yi haske, kamar Yongning Na, Shipibo-Konibo da Giko. Kayan jarrabayanmu masu cikin aikin mai kwanan leksisi biyu na lugha na nuna cewa misalinmu yana samar da metoden da ke cikin kowanta daraja. Haƙĩƙa, waɗannan matsalar sun nuna cewa, kinyuta da gaskiyar ĩmãni, wata misali na fassarar-kodkode yana da amfani ga learning masu tsari cikin-linguin kuma kõ da cikin muhalli na ƙaranci-resource. Da haka, misalinmu yana aiki mai kyau a kan halin sarki, kuma yana sãmun muhimmin-sanar a kan wani aikin mai saukarwa-maganar-Ingiriya na Jaman-Ingiriya.", 'jv': "Awak dhéwé nggunakake sistem Anyar Awakdhéwé sistem gawe word embedding Ato ngomong sampeyan model Parameter sing wis ana ing limun gak bener, model kita ngupakan jhe Awak dhéwé mulai nggunakake pergambar lan akeh-pergambar lan nggawe ngubah gambar tarjamahan kanggo langa sampeyan ortograf Awak dhéwé éntuk dhéwé éntukno karo data-uwong sing kelangan kanggo nganggo langgar, lan nganggo yong Na, Sipibo-Konibo lan Griko. Where's the reference box Rejilan iki ngono nggambar kuwi, mengko nggawe barang pangan, model sing koder-koder kuwi nggawe barang nggawe tarjamahan karo lengkang bantuan. Label, model kita lagi akeh barang-barang nggawe barang nggawe barang-barang, iso nggawe barang-barang-karang nggawe barang kelas-ingles.", 'he': 'אנו מציעים גישה חדשה ללמוד מילים דרך שפת מקונטוקטואליזת מבוססת על קורפוס מקביל קטן (למשל כמה מאות זוגות משפטים). השיטה שלנו מקבלת תוספות מילים באמצעות מודל קודד-קודד LSTM שמתרגם ובאותו הזמן ומבנה מחדש משפט כניסה. דרך החלק של פרמטרים מודל בין שפות שונות, המודל שלנו מאמן ביחד את המילה "קיצוב" בחלל שפותי משותף. אנחנו גם מציעים לשלב מילים ומילות מתחת כדי להשתמש בדומות אורטוגרפיות בשפות שונות. אנחנו מבססים את הניסויים שלנו על נתונים בעולם האמיתי משפות מסוכנות, כלומר יונגינג Na, שיפיבו-קוניבו, וגריקו. הניסויים שלנו על דליקת לקסיקון שתיים לשונות ומשימות התאמת מילים מראים שהדוגמא שלנו יוצאת מעל שיטות קיימות על ידי גבול גדול לרוב זוגות שפות. התוצאות האלה מראות כי, בניגוד לאמונה משותפת, מודל התרגום קוד-קוד הוא מועיל ללמוד מייצגים דרך שפתיים אפילו בתנאים משאבים נמוכים מאוד. בנוסף, המודל שלנו גם עובד טוב על תנאי משאבים גבוהים, להשיג ביצועים חדשים במשימה של מילים גרמנית-אנגלית.', 'bo': 'ང་ཚོས་སྐད་ཡིག་གཟུགས་ཐད་ཆེ་བའི་ནང་དུ་བསླབས་པའི་ཐབས་ལམ་གསར་བ་ཞིག་བསམ་བློ་གཏོང་། Our method obtains word embeddings via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space. ང་ཚོས་སྐད་ཡིག ང་ཚོས་འཛམ་གླིང་གི་སྐྱེས་པའི་སྐད་རིགས་ལས་ང་ཚོའི་བརྟག་ཞིབ་གསལ་བཤད་ཀྱི་མིང་། ང་ཚོའི་བརྟག་འཕྲུལ་གྱི་སྐད་རིགས་དང་ཐ་སྙད་གྲངས་སྒྲིག་ཀྱི་བྱ་ཚིག་ལས་བརྟན་ན། གྲུབ་འབྲས་འདི་དག་གི་མཐུན་ཡིད་ཆ་ལྡན་དང་བསྟུན་ནས་སྤྱོད་མཁན་གསལ་རྩིས་པ་ཅིག་ནི་ཕན་ཚུན་བསྐྱེད་པའི་སྐད་ཡིག ད་དུང་། ང་ཚོའི་མ་དབུགས་ཀྱིས་རྒྱུ་ཆས་མཐོ་རིམ་པ་ལྟར་བཀོལ་སྤྱོད་ཆེན་ཡོད།'}
{'en': 'Do not neglect related languages : The case of low-resource Occitan cross-lingual word embeddings', 'fr': "Ne négligez pas les langues apparentées\xa0: le cas de l'intégration de mots multilingues occitans à faible ressource", 'ar': 'لا تهمل اللغات ذات الصلة: حالة حفلات الزفاف بالكلمات المتقاطعة من لغة الأوكيتان منخفضة الموارد', 'es': 'No descuides los idiomas relacionados: el caso de la incrustación de palabras en occitano en varios idiomas de bajos recursos', 'pt': 'Não negligencie idiomas relacionados: o caso de incorporação de palavras em vários idiomas occitanos de baixo recurso', 'ja': '関連言語を無視しない：低リソースのオック語のクロスリンガルワード埋め込みの場合', 'zh': '莫忽相关语,资源奥克语嵌', 'hi': 'संबंधित भाषाओं की उपेक्षा न करें: कम-संसाधन Occitan क्रॉस-भाषी शब्द एम्बेडिंग का मामला', 'ru': 'Не пренебрегайте родственными языками: случай малоресурсных вложений окситанских кросс-язычных слов', 'ga': 'Ná déan dearmad ar theangacha gaolmhara: Cás leabaithe focal tras-teanga Occitan ar acmhainní íseal', 'el': 'Μην παραμελείτε τις σχετικές γλώσσες: Η περίπτωση των χαμηλής περιεκτικότητας σε Occitan cross-lingual λέξεις ενσωμάτωσης', 'hu': 'Ne hagyja figyelmen kívül a kapcsolódó nyelveket: Az alacsony erőforrású occitán nyelvű szóbeágyazások esete', 'ka': 'The case of low- resource Occitan cross- lingual word embeddings', 'it': 'Non trascurare le lingue correlate: il caso delle incorporazioni di parole cross-linguali occitane a basso contenuto di risorse', 'lt': 'Nepamirškite susijusių kalbų: nedidelių išteklių Okcitano tarpkalbinių žodžių įterpimo atvejis', 'kk': 'Қосымша тілдерді елемеу: The case of low- resource Occitan cross- lingual word embeddings', 'ms': 'Jangan abaikan bahasa berkaitan: Kejadian penyambungan perkataan saling bahasa Occitan sumber rendah', 'mt': 'Tinjorax il-lingwi relatati: Il-każ ta’ inkorporazzjonijiet ta’ kliem translingwi Okċitani b’riżorsi baxxi', 'mn': 'Харин холбогдолтой хэлнүүдийг алга болгохгүй: The case of low-resource Occitan cross-lingual word embeddings', 'ml': 'ബന്ധപ്പെട്ട ഭാഷകള്\u200d ഉപേക്ഷിക്കരുത്: The case of low-resource Occitan cross-language word embedding', 'mk': 'Не заборавајте на поврзаните јазици: Случајот на нискоресурсни окцитански крстојазични вградувања на зборови', 'ro': 'Nu neglijați limbile conexe: Cazul încorporărilor de cuvinte încrucișate occitane cu resurse reduse', 'sr': 'Ne zanemarijte povezane jezike: slučaj okcitanskih krstojezičkih reči', 'no': 'Ikkje neglecter tilhøyrande språk: The case of low- resource Occitan cross- lingual word embeddings', 'so': 'Ha ka tagin luuqadaha la xiriira: The case of low-resource Occitan word-cross-language embedding', 'sv': 'Försumma inte relaterade språk: Fallet med occitanska korsspråkliga inbäddningar med låg resurs', 'pl': 'Nie zaniedbuj języków pokrewnych: przypadek niskich zasobów okcytańskich osadzeń słów wielojęzycznych', 'si': 'The case of low- source Occitan cross- language word Embdings', 'ur': 'The case of low- resource Occitan cross- lingual word embeddings', 'ta': 'தொடர்புடைய மொழிகளை புறக்கணிக்காதே', 'vi': 'Không được bỏ qua ngôn ngữ liên quan: trường hợp chữ ràng buộc thấp nguồn:', 'uz': 'Bog\xa0Ľliq tillarni e\xa0ľtibor berilmasin: The case of low-resource Occitan cross-language word embedded', 'bg': 'Не пренебрегвайте свързаните езици: Случаят с нискоресурсни окситански междуезични вграждания на думи', 'hr': 'Nemojte zanemariti povezane jezike: slučaj okcitanskih krstojezičkih riječi', 'nl': 'Negeer verwante talen niet: het geval van Occitaans cross-lingual woord embeddings met weinig bronnen', 'da': 'Forsøm ikke relaterede sprog: Tilfældet med occitansk tværsproget ordindlejring med lav ressource', 'de': 'Nicht vernachlässigen verwandte Sprachen: Der Fall von ressourcenarmen okzitanischen crosslingualen Worteinbettungen', 'ko': '관련 언어를 소홀히 하지 마라: 저자원적인 서양 크로스 언어 단어 삽입', 'id': 'Jangan mengabaikan bahasa terkait: Kasus penyampaian kata saling bahasa Occitan dengan sumber daya rendah', 'fa': 'زبانهای ارتباطی را فراموش نکنید: The case of low- resource Occitan cross- lingual word embeddings', 'af': 'Moenie verwante tale vergeet nie: The case of low- resource Occitan cross- lingual word embeddings', 'tr': 'Relativ dilleri red et: The case of low-resource Occitan cross-lingual word embeddings', 'sw': 'Usitupuuze lugha zinazohusiana: Kesi ya rasilimali chini ya lugha ya Occitan imeingia', 'hy': 'Մի անտեսեք այն լեզուները, որոնք կապված են նրանով: Նվագ ռեսուրսներով օքսիտացի խաչլեզվով բառերի ներդրման դեպքում', 'sq': 'Mos harroni gjuhët e lidhura: rasti i përfshirjes së fjalëve ndërgjuhësore të Okcitanit me burime të ulta', 'am': 'The case of low-resource Occitan cross-language word embedded', 'az': 'Xüsusi dilləri unutmayın: The case of low-resource Occitan cross-lingual word embeddings', 'bs': 'Ne zanemarite povezane jezike: The case of low-resource Occitan cross-lingual word embeddings', 'ca': "No ignoreu les llengües relacionades: El cas d'incorporacions de paraules translingües occitanes de baix recursos", 'cs': 'Nezanedbávejte příbuzné jazyky: Případ nízkých zdrojů okcitánského vkládání slov v křížových jazycích', 'bn': 'সংশ্লিষ্ট ভাষাগুলোকে উপেক্ষা করো না: কম সম্পদ অক্সিটিয়ান ক্রস-ভাষার শব্দের মামলা', 'et': 'Ärge unustage seotud keeli: vähese ressursiga oksitaani keeleüleste sõnade manustamise juhtum', 'fi': 'Älä unohda aiheeseen liittyviä kieliä: Vähävaraisten oksitaanien monikielisten sanaupotusten tapaus', 'jv': 'politenessoffpolite"), and when there is a change ("assertivepoliteness', 'ha': 'The case of lower-resource Ocitan interlanguage embedded', 'he': 'אל תשכחי מהשפות הקשורות: The case of low- resource Occitan cross- lingual word embeddings', 'sk': 'Ne zanemarjajte sorodnih jezikov: primer večjezičnih vdelav occitanskih besed z nizkimi viri', 'bo': 'སྐད་རིགས་དང་འབྲེལ་བའི་སྐད་ཡིག་ཚོར་དམིགས་མི་བྱེད།'}
{'en': 'Cross-lingual word embeddings (CLWEs) have proven indispensable for various ', 'ar': 'أثبتت عمليات دمج الكلمات متعددة اللغات (CLWEs) أنها لا غنى عنها للعديد من مهام معالجة اللغة الطبيعية ، على سبيل المثال ، تحريض المعجم ثنائي اللغة (BLI). ومع ذلك ، فإن نقص البيانات غالبًا ما يضعف جودة التمثيلات. تم اقتراح مناهج مختلفة لا تتطلب سوى إشراف ضعيف متعدد اللغات ، ولكن الأساليب الحالية لا تزال تفشل في تعلم CLWEs الجيدة للغات ذات مجموعة صغيرة أحادية اللغة فقط. لذلك ندعي أنه من الضروري استكشاف المزيد من مجموعات البيانات لتحسين CLWEs في الإعدادات منخفضة الموارد. في هذه الورقة نقترح تضمين بيانات اللغات عالية الموارد ذات الصلة. على عكس الأساليب السابقة التي تستفيد من حفلات الزفاف المدربة مسبقًا للغات ، فإننا (1) ندرب CLWEs على الموارد المنخفضة واللغة ذات الصلة بشكل مشترك و (2) نرسمها إلى اللغة المستهدفة لبناء مساحة نهائية متعددة اللغات. نركز في تجاربنا على لغة الأوكيتان ، وهي لغة رومانسية منخفضة الموارد غالبًا ما يتم إهمالها بسبب نقص الموارد. نحن نستفيد من البيانات من الفرنسية والإسبانية والكتالونية للتدريب وتقييم مهمة BLI الإنجليزية. من خلال دمج اللغات الداعمة ، تتفوق طريقتنا على الأساليب السابقة بهامش كبير. علاوة على ذلك ، يُظهر تحليلنا أن درجة الارتباط بين اللغة المدمجة واللغة منخفضة الموارد مهمة للغاية.', 'fr': "Les intégrations de mots multilingues (CLWE) se sont révélées indispensables pour diverses tâches de traitement du langage naturel, par exemple l'induction de lexiques bilingues (BLI). Cependant, le manque de données nuit souvent à la qualité des représentations. Diverses approches ne nécessitant qu'une faible supervision interlinguistique ont été proposées, mais les méthodes actuelles ne permettent toujours pas d'apprendre de bons CLWE pour les langues avec un petit corpus monolingue. Nous affirmons donc qu'il est nécessaire d'explorer d'autres ensembles de données pour améliorer les CLWE dans les installations à faibles ressources. Dans cet article, nous proposons d'intégrer des données sur les langues à ressources élevées associées. Contrairement aux approches précédentes qui tirent parti d'intégrations de langues préformées indépendamment, nous (i) formons conjointement les CLWE pour la langue à faible ressource et une langue associée et (ii) les mappons à la langue cible pour construire l'espace multilingue final. Dans nos expériences, nous nous concentrons sur l'occitan, une langue romane à faibles ressources qui est souvent négligée par manque de ressources. Nous exploitons les données du français, de l'espagnol et du catalan pour la formation et évaluons la tâche BLI occitan-anglais. En intégrant des langages de prise en charge, notre méthode surpasse largement les approches précédentes. De plus, notre analyse montre que le degré de parenté entre une langue incorporée et la langue à faibles ressources est d'une importance cruciale.", 'es': 'La incrustación de palabras en varios idiomas (CLWE) ha demostrado ser indispensable para diversas tareas de procesamiento del lenguaje natural, por ejemplo, la inducción de léxico bilingüe (BLI). Sin embargo, la falta de datos a menudo perjudica la calidad de las representaciones. Se propusieron varios enfoques que solo requerían una supervisión multilingüe débil, pero los métodos actuales aún no logran aprender buenos CLWe para idiomas con solo un pequeño corpus monolingüe. Por lo tanto, afirmamos que es necesario explorar más conjuntos de datos para mejorar los CLWE en configuraciones de bajos recursos. En este artículo proponemos incorporar datos de lenguajes relacionados con altos recursos. A diferencia de los enfoques anteriores que aprovechan la incorporación de idiomas previamente entrenada de forma independiente, (i) capacitamos a los CLWE para el lenguaje de bajos recursos y un idioma relacionado de manera conjunta y (ii) los asignamos al idioma de destino para construir el espacio multilingüe final. En nuestros experimentos nos centramos en el occitano, una lengua románica de pocos recursos que a menudo se descuida debido a la falta de recursos. Aprovechamos los datos del francés, el español y el catalán para capacitarnos y evaluar la tarea de BLI occitano-inglés. Al incorporar idiomas de apoyo, nuestro método supera ampliamente los enfoques anteriores. Además, nuestro análisis muestra que el grado de relación entre un idioma incorporado y el idioma de bajos recursos es de vital importancia.', 'pt': 'Embeddings de palavras em vários idiomas (CLWEs) provaram ser indispensáveis para várias tarefas de processamento de linguagem natural, por exemplo, indução de léxico bilíngue (BLI). No entanto, a falta de dados muitas vezes prejudica a qualidade das representações. Foram propostas várias abordagens que requerem apenas uma fraca supervisão multilíngue, mas os métodos atuais ainda falham em aprender boas CLWEs para idiomas com apenas um pequeno corpus monolíngue. Portanto, afirmamos que é necessário explorar mais conjuntos de dados para melhorar os CLWEs em configurações de poucos recursos. Neste artigo propomos incorporar dados de linguagens de alto recurso relacionadas. Em contraste com as abordagens anteriores que alavancam embeddings de idiomas pré-treinados independentemente, (i) treinamos CLWEs para o idioma de poucos recursos e um idioma relacionado em conjunto e (ii) os mapeamos para o idioma de destino para construir o espaço multilíngue final. Em nossos experimentos, focamos no occitano, uma língua românica de poucos recursos que muitas vezes é negligenciada devido à falta de recursos. Aproveitamos dados de francês, espanhol e catalão para treinamento e avaliação na tarefa Occitan-English BLI. Ao incorporar linguagens de suporte, nosso método supera as abordagens anteriores por uma grande margem. Além disso, nossa análise mostra que o grau de relação entre uma linguagem incorporada e a linguagem de poucos recursos é criticamente importante.', 'ja': 'クロスリンガルワード埋め込み（ ＣＬＷＥ ）は、例えば、バイリンガル辞書誘導（ ＢＬＩ ）などの様々な自然言語処理タスクに不可欠であることが証明されている。 しかし、データの欠如は、表現の質を損なうことが多い。 弱いクロスリンガル監視のみを必要とするさまざまなアプローチが提案されたが、現在の方法は依然として、わずかな単一言語コーパスしかない言語のための良いCLWEを学ぶことができない。 したがって、低リソース設定におけるCLWEを改善するために、さらなるデータセットを探索する必要があると主張しています。 本稿では、関連する高リソース言語のデータを取り入れることを提案する。 事前に訓練された言語の組み込みを利用する以前のアプローチとは対照的に、私たちは（ i ）低資源と関連する言語のためのCLWEを共同で訓練し、（ ii ）最終的な多言語スペースを構築するためにターゲット言語にそれらをマッピングします。 私たちの実験では、オクシタン語に焦点を当てています。オクシタン語は、リソース不足のためにしばしば軽視されている低資源のロマンス語です。 オクシタニー英語BLIタスクのトレーニングと評価には、フランス語、スペイン語、カタルーニャ語のデータを活用しています。 サポート言語を組み込むことで、メソッドは以前のアプローチを大幅に上回ります。 さらに、私たちの分析は、組み込まれた言語と低資源言語の間の関連性の程度が非常に重要であることを示しています。', 'zh': '跨语言词嵌 (CLWE) 已证诸自然语言处分必不可少,如双语词典归 (BLI)。 然无数常损。 但须弱跨语言监督诸法,而目前之法犹不能为少单语语料库语学良CLWE。 故称有求益之数,以改CLWE于低资源。 本文中,建议整合相关高资源语言之数。 比之曩时独习之语,(i)共训低资源与相关语言之CLWE,并(ii)映其向语以构其终。 在我实验中,专注于奥克西唐语,此资源匮乏之罗曼语,由乏资源,其常被忽视也。 吾以法语、西班牙语、加泰罗尼亚语之数教评奥克西唐语-英语 BLI 任。 合而言之,优于前法大幅度。 此外,我们的分析表明,并言语与低资源言语相关至重。', 'hi': 'क्रॉस-भाषी शब्द एम्बेडिंग (सीएलडब्ल्यूई) विभिन्न प्राकृतिक भाषा प्रसंस्करण कार्यों के लिए अपरिहार्य साबित हुए हैं, उदाहरण के लिए, द्विभाषी शब्दकोश प्रेरण (बीएलआई)। हालांकि, डेटा की कमी अक्सर प्रतिनिधित्व की गुणवत्ता को खराब करती है। केवल कमजोर क्रॉस-लिंगुअल पर्यवेक्षण की आवश्यकता वाले विभिन्न दृष्टिकोणों का प्रस्ताव किया गया था, लेकिन वर्तमान तरीके अभी भी केवल एक छोटे से मोनोलिंगुअल कॉर्पस वाली भाषाओं के लिए अच्छे सीएलडब्ल्यूई सीखने में विफल रहते हैं। इसलिए हम दावा करते हैं कि कम संसाधन सेटअप में CLWEs में सुधार करने के लिए आगे डेटासेट का पता लगाना आवश्यक है। इस पेपर में हम संबंधित उच्च-संसाधन भाषाओं के डेटा को शामिल करने का प्रस्ताव करते हैं। पिछले दृष्टिकोणों के विपरीत जो भाषाओं के स्वतंत्र रूप से पूर्व-प्रशिक्षित एम्बेडिंग का लाभ उठाते हैं, हम (i) कम संसाधन और संबंधित भाषा के लिए CLWEs को संयुक्त रूप से प्रशिक्षित करते हैं और (ii) अंतिम बहुभाषी स्थान बनाने के लिए उन्हें लक्ष्य भाषा में मैप करते हैं। हमारे प्रयोगों में हम Occitan पर ध्यान केंद्रित करते हैं, एक कम संसाधन रोमांस भाषा जो अक्सर संसाधनों की कमी के कारण उपेक्षित होती है। हम प्रशिक्षण के लिए फ्रेंच, स्पेनिश और कैटलन से डेटा का लाभ उठाते हैं और Occitan-अंग्रेजी BLI कार्य पर मूल्यांकन करते हैं। सहायक भाषाओं को शामिल करके हमारी विधि एक बड़े अंतर से पिछले दृष्टिकोणों को मात देती है। इसके अलावा, हमारे विश्लेषण से पता चलता है कि एक शामिल भाषा और कम संसाधन भाषा के बीच संबंधितता की डिग्री गंभीर रूप से महत्वपूर्ण है।', 'ru': 'Межъязычные вложения слов (CLWE) оказались незаменимыми для различных задач обработки естественного языка, например, двуязычной лексической индукции (BLI). Вместе с тем отсутствие данных зачастую снижает качество представляемых данных. Были предложены различные подходы, требующие лишь слабого межъязыкового контроля, но нынешние методы все еще не позволяют выучить хорошие CLWE для языков с небольшим одноязычным корпусом. Поэтому мы утверждаем, что необходимо изучить дальнейшие наборы данных для улучшения CLWE в условиях нехватки ресурсов. В данной работе мы предлагаем включить данные родственных высокоресурсных языков. В отличие от предыдущих подходов, которые используют независимо предварительно обученные вставки языков, мы (i) совместно обучаем CLWE для малоресурсного и связанного с ним языка и (ii) сопоставляем их с целевым языком для создания окончательного многоязычного пространства. В наших экспериментах мы фокусируемся на окситанском, малоресурсном романском языке, которым часто пренебрегают из-за нехватки ресурсов. Мы используем данные с французского, испанского и каталонского языков для обучения и оценки по задаче BLI на окситанском и английском языках. Включая вспомогательные языки, наш метод значительно превосходит предыдущие подходы. Кроме того, наш анализ показывает, что степень взаимосвязи между встроенным языком и языком с ограниченными ресурсами имеет решающее значение.', 'ga': 'Tá sé cruthaithe go bhfuil leabaithe focal tras-teangacha (CLWEanna) fíor-riachtanach le haghaidh tascanna próiseála teanga nádúrtha éagsúla, m.sh. ionduchtú foclóireachta dátheangach (BLI). Mar sin féin, is minic a chuireann easpa sonraí isteach ar cháilíocht na léiriú. Moladh cineálacha cur chuige éagsúla nach dteastaíonn ach maoirsiú tras-teanga lag uathu, ach teipeann ar na modhanna reatha CLWE maith a fhoghlaim do theangacha nach bhfuil iontu ach corpas beag aonteangach. Maíonn muid, mar sin, go bhfuil sé riachtanach tuilleadh tacair shonraí a iniúchadh chun CLWEanna a fheabhsú i socruithe íseal-acmhainne. Sa pháipéar seo tá sé beartaithe againn sonraí ar theangacha ard-acmhainne gaolmhara a ionchorprú. I gcodarsnacht leis na cineálacha cur chuige a rinneadh roimhe seo a thugann leabú neamhspleách teangacha réamh-oilte, déanaimid (i) oiliúint chomhpháirteach ar CLWEanna don teanga íseal-acmhainne agus do theanga ghaolmhar agus (ii) mapálaimid go dtí an sprioctheanga iad chun an spás ilteangach deiridh a thógáil. Inár dturgnaimh dírímid ar Occitan, teanga Rómánsúil le hacmhainní íseal a ndéantar faillí uirthi go minic mar gheall ar easpa acmhainní. Déanaimid sonraí a ghiaráil ón bhFraincis, ón Spáinnis agus ón gCatalóinis le haghaidh oiliúna agus déanaimid meastóireacht ar an tasc BLI Occitan-Béarla. Trí theangacha tacaíochta a ionchorprú sáraíonn ár modh cur chuige a bhí ann roimhe seo go mór. Ina theannta sin, léiríonn ár n-anailís go bhfuil an leibhéal gaolta idir teanga ionchorpraithe agus teanga íseal-acmhainne ríthábhachtach.', 'hu': 'A keresztnyelvű szóbeágyazások (CLWE-k) nélkülözhetetlennek bizonyultak különböző természetes nyelvfeldolgozási feladatokhoz, például a kétnyelvű lexikon indukcióhoz (BLI). Az adatok hiánya azonban gyakran rontja a reprezentációk minőségét. Különböző megközelítéseket javasoltak, amelyek csak gyenge, többnyelvű felügyeletet igényelnek, de a jelenlegi módszerek még mindig nem tudnak jó CLWE-ket tanulni a csak kis, egynyelvű korpuszú nyelveken. Ezért azt állítjuk, hogy további adatkészleteket kell vizsgálni a CLWE-k javítása érdekében az alacsony erőforrással rendelkező beállítások esetében. Jelen tanulmányban azt javasoljuk, hogy a kapcsolódó nagy erőforrású nyelvek adatait beépítsük. A korábbi megközelítésekkel ellentétben, amelyek függetlenül, előre képzett nyelvek beágyazását használják, közösen képzünk CLWE-ket az alacsony erőforrással rendelkező és a kapcsolódó nyelvre, és (ii) feltérképezzük őket a célnyelvre, hogy kiépítsük a végső többnyelvű tér. Kísérleteink során az okszitán nyelvre összpontosítunk, egy alacsony erőforrású román nyelvre, amelyet gyakran elhanyagolnak erőforrások hiánya miatt. Francia, spanyol és katalán nyelvű adatokat használunk a képzéshez és értékeljük az okszitán-angol BLI feladatot. A támogató nyelvek beépítésével módszerünk nagy mértékben felülmúlja a korábbi megközelítéseket. Továbbá elemzésünk azt mutatja, hogy a beépített nyelv és az alacsony erőforrású nyelv közötti kapcsolat mértéke kritikus fontosságú.', 'ka': 'მრავალური სიტყვები (CLWEs) განსხვავებული თავისუფალური პროცესი დავალებისთვის, მაგალითად, ორიენგური ლექსიკონის ინდექცია (BLI). მაგრამ, მონაცემების არსებობა ზოგიერთად გამოცდილობას გამოსახულებების კაalitეობას. განსხვავებული მიღებები, რომელიც მხოლოდ ძალიან ძალიან ძალიან სიტყვალური დანარწმუნელება იყო, მაგრამ მიმდინარე მეტოვები არ შეუძლებელია გავისწავლობა კარგი CLWEs ენ ამიტომ ჩვენ ვთქვათ, რომ უნდა დამატებული მონაცემების კონფიგურაციაში CLWE-ს გაუქმება. ამ დოკუნში ჩვენ გვეძლევა, რომ შესაბამისი დიდი რესურსის ენების მონაცემები შეიყვანოთ. მიმდინარე წინაღალდეგი წინაღებისთვის, რომლებიც საკუთარ წინაღალდეგი წინაღალდეგი წინაღალდეების შებრძნობას, ჩვენ (i) CLWEs-ს შებრძნობით ბალ რესურსისთვისთვისთვისთვისთვისთვის და შესა ჩვენი ექსპერიმენტებში ჩვენ კონუქტირებულიან, რომელიც პრომინეთის ენაზე, რომელიც ძალიან გამოცდილობულია რესურსის არსებობისთვის. ჩვენ ფრანგულ, სპანუალური და კატალანური მონაცემების გამოყენება და გავამუშავებთ OCcitan-English BLI სამუშაოდ. ჩვენი მეთოდი უფრო დიდი მარტინით წინაღალდეს წინაღალდეს წინაღალდეს. დამატებით, ჩვენი ანალიზი აჩვენებს, რომ კრიტიკურად მნიშვნელოვანია, რომელიც ჩვენი ანალიზიზიზიზიზიზების შესახებ შესახებ ერთ-ერთი ენაში და ჩვენი', 'kk': 'Бірнеше тілді сөздерді ендіру (CLWEs) түрлі тілді өңдеу тапсырмалары үшін, мысалы, екі тілді лексикон индукциясы (BLI). Бірақ мәліметтердің жоқ болуы көпшіліктердің сапасына көмектеседі. Тілдерді тек көп тілдерді бақылау үшін көп жағдайлар керек, бірақ қазіргі әдістер тілдер үшін тек кішкентай монолингі корпус арқылы жақсы CLWE үйренуге болмайды. Сондықтан біз CLWE дегенді төмен ресурстар баптауларында жақсарту үшін қосымша деректер баптауларын зерттеу керек деп ойлаймыз. Бұл қағазда біз сілтемелерді көп ресурс тілдерінің деректерін қосу керек. Алдыңғы тілдердің алдыңғы әдістеріне қарсы, тілдердің алдыңғы оқылған ендірімізге қарсы, біз (i) төмен ресурстар үшін CLWEs және қатынастық тілдерді біріктіріп, ii) оларды аяқтау үшін бірнеше тілдерді құру үшін мақ Өзіміздің тәжірибелерімізде Оцитан тіліне назар аударып, ресурстар жоқ себебі көпшілікті болатын Румындық тілі. Біз французша, испан және каталанша мәліметтерді оқыту және оқыту үшін, оқыту және оқыту үшін ағылшын BLI тапсырмасынан береміз. Тілдерді қолдау арқылы біздің әдіміміздің алдыңғы кеңістіктері үлкен шегімен жасайды. Қосымша, анализ біздің тілі мен төмен ресурстар тілі арасындағы қатынастығының көп маңызды деп көрсетеді.', 'el': 'Οι εγκάρσια ενσωμάτωση λέξεων (CLWE) έχουν αποδειχθεί απαραίτητες για διάφορες εργασίες επεξεργασίας φυσικής γλώσσας, π.χ., δίγλωσση επαγωγή λεξικού (BLI). Ωστόσο, η έλλειψη δεδομένων συχνά μειώνει την ποιότητα των αναπαραστάσεων. Προτάθηκαν διάφορες προσεγγίσεις που απαιτούν μόνο αδύναμη διεπλωσσική επίβλεψη, αλλά οι τρέχουσες μέθοδοι εξακολουθούν να αποτυγχάνουν να μαθαίνουν καλά για γλώσσες με μικρό μονογλωσσικό σώμα. Ως εκ τούτου, ισχυριζόμαστε ότι είναι απαραίτητο να διερευνηθούν περαιτέρω σύνολα δεδομένων για τη βελτίωση των CLWE σε εγκαταστάσεις χαμηλής περιεκτικότητας σε πόρους. Στην παρούσα εργασία προτείνουμε να ενσωματωθούν δεδομένα σχετικών γλωσσών υψηλής περιεκτικότητας. Σε αντίθεση με τις προηγούμενες προσεγγίσεις που αξιοποιούν ανεξάρτητα προ-εκπαιδευμένες ενσωματώσεις γλωσσών, εκπαιδεύουμε από κοινού τους για τη γλώσσα χαμηλής περιεκτικότητας σε πόρους και μια σχετική γλώσσα και τους χαρτογραφήσουμε στη γλώσσα-στόχο για να χτίσουμε τον τελικό πολύγλωσσο χώρο. Στα πειράματά μας εστιάζουμε στα Οκιτανικά, μια ρομαντική γλώσσα χαμηλής περιεκτικότητας που συχνά παραμελείται λόγω έλλειψης πόρων. Αξιοποιούμε δεδομένα από τα γαλλικά, ισπανικά και καταλανικά για την εκπαίδευση και την αξιολόγηση του έργου της Οκσιτανικής-Αγγλικής BLI. Με την ενσωμάτωση υποστηρικτικών γλωσσών η μέθοδος μας ξεπερνά κατά μεγάλο βαθμό τις προηγούμενες προσεγγίσεις. Επιπλέον, η ανάλυσή μας δείχνει ότι ο βαθμός συσχέτισης μεταξύ μιας ενσωματωμένης γλώσσας και της γλώσσας χαμηλής περιεκτικότητας σε πόρους είναι κρίσιμης σημασίας.', 'it': "Le incorporazioni di parole cross-lingual (CLWE) si sono rivelate indispensabili per vari compiti di elaborazione del linguaggio naturale, ad esempio l'induzione bilingue del lessico (BLI). Tuttavia, la mancanza di dati spesso compromette la qualità delle rappresentazioni. Sono stati proposti vari approcci che richiedono solo una supervisione interlinguistica debole, ma i metodi attuali non riescono ancora a imparare buoni CLWE per le lingue con un corpus monolingue limitato. Sosteniamo pertanto che è necessario esplorare ulteriori set di dati per migliorare i CLWE in configurazioni a basso contenuto di risorse. In questo articolo proponiamo di incorporare dati relativi a linguaggi ad alta risorsa correlati. Contrariamente agli approcci precedenti che sfruttano incorporazioni di lingue pre-formate in modo indipendente (i) formiamo congiuntamente i CLWE per la lingua a basso contenuto di risorse e una lingua correlata e (ii) li mappiamo alla lingua di destinazione per costruire lo spazio multilingue finale. Nei nostri esperimenti ci concentriamo sull'occitano, una lingua romanza a basso contenuto di risorse spesso trascurata a causa della mancanza di risorse. Utilizziamo dati dal francese, dallo spagnolo e dal catalano per la formazione e la valutazione del compito BLI occitano-inglese. Incorporando linguaggi di supporto il nostro metodo supera di gran lunga gli approcci precedenti. Inoltre, la nostra analisi mostra che il grado di relazione tra un linguaggio incorporato e il linguaggio a basso contenuto di risorse è di fondamentale importanza.", 'lt': 'Tarpkalbiniai žodžių įterpimai (CLWEs) pasirodė būtini įvairioms gamtinių kalbų apdorojimo užduotims, pvz., dvikalbio leksikono indukcijai (BLI). Tačiau duomenų trūkumas dažnai pakenktų atstovavimų kokybei. Buvo pasiūlyti įvairūs metodai, pagal kuriuos reikalinga tik silpna tarpkalbinė priežiūra, tačiau dabartiniai metodai vis dar nepamoko gerų CLWEs kalboms, kuriose yra tik nedidelis vienakalbis korpusas. Todėl teigiame, kad būtina išnagrinėti tolesnius duomenų rinkinius, kad būtų patobulintos CLWEs mažai išteklių turinčiose struktūrose. Šiame dokumente siūlome įtraukti susijusių didelių išteklių kalbų duomenis. Skirtingai nuo ankstesnių metodų, kurie skatina nepriklausomai i š anksto parengtas kalbų įtraukimas, mes (i) bendrai mokome CLWEs mažai išteklių turinčiais ir susijusia kalba ir (ii) juos paveikslėlime tiksline kalba, kad būtų sukurta galutinė daugiakalbė erdvė. Mūsų eksperimentuose daugiausia dėmesio skiriame okcitanui, mažai išteklių turinčiai romėnų kalbai, kuri dažnai yra pamiršta dėl išteklių trūkumo. Naudojame Prancūzijos, Ispanijos ir Katalonijos duomenis mokymui ir vertiname OCcitano ir anglų BLI užduotį. Įtraukdami palaikomąsias kalbas, mūsų metodas iš esmės viršija ankstesnius metodus. Be to, mūsų analizė rodo, kad labai svarbus ryšys tarp integruotos kalbos ir mažai išteklių turinčios kalbos.', 'ml': 'ക്രോസ്- ഭാഷ വാക്കുകളുടെ അകത്തേക്കുള്ള (CLWEs) വ്യത്യസ്താവികമായ ഭാഷ പ്രവര്\u200dത്തിപ്പിക്കുന്ന ജോലികള്\u200dക്ക് ആവശ്യമില്ലാത്തതാണെന്ന്  എന്നാലും, ഡേറ്റാകളുടെ കുറവ് പലപ്പോഴും പ്രതിനിധികളുടെ ഗുണത്തെ പ്രയോജനപ്പെടുത്തുന്നു. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus.  അതുകൊണ്ട് കൂടുതല്\u200d ഡാറ്റാസറ്റുകള്\u200d പരിശോധിക്കേണ്ടതുണ്ടെന്ന് ഞങ്ങള്\u200d പറയുന്നു. കുറഞ്ഞ വിഭവങ്ങളില്\u200d സിഎല്\u200dവിഎസ ഈ പത്രത്തില്\u200d നമ്മള്\u200d വിചാരിക്കുന്ന ഹൈറ്റോര്\u200dസ്സ് ഭാഷകളുടെ വിവരങ്ങള്\u200d ഉള്\u200dപെടുത്താന്\u200d പ്രായശ്ച സ്വാതന്ത്ര്യം പരിശീലിക്കപ്പെടുന്ന ഭാഷകളുടെ സ്വതന്ത്രതയ്ക്ക് മുമ്പ് പരിശീലിക്കപ്പെടുന്ന മാര്\u200dഗങ്ങള്\u200dക്ക് വേര്\u200dപെടുത്തുന്ന മാര്\u200dഗങ്ങള്\u200dക്ക് വേര്\u200dതിരിച നമ്മുടെ പരീക്ഷണങ്ങളില്\u200d ഞങ്ങള്\u200d ഓക്സിറ്റാനിലേക്ക് ശ്രദ്ധിക്കുന്നു. ഒരു കുറഞ്ഞ വിഭവങ്ങള്\u200d റോമാന്\u200dസ് ഭാഷ, അത് വിഭവ ഞങ്ങള്\u200d ഫ്രെഞ്ച്, സ്പാനിഷ്, കാത്താലാനില്\u200d നിന്നും വിവരങ്ങള്\u200d ഉപയോഗിക്കുന്നു. ഓക്സിട്ടാന്\u200d- ഇംഗ്ലീഷ് ബിലി ജോ ഭാഷകളെ പിന്തുണയ്ക്കുന്നതിനാല്\u200d മുമ്പുള്ള ഒരു വലിയ മാര്\u200dജിനിലൂടെ ഞങ്ങളുടെ രീതിയില്\u200d നിന്നും പ്രവര്\u200dത അതിനുശേഷം, നമ്മുടെ അന്വേഷണം കാണിച്ചുകൊണ്ടിരിക്കുന്നു, ഒരു അകത്തുള്ള ഭാഷയ്ക്കുമിടയിലെ ബന്ധപ്പെട്ട സ്ഥിതിയുടെ', 'mk': 'Кросјазичните зборови (CLWEs) се покажаа неопходни за различни природни задачи за обработување на јазикот, на пример, двојјазичната индукција на лексикони (BLI). Сепак, недостатокот на податоци честопати го намалува квалитетот на претставувањата. Предложени се различни пристапи кои бараат само слаб крстојазичен надзор, но сегашните методи сé уште не учат добри КЛВЕ за јазици со само мал монојазичен корпус. Затоа тврдиме дека е неопходно да се истражуваат понатамошни податоци за подобрување на CLWEs во поставувањата со ниски ресурси. Во овој документ предлагаме вклучување на податоци од поврзаните јазици со високи ресурси. За разлика од претходните пристапи кои влијаат на независно предобучени вложувања на јазици, ние (i) ги обучуваме КЛВЕ за ниски ресурси и поврзан јазик заедно и (ii) ги мапираме на јазикот на целта за изградба на конечниот мултијазичен простор. Во нашите експерименти се фокусираме на окцитански јазик со ниски ресурси, кој честопати е заборавен поради недостатокот на ресурси. Ние ги искористуваме податоците од француски, шпански и каталонски за обука и оценуваме задачата окцитанско-англиски БЛИ. Со вклучување на јазиците за поддршка нашиот метод ги надминува претходните пристапи со голема маргина. Покрај тоа, нашата анализа покажува дека степенот на поврзаност помеѓу вклучен јазик и јазик со ниски ресурси е критично важен.', 'mt': 'L-inkorporazzjonijiet tal-kliem translingwi (CLWEs) urew li huma indispensabbli g ħal diversi kompiti naturali tal-ipproċessar tal-lingwi, pereżempju l-induzzjoni tal-lexicon bilingwi (BLI). Madankollu, in-nuqqas ta’ dejta spiss ifixkel il-kwalità tar-rappreżentazzjonijiet. Ġew proposti diversi approċċi li jeħtieġu biss superviżjoni translingwistika dgħajfa, iżda l-metodi attwali għadhom ma jitgħallmux CLWEs tajbin għal lingwi b’korpus monolingwistiku żgħir biss. Għalhekk niddikjaraw li huwa meħtieġ li jiġu esplorati settijiet ta’ dejta ulterjuri biex jittejbu s-CLWEs f’settijiet b’riżorsi baxxi. F’dan id-dokument nipproponu li ninkorporaw dejta ta’ lingwi relatati b’riżorsi għoljin. B’kuntrast ma’ approċċi pre ċedenti li jiġġeneraw inkorporazzjonijiet ta’ lingwi mħarrġa minn qabel indipendentement, a ħna (i) inħarrġu lill-CLWEs għal lingwa b’riżorsi baxxi u lingwa relatata b’mod konġunt u (ii) inħarrġuhom lejn il-lingwa fil-mira biex nibnu l-ispazju multilingwi finali. Fl-esperimenti tagħna niffokaw fuq l-Oċċitan, lingwa Romana b’riżorsi baxxi li ta’ spiss tiġi injorata minħabba nuqqas ta’ riżorsi. We leverage data from French, Spanish and Catalan for training and evaluate on the Occitan-English BLI task.  Permezz tal-inkorporazzjoni tal-lingwi ta’ appoġġ il-metodu tagħna jaqbeż l-approċċi preċedenti b’marġini kbir. Barra minn hekk, l-analiżi tagħna turi li l-grad ta’ rabta bejn lingwa inkorporata u lingwa b’riżorsi baxxi huwa kruċjali.', 'mn': 'Холон хэл хэлний хэлний нэгдэл (CLWEs) нь олон байгалийн хэл үйлдвэрлэлийн үйлдвэрлэлүүд, жишээ нь хоёр хэл хэлний лексикон үйлдвэрлэл (BLI) гэх мэт харуулсан. Гэвч мэдээллийн алдагдлыг ихэвчлэн үзүүлэлтийн чанарыг нөлөөлдөг. Зөвхөн хэл дахь бага зэрэг хязгаарлалтай удирдлага шаардлагатай олон арга зам санал болсон, гэхдээ одоогийн арга нь зөвхөн жижиг ганц хэл корпус суралцахгүй байна. Тиймээс бид CLWE-г бага нөөцийн байгууллагуудын сайжруулахын тулд нэмэлт өгөгдлийн сангуудыг судлах хэрэгтэй гэдгийг хэлж байна. Энэ цаасан дээр бид холбоотой өндөр боловсролын хэлний мэдээллийг нэгтгэх санал өгдөг. Өмнөх арга баримтуудын эсрэгээр бид (i) хэлний өмнө сургалтын тусламжтайгаар сургалтын тусламжтайгаар сургалтын тусламжтайгаар CLWEs-г бага болон холбоотой хэл хамтран сургалтын тулд сургалтын төгсгөл хэл дээр зураг зураг Бидний туршилтанд Окситан хэл дээр анхаарлаа төвлөрөх болно. Окситан хэл нь бага боловсролын Ромын хэл юм. Бид Француз, Испан, Каталан болон Окитан-Англи BLI ажил дээр суралцах, үнэ цэнэтэй мэдээллийг ашиглаж байна. Дэлхийг дэмжиж буй хэлнүүдийг нэгтгэхэд бидний арга нь өмнөх ойлголтыг маш том зам замаар хийдэг. Мөн бидний шинжилгээ нь нэгдсэн хэл болон бага боловсролын хэл хоорондын харилцааны түвшин чухал гэдгийг харуулдаг.', 'pl': 'Wielojęzyczne osadzenia słów (CLWE) okazały się niezbędne do różnych zadań przetwarzania języka naturalnego, np. indukcji leksykonu dwujęzycznego (BLI). Brak danych często jednak ogranicza jakość reprezentacji. Zaproponowano różne podejścia wymagające jedynie słabego nadzoru między językami, ale obecne metody nadal nie są w stanie nauczyć się dobrych CLWE dla języków z niewielkim korpusem jednojęzycznym. W związku z tym twierdzimy, że konieczne jest zbadanie dalszych zbiorów danych w celu ulepszenia CLWE w konfiguracjach o niskich zasobach. W niniejszym artykule proponujemy uwzględnienie danych pokrewnych języków o wysokich zasobach. W przeciwieństwie do poprzednich podejść, które wykorzystują niezależnie wstępnie przeszkolone osadzenia języków, wspólnie (i) szkolimy CLWE pod kątem niskich zasobów i pokrewnego języka oraz (ii) mapujemy je do języka docelowego, aby zbudować ostateczną przestrzeń wielojęzyczną. W naszych eksperymentach skupiamy się na okcytaninie, niskim języku romańskim, który jest często zaniedbany z powodu braku zasobów. Wykorzystujemy dane z języka francuskiego, hiszpańskiego i katalońskiego do szkolenia i oceny zadania okcytańsko-angielskiego BLI. Dzięki włączeniu języków wspierających nasza metoda znacznie przewyższa poprzednie podejścia. Ponadto nasza analiza pokazuje, że stopień powiązania między językiem włączonym a językiem niskim zasobem ma kluczowe znaczenie.', 'no': 'Krysspråk ordinnbygging (CLWEs) har bevist uverkomsiktig for ulike naturspråk- handlingar, f.eks. bilinguelt lexicon- induksjon (BLI). Men manglar data ofte gjer kvaliteten på representasjonar. Dei fleire tilnærmingane som krev berre svakt krysspråksoversikt vart foreslått, men gjeldande metoder kan fortsatt ikkje lære godt CLWE for språk med berre ein liten monospråkskorpus. Vi så tvar at det er nødvendig å utforske fleire datasett for å forbetra CLWEs i låg ressursoppsett. I denne papiret foreslår vi å inkludere data av tilhøyressursspråk. I contrast to previous approaches which leverage independently pre-trained embedding of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language for to build the final multilingual space. I eksperimentene våre fokuserer vi på Occitan, eit låg ressurs-Romansk språk som ofte vert neglectert på grunn av mangling av ressursar. Vi leverer data frå fransk, spansk og katalansk for opplæring og evaluering av OCcitansk-engelsk BLI-oppgåva. Ved å inkludere støtte språk, metoden vårt utfører førre tilnærmingar med ein stor margin. I tillegg viser analysen vårt at avhengighetsgraden mellom ein inkorprert språk og den låg ressursspråket er kritisk viktig.', 'ro': 'Încorporările de cuvinte interlingve (CLWE) s-au dovedit indispensabile pentru diverse sarcini de prelucrare a limbajului natural, de exemplu inducția lexiconului bilingv (BLI). Cu toate acestea, lipsa datelor afectează adesea calitatea reprezentărilor. Au fost propuse diverse abordări care necesită doar o supraveghere interlingvă slabă, dar metodele actuale nu reușesc încă să învețe CLWE bune pentru limbile cu un corp monolingv mic. Prin urmare, susținem că este necesar să explorăm seturi de date suplimentare pentru a îmbunătăți CLWE în setări cu resurse reduse. În această lucrare propunem includerea datelor din limbile conexe cu resurse ridicate. Spre deosebire de abordările anterioare care utilizează încorporarea independentă pre-instruită a limbilor, noi (i) pregătim împreună CLWE pentru o limbă cu resurse reduse și o limbă conexă și (ii) le cartografiem la limba țintă pentru a construi spațiul multilingv final. În experimentele noastre ne concentrăm pe occitană, o limbă romană cu resurse reduse care este adesea neglijată din cauza lipsei de resurse. Folosim datele din franceză, spaniolă și catalană pentru formare și evaluăm sarcina BLI occitană-engleză. Prin încorporarea limbilor de sprijin metoda noastră depășește abordările anterioare cu o marjă mare. Mai mult decât atât, analiza noastră arată că gradul de relație dintre un limbaj încorporat și limbajul cu resurse reduse este esențial.', 'sr': 'Ukratko jezične reči (CLWEs) dokazale su neophodno za različite prirodne obaveze obrađivanja jezika, npr. indukciju dvojezičkih leksiona (BLI). Međutim, nedostatak podataka često ukazuje na kvalitet predstavljanja. Razni pristupi koji su zahtevali samo slabu prejezičku nadzor, ali trenutne metode još uvijek ne mogu naučiti dobre CLWE za jezike sa samo malim monojezičkim korpusom. Stoga tvrdimo da je potrebno istražiti daljnje podatke kako bi poboljšali CLWE u nizim resursima. U ovom papiru predlažemo da uključimo podatke o povezanim jezicima visokog resursa. Za razliku od prethodnih pristupa koji utiču na nezavisno predobučene integracije jezika, i) treniramo CLWEs za niske resurse i povezan jezik zajedno i ii) ih mapiramo na ciljni jezik kako bi izgradili konačni multijezički prostor. U našim eksperimentima fokusiramo se na Okcitan, mali rimunski jezik koji se često zanemari zbog nedostatka resursa. Učinimo podatke iz francuskog, španjolskog i katalanskog za obuku i procjenu occitanskog-engleskog BLI zadatka. Uključujući podršku jezika, naša metoda iznosi prethodne pristupe velikom marginom. Nadalje, naša analiza pokazuje da je stepenica povezanosti između uključenog jezika i jezika niskog resursa kritično važna.', 'si': 'Cross-language word Embdings නමුත්, දත්ත අවශ්\u200dය වෙලාවට සාමාන්\u200dය විශේෂතාවට ප්\u200dරතිචාරයක් අවශ්\u200dය වෙනවා. විවිධ ප්\u200dරවේශනයක් අවශ්\u200dය විතරයි ක්\u200dරීස් භාෂාවක් ප්\u200dරවේශනයක් විතරයි, නමුත් ප්\u200dරවේශනයේ ප්\u200dරවේශනයක් තාමත් හොඳ CL ඉතින් අපි කියන්නේ ඒක අඩු තොරතුරු සෙට්ටුව පරීක්ෂා කරන්න ඕනි කියලා CLWEs විශ්වාස කරන්න. මේ පත්තරේ අපි ප්\u200dරශ්නයක් කරනවා සම්බන්ධ විශ්වාස භාෂාවන් ගැන දත්ත සම්බන්ධ කරන්න. අන්තිම භාෂාවයේ ප්\u200dරශ්නයක් නිර්මාණය කරපු ප්\u200dරශ්නයක් වෙනුවෙන් පිළිබඳ අනුවෙන් අනුවෙන් ප්\u200dරශ්නයක් වෙනුවෙන්, අපි (i) ප්\u200dරශ්නයක් අඩුම අපේ පරීක්ෂණයේ අපි ඔක්සිතාන් වලට අවධානය කරනවා, අඩුම ප්\u200dරමාණයක් රෝමාන්ස් භාෂාවක්, ඒක හැමවෙලේම අවශ අපි ෆ්\u200dරෑන්ස්, ස්පැනිස් සහ කැතාලාන් වලින් දත්ත ප්\u200dරධානය සහ අවශ්\u200dයය කරන්න, ඔක්සිතාන්-ඉංග්\u200dරීසි BL අපේ භාෂාව සහය සම්බන්ධ කරපු භාෂාව සම්බන්ධ කරලා අපේ විධානය ප්\u200dරශ්නයක් විශාල ප්\u200dරශ්න තවත්, අපේ විශ්ලේෂණය පෙන්වන්නේ, සම්පූර්ණ භාෂාවක් සහ අඩු සම්පූර්ණ භාෂාවක් අතර සම්බන්ධතාවක', 'so': 'Qoraalka luuqadaha iskuulka ah (CLWEs) waxay caddeeyeen kuwa aan u baahnayn shaqaalaha baaraandegista afka kala duduwan, tusaale ahaan ganacsiga leksisika labada luqadood (BLI). Si kastaba ha ahaatee baahida macluumaadka ayaa inta badan saameyn ku yeelata qiimaha wakiilka. Waxaa la soo jeeday qaabab kala duduwan oo loo baahan yahay ilaalinta luuqadaha oo kaliya, laakiin qaababka la soo socday weli waxay u baaqan waayeen inay baraan karto si fiican CLWEs oo luqadaha lagu qoro oo kaliya qof yar oo luqad ah. Sidaa darteed waxaynu sheegaynaa in loo baahan yahay baaritaanka macluumaad dheeraad ah si loo hagaajiyo CLWEs marka lagu jiro dhibaatooyinka hoos-dhexe. Kanu warqaddan waxaan ku talo galaynaa in lagu soo qoro macluumaad ku saabsan luqadaha raslimaha sare. Iska duwan qaababka hore oo ay si xorriyad ah ugu soo bandhigi jireenka luuqadaha hore, waxaynu (i) ku tababarinnaa CLWEs, taasoo ay u bartaan luqada hoose iyo si wadajir ah luqada la xidhiidha iyo (ii) inaannu u karno luuqadda waxyaabaha loogu talagalay si ay ugu dambeeyaan goobta luuqadaha kala duduwan. Imtixaanadeena waxaynu ku kalsoonaan nahay Occitan, luqad hoos u leedahay oo Roomiska ah, kaas oo marar badan la halmaamay, sababtoo ah manfac la’aanta. Macluumaadka waxaannu ka soo bandhignaa Faraansiis, Isbanish iyo Katalaan waxbarashada iyo qiimeynta shaqada BLI ee Occitan-Ingiriis. Isku qabsashada luqadaha kaalmada ah qaababkayaga ayaa ka muuqata qaabab horay ah oo aad u badan. Furthermore, baaritaankeennu wuxuu muujiyaa in shahaadada xiriirka ee ku dhexeeya luqada aan la qorin iyo luqada hoos u leenahay ay muhiim u tahay.', 'sv': 'Cross-lingual word embeddings (CLWE) har visat sig vara oumbärliga för olika naturliga språkbehandlingsuppgifter, t.ex. tvåspråkig lexikoninduktion (BLI). Bristen på data försämrar dock ofta kvaliteten på representationer. Olika tillvägagångssätt som endast kräver svag tvärspråklig övervakning föreslogs, men nuvarande metoder misslyckas fortfarande att lära sig bra CLWE för språk med endast en liten enspråkig korpus. Vi hävdar därför att det är nödvändigt att undersöka ytterligare datamängder för att förbättra CLWE i konfigurationer med låga resurser. I denna uppsats föreslår vi att data från relaterade högresursspråk integreras. I motsats till tidigare tillvägagångssätt som utnyttjar självständigt förutbildade inbäddningar av språk, utbildar vi CLWE gemensamt för lågresursspråk och ett relaterat språk och (ii) kartlägger dem till målspråket för att bygga det slutliga flerspråkiga utrymmet. I våra experiment fokuserar vi på occitanska, ett romanskt lågresursspråk som ofta försummas på grund av brist på resurser. Vi utnyttjar data från franska, spanska och katalanska för utbildning och utvärderar den occitansk-engelska BLI-uppgiften. Genom att införliva stödspråk överträffar vår metod tidigare tillvägagångssätt med stor marginal. Vidare visar vår analys att graden av samband mellan ett införlivat språk och lågresursspråket är avgörande.', 'ms': 'Penampilan perkataan saling bahasa (CLWEs) telah terbukti tidak diperlukan untuk pelbagai tugas pemprosesan bahasa alam, cth., induksi leksikon bilingual (BLI). Namun, kekurangan data sering mengurangi kualiti perwakilan. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus.  Oleh itu, kami menyatakan bahawa perlu mengeksplorasi set data lebih lanjut untuk meningkatkan CLWEs dalam tetapan sumber rendah. Dalam kertas ini kami cadangkan untuk memasukkan data bahasa sumber tinggi yang berkaitan. In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space.  Dalam eksperimen kami kami fokus pada Occitan, bahasa Romansi yang mempunyai sumber rendah yang sering dilupakan kerana kekurangan sumber. Kami menggunakan data dari Perancis, Sepanyol dan Catalan untuk latihan dan menilai tugas BLI Occitan-Inggeris. Dengan memasukkan bahasa sokongan kaedah kita melebihi pendekatan terdahulu dengan margin besar. Selain itu, analisis kami menunjukkan bahawa darjah hubungan antara bahasa yang termasuk dan bahasa sumber rendah sangat penting.', 'ta': 'கிராஸ்- மொழி சொல்லை உள்ளடக்கம் (CLWEs) பல இயற்கையான மொழி செயல்படுத்தல் பணிகளுக்கு தேவையான தெரிவிக்கப்பட்டுள்ளது, உதாரணமாக, இரு மொழி லெக்ச ஆனால், தரவின் குறைவு பெரும்பாலும் பிரதிநிதியின் தரமை பாதிக்கும். பலவிதமான வழிமுறைகள் மட்டுமே பலவீனமான மொழி கண்காணிப்பு தேவைப்படுகிறது, ஆனால் தற்போதைய முறைகள் இன்னும் சிறிய மொன்மொழிக்கு மட்டும் சிற ஆகையால் குறைந்த மூலத்தின் அமைப்புகளில் CLWEs யை மேம்படுத்த வேண்டும் என்று நாம் கூறுகிறோம். இந்த காகிதத்தில் நாம் தொடர்புடைய உயர்மூலத்தின் மொழிகளின் தரவை சேர்க்க பரிந்துரைக்கிறோம். முந்தைய செயல்பாடுகளுக்கு வெற்றி பயிற்சி முன் பயிற்சியான மொழிகளின் உள்ளிடுகளை வெளியீடு செய்யும் முன்னோட்டத்திற்கு எதிராக, நாம் (i) குறைந்த மூலத்திற் எங்கள் சோதனைகளில் நாம் ஒக்சிட்டான் மீது கவனம் செலுத்துகிறோம், ஒரு குறைந்த வளங்கள் இல்லாத காரணத்தால் குறைந்த ரோம நாங்கள் பிரெஞ்சு, ஸ்பானிஷ் மற்றும் காட்டலானிலிருந்து தகவல்களை வழங்குகிறோம் பயிற்சி மற்றும் மதிப்பிடுகிற By incorporating supporting languages our method outperforms previous approaches by a large margin.  அதற்கும், எங்கள் ஆராய்ச்சி காட்டுகிறது ஒரு உள்ளடக்கப்பட்ட மொழி மற்றும் குறைந்த மூலத்தின் மொழி மத்தியின் மத்தியின் தரம்', 'ur': 'Cross-language word embeddings (CLWEs) نے مختلف طبیعی زبان پردازش کے کاموں کے لئے ضرورت نہیں پائی ہے، جیسے دو زبان لکسیون (BLI) کے اندر۔ لیکن ڈیٹا کی کمزوری اغلبہ دکھانے کے کیفیت سے اضافہ کرتی ہے۔ بہت سی طریقے ہیں جو صرف کمزور کرسی زبان کی نظارت کی ضرورت کرتی ہیں، لیکن اکنون طریقے بھی بہترین CLWES کی زبانوں کے لئے صرف ایک چھوٹی ایک زبان کی کورپوس کے ساتھ نہیں سکتے۔ ہم اس لئے کہتے ہیں کہ نیچے منبع سٹ میں CLWES کو بہتر کرنے کے لئے اضافہ ڈیٹ سٹ کا تحقیق کرنا ضرورت ہے. اس کاغذ میں ہم اس کی پیشنهاد کرتے ہیں کہ ارتباط کے ساتھ ارتباط دار سرمایہ کی زبانوں کے داخل کریں۔ مختلف طریقوں کے مقابلہ میں جو زبانوں کی پیش آموزش کی آموزش کی جگہ سے آزاد کر رہے ہیں، ہم (i) CLWEs کو کم منبع کے لئے تربیت کریں اور ایک مرتبہ زبان کے ساتھ اور (ii) ان کو موقع زبان کی جگہ بنانے کے لئے نقشه بنائیں۔ ہمارے آزمائش میں ہم اکسیتان پر تمرکز کریں گے، ایک کم منبع رومانس زبان جو اکثر منبع کمی کے سبب غفلت کی جاتی ہے۔ ہم نے فرانسوی، اسپانیایی اور کاتالان سے ڈیٹا کو آکسٹان-انگلیسی BLI کے کام پر آموزش اور ارزش کے لئے استعمال کیا ہے. زبانوں کی مدد کے ساتھ ہمارا طریقہ ایک بڑی مرز سے پہلے آگے بڑھتا ہے۔ اور اس کے علاوہ ہماری تحلیل دکھاتا ہے کہ ایک جمع زبان اور کم سرمایہ زبان کے درمیان تعلق کا درجہ بہت اہم ہے.', 'vi': 'Sự nhúng tay ngôn ngữ khác nhau (các loại g ô) đã chứng tỏ rất cần thiết cho các công việc xử lý ngôn ngữ tự nhiên, v.v. quy nhập ngôn ngữ Tuy nhiên, việc thiếu dữ liệu thường làm ảnh hưởng đến chất lượng các đài phát biểu. Các phương pháp khác nhau yêu cầu chỉ một sự giám sát ngôn ngữ yếu, nhưng các phương pháp hiện tại vẫn không học được các loại chất trong các ngôn ngữ có một cơ thể nhỏ. Do đó, chúng tôi cho rằng cần phải tìm hiểu các tập tin khác để cải thiện các loại bom ở các nguồn thấp. Trong tờ giấy này chúng tôi đề nghị áp dụng dữ liệu ngôn ngữ cao cấp liên quan. So với các phương pháp trước đó dùng để tạo ra những trường hợp độc lập được đào tạo từ các ngôn ngữ có sẵn, chúng ta (i) huấn luyện các loại gô trong các loại nguồn thấp và ngôn ngữ liên quan, cùng nhau, và (i) vạch ra ngôn ngữ tiêu chuẩn để tạo ra vùng ngôn ngữ cuối cùng đa dạng. Trong các thí nghiệm của chúng ta, chúng ta tập trung vào Occitan, một ngôn ngữ lãng mạn với nguồn ít tài nguyên mà thường bị bỏ quên bởi vì thiếu nguồn lực. Chúng tôi thu thập dữ liệu từ Pháp, Tây Ban Nha và Catalan để được đào tạo và đánh giá công việc bằng tiếng Anh. Bằng cách nhập ngôn ngữ hỗ trợ, phương pháp của chúng ta vượt trội trước nhiều tiến bộ. Hơn nữa, phân tích của chúng tôi cho thấy mức độ liên quan giữa ngôn ngữ ngữ nhập cảnh và ngôn ngữ hạ nguồn là rất quan trọng.', 'uz': "Name Lekin, maʼlumotlar manbasiga ko'p ko'pincha tashkilotlarning sifatiga qo'shiladi. Ko'pchilik tilni boshqarish kerak bo'lgan har xil usullar talab qilingan, lekin ҳозирги usullar faqat kichkina monolingual kopus bilan yaxshi CLWEs tilni o'rganib boʻlmaydi. Biz shunday deymiz, qanchalik manbanlar tartiblarda CLWEsni bajarish kerak. Bu hujjatda, biz juda yuqori rasm tillari haqida maʼlumotni birlashtirishni talab qilamiz. Oldingi tillar bilan birinchi o'rganishdan oldin ishlatilgan usullar bilan boshqa shakllarga qarang, biz (i) qo'shimcha resource uchun CLWEs (CLWEs) va bir xil tilni birlashtirish uchun bogʻliq tilni taʼminlaydimiz va (ii) ularni eng oxirgi tillar joyini yaratish uchun eng tilga qarang. Biz jarayonlarimizda Oscitan tiliga qarasamiz, bu ko'p rasmlar yoʻq sababda qo'shimcha Roman tili. Biz Fransuzcha, Ispancha va Kataladan maʼlumotni olib tashlash va qiymatlashimiz uchun Occitan- Ingliz tili BLI vazifasini bajaramiz. Tillarni qoʻllashni qoʻllash orqali, bizning usuli oldingi usullarni katta margin orqali bajaradi. Ko'rib, bizning analytikimizni ko'rsatadi, o'zgartirilgan tillar bilan bog'liqlar darajasi va kichkina rasm tili muhim.", 'hr': 'Ukratko jezične riječi (CLWEs) dokazale su neophodno za različite zadatke obrađivanja prirodnih jezika, na primjer indukcija dvojezičkih leksiona (BLI). Međutim, nedostatak podataka često utječe na kvalitet predstavljanja. Prijedloženi su razni pristupi koji zahtijevaju samo slabu prekograničnu nadzor, ali trenutne metode još uvijek ne nauče dobre CLWE za jezike sa samo malim monojezičkim korpusom. Stoga tvrdimo da je potrebno istražiti daljnje komplete podataka kako bi poboljšali CLWE u nastavama niskih resursa. U ovom papiru predlažemo uključiti podatke o povezanim jezicima visokog resursa. Za suprotnost prethodnim pristupima koji utječu na nezavisno predobučene integracije jezika, i) treniramo CLWEs za niske resurse i povezani jezik zajedno i ii) ih mapiramo na ciljni jezik kako bi izgradili konačni multijezički prostor. U našim eksperimentima usredotočili smo se na Occitan, mali rimunski jezik koji se često zanemari zbog nedostatka resursa. Primjenjujemo podatke francuskog, španjolskog i katalanskog za obuku i procjenu occitanskog-engleskog BLI zadatka. Uključujući podršku jezika naša metoda iznosi prethodne pristupe velikom marginom. Nadalje, naša analiza pokazuje da je stupnja povezanosti između uključenog jezika i jezika niskog resursa kritično važna.', 'bg': 'Многоезичните словесни вграждания са се оказали незаменими за различни задачи по обработка на естествения език, например двуезична лексиконна индукция (БЛИ). Липсата на данни обаче често нарушава качеството на представянето. Бяха предложени различни подходи, изискващи само слаб междуезичен надзор, но настоящите методи все още не успяват да научат добри езикови езици с малък едноезичен корпус. Поради това твърдим, че е необходимо да се проучат допълнителни набори от данни, за да се подобрят CLWE при настройки с ниски ресурси. В тази статия предлагаме да се включат данни на свързани високоресурсни езици. За разлика от предишните подходи, които използват самостоятелно предварително обучени вграждания на езици, ние (i) обучаваме съвместно CLWE за нисък ресурс и свързан език и (ii) ги картографираме към целевия език, за да изградим окончателното многоезично пространство. В нашите експерименти се фокусираме върху окситанския, романски език с нисък ресурс, който често се пренебрегва поради липса на ресурси. Използваме данни от френски, испански и каталунски за обучение и оценка на задачата окситанско-английски. Чрез включването на поддържащи езици методът ни надминава предишните подходи с голяма разлика. Освен това нашият анализ показва, че степента на свързаност между вградения език и езика с ниски ресурси е критично важна.', 'nl': "Cross-lingual word embeddings (CLWE's) zijn onmisbaar gebleken voor verschillende natuurlijke taalverwerkingstaken, zoals tweetalige lexicon inductie (BLI). Het gebrek aan gegevens schaadt echter vaak de kwaliteit van de representaties. Er werden verschillende benaderingen voorgesteld die slechts zwak cross-lingual toezicht vereisen, maar de huidige methoden slagen er nog steeds niet in om goede CLWE's te leren voor talen met slechts een klein monolingual corpus. Wij stellen daarom dat het noodzakelijk is om verdere datasets te onderzoeken om CLWE's in low-resource opstellingen te verbeteren. In dit document stellen we voor om gegevens van verwante high-resource talen op te nemen. In tegenstelling tot eerdere benaderingen die gebruik maken van onafhankelijk voorgetrainde inbeddingen van talen, trainen we (i) CLWE's gezamenlijk voor de low-resource en een gerelateerde taal en (ii) brengen ze in kaart met de doeltaal om de uiteindelijke meertalige ruimte te bouwen. In onze experimenten richten we ons op het Occitaans, een Romaanse taal die vaak wordt verwaarloosd vanwege gebrek aan middelen. We gebruiken gegevens uit het Frans, Spaans en Catalaans voor training en evaluatie van de Occitaans-Engelse BLI-taak. Door de integratie van ondersteunende talen overtreft onze methode eerdere benaderingen met een grote marge. Verder blijkt uit onze analyse dat de mate van verwantschap tussen een geïntegreerde taal en de low-resource taal van cruciaal belang is.", 'da': "Tværsprogede ordindlejringer (CLWE'er) har vist sig uundværlige til forskellige naturlige sprogbehandlingsopgaver, f.eks. tosproget leksikoninduktion (BLI). Manglen på data forringer imidlertid ofte kvaliteten af repræsentationer. Der blev foreslået forskellige tilgange, der kun kræver svag tværsproget overvågning, men de nuværende metoder er stadig ikke i stand til at lære gode CLWE'er for sprog med kun et lille ensproget korpus. Vi hævder derfor, at det er nødvendigt at undersøge yderligere datasæt for at forbedre CLWE'er i opsætninger med lav ressource. I denne artikel foreslår vi at indarbejde data fra relaterede sprog med høj ressource. I modsætning til tidligere fremgangsmåder, der udnytter uafhængige forududdannede integreringer af sprog, træner vi (i) CLWE'er til det lave ressource og et beslægtet sprog i fællesskab og (ii) kortlægger dem til målsproget for at opbygge det endelige flersprogede rum. I vores eksperimenter fokuserer vi på occitansk, et romantisk sprog, der ofte forsømmes på grund af manglende ressourcer. Vi udnytter data fra fransk, spansk og catalansk til træning og evaluerer den occitansk-engelske BLI-opgave. Ved at indarbejde understøttende sprog overgår vores metode med stor margin tidligere tilgange. Desuden viser vores analyse, at graden af relaterethed mellem et indarbejdet sprog og det lave ressourcesprog er kritisk vigtig.", 'ko': '이중 언어 단어 삽입 (CLWE) 은 이중 언어 사전 귀납 (BLI) 과 같은 다양한 자연 언어 처리 작업에서 빠질 수 없거나 빠질 수 없다는 것이 증명되었다.그러나 데이터의 부족은 종종 표시의 질에 영향을 줄 수 있다.사람들은 약한 다중 언어 감독만 필요한 여러 가지 방법을 제시했지만 소량의 단어 자료 라이브러리만 있는 언어에 대해서는 현재의 방법으로는 좋은 CLWE를 배울 수 없다.따라서 저자원 환경에서 CLWE를 개선하기 위해 데이터 세트를 더욱 탐색할 필요가 있다고 생각합니다.본고에서 우리는 관련 고자원 언어의 데이터를 통합하는 것을 건의합니다.이전에 독립적으로 미리 훈련된 언어를 삽입하는 방법과 달리 우리(i)는 저자원과 관련 언어의 CLWE를 공동으로 교육하고 (ii)를 목표 언어에 비추어 최종 다언어 공간을 구축한다.우리의 실험에서 우리가 주목한 것은 Occitan이다. 이것은 자원이 부족하기 때문에 자주 무시되는 저자원 낭만주의 언어이다.우리는 프랑스어, 스페인어, 카탈로니아어에서 온 데이터를 이용하여 Occitan 영어 BLI 임무에 대해 교육과 평가를 실시한다.지원 언어를 결합함으로써 우리의 방법은 이전의 방법보다 훨씬 낫다.그 밖에 우리의 분석에 의하면 통합 언어와 저자원 언어 간의 관련성이 매우 중요하다고 한다.', 'id': 'Pencampuran kata saling bahasa (CLWEs) telah terbukti tidak penting untuk berbagai tugas proses bahasa alam, misalnya induksi lexikon bilingual (BLI). Namun, kekurangan data sering merusak kualitas representation. Berbagai pendekatan yang hanya memerlukan pengawasan saling bahasa yang lemah diusulkan, tetapi metode saat ini masih gagal belajar CLWEs yang baik untuk bahasa hanya dengan corpus monobahasa kecil. Oleh karena itu kami mengklaim bahwa perlu untuk mengeksplorasi set data lebih lanjut untuk meningkatkan CLWEs dalam setup sumber daya rendah. Dalam kertas ini kami mengusulkan untuk memasukkan data dari bahasa sumber daya tinggi yang berhubungan. Berbeda-beda dengan pendekatan sebelumnya yang mempengaruhi pembangunan bahasa yang dilatih secara independen, kami (i) melatih CLWEs untuk sumber daya rendah dan bahasa terkait bersama-sama dan (ii) peta mereka ke bahasa sasaran untuk membangun ruang multibahasa akhir. Dalam eksperimen kami kami fokus pada Occitan, bahasa romantis yang rendah sumber daya yang sering diabaikan karena kekurangan sumber daya. Kami menggunakan data dari Perancis, Spanyol dan Catalan untuk latihan dan mengevaluasi tugas BLI Occitan-Inggris. By incorporating supporting languages our method outperforms previous approaches by a large margin.  Selain itu, analisis kami menunjukkan bahwa tingkat hubungan antara bahasa terkandung dan bahasa sumber daya rendah sangat penting.', 'de': 'Cross-lingual Word Embeddings (CLWE) haben sich für verschiedene natursprachliche Verarbeitungsaufgaben wie z.B. bilinguale Lexikon Induktion (BLI) als unverzichtbar erwiesen. Der Mangel an Daten beeinträchtigt jedoch häufig die Qualität der Darstellungen. Es wurden verschiedene Ansätze vorgeschlagen, die nur eine schwache translinguale Aufsicht erfordern, aber die derzeitigen Methoden scheitern immer noch daran, gute CLWEs für Sprachen mit nur einem kleinen monolingualen Korpus zu erlernen. Wir behaupten daher, dass es notwendig ist, weitere Datensätze zu erforschen, um CLWEs in ressourcenarmen Setups zu verbessern. In diesem Papier schlagen wir vor, Daten verwandter Sprachen mit hohem Ressourcenbedarf einzubeziehen. Im Gegensatz zu bisherigen Ansätzen, die unabhängig voneinander vorgetrainierte Einbettungen von Sprachen nutzen, schulen wir (i) CLWEs gemeinsam für die ressourcenarme und verwandte Sprache und (ii) ordnen sie der Zielsprache zu, um den endgültigen mehrsprachigen Raum aufzubauen. In unseren Experimenten konzentrieren wir uns auf Okzitanisch, eine ressourcenarme romanische Sprache, die aufgrund mangelnder Ressourcen oft vernachlässigt wird. Wir nutzen Daten aus Französisch, Spanisch und Katalanisch für das Training und evaluieren die okzitanisch-englische BLI-Aufgabe. Durch die Einbindung von unterstützenden Sprachen übertrifft unsere Methode bisherige Ansätze deutlich. Darüber hinaus zeigt unsere Analyse, dass der Grad der Verwandtschaft zwischen einer integrierten Sprache und der ressourcenarmen Sprache von entscheidender Bedeutung ist.', 'fa': 'جمع کردن کلمه\u200cهای مختلف زبان (CLWEs) برای کار\u200cهای پرداخت زبان طبیعی، مثال فعالیت زبان\u200cهای دو زبان (BLI) لازم نیست. ولی کمبود داده\u200cها اغلب کیفیت نمایش\u200cها را تأثیر می\u200cدهد. روش\u200cهای متفاوتی که نیاز به supervision ضعیف از زبان\u200cهای متفاوتی دارند پیشنهاد می\u200cشود، ولی روش\u200cهای فعلی هنوز نمی\u200cتواند CLWE خوب را برای زبان\u200cها با تنها یک کوچک کوچک یک کوچک زبان یاد بگیرند. بنابراین ما ادعا می\u200cکنیم که لازمه برای تحقیق مجموعه\u200cهای داده\u200cهای بیشتری برای توسعه\u200cهای CLWE در تنظیم منابع کم تحقیق کنیم. در این کاغذ ما پیشنهاد می کنیم که داده های زبانهای منابع بالا مرتبط باشند. در مقابل روش های قبلی که با استفاده از ابتدایی های پیش از آموزش زبان استفاده می کنند، ما (i) CLWEs را برای منابع کم و یک زبان ارتباط با هم آموزش می دهیم و (ii) آنها را به زبان هدف نقشه می دهیم تا آخرین فضای متعدد زبان بسازیم. در آزمایشات ما روی اکسیتان تمرکز می کنیم، یک زبان رومانسی کم منبع که اغلب به خاطر کمبود منابع غافل می شود. ما داده های فرانسوی، اسپانیایی و کاتالان را برای آموزش و ارزیابی در کار BLI اکسیتان-انگلیسی تحویل می دهیم. با شامل پشتیبانی زبانها روش ما با یک مرز بزرگ نزدیک قبلی را انجام می دهد. علاوه بر این، تحلیل ما نشان می دهد که درجه ارتباط بین یک زبان جمع شده و زبان کم منابع به طور منظور مهم است.', 'sw': 'Maneno ya lugha ya Cross (CLWEs) imethibitisha kuwa haina uhakika kwa kazi mbalimbali za utaratibu wa lugha za asili, kwa mfano, uzalishaji wa lexico wa lugha mbili (BLI). Hata hivyo, ukosefu wa data mara nyingi unaathiri ubora wa uwakilishi. Matokeo mbalimbali yanayohitaji tu kudhibiti lugha ngumu tu yalipendekezwa, lakini mbinu za sasa bado zinashindwa kujifunza vyema CLWS kwa lugha zenye makampuni madogo tu ya lugha. We therefore claim that it is necessary to explore further datasets to improve CLWEs in low-resource setups.  Katika karatasi hii tunapendekeza kuingiza taarifa za lugha zinazohusiana na rasilimali za juu. Tofauti na mbinu zilizopita ambazo zinatumia vifaa vya lugha kwa uhuru wa kujifunza, sisi (I) tunafundisha CLWEs kwa rasilimali chini na lugha inayohusiana pamoja na (i i) kuwaramani lugha ya lengo ili kujenga nafasi ya mwisho ya lugha za lugha. Katika majaribio yetu tunajikita kwa Occitan, lugha yenye rasilimali duni ya KiRomania ambayo mara nyingi hupuuzwa kutokana na ukosefu wa rasilimali. Tunatoa taarifa kutoka kwa Kifaransa, Kihispania na Kikatalani kwa ajili ya mafunzo na kutathmini kazi ya Kiingereza ya BLI. Kwa kuunganisha lugha za kuunga mkono mbinu yetu inaonyesha hatua zilizopita na msingi mkubwa. Zaidi ya hayo, uchambuzi wetu unaonyesha kuwa kiwango cha mahusiano kati ya lugha inayojumuishwa na lugha ya rasilimali ya chini ni muhimu sana.', 'af': "Kruistale woord inbêdings (CLWEs) het indispensaabel bevestig vir verskeie natuurlike taal verwerking opdragte, bv. twee tale lexicon induksie (BLI). Maar die ontbreek van data het dikwels die kwaliteit van verteenwoordigings invloek. Verskeie toegange wat slegs swak kruistale supervisie nodig het, is voorgestel, maar huidige metodes het nog nie gevaal om goeie CLWES te leer vir tale met slegs 'n klein monotale korpus. Ons sê daarom dat dit nodig is om verdere datastelle te ondersoek om CLWES in lae- hulpbron opstelling te verbeter. In hierdie papier voorstel ons om data van verwante hoë-hulpbron tale te inkorporeer. In contrast to previous approaches which leverage independently pre-trained embedding of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space. In ons eksperimente fokus ons op Occitan, 'n lae hulpbron Romanse taal wat dikwels verwerp word vanweë gebrek van hulpbronne. Ons verwyder data van Frans, Spaanse en Katalaanse vir onderwerp en evalueer op die Occitan-Engelse BLI taak. Deur die inkorporering van ondersteunde tale, uitvoer ons metode vorige toegang deur 'n groot marge. Ons analisie wys ook dat die grade van verwantigheid tussen 'n inkorporeerde taal en die lae hulpbron taal krities belangrik is.", 'sq': 'Përmbajtja e fjalëve ndërgjuhësore (CLWEs) është provuar e domosdoshme për detyra të ndryshme natyrore të procesimit të gjuhës, për shembull induktimi i lexikonëve dygjuhësore (BLI). Megjithatë, mungesa e të dhënave shpesh dëmton cilësinë e përfaqësimeve. U propozuan metoda të ndryshme që kërkojnë vetëm mbikqyrje të dobët ndërgjuhësore, por metodat aktuale ende nuk mësojnë CLWEs të mira për gjuhët me vetëm një korpus të vogël monogjuhësor. Prandaj pretendojmë se është e nevojshme të eksplorohen grupe të dhënash të mëtejshme për të përmirësuar CLWEs në konfigurime me burime të ulëta. In this paper we propose to incorporate data of related high-resource languages.  In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space.  Në eksperimentet tona ne përqëndrohemi në Okcitan, një gjuhë rumune me burime të ulëta që shpesh harrohet për shkak të mungesës së burimeve. Ne përdorim të dhënat nga francezët, spanjollët dhe katalanët për trajnimin dhe vlerësojmë detyrën e BLI-anglisht-okcitan. Duke përfshirë gjuhët mbështetëse metoda jonë mbizotëron metodat e mëparshme me një margin të madh. Përveç kësaj, analiza jonë tregon se shkalli i lidhjes midis një gjuhe të përfshirë dhe gjuhës me burime të ulëta është kritikisht i rëndësishëm.', 'tr': "Çot-dilli söz integrasy (CLWEs) Ýöne maglumatlaryň ýok bolmagynyň köplenç täzelikleriniň keyfiyetine täsirleýär. Diňe çapdaky diller gözlemegine gerek birnäçe golaýlar teklip edildi, ýöne häzirki metodlar diňe kiçi bir monolingua korpus bilen gowy CLWE öwrenmek üçin başaryp bilmeýär. Şol sebäpli biz CLWE'i aşak resurslar düzeltmek üçin ýene bir dataý setirlerini keşfedmek gerek diýip pikir edýäris. Bu kagyzda biz esasy ýokary resurslar dili bilen maglumatlary bölmek teklip edýäris Öňki ýagdaýlaryň özlerinden öň-okuwçy dillerden täsir etmäge mülaýyp, biz (i) i ň az resurslar üçin CLWEs we ýagdaýlaşyk diline gollaşdyrýarys we (ii) olaryň iň soňky multi-dil seleňini gurmak üçin maksady diline tablýarys. Biziň deneylerimizde okcitan diline üns berýäris, çeşmeleriň ýok bolmagyna sebäbi köplenç ýok bir roman dili. Biz fransuzça, espanyola we katalança maglumatlary okcitan-iňlisçe BLI işi üçin ýüze çykarýarys. Dillerimizi gollaşdyrmak üçin öňki gabdaly gabdaly ýagdaýa çykar. Munuň üçin biziň analyzamyz çykyş dili we iň az resurslar dili arasyndaky aýratynyň düýpüni möhüm bolandygyny görkezýär.", 'am': 'የቋንቋ-ቋንቋ ቃላት (CLWEs) የተለየ የፍጥረት ቋንቋ ማቀናቀፍ ስራዎችን ለባሕላዊ ቋንቋ ማቀናቀል ጉዳይ የለውም፡፡ ነገር ግን የዳታ ጎደለበት ብዙ ጊዜ የአካላትን ብልሃት ይጎድላል፡፡ የደካማ የቋንቋ ቋንቋ ብቻ መግለጫ የሚያስፈልጋቸው የልዩ ደረጃዎች ግን አሁን ግን ለቋንቋዎች ጥሩ CLWEs ለመማር አይችልም፤ ትንሽ ንጹሕ በቋንቋ ቋንቋ ብቻ ነው፡፡ ስለዚህ የCLWEs ከታናሽ resource ጉዳዮች ውስጥ ለማሻሻል ሌላ ዳታዎችን መፍጠር ያስፈልጋል ብለን እናስባለን፡፡ በዚህ ጋዜጠኛ ላይ የረጅም የኩነቶች ቋንቋዎች መረጃዎችን ለማሳሰብ እናስጀጋለን፡፡ ለቀድሞው ደረጃዎች ብልሃት ለቋንቋዎች ግንኙነትን ለመቀበል እና ለጥቂት resource እና የተገኘ ቋንቋ እና (i i) የኋለኛውን የቋንቋ ቋንቋ ለመሥራት ለሚያሳየው ቋንቋ ክፍል እና ለክፍል ቋንቋ ማረጃ እና (ii) ክፍተት እናደርጋቸዋለን፡፡ በተፈተናችን ውስጥ የጎደለው የሮማሲ ቋንቋ የኦክቲታንን ላይ እናስከትናለን፡፡ ከፈረንሳይ፣ ስፓኒሽ እና ካቴላን የኦክቲታን-እንግሊዘኛ ቢሊ አድራጊውን ለማስተማርና እናዋርዳለን፡፡ ቋንቋዎችን በመጠቀም የፊተኛውን ደረጃዎች በተለየ ትልቅ ማረፊያ አካባቢ ነው፡፡ በተጨማሪም፣ ማስታወቂያችን በአካባቢው ቋንቋ እና በሚታወቀው የኩነቶች ቋንቋ ግንኙነት ግንኙነት ያስታያል፡፡', 'az': "Çift dilli sözlər inbingəsi (CLWEs) müxtəlif təbiətli dil işləmə işləri üçün münasibdir, məsələn, iki dilli lexicon induqsiyası (BLI). Ancaq məlumatların yoxduğu tərzdə təsirlərin keyfiyyətinə bəsləyir. Müxtəlif tərzlərin yalnız çox dil gözləməsi üçün zəif tələb edilməsi təklif edildi, amma şiddətli tərzlərin yalnız kiçik bir monodil korpusu ilə yaxşı CLWE dillərini öyrənməsi mümkün deyildir. Buna görə də, CLWE'nin düşük ressurs qurğularının düzəltməsi üçün daha çox verilən qurğuları keşfetmək lazım olduğunu iddia edirik. Bu kağızda, əlaqəsiz yüksək ressurs dillərinin məlumatlarını birləşdirmək təklif edirik. Əvvəlki təhsil edilmiş dillərin birlikdə təhsil edilməsini təhsil edən əvvəlki təhsil metodlarına baxmayaraq, biz (i) düşük ressurslar üçün CLWEs təhsil edirik və birlikdə əlaqəsiz dil ilə birlikdə təhsil edirik və ii) onları son çoxlu dil kosmosunu inşa etmək üçün məqsəd dilinə mapa edirik. Bizim təcrübələrimizdə Occitan dilinə odaqlanırıq ki, çox çox səbəb resursu yoxdur. Biz Fransızca, İspanyolca və Katalandan verilən məlumatları Occitan-İngilizce BLI işində təhsil etmək və değerləşdirmək üçün istifadə edirik. Dillərimizi dəstəkləndirəcək məsələmiz əvvəlkilərin yaxınlıqlarını böyük bir margin ilə dəstəkləndirir. Daha sonra, analizimiz belə göstərir ki, birləşdirilmiş dil və düşük ressurs dili arasındakı bağlılıq dərəcəsi kritik olaraq mövcuddur.", 'hy': 'Երլեզվային բառերի ներգրավումը (LCWEs) պարզվեց անհրաժեշտ տարբեր բնական լեզվի վերլուծության խնդիրների համար, օրինակ երկլեզվային լեքսիկոնի ինդուկցիայի համար: Այնուամենայնիվ, տվյալների բացակայությունը հաճախ վնասում է ներկայացումների որակը: Մի քանի մոտեցումներ էին առաջարկում, որոնք պահանջում էին միայն թույլ լեզվի միջև վերահսկողության, բայց ներկայիս մեթոդները դեռևս չեն սովորում լավ լեզուներ միայն փոքր միալեզվի կորպուսով: Հետևաբար մենք պնդում ենք, որ անհրաժեշտ է հետազոտել ավելի շատ տվյալների համակարգեր, որպեսզի բարելավեն ցածր ռեսուրսների կառուցվածքները: Այս թղթի մեջ մենք առաջարկում ենք ներառել տվյալներ կապված բարձր ռեսուրսների լեզուների մասին: Ի հակադրություն նախորդ մոտեցումներին, որոնք ազդում են անկախ նախապատրաստված լեզվի ներդրումներին, մենք i) սովորեցնում ենք ԿԼՈԵ-ներին ցածր ռեսուրսների և կապված լեզվի համար միասին, և i) քարտեզագրում ենք դրանք նպատակային լեզվին վերջնական բազլեզու տարած Մեր փորձարկումներում մենք կենտրոնանում ենք օքսիտացի վրա, որը ցածր ռեսուրսներ ունի ռոմանական լեզու, որը հաճախ անտեսվում է ռեսուրսների պակասի պատճառով: Մենք օգտագործում ենք ֆրանսիացի, իսպաներենի և կատալանի տվյալները օքսիտացի-անգլերենի ԲԼԻ խնդիրների ուսուցման և գնահատման համար: Մեր մեթոդը, ներառելով լեզուների աջակցությունը, շատ ավելի լավ է ընդունում նախորդ մոտեցումները: Ավելին, մեր վերլուծությունը ցույց է տալիս, որ ներառված լեզուների և ցածր ռեսուրսների լեզուների միջև կապվածության մակարդակը կարևոր է:', 'ca': "L'incorporació de paraules translingües (CLWEs) s'ha demostrat indispensable per diverses tasques naturals de processament de llenguatges, per exemple, l'inducció bilingüe de lexicòns (BLI). Però la manca de dades sovint afecta la qualitat de les representacions. Es van proposar diversos enfocaments que només necessitaven una supervisió translingüística dèbil, però els mètodes actuals encara no aprenen bons CLWEs per les llengües amb només un petit corpus monolingüe. Per tant, afirmem que és necessari explorar més conjunts de dades per millorar les CLWEs en configuracions de baix recursos. En aquest paper proposem incorporar dades de llengües relacionades amb molts recursos. A diferència dels enfocaments anteriors que utilitzen integracions de llengües formades de manera independent, i) capacitem CLWEs per a les llengües amb baix recursos i una llengüe relacionada conjuntament i ii) les mapem al llengüe d'objectiu per construir l'espai multilingüe final. In our experiments we focus on Occitan, a low-resource Romance language which is often neglected due to lack of resources.  Utilitzem dades de francès, espanyol i català per a formar-nos i evaluem la tasca BLI occitana-anglesa. Incorporant llengües de suport, el nostre mètode supera els enfocaments anteriors en gran marge. A més, la nostra an àlisi mostra que el grau de relació entre una llengua incorporada i la llengua amb baix recursos és críticament important.", 'bn': 'ক্রস-ভাষাভাষী শব্দের বিভিন্ন প্রাকৃতিক ভাষা প্রক্রিয়ার কাজের জন্য প্রমাণ করেছে যেমন দুই ভাষা লেক্সিকোর শিল্প (বিলি)। তবে তথ্যের অভাব প্রায়শই প্রতিনিধিত্বের মানে ক্ষতিগ্রস্ত হয়। কিন্তু বর্তমান পদ্ধতি শুধুমাত্র একটি ছোট্ট মোনোলিভাল কোর্পাসের মাধ্যমে ভাষার জন্য সিএলউইএস ভাষায় ভাষা শিখতে ব্যর্থ। তাই আমরা দাবি করি যে সিএলউইএস নীচের সম্পদ সংক্রান্ত পরিস্থিতিতে আরো তথ্য বিশেষ করা দরকার। এই পত্রিকায় আমরা প্রস্তাব করছি যে সম্পর্কিত উচ্চ সম্পদ ভাষার তথ্য অন্তর্ভুক্ত করা। পূর্ববর্তী পদক্ষেপের বিপরীতে যা স্বাধীন প্রশিক্ষিত ভাষার পূর্বে প্রশিক্ষিত বিভিন্ন ভাষার স্বাধীন ভাষার প্রতিযোগিতার বিপরীতে, আমরা (ই) নীচের সম্পদ এবং সম্পর্ক আমাদের পরীক্ষায় আমরা অক্সিটানের উপর মনোযোগ দিচ্ছি একটি কম সম্পদ রোমান্স ভাষার উপর যা প্রায়শই সম্পদের অভাবের কারণে অবহেলা হয়। আমরা ফ্রেঞ্চ, স্প্যানিশ এবং ক্যাটালান থেকে তথ্য প্রদান করি অক্সিটান-ইংরেজি বিলি কাজের প্রশিক্ষণ ও মূল্যায়নে ভাষাকে সমর্থন করার মাধ্যমে আমাদের পূর্ববর্তী মার্গিনের মাধ্যমে আমাদের পদ্ধতি প্রদর্শন করেছে। এছাড়াও, আমাদের বিশ্লেষণ দেখাচ্ছে যে একটি অন্তর্ভুক্ত ভাষা এবং কম সম্পদ ভাষা গুরুত্বপূর্ণ গুরুত্বপূর্ণ।', 'cs': 'Cross-jazyčné vložení slov (CLWE) se ukázalo jako nepostradatelné pro různé úkoly zpracování přirozeného jazyka, např. indukci dvojjazyčného lexikonu (BLI). Nedostatek údajů však často narušuje kvalitu reprezentací. Byly navrženy různé přístupy vyžadující pouze slabý cross-jazyčný dohled, ale současné metody se stále nedaří naučit dobré CLWE pro jazyky s malým jednojjazyčným korpusem. Proto tvrdíme, že je nezbytné prozkoumat další datové sady pro zlepšení CLWE v nastaveních s nízkými zdroji. V tomto článku navrhujeme začlenit data souvisejících jazyků s vysokými zdroji. Na rozdíl od předchozích přístupů, které využívají nezávisle předškolené vložení jazyků, společně (i) školíme CLWE pro nízké zdroje a související jazyk a (ii) mapujeme je do cílového jazyka, abychom vytvořili konečný vícejazyčný prostor. V našich experimentech se zaměřujeme na okcitánštinu, romantický jazyk s nízkými zdroji, který je často opomíjený kvůli nedostatku zdrojů. Využíváme data z francouzštiny, španělštiny a katalánštiny pro školení a vyhodnocování úkolu BLI v okcitánsko-anglickém jazyce. Začleněním podpůrných jazyků naše metoda výrazně překonává předchozí přístupy. Dále naše analýza ukazuje, že stupeň souvislosti mezi začleněným jazykem a jazykem s nízkými zdroji je kriticky důležitá.', 'et': 'Keeleülesed sõnade manustamine (CLWE) on osutunud hädavajalikuks mitmesuguste looduskeele töötlemise ülesannete, nt kakskeelse leksikoni induktsiooni (BLI) puhul. Andmete puudumine kahjustab aga sageli esituste kvaliteeti. Pakuti välja mitmesuguseid lähenemisviise, mis nõuavad ainult nõrka keeleülest järelevalvet, kuid praegused meetodid ei õpi ikka veel häid keelte puhul, kus on vaid väike ühekeelne korpus. Seetõttu väidame, et väikeste ressurssidega ettevõtete puhul on vaja uurida täiendavaid andmekogumeid. Käesolevas dokumendis teeme ettepaneku lisada andmed seotud suure ressursiga keeltest. Erinevalt varasematest lähenemisviisidest, mis võimaldavad sõltumatult eelnevalt koolitatud keelte manustamist, koolitame i) ühiselt väikese ressursi ja sellega seotud keele jaoks koolitust CLWE-sid ning ii) kaardistame nad sihtkeelega, et luua lõplik mitmekeelne ruum. Oma eksperimentides keskendume oksitaanile, vähese ressursiga romaani keelele, mida ressursside puudumise tõttu sageli eiratakse. Me kasutame prantsuse, hispaania ja katalaani andmeid, et koolitada ja hinnata oktsitaani-inglise BLI ülesannet. Tugikeelte kaasamisega ületab meie meetod oluliselt varasemaid lähenemisviise. Lisaks näitab meie analüüs, et seostatavuse aste ühendatud keele ja vähese ressursiga keele vahel on kriitiliselt oluline.', 'fi': 'Monikieliset sanaupotukset (CLWE) ovat osoittautuneet välttämättömiksi erilaisissa luonnollisen kielen käsittelytehtävissä, kuten kaksikielisessä sanaston induktiossa (BLI). Tietojen puute heikentää kuitenkin usein esitysten laatua. Ehdotettiin erilaisia lähestymistapoja, jotka edellyttävät vain heikkoa monikielistä valvontaa, mutta nykyisillä menetelmillä ei edelleenkään opita hyviä kieliä, joilla on vain pieni yksikielinen korpus. Siksi väitämme, että on tarpeen tutkia lisää tietokokonaisuuksia CLWE:n parantamiseksi vähävaraisissa kokoonpanoissa. Tässä artikkelissa ehdotamme, että sisällytetään tietoja asiaan liittyvistä korkean resurssin kielistä. Toisin kuin aiemmissa lähestymistavoissa, joissa hyödynnetään itsenäisesti ennalta koulutettuja kielten upotuksia, i) koulutamme yhdessä vähävaraisia kieliä ja siihen liittyvää kieltä varten ja ii) kartoitamme ne kohdekielelle lopullisen monikielisen tilan rakentamiseksi. Kokeissamme keskitymme oksitaaniin, vähävaraiseen romaaniseen kieleen, jota usein laiminlyödään resurssien puutteen vuoksi. Hyödynnämme ranskan, espanjan ja katalaanin tietoja oksitaani-englanti BLI-tehtävän koulutukseen ja arviointiin. Käyttämällä tukevia kieliä menetelmämme on huomattavasti parempi kuin aiemmat lähestymistavat. Lisäksi analyysimme osoittaa, että sulautetun kielen ja vähävaraisen kielen välinen yhteys on ratkaisevan tärkeää.', 'bs': 'Krozjezički integracija riječi (CLWEs) dokazala je neophodno za različite prirodne zadatke obrađivanja jezika, na primjer indukcija dvojezičkih leksiona (BLI). Međutim, nedostatak podataka često ukazuje na kvalitet predstavljanja. Predloženi su razni pristupi koji zahtijevaju samo slabu međujezičku nadzor, ali trenutne metode još uvijek ne nauče dobre CLWE za jezike sa samo malim monojezičkim korpusom. Stoga tvrdimo da je potrebno istražiti daljnje komplete podataka kako bi poboljšali CLWE u nastavama niskih resursa. U ovom papiru predlažemo da uključimo podatke o povezanim jezicima visokog resursa. Za suprotnost prethodnim pristupima koji utječu na nezavisno predobučene integracije jezika, i) treniramo CLWEs za niske resurse i povezan jezik zajedno i ii) ih mapiramo na ciljni jezik kako bi izgradili konačni multijezički prostor. U našim eksperimentima fokusiramo se na Okcitan, mali rimunski jezik koji se često zanemari zbog nedostatka resursa. Učinimo podatke od francuskog, španjolskog i katalanskog za obuku i procjenu occitanskog-engleskog BLI zadatka. Uključujući podršku jezika, naša metoda iznosi prethodne pristupe velikom marginom. Nadalje, naša analiza pokazuje da je stepenica povezanosti između uključenog jezika i jezika niskog resursa kritično važna.', 'he': 'Cross-lingual word embeddings (CLWEs) have proven indispensable for various natural language processing tasks, e.g., bilingual lexicon induction (BLI).  עם זאת, חוסר נתונים לעתים קרובות משפיל את איכות היציגות. הציעו גישות שונות שדורשות רק פיקוח חולש בין שפות, אך השיטות הנוכחיות עדיין לא לומדות CLWEs טובות לשפות עם רק קופוס מונושפתי קטן. לכן אנחנו טוענים שצריך לחקור קבוצות מידע נוספות כדי לשפר את CLWEs במערכות משאבים נמוכים. בעיתון הזה אנו מציעים להשתלב נתונים של שפות משאבים גבוהים. בניגוד לשיטות קודמות שמשתמשות בשפתים מאומנות מראש באופן עצמאי, אנו (i) מאמן את CLWEs למשאבים נמוכים ושפת הקשורה ביחד ו (ii) מפה אותם לשפת המטרה כדי לבנות את המרחב הרב-שפתי הסופי. בניסויים שלנו אנחנו מתמקדים באוקציטני, שפה רומנטית עם משאבים נמוכים שלעתים קרובות נזנחת בגלל חוסר משאבים. אנחנו משתמשים במידע מצרפתי, ספרדית וקטלאנית לאימון ולעריך על משימה BLI אוקיטני-אנגלית. על ידי שילוב שפות תמיכה השיטה שלנו עוברת גישות קודמות על ידי גבול גדול. Furthermore, our analysis shows that the degree of relatedness between an incorporated language and the low-resource language is critically important.', 'ha': "KCharselect unicode block name A lokacin da, ƙarancin data ko da yawa, yana shagala sifar shaidar. An buƙata hanyõyi mãsu yawa wanda ke nufin tsaron sararin-harshen kawai na rauni, kuma amma, hanyõyin nan yanzu ba za'a iya fahimtar da amfani da CLWEs masu cikin harshen da ƙarami kawai. Saba'an nan, Munã dai cewa ana buɗe wasu database dõmin ya kyautata CLWEs cikin kayan-resource. Ga wannan takardan da Muke bukãta in haɗa data na lugha masu husũma na sarki-resource. Tsarin da suka zaman hanyõyin da aka rufe da shi wanda aka tsare masu fara-tunkuɗe da harshen na farko, sai mu (i) tunkuɗe CLWEs wa ƙarƙashin-resource da wata lugha na haɗi da shi da kuma (ii) karna su zuwa harshen goan da za'a gina filin na ƙarshen multilala. Daga jarrabõyinmu, munã fahimta zuwa ga Ocitan, wata harshen duniya-ma'anar Romiya da ake ƙyãma da yawa da kuma bã da mataimaki ba. Munã samun data daga Faransiya, Isbanish da Katalan dõmin yin wa'anar da kuma a kan aikin Ocitan-Ingiriya BLI. Ga ku haɗa cikin harshen da ke ƙarfafa, metodenmu na samar da zaɓen hanyoyin zaman da wani margin mai girma. Furan wannan, analyyinmu yana nũna cewa daraja na haɗi a tsakanin harshe da harshe na ƙasan-resource na muhimu.", 'sk': 'Medjezične besedne vdelave (CLWE) so se izkazale za nepogrešljive za različne naloge obdelave naravnega jezika, npr. dvojezično indukcijo leksikona (BLI). Vendar pa pomanjkanje podatkov pogosto slabi kakovost predstavitev. Predlagani so bili različni pristopi, ki zahtevajo le slab medjezični nadzor, vendar sedanje metode še vedno ne uspejo učiti dobrih CLWE za jezike z majhnim enojezičnim korpusom. Zato trdimo, da je treba raziskati nadaljnje nabore podatkov za izboljšanje CLWE v nastavitvah z nizkimi viri. V tem prispevku predlagamo vključitev podatkov o sorodnih jezikih z visokimi viri. V nasprotju s prejšnjimi pristopi, ki izkoriščajo neodvisno vnaprej usposobljeno vgradnjo jezikov, skupaj (i) usposabljamo CLWE za jezik z nizkimi viri in sorodnim jezikom ter (ii) jih preslikamo v ciljni jezik za gradnjo končnega večjezičnega prostora. V naših eksperimentih se osredotočamo na ocitanščino, romanski jezik z nizkimi viri, ki je pogosto zanemarjen zaradi pomanjkanja virov. Izkoriščamo podatke iz francoščine, španščine in katalonščine za usposabljanje in ocenjevanje naloge okcitansko-angleščine BLI. Z vključitvijo podpornih jezikov je naša metoda precej boljša od prejšnjih pristopov. Poleg tega naša analiza kaže, da je stopnja povezanosti med vključenim jezikom in jezikom z nizkimi viri kritično pomembna.', 'jv': 'ProgressBarUpdates Nanging, kulang data kang dipunangé kuwi nggawe kalite lagi mesthi. @item Text character set Kaya, awak dhéwé ngerasakno ngono cah-cah dumadhi kanggo disindelok dadi sing luwih dumadhi kanggo nggawe CLWs nang otote apa-perusahaan sing kakas Nang kuwi iki, kita supoyata mengko nggawe data kang angkang luwih-ingkang luwih dumadhi Ingkang panggayaan karo akeh sing dibutungan langkung sampeyan luwih dumadhi, awake dhewe (i) luwih CLWs kanggo kelas-nescen lan bantuan ingkang dipun Awak dhéwé éntuk perbudhakan kanggo Kemerdekaan Oksitan, langa kuwi wis kelas nang resuran rumulang sing kudu nggagaan kuwi mau. Awak dhéwé ngejaraké data seng Perancis, Kasilé karo Catalé kanggo nggawe geranglangno karo nggawe barang nggambar barang-inggilis. Gabok Lah bener, akeh-akeh sing ngerasakno meraké awak dhéwé kuwi tindang nggawe gerakan karo langa sing basa gambar lan akeh podho sing nggawe kudu berarti.', 'bo': 'Cross-lingual word embeddings (CLWEs)have proven indispensable for various natural language processing tasks, e.g. bilingual lexicon induction (BLI). ཡིན་ནའང་། ཆ་འཕྲིན་དེ་དག་པས་རྒྱུན་ལྡན་གྱི་ཞབས་ཏོག་ཏུ་ཉུང་བ་ཡིན་པས། སྐད་ཡིག་ཆ་མཐོང་ཚད་ལྡན་པ་ལས་འཕགས་པའི་ལྟ་བ་དག་ལས་ཕན་ཐོགས་ཅིག་བྱུང་བ་རེད། ད་ལྟོའི་ལམ་ལུགས་འདི་སྔོན་སྒྲིག་གི་ཐབས་ལམ་ལྟར་ དེར་བརྟེན། ང་ཚོས་རང་ཉིད་ཀྱི་རྒྱུ་དངོས་སྒྲིག་འཛུགས་ཀྱི་ནང་དུ་ཉེན་ཁ་ཡིག་ཆ་ལེན་སྐྱོང་བྱེད་དགོས་པ་དང་། ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་རྒྱུ་དངོས་ཐོག་ཁུངས་ཀྱི་སྐད་རིགས་མཐོ་དམ་ཀྱི་ཆ་འཕྲིན་ཡིག སྐད་རིགས་ཀྱི་ནང་ཚན་འདི་སྔོན་གྱི་ཐབས་ལམ་ལ་བསྟེན་ནས་རང་ཉིད་ཀྱིས་ཕར་སྒྲིག་འཛུགས་ཀྱི་ཐབས་ལམ་དང་མཐུན་པའི་སྐད་ཡིག་ཅིག་བསྡུར་ན། ང་ཚོའི་སྒེར་ཞུགས་ནང་དུ་ང་ཚོར་Occitan(Occitan)ལ་གནད་དོན་མེད་ཅིག་ཡིན་པས། རྣམ་འབྱུང་ཆེན་ཉུང་བའི་སྐད་ཡིག ང་ཚོས་རྒྱལ་ཁབ་དང་། སྐད་ཡིག་དང་། སྐད་ཡིག་དང་། ཀེ་ཊ་ལཱན་ནས་གནད་སྡུད་ནང་གི་ཚད་ལྟར་བྱེད་ཀྱི་ཡོད། ང་ཚོའི་ཐབས་ལམ་ལུགས་རྒྱབ་སྐྱོར་བའི་སྐད་རིགས འོན་ཀྱང་། ང་ཚོའི་དབྱེ་ཞིབ་དཔྱད་ནི་སྐད་རིགས་འདུག་དང་རྐྱེན་འབྲེལ་ཆེ་བའི་སྣ་བར་མཐུན་ཚད་གལ་ཆེན་རྐྱེན་ཏུ་ཆེན་'}
{'en': 'Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training', 'pt': 'Reconhecimento de Emoções Multilíngue e Multilabel usando Treinamento Adversarial Virtual', 'ar': 'التعرف على المشاعر متعدد اللغات ومتعدد العلامات باستخدام التدريب العدائي الافتراضي', 'fr': "Reconnaissance des émotions multilingue et multiétiquette à l'aide de la formation antagoniste", 'es': 'Reconocimiento de emociones multilingüe y multietiqueta mediante el entrenamiento virtual de adversarios', 'ja': 'バーチャルアドバサリアルトレーニングを使用した多言語・多重ラベル感情認識', 'zh': '用虚拟抗训练者多语言及多标识', 'hi': 'बहुभाषी और बहुभाषी भावना पहचान आभासी प्रतिकूल प्रशिक्षण का उपयोग कर', 'ru': 'Многоязычное и многоуровневое распознавание эмоций с помощью виртуального обучения соперников', 'ga': 'Aitheantas Mothúcháin Ilteangach agus Illipéad ag baint úsáide as Oiliúint Sáraíochta Fhíorúil', 'ka': 'Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training', 'el': 'Πολυγλωσσική και Πολυσήμαντη Συναισθηματική Αναγνώριση με την Εικονική Κατάρτιση Ανταγωνιστών', 'hu': 'Többnyelvű és többcímkés érzelmi felismerés virtuális adversarial tréning segítségével', 'it': 'Riconoscimento emotivo multilingue e multilabel con formazione adversariale virtuale', 'lt': 'Daugiakalbis ir daugiakalbis emocijos pripažinimas naudojant virtualų nepalankų mokymą', 'kk': 'Виртуалды конверсариялық оқыту арқылы көп тілді және көп белгілер көңіл көңіл түсініктемесі', 'mk': 'Мултијазичко и мултиетикетно препознавање на емоции користејќи виртуелно непријатно тренирање', 'ml': 'വിര്\u200dച്ച്വല്\u200d അഡ്വാര്\u200dസറിയല്\u200d പരിശീലനം ഉപയോഗിച്ച് പല ഭാഷകളും Multilabel Emotion Recognition', 'mt': 'Rikonoxximent ta’ Emozzjoni Multilingwi u Multitikketta bl-użu ta’ Taħriġ Adversarju Virtwali', 'mn': 'Олон хэл болон олон тэмдэгтийн мэдрэмжүүдийг виртуал дасгал суралцах', 'no': 'Fleirspråk - og fleirmerkelapp fjernkonkjenning ved bruk av virtuelt rekursarial trening', 'pl': 'Wielojęzyczne i wieloznaczne rozpoznawanie emocji przy użyciu wirtualnego szkolenia przeciwnika', 'ro': 'Recunoașterea emoțională multilingvă și multietichetă folosind instruirea adversară virtuală', 'so': 'Aqoonsashada luuqadaha kala duduwan iyo ogeysiiska iskuulka farsamada', 'si': 'ගොඩක් භාෂාවක් සහ ගොඩක් ලේබල් Emotion අඳුරගන්න', 'sr': 'Poznavanje emocija višejezičkih i višeoznačenih jezika koristeći virtualnu reklamnu obuku', 'sv': 'Flerspråkig och flerspråkig känsloigenkänning med virtuell adversarial träning', 'ms': 'Pengenalan Emosi Berbahasa dan Label Berberbilang menggunakan Latihan Melawan Virtual', 'ta': 'மெய்நிகர் மேம்பாடு பயிற்சி பயன்படுத்தி பல மொழி மற்றும் பல்லெட் உணர்வு அறிவிப்பு', 'ur': 'Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training', 'vi': 'Phát hiện cảm xúc đa ngôn ngữ và đa nhãn qua Hình thức Cơ Đốc...', 'uz': 'Name', 'da': 'Flersproget og flermærket følelsesgenkendelse ved hjælp af virtuel adversarial træning', 'nl': 'Meertalige en meertalige emotionele herkenning met behulp van Virtual Adversarial Training', 'hr': 'Prepoznavanje emocija višejezičkih i višeoznačenih primjena koristeći virtualnu poremećajnu obuku', 'ko': '가상 대항 훈련을 바탕으로 하는 다중 언어 다중 라벨 정서 식별', 'de': 'Mehrsprachige und mehrsprachige Emotionserkennung mittels Virtual Adversarial Training', 'bg': 'Многоезично и многоетикетно разпознаване на емоции чрез виртуално противоречиво обучение', 'fa': 'تعریف احساسات زیادی زبان و زیادی برچسب با استفاده از آموزش تجاوز مجازی', 'sw': 'Tambulisho la lugha nyingi na hisia nyingine kwa kutumia mafunzo ya Tafsiri', 'af': 'Multilingual en Multilabel Emotie Herkenning gebruik Virtuele Adversarial Oefening', 'sq': 'Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training', 'id': 'Pengenalan Emosi Multibahasa dan Multilabel menggunakan Latihan Adversial Virtual', 'am': 'የቋንቋ እና Multilabel Emotion Recognition using Virtual Adversarial Training', 'hy': 'Բազլեզու և բազմապիտակ զգացմունքների ճանաչման օգտագործելով վիրտուալ հակառակ վարժություն', 'az': 'Virtual Adversarial Training', 'bn': 'Name', 'tr': 'Çoklu diller we köp-etiket Emoýunlar Taýýarlamak Virtual Adversarial Training ullanýar', 'bs': 'Prepoznavanje emocija višejezičkih i višeoznačenih primjena koristeći virtualnu reklamnu obuku', 'ca': "Recognició d'emocions multillengües i multietiquetes utilitzant formació adversaria virtual", 'et': 'Mitmekeelne ja mitmemärgiseline emotsioonide äratundmine virtuaalse kõrvaltoolituse abil', 'fi': 'Monikielinen ja monikielinen tunteiden tunnistus virtuaalisen haittavaikutuskoulutuksen avulla', 'cs': 'Vícejazyčné a multilabel rozpoznávání emocí pomocí virtuálního nepřátelského tréninku', 'ha': 'KCharselect unicode block name', 'jv': 'Multilanggar lan Multilabel emuation Rasanjut Ngawe nguwe Worksal Advertary Learning', 'sk': 'Večjezično in večoznačno prepoznavanje čustev z virtualnim neželenim usposabljanjem', 'he': 'Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training', 'bo': 'Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training'}
{'en': 'Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of ', 'ar': 'كان التدريب العدائي الافتراضي (VAT) فعالاً في تعلم نماذج قوية في ظل إعدادات خاضعة للإشراف وشبه خاضعة للإشراف لكل من رؤية الكمبيوتر ومهام البرمجة اللغوية العصبية. ومع ذلك ، لم يتم استكشاف فعالية ضريبة القيمة المضافة في التعرف على المشاعر بلغات متعددة ومتعددة العلامات من قبل. في هذا العمل ، نستكشف ضريبة القيمة المضافة للتعرف على المشاعر متعددة العلامات مع التركيز على الاستفادة من البيانات غير الموسومة من لغات مختلفة لتحسين أداء النموذج. نجري تجارب مكثفة شبه خاضعة للإشراف على مجموعة بيانات SemEval2018 متعددة العلامات ومتعددة اللغات للتعرف على المشاعر وتظهر مكاسب في الأداء بنسبة 6.2٪ (العربية) و 3.8٪ (الإسبانية) و 1.8٪ (الإنجليزية) على التعلم الخاضع للإشراف بنفس المقدار من البيانات المصنفة (10٪ من بيانات التدريب). نقوم أيضًا بتحسين أحدث التقنيات الحالية بنسبة 7٪ و 4.5٪ و 1٪ (مؤشر Jaccard) للإسبانية والعربية والإنجليزية على التوالي وإجراء تجارب استقصائية لفهم تأثير الطبقات المختلفة للنماذج السياقية.', 'fr': "La formation antagoniste virtuelle (VAT) s'est avérée efficace pour l'apprentissage de modèles robustes dans des environnements supervisés et semi-supervisés pour les tâches de vision par ordinateur et de PNL. Cependant, l'efficacité de la TVA pour la reconnaissance des émotions multilingue et multilabel n'a jamais été explorée auparavant. Dans ce travail, nous explorons la TVA pour la reconnaissance des émotions en plusieurs étiquettes en mettant l'accent sur l'exploitation de données non étiquetées provenant de différentes langues afin d'améliorer les performances du modèle. Nous réalisons des expériences semi-supervisées approfondies sur l'ensemble de données de reconnaissance des émotions multi-étiquettes et multilingues SemeVal2018 et montrons des gains de performance de 6,2\xa0% (arabe), 3,8\xa0% (espagnol) et 1,8\xa0% (anglais) par rapport à l'apprentissage supervisé avec la même quantité de données étiquetées (10\xa0% des données d'entraînement). Nous améliorons également la technologie existante de 7\xa0%, 4,5\xa0% et 1\xa0% (indice Jaccard) pour l'espagnol, l'arabe et l'anglais respectivement et réalisons des expériences de sondage pour comprendre l'impact des différentes couches des modèles contextuels.", 'pt': 'O Virtual Adversarial Training (VAT) tem sido eficaz no aprendizado de modelos robustos sob configurações supervisionadas e semi-supervisionadas para tarefas de visão computacional e PNL. No entanto, a eficácia do VAT para reconhecimento de emoções multilíngue e multirrótulo não foi explorada antes. Neste trabalho, exploramos o VAT para reconhecimento de emoções multilabel com foco em alavancar dados não rotulados de diferentes linguagens para melhorar o desempenho do modelo. Realizamos extensos experimentos semissupervisionados no conjunto de dados multilabel e multilíngue de reconhecimento de emoções SemEval2018 e mostramos ganhos de desempenho de 6,2% (árabe), 3,8% (espanhol) e 1,8% (inglês) em relação ao aprendizado supervisionado com a mesma quantidade de dados rotulados (10% de dados de treinamento). Também melhoramos o estado da arte existente em 7%, 4,5% e 1% (Jaccard Index) para espanhol, árabe e inglês, respectivamente, e realizamos experimentos de sondagem para entender o impacto de diferentes camadas dos modelos contextuais.', 'es': 'El entrenamiento virtual de adversarios (IVA) ha sido eficaz para aprender modelos sólidos en entornos supervisados y semisupervisados tanto para tareas de visión artificial como de PNL. Sin embargo, la eficacia del IVA para el reconocimiento de emociones multilingüe y multietiqueta no se ha explorado antes. En este trabajo, exploramos el IVA para el reconocimiento de emociones multietiqueta con un enfoque en aprovechar los datos no etiquetados de diferentes idiomas para mejorar el rendimiento del modelo. Realizamos amplios experimentos semi-supervisados en el conjunto de datos de reconocimiento de emociones multilingüe y multietiqueta SemEval2018 y mostramos ganancias de rendimiento del 6,2% (árabe), 3,8% (español) y 1,8% (inglés) con respecto al aprendizaje supervisado con la misma cantidad de datos etiquetados (10% de los datos de entrenamiento). También mejoramos el estado actual de la técnica en un 7%, 4,5% y 1% (índice de Jaccard) para español, árabe e inglés, respectivamente, y realizamos experimentos de sondeo para comprender el impacto de las diferentes capas de los modelos contextuales.', 'zh': '虚拟抗训练(VAT)于计算机视NLP督半监置学鲁棒模形甚效。 然增值税多言多识有效性未尝探索。 此其事也,论情识之增值税,重者,以言语之未识数也。 吾等于SemEval2018多标与多情识数集上博督实验,显比同数(10%训练数),督学之性增6.2%(阿拉伯语),3.8%(西班牙语)与1.8%(英语)。 又将西班牙语之,阿拉伯语、英语之术各益其7%,4.5%1%(Jaccard Index)而探之实验以知上下文之所在。', 'ru': 'Виртуальное обучение соперничеству (НДС) было эффективным в изучении надежных моделей в контролируемых и полуконтролируемых настройках как для компьютерного зрения, так и для задач NLP. Однако эффективность НДС для многоязычного и многозначного распознавания эмоций ранее не изучалась. В этой работе мы исследуем НДС для многоуровневого распознавания эмоций с акцентом на использование немаркированных данных с разных языков для улучшения эффективности модели. Мы проводим обширные полуконтролируемые эксперименты с использованием набора данных для распознавания эмоций SemEval2018 с несколькими ярлыками и языками и демонстрируем рост эффективности на 6,2% (арабский язык), 3,8% (испанский язык) и 1,8% (английский язык) по сравнению с контролируемым обучением с таким же количеством маркированных данных (10% от данных обучения). Мы также улучшаем существующее современное состояние на 7%, 4,5% и 1% (индекс Жаккарда) для испанского, арабского и английского языков соответственно и проводим зондирующие эксперименты для понимания влияния различных слоев контекстных моделей.', 'ja': 'バーチャル対抗訓練（ VAT ）は、コンピュータビジョンとNLPタスクの両方の監督および半監督設定下で堅牢なモデルを学習するのに効果的です。しかし、多言語および多標識感情認識のためのVATの有効性は、これまでには検討されていませんでした。この研究では、モデルのパフォーマンスを向上させるために、さまざまな言語からのラベルなしデータを活用することに焦点を当てて、マルチラベル感情認識のためのVATを探ります。私たちは、SemEval 2018マルチラベルおよび多言語感情認識データセットで広範囲の半監修実験を行い、同量のラベル付きデータ（トレーニングデータの10%）を使用した監修学習よりも6.2%（アラビア語）、3.8%（スペイン語）、1.8%（英語）のパフォーマンス向上を示します。また、スペイン語、アラビア語、英語の既存の最先端モデルをそれぞれ7 ％、4.5 ％、1 ％ （ Jaccard Index ）向上させ、コンテキストモデルのさまざまなレイヤーの影響を理解するためのプロービング実験を実施します。', 'hi': 'Virtual Adversarial Training (VAT) कंप्यूटर दृष्टि और NLP कार्यों दोनों के लिए पर्यवेक्षित और अर्ध-पर्यवेक्षित सेटिंग्स के तहत मजबूत मॉडल सीखने में प्रभावी रहा है। हालांकि, बहुभाषी और बहु-लेबल भावना मान्यता के लिए VAT की प्रभावकारिता का पहले पता नहीं लगाया गया है। इस काम में, हम मॉडल प्रदर्शन में सुधार करने के लिए विभिन्न भाषाओं से अनलेबल किए गए डेटा का लाभ उठाने पर ध्यान केंद्रित करने के साथ मल्टीलेबल भावना मान्यता के लिए VAT का पता लगाते हैं। हम SemEval2018 मल्टीलेबल और बहुभाषी भावना पहचान डेटासेट पर व्यापक अर्ध-पर्यवेक्षित प्रयोग करते हैं और लेबल किए गए डेटा (प्रशिक्षण डेटा का 10%) की समान मात्रा के साथ पर्यवेक्षित सीखने पर 6.2% (अरबी), 3.8% (स्पेनिश) और 1.8% (अंग्रेजी) के प्रदर्शन लाभ दिखाते हैं। हम स्पेनिश, अरबी और अंग्रेजी के लिए क्रमशः 7%, 4.5% और 1% (जैककार्ड इंडेक्स) द्वारा मौजूदा अत्याधुनिक में भी सुधार करते हैं और प्रासंगिक मॉडल की विभिन्न परतों के प्रभाव को समझने के लिए जांच प्रयोग करते हैं।', 'ga': 'Bhí Oiliúint Sáraíochta Fhíorúil (CBL) éifeachtach maidir le samhlacha láidre a fhoghlaim faoi shuímh maoirsithe agus leath-mhaoirsithe le haghaidh fís ríomhaire agus tascanna NLP. Mar sin féin, níl iniúchadh déanta cheana ar éifeachtúlacht CBL maidir le haithint mothúcháin ilteangach agus illipéid. San obair seo, déanaimid iniúchadh ar CBL d’aithint mhothúcháin illipéid le fócas ar shonraí neamhlipéadaithe ó theangacha éagsúla a ghiaráil chun feidhmíocht na samhla a fheabhsú. Déanaimid turgnaimh fhairsing leath-mhaoirsithe ar thacair sonraí illipéad agus ilteangacha aitheantais mothúcháin SemEval2018 agus taispeánann muid gnóthachain feidhmíochta de 6.2% (Araibis), 3.8% (Spáinnis) agus 1.8% (Béarla) thar fhoghlaim faoi mhaoirseacht leis an méid céanna sonraí lipéadaithe (10% de sonraí oiliúna). Cuirimid feabhas freisin ar an stát-de-aimseartha atá ann faoi láthair faoi 7%, 4.5% agus 1% (Jaccard Innéacs) don Spáinnis, Araibis agus Béarla faoi seach agus a dhéanamh turgnaimh iniúchta chun tuiscint a fháil ar an tionchar na sraitheanna éagsúla de na samhlacha comhthéacsúla.', 'hu': 'A virtuális negatív képzés (héa) hatékony volt a robusztus modellek tanulásában felügyelt és félig felügyelt beállítások mellett mind a számítógépes látás, mind a NLP feladatok esetében. Mindazonáltal korábban még nem vizsgálták a HÉA hatékonyságát a többnyelvű és többcímkés érzelmek felismerésére. Ebben a munkában feltárjuk a többcímkés érzelmek felismerésének áfáját, különös tekintettel arra, hogy a különböző nyelvekről származó címke nélküli adatokat használjuk fel a modell teljesítményének javítása érdekében. Széleskörű, félig felügyelt kísérleteket végzünk a SemEval2018 többcímkés és többnyelvű érzelemfelismerő adatkészleten, és 6,2% (arab), 3,8% (spanyol) és 1,8% (angol) teljesítménynövekedést mutatunk a felügyelt tanulás során ugyanolyan mennyiségű címkézett adatokkal (a képzési adatok 10%-a). Továbbá 7%-kal, 4,5%-kal és 1%-kal (Jaccard Index) javítjuk a meglévő korszerűséget spanyol, arab és angol nyelven, és vizsgálati kísérleteket végezünk a kontextuális modellek különböző rétegeinek hatásának megértésére.', 'el': 'Η εικονική εκπαίδευση αντιπροσώπων (ΦΠΑ) έχει αποδειχθεί αποτελεσματική στην εκμάθηση ισχυρών μοντέλων κάτω από εποπτευόμενες και ημι-εποπτευόμενες ρυθμίσεις τόσο για την οπτική του υπολογιστή όσο και για τις εργασίες NLP. Ωστόσο, η αποτελεσματικότητα του ΦΠΑ για την πολυγλωσσική και πολυσήμαντη αναγνώριση συναισθημάτων δεν έχει διερευνηθεί προηγουμένως. Σε αυτή την εργασία, διερευνούμε τον ΦΠΑ για την αναγνώριση συναισθημάτων πολλαπλών ετικετών με έμφαση στην αξιοποίηση δεδομένων χωρίς σήμανση από διαφορετικές γλώσσες για τη βελτίωση της απόδοσης του μοντέλου. Πραγματοποιούμε εκτεταμένα ημι-εποπτευόμενα πειράματα σε πολλαπλές ετικέτες και πολύγλωσσα δεδομένα αναγνώρισης συναισθημάτων και εμφανίζουμε κέρδη απόδοσης 6.2% (αραβικά), 3.8% (ισπανικά) και 1.8% (αγγλικά) πέρα από την εποπτευόμενη μάθηση με την ίδια ποσότητα επισημασμένων δεδομένων (10% των δεδομένων κατάρτισης). Επίσης βελτιώνουμε την υπάρχουσα κατάσταση της τεχνολογίας κατά 7%, 4.5% και 1% (Δείκτης για ισπανικά, αραβικά και αγγλικά αντίστοιχα και διεξάγουμε πειράματα ανίχνευσης για την κατανόηση των επιπτώσεων των διαφόρων στρωμάτων των πλαισιακών μοντέλων.', 'ka': 'ვირტუალური კომპერსაციალური განათლება (PVT) იყო ეფექტიური მოდელების შესწავლებაში, რომლებიც კომპიუტერიური ხედის და NLP დავალებებისთვის ნახევარულებული და ნახევარულებული პარა მაგრამ, მრავალენგური და მრავალენგური ემოციების განახლებისთვის PVT ეფექტიურობა უკვე არ იქნება. ამ სამუშაოში, ჩვენ გავაკეთებთ VAT-ს მრავალ-ლექტური ემოციების განაცნობისთვის, რომელიც განსხვავებული ენებიდან უფრო უფრო უფრო უფრო უფრო უფრო უფ ჩვენ SemEval2018 მულტილური და მულტილური ემოციონის მონაცემების მონაცემების გავაკეთებთ გავაკეთებთ გავაკეთებთ 6.2% (აპაბიური), 3.8% (შპანელი) და 1.8% (ანგლიური) მონაცემების მარტილური მონაცემების მარტილური მონაცემების გასაღ ჩვენ ასევე 7%, 4.5% (Jaccard Index) და 1% (Jaccard Index) სპანელი, არაბული და ინგლისური განმავლობაში გავაკეთებთ პრობენტის ექსპერიმენტები, რომლებიც კონტექსტური მოდელების განსხვავებაში გავაკეთებთ.', 'it': "La formazione adversariale virtuale (IVA) è stata efficace nell'apprendimento di modelli robusti sotto impostazioni supervisionate e semi-supervisionate sia per le attività di computer vision che NLP. Tuttavia, l'efficacia dell'IVA per il riconoscimento multilingue e multilabel delle emozioni non è stata esplorata prima. In questo lavoro, esploriamo l'IVA per il riconoscimento delle emozioni multilabel con l'obiettivo di sfruttare dati non etichettati provenienti da lingue diverse per migliorare le prestazioni del modello. Eseguiamo ampi esperimenti semi-supervisionati su set di dati SemEval2018 multilabel e multilingue di riconoscimento delle emozioni e mostriamo guadagni di performance del 6,2% (arabo), 3,8% (spagnolo) e 1,8% (inglese) rispetto all'apprendimento supervisionato con la stessa quantità di dati etichettati (10% dei dati di allenamento). Inoltre miglioriamo lo stato dell'arte esistente del 7%, 4,5% e 1% (Indice Jaccard) rispettivamente per spagnolo, arabo e inglese ed eseguiamo esperimenti di sonding per comprendere l'impatto dei diversi strati dei modelli contestuali.", 'kk': 'Виртуалды конверсариялық оқыту (VAT) компьютердің қарау және NLP тапсырмаларының алдындағы және жарты қарау баптауларының күшті моделдерін оқытуға эффективні болды. Бірақ бірнеше тілді және көп жарлық емоцияларды анықтау үшін ДОТ эффективнігі алдында зерттелмеген. Бұл жұмыс ішінде, біз көп жарлық емоцияларды көп жарлық мәліметтердің көп жарлық мәліметтерді түсінуге арналған деректерді түрлі тілдерден арналған көмектесу үшін Д Біз SemEval2018 көптілік белгілер мен көптілік емоциялық деректерді түсініп, 6,2% (арабша), 3,8% (Испан) және 1,8% (ағылшын) деген көпшілікті эксперименттерді жасап, бірнеше мәліметті мәліметтің (оқыту деректерінің 10% шыға Біз сондай-ақ 7%, 4,5% және 1% (Джаккарт индексі) испан, араб және ағылшын тілдеріне жақсартып, контексті моделдердің түрлі қабаттардың әсерін түсіну үшін сынақтар тәжірибелерін жасаймыз.', 'ml': 'വിര്\u200dച്ച്വല്\u200d അഡ്രസറിയല്\u200d പരിശീലനം (VAT) കാര്യങ്ങള്\u200dക്കും കമ്പ്യൂട്ടര്\u200d ദര്\u200dശനത്തിനും NLP ജോലികള്\u200dക്കും കൂടി നിരീക്ഷിക്കപ്പെടുന്ന രോബോസ് എന്നാലും മുന്\u200dപ് വിട്ടിന്റെ വ്യാത്തിന്റെ പ്രഭാവം പല ഭാഷകങ്ങള്\u200dക്കും മള്\u200dട്ടിലേബിള്\u200d തിരിച്ചറിയാനുള്ള ഈ പ്രവര്\u200dത്തനത്തില്\u200d, നമ്മള്\u200d മുള്\u200dട്ടിളേബെല്\u200d വികാരം തിരിച്ചറിയുന്നതിനായി VAT പരിശോധിക്കുന്നു. വ്യത്യസ്ത ഭാഷകളില്\u200d നിന്നും വ സെമ്എവാല്\u200d2018 മള്\u200dട്ടില്ലാബെലിലും പല ഭാഷയിലുള്ള വികാരങ്ങളുടെ തിരിച്ചറിയാനുള്ള ഡാറ്റാസെറ്റിലും നമ്മള്\u200d വിശാലമായി നിരീക്ഷിക്കപ്പെടുന്ന പരീക്ഷണങ്ങള്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന് സ്പാനിഷ്, അറബിക്കും ഇംഗ്ലീഷുമായി നിലവിലുള്ള കലാകാര്യത്തിന്റെ സ്ഥിതിയില്\u200d 7%, 4.5% (ജാക്കാര്\u200dഡ് ഇന്\u200dഡെക്സ്) മുന്\u200dകൂട്ടുന്നു. നിലവിലുള്ള സ്ഥ', 'mt': 'It-Taħriġ Adversarju Virtwali (VAT) kien effettiv fit-tagħlim ta’ mudelli robusti taħt ambjenti sorveljati u semisorveljati kemm għall-kompiti tal-viżjoni tal-kompjuter kif ukoll għall-NLP. Madankollu, l-effikaċja tal-VAT għar-rikonoxximent multilingwi u multitikketta tal-emozzjonijiet ma ġietx esplorata qabel. F’dan ix-xogħol, a ħna nesploraw il-VAT għar-rikonoxximent tal-emozzjonijiet b’tikketta multipla b’enfasi fuq l-ingranaġġ ta’ dejta mhux immarkata minn lingwi differenti biex itejbu l-prestazzjoni tal-mudell. Saru esperimenti estensivi semisuperviżi fuq SemEval2018 multitikketta u sett ta’ dejta dwar ir-rikonoxximent tal-emozzjonijiet multilingwi u juru kisbiet fil-prestazzjoni ta’ 6.2% (Għarbi), 3.8% (Spanjol) u 1.8% (Ingliż) fuq tagħlim superviż bl-istess ammont ta’ dejta ttikkettata (10% tad-dejta dwar it-taħriġ). We also improve the existing state-of-the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for understanding the impact of different layers of the contextual models.', 'lt': 'Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks.  Tačiau PVM veiksmingumas daugiakalbiams ir daugiakalbiams emocijų pripažinimui anksčiau neištirtas. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance.  We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data).  Taip pat patobuliname dabartinę pažangą atitinkamai 7 %, 4,5 % ir 1 % (Jaccard indeksas) ispanų, arabų ir anglų kalbomis ir atliekame tyrimus siekiant suprasti įvairių kontekstinių modelių sluoksnių poveikį.', 'mk': 'Виртуелно непријатно обука (ДДВ) беше ефикасна во учењето на силни модели под надгледувани и полунадгледувани поставувања за компјутерска визија и за задачите на NLP. Сепак, ефикасноста на ДДВ за мултијазичко и мултијазичко признавање на емоциите порано не е истражена. Во оваа работа, ние го истражуваме ДДВ за мултиетикетно емоционално препознавање со фокус на користењето на неозначени податоци од различни јазици за подобрување на моделната перформанса. Ние спроведуваме експерименти со полунадгледувани експерименти на SemEval2018 мултиетикет и мултијазичниот компјутер на податоци за препознавање на емоциите и покажуваме придобивки на перформансата од 6,2 % (арапски), 3,8 % (шпански) и 1,8 % (англиски) над надгледуваното учење со иста количина на подато Ние, исто така, ја подобруваме постојаната техничка состојба за 7 отсто, 4,5 отсто и 1 отсто (Индекс на Јакард) за шпански, арапски и англиски и спроведуваме експерименти за проверка за разбирање на влијанието на различните слоеви на контекстните модели.', 'ms': 'Latihan Melawan Virtual (VAT) telah berkesan dalam mempelajari model yang kuat di bawah tetapan yang dijaga dan semi-dijaga untuk kedua-dua tugas penglihatan komputer dan NLP. Namun, efektiviti VAT untuk pengenalan emosi berbilang bahasa dan berbilang label belum pernah dikenalpasti sebelumnya. Dalam kerja ini, kami mengeksplorasi VAT untuk pengenalan emosi label berbilang dengan fokus pada penggunaan data tidak berlebihan dari bahasa yang berbeza untuk meningkatkan prestasi model. Kami melakukan eksperimen semi-mengawasi luas pada SemEval2018 multilabel dan set data pengenalan emosi berbilang bahasa dan menunjukkan keuntungan prestasi 6.2% (Arab), 3.8% (Spanyol) dan 1.8% (Inggeris) atas belajar mengawasi dengan jumlah yang sama data yang ditandai (10% data latihan). Kami juga meningkatkan keadaan-state-of-the-art yang wujud dengan 7%, 4.5% dan 1% (Indeks Jaccard) untuk bahasa Sepanyol, Arab dan Inggeris sesuai dan melakukan eksperimen penyelidikan untuk memahami kesan dari lapisan berbeza model kontekstual.', 'ro': 'Instruirea adversară virtuală (TVA) a fost eficientă în învățarea modelelor robuste sub setări supravegheate și semi-supravegheate atât pentru viziunea computerizată, cât și pentru sarcinile PNL. Cu toate acestea, eficacitatea TVA-ului pentru recunoașterea emoțiilor multilingvă și multietichetă nu a fost explorată înainte. În această lucrare, explorăm TVA-ul pentru recunoașterea emoțiilor multilabel, cu accent pe valorificarea datelor fără etichete din diferite limbi pentru a îmbunătăți performanța modelului. Efectuăm experimente semi-supravegheate extinse pe setul de date SemEval2018 multilabel și multilingv de recunoaștere a emoțiilor și prezentăm câștiguri de performanță de 6,2% (arabă), 3,8% (spaniolă) și 1,8% (engleză) față de învățarea supravegheată cu aceeași cantitate de date etichetate (10% din datele de formare). Îmbunătățim, de asemenea, stadiul actual de ultimă generație cu 7%, 4,5% și 1% (indice Jaccard) pentru spaniolă, arabă și engleză și efectuăm experimente de sondare pentru înțelegerea impactului diferitelor straturi ale modelelor contextuale.', 'mn': 'Виртуал Конверсарийн Сургууль (ВАТ) компьютерийн харах болон НLP ажлын тухай хоёр талаар ажиллагааны шаардлагатай, хагас дамжуулагдсан загваруудыг сурах нь үр дүнтэй. Гэхдээ олон хэл болон олон тэмдэгтийн сэтгэл хөдлөлийн хүлээн зөвхөн ДОХ-ын үр дүнг өмнө нь судалгаагүй. Энэ ажлын тулд бид олон загварын сэтгэл хөдлөлийн мэдрэмжүүдийг олон загварын мэдрэмжүүдийн тусламжтайгаар ДОХ-г судалж, өөр хэл дээр тайлбаргүй мэдээллийг ашиглаж, загварын үйлдэл Бид SemEval2018 оны олон загвар, олон хэлний сэтгэл хөдлөлийн мэдээллийн санааг хүлээн зөвшөөрүүлж, 6.2% (Араб), 3.8% (Испан хэлний) болон 1.8% (Англи хэлний) суралцах туршилтуудыг харуулж байна. Мөн бид 7%, 4.5%, 1% (Жаккарт индекс) болон Испан, Араб, Англи хэлний хувьд оршиж буй орчин үеийн өөр хэсгүүдийн нөлөөг ойлгохын тулд судалгааны туршилтыг хийдэг.', 'pl': 'Virtual Adversarial Training (VAT) był skuteczny w nauce solidnych modeli w ustawieniach nadzorowanych i pół-nadzorowanych zarówno dla zadań wizji komputerowej, jak i NLP. Nie zbadano jednak wcześniej skuteczności podatku VAT dla rozpoznawania emocji wielojęzycznych i wieloznakowych. W niniejszej pracy badamy VAT dla rozpoznawania emocji wieloznakowych z naciskiem na wykorzystanie nieoznakowanych danych z różnych języków w celu poprawy wydajności modelu. Przeprowadzamy obszerne eksperymenty pół-nadzorowane na wieloznakowym i wielojęzycznym zestawie danych rozpoznawania emocji SemEval2018 i pokazujemy wzrost wydajności 6.2% (arabski), 3.8% (hiszpański) i 1.8% (angielski) w porównaniu z nadzorowaną nauką z taką samą ilością oznakowanych danych (10% danych treningowych). Poprawiamy również istniejący state-of-the-art o 7%, 4,5% i 1% (Jacquard Index) odpowiednio dla hiszpańskiego, arabskiego i angielskiego oraz wykonujemy eksperymenty sondujące w celu zrozumienia wpływu różnych warstw modeli kontekstowych.', 'no': 'Virtuell rekursarial trening (VAT) er effektivt i læring av robust modeller under oversikte og halvoversikte innstillingar for både datavising og NLP-oppgåver. Det er imidlertid ikkje utforska effektiviteten av VAT for fleirspråk og fleirmerkelige følelser. I denne arbeida utforskar vi VAT for å gjenkjenne fleire merkelapper følelser med fokus på å levera ubeligge data frå ulike språk for å forbetra modellen. Vi utfører ekstra semioversikt eksperimenter på fleire merkelapper og fleirspråkskjenningsdataset for å gjenkjenne følelser og viser utviklingar av 6,2% (arabisk), 3,8% (spansk) og 1,8% (engelsk) over oversikt læring med same mengd merkelige data (10% av opplæringsdata). Vi forbedrar også den eksisterande kunsttilstanden med 7%, 4,5% og 1% (Jaccard Index) for spansk, arabisk og engelsk, og utfører prøvingseksperimentane for å forstå effekten av ulike lag i kontekstmodelane.', 'sv': 'Virtual Adversarial Training (VAT) har varit effektivt för att lära sig robusta modeller under övervakade och halvövervakade inställningar för både datorseende och NLP-uppgifter. Effekten av mervärdesskatt för flerspråkig och flerspråkig känsloigenkänning har dock inte undersökts tidigare. I detta arbete undersöker vi mervärdesskatt för multilabel känsloigenkänning med fokus på att utnyttja omärkta data från olika språk för att förbättra modellens prestanda. Vi utför omfattande semi-övervakade experiment på SemEval2018 multilabel och flerspråkig känsloigenkänningsdataset och visar prestationsvinster på 6,2% (arabiska), 3,8% (spanska) och 1,8% (engelska) jämfört med övervakat lärande med samma mängd märkta data (10% av träningsdata). Vi förbättrar också den befintliga toppmodern med 7%, 4,5% och 1% (Jaccard Index) för spanska, arabiska respektive engelska och utför sonderingsexperiment för att förstå effekten av olika lager av kontextuella modeller.', 'sr': 'Virtualno savjetno obučenje (PVN) bilo je efikasno u učenju robnih modela pod nadzornim i polu nadzornim nastavama za kompjutersku viziju i NLP zadatke. Međutim, učinkovitost PVN-a za prepoznavanje emocija multijezičkih i multioznačenih jezika ranije nije istražena. U ovom poslu, istražujemo PVN za prepoznavanje emocija sa fokusom na primjenu nepublaženih podataka iz različitih jezika kako bi poboljšali model izvršnost. Izvedemo široke polu nadzorne eksperimente na semiEval2018 multi-etikete i multi-jezičkih podataka priznanja emocija i pokazujemo postignuće učinka od 6,2% (Arapski), 3,8% (španjolski) i 1,8% (engleski) preko nadzornog učenja s istim količinom označenih podataka (10% podataka o obuci). Takođe ćemo poboljšati postojeće stanje umjetnosti za 7%, 4,5% i 1% (Indeks Jaccard) za španjolski, arapski i engleski, i izvršiti istraživanje eksperimenata za razumevanje utjecaja različitih slojeva kontekstualnih modela.', 'si': 'වාස්තවික ප්\u200dරධාන ප්\u200dරධානය (VAT) පරීක්ෂණය සහ NLP වැඩකට පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා ප්\u200dරශ නමුත්, විශාල භාෂාවක් සහ ගොඩක් ලේබල් භාවිතාවක් අඳුරගන්න ප්\u200dරයෝජනයක් කලින් පරීක්ෂණය කරලා  මේ වැඩේ අපි VAT පරීක්ෂා කරනවා වෙනස් භාෂාවන් නැති දත්ත ප්\u200dරශ්නයක් වෙනුවෙන් වෙනස් භාෂාවන් ප්\u200dරශ්නයක් විද අපි ප්\u200dරශ්නයක් සම්විලේබල් 2018 ගොඩක් ලේබල් සහ ගොඩක් භාවිතාවක් අඳුනු දත්ත සඳහා ප්\u200dරශ්නයක් 6.2% (අරාබික්), 3.8% (ස්පැනිශ්) සහ 1.8% (ඉංග්\u200dරේෂික්) සහ සුභ අපි ස්පැනිස්, අරාබියි ඉංග්\u200dරීසි වලින් 7%, 4.5% සහ 1% (ජකාර්ඩ් ඉංග්\u200dරීස්) වලින් ප්\u200dරශ්නයක් කරනවා සහ පරීක්ෂණ පරීක්ෂණ ප්\u200dරශ්නයක් ස', 'so': 'Waxbarashada hore ee Virtual Adversarin (VAT) waxey saameyn ku leedahay barashada modelalka roboti oo lagu ilaaliyo iyo hababka la ilaaliyo labada muuqashada kambiyuutarka iyo shaqooyinka NLP. Si kastaba ha ahaatee, saamaynta VAT ee aqoonsashada daryeelka luuqadaha badan iyo dhaqanka kala duduwan horumar lama baarayn. Shaqadan ayaannu VAT u baaraynaa aqoonsiga dareemada kala duduwan, kaas oo focus ku leh soo diritaanka macluumaadka aan labo lahayn oo luuqado kala duduwan si aan u hagaajinno muusikada. Waxaannu sameynaa imtixaanka kala duduwan ee SemEval2018 oo ku qoran taariikhda aqoonsashada luuqadaha kala duduwan iyo muujinta waxyaabaha lagu sameeyo 6,2% (Carabi), 3,8% (Isbanish) iyo 1,8% (Ingiriis) oo lagu ilaaliyey waxbarashada si lamid ah (10% macluumaadka waxbarashada). Sidoo kale waxaynu horumarinaynaa xaaladda farshaxanka ah 7%, 4.5 % iyo 1% (Jaccard Index) si waafaqsan Isbanish, Carabi iyo Ingiriis, waxaana sameynaa imtixaanka baaritaanka si aan u garanayno saamaynta sawirada kala duwan ee modellada soo socda.', 'ur': "ویرچولی اپنا اپنا ٹرینینگ (VAT) کمپیوٹر نظر اور NLP کاموں کے لئے نیچے نظر رکھنے والی اور نیچے نظر رکھنے والی سیٹیوں کے ذریعہ مضبوط موڈل سکھانے میں اثر ہے. However, VAT's efficacy for multilingual and multilabel emotion recognition has not been explored before. اس کام میں ہم ملتی لیبل احساسات پہچان کے لئے وی.ٹ. کی تلاش کرتے ہیں اور مختلف زبانوں سے بغیر قابل پڑھنے والی دیٹا کو دکھانے کے لئے مطابق مطابق کریں۔ ہم SemEval2018 کے متعدل لیبلبل اور متعدل زبان احساسات کی شناسایی ڈاٹ سٹ پر بہت زیادہ نصف نظارت کی آزمائش کرتے ہیں اور 6.2% (عربی), 3.8% (اسپانیایی) اور 1.8% (انگلیسی) کے مطابق نیٹ لیبل کی تعلیم کے مطابق نیٹ لیبل اور متعدل زبان کی احساسات کے مطابق ہم نے اسپانیایی, عربی اور انگلیسی کے لئے 7%, 4.5% اور 1% (جک کارڈ انڈیکس) کے ذریعہ سے موجود موجود موجود موجود موجود کی حالت کو بہترین کر دیا ہے اور متوسط موڈلز کے مختلف لائر کا اثر سمجھنے کے لئے تحقیق آزمائش کرتے ہیں۔", 'ta': 'மெய்நிகர் மேம்பாடு பயிற்சி( VAT) செயல்படுத்தப்பட்டுள்ளது கணினி பார்வையும் NLP பணிகளும் இருவரும் கணிப்பொறி பார்வையும் போன்ற மற்றும்  இருந்தாலும், பல மொழி மற்றும் பல மொழி உணர்வு உணர்வுகளுக்கான VAT விளைவு முன்பு தேடியிருக்கவில்லை. இந்த வேலையில், மாதிரி செயல்பாட்டை மேம்படுத்த முடியும் முறைமையை மாற்ற வேண்டிய மொழிகளிலிருந்து குறிப்பிடப்படாத தகவலை கொட SemEval2018 பல்மொழி உணர்வு அறிவிப்பு தரவுத்தளத்தில் நாம் விரிவாக பாதி கண்காணிக்கப்பட்ட சோதனைகளை செய்கிறோம் மற்றும் செயல்பாட்டின் விருப்பங்கள் 6. 2% (அரேபியா), 3. 8% (ஸ்பானிஷ்) மற்று நாம் இப்போது இருக்கும் கலைப்பாட்டின் நிலையை 7%, 4. 5% மற்றும் 1% (ஜாக்கார்ட் சுட்டுவரிசை, அரபி மற்றும் ஆங்கிலத்திற்காக மேம்படுத்துகிறோம் மற்றும்', 'uz': "Name Lekin ko'pchilik tillar va multilabel hissiyotni aniqlash uchun VAT effekti avval aniqlanmagan. Bu ishda, biz multilabel hissiyotni aniqlash uchun VATni o'rganamiz va modelni bajarish uchun boshqa tillardan notoʻgʻri maʼlumot yozib olish uchun foydalanamiz. Biz SemEval2018 multilabel va bir necha tillar hissiyotni aniqlash maʼlumotlarini bajaramiz va bajarish natijasi 6.2% (arab), 3.8% (Ispancha) va 1.8% (Ingliz tilida) o'rganishni bir xil maʼlumot bilan o'rganishni ko'rsatdik (10% ta'lim maʼlumotidan 10%). Bu yerda Ispanchadan, arab va ingliz tilida mavjud sananing holatini 7%, 4.5% va 1% (Jaccard Index) darajada bajaramiz va bu xil modellarning har xil qatlamlarining effektini tushunish uchun imtiyozni bajaramiz.", 'vi': 'Sự huấn luyện trở ngại vật ảo (thuế) đã có hiệu quả trong việc học các mô hình vững chắc dưới sự giám sát và giám sát gần nhau cho cả nhiệm vụ ảo thuật và ngôn ngữ. Tuy nhiên, chưa từng được nghiên cứu về hiệu quả của thuế cho việc nhận dạng cảm xúc đa dạng và nhiều thẻ. Trong công việc này, chúng ta sẽ tìm kiếm giá trị nhận dạng cảm xúc có nhiều hiệu quả, với tập trung vào việc vận dụng các dữ liệu chưa chỉnh từ các ngôn ngữ khác nhau để nâng cao tiến trình mô hình. Chúng tôi thực hiện thí nghiệm bán giám sát rộng rãi trên thẻ cảm xúc bắt được SemEvul238 và cả bộ nhớ các tập tin cảm xúc đa dạng. Chúng tôi hiển thị lợi thế của 6.2 Name Chúng tôi cũng cải thiện trạng thái nghệ thuật hiện thời với 7=, 4.5=.and1= (Jace card Index) for Spanish, Arab and English, và tiến hành các thí nghiệm thăm dò để hiểu tác động của các lớp khác nhau trong các mô- đun ngữ cảnh.', 'bg': 'Виртуалното рекламно обучение (ДДС) е ефективно при изучаването на стабилни модели под надзорни и полунадзорни настройки както за задачи с компютърно зрение, така и за НЛП. Ефективността на ДДС за многоезично и многоетикетно разпознаване на емоции обаче не е проучена досега. В тази работа изследваме ДДС за разпознаване на емоции с множество етикети с фокус върху използването на незабелязани данни от различни езици за подобряване на производителността на модела. Извършваме обширни полу-надзорни експерименти с многоетикетен и многоезичен набор от данни за разпознаване на емоции и показваме увеличение на ефективността от 6,2% (арабски), 3,8% (испански) и 1,8% (английски) в сравнение с обучението под надзор със същото количество етикетирани данни (10% от данните за обучение). Също така подобряваме съществуващите съвременни технологии със 7%, 4,5% и 1% (индекс на Джакард) съответно за испански, арабски и английски и извършваме сондиращи експерименти за разбиране на въздействието на различните слоеве на контекстуалните модели.', 'nl': 'Virtual Adversarial Training (btw) is effectief geweest bij het leren van robuuste modellen onder supervisor en semi-supervisor instellingen voor zowel computer vision als NLP taken. De doeltreffendheid van btw voor meertalige en meertalige emotionele herkenning is echter nog niet onderzocht. In dit werk onderzoeken we btw voor multilabel emotionele herkenning met een focus op het gebruik van niet-gelabelde gegevens uit verschillende talen om de prestaties van het model te verbeteren. We voeren uitgebreide semi-begeleide experimenten uit op SemEval2018 multilabel en meertalige emotionele herkenningsgegevens en tonen prestatiewinsten van 6.2% (Arabisch), 3.8% (Spaans) en 1.8% (Engels) boven begeleid leren met dezelfde hoeveelheid gelabelde gegevens (10% van trainingsgegevens). We verbeteren ook de bestaande state-of-the-art met 7%, 4,5% en 1% (Jacard Index) voor respectievelijk Spaans, Arabisch en Engels en voeren sonde experimenten uit om de impact van verschillende lagen van de contextuele modellen te begrijpen.', 'hr': 'Virtualna poručna obuka (PVN) bila je učinkovita u učenju robnih modela pod nadzornim i polu nadzornim nastavama za računalne vizije i NLP zadatke. Međutim, učinkovitost PVN-a za prepoznavanje višejezičkih i višeoznačenih emocija prije nije istražena. U ovom poslu istražujemo PVN za prepoznavanje emocija multietiketa s fokusom na primjenu neizbiljnih podataka iz različitih jezika kako bi poboljšali modelnu funkciju. Proizvodimo široke polu nadzorne eksperimente na višejezičkim i multijezičkim podacima priznanja emocija i pokazujemo postignuće učnosti od 6,2% (Arapski), 3,8% (španjolski) i 1,8% (engleski) nadzornog učenja s istim količinom označenih podataka (10% podataka o obuci). Također smo poboljšali postojeće stanje umjetnosti za 7%, 4,5% i 1% (Indeks Jaccard) za španjolski, arapski i engleski, i provodili istraživačke eksperimente za razumijevanje utjecaja različitih slojeva kontekstualnih modela.', 'de': 'Virtual Adversarial Training (VAT) hat sich als effektiv erwiesen, robuste Modelle unter überwachten und halbüberwachten Einstellungen für Computer Vision und NLP-Aufgaben zu erlernen. Die Wirksamkeit der Mehrwertsteuer für die mehrsprachige und mehrsprachige Emotionserkennung wurde jedoch noch nicht untersucht. In dieser Arbeit untersuchen wir die Mehrwertsteuer für Multi-Label Emotion Recognition mit einem Fokus auf der Nutzung von nicht markierten Daten aus verschiedenen Sprachen, um die Modellleistung zu verbessern. Wir führen umfangreiche semi-überwachte Experimente mit SemEval2018 multilabel- und multilingualen Emotionserkennungsdaten durch und zeigen Leistungszuwächse von 6.2% (Arabisch), 3.8% (Spanisch) und 1.8% (Englisch) gegenüber überwachtem Lernen mit gleicher Menge markierter Daten (10% Trainingsdaten). Außerdem verbessern wir den bestehenden Stand der Technik um 7%, 4,5% und 1% (Jacard Index) für Spanisch, Arabisch und Englisch und führen Sondierungsexperimente durch, um die Auswirkungen verschiedener Schichten der Kontextmodelle zu verstehen.', 'id': 'Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks.  Namun, efektivitas dari VAT untuk pengakuan emosi multibahasa dan multilabel belum pernah dikeksplorasi sebelumnya. Dalam pekerjaan ini, kami mengeksplorasi VAT untuk pengakuan emosi multilabel dengan fokus pada penggunaan data tanpa label dari bahasa yang berbeda untuk meningkatkan prestasi model. Kami melakukan eksperimen semi-supervised ekstensif di SemEval2018 multilabel dan dataset pengakuan emosi multibahasa dan menunjukkan keuntungan prestasi 6,2% (Arab), 3,8% (Spanyol) dan 1,8% (Inggris) atas pengakuan dengan jumlah yang sama dari data yang ditandai (10% dari data pelatihan). Kami juga meningkatkan state-of-the-art yang ada dengan 7%, 4,5% dan 1% (Jaccard Index) untuk Spanyol, Arab dan Inggris secara sesuai dan melakukan eksperimen penyelidikan untuk memahami dampak dari lapisan berbeda dari model kontekstual.', 'ko': '가상대항훈련(VAT)은 컴퓨터 시각과 NLP 임무의 감독과 반감독 설정에서 루팡 모형을 효과적으로 학습할 수 있다.그러나 VAT가 다중 언어와 다중 라벨 정서 식별에 대한 유효성은 아직 탐색되지 않았다.이 작업에서 우리는 다양한 라벨의 감정 식별을 위한 VAT를 탐색했는데 중심은 서로 다른 언어에서 나온 표기되지 않은 데이터를 이용하여 모델 성능을 향상시키는 것이다.우리는SemEval 2018 멀티탭과 다국어 감정인식 데이터 세트에서 광범위한 반감독 실험을 실시한 결과 같은 수량의 탭 데이터(10%의 훈련 데이터)에서 감독학습에 비해 6.2%(아랍어), 3.8%(스페인어), 1.8%(영어)의 성능이 향상된 것으로 나타났다.스페인어, 아랍어, 영어의 기존 수준을 각각 7%, 4.5%, 1%(Jaccard Index) 높이고, 어경모델의 다양한 측면의 영향을 파악하기 위한 탐색적 실험도 진행했다.', 'da': 'Virtual Adversarial Training (VAT) har været effektiv til at lære robuste modeller under overvågede og halvovervågede indstillinger til både computersyn og NLP-opgaver. Effektiviteten af momsen for flersproget og flermærket følelsesgenkendelse er imidlertid ikke blevet undersøgt før. I dette arbejde undersøger vi moms for multilabel følelsesgenkendelse med fokus på at udnytte ikke-mærkede data fra forskellige sprog for at forbedre modellens ydeevne. Vi udfører omfattende semi-overvågede eksperimenter på SemEval2018 multilabel- og flersprogede følelsesgenkendelsesdatasæt og viser præstationsgevinster på 6,2% (arabisk), 3,8% (spansk) og 1,8% (engelsk) i forhold til overvåget læring med samme mængde mærkede data (10% af træningsdata). Vi forbedrer også den eksisterende state-of-the-art med 7%, 4,5% og 1% (Jaccard Index) for henholdsvis spansk, arabisk og engelsk og udfører sonderingseksperimenter for at forstå effekten af forskellige lag af kontekstuelle modeller.', 'sw': 'Mafunzo ya Udhibiti (VAT) yamekuwa na ufanisi katika kujifunza modeli za roboti chini ya kufuatiliwa na mazingira yanayofuatiliwa kwa ajili ya maoni ya kompyuta na kazi za NLP. Hata hivyo, madhara ya VAT kwa kutambua hisia za lugha mbalimbali na hisia mbalimbali hayajachunguzwa kabla. Katika kazi hii, tunachunguza VAT kwa kutambua hisia mbalimbali kwa lengo la kutumia taarifa zisizo tajwa kutoka lugha tofauti ili kuboresha utendaji wa mifano. Tunafanya majaribio mengi yanayofuatiliwa na sekondari kwenye taarifa za kutambuliwa kwa lugha nyingine za SemEval2018 na kuonyesha mafunzo ya utendaji yanayopata asilimia 6.2 (Kiarabu), asilimia 3.8 (Kihispania) na asilimia 1.8 (Kiingereza) kwa kufuatiliwa na takwimu sawa na takwimu za mafunzo (asilimia 10 ya takwimu za mafunzo). Pia tunaboresha hali ya sanaa iliyopo kwa asilimia 7, 4.5 na asilimia 1 (Hindi ya Jaccard Index) kwa ajili ya Kihispania, Kiarabu na Kiingereza na kufanya majaribio ili kuelewa madhara ya vipande tofauti vya mifano ya sasa.', 'fa': 'آموزش تجاوز مجازی (VAT) در آموزش مدل\u200cهای استوار تحت نظارت و نیمه\u200cنظارت\u200cشده برای هر کار کامپیوتر و NLP موثر است. با این حال، فعالیت VAT برای شناسایی احساسات زیادی زبان و زیادی برچسب قبل از این تحقیق نشده است. در این کار، ما VAT را برای شناسایی احساسات چندین نقاشی با تمرکز روی تأثیر داده های غیرقابل استفاده از زبانهای مختلف تحقیق می کنیم تا عملکرد مدل را بهتر کند. ما آزمایش\u200cهای نیمه تحت نظر زیادی روی نیمه\u200cبرچسب ۲۰۱۸ و مجموعه داده\u200cهای شناسایی احساسات چندین زبان انجام می\u200cدهیم و نشان می\u200cدهیم که ارائه\u200cهای عملکرد ۶.۲% (عربی), ۳.۸% (اسپانیایی) و ۱.۸% (انگلیسی) بر روی یادگیری تحت نظر گرفته شده با همان ما همچنین وضعیت هنر موجود را با 7%, 4.5% و 1% (Index Jaccard) برای اسپانیایی, عربی و انگلیسی به طور مستقل تحقیق می\u200cکنیم و آزمایش\u200cهای تحقیق برای درک تاثیر لایه\u200cهای مختلف مدل\u200cهای متوسط انجام می\u200cدهیم.', 'tr': "Virtual Adversarial Training (VAT) Ýöne, birnäçe diller we çoklu etiket duýgynyň tanyş etkinlik öňünde tapylmady. Bu işde, biz VAT'i çoklu etiket duýgularyny tanamak üçin farklı dillerden çykyş edilmedik maglumatlary düzeltmek üçin üns berýäris. Biz 2018-nji semi-kontrol edilen çykyşlary bilen multi-etiket we multi-dilli duýgular tanyşygynda ýeterlik testiler edýäris we 6.2% (arabça), 3.8% (ispanyola) we 1.8% (iňlisçe) üstünde etiket edilen öwrenmeler bilen üstünde näme etiket edilen hatlaryň (10% öwrenmeleriň ýagdaýynda) üstünde gözetlenýän Biz hem bolan sanat durumyny 7%, 4.5% we 1% (Jakcard Indeks) bilen ispanyol, arabça we iňlisçe bilen gowurap barýarys we çeşitli nusgalaryň nähili katlaryny düşünmek üçin barlap synaglary çykarýarys.", 'af': "Virtuele Adversarial Training (PVT) is effektief in die leer van kragtige modele onder ondersoekte en semi-ondersoekte instellings vir beide rekenaar sien en NLP taak. Maar die effektiviteit van PVT vir multitaalske en multietikelike emosie herken is nie voor uitgesoek nie. In hierdie werk, ondersoek ons PVAT vir multietikette emosie herken met 'n fokus op die verwysing van onbepaalde data van verskillende tale om die model prestasie te verbeter. Ons uitvoer uitbreidige semi-ondersoekte eksperimente op SemEval2018 multi-etiket en multi-tale emosie-herken datastel en wys uitbreidings van 6.2% (Arabs), 3.8% (Spaanse) en 1.8% (Engels) oor ondersoekte leer met dieselfde hoeveelheid etiketeerde data (10% van ondersoek data). Ons het ook die bestaande state-of-the-art verbeter met 7%, 4.5% en 1% (Jaccard Index) vir Spaanse, Arabse en Engels respektief en probeer eksperimente uitvoer vir verstaan van die effek van verskillende lage van die contextual modele.", 'sq': 'Treinimi virtual kundërshtar (VAT) ka qenë efektiv në mësimin e modeleve të forta nën rregullime të mbikqyrura dhe gjysmë-mbikqyrura si për vizionin kompjuterik, ashtu edhe për detyrat NLP. Megjithatë, efektshmëria e TVSH-së për njohjen e emocioneve shumë-gjuhëse dhe shumë-etiketa nuk është eksploruar më parë. Në këtë punë, ne eksplorojmë TVSH për njohjen e emocioneve me shumë etiketa me një përqëndrim në përdorimin e të dhënave të panjohura nga gjuhë të ndryshme për të përmirësuar performancën e modelit. Ne kryejmë eksperimente të gjerë gjysmë-mbikqyrura në SemEval2018 me shumë etiketa dhe me shumë gjuhë të dhëna për njohjen e emocioneve dhe tregojmë fitime të performancës prej 6.2% (arabisht), 3.8% (spanjoll) dhe 1.8% (anglisht) mbi mësimin e mbikqyrur me të njëjtin sasi të dhënash të etiketuara (10% të të dhënave të trajnimit). Ne gjithashtu përmirësojmë gjendjen ekzistuese të artit me 7%, 4.5% dhe 1% (Indeksi i Xhakardit) respektivisht për spanjollët, arabisht dhe anglisht dhe kryejmë eksperimente sondazhi për të kuptuar ndikimin e niveleve të ndryshme të modeleve kontekstuale.', 'am': 'የVirtual Adversarial Training (VAT) በመጠበቅ እና በsemi-supersupervised settings for both computer vision and NLP ስራ በማስተማርና በሮቦት models ጥቅም ሆኖአል፡፡ ነገር ግን የVAT ውጤት ለብዙ ቋንቋ እና ለብዙ ልዩ ልዩ ልዩ አወቃት አስቀድሞ አልተመረጠም፡፡ በዚህ ስራ፣ ለብዙ ልዩ ልዩ ልዩ ቋንቋዎች ያልታወቀ ዳታዎችን በመስጠት እናሳውቃለን፡፡ በSemEval2018 ብዙልጣዊ እና በብዛት ቋንቋ የድምፅ ማወቅ ዳታዎችን እናደርጋለን፡፡ We also improve the existing state-of-the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for understanding the impact of different layers of the contextual models.', 'bs': 'Virtualna poručna obuka (PVN) bila je učinkovita u učenju robnih modela pod nadzornim i polu nadzornim nastavama za računalne vizije i NLP zadatke. Međutim, učinkovitost PVN-a za prepoznavanje emocija multijezičkih i multietiketa prije nije istražena. U ovom poslu, istražujemo PVN za prepoznavanje emocija sa fokusom na primjenu neizbiljnih podataka iz različitih jezika kako bi poboljšali modelnu funkciju. Izvedemo široke polu nadzorne eksperimente na multietiketu i multijezičkim podacima priznanja emocija i pokazujemo postignuće učinka od 6,2% (Arapski), 3,8% (španjolski) i 1,8% (engleski) preko nadzornog učenja s istim količinom označenih podataka (10% podataka o obuci). Također ćemo poboljšati postojeće stanje umjetnosti za 7%, 4,5% i 1% (Indeks Jaccard) za španjolski, arapski i engleski odgovorno i provoditi istraživačke eksperimente za razumijevanje utjecaja različitih slojeva kontekstualnih modela.', 'bn': 'ভার্চুয়াল অ্যাডভারেজারিয়াল প্রশিক্ষণ (ভিটি) কার্যকর হয়েছে কম্পিউটারের দৃশ্য এবং এনএলপি কাজের জন্য রোবস্ট মডেলের নিয়ন্ত্রণ এবং সেমি  তবে বহুভাষী এবং বহুভাষী আবেগের স্বীকৃতির জন্য ভিটির কার্যকর কার্যক্রম পূর্বে তদন্ত করা হয়নি। এই কাজে আমরা মাল্টিলাবেল আবেগের স্বীকৃতির জন্য ভিটি খুঁজে বের করি বিভিন্ন ভাষায় ভিন্ন ভাষা থেকে অলেবেল ডাটা প্রদান করার জন্য যে  আমরা সেমইভাল২০১৮ মাল্টিলাবেল এবং বহুভাষী অনুভূতি স্বীকৃতির ডাটাসেটে বিস্তৃত সেমি পরীক্ষার পরীক্ষা করি এবং প্রদর্শনের ফলাফল ৬. ২% (আরবী), ৩. ৮% (স্প্যানিশ) এবং ১. আমরা স্প্যানিশ, আরবী এবং ইংরেজীর জন্য বিভিন্ন ধরনের মডেলের প্রভাব বুঝতে পারি বর্তমান শিল্পের বর্তমান পরীক্ষায় ৭%, ৪.', 'ca': "La formació adversaria virtual (TVA) ha estat eficaç en aprendre models robustos sota configuracions supervisades i semisupervisades tant per la visió informàtica com per les tasques NLP. No obstant això, l'eficacia del TVA per al reconeixement emocional multilingüe i multietiquetat no ha estat explorada abans. En aquesta feina, explorem el TVA per al reconeixement emocional multietiquetat centrant-nos en aprofitar dades no etiquetades de diferents llengües per millorar el rendiment del model. Fem extensos experiments semisupervisats en SemEval2018 multietiqueta i multilingüe conjunt de dades de reconeixement emocional i mostram guanys de rendiment de 6,2% (àrab), 3,8% (espanyol) i 1,8% (anglès) sobre l'aprenentatge supervisat amb la mateixa quantitat de dades etiquetades (10% de dades d'entrenament). També millorem l'estat actual en 7%, 4,5% i 1% (Índix Jaccard) per espanyol, àrab i anglès respectivament i fem experiments de sonda per entendre l'impacte de diferents capes dels models contextuals.", 'az': 'Virtual Adversarial Training (PVT) bilgisayar görünüşü və NLP işləri üçün çox qüvvətli modelləri öyrənməkdə faydalanır. Ancaq çoxlu dil və çoxlu etiketli duyguların tanıması üçün PVT təsiri daha əvvəl keşfedilmədi. Bu işdə, çoxlu etiketli duyguların tanıması üçün PVT təşkil edirik, modellərin performansını yaxşılaşdırmaq üçün müxtəlif dillərdən istifadə edilməmiş məlumatları təşkil etmək üçün. Biz SemEval2018 çoxlu etiketli və çoxlu dilli duyguları tanıma verilən verilən təcrübələr üçün geniş yarı-gözləyirli təcrübələr və 6.2% (ərəb dilində), 3.8% (İspanyolca) və 1.8% (İngilizə dilində) müəyyən edilmiş öyrənmələr ilə eyni etiketli verilən verilər ilə göstəririk. Biz həmçinin mövcuddur sanatın durumunu 7%, 4.5% və 1% (Jaccard Index) İspanyolca, Arapça və İngilizca üçün daha yaxşı təhsil edirik və müxtəlif modellərin müxtəlif səviyyələrinin təsirini anlamaq üçün sınaq sınaqlarını təşkil edirik.', 'cs': 'Virtuální adversariální školení (DPH) bylo efektivní při učení robustních modelů pod dohledem a polovičním dohledem pro úkoly počítačového vidění a NLP. Účinnost DPH pro vícejazyčné a víceznačkové rozpoznávání emocí však předtím nebyla zkoumána. V této práci zkoumáme DPH pro multilabel rozpoznávání emocí se zaměřením na využití neoznačených dat z různých jazyků ke zlepšení výkonu modelu. Provádíme rozsáhlé semi-supervisované experimenty na multilabel a vícejazyčné datové sadě rozpoznávání emocí SemEval2018 a ukazujeme zvýšení výkonu 6.2% (arabsky), 3.8% (španělština) a 1.8% (angličtina) nad dohledem se stejným množstvím označených dat (10% tréninkových dat). Dále zlepšujeme stávající stav moderních modelů o 7%, 4,5% a 1% (Jacquard Index) pro španělštinu, arabštinu a angličtinu a provádíme sondovací experimenty pro porozumění vlivu různých vrstev kontextových modelů.', 'hy': 'Վիրտուալ հակառակ կրթությունը (ՀՆԱ) արդյունավետ է եղել ուսումնասիրելու ուժեղ մոդելներ վերահսկվող և կիսավերահսկվող միջոցներում համակարգչի տեսողության և ՆԼՊ-ի առաջադրանքների համար: Այնուամենայնիվ, ՀՆԱ-ի արդյունավետությունը բազլեզու և բազմապիտակ զգացմունքների ճանաչման համար նախկինում չի ուսումնասիրել: Այս աշխատանքի ընթացքում մենք ուսումնասիրում ենք բազմաթիկված զգացմունքների ճանաչելու ՀՆԱ-ն, կենտրոնացնելով տարբեր լեզուներից բացահայտված տվյալների օգտագործման վրա՝ մոդելի արդյունքների բարելավման համար: Մենք կատարում ենք էքսպանցիոնալ կիսավերահսկվող փորձարկումներ SEM-2018-ի բազմապիտակ և բազլեզու զգացմունքների ճանաչության տվյալների համակարգում, և ցույց ենք տալիս 6.2 տոկոսի (արաբերեն), 3.8 տոկոսի (իսպաներեն) և 1.8 տոկոսի (անգլերեն) արտադրողականության բարձրացու Մենք նաև բարելավում ենք գոյություն ունեցող տեխնոլոգիաները 7 տոկոսով, 4.5 տոկոսով և 1 տոկոսով՝ իսպաներեն, արաբերեն և անգլերենում, և կատարում ենք փորձեր, որոնք օգնում են հասկանալ կոնտեքստալ մոդելների տարբեր շերտերի ազդեցությունը:', 'et': 'Virtual Adversarial Training (VAT) on olnud tõhus tugevate mudelite õppimisel järelevalve- ja pooljärelevalveseadetes nii arvutinägemise kui ka NLP ülesannete jaoks. Ent käibemaksu tõhusust mitmekeelsel ja mitmemärgisel emotsioonide tuvastamisel ei ole varem uuritud. Selles töös uurime käibemaksu mitme märgisega emotsioonide tuvastamiseks, keskendudes erinevate keelte märgistamata andmete kasutamisele mudeli jõudluse parandamiseks. Teostame ulatuslikke pooljuhitatud katseid SemEval2018 mitme märgisega ja mitmekeelse emotsioonide tuvastamise andmekogumiga ning näitame 6,2% (araabia), 3,8% (hispaania) ja 1,8% (inglise keele) tulemuslikkuse kasvu juhendatud õppega võrreldes sama hulga märgistatud andmetega (10% treeninguandmetest). Samuti parandame olemasolevat tehnika taset vastavalt 7%, 4,5% ja 1% (Jaccardi indeks) hispaania, araabia ja inglise keeles ning teeme proovikatseid kontekstimudelite erinevate kihtide mõju mõistmiseks.', 'fi': 'Virtual Adversarial Training (VAT) on ollut tehokas oppimaan vankkoja malleja valvotuissa ja puolivalvotuissa asetuksissa sekä tietokonenäkö- että NLP-tehtävissä. Arvonlisäveron tehokkuutta monikielisessä ja monikielisessä tunteentunnistuksessa ei ole kuitenkaan tutkittu aiemmin. Tässä työssä tutkimme ALV:tä usean merkin tunteentunnistukseen keskittyen hyödyntämään merkitsemättömiä tietoja eri kielistä mallin suorituskyvyn parantamiseksi. Suoritamme laajoja puolivalvottuja kokeiluja SemEval2018 monikielisellä ja monikielisellä tunnetunnistusaineistolla ja näytämme 6,2%:n (arabia), 3,8%:n (espanja) ja 1,8%:n (englanti) suorituskyvyn paranemista valvottuun oppimiseen verrattuna samalla määrällä merkittyä tietoa (10%:n harjoitustiedoista). Parannamme myös nykytekniikan tasoa espanjan, arabian ja englannin osalta 7%, 4,5% ja 1% (Jaccard Index) sekä teemme luotauskokeita kontekstimallien eri kerrosten vaikutuksen ymmärtämiseksi.', 'sk': 'Virtual Adversarial Training (VAT) je bil učinkovit pri učenju robustnih modelov pod nadzorovanimi in polnadzorovanimi nastavitvami za opravila računalniškega vida in NLP. Vendar učinkovitost DDV za večjezično prepoznavanje čustev in prepoznavanje več znakov še ni bila raziskana. V tem delu raziskujemo DDV za prepoznavanje čustev z več oznakami s poudarkom na izkoriščanju neoznačenih podatkov iz različnih jezikov za izboljšanje učinkovitosti modela. Z enako količino označenih podatkov (10% podatkov o usposabljanju) izvajamo obsežne polnadzorovane eksperimente na večjezičnem in večjezičnem naboru podatkov o prepoznavanju čustev SemEval2018 ter prikazujemo povečanje učinkovitosti 6,2% (arabščina), 3,8% (španščina) in 1,8% (angleščina) v primerjavi z nadzorovanim učenjem. Prav tako izboljšamo obstoječe najsodobnejše različne plasti kontekstualnih modelov za 7%, 4,5% in 1% (Jaccard Index) za špansko, arabsko in angleško ter izvajamo sondiralne poskuse za razumevanje vpliva različnih plasti kontekstualnih modelov.', 'he': 'Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks.  בכל אופן, יעילות המס"א לזהות רגשות רבות-שפותיות ומתוויות רבות לא נחקרה בעבר. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance.  אנו מבצעים ניסויים חצי-מפקחים רחבים על חצי-תווים SemEval2018 ומערכת מידע זיהוי רגשות רבות-שפותית ומראים רווחות ביצועים של 6.2% (ערבית), 3.8% (ספרדית) ו-1.8% (אנגלית) מעל לימוד מפקח עם אותה כמות של מידע תווים (10% מידע אימון). אנחנו משתפרים גם את המצב המיוחד הנוכחי ב-7%, 4.5% ו-1% (אינדקס ג\'אקארד) לספרדית, ערבית ואנגלית בהתאם, ובצענו ניסויים חקירה כדי להבין את השפעה של שכבות שונות של הדוגמנים הקונקסטיים.', 'ha': "@ action: button Haƙĩƙa, ba a jarraba aikin V AT wa ganin hisãbi masu mulki da multilabel ba a gabãni. Daga wannan aikin, munã karatun V AT wa ganin hisãbi masu mulki-abel, da fokus a kan saka da data wanda ba'a yi taƙaita ba daga harshen daban-daban, don ya gyãra tsarin ayuka. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data).  Tuna ƙari da halin-sanar da 7%, 4.5% da 1% (Jakarta index) wa Kispanish, Larabci da Ingiriya kuma ke jarraba jarrabãwa dõmin ka fahimta matsayin zane-zane-zane-zane-zane-zane-zane.", 'bo': 'Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. ཡིན་ནའང་། སྐད་རིགས་དང་ཤོག་བྱང་ཆེ་ཤོས་ཀྱི་བྱ་སྤྱོད་ནི་སྔོན་གྱིས་རྟོགས་བཤེར་མེད་པ་རེད། In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabeled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). ང་ཚོས་ཀྱང་གནས་ཡུལ', 'jv': 'Workspace Advertary Nanging, iku, nggambar PVT kanggo langgar sampeyan luwih lan alam sing berarti Nang barêng-barêng iki, kéné isiha NKAT kanggo masakno luwih ambekan karo percoksi kanggo kuwi nggawe datayang gak nggambar luwih-luwih kanggo ngerasakno paramili. Awak dhéwé éntuk perbudhakan langgar-sistem sing gawe ngupakan karo semi-obligatno karo semi-obligatno karo semi-obligator kanggo ngerasakno dadi nggawe dataset karo perbudhakan kanggo ngerasakno sing 6.2% (arap), 3.8% (Kasempan) lan 1.8% (Inggris) supra supra-obligator kanggo ngerasakno dadi nyong nggawe barang kelas (10%  Awakdhéwé éntuk dhéwé nggawe sistem-kebuturan karo 7%, 4.5% lan 1% (Jackart indeks) kanggo Spanish, arab karo ingles barang nggambar obarané sureten kanggo ngerasakno ujaran dhéwé sing paling nggawe barang alamat sakjané model contextual.'}
{'en': 'Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance', 'pt': 'Analisando os Efeitos dos Tipos de Raciocínio no Desempenho da Transferência Interlíngue', 'fr': 'Analyse des effets des types de raisonnement sur les performances des transferts multilingues', 'ar': 'تحليل آثار أنواع التفكير على أداء التحويل عبر اللغات', 'es': 'Análisis de los efectos de los tipos de razonamiento en el rendimiento de la transferencia multilingüe', 'zh': '推理之类,于言语转移性能', 'ja': '推論タイプがクロスリンガル転送パフォーマンスに及ぼす影響の分析', 'hi': 'क्रॉस-लिंगुअल ट्रांसफर प्रदर्शन पर तर्क प्रकारों के प्रभावों का विश्लेषण करना', 'ru': 'Анализ влияния типов мышления на эффективность кросс-лингвального перевода', 'ga': 'Anailís a dhéanamh ar Éifeachtaí Cineálacha Réasúnúcháin ar Fheidhmíocht Aistrithe Trastheangach', 'ka': 'Name', 'el': 'Ανάλυση των επιπτώσεων των τύπων λογικής στην απόδοση της διασυνοριακής μεταφοράς', 'hu': 'Az érvelési típusok hatásainak elemzése a nyelvközi transzfer teljesítményre', 'it': 'Analizzare gli effetti dei tipi di ragionamento sulle prestazioni di trasferimento cross-lingual', 'kk': 'Тішкі тілдерді тасымалдау мүмкіндігінің себептерінің эффекттерін анализ', 'mk': 'Анализирање на ефектите на размислувањето на типовите на резултатите на трансферот преку јазик', 'lt': 'Pagrindinių tipų poveikio tarpkalbiniam perdavimui analizė', 'ms': 'Menganalisis Kesan Jenis Memakai pada Performasi Pemindahan Selasa-Lingua', 'ml': 'ക്രോസ്- ലിങ്ഗല്\u200d മാറ്റുന്നതിനുള്ള പ്രഭാവങ്ങള്\u200d ശ്രദ്ധിക്കുന്നു', 'mn': 'Дөрвөн хэл дамжуулах үйл ажиллагааны шалтгаан төрлийн нөлөөг шинжилгээ', 'pl': 'Analiza wpływu rodzajów rozumowania na efektywność transferu między językami', 'ro': 'Analiza efectelor tipurilor de raționare asupra performanțelor translingvistice', 'mt': 'Analiżi tal-Effetti tat-Tipi ta’ Raġunar fuq il-Prestazzjoni tat-Trasferiment Translingwali', 'no': 'Analiserer effekten av grunnlag på krysslingsoverføringsprogrammet', 'sr': 'Analiziranje učinka tipa razloga na provedbu transfera preko jezika', 'so': 'Analysing Effects of Reading Types on Cross-Lingual Transfer Performance', 'sv': 'Analysera effekterna av resonanstyper på tvärspråklig överföringsprestanda', 'si': 'Cross- Lingual Transfer Perfection සඳහා හේතුවක් වර්ගයේ ප්\u200dරතිකාර පරීක්ෂණය', 'ur': 'Cross-Lingual Transfer Performance پر Reasoning Types کے اثرات تحلیل کی جاتی ہے', 'ta': 'கிருஸ்- கோட்டு மாற்றும் செயல்பாட்டின் விளைவுகளை ஆய்வு செய்கிறது', 'uz': 'Name', 'vi': 'Phân tích tác động của các loại lý luận lên trình độ truyền thuyết xuyên ngôn ngữ', 'nl': 'Analyse van de effecten van redeneringstypes op crosslinguale transferprestaties', 'bg': 'Анализ на ефектите от типовете разсъждения върху ефективността на междулингвалния трансфер', 'da': 'Analyse af virkningerne af begrundelsestyper på tværs af sproglig overførsel', 'hr': 'Analiziranje učinka različitih tipova na provedbu prekogranične prenošenja', 'ko': '추리 유형이 다중 언어 이동 실적에 미친 영향을 분석하다', 'fa': 'تحلیل اثرات نوع دلیل\u200cسازی روی عملکرد انتقال\u200cهای متصل زبان', 'de': 'Analyse der Auswirkungen von Reasoning Types auf die translinguale Transferleistung', 'tr': 'Çapraz Hat Transfer Performansiýasynda Reaksyonal Taýplaryň etkisini çözümle', 'af': 'Analiseer die Effekte van Redigeerde Tipes op Cross- Lingual Oordrag Performansie', 'am': 'ምርጫዎች', 'hy': 'Տիպերի պատասխանատվության ազդեցությունների վերլուծումը լեզվային հաղորդակցման արդյունքների վրա', 'az': 'Çərz-Çift Transfer Performansındakı Reasoning Types Effects on Analyzing', 'bn': 'ক্রস- লিঙ্গুয়াল পরিবহনের প্রভাব বিশ্লেষণ করা হচ্ছে', 'id': 'Analisasi Efek Jenis Memakai Performasi Transfer Selasa Linguan', 'sw': 'Anachambua Effects of Reading Types on Cross-Lingua Transfer Performance', 'cs': 'Analýza vlivu typů odůvodnění na výkonnost transferu mezi jazyky', 'et': 'Mõistlustüüpide mõju analüüsimine keeleülese ülekande tulemuslikkusele', 'fi': 'Ratkaisutyyppien vaikutusten analysointi kielienväliseen siirtoon', 'bs': 'Analiziranje učinka tipa razloga na provedbu transfera preko jezika', 'sq': 'Analizimi i efekteve të llojeve të arsyetimit në performancën e transferimit ndërgjuhësor', 'ca': 'Analitzar els efectes dels tipus de raonament sobre el rendiment de transferències translingües', 'jv': 'politenessoffpolite"), and when there is a change ("assertive', 'ha': 'Ana analyza Effekt of Resistance on Performance of KCharselect unicode block name', 'he': 'ניתוח ההשפעות של סוגי הגיון על ביצועי העברה לינגולית', 'sk': 'Analiza učinkov razmišljanja na uspešnost medjezikovnega prenosa', 'bo': 'Cross-Lingual Transfer Performance पर Reasoning Types ་དཀའ་ངལ་གྱི་དབྱེ་ཞིབ་དཔྱད་ཞིབ་བྱེད་པ'}
{'en': 'Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of ', 'ar': 'تحقق نماذج اللغات متعددة اللغات دقة مذهلة في إطلاق النار الصفري في العديد من اللغات في المهام المعقدة مثل الاستدلال اللغوي الطبيعي (NLI). غالبًا ما تتعلق الأمثلة في NLI (والمهام المعقدة المكافئة) بأنواع مختلفة من المهام الفرعية ، والتي تتطلب أنواعًا مختلفة من التفكير. أثبتت أنواع معينة من التفكير أنها أكثر صعوبة في التعلم في سياق أحادي اللغة ، وفي سياق اللغات المتقاطعة ، قد تلقي ملاحظات مماثلة الضوء على كفاءة نقل اللقطة الصفرية واختيار عينة قليلة اللقطات. ومن ثم ، لاستقصاء تأثيرات أنواع التفكير على أداء النقل ، نقترح مجموعة بيانات NLI متعددة اللغات مشروحة بفئة ومناقشة تحديات توسيع نطاق التعليقات التوضيحية أحادية اللغة إلى لغات متعددة. نلاحظ إحصائيًا التأثيرات المثيرة للاهتمام التي يحدثها التقاء أنواع التفكير والتشابه اللغوي على أداء النقل.', 'es': 'Los modelos lingüísticos multilingües logran una impresionante precisión cero en muchos idiomas en tareas complejas como la inferencia de lenguaje natural (NLI). Los ejemplos en NLI (y tareas complejas equivalentes) a menudo se refieren a varios tipos de subtareas, que requieren diferentes tipos de razonamiento. Ciertos tipos de razonamiento han demostrado ser más difíciles de aprender en un contexto monolingüe, y en el contexto interlingüístico, observaciones similares pueden arrojar luz sobre la eficiencia de la transferencia de tiro cero y la selección de muestras de pocos disparos. Por lo tanto, para investigar los efectos de los tipos de razonamiento en el rendimiento de la transferencia, proponemos un conjunto de datos NLI multilingüe anotado en categorías y discutimos los desafíos para escalar las anotaciones monolingües a varios idiomas. Observamos estadísticamente efectos interesantes que la confluencia de los tipos de razonamiento y las similitudes lingüísticas tienen en el rendimiento de la transferencia.', 'fr': "Les modèles linguistiques multilingues permettent d'obtenir une précision de tir zéro impressionnante dans de nombreuses langues lors de tâches complexes telles que l'inférence de langage naturel (NLI). Les exemples de NLI (et de tâches complexes équivalentes) concernent souvent divers types de sous-tâches, nécessitant différents types de raisonnement. Certains types de raisonnement se sont révélés plus difficiles à apprendre dans un contexte monolingue, et dans le contexte interlinguistique, des observations similaires peuvent apporter des éclaircissements sur l'efficacité du transfert de dose zéro et sur la sélection de l'échantillon à faible dose. Par conséquent, pour étudier les effets des types de raisonnement sur les performances de transfert, nous proposons un jeu de données NLI multilingue annoté par catégorie et discutons des défis liés à la mise à l'échelle des annotations monolingues à plusieurs langues. Nous observons statistiquement des effets intéressants de la confluence des types de raisonnement et des similitudes linguistiques sur les performances de transfert.", 'pt': 'Os modelos de idiomas multilíngues atingem impressionantes precisão zero em muitos idiomas em tarefas complexas, como Inferência de linguagem natural (NLI). Exemplos em NLI (e tarefas complexas equivalentes) geralmente pertencem a vários tipos de subtarefas, exigindo diferentes tipos de raciocínio. Certos tipos de raciocínio provaram ser mais difíceis de aprender em um contexto monolíngue e, no contexto multilíngue, observações semelhantes podem esclarecer a eficiência da transferência zero-shot e a seleção de amostras de poucos-shot. Assim, para investigar os efeitos dos tipos de raciocínio no desempenho da transferência, propomos um conjunto de dados NLI multilíngue anotado por categoria e discutimos os desafios para dimensionar anotações monolíngues para vários idiomas. Observamos estatisticamente efeitos interessantes que a confluência de tipos de raciocínio e semelhanças de linguagem têm no desempenho da transferência.', 'ja': '多言語モデルは、自然言語推論（ NLI ）などの複雑なタスクで、多くの言語で印象的なゼロショット精度を達成します。NLI （および同等の複雑なタスク）の例は、多くの場合、さまざまな種類の推論を必要とするさまざまな種類のサブタスクに関連しています。特定のタイプの推論は、単一言語の文脈ではより学習が困難であることが証明されており、クロスリンガルの文脈では、同様の観察は、ゼロショット転送効率と少数ショットサンプルの選択を明らかにする可能性がある。したがって、推論の種類が転送パフォーマンスに及ぼす影響を調査するために、カテゴリ注釈付き多言語NLIデータセットを提案し、複数の言語への単語注釈を拡張するための課題について議論します。我々は、推論タイプと言語の類似性の合流が転送パフォーマンスに与える興味深い効果を統計的に観察した。', 'zh': '多言自然语言理 (NLI) 多言深零次精。 NLI之示例(及等效之杂务)常与各种类型子之事,各以其类推之。 夫理已难学于单语境,而于跨语之中,其察可见零次移效率与少样品择也。 是以推理类传输,立别注多言NLI数集,论广单语注多种语言挑战。 以统计学观之,与言语相似性合而有趣也。', 'hi': 'बहुभाषी भाषा मॉडल प्राकृतिक भाषा अनुमान (एनएलआई) जैसे जटिल कार्यों में कई भाषाओं में प्रभावशाली शून्य-शॉट सटीकता प्राप्त करते हैं। एनएलआई (और समकक्ष जटिल कार्यों) में उदाहरण अक्सर विभिन्न प्रकार के उप-कार्यों से संबंधित होते हैं, जिनमें विभिन्न प्रकार के तर्कों की आवश्यकता होती है। कुछ प्रकार के तर्क एक मोनोलिंगुअल संदर्भ में सीखना अधिक कठिन साबित हुए हैं, और क्रॉसलिंगुअल संदर्भ में, इसी तरह की टिप्पणियां शून्य-शॉट ट्रांसफर दक्षता और कुछ-शॉट नमूना चयन पर प्रकाश डाल सकती हैं। इसलिए, स्थानांतरण प्रदर्शन पर तर्क के प्रकारों के प्रभावों की जांच करने के लिए, हम एक श्रेणी-एनोटेट बहुभाषी एनएलआई डेटासेट का प्रस्ताव करते हैं और कई भाषाओं में मोनोलिंगुअल एनोटेशन को स्केल करने के लिए चुनौतियों पर चर्चा करते हैं। हम सांख्यिकीय रूप से दिलचस्प प्रभावों का पालन करते हैं कि तर्क प्रकार और भाषा समानताओं के संगम में स्थानांतरण प्रदर्शन पर होता है।', 'ru': 'Многоязычные языковые модели достигают впечатляющей точности нулевого выстрела во многих языках в сложных задачах, таких как вывод естественного языка (NLI). Примеры в NLI (и эквивалентные сложные задачи) часто относятся к различным типам подзадач, требующих различных видов рассуждений. Некоторые типы рассуждений оказались более трудными для изучения в одноязычном контексте, и в перекрестном контексте аналогичные наблюдения могут пролить свет на эффективность передачи с нулевым выстрелом и выборочный отбор с небольшим выстрелом. Следовательно, для исследования влияния типов рассуждений на эффективность передачи мы предлагаем многоязычный набор данных NLI с категорией аннотации и обсуждаем проблемы масштабирования одноязычных аннотаций для нескольких языков. Мы статистически наблюдаем интересные эффекты, которые слияние типов рассуждений и языковых сходств оказывает на эффективность передачи.', 'ga': 'Baineann samhlacha teanga ilteangacha amach cruinneas suntasach náid ina lán teangacha i dtascanna casta ar nós Tátal Teanga Nádúrtha (NLI). Is minic a bhaineann samplaí in LNÉ (agus tascanna coibhéiseacha casta) le cineálacha éagsúla fo-thascanna, a éilíonn cineálacha éagsúla réasúnaíochta. Tá sé cruthaithe go bhfuil sé níos deacra cineálacha áirithe réasúnaíochta a fhoghlaim i gcomhthéacs aonteangach, agus sa chomhthéacs trasteangach, d’fhéadfadh breathnuithe comhchosúla solas a chaitheamh ar éifeachtúlacht aistrithe náid urchar agus roghnú samplaí cúpla seat. Mar sin, le hiniúchadh a dhéanamh ar éifeachtaí cineálacha réasúnaíochta ar fheidhmíocht aistrithe, molaimid tacar sonraí ilteangacha de chuid LNÉ a bhfuil anót air de réir catagóirí agus pléifimid na dúshláin a bhaineann le scálaiú nótaí aonteangacha chuig iltheangacha. Breathnaímid go staitistiúil ar na héifeachtaí suimiúla atá ag cumar na gcineálacha réasúnaíochta agus na cosúlachtaí teanga ar fheidhmíocht aistrithe.', 'hu': 'A többnyelvű nyelvi modellek számos nyelven lenyűgöző zéró pontosságot érnek el olyan komplex feladatokban, mint a Natural Language Inference (NLI). Az NLI (és ezzel egyenértékű komplex feladatok) példái gyakran különböző típusú alfeladatokra vonatkoznak, amelyek különböző típusú érvelést igényelnek. Bizonyos típusú érvelések egynyelvű kontextusban nehezebbnek bizonyultak, és a többnyelvű kontextusban hasonló megfigyelések rávilágíthatnak a nulla lövéses transzfer hatékonyságára és a kevés lövéses minta kiválasztására. Ezért az érveléstípusok transzfer teljesítményre gyakorolt hatásainak vizsgálata érdekében egy kategóriával megjegyzett, többnyelvű NLI adatkészletet javasolunk, és megvitatjuk azokat a kihívásokat, amelyek az egynyelvű jegyzetek több nyelvre történő skálázásának kihívásait jelentik. Statisztikailag megfigyeljük azokat az érdekes hatásokat, amelyeket az érveléstípusok és a nyelvhasonlóságok összefüggése gyakorol a transzfer teljesítményre.', 'ka': 'მრავალენგური ენის მოდელები დააკეთებენ ინტერფექციური ნულ სტატის კომპლექციების მრავალში კომპლექციური დავალებში, როგორც ნაირთი ენის ინფერენცია (NLI Name მონოლენგური კონტექსტში სწავლად უფრო რთული იქნება რამდენიმე რაოდენობის განსაზღვრება, და კრესენგური კონტექსტში, სხვა დანახვები შეიძლება გახსნა სინამდვილე ნულ-სტრიქტურის გა ამიტომ, გადატანსტრიქტის კონტრიქტის განსაზღვრების ეფექტების შესახებ, ჩვენ კონტრიქტიური მრავალენგური NLI მონაცემების კონტრიქტის განსაზღვრების შესახებ და განსაზღვრებით მონო ჩვენ სტატისტიკურად ინტერესტური ეფექტები ვხედავთ, რომელიც არსებული ტიპების და ენის სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადას', 'it': "I modelli linguistici multilingue raggiungono un'impressionante precisione zero-shot in molte lingue in attività complesse come Natural Language Inference (NLI). Esempi di NLI (e compiti complessi equivalenti) spesso riguardano vari tipi di sotto-compiti, che richiedono diversi tipi di ragionamenti. Alcuni tipi di ragionamento si sono dimostrati più difficili da imparare in un contesto monolingue, e nel contesto translinguale, osservazioni simili possono fare luce sull'efficienza di trasferimento zero-shot e sulla selezione di campioni pochi-shot. Pertanto, per studiare gli effetti dei tipi di ragionamento sulle prestazioni di trasferimento, proponiamo un set di dati NLI multilingue annotato per categoria e discutiamo le sfide per scalare annotazioni monolingue a più lingue. Osserviamo statisticamente effetti interessanti che la confluenza dei tipi di ragionamento e delle somiglianze linguistiche hanno sulle prestazioni di trasferimento.", 'kk': 'Көптеген тіл үлгілері, Түзіндік тілдер инференциясы (NLI) секілді комплексті тапсырмалардың көптеген тілдер тілдерінде нөл түрлі дұрыстығын жеткізеді. NLI мәселелері (және эквиваленттік комплекс тапсырмалар) көбірек төрлі ішкі тапсырмалардың түрлеріне қатынасыз, түрлі түрлерінің түрлері керек. Кейбір түрлері түрлері бірнеше тілдік контекстінде оқыту үшін көбірек болады, және бірнеше тілдік контекстінде ұқсас бақылаулары нөл түрлерінің және бірнеше түрлерінің үлгісін таңдау үшін жарықтығын Сондықтан, транспорттау әрекеттерінің түрлерін зерттеу үшін, біз санатты көптеген NLI деректер жиынын жұмыс істеп, бірнеше тілдерге монолингілік жазбаларды масштабтау үшін өзгерістерді талқылаймыз. Біз статистикалық түрлері мен тілдердің ұқсастықтарының көбіректегі әсерлерін таңдап көреміз.', 'lt': 'Daugiakalbiai kalbų modeliai daugeliu kalbų sukuria įspūdingą nulinio nuotraukos tikslumą, atliekant sudėtingas užduotis, pvz., gamtinės kalbos inferenciją (NLI). NLI pavyzdžiai (ir lygiavertės sudėtingos užduotys) dažnai susiję su įvairių rūšių subužduotimis, kurioms reikalingas skirtingas pagrindimas. Tam tikrų rūšių motyvavimas pasirodė sudėtingesnis vienakalbiu požiūriu, o tarpkalbiniu požiūriu panašios pastabos gali atskleisti nulinio nuotraukos perdavimo efektyvumą ir kelių nuotraukų atranką. Taigi, siekiant ištirti pagrįstumo tipų poveikį perdavimo rezultatams, siūlome kategorijos anotuotą daugiakalbį NLI duomenų rinkinį ir aptariame uždavinius, susijusius su vienokalbių anotacijų mastu į daugelį kalbų. Statistiškai stebime įdomų poveikį, kurį turi motyvavimo tipų ir kalbų panašumo sutrikimas perdavimo rezultatams.', 'ms': 'Model bahasa berbilang mencapai ketepatan-tembakan sifar yang mengesankan dalam banyak bahasa dalam tugas kompleks seperti Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of reasoning.  Jenis-jenis alasan tertentu telah terbukti lebih sukar untuk belajar dalam konteks monobahasa, dan dalam konteks saling bahasa, pengamatan yang serupa mungkin memberikan cahaya pada efisiensi pemindahan 0-shot dan pemilihan sampel beberapa-shot. Oleh itu, untuk menyelidiki kesan jenis alasan pada prestasi pemindahan, kami cadangkan set data NLI berbilang bahasa yang dicatat kategori dan membincangkan cabaran untuk skalakan anotasi monobahasa ke berbilang bahasa. Kami secara statistik mengamati kesan yang menarik bahawa kekacauan jenis pemikiran dan persamaan bahasa mempunyai pada prestasi pemindahan.', 'ml': 'പല ഭാഷ മോഡലുകള്\u200d സ്വാഭാവിക ഭാഷകളില്\u200d പൂര്\u200dണ്ണമായ വെടിവെക്കപ്പെടുന്നതിനുള്ള സൂക്ഷ്മത പൂര്\u200dണ്ണമായ വിശേഷതകള്\u200d പ്രാപ്തികമാ എംലിയിലെ ഉദാഹരണങ്ങള്\u200d( സമാധാനപ്പെട്ട കുഴപ്പമുള്ള ജോലികളും) പല തരം സബ് ജോലികള്\u200dക്കും സംബന്ധിച്ചിടത്തോളം, വ്യത്യസ്ത വിവരങ കുറച്ചു തരം കാരണങ്ങള്\u200d ഒരു മോണോളില്\u200d ഭാഷ പഠിക്കാന്\u200d പ്രയാസമുള്ളതാണെന്ന് തെളിയിച്ചിട്ടുണ്ട്. ക്രോസ്ലിന്\u200dഗ്വാല്\u200d സംസ്കാരത്തില്\u200d ഇതുപോലുള്ള കണ്ടു അതുകൊണ്ട്, മാറ്റങ്ങളുടെ പ്രകടനത്തിന്റെ പ്രഭാവങ്ങള്\u200d പരിശോധിക്കാന്\u200d, നമ്മള്\u200d ഒരു വിഭാഗത്തില്\u200d അഭിപ്രായശ്ചിത്തമായ പല ഭാഷകളുടെ ഡാറ്റാസറ്റ് പ്രൊദ്ദ കാര്യങ്ങളുടെയും ഭാഷയുടെയും സമഭാഷകളുടെയും കൂട്ടത്തില്\u200d കൂടുതല്\u200d ആശ്വാസകരമായ പ്രഭാവങ്ങള്\u200d നമ്മള്\u200d സൂക്ഷിക്കുന്', 'mk': 'Мултијазичните модели на јазик постигнуваат импресивни нула точности на многу јазици во комплексни задачи како што е природната инференција на јазикот (НЛИ). Примери во НЛИ (и еквивалентни комплексни задачи) честопати се однесуваат на различни видови на подзадачи, кои бараат различни видови на размислување. Одредени типови на размислување се покажаа дека се потешко да се научи во монојазичен контекст, и во прекујазичен контекст, слични набљудувања може да објаснат ефикасност на трансференција со нула снимка и избор на примероци со неколку снимки. Затоа, за да ги испитаме ефектите на типовите на размислување за перформансата на трансферот, предложуваме многјазичен нацртан нацрт на податоци на НЛИ со категорија и да разговараме за предизвиците за проширување на монојазичните анотации на повеќе јазици. Статистички ги набљудуваме интересните ефекти кои ги имаат конфлексијата на типовите на размислување и јазичните сличности на пренесувањето.', 'mt': 'Il-mudelli multilingwi jiksbu preċiżjonijiet impressjonanti mingħajr skop f’ħafna lingwi f’kompiti kumplessi bħall-Inferenza tal-Lingwa Naturali (NLI). Eżempji fl-NLI (u kompiti kumplessi ekwivalenti) spiss jappartjenu għal diversi tipi ta’ sottokompiti, li jeħtieġu tipi differenti ta’ raġunament. Ċerti tipi ta’ raġunament urew li huma aktar diffiċli biex jitgħallmu f’kuntest monolingwistiku, u fil-kuntest translingwistiku, osservazzjonijiet simili jistgħu jqajmu dawl fuq l-effiċjenza tat-trasferiment b’ritratt żero u l-għa żla ta’ kampjuni b’ritratt żgħir. Għalhekk, biex ninvestigaw l-effetti ta’ tipi ta’ raġunament fuq il-prestazzjoni tat-trasferiment, nipproponu sett ta’ dejta multilingwi tal-NLI annotat b’kategorija u niddiskutu l-isfidi biex jiġu skalati l-annotazzjonijiet monolingwi għal diversi lingwi. B’mod statistiku, ninsabu effetti interessanti li l-konfluwenza ta’ tipi ta’ raġunament u similaritajiet lingwistiċi għandhom fuq il-prestazzjoni tat-trasferiment.', 'el': 'Τα πολυγλωσσικά μοντέλα γλωσσών επιτυγχάνουν εντυπωσιακές μηδενικές ακρίβειες σε πολλές γλώσσες σε πολύπλοκες εργασίες, όπως το συμπέρασμα φυσικής γλώσσας (NLI). Παραδείγματα σε NLI (και ισοδύναμες σύνθετες εργασίες) συχνά αφορούν διάφορους τύπους υπο-εργασιών, που απαιτούν διαφορετικά είδη συλλογισμού. Ορισμένοι τύποι συλλογισμού έχουν αποδειχθεί ότι είναι πιο δύσκολο να μάθουν σε ένα μονογλωσσικό πλαίσιο, και στο διγλωσσικό πλαίσιο, παρόμοιες παρατηρήσεις μπορεί να ρίξουν φως στην αποδοτικότητα μεταφοράς μηδενικού πυροβολισμού και στην επιλογή δειγμάτων ελάχιστων πυροβολισμών. Ως εκ τούτου, για να διερευνήσουμε τις επιπτώσεις των τύπων συλλογισμού στην απόδοση μεταφοράς, προτείνουμε ένα σύνολο δεδομένων με σχολιασμούς κατηγοριών και συζητάμε τις προκλήσεις για την κλιμάκωση των μονογλωσσικών σχολίων σε πολλές γλώσσες. Παρατηρούμε στατιστικά ενδιαφέροντα αποτελέσματα που έχουν η συγχώνευση τύπων συλλογισμού και γλωσσικών ομοιοτήτων στην απόδοση μεταφοράς.', 'no': 'Fleirspråk-modeller oppnår uttrykkelige null-skrift-akkurasjon i mange språk i komplekse oppgåver som naturspråk-inference (NLI). Eksempel i NLI (og ekvivalente komplekse oppgåver) gjeld ofte forskjellige typar underoppgåver, som krev forskjellige typar motivering. Dette er vanskeleg å lære i ein monospråk kontekst, og i krysspråk-konteksten kan likevel observasjonar gjere lys på effektiviteten for overføring av null-bilete og utval av prøvepunkt for få bilete. For å undersøke effekten av typar rasjonar på overføringsfunksjonen, foreslår vi ein multispråk NLI-dataset med kategori-notasjon og diskuterer utfordringane for å skalera monospråk-notasjonar til fleire språk. Vi observerer statistisk interessante effektar at sammenlikningen av ressurstypar og språk har på overføringsfunksjonen.', 'pl': 'Wielojęzyczne modele językowe osiągają imponującą dokładność zero-shot w wielu językach w złożonych zadaniach, takich jak Natural Language Inference (NLI). Przykłady w NLI (i równoważnych złożonych zadaniach) często odnoszą się do różnego rodzaju podzadań, wymagających różnego rodzaju rozumowania. Niektóre rodzaje rozumowania okazały się trudniejsze do nauczenia się w kontekście jednojęzycznym, a w kontekście wielojęzycznym podobne obserwacje mogą rzucić światło na efektywność transferu zero-shot i selekcję próbek kilku strzałów. W związku z tym, aby zbadać wpływ rodzajów rozumowania na wydajność transferu, proponujemy wielojęzyczny zestaw danych NLI oraz omówimy wyzwania związane ze skalowaniem adnotacji jednojęzycznych do wielu języków. Statystycznie obserwujemy interesujące wpływy typów rozumowania i podobieństw językowych na wydajność transferu.', 'mn': 'Олон хэл загварын загварууд байгалийн хэл инференцийн (NLI) зэрэг олон хэл дээр тэгш шулуун тодорхойлолт гаргадаг. NLI-ийн жишээлүүд (мөн тэнцүү комплекс ажил) нь ихэвчлэн олон төрлийн суб-даалгавартай холбоотой, олон төрлийн бодлого шаарддаг. Зарим төрлийн ойлголтын тухай нэг хэлний байдлаар суралцах нь илүү хэцүү байх боломжтой. Ийм төрлийн ойлголтын тухай нэг хэлний байдлаар тэгш хэлний шилжүүлэлтийн үр дүнтэй гэрэл болон хэд хэдэн шагналын сонголтын тухай гэрэ Иймээс, шилжүүлэх үйл ажиллагаанд урам зориулалтын үр дүнг судалж, олон хэлний NLI өгөгдлийн санг санал болгож, нэг хэлний давтамжлалтыг олон хэлний хэлбэрээр тодорхойлох зорилгоонуудыг ярилцаж байна. Бид статистикийн хувьд ойлголтын төрөл, хэл төрөлхтний төрөлхтний ялгаатай нөлөө үзүүлдэг.', 'so': 'Tusaalada luuqadaha luuqadaha kala duduwan waxay ku helaan saxda nuurka ah oo ku qoran luuqado kala duduwan, tusaale ahaan Nafsurka luqada asalka (NLI). Tusaale ahaan waajibaadka NLI (iyo shaqaalaha adag) waxay inta badan leeyihiin noocyo kala duduwan shaqaalaha sub-shaqada, waxayna u baahan yihiin sababo kala duduwan. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection.  Sidaa darteed, si aan u baarayno saamaynta ka fikirrada sameynta sameynta sameynta wareejinta, waxaan soo jeedinayaa kooxa kooxaha danbiyada NLI ee luuqadaha kala duduwan oo kala duduwan, waxaana kala sheekaynaynaa dhibaatooyinka si ay u kala soocaan alaabta afka noocyada ah oo noocyo badan. Shaqoyinka faa’iidada xiiseynaya, in daboolka noocyada sababta ah iyo isku mid ah luqadu ay ku leedahay sameynta wareejinta.', 'sv': 'Flerspråkiga språkmodeller uppnår imponerande noll-skottnoggrannhet på många språk i komplexa uppgifter som Natural Language Inference (NLI). Exempel på NLI (och motsvarande komplexa uppgifter) gäller ofta olika typer av underuppgifter, vilket kräver olika typer av resonemang. Vissa typer av resonemang har visat sig vara svårare att lära sig i ett enspråkigt sammanhang, och i det korspråkiga sammanhanget kan liknande observationer kasta ljus över noll-skott överföring effektivitet och få-skott urval. Därför, för att undersöka effekterna av typer av resonemang på överföringsprestanda, föreslår vi en kategori-kommenterad flerspråkig NLI dataset och diskuterar utmaningarna att skala enspråkiga kommentarer till flera språk. Vi observerar statistiskt intressanta effekter som samspelet mellan resonemang och språklikheter har på transferprestationen.', 'ta': 'பல மொழி மொழி மாதிரி மாதிரிகள் சூழ்நிலையில் புள்ளியாக்கப்பட்ட சிக்கல் விருப்பங்களை பெறுகிறது. இயல்பான மொழி மொழி முடக்க Name சில வகையான காரணங்கள் ஒரு மொழிமொழியில் கற்றுக் கொள்ள மிகவும் கடினமாக இருக்கும் என்று தெரிவிக்கப்பட்டுள்ளது, மற்றும் குறும்மொழியில் அதே போன்ற பார்வ ஆகையால், மாற்றும் செயல்பாட்டின் விளைவுகளின் விளைவுகளை ஆய்வு செய்வதற்கு, நாம் ஒரு வகையில் அறிவிக்கப்பட்ட பல மொழி NLI தகவல் அமைப்பை பரிந்து பல மொழிகளுக்கு  நாம் புள்ளிவிவரமாக சுவாரஸ்யமான விளைவு', 'ro': 'Modelele lingvistice multilingve obțin acurateți impresionante zero-shot în multe limbi în sarcini complexe, cum ar fi Natural Language Inference (NLI). Exemplele din NLI (și sarcini complexe echivalente) se referă adesea la diferite tipuri de subsarcini, care necesită diferite tipuri de raționament. Anumite tipuri de raționament s-au dovedit a fi mai dificil de învățat într-un context monolingv, iar în contextul translingv, observații similare pot face lumină asupra eficienței transferului zero și selecției eșantioanelor puține fotografii. Prin urmare, pentru a investiga efectele tipurilor de raționament asupra performanței de transfer, propunem un set de date NLI multilingv adnotat pe categorii și discutăm provocările de a scala adnotările monolingve la mai multe limbi. Observăm statistic efecte interesante pe care confluența tipurilor de raționament și similaritățile lingvistice le au asupra performanței de transfer.', 'ur': 'Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). NLI میں مثالیں (اور برابر پیچیدہ کام) کثرت سے مختلف طریقوں کے متعلق ہیں، اور مختلف طریقوں کی تعبیر کی ضرورت ہے. ایک زبان میں سیکھنے کے لئے بہت مشکل ہیں، اور کروسٹ زبان کے متصل میں، مشکل نظریات صفر-شٹ ترنسیٹ فعالیت اور کم-شٹ نمونڈ انتخاب پر نور ڈال سکتے ہیں. لہٰذا، انتقال کے کامیابی پر منطقی طریقوں کے اثرات کی تحقیق کرنے کے لئے، ہم نے ایک کاٹیوں سے منطقی multilingual NLI dataset کی پیشنهاد کرتا ہے اور ایک زبان کے مطابق ایک زبان کے مطابق منطقی اثرات کا مشورہ کرتا ہے۔ ہم اسٹیسٹی طور پر جالب اثرات دیکھتے ہیں کہ منطقی طریقوں اور زبان برابریتیں کے مضابطہ کے ذریعہ تغییرات پر ہے.', 'sr': 'Mnogi jezički modeli postižu impresivne tačnosti nula snimanja na mnogim jezicima u kompleksnim zadacima poput prirodnog jezika (NLI). Primjeri NLI (i ekvivalentni kompleksni zadatak) često se odnose na različite vrste podzadataka, zahtevajući različite vrste razmišljanja. Neke vrste razumanja pokazali su da je teže naučiti u jednojezičkom kontekstu, a u krstojezičkom kontekstu slične promatranja mogu prouzrokovati svjetlost na efikasnost prenošenja nule snimke i izbor uzoraka koji su snimali. Stoga, da bi istražili učinak vrsta razuma na provedbu prenošenja, predlažemo multijezički komplet podataka NLI-a koji su navedeni u kategoriji i razgovarali o izazovima da se između monojezičkih annotacija na višestruke jezike. Statistički posmatramo zanimljive učinke koje utjecaju razumnih tipova i sličnosti jezika imaju na provedbu prenošenja.', 'si': 'ගොඩක් භාෂාවික භාෂාවික මොඩේල්ස් ලබාගන්න පුළුවන් ශුන්වර්ණ ශුන්වර්ණ ශුන්වර්ණ භාෂාවික අවශ Name සම්පූර්ණ වර්ගයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙනවා එක භාෂාවක් සම්පූර්ණයෙන් ඉගෙන ගන්න අමාරුයි කියලා, වර්ගභාෂාවක් සම්ප ඉතින්, ප්\u200dරවර්තනය සඳහා ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් පරීක්ෂණය කරන්න, අපි ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් කරනවා වගේ භාෂ අපි සංඛ්\u200dයාත්මක විශේෂයෙන් ප්\u200dරභාවිත විශේෂය සහ භාෂාත්මක විශේෂයෙන් ප්\u200dරභාවිත විශේෂයේ', 'vi': 'Các mô hình ngôn ngữ đa ngôn ngữ đạt được chuẩn bằng không gây ấn tượng tại nhiều ngôn ngữ trong các công việc phức tạp như Liên Minh ngôn ngữ tự nhiên (NLI). Thí dụ trong NLI (và các nhiệm vụ phức tạp tương đương) thường liên quan tới các dạng phụ trách khác nhau, đòi hỏi các lý lẽ khác nhau. Một số loại lý lẽ đã cho thấy khó khăn hơn để học trong ngữ cảnh ngôn ngữ, và trong ngữ ngoại ngữ, các quan điểm tương tự có thể làm sáng tỏ khả năng chuyển nhượng không phát bắn và chọn mẫu ít bắn. Để nghiên cứu các tác động của các loại lập luận về khả năng chuyển nhượng, chúng tôi đề xuất một tập tin đa chủng loại ghi chú, ghi chú, ghi nhớ, ghi nhớ, ghi vào ngôn ngữ, và thảo luận thử thách áp dụng ghi chú độc ngôn ngữ cho nhiều ngôn ngữ. Theo thống kê chúng ta thấy những hiệu ứng thú vị mà sự kết hợp của các kiểu lập luận và ngôn ngữ tương đồng có trên tiến trình chuyển nhượng.', 'uz': "Bir necha tillar modellari Natalik Tilning Foydalanishi (NLI) kabi murakkab vazifalar bilan bir necha tilda nuqta oʻxshagan foydalanishga ega bo'ladi. Name Ko'pchilik turlari munosabatlar muhimda o'rganish juda qiyin edi. Ko'pchilik tarkibida huddi ko'proq ko'rinish natijalari nuqta shifotsiya va bir necha shaklga yozilgan samolni tanlash mumkin. Shunday qilib, biz bir necha tillar uchun kategori NLI maʼlumot tartibini o'rganish uchun bir xil tildagi narsalarning turini o'rganamiz va bir necha tillar uchun monolingan annotatyatlarni ko'paytirish uchun murakkab qilamiz. Biz qiziqarli effektlarni ko'rib chiqqamiz. Bu haqida ma'lum turlar va tilning huddi bir xil fikrlarini o'zgartirish imkoniyatini ko'rsamiz.", 'bg': 'Многоезичните езикови модели постигат впечатляващи нулеви точности в много езици при сложни задачи като например естествено езиково заключение (НЛИ). Примерите в НЛИ (и еквивалентните сложни задачи) често се отнасят до различни видове подзадачи, изискващи различни видове разсъждения. Някои видове разсъждения са се оказали по-трудни за научаване в едноезичен контекст, а в междуезичния контекст подобни наблюдения могат да хвърлят светлина върху ефективността на трансфера при нулеви изстрели и избора на проби с няколко изстрела. Затова, за да проучим ефекта от типовете разсъждения върху ефективността на трансфера, предлагаме многоезичен набор от данни с анотирани категории и обсъждаме предизвикателствата за мащабиране на едноезичните анотации на множество езици. Статистически наблюдаваме интересни ефекти, които сливането на типове разсъждения и езикови сходства оказва върху ефективността на трансфера.', 'da': 'Flersprogede sprogmodeller opnår imponerende nulskudsnøjagtigheder på mange sprog i komplekse opgaver som Natural Language Inference (NLI). Eksempler i NLI (og tilsvarende komplekse opgaver) vedrører ofte forskellige typer underopgaver, der kræver forskellige former for ræsonnement. Visse typer ræsonnementer har vist sig at være vanskeligere at lære i en ensproget sammenhæng, og i den tværsprogede sammenhæng kan lignende observationer kaste lys over nulskudsoverførselseffektivitet og få skudsprøveudvælgelse. For at undersøge effekten af typer af ræsonnementer på overførsels ydeevne, foreslår vi derfor et kategori-annoteret flersproget NLI datasæt og diskutere udfordringerne med at skalere ensprogede annotationer til flere sprog. Vi observerer statistisk interessante effekter, som sammenhængen mellem ræsonnementstyper og sprogligheder har på transferperformance.', 'nl': 'Meertalige taalmodellen bereiken indrukwekkende zero-shot nauwkeurigheid in vele talen bij complexe taken zoals Natural Language Inference (NLI). Voorbeelden in NLI (en gelijkwaardige complexe taken) hebben vaak betrekking op verschillende soorten subtaken, waarbij verschillende soorten redenering vereist is. Bepaalde soorten redenering zijn moeilijker te leren in een eentalige context, en in de meertalige context kunnen soortgelijke observaties licht werpen op zero-shot transfer efficiency en few-shot steekproefselectie. Om de effecten van soorten redeneren op overdrachtsprestaties te onderzoeken, stellen we daarom een categorie-annotatie meertalige NLI dataset voor en bespreken we de uitdagingen om eentalige annotaties te schalen naar meerdere talen. We observeren statistisch interessante effecten die de samenvloeiing van redeneringstypen en taalovereenkomsten heeft op transferprestaties.', 'hr': 'Mnogi jezički modeli ostvaruju impresivne točnosti nula snimanja na mnogim jezicima u složenim zadatkima poput prirodnog jezika (NLI). Primjeri u NLI (i ekvivalentnih složenih zadataka) često se odnose na različite vrste podzadataka, zahtijevajući različite vrste razumijevanja. U jednojezičkom kontekstu je teže naučiti određene vrste razumijevanja, a u krstojezičkom kontekstu slične promatranja mogu prouzrokovati svjetlost na učinkovitosti prenošenja nule snimke i izbor uzoraka s malo snimka. Stoga, kako bismo istražili učinak vrsta razumjevanja na provedbu prenošenja, predlažemo multijezički komplet podataka NLI-a koji je naveden u kategoriji i razgovarali o izazovima za mjerenje monojezičkih annotacija na višestruke jezike. Statistički posmatramo zanimljive učinke koje utjecaju razumnih vrsta i sličnosti jezika imaju na izvođenje prenošenja.', 'de': 'Mehrsprachige Sprachmodelle erzielen in vielen Sprachen beeindruckende Nullschussgenauigkeiten bei komplexen Aufgaben wie Natural Language Inference (NLI). Beispiele für NLI (und gleichwertige komplexe Aufgaben) beziehen sich oft auf verschiedene Arten von Unteraufgaben, die unterschiedliche Arten von Argumentation erfordern. Bestimmte Arten des Denkens haben sich in einem einsprachigen Kontext als schwieriger erwiesen, und im crosslingualen Kontext können ähnliche Beobachtungen Licht auf die Zero-Shot-Transfer-Effizienz und die Auswahl von wenigen Stichproben werfen. Um die Auswirkungen von Arten von Argumentation auf die Transferleistung zu untersuchen, schlagen wir daher einen kategorie-annotierten mehrsprachigen NLI-Datensatz vor und diskutieren die Herausforderungen, monolinguale Annotationen auf mehrere Sprachen zu skalieren. Wir beobachten statistisch interessante Effekte, die der Zusammenfluss von Argumentationstypen und sprachlichen Ähnlichkeiten auf die Transferleistung hat.', 'ko': '자연 언어 추리(NLI) 등 복잡한 임무에서 다중 언어 모델은 많은 언어에서 인상적인 제로 사격 정밀도를 실현했다.NLI (등효 복잡 작업) 의 예는 보통 여러 종류의 하위 작업과 관련되어 서로 다른 유형의 추리를 필요로 한다.일부 유형의 추리는 단어 환경에서 더욱 배우기 어렵다는 것이 증명되었고, 크로스 언어 환경에서 유사한 관찰 결과는 제로 렌즈의 이동 효율과 소량의 렌즈 샘플 선택을 보여줄 수 있다.따라서 추리 유형이 이전 성능에 미치는 영향을 연구하기 위해 우리는 분류 주석의 다중 언어 NLI 데이터 집합을 제기하고 단어 주석을 다양한 언어로 확장하는 도전에 대해 토론했다.우리는 통계학적 측면에서 추리 유형과 언어의 유사성 융합이 이동 실적에 미치는 흥미로운 영향을 관찰했다.', 'id': 'Model bahasa berbilang mencapai akurasi zero-shot yang mengesankan dalam banyak bahasa dalam tugas kompleks seperti Natural Language Inference (NLI). Contoh dalam NLI (dan tugas kompleks yang sama) sering berkaitan dengan berbagai jenis sub-tugas, yang membutuhkan berbagai jenis alasan. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection.  Oleh karena itu, untuk menyelidiki efek dari jenis alasan pada prestasi transfer, kami mengusulkan sebuah kategori-annotasi dataset NLI berbagai bahasa dan mendiskusikan tantangan untuk skala annotasi monobahasa ke berbagai bahasa. Kami secara statistik mengamati efek yang menarik bahwa konfluensi dari tipe pemikiran dan persamaan bahasa memiliki pada prestasi transfer.', 'fa': 'مدل\u200cهای زبان\u200cهای زیادی دقیقات صفر\u200cشلیک تحت تاثیر قرار می\u200cگیرد در زبان\u200cهای زیادی در کار\u200cهای پیچیده مانند زبان طبیعی (NLI). مثالها در NLI (و کارهای پیچیده\u200cای مشابه) اغلب به گونه\u200cهای زیر کار\u200cهای مختلف ارتباط دارند، که نیاز به گونه\u200cهای منطقی متفاوت دارند. برخی از نوع منطقی ثابت شده\u200cاند که برای یاد گرفتن در یک محیط تک زبان سخت\u200cتر است، و در محیط مختلف زبان، مشاهده\u200cهای مشابه\u200cای ممکن است نور روی فعالیت انتقال تصویر صفر و انتخاب نمونه\u200cهای کمی شلیک کند. بنابراین، برای تحقیق اثرات نوع منطقی روی اجرای انتقال، یک مجموعه داده های NLI چندین زبان را پیشنهاد می\u200cکنیم و در مورد چالش\u200cها برای اندازه\u200cگیری نوشته\u200cهای یک زبان به زبان\u200cهای متعدد بحث می\u200cکنیم. ما تحت نظر آماری اثرات جالبی را می بینیم که مشترک نوع منطقی و شبیه\u200cهای زبان بر عملکرد انتقال دارند.', 'tr': 'Birnäçe dil nusgalary täsirli dillerde, tebigy dil nusgalary (NLI) ýaly täsirli zadlarda täsirli nokatlar ýetip bilýär. NLI misalleri Käbir hilli razylyk bir monodil kontekstinde öwrenmek üçin has kyn bolup görünýär, we cross dil kontekstinde meňzeş seretler 0-aty üstünde ýagtylygy ýüze çykaryp biler. Şol sebäpli, transfers performansynda aklawjyň täsirlerini barlamak üçin, biz kategoriýaly nämli diller hasaplanýan bir multi diller barlag maslahat berýäris we monolingüň täzeliklerini birnäçe dillere gollaşdyrmak üçin kynçylyklary barlaýarys. Biz statistik ýaly düşünüp görnüş hili we dil meňzeşlikleriniň üstünliklerine seredişi ýaly gyzykly täsirlerde gözleýäris.', 'af': "Veelvuldige taal-modelles bereik inpresiewe nul-skoot-akkurasies in baie tale in komplekse taak soos Natuurlike Taal Inferensie (NLI). Voorbeelde in NLI (en gelykivalente komplekse taak) het dikwels aan verskillende tipes sub-taak behoort, nodig verskillende soorte redening. Bepaalde tipes redening het bevestig dat meer moeilik is om te leer in 'n monolinglike konteks, en in die kruistale konteks, soortgelyke observasies mag lig op nul-skoot oordrag effektiviteit en paar-skoot-voorbeeld kies uitgelei word. Daarom, om te ondersoek die effekte van soorte redenings op oordrag uitvoer, voorstel ons 'n kategorie-annotateerde multitaalske NLI datastel en bespreek die uitdrukkings om monotaalse notasies te skaleer na veelvuldige tale. Ons het statisties interessante effekte aanhou dat die samelewing van redekende tipes en taal gelykenisse op oordragspraak het.", 'sq': 'Modelet e gjuhës shumëgjuhëse arrijnë saktësi mbresëlënëse zero-shot në shumë gjuhë në detyra komplekse të tilla si Natural Language Inference (NLI). Shembuj në NLI (dhe detyra ekvivalente komplekse) shpesh i përkasin llojeve të ndryshme të nëndetyrave, duke kërkuar lloje të ndryshme arsyetimi. Disa lloje arsyetimi kanë provuar të jenë më të vështirë për të mësuar në një kontekst monogjuhësor dhe në kontekstin ndërgjuhësor, vëzhgime të ngjashme mund të hedhin dritë mbi efektshmërinë e transferimit zero-shot dhe zgjedhjen e disa-shot kampionëve. Kështu, për të hetuar efektet e llojeve të arsyetimit mbi performancën e transferit, ne propozojmë një grup të dhënash shumëgjuhësore të kategorisë të anotuar NLI dhe diskutojmë sfidat për të shkallëruar anotacionet monogjuhësore në gjuhë të shumëgjuhëshme. Ne statistikisht vëzhgojmë efekte interesante që konfluenca e llojeve të arsyetimit dhe ngjashmërive gjuhësore kanë mbi performancën e transferit.', 'am': 'የቋንቋ ቋንቋዎች ምሳሌዎች የ0-shot እርግጠኝነትን በብዙ ቋንቋዎች እንደሚያሳየው ትክክለኛነት አግኝቷል፤ እንደ ትዳርመኛ ቋንቋ መግለጫ (NLI) በሚመስል ትክክለኛ ስራ ነው፡፡ የNLI ምሳሌዎች (እና ትክክል ተጨማሪ ስራዎችን) ብዙ ጊዜ ለጥያቄ ሥርዓቶች በተለያዩ ዓይነቶች ይታሰራሉ፡፡ አንዳንድ ዓይነት አዋቂዎች በሞያሊ ቋንቋ ውስጥ ለመማር በጣም ችግር ሆኖአል፣ በመስቀል ቋንቋ ውስጥ፣ እንደዚህ የሚመስል ዓይነት በ0-shot transfer ውጤት እና ጥቂት የነጥብ ምሳሌ ምርጫ ላይ ያበራል፡፡ ስለዚህም የልዩ ልዩ ልዩ ልዩ ልዩ ቋንቋ አካባቢዎችን ለመመርመር እናሳውቃለን፡፡ በቁጥር የሚጠቃቀሙትን ነገር እና የቋንቋ ዓይነት እና ምሳሌ በመለወጥ ጥያቄ እናደርጋለን፡፡', 'hy': 'Բազլեզվով լեզվի մոդելները հասնում են բազմաթիվ լեզուներում զարմանահրաշ ճշգրիտություններին բարդ խնդիրներում, ինչպիսիք են օրինակ Բնական լեզվի ինֆերենսը (ՆԼԻ): ՆԼԻ-ի օրինակները (և համարժեք բարդ խնդիրները) հաճախ կապված են տարբեր ենթախնդիրների տեսակների հետ, որոնք պահանջում են տարբեր տեսակի մտածողություններ: Որոշ տեսակի մտածողությունները պարզվեցին, որ ավելի դժվար է սովորել միալեզվով կոնտեքստում, և խաչլեզվով կոնտեքստում, նմանատիպ դիտարկումները կարող են լուսավորել զրոյական նկարների փոխանցման արդյունավետությունը և մի քանի նկարների ընտրությունը: Այսպիսով, վերլուծելու համար տեղափոխման արդյունքները մենք առաջարկում ենք մի կատեգորիայի նշումներ ունեցող բազլեզու ՆԼԻ տվյալների համակարգ և քննարկում ենք մարտահրավերները մեկլեզու նշումների ընդլայնման համար բազմալեզուների համար: Մենք վիճակագրաբար դիտարկում ենք հետաքրքիր ազդեցություններ, որոնք ունեն մտածողության տեսակների և լեզվի նմանությունների շփումը փոխանցման արդյունքների վրա:', 'az': 'Çoxlu dil modelləri növbənöv dillərdə növbənöv Dil Inference (NLI) kimi kompleks işlərdə növbənöv fərqli növbənöv fərqli növbənöv fərqli növbənöv fərqli növbənöv NLI misalları (və eyni müxtəlif kompleks işləri) çox müxtəlif apar işləri ilə məxsusdur, müxtəlif müzakirələr istəyir. Bazı razılıq növlərinin bir monodil məlumatında öyrənmək daha çətin olduğunu göstərmişdir, və çox dil məlumatında, bənzər gözləmələr sıfır-şəkil tərəfdarlığı və bir neçə-şəkil nümunələri seçmək üçün işıq yaratabilir. Buna görə də, transfer performansından razılaşma növlərinin etkisini incitmək üçün, kategoriya ilə çoxlu dil NLI veri qurmasını təklif edirik və monodil məlumatlarını çoxlu dillərə ölçmək üçün çətinlikləri mübahisə edirik. Biz statistik olaraq dəyişiklik növlərin və dillərin bənzərinin istifadə edilməsi haqqında maraqlı etkisini görürük.', 'bn': 'বহুভাষাভাষী ভাষার মডেল অনেক ভাষায় জটিল ভাষায় যেমন প্রাকৃতিক ভাষার ইনফারেন্স (এনলি এনলির উদাহরণ (এবং সমান জটিল কাজ) প্রায়শই বিভিন্ন ধরনের সাব-কাজের প্রতি বিভিন্ন ধরনের বিভিন্ন ধরনের বিভিন্ন ধরনের কাজের কিছু ধরনের কারণ প্রমাণ করেছে যে অনুভূতিভাষায় শিখা কঠিন, আর ক্রস্লাঙ্গুয়েল প্রেক্ষেত্রে একই ধরনের দৃষ্টিভঙ্গি শূন্য-গুলি পরিবর্তনের দক্ষতা এবং কয়েকট যার ফলে আমরা বিভিন্ন ভাষায় বিভিন্ন ভাষার বিভিন্ন ভাষার প্রভাব পরিচালিত করার প্রস্তাব প্রস্তাব করি এবং বেশ কয়েকটি ভাষায় বিভিন্ন ভাষার বিভিন্ন ভাষার ব আমরা পরিসংখ্যানে কৌতুহল প্রভাব দেখি যে কোন ধরনের কাজ এবং ভাষার একই ধরনের সম্মুখীন হচ্ছে সেগুলো বিনিময়ের কাজের উপরে।', 'sw': 'Mradi wa lugha za lugha nyingi hupata uhakika usio na sifa kwa lugha nyingi katika kazi tatizo kama vile Uhamasishaji wa lugha ya asili (NLI). Mifano ya NLI (na kazi za tatizo) mara nyingi huhusiana na aina mbalimbali za kazi za subira, zinazohitaji aina tofauti za maana. Baadhi ya a in a mbalimbali za maana imethibitisha kuwa vigumu kujifunza katika muktadha wa lugha, na katika muktadha wa lugha, maoni yanayoweza kuonyesha mwangaza juu ya ufanisi wa usafirishaji wa sifuri na uchaguzi wa sampuli chache zilizopigwa risasi. Kwa hiyo, ili kuchunguza madhara ya aina ya kujadili kuhusu utendaji wa uhamiaji, tunapendekeza seti ya data za NLI katika lugha mbalimbali na kujadili changamoto za kupambana na matangazo ya lugha za kimonolinguli kwa lugha mbalimbali. Takwimu tunaangalia athari za kusisimua kwamba utangazaji wa aina za maana na aina za lugha zinazofanana na utendaji wa uhamishaji.', 'ca': "Els models de llenguatges multilingües aconsegueixen precisions impressionants de fotografia zero en moltes llengües en tasques complexes com la Inferència de Llingua Natural (NLI). Els exemples de l'INN (i tasques complexes equivalents) sovint pertanyen a diversos tipus de subtasques, que requereixen diferents tipus de raonament. Alguns tipus de raonament s'han demostrat més difícils d'aprendre en un context monolingüe, i en el context translingüe, observacions similars poden fer llum sobre l'eficiència de transfer ència de fotografies zero i la selecció de mostres de fotografies pocs. Per això, per investigar els efectes dels tipus de raonament sobre el rendiment de la transfer ència, proposem un conjunt de dades multilingües de l'INN anotats per categoria i discutem els reptes per escalar les anotacions monolingües a múltiples llengües. Observem estadísticament efectes interessants que la confluencia de tipus de raonament i similituds lingüístices tenen en el rendiment de la transfer ència.", 'fi': 'Monikieliset kielimallit saavuttavat vaikuttavia nollatarkkuuksia monilla kielillä monimutkaisissa tehtävissä, kuten Natural Language Inference (NLI). Esimerkkejä NLI:stä (ja vastaavista monimutkaisista tehtävistä) liittyy usein erityyppisiin alitehtäviin, jotka edellyttävät erityyppistä päättelyä. Tietyntyyppiset päättelyt ovat osoittautuneet vaikeammiksi oppia yksikielisessä kontekstissa, ja monikielisessä kontekstissa samankaltaiset havainnot voivat valaista nollakuvan siirtotehokkuutta ja harvojen otosten valintaa. Tutkiaksemme päättelytyyppien vaikutuksia siirtotehokkuuteen ehdotamme luokkamerkinnällä varustettua monikielistä NLI-aineistoa ja keskustelemme haasteista skaalata yksikielisiä huomautuksia useille kielille. Tilastollisesti havainnoimme mielenkiintoisia vaikutuksia, joita päättelytyyppien ja kielellisten samankaltaisuuksien yhteentörmäyksellä on siirtosuoritukseen.', 'bs': 'Mnogi jezički modeli postigli su impresivne tačnosti nula snimanja na mnogim jezicima u kompleksnim zadacima poput prirodnog jezika Inferencije (NLI). Primjeri u NLI (i ekvivalentnim kompleksnim zadacima) često se odnose na različite vrste podzadataka, zahtijevajući različite vrste razumijevanja. Neke vrste razumanja pokazale su teže naučiti u jednojezičkom kontekstu, a u krstojezičkom kontekstu slične promatranja mogu prouzrokovati svjetlost na učinkovitosti prenošenja nule snimke i izbor uzoraka koji su snimali. Stoga, kako bi istražili učinak vrsta razuma na provedbu prenošenja, predlažemo multijezički komplet podataka NLI-a koji su navedeni u kategoriji i razgovarali o izazovima za mjerenje monojezičkih annotacija na višestruke jezike. Statistički posmatramo zanimljive učinke koje utjecaju razumnih tipova i sličnosti jezika imaju na provedbu prenošenja.', 'cs': 'Vícejazyčné jazykové modely dosahují působivé nulové přesnosti v mnoha jazycích při složitých úlohách, jako je například Natural Language Inference (NLI). Příklady NLI (a ekvivalentních komplexních úkolů) se často týkají různých typů dílčích úkolů, které vyžadují různé druhy uvažování. Některé typy uvažování se ukázaly být obtížnější naučit v jednojjazyčném kontextu a v kontextu více jazyků mohou podobné pozorování vrhnout světlo na efektivitu přenosu nulových výstřelů a výběr vzorků několika výstřelů. Proto pro zkoumání vlivu typů uvažování na přenosovou výkonnost navrhujeme kategorii anotovaný vícejazyčný NLI datový soubor a diskutujeme výzvy škálování monojazyčných anotací do více jazyků. Statisticky pozorujeme zajímavé účinky, které soutok typů uvažování a jazykových podobností má na přenosovou výkonnost.', 'et': 'Mitmekeelsete keelemudelitega saavutatakse paljudes keeltes muljetavaldav nullkatse täpsus keerukates ülesannetes, näiteks loomuliku keele järeldus (NLI). NLI näited (ja samaväärsed keerukad ülesanded) puudutavad sageli erinevat tüüpi alamülesandeid, mis nõuavad erinevat tüüpi arutlust. Teatud tüüpi arutluskäike on osutunud keerulisemaks õppida ühekeelses kontekstis ning keeleüleses kontekstis võivad sarnased vaatlused valgustada nullpildi ülekande efektiivsust ja vähese pildi valimist. Seetõttu, et uurida põhjenduste tüüpide mõju ülekandejõudlusele, pakume välja kategooriaga annoteeritud mitmekeelse NLI andmekogumi ja arutame väljakutseid ühekeelsete annoteerimiste skaaleerimisel mitmekeelsetesse keeltesse. Statistiliselt täheldame huvitavaid mõjusid, mida mõtlemistüüpide ja keeleliste sarnasuste ühinemine avaldab siirdejõudlusele.', 'jv': 'Mulalapun lang model sing gawe akeh sing perthi sapa-ot, 0-ot kuwi sukane ing sakjane tindang karo nggawe barang sing komplikasi koyo ngono Perusahaan Language Info (NLI). Coffeine politenessoffpolite"), and when there is a change ("assertive Kowe dadi, nggunakake efek karo perusahaan akeh sampeyan kanggo nggawe gerakan kanggo nggawe kategori-nambarang multilenguang NLI dataset dan nggunakake perusahaan kanggo masalah sampeyan unggal kanggo ngilanggar. Awak dhéwé éntuk dadi sing ngerasakno perbudhakan nganggep nggawe gerakan karo hal-hal nganggep nggawe gerakan karo nggawe barang', 'ha': "Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI).  Misãlai cikin NLI (da ke daidai wa aikin adadi) ko da yawa, yana da wasu nau'i-aikin sub-taskõki, kuma yana buƙata wasu masu hankali daban-daban. Babu wasu nau'i masu shawara sun jarraba su zama mafi tsanani a karanta cikin mazaɓa na littafan ayuka, kuma a cikin mazaɓa na tsohofalin na tsohon, masu yiwuwa sun yi haske a kan fassarar transfer da sifiri da kuma zaɓen misãlai masu sauri. To, don haka, dõmin ka yi ƙidãya ga fassarar mutane da ke yi amfani da aikin transfer, sai mu bububuɗa wani danne na-danganta na NLI da aka sanar da katori-da-baka mulki-lingui kuma mu yi musu jãyayya masu kansala ta kamata kunnufi masu motsa da sunayen monoli-harshe zuwa wasu harshe. Tuna ganin fassarar amfani da wasu fasanni da ke samun nau'i da harshe da misalin misãlai da za'a yi shige.", 'he': 'דוגמני שפות רבות משיגים מדויקות אפס מרשים בשפות רבות במשימות מורכבות כמו שפות טבעיות (NLI). דוגמאות ב-NLI (ומשימות מורכבות שוויות) לעתים קרובות קשורות לסוגים שונים של תחת-משימות, שדורשות סוגים שונים של הגיון. סוגים מסויימים של הסיבה הוכיחו להיות קשים יותר ללמוד בקשר מונושפתי, ובהקשר המשולב שפתי, תצפיות דומות יכולות לשחרר אור על יעילות העברה של אפס צילומים ובבחירת דגימות של כמה צילומים. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages.  אנו מסתכלים באופן סטטיסטי על השפעות המעניינות שיש להסתובבות של סוגי ההיגיון והדומות לשפה על ביצועי העברה.', 'sk': 'Večjezični jezikovni modeli dosegajo impresivno ničelno natančnost v številnih jezikih pri zapletenih opravilih, kot je sklepanje naravnega jezika (NLI). Primeri v NLI (in enakovrednih kompleksnih nalogah) se pogosto nanašajo na različne vrste podnalog, ki zahtevajo različne vrste razmišljanja. Nekatere vrste razmišljanja se je izkazalo za težje naučiti v enojezičnem kontekstu, v medjezičnem kontekstu pa lahko podobna opazovanja osvetlijo učinkovitost prenosa brez strela in izbiro vzorcev z malo streli. Zato, da bi raziskali učinke vrst razmišljanja na uspešnost prenosa, predlagamo kategorijski večjezični nabor podatkov NLI in razpravljamo o izzivih pri razširitvi enojezičnih opomb v več jezikov. Statistično opazujemo zanimive učinke, ki jih ima združevanje tipov razmišljanja in jezikovnih podobnosti na uspešnost prenosa.', 'bo': 'སྐད་རིགས་ཀྱི་མིང་དཔེ་དབྱིབས་མང་ཆེ་བའི་སྐད་ཡིག NLI ནང་གི་དཔེར་བརྗོད།སྔོན་པ་ནང་དུ་མཐུན་རྐྱེན་གྱི་བྱ་འགུལ་ཡང་རྒྱུན་ལྡན་ཡོད། Some types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. དེར་བརྟེན། ང་ཚོས་དབྱེ་རིགས་ཀྱི་རྒྱུ་རྐྱེན་འདི་སྐྱེལ་འདྲེན་ནང་གི་གོ་སྐབས་ལ་བལྟ་ཞིབ་བྱེད་ན་ང་ཚོས་རིགས་སྣང་ཨང་གྲངས་སྒྲིག་ཆ་རྣམས་ནང་ལ་བཤད ང་ཚོས་གཞི་རྩིས་ནས་མཐོང་ནུས་ཡོད་པའི་དབྱེ་བ་དང་སྐད་རིགས་དང་མཐུན་པ་ཚོར་གནས་སྟངས་འབྲེལ་བ་ཡོད།'}
{'en': 'Shaking Syntactic Trees on the Sesame Street : Multilingual Probing with Controllable Perturbations', 'pt': 'Agitando árvores sintáticas na Vila Sésamo: sondagem multilíngue com perturbações controláveis', 'ar': 'هز الأشجار النحوية في شارع سمسم: سبر متعدد اللغات مع اضطرابات يمكن السيطرة عليها', 'es': 'Sacudiendo árboles sintácticos en Sesame Street: sondeo multilingüe con perturbaciones controlables', 'fr': 'Secouer les arbres syntaxiques dans la rue Sésame\xa0: sondage multilingue avec perturbations contrôlables', 'ja': 'セサミストリートで構文木を揺るがす：制御可能な摂動を備えた多言語プローブ', 'zh': '芝麻街上摇句法树:有可控扰者多言探', 'hi': 'तिल स्ट्रीट पर वाक्यात्मक पेड़ मिलाते हुए: नियंत्रणीय बाधाओं के साथ बहुभाषी जांच', 'ru': 'Встряхивание синтаксических деревьев на Кунжутной улице: многоязычное зондирование с контролируемыми возмущениями', 'ga': 'Croitheadh Crainn Syntactic ar Shráid Sesame: Scrúdú Ilteangach le suaitheadh Inrialaithe', 'el': 'Συγκρατικά Δέντρα στην οδό Σεσάμι: Πολυγλωσσική ανίχνευση με ελεγχόμενες διαταραχές', 'hu': 'Rázás szintaktikus fák a szezám utcán: többnyelvű szondázás kontrollálható zavarokkal', 'it': 'Tremare alberi sintattici sulla via del sesamo: sonde multilingue con perturbazioni controllabili', 'kk': 'Sesame көшесінде синтактикалық ағаштарды түсіру: Көптеген тілдерді басқару мүмкіндігін бақылау', 'mk': 'Синтактичките дрвја се тресаат на улицата Сезаме: Мултијазични проверки со контролирани пречки', 'lt': 'Sintaktiniai drebulys Sesamo gatvėje: daugiakalbis bandymas su kontroliuojamais sutrikimais', 'ms': 'Bergerak Pohon Sintaktik di Jalan Sesame: Pengujian Berbahasa dengan Pergangguan yang boleh Dikawal', 'ka': 'Sesame სტრიტის სინტაქტიკური ხე: მრავალენგური მოწმება კონტროლური Perturbations', 'mt': 'Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations', 'no': 'Trekk syntaktiske tre på Sesame Street: Multilingual Probation with Controlable Perturbations', 'pl': 'Trzęsienie drzew syntaktycznych na ulicy sezamowej: wielojęzyczne sondowanie z kontrolowanymi zakłóceniami', 'ro': 'Tremurarea copacilor sintactici pe strada de susan: sondare multilingvă cu perturbări controlabile', 'sr': 'Trešenje sintaktièkih drveæa na ulici Sesame: Multilingual Probation with Controlable Perturbations', 'si': 'Name', 'so': 'Geedka Syntactic ee jidka Sesame:', 'ta': 'Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations', 'ml': 'സെയോ തെരുവിലെ സിന്റാക്റ്റിക് വൃക്ഷങ്ങള്\u200d ഷാക്കിംഗ് ചെയ്യുന്നു: നിയന്ത്രണത്തിന്റെ നിയന്ത്രണത്തില', 'mn': 'Sesame гудамжны Синтактик мод: Хэрэглэгдэх боломжтой Перturbations-тай олон хэлний судалгаа', 'sv': 'Skaka syntaktiska träd på Sesamgatan: Flerspråkig sondering med kontrollerbara störningar', 'ur': 'سیسم سیٹریٹ پر سینٹکتیک درخت شیک کر رہے ہیں: کنٹرولوبل پرٹربیٹ کے ساتھ Multilingual Probing', 'uz': 'Name', 'vi': 'Chạm tay của cây Tâm pháp trên đường Sesame: phát âm ngôn ngữ chung với khả năng kiểm soát', 'bg': 'Треперещи синтактични дървета на улица Сусам: многоезично сондиране с контролирани смущения', 'da': 'Rystende Syntaktiske Træer på Sesamgaden: Flersproget Probing med kontrollerbare Perturbations', 'nl': 'Het schudden van syntactische bomen op de Sesamstraat: meertalige sondering met regelbare verstoringen', 'de': 'Erschütternde syntaktische Bäume auf der Sesamstraße: Mehrsprachige Sondierung mit kontrollierbaren Störungen', 'hr': 'Trešenje sintaktičkih drveća na ulici Sesame: Multilingual Probation with Controlable Perturbations', 'id': 'Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Control Perturbations', 'ko': '《참깨 거리에서 흔들리는 문법수: 교란을 제어할 수 있는 다중 언어 탐지》', 'fa': 'درخت\u200cهای سنتاکتیک در خیابان Sesame: امتحان زیادی زبان با توجه\u200cهای کنترل قابل کنترل', 'sw': 'Mtuo wa Syntactic kwenye mtaa wa Sesame: Utaratibu wa lugha nyingi kwa Makatiliano ya Udhibiti', 'tr': 'Sesame köçesinde Sintaktik agaç atylmak: Köp diller Kontrol Etkinleýän Perturbasyon bilen syntaksy', 'af': 'Soek Syntaktiese Boom op die Sesame Straat: Multilingual Probeer met Kontrolerbare Perturbasies', 'sq': 'Dridhja e pemëve sintaktike në rrugën Sesame: Provë shumëgjuhëse me shqetësime të kontrollueshme', 'hy': 'Սեսամի փողոցում շարժվող սինտակտիկ ծառերը՝ բազլեզու փորձում կառավարելի շեղումներով', 'am': 'Syntactic Trees on the Sesame Street: Multilingual Probining with Controllable Perturbations', 'az': 'Sesame Sokağında Sintaktik Ağacları Shaking: Kontrol Yapılabilir Perturbations ilə çoxlu Dil Sınama', 'bn': 'সেই স্ট্রীটে সিন্ট্যাক্টিক গাছ Shaking Shaking: Multilingual Probing with Controllable Perturbations', 'cs': 'Třesení syntaktických stromů na sezamové ulici: vícejazyčné sondování s kontrolovatelnými perurbacemi', 'bs': 'Trešenje sintaktičkih drveća na ulici Sesame: Multilingual Probation with Controlable Perturbations', 'ca': 'Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations', 'fi': 'Seesamikadulla tärisevät syntaktiset puut: monikielinen luotaus kontrolloitavilla häiriöillä', 'et': 'Süntaktiliste puude raputamine seesami tänaval: mitmekeelne uurimine kontrollitavate häiretega', 'jv': 'Ngawe Sentraksi Jaring', 'sk': 'Tresenje sintaktičnih dreves na sezamovi ulici: večjezično sondiranje z nadzorovanimi motnjami', 'ha': 'KCharselect unicode block name', 'he': 'עצים סינטאקטיים נוערים ברחוב סיסם: ניסוי רבולוגי עם הפרעות שולטות', 'bo': 'Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations'}
{'en': 'Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the ', 'pt': 'Pesquisas recentes adotaram um novo campo experimental centrado no conceito de perturbações de texto, que revelou que a ordem de palavras embaralhada tem pouco ou nenhum impacto no desempenho downstream de modelos de linguagem baseados em Transformer em muitas tarefas de PNL. Esses achados contradizem o entendimento comum de como os modelos codificam informações hierárquicas e estruturais e até questionam se a ordem das palavras é modelada com embeddings de posição. Para este fim, este artigo propõe nove conjuntos de dados de sondagem organizados pelo tipo de perturbação de texto controlável para três idiomas indo-europeus com um grau variável de flexibilidade de ordem de palavras: inglês, sueco e russo. Com base na análise de sondagem dos modelos M-BERT e M-BART, relatamos que a sensibilidade sintática depende da linguagem e dos objetivos do pré-treinamento do modelo. Também descobrimos que a sensibilidade cresce através das camadas juntamente com o aumento da granularidade da perturbação. Por último, mas não menos importante, mostramos que os modelos mal usam a informação posicional para induzir árvores sintáticas a partir de suas representações intermediárias de autoatenção e contextualizadas.', 'es': 'Investigaciones recientes han adoptado un nuevo campo experimental centrado en el concepto de perturbaciones del texto que ha revelado que el orden de las palabras barajadas tiene poco o ningún impacto en el rendimiento descendente de los modelos de lenguaje basados en Transformer en muchas tareas de PNL. Estos hallazgos contradicen la comprensión común de cómo los modelos codifican información jerárquica y estructural e incluso cuestionan si el orden de las palabras se modela con incrustaciones de posiciones. Con este fin, este artículo propone nueve conjuntos de datos de sondeo organizados por el tipo de perturbación de texto controlable para tres idiomas indoeuropeos con un grado variable de flexibilidad en el orden de las palabras: inglés, sueco y ruso. Basado en el análisis de sondeo de los modelos M-BERT y M-BART, informamos que la sensibilidad sintáctica depende del lenguaje y los objetivos de preentrenamiento del modelo. También encontramos que la sensibilidad crece a través de las capas junto con el aumento de la granularidad de la perturbación. Por último, pero no menos importante, mostramos que los modelos apenas utilizan la información posicional para inducir árboles sintácticos a partir de su autoatención intermedia y representaciones contextualizadas.', 'ar': 'اعتمدت الأبحاث الحديثة مجالًا تجريبيًا جديدًا يتمحور حول مفهوم اضطرابات النص والذي أظهر أن ترتيب الكلمات المترابط له تأثير ضئيل أو معدوم على الأداء النهائي لنماذج اللغة المستندة إلى Transformer عبر العديد من مهام البرمجة اللغوية العصبية. تتناقض هذه النتائج مع الفهم الشائع لكيفية ترميز النماذج للمعلومات الهرمية والهيكلية ، بل وتتساءل أيضًا عما إذا كان ترتيب الكلمات قد تم نمذجته مع تضمين الموضع. تحقيقا لهذه الغاية ، تقترح هذه الورقة تسع مجموعات بيانات استقصائية منظمة حسب نوع اضطراب النص القابل للتحكم لثلاث لغات هندو أوروبية مع درجة متفاوتة من مرونة ترتيب الكلمات: الإنجليزية والسويدية والروسية. استنادًا إلى التحليل الاستقصائي لنماذج M-BERT و M-BART ، أبلغنا أن الحساسية النحوية تعتمد على اللغة وأهداف ما قبل التدريب النموذجي. نجد أيضًا أن الحساسية تنمو عبر الطبقات جنبًا إلى جنب مع زيادة دقة الاضطراب. أخيرًا وليس آخرًا ، نظهر أن النماذج بالكاد تستخدم المعلومات الموضعية لتحفيز الأشجار النحوية من الاهتمام الذاتي المتوسط والتمثيلات السياقية.', 'fr': "Des recherches récentes ont adopté un nouveau champ expérimental centré sur le concept de perturbations du texte qui a révélé que l'ordre des mots mélangés a peu ou pas d'impact sur les performances en aval des modèles de langage basés sur Transformer dans de nombreuses tâches de PNL. Ces résultats contredisent la compréhension commune de la façon dont les modèles encodent les informations hiérarchiques et structurelles et se demandent même si l'ordre des mots est modélisé avec des intégrations de position. À cette fin, cet article propose neuf ensembles de données de sondage organisés par type de perturbation de texte contrôlable pour trois langues indo-européennes avec un degré variable de flexibilité de l'ordre des mots\xa0: l'anglais, le suédois et le russe. Sur la base de l'analyse approfondie des modèles M-BERT et M-BART, nous rapportons que la sensibilité syntaxique dépend du langage et des objectifs de pré-entraînement du modèle. Nous constatons également que la sensibilité augmente à travers les couches avec l'augmentation de la granularité de la perturbation. Enfin et surtout, nous montrons que les modèles utilisent à peine les informations de position pour induire des arbres syntaxiques à partir de leur attention personnelle intermédiaire et de leurs représentations contextualisées.", 'ja': '最近の研究では、テキスト摂動の概念を中心とした新しい実験分野が採用されており、シャッフルされた単語の順序は、多くのNLPタスクにわたってトランスフォーマーベースの言語モデルの下流のパフォーマンスにほとんどまたはまったく影響しないことが明らかになっている。 これらの知見は、モデルが階層的および構造的情報をエンコードする方法の共通理解と矛盾し、単語の順序がポジション埋め込みでモデル化されているかどうかを疑問視することさえある。 この目的のために、この論文は、英語、スウェーデン語、ロシア語の3つのインド・ヨーロッパ語族言語の制御可能なテキスト摂動の種類によって編成された9つのプローブデータセットを提案する。 M - BERTモデルとM - BARTモデルのプローブ分析に基づいて、構文の感度は言語とモデルの事前トレーニング目標に依存することを報告します。 また、摂動粒度の増加とともに、感度が層を越えて成長することもわかっています。 最後に、私たちは、モデルが位置情報をほとんど使用せず、中間的な自己注目と文脈化された表現から構文木を誘導することを示しています。', 'zh': '近者研用一以文本扰动为心实验领域,当实验领域,随机词序于众NLP之任,基于Transformer言下之性几无损益。 此与对模型编码分共识相违,至质疑词序用位嵌建模。 故九探数集,可控文扰动,宜用三词序灵活性印欧语:英语、瑞典语、俄语。 M-BERT、M-BART之探也,吾告句法灵敏度决于言与形。 又见扰粒度之增,灵敏度长于各层之间。 最后非不重者,示几不用位信以诱之句法树自其中上下文化也。', 'hi': 'हाल के शोध ने एक नए प्रयोगात्मक क्षेत्र को अपनाया है जो पाठ की अवधारणा के आसपास केंद्रित है, जिसमें पता चला है कि फेरबदल किए गए शब्द क्रम का कई एनएलपी कार्यों में ट्रांसफॉर्मर-आधारित भाषा मॉडल के डाउनस्ट्रीम प्रदर्शन पर कोई प्रभाव नहीं पड़ता है। ये निष्कर्ष इस बात की आम समझ का खंडन करते हैं कि मॉडल पदानुक्रमित और संरचनात्मक जानकारी को कैसे एन्कोड करते हैं और यहां तक कि सवाल करते हैं कि क्या शब्द क्रम को स्थिति एम्बेडिंग के साथ मॉडलिंग किया गया है। इस अंत के लिए, यह पेपर तीन इंडो-यूरोपीय भाषाओं के लिए नियंत्रणीय पाठ के प्रकार द्वारा आयोजित नौ जांच डेटासेट का प्रस्ताव करता है, जिसमें शब्द आदेश लचीलेपन की अलग-अलग डिग्री होती है: अंग्रेजी, स्वीडिश और रूसी। M-BERT और M-BART मॉडल के जांच विश्लेषण के आधार पर, हम रिपोर्ट करते हैं कि वाक्यात्मक संवेदनशीलता भाषा और मॉडल पूर्व-प्रशिक्षण उद्देश्यों पर निर्भर करती है। हम यह भी पाते हैं कि संवेदनशीलता क्षोभ ग्रैन्युलैरिटी की वृद्धि के साथ परतों में बढ़ती है। अंतिम लेकिन कम से कम नहीं, हम दिखाते हैं कि मॉडल मुश्किल से अपने मध्यवर्ती आत्म-ध्यान और प्रासंगिक प्रतिनिधित्व से वाक्यात्मक पेड़ों को प्रेरित करने के लिए स्थितिगत जानकारी का उपयोग करते हैं।', 'ga': 'Ghlac taighde a rinneadh le déanaí le réimse turgnamhach nua atá dírithe ar choincheap na suaite téacs a thug le fios nach bhfuil mórán tionchair ag ord focal suaite ar fheidhmíocht iartheachtacha na múnlaí teanga atá bunaithe ar an gClaochladán thar go leor tascanna NLP. Tagann na torthaí seo salach ar an gcomhthuiscint ar an gcaoi a n-iondaíonn na samhlacha faisnéis ordlathach agus struchtúrach agus fiú ceistíonn siad an bhfuil ord na bhfocal samhaltaithe le leabú suímh. Chuige sin, molann an páipéar seo naoi dtacar sonraí iniúchta arna n-eagrú de réir an chineáil suaite téacs inrialaithe do thrí theanga Ind-Eorpacha a bhfuil leibhéal éagsúil solúbthachta d’ord focal acu: Béarla, Sualainnis agus Rúisis. Bunaithe ar anailís chisteanach ar mhúnlaí M-BERT agus M-BART, tuairiscímid go mbraitheann íogaireacht chomhréire ar chuspóirí réamhoiliúna na teanga agus na samhla. Faighimid freisin go bhfásann an íogaireacht trasna sraitheanna mar aon leis an méadú ar an granularity perturbation. Ar deireadh, ach ní ar a laghad, léirímid gur ar éigean a úsáideann na samhlacha an fhaisnéis suímh chun crainn chomhréire a aslú óna n-uiríll idirmheánacha féin-aird agus comhthéacsaithe.', 'ru': 'Недавние исследования приняли новое экспериментальное поле, сосредоточенное вокруг концепции текстовых возмущений, которое показало, что перемешанный порядок слов практически не влияет на производительность языковых моделей на основе Трансформера во многих задачах NLP. Эти выводы противоречат общему пониманию того, как модели кодируют иерархическую и структурную информацию, и даже ставят под вопрос, смоделирован ли порядок слов с вложениями положений. С этой целью в настоящем документе предлагаются девять наборов данных зондирования, организованных по типу контролируемого текстового возмущения для трех индоевропейских языков с различной степенью гибкости порядка слов: английского, шведского и русского. Основываясь на зондирующем анализе моделей M-BERT и M-BART, мы сообщаем, что синтаксическая чувствительность зависит от языка и целей предварительного обучения модели. Мы также находим, что чувствительность растет по слоям вместе с увеличением зернистости возмущения. И последнее, но не менее важное, мы показываем, что модели едва ли используют позиционную информацию, чтобы стимулировать синтаксические деревья от их промежуточного самовнимания и контекстуализированных представлений.', 'el': 'Πρόσφατες έρευνες έχουν υιοθετήσει ένα νέο πειραματικό πεδίο που επικεντρώνεται γύρω από την έννοια των διαταραχών κειμένου, το οποίο έχει αποκαλύψει ότι η ανακατεμένη τάξη λέξεων έχει ελάχιστη έως καθόλου επίδραση στην μεταγενέστερη απόδοση των γλωσσικών μοντέλων βασισμένων στον μετασχηματιστή σε πολλές εργασίες. Αυτά τα ευρήματα έρχονται σε αντίθεση με την κοινή κατανόηση του πώς τα μοντέλα κωδικοποιούν ιεραρχικές και δομικές πληροφορίες και ακόμη αμφισβητούν αν η σειρά λέξεων μοντελοποιείται με ενσωμάτωση θέσης. Για το σκοπό αυτό, η παρούσα εργασία προτείνει εννέα σύνολα δεδομένων που οργανώνονται με βάση τον τύπο ελεγχόμενης διαταραχής κειμένου για τρεις ινδοευρωπαϊκές γλώσσες με διαφορετικό βαθμό ευελιξίας λέξεων: αγγλικά, σουηδικά και ρωσικά. Με βάση την ανάλυση ανίχνευσης των μοντέλων Μ-BERT και M-BART, αναφέρουμε ότι η συντακτική ευαισθησία εξαρτάται από τους στόχους της γλώσσας και του μοντέλου προεκπαίδευσης. Επίσης διαπιστώνουμε ότι η ευαισθησία αυξάνεται σε στρώματα μαζί με την αύξηση της κοκκώδους διαταραχής. Τέλος, καταδεικνύουμε ότι τα μοντέλα ελάχιστα χρησιμοποιούν τις πληροφορίες θέσης για να προκαλέσουν συντακτικά δέντρα από την ενδιάμεση αυτοπροσοχή τους και τις περιεκτικοποιημένες αναπαραστάσεις τους.', 'ka': 'ახლა შემდეგ განსხვავება ახალი ექსპერიმენტიური პანელი, რომელიც ტექსტის პერტებურაციის კონცექტის კონცექტურაციის გარეშე, რომელიც აღმოჩნდა, რომ სტრუქტურული სიტყვების წესების შესა ამ მონაცემებების შესახებ საერთო გაგრძნობა, როგორ მოდელები იერაქტიკალური და სტრუქტურული ინფორმაციის კოდირება და კითხვა თუ სიტყვების წესები მოდელურია პო ამ კონტროლისთვის, ეს წიგნის აზრის ცხრა მოცემების მონაცემების კონტროლური ტექსტის პერრუბურაციის ტიპის გამოყენება სამი ინდო-ევროპოული ენებისთვის განსხვავებული სიტყვების წიგნი M-BERT და M-BART მოდელების პრობენტიკური ანალიზაციის ბაზაზე, ჩვენ გვეუბნება, რომ სინტაქტიკური სინტექტიკური სინტექტიკური სინტექტიკური მიხედვით ენაზე და მოდელზე წინ განაკეთ ჩვენ ასევე აღმოჩნეთ, რომ სიგრძნელობა წარმოდგენა სიგრძნელობით, რომელიც პერტუბურაციის დრანულაციის გაზრდება. ბოლოდან, მაგრამ არაფერი, ჩვენ ჩვენ აჩვენებთ, რომ მოდელები ვერ გამოიყენებენ პოზაციალური ინფორმაციას, რომ სინტაქტიური ეხეები მათი საშუალოდ თავიდან დააყენებენ და შე', 'it': "Recenti ricerche hanno adottato un nuovo campo sperimentale incentrato sul concetto di perturbazione del testo che ha rivelato che l'ordine delle parole mescolato ha poco o nessun impatto sulle prestazioni a valle dei modelli linguistici basati su Transformer in molti compiti NLP. Questi risultati contraddicono la comprensione comune di come i modelli codificano informazioni gerarchiche e strutturali e persino si interrogano se l'ordine di parola è modellato con incorporazioni di posizione. A tal fine, questo articolo propone nove set di dati sondanti organizzati per tipo di perturbazione testuale controllabile per tre lingue indoeuropee con un grado variabile di flessibilità nell'ordine delle parole: inglese, svedese e russo. Sulla base dell'analisi probing dei modelli M-BERT e M-BART, riportiamo che la sensibilità sintattica dipende dagli obiettivi di pre-formazione linguistica e modello. Troviamo anche che la sensibilità cresce attraverso gli strati insieme all'aumento della granularità di perturbazione. Ultimo ma non meno importante, mostriamo che i modelli utilizzano a malapena le informazioni posizionali per indurre alberi sintattici dalla loro auto-attenzione intermedia e rappresentazioni contestualizzate.", 'kk': 'Жуырдағы зерттеулері NLP тапсырмалардың көпшілікті түрлендіруші тіл үлгілерінің төмендету үлгілеріне көп нәтижесі жоқ деп көрсетілген мәтін аурулардың концепциясының ортасында жаңа тәжірибелі Бұл тапсырмалар үлгілер иерархикалық және структуралық мәліметтерді қалай кодтамасыз және сөздердің реті орнын ендіру үшін үлгілердің жалпы түсініктеріне қарсы болады. Бұл қағаз үш Индо- Еуропалық тілдер үш тілдерге әртүрлі сөз ретінің гибкостығы: ағылшын, Швед және Орус тілдерге баптаулық мәтін түрінде тәртүрлі тоғыз сынақ деректер жиынын ұсынады. M-BERT және M-BART үлгілерінің сынақ анализациясының негізінде, синтактикалық сезімділігі тіл мен үлгісінің алдын- оқыту мақсаттарына тәуелді деп есептеп береміз. Біз сондай-ақ сезімділігі қабаттардың қабаттарына көтеріп жатқан жағдайды біріктіреміз. Соңғы бірақ кемінде емес, үлгілер өзіңіздің өзіңіздің көмегімен контекстік түрлендірімдерінен синтактикалық ағаштарды синтактикалық ағаштардың мәліметін қолдануға болмайды.', 'mk': 'Неодамнешното истражување усвои ново експериментално поле кое се центрира околу концептот на текстови пертурбирања, кој откри дека зборниот ред на зборови има мало или никакво влијание врз понатамошната изведба на јазичките модели базирани на Трансформер во многу NLP задачи. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings.  За оваа цел, овој весник предложува девет набори на податоци организирани по типот на контролирана текстова пертурбија за три индуевропски јазици со различна степена флексибилност на редот на зборови: англиски, шведски и руски. На основа на анализата на проверката на моделите M-BERT и M-BART, известуваме дека синтактичката чувствителност зависи од јазикот и моделот на предобуката. Исто така, откриваме дека чувствителноста расте преку слоеви заедно со зголемувањето на грануларноста на пертурбирањето. Последно, но не најмалку, покажуваме дека моделите едвај ги користат позиционалните информации за индукција на синтактичките дрвја од нивното меѓувремено себеси внимание и контекстуални претставувања.', 'hu': 'A közelmúltbeli kutatások egy új kísérleti területet alkalmaztak, amely a szövegzavarok fogalmára összpontosít, és kiderült, hogy a kevert szósorrend alig vagy semmilyen hatással van a transzformátor alapú nyelvi modellek downstream teljesítményére számos NLP feladatban. Ezek a megállapítások ellentmondanak annak közös megértésével, hogy a modellek hogyan kódolják hierarchikus és strukturális információkat, sőt megkérdőjelezik, hogy a szósorrendet pozícióbeágyazásokkal modellezik-e. Ebből a célból a tanulmányból kilenc, ellenőrizhető szövegzavarok típusa szerint szervezett adatkészletet javasol három indoeurópai nyelvre, különböző fokú szósorrendű rugalmassággal: angol, svéd és orosz. Az M-BERT és M-BART modellek vizsgálati elemzése alapján jelentjük, hogy a szintaktikai érzékenység a nyelvi és modell előkészítési célkitűzéseitől függ. Azt is megállapítjuk, hogy az érzékenység a rétegeken át nő a perturbációs granuláció növekedésével együtt. Végül, de nem utolsósorban, megmutatjuk, hogy a modellek alig használják a pozíciós információkat arra, hogy szintaktikus fákat indukáljanak közbenső önfigyelmükből és kontextusált reprezentációikból.', 'ms': 'Penelitian baru-baru ini telah mengadopsi medan eksperimen baru yang ditengankan sekitar konsep gangguan teks yang telah mengungkapkan bahawa tertib perkataan terganggu mempunyai sedikit atau tiada kesan pada prestasi turun dari model bahasa berasaskan Transformer melalui banyak tugas NLP. Kesemuan ini bertentangan dengan pemahaman umum bagaimana model mengekod maklumat hierarkis dan struktur dan bahkan bertanya-tanya sama ada urutan perkataan dipodelkan dengan penerbangan kedudukan. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility: English, Swedish and Russian.  Berdasarkan analisis penyelidikan model M-BERT dan M-BART, kami laporkan bahawa sensitiviti sintaktik bergantung pada tujuan pralatihan bahasa dan model. Kami juga mendapati bahawa sensitiviti tumbuh melalui lapisan bersama dengan meningkat granulariti perturbasi. Akhirnya, tetapi tidak sekurang-kurangnya, kami menunjukkan bahawa model hampir tidak menggunakan maklumat kedudukan untuk mengakibatkan pokok sintaktik dari perhatian diri sementara mereka dan perwakilan kontekstualisasi.', 'mt': 'Riċerka reċenti adottat qasam sperimentali ġdid iċċentrat fuq il-kunċett ta’ perturbazzjonijiet tat-test li żvela li l-ordni ta’ kliem imħallat ftit jew xejn għandu impatt fuq il-prestazzjoni downstream ta’ mudelli lingwistiċi bbażati fuq it-Transformer f’ħafna kompiti tal-NLP. Dawn is-sejbiet jikkontradixxu l-fehim komuni ta’ kif il-mudelli jikkodifikaw l-informazzjoni ġerarkika u strutturali u saħansitra jistaqsu jekk l-ordni tal-kelma hijiex immudellata b’inkorporazzjonijiet tal-pożizzjoni. Għal dan il-għan, dan id-dokument jipproponi disa’ settijiet ta’ dejta ta’ sondaġġ organizzati mit-tip ta’ perturbazzjoni tat-test kontrollabbli għal tliet lingwi Indo-Ewropej b’grad differenti ta’ flessibbiltà fl-ordni tal-kliem: l-Ingliż, l-Isvediż u r-Russu. Abbażi tal-analiżi tas-sondaġġ tal-mudelli M-BERT u M-BART, nirrappurtaw li s-sensittività sinrattika tiddependi fuq l-għanijiet tal-lingwa u l-mudell ta’ qabel it-taħriġ. Issibu wkoll li s-sensittività tikber bejn is-saffi flimkien maż-żieda fil-granularità tal-perturbazzjoni. Fl-aħħar iżda mhux l-inqas, nagħmlu evidenza li l-mudelli bilkemm jużaw l-informazzjoni pożizzjonali biex jinduċu siġar sintattiku mill-attenzjoni intermedja tagħhom stess u rappreżentazzjonijiet kuntestwalizzati.', 'no': 'Nyleg har forskning godtatt eit nytt eksperimentelt felt sentrert rundt konsepten av tekstperturbasjonar som har opna at søkte ordrekkefølgje har lite til å ha noko påvirkning på nedtrekkstillinga av transformeringsspråk-modeller i mange NLP-oppgåver. Desse finningane gjer motsatt den vanlege forståelse av korleis modelane koder hierarkiske og strukturelle informasjon, og sjølv spørsmål om ordordrekkefølgja er modelert med innbygging av posisjon. I denne slutten foreslår denne papiren 9 proberingsdatasett organisert av typen kontrollerbare tekstområde for tre Indoeuropeiske språk med ulike grad ordrekkefølgje fleksibilitet: engelsk, svensk og russisk. Basert på proberingsanalysen av M-BERT-og M-BART-modellen, rapporterer vi at syntaksiske følsomhet avhengig av språket og modellen før opplæring. Vi finn også at følsomheten vokser over lag saman med økningen av perturbasjonsgrenulariteten. Sist, men ikkje minst, viser vi at modelane bare brukar posisjonelle informasjon for å indusera syntaktiske trær frå sine gjennomsnittsverdi og kontekstualiserte representasjonar.', 'pl': 'Ostatnie badania przyjęły nową eksperymentalną dziedzinę skupioną wokół koncepcji zakłóceń tekstowych, która ujawniła, że zmieszany porządek słów ma niewielki lub żaden wpływ na wydajność modeli językowych opartych na Transformerze w wielu zadaniach NLP. Ustalenia te są sprzeczne z powszechnym zrozumieniem, w jaki sposób modele kodują informacje hierarchiczne i strukturalne, a nawet kwestionują, czy porządek słów jest modelowany za pomocą osadzeń pozycji. W tym celu w artykule zaproponowano dziewięć badawczych zbiorów danych zorganizowanych według typu kontrolowanej zakłócenia tekstu dla trzech języków indoeuropejskich o różnym stopniu elastyczności porządku słów: angielskiego, szwedzkiego i rosyjskiego. W oparciu o analizę sondową modeli M-BERT i M-BART raportujemy, że wrażliwość składni zależy od celów językowych i modelowych przedtreningowych. Odkrywamy również, że wrażliwość rośnie w warstwach wraz ze wzrostem granularności zakłóceń. Na koniec pokazujemy, że modele ledwo wykorzystują informacje pozycjonalne do wywoływania drzew składniowych ze swojej pośredniej uwagi na siebie i kontekstualizowanych reprezentacji.', 'lt': 'Neseniai atlikti moksliniai tyrimai priėmė naują eksperimentinę sritį, centruotą teksto perturbacijų koncepcijoje, kuri parodė, kad supainiota žodžių tvarka nedaro jokio poveikio tolesniam Transformuojančių kalbų modelių veikimui daugelyje NLP užduočių. Šios išvados prieštarauja bendram supratimui, kaip modeliai koduoja hierarchinę ir struktūrinę informaciją, ir net abejoja, ar žodžių tvarka modeliuojama su pozicijos įterpimais. Šiuo tikslu šiame dokumente siūlomi devyni tyrimo duomenų rinkiniai, organizuojami pagal kontroliuojamą teksto perturbaciją trijose indoeuropinėse kalbose su skirtingu žodžių tvarkos lankstumo laipsniu: anglų, švedų ir rusų. Remiantis M-BERT ir M-BART modelių analize, pranešame, kad sintaksinis jautrumas priklauso nuo kalbos ir modelio parengiamojo mokymo tikslų. Taip pat pastebime, kad jautrumas didėja įvairiuose sluoksniuose kartu su perturbacijos granuliarumo padidėjimu. Galiausiai, bet ne mažiau svarbu, rodome, kad modeliai vos naudoja padėties informaciją sintaksiniams medžiams paskatinti iš tarpinio jų savarankiškumo ir kontekstinės reprezentacijos.', 'ro': 'Cercetările recente au adoptat un nou domeniu experimental centrat în jurul conceptului de perturbare a textului, ceea ce a arătat că ordinea amestecată a cuvintelor are puțin sau deloc impact asupra performanței în aval a modelelor de limbaj bazate pe Transformer în multe sarcini NLP. Aceste constatări contrazic înțelegerea comună a modului în care modelele codează informațiile ierarhice și structurale și chiar pun întrebări dacă ordinea cuvântului este modelată cu încorporarea poziției. În acest scop, lucrarea propune nouă seturi de date de sondare organizate în funcție de tipul de perturbare a textului controlabilă pentru trei limbi indoeuropene cu un grad diferit de flexibilitate a ordinii de cuvinte: engleză, suedeză și rusă. Pe baza analizei de sondare a modelelor M-BERT și M-BART, raportăm că sensibilitatea sintactică depinde de limbajul și modelul obiectivelor pre-training. De asemenea, constatăm că sensibilitatea crește peste straturi împreună cu creșterea granularității perturbației. Nu în ultimul rând, arătăm că modelele abia folosesc informația pozițională pentru a induce copacii sintactici din auto-atenția lor intermediară și reprezentările contextualizate.', 'sr': 'Nedavno istraživanje je usvojilo novo eksperimentalno polje usredotočeno oko koncepta tekstskih perturbacija, koji je otkrio da zabranjena riječi ne može imati utjecaja na sledeće učinke jezičkih modela na transformeri u mnogim NLP zadatkima. Ovi nalazi se suprotstavljaju zajedničkom razumijevanju o tome kako modeli kodiraju hijerarhičke i strukturne informacije i čak pitaju da li je red reči modeliran sa uvođenjem pozicija. Za taj cilj, ovaj papir predlaže devet konzervativnih podataka organizovanih po tipu kontroliranog teksta perturbacije za tri Indoevropske jezika sa različitim stepenicama fleksibilnosti rečenja: engleski, švedski i ruski. Na osnovu istraživanja analize modela M-BERT i M-BART prijavljujemo da sintaktička osetljivost zavisi od ciljeva jezika i modela predobuke. Takođe smatramo da osjetljivost raste preko slojeva zajedno sa povećanjem perturbacijske granularnosti. Posljednji, ali ne najmanje, pokazujemo da modeli jedva koriste pozicionalne informacije kako bi indukovali sintaktične drveće iz njihovih prosječnih samopouzdanja i kontekstualiziranih predstavljanja.', 'ml': 'അടുത്തുള്ള പരീക്ഷണത്തിന്റെ പുതിയ പരീക്ഷണത്തിന്റെ ഭാഗത്തിന്റെ ആശയത്തിന്റെ ചുറ്റുമുള്ള ഒരു പുതിയ പരീക്ഷണത്തിന്റെ ഫീള്\u200dഡ് നടത്തിയിരിക്കുന്ന ഈ കണ്ടെത്തുന്നത് മോഡലുകള്\u200d എങ്ങനെയാണ് ഹിയറാര്\u200dക്കിക്കല്\u200d വിവരങ്ങളും സ്ഥാനത്തിലുള്ള വിവരങ്ങളും എക്സോഡ് ചെയ്യുന്നതെന്നും വിരോ ഈ പേപ്പറിന് മൂന്നു ഇൻഡോ-യൂറോപ്പിലെ നിയന്ത്രിക്കപ്പെട്ട ട ടെക്സ്റ്റ് പരിശോധിപ്പിക്കുന്നതിനായി നിയന്ത്രിക്കുന്ന വാക്കുകളുടെ വ് M-BERT-ന്റെയും M-BART മോഡലുകളുടെയും പരിശോധനത്തിന്റെയും അടിസ്ഥാനത്തില്\u200d ഞങ്ങള്\u200d റിപ്പോര്\u200dട്ട് ചെയ്യുന്നു, ഈ സിന്\u200dടാക്ടിക് സെന്\u200dസിറ്റിക സ്വീകരിക്കുന്നത് തടാകങ്ങളിലൂടെ മുഴുവന്\u200d വളരുന്നതാണെന്നും നമുക്ക് കണ്ടെത്തുന്നു. അവസാനത്തെ പക്ഷെ കുറഞ്ഞതല്ല, നമ്മള്\u200d കാണിച്ചുകൊടുക്കുന്നത്, മോഡലുകള്\u200d സാധാരണമായ വിവരങ്ങള്\u200d ഉപയോഗിക്കുന്നില്ല, അവയുടെ നടുവി', 'so': "Baaritaankii ugu dambeeyey waxay qaadatay beer cusub oo ku xariiran fikrada xarunta waxyaabaha la xiriira qoraalka, taasoo muujiyey in amar la burburay uu saameyn ku yeelan maamulka afka hoose-hoose ee sameynta qaababka afka ee ku qoran ee badan oo qabashada NLP. Kaalmooyinkaas waxay ka duwan yihiin garashada caadiga ah, sida tusaaladu u kooban yihiin macluumaadka hierarkiisa iyo dhismaha, iyo xataa su'aalaha in lagu sameynayo hagitaanka ereyga. Taas darteed warqaddaas wuxuu soo jeedaa sagaal kooban oo lagu tijaabiyey qoraal maamul ah oo loo qabanqaabinayo saddex luuqadood oo Indo-Yurub ah oo ku qoran shahaado kala duduwan oo ereyga kala duduwan: Ingiriis, Iswidish iyo Ruush. Baaritaanka baaritaanka modellada M-BERT iyo M-BART, waxaynu wargelinaynaa in qalabka la soconayo ay ku xiran tahay waxyaabaha la barbaarinta horumarinta. Sidoo kale waxaynu helnaa in xilliga dareemada la koro daruurka burburka. Ugu dambaysta laakiin ugu yaraan waxaynu tusnaynaa in tusaalooyinku si aan ugu yaraan u isticmaalo macluumaadka suurtagalka ah si ay geedaha israacista ah uga soo bandhigaan iskuul-hoosaysiinta iyo xuquuqda kala duwan.", 'mn': 'Шинэ хугацаанд судалгаагаар шинэ туршилтын талбар нь NLP даалгаврын олон ажил дээр Трансфер суурилсан хэл загварын давхар үйл ажиллагааны төвшин нөлөөлөгч байхгүй гэдгийг харуулсан текст хэлбэрүүдийн ойлголтын төвшин төвшин төв Эдгээр ололтууд нь ерөнхий ойлголтын тухай эсрэг байдаг. Загварууд яаж диерархик болон бүтэц мэдээллийг шинэчлэх талаар, мөн үгний дарааллаар байрлалтай загварчлагдсан ч гэсэн асуулт асуудаг. Энэ төгсгөлд энэ цаас 3 Индо-Европын хэлний хувьд өөр хэлбэртэй хэлбэртэй 9 судалгааны өгөгдлийн сангуудыг зохион байгуулдаг. M-BERT болон M-BART загварын судалгааны шалгалтын тулд бид синтактикийн мэдрэмж нь хэл болон загварын өмнөх сургалтын зорилго дээр хамааралтай гэдгийг мэддэг. Мөн бид мэдрэмжлийн мэдрэмж давхар давхар давхар дээр өсөж байгааг харж байна. Хамгийн сүүлийн үед бид загварууд өөрсдийгөө анхаарлаа төвлөрүүлэх, орчин үеийн төвлөрүүлэх загваруудаас синтактик моднуудыг хэрэглэдэггүй гэдгийг харуулж байна.', 'ur': 'اچھی تحقیقات نے ایک نوی تجربہ فیلڈ کو مضبوط کرلیا ہے جو ٹیکسٹ پرٹربیٹ کی نظریہ کے اطراف میں مرکز کی ہے جس نے ظاہر کی ہے کہ شکلڈ لفظ کی اوردر بہت سی NLP کاموں میں ترنسفور کی زبان مدلڈ کے نیچے عملکرد پر کوئی اثر نہیں یہ معلومات ان کے معلومات کی مخالفت کرتی ہیں کہ موڈل کس طرح حیراتیکل اور ساخترکیل معلومات کا کوڈ کرتی ہیں اور سوال بھی کرتی ہیں کہ اگر کلمات کے دستور کے ذریعہ موڈل کئے جاتے ہیں۔ یہاں تک، یہ کاغذ نو پروپینٹ ڈیٹ سٹ کی پیشنهاد کرتا ہے جو تین انڈو یورپی زبانوں کے لئے کنٹرول قابل تغییرات کے ذریعے تین انڈو یورپی زبانوں کے لئے مختلف درجے کے ساتھ تغییر تغییر کرتی ہے: انگلی M-BERT اور M-BART موڈلوں کی تحقیقات کی تحقیقات پر، ہم راپورٹ کرتے ہیں کہ سینٹکتیک حساسیتی زبان اور موڈل پیش تربینی موجودات پر مضبوط ہے. ہم نے بھی دیکھا ہے کہ حساسیت لہروں میں اضافہ ہوتی ہے اور اس کی زیادتی کے ساتھ اضافہ ہوتی ہے۔ آخر لیکن کم نہیں، ہم دکھاتے ہیں کہ موڈلے محض موقعیت معلومات کے استعمال کرتے ہیں کہ ان کے درختوں کو ان کے میانہ سے سینٹیکیسی دکھانے اور متوسط دکھانے کے لئے استعمال کرتے ہیں.', 'si': 'අලුත් පරීක්ෂණයට අලුත් පරීක්ෂණ ක්\u200dෂේත්රයක් ගත්තා අලුත් පරීක්ෂණ ක්\u200dෂේත්රයක් පරීක්ෂණයේ පරීක්ෂණයේ පරීක්ෂණයේ පරීක්ෂණය සඳහා ප මේ හොයාගන්න සාමාන්\u200dය තේරුම් ගැන ප්\u200dරතික්\u200dරියාවට ප්\u200dරතික්\u200dරියා කරන්න පුළුවන් වෙන්නේ නිර්මාණය සහ නිර්මාණික තොරත මේ අවසානයට, මේ පැත්තේ ප්\u200dරශ්නය දත්ත සටහන් නවක් ප්\u200dරශ්නය කරනවා ඉන්දෝරෝපිය භාෂාව තුනක් වෙනස් විදියට වචන පණිවිධානයක් ති M-BERT සහ M-BART මොඩේල් එකේ පරීක්ෂණ විශ්ලේෂණය අධික, අපි වාර්තා කරනවා විදිහට සංකේෂණ සංවේදනය භාෂාව සහ මොඩේල් ප්\u200dරීක අපිට හොයාගන්න පුළුවන් කියලා සංවේදනයක් ස්තූතියෙන් වැඩ කරනවා කියලා. අන්තිමට නමුත් අඩුම, අපි පෙන්වන්නේ මොඩේල් එක්ක ස්ථානය තොරතුරු පාවිච්චි කරන්න බැරි විදිහට එයාලගේ අධ්\u200dයානය සහ සාමා', 'sv': 'Ny forskning har antagit ett nytt experimentellt fält centrerat kring begreppet textstörningar som har visat att blandad ordordning har liten eller ingen inverkan på prestanda nedströms av Transformer-baserade språkmodeller över många NLP-uppgifter. Dessa fynd motsäger den gemensamma förståelsen av hur modellerna kodar hierarkisk och strukturell information och ifrågasätter till och med om ordordningen modelleras med positionsinbäddningar. För detta ändamål föreslår denna uppsats nio sonderande datauppsättningar organiserade efter typ av kontrollerbar textstörning för tre indoeuropeiska språk med varierande grad av ordordningsflexibilitet: engelska, svenska och ryska. Baserat på sondanalysen av M-BERT- och M-BART-modellerna rapporterar vi att syntaktisk känslighet beror på språk och modell pre-training mål. Vi finner också att känsligheten växer över lager tillsammans med ökningen av perturbationsgranulariteten. Sist men inte minst visar vi att modellerna knappt använder positionsinformationen för att inducera syntaktiska träd från deras mellanliggande självuppmärksamhet och kontextualiserade representationer.', 'ta': 'அண்மையில் உள்ள ஒரு புதிய சோதனைப்புலத்தை மையமாக எடுத்துக் கொண்டுள்ளது அது வெளிப்படுத்தப்பட்ட சொல்லின் வரிசையில் மாற்று மொழி மாதிரிகளின் செயல்பா இந்த கண்டுபிடிப்புகள் பொதுவான புரியும் மாதிரிகள் எவ்வாறு குறியீட்டு அசைவூட்டு மற்றும் அமைப்பு தகவல்களை மற்றும் கேள்வி க இந்த காகிதத்திற்கு, மூன்று சிந்தி- ஐரோப்பிய மொழிகளுக்கு கட்டுப்படுத்தப்பட்ட கட்டுப்பாட்டு உரை மாற்றுதல் வகையால் ஒன்பது சோதனை தகவல் அமைப்புகள M-BERT மற்றும் M-BART மாதிரிகளின் பரிசோதனையை அடிப்படையில், ஒத்திசைவு உணர்வு மொழி மற்றும் மாதிரி பயிற்சி முன்பு பயிற்சி செயல்பாடுகளை ச மேலும் நாம் கண்டுபிடிக்கிறோம் அந்த உணர்வு அடுக்குகள் முழுவதும் அதிகரிக்கும் பொருட்களின் விளைவுகளுடன். கடைசி ஆனால் குறைந்தது இல்லை, நாம் மாதிரிகளை குறைவாக பயன்படுத்தும் நேர்ம தகவலை குறைவாக க காண்பிக்கிறோம். நடுநிலை தன்னை கவனம் மற்ற', 'uz': "Yaqinda o'qituvchilar matn tartibiqlarida yangi taʼminlov soni boshladi. Bu bir necha vazifalar boshqa vazifalarga borilgan so'zlar tartib chiqarishni o'zgartirib chiqishni anglatadi. Ushbu natijalar hierarchik va structural maʼlumotini qanday kodlash usulining umumiy tushunishiga murojaat qiladi va xato so'zlar tartibi joylashtirilganligini modellash mumkin. Bu qadam esa, 3 Indo-European tillari uchun boshqaruvchi matn turli bo'lgan 9 taʼminlovchi maʼlumotlar satrlarini boshqarish imkoniyatini beradi. Ingliz, iswidhish va Ruscha darajasi bilan bir xil soʻzlar tartib keladi. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives.  Biz shunday o'ylaymiz, ma'lumotlik qo'llangan qo'llanmalar bilan birga keladi. Oxirgi lekin eng oxirgi emas, biz modellar o'ylab, ularning o'rtasida o'zimning o'zimidan o'zining o'zgarishga va taʼminlovchi tashkilotlaridan foydalanish uchun ma'lumotdan foydalanadi.", 'vi': 'Nghiên cứu gần đây đã phát triển một lĩnh vực thử nghiệm mới tập trung vào khái niệm về phiền hà văn bản, cho thấy rằng trật tự đa dạng từ xáo trộn không có tác động gì đến hiệu ứng phụ của các mô hình ngôn ngữ biến hình trong nhiều công việc của NMB. Những kết quả này trái với hiểu biết chung về cách các mô- đun mã hóa thông tin cấp dưới và cấu trúc và thậm chí đặt câu hỏi nếu các từ trật tự được mô hình hóa bằng sự nhúng vào vị trí. Mục tiêu này đề nghị chín tập tin được tổ chức dựa trên các loại sửa chữa văn bản kiểm soát cho ba ngôn ngữ Đông Âu có độ linh hoạt khác nhau về trật tự chữ: Anh, Thụy Điển và Nga. Dựa trên các phân tích thăm dò của mẫu M-BERT và M-BART, chúng tôi báo cáo rằng độ nhạy cảm cú pháp phụ thuộc vào mục tiêu chuẩn bị sẵn ngôn ngữ và mô hình. Chúng ta cũng thấy sự nhạy cảm qua nhiều lớp cùng với việc tăng cường độ hạt gây rối loạn. Cuối cùng, nhưng không ít quan trọng, chúng tôi cho thấy các mô hình hầu như không sử dụng các thông tin vị trí để tạo ra các cây nối từ sự chú ý tạm thời và các biểu hiện hiện hiện định.', 'da': 'Nylig forskning har vedtaget et nyt eksperimentelt felt centreret omkring begrebet tekstforstyrrelser, som har afsløret, at blandet ordrækkefølge har lidt eller ingen indflydelse på efterstrømsydelsen af Transformer-baserede sprogmodeller på tværs af mange NLP-opgaver. Disse resultater modsiger den fælles forståelse af, hvordan modellerne koder hierarkisk og strukturel information og stiller endda spørgsmålstegn ved, om ordrækkefølgen er modelleret med positionsindlejringer. Til dette formål foreslår denne artikel ni sonderende datasæt organiseret efter typen af kontrollerbar tekstforstyrrelse for tre indoeuropæiske sprog med en varierende grad af ordrækkefølgefleksibilitet: engelsk, svensk og russisk. Baseret på sondeanalysen af M-BERT- og M-BART-modellerne rapporterer vi, at den syntaktiske følsomhed afhænger af sprog og model før træning målsætninger. Vi finder også, at følsomheden vokser på tværs af lag sammen med stigningen i forstyrrelsesgranulariteten. Sidst men ikke mindst viser vi, at modellerne knap nok bruger positionsinformationen til at inducere syntaktiske træer fra deres mellemliggende selvopmærksomhed og kontekstualiserede repræsentationer.', 'bg': 'Последните изследвания възприемат нова експериментална област, съсредоточена около концепцията за смущения на текста, която разкрива, че смесеният ред на думи има малко или никакво въздействие върху изпълнението надолу по веригата на базираните на трансформатори езикови модели при много задачи на НЛП. Тези констатации противоречат на общото разбиране за това как моделите кодират йерархична и структурна информация и дори поставят под въпрос дали реда на думата е моделиран с вграждане на позиции. За тази цел в настоящата статия се предлагат девет сондиращи набора от данни, организирани по типа контролирани текстови смущения за три индоевропейски езика с различна степен на гъвкавост на реда на думите: английски, шведски и руски. Въз основа на сондажния анализ на моделите докладваме, че синтактичната чувствителност зависи от целите на езика и модела преди обучението. Също така откриваме, че чувствителността нараства през слоевете заедно с увеличаването на гранулирането на смущенията. Не на последно място, показваме, че моделите едва използват позиционната информация, за да индуцират синтактични дървета от тяхното междинно самовнимание и контекстуализирани представи.', 'hr': 'Nedavno istraživanje usvojilo je novo eksperimentalno polje usredotočeno oko koncepta tekstskih perturbacija koja je otkrila da je zabranjena riječi riječ manje utjecala na niz učinkovitost jezičkih modela na transformeri u mnogim NLP zadatkima. Ovi nalazi se suprotstavljaju zajedničkom razumijevanju kako modeli kodiraju hijerarhičke i strukturne informacije i čak pitaju je li se redak riječi modelirao sa stavljanjem položaja. Za taj cilj, ovaj papir predlaže devet provjeravajućih podataka organiziranih po vrsti kontroliranog teksta za tri Indoeuropske jezika s različitim stupnjem fleksibilnosti riječi: engleskog, švedskog i ruskog reda. Na temelju ispitivanja analize modela M-BERT i M-BART, izvještavamo da sintaktička osjetljivost zavisi od ciljeva jezika i modela predobuke. Također smatramo da osjetljivost raste preko slojeva zajedno s povećanjem perturbacijske granularnosti. Posljednje, ali ne najmanje, pokazujemo da modeli jedva koriste pozicionalne informacije kako bi izazvali sintaktične drveće iz njihovih prosječnih samopouzdanja i kontekstualiziranih predstavljanja.', 'ko': '최근의 연구는 텍스트 교란 개념을 중심으로 한 새로운 실험 분야를 채택했다. 이 분야는 무질서한 어순이 많은 NLP 작업에서 변환기를 기반으로 하는 언어 모델의 하류 성능에 거의 영향을 미치지 않는다는 것을 보여주었다.이러한 발견은 사람들이 모델이 어떻게 차원과 구조 정보를 인코딩하는지에 대한 보편적인 이해와 모순되고 심지어는 어순이 위치 삽입을 사용하여 모델링을 하는지 의문을 제기하기도 한다.이를 위해 본고는 영어, 스웨덴어, 러시아어 등 세 가지 서로 다른 어순의 유연성을 가진 인구어계 언어에 대해 제어 가능한 텍스트 교란 유형에 따라 9개의 탐지 데이터 집합을 제시했다.M-BERT와 M-BART 모델에 대한 탐색적 분석을 바탕으로 우리는 문법의 민감성은 언어와 모델의 예비 훈련 목표에 달려 있음을 발견했다.우리는 또 미요입도가 증가함에 따라 민감도가 층층이 증가하는 것을 발견했다.마지막으로 가장 중요하지 않은 것은 이러한 모델은 위치 정보를 거의 사용하지 않고 중간 자기주의와 어경화 표현에서 문법 트리를 유도한다는 것이다.', 'id': 'Penelitian baru-baru ini telah mengadopsi bidang eksperimen baru yang ditengah sekitar konsep perturbasi teks yang telah mengungkapkan bahwa perintah kata terganggu tidak memiliki sedikit atau tidak ada dampak pada prestasi turun dari model bahasa berdasarkan Transformer melalui banyak tugas NLP. Penemuan ini bertentangan dengan pemahaman umum bagaimana model mengkode informasi hierarkis dan struktural dan bahkan bertanya apakah perintah kata dipodelkan dengan posisi embedding. Untuk tujuan ini, kertas ini mengusulkan sembilan set data penyelidikan yang terorganisir oleh jenis perturbasi teks yang dapat dikendalikan untuk tiga bahasa Indo-Eropa dengan tingkat yang berbeda fleksibilitas perintah kata: Inggris, Swedia dan Rusia. Berdasarkan analisis probing model M-BERT dan M-BART, kami melaporkan bahwa sensitivitas sintaks tergantung pada tujuan bahasa dan model pre-pelatihan. Kami juga menemukan bahwa sensitivitas tumbuh di sepanjang lapisan bersama dengan meningkat granularitas perturbasi. Akhirnya, tapi tidak setidaknya, kami menunjukkan bahwa model hampir tidak menggunakan informasi posisional untuk mendorong pohon sintaksi dari perhatian-diri intermedium mereka dan representati kontekstualisasi.', 'nl': 'Recent onderzoek heeft een nieuw experimenteel veld aangenomen rond het concept van tekstverstoringen, dat heeft aangetoond dat geschudde woordorde weinig tot geen invloed heeft op de downstream prestaties van Transformer-gebaseerde taalmodellen in vele NLP-taken. Deze bevindingen zijn in tegenspraak met het algemeen begrip van hoe de modellen hiërarchische en structurele informatie coderen en vragen zich zelfs af of de woordorde gemodelleerd wordt met positie embeddings. Om dit doel te bereiken stelt dit document negen soning datasets voor die georganiseerd zijn op het type controleerbare tekstverstoring voor drie Indo-Europese talen met een variërende mate van flexibiliteit in woordvolgorde: Engels, Zweeds en Russisch. Op basis van de soning analyse van de M-BERT en M-BART modellen, rapporteren we dat de syntactische gevoeligheid afhangt van de taal en model pre-training doelstellingen. We merken ook dat de gevoeligheid groeit over lagen samen met de toename van de verstoringsgranulariteit. Last but not least laten we zien dat de modellen nauwelijks gebruik maken van positieinformatie om syntactische bomen te induceren uit hun intermediaire zelfaandacht en contextualiseerde representaties.', 'fa': 'تحقیقات اخیراً یک منطقه آزمایشی جدید را که در اطراف مفهوم تغییرات متن مرکز شده است، پذیرفت کرده است که سفارش کلمه\u200cهای تغییرات کمی بر عملکرد مدل\u200cهای زبان\u200cهای تغییر\u200cدهنده در بسیاری از کارهای NLP تاثیر نداشته است. این نتیجه\u200cها مخالف درک مشترک درباره\u200cی چگونه مدل\u200cها اطلاعات معماری و ساختاری\u200cها و حتی سؤال می\u200cکنند که آیا سفارش کلمه با استفاده\u200cهای موقعیت مدل می\u200cشود. برای این قضیه، این کاغذ نو مجموعه داده\u200cهای تحقیق را پیشنهاد می\u200cدهد که توسط نوع تغییرات متن قابل کنترل برای سه زبان\u200cهای Indo-European با درجه متفاوتی از دستور کلمات: انگلیسی، سوئدی و روسیه است. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. همچنین پیدا می\u200cکنیم که حساسیت در طبقه\u200cهای طبقه\u200cای با افزایش گلوله\u200cهای تغییرات رشد می\u200cکند. آخرین ولی نه حداقل، ما نشان می دهیم که مدل ها به سختی از اطلاعات موقعیت استفاده می کنند تا درختان سنتاکتیک را از توجه خودشان و نمایش\u200cهای موقعیت\u200cهای موقعیت ایجاد کند.', 'tr': 'Ýakyndaky araştyrmalar, NLP görevlerinde terjime eden dil nusgalarynyň indiki täze bir synanym sahypasyny kabul etdi. Bu yzarlar, modelleriň iýerarhiýa we strukturel maglumaty nädip kodlaýandygyny we hatda soraglaryň düzümleri bilen düzülmegini boýunça garşyrýarlar. Bu ýagdaýa, bu kagyz üçin üç Indo-Ýewropa dilleri üçin beýleki derejede fleksibilitäk bilen düzenlenen üçin dokuz sanat sanat setirini teklip edip otyrýar. M-BERT we M-BART nusgalarynyň synaglamasynyň çözümlenmesine daýanýar, syntaktik hasyýatlygyň dil we nusgalaryň öňünden öňünden eğim maksadyna baglanýandygyny bildirip ýöredýäris. Munuň ýene-de, duýdurlyklyk gatlaklaryň içine üýtgeýändigini düşünýäris. Soňky ýöne iň azyndan hem, nusgalaryň öz ünsünden we böleginden syntaktik agaçlaryň üýtgetmesi üçin pozisiýa maglumatyny ulanmaýandygyny görkezýäris.', 'sw': 'Utafiti wa hivi karibuni umechukua uwanja mpya wa majaribio uliokuwa unazunguka dhana ya mabadiliko ya maandishi ambayo imeonyesha kuwa amri iliyochukuliwa na maneno hayana madhara ya utendaji wa mifano ya lugha za zamani katika kazi nyingi za NLP. Matokeo haya yanapingana na uelewa wa kawaida wa namna mifano inavyoweza kuingia taarifa za ubunifu na miundombinu na hata kuuliza kama amri ya neno linatengenezwa kwa nafasi. Kwa mwisho huu, gazeti hili linapendekeza seti tisa za taarifa zilizoandaliwa na aina ya maandishi ya kudhibitiwa kwa lugha tatu za Kihindi-Ulaya kwa kiwango tofauti cha utaratibu wa maneno: Kiingereza, Kiswahili na Urusi. Kutokana na uchambuzi wa mifano ya M-BERT na M-BART, tunaripoti kuwa uelewa wa ufundi unategemea lengo la lugha na mifano ya mafunzo ya kabla. Pia tunagundua kuwa hisia zinaongezeka katika maeneo mengi pamoja na kuongezeka kwa ukatili wa uvunjifu. Mwisho lakini sio angalau, tunaonyesha kwamba mifano si vigumu hutumia taarifa chanya ili kutengeneza miti ya pamoja kutoka kwa uwakilishi wa kati na wenye utangazaji.', 'af': "Onlangse ondersoek het 'n nuwe eksperimenteel veld aangeneem wat omtrent die konsepte van teks perturbasies wat oopgemaak het dat die gemaakte woord volgorde het klein na geen invloek op die onderstreem effektuur van Transformer-gebaseerde taal modele oor baie NLP taak nie. Hierdie bevestings is teen die gemeenskaplike verstanding van hoe die modele hierarkies en strukturele inligting kodeer en selfs vraag as die woord volgorde met posisie inbettings modelleer is. Na hierdie einde, hierdie papier voorstel nege probeerdatumstels wat deur die tipe van kontroleerbaar teks perturbasie vir drie Indo-Europeese tale organiseer word met 'n verskillende grad van woord volgorde fleksibiliteit: Engels, Sweedse en Russe. Op die probeeranaliseer van die M-BERT en M-BART-modele, ons rapporteer dat die sintakteel sensitiviteit afhang van die taal en model voor-onderwerp-doels. Ons vind ook dat die sensitiviteit oor laagte groei saam met die vergroei van die perturbasie granulariteit. Laaste maar nie minste, ons wys dat die modele skaars die posisionele inligting gebruik om sintaktieke bome te induseer van hul tussen self-aandag en contextualiseerde voorstellings.", 'sq': 'Kërkimi i fundit ka miratuar një fushë të re eksperimentale të përqëndruar rreth konceptit të shqetësimeve të tekstit që ka zbuluar se rendi i përzier i fjalëve ka pak ose asnjë ndikim në shfaqjen poshtë të modeleve gjuhësore me bazë në Transformer nëpërmjet shumë detyrave NLP. Këto gjetje kundërshtojnë kuptimin e përbashkët të mënyrës se si modelet kodojnë informacionin hierarkik dhe strukturor dhe madje pyesin nëse fjalët renditet me përfshirje pozicioni. Për këtë qëllim, kjo letër propozon nëntë grupe të dhënash sondazh të organizuar nga lloji i shqetësimit të kontrollueshëm të tekstit për tre gjuhë indo-evropiane me një gradë të ndryshme fleksibiliteti të rendit të fjalëve: anglisht, suedez dhe rusisht. Bazuar në analizën e sondazhit të modeleve M-BERT dhe M-BART, ne raportojmë se ndjeshmëria sintaktike varet nga qëllimet e gjuhës dhe modelit parastërvitës. Gjithashtu gjejmë se ndjeshmëria rritet nëpër shtresa së bashku me rritjen e granularitetit të perturbacionit. Së fundi, por jo më së paku, ne tregojmë se modelet mezi përdorin informacionin pozitiv për të induktuar pemët sintaktike nga vetëvëmendja e tyre e ndërmjetësuar dhe përfaqësimet kontekstuale.', 'de': 'Jüngste Forschungen haben ein neues experimentelles Feld übernommen, das sich um das Konzept der Textstörungen dreht, das gezeigt hat, dass gemischte Wortreihenfolge wenig bis gar keinen Einfluss auf die nachgelagerte Leistung von Transformer-basierten Sprachmodellen in vielen NLP-Aufgaben hat. Diese Erkenntnisse widersprechen dem gängigen Verständnis, wie die Modelle hierarchische und strukturelle Informationen kodieren und sogar hinterfragen, ob die Wortordnung mit Positionseinbettungen modelliert wird. Zu diesem Zweck werden neun Datensätze vorgeschlagen, die nach der Art der kontrollierbaren Textstörung für drei indoeuropäische Sprachen mit unterschiedlichem Grad an Flexibilität der Wortreihenfolge organisiert sind: Englisch, Schwedisch und Russisch. Basierend auf der Sondierungsanalyse der M-BERT- und M-BART-Modelle berichten wir, dass die syntaktische Sensitivität von den Sprach- und Modellvorgaben abhängt. Wir finden auch, dass die Empfindlichkeit über Schichten hinweg wächst zusammen mit der Zunahme der Störgranularität. Last but not least zeigen wir, dass die Modelle die Positionsinformationen kaum nutzen, um syntaktische Bäume aus ihrer intermediären Selbstaufmerksamkeit und kontextualisierten Repräsentation zu induzieren.', 'am': 'በአሁኑ የፊደል ቋንቋ ምሳሌዎች በብዙ NLP ስራቶች ላይ የደረሰ የቃላት ትእዛዝ በጥልቅ ውጤት ላይ ምንም ጥቅም እንዳይኖር አዲስ ፈተና መሬት በመጠቀም ተፈትኖ ነበር፡፡ እነዚህ ፍለጋዎች የፊደል ቅርጽ እና የሠርዓዊ መረጃዎችን እንዴት እንደሚቀድሙት እና የቃላት ትእዛዝ በመስመር ማሰናከል እንደሆነ ጥያቄ ይደረጋሉ፡፡ ለዚህ ምክንያት ይህ ገጾች በተለያዩ የቃላት ክፍል በሦስት የኢንዶና-አውሮፓውያን ቋንቋዎች ላይ የተደረገውን ዘጠኝ የዳታተር ጽሑፎችን በመፍታት ይገልጻል፤ እንግሊዘኛ፣ ስዊድንና ራሽኛ፡፡ በM-BERT እና M-BART ሞዴላዎችን በመግለጽ በመሠረት ላይ እናስታውቃለን፡፡ We also find that the sensitivity grows across layers together with the increase of the perturbation granularity.  መጨረሻ ግን ግን ሳይታወስ ምሳሌዎቹ ከመካከለኛው ራሳቸውን እና በተገኘው መልዕክቶች የስልጣዊ መረጃዎችን ለማግኘት ሲያሳየቁ እናሳያቸዋለን፡፡', 'hy': 'Վերջերս ուսումնասիրությունները ընդունեցին նոր փորձարկման ոլորտ, որը կենտրոնացված է տեքստի խառնաշփոթման գաղափարի վրա, որը ցույց է տալիս, որ խառնաշփոթված բառերի կարգը շատ ՆԼՊ-ի առաջադրանքների ընթացքում փոքր-ինչ ազդեցություն չի ունեն Այս հայտնաբերությունները հակառակ են համայնքային հասկանալու, թե ինչպես են մոդելները կոդավորում հիերարխիկ և կառուցվածքային ինֆորմացիան և նույնիսկ հարցնում, թե արդյոք բառի կարգը մոդելվում է դիրքի ներդրման հետ: To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility: English, Swedish and Russian.  Հաշվի առնելով M-BERT և M-BART մոդելների ուսումնասիրության վրա, մենք հայտարարում ենք, որ սինտակտիկ զգացմունքը կախված է լեզվի և մոդելի նախապատրաստման նպատակներից: Մենք նաև հայտնաբերում ենք, որ զգացմունքը բարձրանում է շերտերի միջև, միասին խեղախուսության գրանուլարության աճի հետ: Վերջապես, բայց ոչ ամենաքիչ, մենք ցույց ենք տալիս, որ մոդելները հազիվ օգտագործում են տեղեկատվությունը սինտակտիկ ծառերի արտադրման համար իրենց միջին ինքնաուշադրությունից և կոնտեքստիալ ներկայացումներից:', 'az': 'Son araştırmalar NLP işlərində Transformer-tabanlı dil modellerinin düşürülmüş işləri üzərində bir az təsiri olmadığını göstərən mətn perturbasyonu tərəfində orta olan yeni təcrübə sahəsini qəbul etdi. Bu tapılar modellərin hiyerarşik və strukturlu məlumatlarının necə kodlamasını və hətta söz sıralamasının məlumatları ilə modelləşdirilməsini təsdiqləyən şəkildə qarşılaşırlar. Bu qədər, bu kağıt üç Indo-Avropa dillərinə müxtəlif bir dərəcə söz sıralaması fleksibiliyyəti ilə təşkil edilən təşkil verilən verilən verilən qurğuları təklif edir: İngilizce, İsveçə və Rus dillərinə. M-BERT və M-BART modellərin sınama analizinə görə, sintaktik duyarlığı dil və modellərin əvvəl təhsil məqsədilərinə bağlı olduğunu bildiririk. Biz də hissləşiklik səviyyətlərin hərəkətində bir-birinin artığı ilə birlikdə büyüdüyünü görürük. Əvvəlcə, amma ən azından, modellərin sintaktik ağaclarını öz tərəflərindən və contextualizə göstərilmələrindən uzaqlaşdırmaq üçün pozisiyal məlumatlarını çox az kullandığını göstəririk.', 'cs': 'Nedávný výzkum přijal nové experimentální pole zaměřené na koncept poruchy textu, což ukázalo, že míchaný řád slov má malý až žádný vliv na následnou výkonnost jazykových modelů založených na Transformeru napříč mnoha úlohami NLP. Tyto zjištění jsou v rozporu s obecným chápáním toho, jak modely kódují hierarchické a strukturální informace a dokonce se ptají, zda je slovní řád modelován pomocí vložení polohy. Za tímto účelem příspěvek navrhuje devět sondujících datových sad organizovaných podle typu kontrolovatelné poruchy textu pro tři indoevropské jazyky s různou mírou flexibility pořadí slov: angličtinu, švédštinu a ruštinu. Na základě sondační analýzy modelů M-BERT a M-BART uvádíme, že syntaktická citlivost závisí na jazykových a modelových cílech předškolení. Zjišťujeme také, že citlivost roste napříč vrstvami spolu se zvýšením poruchové granularity. V neposlední řadě ukazujeme, že modely sotva využívají poziční informace k vyvolání syntaktických stromů ze své střední sebepozornosti a kontextualizovaných reprezentací.', 'et': 'Hiljutised uuringud on kasutusele võtnud uue eksperimentaalse valdkonna, mis keskendub teksti häirete kontseptsioonile, mis on näidanud, et segatud sõnade järjestusel on vähe või mitte mingit mõju Transformer-põhiste keelemudelite jõudlusele paljudes NLP ülesannetes. Need tulemused on vastuolus ühise arusaamaga sellest, kuidas mudelid kodeerivad hierarhilist ja struktuurset infot ning isegi küsivad, kas sõnade järjekorda modelleeritakse positsioonide manustamisega. Selleks pakutakse käesolevas dokumendis välja üheksa uurimisandmekogumit, mis on korraldatud kontrollitava teksti häirete tüübi järgi kolmele Indo-Euroopa keelele, millel on erinev sõnajärjestuse paindlikkus: inglise, rootsi ja vene keel. M-BERT ja M-BART mudelite proovianalüüsi põhjal teatame, et süntaktiline tundlikkus sõltub keele ja mudeli eelkoolituse eesmärkidest. Samuti leiame, et tundlikkus kasvab kihtide vahel koos häirete granulaarsuse suurenemisega. Lõpuks näitame, et mudelid kasutavad vaevalt positsionaalset informatsiooni, et indutseerida süntaktilisi puid oma vahepealsest enesetähelepanust ja kontekstualiseeritud representatsioonidest.', 'fi': 'Viimeaikaiset tutkimukset ovat ottaneet käyttöön uuden kokeellisen kentän, joka keskittyy tekstihäiriöiden käsitteeseen, joka on paljastanut, että sekoitetulla sanajärjestyksellä on vain vähän tai ei lainkaan vaikutusta Transformer-pohjaisten kielimallien suorituskykyyn monissa NLP-tehtävissä. Nämä löydökset ovat ristiriidassa yleisen ymmärryksen kanssa siitä, miten mallit koodaavat hierarkkista ja rakenteellista tietoa ja jopa kyseenalaistavat, onko sanajärjestystä mallinnettu paikkaupotuksilla. Tätä varten tässä työssä ehdotetaan yhdeksää luotaavaa aineistoa, jotka on järjestetty kontrolloitavan tekstihäiriön tyypin mukaan kolmelle indoeurooppalaiselle kielelle, joilla on vaihteleva sanajärjestyksen joustavuus: englanti, ruotsi ja venäjä. M-BERT- ja M-BART-mallien luotausanalyysin perusteella raportoimme, että syntaktinen herkkyys riippuu kielen ja mallin esiopetuksen tavoitteista. Havaitsemme myös, että herkkyys kasvaa kerroksittain yhdessä häiriön rakeisuuden lisääntymisen kanssa. Lopuksi, mutta ei vähäisimpänä, osoitamme, että mallit tuskin käyttävät paikkatietoa saadakseen syntaktisia puita välitoimisesta itsetunnosta ja kontekstualisoiduista representaatioista.', 'bn': 'সম্প্রতি গবেষণা একটি নতুন পরীক্ষা ক্ষেত্র নিয়েছে যা টেক্সট বিভ্রান্তিকর ধারণার চিন্তায় কেন্দ্রে কেন্দ্রীয় এক পরীক্ষা ক্ষেত্র গ্রহণ করেছে যা প্রকাশ করে এই আবিস্কারের সাধারণ বুঝতে পারে যে মডেলগুলো কিভাবে হিয়েরার্কিক এবং কাঠামোর তথ্য এনকোড করে এবং এমনকি প্রশ্ন করে যে শব্দের আদেশ এই প্রতিবেদনের প্রস্তাব করছে যে তিন ইন্দো ইউরোপীয় ভাষার জন্য নিয়ন্ত্রণিত টেক্সট প্রতিযোগিতায় নিয়ন্ত্রণিত টেক্সট প্রতিষ্ঠানের জন্য আয়োজন করা হয়েছ M-BERT এবং M-BART মডেলের পরীক্ষা বিশ্লেষণের ভিত্তিতে আমরা রিপোর্ট করি যে সিন্টাক্টিক সংবেদনশীলতা ভাষা এবং মডেলের পূর্ব প্রশিক্ষণের লক্ষ্ We also find that the sensitivity grows across layers together with the increase of the perturbation granularity.  শেষ কিন্তু অন্তত আমরা দেখাচ্ছি যে মডেলগুলো তাদের মধ্যেকার আত্মমনোযোগ এবং প্রতিনিধিত্বের প্রতিনিধিদের কাছ থেকে সিন্ট্যা', 'bs': 'Nedavno istraživanje je usvojilo novo eksperimentalno polje usredotočeno oko koncepta tekstskih perturbacija, koji je otkrio da zabranjena riječi ne može imati utjecaja na sledeće učinke jezičkih modela na transformeri u mnogim NLP zadatkima. Ovi nalazi se suprotstavljaju zajedničkom razumijevanju o tome kako modeli kodiraju hijerarhičke i strukturne informacije i čak pitaju je li se redak riječi modelirao sa stavljanjem pozicija. Za taj cilj, ovaj papir predlaže devet probnih podataka organiziranih po vrsti kontroliranog teksta perturbacije za tri Indoevropske jezika sa različitim stupnjem fleksibilnosti riječi: engleskog, švedskog i ruskog reda. Na temelju istraživanja analize modela M-BERT i M-BART, izvještavamo da sintaktička osjetljivost zavisi od ciljeva jezika i modela predobuke. Također smatramo da osjetljivost raste preko slojeva zajedno sa povećanjem perturbacijske granularnosti. Posljednji, ali ne najmanje, pokazujemo da modeli jedva koriste pozicionalne informacije kako bi indukovali sintaktične drveće iz njihovih prosječnih samopouzdanja i kontekstualiziranih predstavljanja.', 'ca': "Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks.  Aquests descobriments contradicten la comprensió comú de com els models codifiquen la informació jeràrquica i estructural i fins i tot qüestionen si l'ordre de paraules està modelat amb incorporacions de posició. A tal efecte, aquest paper propon nou conjunts de dades d'investigació organitzats pel tipus de perturbació de text controlable per tres llengües indoeuropees amb un grau diferent de flexibilitat de l'ordre de paraules: anglès, suec i rus. Sobre la base de l'anàlisi de sondages dels models M-BERT i M-BART, informem que la sensibilitat sinàctica depèn dels objectius del llenguatge i del model de pré-entrenament. També descobrim que la sensibilitat creix a través de capes juntament amb l'augment de la granularitat de la perturbació. Finalment, però no menys, demostram que els models gairebé no utilitzen la informació de posició per induir arbres sinàctics a partir de la seva autoatenció intermediària i representacions contextualitzades.", 'ha': "A yanzu research na zãɓi wani field mai jarrabãwa wanda ke tsakanin hancin matsayin turturbatori da shi ya bayyana cewa cewa da tsarin faɗi bai da haske ba ga aikin misalin harshen na Transformer-based a cikin aikin mãsu yawa na NLP. Wannan gandina sunã motsi da ganin ɗabi'a ga yadda misãlai ke kodi information na hiera da tsari kuma kõ dã ana tambayi idan an motsa tsarin maganar da ke cikin shirin fitarwa. Ga wannan waraka, yana bukãtar da zane tara na jarraba data set organized by the kind of text perturbation of controlled text perturbation for lingui uku-Indo-EURopa with dijdin daraja mai sãɓa wa orda-word fleksibinci: Ingiriya, Iswidish and Ruushi. Basan da anayyar da misalin M-BERT da M-BERT, za mu yi bayani ga cẽwa, hisia'a na syntactic yana dõgara a kan harshen da misalin na zaman shawara. Kayya, za'a gane da hisia za'a sami duka da kori na danganiya. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.", 'sk': 'Nedavne raziskave so sprejele novo eksperimentalno področje, osredotočeno na koncept besedilnih motenj, ki je pokazalo, da mešani red besed malo ali sploh ne vpliva na uspešnost jezikovnih modelov na podlagi transformatorjev pri številnih nalogah NLP. Te ugotovitve so v nasprotju s skupnim razumevanjem, kako modeli kodirajo hierarhične in strukturne informacije in celo sprašujejo, ali je vrstni red besed modeliran z vgradnjo položaja. V ta namen v prispevku predlagamo devet naborov podatkov, organiziranih po vrsti nadzorovane motnje besedila za tri indoevropske jezike z različno stopnjo fleksibilnosti besednega reda: angleščina, švedščina in ruščina. Na podlagi analize sondiranja modelov M-BERT in M-BART poročamo, da je sintaktična občutljivost odvisna od ciljev predusposabljanja jezika in modela. Ugotavljamo tudi, da občutljivost raste preko plasti skupaj s povečanjem granularnosti motenj. Nenazadnje pa pokažemo, da modeli komaj uporabljajo pozicijske informacije za inducijo sintaktičnih dreves iz svoje vmesne samopozornosti in kontekstualiziranih reprezentacij.', 'jv': 'R Mbok-Ngerawat iki kuwi mengkar nggawe barang kelas kuwi bisa model sing kode karo akeh-akeh lan structural informasi lan nganggo kesempatan sampek kelas kuwi arah sing model karo akeh stir-stir iki. Sampeyan iki, pebuk iki supoyo sistem 9 asset kanggo ngilanggar sampek perbudhakan tèks sing nguasai kanggo telu luwih-ingkang manuk kanggo ngerasai perbudhakan karo sistem sing ayra-ayra perawaturan karo akeh akeh-ayra batar: Inggal, Suwita karo Rusa. Ngawe ngupakan seneng beraksi dipunanggé ning model M-BERT karo M-BaRT, kita ngupakan seneng sampeyan seneng wis dipunanggé kanggo langgambar lan model kuwi wis dipunanggé uwong. Awakdhéwé éntuk piyambak Kasasat luwih akeh piyambak ngono nguasai nggawe Laptop" and "Desktop', 'he': 'מחקר לאחרונה אימץ שדה ניסוי חדש המרכז סביב המושג של הפרעות טקסט אשר חשף שסדר מילים מעורבב יש מעט או שום השפעה על ההופעה המאוחרת של מודלים שפות מבוססים בטרנספורס ברחבי משימות רבות של NLP. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings.  למטרה זו, העיתון הזה מציע תשע קבוצות מידע חקירה מאורגנות על ידי סוג של הפרעת טקסט בלתי שולטת לשלושה שפות אינדו-אירופאיות עם מדרגה שונה של גמישות סדר מילים: אנגלית, שוודית ורוסית. בהתבסס על ניתוח הבדיקה של המודלים M-BERT ו-M-BART, אנחנו דווחים שהרגישות הסינטקטית תלויה על מטרות השפה והמודל לפני האימון. אנו גם מוצאים שהרגישות גדלה ברחבי שכבות יחד עם הגידול של granularity ההפרעה. בסופו של דבר, אנחנו מראים שהדוגמנים בקושי משתמשים במידע המקומי כדי לגרום לעצים סינטקטיים מהתשומת לב עצמית הביניית שלהם ויציגות קונטוקטואליזציות.', 'bo': 'ཉེ་ཆར་བརྗོད་ཀྱི་འཚོལ་ཞིབ་བྱས་ནས་སྐད་ཆ་བརྩོན་འགྱུར་བའི་ཚིག་ཡིག་གི་མཐོང་སྣང་གསར་བ་ཞིག་བེད་སྤྲོད་ཡོད། These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings. མཇུག་བསྡུ་དེ་ལ། ཤོག་བུ་འདིས་བྱས་པར་བསམ་བློ་གཏོང་གི་ཐབས་ལམ་ལུགས་ཀྱི་ཚིག་ཡིག་གི་སྣུམ་ཚིགས་གསུམ་གྱི་སྐད་ཡིག་ལ་བཟོ་བཅོས་ཐབས་རྒྱུ་དང་། M-BERT དང M-BART དཔེ་དབྱིབས་ཡུལ་གྱི་བརྟག་ཞིབ་དཔེ་གཞི་བྱས་ན། ང་ཚོའི་དབྱིབས་སྐྱེས ང་ཚོས་ཀྱང་མཐོང་ན། ཆེན་རྐྱེན་ཚད་ལ་བགོ་བ་དང་མཉམ་དུ་ཡར་རྒྱས་གཏོང་བ་རེད། མཐའ་མཇུག་དུ་མ་ཡིན་ནའང་དེ་ལྟར་མིན་པར། མིག'}
