{'en': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'ar': 'الاستفادة من خصوم النماذج المستخرجة لتحسين هجمات الصندوق الأسود', 'fr': "Exploiter les modèles d'adversaires extraits pour améliorer les attaques de boîtes noires", 'pt': 'Aproveitando os Adversários do Modelo Extraídos para Melhorar os Ataques de Caixa Preta', 'es': 'Aprovechamiento de los adversarios modelo extraídos para mejorar los ataques de caja negra', 'ja': '抽出されたモデルの敵を活用してブラックボックス攻撃を改善', 'ru': 'Использование извлеченных модельных противников для улучшенных атак «черного ящика»', 'ga': 'Giarálaigh Samhail a Bhaint as Samhail le haghaidh Ionsaithe Bosca Dubh Feabhsaithe', 'hi': 'बेहतर ब्लैक बॉक्स हमलों के लिए निकाले गए मॉडल विरोधियों का लाभ उठाना', 'zh': '因取模敌以攻黑匣子', 'ka': 'Name', 'hu': 'Kivont modellek kihasználása a fekete doboz támadások javításához', 'el': 'Αξιοποίηση των εξαγόμενων υποδειγματικών αντιπάλων για βελτιωμένες επιθέσεις μαύρου κουτιού', 'kk': 'Қара қоршау тіркемелерінде тарқату үлгісін өзгерту', 'mk': 'Разголемување на противниците на екстрактираниот модел за подобрени напади во црна кутија', 'lt': 'Ištrauktų modelių prieštaravimų pagerintiems juodojo langelio atakams didinimas', 'it': 'Sfruttare gli avversari dei modelli estratti per migliorare gli attacchi della scatola nera', 'mn': 'Сайжруулагдсан хар хайрцагны халдваруудын давхарлах загварын давхар', 'ms': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'ml': 'മെച്ചപ്പെടുത്തിയ കറുത്ത പെട്ടിയുടെ ആക്രമണങ്ങള്\u200dക്കായി എടുത്തുപോകുന്ന മോഡല്\u200d മുന്\u200dഗണന', 'ro': 'Valorificarea adversarilor modelelor extrase pentru atacurile îmbunătățite ale cutiilor negre', 'mt': 'L-intensifikazzjoni tal-Avversi Mudell Estratti għal Attakki mtejba tal-Kaxxa l-Iswed', 'pl': 'Wykorzystanie wyciągniętych modelowych przeciwników do ulepszonych ataków Black Box', 'sr': 'Utaknuti izvlačeni model naprednici za poboljšane napade crne kutije', 'sv': 'Utnyttja extraherade modellannonser för förbättrade Black Box-attacker', 'ta': 'மேம்படுத்தப்பட்ட கருப்புப் பெட்டியின் அடிக்குகளுக்கான வெளியேற்றப்பட்ட மாதிரி முன்னோட்டம்', 'so': 'Adversaries for Improved Black Box Attacks', 'si': 'ප්\u200dරවෘති කළු බොක්ස් පැත්තුව සඳහා ප්\u200dරවෘතිකරණ මඩේල් ප්\u200dරවෘතිකරණය', 'no': 'Leverar utpakka modell- konservasjonar for forbetra svart boks- Attacks', 'ur': 'استعمال کئے ہوئے نمڈل اڈورسٹر کے لئے لیورڈ کیا جاتا ہے', 'vi': 'KCharselect unicode block name là:', 'uz': 'Name', 'bg': 'Използване на извлечени рекламни модели за подобрени атаки в черната кутия', 'nl': 'Gebruik maken van geëxtraheerde modelwindragers voor verbeterde Black Box aanvallen', 'de': 'Nutzung extrahierter Modellfeindlicher für verbesserte Black Box Angriffe', 'hr': 'Uzbuđujući izvlačeni model naprednici za poboljšane napade crne kutije', 'da': 'Udnyttelse af ekstraherede model adversarer til forbedrede Black Box angreb', 'id': 'Meningkatkan Adversaries Model Ekstraksi untuk Menembak Serangan Kotak Hitam', 'ko': '추출한 모형으로 상대를 블랙박스 공격 개선', 'tr': 'Gelişmiş Kara Box Attackleri üçin Çiksirlenen Model Namaýyşçysy', 'af': 'Name', 'fa': 'برای حمله\u200cهای جعبه\u200cی سیاه\u200cپوست\u200cهای بهتر', 'sw': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'am': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'sq': 'Duke zgjeruar kundërshtarët e modelit të nxjerrë për sulmet e përmirësuara të kutisë së zezë', 'bs': 'Utaknuti izvlačeni model naprednici za poboljšane napade crne kutije', 'bn': 'উন্নত কালো বাক্সের আক্রমণের জন্য এক্সট্র্যাক্ট মোডেল প্রার্ভারেজিং লেভারেজিং করা হচ্ছে', 'az': 'Güncellənmiş Siyah Qutu Saldırıları üçün çıxarılan Model Adversaries', 'cs': 'Využití extrahovaných modelových nepříznivců pro lepší útoky na černou skříňku', 'fi': 'Poistettujen mallien adversaarien hyödyntäminen parannettujen Black Box -hyökkäysten aikaansaamiseksi', 'ca': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'et': 'Väljavõetud mudeli kõrvalsaaduste kasutamine paremate musta kasti rünnakute jaoks', 'hy': 'Զարգացված սևամորթ արկղի հարձակումների', 'he': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'sk': 'Izkoriščanje izvlečenih modelov za izboljšane napade v črni škatli', 'ha': 'Adversaries for Bach Sure Box Attacks', 'bo': 'Leveraging Extracted Model Adversaries for Improved Black Box Attacks', 'jv': 'Language'}
{'en': 'We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our  approach  is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANYa white box attackperformed on the approximate model by 25 %  F1 , and the ADDSENT attacka black box attackby 11 %  F1 .', 'ar': 'نقدم طريقة لتوليد المدخلات العدائية مقابل نماذج الصندوق الأسود للإجابة على الأسئلة القائمة على الفهم. نهجنا يتكون من خطوتين. أولاً ، نقوم بتقريب نموذج الصندوق الأسود الضحية عبر استخراج النموذج. ثانيًا ، نستخدم طريقة الصندوق الأبيض الخاصة بنا لتوليد اضطرابات في الإدخال تؤدي إلى فشل النموذج التقريبي. يتم استخدام هذه المدخلات المضطربة ضد الضحية. في التجارب وجدنا أن طريقتنا تحسن فعالية ADDANY - هجوم الصندوق الأبيض - الذي تم إجراؤه على النموذج التقريبي بنسبة 25٪ F1 ، وهجوم ADDSENT - هجوم الصندوق الأسود - بنسبة 11٪ F1.', 'pt': 'Apresentamos um método para geração de entrada adversarial contra modelos de caixa preta para resposta a perguntas baseadas em compreensão de leitura. Nossa abordagem é composta de duas etapas. Primeiro, aproximamos um modelo de caixa preta de vítima por meio de extração de modelo. Em segundo lugar, usamos nosso próprio método de caixa branca para gerar perturbações de entrada que fazem com que o modelo aproximado falhe. Essas entradas perturbadas são usadas contra a vítima. Em experimentos, descobrimos que nosso método melhora a eficácia do ADDANY - um ataque de caixa branca - realizado no modelo aproximado em 25% de F1, e o ataque ADDSENT - um ataque de caixa preta - em 11% de F1.', 'es': 'Presentamos un método para la generación de entradas contradictorias contra modelos de caja negra para responder a preguntas basadas en la comprensión lectora. Nuestro enfoque se compone de dos pasos. Primero, aproximamos un modelo de caja negra de la víctima mediante la extracción del modelo. En segundo lugar, utilizamos nuestro propio método de caja blanca para generar perturbaciones de entrada que hacen que el modelo aproximado falle. Estos insumos perturbados se utilizan contra la víctima. En experimentos, descubrimos que nuestro método mejora la eficacia del ADDANY, un ataque de caja blanca, realizado en el modelo aproximado en un 25% F1, y el ataque ADDSENT, un ataque de caja negra, en un 11% de F1.', 'fr': "Nous présentons une méthode de génération d'entrées contradictoires par rapport à des modèles de boîte noire pour répondre à des questions basées sur la compréhension de la lecture. Notre approche se compose de deux étapes. Tout d'abord, nous approximons un modèle de boîte noire victime par extraction de modèle. Ensuite, nous utilisons notre propre méthode de boîte blanche pour générer des perturbations d'entrée qui entraînent l'échec du modèle approximatif. Ces entrées perturbées sont utilisées contre la victime. Dans des expériences, nous avons constaté que notre méthode améliore l'efficacité de l'attaque ADDany, une attaque de boîte blanche, exécutée sur le modèle d'environ 25\xa0% F1, et de l'attaque ADDSENT, une attaque de boîte noire, de 11\xa0% F1.", 'ja': '理解度ベースの質問回答を読み取るためのブラックボックスモデルに対する対抗入力生成の方法を提示した。私たちのアプローチは2つのステップで構成されています。まず、モデル抽出を介して被害者のブラックボックスモデルを近似します。第二に、独自のホワイトボックス法を使用して、近似モデルが失敗する原因となる入力摂動を生成します。これらの乱れた入力は、被害者に対して使用されます。実験では、我々の方法は、およそのモデルで実行されるホワイトボックス攻撃であるＡＤＤＡＮＹの有効性を２ ５ ％ Ｆ１によって向上させ、ＡＤＤＳＥＮＴ攻撃であるブラックボックス攻撃である１ １ ％ Ｆ１によって向上させることが見出された。', 'ru': 'Предложен метод генерации состязательных входных данных в сравнении с моделями черного ящика для ответа на вопросы, основанные на понимании прочитанного. Наш подход состоит из двух этапов. Во-первых, мы аппроксимируем модель «черного ящика» жертвы с помощью извлечения модели. Во-вторых, мы используем наш собственный метод белого ящика для генерации входных возмущений, которые приводят к сбою приближенной модели. Эти возмущенные входные данные используются против жертвы. В экспериментах мы обнаружили, что наш метод улучшает эффективность атаки ADDANY - белого ящика, выполненного на приблизительной модели на 25% F1, и атаки ADDSENT - черного ящика на 11% F1.', 'ga': 'Cuirimid i láthair modh chun ionchur sáraíochta a ghiniúint i gcoinne samhlacha an bhosca dhubh chun ceisteanna a fhreagairt atá bunaithe ar léamhthuiscint. Tá ár gcur chuige comhdhéanta de dhá chéim. Ar dtús, déanaimid múnla bosca dubh íospartach a chomhfhogasú trí eastóscadh samhail. Ar an dara dul síos, bainimid úsáid as ár modh bosca bán féin chun suaitheadh ionchuir a ghiniúint a fhágann go dteipeann ar an múnla gar. Úsáidtear na hionchuir suaite seo i gcoinne an íospartaigh. I dturgnaimh feicimid go bhfeabhsaítear ár modh ar éifeachtúlacht an ADDANY - ionsaí bosca bán - a rinneadh ar an tsamhail thart ar 25% F1, agus an t-ionsaí ADDSENT - ionsaí bosca dubh - faoi 11% F1.', 'zh': '发针于读解之问答案黑盒模形对抗性输生成之法。 吾法以两步成之。 先取近似受害者黑匣子模。 其次,我用自己的白盒法生成近似模败的输入扰动。 其扰动者输以应受害者。 实验之中,见吾道之ADDANY(白盒攻)之功益25%F1,ADDSENT攻(黑盒攻)益11%F1。', 'hi': 'हम समझ आधारित प्रश्न उत्तर पढ़ने के लिए ब्लैक बॉक्स मॉडल के खिलाफ प्रतिकूल इनपुट पीढ़ी के लिए एक विधि प्रस्तुत करते हैं। हमारा दृष्टिकोण दो चरणों से बना है। सबसे पहले, हम मॉडल निष्कर्षण के माध्यम से एक शिकार ब्लैक बॉक्स मॉडल का अनुमान लगाते हैं। दूसरा, हम इनपुट गड़बड़ी उत्पन्न करने के लिए अपनी खुद की सफेद बॉक्स विधि का उपयोग करते हैं जो अनुमानित मॉडल को विफल करने का कारण बनता है। इन परेशान इनपुट का उपयोग पीड़ित के खिलाफ किया जाता है। प्रयोगों में हम पाते हैं कि हमारी विधि ADDANY की प्रभावकारिता पर सुधार करती है - एक सफेद बॉक्स हमला - अनुमानित मॉडल पर 25% F1 द्वारा प्रदर्शन किया जाता है, और ADDSENT हमला - एक ब्लैक बॉक्स हमला - 11% F1 द्वारा।', 'ka': 'ჩვენ გავაკეთებთ გარეშე კითხვის მისამართლად გარეშე კითხვის მისამართლად გარეშე განცემების მისამართლად გარეშე მეტი. ჩვენი მოხმარება ორი ნაწილის შექმნა. პირველად, ჩვენ მივიღეთ ზედაპირებული შავი კოსტის მოდელს მოდელური ექსტრექციის გამოყენებით. მეორე, ჩვენ გამოყენებთ ჩვენი თავისუფლიო ბელი კოსტის მეტი, რომელიც შეიძლება გადავიწყენოთ პერტუბურაციები, რომელიც გადავიწყენება მოდე რვჱთ ოპვრპსბპთპანთ გპყჱკთ ჟვ თჱოჲლჱგარ ოპჲრთგ ზვპრგარა. ექსპერიმენტებში ჩვენ ვაკვირდებით, რომ ჩვენი მეტი აფექტიურება ADDANY-ის ბელი კოსტის ატაქტის აფექტიურობაზე, რომელიც გავაკეთებული 25% F1 მოდელზე, და ADDSENT ატაქტის შავი კოსტის ატაქტის 11', 'hu': 'Bemutatjuk a fekete doboz modellekkel szembeni ellentmondásos bemenetek generálásának módszerét az olvasásértésen alapuló kérdések megválaszolására. A megközelítésünk két lépésből áll. Először is, megközelítjük az áldozat fekete doboz modelljét modellkivonással. Másodszor, saját fehér dobozos módszerünket használjuk a bemeneti perturbációk generálására, amelyek miatt a hozzávetőleges modell meghibásodik. Ezeket a zavaros bemeneteket az áldozat ellen használják. Kísérleteinkben azt találtuk, hogy módszerünk 25%-kal F1-vel javítja az ADDANY – egy fehér dobozos támadás – hatékonyságát a hozzávetőleges modellre, míg az ADDSENT támadás – egy fekete dobozos támadás – 11%-kal F1.', 'el': 'Παρουσιάζουμε μια μέθοδο για τη δημιουργία αντικρουόμενων εισροών έναντι μοντέλων μαύρων κουτιών για την απάντηση ερωτήσεων βασισμένων στην κατανόηση ανάγνωσης. Η προσέγγισή μας αποτελείται από δύο βήματα. Πρώτα, προσεγγίζουμε ένα μοντέλο μαύρου κουτιού θύματος μέσω εξαγωγής μοντέλου. Δεύτερον, χρησιμοποιούμε τη δική μας μέθοδο λευκού πλαισίου για να παράγουμε διαταραχές εισόδου που προκαλούν αποτυχία του κατά προσέγγιση μοντέλου. Αυτές οι διαταραγμένες εισόδοι χρησιμοποιούνται εναντίον του θύματος. Σε πειράματα διαπιστώνουμε ότι η μέθοδος μας βελτιώνει την αποτελεσματικότητα της επίθεσης λευκού κουτιού που εκτελείται στο κατά προσέγγιση μοντέλο κατά 25% F1 και της επίθεσης κατά μαύρο κουτί κατά 11% F1.', 'lt': 'Pateikiame priešingos įvedimo generacijos metodą prieš juodos dėžutės modelius skaitant supratimu pagrįstus klausimų atsakymus. Mūsų požiūrį sudaro du žingsniai. Pirma, mes apytikriai apytikriai apytikriai apytikriai nukentėjusios juodos dėžutės model į naudojant modelio ekstrakciją. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail.  Šie sutrikę įvedimai naudojami prieš auką. Eksperimentuose nustatome, kad mūsų metodas pagerina ADDANY - baltosios dėžutės atako veiksmingumą, atlikto apytikriai modeliui 25% F1, o ADDSENT atakas - juodos dėžutės atakas - 11% F1.', 'it': "Presentiamo un metodo per la generazione di input avversi contro modelli di black box per la risposta alle domande basate sulla comprensione della lettura. Il nostro approccio è composto da due fasi. Per prima cosa, approximiamo un modello di scatola nera della vittima tramite l'estrazione del modello. In secondo luogo, utilizziamo il nostro metodo white box per generare perturbazioni di input che causano il fallimento del modello approssimativo. Questi input perturbati vengono usati contro la vittima. Negli esperimenti scopriamo che il nostro metodo migliora l'efficacia dell'ADDANY, un attacco a scatola bianca, eseguito sul modello approssimativo del 25% F1, e dell'attacco ADDSENT, un attacco a scatola nera, dell'11% F1.", 'mk': 'Презентираме метод за генерација на противници против моделите на црна кутија за читање на одговори на прашања базирани на разбирање. Нашиот пристап е составен од два чекори. Прво, го приближуваме моделот на жртвата со црна кутија преку екстракција на моделот. Второ, го користиме нашиот метод на бела кутија за да генерираме внатрешни пречки кои предизвикуваат неуспех на приближниот модел. Овие нервозни внесувања се користат против жртвата. Во експериментите откриваме дека нашиот метод ја подобрува ефикасноста на АДДАНИ-белата кутија нападна на приближниот модел за 25% Ф1, а АДДСЕНТ нападна црна кутија нападна за 11% Ф1.', 'kk': 'Біз қарсы енгізу әдісін түсінікті сұрақ жауап беру үшін қара қоршау үлгілеріне қарсы құру әдісін таңдаймыз. Біздің тәсіліміз екі қадам құрылады. Біріншіден, үлгісін тарқату арқылы жетілдірген қара жазу үлгісін қолданамыз. Екіншіден, біз өзіміздің ақ қоршау әдімімізді қолдану үшін келтірілген ауруларды құру үшін қолданамыз. Бұл үлгі қайталанбады. Бұл қарқынышты жазулар жетістіктерге қарсы қолданылады. Тәжірибелерде біз әдіміміздің ADDANY- ақ қоршау үлгісінде 25% F1 деген үлгісінде жұмыс істейтін әдістерді жақсы көтереді, және ADDSENT қара қоршау үлгісінде 11% F1 деген қара қоршауын жасайды.', 'ms': 'Kami perkenalkan kaedah untuk generasi input lawan terhadap model kotak hitam untuk membaca jawapan soalan berdasarkan pemahaman. Our approach is composed of two steps.  Pertama, kita kira-kira model kotak hitam mangsa melalui ekstraksi model. Kedua, kita gunakan kaedah kotak putih untuk menghasilkan gangguan input yang menyebabkan model kira-kira gagal. These perturbed inputs are used against the victim.  In experiments we find that our method improves on the efficacy of the ADDANY-a white box attack-performed on the approximate model by 25% F1, and the ADDSENT attack-a black box attack-by 11% F1.', 'ml': 'ബ്ലാക്ക് ബോക്സ് മോഡലുകള്\u200dക്കെതിരായി വായിക്കുന്നതിന് വിരോധമായി ഇന്\u200dപുട്ട് തലമുറയ്ക്കുന്ന ഒരു രീ നമ്മുടെ അടുത്തേക്ക് രണ്ട് ചുവടുകള്\u200d ഉണ്ടാക്കിയിരിക്കുന്നു. ആദ്യം, നമ്മള്\u200d ഒരു ബ്ലാക്ക് ബോക്സിന്റെ മോഡല്\u200d മാതൃകയെ മാറ്റിക്കൊണ്ട്. രണ്ടാമത്, നമ്മള്\u200d നമ്മുടെ സ്വന്തം വെളുത്ത പെട്ടിയുടെ രീതിയില്\u200d ഉപയോഗിക്കുന്നു. അതിന്റെ അടുത്ത മോഡല്\u200d പരാജയപ്പെട ഈ കുഴപ്പമുള്ള ഇന്\u200dപുട്ടുകള്\u200d പ്രയോഗിക്കപ്പെട്ടവന്\u200dറെ നേരെ ഉപയോഗിക്കുന്നു. പരീക്ഷണങ്ങളില്\u200d ഞങ്ങള്\u200d കണ്ടെത്തുന്നത് നമ്മുടെ രീതിയില്\u200d മെച്ചപ്പെടുത്തുന്നതാണ്- ഒരു വെളുത്ത പെട്ടിയുടെ ആക്രമണത്തിന്റെ പ്രഭാവം മോഡലില്\u200d പ്രവര്\u200dത്ത', 'pl': 'Przedstawiamy metodę generowania przeciwnych danych wejściowych na podstawie modeli czarnej skrzynki do odpowiedzi na pytania oparte na rozumieniu czytania. Nasze podejście składa się z dwóch etapów. Najpierw przybliżamy model czarnej skrzynki ofiary poprzez ekstrakcję modelu. Po drugie, używamy własnej metody white box do generowania zakłóceń wejściowych, które powodują awarię modelu przybliżonego. Te zakłócone wejścia są używane przeciwko ofierze. W eksperymentach stwierdzono, że nasza metoda poprawia skuteczność ataku ADDANY – białej skrzynki – wykonywanego na modelu przybliżonym o 25% F1, a ataku ADSDENT – ataku czarnej skrzynki – o 11% F1.', 'mt': 'Aħna nippreżentaw metodu għall-ġenerazzjoni ta’ input avversarju kontra mudelli tal-kaxxa sewda għall-qari tat-tweġibiet għall-mistoqsijiet ibbażati fuq il-komprensjoni. Our approach is composed of two steps.  L-ewwel nett, nipprossimaw mudell ta’ kaxxa sewda tal-vittmi permezz ta’ estrazzjoni tal-mudell. It-tieni nett, aħna nużaw il-metodu tal-kaxxa bajda tagħna stess biex niġġeneraw perturbazzjonijiet tal-input li jikkawżaw li l-mudell approssimattiv ifalli. Dawn l-inputs imfixkla jintużaw kontra l-vittma. Fl-esperimenti nsibu li l-metodu tagħna jtejjeb fuq l-effikaċja ta’ ADDANY-attakk ta’ kaxxa bajda mwettaq fuq il-mudell approssimattiv b’25% F1, u l-ADDSENT attakk ta’ kaxxa sewda b’11% F1.', 'mn': 'Бид хар хайрцаг загваруудын эсрэг орлуулах арга загварыг харуулдаг. Бидний арга зам нь хоёр алхам байдаг. Эхлээд бид хохирогчдын хар хайрцаг загварыг загвар гаргаж ирсэн. Хоёрт, бид өөрсдийн цагаан хайрцаг аргыг ашиглаж, ойролцоогоор загвар алдагдахын тулд орж ирсэн пертурбацийг бий болгодог. Эдгээр сэтгэл хөдлөлт нь хохирогчийн эсрэг хэрэглэгддэг. Туудлын туршилтад бидний арга нь АДДАНИЙ-ын цагаан хайрцаг шигдээний үр дүнг 25% F1-д гаргаж байгаа загвар дээр хөгжүүлдэг ба АДДСЕНТ-ын хар хайрцаг 11% F1-д гаргаж байгаа хар хайрцаг.', 'ro': 'Vă prezentăm o metodă de generare a intrărilor adversare împotriva modelelor de cutie neagră pentru răspunsul la întrebări bazate pe înțelegerea citirii. Abordarea noastră este compusă din două etape. Mai întâi, aproximăm modelul unei cutii negre a victimei prin extragerea modelului. În al doilea rând, folosim propria noastră metodă a casetei albe pentru a genera perturbații de intrare care provoacă eșecul modelului aproximativ. Aceste intrări perturbate sunt folosite împotriva victimei. În experimente descoperim că metoda noastră îmbunătățește eficacitatea ADDANY-un atac de cutie albă-efectuat pe modelul aproximativ cu 25% F1, iar atacul ADDSENT-un atac de cutie neagră-cu 11% F1.', 'no': 'Vi presenterer ein metode for negativ inndata mot svart boksmodular for lesing av forståelse basert spørsmål. Tilnærminga vårt er lagt av to steg. Først er vi omtrent ein svart boks-modell med utpakking av modell. Andre, vi bruker vårt eigen kvit boks-metode for å laga inndataperturbasjonar som fører til at den omtrente modellen mislukkast. Desse opplysningane blir brukte mot oftene. I eksperimenter finn vi at metoden vårt forbetrar effekten av ADDANY-en kvit boks-attack-utført på omtrent modellen med 25% F1, og ADDSENT-en svart boks-attack-med 11% F1.', 'so': "Waxaynu soo bandhignaa qaab ka gees ah muuqashada qoriga ah oo ka gees ah tusaale madow, si aan u akhrinno jawaabta su'aalaha aasaasiga ah. Dhaqdhaheenna waa laba tallaabo. Marka ugu horeysa, waxaynu sameynaa tusaale madow ah oo dhibbanaha ah Second, waxaynu isticmaalnaa qaababkayaga cadowga ah si aan u sameyno qalloocan, taasoo sababta tilmaanka ku dhow uu baabba'o. Waxyaabahan la soo saaro waxaa loogu isticmaalaa dhibbanaha. Imtixaanka waxaynu ka heleynaa in qaababkayagu uu korodhsadaa saameynta ADDANY-weerarka cadowga lagu sameeyo modelka ku dhow 25% F1, kadibna ADDSENT weerarka qoriga ah oo ku dhacay 11% F1.", 'sv': 'Vi presenterar en metod för motstridig inmatning generering mot svartboxmodeller för läsförståelse baserat frågesvar. Vårt tillvägagångssätt består av två steg. Först approximerar vi en modell av offrets svarta låda via modellutvinning. För det andra använder vi vår egen white box-metod för att generera indatastörningar som orsakar att den ungefärliga modellen misslyckas. Dessa störningar används mot offret. I experiment finner vi att vår metod förbättrar effekten av ADDANY – en vit låda attack – utförd på den ungefärliga modellen med 25% F1, och ADDSENT attack – en svart låda attack – med 11% F1.', 'ta': 'கருப்பெட்டி மாதிரிகளுக்கு எதிரான உள்ளீட்டு உருவாக்க ஒரு முறையை நாம் கொண்டுள்ளோம் கேள்வி கேட்கை வி எங்கள் அணுக்கம் இரண்டு படிகளில் உள்ளது. முதலில், நாம் ஒரு பிடிக்கப்பட்ட கருப்பு பெட்டியின் மாதிரி முறைமையை மாதிரி பெற்றுவிடுகிற இரண்டாவது, நாம் எங்கள் சொந்த வெள்ளை பெட்டியின் முறைமையை பயன்படுத்தி உள்ளீட்டு பிரச்சனைகளை உருவாக்குகிறோம். அது  இந்த குழப்பமான உள்ளீடுகள் பாதிக்கப்பட்டவருக்கு எதிராக பயன்படுத்தப்படுகிறது. In experiments we find that our method improves on the efficacy of the ADDANY-a white box attack-performed on the approximate model by 25% F1, and the ADDSENT attack-a black box attack-by 11% F1.', 'si': 'අපි ප්\u200dරශ්න ප්\u200dරශ්න ප්\u200dරතිච්චාරයක් කළු බොක්ස් මොඩේල්ස් විරුද්ධ වෙනුවෙන් විරුද්ධ විදිහට අපේ පැත්තේ පැත්තක් දෙකක් තියෙනවා. මුලින්ම, අපි ප්\u200dරතිකාරයෙක්ගේ කළු බොක්ස් මොඩේල් එකක් පිළිගන්නේ. දෙවෙනි විදියට, අපි අපේ පුළුවන් සුදු පෙට්ටිය ප්\u200dරවේශනය භාවිතා කරනවා ඇතුළු ප්\u200dරවේශනය නිර්මාණය කරන මේ ප්\u200dරශ්නයක් වෙනුවෙන් පාවිච්චි කරනවා. අපි පරීක්ෂණාවට හොයාගන්නවා අපේ විධානය ADDANY-සුදු පෙට්ටියක් ප්\u200dරතික්\u200dරියාවට ප්\u200dරතික්\u200dරියා වෙන්න පුළුවන් කියලා, සහ ADDSENT ප්\u200dරතික්\u200dරියාව, ක', 'sr': 'Predstavljamo metodu za neprijateljsku generaciju unosa protiv modela crne kutije za čitanje razumijevanja na osnovu odgovora na pitanja. Naš pristup je sastavljen od dva koraka. Prvo, približavamo model žrtve crne kutije putem izvlačenja model a. Drugo, koristimo svoju metodu bele kutije da bi stvorili ulazne perturbacije koje uzrokuju da približni model ne uspe. Ovi uznemireni ulazi se koriste protiv žrtve. U eksperimentima nalazimo da se naš metod poboljšava na efikasnost napada na ADDANY-bijeloj kutiji na približnom modelu za 25% F1, i napad na ADDSENT-crnu kutiju za 11% F1.', 'ur': 'ہم ایک طریقہ مقابلہ اینپیٹ نسل کے لئے سیاه باکس موڈل کے مقابلہ میں نشان دیتے ہیں جو سمجھنے کی بنیاد سوال جواب کے لئے ہے۔ ہمارے قریب دو قدم سے پیدا ہوا ہے پہلے، ہم ایک قربانی سیاہ باکس موڈل کو مدل اخراج کے ذریعے تقریباً کر رہے ہیں۔ دوسرا، ہم اپنے سفید باکس طریقہ سے استعمال کرتے ہیں کہ انپ پیٹربیٹ پیدا کریں جو تقریباً موڈل ناکام ہو جاتا ہے۔ یہ مصیبت مصیبت قربانی کے خلاف استعمال کئے جاتے ہیں. آزمائش میں ہم دیکھتے ہیں کہ ہمارا طریقہ ADDANY-ایک سفید باکس حملہ پر تقریباً 25% F1 کے ذریعہ عمل کئے گئے ہیں اور ADDSENT حملہ-ایک سیاه باکس حملہ-11% F1 کے ذریعہ۔', 'uz': "Biz oddiy narsalarni o'qish asosiy savol javob berish uchun qoq qutis modellari bilan foydalanishimiz usuli. Bizning o'zgarimiz ikki qadamdan yaratiladi. Birinchisi, biz modelni chiqarish orqali qizil qutis modelini ko'rib chiqaramiz. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail.  Bu qo'llangan narsalar qo'shilga ishlatiladi. Tez tajribalarda bizning usuli ADDANY'ning effektini oshirish imkoniyatini o'zgartiradi. Bu qisqa modelda 25% F1 tomonidan bajarilgan oq qutisi, va ADSENT harakati 11% F1 bilan bajarishga ega.", 'vi': 'Chúng tôi giới thiệu một phương pháp sản xuất nhập từ hộp đen để đọc các câu hỏi dựa trên sự thấu hiểu. Cách tiếp cận của chúng ta gồm hai bước. Trước tiên, chúng ta phải xác định lại mô hình hộp đen nạn nhân. Thứ hai, chúng tôi dùng phương pháp hộp màu trắng riêng để tạo ra bất cập nhập làm cho mô hình xấp xỉ bị hỏng. Những thứ gây xáo trộn này được dùng chống lại nạn nhân. Trong các thí nghiệm, chúng tôi tìm thấy phương pháp của chúng tôi cải thiện hiệu quả của tập tấn hộp màu trắng d.A.A.A.A.A.R. được thực hiện trên mô hình xấp xỉ gồm 25. F1. và tập tấn công từ hộp đen.', 'bg': 'Представяме метод за генериране на противоречиви входни данни срещу модели черни кутии за четене, базирани на отговор на въпроси. Нашият подход се състои от две стъпки. Първо, приближаваме модела на черна кутия на жертвата чрез извличане на модел. Второ, използваме собствен метод на бялата кутия, за да генерираме смущения на входа, които причиняват неуспех на приблизителния модел. Тези смущени входове се използват срещу жертвата. В експерименти откриваме, че нашият метод подобрява ефикасността на атаката на бялата кутия, извършена върху приблизителния модел с 25% и атаката на АДСЕНТ - атака на черната кутия - с 11%.', 'hr': 'Predstavljamo metodu za neprijateljsku generaciju unosa protiv modela crne kutije za čitanje razumijevanja na osnovu odgovora na pitanja. Naš pristup je sastavljen od dva koraka. Prvo, približavamo model žrtve crne kutije putem izvlačenja model a. Drugo, koristimo vlastitu metodu bijele kutije kako bi stvorili ulazne perturbacije koje uzrokuju propast približnog modela. Ovi uznemireni ulazi se koriste protiv žrtve. U eksperimentima nalazimo da se naš metod poboljšava na učinkovitosti napada ADDANY-a bijelog kutija na približnom modelu za 25% F1, i napad ADDSENT-a na crnu kutiju za 11% F1.', 'nl': 'We presenteren een methode voor het genereren van tegenstrijdige input tegen black box modellen voor het beantwoorden van vragen op basis van leesbegrip. Onze aanpak bestaat uit twee stappen. Eerst benaderen we een slachtoffer black box model via model extractie. Ten tweede gebruiken we onze eigen white box methode om input verstoringen te genereren die ervoor zorgen dat het approximatieve model mislukt. Deze gestoorde inputs worden gebruikt tegen het slachtoffer. In experimenten zien we dat onze methode de effectiviteit van de ADDANY-een white box aanval-uitgevoerd op het geschatte model met 25% F1, en de ADDSENT aanval-een black box aanval-met 11% F1 verbetert.', 'id': 'Kami mempersembahkan metode untuk generasi input musuh melawan model kotak hitam untuk membaca pemahaman berdasarkan jawaban pertanyaan. pendekatan kita terdiri dari dua langkah. Pertama, kita mendekati model kotak hitam korban melalui ekstraksi model. Kedua, kita menggunakan metode kotak putih kita sendiri untuk menghasilkan perturbasi input yang menyebabkan model sekitar gagal. Masukan kacau ini digunakan melawan korban. Dalam eksperimen kami menemukan bahwa metode kami memperbaiki efektivitas dari ADDANY-sebuah kotak putih serangan-dilakukan pada model sekitar 25% F1, dan ADDSENT serangan-kotak hitam serangan-oleh 11% F1.', 'de': 'Wir prรคsentieren eine Methode zur Generierung von kontroversen Eingaben gegen Black-Box-Modelle zur Leseverstรคndnisbasierten Beantwortung von Fragen. Unser Ansatz besteht aus zwei Schritten. Zunรคchst nรคhern wir ein Opfer-Black-Box-Modell mittels Modellextraktion an. Zweitens verwenden wir unsere eigene White Box Methode, um Eingabestรถrungen zu generieren, die dazu fรผhren, dass das approximative Modell fehlschlรคgt. Diese gestรถrten Eingaben werden gegen das Opfer verwendet. In Experimenten fanden wir heraus, dass unsere Methode die Wirksamkeit des ADDANY โ\x80\x93 eines White Box Angriffs โ\x80\x93 am ungefรคhren Modell um 25% F1 und des ADDSENT Angriffs โ\x80\x93 um 11% F1 verbessert.', 'ko': '우리는 읽기와 이해를 바탕으로 하는 퀴즈 블랙박스 모델에 대한 대항적 입력 생성 방법을 제시했다.우리의 방법은 두 단계로 구성되어 있다.우선, 우리는 모델 추출을 통해 피해자의 블랙박스 모델과 비슷하다.그 다음으로 우리는 우리의 백합 방법을 사용하여 근사 모델의 실패를 초래하는 입력 교란을 생성한다.이런 방해받은 입력은 피해자를 대처하는 데 쓰인다.실험에서 우리는 우리의 방법이 25%의 F1이 근사 모델에 대한 ADDANY-a 백합 공격과 11%의 F1이 ADDSENT-a 블랙박스 공격에 대한 유효성을 높인 것을 발견했다.', 'da': 'Vi præsenterer en metode til modstridende input generering mod sorte boksmodeller til læseforståelse baseret spørgsmål besvarelse. Vores tilgang består af to trin. For det første, vi omtaler en offers sort boks model via model udtrækning. For det andet bruger vi vores egen hvide boks metode til at generere input forstyrrelser, der forårsager den omtrentlige model til at mislykkes. Disse forstyrrede input bruges mod ofret. I eksperimenter finder vi, at vores metode forbedrer effektiviteten af ADDANY-et hvid boksangreb-udført på den omtrentlige model med 25% F1, og ADDSENT-angrebet-et sort boksangreb-med 11% F1.', 'sw': 'Tunaweza kuweka mbinu ya kizazi cha upinzani dhidi ya mifano nyeusi ya kisasa ya kusoma majibu ya maswali yenye msingi wa maswali. Hatua yetu imetengenezwa na hatua mbili. Kwanza, tunakaribia muundo wa kisanduku mweusi wa mhanga kupitia utekelezaji wa modeli. Pili, tunatumia mbinu zetu wenyewe weupe ili kutengeneza mabadiliko yanayosababisha mtindo wa karibu kushindwa. Matukio haya yanatumiwa dhidi ya mwathirika. Katika majaribio tunagundua kuwa mbinu yetu inaboresha ufanisi wa shambulio la ADDANY-boksi nyeupe lililofanyika kwenye muundo wa karibu wa asilimia 25 F1, na shambulio la ADDSENT lililofanywa na shambulio la boksi nyeusi kwa asilimia 11 F1.', 'tr': 'Biz 챌yky힊 girdi d철redigine kara guty nusgalaryny d체힊체nmek 체챌in bir nusga g철rke첵채ris. Bizi흫 첵ary힊ymyz 2 ad캇mdan b철l첵채r. Ilkinji gezek, bir kurbany흫 gara guty nusgasyny 챌ekmek 체챌in belle첵채ris Ikinci olarak, yakla힊캇k modan캇n bo힊almas캇na sebep olan beyaz kutu y철ntemimizi kullan캇r캇z. Bular baglan첵an go힊ulary kurbany흫 gar힊y bilen ulan첵arlar. Deneylerde bizi흫 metodumyz ADDANY-i흫 ak gutyny흫 etkinli첵asynda 25% F1 we ADDSENT-i흫 gara gutyny흫 salykynda 11% F1 첵akyn nusgasynda t채sirle첵채r.', 'fa': 'ما یک روش برای نسل ورودی دشمنی در مقابل مدلهای جعبه سیاه برای خواندن جواب سوال\u200cهای بنیاد درک را پیشنهاد می\u200cکنیم. نزدیک ما از دو قدم ساخته شده است. اول، ما تقریباً یک مدل جعبه سیاه از طریق استخراج مدل تقریباً می\u200cکنیم. دوم، ما از طریق جعبه سفید خودمان استفاده می کنیم تا تولید تغییرات وارد شود که باعث می شود مدل تقریباً شکست خورد. این وسیله\u200cهای ناراحتی در مقابل قربانی استفاده می\u200cشود. در آزمایشات پیدا می\u200cکنیم که روش ما بر اثرات حمله\u200cی جعبه سفید ADDANY در مدل تقریباً 25% F1، و حمله\u200cی ADDSENT-یک جعبه سیاه با 11% F1، بهتر می\u200cشود.', 'af': "Ons stel 'n metode vir teenstandaarlike invoer generasie teen swart boks modele vir lees verstandigheid gebaseerde vraag antwoord. Ons toegang is gemaak van twee stappe. Eerste, ons omtrent 'n slagoffer swart boks model deur model uitvoer. Tweede, ons gebruik ons eie wit boks metode om invoer perturbasies te genereer wat veroorsaak dat die omtrent model misluk word. Hierdie perturbeerde inputs word gebruik teen die victim. In eksperimente vind ons dat ons metode verbeter op die effektiviteit van die ADDANY-â\x80\x99n wit boks atak-uitgevoer op die omtrent model deur 25% F1, en die ADDSENT atak-â\x80\x99n swart boks atak-deur 11% F1.", 'sq': 'Ne paraqesim një metodë për gjeneratën e hyrjes kundërshtare kundër modeleve të kutisë së zezë për të lexuar përgjigjet e pyetjeve të bazuara në kuptim. Përqafimi ynë është i përbërë nga dy hapa. Së pari, ne përafërsisim një model të kutisë së zezë të viktimave nëpërmjet nxjerrjes së modelit. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail.  Këto hyrje të shqetësuara përdoren kundër viktimës. In experiments we find that our method improves on the efficacy of the ADDANY-a white box attack-performed on the approximate model by 25% F1, and the ADDSENT attack-a black box attack-by 11% F1.', 'hy': 'Մենք ներկայացնում ենք հակառակ մուտքային սերունդների մեթոդ սև արկղի մոդելների դեմ ընկալելու ընկալումների հիմնված հարցերի պատասխանների համար: Մեր մոտեցումը կազմում է երկու քայլ: Առաջինը, մենք մոտավորում ենք զոհի սև արկղի մոդելը մոդելների վերացման միջոցով: Երկրորդ, մենք օգտագործում ենք մեր սեփական սպիտակ արկղի մեթոդը, որպեսզի ստեղծենք ներմուծքային շեղումներ, որոնք ստիպում են մոտավորապես մոդելը ձախողվել: Այս անհանգստացած ներմուծները օգտագործվում են զոհի դեմ: Փորձերում մենք հայտնաբերում ենք, որ մեր մեթոդը բարելավում է ADDANY-ի արդյունավետությունը սպիտակ արկղի հարձակում կատարված մոտավորապես մոտավորապես 25 տոկոսով F1, իսկ ADDSENT-ը սև արկղի հարձակում 11 տոկոսով F1:', 'bs': 'Predstavljamo metodu za neprijateljsku generaciju unosa protiv modela crne kutije za čitanje razumijevanja na osnovu odgovora na pitanja. Naš pristup je sastavljen od dva koraka. Prvo, približavamo model žrtve crne kutije putem izvlačenja model a. Drugo, koristimo svoju metodu bele kutije da bi stvorili ulazne perturbacije koje uzrokuju da približni model ne uspije. Ovi uznemireni ulazi se koriste protiv žrtve. U eksperimentima nalazimo da se naš metod poboljšava na učinkovitosti napada ADDANY-bijele kutije na približnom modelu za 25% F1, i napad ADDSENT-crne kutije za 11% F1.', 'ca': "Presentam un mètode per a la generació d'entrades adversaries contra models de caixa negra per a llegir respostes a preguntes basades en la comprensió. El nostre enfocament està compost de dos passos. Primer, aproximam un model de caixa negra per víctima mitjançant l'extracció de model. Segon, utilitzem el nostre propi mètode de caixa blanca per generar perturbacions d'entrada que fan fallar el model aproximat. Aquestes entrades perturbades s'utilitzen contra la víctima. En experiments trobem que el nostre mètode millora l'eficacia d'un atac de caixa blanca ADDANY al model aproximat un 25% F1, i l'ADDSENT un atac de caixa negra un 11% F1.", 'bn': 'আমরা কালো বাক্স মডেলের বিরুদ্ধে বিরোধী ইনপুট প্রজন্মের জন্য একটি পদ্ধতি উপস্থাপন করি যা কালো প্রশ্নের উত্তর পা Our approach is composed of two steps.  First, we approximate a victim black box model via model extraction.  দ্বিতীয়, আমরা নিজেদের নিজেদের সাদা বাক্সের পদ্ধতি ব্যবহার করি ইনপুটের বিপর্যয় তৈরি করার জন্য যা প্রায় মোডেল ব্য এই বিষয়গুলো শিকারের বিরুদ্ধে ব্যবহার করা হয়েছে। পরীক্ষার মধ্যে আমরা দেখতে পাচ্ছি যে আমাদের পদ্ধতি সাদা বাক্সের কার্যক্রমে উন্নতি পাওয়া যায়, যা প্রায় ২৫% এফ১-এর মোডেলে এক সাদা বাক্স আক্রমণ করা হয়েছে, আর এডিসে', 'et': 'Esitleme kontrastaarse sisendi genereerimise meetodit musta kasti mudelite vastu lugemispõhiseks küsimustele vastamiseks. Meie lähenemisviis koosneb kahest etapist. Esiteks lähendame ohvri musta kasti mudelit mudeli väljavõtmise kaudu. Teiseks kasutame oma valge kasti meetodit sisendhäirete genereerimiseks, mis põhjustavad ligikaudse mudeli ebaõnnestumist. Neid häirunud sisendeid kasutatakse ohvri vastu. Eksperimentides leiame, et meie meetod parandab ligikaudsel mudelil tehtud valge kasti rünnaku ADDANY efektiivsust 25% F1 võrra ja musta kasti rünnaku ADDSENT 11% F1 võrra.', 'cs': 'Představujeme metodu pro generování negativních vstupů proti modelům černé skříňky pro zodpovězení otázek založených na porozumění čtení. Náš přístup se skládá ze dvou kroků. Nejprve přibližujeme model černé skříňky oběti pomocí extrakce modelu. Za druhé používáme vlastní metodu white box k generování vstupních poruch, které způsobují selhání přibližného modelu. Tyto narušené vstupy jsou použity proti oběti. V experimentech zjišťujeme, že naše metoda zlepšuje účinnost ADDANY – útoku bílé skříňky – provedeného na přibližném modelu o 25% F1 a útoku ADDSENT – útoku černé skříňky o 11% F1.', 'fi': 'Esitämme menetelmän kontrastiiviseen syötteen tuottamiseen mustan laatikon malleihin lukemisen ymmärtämiseen perustuvaan kysymykseen vastaamiseen. Lähestymistapamme koostuu kahdesta vaiheesta. Ensin arvioimme uhrin mustan laatikon mallin mallin avulla. Toiseksi käytämme omaa white box -menetelmää luodaksemme syötteen häiriöitä, jotka aiheuttavat likimääräisen mallin epäonnistumisen. Näitä häiriintyneitä syötteitä käytetään uhria vastaan. Kokeissa havaitsemme, että menetelmämme parantaa ADDANY-valkoisen laatikon hyökkäystä, joka suoritetaan likimääräisellä mallilla 25% F1, ja ADDSENT-hyökkäystä, joka on mustan laatikon hyökkäystä, 11% F1.', 'az': 'Biz düşmənçilik giriş nəsillərinin qara qutusu modellərinə qarşı bir yol göstəririk ki, anlama tabanlı sual cavabı oxuyaraq. Bizim yaxınlığımız iki adımdan olub. İlk dəfə, modeli çıxartmaq vasitəsilə bir qurban siyah qutusu modelini yaxınlaşdırırıq. İkincisi, biz özümüzün ağ qutusu metodlarını istifadə edirik ki, yaxın modellərin başarısız olaraq giriş perturbasyonu yaratmaq üçün istifadə edirik. Bu müxtəlif inputlər qurbana qarşı istifadə edilir. İşlemlərdə bizim metodumuzun ADDANY-nin a ğ qutusu saldırılışını 25% F1 modeli ilə yaxınlaşdırılmış və ADDSENT saldırılış-qara qutusu saldırılışını 11% F1 ilə yaxşılaşdırır.', 'am': 'የጥቁር ቦስል ምሳሌዎችን ለማንበብ ጥቁር ጥቁር ጥያቄን ለማንበብ ጥያቄ መልስ እናቀርባለን፡፡ አካሄዳችን በሁለት ደረጃዎች ውስጥ ነው፡፡ መጀመሪያ፣ የጥቁር ቦታ model በሞዴል ውጤት እናሳድጋለን፡፡ በሁለተኛው፣ ነጭ አቁራጮችን ማድረግ እናስቀምጣለን፡፡ እነዚህም ተጨማሪ የውጤቶች በተጨማሪው ላይ ይደረጋሉ፡፡ በተፈተና ውስጥ የADDANY አካባቢ አካባቢ አካባቢ በ25% F1፣ የADDSENT መቅሠፍት 11% F1 በጥቁር ቁጥጥር ላይ የተመሳሰለ አካባቢ አካባቢ መሆኑን እናገኛለን፡፡', 'sk': 'Predstavljena je metoda za kontradikcijsko generacijo vhodnih vhodov v primerjavi z modeli črnih škatlic za branje na podlagi razumevanja vprašanj. Naš pristop je sestavljen iz dveh korakov. Najprej približujemo model črne škatle žrtve z odstranitvijo modela. Drugič, uporabljamo lastno metodo bele škatle za ustvarjanje vhodnih motenj, ki povzročijo neuspeh približnega modela. Te motene vnose se uporabljajo proti žrtvi. V poskusih smo ugotovili, da naša metoda izboljša učinkovitost ADDANY-napada bele škatle-izvedenega na približnem modelu za 25% F1, napada ADDSENT-napada črne škatle-pa za 11% F1.', 'ha': "Tuna gabatar da wata hanyoyi wa kizata masu motsi da za'a iya motsa masu motsi da misãlai masu haske wa karãtun matsayin a bincike da tambayi mai tambaya. Ga hanyõyinMu ta haɗi hanyõyi biyu. Kayyan da, za'a sami misalin akwatin haske mai haske da misalin ayuka. Kijan da, za mu yi amfani da hanyoyinmu masu buɗe wa matsayin akwatin bayani, don ka sami surori masu cikin shirin ayuka da ke kusa ya fizge. Wannan matsayin da aka yi amfani da shi a kan matalauci. Aka cikin jarrabai, za mu sãmi hanyoyinmu yana ƙaranci game da Eficakin ADDANY-wata kashi mai fari da aka samar da shi a kan motsi taki 25% F1, da shirin ADDSANT-da wani aikin kashi mai baƙi da 11% F1.", 'jv': 'text-tool-action Ndheke nambah iki sak diolah Awak dhéwé, awak dhéwé butuh-butuh nganggo model model dar akeh-butuh Second, we use the same brush method to create input pertruations that make the about model to failed. Mbak kuwi arah-arah sing digawe ngomong kebonah. Awak dhéwé éntukno karo hal-hal ngerasakno ning acara dadi adikuné nggawe barang kelas barang kanggo basa gampang saben nggo model sing katêpakan karo F1, lan atake ANDESSENT kanggo atake batik saben 11% F1.', 'bo': 'ང་ཚོས་ཤུལ་མཁན་གྱི་འཇུག་སྣོད་ཀྱི་ཐབས་ལམ་ལ་བསམ་བློ་གཏོང་གི་ཐབས་ལམ་ཞིག་གནད་དོན་ནག་པོ ང་ཚོའི་ཕྱོགས་གཟུགས་འདི་རྨང་གཉིས་ཀྱི་ནང་དུ་ཡོད། དང་པོ་ན། ང་ཚོས་ཐུག་མའི་མིག་དཔེ་དབྱིབས་བསྐྲུན་བྱས་པ་ཞིག་དང་། གཉིས་པ། ང་ཚོས་རང་གི་ཕྱོགས་གཙང་པོ་བེད་སྤྱོད་ཀྱིས་ནང་འཇུག་གི་བརྒྱུད་རྐྱེན་ཅིག་བཟོ་བྱེད་ཀྱི་ཡོད། འཕགས་རིས་ཀྱི་འགྲུལ་བཙུགས་འདི་ཚོར་བ་ཐུབ་མཁན་གྱི་ཉེན་ཁ་ཕྱེ་བ་རེད། In experiments we find that our method improves on the efficacy of the ADDANY-a white box attack-performed on the approximate model by 25% F1, and the ADDSENT attack-a black box attack-by 11% F1.', 'he': 'אנחנו מציגים שיטה לדור כניסה יריבית נגד דוגמני קופסה שחורה לקרוא תשובות על שאלות מבוססות על הבנה. Our approach is composed of two steps.  ראשית, אנחנו מתקרבים לדוגמא של הקורבן בקופסה שחורה באמצעות חיפוש דוגמא. שנית, אנחנו משתמשים בשיטת הקופסה הלבנה שלנו כדי ליצור הפרעות כניסה שגורמות לדוגמא הקרובה להיכשל. These perturbed inputs are used against the victim.  בניסויים אנו מוצאים שהשיטה שלנו משתפרת על היעילות של ADDANY-התקפה קופסה לבנה ביצעה על המודל בערך 25% F1, והADDSENT התקפה קופסה שחורה על ידי 11% F1.'}
{'en': 'The elephant in the interpretability room : Why use attention as explanation when we have  saliency methods ?', 'ar': 'الفيل في غرفة التفسير: لماذا نستخدم الانتباه كتفسير عندما تكون لدينا طرق التميز؟', 'fr': "L'éléphant dans la salle d'interprétabilité\xa0: Pourquoi utiliser l'attention comme explication lorsque nous avons des méthodes de saillance\xa0?", 'pt': 'O elefante na sala de interpretabilidade: por que usar a atenção como explicação quando temos métodos de saliência?', 'ja': '解釈可能性室の象：なぜ注目を説明として使うのですか？', 'zh': '可解释性房中象:当我有显著性法,何用注意为解?', 'hi': 'Interpretability कमरे में हाथी: क्यों स्पष्टीकरण के रूप में ध्यान का उपयोग करें जब हम saliency तरीके है?', 'ru': 'Слон в комнате интерпретируемости: Зачем использовать внимание в качестве объяснения, когда у нас есть методы солености?', 'ga': 'An eilifint sa seomra léirmhínithe: Cén fáth a n-úsáidtear aird mar mhíniú nuair a bhíonn modhanna sásúla againn?', 'es': 'El elefante en la sala de interpretabilidad: ¿Por qué usar la atención como explicación cuando tenemos métodos de prominencia?', 'ka': 'ჟლჲნყრ გ რყპჟოჲლკსგაღთწ ჟრაწრ: ჱაღჲ გნთმაგაირვ გნთმანთვ კარჲ ჲბწჟნჲჟრ, კჲდარჲ თმამვ ჟლჲნჲჟრ?', 'hu': 'Az elefánt az értelmezhetőség teremben: Miért használjuk a figyelmet magyarázatnak, ha vannak nyálas módszereink?', 'el': 'Ο ελέφαντας στο δωμάτιο ερμηνείας: Γιατί να χρησιμοποιήσουμε την προσοχή ως εξήγηση όταν έχουμε μεθόδους διαφάνειας;', 'it': "L'elefante nella stanza dell'interpretabilità: Perché usare l'attenzione come spiegazione quando abbiamo metodi salienti?", 'lt': 'The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?', 'mk': 'Слонот во собата за интерпретабилност: зошто да се користи вниманието како објаснување кога имаме методи на солитност?', 'kk': 'Интерпретациялық өрісіндегі тілі: Біздің қазіргі тәсілдеріміз барлық тәсілдерімізді түсініктеме ретінде қалай болады?', 'ms': 'Gajah di bilik interpretabiliti: mengapa menggunakan perhatian sebagai penjelasan apabila kita mempunyai kaedah saliency?', 'ml': 'വ്യക്തിപ്പെടുത്താനുള്ള മുറിയിലുള്ള ഹാനീന്\u200d: നമുക്ക് വിലക്കുന്ന രീതികള്\u200d കിട്ടുമ്പോള്\u200d എന്തിനാ', 'mn': 'Тодорхойлж чадах өрөөнд загас: Яагаад анхаарал таталцлын арга баригдах вэ?', 'no': 'Elefant i tolkingslommet: Hvorfor bruk oppmerksomheten som forklaring når vi har saliensmetodar?', 'pl': 'Słoń w pokoju interpretacji: Dlaczego używać uwagi jako wyjaśnienia, skoro mamy metody wyraźności?', 'ro': 'Elefantul din camera de interpretare: De ce să folosim atenția ca explicație atunci când avem metode de saliență?', 'sr': 'Slon u sobi za interpretaciju: Zašto koristite pažnju kao objašnjenje kada imamo metode salicije?', 'mt': 'Il-elefant fil-kamra tal-interpretabbiltà: Għaliex tuża l-attenzjoni bħala spjegazzjoni meta għandna metodi ta’ salinċja?', 'si': 'ඇයි අපිට ප්\u200dරශ්නයක් තියෙන විදිහට අවධානය කරන්නේ?', 'so': 'Temelegga qolka turjubaanka ku yaala: Maxaa u isticmaalaya inaad turjubaan u noqotid markaynu heysanno qaababka iibsashada?', 'sv': 'Elefanten i tolkningsrummet: Varför använda uppmärksamhet som förklaring när vi har saliency metoder?', 'ta': 'விளக்கம் அறையில் இருக்கும் elephant: விற்பனை முறைகள் இருக்கும்போது ஏன் கவனத்தை விளக்கமாக பயன்படுத்துகிறது?', 'ur': 'تعبیر قابل تعبیر اتاق میں ہاتھی: کیوں توجہ کو واضح کے طور پر استعمال کرتا ہے جب ہمارے پاس سائل طریقے ہیں؟', 'uz': "Faylning tarjima xonada: Bizda sotuvchi metodalar bo'lganda, nima uchun foydalanish mumkin?", 'vi': 'Con voi trong phòng giải thích: Sao lại dùng sự chú ý làm lời giải thích khi chúng ta có phương pháp nổi tiếng?', 'bg': 'Слонът в стаята за интерпретация: Защо да използваме вниманието като обяснение, когато имаме методи за изясняване?', 'nl': 'De olifant in de interpretatiekamer: Waarom aandacht gebruiken als uitleg als we salency methodes hebben?', 'da': 'Elefanten i fortolkningsrummet: Hvorfor bruge opmærksomhed som forklaring, når vi har fremhævede metoder?', 'hr': 'Slon u sobi za interpretaciju: Zašto koristite pažnju kao objašnjenje kada imamo metode salicije?', 'id': 'Gajah di ruang interpretabilitas: mengapa menggunakan perhatian sebagai penjelasan ketika kita memiliki metode saliency?', 'de': 'Der Elefant im Deutbarkeitsraum: Warum Aufmerksamkeit als Erklärung verwenden, wenn wir Salinzmethoden haben?', 'ko': '해석 가능한 방 안의 코끼리: 우리가 현저한 방법이 있을 때 왜 주의력을 해석해야 합니까?', 'tr': 'Ta첵첵arlandyrylyk otagynda fil: N채me 체챌in saglyk y철ntemimiz bardygynda 체ns berip d체힊체ndirmek 체챌in ulan첵arsy흫yz?', 'sw': 'Tembo katika chumba cha tafsiri: Kwa nini kutumia usimamo kama tafsiri wakati tuna njia za usambazaji?', 'fa': 'فیل در اتاق تفسیر قابلیت: چرا توجه را به عنوان توضیح استفاده کنید وقتی روش تفسیر داریم؟', 'af': 'Die elefant in die uitleggingskamer: Waarom gebruik aandag as uitduidelik wanneer ons saliensmetodes het?', 'sq': 'Elefanti në dhomën e interpretueshmërisë: Pse përdorni vëmendjen si shpjegim kur kemi metoda saliency?', 'hy': 'Ինչու՞ օգտագործել ուշադրությունը որպես բացատրություն, երբ մենք ունենք արտաքին մեթոդներ:', 'az': 'Tərəfsizlik odasındaki fil: Nə üçün təsirlik metodları olduğumuz zaman dikkatimizi açıqlama kimi istifadə edirsiniz?', 'am': 'ፈረስ በሚተርጉም ጓዳው ውስጥ ነው፤ የሽቱ ልማድ ስላለን ስለ ምን ትምህርት ትጠይቃላችሁ?', 'bn': 'ব্যাখ্যাত রুমের হাতিরা কেন মনোযোগ দিচ্ছেন যখন আমাদের কাছে ব্যয়ের পদ্ধতি আছে?', 'bs': 'Slon u sobi za interpretaciju: Zašto koristite pažnju kao objašnjenje kada imamo metode salicije?', 'ca': "L'elefant de la sala d'interpretació: Per què utilitzar l'atenció com a explicació quan tenim mètodes de saliència?", 'cs': 'Slon v interpretační místnosti: Proč používat pozornost jako vysvětlení, když máme metody významnosti?', 'et': 'Elevant tõlgendatavuse ruumis: Miks kasutada tähelepanu seletusena, kui meil on silmapaistvad meetodid?', 'fi': 'Norsu tulkittavuushuoneessa: Miksi kÃĪyttÃĪÃĪ huomiota selityksenÃĪ, kun meillÃĪ on nÃĪkyviÃĪ menetelmiÃĪ?', 'jv': 'Ero-eleman nganggo kuwi kapan kanggo ngerasara: lah piye ngerasara dadi kapan awak dhéwé nduwé ngerasara winih?', 'sk': 'Slon v sobi za razlaganje: Zakaj uporabljati pozornost kot pojasnilo, če imamo posebne metode?', 'ha': 'Temani cikin wuri mai fassarawa: Don me ka yi amfani da amfani da amfani kamar fassarar idan ana da hanyoyin salon?', 'he': 'הפיל בחדר הפרשנות: למה להשתמש תשומת לב כהסבר כאשר יש לנו שיטות מלחיות?', 'bo': 'སྔོན་ཕྲུག་གི་དོ་སྐབས་འཕགས་ཐབས་ཁང་ནང་གི་ནང་དུ་རླངས་པ་: ང་ཚོའི་ནང་དུ་གསལ་བཤད་ཞིག་ཡོད་དུས་ང་ཚོར'}
{'en': 'There is a recent surge of interest in using  attention  as explanation of model predictions, with mixed evidence on whether  attention  can be used as such. While  attention  conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use  attention , despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.', 'ar': 'هناك زيادة في الاهتمام مؤخرًا باستخدام الانتباه كتفسير لتنبؤات النماذج ، مع وجود أدلة مختلطة حول ما إذا كان يمكن استخدام الانتباه على هذا النحو. في حين أن الانتباه يمنحنا بشكل ملائم وزناً واحداً لكل رمز إدخال ويمكن استخراجه بسهولة ، فغالبًا ما يكون غير واضح تجاه الهدف الذي يتم استخدامه كتفسير. نجد في كثير من الأحيان أن هذا الهدف ، سواء أكان مذكورًا صراحة أم لا ، هو معرفة الرموز المميزة للإدخال الأكثر صلة بالتنبؤ ، وأن المستخدم الضمني للتفسير هو مطور نموذج. بالنسبة لهذا الهدف والمستخدم ، نجادل بأن طرق بروز المدخلات هي الأنسب بشكل أفضل ، وأنه لا توجد أسباب مقنعة لاستخدام الانتباه ، على الرغم من المصادفة أنها توفر وزنًا لكل إدخال. من خلال ورقة الموقف هذه ، نأمل في تحويل بعض التركيز مؤخرًا على الاهتمام بأساليب البروز ، وعلى المؤلفين أن يذكروا بوضوح الهدف والمستخدم لتفسيراتهم.', 'es': 'Hay un reciente aumento de interés en usar la atención como explicación de las predicciones de los modelos, con pruebas contradictorias sobre si la atención puede usarse como tal. Si bien la atención nos da convenientemente un peso por token de entrada y se extrae fácilmente, a menudo no está claro hacia qué objetivo se usa como explicación. Descubrimos que a menudo ese objetivo, ya sea explícito o no, es averiguar qué tokens de entrada son los más relevantes para una predicción, y que el usuario implícito para la explicación es un desarrollador de modelos. Para este objetivo y usuario, argumentamos que los métodos de prominencia de entrada son más adecuados y que no hay razones convincentes para usar la atención, a pesar de la coincidencia de que proporcionan un peso para cada entrada. Con este documento de posición, esperamos cambiar parte del enfoque reciente en la atención a los métodos de prominencia, y que los autores establezcan claramente el objetivo y el usuario de sus explicaciones.', 'pt': 'Há uma onda recente de interesse em usar a atenção como explicação das previsões do modelo, com evidências mistas sobre se a atenção pode ser usada como tal. Embora a atenção convenientemente nos dê um peso por token de entrada e seja facilmente extraída, muitas vezes não está claro para qual objetivo é usado como explicação. Descobrimos que muitas vezes esse objetivo, explicitamente declarado ou não, é descobrir quais tokens de entrada são os mais relevantes para uma previsão e que o usuário implícito para a explicação é um desenvolvedor de modelo. Para esse objetivo e usuário, argumentamos que os métodos de saliência de entrada são mais adequados e que não há razões convincentes para usar a atenção, apesar da coincidência de fornecer um peso para cada entrada. Com este documento de posicionamento, esperamos mudar um pouco do foco recente na atenção aos métodos de saliência e que os autores declarem claramente o objetivo e o usuário de suas explicações.', 'fr': "Il y a eu récemment un regain d'intérêt pour l'utilisation de l'attention comme explication des prévisions du modèle, avec des preuves mitigées quant à savoir si l'attention peut être utilisée comme telle. Bien que l'attention nous donne facilement un poids par jeton d'entrée et qu'elle soit facilement extraite, il n'est souvent pas clair dans quel but elle est utilisée comme explication. Nous constatons que souvent cet objectif, qu'il soit explicitement énoncé ou non, est de déterminer quels jetons d'entrée sont les plus pertinents pour une prédiction, et que l'utilisateur implicite de l'explication est un développeur de modèle. Pour cet objectif et cet utilisateur, nous soutenons que les méthodes de prépondérance des entrées sont mieux adaptées et qu'il n'existe aucune raison impérieuse d'utiliser l'attention, malgré le fait qu'elles fournissent un poids pour chaque entrée. Avec ce document de position, nous espérons déplacer l'attention accordée récemment aux méthodes de mise en évidence et permettre aux auteurs d'indiquer clairement l'objectif et l'utilisateur de leurs explications.", 'ja': '最近、モデル予測の説明として注意を使用することへの関心が高まっており、注意をそのまま使用できるかどうかについての証拠が混在しています。注意は、入力トークンごとに1つの重みを便利に与え、容易に抽出されますが、説明としてどのような目標に向かって使用されるかはしばしば不明です。明示的に述べられているかどうかに関わらず、その目標は、どの入力トークンが予測に最も関連しているかを見つけることであり、説明のための暗黙のユーザーはモデル開発者であることがよくわかります。この目標とユーザーについて、私たちは、インプットの優位性の方法がより適しており、各インプットに重みを提供するという偶然にもかかわらず、注意を使用する説得力のある理由はないと主張します。このポジションペーパーでは、最近の注目点のいくつかを顕著性の方法に移し、著者がその説明のために目標とユーザーを明確に述べることを望んでいます。', 'zh': '近者,人以意力为模范之说激增,意力可否之验参半。 虽注意便宜,每输令牌一权重,甚易提取,然常不详其为解说也。 吾见明言,常所输令牌与占最相关,而说隐用户为模开发人员。 其于趋用户,臣等以为输入显著性法尤宜,且无服力之用,虽为每输权重。 因此立场,愿移近关注点于显著性术,使明其指归用户。', 'hi': 'मॉडल भविष्यवाणियों के स्पष्टीकरण के रूप में ध्यान का उपयोग करने में रुचि की एक हालिया वृद्धि हुई है, इस पर मिश्रित सबूत के साथ कि क्या ध्यान का उपयोग इस तरह किया जा सकता है। जबकि ध्यान आसानी से हमें प्रति इनपुट टोकन एक वजन देता है और आसानी से निकाला जाता है, यह अक्सर स्पष्ट नहीं होता है कि इसका उपयोग स्पष्टीकरण के रूप में किस लक्ष्य की ओर किया जाता है। हम पाते हैं कि अक्सर वह लक्ष्य, चाहे स्पष्ट रूप से कहा गया हो या नहीं, यह पता लगाना है कि इनपुट टोकन भविष्यवाणी के लिए सबसे अधिक प्रासंगिक हैं, और स्पष्टीकरण के लिए निहित उपयोगकर्ता एक मॉडल डेवलपर है। इस लक्ष्य और उपयोगकर्ता के लिए, हम तर्क देते हैं कि इनपुट सैलिएंसी विधियां बेहतर अनुकूल हैं, और यह कि ध्यान का उपयोग करने के लिए कोई सम्मोहक कारण नहीं हैं, इस संयोग के बावजूद कि यह प्रत्येक इनपुट के लिए वजन प्रदान करता है। इस स्थिति के पेपर के साथ, हम हाल ही में कुछ ध्यान केंद्रित करने की उम्मीद करते हैं, और लेखकों के लिए अपने स्पष्टीकरण के लिए लक्ष्य और उपयोगकर्ता को स्पष्ट रूप से बताने के लिए।', 'ru': 'В последнее время наблюдается всплеск интереса к использованию внимания в качестве объяснения модельных прогнозов, при этом имеются неоднозначные данные о том, можно ли использовать внимание как таковое. Хотя внимание удобно дает нам один вес на входной токен и легко извлекается, часто неясно, к какой цели оно используется в качестве объяснения. Мы находим, что часто эта цель, явно указанная или нет, заключается в том, чтобы выяснить, какие входные токены являются наиболее релевантными для прогнозирования, и что подразумеваемый пользователь для объяснения является разработчиком модели. Для этой цели и пользователя мы утверждаем, что методы входной значимости лучше подходят, и что нет убедительных причин использовать внимание, несмотря на совпадение, что оно обеспечивает вес для каждого входа. С помощью этого позиционного документа мы надеемся переключить внимание на методы солености, а также на то, чтобы авторы четко указали цель и пользователя для своих объяснений.', 'ga': 'Tá borradh spéise le déanaí in úsáid aird mar mhíniú ar thuar na samhla, le fianaise mheasctha maidir le cibé an féidir aird a úsáid mar sin. Cé go dtugann aird áisiúil dúinn meáchan amháin in aghaidh an chomhartha ionchuir agus go mbaintear amach go héasca é, is minic nach mbíonn sé soiléir cén sprioc a úsáidtear é mar mhíniú. Faighimid amach go minic gurb é an sprioc sin, cibé acu a luaitear go sainráite nó nach ea, ná a fháil amach cad iad na comharthaí ionchuir is ábhartha do thuar, agus gurb é an t-úsáideoir intuigthe don mhíniú ná forbróir samhail. Maidir leis an sprioc seo agus leis an úsáideoir seo, áitímid go bhfuil modhanna sásúla ionchuir níos oiriúnaí, agus nach bhfuil aon chúiseanna láidre ann chun aird a úsáid, in ainneoin an chomhtharlú go soláthraíonn sé meáchan do gach ionchur. Leis an bpáipéar seasaimh seo, tá súil againn cuid den fhócas le déanaí ar an aird a dhíriú ar mhodhanna sábháltachta a athrú, agus go mbeidh ar na húdair an sprioc agus an t-úsáideoir lena mínithe a lua go soiléir.', 'hu': 'A közelmúltban növekedett az érdeklődés, hogy a figyelmet a modellek előrejelzéseinek magyarázataként használják fel, vegyes bizonyítékokkal arra, hogy a figyelmet lehet-e használni mint ilyen. Bár a figyelem kényelmesen megad egy súlyt beviteli tokenként és könnyen kivonható, gyakran nem világos, hogy milyen célt használják magyarázatként. Gyakran tapasztaljuk, hogy ez a cél, akár kifejezetten kifejtve, akár nem, az, hogy kiderítsük, mely beviteli tokenek a legrelevánsabbak egy előrejelzés szempontjából, és hogy a magyarázat implicit felhasználója modellfejlesztő. Ehhez a célhoz és a felhasználóhoz azt állítjuk, hogy a bemeneti kiemelkedő módszerek jobban megfelelnek, és nincs kényszerítő ok arra, hogy felhasználjuk a figyelmet, annak ellenére, hogy véletlen, hogy minden bemenet súlyát biztosít. Ezzel az állásponttal reméljük, hogy a közelmúltban a figyelmet a kiemelkedő módszerekre helyezzük át, és hogy a szerzők egyértelműen meghatározzák magyarázataik célját és felhasználóját.', 'el': 'Υπάρχει μια πρόσφατη αύξηση του ενδιαφέροντος για τη χρήση της προσοχής ως εξήγηση των προβλέψεων μοντέλων, με μικτές ενδείξεις για το αν η προσοχή μπορεί να χρησιμοποιηθεί ως τέτοια. Ενώ η προσοχή μας δίνει βολικά ένα βάρος ανά σήμα εισόδου και εξάγεται εύκολα, συχνά δεν είναι σαφές προς το σκοπό που χρησιμοποιείται ως εξήγηση. Διαπιστώνουμε ότι συχνά αυτός ο στόχος, είτε ρητά αναφέρεται είτε όχι, είναι να βρούμε ποια σήματα εισόδου είναι τα πιο συναφή για μια πρόβλεψη, και ότι ο υπονοούμενος χρήστης για την εξήγηση είναι ένας προγραμματιστής μοντέλου. Για αυτόν τον στόχο και χρήστη, υποστηρίζουμε ότι οι μέθοδοι διαφάνειας εισόδου είναι πιο κατάλληλες, και ότι δεν υπάρχουν επιτακτικοί λόγοι για να χρησιμοποιήσετε την προσοχή, παρά τη σύμπτωση ότι παρέχει ένα βάρος για κάθε εισαγωγή. Με αυτό το έγγραφο θέσης, ελπίζουμε να μετατοπίσουμε κάποια από τα πρόσφατα σημεία εστίασης στην προσοχή στις μεθόδους διαφάνειας, και οι συγγραφείς να δηλώσουν σαφώς τον στόχο και τον χρήστη για τις εξηγήσεις τους.', 'ka': 'მოდელური წარმოდგენების გამოყენებას, როგორც მოდელური წარმოდგენების გამოყენებას, მსგავსი წარმოდგენების გამოყენება თუ არა შეიძლება გამოყენება. მაგრამ დაახლოებით საუკეთესოდ გვაქვს ერთი სიმაღლე პირველ ჩანაწერა და ადვილურად გამოყენება, ეს ხოლოდ არაფერი იქნება რა მიზეზით გამოყენება როგორც განახლო ჩვენ ვფიქრობთ, რომ ზოგიერთად ეს მიზეზი, თუ არა განსაკუთრებულია თუ არა, არის გავიგოთ რა მონაცემები უფრო მნიშვნელოვანია პროგრამებისთვის, და რომ მონაცემების მომხმარებელი მოდილე ამ მიზეზისთვის და გამოყენებელისთვის, ჩვენ არსებობთ, რომ ჩემდინარე სილექციის მეტოვები უფრო მსგავსია და რომ არ არსებობს ძალიან წარმოადგენებელი მიზეზები გამოყენებას, თუმცა ამ პოზიციის დავალებით, ჩვენ გვემედით გადავიცვლოთ რამდენიმე ახალგაზრულების განახლებისთვის დაახლოების მეტოვებისთვის, და ავტორებისთვის დაახლოებით მიზეზი და გამოყენ', 'it': "C'è un recente aumento di interesse nell'usare l'attenzione come spiegazione delle previsioni dei modelli, con prove contrastanti sul fatto che l'attenzione possa essere utilizzata come tale. Mentre l'attenzione ci dà convenientemente un peso per token di input ed è facilmente estratta, spesso non è chiaro verso quale obiettivo viene utilizzato come spiegazione. Scopriamo che spesso quell'obiettivo, esplicitamente dichiarato o meno, è quello di scoprire quali token di input sono i più rilevanti per una previsione, e che l'utente implicito per la spiegazione è uno sviluppatore di modelli. Per questo obiettivo e utente, sosteniamo che i metodi di saliency degli input sono più adatti, e che non ci sono motivi convincenti per utilizzare l'attenzione, nonostante la coincidenza che fornisce un peso per ogni input. Con questo documento di posizione, speriamo di spostare parte della recente attenzione ai metodi salienti, e che gli autori indichino chiaramente l'obiettivo e l'utente per le loro spiegazioni.", 'lt': 'Pastaruoju metu susidomėjo, kad dėmesys būtų naudojamas kaip modelių prognozių paaiškinimas, kartu su įvairiais įrodymais, ar dėmesį galima panaudoti kaip tokį. Nors dėmesys mums patogiai suteikia vieną svorį kiekvienam įėjimo ženklui ir lengvai jį ištraukti, dažnai neaišku, kokiam tikslui jis naudojamas kaip paaiškinimas. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer.  Šiam tikslui ir naudotojui mes teigiame, kad įvesties druskos metodai yra geriau pritaikyti ir kad nėra įtikinamų priežasčių skirti dėmesio, nepaisant sutapimo, kad jis suteikia kiekvienam įvesties svorį. Šiame pozicijos dokumente tikimės, kad pastaruoju metu dėmesys bus sutelktas į druskos metodus ir kad autoriai aiškiai nurodytų tikslą ir naudotoją savo paaiškinimams.', 'mk': 'Постои неодамнешно зголемување интерес за користење на вниманието како објаснување на моделните предвидувања, со мешани докази за тоа дали вниманието може да се користи како такво. И покрај тоа што вниманието комфортно ни дава една тежина за секој влезен знак и е лесно извадено, честопати не е јасно кон која цел се користи како објаснување. Откриваме дека често таа цел, експлицитно објаснета или не, е да дознаеме кои влезни знаци се најрелевантни за предвидување, и дека имплицираниот корисник за објаснувањето е развивач на модел. За оваа цел и корисникот, тврдиме дека методите на вложување на ослободување се подобро соодветни и дека нема причини да се користи вниманието, и покрај случајноста дека тоа обезбедува тежина за секој влог. Со оваа позициска хартија, се надеваме дека ќе го смениме неодамнешниот фокус на вниманието на методите на ослободување, а авторите јасно ќе ја објаснат целта и корисникот за нивните објаснувања.', 'kk': 'Бақылау үлгісін түсініктіру үшін жаңа көңіл көмектесу керек. Бұл түсінікті қолдануға болады. Бізге енгізу белгісінің бір түсініктерінде бір қатты береді және оңай түсінікті, ол қандай мақсатты түсініктеме ретінде қолданылатын жоқ. Біз бұл мақсатты, түсінікті түсініп тұрса да, келтіріс белгілерін көрсету үшін, таңдау үшін пайдаланушының үлгі жасаушысы деп ойлаймыз. Бұл мақсат мен пайдаланушы үшін, біз келтіру әдістері жақсы жұмыс істейді деп айтып тұрамыз, және бұл әрбір кіріс үшін қатты көлеміне қарсы болғанда, қайталап, қызықтық себептері Бұл позициялық қағазымен, біз жаңа бағдарламалардың бірнеше көбірек көбірек көбірек көзгерту әдістеріне, авторлардың мақсатын және пайдаланушының түсініктемелерін түсіні', 'ml': 'There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such.  സാധാരണ ശ്രദ്ധ നല്\u200dകുമ്പോള്\u200d ഞങ്ങള്\u200dക്ക് ഉള്\u200dപ്പെടുത്തുന്നതിന്റെ അടയാളങ്ങളില്\u200d ഒരു ഭാരം നല്\u200dകുന്നു. എളുപ്പമായി പുറത്തെടുക നമ്മള്\u200d കണ്ടെത്തുന്നത് എപ്പോഴും ആ ലക്ഷ്യം വ്യക്തമായി പറഞ്ഞാലും അല്ലെങ്കിലും എന്താണെന്ന് മനസ്സിലാക്കുന്നത് പ്രവചനത്തിന്റെ ഏറ്റവും പ്രധാനപ്പെട് ഈ ലക്ഷ്യത്തിനും ഉപയോക്താവിനും ഞങ്ങള്\u200d വാദിക്കുന്നു, ഇന്\u200dപുട്ടിന്\u200dറെ വില മാറ്റങ്ങള്\u200d നല്ലതാണെന്നും, ശ്രദ്ധിക്കാന്\u200d നിര്\u200dബന്ധിതമായ കാരണങ്ങള്\u200d ഇല്ല ഈ സ്ഥാനത്തിന്റെ പേപ്പറുകള്\u200d കൊണ്ട്, നമുക്ക് പ്രതീക്ഷിക്കുന്നു അവസാനത്തെ കുറച്ച് ശ്രദ്ധയോടെ മാറ്റിക്കൊണ്ടിരിക്കുന്നത്,', 'ms': 'Terdapat meningkat kepentingan baru-baru ini menggunakan perhatian sebagai penjelasan ramalan model, dengan bukti campuran sama ada perhatian boleh digunakan sebagai seperti itu. Sementara perhatian dengan selesa memberikan kita satu berat per token input dan mudah dikekstrak, ia sering tidak jelas ke arah tujuan apa ia digunakan sebagai penjelasan. Kami mendapati bahawa kebanyakan tujuan, sama ada diterangkan secara jelas atau tidak, adalah untuk mencari tahu apa token input yang paling berkaitan dengan ramalan, dan bahawa pengguna yang termasuk untuk penjelasan adalah pembangun model. Untuk tujuan dan pengguna ini, kami menyangka bahawa kaedah salinsi input lebih sesuai, dan bahawa tiada sebab yang meyakinkan untuk menggunakan perhatian, walaupun kebetulan bahawa ia menyediakan berat untuk setiap input. Dengan kertas kedudukan ini, kami berharap untuk mengubah beberapa fokus baru-baru ini pada perhatian kepada kaedah saliency, dan untuk penulis untuk menyatakan dengan jelas tujuan dan pengguna untuk penjelasan mereka.', 'mt': 'Hemm żieda reċenti ta’ interess fl-użu tal-attenzjoni bħala spjegazzjoni tat-tbassir tal-mudell, b’evidenza mħallta dwar jekk l-attenzjoni tistax tintuża bħala tali. Filwaqt li l-attenzjoni tagħtina b’mod konvenjenti piż wieħed għal kull token ta’ input u tiġi estratta faċilment, ħafna drabi mhuwiex ċar lejn liema għan jintuża bħala spjegazzjoni. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer.  Għal dan l-għan u l-utent, a ħna jargumentaw li l-metodi ta’ salinċja tal-input huma adattati aħjar, u li m’hemmx raġunijiet konvinċenti biex tintuża l-attenzjoni, minkejja l-koinċidenza li tipprovdi piż għal kull input. B’dan id-dokument ta’ pożizzjoni, nittamaw li nibdlu xi wħud mill-attenzjoni reċenti fuq il-metodi ta’ salinċja, u li l-awturi jiddikjaraw b’mod ċar l-għan u l-utent għall-ispjegazzjonijiet tagħhom.', 'pl': 'Ostatnio nastąpił wzrost zainteresowania wykorzystaniem uwagi jako wyjaśnienia przewidywań modeli, z mieszanymi dowodami na to, czy uwaga może być wykorzystana jako taka. Podczas gdy uwaga wygodnie daje nam jedną wagę na token wejściowy i jest łatwo wyodrębnić, często nie jest jasne, jaki cel jest używany jako wyjaśnienie. Stwierdzamy, że często celem, niezależnie od tego, czy jest wyraźnie określone czy nie, jest dowiedzenie się, jakie tokeny wejściowe są najbardziej istotne dla prognozy, a że domniemanym użytkownikiem wyjaśnienia jest twórca modelu. Do tego celu i użytkownika argumentujemy, że metody wyraźności wejściowej są lepiej odpowiednie i że nie ma istotnych powodów, aby wykorzystać uwagę, pomimo zbiegu okoliczności, że daje ona wagę dla każdego wejścia. Dzięki temu dokumentowi mamy nadzieję przenieść niektóre z ostatnich czasów uwagę na metody wyraźności, a autorzy jasno określą cel i użytkownika ich wyjaśnień.', 'mn': 'Саяхан анхаарлыг загварын тодорхойлолт болон анхаарлын тухай ашиглах сонирхолтой нэмэгдүүлэлт бий. Хэдийгээр анхаарал нь бидэнд орлуулах тодорхой нэг жин өгч амархан гарч ирдэг ч энэ нь ямар зорилго ашиглах вэ гэдгийг ойлгохгүй байдаг. Бид ихэвчлэн энэ зорилго, тодорхой хэлж, эсвэл биш, хариултын тодорхойлолт нь ямар байдаг вэ гэдгийг ойлгох, тодорхойлолтын хэрэглэгч загварын хөгжүүлэгч гэдгийг ойлгох юм. Энэ зорилго болон хэрэглэгчийн хувьд бид орлуулах шингэний аргыг илүү зөвхөн зөвхөн хэрэглэх шалтгаан байхгүй гэдгийг хэлэхдээ бид хэлж байна. Энэ байр суурь цаасаар бид саяхан анхаарлын төвлөрөх, зохиолчдын зорилго болон хэрэглэгчийг тодорхой тодорхойлох гэж найдаж байна.', 'no': 'Det finst ein nyleg oppløysing av interesse i å bruka oppmerksomheten som forklaring av modelleforegåver, med blandet bevis om oppmerksomheten kan brukast som slik. Mens oppmerksomheten gjev oss ein vekt per inndatatoken og er lett utpakka, er det ofte uventa mot kva mål det vert brukt som forklaring. Vi finn at ofte det målet, om det er eksplisitt oppgitt eller ikkje, er å finna ut kva inndata-teikn er det mest relevante for eit foregåve, og at det impliserte brukaren for utklaringa er ein modell utviklar. For dette målet og brukaren, argumenterer vi at inndatasaliseringsmetodar er betre passa, og at det ikkje finst nokon forskjellige grunnlag til å bruka oppmerksomhet, selv om tilfeldigheten det gjev ein vekt for kvar inndata. Med denne posisjonspapiret håper vi å flytta nokre av dei siste fokusa på oppmerksomheten til saliseringsmetodar, og for forfatarane å tydelig oppgje målet og brukaren for forklaringane sine.', 'ro': 'Există o creștere recentă a interesului în utilizarea atenției ca explicație a previziunilor modelului, cu dovezi mixte privind dacă atenția poate fi folosită ca atare. În timp ce atenția ne oferă convenabil o greutate per token de intrare și este ușor de extras, adesea este neclar spre ce scop este folosit ca explicație. Observăm că adesea acel obiectiv, indiferent dacă este explicit sau nu, este de a afla ce jetoane de intrare sunt cele mai relevante pentru o predicție, și că utilizatorul implicit pentru explicație este un dezvoltator de model. Pentru acest scop și utilizator, susținem că metodele de saliență a intrărilor sunt mai potrivite și că nu există motive convingătoare de a folosi atenția, în ciuda coincidenței că oferă o greutate pentru fiecare intrare. Cu această lucrare de poziție, sperăm să schimbăm o parte din concentrarea recentă asupra metodelor de saliență și ca autorii să afirme clar scopul și utilizatorul pentru explicațiile lor.', 'so': 'There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such.  Inta si fudud looga jeedo, waxaa ina siiya miisaan kaliya calaamada input oo si fudud looga soo bixiyo, marwalba ma oga in goalka loo isticmaalo sida fasax loo isticmaalo. Waxaynu ogaanaynaa in marar badan in goalku yahay mid si cad u muuqda iyo in kale, in uu ogaado calaamadaha injiilku ay ugu muhiimsan yihiin wax la sii sheego, iyo in isticmaalaha lagu talo galay turjumaanku uu yahay qaab horumarinta. Waayo, waxaynu ka sheekaynaynaa in qalabka kirada lagu kireeyo ay habboon yihiin, iyo in aan jirin sababo qasab ah in la isticmaalo, in kastoo ay dhacdo in ay miisaan u keento input kasta. Waxaan rajaynaynaa in aan warqaddan booska ku wareejinno qaar ka mid ah hababka badbaadada, iyo in ay qoraalayaasha si caddayn ugu caddeeyaan goalka iyo isticmaalayaashaas fasirkooda.', 'sv': 'Intresset för att använda uppmärksamheten som förklaring till modellprognoser har ökat nyligen, med blandade bevis på huruvida uppmärksamheten kan användas som sådan. Även om uppmärksamheten bekvämt ger oss en vikt per inmatning token och lätt extraheras, är det ofta oklart mot vilket mål det används som förklaring. Vi finner ofta att målet, oavsett om det uttryckligen anges eller inte, är att ta reda på vilka inmatningstecken som är mest relevanta för en förutsägelse, och att den underförstådda användaren för förklaringen är en modellutvecklare. För detta mål och användare hävdar vi att inmatningsmetoder är bättre lämpade, och att det inte finns några tvingande skäl att använda uppmärksamhet, trots slumpen att det ger en vikt för varje inmatning. Med detta positionsdokument hoppas vi kunna flytta en del av den senaste tidens fokus på fokus på saliency metoder, och för författarna att tydligt ange mål och användare för sina förklaringar.', 'sr': 'Postoji nedavni porast interesa za upotrebu pažnje kao objašnjenje modelnih predviđanja, sa mešanim dokazima o tome da li se pažnja može koristiti kao takva. Iako nam pažnja prikladno daje jednu težinu po znaku ulaska i lako se izvlači, često nije jasno prema cilju koji se koristi kao objašnjenje. Često smatramo da je taj cilj, bez objašnjenja ili ne, otkrivanje znakova ulaska najvažnija za predviđanje, i da je implicirani korisnik za objašnjenje model razvijač. Za ovaj cilj i korisnik, tvrdimo da su metode uloženja salicije bolje odgovarajuće, i da nema prisiljivih razloga da upotrebimo pažnju, uprkos slučajnosti da pruža težinu za svaki ulaz. Sa ovim novinama o poziciji, nadamo se da ćemo prebaciti neke od nedavnih fokusa na pažnju na metode salicije, i da autori jasno pokažu cilj i korisnik njihovih objašnjenja.', 'ta': 'மாதிரி எதிர்பார்ப்புகளின் விளக்கம், கலப்பு தெளிவான ஆதாரங்களுடன் கவனம் பயன்படுத்தலாமா என்பதை பயன்படுத்தி அண்மையில் வட்ட பொதுவாக கவனம் நமக்கு உள்ளீட்டு குறியீட்டிற்கு ஒரு எடையை கொடுக்கும் போது எளிதாக வெளியேற்றப்படும், பெரும்பாலும் அது  நாம் கண்டுபிடிக்கும் பொழுது அந்த இலக்கு, வெளிப்படையாக குறிப்பிட்டாலும் அல்லது இல்லையானாலும், எது உள்ளீட்டு குறியீடுகள் ஒரு முறை முன்னே இந்த குறிப்புக்கும் பயனருக்கும், உள்ளீடு விற்பனை முறைகள் சிறந்ததாக இருக்கும் என்று நாம் விவாதம் செய்கிறோம். ஒவ்வொரு உள்ளீடு With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.', 'ur': 'توجه کے مطابق مدل پیش بینی کی توضیح کے طور پر اخیر علاقه کا اضافہ ہوتا ہے، اور اس کے بارے میں مختلف دلیل ہے کہ اس طرح توجه کا استعمال کیا جاتا ہے. اگرچہ ہمیں اچھی طرح اچھی طرح توجه دیتا ہے اور آسانی طرح اٹھایا جاتا ہے، اچھی طرح اس کا مقصد کس طرح مفصل کے طور پر استعمال کیا جاتا ہے۔ ہم دیکھتے ہیں کہ اغلب یہ موقع، صریح طور پر بیان کیا گیا یا نہیں، یہ ہے کہ ایک پیش بینی کے لئے اپنا انتظام ٹوکینوں کو معلوم کرنا ہے، اور یہ کہ واضح کے لئے اپنا کارساز ایک مدل ڈوکیلور ہے. یہ مقصد اور کارساز کے لئے ہم جھگڑتے ہیں کہ انپیٹ سیلینست طریقے بہتر مناسب ہیں، اور یہ کہ توجه کے استعمال کرنے کے لئے کوئی ضروری دلیل نہیں ہے، اگرچہ یہ ہر انپیٹ کے لئے ایک وزن دیتا ہے۔ یہ موقعیت کاغذ کے ساتھ، ہم امید رکھتے ہیں کہ اگلے لوگوں میں سے کچھ موقعیت کی توضیح کے ذریعہ تغییر دیں، اور لکھنے والوں کے ذریعہ موقعیت اور کارساز کو ان کی توضیح کے لئے واضح تعریف کریں.', 'si': 'මොඩේල් ප්\u200dරශ්නයක් විස්තර කරන්න පුළුවන් කියලා, අවධානයක් විස්තර කරන්න පුළුවන් කියලා ප්\u200dරශ්නයක් තියෙනවා  අවධානය සාමාන්\u200dයයෙන් අපිට ඇතුළත් ටොක්න් එක්ක බලයක් දෙන්න පුළුවන් වුනත්, ඒක ලේසියෙන් පිළිගන්න පුළුවන් ව අපිට හොයාගන්න පුළුවන් ඒ අරක්\u200dෂාව, පැහැදිලිවම කියලා නැත්නම්, ඇතුළු ටොකන්ස් මොකක්ද කියලා හොයාගන්න පුළුවන් වෙන්නේ, ඒ මේ අරමුණ සහ ප්\u200dරයෝජකයෙන්, අපි ප්\u200dරශ්නයක් කරනවා ඇතුල් ප්\u200dරයෝජනය විදියට හොඳයි කියලා, ඒවගේම අවධානයක් ප්\u200dරයෝජනය කරන්න ප්\u200dර මේ ස්ථානය පත්තර සමග, අපි හිතන්නේ අලුත් විදියට අවධානය විදියට අවධානය කරන්න, සහ ලේඛකයන්ට ඔවුන්ගේ ප්\u200dරකාරය සඳහා අර', 'uz': "Keyingi qiziqaruv model prediktlarini o'rganish uchun foydalanuvchi qiziqarli qiziqarli mavjud, va bir qanchalik ko'paytirish mumkin. Ko'p paytda, bizga bir qismni qo'yish imkoniyatini beradi va oddiy ajratish mumkin, ko'pincha bu qanday foydalanilgan maqola deb o'ylab kelmaydi. Biz ko'p paytda shu maqola aniqlangan yoki notoʻgʻri deb o'ylaymiz, deb hisoblash uchun qanday narsalarni aniqlash mumkin, va faqat uchun foydalanuvchi model tuzuvchidir. Bu maqsad va foydalanuvchi uchun, biz qo'shish usullari yaxshi bo'lishi mumkin, va u har bir kiritish uchun eng qanday sabablar yo'q. Bu nafaqat qogʻoz bilan, biz yaqinda bir necha foydalanuvchini foydalanish usullarini o'zgartiradimiz, va mualliflarning maqsadini va foydalanuvchilarini o'rganish uchun kerak.", 'vi': 'Có một sự chú ý gần đây về việc sử dụng sự chú ý để giải thích dự đoán của mô hình, với bằng chứng hỗn hợp về việc chú ý có thể được sử dụng như thế hay không. Trong khi sự chú ý thuận tiện cho chúng ta một lượng mỗi vật nhập biểu tượng và dễ dàng được lấy ra, nó thường không rõ mục đích nó được dùng để giải thích. Chúng tôi thấy rằng thường thì mục tiêu đó, có nói rõ hay không, là tìm ra hiệu nhập nào là liên quan nhất với dự đoán, và người dùng có ngụ ý cho lời giải thích là một nhà phát triển mô hình. Với mục tiêu này và người dùng, chúng tôi cho rằng phương pháp sinh tồn là phù hợp hơn, và không có lý do chính đáng để chú ý, mặc dù trùng hợp nó mang lại trọng lượng cho từng người nhập. Với bài báo này, chúng tôi hy vọng có thể chuyển một phần của tập trung gần đây vào các phương pháp nổi tiếng, và để các tác giả xác định rõ mục tiêu và người dùng để giải thích.', 'nl': 'Er is een recente toename van de interesse in het gebruik van aandacht als verklaring voor modelvoordelen, met gemengd bewijs over de vraag of aandacht als zodanig kan worden gebruikt. Hoewel aandacht ons gemakkelijk één gewicht per invoertoken geeft en gemakkelijk kan worden geëxtraheerd, is het vaak onduidelijk voor welk doel het wordt gebruikt als uitleg. We merken vaak dat dat doel, al dan niet expliciet vermeld, is om erachter te komen welke input tokens het meest relevant zijn voor een voorspelling, en dat de impliciete gebruiker voor de uitleg een model ontwikkelaar is. Voor dit doel en de gebruiker stellen we dat input salency methodes beter geschikt zijn, en dat er geen dwingende redenen zijn om aandacht te gebruiken, ondanks het toeval dat het een gewicht geeft voor elke input. Met dit positiedocument hopen we een deel van de recente aandacht te verschuiven naar saliency methodes, en dat auteurs duidelijk het doel en de gebruiker voor hun uitleg aangeven.', 'bg': 'Налице е скорошна вълна от използването на вниманието като обяснение на прогнозите на моделите, със смесени доказателства за това дали вниманието може да се използва като такова. Докато вниманието удобно ни дава едно тегло на входен токен и лесно се извлича, често не е ясно каква цел се използва като обяснение. Ние откриваме, че често тази цел, независимо дали изрично е посочена или не, е да разберем кои входни токени са най-подходящи за дадена прогноза и че подразбиращият се потребител за обяснението е разработчик на модели. За тази цел и потребител ние твърдим, че методите на входящата видимост са по-подходящи и че няма убедителни причини да се използва внимание, въпреки съвпадението, че тя осигурява тежест за всеки входящ материал. С тази позиция се надяваме да преместим част от скорошния фокус върху вниманието върху методите за видимост и авторите да посочат ясно целта и потребителя за техните обяснения.', 'da': 'Der er for nylig en stigning i interessen i at bruge opmærksomhed som forklaring på modelforudsigelser, med blandet dokumentation for, om opmærksomhed kan bruges som sådan. Mens opmærksomhed bekvemt giver os en vægt pr. input token og let udtrækkes, er det ofte uklart mod, hvilket mål det bruges som forklaring. Vi oplever ofte, at målet, uanset om det udtrykkeligt er angivet eller ej, er at finde ud af, hvilke input tokens der er mest relevante for en forudsigelse, og at den underforståede bruger for forklaringen er en modeludvikler. Til dette mål og bruger hævder vi, at input saliency metoder er bedre egnet, og at der ikke er nogen overbevisende grunde til at bruge opmærksomhed, på trods af tilfældigheden, at det giver en vægt for hvert input. Med dette positionsdokument håber vi at flytte noget af det seneste fokus på opmærksomhed på fremhævelsesmetoder, og at forfattere klart angiver mål og bruger for deres forklaringer.', 'de': 'Es gibt in jüngster Zeit einen Anstieg des Interesses, Aufmerksamkeit als Erklärung für Modellvorhersagen zu verwenden, mit gemischten Beweisen, ob Aufmerksamkeit als solche verwendet werden kann. Während Aufmerksamkeit uns bequem ein Gewicht pro Eingabetoken gibt und leicht extrahiert werden kann, ist es oft unklar, auf welches Ziel es als Erklärung verwendet wird. Wir stellen fest, dass dieses Ziel, ob explizit angegeben oder nicht, ist herauszufinden, welche Eingabetoken für eine Vorhersage am relevantesten sind und dass der implizierte Benutzer für die Erklärung ein Modellentwickler ist. Für dieses Ziel und den Benutzer argumentieren wir, dass Input Saliency Methoden besser geeignet sind und dass es keine zwingenden Gründe gibt, Aufmerksamkeit zu verwenden, trotz des Zufalls, dass es für jeden Input ein Gewicht gibt. Mit diesem Positionspapier hoffen wir, einen Teil der jüngsten Aufmerksamkeit auf Saliency-Methoden zu verlagern und dass Autoren das Ziel und den Nutzer ihrer Erklärungen klar angeben können.', 'id': 'Ada tumbuhan tertarik baru-baru ini untuk menggunakan perhatian sebagai penjelasan dari prediksi model, dengan bukti campuran tentang apakah perhatian dapat digunakan sebagai seperti itu. Sementara perhatian dengan nyaman memberikan kita satu berat per token masukan dan mudah diekstraksi, sering tidak jelas ke tujuan apa yang digunakan sebagai penjelasan. Kami menemukan bahwa sering tujuan tersebut, apakah secara eksplisit dinyatakan atau tidak, adalah untuk mencari tahu apa token input yang paling relevan untuk prediksi, dan bahwa pengguna implikasi untuk penjelasan adalah seorang pengembang model. Untuk tujuan dan pengguna ini, kami berdebat bahwa metode salinsi masukan lebih cocok, dan bahwa tidak ada alasan yang memaksa untuk menggunakan perhatian, meskipun kebetulan bahwa itu menyediakan berat untuk setiap masukan. Dengan kertas posisi ini, kami berharap untuk mengubah beberapa fokus baru-baru ini pada perhatian pada metode saliency, dan para penulis untuk jelas menyatakan tujuan dan pengguna untuk penjelasan mereka.', 'ko': '최근 주의력을 이용해 모델 예측을 해석하는 흥미가 급증하면서 주의력이 모델 예측으로 활용될 수 있는지에 대한 증거가 다르다.주의력은 모든 입력 표시에 권한을 부여하고 추출하기 쉽지만, 어떤 목표를 설명하는 데 쓰일지 잘 모른다.우리는 명확한 설명이 있든 없든 목표는 예측과 가장 관련된 입력 표시를 찾아내는 것이고 설명하는 은밀한 사용자는 모델 개발자라는 것을 발견했다.이 목표와 사용자에 대해 우리는 입력의 현저한 방법이 더욱 적합하다고 생각하고, 모든 입력에 권한을 제공했지만, 믿을 만한 이유가 없다.이 입장 논문을 통해 우리는 최근의 관심을 현저한 방법으로 옮기고 저자가 그들이 해석한 목표와 사용자를 명확하게 설명하기를 바란다.', 'fa': 'اخیراً برای استفاده از توجه به عنوان توضیح پیش بینی\u200cهای مدل، با مدرک مختلف در مورد اینکه آیا توجه به عنوان اینگونه استفاده می\u200cشود، علاقه\u200cای برای استفاده از توجه وجود دارد. در حالی که توجه به راحتی به ما یک وزن به هر نشان ورودی می دهد و به راحتی اخراج می شود، اغلب در مقابل چه هدف به عنوان توضیح استفاده می شود آشکار نیست. ما اغلب این هدف را می\u200cبینیم، چه مشخص و نه، اینه که بفهمیم نشانه\u200cهای ورودی برای پیش\u200cبینی چه ارتباطی\u200cترین برای پیش\u200cبینی است، و این کاربر مشخص برای توضیح یک توسعه\u200cکننده مدل است. برای این هدف و استفاده، ما بحث می\u200cکنیم که روش\u200cهای استفاده از استفاده از توجه بهتر مناسب هستند، و دلایل مجبوری برای استفاده از توجه وجود ندارد، با وجود اتفاقی که آن برای هر ورودی وزن می\u200cدهد. با این کاغذ موقعیت، امیدواریم بعضی از اخیرا روی توجه به روش سازی تغییر دهیم، و برای نویسندگان برای مشخص کردن هدف و کاربر برای توضیح\u200cهایشان نشان دهیم.', 'sw': 'Kuna ongezeko la hivi karibuni la maslahi ya kutumia maoni kama maelezo ya utabiri wa model, na ushahidi mchanganyiko juu ya ikiwa mtazamo unaweza kutumika kama vile. Wakati kwa urahisi unatupa uzito mmoja kwa ishara ya input na kwa urahisi unatolewa, mara nyingi si wazi kwa lengo gani ambalo linatumiwa kama maelezo. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer.  Kwa lengo hili na mtumiaji, tunahoji kuwa mbinu za usambazaji wa mazingira inafaa zaidi, na kwamba hakuna sababu za lazima za kutumia maoni, pamoja na hali ya kuwa inatoa uzito kwa kila input. Kwa karatasi hii ya nafasi, tunatumaini kubadilisha baadhi ya malengo ya hivi karibuni kuhusiana na njia za usambazaji, na kwa waandishi kuonyesha lengo na mtumiaji kwa maelezo yao.', 'tr': '횦akynda n채챌e 체ns beri힊i 철r채n nusga 철흫체nlerini d체힊체ndirmek 체챌in 체ns almak 체챌in gyzyklan첵an bir d철w체r bar. Dikkati gowy g철r첵채n bolsa bize giri힊 tokenasynda bir a휓캇rlyk ber첵채r we a흫sat 챌ekil첵채r, ol k철plen챌 bu maksady흫 n채hili d체힊체ndiri힊i 첵aly ulanyl첵andygyny d체힊체ndirme첵채r. Biz k철plen챌 bu maksady흫, a 챌캇k 첵a-da so흫ra belirtilmedigini, giri힊 i힊aretlerini흫 n채hili tahmin etmegi흫 i흫 m철h체mdigini tapmak bol첵arys we d체힊체ndiri힊i 체챌in ullany힊y흫 nusga d체zenleyicidir. Bu maksady we wu탑iwar 체챌in, girdi s체첵힊iklik y철ntemlerini흫 gowy dogry bolandygyny a 첵d첵arys we ol 체ns ulanmak 체챌in hi챌 hili t채sirli seb채pleri 첵ok, her girdi 체챌in bir t채sirli bolmagyna ra휓men. Bu 첵er makalesi bilen, so흫ky wagtlary흫 biraz 체ns체ni s체첵tge힊ik y철ntemlerine, awtorlary흫 maksadyny we wu탑iwarlaryny d체힊체ndirmelerini belli etmegi umyt ed첵채ris.', 'hr': 'Postoji nedavni porast interesa za upotrebu pažnje kao objašnjenje predviđanja model a, s mješenim dokazima o tome da li se pažnja može koristiti kao takva. Iako nam pozornost prikladno daje jednu težinu po znaku ulaska i lako se izvlači, često nije jasno prema cilju koji se koristi kao objašnjenje. Često smatramo da je taj cilj, objašnjenje ili ne, otkrivanje znakova ulaska najvažnija za predviđanje, i da je implicirani korisnik objašnjenja model razvijač. Za ovaj cilj i korisnik, tvrdimo da su metode uloženja salicije bolje odgovarajuće, i da nema prisiljnih razloga za upotrebu pažnje, uprkos slučajnosti da pruža težinu za svaki ulaz. S ovim novinama o poziciji, nadamo se da ćemo prebaciti neke od nedavnih fokusa na pažnju na metode salicije, i da autori jasno pokažu cilj i korisnik njihovih objašnjenja.', 'sq': 'Ka një rritje interesi të fundit në përdorimin e vëmendjes si shpjegim të parashikimeve të modelit, me prova të përziera në se vëmendja mund të përdoret si e tillë. Ndërsa vëmendja në mënyrë të përshtatshme na jep një peshë për token e hyrjes dhe është e nxjerrë lehtë, shpesh nuk është e qartë se cila qëllim përdoret si shpjegim. Ne zbulojmë se shpesh ky qëllim, qoftë shprehur saktësisht apo jo, është të zbulojmë se cilat shenjat e hyrjes janë më të rëndësishme për një parashikim dhe se përdoruesi i implikuar për shpjegimin është një zhvillues model. Për këtë qëllim dhe përdoruesin, ne argumentojmë se metodat e hyrjes së saliencës janë më të përshtatshme dhe se nuk ka arsye bindëse për të përdorur vëmendjen, pavarësisht nga rastësia se ajo ofron një peshë për çdo hyrje. Me këtë letër pozicioni, shpresojmë të lëvizim disa nga fokuset e fundit në vëmendjen ndaj metodave të shenjta dhe për autorët të shpjegojnë qartë qëllimin dhe përdoruesin për shpjegimet e tyre.', 'af': "Daar is 'n onlangse verhoeging van belang in die gebruik van aandag as uitduidelik van model voorskou, met gemengde getuienis oor of aandag so gebruik kan word. Alhoewel aandag goed ons een gewig per invoer token gee en is maklik uitgevoer, is dit dikwels onbekend teenoor watter doel dit gebruik word as uitduidelik. Ons vind dat dikwels daardie doel, of explicitly gespesifiseer of nie, is om uit te vind wat invoer tekens is die mees relevante vir 'n voorskou, en dat die impliseerde gebruiker vir die uitduideling 'n model ontwikkelaar is. Vir hierdie doel en gebruiker, ons argumenteer dat invoer saliensmetode beter geskik is, en dat daar geen verstandige rede is om aandag te gebruik nie, ten ag van die gelykheid dat dit 'n gewig vir elke invoer verskaf nie. Met hierdie posisie papier hoop ons om sommige van die onlangse fokus te verander op aandag na saliensmetodes, en vir outeurs om die doel en gebruiker duidelik te stel vir hulle uitduidelike.", 'am': 'በአሁኑ ጊዜ የጥያቄ ማስታወቂያ እንደምሳሌ ውይይት ለመጠቀም የሚጠይቅ ጥያቄ አለበት፡፡ While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation.  ብዙ ጊዜም ይህ ጉዳይ ግልፅ ቢሆን ወይም ባይሆን የመግለጫ ምልክቶች ምን እንደሆነ ለመፍጠር የሚጠያየቁ እና ለመግለጫ የሚጠይቁት ተጠቃሚ ምሳሌ አካባቢ መሆኑን እናውቃለን፡፡ ለዚህ አቃውሞ እና ተጠቃሚው እንከራከራለን፡፡ በዚህ ቦታ ካላት፣ የቀድሞው አካባቢ አንዳንዶችን አካባቢ እና መቃወሚያውን እና ተጠቃሚዎቹን ለመግለጽ ለመግለጽ ተስፋ እናደርጋለን፡፡', 'hy': 'Վերջերս հետաքրքրություն է առաջացել ուշադրությունը որպես մոդելների կանխատեսումների բացատրություն օգտագործելու մեջ, որտեղ բաժին ապացույցներ կան այն մասին, թե արդյոք ուշադրությունը կարող է օգտագործվել որպես այդպիսի: While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation.  Մենք հաճախ հայտնաբերում ենք, որ այդ նպատակը, անկախ նրանից, թե ոչ, պարզելու է, թե ինչ մուտքագրական նշաններ են ամենակարևոր կանխատեսման համար, և որ բացատրության ենթադրված օգտագործողը մոդել զարգացող է: For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input.  Այս դիրքի թղթի միջոցով մենք հույս ունենք տեղափոխել վերջին կենտրոնացումը ուշադրության վրա արտացոլության մեթոդների վրա, և հեղինակների համար պարզ նպատակը և օգտագործողը իրենց բացատրությունների համար:', 'az': 'Özünü model tədbirlərinin a çıq-aydınlıqları kimi istifadə etmək üçün çox yeni maraqlı bir təsiri var. Dikkati bizə hər tərəfdən bir ağırlığı verir və asanlıqla çıxarılır, çox zaman bu, hansı məqsəd açıq-aydın olaraq istifadə ediləcəyini bilmir. Gördük ki, bu məqsəd, a çıq-aydın belə deyil və ya yoxdur, giriş işaretlərinin bir tədbir üçün ən mövcuddur olduğunu öyrənmək və açıq-aydın üçün müəyyən edilən istifadəçinin modeli tədbir edəndir. Bu məqsəd və istifadəçi üçün, girişin salikati metodlarının daha yaxşı uyğunluğunu iddia edirik və hər girişin a ğırlığını sağlayaraq, dikkatini istifadə etmək üçün zor bir səbəb yoxdur. Bu pozisyon kağıdı ilə, az öncə bir neçə təsirlik metodlarına dikkatini dəyişdirmək istəyirik, yazıcıların məqsədilə və istifadəçilərini açıq-aydın təsdiqləməsini istəyirik.', 'bs': 'Nedavno se pojavljuje interesovanje koristeći pažnju kao objašnjenje predviđanja model a, sa mješenim dokazima o tome da li se pažnja može koristiti kao takva. Iako nam pažnja prikladno daje jednu težinu po znaku ulaska i lako se izvlači, često nije jasno prema cilju koji se koristi kao objašnjenje. Često smatramo da je taj cilj, bez objašnjenja ili ne, otkrivanje znakova ulaska najvažnija za predviđanje, i da je implicirani korisnik za objašnjenje model razvijač. Za ovaj cilj i korisnik, tvrdimo da su metode uloženja salicije bolje odgovarajuće, i da nema prisiljnih razloga da upotrebimo pažnju, uprkos slučajnosti da pruža težinu za svaki ulaz. S ovim novinama o poziciji, nadamo se da ćemo prebaciti neke od nedavnih fokusa na pažnju na metode salicije, i da autori jasno pokažu cilj i korisnik njihovih objašnjenja.', 'cs': 'V poslední době dochází k nárůstu zájmu o využití pozornosti jako vysvětlení modelových predikcí, se smíšenými důkazy o tom, zda lze pozornost využít jako takovou. Zatímco pozornost nám pohodlně dává jednu váhu na vstupní token a je snadno extrahována, často není jasné, jaký cíl se používá jako vysvětlení. Často zjišťujeme, že cílem, ať už výslovně uvedeným či nikoli, je zjistit, jaké vstupní tokeny jsou pro predikci nejvíce relevantní a že implikovaným uživatelem pro vysvětlení je vývojář modelu. Pro tento cíl a uživatele argumentujeme, že metody vstupní významnosti jsou vhodnější a že neexistují žádné přesvědčivé důvody používat pozornost, navzdory shodě okolností, že poskytuje váhu pro každý vstup. Doufáme, že s tímto postojem přesuneme některé z nedávných zaměření na metody významnosti a že autoři jasně uvádí cíl a uživatele pro jejich vysvětlení.', 'ca': "Hi ha un augment recent d'interès en utilitzar l'atenció com explicació de les prediccions models, amb evidències mixtes sobre si l'atenció pot ser utilitzada com tal. Mentre l'atenció ens dóna un pes per fitxa d'entrada i es extrau fàcilment, sovint no és clar cap a quin objectiu s'utilitza com a explicació. Trobem que sovint aquest objectiu, que sigui declarat explícitament o no, és descobrir quines fitxes d'entrada són les més rellevants per a una predicció, i que l'usuari implicat per a l'explicació és un desenvolupant model. Per aquest objectiu i l'usuari, argumentem que els mètodes de saliència d'entrada són millor adaptats, i que no hi ha raons convincents d'utilitzar l'atenció, malgrat la coincidència que proporciona un pes per cada entrada. Amb aquest paper de posició, esperem canviar la atenció recent als mètodes de saliència i que els autors indiquen clarament l'objectiu i l'usuari per les seves explicacions.", 'bn': 'মোডেল ভবিষ্যদ্বাণীর ব্যাখ্যা হিসেবে মনোযোগ ব্যবহার করার জন্য সাম্প্রতিক আগ্রহের ব্যাপার রয়েছে, যার মিশ্র প্রমাণ রয়েছ যদিও সুবিধাজনকভাবে মনোযোগ আমাদের প্রতি ইনপুটের চিহ্নের একটি ওজন দেয় এবং সহজে বের করা যায়, এটা প্রায়শই ব্যাখ্যা কি লক্ষ্য হিস আমরা প্রায়শই খুঁজে পাচ্ছি যে এই লক্ষ্য, স্পষ্ট ভাবে বলা হয় অথবা না বলা হয়, তা হলো জানতে পারে যে ইনপুটের চিহ্ন কি ভবিষ্যতের প্রতি সবচেয়ে প্রযুক্ত এবং ব্ এই লক্ষ্য এবং ব্যবহারকারীর জন্য আমরা যুক্তি দিচ্ছি যে ইনপুটের ব্যয়ের পদ্ধতি ভালোভাবে পরিমাণ এবং মনোযোগ ব্যবহারের কোন বাধ্য নেই, যদিও এই ঘটনার কোন কা এই অবস্থানের কাগজের মাধ্যমে আমরা আশা করি সাম্প্রতিক কিছু মনোযোগ পাল্টাতে পারি বিক্রেতা পদ্ধতির দিকে মনোযোগ দিয়ে, আর লেখকদের জন্য তাদ', 'et': 'Hiljuti on suurenenud huvi tähelepanu kasutamise vastu mudelite prognooside selgitusena ning erinevad tõendid selle kohta, kas tähelepanu saab sellisena kasutada. Kuigi tähelepanu annab meile mugavalt ühe kaalu sisendmärgi kohta ja seda on lihtne ekstraheerida, on sageli ebaselge, millist eesmärki seda selgitusena kasutatakse. Me leiame, et sageli on see eesmärk, olgu see sõnaselgelt välja öeldud või mitte, välja selgitada, millised sisendtokenid on prognoosi jaoks kõige asjakohasemad ja et selgituse kaudne kasutaja on mudeli arendaja. Selle eesmärgi ja kasutaja jaoks väidame, et sisendite silmapaistvuse meetodid sobivad paremini ja et tähelepanu kasutamiseks ei ole kaalukaid põhjusi, hoolimata kokkusattumusest, et see annab kaalu iga sisendi jaoks. Selle seisukoha dokumendiga loodame viia osa hiljutisest tähelepanu tähelepanu silmapaistvuse meetoditele ning autorid märgivad selgelt oma selgituste eesmärgi ja kasutaja.', 'fi': 'Kiinnostus huomion käyttämiseen malliennusteiden selityksenä on noussut hiljattain, ja siitä, voidaanko huomiota käyttää sellaisenaan, on saatu ristiriitaisia todisteita. Vaikka huomio antaa meille kätevästi yhden painon per syöttötunnus ja se on helppo purkaa, on usein epäselvää, mihin tavoitteeseen sitä käytetään selityksenä. Huomaamme, että usein tämä tavoite, riippumatta siitä, onko se nimenomaisesti mainittu tai ei, on selvittää, mitkä syöttöpoletit ovat olennaisimpia ennusteen kannalta, ja että implisiittinen käyttäjä selitykselle on mallikehittäjä. Tähän tavoitteeseen ja käyttäjään katsomme, että syötteen näkyvyysmenetelmät sopivat paremmin, eikä ole pakottavia syitä kiinnittää huomiota, vaikka se antaa painoarvon jokaiselle syötteelle. Tämän kannanottopaperin avulla toivomme, että osa viimeaikaisesta huomiosta kiinnitetään huomiota korostuneisuusmenetelmiin ja että kirjoittajat ilmoittavat selkeästi tavoitteensa ja käyttäjänsä selityksilleen.', 'jv': 'Tool Options, Layersdock Attribute Awak dhéwé éntukno karo nggalakno, dadi kesalahan apik apa or a bisa, dadi nggawe layakno karo perusahaan ingkang dipune nggo ngerasakno, lan kelompok nggawe nguasakno Awak iki nggo nambah karo pengguna-nambah, kita ngomong nik wektu nggo ngerasai winih sing luwih apik, lan ora ono wektu nggo ndelok kebebasan Awak dhéwé nggawe akeh pisan iki, kita supoyo nggawe perspekter nggawe barang nggawe barang nggawe barang nggawe barang urip, lan autor kanggo ngerasakno perusahaan bukal karo pawaran dhéwé.', 'ha': "An samu wani suri da za'a yi amfani da fassarar amfani da fassarar kamar fassarar ɗabi'ar misãlai, da bayanin da aka gaura, a kan ka yi amfani da ƙyãma kamar wannan. Waka da muhimmi yana bã mu nauyi guda ko da alama na shiga kuma yana da sauƙi, kuma bã ya kasancẽwa ga wane goal da ake amfani da shi kamar fassarar. Tuna gane cewa, ko da yawa wannan goal, ko da bayyane, za'a gane ni'anar ayukan ayukan ayuka da ke da muhimmi zuwa wani basĩri, kuma da wanda ke amfani da shi da aka buƙata wa fassarar, shi ne mai motsi. Ga wannan goa da mai amfani da shi, Munã jãyayya cẽwa metoden ayuka da za'a fito da shi mafi alhẽri, kuma bã da wani saba wanda ya lazimta wa su yi amfani da shi, kuma kõ da daidai da ya sami nauyi ga duk inputi. Daga wannan takardan wurãren kwanan, Munã kwaɗayin mu musanya wasu masu muhimmin zura kiyãye zuwa hanyoyin sali, kuma don ma'abũta su bayyana goani da misalin su.", 'he': 'יש התעניין האחרון בשימוש בתשומת לב כהסבר של חזיונות מודל, עם ראיות מעורבות על אם תשומת לב יכולה להשתמש כזו. בעוד תשומת לב נוח נותנת לנו משקל אחד לכל סימן הכניסה והוא בקלות מווצא, זה לעתים קרובות לא ברור לאיזו מטרה הוא משתמש כהסבר. אנו מוצאים לעתים קרובות שהמטרה הזו, בין אם היא מוצהירה באופן ברור או לא, היא לגלות אילו סימני הכניסה הם הכי רלוונטיים לחזוי, ושהמשתמש המרווקא להסבר הוא מתפתח מודל. למען המטרה והמשתמש הזה, אנו טוענים ששיטות מילוי הכניסה מתאימות יותר, ושאין סיבות משכנעות להשתמש בשימוש תשומת לב, למרות צירוף מקרים שהוא מספק משקל לכל הכניסה. עם נייר העמדה הזה, אנו מקווים להעביר חלק מהמרכז האחרון על תשומת לב לשיטות מלוחות, ולהסופרים להציג בבירור את המטרה והמשתמש להסברים שלהם.', 'sk': 'V zadnjem času je prišlo do povečanja zanimanja za uporabo pozornosti kot razlage napovedi modela, z mešanimi dokazi o tem, ali je pozornost mogoče uporabiti kot taka. Čeprav nam pozornost priročno daje eno težo na vhodni žeton in je enostavno ekstrahirati, pogosto ni jasno, kateri cilj se uporablja kot pojasnilo. Ugotavljamo, da je pogosto ta cilj, ne glede na to, ali je izrecno naveden ali ne, ugotoviti, kateri vhodni žetoni so najpomembnejši za napoved, in da je implicitni uporabnik razlage razvijalec modela. Za ta cilj in uporabnika trdimo, da so vhodne metode bolj primerne in da ni nujnih razlogov za uporabo pozornosti, kljub naključju, da zagotavlja težo za vsak vhod. S tem stališčem upamo, da bomo nekaj nedavnega osredotočanja preusmerili na metode izpostavljenosti in da bodo avtorji jasno navedli cilj in uporabnika svojih pojasnil.', 'bo': 'དེ་ལྟ་བུའི་མཐོང་སྣང་གསལ་བཤད་ཀྱི་རྣམ་གྲངས་དང་མཐོང་བའི་རྐྱེན་སྐྱེན་རྐྱེན་བྱས་པ་ཞིག་ཡིན། གནད་དོན་ཡོད་ཚད་ལྟར་ཞིག་ནི་འཇུག་སྣོད་ཀྱི་མིང་རྒྱ་ཚད་གཅིག་ལས་སླ་གཏོང་ནི་མེད། ང་ཚོས་རྒྱུན་དུ་དམིགས་ཡུལ་འདི་ཡང་ན་གསལ་བཤད་བྱས་མིན་ནའང་མིན་ན། འོག་གི་འགྲེལ་བཤད་ཀྱི་དཔེ་གཞི་གྲངས་ཀ་གང་ཡིན་པ་དང་གསལ་བཤད་བྱས་པའི་ For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. ང་ཚོས་གནས་ཡུལ་གྱི་ཤོག་བུ་འདི་ལ་ཉེ་ཆར་མཁན་གྱི་གནད་དོན་དག་ཕྱིར་མཐོང་ནི་ཐབས་ལམ་ལ་དང་། རྩོམ་སྒྲིག་པ་ཚོས་མི་སྟོན་པར་དམིགས་'}
{'en': 'Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation', 'es': 'Los modelos de inferencia del lenguaje natural neuronal incorporan parcialmente las teorías de la implicación léxica y la negación', 'ar': 'نماذج الاستدلال اللغوي الطبيعي العصبي تتضمن جزئيًا نظريات الاستنتاج المعجمي والنفي', 'pt': 'Modelos de Inferência de Linguagem Natural Neural Incorporam Parcialmente Teorias de Implicação e Negação Lexical', 'fr': "Les modèles d'inférence neuronale en langage naturel intègrent partiellement les théories de l'implication lexicale et de la négation", 'ja': 'ニューラル・ナチュラル・ランゲージ・インジェクション・モデルは、語尾と否定の理論を部分的に埋め込んでいる', 'zh': '神经自然语言推理模型分嵌词法蕴涵否', 'hi': 'तंत्रिका प्राकृतिक भाषा अनुमान मॉडल आंशिक रूप से लेक्सिकल Entailment और Negation के सिद्धांतों एम्बेड', 'ru': 'Нейронные модели вывода на естественном языке частично внедрены в теории лексического влечения и отрицания', 'ga': 'Samhlacha Néaracha Tátail Teanga Nádúrtha Leabú go Páirteach Teoiricí na Foclaíochta agus na hIdirghabhála', 'ka': 'ნეიროლური ნაირადი ენერგიის ინფერენციის მოდელები დაფართობით ლექსიკალური ინტელექცია და განახლების ტეორიები', 'hu': 'Neurális természetes nyelvi inferencia modellek részben beágyazzák a lexikális kiterjedés és negatív elméleteit', 'el': 'Τα μοντέλα συμπερασμάτων της Νευρικής Φυσικής Γλώσσας ενσωματώνουν εν μέρει θεωρίες της Λεξικής Ενσωμάτωσης και Αρνείας', 'kk': 'Нейралық натуралды тіл қатынасының үлгілері Лексикалық толық мен қатынасының теориясы', 'lt': 'Neuralinės natūralios kalbos Inferencijos modeliai iš dalies įterptos leksinio susirgimo ir neigiamo poveikio teorijos', 'it': "Modelli di inferenza del linguaggio naturale neurale incorporano parzialmente teorie dell'entazione lessicale e della negazione", 'ml': 'ലെക്സിക്സിക്കല്\u200d മെന്\u200dറിലും നെഗറിനും പ്രധാനപ്പെട്ട സ്വാഭാവിക ഭാഷ മോഡലുകള്\u200d', 'ms': 'Name', 'mt': 'Mudelli ta’ Inferenza tal-Lingwa Naturali Newrali Teoriji ta’ Mard u Negazzjoni Lessiċi Partialment Embed', 'mn': 'Сэтгэл байгалийн хэл хамааралтай загварууд Лексиксикийн бүтээлч болон хамааралтай теорийг хэсэг хэсэг', 'no': 'Neural Natural Language Inference Modellar Delvis Innebygd Teoriar av Lexical Entailment and Negation', 'pl': 'Modele wniosków języka naturalnego neuronalnego częściowo osadzają teorie dopracowania i negacji leksykalnej', 'ro': 'Modelele de inferență a limbajului natural neural încorporează parțial teorii ale înțelegerii lexicale și negării', 'sr': 'Modeli neurološke prirodne jezike delovično uključene teorije leksičke komplekcije i pregovore', 'si': 'න්\u200dයූරාල් ස්වභාවික භාෂාව ප්\u200dරමාණය අංශික විදිහට ඇම්බෙඩ් තියෝරිස් ලෙක්සිකාල් ඇන්තිය', 'mk': 'Неурални природни јазички инференциски модели делумно вградени теории на лексикална болест и негативност', 'so': 'Istiyaatarka afka asalka ah', 'sv': 'Neural Natural Language Inference Modeller delvis inbädda teorier om Lexical Entailment och Negation', 'ta': 'இயல்பான மொழி புகுதிய மாதிரி', 'ur': 'نئورل طبیعی زبان انفارنس موڈلز حصہ میں لکسیکل انٹیلنٹ اور ناگواری تئوری', 'vi': 'KCharselect unicode block name Lý thuyết ngôn ngữ tự nhiên', 'uz': 'Name', 'bg': 'Моделите на невронните естествени езикови изводи частично вграждат теориите на лексикалното вписване и отрицание', 'da': 'Neural Natural Language Inference Models Delvist indlejrede teorier om Lexical Entailment og Negation', 'hr': 'Modeli neurološke prirodne jezike djelomično uključene teorije leksičke komplekcije i pregovora', 'nl': 'Neural Natural Language Inference Modellen bevatten gedeeltelijk theorieën van Lexical Entailment en Negation', 'de': 'Neuronale Natursprache-Inferenzmodelle binden teilweise Theorien der Lexikal Entailment und Negation ein', 'fa': 'Models Inference of Natural Language Partially Embedded Theories of Lexical Entailment and Negation', 'id': 'Model Inferensi Bahasa Alami Neural Terisi Teori Kesakitan dan Negasi Lexik', 'ko': '신경 자연 언어 추리 모델 부분에 어휘 함축과 부정 이론이 박혀 있다', 'sw': 'Mradi wa Kuzuia Lugha za asili wa Kiasili Partially Mitandao ya Ujumbe na Kuzuia', 'tr': 'Nural Teýdaly Diller', 'af': 'Neurale Natuurlike Taal Inferensie Modele Deel Inbeter Teories van Leksiese Entailment en Negasie', 'sq': 'Modelet e Inferencës së Gjuhave Neurale Natyrore janë pjesërisht të përfshira Teoriat e Ndryshimit dhe Negimit Leksikal', 'am': 'የኩነቶች ቋንቋ አቀማመጥ', 'az': 'N칬ral T톛bi톛tli Dil Inference Modell톛ri 칐z칲-칬l칞칲d톛 Lexical Entailment and Negation Teoriyalar캼', 'bn': 'স্বাভাবিক ভাষার প্রতিরোধ মোডেল বিশেষ করে লেক্সিকাল ইমেইলেমেন্ট এবং নেগেশনের থিওরিস', 'bs': 'Modeli neurološke prirodne jezike djelomično uključene teorije leksičke integracije i pregovora', 'ca': "Models d'Inferència de Llingua Natural Neural Partialment Embedded Theories of Lexical Entailment and Negation", 'cs': 'Modely inference neuronálního přirozeného jazyka částečně integrují teorie Lexického vylepšení a negace', 'et': 'Neural Natural Language Inference Models Osaliselt põimivad Lexical Entailment ja Negation teooriaid', 'fi': 'Neuroiden luonnollisen kielen inferenssimallit sulauttavat osittain leksiksen päättelyn ja negaation teorioita', 'hy': 'Նյարդային բնական լեզվի ինֆերանսի մոդելները մասամբ ներգրավված լեքսիկական հիվանդության և նեգացիայի տեսությունները', 'he': 'דוגמני השפה הטבעית העצבית מודלים תיאוריות של מחלה וקסמית', 'sk': 'Modeli nevronskih naravnih jezikovnih sklepanj delno vključujejo teorije leksičnega razkrivanja in zanikanja', 'jv': 'Menu item title', 'ha': '@ action', 'bo': 'Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation'}
{'en': 'We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods : the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on  lexical entailment  and  negation . In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing  negation , but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the  causal dynamics  of the model mirror the  causal dynamics  of this  algorithm  on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.', 'ar': 'نتناول ما إذا كانت النماذج العصبية لاستدلال اللغة الطبيعية (NLI) يمكنها تعلم التفاعلات التركيبية بين الاستدلال المعجمي والنفي ، باستخدام أربع طرق: طرق التقييم السلوكي لـ (1) مجموعات اختبار التحدي و (2) مهام التعميم النظامي ، والتقييم الهيكلي طرق (3) تحقيقات و (4) تدخلات. لتسهيل هذا التقييم الشامل ، نقدم Monotonicity NLI (MoNLI) ، وهي مجموعة بيانات طبيعية جديدة تركز على الاستدلال المعجمي والنفي. في تقييماتنا السلوكية ، وجدنا أن النماذج المدربة على مجموعات بيانات NLI للأغراض العامة تفشل بشكل منهجي في أمثلة MoNLI التي تحتوي على نفي ، لكن ضبط MoNLI يعالج هذا الفشل. في تقييماتنا الهيكلية ، نبحث عن دليل على أن نموذجنا القائم على BERT الأفضل أداءً قد تعلم تنفيذ خوارزمية الرتابة وراء MoNLI. تقدم المجسات أدلة متسقة مع هذا الاستنتاج ، وتدعم تجارب التدخل لدينا ذلك ، مما يدل على أن الديناميكيات السببية للنموذج تعكس الديناميكيات السببية لهذه الخوارزمية على مجموعات فرعية من MoNLI. يشير هذا إلى أن نموذج BERT يشتمل جزئيًا على الأقل على نظرية الاستنتاج المعجمي والنفي على المستوى الحسابي.', 'es': 'Abordamos si los modelos neuronales para la Inferencia del Lenguaje Natural (NLI) pueden aprender las interacciones composicionales entre la implicación léxica y la negación, utilizando cuatro métodos: los métodos de evaluación del comportamiento de (1) conjuntos de pruebas de desafío y (2) tareas de generalización sistemática, y los métodos de evaluación estructural de (3) sondas y (4) intervenciones. Para facilitar esta evaluación holística, presentamos Monotonicity NLI (MonLI), un nuevo conjunto de datos naturalista centrado en la implicación y negación léxicas. En nuestras evaluaciones de comportamiento, encontramos que los modelos entrenados en conjuntos de datos de NLI de propósito general fallan sistemáticamente en los ejemplos de MonLI que contienen negación, pero que el ajuste fino de MonLI resuelve este error. En nuestras evaluaciones estructurales, buscamos evidencia de que nuestro modelo basado en BERT de alto rendimiento haya aprendido a implementar el algoritmo de monotonicidad detrás de MonLi. Las sondas arrojan pruebas consistentes con esta conclusión, y nuestros experimentos de intervención lo refuerzan, demostrando que la dinámica causal del modelo refleja la dinámica causal de este algoritmo en subconjuntos de MonLi. Esto sugiere que el modelo BERT incorpora, al menos parcialmente, una teoría de la implicación léxica y la negación a nivel algorítmico.', 'fr': "Nous examinons si les modèles neuronaux pour l'inférence du langage naturel (NLI) peuvent apprendre les interactions compositionnelles entre l'implication lexicale et la négation, en utilisant quatre méthodes\xa0: les méthodes d'évaluation comportementale des (1) ensembles de tests de défi et (2) les tâches de généralisation systématique, et les méthodes d'évaluation structurelle de (3) sondes et (4) interventions. Pour faciliter cette évaluation holistique, nous présentons Monotonicity NLI (MonLI), un nouveau jeu de données naturaliste axé sur l'implication lexicale et la négation. Dans nos évaluations comportementales, nous constatons que les modèles formés sur des ensembles de données NLI à usage général échouent systématiquement sur les exemples MonLI contenant une négation, mais que le réglage fin de MonLi corrige cet échec. Dans nos évaluations structurelles, nous recherchons des preuves que notre modèle BERT le plus performant a appris à implémenter l'algorithme de monotonicité sous-jacent à MonLi. Les sondes fournissent des preuves cohérentes avec cette conclusion, et nos expériences d'intervention renforcent cela, montrant que la dynamique causale du modèle reflète la dynamique causale de cet algorithme sur des sous-ensembles de MonLi. Cela suggère que le modèle BERT intègre au moins partiellement une théorie de l'implication lexicale et de la négation au niveau algorithmique.", 'pt': 'Abordamos se os modelos neurais para Inferência de Linguagem Natural (NLI) podem aprender as interações composicionais entre vinculação lexical e negação, usando quatro métodos: os métodos de avaliação comportamental de (1) conjuntos de testes de desafio e (2) tarefas de generalização sistemática e a avaliação estrutural métodos de (3) sondas e (4) intervenções. Para facilitar essa avaliação holística, apresentamos Monotonicity NLI (MoNLI), um novo conjunto de dados naturalístico focado em vinculação e negação lexical. Em nossas avaliações comportamentais, descobrimos que os modelos treinados em conjuntos de dados NLI de uso geral falham sistematicamente em exemplos MoNLI contendo negação, mas que o ajuste fino do MoNLI aborda essa falha. Em nossas avaliações estruturais, procuramos evidências de que nosso modelo baseado em BERT de alto desempenho aprendeu a implementar o algoritmo de monotonicidade por trás do MoNLI. As sondas fornecem evidências consistentes com essa conclusão, e nossos experimentos de intervenção reforçam isso, mostrando que a dinâmica causal do modelo reflete a dinâmica causal desse algoritmo em subconjuntos de MoNLI. Isso sugere que o modelo BERT, pelo menos parcialmente, incorpora uma teoria de vinculação e negação lexical em um nível algorítmico.', 'ja': '自然言語推論（ NLI ）のニューラルモデルが、（ 1 ）チャレンジテストセットと（ 2 ）体系的な一般化タスクの行動評価方法、（ 3 ）プローブと（ 4 ）介入の構造評価方法の4つの方法を使用して、語彙的帰結と否定の間の構成的相互作用を学習できるかどうかに取り組む。 この全体的な評価を容易にするために、私たちは単調性NLI （ MoNLI ）を提示します。これは、語彙的帰結と否定に焦点を当てた新しい自然主義データセットです。 私たちの行動評価では、汎用NLIデータセットでトレーニングされたモデルは、否定を含むMoNLIの例ではシステム的に失敗するが、MoNLIの微調整はこの失敗に対処することがわかります。 構造的評価では、当社の最高業績を誇るBERTベースのモデルがMoNLIの背後に単調性アルゴリズムを実装することを学んだ証拠を探します。 プローブはこの結論と一致する証拠を得て、我々の介入実験はこれを支持し、モデルの因果動態がMoNLIのサブセット上のこのアルゴリズムの因果動態を反映していることを示している。 これは、BERTモデルが少なくとも部分的に、アルゴリズムレベルでの語彙的帰結と否定の理論を埋め込んでいることを示唆している。', 'zh': '论自然语言理(NLI)者神经模型可以学词汇蕴涵否之间,用四法:(1)挑战试集与(2)统泛化事者评之,与(3)探针与(4)之结构评法。 为促进此体评估,我们出了Montonicity NLI(MoNLI),这是一个新自然主义数据集,专注于词汇蕴涵和否定。 于吾行评估,见通用NLI数集上训形于包否之MoNLI示例系统地败,而MoNLI微调决之。 于我结构评估,我等寻证,证我最佳的基于BERT模形已学成MoNLI背单调性算法。 探针生与证同,吾与实验持之,明其因果动力学见MoNLI子集之因果动力学。 此明BERT少于算法水平上嵌词汇蕴涵否之论也。', 'ru': 'Мы рассматриваем, могут ли нейронные модели для вывода естественного языка (NLI) изучать композиционные взаимодействия между лексическим влечением и отрицанием, используя четыре метода: методы поведенческой оценки (1) наборов контрольных тестов и (2) систематических задач обобщения, и методы структурной оценки (3) зондов и (4) вмешательств. Чтобы облегчить эту целостную оценку, мы представляем Monotonicity NLI (MoNLI), новый натуралистический набор данных, сосредоточенный на лексическом влечении и отрицании. В наших поведенческих оценках мы обнаруживаем, что модели, обученные на наборах данных NLI общего назначения, систематически терпят неудачу на примерах MoNLI, содержащих отрицание, но точная настройка MoNLI устраняет этот сбой. В наших структурных оценках мы ищем доказательства того, что наша высокоэффективная модель на основе BERT научилась реализовывать алгоритм монотонности, лежащий в основе MoNLI. Зонды дают доказательства, согласующиеся с этим выводом, и наши эксперименты по вмешательству подтверждают это, показывая, что причинно-следственная динамика модели зеркально отражает причинно-следственную динамику этого алгоритма на подмножествах MoNLI. Это говорит о том, что модель БЕРТА, по крайней мере, частично воплощает теорию лексического влечения и отрицания на алгоритмическом уровне.', 'hi': 'हम इस बात को संबोधित करते हैं कि क्या प्राकृतिक भाषा अनुमान (एनएलआई) के लिए तंत्रिका मॉडल चार तरीकों का उपयोग करके लेक्सिकल अनिवार्यता और निषेध के बीच रचनात्मक बातचीत सीख सकते हैं: (1) चुनौती परीक्षण सेट और (2) व्यवस्थित सामान्यीकरण कार्यों के व्यवहार मूल्यांकन विधियां, और (3) जांच और (4) हस्तक्षेप के संरचनात्मक मूल्यांकन विधियां। इस समग्र मूल्यांकन को सुविधाजनक बनाने के लिए, हम Monotonicity NLI (MoNLI) प्रस्तुत करते हैं, जो एक नया प्राकृतिक डेटासेट है जो लेक्सिकल अनिवार्यता और निषेध पर केंद्रित है। हमारे व्यवहार मूल्यांकन में, हम पाते हैं कि सामान्य-उद्देश्य वाले एनएलआई डेटासेट पर प्रशिक्षित मॉडल नकारात्मकता वाले MoNLI उदाहरणों पर व्यवस्थित रूप से विफल हो जाते हैं, लेकिन यह कि MoNLI फाइन-ट्यूनिंग इस विफलता को संबोधित करती है। हमारे संरचनात्मक मूल्यांकन में, हम सबूत की तलाश करते हैं कि हमारे शीर्ष प्रदर्शन वाले BERT-आधारित मॉडल ने MoNLI के पीछे monotonicity एल्गोरिथ्म को लागू करना सीखा है। जांच इस निष्कर्ष के अनुरूप सबूत देती है, और हमारे हस्तक्षेप प्रयोगइसे मजबूत करते हैं, यह दिखाते हुए कि मॉडल की कारण गतिशीलता एमओएनएलआई के सबसेट पर इस एल्गोरिथ्म की कारण गतिशीलता को दर्पण करती है। इससे पता चलता है कि BERT मॉडल कम से कम आंशिक रूप से एक एल्गोरिथम स्तर पर लेक्सिकल अनिवार्यता और निषेध के सिद्धांत को एम्बेड करता है।', 'ga': 'Tugaimid aghaidh ar cé acu an féidir le samhlacha néaracha le haghaidh Tátail Theanga Nádúrtha (NLI) na hidirghníomhaíochtaí comhdhéanaimh idir gabháil fhoclóra agus diúltú a fhoghlaim, trí úsáid a bhaint as ceithre mhodh: modhanna meastóireachta iompraíochta (1) tacair tástála dúshláin agus (2) tascanna ginearálaithe córasacha, agus an mheastóireacht struchtúrach. modhanna (3) tóireadóirí agus (4) idirghabhálacha. Chun an mheastóireacht iomlánaíoch seo a éascú, cuirimid Monotonicity NLI (MoNLI) i láthair, tacar sonraí nádúraíoch nua atá dírithe ar shníomh foclóireachta agus diúltú. Inár meastóireachtaí iompraíochta, feicimid go dteipeann go córasach ar mhúnlaí a gcuirtear oiliúint orthu ar thacair sonraí ginearálta NLI ar shamplaí MoNLI ina bhfuil diúltú, ach go dtugann mionchoigeartú MoNLI aghaidh ar an teip seo. Inár meastóireachtaí struchtúracha, lorgaimid fianaise go bhfuil foghlamtha ag ár múnla barrfheidhmíochta atá bunaithe ar BERT conas an t-algartam monotonicity taobh thiar de MoNLI a chur i bhfeidhm. Tugann tóireadóirí fianaise atá comhsheasmhach leis an gconclúid seo, agus treisíonn ár dturgnaimh idirghabhála é seo, ag taispeáint go bhfuil dinimic chúise na samhla ag teacht le dinimic chúiseach an algartam seo ar fho-thacair de MoNLI. Tugann sé seo le tuiscint go n-leabaíonn samhail BERT go páirteach, ar a laghad, teoiric na heagla agus na ndiúltuithe foclóireachta ar leibhéal algartamaíoch.', 'ka': 'ჩვენ მივიღებთ, თუ ნეიროლური მოდელები თავისუფალური ენერგიის ინფერენციაზე (NLI) შეუძლია ვისწავლოთ კომპოციოლური ინტერქექცია ლექსიკალური ინტერქექცია და ნეგრაციის შორის, გამოყენებული ოთხი მეტირები: მოცემების (1) განცემების განცემებ ამ ჰოლესტიკური განსაზღვრებისთვის, ჩვენ მონტონონისტის NLI (MoNLI) ახალი ნატრალისტიკური მონაცემების სექტი, რომელიც ლექსიკალური მოწყობინებაზე და ნექციაზე დავ ჩვენი ქცევის განსაზღვრებით, ჩვენ აღმოჩნეთ, რომ მოდელები, რომლებიც საერთო მიზეზი NLI მონაცემების კონფიგურაციისთვის შეუძლებელია სისტემატიკურად MoNLI მაგალითად, რომლებიც ნეგურა ჩვენი სტრუქტურური განსაზღვრებულებაში ჩვენ ძირითად ვაკეთებთ, რომ ჩვენი უფრო მუშაობელი მოდელი მონაცემი მონოტონონიტის ალგორიტიმა მოNLI-ის შემდეგ იყო. პრობეტები მიიღებენ წარმოდგენები, რომლებიც ამ გადაწყვეტის შემდეგ, და ჩვენი ინტერგენციის ექსპერიმენტები ამას გაუკეთებენ, რომ მოდელის წარმოდგენელი დინამიკა მოდგენა ამ ალგორიტიმს მიზეზ ეს იყველას, რომ BERT მოდელის ნამდვილად ნაწილად ნაწილად ლექსიკალური წარმოდგენის და ნექციის თეორია ალგორტიმური დონეში.', 'hu': 'Négy módszer segítségével vizsgáljuk meg, hogy a Természetes Nyelvi Inferencia (NLI) neurális modelljei meg tudják-e tanulni a lexikai vonatkozás és negatív kölcsönhatásait: a (1) kihívási tesztkészletek és (2) szisztematikus generalizációs feladatok viselkedésének értékelési módszereit, valamint (3) szondák és (4) beavatkozások strukturális értékelési módszereit. A holisztikus értékelés megkönnyítése érdekében bemutatjuk a Monotonicity NLI (MoNLI), egy új naturalisztikai adatkészletet, amely a lexikai vonatkozásra és negatívásra összpontosít. Viselkedési értékeléseinkben azt találtuk, hogy az általános célú NLI adatkészletekre képzett modellek szisztematikusan hibásak a negatív MoNLI példákon, de a MoNLI finomhangolása ezt a hibát orvosolja. Strukturális értékeléseinkben bizonyítékokat keresünk arra, hogy a legjobb teljesítményű BERT-alapú modellünk megtanulta a MoNLI mögötti monotonitás algoritmus megvalósítását. A szondák ezzel a következtetéssel összhangban szolgálnak, és intervenciós kísérleteink ezt alátámasztják, azt mutatják, hogy a modell okozati dinamikája tükrözi ennek az algoritmusnak az okozati dinamikáját a MoNLI részhalmazain. Ez arra utal, hogy a BERT modell legalább részben algoritmikus szinten beágyaz egy lexikai vonatkozás és negatív elméletet.', 'el': 'Εξετάζουμε κατά πόσον τα νευρωνικά μοντέλα για το Συμπέρασμα Φυσικής Γλώσσας (ΝΛΙ) μπορούν να μάθουν τις σύνθετες αλληλεπιδράσεις μεταξύ λεξικού και άρνησης, χρησιμοποιώντας τέσσερις μεθόδους: τις μεθόδους συμπεριφορικής αξιολόγησης των (1) δοκιμαστικών συνόλων και (2) εργασιών συστηματικής γενικοποίησης, και τις μεθόδους δομικής αξιολόγησης των (3) ανιχνευτών και (4) παρεμβάσεων. Για να διευκολυνθεί αυτή η ολιστική αξιολόγηση, παρουσιάζουμε ένα νέο νατουραλιστικό σύνολο δεδομένων που επικεντρώνεται στη λεξική εμπλοκή και άρνηση. Στις συμπεριφορικές αξιολογήσεις μας, διαπιστώνουμε ότι μοντέλα εκπαιδευμένα σε σύνολα δεδομένων γενικής χρήσης αποτυγχάνουν συστηματικά σε παραδείγματα που περιέχουν άρνηση, αλλά ότι ο συντονισμός του αντιμετωπίζει αυτή την αποτυχία. Στις δομικές μας αξιολογήσεις, αναζητούμε στοιχεία ότι το κορυφαίο μοντέλο μας με βάση το BERT έχει μάθει να εφαρμόζει τον αλγόριθμο μονοτονίας πίσω από το MoNLI. Οι ανιχνευτές παρέχουν στοιχεία συνεπή με αυτό το συμπέρασμα, και τα πειράματα επέμβασής μας ενισχύουν αυτό, δείχνοντας ότι η αιτιώδης δυναμική του μοντέλου αντικατοπτρίζει την αιτιώδη δυναμική αυτού του αλγόριθμου σε υποσύνολα του MoNLI. Αυτό υποδηλώνει ότι το μοντέλο BERT ενσωματώνει τουλάχιστον εν μέρει μια θεωρία της λεξικής εμπλοκής και άρνησης σε αλγοριθμικό επίπεδο.', 'it': "Esaminiamo se i modelli neurali per l'inferenza del linguaggio naturale (NLI) possono apprendere le interazioni compositive tra implicazione lessicale e negazione, utilizzando quattro metodi: i metodi di valutazione comportamentale di (1) test set di sfida e (2) compiti di generalizzazione sistematica, e i metodi di valutazione strutturale di (3) sonde e (4) interventi. Per facilitare questa valutazione olistica, presentiamo Monotonicity NLI (MoNLI), un nuovo dataset naturalistico incentrato sull'implicazione lessicale e sulla negazione. Nelle nostre valutazioni comportamentali, scopriamo che i modelli formati su set di dati NLI per scopi generali falliscono sistematicamente su esempi MoNLI contenenti negazione, ma che la messa a punto di MoNLI risolve questo fallimento. Nelle nostre valutazioni strutturali, cerchiamo prove che il nostro modello BERT ad alte prestazioni abbia imparato ad implementare l'algoritmo di monotonia dietro MoNLI. Le sonde forniscono prove coerenti con questa conclusione, e i nostri esperimenti di intervento lo sostengono, mostrando che la dinamica causale del modello rispecchia la dinamica causale di questo algoritmo su sottoinsiemi di MoNLI. Ciò suggerisce che il modello BERT incorpora almeno parzialmente una teoria dell'implicazione lessicale e della negazione a livello algoritmico.", 'kk': 'Біз Nature Language Inference (NLI) үшін невралдық үлгілер лексикалық қатынас мен негативтің арасындағы төрт әдістерді қолдануға болады, әрекеттерді бағалау әдістерін (1) және (2) жүйелік жалпы түрлендіру тапсырмаларын және (3) проб және (4) интервенцияларының структур Бұл толық оқиғаны көмектесу үшін, біз NLI (MoNLI) монотонициялық, лексикалық жетілдіру мен негативті көмектесетін жаңа натуралистик деректер жинағын таңдаймыз. Біздің әрекеттеріміздің оқиғаларымызда, NLI деректер жиындарында оқиға берілген моделдердің жүйелік мәселелерінде негативті мәселелерде болмайды, бірақ MoNLI баптауларының бұл жаңылыс адрестерін жасайд Біздің структуралық оқиғаларымызда, BERT негіздеген үлгіміздің MONLI артындағы монотониялық алгоритмін орындау алгоритмін үйрендік. Сынақтар осы соңында тұратын дәлелдерді көрсетеді, және біздің тәжірибеміздің тәжірибелеріміз бұл дәлелдерді көтереді, моделінің негізгі динамикасы MONLI бағдарламаларының алгоритмнің себепті динамикасын Бұл BERT үлгісінің кемінде кемінде лексикалық теориясын және негативті алгоритмдік деңгейінде ендіру үшін ұсынады.', 'lt': 'Mes sprendžiame, ar natūralios kalbos Inferencijos (NLI) neurologiniai modeliai gali išmokti sudėtinę sąveiką tarp leksinio įtraukimo ir neigiamo įtraukimo, taikant keturis metodus: 1) iššūkių bandymų rinkinių ir 2) sisteminių generalizacijos užduočių elgesio vertinimo metodus ir 3) sondų ir 4) intervencijų struktūrinio vertinimo metodus. Siekiant palengvinti šį holistinį vertinimą, pristatome Monotonicity NLI (MoNLI), naują natūralių duomenų rinkinį, kuriame daugiausia dėmesio skiriama leksiniam įtraukimui ir neigiamam veikimui. Mūsų elgsenos vertinimuose nustatyta, kad modeliai, parengti bendros paskirties NLI duomenų rinkiniais, sistemingai nepavyksta gauti MoNLI pavyzdžių, kuriuose yra neigiamų rezultatų, tačiau kad MoNLI patobulinimas sprendžia šį nepakankamumą. Mūsų struktūriniuose vertinimuose ieškome įrodymų, kad mūsų geriausiai veikiantis BERT pagrįstas modelis išmoko įgyvendinti MoNLI monotoniškumo algoritmą. Bandymai duoda įrodymų, atitinkančių šią išvadą, ir mūsų intervenciniai eksperimentai tai sustiprina, rodydami, kad modelio priežastinis dinamika atspindi šio algoritmo priežastinę dinamiką MoNLI pogrupiuose. Tai rodo, kad BERT modelis bent iš dalies įtraukia teoriją apie lexinį įtraukimą ir neigiamą poveikį algoritminiu lygiu.', 'mk': 'Ние решаваме дали невровните модели за природна инференција на јазикот (НЛИ) можат да ги научат композициските интеракции помеѓу лексикалното вмешање и негативноста, користејќи четири методи: методите на однесувачка проценка на (1) тестови и (2) систематични генерализациски задачи, и структурните методи на проценка на За да ја олесниме оваа холистичка проценка, го претставуваме Монотонитетот НЛИ (МОНЛИ), нов натуралистички податок фокусиран на лексикалното вмешање и негирање. Во нашите однесувачки проценки, откриваме дека моделите обучени на генерални податоци на НЛИ систематски не успеваат на примерите на МоНЛИ кои содржат негативност, но дека финетизирањето на МоНЛИ го решава овој неуспех. Во нашите структурни проценки, бараме докази дека нашиот најдобар модел базиран на БЕРТ научил да го имплементира монотоничниот алгоритм зад МоНЛИ. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI.  This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.', 'ms': 'Kita alami sama ada model saraf untuk Inferensi Bahasa Alami (NLI) boleh belajar interaksi komposisi antara penyelesaian leksikal dan negatif, menggunakan empat kaedah: kaedah penilaian perilaku set ujian (1) cabaran dan (2) tugas generalisasi sistemik, dan kaedah penilaian struktur (3) sond dan (4) intervensi. Untuk memudahkan penilaian holistik ini, kami memperkenalkan Monotonicity NLI (MoNLI), set data semulajadi baru yang fokus pada penyelesaian dan negatif leksikal. Dalam penilaian perilaku kami, kami mendapati bahawa model yang dilatih pada set data NLI-tujuan umum gagal secara sistematik pada contoh MoNLI yang mengandungi negatif, tetapi bahawa penyesuaian MoNLI mengatasi kegagalan ini. Dalam penilaian struktur kami, kami mencari bukti bahawa model terbaik kami berdasarkan BERT telah belajar untuk melaksanakan algoritma monotoniti di belakang MoNLI. Ujian memberikan bukti yang konsisten dengan kesimpulan ini, dan eksperimen intervensi kami meningkatkan ini, menunjukkan bahawa dinamik penyebab model mirror dinamik penyebab algoritma ini pada subset MoNLI. Ini menunjukkan bahawa model BERT sekurang-kurangnya sebahagian memasukkan teori penyelesaian dan negatif leksikal pada tahap algoritmi.', 'ml': 'നാല് രീതികള്\u200d ഉപയോഗിച്ച് ലെക്സിക്സിക്കല്\u200d മെന്റിമെന്റിനും നേരിയേഷനും തമ്മിലുള്ള സങ്കീര്\u200dണ്ണമായ ഇടപാടുകള്\u200d പഠിക്കാന്\u200d സാധ്യഭാഷ മോഡലുകള്\u200d നമ്മള്\u200d വിശദീകരിക്കുന്നു ഈ ഹോലിസ്റ്റിക്കല്\u200d വിലാസങ്ങള്\u200dക്ക് ലഭ്യമാക്കാന്\u200d, നമ്മള്\u200d മോണോട്ടോണിക്കിറ്റി NLI (മോണ്\u200dലി), ഒരു പുതിയ സ്വാഭാവികമായ ഡാറ്റാസറ് നമ്മുടെ പ്രവര്\u200dത്തനങ്ങളുടെ പരിശീലനത്തില്\u200d നമുക്ക് കണ്ടെത്താം, ജനറല്\u200d ലക്ഷ്യത്തില്\u200d NLI ഡാറ്റാസറ്റുകളില്\u200d പരിശീലിക്കപ്പെട്ട മോഡലുകള്\u200d മോന്\u200dലിയില്\u200d ന നമ്മുടെ അടിസ്ഥാനത്തില്\u200d നമ്മള്\u200d തെളിവുകള്\u200d അന്വേഷിക്കുന്നു, നമ്മുടെ മുകളില്\u200d ബെര്\u200dട്ടി അടിസ്ഥാനമായി പ്രവര്\u200dത്തിക്കുന്ന മോഡല്\u200d മോണ ഈ അവസാനത്തോടൊപ്പം തെളിവുകള്\u200d കൊണ്ടുവരുന്നു. നമ്മുടെ ഇടപാടുകള്\u200d പരീക്ഷിക്കുന്ന പരീക്ഷണങ്ങള്\u200d ഇതിനെ പിടിച്ചുകൊണ്ടിരിക്കുന്നു. മോഡലിന്റെ കാരണമായ ദൈവ This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.', 'mt': 'Aħna nindirizzaw jekk mudelli newrali għall-Inferenza tal-Lingwa Naturali (NLI) jistgħux jitgħallmu l-interazzjonijiet kompożittivi bejn l-involviment lexikali u n-negazzjoni, bl-użu ta’ erba’ metodi: il-metodi ta’ evalwazzjoni tal-imġiba ta’ (1) settijiet ta’ ttestjar ta’ sfida u (2) kompiti ta’ ġeneralizzazzjoni sistematika, u l-metodi ta’ evalwazzjoni strutturali ta’ (3) son Biex tiffaċilita din l-evalwazzjoni olistika, qed nippreżentaw Monotoniċità NLI (MoNLI), sett ġdid ta’ dejta naturalistika ffukat fuq involviment u negazzjoni lexikali. Fl-evalwazzjonijiet tal-imġiba tagħna, isibu li mudelli mħarrġa fuq settijiet ta’ dejta NLI ta’ skop ġenerali jfallu sistematikament fuq eżempji MoNLI li fihom in-negazzjoni, iżda li l-aġġustament tal-MoNLI jindirizza dan il-falliment. Fl-evalwazzjonijiet strutturali tagħna, aħna qed ifittxu evidenza li l-mudell tagħna bl-aqwa prestazzjoni bbażat fuq il-BERT għallem jimplimenta l-algoritmu tal-monotoniċità wara l-MoNLI. Is-sondi jagħtu evidenza konsistenti ma’ din il-konklużjoni, u l-esperimenti ta’ intervent tagħna jsaħħu dan, u juru li d-dinamika kawżali tal-mudell tirrifletti d-dinamika kawżali ta’ dan l-algoritmu fuq is-sottosetti tal-MoNLI. Dan jissuġġerixxi li l-mudell BERT tal-anqas parzjalment jinkorpora teorija ta’ involviment u negazzjoni lexikali f’livell algoritmiku.', 'pl': 'Badamy, czy modele neuronowe dla wniosków języka naturalnego (NLI) mogą nauczyć się interakcji kompozycyjnych pomiędzy leksykalnym zaangażowaniem a negacją, stosując cztery metody: metody oceny behawioralnej zestawów testów (1) wyzwań i (2) zadań systematycznych uogólniania oraz metody oceny strukturalnej (3) sond i (4) interwencji. Aby ułatwić tę całościową ocenę, prezentujemy Monotonicity NLI (MoNLI), nowy zbiór danych naturalistycznych skupiający się na leksykalnym uwzględnieniu i negacji. W naszych ocenach behawioralnych okazujemy się, że modele przeszkolone na ogólnych zbiorach danych NLI systematycznie zawodzą na przykładach MoNLI zawierających negację, ale że dostosowanie MoNLI rozwiązuje tę awarię. W naszych ocenach strukturalnych szukamy dowodów, że nasz najlepiej wydajny model oparty na BERT nauczył się implementować algorytm monotoniczny stojący za MoNLI. Sondy dają dowody zgodne z tym wnioskiem, a nasze eksperymenty interwencyjne to potwierdzają, pokazując, że dynamika przyczynowa modelu odzwierciedla dynamikę przyczynową tego algorytmu na podzbiorach MoNLI. Sugeruje to, że model BERT przynajmniej częściowo osadza teorię leksykalnego pociągnięcia i negacji na poziomie algorytmicznym.', 'mn': 'Бид байгалийн хэл хамааралтай (NLI) мэдрэлийн загваруудын төвөгтэй загваруудын төвөгтэй харилцаа сурах боломжтой эсэхийг харуулж, 4 арга загварыг ашиглаж, 4 арга загварыг ашиглаж, 1) сорилтын шалгалтын шалгалтын төлөө, 2) систематикийн ерөнхийлөгчийн даалгаваруудын төвөгтэй харилцаа, 3) суд Эдгээр бүрэн үнэлгээг дэмжихийн тулд бид НLI (MoNLI) Монотонист, сэтгэл санаанд төвлөрсөн шинэ натуралист өгөгдлийн санг харуулж байна. Бидний үйл явдлын үнэлгээнд бид NLI өгөгдлийн сан дээр сургалтын загвар нь сөрөг байдлын жишээнд систематик боловсруулагдаж байгааг олж мэднэ. Гэхдээ MoNLI-ын сайхан дүгнэлт нь энэ бүтэлгүйтгэлийг удирдаж байна. Бидний бүтэц дүгнэлтийн даалгавар дээр хийсэн BERT-ийн загвар нь МоNLI-ийн ард монотоницийн алгоритмыг дамжуулахыг сурсан гэдгийг баталж байна. Судалгаанууд энэ дүгнэлтээс харьцуулсан баталгаа өгдөг. Бидний оролцооны туршилтууд үүнийг нэмэгдүүлж, загварын шалтгаан дүгнэлт нь МоNLI-ын суурь хэсэгт энэ алгоритмын шалтгаан динамикийг харуулж байна. Энэ нь BERT загвар нь хамгийн бага хэсгийг алгоритмын түвшинд лексикийн загвар болон сөрөг байдлын теорийг оруулдаг гэсэн үг.', 'ro': 'Analizăm dacă modelele neurale pentru Inferența Limbajului Natural (NLI) pot învăța interacțiunile compoziționale dintre implicarea lexicală și negarea, utilizând patru metode: metodele de evaluare comportamentală a (1) seturilor de teste de provocare și (2) sarcini sistematice de generalizare, precum și metodele de evaluare structurală a (3) sondelor și (4) intervențiilor. Pentru a facilita această evaluare holistică, prezentăm Monotonicity NLI (MoNLI), un nou set de date naturalistice axat pe implicarea și negarea lexicală. În evaluările noastre comportamentale, constatăm că modelele instruite pe seturi de date NLI de uz general eșuează sistematic pe exemple MoNLI care conțin negație, dar că ajustarea fină MoNLI abordează acest eșec. În evaluările noastre structurale, căutăm dovezi că modelul nostru BERT performant a învățat să implementeze algoritmul monotonic din spatele MoNLI. Sondele dau dovezi compatibile cu această concluzie, iar experimentele noastre de intervenție susțin acest lucru, arătând că dinamica cauzală a modelului reflectă dinamica cauzală a acestui algoritm pe subseturi ale MoNLI. Acest lucru sugerează că modelul BERT încorporează cel puțin parțial o teorie a implicării și negației lexicale la nivel algoritmic.', 'no': 'Vi adresserer om neirale modeller for naturspråk-inferensen (NLI) kan lære samordninga mellom leksiske innstillingar og negasjon, med fire metodar: utfordringsevalueringsmetodane for (1) utfordringstestsett og (2) systemoppgåver for generalisering, og strukturelle evalueringsmetodane for (3) prober og (4) intervensjonar. For å letta denne holistiske evalueringa, presenterer vi Monotonicity NLI (MoNLI), ei ny naturalistisk dataset fokusert på leksisk innsatning og negasjon. I våre oppførselsevalueringar finn vi at modeller som er trengte på generelle målsettingar for NLI-datasett mislukkast systematisk på MoNLI-eksemplar som inneheld negasjon, men at MoNLI-finnstillingar gjer denne feilen. I våre strukturelle evalueringar søker vi etter beviser at vårt øvre utføringsbasert modell har lært å implementere monotoniske algoritmen bak MoNLI. Prøver å gje beviser som er konsistent med denne konklusjonen, og intervensjonen våre eksperimenterer større dette, viser at den grunnleggjande dynamikken av modellen spegeljer den grunnleggjande dynamikken av denne algoritma på undergruppene av MoNLI. Dette tyder på at BERT-modellen innebyr minst delvis ein teori på leksisk innhalding og negasjon på ein algoritmisk nivå.', 'sr': 'Mi se obraćamo da li neuronski modeli za prirodnu jeziku (NLI) mogu naučiti kompozicionalne interakcije između leksičke želje i negacije, koristeći četiri metode: metode procjene ponašanja (1) seta testova izazova i (2) sistematske generalizacije zadataka i strukturne metode procjene (3) sonde i (4) intervencija. Da bi olakšali ovu holističku procjenu, predstavljamo Monotonicnost NLI (MoNLI), novu naturalistsku datumsku setu fokusiranu na leksičku želju i negaciju. U našim procjenama ponašanja, nalazimo se da modeli obučeni na općem cilju NLI podataka sistematski ne uspevaju na primjerima MoNLI koji sadrže negaciju, ali da se MoNLI fino navodi na ovaj neuspjeh. U našim strukturnim procenacijama, tražimo dokaze da je naš vrhunski model na BERT-u naučio da implementira algoritam monotoniciteta iza MoNLI-a. Probe pružaju dokaze koji su u skladu sa ovim zaključkom, i naši intervencijski eksperimenti su to poboljšali, pokazujući da uzrokovana dinamika modela ogledala uzrokovanu dinamiku ovog algoritma na podskupinama MoNLI. To predlaže da model BERT najmanje delimično uključuje teoriju leksičke želje i negacije na algoritmičkom nivou.', 'so': 'Waxaan ka sheekaynaynaa in noocyada caqabadaha ee ka mid ah iskudarka afka asalka ah (NLI) uu barto iskaasha dhexe u dhexeeya dhibaatooyinka leksikalka iyo waxyaabaha galmada, waxaynu isticmaali karnaa afar qaab: qaababka qiimeynta dabiicadda (1) iyo kooxda imtixaanka shakhsiyadda iyo (2) hawlaha sameynta si systemic ah, iyo qaababka qiimeynta kooxaha (3) iyo (4) qaabiyayaasha qaababka lagu sameeyo. Si aan u fududeeyo qiimeyntan quduuska ah, waxaynu soo bandhignaynaa Monotonicity NLI (MoNLI), taas oo cusub ah macluumaad natural ah oo ku cusboonaysa cilmiga leksikal iyo negative. Qiimeynta dabeecadeena, waxaynu helnaa in modellada lagu tababaray oo lagu tababaray koobashada asalka ah NLI ay si rasmi ah ugu baaqan yihiin tusaalayaasha MoNLI ee ku jirta diidmada, laakiin in MoNLI si fiican looga barto ayaa ku habboon khaladdaas. Qiimeynta dhismaha, waxaynu raadinaynaa caddeyda in qaababka sameynta ee ugu sareeya ee BERT waxay bartay in uu sameeyo algorithiga banaaninimada ee ka dambeeya MoNLI. Dhibaatooyinka waxaa soo saara caddeynta ku haboon dhamaanshaan, jirrabaadka dib u qabsashadana waxay soo bandhigtaa tan, waxaana tusinayaa in dabeecada dabeecada muusikada sababta ah dhaqdhaqaalaha algorithmkan ku jira kooxaha MoNLI. Tani waxay ka jeedaa in Tusaale BERT ugu yaraan qeyb ka mid ah uu ku sameeyaa fikrada cilmiga lexicka iyo diidmada darajada algorithiga.', 'sv': 'Vi undersöker om neurala modeller för Natural Language Inference (NLI) kan lära sig kompositionella interaktioner mellan lexikal involvering och negation, med hjälp av fyra metoder: beteendeutvärderingsmetoderna för (1) provuppsättningar och (2) systematiska generaliseringsuppgifter, samt strukturella utvärderingsmetoder för (3) sonder och (4) interventioner. För att underlätta denna holistiska utvärdering presenterar vi Monotonicity NLI (MoNLI), ett nytt naturalistiskt dataset fokuserat på lexikal involvering och negation. I våra beteendeutvärderingar finner vi att modeller utbildade på generella NLI-datauppsättningar misslyckas systematiskt på MoNLI-exempel som innehåller negation, men att MoNLI finjustering åtgärdar detta fel. I våra strukturella utvärderingar letar vi efter bevis för att vår topppresterande BERT-baserade modell har lärt sig att implementera monotonitetsalgoritmen bakom MoNLI. Sonder ger bevis som överensstämmer med denna slutsats, och våra interventionsexperiment stärker detta, vilket visar att orsaksdynamiken i modellen speglar orsaksdynamiken hos denna algoritm på delmängder av MoNLI. Detta tyder på att BERT-modellen åtminstone delvis inbäddar en teori om lexikal involvering och negation på algoritmisk nivå.', 'ta': 'இயல்பான மொழி முரண்பாடு இந்த புதிய பரிசோதனையை உபயோகிக்க, நாம் மோன்டோனிசிட்டி NLI (மோன்டோனிசிடி)யை காண்பிக்கிறோம், ஒரு புதிய இயற்கையான தகவல் அமைப்ப எங்கள் நடத்தையின் மதிப்புகளில், நாங்கள் கண்டுபிடிக்க முடியும் மாதிரிகள் பொதுவான NLI தரவுத்தளங்களில் பயிற்சி செய்யப்பட்டுள்ளது மோன்லி உதார எங்கள் கட்டுப்பாட்டு மதிப்புகளில், நாங்கள் தேடுகிறோம் எங்கள் மேல் செயல்படுத்தும் பிரெட்டின் மாதிரி மாதிரி மாதிரி மாதிரி மோ பிரபஸ்கள் இந்த முடிவுடன் பொருத்தமான ஆதாரங்கள் கொடுக்கும் மற்றும் எங்கள் இடைவெளி சோதனைகள் இதை மூடுகிறது, மாதிரியின் காரணத்தின் காரணத்தை காட்டுகிறது மோ இது பிரெட் மாதிரி குறைந்தது பகுதியாவது ஒரு லெக்சிக்சியல் அறிவிப்பு மற்றும் எதிர்பார்ப்பு மட்டத்தில் உள்ள ஒரு திட்டம் பொர', 'si': 'අපි ප්\u200dරශ්නයක් කරනවා නැත්තම් භාෂාව Inferno (NLI) සඳහා ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ඉන්න පුළුවන් නැත්තම් භාෂා කරන්න පුළුවන් නැත්තම් භාෂා කරන්න පුළුවන් කියලා, ප්\u200dරශ්නයක්  මේ හෝලිස්ටික් විශ්ලේෂණය සැලසුම් කරන්න, අපි මොනෝටෝනිකිටි NLI (MoNLI) වෙනුවෙන්, අළුත් ස්වභාවිත්\u200dයාත්මක දත්ත සැ අපේ ව්\u200dයාපෘතික විශ්වාසයෙන්, අපි හොයාගන්නේ සාමාන්\u200dය-අරමුණු NLI දත්ත සේට් වලින් ප්\u200dරශ්නය කරලා තියෙන මොඩේල් ප්\u200dරශ්නයක් නැති වෙ අපේ සංස්ථාපනය විශ්ලේෂණයේ අපි හොයාගන්න සාක්ෂියක් හොයාගන්නේ අපේ උපරිම ප්\u200dරමාණය BERT-අධාරණය විදිහට ඉගෙන ගත්තා MoN ප්\u200dරශ්න සාක්ෂියක් දෙන්නේ මේ අවස්ථාව සමඟ සාක්ෂියක්, අපේ අවස්ථාව ප්\u200dරශ්නයක් මේක බොල්ස් කරනවා, ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ මේක ප්\u200dරශ්නය කරනවා BERT මොඩල් එක අඩුම අඩුම අඩුම අඩුම අඩුම අඩුම අඩුවෙන් ලෙක්සිකාලික සිද්ධාවක් සහ අඩුම අඩුම', 'ur': 'ہم مشورہ کرتے ہیں کہ طبیعی زبان انفارنس کے لئے نیورال موڈل (NLI) لکسیکل انفارنس اور منفی کے درمیان مکانوسٹی تفاوت کو سیکھ سکتے ہیں، چار طریقے کے مطابق: چار طریقے کے مطابق (1) چلنے کی آزمائش کے مطابق (2) چلنے کی آزمائش کے مطابق اور (2) سیستماتیک عمومی تفاوت کے مطاب ہم نے ایک نئی طبیعی ڈاٹ سٹ کو لکسیکی اضطراری اور ناپسند پر تمرکز کیا ہے۔ ہمارے رفتار کی ارزیابی میں، ہم دیکھتے ہیں کہ نمونے NLI ڈاٹ سٹ پر آموزش کی جاتی ہیں جو موNLI مثالوں میں منفی ہے، لیکن موNLI نیٹ تنظیم اس غلطی کے بارے میں سیستماتیک طور پر ناکام ہیں. ہمارے ساختار ارزیابی میں، ہم نے ثابت کی تلاش کی کہ ہمارے اوپر عمل کرنے والے BERT-based موڈل نے MONLI کے پیچھے موٹنونیٹی الگوریتم کو لازم کرنے کی تعلیم دی ہے. اس نتیجہ کے ساتھ موجود دلیلیں حاصل کرتی ہیں، اور ہماری انتشار کی تجربیاں اس کو بلند کرتی ہیں، یہ دکھاتے ہیں کہ موڈل کی دلیل ڈینامیکٹ موNLI کے سپٹوں پر اس الگوریٹم کی دلیل ڈینامیکٹ کی وجہ سے موجود ہوتی ہے. یہ بتاتا ہے کہ BERT موڈل کم کم حصہ سے لکسیکل تحمل اور ناپسند کا نظریہ ایک الگوریٹمیک سطح پر ہے.', 'uz': "Biz tabiiy tilning tarkibini aniqlash (NLI) va hech qanday narsalarni o'rganish mumkin, va to'rt usuldan foydalanishimiz mumkin: tabiiy qiymatning usuli (1) ta'sir qilish usullari va (2) tizim generaliz vazifalari va tuzuvchilarning structural qiymatlari usullarini (3) davom etish va (4) interventionlarni ishlatish mumkin. Bu taqdimlik qiymatni foydalanish uchun, biz Monotonikity NLI (MoNLI), yangi naturalistik maʼlumot tizimi leksikal taʼminlovchi va negativga bog'langan. Bizning tabiiy qiymatlarimizda o'rganamiz, umumiy NLI maʼlumotlar tizimida o'rganilgan modellar MoNLI misollarida hech qanday narsa yoʻq emas, lekin MoNLI yaxshi ko'payishi mumkin bu muvaffaqiyatlarni ishlatadi. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI.  Probeslar bu tuzuvga bir narsa ishlatadi, va bizning intervention imtiyozlarimiz buni ko'rsatadi modelning sababchi dynamikasini ko'rsatish mumkin. Bu algoritning sababchi dynamikasini MoNLI guruhlariga ko'rsatish mumkin. Bu esa BERT modeli qismidan qismdan, leksikal taʼminlovchi va negativ darajada qo'shiladi.", 'vi': 'Chúng tôi xem liệu các mô hình thần kinh của Liên Minh ngôn ngữ tự nhiên (NLI) có thể học các tương tác phân phối giữa tính chất và âm bản, sử dụng bốn phương pháp: các phương pháp đánh giá về hành vi của (1) các bộ thử thách và (2) các công việc tổng hợp có hệ thống, và các phương pháp đánh giá cấu trúc của (3) các vấn đề và (4) các biện pháp. Để dễ dàng đánh giá tổng thể này, chúng tôi giới thiệu Monotoniđô NLl (MoNLl), một tập tin mới về tính chất tự nhiên có tiêu hóa và tiêu cực. Trong những đánh giá hành vi, chúng tôi thấy các mô hình được đào tạo trên các tập tin NIL phổ biến đều thất bại một cách hệ thống về các trường mẫu MoNLI có chứa sự từ chối, nhưng tính chất MoNLI sửa chữa sai lầm này. Trong phần đánh giá cấu trúc, chúng tôi tìm kiếm bằng chứng cho thấy mô hình BERT nổi tiếng của chúng tôi đã học được cách sử dụng thuyết độc thoại phía sau MoNli. Có bằng chứng rõ ràng với kết luận này, và thí nghiệm can thiệp của chúng ta tăng cường điều này, cho thấy sự kiện sinh động của mô hình phản ánh sự kiện này theo nguyên nhân của thuật toán này trên các nhóm của MoNli. Điều này cho thấy mô hình của BERT ít nhất lại gồm một giả thuyết về tính toán và âm bản theo thuật toán.', 'bg': 'Разглеждаме дали невронните модели за естествено езиково заключение (НЛИ) могат да научат композиционните взаимодействия между лексикалното обвързване и отрицание, като използваме четири метода: методите за поведенческа оценка на (1) тестови комплекти предизвикателни и (2) задачи за систематична обобщаване, както и методите за структурна оценка на (3) сонди и (4) интервенции. За да улесним тази цялостна оценка, представяме нов натуралистичен набор от данни, фокусиран върху лексикалното обвързване и отрицание. В нашите поведенчески оценки откриваме, че моделите, обучени за набори от данни с общо предназначение, се провалят систематично при примери, съдържащи отрицание, но фината настройка на МоНЛИ решава този неуспех. В нашите структурни оценки търсим доказателства, че нашият най-ефективен модел базиран на BERT се е научил да прилага алгоритъма за монотонност зад МоНЛИ. Сондите дават доказателства, съответстващи на това заключение, и нашите интервенционни експерименти потвърждават това, показвайки, че причинно-следствената динамика на модела отразява причинно-следствената динамика на този алгоритъм върху подмножество на MoNLI. Това предполага, че моделът BERT поне частично вгражда теория на лексикалното обвързване и отрицание на алгоритмично ниво.', 'da': 'Vi beskæftiger os med, om neurale modeller for Natural Language Inference (NLI) kan lære de sammensætningsmæssige interaktioner mellem lexikal involvering og negation ved hjælp af fire metoder: adfærdsmæssige evalueringsmetoder for (1) challenge testsæt og (2) systematiske generaliseringsopgaver, og strukturelle evalueringsmetoder for (3) prober og (4) interventioner. For at lette denne holistiske evaluering præsenterer vi Monotonicity NLI (MoNLI), et nyt naturalistisk datasæt fokuseret på leksikisk involvering og negation. I vores adfærdsmæssige evalueringer finder vi, at modeller trænet på generelle NLI datasæt mislykkes systematisk på MoNLI eksempler indeholdende negation, men at MoNLI finjustering løser denne fejl. I vores strukturelle evalueringer leder vi efter beviser på, at vores topydende BERT-baserede model har lært at implementere monotonicitetsalgoritmen bag MoNLI. Sonder giver beviser i overensstemmelse med denne konklusion, og vores interventionseksperimenter understøtter dette, hvilket viser, at årsagsdynamikken i modellen afspejler årsagsdynamikken i denne algoritme på delmængder af MoNLI. Dette tyder på, at BERT-modellen i det mindste delvist integrerer en teori om leksikalsk involvering og negation på et algoritmisk niveau.', 'nl': 'We onderzoeken of neurale modellen voor Natural Language Inference (NLI) de compositionele interacties tussen lexicale implicatie en negatie kunnen leren, met behulp van vier methoden: de gedragsevaluatiemethoden van (1) challenge testsets en (2) systematische generalisatie taken, en de structurele evaluatiemethoden van (3) sondes en (4) interventies. Om deze holistische evaluatie te vergemakkelijken presenteren we Monotonicity NLI (MoNLI), een nieuwe naturalistische dataset gericht op lexicale implicatie en ontkenning. In onze gedragsevaluaties zien we dat modellen getraind op algemene NLI-datasets systematisch falen op MoNLI-voorbeelden die negatie bevatten, maar dat MoNLI-finetuning deze fout aanpakt. In onze structurele evaluaties zoeken we naar bewijs dat ons best presterende BERT-model heeft geleerd om het monotoniciteitsalgoritme achter MoNLI te implementeren. Sondes leveren bewijs dat consistent is met deze conclusie, en onze interventieexperimenten ondersteunen dit, waaruit blijkt dat de causale dynamiek van het model de causale dynamiek van dit algoritme weerspiegelt op subsets van MoNLI. Dit suggereert dat het BERT-model ten minste gedeeltelijk een theorie van lexicale implicatie en ontkenning op algoritmisch niveau inbedde.', 'hr': 'Mi se obraćamo da li neuronski modeli prirodnog jezika (NLI) mogu naučiti kompozicionalne interakcije između leksičke želje i negacije, koristeći četiri metode: metode procjene ponašanja (1) seta ispitivanja izazova i (2) sistematske generalizacije zadataka i strukturne metode procjene (3) sonde i (4) intervencija. Da bi olakšali ovu holističku procjenu, predstavljamo Monotoničnost NLI (MoNLI), novu prirodnu kompetu podataka usredotočenu na leksičku želju i negaciju. U našim procjenama ponašanja, nalazimo se da modeli obučeni na općem cilju NLI podaci sistematski ne uspevaju na primjerima MoNLI koji sadrže negaciju, ali da se MoNLI fino navodi na ovaj neuspjeh. U našim strukturnim procjenama, tražimo dokaze da je naš vrhunski model na BERT-u naučio provesti algoritam monotoniciteta iza MoNLI-a. Probe pružaju dokaze u skladu s ovim zaključkom, a naši intervencijski eksperimenti to povećavaju, pokazujući da uzročna dinamika modela ogledala uzročnu dinamiku ovog algoritma na podskupinama MoNLI. To predlaže da model BERT najmanje djelomično uključuje teoriju leksičke namjere i negacije na algoritmičkoj razini.', 'de': 'Wir untersuchen, ob neuronale Modelle für Natural Language Inference (NLI) die kompositorischen Wechselwirkungen zwischen lexikalischem Implementieren und Negation erlernen können, indem wir vier Methoden verwenden: die Verhaltensbewertungsmethoden von (1) Challenge Testsets und (2) systematischen Generalisierungsaufgaben sowie die strukturellen Auswertungsmethoden von (3) Sonden und (4) Interventionen. Um diese ganzheitliche Bewertung zu erleichtern, stellen wir Monotonicity NLI (MoNLI) vor, einen neuen naturalistischen Datensatz, der sich auf lexikalische Einbeziehung und Negation konzentriert. In unseren Verhaltensanalysen stellen wir fest, dass Modelle, die auf universellen NLI-Datensätzen trainiert wurden, systematisch an MoNLI-Beispielen scheitern, die Negation enthalten, aber dass MoNLI-Feinabstimmung diesen Fehler behebt. In unseren Strukturauswertungen suchen wir nach Beweisen dafür, dass unser hochperformantes BERT-basiertes Modell gelernt hat, den Monotonizitätsalgorithmus hinter MoNLI zu implementieren. Sonden liefern Beweise, die mit dieser Schlussfolgerung übereinstimmen, und unsere Interventionsexperimente unterstützen dies, indem sie zeigen, dass die kausale Dynamik des Modells die kausale Dynamik dieses Algorithmus auf Teilmengen von MoNLI widerspiegelt. Dies deutet darauf hin, dass das BERT-Modell zumindest teilweise eine Theorie der lexikalischen Einbeziehung und Negation auf algorithmischer Ebene einbettet.', 'id': 'Kami mengatasi apakah model saraf untuk Inferensi Bahasa Alami (NLI) dapat mempelajari interaksi komposisi antara entailment leksikal dan negatif, menggunakan empat metode: metode evaluasi perilaku dari (1) set tes tantangan dan (2) tugas generalisasi sistematis, dan metode evaluasi struktur dari (3) sonda dan (4) intervensi. Untuk memudahkan evaluasi holistik ini, kami mempersembahkan Monotonicity NLI (MoNLI), set data alam baru yang fokus pada penyelesaian dan negati lexik. Dalam evaluasi perilaku kami, kami menemukan bahwa model yang dilatih pada set data NLI tujuan umum gagal secara sistematis pada contoh MoNLI yang mengandung negatif, tetapi bahwa penyesuaian MoNLI mengatasi gagal ini. Dalam evaluasi struktur kami, kami mencari bukti bahwa model terbaik kami berdasarkan BERT telah belajar untuk mengimplementasi algoritma monotonisitas di balik MoNLI. Probe memberikan bukti yang konsisten dengan kesimpulan ini, dan eksperimen intervensi kami meningkatkan ini, menunjukkan bahwa dinamika penyebab model mirror dinamika penyebab algoritma ini pada subset MoNLI. Ini menunjukkan bahwa model BERT setidaknya sebagian memasukkan teori penyelesaian dan negati lexik pada tingkat algoritma.', 'sw': 'Tunaongelea kama miundo mbinu ya asili ya Kungilia Lugha ya Kiasili (NLI) inaweza kujifunza mahusiano ya mikutano kati ya maarifa ya kisaikolojia na chuki, kwa kutumia njia nne: mbinu za uchunguzi wa tabia (1) za mashindano ya changamoto na (2) kazi za uzalishaji za mfumo, na mbinu za uchunguzi wa miundo mbinu (3) za wateja mahojiano na (4) kuingilia kati. Ili kusaidia uchunguzi huu wa kitakatifu, tunatoa NLI (MoNLI), kituo kipya cha taarifa za asili kilichojikita kwenye suala la watu wasiofahamika na ubaguzi. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure.  Katika tathmini zetu za miundombinu, tunatafuta ushahidi kuwa mtindo wetu wa msingi wa BERT wa juu umejifunza kutekeleza utambulisho wa kidini nyuma ya MoNLI. Mashambulizi yanatoa ushahidi ukilinganishwa na hitimisho hili, na majaribio yetu ya kuingilia yanazuia hili, wakionyesha kwamba mabadiliko yanayosababishwa na muundo huo yanaonyesha mabadiliko ya utambulisho huu kwenye vipande vya MoNLI. Hii inapendekeza kwamba modeli ya BERT angalau kwa kiasi fulani inaweka nadharia ya maarifa na chuki kwa kiwango cha algorithi.', 'tr': "Natural Language Inference (NLI) üçin neural modelleriň leksik jemgyýet we negasiýasynyň arasynda kompozisyonal terjimeleri öwrenip biljekdigini we ol dört yöntemlerden ullanyp bilýäris. Bu holistik değerlendirmeyi kolaylaştırmak üçin, Monotonik NLI (MoNLI) 'yi teslim ediyoruz. Täze bir naturalistik veri seti leksik sorumluluğa ve bölümlenmeye odaklanmıştır. Davranyşymyz barlag çykyşlarynda, NLI maglumatyň halkara maksadyň üstünde eğlenen nusgalaryň MoNLI misallerinde sistematik sebäpli boýunça ýok däldir emma MoNLI howpsuzlyk üçin hat çykyş edip barýar. Biziň struktur deňlemelerimizde, BERT'yň üst üsti bellenilýän nusgamyzyň MoNLI arkasyndaky monotoniýa algoritminiň implementasyny öwrendigini gözleýäris. Sonuçlar bu çözüm ile uygun bir kanıt sağlayar, ve bizim aralygymyz deneylerimiz bu şekilde bolup geçirir. Modelin aynalaryň nedeniyle bu algoritmin MoNLI subüntlerindeki sebep dinamiklerini gösterir. Bu BERT modeliniň iň azyndan hem bir läksik baglama we negasiýasynyň teoriýasyny algoritmik derejede içerilýär.", 'ko': '우리는 네 가지 방법을 사용하여 자연언어추리(NLI)의 신경모델이 어휘의 함축과 부정 간의 조합 상호작용을 배울 수 있는지 연구한다. 도전 테스트집과 시스템 개괄 임무의 행위 평가 방법, 그리고 (3) 탐지와 (4) 관여하는 구조 평가 방법이다.이러한 전체적인 평가를 촉진하기 위해 우리는 단조로운 NLI(MonLI)를 제기했다. 이것은 새로운 자연주의 데이터 집합으로 어휘의 함축과 부정에 주목한다.우리의 행위 평가에서, 우리는 유니버설 NLI 데이터 집합에서 훈련된 모델이 부정을 포함하는 MonLI 예시에서 체계적으로 실패한 것을 발견했지만, MonLI 마이크로스피커가 이 실패를 해결했다.우리의 구조 평가에서 우리는 우리의 가장 좋은 버트 기반 모델이 MonLI 배후의 단조로운 알고리즘을 실현하는 것을 배웠다는 증거를 찾았다.탐침이 만들어낸 증거는 이 결론과 일치하며 우리의 관여 실험이 이를 뒷받침하고 모델의 인과동력학이 MonLI 서브집합에 있는 이 알고리즘의 인과동력학을 반영했음을 보여준다.이는 버트 모델이 적어도 알고리즘 차원에서 어휘의 함축과 부정 이론을 삽입한 것을 보여준다.', 'fa': 'ما با استفاده از چهار روش استفاده می\u200cکنیم آیا مدل عصبی برای تفاوت زبان طبیعی (NLI) می\u200cتواند تفاوت ترکیبی بین تفاوت زبان و منفی را یاد بگیرد، با استفاده از چهار روش استفاده می\u200cکند: روش ارزیابی رفتاری تفاوت\u200cهای (1) تفاوت\u200cها و (2) وضعیت\u200cهای عمومی\u200cسازی سیستمی و روش برای آسانی این ارزیابی holistic، ما یک مجموعه داده\u200cهای طبیعی جدید که روی ارزیابی و منفی زبان\u200cشناسی تمرکز می\u200cکنیم. در ارزیابی رفتاری ما، می\u200cبینیم که مدل\u200cهای آموزش یافته در مجموعه\u200cهای داده\u200cهای NLI عمومی در مورد مثال\u200cهای MONLI که شامل منفی است شکست خورده است، ولی این مدل\u200cهای آموزش داده\u200cهای MONLI به این شکست دریافت می\u200cکند. در ارزیابی ساختاری ما، ما دنبال مدرک می گردیم که مدرک بالاترین عملکرد BERT ما یاد گرفته است که الگوریتم monotonicity پشت MONLI را انجام دهد. تحقیقات مدرک\u200cهایی که با این نتیجه مواجه می\u200cشوند، و تجربه\u200cهای مهاجرت ما این را بالا می\u200cبرند، نشان می\u200cدهند که دینامیک\u200cهای دلیل آینده\u200cی مدل دینامیک\u200cهای دلیل این الگوریتم را در زیر زیر زیر\u200cهای MONLI نشان می\u200cدهد. این پیشنهاد می\u200cدهد که مدل BERT حداقل تقسیم یک تئوری از اراده\u200cی زبانی و منفی در سطح الگوریتم قرار می\u200cدهد.', 'af': "Ons adres of neurale modele vir Natuurlike Taal Inferensie (NLI) die samenskaplike interaksies tussen leksiese inhaling en negasie kan leer, gebruik vier metodes: die gedrag evaluering metodes van (1) uitdrukking toets stelle en (2) sistematiese generalisering taak, en die struktuurlike evaluering metodes van (3) probes en (4) intervencies. Om hierdie holistiese evaluasie te eenvoudig, laat ons Monotonisiteit NLI (MoNLI) voorsien, 'n nuwe natuurlike datastel op leksiese opstanding en negasie gefokus word. In ons gedrag evaluasies, vind ons dat model wat op algemene- doel opgelei is op NLI datastelle misluk systematiese op MoNLI voorbeelde wat negasie bevat, maar dat MoNLI fyn- tuning adresse hierdie mislukking. In ons strukturele evaluasies, soek ons na getuienis dat ons bo-uitvoerde BERT-gebaseerde model geleer het om die monotonisiteit algoritme agter MoNLI te implementeer. Probes gee getuienis wat ooreenkomstig met hierdie konklusie, en ons intervensie eksperimente sal hierdie ooreenkomstige dynamiek van die model spegelaat die oorsaaklike dinamike van hierdie algoritme op subartikels van MoNLI. Hierdie stel voorstel dat die BERT model ten minste gedeeltelik 'n teorie van leksiese verhouding en negasie by 'n algoritmiske vlak inbeter.", 'sq': 'Ne trajtojmë nëse modelet neuronale për Inferencën e Gjuhave Natyrore (NLI) mund të mësojnë ndërveprimet kompozitive midis përfshirjes lexike dhe negativës, duke përdorur katër metoda: metodat e vlerësimit sjellësor të (1) grupeve testesh të sfidave dhe (2) detyrave sistematike të gjeneralizimit dhe metodat strukturore të vlerësimit të (3) sondave dhe (4) ndërhyrjeve. Për të lehtësuar këtë vlerësim holistik, ne paraqesim Monotonicity NLI (MoNLI), një grup të ri të dhënash naturaliste fokusuar në përfshirjen lexike dhe negativën. Në vlerësimet tona të sjelljes, gjejmë se modelet e stërvitura me qëllim të përgjithshëm të të dhënave NLI dështojnë sistematikisht në shembujt e MoNLI që përmbajnë negativë, por që rregullimi i MoNLI trajton këtë dështim. Në vlerësimet tona strukturore, ne kërkojmë prova se modeli ynë më i mirë me bazë në BERT ka mësuar të zbatojë algoritmin e monotonitetit pas MoNLI. Probat sjellin prova të konsistenta me këtë përfundim, dhe eksperimentet tona ndërhyrjesh e forcojnë këtë, duke treguar se dinamika shkakuese e modelit pasqyron dinamikën shkakuese të këtij algoritmi në nëngrupet e MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.', 'bn': 'প্রাকৃতিক ভাষা ইনফারেন্সের (এনলি) নিউরেল মডেল শিখতে পারে কিনা, চারটি পদ্ধতি ব্যবহার করে লেক্সিক্সিল বিজ্ঞান এবং নেতিবিজ্ঞানের মধ্যে সংগঠনের মধ্যে সংযুক্ত প্রতিক্রিয়া শিখতে পারে: চ্যালেঞ্ এই পবিত্র মূল্যের সুবিধা প্রদান করার জন্য আমরা মনোটোনিকিটি এনলি (মনলিআই) উপস্থাপন করি, একটি নতুন প্রাকৃতিক ডাটাসেট লেক্সিক্সিক বিজ্ঞ In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure.  আমাদের কাঠামোক্তিক পরিমাণে আমরা প্রমাণ খুঁজছি যে আমাদের শীর্ষ ভিত্তিক বের্টি-ভিত্তিক মডেল মোনালির পেছনের দানবিকতার অ্যালগরি প্রোবেস এই সমাপ্তির সাথে প্রমাণ প্রদান করেছে এবং আমাদের হস্তক্ষেপের পরীক্ষা বাড়িয়ে দিচ্ছে যে মডেলের কারণে মোনলির সাবটিতে এই অ্যালগরিদমের কারণে প্রমাণে এটি পরামর্শ দেয় যে বেরেটি মডেল অন্তত অংশের মধ্যে একটি লেক্সিক্সিয়াল বিজ্ঞান এবং অ্যালগরিদমিক স্তরে নেতিবিদ্বেষের তত্ত্', 'bs': 'Mi se obraćamo da li neuronski modeli za prirodnu jezičku infekciju (NLI) mogu naučiti kompozicionalne interakcije između leksičke želje i negacije, koristeći četiri metode: metode procjene ponašanja (1) seta ispitivanja izazova i (2) sistematske generalizacije zadataka i metode strukturne procjene (3) sonde i (4) intervencija. Da bi olakšali ovu holističku procjenu, predstavljamo Monotoničnost NLI (MoNLI), novu prirodnjačku setu podataka usredotočenu na leksičku želju i negaciju. U našim procjenama ponašanja, nalazimo se da modeli obučeni na općem cilju NLI podataka sistematski nisu uspjeli na primjerima MoNLI koji sadrže negaciju, ali da se MoNLI fino navodi na ovaj neuspjeh. U našim strukturnim procjenama, tražimo dokaze da je naš najveći model na BERT-u naučio implementirati algoritam monotoniciteta iza MoNLI-a. Probe daju dokaze u skladu s ovim zaključkom, a naši intervencijski eksperimenti su to povećali, pokazujući da uzrokovana dinamika modela ogledala uzrokovanu dinamiku ovog algoritma na podskupinama MoNLI. To predlaže da model BERT najmanje djelomično uključuje teoriju leksičke želje i negacije na algoritmičkom nivou.', 'am': 'የአፍሪካዊ ቋንቋ ማቀናቀል (NLI) የነጥብ ዓይነቶች በሌክሲካዊ እና በሽብር መካከል የተሰናከረውን ግንኙነት ማወቅ ይችላልን፡፡ ይህንን ቅድስናዊ ማስታወቂያ ለማግኘት፣ አዲስ የፍጥረታዊ ዳታዊ አዲስ ስህተት እና ውጤታዊ ስህተት እና ትክክል ላይ ማቆየትን እናቀርባለን፡፡ በሥርዓታችን ማስታወቂያው፣ በጠቅላላ አዲስ አዲስ ዳታ ማተማር የተጠቃሚ ምሳሌዎች በሞNLI ምሳሌዎች ላይ በተስማማማሚ ስህተት ይሳካሉ፣ ግን ሞNLI በመጠቀም ወደዚህ ስህተት ያሳያል፡፡ በአካባቢነታችን ማስረጃ ውስጥ የBERT-መሠረት ሞዴል በሞንኤሊ ጀርባ የነበረውን የሞቶኒክነት አልጎርቲምን ለማድረግ እንደተማረ ማስረጃዎችን እንፈልጋለን፡፡ ፕሮቤሶች በዚህ ፍጻሜ የሚታወቁ ማስረጃዎችን ይሰጣሉ፤ የሞዴል ዲናሞክራሲ የዚህን አልጎርቲም ውጤት በሞንሊ አካባቢዎች ላይ ያሳያል፡፡ This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.', 'ca': "Ens ocupem de si els models neurals de la Inferència de Llingua Natural (NLI) poden aprendre les interaccions de composició entre l'involucració lèxica i la negatió, fent servir quatre mètodes: els mètodes d'evaluació comportamental de (1) conjunts de provas de desafiament i (2) tasques de generalització sistemàtica, i els mètodes d'evaluació estructural de (3) sondes i (4)  Per facilitar aquesta evaluació holística, presentem la Monotonitat NLI (MoNLI), un nou conjunt de dades naturalistes centrat en l'involucració lexical i la negatió. En les nostres evaluacions de comportament, descobrim que els models entrenats en conjunts de dades de NLI de propòsit general fracassen sistemàticament en exemples de MoNLI que contenen negatió, però que la perfeccionació de MoNLI aborda aquest fracàs. En les nostres evaluacions estructurals busquem proves que el nostre model basat en BERT ha après a implementar l'algoritme de monotonitat darrere del MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI.  Això suggereix que el model BERT, almenys en part, incorpora una teoria d'involucració lexical i negatió a nivell algorítmic.", 'hy': 'Մենք քննարկում ենք, թե արդյոք բնական լեզվի ինֆերանսիայի (ՆԱԼ) նյարդային մոդելները կարող են սովորել լեքսիկական ներգրավման և բացարձակումների միջև կազմակերպված փոխազդեցությունը, օգտագործելով չորս մեթոդներ. 1) մարտահրավերի փորձարկման մեթոդները և 2) սիստեմատիկ ընդհանուր աշխատանքները, ինչպես նաև 3 Այս ամբողջական գնահատման հեշտացնելու համար մենք ներկայացնում ենք Մոնոտոնիստ ՆԼԻ (ՄՕՆԼԻ), նոր նատուրալիստ տվյալների համակարգ, որը կենտրոնացված է լեքսիկական ներգրավման և բացարձակումների վրա: Մեր վարքագծային գնահատումների արդյունքում մենք հայտնաբերեցինք, որ ՆԼՀ-ի ընդհանուր նպատակի տվյալների համակարգերի վրա սովորեցված մոդելները սիստեմատիկ ձախողվում են MoՆԼՀ-ի օրինակների վրա, որոնք պարունակում են բացասականություն, բայց որ ՄՈ Մեր կառուցվածքային գնահատումներում մենք փնտրում ենք ապացույցներ, որ մեր լավագույն արդյունավետ BER-ի հիմնված մոդելը սովորել է կիրառել ՄՕՆԼԻ-ի ետևում գտնվող մոնոտոնիզմի ալգորիթմը: Փորձարկումները ապացույցներ են տալիս, որոնք համապատասխանում են այս եզրակացությանը, և մեր ներգրավման փորձարկումները խրախուսում են սա, ցույց տալով, որ մոդելի պատճառային դինամիկան արտացոլում է այս ալգորիթմի պատճառային դինամիկան MoNSI-ի ենթա Սա առաջարկում է, որ BER մոդելը առնվազն մասամբ ներառում է լեքսիկական ներգրավման և բացասական տեսությունը ալգորիթմային մակարդակում:', 'az': 'Biz təbiətli Dil Inference (NLI) üçün nöral modellərinin leksik hökmləri və negasyon arasındakı müxtəlif tərəfləri öyrənib, dörd metodları istifadə edirik: tədbir sınama metodları və (2) sistematik generalizasiya metodları və (3) sonların və (4) müxtəlif tərəflərinin müxtəlif tərzlərinin müxtəlif tərzlərinin müxtəlif tərzlərinin müxtəlif tərzlərin Bu holistik değerlendirməyi asanlaşdırmaq üçün, biz Monotonicity NLI (MoNLI), yeni naturalistik veri qutusunu leksik hökmləri və negasyonu üzərində təsirləndiririk. Bizim davranışlarımızda, genel məqsədilə təhsil edilmiş NLI veri qurğularında sistematik olaraq imkanları barəsində olan MoNLI məsəllərdə sistematik xəta edir, amma MoNLI təhsil edilməsi bu başarısızlığı təhsil edir. Bizim strukturlu değerlendirmələrimizdə, ən yüksək göstərici BERT modeliyimizin MoNLI arxasındaki monotonitik algoritminin istifadə etməsini öyrəndiyi kanıtları gözləyirik. Sonuçlar bu sonuçla uyğun dəlillərə bənzəyir, və intervencimiz təcrübələrimiz bunu artırar, modelinin nedeniyyətli dinamikinin MoNLI subgruplarında olan bu algoritmin causal dinamikini göstərir. Bu, BERT modeli ən azından bir qismi olaraq leksik hörmət və negatiflik nəzəriyyəsini algoritmik səviyyədə yerləşdirir.', 'cs': 'Zabýváme se tím, zda se neuronové modely pro inferenci přirozeného jazyka (NLI) mohou naučit kompoziční interakce mezi lexikální implikací a negací pomocí čtyř metod: metod behaviorálního hodnocení (1) testovacích sad a (2) systematických zobecňovacích úloh a metod strukturálního hodnocení (3) sond a (4) intervencí. Abychom usnadnili toto celostní hodnocení, představujeme Monotonicity NLI (MoNLI), nový naturalistický datový soubor zaměřený na lexikální implikaci a negaci. V našich hodnoceních chování zjišťujeme, že modely trénované na univerzálních datových sadách NLI systematicky selhávají na příkladech MoNLI obsahujících negaci, ale že jemné ladění MoNLI řeší tuto chybu. V našich strukturálních hodnoceních hledáme důkazy, že náš nejvýkonnější model založený na BERT se naučil implementovat monotonický algoritmus za MoNLI. Sondy přinášejí důkazy odpovídající tomuto závěru a naše intervenční experimenty to podporují a ukazují, že kauzální dynamika modelu odráží kauzální dynamiku tohoto algoritmu na podmnožinách MoNLI. To naznačuje, že BERT model alespoň částečně zakládá teorii lexikální implikace a negace na algoritmické úrovni.', 'et': 'Uurime, kas loodusliku keele järelduse neuromudelid (NLI) suudavad õppida leksikaalse kaasamise ja negatsiooni kompositsioonivastaseid interaktsioone, kasutades nelja meetodit: käitumusliku hindamise meetodid (1) väljakutsetestide komplektides ja (2) süstemaatilise üldistamise ülesannetes ning (3) sondide ja (4) sekkumiste struktuurilise hindamise meetodid. Selle tervikliku hindamise hõlbustamiseks tutvustame Monotonicity NLI (MoNLI), uut naturalistilist andmekogumit, mis keskendub leksikaalsele kaasamisele ja negatsioonile. Meie käitumishinnangutes leiame, et üldotstarbelistel NLI andmekogumitel koolitatud mudelid ebaõnnestuvad süstemaatiliselt MoNLI näidetel, mis sisaldavad negatsiooni, kuid MoNLI peenhäälestus lahendab selle ebaõnnestumise. Struktuurihinnangutes otsime tõendeid selle kohta, et meie parim BERT-põhine mudel on õppinud rakendama MoNLI monotoonsuse algoritmi. Sondid annavad tõendeid, mis on kooskõlas selle järeldusega ja meie sekkumiskatsed toetavad seda, näidates, et mudeli põhjuslik dünaamika peegeldab selle algoritmi põhjuslikku dünaamikat MoNLI alamhulgadel. See näitab, et BERT mudel sisaldab vähemalt osaliselt leksikaalse kaasamise ja negatsiooni teooriat algoritmilisel tasandil.', 'fi': 'Tutkimme, voidaanko luonnollisen kielen inferenssin (NLI) neuromallit oppia lexikaalisen implikaation ja negation välisiä kompositiivisia vuorovaikutuksia käyttäen neljää menetelmää: käyttäytymisen arviointimenetelmiä (1) haastetestisarjoissa ja (2) systemaattisissa yleistystehtävissä sekä rakenteellisia arviointimenetelmiä (3) luotaimilla ja (4) interventioilla. Tämän kokonaisvaltaisen arvioinnin helpottamiseksi esittelemme Monotonicity NLI (MoNLI), uuden naturalistisen aineiston, joka keskittyy sanastoon ja kieltämiseen. Käyttäytymisarvioinneissamme havaitsemme, että yleishyödyllisiin NLI-aineistoihin koulutetut mallit epäonnistuvat systemaattisesti MoNLI-esimerkeissä, jotka sisältävät kieltämistä, mutta MoNLI-hienosäätö korjaa tämän virheen. Rakenteellisissa arvioinneissamme etsimme todisteita siitä, että suorituskykyinen BERT-mallimme on oppinut toteuttamaan MoNLI:n taustalla olevan monotonisuusalgoritmin. Anturit tuottavat näyttöä tämän johtopäätöksen kanssa, ja interventiokokeemme vahvistavat tätä, osoittaen, että mallin syy-dynamiikka heijastaa tämän algoritmin syy-dynamiikkaa MoNLI:n osajoukoissa. Tämä viittaa siihen, että BERT-malli sisältää ainakin osittain teorian sanamuodosta ja kieltämisestä algoritmitasolla.', 'sk': 'Preučujemo, ali se lahko nevronski modeli za sklepanje naravnega jezika (NLI) naučijo kompozicijskih interakcij med leksikalnim posledicam in zanikanjem z uporabo štirih metod: vedenjskih evalvacijskih metod (1) izzivnih testnih nizov in (2) sistematičnih generalizacijskih nalog ter strukturnih evalvacijskih metod (3) sond in (4) intervencij. Da bi olajšali celostno ocenjevanje, predstavljamo Monotonicity NLI (MoNLI), nov naturalistični nabor podatkov, osredotočen na leksikalno posledico in zanikanje. V naših vedenjskih ocenah ugotavljamo, da modeli, usposobljeni za splošno namenske nabore podatkov NLI, sistematično ne uspevajo na primerih MoNLI, ki vsebujejo zanikanje, vendar da MoNLI fino nastavitev obravnava to napako. V naših strukturnih ocenah iščemo dokaze, da se je naš najboljši model BERT naučil implementirati algoritem monotoničnosti za MoNLI. Sonde prinašajo dokaze, ki so skladni s tem sklepom, in naši intervencijski eksperimenti to podpirajo, kažejo, da vzročna dinamika modela odraža vzročno dinamiko tega algoritma na podmnožicah MoNLI. To kaže, da BERT model vsaj delno vključuje teorijo leksikalne posledice in negacije na algoritmični ravni.', 'he': 'אנו מתייחסים אם דוגמנים עצביים עבור השפה הטבעית אינפרנציה (NLI) יכולים ללמוד את האינטראקציות המרכיבית בין התערבות הלקסית לשליטה, באמצעות ארבעה שיטות: שיטות הערכה התנהגותית של (1) קבוצות מבחן אתגרים (2) משימות הגנרליזציה שיטתית, ושיטות הערכה המבנית של (3) סונדים ( כדי להקל על הערכה הקוליסטית הזו, אנחנו מציגים את מונוטוניטיות NLI (MoNLI), קבוצת נתונים טבעית חדשה שמוקדת על התערבות לקסית ושליטה. בהערכות התנהגותית שלנו, אנו מוצאים שמודלים מאומנים על קבוצות נתונים של NLI למטרה כללית נכשלים באופן שיטתי על דוגמאות של MoNLI שמכילים שליטה, אך שהתיקון של MoNLI מטפל בכישלון הזה. בהערכות המבנית שלנו, אנו מחפשים ראיות שהמודל המופעיל ביותר שלנו מבוסס על BERT למד להפעיל את האלגוריתם המונוטוניות מאחורי MoNLI. הניסויים נותנים ראיות מתאימות למסקנה הזו, וניסויים ההתערבות שלנו עודדים את זה, מראים שהדינמיקה הסיבית של המודל מראה את הדינמיקה הסיבית של האלגוריתם הזה על תת-קבוצות של MoNLI. זה מציע שהמודל BERT לפחות חלקית מכיל תיאוריה של התערבות לקסית ושליטה ברמה אלגוריתמית.', 'ha': "Tuna faɗi ko misãlai na takarda wa Infece wa Lugha Natural (NLI) za'a iya amfani da haɗi masu haɗi a tsakanin mataimaki da kuma haske, ko kuma, ana yi amfani da shiryoyin huɗu: hanyor muhalli wa masu aikin muhimmanci (1) ta daidaita jarrabo masu kanana da (2) aikin mai shagala da aikin ajalin da aka ƙayyade (3) da shiryoyin ajiya masu ƙayyade (4). Ko iya sauƙar da wannan littãfi, munã halatar da Monotonicity NLI (MoNLI), wata shekara na takardar da aka yi fokus a kan mutane na zafi. Kayan rabon aikin mu, za mu sãmi misãlai wanda aka yi wa tsari a kan jumla-goan NLI, za'a buƙata misãlai na MoNLI wanda ke ƙunsa da haske, kuma amma, MoNLI mai gyara na samun wannan fizgi ne. Ina ƙidãya masu bincike, munã neman bayani kuwa misalinmu na samar-baƙatan BERT ya sanar da ya cika algoritim bakin MoNLI. Mawallafin na sami da wannan ƙaramar, kuma jarrabai da ke sami wannan, ya nuna cewa misalin hanyoyin misalin ayuka na yi tsuru ga kanada wannan algoritm a kan ƙarƙashin MoNLI. Wannan yana bukãtar da cewa misalin BERT ya ƙunsa da raba wani littafi na zayen mutane da kuma nega a cikin daraja algoritmic.", 'jv': 'Awakdhéwé mengko karo sistem sing dibenalke (NLI) iso nggambar kelangan kelangan langgar sampeyan gambar kelangan lan nggawe barang kelangan (4) kanggo ngejaraké beraksi barêng-barêng iki, kita ngulinakake Monotonik NLI Nang assenting matik dhéwé, kita gawe akeh model sing beraksi barêng-bukané NLI dataset sistematik uwong ing modal nang MoNLI dadi sing beraksi seneng gawe balé, kepiye MoNLI dadi nambah sing paling dhéwé. Nang structural assertions, kita buku nglanggar ngobudhakan kanggo diangkat awak dhéwé luwih-luwih hayo nggawe modèl sing nyebuté BERT wis ngerti Mbak bakal urip nggawe gerambut karo akeh sampek iki, lan arno sakjane sampek karo hal dadi nggawe barang nggawe dolanan dynamics lakusul ning model Iki suggeruju karo model BERT sing paling kelas kotak, akeh téri tahun tekan kelas lan kelas nang sampek Algorithm.', 'bo': 'ང་ཚོས་རང་བཞིན་པའི་སྐད་ཡིག་གཟུགས་ཀྱི་ཐབས་ལམ་ལ་གནད་དོན་དག་ཚད་ལྟར་བསམ་ན། འཇིག་རྟེན་འདི་ལྟ་བུའི་གྲངས་སུ་མཐུན་བཟོ་བྱེད་པར་བཟོ་བཅོས་པ་དེ་ནི་ང་ཚོས་རང་ཉིད་ཀྱི་རྣམ་པ་NLI (MoNLI)ལ་ཡོད། In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure. ང་ཚོའི་ལས་འཆར་གཞུང་གི་དཔྱད་ཞིབ་ལྟར་ན། ང་ཚོའི་མཐོ་རིམ་མཐོང་ནུས་ཡོད་པའི་ BERT་ལྟ་བུའི་རྣམ་གྲངས་ཀ་ལས་སྦྱོར་བའི་སྒྲིག་ཆ་རྐྱེ Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. The results of this algorithm are similar to that of the selected variables. འདིས་BERT་རྣམ་གྲངས་ཀྱི་ནང་དོན་དམིགས་ཡུལ་གྱི་གྲངས་སྒྲིག་དང་ཟུར་བ་དག་གི་སྐྱེས་ཚད་ཅིག་ནང་དུ་བཅུག་པ་དང་།'}
{'en': 'Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation', 'fr': 'Dissection de transformateurs de billets de loterie\xa0: étude structurelle et comportementale de la traduction automatique neuronale clairsemée', 'pt': 'Dissecando Transformadores de Bilhetes de Loteria: Estudo Estrutural e Comportamental da Tradução Automática Neural Esparsa', 'ar': 'تشريح محولات تذاكر اليانصيب: دراسة هيكلية وسلوكية لترجمة الآلة العصبية المتفرقة', 'es': 'Disección de transformadores de billetes de lotería: estudio estructural y de comportamiento de la traducción automática neuronal dispersa', 'hi': 'विच्छेदन लॉटरी टिकट ट्रांसफॉर्मर: विरल तंत्रिका मशीन अनुवाद के संरचनात्मक और व्यवहार अध्ययन', 'ja': '解剖宝くじトランスフォーマー：まばらな神経機械翻訳の構造と行動の研究', 'zh': '析彩票变形金刚:疏神经机器译,行也', 'ru': 'Рассекающие трансформаторы лотерейных билетов: структурное и поведенческое исследование редкого нейронного машинного перевода', 'ga': 'Claochladáin Ticéad Crannchuir a Dhíroinnt: Staidéar Struchtúrtha agus Iompraíochta ar Aistriúchán Meaisíní Néaracha Gann', 'ka': 'ლოტერიის ტიკეტის ტრანფორმაციების გაყოფილი: სტრუქტურული და ქეგურაციის შესწავლება', 'el': 'Διάταξη μετασχηματιστών λαχειοφόρων εισιτηρίων: Δομική και Συμπεριφερειακή Μελέτη Σπανιών Νευρικών Μηχανικών Μεταφράσεων', 'hu': 'Lottószelvény transzformátorok disszekciója: a ritka neurális gépi fordítás strukturális és viselkedési vizsgálata', 'it': 'Dissettare i trasformatori dei biglietti della lotteria: studio strutturale e comportamentale della traduzione automatica neurale sparsa', 'kk': 'Лотериялық беттерді түрлендірушілерді бөліп тастау: Қосымшалық нейралық машинаның аудармасының структуралық және қасиеттерді зерттеу', 'lt': 'Loterijos bilietų keitikliai: struktūrinis ir elgsenos tyrimas sparčiojo nervinės mašinos vertimo būdu', 'mk': 'Трансформирачи на лотарски билети: Структурна и однесувачка студија за преведување на нервозни машини', 'ml': 'ലോട്ടറി ടിക്കറ്റി ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d വിതരണം ചെയ്യുന്നു: സ്പെയിന്\u200dസ് നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷ', 'mn': 'Лотерийн салбарын шилжүүлэгчид бөлөөлөх: Структурал болон Behavioral Study of Sparse Neural Machine Translation', 'ms': 'Penukar Tiket Loteri Mempecah: Pelajaran Struktur dan Perilaku Terjemahan Mesin Neural Tersingkat', 'mt': 'Trasformaturi tal-Karti tal-Lotterija li Jqassmu: Studju Strutturali u ta’ l-Imġiba ta’ Traduzzjoni ta’ Magni Newrali Sparse', 'no': 'Avskjæring av lotteriske kartomformarar: Struktural og oppførsel- studie av omsetjing av avslag neuralmaskin', 'pl': 'Analiza transformatorów losów loterii: badanie strukturalne i behawioralne rzadkiego neuronowego tłumaczenia maszynowego', 'ro': 'Disectarea transformatorilor de bilete de loterie: Studiul structural și comportamental al traducerii automate neurale rare', 'sr': 'Razvajanje Loterijskih kartica: Strukturalno i ponašanje istraživanja prijevoza neuroloških mašina', 'si': 'ලොට්\u200dරියි ටිකෙට් වෙනස් කරනවා: ස්ට්\u200dරූක්ටරුල් සහ ව්\u200dයාපෘතික අභ්\u200dයාසයේ ස්පාර්ස් න්\u200dයූරාල් මැෂ', 'so': 'Dissecting Lottery Ticket Transformers: structural and Behavior Study of Sparse Neural Machine Translation', 'sv': 'Dissecting Lottery Ticket Transformers: Struktur- och beteendestudie av sparsam neural maskinöversättning', 'ta': 'முழுமையான குறிப்பு மாற்றுபவர்களை விட்டுவிடு', 'ur': 'لوٹری ٹیکٹ تبدیل کرنے والوں کو تقسیم کرتا ہے: اسپارس نیورال ماشین تعلیم کی ساختاری اور رفتاری تحقیق', 'uz': 'Name', 'vi': 'Dịch biến hình Vé nhòe nhoẹt: Nghiên cứu cấu trúc và hành vi về máy móc thần kinh', 'bg': 'Разпространяване на лотарийни билетни трансформатори: структурно и поведенческо изследване на оскъдния неврален машинен превод', 'hr': 'Raskomadanje Loterijskih preobraćaja kartica: Strukturalno i ponašanje ispitivanja prijevoza neuroloških strojeva', 'da': 'Dissecting Lottery Ticket Transformers: Strukturel og adfærdsmæssig undersøgelse af sparsom neural maskinoversættelse', 'nl': 'Het ontleden van loterijkaarttransformaties: Structurele en Gedragsstudie van schaarse neuronale machinevertaling', 'ko': '복권 변압기 분석: 희소 신경 기계 번역의 구조와 행위 연구', 'fa': 'تقسیم کردن تغییرات برچسب\u200cهای لوتری: مطالعه ساختاری و رفتاری از ترجمه ماشین عصبی فضایی', 'de': 'Analyse von Lotteriescheintransformatoren: Struktur- und Verhaltensstudie zur spärlichen neuronalen maschinellen Übersetzung', 'tr': 'Loteriýa bilen terjimelerini pozmak: structural and Behavioral Study of Sparse Neural Machine Translation', 'sw': 'Utafiti wa Kiteknolojia: Utafiti wa Miundombinu na Utafiti wa Utafiti wa Kihispania Utafiti wa Mashine ya Kiurahisi', 'af': 'Skakel Lotterie Tikket Transformeerders: Struktuurale en Gedrag Studie van Sparse Neurale Masjien Vertaling', 'am': 'Transformers: Structural and Behavior Study of Sparse Neural Machine translation', 'az': 'Lotteri etiket transformatçıları parçalayır: Sərər nöral maşına çevirilməsinin strukturlu və davranışlıq öyrənməsi', 'bn': 'লোটারি টিকেট ট্রান্সফর্মার বিভক্ত করা হচ্ছে: স্পের্স নিউরাল মেশিন অনুবাদ', 'id': 'Transformer Tiket Loteri Memotong: Pengelajaran Struktur dan Perilaku Terjemahan Mesin Neural Tanpa', 'bs': 'Raspoređivanje Loterijskih preobraćaja kartica: Strukturalno i ponašanje proučavanja neuralnog preobraćanja', 'ca': 'Transformers de bitllets de loteria disseccionants: Estudio estructural i comportamental de traducció de màquines neuronals ràpides', 'hy': 'Բաժանող վիճակախաղի տոմսերի փոխակերպողները. Փոքր նյարդային մեքենայի թարգմանման կառուցվածքային և վարքագծային ուսումնասիրություն', 'cs': 'Disekce transformátorů loterijních lístků: Strukturální a behaviorální studie řídkého neuronového strojového překladu', 'et': 'Loteripiletite muundajate levitamine: hõreda neuroalse masintõlke struktuuri- ja käitumisuuring', 'fi': 'Lottery Ticket Transformers: Harvoin hermojen konekäännöksen rakenne- ja käyttäytymistutkimus', 'sq': 'Transformuesit e biletave të shpërndarë të lotisë: Studimi strukturor dhe sjelljeje i përkthimit të makinës së shpejtë nervore', 'jv': 'Dissection', 'ha': 'Transformers: Fractural and Atomic Research of spaspastic Neural Machine Translate', 'sk': 'Disekacija loterijskih vstopnic transformatorjev: strukturna in vedenjska študija redkega živčnega strojnega prevajanja', 'he': 'משתני כרטיסי לוטריה מתפרקים: מחקר מבנה ותנהגות של תרגום של מכונות נוירות נמוכות', 'bo': 'Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation'}
{'en': 'Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for  NMT  while maintaining  BLEU . However, it is unclear how such pruning techniques affect a  model ’s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more  encoding . Attention mechanisms remain remarkably consistent as sparsity increases.', 'es': 'El trabajo reciente sobre la hipótesis del billete de lotería ha producido Transformers muy dispersos para NMT mientras se mantiene BLEU. Sin embargo, no está claro cómo estas técnicas de poda afectan las representaciones aprendidas de un modelo. Al sondear Transformers con más y más pesos de baja magnitud eliminados, descubrimos que la información semántica compleja es la primera en degradarse. El análisis de las activaciones internas revela que las capas superiores divergen más a lo largo de la poda, convirtiéndose gradualmente en menos complejas que sus contrapartes densas. Mientras tanto, las primeras capas de modelos dispersos comienzan a realizar más codificación. Los mecanismos de atención siguen siendo notablemente consistentes a medida que aumenta la dispersión.', 'ar': 'أنتج العمل الأخير على فرضية بطاقة اليانصيب محولات متفرقة للغاية لـ NMT مع الحفاظ على BLEU. ومع ذلك ، فمن غير الواضح كيف تؤثر تقنيات التقليم هذه على التمثيلات المكتسبة للنموذج. من خلال فحص المحولات التي تحتوي على المزيد والمزيد من الأوزان المنخفضة الحجم التي تم تشذيبها بعيدًا ، نجد أن المعلومات الدلالية المعقدة هي أولًا يتم تدهورها. يكشف تحليل التنشيطات الداخلية أن الطبقات العليا تتباعد أكثر على مدار عملية التقليم ، وتصبح تدريجياً أقل تعقيدًا من نظيراتها الكثيفة. وفي الوقت نفسه ، تبدأ الطبقات المبكرة من النماذج المتفرقة في أداء المزيد من التشفير. تظل آليات الانتباه متسقة بشكل ملحوظ مع زيادة التباين.', 'pt': 'Trabalhos recentes sobre a hipótese do bilhete de loteria produziram Transformers altamente esparsos para NMT, mantendo o BLEU. No entanto, não está claro como essas técnicas de poda afetam as representações aprendidas de um modelo. Ao sondar Transformadores com pesos cada vez mais de baixa magnitude removidos, descobrimos que a informação semântica complexa é a primeira a ser degradada. A análise das ativações internas revela que as camadas mais altas divergem mais ao longo da poda, tornando-se gradualmente menos complexas do que suas contrapartes densas. Enquanto isso, as primeiras camadas de modelos esparsos começam a realizar mais codificação. Os mecanismos de atenção permanecem notavelmente consistentes à medida que a dispersão aumenta.', 'fr': "Des travaux récents sur l'hypothèse du billet de loterie ont produit des transformateurs très clairsemés pour NMT tout en maintenant l'UEBL. Cependant, il n'est pas clair comment ces techniques d'élagage affectent les représentations apprises d'un modèle. En sondant les Transformers avec de plus en plus de poids de faible magnitude éliminés, nous découvrons que les informations sémantiques complexes sont les premières à être dégradées. L'analyse des activations internes révèle que les couches supérieures divergent le plus au cours de la taille, devenant progressivement moins complexes que leurs homologues denses. Pendant ce temps, les premières couches de modèles épars commencent à effectuer davantage d'encodage. Les mécanismes d'attention restent remarquablement cohérents à mesure que la clairsemence augmente.", 'ja': '宝くじの仮説に関する最近の研究では、BLEUを維持しながらNMTのための非常にまばらなトランスフォーマーが作られています。しかしながら、そのような枝刈り技術がモデルの学習された表現にどのように影響するかは不明である。より多くの低マグニチュードの重みを刈り取ったトランスフォーマーを探索することで、複雑な意味情報が最初に劣化することがわかります。内部活性化の分析により、高層層は枝刈りの過程で最も発散し、密度の高い層よりも徐々に複雑になっていくことが明らかになりました。一方、まばらなモデルの初期の層は、より多くの符号化を実行し始めます。注目メカニズムは、希少性が増加するにつれて著しく一貫したままである。', 'ru': 'Недавняя работа над гипотезой лотерейных билетов произвела крайне редкие Трансформеры для NMT при сохранении BLEU. Однако неясно, как такие методы обрезки влияют на полученные представления модели. Зондируя Трансформаторы с все большим и большим количеством урезанных весов малой величины, мы обнаруживаем, что сложная семантическая информация сначала деградирует. Анализ внутренних активаций показывает, что более высокие слои расходятся больше всего в ходе обрезки, постепенно становясь менее сложными, чем их плотные аналоги. Между тем, ранние слои разреженных моделей начинают выполнять больше кодирования. Механизмы привлечения внимания остаются в высшей степени последовательными по мере увеличения их скудности.', 'zh': '近彩票伪事NMT生疏变形金刚,兼守BLEU。 然尚未详此类修剪之术,何以加于模形之学也。 探剪愈多低量级权变形金刚见语义先降。 内激活之分析表明,剪剪之际,高者最大,渐不如密者应物杂。 同时疏早期,更行更多编码。 随疏增益,意机犹一。', 'hi': 'लॉटरी टिकट परिकल्पना पर हाल के काम ने BLEU को बनाए रखते हुए NMT के लिए अत्यधिक विरल ट्रांसफॉर्मर का उत्पादन किया है। हालांकि, यह स्पष्ट नहीं है कि इस तरह की छंटाई तकनीकें मॉडल के सीखे गए अभ्यावेदन को कैसे प्रभावित करती हैं। अधिक से अधिक कम परिमाण वजन के साथ ट्रांसफॉर्मर की जांच करके, हम पाते हैं कि जटिल शब्दार्थ जानकारी को सबसे पहले नीचा दिखाया जाना है। आंतरिक सक्रियणों के विश्लेषण से पता चलता है कि उच्च परतें छंटाई के दौरान सबसे अधिक अलग हो जाती हैं, धीरे-धीरे उनके घने समकक्षों की तुलना में कम जटिल हो जाती हैं। इस बीच, विरल मॉडल की शुरुआती परतें अधिक एन्कोडिंग करना शुरू कर देती हैं। ध्यान तंत्र उल्लेखनीय रूप से सुसंगत रहते हैं क्योंकि स्पार्सिटी बढ़ जाती है।', 'ga': 'Tá obair le déanaí ar an hipitéis ticéad crannchuir tar éis Claochladáin an-bheag a tháirgeadh do NMT agus BLEU á chothabháil. Níl sé soiléir, áfach, conas a théann a leithéid de theicnící bearradh i bhfeidhm ar léirithe foghlamtha samhla. Trí iniúchadh a dhéanamh ar Chlaochladáin a bhfuil níos mó agus níos mó meáchain íseal-mhéid acu á ngearradh ar shiúl, feicimid go bhfuil faisnéis shéimeantach chasta le díghrádú ar dtús. Léiríonn anailís ar ghníomhartha inmheánacha gurb iad na sraitheanna níos airde an difríocht is mó le linn bearradh, ag éirí níos lú casta de réir a chéile ná a gcomhghleacaithe dlúth. Idir an dá linn, tosaíonn sraitheanna luath de mhúnlaí tanaí ag déanamh ionchódú níos mó. Fanann meicníochtaí aird thar a bheith comhsheasmhach de réir mar a mhéadaíonn an teimhneacht.', 'el': 'Πρόσφατες εργασίες σχετικά με την υπόθεση λαχειοφόρων λαχειοφόρων εισιτηρίων έχουν δημιουργήσει εξαιρετικά σπάνιους μετασχηματιστές για NMT διατηρώντας παράλληλα BLEU. Ωστόσο, δεν είναι σαφές πώς τέτοιες τεχνικές κλαδέματος επηρεάζουν τις διδαγμένες αναπαραστάσεις ενός μοντέλου. Εξετάζοντας τους μετασχηματιστές με όλο και περισσότερα βάρη χαμηλού μεγέθους που κόβονται μακριά, διαπιστώνουμε ότι οι σύνθετες σημασιολογικές πληροφορίες πρέπει πρώτα να υποβαθμιστούν. Η ανάλυση των εσωτερικών ενεργοποιήσεων αποκαλύπτει ότι τα υψηλότερα στρώματα αποκλίνουν περισσότερο κατά τη διάρκεια του κλαδέματος, γίνονται σταδιακά λιγότερο πολύπλοκα από τα πυκνά ομόλογά τους. Εν τω μεταξύ, τα πρώτα στρώματα των αραίων μοντέλων αρχίζουν να εκτελούν περισσότερη κωδικοποίηση. Οι μηχανισμοί προσοχής παραμένουν αξιοσημείωτα συνεπείς καθώς αυξάνεται η σπανιότητα.', 'hu': 'A lottószelvény hipotézisének közelmúltbeli munkája rendkívül ritka transzformátorokat hozott létre NMT-hez, miközben fenntartja a BLEU-t. Nem világos azonban, hogy az ilyen metszési technikák hogyan befolyásolják a modell tanult reprezentációit. Egyre több és több alacsony nagyságrendű transzformátort vizsgálva úgy találjuk, hogy az összetett szemantikai információk elsőként romlanak le. A belső aktivációk elemzése azt mutatja, hogy a magasabb rétegek a metszés során a legtöbbször eltérnek, fokozatosan kevésbé bonyolultak, mint sűrű társaik. Eközben a ritka modellek korai rétegei elkezdenek több kódolást végezni. A figyelmeztetési mechanizmusok továbbra is rendkívül következetesek maradnak, ahogy a ritkaság növekszik.', 'it': "Il recente lavoro sull'ipotesi dei biglietti della lotteria ha prodotto Transformers molto scarsi per NMT pur mantenendo BLEU. Tuttavia, non è chiaro come tali tecniche di potatura influenzino le rappresentazioni apprese di un modello. Analizzando Transformers con sempre più pesi di bassa magnitudine tagliati via, scopriamo che le informazioni semantiche complesse vengono prima degradate. L'analisi delle attivazioni interne rivela che gli strati più alti differiscono maggiormente nel corso della potatura, diventando gradualmente meno complessi rispetto alle loro controparti dense. Nel frattempo, i primi livelli di modelli rari iniziano ad eseguire più codifica. I meccanismi di attenzione rimangono notevolmente coerenti man mano che aumenta la scarsità.", 'lt': "Neseniai atliktas loterijos bilietų hipotezės tyrimas sukėlė labai retas NMT transformatorius ir kartu išlaikė BLEU. However, it is unclear how such pruning techniques affect a model's learned representations.  By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded.  Vidaus aktyvinimo analizė rodo, kad aukštesni sluoksniai labiausiai svyruojant skiriasi, laipsniškai tampa mažiau sudėtingi nei jų tankūs lygiaverčiai sluoksniai. Tuo tarpu pradedami naudoti ankstyvuosius nedidelių modelių sluoksnius. Atsižvelgiant į tai, kad sparčiai didėja, dėmesio mechanizmai išlieka nepaprastai nuoseklūs.", 'mk': 'Неодамнешната работа на хипотезата за лотарски билети произведе многу ретки Трансформери за НМТ, истовремено одржувајќи го БЛЕ. Сепак, не е јасно како ваквите техники на исцепување влијаат на научените претставувања на еден модел. Со истражување на Трансформерите со сé повеќе и пониски тегови, откриваме дека комплексните семантични информации прво се деградираат. Анализата на внатрешните активации открива дека повисоките слоеви се разликуваат најмногу во текот на прскањето, постепено станувајќи помалку комплексни од нивните густи колеги. Во меѓувреме, раните слоеви на мали модели почнуваат да извршуваат повеќе кодирање. Механизмите за внимание остануваат исклучително константни со зголемувањето на реткоста.', 'ms': 'Kerja baru-baru ini pada hipotesis tiket loteri telah menghasilkan Transformers yang sangat jarang untuk NMT semasa mengekalkan BLEU. Namun, tidak jelas bagaimana teknik pemotongan seperti ini mempengaruhi perwakilan belajar model. Dengan menyelidiki Transformers dengan berat-berat yang lebih rendah dan lebih rendah dipotong, kita mendapati bahawa maklumat semantik kompleks adalah pertama untuk dihina. Analisi aktivasi dalaman menunjukkan bahawa lapisan yang lebih tinggi bergerak paling dalam perjalanan pemotongan, secara perlahan-lahan menjadi lebih rumit daripada rakan-rakan yang padat. Sementara itu, lapisan awal model jarang mula melakukan pengekodan lebih. Mekanisme perhatian tetap sangat konsisten semasa kecepatan meningkat.', 'mt': 'Xogħol reċenti dwar l-ipoteżi tal-biljetti tal-lotterija pproduċa Transformers rari ħafna għall-NMT filwaqt li żammet il-BLEU. Madankollu, mhuwiex ċar kif dawn it-tekniki ta’ pruning jaffettwaw ir-rappreżentazzjonijiet miksuba ta’ mudell. Billi jinstabu Transformers b’piżijiet ta’ daqs dejjem aktar baxx imnaqqsa ’l bogħod, isibu li l-informazzjoni semantika kumplessa l-ewwel trid tiġi degradata. L-analiżi tal-attivazzjonijiet interni turi li saffi ogħla jvarjaw l-aktar matul il-pruning, u gradwalment isiru inqas kumplessi mill-kontropartijiet densi tagħhom. Sadanittant, saffi bikrin ta’ mudelli żgħar jibdew iwettqu aktar kodifikazzjoni. Il-mekkaniżmi ta’ attenzjoni jibqgħu konsistenti b’mod notevoli hekk kif tiżdied l-iskarsezza.', 'ml': 'ലോട്ടറി ടിക്കറ്റ് ഹൈപ്പിറ്റസിസിന്റെ അടുത്ത പ്രവര്\u200dത്തിയ്ക്കുന്നത് ബിലിയു സൂക്ഷിച്ചുകൊണ്ടിരിക്കുമ്പോ എങ്കിലും ഒരു മോഡലിന്റെ പഠിച്ച പ്രതിനിധികളെ എങ്ങനെ ബാധിക്കുന്നുവെന്ന് അത് വ്യക്തമായിട്ടില്ല. ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d പരിശോധിക്കുന്നത് കൂടുതല്\u200d കുറച്ചും കൂടുതല്\u200d വലിപ്പമുള്ള തൂക്കങ്ങള്\u200d നീക്കം ചെയ്യുന്നതാണെന്ന് നമുക ആന്തരീക പ്രവര്\u200dത്തനങ്ങളുടെ അന്വേഷണം വെളിപ്പെടുത്തുന്നു, കൂടുതല്\u200d തട്ടുകള്\u200d ബുദ്ധിമുട്ടിയുടെ കാര്യത്തില്\u200d ഏറ്റവും വികസിച് അതുകൊണ്ട്, സ്പാസ് മോഡലുകളുടെ ആദ്യമായ തലകള്\u200d കൂടുതല്\u200d കോഡിങ് പ്രവര്\u200dത്തിപ്പിക്കാന്\u200d തുടങ്ങി. ശ്രദ്ധിക്കുന്ന മെക്കിനസികങ്ങള്\u200d സ്പെയിസിറ്റി വര്\u200dദ്ധിപ്പിക്കുന്നത് പോലെയാണ്.', 'no': 'Nyleg arbeidet på lotteringshypotesien har produsert svært sparse transformerer for NMT under behandling av BLEU. Det er imidlertid ukjent korleis slike teknikk påvirkar ein modell lærte representasjonar. Ved å prøve transformerer med meir og meir låg størrelsesvekt, finn vi at komplekse semantiske informasjon først skal degraderast. Analyser av interne aktivasjonar viser at høgare lag forskjeller dei fleste over trekking, og gradvis blir mindre komplekse enn dei tette trekantane. I mellomtida startar tidlegare lag med sparse modeller med meir koding. Merknadsmekanismar blir merkelig konsistent når sparsitet øker.', 'mn': 'Сүүлийн үед Лотерийн бичлэгийн таамаглалын талаар ажиллах нь БЛУ-г хадгалах үед NMT-ийн төлөөлөгчийн төлөөлөгчийн төлөөлөгчийг бүтээсэн. Гэхдээ ийм загварын сурсан үзүүлэлтийг хэрхэн нөлөөлж байгааг мэдэхгүй. Трансформацуудыг илүү бага хэмжээтэй судалгаагаар бид комплекс семантик мэдээллийг эхлээд ухамсарлах болно. Дотоод дахь үйл ажиллагааны шинжилгээс илүү өндөр давхар нь хамгийн их хэмжээний хувьд хуваагдаж, мөргөн хамтрагчдаас бага цогц болж байна. Гэвч эхний загварын давхар нь илүү кодлого хийж эхэлсэн. Хэрэв анхаарлын механизм нэмэгдэхэд гайхалтай байдаг.', 'ka': 'მხოლოდ ლოტერიის ჰიპოტეზაზე სამუშაო სამუშაო ტრანფორმეტრები NMT-სთვის გამოყენება BLEU-სთვის. მაგრამ არ არის წარმოიდგინელი, როგორ ასეთი წარმოდგინული ტექნოგიები მოდელის გასწავლილი რესპენტირებების შესახებ. ტრანფორმენტირების შესაბამისთვის უფრო და უფრო მეტი მანგნიტური მანგნიტური გაზრუნებით, ჩვენ ვიღებთ, რომ კომპლექსი სიმენტიკური ინფორმაცია პირველი და ინტერული აქტივაციების ანალიზი აღმოჩნდება, რომ უფრო მეტი სინამდვილეები უფრო მეტი სინამდვილეების განსხვავება, რომელიც უფრო მეტი სინამდვილეების განსხვავება საშუალოდ, საწყისო მოდელეების მონაცემები უფრო კოდირებას დაიწყება. მექანიზმი დაახლოებით უფრო შესაძლებელია, როგორც სწრაფად სწრაფად სწრაფად სწორებულია.', 'kk': 'Жуырдағы лотериялық билеттер гипотезасының жұмысы BLEU сақтау кезінде NMT үшін артық түрлендірушілерді жасады. Бірақ үлгісінің оқылған түсініктеріне қалай әсер ететін техникалар білмейді. Трансформацияларды біріншіден көп және төмен үлкендердің теңдігін теңдіру арқылы, біз комплекс семантикалық мәліметтер біріншіден деградицияланады. Ішкі белсенділіктердің анализиясы, қабаттардың көпшілігін бұл қабаттардың көпшілігін бұл көпшіліктерінде айырып, тұтас партнерінен артық көпшілікті Осы уақытта, кеңістік моделдердің алдыңғы қабаттары көп кодтамасын орындау бастады. Қарапайым механизмтері көтерілген кезде қарапайым болады.', 'pl': 'Ostatnie prace nad hipotezą losów loterii stworzyły bardzo rzadkie transformatory dla NMT przy jednoczesnym utrzymaniu BLEU. Nie jest jednak jasne, w jaki sposób takie techniki przycinania wpływają na nauczone reprezentacje modelu. Badając transformatory z coraz większą liczbą ciężarów niskiej wielkości, odkrywamy, że złożone informacje semantyczne są najpierw degradowane. Analiza aktywacji wewnętrznych ujawnia, że wyższe warstwy najbardziej różnią się w trakcie przycinania, stopniowo stając się mniej złożone niż ich gęste odpowiedniki. Tymczasem wczesne warstwy rzadkich modeli zaczynają wykonywać więcej kodowania. Mechanizmy uwagi pozostają niezwykle spójne wraz ze wzrostem słabości.', 'ro': 'Lucrările recente asupra ipotezei biletelor de loterie au produs transformatoare foarte rare pentru NMT, menținând în același timp BLEU. Cu toate acestea, nu este clar cum astfel de tehnici de tăiere afectează reprezentările învățate ale unui model. Prin sondarea Transformers cu greutăți din ce în ce mai mici tăiate departe, descoperim că informațiile semantice complexe sunt prima care sunt degradate. Analiza activărilor interne arată că straturile superioare diferă cel mai mult pe parcursul tăierii, devenind treptat mai puțin complexe decât omologii lor densi. Între timp, straturile timpurii de modele rare încep să efectueze mai multe codări. Mecanismele de atenție rămân remarcabil de consecvente pe măsură ce cantitatea scăzută crește.', 'sr': 'Nedavni rad na hipotezi karte loterije proizveo je vrlo rezervne transformere NMT-a dok održava BLEU. Međutim, nije jasno kako takve brižne tehnike utiču na naučene predstave model a. Probajući transformatore sa većim i visokim težinama niskog veličine, otkrivamo da će prvo biti smanjena kompleksna semantična informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tokom pružanja, postupno postaju manje kompleksni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju izvanredno konsistentni dok se povećava sparsitet.', 'so': "Shaqo ku saabsan tijaatarka tigidhada lottery waxay sameeyeen tarjumayaal aad u yar oo ay u dhaqdhaqaaqaan BLEU. Si kastaba ha ahaatee ma ahan sida qaababka caqliga ah ay u saameyso noocyada lagu bartay. Marka lagu imtixaamo turjumayaasha iyo miisaan badan oo hoos u yar, waxaynu aragnaa in macluumaad adag ee semantika marka hore la hoosaysiiyo. Analyska waxqabadka gudaha ah wuxuu muujiyaa in qasnadaha sare aad u kala bedelaan xiliga waxgarashada, si taxadar ah waxay u noqdaan wax ka yar murugaysan saaxiibbadooda hoose. Wakhtigaas waxaa bilaabaya inay sameeyaan codsiga badan. Meherka daryeelka waxyaabaha la'aanta ah waxay u sii socon yihiin sida kordhiya cimriga.", 'sv': 'Nyligen arbete med lotteri lotter hypotesen har producerat mycket glesa Transformers för NMT samtidigt som BLEU upprätthålls. Det är dock oklart hur sådana beskärningstekniker påverkar en modells lärda representationer. Genom att sondera Transformers med fler och fler låga vikter avskurna, finner vi att komplex semantisk information först försämras. Analys av interna aktiveringar visar att högre skikt skiljer sig mest under beskärningen och gradvis blir mindre komplexa än deras täta motsvarigheter. Samtidigt börjar tidiga lager av glesa modeller utföra mer kodning. Uppmärksamhetsmekanismerna förblir anmärkningsvärt konsekventa i takt med att sparnivån ökar.', 'ta': 'சமீபத்தில் லாட்டரி டிக்கெட் துப்பாக்கத்தில் செயல்பாடு BLEU வைத்திருக்கும் போது NMT மாற்றுபவர்களை மிகவும் வெற ஆனால், இவ்வாறு புதிய தொழில்நுட்பம் எப்படி ஒரு மாதிரியின் கற்றப்பட்ட பங்குகளை பாதிக்கும் என்பது தெளிவா மாற்றுபவர்களை மேலும் அதிக குறைந்த அளவுகளை நீக்கி விட்டதால், சிக்கலான பெம்பெண்டிக் தகவல் முதலில் குறைந்துவிடும் என்பதை நா உள்ளார்ந்த செயல்பாடுகளின் ஆய்வு அதிக குறியீட்டை செய்ய மாதிரிகளின் ஆரம்ப அடுக்குகள் கவனம் முறைமைகள் மிகவும் பொருத்தமாக இருக்கும் வெளிப்பாடு அதிகரிக்கும் போது.', 'si': 'ලොට්\u200dරියි ටිකෙට් විශ්වාසයෙන් අලුත් වැඩේ නැවත NMT වෙනුවෙන් ගොඩක් ප්\u200dරමාණයක් නිර්මාණය කරලා තියෙ නමුත්, ඒ වගේම නැහැ මොඩල් එක්ක ඉගෙන ගත්ත ප්\u200dරතිනිධානයකට කොහොමද ප්\u200dරතිකාර කරන්නේ කියලා. ප්\u200dරමාණ කරණාකරුවන්ට වැඩිය හා වැඩියෙන් ප්\u200dරමාණය කරන්න, අපිට හොයාගන්න පුළුවන් සැමැන්තික තොරතුරු විනාශ කරනව ඇතුළු සක්\u200dරීය විශ්ලේෂණය ප්\u200dරකාශ කරනවා විශ්ලේෂණයෙන් විශ්ලේෂණය කරනවා කියලා වඩා විශ්වාස කරනවා කියලා වඩා ව අනුවෙන් වෙලාවෙන්, පළවෙනි ස්පර්ස් මෝඩේල්ස් වලින් තරම් සංකේතනය කරන්න පටන් ගන්නවා. බලාපොරොත්තු පද්ධතිය සාමාන්\u200dය විශ්වාස කරන්න පුළුවන් වෙනවා.', 'ur': 'لوتری ٹیکٹ کی فرضی پر اچھا کام NMT کے لئے بہت اچھا ترفنسر پیدا کیا گیا ہے جبکہ BLEU حفاظت کرتا ہے۔ لیکن یہ معلوم نہیں کہ ایک موڈل کی تعلیم کا کس طرح اثر کرتا ہے۔ ٹرانسفور کو آزمائش کے ذریعہ سے زیادہ اور زیادہ کم بڑائی وزن سے دور کر دیا گیا ہے، ہم دیکھتے ہیں کہ پیچیدہ سیمانٹی معلومات پہلے ذلیل ہونے والی ہے. داخلی فعالیت کا تحلیل ظاہر کرتا ہے کہ بالاترین لائٹوں سے زیادہ تغییر کرتا ہے اور ان کے گہرے کنٹوروں سے کم پیچیدہ ہوجاتا ہے۔ یہاں تک کہ اسپرس موڈل کے پہلے لہروں سے زیادہ اکڈینڈ کرنا شروع ہوتا ہے۔ توجه کے مکانیزوں باقی رہتے ہیں جب تکلیف اضافہ ہوتی ہے۔', 'uz': "Name Lekin, bu narsalar modelning o'rganilgan tashkilotlariga qanday qo'llanmaydi. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded.  Ichki harakatlarni taʼminlovchi, yuqori qatlamlarning ko'p chegarasini o'zgartiradi, tez qismlaridan murakkab bo'ladi. Hozirga, kichkina modellarning birinchi qatlam kodlash usulini ishga tushirishni boshlaydi. Aniqlik mechanislari kichkina ko'paytirishda juda qiziqarli bo'ladi.", 'vi': 'Công trình gần đây về giả thuyết vé số đã sản xuất ra các biến hình của công ty NMT rất ít trong khi còn nguyên tiếng bíp. Tuy nhiên, không rõ làm thế nào các kỹ thuật cắt tỉa ảnh hưởng đến các biểu tượng được học hỏi. Bằng cách dò tìm các Transformers với các lượng trọng lượng thấp bị tỉa dần, chúng tôi thấy thông tin cơ bản đầu tiên bị thoái hoá. Phân tích kích hoạt nội bộ cho thấy các lớp cao khác biệt nhiều nhất trong quá trình cắt tỉa, dần dần trở nên phức tạp hơn so với các lớp đông đúc. Trong khi đó, lớp đầu của các mô hình rải rác bắt đầu đặt thêm mã số. Các cơ quan chú ý vẫn ổn định mỗi khi số lượng hẹp tăng.', 'bg': 'Последната работа по хипотезата за лотарийни билети доведе до изключително редки трансформатори за НМТ, като същевременно се поддържа Блеу. Въпреки това, не е ясно как такива техники за подрязване влияят на наученото представяне на модела. Чрез сондиране на трансформатори с все повече и повече тежести с ниска магнитуда, откриваме, че сложната семантична информация първо трябва да бъде деградирана. Анализът на вътрешните активирания разкрива, че по-високите слоеве се различават най-много в хода на подрязването, постепенно стават по-малко сложни от техните плътни колеги. Междувременно ранните слоеве от редки модели започват да изпълняват повече кодиране. Механизмите за внимание остават забележително последователни с увеличаването на оскъдността.', 'da': 'Nyligt arbejde med lotteri billet hypotesen har produceret meget sparsomme Transformers til NMT samtidig med at BLEU opretholdes. Det er imidlertid uklart, hvordan sådanne beskæringsteknikker påvirker en models lærte repræsentationer. Ved at undersøge Transformers med flere og flere lav størrelse vægte beskåret væk, finder vi, at komplekse semantiske oplysninger er først til at blive nedbrudt. Analyse af interne aktiveringer viser, at højere lag adskiller sig mest i løbet af beskæringen og gradvist bliver mindre komplekse end deres tætte modstykker. I mellemtiden begynder tidlige lag af sparsomme modeller at udføre mere kodning. Opmærksomhedsmekanismerne forbliver bemærkelsesværdigt konsekvente, efterhånden som sparsomheden stiger.', 'hr': 'Nedavni rad na hipotezi karte loterije proizveo je vrlo rezervne transformere NMT tijekom održavanja BLEU-a. Međutim, nije jasno kako takve brižne tehnike utječu na naučene predstave model a. Provjeravajući transformere sa visokom i visokom težinom niskog veličine, otkrili smo da će prvo biti smanjena kompleksna semantička informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tijekom pružanja, postupno postaju manje složeni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju izvanredno odgovarajući kad povećava rezervnost.', 'nl': 'Recent werk aan de loterij lot hypothese heeft zeer schaarse Transformers voor NMT geproduceerd met behoud van BLEU. Het is echter onduidelijk hoe dergelijke snoei technieken invloed hebben op de geleerde representaties van een model. Door Transformers te onderzoeken met steeds meer gewichten van lage grootte weggesneden, ontdekken we dat complexe semantische informatie eerst wordt gedegradeerd. Analyse van interne activeringen toont aan dat hogere lagen het meest uiteenlopen in de loop van het snoeien, geleidelijk minder complex worden dan hun dichte tegenhangers. Ondertussen beginnen vroege lagen van schaarse modellen meer codering uit te voeren. Aandacht mechanismen blijven opmerkelijk consistent naarmate de schaarste toeneemt.', 'de': 'Jüngste Arbeiten an der Lotteriescheinhypothese haben sehr spärliche Transformatoren für NMT unter Beibehaltung der BLEU produziert. Es ist jedoch unklar, wie solche Beschneidungstechniken die erlernten Darstellungen eines Modells beeinflussen. Indem wir Transformatoren mit immer mehr niederen Gewichten abtasten, stellen wir fest, dass komplexe semantische Informationen zuerst degradiert werden. Die Analyse der inneren Aktivierungen zeigt, dass höhere Schichten im Laufe des Beschnitts am meisten divergieren und allmählich weniger komplex werden als ihre dichten Pendants. Inzwischen beginnen frühe Schichten von spärlichen Modellen, mehr Codierung durchzuführen. Aufmerksamkeitsmechanismen bleiben bemerkenswert konsistent, wenn die Sparsität zunimmt.', 'id': 'Pekerjaan baru-baru ini pada hipotesis tiket loteri telah menghasilkan Transformers sangat jarang untuk NMT sementara mempertahankan BLEU. Namun, tidak jelas bagaimana teknik pemotong tersebut mempengaruhi representation belajar model. Dengan menyelidiki Transformers dengan berat badan yang semakin rendah, kami menemukan bahwa informasi semantis kompleks adalah pertama untuk menjadi rendah. Analisi aktivasi interna mengungkapkan bahwa lapisan yang lebih tinggi paling bergerak sepanjang perjalanan pemotongan, secara perlahan-lahan menjadi lebih rumit dari rekan-rekan yang padat mereka. Sementara itu, lapisan awal dari model kecil mulai melakukan lebih banyak pengekodan. Mekanisme perhatian tetap sangat konsisten saat kecepatan meningkat.', 'fa': 'اخیراً کار روی فرضیه بلیط لوتری در زمان حفظ BLEU، تغییر\u200cدهنده\u200cهای زیادی برای NMT تولید کرده است. با این حال، این تکنیک\u200cهای پاره\u200cگیری چگونه بر نمایش\u200cهای یاد گرفته\u200cی یک مدل تأثیر می\u200cدهد، مشخص نیست. با امتحان تغییر\u200cدهندگان با وزن\u200cهای بیشتر و کمتر از ارتفاع پایین، می\u200cبینیم که اول اطلاعات سنتی\u200cهای پیچیده\u200cای نابود شده است. تحلیل فعالیت های داخلی نشان می دهد که لایه های بالاتر بیشتر در مسیر تغییر کردن و به تدریج کمتر از همکاران dense آنها پیچیده می شوند. در ضمن، لایه\u200cهای اولین مدل\u200cهای خاکستری شروع می\u200cکنند که کودکان بیشتری انجام دهند. مکانیسم توجه به طور کامل هماهنگی می\u200cماند، در حالی که آرامش افزایش می\u200cیابد.', 'sw': 'Kazi za hivi karibuni kuhusu nadharia ya tiketi ya madini imetengeneza WaTransformers kwa ajili ya NMT wakati wa kuendelea BLEU. Hata hivyo, haijulikani jinsi mbinu hizi za akili zinavyoathiri uwakilishi wa modeli waliojifunza. Kwa kuwajaribu WaTransfers na mizani yenye kiwango cha chini imeondolewa, tunagundua kuwa taarifa tatizo ni ya kwanza ya kupunguza. Uchambuzi wa shughuli za ndani unaonyesha kuwa vipande vya juu vinavyotofautiana zaidi katika kipindi cha kuelewa, kwa taratibu kinakuwa tatizo kuliko wapenzi wao wenye msingi. Wakati huo huo, vipande vya mwanzo vya mifano ya uchimbaji vinaanza kufanya kodi zaidi. Attention mechanisms remain remarkably consistent as sparsity increases.', 'af': "Onlangse werk op die loterie belet hipotees het baie sparse transformeerders vir NMT produseer terwyl BLEU behou. Maar dit is onbekend hoe sodanige pruning teknike 'n model se leer verteenwoordighede beïnvloor. Deur te probeer Transformers met meer en meer lae magnitude gewigte wat weg uitgebreek is, vind ons dat kompleks semantiese inligting eerste is om afgebreek te word. Analiseer van interne aktiwiteite vertoon dat hoëre lage mees oor die loop van pruning verskuif word, gradief minder kompleks word as hulle dense kunstenaars. Name Aangaande mekanisme bly remarkante konsistent as sparsiteit vergroot word.", 'tr': 'Ýakynda loteriýan bilet teorisinde işlenýän işi BLEU tutulanda NMT üçin gaty uly gaýd edilen Transformerçiler üretildi. Ama bu kadar akıllı teknolojiler modelinin öğrenmiş temsillerine nähili etkisi yaratmaz. Transformerleri köp we azaltrak ağırlıklar bilen denedip, ilkinji gezek kompleks semantik maglumaty azaltmak üçin pikir edýäris. Daşary janlaşdyrmalaryň analizi ýokary katlaryň ýokary ýokary ýokary ýokary düşürip, ýokary ýokary ýakynlaşyklaryndan az karmaşık bolup görünýär. Bu arada, irden depler nusgalarynyň ködlemeleri başarmak üçin başlaýar. Seresap mekanizmalary ýuwaşlyk bilen aýratyn bir şekilde dowam edýärler.', 'am': 'በሎትራ ቲኪት hypothesis ላይ የሚሠራ ሥራ BLEU በመጠበቅ ጊዜ ለNMT የተለየ ፍላጻዎችን እጅግ ያሳያል፡፡ ነገር ግን እንዲህ ያሉ ብልሃት የሞዴል ተማሪዎችን እንዴት እንደሚያስጨንቁበት አይገለጽም፡፡ በተጨማሪና በሚያነካው ሚዛን በመፈታት፣ የተጨማሪው የsemantic መረጃ በመጀመሪያ እንዲያሳፍር እናገኛለን፡፡ የውስብ አካባቢዎች ትምህርት፣ ከፍተኛ ደረጃዎች በአስተዋይ ክፍል ላይ አብዛኛውን ይለየቃሉ፡፡ በዚያን ጊዜም፣ የቀድሞው ደረጃዎች የጭብጥ ምርጫዎች አካባቢ አካባቢ ማድረግ ይጀምራሉ፡፡ የጥያቄ አካባቢዎች እየጨመረ ቁጥጥር እንደሚያበዛ ይኖራል፡፡', 'ko': '최근 로또 가설에 관한 작업은 NMT에 고도로 드문 변압기를 만들어내면서 BLEU를 유지했다.그러나 이런 가위질 기술이 모델의 학습 표시에 어떻게 영향을 미치는지는 아직 분명하지 않다.점점 더 많은 저량급 권한을 가진 변압기를 탐측함으로써 우리는 복잡한 의미 정보가 먼저 강등되는 것을 발견했다.내부 활성화에 대한 분석에 따르면 더 높은 층은 가위질 과정에서 가장 크게 분화되고 밀집층보다 점점 복잡하지 않게 변한다.또한 희소 모형의 초기 층에서 더 많은 인코딩을 실행하기 시작합니다.희소도가 증가함에 따라 주의력 메커니즘은 현저한 일치성을 유지한다.', 'hy': 'Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU.  Այնուամենայնիվ, անհասկանալի չէ, թե ինչպես են նմանատիպ խզբզելու մեթոդները ազդում մոդելի սովոր ներկայացումների վրա: Երբ փորձում ենք վերափոխողներին ավելի ու ավելի ցածր քաշի վրա, մենք հայտնաբերում ենք, որ բարդ սեմանտիկ ինֆորմացիան առաջին հերթին դեգրոդացվում է: Ներքին ակտիվացիաների վերլուծությունը բացահայտում է, որ բարձր շերտերը ամենաբարձր տարբերակում են մաքրման ընթացքում, դառնալով ավելի քիչ բարդ, քան իրենց խտուն համեմատությունները: Մինչդեռ, փոքր մոդելների վաղ շերտերը սկսում են ավելի շատ կոդավորել: Ուշադրություն դարձնելու մեխանիզմները շարունակում են նշանակալի համապատասխան լինել, մինչ արագությունը աճում է:', 'bn': 'লোটারি টিকিট হিপাইথিসিসের সম্প্রতি কাজ বিলিউ রাখার সময় এনএমটির জন্য অনেক বেশী স্প্যারাস্ফার্মার তৈরি করেছে। তবে এটা পরিষ্কার নয় যে কিভাবে এই ধরনের বুদ্ধিমান প্রযুক্তিগুলো একটি মডেলের শিক্ষা প্রতিনিধিত্বের উপর প্ ট্রান্সফর্মারকে পরীক্ষা করার মাধ্যমে বেশী এবং কম মাত্রার ওজন ছিনিয়ে দেয়া হয়েছে, আমরা দেখতে পাচ্ছি যে জটিল সেম্যান্টিক তথ্য Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts.  এদিকে, স্প্যাস মডেলের প্রাথমিক ক্ষেত্রে আরো এনকোডিং করা শুরু করে। মনোযোগ প্রদান করা মেকিনসমূহ চমৎকার ব্যাপারে একত্রিত থাকে।', 'az': 'Qısa zamanda loteriya bileti hipotezi üzərində NMT üçün çox küçük Transformer yaratdı. Ancaq bu təklif metodların modelinin öyrəndiyi təsirlərinin necə etkisini bilmir. Transformers daha çox və daha düşük böyüklük ağırlığını təşviq edirək, kompleks semantik məlumatları ilk dəfə rüsvay edilməlidir. İçəri fəaliyyətlərin analizi göstərir ki, yüksək səviyyələr dəyişmək yolunda daha çox fəaliyyət edir, yavaş-yavaş onların yoxluq yoldaşlarından daha az kompleks olur. Bu sırada, əkin modellərin əvvəlki səviyyələri daha çox kodlamağa başlar. Dikkati mehānismi çoxluğunda çoxluğunda mövcuddur.', 'bs': 'Nedavni rad na hipotezi karte loterije proizveo je visoko rezervne transformere NMT-a dok održava BLEU. Međutim, nije jasno kako takve tehnike pružanja utječu na naučene predstave model a. Probajući transformatore sa visokom i visokom težinom niskog veličine, otkrivamo da će prvo biti smanjena kompleksna semantička informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tijekom pružanja, postupno postaju manje kompleksni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju nevjerojatno konsistentni dok se povećava sparsitet.', 'ca': "Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU.  No obstant això, no és clar com aquestes tècniques de pruning afecten les representacions aprengutes d'un model. Investigant els Transformers amb més i més pesos de baixa magnitud, trobem que la informació semàntica complexa s'ha de degradar primer. L'anàlisi de l'activació interna revela que les capes més altes divergeixen més al llarg del pruning, tornant-se gradualment menys complexes que les seves denses contrapartides. Mentrestant, les primeres capes de models poc codificats comencen a fer més codificació. Els mecanismes d'atenció segueixen sorprenentment consistents a mesura que l'escassetat augmenta.", 'sq': "Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU.  However, it is unclear how such pruning techniques affect a model's learned representations.  Duke vëzhguar Transformuesit me pesha më të vogla dhe më të vogla të rrënuara larg, ne gjejmë se informacioni kompleks semantik është i pari për të degraduar. Analiza e aktiviteteve të brendshme tregon se shtresa më të larta ndryshojnë më shumë gjatë rrjedhës së rrjedhjes, duke u bërë gradualisht më pak komplekse se homologët e tyre të dendur. Ndërkohë, nivelet e hershme të modeleve të vogla fillojnë të kryejnë më shumë kodim. Mekanizmat e vëmendjes mbeten jashtëzakonisht të konsistenta ndërsa pakësia rritet.", 'cs': 'Nedávná práce na hypotéze loterijních lístků vytvořila velmi řídké transformátory pro NMT při zachování BLEU. Není však jasné, jak tyto techniky prořezávání ovlivňují naučené reprezentace modelu. Zkoumáním transformátorů s více a více nízkými velikostmi odříznutými hmotnostmi zjišťujeme, že složité sémantické informace jsou nejprve degradovány. Analýza vnitřních aktivací ukazuje, že vyšší vrstvy se v průběhu prořezávání nejvíce liší a postupně se stávají méně složitými než jejich husté protějšky. Mezitím rané vrstvy řídkých modelů začínají provádět více kódování. Mechanismy pozornosti zůstávají pozoruhodně konzistentní s rostoucí řídkostí.', 'et': 'Hiljutine töö loteriipileti hüpoteesi on toonud NMT jaoks väga hõredaid transformaatoreid, säilitades samas BLEU. Siiski on ebaselge, kuidas sellised pügamismeetodid mõjutavad mudeli õppitud representatsioone. Uurides üha enam madala suurusega kaaluga transformaatoreid, leiame, et keeruline semantiline informatsioon tuleb esmalt halveneda. Sisemise aktiveerimise analüüs näitab, et kõrgemad kihid erinevad kõige enam pügamise käigus, muutudes järk-järgult vähem keeruliseks kui nende tihedad kolleegid. Samal ajal hakkavad hõredate mudelite varajased kihid rohkem kodeerima. Tähelepanu mehhanismid jäävad märkimisväärselt järjepidevaks, sest hõredus suureneb.', 'fi': 'Viimeaikainen työ lottokuponin hypoteesista on tuottanut erittäin harvoja muuntajia NMT:lle säilyttäen BLEU:n. On kuitenkin epäselvää, miten tällaiset karsimistekniikat vaikuttavat mallin opittuihin representaatioihin. Tutkimalla muuntajia, joilla on yhä enemmän pienikokoisia painoja, huomaamme, että monimutkaista semanttista tietoa on ensin hajotettava. Sisäisten aktivaatioiden analyysi paljastaa, että korkeammat kerrokset eroavat eniten karsimisen aikana, vähitellen vähemmän monimutkaisia kuin niiden tiheät vastineet. Samaan aikaan harvojen mallien varhaiset kerrokset alkavat suorittaa enemmän koodausta. Huomiomekanismit ovat edelleen huomattavan johdonmukaisia, kun harvaluus kasvaa.', 'sk': 'Nedavno delo na hipotezi loterijskih vstopnic je ustvarilo zelo redke transformatorje za NMT, medtem ko je ohranilo BLEU. Vendar pa ni jasno, kako takšne tehnike obrezovanja vplivajo na znane predstavitve modela. S sondiranjem transformatorjev z vedno več težami nizke magnitude, ugotovimo, da je treba kompleksne semantične informacije najprej razgraditi. Analiza notranjih aktivacij razkriva, da se višje plasti med obrezovanjem najbolj razlikujejo in postopoma postajajo manj kompleksne od njihovih gostih kolegov. Medtem pa zgodnje plasti redkih modelov začnejo izvajati več kodiranja. Mehanizmi pozornosti ostajajo izjemno usklajeni, saj se redkost povečuje.', 'jv': 'Olo-luwih nggawe barang karo akeh kapan-kapan kuwi kapan kawit bagian sing nganggep bantuan Transformer kanggo NMT, sanes memperbudhakan CLUE Nanging, kuwi ora ngerti piye ngerti, teknik kuwi susahe nêmêr kuwi model model sing apik nyeanye Dijejer-jejer Ndelengke alam sing akeh tanggal dipunangke dipunangke alam luwih luwih-luwih jenis diolah punika dipunangke punika dipunangke dipunangke kapan politenessoffpolite"), and when there is a change ("assertivepoliteness Desaturan', 'he': 'העבודה האחרונה על ההיפתוזיה של כרטיסי הלוטו יצרה Transformers מאוד נדיר עבור NMT בזמן שמירה BLEU. בכל אופן, אינו ברור איך טכניקות טיפול כאלה משפיעות על מייצגים למדוגמנים. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded.  Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding.  מנגנוני תשומת לב נשארים עקביים באופן יוצא דופן בזמן שהנדירות עולה.', 'ha': "Yin aikin nan da aka samu a kan mala'a mai Lottori ya sami mai girma Transformers wa NMT a lokacin da ke tsare BLEU. Amma, bã shi da gane yadda misãlai masu fahimta ke amfani da misãlai. Ga a jarraba Transformers da sikẽli masu ƙaranci ko ƙaranci, sai za mu gane cewa masu adadi na semantic ya zama kwanza a kunyatar. Anarari ga aikin aiki na guda ya bayyana cewa abubuwa masu sarrafa za'a gaura mafi yawa a tsakanin hankalin, kuma yana kasa sakan kammala musamman da tarakin nan bakin. A lokacin da za'a fara ƙananan masu motsi na tsumarni. Akwai matsayin saurãre yana daidai kamar an ƙara zafi.", 'bo': 'འཕྲལ་ཁམས་ཀྱི་སྐོར་གྱི་ལྕགས་གླེང་སྡུད་པར་བཟོ་བར་མཐུན་པ་ཁག་གིས་མཐུན་བཟོ་བཅོས་པ་དེ་ཡིན། ཡིན་ནའང་། དབྱིན་རྩལ་གྱི་ཐབས་ལམ་དེ་གིས་མི་ཤེས་པས་རྣམ་གྲངས་བསྡུར་བྱེད་ཀྱི་ཡོད། དབྱིབས་བཟོ་བ་དག་པ་ཞིག་གིས་མཐོ་དམའ་བའི་ཚད་ལྡན་བ་ཞིག་ནས དབྱེ་ཞིབ་ཀྱི་ནང་འཁོད་བྱ་འགུལ་གྱི་དཔྱད་ཞིག་ནི་བགོ་རིམ་མཐོ་ཚད་རྒྱ་ཆེ་མཐོ་ཁག་གི་ཡིག་རྟགས་ལ་ཕར་ཆེ་བ་བསྐྱེད་ཚད་འདྲ་བྱེད་ཀྱི་ ད་ནའང་ཡང་སྔོན་གྱི་བང་རིམ་པ་ཁང་གི་དབྱེ་རིས་མང་ཙམ་ཨང་ཀོ་གཏོང་འགོ་འཛུགས་བྱེད་ཀྱི་ཡོད། ཆེད་འཛིན་གྱི་ཐབས་ལམ་དེ་ཚོ་ཆེད་པོ་ཞིག་ཏུ་ཉར་ཆེན་པོ་ཞིག་ཏུ་ཉར་ཡོད།'}
{'en': 'BERTs of a feather do not generalize together : Large variability in generalization across models with similar test set performance BERT s of a feather do not generalize together: Large variability in generalization across models with similar test set performance', 'ar': 'لا يتم تعميم BERTs للريشة معًا: تباين كبير في التعميم عبر النماذج ذات أداء مجموعة الاختبار المماثل', 'es': 'Los BERT del mismo plumaje no se generalizan juntos: gran variabilidad en la generalización entre modelos con un rendimiento de conjunto de pruebas similar', 'pt': 'BERTs de uma pena não generalizam juntos: Grande variabilidade na generalização entre modelos com desempenho de conjunto de teste semelhante', 'fr': "Les BERT d'une plume ne se généralisent pas ensemble\xa0: grande variabilité dans la généralisation entre les modèles ayant des performances d'ensemble de test similaires", 'ja': 'フェザーのBERTは一緒に一般化することはありません：類似したテストセットパフォーマンスを持つモデル間での一般化における大きなばらつき', 'hi': 'एक पंख के BERTs एक साथ सामान्यीकरण नहीं करते हैं: समान परीक्षण सेट प्रदर्शन के साथ मॉडल में सामान्यीकरण में बड़ी परिवर्तनशीलता', 'zh': '羽之BERT不可同泛化:有似试者泛化异甚', 'ru': 'BERT пера не обобщаются вместе: большая вариабельность в обобщении между моделями с аналогичной производительностью тестового набора', 'ga': 'Ní ghinearáltar BERTanna cleite le chéile: Éagsúlacht mhór sa ghinearálú trasna samhlacha a bhfuil feidhmíocht tacair tástála comhchosúil acu', 'ka': 'პერტის ბერტი არ გენერალიზაცის ერთად: დიდი განრავლობა გენერალიზაციაში მოდელში, რომელიც სხვადასხვა ტესტის შესაძლებელობა', 'hu': 'Egy toll BERT-jei nem általánosítanak együtt: Nagy változékonyság az általánosításban a hasonló tesztkészlet teljesítményű modellek között', 'el': 'Οι BERT ενός φτερού δεν γενικεύονται μαζί: Μεγάλη μεταβλητότητα στη γενίκευση μεταξύ μοντέλων με παρόμοιες επιδόσεις συνόλου δοκιμών', 'it': 'I BERT di una piuma non generalizzano insieme: Grande variabilità nella generalizzazione tra modelli con prestazioni simili del set di test', 'kk': 'Бұл үлкен BERTS бірге жалпы түрлендірмейді: ұқсас сынақтар орнатылған моделдердің жалпы түрлендірімі үлкен түрлендірімі', 'lt': 'Pelenų BERTs bendrai nesuplaukia: didelis generalizacijos kintamumas modeliuose, kurių bandymų rinkinio charakteristikos panašios', 'mk': 'БЕРТ од пердуво не се генерализираат заедно: Голема варијабилност во генерализацијата меѓу моделите со слична резултатност на тестовите', 'ms': 'BERTs bulu tidak menyebarkan bersama-sama: Variabiliti besar dalam penyerahan melalui model dengan prestasi set ujian yang sama', 'ml': 'ഒരു കാലാവസ്ഥയുടെ ബെര്\u200dടെസ് ഒരുമിച്ച് സാധാരണ ചെയ്യുന്നില്ല: ഒരുപോലുള്ള ടെസ്റ്റ് സെറ്റ് പ്രവര്\u200dത്തനപ്രകൃതി', 'mn': 'Бурт нь ерөнхийлөгч байдаггүй: төстэй шалгалтын бүтээгдэхүүнтэй маш том өөрчлөлт', 'no': 'BERTS av ein fjør generelliserer ikkje saman: Stor variabel i generellisering over modeller med liknande testinnstillingar', 'pl': 'BERT pióra nie uogólniają się razem: Duża zmienność uogólnienia w modelach o podobnych wydajnościach zestawu testowego', 'mt': 'BERTs ta’ rix ma jiġġeneralizzawx flimkien: Varjabbiltà kbira fil-ġeneralizzazzjoni fost mudelli b’prestazzjoni simili tas-sett tat-test', 'ro': 'BERT-urile unei pene nu generalizează împreună: Variabilitate mare în generalizare la modele cu performanțe similare ale setului de testare', 'sr': 'BERTS perja ne generalizuju zajedno: Velika variabilnost generalizacije preko modela sa sličnim testovima', 'si': 'BERTs of a pear do not General lize with: Large variants in General lization over Models with Simal testing set Perfection', 'so': 'BERTs oo cir ah lama wada samaysto: variations weyn in generalisation throughout models with similar performance set', 'ta': 'BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance', 'ur': 'ایک فائر کے BERTS ایک دوسرے کے ساتھ نہیں آگاہ ہوتے: ایک دوسرے کے سامنے موڈل میں بہت بڑی بدل دینے والی', 'sv': 'BERT för en fjäder generaliserar inte tillsammans: Stor variation i generalisering mellan modeller med liknande testset prestanda', 'uz': 'Name', 'vi': 'Vớ vẩn. Sự biến thiên vi lớn trong việc tổng hợp các mô- đun với khả năng thử nghiệm tương tự.', 'bg': 'ЕРТ на перото не се обобщават заедно: Голяма вариабилност в обобщаването между модели със сходна производителност на тестовия комплект', 'nl': "BERT's van een veer generaliseren niet samen: Grote variabiliteit in generalisatie tussen modellen met vergelijkbare testsetprestaties", 'hr': 'BERTS perja se ne generalizira zajedno: Velika variabilnost generalizacije preko modela s sličnim testovima', 'da': "BERT'er af en fjer generaliserer ikke sammen: Stor variation i generalisering på tværs af modeller med lignende prøvesæt ydeevne", 'ko': '동일한 특성의 Bert는 함께 범용할 수 없음: 비슷한 테스트 세트 성능을 가진 모델의 범용 차이가 매우 크다', 'de': 'BERTs einer Feder verallgemeinern sich nicht zusammen: Große Variabilität in der Verallgemeinerung über Modelle mit ähnlicher Testsatzleistung', 'tr': 'BERTS', 'fa': 'BERTs of a feather do not generalize together: Variability big in generalization across models with similar test set performance', 'id': 'BERTs bulu tidak generalisasi bersama-sama: variabilitas besar dalam generalisasi melalui model dengan prestasi set tes yang sama', 'sq': 'BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance', 'sw': 'BERTs ya paa hazijengeneza pamoja: mabadiliko makubwa katika kutengeneza mifano yenye utendaji wa majaribio yanayofanana', 'af': "BERTS van 'n vloer doen nie generaliseer saam nie: Groot veranderlikheid in generalisering oor modele met gelyke toets stel prestasie", 'am': 'ምርጫዎች', 'bn': 'একটি ফায়ারের বেরেস একসাথে সাধারণ করে না: একই ধরনের পরীক্ষা সেট প্রদর্শনের সারা মডেলের সারা পৃথিবীতে বিশাল পরিবর্তন', 'bs': 'BERTI perja ne generalizuju zajedno: Velika variabilnost generalizacije preko modela sa sličnim testovima', 'az': 'Süfür BERTS birlikdə generalizasyon etməz: modellərdə böyük dəyişiklik, bənzər sınama qurğulu performansı ilə', 'hy': 'Փետրի BER-ները միասին չեն ընդհանրացվում. ընդհանրացման մեծ տարբերակությունը մոդելների միջև, որոնք ունեն նմանատիպ փորձարկումներ', 'cs': 'BERT peří nezobecňují dohromady: Velká variabilita generalizace mezi modely s podobným výkonem testovací sady', 'et': 'Sule BERT-d ei generaliseeru koos: suur generaliseerimise varieeruvus sarnase testikomplekti jõudlusega mudelites', 'ca': 'BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance', 'fi': 'Sulan BERT-arvot eivät yleisty yhdessä: Suuri yleistymisen vaihtelu malleissa, joilla on samanlainen testisarjan suorituskyky', 'he': 'BERTs של נוצה לא מגנרל יחד: שונות גדולה בגנרליזציה בין דוגמנים', 'ha': '@ item: inmenu', 'sk': 'BERT perja se ne generalizirajo skupaj: Velika variabilnost generalizacije med modeli s podobno zmogljivostjo preskusnega niza', 'jv': 'jer', 'bo': 'BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance'}
{'en': 'If the same neural network architecture is trained multiple times on the same  dataset , will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with  accuracy  ranging between 83.6 % and 84.8 %. In stark contrast, the same  models  varied widely in their  generalization  performance. For example, on the simple case of subject-object swap (e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor),  accuracy  ranged from 0.0 % to 66.2 %. Such variation is likely due to the presence of many  local minima  in the loss surface that are equally attractive to a low-bias learner such as a  neural network  ; decreasing the variability may therefore require models with stronger  inductive biases .', 'ar': 'إذا تم تدريب نفس بنية الشبكة العصبية عدة مرات على نفس مجموعة البيانات ، فهل ستقدم تعميمات لغوية مماثلة عبر المسارات؟ لدراسة هذا السؤال ، قمنا بضبط 100 حالة من BERT على مجموعة بيانات استدلال اللغة الطبيعية متعدد الأنواع (MNLI) وقمنا بتقييمها على مجموعة بيانات HANS ، التي تقيم التعميم النحوي في استدلال اللغة الطبيعية. في مجموعة تطوير MNLI ، كان سلوك جميع الحالات متسقًا بشكل ملحوظ ، حيث تراوحت الدقة بين 83.6٪ و 84.8٪. في تناقض صارخ ، تباينت النماذج نفسها بشكل كبير في أداء التعميم. على سبيل المثال ، في حالة مبادلة الموضوع والهدف (على سبيل المثال ، تحديد أن "الطبيب زار المحامي" لا يعني "زيارة المحامي للطبيب") ، تراوحت الدقة من 0.0٪ إلى 66.2٪. من المحتمل أن يكون هذا الاختلاف بسبب وجود العديد من الحدود الدنيا المحلية في سطح الخسارة والتي تكون جذابة بنفس القدر لمتعلم منخفض التحيز مثل الشبكة العصبية ؛ لذلك قد يتطلب تقليل التباين نماذج ذات تحيزات استقرائية أقوى.', 'es': 'Si la misma arquitectura de red neuronal se entrena varias veces en el mismo conjunto de datos, ¿realizará generalizaciones lingüísticas similares en las corridas? Para estudiar esta pregunta, ajustamos 100 instancias de BERT en el conjunto de datos de Inferencia de Lenguaje Natural Multigénero (MNLI) y las evaluamos en el conjunto de datos HANS, que evalúa la generalización sintáctica en la inferencia de lenguaje natural. En el conjunto de desarrollo de MNLI, el comportamiento de todas las instancias fue notablemente consistente, con una precisión que osciló entre el 83,6% y el 84,8%. En marcado contraste, los mismos modelos variaron ampliamente en su rendimiento de generalización. Por ejemplo, en el caso simple de intercambio sujeto-objeto (por ejemplo, determinar que «el médico visitó al abogado» no implica que «el abogado visitó al médico»), la precisión osciló entre el 0,0% y el 66,2%. Esta variación probablemente se deba a la presencia de muchos mínimos locales en la superficie de pérdida que son igualmente atractivos para un estudiante de bajo sesgo, como una red neuronal; por lo tanto, la disminución de la variabilidad puede requerir modelos con sesgos inductivos más fuertes.', 'pt': 'Se a mesma arquitetura de rede neural for treinada várias vezes no mesmo conjunto de dados, ela fará generalizações linguísticas semelhantes entre as execuções? Para estudar essa questão, ajustamos 100 instâncias do BERT no conjunto de dados Multi-genre Natural Language Inference (MNLI) e as avaliamos no conjunto de dados HANS, que avalia a generalização sintática na inferência de linguagem natural. No conjunto de desenvolvimento MNLI, o comportamento de todas as instâncias foi notavelmente consistente, com precisão variando entre 83,6% e 84,8%. Em forte contraste, os mesmos modelos variaram amplamente em seu desempenho de generalização. Por exemplo, no caso simples de troca sujeito-objeto (por exemplo, determinar que “o médico visitou o advogado” não implica “o advogado visitou o médico”), a precisão variou de 0,0% a 66,2%. Essa variação é provavelmente devido à presença de muitos mínimos locais na superfície de perda que são igualmente atraentes para um aprendiz de baixo viés, como uma rede neural; diminuir a variabilidade pode, portanto, exigir modelos com vieses indutivos mais fortes.', 'fr': "Si la même architecture de réseau neuronal est entraînée plusieurs fois sur le même jeu de données, fera-t-elle des généralisations linguistiques similaires entre les essais\xa0? Pour étudier cette question, nous avons affiné 100 instances de BERT sur le jeu de données MNLI (Multi-genre Natural Language Inference) et les avons évaluées sur le jeu de données HANS, qui évalue la généralisation syntaxique dans l'inférence en langage naturel. Sur l'ensemble de développement MNLI, le comportement de toutes les instances était remarquablement cohérent, avec une précision comprise entre 83,6\xa0% et 84,8\xa0%. En contraste frappant, les mêmes modèles variaient considérablement en termes de performances de généralisation. Par exemple, dans le cas simple de l'échange sujet-objet (par exemple, déterminer que «\xa0le médecin a consulté l'avocat\xa0» n'implique pas «\xa0l'avocat a consulté le médecin\xa0»), la précision variait de 0,0\xa0% à 66,2\xa0%. Une telle variation est probablement due à la présence de nombreux minima locaux dans la surface de perte qui sont tout aussi attrayants pour un apprenant à faible biais tel qu'un réseau neuronal\xa0; la diminution de la variabilité peut donc nécessiter des modèles avec des biais inductifs plus forts.", 'ja': '同じニューラルネットワークアーキテクチャが同じデータセット上で複数回トレーニングされた場合、実行全体で同様の言語学的一般化が行われますか？ この質問を研究するために、私たちは多ジャンル自然言語推論（ MNLI ）データセット上のBERTの100のインスタンスを微調整し、自然言語推論における構文の一般化を評価するHANSデータセット上でそれらを評価しました。 MNLI開発セットでは、すべてのインスタンスの挙動は著しく一貫しており、精度は83.6 ％から84.8 ％の間でした。 対照的に、同じモデルは一般化性能において大きく異なっていた。 例えば、対象スワップの単純なケース（例えば、「医師が弁護士を訪問した」が「弁護士が医師を訪問した」を必要としないと判断する）では、正確さは0.0 ％から66.2 ％の範囲でした。 このような変動は、ニューラルネットワークなどの低バイアス学習者に同様に魅力的な損失面に多くの局所的最小値が存在することに起因する可能性が高い。したがって、変動性を減少させるには、より強力な帰納的バイアスを有するモデルが必要となり得る。', 'hi': 'यदि एक ही तंत्रिका नेटवर्क आर्किटेक्चर को एक ही डेटासेट पर कई बार प्रशिक्षित किया जाता है, तो क्या यह रन में समान भाषाई सामान्यीकरण करेगा? इस प्रश्न का अध्ययन करने के लिए, हमने बहु-शैली प्राकृतिक भाषा अनुमान (MNLI) डेटासेट पर BERT के 100 उदाहरणों को ठीक किया और HANS डेटासेट पर उनका मूल्यांकन किया, जो प्राकृतिक भाषा अनुमान में वाक्यात्मक सामान्यीकरण का मूल्यांकन करता है। MNLI विकास सेट पर, सभी उदाहरणों का व्यवहार उल्लेखनीय रूप से सुसंगत था, जिसमें सटीकता 83.6% और 84.8% के बीच थी। इसके विपरीत, एक ही मॉडल उनके सामान्यीकरण प्रदर्शन में व्यापक रूप से भिन्न थे। उदाहरण के लिए, विषय-वस्तु स्वैप के सरल मामले पर (उदाहरण के लिए, यह निर्धारित करना कि "डॉक्टर ने वकील का दौरा किया" "वकील ने डॉक्टर का दौरा किया") को शामिल नहीं करता है, सटीकता 0.0% से 66.2% तक होती है। इस तरह की भिन्नता हानि की सतह में कई स्थानीय मिनीमा की उपस्थिति के कारण होने की संभावना है जो एक कम-पूर्वाग्रह शिक्षार्थी जैसे कि तंत्रिका नेटवर्क के लिए समान रूप से आकर्षक हैं; परिवर्तनशीलता को कम करने के लिए इसलिए मजबूत आगमनात्मक पूर्वाग्रहों के साथ मॉडल की आवश्यकता हो सकती है।', 'zh': '同数而数同神经网络架构,其言类乎? 以此观之,自然语言理(MNLI)数集上微调100BERT实,HANS数集上质,当数自然语言理之句法泛化。 MNLI 发集上,诸例皆同,准确率介 83.6% 84.8% 间。 与此成鲜明对比者,同型号异于泛化性也。 如主客体易例(定"医谒"律师不意"律师访医"),准确性自0.0%至66.2%有差。 此盖损阳多局最小值,最小值于神经网络下偏学者同吸引力。 是故降可变性者,所以强归偏置也。', 'ru': 'Если одна и та же архитектура нейронной сети обучается несколько раз на одном и том же наборе данных, сделает ли она похожие лингвистические обобщения во всех прогонах? Для изучения этого вопроса мы доработали 100 экземпляров BERT на наборе данных Multi-genre Natural Language Inference (MNLI) и оценили их на наборе данных HANS, который оценивает синтаксическое обобщение в естественном языковом выводе. На наборе разработки MNLI поведение всех случаев было удивительно последовательным, с точностью от 83,6% до 84,8%. Напротив, одни и те же модели сильно различались по своим характеристикам обобщения. Например, в случае простого обмена субъектом-объектом (например, определение того, что «врач посетил адвоката» не влечет за собой «адвокат посетил врача»), точность варьировалась от 0,0% до 66,2%. Такое изменение, вероятно, связано с наличием множества локальных минимумов на поверхности потерь, которые одинаково привлекательны для обучающегося с низкой степенью смещения, такого как нейронная сеть; поэтому для уменьшения изменчивости могут потребоваться модели с более сильными индуктивными смещениями.', 'ga': "Má dhéantar an ailtireacht líonra néar-líonra céanna a oiliúint go minic ar an tacar sonraí céanna, an ndéanfaidh sé ginearáluithe teanga comhchosúla thar rití? Chun staidéar a dhéanamh ar an gceist seo, rinneamar mionchoigeartú ar 100 cás de BERT ar an tacar sonraí Tátail Teanga Nádúrtha Il-seánra (MNLI) agus rinneamar iad a mheas ar thacar sonraí HANS, a dhéanann meastóireacht ar ghinearálú comhréire i tátal teanga nádúrtha. Ar thacar forbartha an MNLI, bhí iompar gach cás thar a bheith comhsheasmhach, le cruinneas idir 83.6% agus 84.8%. I gcodarsnacht lom, bhí éagsúlacht mhór sna samhlacha céanna ina bhfeidhmíocht ginearálaithe. Mar shampla, maidir le cás simplí babhtála ábhar-ábhar (m.sh., a chinneadh nach bhfuil “cuireadh an dlíodóir ar an dochtúir” i gceist leis an gcinneadh “cuairtigh an dochtúir ar an dochtúir”), bhí an cruinneas idir 0.0% agus 66.2%. Is dócha go bhfuil éagsúlacht den sórt sin ann mar gheall ar láithreacht go leor íosta áitiúil sa dromchla caillteanas atá chomh tarraingteach céanna d’fhoghlaimeoir ísealchlaonta amhail líonra néareolaíoch; d'fhéadfadh go mbeadh gá le samhlacha le laofachtaí ionduchtaithe níos láidre chun an éagsúlacht a laghdú.", 'ka': "თუ იგივე ნეიროლური ქსელის აქტიქტიქტურა იგივე მონაცემების კონფიგურაციაში უფრო მეტად განაკეთებულია, შეიძლება იგივე ლენგური გენერალიზაცი ამ კითხვის შესწავლისთვის, ჩვენ ბერტის 100 ინსტენსტის მრავალ გენერგიის ინფერენციაზე (MNLI) მონაცემების ინფერენციაზე დავაკეთეთეთ და მათ HANS მონაცემების ინფერენციაში გავუმუშავეთ, რომელიც სინტაქტიული გენ MNLI განვითარებაში, ყველა მოცემების მოქმედება მართლად შემდგომარებელია, რომელიც მართლაც 83,6% და 84,8%-ის განმავლობაში იყო. სტარკონტრესტში იგივე მოდელები განსხვავებულია საერთო გენერალიზაციაში. მაგალითად, საქმე საქმე საქმე საქმე საქმე საქმე (მაგალითად, განსაზღვრება, რომ 'ექმენტი დავიყვანეთ აექმენტი' არ შეიძლება 'აექმენტი დავიყვანეთ ექმენტი'), მართლაც 0,0% დან 66,2% ასეთი განსხვავება შესაძლებელია რამდენიმე ლოკალური მინიმატის მისამართობისთვის, რომელიც განსხვავებული გვერდის განსხვავებაში, რომელიც მხოლოდ ატრუქტურებული მასწავლისთვის,  შესაძლებელია, რომ განსხვავებულობას შეუძლებელია, რომ მოდელები უფრო ძალიან ინდექტიური განსხვავებულებებით მოჭირდება.", 'el': 'Αν η ίδια αρχιτεκτονική νευρωνικών δικτύων εκπαιδευτεί πολλές φορές στο ίδιο σύνολο δεδομένων, θα κάνει παρόμοιες γλωσσικές γενικεύσεις σε όλες τις διαδρομές; Για να μελετήσουμε αυτή την ερώτηση, τελειοποιήσαμε εκατοντάδες περιπτώσεις του BERT στο σύνολο δεδομένων πολλαπλών ειδών Φυσικής Γλώσσας (MNLI) και τις αξιολογήσαμε στο σύνολο δεδομένων το οποίο αξιολογεί τη συντακτική γενίκευση στην εξαγωγή φυσικής γλώσσας. Στο σύνολο ανάπτυξης η συμπεριφορά όλων των περιπτώσεων ήταν εξαιρετικά συνεπής, με ακρίβεια που κυμαίνεται μεταξύ 83.6% και 84.8%. Σε έντονη αντίθεση, τα ίδια μοντέλα ποικίλουν ευρέως στην απόδοση γενικοποίησης τους. Για παράδειγμα, στην απλή περίπτωση ανταλλαγής αντικειμένου-αντικειμένου (π.χ., ο προσδιορισμός ότι "ο γιατρός επισκέφθηκε τον δικηγόρο" δεν συνεπάγεται "ο δικηγόρος επισκέφθηκε τον γιατρό"), η ακρίβεια κυμαίνεται από 0.0% έως 66.2%. Αυτή η διακύμανση οφείλεται πιθανώς στην παρουσία πολλών τοπικών ελάχιστων στην επιφάνεια απώλειας που είναι εξίσου ελκυστικά για έναν μαθητή με χαμηλή προκατάληψη, όπως ένα νευρωνικό δίκτυο. Η μείωση της μεταβλητότητας μπορεί επομένως να απαιτήσει μοντέλα με ισχυρότερες επαγωγικές προκατάληψη.', 'it': 'Se la stessa architettura di rete neurale viene addestrata più volte sullo stesso set di dati, farà generalizzazioni linguistiche simili tra run? Per studiare questa domanda, abbiamo messo a punto 100 istanze di BERT sul set di dati multi-genere Natural Language Inference (MNLI) e le abbiamo valutate sul set di dati HANS, che valuta la generalizzazione sintattica nell\'inferenza del linguaggio naturale. Sul set di sviluppo MNLI, il comportamento di tutte le istanze è stato notevolmente coerente, con precisione compresa tra l\'83,6% e l\'84,8%. In netto contrasto, gli stessi modelli variavano ampiamente nelle loro prestazioni di generalizzazione. Per esempio, nel caso semplice di scambio soggetto-oggetto (ad esempio, determinare che "il medico ha visitato l\'avvocato" non implica "l\'avvocato ha visitato il medico"), l\'accuratezza variava dallo 0,0% al 66,2%. Tale variazione è probabilmente dovuta alla presenza di molti minimi locali nella superficie di perdita che sono ugualmente attraenti per un discente a basso bias come una rete neurale; La diminuzione della variabilità può quindi richiedere modelli con distorsioni induttive più forti.', 'hu': 'Ha ugyanazt az ideghálózati architektúrát többször képezik ugyanazon adatkészleten, akkor hasonló nyelvi általánosításokat fog végrehajtani futások között? A kérdés tanulmányozásához a BERT 100 példányát finomhangoltuk a Multiműfajú Natural Language Inference (MNLI) adatkészleten és értékeltük azokat a HANS adatkészleten, amely a szintaktikus általánosítást értékeli a természetes nyelvi következtetésben. Az MNLI fejlesztési készleten az összes példány viselkedése rendkívül következetes volt, a pontosság 83,6% és 84,8% között mozog. Erős ellentétben ugyanazok a modellek általánosítási teljesítményükben nagymértékben eltértek. Például az alany-tárgy cseréjének egyszerű esetében (például annak megállapítása, hogy "az orvos meglátogatta az ügyvédet", nem jelenti azt, hogy "az ügyvéd meglátogatta az orvost"), a pontosság 0,0% és 66,2% között mozog. Ez a változás valószínűleg annak köszönhető, hogy a veszteségfelületen számos helyi minimum jelen van, amelyek ugyanolyan vonzóak egy alacsony elfogultságú tanuló számára, mint például egy neurális hálózat; A változékonyság csökkentése ezért erősebb induktív elfogultságú modelleket igényelhet.', 'kk': "Егер бір невралдық желінің архитектурасы бір деректер жиында бірнеше рет оқылған болса, ол бірнеше тіл жиындарына ұқсас болады ба? Бұл сұрақты зерттеу үшін біз көптеген натурал тілдер инференциясының (MNLI) деректер жинағына BERT 100 инстанцияларын баптап, оларды табиғи тілдер инференциясында синтактикалық генерализациялауды бағалады. MNLI жасау бағдарламасында, барлық әрекеттердің әрекеті өте тәуелді, 83,6% мен 84,8% арасында дұрыс болып тұрды. Жұлдыз контрастығында бір моделдер жалпы түрде өзінің жалпы түрлі болды. Мысалы, тақырыбы нысандарының ауыстыруы қарапайым болса (мысалы, 'дәрігер ауыстырылды' дегенді анықтау 'дәрігер ауыстырылды' деген дәрігер ауыстырылмайды), дәрежесі 0, 0% мен 66, 2% дегенге ауыстырылды. Бұл өзгерістер невралдық желінің секілді жергілікті көптеген жоғалу көпшілігінде көптеген көптеген кішіліктердің себебі болуы мүмкін. Өзгерістерді азайту мүмкін болады, сондықтан үлгілерді күшті индуктивті өзгерістерді талап етеді.", 'mk': 'Ако истата архитектура на нервната мрежа е обучена повеќе пати на истиот податок, дали ќе направи слични јазични генерализации во тековите? За да го проучуваме ова прашање, најдовме 100 примери на БЕРТ на многубројниот набор на податоци за природен јазик (МНЛИ) и ги проценивме на наборот на податоци на ХАНС, кој ја проценува синтактичката генерализација во природната инференција на јазик. Во сетот на развојот на МНЛИ, однесувањето на сите случаи беше исклучително конзистентно, со точност од 83,6 до 84,8 отсто. Во голем контраст, истите модели многу се разликуваа во нивната генерализација. На пример, за едноставниот случај на размена на предмети (на пример, одредувањето дека „докторот го посетил адвокатот“ не вклучува „адвокатот го посетил докторот“), точноста се движеше од 0,0 до 66,2 отсто. Таквата варијација најверојатно е резултат на присуството на многу локални минимали на површината на загубата кои се еднакво атрактивни за мал ученик како неурална мрежа; намалувањето на варијабилноста може да бара модели со посилни индуктивни предрасуди.', 'ms': "Jika arkitektur rangkaian saraf yang sama dilatih berbilang kali pada set data yang sama, adakah ia akan membuat generalisasi bahasa yang sama di seluruh jalur? Untuk mempelajari soalan ini, kami menyesuaikan 100 contoh BERT pada set data Multi-genre Natural Language Inference (MNLI) dan menelitinya pada set data HANS, yang meneliti keseluruhan sintaktik dalam kesimpulan bahasa alam. Dalam set pembangunan MNLI, perilaku semua kes adalah sangat konsisten, dengan ketepatan berlainan antara 83.6% dan 84.8%. Dalam perbezaan ketat, model yang sama berbeza dengan luas dalam prestasi generalisasi mereka. Contohnya, dalam kes sederhana penyelesaian subjek-objek (cth., menentukan bahawa 'doktor melawat peguam' tidak melibatkan 'peguam melawat doktor'), ketepatan berlainan dari 0.0% hingga 66.2%. Variasi seperti ini mungkin disebabkan kehadiran banyak minima setempat di permukaan kehilangan yang sama menarik bagi pelajar bias rendah seperti rangkaian saraf; pengurangan variabiliti mungkin memerlukan model dengan bias induktif yang lebih kuat.", 'lt': 'Jei ta pati neurologinio tinklo architektūra kelis kartus mokoma toje pačioje duomenų rinkinyje, ar ji atliks panašias kalbų generalizacijas įvairiose runs? Norėdami išnagrinėti šį klausimą, tiksliai pritaikėme 100 BERT atvejų daugiasluoksnės gamtinės kalbos Inferencijos (MNLI) duomenų rinkinyje ir įvertinome juos HANS duomenų rinkinyje, kuriame vertinama sintaksinė generalizacija gamtinės kalbos išvadose. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%.  Labai priešingai, tie patys modeliai labai skiriasi jų generalizacijos rezultatais. Pavyzdžiui, paprasta apsikeitimo objektais atveju (pvz., nustačius, kad "gydytojas lankėsi advokatą" nereiškia "advokatas lankėsi gydytoją"), tikslumas svyravo nuo 0,0 % iki 66,2 %. Toks skirtumas greičiausiai atsiranda dėl to, kad praradimo paviršiuje yra daug vietinių minimumų, kurie yra vienodai patrauklūs nedidelio pobūdžio besimokančiam asmeniui, pavyzdžiui, nerviniam tinklui; Todėl dėl kintamumo mažinimo gali prireikti modelių, kurių indukciniai nukrypimai yra stipresni.', 'mn': "Хэрвээ ижил мэдрэлийн сүлжээний архитектур ижил өгөгдлийн санд олон удаа сургалтын тухай байвал энэ нь хэлний ерөнхийлөгчийг дамжуулах уу? Энэ асуултыг судлах үед бид олон төрлийн байгалийн хэл хамааралтай (MNLI) өгөгдлийн санд БЕРТ-ын 100 жишээг тодорхойлж, багалийн хэл халдварын синтактик ерөнхийлөгчийг үнэлдэг HANS өгөгдлийн санд үнэлдэг. MNLI хөгжлийн хэмжээнд бүх үйл явдлын үйл явдал маш тохиромжтой байлаа. 83.6% болон 84.8% хоорондоо тохиромжтой байлаа. Харамсалтай нь ижил загвар нь ерөнхийлөгчийн үйл ажиллагаанд маш их өөрчлөгдсөн. Жишээлбэл, сургуулийн объектын хувьсалын энгийн тохиолдолд (жишээ нь 'эмч хуульч ирсэн' гэдэг нь 'хуульч эмч ирсэн' гэдгийг тодорхойлж чадахгүй), зөв тохиолдол нь 0.0% ээс 66.2% хүртэл байдаг. Ийм өөрчлөлт нь мэдрэлийн сүлжээнд маш олон орон нутгийн хамгийн бага хэмжээнд байх боломжтой. Энэ нь мэдрэлийн сүлжээнд бага хэмжээний сурагчид адилхан татах боломжтой. өөрчлөлтийг багасгах нь илүү хүчтэй байдалтай загваруудыг шаардах боломжтой.", 'mt': 'Jekk l-istess arkitettura tan-netwerk newrali tiġi mħarrġa diversi drabi fuq l-istess sett ta’ dejta, se tagħmel ġeneralizzazzjonijiet lingwistiċi simili bejn l-istadji? Biex nistudjaw din il-mistoqsija, irranġajna 100 każ ta’ BERT fuq is-sett tad-dejta dwar l-Inferenza tal-Lingwa Naturali Multi ġenerika (MNLI) u vvalutawhom fuq is-sett tad-dejta HANS, li jevalwa l-ġeneralizzazzjoni sintattika fl-inferenza tal-lingwa naturali. Dwar is-sett ta’ żvilupp tal-MNLI, l-imġiba tal-każijiet kollha kienet konsistenti b’mod notevoli, b’preċiżjoni li tvarja bejn 83.6% u 84.8%. B’kuntrast qawwi, l-istess mudelli varjaw ħafna fil-prestazzjoni tal-ġeneralizzazzjoni tagħhom. Pereżempju, dwar il-każ sempliċi ta’ skambju ta’ oġġetti suġġetti (pereżempju, id-determinazzjoni li “t-tabib iżur l-avukat” ma jinvolvix “l-avukat iżur it-tabib”), il-preċiżjoni varjat minn 0.0% sa 66.2%. Tali varjazzjoni hija probabbli minħabba l-preżenza ta’ ħafna minimi lokali fis-superfiċje tat-telf li huma ugwalment attraenti għal student bi preġudizzju baxx bħan-netwerk newrali; it-tnaqqis tal-varjabilità jista’ għalhekk jeħtieġ mudelli b’preġudizzji induttivi aktar b’saħħithom.', 'ml': "If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs?  ഈ ചോദ്യത്തെ പഠിപ്പിക്കാന്\u200d ഞങ്ങള്\u200d 100 സാധാരണങ്ങള്\u200d ബെര്\u200dട്ടിയുടെ സ്വാഭാവിക ഭാഷയുടെ (MNLI) ഡാറ്റാസസെറ്റില്\u200d നിര്\u200dണയിച്ചിരിക്കുന്നു. അത് സ്വാഭാവിക ഭാഷയിലെ  എംഎംഎലി വികസിപ്പിക്കുന്ന സെറ്റില്\u200d, എല്ലാ സംഭവ്യങ്ങളുടെയും പ്രവര്\u200dത്തനത്തിന്റെയും പൂര്\u200dണ്ണമായിരുന്നു. 83. 6% വരെ 84. 8%. സ്റ്റാര്\u200dക്കിന്റെ വിരോധത്തില്\u200d, അതേ മോഡലുകള്\u200d അവരുടെ ജനറലേഷന്\u200d പ്രകടനത്തില്\u200d വ്യത്യസ്തമായി മാറിയിര ഉദാഹരണത്തിന് ഉദാഹരണമാകുന്നു, വിഷയത്തിന്റെ സ്വാപ്പിന്റെ എളുപ്പമായ കേസില്\u200d (ഉദാഹരണത്തിനായി 'ഡോക്ടര്\u200d വക്കീലിനെ സന്ദര്\u200dശിച്ചു' എന്ന് നിര്\u200dണയിക്ക ഇത്തരം മാറ്റങ്ങള്\u200d ന്യൂറല്\u200d നെറ്റര്\u200d നെറ്റര്\u200d നെറ്റെര്\u200dവര്\u200dക്ക് പോലുള്ള ഒരു കുറഞ്ഞ ബിയാസ് പഠിക്കുന്നവന്റെ സമാധാനത്തില്\u200d പല അതുകൊണ്ട് ശക്തിയുള്ള പ്രവര്\u200dത്തനങ്ങളുമായി മോഡലുകള്\u200d ആവശ്യമുണ്ടാകും.", 'no': 'Viss det same neuralnettverksarkitekturen er trent fleire gonger på det same dataset, vil det gjera liknande lingviske generaliseringar over køyr? For å studera dette spørsmålet, finn vi 100 instansar av BERT på datasettet for fleire generelle naturspråk (MNLI) og evaluerer dei på HANS-datasettet, som evaluerer syntaktiske generellisering i naturspråk. På MNLI-utviklingssettet vart oppførselen av alle instansar merkelig konsistent med nøyaktighet mellom 83,6 % og 84,8 %. I stjernkontrasten varierte dei same modelane i generelliseringsfunksjonen. For eksempel på enkelt tilfelle på byte av temaobjektet (f.eks. « legen besøkt advokaten » inneheld ikkje « advokaten besøkt legen »), er nøyaktighet variert frå 0,0 % til 66,2 %. Desse variasjonane er sannsynleg på grunn av tilstand til mange lokale minimum i tapte overflata som er likevel attraktiv til ein lærar med låg forståelse som ein neuralnettverk. Det kan derfor kreve modeller med sterke induksjonsbehandlingar for å redusera variabelen.', 'pl': 'Jeśli ta sama architektura sieci neuronowej jest trenowana wielokrotnie na tym samym zbiorze danych, czy dokona ona podobnych uogólnień językowych w różnych przebiegach? Aby zbadać to pytanie, dostroiliśmy setki instancji BERT na zbiorze danych Multi-gatunkowym Natural Language Inference (MNLI) i oceniliśmy je na zbiorze danych HANS, który ocenia uogólnienie składni w wnioskowaniu języka naturalnego. W zestawie rozwojowym MNLI zachowanie wszystkich instancji było niezwykle spójne, z dokładnością od 83.6% do 84.8%. W wyraźnym przeciwieństwie te same modele różniły się szeroko pod względem wydajności uogólnienia. Na przykład w prostym przypadku wymiany przedmiotów (np. ustalenie, że "lekarz odwiedził prawnika" nie oznacza "prawnik odwiedził lekarza"), dokładność wyróżniała się od 0,0% do 66.2%. Takie zróżnicowanie jest prawdopodobnie spowodowane obecnością wielu lokalnych minimal na powierzchni straty, które są równie atrakcyjne dla uczących się o niskim uprzedzeniu, takich jak sieć neuronowa; zmniejszenie zmienności może zatem wymagać modeli o silniejszych uprzedzeniach indukcyjnych.', 'ro': 'Dacă aceeași arhitectură de rețea neurală este instruită de mai multe ori pe același set de date, va face generalizări lingvistice similare în cadrul rulărilor? Pentru a studia această întrebare, am reglat fin 100 de instanțe de BERT pe setul de date Multi-gen Natural Language Inference (MNLI) și le-am evaluat pe setul de date HANS, care evaluează generalizarea sintactică în inferența limbajului natural. Pe setul de dezvoltare MNLI, comportamentul tuturor instanțelor a fost remarcabil de consistent, cu precizie cuprinsă între 83,6% și 84,8%. În contrast puternic, aceleași modele au variat foarte mult în performanța lor de generalizare. De exemplu, în cazul simplu de schimb subiect-obiect (de exemplu, stabilirea faptului că "medicul a vizitat avocatul" nu implică "avocatul a vizitat medicul"), acuratețea a variat între 0,0% și 66,2%. O astfel de variație se datorează probabil prezenței multor minime locale în suprafața pierderilor, care sunt la fel de atractive pentru un elev cu părtinire scăzută, cum ar fi o rețea neurală; Prin urmare, reducerea variabilităţii poate necesita modele cu prejudecăţi inductive mai puternice.', 'sr': "Ako je ista arhitektura neuronske mreže obučena više puta na istom setu podataka, hoće li to učiniti slične jezičke generalizacije preko puta? Da bi proučili ovo pitanje, ispravili smo 100 slučajeva BERT-a o višegenarnoj prirodnoj jezičkoj infekciji (MNLI) podataka i procenili ih na setu podataka HANS-a, koja procjenjuje sintaktičku generalizaciju u infekciji prirodnog jezika. Na setu razvoja MNLI-a, ponašanje svih slučajeva je bilo izuzetno konsistentno, sa tačnošću koja se raspada između 83,6% i 84,8%. U suprotnosti, isti modeli su se široko razlikuli u njihovoj generalizaciji. Na primer, u jednostavnom slučaju zamjene predmeta (na primer, utvrđivanje da 'doktor posjetio advokata' ne uključuje 'advokat posjetio doktora'), tačnost je bila od 0,0% do 66,2%. Takva varijacija je vjerojatno zbog prisustva mnogih lokalnih minima na površini gubitka koji su jednako privlačni učitelju niske predrasude kao što je neuralna mreža; Stoga smanjenje varijancije može zahtevati modele sa jačim induktivnim predrasudama.", 'si': "එකම න්\u200dයූරල් ජාලය සංවිධානය සමාන දත්ත සූදානයේ වඩා වතාවක් ප්\u200dරශ්නය කරලා තියෙනවනම්, ඒක වගේ භාෂාවික මේ ප්\u200dරශ්නය අධ්\u200dයානය කරන්න, අපි බෙර්ට් වලින් සාමාන්\u200dයය භාෂාවක් සම්පූර්ණය (MNLI) තොරතුරු සම්පූර්ණය සඳහා සම්පූර්ණය කරලා තියෙන්නේ HANS දත MNLI විකාශය සම්පූර්ණයෙන්, සියලුම සිද්ධ වෙනුවෙන් සිද්ධ වෙනුවෙන් සිද්ධ වෙනුවෙන්, 83.6% සහ 84.8% අතර අතර අතර ස්ටාර්ක් ප්\u200dරතිරූපයෙන්, ඒ වගේ මොඩේල්ස් වෙනස් වුනා ඔවුන්ගේ සාමාන්\u200dය ප්\u200dරතිරූපයේ උදාහරණය විදිහට, විදිහට-විදිහට සාමාන්\u200dය ස්වාප්ත විදිහට (උදාහරණයෙන්, විදිහට 'ඩොක්ටර් නියෝජිතයෙන් ආවා කියලා' විදිහට 'නීතිඥයෙන් ආව ඒ වගේ වෙනස් සමහරවිට ස්ථානික ප්\u200dරතිශාලයක් නිසා ස්ථානික ප්\u200dරතිශාලයක් තියෙන්න පුළුවන් වෙනුවෙන් ප්\u200dරතිශාල වෙනස් විදියට අඩුවෙන්න පුළුවන් ඒ නිසා වඩා ශක්තිමත් විදියට ප්\u200dරශ්නයක් තියෙන්නේ.", 'so': "Haddii la barto dhismaha shabakadda neurada oo isku mid ah, ma waxaa lagu sameyn karaa wax la mid ah oo la mid ah luqada luuqadda? Si aan u barno su'aalahan, waxaan 100 tusaalood oo BERT ah oo ku qornay macluumaadka ku saabsan Interference Luqada asalka ah (MNLI) oo kala duduwan, waxaana qiimeynay sawirada HANS, taasoo qiimeynaya dhalashada muuqashada afka dabiicadda ah. Xaaladaha horumarinta ee MNLI, dhaqdhaqaaqa xaaladaha oo dhammu waxay ahayd mid la mid ah, si saxda ah oo u dhexaysa 83.6% iyo 84.8 boqolkiiba. Iska mid ah samooyinkoodu waxay si badan u kala duwanyihiin tababarka dhalashada. Tusaale ahaan marka la wareejiyo arrimaha fudud (tusaale ahaan goobta dhakhtarku uu soo booqday qareenku ma ahan 'sharciga qareenka u soo booqday dhakhtarka'), si saxda ah waa 0,0 % ilaa 66,2 boqol. Isbedelka caynkaas ah waxaa suurtowda in laga helo kuwa ugu yaraan dhulka khasaare ah, taasoo si isku mid ah u macquul karta barashada hoose-beenta sida shabakadda neurada ah; Lacagta bedelka waxaa laga yaabaa in ay u baahan karaan tusaalooyin ay leedahay qalabka shaqada oo xoogga badan.", 'sv': 'Om samma neurala nätverksarkitektur tränas flera gånger på samma datauppsättning, kommer den att göra liknande språkliga generaliseringar över körningar? För att studera denna fråga finjusterade vi 100 instanser av BERT på datauppsättningen Multi-genre Natural Language Inference (MNLI) och utvärderade dem på HANS datauppsättningen, som utvärderar syntaktisk generalisering i naturlig språkinferens. På MNLI-utvecklingsuppsättningen var beteendet hos alla instanser anmärkningsvärt konsekvent, med en noggrannhet på mellan 83,6% och 84,8%. I skarp kontrast varierade samma modeller mycket i sin generaliseringsförmåga. I det enkla fallet med byte av objekt och objekt (t.ex. att fastställa att "läkaren besökte advokaten" inte innebär "advokaten besökte läkaren") varierade noggrannheten mellan 0,0% och 66,2%. Sådan variation beror sannolikt på förekomsten av många lokala minimivärden i förlustytan som är lika attraktiva för en lågpartisk elev, såsom ett neuralt nätverk. Att minska variabiliteten kan därför kräva modeller med starkare induktiva biaser.', 'ta': "அதே புதிய பிணைய வலைப்பின்னல் உருவாக்கி அதே தரவுத் தளத்தில் பல முறை பயிற்சி செய்யப்பட்டால், அது இயக்கும் மொழியில் போன்ற ம இந்த கேள்வியைப் படிக்க 100 நிகழ்வுகளை பெர்ட் முதலில் நாம் பல மரபணு இயற்கையான மொழி பாதுகாப்பு (MNLI) தகவல் அமைப்பில் குறிப்பிட்டோம் மற்றும் HANS தரவுத்தளத்தில் அவற்ற MNLI உருவாக்கும் அமைப்பில், அனைத்து நிகழ்வுகளின் நடத்தையும் ஒத்திசையாக இருந்தது, சரியாக 83. 6% மற்றும் 84. 8%. ஸ்டார்க் மாறுபாட்டில், அதே மாதிரிகள் பொதுவான செயல்பாட்டில் விரிவாகும். உதாரணத்திற்கு, பொருள் மாற்றத்தின் சுலபமான விஷயத்தில் ('மருத்துவர் வழக்கறிஞர் பார்த்தது' என்று உறுதிப்படுத்தினால், 'மருத்துவர் மருத்துவரை பார்த இது போன்ற மாறுபாடு புதிய வலைப்பின்னல் போன்ற ஒரு குறைந்த பியா கற்றுக்கொள்ளும் தோற்றத்தில் பல உள்ள குறைந்தபட்ச முறையில்  மாறுபாட்டை குறைத்து அதனால் வலுவான தொழில்நுட்ப புகளுடன் மாதிரிகளை தேவைப்படும்.", 'ur': "اگر یکساں نیورال نیٹ ورک معماری ایک ڈیٹ سٹ پر چند بار تعلیم کی جاتی ہے تو کیا یہ رانوں میں ویسی ہی زبان تعلیم کرے گا؟ اس سؤال کی تعلیم کے لئے ہم نے BERT کے 100 مثالیں بہت سی ژنرال طبیعی زبان انفارنس (MNLI) کے ذریعے مطالعہ پر قائم کر دیں اور ان کو HANS ڈیٹ سٹ کے ذریعے مطالعہ کر لیں، جو طبیعی زبان انفارنس میں سینٹکتیک ژنرالیزی کا ارز MNLI توسعہ کے سٹے پر، تمام امکانات کا رفتار معمولی طور پر قابل تھا، جسے 83.6% اور 84.8% کے درمیان تقریبا ہے۔ استارک مخالفت میں، اسی طرح کے نمونے ان کی عمومی کرم میں متفاوت ہوگئے۔ مثال کے مطابق، subject-object swap کے سادھے موضوع پر (مثال فیصلہ کرنا کہ 'دکتر نے وکیل کو ملا' کا 'وکیل نے ڈاکٹر کو ملا' نہیں ہے)، دقیق 0.0% سے 66.2% تک۔ یہ تغییر امید ہے کہ خسارہ سطح میں بہت سی موقعیت کی وجہ سے بہت سی موقعیت کی وجہ سے ہے جو ایک نیورل نیٹ ورک کے لئے برابر آراستہ ہیں۔ تبدیلی کاٹنے کی ضرورت ہے کہ مدل مضبوط طریقے کے ساتھ مضبوط طریقے کے مطابق ضرورت کریں.", 'uz': "If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs?  Bu savol o'rganish uchun biz bir necha xil tilning tarkibi (MNLI) haqida BERT 100 misol bilan bir necha xil tilning tarkibini o'rganamiz va ularni HANS maʼlumotlari tarkibida qiymatdik. Bu asl tilda sintaktik generalisisini qiymatga ega bo'ladi. MNLI tuzishda hamma hodisalarning xususiyatlari juda muhimiy bir xil edi, 83. 6% va 84.8% dan foydalanadi. Name Masalan, mavzu obʼekt oʻzgarishda oddiy narsa (масалан, doktor qarsoodi boshqaruvchiga qaragan qarsoodi 'shifokorga qarang' degan narsa emas), tashkilotni 0.0% dan 66.2%. Bu yerda o'zgarishlar nazar tarmoq kabi bir qancha lokal qismi mavjud boʻlishi sababchi bo'lishi mumkin. o'zgarishni kamaytirish mumkin, shunday qilib qo'lbola'yish mumkin.", 'vi': 'Nếu cấu trúc mạng thần kinh tương tự được huấn luyện nhiều lần trên cùng một bộ dữ liệu, nó có tạo ra những quy mô ngôn ngữ tương tự trên các đường chạy không? Để nghiên cứu câu hỏi này, chúng tôi hoàn thiện các trường hợp của BERT trong tập tin ngôn ngữ tự nhiên đa thể loại (MILL) và đánh giá chúng trên bộ dữ liệu HAS, ước lượng tổng hợp pháp trong ngụ ý ngôn ngữ tự nhiên. Trong trường hợp phát triển MNLI, hành vi của mọi trường hợp đều tương ứng đáng kể, với độ chính xác bắt đầu từ 83.6. và 84.8 Name Tuy nhiên, các mô hình giống nhau rất khác nhau trong quá trình tổng hợp. Ví dụ, trong trường hợp đơn giản về việc trao đổi vật thể (v.d., quyết định rằng "bác sĩ đã đến g ặp luật sư" không có nghĩa là "luật sư đã đến gặp bác sĩ". Sự chính xác diễn ra từ 0.0=-62 Name Sự biến đổi này có thể do sự hiện diện của nhiều tối thiểu địa phương trên bề mặt mất mát, cũng hấp dẫn đối với một học sinh có nhược điểm thấp như mạng thần kinh. giảm độ biến đổi có thể đòi hỏi mô- đun có suy nhược dẫn hoá mạnh hơn.', 'da': 'Hvis den samme neurale netværksarkitektur trænes flere gange på det samme datasæt, vil den så lave lignende sproglige generaliseringer på tværs af kørsler? For at undersøge dette spørgsmål finjusterede vi 100 forekomster af BERT på datasættet MNLI (Natural Language Inference) og evaluerede dem på HANS datasættet, som evaluerer syntaktisk generalisering i naturlig sprog inference. På MNLI-udviklingssættet var adfærden af alle instanser bemærkelsesværdigt konsekvent med en nøjagtighed på mellem 83,6% og 84,8%. I stærk kontrast varierede de samme modeller meget i deres generalisering ydeevne. For eksempel var nøjagtigheden mellem 0,0% og 66,2% i forbindelse med det simple tilfælde med udveksling af emner og genstande (f.eks. at fastslå, at"lægen besøgte advokaten"ikke indebærer"advokaten besøgte lægen"). En sådan variation skyldes sandsynligvis tilstedeværelsen af mange lokale minima i tabsoverfladen, der er lige så attraktive for en lav-bias elev, såsom et neuralt netværk; En reduktion af variabiliteten kan derfor kræve modeller med stærkere induktive skævheder.', 'nl': "Als dezelfde neurale netwerkarchitectuur meerdere keren wordt getraind op dezelfde dataset, zal het dan vergelijkbare linguïstische generalisaties maken over verschillende runs? Om deze vraag te bestuderen, hebben we honderden exemplaren van BERT op de multi-genre Natural Language Inference (MNLI) dataset verfijnd en geëvalueerd op de HANS dataset, die syntactische generalisatie in natuurlijke taal inferentie evalueert. Op de MNLI development set was het gedrag van alle instanties opmerkelijk consistent, met nauwkeurigheid variërend tussen 83.6% en 84.8%. In schril contrast verschilden dezelfde modellen sterk in hun generalisatieprestaties. Bijvoorbeeld, in het eenvoudige geval van subject-object swap (bijvoorbeeld vaststellen dat 'de arts de advocaat bezocht' niet betekent 'de advocaat bezocht de dokter'), varieerde de nauwkeurigheid van 0,0% tot 66.2%. Een dergelijke variatie is waarschijnlijk te wijten aan de aanwezigheid van veel lokale minima in het verliesoppervlak die even aantrekkelijk zijn voor een low-bias leerling, zoals een neuraal netwerk; Het verminderen van de variabiliteit kan daarom modellen met sterkere inductieve biases vereisen.", 'bg': 'Ако една и съща архитектура на невронната мрежа се обучава многократно на един и същ набор от данни, ще направи ли подобни лингвистични обобщения през пробите? За да проучим този въпрос, преценихме 100 инстанции на БРТ върху множествения жанров естествен езиков извод (MNLI) и ги оценихме на набор от данни който оценява синтактичното обобщение в естествения език извод. При разработката на MNLI поведението на всички инстанции е забележително последователно, с точност между 83,6% и 84,8%. В ярък контраст същите модели варират широко в тяхната обобщаваща ефективност. Например при простия случай на размяна субект-обект (например определяне, че "лекарят е посетил адвоката" не означава "адвокатът е посетил лекаря"), точността варира от 0,0% до 66,2%. Такава вариация вероятно се дължи на наличието на много локални минимуми в повърхността на загубата, които са еднакво привлекателни за обучаемите с ниски предразсъдъци, като невронна мрежа; Поради това намаляването на променливостта може да изисква модели с по-силни индуктивни отклонения.', 'de': 'Wenn dieselbe neuronale Netzwerkarchitektur mehrfach auf demselben Datensatz trainiert wird, wird sie dann ähnliche linguistische Verallgemeinerungen über Läufe hinweg vornehmen? Um diese Frage zu untersuchen, haben wir 100-Instanzen von BERT auf dem Multi-genre Natural Language Inference (MNLI) Datensatz feinjustiert und auf dem HANS-Datensatz ausgewertet, der syntaktische Generalisierung in der natürlichen Sprachinferenz evaluiert. Im MNLI-Entwicklungsset war das Verhalten aller Instanzen bemerkenswert konsistent, mit einer Genauigkeit zwischen 83.6% und 84.8%. Im krassen Gegensatz dazu variierten die gleichen Modelle sehr stark in ihrer Generalisierungsleistung. Zum Beispiel im einfachen Fall des Subjekt-Objekt-Swaps (z.B. die Feststellung, dass "der Arzt den Anwalt besucht" nicht bedeutet, dass "der Anwalt den Arzt besucht hat"), reichte die Genauigkeit von 0,0% bis 66,2%. Diese Variation ist wahrscheinlich auf das Vorhandensein vieler lokaler Minima in der Verlustoberfläche zurückzuführen, die für einen Low-Bias-Lernenden gleichermaßen attraktiv sind, wie ein neuronales Netzwerk; Die Verringerung der Variabilität kann daher Modelle mit stärkeren induktiven Verzerrungen erfordern.', 'hr': "Ako je ista arhitektura neuronske mreže obučena višestruko na istom setu podataka, hoće li to učiniti slične jezičke generalizacije preko puta? Za proučavanje ovog pitanja, ispravili smo 100 slučajeva BERT-a o skupini podataka o multigenarnoj prirodnoj jezici (MNLI) i procijenili ih na skupini podataka HANS-a, koja procjenjuje sintaktičku generalizaciju u infekciji prirodnog jezika. Na setu razvoja MNLI-a, ponašanje svih slučajeva bila je izuzetno konsistentno, s preciznošću u rasponu između 83,6% i 84,8%. U suprotnosti, isti modeli se široko razlikuju u njihovoj generalizaciji. Na primjer, u jednostavnom slučaju zamjene predmeta (npr. utvrđivanje da 'liječnik posjetio odvjetnika' ne uključuje 'odvjetnik posjetio liječnika'), to čnost je bila od 0,0% do 66,2%. Takva varijacija je vjerojatno zbog prisutnosti mnogih lokalnih minima na površini gubitka koji su jednako privlačni učitelju niske predrasude kao što je neuralna mreža; Stoga smanjenje varijancije može zahtijevati modele s jačim induktivnim predrasudama.", 'id': "Jika arsitektur jaringan saraf yang sama dilatih berbilang kali pada set data yang sama, apakah itu akan membuat generalisasi bahasa yang sama di seluruh jalur? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference.  On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%.  In stark contrast, the same models varied widely in their generalization performance.  Contohnya, dalam kasus sederhana swap subjek-objek (contohnya, menentukan bahwa 'dokter mengunjungi pengacara' tidak melibatkan 'pengacara mengunjungi dokter'), akurasi berlangsung dari 0,0% hingga 66,2%. Variasi seperti ini mungkin karena kehadiran banyak minima lokal di permukaan kehilangan yang sama menarik bagi pelajar bias rendah seperti jaringan saraf; menurunkan variabilitas mungkin membutuhkan model dengan bias induktif yang lebih kuat.", 'fa': 'اگر معماری شبکه عصبی چندین بار روی مجموعه داده\u200cها آموزش داده شود، آیا این مجموعه\u200cهای ژنرال زبان مشابه می\u200cکند؟ برای مطالعه این سؤال، ما ۱۰۰ نمونه از BERT را در مجموعه داده های طبیعی متعدد ژنتی (MNLI) تنظیم کردیم و آنها را در مجموعه داده های HANS ارزیابی می\u200cکردیم، که در آلودگی زبان طبیعی ارزیابی می\u200cکند. در مجموعه توسعه MNLI، رفتار همه\u200cی موارد بسیار مشترک بود، با دقیق بین 83.6% و 84.8%. در مقابل استارک، همان مدل\u200cها در عملکرد ژنرال\u200cسازی آنها متفاوت شدند. برای مثال، در مورد پرونده ساده تغییر شیوه\u200cهای موضوع (مثلاً تصمیم گرفتن که «دکتر به وکيل ملاقات کرد» به عنوان «وکيل به دکتر ملاقات کرد») دقیق از 0.0% تا 66.2% متفاوت نیست. این تغییرات احتمالا به خاطر وجود بسیاری از کمینه\u200cهای محلی در سطح خسارت است که همانطور به یک دانش\u200cآموز پایین طبقه\u200cای مثل شبکه عصبی جذاب دارند. در نتیجه کاهش تغییر قابل نیاز به مدل\u200cها با تغییر قوی\u200cترین تغییرات دارد.', 'sw': "Kama ujenzi huo ule wa mtandao wa neura unafundishwa mara kadhaa kwenye seti hiyo ya data, je, utafanya vizazi vinavyofanana na lugha katika maeneo mengine yanayoendeshwa? Ili kusoma swali hili, tulitumia matukio 100 ya BERT kwenye seti ya taarifa za kuzuia lugha za asili za asili (MNLI) na kutathmini kwenye seti ya taarifa za HANS, ambazo zinatathmini uzalishaji wa muungano kwa lugha ya asili. Katika seti ya maendeleo ya MNLI, tabia za matukio yote zilikuwa ni sawa sana, na kwa uhakika ulikuwa kati ya asilimia 83.6 na asilimia 84.8. Katika tofauti na macho, mifano hiyo ilitofautiana sana katika utendaji wao wa uzalishaji. For example, on the simple case of subject-object swap (e.g., determining that 'the doctor visited the lawyer' does not entail 'the lawyer visited the doctor'), accuracy ranged from 0.0% to 66.2%.  Mabadiliko hayo yanawezekana kutokana na uwepo wa madini mengi katika uso wa kupoteza ambao unavutiwa sana na mwandishi wa kibiashara wa chini kama vile mtandao wa neura; kupunguza mabadiliko yanaweza kwa hiyo inahitaji mifano yenye upendeleo mkali wa viwanda.", 'ko': "만약 같은 신경 네트워크 구조가 같은 데이터 집합에서 여러 번 훈련한다면, 그것은 서로 다른 운행에서 유사한 언어 개괄을 할 수 있습니까?이 문제를 연구하기 위해 우리는 여러 장르의 자연언어추리(MNLI) 데이터 집합에 100개의 BERT 실례를 미세하게 조정하고 자연언어추리에서 문법이 범용된HANS 데이터 집합을 평가하여 그것들을 평가했다.MNLI 개발집에서 모든 실례의 행동은 83.6%에서 84.8% 사이로 매우 일치한다.이와 뚜렷한 대비를 이루는 것은 같은 모델이 범화 성능에 있어 차이가 크다는 것이다.예를 들어 주객체 교환의 간단한 사례(예를 들어'의사가 변호사를 방문했다'고 확정한 것은'변호사가 의사를 방문했다'는 것을 의미하지 않는다)에서 정확도는 0.0%에서 66.2%까지 다르다.이런 변화는 손실면에 많은 부분적인 극소값이 존재하기 때문에 이런 극소값은 낮은 편차 학습자(예를 들어 신경 네트워크)에 동등한 흡인력을 가진다.따라서 가변성을 낮추려면 더 강한 유도 편차를 가진 모델이 필요할 수 있다.", 'tr': "Eger bir neural şebeke arhitektegi birnäçe gezek bir datasetde okuwılan bolsa, şol ýerlerde bir lingwistiki jenerallaşdyrmak üçin meňzeýärmi? Bu soragy öwrenmek üçin, birnäçe ژençe Taryh dili Inferensiýasy (MNLI) barada BERT'yň 100 sanyny düzenledik we olaryň HANS veri setinde ynamly gürrüňde syntaktik döredilişini çykardik. MNLI ösüşiklik düzeninde, ähli ýagdaýlaryň davranmasy gaty bir ýagdaýdyr, 83.6% we 84.8% aralygynda degişlidi. Ýyldyr kontrast, birnäçe nusgalary döredilik ukyplarynda üýtgeşik boldy. mysal, subjekt üýtgetmeginiň basit ýagdaýynda (meselâ, 'lukman tarapyndan ýüz tutan lukmanyň 'lukmanyň g örmegi') dogrylygyny 0,0% we 66,2% diýip kabul etmeýär. Bärde ýerleşdirilen iň kiçi ýerleşdirilen iň kiçi ýerleşdirilen iň kiçi görnüşlere, näyral şebeke ýaly iň kiçi-biçi öwrenmegi üçin beýle bir üýtgeşik bar. Äýtgeşik düşürmek üçin nusgalary ýok etmäge gerek bolup biler.", 'sq': "Nëse e njëjta arkitekturë e rrjetit nervor është stërvitur shumë herë në të njëjtin set të dhënash, a do të bëjë gjeneralizime gjuhësore të ngjashme nëpër rrugë? Për të studiuar këtë pyetje, ne rregulluam 100 raste të BERT në grupin e të dhënave të Multi-Genre Natural Language Inference (MNLI) dhe i vlerësuam ato në grupin e të dhënave HANS, i cili vlerëson gjeneralizimin sintaktik në inferencën natyrore të gjuhës. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%.  Në kontrast të fortë, të njëjtat modele ndryshuan gjerësisht në performancën e tyre të gjeneralizimit. Për shembull, në rastin e thjeshtë të ndërrimit subjekt-objekt (për shembull, përcaktimi se 'mjeku vizitoi avokatin' nuk përfshin 'avokati vizitoi mjekun'), saktësia ishte nga 0.0% në 66.2%. Variacioni i tillë ka gjasa për shkak të pranisë së shumë minimave lokale në sipërfaqen e humbjes që janë njëlloj tërheqëse për një mësues të pakëndshëm të tillë si një rrjet nervor; zvogëlimi i variabilitetit mund të kërkojë kështu modele me paragjykime më të forta induktive.", 'af': "As die selfde neuralnetwerk-arkitektuur veelvuldige maal op dieselfde datastel onderwerp is, sal dit soortgelyke lingwisiese generalisering maak oor hardloope? Om hierdie vraag te ondersoek, het ons 100 voorbeelde van BERT gevind op die Multi genre Natuurlike Taal Inferensie (MNLI) datastel en hulle geevalueer op die HANS datastel, wat sintaktika generalisering in natuurlike taal inferensie evalueer. Op die MNLI ontwikkelingsstel was die gedrag van alle gevalle betekenlik konsistent, met die presisie tussen 83,6% en 84,8%. In sterk kontras het dieselfde modele vaste verskil in hulle generellisering prestasie. Byvoorbeeld, op die eenvoudige geval van onderwerp- voorwerp verandering (bv. die bepaal dat 'die dokter besoek die advokaat' nie 'die advokaat besoek die dokter' bedoel nie), waarskynlik gewys van 0.0% tot 66.2%. Soos veranderinge is waarskynlik vanweë die aangesig van baie plaaslike minima in die verlore oortjie wat gelyk aantrek is aan 'n lae-bias leerneerder soos 'n neurale netwerk; Die veranderlikheid kan daarom modele met sterker induktiewe voorspoedings nodig.", 'am': "በአንድ ደብዳቤ መረብ የመዝገብ መሠረት እጥፍ እጥፍ በሚያስተምርበት ከሆነ፥ በመሮጫው የሚመስል የቋንቋ ቋንቋ አቀማመጥ ይችላልን? ይህንን ጥያቄ ለማስተምር 100 ምሳሌዎች BERT በተለየ ብሔራዊ የፍጥረት ቋንቋ መግለጫ (MNLI) ዳታተር ሰጥተናል፡፡ በተለየ ቋንቋ ውጤት ላይ ሲንስታዊ አፍቃሪውን በሚያስተምርበት የሐናንS ዳታተር ሰርቨርስቲ እና አስተዋልናቸዋለን፡፡ በMNLI ግንኙነት ላይ፣ የሁኔታ ሁኔታ ሁሉ እየተመሳሳይ ነው፣ በ83.6 በመቶ እና 84.8 በመቶ መካከል የተለየ ነው፡፡ በተጨማሪው ግንኙነት፣ እንዲሁ ምሳሌዎች በተለየ አቀማመጥ እውቀት ይለየቃሉ፡፡ For example, on the simple case of subject-object swap (e.g., determining that 'the doctor visited the lawyer' does not entail 'the lawyer visited the doctor'), accuracy ranged from 0.0% to 66.2%.  እንደዚህ መልዕክት ብዙዎች የቅርብ አነስተኛዎችን በመቀበል፣ እንደነዚህ የነጥብ መረብ እንደተማረ ታዋቂውን በሚያሳስብ ጥያቄ ነው፡፡ ለውጤት ማጎድል ይችላል፡፡", 'bn': "যদি একই নিউরেল নেটওয়ার্ক কাঠামো একই ডাটাসেটে বেশ কয়েকবার প্রশিক্ষণ প্রদান করা হয়, তাহলে এটি কি পার্শ্ববর্তী ভাষাগত জেনারেলিজ এই প্রশ্ন গবেষণার জন্য আমরা প্রাকৃতিক ভাষার সংক্রান্ত স্বাভাবিক ভাষার (এমএনলি) ডাটাসেট (এমএনলি) বিভিন্ন ১০০ অনুষ্ঠানের মাধ্যমে ভালোভাবে প্রাকৃতিক ভাষা On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%.  স্ট্রাক বিপরীতে, একই মডেল তাদের সাধারণ প্রদর্শনীতে ব্যাপক ভিন্ন। উদাহরণস্বরূপ, বিষয়বস্তু পরিবর্তনের সাধারণ মামলায় (উদাহরণস্বরূপ 'ডাক্তার আইনজীবীকে পরিদর্শন করেছে' উকিল 'ডাক্তার পরিদর্শন করেছেন' তার মানে নেই) সঠিক পর এই ধরনের পরিবর্তন সম্ভবত নিউরেল নেটওয়ার্কের মতো অনেক স্থানীয় মিনিমার উপস্থিতির কারণে; তাই শক্তিশালী শিল্পী বিভেদের মোডেলের প্রয়োজন হতে পারে।", 'hy': 'If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs?  To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference.  MNSI-ի զարգացման համակարգում բոլոր դեպքերի վարքագիծը շատ համապատասխան էր, ճշգրտությունը տարբերվում էր 836-84.8 տոկոսից: Ի հսկայական հակադրություն, նույն մոդելները շատ տարբերվում էին իրենց ընդհանուր գործողություններում: Օրինակ, առարկաների փոխանակության պարզ դեպքում (օրինակ\' որոշելով, որ "բժիշկը այցելել է իրավաբանին" չի ներառում "իրավաբանը այցելել է բժիշկին"), ճշգրտությունը տարբերվում էր 0.0-66.2 տոկոսից: Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network;  այդ պատճառով, փոքրացնելով տարբերակությունը կարող է պահանջել մոդելներ, որոնք ավելի ուժեղ ինդուկտիվ կողմնականություններ ունեն:', 'ca': 'Si la mateixa arquitectura de xarxa neural està entrenada múltiples vegades en el mateix conjunt de dades, farà generalitzacions lingüístices semblants a través de les execucions? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference.  On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%.  En gran contrast, els mateixos models van variar molt en el seu rendiment de generalització. Per exemple, en el simple cas d\'intercanvi d\'objectes (per exemple, determinar que "el metge va visitar l\'advocat" no implica "l\'advocat va visitar el metge"), la precisió va variar del 0,0% al 66,2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network;  disminuir la variabilitat pot, per tant, requerir models amb tendències inductives més fortes.', 'cs': 'Pokud bude stejná architektura neuronové sítě trénována několikrát na stejné datové sadě, bude provádět podobné jazykové zobecnění napříč běhy? Pro studium této otázky jsme doladili sto instancí BERT na multižánrovém datovém setu MNLI (Natural Language Inference) a vyhodnotili je na datovém setu HANS, který hodnotí syntaktickou generalizaci v inferenci přirozeného jazyka. Ve vývojové sadě MNLI bylo chování všech instancí pozoruhodně konzistentní, s přesností mezi 83,6% a 84,8%. Naproti tomu se stejné modely velmi lišily ve svém zobecnění výkonu. Například v jednoduchém případě výměny předmětu a objektu (např. stanovení, že "lékař navštívil právníka" neznamená "právník navštívil lékaře"), přesnost sahala od 0,0% do 66,2%. Taková odchylka je pravděpodobně způsobena přítomností mnoha lokálních minim na povrchu ztráty, které jsou stejně atraktivní pro studenty s nízkým zaujatostí, jako je neuronová síť; Snížení variability proto může vyžadovat modely s silnějšími indukčními předběhy.', 'et': 'Kui sama närvivõrgu arhitektuuri koolitatakse mitu korda samas andmekogumis, kas see teeb sarnaseid keelelisi üldistusi läbi jooksude? Selle küsimuse uurimiseks täpsustasime 100 BERT eksemplari Multi-žanr Natural Language Inference (MNLI) andmekogumil ja hindasime neid HANS andmekogumil, mis hindab süntaktilist üldistamist looduskeele järeldustes. MNLI arenduskomplekti puhul oli kõigi juhtumite käitumine märkimisväärselt järjepidev, täpsusega vahemikus 83,6% kuni 84,8%. Täiesti vastupidiselt varieerusid samad mudelid oma üldistamisjõudluse poolest suuresti. Näiteks subjekti ja objekti vahetamise lihtsa juhtumi puhul (nt selle kindlaksmääramine, et "arst külastas advokaati" ei tähenda "advokaat külastas arsti") oli täpsus vahemikus 0,0–66,2%. Selline varieerumine on tõenäoliselt tingitud paljude kohalike miinimumide olemasolust kaotuspinnal, mis on võrdselt atraktiivsed madala kalduvusega õppijale, näiteks närvivõrgule; Seetõttu võib varieeruvuse vähendamine nõuda tugevamate induktiivsete kallakutega mudeleid.', 'fi': 'Jos samaa neuroverkkoarkkitehtuuria koulutetaan useita kertoja samasta aineistosta, tehdäänkö se samanlaisia kielellisiä yleistyksiä juoksuissa? Tätä kysymystä varten hienosäädimme 100 BERT-tapausta Multi-genre Natural Language Inference (MNLI) -aineistossa ja arvioimme niitä HANS-aineistossa, joka arvioi synteettistä yleistymistä luonnollisen kielen päättelyssä. MNLI-kehityssarjassa kaikkien tapausten käyttäytyminen oli huomattavan johdonmukaista, tarkkuus oli 83,6–84,8%. Samojen mallien yleistyskyky vaihteli jyrkästi päinvastoin suuresti. Esimerkiksi kohteen ja esineen vaihtoon liittyvässä yksinkertaisessa tapauksessa (esimerkiksi sen toteaminen, että "lääkäri kävi asianajajan luona" ei merkitse sitä, että "asianajaja kävi lääkärin luona") tarkkuus vaihteli 0,0 prosentista 66,2 prosenttiin. Tällainen vaihtelu johtuu todennäköisesti siitä, että häviön pinnalla on monia paikallisia minimejä, jotka ovat yhtä houkuttelevia matalan puolueen oppijalle, kuten hermoverkolle. Muuttuvuuden vähentäminen voi siksi edellyttää malleja, joilla on voimakkaammat induktiiviset harhat.', 'az': 'Əgər aynı nöral ağ arhitektarı eyni verilən qutusunda çox dəfə təhsil edilirsə, bu cürbəcür dil generalizasyonuna bənzər ola bilər mi? Bu sual öyrənmək üçün, çoxlu-genr Təbiətli Dil Inference (MNLI) veri qutusunda BERT 100 misallarını düzəltdik və onları təbiətli dildə sintaktik generalizasyonu değerlendirir HANS veri qutusunda değerlendirdik. MNLI inkişaf qurduğunda, bütün məsələlərin davranışları, 83,6% ilə 84,8% arasında ədalətlə müəyyən edilmişdir. Yıldız müqayisədə, aynı modellər generalizasyon performansında genişliyə çevrildi. Misal olaraq, subjekt dəyişdirilməsi üçün basit vəziyyətdə (məsələn, "doktor ziyarət etdi" adətinin "avukat ziyarət etdi" deyildir), doğruluğu 0,0 % ilə 66,2 % dəyişdi. Bütün bu dəyişikliklər, neyral a ğı kimi aşağı təsirli öyrənənənlərə bənzər bir çox yerli minim olmasına görədir. Müxtəlifliyi azaltmaq üçün daha qüvvətli təsirli modellər lazımdır.', 'bs': "Ako je ista arhitektura neuronske mreže obučena više puta na istom setu podataka, hoće li to učiniti slične jezičke generalizacije preko puta? Da bi proučili ovo pitanje, ispravili smo 100 slučajeva BERT-a o kompletu podataka o multigenarnoj prirodnoj jezici (MNLI) i procenili ih na kompletu podataka HANS-a, koja procjenjuje sintaktičku generalizaciju u infekciji prirodnog jezika. Na setu razvoja MNLI-a, ponašanje svih slučajeva bilo je izuzetno konsistentno, s preciznošću u rasponu između 83,6% i 84,8%. U suprotnosti, isti modeli su se široko razlikovali u svojoj generalizaciji. Na primjer, u jednostavnom slučaju zamjene predmeta (npr. utvrđivanje da 'doktor posjetio advokata' ne uključuje 'advokat posjetio doktora'), tačnost je bila od 0,0% do 66,2%. Takva varijacija je vjerojatno zbog prisustva mnogih lokalnih minima na površini gubitka koji su jednako privlačni učitelju niske predrasude kao što je neuralna mreža; Stoga smanjenje variabilnosti može zahtijevati modele sa jačim induktivnim predrasudama.", 'sk': 'Če je enaka arhitektura nevronskega omrežja večkrat usposobljena na istem naboru podatkov, ali bo naredila podobne jezikovne posplošitve med teki? Za preučitev tega vprašanja smo natančno nastavili 100 primerov BERT na naboru podatkov Multi-žanrske sklepe naravnega jezika (MNLI) in jih ocenili na naboru podatkov HANS, ki ocenjuje sintaktično generalizacijo v sklepanju naravnega jezika. Pri množici razvoja MNLI je bilo vedenje vseh primerov izjemno dosledno, z natančnostjo med 83,6% in 84,8%. V velikem nasprotju so se isti modeli zelo razlikovali v njihovi generalizacijski zmogljivosti. Na primer, pri preprostem primeru zamenjave subjekta-predmeta (npr. ugotovitev, da "zdravnik je obiskal odvetnika" ne pomeni "odvetnik je obiskal zdravnika") je bila natančnost med 0,0% in 66,2%. Takšna razlika je verjetno posledica prisotnosti številnih lokalnih minimumov na površini izgube, ki so enako privlačni za učence z nizkimi pristranskostmi, kot je nevronsko omrežje; zmanjšanje variabilnosti lahko zato zahteva modele z močnejšimi induktivnimi pristranskostmi.', 'he': 'אם אותה ארכיטקטורת רשת עצבית מאומנת מספר פעמים באותו קבוצת נתונים, האם היא תעשה כלליות שפתיות דומות ברציפות? כדי ללמוד את השאלה הזאת, אנחנו מצדכנו 100 מקרים של BERT על קבוצת נתונים של שפת טבעית (MNLI) Multi-Genre Inference טבעית (MNLI) והערכנו אותם על קבוצת נתונים HANS, אשר מעריכה את הגנרליזציה סינטקטית בהערכת שפת טבעית. על קבוצת הפיתוח של MNLI, התנהגות של כל המקרים היתה תואמת באופן יוצא דופן, עם מדויקה בין 83.6% ל-84.8%. בניגוד לגמרי, אותם דוגמנים השתנו באופן רחב בהופעה הגנרליזציה שלהם. לדוגמה, במקרה הפשוט של החלפה של נושאים (לדוגמה, לקבוע כי "הרופא ביקר את עורך הדין" לא מעורב "עורך הדין ביקר את הרופא"), מדויקת התרחשה מ-0.0% ל-66.2%. שינוי כזה סביר להניח בגלל הנוכחות של מינימום מקומיים רבים בשטח האובדן שהם באותה מידה מושכים למלמד בעוון נמוך כמו רשת עצבית; להפחית את ההתנהגות עלולה לכן לדרוש דוגמנים עם חיונים induktivיים חזקים יותר.', 'ha': "Idan an sanar da tsarin tsarin taruwar neural masu yawa a kan daidaita data guda, shin, za ya sami abubuwa masu daidaita linguistic bakin tafiyar da? Ko karanta wannan tambayar, ba mu sami 100 misãlai na BERT a kan data na multi-genre Lugha Infekt (MNLI) kuma muka ƙaddara su a kan tsarin hasNS, wanda yana iya ƙaddara samanin syntactic cikin kasar harshe na natura. On the set of MNLI, aikin duk shiryoyin ayuka sun kasance mai girma yana daidaita, da tsari tsakanin 83.6% da 84.8%. In motsi da ke cikin jerk, misãlai masu motsi suka fi girma cikin shawarar da suka nuna. Misali, a kan musanya mai sauri na abun-abun (misali, ka ƙayyade cewa 'daki'in ya visiti shekara' bã ya kamãta 'Advokaci ya visiti daktari'), gaske na tsakanin daga 0.0% zuwa 66.2%. Ana yiwuwa a ƙididdige wannan variant ya kamata a presence da wasu ƙarami masu ƙaranci cikin bakin haske da za'a daidai da kwamfyuta zuwa wani mai kwamfyuta-biyar kamar wata shirin neural; Reduce variance kan kwamfyutan ya buƙata misãlai da sauri masu ƙaranci.", 'jv': "Drono iku, nek akeh sistem sing beraksi sing ditambah akeh sistem sing ngewehi, kaya ngono nggawe barang langgar sampek ? Ngawe ngerasai kesempatan iki, kéné iso nggambar barang-barang kotak tentang karo BERT tentang kanggo langgambar terêng-terêng (MNLI) Nang setung nggawe MNLI, dadi kapan kanggo kalah-kapan ingkang dadi iki dadi, lan kabèh dumadhi kanggo tatara wigatining Jejaring Saiki, ngenanye sake perusahaan kanggo ngilanggar-object (mm.isar, n.e.g. 'Doktoran njaluk dhèwèké bakal terusani 'Aku dhèwèké bakal terusani uwong'), dakasakno sing wis mulai 0.0% sampek 6.2%. Meh berusahaan langkung kayané piye paling-piye paling mbutuhak drawable-action", 'bo': "གལ འདི་ལྟ་ཞིབ་བྱས་ན། ང་ཚོའི་རྗེས་སུ་བཏོན་ཡོད་པའི་སྐད་ཆ་༡༠༠་ཙམ་ལ་ཞིབ་འཇུག་བྱས་ན། MNLI་གོང་འཕེལ་རྒྱབ་སྐྱོར་གྱི་གནས་སྟངས་ལ་བཤད་པ་ཡིན། དཔེར་རྣམས་ཀྱི་བྱ་སྟངས་ཡོད་ཚད་གསལ་པོ་ཞིག་ཡོད། མཐའ་ནས་འགྱུར་མཚམས་ ལྕགས་རིས་མཐོ་བོ་དང་མིན་པ་གཉིས་ཀྱི་སྤྱིར་བཏང་བའི་སྣེ་ཚོགས་རྣམས་འགྱུར་བ་ཡིན། དཔེར་ན། དངོས་པོ་དངོས་པོ་ཚགས་ཀྱི་སྟབས་བདེ་བཤད་ན་(དཔེར་ན། 'doktor་གཟུགས་བླངས་པ་དེ་' སྨན་པ་ཞིག་ནང་ལ་བསྐྱོད་བྱེད་སྐབས་མེད་'ཁས་པ་ཞིག་གི་གནད་སྡ གལ་སྲིད་འདི་ལྟ་བུའི་ནང་ནས་རང་ཁུལ་གྱི་ཆུང་ཤོས་ཡོད་ཚད་མང་པོ་ཞིག་ཡིན་པས། འདི་ལྟར་འགྱུར་ཚད་དམའ་རུ་གཏོང་བ་ཡིན་པའི་མིག་གཟུགས་རིས་ནི་སྟོབས་ཤུགས་ཀྱི་འགྱུར་བ་དགོས་རེད།"}
{'en': 'Second-Order NLP Adversarial Examples NLP  Adversarial Examples', 'ar': 'أمثلة على الخصومة من الدرجة الثانية في البرمجة اللغوية العصبية', 'pt': 'Exemplos Adversários de PNL de Segunda Ordem', 'fr': 'Exemples contradictoires de PNL de second ordre', 'es': 'Ejemplos adversarios de PNL de segundo orden', 'ja': 'セカンドオーダーNLP対抗例', 'hi': 'द्वितीय-क्रम NLP प्रतिकूल उदाहरण', 'zh': '二阶 NLP 对抗性示例', 'ru': 'Примеры состязательности NLP второго порядка', 'ga': 'Samplaí Sáraíochta NLP den Dara hOrdú', 'hu': 'Másodrendű NLP-negatív példák', 'ka': 'Comment', 'it': 'Esempi avversi di PNL di secondo ordine', 'kk': 'Екінші рет NLP конверсариялық мысалдар', 'el': 'Αντίθετα παραδείγματα NLP δεύτερης τάξης', 'mk': 'Second-Order NLP Adversarial Examples', 'ml': 'രണ്ടാമത്തെ ഓര്\u200dഡര്\u200d NLP മുന്\u200dഗണന ഉദാഹരണങ്ങള്\u200d', 'mt': 'Eżempji Adversarji tat-Tieni Ordni NLP', 'lt': 'Antrojo eilės NLP prieštaringi pavyzdžiai', 'ms': 'Contoh Perlawanan NLP Arahan Kedua', 'mn': 'Хоёрдугаар дарааллаар NLP дэмжлэх жишээ', 'no': 'Eksempel på andre rekkefølgje NLP-rekkefølgje', 'ro': 'Exemple adverse PNL de ordinul doi', 'pl': 'Przykłady niekorzystne NLP drugiego rzędu', 'sr': 'Drugi red NLP Adversarial primjeri', 'si': 'Comment', 'so': 'Tusaale-asalka labaad ee NLP', 'sv': 'Andra ordningens NLP-negativa exempel', 'ta': 'Second-Order NLP Adversarial Examples', 'ur': 'دوسری اوردر NLP اپنا مثال', 'uz': '@ info: whatsthis', 'vi': 'Nhị triệu tập:', 'bg': 'Рекламни примери за НЛП от втори ред', 'nl': 'Tweede orde NLP Adversaire voorbeelden', 'da': 'Anden ordens NLP-negative eksempler', 'hr': 'Drugi redovni primjeri NLP-a', 'de': 'NLP-Gegenbeispiele zweiter Ordnung', 'id': 'Contoh Adversarial NLP Order Kedua', 'ko': '2 단계 NLP 적합성 예', 'fa': 'نمونه های مخالف NLP دوم دستور', 'sw': 'Mfano wa NLP wa Pili', 'tr': 'Ikinji-ýyl NLP Namaýyşy', 'sq': 'Shembuj kundërshtarë NLP të rendit të dytë', 'af': 'Tweede volgorde NLP Adversariale Voorbeelde', 'am': 'የፊደል ቅርጽ ምርጫዎች', 'hy': 'Երկրորդ հակառակ օրինակներ', 'az': '캻kinci S캼radan NLP M톛s톛ll톛r', 'bn': 'দ্বিতীয়- ক্ষেত্র NLP প্রধান উদাহরণ', 'bs': 'Drugi red NLP Adversarial primjeri', 'ca': 'Exemples adversaris del NLP de segona ordre', 'cs': 'Nepříznivé příklady NLP druhého řádu', 'et': 'NLP teise järgu kõrvaltoimed', 'fi': 'Toisen luokan NLP-haittaesimerkkejä', 'jv': 'Second-order NLP Advertary Samsul', 'sk': 'Neželeni primeri NLP drugega reda', 'he': 'דוגמאות נגד NLP מסדר שני', 'bo': 'Second-Order NLP Adversarial Examples', 'ha': '@ action'}
{'en': 'Adversarial example generation methods in  NLP  rely on models like  language models  or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the  model  being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the  model  that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the  robustness  of a  constraint  to second-order adversarial examples. To generate this  curve , we design an  adversarial attack  to run directly on the semantic similarity models. We test on two  constraints , the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as  constraint  on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.', 'ar': 'تعتمد طرق توليد الأمثلة العدائية في البرمجة اللغوية العصبية على نماذج مثل نماذج اللغة أو مشفرات الجمل لتحديد ما إذا كانت أمثلة الخصومة المحتملة صالحة أم لا. في هذه الأساليب ، يخدع مثال الخصم الصالح النموذج الذي يتم مهاجمته ، ويتم تحديده على أنه صالح لغويًا أو نحويًا بواسطة نموذج ثانٍ. حسب البحث حتى الآن كل هذه الأمثلة على أنها أخطاء بالنموذج المهاجم. نؤكد أن هذه الأمثلة العدائية قد لا تكون عيوبًا في النموذج المهاجم ، ولكنها عيوب في النموذج الذي يحدد الصلاحية. نحن نطلق على هذه المدخلات غير الصالحة أمثلة معادية من الدرجة الثانية. نقترح منحنى متانة القيد ، و ACCS المتري المرتبط به ، كأدوات لتقييم متانة أحد القيود لأمثلة الخصومة من الدرجة الثانية. لإنشاء هذا المنحنى ، نصمم هجومًا عدائيًا ليعمل مباشرة على نماذج التشابه الدلالية. نقوم باختبار قيدين ، التشفير الشامل (USE) و BERTScore. تشير النتائج التي توصلنا إليها إلى وجود مثل هذه الأمثلة من الدرجة الثانية ، ولكنها عادةً ما تكون أقل شيوعًا من أمثلة الخصومة من الدرجة الأولى في أحدث النماذج. تشير أيضًا إلى أن الاستخدام فعال كقيد على أمثلة معادية في البرمجة اللغوية العصبية ، في حين أن BERTScore غير فعال تقريبًا. رمز إجراء التجارب في هذه الورقة متاح هنا.', 'es': 'Los métodos de generación de ejemplos contradictorios en PNL se basan en modelos como modelos de lenguaje o codificadores de oraciones para determinar si los posibles ejemplos contradictorios son válidos. En estos métodos, un ejemplo contradictorio válido engaña al modelo que está siendo atacado, y un segundo modelo determina que es semántica o sintácticamente válido. Las investigaciones realizadas hasta la fecha han considerado todos esos ejemplos como errores del modelo atacado. Sostenemos que estos ejemplos contradictorios pueden no ser fallas en el modelo atacado, sino fallas en el modelo que determina la validez. A estas entradas inválidas las denominamos ejemplos contradictorios de segundo orden. Proponemos la curva de robustez de restricciones y el ACCS métrico asociado, como herramientas para evaluar la solidez de una restricción a ejemplos contradictorios de segundo orden. Para generar esta curva, diseñamos un ataque de confrontación para que se ejecute directamente en los modelos de similitud semántica. Hacemos pruebas en dos restricciones, Universal Sentence Encoder (USE) y BertScore. Nuestros hallazgos indican que existen tales ejemplos de segundo orden, pero por lo general son menos comunes que los ejemplos contradictorios de primer orden en los modelos de última generación. También indican que USE es efectivo como restricción en los ejemplos adversarios de PNL, mientras que BertScore es casi ineficaz. El código para ejecutar los experimentos de este documento está disponible aquí.', 'pt': 'Os métodos de geração de exemplos adversários na PNL dependem de modelos como modelos de linguagem ou codificadores de sentenças para determinar se os exemplos adversários em potencial são válidos. Nesses métodos, um exemplo adversário válido engana o modelo que está sendo atacado e é determinado como válido semanticamente ou sintaticamente por um segundo modelo. As pesquisas até o momento contaram todos esses exemplos como erros do modelo atacado. Defendemos que esses exemplos adversários podem não ser falhas no modelo atacado, mas falhas no modelo que determina a validade. Chamamos essas entradas inválidas de exemplos adversários de segunda ordem. Propomos a curva de robustez da restrição, e a métrica associada ACCS, como ferramentas para avaliar a robustez de uma restrição para exemplos adversários de segunda ordem. Para gerar essa curva, projetamos um ataque adversário para ser executado diretamente nos modelos de similaridade semântica. Testamos em duas restrições, o Universal Sentence Encoder (USE) e o BERTScore. Nossas descobertas indicam que tais exemplos de segunda ordem existem, mas são tipicamente menos comuns do que exemplos adversários de primeira ordem em modelos de última geração. Eles também indicam que USE é eficaz como restrição em exemplos adversários de PNL, enquanto BERTScore é quase ineficaz. O código para executar os experimentos neste documento está disponível aqui.', 'fr': "Les méthodes de génération d'exemples contradictoires en PNL s'appuient sur des modèles tels que des modèles de langage ou des encodeurs de phrases pour déterminer si les exemples contradictoires potentiels sont valides. Dans ces procédés, un exemple contradictoire valide trompe le modèle attaqué, et est déterminé comme étant sémantiquement ou syntaxiquement valide par un second modèle. Les recherches effectuées à ce jour ont compté tous ces exemples comme des erreurs du modèle attaqué. Nous soutenons que ces exemples contradictoires ne sont peut-être pas des défauts dans le modèle attaqué, mais des défauts dans le modèle qui détermine la validité. Nous appelons ces entrées invalides des exemples contradictoires de second ordre. Nous proposons la courbe de robustesse de la contrainte, et la métrique ACCS associée, comme outils pour évaluer la robustesse d'une contrainte par rapport à des exemples contradictoires de second ordre. Pour générer cette courbe, nous concevons une attaque contradictoire qui s'exécute directement sur les modèles de similarité sémantique. Nous testons deux contraintes, Universal Sentence Encoder (USE) et BertScore. Nos résultats indiquent que de tels exemples de second ordre existent, mais qu'ils sont généralement moins courants que les exemples contradictoires de premier ordre dans les modèles de pointe. Ils indiquent également que USE est efficace en tant que contrainte sur les exemples contradictoires de PNL, tandis que BertScore est presque inefficace. Le code pour exécuter les expériences décrites dans cet article est disponible ici.", 'ja': 'NLPにおける対抗例生成方法は、潜在的な対抗例が有効かどうかを決定するために、言語モデルまたは文エンコーダのようなモデルに依存する。 これらの方法では、有効な対抗例は、攻撃されるモデルを騙し、第２のモデルによって意味的または構文的に有効であると判定される。 これまでの研究では、攻撃されたモデルによる誤差などの例をすべて数えてきた。 これらの対抗例は、攻撃されたモデルの欠陥ではなく、妥当性を決定するモデルの欠陥である可能性があると主張します。 このような無効な入力を二階対抗例と呼んでいる。 制約の堅牢性を評価するためのツールとして、制約堅牢性曲線、および関連するメトリックACCを提案します。 この曲線を生成するために、セマンティック類似性モデルで直接実行する対抗攻撃を設計します。 Universal Sentence Encoder （ USE ）とBERTScoreの2つの制約をテストします。 我々の知見は、そのような二階の例が存在するが、通常、最先端のモデルにおける一階の対立例よりも一般的ではないことを示している。 これらはまた、使用がNLP対抗例の制約として有効である一方で、BERTScoreはほとんど効果がないことを示しています。 この論文の実験を実行するためのコードは、ここで入手可能です。', 'zh': 'NLP之对抗性示例生法,依于言语编码器以定潜对抗性示例之效。 凡此诸法,有效者对抗性示例欺见攻,二为语义语法。 迄今为止之论已尽此类示例皆为攻击之误。 臣愚以为此对抗性示例或不为击模之缺陷,而有效性之缺陷也。 吾等输此不效为二阶对抗性示例。 约束鲁棒性曲线关指标ACCS,为评估二阶对抗性示例约束鲁棒性具。 为此曲线者,设对抗性攻击,直行语义相似性形。 试于两束,通句编码器(USE)BERTScore。 臣等考结果表明,此二阶示例在,然先进之法,常不如一阶抗示例常见。 明USE为NLP对抗性示例约束有效,而BERTScore几无效。 此处供行实验之代码。', 'ru': 'Методы генерации примеров соперничества в NLP полагаются на такие модели, как языковые модели или кодеры предложений, чтобы определить, являются ли потенциальные примеры соперничества действительными. В этих методах действительный враждебный пример обманывает атакуемую модель и определяется как семантически или синтаксически действительная второй моделью. Исследования, проведенные до настоящего времени, подсчитали все такие примеры как ошибки атакуемой модели. Мы утверждаем, что эти враждебные примеры могут быть не недостатками атакованной модели, а недостатками модели, которая определяет действительность. Мы называем такие недействительные входы состязательными примерами второго порядка. Мы предлагаем кривую робастности ограничений и связанные с ней метрические ACC в качестве инструментов для оценки робастности ограничения к примерам соперничества второго порядка. Чтобы сгенерировать эту кривую, мы проектируем соперническую атаку для запуска непосредственно на семантических моделях сходства. Мы тестируем два ограничения: универсальный кодировщик предложений (USE) и BERTScore. Наши выводы показывают, что такие примеры второго порядка существуют, но, как правило, встречаются реже, чем состязательные примеры первого порядка в современных моделях. Они также указывают на то, что ИСПОЛЬЗОВАНИЕ эффективно в качестве ограничения для сопернических примеров NLP, в то время как оценка BERTS почти неэффективна. Код для проведения экспериментов в этой статье доступен здесь.', 'hi': 'एनएलपी में प्रतिकूल उदाहरण पीढ़ी के तरीके यह निर्धारित करने के लिए भाषा मॉडल या वाक्य एन्कोडर जैसे मॉडल पर भरोसा करते हैं कि क्या संभावित प्रतिकूल उदाहरण मान्य हैं। इन तरीकों में, एक वैध प्रतिकूल उदाहरण मॉडल पर हमला किए जाने को मूर्ख बनाता है, और दूसरे मॉडल द्वारा शब्दार्थ या वाक्यात्मक रूप से मान्य होने के लिए निर्धारित किया जाता है। आज तक के शोध ने हमले वाले मॉडल द्वारा त्रुटियों के रूप में ऐसे सभी उदाहरणों की गणना की है। हम तर्क देते हैं कि ये प्रतिकूल उदाहरण हमले वाले मॉडल में खामियां नहीं हो सकते हैं, लेकिन मॉडल में खामियां जो वैधता निर्धारित करती हैं। हम इस तरह के अमान्य इनपुट को दूसरे क्रम के प्रतिकूल उदाहरण कहते हैं। हम बाधा मजबूती वक्र, और संबंधित मीट्रिक एसीसीएस का प्रस्ताव करते हैं, दूसरे क्रम के प्रतिकूल उदाहरणों के लिए एक बाधा की मजबूती का मूल्यांकन करने के लिए उपकरण के रूप में। इस वक्र को उत्पन्न करने के लिए, हम शब्दार्थ समानता मॉडल पर सीधे चलाने के लिए एक प्रतिकूल हमले को डिजाइन करते हैं। हम दो बाधाओं पर परीक्षण करते हैं, यूनिवर्सल वाक्य एनकोडर (USE) और BERTScore। हमारे निष्कर्षों से संकेत मिलता है कि इस तरह के दूसरे क्रम के उदाहरण मौजूद हैं, लेकिन आमतौर पर अत्याधुनिक मॉडल में पहले-क्रम के प्रतिकूल उदाहरणों की तुलना में कम आम हैं। वे यह भी इंगित करते हैं कि उपयोग एनएलपी प्रतिकूल उदाहरणों पर बाधा के रूप में प्रभावी है, जबकि BERTScore लगभग अप्रभावी है। इस पेपर में प्रयोगों को चलाने के लिए कोड यहां उपलब्ध है।', 'ga': 'Bíonn modhanna ginte samplaí sáraíochta in NLP ag brath ar mhúnlaí cosúil le samhlacha teanga nó ionchódóirí abairtí chun a chinneadh an bhfuil samplaí sáraíochta féideartha bailí. Ar na modhanna seo, déanann sampla sáraíochta bailí amadán an mhúnla a bhfuil ionsaí á déanamh air, agus socraítear é a bheith bailí go séimeantach nó go síntach ag an dara samhail. Tá gach sampla dá leithéid san áireamh i dtaighde go dtí seo mar earráidí sa tsamhail faoi ionsaí. Áitímid go mb’fhéidir nach lochtanna sa mhúnla faoi ionsaí iad na samplaí sáraíochta seo, ach lochtanna sa mhúnla a chinneann bailíocht. Is samplaí sáraíochta dara hord a thugtar ar ionchuir neamhbhailí dá leithéid. Molaimid an cuar stóinseachta sriantachta, agus an ACCS méadrach gaolmhar, mar uirlisí chun stóinseacht an tsrianta ar shamplaí sáraíochta den dara hordú a mheas. Chun an cuar seo a ghiniúint, déanaimid ionsaí sáraíochta a dhearadh chun oibriú go díreach ar na samhlacha cosúlachta shéimeantacha. Déanaimid tástáil ar dhá shrian, an tIonchódóir Pianbhreithe Uilíoch (USE) agus BERTScore. Tugann ár dtorthaí le fios go bhfuil a leithéid de shamplaí dara hord ann, ach go hiondúil nach mbíonn siad chomh coitianta ná samplaí sáraíochta den chéad ordú i múnlaí den scoth. Tugann siad le fios freisin go bhfuil ÚSÁID éifeachtach mar shrian ar shamplaí sáraíochta NLP, cé go bhfuil BERTScore beagnach neamhéifeachtúil. Tá cód le haghaidh reáchtáil na dturgnaimh sa pháipéar seo ar fáil anseo.', 'el': 'Οι μέθοδοι δημιουργίας αντιπαραβαλλόμενων παραδειγμάτων στο σύστημα βασίζονται σε μοντέλα όπως μοντέλα γλώσσας ή κωδικοποιητές προτάσεων για να καθορίσουν αν τα πιθανά αντιπαραβαλλόμενα παραδείγματα είναι έγκυρα. Σε αυτές τις μεθόδους, ένα έγκυρο αντίθετο παράδειγμα ξεγελά το μοντέλο που δέχεται επίθεση και καθορίζεται ότι είναι σημασιολογικά ή συντακτικά έγκυρο από ένα δεύτερο μοντέλο. Η έρευνα μέχρι σήμερα έχει μετρήσει όλα αυτά τα παραδείγματα ως λάθη από το επιτιθέμενο μοντέλο. Υποστηρίζουμε ότι αυτά τα αντικρουόμενα παραδείγματα μπορεί να μην είναι ελαττώματα στο επιτιθέμενο μοντέλο, αλλά ελαττώματα στο μοντέλο που καθορίζει την εγκυρότητα. Τέτοιες μη έγκυρες εισροές ονομάζουμε αντικρουόμενα παραδείγματα δεύτερης τάξης. Προτείνουμε την καμπύλη ανθεκτικότητας περιορισμού και τη σχετική μετρική ως εργαλεία αξιολόγησης της ανθεκτικότητας ενός περιορισμού σε αντικρουόμενα παραδείγματα δεύτερης τάξης. Για να δημιουργήσουμε αυτή την καμπύλη, σχεδιάζουμε μια αντίπαλη επίθεση για να τρέξει απευθείας στα πρότυπα σημασιολογικής ομοιότητας. Δοκιμάζουμε σε δύο περιορισμούς, τον καθολικό κωδικοποιητή φράσεων (USE) και τον BERTScore. Τα ευρήματά μας δείχνουν ότι τέτοια παραδείγματα δεύτερης τάξης υπάρχουν, αλλά συνήθως είναι λιγότερο κοινά από τα αντικρουόμενα παραδείγματα πρώτης τάξης σε μοντέλα τελευταίας τεχνολογίας. Δείχνουν επίσης ότι η USE είναι αποτελεσματική ως περιορισμός σε αντικρουόμενα παραδείγματα NLP, ενώ το BERTScore είναι σχεδόν αναποτελεσματικό. Ο κώδικας για τη διεξαγωγή των πειραμάτων σε αυτή την εργασία είναι διαθέσιμος εδώ.', 'it': "I metodi di generazione di esempi avversi in PNL si basano su modelli come modelli linguistici o codificatori di frasi per determinare se sono validi esempi avversi potenziali. In questi metodi, un valido esempio avversario inganna il modello attaccato ed è determinato ad essere semanticamente o sintatticamente valido da un secondo modello. La ricerca ad oggi ha contato tutti gli esempi come errori del modello attaccato. Sosteniamo che questi esempi avversari potrebbero non essere difetti nel modello attaccato, ma difetti nel modello che determina la validità. Definiamo tali input non validi esempi avversari di secondo ordine. Proponiamo la curva di robustezza del vincolo, e l'ACCS metrico associato, come strumenti per valutare la robustezza di un vincolo ad esempi avversi di secondo ordine. Per generare questa curva, progettiamo un attacco avversario da eseguire direttamente sui modelli di somiglianza semantica. Testiamo su due vincoli, l'Universal Sentence Encoder (USE) e BERTScore. I nostri risultati indicano che tali esempi di secondo ordine esistono, ma in genere sono meno comuni di esempi di primo ordine avversari in modelli all'avanguardia. Essi indicano anche che USE è efficace come vincolo per gli esempi avversi di PNL, mentre BERTScore è quasi inefficace. Il codice per eseguire gli esperimenti in questo articolo è disponibile qui.", 'ka': 'Name ამ პროცემებში, საუკეთესო ნოტერატიური მაგალითი მაგალითი მოდელს ატარებულია და განსაზღვრებულია, რომ სენმატიურად ან სინტაქტიურად მუშაობა მეორე მოდელზე. ახლა განსხვავება ყველა მაგალითი მაგალითი, როგორც შეცდომა მოდელზე. ჩვენ ვთქვათ, რომ ეს განსაკუთრებული მაგალითი მაგალითები შეიძლება არაფერი იყოს მოდელში, მაგრამ არაფერი მოდელში, რომელიც განსაკუთრებს მნიშვნელობა. ჩვენ ასეთი უცნობიერი მონაცემების მეორე წერტილის განსაზღვრებული მაგალითების გამოყენება. ჩვენ მინდომათ შევცვალობა ძალიან ძალიან ძალიან ძალიან ძალიან ძალიან, და დაკავშირებული მეტრიკური ACCS-ს, როგორც ხელსაწყობილობა, როგორც შევცვალობა ძალიან ძალიან ძალია ამ კრიგურის შექმნა, ჩვენ განაზღვრებით განაზღვრებით განაზღვრებით სემონტიკური განსხვავება მოდელზე. ჩვენ შევცვალობთ ორი ზომის, სამყარო სიტყვების კოდერი (USE) და BERTScore. ჩვენი შესაძლებლობები გვეჩვენება, რომ ასეთი მეორე წერტილის მაგალითები არსებობს, მაგრამ ტიპოლურად არსებობს პირველი წერტილის განსაკუთრებული მაგალითებისგან. ისინი ამოცანებენ, რომ სამყარო გამოყენება ეფექტიურია, როგორც NLP-ის განსაკუთრებული მაგალითად, მაგრამ BERTScore უფრო არაფექტიურია. კოდი ექსპერიმენტების გამოყენებისთვის ამ დომენტის შესაძლებელია.', 'hu': 'Az ellentétes példagenerálási módszerek az NLP-ben olyan modellekre támaszkodnak, mint a nyelvi modellek vagy mondatkódolók, amelyek meghatározzák, hogy a potenciális ellentétes példák érvényesek-e. Ezekben a módszerekben egy érvényes ellentétes példa megtéveszti a támadott modellt, és úgy határozza meg, hogy egy második modell szemantikailag vagy szintaktikailag érvényes. Az eddigi kutatások minden olyan példát számoltak, mint a támadott modell hibái. Azt állítjuk, hogy ezek az ellentétes példák nem a megtámadott modell hibái, hanem az érvényességet meghatározó modell hibái. Az ilyen érvénytelen bemeneteket másodrendű ellentétes példáknak nevezzük. Javasoljuk a korlátozás robusztussági görbéjét és a hozzá tartozó metrikus ACCS-t, mint eszközöket a másodrendű ellentétes példákhoz viszonyított korlátozás robusztusságának értékelésére. Ennek a görbe létrehozásához ellentétes támadást tervezünk, amely közvetlenül a szemantikai hasonlósági modelleken fut. Két korláton teszteljük, az Universal Sentence Encoder (USE) és a BERTScore-on. Eredményeink azt mutatják, hogy ilyen másodrendű példák léteznek, de jellemzően kevésbé gyakoriak, mint az elsőrendű ellentétes példák a korszerű modellekben. Ezek azt is jelzik, hogy a HASZNÁLAT hatékony az NLP ellentétes példáira, míg a BERTScore szinte hatástalan. A kísérletek lefuttatásának kódja ebben a cikkben itt található.', 'kk': 'NLP- дегі конверсариялық мысалдарды құру әдістері тіл үлгілеріне не сөз кодерлеріне сәйкес келетін үлгілерде тәуелді, негатриялық мысалдар дұрыс екенін анықтау үшін. Бұл әдістерде дұрыс негатриялық мысал үлгісін қалдырып, екінші үлгісімен semantiкалық немесе синтактикалық синтактикалық деп анықталады. Қазіргі зерттеу үлгісінің барлық мысалдарды қателері ретінде есептеп отырды. Біз бұл қарсы мысалдар қалдырылған үлгінде қатты емес болуы мүмкін, бірақ дұрыстығын анықтайтын үлгінің қатесі болуы мүмкін. Біз дұрыс емес енгізіміз екінші реттік қарсы мысалдар деп аталамыз. Біз шектеулердің шектеулердің шектеулердің шектеулердің шектеулерін және қатынастырылған метрикалық ACCS дегенді, екінші ретінде қарсы мысалдарды бағалау құралы ретінде ұсынамыз. Бұл сызықты құру үшін біз симантикалық ұқсас үлгілеріне тұратын қарсы атқаруды құрамыз. Біз екі шектеулерді, Universal sentence Encoder (USE) және BERTScore тексереміз. Біздің іздегеніміз бұл екінші реттегі мысалдар бар деп белгіледі, бірақ әдетте бірінші реттегі қарсылық мысалдарынан төмен болады. Олар сонымен қатар, NLP негізгі мысалдар үшін пайдаланушының эффективнігін көрсетеді, бірақ BERTScore жақын әсер етпейді. Бұл қағаздағы тәжірибелерді орындау коды осында бар.', 'mk': 'Противните методи на генерација на примери во NLP се потпираат на модели како јазички модели или кодери на реченици за да се одреди дали потенцијалните противници се валидни. Во овие методи, валиден противник пример го лаже моделот кој е нападнат, и е решен да биде семантично или синтактички валиден од друг модел. Досега истражувањето ги брои сите примери како грешки од нападнатиот модел. Ние тврдиме дека овие противници можеби не се недостатоци во нападнатиот модел, туку недостатоци во моделот кој ја одредува валидноста. Ние ги нарекуваме вакви невалидни внесувања втор ред противници примери. Ние ја предложуваме кривата на силноста на ограничувањата и поврзаната метрична АКС, како алатки за проценка на силноста на ограничувањата на вториот ред противниците. За да ја генерираме оваа крива, дизајнираме непријателски напад за да трчаме директно на семантичните модели на сличност. Ние тестираме на два ограничувања, универзалниот кодер на реченици (USE) и BERTScore. Нашите откритија покажуваат дека вакви примери од вториот ред постојат, но обично се помалку чести од примерите од првиот ред во најновите модели. Тие, исто така, покажуваат дека УСЕ е ефикасна како ограничување на противниците на НЛП, додека БЕРТСкоре е речиси неефикасна. Кодот за спроведување на експериментите во овој весник е достапен тука.', 'ms': 'Kaedah generasi contoh lawan dalam NLP bergantung pada model seperti model bahasa atau pengekod kalimat untuk menentukan sama ada contoh lawan berpotensi sah. Dalam kaedah ini, contoh musuh yang sah menipu model yang diserang, dan ditentukan untuk sah secara semantik atau sintaktik oleh model kedua. kajian hingga kini telah menghitung semua contoh seperti ralat oleh model yang diserang. Kami menegaskan bahawa contoh musuh ini mungkin bukan kesalahan dalam model yang diserang, tetapi kesalahan dalam model yang menentukan kehendak. Kami menamakan input tidak sah contoh lawan tertib kedua. Kami cadangkan lengkung ketakutan batasan, dan ACCS metrik yang berkaitan, sebagai alat untuk menilai ketakutan batasan kepada contoh lawan tertib kedua. Untuk menghasilkan lengkung ini, kita merancang serangan musuh untuk menjalankan secara langsung pada model persamaan semantik. Kami menguji pada dua keterangan, Pengekod Hukuman Universal (USE) dan BERTScore. Kesemuan kami menunjukkan bahawa contoh-contoh tertib kedua tersebut wujud, tetapi biasanya kurang biasa daripada contoh-contoh lawan tertib pertama dalam model-state-of-the-art. Mereka juga menunjukkan bahawa USE berkesan sebagai penghalang pada contoh lawan NLP, sementara BERTScore hampir tidak berkesan. Kod untuk menjalankan eksperimen dalam kertas ini tersedia di sini.', 'lt': 'Nepageidaujamų pavyzdžių generavimo metodai NLP grindžiami pavyzdžiais, pavyzdžiui, kalbos modeliais arba sakinių kodais, siekiant nustatyti, ar galimi priešingi pavyzdžiai yra tinkami. Taikant šiuos metodus pagrįstas priešingas pavyzdys suklaidina užpuolamą model į ir nustatoma, kad jis semantiškai arba sintaktiškai galioja antruoju modeliu. Iki šiol atliekant mokslinius tyrimus buvo atsižvelgta į visus tokius pavyzdžius kaip atakuoto modelio klaidos. Mes tvirtiname, kad šie prieštaringi pavyzdžiai gali būti ne užpuolimo modelio trūkumai, bet modelio, kuris nustato galiojimą, trūkumai. Mes apibrėžiame tokius netinkamus įvestus antrosios eilės priešingus pavyzdžius. Siūlome suvaržymų patikimumo kreivę ir susijusią metrinę ACCS kaip priemonę įvertinti suvaržymų patikimumą antrosios eilės priešiniams pavyzdžiams. Norėdami sukurti šią kreivę, mes sukonstruojame priešingą ataką, kad važiuotume tiesiogiai ant semantinio panašumo modelių. Bandome su dviem apribojimais: Universal Sentence Encoder (USE) ir BERTScore. Mūsų išvados rodo, kad tokie antrosios eilės pavyzdžiai egzistuoja, tačiau paprastai yra mažiau dažni nei pirmosios eilės priešingi pavyzdžiai moderniausiuose modeliuose. Jos taip pat rodo, kad naudojimas yra veiksmingas kaip prieštaringų NLP pavyzdžių apribojimas, o BERTScore beveik neveiksmingas. Šiame dokumente pateikiamas eksperimentų vykdymo kodas.', 'mt': 'Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid.  F’dawn il-metodi, eżempju avversarju validu jfixkel il-mudell li qed jiġi attakkat, u huwa determinat li huwa semantikament jew sintetikament validu mit-tieni mudell. Sal-lum ir-riċerka għenet l-eżempji kollha bħal żbalji mill-mudell attakkat. Aħna nindirizzaw li dawn l-eżempji avversarji jistgħu ma jkunux nuqqasijiet fil-mudell attakkat, iżda nuqqasijiet fil-mudell li jiddetermina l-validità. Aħna ntemmu tali inputs invalidi eżempji avversarji tat-tieni ordni. Aħna nipproponu l-kurva tar-robustezza tar-restrizzjoni, u l-ACCS metriku assoċjat, bħala għodod għall-evalwazzjoni tar-robustezza ta’ restrizzjoni għal eżempji avversarji tat-tieni ordni. Biex niġġeneraw din il-kurva, niddisinjaw attakk avversarju biex nimxu direttament fuq il-mudelli semantiċi ta’ similarità. Aħna nistestjaw fuq żewġ restrizzjonijiet, l-Universal Sentence Encoder (USE) u l-BERTScore. Is-sejbiet tagħna jindikaw li jeżistu eżempji tat-tieni ordni bħal dawn, iżda huma tipikament inqas komuni minn eżempji avversarji tal-ewwel ordni f’mudelli l-aktar avvanzati. Huma jindikaw ukoll li l-użu huwa effettiv bħala restrizzjoni fuq eżempji avversarji NLP, filwaqt li l-BERTScore huwa kważi ineffettiv. Il-kodiċi għat-tmexxija tal-esperimenti f’din id-dokument huwa disponibbli hawnhekk.', 'ml': 'NLP-ലുള്ള മാതൃകയുടെ മാറ്റങ്ങള്\u200d ഭാഷ മോഡലോ വാക്കിന്റെ എക്കോഡുകളോ പോലുള്ള മോഡലുകളില്\u200d ആശ്രയിക്കുന്നു. സാധ്യതക്ക വ ഈ രീതികളില്\u200d, ഒരു വിരോധമായ വിരോധമായ ഉദാഹരണത്തിന് മോഡല്\u200d ആക്രമിക്കപ്പെടുന്നതിനെ വിഡ്ഢികളാക്കുകയും, ഒരു രണ്ടാമത്തെ മോഡലിന്\u200dറെ മൂ തീയതിയ്ക്കുള്ള പരിശോധന എല്ലാ ഉദാഹരണങ്ങളും ആക്രമിക്കപ്പെട്ട മോഡലിന്റെ പിശകുകള്\u200d പോലെ എണ്ണിയിരിക ഈ വിരോധമായ ഉദാഹരണങ്ങള്\u200d ആക്രമിക്കപ്പെട്ട മോഡലില്\u200d തെറ്റുകള്\u200d അല്ലെന്ന് ഞങ്ങള്\u200d വാദിക്കുന്നു, പക്ഷെ സത്യത്തെ നിര്\u200dണയിക് ഇത്തരം അസാധുവായ ഇന്\u200dപുട്ടുകള്\u200d രണ്ടാമത്തെ ക്രമീകരണങ്ങള്\u200dക്ക് വിരോധമായ ഉദാഹരണങ്ങളാണെന്ന് ഞങ്ങള്\u200d  നമ്മള്\u200d നിര്\u200dബന്ധിതമായ റോബ്സ്റ്റ് കോര്\u200dവിനെയും ബന്ധപ്പെട്ട മെട്രിക്കസിസിനെയും പരിഗണിക്കാനുള്ള ഉപകരണങ്ങളായി പ്രായശ്ചിത്തം ച ഈ കയറിനെ സൃഷ്ടിക്കാന്\u200d, നമ്മള്\u200d ഒരു എതിരാളിയുടെ ആക്രമണം നേരിട്ട് പ്രവര്\u200dത്തിപ്പിക്കാന്\u200d സെമാന്റിക് സമമാതൃ നമ്മള്\u200d രണ്ട് നിയന്ത്രണങ്ങളില്\u200d പരീക്ഷിക്കുന്നു, യൂണിറല്\u200d വിധിക്കുന്നതിന്റെ കോഡിനും ബെര്\u200dട്ടിസ്കോരും. നമ്മുടെ കണ്ടുപിടികള്\u200d ഇത്തരം രണ്ടാമത്തെ കല്\u200dപനയുടെ ഉദാഹരണങ്ങള്\u200d നിലവിലുണ്ടെന്ന് തെളിയിക്കുന്നു. പക്ഷെ സാധാരണയായി ആദ്യത്തെ കല നിങ്ങളുടെ നിര്\u200dബന്ധമായ ഉദാഹരണങ്ങള്\u200dക്ക് നിര്\u200dബന്ധമായി യൂഎസ് പ്രാവര്\u200dത്തികമാണെന്നും അവര്\u200d പറയുന്നു, ബെര്\u200dട്ടിസ്കോര്\u200d കുറച Code for running the experiments in this paper is available here.', 'mn': 'NLP-ийн эсрэг жишээ нь хэл загвар, өгүүлбэрийн коддогчид зөв эсрэг жишээг тодорхойлох боломжтой эсрэг жишээг тодорхойлох үед загваруудыг ашигладаг. Эдгээр аргаар зөвхөн эсрэг жишээ нь загварыг дайрагдаж, хоёр дахь загвараар семантик эсвэл синтактик зөвхөн зөвхөн зөвхөн зөвшөөрөгдөж байна. Өнөөдрийн судалгаагаар дайрагдсан загварын алдаа гэдгийг бүх жишээг тооцогдсон. Бид эдгээр эсрэг жишээ нь халдвар авсан загварын алдаа биш байж магадгүй, гэхдээ үнэ цэнэтэй тодорхойлж буй загварын алдаа байж магадгүй. Бид хоёр дахь дараагийн эсрэг жишээ гэж нэрлэдэг. Бид хоёр дахь дараагийн эсрэг жишээнд хязгаарлагдмал байдлыг дүгнэх хэрэгсэл болгоно. Энэ муруй бий болгохын тулд бид эсрэг атлаа зохион байгуулахын тулд семантик төстэй загвар дээр шууд гүйцэтгэдэг. Бид хоёр хязгаар, Universal Sentence Encoder (USE) болон BERTScore-д туршиж байна. Бидний ололтууд нь хоёр дахь дараагийн жишээ байдаг гэхдээ ерөнхийдөө анхны дараагийн эсрэг жишээнээс бага байдаг. Тэд мөн USE нь NLP эсрэг жишээ дээр хязгаарлагддаг гэдгийг харуулж байна. BERTScore бараг үр дүнгүй. Энэ цаасан дээрх туршилтуудыг ажиллах код энд байдаг.', 'ro': 'Metodele de generare a exemplelor adverse în PNL se bazează pe modele cum ar fi modele lingvistice sau codificatoare de propoziții pentru a determina dacă potențiale exemple adverse sunt valide. În aceste metode, un exemplu adversar valid păcălește modelul atacat și este determinat să fie valid semantic sau sintactic printr-un al doilea model. Cercetările de până în prezent au numărat toate exemplele precum erorile modelului atacat. Susținem că aceste exemple adversare nu pot fi defecte în modelul atacat, ci defecte în modelul care determină validitatea. Noi denumim astfel de intrări invalide exemple adversare de ordinul doi. Propunem curba de robustețe a constrângerii și ACCS metric asociat, ca instrumente pentru evaluarea robusteții unei constrângeri la exemple adversare de ordinul al doilea. Pentru a genera această curbă, proiectăm un atac adversar care să ruleze direct pe modelele de similitudine semantică. Testăm pe două constrângeri, Encoder Universal Sentence (USE) și BERTScore. Rezultatele noastre indică faptul că astfel de exemple de ordinul doi există, dar sunt de obicei mai puțin frecvente decât exemplele adversare de ordinul întâi în modele de ultimă generație. Acestea indică, de asemenea, că USE este eficientă ca constrângere a exemplelor adversare ale PNL, în timp ce BERTScore este aproape ineficient. Codul pentru efectuarea experimentelor din această lucrare este disponibil aici.', 'no': 'Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine whether potential adversarial examples are valid. I desse metodane er eit gyldig adversarisk eksempel forfeil modellen som vert brukt, og er avgjort å vera semantisk eller syntaksisk gyldig av ein andre modell. Forskning til dato har tella alle slike eksemplar som feil i den attackerte modellen. Vi tvar at desse adversariale eksemplene kan kanskje ikkje vera feil i den attackerte modellen, men feil i modellen som bestemmer gyldighet. Vi uttrykker slike ugyldige inndata for andrerekkefølgjande adversariske eksemplar. Vi foreslår begrensningskurva for kraftighet, og tilknytte metriske ACCS, som verktøy for å evaluera kraftighet for ein begrensning til andre rekkefølgje adversariske eksemplar. For å laga denne kurva, designerer vi ein adversarial attack for å køyra direkte på semantiske similaritetsmodellen. Vi testar på to begrensningar, Universell teiknkoder (USE) og BERTScore. Finningane våre tyder på at slike eksemplar i andre rekkefølgje finst, men er vanlegvis mindre vanlege enn første rekkefølgje adversariske eksemplar i kunstmodeller. Dei viser også at bruk er effektivt som begrensning på NLP-adversarial eksemplar, mens BERTScore er nesten ineffektiv. Kode for køyring av eksperimentane i denne papira er tilgjengeleg her.', 'pl': 'Metody generowania przykładów przeciwnych w NLP opierają się na modelach takich jak modele językowe lub kodery zdań, aby określić, czy potencjalne przykłady przeciwne są poprawne. W tych metodach prawidłowy przykład przeciwny oszukuje atakowany model i jest określony jako ważny semantycznie lub składniowo przez drugi model. Dotychczasowe badania liczyły wszystkie takie przykłady jak błędy atakowanego modelu. Twierdzimy, że te przeciwne przykłady mogą nie być wadami atakowanego modelu, lecz wadami modelu, które decydują o wadzności. Nazywamy takie nieprawidłowe wejścia drugiego rzędu przykłady przeciwne. Proponujemy krzywą solidności ograniczeń i powiązaną z nią metrykę ACCS jako narzędzia do oceny solidności ograniczenia do przykładów przeciwnych drugiego rzędu. Aby wygenerować tę krzywą, projektujemy atak przeciwny, który uruchamia się bezpośrednio na semantycznych modelach podobieństwa. Testujemy na dwóch ograniczeniach, Universal Sentence Encoder (USE) i BERTScore. Nasze ustalenia wskazują, że takie przykłady drugiego rzędu istnieją, ale zazwyczaj są rzadziej powszechne niż przykłady przeciwne pierwszego rzędu w najnowocześniejszych modelach. Wskazują one również, że USE jest skuteczne jako ograniczenie dla przykładów przeciwników NLP, podczas gdy BERTScore jest prawie nieskuteczny. Kod prowadzenia eksperymentów w niniejszym artykule jest dostępny tutaj.', 'sr': 'Preporučne metode generacije primjera u NLP oslanjaju se na modele poput jezičkih modela ili kodera rečenica kako bi utvrdili da li su potencijalni negativni primjeri validni. U ovim metodama, dobar neprijateljski primer prevari model koji se napada i odlučuje da je semantički ili sintaktički validan drugim modelom. Istraživanje do sada računalo je sve takve primjere kao greške napadnutim modelom. Svjedočili smo se da ovi neprijateljski primjeri možda nisu nedostatke u napadnutom modelu, već nedostatke u modelu koji određuje validnost. Mi izrazujemo takve nepravilne ulaze protivnih primjera drugog reda. Predlažemo ograničenu krivu snage i povezanu metričku ACCS-u kao alat za procjenu snage ograničenja prema drugom naređenju neprijateljskim primjerima. Da bi stvorili ovu krivu, dizajnirali smo neprijateljski napad da bi se direktno pokrenuli na semantičke modele sličnosti. Testiramo na dva ograničenja, Univerzalni koder kazne (USE) i BERTScore. Naši nalazi ukazuju na to da postoje primjeri drugog reda, ali obično su manje česti od prvog reda neprijateljskih primjera u modelima stanja umjetnosti. Oni takođe ukazuju da je korisnik korisnika efikasan kao ograničenje na neprijateljske primjere NLP-a, dok je BERTScore skoro neefektivan. Kod za pokretanje eksperimenata u ovom papiru je dostupan ovde.', 'so': 'Tusaaladaha dhalashada tusaale ahaan ee NLP waxay ku xiran yihiin tusaale ahaan noocyada luuqada ama qoraalka ereyga si ay u ogaadaan in tusaalooyin cadaawayaasha ah ay shaqeeyaan. Tusaale saameyn ah oo cadow ka shaqeynaya ayaa foolaya modelka lagu weerarayo, waxaana lagu qasdaa in uu u shaqeeyo mid labaad. Baaritaanka ilaa waqtigaas waxay xisaabtay tusaale ahaan qalad ku dhacday tusaale ahaan tusaale ahaan. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity.  Waxan ku qornaa tusaale cadaawayaal labaad oo ka mid ah Waxaynu soo jeedaynaa qalabka dharka xadda ah, waxaana la xidhiidhi karnaa ACCS, sida qalabka qiimeynaya dharka xada ee masaallta cadaawayaasha labaad. Si aan u sameyno qalabkan, waxaynu u sameynaa weerar cadaawayaasha ah si toos u socota modelalka siman. Waxaan imtixaamaynaa labo xad ah, Qoidaha xukunka caalamiga ah (USE) iyo BERTScore. Shaqooyinkayada waxaa loola jeedaa in tusaalooyin noocyada labaad ay jiraan, laakiin sida caadiga ah waa mid ka yar tusaalooyin ka gees ah oo ku saabsan qoraalka farshaxanka. Waxay sidoo kale muujin karaan in USE uu u shaqeeyo sida qasab u saaran tusaalayaal ka gees ah ee NLP, xitaa BERTScore waxay u dhow tahay mid aan saamayn. Codeynta baaritaanka warqaddan waxaa laga helaa halkan.', 'sv': 'Metoder för generering av negativa exempel i NLP bygger på modeller som språkmodeller eller meningskoder för att avgöra om potentiella negativa exempel är giltiga. I dessa metoder lurar ett giltigt kontradiktoriskt exempel modellen som attackeras och bestäms vara semantiskt eller syntaktiskt giltig av en andra modell. Forskning hittills har räknat alla sådana exempel som fel av den angripna modellen. Vi hävdar att dessa motstridiga exempel kanske inte är brister i den angripna modellen, utan brister i den modell som bestämmer giltigheten. Vi kallar sådana ogiltiga inmatningar andra ordningens motstridiga exempel. Vi föreslår begränsningsrobusthetskurvan, och tillhörande metrisk ACCS, som verktyg för att utvärdera robustheten hos en begränsning till andra ordningens motstridiga exempel. För att generera denna kurva utformar vi en motpartsattack som körs direkt på semantiska likhetsmodeller. Vi testar på två begränsningar, Universal Sentence Encoder (USE) och BERTScore. Våra resultat tyder på att sådana andra ordningens exempel finns, men är vanligtvis mindre vanliga än första ordningens motstridiga exempel i state-of-the-art modeller. De tyder också på att ANVÄNDNING är effektivt som begränsning för NLP motstridiga exempel, medan BERTScore nästan är ineffektivt. Koden för att köra experimenten i denna uppsats finns tillgänglig här.', 'ta': 'NLP இல் உள்ள மாதிரியான உதாரணங்கள் உருவாக்க முறைமைகள் மொழி மாதிரிகள் அல்லது வாக்கு குறியீடுகள் சாத்தியமான எதிரியான உதா இந்த முறைகளில், ஒரு செல்லுபடியான எதிர்பார்வை எடுத்துகாட்டு மாதிரி தாக்கப்படுகிறது, மற்றும் ஒரு இரண்டாவது மாதிரி செல்லுபடியாக இர தேதிக்கு வரை ஆராய்ச்சி அனைத்து உதாரணங்களையும் தாக்கப்பட்ட மாதிரியால் பிழைகள் என்று எண்ணியி இந்த எதிர்பார்த்த உதாரணங்கள் தாக்கப்பட்ட மாதிரியில் தவறாக இருக்கலாம், ஆனால் செல்லும் மாதிரியில் தவறு நாம் இவ்வாறு செல்லாத உள்ளீடுகளை சொல்கிறோம் இரண்டாவது வரிசையில் எதிர்பார்த்த உதாரணங்களை எட நாம் கட்டுப்படுத்தப்பட்ட மெட்ரிக் ACCS மற்றும் தொடர்புடைய மெட்ரிக் கருவிகளை பரிந்துரைக்கிறோம், இரண்டாவது வரிசையில் இருந்து எதிரிய இந்த வளைவை உருவாக்க, நாம் ஒரு எதிர்மறை தோற்றம் வடிவமைக்க, பெம்மான்டிக் ஒத்த மாதிரிகளில் நேரடியாக இயக்க. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore.  எங்கள் கண்டுபிடிப்புகள் இந்த இரண்டாவது வரிசையில் உதாரணங்கள் இருக்கிறது என்பதை குறிப்பிடுகிறது, ஆனால் வழக்கமாக முதல் வரிசையில்  பெர்ட்ஸ்கோர் கிட்டத்தட்ட உதாரணங்களுக்கு அமைப்பு என்று யூஎஸ் பயனுள்ளது என்பதை குறிப்பிடுகிறது. இந்த தாளில் சோதனைகளை இயக்குவதற்கான குறியீடு இங்கே கிடைக்கும்.', 'si': 'Name මේ විදියට, විශේෂ විරෝධ උදාහරණයක් පහර දෙන්න ප්\u200dරමාණයට පහර දෙන්න පුළුවන් විදියට පහර දෙන්න ප්\u200dරමාණයක් නි දවසට පරීක්ෂණාව හැම උදාහරණයක්ම ගිණිල්ලා තියෙන්නේ පහර දුන්න ප්\u200dරමාණයෙන් වැරදි වගේ. අපි ප්\u200dරශ්නය කරනවා මේ විරෝධික උදාහරණ වලින් පහර දුන්න පුළුවන් නැහැ, ඒත් විරෝධික විශ්වාස කරපු මොඩේ අපි අවශ්\u200dයයෙන් අවශ්\u200dයයි, දෙවෙනි විරෝධ විරෝධ උදාහරණ වලින්. අපි ප්\u200dරශ්නය කරන්නේ සීමාවක් ශක්තිමත් වර්ගය, සම්බන්ධ මෙට්\u200dරික් ACCS, දෙවෙනි විරෝධ විරෝධ වර්ගයෙන් සීමාවක් විශ්වාස මේ කුරුවක් නිර්මාණය කරන්න, අපි විරෝධ වෙනුවෙන් විරෝධ වෙනුවෙන් ප්\u200dරතිකාරයක් සිමාන්තික වගේ මොඩේල අපි පරීක්ෂා කරන්නේ අවධානය දෙකක්, ජාතික වාර්තාවක් සංකේතකය (USE) සහ BERTScore. අපේ හොයාගන්න පුළුවන් කියනවා ඒ වගේම දෙවෙනි ක්\u200dරමණ උදාහරණ් තියෙනවා නමුත් සාමාන්\u200dයයෙන් පළමු ක්\u200dරමණික විරෝධික උ ඔවුන් කියන්නේ USE එක්ක NLP විරෝද්ධ උදාහරණ වලින් ප්\u200dරශ්නයක් තියෙනවා කියලා, BERTScore තවමත් නිර්භාවිතයි. මේ පරීක්ෂණයේ පරීක්ෂණය කරන්න කෝඩ් තියෙනවා මෙතන.', 'ur': 'Name ان طریقوں میں، ایک قابل مقابلہ مثال موڈل کو حملہ کیا جاتا ہے، اور ایک دوسرے موڈل کے ذریعہ سیمنٹی یا سینٹاکیٹی طور پر قابل ہو جانے کا فیصلہ کیا جاتا ہے. تازگی تک تحقیق نے تمام مثالیں گنی ہیں جیسے حملہ کی موڈل کے ذریعہ غلطی. ہم نے جھگڑا تھا کہ یہ مخالف مثالیں حملہ کی مدل میں نقصان نہیں ہوسکتے بلکہ مدل میں نقصان ہوسکتے ہیں جو مطابق مقرر کرتا ہے ہم ایسی غلط مثالیں کہتے ہیں کہ دوسری طرح مخالف مثالیں ہیں ہم نے محدودیت کی قوت کی کور اور متریک ACCS کے ساتھ مشترک مثالیں پیشنهاد کرتی ہیں، دوسری اورداری مثالوں کے مطابق محدودیت کی قوت کا ارزش کرنے کے لئے ابزار بناتے ہیں. ہم نے ایک مخالف حملہ طراحی کی کہ سیمنٹی شباهت موڈل پر مستقیم چل سکیں۔ ہم دو محدودیت پر آزماتے ہیں، Universal Sentence Encoder (USE) اور BERTScore. ہمارے نتیجے نشان دیتے ہیں کہ یہ دوسری نمونہ مثالیں موجود ہیں، لیکن معمولاً پہلی نمونہ کے مخالف نمونوں سے کم مشترک ہیں. یہ بھی نشان دیتے ہیں کہ استعمال NLP مقابلہ مثالوں پر محدود ہے، حالانکہ BERTScore تقریباً غیر اثبات ہے. اس کاغذ میں آزمائش چلنے کے لئے کوڈ یہاں موجود ہے.', 'uz': "Name Ushbu usullarda, haqiqiqiy taʼminlovchi misol modelni kichiklashni o'ylaydi, va ikkinchi model bilan semantik yoki syntactikk boʻyicha to ʻgʻri bo'lishi mumkin. Tanlangan taʼminlovchi hamma misollarni tahrirlash Biz murojaat qilamiz, bu ishlatilgan misollar haqiqiqiy aniqlanish modelidagi notoʻgʻri emas. Biz shunday notoʻgʻri narsa ikkinchi tarkibi misollariga aytishimiz mumkin. Biz cheksiz o'zgaruvchining chegarasi, metrik ACCS bilan bog'liq narsa va ikkinchi tarkibi misollarni qiymatlashning chegarasining asboblari kabi. Bu kursorni yaratish uchun, biz semantik bir xil modellarda ishlab chiqarish uchun ishlab chiqaramiz. Biz ikkita tartibi, Universal maxsus kodlash (USE) va BERTScore bilan imtixaamiz. Bizning natijalarimiz shu ikkinchi tartibi misollarida mavjud, lekin odatda birinchi tartib modelidagi birinchi tartibi misollaridan juda ko'p. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual.  Bu qogʻozdagi jarayonlarni ishga tushirish uchun kodi bu yerda mavjud.", 'vi': 'Cách chế tạo gương mẫu trên ngôn ngữ Chọc thuộc vào mẫu như mô hình ngôn ngữ hoặc mã hóa câu để xác định xem trường hợp nào là hợp cả. Trong những phương pháp này, một ví dụ đối nghịch hợp lệ ngu ngốc với người mẫu bị tấn công, và được quyết định theo ngữ pháp hay chỉ ra duy trì bằng một mẫu thứ hai. Nghiên cứu đến nay đã tính đến tất cả các ví dụ như lỗi của mô hình tấn công. Chúng tôi tranh luận rằng những ví dụ này không phải là khuyết điểm trong mô hình bị tấn công, mà là khuyết điểm trong mô hình xác định giá trị. Chúng tôi gọi những nội dung vô hợp pháp là ví dụ. Chúng tôi đề xuất độ bền vững của giới hạn, và kết hợp mét ACCS, như một công cụ để đánh giá độ bền vững của một giới hạn dẫn đến trường hợp ngược lại. Để tạo ra đường cong này, chúng tôi thiết kế một cuộc tấn công đối nghịch để chạy trực tiếp trên mô hình nét giống nhau. Chúng ta kiểm tra về hai vấn đề với nó nặng chung, the Universal Sensence Encoder (USE) và BERTScore. Những phát hiện của chúng tôi cho thấy có những ví dụ thứ hai tồn tại, nhưng thường ít phổ biến hơn những trường hợp đối đầu trong các mô hình hiện đại. Chúng cũng cho thấy việc sử dụng có hiệu quả như là hạn chế đối nghịch trong đài NLP, trong khi sót lại gần như vô hiệu quả. Ở đây có mã để tiến hành thí nghiệm trên tờ giấy này.', 'bg': 'Методите за генериране на рекламни примери в НЛП разчитат на модели като езикови модели или кодери на изречения, за да определят дали потенциалните примери са валидни. В тези методи валиден конкуренционен пример заблуждава модела, който е атакуван, и се определя като семантично или синтактично валиден от втори модел. Изследванията досега са преброили всички такива примери като грешки от атакувания модел. Ние твърдим, че тези противоречиви примери може да не са недостатъци в атакувания модел, а недостатъци в модела, който определя валидността. Ние наричаме такива невалидни входове втори ред контразартни примери. Предлагаме кривата на устойчивост на ограничението и свързаната с него метрична ACCS, като инструменти за оценка на устойчивостта на ограничението към примери от втори ред. За да генерираме тази крива, проектираме контразартна атака, която да се изпълнява директно върху семантичните модели на сходство. Тестваме на две ограничения, универсалния кодер на изреченията (УЕЗ) и BERTScore. Нашите констатации показват, че такива примери от втори ред съществуват, но обикновено са по-рядко срещани от примерите от първи ред в съвременните модели. Те също така посочват, че УПО е ефективна като ограничаване на примерите за НЛП, докато BERTScore е почти неефективна. Кодът за провеждане на експериментите в тази статия е достъпен тук.', 'da': 'Modstridende eksempelgenereringsmetoder i NLP er baseret på modeller som sprogmodeller eller sætningskodere til at afgøre, om potentielle modstridende eksempler er gyldige. I disse metoder narrer et gyldigt modstridende eksempel den model, der angribes, og er bestemt til at være semantisk eller syntaktisk gyldig ved en anden model. Forskning til dato har tænkt alle eksempler som fejl fra den angrebne model. Vi hævder, at disse modstridende eksempler måske ikke er fejl i den angrebne model, men fejl i den model, der bestemmer gyldigheden. Vi betegner sådanne ugyldige input anden ordens modstridende eksempler. Vi foreslår begrænsningsrobusthedskurven og tilhørende metrisk ACCS som værktøjer til at vurdere robustheden af en begrænsning til anden ordens modstridende eksempler. For at generere denne kurve, designer vi et modstridende angreb til at køre direkte på de semantiske lighedsmodeller. Vi tester på to begrænsninger, Universal Sentence Encoder (USE) og BERTScore. Vores resultater indikerer, at sådanne anden ordens eksempler findes, men er typisk mindre almindelige end første ordens modstridende eksempler i state-of-the-art modeller. De indikerer også, at ANVENDELSE er effektiv som begrænsning for NLP modstridende eksempler, mens BERTScore næsten er ineffektiv. Koden til at køre eksperimenterne i denne artikel er tilgængelig her.', 'nl': 'De methoden voor het genereren van tegenstrijdige voorbeelden in NLP zijn gebaseerd op modellen zoals taalmodellen of zinsencoders om te bepalen of potentiële tegenstrijdige voorbeelden geldig zijn. In deze methoden houdt een geldig tegenstrijdig voorbeeld het model voor de gek dat aangevallen wordt, en wordt bepaald dat het semantisch of syntactisch geldig is door een tweede model. Onderzoek tot nu toe heeft al dergelijke voorbeelden geteld als fouten door het aangevallen model. We stellen dat deze tegenstrijdige voorbeelden misschien geen gebreken zijn in het aangevallen model, maar gebreken in het model dat de geldigheid bepaalt. We noemen dergelijke ongeldige inputs tweederangs tegenstrijdige voorbeelden. We stellen de constraint robustness curve en de bijbehorende metrische ACCS voor als hulpmiddelen voor het evalueren van de robuustheid van een constraint aan tweederangs tegengestelde voorbeelden. Om deze curve te genereren ontwerpen we een tegenstrijdige aanval die direct op de semantische vergelijkingsmodellen wordt uitgevoerd. We testen op twee beperkingen, de Universal Sentence Encoder (USE) en BERTScore. Onze bevindingen wijzen erop dat dergelijke tweede orde voorbeelden bestaan, maar meestal minder vaak voorkomen dan eerste orde tegenstrijdige voorbeelden in state-of-the-art modellen. Ze geven ook aan dat USE effectief is als beperking op NLP tegenstrijdige voorbeelden, terwijl BERTScore bijna ineffectief is. Code voor het uitvoeren van de experimenten in dit artikel is hier beschikbaar.', 'hr': 'Navodne metode generacije primjera u NLP oslanjaju se na modele poput jezičkih modela ili kodera rečenica kako bi utvrdili da li su potencijalni negativni primjeri validni. U ovim metodama, vrijedni neprijateljski primjer prevari napad model a i odlučuje se da je semantički ili sintaktički vrijedan drugim modelom. Istraživanje do sada računalo je sve takve primjere kao greške napadnutim modelom. Svjedočili smo da ovi neprijateljski primjeri možda nisu nedostatke u napadnutom modelu, već nedostatke u modelu koji određuje vrijednost. Primjeravamo takve nepravilne ulaze protivnih primjera drugog reda. Predlažemo ograničenu krvnu snagu i povezanu metričku ACCS-u kao alat za procjenu snage ograničenja na druge naređene neprijateljske primjere. Da bi stvorili ovu krivu, dizajnirali smo neprijateljski napad da bi se direktno pokrenuli na semantičke modele sličnosti. Testiramo na dva ograničenja, Univerzalni koder kazne (USE) i BERTScore. Naši nalazi ukazuju na to da postoje primjeri drugog reda, ali obično su manje česti od prvog reda neprijateljskih primjera u modelima stanja umjetnosti. Oni također ukazuju na to da je korisnik korisnika učinkovit kao ograničenje na neprijateljske primjere NLP-a, dok je BERTScore skoro neiffektivan. Kod za provođenje eksperimenata u ovom papiru je dostupan ovdje.', 'de': 'Adversarische Beispielgenerierungsmethoden in NLP stützen sich auf Modelle wie Sprachmodelle oder Satzkodierer, um festzustellen, ob potenzielle widersprüchliche Beispiele gültig sind. Bei diesen Methoden täuscht ein gültiges kontraproduktives Beispiel das angegriffene Modell und wird durch ein zweites Modell als semantisch oder syntaktisch gültig bestimmt. Bisherige Forschung hat all diese Beispiele als Fehler des angegriffenen Modells gezählt. Wir behaupten, dass diese widersprüchlichen Beispiele keine Fehler im angegriffenen Modell sind, sondern Fehler im Modell, die die Gültigkeit bestimmen. Wir bezeichnen solche ungültigen Eingaben als widersprüchliche Beispiele zweiter Ordnung. Wir schlagen die Constraint Robustheitskurve und die dazugehörige Metrik ACCS als Werkzeuge für die Bewertung der Robustheit einer Constraint an gegensätzlichen Beispielen zweiter Ordnung vor. Um diese Kurve zu generieren, entwerfen wir einen gegnerischen Angriff, der direkt auf die semantischen Ähnlichkeitsmodelle läuft. Wir testen mit zwei Einschränkungen, dem Universal Satence Encoder (USE) und BERTScore. Unsere Ergebnisse deuten darauf hin, dass solche Beispiele zweiter Ordnung existieren, aber typischerweise weniger verbreitet sind als gegensätzliche Beispiele erster Ordnung in modernen Modellen. Sie weisen auch darauf hin, dass USE als Einschränkung für NLP-Beispiele wirksam ist, während BERTScore nahezu ineffektiv ist. Code für die Durchführung der Experimente in dieser Arbeit finden Sie hier.', 'id': 'Metode generasi contoh adversial dalam NLP bergantung pada model seperti model bahasa atau koder kalimat untuk menentukan apakah contoh adversial potensial adalah valid. Dalam metode ini, contoh musuh yang valid membodohi model yang diserang, dan ditentukan untuk menjadi semantis atau sintaksi valid oleh model kedua. Penelitian sampai sekarang telah menghitung semua contoh seperti kesalahan oleh model yang diserang. Kami menegaskan bahwa contoh-contoh musuh ini mungkin bukan kesalahan dalam model yang diserang, tetapi kesalahan dalam model yang menentukan validitas. Kami menamakan masukan tidak valid contoh lawan kedua. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples.  Untuk menghasilkan kurva ini, kami merancang serangan musuh untuk menjalankan langsung pada model semantis persamaan. Kami menguji dua batasan, Universal Sentence Encoder (USE) dan BERTScore. Penemuan kami menunjukkan bahwa contoh-contoh kedua ada, tapi biasanya lebih rendah dari contoh-contoh lawan pertama dalam model yang terbaik. Mereka juga menunjukkan bahwa USE efektif sebagai batas pada contoh-contoh musuh NLP, sementara BERTScore hampir tidak efektif. Code for running the experiments in this paper is available here.', 'fa': 'روش نسل\u200cهای مثال مخالف در NLP بر مدل\u200cهای مانند مدل\u200cهای زبان یا رمز\u200cدهنده\u200cهای جمله وابستگی دارند تا تعریف کنند که آیا مثال\u200cهای مخالف potentially valid هستند. در این روش، یک مثال دشمنی قابل توجه به مدل حمله می\u200cشود و تصمیم گرفته می\u200cشود که با یک مدل دوم به semantic یا syntactically valid باشد. تحقيقات تا الان همه مثالهايي که توسط مدل حمله شده رو اشتباه کرده رو شماره کرده ما توضیح دادیم که این مثالهای دشمنی ممکن است در مدل حمله کمی نباشه، بلکه کمی در مدل که قابلیت را تعیین می کند. ما به عنوان مثالهای دشمنی دوم دستور غیرقابل توجه می کنیم. ما به عنوان ابزارها برای ارزیابی قوت یک محدودیت به مثال مخالف دوم از محدودیت محدودیت را پیشنهاد می کنیم. برای تولید این کروپ، ما یک حمله دشمنی را طراحی می کنیم تا مستقیماً روی مدل شباهت semantic run. ما روی دو محدودیت آزمایش می کنیم، رمزگذاری مجموعه جهانی (USE) و BERTScore. نتیجه\u200cهای ما نشان می\u200cدهند که این نمونه\u200cهای دوم وجود دارد، ولی معمولاً کمتر از نمونه\u200cهای مخالف اولین نمونه\u200cهای هنری هستند. آنها همچنین نشان می دهند که استفاده از استفاده به عنوان محدودیت بر مثالهای مخالف NLP موثر است، در حالی که BERTScore تقریباً بی اثر است. کد برای اجرای آزمایشات در این کاغذ اینجا موجود است.', 'sw': 'mbinu za uzalishaji wa mifano mbalimbali katika NLP zinategemea mifano kama vile mifano ya lugha au kupunguza hukumu ili kuamua kama mifano yanayoweza kuwa ni sahihi. Katika njia hizi, mifano inayotokana na upinzani maarufu huifanyia mifano inayoshambuliwa, na inakusudiwa kuwa imethibitishwa na mifano ya pili. Utafiti mpaka sasa umehesabu mifano yote kama vile makosa ya modeli ya shambulio. Tunagombana kuwa mifano haya yanayopingana inawezekana hayakuwa mabaya katika mtindo wa shambulio, lakini ni mabaya katika mtindo ambao unahitimisha ukweli. Tunaiita taratibu hizo zisizo sahihi inaweka mifano mbadala ya mfumo wa pili. Tunazipendekeza vizuizi vikwazo vya uchumi, na kinachohusiana na ACCS, kama vifaa vya kutathmini nguo za vizuizi vya mifano mbadala ya pili. Ili kutengeneza kizuizi hiki, tunaunda shambulio la upinzani ili kuendelea moja kwa moja kwenye mifano inayofanana na kigaidi. Tunajaribu vikwazo viwili, Kifungu cha Sheria Duniani (Umoja wa Mataifa) na BERTScore. Matokeo yetu yanaonyesha kuwa mifano ya aina hii ya pili yanakuwepo, lakini kwa kawaida ni ndogo ya kawaida kuliko mifano mbadala ya aina ya kwanza katika mitindo ya sanaa. Vilevile wanaonyesha kuwa Umoja wa Mataifa una ufanisi kama vikwazo dhidi ya mifano mbadala ya NLP, wakati BERTScore inakaribia kuwa haina ufanisi. Code for running the experiments in this paper is available here.', 'ko': 'NLP의 대항성 예시 생성 방법은 언어 모델이나 문장 인코더 등 모델에 의존하여 잠재적인 대항성 예시가 유효한지 확인한다.이러한 방법에서 효과적인 대항성 예는 공격당한 모델을 우롱하고 두 번째 모델에 의해 의미나 문법이 유효하다고 확정되었다.지금까지의 연구는 이러한 모든 예시를 공격 모델의 오류로 계산했다.우리는 이러한 대항적인 예가 피공격 모델의 결함이 아니라 유효성을 결정하는 모델의 결함일 수 있다고 생각한다.우리는 이런 무효 입력을 2 단계 대항성 예시라고 부른다.우리는 제약 노봉성 곡선과 관련된 도량 ACCS를 제시하여 제약이 2 단계의 대항성에 대한 예시를 평가하는 노봉성을 평가하는 도구로 삼았다.이 곡선을 생성하기 위해 우리는 의미 유사성 모델에서 직접 운행하는 대항적 공격을 설계했다.우리는 유니버설 문장 인코더 (USE) 와 BERTScore 두 가지 제약 조건을 테스트했다.우리의 연구 결과에 의하면 이러한 2단계 예는 존재하지만 가장 선진적인 모델에서는 일반적으로 1단계 대항성 예보다 흔히 볼 수 없다.NLP 대항성 예시에서 사용은 유효한 제약이고 BERTScore는 거의 무효임을 나타냈다.여기에 본문의 실험을 실행하는 코드를 제공하였다.', 'tr': "NLP içinde nädogry örnekler döredilmeler dili modelleri ýaly modlere, sözlem kodeleri mümkin deňil ýaly modlere ynanýar. Bu ýollarda, gowy täsirli mysal nusgasy nusgasyny çalyşyrýar we ikinji nusgasyna görä semantik ýa-da syntaktik ýagdaý bolmagy karar berýär. Araşdyrma şu wagt hemme mysal goşulan nusga tarapynda ýalňyşlyk ýaly hasaplanýar. Biz bu düşmanlyk örnekleri salyk nusgasynda ýalňyşlyk däldir diýip pikir edýäris, ýöne ýalňyşlyk baradaky nusgalaryň ýetleri däldir. Biz bu şekilde maýyp girişi ikinji dürniş täsirli mysal diýýäris. Biz çykyş güýçli ýigrengi we gerekli metrikli ACCS-i, ikinji рет döwletlerde çykyş döwletlerini deňlemek üçin synplanlaýarys. Bu kurva döretmek üçin, semantik benzeri modellerde direkt ýöremek üçin düşmanlyk salyky tasarlandyrys. Biz iki mümkin edip, Universal Sentence Ködler (USE) we BERTScore'a synanyşýarys. Biziň tapylyklarymyz ýaly ikinji görnüş mysal bar diýip görkeýär, ýöne adatça birinji görnüş düzgün-süzgün nusgalaryň durumynda ýakyn düzgün däldir. Olar hem USEKI NLP teňkili mysallary ýaly täsirli bolup täsirli bolandygyny aýdýarlar, BERTScore ýaly täsirli däldir. Bu kagyzdaki eksperimentler çykarmak üçin kody şu ýerde bar.", 'af': "Adversariale voorbeeld generasie metodes in NLP vertrou op modele soos taal modele of sentence koders om te bepaal of potensiele teenstandaaries voorbeelde geldige is. In hierdie metodes, 'n geldige teëstandersvoorbeeld dwaas die model wat aangeval word, en is bepaal om semantiese of sintaktisies geldig te wees deur 'n tweede model. Voorskou tot dag het al soos voorbeelde as foute getel deur die gevaardige model. Ons het gespreek dat hierdie teëstandersvoorbeelde dalk nie foute in die gevaakte model is nie, maar foute in die model wat die geldigheid bepaal. Ons bepaal sodanige ongeldige inputs tweede volgorde teenstandaarde voorbeelde. Ons voorstel die beperking kragtige kurve en geassosieerde metriese ACCS as nutsprogramme om die kragtigheid van 'n beperking te evalueer na tweede volgorde teenstandaaries voorbeelde. Om hierdie kurve te genereer, ontwerp ons 'n teëstandige atak om direk op die semantiese gelykheidsmodele te hardloop. Ons toets op twee beheinings, die Universele Sentence Encoder (USE) en BERTScore. Ons bevestings wys dat sulke tweede volgorde voorbeelde bestaan, maar is tipies minder gemeenskap as eerste volgorde teenstandaarie voorbeelde in staat-van-die-kunstenmodele. Hulle indiek ook dat gebruik effektief is as beheing op NLP teenstandaaries voorbeelde, terwyl BERTScore is byna oneffektief. Kode vir die loop van die eksperimente in hierdie papier is beskikbaar hier.", 'sq': 'Metodat e gjenerimit të shembujve kundërshtarë në NLP mbështeten në modele si modelet gjuhësore apo koduesit e fjalëve për të përcaktuar nëse shembujt e mundshëm kundërshtarë janë të vlefshëm. Në këto metoda, një shembull i vlefshëm kundërshtar mashtron modelin që sulmohet dhe është i përcaktuar të jetë i vlefshëm semantikisht apo sintaktikisht nga një model i dytë. Kërkimi deri tani ka numëruar të gjitha shembujt si gabime nga modeli i sulmuar. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity.  Ne i quajmë të tilla hyrje të pavlefshme shembuj kundërshtarë të rendit të dytë. Ne propozojmë kurvën e fuqisë së kufizimit dhe ACCS metrike të lidhura, si mjete për vlerësimin e fuqisë së kufizimit për shembuj kundërshtarë të rendit të dytë. Për të gjeneruar këtë kurvë, ne dizajnojmë një sulm kundërshtar për të ecur drejtpërdrejt mbi modelet semantike të ngjashmërisë. Ne testojmë dy kufizime, Universal Sentence Encoder (USE) dhe BERTScore. Zbulimet tona tregojnë se të tilla shembuj të rendit të dytë ekzistojnë, por janë tipikisht më pak të zakonshëm se shembuj kundërshtarë të rendit të parë në modelet më të moderne. Ata tregojnë gjithashtu se përdorimi është efektiv si kufizim në shembujt kundërshtarë të NLP, ndërsa BERTScore është pothuajse jo efektiv. Kodi për drejtimin e eksperimenteve në këtë letër është në dispozicion këtu.', 'am': 'የNLP ምሳሌ ምርጫዎች ምሳሌዎች እንደ ቋንቋ ምሳሌዎች ወይም የፊደል አካባቢዎች እውነተኛ ምሳሌዎች መሆኑን ለማረጋገጥ ይጠብቃሉ፡፡ በዚህ ሥርዓቶች፣ የእውነት ተቃዋሚ ምሳሌ ምሳሌውን በመጋደል ሰነፎታል፡፡ ምርመራ እስከ ዛሬ ድረስ እንደተመሳሳይ ስህተቶችን እንደ ተመሳሳይ ሞዴል ቆጥሯል፡፡ እነዚህ ተቃዋሚዎች ምሳሌዎች ውሸትን በሚያረጋግጡ ምሳሌ ላይ ስሕተት እንዳይሆኑ እንከራከራለን፡፡ እንደዚህ የዋጋ ጥያቄ ሁለተኛ ተቃዋሚ ምሳሌዎችን እናስገራለን፡፡ የግድ ልብስ መቆረጥ እና የሜትሪክ ACCS እና የሁለተኛ ተቃዋሚዎች ምሳሌዎችን ለማስተምር የግድ ልብስ መሣሪያን እናስባለን፡፡ To generate this curve, we design an adversarial attack to run directly on the semantic similarity models.  ሁለትን ግንኙነት፣ የዓለማዊ ስርዓት ኮድ (የአሜሪካ) እና BERTScore እንሞክራለን፡፡ ፍለጋዎቻችን የሁለተኛ ደረጃ ምሳሌዎች እንዲኖሩ ያሳያል፣ ነገር ግን የፊተኛይቱ ተቃውሞ በተቃዋሚ ምሳሌዎች ትልቅ ነው፡፡ እንደገናም ብERTScore በቁጥር ምንም ፍላጎት የሌለው ሆኖ የአሜሪካ ዩነቨርስቲ በNLP በተቃዋሚ ምሳሌዎች ላይ ግድ እንዳለው ያስታውቃሉ፡፡ በዚህ ገጽ ውስጥ ፈተናዎችን ለመፈለግ የኮድ ወደዚህ ነው፡፡', 'hy': 'ՆԼՊ-ում գտնվող հակառակ օրինակների ստեղծման մեթոդները հիմնված են լեզվի մոդելների կամ նախադասությունների կոդավորների վրա, որպեսզի որոշենք, արդյոք պոտենցիալ հակառակ օրինակները ճշմարիտ են Այս մեթոդներում ճշգրիտ հակառակորդ օրինակը հիմարանում է մոդելը, որը հարձակվում է, և որոշվում է, որ սեմանտիկ կամ սինտակտիկ ճշգրիտ է երկրորդ մոդելի միջոցով: Այսօրվա հետազոտությունները հաշվարկել են բոլոր օրինակները, ինչպիսիք են հարձակված մոդելի սխալները: Մենք պնդում ենք, որ այս հակառակորդ օրինակները կարող են չլինել հարձակված մոդելի թերություններ, այլ մոդելի թերություններ, որոնք որոշում են ճշմարտությունը: We term such invalid inputs second-order adversarial examples.  Մենք առաջարկում ենք սահմանափակումների ուժեղության կոր և կապված մետրական ACCS-ը, որպես միջոցներ երկրորդ հակառակորդ օրինակների սահմանափակումների ուժեղության գնահատման համար: Այս կորի ստեղծելու համար մենք դիզայնում ենք հակառակ հարձակում, որպեսզի ուղղակի աշխատենք սեմանտիկ նմանության մոդելների վրա: Մենք փորձում ենք երկու սահմանափակումների վրա՝ Համաշխարհային նախադասությունների կոդերը (USB) և BER-ը: Մեր հայտնաբերությունները ցույց են տալիս, որ այդպիսի երկրորդ շարժման օրինակներ գոյություն ունեն, բայց սովորաբար ավելի քիչ հաճախ են, քան առաջին շարժման հակառակորդ օրինակները ամենաբարձր մոդելներում: Նրանք նաև ցույց են տալիս, որ օգտագործումը արդյունավետ է որպես սահմանափակում ՆԼՊ հակառակյալ օրինակների վրա, մինչդեռ ԲԵՌՏՍկորը գրեթե անարդյունավետ է: Այս թղթի փորձարկումների կատարման կոդը հասանելի է այստեղ:', 'az': 'NLP içində müəllif nümunələri dil modelləri və cümlələr kodlayıcıları kimi modellərə təvəkkül edir ki, mümkün müəllif nümunələrin doğru olub olmadığını belə müəyyən edirlər. Bu metodlarda, müəyyən bir düşmənçilik məsəli modeli saldırılır və ikinci modeli ilə semantik və sintaktik olaraq müəyyən edilir. Şimdiye qədər araştırma bütün məsəlləri salınmış modellərin xətaları hesabladı. Biz mübahisə edirik ki, bu düşmənçilik məsəllər saldıqları modellərdə zəif olmayabilir, ancaq valideyit təyin edən modellərdə zəif olmayabilir. Biz bu müəyyən deyirik ki, ikinci sırada düşmənçi misallar. Biz həddi-büluq müddətini və birlikdə metrik ACCS-ni, ikinci sırada düşmənçilik örnəklərinə müdafiə etmək üçün vasitələr kimi təklif edirik. Bu ipəki yaratmaq üçün, semantik similarlıq modellerinə düzgün davam etmək üçün bir düşmənçi saldırımı tasarlayırıq. Biz iki müəyyən edirik, Universal sentence Encoder (USE) və BERTScore. Bizim tapılarımız bu ikinci sıralar məsəllərinin var, amma ilk sıralar müxtəlif məsəllərdən daha az ortaqdır. Onlar həmçinin NLP düşmənçilik məsəllərində istifadə edilməsi üçün istifadə edirlər, BERTScore yaxınlaşıq deyildir. Bu kağızdaki eksperimentləri çalışmaq üçün kodu burada faydalanır.', 'bn': 'এনএলপিতে প্রথম উদাহরণ প্রজন্মের পদ্ধতি নির্ভর করে ভাষার মডেল অথবা শাস্তি এনকোডারের মত মডেলের উপর নির্ভর করে যাতে সম্ভাব্য বি এই পদ্ধতিতে একটি বৈধ বিরোধী উদাহরণ বোকা হয়েছে যে মডেল আক্রমণ করা হচ্ছে, এবং দ্বিতীয় মডেল দ্বারা সামান্যিক বা সিন্ট্যাকটিক্যালিক ভ পর্যন্ত গবেষণা হামলার মডেলের দ্বারা সমস্ত উদাহরণ হিসেবে গবেষণা করেছে। আমরা যুক্তি দিচ্ছি যে এই বিরোধী উদাহরগুলো হয়ত আক্রমণের মডেলে ভুল নয়, কিন্তু মডেলে যা বৈধতা নির্ধারণ করে। আমরা এই ধরনের অকার্যকর ইনপুট দ্বিতীয় আদেশের বিরুদ্ধে বিরোধী উদাহরণ দিয়েছি। আমরা সীমাবদ্ধ রোবাস্টস কার্ভ এবং যোগাযোগী মেট্রিক এসিসিসিকে প্রস্তাব করি, দ্বিতীয় নির্দেশের বিরোধী উদাহরণের মাধ্যমে বাধ্যতার পো এই বক্রটি তৈরি করার জন্য আমরা একটি বিরোধী আক্রমণের ডিজাইন করি যাতে সেমেন্টিকের সমতামূলক মডেলে সরাসরি চালানোর জন্য। আমরা বিশ্ববিদ্যালয়ের শাস্তি এনকোডার (যুক্তরাষ্ট্র) এবং বের্ট স্কোরের দুটি নিয়ন্ত্রণের উপর পরীক্ষা করি। আমাদের আবিস্কার নির্দেশ দেয়া হয়েছে যে এই দ্বিতীয় ক্ষেত্রের উদাহরণ রয়েছে, কিন্তু সাধারণত প্রথম ক্ষেত্রের বিরোধী উদাহরণের They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual.  এই পত্রিকায় পরীক্ষা চালানোর কোড এখানে পাওয়া যাচ্ছে।', 'ca': "Els mètodes de generació d'exemples adversaris en NLP es basan en models com els models lingüístics o codificadors de frases per determinar si els exemples adversaris potencials són válids. En aquests mètodes, un exemple adversari válid enganya el model que és atacat, i està determinat per ser semànticament o sinàcticament válid per un segon model. Fins ara la recerca ha comptat tots els exemples com els errors del model atacat. Afirmem que aquests exemples adversaris potser no són defectes en el model atacat, sinó defectes en el model que determina la validez. Nominem aquestes entrades invalides exemples adversaris de segon ordre. Proposem la corba de robustet de la restricció, i l'ACCS mètrica associada, com eines per avaluar la robustet d'una restricció a exemples adversaris de segon ordre. Per generar aquesta corba, dissenyem un atac adversari per executar directament els models semàntics de similitud. Ens provem en dues restriccions, l'codificador universal de sentences (USE) i BERTScore. Els nostres descobriments indican que existeixen exemples de segon ordre, però són normalment menys comuns que exemples adversaris de primer ordre en models d'última generació. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual.  Aquí hi ha codi per executar els experiments d'aquest article.", 'bs': 'Preporučne metode generacije primjera u NLP oslanjaju se na modele poput jezičkih modela ili kodera rečenica kako bi utvrdili da li su potencijalni negativni primjeri validni. U ovim metodama, vrijedan neprijateljski primjer prevari model koji se napada i odlučuje da je semantički ili sintaktički validan drugim modelom. Istraživanje do sada računalo je sve takve primjere kao greške napadnutim modelom. Svjedočili smo se da ovi neprijateljski primjeri možda nisu nedostatke u napadnutom modelu, već nedostatke u modelu koji određuje validnost. Mi smatramo takvim nepravilnim ulazima protivnicima drugog reda. Predlažemo ograničenu krivu snage i povezanu metričku ACCS-u kao alat za procjenu snage ograničenja prema drugom naređenju neprijateljskim primjerima. Da bi stvorili ovu krivu, dizajnirali smo neprijateljski napad da bi se direktno pokrenuli na semantičke modele sličnosti. Testiramo na dva ograničenja, Univerzalni koder kazne (USE) i BERTScore. Naši nalazi ukazuju na to da postoje primjeri drugog reda, ali obično su manje česti od prvog reda neprijateljskih primjera u modelima stanja umjetnosti. Oni također ukazuju da je korisnička upotreba efikasna kao ograničenja na neprijateljske primjere NLP-a, dok je BERTScore skoro neiefikasna. Kod za pokretanje eksperimenata u ovom papiru je dostupan ovdje.', 'cs': 'Metody generování nepříznivých příkladů v NLP se spoléhají na modely, jako jsou jazykové modely nebo kódéry vět, aby určily, zda jsou potenciální příklady nepříznivých příkladů platné. V těchto metodách platný příklad opatrně oklamá napadený model a je určen jako sémanticky nebo syntakticky platný druhým modelem. Dosavadní výzkum započítal všechny takové příklady jako chyby napadeného modelu. Tvrdíme, že tyto nepříznivé příklady nemusí být chybami napadeného modelu, ale chybami modelu, které určují platnost. Takové neplatné vstupy označujeme za příklady druhého řádu. Navrhujeme křivku robustnosti omezení a související metriku ACCS jako nástroje pro hodnocení robustnosti omezení na příklady druhého řádu. Pro vytvoření této křivky navrhujeme nepřátelský útok, který běží přímo na sémantických modelech podobnosti. Testujeme na dvou omezeních, Universal Sentence Encoder (USE) a BERTScore. Naše zjištění naznačují, že takové příklady druhého řádu existují, ale jsou obvykle méně běžné než příklady prvního řádu v moderních modelech. Také naznačují, že USE je efektivní jako omezení na příkladech NLP, zatímco BERTScore je téměř neefektivní. Kód pro provádění experimentů v tomto článku je k dispozici zde.', 'et': 'NLP-s kasutatavad kõrvalsaaduste näidete genereerimise meetodid tuginevad sellistele mudelitele nagu keelemudelid või lausekodeerijad, et teha kindlaks, kas võimalikud vastandlikud näited on kehtivad. Nende meetodite puhul petab kehtiv vastandlik näide rünnatavat mudelit ja teise mudeli puhul määratakse see semantiliselt või süntaktiliselt kehtivaks. Seni on uuringud lugenud kõiki selliseid näiteid rünnatud mudeli vigadena. Väidame, et need vastandlikud näited ei pruugi olla vigad rünnatavas mudelis, vaid vead mudelis, mis määrab kehtivuse. Me nimetame selliseid vigaseid sisendeid teise järgu vastandlikuks näiteks. Pakume välja piirangu tugevuskõvera ja sellega seotud meetrilise ACCS-i kui vahendeid piirangu tugevuse hindamiseks teise astme konkurentsiaalsete näidete puhul. Selle kõvera genereerimiseks kavandame vastandliku rünnaku, mis käivitub otse semantilise sarnasuse mudelitel. Testime kahel piirangul: universaalne lausekodeerija (USE) ja BERTScore. Meie tulemused näitavad, et sellised teise astme näited on olemas, kuid tavaliselt on need vähem levinud kui esimese astme vastandlikud näited kaasaegsete mudelite puhul. Samuti näitavad nad, et kasutamine on tõhus piiranguna uue õppekava vastastikustele näidetele, samas kui BERTScore on peaaegu ebatõhus. Kood eksperimentide käivitamiseks selles paberis on saadaval siit.', 'fi': 'NLP:n adversaaristen esimerkkien generointimenetelmät perustuvat kielimallien tai lauseenkooderien kaltaisiin malleihin selvittääkseen, ovatko mahdolliset vastakkaiset esimerkit päteviä. Näissä menetelmissä kelvollinen vastakkainasetteluesimerkki huijaa hyökkäävää mallia, ja toinen malli määrittää semanttisesti tai syntaktisesti kelvollisen mallin. Tähän mennessä tutkimukset ovat laskeneet kaikki tällaiset esimerkit hyökkäyksen kohteena olevan mallin virheiksi. Väitämme, että nämä vastakkaiset esimerkit eivät välttämättä ole vikoja hyökätyssä mallissa, vaan vikoja mallissa, joka määrittää pätevyyden. Me kutsumme tällaisia virheellisiä syöttöjä toisen asteen vastakkaisiksi esimerkeiksi. Ehdotamme rajoitteen kestävyyskäyrää ja siihen liittyvää metristä ACCS:ää työkaluiksi, joilla arvioidaan rajoitteen kestävyyttä toisen asteen kontrastiaalisiin esimerkkeihin. Tämän käyrän luomiseksi suunnittelemme vastakkainasetteluhyökkäyksen suoraan semanttisten samankaltaisuusmallien päälle. Testaamme kahdella rajoituksella, Universal Sentence Encoder (USE) ja BERTScore. Löydöksemme osoittavat, että tällaisia toisen asteen esimerkkejä on olemassa, mutta ne ovat tyypillisesti vähemmän yleisiä kuin ensimmäisen asteen vastakkaiset esimerkit uusimmissa malleissa. Ne osoittavat myös, että käyttö rajoittaa tehokkaasti NLP:n vastakkaisia esimerkkejä, kun taas BERTScore on lähes tehoton. Koodi kokeiden suorittamiseen tässä artikkelissa on saatavilla täältä.', 'jv': 'Advertorial example Generation Methods in NLP MyUserName on freenode Ngawe Perintah sing iki, akeh dumadhi sing deweke seneng pisan seneng pisan banget, dadi model kang asêpêpakan, wipe ngêpakan ngersampek sematik lan seneng pisan kelangan seneng model segondi. Pesenyahan karo nganggo dino sing komplit gak bener example lagi nggawe eror ning model atake Awak dhéwé pisan karo hal-hal bener bayaké iki, mengko ora ngerasah sing nyimpen ning modèl dadi, nanguwis neng modèl sing dadi apik dhéwé Add a new criterion to this string Awak dhéwé nggawe nguasai nggawe kesemplak apat-apat, lan aceptaaken meta-CS, iki bakal ono alat kanggo hasil nggawe nguasai angat sawar-sistem sing berarti iki bakal sing diradirampakan tanggal apat. To Genere this curve, we design an Counter Attack to ran directively on the semanti Simlarty model. The first letter of the first letter is from the second letter in the translation. Awak dhéwé seneng mulai gawe ngubungan kelas telu, Universal Sentense koder (USE) lan BERT Point. Kita mbutuhan kelompok ngono kuwi tindakan kelompok sekondi, pero ngono sakjane podho kelompok sing berarti karo perusahaan-sekondi, nan model-saka-perusahaan. Punika mengko ngomong gunakake USE penggunaké bukane dadi nggawe gerarané NLP pakan-pakan bisa, terus BERT Kadeh kanggo ngilanggar ujaran neng sapar iki ning kene', 'ha': "Metoden kizaɓa na dabar-daban da ke cikin NLP, yana dõgara a kan misãlai kamar misãlai cikin harshen ko kodkodi na maganar dõmin ya ƙayyade idan masu yiwuwa masu motsi na inganci ne. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model.  Ana ƙidãya cikin shirin a yanzu ya lissafa duk misãlai kamar misali da misalin an yi attacka. Munã jãyayya da waɗannan misãlai da ke sãɓa wa tsakanin a cikin misãlai da aka yi attacka, amma misãlai da ke cikin shirin da ya ƙayyade gaskiyar. Munã ƙayyade wannan masu maras da inganci, za'a jẽfa misãlai na biyu. Tuna goyya da kuro na damƙara, kuma masu haɗi da mataimaki na AC, kamar zance da za'a iya ƙayyade tufãfin da aka haramta zuwa misãlai na biyu. Za iya ƙiƙiro wannan curve, za mu ƙayyade wata shawara mai motsi dõmin ka yi tafiya dira kan motsi masu daidaita na semantic. Lalle ne, Munã jarraba cewa biyu, kodi na Jalal Duniani da BERTscore. FayiyinMu na nũna cewa misãlai na biyu na kasance, amma ko da kawaici, su zama mafi daidai wa misãlai masu motsi na farko a cikin-state-of-the-art. Suna nuna cewa, U'S mai amfani ne kamar an ƙudura a kan NLP-motsi masu motsi, alhãli kuwa BERTscore na kusa bã da amfani ba. Kodi na tafiyar da jarrabai cikin wannan takardan, za'a iya sãmu.", 'he': 'שיטות יוצר דוגמאות נוגדות ב- NLP תלויות על דוגמאות כמו דוגמאות שפה או קודפי משפטים כדי לקבוע אם דוגמאות נוגדות פוטנציאליות נכונות. בשיטות אלה, דוגמא יריבית נכונה מטופשת את המודל שנתקף, והנחושה להיות נכונה סמנטית או סינטקטית על ידי מודל שני. מחקר עד היום ספר את כל דוגמאות כאלה כמו שגיאות על ידי המודל המותקף. אנחנו טוענים שהדוגמאות הירידות האלה אולי לא פגיעות במודל המותקף, אלא פגיעות במודל שמקביל את האישיות. אנחנו מתייחסים כזה כניסות בלתי נכונות דוגמאות נוגדות מסדר שני. אנו מציעים את עקוב החזקה של המגבלות, והACCS המטרי הקשור, ככלים להעריך את החזקה של המגבלה לדוגמאות נוגדות בשנייה. כדי ליצור את הקורב הזה, אנחנו מעצבים התקפה יריבית כדי לרוץ ישירות על דוגמני הדמיון הסמנטי. אנחנו בודקים על שני מחסומים, קודד המשפטים היניברסלי (USU) וברט. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models.  הם גם מצביעים כי השימוש הוא יעיל כחוסר על דוגמאות נוגדות NLP, בעוד BERTScore הוא כמעט לא יעיל. קוד לנהל את הניסויים בעיתון הזה זמין כאן.', 'sk': 'Metode ustvarjanja neželenih primerov v NLP se zanašajo na modele, kot so jezikovni modeli ali kodirniki stavkov, da ugotovijo, ali so potencialni kontradirni primeri veljavni. V teh metodah veljaven kontradiktorski primer preslepi napadenega modela in je določen, da je semantično ali sintaktično veljaven z drugim modelom. Raziskave do danes so vse take primere štela kot napake napadenega modela. Trdimo, da ti kontradiktni primeri morda niso pomanjkljivosti napadenega modela, ampak pomanjkljivosti modela, ki določa veljavnost. Takšne neveljavne vnose imenujemo kontradiktorski primeri drugega reda. Predlagamo krivuljo robustnosti omejitve in s tem povezano metrično ACCS kot orodje za ocenjevanje robustnosti omejitve na primere konkurence drugega reda. Za ustvarjanje te krivulje načrtujemo kontradikcijski napad, ki bo potekal neposredno na semantičnih podobnih modelih. Testiramo na dveh omejitvah, univerzalnem kodirniku stavkov (USE) in BERTScore. Naše ugotovitve kažejo, da takšni primeri drugega reda obstajajo, vendar so običajno manj pogosti kot primeri prvega reda v najsodobnejših modelih. Prav tako navajajo, da je uporaba učinkovita kot omejitev primerov konkurenčnega novega programa, medtem ko je BERTScore skoraj neučinkovit. Koda za izvajanje poskusov v tem članku je na voljo tukaj.', 'bo': 'NLP ནང་གི་Adversarial example generation methods rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. ཐབས་ལམ་འདི་དག་ལྟར་ཕན་ཚུལ་མཁན་མེད་པའི་དཔེ་བརྗོད་ཀྱི་རྣམ་པ་ཞིག་གིས་གནོད་བཀོལ་ཡོད་པའི་མིག་དཔེ་བརྗོད་བྱས་ན་ཟེར་ ཟླ་ཚེས་ལྟ་བརྟག་ཞིབ་བྱས་པའི་དཔེ་བརྗོད་འདིའི་ནང་གི་ནོར་འཁྲུལ་ཞིག་ཡོད་པ་རེད། ང་ཚོས་གདོང་ལེན་བྱེད་པའི་དཔེ་བརྗོད་འདི་དག་གི་ནང་དུ་འཕགས་རིས་བཀོད་པའི་མ་དཔེ་བརྗོད་ཡོད་མིན་ནམ། ང་ཚོས་ཕན་འབྲས་མེད་པའི་འཇུག་སྣོད་ཀྱི་དཔེར་བརྗོད་ཀྱི་རྣམ་པ་གཉིས་ཀྱི་དབྱིབས་འགོད་པ་ཚོར་མཚོ We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. ང་ཚོའི་མཐོང་སྣང་གི་དཔེ་བརྗོད་པ་དེ་བཞིན་པའི་གོ་རིམ་པ་གཉིས་ཀྱི་དཔེ་བརྗོད་ནི་ཡོད་པ་རེད། ཁོང་ཚོས་ཀྱང་USE་ནི་NLP གཟུགས་བརྡན་གྱི་དཔེར་བརྗོད་ལ་སྒྲིག་འགོད་པ་ལྟར་ཕན་ཐོགས་ཡོད་པ་དང་། BERTScore་འདི་ཡང་ཉེ་གནས་སྦྱོ ཤོག་བྱང་འདིའི་ནང་གི་སྔོན་སྒྲིག་ལག་ལེན་འཐབ་པའི་ཨང་གྲངས་འདིར་ཡོད་པ་རེད།'}
{'en': 'Investigating Novel Verb Learning in BERT : Selectional Preference Classes and Alternation-Based Syntactic Generalization BERT : Selectional Preference Classes and Alternation-Based Syntactic Generalization', 'ar': 'التحقيق في تعلم فعل الرواية في BERT: فئات التفضيل الانتقائي والتعميم النحوي القائم على التناوب', 'fr': "Étude de l'apprentissage de nouveaux verbes dans le BERT\xa0: classes de préférences sélectives et généralisation syntaxique basée sur l'alternance", 'es': 'Investigando el aprendizaje de verbos novedosos en BERT: clases de preferencia selectiva y generalización sintáctica basada en alternativas', 'pt': 'Investigando o aprendizado de novos verbos no BERT: classes de preferência de seleção e generalização sintática baseada em alternância', 'ja': 'BERTにおける新規動詞学習の調査：選択的嗜好クラスと代替ベースの構文一般化', 'zh': '治BERT之新动词学:择偏好类与交句法泛化', 'hi': 'BERT में उपन्यास क्रिया सीखने की जांच: चयन वरीयता कक्षाएं और परिवर्तन-आधारित वाक्यात्मक सामान्यीकरण', 'ru': 'Исследование нового обучения глаголам в BERT: селективные классы предпочтений и синтаксическое обобщение на основе чередования', 'ga': 'Ag Fiosrú Foghlaim Briathar Úrscéalta i BERT: Ranganna Tosaíochta Roghnacha agus Ginearálú Comhréire Malartach-Bhunaithe', 'hu': 'Új igék tanulásának vizsgálata a BERT-ben: Szelekciós preferencia osztályok és alternatív alapú szintaktikus általánosítás', 'el': 'Ερευνώντας τη νέα μάθηση ρήμων στο ΒΕΡΤ: Επιλεκτικές τάξεις προτίμησης και εναλλασσόμενη συντακτική γενίκευση', 'ka': 'BERT- ში მონიშნული პრეფირენციის კლასები და ალტენტაციის ბაზეული სინტაქტიული გენერალაციაცია', 'kk': 'BERT- дегі жаңалық тізбекті оқыту: Таңдау параметрлері класс мен альтернативті негіздеген синтактикалық жасау', 'mk': 'Истражување на ново учење на реченици во БЕРТ: Селекционални класи на преференции и алтернативна синтактичка генерализација', 'lt': 'Naujų žodžių mokymosi BERT tyrimas: atrankos prioritetų klasės ir alternatyva grindžiama sintaktinė generalizacija', 'it': "Indagine sull'apprendimento dei verbi novelli in BERT: classi di preferenze selettive e generalizzazione sintattica basata sull'alternanza", 'ms': 'Menyelidiki Pempelajaran Verb Novel dalam BERT: Kelas Keutamaan Pemilihan dan Jeneralisasi Sintaktik Berasas Alternatif', 'mt': 'Investigazzjoni ta’ Tagħlim ta’ Verbi Ġdid fil-BERT: Klassijiet ta’ Preferenza Selezzjonali u Ġeneralizzazzjoni Sintattika Bażata fuq Alternattiva', 'ml': 'BERT-ലെ നോവല്\u200d വെര്\u200dബ് പഠിക്കുന്നത് അന്വേഷിക്കുന്നു: തെരഞ്ഞെടുക്കുന്ന മുന്\u200dഗണന ക്ലാസുകളും മാറ്റങ്ങള്\u200d അടിസ്ഥാനമാ', 'mn': 'БЕРТ-д Шинэ Верб суралцах судалгаа: Шинэ сонголтын сонголт класс болон Alternation-Based Syntactic Generalization', 'pl': 'Badanie nowatorskiego uczenia się czasowników w BERT: klasy preferencji selekcyjnych i generalizacja syntaktyczna oparta na alternacji', 'no': 'Investigering av Novel Verb-læring i BERT: Valfriske preferensklasser og alternativ-basert syntaktisk generering', 'ro': 'Investigarea învățării verbelor noi în BERT: Clase de preferință selecționale și generalizarea sintactică bazată pe alternatii', 'sr': 'Istraživanje novog verba učenja u BERT: klase izbornih preferencija i alternacijske sintaktičke generalizacije', 'si': 'පරීක්ෂණය නොවෙල් වර්බ් ඉගෙනීම BERT වල: තෝරාගන්න ප්\u200dරියෝජනය ක්ලාස් සහ වෙනස්ථානය- ආධාරිත සංවිධානය', 'so': 'Qiimeynta dhamaanka ee barta BERT: Fasalaha doorashada iyo Alternation-Based Syntactic Generalization', 'sv': 'Undersökning av nytt verblärande i BERT: Valpreferensklasser och alternativ-baserad syntaktisk generalisering', 'ta': 'பெர்டியில் படிப்பில் நிலை பதிப்பு கற்றுக் கொண்டிருக்கிறது: தேர்வு முன்னுரிமை வகுப்பு', 'ur': 'BERT میں نوئل ورب سکھانے کی تحقیق کرتی ہے: انتخاب پتہ کلاس اور alternation-Based Syntactic Generalization', 'uz': 'Name', 'vi': 'Nghiên cứu thuyết ngôn ngữ ngắn của Berlin: các lớp ưu tiên chọn lọc và chế độ hoang sơ', 'bg': 'Изследване на новото изучаване на глаголи в Изборни класове за предпочитане и синтактична генерализация въз основа на алтернативи', 'nl': 'Onderzoek naar nieuw woordenlernen in BERT: Selectionele voorkeursklassen en alternatiegebaseerde syntactische generalisatie', 'hr': 'Istraživanje novog učenja verba u BERT: klase izbornih preferencija i Syntaktička generalizacija na alternaciji', 'da': 'Undersøgelse af nyt verblæring i BERT: Valgspræferenceklasser og alternativ-baseret syntaksisk generalisering', 'de': 'Untersuchung neuartiger Verbenlernen im BERT: Selektive Präferenzklassen und alternationsbasierte syntaktische Generalisierung', 'id': 'Menyelidiki Belajar Verb Novel di BERT: Kelas Preferensi Seleksi dan Generalisasi Sintaktik Berdasarkan Alternatif', 'ko': 'BERT에서의 동사 학습 연구: 선호류 선택과 교체에 기초한 문법 범화', 'fa': 'تحقیقات یادگیری ورب نوئل در BERT: کلاس\u200cهای ترجیح گزینه\u200cهای انتخاب و تولید سینتیکی بنیاد جایگزینی', 'sw': 'Uchunguzi wa Novel Verb Learning in BERT: Mipango ya Uchaguzi na Umoja wa Kiunganishaji wa Kitengo', 'tr': "BERT'da Saýlaw Tercihleri", 'af': 'Inligting van Novel Verb Leer in BERT: Keuse Voorkeure Klasse en Alternasie- Baseerde Syntaktiese Generalisasie', 'sq': 'Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization', 'am': 'Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization', 'hy': 'Նոր բայերի ուսումնասիրությունը BER-ում. ընտրական նախընտրությունների դասեր և այլընտրանքային հիմնված սինտակտիկ գեներալիզացիա', 'az': "BERT'da Novel Verb 칐yr톛nm톛si: Se칞iml톛r Preference Klasi v톛 Alternation-Based Syntactic Generalization", 'bn': 'বেরেট- এ নোভেল ভার্ব শিক্ষা অনুসন্ধান করা হচ্ছে: নির্বাচনের পছন্দ ক্লাস এবং বিকল্প- ভিত্তিক সিন্টেক্টিক সাধারণ', 'bs': 'Istraživanje novog verba učenja u BERT: klase izbornih preferencija i alternativna sintaktička generalizacija', 'ca': 'Investigar aprenentatge de verbs noves en BERT: Classes de preferències seleccionals i generalització sintàtica basada en alternatives', 'cs': 'Výzkum nového učení slovesníků v BERT: selekční preferenční třídy a alternace-based syntaktická generalizace', 'et': 'Uudised verbiõppe uurimine BERT-is: valikulised eelistusklassid ja alternatsioonipõhine süntaktiline üldistamine', 'fi': 'Uusien verbien oppimisen tutkiminen BERT:ssä: Valinnaiset preferenssiluokat ja Alternation-based syntaktic generalization', 'jv': 'New', 'sk': 'Raziskovanje novega učenja glagolov v BERT: izbirni prednostni razredi in sintaktična generalizacija na podlagi alternacij', 'he': 'חוקר למידת מילים חדשות ב-BERT: שיעורי העדיפות הבחירה', 'ha': '@ item Text character set', 'bo': 'Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization'}
{'en': 'Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT’s few-shot learning capabilities for two aspects of English verbs : alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the  model  expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb / object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in  fine-tuning . For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias : verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.', 'ar': 'لم تستهدف الدراسات السابقة التي تبحث في القدرات النحوية لنماذج التعلم العميق العلاقة بين قوة التعميم النحوي وكمية الأدلة التي يتعرض لها النموذج أثناء التدريب. نعالج هذه المشكلة من خلال نشر نموذج جديد لتعلم الكلمات لاختبار قدرات التعلم قليلة اللقطات في BERT لجانبين من الأفعال الإنجليزية: البدائل وفئات التفضيلات الانتقائية. بالنسبة للأولى ، قمنا بضبط BERT على إطار واحد في زوج تناوب لفظي ونسأل عما إذا كان النموذج يتوقع أن يحدث الفعل الجديد في إطار أخته. بالنسبة للأخير ، قمنا بضبط BERT على شبكة انتقائية غير كاملة من الكائنات اللفظية ونسأل عما إذا كانت تتوقع أزواج فعل / مفعول غير مدروسة ولكنها معقولة. نجد أن BERT يقوم بعمل تعميمات نحوية قوية بعد حالة أو حالتين فقط من كلمة جديدة في الضبط الدقيق. بالنسبة لاختبارات التناوب اللفظي ، نجد أن النموذج يعرض سلوكًا يتوافق مع تحيز العبور: من المتوقع أن تأخذ الأفعال التي شوهدت عدة مرات كائنات مباشرة ، لكن لا يُتوقع أن تحدث الأفعال التي تُرى بأشياء مباشرة بشكل لازم.', 'es': 'Estudios anteriores que investigan las habilidades sintácticas de los modelos de aprendizaje profundo no se han centrado en la relación entre la fuerza de la generalización gramatical y la cantidad de evidencia a la que se expone el modelo durante el entrenamiento. Abordamos este problema mediante el despliegue de un novedoso paradigma de aprendizaje de palabras para poner a prueba las capacidades de aprendizaje de pocos intentos de BERT para dos aspectos de los verbos en inglés: alternancias y clases de preferencias selectivas. Para el primero, ajustamos BERT en un solo fotograma en un par de alternancia verbal y preguntamos si el modelo espera que el verbo novedoso aparezca en su marco hermano. Para esto último, ajustamos BERT en una red selectiva incompleta de objetos verbales y preguntamos si espera pares verbos/objetos no comprobados pero plausibles. Encontramos que BERT hace generalizaciones gramaticales sólidas después de solo una o dos instancias de una palabra nueva en afinación. Para las pruebas de alternancia verbal, encontramos que el modelo muestra un comportamiento que es consistente con un sesgo de transitividad: se espera que los verbos que se ven pocas veces tomen objetos directos, pero no se espera que los verbos que se ven con objetos directos ocurran de forma intransitiva.', 'fr': "Des études antérieures portant sur les capacités syntaxiques des modèles d'apprentissage profond n'ont pas ciblé la relation entre la force de la généralisation grammaticale et la quantité de preuves auxquelles le modèle est exposé pendant la formation. Nous abordons ce problème en déployant un nouveau paradigme d'apprentissage des mots afin de tester les capacités d'apprentissage ponctuelles du BERT pour deux aspects des verbes anglais\xa0: les alternances et les classes de préférences sélectives. Pour le premier, nous affinons BERT sur une seule trame d'une paire verbale-alternance et nous demandons si le modèle s'attend à ce que le verbe roman apparaisse dans son cadre frère. Pour ce dernier, nous ajustons le BERT sur un réseau sélectif incomplet d'objets verbaux et nous nous demandons s'il attend des paires verbe/objet non attestées mais plausibles. Nous trouvons que BERT fait de solides généralisations grammaticales après seulement une ou deux instances d'un mot nouveau en phase de mise au point. Pour les tests d'alternance verbale, nous constatons que le modèle affiche un comportement cohérent avec un biais de transitivité\xa0: les verbes vus quelques fois sont censés prendre des objets directs, mais les verbes vus avec des objets directs ne sont pas censés apparaître de manière intransitive.", 'pt': 'Estudos anteriores investigando as habilidades sintáticas de modelos de aprendizado profundo não visaram a relação entre a força da generalização gramatical e a quantidade de evidências a que o modelo é exposto durante o treinamento. Abordamos esse problema implantando um novo paradigma de aprendizado de palavras para testar as capacidades de aprendizado de poucos tiros do BERT para dois aspectos dos verbos em inglês: alternâncias e classes de preferências de seleção. Para o primeiro, ajustamos o BERT em um único quadro em um par de alternância verbal e perguntamos se o modelo espera que o novo verbo ocorra em seu quadro irmão. Para o último, ajustamos o BERT em uma rede de seleção incompleta de objetos verbais e perguntamos se ele espera pares verbo/objeto não atestados, mas plausíveis. Descobrimos que o BERT faz generalizações gramaticais robustas após apenas uma ou duas instâncias de uma nova palavra em ajuste fino. Para os testes de alternância verbal, descobrimos que o modelo apresenta um comportamento consistente com um viés de transitividade: espera-se que verbos vistos poucas vezes levem objetos diretos, mas verbos vistos com objetos diretos não devem ocorrer intransitivamente.', 'zh': '昔研深学模句法力无语法泛化强,形于练证明量之间。 吾以一新单词学范式为之,以试BERT于英语动词者二:更择偏善之类。 其在前者,动词更对中单BERT,并问模形期新动词见姊妹帻中。 其于后者,吾于不尽之动词择网络而BERT之,问其所望于未然而理动词/是也。 BERT于微调中,止于一二新词之实,语法概之。 动词之交试也,见行与传递性偏:少动词直宾语,与直宾语俱见者动词不可以不传也。', 'ja': 'ディープラーニングモデルの構文能力を調査した以前の研究では、文法的一般化の強さと、トレーニング中にモデルが暴露される証拠の量との間の関係を対象としていない。 私たちは、英語動詞の2つの側面：交替と選択的嗜好のクラスについて、BERTの数少ない学習能力をテストするために、新しい単語学習パラダイムを導入することによって、この問題に対処します。 前者については、言葉の交替ペアの単一フレーム上でBERTを微調整し、モデルがその姉妹フレームで新規動詞が発生することを期待しているかどうかを尋ねます。 後者については、BERTを不完全な言語オブジェクトの選択ネットワーク上で微調整し、未検証でありながら妥当な動詞/オブジェクトのペアを期待するかどうかを尋ねる。 私たちは、BERTが微調整における新規の単語のわずか1つまたは2つのインスタンスの後に、堅牢な文法的一般化を行うことを発見しました。 言語交替テストでは、モデルは遷移バイアスと一致する挙動を表示することがわかります。数回見られる動詞は直接物を取ることが予想されますが、直接物で見られる動詞は非遷移的には起こらないことが予想されます。', 'ru': 'Предыдущие исследования, исследующие синтаксические способности моделей глубокого обучения, не нацелены на взаимосвязь между силой грамматического обобщения и количеством доказательств, которым подвергается модель во время обучения. Мы решим эту проблему, развернув новую парадигму изучения слов, чтобы проверить способности БЕРТА к изучению двух аспектов английских глаголов: чередования и классов селективных предпочтений. Для первого, мы тонко настраиваем BERT на одном кадре в словесно-измененной паре и спрашиваем, ожидает ли модель, что романный глагол произойдет в ее родственном кадре. Для последнего мы тонко настраиваем BERT на неполную селективную сеть вербальных объектов и спрашиваем, ожидает ли он неатестированных, но правдоподобных пар глагол/объект. Мы обнаружили, что БЕРТ делает надежные грамматические обобщения после одного или двух экземпляров нового слова в тонкой настройке. Для тестов вербального чередования мы обнаружили, что модель отображает поведение, которое согласуется с погрешностью транзитивности: глаголы, увиденные несколько раз, как ожидается, будут принимать прямые объекты, но глаголы, увиденные с прямыми объектами, как ожидается, не будут происходить непереходным образом.', 'hi': 'गहरे सीखने के मॉडल की वाक्यात्मक क्षमताओं की जांच करने वाले पिछले अध्ययनों ने व्याकरणिक सामान्यीकरण की ताकत और प्रशिक्षण के दौरान मॉडल को उजागर करने वाले सबूतों की मात्रा के बीच संबंध को लक्षित नहीं किया है। हम अंग्रेजी क्रियाओं के दो पहलुओं के लिए BERT की कुछ-शॉट सीखने की क्षमताओं का परीक्षण करने के लिए एक उपन्यास शब्द-सीखने के प्रतिमान को तैनात करके इस मुद्दे को संबोधित करते हैं: परिवर्तन और चयन प्राथमिकताओं की कक्षाएं। पूर्व के लिए, हम एक मौखिक-परिवर्तन जोड़ी में एक एकल फ्रेम पर BERT को ठीक करते हैं और पूछते हैं कि क्या मॉडल उपन्यास क्रिया को अपनी बहन फ्रेम में होने की उम्मीद करता है। उत्तरार्द्ध के लिए, हम मौखिक वस्तुओं के एक अधूरे चयन नेटवर्क पर BERT को ठीक करते हैं और पूछते हैं कि क्या यह अनाकर्षित लेकिन प्रशंसनीय क्रिया / वस्तु जोड़े की उम्मीद करता है। हम पाते हैं कि BERT ठीक ट्यूनिंग में एक उपन्यास शब्द के सिर्फ एक या दो उदाहरणों के बाद मजबूत व्याकरणिक सामान्यीकरण बनाता है। मौखिक परिवर्तन परीक्षणों के लिए, हम पाते हैं कि मॉडल व्यवहार को प्रदर्शित करता है जो एक पारगमन पूर्वाग्रह के अनुरूप है: कुछ बार देखी गई क्रियाओं को प्रत्यक्ष वस्तुओं को लेने की उम्मीद की जाती है, लेकिन प्रत्यक्ष वस्तुओं के साथ देखी जाने वाली क्रियाओं को अकर्मक रूप से होने की उम्मीद नहीं है।', 'ga': "Níor dhírigh staidéir roimhe seo a imscrúdaíodh ar chumais chomhréire na múnlaí domhainfhoghlama ar an ngaol idir neart an ghinearálú gramadaí agus an méid fianaise a nochtar an tsamhail le linn na hoiliúna. Tugaimid aghaidh ar an tsaincheist seo trí pharaidím foghlama focal úrnua a úsáid chun cumas foghlama cúpla seat BERT do dhá ghné de na briathra Béarla a thástáil: malartaithe agus aicmí de roghanna roghnaitheacha. Don chéad cheann, déanaimid mionchoigeartú ar BERT ar fhráma amháin i bpéire malartach briathartha agus fiafraíonn muid an bhfuil an tsamhail ag súil go dtarlóidh an briathar úrscéalta ina dheirfiúr. Maidir leis an dara ceann, déanaimid mionchoigeartú ar BERT ar ghréasán roghnaitheach neamhiomlán de rudaí briathartha agus fiafraí de cé acu an bhfuil sé ag súil le péirí briathartha/réada nach bhfuil deimhnithe ach sochreidte. Faighimid amach go ndéanann CRET ginearáluithe láidre gramadaí tar éis ach cás nó dhó d'fhocal úrnua i mionchoigeartú. Maidir leis na trialacha malartacha briathartha, feicimid go léiríonn an tsamhail iompraíocht atá ag teacht le claonadh trasdultach: bítear ag súil go nglacfaidh briathra a fheictear cúpla uair oibiachtaí díreacha, ach ní mheastar go dtarlóidh briathra a fheictear le réada díreacha go hidiraistrithe.", 'ka': 'პირველი სწავლება, რომელიც სინტაქტიური შესაძლებლობას შავის მოდელების სინტაქტიური შესაძლებლობა, არ მიდგენა გრამეტური გენერალიზაციის ძალის და სწავლების რაოდენობა, რომელიც მოდელი განახს ჩვენ ამ პრობლემას გავაკეთებთ, რომელიც პრომენტის სიტყვების სწავლების პარადიგმის გამოყენებით, რომ BERT-ის პატარა სწავლების შესაძლებლობა ანგლისური გერბის ორი აპექტების შესაძლებლობა: შე მხოლოდ, ჩვენ BERT-ს ვერბალური გადაცვლილების ზოგის ერთი ფრამეტში დავკითხავთ თუ არა მოდელი მოდგინდება რომელური ვერბი იქნება მისი სისტერის ფრამეტში. ბოლოდან, ჩვენ BERT-ს ვერბალური ობიექტების უკვე დასრულებული მონიშნული ქსელექტური ქსელექტურაში დავკითხავთ თუ არა დარწმუნდება, მაგრამ შეგვიძლია გადარწმუნებული ჩვენ ვიფიქრობთ, რომ BERT გავაკეთებ ძალიან გრამიკალური გენერალიზაციების შემდეგ ერთი ან ორი პრომენტიური სიტყვის შემდეგ. გვერბალური შეცვლების ტესტისთვის, ჩვენ აღმოჩნეთ, რომ მოდელის მონაცვლების ქცევა, რომელიც შესაძლებელია გადატანიცხოვრებას წარმოდგენებას: მონაცვლებული გვერბები რამდენიმე პარამეტრებით დაიწყება დირე', 'el': 'Προηγούμενες μελέτες που διερευνούν τις συντακτικές ικανότητες των μοντέλων βαθιάς μάθησης δεν έχουν στοχεύσει στη σχέση μεταξύ της δύναμης της γραμματικής γενικοποίησης και του ποσού των αποδεικτικών στοιχείων στην οποία εκτίθεται το μοντέλο κατά τη διάρκεια της εκπαίδευσης. Αντιμετωπίζουμε αυτό το ζήτημα με την ανάπτυξη ενός νέου παραδείγματος εκμάθησης λέξεων για να δοκιμάσουμε τις μαθησιακές ικανότητες του για δύο πτυχές των αγγλικών ρήμων: εναλλαγές και τάξεις επιλεκτικών προτιμήσεων. Για το πρώτο, συντονίζουμε το BERT σε ένα μόνο πλαίσιο σε ένα ζευγάρι λεκτικής εναλλαγής και ρωτάμε αν το μοντέλο αναμένει το νέο ρήμα να εμφανιστεί στο αδελφικό του πλαίσιο. Για το τελευταίο, συντονίζουμε το BERT σε ένα ελλιπές επιλεκτικό δίκτυο λεκτικών αντικειμένων και ρωτάμε αν αναμένει ανεπίσημα αλλά εύλογα ζεύγη ρήματος/αντικειμένου. Ανακαλύπτουμε ότι ο BERT κάνει ισχυρές γραμματικές γενικεύσεις μετά από μία ή δύο περιπτώσεις μιας νέας λέξης στην τελειοποίηση. Για τις δοκιμές λεκτικής εναλλαγής, διαπιστώνουμε ότι το μοντέλο εμφανίζει συμπεριφορά που συνάδει με μια προκατάληψη μεταβίβασης: τα ρήματα που παρατηρούνται λίγες φορές αναμένεται να λάβουν άμεσα αντικείμενα, αλλά τα ρήματα που παρατηρούνται με άμεσα αντικείμενα δεν αναμένεται να εμφανίζονται αδιάλλακτα.', 'hu': 'A mélytanulási modellek szintaktikai képességeit vizsgáló korábbi tanulmányok nem célozták meg a nyelvtani általánosítás erőssége és az edzés során felmerülő bizonyítékok mennyisége közötti kapcsolatot. Ezt a problémát egy új szótanulási paradigma alkalmazásával kezeljük, hogy teszteljük a BERT néhányszoros tanulási képességeit az angol igék két aspektusára: váltakozásokra és a szelekciós preferenciák osztályaira. Az előbbi esetében egy verbális-váltakozási párban finomhangoljuk a BERT-et, és megkérdezzük, vajon a modell elvárja-e, hogy a regény ige a testvér keretében jelenjen meg. Ez utóbbi esetében finomhangoljuk a BERT-t a verbális objektumok hiányos szelekciós hálózatára, és megkérdezzük, hogy nem számít-e ellenőrzött, de valószínű ige/objektum párokra. Úgy találjuk, hogy a BERT erőteljes nyelvtani általánosításokat készít egy-két új szó finomhangolása után. A verbális váltakozási teszteknél azt találtuk, hogy a modell olyan viselkedést mutat, amely összhangban van a tranzitivitás elfogultságával: a néhányszor látott igék várhatóan közvetlen objektumokkal foglalkoznak, de a közvetlen objektumokkal látott igék várhatóan nem jelennek meg intranzív módon.', 'kk': 'Алдыңғы зерттеулері үлкен оқыту үлгілерінің синтактикалық мүмкіндіктерін зерттеу үшін грамматикалық жалпы жалпы күштерінің қатынасын және үлгісінің оқыту кезінде көрсетілген құқықтар қатынас Біз бұл мәселеді жаңа сөздерді оқыту парадигмін жасап, BERT-нің бірнеше шарт оқыту мүмкіндігін тексеру үшін, ағылшын верболардың екі аспекті үшін: альтернативтер мен таңдау параметрлерінің клас Бұрынғы үшін біз BERT-ті вербалды өзгертілген екеуінде бір фреймді жақсы түзеп, моделі романдық вербалдың көзінің фреймінде болуын күтпегенін сұрақ береміз. Соңғы үшін, біз BERT- ті вербалды нысандардың толық таңдау желіне баптап, ол келтірілмеген, бірақ сенімді верб/ нысандардың екеуіне күту үшін сұрақ береміз. Біз БЕРТ тек бір немесе екі жаңа сөздің бір немесе екі жағдайдан кейін грамматикалық жалпы жалпы түрлерді жасайды деп ойлаймыз. Вербалды өзгерту сынақтары үшін, үлгісі өзгертулердің қасиеттерін көрсетеді: бірнеше рет көрінетін вербалдар тізбекті нысандарды алып тастайды, бірақ тізбекті нысандармен көрінетін вербалдар интрансификалық түрде болмайды.', 'it': "Studi precedenti che indagano le capacità sintattiche dei modelli di deep learning non hanno mirato alla relazione tra la forza della generalizzazione grammaticale e la quantità di prove a cui il modello è esposto durante l'allenamento. Affrontiamo questo problema implementando un nuovo paradigma di apprendimento delle parole per testare le capacità di apprendimento poche-shot di BERT per due aspetti dei verbi inglesi: alternanze e classi di preferenze selettive. Per il primo, mettiamo a punto BERT su un singolo fotogramma in una coppia di alternanze verbali e chiediamo se il modello si aspetta che il verbo novello si presenti nella sua cornice gemella. Per quest'ultimo, mettiamo a punto BERT su una rete selettiva incompleta di oggetti verbali e chiediamo se si aspetta coppie verbo/oggetto non confermate ma plausibili. Troviamo che BERT fa robuste generalizzazioni grammaticali dopo solo una o due istanze di una parola nuova in messa a punto. Per i test di alternanza verbale, troviamo che il modello mostra un comportamento coerente con un bias di transizione: i verbi visti poche volte dovrebbero prendere oggetti diretti, ma i verbi visti con oggetti diretti non dovrebbero verificarsi intransivamente.", 'mk': 'Претходните студии кои ги истражуваат синтактичките способности на моделите на длабоко учење не го насочија односот помеѓу силата на граматичката генерализација и количината на докази на кои моделот е изложен за време на обуката. Ние го решаваме ова прашање со распоредување на нов парадигм за учење зборови за тестирање на способностите на Берт за неколку снимки на учење за два аспекти на англиските речници: алтернативи и класи на селективни преференции. За првите, ние финетизираме BERT на една рамка во вербално-алтернативен пар и прашуваме дали моделот очекува нов јазик да се случи во нејзината сестра рамка. За последните, ние финетизираме БЕРТ на нецелосна селекционална мрежа на вербални објекти и прашуваме дали очекува неиспитани, но веројатни парови на гласници/објекти. Најдовме дека БЕРТ прави силни граматични генерализации после само еден или два примери на нов збор во финетизирање. За вербалните алтернативни тестови, откриваме дека моделот покажува однесување кое е во согласност со транзитивната пристрасност: вербалите видени неколку пати се очекува да земат директни објекти, но вербалите видени со директни објекти не се очекува да се случат нечувствително.', 'lt': 'Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training.  Šis klausimas sprendžiamas naudojant naują žodžių mokymosi paradigm ą, kad išbandytume BERT nedidelių gebėjimų mokytis dviem anglų žodžių aspektais: alternatyvas ir atrankinių preferencijų klases. Pirmajai, mes subtinuojame BERT ant vieno rėmo žodinio pakeitimo pora ir klausiame, ar modelis tikisi, kad naujo žodžio įvyks savo seserio rėme. Pastarajam tiksliai pritaikome BERT nekomplektiniam žodinių objektų tinklui ir klausiame, ar jis tikisi nepatvirtintų, bet tikėtinų žodžių ir objektų poros. Mes suprantame, kad BERT atlieka tvirtas gramatines generalizacijas tik po vieno ar dviejų naujų žodžių atvejų tiksliai sureguliuojant. Kalbant apie žodinius pakaitinius bandymus, matome, kad model is rodo elgesį, kuris atitinka pereinamojo laikotarpio sklaidą: tikėtina, kad kelis kartus pastebėti žodžiai paima tiesioginių objektų, tačiau tikimasi, kad su tiesioginiais objektais pastebėti žodžiai atsiranda jautriu būdu.', 'ml': "ആഴത്തെ പഠിക്കുന്ന മോഡലുകളുടെ സിനിട്ടാക്കിക് കഴിവുകള്\u200d പരിശോധിക്കുന്നതിന് മുമ്പുള്ള പഠനത്തിന്റെ സാധ്യതകള്\u200d പരിശോധിക്കുന്നതിന്  We address this issue by deploying a novel word-learning paradigm to test BERT's few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences.  മുമ്പുള്ളതിന് വേണ്ടി നമ്മള്\u200d ബെര്\u200dട്ടിനെ ഒരു ഫ്രെയിമില്\u200d സുന്ദരിയാക്കുന്നു. ഒരു വാര്\u200dബല്\u200d മാറ്റം രണ്ടുപേരില്\u200d നിര്\u200dത്തുന്നു. മോഡ അവസാനത്തേക്ക്, നമ്മള്\u200d ഒരു പൂര്\u200dണ്ണമായ തെരഞ്ഞെടുക്കപ്പെട്ട വെര്\u200dബല്\u200d വസ്തുക്കളുടെ ശേഖരത്തില്\u200d ബെര്\u200dട്ടിനെ സുന്ദരിക്കുന്നു. അത് പര ബെര്\u200dട്ട് ഗ്രാമാറ്റിക്കല്\u200d ജനറലിലേഷന്\u200d ഉണ്ടാക്കുന്നു എന്ന് നമുക്ക് കണ്ടെത്തുന്നു. ഒരോ രണ്ടോ സംഭവം മാത്രമേ ഒരു നോവല്\u200d വ സാധാരണ പരീക്ഷണങ്ങള്\u200dക്ക് വേണ്ടി നാം കണ്ടെത്തുന്നു, മോഡല്\u200d ഒരു ക്രിസ്റ്റിവിറ്റിയില്\u200d പൊരുതുന്ന സ്വഭാവം പ്രദര്\u200dശിപ്പിക്കുന്നു: കുറച്ചു തവണ കണ്ട വാക്കു", 'ms': 'Ujian terdahulu yang menyelidiki kemampuan sintaktik model belajar dalam tidak menargetkan hubungan antara kekuatan generalisasi grammatik dan jumlah bukti yang model terkena semasa latihan. Kami mengatasi isu ini dengan menggunakan paradigm a pembelajaran perkataan yang baru untuk menguji kemampuan pembelajaran beberapa tembakan BERT untuk dua aspek verb Inggeris: alternatif dan kelas keutamaan pemilihan. Untuk yang pertama, kami tune-fine BERT pada satu bingkai dalam pasangan alternatif-verbal dan bertanya sama ada model mengharapkan verb novel berlaku dalam bingkai adiknya. Untuk yang terakhir, kami tune-fine BERT pada rangkaian pemilihan tidak lengkap objek verbal dan bertanya sama ada ia mengharapkan verb/objek pasangan yang tidak disahkan tetapi boleh diterima. Kami mendapati bahawa BERT membuat generalisasi grammatik yang kuat selepas hanya satu atau dua contoh perkataan novel dalam penyesuaian. Untuk ujian alternatif verbal, kami mendapati bahawa model memaparkan perilaku yang konsisten dengan bias transitiviti: verb yang terlihat beberapa kali dijangka mengambil objek langsung, tetapi verb yang terlihat dengan objek langsung tidak dijangka berlaku tanpa sensitif.', 'mt': "Studji preċedenti li investigaw il-ħiliet sintattiċi ta’ mudelli ta’ tagħlim profond ma mmiraw ir-relazzjoni bejn is-saħħa tal-ġeneralizzazzjoni grammatika u l-ammont ta’ evidenza li għaliha l-mudell huwa espost matul it-taħriġ. Aħna nindirizzaw din il-kwistjoni billi nużaw paradigm a ġdida tat-tagħlim tal-kliem biex jittestjaw il-kapaċitajiet ta' tagħlim b'ftit skopijiet tal-BERT għal żewġ aspetti tal-verbi Ingliżi: alternattivi u klassijiet ta' preferenzi selettivi. Għall-ewwel, a ħna fine-tune BERT fuq qafas wieħed f'par ta' alternattiva verbali u jistaqsi jekk il-mudell jistennax il-verb ġdid li jseħħ fil-qafas oħtu tiegħu. Għal dan ta’ l-aħħar, aħna nirranġaw il-BERT fuq netwerk selettiv mhux komplut ta’ oġġetti verbali u nistaqsu jekk jistennewx pari ta’ verb/oġġett mhux ittestjati iżda plawżibbli. Aħna nsibu li BERT jagħmel ġeneralizzazzjonijiet grammatiċi b’saħħithom wara biss każ wieħed jew żewġ każijiet ta’ kelma ġdida fl-irfinar. Għat-testijiet ta’ alternattiva verbali, isibu li l-mudell juri mġiba li hija konsistenti ma’ preġudizzju ta’ tranżitività: verbs li jidhru ftit drabi huma mistennija li jieħdu oġġetti diretti, iżda verbs li jidhru b’oġġetti diretti mhumiex mistennija li jseħħu b’mod intransittiv.", 'mn': 'Өмнөх судалгаагаар гүнзгий суралцах загварын синтактикийн чадварыг судалж байгаа нь грамматикийн ерөнхийлөгчийн чадварын хоорондох харилцаа, загварын суралцах үед харуулсан баталгааны хэмжээний холбоотой байхгүй. Бид энэ асуудлыг БЕРТ-ын хэд хэдэн зургийн суралцах чадварыг Англи хэлний хэлний хоёр талаар шалгахын тулд шинэ үг суралцах парадигм ашиглаж, өөрчлөлт болон сонголтын сонголтуудын хэсэг. Эхний хувьд бид БЕРТ-г үгийн өөрчлөлтийн хоёрын нэг фрэйм дээр сайжруулж, загвар нь дүү дүү дээр шинэ үг болох эсэхийг хүсэж байгааг асууж байна. Сүүлийн үед бид BERT-г хэлсэн объектийн бүтээгдэхүүний сонголтгүй сүлжээнд зохион байгуулж, үүнийг тодорхойгүй гэхдээ итгэлтэй үг/объектийн хоёрыг хүлээн зөвшөөрөх эсэхийг асуудаг. Бид БЕРТ зөвхөн нэг эсвэл хоёр тохиолдолд шинэ үгийн дараа грамматикийн ерөнхийлөгч байдаг. Үгийн өөрчлөлтийн шалгалтуудын тулд бид загвар нь шилжүүлэлтийн харилцааны байдлыг харуулж байна: хэдэн удаа харагдаж байгаа үг нь шууд объектуудыг авах гэж хүлээж байна, гэхдээ шууд объектуудыг харагдаж байгаа үг нь интранзивтын харилцааны байдлыг', 'pl': 'Poprzednie badania badające zdolności składni modeli głębokiego uczenia nie ukierunkowały na związek między siłą uogólnienia gramatycznego a ilością dowodów, na jakie model jest narażony podczas treningu. Rozwiązujemy ten problem poprzez wdrożenie nowego paradygmatu uczenia się słów w celu testowania możliwości uczenia się kilku strzałów BERT dla dwóch aspektów czasowników angielskich: alternatywy i klasy preferencji selekcyjnych. W przypadku pierwszego dostosowujemy BERT na pojedynczej klatce w parze werbalno-alternatywnej i pytamy, czy model oczekuje, że nowy czasownik wystąpi w ramce siostrzanej. W przypadku tych ostatnich dostrajamy BERT na niekompletnej selekcyjnej sieci obiektów werbalnych i pytamy, czy oczekuje on niezbadanych, ale wiarygodnych par czasownika/obiektów. Odkrywamy, że BERT dokonuje solidnych uogólnień gramatycznych już po jednym lub dwóch przypadkach nowego słowa w dostrojeniu. W przypadku testów alternatywności werbalnej okazuje się, że model wyświetla zachowanie zgodne z tendencją przejściową: czasowniki widziane kilka razy mają przyjmować obiekty bezpośrednie, ale czasowniki widziane z obiektami bezpośrednimi nie są oczekiwane, że występują niewrażliwie.', 'no': 'Førre studiar som undersøker syntaktiske kapasiteten for dype læringsmodeller har ikkje målt forholdet mellom styrken på grammatiske generellisering og kva mykje bevis modellen er eksponert ved opplæring. Vi handler dette problemet ved å utføra eit nytt ordlæringsparadigm for å test a BERT sine få bilete læringskapasiteten for to aspektar av engelske verber: alternativ og klasser av utvalde innstillingar. For tidlegare, finn vi BERT på ein enkel ramme i ein verbal-alternativ par og spør om modellen forventar at romanverben skal skje i søsterramma sin. For dei siste, finn vi opp BERT på ein ufullstendig utvalsnettverk av verbaler objekt og spør om det ventar uventa, men uventa verb/objektpar. Vi finn at BERT gjer robust grammatiske generaliseringar etter bare eitt eller to instansar av eit roman ord i fin-tuning. For verbale alternativ-testane, finn vi at modellen viser oppførsel som er konsistent med ein overgangsforstand. Verba som er sett få gonger ventar å ta direkte objekt, men verbane som er sett med direkte objekt er ikkje forventa å skje intransitiv.', 'sr': 'Prethodne studije koje istražuju sintaktične sposobnosti dubokog učenja modela nisu ciljale vezu između snage gramatičke generalizacije i količine dokaza kojima je model izložen tokom obuke. Razgovaramo s tim pitanjem, pokrenući novu paradigmu učenja riječi kako bi testirali BERT-ove mogućnosti za nekoliko snimanja učenja za dva aspekta engleskih verba: izmjene i klase selektivnih preferencija. Za bivše, dobro tuniramo BERT na jednom okviru u parovima verbalnih promjena i pitamo hoće li model očekivati da se novi verb dogodi u svojoj sestri okviru. Za poslednje, dobro određujemo BERT na nepotpunoj selektivnoj mreži verbalnih objekata i pitamo da li očekuje neodređene ali uvjerljive verbalne/objektivne pare. Pronašli smo da BERT pravi jače gramatičke generalizacije nakon samo jednog ili dva slučaja novog reči u finom naslovu. Za testove verbalne promjene, nalazimo se da model pokazuje ponašanje koje odgovara predrasudama prema transiciji: očekuje se da će se verbi viđeni nekoliko puta uzeti direktne objekte, ali se ne očekuju da će se pojaviti verbi viđeni direktnim objektima.', 'ro': 'Studiile anterioare care au investigat capacitățile sintactice ale modelelor de învățare profundă nu au vizat relația dintre puterea generalizării gramaticale și cantitatea de dovezi la care modelul este expus în timpul antrenamentului. Rezolvăm această problemă prin implementarea unei paradigme noi de învățare a cuvintelor pentru a testa capacitățile de învățare puțin-shot ale BERT pentru două aspecte ale verbelor engleze: alternări și clase de preferințe selecționale. Pentru prima, reglăm BERT pe un singur cadru într-o pereche de alternanță verbală și întrebăm dacă modelul se așteaptă ca verbul roman să apară în cadrul surorii sale. Pentru aceasta din urmă, reglăm BERT pe o rețea selecțională incompletă de obiecte verbale și întrebăm dacă se așteaptă perechi verb/obiect neattestate, dar plauzibile. Descoperim că BERT face generalizări gramaticale robuste după doar una sau două cazuri ale unui cuvânt nou în ajustare fină. Pentru testele de alternanță verbală, constatăm că modelul afișează un comportament care este în concordanță cu o părtinire a tranzitivității: verbele văzute de câteva ori sunt de așteptat să ia obiecte directe, dar verbele văzute cu obiecte directe nu sunt de așteptat să apară intransiv.', 'si': 'මුලින් පරීක්ෂණ අභ්\u200dයාස කරනවා ගොඩක් ඉගෙන ගන්න ප්\u200dරමාණයේ සම්බන්ධතාවය සහ ප්\u200dරධානයේ සාක්ෂික සාක්ෂික සාක්ෂික සැලකිල්ලක අපි මේ ප්\u200dරශ්නය සම්බන්ධ කරනවා බෙර්ට්ට්ගේ ප්\u200dරශ්නයක් පරීක්ෂා කරනවා බෙර්ට්ට්ගේ ප්\u200dරශ්නයක් කිහිපයක් ඉන්ග්\u200dරීසි වර්ගයක් දෙ මුලින් වෙනුවෙන්, අපි BERT එක ප්\u200dරමාණයක් වෙනුවෙන් ප්\u200dරමාණයක් කරලා අහන්නේ නැද්ද කියලා. අන්තිමයෙන්, අපි BERT විශ්වාස කරන්න පුළුවන් විශ්වාස සැකසුම් ජාලයෙන් සම්පූර්ණ විශ්වාස කරන්න පුළුවන් වෙන්නේ නැති ව අපිට හොයාගන්න පුළුවන් BERT කියලා සාමාන්\u200dය සාමාන්\u200dය විශ්වාස කරන්න පුළුවන් විතරයි නිකන් එක නැත්තම් දෙකක් ව වාර්ථ වෙනස් පරීක්ෂණාවට, අපි හොයාගන්නවා මොඩල් පෙන්වන්න පුළුවන් වෙන්න පුළුවන් විදිහට සාමාන්\u200dය වෙන්න පුළුවන් විදිහට: වාර්ථාවක්', 'sv': 'Tidigare studier som undersökte djupinlärningsmodellers syntaktiska förmågor har inte inriktat sig på förhållandet mellan styrkan i den grammatiska generaliseringen och mängden bevis som modellen exponeras för under träning. Vi tar itu med detta problem genom att använda ett nytt paradigm för ordinlärning för att testa BERT:s begränsade inlärningsförmåga för två aspekter av engelska verb: alterneringar och klasser av selektiva preferenser. För det första finjusterar vi BERT på en enda bildruta i ett verbalt alterneringspar och frågar om modellen förväntar sig att det nya verbet ska förekomma i sin systerbildruta. För det senare finjusterar vi BERT på ett ofullständigt selektivt nätverk av verbala objekt och frågar om det förväntar sig oattestade men rimliga verb/objekt par. Vi finner att BERT gör robusta grammatiska generaliseringar efter bara ett eller två fall av ett nytt ord i finjustering. För de verbala alterneringstesterna finner vi att modellen visar beteende som är förenligt med en transitivitetsbias: verb som ses få gånger förväntas ta direkta objekt, men verb som ses med direkta objekt förväntas inte förekomma intransivt.', 'so': 'Waxbarashada hore ee baaritaanka awoodda iskuulka hoose ee muusikada waxbarashada, waxyaabaha lagu baaraandegay lama jeedin xiriirka u dhexeeya xoogga dhalashada grammatika iyo cadadda caddeynta tusaaladu ay ka muuqaneyso xilliga waxbarashada. Arrintan ayaannu ku sheekaynaa si aan u imtixaano awoodda waxbarashada ee barbaarinta ee BERT ee qiimo yar ee labada dhinac oo afka ingiriisiga ah: kala duwan iyo fasalka doorashada. Kii hore, waxaynu BERT ku qornaa hal qashin oo keliya, labo kala duwan oo maamul ah, waxaynu waydiinaa in modelku uu rajaynayo qoraalka qoyska ee walaashiis ku soo socota. Marka ugu dambeyso, waxaynu BERT ku qornaa shabakadda aan dhamaan oo la doortay waxyaabaha habaarka ah, waxaynu waydiinnaa in ay rajaynayso in aysan la imtixaamo laakiin aan la filanayn labada qof oo hab leh. Waxaynu heli nahay in BERT uu sameeyo wax lagu soo daabacay grammatical generalis kadib kaliya hal ama laba jeer oo kaliya hadal saxda ah oo ku qoran. Imtixaanka kala duwan ee caadiga ah, waxaynu aragnaa in modelku muujiyaa tababar ku haboon dabeecada ku habboon tababar dheeraad ah: hadallada lagu arko in dhawr jeer la arko ay qaataan wax toos ah, laakiin hadallada lagu arko waxyaabo toos ah looma yaabo in la soo dhici karo.', 'ta': 'முந்தைய ஆய்வுகள் ஆழமான கல்வி மாதிரிகளின் ஒத்திசைப்படுத்தும் துண்டுகளை ஆய்வு செய்து கொண்டிருக்கவில்லை குறிப்பு உருவாக்கத்தின்  பிரெட்டின் சில தோற்றம் கற்றுக் கொள்ளும் தன்மைகளை சோதிக்க இந்த பிரச்சனையை நாம் புதிய வார்த்தை - கற்றுக் கொள்ளும் அளபுருவை வைத்து பி முந்தைய, நாம் ஒரு சட்டத்தில் பெர்டை நன்றாக குறிப்பிடுகிறோம் ஒரு சொல்லை மாற்று ஜோடியில் ஒரு சட்டத்தில் மாதிரி அவரது சகோதரி சட்டத இறுதிக்கு, நாம் முழுமையான தேர்ந்தெடுக்கப்பட்ட பொருட்களின் வலைப்பின்னலில் பெர்ட்டை நன்றாக குறிப்பிடுகிறோம் அது சோதிக் நாம் கண்டுபிடிக்க வேண்டும் என்றால் பெர்ட் ஒரு புதிய வார்த்தையில் ஒரு அல்லது இரண்டு நிகழ்வுகளுக்கு பிறகு ரோப்ட் செ For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.', 'ur': 'پچھلی مطالعہ جو عمیق سیکھنے کی مدلکوں کی سینٹکتیک قابلیت کی تحقیق کرتی ہیں ان کے درمیان گراماتیکی ژنرالیزی کی قوت اور مدلکوں کی مقدار کا ارتباط مقرر نہیں کیا ہے جسے مدلک تعلیم کے وقت کھول دی جاتی ہے. ہم اس مسئلہ کو اس کے بارے میں مشورہ کریں گے کہ BERT کے کم شٹ کی تعلیم کے قابلیت کو امتحان کریں انگلیسی ویروں کے دو قسموں کے لئے: تبدیل اور برگزیدہ ترجیح کے کلاس. پہلے کے لئے ہم BERT کو ایک فرم پر ایک کلام تبدیلی جفت میں ٹھیک ٹھیک ٹھیک ٹھیک ٹھیک کر پوچھتے ہیں کہ مدل کی انتظار کرتی ہے کہ اس کی بہن کے فرم میں اتر جائے آخرین کے لئے ہم BERT کو ویروبال اوسٹوں کی ناپول انتخاب کرنے والی نیٹورک پر خوب تنظیم کرتے ہیں اور پوچھتے ہیں کہ اس کا انتظار نہیں ہے مگر قابل اعتبار/اوسٹ جوڑے. ہم دیکھتے ہیں کہ BERT صرف ایک یا دو مثالیں کے بعد ایک نئی کلمہ کے مطابق مضبوط گراماتیکی ژنرالیز کرتا ہے۔ ویربال تبدیلی آزمائش کے لئے، ہمیں معلوم ہے کہ موڈل کی رفتاری دکھاتا ہے جو تغییرات کی حفاظت کے ساتھ مطابق ہے: چند دفعہ دفعہ دکھائی ہوئی کلمات کی انتظار کی جاتی ہے کہ مستقیم اتحادیوں کے ساتھ دکھائے جاتے ہیں، لیکن مستقیم اتحادیوں کے ساتھ د', 'uz': "Oldingi o'rganish modellarining qobiliyatini o'rganishni o'rganishga o'rganishda, grammatikal generalising imkoniyatini va model taʼminlovchi sohasida ko'rsatilgan foydalanishni anglatmaydi. Biz bu muammolarni birinchi ingliz tilining ikki tomoniga o'rganish imkoniyatlarini tekshirish uchun novel so'z o'rganish paradigmni o'rganish uchun boshqa so'zlarni o'rganish imkoniyatini o'rganish uchun o'ylab turamiz. Boshqa va tanlangan parametrlar For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame.  Keyingi davomida, biz notoʻgʻri tanlangan obʼektlar tarmoqda BERT'ni yaxshi ko'rinamiz va bu ishlab chiqarilmagan narsa va obʼekt qoʻllanmagan narsa ko'rinishini talab qilamiz. Biz o'ylaymiz, BERT faqat bir yoki ikki darajadagi novel so'zlaridan keyin robot grammatikal generalizlarini yaratadi. Oʻzgaruvchining boshqa tugmalar uchun model transitiviy xil bilan bir xil amalni koʻrsatiladi: bir nechta marta koʻrsatilgan so'zlar direktoriya obʼektlarni olib tashlanadi, lekin direkt obʼektlar bilan koʻrsatilgan so'zlar chaqaloqlarni ishlab chiqaradi.", 'vi': 'Những nghiên cứu trước về khả năng đồng pháp của các mô hình học sâu chưa xác định được mối quan hệ giữa sức mạnh tổng thống ngôn ngữ và lượng bằng chứng mà mô hình này đã được phơi bày trong suốt thời gian huấn luyện. Chúng ta giải quyết vấn đề này bằng cách s ử dụng một mô hình mới về học từ để thử khả năng học tập ít ảnh của BERT cho hai khía cạnh của ngôn ngữ tiếng Anh: thay đổi và phân loại lựa chọn lựa chọn. Trước hết, chúng tôi hoàn thiện giao kèo BERT trong một khung duy nhất của một cặp đôi xoay vần và hỏi rằng người mẫu có trông đợi động từ mới diễn ra trong khung em gái của nó không. Với vế sau, chúng tôi tinh chỉnh BERT trên một mạng lựa chọn chưa hoàn chỉnh của các vật thể nói và hỏi xem liệu nó có trông đợi những cặp từ không thân thích nhưng hợp không. Chúng tôi thấy rằng BERT tạo ra chiến lược ngữ pháp mạnh mẽ sau một hoặc hai trường hợp của một từ mới trong việc chỉnh sửa. Đối với các thử nghiệm xoay vần, chúng tôi thấy rằng mô hình hiển thị hành vi phù hợp với khuynh hướng chuyển tiếp: các động từ được nhìn thấy vài lần được dự kiến sẽ lấy các đối tượng trực tiếp, nhưng các động từ được nhìn thấy với các đối tượng trực tiếp không phải là gây ra theo chiều hướng.', 'nl': "Eerdere studies die de syntactische vaardigheden van deep learning modellen onderzoeken, hebben zich niet gericht op de relatie tussen de kracht van de grammaticale generalisatie en de hoeveelheid bewijs waaraan het model tijdens de training wordt blootgesteld. We pakken dit probleem aan door een nieuw woordleerparadigma te implementeren om BERT's weinige leermogelijkheden te testen voor twee aspecten van Engelse werkwoorden: afwisselingen en klassen van selectieve voorkeuren. Voor de eerste verfijnen we BERT op een enkel frame in een verbaal-afwisselingspaar en vragen we of het model verwacht dat het nieuwe werkwoord in zijn zusterframe voorkomt. Voor dit laatste verfijnen we BERT op een onvolledig selectief netwerk van verbale objecten en vragen we of het ongepaste maar plausibele werkwoord/object paren verwacht. We vinden dat BERT robuuste grammaticale generalisaties maakt na slechts één of twee gevallen van een nieuw woord in fine-tuning. Voor de verbale alternatietests vinden we dat het model gedrag weergeeft dat consistent is met een transitiviteitsbias: werkwoorden die weinig keer worden gezien, worden verwacht dat ze directe objecten aannemen, maar werkwoorden die met directe objecten worden gezien, worden niet verwacht dat ze intransief optreden.", 'bg': 'Предишни проучвания, изследващи синтактичните способности на моделите за дълбоко обучение, не са насочени към връзката между силата на граматическото обобщаване и количеството доказателства, на които моделът е изложен по време на обучение. Ние решаваме този въпрос чрез внедряване на нова парадигма за изучаване на думи, за да тестваме възможностите за изучаване на няколко изстрела за два аспекта на английските глаголи: редуване и класове на селекционни предпочитания. За първата, ние фино настройваме върху един кадър в вербално-редуваща двойка и питаме дали моделът очаква новаторският глагол да се появи в сестрата си рамка. За последното, ние фино настройваме БЕРТ върху непълна селекционна мрежа от словесни обекти и питаме дали очаква неоткривани, но правдоподобни двойки глагол/обект. Откриваме, че прави силни граматически обобщения само след един или два случая на нова дума в фина настройка. За тестовете за вербална алтернатива откриваме, че моделът показва поведение, което е в съответствие с преходно отклонение: глаголите, които се виждат няколко пъти, се очаква да приемат директни обекти, но глаголите, които се виждат с директни обекти, не се очаква да се появяват интранзивно.', 'da': "Tidligere undersøgelser, der undersøgte dyb læringsmodellers syntaktiske evner, har ikke målrettet forholdet mellem styrken af den grammatiske generalisering og mængden af beviser, som modellen udsættes for under træning. Vi løser dette problem ved at implementere et nyt ordlæringsparadigme til at teste BERT's få-skud-læringsevner for to aspekter af engelske udsagnsord: veksler og klasser af selektive præferencer. For førstnævnte finjusterer vi BERT på en enkelt ramme i et verbal-vekselpar og spørger, om modellen forventer, at det nye verbum forekommer i sin søsterramme. For sidstnævnte finjusterer vi BERT på et ufuldstændigt selektivt netværk af verbale objekter og spørger, om det forventer uatteste, men plausible verbe/objekt par. Vi finder ud af, at BERT laver robuste grammatiske generaliseringer efter blot et eller to forekomster af et nyt ord i finjustering. For de verbale vekseltests finder vi, at modellen viser adfærd, der er i overensstemmelse med en transitivitetsskævhed: verber set få gange forventes at tage direkte objekter, men verber set med direkte objekter forventes ikke at forekomme intransitivt.", 'hr': 'Prije ispitivanja koje istražuju sintaktične sposobnosti dubokog učenja modela nisu ciljale odnos između snage gramatičke generalizacije i količine dokaza kojima je model izložen tijekom obuke. Razgovaramo s tim pitanjem, pokrenući novu paradigmu učenja riječi kako bi testirali BERT-ove sposobnosti za učenje s malo snimka za dva aspekta engleskih govora: izmjene i klase selektivnih preferencija. Za bivše, dobro sređujemo BERT na jednom okviru u parovima verbalnih promjena i pitamo da li model očekuje da se nova verb a dogodi u svom sestrom okviru. Za sljedeće, dobro određujemo BERT na nepotpunoj selektivnoj mreži verbalnih objekata i pitamo da li očekuje neodređene ali uvjerljive verbalne/objektivne pare. Pronašli smo da BERT čini jače gramatičke generalizacije nakon samo jednog ili dva slučaja novog riječi u finom prilagodbi. Za testove verbalne promjene, nalazimo se da model pokazuje ponašanje koje odgovara predrasudama prema transiciji: očekuje se da će se verbi viđeni nekoliko puta uzimati direktne objekte, ali se očekuje da će se verbi viđeni direktnim objektima dogoditi intransitivno.', 'de': 'Bisherige Studien, die die syntaktischen Fähigkeiten von Deep Learning Modellen untersuchten, haben nicht den Zusammenhang zwischen der Stärke der grammatischen Generalisierung und der Menge an Evidenz untersucht, denen das Modell während des Trainings ausgesetzt ist. Wir adressieren dieses Problem, indem wir ein neuartiges Wort-Lernparadigma implementieren, um BERTs seltene Lernfähigkeiten für zwei Aspekte englischer Verben zu testen: Abwechslung und Klassen von Selektionspräferenzen. Für ersteres stimmen wir BERT auf einem Einzelbild in einem verbal-alternierenden Paar ab und fragen, ob das Modell erwartet, dass das neuartige Verb in seinem Schwesterbild vorkommt. Für letzteres verfeinern wir BERT auf einem unvollständigen Selektionsnetzwerk verbaler Objekte und fragen, ob es ungeprüfte, aber plausible Verb/Objekt Paare erwartet. Wir finden, dass BERT robuste grammatische Verallgemeinerungen nach nur ein oder zwei Instanzen eines neuen Wortes in Feinabstimmung macht. Für die verbalen Alterationstests stellen wir fest, dass das Modell Verhalten zeigt, das mit einem Transitivitätsbias konsistent ist: Wenige Verben werden erwartet, dass sie direkte Objekte annehmen, aber Verben, die mit direkten Objekten gesehen werden, werden nicht erwartet, dass sie intransiv auftreten.', 'id': "Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training.  We address this issue by deploying a novel word-learning paradigm to test BERT's few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences.  Untuk yang pertama, kami memperbaiki BERT pada satu bingkai dalam pasangan alternatif verbal dan bertanya apakah model mengharapkan verb novel terjadi di bingkai adiknya. Untuk yang terakhir, kami memperbaiki BERT pada jaringan seleksi tidak lengkap dari objek verbal dan bertanya apakah itu mengharapkan verb/objek pasangan yang tidak diuji tetapi plausible. Kami menemukan bahwa BERT membuat generalisasi grammatik yang kuat setelah hanya satu atau dua contoh kata novel dalam penyesuaian. Untuk tes alternatif verbal, kami menemukan bahwa model menunjukkan perilaku yang konsisten dengan bias transitivitas: verb yang terlihat beberapa kali diharapkan untuk mengambil objek langsung, tetapi verb yang terlihat dengan objek langsung tidak diharapkan untuk terjadi secara intransitif.", 'ko': '이전에 깊이 있는 학습 모델의 문법 능력에 대한 연구는 문법이 범화되는 강도와 모델이 훈련 과정에서 드러난 증거 수량 간의 관계를 겨냥하지 않았다.이 문제를 해결하기 위해 우리는 새로운 어휘 학습 모델을 채택하여 BERT가 영어 동사의 두 가지 측면에서 소량의 학습 능력을 테스트했다. 그것이 바로 교체와 선호하는 유형을 선택하는 것이다.전자에 대해 우리는 동사 교체 중의 단일 프레임에서 BERT를 미세하게 조정하고 모델이 자매 프레임에 새로운 동사가 나타나기를 기대하는지 물었다.후자에 대해 우리는 동사 목적어의 불완전한 선택 네트워크에서 BERT를 미세하게 조정하고 테스트되지 않았지만 합리적인 동사 / 목적어가 맞기를 기대하는지 물었다.우리는 BERT가 하나의 신조어를 미세하게 조정한 후에 한두 개의 실례만 있으면 강력한 문법 개괄을 할 수 있다는 것을 발견했다.동사교체 테스트에서 우리는 이 모델이 물적 편견과 일치하는 행위를 나타낸다. 보기 드문 동사는 직접 목적어를 사용할 것으로 예상하지만 직접 목적어를 본 동사는 물성에 미치지 못할 것으로 예상된다.', 'fa': 'مطالعات قبلی که در مورد تحقیق قابلیت\u200cهای سنتاکتیک مدل\u200cهای یادگیری عمیق، رابطه بین قوت ژنرال گراماتیک و مقدار مدرک که مدل در زمان آموزش به آن نشان داده می\u200cشود هدف نداشته است. ما این مسئله را با استفاده از پارادیگ یادگیری کلمات جدید برای آزمایش توانایی یادگیری کمی از BERT برای دو قسمت کلمات انگلیسی حل می کنیم: تغییرات و کلمات ترجیح\u200cهای انتخابی. برای اولین، ما BERT را روی یک فرم در جفت تغییر کلمه درست می\u200cکنیم و می\u200cپرسیم آیا مدل انتظار دارد که کلمه رمانی در فرم خواهرش اتفاق بیفته. برای آخرین، ما BERT را در شبکه انتخاب کامل انتخاب کننده ای از اشیاء حرفه\u200cای تغییر می\u200cدهیم و می\u200cپرسیم که آیا انتظار آن جفت\u200cهای حرفه\u200cای غیر قابل اطمینان است ولی قابل اعتماد است. ما متوجه شدیم که BERT بعد از یک یا دو نمونه از یک کلمه رمانی در حالی که درست درست می\u200cکند، گراماتیکی قوی می\u200cسازد. برای آزمایش تغییرات کلمه، ما می\u200cبینیم که مدل رفتار را نمایش می\u200cدهد که با یک تغییرات تغییرات قابل تغییرات است: کلمهای چند بار دیده شده انتظار دارد که اشیاء مستقیم را بگیرند، ولی کلمهای با اشیاء مستقیم را انتظار نمی\u200cدهد که در حال تغییر', 'sw': 'Tafiti zilizopita zinachunguza uwezo wa ushirikiano wa mifano ya elimu ya kina bado haulenga uhusiano kati ya nguvu ya uzalishaji wa grammani na kiasi cha ushahidi ambao mfano huo unaonyesha wakati wa mafunzo. Tunaongelea suala hili kwa kutumia mchanganyiko wa kusoma maneno ya kitabu ili kujaribu uwezo wa kujifunza kwa vifaa vichache vya BERT kwa vipande viwili vya maneno ya Kiingereza: mabadiliko na darasa la chaguo. Kwa mtazamo wa zamani, tunaweka vizuri BERT kwenye mfumo mmoja katika mbili mbadala wa maneno na kuuliza kama mfano anatarajia neno la riwaya litatokea katika mfumo wa dada wake. Kwa mwisho, tunaweka vizuri BERT kwenye mtandao usio kamili wa uchaguzi wa vitu vinavyoendelea na kuuliza kama inatarajia viwili vinavyojaribiwa lakini vinginevyo vizuri. Tunapata kwamba BERT inatengeneza viwanda vya ubora mara baada ya matukio moja au mbili ya neno la riwaya kwa ujuzi mzuri. Kwa majaribio mbadala wa kawaida, tunagundua kuwa model inaonyesha tabia ambayo ina msingi na upendeleo wa mpito: maneno yanayoonekana mara chache yanatarajiwa kuchukua vitu vya moja kwa moja, lakini maneno yanayoonekana kwa vitu vya moja kwa moja hayatatarajiwa kutokea kwa kiasi kikubwa.', 'tr': "횜흫ki 철wrenmeler derin 철wrenme nusgalaryny흫 sintaktik ukyplaryny barlap gramatik jeneralizasyony흫 g체첵s체 we okuw wagtynda nusgalary흫 첵체z체ne seredil첵채n 첵agda첵yny bejermedi. BERT'i흫 birn채챌e ta첵첵arlanan s 철z 철wrenmek paradigmanyny barlamak 체챌in bu meseleyi i흫lis챌e s철zlerini흫 iki aspekti 체챌in barlap 챌yk첵arys: se챌meler we se챌meler t채sleri. 횜흫ki 체챌in, BERT'i verbal-체첵tge힊ik 챌iftde 첵eke bir frame bilen bejerilip sora첵ar캇z we nusgasy흫 gyz dogany흫 챌er채sinde roman verbini흫 bolmasyny흫 gara힊andygyny sora첵ar캇z. So흫ky 체챌in, biz BERT'i verbal zady흫 incomplete netijesinde 챌ykaryp, we muny흫 belli edilmedigini, 첵철ne biljek verb/zady흫 챌iftlerine gara힊an netijesini sora첵arys. BERT'y흫 bir 첵a-da iki 첵agda첵dan so흫ra g체첵챌li gramatik d철redili힊iklerini gowy d체zenlemek 체챌in edip bil첵채ris. Verbal 체첵tge힊im testiler 체챌in, nusga g철rkezili힊im bi첵atlary bilen bir hereket edip g철r체n첵채r: birn채챌e gezek g철r체n첵채n wersi첵alar direkt objekler tutmak 체챌in beklen첵채r, 첵철ne direkt objekler bilen g철r체n첵채n wersi첵alar d철redilmez.", 'af': "Vorige studie wat die sintaktiske kapasiteite van diep leer modele ondersoek het, het nie die verwanting tussen die krag van die grammatiese generalisering en die hoeveelheid van getuienis waarmee die model in die oefening uitgewys word nie. Ons adres hierdie probleem deur 'n nuwe woord-leer paradigme te verwyder om BERT se paar-skoot leer kapasiteite te toets vir twee aspekte van Engelske verbe: alternasies en klasse van seleksionale voorkeure. Vir die vorige, ons fin-tune BERT op 'n enkele raam in 'n verbale-alternasie paar en vra of die model verwag die novel verb in sy suster raam te voorkom. Vir die laaste, ons fin-tuneer BERT op 'n onvolledige seleksionale netwerk van verbale voorwerpe en vra of dit verwag onbevestig maar veroorsaaklike verb/voorwerp paar. Ons vind dat BERT sterk grammatiese generalisering maak na net een of twee voorbeelde van 'n roman woord in fyn-tuning. Vir die verbale alternasie toets, vind ons dat die model vertoon gedrag wat gelyk is met 'n transitiviteit bias: verbe gesien paar maal word verwag om direk objekte te neem, maar verbe gesien met direk objekte word nie verwag om intransitive te voorkom nie.", 'sq': 'Studimet e mëparshme që hetojnë aftësitë sintaktike të modeleve të mësimit të thellë nuk kanë synuar marrëdhëniet midis forcës së gjeneralizimit grammatik dhe sasisë së provave që modeli ekspozohet gjatë trajnimit. Ne e trajtojmë këtë çështje duke vendosur një paradigm ë të re të mësimit të fjalëve për të testuar aftësitë e mësimit të pak-gjuajtur të BERT për dy aspekte të verbeve angleze: alternativa dhe klasa të preferimeve zgjedhore. Për të mëparshmen, ne fine-tune BERT në një frame të vetëm në një çift verbal-alternation dhe pyesin nëse modeli pret verb novel të ndodhë në frame e saj motër. Për të fundit, ne rregullojmë BERT në një rrjet të pakompletuar zgjedhor të objekteve verbale dhe pyesim nëse pret një çift verb/objekt të pakompletuar por të besueshëm. Ne zbulojmë se BERT bën gjeneralizime të forta grammatike pas vetëm një apo dy rasteve të një fjale të re në rregullim. Për testet e ndryshimit verbal, gjejmë se modeli shfaq sjellje që është konsistente me një paragjykim tranzitiv: verbët e parë pak herë pritet të marrin objekte të drejtpërdrejta, por verbët e parë me objekte të drejtpërdrejta nuk pritet të ndodhin pa ndjenjë.', 'am': "የቀድሞው ትምህርት ዓይነቶች የጥልቅ ትምህርት ማድረግ ስልጣናዊ ኃይል እና ሞዴል በተማሩት ጊዜ የግንኙነቱን ግንኙነት አላደረገም፡፡ We address this issue by deploying a novel word-learning paradigm to test BERT's few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences.  ለቀድሞው፣ አንዲት ክፍል BERT በተለይ ሁለት እጥፍ ላይ እናስጠይቃለን፡፡ ለኋለኛይቱ፣ በሙሉ የተመረጠውን መረጃ አካባቢዎች ላይ BERT እናስጠይቃለን፡፡ BERT በ አንድ ወይም ሁለት ምሳሌ በመስጠት የረኀብ ቃላትን መፍጠር የሚያደርግ እንደሆነ እናገኛለን፡፡ በተለይ ምርጫዎች ፈተናዎች፣ ሞዴል በጥቅምነት የሚታወቀውን ድርጊት እንዲያሳየው እናገኛለን፤ የተመለከቱ ቃላት ጥቂት ጊዜ ቀጥተኛ አካሄድ እንዲወስዱ ተስፋ ያደርጋሉ፥ ነገር ግን በአቀናኝ ዕቃዎች የተታዩት ቃላት በአስቸጋሪ መሆኑን አይስፋ።", 'hy': "Անցյալ ուսումնասիրությունները, որոնք ուսումնասիրում են խորը ուսումնասիրության մոդելների սինտակտիկ ունակությունները, չեն նպատակացել գրամատիկական ընդհանրացման ուժի և այն ապացույցների քանակի միջև, որոնց մոդելը ցույց է տալիս ուսուցման ընթացքում: Մենք լուծում ենք այս խնդիրը, օգտագործելով նոր բառեր սովորելու պարադիգմա BER-ի փոքրիկ կրկնակի սովորելու ունակությունները անգլերեն բայերի երկու ասպեկտների համար' ալտերցիաներ և ընտրական նախընտրությունների դասեր: Առաջինների համար մենք բարձրացնում ենք BERT-ը մեկ շրջանակի վրա, և հարցնում ենք, արդյոք մոդելը ակնկալում է, որ նոր բայը տեղի կունենա իր քույր շրջանակում: Վերջին հատվածի համար մենք կազմակերպում ենք BER-ը բառային օբյեկտների ոչ ամբողջական ընտրական ցանցի վրա և հարցնում, արդյոք այն ակնկալում է ոչ ստուգելի, բայց հավատալի բայի և օբյեկտի զույգերի: Մենք հայտնաբերում ենք, որ BER-ը ստեղծում է ուժեղ գրամատիկական ընդհանուր ընդլայնումներ միայն մեկ կամ երկու նոր բառի օրինակների հետո, երբ փորձարկում է: Բառային այլընտրանքային փորձարկումների համար մենք հայտնաբերում ենք, որ մոդելը ցույց է տալիս վարքագիծ, որը համապատասխանում է տրանզինցիվության կողմնականությանը. մի քանի անգամ տեսված բայերը ակնկալում են, որ վերցնում են ուղիղ առարկաներ, բայց ուղիղ առարկաների հետ տեսված բայերը չեն ակնկա", 'az': "Əvvəlki öyrənmək modellərin sintaktik qabiliyyətlərini incitməyən əvvəlki öyrənmək məqsədilə grammatik generalizasiyanın gücünün və modelinin təhsil sırasında göstərilən kanıtlar arasındakı ilişkisini nişan etmədi. Biz bu məs ələyi yeni söz öyrənməsi paradigmi təyin edirik ki, BERT'nin az şanslı öyrənmə qabiliyyətini İngilis sözlərinin iki hissəsi üçün imtahana çəkirik: dəyişiklik və seçici seçimlərin sınıfları. Əvvəlkilərin üçün BERT'u verbal-alternativ çiftində tək bir çerçiftə təmizləyirik və modellərin bacısının çerçisində olmasını gözləyirik mi? Sonrakılar üçün BERT'u verbal objeklərin müəyyən edilməz seçməli şəklə təyin edirik və soruşuruq ki, onun təyin edilməyən, ancaq inanılmaz verb/objek çiftləri gözləyir. Beləliklə, BERT yeni sözlərin bir ya da iki hissəsindən sonra qüvvətli gramatik generalizasyonu yaratdığını görürük. Verbal dəyişiklik sınaqları üçün modellərin hərəkət tərzinə uyğun davranışlarını görürük: bir neçə dəfə gördüyü fərqlər doğru objeklər götürəcəklərinə ümid edilir, amma doğru objeklər ilə görünən fərqlər intransitiv olaraq gəlməyəcəklərinə ümid edilməz.", 'bn': "পূর্ববর্তী গবেষণা গভীর শিক্ষা মডেলের সাথে সিন্ট্যাক্টিক ক্ষমতার তদন্ত করা হয়েছে তারা গ্রামাটিক্যাল জেনারেলেশনের শক্তি এবং প্রশিক্ষণে We address this issue by deploying a novel word-learning paradigm to test BERT's few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences.  প্রাক্তনের জন্য আমরা ভালোভাবে বের্টের একটি ফ্রেমে একটি ভার্বাল-বিকল্প জোড়ায় ভালোভাবে প্রশ্ন করি এবং মডেল প্রত্যাশা করি যে তার বোনের ফ্র পরবর্তীতে আমরা ভালো নির্বাচনের নেটওয়ার্কে বের্টের সুন্দর ভালো নির্বাচন করি এবং জিজ্ঞাসা করি যে এটি অপ্রত্যাশিত কিন্তু ব্যাপারটি আমরা দেখতে পাচ্ছি যে বের্ট গ্রাম্যাটিক্যাল জেনারেলিজেশন বানিয়ে দিয়েছে সুন্দর ভাবে একটি নভেল শব্দের পরেই। সাধারণ বিকল্প পরীক্ষার জন্য আমরা দেখতে পাচ্ছি যে মডেলের আচরণ প্রদর্শন করে যা অতিরিক্ত বিভাগের সাথে একত্রিত: কয়েকবার দেখা হয়েছে শব্দগুলো সরাসরি বস্তু নিয়ে যাবে, কিন্", 'bs': 'Prethodne studije koje istražuju sintaktične sposobnosti dubokog učenja modela nisu ciljale vezu između snage gramatičke generalizacije i količine dokaza kojima se model izložio tijekom obuke. Razgovaramo s tim pitanjem, pokrenući novu paradigmu učenja riječi kako bi testirali BERT-ove mogućnosti za učenje nekoliko snimaka za dva aspekta engleskih verba: izmjene i klase selektivnih preferencija. Za bivše, dobro tuniramo BERT na jednom okviru u parovu verbalne promjene i pitamo da li model očekuje da se nova verb a dogodi u svom sestrom okviru. Za sljedeće, ispravljamo BERT na nepotpunoj selektivnoj mreži verbalnih objekata i pitamo da li očekuje neodređene ali uvjerljive verbalne/objektivne pare. Pronašli smo da BERT pravi jače gramatičke generalizacije nakon samo jednog ili dva slučaja novog riječi u finom naslovu. Za testove verbalne promjene, nalazimo se da model pokazuje ponašanje koje odgovara predrasudama prema transitivnosti: očekuje se da će se verbi viđeni nekoliko puta uzimati direktne objekte, ali se očekuje da će se verbi viđeni direktnim objektima dogoditi intransitivno.', 'ca': "Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training.  Ens ocupem d'aquest tema desplegant un nou paradigm a d'aprenentatge de paraules per provar les capacitats d'aprenentatge pocs fets de BERT en dos aspectes dels verbs anglesos: alternances i classes de preferències selectives. Per a la primera, ajuntem el BERT en un únic marc en un parell d'alternació verbal i preguntem si el model espera que el verb nou surti al seu marc germana. Per a aquest últim, ajuntem BERT en una xarxa selectiva incompleta d'objectes verbals i preguntem si espera parells de verbs/objectes no comprovats però plausibles. Trobem que BERT fa generalitzacions gramàtiques robustes després de només una o dues instances d'una paraula nova en fins ajustes. Per als tests d'alternació verbal, trobem que el model mostra comportament que és coherent amb un bias de transicionalitat: els verbs vists poces vegades s'espera que prenguin objectes directs, però els verbs vists amb objectes directs no s'espera que surtin intransitivament.", 'cs': 'Předchozí studie zkoumající syntaktické schopnosti modelů hlubokého učení se nezaměřily na vztah mezi sílou gramatické zobecnění a množstvím důkazů, kterým je model vystaven během tréninku. Tento problém řešíme nasazením nového paradigmatu učení slov k testování schopností BERT pro dva aspekty anglických slovesů: střídání a třídy selektivních preferencí. V prvním případě jemně ladíme BERT na jednom snímku v verbálně-střídavém páru a ptáme se, zda model očekává, že nové sloveso vyskytne ve svém sesterském rámci. U druhého vyladíme BERT na neúplné selekční síti verbálních objektů a ptáme se, zda očekává neočekávané, ale věrohodné páry slovesa/objektů. Zjišťujeme, že BERT dělá robustní gramatické zobecnění pouze po jednom nebo dvou instancích nového slova v jemném ladění. U testů verbální alternativy zjišťujeme, že model zobrazuje chování, které je v souladu s přechodným biasem: slovesa, která jsou několikrát viděná, se očekává, že berou přímé objekty, ale slovesa viděná s přímými objekty se neočekává, že se vyskytují intracitivně.', 'et': 'Varasemad uuringud sügavõppe mudelite süntaktiliste võimete uurimiseks ei ole suunanud seost grammatilise üldistamise tugevuse ja tõendite hulga vahel, millele mudel treeningu ajal kokku puutub. Selle probleemiga tegeleme uudse sõnaõppe paradigma kasutuselevõtmisega, et testida BERTi vähese võimalusega õppimise võimalusi inglise keele tegusõnade kahes aspektis: vaheldumisi ja valikuliste eelistuste klasse. Esimese puhul häälestame BERT-i ühele kaadrile verbaalse vaheldumise paaris ja küsime, kas mudel eeldab uudse tegusõna tekkimist oma sõsarraamis. Viimase puhul häälestame BERT-i mittetäielikule selektiivsele verbaalsete objektide võrgustikule ja küsime, kas see eeldab proovimata, kuid usutavaid verbi/objekti paare. Leiame, et BERT teeb tugevaid grammatilisi üldistusi pärast vaid ühte või kahte korda uudsest sõnast peenhäälestuses. Suulise vahelduvuse testide puhul leiame, et mudel näitab käitumist, mis on kooskõlas transitiivsusega: vähe kordi nähtud tegusõnad peavad võtma otseseid objekte, kuid otsesete objektidega nähtud tegusõnad ei pea ilmnema intransiivselt.', 'fi': 'Aiemmat syväoppimismallien syntaktisia kykyjä tutkivat tutkimukset eivät ole kohdistaneet kieliopillisen yleistymisen vahvuuden ja sen näyttömäärän suhdetta, jolle malli altistuu harjoittelun aikana. Käsittelemme tätä ongelmaa ottamalla käyttöön uuden sanaoppimisen paradigman testataksemme BERT:n vähäpätöisiä oppimismahdollisuuksia englannin verbien kahdessa osatekijässä: vuorotteluissa ja valikoivien mieltymysten luokissa. Ensimmäisessä tarkennamme BERT:tä sanallisen ja vuorotteluparin yksittäiselle kehykselle ja kysymme, odottaako malli uuden verbin esiintyvän sisarkehyksessään. Jälkimmäistä varten hienosäädämme BERT:tä epätäydellisellä sanallisten objektien valikoivalla verkostolla ja kysymme, odottaako se testaamattomia mutta uskottavia verbi/objektipareja. Huomaamme, että BERT tekee vankkoja kieliopillisia yleistyksiä vain yhden tai kahden tapauksen jälkeen uuden sanan hienosäätöä. Verbaalisia vuorottelutestejä varten havaitsemme, että malli näyttää käyttäytymisen, joka on johdonmukainen transitiivisuuden kanssa: muutamaan kertaan nähtyjen verbien odotetaan ottavan suoria objekteja, mutta suorien objektien kanssa nähtyjen verbien ei odoteta esiintyvän intransitiivisesti.', 'sk': 'Prejšnje študije, ki raziskujejo sintaktične sposobnosti modelov globokega učenja, niso usmerjale povezave med močjo slovnične generalizacije in količino dokazov, ki jim je model izpostavljen med vadbo. To vprašanje rešujemo z uvajanjem nove paradigme učenja besed, da preizkusimo BERT-ove učne zmogljivosti za dva vidika angleških glagolov: izmenjave in razrede selekcijskih preferenc. Pri prvem natančno nastavimo BERT na enem okvirju v verbalno-alternativnem paru in vprašamo, ali model pričakuje, da se bo novi glagol pojavil v sestrskem okvirju. Za slednje natančno nastavimo BERT na nepopolni selekcijski mreži verbalnih objektov in se vprašamo, ali pričakuje nepreizkušene, vendar verjetne pare glagolov/objektov. Ugotavljamo, da BERT naredi robustne slovnične posplošitve po samo enem ali dveh primerih nove besede v finem nastavitvi. Pri testih verbalne izmeničnosti smo ugotovili, da model prikazuje vedenje, ki je skladno s tranzitivno pristranskostjo: glagoli, ki jih vidimo nekajkrat, naj bi vzeli neposredne predmete, glagoli, ki jih vidimo z neposrednimi predmeti, pa se ne pričakujejo, da se bodo pojavili intračutno.', 'ha': "Haƙĩƙa, ƙidãya masu jarraba abincin masu iya santactic da misãlai masu ƙari na karanta, ba su kula da mazaunin tsakanin ƙarfin mai dangantar littafin grammaci da kuma ma'aunin shaidar da ake samu da shi a lokacin da za'a yi amfani da shi. Munã jãyayya wannan masu al'amarin da za'a sami wani paradigm da aka sanar da yanzu zuwa a jarraba abincin da masu ƙaranci na BERT masu karatun karatun biyu na maganar Ingiriya: zaɓallin da daraja na zaɓen zaɓen. Ga na farko, za mu yi amfani da BERT a kan firam guda, cikin shirin wata nau'i biyu, ko kuma mu tambayi idan misali na ƙayyade maganar nan zai iya ƙara cikin firam ɗin 'yar'uwarsa. Gani da na ƙarshe, za'a sami BERT a kan wata shirin cire-ƙunsa na abun abubuwa ba'a iya cikakken ba kuma Mu tambaya ko, za'a yi amfani da idan ba za'a sami ba, kuma amma nau'in abun da bã za'a yi ba. Ana gane cewa BERT na samar da matsayin grammatisk bakin wani misali guda ko biyu na maganar nowaya cikin mai kyau. Ji da jarrabar shirin ayuka, za'a gane cewa misalin ya nuna aikin da ke daidai da kibiyoyi masu shige: An yiwu ƙayyade abubuwa waɗanda aka gan su sau kaɗan za'a karɓi abubuwa masu madaidaici, kuma amma, ba za'a ƙayyade maganar da aka gane da abubuwa da abubuwa masu shirya za'a fito a lokacin.", 'jv': 'Genjer-Genjer sing isih perbudhakan langgar sampeyan sistem sing nguasai winih Awak dhéwé jagad kuwi nggawe ngubah perbudhakan kelas-kelas perangkat sabanjuré nggawe barang nggawe barang nggawe barang BERT kuwi wis kelas-kelas perangkat sing apik dhéwé versi Inggris: turuné karo calasé sing dibenalke tualanggar. Saiki banget, kita diwatuh "BERT" sing sampeyan nganggo perbudhakan kelas-Alternate Kernel Awak dhéwé ngerti BERT kuwi nggawe barang kelas ramatik dhéwé kayané sampeyan perusahaan winih sing narihan winih. Genjer perbudhakan langkung pawar urip dadi, kita mulai model lagi tau ngomong nik sekondirno karo bias a tarjamahan: verb sing ngebekend to dadi object seneng dadi, pero cekelan verb sing ngebekend to oleh nesaturan kelas', 'he': 'מחקרים קודמים שחקירים את היכולות הסינטקטיות של דוגמני למידה עמוקה לא מטרידים את מערכת היחסים בין כוח הגנרליזציה הגרמטית לבין כמות הראיות שהדוגמן נחשף אליה במהלך האימונים. אנו מתמודדים עם הנושא הזה על ידי שימוש פרדיגמה חדשה ללמוד מילים כדי לבדוק את יכולות הלימוד המעט של BERT לשני היבטים של verbs אנגליים: חליפות וכיתות של עדיפות בוחריות. למען הקודם, אנו מתאימים את ברט על מסגרת אחת בזוג חלופי מילים ושאלו אם המודל מצפה לחבר הרומני יקרה במסגרת אחותו. עבור האחרון, אנחנו מתאימים את BERT על רשת בוחרת בלתי מלאה של אובייקטים מילוליים ושאלו אם הוא מצפה לחבר בלתי מאושר אבל אמין זוגות אובייקטים. אנו מוצאים שברט עושה גנרליזציות גרמטיות חזקות אחרי רק מקרה אחד או שניים של מילה חדשה בהתאמה. עבור בדיקות החלפת מילוליות, אנו מוצאים שהמודל מציג התנהגות שמתאימה לציון העברות: פעמים נראות פעמים מסתכלות על פעמים לקחת אובייקטים ישירים, אך פעמים נראים עם אובייקטים ישירים לא צפויים להתרחש בלתי רגישים.', 'bo': 'སྔོན་གྱི་བརྗོད་ནས་དབྱེ་བ་དག་གི་སྐྱེས་འབྲེལ་བ་དག་གི་མཐུན་རྐྱེན་ཚད་ལྡན་བྱས་པ་དེ་གིས་བྱ་ཚིག་དང་ལས་འཆར་བརྩོན་བྱས་ཆོས་ཉིད་དེ་ ང་ཚོས་དུས་འདི་ལྟ་བུ་ཞིག སྔོན་པ་དེ་ལ། ང་ཚོས་བྱ་ཚིག་གི་ཆོས་ཉིད་ཅིག་གི་ནང་གི་སྒྲ་ཚིག་གཅིག་གི་ནང་གི་རྣམ་པ་ལྟ་བུ་བསམ་དགོས་མིན མཐའ་མཇུག་གི་དོན་ལ་ང་ཚོས་BERT སྒྲིག་འགོད་མེད་པའི་བྱ་ཚིག་དངོས་པོ་ཚུ་གི་དྲ་རྒྱ་སྒྲིག་འཛུགས་བྱེད་ཀྱི་ཡོད་པ་མ་རེ་འདུག BERT ཡིས་བརྗོད་རང་ཉིད་ཀྱིས་གསར་གཏོད་ཀྱི་གནད་དོན་གཅིག་ཡང་ན་གཉིས་ཀྱི་རྗེས་སུ་གཏོང་ཐུབ་པའི་སྐད་ཆ་གསལ་གཏོང་ར For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to happen intransitively.'}
{'en': 'Defining Explanation in an AI Context AI  Context', 'ar': 'تعريف الشرح في سياق الذكاء الاصطناعي', 'es': 'Definición de la explicación en un contexto de IA', 'fr': "Définition de l'explication dans un contexte d'IA", 'pt': 'Definindo a explicação em um contexto de IA', 'ja': 'AIコンテキストでの説明の定義', 'zh': '定义 AI 上下文中', 'ru': 'Определение объяснения в контексте ИИ', 'hi': 'एक AI संदर्भ में स्पष्टीकरण को परिभाषित करना', 'ga': 'Míniú a Shainmhíniú i gComhthéacs AI', 'el': 'Ορισμός επεξήγησης σε ένα πλαίσιο τεχνητής νοημοσύνης', 'hu': 'A magyarázat meghatározása AI kontextusban', 'ka': 'Name', 'lt': 'Paaiškinimas AI kontekste', 'kk': 'AI контекстінің түсініктемесін анықтау', 'it': 'Definizione della spiegazione in un contesto IA', 'mk': 'Дефинирање на објаснување во контекст на AI', 'ms': 'Mengefinisikan Penjelasan dalam Konteks AI', 'mt': 'Definizzjoni ta’ Spjegazzjoni f’Kuntest tal-AI', 'ml': 'AI കൂട്ടത്തില്\u200d എക്സ്പ്ലാനേഷന്\u200d നിര്\u200dണ്ണയിക്കുന്നു', 'mn': 'Тодорхойлолтыг AI контекст тодорхойлох', 'pl': 'Definiowanie wyjaśnienia w kontekście AI', 'no': 'Definerer utklaring i ein AI- kontekst', 'ro': 'Definirea explicației într-un context AI', 'sr': 'Definiranje objašnjenja u kontekstu AI-a', 'so': 'Tilmaamaha qalabka AI', 'sv': 'Definiera förklaring i ett AI-sammanhang', 'si': 'Name', 'ta': 'AI உள்ளடக்கத்தில் வெளியீட்டை வரையறுக்கிறது', 'ur': 'AI کنٹکسٹ میں واضح کی تعریف کرتی ہے', 'uz': 'Defining Explanation in an AI Context', 'vi': 'Xác định giải thích trong tình hình AI', 'bg': 'Определяне на обяснение в контекст на изкуствения интелект', 'nl': 'Uitleg definiëren in een AI-context', 'hr': 'Definirajuće objašnjenje u kontekstu AI-a', 'da': 'Definition af forklaring i en AI-sammenhæng', 'de': 'Definition der Erklärung in einem KI-Kontext', 'ko': '인공지능 환경에서 정의 해석', 'id': 'Mengefinisikan Penjelasan dalam Konteks AI', 'fa': 'تعریف توضیح در یک محیط AI', 'sw': 'Kuelezea Tamko katika Kizunguko cha AI', 'tr': 'AI Kontekstinde düşündirişi', 'sq': 'Përcaktimi i shpjegimit në një kontekst AI', 'af': "Verduideling in 'n AI Konteks", 'am': "ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s", 'hy': 'ԱԲ կոնտեքստում բացատրություն սահմանելը', 'az': 'AI m톛lumat캼nda ayd캼nl캼q t톛yin edir', 'bn': 'AI বিষয়বস্তুতে এক্সপ্লেনেশন নির্ধারণ করুন', 'bs': 'Definirajuće objašnjenje u kontekstu AI-a', 'ca': "Definir explicació en un context de l'AI", 'cs': 'Definice vysvětlení v kontextu AI', 'et': 'Selgituse määratlemine tehisintellekti kontekstis', 'fi': 'Selityksen määrittäminen tekoälyn kontekstissa', 'jv': 'politenessoffpolite"), and when there is a change ("assertive', 'he': 'הגדרת הסבר בקשר AI', 'sk': 'Opredelitev pojasnila v kontekstu umetne inteligence', 'ha': 'Define Exploding context menu item', 'bo': 'AI་ཡུལ་ཁོར་ཡུག་ནང་དུ་གསལ་བཤད་བྱེད་བཞིན་པ'}
{'en': 'With the increase in the use of  AI systems , a need for explanation systems arises. Building an  explanation system  requires a definition of  explanation . However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as  psychology ,  philosophy , and  cognitive sciences . We study multiple perspectives and aspects of explainability of recommendations or predictions made by  AI systems , and provide a generic definition of  explanation . The proposed  definition  is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.', 'pt': 'Com o aumento do uso de sistemas de IA, surge a necessidade de sistemas de explicação. Construir um sistema de explicação requer uma definição de explicação. No entanto, o termo explicação em linguagem natural é difícil de definir formalmente, pois inclui múltiplas perspectivas de diferentes domínios, como psicologia, filosofia e ciências cognitivas. Estudamos múltiplas perspectivas e aspectos de explicabilidade de recomendações ou previsões feitas por sistemas de IA e fornecemos uma definição genérica de explicação. A definição proposta é ambiciosa e difícil de aplicar. Com a intenção de preencher a lacuna entre teoria e aplicação, também propomos uma possível arquitetura de um sistema de explicação automatizado baseado em nossa definição de explicação.', 'ar': 'مع زيادة استخدام أنظمة الذكاء الاصطناعي ، تنشأ الحاجة إلى أنظمة التفسير. يتطلب بناء نظام التفسير تعريفًا للتفسير. ومع ذلك ، يصعب تعريف تفسير مصطلح اللغة الطبيعية بشكل رسمي لأنه يتضمن وجهات نظر متعددة من مجالات مختلفة مثل علم النفس والفلسفة والعلوم المعرفية. ندرس وجهات نظر وجوانب متعددة لإمكانية شرح التوصيات أو التنبؤات التي تقدمها أنظمة الذكاء الاصطناعي ، ونقدم تعريفًا عامًا للتفسير. التعريف المقترح طموح وصعب التطبيق. بهدف سد الفجوة بين النظرية والتطبيق ، نقترح أيضًا بنية محتملة لنظام تفسير آلي بناءً على تعريفنا للتفسير.', 'es': 'Con el aumento del uso de sistemas de IA, surge la necesidad de sistemas de explicación. La creación de un sistema de explicación requiere una definición de explicación. Sin embargo, el término explicación en lenguaje natural es difícil de definir formalmente, ya que incluye múltiples perspectivas de diferentes dominios, como la psicología, la filosofía y las ciencias cognitivas. Estudiamos múltiples perspectivas y aspectos de la explicabilidad de las recomendaciones o predicciones realizadas por los sistemas de IA y proporcionamos una definición genérica de explicación. La definición propuesta es ambiciosa y difícil de aplicar. Con la intención de cerrar la brecha entre la teoría y la aplicación, también proponemos una posible arquitectura de un sistema de explicación automatizado basado en nuestra definición de explicación.', 'fr': "Avec l'augmentation de l'utilisation des systèmes d'IA, un besoin de systèmes d'explication apparaît. La création d'un système d'explication nécessite une définition de l'explication. Cependant, l'explication du terme en langage naturel est difficile à définir formellement car elle inclut de multiples perspectives issues de différents domaines tels que la psychologie, la philosophie et les sciences cognitives. Nous étudions de multiples perspectives et aspects de l'explicabilité des recommandations ou des prévisions faites par les systèmes d'IA, et fournissons une définition générique de l'explication. La définition proposée est ambitieuse et difficile à appliquer. Dans le but de combler le fossé entre la théorie et l'application, nous proposons également une architecture possible d'un système d'explication automatisé basé sur notre définition de l'explication.", 'ja': 'AIシステムの利用の増加に伴い、説明システムの必要性が生じる。説明システムを構築するには、説明の定義が必要です。しかし、自然言語の用語説明は、心理学、哲学、認知科学などの異なる領域からの複数の視点を含むため、形式的に定義することは困難である。AIシステムによる推奨や予測の説明可能性に関する複数の視点と側面を研究し、説明の一般的な定義を提供します。提案された定義は野心的であり、適用するのは困難です。理論と応用のギャップを埋めることを意図して、説明の定義に基づいた自動説明システムのアーキテクチャも提案します。', 'zh': '随人工智能之用增,而说之所求见矣。 构释统须说定义。 然自然语言术语说难正,盖出于异域之论,如心理学,哲学认知科学也。 臣等考人工智能统言占多端可解释性,并供解释通用定义。 议定义雄心勃勃,难用。 以弥合之理,与应用之间,以吾等定义为架构。', 'hi': 'एआई प्रणालियों के उपयोग में वृद्धि के साथ, स्पष्टीकरण प्रणालियों की आवश्यकता उत्पन्न होती है। एक स्पष्टीकरण प्रणाली के निर्माण के लिए स्पष्टीकरण की परिभाषा की आवश्यकता होती है। हालांकि, प्राकृतिक भाषा शब्द स्पष्टीकरण को औपचारिक रूप से परिभाषित करना मुश्किल है क्योंकि इसमें मनोविज्ञान, दर्शन और संज्ञानात्मक विज्ञान जैसे विभिन्न डोमेन से कई दृष्टिकोण शामिल हैं। हम एआई सिस्टम द्वारा की गई सिफारिशों या भविष्यवाणियों की व्याख्या के कई दृष्टिकोणों और पहलुओं का अध्ययन करते हैं, और स्पष्टीकरण की एक सामान्य परिभाषा प्रदान करते हैं। प्रस्तावित परिभाषा महत्वाकांक्षी है और लागू करने के लिए चुनौतीपूर्ण है। सिद्धांत और अनुप्रयोग के बीच की खाई को पाटने के इरादे से, हम स्पष्टीकरण की हमारी परिभाषा के आधार पर एक स्वचालित स्पष्टीकरण प्रणाली की संभावित वास्तुकला का भी प्रस्ताव करते हैं।', 'ru': 'С ростом использования систем ИИ возникает потребность в системах объяснения. Построение системы объяснения требует определения объяснения. Тем не менее, объяснение термина естественного языка трудно определить формально, поскольку он включает в себя несколько перспектив из различных областей, таких как психология, философия и когнитивные науки. Мы изучаем несколько перспектив и аспектов объяснимости рекомендаций или прогнозов, сделанных системами ИИ, и предоставляем общее определение объяснения. Предлагаемое определение является амбициозным и его применение сопряжено с трудностями. С целью преодоления разрыва между теорией и применением мы также предлагаем возможную архитектуру автоматизированной системы объяснения, основанную на нашем определении объяснения.', 'ga': 'Leis an m챕ad첬 ar 첬s찼id na gc처ras AI, t찼 g찼 le c처rais m챠nithe. Teasta챠onn sainmh챠ni첬 ar mh챠ni첬 chun c처ras m챠nithe a th처g찼il. Mar sin f챕in, is deacair an m챠ni첬 ar an t챕arma teanga n찼d첬rtha a shaini첬 go foirmi첬il mar go n-찼ir챠tear ann peirspict챠ochta챠 iolracha 처 r챕ims챠 챕ags첬la cos첬il le s챠ceola챠ocht, feals첬nacht, agus eola챠ochta챠 cogna챠ocha. D챕anaimid staid챕ar ar pheirspict챠ochta챠 iolracha agus ar ghn챕ithe inmh챠nithe molta챠 n처 tuar a dh챕anann c처rais AI, agus sol찼thra챠mid sainmh챠ni첬 cine찼lach ar mh챠ni첬. T찼 an sainmh챠ni첬 at찼 beartaithe uaillmhianach agus d첬shl찼nach a chur i bhfeidhm. Agus 챕 ar intinn againn an bhearna idir teoiric agus cur i bhfeidhm a l챠onadh, molaimid freisin ailtireacht fh챕ideartha de ch처ras m챠ni첬ch찼in uathoibrithe bunaithe ar 찼r sainmh챠ni첬 ar mh챠ni첬.', 'ka': 'AI სისტემების გამოყენებაზე გაზრდება, სისტემების გამოყენება იქნება უნდა გაახსნა სისტემები. განახსნა სისტემის შექმნა უნდა განახსნა განახსნა. მაგრამ, ბუნებრივი ენის ტერმინი განახსნა ძალიან რთულია განახსენოთ ფორმალურად, რადგან ის აქვს მრავალ პეროვნტიკები განსხვავებული დიომენეებიდან, როგორც ფსიკოლოგია,  ჩვენ ვისწავლობთ მრავალ პერსპექტიკები და აპექტიკები, რომლებიც AI სისტემებით გავაკეთებულია პროცექტიკების ან პროცექტიკების განსაზღვრებას და გავა პროგრამის განსაზღვრება არის ამბიციო და გამოსაყენებელი. თეორია და პროგრამის შორის განსხვავებას გადავიწყოთ, ჩვენ შეგვიძლია ავტომატიური განსხვავების სისტემის შესაძლებელი აქტიქტიქტურა, რომელიც ჩვენი განსხვავების განსაზღვრებაზე და', 'hu': 'A mesterséges intelligencia-rendszerek használatának növekedésével magyarázat-rendszerekre van szükség. Egy magyarázatrendszer kiépítéséhez a magyarázat definíciója szükséges. A természetes nyelvi kifejezés magyarázatát azonban nehéz formálisan meghatározni, mivel több perspektívát tartalmaz különböző területeken, mint a pszichológia, a filozófia és a kognitív tudományok. Az AI rendszerek által tett ajánlások vagy előrejelzések megmagyarázhatóságának számos perspektíváját és aspektusát tanulmányozzuk, és általános magyarázatot adunk. A javasolt meghatározás ambiciózus és kihívást jelent. Annak érdekében, hogy áthidaljuk az elmélet és az alkalmazás közötti szakadékot, javasoljuk egy automatizált magyarázatrendszer lehetséges architektúráját is, amely a magyarázat definícióján alapul.', 'kk': 'AI жүйелерін қолдану үшін түсініктеме жүйелерінің қажеті болады. Түсініктеме жүйесін құру түсініктеменің анықтамасы керек. Бірақ табиғи тіл термині психология, философия және конифициялық ғылымдардың бірнеше түсініктерін анықтау қиын болады. Біз AI жүйелерінде жасалған рекомендациялардың немесе таңдаудың бірнеше түсініктемесін және түсініктемесін зерттеп, жалпы түсініктемесін анықтап береміз. Келтірілген анықтамасы амбициялық және қолдану үшін қиын. Теория мен қолданбалардың арасындағы аралығын көшірмелеу мақсатымен, біз сондай-ақ автоматты түсініктеме жүйесіне негізделген түсініктеме анықтамасына негізделген архитектураны таңдаймыз.', 'el': 'Με την αύξηση της χρήσης συστημάτων τεχνητής νοημοσύνης, δημιουργείται ανάγκη για συστήματα επεξήγησης. Η οικοδόμηση ενός συστήματος επεξήγησης απαιτεί έναν ορισμό της επεξήγησης. Ωστόσο, η εξήγηση του όρου φυσικής γλώσσας είναι δύσκολο να οριστεί επίσημα καθώς περιλαμβάνει πολλαπλές προοπτικές από διαφορετικούς τομείς όπως η ψυχολογία, η φιλοσοφία και οι γνωστικές επιστήμες. Μελετάμε πολλαπλές προοπτικές και πτυχές της επεξήγησης συστάσεων ή προβλέψεων που γίνονται από συστήματα τεχνητής νοημοσύνης και παρέχουμε έναν γενικό ορισμό της επεξήγησης. Ο προτεινόμενος ορισμός είναι φιλόδοξος και δύσκολο να εφαρμοστεί. Με την πρόθεση να γεφυρώσουμε το χάσμα μεταξύ θεωρίας και εφαρμογής, προτείνουμε επίσης μια πιθανή αρχιτεκτονική ενός αυτοματοποιημένου συστήματος επεξήγησης βασισμένου στον ορισμό μας της επεξήγησης.', 'lt': 'Padidėjus AI sistemų naudojimui kyla poreikis aiškinimo sistemoms. Paaiškinimo sistemos kūrimui reikalingas paaiškinimo apibrėžimas. Tačiau natūralaus kalbos termino paaiškinimą sunku formaliai apibrėžti, nes jame pateikiama įvairių sričių, pavyzdžiui, psichologijos, filosofijos ir kognityvinių mokslų, perspektyva. Tiriame įvairias AI sistemų rekomendacijų ar prognozių paaiškinimo perspektyvas ir aspektus ir pateikiame bendrą paaiškinimo apibrėžimą. Siūloma apibrėžtis yra plataus užmojo ir sudėtinga. Siekdami užkirsti kelią atotrūkiui tarp teorijos ir taikymo, taip pat siūlome galimą automatizuotos paaiškinimo sistemos struktūrą, pagrįstą mūsų paaiškinimo apibrėžimu.', 'it': "Con l'aumento dell'uso di sistemi AI, sorge la necessità di sistemi di spiegazione. Costruire un sistema di spiegazione richiede una definizione di spiegazione. Tuttavia, la spiegazione del termine del linguaggio naturale è difficile da definire formalmente in quanto include molteplici prospettive da diversi domini come psicologia, filosofia e scienze cognitive. Studiamo molteplici prospettive e aspetti di spiegabilità delle raccomandazioni o previsioni fatte dai sistemi AI e forniamo una definizione generica di spiegazione. La definizione proposta è ambiziosa e difficile da applicare. Con l'intenzione di colmare il divario tra teoria e applicazione, proponiamo anche una possibile architettura di un sistema di spiegazione automatizzato basato sulla nostra definizione di spiegazione.", 'ms': 'With the increase in the use of AI systems, a need for explanation systems arises.  Bina sistem penjelasan memerlukan definisi penjelasan. Namun, penjelasan bahasa semulajadi sukar untuk ditakrif secara rasmi kerana ia termasuk perspektif berbilang dari domain yang berbeza seperti psikologi, filosofi, dan sains kognitif. Kami mempelajari perspektif berbilang dan aspek penjelasan rekomendasi atau ramalan yang dibuat oleh sistem AI, dan menyediakan definisi generik penjelasan. Definisi yang diusulkan adalah ambisius dan mencabar untuk dilaksanakan. Dengan niat untuk memecahkan ruang antara teori dan aplikasi, kita juga melamar arkitektur kemungkinan sistem penjelasan automatik berdasarkan definisi penjelasan kita.', 'mk': 'Со зголемувањето на употребата на системите на AI, се појавува потреба од системи за објаснување. Изградбата на систем за објаснување бара дефиниција на објаснување. Сепак, природното објаснување на јазикот е тешко да се дефинира формално бидејќи вклучува повеќе перспективи од различни области како што се психологијата, филозофијата и когнитивните науки. We study multiple perspectives and aspects of explainability of recommendations or predictions made by AI systems, and provide a generic definition of explanation.  Предложената дефиниција е амбициозна и предизвикувачка за апликација. Со намера да го преминеме јазот помеѓу теоријата и апликацијата, ние, исто така, предложуваме евентуална архитектура на автоматизиран систем на објаснување базиран на нашата дефиниција на објаснување.', 'mt': 'Biż-żieda fl-użu tas-sistemi AI, jinqala’ ħtieġa għal sistemi ta’ spjegazzjoni. Il-bini ta’ sistema ta’ spjegazzjoni jeħtieġ definizzjoni ta’ spjegazzjoni. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as psychology, philosophy, and cognitive sciences.  Aħna nistudjaw diversi perspettivi u aspetti ta’ spjegabbiltà tar-rakkomandazzjonijiet jew tbassir magħmula mis-sistemi AI, u nipprovdu definizzjoni ġenerika ta’ spjegazzjoni. Id-definizzjoni proposta hija ambizzjuża u ta’ sfida biex tapplika. Bil-ħsieb li nilqgħu d-distakk bejn it-teorija u l-applikazzjoni, nipproponu wkoll arkitettura possibbli ta’ sistema awtomatizzata ta’ spjegazzjoni bbażata fuq id-definizzjoni tagħna ta’ spjegazzjoni.', 'mn': 'AI системийн хэрэглээнд нэмэгдэхэд тайлбарлалтын систем үүсгэдэг. Тодорхойлолтын системийг бүтээх нь тодорхойлолтын тодорхойлолт хэрэгтэй. Гэвч байгалийн хэл хэлний тодорхойлолт нь официально тодорхойлох нь хэцүү. Яг л психологи, философи, мэдлэг ухааны шинжлэх ухаанаас олон ойлголттой байдаг. Бид AI системээр хийсэн зөвлөгөө эсвэл таамаглах боломжтой байдлын олон ойлголт, асуудлыг судалж, тодорхойлолтын ерөнхий тодорхойлолт өгдөг. Үүний санал өгсөн тодорхойлолт бол амжилттай, хэрэглэх зорилготой. Онол болон хэрэглээний хоорондын ялгааг нэмэгдүүлэх зорилгоор бид мөн тодорхойлолтын тодорхойлолтын үндсэн автоматжуулан тайлбарлах системийн архитектурыг санал болгож байна.', 'ro': 'Odată cu creșterea utilizării sistemelor AI, apare nevoia de sisteme de explicații. Construirea unui sistem de explicații necesită o definiție a explicației. Cu toate acestea, explicația termenului de limbaj natural este dificil de definit formal, deoarece include mai multe perspective din diferite domenii, cum ar fi psihologia, filozofia și științele cognitive. Studiem mai multe perspective și aspecte de explicabilitate a recomandărilor sau predicțiilor făcute de sistemele AI și oferim o definiție generică a explicației. Definiția propusă este ambițioasă și dificilă de aplicat. Cu intenția de a depăși decalajul dintre teorie și aplicație, propunem, de asemenea, o posibilă arhitectură a unui sistem automatizat de explicații bazat pe definiția noastră a explicației.', 'pl': 'Wraz ze wzrostem wykorzystania systemów AI pojawia się potrzeba systemów wyjaśniających. Budowa systemu wyjaśnień wymaga definicji wyjaśnienia. Jednak wyjaśnienie terminu językowego naturalnego trudno jest formalnie zdefiniować, ponieważ obejmuje wiele perspektyw z różnych dziedzin, takich jak psychologia, filozofia i nauki poznawcze. Badamy wiele perspektyw i aspektów wyjaśnialności rekomendacji lub prognoz dokonywanych przez systemy AI oraz dostarczamy ogólną definicję wyjaśnienia. Proponowana definicja jest ambitna i trudna do stosowania. W celu wypełnienia luki między teorią a aplikacją proponujemy również możliwą architekturę zautomatyzowanego systemu wyjaśniania opartego na naszej definicji wyjaśnienia.', 'no': 'Med økninga i bruk av AI-systemet oppstår eit nødvendig for forklaringssystemet. Name Dette naturspråket er imidlertid vanskeleg å definera formelt fordi det inkluderer fleire perspektiv frå ulike domene, som psykologi, filosofi og kognitiv vitenskap. Vi studerer fleire perspektiv og aspektar på forklarability av anbefalingar eller forebygging gjeven av AI-systemet, og gir ein generell definisjon av forklaringar. Foreslått definisjon er ambisjonell og vanskeleg å bruka. Med hjelp til å bryta mellom teorien og programmet, foreslår vi også ein mulig arkitektur av eit automatisk forklaringssystem basert på definisjonen vårt av forklaringar.', 'sr': 'Uz povećanje korištenja AI-ovih sustava se pojavljuje potreba za objašnjavanjem. Izgraditi sistem objašnjenja zahteva definiciju objašnjenja. Međutim, prirodno jezičko objašnjenje je teško formalno definisati jer uključuje više perspektiva iz različitih domena poput psihologije, filozofije i kognitivne nauke. Proučavamo višestruke perspektive i aspekte objašnjivosti preporuka ili predviđanja koje su napravili AI sistemi i pružamo generičnu definiciju objašnjenja. Predložena definicija je ambiciozna i izazovna za primjenu. Sa namjerom da prebrojimo prazninu između teorije i aplikacije, takođe predlažemo moguću arhitekturu automatskog sistema objašnjenja na osnovu naše definicije objašnjenja.', 'si': 'AI පද්ධතිය භාවිතා විශාලනය සමග, ප්\u200dරශ්නයක් පද්ධතියට අවශ්\u200dය වෙනවා. විස්තර පද්ධතියක් නිර්මාණය කරන්න අවශ්\u200dය විස්තර විස්තර කරන්න. නමුත්, ස්වභාවික භාෂාව ප්\u200dරශ්නයක් අමාරුයි සාමාන්\u200dය විශ්වාසයෙන් විශේෂ විද්\u200dයාත්මක විශ්වාස කරන්න, ඒ වගේම ව අපි AI පද්ධතියෙන් කරපු ප්\u200dරශ්නයක් සහ ප්\u200dරශ්නයක් ගැන විස්තර කරන්න හැකි විස්තර සහ ප්\u200dරශ්නයක් අධික්\u200dරීය කරනවා ප්\u200dරයෝජනය විශ්වාස කරන්න පුළුවන් විශ්වාසය තමයි ප්\u200dරයෝජනය කරන්න පුළුවන්. අපි ස්වයංක්\u200dරියාව සහ ප්\u200dරයෝජනය අතර විශාලයක් පිරිසිදු කරන්න හැකියාව සමඟ, අපි ස්වයංක්\u200dරියාව සඳහා ස්වයංක්\u200dරියා', 'so': "Markii la kordhiyo isticmaalka nidaamka AI, waxaa soo socda baahida u baahan in ay u baahan yihiin nidaamka kala garsoorida. Buildidda nidaamka turjubaan wuxuu u baahan yahay mid faahin ah. Si kastaba ha ahaatee waxaa ku adag in si rasmi ah loo caddeeyo hadalka afka asalka ah, sababtoo ah waxaa ku jira aragtida kala duduwan sida cilmi cilmi, filsaf iyo cilmi aqoon. Waxaannu barannaa aragtido badan iyo dhinacyo kala duduwan oo ku saabsan faa'iido ama wax sii sheegi karno nidaamka AI, waxaana siinaynaa qoraal qarsoodi ah. Aqoonta la soo jeeday waa mid xiiso leh oo dhibaato leh in la codsado. Hadii aad dooneyso in aad isqabsato kala duwan tiyaatarka iyo codsiga, waxaynu sidoo kale soo jeedaynaa dhismo suurtagal ah oo lagu qorayo nidaamka turjumista oo iskuul ah oo ku saleysan fasiraadda.", 'sv': 'Med den ökade användningen av AI-system uppstår ett behov av förklaringssystem. Att bygga ett förklarande system kräver en definition av förklaring. Den naturliga språkförklaringen är dock svår att definiera formellt eftersom den innehåller flera perspektiv från olika områden såsom psykologi, filosofi och kognitiva vetenskaper. Vi studerar flera perspektiv och aspekter av förklaringar av rekommendationer eller förutsägelser gjorda av AI-system, och ger en generisk definition av förklaring. Den föreslagna definitionen är ambitiös och utmanande att tillämpa. För att överbrygga klyftan mellan teori och tillämpning föreslår vi också en möjlig arkitektur av ett automatiserat förklaringssystem baserat på vår definition av förklaring.', 'ml': 'AI സിസ്റ്റമുകള്\u200d ഉപയോഗിക്കുന്നതിനാല്\u200d വിശദീകരിക്കുന്ന സിസ്റ്റത്തിന്റെ ആവശ്യമാണ്. Building an explanation system requires a definition of explanation.  എന്നാലും സ്വാഭാവികമായ ഭാഷ വാക്ക് വിശദീകരിക്കാന്\u200d ബുദ്ധിമുട്ടുണ്ട്, അതില്\u200d വ്യത്യസ്ത ഡോമീനുകളില്\u200d നിന്നും പല ദൃഷ്ടാന്തങ്ങളും വ്യത് ആല്\u200d സിസ്റ്റത്തില്\u200d നിന്നും നിര്\u200dദ്ദേശിക്കുന്നതിന്റെയോ പ്രവചനങ്ങളുടെയോ വിശദീകരണത്തിന്റെയോ പല കാഴ്ചകളെയും ഞങ്ങള്\u200d പഠിക് പ്രൊദ്ദേശിക്കപ്പെട്ട വിശദീകരണങ്ങള്\u200d പ്രയോഗിക്കുന്നതിന് ആഗ്രഹിക്കുന്നതും വ്യാല്\u200dക്കാ തിയറിയും പ്രയോഗത്തിനും തമ്മിലുള്ള വ്യത്യാസങ്ങള്\u200dക്കും തമ്മിലുള്ള വ്യത്യാസങ്ങള്\u200d നിര്\u200dമ്മിക്കാനുള്ള ഉദ്ദേശം കൊണ്ട്, നമ്മുടെ വ', 'ta': 'AI அமைப்புகளின் பயன்பாட்டை அதிகரித்து, விளக்க அமைப்புகளுக்கு ஒரு தேவை ஏற்படுகிறது. ஒரு விளக்கம் அமைப்பை உருவாக்குதல் விளக்கம் தேவைப்படுகிறது. ஆனால், இயல்பான மொழி வார்த்தை விளக்கம் வடிவமைப்பாக வரையறுக்க கடினமானது ஏனெனில் அது வேறு இடங்களில் இருந்து பல பார்வைகளை சேர்க்கிறது, மைக்கலோ நாம் பல பார்வைகளையும், பரிந்துரைகளையும் முன்னோட்டங்களையும் கற்றுக் கொள்கிறோம் மற்றும் ஒரு பொதுவான விளக்கம் கொடுக்கிறோ பரிந்துரைக்கப்பட்ட வரையறையெல்லாம் ஆச்சரியமானது மற்றும் பயன்படுத்த வேண்டிய சவாலானது. திடியோர் மற்றும் பயன்பாட்டிற்கும் இடையே இடைவெளியை பிரித்துச் செய்ய நினைவில், நாம் ஒரு சாத்தியமான அடிப்படையாளம் தானாக விள', 'ur': 'AI سیستموں کے استعمال میں بڑھنے کے ساتھ، توضیح سیستموں کے لئے ایک ضرورت پیدا ہوتی ہے۔ ایک تفصیل سیستم بنانے کی ضرورت ہے ایک تفصیل کی تعریف۔ However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as psychology, philosophy, and cognitive sciences. ہم بہت سی نظریں اور روشنی کی توضیح یا پیش بینی کی تعلیم دیتے ہیں اور ایک معمولی توضیح کی تعلیم دیتے ہیں. پیشنهاد کی تعریف مہربانی اور اضافہ کرنے کا مشکل ہے. نظریہ اور کاربرد کے درمیان فاصلہ کے مطابق اضافہ کرنے کے مطابق، ہم نے بھی اپنی توضیح کی تعریف پر بنیاد رکھی ہوئی اتماٹی توضیح سیستم کی ایک امکان معماری پیشنهاد کرتا ہے.', 'uz': "AI tizimi ishlatishni oshirishda, faqat tizimni bajarish kerak. Comment Lekin tabiiy tilning tilini aniqlash juda qiyin, chunki u har xil domenadagi bir necha ko'plab koʻrinishini, psykologi, filosofi, va aqlli ilmiy fan kabi ko'plab koʻrinishini aniqlash mumkin. Biz AI tizimi orqali qanchalik ko'plab ko'plab koʻrinishini o'rganamiz va so'zlarini o'rganimiz va bir necha ko'plamni o'rganamiz va o'zgarishni bir genediy ajoyib qilamiz. Talab qilingan aniqlashni yetarlicha va qo'llash uchun qiyin edi. Biz teori va dastur orasidagi gapirarni birlashtirish uchun, biz faqatlarimizni aniqlashga asosida avtomatik o'rganish tizimni tasavvur qilamiz.", 'vi': 'Với sự gia tăng việc sử dụng hệ thống AI, sẽ có nhu cầu giải thích hệ thống. Xây dựng một hệ thống giải thích đòi hỏi một định nghĩa giải thích. Tuy nhiên, giải thích ngôn ngữ tự nhiên rất khó xác định chính thức vì nó bao gồm nhiều quan điểm từ các lĩnh vực khác nhau như tâm lý, triết học, và khoa học nhận thức. Chúng tôi nghiên cứu nhiều triển vọng và khía cạnh có thể xác định được các lời khuyên hay dự đoán do hệ thống AI thực hiện, và cung cấp một định nghĩa chung về giải thích. Định nghĩa đã đề nghị là đầy tham vọng và đầy thử thách. Với mục đích kết nối giữa giả thuyết và ứng dụng, chúng tôi cũng đề xuất một kiến trúc có thể của một hệ thống giải thích tự động dựa trên định nghĩa giải thích của chúng tôi.', 'bg': 'С увеличаването на използването на системи от ИИ възниква необходимост от системи за обяснение. Изграждането на система за обяснение изисква дефиниция на обяснение. Обаче, обяснението на термина естествен език е трудно да се определи формално, тъй като включва множество перспективи от различни области като психология, философия и когнитивни науки. Изследваме множество перспективи и аспекти на обяснимостта на препоръките или прогнозите, направени от системи за ИИ, и предоставяме общо определение за обяснение. Предложеното определение е амбициозно и предизвикателно за прилагане. С цел преодоляване на пропастта между теория и приложение предлагаме и възможна архитектура на автоматизирана система за обяснение въз основа на нашата дефиниция за обяснение.', 'nl': 'Met de toename van het gebruik van AI-systemen ontstaat er behoefte aan verklaringssystemen. Het bouwen van een verklaringssysteem vereist een definitie van uitleg. Echter, de natuurlijke taal term uitleg is moeilijk formeel te definiëren omdat het meerdere perspectieven omvat uit verschillende domeinen zoals psychologie, filosofie en cognitieve wetenschappen. We bestuderen meerdere perspectieven en aspecten van uitlegbaarheid van aanbevelingen of voorspellingen van AI-systemen en geven een generieke definitie van uitleg. De voorgestelde definitie is ambitieus en moeilijk toe te passen. Met de bedoeling de kloof tussen theorie en toepassing te overbruggen, stellen we ook een mogelijke architectuur voor van een geautomatiseerd verklaringssysteem gebaseerd op onze definitie van uitleg.', 'da': 'Med stigningen i brugen af AI-systemer opstår der behov for forklaringssystemer. At opbygge et forklaringssystem kræver en definition af forklaring. Men det naturlige sprog term forklaring er vanskelig at definere formelt, da det omfatter flere perspektiver fra forskellige domæner såsom psykologi, filosofi og kognitive videnskaber. Vi studerer flere perspektiver og aspekter af forklarelighed af anbefalinger eller forudsigelser lavet af AI-systemer, og giver en generisk definition af forklaring. Den foreslåede definition er ambitiøs og udfordrende at anvende. Med henblik på at bygge bro over kløften mellem teori og anvendelse foreslår vi også en mulig arkitektur af et automatiseret forklaringssystem baseret på vores definition af forklaring.', 'hr': 'Uz povećanje upotrebe AI-ovih sustava, pojavljuje se potreba za sustavima objašnjavanja. Izgraditi sustav objašnjenja zahtijeva definiciju objašnjenja. Međutim, objašnjenje prirodnog jezika je teško formalno definirati jer uključuje više perspektiva iz različitih domena poput psihologije, filozofije i kognitivne znanosti. Proučavamo višestruke perspektive i aspekte objašnjivosti preporuka ili predviđanja koje su napravili sustavi AI-a i pružamo generičnu definiciju objašnjenja. Predložena definicija je ambiciozna i izazovna za primjenu. S namjerom premostiti prazninu između teorije i primjene, također predlažemo moguću arhitekturu automatskog sustava objašnjenja na temelju naše definicije objašnjenja.', 'de': 'Mit dem zunehmenden Einsatz von KI-Systemen entsteht ein Bedarf an Erklärungssystemen. Der Aufbau eines Erklärungssystems erfordert eine Definition von Erklärung. Allerdings ist die natursprachliche Begriffserklärung schwer formal zu definieren, da sie mehrere Perspektiven aus verschiedenen Bereichen wie Psychologie, Philosophie und Kognitionswissenschaften umfasst. Wir untersuchen verschiedene Perspektiven und Aspekte der Erklärbarkeit von Empfehlungen oder Vorhersagen von KI-Systemen und liefern eine generische Definition der Erklärung. Die vorgeschlagene Definition ist ehrgeizig und schwierig anzuwenden. Mit der Absicht, die Lücke zwischen Theorie und Anwendung zu schließen, schlagen wir auch eine mögliche Architektur eines automatisierten Erklärungssystems vor, basierend auf unserer Definition von Erklärung.', 'id': 'Dengan meningkat penggunaan sistem AI, kebutuhan sistem penjelasan muncul. Membangun sistem penjelasan membutuhkan definisi penjelasan. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as psychology, philosophy, and cognitive sciences.  Kami mempelajari berbagai perspektif dan aspek penjelasan rekomendasi atau prediksi yang dibuat oleh sistem AI, dan menyediakan definisi umum penjelasan. Definisi yang diusulkan adalah ambisius dan tantangan untuk diaplikasikan. Dengan niat untuk menyembunyikan ruang antara teori dan aplikasi, kami juga mengusulkan arkitektur kemungkinan sistem penjelasan otomatis berdasarkan definisi penjelasan kami.', 'fa': 'با افزایش استفاده از سیستم\u200cهای AI، نیازی برای سیستم توضیح وجود دارد. ساختن یک سیستم توضیح نیاز به تعریف توضیح است. ولی توضیح کلمه طبیعی زبان طبیعی برای تعریف رسمی سخت است، همانطور که شامل تعداد زیادی از نظر\u200cهای مختلف از منطقه\u200cهای مانند روانشناسی، فلسفه و علم شناختی است. ما مشاهده های متعدد و نقطه های توضیح یا پیش بینی های سیستم های AI را مطالعه می کنیم و تعریف عمومی توضیح می دهیم. این تعریف پیشنهاد اراده\u200cای است و سخت\u200cگیری برای کاربرد است. با قصد برداشتن فاصله بین تئوری و کاربرد، ما همچنین یک معماری ممکن از سیستم توضیح خودکار بر اساس تعریف توضیح ما پیشنهاد می کنیم.', 'sw': 'Kutokana na kuongezeka kwa matumizi ya mifumo ya UKIMWI, haja ya maelezo yanatokea. Kujenga mfumo wa ufafanuzi unahitaji ufafanuzi. Hata hivyo, maelezo ya lugha ya asili ni vigumu kuelezea rasmi kwa sababu inajumuisha mitazamo mbalimbali kutoka kwenye maeneo mbalimbali kama vile psykology, falsafa, na sayansi za maarifa. Tunafuatilia mitazamo mbalimbali na mambo ya kuelezea maelezo ya mapendekezo au utabiri uliofanywa na mfumo wa AI, na kutoa maelezo ya msingi ya maelezo. Utafiti huu unapendekezwa ni wenye hamasa na changamoto ya kutekeleza. Kwa lengo la kutangaza tofauti kati ya nadharia na matumizi, pia tunapendekeza ujenzi wa mfumo wa maelezo yanayoweza kujitegemea kutokana na ufafanuzi wetu.', 'af': "Met die vergroot gebruik van AI-stelsels word 'n nodig vir verduidelingsstelsels opgestaan. Opbou 'n uitduidelingsstelsel benodig 'n definisie van uitduidelings. Maar die natuurlike taal uitduidelikheid is moeilik om formeel te definieer, omdat dit veelvuldige perspeksies van verskillende domeine, soos psihologie, filosofie en kognitiewe wetenskappe, insluit. Ons studeer veelvuldige perspeksies en aspekte van verduidelikheid van aanbevelings of voorskoue wat deur AI stelsels gemaak is, en verskaf 'n generieke definisie van uitduidelikheid. Die voorgestelde definisie is ambisieus en moeilik om toepassing te doen. Met die intensie om die gap tussen teorie en toepassing te brei, voorstel ons ook 'n moontlike arkitektuur van' n outomatiese uitduidelingsstelsel gebaseer op ons definisie van uitduidelings.", 'tr': 'AI sistemalaryň ullanyşyny artyp bilen, düşündirim sistemalary üçin gerek bolar. Taýýarlama sistemasyny guramak düşündirilmek gerek. Ýöne, tebigy dil sözleriniň düşündirişi formaly ýaly dürli sahyplardan soňky, filosofiýa we bilgili bilim ýaly görnüşdürmek kyndyr. Biz AI sistemalaryň eden maslahatlaryň we öngörümleriniň a ňsatylylylygynyň birnäçe perspektive we aspektlerini öwrenýärik we ol üns bir düşündirişlik definisyony saýlaýarlar. Maslahatlandyrylýan tanyşdyrma höwessizdir we uygulamak kynçylykdyr. Teoriýa we uygulamalaryň arasyndaky gapysyny köçürmek niýeti bilen, biz hem öz düşündirişimize görä awtomatik bir düşündirim sistemasynyň mümkin arhitekturuny teklip edip bilýäris.', 'sq': 'Me rritjen e përdorimit të sistemeve AI, ngrihet një nevojë për sisteme shpjegimi. Ndërtimi i një sistemi shpjegimi kërkon një përcaktim shpjegimi. Megjithatë, shpjegimi natyror i gjuhës është i vështirë të përcaktohet zyrtarisht pasi përfshin perspektiva të shumta nga fusha të ndryshme si psikologjia, filozofia dhe shkencat kognitive. Ne studiojmë perspektiva të shumta dhe aspekte të shpjegimisë së rekomandimeve apo parashikimeve të bërë nga sistemet AI dhe ofrojmë një përcaktim gjeneral të shpjegimit. Përcaktimi i propozuar është ambicioz dhe sfidues për të aplikuar. Me qëllim të mbulojmë dallimin midis teorisë dhe aplikimit, ne gjithashtu propozojmë një arkitekturë të mundshme të një sistemi të shpjegimit automatik bazuar në përcaktimin tonë të shpjegimit.', 'am': 'With the increase in the use of AI systems, a need for explanation systems arises.  መግለጫ ሲስተም መግለጫ ያስፈልጋል፡፡ ነገር ግን የፍጥረቱ ቋንቋ ቃላት መግለጫ በአካባቢው ቋንቋ ላይ መግለጫ አስቸጋሪ ነው፡፡ በAI ስርዓቶች የተደረገውን ምክንያቶች ወይም ትንቢት ማብራራትን እናስተምራለን፡፡ በተዘጋጀው ግንኙነት አፍላጎት እና ለመጠቀም የሚችል ነው፡፡ በቴዮርዮ እና በፕሮግራም መካከል የግንኙነትን ለመቀላቀል ለመፈለግ፣ በተለየን ማስታወቂያውን በመሠረት የተቻለ የፍትሕርት መሠረት መሠረትን እናሳስባታለን፡፡', 'hy': 'ԱԲ համակարգերի օգտագործման աճի դեպքում բացատրության համակարգերի կարիքն է առաջանում: Պատմելու համակարգ կառուցելը պահանջում է բացատրության սահմանումը: Այնուամենայնիվ, բնական լեզվի բացատրությունը պաշտոնապես դժվար է սահմանել, քանի որ այն ներառում է բազմաթիվ տեսանկյուններ տարբեր ոլորտներից, ինչպիսիք են հոգեբանությունը, փիլիսոփայությունը և ճանաչողական գիտությունները: We study multiple perspectives and aspects of explainability of recommendations or predictions made by AI systems, and provide a generic definition of explanation.  Առաջարկված սահմանումը հաճելի և դժվար է կիրառել: Որպեսզի հաղթահարենք տեսության և ծրագրի միջև տարբերությունը, մենք նաև առաջարկում ենք ավտոմատիկ բացատրման համակարգի հնարավոր ճարտարապետություն, որը հիմնված է բացատրության սահմանման վրա:', 'az': 'AI sistemlərinin istifadəsində artıqlığı ilə aydınlaşdırma sistemlərinə ihtiyacı var. Açıklama sistemi inşa etmək a çıq-aydın tanımlaması lazımdır. Lakin, təbiətli dil termini təfsil etmək formal olaraq təsdiqlənmək çətindir, çünki psikoloji, filosofi və kognitiv bilim kimi fərqli dəstələrdən çoxlu perspektivlər içərisində olmaq çətindir. Biz AI sistemlərindən yapılmış tədbirlərin və tədbirlərin müəyyən edilməsi və tədbirlərin çoxlu perspektivlərini və aspektlərini təhsil edirik və təfsirlərin generik tanımlamasını təmin edirik. Önülləşdirilən tanımlama məcburiyyət və istifadə etmək çətindir. Teoriya və uyğulama arasındakı boşluqları köçürmək niyyətində, biz də təfsilatımızın tanımlamasına dayanan avtomatik a çıqlama sisteminin mümkün bir arhitektür təklif edirik.', 'ca': "Amb l'augment de l'ús dels sistemes d'AI, hi ha una necessitat de sistemes d'explicació. Construir un sistema d'explicació requereix una definició d'explicació. No obstant això, la explicació del terme de llenguatge natural és difícil de definir formalment perquè inclou múltiples perspectives de dominis diferents com la psicologia, la filosofia i les ciències cognitives. Estudem múltiples perspectives i aspectes d'explicabilitat de recomanacions o prediccions fetes pels sistemes d'AI, i proporcionem una definició genèrica d'explicació. La definició proposta és ambiciosa i difícil d'aplicar. Amb l'intenció de superar el buit entre la teoria i l'aplicació, també proposem una arquitectura possible d'un sistema d'explicació automatitzat basat en la nostra definició d'explicació.", 'ko': '인공지능 시스템 사용이 늘면서 해석 시스템에 대한 수요도 증가했다.해석 체계를 구축하려면 해석이 필요한 정의가 필요하다.그러나 자연언어 용어 해석은 심리학, 철학, 인지과학 등 다양한 분야의 다양한 시각을 포함하기 때문에 정식으로 정의하기 어렵다.우리는 인공지능 시스템이 제기한 건의나 예측의 해석 가능한 여러 각도와 측면을 연구하고 해석의 일반적인 정의를 제공한다.의안 중의 정의는 웅장하여 응용하기 어렵다.이론과 응용 간의 갭을 메우기 위해 우리는 해석 정의를 바탕으로 하는 자동 해석 시스템의 가능성 구조를 제시했다.', 'bn': 'এইআই সিস্টেম ব্যবহারের বৃদ্ধি হলে ব্যাখ্যা সিস্টেমের প্রয়োজন হয়। একটি ব্যাখ্যা সিস্টেম বানানোর জন্য ব্যাখ্যা প্রয়োজন। তবে প্রাকৃতিক ভাষার কথা ব্যাখ্যা করা কঠিন যেহেতু বিভিন্ন দেশের বিভিন্ন দৃষ্টিভঙ্গি যেমন মানসিকোলজি, ফিলোসাফি এবং জ্ঞানীয় বিজ আমরা বেশ কিছু দৃষ্টিভঙ্গি এবং প্রাপ্ত পরামর্শ বা ভবিষ্যতের ব্যাখ্যা ব্যাখ্যা করি এবং সাধারণ ব্যাখ্যা দেই। The proposed definition is ambitious and challenging to apply.  তত্ত্ব এবং অ্যাপ্লিকেশনের মধ্যে বিভ্রান্ত বিভ্রান্ত প্রস্তাব করার উদ্দেশ্যে আমরা স্বয়ংক্রিয়ভাবে ব্যাখ্যা করা একটি স্বয়ংক্', 'bs': 'Uz povećanje korištenja AI-ovih sustava, raste potreba za objašnjavanjem sustava. Izgraditi sistem objašnjenja zahtijeva definiciju objašnjenja. Međutim, prirodno jezičko objašnjenje je teško formalno definisati jer uključuje više perspektiva iz različitih domena poput psihologije, filozofije i kognitivne nauke. Proučavamo višestruke perspektive i aspekte objašnjivosti preporuka ili predviđanja koje su napravili AI sistemi i pružamo generičnu definiciju objašnjenja. Predložena definicija je ambiciozna i izazovna za primjenu. S namjerom da prebrojimo prazninu između teorije i aplikacije, također predlažemo moguću arhitekturu automatskog sistema objašnjenja na temelju naše definicije objašnjenja.', 'et': 'Tehisintellektisﾃｼsteemide kasutamise suurenemisega tekib vajadus selgitussﾃｼsteemide jﾃ､rele. Selgitussﾃｼsteemi loomine nﾃｵuab selgituse mﾃ､ﾃ､ratlemist. Kuid looduskeele termini selgitus on raske formaalselt mﾃ､ﾃ､ratleda, sest see sisaldab mitmeid vaatenurki erinevatest valdkondadest nagu psﾃｼhholoogia, filosoofia ja kognitiivsed teadused. Uurime AI sﾃｼsteemide soovituste vﾃｵi prognooside selgitatavuse mitmeid perspektiive ja aspekte ning anname ﾃｼldise selgituse. Kavandatud mﾃ､ﾃ､ratlus on ambitsioonikas ja keeruline kohaldada. Eesmﾃ､rgiga ﾃｼletada lﾃｵhe teooria ja rakenduse vahel pakume vﾃ､lja ka automatiseeritud selgitussﾃｼsteemi vﾃｵimaliku arhitektuuri, mis pﾃｵhineb meie selgituse definitsioonil.', 'cs': 'S rostoucím používáním AI systémů vzniká potřeba vysvětlovacích systémů. Vytvoření vysvětlovacího systému vyžaduje definici vysvětlení. Nicméně, vysvětlení termínu přirozeného jazyka je obtížné formálně definovat, protože zahrnuje více perspektiv z různých oblastí, jako je psychologie, filozofie a kognitivní vědy. Studujeme několik perspektiv a aspektů vysvětlitelnosti doporučení či predikcí systémů AI a poskytujeme obecnou definici vysvětlení. Navrhovaná definice je ambiciózní a náročná na uplatnění. S cílem překlenout propast mezi teorií a aplikací navrhujeme také možnou architekturu automatizovaného vysvětlovacího systému založeného na naší definici vysvětlení.', 'fi': 'Tekoälyjärjestelmien käytön lisääntyessä syntyy tarve selitysjärjestelmille. Selitysjärjestelmän rakentaminen edellyttää selityksen määrittelyä. Luonnonkielen termin selitys on kuitenkin vaikea määritellä muodollisesti, koska se sisältää useita näkökulmia eri aloilta, kuten psykologiasta, filosofiasta ja kognitiivisista tieteistä. Tutkimme useita näkökulmia ja näkökulmia tekoälyjärjestelmien suositusten tai ennusteiden selittävyyteen ja annamme yleisen selityksen. Ehdotettu määritelmä on kunnianhimoinen ja haastava soveltaa. Tarkoituksena on kuroa umpeen teorian ja soveltamisen välinen kuilu, joten ehdotamme myös automaattisen selitysjärjestelmän mahdollista arkkitehtuuria, joka perustuu selityksen määritelmäämme.', 'jv': 'Gambar aturan ing nggawe sistem AI, kudu kanggo sabanjuré sistem kapan bangsane. Ngawe Sudané, akeh langkung kelangan kelangan kelangan kanggo masaké nggawe geraluké karo hal-hal perspeksiyen sing sampeyan karo perspeksiyen, dipulusoji, lan saiki kowisiyé. Awak dhéwé éntuk karo perspektur karo karo akeh kapan kelangan karo alé karo alé sistem AI, lan nyimpen akeh perusahaan kelangan barang kelangan Definisi aparat ambekan lan kelangan kuwi nggawe Saiki nggawe nggawe gap ning theori lan aplikasi, kéné supoyo akeh architecture sing bisa perusahaan sistem sing nyimpen winih kanggo mbataluraké ning awak dhéwé.', 'sk': 'S povečanjem uporabe sistemov umetne inteligence se pojavlja potreba po sistemih pojasnjevanja. Izgradnja sistema pojasnjevanja zahteva opredelitev pojasnila. Vendar pa je izraz naravnega jezika težko formalno opredeliti, saj vključuje več perspektiv z različnih področij, kot so psihologija, filozofija in kognitivne vede. Proučujemo različne perspektive in vidike pojasnljivosti priporočil ali napovedi sistemov umetne inteligence in zagotavljamo splošno definicijo pojasnila. Predlagana opredelitev je ambiciozna in zahtevna za uporabo. Z namenom premostitve vrzeli med teorijo in aplikacijo predlagamo tudi možno arhitekturo avtomatiziranega pojasnjevalnega sistema, ki temelji na naši definiciji pojasnila.', 'he': 'עם הגידול בשימוש במערכות AI, נוצר צורך למערכות הסבר. בניית מערכת הסבר דורשת הגדרה של הסבר. עם זאת, מונח השפה הטבעי קשה להגדיר באופן רשמי, כיוון שהוא מכיל מרובות מבטים ממשפחות שונות כמו פסיכולוגיה, פילוסופיה ומדעים קוגניטיביים. אנו לומדים מרובות מבטים ובהיבטים של הסבר של המלצות או חזיונות שנעשו על ידי מערכות AI, ומספקים הגדרה כללית של הסבר. ההגדרה המוצעת היא שאפתית ומתגברת להפעיל. בכוונה לשבור את הפער בין התיאוריה והשימוש, אנחנו גם מציעים ארכיטקטורה אפשרית של מערכת הסבר אוטומטית מבוססת על הגדרת ההסבר שלנו.', 'ha': "Idan ana ƙara da amfani da na'urar AI, wata haja na buƙata wa fassarar ayuka za'a fito. Babu samar da wani tsari na bayyana bayyana shi, sai ya ƙayyade wani fassara. A lokacin da, ma'anar maganar harshen na natura ya zama mai ƙunci a bayyana rasmi, kwani shi yana da misãlai masu yawa daga dukkan wurãre dabam, kamar misali, filosofi da kuma ma'anan sanki. Tuna karatun misãlai masu yawa da masu hankali ga masu shawara ko bashirin da aka aikata na'urar AI, kuma Muke gaura fassarar bayani na daban. Babu fassarar da aka faɗa shi yana mai kwaɗayi da gajiya ta tambayi. Ko da aimin ya sami gaura a tsakanin teori da shiryoyin ayuka, za'a buƙata wani matsayi na da ɗabi'a kan fassarar-bayani farat ɗaya.", 'bo': 'AI་མ་ལག་གི་སྤྱོད་ཚད་དར་བསྐྱེད་བཞིན་པ་ལས་མ་ལག་གི་གསལ་བཤད་དགོས་པ་ཞིག་མཐོང་བཞིན། མ་ལག་གི་གསལ་བཤད་ཞིག་བཟོ་བྱེད་ན་ལ་གསལ་བཤད་ཞིག་དགོས་ཡོད། ཡིན་ནའང་མིན་པར། སྤྱིར་བཏུབ་པའི་སྐད་ཡིག་གི་གསལ་བཤད་འདི་རྣམས་ལས་ཕན་ཚོར་ཐོག་ལས་མཐོང་དང་། ང་ཚོས་AI་རིགས་མ་ལག་གིས་བཟུང་བའི་གསལ་བཤད་དང་གནད་དོན་དག་ཕྱོགས་མང་པོ་ཞིག་ཡིན་ཏེ། དམིགས་འཛུགས་ཀྱི་ངོས་འཛིན་དེ་མཚོན་ན་ཧ་ཅང་དང་གནད་དོན་དག་ཐུབ་པ་ཡིན། དམིགས་ཡུལ་ནི་ལྟ་བུའི་བར་སྟོང་དང་ཉེར་སྤྱོད་དབར་གྱི་བར་སྟོང་དང་མཉམ་དུ་འུ་ཚོས་རང་འགུལ་གྱི་མ་ལག་ཐོག་གི་སྒྲིག'}
