{'en': 'Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting', 'ar': 'تحسين تصنيف النص عبر اللغات باستخدام ترجيح مثيل اللقطة الصفري', 'es': 'Mejora de la clasificación de textos en varios idiomas con ponderación de instancias cero', 'fr': "Amélioration de la classification de texte multilingue grâce à la pondération d'instance zéro shot", 'pt': 'Melhorando a classificação de texto em vários idiomas com ponderação de instância zero-shot', 'ja': 'ゼロショットインスタンス重み付けによるクロスリンガルテキスト分類の改善', 'hi': 'शून्य-शॉट इंस्टेंस-वेटिंग के साथ क्रॉस-भाषी पाठ वर्गीकरण में सुधार', 'zh': '因零次例加权改进跨语言文本分类', 'ru': 'Улучшение межъязыковой классификации текста с нулевым взвешиванием экземпляров', 'ga': 'Aicmiú Téacs Trastheangach a Fheabhsú le Meáchain Chánachais Nialais', 'el': 'Βελτίωση της γλωσσικής ταξινόμησης κειμένου με μηδενική στάθμιση παρουσίας', 'it': 'Miglioramento della classificazione del testo multilingue con la ponderazione delle istanze a scatto zero', 'kk': 'Тұтас тілді мәтін классификациясын нөл- шоу инстанциясы үлкендіру', 'hu': 'A többnyelvű szövegosztályozás javítása a nullás példánysúlyozással', 'lt': 'Gerinti tarpkalbinį teksto klasifikavimą nulinio nuotraukos instancijos svyravimu', 'ml': 'ക്രോസ്- ലാങ്കുകളുടെ പദാവലിയുടെ ക്ലാസ്സിഷന്\u200d പൂര്\u200dണ്ണമായി വെയ്റ്റിങ്ങ് ചെയ്യുന്നു', 'ms': 'Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting', 'mk': 'Подобрување на крстојазичната класификација на текст со нула- снимка инстантна тежина', 'mt': 'It-titjib tal-Klassifikazzjoni tat-Test Translingwi b’Piżijiet tal-Instanza b’Żero-Shots', 'pl': 'Poprawa klasyfikacji tekstu między językami dzięki zerowemu ważeniu instancji', 'mn': 'Төрвөн хэл хэлний текст классификацийг Нэг шалтгаан Instance-Weighting-тай сайжруулах', 'ka': 'კრესენგური ტექსტის კლასიფიკაციაცია ნულ სტატის ინსტანსის განმავლობით', 'ro': 'Îmbunătățirea clasificării textelor translingve cu cântărirea instanțelor zero-shot', 'so': 'Improving Cross-language Text Classification with Zero-shot Instance-Weighting', 'si': 'ක්\u200dරොස්-භාෂාවක් පාළුවක් විශේෂණය සමග සීරෝවිට් විශේෂණය කරන්න', 'sv': 'Förbättra flerspråkig textklassificering med noll instansvägning', 'ur': 'کروس زبان کے متن کلاسیفون کو صفر-شٹ انٹنس-وایٹنگ کے ساتھ بہتر کر رہا ہے', 'no': 'Forbetra krysspråk- tekstklassifikasjon med null- snittvekking', 'sr': 'Poboljšavanje klasifikacije krstojezičkog teksta sa nuloštećenjem Instancijskog težina', 'ta': 'கிராஸ்- மொழி உரை வகைப்படுத்தல் சூழ்நிலைப்படுத்தப்பட்ட நிறுவனத்தை மேம்படுத்துகிறது', 'vi': 'Tăng cường độ cân bằng chữ thập', 'uz': 'Name', 'bg': 'Подобряване на междуезичната класификация на текста с нулева претегляне на инстанциите', 'nl': 'Verbetering van de meertalige tekstclassificatie met Zero-shot Instance-Weighting', 'da': 'Forbedring af tværsproget tekstklassifikation med nulskudsinstansvægtning', 'hr': 'Poboljšanje klasifikacije krstojezičkog teksta s nulotvorenim temeljom instanca', 'id': 'Menembangkan Klasifikasi Teks Selasa Bahasa dengan Pembilangan Instans Zero-shot', 'de': 'Verbesserung der sprachübergreifenden Textklassifizierung mit Zero-Shot Instance-Gewighting', 'sw': 'Kuboresha Makala ya Kimataifa yenye lugha ya Kusini kwa Utafiti-Weighting', 'fa': 'بهتر کردن کلاس\u200cسازی متن\u200cهای عبارت زبانی با وزن فصل\u200cهای صفر', 'tr': 'Çoklu dilli Metin Sönlenmesini Zero-shot Instance-Weighting bilen bejer', 'sq': 'Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting', 'af': 'Verbeter Kruistale Teks Klassifikasie met Nuwe- skoot Instans- Wegting', 'am': 'ቦታ፦', 'hy': 'Խոսել լեզվի միջև տեքստի դասակարգումը զրո նկարի միջոցով', 'bn': 'ক্রস-ভাষায় টেক্সট ক্লাসিং এর সাথে জিরো- শুট ইনস্ট্যান্স- উইটিং  এর সাথে সংশোধন করা হচ্ছে', 'az': 'Sıfır-vuruş Instance-Weighting ilə Xərc dilli Metin Klasifikasiyasını yaxşılaşdırma', 'bs': 'Poboljšavanje klasifikacije krstojezičkog teksta sa nulo-pucanjem Instance-Weighting', 'ca': 'millorar la classificació del text translingüe amb pesar instantària zero', 'cs': 'Zlepšení klasifikace textu mezi jazyky pomocí nulového vážení instancí', 'ko': '제로 렌즈 실례로 크로스 언어 텍스트 분류 개선', 'et': 'Keeleülese teksti klassifitseerimise parandamine nullkatse eksemplari kaalumisega', 'fi': 'Kieltenvälisen tekstin luokittelun parantaminen nollaotoksen painotuksella', 'jv': 'Yukono Géwan Kros-dilangul Teks Kelassistasi karo 0-shot Instance-weiting', 'ha': 'Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting', 'he': 'שיפור שיעור טקסט בין שפתיים עם משקל רגע אפס', 'sk': 'Izboljšanje razvrščanja medjezičnega besedila s ponderiranjem primerkov z ničelnim posnetkom', 'bo': 'Cross-lingual Text Classification with Zero-shot Instance-Weighting'}
{'en': 'Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During  training , the framework utilizes  gradient descent  that is weighted by instance weights to update parameters. We evaluate this  framework  over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4 % in single-source transfer and 8 % in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer.', 'ar': 'يعتبر تصنيف النص عبر اللغات (CLTC) مهمة صعبة تزداد صعوبة بسبب نقص البيانات المصنفة باللغات منخفضة الموارد. في هذه الورقة ، نقترح ترجيح مثيلات اللقطة الصفرية ، وهو إطار عمل تعليمي عام لا يعرف النموذج بدون طلقة لتحسين CLTC من خلال الاستفادة من ترجيح مثيل المصدر. يضيف وحدة فوق نماذج اللغة المدربة مسبقًا لحساب التشابه لأوزان المثيل ، وبالتالي محاذاة كل مثيل مصدر مع اللغة الهدف. أثناء التدريب ، يستخدم إطار العمل النسب المتدرج الذي يتم ترجيحه حسب أوزان المثيل لتحديث المعلمات. نقوم بتقييم هذا الإطار على سبع لغات مستهدفة في ثلاث مهام أساسية ونظهر فعاليته وقابليته للتوسع ، من خلال تحسين درجة F1 حتى 4٪ في النقل أحادي المصدر و 8٪ في النقل متعدد المصادر. على حد علمنا ، فإن طريقتنا هي الأولى في تطبيق ترجيح المثيل في CLTC بدون طلقة. إنه بسيط ولكنه فعال ويمكن توسيعه بسهولة إلى نقل متعدد المصادر.', 'es': 'La clasificación de textos multilingües (CLTC) es una tarea difícil que se hace aún más difícil debido a la falta de datos etiquetados en idiomas de pocos recursos. En este artículo, proponemos la ponderación de instancias zero-shot, un marco general de aprendizaje zero-shot independiente del modelo para mejorar el CLTC mediante el aprovechamiento de la ponderación de las instancias de origen. Agrega un módulo a los modelos de lenguaje previamente entrenados para el cálculo de similitud de los pesos de las instancias, alineando así cada instancia de origen con el idioma de destino. Durante el entrenamiento, el marco utiliza el descenso de gradiente que se pondera por pesos de instancia para actualizar los parámetros. Evaluamos este marco en siete idiomas de destino en tres tareas fundamentales y demostramos su eficacia y extensibilidad, mejorando la puntuación F1 hasta un 4% en la transferencia de una sola fuente y un 8% en la transferencia de múltiples fuentes. Hasta donde sabemos, nuestro método es el primero en aplicar la ponderación de instancias en CLTC zero-shot. Es simple pero eficaz y fácilmente extensible a la transferencia de múltiples fuentes.', 'fr': "La classification de texte multilingue (CLTC) est une tâche difficile rendue encore plus difficile en raison du manque de données étiquetées dans les langues à faibles ressources. Dans cet article, nous proposons la pondération d'instance zero-shot, un cadre général d'apprentissage zero-shot agnostique pour améliorer CLTC en tirant parti de la pondération des instances source. Il ajoute un module en plus des modèles de langage pré-entraînés pour le calcul de similarité des pondérations d'instance, alignant ainsi chaque instance source sur la langue cible. Pendant l'entraînement, le cadre utilise la descente de pente pondérée par des poids d'instance pour mettre à jour les paramètres. Nous évaluons ce framework sur sept langues cibles sur trois tâches fondamentales et montrons son efficacité et son extensibilité, en améliorant le score F1 jusqu'à 4\xa0% dans le transfert source unique et 8\xa0% dans le transfert multi-sources. À notre connaissance, notre méthode est la première à appliquer la pondération d'instance dans le CLTC zero-shot. Il est simple mais efficace et facilement extensible en transfert multi-sources.", 'pt': 'A classificação de texto em vários idiomas (CLTC) é uma tarefa desafiadora ainda mais difícil devido à falta de dados rotulados em idiomas de poucos recursos. Neste artigo, propomos a ponderação de instância zero-shot, uma estrutura geral de aprendizado zero-shot independente do modelo para melhorar o CLTC aproveitando a ponderação da instância de origem. Ele adiciona um módulo em cima de modelos de linguagem pré-treinados para cálculo de similaridade de pesos de instância, alinhando assim cada instância de origem ao idioma de destino. Durante o treinamento, a estrutura utiliza gradiente descendente que é ponderado por pesos de instância para atualizar os parâmetros. Avaliamos essa estrutura em sete idiomas de destino em três tarefas fundamentais e mostramos sua eficácia e extensibilidade, melhorando a pontuação da F1 em até 4% na transferência de fonte única e 8% na transferência de várias fontes. Até onde sabemos, nosso método é o primeiro a aplicar a ponderação de instância no CLTC zero-shot. É simples, mas eficaz e facilmente extensível à transferência de várias fontes.', 'ja': 'クロスリンガルテキスト分類（ CLTC ）は、低リソース言語でのラベル付けされたデータの欠如により、さらに困難になる困難な課題です。本稿では，ソースインスタンスの重み付けを活用してCLTCを改善するための一般的なモデル非推論的なゼロショット学習フレームワークであるゼロショットインスタンス重み付けを提案する．インスタンス重みの類似性計算のために、事前にトレーニングされた言語モデルの上にモジュールを追加し、各ソースインスタンスをターゲット言語に整列させます。トレーニング中、フレームワークは、インスタンス重みによって重み付けされた勾配降下を利用してパラメータを更新します。私たちは、このフレームワークを3つの基本的なタスクについて7つのターゲット言語にわたって評価し、シングルソース転送で最大4%、マルチソース転送で8%のF 1スコアを改善することによって、その有効性と拡張性を示します。私たちの知る限りでは、私たちのメソッドはゼロショットCLTCでインスタンス加重を適用する最初のものです。シンプルながら効果的で、マルチソース転送に簡単に拡張できます。', 'zh': '跨语言文本类 (CLTC) 一挑战性之任也,无低资源言之标数,是以益难。 本文有零次例加权,此模形通用零次学框架,所以利用源例加权改进CLTC也。 加模块于先训之言,以权重之相似性计之,而使每源实例与招言相保。 教练之间,框架以梯度降(例权重加权)以新参数。 凡三务者,论其七框架,单源传输其分F1于4%,8%于多源传输,以展其有效性可扩展性。 以吾所知,吾法首用实零次 CLTC 之法。 简而有效,易至于多源传输。', 'hi': 'क्रॉस-भाषी टेक्स्ट क्लासिफिकेशन (सीएलटीसी) एक चुनौतीपूर्ण कार्य है जो कम संसाधन वाली भाषाओं में लेबल किए गए डेटा की कमी के कारण अभी भी कठिन बना है। इस पेपर में, हम स्रोत उदाहरण भार का लाभ उठाकर सीएलटीसी में सुधार के लिए एक सामान्य मॉडल-अज्ञेयवादी शून्य-शॉट सीखने के ढांचे, शून्य-शॉट इंस्टेंस-वेटिंग का प्रस्ताव करते हैं। यह उदाहरण वजन की समानता गणना के लिए पूर्व-प्रशिक्षित भाषा मॉडल के शीर्ष पर एक मॉड्यूल जोड़ता है, इस प्रकार प्रत्येक स्रोत उदाहरण को लक्ष्य भाषा में संरेखित करता है। प्रशिक्षण के दौरान, फ्रेमवर्क ग्रेडिएंट वंश का उपयोग करता है जो पैरामीटर अपडेट करने के लिए इंस्टेंस वेट द्वारा भारित होता है। हम तीन मौलिक कार्यों पर सात लक्ष्य भाषाओं पर इस ढांचे का मूल्यांकन करते हैं और एकल-स्रोत हस्तांतरण में 4% और बहु-स्रोत हस्तांतरण में 8% तक एफ 1 स्कोर में सुधार करके इसकी प्रभावशीलता और विस्तार दिखाते हैं। हमारे ज्ञान का सबसे अच्छा करने के लिए, हमारी विधि शून्य शॉट CLTC में उदाहरण भार लागू करने के लिए पहली बार है। यह सरल अभी तक प्रभावी है और आसानी से बहु स्रोत हस्तांतरण में extensible है.', 'ru': 'Классификация текстов на разных языках (CLTC) является еще более сложной задачей, которая еще более усложняется из-за отсутствия маркированных данных на языках с ограниченными ресурсами. В этой статье мы предлагаем взвешивание экземпляров с нулевым выстрелом, общую модель-диагностическую структуру обучения с нулевым выстрелом для улучшения CLTC путем использования взвешивания экземпляров источника. Он добавляет модуль поверх предварительно обученных языковых моделей для вычисления сходства весов экземпляров, тем самым выравнивая каждый экземпляр источника к целевому языку. Во время обучения фреймворк использует градиентный спуск, который взвешивается по весам экземпляра для обновления параметров. Мы оцениваем этот фреймворк на семи целевых языках по трем фундаментальным задачам и показываем его эффективность и расширяемость, улучшая показатель F1 до 4% при передаче из одного источника и 8% при передаче из нескольких источников. Насколько нам известно, наш метод является первым, который применяет взвешивание экземпляров в CLTC с нулевым выстрелом. Он прост, но эффективен и легко расширяется в многоисточниковую передачу.', 'ga': 'Is tasc dúshlánach é rangú téacs tras-teangach (CLTC) a dhéantar níos deacra fós mar gheall ar an easpa sonraí lipéadaithe i dteangacha íseal-acmhainne. Sa pháipéar seo, molaimid ualú samplaí náid urchar, creat ginearálta foghlama samhail-agnostic náid-shots chun CLTC a fheabhsú trí ualú ásc foinse a ghiaráil. Cuireann sé modúl anuas ar mhúnlaí teanga réamh-oilte le haghaidh ríomh cosúlachtaí ar mheáchan na samplaí, ag ailíniú gach foinse shampla leis an sprioctheanga. Le linn na hoiliúna, baineann an creat úsáid as shliocht grádáin a ualú de réir meáchain shamplacha chun paraiméadair a nuashonrú. Déanaimid measúnú ar an gcreat seo thar sheacht sprioctheanga ar thrí thasc bhunúsacha agus léirímid a éifeachtúlacht agus a fhairsinge, trí fheabhas a chur ar scór F1 suas go dtí 4% in aistriú aonfhoinse agus 8% in aistriú ilfhoinse. Chomh fada agus is eol dúinn, is é ár modh an chéad cheann a chuirfí i bhfeidhm ualú mar shampla i CLTC náid. Tá sé simplí ach éifeachtach agus is furasta é a shíneadh isteach in aistriú ilfhoinse.', 'ka': 'მრავალური ტექსტის კლასიფიკაცია (CLTC) არის უფრო ძალიან ძალიან ძალიან რთული რაოდენობა, რადგან მარტივი მონაცემები არაა მარტივი რესურსის ენაში. ამ დოკუნტში ჩვენ ჩვენ მინდომენეთ ნულ სტატის განმავლობა, სხვა მოდელი-ადნოსტიული ნულ სტატის სტატის სტატისტატისტატისტატისტატის სტატისტატისტატისტა ის დამატებს მოდულის წინ შესწავლილი ენის მოდელეების ზემოთ, რომლებიც კომპუტაციის სიმბოლოების გამოსახულებლად, ამიტომ ყველა ფოსტაციის ინსტანსის მისახულებლად და პრამეტრების განახლების განმავლობაში პარამეტრების განახლებისთვის პარამეტრების განმავლობაში გამოყენება. ჩვენ ამ ფრამეტრის შვიდი მინიშვნელოვანი ენათების განსაზღვრებით სამი ფუნდამეტური დავალებებით და გამოჩვენებთ მისი ეფექტიურობა და გაფართლებელობა, რომელიც F1 წერტილის განსაზღვრებით გაფართლებით 4 ჩვენი მეცნიერების უკეთესი საკმაოდ, ჩვენი მეცნიერი არის პირველი, რომელიც აყენებს ინსტანციას, რომელიც ნულ-სურათის CLTC-ში. ეს მხოლოდ, მაგრამ ეფექტიურია და ადვილურად უფრო გამოყენებელია მრავალური გამოყენებში.', 'hu': 'A többnyelvű szövegosztályozás (CLTC) egy kihívást jelentő feladat, amely még nehezebbé teszi az alacsony erőforrású nyelveken megjelenő címkézett adatok hiányát. Ebben a tanulmányban javasoljuk a zero-shot példánysúlyozást, egy általános modell-agnosztikus zero-shot tanulási keretrendszert, amely a CLTC forrás példánysúlyozás hasznosításával javítja. A példánysúlyok hasonlóságának kiszámítására az előre képzett nyelvi modellek mellett modult ad hozzá, így az egyes források példányait a célnyelvhez igazítja. Az edzés során a keretrendszer olyan gradienssüllyedést használ, amelyet példánysúlyokkal súlyoznak, hogy frissítse a paramétereket. Ezt a keretet hét célnyelven értékeljük három alapvető feladat alapján, és megmutatjuk annak hatékonyságát és kiterjeszthetőségét, mivel az F1 pontszámot 4%-kal javítjuk az egyforrásos transzfer esetében és 8%-kal a többforrásos transzfer esetében. Legjobb tudásunk szerint a mi módszerünk az első, amely példánysúlyozást alkalmaz a nullás CLTC-ben. Egyszerű, mégis hatékony és könnyen kiterjeszthető többforrásos átvitelre.', 'el': 'Η ταξινόμηση των γλωσσών κειμένου (CLTC) είναι ένα δύσκολο έργο που καθίσταται ακόμα πιο δύσκολο λόγω της έλλειψης δεδομένων με ετικέτα σε γλώσσες χαμηλής περιεκτικότητας. Σε αυτή την εργασία, προτείνουμε μηδενική στάθμιση Instanzen, ένα γενικό μοντέλο-agnostic μηδενική-βολή μαθησιακό πλαίσιο για τη βελτίωση της CLTC με τη μόχλευση στάθμισης Instanzen πηγής. Προσθέτει μια ενότητα πάνω από προ-εκπαιδευμένα μοντέλα γλώσσας για τον υπολογισμό ομοιότητας των βαρών παρουσίας, ευθυγραμμίζοντας έτσι κάθε παρουσία προέλευσης με τη γλώσσα-στόχο. Κατά τη διάρκεια της προπόνησης, το πλαίσιο χρησιμοποιεί την κάθοδο κλίσης που σταθμίζεται με βάρη παρουσίας για να ενημερώσει τις παραμέτρους. Αξιολογούμε αυτό το πλαίσιο σε επτά γλώσσες-στόχους σε τρία θεμελιώδη καθήκοντα και καταδεικνύουμε την αποτελεσματικότητά του και την επεκτασιμότητα του, βελτιώνοντας τη βαθμολογία F1 έως 4% στη μεταφορά μιας πηγής και 8% στη μεταφορά πολλών πηγών. Από ό,τι γνωρίζουμε, η μέθοδος μας είναι η πρώτη που εφαρμόζει στάθμιση Instances σε μηδενική βολή CLTC. Είναι απλό αλλά αποτελεσματικό και εύκολα επεκτάσιμο σε μεταφορά πολλαπλών πηγών.', 'it': "La classificazione del testo multilingue (CLTC) è un compito impegnativo reso ancora più difficile a causa della mancanza di dati etichettati in lingue a basso contenuto di risorse. In questo articolo, proponiamo la ponderazione delle istanze zero-shot, un framework generale di apprendimento zero-shot agnostico modello per migliorare la CLTC sfruttando la ponderazione delle istanze sorgente. Aggiunge un modulo oltre ai modelli linguistici pre-addestrati per il calcolo della somiglianza dei pesi delle istanze, allineando così ogni istanza sorgente alla lingua di destinazione. Durante l'allenamento, il framework utilizza la discesa del gradiente ponderata dai pesi delle istanze per aggiornare i parametri. Valutiamo questo framework in sette lingue target su tre compiti fondamentali e ne mostriamo l'efficacia e l'estensibilità, migliorando il punteggio F1 fino al 4% nel trasferimento single-source e all'8% nel trasferimento multi-source. Per quanto ne sappiamo, il nostro metodo è il primo ad applicare la ponderazione delle istanze in CLTC zero-shot. È semplice ma efficace e facilmente estendibile in trasferimento multi-sorgente.", 'mk': 'Кросјазичната класификација на текст (CLTC) е предизвикувачка задача што е уште потешка поради недостатокот на означени податоци на јазици со ниски ресурси. Во овој документ, предложуваме нула-стрела тежирање на пример, генерална модел-агностичка нула-стрела рамка за учење за подобрување на КЛТЦ со влијание на тежирање на извор-пример. Истиот додава модул врз предобучените јазички модели за сличност пресметка на тегови на примероци, со што секоја изворна инстанција се прилагодува на јазикот на целта. За време на обуката, рамката користи пад на градиентот кој се тежи по тегови на инстанциите за да ги оновреди параметрите. Ние ја проценуваме оваа рамка во однос на седум метни јазици на три фундаментални задачи и ја покажуваме нејзината ефикасност и екстрензибилност, со подобрување на оценката F1 до 4 отсто во трансфер од еден извор и 8 отсто во трансфер од повеќе извори. Според нашето знаење, нашиот метод е првиот кој применува тежина на инстанции во нула-снимка КЛТЦ. Тоа е едноставно но ефикасно и лесно проширувачко во трансфер од мултиизвори.', 'lt': 'Tarpkalbinė tekstų klasifikacija (CLTC) yra sunkesnė užduotis, dar sunkesnė dėl pažymėtų duomenų trūkumo mažai išteklių turinčiomis kalbomis. Šiame dokumente siūlome nulinį pavyzdžio koeficientą, bendrą modelinį ir agnostinį nulinį mokymosi sistemą, skirtą CLTC tobulinimui, naudojant svertą šaltinio pavyzdžio koeficientui. Be iš anksto parengtų kalbų modelių pridedamas modulis, skirtas panašumui apskaičiuoti pavyzdžių svorius, taip suderinant kiekvieną šaltinį su tiksline kalba. Mokymo metu sistema, atnaujinant parametrus, naudoja gradientinį žemėjimą, svertinį pagal instancijos svorius. Šią sistemą vertiname per septynias tikslines kalbas pagal tris pagrindines užduotis ir parodysime jos veiksmingumą ir išplėtimą, pagerindami F1 rezultatus iki 4 % vieno šaltinio perdavimo ir 8 % daugialypio perdavimo atveju. Mes geriausiai žinome, kad mūsų metodas yra pirmasis, kuris taiko pavyzdžio svorį nulinio nuotraukos CLTC. Jis paprastas, tačiau veiksmingas ir lengvai išplečiamas į daugialypį perdavimą.', 'kk': 'Төменгі тілді мәтін классификациясы (CLTC) деген шақыру тапсырмасы, әлі жеткілікті ресурстар тілдерінде жарлық деректер жоқ деген себебі көбірек болады. Бұл қағазда, көзі инстанцияларды бағалау үшін CLTC- ді жақсарту үшін жалпы мөлшерлерді бағалау үшін нөл- сүрлеу үлгісін бағалаймыз. Ол мысалы тең есептеу үшін алдын- оқылған тіл үлгілерінің үстінде модульді қосу, сондықтан әрбір көзінің инстанциясын мақсатты тіліне тұрады. Оқыту кезінде, параметрлерді жаңарту үшін градиенттің көтерілігін қолданады. Біз бұл фреймді 7 мақсатты тілдерден үш негізгі тапсырмаларды бағалап, оның эффективнігін және кеңейтілігін көрсету үшін, F1 нүктесін бір көзге аударып, бірнеше көзге аударып, 8% нү Біздің біліміздің ең жақсы мәліметіміздің әдіміміз - нөл-шарт CLTC дегенде бірінші мәліметті қолдану. Бұл көптеген көп көзгертулерге қарапайым және әсер ететін және оңай көмектесуі болады.', 'mt': "Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages.  F’dan id-dokument, qed nipproponu l-ippeżar b’eżempju żero-shot, qafas ġenerali ta’ tagħlim b’eżempju żero-shot mudell-agnostiku għat-titjib tas-CLTC billi ninfurzaw l-ippeżar b’eżempju tas-sors. Huwa jżid modulu fuq mudelli lingwistiċi mħarrġa minn qabel għall-kalkolu ta’ similarità ta’ piżijiet ta’ eżempju, u b’hekk jallinja kull eżempju tas-sors mal-lingwa fil-mira. Matul it-taħriġ, il-qafas juża d-dixxendenza gradjenti li hija peżata skont il-piżijiet tal-istanza biex taġġorna l-parametri. Aħna jevalwaw dan il-qafas fuq seba' lingwi fil-mira fuq tliet kompiti fundamentali u nuru l-effettività u l-estensibbiltà tiegħu, billi ntejbu l-punteġġ F1 sa 4% fit-trasferiment ta' sors wieħed u 8% fit-trasferiment ta' sorsi multipli. Għall-aħjar għarfien tagħna, il-metodu tagħna huwa l-ewwel li tapplika l-ippeżar tal-istanza f’CLTC b’żero-shot. Huwa sempliċi iżda effettiv u faċilment estensibbli fi trasferiment b’diversi sorsi.", 'mn': 'Хөгжлийн текст хэлэлцүүлэлт (CLTC) нь бага нөөцийн хэл дээр нэрлэгдсэн өгөгдлийн алдаагүй учраас илүү хэцүү задалга юм. Энэ цаасан дээр бид Нэг шат жинтэй, нийтлэг загвар-агностик 0 шат суралцах суралцах үйл ажиллагааны хэмжээсүүд нь эх үүсвэрийн жинтэй сайжруулах боломжтой. Энэ нь жинхэнэ жинхэнэ тооцоолох төстэй хэл загварын дээд модуль нэмэгдүүлдэг. Иймээс эх үүсвэрийн тооцоолол бүр зориулагдсан хэл дээр тооцоолж байна. Сургуулалт хийх үед градиентын түвшинг хэрэглэдэг. Жишээ нь жинг нь параметрыг шинэчлэхэд хэрэглэдэг. Бид үүнийг 7 зорилготой хэл дээр 3 үндсэн даалгавар дээр үнэлгээд, үр дүнтэй болон өргөн байдлыг харуулж, F1-ийн тоо нь нэг эх үүсвэрийн шилжүүлэлт 4%, олон эх үүсвэрийн шилжүүлэлт нь 8%. Бидний хамгийн сайн мэдлэгтэй хувьд, бидний арга бол анхны хэрэглэх жишээ нь тэгш шат CLTC-д жинтэй. Энэ нь олон эх үүсвэрийн шилжүүлэлт амархан, үр дүнтэй.', 'ms': 'Klasifikasi teks saling-bahasa (CLTC) adalah tugas mencabar yang dibuat lebih sukar lagi kerana kekurangan data yang ditabel dalam bahasa sumber rendah. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting.  It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language.  Semasa latihan, kerangka menggunakan turun gradien yang berat mengikut berat contoh untuk kemaskini parameter. Kami menilai kerangka ini lebih dari tujuh bahasa sasaran pada tiga tugas dasar dan menunjukkan kegunaan dan keterbatasannya, dengan memperbaiki skor F1 sehingga 4% dalam pemindahan sumber tunggal dan 8% dalam pemindahan sumber berbilang. Untuk yang terbaik dari pengetahuan kita, kaedah kita adalah yang pertama untuk melaksanakan penimbangan contoh dalam CLTC tembakan sifar. Ia mudah namun berkesan dan mudah diterbangkan ke dalam pemindahan sumber berbilang.', 'ml': 'ക്രോസ്- ലാങ്കുകളുടെ പദാവലി ക്ലാസ്സിഫിക്കേഷന്\u200d (CLTC) കുറഞ്ഞ വിഭവഭാഷകളില്\u200d ലേബെല്ല് ചെയ്ത വിവരങ്ങളുടെ ഇല്ലാത്തതിനാല്\u200d വി ഈ പത്രത്തില്\u200d, സിഎല്\u200dടിസിയുടെ സ്രോതസ്സ് എക്സെന്\u200dസിന്\u200dറെ തൂക്കങ്ങള്\u200d മുന്\u200dകൂട്ടിക്കൊടുക്കുന്നതിനായി പൂര്\u200dണ്ണമായ മോഡല്\u200d-ആഗ്നോസ്റ് എക്സ്റ്റാന്\u200dസിന്റെ തൂക്കുകണക്കിനുള്ള മുമ്പ് പരിശീലിക്കപ്പെട്ട ഭാഷ മോഡലുകളുടെ മുകളില്\u200d അത് ഒരു ഘടകം ചേര്\u200dക്കുന്നു. അതുകൊണ് During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters.  മൂന്നു അടിസ്ഥാനപരമായ ജോലികളില്\u200d ഏഴ് ലക്ഷ്യഭാഷകള്\u200dക്കുമേല്\u200d ഈ ഫ്രെയിമെക്ക് നാം വിലയിക്കുകയും, അതിന്\u200dറെ ഫലവും വിശാലതയും കാണിക്കുകയും ചെയ്യുന്നു. ഫെയില നമ്മുടെ അറിവിന്റെ ഏറ്റവും മികച്ച രീതിയില്\u200d നമ്മുടെ രീതിയാണ് പൂര്\u200dണ്ണമായ സിഎല്\u200dടിസിയില്\u200d തൂക്കുന്നതെന്നതിനാ ഇത് വളരെ എളുപ്പമാണ് ഇപ്പോഴും പ്രവര്\u200dത്തനപ്പെടുത്തുന്നതും വളരെ എളുപ്പമുള്ളതും multi-source transfer.', 'pl': 'Klasyfikacja tekstu wielojęzycznego (CLTC) to trudne zadanie, które jeszcze utrudnia brak etykietowanych danych w językach o niskich zasobach. W niniejszym artykule proponujemy zero-shot instance waging, ogólny model-agnostyczny zero-shot framework uczenia się do poprawy CLTC poprzez wykorzystanie wagi instancji źródłowych. Dodaje moduł na wierzchu wstępnie przeszkolonych modeli językowych do obliczania podobieństwa wag instancji, w ten sposób dostosowując każdą instancję źródłową do języka docelowego. Podczas treningu framework wykorzystuje spadek gradientu, który jest ważony przez wagi instancji, aby uaktualnić parametry. Oceniamy te ramy w siedmiu językach docelowych pod kątem trzech podstawowych zadań i pokazujemy jego skuteczność i rozszerzalność, poprzez poprawę wyniku F1 do 4% w transporcie pojedynczego źródła oraz 8% w transporcie wielu źródeł. Według naszej najlepszej wiedzy, nasza metoda jest pierwszą, która stosuje wagę instancji w zero-shot CLTC. Jest prosty, ale skuteczny i łatwo rozszerzyć do transferu wielu źródeł.', 'sr': 'Prejezička klasifikacija teksta (CLTC) je izazovni zadatak koji je još teži zbog nedostatka označenih podataka na jezicima niskih resursa. U ovom papiru predlažemo težinu primjera od nule pucnjave, općem modelu agnostičkom okviru učenja nule pucnjave za poboljšanje CLTC-a uvećanjem težine izvornog primjera. To dodaje modul na vrh predobučenih jezičkih modela za računalo sličnosti težina primjeraka, tako da poravna svaki izvorni primer na ciljni jezik. Tijekom treninga okvir koristi spuštanje gradienta koji je težina primjerom težina za aktualizaciju parametara. Procjenjujemo ovaj okvir preko sedam ciljnih jezika na tri temeljna zadatka i pokazujemo njegovu efikasnost i proširenost, poboljšavajući rezultat F1 do 4% u jednoizvornom prenošenju i 8% u multiizvornom prenošenju. Najbolji od naših znanja, naš metod je prvi koji primjenjuje primjer težine u nulom pucnjavi CLTC. Jednostavno je ali efikasno i lako prošireno u multiizvorski transfer.', 'so': 'Kalajarida qoraalka luuqadaha iskuulka ah (CLTC) waa shaqo dhibaato badan oo aad u adag, sababtoo ah baahida macluumaadka ku qoran luqadaha hoose-resource. Qoraalkan waxaan ka soo jeedaynaa miisaanka tusaale ahaan zero-shot, kaas oo ah koobka waxbarashada nooca-agnostic zero-shot, si loo horumariyo CLTC si aan u sameynno tusaale ahaan noocyada. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language.  Waxbarashada waxaa lagu isticmaalaa fasaxa rasmiga ah oo miisaanka tusaale ahaan u miisaamay si ay u cusboonaato parameters. Shaqoolkan waxaan ku qiimeynayaa todobo luuqadood oo waxqabadka aasaasiga ah, waxaana tusnaa shaqooyinkeeda iyo faa’iidada, marka horumarinta kooxda F1 ilaa 4% in la wareejiyo koox kali ah iyo 8% in la wareejiyo wareejinta asal badan. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC.  Waxaa fudud oo weli shaqeyn kara oo si fudud u sahlan in loo wareejiyo wareejinta Source badan.', 'no': 'Krysspråk tekstklassifikasjon (CLTC) er eit vanskeleg oppgåve som er lagt til og med vanskeleg framleis manglar merkelige data i låg ressursspråk. I denne papiret foreslår vi ein generell modell-agnostisk nullsatt læringsrammeverk for å forbetra CLTC ved å levera vekking av kjeldeinstansen. Det legg til ein modul øvre på føretrengte språk- modeller for liknande rekningar av eksempelvekt, slik at kvar kjeldeinstans skal justerast til målspråket. I læring brukar rammeverket fargeovergangspunktet som er vekt av eksempel vekt for å oppdatera parametrar. Vi evaluerer dette rammeverket over sju målspråk på tre grunnleggjande oppgåver og viser effektiviteten og utvidingar, ved å forbetra F1- punktet opp til 4% i enkelkjeldeoverføring og 8% i fleirkjeldeoverføring. For det beste vitninga vårt er metoden vårt første som brukar instansen som vektar i nullsatt CLTC. Det er enkelt enno effektivt og lett utvidelse i fleire kjeldeoverføring.', 'si': 'විශාල භාෂාවක් පණිවිධානය (CLTC) ක්\u200dරියාත්මක විශ්වාස කරන්න ප්\u200dරශ්නයක් තවමත් වඩා අමාරුයි වෙලා තියෙන්නේ නැත මේ පැත්තට, අපි සුන්ධ වෙඩි තියෙන්නේ සුන්ධ වෙඩි තියෙන්න, සාමාන්\u200dය ප්\u200dරමාණයක් සුන්ධ වෙඩි තියෙන්නේ සුන්ධ වෙඩි තියෙන්න ඒක ප්\u200dරශ්ණ භාෂාව ප්\u200dරශ්ණිත භාෂාව මොඩියුල් එකක් එකතු කරනවා, මොඩියුල් වර්ගයක් සඳහා සාමාන්\u200dයතාවක් ගැන ගණන් පරීක්ෂණය වෙලාවේ, පරීක්ෂණය වැඩ කරන්න සැකසුම් විශාල පරීක්ෂණය ප්\u200dරයෝජනය කරනවා. අපි මේ පරීක්ෂණය විශ්වාස කරන්නේ ඉලක්ෂණ භාෂා 7ක් වලින් මූලික වැඩක් තුනක් වලින් මෙයාගේ පරීක්ෂණය සහ විස්තරාවත් පෙන්වන්න, F1 ප්\u200dර අපේ දැනගන්න හොඳම දේවල්, අපේ විධානය තමයි පළමු විධානය සුන්ධ වෙඩි තියෙන්නේ සුන්ධ වෙඩි CLTC වල. ඒක සරලයි තාමත් ප්\u200dරශ්ණයි, ලේසියෙන් ප්\u200dරශ්ණයක් විශාල වෙන්න පුළුවන්.', 'ro': 'Clasificarea textelor între limbi (CLTC) este o sarcină dificilă și mai dificilă datorită lipsei de date etichetate în limbi cu resurse reduse. În această lucrare, propunem ponderarea instanțelor zero-shot, un cadru general de învățare zero-shot agnostic model pentru îmbunătățirea CLTC prin utilizarea ponderării instanțelor sursă. Acesta adaugă un modul deasupra modelelor lingvistice pre-instruite pentru calcularea similarității greutăților instanțelor, aliniind astfel fiecare instanță sursă la limba țintă. În timpul antrenamentului, cadrul utilizează coborârea gradientului care este ponderată cu greutățile instanței pentru a actualiza parametrii. Evaluăm acest cadru în șapte limbi țintă pe trei sarcini fundamentale și demonstrăm eficacitatea și extensibilitatea acestuia, prin îmbunătățirea scorului F1 cu până la 4% în transferul de o singură sursă și 8% în transferul de mai multe surse. Din câte știm, metoda noastră este prima care aplică ponderea instanțelor în CLTC zero-shot. Este simplu, dar eficient și ușor extensibil în transfer multi-sursă.', 'sv': 'Tvärspråkig textklassificering (CLTC) är en utmanande uppgift som ännu svårare görs på grund av bristen på märkta data på språk med låg resurs. I denna uppsats föreslår vi noll-skott instansvätning, en allmän modell-agnostisk noll-skott inlärningsram för att förbättra CLTC genom att utnyttja källinstansvätning. Den lägger till en modul ovanpå förintränade språkmodeller för liknelsesberäkning av instansvikt, vilket anpassar varje källinstans till målspråket. Under träningen använder ramverket gradient nedstigning som vägs med instansvikt för att uppdatera parametrarna. Vi utvärderar detta ramverk över sju målspråk på tre grundläggande uppgifter och visar dess effektivitet och utbyggbarhet, genom att förbättra F1-poäng med upp till 4% i överföring av en enda källa och 8% i överföring av flera källor. Så vitt vi vet är vår metod den första som tillämpar instansvikt i noll-skott CLTC. Det är enkelt men effektivt och lätt att utöka till överföring av flera källor.', 'ta': 'கிராஸ்- மொழி உரை வகைப்படுத்தல் (CLTC) ஒரு சவால் செய்யும் பணி இந்த காகிதத்தில், நாம் பூஜ்ஜியத்தை எடுத்துக் கொள்ள முடியும், ஒரு பொதுவான மாதிரி- கான்ஸ்டிக் பூஜ்ஜியத்தை கற்றுக் கொள்ளும் சட்டத்த It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language.  பயிற்சியில், சட்டத்தின் அளபுருக்களை புதுப்பிக்க முடியும் நிமிடத்தின் அளவுருக்களால் எடுக்கப்பட்டுள்ள சர மூன்று அடிப்படை பணிகளில் இந்த சட்டத்தை ஏழு இலக்கு மொழிகளுக்கு மேல் மதிப்பிடுகிறோம் மற்றும் அதன் விருப்பத்தையும் விரிவாக்கத்தையும் காட்டுகிறோ எங்கள் அறிவின் சிறந்த முறையில், எங்கள் முறைமையாகும் பூஜ்ஜியத்தில் சுட்டு CLTC-ல் எடுக்கும் நிகழ்வுகளை பயன்பட இது சுலபமானதும் இன்னும் செயல்படுத்தலும் எளிதாகவும் பல மூலத்தை மாற்றுதலுக்கு எளிதாகவும் உள்ளது.', 'ur': 'Cross-lingual text classification (CLTC) ایک مشکل کام ہے جو اس سے بھی زیادہ مشکل کیا گیا ہے کہ کم-resource زبانوں میں لابلیٹ ڈیٹا نہیں ہوتا۔ اس کاغذ میں، ہم صفر-شٹ کی مثال وزن کی پیشنهاد کریں گے، ایک عمومی موڈل-agnostic صفر-شٹ کی تعلیم فرمیک CLTC کو بہتر کرنے کے لئے سراسر مثال وزن کے ذریعہ. یہ مثال وزن کے مطابق مثال کی کمپیوٹریت کے لئے پیش آموزش کی زبان موڈل کے اوپر ایک موڈل اضافہ کرتا ہے، یہاں تک کہ ہر سراسر مثال موجود زبان کے ساتھ متصل کرتا ہے. تعلیم کے موقع، فرمیٹ گریڈینٹ سٹونٹ کا استعمال کرتا ہے جو مثال وزن کے ذریعہ تول کی جاتی ہے پارامیٹوں کو آدٹ کرنے کے لئے۔ ہم اسے سات موجود زبانوں پر مطابق کرتے ہیں تین بنیادی کاموں پر اور اسے اثبات اور پھیلانے والی دکھاتے ہیں، F1 اسکور پر 4% تک زیادہ مطابق کر رہے ہیں، ایک سورج ترنسفور میں اور 8% ملتی سورج ترنسفور میں۔ ہمارے سب سے بہترین علم کے لئے، ہمارا طریقہ سب سے پہلا ہے جو صفر-شٹ CLTC میں وزن کرتا ہے۔ یہ بہت سادہ اور آسان ہے اور بہت سادہ سورج ٹرانسٹر میں پھیلانے والا ہے.', 'uz': "Name Bu qogʻozda, biz nuqta shaklga o'zgarishni tahlil qilamiz, umumiy model-agnostik nuqta o'rganishni o'rganish chegarasini o'rganish natijasi bilan CLTC yordamida o'zgartirish mumkin. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language.  Tashqi davomida, freym moslamalarini yangilash uchun gradient descent yordamida foydalanadi. Biz bu freymni uchta asosiy vazifalarda uchta asosiy vazifalar bilan qiymatmiz va uning effektligini va kengaytmalarini ko'rsamiz, F1 qiymatini bir manba uzoqlarida 4% ko'proq qiymatga oshirish va 8% ko'proq manba uzoqlarida ko'paytirish mumkin. Bizning eng yaxshi aniqligimizga, bizning usuli CLTC bilan birinchi misol qo'llash mumkin. Bu juda oddiy va juda foydalanish va ko'plab manba tarkibini ajratish mumkin.", 'vi': 'Phân loại văn bản xuyên ngôn ngữ (CLTC) là một nhiệm vụ khó khăn còn khó khăn hơn bởi vì không có dữ liệu được đánh dấu ở các ngôn ngữ ít tài nguyên. Trong tờ giấy này, chúng tôi đề nghị cân bằng trường hợp không bắn, một cơ sở học không nhắm tổng quát chuẩn chuẩn chuẩn để cải tiến cử cử cử cử cử CLTC bằng cách dùng cân bằng các trường hợp gốc. Nó thêm một mô- đun trên đầu các mô- đun ngôn ngữ được đào tạo để tính to án nét tương tự các tạ thể, bằng cách chỉnh mỗi tiến trình độc tố vào ngôn ngữ đích. Trong thời gian huấn luyện, cơ chế sử dụng độ cao có dốc được cân bằng bằng bằng các đường tạ để cập nhật các thông số. Chúng tôi đánh giá bộ dạng này hơn bảy ngôn ngữ tiêu đề về ba nhiệm vụ cơ bản và thể hiện hiệu quả và độ rộng của nó, bằng cách cải thiện điểm F1 trên bốn phần trăm trong việc chuyển nhượng một nguồn và 8+ trong việc chuyển nhượng đa nguồn. Theo những gì chúng tôi biết, phương pháp của chúng tôi là phương pháp đầu tiên áp dụng cân bằng tiến trình không bắn trong CLTC. Nó đơn giản nhưng hiệu quả và dễ mở rộng trong việc chuyển nhượng đa nguồn.', 'nl': 'Cross-lingual text classification (CLTC) is een uitdagende taak die nog moeilijker wordt door het ontbreken van gelabelde gegevens in low-resource talen. In dit artikel stellen we nul-shot instance-weging voor, een algemeen model-agnostisch zero-shot leerframework voor het verbeteren van CLTC door gebruik te maken van broninstance weging. Het voegt een module toe bovenop vooraf getrainde taalmodellen voor de berekening van de gelijkenis van instancegewichten, waardoor elke broninstantie op de doeltaal wordt uitgelijnd. Tijdens de training maakt het framework gebruik van gradiënt afdaling die wordt gewogen door instancegewichten om parameters bij te werken. We evalueren dit raamwerk over zeven doeltalen op drie fundamentele taken en laten de effectiviteit en uitbreidbaarheid zien, door de F1 score te verbeteren tot 4% bij single-source transfer en 8% bij multi-source transfer. Voor zover wij weten is onze methode de eerste die instanceweging toepast in zero-shot CLTC. Het is eenvoudig maar effectief en gemakkelijk uit te breiden naar multi-source overdracht.', 'hr': 'Prejezička klasifikacija teksta (CLTC) je izazovni zadatak koji je još teži zbog nedostatka označenih podataka na jezicima niskih resursa. U ovom papiru predlažemo težinu primjera nule snimanja, općeg model a agnostičkog okvira učenja nule snimanja za poboljšanje CLTC-a s mjerom težine izvornog primjera. To dodaje modul na vrh predobučenih jezičkih modela za računalo sličnosti težina primjeraka, što se tako uključuje svaki izvorni primjer ciljnom jeziku. Tijekom treninga okvir koristi propast gradienta koji se teži primjerice težine za aktualizaciju parametara. Procjenjujemo ovaj okvir preko sedam ciljnih jezika na tri temeljna zadatka i pokazujemo njegovu učinkovitost i proširenost, poboljšavajući rezultat F1 do 4% u prijenosu jednoizvora i 8% u multiizvornom prijenosu. Najbolje od naših znanja, naša metoda je prva koja primjenjuje primjer težine u nulom pucnjavi CLTC. Jednostavno je ali efikasno i lako prošireno u multiizvorni transfer.', 'bg': 'Междуезичната класификация на текста (CLTC) е трудна задача, която е още по-трудна поради липсата на етикетирани данни на езици с нисък ресурс. В тази статия ние предлагаме нулево теглене на инстанции, обща модел-агностична рамка за обучение с нулеви изстрели за подобряване на CLTC чрез използване на тегленето на инстанции източник. Той добавя модул върху предварително обучените езикови модели за изчисляване на сходство на теглото на инстанциите, като по този начин подравнява всеки изходен инстанция към целевия език. По време на обучението рамката използва градиентно спускане, което се претегля от тежести на инстанциите за актуализиране на параметрите. Ние оценяваме тази рамка на седем целеви езика по три основни задачи и показваме нейната ефективност и разширяемост, като подобряваме резултата до 4% при трансфера от един източник и 8% при трансфера от няколко източника. Доколкото знаем, нашият метод е първият, който прилага претегляне на инстанции при нулев изстрел. Той е прост, но ефективен и лесно се разширява в многоизточник трансфер.', 'da': 'Krydsproget tekstklassifikation (CLTC) er en udfordrende opgave, der er gjort endnu sværere på grund af manglen på mærkede data på sprog med lav ressource. I denne artikel foreslår vi nul-shot instansvægtning, en generel model-agnostisk nul-shot læringsramme til forbedring af CLTC ved at udnytte kilde instansvægtning. Det tilføjer et modul oven på forududdannede sprogmodeller til lighedsberegning af eksempelvægte, således at hver kilde instans justeres til målsproget. Under træningen bruger rammen gradient nedstigning, der vægtes af instansvægte for at opdatere parametre. Vi evaluerer denne ramme over syv målsprog på tre grundlæggende opgaver og viser dens effektivitet og udvidelighed ved at forbedre F1 score op til 4% i single-source overførsel og 8% i multi-source overførsel. Så vidt vi ved, er vores metode den første til at anvende instansvægtning i nulskud CLTC. Det er enkelt, men effektivt og let udvides til multi-source overførsel.', 'fa': 'کلیسازی متن عبارت زبانی (CLTC) یک کار سخت\u200cتر است که هنوز به دلیل ناتوانی داده\u200cهای نقاشی در زبانهای منابع کم سخت\u200cتر انجام می\u200cشود. در این کاغذ، ما پیشنهاد می\u200cکنیم وزن نمونه\u200cهای صفر، یک چهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچ این مدل بر بالای مدل زبان پیش آموزش شده برای محاسبات شبیه وزن مثال را اضافه می\u200cکند، بنابراین هر مثال منبع را به زبان هدف تنظیم می\u200cکند. در طول تمرین، چهارچوب سقوط گراده را استفاده می\u200cکند که با توجه به وزن مثال برای آغاز کردن پارامترها وزن می\u200cشود. ما این چهارچوب را بر روی هفت زبان هدف بر سه کار بنیادی ارزیابی می کنیم و با افزایش امتیاز F1 تا ۴ درصد در انتقال یک منبع و ۸ درصد در انتقال چندین منبع نشان می دهیم. برای بهترین دانش ما، روش ما اولین نفر است که تولید نمونه وزن در CLTC صفر است. این ساده است که هنوز موثر است و به آسانی در انتقال منبع متعدد است.', 'id': 'Klasifikasi teks saling bahasa (CLTC) adalah tugas yang menantang yang masih lebih sulit karena kekurangan data yang ditabel dalam bahasa sumber daya rendah. Dalam kertas ini, kami mengusulkan pembagian contoh-zero-shot, model-agnostik model-zero-shot-learning framework untuk meningkatkan CLTC dengan menggunakan pembagian contoh sumber. Ini menambah modul di atas model bahasa yang dilatih-dilatih untuk penghitungan persamaan berat contoh, sehingga mengalirkan setiap contoh sumber ke bahasa sasaran. Selama latihan, kerangka menggunakan turun gradien yang berat oleh berat instance untuk memperbarui parameter. Kami mengevaluasi rangkaian ini lebih dari tujuh bahasa sasaran pada tiga tugas dasar dan menunjukkan efektifitas dan ekstensibilitas, dengan memperbaiki skor F1 sampai 4% dalam transfer sumber tunggal dan 8% dalam transfer sumber berbilang. Untuk yang terbaik dari pengetahuan kita, metode kita adalah yang pertama untuk menerapkan berat contoh dalam CLTC 0-shot. Ini sederhana namun efektif dan mudah diperluaskan ke transfer multi-sumber.', 'sw': 'Utambazaji wa maandishi ya lugha (CLTC) ni kazi yenye changamoto ambazo bado ni vigumu zaidi kutokana na ukosefu wa taarifa zilizowekwa katika lugha ndogo ya rasilimali. Katika gazeti hili, tunapendekeza uzito wa mifano sifuri, mfumo wa kujifunza kwa mifano yenye picha sifuri kwa kuboresha CLTC kwa kutumia mizani ya vyanzo vya habari. Inaongeza kifaa juu ya mifano ya lugha zilizofunzwa kabla kwa ajili ya kuhesabu mizani sawa na mifano, kwa hiyo inaongeza kila aina ya chanzo katika lugha inayolenga. Wakati wa mafunzo, mfumo huo unatumia kuongezeka kwa kiwango kinachovumiwa kwa kiasi kikubwa ili upya vipimo. Tutathmini mfumo huu zaidi ya lugha saba zenye lengo katika kazi tatu za msingi na kuonyesha ufanisi wake na uwezekano wake, kwa kuboresha vipindi vya F1 hadi asilimia 4 katika uhamishaji wa vyanzo pekee na asilimia 8 katika uhamishaji wa vyanzo vingi vingi. Kwa ufahamu mzuri zaidi, mbinu yetu ni ya kwanza kutumia mifano yenye uzito katika CLTC. Ni rahisi na bado ni yenye ufanisi na kwa urahisi katika uhamishaji wa vyanzo vingi.', 'tr': "Çot-dilli metin klasifikasy Bu kagyzda, biz nul atly örnek taýýarlamagy teklip edýäris, çeşme örneklerini taýýarlamak üçin umumy mod-agnostik nul atly öwrenme frameworky. Öň bellenen diller nusgalarynyň üstünde bir modul ekleýär. Öň ýaly çyzgylyklar üçin bir modul ekleýär, şonuň üçin her çeşme durumyny maksady diline çykarýar. Okuwçylyk wagty, çerýädçi parameterleriň güncellemegi üçin pikirlenýän градиент aklymyny ulanýar. Biz bu Framework üç esasy gören üç maksady dilinde deňleýäris we onuň täsirini we golaýlygyny görkez, F1'den ýeke-çeşme suwlamasynda 4% we multi-çeşme suwlamasynda 8%. Bilgimiziň iň gowy ýoly üçin, biziň ýolymyz nul atly CLTC-iň ağırlygynyň ilkinji ýagdaýdyr. Bu ýeňil hem täsirli we aňsat bir çoklu çeşme göçürmekde.", 'sq': 'Klasifikimi i tekstit ndërgjuhësor (CLTC) është një detyrë sfiduese e bërë edhe më e vështirë për shkak të mungesës së të dhënave të etiketuara në gjuhët me burime të ulëta. Në këtë letër, propozojmë peshimin zero-shot në rast, një kuadër të përgjithshëm model-agnostik zero-shot mësim për përmirësimin e CLTC duke përdorur peshimin e burimit. Ai shton një modul mbi modelet e gjuhës së paratrajnuar për llogaritjen e ngjashmërisë të peshëve të rasteve, duke e renditur kështu çdo rast burimi me gjuhën e objektivit. Gjatë stërvitjes, kuadri përdor uljen e gradientit që peshon nga peshat e rastit për të përditësuar parametrat. Ne vlerësojmë këtë kuadër mbi shtatë gjuhë objektive në tre detyra themelore dhe tregojmë efektshmërinë dhe zgjerueshmërinë e saj, duke përmirësuar rezultatin e F1 deri në 4% në transferimin me burim të vetëm dhe 8% në transferimin me shumë burime. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC.  Është e thjeshtë por e efektshme dhe lehtë e zgjerueshme në transferimin me shumë burime.', 'af': "Kruistale teks klasifikasie (CLTC) is 'n pragtige taak wat nog harder gemaak is vanweë die mislukking van etiketeerde data in lae- hulpbron tale. In hierdie papier, voorstel ons nul-skoot voorbeeld-gewig, 'n algemene model-agnostiese nul-skoot leer raamwerk vir verbetering van CLTC deur gebruik van bron voorbeeld gewig te maak. Dit byvoeg 'n module bo-op van voorafgevorderde taal modele vir gelykenis rekenaar van voorbeeld gewigte, sodat elke bron voorbeeld aan die doel taal gelyk word. Die raamwerk gebruik gradient descent wat geweeg word deur voorbeeld weegte na opdateer parameters. Ons evalueer hierdie raamwerk oor sewe doel tale op drie fundamentele taak en wys sy effektiviteit en uitbreidigheid deur die verbetering van F1 aantal tot 4% in enkelke bron oordrag en 8% in multibron oordrag. Op die beste van ons kennis is ons metode die eerste om voorbeeld te wend wat gewig in nul-skoot CLTC. Dit is eenvoudig nog effektief en maklik uitbreidig in multibronne oordrag.", 'ko': '다국어 텍스트 분류(CLTC)는 도전적인 작업으로 저자원 언어의 표기 데이터가 부족하기 때문에 이 작업은 더욱 어려워진다.본고에서 우리는 제로 렌즈 실례 가중치를 제기했다. 이것은 통용되는 모델로 제로 렌즈 학습 구조와 무관하고 원본 실례 가중치를 이용하여CLTC를 개선한다.이것은 미리 훈련된 언어 모델 위에 실례를 중시하는 유사성 계산에 사용되는 모듈을 추가하여 모든 원본의 실례를 목표 언어와 맞춘다.훈련 과정에서 이 프레임워크는 실례권 가중권의 사다리를 이용하여 파라미터를 업데이트한다.우리는 세 가지 기본 임무에서 일곱 가지 목표 언어의 구조를 평가했고 단원 전송과 다원 전송에서 각각 F1 점수를 4%와 8% 높여 유효성과 확장성을 보여 주었다.Dell의 접근 방식은 CLTC 제로포에 인스턴스 가중치를 적용하는 첫 번째 방법인 것으로 알고 있습니다.그것은 간단하고 효과적이며 다원 전송으로 확장하기 쉽다.', 'de': 'Die crosslingual text classification (CLTC) ist eine anspruchsvolle Aufgabe, die aufgrund des Fehlens von markierten Daten in ressourcenarmen Sprachen noch schwieriger wird. In diesem Beitrag schlagen wir Zero-Shot Instance-Gewichtung vor, ein allgemeines Modell-agnostisches Zero-Shot Learning Framework zur Verbesserung von CLTC durch Nutzung der Quellinstanzgewichtung. Es fügt ein Modul zu vortrainierten Sprachmodellen für die Ähnlichkeitsberechnung von Instanzgewichten hinzu, wodurch jede Quellinstanz auf die Zielsprache ausgerichtet wird. Während des Trainings nutzt das Framework Gradientenabgang, der durch Instanzgewichte gewichtet wird, um Parameter zu aktualisieren. Wir evaluieren dieses Framework über sieben Zielsprachen auf drei grundlegende Aufgaben und zeigen seine Effektivität und Erweiterbarkeit auf, indem wir den F1-Score bis zu 4% im Single-Source-Transfer und 8% im Multi-Source-Transfer verbessern. Nach bestem Wissen ist unsere Methode die erste, die Instanzgewichtung in Zero-Shot CLTC anwendet. Es ist einfach, aber effektiv und leicht in Multi-Source-Übertragung erweiterbar.', 'am': 'የቋንቋ-ቋንቋ ጽሑፍ መግለጫ (CLTC) በተጨማሪው ቋንቋዎች ውስጥ የጽሑፍ መረጃዎች በጭንቀት በማድረግ የሚደረግ ስራ ነው፡፡ በዚህ ካላት፣ የኮሌክቲ ምሳሌ-ሚዛን፣ የሞዴል-አጎኖስቲክ የzero-ፎቶ ትምህርት ፍሬም በኩል CLTC በመስመር በሚዛን በመስጠት ያሳልፋል፡፡ ለምሳሌ ሚዛን ለመቆጣጠር በፊት የተማረከ ቋንቋ ሞዴላዎችን ላይ የሚጨምር ሞዴል ይጨምርበታል፥ እንዲሁም የምንጭ ምሳሌ ምሳሌዎችን ለtarget ቋንቋ ያሳርፋል። በአስተማሪው ጊዜ የሥርዓት ምርጫዎች ማሻሻል በሚዛን በሚያስቀምጥ የgradient descent ይጠቅማል፡፡ ይህንን ፍሬማር በሰባት ደረጃ ቋንቋዎች ላይ በሦስት መሠረታዊ ስራ እናሳውቃለን እና ፍሬታዋን እና ስፋታውን እናሳያቸዋለን፡፡ ከውቀታችን የተሻለ ሆኖ የCLTC ምሳሌ መፍጠር መጀመሪያ ነው፡፡ ቀላል እና በብዙ-source መዘዋወር የሚቻል ነው፡፡', 'az': "Çərz dilli metin klasifikasyonu (CLTC) düşük ressurs dillərində etiketli məlumatlar yoxdur. Bu kağızda, sıfır-atlı örnek-a ğırlığını təklif edirik, genel model-agnostik sıfır-atlı öyrənmə framework ü CLTC'i yaxşılaşdırmaq üçün mənbə örneklərinin ağırlığını təmin edir. Bu, məsəllərin a ğırlığı hesablaması üçün əvvəlcə təhsil edilmiş dil modellerinin üstündə modulu əlavə edir, beləliklə hər meyvənin məsəllərini məqsəd dilinə tərəf çəkir. Eğitimin sırasında, çerçive parametrləri güncelleştirmək üçün çəkilən səviyyənin düşüşünü istifadə edir. Biz bu framework ü yeddi məqsəd dilində üç temel işlərdə değerləşdiririk və onun etkinlik və genişliyini göstərdik, F1 dəyişikliyi tək mənbə transferisində 4%-ə qədər yaxşılaşdırır və çoxlu mənbə transferisində 8%-ə qədər yaxşılaşdırırıq. Bizim elmimizin ən yaxşısına gəldikdə, bizim metodumuz sıfır-vuruş CLTC içində çəkilən məsələlərdən birincisi istifadə edəndir. Bu çoxlu mənbə tərəfindən asanlıqla və çoxlu istifadə ediləndir.", 'bs': 'Prejezička klasifikacija teksta (CLTC) je izazovni zadatak koji je još teži zbog nedostatka označenih podataka na jezicima niskih resursa. U ovom papiru predlažemo težinu primjera nule snimke, općeg model a agnostičnog okvira učenja nule snimke za poboljšanje CLTC-a, uvećavajući težinu izvornog primjera. To dodaje modul na vrh predobučenih jezičkih modela za računalo sličnosti težina primjeraka, pa se tako uključuje svaki izvorski primjer na ciljni jezik. Tijekom treninga okvir koristi spuštanje gradienta koji je težina primjerice težina za aktualizaciju parametara. Procjenjujemo ovaj okvir preko sedam ciljnih jezika na tri temeljna zadatka i pokazujemo njegovu učinkovitost i proširenost, poboljšavajući rezultat F1 do 4% u jednoizvornom prenošenju i 8% u multiizvornom prenošenju. Najbolje od našeg znanja, naš metod je prvi koji primjenjuje primjer težine u nulom pucnjavi CLTC. Jednostavno je ali efikasno i lako prošireno u multiizvorski transfer.', 'hy': "Խաղալեզվով տեքստի դասակարգման (CILT) խնդիր է, որը դեռևս ավելի դժվար է դարձնում այն պատճառով, որ ցածր ռեսուրսների լեզուներում պիտակուցված տվյալներ պակաս են: Այս թղթի մեջ մենք առաջարկում ենք զրոյական օրինակի կենտրոնացումը, ընդհանուր մոդել-ագնոստիկ զրոյական կենտրոնացումների ուսումնասիրության շրջանակը, որպեսզի բարելավենք ԿԼԹԿ-ը՝ օգտագործելով աղբյուրի Այն ավելացնում է մոդուլը նախապատրաստված լեզվի մոդելների վրա, օրինակ կշիռների նմանության հաշվարկների համար, այնպես հավասարեցնելով յուրաքանչյուր աղբյուր օրինակը նպատակային լեզվին: Արժեքի ընթացքում կառուցվածքը օգտագործում է մակարդակի նվազում, որը կշռում է օրինակի կշռություններով' վերականգնելու պարամետրերը: Մենք գնահատում ենք այս շրջանակը յոթ նպատակային լեզուների ընթացքում երեք հիմնական առաջադրանքների վրա և ցույց ենք տալիս դրա արդյունավետությունը և ընդլայնելիությունը, բարելավելով F1-ի գնահատականը մինչև 4 տոկոս մեկ աղբյուրի փոխանցման և 8 տո Մեր գիտելիքներից լավագույնն այն է, որ մեր մեթոդը առաջինն է, որ օգտագործում է օրինակի կենտրոնացումը զրոյական կտրոնացման համակարգում: Այն պարզ է, բայց արդյունավետ, և հեշտությամբ ընդլայնելի է բազմա աղբյուրների փոխանցման մեջ:", 'ca': "La classificació translingüística del text (CLTC) és una tasca difícil encara més difícil a causa de la falta de dades etiquetades en llengües de baix recursos. En aquest paper, proposem un quadre general d'aprenentatge model-agnòstic zero-shot per a millorar el CLTC aprofitant la ponderació de les instances fonts. Engadiu un módul a sobre de models de llenguatge pré-entrenats per a calcular la similitud de pes d'instants, alliniant cada instant de font al llenguatge d'objectiu. Durant l'entrenament, el marc utilitza una descendència de gradient pesada per pesos instantàries per actualitzar els paràmetres. Evaluam aquest marc sobre set llengües d'objectiu en tres tasques fonamentals i mostram la seva eficacia i extensibilitat, millorant la puntuació F1 fins al 4% en transfer ència d'una sola font i l'8% en transferència de múltiples fonts. Per millor del nostre coneixement, el nostre mètode és el primer a aplicar ponderació instantària en CLTC de fotografies zero. És simple però efectiu i fàcilment extensible a la transfer ència de múltiples fonts.", 'bn': 'ক্রস-ভাষায় টেক্সট ক্লাস্ফিকেশন (সিএলটিসি) একটি চ্যালেঞ্জালিকার কাজ এখনো আরো কঠিন হয়েছে কম সম্পদ ভাষায় লেবেলেড ডাটা অভাবে। এই কাগজটিতে আমরা প্রস্তাব করি শুধুমাত্র গুলি বাজানোর জন্য, একটি সাধারণ মডেল-অ্যাঙ্নোস্টিক শিক্ষা শিক্ষা ফ্রেম্যাকার্ক, সিএলটিসির এটি পূর্ব প্রশিক্ষিত ভাষার মডেলের উপর একটি মডিউল যোগ করে যোগ করে, যার ফলে প্রত্যেকটি উৎসের উদ্দেশ্য লক্ষ্য ভাষার সাথে সামান্য করা হয়। প্রশিক্ষণের সময়, ফ্রেমের কাঠামো গ্রেডিয়েন্ড ডেজ ব্যবহার করে যা উদাহরণস্বরূপ পরামিটার আপডেট করতে পারে। আমরা তিনটি মৌলিক কাজের উপর সাত টার্গেট ভাষার উপর এই ফ্রেমের মূল্য মূল্যায়ন করি এবং তার কার্যকরী এবং প্রসারিত বিষয়টি দেখাই, এক সোর্স পরিবর্তনে এফ১ স্কোরে ৪ আমাদের জ্ঞানের সবচেয়ে ভালোর জন্য, আমাদের পদ্ধতি হচ্ছে প্রথম যে উদাহরণ নির্ধারণ করা হচ্ছে শুধুমাত্র সিএলটিসিতে  এটা সাধারণ এখনো কার্যকর এবং সহজে বহুস্থ সোর্স পরিবর্তনে প্রস্তুত।', 'et': 'Keeleülene teksti klassifitseerimine (CLTC) on keeruline ülesanne, mis muutub veelgi raskemaks märgistatud andmete puudumise tõttu vähese ressursiga keeltes. Käesolevas dokumendis pakume välja null-shot instance kaalumine, üldine mudeliga agnostiline null-shot õpperaamistik CLTC parandamiseks lähte-instance kaalumine võimendades. See lisab eelnevalt koolitatud keelemudelitele mooduli eksemplaride kaalude sarnasuse arvutamiseks, viies seega iga lähteeksemplari sihtkeelega vastavusse. Treeningu ajal kasutab raamistik parameetrite värskendamiseks gradienti laskumist, mida kaalutakse eksemplari kaaludega. Hindame seda raamistikku seitsmes sihtkeeles kolmes põhiülesandes ning näitame selle tõhusust ja laiendatavust, parandades F1 skoori kuni 4% ühe allika edastamisel ja 8% mitme allika edastamisel. Meie parimate teadmiste kohaselt on meie meetod esimene, mis rakendab eksemplari kaalumist null-shot CLTC-s. See on lihtne, kuid tõhus ja kergesti laiendatav mitme allika edastamiseks.', 'cs': 'Klasifikace textu mezi jazyky (CLTC) je náročným úkolem, který je ještě těžší kvůli nedostatku označených dat v jazycích s nízkými zdroji. V tomto článku navrhujeme nulovou váhu instancí, obecný model-agnostický nulový učební rámec pro zlepšení CLTC využitím vážení zdrojových instancí. Přidává modul na vrchol předškolených jazykových modelů pro výpočet podobnosti váhy instancí, čímž každou zdrojovou instanci zarovnává s cílovým jazykem. Během tréninku využívá rámec sestup gradientu, který je vážen instancí pro aktualizaci parametrů. Tento rámec hodnotíme v sedmi cílových jazycích na třech základních úkolech a ukazujeme jeho efektivitu a rozšířitelnost zlepšením skóre F1 až 4% v přenosu jednoho zdroje a 8% v přenosu více zdrojů. Podle našich nejlepších znalostí je naše metoda první, která aplikuje instanční váhu v nulovém CLTC. Je jednoduchý, ale efektivní a snadno rozšířitelný do přenosu více zdrojů.', 'fi': 'Kieltenvälinen tekstiluokitus (Cross-Language Text Classification, CLTC) on haastava tehtävä, joka vaikeutuu entisestään, koska vähäresurssisilla kielillä ei ole merkintää. Tässä artikkelissa ehdotamme nollakuvan instanssipainotusta, yleistä malliagnostista nollakuvan oppimiskehystä CLTC:n parantamiseksi hyödyntämällä lähdekoodin instanssipainotusta. Se lisää moduulin esikoulutettujen kielimallien päälle instanssipainojen samankaltaisuuden laskentaan, jolloin jokainen lähdeesiintymä sovitetaan kohdekieleen. Harjoittelun aikana viitekehyksessä käytetään gradientin laskua, jota painotetaan instanssipainoilla parametrien päivittämiseksi. Arvioimme tätä viitekehystä seitsemällä kohdekielellä kolmessa perustehtävässä ja osoitamme sen tehokkuuden ja laajennettavuuden parantamalla F1-pisteitä jopa 4% yhden lähteen siirrossa ja 8% useiden lähteiden siirrossa. Tietojemme mukaan menetelmämme on ensimmäinen, joka soveltaa instanssipainotusta nollalaukauksen CLTC:ssä. Se on yksinkertainen mutta tehokas ja helposti laajennettavissa monilähdesiirtoon.', 'ha': "Classification na matsayin mai fassara na-linguin (CLTC) yana da wani aikin mai musamma wanda aka aikata yana da mafi tsanani, kuma amma sun yi ƙaranci da manyan data na rubutu cikin harshen-resource. Ga wannan takardan, Munã buɗa masu sami-nau'i da sikẽli, wata firam mai motsi-agnostic-nufi wa'anar-zane wa ya kyautata CLTC da kuma da mai gajiya misali na'ura. Yana ƙara wata module kan samun misãlai na zaman wanda aka yi wa zaman tsari da wa lissafi masu nau'i na misali, don haka yana daidaita kowace misali na farko zuwa harshen wanda aka yi amfani da. During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters.  Mu ƙaddara wannan firam a kan harshen saba masu goani a kan aikin aiki uku na rubutu kuma Mu nuna fassararsa da iyalinsa, a kan gyaranta na F1 score up to 4% cikin transfer-source guda da 8% a shige multi-source. Ga mafi kyaun saninmu, metodenmu ne na farkon da za'a ajiye misali da za'a sami cikin CLTC wanda aka yi nufi. Ina kasa mai amfani ko kuma mai sauƙi zuwa transfer-source masu yawa.", 'he': 'מסווג טקסט צלב-שפתי (CLTC) הוא משימה מאתגרת שעושה אפילו יותר קשה עדיין בגלל חוסר נתונים מסוימים בשפות משאבים נמוכות. בעיתון הזה, אנו מציעים משקל דוגמא אפס-ירי, סגרת לימוד דוגמא-אגנוסטית אפס-ירי כללית לשיפור CLTC על ידי השימוש משקל מקור דוגמא. הוא מוסיף מודול על גבי דוגמני שפה מאומנים מראש לחישוב דומה של משקולות דוגמאות, כך מאיין כל דוגמא מקור לשפה המטרה. During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters.  אנו מעריכים את המסגרת הזו מעל שבעה שפות מטרה על שלושה משימות בסיסיות ולהראות את היעילות והתוארות שלה, על ידי שיפור בתוצאות F1 עד 4% בהעברה מקור אחד ו-8% בהעברה מקור רב. לפי הידע הטוב ביותר שלנו, השיטה שלנו היא הראשונה להפעיל משקל דוגמאות ב-0-shot CLTC. היא פשוטה, אך יעילה, וקל להתרחב לתוך העברה ממקורים רבים.', 'bo': 'Cross-lingual text classification (CLTC) is a challenging task made even harder because of the lack of labeled data in low-resource languages. ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་རྣམ་པ་ལྡན་པའི་དཔེ་བརྗོད་རྐྱེན་ཚད་ལྡན་དགོས་པ་ཞིག་ལ་སྤྲོད་རྒྱུ་དང་། It adds a module on top of pre-trained language models for similar computing of instance weights, thus aligning each source instance to the target language. During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters. ང་ཚོས་གཞུང་གཞུང་གི་བྱ ང་ཚོའི་ཤེས་ཚད་དགའ་ཕྱོགས་ཀྱི་ལེགས་ཤོག་ཚད་ལ། ང་ཚོའི་ཐབས་ལམ་ནི་དཔེར་བརྗོད་འདི་(zero-shot CLTC)ནང་བྱ་རྐྱེན་ཚད་ དེ་ནི་སླ་བོ་ཞིག་ཡིན་ནའང་ལག་སྟར་བྱེད་རུང་བ་དང་སླ་པོ་རྒྱ་བསྐྱེད་ཐབས་ཤིག་རེད།', 'sk': 'Medjezična klasifikacija besedila (CLTC) je zahtevna naloga, ki je še otežena zaradi pomanjkanja označenih podatkov v jezikih z nizkimi viri. V tem prispevku predlagamo ničelno uteževanje primerov, splošni model agnostični ničelni učni okvir za izboljšanje CLTC z uporabo uteževanja izvornih primerov. Poleg vnaprej usposobljenih jezikovnih modelov doda modul za izračun podobnosti uteži primerkov, s čimer se vsak izvorni primerek uskladi s ciljnim jezikom. Med usposabljanjem okvir uporablja spuščanje gradientov, ki se tehtajo z utežmi primerkov za posodobitev parametrov. Ta okvir ocenjujemo v sedmih ciljnih jezikih pri treh temeljnih nalogah in pokažemo njegovo učinkovitost in razširljivost, tako da izboljšamo oceno F1 do 4% pri prenosu enega vira in 8% pri prenosu več virov. Kolikor vemo, je naša metoda prva, ki uporablja tehtanje primerkov v CLTC brez strela. Je preprost, vendar učinkovit in enostavno razširljiv v prenos več virov.', 'jv': 'politenessoffpolite"), and when there is a change ("assertivepoliteness Nang pepul iki, kita mulai nggawe "trieng-thong" yen apa-ngêngguna, kuwi gewong modèl-agestak nul-ot kuwi nggawe barang nggawe CLT luwih apik bakal terus nggawe gerakan deweke nggawe barang suok diagram kuwi. FindOK Anytime Awak dhéwé nggunian sistem iki luwih akeh saben tarjamahan kanggo telu nggawe barang basa lan ngawe barang nggawe barang lan kowé nggawe barang kanggo nyebute kanggo nyebute F1 puntuang sing nambah 4% sing sekond-perusahaan lan 8% sing nyebute multi-source nggawe barang langgar. Perintah sing dipunangé awakdhéwé, nik kabèh sing perusahaan kanggo nggawe balêh kanggo nggawe barang nggawe barang nêr. Yute sampeyan ambem efek lan gampang'}
{'en': 'Comprehension Based Question Answering using  Bloom’s Taxonomy', 'ar': 'الإجابة على الأسئلة القائمة على الفهم باستخدام تصنيف بلوم', 'es': 'Respuestas a preguntas basadas en la comprensión utilizando la taxonomía de Bloom', 'fr': 'Réponse aux questions basée sur la compréhension en utilisant la taxonomie de Bloom', 'ja': 'Bloomの分類法を使用した理解に基づく質問の回答', 'pt': 'Resposta a perguntas com base na compreensão usando a taxonomia de Bloom', 'zh': '以布鲁姆分类法对', 'ru': 'Ответы на вопросы на основе понимания с использованием таксономии Блума', 'hi': 'समझ आधारित प्रश्न ब्लूम के वर्गीकरण का उपयोग कर जवाब', 'ga': 'Freagra ar Cheisteanna Tuisceana ag baint úsáide as Tacsanomaíocht Bloom', 'el': 'Απαντήσεις ερωτήσεων με βάση την κατανόηση χρησιμοποιώντας την Ταξινομία του Μπλουμ', 'hu': 'Megértésen alapuló kérdések megválaszolása Bloom Taxonómiájával', 'ka': 'კომპრენციის ბაზი კითხვების პასუხი გამოყენება', 'kk': 'Блум Таксониясын қолдану негізгі сұрақ жауап беру', 'it': 'Risposta alle domande basate sulla comprensione utilizzando la tassonomia di Bloom', 'ml': 'ബ്ലോമിന്റെ ടാക്സോനോമി ഉപയോഗിക്കുന്ന ചോദ്യം അടിസ്ഥാനമാക്കിയ ചോദ്യം', 'mt': "Comprehension Based Question Answering using Bloom's Taxonomy", 'ms': 'Jawapan soalan Berasas Pemahaman menggunakan Taksonomi Bloom', 'mn': 'Блумын Таксономийг ашиглаж буй асуулт', 'mk': 'Одговори на прашања базирани на разбирање со користење на таксономијата на Блум', 'no': 'Comment', 'lt': 'Išsamus atsakymas į klausimus naudojant Bloom taksonomiją', 'pl': 'Odpowiedzi na pytania oparte na zrozumieniu przy użyciu taksonomii Blooma', 'ro': 'Răspunsul la întrebări bazate pe înțelegere utilizând taxonomia Bloom', 'sr': 'Odgovor na osnovu pitanja koristeći Bloomovu taksonomiju', 'si': 'බ්ලූම් ටැක්සෝනෝමි භාවිතා කරන්න ප්\u200dරශ්න ප්\u200dරශ්න ප්\u200dරතික්ෂණය', 'sv': 'Förståelsebaserat frågesvar med Blooms taxonomy', 'so': 'Jawaabta su’aalaha aasaasiga ah ee lagu jawaabayo isticmaalka Bloom’s Taxonomy', 'ur': 'بلوم کی تاکسونمی کے استعمال سے جواب دینے کی بنیاد سوال', 'ta': 'பொருளின் Taxonomy பயன்படுத்தி முடிவு அடிப்படையான கேள்வி பதில்', 'uz': 'Name', 'vi': 'Trả lời câu hỏi hoàn toàn bằng cơ s ở thuế của Bloom', 'nl': "Op begrip gebaseerde vragen beantwoorden met behulp van Bloom's taxonomie", 'bg': 'Отговаряне на въпроси въз основа на разбиране чрез таксономията на Блум', 'de': 'Verstehensbasierte Beantwortung von Fragen mit Blooms Taxonomie', 'hr': 'Odgovor na pitanju na osnovu kompresije koristeći Bloomovu taksonomiju', 'da': 'Forståelsesbaseret spørgsmål besvarelse ved hjælp af Blooms Taxonomi', 'ko': '브룸 분류법의 이해에 기초한 문답을 사용하다', 'sw': "Comprehension Based Question Answering using Bloom's Taxonomy", 'id': 'Jawaban Pertanyaan Berdasarkan Pemahaman menggunakan Taksonomi Bloom', 'fa': 'پاسخ\u200cجواب سوال\u200cهای بنیادی با استفاده از تاکسون\u200cهای بلوم', 'am': "Comprehension Based Question Answering using Bloom's Taxonomy", 'hy': 'Ամբողջության հիմնված հարցերի պատասխանը Բլումի տաքսոնամիայի օգտագործման միջոցով', 'tr': 'Syýasatçylyk Blom Taxonomiýasyny ulanyp bermegi temel sorag', 'af': 'Comprehension Based Question Answering deur Bloom se Taxonomy te gebruik', 'az': "Bloom's Taxonomy vasit…Щsil…Щ istifad…Щ edil…Щn sual cavab verm…Щk", 'bn': 'ব্লগের ট্যাক্সোনোমি ব্যবহার করে প্রশ্নের ভিত্তিক প্রশ্নের উত্তর', 'sq': 'Përgjigja e pyetjeve të bazuara në kuptim duke përdorur taksonominë e Bloomit', 'ca': 'Respondent a preguntes basades en la comprensió utilitzant la taxonomia de Bloom', 'cs': 'Odpovědi na otázky založené na porozumění pomocí Bloomovy taxonomie', 'fi': 'Ymmärysperusteiset kysymykset Bloomin taksonomian avulla', 'bs': 'Odgovor na osnovu pitanja koristeći Bloomovu taksonomiju', 'et': 'Mõistmisel põhinev küsimustele vastamine Bloomi taksonoomia abil', 'jv': 'Compréncion', 'he': 'תשובה על שאלות מבוססת בהבנה באמצעות טקסונומיה של בלום', 'ha': 'KCharselect unicode block name', 'sk': 'Odgovarjanje na vprašanja na podlagi razumevanja z uporabo Bloomove taksonomije', 'bo': "Comprehension Based Question Answering using Bloom's Taxonomy"}
{'en': 'Current pre-trained language models have lots of knowledge, but a more limited ability to use that knowledge. Bloom’s Taxonomy helps educators teach children how to use knowledge by categorizing comprehension skills, so we use it to analyze and improve the  comprehension skills  of large pre-trained language models. Our experiments focus on zero-shot question answering, using the  taxonomy  to provide proximal context that helps the model answer questions by being relevant to those questions. We show targeting context in this manner improves performance across 4 popular common sense question answer datasets.', 'fr': "Les modèles linguistiques pré-formés actuels possèdent de nombreuses connaissances, mais une capacité plus limitée à les utiliser. Bloom's Taxonomy aide les éducateurs à enseigner aux enfants comment utiliser les connaissances en catégorisant les compétences de compréhension, afin que nous les utilisions pour analyser et améliorer les compétences de compréhension de grands modèles linguistiques pré-entraînés. Nos expériences se concentrent sur la réponse aux questions zéro, en utilisant la taxonomie pour fournir un contexte proximal qui aide le modèle à répondre aux questions en étant pertinent par rapport à ces questions. Nous montrons que le contexte de ciblage de cette manière améliore les performances de 4 ensembles de données de questions et réponses de bon sens.", 'pt': 'Os atuais modelos de linguagem pré-treinados têm muito conhecimento, mas uma capacidade mais limitada de usar esse conhecimento. A Taxonomia de Bloom ajuda os educadores a ensinar as crianças a usar o conhecimento categorizando as habilidades de compreensão, por isso a usamos para analisar e melhorar as habilidades de compreensão de grandes modelos de linguagem pré-treinados. Nossos experimentos se concentram em respostas a perguntas de tiro zero, usando a taxonomia para fornecer contexto proximal que ajuda o modelo a responder perguntas sendo relevante para essas perguntas. Mostramos que o contexto de segmentação dessa maneira melhora o desempenho em 4 conjuntos de dados populares de respostas a perguntas de senso comum.', 'ja': '現在の事前にトレーニングされた言語モデルは、多くの知識を持っていますが、その知識を使用する能力はより制限されています。Bloomの分類法は、理解スキルを分類することで、教育者が子供たちに知識の使用方法を教えるのに役立つため、私たちはそれを使用して、大規模な事前にトレーニングされた言語モデルの理解スキルを分析し、改善します。私たちの実験は、ゼロショットの質問への回答に焦点を当てており、タクソノミーを使用して、モデルが質問に関連性を持つことで質問に回答するのに役立つ近接的なコンテキストを提供します。このようにターゲティングコンテキストを示すと、4つの一般的な常識的な質問の回答データセットのパフォーマンスが向上します。', 'ar': 'نماذج اللغة الحالية المدربة مسبقًا لديها الكثير من المعرفة ، ولكن قدرة محدودة على استخدام تلك المعرفة. يساعد تصنيف بلوم المعلمين على تعليم الأطفال كيفية استخدام المعرفة من خلال تصنيف مهارات الفهم ، لذلك نستخدمها لتحليل وتحسين مهارات الفهم لنماذج اللغة الكبيرة المدربة مسبقًا. تركز تجاربنا على الإجابة على الأسئلة بدون طائل ، باستخدام التصنيف لتوفير سياق قريب يساعد النموذج في الإجابة عن الأسئلة من خلال كونه وثيق الصلة بهذه الأسئلة. نعرض سياق الاستهداف بهذه الطريقة يحسن الأداء عبر 4 مجموعات بيانات للإجابة عن أسئلة الفطرة السليمة.', 'es': 'Los modelos lingüísticos actuales previamente entrenados tienen muchos conocimientos, pero una capacidad más limitada para utilizarlos. La taxonomía de Bloom ayuda a los educadores a enseñar a los niños cómo usar el conocimiento mediante la categorización de las habilidades de comprensión, por lo que lo utilizamos para analizar y mejorar las habilidades de comprensión de grandes modelos lingüísticos preentrenados. Nuestros experimentos se centran en la respuesta cero a las preguntas, utilizando la taxonomía para proporcionar un contexto próximo que ayude al modelo a responder preguntas al ser relevante para esas preguntas. Mostramos que el contexto de segmentación de esta manera mejora el rendimiento en 4 conjuntos de datos populares de respuestas a preguntas de sentido común.', 'zh': '今预训之言,模范多知,而用之有限。 Bloom之分类法助教育工作者以类教诸子,故析而教之理解能力。 臣等实验侧重于零镜头对,以分类法供近端上下文,以助模样。 以此展定位上下文可以崇 4 常用常识问答数据集之性。', 'ru': 'Нынешние предварительно обученные языковые модели обладают большим количеством знаний, но более ограниченной способностью использовать эти знания. Таксономия Блума помогает педагогам научить детей использовать знания, классифицируя навыки понимания, поэтому мы используем их для анализа и улучшения навыков понимания больших предварительно обученных языковых моделей. Наши эксперименты сосредоточены на ответах на вопросы с нулевым выстрелом, используя таксономию для обеспечения проксимального контекста, который помогает модели отвечать на вопросы, будучи релевантной для этих вопросов. Таким образом, мы показываем, что контекст таргетинга улучшает производительность в 4 наборах ответов на популярные вопросы здравого смысла.', 'hi': 'वर्तमान पूर्व-प्रशिक्षित भाषा मॉडल में बहुत सारे ज्ञान हैं, लेकिन उस ज्ञान का उपयोग करने की अधिक सीमित क्षमता है। ब्लूम का वर्गीकरण शिक्षकों को यह सिखाने में मदद करता है कि समझ कौशल को वर्गीकृत करके ज्ञान का उपयोग कैसे किया जाए, इसलिए हम इसका उपयोग बड़े पूर्व-प्रशिक्षित भाषा मॉडल के समझ कौशल का विश्लेषण और सुधार करने के लिए करते हैं। हमारे प्रयोग शून्य-शॉट प्रश्न का उत्तर देने पर ध्यान केंद्रित करते हैं, वर्गीकरण का उपयोग करके समीपस्थ संदर्भ प्रदान करने के लिए जो मॉडल को उन प्रश्नों के लिए प्रासंगिक होने के कारण प्रश्नों के उत्तर देने में मदद करता है। हम इस तरह से लक्ष्यीकरण संदर्भ दिखाते हैं जो 4 लोकप्रिय सामान्य ज्ञान प्रश्न उत्तर डेटासेट में प्रदर्शन में सुधार करता है।', 'ga': 'Tá go leor eolais ag múnlaí teanga réamhoilte reatha, ach cumas níos teoranta chun an t-eolas sin a úsáid. Cuidíonn Tacsanomaíocht Bloom le hoideachasóirí páistí a mhúineadh conas eolas a úsáid trí scileanna tuisceana a chatagóiriú, mar sin úsáidimid é chun anailís a dhéanamh agus feabhas a chur ar scileanna tuisceana na múnlaí teanga móra réamhoilte. Díríonn ár dturgnaimh ar fhreagra ceisteanna gan urchar, ag baint úsáide as an tacsanomaíocht chun comhthéacs cóngarach a sholáthar a chuidíonn leis an tsamhail ceisteanna a fhreagairt trí bheith ábhartha do na ceisteanna sin. Léirímid go bhfeabhsaítear an fheidhmíocht i gcomhthéacs spriocdhírithe ar an mbealach seo thar 4 thacar sonraí ar fhreagraí ceisteanna ciallmhara coitianta.', 'el': 'Τα τρέχοντα προσχεδιασμένα γλωσσικά μοντέλα έχουν πολλές γνώσεις, αλλά μια πιο περιορισμένη ικανότητα να χρησιμοποιούν αυτές τις γνώσεις. Η Ταξινομία του βοηθά τους εκπαιδευτικούς να διδάξουν στα παιδιά πώς να χρησιμοποιούν τη γνώση κατηγοριοποιοώντας τις δεξιότητες κατανόησης, οπότε τις χρησιμοποιούμε για να αναλύσουμε και να βελτιώσουμε τις δεξιότητες κατανόησης μεγάλων προ-εκπαιδευμένων γλωσσικών μοντέλων. Τα πειράματά μας επικεντρώνονται στην απάντηση ερωτήσεων μηδενικού πυροβολισμού, χρησιμοποιώντας την ταξινομία για να παρέχει κοντινό πλαίσιο που βοηθά το μοντέλο να απαντήσει σε ερωτήσεις, όντας σχετικό με αυτές τις ερωτήσεις. Δείχνουμε το πλαίσιο στόχευσης με αυτόν τον τρόπο βελτιώνει την απόδοση σε τέσσερα σύνολα δεδομένων απαντήσεων κοινής λογικής.', 'ka': 'მიმდინარე წინასწარმოადგენებული ენის მოდელები აქვს ძალიან მეცნიერები, მაგრამ უფრო დასაზღვრებული შესაძლებლობა ამ ცნობილების გამოყენება. ბლუმს ტაკჟონომია დახმარებს განსწავლებელებს, როგორ გამოიყენება მეცნიერების გამოყენება კატეგორიზაციის შესაძლებლობით, ამიტომ ჩვენ გამოყენებთ ის ანალიზაციისთვის და უფრო უფრო უფრო ჩვენი ექსპერიმენტები ნულ-სტარტის კითხვის პასუხისთვის, რაქტონომის გამოყენებას, რომელიც მოდელის პასუხისთვის დახმარებს, რაქტონომის გამოყენებას, რომ ჩვენ ჩვენ ჩვენ ჩვენ აჩვენებთ მისაღების კონტექსტის შესაძლებლობა, როგორც 4 პოლუბური საუკეთესო საუკეთესო საუკეთესო', 'hu': 'A jelenlegi, előre képzett nyelvi modellek sok tudással rendelkeznek, de korlátozottabb képességgel rendelkeznek az ismeretek használatára. A Bloom Taxonómia segítségével a pedagógusok megtanítják a gyerekeknek a tudás használatát a megértési készségek kategorizálásával, ezért arra használjuk, hogy elemezzük és javítsuk a nagy, előre képzett nyelvi modellek megértési készségeit. Kísérleteink középpontjában a nulla lövéses kérdések megválaszolása áll, a taxonómia segítségével proximális kontextust biztosítunk, amely segít a modell válaszolni a kérdésekre azáltal, hogy releváns a kérdésekre. A célzási környezetet így mutatjuk meg, javítjuk a teljesítményt 4 népszerű józan ész kérdésválasz adatkészletben.', 'it': 'Gli attuali modelli linguistici pre-formati hanno molte conoscenze, ma una capacità più limitata di utilizzare quelle conoscenze. La Taxonomy di Bloom aiuta gli educatori a insegnare ai bambini come utilizzare le conoscenze classificando le capacità di comprensione, quindi le usiamo per analizzare e migliorare le capacità di comprensione di grandi modelli linguistici pre-formati. I nostri esperimenti si concentrano sulla risposta a domande zero-shot, utilizzando la tassonomia per fornire un contesto proximale che aiuta il modello a rispondere alle domande essendo pertinente a quelle domande. Mostriamo il contesto di targeting in questo modo migliora le prestazioni in 4 set di dati popolari di risposta alle domande di buon senso.', 'kk': 'Қазіргі алдын- оқылған тіл үлгілерінде көп білім бар, бірақ бұл білімді қолдану мүмкіндігін шектеу мүмкіндігі бар. Блум Таксономиясы балаларды білім пайдалануын балаларға көмектеседі. Біз оны анализ және түсіндірілген тіл моделдерінің үлкен түсініктерін жақсарту үшін қолданамыз. Біздің тәжірибеміз нөл сұрақтар жауаптарына көмектеседі. Таксономиясын қолдану үшін жақын контексті қолданады. Бұл сұрақтар үлгісіне жауап беру сұрақтарына көмектесед Біз мақсатталған контексті осылай көрсету үшін 4 жалпы мәліметтің сұрақ мәліметтерінің жауап беру бағдарламасын жақсартады.', 'mk': 'Current pre-trained language models have lots of knowledge, but a more limited ability to use that knowledge.  Таксономијата на Блум им помага на образовниците да ги научат децата како да користат знаење со категоризирање на способностите за разбирање, па ние ги користиме за анализирање и подобрување на способностите за разбирање на големите предобучени јазички модели. Нашите експерименти се фокусираат на одговорот на прашањата со нула, користејќи ја таксономијата за да обезбедат проксимален контекст кој му помага на моделот да одговори на прашањата со тоа што е релевантно за тие прашања. Ние го покажуваме контекстот на цел на овој начин подобрува резултатот во четирите популарни компјутери на одговори на прашањата на здраво разум.', 'ms': "Model bahasa pralatihan semasa mempunyai banyak pengetahuan, tetapi kemampuan yang lebih terbatas untuk menggunakan pengetahuan itu. Bloom's Taxonomy membantu pendidikan mengajar kanak-kanak bagaimana menggunakan pengetahuan dengan mengkategorisasi kemampuan pemahaman, jadi kami menggunakannya untuk menganalisis dan meningkatkan kemampuan pemahaman model bahasa yang besar yang dilatih dahulu. Eksperimen kami fokus pada jawapan soalan 0-shot, menggunakan taksonomi untuk menyediakan konteks proksimal yang membantu model menjawab soalan dengan menjadi relevan untuk soalan tersebut. Kami menunjukkan konteks sasaran dengan cara ini meningkatkan prestasi di 4 set data jawapan soalan masuk akal terkenal.", 'mt': 'Il-mudelli attwali tal-lingwi mħarrġa minn qabel għandhom ħafna għarfien, iżda kapaċità aktar limitata biex jużaw dak l-għarfien. It-Tassonomija ta’ Bloom tgħin lill-edukaturi jgħallmu lit-tfal kif jużaw l-għarfien billi jikkategorizzaw il-ħiliet ta’ komprensjoni, għalhekk aħna nużaw dan biex janalizzaw u jtejbu l-ħiliet ta’ komprensjoni ta’ mudelli kbar ta’ lingwi mħarrġa minn qabel. L-esperimenti tagħna jiffukaw fuq it-tweġiba għal mistoqsijiet mingħajr skop, billi jużaw it-tassonomija biex jipprovdu kuntest prossimali li jgħin lill-mudell iwieġeb mistoqsijiet billi jkun rilevanti għal dawk il-mistoqsijiet. Aħna nuru kuntest immirat b’dan il-mod itejjeb il-prestazzjoni f’erba’ settijiet ta’ dejta dwar it-tweġibiet għall-mistoqsijiet tas-sens komuni popolari.', 'ml': 'ഇപ്പോഴത്തെ പരിശീലിക്കപ്പെട്ട ഭാഷ മോഡലുകള്\u200dക്ക് ഒരുപാട് അറിവുണ്ട്, പക്ഷെ അതിന്റെ അറിവ് ഉപയോഗിക്കാന്\u200d കൂട ബ്ലോമിന്റെ ടാക്സോനോമി വിദ്യാഭ്യാസികളെ പഠിപ്പിക്കുന്നത് കുട്ടികള്\u200dക്ക് ഉപയോഗിക്കാന്\u200d സഹായിക്കുന്നു. പരിശീലിക്കപ്പെട്ട മോഡലുകളുടെ ഗു നമ്മുടെ പരീക്ഷണങ്ങള്\u200d ശൂന്യമായ ചോദ്യങ്ങളുടെ ഉത്തരം ഉത്തരം ചോദിക്കുന്നു, ടാക്സോനോമി ഉപയോഗിക്കുന്നത് പ്രോക്സിമാല്\u200d കോണ്\u200dസ് നാല് പ്രധാനപ്പെട്ട മനസ്സിലുള്ള ചോദ്യങ്ങള്\u200dക്ക് ഉത്തരം ഡാറ്റാസറ്റുകള്\u200dക്ക് ഉത്തരം നല്\u200dകുന്നതിനാല', 'mn': 'Өнөөдөр сургалтын өмнө сургалтын хэл загварууд маш олон мэдлэг байдаг. Гэхдээ энэ мэдлэгийг ашиглах илүү хязгаарлагддаг. Блум-ын Таксономийн багш нарт хүүхдүүдэд ойлголтын чадварыг хэрхэн хэрхэн ашиглах талаар багш нарт тусалдаг. Тиймээс бид үүнийг шинжилгээр, сургалтын өмнө сургалтын том хэл загварын чадварыг сайжруулахын тулд ашиг Бидний туршилтууд тэр асуултын хариулт дээр анхаарлаа анхаарлаа хандуулдаг. Таксономикийг ашиглан ойролцоогоор хариултын асуултад тусалдаг. Бид зориулалттай байдлыг харуулж байна. Энэ арга хэмжээнд 4 хүмүүсийн мэдрэлийн хариултын өгөгдлийн сангуудыг сайжруулдаг.', 'lt': 'Dabartiniai iš anksto parengti kalbų modeliai turi daug žinių, tačiau jų gebėjimas naudoti yra ribotas. Bloomo taksonomija padeda mokytojams mokyti vaikus naudotis žiniomis klasifikuojant supratimo įgūdžius, taigi mes juos naudojame analizuojant ir gerinant didelių iš anksto parengtų kalbų modelių supratimo įgūdžius. Mūsų eksperimentuose daugiausia dėmesio skiriama nuliniam atsakymui į klausimus, naudojant taksonomiją, kad būtų užtikrintas artimas kontekstas, padedantis modeliui atsakyti į klausimus, nes jis yra svarbus šiems klausimams. Tokiu būdu rodome tikslinį kontekstą, kuris pagerina rezultatus keturiose populiarios bendros prasmės klausimų atsakymo duomenų rinkiniuose.', 'pl': 'Obecne wstępnie przeszkolone modele językowe mają dużo wiedzy, ale bardziej ograniczoną zdolność do wykorzystania tej wiedzy. Taksonomia Bloom pomaga nauczycielom nauczyć dzieci, jak korzystać z wiedzy poprzez kategoryzowanie umiejętności zrozumienia, dlatego wykorzystujemy ją do analizy i poprawy umiejętności zrozumienia dużych wstępnie przeszkolonych modeli językowych. Nasze eksperymenty koncentrują się na odpowiedzi na pytania zero-shot, wykorzystując taksonomię do zapewnienia kontekstu proximalnego, który pomaga modelowi odpowiedzieć na pytania poprzez odpowiedź na te pytania. Pokazujemy kontekst kierowania w ten sposób poprawia wydajność w czterech popularnych zbiorach danych odpowiedzi na pytania o zdrowy rozsądek.', 'no': 'Gjeldande først trengte språk-modeller har mye kunnskap, men ein meir grensert kapasitet for å bruka denne kunnskapen. Bloom s taksonomie hjelper til utdannar å lære barna korleis å bruke kunnskap ved å kategorisera forståkingskyldighetar, s å vi bruker det for å analysera og forbedra forståkingskyldighetane av stor før-trengte språk-modeller. Eksperimentane våre fokuserer på spørsmålet med null-snitt, ved å bruka taxonomien for å gjera proksimalt kontekst som hjelper modellen svar på spørsmålene ved å vera relevant til dei spørsmålene. Vi viser målsettingskontekst slik forbetrar utviklinga over 4 populære spørsmålsetter for spørsmålsetter.', 'ro': 'Modelele lingvistice pre-instruite actuale au o mulțime de cunoștințe, dar o capacitate mai limitată de a utiliza aceste cunoștințe. Taxonomy Bloom îi ajută pe educatori să învețe pe copii cum să folosească cunoștințele prin clasificarea abilităților de înțelegere, astfel încât le folosim pentru a analiza și îmbunătăți abilitățile de înțelegere ale modelelor lingvistice mari pre-instruite. Experimentele noastre se concentrează pe răspunsul la întrebări zero-shot, folosind taxonomia pentru a furniza context proximal care ajută modelul să răspundă la întrebări prin a fi relevant pentru aceste întrebări. Prezentăm contextul de direcționare în acest mod îmbunătățește performanța în 4 seturi de date populare de răspunsuri la întrebări de bun simț.', 'sr': 'Trenutni predobučeni jezički modeli imaju mnogo znanja, ali ograničenija sposobnost koristiti to znanje. Bloomova taksonomija pomaže učiteljima da uče decu kako koriste znanje kategorizirajući vještine razumevanja, pa ga koristimo da analiziramo i poboljšamo vještine razumevanja velikih predobučenih jezičkih modela. Naši eksperimenti se fokusiraju na odgovor na pitanje na nulu pucnjavu, koristeći taksonomiju kako bi pružili proksimalni kontekst koji pomaže modelu odgovornim pitanjima tako što su relevantni na ta pitanja. Pokazujemo ciljni kontekst na taj naèin poboljšava izvodnju u četiri popularne opšte razumne odgovore na podatke o pitanju pitanja.', 'si': 'මුලින් ප්\u200dරධානය කරපු භාෂා මොඩේල් තියෙනවා ගොඩක් දැනගන්න, ඒත් ඒ දැනගන්න පුළුවන් වැඩි සීමා බ්ලෝම්ස් ටැක්සෝනෝමි උදව් කරනවා පුරුද්ධියෝ ළමයින්ට දන්නවන්න පුළුවන්ට උදව් කරනවා කොහොමද දන්නවන්නේ කියලා, අපි ඒක විශ්ලේෂණ අපේ පරීක්ෂණ ප්\u200dරශ්ණ ප්\u200dරශ්ණ ප්\u200dරතික්\u200dරියාවට පරික්ෂා කරන්න, ටැක්සිනෝමිය ප්\u200dරශ්ණ ප්\u200dරශ්ණ ප්\u200dරයෝජනය සඳ අපි මේ විදියට ඉලක්කම් සංවේදනය පෙන්වන්න පුළුවන් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තොර', 'so': "Tusaalada afka hore ee la tababaray waxay leeyihiin aqoon badan, laakiin aqoontaas waa awood aad u xadan tahay. Bogga Taxonomy wuxuu carruurta baraa aqoonta sida ay u isticmaalaan aqoonta aasaasiga, sidaa darteed waxaynu u isticmaalnaa si aynu u u analysno oo u hagno aqoonta aasaasiga ah tusaalaha afka hore oo aad u baratay. Imtixaankayadu waxay ku focus yihiin jawaabta su'aalaha zero-ganacsiga ah, isticmaalka canshuuraha si ay u fidiso xaalad soo dhowaad oo u caawinaya jawaabta sameynta sameynta su'aalahan si ay u xiriiraan su'aalahan. We show targeting context in this manner improves performance across 4 popular common sense question answer datasets.", 'ur': 'اس سے پہلے کی تعلیم کی زبان مدل بہت زیادہ علم رکھتے ہیں، لیکن اس علم کا استعمال کرنے کے لئے زیادہ محدودہ قابلیت ہے. بلوم کی تاکسونمی نے بچوں کو علم کی تعلیم دینے کی مدد کی ہے، سو ہم اسے سمجھنے کی طاقتوں کے ذریعہ کائنات کا استعمال کریں، اس لئے ہم اسے تحلیل کرنے اور سمجھنے کے لئے استعمال کرتے ہیں، بڑے پیش آموزش کی زبان مدلکوں کی سمجھنے کی طاقتوں کے ساتھ. ہماری آزمائش صفر-شٹ سوال کے جواب پر منتظر ہوتی ہے، تاکسونومی کا استعمال کرتا ہے تاکسونومی کے ساتھ قریبی کنٹنسیٹ کی مدد کرتا ہے جو مدل سوال کے جواب دینے کے ذریعہ ان سوالوں کے معاملہ سے مدد کرتا ہم اس طرح موقعیت کا موقعیت دکھاتے ہیں کہ چرے مشترک سمجھ کے سوال ڈیٹ سٹ میں کامیابی کو بہتر کر دیتا ہے.', 'sv': 'Nuvarande förberedda språkmodeller har mycket kunskap, men en mer begränsad förmåga att använda den kunskapen. Blooms Taxonomi hjälper pedagoger att lära barn hur man använder kunskap genom att kategorisera förståelsefärdigheter, så vi använder den för att analysera och förbättra förståelseförmågan hos stora förklädda språkmodeller. Våra experiment fokuserar på noll-skott frågesvar och använder taxonomin för att ge proximal kontext som hjälper modellen att svara på frågor genom att vara relevant för dessa frågor. Vi visar målinriktad kontext på detta sätt förbättrar prestandan i fyra populära frågeuppsättningar med sunt förnuft.', 'ta': 'தற்போது பயிற்சி முன் பயிற்சி மொழி மாதிரிகளுக்கு பல அறிவு உள்ளது, ஆனால் அந்த அறிவை பயன்படுத்த முடியும் அதிகம் வர பொருளின் மாதிரி பெரிய மொழி மாதிரி மாதிரிகளின் சூழ்ச்சி திறமைகளைப் பிரித்து எவ்வாறு அறிவை பயன்படுத்துவதற்கு குழந்தைகளை கற்றுக்கொள்ள உதவுகிறது.  எங்கள் சோதனைகள் சூழ்நிலைக்கான கேள்விக்கு பதில் கவனம் செலுத்துகிறது, வரிசையில் தொடர்புடைய சூழலை வழங்குகிறது, மாதிரி விடைக நாம் இலக்கு சூழலை இந்த வழியில் காட்டுகிறோம் 4 பிரபலமான பொதுவான கேள்வி தகவல் அமைப்புகளை மேம்படுத்த', 'uz': "Joriy o'rganilgan tillar modellarida juda ko'p илм bor, lekin bu bilimni foydalanish uchun juda chegara. Name Bizning imtiyozlarimiz bu savollarga bog'liq bo'lgan savol javob beradi, taxonomni ishlatish uchun proksikal muktadha yordam beradi. Ko'rsatish muvaffaqiyatlarini shunday qilib ko'rsatumiz. 4 oddiy oddiy soʻzlar soʻzlariga javob beradi.", 'vi': 'Tài liệu ngôn ngữ hiện tại được đào tạo có rất nhiều kiến thức, nhưng khả năng sử dụng nó hạn chế hơn. Công ty Taxonomy of Bloom đã giúp giáo viên dạy trẻ em cách s ử dụng tri thức bằng cách phân loại các kỹ năng thấu hiểu, nên chúng tôi dùng nó để phân tích và cải thiện khả năng thấu hiểu của các mô hình ngôn ngữ lớn. Các thí nghiệm của chúng tôi tập trung vào việc trả lời câu hỏi bằng không bắn, sử dụng the taxonomy để cung cấp sơ suất ngữ cảnh giúp mô hình trả lời câu hỏi bằng cách liên quan tới những câu hỏi đó. Chúng tôi cho rằng tất cả mục tiêu bằng kiểu này có thể giúp tốt bằng thành trình bằng tế.', 'bg': 'Настоящите предварително обучени езикови модели имат много знания, но по-ограничена способност за използване на тези знания. Таксономията помага на учителите да научат децата как да използват знанията чрез категоризиране на уменията за разбиране, така че ние ги използваме, за да анализираме и подобрим уменията за разбиране на големи предварително обучени езикови модели. Нашите експерименти се фокусират върху нулевото отговаряне на въпроси, като използват таксономията за осигуряване на проксимален контекст, който помага на модела да отговори на въпроси, като е уместен към тези въпроси. Показваме целевият контекст по този начин подобрява ефективността на 4 набора от данни за отговори на въпроси с общ разум.', 'hr': 'Trenutni predobučeni jezički modeli imaju mnogo znanja, ali ograničenija sposobnost koristiti to znanje. Bloomova taksonomija pomaže učiteljima učiti djecu kako koristiti znanje kategorizirajući vještine razumijevanja, tako da ga koristimo za analizu i poboljšanje razumijevanja velikih predobučenih jezičkih modela. Naši eksperimenti su usredotočeni na odgovor na pitanje bez pucnjave, koristeći taksonomiju kako bi pružili proksimalni kontekst koji pomaže modelu odgovornim pitanjima što su relevantni na te pitanja. Mi pokazujemo ciljani kontekst na taj način poboljšava učinkovitost u četiri popularne podatke o pitanju pitanja.', 'nl': "De huidige voorgetrainde taalmodellen hebben veel kennis, maar een meer beperkte mogelijkheid om die kennis te gebruiken. Bloom's Taxonomie helpt opvoeders kinderen te leren hoe ze kennis kunnen gebruiken door begripsvaardigheden te categoriseren, dus gebruiken we het om de begripsvaardigheden van grote vooraf getrainde taalmodellen te analyseren en te verbeteren. Onze experimenten richten zich op het beantwoorden van zero-shot vragen, waarbij de taxonomie wordt gebruikt om proximale context te bieden die het model helpt vragen te beantwoorden door relevant te zijn voor die vragen. We tonen targeting context op deze manier verbetert de prestaties in vier populaire datasets met gezond verstand.", 'de': "Aktuelle vortrainierte Sprachmodelle haben viel Wissen, aber eine begrenztere Fähigkeit, dieses Wissen zu verwenden. Bloom's Taxonomy hilft Pädagogen, Kindern beizubringen, wie sie Wissen nutzen können, indem sie Verständnisfähigkeiten kategorisieren. Daher verwenden wir es, um die Verständnisfähigkeiten großer vortrainierter Sprachmodelle zu analysieren und zu verbessern. Unsere Experimente konzentrieren sich auf die Beantwortung von Null-Schuss-Fragen, wobei die Taxonomie verwendet wird, um einen proximalen Kontext bereitzustellen, der dem Modell hilft, Fragen zu beantworten, indem es für diese Fragen relevant ist. Wir zeigen, dass der Targeting-Kontext auf diese Weise die Leistung in vier gängigen Frageantwortdatensätzen mit gesundem Menschenverstand verbessert.", 'ko': '현재 미리 훈련된 언어 모형에는 많은 지식이 있지만 이런 지식을 사용하는 능력은 더욱 제한되어 있다.브룸의 분류법은 교육자들이 이해 기능에 대한 분류를 통해 아이들에게 지식을 어떻게 사용하는지 가르치는 데 도움을 주기 때문에 우리는 이를 이용하여 대형 예훈련 언어 모델의 이해 기능을 분석하고 향상시킨다.우리의 실험은 제로 렌즈 퀴즈에 중심을 두고 분류법을 사용하여 근접 상하문을 제공하여 이런 문제와 관련을 통해 모델이 문제를 대답하도록 돕는다.우리는 이런 방식으로 상하문을 포지셔닝하면 4개의 흔한 상식 문답 데이터 집합의 성능을 향상시킬 수 있다고 밝혔다.', 'id': "Model bahasa prapelatih saat ini memiliki banyak pengetahuan, tapi kemampuan yang lebih terbatas untuk menggunakan pengetahuan itu. Bloom's Taxonomy membantu pendidikan mengajar anak-anak bagaimana menggunakan pengetahuan dengan mengkategorisasi keterampilan pemahaman, jadi kami menggunakannya untuk menganalisis dan meningkatkan keterampilan pemahaman dari model bahasa besar yang terlatih sebelumnya. Eksperimen kami fokus pada menjawab pertanyaan nol, menggunakan taksonomi untuk menyediakan konteks proksimal yang membantu model menjawab pertanyaan dengan relevan untuk pertanyaan tersebut. Kami menunjukkan konteks sasaran dengan cara ini meningkatkan prestasi di 4 set data pertanyaan yang masuk akal umum popular.", 'fa': 'مدل های پیش آموزش زبانی بسیار زیادی دانش دارند، ولی توانایی محدود تر برای استفاده از این دانش. تاکسوني بلوم کمک ميکنه به آموزگاران ياد بده که چطوري از دانش استفاده کنند با توجه به مهارت درک کردن، پس از آن استفاده کنيم تا درک و تحليل کردن مهارت درک هاي مدل هاي زبان پيش آموزش شده بزرگ رو بهتر کنيم. آزمایش\u200cهای ما روی جواب سوال\u200cهای صفر-شلیک تمرکز می\u200cکنند، از تاکسونومی استفاده می\u200cکنند تا موقعیت نزدیک را پیشنهاد کند که به سوال\u200cهای جواب مدل کمک می\u200cکند با ارتباط به این سوال\u200cها. ما موضوع هدف\u200cگیری را به این طریق نشان می\u200cدهیم که عملکرد را در مجموعه\u200cهای داده\u200cهای سوال\u200cهای معمولی در چهار حس معمولی بهتر می\u200cکند.', 'da': "Nuværende prætrænede sprogmodeller har masser af viden, men en mere begrænset evne til at bruge den viden. Bloom's Taxonomi hjælper undervisere med at lære børn, hvordan man bruger viden ved at kategorisere forståelsesfærdigheder, så vi bruger den til at analysere og forbedre forståelsesfærdighederne i store prætrænede sprogmodeller. Vores eksperimenter fokuserer på zero-shot spørgsmål besvarelse, ved at bruge taksonomien til at give proximal kontekst, der hjælper modellen med at besvare spørgsmål ved at være relevant for disse spørgsmål. Vi viser målretning kontekst på denne måde forbedrer ydeevnen på tværs af 4 populære sund fornuft spørgsmål svar datasæt.", 'tr': 'Häzirki öňünden bilim nusgalarynda köp bilgi bar, ýöne ol bilgi ulanmak üçin biraz ýok ukyp bar. Bloomiň taksonomiýa çagalara bilgileri nähili ulanmalydygyny çagalara öwretmekde kömek edýär, şeýle däl biz muny öňünden öňünden öňünden bilim taýýarlamak üçin analyze we düzeltmek üçin ulanýarys. Biziň deneylerimiz 0-atly soraglaryň jogabyny ulanyp, taksonomiýany ýakyn kontekst saýlamak üçin nusga soraglary üçin kömek edip otyrýar. Biz bu şekilde maksadalan bir konteksti görkezip otyrýarys 4 meýdany duýgularyň soragy soragy düzümlerini geliştirir.', 'af': "Huidige voorafgeleerde taal modele het baie kennis, maar 'n meer beperk moontlik om daardie kennis te gebruik. Bloom se Taxonomy help onderwysers leer kinders hoe om kennis te gebruik deur verstandige verstandighede te kategoriseer, sodat ons dit gebruik om die verstandige verstandighede te analyseer en te verbeter van groot vooraf onderwyselde taal modele. Ons eksperimente fokus op nul-skoot vraag antwoord, gebruik die taksonomie om proximale konteks te verskaf wat die model antwoord vrae hulp deur te wees relevant aan dié vrae. Ons wys die doel-konteks op hierdie manier verbeter die prestasie oor 4 populêre verstandige verstandige vraag-datastel.", 'sw': 'Mfano wa lugha zilizofunzwa kwa sasa una maarifa mengi, lakini uwezo mdogo zaidi wa kutumia maarifa hayo. Taxonomy ya Blogu inawasaidia walimu kuwafundisha watoto namna ya kutumia maarifa kwa kutenganisha ujuzi wa ufahamu, kwa hiyo tunatumia uchambuzi na kuboresha ujuzi wa ufahamu wa mifano makubwa ya lugha zilizofunzwa kabla. Majaribio yetu yanajikita kwenye majibu ya swali lisilo na sifa, kwa kutumia utamaduni wa taxonoma kutoa muktadha unaosaidia kujibu maswali hayo kwa kuwa na umuhimu na maswali hayo. Tunaonyesha mukhtadha wa lengo kwa namna hii unaboresha utendaji wa maswali ya watu 4 maarufu ya majibu ya data.', 'am': 'የአሁኑ የቋንቋ ምሳሌዎች ብዙ እውቀት አላቸው ግን ያንን እውቀት ለመጠቀም የሚችል ስልጣን ነው፡፡ የጦማር ትክሲኖም ተማሪዎችን እውቀትን እንዴት እንደሚጠቀም እውቀትን እንዲያስተምሩ ይረዳቸዋል፤ ስለዚህም የበለጠ የቋንቋ ምሳሌዎችን ለማስተዋል እናሳውቀውም ዘንድ እናሳውቃለን፡፡ ፈተናዎቻችን የzero-shot ጥያቄን መልስ በመጠየቅ ላይ ነው፣ የጥያቄውን ትርጉም በመጠቀም የሞዴል ጥያቄዎችን በተጠቃሚ ጥያቄ በመስጠት ይረዳል፡፡ በአሳባቢው የውይይት ጥያቄን በ4 የሆኑት የጥያቄ ጥያቄ የዳታ ጥያቄዎችን እናሳየዋለን፡፡', 'sq': "Modelet e tanishme të paratrajnuara gjuhësh kanë shumë njohuri, por një aftësi më të kufizuar për të përdorur atë njohuri. Taksonomia e Bloom ndihmon arsimorët t'i mësojnë fëmijët si të përdorin njohuritë duke kategorizuar aftës it ë e kuptimit, kështu që ne e përdorim atë për të analizuar dhe përmirësuar aftësitë e kuptimit të modeleve të mëdha gjuhësh të trajnuara para. Eksperimentet tona përqëndrohen në përgjigjen e pyetjeve zero-shot, duke përdorur taksonominë për të ofruar kontekst proximal që ndihmon modelin të përgjigjet pyetjeve duke qenë i rëndësishëm për ato pyetje. Ne tregojmë kontekstin objektiv në këtë mënyrë përmirëson performancën në katër grupe të përgjigjeve të pyetjeve të kuptimit të përbashkët popullore.", 'bn': 'বর্তমানে প্রশিক্ষিত ভাষার মডেল অনেক জ্ঞান আছে, কিন্তু এই জ্ঞান ব্যবহারের ক্ষমতা বেশী সীমিত। ব্লগের ট্যাক্সোনোমি শিক্ষার্থীদের শিক্ষার্থীদের জ্ঞান শিক্ষা দেয় কিভাবে সম্পূর্ণ দক্ষতা বিভাগ করে ব্যবহার করে, তাই আমরা এটা ব্যবহার করি বিশ্লেষণ আমাদের পরীক্ষাগুলোর প্রশ্নের উত্তরের দিকে মনোযোগ দিয়েছে, ট্যাক্সোনোমি ব্যবহার করে প্রক্সিমাল প্রেক্সিমেন্ট প্রদান করার জন্য আমরা লক্ষ্যবস্তুকে এভাবে দেখাচ্ছি ৪ জনপ্রিয় মানুষের সাধারণ প্রশ্নের উত্তর তথ্যের সাথে প্রশ্নের প্রশ', 'bs': 'Trenutni predobučeni jezički modeli imaju mnogo znanja, ali ograničenija sposobnost koristiti to znanje. Bloomova taksonomija pomaže učiteljima učiti djecu kako koristiti znanje kategorizirajući vještine razumijevanja, tako da ga koristimo da analiziramo i poboljšamo vještine razumijevanja velikih predobučenih jezičkih modela. Naši eksperimenti su usredotočeni na odgovor na pitanje bez pucnjave, koristeći taksonomiju kako bi omogućili proksimalni kontekst koji pomaže modelu odgovornim pitanjima odgovoreći odgovore na te pitanja. Mi pokazujemo ciljani kontekst na taj način poboljšava učinkovitost u četiri popularne općeg smisla odgovarajuće podatke o pitanju pitanja.', 'cs': 'Současné předškolené jazykové modely mají mnoho znalostí, ale omezenější schopnost tyto znalosti využívat. Bloomova taxonomie pomáhá pedagogům učit děti, jak používat znalosti kategorizováním dovedností porozumění, takže ji používáme k analýze a zlepšení porozumění velkých předškolených jazykových modelů. Naše experimenty se zaměřují na odpověď na otázky s nulovým záběrem, pomocí taxonomie poskytují proximální kontext, který pomáhá modelu odpovědět na otázky tím, že je relevantní pro tyto otázky. Ukazujeme kontext cílení tímto způsobem zlepšuje výkon napříč čtyřmi datovými sadami odpovědí na otázky populárního zdravého rozumu.', 'et': 'Praegustel eelõpetatud keelemudelitel on palju teadmisi, kuid piiratum võime neid teadmisi kasutada. Bloomi taksonoomia aitab õpetajatel õpetada lastele, kuidas kasutada teadmisi, liigitades arusaamise oskusi, nii et me kasutame seda suurte eelõpetatud keelemudelite arusaamise oskuste analüüsimiseks ja parandamiseks. Meie eksperimendid keskenduvad null-shot küsimustele vastamisele, kasutades taksonoomiat proksimaalse konteksti pakkumiseks, mis aitab mudelil vastata küsimustele, olles nende küsimuste jaoks asjakohane. Näitame, et sihtimise kontekst sellisel viisil parandab jõudlust nelja populaarse mõistuse küsimuste vastuste andmekogumi puhul.', 'az': 'Şimdiki əvvəl təhsil edilmiş dil modelləri çox bilgi var, amma bu bilgi istifadə etmək üçün daha çox sınırlı. Bloomin Taxonomisi müəllimlərə çocuklara elmi nasıl istifadə etməyi öyrətməyə kömək edir ki, anlama yeteneklərini kategoriyasiyarlayıb, böylece bunu analiz etmək və öyrənmək üçün böyük dil modellerinin anlama yeteneklərini düzəltmək üçün istifadə edirik. Bizim eksperimentlərimiz sıfır-vuruş sualının cavabına, taxonomiya istifadə edərək, bu suallara bağlı olaraq modellərin cavabı suallarına kömək edər. Biz bu yolla məqsədilə məlumatları göstəririk ki, 4 ünlü məlumatlar haqqında sual verilən verilənlər arasında performansını yaxşılaşdırır.', 'fi': "Nykyisillä esikoulutetuilla kielimalleilla on paljon tietoa, mutta rajallisempi kyky käyttää tätä tietoa. Bloom's Taxonomy auttaa kasvattajia opettamaan lapsia käyttämään tietoa luokittelemalla ymmärtämistaitoja, joten käytämme sitä analysoimaan ja parantamaan suurten esikoulutettujen kielimallien ymmärtämistaitoja. Kokeemme keskittyvät nollashot-kysymykseen vastaamiseen käyttämällä taksonomiaa tarjoamaan proximaalisen kontekstin, joka auttaa mallia vastaamaan kysymyksiin olemalla relevantti näihin kysymyksiin. Osoitamme kohdentamisen kontekstin tällä tavalla parantavan suorituskykyä neljässä terveen järjen kyselyvastausaineistossa.", 'ca': "Els models de llenguatge actuals pré-entrenats tenen molt coneixement, però una habilitat més limitada d'utilitzar aquest coneixement. La taxonomia de Bloom ajuda els educadors a ensenyar als nens com utilitzar el coneixement categoritzant les habilitats de comprensió, així que l'utilitzem per analitzar i millorar les habilitats de comprensió de grans models de llenguatge pré-entrenats. Els nostres experiments s'centren en respondre a preguntes de zero, utilitzant la taxonomia per proporcionar un context proximal que ajuda al model a respondre a preguntes siguint rellevants a aquestes preguntes. Mostrem el context d'orientació d'aquesta manera millora el rendiment en quatre conjunts de dades de resposta a preguntes de sentit comú popular.", 'hy': 'Այսօրվա նախապատրաստված լեզվի մոդելները շատ գիտելիք ունեն, բայց ավելի սահմանափակ կարողություն են օգտագործել այդ գիտելիքը: Բլումի Տաքսոնամիան օգնում է ուսուցիչներին սովորեցնել երեխաներին, թե ինչպես օգտագործել գիտելիքը, դասակարգելով ընկալումների հմտությունները, ուստի մենք օգտագործում ենք այն վերլուծելու և բարելավելու համար մեծ նախապատրաստված լեզվի մոդելների ընկալումների հմտությունները Our experiments focus on zero-shot question answering, using the taxonomy to provide proximal context that helps the model answer questions by being relevant to those questions.  Մենք ցույց ենք տալիս, որ նպատակային կոնտեքստը այս կերպ բարելավում է արտադրողությունը 4 հանրային առողջ իմաստության հարցերի պատասխանների տվյալների համակարգերում:', 'jv': "Model sing prakarga bantuan banget dadi akèh, njuk kesempatan kanggo ngerasah tindang kuwi. Tax's Tax's Wang Awak dhéwé iso nglanggar tarjamahan kanggo ngerasakno iki bakal luwih akeh perusahaan kanggo 4 populer sing larang pangan dataset.", 'he': 'לדוגמאות לשפה המאמנות הנוכחיות יש המון ידע, אבל יכולת מוגבלת יותר להשתמש בידע הזה. טקסונומיה של בלום עוזרת לחינוכים ללמד ילדים איך להשתמש בידע על ידי הקטגוריזציה כישורי הבנה, אז אנחנו משתמשים בה כדי לנתח ולשיפר את כישורי הבנה של דוגמנים גדולים לשפה מאומנים מראש. Our experiments focus on zero-shot question answering, using the taxonomy to provide proximal context that helps the model answer questions by being relevant to those questions.  אנחנו מראים הקשר המטרה בדרך זו משפר את ההופעה בארבעה קופסאות נתונים של תשובות לשאלות בהיגיון פופולרי.', 'ha': "Ana da misalin harshen wanda aka tsare a yanzu masu yawa na da ilmi, kuma amma yana da mafi ƙunsa ga amfani da wannan ilmi. Bloom's TaXnomy yana taimakon masu karanta ilmi yadda za su yi amfani da ilmi game da categorizi abinci ga fassari, don haka tuna amfani da shi dõmin ka yi analyi kuma ka kyautata fasalin masu cikin misalin harshen da aka yi wa zaman-wa-fasahan. Kayan jarrabai masu muhimmi zuwa masu karɓa wa tambayi na sifo, da kuma ya yi amfani da akan taksonomi dõmin ya bãyar da mazaɓa da muhimmada wanda ke taimakon misalin su sami masu tambayar. Kana nũna muhallin goani, kamar wannan ne Muke improve performance a kowace matsalar masu tambayar su na zaman shawara 4 ta jama'a.", 'sk': 'Trenutni vnaprej usposobljeni jezikovni modeli imajo veliko znanja, vendar bolj omejeno sposobnost uporabe tega znanja. Bloomova taksonomija pomaga vzgojiteljem učiti otroke, kako uporabljati znanje s kategorizacijo spretnosti razumevanja, zato ga uporabljamo za analizo in izboljšanje spretnosti razumevanja velikih vnaprej usposobljenih jezikovnih modelov. Naši eksperimenti se osredotočajo na odgovor na vprašanja brez strela, pri čemer uporabljamo taksonomijo za zagotavljanje proksimalnega konteksta, ki model pomaga odgovarjati na vprašanja tako, da je relevanten za ta vprašanja. Prikazujemo, da ciljni kontekst na ta način izboljša učinkovitost v štirih naborih podatkov o odgovorih na vprašanja zdrave pameti.', 'bo': "ད་ལྟོའི་སྔོན་གྲངས་བསྐོར་གྱི་སྐད་ཆ་དཔེ་དབྱིབས་མང་པོ་ཡོད། ཡིན་ནའང་ཤེས་ཡོད་ཚད་འདི་ལག་ལེན་འཐབ་ནི་ལྟ Bloom's Taxonomy་གིས་གཟུགས་ཅན་གྱི་སློབ་ཆེན་ཚོར་ལ་གྱིས་ཤེས་ཚད་རྟོགས་པ་ཡིན། ང་ཚོའི་བརྟག་ཞིག་གིས་གནད་གཟུགས་རིས་ལན་གསལ་བྱེད་ཀྱི་ཐབས་ལམ་ལ་དམིགས་བསལ་བྱེད། ང་ཚོས་རྒྱུན་ལྡན་དམིགས་འབེན་ཡུལ་ཐབས་ལམ་འདིའི་ནང་གི་སྒྲུབ་གྲངས་མིའི་མཐུན་སྣེ་བཞིན་པའི་མི་ཚིག་གྲ"}
{'en': 'Learning Sparse Sentence Encoding without Supervision : An Exploration of Sparsity in Variational Autoencoders', 'pt': 'Aprendendo a Codificação de Sentenças Esparsas sem Supervisão: Uma Exploração da Esparsidade em Autoencoders Variacionais', 'es': 'Aprendizaje de la codificación de oraciones dispersas sin supervisión: una exploración de la dispersión en autocodificadores variacionales', 'ar': 'تعلم تشفير الجمل المتفرقة دون إشراف: استكشاف التباين في المبرمجات التلقائية المتغيرة', 'ja': '監督なしでまばらな文章符号化を学ぶ：バリエーションのあるオートエンコーダにおけるまばらさの探求', 'hi': 'पर्यवेक्षण के बिना विरल वाक्य एन्कोडिंग सीखना: परिवर्तनीय Autoencoders में Sparsity की एक खोज', 'ru': 'Изучение кодирования разреженных предложений без надзора: исследование разреженности в вариационных автокодерах', 'fr': 'Apprendre le codage de phrases creuses sans supervision\xa0: une exploration de la parcimonie dans les auto-encodeurs variationnels', 'zh': '学疏句于未监者编码:变分自编码器中疏索', 'ga': 'Ionchódú Pianbhreithe Neamhfhorleathana a Fhoghlaim gan Maoirseacht: Iniúchadh ar thearcúlacht in Uath-ionchódóirí Athróg', 'hu': 'A ritka mondatkódolás felügyelet nélküli tanulása: a ritkaság feltárása a variációs autokódolókban', 'el': 'Μάθηση της αραιής κωδικοποίησης προτάσεων χωρίς επίβλεψη: Μια εξερεύνηση της αραιότητας στους αυτόματους κωδικοποιητές παραλλαγών', 'ka': 'სწავლის სიტყვების კოდირება შესახებ გარეშე არსებული: განსხვავებული ავტოკოდირების განსხვავება', 'it': "Imparare la codifica di frasi sparse senza supervisione: un'esplorazione della sparsità negli autocodificatori variabili", 'lt': 'Mokymasis nedidelių sakinių kodavimas be priežiūros: spartumo tyrimas įvairiuose automatiniuose koduotojuose', 'kk': 'Көрсетілген сөз кодтамасын үйрену: Айнымалы автокодтамасындағы кеңістіктерді зерттеу', 'mk': 'Learning Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders', 'ms': 'Mempelajari Pengekodan Perkataan Tanpa Supervisi: Penjelasan Kecerahan dalam Pengekod Oto Berubah', 'ml': 'സൂപ്പരിഷയല്ലാതെ പഠിക്കുന്ന സ്പേഴ്സിന്റെ ശിക്ഷ കോഡിങ്ങ്: വേറിയറഷന്\u200d സ്പേര്\u200dസിറ്റിയുടെ സ്പെയിന്\u200dസി', 'mn': 'Хэрэглэгч байхгүй Sparse өгүүлбэрийн кодлог сурах: Хэрэглэгч Автоматикийн Кодлогчдын Хэрэглэгч', 'mt': 'It-Tagħlim tal-Kodifikazzjoni tas-Sentenzi Żgħar mingħajr Superviżjoni: Esplorazzjoni tal-Sparsità fl-Awtokodifikaturi Varjazzjonali', 'pl': 'Uczenie się kodowania rzadkich zdań bez nadzoru: badanie oszczędności w automatycznych kodowaniach wariantowych', 'ro': 'Învățarea codării sentințelor slabe fără supraveghere: o explorare a spartității în encoderele automate variaționale', 'no': 'Læring av mellomromsetkoding utan oversikt: Eit utforsk av mellomrom i variasjonale autokodar', 'sr': 'Naučenje kodiranja slova bez nadzora: istraživanje sparsiteta u varijacijskim autokoderima', 'si': 'Name', 'so': 'Learning Sparse Encoding Sentences without Supervision: An Exploration of Sparsity in Variable Autocoders', 'sv': 'Att lära sig sparsam meningskodning utan tillsyn: En utforskning av sparsamhet i Variational Autoencoders', 'ta': 'மேலாண்மை இல்லாமல் வாய்ப்பு குறியீட்டை கற்றுக்கொள்ளும் வாக்கியம்:', 'ur': 'Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders', 'uz': 'Learning Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders', 'vi': 'Học về bản án ngắn mà không cần sự giám sát: Một cuộc thám hiểm về ô nhiễm tự động biến đổi', 'bg': 'Изучаване на оскъдно кодиране на изречения без надзор: изследване на оскъдността в вариационните автокодери', 'da': 'Læring af sparsom sætningskodning uden overvågning: En udforskning af sparsomhed i Variational Autoencodere', 'hr': 'Naučenje kodiranja izraza bez nadzora: Istraživanje sparsiteta u varijacijskim autokoderima', 'nl': 'Leren van schaarse zinnencodering zonder toezicht: een verkenning van schaarse zinnencodering in variatieautoencoders', 'de': 'Lernen von Sparse Satzence Encoding ohne Aufsicht: Eine Erforschung der Sparsität in Variations-Autoencodern', 'id': 'Belajar Kodifikasi Sentensi Tanpa Supervisi: Sebuah Penjelasan Kecerahan dalam Autokoder Variasi', 'ko': '감독 없이 희소 문장 인코딩 학습: 변분 자동 인코더에서의 희소성 탐색', 'fa': 'یاد گرفتن رمزبندی کلمه\u200cهای بین نظارت: یک توسعه جستجو در رمزبندی\u200cهای خودکار متفاوتی', 'sw': 'Kujifunza Uhispania Kufunguliwa kwa hukumu isiyo na Uchunguzi: Utafiti wa Uhispania katika Kumbukumbu Kutofauti', 'af': "Leer Sparse Sentence Enkodering sonder Supervisie: ' n Utspraak van Sparsiteit in Variasionale Autoencoders", 'tr': 'Gözlemsiz Gaýd Edilmegi Öwrenmek: Aýratyn Otomatik Ködlemeler', 'sq': 'Mësimi i kodifikimit të dënimeve të shpejtë pa mbikqyrje: Një zbulim i shpejtësisë në Autokoduesit Variacional', 'am': 'ምርጫዎች', 'hy': 'Սովորել արագ արտահայտությունների կոդավորումը առանց վերահսկողության. արագ արտահայտություն տարբերակային ավտոկոդավորումներում', 'bn': 'সাপার্ভিশন ছাড়া স্পের্সের শিক্ষার্থীর শিক্ষা এনকোডিং: বিভিন্ন স্বয়ংক্রিয়ভাবে স্প্যান্সরিটির এক এক্সপ্', 'az': 'Gözləmədən Sparse Sözü Kodlaması öyrənmək: Dəyişikli Avtomatik Kodlayıcılarda Sparsity Exploration', 'bs': 'Naučenje kodiranja izraza bez nadzora: istraživanje sparsiteta u varijacijskim autokoderima', 'ca': 'Aprendre codificació de frases ràpides sense supervisió: Una exploració de la velocitat en autocodificadors variacions', 'et': 'Hõreda lausekodeerimise õppimine ilma järelevalveta: vaesuse uurimine variatsioonilistes automaatkodeerijates', 'cs': 'U훾en챠 se k처dov찼n챠 힂챠dk첵ch v휎t bez dohledu: zkoum찼n챠 힂챠dkosti v varia훾n챠ch autokod챕rech', 'fi': 'Sparse lausekoodauksen oppiminen ilman valvontaa: Sparsity-tutkimus vaihtelevissa automaattikoodereissa', 'jv': 'politenessoffpolite"), and when there is a change ("assertive', 'he': 'ללמוד קוד משפטים נמוכים ללא פיקוח: חקירה של קידום במקודים אוטומטיים שונים', 'ha': 'KCharselect unicode block name', 'sk': 'Učenje šifriranja redkih stavkov brez nadzora: raziskovanje šibkosti v variacijskih samokodirnikih', 'bo': 'སྒྲ་ཚུལ་ལྟ་བུ་མེད་པའི་བར་སྟོང་པའི་ཚིག་རྐང་སྒྲ་ཚིག་ཀློག་ནུས་མེད་པའི་སྐད་ཡིག་གཏོང་གཏོང：'}
{'en': 'It has been long known that sparsity is an effective  inductive bias  for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of  representation learning . Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in  NLP . Additionally,  NLP  is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.', 'ar': 'من المعروف منذ فترة طويلة أن التباين هو تحيز استقرائي فعال لتعلم التمثيل الفعال للبيانات في المتجهات ذات الأبعاد الثابتة ، وقد تم استكشافه في العديد من مجالات التعلم التمثيلي. من الأمور ذات الأهمية الخاصة لهذا العمل التحقيق في التباين داخل إطار عمل VAE الذي تم استكشافه كثيرًا في مجال الصورة ، ولكنه يفتقر حتى إلى المستوى الأساسي من الاستكشاف في البرمجة اللغوية العصبية. بالإضافة إلى ذلك ، يتخلف البرمجة اللغوية العصبية أيضًا من حيث تعلم التمثيلات المتفرقة لوحدات كبيرة من النص مثل الجمل. نحن نستخدم VAEs التي تحفز التمثيلات الكامنة المتناثرة لوحدات كبيرة من النص لمعالجة أوجه القصور المذكورة أعلاه. أولاً ، نتحرك في هذا الاتجاه من خلال قياس نجاح أحدث التقنيات غير الخاضعة للإشراف (SOTA) وغيرها من خطوط الأساس القوية القائمة على VAE للنصوص ونقترح نموذج VAE المتناثر الهرمي لمعالجة مشكلة الاستقرار في SOTA. بعد ذلك ، ننظر إلى الآثار المترتبة على التباين في تصنيف النص عبر 3 مجموعات بيانات ، ونسلط الضوء على الرابط بين أداء التمثيلات الكامنة المتناثرة في المهام النهائية وقدرتها على تشفير المعلومات المتعلقة بالمهمة.', 'es': 'Se sabe desde hace tiempo que la dispersión es un sesgo inductivo eficaz para aprender la representación eficiente de datos en vectores con dimensionalidad fija, y se ha explorado en muchas áreas del aprendizaje de la representación. De particular interés para este trabajo es la investigación de la dispersión dentro del marco del VAE, que se ha explorado mucho en el dominio de la imagen, pero que carece incluso de un nivel básico de exploración en PNL. Además, la PNL también se está quedando atrás en términos de aprendizaje de representaciones dispersas de grandes unidades de texto, por ejemplo, oraciones. Utilizamos los VAE que inducen representaciones latentes dispersas de grandes unidades de texto para abordar las deficiencias antes mencionadas. En primer lugar, avanzamos en esta dirección al medir el éxito de SOTA (SOTA) no supervisado y otras líneas de base de dispersión sólidas basadas en VAE para el texto y proponemos un modelo de VAE disperso jerárquico para abordar el problema de la estabilidad de SOTA. A continuación, analizamos las implicaciones de la dispersión en la clasificación de textos en 3 conjuntos de datos y destacamos un vínculo entre el rendimiento de las representaciones latentes dispersas en las tareas posteriores y su capacidad para codificar información relacionada con las tareas.', 'pt': 'Há muito se sabe que a esparsidade é um viés indutivo eficaz para aprender representação eficiente de dados em vetores com dimensionalidade fixa, e tem sido explorada em muitas áreas de aprendizagem de representação. De particular interesse para este trabalho é a investigação da esparsidade dentro da estrutura VAE, que tem sido muito explorada no domínio da imagem, mas ainda carece de um nível básico de exploração em PNL. Além disso, a PNL também está atrasada em termos de aprendizado de representações esparsas de grandes unidades de texto, por exemplo, frases. Usamos os VAEs que induzem representações latentes esparsas de grandes unidades de texto para resolver as deficiências acima mencionadas. Primeiro, avançamos nessa direção medindo o sucesso do estado da arte não supervisionado (SOTA) e outras linhas de base de esparsificação baseadas em VAE para texto e propomos um modelo VAE esparso hierárquico para resolver o problema de estabilidade do SOTA. Em seguida, analisamos as implicações da esparsidade na classificação de texto em 3 conjuntos de dados e destacamos um link entre o desempenho de representações latentes esparsas em tarefas downstream e sua capacidade de codificar informações relacionadas à tarefa.', 'fr': "On sait depuis longtemps que la dispersion est un biais inductif efficace pour l'apprentissage de la représentation efficace des données dans des vecteurs à dimensionnalité fixe, et elle a été explorée dans de nombreux domaines de l'apprentissage des représentations. L'étude de la fragilité du cadre VAE, qui a été beaucoup explorée dans le domaine de l'image, est particulièrement intéressante dans le domaine de l'image, mais qui manque même d'un niveau élémentaire d'exploration en PNL. De plus, la PNL est également à la traîne en termes d'apprentissage de représentations clairsemées de grandes unités de texte, par exemple des phrases. Nous utilisons les VAE qui induisent des représentations latentes clairsemées de grandes unités de texte pour pallier les lacunes susmentionnées. Tout d'abord, nous allons dans cette direction en mesurant le succès de l'état de la technique non supervisée (SOTA) et d'autres bases de sparsification basées sur la VAE pour le texte et proposons un modèle VAE hiérarchique clairsemé pour résoudre le problème de stabilité de SOTA. Ensuite, nous examinons les implications de la parcimonie sur la classification de texte dans 3 ensembles de données et soulignons un lien entre les performances des représentations latentes clairsemées sur les tâches en aval et sa capacité à coder des informations liées aux tâches.", 'ja': 'スパルシティは、固定された次元を持つベクトルにおけるデータの効率的な表現を学習するための効果的な帰納的バイアスであることは長い間知られており、表現学習の多くの分野で探求されてきた。 この研究で特に興味深いのは、VAEフレームワーク内の希少性の調査であり、画像領域では多くのことが検討されてきたが、NLPでは基本的なレベルの探索すら不足していた。 加えて、NLPは、文章などの大きな単位の疎な表現を学習するという点でも遅れている。 我々は、前述の欠点に対処するために、大単位のテキストのまばらな潜在的表現を誘発するVAEを使用する。 まず、テキストについて、監督されていない最先端（ SOTA ）および他の強力なVAEベースのスパージベースラインの成功を測定することによって、この方向に進み、SOTAの安定性の問題に対処するための階層的スパースVAEモデルを提案する。 次に、3つのデータセットにわたるテキスト分類における希少性の影響を見て、ダウンストリームタスクにおける希薄な潜在的表現のパフォーマンスと、タスク関連情報をエンコードするその能力との間のリンクを強調します。', 'zh': '人固知疏性是之归差,以学有定维度之向量,而已求之于众域矣。 其事尤感VAE框架内疏,当框架图画领域已大探索,而NLP中至乏本末。 此外NLP学大文本单元(如句)疏疏亦后。 导VAE以疏,以决其短。 先量无监之先进(SOTA)与其余强大之基于VAE之文本疏基线之成功而朝之,发一疏VAE以决SOTA之稳定性。 然后论疏性对 3 数集之文本,而调其下流之疏者,与编码相关。', 'hi': 'यह लंबे समय से ज्ञात है कि निश्चित आयामीता के साथ वैक्टर में डेटा के कुशल प्रतिनिधित्व को सीखने के लिए स्पार्सिटी एक प्रभावी आगमनात्मक पूर्वाग्रह है, और इसे प्रतिनिधित्व सीखने के कई क्षेत्रों में खोजा गया है। इस काम के लिए विशेष रुचि वीएई ढांचे के भीतर स्पार्सिटी की जांच है जिसे छवि डोमेन में बहुत कुछ खोजा गया है, लेकिन एनएलपी में अन्वेषण के बुनियादी स्तर की भी कमी रही है। इसके अतिरिक्त, एनएलपी पाठ की बड़ी इकाइयों जैसे वाक्यों के विरल प्रतिनिधित्व सीखने के मामले में भी पिछड़ रहा है। हम VAEs का उपयोग करते हैं जो उपर्युक्त कमियों को दूर करने के लिए पाठ की बड़ी इकाइयों के विरल अव्यक्त प्रतिनिधित्व को प्रेरित करते हैं। सबसे पहले, हम पाठ के लिए असुरक्षित अत्याधुनिक (SOTA) और अन्य मजबूत VAE-आधारित sparsification baselines की सफलता को मापने के द्वारा इस दिशा में आगे बढ़ते हैं और SOTA की स्थिरता के मुद्दे को संबोधित करने के लिए एक पदानुक्रमित विरल VAE मॉडल का प्रस्ताव करते हैं। फिर, हम 3 डेटासेट में पाठ वर्गीकरण पर स्पार्सिटी के निहितार्थ को देखते हैं, और डाउनस्ट्रीम कार्यों पर विरल अव्यक्त प्रतिनिधित्व के प्रदर्शन और कार्य से संबंधित जानकारी को एन्कोड करने की इसकी क्षमता के बीच एक लिंक को उजागर करते हैं।', 'ru': 'Давно известно, что редкость является эффективным индуктивным смещением для обучения эффективному представлению данных в векторах с фиксированной размерностью, и она была исследована во многих областях репрезентативного обучения. Особый интерес для этой работы представляет исследование редкости в рамках VAE, которое было много изучено в области изображений, но не имело даже базового уровня исследования в NLP. Кроме того, NLP также отстает в изучении редких представлений больших единиц текста, например, предложений. Мы используем VAE, которые вызывают редкие скрытые представления больших единиц текста, чтобы устранить вышеупомянутые недостатки. Во-первых, мы двигаемся в этом направлении, измеряя успех неконтролируемых современных (SOTA) и других сильных спарсификационных базовых линий на основе VAE для текста и предлагаем иерархическую редкую модель VAE для решения проблемы стабильности SOTA. Затем мы рассмотрим влияние редкости на классификацию текста в 3 наборах данных и выделим связь между производительностью редких латентных представлений на последующих задачах и его способностью кодировать информацию, связанную с задачами.', 'ga': 'Tá sé ar eolas le fada gur claonadh éifeachtach ionduchtach í an ghanntanas chun léiriú éifeachtach sonraí a fhoghlaim i veicteoirí le toise seasta, agus rinneadh iniúchadh uirthi i go leor réimsí d’fhoghlaim ionadaíochta. Is ábhar spéise ar leith don obair seo an t-imscrúdú ar theannas laistigh den chreat VAE a ndearnadh iniúchadh mór air i réimse na n-íomhánna, ach a raibh easpa taiscéalaíochta air in NLP fiú. Ina theannta sin, tá NLP chun deiridh freisin maidir le hléirithe tearca d’aonaid mhóra téacs a fhoghlaim m.sh. abairtí. Bainimid úsáid as na VAEanna a chothaíonn léirithe tearca folaigh ar aonaid mhóra téacs chun dul i ngleic leis na heasnaimh thuasluaite. Ar an gcéad dul síos, bogaimid sa treo seo trí rathúlacht na mbonnlínte tearcaithe den scoth gan mhaoirseacht (SOTA) agus eile atá bunaithe ar VAE le haghaidh téacs a thomhas agus molaimid samhail ordlathach tanaí VAE chun aghaidh a thabhairt ar shaincheist chobhsaíochta SOTA. Ansin, féachaimid ar na himpleachtaí a bhaineann le tearcrochtaineacht ar aicmiú téacs thar 3 thacar sonraí, agus leagaimid béim ar nasc idir feidhmíocht léirithe tearca folaithe ar thascanna iartheachtacha agus a cumas faisnéis a bhaineann le tascanna a ionchódú.', 'el': 'Είναι γνωστό από καιρό ότι η σπανιότητα είναι μια αποτελεσματική επαγωγική προκατάληψη για την εκμάθηση αποτελεσματικής αναπαράστασης δεδομένων σε διανύσματα με σταθερή διαστασιμότητα, και έχει διερευνηθεί σε πολλούς τομείς της μάθησης αναπαράστασης. Ιδιαίτερο ενδιαφέρον για την εργασία αυτή είναι η διερεύνηση της λιγοστότητας μέσα στο πλαίσιο της VAE που έχει διερευνηθεί πολύ στον τομέα της εικόνας, αλλά δεν έχει ακόμη ένα βασικό επίπεδο εξερεύνησης στο NLP. Επιπλέον, το ΝΛΠ υστερεί επίσης όσον αφορά την εκμάθηση αραίων αναπαραστάσεων μεγάλων μονάδων κειμένου, π.χ. προτάσεων. Χρησιμοποιούμε τα VAE που προκαλούν αραιές λανθάνουσες αναπαραστάσεις μεγάλων μονάδων κειμένου για την αντιμετώπιση των προαναφερθέντων ελλείψεων. Πρώτον, κινούμαστε προς αυτή την κατεύθυνση μετρώντας την επιτυχία των μη εποπτευόμενων γραμμών βάσης για το κείμενο και προτείνουμε ένα ιεραρχικό μοντέλο για την αντιμετώπιση του ζητήματος σταθερότητας του SOTA. Στη συνέχεια, εξετάζουμε τις επιπτώσεις της αραιότητας στην ταξινόμηση κειμένου σε τρία σύνολα δεδομένων, και επισημαίνουμε μια σύνδεση μεταξύ της απόδοσης αραίων λανθάνοντων αναπαραστάσεων σε μεταγενέστερες εργασίες και της δυνατότητάς της να κωδικοποιεί πληροφορίες σχετικές με τις εργασίες.', 'hu': 'Régóta ismert, hogy a ritkaság hatékony induktív elfogultság az adatok hatékony reprezentációjának megtanulásához fix dimenziós vektorokban, és ezt a reprezentációs tanulás számos területén feltárták. A munka számára különösen érdekes a VAE keretrendszeren belüli ritkaság vizsgálata, amelyet sokat feltártak a kép területén, de még az NLP alapvető szintjének is hiányzott. Ezenkívül az NLP is lemarad a nagy szövegegységek, például mondatok ritka ábrázolásával kapcsolatban. A fent említett hiányosságok kezelésére olyan VAE-ket használunk, amelyek ritkán látens megjelenítéseket idéznek elő nagy szövegegységek ábrázolására. Először is ebben az irányban haladunk azáltal, hogy mérjük a felügyelet nélküli legkorszerűbb (SOTA) és más erős VAE-alapú sparzifikációs alapvonalak sikerét a szövegekhez, és javasoljuk egy hierarchikus ritka VAE modellt a SOTA stabilitási kérdésének kezelésére. Ezt követően megvizsgáljuk a ritkaságnak a szövegosztályozásra gyakorolt hatásait 3 adatkészleten, és kiemeljük a kapcsolatot a ritka látens reprezentációk teljesítménye és a feladatokkal kapcsolatos információk kódolásának képessége között.', 'ka': 'ძალიან ცნობილი იყო, რომ სპერტიცია ეფექტიური ინექტიური განმავლობაში მონაცემების ეფექტიური გამოსახულება გვექტორებში, რომელიც განმავლობული განმავლობაში, და ის განმავლობა მრავალ განმა განსაკუთრებული ინტერესტია ამ სამუშაოში არის VAE ფრამეტრის შიგნის გასწავლობა, რომელიც გამოსახულების დიომინში ძალიან გასწავლობულია, მაგრამ განსახულების ფუნქციური დონე არსებობს NLP დამატებით, NLP კიდევ შეუძლებელია ტექსტის დიდი ერთეულების გამოსახულების შესახებ. ჩვენ გამოყენებთ გარეშე, რომელიც გამოიყენებს გარეშე ტექსტის დიდი ერთეულების ლატენტური გამოსახულებები. პირველად, ჩვენ ამ მხარეს გადავიწყეთ, რომელიც SOTA-ის მუშაობის და სხვა ძალიან VAE-ის სპერსიფიკაციის ფესპერტიფიკაციის ფესპერტიფიკაცია ტექსტის მხარეს წარმატებით გადავიწყეთ და იერაქტიკური მხარეს VAE მოდელ შემდეგ, ჩვენ დავხედავთ ტექსტის კლასიფიკაციაში სამი მონაცემების კონფიკაციაზე სინამდვილეობის შემდეგ, და დააწერეთ სამი მონაცემების კონფიკაციაში შემდეგ სინამდვილეობის შემდეგ სინამ', 'it': "È noto da tempo che la sparsity è un bias induttivo efficace per l'apprendimento di rappresentazione efficiente dei dati in vettori con dimensionalità fissa, ed è stato esplorato in molte aree dell'apprendimento della rappresentazione. Di particolare interesse per questo lavoro è l'indagine della scarsità all'interno del framework VAE che è stato esplorato molto nel dominio dell'immagine, ma è stato carente anche un livello di esplorazione di base in NLP. Inoltre, NLP è anche in ritardo in termini di apprendimento di rappresentazioni scarne di grandi unità di testo, ad esempio frasi. Utilizziamo i VAE che inducono scarse rappresentazioni latenti di grandi unità di testo per affrontare le carenze di cui sopra. In primo luogo, ci muoviamo in questa direzione misurando il successo di SOTA (non supervisionato state-of-the-art) e di altre forti linee di base di sparsificazione basate su VAE per il testo e proponiamo un modello VAE scarno gerarchico per affrontare il problema della stabilità di SOTA. Quindi, esaminiamo le implicazioni della scarsità sulla classificazione del testo in 3 set di dati e evidenziamo un collegamento tra le prestazioni di rappresentazioni latenti sparse sulle attività a valle e la sua capacità di codificare le informazioni relative alle attività.", 'lt': 'Jau seniai žinoma, kad nedažnumas yra veiksmingas paskatinimas mokytis veiksmingai atspindėti duomenis fiksuotojo matmens vektoriuose, ir jis buvo tiriamas daugelyje reprezentacinio mokymosi sričių. Ypač svarbu atlikti šį darbą – tirti nedidelį VAE sistemų skaičių, kuris buvo daug išnagrinėtas vaizdo srityje, tačiau netgi trūko pagrindinio tyrimo NLP srityje. Be to, NLP taip pat atsilieka mokymosi požiūriu, kad didelių teksto vienetų, pvz., sakinių, reprezentacijos yra nedidelės. Siekdami pašalinti pirmiau minėtus trūkumus, naudojame VAE, kurios skatina mažai latentišką didelių teksto vienetų atstovavimą. Pirma, mes judame šia kryptimi išmatuodami nepastebimos pažangiausios technologijos (SOTA) ir kitų tvirtų VAE pagrįstų spartifikacijos pagrindų tekstui sėkmę ir pasiūlydami hierarchinį spartiųjų VAE model į SOTA stabilumo klausimui spręsti. Tuomet išnagrinėjame nedidelio skaičiaus poveikį tekstų klasifikacijai trijuose duomenų rinkiniuose ir pabrėžiame ryšį tarp nedidelio latentinio atspaudimo rezultatų tolesnėse užduotyse ir jos gebėjimo koduoti su užduotimis susijusią informaciją.', 'ms': 'Sudah lama diketahui bahawa kecepatan adalah bias induktif yang efektif untuk mempelajari perwakilan efisien data dalam vektor dengan dimensi tertentu, dan ia telah dieksplorasi dalam banyak kawasan pembelajaran perwakilan. Yang tertarik pada kerja ini ialah penyelidikan kecepatan dalam kerangka VAE yang telah dieksplorasi banyak dalam domain imej, tetapi telah kekurangan walaupun tahap dasar penyelidikan dalam NLP. Selain itu, NLP juga tertinggal dalam terma pembelajaran perwakilan jarang bagi unit teks besar, contohnya kalimat. Kami menggunakan VAE yang menyebabkan perwakilan tersembunyi jarang bagi unit teks besar untuk mengatasi kekurangan yang terdahulu. Pertama, kita bergerak ke arah ini dengan mengukur keberhasilan keadaan-state-of-the-art (SOTA) dan garis dasar penerangan berdasarkan VAE yang kuat lainnya untuk teks dan melamar model VAE yang jarang hierarkis untuk mengatasi isu kestabilan SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.', 'kk': 'Бұл көп бөлшектердің векторлардың ефективті мәліметті көрсету үшін бөлшектердің ефективті индуктивті көзі деп біледі. Бұл көптеген мәліметтердің көптеген жерлерде зерттеулі Бұл жұмыс үшін өзгеше қызықтығы - VAE қоршауындағы кескіндердің доменінде көп зерттеп, бірақ NLP-де негізгі зерттеу деңгейі жоқ. Қосымша, NLP мәтіннің үлкен бірліктерін, мысалы, сөйлемелерді оқыту үшін қалды. Мәтін үлкен бірліктерін өзгерту үшін, келесі бірліктердің келесі келтіріліктерін қолданамыз. Біріншіден, біз бұл бағытта SOTA және басқа VAE-негіздеген кеңістіктер негіздеген кеңістіктерді өлшеп, мәтінді иерархикалық кеңістік VAE моделін SOTA-ның стабильностік мәселесін шешу үшін ұсынамыз. Содан кейін біз 3 деректер қорларындағы мәтін бағалау үшін бөлігінің нәтижесін қарап, төменгі тапсырмалар мен тапсырмалар туралы мәліметті кодтау мүмкіндігі арасындағы сұлбаны бояулады.', 'ml': 'It has been long known that sparsity is an effective inductive bias for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of representation learning.  ഈ പ്രവര്\u200dത്തിക്കുന്നതിന് പ്രത്യേകിച്ച് താല്പര്യമുള്ള വിഎയ് ഫ്രെയിമ്പിലെ സ്പെയിസിറ്റിയെ അന്വേഷിക്കുന്നത് തന്നെ കൂടാതെ, എംഎല്\u200dപി വാക്കുകളുടെ വലിയ യൂണിറ്റുകളുടെ പ്രതിനിധികള്\u200d പഠിക്കുന്നതിന്റെ കാര്യത്തില്\u200d പിന്നില്\u200d നില്\u200dക് നമ്മള്\u200d വിഎസിനെ ഉപയോഗിക്കുന്നു. പുമ്പോഴേക്കുള്ള ചുരുക്കുകളെക്കുറിച്ച് മുന്\u200dകൂട്ടിയുള്ള പ്രതിനിധികള്\u200d  ആദ്യം, സൂക്ഷിച്ചിട്ടില്ലാത്ത സ്റ്റേറ്റ്-ഓ-ആർട്ട് (SOTA) വിജയത്തിനും മറ്റു ശക്തിയുള്ള VAE-അടിസ്ഥാനത്തുള്ള സ്പ്രിസിഫിഷന്\u200d ബേസ്റ്റ് ലെസ്റ്റുകളും അളന്ന പിന്നീട്, മൂന്നു ഡാറ്റാസറ്റുകളില്\u200d പദാവലിയുടെ വിഭവങ്ങളില്\u200d സ്പെസിസ്റ്റിയുടെ പ്രഭാവങ്ങള്\u200d നാം നോക്കുന്നു, പിന്നീട് പുറത്തുള്ള പ്രതിന', 'mk': 'Долго време е познато дека скратноста е ефикасна индуктивна предрасуда за учење ефикасна претстава на податоците во векторите со фиксна димензионалност, и таа е истражена во многу области на учење на претставување. Од особен интерес за оваа работа е истрагата за реткоста во рамките на ВАЕ, која е многу истражена во доменот на сликите, но недостасува дури и основно ниво на истражување во НЛП. Покрај тоа, НЛП, исто така, задоцнува во поглед на учењето на ретки претставувања на големи единици на текст, на пример, реченици. Ние ги користиме ВЕИ кои индуцираат ретки лантни претставувања на големи единици на текст за да ги решиме наведените недостатоци. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA.  Потоа, ги разгледуваме импликациите на малоста на класификацијата на текстот низ три податоци, и ја истакнуваме врската помеѓу изведувањето на мали тајни претставувања на долните задачи и нејзината способност да кодира информации поврзани со задачите.', 'no': 'Det er langt kjent at sparsitet er ein effektiv induktiv bias for å lære effektivt representasjon av data i vektorar med fast dimensjonalitet, og det er utforska i mange område av representasjonslæring. Dette arbeidet er utforskinga av sparsiteten i VAE-rammeverket som er utforska mykje i biletdområdet, men har ikkje noko grunnleggjande utforsking i NLP. I tillegg vil NLP også lage bak med læring av spare representasjonar av store teksteiningar, f.eks. setningar. Vi bruker VAE-ane som induserer latente representasjonar av store teksteiningar for å handtera dei førehandsviste kortbrukane. Først flyttar vi i denne retninga ved å måle suksessen på ikkje-oppretta kunsttilstanden (SOTA) og andre sterke VAE-baserte sparsifikasjonsbaser for tekst og foreslå eit hierarkisk sparse VAE-modell for å handtera SOTA-problemet for stabiliteten. Så ser vi på implikasjonane av sparsitet på tekstklassifikasjon over 3 datasett, og markerer ei lenkje mellom utviklinga av sparse latente representasjonar på nedstrekkoppgåver og kapasiteten til å koda oppgåver-relaterte informasjon.', 'pl': 'Od dawna wiadomo, że rzadkość jest skutecznym indukcyjnym uprzedzeniem do nauki efektywnej reprezentacji danych w wektorach o stałej wymiarowości, i została zbadana w wielu obszarach uczenia się reprezentacji. Szczególnie interesujące dla niniejszej pracy jest badanie rzadkości w ramach VAE, które zostało dużo zbadane w dziedzinie obrazu, ale brakuje nawet podstawowego poziomu eksploracji w NLP. Ponadto NLP pozostaje w tyle pod względem nauki rzadkich reprezentacji dużych jednostek tekstu, np. zdań. Używamy VAE, które wywołują rzadkie ukryte reprezentacje dużych jednostek tekstu, aby rozwiązać wyżej wymienione niedociągnięcia. Po pierwsze, zmierzamy w tym kierunku, mierząc sukces bez nadzoru state-of-the-art (SOTA) i innych silnych opartych na VAE linii bazowych sparsyfikacji tekstu i proponujemy hierarchiczny model rzadkiego VAE, aby rozwiązać problem stabilności SOTA. Następnie przyjrzymy się konsekwencjom słabości na klasyfikację tekstu w trzech zbiorach danych i podkreślamy związek między wydajnością rzadkich utajonych reprezentacji w zadaniach kolejnych a jej zdolnością do kodowania informacji związanych z zadaniami.', 'mn': 'Бүтэн хэмжээсүүд нь тодорхой хэмжээсүүдтэй өгөгдлийн үр дүнтэй үзүүлэлтийг сурах үйлдвэрлэх үйл ажиллагаатай байдал гэдгийг олон хэсэгт мэдэж байна. Энэ ажлын тухай ихэвчлэн сонирхолтой зүйл бол ДОЭ-н хэмжээнд маш олон судалгааг судалсан, Гэвч NLP-д суурь судалгааны үндсэн түвшинд ч байхгүй. Мөн НLP нь мөн том хэмжээний хэмжээний жишээлбэл, өгүүлбэрүүдийг сурах талаар үлдээж байна. Бид ДОХ-г ашиглаж, өнгөрсөн алдагдлыг зохицуулахын тулд том хэмжээний хэмжээсүүдийг ашигладаг. Эхлээд, бид энэ талаар хөдөлгөж, SOTA-ын амжилтыг хэмжээгээр, өөр хүчтэй VAE-ын суурь шугам, мөн SOTA-ын тогтвортой асуудлыг зохицуулахын тулд төрөлхтний төлөвлөгөө VAE-ын төлөвлөгөөг хэмжээгээр хөдөлгөж байна. Дараа нь бид 3 өгөгдлийн сангийн хэлбэрээр текст хэлбэрийн нөлөөлөлтийг харж, дамжуулалтын хамааралтай мэдээллийг кодлох чадварын хоорондох холбоотой.', 'mt': 'Kien magħruf għal żmien twil li l-iskarsezza hija preġudizzju induttiv effettiv għat-tagħlim rappreżentazzjoni effiċjenti tad-dejta f’vetturi b’dimensjonalità fissa, u ġiet esplorata f’ħafna oqsma tat-tagħlim tar-rappreżentanza. Ta’ interess partikolari għal dan ix-xogħol hija l-investigazzjoni tal-iskarsezza fi ħdan il-qafas tal-VAE li ġiet esplorata ħafna fid-dominju tal-immaġni, iżda kienet nieqsa anke livell bażiku ta’ esplorazzjoni fil-NLP. Barra minn hekk, il-NLP g ħadha lura wkoll f’termini ta’ rappreżentazzjonijiet baxxi tat-tagħlim ta’ unitajiet kbar ta’ test e ż., sentenzi. Aħna nużaw l-VAEs li jinduċu rappreżentazzjonijiet baxxi moħbija ta’ unitajiet kbar ta’ test biex nindirizzaw in-nuqqasijiet imsemmija hawn fuq. L-ewwel nett, a ħna nimxu f’din id-direzzjoni billi nkejlu s-suċċess tal-aktar avvanzati mhux sorveljati (SOTA) u linji bażi oħra b’saħħithom ta’ sparsifikazzjoni bbażati fuq VAE għat-test u nipproponu mudell ġerarkiku ta’ VAE baxx biex tiġi indirizzata l-kwistjoni tal-istabbiltà tas-SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.', 'ro': 'Este cunoscut de mult timp că sparsitatea este o părtinire inductivă eficientă pentru învățarea reprezentării eficiente a datelor în vectori cu dimensiune fixă și a fost explorată în multe domenii ale învățării reprezentării. De interes deosebit pentru această lucrare este investigarea limitatății în cadrul VAE, care a fost explorată foarte mult în domeniul imaginii, dar a lipsit chiar și un nivel de explorare de bază în PNL. În plus, PNL este, de asemenea, în urmă în ceea ce privește învățarea reprezentărilor rare ale unităților mari de text, de exemplu, propoziții. Folosim VAE-urile care induc reprezentări latente slabe ale unităților mari de text pentru a remedia deficiențele menționate mai sus. În primul rând, ne mișcăm în această direcție prin măsurarea succesului de stat-of-the-art nesupravegheat (SOTA) și a altor linii de bază puternice de sparsificare bazate pe VAE pentru text și propunem un model ierarhic de VAE rar pentru a aborda problema stabilității SOTA. Apoi, ne uităm la implicațiile lipsității asupra clasificării textului în 3 seturi de date și evidențiem o legătură între performanța reprezentărilor latente rare asupra sarcinilor din aval și capacitatea sa de a codifica informațiile legate de sarcini.', 'sr': 'Dugo je poznato da je sparsitet efikasna induktivna predrasuda za učenje efikasnog predstavljanja podataka u vektorima sa fiksnim dimenzionalnošću, i istražena je u mnogim područjima učenja predstavljanja. Od posebnog interesa za ovaj rad je istraga sparsiteta u okviru VAE-a koja je mnogo istražena u domenu slika, ali nije imala čak i osnovnu razinu istraživanja u NLP-u. Osim toga, NLP takođe ostaje u smislu učenja rezervnih predstavljanja velikih jedinica teksta, na primjer rečenica. Koristimo VAE koji induciraju rezervne latentne predstave velikih jedinica teksta kako bi se riješili predviđene nedostatke. Prvo, krećemo u ovom smjeru mjerenjem uspeha neodređenog stanja umjetnosti (SOTA) i drugih jakih osnovnih linija sparsificiranja na temelju VAE-a za tekst i predlažemo hijerarhički rezervni model VAE-a kako bi se riješili pitanje stabilnosti SOTA-a. Onda pogledamo implikacije sparsiteta na tekstualnu klasifikaciju preko 3 podataka, i naglašavamo vezu između izvršnosti rezervnih latentnih predstavljanja na zadataka koji se nalaze u potpunosti i sposobnosti kodiranja informacija povezanih sa zadatakom.', 'sv': 'Det har länge varit känt att sparsity är en effektiv induktiv bias för att lära sig effektiv representation av data i vektorer med fast dimension, och det har utforskats inom många områden av representationslärande. Av särskilt intresse för detta arbete är utredningen av sparheten inom VAE-ramverket som har utforskats mycket inom bildområdet, men har saknat även en grundläggande nivå av utforskning inom NLP. Dessutom släpar NLP också efter när det gäller att lära sig glesa representationer av stora textenheter, t.ex. meningar. Vi använder VAE-enheter som framkallar glesa latenta representationer av stora textenheter för att åtgärda ovannämnda brister. För det första rör vi oss i denna riktning genom att mäta framgången för oövervakad state-of-the-art (SOTA) och andra starka VAE-baserade sparsifieringsbaselines för text och föreslå en hierarkisk sparsam VAE-modell för att ta itu med stabilitetsfrågan för SOTA. Sedan tittar vi på konsekvenserna av sparsamhet på textklassificering över 3 datauppsättningar, och belyser en koppling mellan prestanda av sparsamma latenta representationer på nedströms uppgifter och dess förmåga att koda uppgiftsrelaterad information.', 'si': 'ඒක දිගටම දැනගත්තා වෙක්ටර් වලට ස්ථිර විශාල විශේෂයෙන් දත්ත ප්\u200dරතිචාරයක් ඉගෙන ගන්න ප්\u200dරයෝජනය සඳහා ප්\u200dරයෝජනය සඳහා ප මේ වැඩේ විශේෂයෙන් විශේෂ ප්\u200dරශ්නයක් තියෙන්නේ VAE පරීක්ෂණයේ පරීක්ෂණය, ඒත් පින්තූර පරීක්ෂණයේ ගොඩක් පරීක්ෂණය කරලා තිය තවත්, NLP වලින් පස්සේ ඉගෙන ඉගෙන ගන්න පුළුවන් විශාල විශාල යුනිකම් වලින් ප්\u200dරතිනිශ්ණතාවක් වගේ. අපි ප්\u200dරයෝජනය කරනවා VAE වලින් ප්\u200dරයෝජනය කරනවා විශාල ප්\u200dරයෝජනයේ ලොකු යුනිටික් වලින් ප්\u200dරයෝජනය කරනවා වග මුලින්ම, අපි මේ පැත්තට යන්නේ නැති විශ්වාසයේ ස්ථානය (SOTA) සහ අනිත් ශක්තිමත් VAE-අධාරිත ප්\u200dරශ්ණ ප්\u200dරශ්ණ ප්\u200dරශ්ණ ප්\u200dරශ්ණ ප්\u200dරශ්ණ ප්\u200dරශ ඊට පස්සේ, අපි බලන්න පාළුවන් විශේෂයේ පරික්ෂණය තුන් දත්ත සේට් වලින් ප්\u200dරතික්ෂණාවක් වලින් ප්\u200dරතික්ෂණාවක් වලින් ප්\u200dරතික්ෂණය', 'so': 'Waxaa waqti dheer la ogaaday in dhaqdhaqaaq uu yahay hab faa’iido leh oo ku barashada faa’iido ku saabsan macluumaadka waddooyinka ku jira, waxaana lagu baaraandegay meelo badan oo lagu barto waxbarashada wakiilka. Kharashka gaarka ah ee shuqulkaas waa baaritaanka dhaqdhaqaalaha ee gudaha VAE, kaas oo aad looga baaraandegay gudaha sawirada, laakiin waxaa ka baahan jiray xitaa heer baaritaanka aasaasiga ah ee NLP. Sidoo kale NLP waxey dib ugu hadhaysaa waxbarashada dhamaantooda oo ah noocyo faro badan oo qoraal ah, tusaale ahaan imtixaanka. Waxaynu isticmaalnaa VAEs, kaas oo soo bandhigaya noocyada ugu dambeeya ee qoraalka oo waaweyn, si aan uga sheekeyno kaalmada hore ee la soo sheegay. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA.  Markaas waxaynu fiirinaynaa saamaynta jimicsiga qoraalka ku saabsan koobsiga saddexda databases, waxaana tusaynaa xiriir u dhexeeya sameynta qaabilaada ugu dambeysa ee shaqada hoose iyo awooddiisa kooban macluumaadka la xiriira shaqada.', 'ta': 'நீண்ட காலம் தெரியப்பட்டுள்ளது வெக்டார்களில் தரவுகளுக்கு தேவையான பங்கீட்டு பிரிவுகளை கற்றுக் கொள்ள ஒரு செயல்படுத்தும் பிரிவுகள் ஆகும். அது ந இந்த வேலையில் குறிப்பிட்ட ஆர்வம் என்னவென்றால் VAE சட்டத்தில் உள்ள வெளிர்ச்சியின் ஆய்வு அது பிம்பத்தின் களத்தில் நிறைய தேடியிருக்க கூடுதலாக, NLP பெரிய உரை அலகுகளின் குறிப்புகளின் குறிப்புகளை கற்றுக் கொள்ளும் பொதுவான வாக்கியங்களில் தான் பின் நாம் விஏஸ் பயன்படுத்துகிறோம் அது சிறிது சமீபத்தில் உள்ள உரையின் பெரிய அலகுகளை தொகுக்குகிறது முந்தைய குறைந்த க முதலில், நாம் இந்த திசையில் நகர்த்துகிறோம் பாதுகாப்பாக்கப்படாத மாநிலையின் வெற்றியையும் மற்றும் வலிமையான VAE-அடிப்படையில் உள்ள வெற்றிப்பு அடிப்படைகளையும் அளவிட மூன்று தரவுத்தளங்கள் முழுவதும் உரை வகைப்படுத்தல் பார்ப்போம் மற்றும் குறிப்பிட்ட செயல்பாடுகளின் சிறிய பிரதிநிதிகளின் செயல்பாடுகளின் இடைய', 'ur': 'بہت سی وقت معلوم ہوا ہے کہ سپٹیٹی ایک اثر انگیز غرض ہے کہ ویکتروں میں مضبوط اندازے کے ساتھ دکھانے کے مطابق دکھانے کے لئے دکھانے کے لئے دکھانے کے مطابق فعال ہے، اور اس کو بہت سی منطق میں تحقیق کی گئی ہے اس کام کے لئے مخصوص علاقہ ہے کہ VAE فرمیٹ کے اندر سستی کا تحقیق ہے جو تصویر ڈمین میں بہت زیادہ تحقیق کیا گیا ہے، لیکن NLP میں بھی ایک بنیادی سطح کی تحقیق کمی ہے. اور اضافہ، NLP بھی پیچھے چھوڑ رہا ہے لکھنے کے بہت بڑے واحدوں کی تعلیمات کے مطابق، جماعت. ہم ان وائن کو استعمال کرتے ہیں جو اس سے پہلے گزرے ناکاموں کے بارے میں لٹینٹ وائن کی نشانیاں اٹھاتے ہیں۔ پہلی بار، ہم اس طرح چلتے ہیں اس طرح ناپابندی کی موفقیت (SOTA) اور دوسری طاقتور VAE-based sparsification baselines کی موفقیت کا اندازہ کر رہے ہیں، اور SOTA کی استواری مسئلہ کے بارے میں ایک حیراتیکل sparse VAE موڈل کی پیشنهاد کریں۔ پھر ہم تین ڈیٹ سٹ کے درمیان ٹیکسٹ کلاسیٹ پر استرسی کے اثرات کو دیکھتے ہیں، اور نیچے ٹیکسٹ ٹیکسٹ معلومات کے عملکرد کے درمیان ایک لینک مشخص کر دیتے ہیں اور اس کی قابلیت کو ٹیکسٹ رابطہ دار معلومات کے ساتھ کوڈ کر', 'vi': 'Đã lâu lắm rồi, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài, thưa ngài. Đặc biệt quan tâm tới việc này là nghiên cứu về căn lều của các biến dạng đã được tìm hiểu rất nhiều trong lĩnh vực hình ảnh, nhưng lại thiếu cả một cấp độ cơ bản để thăm dò tại Njala. Thêm vào đó, chọc dò tủy sống đang bị chậm về việc học được các mô tả của các đơn vị văn bản lớn, ví dụ như câu. Chúng tôi sử dụng các V.E (VAE) để thực hiện các mô tả nhỏ trên các đơn vị văn bản lớn để xử lý các thiếu sót trên đó. Trước tiên, chúng ta đi theo hướng này bằng cách đo lường thành công của những thứ chưa được giám sát thời trang (SOTA) và những căn bản phân tán nhiệt nhiệt cao cấp của VAE cho văn bản, và đề xuất một mô hình báo cáo cao cấp của VAE để xử lý vấn đề ổn định của SOTA. Sau đó, chúng ta nhìn vào tác động của hạn chế về việc phân loại văn bản trên các tập tin ba, và nhấn mạnh mối liên hệ giữa các biểu đồ hở hang trên các công việc xuôi dòng và khả năng mã hóa các thông tin liên quan đến nhiệm vụ.', 'uz': "Ko'p paytda, qismlik, vektordagi ma'lumotlarning effektiv qo'llanmalarni o'rganish uchun ishlatuvchi harakat qilinadi, va bu ko'pchilik o'rganishning ko'plab sohalarda qidirilgan. Bu ish uchun qiziqarli qiziqarli, rasm domen ichida ko'p qidirilgan VAE chegarasini qidirish mumkin, lekin NLP'da ko'p qidirish imkoniyati yo'q. Qo'shimcha, NLP matnning katta birlashtirilgan birliklarni o'rganishda esa keladi. Biz VAEsdan foydalanamiz, birinchi oldingi taʼminlovchi qisqa matnning katta bir bir bir bir necha tashkilotlarini boshlaydi. Birinchi so'zda, biz xavfsiz saqlangan holatning muvaffaqiyatlarini aniqlash va boshqa sterk VAE asosida saqlash asosini o'zgartirish va SOTA muammolarini murojaat qilish uchun hierarchik saqlash modelini talab qilamiz. Keyin biz 3 maʼlumotlar tarkibidagi matn darajasining muammolarini ko'rib turamiz va quyidagi vazifalar bilan kichik qisqa taʼminlovchi tashkilotlarni bajarish va vazifa bilan maʼlumot kodlash imkoniyatini ko'rib chiqaramiz.", 'bg': 'Отдавна е известно, че рядкостта е ефективен индуктивен пристраст за учене на ефективно представяне на данни във вектори с фиксирани измерения и е проучен в много области на обучението за представяне. Особен интерес за тази работа е изследването на рядкостта в рамките на ВАЕ, което е изследвано много в областта на изображенията, но липсва дори основно ниво на проучване в НЛО. Освен това НЛП изостава и по отношение на изучаването на редки изображения на големи единици текст, например изречения. Използваме ВАЕ, които предизвикват рядко латентно представяне на големи единици текст, за да преодолеем гореспоменатите недостатъци. Първо, ние се придвижваме в тази посока, като измерваме успеха на ненадзорните най-съвременни (СОТА) и други силни базирани на СПАЕ базови линии за текст и предлагаме йерархичен модел на СПАЕ за решаване на проблема със стабилността на СОТА. След това разглеждаме последиците от оскъдността върху класификацията на текста в 3 набора от данни и подчертаваме връзката между изпълнението на оскъдни латентни представяния на задачи надолу по веригата и способността му да кодира информация, свързана със задачите.', 'nl': "Het is al lang bekend dat sparsity een effectieve inductieve bias is voor het leren van efficiënte representatie van gegevens in vectoren met vaste dimensionaliteit, en het is onderzocht op veel gebieden van representatie leren. Van bijzonder belang voor dit werk is het onderzoek naar de schaarste binnen het VAE framework dat veel is verkend in het beelddomein, maar zelfs ontbreekt aan een fundamenteel niveau van verkenning in NLP. Daarnaast loopt NLP ook achter op het gebied van het leren van schaarse representaties van grote eenheden tekst, bijvoorbeeld zinnen. We gebruiken de VAE's die schaarse latente representaties van grote teksteenheden veroorzaken om bovengenoemde tekortkomingen aan te pakken. Ten eerste gaan we in deze richting door het succes te meten van onbeheerde state-of-the-art (SOTA) en andere sterke VAE-based sparsificatie baselines voor tekst en stellen we een hiërarchisch sparse VAE-model voor om het stabiliteitsprobleem van SOTA aan te pakken. Vervolgens kijken we naar de implicaties van sparsity op tekstclassificatie in drie datasets en benadrukken we een verband tussen de prestaties van sparse latente representaties op downstreamtaken en de mogelijkheid om taakgerelateerde informatie te coderen.", 'da': "Det har længe været kendt, at sparsitet er en effektiv induktiv bias til at lære effektiv repræsentation af data i vektorer med fast dimension, og det er blevet undersøgt på mange områder af repræsentation læring. Af særlig interesse for dette arbejde er undersøgelsen af sparsommeligheden inden for VAE-rammerne, som er blevet udforsket meget inden for billeddomænet, men har manglet selv et grundlæggende niveau af udforskning i NLP. Derudover er NLP også bagud med hensyn til at lære sparsomme repræsentationer af store enheder tekst, f.eks. sætninger. Vi bruger VAE'er, der fremkalder sparsomme latente repræsentationer af store enheder tekst til at afhjælpe ovennævnte mangler. For det første bevæger vi os i denne retning ved at måle succesen af ukontrolleret state-of-the-art (SOTA) og andre stærke VAE-baserede sparsifikationsbasislinjer for tekst og foreslå en hierarkisk sparse VAE-model for at løse stabilitetsspørgsmålet i SOTA. Derefter ser vi på konsekvenserne af sparsomhed på tekstklassifikation på tværs af 3 datasæt og fremhæver en forbindelse mellem ydeevnen af sparsomme latente repræsentationer på downstream opgaver og dens evne til at kode opgaverelaterede oplysninger.", 'hr': 'Dugo je poznato da je sparsitet učinkovit induktivni pristrasnost učenja učinkovitog zastupanja podataka u vektorima s fiksnim dimenzionalnošću, a istražena je u mnogim područjima učenja zastupanja. Posebno zanimljivo za ovaj rad je istraga rezerviteta u okviru VAE-a koja je mnogo istražena u domenu slika, ali nije imala čak i osnovnu razinu istraživanja u NLP-u. Osim toga, NLP također ostaje u smislu učenja rezervnih predstavljanja velikih jedinica teksta, na primjer rečenica. Koristimo VAE-ove koji induciraju rezervne latentne predstave velikih jedinica teksta kako bi se riješili navedene nedostatke. Prvo, krećemo u ovom smjeru mjerenjem uspjeha neodređenog stanja umjetnosti (SOTA) i drugih jakih osnova sparsificiranja na temelju VAE-a za tekst i predlažemo hijerarhički model rezervnih VAE-a kako bi se riješili pitanje stabilitete SOTA-a. Onda pogledamo implikacije rezerviteta na klasifikaciju teksta na tri podataka i naglašavamo povezanost između učinka rezervnih latentnih predstavljanja na zadataka u donjem nivou i sposobnosti kodiranja informacija povezanih s zadatakom.', 'id': 'Sudah lama diketahui bahwa kecepatan adalah bias induktif yang efektif untuk belajar representation efisien data dalam vektor dengan dimensi tetap, dan telah dieksplorasi di banyak bidang dari belajar representation. Yang tertarik pada pekerjaan ini adalah penyelidikan kecepatan dalam cadangan VAE yang telah banyak dieksplorasi dalam daerah gambar, tetapi bahkan kekurangan tingkat dasar eksplorasi di NLP. Selain itu, NLP juga tertinggal dalam hal mempelajari representation jarang dari unit besar teks, misalnya kalimat. Kami menggunakan VAE yang mengakibatkan representation rahasia yang sedikit dari unit besar teks untuk mengatasi kekurangan-kekurangan yang sebelumnya. Pertama, kita bergerak ke arah ini dengan mengukur keberhasilan dari state-of-the-art (SOTA) yang tidak diawasi dan garis dasar sparsifikasi VAE yang kuat lainnya untuk teks dan mengusulkan sebuah model VAE yang tidak diawasi untuk mengatasi masalah stabilitas SOTA. Kemudian, kita melihat implikasi kecepatan pada klasifikasi teks melalui 3 set data, dan mempertimbangkan hubungan antara prestasi dari representation rahasia yang sedikit pada tugas turun dan kemampuannya untuk mengkode informasi berkaitan dengan tugas.', 'ko': '사람들은 희소성이 효과적인 귀납 편차라는 것을 일찍부터 알고 있다. 학습 데이터가 고정된 차원 벡터에서 효과적인 표시를 하는 데 사용되고 학습을 나타내는 많은 분야에서 탐색을 진행했다.이 작업은 VAE 프레임워크 내의 희소성에 대한 연구로 이미지 분야에서 이미 대량의 탐색을 진행했지만 NLP에서는 기본적인 탐색 수준조차 부족하다.또한 NLP는 문장과 같은 대형 텍스트 셀의 드문드문 표현을 배우는 데도 뒤떨어진다.이러한 단점을 해결하기 위해 VAE를 사용하여 대용량 텍스트 셀의 희소한 잠재적 표현을 유도합니다.우선, 우리는 무감독 최신기술(SOTA)과 다른VAE 기반의 텍스트 희소화 기선의 성공률을 측정하여 이 방향으로 전진하고 차원 희소화VAE모델을 제시하여 SOTA의 안정성 문제를 해결한다.그 다음에 우리는 희소성이 3개의 데이터 집합 텍스트 분류에 미치는 영향을 연구했고 하류 작업에서 희소성이 잠재적으로 나타내는 성능과 인코딩 작업과 관련된 정보의 능력 간의 관계를 강조했다.', 'fa': 'خیلی وقته شناخته شده است که اسپارتیتی یک پیشرفت تاثیر فعالی برای یادگیری نمایش داده\u200cهای موثر در ویکتورها با اندازه ثابت است، و در بسیاری از منطقه\u200cهای یادگیری نمایش داده شده است. از علاقه خاصی برای این کار، تحقیقات آرامشی در چهارچوب VAE است که خیلی در دامنه تصویر تحقیق شده است، ولی حتی سطح بنیادی تحقیقات در NLP وجود ندارد. اضافه\u200cای، NLP همچنین در مورد تعلیم نمونه\u200cهای کمی از واحدهای بزرگ متن، مثال جمله\u200cها پشت سر می\u200cگذارد. ما از VAEs استفاده می\u200cکنیم که نشان\u200cدهندگان latent از واحدهای بزرگ متن را برای رسیدن به shortcomings پیش\u200cفرستاده می\u200cکنند. اول، ما در این مسیر حرکت می کنیم با اندازه موفقیت موفقیت ایالت هنر غیرقابل استفاده (SOTA) و دیگر پایگاه\u200cهای استفاده از VAE بر اساس متن استفاده می\u200cکنیم و یک مدل قابل استفاده VAE را پیشنهاد می\u200cکنیم تا مسئله استواری SOTA را حل کند. سپس، ما به تأثیرات سفارشی از تأثیر متن در مجموعه\u200cهای داده\u200cای نگاه می\u200cکنیم، و یک ارتباط بین فعالیت نمایش\u200cهای سفارشی لاتین در کار های پایین\u200cترین و توانایی آن برای تأثیر اطلاعات رابطه\u200cی کار را تأثیر می\u200c', 'tr': "Uzun süre berildi ki, sergilik vektörlerde sabit boyutlu verileri ifade etmek için etkisiz bir etkisiz önlemdir ve birçok alanda temsil öğrenmek için çalıştırılmıştır. Bu işiň has gyzyklanmasy, VAE çerçevesinde köp gezek keşfedildi, ýöne NLP'da esasy bir gezek keşif derejesi ýok. Mundan hem NLP sözler üçin uly metin birimlerini öwrenmek üçin yzynda galýar. Biz VAE'yi kullanıyoruz ki, önceden gelişmelere göre büyük bir metin oluşturmalarını kaynaklayan büyük bir şekilde gösterirler. Birinjisi, biz bu yönde sungatyň başarıgyny ölçüp, SOTA we beýleki güýçli VAE-dan daşary gaýşartma tabanlyklaryň üstesini ölçüp, we SOTA'yň stability meselesini çözmek üçin bir hijerarşik rezil VAE modelini teklif edip görýäris. Sonra, tekst klasifikasynda 3 sany guruldygynda gözlemçilik etkinleşigine seredip görýäris we işe baran maglumatlar arasynda gözlemçilik bolup geçirilýäris.", 'de': 'Es ist seit langem bekannt, dass Sparsity ein effektiver induktiver Bias ist, um effiziente Repräsentation von Daten in Vektoren mit fester Dimensionalität zu lernen, und es wurde in vielen Bereichen des Repräsentationslernens erforscht. Von besonderem Interesse für diese Arbeit ist die Untersuchung der Sparsität innerhalb des VAE-Frameworks, die zwar im Bildbereich viel erforscht wurde, jedoch nicht einmal ein grundlegendes Explorationsniveau in NLP aufweist. Darüber hinaus hinkt NLP auch beim Erlernen spärlicher Darstellungen großer Texteinheiten, z.B. Sätze, hinterher. Wir verwenden die VAEs, die spärliche latente Darstellungen großer Texteinheiten induzieren, um die oben genannten Mängel zu beheben. Zunächst gehen wir in diese Richtung, indem wir den Erfolg von unüberwachten State-of-the-Art (SOTA) und anderen starken VAE-basierten Sparsifikationsbasislinien für Text messen und ein hierarchisches Sparse-VAE-Modell vorschlagen, um das Stabilitätsproblem von SOTA anzugehen. Anschließend betrachten wir die Auswirkungen von Sparsity auf die Textklassifizierung in drei Datensätzen und zeigen einen Zusammenhang zwischen der Leistung von spärlichen latenten Darstellungen auf nachgelagerten Aufgaben und ihrer Fähigkeit, aufgabenbezogene Informationen zu kodieren.', 'af': "Dit is lank bekend dat sparsiteit 'n effektief induktiewe bias is vir die leer van effektief verskyning van data in vektore met vaste dimensionaliteit, en dit is ondersoek in baie areas van verskyning leer. Van bepaalde belang vir hierdie werk is die ondersoek van die sparsiteit binne die VAE raamwerk wat baie in die beelddomein uitgevoer is, maar het al 'n basiese vlak van uitvoering in NLP ontbreek. In addition, NLP is also lagging behind in terms of learning sparse representations of large units of text, e.g. sentences. Ons gebruik die VAE wat sparse latente voorstellings van groot eenhede van teks induseer om die voorgeskryfde kortpaaklikhede te adres. Eerste, ons beweeg in hierdie rigting deur die sukses van ononderwerpende staat van die kuns (SOTA) en ander sterk VAE-gebaseerde sparsifikasie basisline vir teks te maak en voorstel 'n hierarkies sparse VAE model om die stabiliteit probleem van SOTA te adres. Dan kyk ons na die implikasies van sparsiteit op teks klasifikasie oor 3 datastelle, en verlig 'n skakel tussen prestasie van sparse latente voorstellings op onderstreem taak en sy moontlik om taak verwante inligting te kodeer.", 'sw': 'Kwa muda mrefu imejulikana kuwa uchochezi ni upendeleo wenye ufanisi wa kujifunza uwakilizaji ufanisi wa taarifa katika vectors wenye utofauti wa hali ya juu, na umekuwa ukichunguzwa katika maeneo mengi ya kujifunza uwakilizaji. Tatizo has a kwa kazi hii ni uchunguzi wa uchunguzi wa ukame ndani ya mfumo wa VAE ambao umekuwa ukichunguzwa sana katika maeneo ya picha, lakini umekuwa na ukosefu wa kiwango cha msingi cha uchunguzi katika NLP. Additionally, NLP is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences.  Tunatumia VAEs ambalo hutengeneza uwakilishi wa hivi karibuni wa vitengo vikubwa vya maandishi ili kuhusiana na upungufu uliotajwa hapo awali. Kwanza, tunahamia mwelekeo huu kwa kupima mafanikio ya hali ya sanaa isiyohifadhiwa (SOTA) na misingi mengine yenye nguvu ya uongozi wa VAE kwa ajili ya ujumbe wa kuandika na kupendekeza mtindo wa VAE ili kutangaza suala la usalama wa SOTA. Kisha, tunaangalia matokeo ya ukame kuhusu usambazaji wa maandishi katika seti tatu za takwimu, na kuonyesha kiungo kati ya utendaji wa uwakilishi wa hivi karibuni katika kazi za mito ya chini na uwezo wake wa kuweka taarifa zinazohusiana na kazi za kazi.', 'am': 'ብዛት በጥቅልነት የዳታ አካባቢ መሆኑን ለመማር የተሻለ የጥያቄ ውሳኔ ነው፡፡ ለዚህ ሥራ የተጠቃሚ ውጤት የVAE ፍሬማት ውስጥ በብዙ ነገር በተመረጠው ነገር ግን የNLP መሠረት መሠረት እንኳ ጎደለበት፡፡ በተጨማሪም፣ NLP የጽሑፍ ብዛት ምሳሌ፣ የጽሑፍ ብዛት ምሳሌ ክፍሎች በተማረ ግንኙነት በኋላ ይቆያል፡፡ የቀድሞው የጽሑፍ ብዛት የጽሑፎችን አካባቢዎች ለመጠቀም እናስቀምጣለን፡፡ በመጀመሪያው፣ በተጠበቀው የ-የ-የ-ዐርድ ስኬት (SOTA) እና ሌሎችን ብርቱ VAE-based ስፋፊያ መሠረት መሠረት እና የSOTA ጥናት ለመጠቀም የሀይሬርክቲክ ምርጫዎች የVAE ሞዴል እናስባለን፡፡ ከዚያም በኋላ በ3 ዳታዎች ላይ የጽሑፍ ክፍተት ጥቅምት እናየዋለን፤ እና በአውራው ፈቃድ ላይ የሚቆጠሩ የቅርብ መልዕክቶች እና የስራ ማህተት ማቀናጃ ማቀናጃ ላይ የሚችሉትን ግንኙነት እና ማክበር እናደርጋለን፡፡', 'hy': "It has been long known that sparsity is an effective inductive bias for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of representation learning.  Այս աշխատանքի մեջ հատկապես հետաքրքրված է այն հազվադեպության հետազոտությունը, որը հայտնաբերվել է պատկերի ոլորտում, բայց նույնիսկ բացակայում է ՆԼՊ-ի ուսումնասիրության հիմնական մակարդակի: Ավելին, ՆԼՊ-ը նույնպես հետաքրքիր է ուսումնասիրության առումով, ինչպես օրինակ նախադասությունները: Մենք օգտագործում ենք ՎԱԵ-ները, որոնք առաջացնում են մեծ տեքստի միավորների բազմաթիվ թաքնված ներկայացումներ, որպեսզի լուծենք նախորդ բացակայությունները: Առաջինը, մենք շարժվում ենք այս ուղղությամբ' չափելով անվերահսկված տեխնոլոգիայի (SOTEA) և այլ ուժեղ, ՎԵԵ-ի հիմքում հիմնագծերի հաջողությունը տեքստի համար և առաջարկում ենք հիերարխիկ անվերահսկված ՎԵ մոդել SOTEA կայունության խնդիրը լուծելու համար: Այնուհետև, մենք նայում ենք հազվադեպության ազդեցություններին տեքստի դասակարգման վրա 3 տվյալների համակարգերի միջև և շեշտում ենք կապը հազվադեպ թաքնված ներկայացումների արտադրողության և դրա կարողության կոդավորել խնդիրների հետ կապված տեղեկությունը:", 'az': "Çox uzun zamandır bilirdi ki, küçük səviyyəti, vektorlarda möhkəm ölçülərlə verilən məlumatları öyrənmək üçün effektiv təsirli təsirdir və bu təsiri öyrənmək məlumatlarının çoxlu bölgelerində keşfedildi. Bu işin məqsədilə məxluqatı VAE framework ündə çox keşif edilmiş, amma NLP'də hətta temel keşif seviyyəsi yoxdur. Üstəlik, NLP də böyük mətnlərin, məsələlər kimi cümlələr öyrənməsi üçün geridə qalar. Biz, əvvəlkilərin azğınlıqlarını çəkmək üçün böyük mətn birliklərinin latent göstərişlərini istifadə edirik. İlk dəfə, biz bu tərəfdə, SOTA və başqa güclü VAE-tərəfli sparsifikasyon səhifələrinin başarısızlığını ölçürük və SOTA-nın istiqamətlıq məqsədilə çəkmək üçün hiyerarşik sparse VAE modelini təklif edirik. Sonra, üç veri qutusu arasında metin klasifikasiyasının küçük səviyyəsinə baxıb, a şağıdaki işlərdə küçük latent göstəricilərin və işlərlə bağlı məlumatların kodlaması üçün bir bağlantısını aydınlaşdırırıq.", 'bn': 'দীর্ঘদিন পরিচিত হয়েছে যে স্প্যারিসি একটি কার্যকর প্রতিনিধিত্বের জন্য ভেক্টরের তথ্যের কার্যকর প্রতিনিধিত্ব শিখতে পারে এবং প্রতিনিধিত্ব শিক এই কাজের জন্য বিশেষ আগ্রহ হচ্ছে ভিএই ফ্রেমের মধ্যে স্মৃতির তদন্ত, যা ছবির ডোমেইনে অনেক বেশী তদন্ত করা হয়েছে, কিন্তু এনএলপিতে এমনকি একটি মৌলিক পর্যায় এর সাথে এনএলপি বিশাল টেক্সট ইউনিটের প্রতিনিধিত্বের স্মান্য প্রতিনিধিত্ব শেখার ব্যাপারে পিছনে থাকেন, যেমন শাস আমরা ভিএএস ব্যবহার করি যে সাম্প্রতিক প্রতিনিধিত্বের সাম্প্রতিক প্রতিনিধিত্ব ব্যবহার করে আগের উল্লেখিত সংক্রান্ত ব প্রথমত, আমরা এই দিকে যাচ্ছি অরক্ষিত রাষ্ট্র-অফ-শিল্প (SOTA) এবং অন্যান্য শক্তিশালী ভিয়াই-ভিত্তিক স্প্রিসিফিকেশনের বেসাইজেনের মাপ মাপের মাধ্যমে এবং সোটার স্থিত তারপর আমরা তিন ডাটাসেটের বিভিন্ন টেক্সট বিভাগের উপর স্প্রাইসির প্রভাবের দিকে তাকিয়ে থাকি এবং নীচের কাজের উপর সাম্প্রতিক প্রতিনিধিত্বের সাম্', 'sq': 'Ka qenë e njohur prej kohësh se pakësia është një paragjykim induktiv efektiv për mësimin e përfaqësimit të efektshëm të të dhënave në vektorë me dimensionalitet fikse dhe është eksploruar në shumë fusha të mësimit të përfaqësimit. Me interes të veçantë për këtë punë është hetimi i pakësisë brenda kuadrit VAE që është eksploruar shumë në domenin e imazhit, por ka munguar edhe një nivel bazë eksplorimi në NLP. Përveç kësaj, NLP po mbetet gjithashtu prapa në lidhje me mësimin e përfaqësimeve të pakta të njësive të mëdha të tekstit, për shembull, fjalëve. Ne përdorim VAE-të që nxisin përfaqësime të pakta të fshehta të njësive të mëdha të tekstit për të trajtuar mungesat e mëparshme. Së pari, ne lëvizim në këtë drejtim duke matur suksesin e gjendjes së lartë të pa mbikqyrur (SOTA) dhe linjave bazë të tjera të forta të shpërndarjes me bazë në VAE për tekst dhe duke propozuar një model hierarkik të shkurtër të VAE për të trajtuar çështjen e stabilitetit të SOTA. Pastaj, ne shohim pasojat e pakësisë në klasifikimin e tekstit nëpër 3 grupe të dhënash, dhe theksojmë një lidhje midis performancës së paraqitjeve të pakta të fshehta në detyrat poshtë rrjedhës dhe aftësisë së saj për të koduar informacionin lidhur me detyrat.', 'cs': 'Je již dlouho známo, že řídkost je efektivní indukční bias pro učení efektivní reprezentace dat ve vektorech s pevnou dimenzionalitou, a to bylo zkoumáno v mnoha oblastech reprezentačního učení. Zvláštním zájmem této práce je zkoumání řídkosti v rámci VAE, které bylo hodně zkoumáno v obrazové oblasti, ale chybí dokonce i základní úroveň průzkumu v NLP. Navíc NLP zaostává také v oblasti učení řídkých reprezentací velkých jednotek textu, např. vět. K řešení výše uvedených nedostatků používáme VAE, které vyvolávají řídké latentní reprezentace velkých jednotek textu. Nejprve se pohybujeme tímto směrem měřením úspěchu nedozorovaných state-of-the-art (SOTA) a dalších silných základních linií sparsifikace založených na VAE pro text a navrhneme hierarchický model řídké VAE pro řešení problému stability SOTA. Dále se podíváme na důsledky řídkosti na klasifikaci textu napříč třemi datovými sadami a zdůrazňujeme souvislost mezi výkonem řídkých latentních reprezentací na následných úkolech a její schopností kódovat informace související s úkoly.', 'et': 'On ammu teada, et hõredus on tõhus induktiivne eelarvamus andmete tõhusa esitamise õppimiseks fikseeritud dimensiooniga vektorites ja seda on uuritud paljudes representatsiooniõppe valdkondades. Eriti huvitav on uurida VAE raamistiku hõredust, mida on pildi valdkonnas palju uuritud, kuid millel puudub isegi põhitase uurimise tase NLP-s. Lisaks on NLP maha jäänud ka suurte tekstiühikute (nt lausete) hõredate esituste õppimise osas. Eespool nimetatud puuduste kõrvaldamiseks kasutame VAE-sid, mis tekitavad suurte tekstiühikute varjatud esitusi. Esiteks liigume selles suunas, mõõtes järelevalveta kaasaegsete (SOTA) ja muude tugevate VAE-põhiste hõreduse lähtejoonte edukust teksti jaoks ning pakume välja hierarhilise hõreda VAE mudeli, et lahendada SOTA stabiilsuse probleem. Seejärel uurime hõreduse mõju teksti klassifitseerimisele kolme andmekogumi vahel ning rõhutame seost järgnevate ülesannete hõredate latentsete esituste sooritamise ja ülesannetega seotud teabe kodeerimise võime vahel.', 'fi': 'On jo pitkään tiedetty, että sparsiteetti on tehokas induktiivinen bias tiedon tehokkaaseen esittämiseen vektoreissa, joilla on kiinteä ulottuvuus, ja sitä on tutkittu monilla representaatiooppimisen osa-alueilla. Erityisen kiinnostavaa tässä työssä on VAE-kehyksen niukkuuden tutkiminen, jota on tutkittu paljon kuvaalalla, mutta jota ei ole tutkittu edes perustasolla NLP:ssä. Lisäksi NLP on jäljessä myös suurten tekstiyksiköiden, esim. lauseiden, niukkojen esitysten oppimisessa. Käytämme VAE:itä, jotka aiheuttavat harvoja piileviä representaatioita suurista tekstiyksiköistä edellä mainittujen puutteiden korjaamiseksi. Ensin siirrymme tähän suuntaan mittaamalla valvomattoman nykytekniikan (SOTA) ja muiden vahvojen VAE-pohjaisten sparsifiointiperusteiden menestystä tekstille ja ehdottamalla hierarkkista harvojen VAE-mallia SOTA:n vakausongelman ratkaisemiseksi. Sitten tarkastelemme sparsiteetin vaikutuksia kolmen aineiston tekstin luokitteluun ja korostamme linkkiä harvojen piilevien esitysten suorittamisen ja sen kyvyn koodata tehtävään liittyviä tietoja.', 'bs': 'Dugo je poznato da je sparsitet efikasna induktivna predrasuda za učenje učinkovitog predstavljanja podataka u vektorima s fiksnim dimenzionalnošću, i istražena je u mnogim područjima učenja predstavljanja. Od posebnog interesa za ovaj rad je istraga rezerviteta u okviru VAE-a koja je mnogo istražena u domenu slika, ali nije imala čak i osnovnu razinu istraživanja u NLP-u. Osim toga, NLP također ostaje u smislu učenja rezervnih predstavljanja velikih jedinica teksta, na primjer rečenica. Koristimo VAE koji induciraju rezervne latentne predstave velikih jedinica teksta kako bi se riješili navedene nedostatke. Prvo, krećemo u ovom smjeru mjerenjem uspjeha neodređenog stanja umjetnosti (SOTA) i drugih jakih osnova sparsificiranja na temelju VAE-a za tekst i predlažemo hijerarhički model rezervnih VAE-a kako bi se riješili pitanje stabilitete SOTA-a. Onda pogledamo implikacije rezerviteta na tekstsku klasifikaciju preko 3 kompeta podataka, i istaknemo vezu između učinka rezervnih latentnih predstavljanja na zadataka koji se nalaze u potpunosti i sposobnosti kodiranja informacija povezanih sa zadatakom.', 'ca': "Fa molt de temps que la escassetat és un bias inductiu efectiu per aprendre una representació eficient de dades en vectors amb dimensionalitat fixa, i s'ha explorat en moltes àrees d'aprenentatge de representació. De especial interès per aquesta feina és l'investigació de la escassetat dins el marc VAE que s'ha explorat molt en el domini de la imatge, però que fins i tot ha faltat un nivell bàsic d'exploració en NLP. A més, la NLP també està atrasada en termes d'aprenentatge representacions escases de grans unitats de text, per exemple frases. Utilitzem les VAE que indueixen escases representacions latents de grans unitats de text per abordar les deficiències mencionades. Primer, ens movem en aquesta direcció mitjançant l'èxit d'un avançat avançat no supervisat (SOTA) i d'altres línies de base fortes de sparsificació basades en VAE per al text i proposant un model jeràrquic escas de VAE per abordar la qüestió d'estabilitat de SOTA. Llavors mirem les implicacions de la escassetat en la classificació de textos en tres conjunts de dades, i destaquem un enllaç entre el rendiment de petites representacions latents en tasques avall i la seva habilitat de codificar informació relacionada amb tasques.", 'ha': "An san cewa cewa mai sauri shi ne wata tamko mai amfani da za'a sanar da masu da sha'awa masu tsari da data a cikin masu sakarattu da sifanci mai daidaita, kuma an sami shi a cikin wurãre mãsu yawa na karantar mutane. Ina da amfani da wannan aikin yana tambayar suriyar muhimmi a cikin firam na WAU wanda aka samu masu yawa a cikin shekarar zanen, kuma amma yana ƙaranci ko wata daraja na ƙaranci a NLP. Da ƙaranci, NLP yana saka bayan da aka sani masu ƙaranci masu tsari wa sunaye masu girma na matsayin, misali, sonar. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings.  Kayyan da, za mu motsa a cikin wannan gefen da za'a ƙayyade babban rabo na-tsari-the-art (SOTA) da wasu masu ƙarfin maɓallin sarrafiya masu nau'a wa SOTA zuwa matsayin kuma munãƙayyade wata misali na hierrrchical savings vAU dõmin ka yi addu'in masu tabbatarwa na SOTA. Sa'an nan, muna dũba masu hushi ga fasalin matsayi a bayan tsari 3, kuma Mu nuna wata link tsakanin aikin masu ƙaranci a kan aikin kwanan da ke ƙaranci a downriver da awonsa ga kodi-da-yanzu-yanzu-yanzu.", 'he': 'כבר הרבה זמן ידוע שהנדירות היא היחידה induktivת יעילה ללמוד מייצג יעיל של נתונים בוקטורים עם מימדיות קבועה, והיא נחקרה באזורים רבים של לימוד מייצג. Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in NLP.  בנוסף, NLP גם מאחור בנוגע ללמוד מייצגים נדירים של יחידות גדולות של טקסט, למשל משפטים. אנו משתמשים באמצעי החשמל המעוררים מייצגים חסרים של יחידות גדולות של טקסט כדי להתמודד עם החסרונות הללו. ראשית, אנו זזים לכיוון הזה על ידי למדוד הצלחה של מצב המיוחד ללא השגחה (SOTA) ובקווי הבסיס החזקים החזקים אחרים על בסיס VAE לטקסט ולהציע דוגמנית VAE היררכית ללא השגחה כדי להתמודד עם עניין היציבות של SOTA. ואז, אנו מסתכלים על השלכות של נדירות על מסווג טקסטים בשלושה קבוצות נתונים, ומדגישים קשר בין ביצועים של מייצגות נדירות מוסתרות על משימות מתחתיות והיכולת שלה לקוד מידע קשור למשימות.', 'bo': 'འདི་ལས་ཕར་ཆེན་བྱུང་བར་ཆེ། ཟླ་བ་ནི་གནས་ཚུལ་དང་མཉམ་དུ་འཇུག་ནུས་ཅན་གྱི་རྐྱེན་རིས་སྟོན་རུང་ཅན་དང་། Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in NLP. མ་ཟད། NLP་དེ་ལས་ཀྱང་སྦྱར་བའི་ཡིག་གེ་ཤོག་འཇོག་བྱེད་པའི་སྐབས་བཤད་ཆེན་པོ་ཞིག་ཏུ་ཉར་འཇུག་ཡོད། We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.', 'sk': 'Že dolgo je znano, da je redkost učinkovita induktivna pristranskost za učenje učinkovite reprezentacije podatkov v vektorjih s fiksno dimenzionalnostjo, raziskana pa je bila tudi na številnih področjih reprezentacijskega učenja. Posebej zanimiva je raziskava redkosti znotraj okvira VAE, ki je bila veliko raziskana na področju slike, vendar ji manjka celo osnovna raven raziskovanja NLP. Poleg tega NLP zaostaja tudi v smislu učenja redkih predstavitev velikih enot besedila, npr. stavkov. Za odpravo omenjenih pomanjkljivosti uporabljamo VAE, ki inducirajo redke latentne predstavitve velikih enot besedila. Najprej se premaknemo v to smer z merjenjem uspeha nenadzorovanih najsodobnejših (SOTA) in drugih močnih osnovnih črt sparzifikacije na osnovi VAE za besedilo ter predlagamo hierarhični model redkih VAE za reševanje vprašanja stabilnosti SOTA. Nato preučimo posledice redkosti na klasifikacijo besedila v treh naborih podatkov in poudarimo povezavo med izvedbo redkih latentnih predstavitev pri nadaljnjih opravilih in njegovo sposobnostjo kodiranja informacij, povezanih z opravili.', 'jv': 'section Genjer-genjer perusahaan kanggo obahan iki dadi keujahan paten pating nggawe barang karo CVS sing dumadhi iki, sane ono sakdurungé sakdurungé sakdurungé NLP tambah, NLP iso ngejaraké mbutuhak cara-cara sing paling kelas @item:checkbox First, we are move in in this direction by measurement the success of the unuppidged state-of-the-arts (So MT) and the next strength of PVS-supported partitions basic linear for text and proposal a herearchitectureal spase VE model to Address the stable question of So ta. Amarok'}
{'en': 'Temporal-aware Language Representation Learning From Crowdsourced Labels', 'pt': 'Representação de linguagem com reconhecimento temporal aprendendo com rótulos de crowdsourcing', 'es': 'Aprendizaje de la representación lingüística con conciencia temporal de etiquetas colaborativas', 'fr': "Apprentissage de la représentation linguistique sensible au temps à partir d'étiquettes participatives", 'ar': 'تعلم التمثيل اللغوي الواعي للوقت من تسميات التعهيد الجماعي', 'hi': 'लौकिक-जागरूक भाषा प्रतिनिधित्व Crowdsourced लेबल से सीखना', 'ja': 'クラウドソーシングされたラベルからの時間認識言語表現学習', 'zh': '从众包标签中学习的时候感知言语', 'ru': 'Обучение языку с учетом временных особенностей с помощью краудсорсинговых меток', 'ga': 'Léiriú Teanga atá feasach ar an Am Ag Foghlaim Ó Lipéid Sluaite', 'hu': 'Temporal-award Language Representation Learning from Crowdsourced Labels', 'kk': 'Температуралық түсінікті тілді таңдау жарлықтардан үйрену', 'el': 'Προσωρινή γλωσσική αναπαράσταση που μαθαίνει από ετικέτες με πολλαπλές πηγές', 'ka': 'Name', 'it': 'Apprendimento della rappresentazione linguistica consapevole del tempo dalle etichette Crowdsourced', 'mk': 'Претстава на јазикот на привремено свесен за учење од народни етикети', 'ms': 'Perwakilan Bahasa Sedia-sementara Belajar Dari Label Sumber Kerumunan', 'mt': 'Rappreżentanza tal-Lingwi li tkun konxja temporanjament Tagħlim minn Tikketti ta’ Sors tal-Kultura', 'lt': 'Temporal-aware Language Representation Learning From Crowdsourced Labels', 'ml': 'ക്രോഡ്സോര്\u200dസ്സെര്\u200dസ്സ് ലേബുകളില്\u200d നിന്നും പഠിക്കുന്ന ഭാഷ', 'mn': 'Температур ойлголттой хэл төлөөлөгчийн суралцах', 'ro': 'Învățarea reprezentării limbii conștiente de timp de la etichete Crowdsourced', 'si': 'Name', 'pl': 'Czasowa reprezentacja języka uczenie się z etykiet społecznościowych', 'so': 'Waxbarashada luqada ee ku meelgaar ah ee ku meelgaar ah aqoonsan', 'sr': 'Predstavljanje jezika koji je poznat na vreme, učenje iz etiketa Crowdsourced', 'ur': 'Name', 'sv': 'Temporal-medveten språkrepresentation lärande från crowdsourced etiketter', 'ta': 'தற்காலிக அறிவிக்கப்பட்ட மொழி பிரதிரிவு படிப்பு', 'no': 'Læring av mellombels språk- representasjon frå Crowdsourced Labels', 'uz': 'Name', 'vi': 'Truyền thuyết thuyết thuyết thuyết về ngôn ngữ học từ nhãn tụ tập', 'bg': 'Образование на езиково представяне с времево съзнание от етикети', 'nl': 'Tijdelijke taalvertegenwoordiging Leren Van Crowdsourced Labels', 'de': 'Zeitbewusste Sprachrepräsentation Lernen von Crowdsourced Labels', 'da': 'Temporal-award Language Representation Learning From Crowdsourced Labels', 'hr': 'Predstavljanje jezika na vrijeme svjesne učenje iz etiketa Crowdsourced', 'ko': '패키지 라벨 기반 시제 감지 언어 표징 학습', 'fa': 'نمایش زبان معمولی از برچسب\u200cهای Crowdsourced Learning from Crowdsourced Language', 'sw': 'Kujifunza maoni ya lugha kwa muda mfupi', 'af': 'Name', 'tr': 'Taryh Etiketlerden öwrenmek üçin Wagtlaýyn Bilen Dil Temsilcisi', 'sq': 'Temporal-aware Language Representation Learning From Crowdsourced Labels', 'hy': 'Ժամանակական գիտակցություն ունեցող լեզվի ներկայացումը, սովորելով ժողովրդի կողմից', 'az': 'Q캼ss캼z Etiketl톛rd톛n 칐yr톛nm톛k', 'id': 'Perwakilan Bahasa yang sadar-sementara belajar dari Label Sumber Kerajaan', 'bs': 'Predstavljanje jezika koji je poznat na vrijeme, učenje iz etiketa Crowdsourced', 'am': 'source', 'cs': 'Časově orientovaná jazyková reprezentace Učení se z crowdsource štítků', 'et': 'Ajaliselt teadlik keele esindamine õppimine ühisallikatest märgistest', 'bn': 'ক্রুডসোর্স লেবেল থেকে প্রতিনিধিত্ব শিক্ষা', 'ca': "Representació temporal de llenguatges aprenent a partir d'etiquetes d'origen públic", 'fi': 'Ajankohtainen kielten edustaminen Oppiminen joukkolähdemerkinnöistä', 'jv': 'Language', 'sk': 'Časovno zavedanje jezikovnega predstavništva Učenje iz množičnih nalepk', 'he': 'מייצגת שפה מודעת לזמנית ללמוד מתוויות', 'ha': 'KCharselect unicode block name', 'bo': 'ཤོག་བྱང་ཐོག་དང་བསྟུན་ནས་སྐད་ཆ་ཤོས་པའི་སྐད་ཡིག་Representative Learning From Crowdsourced Labels'}
{'en': 'Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions. In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism ; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed  heuristic  is extremely easy to implement in around 5 lines of code. The proposed  heuristic  is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art  baselines  in terms of  prediction accuracy  and  AUC . To encourage the reproducible results, we make our code publicly available at. TACMA , a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed heuristic is extremely easy to implement in around 5 lines of code. The proposed heuristic is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at  https://github.com/CrowdsourcingMining/TACMA .', 'ar': 'يعد تعلم تمثيلات اللغة الفعالة من تسميات التعهيد الجماعي أمرًا بالغ الأهمية للعديد من مهام التعلم الآلي في العالم الحقيقي. يتمثل أحد الجوانب الصعبة لهذه المشكلة في أن جودة تسميات التعهيد الجماعي تعاني من تقلبات عالية داخل وبين المراقبين. نظرًا لأن الشبكات العصبية العميقة عالية السعة يمكنها بسهولة حفظ جميع الخلافات بين تسميات التعهيد الجماعي ، فإن التطبيق المباشر لخوارزميات تعلم تمثيل اللغة الخاضعة للإشراف قد يؤدي إلى حلول دون المستوى الأمثل. في هذه الورقة ، نقترح TACMA ، وهو دليل تعليمي لتمثيل اللغة مدركًا للوقت للتسميات التعهيد الجماعي مع العديد من التعليقات التوضيحية. النهج المقترح (1) نماذج صريحة للتغير داخل المراقب مع آلية الانتباه ؛ (2) يحسب ويجمع درجات الثقة لكل عينة من عدة عمال لمعالجة الخلافات بين المراقبين. من السهل جدًا تنفيذ الاستدلال المقترح في حوالي 5 أسطر من التعليمات البرمجية. يتم تقييم الاستدلال المقترح على أربع مجموعات بيانات تركيبية وأربع مجموعات بيانات حقيقية. تظهر النتائج أن نهجنا يتفوق في الأداء على مجموعة واسعة من خطوط الأساس الحديثة من حيث دقة التنبؤ والجامعة الأمريكية بالقاهرة. لتشجيع النتائج القابلة للتكرار ، نجعل الكود الخاص بنا متاحًا للجمهور على <https://github.com/CrowdsourcingMining/TACMA>.', 'fr': "L'apprentissage de représentations linguistiques efficaces à partir d'étiquettes participatives est essentiel pour de nombreuses tâches d'apprentissage automatique dans le monde réel. Un aspect difficile de ce problème est que la qualité des labels issus du crowdsourcing souffre d'une variabilité élevée entre les observateurs et entre les observateurs. Étant donné que les réseaux de neurones profonds à haute capacité peuvent facilement mémoriser tous les désaccords entre les labels issus du crowdsourcing, l'application directe des algorithmes d'apprentissage supervisé de la représentation linguistique existants peut aboutir à des solutions sous-optimales. Dans cet article, nous proposons TACMA, une heuristique d'apprentissage de la représentation linguistique sensible au temps pour les étiquettes crowdsourcées avec plusieurs annotateurs. L'approche proposée (1) modélise explicitement la variabilité intra-observateur avec le mécanisme d'attention\xa0; (2) calcule et agrège les scores de confiance par échantillon de plusieurs travailleurs afin de résoudre les désaccords entre observateurs. L'heuristique proposée est extrêmement facile à implémenter en 5 lignes de code environ. L'heuristique proposée est évaluée sur quatre ensembles de données synthétiques et quatre ensembles de données réels. Les résultats montrent que notre approche surpasse un large éventail de bases de pointe en termes de précision des prévisions et de SSC. Pour favoriser la reproductibilité des résultats, nous mettons notre code à la disposition du public à l'adresse < https://github.com/CrowdsourcingMining/TACMA >.", 'es': 'Aprender representaciones lingüísticas eficaces a partir de etiquetas colaborativas es crucial para muchas tareas de aprendizaje automático del mundo real. Un aspecto desafiante de este problema es que la calidad de las etiquetas colaborativas sufre una alta variabilidad intra e interobservador. Dado que las redes neuronales profundas de alta capacidad pueden memorizar fácilmente todos los desacuerdos entre las etiquetas colaborativas, la aplicación directa de los algoritmos de aprendizaje de representación lingüística supervisada existentes puede producir soluciones subóptimas. En este artículo, proponemos TACMA, una heurística de aprendizaje de representación lingüística con conciencia temporal para etiquetas colaborativas con múltiples anotadores. El enfoque propuesto (1) modela explícitamente la variabilidad intraobservador con el mecanismo de atención; (2) calcula y agrega las puntuaciones de confianza por muestra de múltiples trabajadores para abordar los desacuerdos entre observadores. La heurística propuesta es extremadamente fácil de implementar en unas 5 líneas de código. La heurística propuesta se evalúa en cuatro conjuntos de datos sintéticos y cuatro del mundo real. Los resultados muestran que nuestro enfoque supera a una amplia gama de líneas de referencia de última generación en términos de precisión de predicción y AUC. Para fomentar los resultados reproducibles, publicamos nuestro código en < https://github.com/CrowdsourcingMining/TACMA >.', 'pt': 'Aprender representações de linguagem eficazes a partir de rótulos de crowdsourcing é crucial para muitas tarefas de aprendizado de máquina do mundo real. Um aspecto desafiador desse problema é que a qualidade dos rótulos crowdsourced sofre alta variabilidade intra e interobservador. Como as redes neurais profundas de alta capacidade podem memorizar facilmente todos os desacordos entre os rótulos de crowdsourcing, a aplicação direta dos algoritmos de aprendizado de representação de linguagem supervisionada existentes pode produzir soluções abaixo do ideal. Neste artigo, propomos o TACMA, uma heurística de aprendizagem de representação de linguagem com reconhecimento temporal para rótulos de crowdsourcing com vários anotadores. A abordagem proposta (1) modela explicitamente a variabilidade intra-observador com mecanismo de atenção; (2) calcula e agrega escores de confiança por amostra de vários trabalhadores para abordar as discordâncias entre observadores. A heurística proposta é extremamente fácil de implementar em cerca de 5 linhas de código. A heurística proposta é avaliada em quatro conjuntos de dados sintéticos e quatro do mundo real. Os resultados mostram que nossa abordagem supera uma ampla gama de linhas de base de última geração em termos de precisão de previsão e AUC. Para incentivar os resultados reproduzíveis, disponibilizamos nosso código publicamente em <https://github.com/CrowdsourcingMining/TACMA>.', 'zh': '学有效于众包标签者,言机器于世也。 一有挑战性者,众包标在观察者、观察者之间,皆有高可变性。 高容量深神经网络可轻记众包标签之间所有不同,故直以见监言学算生次优解决方案。 本文发TACMA,一时感言学启发式,施于众包注器。 所建之法(1)明拟观察者内变异性注意。 (2) 计会数工作人员样本置信度分数,以决观察者之不同。 其言启发式法甚易于大约5行代码中。 于四合成数集及四真数集上评其启发式法。 结果表明,吾道胜准确性AUC先进之基线。 劝可重复,<https://github.com/CrowdsourcingMining/TACMA>明代码。', 'ja': 'クラウドソーシングされたラベルから効果的な言語表現を学ぶことは、多くの現実の機械学習タスクにとって重要です。 この問題の困難な側面は、クラウドソーシングされたラベルの品質が、サーバ内およびサーバ間の変動が大きいことです。 大容量の深層ニューラルネットワークは、クラウドソーシングされたラベル間のすべての不一致を容易に記憶することができるため、既存の監視対象言語表現学習アルゴリズムを直接適用すると、最適以下の解決策が得られる可能性がある。 本稿では、複数のアノテーターを持つクラウドソーシングされたラベルのための時間認識言語表現学習ヒューリスティックであるTACMAを提案する。 提案されたアプローチは、（ 1 ）注意メカニズムで観察者内の変動を明示的にモデル化する。（ 2 ）複数の作業者からのサンプルごとの信頼スコアを計算して集計し、観察者間の意見の相違に対処する。 提案されているヒューリスティックは、5行前後のコードで実装するのが非常に簡単です。 提案されたヒューリスティックは、4つの合成データセットと4つの現実世界のデータセットで評価されます。 その結果、我々のアプローチは、予測精度とAUCの点で幅広い最先端のベースラインを上回っていることが示されました。 再現可能な結果を促進するため、コードはで公開されてい<https://github.com/CrowdsourcingMining/TACMA>ます。', 'hi': 'Crowdsourced लेबल से प्रभावी भाषा प्रतिनिधित्व सीखना कई वास्तविक दुनिया मशीन सीखने के कार्यों के लिए महत्वपूर्ण है। इस समस्या का एक चुनौतीपूर्ण पहलू यह है कि crowdsourced लेबल की गुणवत्ता उच्च इंट्रा- और अंतर-पर्यवेक्षक परिवर्तनशीलता का सामना करती है। चूंकि उच्च क्षमता वाले गहरे तंत्रिका नेटवर्क आसानी से क्राउडसोर्स्ड लेबल के बीच सभी असहमतियों को याद कर सकते हैं, इसलिए सीधे मौजूदा पर्यवेक्षित भाषा प्रतिनिधित्व सीखने के एल्गोरिदम को लागू करने से सबऑप्टिमल समाधान प्राप्त हो सकते हैं। इस पेपर में, हम TACMA का प्रस्ताव करते हैं, एक अस्थायी-जागरूक भाषा प्रतिनिधित्व कई annotators के साथ crowdsourced लेबल के लिए हेरिस्टिक सीखना। प्रस्तावित दृष्टिकोण (1) स्पष्ट रूप से ध्यान तंत्र के साथ इंट्रा-ऑब्जर्वर परिवर्तनशीलता को मॉडल करता है; (2) अंतर-पर्यवेक्षक असहमति को संबोधित करने के लिए कई श्रमिकों से प्रति-नमूना विश्वास स्कोर की गणना और समुच्चय करता है। प्रस्तावित हेरिस्टिक कोड की लगभग 5 लाइनों में लागू करना बेहद आसान है। प्रस्तावित हेरिस्टिक का मूल्यांकन चार सिंथेटिक और चार वास्तविक दुनिया के डेटा सेट पर किया जाता है। परिणाम बताते हैं कि हमारा दृष्टिकोण भविष्यवाणी सटीकता और एयूसी के मामले में अत्याधुनिक आधार रेखाओं की एक विस्तृत श्रृंखला को मात देता है। पुन: प्रस्तुत करने योग्य परिणामों को प्रोत्साहित करने के लिए, हम अपने कोड को सार्वजनिक रूप से <https://github.com/CrowdsourcingMining/TACMA> पर उपलब्ध कराते हैं।', 'ru': 'Изучение эффективных языковых представлений от краудсорсинговых меток имеет решающее значение для многих реальных задач машинного обучения. Сложным аспектом этой проблемы является то, что качество краудсорсинговых этикеток характеризуется высокой внутри- и межнаблюдательной изменчивостью. Поскольку высокопроизводительные глубокие нейронные сети могут легко запоминать все разногласия среди краудсорсинговых меток, непосредственное применение существующих контролируемых алгоритмов обучения языковому представлению может привести к неоптимальным решениям. В этой статье мы предлагаем TACMA, эвристику обучения языку с учетом времени для краудсорсинговых меток с несколькими аннотаторами. Предлагаемый подход (1) явно моделирует вариабельность внутри наблюдателя с механизмом внимания; (2) вычисляет и агрегирует баллы доверия на выборку от нескольких работников для устранения разногласий между наблюдателями. Предлагаемая эвристика чрезвычайно проста в реализации примерно в 5 строках кода. Предлагаемая эвристика оценивается на основе четырех синтетических и четырех реальных наборов данных. Результаты показывают, что наш подход превосходит широкий спектр современных базовых линий с точки зрения точности прогнозирования и AUC. Чтобы стимулировать воспроизводимые результаты, мы делаем наш код общедоступным по адресу<https://github.com/CrowdsourcingMining/TACMA>.', 'ga': "Tá sé ríthábhachtach léirithe teanga éifeachtacha a fhoghlaim ó lipéid sluafhoinsithe le haghaidh go leor tascanna meaisínfhoghlama sa saol fíor. Gné dhúshlánach den fhadhb seo is ea go bhfulaingíonn caighdeán na lipéid sluafhoinsithe éagsúlacht ard laistigh agus idir- bhreathnadóirí. Ós rud é gur féidir leis na líonraí néaracha doimhne ard-acmhainne gach easaontas i measc lipéad sluafhoinsithe a chur de ghlanmheabhair, d'fhéadfadh go n-eascródh réitigh fho-optamacha trí na halgartaim foghlama ionadaíochta teanga maoirsithe atá ann faoi láthair a chur i bhfeidhm go díreach. Sa pháipéar seo, molaimid TACMA, heorastúil foghlama ionadaíochta teanga atá feasach ar an aimsir le haghaidh lipéid sluafhoinsithe le nótaíoirí iolracha. Sa chur chuige atá beartaithe (1) samhlaítear go sainráite an éagsúlacht laistigh den bhreathnadóir le meicníocht aird; (2) déanann sé scóir muiníne in aghaidh an tsampla ó iloibrithe a ríomh agus a chomhiomlánú chun aghaidh a thabhairt ar easaontais idir breathnadóirí. Tá sé thar a bheith éasca an heuristic atá molta a chur i bhfeidhm i thart ar 5 líne de chód. Déantar an heuristic atá beartaithe a mheas ar cheithre thacar sonraí shintéiseacha agus ar cheithre thacar sonraí ón bhfíorshaol. Léiríonn na torthaí go sáraíonn ár gcur chuige raon leathan bonnlínte úrscothacha maidir le cruinneas tuartha agus AUC. Chun na torthaí atáirgthe a spreagadh, cuirimid ár gcód ar fáil go poiblí ag <https://github.com/CrowdsourcingMining/TACMA>.", 'hu': 'A közösségi forrásból származó címkék hatékony nyelvi reprezentációjának tanulása kulcsfontosságú számos valós gépi tanulási feladathoz. Ennek a problémának egy kihívást jelentő aspektusa, hogy a közösségi forrásokból származó címkék minősége nagy a megfigyelőkön belüli és megfigyelőközi változékonysággal jár. Mivel a nagy kapacitású mélyneurális hálózatok könnyen megjegyzik a közösségi forrásból származó címkék közötti nézeteltéréseket, a meglévő felügyelt nyelvreprezentációs tanulási algoritmusok közvetlen alkalmazása szuboptimális megoldásokat eredményezhet. Ebben a tanulmányban javasoljuk a TACMA-t, egy időtudatos nyelvreprezentációs heurisztikát a több kommentátorral rendelkező, közösségi forrású címkék számára. A javasolt megközelítés (1) kifejezetten a megfigyelőn belüli változékonyságot figyelemmel kíséri; (2) a megfigyelők közötti nézeteltérések kezelése érdekében több munkavállaló mintánkénti konfidenciapontszámát számítja és aggregálja. A javasolt heurisztika rendkívül könnyen megvalósítható körülbelül 5 sor kódban. A javasolt heurisztikát négy szintetikus és négy valós adatkészleten értékelik. Az eredmények azt mutatják, hogy megközelítésünk meghaladja a legkorszerűbb referenciaértékek széles skáláját az előrejelzési pontosság és az AUC tekintetében. A reprodukálható eredmények előmozdítása érdekében kódunkat nyilvánosan elérhetővé tesszük < https://github.com/CrowdsourcingMining/TACMA >.', 'el': 'Η εκμάθηση αποτελεσματικών γλωσσικών αναπαραστάσεων από ετικέτες είναι κρίσιμη για πολλές εργασίες μηχανικής μάθησης στον πραγματικό κόσμο. Μια προκλητική πτυχή αυτού του προβλήματος είναι ότι η ποιότητα των ετικετών υφίσταται υψηλή μεταβλητότητα εντός και μεταξύ παρατηρητών. Δεδομένου ότι τα βαθιά νευρωνικά δίκτυα υψηλής χωρητικότητας μπορούν εύκολα να απομνημονεύσουν όλες τις διαφωνίες μεταξύ ετικετών με πολλαπλές πηγές, η άμεση εφαρμογή υφιστάμενων αλγόριθμων εκμάθησης εποπτευόμενης γλωσσικής αναπαράστασης μπορεί να δώσει υποβέλτιστες λύσεις. Σε αυτή την εργασία, προτείνουμε μια χρονική συνειδητή γλωσσική αναπαράσταση εκμάθησης heuristic για ετικέτες με πολλαπλούς σχολιαστές. Η προτεινόμενη προσέγγιση (1) μοντελοποιεί ρητά τη μεταβλητότητα εντός παρατηρητή με μηχανισμό προσοχής· (2) υπολογίζει και συγκεντρώνει τις βαθμολογίες εμπιστοσύνης ανά δείγμα από πολλούς εργαζομένους για την αντιμετώπιση των διαφωνιών μεταξύ παρατηρητών. Η προτεινόμενη heuristik είναι εξαιρετικά εύκολη να εφαρμοστεί σε περίπου πέντε γραμμές κώδικα. Η προτεινόμενη heuristik αξιολογείται σε τέσσερα συνθετικά και τέσσερα σύνολα πραγματικών δεδομένων. Τα αποτελέσματα δείχνουν ότι η προσέγγισή μας ξεπερνά ένα ευρύ φάσμα σύγχρονων βασικών γραμμών όσον αφορά την ακρίβεια πρόβλεψης και την AUC. Για να ενθαρρύνουμε τα αναπαραγώγιμα αποτελέσματα, κάνουμε τον κώδικα μας δημοσίως διαθέσιμο στο < https://github.com/CrowdsourcingMining/TACMA >.', 'kk': 'Көпшілікті жарлықтардан эффективті тілдерді үйрену көпшілік әлемді машиналық оқыту тапсырмалары үшін маңызды. Бұл мәселенің көпшілік аспекті - көпшілікті жарлықтардың сапасы көпшілікті интро- және қарау арасындағы айнымалылығы жоғары. Жоғары көмектесетін невралдық желілер көмектесетін жарлықтардың барлық кейбістіктерін оңай еске салуға мүмкін болып, барлық тілдерді бақылау алгоритмдері көмектесетін тілдерді бақылау алгоритмд Бұл қағазда, біз TACMA дегенді таңдаймыз, бірнеше жазбалармен бірнеше жазбалар үшін көпкүлікті жазбалар үшін бір уақытша түсінікті тілдерді көрсету үшін. Келтірілген тәсілі (1) қарау механизмінің интернет бақылаушысының айнымалылығын түсіндіреді. (2) Бірнеше жұмыс істеушілерден бірнеше сенімдік нәтижелерін есептеп, бірнеше жұмыс істеушілердің қайсылықтарын шешу үшін есептеп береді. Келтірілген геуристика 5 сызық кодтың орындалуы өте оңай. Келтірілген геуристика төрт синтетикалық және төрт әлемдегі деректер жиындарына тең алады. Нәтижелер біздің тәсіліміздің көпшілігіміз өзгертілген суреттің негізгі сызықтарын көрсетеді. Қайталануға болатын нәтижелерді көмектесу үшін, біз кодымызды көмектесу үшін < https://github.com/CrowdsourcingMining/TACMA >.', 'it': "Imparare rappresentazioni linguistiche efficaci da etichette crowdsourcing è fondamentale per molte attività di apprendimento automatico del mondo reale. Un aspetto impegnativo di questo problema è che la qualità delle etichette crowdsourcing soffre di un'elevata variabilità intra- e inter-osservatore. Poiché le reti neurali profonde ad alta capacità possono facilmente memorizzare tutti i disaccordi tra etichette crowdsourcing, l'applicazione diretta degli algoritmi di apprendimento supervisionati della rappresentazione linguistica esistenti può fornire soluzioni subottimali. In questo articolo, proponiamo TACMA, un euristico di apprendimento della rappresentazione linguistica consapevole del tempo per etichette crowdsourcing con più annotatori. L'approccio proposto (1) modella esplicitamente la variabilità intra-osservatore con un meccanismo di attenzione; (2) calcola e aggrega i punteggi di confidenza per campione di più lavoratori per affrontare i disaccordi tra osservatori. L'euristica proposta è estremamente facile da implementare in circa 5 righe di codice. L'euristica proposta è valutata su quattro set di dati sintetici e quattro nel mondo reale. I risultati mostrano che il nostro approccio supera un'ampia gamma di linee di base all'avanguardia in termini di precisione delle previsioni e AUC. Per incoraggiare i risultati riproducibili, rendiamo il nostro codice disponibile pubblicamente a < https://github.com/CrowdsourcingMining/TACMA >.", 'ms': 'Belajar mewakili bahasa yang efektif dari label sumber ramai adalah penting untuk banyak tugas pembelajaran mesin dunia nyata. Aspek yang mencabar bagi masalah ini adalah kualiti label sumber ramai menderita variabiliti yang tinggi dalam dan antar-pengamati. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions.  Dalam kertas ini, kami cadangkan TACMA, perwakilan bahasa yang sedar-sementara belajar heuristik untuk label sumber ramai dengan beberapa annotator. pendekatan yang direncanakan (1) secara eksplicit model variabiliti intra-observer dengan mekanisme perhatian; (2) menghitung dan agregat skor kepercayaan per sampel dari pekerja berbilang untuk mengatasi ketidaksetujuan antar-pengamati. heuristik yang diusulkan sangat mudah untuk dilaksanakan dalam sekitar 5 baris kod. The proposed heuristic is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC.  Untuk mendorong hasil yang boleh dikembalikan, kami membuat kod kami tersedia secara umum di < https://github.com/CrowdsourcingMining/TACMA >.', 'ml': 'സാധാരണ ഭാഷ പഠിപ്പിക്കുന്നത് വളരെ യഥാര്\u200dത്ഥ ലോക മെഷീന്\u200d പഠിക്കുന്ന ജോലികള്\u200dക്ക് വേണ്ടിയാണ്. ഈ പ്രശ്നത്തിന്റെ വിലപിടിക്കുന്ന ഭാഗമെന്തെന്നാല്\u200d പ്രധാനപ്പെട്ട മുഴുവന്\u200d മൂലസ്സോര്\u200dസ്സ് ലേബുകളുടെ ഗുണവ ഉയരത്തിലുള്ള ആഴത്തിലുള്ള ന്യൂറല്\u200d നെറുല്\u200d നെറ്ററുകളുടെ നേര്\u200dക്കുകള്\u200dക്കിടയിലുള്ള എല്ലാ വ്യത്യാസങ്ങളെയും എളുപ്പമായി ഓര്\u200dമ്മിക്കാന്\u200d കഴിയുന ഈ പത്രത്തില്\u200d നമ്മള്\u200d TACMA പ്രായശ്ചിത്തം ചെയ്യുന്നു, ഒരു സമയത്തേക്കാള്\u200d പരിഗണിക്കുന്ന ഭാഷ പ്രതിനിധിയാണ്, കൂട്ടം മൂലസ്സോര പ്രൊദ്ദേശിക്കപ്പെട്ട പ്രായോഗ്യം (1) ശ്രദ്ധിക്കുന്ന മെക്കയുമായി നിരീക്ഷിക്കുന്ന വ്യത്യാസവും പ (2) നിരീക്ഷിക്കുന്ന അഭിപ്രായത്തിന്റെ വിശ്വാസത്തിന്റെ സ്കോര്\u200dട്ടുകളില്\u200d നിന്നും ഒരുപാട് പണിക്കാരുടെ കണക പ്രൊദ്ദേശിക്കപ്പെട്ട ഹൂരിസ്റ്റിക്ക് 5 വരികളില്\u200d പ്രവര്\u200dത്തിപ്പിക്കാന്\u200d വളരെ എളുപ്പമാണ്. പ്രൊദ്ദേശിക്കപ്പെട്ട ഹെയൂരിസ്റ്റിക്ക് നാല് സിനിറ്റിക്കും നാല് ലോകത്തെ വിവരങ്ങളുടെ സേറ്റുകള The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC.  പ്രവർത്തിക്കാവുന്ന ഫലങ്ങളെ ആശ്വാസപ്പെടുത്താൻ, നമ്മുടെ കോഡ് പ്രസിദ്ധമാക്കുന്നു < https://github.com/CrowdsourcingMining/TACMA >.', 'mt': 'It-tagħlim ta’ rappreżentazzjonijiet effettivi tal-lingwi minn tikketti crowdsourced huwa kruċjali għal ħafna kompiti ta’ tagħlim tal-magni fid-dinja reali. Aspett ta’ sfida ta’ din il-problema huwa li l-kwalità tat-tikketti ta’ crowdsourced tbati minn varjabilità għolja bejn l-osservaturi u l-oħrajn. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions.  In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators.  L-approċċ propost (1) jimmudella espliċitament il-varjabilità bejn l-osservaturi b’mekkaniżmu ta’ attenzjoni; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements.  L-ewristika proposta hija faċli ħafna biex tiġi implimentata f’madwar 5 linji ta’ kodiċi. L-ewristika proposta hija evalwata fuq erba’ settijiet ta’ dejta sintetiċi u erba’ settijiet ta’ dejta tad-dinja reali. Ir-riżultati juru li l-approċċ tagħna jaqbeż firxa wiesgħa ta’ linji bażi l-aktar avvanzati f’termini ta’ preċiżjoni tat-tbassir u AUC. Biex niħeġġu r-riżultati riproduċibbli, nagħmlu l-kodiċi tagħna disponibbli għall-pubbliku fuq < https://github.com/CrowdsourcingMining/TACMA >.', 'lt': 'Daugeliui tikrojo pasaulio mašinų mokymosi užduočių labai svarbu mokytis veiksmingų kalbų atstovavimų iš daugelio šaltinių gautų etikečių. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability.  Kadangi didelio pajėgumo gilieji nervų tinklai gali lengvai prisiminti visus daugumos šaltinių etiketėse esančius nesutarimus, tiesiogiai taikant esamus prižiūrimus kalbų atstovavimo mokymosi algoritmus gali atsirasti nepakankamai optimalių sprendimų. Šiame dokumente siūlome TACMA, laikinai žinomą kalbų atstovavimą mokantis heuristiką daugelio šaltinių etiketėse su daugeliu annotatorių. Siūlomas metodas (1) aiškiai modeliuoja stebėtojų tarpusavio kintamumą su dėmesio mechanizmu; (2) apskaičiuoja ir suvestiniai daugelio darbuotojų kiekvieno mėginio pasitikėjimo rezultatai, kad būtų pašalinti stebėtojų tarpusavio nesutarimai. Siūloma heuristika yra labai lengva įgyvendinti maždaug 5 kodų eilutėse. Siūloma heuristika vertinama remiantis keturiais sintetiniais ir keturiais realaus pasaulio duomenų rinkiniais. Rezultatai rodo, kad mūsų požiūris viršija įvairias naujausias bazines linijas prognozės tikslumo ir AUC požiūriu. To encourage the reproducible results, we make our code publicly available at < https://github.com/CrowdsourcingMining/TACMA >.', 'mk': 'Учењето ефикасни претставувања на јазикот од попултови етикети е клучно за многу задачи за машинско учење во реалниот свет. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability.  Бидејќи длабоките нервни мрежи со висок капацитет лесно можат да ги запамтат сите несогласувања помеѓу народните етикети, директно апликација на постоечките надгледувани алгоритми за учење на јазикот може да донесе субоптимални решенија. Во овој весник, предложуваме TACMA, привремено свесна репрезентација на јазикот учејќи херистика за популарни етикети со повеќе анотатори. Предложениот пристап (1) експлицитно ја моделира внатрешната варијабилност на набљудувачите со механизам на внимание; (2) ги пресметува и агрегетира резултатите на довербата на секој примерок од многуте работници за решавање на несогласувањата меѓу набљудувачите. Предложениот хеористик е екстремно лесен да се спроведе во околу 5 линии код. Предложениот хеористик се проценува на четири синтетички и четири реални податоци. Резултатите покажуваат дека нашиот пристап го надминува широкиот опсег на најсовремени бази во поглед на претпоставувачката точност и АКС. За да ги охрабриме репродуктивните резултати, го правиме нашиот код јавно достапен на < https://github.com/CrowdsourcingMining/TACMA >.', 'pl': 'Nauka efektywnych reprezentacji języków z etykiet crowdsourcingowych ma kluczowe znaczenie dla wielu rzeczywistych zadań uczenia maszynowego. Wymagającym aspektem tego problemu jest to, że jakość etykiet crowdsourcingowych cierpi na dużą zmienność wewnątrz i między obserwatorami. Ponieważ głębokie sieci neuronowe o dużej pojemności mogą łatwo zapamiętać wszystkie niezgodności między etykietami crowdsourcingowymi, bezpośrednie zastosowanie istniejących nadzorowanych algorytmów uczenia się reprezentacji języka może dać nieoptymalne rozwiązania. W niniejszym artykule proponujemy TACMA, czyli heurystykę nauczania się reprezentacji języka czasowego dla etykiet społecznościowych z wieloma adnotatorami. Proponowane podejście (1) wyraźnie modeluje zmienność wewnątrz obserwatorów z mechanizmem uwagi; (2) oblicza i agreguje wyniki zaufania dla każdej próby od wielu pracowników w celu rozwiązania sporów między obserwatorami. Proponowana heurystyka jest niezwykle łatwa do wdrożenia w około pięciu liniach kodu. Proponowana heurystyka jest oceniana na czterech syntetycznych i czterech rzeczywistych zbiorach danych. Wyniki pokazują, że nasze podejście przewyższa szeroki zakres najnowocześniejszych linii bazowych pod względem dokładności przewidywania i AUC. Aby zachęcić do powtarzalnych wyników, udostępniamy nasz kod publicznie pod adresem < https://github.com/CrowdsourcingMining/TACMA >.', 'mn': 'Хүмүүсийн эх үүсвэрээс эффективны хэл илтгэлийг сурах нь машин суралцах үйл ажиллагаанд чухал. Энэ асуудлын шаардлагатай асуудал бол олон нийтийн нэвтрүүлсэн etiketүүдийн чанар интро болон интернет ажиглагчийн өөрчлөлт өндөр өвчинтэй. Учир нь өндөр чадварын гүн гүнзгий мэдрэлийн сүлжээнд хүн төрөлхтний тэмдэглэгдсэн бүх зөрчилдөөнийг амархан санах боломжтой болсон учраас суралцах алгоритмыг шууд удирдлагатай хэл илэрхийлэх алгоритмыг аш Энэ цаасан дээр бид TACMA-г хугацааны ухаантай хэл хэлний суралцах хэлбэрээр олон анзаарагчидтай олон нийтлэгдсэн тэмдэглэлтүүдийн хувьд хюрист суралцах боломжтой санал болгож байна Хэрэглэгдсэн арга баримт (1) анхаарлын механизмтай интернет ажиглагчийн өөрчлөлтийг тодорхой загварчлах болно. (2) Хэдий ажилтнуудын нэг номын итгэл үндэслэлийг тооцоолж, нийлүүлж байдаг. Хюристик санал өгсөн нь 5 шугам кодын орчинд маш амархан байдаг. Хуристикийн санал болгон дөрвөн синтетик болон дөрвөн бодит ертөнцийн мэдээллийн хэлбэрээр дүгнэж байна. Үүний үр дүнд бидний арга хэмжээний хувьд урлагийн үндсэн шулуунуудыг тодорхойлох зөв болон АС-ын төлөө олон төрлийн байдал хийдэг. Дараагдах үр дүнг дэмжихийн тулд бидний кодыг олон нийтэд ашиглах боломжтой болно. https://github.com/CrowdsourcingMining/TACMA >.', 'no': 'Læring av effektive språk-representasjonar frå crowdsourced labels er viktig for mange oppgåver med verkeleg maskinelæring. Ein vanskeleg aspekt på dette problemet er at kvaliteten av folkkjerne etikettar har høg variabel for intra- og interobservatorar. Sidan dei dype neuralnettverkene har høg kapasitet kan enkelt minne alle ulike forskjeller mellom crowdsourced labels, kan algoritme for å lære etter det eksisterande oversikte språk-representasjonen gjere underoptimal løysing. I denne papiret foreslår vi TACMA, eit mellombels språk- representasjon som lærer heuristisk for fleire merkelappar med fleire merkelappar. Den foreslåde tilnærminga (1) uttrykt modeller variabelen med merkingsmekanismenyen inne-observatoren. (2) reknar og aggregar tillit på prøveprøver frå fleire arbeidsgiver for å handtera ulike mottakar med observeringar. Den foreslåde heuristiske er ekstremt lett å implementera i rundt 5 linjer med kode. Den foreslåde heuristiske verdien er evaluert på fire syntetiske og fire verdsdata-sett. Resultatet viser at tilnærminga vårt utfører eit brett område av kunstbaselinjer i forhåndsvisningsnøyaktighet og AUC. For å oppfordre gjenopprettbare resultat, gjer vi koden vårt offentlig tilgjengeleg på < https://github.com/CrowdsourcingMining/TACMA >', 'ro': 'Învățarea reprezentărilor lingvistice eficiente de la etichetele crowdsourcing este esențială pentru multe sarcini de învățare automată din lumea reală. Un aspect provocator al acestei probleme este faptul că calitatea etichetelor crowdsourcing suferă de o variabilitate ridicată intra- și inter-observatori. Deoarece rețelele neuronale profunde de mare capacitate pot memora cu ușurință toate dezacordurile dintre etichetele crowdsourced, aplicarea directă a algoritmilor existenți de învățare a reprezentării limbii supravegheate poate oferi soluții suboptime. În această lucrare, propunem TACMA, o euristică de învățare a reprezentării limbii conștiente de timp pentru etichete crowdsourcing cu mai mulți adnotatori. Abordarea propusă (1) modelează în mod explicit variabilitatea intraobservatorului cu mecanism de atenție; (2) calculează și agrează scorurile de încredere pe eșantion de la mai mulți lucrători pentru a aborda dezacordurile dintre observatori. Euristica propusă este extrem de ușor de implementat în aproximativ 5 linii de cod. Euristica propusă este evaluată pe patru seturi de date sintetice și patru seturi de date din lumea reală. Rezultatele arată că abordarea noastră depășește o gamă largă de linii de bază de ultimă generație în ceea ce privește precizia predicției și ASC. Pentru a încuraja rezultatele reproductibile, facem codul nostru disponibil public la < https://github.com/CrowdsourcingMining/TACMA >.', 'so': 'Barashada luuqadda faa’iido leh oo ka mid ah calaamadaha dadku waa muhiim u ah shaqooyinka barashada machine badan ee caalamiga ah. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability.  Sida darteed shabakadda neurada ee aad u dheer ayaa si fudud u xusuusan kara dhammaan iskala xiriirka calaamadaha aad u leedahay, si toos ah u codsashada noocyada aqoonta afka lagu ilaaliyo oo la ilaaliyo, waxay heli karaan xalal hoos u ah. Qoraalkan waxaynu ka soo jeedaynaa TACMA, taas oo ku qoran luqada ku meelgaarka ah oo lagu aqoonsan karo heuristic looga baranayo calaamado badan oo ay leeyihiin dhibaatooyin badan. Dhaqdhaqaaqa la soo jeeday (1) si cad ayuu u muujiyaa bedelka qofka ilaaliya ee uu haysto mekaniisada daryeelka; (2) Shaqaalaha kalsoonaanshaha iyo qeybta sameynta waxaa laga xisaabinayaa shaqaalaha kala duduwan si ay u kala hadlaan khilaafka ilaaliyayaasha dhexdooda. Aqoonta la soo jeeday waa mid aad u fudud in lagu sameeyo qiyaastii 5 saf oo kooban. Waxaa lagu qiimeeyaa afarta kooxaha data ee caalamiga ah iyo afar kooban. Midhihiisu waxay muuqataa in dhaqdhaqaalahayagu uu soo saaraa wadamada-farshaxanta oo kala duduwan, taasoo ah saxda la sii daahiriyo iyo AUC. Si aan ugu dhiirrigelino arimaha qofka la soo celiyo, waxaynu si bayaan ah ugu soo bandhignaa sumadda https://github.com/CrowdsourcingMining/TACMA >.', 'sv': 'Att lära sig effektiva språkrepresentationer från crowdsourcerade etiketter är avgörande för många verkliga maskininlärningsuppgifter. En utmanande aspekt av detta problem är att kvaliteten på crowdsourcing-etiketter lider av stor variation inom och mellan observatörer. Eftersom de djupa neurala nätverken med hög kapacitet enkelt kan memorera alla meningsskiljaktigheter mellan crowdsourcerade etiketter, kan direkt tillämpning av befintliga övervakade språkrepresentationsinlärningsalgoritmer ge suboptimala lösningar. I denna uppsats föreslår vi TACMA, en tidsmedveten språkrepresentation lärande heuristisk för crowdsourcerade etiketter med flera kommentatorer. Den föreslagna metoden (1) modellerar uttryckligen variabiliteten inom observatörer med uppmärksamhetsmekanism. (2) Beräknar och aggregerar förtroendepoäng per urval från flera arbetstagare för att åtgärda oenigheterna mellan observatörer. Den föreslagna heuristiken är extremt enkel att implementera i cirka 5 rader kod. Den föreslagna heuristiken utvärderas på fyra syntetiska och fyra verkliga datauppsättningar. Resultaten visar att vårt tillvägagångssätt överträffar ett brett spektrum av state-of-the-art baselines vad gäller förutsägelsenoggrannhet och AUC. För att uppmuntra reproducerbara resultat gör vi vår kod offentligt tillgänglig på < https://github.com/CrowdsourcingMining/TACMA >.', 'ta': 'Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks.  இந்த பிரச்சனையின் சவாலிக்கையான பகுதி என்னவென்றால் மக்கள் மூலம் மூலம் குறிகளின் தரம் உயர்ந்த மாற்றம் ஏற்படுகிறது மற்ற அதிக தேவையான ஆழமான புதிர பிணைய வலைப்பின்னல்களில் உள்ள அனைத்து வேறுபாடுகளையும் சுலபமாக நினைவில் கொள்ள முடியும் என்பதால், நேரடியாக இருக்கு இந்த காகிதத்தில், நாம் TACMA, ஒரு தற்காலிக அறிவிக்கப்பட்ட மொழி பிரதிநிர்வாகிய மொழி குறிப்பிட்டு பல அறிவிப்பாளர்களுடன் மூலம் பட பரிந்துரைக்கப்பட்ட முறை( 1) கவனத்தின் முறைமையுடன் உள்நோக்கி மாறுபாட்டை வெளிப்படையாக மாற்றுகிறது. (2) ஒவ்வொரு உதாரணத்திற்கும் நம்பிக்கை மதிப்புகளை கணக்கிடும் மற்றும் குறிப்பிடும் மற்றும் பல பணியாளர் பரிந்துரைக்கப்பட்ட உயரம் மிகவும் எளிதாக 5 குறியீட்டு வரிகளில் செயல்படுத்த முடியும். நான்கு கூட்டிணைப்பு மற்றும் நான்கு உண்மையான உலக தரவு அமைப்புகளில் முயற்சிக்கப்படுகிறது. முடிவுகள் காண்பிக்கப்படுகிறது என்றால் நம் நெருக்கம் வெளியேறும் நிலையின்- கலை அடிப்படை கோடுகளை முன்னோட்டு சரியாக மற்ற பெருக்கக்கூடிய முடிவுகளை உறுதிப்படுத்துவதற்கு, நாம் < https://github.com/CrowdsourcingMining/TACMA >.', 'ur': 'جماعت کے سراسر لیبل سے فعال زبان کی تعلیم کی تعلیم بہت سی حقیقی دنیا کے ماشین کی تعلیم کے لئے ضرورت ہے. اس مسئلہ کا ایک مشکل ہے کہ جماعت کے ذریعہ لیبلوں کی کیفیت بالا دروازہ اور بین بیناوروں کی اختلافات ہے. کیونکہ عمیق نیورل نیورل نیٹورک آسانی سے تمام مخالفت کو جماعت کے ذریعے لیبل کے درمیان یاد کرسکتے ہیں، مستقیماً موجود موجود موجود زبان کی تعلیم الگوریٹم کی تعلیم کے مطابق موجود ہونے کے لئے استعمال کرنا ہے، ممکن ہے زیادی اس کاغذ میں ہم TACMA کو پیشنهاد کرتے ہیں، ایک موقت غیر معلوم زبان کی تعلیم کے لئے بہت سی انٹاتوروں کے ساتھ ملے ہوئے لیبل کے لئے ہوریست کی تعلیم لیتے ہیں. پیشنهاد کی تقریبا (1) صریح طریقے سے توجه کے مکانیسم کے ساتھ دکھائے جاتے ہیں۔ (2) ایک نمونہ کے مطابق مطمئن اعتمادی اسکور کو بہت سے کارکنوں سے کمپیوتر کرتا ہے اور جمع کرتا ہے۔ پیغمبر کی ہوریستیک کوڈ کے پیچھے پنج خطوط میں کامل کرنے کے لئے بہت آسان ہے۔ پیشنهاد کی ہوریستیک چار سینٹیٹیک اور چار حقیقی دنیا ڈیٹ سٹ پر ارزش کیا جاتا ہے. نتیجے دکھاتے ہیں کہ ہماری تقریبا ایک گھیری حالت کی بنسٹ لینوں کو پیش بینی دقیق اور AUC کے مطابق انجام دیتا ہے. ہم اپنے کد کو ظاہر طور پر موجود بناتے ہیں۔ https://github.com/CrowdsourcingMining/TACMA >', 'sr': 'Učenje efikasnih jezičkih predstavljanja iz masovnih etiketa je ključno za mnoge zadatke za učenje mašina u stvarnom svijetu. Zaključan aspekt ovog problema je da kvalitet masovnih etiketa pati visoka variabilnost unutar i međupromatrača. Pošto duboke neuralne mreže visoke kapacitete mogu lako pamtiti sve neslaganja među publikovanim etiketama, direktno primjenjujući postojeće algoritme za učenje nadgledanog jezika, mogu donijeti podoptimalne rešenja. U ovom papiru predlažemo TACMA, temporalno svestan jezik učenje heurističkog učenja za masovne etikete sa višestrukim annotatorima. Predloženi pristup (1) pojasno modeli variabilnost unutar promatrača sa mehanizam pažnje; (2) računaju i skuplja rezultate samopouzdanja po uzorku od višestrukih radnika kako bi se riješili neslaganja međupromatrača. Predloženo je heurističko lako provesti oko 5 linija koda. Predložena je heuristička procjena na četiri sintetičke i četiri kompleta podataka o stvarnom svijetu. Rezultati pokazuju da naš pristup iznosi širok opseg stanja umjetnosti u pogledu tačnosti predviđanja i AUC-a. Da bi ohrabrili reproduktivne rezultate, javno dostavljamo naš kod na < https://github.com/CrowdsourcingMining/TACMA >', 'si': 'ප්\u200dරභාවිත භාෂාව ප්\u200dරතිචාරයක් ඉගෙන ගන්න පුළුවන් ලේබල් වලින් ප්\u200dරතිචාරයක් ඇත්ත ලෝකයේ මැ මේ ප්\u200dරශ්නයේ ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙන්නේ මිනිස්සුන් ප්\u200dරශ්නයක් ලෙබෙල්ලගේ ප්\u200dරශ්ණතාවය තමයි අනුව ඉහළ ක්\u200dරියාත්මක ගොඩක් න්\u200dයූරල් ජාලයේ සියළු අවිශ්වාසය ලේබල් වලින් සියළු අවිශ්වාසය මතක් කරන්න පුළුවන් නිසා, ප්\u200dරතික්\u200d මේ පත්තරේ අපි TACMA ප්\u200dරතිචාරයක් කරනවා, කාලයෙන් අනතුරු භාෂාව ප්\u200dරතිචාරයක් ඉගෙනගන්න හෙයුරිස්ටික් ලේබල් ව ප්\u200dරශ්න විදිහට (1) පැහැදිලි විදිහට ප්\u200dරශ්න විදිහට ප්\u200dරශ්න විදිහට පරීක්ෂකයේ අවධාන පද්ධති (2) පරීක්ෂණය කරනවා සහ සැම්පල් විශ්වාස ස්කෝර්ඩ් වලින් විශ්වාස විශ්වාස ක්\u200dරියාකරුවන් වලින් සම්බන් ප්\u200dරශ්නයක් තියෙන්නේ හෙයුරිස්ටික් එක ගොඩක් ලේසියි කෝඩ් 5ක් වලින් ප්\u200dරශ්නය කරන්න. ප්\u200dරයෝජනය විශ්වාස කරලා තියෙන්නේ සින්ටෙටික් හතරයි ඇත්ත ලෝකයේ තොරතුරු හතරයි. ප්\u200dරතිචාරය පෙන්වන්නේ අපේ ප්\u200dරතිචාරය ප්\u200dරතිචාරයක් ප්\u200dරතිචාරයක් ප්\u200dරතිචාරයක් ප්\u200dරතිචාරයක් වෙනවා කියල ප්\u200dරවර්තනය කරන්න පුළුවන් ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරධානය කරන්න, අපි අපේ කෝඩ https://github.com/CrowdsourcingMining/TACMA >', 'ka': 'ეფექტიური ენის გამოსახულებების შესწავლობა ძალიან გასაკეთებელია მსოფლიო მსოფლიო მაქანის სწავლების მუშაობისთვის. ამ პრობლემას შესაძლებელი ადექტი არის, რომ მსოფლიოს კოლუფიკაციის კოლუფიკაცია უფრო დიდი ინტერია- და ინტერნომენტური მონაცემების განსხვავება. რადგან უფრო ძალიან კონტაქტიური ნეიროლური ქსელები უფრო მარტივიდ შეუძლიათ ყველა განსხვავებას მუშაობელია, რომელიც მუშაობელია მუშაობელია მუშაობელია მუშაობელია ამ დოკუნეში, ჩვენ დავიწყებთ TACMA-ს, ტემპალური საუცნო ენის რესპეზენტაციას, რომელიც სწავლის ჰუურისტიკური მარტიკებისთვის, რამდენიმე ანტოტორორ პროგრამა (1) განსაზღვრებული მოდელები გარეთილებას ინტერე მონაცემების შესაძლებლობა აღმოჩვენება მექანიზმისთვის; (2) კომპიუტერება და აგგრეგება მონაცემების შეცდომა მრავალ მუშაობელიდან მრავალ მუშაობელიდან, რომლებიც შეუძლებელიან შეცდომა. პროგრამეტული ჰეურისტიკი ძალიან ადვილი იყო, რომ ხუთი ხაზები კოდის გამოყენება. საზოგადომი ჰერისტიკი 4 სინტეტიკური და 4 რეალური მსოფლიოს მონაცემების კონფიგურაციაში იმუშაობა. წარმოდგენები გამოჩვენებენ, რომ ჩვენი წარმოდგენები უფრო დიდი წარმოდგენების განსაზღვრებით და AUC-ის განსაზღვრებით. ჩვენი კოდის გარეშექმნა შედეგების შესაძლებლობისთვის, ჩვენ გარეშექმნა კოდის გარეშექმნა https://github.com/CrowdsourcingMining/TACMA - ეა.', 'uz': "Muddati dunyoni o'rganish vazifalar uchun juda muhim. Bu muammolarning qiziqarli darajasi - jamoatlar tugmalarining sifatida juda katta va ko'rib turuvchi variability. Chunki eng yuqori neyron tarmoqlar tarkibi jamoatlar tarkibida hamma murakkablarni eslab qolishi oddiy darajada qo'llaniladi, mavjud qoʻllangan tillar algoritni o'rganish uchun bir tub optimal usullarni qoʻllash mumkin. Bu takarda biz TACMA, bir nechta annotator bilan bir necha bog'liq tugmalar bilan heuristik o'rganish uchun vaqt-yaxshi tilni tahrirlashni talab qilamiz. Koʻrib chiqarilgan usul (1) taʼminlovchi mekanisme bilan bir xil koʻruvchi variabiliyatini ko'rsatadi; @ info Aniqlanadigan heuristik 5 satrda bajarish juda sodda. Aniqlanadigan heuristik to'rt syntetik va to'rtta dunyo haqiqiqiy maʼlumot tarkibida qiymatdir. Natijalar shunday ko'rsatadi, bizning murakkablarimiz taqdimot va AUC tashkilotlari bilan ajratiladi. Kodlash muvaffaqiyatlarini amalga oshirish uchun, biz qoidamizni faqat tanlash mumkin. https://github.com/CrowdsourcingMining/TACMA >.", 'vi': 'Học những biểu hiện ngôn ngữ hiệu quả từ nhãn cầu là điều quan trọng cho nhiều công việc học máy trên thế giới thực. Một vấn đề khó khăn trong vấn đề này là chất lượng của nhãn cầu bị thay đổi thường xuyên và liên quan. Vì các mạng thần kinh cao cấp có thể dễ dàng ghi nhớ tất cả các bất đồng giữa các nhãn tụ họp, trực tiếp áp dụng các thuật toán học đại diện ngôn ngữ có mặt có thể cung cấp giải pháp tối ưu. Trong tờ giấy này, chúng tôi đề nghị TAMA, một đại diện ngôn ngữ hiểu biết về thần kinh cho nhãn tụ họp với nhiều nhà biên niên. Cách tiếp cận (1) đề xuất hiển thị mô tả sự biến thiên trong người quan sát với cơ chế tập trung; (2) Tính to án và kết hợp mỗi mẫu tin cậy điểm số của nhiều người để giải quyết bất đồng giữa người giám sát. Cái thần kinh đề xuất rất dễ thực hiện trong khoảng 5p đường mã. Dự án thần kinh được đánh giá dựa trên bốn bộ phân tích tổng hợp và bốn thế giới thực. Kết quả cho thấy phương pháp của chúng ta đạt được một loạt các bản căn cứ tối tân về độ chính xác dự đoán và AUC. Để khuyến khích kết quả có khả năng sinh sản, chúng tôi công khai đoạn mã của chúng tôi https://github.com/CrowdsourcingMining/TACMA -̀.', 'bg': 'Научаването на ефективни езикови представяния от етикети с crowdsourcing е от решаващо значение за много задачи от реалния свят за машинно обучение. Предизвикателен аспект на този проблем е, че качеството на марките с crowdsourcing страда от висока вътрешна и междунаблюдателска вариабилност. Тъй като дълбоките невронни мрежи с висок капацитет могат лесно да запомнят всички разногласия между етикетите, прякото прилагане на съществуващите алгоритми за изучаване на езика под надзор може да доведе до субоптимални решения. В настоящата статия предлагаме Езикова евристика за изучаване на езиково представяне за етикети с множество анотатори. Предложеният подход (1) изрично моделира вариабилността в рамките на наблюдателя с механизъм на вниманието; (2) изчислява и обобщава доверителните оценки на извадка от множество работници, за да се справи с несъгласията между наблюдателите. Предложената евристика е изключително лесна за изпълнение в около 5 реда код. Предложената евристика се оценява на четири синтетични и четири реални набора данни. Резултатите показват, че нашият подход надминава широк спектър от най-съвременни базови линии по отношение на точността на прогнозирането и AUC. За да насърчим възпроизводимите резултати, ние правим нашия код публично достъпен на < https://github.com/CrowdsourcingMining/TACMA >.', 'nl': 'Effectieve taalrepresentaties leren van crowdsourcing labels is cruciaal voor veel real-world machine learning taken. Een uitdagend aspect van dit probleem is dat de kwaliteit van crowdsourced labels onder hoge intra- en inter-observer variabiliteit lijdt. Aangezien de diepe neurale netwerken met hoge capaciteit gemakkelijk alle meningsverschillen tussen crowdsourced labels kunnen onthouden, kan het rechtstreeks toepassen van bestaande begeleide taalrepresentatie-leeralgoritmen suboptimale oplossingen opleveren. In dit artikel stellen we TACMA voor, een tijdbewuste heuristiek voor het leren van taalrepresentatie voor crowdsourced labels met meerdere annotatoren. De voorgestelde aanpak (1) modelleert expliciet de variabiliteit binnen waarnemers met aandachtsmechanisme; (2) berekent en aggregeert de vertrouwensscores per steekproef van meerdere werknemers om de meningsverschillen tussen waarnemers aan te pakken. De voorgestelde heuristiek is uiterst eenvoudig te implementeren in ongeveer vijf regels code. De voorgestelde heuristiek wordt geëvalueerd op vier synthetische en vier real-world datasets. De resultaten tonen aan dat onze aanpak een breed scala aan state-of-the-art baselines overtreft wat betreft voorspellingsnauwkeurigheid en AUC. Om de reproduceerbare resultaten te stimuleren, maken we onze code openbaar beschikbaar op < https://github.com/CrowdsourcingMining/TACMA >.', 'da': 'At lære effektive sprogrepræsentationer fra crowdsourcerede etiketter er afgørende for mange virkelige maskinlæringsopgaver. Et udfordrende aspekt af dette problem er, at kvaliteten af crowdsourcerede etiketter lider af høj variation inden for og mellem observatører. Da de dybe neurale netværk med høj kapacitet nemt kan huske alle uoverensstemmelser mellem crowdsourcerede etiketter, kan direkte anvende eksisterende overvågede sprogrepræsentationslæringsalgoritmer give suboptimale løsninger. I denne artikel foreslår vi TACMA, en tidsbevidst sprogrepræsentationslæringsheuristik for crowdsourcerede etiketter med flere kommentatorer. Den foreslåede fremgangsmåde (1) modellerer udtrykkeligt variabiliteten mellem observatører med opmærksomhedsmekanisme (2) beregner og aggregerer tillidsscorer pr. stikprøve fra flere arbejdstagere for at løse uoverensstemmelserne mellem observatører. Den foreslåede heuristik er ekstremt let at implementere i omkring 5 linjer kode. Den foreslåede heuristik evalueres på fire syntetiske og fire virkelige datasæt. Resultaterne viser, at vores tilgang overgår en bred vifte af state-of-the-art baselines med hensyn til forudsigelsenøjagtighed og AUC. For at fremme reproducerbare resultater gør vi vores kode offentligt tilgængelig på < https://github.com/CrowdsourcingMining/TACMA >.', 'hr': 'Učenje učinkovitih jezičkih predstavljanja iz crowdsourced etiketa ključno je za mnoge zadatke za učenje strojeva u stvarnom svijetu. Zaključavajući aspekt ovog problema je da kvalitet masovnih etiketa pati visoka variabilnost intra- i međupromatrača. Budući da duboke neuralne mreže visoke kapacitete mogu lako pamtiti sve neslaganja među crowdsourced etiketama, direktno primjenjujući postojeće algoritme za učenje nadzornog jezika, mogu donijeti podoptimalne rješenje. U ovom papiru predlažemo TACMA, temporalno svestan jezik učenje heurističkog učenja za crowdsourced etikete s višestrukim annotatorima. Predloženi pristup (1) pojasno modeliraju variabilnost unutar promatrača s mehanizam pažnje; (2) računa i skuplja rezultate povjerenja po uzorku od višestrukih radnika kako bi se riješila neslaganja međupromatrača. Predloženo je heurističko lako provesti oko 5 linija koda. Predložena je heuristička procjena na četiri sintetičke i četiri podatke o stvarnom svijetu. Rezultati pokazuju da naš pristup iznosi širok raspon početnih linija u smislu predviđanja preciznosti i AUC-a. Za poticanje reproduktivnih rezultata, javno dostavljamo naš kod na < https://github.com/CrowdsourcingMining/TACMA >', 'id': 'Mempelajari representation bahasa efektif dari label sumber masyarakat sangat penting untuk banyak tugas belajar mesin di dunia nyata. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability.  Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions.  Dalam kertas ini, kami mengusulkan TACMA, sebuah representation bahasa yang sadar sementara belajar heuristik untuk label crowdsourced dengan banyak annotator. pendekatan yang diusulkan (1) secara eksplicit model variabilitas intra-pengamati dengan mekanisme perhatian; (2) menghitung dan agregat skor kepercayaan per sampel dari beberapa pekerja untuk mengatasi perbedaan antara pengamati. heuristik yang diusulkan sangat mudah untuk dieksploitasi dalam sekitar 5 garis kode. heuristik yang diusulkan diuji pada empat set data sintetik dan empat set data dunia nyata. Hasilnya menunjukkan bahwa pendekatan kita melebihi jangkauan luas dari garis dasar state-of-the-art dalam terma akurasi prediksi dan AUC. Untuk mendorong hasil reproduksi, kami membuat kode kami tersedia publik di < https://github.com/CrowdsourcingMining/TACMA >.', 'ko': '패키지 라벨에서 효과적인 언어 표현을 배우는 것은 많은 현실 세계의 기계 학습 임무에 매우 중요하다.이 문제의 도전적인 측면은 패키지 라벨의 질이 관찰자의 내부와 내부 변화의 영향을 받는다는 것이다.고용량의 심층신경 네트워크는 패키지 라벨 간의 모든 불일치를 쉽게 기억할 수 있기 때문에 기존의 감독 언어를 직접 응용하면 학습 알고리즘이 차우해를 일으킬 수 있음을 나타낸다.본고에서 우리는 TACMA를 제기했는데 이것은 여러 개의 주석기를 가진 패키지 라벨을 대상으로 하는 시제 감지 언어 표시 학습 계발식 알고리즘이다.이 방법 (1) 주의 메커니즘을 이용하여 관찰자 내부의 가변성에 대해 현식 모델링을 실시했다.(2) 여러 직원의 각 견본의 신뢰 점수를 계산하고 종합하여 관찰자 간의 불일치를 해결한다.제시된 계발식 알고리즘은 약 5줄 코드에서 쉽게 실현된다.네 개의 합성 데이터 집합과 네 개의 실제 데이터 집합에서 제시된 계발식 알고리즘을 평가했다.그 결과 예측 정밀도 및 AUC 측면에서 Dell의 접근 방식이 가장 최첨단 베이스라인보다 우수하다는 것을 알 수 있습니다.복제 가능한 결과를 장려하기 위해<https://github.com/CrowdsourcingMining/TACMA>.', 'fa': 'یاد گرفتن نمایش\u200cهای زبان موثر از نقاشی\u200cهای جامعه\u200cی مردم برای بسیاری از کارهای آموزش ماشین\u200cهای جهانی واقعی مهم است. یک نقطه مشکلی از این مشکل این است که کیفیت نقطه\u200cهای جامعه\u200cی جمعیت به تغییرات درونی و بین\u200cبین\u200cنگهبان درمانده می\u200cشوند. از آنجا که شبکه های عصبی عمیق قابلیت بالا می توانند به آسانی از همه اختلافات بین برچسب های متوسط جمعیت یادآوری کنند، مستقیماً کاربرد الگوریتم های یادآوری زبان موجود موجود است، می تواند راه حل های زیر optimal به وجود بیاورد. در این کاغذ، ما TACMA را پیشنهاد می\u200cکنیم، یک نمایش زبان\u200cهای موقتی آگاهی یادگیری هوریستیک برای نقاشی\u200cهای جامعه\u200cای با چندین آگاهی\u200cکننده\u200cها. دستور پیشنهاد (۱) با مکانیسم توجه تغییرات داخلی بیننده را مشخص می\u200cکند. (۲) نقطه اعتماد در نمونه\u200cها از کارگران چندین را محاسبه می\u200cکند و جمع می\u200cکند تا اختلاف بین بین بیننده\u200cها را حل کند. پیشنهاد هوریستیک بسیار آسان است که در حدود ۵ خط کد عملکرد شود. این پیشنهاد هوریستیک بر چهار مجموعه داده\u200cهای سینتاتیک و چهار مجموعه جهانی واقعی ارزیابی می\u200cشود. نتیجه\u200cها نشان می\u200cدهند که دسترسی ما به عنوان دقیق پیش\u200cبینی و AUC یک مجموعه بسیاری از خط\u200cهای پایگاه\u200cهای هنر انجام می\u200cدهد. برای تشویق نتیجه\u200cهای تولید کننده، ما کد خود را به طور عمومی در < https://github.com/CrowdsourcingMining/TACMA .', 'de': 'Das Erlernen effektiver Sprachdarstellungen von Crowdsourcing-Labels ist für viele reale maschinelle Lernaufgaben entscheidend. Ein herausfordernder Aspekt dieses Problems ist, dass die Qualität von Crowdsourcing-Labels unter einer hohen Variabilität innerhalb und zwischen Beobachtern leidet. Da die hochleistungsfähigen tiefen neuronalen Netze alle Meinungsverschiedenheiten zwischen Crowdsourcing-Labels leicht merken können, kann die direkte Anwendung bestehender überwachter Sprachrepräsentationslernalgorithmen suboptimale Lösungen liefern. In diesem Beitrag schlagen wir TACMA vor, eine zeitlich bewusste Heuristik zur Sprachrepräsentation für Crowdsourcing-Labels mit mehreren Annotatoren. Der vorgeschlagene Ansatz (1) modelliert explizit die Variabilität innerhalb des Beobachters mit Aufmerksamkeitsmechanismus; (2) berechnet und aggregiert Vertrauenswerte pro Stichprobe von mehreren Arbeitern, um die Meinungsverschiedenheiten zwischen den Beobachtern zu beheben. Die vorgeschlagene Heuristik ist extrem einfach in etwa fünf Zeilen Code zu implementieren. Die vorgeschlagene Heuristik wird auf vier synthetischen und vier realen Datensätzen ausgewertet. Die Ergebnisse zeigen, dass unser Ansatz in Bezug auf Vorhersagegenauigkeit und AUC eine breite Palette modernster Baselines übertrifft. Um die reproduzierbaren Ergebnisse zu fördern, stellen wir unseren Code öffentlich zur Verfügung unter < https://github.com/CrowdsourcingMining/TACMA >.', 'tr': "Köp sanly etiketlerden etkinlik dil täsirlerini öwrenmek üçin köp dünýäde enjamlary öwrenmek üçin wajyp däldir. Bu sorunun zorlaştırıcı bir durum, kalabalık kaynaklı etiketlerin keyfiyeti yüksek intra ve izleyici farklılığına uğraşıyor. Ýüksek ukyplaryň derin näyral şebekeleri köpüň etiketleriň arasynda ähli närazlyklaryny aňsatlyk goýup biler, meýdança gözetleýän dil algoritmalaryny öwrenmek üçin ýüz optimale çözümlerini etmäge mümkin edip biler. Bu kagyzda, biz TACMA'y bir nusga görkezilýän zaman bilgili dili, köpürli etiketler bilen heuristik öwrenmegi teklip edip bilýäris. Taýýarlanan golaýy (1) gözlemçi daňry ählisiniň üns meýdançasyny bilen taýýarlyk görkezilýänligini tassyklanýar; (2) Gözleyici arasında farklılıkları çözemek için örneklerde bir çok işçiden güvenilir notlarını hesaplar ve toplar. Heuristik teklip eden 5 hat çyzgynda aňsatly. Öň teklip edilen heuristik dört sintetik we dört sany dünýäde maglumat düzümlerinde deňlenýär. Netijiler biziň ýagdaýymyzyň öngörüm dogrylygyna we AUC-nyň üýtgeşik derejesini çykarýandygyny görkez. Täzelikleýän netijeleri güýjä etmek üçin, kodymyzy publika görüşmek üçin <at> https://github.com/CrowdsourcingMining/TACMA >", 'sq': 'Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks.  Një aspekt sfidues i këtij problemi është se cilësia e etiketave të burimeve të turmës vuan variabilitet të lartë brenda dhe ndër-vëzhgues. Meqë rrjetet nervore të thella me kapacitet të lartë mund të kujtojnë lehtë të gjitha mosmarrëveshjet midis etiketave të burimeve publike, zbatimi drejtpërdrejt i përfaqësimit ekzistues të mbikqyrur të gjuhës algoritmet e mësimit mund të japin zgjidhje jo optimale. Në këtë letër, ne propozojmë TACMA, një përfaqësim të gjuhës të ndërgjegjshme për kohë që mëson heuristik për etiketa të burimeve të shumta me shumë anotatorë. Përqasja e propozuar (1) modelon shprehësisht variabilitetin brenda vëzhguesit me mekanizmin e vëmendjes; (2) llogarit dhe agregon rezultatet e besimit për çdo mostër nga punëtorë të shumtë për të trajtuar mosmarrëveshjet ndër-vëzhgues. Heuristika e propozuar është jashtëzakonisht e lehtë për të zbatuar në rreth 5 linja kodi. Heuristika e propozuar vlerësohet në katër grupe të dhënash sintetike dhe katër grupe të botës reale. Rezultatet tregojnë se qasja jonë mbizotëron një gamë të gjerë bazash në lidhje me saktësinë e parashikimit dhe AUC. Për të inkurajuar rezultatet e riprodhueshme, ne e bëjmë kodin tonë të disponueshëm publikisht në < https://github.com/CrowdsourcingMining/TACMA >.', 'am': 'የቋንቋ አካባቢ ቋንቋ ማምረጥ የሕዝብ ቋንቋዎች ማምረጥ ለብዙዎቹ እውነተኛ የዓለም መሳሪያዎች ስራዎችን ለመማር ያስፈልጋል፡፡ የዚህ ችግር ግንኙነት ግንኙነት ግን የብሔራዊ ቋንቋዎች ብልሃት ከፍ ያለበት ውጤት፣ እና የመካከለኞቹ አስተያየት መለወጫ እንዲቀበል ነው፡፡ ከፍተኛ ኃይል የጠለቀ የናውሬል መረብ በጉዳዩ ቋንቋዎች መካከል የውይይት ግንኙነትን ሁሉ ማሳሰብ በተቻለ ምክንያት፣ በተገኘው ቋንቋ ማስተማርት መግለጫ አቅራቢያ መፍትሄቶችን በመጠቀም ይችላል፡፡ In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators.  የተዘጋጀው ሥርዓት (1) በተለየ የግልፅ ተጠቃሚው ልውጤት በጥያቄ አካባቢ ጋር ያሳያል፡፡ (2) በየምሳሌው የመታመነ ጥያቄ እና መቃወሚያ ጥያቄዎችን ከብዙ ሠራተኞች ጋር በመቃወም አስተያየት ይቆጥራል፡፡ በተዘጋጀው የአውራሲ ግንኙነት በአምስት መስመር ውስጥ ለመፈጸም በጣም ቀላል ነው፡፡ የተዘጋጀው ሀሪስቲ በአራቱ ሰንስቲካዊ እና አራት እውነተኛ ዓለም ዳታ መስመር ላይ ነው፡፡ ፍሬዎቹ የግዛት ግንኙነታችን የሀገር-የ-art መሠረት እና የአውቀት ግንኙነት እና የሀገር ግንኙነታችንን ያሳያል፡፡ ተወቃሚ ፍሬዎችን ለማጽናናት፣ የኮድራችንን በ < https://github.com/CrowdsourcingMining/TACMA >', 'sw': 'Kujifunza maonesho ya lugha yenye ufanisi kutoka kwenye alama za mashine za watu ni muhimu kwa kazi za kujifunza kwa mashine nyingi duniani. Jambo la changamoto la tatizo hili ni kwamba kiwango cha mabango yanayotangazwa na watu wanakabiliwa na mabadiliko makubwa ya mtazamo wa kati. Kwa kuwa mitandao ya juu yenye uwezo mkubwa ya neurali inaweza kukumbusha tofauti zote kati ya alama zinazotumiwa na watu, moja kwa moja kutumia uwakilishi wa lugha ulioangaliwa na utaratibu wa kujifunza unaweza kusababisha suluhisho la kidini. Katika gazeti hili, tunapendekeza TACMA, uwakilishi wa lugha inayofahamika kwa muda wa kujifunza vizuri kwa mabango yanayotumiwa na matangazo mengi. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism;  (2) Takwimu na mabadiliko ya uaminifu kwa kila sampuli kutoka kwa wafanyakazi kadhaa ili kuzungumzia tofauti kati ya waangalizi. Utamaduni unapendekezwa ni rahisi sana kutekeleza katika mistari matano. Utamaduni unapendekezwa unapitiwa kwenye seti za data nne za pamoja na nne za dunia halisi. Matokeo yanaonyesha kwamba mbinu yetu inaonyesha maeneo mengi ya hali ya sanaa kwa sababu ya utabiri sahihi na AUC. Ili kuhamasisha matokeo yanayoweza kuzaliwa, tunatengeneza sheria yetu kwa uwazi kwenye < https://github.com/CrowdsourcingMining/TACMA >.', 'af': "Die leer van effektiewe taal voorstellings van gemeenskare etikette is crucial vir baie werklike wêreld masjien leer opdragte. 'n Vanskerpende aspekt van hierdie probleem is dat die kwaliteit van gemeenskap etikette lyk hoë intra- en inter- observer veranderlikheid. Omdat die hoë-kapasiteit diep neuralnetwerke kan maklik alle ongelukke onder verskeidende etikette onthou, direk toepassing van bestaande ondersoekte taal voorsiening-algoritme om te leer, mag suboptimale oplossing gee. In hierdie papier stel ons TACMA voorstel, 'n tydelike-bevestig taal voorstelling van die leer heuristiese vir skaamte etikette met veelvuldige annotators. Die voorgestelde toegang (1) uitduidelik model die intra-observer veranderlikheid met aandag mekanisme; (2) Bereken en saamgenereer per-voorbeeld vertroue telling van veelvuldige werkers om die ondersoekte ondersoekte ondersoekte ondersoekte ondersoekte ondersoekte ondersoekte te adres. Die voorgestelde heuristiese is ekstrem maklik om in rondom 5 lyn van kode te implementeer. Die voorgestelde heuristiese is evalueer op vier sintetiese en vier regte wêreld data stelle. Die resultate wys dat ons toegang uitvoer 'n wyse omvang van staat-van-kuns basisline in terms van voorskou presies en AUC. Om die reproduseerbare resultate te bevestig, maak ons kode openlik beskikbaar by < https://github.com/CrowdsourcingMining/TACMA >", 'bn': 'জনসোর্সেড লেবেল থেকে কার্যকর ভাষার প্রতিনিধিত্ব শিক্ষা শিখার জন্য গুরুত্বপূর্ণ। এই সমস্যার একটি চ্যালেঞ্জ প্রান্ত হচ্ছে যে জনসোর্স উৎসের লেবেলের মান উচ্চ বিভিন্ন এবং পর্যবেক্ষকের মধ্যে বিভিন্ন ভিন যেহেতু উচ্চ ক্ষমতার গভীর নিউরেল নেটওয়ার্ক সহজে জনসোর্সের লেবেলের মধ্যে সমস্ত পার্থক্য স্মরণ করতে পারে, সরাসরি বিদ্যমান পর্যবেক্ষণ করা ভাষার প্রতিনিধিত এই কাগজটিতে আমরা তাসিমা প্রস্তাব করি যে একটি সাময়িক ভাষার প্রতিনিধিত্ব নিয়ে বেশ কয়েকটি অ্যান্ট্যান্টেটারের সাথে বিভিন্ন ব প্রস্তাবিত পদক্ষেপ (১) স্পষ্ট মডেলে দেখা যাচ্ছে ইন্টা-পর্যবেক্ষকের বিভিন্ন ভিন্নভাবে মনোযোগ দিয়ে  (২) পর্যবেক্ষকদের মধ্যে বিভিন্ন মতামতের কথা বলার জন্য প্রতি নমুনা প্রতি বিশ্বাস স্কোর হিসেবে হিসেবে গ্রহণ করা এবং বিভিন প্রস্তাবিত হুরিস্টিক প্রায় ৫ লাইনে বাস্তবায়নের জন্য খুব সহজ। প্রস্তাবিত হীরিস্টিক চারটি সিনেটিক এবং চারটি বিশ্বের তথ্য সেটে মূল্যায়ন করা হয়েছে। ফলাফল দেখা যাচ্ছে যে আমাদের প্রতিযোগিতা ভবিষ্যতের সঠিকভাবে এবং এউসির মাধ্যমে বিস্তৃত রাষ্ট্র-অফ-শিল্পের বেসাইনের প্রবণতার ফলাফল উৎসাহ প্রদান করার জন্য, আমরা আমাদের কোড প্রকাশ্যে পাওয়া যাচ্ছি < https://github.com/CrowdsourcingMining/TACMA >', 'az': 'İnsanların təşkil etiketlərindən etkili dil göstəricilərini öyrənmək çoxlu real dünya makinelərin öyrənməsi üçün çox vacib olan şeydir. Bu problemin çətinlikli aspekti, qüvvətli etiketlərin keyfiyeti yüksək intra- və inter-observer dəyişiklikdir. Yüksek qabiliyyət derin nöral ağları küfr etiketlərinin arasındakı mübahisələri asanlıqla yada salıb, mövcud edilən dil göstərilməsi algoritmi öyrənməsi altoptimal çətinləri olar. Bu kağızda, çoxlu annotatorla birlikdə çoxlu qüvvətli etiketlər üçün heuristik öyrənmək üçün TACMA təklif edirik. Önülləşdirilmiş tərzim (1) gözləyirci içərisində dəyişiklik mehānismi ilə açıq-aydın modellər çəkir; (2) bir nümunə təvəkkül mükafatlarını çoxlu işçilərdən hesablar və toplar. Önülləşdirilmiş heuristik 5 sətirdə çox asandır. Məsləhət edilən heuristik dörd sintetik və dörd real dünya veri qurularında değerlənir. Sonuçları göstərir ki, bizim tərəfimiz tədbir və AUC ilə çox müxtəlif tədbir səhifələrinin dəyişdirir. Yenidən təkrarlanabilir sonuçlarını təşkil etmək üçün, kodumuzu açıq-aşkar mövcuddur < https://github.com/CrowdsourcingMining/TACMA >', 'bs': 'Učenje efikasnih jezičkih predstavljanja iz publikovanih etiketa je ključno za mnoge zadatke za učenje mašina u stvarnom svijetu. Zaključavajući aspekt ovog problema je da kvalitet masovnih etiketa pati visoka variabilnost unutar i međupromatrača. Pošto duboke neuralne mreže visoke kapacitete mogu lako pamtiti sve neslaganja među crowdsourced etiketama, direktno primjenjujući postojeće algoritme za učenje nadgledanih jezika, mogu donijeti podoptimalne rješenje. U ovom papiru predlažemo TACMA, temporalno svestan jezik učenje heurističkog učenja za masovne etikete s višestrukim annotatorima. Predloženi pristup (1) pojasno modeliraju variabilnost unutar promatrača sa mehanizam pažnje; (2) računa i skuplja rezultate povjerenja po uzorku od višestrukih radnika kako bi se riješila neslaganja međupromatrača. Predloženi heurističar je izuzetno lako provesti oko 5 linija koda. Predložena je heuristička procjena na četiri sintetičke i četiri kompleta podataka o stvarnom svijetu. Rezultati pokazuju da naš pristup nadmašuje širok niz početnih linija stanja umjetnosti u smislu preciznosti predviđanja i AUC. Da bi ohrabrili reproduktivne rezultate, javno dostavljamo naš kod na < https://github.com/CrowdsourcingMining/TACMA - Da.', 'cs': 'Učení se efektivních jazykových reprezentací z crowdsourcingových štítků je klíčové pro mnoho úkolů strojového učení v reálném světě. Náročným aspektem tohoto problému je, že kvalita crowdsourcingových etiket trpí vysokou variabilitou uvnitř a mezi pozorovateli. Vzhledem k tomu, že vysokokapacitní hluboké neuronové sítě mohou snadno zapamatovat všechny neshody mezi crowdsourcingovými štítky, může přímé aplikace existujících algoritmů pro výuku reprezentace jazyka přinést suboptimální řešení. V tomto článku navrhujeme TACMA, časově orientovanou heuristiku jazykové reprezentace pro crowdsourcingové štítky s více anotátory. Navrhovaný přístup (1) výslovně modeluje variabilitu uvnitř pozorovatele s mechanismem pozornosti; (2) vypočítá a agregová skóre důvěry jednotlivých vzorků od více pracovníků za účelem řešení neshod mezi pozorovateli. Navrhovaná heuristika se velmi snadno implementuje do pěti řádků kódu. Navržená heuristika je hodnocena na čtyřech syntetických a čtyřech reálných datových sadách. Výsledky ukazují, že náš přístup překonává širokou škálu nejmodernějších základních linek z hlediska přesnosti predikce a AUC. Abychom podpořili reprodukovatelné výsledky, zveřejňujeme náš kód na adrese < https://github.com/CrowdsourcingMining/TACMA >.', 'et': 'Tõhusate keeleesituste õppimine ühiskasutusega sildidest on paljude reaalsete masinõppeülesannete jaoks väga oluline. Selle probleemi keeruliseks aspektiks on see, et ühiskaubanduslike märgiste kvaliteet varieerub väga vaatlejate vahel ja vaatlejate vahel. Kuna suure võimsusega sügavad närvivõrgud suudavad hõlpsasti meelde jätta kõik erinevused ühiskasutusega siltide vahel, võib olemasolevate järelevalvega keeleõppe algoritmide otsene rakendamine anda ebaoptimaalseid lahendusi. Käesolevas töös pakume välja TACMA, ajalise teadliku keele esitamise heuristika mitme annotatoriga ühiskasutusega siltide jaoks. Kavandatud lähenemisviisis (1) modelleeritakse selgesõnaliselt vaatlejasisest varieeruvust tähelepanumehhanismiga; (2) arvutab ja koondab mitme töötaja valimi usaldusskoorid vaatlejatevaheliste lahkarvamuste lahendamiseks. Kavandatud heuristikat on väga lihtne rakendada umbes 5 rida koodis. Kavandatud heuristikat hinnatakse neljal sünteetilisel ja neljal reaalmaailma andmekogumil. Tulemused näitavad, et meie lähenemisviis on prognoositapsuse ja AUC poolest suurem kui paljud kaasaegsed lähtejooned. Korratavate tulemuste soodustamiseks teeme oma koodi avalikult kättesaadavaks aadressil < https://github.com/CrowdsourcingMining/TACMA >.', 'fi': 'Tehokkaiden kieliesitysten oppiminen crowdsourcing-etiketeistä on ratkaisevan tärkeää monissa reaalimaailman koneoppimistehtävissä. Haastava näkökohta tässä ongelmassa on se, että joukkohankinnoilla tuotettujen merkkien laatu vaihtelee suuresti tarkkailijoiden sisällä ja niiden välillä. Koska suurikapasiteettiset syvähermoverkot pystyvät helposti muistamaan kaikki joukkolähdekoodien väliset erimielisyydet, olemassa olevien valvottujen kielten oppimisalgoritmien suora soveltaminen saattaa tuottaa epäoptimaalisia ratkaisuja. Tässä työssä ehdotamme TACMA:a, aikatietoista kieliedustuksen oppimisheuristiikkaa joukkolähdekoodilla varustettuihin etiketteihin, joissa on useita annotattoreita. Ehdotettu lähestymistapa (1) mallintaa havainnoitsijan sisäistä vaihtelua tarkasti huomiomekanismin avulla; (2) laskee ja kokoaa otoskohtaiset luottamuspisteet useista työntekijöistä tarkkailijoiden välisten erimielisyyksien ratkaisemiseksi. Ehdotettu heuristiikka on erittäin helppo toteuttaa noin viidellä koodirivillä. Ehdotettua heuristiikkaa arvioidaan neljällä synteettisellä ja neljällä reaalimaailman datalla. Tulokset osoittavat, että lähestymistapamme on ennustetarkkuuden ja AUC:n suhteen parempi kuin monet huipputekniset lähtökohdat. Kehittääksemme toistettavissa olevia tuloksia julkaisemme koodimme julkisesti osoitteessa < https://github.com/CrowdsourcingMining/TACMA >.', 'ca': "Aprendre representacions de llenguatges efectives a partir d'etiquetes d'origen públic és crucial per a moltes tasques d'aprenentatge de màquines del món real. Un aspecte desafiant d'aquest problema és que la qualitat de les etiquetes de crowdsourced pateix gran variabilitat entre i entre els observadors. Com que les xarxes neurals profundes d'alta capacitat poden memoritzar fàcilment tots els desagreements entre etiquetes de crowdsourced, aplicant directament algoritmes d'aprenentatge supervisades de llengües poden produir solucions suboptimals. En aquest article, proposem TACMA, una representació de llenguatges conscients temporalment aprenent heurística per etiquetes d'origen multiple amb múltiples anotators. L'enfocament proposat (1) modela explícitament la variabilitat intraobservadora amb mecanisme d'atenció; (2) calcula i agrega puntuacions de confiança per mostra de múltiples treballadors per abordar els desagreements entre observadors. L'heurística proposada és extremadament fàcil d'implementar en uns cinc línies de codi. L'heurística proposada es valora en quatre conjunts de dades sintètiques i de dades del món real. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC.  Per animar els resultats reproducibles, fem públic el nostre codi a < https://github.com/CrowdsourcingMining/TACMA >.", 'hy': 'Հանդիսատես պիտակներից արդյունավետ լեզվի ներկայացումներ սովորելը շատ իրական աշխարհի մեքենային ուսումնասիրության խնդիրների համար կարևոր է: Այս խնդրի բարդ ասպեկտը նրանում է, որ բազմաթիվ ծառայություն ունեցող պիտակների որակը տառապում է մեծ ներսում և հետազոտողների միջև տարբերակությունը: Քանի որ բարձր հնարավորություններով խորը նյարդային ցանցերը հեշտությամբ կարող են հիշել բոլոր անհամաձայնությունները խմբի կողմից, անմիջապես կիրառելով գոյություն ունեցող վերահսկված լեզվի ներկայացման ալգորիթմները սովորելու համար կարող են ենթաօպտիմա Այս թղթի մեջ մենք առաջարկում ենք ԹԱԿՄԱ-ը, ժամանակակից գիտակցած լեզվի ներկայացումը, որը սովորում է ժողովրդի կողմից բազմաթիվ նոտորատորներով բազմաթիվ պիտակների համար: The proposed approach (1) explicitly models the intra-observer variability with attention mechanism;  (2) բազմաթիվ աշխատակիցներից բազմաթիվ հաշվարկներ և համախմբում են վստահության գնահատականները, որպեսզի լուծեն հետազոտողների միջև եղած անհամաձայնությունները: Առաջարկված հեռուստականությունը շատ հեշտ է կիրառել կոդի մոտ հինգ գծերում: Առաջարկված հորիստիկ գնահատվում է չորս սինթետիկ և չորս իրական աշխարհի տվյալների համակարգերի վրա: Արդյունքները ցույց են տալիս, որ մեր մոտեցումը գերազանցում է ամենաբարձր հիմքերի լայն տարբերակը կանխատեսման ճշգրիտության և ԱՍՔ տեսանկյունից: Որպեսզի խրախուսենք վերարտադրվող արդյունքները, մենք մեր կոդը հանրային հասանելի ենք դարձնում https://github.com/CrowdsourcingMining/TACMA ]:', 'jv': 'politenessoffpolite"), and when there is a change ("assertive Aspect ratio sing mengko nggunakake uwong iki dadi iki, akeh masalah kanggo masalah politenessoffpolite"), and when there is a change ("assertivepoliteness Nanging kuwi iki, kita suggere TatSMA, langkung sampeyan luwih akeh jutaan kanggo masa luwih dumaten kanggo tukang layar tentang karo akeh nyong. Laptop" and "Desktop (2) Samsul lan ngawe samsul Genjer-genjer esuk-esuk didasar tentang kanggo nguasai 5 linen kode. Heuristik sing dipunanggé, dadi sing katêpakan seneng pisan-pakan lan alam sing dadi dong anyar. Rejalong kita ngerti dadi, akeh podho aken langkung banjur akeh barang-podho sing nganggo barang langkung sampek karo iso dianteksi ning autoC. Genjer-genjer esuk bukane depasar ditambah, kita nguasar kode kenal < https://github.com/CrowdsourcingMining/TACMA >', 'sk': 'Učenje učinkovitih jezikovnih predstavitev iz množičnih etiket je ključnega pomena za številna dejanska opravila strojnega učenja. Izziv tega problema je, da je kakovost množičnih oznak zelo variabilna znotraj in med opazovalci. Ker lahko globoka nevronska omrežja z visoko zmogljivostjo enostavno zapomnijo vsa nesoglasja med množičnimi oznakami, lahko neposredna uporaba obstoječih algoritmov za nadzorovano učenje jezikov prinese suboptimalne rešitve. V tem prispevku predlagamo TACMA, časovno zavedano jezikovno reprezentacijo, ki se uči heuristiko za množične etikete z več opozorili. Predlagani pristop (1) izrecno modelira variabilnost znotraj opazovalca z mehanizmom pozornosti; (2) izračuna in združuje rezultate zaupanja na vzorec več delavcev za reševanje nesoglasij med opazovalci. Predlagano heuristiko je izjemno enostavno izvesti v približno 5 vrsticah kode. Predlagana heuristika je ovrednotena na štirih sintetičnih in štirih realnih podatkovnih nizih. Rezultati kažejo, da naš pristop presega široko paleto najsodobnejših izhodišč v smislu natančnosti napovedi in AUC. Da bi spodbudili ponovljive rezultate, dajemo našo kodo javno dostopno na < https://github.com/CrowdsourcingMining/TACMA >.', 'ha': "Akwai karatun harshen masu inganci daga alama na umarni, yana da muhimu wa masu amfani da aikin masu ƙaranci cikin duniya. Babu wani abu na tsõratar da wa wannan masĩfa yana cẽwa, tsarin labãrin da aka rufe na umarni, za'a haɗe variabili mai tsawo na guda da mai gani. Dõmin da tsarin da ya kai-iyawa masu ƙari na neural sun iya iya sauƙin tunãtar da kowane iskaffiyar cikin alama na danganta, mai amfani da shiryoyin ayuka da aka tsare wa lugha da aka lissafa, zai iya ƙara masu buƙata masu kanana. In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators.  Tsarin da aka faɗa (1) ya motsa mai varianci na mai tsaro da zane-wato; (2) Yana ƙidãya da aggregers kowace misali masu amince daga wasu aikin aiki mãsu yawa dõmin su address sãɓãnin-gani. Akwai mai sauƙi da aka gozartar da heuristic cikin kode 5. Ana ƙaddara heuristic da aka faɗa a kan tsarin data masu cikin ƙasa huɗu da huɗu. Mataimakin ya nuna cewa hanyarmu yana samar da gwargwadon-state-of-the-art-line cikin muhimmin littafa da AUC. Ga mu yi fara ga matsalar da za'a zartar da shi, sai mu sami kodinmu da bayani da < https://github.com/CrowdsourcingMining/TACMA >", 'he': 'ללמוד מייצגות שפות יעילות מתוויות מקורות קהל זה קריטי עבור משימות מלמדת מכונות בעולם האמיתי רבות. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability.  מאחר שהרשתות העצביות העמוקות בעלת יכולת גבוהה יכולות בקלות לזכור את כל ההסכמות בין תוויות מקורשת הקהל, שימוש ישירות באמצעות מייצג שפת מושגת קיימת אלגוריתמים ללמוד יכולים להביא פתרונות לא אופטימיים. בעיתון הזה, אנו מציעים TACMA, מייצג שפה מודע לזמן, ללמוד היוריסטיקה לתוויות מקורות קהל עם מספר ציונים. הגישה המוצעת (1) דוגמה באופן ברור את ההתנהגות בתוך המצפים עם מנגנון תשומת לב; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements.  ההוריסטיקה המוצעת קלה מאוד להפעיל בסביבות 5 קודים. ההוריסטיקה המוצעת מוערכת על ארבעה קבוצות נתונים סינטטיים וארבעה קבוצות נתונים בעולם האמיתי. התוצאות מראות שהגישה שלנו עולה מעל טווח רחב של קווי בסיס מצפונים במונחים של מדויקת חיזוי ו AUC. כדי לעודד את התוצאות החדשות, אנו פועלים את הקוד שלנו פנוי לציבור ב < https://github.com/CrowdsourcingMining/TACMA >.', 'bo': 'སྐད་ཡིག་ལགས་ཅན་གྱི་ཤོག་བྱང་ཐོག་ནས་དབུས་ཡོད་པའི་སྐད་ཡིག་ཆ་རྟོགས་ནི་དང་མཉམ་དུ་འཇུག་རྟེན་གྱི་མ་ལག A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability. གནད་དོན་དག་ཚད་མཐོ་བོ་ཞིག་ཡོད་པ་ལས། མི་མང་གསོག་པའི་ཤོག་བྱང་ཐོག་ཏུ་གནད་དོན་མེད་པའི་རྒྱུ་མཚན་ནི་ In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. སྔོན་འཛིན་པའི་གཟུགས་སྐོར་དང་། གསལ་བཤད་ཀྱི་མིག་ཆས་གཞན་པའི་ནང་ལྟ་ཀློག་ཆས་འདྲ་བ་སྐྱེན་པའི་ཐབས་ལམ་དང་། (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. འདིས་འཆར་བཀོད་པའི་རྩིས་འབྲེལ་གྱི་ཐབས་ལམ་དེ་གསལ་པོ་ཞིག་ལས་སྒྲིག་འགོད་ཐབས་མེད་པར་ལས་སླ་པོ་རེད། གྲངས་སུ་འཆར་ཟིན་པའི་རྩ་འབྲེལ་གྱི་ནང་དོན་དག་གིས་དབྱེ་སེལ་བཞིནམ་དང་། ངོ་མ་འཛམ་གླིང་གི་གནས་སྟངས་ཀ གྲུབ་འབྲས་བ་དེ་ནི་ང་ཚོའི་གཟུགས་སྐོར་འདི་དག་གི་གནས་སྟངས་གཟུགས་རིས་མང་ཙམ་བྱེད་ཀྱི་ཡོད་པ་ཤར་ཡོད། ལོ་བསྐྱེད་པའི་འབྲས་བ་རྐྱེན་འབྲས་མེད་པའི་གནད་སྡུད་དག་གིས། ང་ཚོའི་ཨང་རྩིས་མང་ཆོས་སྤྱོད་ཐ https://github.com/CrowdsourcingMining/TACMA >'}
{'en': 'Knodle : Modular Weakly Supervised Learning with PyTorch P y T orch', 'ar': 'العقدة: وحدات التعلم الخاضع للإشراف الضعيف باستخدام PyTorch', 'pt': 'Knodle: Aprendizado modular fracamente supervisionado com PyTorch', 'fr': 'Kodle\xa0: Apprentissage modulaire faiblement supervisé avec PyTorch', 'es': 'Knodle: Aprendizaje modular débilmente supervisado con PyTorch', 'ja': 'Knodle ： PyTorchを使用したモジュール式の弱く監督された学習', 'zh': 'Knodle:用PyTorch模块化弱监学', 'ru': 'Knodle: Модульное обучение под слабым контролем с PyTorch', 'hi': 'Knodle: मॉड्यूलर कमजोर PyTorch के साथ पर्यवेक्षित सीखने की निगरानी', 'ga': 'Knodle: Foghlaim Mhodúil faoi Mhaoirseacht Lag le PyTorch', 'ka': 'Name', 'el': 'Κόμβος: Μορφοποιημένη αδύναμη Εποπτευμένη Μάθηση με τον Πίαρτ', 'hu': 'Knodle: Moduláris, gyengén felügyelt tanulás PyTorch segítségével', 'it': 'Knodle: apprendimento modulare debolmente supervisionato con PyTorch', 'lt': 'Knodle: Modular Weakly Supervised Learning with PyTorch', 'kk': 'Батырма: PyTorch- мен модульдерді бақылау бақылау', 'mk': 'Кукло: Модуларно слабо надгледувано учење со PyTorch', 'ms': 'Knodle: Pembelajaran Modular yang Melihat lemah dengan PyTorch', 'ml': 'കെനോഡില്\u200d: പൈട്ടോര്\u200dച്ചുകൊണ്ട് മൊഡ്യൂള്\u200d വാഴ്ചയില്\u200d പ്രദര്\u200dശിപ്പിക്കപ്പെട്ട പഠനം', 'mt': 'Knodle: Tagħlim Modulari b’Superviżjoni dgħajfa b’PyTorch', 'no': 'Knodle: Modular vekk oversikt læring med PyTorch', 'pl': 'Węzłek: Modułowe słabo nadzorowane uczenie się z PyTorch', 'mn': 'Нодлол: PyTorch-тэй модуль багахан удирдагдсан суралцах', 'ro': 'Knodle: Învățare modulară slab supravegheată cu PyTorch', 'sr': 'Knodle: Modular Weakly Supervised Learning with PyTorch', 'si': 'Name', 'so': 'Knodle: Modular Weekly Supervised Learning with PyTorch', 'ur': 'Name', 'ta': 'கேநோடில்: பைட்ராக் கொண்டு வெளிப்படையான படிப்பு கூறு', 'sv': 'Knodle: Modulärt svagt övervakat lärande med PyTorch', 'uz': 'Name', 'vi': 'Cần phải theo dõi cẩn thận, cẩn thận.', 'bg': 'Възел: Модулно слабо контролирано обучение с PyTorch', 'hr': 'Knodle: Modular Weakly Supervised Learning with PyTorch', 'da': 'Knodle: Modulær svagt overvåget læring med PyTorch', 'nl': 'Knopel: Modulair zwak begeleid leren met PyTorch', 'id': 'Knodle: Modular Weakly Supervised Learning dengan PyTorch', 'fa': 'Knodle: یادگیری با PyTorch مدول ضعیف تحت نظر', 'ko': 'Knodle: Pytork를 사용한 모듈화된 약한 감독 학습', 'de': 'Knodle: Modulares schwach überwachtes Lernen mit PyTorch', 'sw': 'Knodle: Mafunzo yanayoshindwa na PyTorch', 'sq': 'Knodle: Mësimi modular i dobët i mbikqyrur me PyTorch', 'am': 'እና', 'af': 'Knodle: Moduleer Weekly Supervised Learning with PyTorch', 'tr': 'Töwür: PyTorch bilen Modüler Aşak Gözetli Öwrenme', 'hy': 'Knodle: Մոդուլյար, թույլ վերահսկված ուսումնասիրությունը', 'az': 'Knodle: PyTorch ilə Modular Zayıf Gözlənmiş Öyrənmə', 'bs': 'Knodle: Modular Weakly Supervised Learning with PyTorch', 'bn': 'কে- নোডেল: পাইটর্চের সাথে মোডুলার সপ্তাহে সুপারভিস্ট শিক্ষা শিক্ষা', 'cs': 'Uzel: Modulární slabě dohlížené učení s PyTorchem', 'et': 'Knodle: modulaarne nõrgalt järelevalvega õppimine PyTorchi abil', 'ca': 'Knodle: Aprendiment mòdul debilment supervisat amb PyTorch', 'fi': 'Knodle: Modulaarinen heikosti valvottu oppiminen PyTorchin avulla', 'jv': 'knogle', 'sk': 'Knodle: Modularno šibko nadzorovano učenje s PyTorch', 'ha': 'Knodle: Modular Weakly Supervised Learning with PyTorch', 'he': 'Knodle: Modular Weakly Supervised Learning with PyTorch', 'bo': 'Knodle: PyTorch དང་བསྟུན་ནས་Modular Weakly Supervised Learning with PyTorch'}
{'en': 'Strategies for improving the  training and prediction quality  of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a  software framework  that treats weak data annotations,  deep learning models , and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of  heuristic rules , or elements of the  deep learning model  ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the  machine learning model  trained with the resulting labels), to those that harness the interplay of  neural networks  and weakly labeled data. We illustrate the benchmarking potential of the  framework  with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.', 'pt': 'As estratégias para melhorar a qualidade do treinamento e da previsão de modelos de aprendizado de máquina fracamente supervisionados variam no quanto elas são adaptadas a uma tarefa específica ou integradas a uma arquitetura de modelo específica. Neste trabalho, apresentamos o Knodle, uma estrutura de software que trata anotações de dados fracas, modelos de aprendizado profundo e métodos para melhorar o treinamento supervisionado fracamente como componentes modulares separados. Essa modularização dá ao processo de treinamento acesso a informações refinadas, como características do conjunto de dados, correspondências de regras heurísticas ou elementos do modelo de aprendizado profundo usado para previsão. Portanto, nossa estrutura pode abranger uma ampla gama de métodos de treinamento para melhorar a supervisão fraca, desde métodos que analisam apenas correlações de regras e classes de saída (independentemente do modelo de aprendizado de máquina treinado com os rótulos resultantes) até aqueles que aproveitam a interação de redes neurais e dados fracamente rotulados. Ilustramos o potencial de benchmarking do framework com uma comparação de desempenho de várias implementações de referência em uma seleção de conjuntos de dados que já estão disponíveis no Knodle.', 'ar': 'تختلف استراتيجيات تحسين جودة التدريب والتنبؤ لنماذج التعلم الآلي الخاضعة للإشراف الضعيف من حيث مدى تصميمها لمهمة محددة أو دمجها مع بنية نموذجية محددة. في هذا العمل ، نقدم Knodle ، وهو إطار عمل برمجي يعالج التعليقات التوضيحية للبيانات الضعيفة ونماذج التعلم العميق وطرق تحسين التدريب الخاضع للإشراف الضعيف كمكونات نمطية منفصلة. تتيح هذه الوحدة النمطية لعملية التدريب الوصول إلى المعلومات الدقيقة مثل خصائص مجموعة البيانات أو مطابقة القواعد التجريبية أو عناصر نموذج التعلم العميق المستخدم في النهاية للتنبؤ. ومن ثم ، يمكن أن يشمل إطار عملنا مجموعة واسعة من طرق التدريب لتحسين الإشراف الضعيف ، بدءًا من الأساليب التي تنظر فقط إلى ارتباطات القواعد وفئات الإخراج (بشكل مستقل عن نموذج التعلم الآلي المدرب مع التسميات الناتجة) ، إلى تلك التي تسخر التفاعل الشبكات العصبية والبيانات ذات العلامات الضعيفة. نوضح إمكانات قياس الأداء لإطار العمل من خلال مقارنة أداء العديد من التطبيقات المرجعية على مجموعة مختارة من مجموعات البيانات المتوفرة بالفعل في Knodle.', 'es': 'Las estrategias para mejorar la calidad del entrenamiento y la predicción de los modelos de aprendizaje automático débilmente supervisados varían en cuanto a la medida en que se adaptan a una tarea específica o se integran con una arquitectura de modelo específica. En este trabajo, presentamos Knodle, un marco de software que trata las anotaciones de datos débiles, los modelos de aprendizaje profundo y los métodos para mejorar la capacitación supervisada débilmente como componentes modulares separados. Esta modularización proporciona al proceso de entrenamiento acceso a información detallada, como las características del conjunto de datos, las coincidencias de reglas heurísticas o los elementos del modelo de aprendizaje profundo que se utilizan en última instancia para la predicción. Por lo tanto, nuestro marco puede abarcar una amplia gama de métodos de entrenamiento para mejorar la supervisión débil, que van desde métodos que solo miran las correlaciones de las reglas y las clases de salida (independientemente del modelo de aprendizaje automático entrenado con las etiquetas resultantes), hasta aquellos que aprovechan la interacción de redes y datos débilmente etiquetados. Ilustramos el potencial de evaluación comparativa del marco con una comparación del rendimiento de varias implementaciones de referencia en una selección de conjuntos de datos que ya están disponibles en Knodle.', 'fr': "Les stratégies visant à améliorer la qualité de la formation et de la prédiction des modèles d'apprentissage automatique faiblement supervisés varient selon qu'ils sont adaptés à une tâche spécifique ou intégrés à une architecture de modèle spécifique. Dans ce travail, nous présentons Knodle, un framework logiciel qui traite les annotations de données faibles, les modèles de deep learning et les méthodes permettant d'améliorer la formation faiblement supervisée en tant que composants distincts et modulaires. Cette modularisation permet au processus de formation d'accéder à des informations précises telles que les caractéristiques des ensembles de données, les correspondances de règles heuristiques ou des éléments du modèle d'apprentissage profond finalement utilisés pour la prédiction. Par conséquent, notre framework peut englober un large éventail de méthodes de formation pour améliorer la supervision faible, allant des méthodes qui examinent uniquement les corrélations des règles et des classes de sortie (indépendamment du modèle d'apprentissage automatique entraîné avec les étiquettes résultantes), à celles qui exploitent l'interaction des neurones réseaux et données faiblement étiquetées. Nous illustrons le potentiel de benchmarking du framework avec une comparaison des performances de plusieurs implémentations de référence sur une sélection d'ensembles de données déjà disponibles dans Kodle.", 'zh': '增弱督机器习范、测质之策,特定制与特定体系结构异。 此一软件框架也,以弱为注,以弱为单模块化组件。 此模块化使训练访问细粒度息,如数集特征,启发式则之匹配,终于测验之元素。 故吾框架可以包百监之训练方法,从仅稽则输类之相关性(独立于用机器学形),及用神经网络弱之数相用也。 以 Knodle 中已有之数集上多参以成其能,以明其框架之粗试也。', 'ja': '弱く監督された機械学習モデルのトレーニングと予測の質を改善するための戦略は、特定のタスクに合わせて調整されているか、特定のモデルアーキテクチャと統合されているかによって異なります。 この研究では、弱いデータ注釈、ディープラーニングモデル、弱い監督下のトレーニングを改善するための方法を別個のモジュール式コンポーネントとして扱うソフトウェアフレームワークであるKnodleを紹介します。 このモジュール化は、トレーニングプロセスに、データセットの特性、ヒューリスティックルールの一致、または最終的に予測に使用されるディープラーニングモデルの要素などの細かい情報へのアクセスを提供します。 したがって、我々のフレームワークは、（結果として生じるラベルで訓練された機械学習モデルとは無関係に）ルールと出力クラスの相関関係のみを見る方法から、ニューラルネットワークと弱くラベル付けされたデータの相互作用を利用する方法まで、弱い監督を改善するための幅広い訓練方法を包含することができる。 私たちは、Knodleで既に利用可能ないくつかのデータセットの選択に関するいくつかの参照実装のパフォーマンス比較を用いて、フレームワークのベンチマークの可能性を示します。', 'ru': 'Стратегии улучшения качества обучения и прогнозирования слабо контролируемых моделей машинного обучения варьируются в зависимости от того, насколько они адаптированы к конкретной задаче или интегрированы с конкретной модельной архитектурой. В этой работе мы представляем Knodle, программный фреймворк, который рассматривает слабые аннотации данных, модели глубокого обучения и методы улучшения слабо контролируемого обучения как отдельные модульные компоненты. Эта модуляризация дает учебному процессу доступ к мелкозернистой информации, такой как характеристики набора данных, совпадения эвристических правил или элементы модели глубокого обучения, в конечном счете используемые для прогнозирования. Следовательно, наша структура может охватывать широкий спектр методов обучения для улучшения слабого надзора, начиная от методов, которые рассматривают только корреляции правил и выходных классов (независимо от модели машинного обучения, обученной с результирующими метками), до методов, которые используют взаимодействие нейронных сетей и слабо меченых данных. Мы иллюстрируем сравнительный потенциал структуры сравнением производительности нескольких эталонных реализаций на выборке наборов данных, которые уже доступны в Knodle.', 'hi': 'कमजोर पर्यवेक्षित मशीन लर्निंग मॉडल के प्रशिक्षण और भविष्यवाणी की गुणवत्ता में सुधार के लिए रणनीतियां इस बात में भिन्न होती हैं कि वे एक विशिष्ट कार्य के अनुरूप हैं या एक विशिष्ट मॉडल आर्किटेक्चर के साथ एकीकृत हैं। इस काम में, हम Knodle, एक सॉफ्टवेयर रूपरेखा है कि कमजोर डेटा एनोटेशन, गहरी सीखने के मॉडल, और अलग, मॉड्यूलर घटकों के रूप में कमजोर पर्यवेक्षित प्रशिक्षण में सुधार के लिए तरीकों का इलाज करता है परिचय. यह मॉड्यूलराइजेशन प्रशिक्षण प्रक्रिया को डेटा सेट विशेषताओं, हेरिस्टिक नियमों के मैचों, या अंततः भविष्यवाणी के लिए उपयोग किए जाने वाले गहरे सीखने के मॉडल के तत्वों जैसी बारीक जानकारी तक पहुंच प्रदान करता है। इसलिए, हमारा ढांचा कमजोर पर्यवेक्षण में सुधार के लिए प्रशिक्षण विधियों की एक विस्तृत श्रृंखला को शामिल कर सकता है, जो उन तरीकों से लेकर है जो केवल नियमों और आउटपुट कक्षाओं के सहसंबंधों को देखते हैं (परिणामस्वरूप लेबल के साथ प्रशिक्षित मशीन लर्निंग मॉडल से स्वतंत्र रूप से), उन लोगों के लिए जो तंत्रिका नेटवर्क और कमजोर रूप से लेबल किए गए डेटा के इंटरप्ले का उपयोग करते हैं। हम पहले से ही Knodle में उपलब्ध डेटासेट के चयन पर कई संदर्भ implementations के प्रदर्शन की तुलना के साथ ढांचे की बेंचमार्किंग क्षमता को स्पष्ट करते हैं।', 'ga': 'Athraíonn na straitéisí chun cáilíocht oiliúna agus tuar na múnlaí meaisínfhoghlama faoi mhaoirseacht lag a fheabhsú maidir leis an méid a shaincheaptar iad do thasc ar leith nó a chomhtháthaítear le hailtireacht mhúnla ar leith. San obair seo, tugaimid isteach Knodle, creat bogearraí a dhéileálann le nótaí laga sonraí, samhlacha foghlama domhain, agus modhanna chun oiliúint lag-mhaoirsithe a fheabhsú mar chomhpháirteanna modúlacha ar leithligh. Tugann an modúlú seo rochtain don phróiseas oiliúna ar fhaisnéis mhionsonraithe amhail tréithe tacair sonraí, meaitseáil le rialacha heorastúla, nó gnéithe den tsamhail dhomhainfhoghlama a úsáidtear ar deireadh le haghaidh tuartha. Mar sin, féadann ár gcreat raon leathan modhanna oiliúna a chuimsiú chun maoirseacht lag a fheabhsú, ó mhodhanna nach bhféachtar ach ar chomhghaolta rialacha agus aicmí aschuir (go neamhspleách ar an tsamhail meaisínfhoghlama atá oilte leis na lipéid a thagann as), go dtí na cinn a bhaineann leas as an idirghníomhú. líonraí néarúla agus sonraí lag-lipéadaithe. Léirímid poitéinseal tagarmharcála an chreata trí chomparáid feidhmíochta a dhéanamh ar roinnt feidhmiúcháin tagartha ar rogha tacar sonraí atá ar fáil cheana féin in Knodle.', 'el': 'Οι στρατηγικές για τη βελτίωση της ποιότητας εκπαίδευσης και πρόβλεψης των αδύναμα εποπτευόμενων μοντέλων μηχανικής μάθησης διαφέρουν ως προς το πόσο είναι προσαρμοσμένα σε μια συγκεκριμένη εργασία ή ενσωματωμένα με μια συγκεκριμένη αρχιτεκτονική μοντέλου. Σε αυτή την εργασία, εισάγουμε ένα πλαίσιο λογισμικού που αντιμετωπίζει αδύναμες σχολιασμούς δεδομένων, μοντέλα βαθιάς μάθησης και μεθόδους για τη βελτίωση της ασθενούς εποπτευόμενης εκπαίδευσης ως ξεχωριστά, μορφωματικά συστατικά. Αυτή η διαφοροποίηση δίνει στη διαδικασία κατάρτισης πρόσβαση σε λεπτόκοκκες πληροφορίες, όπως χαρακτηριστικά συνόλου δεδομένων, αντιστοιχίες των heuristic κανόνων ή στοιχεία του μοντέλου βαθιάς μάθησης που χρησιμοποιούνται τελικά για την πρόβλεψη. Ως εκ τούτου, το πλαίσιο μας μπορεί να περιλαμβάνει ένα ευρύ φάσμα μεθόδων κατάρτισης για τη βελτίωση της αδύναμης εποπτείας, που κυμαίνονται από μεθόδους που εξετάζουν μόνο συσχετισμούς κανόνων και κλάδων παραγωγής (ανεξάρτητα από το μοντέλο μηχανικής μάθησης που εκπαιδεύεται με τις προκύπτουσες ετικέτες), μέχρι εκείνες που αξιοποιούν την αλληλεπίδραση των νευρωνικών δικτύων και των ασθενώς επισημασμένων δεδομένων. Παρουσιάζουμε το δυναμικό συγκριτικής αξιολόγησης του πλαισίου με μια σύγκριση επιδόσεων αρκετών εφαρμογών αναφοράς σε μια επιλογή συνόλων δεδομένων που είναι ήδη διαθέσιμα στο Knodle.', 'ka': 'სტრატიგიები, რომლებიც სპექტიკური რაოდენობას განსაზღვრებულია ან განსაზღვრებულია სპექტიკური მოდელების განსაზღვრება და განსაზღვრებულია. ამ სამუშაოში ჩვენ მოვიყენებთ ნოდელს, პროგრამეტური ფრამეტრი, რომელსაც ძალიან მონაცემები ანოტაციები, ძალიან სწავლების მოდელები და მეთოდიები, რომელსაც ძალიან დააკეთებული გან ეს მოდულიარიზაცია მოცემების პროცესის შესაძლებლობა საკუთარი ინფორმაციისთვის, როგორც მონაცემების შესაძლებლობა, ჰერისტიკური წესების შესაძლებლობა, ან საკუთარი წესების ელე ამიტომ, ჩვენი ფრამეტრი შეუძლიათ გავაკეთოთ მნიშვნელოვანი განაკეთება მეტისთვის, რომელიც უფრო ძალიან დარწმუნებელოვანის მეტისთვის, რომელიც მხოლოდ ხედავთ წესების და გადასვლის კლასების კოლექციების კოლექციების კოლექციების ჩვენ გამოყენებთ ფრამეტრის ბანქმერის პონციალური პონციალური გადაწყენება რამდენიმე რეფერენტური ინმპლექციების გადაწყენებაზე, რომელიც კონდელში უკვე ხელხ', 'hu': 'A gyengén felügyelt gépi tanulási modellek képzési és előrejelzési minőségének javítására irányuló stratégiák attól függően változnak, hogy azok milyen mértékben vannak egy adott feladathoz igazítva vagy egy adott modellarchitektúrához integrálva. Ebben a munkában bemutatjuk a Knodle-t, egy olyan szoftver keretrendszert, amely különálló, moduláris összetevőként kezeli a gyenge adatmegjegyzéseket, a mélytanulási modelleket és a gyengén felügyelt képzések fejlesztésére szolgáló módszereket. Ez a modulárizáció hozzáférést biztosít a képzési folyamat finom szemű információkhoz, mint például az adatkészlet jellemzői, heurisztikus szabályok egyezései vagy a mélytanulási modell elemei, amelyeket végső soron használnak a predikcióhoz. Ezért keretrendszerünk a gyenge felügyelet javítására szolgáló képzési módszerek széles skáláját foglalhatja magába, olyan módszerektől kezdve, amelyek csak a szabályok és a kimeneti osztályok összefüggéseit vizsgálják (függetlenül a kapott címkékkel képzett gépi tanulási modelltől), azokig, amelyek kihasználják az ideghálózatok és a gyengén jelölt adatok kölcsönhatását. A keretrendszer teljesítményértékelési potenciálját több referencia implementáció teljesítményének összehasonlításával illusztráljuk a Knodle-ben már elérhető adatkészleteken.', 'it': "Le strategie per migliorare la qualità della formazione e della previsione dei modelli di apprendimento automatico scarsamente supervisionati variano in quanto sono adattati a un compito specifico o integrati con una specifica architettura di modello. In questo lavoro presentiamo Knodle, un framework software che tratta annotazioni di dati deboli, modelli di deep learning e metodi per migliorare la formazione debolmente supervisionata come componenti modulari separati. Questa modulazione dà al processo di formazione l'accesso a informazioni a grana fine come caratteristiche del set di dati, corrispondenze di regole euristiche o elementi del modello di deep learning utilizzato in ultima analisi per la previsione. Pertanto, il nostro framework può comprendere una vasta gamma di metodi di formazione per migliorare la supervisione debole, che vanno da metodi che guardano solo alle correlazioni di regole e classi di output (indipendentemente dal modello di machine learning addestrato con le etichette risultanti), a quelli che sfruttano l'interazione di reti neurali e dati debolmente etichettati. Illustriamo il potenziale di benchmarking del framework con un confronto delle prestazioni di diverse implementazioni di riferimento su una selezione di set di dati già disponibili in Knodle.", 'kk': 'Бақылау және бақылау үлгілерінің бақылау және бақылау сапатын жақсарту стратегиялары бір тапсырмаға немесе нақты үлгілер архитектурасынан қанша өзгертілген тапсырмаға не қанша өзгерті Бұл жұмыстың ішінде Knodle- ді, бағдарламалық бағдарламалық бағдарламалық бағдарламалық бағдарламалық жазбаларды, терең оқыту үлгілерін және бақылау бағдарламалық бағдарламалық бағдарламалық Бұл модуляризация деректер жинақталған мәліметтерге, геуристикалық ережелердің сәйкестігі немесе түсінікті оқыту үлгісінің элементтеріне қатынауға мүмкіндік береді. Сондықтан, біздің қоршауымыз көп бақылау әдістерін жақсарту үшін, тек ережелер мен шығару классының қатынасын қарайтын әдістерінен артылған (нәтижелерімен үйренген машина үйрену үлгісінің тәуелсіз), невралдық желілердің интерплейін және жарлық деректер Біз бағдарламаның бағдарламасының бағдарламасын Knodle-де бар деректер жинақтарын таңдау үшін бірнеше сілтеме жұмысын салыстыру мүмкіндігін көрсетедік.', 'ms': 'Strategi untuk meningkatkan kualiti latihan dan ramalan model pembelajaran mesin yang mengawasi lemah beragama dalam berapa banyak mereka disesuaikan untuk tugas tertentu atau disertai dengan arkitektur model tertentu. Dalam kerja ini, kami memperkenalkan Knodle, kerangka perisian yang menjaga anotasi data lemah, model belajar dalam, dan kaedah untuk meningkatkan latihan yang diawasi lemah sebagai komponen modular terpisah. Modularisasi ini memberikan proses latihan akses kepada maklumat berwarna ganda seperti ciri-ciri set data, persamaan peraturan heuristik, atau elemen model belajar dalam yang akhirnya digunakan untuk ramalan. Oleh itu, kerangka kami boleh meliputi julat luas kaedah latihan untuk meningkatkan pengawasan lemah, berlainan dari kaedah yang hanya melihat korelasi peraturan dan kelas output (secara independen dari model belajar mesin dilatih dengan label yang berasal), kepada yang menggunakan persimpangan rangkaian saraf dan data yang lemah. Kami memperlihatkan potensi benchmarking kerangka dengan perbandingan prestasi beberapa implementasi rujukan pada pemilihan set data yang sudah tersedia di Knodle.', 'lt': 'Mažiau prižiūrimų mašin ų mokymosi modelių mokymo ir prognozės kokybės gerinimo strategijos skiriasi atsižvelgiant į tai, kiek jie pritaikyti konkrečiai užduotims arba integruoti į konkrečią modelio architektūrą. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components.  Ši modulizacija suteikia mokymo procesui galimybę susipažinti su smulkiai apdorota informacija, pavyzdžiui, duomenų rinkinio charakteristikomis, heuristinių taisyklių atitiktimis arba gilaus mokymosi modelio elementais, galiausiai naudojamais prognozuoti. Taigi mūsų sistema gali apimti įvairius mokymo metodus, skirtus gerinti silpną priežiūrą, nuo metodų, kuriais atsižvelgiama tik į taisyklių ir išvesties klasių koreliacijas (nepriklausomai nuo mašininio mokymosi modelio, parengto su gautais ženklais), iki tų, kurie naudojasi nervinių tinklų sąveika ir silpnai pažymėtais duomenimis. Mes iliustruojame sistemos lyginamąjį vertinimo potencialą, palygindami kelis orientacinius įgyvendinimus, susijusius su duomenų rinkinių, kurie jau yra Knodle, atranka.', 'mk': 'Стратегиите за подобрување на квалитетот на обуката и предвидувањето на слабо надгледуваните модели на машинско учење разликуваат во колку се прилагодени на специфична задача или интегрирани со специфична моделна архитектура. Во оваа работа, го воведуваме Нодл, софтверска рамка која ги третира слабите анатации на податоците, моделите за длабоко учење и методите за подобрување на слабо надгледуваната обука како одделни модуларни компоненти. Оваа модуларизација му овозможува на процесот на обука пристап до фини информации како што се карактеристиките на податоците, соодветностите со хеористичките правила или елементите на моделот на длабоко учење кој на крајот се користи за предвидување. Затоа, нашата рамка може да вклучи широк опсег на методи за обука за подобрување на слабиот надзор, од методи кои само ги гледаат корелациите на правилата и класите на излез (независно од моделот на машинско учење обучен со резултатните етикети), до оние кои ја искористуваат интервјуацијата на невровните мрежи и слабо означени под Ние го илустрираме споредниот потенцијал на рамката со споредба на перформансите на неколку референциски имплементации на селекцијата на податоци кои веќе се достапни во Knodle.', 'mt': 'L-istrateġiji għat-titjib tal-kwalità tat-taħriġ u t-tbassir ta’ mudelli ta’ tagħlim bil-magni b’superviżjoni dgħajfa jvarjaw skont kemm huma mfassla għal kompitu speċifiku jew integrati ma’ arkitettura ta’ mudell speċifiku. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components.  This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction.  Għalhekk, il-qafas tagħna jista’ jinkludi firxa wiesgħa ta’ metodi ta’ taħriġ għat-titjib tas-superviżjoni dgħajfa, li jvarjaw minn metodi li jħarsu biss lejn il-korrelazzjonijiet tar-regoli u l-klassijiet ta’ ħruġ (indipendentement mill-mudell ta’ tagħlim bil-magni mħarreġ mat-tikketti li jirriżultaw), għal dawk li jużaw l-interazzjoni tan-netwerks newrali u d-dejta b’tikketta Aħna nuru l-potenzjal ta’ benchmarking tal-qafas b’paragun tal-prestazzjoni ta’ bosta implimentazzjonijiet ta’ referenza dwar għa żla ta’ settijiet ta’ dejta li diġà huma disponibbli f’Knodle.', 'ml': 'മെഷീന്\u200d പഠിക്കുന്ന മോഡലുകളുടെ പരിശീലനവും പ്രവചനങ്ങളും മുന്\u200dകൂട്ടുവാനുള്ള സ്ട്രേറ്റീജികള്\u200dക്ക് വ്യത്യാസങ്ങള്\u200d എത്ര വ്യത്യസ്തമായ ഒരു പ്രത്യേക ഈ പ്രവര്\u200dത്തനത്തില്\u200d, നമ്മള്\u200d കെനോഡില്\u200d, ദുര്\u200dബലരായ ഡേറ്റാ വിവരങ്ങള്\u200d, ആഴത്തെ പഠിപ്പിക്കുന്ന മോഡലുകള്\u200d നോക്കുന്ന ഒരു ഫ്രെയിമെന്\u200dറ് ഫ്രെയിമെ ഈ പ്രക്രിയമങ്ങള്\u200d പരിശീലനത്തിന്റെ പ്രക്രിയയ്ക്ക് സുന്ദരിതമായ വിവരങ്ങള്\u200d പോലെയുള്ള വിവരങ്ങള്\u200dക്ക് ലഭ്യമാക്കുന്നു. ഡേറ്റാ സെറ്റിക്റ്റ അതുകൊണ്ട്, നമ്മുടെ ഫ്രെയിമേര്\u200dക്ക് ദുര്\u200dബലനിരീക്ഷിക്കാനുള്ള പരിശീലന മാര്\u200dഗ്ഗങ്ങള്\u200d മുന്\u200dകൂട്ടുവാന്\u200d വേണ്ടിയാകുന്നു. നിയമങ്ങളുടെയും പുറത്തുള്ള ക്ലാസുകളുടെയും ബന്ധങ്ങള്\u200d മാത് കെന്നോഡില്\u200d നിലവിലുള്ള ഡേറ്റാസറ്റുകളുടെ തെരഞ്ഞെടുക്കുമ്പോള്\u200d കുറച്ച് റെഫറന്\u200dസ് പ്രവര്\u200dത്തനങ്ങള്\u200dക്ക് തുല്യമായ ഒരു പ്രദര', 'no': 'Strategier for å forbetra opplæringa og forhåndsvisingskvaliteten av vekk oversikte maskinelæringsmodeller varierer i kor mykje dei er tilpassa til eit spesifikke oppgåve eller integrert med ein spesifikke modell-arkitektur. I denne arbeida introduserer vi Knodle eit programvare-rammeverk som behandler svake data-notasjonar, dype læringsmodeller og metodar for å forbedra viktig oversikt opplæring som separe modular komponentar. Denne modulariseringa gjev opplæringsprosessen tilgang til fyrste informasjon, slik som datasett-karakteristikk, treff med heuristiske reglar, eller elementa av den dype læringsmodellen som er til slutt brukt for foregåve. Dette kan derfor vårt rammeverk omfatta eit stor rekke opplæringsmetodar for å forbetra svakt overvåking, avhengig av metodar som berre ser på korrelasjonar av reglar og utdata klasser (uavhengig av maskinelæringsmodellen treng med resultatet merkelappar), til dei som køyrer samsvaret med neuralnettverk og svakt merkelapp. Vi illustrerer benchmarkpotensialen for rammeverket med ein sammenligning av fleire referanse-implementasjonar på eit utval av datasett som allereie er tilgjengelege i Knodle.', 'mn': 'Сургууль болон таамаглалтын сайжруулах стратеги нь машины суралцах загварын суралцах загварын хэмжээг тодорхой ажил дээр, эсвэл тодорхой загварын архитектуртай хамтдаа хэр их өөрчлөгдөж байгааг харуулах. Энэ ажлын тулд бид Knodle-г бага өгөгдлийн нээлттэй, гүн гүнзгий суралцах загваруудыг харуулдаг програм хангамжийг танилцуулж, бага зэрэг удирдлагатай суралцах үйл ажиллагааг өөр, модуль компонентүүд болго Энэ модульчлалын дасгал үйлдвэрлэл нь өгөгдлийн хэмжээсүүд, хэмжээсүүд, хэмжээсүүд болон гүн гүнзгий суралцах загварын элементүүдийг төгсгөлд таамаглахад хэрэглэгддэг. Тиймээс бидний хэлбэрээр сул хязгаарлагч байдлыг сайжруулахын тулд олон төрлийн сургалтын арга загвар нь зөвхөн дүрслэл болон гаргах хичээлийн хоорондоо харах арга загвараас хамаарч болно. Бид хэд хэд хэдэн давтамжтайгаа харьцуулахын тулд хэдэн давтамжтайгаар Кнодлд байгаа өгөгдлийн сангуудын сонголт дээр харьцуулах боломжтой байдлыг харуулж байна.', 'pl': 'Strategie poprawy jakości szkoleń i prognozowania słabo nadzorowanych modeli uczenia maszynowego różnią się od tego, jak bardzo są one dostosowane do konkretnego zadania lub zintegrowane z konkretną architekturą modelu. W niniejszej pracy przedstawiamy Knodle, framework oprogramowania, który traktuje słabe adnotacje danych, modele głębokiego uczenia oraz metody poprawy słabo nadzorowanego szkolenia jako oddzielne, modułowe komponenty. Modularyzacja ta daje procesowi szkoleniowemu dostęp do precyzyjnych informacji, takich jak charakterystyka zbioru danych, dopasowanie reguł heurystycznych lub elementy modelu głębokiego uczenia ostatecznie wykorzystywane do prognozowania. W związku z tym nasze ramy mogą obejmować szeroki zakres metod szkoleniowych służących poprawie słabego nadzoru, począwszy od metod, które patrzą wyłącznie na korelacje reguł i klas wyjściowych (niezależnie od modelu uczenia maszynowego trenowanego z powstałymi etykietami), po metody wykorzystujące interakcję sieci neuronowych i słabo oznaczonych danych. Ilustrujemy potencjał porównawczy frameworku poprzez porównanie wydajności kilku implementacji referencyjnych na wybranych zbiorach danych, które są już dostępne w Knodle.', 'ro': 'Strategiile pentru îmbunătățirea calității pregătirii și predicției modelelor de învățare automată supravegheate slab variază în măsura în care acestea sunt adaptate unei sarcini specifice sau integrate cu o arhitectură specifică a modelului. În această lucrare, introducem Knodle, un cadru software care tratează adnotările slabe de date, modelele de învățare profundă și metodele de îmbunătățire a instruirii slab supravegheate ca componente separate, modulare. Această modularizare oferă procesului de formare acces la informații fine, cum ar fi caracteristicile setului de date, meciurile regulilor euristice sau elementele modelului de învățare profundă utilizat în cele din urmă pentru predicție. Prin urmare, cadrul nostru poate cuprinde o gamă largă de metode de instruire pentru îmbunătățirea supravegherii slabe, variind de la metode care analizează doar corelațiile regulilor și claselor de ieșire (independent de modelul de învățare automată instruit cu etichetele rezultate), la cele care valorifică interacțiunea rețelelor neurale și datelor slab etichetate. Ilustrăm potențialul de benchmarking al cadrului prin compararea performanțelor mai multor implementări de referință pe o selecție de seturi de date care sunt deja disponibile în Knodle.', 'so': 'Tilmaamaha horumarinta waxbarashada iyo sii-sheegidda qaababka barashada maskaxda la taagan yahay waxay ku kala duwan yihiin siduu u qoran yahay shaqo gaar ah ama la wada ururiyo dhismo qaab cayiman ah. Shaqadan, waxaan Knodle ka soo bandhignaynaa qalabka software, kaas oo ka maareysa dhibaatooyinka macluumaadka itaaldaran, tusaalaha waxbarashada mool dheer, iyo qaababka horumarinta waxbarashada tababarka itaalka la ilaaliyo si gooni ah oo qayb gaar ah. Isku-dhigistan wuxuu heli karaa macluumaadka la xiriira barbaarinta, tusaale ahaan takhasuska takhasuska, u eg sharciyada heuristiga, ama qeybaha modelka waxbarashada moolka ah ee ugu dambaysta loo isticmaalayo in la sii sheego. Sababtaas darteed waxaa ku qoran kara qaabab waxbarasho oo kala duduwan, si loo horumariyo ilaalinta dhaqdhaqaaqa ah, taasoo ka duwan qaababka loo fiiriyo xiriirka qaynuunnada iyo fasalka soo bixinta (si gaar ah oo ka mid ah modelka waxbarashada mashiinka lagu baray baalasha resultinta), kuwa hareereeya iskuulka shabakadda neurada ah iyo macluumaadka taageeriga ah. Waxaynu sawiraynaa awoodda bangiga firaamka oo sameynaya sameynta habbooyinka qaar ka mid ah habboon doorashada macluumaadka ee ku jira Knodle.', 'si': 'ප්\u200dරධානය සහ ප්\u200dරශ්නයක් විශ්වාස කරන්න සඳහා දුර්වලින් පරික්ෂා කරපු පරික්ෂණයේ පරික්ෂා කරපු පරික්ෂණාවක් වෙනස් වෙන්න මේ වැඩේ අපි නොඩ්ල්, සොෆ්ටවේර් ෆ්\u200dරේම්වර් එකක් ප්\u200dරදේශ කරනවා, දුර්වල් දත්ත ප්\u200dරදේශනය, ගොඩක් ඉගෙනීම් මෝඩේල් වලට, සහ දුර්ව මේ මොඩියුලාර්ජිකරණය ප්\u200dරශ්නයක් ප්\u200dරවේශනය සඳහා දත්ත සැකසුම් විශේෂතාවක්, හෙවුරිස්ටික් නීතිය, නැත්තම් ගොඩක් ඉගෙන ඉතින්, අපේ පරීක්ෂණය පුළුවන් විශාල ප්\u200dරශ්නයක් තියෙන්න පුළුවන් විශාල ප්\u200dරශ්නයක් විතරයි, ප්\u200dරශ්නයක් විතරයි, ප්\u200dරශ්නයක් විතරයි, නීති සහ ප්\u200dරශ්නයක් ප අපි පෙන්වන්නේ ක්\u200dරීමාර්කයේ බෙන්ච්මාර්ක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් වෙන්නේ ක්\u200dරියාත්මක ප්\u200dරශ්නයක් සඳහා ප්\u200dරශ', 'sv': 'Strategier fﾃｶr att fﾃｶrbﾃ､ttra utbildnings- och fﾃｶrutsﾃ､gelsekvaliteten hos svagt ﾃｶvervakade maskininlﾃ､rningsmodeller varierar i hur mycket de ﾃ､r skrﾃ､ddarsydda fﾃｶr en specifik uppgift eller integrerade med en specifik modellarkitektur. I detta arbete introducerar vi Knodle, ett ramverk som behandlar svaga dataanteckningar, djupinlﾃ､rningsmodeller och metoder fﾃｶr att fﾃｶrbﾃ､ttra svagt ﾃｶvervakad utbildning som separata modulﾃ､ra komponenter. Denna modularisering ger trﾃ､ningsprocessen tillgﾃ･ng till finkornig information som datauppsﾃ､ttningens egenskaper, matchningar av heuristiska regler eller element i den djupinlﾃ､rningsmodell som slutligen anvﾃ､nds fﾃｶr fﾃｶrutsﾃ､gelse. Dﾃ､rfﾃｶr kan vﾃ･rt ramverk omfatta ett brett utbud av utbildningsmetoder fﾃｶr att fﾃｶrbﾃ､ttra svag ﾃｶvervakning, allt frﾃ･n metoder som endast tittar pﾃ･ korrelationer mellan regler och outputklasser (oberoende av maskininlﾃ､rningsmodellen trﾃ､nad med de resulterande etiketterna), till dem som utnyttjar samspelet mellan neurala nﾃ､tverk och svagt mﾃ､rkta data. Vi illustrerar ramverkets benchmarkingpotential med en prestationsjﾃ､mfﾃｶrelse av flera referensimplementeringar pﾃ･ ett urval av datauppsﾃ､ttningar som redan finns tillgﾃ､ngliga i Knodle.', 'ta': 'மேம்படுத்தப்பட்ட பயிற்சி மற்றும் முன்வாக்குதல் தரம் பலவீனமாக கண்காணிக்கப்பட்ட இயந்திர கற்பிக்கும் மாதிரிகளுக்கு மாறுபடும் திட்டங்கள் எவ்வளவு ம இந்த வேலையில், நாம் Knodle, ஒரு மென்பொருள் சட்டத்தை குறிப்பிடுகிறோம். அது பலவீனமான தகவல் அறிவிப்பு, ஆழமான கற்றுக்கொள்ள மாதிரிகள், மற்றும் முறை இந்த கூற்று பயிற்சி செயல்பாடு நன்றாக கிடைக்கப்பட்ட தகவல்களை போன்ற தரவு அமைப்பு தன்மைகள், ஹிரிசிடிஸ் விதிகளின் பொருத்தம், அல்லது முன்னோட்டி அதனால், எங்கள் சட்டத்தை பலவீனமான கண்காணிப்பு மேம்படுத்துவதற்கான பயிற்சி முறைகளை சுற்றி செல்ல முடியும், வெளியீட்டு வகுப்புகளின் தொடர்புகளை மட்டும் பார்க்க முடியும் (முடிவு சிட்டுகள நாம் கேநோட்லில் ஏற்கனவே கிடைக்கும் தரவுத்தளத்தின் தேர்ந்தெடுப்பு தேர்ந்தெடுப்பில் பல குறிப்பு செயல்பாடுகளை ஒப்பிடு', 'ur': 'کمزور تحقیق کی ماشین یادگیری مدلکوں کی تدریس اور پیش بینی کی کیفیت کے لئے استراتژیات کس طرح متفاوت ہوتے ہیں کہ ان کو ایک خاص تابع یا ایک خاص مدل معماری کے ساتھ تدریج کیا جاتا ہے۔ اس کام میں ہم نے نوڈل کو معرفی کیا ہے، ایک سافٹوفر فرم جو کمزور ڈاٹ انٹیٹیزوں، عمیق تعلیم نمڈلوں اور کمزور تحقیق کی تعلیم کو مختلف، موڈولر رقموں کے طور پر استعمال کرتا ہے۔ یہ موڈولریزی تعلیم پرسس کو پاکیزہ دانے کی معلومات کے لئے پہنچ دیتی ہے جیسے ڈیٹا سٹ کے معلومات، ہوریستیک قانون کے مطابق، یا عمیق تعلیم موڈل کے عنصر کو بالکل پیش بینی کے لئے استعمال کیا جاتا ہے. لہٰذا ہمارے فرمود کمزور نظر کے ساتھ کمزور نظر کے مطابق بہترین ترکین طریقوں کو احاطہ کر سکتا ہے، جو صرف قانون اور آوٹ کلاس کی تعلقات کی طرف دیکھتے ہیں (ماشین تعلیم مدل کے بغیر، نتیجہ لیبلوں کے ساتھ تعلیم کی جاتی ہے) ان لوگوں کے ساتھ جو نیورل نیٹورک کی تعلقات اور کمزور لیبل کی ہم چوکاٹ کی بنچم مارکینگ کے امکانات کو دکھا رہے ہیں کہ نوڈلز میں پہلے موجود ہیں ایک ڈیٹ سٹ کے انتخاب پر چند سراسر سراسر کاربرد کے مقایسے کے ساتھ.', 'sr': 'Strategije za poboljšanje kvalitete obuke i predviđanja slabi nadzornih model a učenja mašina razlikuju se u tome koliko ih je prilagođeno određenom zadatku ili integrisano sa specifičnom modelom arhitekture. U ovom poslu predstavljamo Knodle, softver okvir koji tretira slabe annotacije podataka, model dubokog učenja i metode za poboljšanje slabe nadzorne obuke kao odvojene modularne komponente. Ova modularizacija daje pristup procesu obuke detaljnim informacijama kao što su karakteristike podataka, odgovarajuće heurističkim pravilima ili elementima dubokog modela učenja koji se konačno koristi za predviđanje. Stoga, naš okvir može obuhvatiti širok niz metoda obuke za poboljšanje slabe nadzore, od metoda koje samo gledaju korelacije pravila i klasa izlaza (nezavisno od model a učenja mašine obučenog sa rezultatnim etiketama), do onih koji koriste interakciju neuralnih mreža i slabe etiketirane podatke. Ilustriramo potencijal preglede okvira sa usporedbom provedbe nekoliko referentnih implementacija na izboru seta podataka koji su već dostupni u Knodle-u.', 'uz': "Name Bu ishda, biz Knodle, bir dasturni ko'rib chiqaramiz, yaxshi maʼlumot taʼminotlari, juda o'rganish modellarini va o'rganish usullarini o'zgartirish qo'llanmalarni alohida, modulyat komponentlarni o'zgartirish uchun qo'llanmalar. @ info Shunday qilib, bizning freymimiz qoidalar va tashqari sinfning bog'liqlariga o'rganish usullarini ko'paytirish uchun juda ko'p ta'lim usullarini o'zgartirish mumkin. Bu usullardan faqat qoidalar va natijalar darajalariga o'rganish modelini o'rganish mumkin, va neyrol tarmoqlarining interfeys o'zgarishlarini o'zgartirish mumkin. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.", 'vi': 'Kỹ thuật cải thiện chất lượng huấn luyện và dự đoán của những mô hình máy học có trình độ ít người giám sát khác nhau về mức độ chúng được phù hợp với một nhiệm vụ cụ thể hoặc được cấu trúc mô hình cụ thể. Trong công việc này, chúng tôi giới thiệu Knodle, một phần mềm mà phần mềm cần phải xem nhẹ ghi chú dữ liệu, các mô hình học sâu, và phương pháp cải tiến việc huấn luyện thiếu người giám sát là bộ phận khác biệt. Sự chuyển động này cho quá trình đào tạo truy cập vào các thông tin đặc biệt như các dữ liệu, khớp với các quy tắc thần kinh, hoặc các yếu tố của mô hình học sâu cuối cùng dùng để dự đoán. Hệ thống của chúng ta có thể bao gồm một loạt các phương pháp huấn luyện nhằm cải thiện sự giám sát yếu, từ các phương pháp chỉ xem các mối liên hệ của các quy tắc và các lớp xuất (độc lập với mô hình học máy được huấn luyện với các nhãn kết quả), cho đến những phương pháp nắm giữ liên hệ thần kinh và các dữ liệu bị đánh dấu thiếu sót. Chúng tôi mô tả khả năng đo định giới tính của khung với một so sánh hiệu suất của một số dụng cụ tham khảo trên một loạt các tập tin đã có sẵn ở Knodle.', 'da': 'Strategier til forbedring af trænings- og forudsigelseskvaliteten af svagt overvågede maskinlæringsmodeller varierer i, hvor meget de er skræddersyet til en bestemt opgave eller integreret med en bestemt modelarkitektur. I dette arbejde introducerer vi Knodle, en softwareramme, der behandler svage dataannoteringer, deep learning modeller og metoder til forbedring af svagt overvåget træning som separate, modulære komponenter. Denne modularisering giver træningsprocessen adgang til finkornede oplysninger såsom datasæt karakteristika, matches af heuristiske regler eller elementer i den dybe læringsmodel, der i sidste ende anvendes til forudsigelse. Derfor kan vores rammer omfatte en lang række træningsmetoder til forbedring af svag overvågning, lige fra metoder, der kun ser på sammenhængen mellem regler og outputklasser (uafhængigt af maskinlæringsmodellen uddannet med de resulterende etiketter), til dem, der udnytter samspillet mellem neurale netværk og svagt mærkede data. Vi illustrerer rammernes benchmarkingpotentiale med en præstationssammenligning af flere referenceimplementeringer på et udvalg af datasæt, der allerede er tilgængelige i Knodle.', 'bg': 'Стратегиите за подобряване качеството на обучението и прогнозирането на слабо контролираните модели на машинно обучение варират в това доколко те са съобразени с конкретна задача или интегрирани със специфична архитектура на модела. В тази работа представяме софтуерна рамка, която третира слабите анотации на данни, моделите на дълбоко обучение и методите за подобряване на слабо контролираното обучение като отделни модулни компоненти. Тази модуларизация дава на процеса на обучение достъп до фина информация като характеристики на набор от данни, съвпадения на евристични правила или елементи от модела на дълбоко обучение, използван в крайна сметка за прогнозиране. Следователно нашата рамка може да обхване широк спектър от методи за обучение за подобряване на слабия надзор, вариращи от методи, които разглеждат само корелациите на правилата и изходните класове (независимо от модела на машинно обучение, обучен с получените етикети), до тези, които използват взаимодействието на невронни мрежи и слабо етикетирани данни. Ние илюстрираме потенциала за сравнителен анализ на рамката с сравнение на ефективността на няколко референтни внедрения върху подбор от набори от данни, които вече са налични в Нодъл.', 'nl': 'Strategieën voor het verbeteren van de training- en voorspellingskwaliteit van zwak begeleide machine learning modellen variëren in hoeverre ze zijn afgestemd op een specifieke taak of geïntegreerd met een specifieke modelarchitectuur. In dit werk introduceren we Knodle, een softwareframework dat zwakke data annotaties, deep learning modellen en methoden voor het verbeteren van zwak begeleide training behandelt als afzonderlijke, modulaire componenten. Deze modularisering geeft het trainingsproces toegang tot fijngranige informatie zoals kenmerken van datasets, matches van heuristische regels of elementen van het deep learning model die uiteindelijk worden gebruikt voor voorspelling. Daarom kan ons framework een breed scala aan trainingsmethoden omvatten om zwakke supervisie te verbeteren, variërend van methoden die alleen kijken naar correlaties van regels en outputklassen (onafhankelijk van het machine learning model dat getraind is met de resulterende labels), tot methoden die gebruikmaken van het samenspel van neurale netwerken en zwak gelabelde data. We illustreren het benchmarkingspotentieel van het framework met een prestatievergelijking van verschillende referentieimplementaties op een selectie datasets die al beschikbaar zijn in Knodle.', 'de': 'Strategien zur Verbesserung der Trainings- und Vorhersagequalität schwach überwachter Machine Learning-Modelle unterscheiden sich darin, wie sehr sie auf eine bestimmte Aufgabe zugeschnitten oder in eine bestimmte Modellarchitektur integriert sind. In dieser Arbeit stellen wir Knodle vor, ein Software-Framework, das schwache Datenannotationen, Deep Learning-Modelle und Methoden zur Verbesserung schwach überwachten Trainings als separate, modulare Komponenten behandelt. Durch diese Modularisierung erhält der Trainingsprozess Zugang zu detaillierten Informationen wie Datensatzeigenschaften, Übereinstimmungen heuristischer Regeln oder Elementen des Deep Learning Modells, die letztendlich für die Vorhersage verwendet werden. Daher kann unser Framework ein breites Spektrum an Trainingsmethoden zur Verbesserung schwacher Supervision umfassen, von Methoden, die nur auf Korrelationen von Regeln und Output-Klassen achten (unabhängig vom maschinellen Lernmodell, das mit den resultierenden Labels trainiert wird), bis hin zu solchen, die das Zusammenspiel von neuronalen Netzwerken und schwach markierten Daten nutzen. Wir veranschaulichen das Benchmarking-Potenzial des Frameworks mit einem Leistungsvergleich mehrerer Referenzimplementierungen auf einer Auswahl von Datensätzen, die bereits in Knodle verfügbar sind.', 'id': 'Strategi untuk meningkatkan pelatihan dan prediksi kualitas model belajar mesin yang terlihat lemah berbeda dalam berapa banyak mereka disesuaikan untuk tugas spesifik atau terintegrasi dengan arsitektur model spesifik. Dalam pekerjaan ini, kami memperkenalkan Knodle, sebuah rangkaian perangkat lunak yang memperlakukan anotasi data lemah, model belajar dalam, dan metode untuk meningkatkan pelatihan yang diawasi lemah sebagai komponen modular terpisah. Modularisasi ini memberikan proses pelatihan akses ke informasi yang sempurna seperti karakteristik set data, persamaan peraturan heuristik, atau elemen model belajar dalam yang akhirnya digunakan untuk prediksi. Oleh karena itu, kerangka kita dapat meliputi jangkauan luas metode pelatihan untuk meningkatkan pengawasan lemah, berasal dari metode yang hanya melihat korelasi aturan dan kelas keluaran (bebas dari model belajar mesin yang dilatih dengan label hasilnya), hingga yang menggunakan interaksi jaringan saraf dan data yang lemah. Kami memperlihatkan potensi benchmarking dari kerangka dengan perbandingan prestasi dari beberapa implementasi referensi pada seleksi set data yang sudah tersedia di Knodle.', 'ko': '약한 감독 기계 학습 모델의 훈련과 품질 예측 전략을 개선하는 것은 어느 정도에 특정한 임무를 대상으로 맞춤형으로 만들었는지, 아니면 특정한 모델 체계 구조와 통합되었는지에 달려 있다.이 작업에서 우리는 Knodle을 소개할 것이다. 이것은 약한 데이터 주석, 깊이 있는 학습 모델, 약한 감독 훈련을 개선하는 방법을 단독 모듈화 구성 요소로 간주하는 소프트웨어 프레임워크이다.이런 모듈화는 교육 과정으로 하여금 세립도 정보, 예를 들어 데이터 집합 특징, 계발식 규칙의 일치 또는 최종적으로 예측하는 깊이 학습 모델의 요소에 접근할 수 있게 한다.따라서 우리의 프레임워크는 약한 감독을 개선하는 훈련 방법을 포함할 수 있다. 규칙과 출력 클래스 간의 관련성만 보는 방법(결과 라벨 훈련을 사용하는 기계 학습 모델에 독립)부터 신경 네트워크와 약한 라벨 데이터의 상호작용을 이용하는 방법까지 다양하다.우리는 Knodle에 이미 있는 일련의 데이터 집합에 대한 몇 가지 참고로 이루어진 성능 비교를 통해 이 프레임워크의 기준 테스트 잠재력을 설명한다.', 'fa': 'استراتژی\u200cها برای بهترین کیفیت آموزش و پیش\u200cبینی از مدل یادگیری ماشین ضعیف تحت نظر تغییر می\u200cگیرند و چقدر به یک کار خاص تغییر داده می\u200cشوند یا با یک معماری مدل خاص تغییر داده می\u200cشوند. در این کار، ما نودل را معرفی می\u200cکنیم، یک چهارچوب نرم\u200cافزار که با توضیح داده\u200cهای ضعیف درمان می\u200cکند، مدل\u200cهای یادگیری عمیق و روش\u200cهایی برای بهتر آموزش\u200cهای ضعیف تحت نظر به عنوان بخش\u200cهای متفاوتی، متفا این مدول\u200cسازی به فرایند آموزش دسترسی به اطلاعات خوش\u200cخرد مانند ویژگی\u200cهای مجموعه داده\u200cها، مسابقه\u200cهای قانون حوریست، یا عناصر مدل یادگیری عمیق را در نهایت برای پیش\u200cبینی استفاده می\u200cکند. بنابراین، چهارچوب ما می\u200cتواند از طریق آموزش\u200cهای گسترده برای بهترین کنترل ضعیف محیط کند، از طریق\u200cهایی که تنها به ارتباطات قانون و کلاس\u200cهای خروجی نگاه می\u200cکنند (به تنهایی از مدل یادگیری ماشین که با نقاشی\u200cهای نتیجه آموزش می\u200cشود) به کسانی که با ارتباط شبکه\u200cهای عصبی و ما پتانسیل برچسب\u200cبندی چهارچوب را با مقایسه\u200cای از تعداد برچسب\u200cهای برچسب\u200cبندی در انتخاب مجموعه\u200cهای داده\u200cبندی که قبلا در Knodle موجود هستند نشان می\u200cدهیم.', 'sw': 'Mikakati ya kuboresha mafunzo na kutabiri kiwango cha miundo ya elimu ya mashine yenye udhaifu unaofanywa tofauti ni kiasi gani vinavyogeuzwa kwenye kazi maalum au zinajumuishwa na ujenzi maalum. Katika kazi hii, tunautambulisha Knodle, mfumo wa programu wa programu unaotumia matangazo ya taarifa dhaifu, mitindo ya kujifunza kwa kina, na njia za kuboresha mafunzo yanayofuatiliwa dhaifu kama sehemu tofauti, za kawaida. Utawala huu unawapa mchakato wa mafunzo upatikanaji wa taarifa nzuri kama vile taarifa zinazoweka sifa za taarifa, zinazochanganya sheria za heuristi, au vipengele vya modeli ya kujifunza kwa mwisho unaotumiwa kwa kutabiri. Kwa hiyo, mfumo wetu unaweza kuzungumzia mbinu nyingi za mafunzo kwa ajili ya kuboresha ufuatiliaji dhaifu wa udhaifu, ukilinganisha na mbinu ambazo tu huangalia uhusiano wa sheria na darasa la output (huru ya muundo wa kujifunza mashine uliojifundishwa na maabara yanayotokana), kwa wale wanaoelekeza mitandao ya ubongo na data dhaifu. Tunaelezea uwezekano wa benchmark wa mfumo huo ukilinganisha matumizi kadhaa ya maoni katika uchaguzi wa seti za taarifa ambazo tayari zinapatikana Knodle.', 'af': "Strategies vir die verbetering van die onderwerp en voorskou kwaliteit van swak onderwerp masjien onderwerp modele verander in hoe veel hulle versterk word aan 'n spesifieke taak of integreer met 'n spesifieke model-arkitektuur. In hierdie werk, introduseer ons Knodle, 'n sagteware raamwerk wat swak data annotasies behandel, diep leer modele en metodes vir die verbetering van swak ondersoekte onderwerp as aparte, modulare komponente. Hierdie modularisering gee die oefening proses toegang tot fyn-koring inligting soos data stel karakteristieke, ooreenstemmende van heuristiese reëls, of elemente van die diep leer model wat eindelik gebruik word vir voorskou. Ons raamwerk kan daarom 'n wyse reek van onderwerp metodes omsluit vir die verbetering van swak onderwerp, vanaf metodes wat slegs kyk na korrelasies van reëls en uitvoer klasse (onveiligheid van die masjien leer model wat opgelei is met die resulteerde etikette), tot die wat die onderwerp van neurale netwerke en swak etiketeerde data ontvang. Ons illustreer die benchmarking potensieel van die raamwerk met 'n prestasie vergelyking van verskeie verwysing implementasies op 'n keuse van datastelle wat reeds beskikbaar is in Knodle.", 'tr': "A첵ardan g철zle첵채n ma힊yny흫 철wrenme nusgalaryny흫 e휓itimi we 철ng철r체m kalitesini geli힊tirmek 체챌in strateji첵asy n채챌e n채hili takyk i힊e t채zedil첵채ndir 첵a-da spesifik bir nusga arhitektura 체첵tgedil첵채ndir. Bu i힊de Knodle'i zay캇f veri a 챌캇klamalar캇na, derin 철휓renme modellerine ve zay캇f g철zlemli e휓itim 힊eklinde ayr캇 ve mod체ler komponentlere geli힊tirmek 체챌in bir software 챌 챌er챌evesini tan캇t캇yoruz. Bu modularla힊dyrma 첵aly data d체z체mleri, heuristik d체zg체nlerini흫 e힊le힊enleri 첵a-da gaty 철wrenme modelini흫 elementlerine ula힊ma prosesini berir. 힇ol seb채pli, bizi흫 챌er챌ebimiz za첵ap supervisi첵asyny geli힊tirmek 체챌in 철r채n 챌arpy힊 metodlary bolup biler, di흫e kurallar we 챌yky힊 klaslary흫 arasynda g철zle첵채n 첵ullardan (ma힊yny흫 철wrenme modelini흫 netijesi bilen bilinen etiketlerden so흫ra), n채yral 힊ebekeleri흫 aralygyny we azajyk etiketlenen maglumatlary흫 챌arpy Biz 챌er채d채ni흫 benkmak potansynyny Knodle'da e첵첵채m bar veri setirlerinde birn채챌e Reference implementasynlary흫 kar힊캇la힊tyrylygy bilen g철rkez.", 'hr': 'Strategije za poboljšanje kvalitete obuke i predviđanja slabi nadzornih model a učenja strojeva razlikuju se u tome koliko se prilagođavaju određenom zadatku ili integriranom s specifičnom modelom arhitekture. U ovom poslu predstavljamo Knodle, softver okvir koji tretira slabe annotacije podataka, model dubokog učenja i metode za poboljšanje slabe nadzorne obuke kao odvojene modularne komponente. Ova modularizacija daje pristup procesu obuke detaljnim informacijama kao što su karakteristike podataka, odgovarajuće heurističkim pravilima ili elemente dubokog modela učenja koji se konačno koristi za predviđanje. Stoga, naš okvir može obuhvatiti širok niz metoda obuke za poboljšanje slabe nadzore, od metoda koje samo gledaju korelacije pravila i klasa izlaza (nezavisno od model a učenja strojeva obučenog s rezultatim etiketama), do onih koji koriste interakciju neuralnih mreža i slabe označene podatke. Ilustrujemo potencijal preglede okvira s usporedbom provedbe nekoliko referentnih provedba na izboru seta podataka koji su već dostupni u Knodle-u.', 'sq': 'Strategjitë për përmirësimin e cilësisë së trajnimit dhe parashikimit të modeleve të mësimit të makinave të mbikqyrur dobësisht ndryshojnë në se sa janë të përshtatshme për një detyrë të veçantë apo të integruara me një arkitekturë të veçantë modeli. Në këtë punë, ne paraqesim Knodle, një kuadër programi që trajton anotacionet e dobëta të të dhënave, modelet e mësimit të thellë dhe metodat për përmirësimin e trainimit të mbikqyrur dobësisht si komponente të veçanta modulare. Kjo modularizim i jep procesit të trajnimit akses ndaj informacionit të hollë si karakteristikat e grupit të të dhënave, përputhjet me rregullat heuristike apo elementet e modelit të mësimit të thellë të përdorur përfundimisht për parashikim. Kështu, kuadri ynë mund të përfshijë një gamë të gjerë metodash trainimit për përmirësimin e mbikqyrjes së dobët, që shkojnë nga metodat që shikojnë vetëm korrelacionet e rregullave dhe klasave të daljes (pavarësisht nga modeli i mësimit të makinave i trajnuar me etiketat që rezultojnë), deri në a to që përdorin ndërveprimin e rrjeteve nervore dhe të dhënave me etiketa të dobët. Ne ilustrojmë potencialin e përmbledhjes së kuadrit me një krahasim performancë të disa zbatimeve të referimit në një zgjedhje të grupeve të dhënash që tashmë janë në dispozicion në Knodle.', 'hy': 'Համաքրքիր վերահսկված մեքենային ուսումնասիրության մոդելների ուսուցման և կանխատեսման որակի բարելավման ռազմավարությունները տարբերվում են նրանում, թե ինչքանով են դրանք պատրաստված հատուկ խնդիրներին կամ ինտեգրված հատուկ մոդելի ճարտարապետության հետ: Այս աշխատանքի ընթացքում մենք ներկայացնում ենք Knodle-ին, ծրագրային համակարգը, որը վերաբերում է թույլ տվյալների նոտացիաներին, խորը ուսումնասիրության մոդելներին և մեթոդներին թույլ վերահսկված ուսուցման բարելավման համար որպես առանձին,  Այս մոդուլարիզացիան հնարավորություն է տալիս ուսումնասիրության գործընթացին հասանելի լինել գեղեցիկ ինֆորմացիային, ինչպիսիք են տվյալների հավաքածու հատկությունները, հորիստիկ կանոնների համապատասխանությունը կամ խորը ուսումնասիրության մոդելի տարրերը, որոնք Այսպիսով, մեր շրջանակը կարող է ներառել բազմաթիվ ուսումնասիրության մեթոդներ թույլ վերահսկողության բարելավման համար, սկսած այն մեթոդներից, որոնք միայն վերաբերում են կանոնների և արտադրողական դասերի համեմատություններին (անկախ նրանից, թե մեքենայի ուսումնասիրության մոդելը, որը ուսուցանվում է արդյունքում ստացված պիտակներո Մենք ներկայացնում ենք շրջանակի համեմատական պոտենցիալը մի քանի համեմատական իրականացումների արդյունավետության համեմատական օգնությամբ Knodle-ում արդեն հասանելի տվյալների համակարգերի ընտրության վրա:', 'am': 'Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture.  በዚህ ስራ፣ Knodle፣ ደካማ ዳታ ማስታወቂያውን፣ ጥልቅ ትምህርት ሞዴላዎችን እና የደካማ ትምህርት በተለየ፣ በተለየ ሚናሌ ክፍተቶችን ለመጠቀም የሚችሉትን የሶፍትዌር ፍሬማር እናቀርባታለን፡፡ ይህ አካባቢነት የጥልቅ ትምህርት ሞዴል ለመፍትወት የሚጠቀሙትን የጥልቅ ትምህርት ማድረግ ማድረግ ማግኘት የሚችል መረጃዎችን እንደ ዳታ ማዘጋጀት ፊርክስቶችን፣ ለጥልቅ ትምህርት መሠረት ወይም ለትንቢት የሚጠቀሙትን የጥልቅ ትምህርት model ስለዚህም፣ ፍሬማችን የደካማ ትምህርት ማድረግ እና የወጣ ክፍሎች ግንኙነት ብቻ በተመለከተ የሥርዓትና የውጤት ክፍሎች ማቀናቀል ማተማር ሞዴል ለደካማ መረብ ማቀናቀል እና ለደካማ ዳታዎችን ለመጠቀም የሚችሉትን ሥርዓቶች ይዞራል፡፡ በKnodle ውስጥ በተገኘው የዳታ ማረጃ ምርጫዎች ላይ በብዙ የሥልጣን አካባቢ ትክክል እናሳውቀዋለን፡፡', 'bn': 'প্রশিক্ষণ এবং ভবিষ্যতের মান দুর্বল পর্যবেক্ষিত মেশিন শিক্ষা মডেলের মান উন্নতি এবং প্রতিষ্ঠানের কৌশলগুলো ভিন্ন ভিন্ন ভিন্ন ভিন্ন যে কোন নির্দি এই কাজে আমরা কেনোডেলের একটি সফটওয়্যার ফ্রেমের সাথে পরিচয় করিয়ে দিচ্ছি যা দুর্বল তথ্য প্রতিবেদন, গভীর শিক্ষা মডেল এবং দুর্বল প্রশিক্ষণের প্রশিক্ষণে এই বৈশিষ্ট্য প্রশিক্ষণের প্রক্রিয়ার সুনির্দিষ্ট তথ্য যেমন ডাটা সেট বৈশিষ্ট্য, হারিস্টিক নিয়মের মিল, অথবা গভীর শিক্ষা মডেলের মূল্য তাই, আমাদের ফ্রেমের কাঠামো দুর্বল পর্যবেক্ষণের জন্য প্রশিক্ষণের ব্যাপারে প্রযুক্তির মাধ্যমে পরিচালনা করতে পারে, যারা শুধুমাত্র নিয়ম এবং আউটপুট ক্লাসের সম্পর্কের দিকে তাকিয়ে থাকে (ফলা কেনোডেলে ইতোমধ্যে প্রাপ্ত তথ্য নির্বাচনের বিভিন্ন তথ্য নির্বাচনের তুলনার তুলনায় আমরা এই ফ্রেমের বেনম্যাঙ্কিং সম্ভাবনা ব্যান', 'az': "Müəllif və təcrübəsini yaxşılaşdırmaq üçün zəif təşkil edilmiş maşın öyrənmə modellerinin təcrübəsi və təcrübəsinin kalitetini müxtəlif təcrübələrinin müxtəlif təcrübələrinin müəyyən edilməsi və ya müəyyən bir model arhitektürü ilə birlə Bu işdə Knodle'i, zəif verilən məlumatları, derin öyrənmə modelləri və zəif təhsil edilən təhsil müəyyən edilmə metodlarını ayrı, modular komponentlər kimi yaxşılaşdıran bir proqramlı framework ü tanıdırıq. Bu modularizasyon təhsil sürətini təhsil edir, məlumat qurğuları, heyristik qanunların uyğunluğunu, yaxud təhsil üçün istifadə edilən dərin öyrənmə modelinin elementlərini. Beləliklə, qurğumuz zəif gözləmə yollarını yaxşılaşdırmaq üçün çoxlu təhsil metodlarını daxil edə bilər, ancaq qaydaların və çıxış sınıflarının bağlantılarına baxan metodlardan ( müxtəlif etiketlərlə təhsil edilən mašin öyrənmə modelinin bağımsız olaraq), nöral a ğların bağlantısını və zəif etiketli məlumatların bağlantısını Biz frameworklərin benchmarking potensialini Knodle'da hazırlanmış verilən qurğular barəsində çoxlu referens implementasiyonların müqayisədə göstəririk.", 'bs': 'Strategije za poboljšanje kvalitete obuke i predviđanja slabi nadzornih model a učenja strojeva razlikuju se u tome koliko se njih prilagođavaju specifičnom zadatku ili integriraju sa specifičnom modelom arhitekture. U ovom poslu predstavljamo Knodle, softver okvir koji tretira slabe annotacije podataka, duboke modele učenja i metode za poboljšanje slabe nadzorne obuke kao odvojene modularne komponente. Ova modularizacija daje pristup procesu obuke detaljnim informacijama kao što su karakteristike podataka, odgovarajuće heurističkim pravilima, ili elemente dubokog modela učenja koji se konačno koristi za predviđanje. Stoga, naš okvir može obuhvatiti širok niz metoda obuke za poboljšanje slabe nadzore, od metoda koje samo gledaju korelacije pravila i klasa izlaza (nezavisno od model a učenja mašine obučenog s rezultatim etiketama), do onih koji koriste interakciju neuronskih mreža i slabe etiketirane podatke. Ilustrujemo potencijal preglede okvira sa usporedbom provedbe nekoliko referentnih implementacija na izboru seta podataka koji su već dostupni u Knodle-u.', 'ca': "Les estratègies per millorar la qualitat d'entrenament i predicció dels models d'aprenentatge màquinari debidament supervisats varien en la mesura en què estan adaptats a una tasca específica o integrats amb una arquitectura model específica. En aquesta feina introduïm Knodle, un marc de software que tracta anotacions de dades dèbils, models d'aprenentatge profund i mètodes per millorar l'entrenament dèbilment supervisat com components modulars separats. Aquesta modularització dóna al procés d'entrenament accés a informació fina, com les característiques del conjunt de dades, les coincidences de regles heurístiques o els elements del model d'aprenentatge profund que finalment es fan servir per predir. Per tant, el nostre marc pot abarcar una gran varietat de mètodes de formació per millorar la supervisió dèbil, que van des de mètodes que només miren les correlacions de regles i classes de sortida (independentment del model d'aprenentatge màquinari entrenat amb les etiquetes resultants), fins a aquells que aprofiten l'interacció de xarxes neurals i dades dèbilment etiquetades. Illustrem el potencial de comparació del marc amb una comparació de performance de diverses implementacions de referència en una selecció de conjunts de dades que ja estan disponibles a Knodle.", 'cs': 'Strategie pro zlepšení kvality tréninku a predikce slabě dohlížených modelů strojového učení se liší v tom, jak moc jsou přizpůsobeny konkrétnímu úkolu nebo integrovány s konkrétní architekturou modelu. V této práci představujeme Knodle, softwarový framework, který zachází se slabými datovými anotacemi, modely hlubokého učení a metodami zlepšování slabě dohlíženého školení jako samostatné modulární komponenty. Tato modularizace umožňuje tréninkovému procesu přístup k jemně zpracovaným informacím, jako jsou charakteristiky datových sad, shody heuristických pravidel nebo prvky modelu hlubokého učení, které se nakonec používají pro predikci. Náš rámec tedy může zahrnovat širokou škálu tréninkových metod pro zlepšení slabého dohledu, od metod, které se zabývají pouze korelací pravidel a výstupních tříd (nezávisle na modelu strojového učení trénovaném s výslednými štítky), až po metody, které využívají souhry neuronových sítí a slabě označených dat. Benchmarkingový potenciál frameworku ilustrujeme porovnáním výkonnosti několika referenčních implementací na výběru datových sad, které jsou již dostupné v Knodle.', 'et': "Nõrgalt juhitavate masinõppemudelite koolituse ja prognoosimise kvaliteedi parandamise strateegiad erinevad sellest, kui palju need on kohandatud konkreetsele ülesandele või integreeritud konkreetse mudeli arhitektuuriga. Selles töös tutvustame Knodle'i tarkvararaamistikku, mis käsitleb nõrku andmete annotatsioone, sügavõppe mudeleid ja nõrgalt juhendatud koolituse parandamise meetodeid eraldiseisvate modulaarsete komponentidena. See modulariseerimine annab koolitusprotsessile juurdepääsu täpsele teabele, nagu näiteks andmekogumi omadused, heuristiliste reeglite vasted või sügavõppe mudeli elemendid, mida lõpuks kasutatakse prognoosimiseks. Seega võib meie raamistik hõlmata laia valikut koolitusmeetodeid nõrga järelevalve parandamiseks, alates meetoditest, mis vaatavad ainult reeglite ja väljundklasside korrelatsioone (sõltumata masinõppe mudelist, mis on koolitatud tulemuslike märgistustega), kuni meetoditeni, mis kasutavad ära närvivõrkude ja nõrgalt märgistatud andmete vastastikust mõju. Näitame raamistiku võrdlusanalüüsi potentsiaali mitmete võrdlusrakenduste tulemuslikkuse võrdlemisega Knodle'is juba kättesaadavate andmekogumite valikul.", 'fi': 'Strategiat heikosti valvottujen koneoppimismallien koulutuksen ja ennakoinnin laadun parantamiseksi vaihtelevat sen mukaan, kuinka paljon ne räätälöidään tiettyyn tehtävään tai integroidaan tiettyyn malliarkkitehtuuriin. Tässä työssä esittelemme Knodlen, ohjelmistokehyksen, joka käsittelee heikkoja tietomerkintöjä, syväoppimismalleja ja heikosti ohjatun koulutuksen parantamismenetelmiä erillisinä modulaarisina komponentteina. Modulaarisointi antaa koulutusprosessille mahdollisuuden saada hienojakoista tietoa, kuten datajoukon ominaisuuksia, heurististen sääntöjen vastaavuuksia tai ennusteessa käytetyn syväoppimisen mallin elementtejä. Kehyksemme voi näin ollen kattaa laajan valikoiman koulutusmenetelmiä heikon valvonnan parantamiseksi, aina menetelmistä, jotka tarkastelevat vain sääntöjen ja tuotosluokkien korrelaatioita (riippumatta koneoppimismallista, joka on koulutettu tuloksena olevilla etiketeillä), niihin, jotka valjastavat hermoverkkojen ja heikosti merkityn datan vuorovaikutuksen. Kuvaamme viitekehyksen benchmarking-potentiaalia vertailemalla useiden referenssitoteutusten suorituskykyä Knodlessa jo saatavilla olevien aineistojen perusteella.', 'jv': 'Siji aturan kanggo nglanggar aturan karo perusahaan kanggo ngerasai kapan ingkang sampeyan nguasai model sing nyimpen di manut kuwi tindakan segala macem kuwi tindakan. Nang gunggo iki, kéné gunakake knogle, akeh basa sing perusahaan kang angkang data maning, model sing di antara awak dhéwé, lan maneh kanggo ngerasakno nggawe aturan kapungi luwih apik tur ayu, sampeyan modul. module Lakok, nggawe sistem iso ngubah akeh akeh sistem sing beraksi karo perbudhakan langkung wih-wih akeh kudu supaya winih, iso nglanggar wih-wih apik dhéwé kuwi kesempatan karo perusahaan lan kelas sing ditambah (sampek iso nguasai model sing dino awak dhéwé karo etiket sing dadi), iso nglanggar oleh dumaten sing mbutuhak ilegal nggambar alat lan ala Awak dhéwé éntuk nggawe bendhèwèké iso nggawe sistem sing nggawe gerarané karo perusahaan sing nyimpen maning', 'he': 'אסטרטגיות לשפר את איכות האימונים והצפייה של דוגמנים ללימוד מכונות ששולטים בחולשה שונות בכמה הם מתאימים למשימה מסויימת או מושלמים עם ארכיטקטורת דוגמנית מסויימת. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components.  המודולריזציה הזו נותנת לתהליך האימונים גישה למידע מעורפל כמו אופיינים של קבוצת נתונים, התאמות של חוקים היוריסטיים, או אלמנטים של מודל הלימוד העמוק שמשתמשים בסופו של דבר לחזוי. לכן, המסגרת שלנו יכולה לכלול מגוון רחב של שיטות אימונים לשיפור הפיקוח החלש, מגוון משיטות שמסתכלות רק על קשרים של חוקים ושיעורי יציאה אנו מציגים את פוטנציאל השוואה של המסגרת עם שיוואי ביצועים של מספר השימושים התייחסים על בחירה של קבוצות נתונים שכבר זמינות בנודל.', 'ha': "Tayarori na improve wa tsarin mai amfani da bashiri da misãlai masu tsari na mashine mai rauni da aka tsare su yana sãɓã wa inda aka taimaki su zuwa wani aikin da aka ƙayyade ko kuma aka haɗe su da wani matsayi mai ƙayyade. Daga wannan aikin, Muke ƙarfafa Knodle, wata firam mai kwamfyuta wanda ke yi mataimaki ga kunnuwa masu rauni da data, misãlai masu ƙari da amfani da kuma hanyoyin ƙaranci da ake tsara wa tsarin mai rauni kamar ƙanshi na daban-daban. @ info Daga wannan, firam masu iya kamfata wasu hanyõyi masu amfani da wa improve tsarin masu rauni, masu rangi daga metode waɗand a kawai ke dũba zuwa masu husũma na masu tsari na rubutu da fitarwa (masu buɗa ɗe da misalin ayuka da aka sanar da shi na ƙaramata), zuwa waɗanda ke ƙarfafa interplay na taryutan neural da kuma masu rauni da data wanda aka rubũta. Tuna bayyana awon bangẽwa na firam da sami'ar misalin ajiya masu amfani da misalin mutane a kan zaɓen zaɓa ɓen danahattun da ke da yanzu a Knodle.", 'sk': 'Strategije za izboljšanje kakovosti usposabljanja in napovedovanja slabo nadzorovanih modelov strojnega učenja se razlikujejo glede na to, koliko so prilagojeni določeni nalogi ali integrirani z določeno arhitekturo modela. V tem delu predstavljamo Knodle, programski okvir, ki šibke podatkovne opombe, modele globokega učenja in metode za izboljšanje šibkega nadzorovanega usposabljanja obravnava kot ločene modularne komponente. Ta modularizacija omogoča procesu usposabljanja dostop do natančnih informacij, kot so značilnosti nabora podatkov, ujemanja heurističnih pravil ali elementi modela globokega učenja, ki se končno uporablja za napovedovanje. Zato lahko naš okvir zajema širok nabor metod usposabljanja za izboljšanje šibkega nadzora, od metod, ki obravnavajo le korelacije pravil in izhodnih razredov (neodvisno od modela strojnega učenja, usposobljenega z nastalimi oznakami), do metod, ki izkoriščajo interakcijo nevronskih omrežij in šibko označenih podatkov. Potencial primerjalne analize okvira ponazarjamo s primerjavo uspešnosti več referenčnih implementacij na izboru naborov podatkov, ki so že na voljo v Knodle-u.', 'bo': 'དབྱིབས་གཙོ་སློང་རིམ་དང་མཐོང་སྣེ་མཐོང་བའི་མ་ལག་གི་མཐུན་སྣེ་མཐོང་ནུས་ཚོད་རང་ཉིད་སྒྲིག་འཛིན་གྱི་ཐབས་ལམ་ལུགས་དང་མཐོང་སྔོན་སྒྲིག In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. ང་ཚོས་Knodle ནང་དུ་ཡོད་པའི་གཞུང་ཚབ་ལྟ་བུའི་གྲངས་སྒྲིག་བཀོད་མིའི་གྲངས་སྒྲིག་བཀོད་པའི་མཐའ་འཁོར་སྐྱོད་རུང'}
{'en': 'Probing Cross-Modal Representations in Multi-Step Relational Reasoning', 'pt': 'Sondando Representações Multimodais no Raciocínio Relacional de Várias Etapas', 'ar': 'التحقق من التمثيلات عبر الوسائط في التفكير العلائقي متعدد الخطوات', 'es': 'Sondeo de representaciones intermodales en el razonamiento relacional de varios pasos', 'fr': 'Sondage des représentations intermodales dans le raisonnement relationnel en plusieurs étapes', 'ja': '多段階関係推論におけるクロスモーダル表現の探索', 'zh': '于多步骤推理中探跨模态', 'hi': 'मल्टी-स्टेप रिलेशनल रीजनिंग में क्रॉस-मोडल अभ्यावेदन की जांच करना', 'ru': 'Зондирование кросс-модальных представлений в многоступенчатом реляционном мышлении', 'ga': 'Léirithe Trasmhódúla a Scrúdú i Réasúnaíocht Choibhneasta Ilchéimneach', 'ka': 'მრავალური მოდიალური გამოსახულებების გამოყენება', 'el': 'Δοκιμή διασταυρούμενων αναπαραστάσεων σε πολλαπλά στάδια σχετικής λογικής', 'hu': 'Keresztmodális reprezentációk vizsgálata a többlépéses relatív észlelésben', 'it': 'Sondare le rappresentazioni cross-modali nella ragione relazionale multi-step', 'kk': 'Көп қадам қатынаслық себептерінде көптеген модельді таңдау', 'lt': 'Daugiapakopės santykinės priežasties tarpmodulių atstovavimų bandymas', 'mk': 'Probing Cross-Modal Representations in Multi-Step Relational Reasoning', 'ml': 'Multi- Step Relational Reading', 'mt': 'Probar ta’ Rappreżentazzjonijiet Cross-Modali f’Raġunar Relattiv Multi-Pass', 'mn': 'Олон-Step Relational Reason', 'ms': 'Menguji Perwakilan Salib-Modal dalam Pengiraan Hubungan Berbagai Langkah', 'no': 'Prøver krysmodale representasjonar i fleire steg relasjonsretting', 'pl': 'Badanie reprezentacji crossmodalnych w wielostopniowym rozumowaniu relacyjnym', 'ro': 'Proiectarea reprezentărilor cross-modale în raționarea relațională în mai multe etape', 'sr': 'Provjeravanje krstomodalnih predstavljanja u vezanom razlogu višestrukog koraka', 'si': 'ප්\u200dරශ්නය ක්\u200dරොස් මොඩාල් ප්\u200dරතිනිස්ථානය ගොඩක් ස්ටප් සම්බන්ධ කාරණය', 'sv': 'Sondera tvärmodala representationer i relationell resonemang i flera steg', 'so': 'Ka baaraandegista wakiilada iskutallada ee jardiinada kala duduwan', 'ta': 'Multi- Step Relations Reading', 'ur': 'Multi-Step Relational Reasoning میں Cross-Modal Representations', 'uz': 'Name', 'vi': 'KCharselect unicode block name', 'bg': 'Пробване на кръстосаните модални представи в многостепенно относително разсъждаване', 'nl': 'Onderzoek van crossmodale representaties in multi-step relationele redenering', 'hr': 'Provjeravanje krstomodalnih predstavljanja u mnogim koracima veznih razloga', 'de': 'Untersuchung von crossmodalen Repräsentationen in der mehrstufigen Beziehungsberechnung', 'ko': '다단계 관계 추리에서 다중모드적 표시에 대한 탐구', 'da': 'Undersøgelse af tværmodelle repræsentationer i Multi-Step Relational Reasoning', 'fa': 'امتحان نمایش\u200cهای متوسط مدل در دلیل رابطه\u200cهای متوسط قدم\u200cها', 'af': 'Probeer kruismodale voorstellings in Multi-Step Relatiewe Redigering', 'id': 'Probing Cross-Modal Representations in Multi-Step Relational Reasoning', 'sq': 'Duke provuar përfaqësime ndër-modali në arsyetimin e lidhjes me shumë hapa', 'sw': 'Kudhibiti maoni ya Msalaba wa Kusini katika Kusoma hatua nyingi za Kuhusiana', 'am': 'ምርጫዎች', 'az': 'Ã‡oxlu-adÄ±m Ä°liÅŸkil ReasonlarÄ±nda Ã§oxlu Modal Ä°ÅŸkilÉ™ri SÉ™xlama', 'tr': 'Çoklu-Step Görnöşimleri Derjesi Sebäpli', 'bs': 'Provjeravanje krstomodalnih predstavljanja u multikoracijskom odnosu', 'bn': 'প্রতিনিধি বিভিন্ন সংশ্লিষ্ট কারণে ক্রস-মডেল প্রতিনিধি প্রমাণ করা হচ্ছে', 'hy': 'Probing Cross-Modal Representations in Multi-Step Relational Reasoning', 'ca': 'Probar representacions transmòdiques en una raonació relativa a múltiples etapes', 'cs': 'Snímání cross-modálních reprezentací ve vícestupňovém vztahovém odůvodnění', 'et': 'Modaalsete esinduste uurimine mitmeastmelises suhtelises mõistmises', 'fi': 'Modaalisten edustustojen kartoittaminen monivaiheisessa suhteellisessa järkeilyssä', 'he': 'חוקר מייצגים מודיאליים בצעדים רבים בהגיון יחסי', 'sk': 'Sondiranje medmodalnih predstavitev v večstopenjskem relativnem razumevanju', 'jv': 'Mbale représane Kros-modal nang Rehasun Multi-Stap Relational', 'ha': 'KCharselect unicode block name', 'bo': 'Probing Cross-Modal Representations in Multi-Step Relational Reasoning'}
{'en': 'We investigate the  representations  learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new  dataset  of three-image scenes and define a task that requires  reasoning  at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.', 'fr': "Nous étudions les représentations apprises par les modèles de vision et de langage dans des tâches qui nécessitent un raisonnement relationnel. En nous concentrant sur le problème de l'évaluation de la taille relative des objets dans des contextes visuels abstraits, nous analysons le raisonnement en une étape et en deux étapes. Pour ce dernier, nous construisons un nouveau jeu de données de scènes à trois images et définissons une tâche qui nécessite un raisonnement au niveau des images individuelles et entre les images d'une scène. Nous explorons les représentations de modèles apprises à l'aide de classificateurs diagnostiques Nos expériences montrent que les architectures multimodales basées sur des transformateurs préentraînés peuvent effectuer un raisonnement relationnel de haut niveau et sont capables d'apprendre des représentations pour des tâches et des données nouvelles qui sont très différentes de ce qui a été observé lors de la pré-formation.", 'es': 'Investigamos las representaciones aprendidas por los modelos de visión y lenguaje en tareas que requieren razonamiento relacional. Centrándonos en el problema de evaluar el tamaño relativo de los objetos en contextos visuales abstractos, analizamos tanto el razonamiento de un paso como el de dos pasos. Para esto último, construimos un nuevo conjunto de datos de escenas de tres imágenes y definimos una tarea que requiere razonamiento a nivel de las imágenes individuales y a través de las imágenes de una escena. Sondeamos las representaciones de modelos aprendidas mediante clasificadores de diagnóstico. Nuestros experimentos muestran que las arquitecturas basadas en transformadores multimodales previamente entrenadas pueden realizar un razonamiento relacional de mayor nivel y pueden aprender representaciones de tareas y datos novedosos que son muy diferentes de lo que se vio en la capacitación previa.', 'ar': 'نحن نحقق في التمثيلات التي تعلمتها نماذج الرؤية واللغة في المهام التي تتطلب التفكير العلائقي. بالتركيز على مشكلة تقييم الحجم النسبي للأشياء في السياقات المرئية المجردة ، نقوم بتحليل كل من التفكير المكون من خطوة واحدة وخطوتين. بالنسبة للأخير ، نقوم ببناء مجموعة بيانات جديدة من ثلاث مشاهد للصور وتحديد مهمة تتطلب التفكير على مستوى الصور الفردية وعبر الصور في المشهد. نحن نفحص تمثيلات النموذج المكتسبة باستخدام المصنفات التشخيصية. تُظهر تجاربنا أن البنى القائمة على المحولات متعددة الوسائط المدربة مسبقًا يمكن أن تؤدي تفكيرًا علائقيًا عالي المستوى ، وتكون قادرة على تعلم تمثيلات للمهام الجديدة والبيانات التي تختلف تمامًا عما شوهد في التدريب المسبق.', 'pt': 'Investigamos as representações aprendidas por modelos de visão e linguagem em tarefas que requerem raciocínio relacional. Concentrando-se no problema de avaliar o tamanho relativo de objetos em contextos visuais abstratos, analisamos o raciocínio de uma e duas etapas. Para este último, construímos um novo conjunto de dados de cenas de três imagens e definimos uma tarefa que requer raciocínio no nível das imagens individuais e entre imagens em uma cena. Analisamos as representações do modelo aprendido usando classificadores de diagnóstico. Nossos experimentos mostram que arquiteturas baseadas em transformadores multimodais pré-treinados podem executar raciocínio relacional de alto nível e são capazes de aprender representações para novas tarefas e dados que são muito diferentes do que foi visto no pré-treinamento.', 'zh': '我们研究了视听和言语模样在要推理的事务中学到的。 注于抽象视中评对大小,析一步两步推理。 后来,我们构建了一个新三图像场景数据集,并定义了一个职务,该在场景中的单个图像和图像等级上推理。 吾以诊分类器测学之形。 臣等实验明,先训之基于多模态转换器架构可以行更高层次之理,而能习殊新之任数。', 'hi': 'हम उन कार्यों में दृष्टि और भाषा मॉडल द्वारा सीखे गए अभ्यावेदनों की जांच करते हैं जिनके लिए संबंधपरक तर्क की आवश्यकता होती है। अमूर्त दृश्य संदर्भों में वस्तुओं के सापेक्ष आकार का आकलन करने की समस्या पर ध्यान केंद्रित करते हुए, हम एक-चरण और दो-चरणीय तर्क दोनों का विश्लेषण करते हैं। उत्तरार्द्ध के लिए, हम तीन-छवि दृश्यों का एक नया डेटासेट बनाते हैं और एक ऐसे कार्य को परिभाषित करते हैं जिसके लिए व्यक्तिगत छवियों के स्तर पर और एक दृश्य में छवियों में तर्क की आवश्यकता होती है। हम नैदानिक क्लासिफायरका उपयोग करके सीखे गए मॉडल अभ्यावेदन की जांच करते हैं। हमारे प्रयोगों से पता चलता है कि प्रीट्रेन्ड मल्टीमॉडल ट्रांसफॉर्मर-आधारित आर्किटेक्चर उच्च-स्तरीय रिलेशनल तर्क कर सकते हैं, और उपन्यास कार्यों और डेटा के लिए अभ्यावेदन सीखने में सक्षम हैं जो प्रीट्रेनिंग में जो देखा गया था उससे बहुत अलग हैं।', 'ja': '私たちは、人間関係の推論を必要とするタスクにおいて、ビジョンと言語モデルによって学習された表現を調査します。抽象的な視覚的文脈における物体の相対的な大きさを評価する問題に焦点を当て、一段階と二段階の推論の両方を分析する。後者では、3つの画像シーンの新しいデータセットを構築し、個々の画像のレベルとシーン内の画像間で推論を必要とするタスクを定義します。診断分類子を使用して学習されたモデル表現を探索します。私たちの実験では、事前に訓練されたマルチモーダル変圧器ベースのアーキテクチャは、より高いレベルの関係推論を実行でき、事前訓練で見られたものとは大きく異なる新規のタスクとデータの表現を学ぶことができることが示されています。', 'ru': 'Мы исследуем представления, полученные с помощью видения и языковых моделей, в задачах, требующих реляционного мышления. Ориентируясь на задачу оценки относительных размеров объектов в абстрактных визуальных контекстах, мы анализируем как одношаговое, так и двухшаговое мышление. Для последнего мы строим новый набор данных из трех изображений сцен и определяем задачу, которая требует рассуждения на уровне отдельных образов и между изображениями в сцене. Мы исследуем полученные представления модели с помощью диагностических классификаторов. Наши эксперименты показывают, что предварительно обученные мультимодальные архитектуры на основе трансформаторов могут выполнять реляционное мышление более высокого уровня и могут изучать представления для новых задач и данных, которые сильно отличаются от того, что наблюдалось при предварительном обучении.', 'ga': 'Fiosraíonn muid na hléirithe a d’fhoghlaimíonn fís agus samhlacha teanga i dtascanna a éilíonn réasúnaíocht choibhneasta. Ag díriú ar an bhfadhb a bhaineann le measúnú a dhéanamh ar mhéid choibhneasta rudaí i gcomhthéacsanna teibí amhairc, déanaimid anailís ar réasúnaíocht aonchéime agus dhá chéim. Maidir leis an dara ceann, tógaimid tacar sonraí nua de radhairc trí-íomhá agus sainímid tasc a éilíonn réasúnú ag leibhéal na n-íomhánna aonair agus trasna na n-íomhánna i radharc. Déanaimid iniúchadh ar léiriú na samhla foghlamtha ag baint úsáide as aicmitheoirí diagnóiseacha. Léiríonn ár dturgnaimh gur féidir le hailtireachtaí ilmhódacha atá bunaithe ar chlaochladán réamhoiliúint réasúnaíocht choibhneasta ardleibhéil a dhéanamh, agus go bhfuil siad in ann uiríll a fhoghlaim le haghaidh tascanna agus sonraí núíosacha atá an-difriúil ón méid a chonacthas i réamhoiliúint.', 'ka': 'ჩვენ განსხვავებთ მონაცემებები, რომლებიც შეგვიძლია დაკავშირებული პარამეტრებების შესახებ და ენის მოდელების შესახებ. აბსტრაქტური ვიზუალური კონტექსტში პრობლემების შესაბამისი ზომის შესაბამისი პრობლემებზე, ჩვენ ანალიზაცით ერთ-კვადი და ორ-კვადი პარამენტი. შემდეგ, ჩვენ სამი გამოსახულების ახალი მონაცემების კონფიგურაციას შევქმნით და განსახულებთ რაოდენობა, რომელიც განსახულებელია განსახულების დონეზე და გამოსახულების დონეში. ჩვენ დავიწყებთ მოდელური გამოყენება დიაგონტიკური კლასიფიკაციების გამოყენებით. ჩვენი ექსპერიმენტები აჩვენებენ, რომ მლიტიმოდიალური ტრანფორმენტების აქტიქტიქტურები შეუძლიათ გავაკეთოთ უფრო მეტი დონე შესაბამისი პარამენტი, და შეუძლიათ ვისწავლოთ პრომენტური და', 'hu': 'Megvizsgáljuk a látásmód és a nyelvi modellek által tanult reprezentációkat olyan feladatokban, amelyek kapcsolati érvelést igényelnek. Az objektumok relatív méretének értékelésének problémájára összpontosítva absztrakt vizuális kontextusokban, egylépéses és kétlépéses érvelést egyaránt elemzünk. Ez utóbbi esetében háromképes jelenetekből álló új adatkészletet építünk fel, és olyan feladatot határozunk meg, amely az egyes képek szintjén és egy jelenet képein keresztül érvelést igényel. Diagnosztikai osztályozók segítségével vizsgáljuk a tanult modell reprezentációit. Kísérleteink azt mutatják, hogy az előkészített multimodális transzformátor alapú architektúrák képesek magasabb szintű relációs érvelést végezni, és képesek olyan új feladatok és adatok reprezentációit tanulni, amelyek nagyon különböznek attól, amit az előkészítés során tapasztaltak.', 'el': 'Ερευνούμε τις αναπαραστάσεις που μαθαίνονται από το όραμα και τα γλωσσικά μοντέλα σε εργασίες που απαιτούν σχεσιακό συλλογισμό. Εστιάζοντας στο πρόβλημα της αξιολόγησης του σχετικού μεγέθους των αντικειμένων σε αφηρημένα οπτικά πλαίσια, αναλύουμε τη λογική ενός και δύο βημάτων. Για το τελευταίο, κατασκευάζουμε ένα νέο σύνολο δεδομένων σκηνών τριών εικόνων και καθορίζουμε μια εργασία που απαιτεί συλλογισμό στο επίπεδο των μεμονωμένων εικόνων και μεταξύ των εικόνων σε μια σκηνή. Εξετάζουμε τις μαθημένες αναπαραστάσεις μοντέλων χρησιμοποιώντας διαγνωστικούς ταξινομητές. Τα πειράματά μας δείχνουν ότι οι προ-εκπαιδευμένες πολυπροπικές αρχιτεκτονικές με βάση μετασχηματιστή μπορούν να εκτελέσουν ανώτερο επίπεδο σχεσιακής σκέψης, και είναι σε θέση να μάθουν αναπαραστάσεις για νέες εργασίες και δεδομένα που είναι πολύ διαφορετικές από ό, τι παρατηρήθηκε στην προεπιλογή.', 'it': "Investighiamo le rappresentazioni apprese dai modelli di visione e linguaggio in compiti che richiedono ragionamento relazionale. Concentrandoci sul problema della valutazione della dimensione relativa degli oggetti in contesti visivi astratti, analizziamo sia il ragionamento in uno stadio che in due fasi. Per quest'ultimo, costruiamo un nuovo dataset di scene a tre immagini e definiamo un compito che richiede ragionamenti a livello delle singole immagini e attraverso le immagini in una scena. Sondiamo le rappresentazioni dei modelli appresi utilizzando classificatori diagnostici. I nostri esperimenti mostrano che architetture multimodali pre-addestrate basate su trasformatori possono eseguire ragionamenti relazionali di livello superiore, e sono in grado di imparare rappresentazioni di nuovi compiti e dati che sono molto diversi da quanto è stato visto nel pre-training.", 'kk': 'Біз көрініс мен тіл үлгілерін үйренген тапсырмалардың қатынастық түсініктемелерін зерттейміз. Абстракты визуалдық контексттерде нысандардың қатынастық өлшемін оқу мәселесіне қарсы болып, бір қадам мен екі қадам сезімін анализирақ. Соңғылардың үш кескіндегі жаңа деректер жиынын құрып, әрбір кескіндердің деңгейінде және кескіндердің арасындағы бақылау керек тапсырманы анықтаймыз. Біз диагностикалық классификацияларды қолдану үлгілерін тексереміз. Біздің тәжірибеміздің көп модельді түрлендіруші архитектураларының көп деңгейіндегі қатынастық түсініктері жоғары деңгейінде жұмыс істеуге болады, және өзгертілген жаңа тапсырмалар мен деректерінің тү', 'lt': 'Mes tiriame vizijos ir kalbos modelių įgytus atstovavimus užduotyse, kurioms reikalingas santykinis pagrindimas. Pagrindinę problem ą vertinant santykinį objektų dydį abstrakčiose vizualinėse aplinkybėse analizuojame tiek vieno, tiek dviejų etapų pagrįstumą. Pastaruoju atveju sukuriame naują trijų vaizdų scenų duomenų rinkinį ir apibrėžiame užduotį, kuri reikalauja pagrįsti atskirų vaizdų lygiu ir įvairiuose vaizduose scenoje. Ištiriame išmoktus modelius naudojant diagnostinius klasifikatorius. Mūsų eksperimentai rodo, kad iš anksto parengtos daugiarūšio modelio transformatorių grindžiamos architektūros gali atlikti aukštesnio lygio santykinius motyvus ir sugebėti išmokti naujų užduočių ir duomenų, kurie labai skiriasi nuo to, ką buvo matyti iš anksto rengiant mokymus, atstovavimus.', 'mk': 'Ги истражуваме претставувањата научени со визија и јазички модели во задачите кои бараат релативно размислување. Кога се фокусираме на проблемот со проценката на релативната големина на објектите во апстрактни визуелни контексти, анализираме размислување во еден и два чекори. За последните, конструираме нов набор на податоци од три слики и дефинираме задача која бара размислување на нивото на индивидуалните слики и преку слики на една сцена. Ги проверуваме научените модели користејќи дијагностички класификатори. Нашите експерименти покажуваат дека претренираните мултимодилни трансформаторски архитектури можат да извршат врска на повисоко ниво размислување и можат да научат претставувања за нови задачи и податоци кои се многу различни од она што беше видено во претренирањето.', 'ml': 'കാഴ്ചകളും ഭാഷ മോഡലുകളും പഠിച്ച പ്രതിനിധികളെ ഞങ്ങള്\u200d അന്വേഷിക്കുന്നു. ബന്ധപൂര്\u200dവ്വം കാരണങ്ങള്\u200d ആവശ്യമുള് അബ്ബ്രാക്ട്രാക്റ്റ് കാഴ്ചകളില്\u200d വസ്തുക്കളുടെ വലിപ്പത്തിന്റെ വലിപ്പം വിശദീകരിക്കുന്നതിന്റെ പ്രശ്നത്തില്\u200d ശ്രദ്ധിച അവസാനം, നമ്മള്\u200d മൂന്നു ഇമേജ് സീനുകളുടെ പുതിയ ഡാറ്റാസേറ്റ് നിര്\u200dമ്മിക്കുകയും, ഒരു ജോലി നിര്\u200dണ്ണയിക്കുകയും ചെയ്യുന്നു. അത് സ്വക ഞങ്ങള്\u200d പഠിച്ച മോഡല്\u200d പ്രതിനിധികളെ പരിശോധിക്കുക നമ്മുടെ പരീക്ഷണങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നു പല്ലിമോഡാല്\u200d മാറ്റങ്ങള്\u200d അടിസ്ഥാനമാക്കിയിരിക്കുന്ന സ്ഥാനങ്ങള്\u200dക്ക് ഉയരത്തിലെ ബന്ധപൂര്\u200dണ്', 'mn': 'Бид харилцааны урьдчилан шаардлагатай зүйлсийн даалгаврууд болон хэл загвараар сурсан илтгэлийг судалж байна. Объектуудын харьцаатай хэмжээг abstract visual нөхцөлд тодорхойлох асуудалд бид нэг алхам болон хоёр алхам шинжилгээ хийдэг. Сүүлийн үед бид гурван зураг хувилбарын шинэ өгөгдлийн хэлбэрийг бүтээж, хувилбарын хэмжээнд бодлого шаардлагатай ажил тодорхойлдог. Бид шинжлэх ухааны хэлбэрийг ашиглан сурсан загварын үзүүлэлтийг судалж байна. Бидний туршилтууд олон төрлийн шилжүүлэгчийн архитектурууд өндөр төрлийн харилцааны урьдчилан хийж чадна гэдгийг харуулж чадна. Мөн урьдчилан харагдаж байгаа шинэ ажил, өгөгдлийг сурах боломжтой.', 'no': 'Vi undersøker representasjonane lærte av vising og språk-modeller i oppgåver som krev relasjonell motivering. Fokuserer vi på problemet for å vurdere relativt storleik på objektar i abstrakt visuelle kontekstar, så analyserer vi både ein steg og to steg. For dei siste, konstruerer vi ei ny dataset med tre bilete scenar og definerer ei oppgåve som krev å forstørre på nivået av dei individuelle bileta og gjennom bileta i eit scene. Vi prøver dei lærte modellerepresentasjonane med diagnostiske klassifikatorar. Eksperimentane våre viser at multimodal transformeringsarkitekturar kan utføre høgare nivå relasjonell rasjon, og kan lære representasjonar for novel oppgåver og data som er svært ulike frå det som er vist i trekking.', 'pl': 'Badamy reprezentacje nauczane przez modele wizji i językowe w zadaniach wymagających rozumowania relacyjnego. Koncentrując się na problemie oceny względnej wielkości obiektów w abstrakcyjnych kontekstach wizualnych, analizujemy zarówno jednostopniowe, jak i dwustopniowe rozumowanie. Dla tych ostatnich budujemy nowy zestaw danych scen trójobrazowych i definiujemy zadanie, które wymaga rozumowania na poziomie poszczególnych obrazów i między obrazami w scenie. Badamy nauczone reprezentacje modeli za pomocą klasyfikatorów diagnostycznych. Nasze eksperymenty pokazują, że wstępnie trenowane multimodalne architektury oparte na transformatorach mogą wykonywać rozumowanie relacyjne wyższego poziomu i są w stanie uczyć się reprezentacji dla nowych zadań i danych, które są bardzo różne od tego, co widziano w treningu wstępnym.', 'ro': 'Investigăm reprezentările învățate de viziune și modele lingvistice în sarcini care necesită raționament relațional. Concentrandu-ne pe problema evaluării dimensiunii relative a obiectelor în contexte vizuale abstracte, analizăm atât raționamentul într-un singur pas, cât și cel în doi pași. Pentru aceasta din urmă, construim un nou set de date de scene cu trei imagini și definim o sarcină care necesită raționament la nivelul imaginilor individuale și peste imagini dintr-o scenă. Sondăm reprezentările modelului învăţat folosind clasificatoare de diagnosticare. Experimentele noastre arată că arhitecturile multimodale pre-instruite bazate pe transformatori pot efectua raționamente relaționale de nivel superior și sunt capabile să învețe reprezentări pentru sarcini și date noi care sunt foarte diferite de ceea ce a fost văzut în pre-instruire.', 'sr': 'Istražujemo predstave naučene vizijskim i jezičkim modelima u zadatkima koji zahtevaju vezano razumljivanje. Fokusirajući se na problem procjene relativne veličine objekata u apstraktivnim vizualnim kontekstima, analiziramo i jednu korak i dva koraka razgovora. Za poslednje, izgradimo novi set podataka o scenama tri slike i definišemo zadatak koji zahteva razmišljanje na nivou individualnih slika i preko slika na sceni. Provjeravamo naučene predstave modela koristeći dijagnostičke klasifikacije. Naši eksperimenti pokazuju da pretkivne multimodalne arhitekture bazirane na transformaciji mogu izvršiti povezane razine na višem nivou i da mogu naučiti predstave za nove zadatke i podatke koje su veoma različite od onoga što je viđeno u pretkivanju.', 'ms': 'Kami menyelidiki perwakilan yang dipelajari oleh penglihatan dan model bahasa dalam tugas yang memerlukan alasan relatif. Berfokus pada masalah penilaian saiz relatif objek dalam konteks visual abstrak, kita menganalisis satu-langkah dan dua-langkah alasan. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene.  We probe the learned model representations using diagnostic classifiers.  Eksperimen kami menunjukkan bahawa arkitektur berasaskan pengubah multimodal yang dilatih dahulu boleh melakukan alasan relatif tinggi, dan mampu belajar perwakilan untuk tugas baru dan data yang sangat berbeza dari apa yang dilihat dalam latihan dahulu.', 'si': 'අපි දර්ශනය සහ භාෂා මොඩල් වලින් ඉගෙන ගත්ත ප්\u200dරතිනිධානය පරීක්ෂණය කරනවා ඒ වැඩේ සම්බන්ධ ප්\u200dරත ප්\u200dරශ්නයක් විශ්වාස කරන්නේ ප්\u200dරශ්නයක් විශ්වාස කරන්නේ ප්\u200dරශ්නයක් විශ්වාස කරන්න, අපි ප්\u200dරශ්නයක් එක පැත්ත සහ පැත් අන්තිමට, අපි පින්තූර තුන් පින්තූර ස්ථානයක් නිර්මාණය කරනවා වගේම පින්තූර තුන් පින්තූර ස්ථානයක් සහ පින්තූර ව අපි පරීක්ෂණය කරන්නේ ඉගෙන ගත්ත මොඩල් ප්\u200dරතිනිධානය ප්\u200dරතිනිධානය ප්\u200dරයෝජනය කරන්න. අපේ පරීක්ෂණය පෙන්වන්න පුළුවන් විදිහට ප්\u200dරමාණ විදිහට ස්ථාපනය වෙන්න පුළුවන් විදිහට වඩා ස්ථාපනය සම්බන්ධ විදිහට ප්\u200dරමාණය කරන්න', 'so': 'Waxaannu baaraynaa noocyada muuqashada iyo tusaalaha afka lagu baray shaqada u baahan yahay sabab xiriir ah. Iska fiirsanaynaa dhibaatada qiimeynta qiyaastii la xiriira alaabta aragga ee ka mid ah, waxaynu baaraynaa sababta hal qadood iyo laba qadood. Marka ugu dambeyso, waxaynu dhisnaa sawir cusub oo saddex sawir ah, waxaana qoraynaa shaqo looga baahan yahay si looga hadlo darajada sawirada gaarka ah iyo sawirada gaarka ah. Waxaynu tijaabin karnaa noocyada barashada oo isticmaalaya fasaxyada yaqaanada. Imtixaanadayada waxay muuqan karaan in layaasha beddelka oo badan lagu soo hor jeeday ay ay sameyn karaan sababaha aad u sarreeya, waxayna baran karaan noocyo ka mid ah shaqooyinka warqada iyo macluumaadyo aad u kala duwan yihiin waxa lagu arkay wakhtiga hore.', 'mt': 'Aħna ninvestigaw ir-rappreżentazzjonijiet imgħallma permezz ta’ viżjoni u mudelli lingwistiċi f’kompiti li jeħtieġu raġunament relattiv. Filwaqt li niffokaw fuq il-problema tal-valutazzjoni tad-daqs relattiv tal-oġġetti f’kuntesti viżwali astratti, nagħmlu analiżi kemm tar-raġunament f’pass wieħed kif ukoll ta’ żewġ passi. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene.  Aħna nistudjaw ir-rappreżentazzjonijiet tal-mudell imgħallem bl-użu ta’ klassifikaturi dijanjostiċi. L-esperimenti tagħna juru li arkitetturi multimodali mħarrġa minn qabel ibbażati fuq trasformaturi jistgħu jwettqu raġunament relattiv ta’ livell ogħla, u jistgħu jitgħallmu rappreżentazzjonijiet għal kompiti u dejta ġodda li huma differenti ħafna minn dak li deher fit-taħriġ minn qabel.', 'sv': 'Vi undersöker de representationer som lärts av visioner och språkmodeller i uppgifter som kräver relationellt resonemang. Med fokus på problemet med att bedöma objektens relativa storlek i abstrakta visuella sammanhang analyserar vi både ett- och tvåstegs resonemang. För det senare konstruerar vi en ny datauppsättning av tre-bildsscener och definierar en uppgift som kräver resonemang på nivå av enskilda bilder och över bilder i en scen. Vi undersöker de lärda modellrepresentationerna med hjälp av diagnostiska klassificerare. Våra experiment visar att förövade multimodala transformatorbaserade arkitekturer kan utföra relationella resonemang på högre nivå, och kan lära sig representationer för nya uppgifter och data som skiljer sig mycket från vad som sågs i förövande.', 'ta': 'நாம் தொடர்பு காரணங்கள் தேவைப்படும் பணிகளில் பார்வையும் மொழி மாதிரிகளாலும் கற்றுக் கொண்டிருக்க நாம் ஒரு படி மற்றும் இரண்டு படி காரணங்களையும் ஆய்வு செய்கிறோம். அடுத்தத்திற்கு, நாம் மூன்று பிம்பத்தின் காட்சிகளின் புதிய தகவல் அமைப்பை உருவாக்கி ஒரு செயலை வரையறுக்கிறோம். இது தனிப்பட்ட உருவங்களி நாங்கள் கண்டறிவு வகுப்பாளர்களை பயன்படுத்தி கற்ற மாதிரி பிரதிநிதிகளை கண்டறியும். நம்முடைய சோதனைகள் பல மாற்றங்களை மாற்றும் அடிப்படையிலுள்ள அடிப்படைகளை மாற்றி வைத்துள்ளார்கள் என்பதை காட்டுகிறது அதிக நிலையில் தொடர்பு காரணத்தை ச', 'ur': 'ہم دیکھنے اور زبان مدل کے ذریعے سیکھے ہوئے نمونے کی تحقیق کرتے ہیں جو کاموں میں رابطہ اختلاف کی ضرورت رکھتے ہیں. ہم ایک قدم اور دو قدم رابطہ کرنے کے مشکل پر مشکل کی نسبت سائز کی آزمائش کریں گے۔ آخرین کے لئے ہم تین تصاویر صحیفوں کے ایک نئی ڈیٹ سٹ بناتے ہیں اور ایک کام کی تعریف کرتے ہیں جو شخصی تصاویروں کے سطح اور تصاویروں کے مختلف سطح میں بحث کی ضرورت ہے۔ ہم نے سیکھا ہوا موڈل کی نمونہ کی تصدیق کرتی ہیں جو ڈاگنٹیک کلاسیفوں کے مطابق استعمال کرتے ہیں۔ ہمارے آزمائش دکھاتے ہیں کہ بہت سی موڈال تغییر دینے والی معماری عمارتیں بالاتر سطح کے ارتباط منطق کر سکتی ہیں، اور وہ نئی کاموں اور ڈیٹوں کے لئے نمونات سکھا سکتے ہیں جو ان چیزوں سے بہت مختلف ہیں جن کو پہلے دکھایا گیا تھا۔', 'uz': "Biz tashkilotlarda ko'rinish va tillar modellari bilan o'rganish natijalarini o'rganamiz, murakkab sabablar kerak. Biz obʼektlarning qiymatlarini abstract ko'rinish muvaffaqiyatlariga qiymatish muammolari bilan bir qadam va ikki qadam sabablarini bajaramiz. Keyingi uchun biz uch rasm uslublarining yangi maʼlumotlar tarkibini yaratib, va bir vazifani aniqlash kerak. Biz shaxsiy rasmlarning darajada va ko'pchilik rasmlarning hamma darajada murojaat qilish kerak. Biz o'rganilgan modellarni diagnostic klassiplarini ishlatish mumkin. Bizning imtiyozlarimizni ko'pchilik o'zgartirish asosiy maktablari eng darajada munosabatlarni bajarishi mumkin, va yangi vazifalar va maʼlumotlar o'rganish mumkin. Ko'rib chiqishda ko'rinadigan narsalardan juda ajratilgan narsalar va ma'lumotlarni o'rganish mumkin.", 'vi': 'Chúng tôi điều tra các biểu tượng được học qua các mô hình ngôn ngữ trong các công việc cần lý lẽ liên quan. Tập trung vào vấn đề đánh giá kích thước tương đối của vật thể trong cấu hình ảnh trừu tượng, chúng ta phân tích cả phương trình một bước và hai bước. Với vế sau, chúng tôi xây dựng một tập tin mới về các hiện trường ba ảnh và xác định một nhiệm vụ cần phải lập trình cho mức độ của các ảnh cá nhân và hình ảnh trong một cảnh. Chúng tôi đã dò các mô hình học sử dụng phân loại chẩn đoán. Những thí nghiệm của chúng ta cho thấy các kiến trúc đa chiều được cấu trúc có thể đưa ra quan hệ cấp cao hơn, và có thể học các biểu tượng cho các công việc mới và dữ liệu hoàn to àn khác với những gì đã thấy trong quá trình chờ đợi.', 'bg': 'Проучваме образите, научени от визионните и езиковите модели в задачи, които изискват релационно разсъждаване. Фокусирайки се върху проблема за оценяване на относителния размер на обектите в абстрактни визуални контексти, анализираме едностепенно и двустепенно разсъждение. За последното конструираме нов набор от данни от триобразни сцени и дефинираме задача, която изисква разсъждаване на нивото на отделните изображения и през изображенията в дадена сцена. Проучваме научените модели с помощта на диагностични класификатори. Нашите експерименти показват, че предварително обучените мултимодални трансформаторни архитектури могат да изпълняват релационни разсъждения на по-високо ниво и са в състояние да научат представяне на нови задачи и данни, които са много различни от това, което се вижда в предтренирането.', 'da': 'Vi undersøger de repræsentationer, visioner og sprogmodeller lærer i opgaver, der kræver relationel ræsonnement. Med fokus på problemet med at vurdere objekternes relative størrelse i abstrakte visuelle sammenhænge analyserer vi både et- og to-trins ræsonnement. For sidstnævnte konstruerer vi et nyt datasæt af tre-billedscener og definerer en opgave, der kræver ræsonnement på niveau for de enkelte billeder og på tværs af billeder i en scene. Vi undersøger de lærte model repræsentationer ved hjælp af diagnostiske klassifikationer. Vores eksperimenter viser, at forudtrænede multimodale transformer-baserede arkitekturer kan udføre relationelle ræsonnementer på højere niveau, og er i stand til at lære repræsentationer for nye opgaver og data, der er meget forskellige fra, hvad der blev set i forudtræning.', 'nl': 'We onderzoeken de representaties geleerd door visie- en taalmodellen in taken die relationeel redeneren vereisen. We richten ons op het probleem van het beoordelen van de relatieve grootte van objecten in abstracte visuele contexten, analyseren zowel one-step als two-step redenering. Voor dit laatste construeren we een nieuwe dataset van drie-beeldscènes en definiëren we een taak die redenering vereist op het niveau van de individuele beelden en tussen beelden in een scène. We onderzoeken de geleerde modellrepresentaties met behulp van diagnostische classificatoren. Onze experimenten tonen aan dat voorgetrainde multimodale transformatorgebaseerde architecturen relationeel redeneren op hoger niveau kunnen uitvoeren en representaties kunnen leren voor nieuwe taken en gegevens die sterk verschillen van wat werd gezien in pretraining.', 'hr': 'Istražujemo predstave naučene vizijskim i jezičkim modelima u zadatkima koji zahtijevaju vezano razumjevanje. Fokusirajući se na problem procjene relativne veličine objekata u apstraktivnim vizualnim kontekstima, analiziramo i jednokorak i dvokorak razumljivanja. Za sljedeće, izgradimo novi set podataka o scenama tri slike i definiramo zadatak koji zahtijeva razmišljanje na razini pojedinačnih slika i preko slika na sceni. Istražujemo naučene predstave modela koristeći dijagnostičke klasifikacije. Naši eksperimenti pokazuju da pretkivne multimodalne arhitekture bazirane na transformaciji mogu izvršiti povezane razine na višem nivou, i mogu naučiti zastupanje za nove zadatke i podatke koje su veoma različite od onoga što je vidjelo u pretkivanju.', 'de': 'Wir untersuchen die Darstellungen, die Vision und Sprachmodelle in Aufgaben erlernen, die relationales Denken erfordern. Wir konzentrieren uns auf das Problem der Beurteilung der relativen Größe von Objekten in abstrakten visuellen Kontexten und analysieren sowohl ein- als auch zweistufiges Denken. Für letztere konstruieren wir einen neuen Datensatz von Drei-Bild-Szenen und definieren eine Aufgabe, die auf Ebene der einzelnen Bilder und über Bilder in einer Szene nachdenken muss. Wir untersuchen die erlernten Modellrepräsentationen mit diagnostischen Klassifikatoren. Unsere Experimente zeigen, dass prätrainierte multimodale Transformatorarchitekturen ein höheres relationales Denken ausführen können und Repräsentationen für neue Aufgaben und Daten lernen können, die sich sehr von dem unterscheiden, was beim Vortraining gesehen wurde.', 'fa': 'ما توسط نمایش\u200cهای دید و مدل\u200cهای زبان یاد گرفته شده را تحقیق می\u200cکنیم که به دلیل رابطه نیاز دارند. با توجه به مشکل ارزیابی اندازه نسبت به اشیاء در موقعیتهای تصویر مطلق، هر دو مرحله یک مرحله و دو مرحله را تحلیل می کنیم. برای آخرین، ما یک مجموعه داده\u200cهای جدید از صحنه\u200cهای سه تصویر ساخته می\u200cکنیم و یک کار را تعریف می\u200cکنیم که نیاز به منطقی در سطح تصویر فردی و در سطح تصویر فردی در صحنه است. ما نمایش\u200cهای مدل یاد گرفته را با استفاده از طریق\u200cهای تشخیص تحقیق می\u200cکنیم. آزمایشات ما نشان می دهند که معماری بسیار متغیر متغیر مدال\u200cهای زیادی می\u200cتوانند دلیل ارتباطی بالاتر را انجام دهند، و می\u200cتوانند نمایش\u200cدهندگان برای وظیفه\u200cهای رمانی و داده\u200cها یاد بگیرند که بسیار متفاوت هستند از آنچه در پیش\u200cگیری دیده شده است.', 'id': 'Kami menyelidiki representati yang belajar oleh penglihatan dan model bahasa dalam tugas yang membutuhkan alasan relatif. Fokus pada masalah penilaian ukuran relatif objek dalam konteks visual abstrak, kami menganalisis alasan satu langkah dan dua langkah. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene.  Kami memeriksa representati model belajar menggunakan klasifikasi diagnostik. Eksperimen kami menunjukkan bahwa arsitektur multimodal berdasarkan transformator terlatih dapat melakukan alasan relatif tingkat lebih tinggi, dan dapat belajar representation untuk tugas dan data baru yang sangat berbeda dari apa yang terlihat dalam pelatihan terlatih.', 'tr': 'Biz g철rn체힊 we dil nusgalaryndan 철wrenen suratlaryny g철r채 g철rn체힊 seb채plere gerek zadlarda 챌ykar첵arys. Abstrakt g철rn체힊 durumlarda zady흫 relativ 철l챌체sini 챌철zmek meselesine 체ns berip, we hem bir ad캇m hem iki ad캇m razylygyny 챌철z체r첵채ris. So흫ky 체챌in 체챌 surat sahypalaryny흫 t채ze bir veri setirini in 힊a edip, bir g철rn체힊 sahypalary흫 we suratlary흫 arasynda d체힊체nmek gereken zady takykla첵arys. Biz dijagnostik klasifikat철rleri kullanarak 철휓renmi힊 modelleri tahmin ediyoruz. Bizi흫 deneylerimiz 철흫체nden 철r채n modal transformer arhitekturlarymyz 첵okary derejede g철rn체힊 seb채plerini edip biler we 철n체nde g철r체len t채zeliklerden 철r채n farkl캇 힊ekilleri 철wrenip biler.', 'sq': 'Ne hetojmë përfaqësimet e mësuara nga vizioni dhe modelet gjuhësore në detyra që kërkojnë arsyetim marrëdhënies. Duke u përqëndruar në problem in e vlerësimit të madhësisë relative të objekteve në kontekste abstrakte vizuale, ne analizojmë si arsyetimin me një hap ashtu edhe dy hapa. Për të fundit, ne ndërtojmë një grup të ri të dhënash me tre imazhe dhe përcaktojmë një detyrë që kërkon arsyetim në nivelin e imazheve individuale dhe nëpër imazhe në një skenë. We probe the learned model representations using diagnostic classifiers.  Eksperimentet tona tregojnë se arkitekturat multimodale të trajnuara me bazë në transformues mund të kryejnë arsyetimin e nivelit të lartë të marrëdhënieve dhe janë në gjendje të mësojnë përfaqësime për detyra dhe të dhëna të reja që janë shumë të ndryshme nga ajo që shihej në paratrajnimin.', 'af': "Ons ondersoek die voorstellings wat deur visie en taal modele geleer het in opdragte wat relatiewe redening nodig het. As ons fokus op die probleem van die relatiewe grootte van voorwerpe in abstrakte visuele konteks besluit, analyseer ons beide een-stap en twee-stap redering. Vir die laaste, ons bou 'n nuwe datastel van drie-beeldskene en definieer 'n taak wat benodig redensie op die vlak van die individuele beelde en oor beelde in 'n sken. Ons probeer die geleerde model voorstellings deur diagnosiese klassifiseerders te gebruik. Ons eksperimente wys dat multimodaal transformeerder-gebaseerde arkitekturke hoër vlak relasionele redening kan uitvoer en kan leer voorstellings vir nuwe opdragte en data wat baie anders is van wat gesien is in voorstelling.", 'ko': '우리는 시각과 언어 모델이 관계 추리가 필요한 임무에서 배운 표징을 연구했다.추상적인 시각 환경에서 물체의 상대적인 크기를 평가하는 문제에 대해 우리는 한 걸음과 두 걸음의 추리를 분석했다.후자에 대해 우리는 세 개의 이미지 장면을 포함하는 새로운 데이터 집합을 구축하고 하나의 임무를 정의했다. 이 임무는 장면의 단일 이미지와 크로스 이미지에서 추리를 해야 한다.우리는 진단 분류기 탐색 학습의 모형 표시를 사용한다.우리의 실험에 의하면 예비 훈련의 다중모드 변환기를 바탕으로 하는 체계 구조는 더욱 높은 수준의 관계 추리를 수행할 수 있고 예비 훈련에서 본 것과 매우 다른 새로운 임무와 데이터의 표시를 학습할 수 있다.', 'am': 'የራእይና የቋንቋ ምሳሌዎች በሚያስተምሩበት ስራ ውስጥ ተማርተዋል፡፡ የአካባቢዎች ቁጥጥር በሚያሳየው ውጤት ላይ በመቆጣጠር ላይ እናሳውቃለን፣ አንድ ደረጃ እና ሁለት ደረጃ ምክንያት እናስተምር፡፡ ለኋለኛይቱ፣ የሦስት ምስል ዓይነቶች አዲስ ዳታተር መሥራት እና በጣቢያ ምስሎች እና ምስሎችን በተለየ ደረጃ ላይ ማስታወቂያ የሚያስፈልጋቸውን ስራ እናሳውቃለን፡፡ ተማርነው የሞዴል ምሳሌ ምናረጋገጥን በdiagnostic ክፍተቶችን በመጠቀም እንሞክራለን፡፡ Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.', 'sw': 'Tunawachunguza uwakilishi waliojifunza kwa maono na mitindo ya lugha katika kazi zinazohitaji sababu za mahusiano. Kuelekea kwenye tatizo la kutathmini ukubwa wa vitu vya karibu katika mikutano ya kuona ya abstract visual, tunachambua sababu za hatua moja na hatua mbili. Kwa mwisho, tunajenga seti mpya ya taarifa za picha tatu na kuelezea juhudi ambalo linahitaji kujadili kiwango cha picha binafsi na katika picha nyingine katika eneo hilo. Tunawajaribu wawakilishaji wa mifano ya kujifunza kwa kutumia wataalamu wa uchunguzi. Majaribio yetu yanaonyesha kuwa majengo ya mabadiliko yanayotokana na mifano mingi yanayoweza kufanya mazingira ya juu ya kiwango cha mahusiano, na wanaweza kujifunza wakilishi katika kazi za riwaya na takwimu ambazo ni tofauti sana na yale yaliyoonekana katika kutengeneza mvua.', 'bn': 'আমরা দৃষ্টিভঙ্গি এবং ভাষার মডেল দ্বারা প্রতিনিধিদের তদন্ত করি যাদের সম্পর্কের কারণ দরকার। দৃষ্টিভঙ্গি প্রতিযোগিতায় বস্তুর আত্মিক আকার মূল্যের ব্যাপারে মনোযোগ দিয়ে আমরা এক ধাপ ও দুই ধাপের কারণে বিশ্লেষণ করি। পরবর্তীতে আমরা তিন ছবি দৃশ্যের একটি নতুন ডাটাসেট তৈরি করি এবং একটি কাজ নির্ধারণ করি যা ব্যক্তিগত ছবির স্তরে বিবেচনা করা দরকার এবং একটি দৃশ্যে পার্ আমরা শিক্ষিত মডেলের প্রতিনিধিদের পরীক্ষা করি ডায়াগনিস্টিক ব্যবহার করে। আমাদের পরীক্ষাগুলো দেখাচ্ছে যে মাল্টিমোডাল পরিবর্তনের ভিত্তিক কাঠামো উচ্চ পর্যায়ের সম্পর্কের কারণ প্রকাশ করতে পারে, আর তারা নতুন কাজ এবং তথ্যের প্রতিন', 'hy': 'Մենք ուսումնասիրում ենք տեսողության և լեզվի մոդելների միջոցով սովորված ներկայացումները այն խնդիրներում, որոնք պահանջում են հարաբերական մտածողություն: Մենք կենտրոնացնում ենք առարկաների հարաբերական չափի գնահատման խնդիրը վերացական տեսողական կոնտեքստում, մենք վերլուծում ենք մեկ քայլ և երկու քայլ մտածողությունները: Վերջիններին մենք կառուցում ենք երեք նկարների նոր տվյալների համակարգ և սահմանում ենք մի առաջադրանք, որը պահանջում է մտածողություն առանձին նկարների մակարդակում և պատկերների միջև: We probe the learned model representations using diagnostic classifiers.  Մեր փորձարկումները ցույց են տալիս, որ նախավարժեցված բազմամոդալ վերափոխողների հիմնված ճարտարապետությունները կարող են կատարել ավելի բարձր մակարդակի հարաբերական մտածողություն և կարող են սովորել նոր առաջադրանքների և տվյալների ներկայացումներ, որոնք շատ տարբերվում են նախավարժեցման ժամանակ', 'az': 'Biz görünüş və dil modellərinin öyrəndiyi göstəriciləri araşdırırıq ki, əlaqəsiz dəyişiklik lazımdır. Görünüş müxtəliflərdə objektlərin relativ böyüklüyünü müəyyən etmək problem in ə odaklanırsak, hər ikisini bir adım və iki adım razılığını analiz edirik. Sonuncu üçün üç görüntü sahələrinin yeni verilən qurğunu in şa edirik və bir sahədə görüntülərin səviyyəsində dəyişiklik lazımdır. Biz diagnostik klassifikatlarını istifadə edərək öyrənmiş modellərin göstərilmələrini incidirik. Bizim təcrübələrimiz çoxlu modal transformer-tabanlı arhitektürlər yüksək seviyyətli müxtəlif fikirləşdirmək və yeni işlər və məlumatları öyrənə bilərlər ki, əvvəlcə gördüyünüz şeylərdən çox farklı olduğunu öyrənə bilərlər.', 'ca': "Investiguem les representacions aprendes per la visió i els models lingüístics en tasques que requereixen raonament relacional. En centrar-nos en el problema de valorar la mida relativa dels objectes en contextes visuals abstracts, analitzem el raonament en un pas i en dos pas. Per a aquesta última, construïm un nou conjunt de dades d'escenes de tres imatges i definim una tasca que requereix raonament a nivell de les imatges individuals i a través d'imatges d'una escena. Investiguem les representacions del model aprengut fent servir classificadors diagnòstics. Els nostres experiments demostren que arquitectures basades en transformadors multimodals pré-treinades poden fer raonament relacional de nivell superior i poden aprendre representacions per noves tasques i dades que són molt diferents del que es va veure en pré-treinar.", 'et': 'Uurime nägemus- ja keelemudelitega õppitud representatsioone ülesannetes, mis nõuavad suhtelist mõtlemist. Keskendudes objektide suhtelise suuruse hindamise probleemile abstraktses visuaalses kontekstis analüüsime nii ühe- kui ka kaheastmelist arutlust. Viimase jaoks ehitame uue andmekogumi kolmest pildist stseenist ja määratleme ülesande, mis nõuab arutlemist üksikute piltide tasandil ja stseeni piltide vahel. Me uurime õppitud mudeli esitusi diagnostiliste klassifikaatorite abil. Meie eksperimendid näitavad, et eeltreenitud multimodaalsed transformaatoripõhised arhitektuurid suudavad teostada kõrgema taseme relatsioonilist arutlust ja õppida esitusi uudsete ülesannete ja andmete kohta, mis erinevad väga sellest, mida nähti eeltreeningus.', 'bs': 'Istražujemo predstave naučene vizijskim i jezičkim modelima u zadatkima koji zahtijevaju vezano razumjanje. Fokusirajući se na problem procjene relativne veličine objekata u apstraktivnim vizualnim kontekstima, analiziramo i jednokorak i dvokorak razgovora. Za poslednje, izgradimo novi set podataka o scenama tri slike i definiramo zadatak koji zahtijeva razmišljanje na nivou individualnih slika i preko slika na sceni. Provjeravamo naučene predstave modela koristeći dijagnostičke klasifikacije. Naši eksperimenti pokazuju da pretkivne multimodalne arhitekture bazirane na transformaciji mogu izvršiti povezane razine na višem nivou, i mogu naučiti predstave za nove zadatke i podatke koje su veoma različite od onoga što je viđeno u pretkivanju.', 'cs': 'Zkoumáme reprezentace získané vizí a jazykovými modely v úkolech, které vyžadují relační uvažování. Zaměřujeme se na problematiku hodnocení relativní velikosti objektů v abstraktních vizuálních kontextech a analyzujeme jednostupňové i dvoustupňové uvažování. Pro druhé vytvoříme novou datovou sadu tříobrazových scén a definujeme úkol, který vyžaduje uvažování na úrovni jednotlivých snímků a napříč obrazy ve scéně. Naučené modelové reprezentace zkoumáme pomocí diagnostických klasifikátorů. Naše experimenty ukazují, že předtrénované multimodální transformátorové architektury mohou provádět vyšší úroveň relačního uvažování a jsou schopny se naučit reprezentace nových úkolů a dat, které se velmi liší od toho, co bylo vidět v předtréninku.', 'fi': 'Tutkimme näkemys- ja kielimallien oppimia representaatioita suhteessa päättelyä vaativissa tehtävissä. Keskitymme objektien suhteellisen koon arviointiin abstraktissa visuaalisissa konteksteissa ja analysoimme sekä yhden- että kaksivaiheista päättelyä. Jälkimmäistä varten rakennamme uuden kolmikuvamaisen kohtauksen aineiston ja määrittelemme tehtävän, joka vaatii järkeilyä yksittäisten kuvien tasolla ja kohtauksen kuvien välillä. Tutkimme opittuja malliesityksiä käyttäen diagnostisia luokittelijoita. Kokeemme osoittavat, että esikoulutetut multimodaaliset muuntajapohjaiset arkkitehtuurit pystyvät suorittamaan korkeamman tason relaatiopäättelyä, ja pystyvät oppimaan representaatioita uusista tehtävistä ja datasta, jotka ovat hyvin erilaisia kuin esikoulutuksessa nähtiin.', 'ha': "Munã jãyayya misãlai da aka sanar da idãnun da harshe cikin aikin da ke da muhimmada masu husũma. Fokus kan masu zartar da kure ga girmar abubuwa cikin muhimman abubuwa na kanana, za'a yi anarwa ga kwanza ta guda da taki biyu. Ganin da ya ƙara, muna samar da wani tsari na zane-surar uku kuma munãƙayyade wani aikin da za'a buƙata yin magana a tsakanin zanen guda da bayan zane-zane cikin wani fili. Ina jarraba misalin da aka sanar da su a yi amfani da wasu fassarai. Kayan jarrabõnmu na nũna cewa misalin multi-multiodal da aka danne shi, yana iya iya karatun matsayin masu sarrafa a matsayin mazaɓa, kuma sunã iya karatun masu tsari ga aikin kwanan da aka samu da data waɗanda ke sãɓã wa abin da aka gan shi a bakin matuƙar.", 'sk': 'Raziskujemo reprezentacije, ki se jih naučijo vizijski in jezikovni modeli v nalogah, ki zahtevajo relacijsko razmišljanje. S poudarkom na problemu ocenjevanja relativne velikosti objektov v abstraktnih vizualnih kontekstih analiziramo enostopenjsko in dvokostopenjsko razmišljanje. Za slednje gradimo nov nabor podatkov trislikovnih prizorov in opredelimo nalogo, ki zahteva razmišljanje na ravni posameznih slik in med slikami v prizoru. S pomočjo diagnostičnih klasifikatorjev preiskujemo znane predstavitve modela. Naši eksperimenti kažejo, da lahko predtrenirane multimodalne transformatorske arhitekture izvajajo relacijsko razmišljanje na višji ravni in se naučijo reprezentacij za nove naloge in podatke, ki so zelo različni od tistega, kar je bilo videno v predtreningu.', 'he': 'אנחנו חוקרים את היציגות שלמדו על ידי חזון ודוגמנים לשפה במשימות שדורשות הגיון יחסי. מתמקדת בבעיה של הערכה של גודל יחסי של אובייקטים בקשר ויזואלי אסטרקטי, אנו מנתחים את ההיגיון צעד אחד ושני צעדים. עבור האחרון, אנחנו בונים קבוצת מידע חדשה של סצנות שלושה תמונות ולהגדיר משימה שדורשת הגיון ברמה של התמונות הפרטיות ובדרך תמונות בסצנה. אנו חוקרים את התצגות המודל למדות באמצעות מערכות אבחנה. הניסויים שלנו מראים שהארכיטקטורות המולטומודליות המבוססות מראש יכולות לבצע הגיון מערכי יחסים ברמה גבוהה יותר, ויכולות ללמוד מייצגים למשימות חדשות ומידע שונים מאד ממה שנראה בהימון מראש.', 'jv': 'Awak dhéwé nyulung perusahaan repréntasi dipunangé karo tindang karo perusahaan langgar nganggep nggawe barang nggawe string" in "context_BAR_stringLink Sampeyan XMPP Kernel Awak dhéwé éntuk perbudhakan karo akeh multimodal sing bisa dianggawe akeh perusahaan langgar sampek luwih, lan ijol-ijolan iso nggambar tarjamahan kanggo nganggo perusahaan karo nganggo dolanan sing paling dhéwé lan data sing wis diparahak sak titimbang langgar-itimbang langgar.', 'bo': 'ང་ཚོས་མཐོང་ནི་ལྟ་བ་དང་སྐད་ཡིག་ཆ་རྣམས་ལས་རྟོགས་པའི་གསལ་བཤད་ལ་བཙལ་ཞིབ་བྱེད་ཀྱི་ཡོད། ང་ཚོས་མཐོང་བའི་གནས་ཚུལ་ནང་གི་དངོས་པོ་ཡི་ཆེ་ཆུང་ལ་བསམ་བློ་གཏོང་བའི་དཀའ་ངལ འཛམ་གླིང་གི་ལྟ་བུའི་ནང་དུ་ང་ཚོས་བརྙན་རིས་གསུམ་ཀྱི་གནད་སྡུད་གསར་བ་ཞིག་འཛུགས་བྱས་ནས་སྤྱོད་ཐོག་ཅིག་ངེས་འཛིན་བྱེད་དགོ ང་ཚོས་དབྱེ་རིག་གི་མ་དབྱིབས་དཔྱད་འཛིན་བྱེད་པའི་རྣམ་པ་གྱི་གསལ་བཤད་དག་ལ་ཞིབ ང་ཚོའི་བརྟག་ཞིག་བྱས་ནས་བཟོ་བཅོས་ཐབས་བྱུང་བའི་སྒྲིག་ཆ་རྩལ་གཞི་བརྟེན་ནས་མཐུན་གྱི་མཐུན་རིམ་མཐུན་ཚད་མཐོ་ཤོས་བྱེད་སྲིད། བྱས་ཙང་གསར་འགུལ་གྱི་བྱ་བ'}
{'en': 'Predicting the Success of  Domain Adaptation  in Text Similarity', 'pt': 'Prevendo o sucesso da adaptação de domínio na similaridade de texto', 'fr': "Prédire le succès de l'adaptation de domaine dans la similitude de texte", 'ar': 'توقع نجاح تكييف المجال في تشابه النص', 'es': 'Predecir el éxito de la adaptación de dominios en similitud de texto', 'ja': 'テキストの類似性におけるドメイン適応の成功の予測', 'zh': '占文本相似性域应成功', 'hi': 'पाठ समानता में डोमेन अनुकूलन की सफलता की भविष्यवाणी', 'ru': 'Прогнозирование успеха адаптации домена в текстовом сходстве', 'ga': 'Rathúlacht Oiriúnú Fearainn i gCosúlacht Téacs a Thuar', 'hu': 'A tartomány-adaptáció sikerének előrejelzése a szöveghasonlóságban', 'el': 'Προβλέποντας την επιτυχία της προσαρμογής τομέα στην ομοιότητα κειμένου', 'lt': 'Teksto panašumo domeno pritaikymo sėkmės prognozavimas', 'it': "Prevedere il successo dell'adattamento del dominio nella somiglianza del testo", 'ms': 'Menyegerakkan Kesuksesan Penyesuaian Domain dalam Persamaan Teks', 'kk': 'Мәтін ұқсастығында домен адаптациясының сәттілігін бақылау', 'mk': 'Предвидување на успехот на адаптација на домен во сличност со текст', 'ml': 'ടെക്സ്റ്റ് സമമാകുന്നതില്\u200d ഡൊമെയിന്\u200d അഡാപ്റ്റേഷന്\u200dറെ വിജയം മുന്\u200dകൂട്ടുന്നു', 'ka': 'ტექსტის სინამდვილეობაში დომენის ადაპტიფიკაციის წარმატებას დაწყვება', 'mt': 'It-Tbassir tas-Suċċess tal-Adattament tad-Dominju fis-Similarità tat-Test', 'mn': 'Текст тэнцүү байдлын домон загварын амжилтыг анзаарах', 'no': 'Forhåndsvising av vellukka på domeneadaptasjonen i tekstliknande tekst', 'pl': 'Przewidywanie sukcesu adaptacji domen w podobieństwie tekstu', 'sr': 'Predviđajući uspjeh domena adaptacije u tekstu sličnosti', 'si': 'පාළුවන් සම්බන්ධතාවයෙන් ඩොමේන් සැකසුම් සමහර විශ්වාස කරනවා', 'so': 'Horumarinta liibaanka Adaptation of Domain in Text Similarity', 'ro': 'Prezicerea succesului adaptării domeniului în similitudinea textului', 'ur': 'ڈومین اڈپٹیٹ کے کامیابی کی پیش آنے والی', 'sv': 'Förutsägelse av lyckad domänanpassning i textlikhet', 'ta': 'உரையில் ஒற்றைப்படுத்தலில் டொமைன் செயல்பாட்டின் வெற்றியைக் குறிப்பிடுகிறது', 'uz': 'Name', 'vi': 'Xác định khả năng sửa lỗi miền trong hiệu ứng nhãn tương đồng', 'bg': 'Прогнозиране на успеха на адаптацията на домейна в текстово сходство', 'nl': 'Voorspelling van het succes van domeinaanpassing in tekstgelijkenis', 'da': 'Forudsigelse af succes med domænetilpasning i tekstlighed', 'hr': 'Predviđanje uspjeha adaptacije domena u sličnosti teksta', 'id': 'Memprediksi Sukses Adaptasi Domain dalam Similaritas Teks', 'ko': '텍스트 싱크로율 예측 중 필드 적응 성공률', 'de': 'Vorhersage des Erfolgs der Domänenanpassung in Textgleichheit', 'fa': 'پیش\u200cبینی موفقیت تغییرات دامنی در شباهت متن', 'tr': 'Metin Görnöşinde Aýry Gaýd Etmeniň üstesini Prezenlenýär', 'af': 'Voorskou die suksesvol van Domein Aanpassering in Teks Likveldigheid', 'sw': 'Akiandalia mafanikio ya Adaptation Domain katika Simulifu ya Matambo', 'sq': 'Parashikimi i suksesit të adaptimit të dominit në ngjashmërinë e tekstit', 'am': "ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s", 'bs': 'Predviđanje uspjeha adaptacije domena u sličnosti teksta', 'hy': 'Տեքստի նման դաշտի հաջողության կանխատեսումը', 'az': 'Mətn oxuması içində Domain Adjustasyonun başarısızlığını təmin edir', 'bn': 'Predicting the Success of Domain Adaptation in Text Similarity', 'ca': "Predir l'èxit de l'adaptació del domini a la similitud de text", 'cs': 'Předpověď úspěchu adaptace domén v textové podobnosti', 'fi': 'Verkkotunnuksen mukauttamisen onnistumisen ennustaminen tekstin samankaltaisuudessa', 'et': 'Domeeni kohandamise edu prognoosimine teksti sarnasuses', 'sk': 'Napovedovanje uspeha prilagajanja domene v podobnosti besedila', 'he': 'צפוי הצלחה של ההתאמה למשפחה בדמיון טקסט', 'ha': 'Predicting the Successful Adadation of Domen in Text Similarity', 'jv': 'Learn Mode', 'bo': 'ཚིག་ཡིག་དང་མཚོན་རྟགས་ནང་དུ་ডང་ཁ་སྒྲིག་གི་གྲངས་སུ་སྔོན་འཛུགས་བྱེད་བཞིན་པ'}
{'en': 'Transfer learning methods, and in particular domain adaptation, help exploit labeled data in one domain to improve the performance of a certain task in another domain. However, it is still not clear what factors affect the success of  domain adaptation . This paper models  adaptation  success and selection of the most suitable source domains among several candidates in  text similarity . We use descriptive domain information and cross-domain similarity metrics as  predictive features . While mostly positive, the results also point to some domains where  adaptation  success was difficult to predict.', 'pt': 'Os métodos de aprendizado de transferência e, em particular, a adaptação de domínio, ajudam a explorar dados rotulados em um domínio para melhorar o desempenho de uma determinada tarefa em outro domínio. No entanto, ainda não está claro quais fatores afetam o sucesso da adaptação do domínio. Este artigo modela o sucesso da adaptação e a seleção dos domínios de origem mais adequados entre vários candidatos em similaridade de texto. Usamos informações descritivas de domínio e métricas de similaridade entre domínios como recursos preditivos. Embora principalmente positivos, os resultados também apontam para alguns domínios onde o sucesso da adaptação foi difícil de prever.', 'fr': "Les méthodes d'apprentissage par transfert, et en particulier l'adaptation de domaine, aident à exploiter les données étiquetées dans un domaine afin d'améliorer les performances d'une tâche donnée dans un autre domaine. Cependant, on ne sait toujours pas quels facteurs influent sur le succès de l'adaptation des domaines. Cet article modélise le succès de l'adaptation et la sélection des domaines sources les plus appropriés parmi plusieurs candidats en termes de similitude de texte. Nous utilisons des informations de domaine descriptives et des mesures de similarité entre domaines comme fonctionnalités prédictives. Bien que généralement positifs, les résultats indiquent également certains domaines dans lesquels le succès de l'adaptation était difficile à prévoir.", 'ar': 'تساعد طرق التعلم المنقولة ، ولا سيما تكييف المجال ، على استغلال البيانات المصنفة في مجال واحد لتحسين أداء مهمة معينة في مجال آخر. ومع ذلك ، لا يزال من غير الواضح ما هي العوامل التي تؤثر على نجاح تكييف المجال. نماذج هذه الورقة للنجاح في التكيف واختيار أنسب مجالات المصدر من بين عدة مرشحين في تشابه النص. نحن نستخدم معلومات المجال الوصفية ومقاييس التشابه عبر المجالات كميزات تنبؤية. في حين أن النتائج إيجابية في الغالب ، إلا أن النتائج تشير أيضًا إلى بعض المجالات حيث يصعب التنبؤ بنجاح التكيف.', 'es': 'Los métodos de aprendizaje de transferencia y, en particular, la adaptación de dominios, ayudan a aprovechar los datos etiquetados en un dominio para mejorar el rendimiento de una determinada tarea en otro dominio. Sin embargo, aún no está claro qué factores afectan el éxito de la adaptación de dominios. Este artículo modela el éxito de la adaptación y la selección de los dominios fuente más adecuados entre varios candidatos en similitud de texto. Utilizamos información de dominio descriptiva y métricas de similitud entre dominios como características predictivas. Aunque en su mayoría positivos, los resultados también apuntan a algunos ámbitos en los que el éxito de la adaptación era difícil de predecir.', 'zh': '迁学之道,特为域适,助一域之标数以崇一域之任。 然尚未详哪些因素应境之成也。 本文于文本相似性中数候选者应成最宜源域之选建模。 吾以描述性域信与跨域相似性指标为占候。 虽大抵积极,然亦指其难测应成否之域。', 'ja': '学習方法の転送、特にドメインの適応は、あるドメインのラベル付けされたデータを利用して、別のドメインの特定のタスクのパフォーマンスを向上させるのに役立ちます。しかし、どのような要因がドメイン適応の成功に影響を与えるかはまだ明らかではない。この論文は、テキスト類似性のいくつかの候補の中で最も適切なソースドメインの適応の成功と選択をモデル化する。予測機能として、記述的なドメイン情報とクロスドメイン類似性メトリックを使用しています。ほとんどは肯定的であるが、結果はまた、適応の成功が予測されにくいいくつかの領域を指し示している。', 'hi': 'स्थानांतरण सीखने की विधियाँ, और विशेष रूप से डोमेन अनुकूलन में, किसी अन्य डोमेन में किसी निश्चित कार्य के प्रदर्शन को बेहतर बनाने के लिए एक डोमेन में लेबल किए गए डेटा का शोषण करने में मदद करते हैं। हालांकि, यह अभी भी स्पष्ट नहीं है कि कौन से कारक डोमेन अनुकूलन की सफलता को प्रभावित करते हैं। यह पेपर पाठ समानता में कई उम्मीदवारों के बीच अनुकूलन सफलता और सबसे उपयुक्त स्रोत डोमेन का चयन मॉडल करता है। हम वर्णनात्मक डोमेन जानकारी और क्रॉस-डोमेन समानता मीट्रिक का उपयोग भविष्य कहनेवाली सुविधाओं के रूप में करते हैं। जबकि ज्यादातर सकारात्मक, परिणाम कुछ डोमेन को भी इंगित करते हैं जहां अनुकूलन सफलता की भविष्यवाणी करना मुश्किल था।', 'ru': 'Методы передачи обучения, и в частности адаптация домена, помогают использовать помеченные данные в одном домене для улучшения выполнения определенной задачи в другом домене. Однако до сих пор не ясно, какие факторы влияют на успех адаптации домена. Этот документ моделирует успех адаптации и выбор наиболее подходящих исходных доменов среди нескольких кандидатов по сходству текста. В качестве прогностических признаков мы используем описательную информацию о домене и показатели междоменного сходства. Хотя эти результаты в основном являются позитивными, они также указывают на некоторые области, в которых трудно предсказать успех адаптации.', 'ga': 'Cuidíonn modhanna foghlama aistrithe, agus go háirithe oiriúnú fearainn, le sonraí lipéadaithe a shaothrú in aon fhearann amháin chun feabhas a chur ar fheidhmíocht taisc áirithe i bhfearann eile. Mar sin féin, níl sé soiléir fós cad iad na fachtóirí a théann i bhfeidhm ar rathúlacht oiriúnú fearainn. Múnlaíonn an páipéar seo an rathúlacht maidir le hoiriúnú agus roghnú na bhfearann foinse is oiriúnaí i measc roinnt iarrthóirí ó thaobh cosúlachtaí téacs de. Bainimid úsáid as faisnéis thuairisciúil fearainn agus méadracht cosúlachta tras-fhearainn mar ghnéithe réamh-mheastacháin. Cé go raibh siad dearfach don chuid is mó, léiríonn na torthaí freisin roinnt réimsí ina raibh sé deacair rath oiriúnaithe a thuar.', 'el': 'Οι μέθοδοι μεταφοράς μάθησης, και ιδίως η προσαρμογή του τομέα, βοηθούν στην εκμετάλλευση δεδομένων με ετικέτα σε έναν τομέα για τη βελτίωση της απόδοσης μιας συγκεκριμένης εργασίας σε έναν άλλο τομέα. Ωστόσο, δεν είναι ακόμη σαφές ποιοι παράγοντες επηρεάζουν την επιτυχία της προσαρμογής του τομέα. Η παρούσα εργασία παρουσιάζει την επιτυχία προσαρμογής και την επιλογή των καταλληλότερων τομέων πηγής μεταξύ αρκετών υποψηφίων για ομοιότητα κειμένου. Χρησιμοποιούμε περιγραφικές πληροφορίες τομέα και μετρήσεις ομοιότητας μεταξύ τομέων ως προβλέψιμα χαρακτηριστικά. Αν και ως επί το πλείστον θετικά, τα αποτελέσματα δείχνουν επίσης κάποιους τομείς όπου η επιτυχία της προσαρμογής ήταν δύσκολο να προβλεφθεί.', 'ka': 'ტრანსპერტის სწავლის მეტოვები, და განსაკუთრებით დემომინის აკაპეტიფიკაციაში, დახმარება ექსპერტის მართლა ერთი დემომინიში მონაცემები, რომ სხვა დემომინი მაგრამ არ არის წარმოიდგინე, რომლებიც ფაქტორები ემინის ადატაციის წარმატების შესაძლებლობას. ამ დოკუნტის მოდელების შესაძლებლობა და არჩევა ყველაზე საუკეთესო გამოსახულებელი კონდიდენტების განსხვავებაში. ჩვენ გამოყენებთ განახსოვრებული დიომინის ინფორმაცია და კრასი დიომინის სინამდვილეობის მეტრიკა, როგორც განახსოვრებული ფუნქციები. მაგრამ უფრო პოტიფიკაციურად, შედეგი მონაცემები კიდევ სხვა დომენტებს, სადაც ადაპტიკაციის წარმატება ძალიან რთული იყო.', 'hu': 'A tanulási módszerek átvitele, és különösen a domain adaptációja segít az egyik területen megjelölt adatok kihasználásában egy bizonyos feladat teljesítményének javítása érdekében egy másik területen. Azonban még mindig nem világos, hogy milyen tényezők befolyásolják a domain adaptáció sikerét. Ez a tanulmány modellezi az adaptációs sikert és a legmegfelelőbb forrásterületek kiválasztását több jelölt között a szöveghasonlóságban. Leíró tartományi információkat és tartományközi hasonlósági mutatókat használunk prediktív funkcióként. Bár az eredmények leginkább pozitívak, az adaptáció sikerét nehéz volt előre megjósolni.', 'mk': 'Методите на пренесување на училиште, и особено адаптација на доменот, помагаат во искористувањето на означени податоци во еден домен за подобрување на извршувањето на одредена задача во друг домен. Сепак, сé уште не е јасно кои фактори влијаат на успехот на адаптацијата на доменот. Оваа хартија модели успех на адаптација и избор на најсоодветните изворни домени помеѓу неколку кандидати во сличност со текстот. We use descriptive domain information and cross-domain similarity metrics as predictive features.  While mostly positive, the results also point to some domains where adaptation success was difficult to predict.', 'kk': 'Оқыту әдістерін транспорттау, әдетте доменге адаптациялау, бір доменде деректерді жарлықтау үшін бір доменде бір тапсырманың істеуін жақсарту үшін көмектеседі. Бірақ доменге қолдану сәттігіне қандай факторлардың нәтижесін білмейді. Бұл қағаз үлгілерінде мәтін ұқсастығындағы көзінің ең жақсы көзінің домендерін таңдау және таңдау сәтті. Біз домен мәліметін түсініктіріп, доменге ұқсас емес метрикалық мәліметтерді таңдау мүмкіндіктері ретінде қолданамыз. Көпшілігі оң жақсы болса да, нәтижелер бірнеше домендерге қарай адаптациялау сәттігін бақылау қиын болды.', 'lt': 'Perdavimo mokymosi metodai, visų pirma pritaikymas prie sričių, padeda panaudoti vienoje srityje pažymėtus duomenis, kad pagerintų tam tikros užduoties vykdymą kitoje srityje. Tačiau vis dar neaišku, kokie veiksniai turi įtakos prisitaikymo prie srities sėkmei. Šis dokumentas modeliuoja pritaikymo sėkmę ir tinkamiausių šaltinio sričių pasirinkimą kelių kandidatų teksto panašumo srityje. Naudojame aprašomąją domeno informaciją ir tarpdomeno panašumo metrijas kaip prognozuojamąsias savybes. Nors daugiausia teigiami, rezultatai taip pat rodo kai kurias sritis, kuriose prisitaikymo sėkmė buvo sunku prognozuoti.', 'it': "I metodi di apprendimento trasferiti, in particolare l'adattamento del dominio, aiutano a sfruttare i dati etichettati in un dominio per migliorare le prestazioni di un determinato compito in un altro dominio. Tuttavia, non è ancora chiaro quali fattori influenzano il successo dell'adattamento del dominio. Questo articolo modella il successo dell'adattamento e la selezione dei domini sorgente più adatti tra diversi candidati in somiglianza testuale. Utilizziamo informazioni descrittive sul dominio e metriche di somiglianza tra domini come funzionalità predittive. Sebbene per lo più positivi, i risultati indicano anche alcuni settori in cui il successo dell'adattamento era difficile da prevedere.", 'mn': 'Сургууль суралцах аргыг дамжуулах, ялангуяа холбоотой загвар зохицуулах, зарим үйл ажиллагааны үйл ажиллагааг өөр холбоотой нэг холбоотой мэдээллийг ашиглахад тусалдаг. Гэхдээ хүрээлэн буй адилтгалын амжилтын хүчин зүйлсийг юу нөлөөлдөг вэ гэдгийг ойлгохгүй. Энэ цаасан загварын адилтгал амжилт болон хамгийн тохиромжтой эх үүсвэрийн загварын сонголт нь текст төстэй. Бид тодорхойлж буй холбооны мэдээлэл, олон холбооны адилхан хэмжээсүүдийг таамаглах чадвартай болгон ашиглаж байна. Харин ихэнх нь эерэг, үр дүн нь мөн зарим хэсэгт адаптацийн амжилтыг таамаглах хэцүү байсан хэсэгт илэрхийлдэг.', 'ml': 'പഠിക്കാനുള്ള രീതികള്\u200d മാറ്റുക, പ്രത്യേകിച്ച് ഡൊമെയിന്\u200d മാറ്റുക, മറ്റൊരു ഡൊമെയിനില്\u200d കുറച്ച് ജോലിയുടെ പ്രവര്\u200dത്തനം മെ എന്നാലും ഡൊമെയിനുള്ള വിജയത്തിന് എന്ത് കാരണങ്ങള്\u200d ബാധിക്കുന്നു എന്നത് വ്യക്തമാക്കുന്നില്ല. ഈ പേപ്പറിന്റെ മോഡലുകള്\u200d വിജയത്തിന്റെയും ഏറ്റവും ശരിയായ സോര്\u200dസ്സ് ഡോമെനുകളുടെയും തെരഞ്ഞെടുക്കുന്നതിന്റെയും  നമ്മള്\u200d വിശദീകരിക്കുന്ന ഡൊമെയിന്\u200dറെ വിവരങ്ങളും ക്രിസ്റ്റോമെന്\u200d മെറ്റിക്കളും പ്രവചിക്കുന്ന വിവരങ്ങളായ മിക്കവാറും സ്ഥിരീതിയായിരുന്നില്ലെങ്കില്\u200d, അതിന്റെ ഫലങ്ങള്\u200d ചില ഡോമീനുകള്\u200dക്കും കാണിച്ചുകൊടുക്കുന', 'ms': 'Name Namun, masih belum jelas faktor apa yang mempengaruhi kejayaan penyesuaian domain. Model kertas ini berjaya menyesuaikan dan pemilihan domain sumber yang paling sesuai diantara beberapa calon dalam persamaan teks. Kami menggunakan maklumat domain deskriptif dan metrik persamaan-domain sebagai ciri-ciri ramalan. Walaupun kebanyakan positif, hasil juga menunjukkan kepada beberapa domain di mana sukses penyesuaian sukar untuk dijangka.', 'mt': 'Il-metodi tat-tagħlim tat-trasferiment, u b’mod partikolari l-adattament tad-dominju, jgħinu fl-isfruttar tad-dejta ttikkettata f’dominju wieħed biex itejbu l-prestazzjoni ta’ ċertu kompitu f’dominju ieħor. Madankollu, għadu mhux ċar liema fatturi jaffettwaw is-suċċess tal-adattament fid-dominju. Dan il-mudell tal-karta jimmudella s-suċċess tal-adattament u l-għażla tal-aktar dominji tas-sors adattati fost diversi kandidati f’similarità tat-test. Aħna nużaw informazzjoni deskrittiva tad-dominju u metriċi ta’ similarità bejn id-dominji bħala karatteristiċi prevedibbli. Filwaqt li l-biċċa l-kbira tagħhom huma pożittivi, ir-riżultati jindikaw ukoll xi oqsma fejn kien diffiċli li wieħed jipprevedi s-suċċess tal-adattament.', 'no': 'Overføring av læringsmetodar, og spesielt domeneadaptasjon, hjelp til å bruka merkelappe data i ein domene for å forbetra utviklinga av ein viss oppgåve i ein annan domene. Men det er fortsatt ikkje klart kva faktorer påvirkar søket på domenetilpassing. Denne papirmodellen er vellukka og utvalet av dei mest passande kjeldedomene mellom fleire kandidatar i tekstlikhet. Vi brukar beskrivingsinformasjon om domenet og tilsvarande metrikar for krysdomenet som foregåande funksjonar. Selv om det meste er positivt, vil resultatet også gjera til nokre domene der tilpassasjonsfullsten var vanskeleg for å foregå.', 'pl': 'Transfer metod uczenia się, a w szczególności adaptacja domeny, pomaga wykorzystać etykietowane dane w jednej dziedzinie w celu poprawy wykonywania określonego zadania w innej dziedzinie. Nie jest jednak jasne, jakie czynniki wpływają na sukces adaptacji domen. W artykule przedstawiono sukces adaptacji i wybór najbardziej odpowiednich domen źródłowych wśród kilku kandydatów w zakresie podobieństwa tekstowego. Wykorzystujemy opisowe informacje o domenie i wskaźniki podobieństwa między domenami jako funkcje predykcyjne. Chociaż w większości pozytywne wyniki wskazują również na niektóre dziedziny, w których sukces adaptacyjny był trudny do przewidzenia.', 'sr': 'Prebacivanje metoda učenja, i posebno adaptacija domena, pomaže iskoristiti označene podatke u jednom domenu kako bi poboljšali funkciju određenog zadatka u drugoj domenu. Međutim, još uvek nije jasno što faktori utiču na uspjeh adaptacije domena. Ovi papirni modeli su uspešni i izbor najprikladnijih izvorskih domena među nekoliko kandidata u sličnosti teksta. Koristimo opisivne informacije domena i metrike sličnosti preko domena kao predviđavajuće karakteristike. Iako su uglavnom pozitivni, rezultati takođe ukazuju na neke domene gde je uspjeh adaptacije teško predvidjeti.', 'ro': 'Metodele de învățare transferate, în special adaptarea domeniilor, contribuie la exploatarea datelor etichetate într-un domeniu pentru a îmbunătăți performanța unei anumite sarcini într-un alt domeniu. Cu toate acestea, nu este încă clar ce factori afectează succesul adaptării domeniului. Această lucrare modelează succesul adaptării și selecția celor mai potrivite domenii sursă în rândul mai mulți candidați în similitudinea textului. Utilizăm informații descriptive de domeniu și măsurători de similitudine între domenii ca caracteristici predictive. Deși majoritatea pozitive, rezultatele indică, de asemenea, unele domenii în care succesul adaptării a fost dificil de anticipat.', 'so': 'Ka wareeji qaababka waxbarashada, gaar ahaan bedelka deegaanka, waxaad ka caawisaa in laguugu baaraandegayo macluumaad hal domain ah si uu u hagaajiyo sameynta shaqada gooni kale. Si kastaba ha ahaatee, weli ma cadayn waxyaabaha ay saameyn ku leedahay guulaynta beddelka gudaha. Tusaaladan warqadda ah oo ku habboon liibaanshaha iyo doorashada ugu haboon meelaha ugu haboon ee ugu habboon qoraal isku mid ah. Macluumaad faa’iido ah oo ku saabsan deegaanka iyo metriciyada la isku mid ah oo aan u isticmaalno sida faa’iido la sii sheegayo. Inta badan suurtagalka waxaa sidoo kale ku qoran meelo ay ku jirto suurtagalnimada lagu beddelo ay ku adag tahay in la sii sheego.', 'si': 'ඉගෙන ගන්න විදියට, සහ විශේෂයෙන් ඩෝමින් සැකසුම් විදියට, දත්ත ප්\u200dරවර්තනය කරන්න පුළුවන් විදියට, එක ඩෝම ඒත් ඒක තාමත් පැහැදිලි නැහැ කොච්චර ප්\u200dරශ්නයක් තියෙන්නේ ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් නැ මේ පත්තු මොඩේල් සැලසුම් සමහරවිට සහ හ හොඳම ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප අපි විශ්වාස කරන්න පුළුවන් ඩොමේන් තොරතුරු සහ ක්\u200dරීස් ඩොමේන් එක්ක සාමාන්\u200dය මීටරික් විදිහට ප්\u200d සාමාන්\u200dය විශේෂයෙන්, ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200d', 'ta': 'கற்றல் முறைமைகளை மாற்றுக, குறிப்பிட்ட ட டொமைன் மேம்படுத்தல், ஒரு டொமைனில் குறிப்பிட்ட தகவலை பயன்படுத்த உதவுகிறது, மற்றொரு களமில்  ஆனாலும், இது இன்னும் தெளிவாக இல்லை என்ன காரணிகள் களம் ஒதுக்க வெற்றியை பாதிக்கும். இந்த தாள் மாதிரிகள் உரையில் ஒத்திசைக்கும் பல தேர்ந்தெடுக்கப்பட்ட மூலங்களில் தேர்ந்தெடுக்கப்பட்ட மூலங் நாம் விவரிப்பான களம் தகவல் மற்றும் குறுக்கும் களம் ஒத்த மெட்ரிக்களை முன்வாக்குதல் குணங்களாக பயன்படுத்து பெரும்பாலும் நேர்மையாக இருந்த போது, முடிவுகள் சில களங்களுக்குக் குறிப்பிடுகிறது அங்கு ஏற்றுக்கொள', 'ur': 'سیکھنے کے طریقے، اور مخصوص ڈومین ادامه کے ساتھ، ایک ڈومین میں ڈاٹ لیبل کیا گیا ہے، اور ایک دوسرے ڈومین میں ایک ایسے کام کا عملکرد بہتر کرنے کے لئے مدد کریں۔ لیکن یہ ابھی بھی واضح نہیں ہے کہ ڈومین کے کامیابی پر کس فاکتور اثر دیتے ہیں۔ یہ کاغذ نمڈلوں کی تعمیر موفقیت اور انتخاب کے زیادہ اچھے سراسر ڈومین میں بہت سی کائندیوں میں سے ہے. ہم دکھانے والی ڈومین معلومات اور کروس ڈومین کی مثالی متریک کو پیش بینی کے طور پر استعمال کرتے ہیں. اگرچہ اکثر مثبت ہے، نتیجے بھی بعض دامنوں کو نشان دیتے ہیں جہاں سامان موفقیت کا پیش آنے کا مشکل تھا.', 'sv': 'Överföringsmetoder, särskilt domänanpassning, hjälper till att utnyttja märkta data på ett område för att förbättra prestandan för en viss uppgift på ett annat område. Det är dock fortfarande oklart vilka faktorer som påverkar framgången med domänanpassning. Denna uppsats modellerar anpassningsprocess och urval av de lämpligaste källdomänerna bland flera kandidater i textlikhet. Vi använder beskrivande domäninformation och mätvärden för domänöverskridande likheter som prediktiva funktioner. Även om resultaten mestadels är positiva pekar resultaten också på vissa områden där anpassningsprocess var svårt att förutsäga.', 'uz': "Transfer learning methods, and in particular domain adaptation, help exploit labeled data in one domain to improve the performance of a certain task in another domain.  Lekin, bu hozir domen adaptsiyagi muvaffaqiyatlariga nima sabablar beradi emas. @ info @ info Ko'pchilik yaxshi bo'lganda, natijalar o'zgartirish muvaffaqiyatli kutishi juda qiyin edi.", 'vi': 'Phương pháp truyền học, và đặc biệt là sửa chữa miền, giúp khai thác dữ liệu được dán nhãn trong một miền để nâng cao khả năng thực hiện một nhiệm vụ trong một miền khác. Tuy nhiên, vẫn chưa rõ các yếu tố ảnh hưởng đến thành công của việc thích nghi miền. Giấy này mô phỏng khả năng sửa đổi thành công và chọn những miền nguồn thích hợp nhất giữa nhiều ứng viên về nét giống nhau. Chúng tôi sử dụng các thông tin miền mô tả và tỉ lệ nét giống nhau chéo như tính năng dự đoán. Tuy rằng kết quả phần lớn là tích cực, nhưng cũng chỉ ra một số lĩnh vực khó đoán được thành công sửa chữa.', 'da': 'Overførsel af læringsmetoder, navnlig tilpasning af domæner, hjælper med at udnytte mærkede data på ét domæne for at forbedre udførelsen af en bestemt opgave på et andet domæne. Det er dog stadig ikke klart, hvilke faktorer der påvirker succesen med domænetilpasning. Denne artikel modellerer tilpasning succes og udvælgelse af de mest egnede kildeområder blandt flere kandidater i tekstlighed. Vi bruger beskrivende domæneoplysninger og lighedsmålinger på tværs af domæner som forudsigende funktioner. Selvom resultaterne for det meste er positive, peger resultaterne også på nogle områder, hvor tilpasningssucces var svært at forudsige.', 'bg': 'Методите за трансфер на обучение, и по-специално адаптирането на домейна, помагат за експлоатирането на етикетирани данни в един домейн, за да се подобри изпълнението на определена задача в друг домейн. Все още не е ясно какви фактори влияят върху успеха на адаптацията на домейна. Настоящата статия моделира успеха на адаптацията и подбора на най-подходящите източници сред няколко кандидата по сходство на текста. Използваме описателна информация за домейните и показатели за сходство между домейните като предсказуеми характеристики. Макар и предимно положителни, резултатите сочат и към някои области, в които успехът на адаптацията е труден за предвиждане.', 'id': 'Transfer metode belajar, dan khususnya adaptasi domain, membantu mengeksploitasi data labeled di satu domain untuk meningkatkan prestasi tugas tertentu di domain lain. Namun, masih belum jelas faktor apa yang mempengaruhi sukses adaptasi domain. Model kertas ini sukses adaptasi dan seleksi domain sumber yang paling cocok diantara beberapa kandidat dalam kesamaan teks. Kami menggunakan informasi domain deskriptif dan metrik persamaan cross-domain sebagai fitur prediksif. Meskipun kebanyakan positif, hasilnya juga menunjukkan beberapa daerah di mana sukses adaptasi sulit diprediksi.', 'ko': '이동 학습 방법, 특히 영역 적응은 한 영역의 표기 데이터를 이용하여 다른 영역의 특정 임무의 성능을 향상시키는 데 도움이 된다.그러나 어떤 요소가 분야 적응의 성공에 영향을 미칠지는 아직 분명하지 않다.본고는 텍스트의 유사성 중 몇 개의 후보 원역의 자체 적응 성공과 선택에 대해 모델링을 실시했다.우리는 묘사적 영역 정보와 크로스 영역 유사성 도량을 예측 특징으로 사용한다.대부분의 결과는 긍정적이지만 결과는 적응의 성공을 예측하기 어려운 분야도 지적했다.', 'fa': 'انتقال روش یادگیری، و در خصوص تعادل دامنی، کمک به استفاده از داده\u200cها در یک دامنه برای تغییر فعالیت یک کار خاص در یک دامنه دیگر کمک می\u200cکند. ولی هنوز مشخص نیست که چگونه باعث تأثیر موفقیت تأثیر دومین است. این مدل تغییرات موفقیت و انتخاب از مناسب ترین دامنه منبع در میان چند کاندیدا در شباهت متن است. ما از اطلاعات دامنی توصیف و متریک شبیه\u200cی دامنی به عنوان ویژه\u200cهای پیش\u200cبینی استفاده می\u200cکنیم. در حالی که بیشتر مثبت است، نتیجه\u200cها به بعضی دامنه\u200cها نشان می\u200cدهند که موفقیت تغییر دادن برای پیش\u200cبینی سخت بود.', 'hr': 'Prebacivanje metoda učenja, i posebno adaptacija domena, pomaže iskoristiti označene podatke u jednom domenu kako bi poboljšali učinkovitost određenog zadatka u drugoj domenu. Međutim, još uvijek nije jasno što činjenici utječu na uspjeh adaptacije domena. Ovaj papirni modeli uspjeha adaptacije i izbora najprikladnijih izvorskih domena među nekoliko kandidata u sličnosti teksta. Koristimo opisivne informacije domena i metrike sličnosti preko domena kao predviđavajuće karakteristike. Iako su uglavnom pozitivni, rezultati također ukazuju na neke domene gdje je uspjeh adaptacije teško predvidjeti.', 'nl': 'Transferleermethoden, en met name domeinaanpassing, helpen bij het benutten van gelabelde gegevens in een domein om de prestaties van een bepaalde taak in een ander domein te verbeteren. Het is echter nog niet duidelijk welke factoren het succes van domeinaanpassing beïnvloeden. Deze paper modelleert adaptatiesucces en selectie van de meest geschikte brondomeinen onder verschillende kandidaten in tekstgelijkenis. We gebruiken beschrijvende domeininformatie en domeinoverschrijdende vergelijkingsmetrieken als voorspellende functies. Hoewel de resultaten meestal positief zijn, wijzen de resultaten ook op een aantal domeinen waar adaptatiesucces moeilijk te voorspellen was.', 'sw': 'Tafsiri mbinu za kujifunza, na hususani kuboreshwa kwa ndani, kusaidia kutumia taarifa zilizoonyesha katika eneo moja ili kuboresha utendaji wa kazi fulani katika eneo jingine. Hata hivyo, bado si wazi ni sababu gani zinaathiri mafanikio ya mabadiliko ya ndani. Mifano hii ya makaratasi ya kuboresha mafanikio na kuchaguliwa kwa maeneo yanayofaa zaidi ya vyanzo vingi miongoni mwa wagombea kadhaa katika simu za maandishi. Tunatumia taarifa za ndani za maelezo na mitindo inayofanana na maeneo ya ndani kama utabiri. Wakati bado ni chanya, matokeo pia yanaonyesha baadhi ya maeneo ambapo mafanikio ya kuboresha yalikuwa vigumu kutabiri.', 'de': 'Transfer-Lernmethoden, insbesondere Domänenanpassung, helfen, beschriftete Daten in einer Domäne auszunutzen, um die Leistung einer bestimmten Aufgabe in einer anderen Domäne zu verbessern. Es ist jedoch noch nicht klar, welche Faktoren den Erfolg der Domänenanpassung beeinflussen. Diese Arbeit modelliert den Anpassungserfolg und die Auswahl der am besten geeigneten Quelldomänen unter mehreren Kandidaten in Textähnlichkeit. Wir verwenden deskriptive Domäneninformationen und domänenübergreifende Ähnlichkeitsmetriken als prädiktive Funktionen. Obwohl die Ergebnisse überwiegend positiv sind, weisen die Ergebnisse auch auf einige Bereiche hin, in denen Anpassungserfolg schwer vorherzusagen war.', 'tr': 'Öğrenme yöntemlerini, ve özellikle domeny adaptasyonu, bir domenyde belli bir görevinin etkinliğini geliştirmek için etiketli verileri kullanmak için yardım et. Ýöne, domain adaptasiýasynyň başarnygyna nähili faktörler täsirleýändigini heniz belli etmeýär. Bu kagyz nusgalary başarnygy we metin meňzeşliklerinde iñ gowy çeşme sahypalarynyň saýlamagy. We use descriptive domain information and cross-domain similarity metrics as predictive features. Köplenç pozitiv bolsa, netijeler ýagdaýynyň önlemek üçin adaptasiýanyň başarnygyny çaklamak kyn bir ýere görkezýär.', 'af': "Oordrag leer metodes, en in besonderhede domein aanpassing, hulp om data in een domein te uitbrei om die prestasie van 'n sekere taak in 'n ander domein te verbeter. Maar dit is nog nie duidelik wat faktore effekteer die sukses van domein aanpassing. Hierdie papier modele aanpassing sukses en kies van die mees geskikte bron domeine onder verskeie kandidate in teks gelykenis. Ons gebruik beskryfbare domein inligting en kruisdomein gelykenis metriek as voorskoulike funksies. Terwyl meeste positiewe, die resultate ook na sommige domene aanwys waar aanpassing sukses moeilik was om te voorskou.", 'am': 'ትምህርት ማድረጊያውን ለወጡ፥ በተለይም ዶሜን አቀማመጥ፣ በአንድ ዶሜን የጽሑፍ ዳታዎችን በመጠቀም ይረዳሉ፡፡ ምንም እንኳን የዶሜን አካባቢ ማድረግ ምን የሚያስጠቅመው ምክንያት አይገልጽም፡፡ ይህ የሕትመት ዓይነቶች በጽሑፍ ብጤት በሚያስተካክሉ ቁጥጥር እና የመረጠውን የክፍተት ዕድል አካባቢ ዶሜኖችን ማሻሻል፡፡ የውይይት የዶሜን መረጃ እና የክፍል ዶሜኔን የሚመስል ማተሚያዎች እንደ ትንቢት ምርጫዎች እናስቀምጣለን፡፡ አብዛኛውም አካባቢ ቢሆን ፍሬዎቹ አካባቢ ማቀናቀል የደረሰበት አካባቢ ግንኙነት ለመቀበል አግኝቷል፡፡', 'bn': 'শিক্ষা পদ্ধতি পরিবর্তন করুন, বিশেষ করে ডোমেইনের অ্যাডাপেটশন, একটি ডোমেইনে লেবেলেড ডাটা ব্যবহার করুন, যাতে অন্য ডোমেইনে একটি নির্দিষ্ তবে এটা এখনো পরিষ্কার নয় যে ডোমেইনের সফলতার প্রভাব ফেলেছে। এই কাগজের মডেলটি টেক্সটের সমতামূলক প্রার্থীদের মধ্যে সবচেয়ে যথেষ্ট সোর্স ডোমেনের সফলতা এবং নির্বাচন করা। আমরা বর্ণনা করি ডোমেইনের তথ্য এবং ক্রিস্টোমেইনের একই ধরনের মেট্রিক ব্যবহার করি ভবিষ্যতের বৈশিষ্ট্য হিসেবে। বেশীরভাগ ইতিবাচক হলেও এর ফলাফল কিছু ডোমেইনের দিকে নির্দেশ করেছে যেখানে আপেটশন সফলতা ভবিষ্যৎ করতে কঠিন ছিল।', 'bs': 'Prebacivanje metoda učenja, i posebno adaptacija domena, pomaže iskoristiti etiketirane podatke u jednom domenu kako bi poboljšala učinkovitost određenog zadatka u drugoj domenu. Međutim, još uvijek nije jasno što faktori utiču na uspjeh adaptacije domena. Ovaj papirni model uspjeha adaptacije i izbora najprikladnijih izvorskih domena među nekoliko kandidata u sličnosti teksta. Koristimo opisivne informacije domena i metrike sličnosti preko domena kao predviđavajuće karakteristike. Iako su uglavnom pozitivni, rezultati također ukazuju na neke domene gdje je uspjeh adaptacije teško predvidjeti.', 'hy': 'Գործողության փոխանցման մեթոդները, և հատկապես բնագավառի ադապտացիան, օգնում են օգտագործել մեկ բնագավառում նշված տվյալները, որպեսզի բարելավեն որոշ խնդիրների կատարումը մեկ այլ բնագավառում: Այնուամենայնիվ, դեռևս պարզ չէ, թե ինչ գործոններ են ազդում բնագավառի ադապտացիայի հաջողության վրա: Այս թղթի մոդելները հաջողությունը հարմարեցնում են և ընտրում են առավել համապատասխան աղբյուրի բնագավառները տեքստի նմանության մեջ մի քանի թեկնածուների միջև: Մենք որպես կանխատեսողական հատկանիշներ օգտագործում ենք բնագավառի նկարագրական տեղեկատվությունը և բնագավառի միջև նմանությունները: Մինչդեռ հիմնականում դրական են, արդյունքները նույնպես ցույց են տալիս որոշ ոլորտներ, որտեղ հարմարեցման հաջողությունը դժվար էր կանխատեսել:', 'az': '√Ėyr…ônm…ô metodlarńĪnńĪ daŇüńĪyńĪn, √∂zl…ôrin…ô d…ô domain adaptasiyonu, bir domeind…ô m…ôlumatlar etiketl…ônm…ôsin…ô k√∂m…ôk edir ki, baŇüqa domeind…ô b…ôzi iŇül…ôrin performansńĪnńĪ yaxŇüńĪlaŇüdńĪrsńĪn. Ancaq, domain adaptasiyasńĪnńĪn baŇüarńĪsńĪzlńĪńüńĪna n…ôl…ôr t…ôsir edir? Bu kańüńĪt modell…ôrinin m√ľv…ôff…ôqiyy…ôti v…ô m…ôtn b…ônz…ôrind…ô …ôn uyńüun m…ônb…ô domeninin se√ßimi. Biz tanńĪmlayan domenin m…ôlumatńĪnńĪ v…ô √ßox domenin similarit…ôsi metrikl…ôrini t…ôdbir √∂zellikl…ôri olaraq istifad…ô edirik. ∆Źlb…ôtt…ô d…ô pozitiv olaraq, sonu√ßlar da b…ôzi sah…ôl…ôr…ô g√∂st…ôrir ki, uyńüunlaŇüdńĪrma uńüursuzluńüu t…ôxmin…ô √ß…ôtin idi.', 'sq': 'Transfero metodat e mësimit dhe veçanërisht përshtatjet në domeni, ndihmojnë në shfrytëzimin e të dhënave të etiketuara në një domeni për të përmirësuar performancën e një detyre të caktuar në një domeni tjetër. Megjithatë, nuk është ende e qartë se cilat faktorë ndikojnë në suksesin e përshtatjes në domeni. Kjo letër modelon suksesin e përshtatjes dhe zgjedhjen e domeneve më të përshtatshme burimi midis disa kandidatëve në ngjashmërinë e tekstit. Ne përdorim informacionin përshkrimtar të domenit dhe metrikat e ngjashmërisë ndër-domenit si karakteristika parashikuese. Ndërsa kryesisht pozitive, rezultatet tregojnë gjithashtu disa fusha ku suksesi i përshtatjes ishte i vështirë për të parashikuar.', 'cs': 'Metody přenosu učení, a zejména adaptace domény, pomáhají využít označená data v jedné doméně ke zlepšení výkonu určitého úkolu v jiné doméně. Stále však není jasné, jaké faktory ovlivňují úspěch adaptace domén. Tento článek modeluje úspěch adaptace a výběr nejvhodnějších zdrojových domén mezi několika kandidáty v oblasti podobnosti textu. Jako prediktivní funkce používáme popisné informace o doméně a metriky podobnosti mezi doménami. I když většinou pozitivní, výsledky ukazují také na některé oblasti, kde bylo obtížné předvídat úspěch adaptace.', 'et': 'Õppe edastamise meetodid ja eelkõige domeeni kohandamine aitavad ära kasutada märgistatud andmeid ühes valdkonnas, et parandada teatud ülesannete täitmist teises valdkonnas. Siiski ei ole veel selge, millised tegurid mõjutavad domeeni kohandamise edukust. Käesolev töö modelleerib kohandamise edukust ja kõige sobivamate lähtedomeenide valikut mitme kandidaadi seas teksti sarnasuse osas. Ennustusfunktsioonidena kasutame kirjeldavat domeeniteavet ja domeenidevahelist sarnasust mõõdikuid. Kuigi tulemused on enamasti positiivsed, viitavad need ka mõnedele valdkondadele, kus kohanemise edu oli raske ennustada.', 'fi': 'Oppimismenetelmien siirto ja erityisesti toimialojen mukauttaminen auttavat hyödyntämään merkittyjä tietoja yhdellä toimialueella tietyn tehtävän suorituskyvyn parantamiseksi toisella toimialueella. Vielä ei ole kuitenkaan selvää, mitkä tekijät vaikuttavat toimialojen sopeutumisen onnistumiseen. Tämä artikkeli mallintaa sopeutumisen onnistumista ja sopivimpien lähdeverkkotunnusten valintaa useiden ehdokkaiden joukosta tekstin samankaltaisuuden osalta. Ennustettavina ominaisuuksina käytämme kuvailevaa verkkotunnustietoa ja toimialueiden välisiä samankaltaisuusmittareita. Vaikka tulokset ovat enimmäkseen myönteisiä, ne viittaavat myös joihinkin osa-alueisiin, joilla sopeutumisen onnistumista oli vaikea ennustaa.', 'ca': "Els mètodes d'aprenentatge de transferència, en particular l'adaptació de dominis, ajuden a explotar les dades etiquetades en un domini per millorar el desempeny d'una determinada tasca en un altre domini. No obstant això, encara no és clar quins factors afecten l'èxit de l'adaptació al domini. Aquest paper modela l'èxit d'adaptació i la selecció de dominis de fonts més adequats entre varis candidats en similitud de text. Utilitzem la informació descriptiva del domini i les mètriques de similitud entre dominis com característiques preditives. While mostly positive, the results also point to some domains where adaptation success was difficult to predict.", 'ha': "Sauya shiryoyin ayuka da aka sani, kuma, hususan adadin ayuka, sai ka yi taimako su samu da data wanda aka rubũta shi a cikin guda dõmin ya gyãra aikin wani aikin mai ƙayyade cikin wata guda. Amma, bã shi da bayyana wanne matsayi sun yi amfani da mafarin adadin wanda ke cikin guda. @ action: button Tuna amfani da motsi na guda da ke daidaita metricki kamar sifati masu basĩri. Aka da mafiya fara, matsala na pointe zuwa wasu jama'a, inda ma'anar adadi ya kasance mai nauyi wa kuskure.", 'sk': 'Načini prenosa učenja in zlasti prilagajanja domenskih podatkov pomagajo izkoristiti označene podatke na enem področju za izboljšanje izvedbe določene naloge na drugem področju. Vendar še vedno ni jasno, kateri dejavniki vplivajo na uspešnost prilagajanja domen. Prispevek predstavlja uspeh prilagajanja in izbiro najprimernejših izvornih domen med več kandidati v podobnosti besedila. Kot napovedne funkcije uporabljamo opisne domenske informacije in meritve meddomenske podobnosti. Čeprav so rezultati večinoma pozitivni, kažejo tudi na nekatera področja, kjer je bilo uspeh prilagajanja težko napovedati.', 'he': 'שיטות לימוד העברה, ובמיוחד שימוש בתחום, עוזרים לנצל נתונים מסוימים בתחום אחד כדי לשפר את ביצוע משימה מסוימת בתחום אחר. בכל אופן, עדיין לא ברור אילו גורמים משפיעים על הצלחה של התאמה לתחום. דוגמני נייר זה הצלחה בהתאמה ובבחירה של המקורים המתאימים ביותר בין מספר מועמדים בדמיון טקסט. אנו משתמשים במידע תואר של שטח ומטריקות דומיות בין השטח כתכונות צפויים. בעוד בעיקר חיוביים, התוצאות מצביעות גם על איזושהי תחומות שבו הצלחה בהתאמה הייתה קשה לחזות.', 'jv': 'transfer Learning Methods, and in special domain modification, help expand label data in one domain to advance the success of a some task in a second domain. politenessoffpolite"), and when there is a change ("assertivepoliteness politenessoffpolite"), and when there is a change ("assertivepoliteness We use descripative domain information and inter-domain Simlarty Metika as preview parameters. politenessoffpolite"), and when there is a change ("assertivepoliteness', 'bo': 'Transfer learning methods, and in particular domain adaptation, help exploit labeled data in one domain to improve the performance of a certain task in another domain. ཡིན་ནའང་། ཆ་རྐྱེན་གྱིས་དུས་ཁྱབ་ཆོས་ཉིད་ལ་གོ་སྐབས་ཡོད་མེད། ཆ་རྐྱེན་གྱི་ནང་དུ་ཕན་ཚུན་ཆེ་ཤོག་བྱས་པའི་མིག་ཆས་འདིས་གྲུབ་སྒྲིག་འགོད་བྱེད་སྟངས་དང་ཁ་རྟགས་ཤོག་བྱས We use descriptive domain information and cross-domain similarity metrics as predictive features. གྲངས་སུ་མཐུན་རྐྱེན་བྱས་པ་ཡིན་ནའང་། གྲུབ་འབྲས་ཀྱི་ཆེད་དུ་ཉེན་སྒྲིག་གི་གྲུབ་སྐྱོབ་རྒྱུ་རྐྱེན་ལ་ཡང་'}
{'en': 'Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings BERT  (and Other Transformer Model) Embeddings', 'ar': 'اشتقاق السمات الدلالية السياقية من تضمين BERT (ونموذج المحولات الأخرى)', 'fr': "Dériver des caractéristiques sémantiques contextualisées à partir d'intégrations BERT (et d'autres modèles de transformateurs)", 'es': 'Derivación de características semánticas contextualizadas a partir de incrustaciones BERT (y otros modelos de transformadores)', 'ja': 'BERT （およびその他の変圧器モデル）の埋め込みからコンテキスト化されたセマンティック機能を導き出す', 'zh': '自BERT(及诸转换器)嵌中派生上下文语义特征', 'hi': 'BERT (और अन्य ट्रांसफॉर्मर मॉडल) एम्बेडिंग से Contextualized शब्दार्थ विशेषताएं व्युत्पन्न', 'pt': 'Derivando recursos semânticos contextualizados de incorporações BERT (e outros modelos de transformador)', 'ru': 'Получение контекстуализированных семантических признаков из вложений BERT (и других моделей трансформаторов)', 'ga': 'Gnéithe Séimeantacha Comhthéacsúla a Dhíorthú ó Leabaithe BERT (agus Múnla Trasfhoirmeora Eile).', 'el': 'Παράγωγη σημασιολογικών χαρακτηριστικών από ενσωμάτωση BERT (και άλλου μοντέλου μετασχηματιστή)', 'hu': 'Kontextualizált szemantikus jellemzők származtatása a BERT-ből (és más transzformátor modellekből)', 'ka': 'BERT (და სხვა ტრანფორმეტრის მოდელის) ინბვებებიდან კონტექსტუალურებული სემანტიკური ფექტურების მიღება', 'it': 'Derivazione di caratteristiche semantiche contestualizzate da BERT (e altri modelli di trasformatori)', 'kk': 'BERT (және басқа түрлендіру үлгісінің) ендірулерінен контекстуалды семантикалық мүмкіндіктерді алу', 'ms': 'Menarik Ciri-ciri Seman Berkonteks Dari Penjelmaan BERT (dan Model Transformer Lain-lain)', 'ml': 'Comment', 'mt': 'Id-Derivazzjoni ta’ Karatteristiċi Semantiċi Kuntest minn Embeddings BERT (u Mudell ieħor ta’ Trasformazzjoni)', 'mn': 'BERT (болон бусад Төрөгдлийн загвар) нэвтрүүлэлт', 'no': 'Hentar kontekstualiserte semantiske funksjonar frå innbygging av BERT (og andre transformeringsmodell)', 'lt': 'Iš BERT (ir kito transformatoriaus modelio) įrangos išvedamos kontekstinės Semantinės savybės', 'pl': 'Pochodzenie kontekstualizowanych cech semantycznych z osadzeń BERT (i innych modeli transformatorów)', 'mk': 'Отворање контекстни семантични функции од вградувања на BERT (и друг трансформиран модел)', 'ro': 'Derularea caracteristicilor semantice contextualizate din încorporarea BERT (și a altor modele de transformator)', 'sr': 'Dobivanje kontekstualiziranih semantičnih karakteristika iz BERT (i drugih transformerskih modela) integracija', 'si': 'BERT (සහ අනිත් වෙනස් ප්\u200dරමාණකරණය මඩෝල්) සම්බන්ධ විශේෂය', 'sv': 'Att härleda kontextualiserade semantiska funktioner från BERT (och andra transformatormodeller) inbäddningar', 'ta': 'Comment', 'ur': 'BERT (اور دوسری ترفنٹر موڈل) ایمبڈینگ سے کنٹکسٹولیز سیمانٹیک فرصت حاصل کرتے ہیں', 'so': 'Xafiisyada habaarka ah ee ka soo baxa BERT (iyo Model kale turjumista)', 'uz': 'Comment', 'vi': 'Độ lệch tinh thần giao tiếp từ LAT (và other transformer Model) Nhúng', 'da': 'Aflede kontekstualiserede semantiske funktioner fra BERT (og andre transformatormodeller) Embeddings', 'nl': 'Gecontextualiseerde semantische kenmerken afleiden uit BERT (en andere transformatormodellen)', 'bg': 'Извличане на контекстуализирани семантични функции от вграждания на BERT (и друг модел на трансформатор)', 'hr': 'Dobivanje kontekstualiziranih semantičkih karakteristika iz BERT (i drugih transformerskih modela) uključenja', 'ko': 'BERT(기타 변환기 모델) 삽입에서 어경화 의미 특징 내보내기', 'id': 'Menarik Karakteristik Semantik Konteksualisasi dari Embeddings BERT (dan Model Transformer lainnya)', 'sw': 'Tamko za kimapenzi zilizotokana na BERT (na Modeli nyingine za Tafsiri)', 'de': 'Ableitung kontextualisierter semantischer Features aus BERT-Einbettungen (und anderen Transformatormodellen)', 'fa': 'دریافت ویژگی\u200cهای سیم\u200cمانتیک\u200cهای متصل از ابتدایی BERT (و مدل دیگر تغییر\u200cدهنده\u200cها)', 'sq': 'Deriving Contextualized Semantic Features from BERT (and Other Transformer Model)', 'af': 'Ontvang Konteksualiseerde Semantiese Funksies van BERT (en ander Transformer Model) Inbêdings', 'am': 'የፊደል ቅርጽ ምርጫዎች', 'tr': 'BERT (we Başga Transformer Modeli) अंतरीकरण', 'hy': 'Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings', 'bn': 'Comment', 'bs': 'Dobivanje kontekstualiziranih semantičnih karakteristika iz BERT (i drugih transformerskih modela) integracija', 'az': 'BERT (v톛 Dig톛r Transformer Modeli) 캻fad톛l톛rind톛n m칲xt톛lif Semantik 칐zellikl톛ri', 'ca': "Derint les característiques Semàtiques Contextualitzades d'incorporacions BERT (i altres models de transformació)", 'cs': 'Odvození kontextualizovaných sémantických vlastností z BERT (a dalších transformátorových modelů)', 'et': 'Kontekstuliseeritud semantiliste funktsioonide tuletamine BERT-i (ja muude transformaatorite mudelite) manustest', 'fi': 'Kontekstualisten semanttisten ominaisuuksien johtaminen BERT:stä (ja muista muuntajamalleista) upotuksista', 'jv': 'dering contextual semanti Attribute from BERT (and Second Transformer model) embedding', 'sk': 'Izpeljanje kontekstualiziranih semantičnih funkcij iz BERT (in drugih modelov transformatorjev)', 'he': 'הוצאת תכונות סמנטיות קונטקסטולוגיות מתכניות BERT (ומודל מעבר אחר)', 'ha': 'KCharselect unicode block name', 'bo': 'Deriving Contextualized Semantic Features from BERT (and Other Transformer Model) Embeddings'}
{'en': 'Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of  Natural Language Processing . Importantly, they allow the creation of  word embeddings  that capture important semantic information about words in context. However, as single entities, these  embeddings  are difficult to interpret and the  models  used to create them have been described as opaque. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. Unfortunately, the  space  only exists for a small data-set of 535 words, limiting its uses. Previous work (Utsumi, 2018, 2020 ; Turton et al., 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. This provides two things ; (1) semantic feature values derived from contextualised word embeddings and (2) insights into how semantic features are represented across the different layers of the BERT model.', 'fr': "Les modèles basés sur l'architecture du transformateur, tels que BERT, ont marqué une étape cruciale dans le domaine du traitement du langage naturel. Plus important encore, ils permettent de créer des intégrations de mots qui capturent des informations sémantiques importantes sur les mots en contexte. Cependant, en tant qu'entités uniques, ces intégrations sont difficiles à interpréter et les modèles utilisés pour les créer ont été décrits comme opaques. Binder et ses collègues ont proposé un espace d'intégration intuitif dans lequel chaque dimension est basée sur l'une des 65 caractéristiques sémantiques principales. Malheureusement, l'espace n'existe que pour un petit ensemble de données de 535 mots, ce qui limite ses utilisations. Des travaux antérieurs (Utsumi, 2018, 2020\xa0; Turton et al., 2020) ont montré que les fonctionnalités de Binder peuvent être dérivées d'intégrations statiques et extrapolées avec succès à un vaste nouveau vocabulaire. Passant à l'étape suivante, cet article montre que les fonctionnalités de Binder peuvent être dérivées de l'espace d'intégration BERT. Cela fournit deux éléments\xa0: (1) des valeurs de caractéristiques sémantiques dérivées de l'intégration de mots contextualisés et (2) des informations sur la façon dont les caractéristiques sémantiques sont représentées dans les différentes couches du modèle BERT.", 'ar': 'شكلت النماذج القائمة على بنية المحولات ، مثل BERT ، خطوة حاسمة إلى الأمام في مجال معالجة اللغة الطبيعية. الأهم من ذلك ، أنها تسمح بإنشاء عمليات تضمين الكلمات التي تلتقط معلومات دلالية مهمة حول الكلمات في السياق. ومع ذلك ، ككيانات فردية ، يصعب تفسير هذه الزخارف ، وقد وصفت النماذج المستخدمة في إنشائها بأنها غير شفافة. اقترح بيندر وزملاؤه مساحة تضمين بديهية حيث يعتمد كل بُعد على واحدة من 65 ميزة دلالية أساسية. لسوء الحظ ، المساحة موجودة فقط لمجموعة بيانات صغيرة من 535 كلمة ، مما يحد من استخداماتها. أظهر العمل السابق (Utsumi ، 2018 ، 2020 ؛ Turton et al. ، 2020) أن ميزات Binder يمكن اشتقاقها من الزخارف الثابتة واستنباطها بنجاح إلى مفردات جديدة كبيرة. من خلال اتخاذ الخطوة التالية ، توضح هذه الورقة أنه يمكن اشتقاق ميزات Binder من مساحة تضمين BERT. هذا يوفر شيئين ؛ (1) قيم السمات الدلالية المشتقة من تضمين الكلمات السياقية و (2) رؤى حول كيفية تمثيل السمات الدلالية عبر الطبقات المختلفة لنموذج BERT.', 'pt': 'Modelos baseados na arquitetura do transformador, como o BERT, marcaram um passo crucial no campo do Processamento de Linguagem Natural. É importante ressaltar que eles permitem a criação de embeddings de palavras que capturam informações semânticas importantes sobre as palavras no contexto. No entanto, como entidades únicas, esses embeddings são difíceis de interpretar e os modelos usados para criá-los foram descritos como opacos. Binder e colegas propuseram um espaço de incorporação intuitivo onde cada dimensão é baseada em um dos 65 recursos semânticos principais. Infelizmente, o espaço só existe para um pequeno conjunto de dados de 535 palavras, limitando seus usos. Trabalhos anteriores (Utsumi, 2018, 2020; Turton et al., 2020) mostraram que os recursos do Binder podem ser derivados de incorporações estáticas e extrapolados com sucesso para um novo e grande vocabulário. Dando o próximo passo, este artigo demonstra que os recursos do Binder podem ser derivados do espaço de incorporação do BERT. Isso fornece duas coisas; (1) valores de recursos semânticos derivados de incorporações de palavras contextualizadas e (2) insights sobre como os recursos semânticos são representados nas diferentes camadas do modelo BERT.', 'es': 'Los modelos basados en la arquitectura de transformadores, como BERT, han marcado un paso crucial en el campo del procesamiento del lenguaje natural. Es importante destacar que permiten la creación de incrustaciones de palabras que capturan información semántica importante sobre las palabras en contexto. Sin embargo, como entidades individuales, estas incrustaciones son difíciles de interpretar y los modelos utilizados para crearlas se han descrito como opacos. Binder y sus colegas propusieron un espacio de incrustación intuitivo donde cada dimensión se basa en una de las 65 características semánticas principales. Desafortunadamente, el espacio solo existe para un pequeño conjunto de datos de 535 palabras, lo que limita sus usos. Trabajos anteriores (Utsumi, 2018, 2020; Turton et al., 2020) han demostrado que las características de Binder pueden derivarse de incrustaciones estáticas y extrapolarse con éxito a un vocabulario nuevo y extenso. Dando el siguiente paso, este documento demuestra que las funciones de Binder se pueden derivar del espacio de incrustación BERT. Esto proporciona dos cosas; (1) valores de características semánticas derivados de incrustaciones de palabras contextualizadas y (2) información sobre cómo se representan las características semánticas en las diferentes capas del modelo BERT.', 'ja': 'BERTのような変圧器アーキテクチャに基づくモデルは、自然言語処理の分野で重要な一歩を踏み出しました。 重要なことに、これらは、文脈内の単語に関する重要な意味情報を取り込む単語埋め込みの作成を可能にします。 ただし、単一のエンティティとして、これらの埋め込みは解釈が難しく、作成に使用されたモデルは不透明であると説明されています。 Binderと同僚は、各ディメンションが65のコアセマンティック機能の1つに基づいている直感的な埋め込み空間を提案しました。 残念ながら、スペースは535ワードの小さなデータセットにのみ存在し、その使用を制限します。 以前の研究（ Utsumi, 2018, 2020; Turton et al., 2020 ）は、Binderの特徴が静的埋め込みから導き出され、大きな新しい語彙にうまく外挿すことができることを示しています。 次のステップでは、バインダーの特徴がBERT埋め込み空間から導出できることを実証する。 これにより、(1)文脈化された単語埋め込みから派生したセマンティック機能値と、(2) BERTモデルの異なるレイヤー全体でセマンティック機能がどのように表現されるかについての洞察という2つのことが提供されます。', 'hi': 'ट्रांसफॉर्मर आर्किटेक्चर पर आधारित मॉडल, जैसे कि BERT, ने प्राकृतिक भाषा प्रसंस्करण के क्षेत्र में एक महत्वपूर्ण कदम आगे बढ़ाया है। महत्वपूर्ण रूप से, वे शब्द एम्बेडिंग के निर्माण की अनुमति देते हैं जो संदर्भ में शब्दों के बारे में महत्वपूर्ण शब्दार्थ जानकारी को कैप्चर करते हैं। हालांकि, एकल संस्थाओं के रूप में, इन एम्बेडिंग की व्याख्या करना मुश्किल है और उन्हें बनाने के लिए उपयोग किए जाने वाले मॉडल को अपारदर्शी के रूप में वर्णित किया गया है। बाइंडर और सहयोगियों ने एक सहज ज्ञान युक्त एम्बेडिंग स्पेस का प्रस्ताव दिया जहां प्रत्येक आयाम 65 कोर शब्दार्थ विशेषताओं में से एक पर आधारित है। दुर्भाग्य से, अंतरिक्ष केवल 535 शब्दों के एक छोटे से डेटा-सेट के लिए मौजूद है, इसके उपयोगों को सीमित करता है। पिछला काम (Utsumi, 2018, 2020; टर्टन एट अल., 2020) ने दिखाया है कि बाइंडर सुविधाओं को स्थैतिक एम्बेडिंग से प्राप्त किया जा सकता है और सफलतापूर्वक एक बड़ी नई शब्दावली में एक्सट्रपलेटेड किया जा सकता है। अगला कदम उठाते हुए, यह पेपर दर्शाता है कि बाइंडर सुविधाओं को BERT एम्बेडिंग स्पेस से प्राप्त किया जा सकता है। यह दो चीजें प्रदान करता है; (1) संदर्भित शब्द एम्बेडिंग से व्युत्पन्न शब्दार्थ सुविधा मूल्यों और (2) BERT मॉडल की विभिन्न परतों में शब्दार्थ सुविधाओं का प्रतिनिधित्व कैसे किया जाता है, इस बारे में अंतर्दृष्टि।', 'zh': '盖转换器架构之形,如BERT,表自然语言理域之趋机也。 其要者,许创立词嵌,以获上下文中单词要语义信息。 然为单实,此嵌难解,并于创造为不明。 Binder及同列发一直观嵌空,每维度皆65核语义特征之一。 不幸者,空存535单词小数集,以限其用。 前事(内海,2018年,2020年。 Turton等,2020)已验,Binder特徵可从静嵌中出,并成功地外推至一大新词汇表中。 下一步,本文演 Binder 特徵从 BERT 嵌空派生。 二事也; (1)从上下文化词嵌入中得语义特征直,及(2)对语义特征于BERT形之异层中。', 'ru': 'Модели, основанные на трансформаторной архитектуре, такие как BERT, стали важным шагом вперед в области обработки естественного языка. Важно отметить, что они позволяют создавать вложения слов, которые фиксируют важную семантическую информацию о словах в контексте. Однако, будучи отдельными объектами, эти вложения трудно интерпретировать, и модели, используемые для их создания, были описаны как непрозрачные. Binder и коллеги предложили интуитивно понятное пространство вложений, где каждое измерение основано на одной из 65 основных семантических особенностей. К сожалению, пространство существует только для небольшого набора данных из 535 слов, что ограничивает его использование. Предыдущая работа (Utsumi, 2018, 2020; Turton et al., 2020) показала, что признаки Binder могут быть получены из статических вложений и успешно экстраполированы в большой новый словарь. Следующий шаг - эта статья демонстрирует, что свойства Binder могут быть получены из пространства вложений BERT. Это обеспечивает две вещи: (1) значения семантических признаков, полученные из контекстуализированных вложений слов, и (2) представления о том, как семантические признаки представлены на различных уровнях модели BERT.', 'ga': 'Is céim ríthábhachtach chun tosaigh iad múnlaí atá bunaithe ar ailtireacht an chlaochladáin, mar BERT, i réimse na Próiseála Teanga Nádúrtha. Rud atá tábhachtach, ceadaíonn siad leabaithe focal a chruthú a ghlacann faisnéis shéimeantach thábhachtach faoi fhocail i gcomhthéacs. Mar aonáin aonair, áfach, is deacair na leabaithe seo a léirmhíniú agus tá na samhlacha a úsáideadh chun iad a chruthú teimhneach. Mhol ceanglóra agus comhghleacaithe spás neadaithe iomasach ina bhfuil gach gné bunaithe ar cheann de 65 croí-ghné shéimeantach. Ar an drochuair, níl an spás ann ach do thacar sonraí beag de 535 focal, rud a chuireann teorainn lena úsáidí. Léiríodh i saothar roimhe seo (Utsumi, 2018, 2020; Turton et al., 2020) gur féidir gnéithe ceanglóra a dhíorthú ó leabú statacha agus gur féidir iad a eachtarshuíomh go rathúil chuig stór focal mór nua. Agus an chéad chéim eile á ghlacadh, léiríonn an páipéar seo gur féidir gnéithe ceanglóra a dhíorthú ó spás leabaithe BERT. Soláthraíonn sé seo dhá rud; (1) luachanna gné shéimeantacha díorthaithe ó leabú focal comhthéacsaithe agus (2) léargais ar an gcaoi a léirítear gnéithe shéimeantacha ar fud na sraitheanna éagsúla de shamhail CRET.', 'hu': 'A transzformátor architektúráján alapuló modellek, mint például a BERT, kulcsfontosságú előrelépést jelentettek a természetes nyelvfeldolgozás területén. Fontos, hogy lehetővé teszik szóbeágyazások létrehozását, amelyek fontos szemantikai információkat rögzítenek a szavakról kontextusban. Egyetlen entitásként azonban ezeket a beágyazásokat nehéz értelmezni, és a létrehozásukhoz használt modelleket átlátszatlannak írták le. Binder és kollégái egy intuitív beágyazási térre javasoltak, ahol minden dimenzió 65 alapvető szemantikai funkció egyikén alapul. Sajnos a hely csak egy kis, 535 szóból álló adatkészlet létezik, ami korlátozza a használatát. Korábbi munka (Utsumi, 2018, 2020; Turton et al., 2020) azt mutatta, hogy a Binder funkciók statikus beágyazásokból származhatók, és sikeresen extrapolálhatók egy nagy új szókincsre. A következő lépésben ez a tanulmány bemutatja, hogy a Binder funkciók a BERT beágyazási területéből származhatók. Ez két dolgot biztosít: (1) kontextuális szóbeágyazásokból származó szemantikai jellemzők értékei és (2) betekintés arra, hogy a szemantikai jellemzők miként vannak ábrázolva a BERT modell különböző rétegeiben.', 'ka': 'მოდელები, როგორც BERT, ტრანფორმეტრის აქტიქტიქტურაციაზე ბაზიან, უფრო მნიშვნელოვანი ნაწილის ნაწილის ნაწილის პროცესი. მნიშვნელოვანია, რომ ისინი შესაძლებელია სიტყვების შექმნა, რომელიც მნიშვნელოვანი სემონტიკური ინფორმაცია სიტყვების შესახებ კონტექსტში. მაგრამ, როგორც ერთი ინტერნეტიები, ეს ინტერნეტირება ძალიან რთულია და მოდელები, რომლებიც გამოყენებულია მათ შექმნა, აღწერა როგორც არაფერი. დაკავშირები და კოლეგებები შეგიძლიათ ინტუტიური დაკავშირების ადგილას, სადაც ყოველ განზომილება 65 წესი სიმენტიკური განზომილებების ერთი დაბაზიან. მართლად, მხოლოდ 535 სიტყვების პატარა მონაცემებისთვის მხოლოდ არსებობს. პირველი სამუშაო (Utsumi, 2018, 2020; Turton et al., 2020) გამოჩვენეთ, რომ ბინდერის ფუნქციები შეიძლება სტატიკალური ინბედინგიდან დაიწყება და წარმატებით ახალი სიტყვებულაში ექსპოლე შემდეგი კონფიგურაციის შესაძლებლობა, ეს დონფიგურაცია აჩვენებს, რომ ბინდერის ფუნქციები შეიძლება BERT-ის შესაძლებლობადან გამოიყენება. ეს იქნება ორი რამ; (1) სემონტიკური ფუნქციების მნიშვნელობები, რომლებიც განსხვავებული BERT მოდელის განსხვავებული სიტყვებისგან გამოიყენებული სიტყვებისგან და (2) სიტყვებისგან გამოყენებულია.', 'it': "I modelli basati sull'architettura del trasformatore, come BERT, hanno segnato un importante passo avanti nel campo dell'elaborazione del linguaggio naturale. È importante sottolineare che consentono la creazione di incorporazioni di parole che catturano importanti informazioni semantiche sulle parole nel contesto. Tuttavia, come entità singole, queste incorporazioni sono difficili da interpretare e i modelli utilizzati per crearle sono stati descritti come opachi. Binder e colleghi hanno proposto uno spazio di embedding intuitivo in cui ogni dimensione si basa su una delle 65 caratteristiche semantiche principali. Purtroppo, lo spazio esiste solo per un piccolo set di dati di 535 parole, limitandone l'uso. Un lavoro precedente (Utsumi, 2018, 2020; Turton et al., 2020) ha dimostrato che le funzionalità di Binder possono essere derivate da incorporazioni statiche ed estrapolate con successo in un ampio nuovo vocabolario. Facendo il passo successivo, questo articolo dimostra che le caratteristiche di Binder possono essere derivate dallo spazio di incorporazione BERT. Ciò fornisce due elementi: (1) valori di caratteristiche semantiche derivati da incorporazioni contestualizzate di parole e (2) approfondimenti su come le caratteristiche semantiche sono rappresentate nei diversi livelli del modello BERT.", 'mk': 'Моделите базирани на архитектурата на трансформаторите, како што е БЕРТ, означија клучен чекор напред во полето на процесорот на природниот јазик. Важно е, тие овозможуваат создавање на зборови внатрешни кои capture важни семантични информации за зборовите во контекст. Сепак, како единствени ентитети, овие вградувања се тешки да се интерпретираат и моделите кои се користат за нивно создавање се опишани како непријатни. Врзувачот и колегите предложија интуитивен вставен простор каде што секоја димензија се базира на една од 65 основни семантични карактеристики. За жал, просторот постои само за мал набор на податоци од 535 зборови, што ги ограничува неговите употреби. Претходната работа (Утсуми, 2018, 2020; Туртон и други, 2020) покажа дека карактеристиките на Биндер можат да се извлечат од статички вградувања и успешно да се екстраполираат на голем нов речник. Преземајќи го следниот чекор, овој документ демонстрира дека карактеристиките на Binder може да се извлечат од просторот за внесување на BERT. Ова обезбедува две работи; (1) семантични вредности на карактеристиките кои се извлечени од контекстуализираните вложувања на зборови и (2) информации за тоа како семантичните карактеристики се претставени во различните слоеви на моделот BERT.', 'lt': 'Modeliai, pagrįsti transformatoriaus architektūra, pavyzdžiui, BERT, parodė svarbų žingsnį į priekį gamtos kalbų apdorojimo srityje. Svarbu, kad jos leidžia kurti žodžių įterpimus, kuriuose būtų pateikta svarbi semantinė informacija apie žodžius kontekste. Tačiau, kaip atskiri subjektai, šiuos įterpimus sunku aiškinti, o jų kūrimui naudojami modeliai buvo apibūdinti kaip neskaidrius. Binder ir kolegos pasiūlė intuityvią įdėjimo erdvę, kurioje kiekvienas matmuo grindžiamas viena iš 65 pagrindinių semantinių savybių. Deja, vieta yra tik nedideliam 535 žodžių duomenų rinkiniui, ribojant jo naudojimą. Ankstesnis darbas (Utsumi, 2018, 2020; Turton et al., 2020) parodė, kad Binder savybės gali būti gautos iš statinių įdėjimų ir sėkmingai ekstrapoliuojamos į didelį naują žodyną. Kitu žingsniu šis dokumentas rodo, kad jungiklio savybės gali būti gautos iš BERT įdėjimo vietos. Tai reiškia du dalykus: (1) semantinių savybių vertės, gautos iš kontekstinių žodžių įterpimų ir (2) supratimo, kaip semantinių savybių atstovaujama įvairiuose BERT modelio sluoksniuose.', 'el': 'Τα μοντέλα που βασίζονται στην αρχιτεκτονική του μετασχηματιστή, όπως το έχουν σηματοδοτήσει ένα κρίσιμο βήμα μπροστά στον τομέα της επεξεργασίας φυσικής γλώσσας. Σημαντικό είναι ότι επιτρέπουν τη δημιουργία ενσωμάτωσης λέξεων που συλλαμβάνουν σημαντικές σημασιολογικές πληροφορίες σχετικά με τις λέξεις στο πλαίσιο. Ωστόσο, ως μεμονωμένες οντότητες, αυτές οι ενσωματώσεις είναι δύσκολο να ερμηνευθούν και τα μοντέλα που χρησιμοποιούνται για τη δημιουργία τους έχουν περιγραφεί ως αδιαφανή. Ο Binder και οι συνάδελφοι πρότειναν έναν διαισθητικό χώρο ενσωμάτωσης όπου κάθε διάσταση βασίζεται σε ένα από τα 65 βασικά σημασιολογικά χαρακτηριστικά. Δυστυχώς, ο χώρος υπάρχει μόνο για ένα μικρό σύνολο δεδομένων 535 λέξεων, περιορίζοντας τις χρήσεις του. Προηγούμενες εργασίες (Τούτσουμι, 2018, 2020; Τούρτον κ.α., 2020) έχουν δείξει ότι τα χαρακτηριστικά του συνδετήρα μπορούν να αντληθούν από στατικές ενσωματώσεις και να επεκταθούν επιτυχώς σε ένα μεγάλο νέο λεξιλόγιο. Λαμβάνοντας το επόμενο βήμα, αυτή η εργασία καταδεικνύει ότι τα χαρακτηριστικά του συνδετήρα μπορούν να αντληθούν από το χώρο ενσωμάτωσης BERT. Αυτό παρέχει δύο πράγματα: (1) σημασιολογικές τιμές χαρακτηριστικών που προέρχονται από ενσωματωμένες λέξεις στο πλαίσιο και (2) γνώσεις σχετικά με τον τρόπο με τον οποίο τα σημασιολογικά χαρακτηριστικά αναπαρίστανται στα διάφορα στρώματα του μοντέλου BERT.', 'mt': "Mudelli bbażati fuq l-arkitettura tat-trasformatur, bħall-BERT, immarkaw pass kruċjali 'l quddiem fil-qasam tal-Ipproċessar tal-Lingwi Naturali. Importantly, they allow the creation of word embeddings that capture important semantic information about words in context.  Madankollu, bħala entitajiet individwali, dawn l-inkorporazzjonijiet huma diffiċli biex jiġu interpretati u l-mudelli użati biex jinħolqu ġew deskritti bħala opaki. Binder u kollegi pproponew spazju ta’ inkorporazzjoni intwittiv fejn kull dimensjoni hija bbażata fuq waħda minn 65 karatteristika semantika ewlenija. Sfortunatament, l-ispazju jeżisti biss għal sett żgħir ta’ dejta ta’ 535 kelma, li jillimitaw l-użu tiegħu. Xogħol preċedenti (Utsumi, 2018, 2020; Turton et al., 2020) wera li l-karatteristiċi Binder jistgħu jiġu derivati minn inkorporazzjonijiet statiċi u jiġu estrapolati b’suċċess għal vokabulari ġdid kbir. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space.  Dan jipprovdi żewġ affarijiet; (1) valuri ta’ karatteristiċi semantiċi derivati minn inkorporazzjonijiet ta’ kliem kuntestwalizzati u (2) fehmiet dwar kif karatteristiċi semantiċi huma rappreżentati fis-saffi differenti tal-mudell BERT.", 'kk': 'BERT секілді түрлендіруші архитектурасына негізделген үлгілер, Түзіндік тіл процессорының өрісінде маңызды қадам алдында белгіледі. Мүмкін, олар сөздер туралы маңызды семантикалық мәліметті контекстік туралы құрылған сөздерді құруға рұқсат етіледі. Бірақ, бұл ендіру қиындықтары толықтыруға қиын және оларды құру үшін қолданылатын үлгілер мөлдірлі деп анықталды. Бағыттаушы мен жұмыстар 65 негізгі семантикалық мүмкіндіктерінің біріне негізделген интуитивті ендіру орынын таңдады. Кешіріңіз, олардың қолданбаларын шектеу үшін тек кішкентай деректер жиыны 535 сөздер үшін орын бар. Алдыңғы жұмыс (Utsumi, 2018, 2020; Turton et al., 2020) Binder мүмкіндіктері статикалық ендіруден шығарылады және үлкен жаңа сөз сөздеріне экстраполяциялануға болады. Келесі қадам алу үшін Бұл қағаз Binder мүмкіндіктері BERT ендіру орындан шығарылатын көрсетеді. Бұл екі нәрсе береді. (1) БеRT үлгісінің әртүрлі қабаттарында семантикалық мүмкіндіктерді қалай көрсету үшін көзгертілген сөздерді ендіру және (2) мәліметтерінен келтірілген семантикалық мүмкіндіктері.', 'no': 'Modellar basert på transformeringsarkitekturen, som BERT, har markert eit viktig steg framover i feltet for naturspråkshandtering. Det er viktig at dei lèt opprettinga av ordinnbygging som tar viktig semantisk informasjon om ord i kontekst. Som enkle einingar er imidlertid desse innbygginga vanskeleg å tolka, og modelane som brukar for å laga dei er beskriven som ugjennomsiktige. Binder og kollegar foreslåde ein intuitivt innbyggingsplass der kvar dimensjon er basert på ein av 65 kjernesemantiske funksjonar. Dessverre finst det berre plassen for ein liten datasett med 535 ord som avgrenserer bruka. Førre arbeid (Utsumi, 2018, 2020; Turton et al., 2020) har vist at binderingsfunksjonar kan deriverast frå statiske innbygging og fullførleg ekstrapolerast til ein stor ny ordbok. Når du ta neste steg, viser denne papiret at binderingsfunksjonane kan avhennast frå BERT-innbyggingsrommet. Dette tilbyr to ting; (1) semantiske funksjonsverdiar deriverte frå kontekstualiserte ordinnbygging og (2) innsyningar til korleis semantiske funksjonar vert representerte over dei ulike lag i BERT-modellen.', 'ml': 'ബെര്\u200dട്ടി പോലെ മോഡലുകള്\u200d സ്വാഭാവ ഭാഷ പ്രവര്\u200dത്തനത്തിന്റെ പ്രദേശത്തില്\u200d ഒരു പ്രധാനപ്പെട്ട പടി മുന്നോട്ട് ചെയ്യുന്നു. പ്രധാനപ്പെട്ടത്, വാക്കുകള്\u200d സംബന്ധിച്ച് പ്രധാനപ്പെട്ട സെമാന്റിക് വിവരങ്ങള്\u200d പിടികൂടുന്ന വാക്കുകള എന്നാലും, ഒരേ വസ്തുക്കളായിരിക്കുമ്പോള്\u200d, ഈ അകത്തേക്ക് വിശദീകരിക്കാന്\u200d ബുദ്ധിമുട്ടുകളാണ്. അത് സൃഷ്ടിക്കാന്\u200d ഉപയോ ബിണ്ടരും സഹപ്രവര്\u200dത്തകരും ഒരു ഉള്ളിലുള്ള സ്പെയിന്\u200dറ് പ്രായശ്ചിത്തം ചെയ്തിരിക്കുന്നു. അവിടെ ഓരോ ഡിമെന്\u200dഷനും 65 മൂല്യം  നിര്\u200dഭാഗ്യവശാല്\u200d, ഈ സ്ഥലം 535 വാക്കുകളുടെ ചെറിയ വിവരങ്ങള്\u200dക്ക് മാത്രമേ നിലനില്\u200dക്കുന്നുള്ളൂ. മുമ്പുള്ള ജോലി (ഉത്സുമി, 2018, 2020; ടുര്\u200dട്ടണ്\u200d എറ്റ് അല്\u200d, 2020) കാണിച്ചിരിക്കുന്നുവെങ്കില്\u200d ബിന്ഡറിന്റെ വിശേഷതകള്\u200d സ്റ്റാറ്റിക് അംഗീകര അടുത്ത പടി എടുക്കുന്നുവെങ്കില്\u200d ബിന്\u200dഡര്\u200d വിശേഷങ്ങള്\u200d BERT അകത്തുള്ള സ്ഥലത്തുനിന്ന് നീക്കം ചെയ്യാന്\u200d സാധി ഇത് രണ്ടു കാര്യങ്ങള്\u200d നല്\u200dകുന്നു; (1) സംസാരിക്കപ്പെട്ട വാക്കുകളില്\u200d നിന്നും സെമാന്റിക് ഗുണഗണങ്ങളുടെ മൂല്യങ്ങളും, ബെര്\u200dട്ടി മോഡലിന്\u200dറെ വ്യത്യസ്ത തലകളില്\u200d നിന്നും എങ്ങനെയാ', 'ms': 'Model berdasarkan arkitektur pengubah, seperti BERT, telah menandakan langkah penting ke hadapan dalam medan Pemprosesan Bahasa Alami. Yang penting, ia membenarkan ciptaan penyembedding perkataan yang menangkap maklumat semantik penting mengenai perkataan dalam konteks. Namun, sebagai entiti tunggal, penerbangan ini sukar diterangkan dan model yang digunakan untuk menciptanya telah diterangkan sebagai tidak jelas. Pengikat dan rakan-rakan mencadangkan ruang penerbangan intuitif dimana setiap dimensi berdasarkan salah satu daripada 65 ciri semantik utama. Malangnya, ruang ini hanya wujud untuk set data kecil 535 kata, membatasi penggunaannya. Kerja terdahulu (Utsumi, 2018, 2020; Turton et al., 2020) telah menunjukkan bahawa ciri-ciri Binder boleh dihimpunkan dari penyembedding statik dan berjaya ekstrapolasi ke vokbulari baru yang besar. Melangkah langkah berikutnya, kertas ini menunjukkan bahawa ciri-ciri Binder boleh dihimpunkan dari ruang penyambungan BERT. Ini memberikan dua perkara; (1) nilai ciri semantik yang berasal dari penyampilan perkataan kontekstualisasi dan (2) pandangan bagaimana ciri-ciri semantik diwakili melalui lapisan berbeza model BERT.', 'pl': 'Modele oparte na architekturze transformatora, takie jak BERT, stanowiły kluczowy krok naprzód w dziedzinie przetwarzania języka naturalnego. Co ważne, pozwalają one na tworzenie osadzeń słów, które przechwytują ważne informacje semantyczne o słowach w kontekście. Jednakże, jako pojedyncze podmioty, osadzenia te są trudne do zinterpretowania, a modele użyte do ich tworzenia zostały opisane jako nieprzejrzyste. Binder i koledzy zaproponowali intuicyjną przestrzeń osadzania, w której każdy wymiar opiera się na jednej z 65-podstawowych cech semantycznych. Niestety, miejsce istnieje tylko dla małego zbioru danych 535 słów, ograniczając jego zastosowanie. Poprzednie prace (Utsumi, 2018, 2020; Turton et al., 2020) pokazały, że cechy Bindera mogą być wywodzone ze statycznych osadzeń i z powodzeniem ekstrapolowane do dużego nowego słownictwa. W kolejnym kroku niniejszy artykuł pokazuje, że funkcje Bindera mogą być wywodzone z przestrzeni osadzania BERT. Zapewnia to dwie rzeczy: (1) wartości cech semantycznych pochodzących z kontekstualizowanych osadzeń słów oraz (2) wglądu w to, jak cechy semantyczne są reprezentowane na różnych warstwach modelu BERT.', 'ro': 'Modelele bazate pe arhitectura transformatorului, cum ar fi BERT, au marcat un pas crucial înainte în domeniul prelucrării limbajului natural. Mai important, ele permit crearea de încorporări de cuvinte care captează informații semantice importante despre cuvinte în context. Cu toate acestea, ca entități unice, aceste încorporări sunt dificil de interpretat și modelele folosite pentru a le crea au fost descrise ca opace. Binder și colegii au propus un spațiu intuitiv de încorporare în care fiecare dimensiune se bazează pe una dintre cele 65 de caracteristici semantice de bază. Din păcate, spațiul există doar pentru un set mic de date de 535 de cuvinte, limitând utilizările sale. Lucrarea anterioară (Utsumi, 2018, 2020; Turton et al., 2020) a arătat că caracteristicile Binder pot fi derivate din încorporări statice și extrapolate cu succes într-un vocabular nou mare. Următorul pas, această lucrare demonstrează că caracteristicile Binder pot fi derivate din spațiul de încorporare BERT. Aceasta oferă două lucruri: (1) valorile caracteristicilor semantice derivate din încorporarea contextualizată a cuvintelor și (2) perspective asupra modului în care caracteristicile semantice sunt reprezentate în diferitele straturi ale modelului BERT.', 'mn': 'Байгалийн хэл Процессорын талбар дахь чухал алхам гэсэн үг. Хамгийн чухал нь тэд үгний тухай чухал хэмжээний мэдээллийг барьж буй үгнүүд бий болгох боломж олгодог. Гэхдээ ганц бүтээлүүд шиг эдгээр бүтээлүүд нь хэцүү байдаг. Тэдгээрийг бүтээх үед хэрэглэгдсэн загварууд нь амархан гэж тодорхойлдог. Хамтрагч болон хамтрагч нар хэмжээсүүд бүр 65 төвөгтэй семантийн нэг зүйлээр суурилсан санал болгосон. Харамсалтай нь хэрэглээний хязгаарлалттай 535 үгний жижиг хэмжээний орон зай байдаг. Өмнөх ажил (Utsumi, 2018, 2020; Turton et al., 2020) нь "Binder" тодорхойлолтуудыг статистик нэвтрээс гаргаж, амжилттай шинэ үгүйлтэй болгож чадна гэдгийг харуулсан. Дараагийн алхмыг аваад, энэ цаас нь Binder-ын тодорхойлолтуудыг BERT-ын орон зайд гаргаж болно гэдгийг харуулдаг. Энэ нь хоёр зүйл гаргадаг. (1) БЕРТ загварын өөр хэлбэрээр semantic features хэрхэн харагдаж байгааг ойлгож эхэлсэн semantic feature values.', 'sr': 'Modeli bazirani na transformatorskoj arhitekturi, kao što je BERT, označili su ključni korak napred u oblasti procesa prirodnog jezika. Važno je da oni dozvoljavaju stvaranje reèi ukljuèujuæi važne semantièke informacije o reèima u kontekstu. Međutim, kao jednostavne entitete, ove komplekse su teško interpretirati, a modeli koji su koristili za njih su opišeni kao opasni. Zaveznici i kolegi su predložili intuitivno ugrađenje prostora u kojem se svaka dimenzija temelji na jednom od 65 osnovnih semantičkih karakteristika. Nažalost, prostor postoji samo za mali set podataka od 535 reči, ograničavajući korištenje. Prethodni rad (Utsumi, 2018, 2020; Turton et al., 2020) pokazao je da se karakteristike vezivača mogu izvući iz statičkih integracija i uspešno ekstrapolirati na veliki novi rečnik. Uzimajući sljedeći korak, ovaj papir pokazuje da karakteristike vezivača mogu biti izvedene iz svemira integracije BERT-a. Ovo pruža dve stvari; (1) semantičke vrijednosti karakteristike iz kontekstualizacije riječi i (2) uvjeta o tome kako se semantičke karakteristike predstavljaju preko različitih slojeva modela BERT.', 'sv': 'Modeller baserade på transformatorarkitekturen, som BERT, har markerat ett avgörande steg framåt inom området Natural Language Processing. Viktigt är att de tillåter skapandet av ordinbäddningar som fångar viktig semantisk information om ord i sammanhang. Men som enskilda entiteter är dessa inbäddningar svåra att tolka och de modeller som används för att skapa dem har beskrivits som ogenomskinliga. Binder och kollegor föreslog ett intuitivt inbäddningsutrymme där varje dimension baseras på en av 65 centrala semantiska funktioner. Tyvärr finns utrymmet bara för en liten datamängd på 535 ord, vilket begränsar dess användning. Tidigare arbete (Utsumi, 2018, 2020; Turton et al., 2020) har visat att Binderfunktioner kan härledas från statiska inbäddningar och framgångsrikt extrapoleras till ett stort nytt ordförråd. Med nästa steg visar denna uppsats att Binderfunktioner kan härledas från BERT inbäddningsutrymme. Detta ger två saker: (1) semantiska funktionsvärden som härrör från kontextuella ordinbäddningar och (2) insikter i hur semantiska funktioner representeras över BERT-modellens olika lager.', 'so': 'Modeller waxay ku saleysan dhismaha beddelka, sida BERT, waxay markey leedahay tallaabo muhiim ah oo ugu muhiimsan beerta baaritaanka afka dabiiciga ah. Si muhiim ah, waxay u oggolaan karaan abuuridda hadalka, taasoo qabta macluumaad muhiim ah oo ku saabsan hadallada hoose. Si kastaba ha ahaatee marka ay isku xal yihiin, dallaamahan waa ku adag yihiin in ay turjumaan, tusaalayaasha lagu isticmaalayona waxaa loo qoray mid aan suurtagal ahayn. Binder iyo saaxiibbadu waxay soo jeedeen goob ka mid ah meesha lagu saleeyo mid ka mid ah 65 tababar. Nasiib xumaatooyinku waxay leedahay hal macluumaad yar oo lagu qoro 535 erayo, kaas oo ku xadidaya isticmaalkiisa. Shaqo hore (Utsumi, 2018, 2020; Turton et al., 2020) wuxuu tusay in xeerarka Binder laga soo saari karo meelaha lagu soo diro oo lagu liibaaniyey hadal cusub. Markaad qaadanayso tallaabo soo socda, warqaddaas wuxuu muujiyaa in xerada BERT laga soo saari karo. taasi waxay bixiyaan laba waxyaalood; (1) Qiimaha semantika ee laga soo jeedo hadalka ku habboon iyo (2) aragtida sida xayawaanaha semantika looga jeedo sawirada kala duwan ee modelka BERT.', 'si': 'මොඩිල්ස් පරිවර්තනය විස්තර විද්\u200dයාපාරකය විස්තර, BERT වගේ, ස්වභාවික භාෂාව ප්\u200dරක්\u200dරියාපනයේ ප්\u200dරශ්නයක ප්\u200dරශ්නයෙන්ම, ඔවුන් වචන සම්බන්ධතාවක් නිර්මාණය කරන්න අවසර කරන්න පුළුවන් වචන සම්බන්ධතාවක් ගැන වැ නමුත්, එකම අයිතියක් විදිහට, මේ සංවිධානය අමාරුයි, ඒවා නිර්මාණය කරන්න භාවිත විදිහට ප්\u200dරයෝජනය කරලා ති බායින්ඩර් සහ සහයෝගියෝ ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් කළා, හැම විශේෂයක්ම 65 කොටස් සෙමැන්ටික් අවශ්\u200dයයෙ අවාසනාවෙන්න, පොඩි දත්ත සැට 535 වචනයක් විතරයි තියෙන්නේ, ඒක ප්\u200dරයෝජනය සීමාවෙන්. මුලින් වැඩ (Utsum, 2018, 2020යි; Turton et al., 2020යි) පෙන්වන්න පුළුවන් විදිහට බින්ඩර් විශේෂතාවක් ස්ථිර සංවිධානයෙන් පිළිගන්න පුළ ඊළඟ පැත්ත ගත්තොත්, මේ පැත්ත පෙන්වන්නේ බායින්ඩර් අවශ්\u200dයය BERT සම්බන්ධ කාමරයෙන් පිළිගත්ත හැක මේක දෙකක් දෙනවා; (1) සෙමැන්ටික් විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ ව', 'ta': 'BERT போன்ற மாற்று உருவத்தை அடிப்படையான மாதிரிகள் இயல்பான மொழி செயல்பாட்டின் புலத்தில் ஒரு முக்கியமான படி முன்னேற்றி குற முக்கியமானது, அவர்கள் வார்த்தைகளை உருவாக்க அனுமதிக்கும் முக்கியமான பெம்பென்டிக் தகவலை பிடிக்க அனுமதி ஆனால், ஒரே பொருளாக, இந்த பொருள்கள் விளக்க கடினமாக இருக்கும் மற்றும் அவற்றை உருவாக்க பயன்படும் மாதிரிகள் ஒற்றைப்படையா பிண்டர் மற்றும் சகோதரர் ஒரு உணர்வுள்ள இடைவெளியில் உள்ள இடைவெளியை பரிந்துரைக்கிறது ஒவ்வொரு பரிமானமும் 65 மூல குணங்களில் ஒன Unfortunately, the space only exists for a small data-set of 535 words, limiting its uses.  முந்தைய பணி அடுத்த படியை எடுத்து, இந்த காகிதத்தை காட்டுகிறது BERT உள்ளீட்டு இடைவெளியிலிருந்து பிண்டர் குணங்கள் கொண்டு வரு இது இரண்டு விஷயங்களை வழங்குகிறது; (1) பிரெட்டி மாதிரியின் வேறு அடுக்குகளில் இருந்து அரை குணங்களின் மதிப்புகள் மற்றும் (2) குறிப்புகளை எப்படி பெரும்பான்டிக் குணங்கள் பிர', 'ur': 'مڈلس ترنسفور معماری پر بنیاد رکھتے ہیں، جیسے BERT، طبیعی زبان پرسس کے کھیل میں ایک ضروری قدم آگے نشان دی گئی ہے. اہم بات ہے کہ یہ لفظوں کے پیدا کرنے کی اجازت دیتے ہیں جو لفظوں کے بارے میں اہم سیمنٹی معلومات کو پکڑتے ہیں۔ اگرچہ ان کی تعبیر کرنے کے لئے یہ انڈینگ مشکل ہے اور ان کے پیدا کرنے کے لئے استعمال کئے ہوئے مدل ناپسند ہیں۔ بائینڈر اور همکاروں نے ایک نظریہ اندازے کی جگہ پیش کیا ہے جہاں ہر اندازہ 65 کور سیمانٹی ویٹیوں پر بنیاد ہے. بدبختی، جگہ صرف 535 کلمات کے چھوٹے ڈیٹ سٹ کے لئے موجود ہے، اس کے استعمال سے محدود ہے. پہلے کام (Utsumi, 2018, 2020; Turton et al., 2020) نے دکھایا ہے کہ Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. اس کاغذ کو دکھاتا ہے کہ بائینڈر فوکتوروں کو BERT انڈینگ جگہ سے پھیر سکتا ہے۔ یہ دو چیزوں کا فائدہ دیتا ہے (1) سیمنٹی فکرت کی ارزش جو متوسط لکھی ہوئی کلمات انڈینگ اور (2) نظر میں آتی ہیں کہ سیمنٹی فکرت کس طرح BERT موڈل کے مختلف لائٹوں میں نمایش کی جاتی ہیں۔', 'vi': 'Những mẫu dựa trên kiến trúc máy biến hình, như BERT, đã đóng dấu một bước tiến quan trọng trong lĩnh vực Bản Sản Ngôn ngữ tự nhiên. Điều quan trọng là, chúng cho phép tạo ra những từ mới ghép mà nắm bắt thông tin cơ bản quan trọng về từ ngữ cảnh trong ngữ cảnh. Tuy nhiên, là những thực thể đơn lẻ, những sự ghép nối này rất khó diễn tả và các mẫu được dùng để tạo ra chúng đã được miêu tả là mờ. Binder và các đồng nghiệp đề xuất một khoảng cách cấy ghép trực tiếp nơi mỗi chiều được dựa trên một trong những tính chất ngữ pháp của lõi 65. Không may là, không gian chỉ tồn tại cho một nhóm dữ liệu nhỏ chứa Đa số phần của cô ta, hạn chế sử dụng. Những tác phẩm trước (Utsumi, 208, 2020; Rùa et al., 2020) đã cho thấy hiệu quả của Binder có thể lấy ra từ sự nhúng cố tích cực và được ngoại trừ thành một từ mới rất lớn. Làm bước tiếp theo, tờ giấy này chứng minh rằng tính năng Binder có thể bắt nguồn từ góc Tham ghép BERT. Nó cung cấp hai điều; (1) giá trị đặc trưng theo lề chiết xuất từ sự ghép các từ ngữ và (2) hiểu biết cách các tính năng ngữ pháp được đại diện qua các lớp khác nhau của mô hình BERT.', 'uz': "Comment Muhimlik bo'lsa, ularga muhim so'zlar haqida xabar soʻzlarni yaratishga ruxsat beradi. Lekin bitta obʼektlar sifatida, bu tuzuvlar tarjima qilish juda qiyin va ularni yaratish uchun foydalanuvchi modellarni o'zgartirish mumkin. Name Uzunasiz, faqat joy 535 so'zlarning kichkina maʼlumot tarkibi uchun mavjud. Oldingi ishni (Utsumi, 2018, 2020; Turton et al., 2020) koʻrsatilgan boshqaruv imkoniyatlarini static embedlaridan olib tashlash va muvaffaqiyatli yangi vosita olib tashlash mumkin. Keyingi qadam olib, bu qogʻoz bogʻ xossalarini BERT ichki boʻsh joydan olib tashlash mumkin. Bu ikkita narsa aytadi. (1) Aniqlik xossalari qiymatlari BERT modelining turli qanday semantik xossalarini koʻrsatish mumkin.", 'da': 'Modeller baseret på transformatorarkitekturen, såsom BERT, har markeret et afgørende skridt fremad inden for Natural Language Processing. Det er vigtigt, at de giver mulighed for oprettelse af ordindlejringer, der fanger vigtige semantiske oplysninger om ord i kontekst. Som enkelte enheder er disse indlejringer imidlertid vanskelige at fortolke, og de modeller, der bruges til at oprette dem, er blevet beskrevet som uigennemsigtige. Binder og kolleger foreslog et intuitivt indlejringsrum, hvor hver dimension er baseret på en af 65 kernesemantiske funktioner. Desværre findes der kun plads til et lille datasæt på 535 ord, hvilket begrænser dens anvendelse. Tidligere arbejde (Utsumi, 2018, 2020; Turton et al., 2020) har vist, at Binder funktioner kan afledes fra statiske indlejringer og med succes ekstrapoleres til et stort nyt ordforråd. Næste skridt viser dette papir, at Binder-funktioner kan udledes af BERT-indlejringspladsen. Det giver to ting: (1) semantiske egenskabsværdier afledt af kontekstualiserede ordindlejringer og (2) indsigt i, hvordan semantiske egenskaber repræsenteres på tværs af BERT-modellens forskellige lag.', 'hr': 'Modeli koji su temeljeni na arhitekturi transformer a, poput BERT, označili su ključni korak naprijed u području procesa prirodnog jezika. Važno je da oni dozvoljavaju stvaranje riječi ugrađenja koje uključuju važne semantičke informacije o riječima u kontekstu. Međutim, kao jedinstvene entitate, te integracije su teško interpretirati, a modeli koji se koriste za njih stvoriti opišeni su kao opasni. Zaveznici i kolege predložili su intuitivno ugrađenje prostora u kojem se svaka dimenzija temelji na jednom od 65 osnovnih semantičkih karakteristika. Nažalost, prostor postoji samo za mali skup podataka od 535 riječi, ograničavajući korištenje. Prethodni rad (Utsumi, 2018, 2020; Turton et al., 2020) pokazao je da se objektivi vezivača mogu izvući iz statičkih ugrađenja i uspješno ekstrapolirati na veliki novi riječ. Uzimajući sljedeći korak, ovaj papir pokazuje da karakteristike vezivača mogu proizvesti iz BERT-a ugrađenog prostora. To pruža dvije stvari; (1) vrijednosti semantičkih karakteristika iz kontekstualizacije riječi i (2) uvjeta o tome kako se semantičke karakteristike predstavljaju u raznim slojevima modela BERT.', 'id': 'Model berdasarkan arsitektur transformer, seperti BERT, telah menandai langkah penting maju dalam bidang Proses Bahasa Alam. Yang penting, mereka memungkinkan pembangunan kata yang menangkap informasi semantis penting tentang kata-kata dalam konteks. Namun, sebagai entitas tunggal, penerbangan ini sulit diterjemahkan dan model yang digunakan untuk menciptakannya telah diterangkan sebagai opak. Binder dan rekan-rekan mengusulkan ruang penerbangan intuitif dimana setiap dimensi berdasarkan salah satu dari 65 karakteristik semantis inti. Sayangnya, ruang itu hanya ada untuk set data kecil dari 535 kata, membatasi penggunaannya. Pekerjaan sebelumnya (Utsumi, 2018, 2020; Turton et al., 2020) menunjukkan bahwa ciri-ciri Binder dapat dihimpunkan dari pembangunan statis dan berhasil ekstrapolasi ke vocabulari baru yang besar. Melangkah langkah berikutnya, kertas ini menunjukkan bahwa fitur Binder dapat diproduksi dari ruang penerbangan BERT. This provides two things;  (1) nilai karakteristik semantis yang didapat dari pembangunan kata kontekstualisasi dan (2) penglihatan bagaimana karakteristik semantis dipilih melalui lapisan yang berbeda dari model BERT.', 'bg': 'Моделите, базирани на архитектурата на трансформаторите, като BERT, бележат решаваща стъпка напред в областта на обработката на естествения език. Важно е, че те позволяват създаването на вграждания на думи, които улавят важна семантична информация за думите в контекста. Въпреки това, като единични единици, тези вграждания са трудни за интерпретиране и моделите, използвани за създаването им, са описани като непрозрачни. Binder и колеги предложиха интуитивно вграждане пространство, където всяко измерение се основава на една от 65 основни семантични характеристики. За съжаление, пространството съществува само за малък набор от данни от 535 думи, което ограничава употребата му. Предишна работа (Утсуми, 2018, 2020; Туртън и др., 2020) показва, че функциите на биндер могат да бъдат извлечени от статични вграждания и успешно екстраполирани към голям нов речник. Като направим следващата стъпка, тази статия демонстрира, че функциите на свързващото устройство могат да бъдат извлечени от пространството за вграждане на BERT. Това осигурява две неща: (1) стойности на семантичните характеристики, получени от контекстуализираните вграждания на думи, и (2) прозрения за това как семантичните характеристики са представени в различните слоеве на модела BERT.', 'ko': 'transformer 구조를 바탕으로 하는 모델, 예를 들어 BERT는 자연 언어 처리 분야가 관건적인 일보를 내디뎠음을 상징한다.중요한 것은 단어가 삽입되어 상하문에 있는 단어의 중요한 의미 정보를 포착할 수 있도록 하는 것이다.그러나 하나의 실체로서, 이 삽입들은 설명하기 어려우며, 그것들을 만드는 데 사용되는 모델은 불투명하다고 묘사된다.Binder와 그의 동료들은 직관적인 삽입 공간을 제시했는데 그 중에서 각 차원은 65개의 핵심 의미 특징 중 하나를 바탕으로 한다.불행하게도 이 공간은 535글자의 작은 데이터 집합에만 존재해 사용을 제한했다.이전 작업(Utsumi, 2018년, 2020년, 터튼 등, 2020년)은 폴더 특징이 정적 삽입에서 파생되어 하나의 큰 새로운 어휘표로 성공적으로 밖으로 밀려날 수 있음을 나타냈다.다음에 본고는 버트가 끼워 넣은 공간에서 귀속 특징을 도출할 수 있음을 증명했다.이것은 두 가지 일을 제공했다.(1) 의미특징값은 어경화된 단어의 삽입에서 유래한다. (2) 의미특징이 버트모델의 서로 다른 층에서 어떻게 표시되는지 깊이 있게 이해한다.', 'nl': "Modellen gebaseerd op de transformatorarchitectuur, zoals BERT, hebben een cruciale stap voorwaarts gezet op het gebied van Natural Language Processing. Belangrijk is dat ze het maken van woord embeddings mogelijk maken die belangrijke semantische informatie over woorden in context vastleggen. Als afzonderlijke entiteiten zijn deze insluitingen echter moeilijk te interpreteren en de modellen die worden gebruikt om ze te maken zijn beschreven als ondoorzichtig. Binder en collega's stelden een intuïtieve insluitruimte voor waarbij elke dimensie is gebaseerd op een van 65 kernsemantische kenmerken. Helaas bestaat de ruimte alleen voor een kleine dataset van 535 woorden, waardoor het gebruik ervan beperkt wordt. Eerder onderzoek (Utsumi, 2018, 2020; Turton et al., 2020) heeft aangetoond dat Binder-functies kunnen worden afgeleid van statische embeddings en succesvol geëxtrapoleerd naar een grote nieuwe woordenschat. In de volgende stap demonstreert dit artikel dat Binder-functies kunnen worden afgeleid van de BERT-insluitruimte. Dit biedt twee dingen: (1) semantische merkwaarden afgeleid van contextualiseerde woordinsluitingen en (2) inzichten in hoe semantische kenmerken worden weergegeven in de verschillende lagen van het BERT-model.", 'tr': 'BERT ýaly transformer arhitektura daýanýan nusgalar, tebigy dil işleýişinde örän möhüm adım öňünde bellenýärler. Öň wajyp şu wagt, semantik sözler barada möhüm maglumat alan sözler guralyşyna rugsat berýärler. Ýöne ýeke bir zat bolsa, bu düzmekler çykarmak kyn we olary bejermek üçin ullanýan nusgalar näbelli bolup görkezildi. Başlyg we meslektaşlar 65 çekim semantik karakterleriň birine dayanan bir taýýarlama ýerini teklip etdiler. Gynansakda, seleň diňe 535 sözler üçin kiçi maglumaty gabdalýar. Öňki işi Indiki adı çykmak üçin bu kagyz baýramyň özellikleri BERT baglançy seleňden çykyp biljekdigini görkez. Bu iki zat temin edýär; (1) Semantik karakterler BERT modeliniň farklı katlarynda nähili semantik karakterlerden çykýan düzümleri.', 'de': 'Modelle, die auf der Transformatorarchitektur basieren, wie BERT, haben im Bereich der Natural Language Processing einen entscheidenden Schritt nach vorne gemacht. Wichtig ist, dass sie die Erstellung von Wort-Einbettungen ermöglichen, die wichtige semantische Informationen über Wörter im Kontext erfassen. Als einzelne Entitäten sind diese Einbettungen jedoch schwer zu interpretieren und die Modelle, mit denen sie erstellt wurden, wurden als undurchsichtig beschrieben. Binder und Kollegen schlugen einen intuitiven Einbettungsraum vor, in dem jede Dimension auf einem der 65-Kern-semantischen Merkmale basiert. Leider ist der Platz nur für einen kleinen Datensatz von 535 Wörtern vorhanden, was die Verwendung einschränkt. Bisherige Arbeiten (Utsumi, 2018, 2020; Turton et al., 2020) haben gezeigt, dass Binder-Features aus statischen Einbettungen abgeleitet und erfolgreich auf ein großes neues Vokabular extrapoliert werden können. Im nächsten Schritt wird gezeigt, dass Binder-Features aus dem BERT-Einbettungsraum abgeleitet werden können. Dies bietet zwei Dinge: (1) semantische Merkmalswerte, die aus kontextualisierten Worteinbettungen abgeleitet werden, und (2) Erkenntnisse darüber, wie semantische Merkmale über die verschiedenen Ebenen des BERT-Modells dargestellt werden.', 'af': "Modele gebaseer op die transformeerder arkitektuur, soos BERT, het 'n kruipende stap voort gemerk in die veld van Natuurlike Taal Prosessering. Onderwys, hulle laat toe die skepping van woord inbêding wat belangrik semantiese inligting oor woorde in konteks vang. Alhoewel, as enkele entiteite, is hierdie inbêding moeilik om te interpreteer en die modele wat gebruik word om hulle te skep het beskrywe as ongelukkige. Binder en kollegas het 'n intuitief inbetering ruimte voorgestel waar elke dimensie gebaseer is op een van 65 kern semantiese funksies. Ongelukkig, die spasie bestaan slegs vir 'n klein data- set van 535 woorde, beperk sy gebruike. Vorige werk (Utsumi, 2018, 2020; Turton et al., 2020) het vertoon dat Binder funksies kan afgelei word van statiese inbêdings en suksesvol uitgelei word na 'n groot nuwe woordeboek. As die volgende stap neem, hierdie papier wys dat Binder funksies kan van die BERT inbêring spasie afgelei word. Dit verskaf twee dinge; (1) semantiese funksie waardes afgeleide van contextualiseerde woord inbettings en (2) inwerkings in hoe semantiese funksies verteenwoordig word deur die verskillende lagte van die BERT model.", 'sw': 'Modeli yenye msingi wa ujenzi wa mabadiliko kama BERT, imeonyesha hatua muhimu mbele katika uwanja wa mchakato wa lugha ya asili. Kimuhimu, wanaruhusu kutengeneza ujumbe wa neno ambalo linachukua taarifa muhimu kuhusu maneno katika muktadha. Hata hivyo, kama vyombo pekee, vifaa hivi vina vigumu kutafsiri na mifano iliyotumiwa kutengeneza vimeelezwa kama haina usawa. Bindera na wenzake walipendekeza nafasi yenye ufanisi wa kuingia ambapo kila upeo unajikita na moja ya vipengele vya sekunde 65. Kwa bahati mbaya, nafasi pekee inapo kwa seti ya takwimu ndogo ya maneno 535, inayozuia matumizi yake. Previous work (Utsumi, 2018, 2020; Turton et al., 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary.  Kuchukua hatua inayofuata, gazeti hili linaonyesha kuwa vipengele vya Binder vinaweza kutengenezwa kutoka kwenye anga la BERT. Hii inatoa vitu viwili; (1) Takwimu za kigaidi zinazotokana na maneno yanayotumika na (2) ya kuona jinsi tabia za semanti zinavyowakilishwa katika vipande tofauti vya mifano ya BERT.', 'sq': "Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing.  Më e rëndësishme, ato lejojnë krijimin e përfshirjeve të fjalëve që përfshijnë informacion të rëndësishëm semantik rreth fjalëve në kontekst. Megjithatë, si njësi të vetme, këto përfshirje janë të vështira për të interpretuar dhe modelet e përdorura për t'i krijuar ato janë përshkruar si opake. Binder dhe kolegët propozuan një hapësirë intuitive të përfshirjes ku çdo dimension bazohet në një nga 65 karakteristikat kryesore semantike. Fatkeqësisht, hapësira ekziston vetëm për një grup të vogël të dhënash prej 535 fjalësh, duke kufizuar përdorimin e saj. Puna e mëparshme (Utsumi, 2018, 2020; Turton et al., 2020) ka treguar se funksionet e Binder mund të nxirren nga përfshirjet statike dhe të ekstrapolohen me sukses në një fjalor të madh të ri. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space.  Kjo jep dy gjëra: (1) vlerat e karakteristikave semantike të nxjerra nga përfshirjet kontekstuale të fjalëve dhe (2) pamje se si karakteristikat semantike janë përfaqësuar nëpër nivelet e ndryshme të modelit BERT.", 'am': 'የመለወጥ አካውንት እንደዚህ BERT በመሠረት ላይ የተመሳሳይ ሞዴል በአዳራዊ ቋንቋ በተቃውሞ እርሻ ውስጥ የግድ እርምጃን አስታወቀ፡፡ Importantly, they allow the creation of word embeddings that capture important semantic information about words in context.  ነገር ግን አንዲት አካባቢ ሆኖ፣ እነዚህ አካባቢዎች ለመተረጉም አስቸጋሪ ናቸው እናም ለመፍጠር የሚጠቀሙት ምሳሌዎች በጥያቄ ነው፡፡ አዲስ እና ጓደኞችም የ65 የሆኑን የስሜት ምርጫዎች በመሠረት ላይ የሚደረገውን ስፍራን አቀረቡ፡፡ በርግጥ፣ ቦታ በ535 ቃላት ለትንሽ ዳታ-ማተሚያ ብቻ ነው፡፡ የቀድሞው ሥራ (Utsumi, 2018, 2020; ቱርton et al., 2020) እንዳሳየው የቢንዴር ምርጫዎች ከstatic embedding እና ወደ ትልቅ አዲስ ቃላት ማሳየት ይችላል፡፡ የሚቀጥለውን ደረጃዎች በመስጠት፣ ይህች ገጽ የቢንደር ምርጫዎች BERT ከክፍለ ስፍራው እንዲወስዱ ያሳያል፡፡ ይሄ ሁለት ነገሮች ነው:: (1) በተለየ BERT model ላይ እንዴት የተለየ የsemantic የፊደል ግምገማዎች እና (2) ማስታወቂያ የደረጃ ምርጫዎች እንዴት እንደምታዩ ነው፡፡', 'fa': 'Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing. مهم است، آنها اجازه می دهند که ایجاد کردن کلمه\u200cهایی که اطلاعات semantic مهم در مورد کلمه\u200cها در محیط بگیرند. با این حال، به عنوان یک تنظیم، این تنظیم ها برای تعبیر کردن سخت است و مدل های استفاده برای ایجاد آنها به عنوان غیرقابل توصیف شده است. رابطه\u200cکننده و همکاران یک فضای رابطه\u200cای را پیشنهاد کردند که هر اندازه بر روی یکی از 65 ویژه\u200cهای semantic مرکزی بنیاد می\u200cآید. متاسفانه، فضا فقط برای یک مجموعه داده کوچک از 535 کلمه وجود دارد، که از استفاده\u200cهای آن محدود می\u200cکند. کارهای قبلی (Utsumi, 2018, 2020; Turton et al., 2020) نشان داده است که ویژه\u200cهای Binder می\u200cتوانند از جمع\u200cآوری\u200cهای استاندیکی و به موفقیت به یک کلمه جدید استفاده شود. از قدم بعدی، این کاغذ نشان می\u200cدهد که ویژه\u200cهای بیندر می\u200cتوانند از فضای ابتدایی BERT برداشته شود. این دو چیز را می\u200cدهد، (۱) ارزش ویژه\u200cهای semantic derived from contextualized word embedding and (2) insights into how semantic features are represented across the layers of the BERT model.', 'hy': 'Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing.  Կարևոր է, որ նրանք թույլ են տալիս ստեղծել բառերի ներդրումներ, որոնք ներառում են խոսքերի մասին կարևոր սեմանտիկ տեղեկատվություն կոնտեքստում: Այնուամենայնիվ, որպես միակ միավոր, այս ներդրումները դժվար է մեկնաբանել, և նրանց ստեղծման մոդելները նկարագրվել են որպես թափանցիկ: Բինդերն ու գործընկերները առաջարկեցին ինտուիտիվ ներգրավման տարածք, որտեղ յուրաքանչյուր չափով հիմնված է 65 հիմնական սեմանտիկ հատկություններից մեկի վրա: Դժբախտաբար, տիեզերքը գոյություն ունի միայն 535 բառի փոքր տվյալներ, որոնք սահմանափակում են դրա օգտագործումը: Անցյալ աշխատանքը (Ուսումի 2018, 2020, Թուրթոն և այլն., 2020) ցույց է տվել, որ Բինդերի հատկությունները կարող են ստատիկ ներդրումներից վերածվել և հաջողությամբ վերածվել նոր մեծ բառարանի: Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space.  Սա երկու բան է տրամադրում: (1) սեմանտիկ հատկանիշների արժեքները, որոնք ստացվում են կոնտեքստիալ բառերի ներդրումներից և (2) հասկացություններից, թե ինչպես են սեմանտիկ հատկանիշները ներկայացված BER մոդելի տարբեր շերտերում:', 'az': 'BERT kimi transformer arhitektüsünə dayanan modellər təbiətli dil işləməsi sahəsində çox önemli adım göstərdilər. Önemli olaraq, sözlər haqqında möhüm semantik məlumatları daxil edən sözlər yaratmasına izin verirlər. Ancaq tək entitlər kimi, bu inşallar təfsil etmək çətin və onları yaratmaq üçün kullanılan modellər necə görünür. Bağlayıcı və yoldaşlar, hər ölçü 65 nütfədə semantik özelliklərdən birinə dayanan intuitiv bir yerə təklif etdilər. Necə olaraq, yer yalnız 535 sözlərin kiçik məlumat qurulması üçün mövcuddur. Önceki işin (Utsumi, 2018, 2020; Turton et al., 2020) Binder özelliklərinin statik inşallardan al ınmasını və böyük yeni sözlərə müvəffəq edilməsini göstərdi. Növbəti adımı almaq üçün bu kağıt bağlayıcı xüsusiyyətləri BERT içərisindən alınabilir. Bu iki şey təmin edir. (1) BERT modelinin müxtəlif səviyyələrində semantik xüsusiyyətlərinin necə göstərilməsini təsdiqləndirmək üçün müxtəlif sözlərin içərisindən və (2) nəzərlərindən alınan semantik xüsusiyyətlər.', 'bs': 'Modeli bazirani na transformatorskoj arhitekturi, poput BERT, označili su ključni korak naprijed u oblasti procesa prirodnog jezika. Važno je da oni dozvoljavaju stvaranje riječi ugrađenja koje uhvate važne semantičke informacije o riječima u kontekstu. Međutim, kao jednostavne entitate, ove ugrađenje je teško interpretirati, a modeli koji su koristili za njih su opišeni kao opasni. Zaveznici i kolegi su predložili intuitivno ugrađenje prostora u kojem se svaka dimenzija temelji na jednom od 65 osnovnih semantičkih karakteristika. Nažalost, prostor postoji samo za mali skup podataka od 535 riječi, ograničavajući korištenje. Prethodni rad (Utsumi, 2018, 2020; Turton et al., 2020) pokazao je da se karakteristike vezivača mogu izvući iz statičkih ugrađenja i uspješno ekstrapolirati na veliki novi rečnik. Uzimajući sljedeći korak, ovaj papir pokazuje da karakteristike vezivača mogu biti izvedene iz svemira integracije BERT-a. To pruža dvije stvari. (1) semantičke vrijednosti karakteristike iz kontekstualizacije riječi i (2) uvjeta o tome kako se semantičke karakteristike predstavljaju preko različitih slojeva modela BERT.', 'ca': "Models basats en l'arquitectura del transformador, com BERT, han marcat un pas crucial cap a endavant en el camp del processament de llenguatges naturals. El que és important és que permeten crear integracions de paraules que captin informació semàntica important sobre paraules en el context. Tanmateix, com a entitats únices, aquestes incorporacions són difícils d'interpretar i els models utilitzats per crear-les han estat descrits com opacs. Binder i col·legues van proposar un espai intuïtiu d'incorporació on cada dimensió es basa en una de 65 característiques semàntiques fonamentals. Malauradament, l'espai només existeix per a un petit conjunt de dades de 535 paraules, limitant els seus usos. La feina anterior (Utsumi, 2018, 2020; Turton et al., 2020) ha demostrat que les característiques de Binder es poden derivar d'incorporacions estatiques i extrapolar amb èxit a un gran nou vocabulari. En el següent pas, aquest paper demostra que les característiques de Binder es poden derivar de l'espai d'incorporació BERT. Això proporciona dues coses: (1) valors de característiques semàntiques derivats d'incorporacions de paraules contextualitzades i (2) vistes sobre com es representan característiques semàntiques a través de les diferents capes del model BERT.", 'cs': 'Modely založené na architektuře transformátoru, jako je BERT, představily klíčový krok vpřed v oblasti zpracování přirozeného jazyka. Důležité je, že umožňují vytvářet slovní vložení, které zachycují důležité sémantické informace o slovech v kontextu. Nicméně jako jednotlivé entity jsou tato vložení obtížně interpretovatelná a modely použité k jejich tvorbě byly popsány jako neprůhledné. Binder a kolegové navrhli intuitivní vkládací prostor, kde každá dimenze je založena na jedné ze 65 jádrových sémantických vlastností. Bohužel prostor existuje pouze pro malou datovou sadu 535 slov, což omezuje její použití. Předchozí práce (Utsumi, 2018, 2020; Turton et al., 2020) ukázaly, že funkce Binderu lze odvodit ze statických vložení a úspěšně extrapolovat do velké nové slovní zásoby. V dalším kroku tento příspěvek demonstruje, že funkce pojiva lze odvodit z prostoru pro vložení BERT. To poskytuje dvě věci: (1) hodnoty sémantických prvků odvozené z kontextualizovaných vložení slov a (2) poznatky o tom, jak jsou sémantické prvky reprezentovány napříč různými vrstvami modelu BERT.', 'bn': 'স্বাভাবিক ভাষা প্রক্রিয়ার প্রক্রিয়ার ক্ষেত্রে পরিবর্তনের কাঠামোর উপর ভিত্তিক মডেলগুলো প্রকৃত ভাষার প্রক্রিয়ার ক্ষেত গুরুত্বপূর্ণ, তারা শব্দের বিভিন্ন সৃষ্টির অনুমতি দেয় যারা প্রেক্ষাপটে শব্দ সম্পর্কে গুরুত্বপূর্ তবে একক প্রতিষ্ঠান হিসেবে এই প্রবেশগুলো ব্যাখ্যা করতে কঠিন এবং তাদের সৃষ্টি করতে ব্যবহৃত মডেলগুলো অযোগ্য হিসেবে বর্ণ বাইন্ডার এবং সহকর্মীরা একটি উল্লেখযোগ্য স্থানের প্রস্তাব করেছেন যেখানে প্রত্যেক মাত্রা ৬৫ ভূমিকা সেমেন্টিক বৈশিষ্ দুর্ভাগ্যবশত, শুধুমাত্র ৫৫ শব্দের একটি ছোট তথ্য সেটের জন্য এই স্থান আছে, যা তার ব্যবহার সীমাবদ্ধ করে। পূর্ববর্তী কাজ (উটসুমি, ২০১৮, ২০২০০; টার্টন এন্ট এল, ২০২০০) দেখিয়েছেন যে বিন্দারের বৈশিষ্ট্যাবলীর বৈশিষ্ট্য থেকে পাওয়া যাবে এবং সফল নতুন শব্দভ পরবর্তী পদক্ষেপ নিয়ে এই পত্রিকাটি প্রদর্শন করে যে বিন্দারের বৈশিষ্ট্য বিবের্টের বিভিন্ন স্থান থেকে উদ্ধার করা  এটা দুই জিনিস প্রদান করে; (1) বিবেরেট মডেলের বিভিন্ন স্তরের বিভিন্ন স্তর থেকে সেমেন্টিক বৈশিষ্ট্যাবলীর মূল্য এবং (২) দৃষ্টিভঙ্গিতে দেখা যাচ্ছে।', 'et': 'Trafo arhitektuuril põhinevad mudelid, näiteks BERT, on tähistanud olulist sammu edasi looduskeele töötlemise valdkonnas. Oluline on see, et need võimaldavad luua sõnade manustamist, mis koguvad olulist semantilist teavet sõnade kohta kontekstis. Kuid üksikute üksustena on neid manustamisi raske tõlgendada ja nende loomiseks kasutatud mudeleid on kirjeldatud läbipaistmatutena. Binder ja kolleegid pakkusid välja intuitiivse manustamisruumi, kus iga dimensioon põhineb ühel 65-st semantilisest funktsioonist. Kahjuks on ruum olemas ainult väikesele 535-sõnalisele andmekogumile, mis piirab selle kasutamist. Varasemad tööd (Utsumi, 2018, 2020; Turton jt., 2020) on näidanud, et Binderi funktsioone saab tuletada staatilistest manustamistest ja edukalt ekstrapoleerida suurele uuele sõnavarale. Järgmise sammu võttes näidatakse käesolevas dokumendis, et sideme funktsioone saab tuletada BERT manustamisruumist. See annab kaks asja: (1) semantiliste omaduste väärtused, mis tulenevad kontekstipõhistest sõnade manustamisest, ja (2) ülevaade semantiliste omaduste esindamisest BERTi mudeli eri kihtides.', 'fi': 'Muuntajien arkkitehtuuriin perustuvat mallit, kuten BERT, ovat merkinneet ratkaisevan askeleen eteenpäin luonnollisen kielen prosessoinnissa. Tärkeää on, että ne mahdollistavat sanaupotusten luomisen, joka tallentaa tärkeää semanttista tietoa sanoista kontekstissa. Yksittäisinä kokonaisuuksina näitä upotuksia on kuitenkin vaikea tulkita ja niiden luomiseen käytetyt mallit on kuvattu läpinäkymättömiksi. Binder ja kollegat ehdottivat intuitiivista upotustilaa, jossa jokainen ulottuvuus perustuu yhteen 65 keskeisestä semanttisesta ominaisuudesta. Valitettavasti tila on olemassa vain pienelle 535 sanan tietojoukolle, mikä rajoittaa sen käyttöä. Aiempi työ (Utsumi, 2018, 2020; Turton et al., 2020) on osoittanut, että Binder-ominaisuudet voidaan johtaa staattisista upotuksista ja onnistuneesti ekstrapoloida suureen uuteen sanastoon. Seuraavassa vaiheessa tämä artikkeli osoittaa, että Binder-ominaisuudet voidaan johtaa BERT-upotustilasta. Tässä on kaksi asiaa: (1) kontekstuaalisista sanaupotuksista johdetut semanttisten ominaisuuksien arvot ja (2) näkemykset siitä, miten semanttiset ominaisuudet ovat edustettuina BERT-mallin eri kerroksissa.', 'sk': 'Modeli, ki temeljijo na transformatorski arhitekturi, kot je BERT, so pomenili ključen korak naprej na področju obdelave naravnega jezika. Pomembno je, da omogočajo ustvarjanje besednih vdelav, ki zajemajo pomembne semantične informacije o besedah v kontekstu. Vendar pa je te vdelave kot posamezne entitete težko razlagati, modeli, ki se uporabljajo za njihovo ustvarjanje, pa so opisani kot neprozorni. Binder in kolegi so predlagali intuitiven vgradni prostor, kjer vsaka dimenzija temelji na eni od 65 osrednjih semantičnih značilnosti. Na žalost prostor obstaja le za majhen podatkovni nabor 535 besed, kar omejuje njegovo uporabo. Prejšnje delo (Utsumi, 2018, 2020; Turton et al., 2020) je pokazalo, da je mogoče funkcije Binder izpeljati iz statičnih vdelav in uspešno ekstrapolirati v velik nov besednjak. Če naredimo naslednji korak, ta dokument dokazuje, da je mogoče funkcije vezave izpeljati iz prostora za vdelavo BERT. To zagotavlja dve stvari: (1) vrednosti semantičnih značilnosti, ki izhajajo iz kontekstualnih vključitev besed, in (2) vpogledi v to, kako so semantične značilnosti predstavljene v različnih plasteh modela BERT.', 'jv': 'model sing dimulai karo architecture transformer, kaya BERT, dadi wis diapakan kelompok sing berarti ning arep Perusahaan Pribadir Manual Awak dhéwé, wong iso nggawe akeh stiftar word embedding politenessoffpolite, "), and when there is a change ("assertivepoliteness politenessoffpolite"), and when there is a change ("assertivepoliteness item-set Laptop" and "Desktop politenessoffpolite"), and when there is a change ("assertivepoliteness Iki ngewehke duluran pangan; iki (1) semanti-caret barang gambar kelas contextual', 'ha': "Modellu na daidaita matsayin transformer, kamar BERT, sun ƙayyade wata matsayi mai muhimu gaba a gaba cikin field da Cikakken aiki na Natural. Importantly, they allow the creation of word embeddings that capture important semantic information about words in context.  Kayya, amma, kamar duk abu guda, za'a buƙata masu fassarawa kuma an bayyana misãlai da ake amfani da su da za'a ƙiƙira su kamar ba'a so ba. Binder da abõkan aiki sun buɗa wani fili na fitarwa da za'a sami duk dimiyanci a kan wani na 65 na sifilati. Babu'am, filin aiki na ƙunsa da amfani da shi kawai don a daidaita matsayin bayani na 535. @ info: status Yi motsa na gaba gaba, wannan takardar na nuna cewa za'a iya samar da tsarin Binder daga filin mai fitarwa na BERT. Wannan yana da abu biyu. (1) Kima masu daidaita na semantiki wanda aka mottar daga maganar da aka shigar da shi a cikin kwanan da (2) na gannai a kan yadda aka nuna masu tsarin semantic cikin duk abun-zanen motel na BERT.", 'he': 'מודלים המבוססים על הארכיטקטורה של המעבר, כמו BERT, סימנים צעד קריטי קדימה בתחום התהליך השפה הטבעית. חשוב, הם מאפשרים ליצור קישורים מילים שמכילים מידע סמנטי חשוב על מילים בקשר. בכל אופן, כישויות בודדות, קשה לפרש את התכניות הללו, והדוגמנים שנמשכו ליצור אותן תוארו כחופשיים. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features.  למרבה הצער, המרחב קיים רק עבור קבוצת מידע קטנה של 535 מילים, שמגבלת את השימושים שלו. העבודה הקודמת (Utsumi, 2018, 2020; Turton et al., 2020) הראה שהתכונות Binder יכולות להיות מווצאות ממערכות סטטיות ובהצלחה להוציא מילים חדשים גדולים. לקחת את הצעד הבא, העבודה הזו מראה כי תכונות Binder יכולים להיות מוצאים מרחב הקליטה BERT. זה מספק שני דברים; (1) ערכים של תכונות סמנטיות שנוצרו ממערכות מילים קונטוקטואליזציות (contextualized word embeddings) ו (2) תובנות על איך תכונות סמנטיות מייצגות ברחבי שכבות שונות של מודל BERT.', 'bo': 'མ་དབྱིབས་བཟོ་བརྩོན་གཞུང་གི་བཟོ་བཅོས་གཞུང་ལ་གཞི་བརྗོད་པ། དཔེར་ན BERT(BERT)དང་དེ་ལྟ་བུ་ནུས་པ་སྐད་ཀྱི་ལས་སྦྱོར་ནང་དུ་གལ གལ་ཆེ་ཆོས་རེད། ཁོང་ཚོས་ཡིག་གེ་སྒྲིག་འཇུག་བྱས་པའི་ཚིག་རྟགས་ཀྱི་གནས་ཚུལ་མང་ཙམ་འཛིན་གྱི་ཡོད་པ་དང་ ཡིན་ནའང་། ཨ་མ་གཅིག་གི་དབང་ཆ་མཚུངས་ཞིག་ལ་སྒྲིག་འཛུགས་འདི་དག་གནད་མེད་སྤྲོད་ཀྱི་རྣམ་པ་ཞིག་བཤད་ཀྱི་ཡོད། Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. ཉེ་ཆུང་ནི་བར་སྟོང་དེ་ཡིན་པ་མིན་ན་སྒྲིག་ཆ་ཆུང་ཅིག་གི་ནང་དུ་ཡོད་པ་རེད། སྔོན་གྱི་ལས་ཀ (Utsumi, 2018; Turton et al., 2020) ཡིད་པས། Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. འདིས་དངོས་པོ་གཉིས་བྱིན་པ་རེད། (1) semantic feature values derived from contextualised word embeddings and (2) insights into how semantic features are represented across the different layers of the BERT model.'}
{'en': 'An Overview of Uncertainty Calibration for  Text Classification  and the Role of Distillation', 'fr': "Vue d'ensemble de l'étalonnage des incertitudes pour la classification de texte et du rôle de la distillation", 'ar': 'نظرة عامة على معايرة الارتياب في تصنيف النص ودور التقطير', 'pt': 'Uma visão geral da calibração de incerteza para classificação de texto e o papel da destilação', 'es': 'Descripción general de la calibración de incertidumbre para la clasificación de textos y el papel de la destilación', 'ja': 'テキスト分類の不確実性キャリブレーションと蒸留の役割の概要', 'ru': 'Обзор калибровки неопределенности для классификации текста и роли дистилляции', 'zh': '文本之不确定性校准概述及蒸馏之用也', 'hi': 'पाठ वर्गीकरण और आसवन की भूमिका के लिए अनिश्चितता अंशांकन का एक सिंहावलोकन', 'ga': 'Forbhreathnú ar Chalabrú Éiginnte maidir le hAicmiú Téacs agus Ról an Driogtha', 'ka': 'ტექსტის კლასიფიკაციის და განსხვავების პროლის დანახვა', 'el': 'Μια επισκόπηση της βαθμονόμησης αβεβαιότητας για την ταξινόμηση κειμένου και το ρόλο της απόσταξης', 'it': "Una panoramica della calibrazione dell'incertezza per la classificazione del testo e il ruolo della distillazione", 'kk': 'Мәтін классификациялау және жетілдірудің рөлі', 'lt': 'Teksto klasifikavimo ir distiliavimo vaidmens neapibrėžtumo kalibravimo apžvalga', 'ms': 'Name', 'mk': 'Преглед на калибарацијата на несигурноста за класификацијата на текстот и улогата на дистилацијата', 'ml': 'പദാവലി ക്ലാസിഷന്\u200d വേണ്ടി വിശ്വസിക്കുന്നതിനുള്ള വിലാസങ്ങള്\u200d', 'mt': 'Ħarsa ġenerali lejn il-Kalibrazzjoni ta’ Inċertezza għall-Klassifikazzjoni tat-Test u r-Rwol tad-Distillazzjoni', 'hu': 'A szövegosztályozás bizonytalansági kalibrálásának áttekintése és a desztilláció szerepe', 'no': 'Name', 'mn': 'Текст классификацийн үнэмшилгүй калибрын тухай харах', 'pl': 'Przegląd kalibracji niepewności dla klasyfikacji tekstu i roli destylacji', 'ro': 'O prezentare generală a calibrării incertitudinii pentru clasificarea textelor și rolul distilării', 'sr': 'Pregled kalibracije nesigurnosti za klasifikaciju teksta i ulogu destilacije', 'si': 'Name', 'so': "Ka fiiriso kalibration la'aanta xaqiiqa ah", 'ta': 'Name', 'sv': 'En översikt över osäkerhetskalibrering för textklassificering och destillationens roll', 'ur': 'Text Classification and the Role of Distillation', 'uz': 'Name', 'vi': 'Một dư luận về quy trình chưa rõ ràng cho việc đánh dấu nhãn và vai trò phân tách', 'bg': 'Преглед на калибрирането на несигурността при класификацията на текста и ролята на дестилацията', 'nl': 'Een overzicht van onzekerheidskalibratie voor tekstclassificatie en de rol van destillatie', 'hr': 'Pregled kalibracije nestabilnosti za klasifikaciju teksta i ulogu destilacije', 'da': 'En oversigt over usikkerhedskalibrering for tekstklassificering og destillationens rolle', 'de': 'Ein Überblick über die Unsicherheitskalibrierung für die Textklassifikation und die Rolle der Destillation', 'id': 'Sebuah Overview of Uncertainty Calibration for Text Classification and the Role of Distillation', 'ko': '텍스트 분류의 불확정도 교정 종합 및 증류 작용', 'sw': 'Mtazamo wa Calibration for Signature Text Classification and Role of Disillusion', 'tr': 'Metin Sınıflandyrmasy we Ýygyrymyň Roli', 'fa': 'نظرۀ کلیسازی نایقین برای کلیسازی متن و نقش نابودی', 'af': 'Name', 'hy': 'Comment', 'bn': 'Name', 'sq': 'Një përmbledhje e kalibrimit të pasigurisë për klasifikimin e tekstit dhe rolin e distilimit', 'am': 'Calibration', 'az': 'Mətn Klasifikasyonu və Bölümünün Rulluğunu Qıyamətsiz Kalibrasını Görüş', 'cs': 'Přehled kalibrace nejistoty pro klasifikaci textu a role destilace', 'bs': 'Pregled kalibracije nestabilnosti za klasifikaciju teksta i ulogu destilacije', 'et': 'Ülevaade ebakindluse kalibreerimisest teksti klassifitseerimisel ja destilleerimise rollist', 'fi': 'Yleiskatsaus epävarmuuden kalibroinnista tekstiluokituksessa ja tislauksen roolista', 'ca': "Una visió general de la calibració d'incertesa per a la classificació del text i el paper de la destilació", 'jv': 'ProgressBar', 'he': 'Name', 'sk': 'Pregled kalibracije negotovosti za klasifikacijo besedila in vloge destilacije', 'ha': '@ action', 'bo': 'An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation'}
{'en': 'Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these  systems  are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of  complexity . In this work, we present a systematic study of a few of these  methods . Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as  ensembles , temperature scaling). Then, we empirically illustrate a connection between  distillation  and  calibration . We view  distillation  as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on  distillation  with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include  ablations  to understand the usefulness of components of our proposed method and examine the transferability of  calibration  via  distillation .', 'ar': 'حققت التطورات الحديثة في أنظمة البرمجة اللغوية العصبية ، ولا سيما نموذج التدريب المسبق والضبط الدقيق ، نجاحًا كبيرًا في الدقة التنبؤية. ومع ذلك ، فإن هذه الأنظمة عادة لا يتم معايرتها بشكل جيد لعدم اليقين خارج الصندوق. تم اقتراح العديد من طرق إعادة المعايرة في الأدبيات لتقدير عدم اليقين التنبئي ومعايرة مخرجات النموذج ، بدرجات متفاوتة من التعقيد. في هذا العمل ، نقدم دراسة منهجية لبعض هذه الأساليب. بالتركيز على مهمة تصنيف النص والنماذج اللغوية الكبيرة المحددة مسبقًا ، نظهر أولاً أن العديد من النماذج الدقيقة لم يتم معايرتها بشكل جيد خارج الصندوق ، خاصةً عندما تأتي البيانات من إعدادات خارج المجال. بعد ذلك ، نقارن فعالية عدد قليل من طرق إعادة المعايرة المستخدمة على نطاق واسع (مثل المجموعات وقياس درجة الحرارة). بعد ذلك ، نوضح بشكل تجريبي وجود علاقة بين التقطير والمعايرة. نحن ننظر إلى التقطير على أنه مصطلح تنظيم يشجع نموذج الطالب على إخراج أوجه عدم اليقين التي تتطابق مع نموذج المعلم. من خلال هذه الرؤية ، نطور طرقًا بسيطة لإعادة المعايرة تعتمد على التقطير بدون تكلفة إضافية لوقت الاستدلال. نظهر في معيار GLUE أن أساليبنا البسيطة يمكن أن تحقق أداء معايرة تنافسيًا خارج المجال (OOD). نهج أكثر تكلفة. أخيرًا ، نقوم بتضمين عمليات الاجتثاث لفهم فائدة مكونات طريقتنا المقترحة وفحص قابلية نقل المعايرة عبر التقطير.', 'es': 'Los avances recientes en los sistemas de PNL, en particular el paradigma de preentrenamiento y ajuste fino, han logrado un gran éxito en la precisión predictiva. Sin embargo, estos sistemas no suelen estar bien calibrados para la incertidumbre de forma inmediata. En la literatura se han propuesto muchos métodos de recalibración para cuantificar la incertidumbre predictiva y calibrar los resultados del modelo, con diversos grados de complejidad. En este trabajo, presentamos un estudio sistemático de algunos de estos métodos. Centrándonos en la tarea de clasificación de textos y en los modelos lingüísticos grandes preentrenados y ajustados, primero mostramos que muchos de los modelos ajustados no están bien calibrados de forma inmediata, especialmente cuando los datos provienen de configuraciones fuera del dominio. A continuación, comparamos la eficacia de algunos métodos de recalibración ampliamente utilizados (como conjuntos, escala de temperatura). A continuación, ilustramos empíricamente una conexión entre la destilación y la calibración. Consideramos la destilación como un término de regularización que alienta al modelo estudiantil a generar incertidumbres que coincidan con las de un modelo docente. Con esta visión, desarrollamos métodos de recalibración sencillos basados en la destilación sin coste adicional de tiempo de inferencia. En el punto de referencia GLUE demostramos que nuestros métodos simples pueden lograr un rendimiento competitivo de calibración fuera del dominio (OOD) frente a enfoques más costosos. Por último, incluimos ablaciones para comprender la utilidad de los componentes del método propuesto y examinar la transferibilidad de la calibración mediante destilación.', 'fr': "Les récents progrès dans les systèmes de PNL, notamment le paradigme de préentraînement et de réglage fin, ont connu un grand succès en termes de précision prédictive. Cependant, ces systèmes ne sont généralement pas bien calibrés en raison de l'incertitude prête à l'emploi. De nombreuses méthodes de recalibrage ont été proposées dans la littérature pour quantifier l'incertitude prédictive et calibrer les sorties du modèle, avec divers degrés de complexité. Dans ce travail, nous présentons une étude systématique de quelques-unes de ces méthodes. En nous concentrant sur la tâche de classification de texte et sur les grands modèles linguistiques préentraînés affinés, nous montrons d'abord que bon nombre des modèles affinés ne sont pas correctement calibrés dès le départ, en particulier lorsque les données proviennent de paramètres hors domaine. Ensuite, nous comparons l'efficacité de quelques méthodes de recalibrage largement utilisées (telles que les ensembles, la mise à l'échelle de température). Ensuite, nous illustrons empiriquement un lien entre la distillation et l'étalonnage. Nous considérons la distillation comme un terme de régularisation encourageant le modèle étudiant à générer des incertitudes qui correspondent à celles d'un modèle enseignant. Grâce à ces informations, nous développons des méthodes de recalibrage simples basées sur la distillation sans coût de temps d'inférence supplémentaire. Nous montrons sur le banc d'essai GLUE que nos méthodes simples peuvent atteindre des performances d'étalonnage hors domaine (OOD) compétitives avec des approches plus coûteuses. Enfin, nous incluons des ablations pour comprendre l'utilité des composants de notre méthode proposée et examiner la transférabilité de l'étalonnage par distillation.", 'pt': 'Avanços recentes em sistemas de PNL, notadamente o paradigma de pré-treinamento e ajuste fino, alcançaram grande sucesso em precisão preditiva. No entanto, esses sistemas geralmente não são bem calibrados para incertezas prontas para uso. Muitos métodos de recalibração têm sido propostos na literatura para quantificar a incerteza preditiva e calibrar as saídas do modelo, com vários graus de complexidade. Neste trabalho, apresentamos um estudo sistemático de alguns desses métodos. Concentrando-se na tarefa de classificação de texto e nos grandes modelos de linguagem pré-treinados, primeiro mostramos que muitos dos modelos ajustados não são bem calibrados prontos para uso, especialmente quando os dados vêm de configurações fora do domínio. Em seguida, comparamos a eficácia de alguns métodos de recalibração amplamente utilizados (como conjuntos, escala de temperatura). Em seguida, ilustramos empiricamente uma conexão entre destilação e calibração. Vemos a destilação como um termo de regularização que encoraja o modelo do aluno a produzir incertezas que correspondem às de um modelo de professor. Com essa percepção, desenvolvemos métodos simples de recalibração baseados em destilação sem custo adicional de tempo de inferência. Mostramos no benchmark GLUE que nossos métodos simples podem alcançar um desempenho competitivo de calibração fora de domínio (OOD) w.r.t. abordagens mais caras. Finalmente, incluímos ablações para entender a utilidade dos componentes do nosso método proposto e examinar a transferibilidade da calibração via destilação.', 'ja': 'NLPシステムの最近の進歩、特に事前訓練と調整のパラダイムは、予測精度において大きな成功を収めています。 しかしながら、これらのシステムは通常、即時の不確実性のために適切に校正されていません。 予測不確実性を定量化し、モデル出力を較正するための多くの再較正方法が、文献で提案されており、複雑さの程度は様々である。 この研究では、これらの方法のいくつかの体系的な研究を提示します。 テキスト分類タスクと細かく調整された大規模な事前訓練された言語モデルに焦点を当てると、まず、細かく調整されたモデルの多くが、特にドメイン外の設定からデータが出てくる場合は、適切に校正されていないことが示されます。 次に、いくつかの広く使用されている再キャリブレーション方法（アンサンブル、温度スケーリングなど）の有効性を比較します。 次に、蒸留とキャリブレーションの関連を経験的に示します。 蒸留は、学習者モデルが教師モデルと一致する不確実性を出力することを奨励する規則化項として捉えている。 この洞察により、追加の推論時間コストなしで蒸留に基づいた簡単な再キャリブレーション方法を開発しました。 私たちは、シンプルな方法がより高価なアプローチで競争力のあるドメイン外（ OOD ）キャリブレーションパフォーマンスを達成できることをグルーベンチマークで示します。 最後に、提案された方法のコンポーネントの有用性を理解し、蒸留によるキャリブレーションの移送性を検討するためのアブレーションを含めます。', 'zh': 'NLP系统之最新进展,特预训练微调,取巨大成功于测准确性。 然此统常无开箱即用之不确定性为良校准。 文献多建校准法,量化测不确定性校准,其复杂性有差。 于此等事,吾等条上其法。 注于文本分类及微调大预训练言语模样,先明诸微调未开箱即以地为良校准,特当数自域外设时。 比之博用重校准法(合温缩放)之有效性。 然后以言蒸馏校准之际。 吾以蒸馏为正则化术语,劝弟子输之以不确定性。 因此洞察,开基蒸馏简重校准法,无须额外推理成本。 吾等于 GLUE 准试中,吾道可以成更具竞争力域外 (OOD) 校准性能,而法益贵。 至于烧蚀,以知其组分之有用性,以省蒸馏校准之可转移性。', 'hi': 'एनएलपी प्रणालियों में हाल ही में प्रगति, विशेष रूप से pretraining-and-finetuning प्रतिमान, भविष्यवाणी सटीकता में बड़ी सफलता हासिल की है। हालांकि, इन प्रणालियों को आमतौर पर अनिश्चितता आउट-ऑफ-द-बॉक्स के लिए अच्छी तरह से कैलिब्रेट नहीं किया जाता है। साहित्य में कई रीकैलिब्रेशन विधियों को भविष्यवाणी अनिश्चितता को मापने और मॉडल आउटपुट को कैलिब्रेट करने के लिए प्रस्तावित किया गया है, जिसमें जटिलता की अलग-अलग डिग्री है। इस काम में, हम इनमें से कुछ तरीकों का एक व्यवस्थित अध्ययन प्रस्तुत करते हैं। पाठ वर्गीकरण कार्य पर ध्यान केंद्रित करते हुए और बड़े प्रीट्रेन्ड भाषा मॉडल को ठीक करने पर ध्यान केंद्रित करते हुए, हम पहले दिखाते हैं कि कई महीन मॉडल अच्छी तरह से आउट-ऑफ-द-बॉक्स कैलिब्रेटेड नहीं हैं, खासकर जब डेटा आउट-ऑफ-डोमेन सेटिंग्स से आता है। इसके बाद, हम कुछ व्यापक रूप से उपयोग किए जाने वाले रीकैलिब्रेशन विधियों (जैसे ensembles, तापमान स्केलिंग) की प्रभावशीलता की तुलना करते हैं। फिर, हम अनुभवजन्य रूप से आसवन और अंशांकन के बीच एक संबंध का वर्णन करते हैं। हम आसवन को एक नियमितीकरण शब्द के रूप में देखते हैं जो छात्र मॉडल को अनिश्चितताओं को आउटपुट करने के लिए प्रोत्साहित करता है जो शिक्षक मॉडल से मेल खाते हैं। इस अंतर्दृष्टि के साथ, हम आसवन के आधार पर सरल रीकैलिब्रेशन विधियों को विकसित करते हैं, जिसमें कोई अतिरिक्त अनुमान-समय लागत नहीं होती है। हम GLUE बेंचमार्क पर दिखाते हैं कि हमारे सरल तरीके प्रतिस्पर्धी आउट-ऑफ-डोमेन (OOD) अंशांकन प्रदर्शन w.r.t. अधिक महंगे दृष्टिकोण प्राप्त कर सकते हैं। अंत में, हम अपनी प्रस्तावित विधि के घटकों की उपयोगिता को समझने और आसवन के माध्यम से अंशांकन की हस्तांतरणीयता की जांच करने के लिए एब्लेशन शामिल करते हैं।', 'ru': 'Недавние выдвижения в системах NLP, в частности парадигма pre-тренировки-и-finetuning, достигали большого успеха в предсказательной точности. Тем не менее, эти системы, как правило, не хорошо откалиброваны для неопределенности из коробки. Многие методы повторной калибровки были предложены в литературе для количественной оценки прогнозной неопределенности и калибровки выходных данных модели с различной степенью сложности. В этой работе мы представляем систематическое исследование некоторых из этих методов. Сосредоточившись на задаче классификации текста и тонкой настройке больших предварительно подготовленных языковых моделей, мы сначала показываем, что многие из тонко настроенных моделей плохо откалиброваны из коробки, особенно когда данные поступают из внедоменных настроек. Далее мы сравниваем эффективность нескольких широко используемых методов повторной калибровки (таких как ансамбли, температурное масштабирование). Затем мы эмпирически иллюстрируем связь между дистилляцией и калибровкой. Мы рассматриваем дистилляцию как термин регуляризации, поощряющий модель ученика выводить неопределенности, которые соответствуют неопределенностям модели учителя. С помощью этого анализа мы разрабатываем простые методы повторной калибровки, основанные на дистилляции без дополнительных затрат времени на вывод. Мы показываем на эталоне КЛЕЯ что наши простые методы могут достигнуть конкурентоспособного представления калибровки вне домена (OOD) w.r.t. более дорогих подходов. Наконец, мы включаем абляции, чтобы понять полезность компонентов предложенного нами метода и изучить возможность переноса калибровки посредством дистилляции.', 'ga': "D'éirigh thar barr leis an dul chun cinn a rinneadh le déanaí i gcórais NLP, go háirithe an paraidím réamhoiliúint agus mionchoigeartaithe, maidir le cruinneas tuar. Mar sin féin, de ghnáth ní dhéantar na córais seo a chalabrú go maith le haghaidh éiginnteachta lasmuigh den bhosca. Tá go leor modhanna athchalabrú molta sa litríocht chun éiginnteacht thuarthach a chainníochtú agus chun aschuir samhlacha a chalabrú, le céimeanna éagsúla castachta. Sa saothar seo, cuirimid i láthair staidéar córasach ar roinnt de na modhanna seo. Ag díriú ar thasc aicmithe an téacs agus ar mhúnlaí móra teanga réamhthraenáilte mionchoigeartaithe, léirímid ar dtús nach bhfuil go leor de na samhlacha mionchoigeartaithe calabraithe go maith lasmuigh den bhosca, go háirithe nuair a thagann na sonraí ó shuíomhanna lasmuigh den fhearann. Ansin, déanaimid comparáid idir éifeachtacht roinnt modhanna athchalabrú a úsáidtear go forleathan (cosúil le ensembles, scálaithe teochta). Ansin, léirímid go heimpíreach an nasc idir driogadh agus calabrú. Breathnaímid ar dhriogadh mar théarma rialtachta a spreagann múnla an dalta chun neamhchinnteachtaí a aschur a thagann le múnla múinteora. Leis an léargas seo, forbraímid modhanna simplí athchalabrú bunaithe ar dhriogadh gan aon chostas breise ama tátail. Léirímid ar thagarmharc GLUE gur féidir lenár modhanna simplí feidhmíocht chalabrú iomaíoch lasmuigh den fhearann (OOD) a bhaint amach w.r.t. cur chuige níos costasaí. Mar fhocal scoir, cuirimid áireamháin chun tairbhe na gcomhpháirteanna den mhodh atá beartaithe againn a thuiscint agus chun inaistritheacht an chalabrúcháin trí dhriogadh a scrúdú.", 'el': 'Οι πρόσφατες προόδους στα συστήματα ΝΛΠ, ιδίως το παράδειγμα προεπιλογής και τελειοποίησης, έχουν επιτύχει μεγάλη επιτυχία στην προληπτική ακρίβεια. Ωστόσο, αυτά τα συστήματα συνήθως δεν είναι καλά βαθμονομημένα για αβεβαιότητα έξω από το κουτί. Πολλές μέθοδοι επαναβαθμονόμησης έχουν προταθεί στη βιβλιογραφία για τον ποσοτικό προσδιορισμό της προβλέψιμης αβεβαιότητας και τη βαθμονόμηση των αποτελεσμάτων μοντέλων, με διαφορετικούς βαθμούς πολυπλοκότητας. Στην εργασία αυτή, παρουσιάζουμε μια συστηματική μελέτη ορισμένων από αυτές τις μεθόδους. Εστιάζοντας στην εργασία ταξινόμησης κειμένου και στα ειδικά ρυθμισμένα μεγάλα προ-εκπαιδευμένα μοντέλα γλώσσας, δείχνουμε πρώτα ότι πολλά από τα εκλεπτυσμένα μοντέλα δεν είναι καλά βαθμονομημένα έξω από το κουτί, ειδικά όταν τα δεδομένα προέρχονται από ρυθμίσεις εκτός πεδίου. Στη συνέχεια, συγκρίνουμε την αποτελεσματικότητα ορισμένων ευρέως χρησιμοποιούμενων μεθόδων επαναβαθμονόμησης (όπως σύνολα, κλιμάκωση θερμοκρασίας). Στη συνέχεια, απεικονίζουμε εμπειρικά μια σχέση μεταξύ απόσταξης και βαθμονόμησης. Θεωρούμε την απόσταξη ως έναν όρο κανονικοποίησης που ενθαρρύνει το μαθητικό μοντέλο να παράγει αβεβαιότητες που ταιριάζουν με αυτές ενός εκπαιδευτικού μοντέλου. Με αυτή τη γνώση, αναπτύσσουμε απλές μεθόδους επαναβαθμονόμησης βασισμένες στην απόσταξη χωρίς επιπλέον κόστος συναπόφασης-χρόνου. Δείχνουμε στο σημείο αναφοράς ότι οι απλές μέθοδοι μας μπορούν να επιτύχουν ανταγωνιστικές επιδόσεις βαθμονόμησης εκτός πεδίου χωρίς ακριβότερες προσεγγίσεις. Τέλος, συμπεριλαμβάνουμε απολύσεις για να κατανοήσουμε τη χρησιμότητα των συστατικών της προτεινόμενης μεθόδου μας και να εξετάσουμε τη δυνατότητα μεταφοράς της βαθμονόμησης μέσω απόσταξης.', 'hu': 'Az NLP-rendszerek közelmúltbeli fejlődése, különösen az előkészítés és finomhangolás paradigmája nagy sikert ért el a prediktív pontosság terén. Ezeket a rendszereket azonban általában nem kalibrálták megfelelően a bizonytalanságra. A szakirodalomban számos újrakalibrálási módszert javasoltak a prediktív bizonytalanság számszerűsítésére és a modell kimeneteinek kalibrálására, különböző komplexitású mértékű kalibrálására. Ebben a munkában ezek közül néhány módszer szisztematikus tanulmányozását mutatjuk be. A szövegosztályozási feladatra és a finomhangolt nagyméretű, előkészített nyelvi modellekre összpontosítva először azt mutatjuk meg, hogy a finomhangolt modellek közül sok nem rendelkezik jól kalibrálva a dobozból, különösen, ha az adatok tartományon kívüli beállításokból származnak. Ezután összehasonlítjuk néhány széles körben használt újrakalibrálási módszer hatékonyságát (például együttesek, hőmérséklet skálázás). Ezután empirikusan illusztráljuk a desztilláció és kalibrálás közötti kapcsolatot. A desztillációt regularizációs kifejezésként tekintjük, amely arra ösztönzi a hallgatói modellt, hogy olyan bizonytalanságokat adjon ki, amelyek megfelelnek a tanári modellhez. Ezzel a betekintéssel egyszerű, lepárláson alapuló átkalibrálási módszereket dolgozunk ki, további következtetési időköltségek nélkül. A GLUE referenciaértékén megmutatjuk, hogy egyszerű módszereink versenyképes, domain kívüli kalibrációs teljesítményt érhetnek el drágább megközelítésekkel. Végezetül ablációkat is tartalmazunk, hogy megértsük a javasolt módszerünk komponenseinek hasznosságát és vizsgáljuk a kalibrálás transzferálhatóságát desztillációval.', 'ka': 'NLP სისტემებში, განსაკუთრებით პრადიგმა წარმოდგენა, განსაკუთრებით წარმოდგენა საკუთარი წარმოდგენა. მაგრამ, ეს სისტემები უბრალოდ არაა კალიბრებულია, რომ უცნობიერებას გარეშე. ბევრი რეკალიბრაციის მეტოვები ლიტერტურაში იყენებულია წარმოდგენისთვის წარმოდგენისთვის და კალიბრაციის მოდელის გამოყენების კვანტიფიკაციისთვის. ამ სამუშაოში, ჩვენ ამ მეტირების რამდენიმე სისტემატიკური სწავლა ჩვენ. ტექსტის კლასიფიკაციის რაოდენობაზე და დიდი წინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინაწინა შემდეგ, ჩვენ გადასწორებთ რამდენიმე მზად გამოყენებული რეკალიბრაციის მეტოვების ეფექტიურობას (როგორც ანსტემბულები, ტემპერაციის სკალიბრება). შემდეგ, ჩვენ ემპერიკურად ინლისტრაციას და კალიბრაციის შორის კავშირის გამოყენება. ჩვენ ვიხედავთ განსხვავება როგორც რეგილარიზაციის ტერმინი, რომლებიც სტუდენტის მოდელის მოწყობილობას, რომლებიც სტუდენტის მოდელის შესაძლებლობად გავამუ ამ შესახებით, ჩვენ განვითარებთ განვითარებით ერთადერთი რეკალიბრაციის მეტოვები, განსხვავებული განსხვავებაზე, რომელიც არ აქვს დამატებითი ინფრენციის დროის ღ ჩვენ ჩვენ GLUE ბენქმარკზე გამოჩვენებთ, რომ ჩვენი მარტივი მეტოვები შეგვიძლია კონპექციური გარეშე დემომინის (OOD) კალიბრაციის გამოსახულება, რომელიც უფრო ძალიან ძალიან ძალია საბოლოოდ, ჩვენ შეგვიყვანეთ შესაძლებლობები, რომლებიც გავიგოთ ჩვენი მოძლებელი მეტოდის კომპონენტების გამოსაყენებელობას და გავწეროთ კალიბრაციის გადატანი', 'it': "I recenti progressi nei sistemi PNL, in particolare il paradigma di pretraining e finetuning, hanno raggiunto un grande successo nella precisione predittiva. Tuttavia, questi sistemi di solito non sono ben calibrati per l'incertezza fuori dagli schemi. Molti metodi di ricalibrazione sono stati proposti in letteratura per quantificare l'incertezza predittiva e calibrare i risultati dei modelli, con diversi gradi di complessità. In questo lavoro presentiamo uno studio sistematico di alcuni di questi metodi. Concentrandoci sull'attività di classificazione del testo e sui modelli di lingue pre-addestrate di grandi dimensioni, mostriamo innanzitutto che molti dei modelli perfezionati non sono ben calibrati fuori dagli schemi, specialmente quando i dati provengono da impostazioni fuori dominio. Successivamente, confrontiamo l'efficacia di alcuni metodi di ricalibrazione ampiamente utilizzati (come ensemble, scala della temperatura). Poi, illustreremo empiricamente una connessione tra distillazione e taratura. Consideriamo la distillazione come un termine di regolarizzazione che incoraggia il modello studentesco a produrre incertezze che corrispondono a quelle di un modello di insegnante. Con questa intuizione, sviluppiamo semplici metodi di ricalibrazione basati sulla distillazione senza costi aggiuntivi di inferenza-tempo. Mostriamo sul benchmark GLUE che i nostri metodi semplici possono ottenere prestazioni di calibrazione competitive fuori dominio (OOD) con approcci più costosi. Infine, includiamo le ablazioni per comprendere l'utilità dei componenti del nostro metodo proposto ed esaminare la trasferibilità della taratura tramite distillazione.", 'lt': 'Neseniai padaryta pažanga NLP sistemose, visų pirma išankstinio mokymo ir tobulinimo paradigma, pasiekė didelę sėkmę prognozuojant tikslumą. Vis dėlto šios sistemos paprastai nėra gerai kalibruotos dėl neapibrėžtumo išorinėje dėžutėje. Literatūroje pasiūlyta daug pakartotinio kalibravimo metodų prognozuojamam neapibrėžtumui ir modelio rezultatams kalibruoti, o sudėtingumas skiriasi. Šiame darbe pateikiame sistemingą kelių šių metodų tyrimą. Pagrindinį dėmesį skiriant teksto klasifikavimo uždaviniui ir patobulintam dideliam išankstinio mokymo kalbų modeliui, pirmiausia parodome, kad daugelis patobulintų modelių nėra gerai kalibruoti išorinėje dėžutėje, ypač kai duomenys gaunami iš išorinių nustatymų. Toliau palyginame kelių plačiai naudojamų pakartotinio kalibravimo metodų (pvz., komplektų, temperatūros skalės) veiksmingumą. Tada empiriniu būdu iliustruojame ryšį tarp distiliacijos ir kalibravimo. Mes manome, kad distiliavimas yra reguliarizavimo terminas, skatinantis student ų model į išleisti neapibrėžtis, atitinkančias mokytojo modelio neapibrėžtis. Atsižvelgdami į tai, kuriame paprastus pakartotinio kalibravimo metodus, grindžiamus distiliavimu, be jokių papildomų išvadų laiko sąnaudų. Atsižvelgdami į GLUE lyginamąjį rodiklį parodome, kad mūsų paprasti metodai gali pasiekti konkurencingą ne srities kalibravimo veiksmingumą, t. y. brangesnius metodus. Galiausiai įtraukiame gebėjimus suprasti mūsų siūlomo metodo komponentų naudingumą ir išnagrinėti kalibravimo perkeliamumą distiliavimo būdu.', 'mk': 'Неодамнешните напредоци во системите на НЛП, особено парадигмата за претренирање и финетизирање, постигнаа голем успех во предвидувачката точност. Сепак, овие системи обично не се добро калибрирани за несигурност надвор од кутијата. Во литературата се предложени многу методи за рекалибрирање за квантификација на предвидливата несигурност и калибрирање на излезите на моделот, со различни степени на комплексност. Во оваа работа, претставуваме систематска студија на неколку од овие методи. Со фокус на текстовата класификација задача и финетизираните големи предобучени јазички модели, прво покажуваме дека многу од финетизираните модели не се добро калибрирани надвор од кутијата, особено кога податоците доаѓаат од поставувања надвор од домен. Следно, ја споредуваме ефикасноста на неколку широко употребени методи за рекалибарација (како што се ансемблите, степенот на температурата). Тогаш емпирички ја илустрираме врската помеѓу дистилацијата и калибрацијата. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model.  Со ова разбирање, развиваме едноставни методи за рекалибарација базирани на дистилација без дополнителни трошоци на инференција-времето. На споредбата GLUE покажуваме дека нашите едноставни методи можат да постигнат конкурентна извршност на калибрација надвор од домен (OOD) во врска со поскапите пристапи. Конечно, вклучуваме аблации за да ја разбереме корисноста на компонентите на нашиот предложен метод и да ја испитаме трансферентноста на калибрацијата преку дистилација.', 'kk': 'Жуырдағы NLP жүйелерінде жұмыс істеу жүйелері, әсіресе, тұрақтық және тұрақтық парадигмі, таңдау дұрыстығында үлкен сәтті жеткізді. Бірақ бұл жүйелер кәдімгі қауіпсіздігін шектеу үшін жақсы калибрлемейді. Көптеген қайталау әдістері литературада бірнеше кәдімгінің көптеген тәуелсіздігін және калибрлеу үлгілерінің шығысын көптеген көптеген. Бұл жұмыс ішінде бірнеше әдістерді жүйелік зерттеуді таңдаймыз. Мәтін шектеу тапсырмасына және үлкен тіл үлгілеріне көмектесіп, біріншіден кейбір үлгілердің көпшілігі жоқ үлгілердің шегінен шығып калибрлемегенін көрсетедік, осымен қатар, деректер доменден шығып келгенде. Келесіден біз көп қолданылатын қайта калибрлеу әдістерін салыстырып көреміз (мысалы, енсемблер, температура масштабы). Содан кейін, біз дистилляция мен калибрлеу арасындағы байланысын көрсетедік. Біз дистриляцияны мұғалім үлгісіне сәйкес келетін мәліметтердің үлгісіне сәйкес келетін тәуелдік үлгісі ретінде көреміз. Бұл түсініктермен біз кәдімгі қайта калибрлеу әдістерін өзгертеміз. Қосымша бағасы жоқ. Біз GLUE бағдарламасында қарапайым әдістеріміздің доменге (OOD) сыртқы калибрлеу әдістерін жеткізуге болады. Соңында, біз келтірілген әдімінің компоненттерінің пайдалығын түсініп, калибрлеу көмегімен дистриляциялау мүмкіндігін тексеру үшін мүмкіндіктерді қолданамыз.', 'ms': 'Kemajuan baru-baru ini dalam sistem NLP, terutama paradigma pretraining-and-finetuning, telah mencapai sukses besar dalam persamaan ramalan. Namun, sistem ini biasanya tidak dikalibrat dengan baik untuk ketidakpastian keluar dari kotak. Banyak kaedah kalibrasi semula telah diusulkan dalam literatur untuk kuantifikasi ketidakpastian ramalan dan kalibrasi output model, dengan darjah yang berbeza kompleksiti. Dalam kerja ini, kami memperkenalkan kajian sistemik beberapa kaedah ini. Berfokus pada tugas klasifikasi teks dan model bahasa besar yang dilatih dahulu, kita pertama-tama menunjukkan bahawa banyak model yang dilatih tidak dikalibrat dengan baik keluar dari kotak, terutama apabila data datang dari tetapan luar domain. Seterusnya, kita membandingkan kegunaan beberapa kaedah kalibrasi yang digunakan secara luas (seperti ensembles, skala suhu). Kemudian, kita empirik memperlihatkan hubungan antara penapisan dan kalibrasi. Kami melihat penapisan sebagai istilah regularisasi yang menguatkan model pelajar untuk keluarkan ketidakpastian yang sepadan dengan yang model guru. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost.  We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches.  Akhirnya, kami termasuk ablasi untuk memahami kegunaan komponen kaedah yang kami cadangkan dan memeriksa kemudahan pemindahan kalibrasi melalui penapisan.', 'mt': 'L-avvanzi reċenti fis-sistemi NLP, notevolment il-paradigma ta’ qabel it-taħriġ u l-irfinar, kisbu suċċess kbir fil-preċiżjoni prevedibbli. Madankollu, dawn is-sistemi normalment mhumiex ikkalibrati tajjeb għal inċertezza barra mill-kaxxa. Bosta metodi ta’ rikalibrazzjoni ġew proposti fil-letteratura għall-kwantifikazzjoni tal-in ċertezza prevedibbli u l-kalibrar tal-outputs tal-mudell, bi gradi differenti ta’ kumplessità. F’dan ix-xogħol, nippreżentaw studju sistematiku ta’ ftit minn dawn il-metodi. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings.  Imbagħad, a ħna nqabblu l-effettività ta’ ftit metodi ta’ rikalibrazzjoni użati b’mod wiesa’ (bħall-ensembles, l-iskala tat-temperatura). Imbagħad, empirikament nistru konnessjoni bejn id-distillazzjoni u l-kalibrazzjoni. Aħna nqisu d-distillazzjoni bħala terminu ta’ regolarizzazzjoni li jinkoraġġixxi l-mudell tal-istudenti biex jipproduċi inċertezzi li jaqblu ma’ dawk ta’ mudell tal-għalliema. B’dan l-għarfien, niżviluppaw metodi sempliċi ta’ rikalibrazzjoni bbażati fuq distillazzjoni mingħajr spejjeż addizzjonali ta’ inferenza-ħin. Fil-punt ta’ riferiment tal-GLUE nuru li l-metodi sempliċi tagħna jistgħu jiksbu prestazzjoni kompetittiva ta’ kalibrazzjoni barra d-dominju (OOD) b’approċċi aktar għaljin. Fl-aħħar nett, aħna inkludew abblazzjonijiet biex nifhmu l-utilità tal-komponenti tal-metodu propost tagħna u jeżaminaw it-trasferibbiltà tal-kalibrazzjoni permezz tad-distillazzjoni.', 'ml': 'NLP സിസ്റ്റത്തിലെ അടുത്തുള്ള പുരോഗങ്ങള്\u200d, പ്രത്യേകിച്ച് മഴയില്\u200d നിന്നും മുന്നോട്ടുനില്\u200dക്കുന്ന പാര്\u200dഡിഡിമില്\u200d, പ്രത എന്നാലും ഈ സിസ്റ്റം സാധാരണ പെട്ടിയില്\u200d നിന്നും പുറത്തുകടക്കാന്\u200d സാധാരണ ക്ലാബിള്\u200d ചെയ്യുന്നില്ല. സാഹിത്യത്തില്\u200d ധാരാളം രീതിയിലേക്ക് പ്രായശ്ചിത്വം ചെയ്തിരിക്കുന്നു. പ്രത്യേകിച്ചിരിക്കുന്ന അസാധ്യതയും മോഡലിന്റെ പുറത ഈ ജോലിയില്\u200d നമ്മള്\u200d ഈ രീതികളുടെ കുറച്ച് സിസ്റ്റമിക്ക് പഠനം കൊടുക്കുന്നു. ടെക്സ്റ്റ് ക്ലാസ്ഫിക്ഷന്\u200d ജോലിയില്\u200d ശ്രദ്ധിക്കുകയും വലിയ ഭാഷ മോഡലുകളില്\u200d മുന്\u200dകൂട്ടുകയും ചെയ്യുന്നതില്\u200d ആദ്യം ഞങ്ങള്\u200d കാണിക്കുന്നു, മുന്\u200dകൂട്ടിയ മോഡ അടുത്തത്, കുറച്ച് വിശാലമായ ഉപയോഗിക്കപ്പെട്ട റിസലിബ്രേഷന്\u200d രീതികളുടെ പ്രവർത്തിയെക്കുറിച്ച് നമുക്ക് താല്പര Then, we empirically illustrate a connection between distillation and calibration.  ഞങ്ങള്\u200d വേര്\u200dതിരിച്ചറിയുന്നത് ഒരു നിയമങ്ങളുടെ വാക്കായിട്ടാണ്. വിദ്യാര്\u200dത്ഥികളുടെ മോഡല്\u200d പൊരുതുന്ന ഒരു ടീച്ചര്\u200d മ ഈ കണ്ണുപിടിച്ചാല്\u200d, കൂടുതല്\u200d ദുര്\u200dബലമില്ലാത്ത വിലയ്ക്ക് അടിസ്ഥാനത്ത് നമ്മള്\u200d സാധാരണ റിസലിബ്രഷന്\u200d രീതിയില്\u200d നി നമ്മുടെ എളുപ്പമുള്ള മാര്\u200dഗങ്ങള്\u200dക്ക് കാണിച്ചുകൊടുക്കാന്\u200d സാധിക്കുന്നു. നമ്മുടെ സാധാരണ രീതികള്\u200d അവസാനം, നമ്മുടെ പ്രൊദ്ദേശിച്ച രീതിയിലെ വിഭവങ്ങളുടെ ഉപയോഗങ്ങള്\u200d മനസ്സിലാക്കാന്\u200d നമുക്ക് ഉള്\u200dപ്പെടുത്തുന്ന സാധ്യതകള', 'mn': 'НЛП системийн саяхан хөгжлийн хөгжлийн талаар, ялангуяа хөгжлийн болон сайхан парадигм нь таамаглах тохиромжтой зөв амжилт гарч ирсэн. Гэхдээ эдгээр системүүд хайрцагаас гадна тодорхойгүй байдлаар тодорхойлж чадахгүй. Олон дахин шийдвэрлэх арга загвар нь уран зохиолд таамаглалтай тодорхойгүй байдлыг тооцоолж, хэмжээгээр тооцоолж байгаа загварын үр дүнг олон хэмжээний төвөгтэй. Энэ ажлын тухай бид эдгээр хэдэн арга замыг систематикийн судалгаа үзүүлнэ. Текст хуваалтын ажил болон том хэл загваруудыг анхаарлаа анхаарлаа анхаарлаа бид ихэнх сайн байхгүй загваруудыг хайрцагнаас гаргаж чадахгүй, ялангуяа мэдээллүүд дотоод гарч ирэхэд сайн тодорхойлж байна. Дараа нь бид хэдэн шилжих хэрэглэгдсэн дахин шилжүүлэх аргын үр дүнг харьцуулдаг. Дараа нь бид шинжилгээ болон калибрийн хоорондын холбоотой байдлыг харуулж байна. Бид тайлбарлалыг багш нарын загвартай холбоотой тодорхойгүй байдлыг багш нарын загвартай холбоотой байдлыг багш нарын загварыг урам зориулдаг гэсэн үг гэж үздэг. Энэ ойлголтын тулд бид энгийн дахин шинжилгээ хийх арга замыг нэмэлт халдварын цаг хугацааны үнэ цэнэтэй байдлаар хөгжүүлдэг. Бид GLUE багц дээр бидний энгийн арга зам нь өрсөлдөөн бус холбоотой (OOD) калибрын үйл ажиллагаанд илүү үнэтэй арга зам гаргаж чадна гэдгийг харуулж байна. Эцэст нь, бид санал өгсөн арга хэмжээсүүдийн хэрэгцээг ойлгох боломжтой байдлаар калибрын дамжуулах боломжтой талаар шалгаж байна.', 'no': 'Nyleg utviklingane i NLP-systemet, særleg paradigmen for pretraining og finetuning, har oppnådd stor suksess i forhåndsvising av nøyaktighet. Desse systema er likevel ikkje veldig kalibrert for usikkerhet ut av boksen. Mange rekalibrasjonsmetodar er foreslått i litteraturen for kvantifikasjon av foreslåande usikkerhet og kalibrasjonsmodelleutgåver med forskjellige gradar av kompleksitet. I dette arbeidet presenterer vi eit systematisk studie av noen av desse metodane. Fokuserer vi på oppgåva for klassifikasjon av tekst og store språkkmodeller som er sett inn, viser vi først at mange av dei finerte modelane ikkje er veldig kalibrert ut av boksen, spesielt når data kommer frå innstillingane frå domenet. Neste samanliknar vi effektiviteten av fleire breidde brukte rekalibrasjonsmetodar (som ensembler, temperatursskalering). Så illustrerer vi empirisk eit tilkopling mellom distillasjon og kalibrering. Vi ser distillasjon som eit regulært uttrykk som stimulerer studentmodellen for å utføra usikkerhet som passar med dei som passar med ein læringsmodell. Med denne innsyninga utviklar vi enkle rekalibrasjonsmetodar basert på distillasjon utan noko tilleggstidskostnader for inferens. Vi viser på GLUE-benchmarket at våre enkle metodar kan oppnå konkurrentiv ut-domenet (OOD) kalibreringsfunksjon, t.d. meir dykkare tilnærmingar. Til slutt, inkluderer vi tilgang til å forstå korleis komponentane er nyttig av vår foreslått metode og undersøke overføringsfeiligheten av kalibrasjon via distillasjon.', 'ro': 'Progresele recente în sistemele PNL, în special paradigma pregătirii și finăririi, au obținut un mare succes în ceea ce privește precizia predictivă. Cu toate acestea, aceste sisteme nu sunt de obicei bine calibrate pentru incertitudine. Multe metode de recalibrare au fost propuse în literatura de specialitate pentru cuantificarea incertitudinii predictive și calibrarea rezultatelor modelului, cu grade diferite de complexitate. În această lucrare, prezentăm un studiu sistematic al câtorva dintre aceste metode. Focalizându-ne pe sarcina de clasificare a textului și pe modelele mari de limbi pre-instruite, am arătat mai întâi că multe dintre modelele fin-reglate nu sunt bine calibrate din cutie, mai ales atunci când datele provin din setări în afara domeniului. Apoi, comparăm eficacitatea câtorva metode de recalibrare utilizate pe scară largă (cum ar fi ansamblurile, scalarea temperaturii). Apoi, ilustrăm empiric o legătură între distilare și calibrare. Considerăm distilarea ca un termen de regularizare care încurajează modelul elevilor să producă incertitudini care se potrivesc cu cele ale unui model de profesor. Cu această perspectivă, dezvoltăm metode simple de recalibrare bazate pe distilare, fără costuri suplimentare de inferență-timp. Aratăm pe benchmark GLUE că metodele noastre simple pot obține performanțe competitive de calibrare în afara domeniului (OOD) cu abordări mai scumpe. În cele din urmă, includem ablații pentru a înțelege utilitatea componentelor metodei propuse și pentru a examina transferabilitatea calibrării prin distilare.', 'pl': 'Ostatnie postępy w systemach NLP, w szczególności paradygmat wstępnego treningu i dostrajania, osiągnęły ogromny sukces w zakresie dokładności predykcyjnej. Jednak systemy te zwykle nie są dobrze skalibrowane pod kątem niepewności gotowej. W literaturze zaproponowano wiele metod rekalibracji w celu ilościowego określenia niepewności predykcyjnej i kalibracji wyników modeli o różnym stopniu złożoności. W niniejszej pracy przedstawiamy systematyczne badanie kilku z tych metod. Skupiając się na zadaniu klasyfikacji tekstu i precyzyjnie dostrojonych dużych wstępnie przeszkolonych modelach językowych, pokazujemy najpierw, że wiele z dostrojonych modeli nie jest dobrze skalibrowanych poza pudełkiem, zwłaszcza gdy dane pochodzą z ustawień poza domeną. Następnie porównujemy skuteczność kilku szeroko stosowanych metod rekalibracji (takich jak zespoły, skalowanie temperatury). Następnie empirycznie ilustrujemy związek między destylacją a kalibracją. Postrzegamy destylację jako termin regularyzacyjny zachęcający ucznia do wydawania niepewności odpowiadających modelowi nauczyciela. Dzięki temu wglądowi opracowujemy proste metody rekalibracji oparte na destylacji bez dodatkowych kosztów czasu wnioskowania. Na GLUE benchmark pokazujemy, że nasze proste metody mogą osiągnąć konkurencyjną wydajność kalibracji poza domeną (OOD) bez droższych podejść. Wreszcie uwzględniamy ablacje, aby zrozumieć przydatność składników proponowanej metody i zbadać możliwość przenoszenia kalibracji poprzez destylację.', 'so': "Horumarinta ugu dambeysa ee nidaamka NLP, khusuusan qaybaha hore-roobka iyo faa'iidada, waxay liibaanay liibaan weyn oo la sii sheegay. Si kastaba ha ahaatee nidaamkan sida caadiga ah looma xisaabin karo mid aan la garanayn. Qoraalka waxaa lagu talo galay qaabab badan oo la soo bedelay si ay u qiyaasaan aqoonta la'aanta iyo soo baxayaasha modellka oo qiyaasa, waxayna leeyihiin shahaado kala duduwan. Shaqadan, waxaynu keenaynaa waxbarasho nidaamka ah oo kooban qaababkan ah. Markii aad ku fiirsaneyso shaqada fasaxda qoraalka iyo tusaalaha luuqada aad u weyn oo la soo bandhigay, marka ugu horeysa waxaynu tusnaynaa in noocyo badan oo dhaqaalaha ah aan looga xisaabin karo-ka-baxsan, khusuusan marka macluumaadku ay ka timaadaan xarumaha gudaha. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling).  Markaas waxaan si fiican ugu muujinnaa xiriir kala duwan iyo kalibrin. Waxaynu u aragnaa inay u kala soocaan sida hadal go’aanka ah oo ku dhiirrigelinaya tusaale waxbarashada inay soo bixiso tusaale la'aan oo u eg tusaale macallimiin. Aragtan darteed, waxaynu horumarinnaa qaabab fudud oo ku saleysan furid aan kharash dheeraad ah oo waqtiga cayiman. Waxaynu ka muujinnaa bangiga GLUE in qaababkayaga fudud ay sameyn karaan tartanka kalibrinta ka baxa (OOD), tusaale ahaan hababka qaali ah. Ugu dambaysta waxaan ku qornaa qalab aan garanayno faa'iidada nooca loo talagalay, waxaana baaritaan beddelinta kalibrinta si loo beddelo.", 'sv': 'De senaste framstegen inom NLP-system, särskilt paradigmet för förberedelse och finjustering, har uppnått stor framgång när det gäller förutseende noggrannhet. Dessa system är dock vanligtvis inte väl kalibrerade för osäkerhet direkt. Många omkalibreringsmetoder har föreslagits i litteraturen för att kvantifiera prediktiv osäkerhet och kalibrera modellutgångar, med varierande grad av komplexitet. I detta arbete presenterar vi en systematisk studie av några av dessa metoder. Med fokus på textklassificeringsuppgiften och finjusterade stora förkränade språkmodeller visar vi först att många av de finjusterade modellerna inte är väl kalibrerade direkt, särskilt när data kommer från inställningar utanför domänen. Därefter jämför vi effektiviteten av några allmänt använda omkalibreringsmetoder (såsom ensembler, temperaturskalning). Därefter illustrerar vi empiriskt ett samband mellan destillation och kalibrering. Vi ser destillation som en regulariseringsterm som uppmuntrar elevmodellen att producera osäkerheter som matchar en lärarmodell. Med denna insikt utvecklar vi enkla omkalibreringsmetoder baserade på destillation utan extra kostnader för inferens-tid. Vi visar på GLUE-riktmärket att våra enkla metoder kan uppnå konkurrenskraftiga kalibreringsprestanda utanför domänen (OOD) med dyrare tillvägagångssätt. Slutligen inkluderar vi ablationer för att förstå nyttan av komponenter i vår föreslagna metod och undersöka överförbarheten av kalibrering via destillation.', 'sr': 'Nedavni napredak u NLP-ovim sistemima, posebno paradigma pretkivanja i finetuiranja, ostvario je veliki uspeh u predvidnoj tačnosti. Međutim, ovi sistemi obično nisu dobro kalibrirani zbog nesigurnosti izvan kutije. U literaturi je predloženo mnoge metode rekalibracije za kvantificiranje predviđene nesigurnosti i kalibriranje modela s različitim stupnjevima kompleksnosti. U ovom poslu predstavljamo sistematsku studiju o nekoliko ovih metoda. Fokusirajući se na zadatak klasifikacije teksta i najfiniji veliki jezički modeli, prvo pokazujemo da mnogi od najfinijih modela nisu dobro kalibrirani izvan kutije, posebno kada podaci dolaze iz nastava izvan domena. Sledeće, uspoređujemo učinkovitost nekoliko široko korišćenih metoda rekalibracije (kao što su ensemble, skaliranje temperature). Onda, empirički ilustrujemo vezu između destilacije i kalibracije. Mi smatramo destilaciju kao regularizacijski termin koji poticava studentski model da izvede nesigurnosti koje odgovaraju onima koji odgovaraju učiteljskom modelu. Sa ovim uvidom, razvijamo jednostavne metode rekalibracije na osnovu destilacije bez dodatnih troškova infekcije. Mi pokazujemo na kriteriji GLUE da naši jednostavni metodi mogu postići konkurentne izvan domena (OOD) kalibracije skupljih pristupa. Konačno uključujemo mogućnosti da razumemo korisnost komponenta našeg predloženog metoda i pregledamo prebacivanje kalibracije putem destilacije.', 'si': 'NLP පද්ධතියේ අලුත් ප්\u200dරධානයක්, විශේෂයෙන්ම ප්\u200dරධානය සහ ප්\u200dරධානයක්, ප්\u200dරධානය සඳහා ප්\u200dරධානයක්, ප්\u200dරශ්නයක නමුත්, මේ පද්ධතිය සාමාන්\u200dයයෙන්ම හොඳ විශ්වාස කරන්නේ නැහැ. ගොඩක් ආපහු කැලිබ්රේෂණ් විදියට ප්\u200dරශ්නය කරලා තියෙන්නේ ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් වෙනස් විදියට ප්\u200dරශ්නයක් තියෙන මේ වැඩේ අපි මේ විදියට පද්ධතිය පරීක්ෂණයක් තියෙනවා. පාළුවන් විශේෂණ වැඩය සහ ලොකු ප්\u200dරතික්ෂණ භාෂා මොඩේල්ස් වලට ප්\u200dරතික්ෂා කරනවා, අපි මුලින්ම පෙන්වන්නේ හොඳ විශේෂ විශේෂ විශේෂ විශේ ඊළඟට, අපි වැඩියෙන් ප්\u200dරයෝජනය කරපු ප්\u200dරයෝජනය විධානය කිහිපයක් සම්බන්ධ කරනවා. ඊට පස්සේ, අපි සාමාන්\u200dය විශේෂණය සහ කැලිබ්\u200dරේෂණය අතර සම්බන්ධයක් පෙන්වන්නේ. අපි විශේෂණය සාමාන්\u200dය විධානයක් විදියට බලන්නේ විදියට විදියට විදියට පිළිගන්න විදියට පිළිගන්න පුළ මේ අදහස් එක්ක, අපි සාමාන්\u200dය ආපහු සම්පූර්ණ විධානය කරනවා විශේෂණය සඳහා විශේෂණ විධානය නැති විශ අපි GLUE බෙන්ච්මාර්ක් එක පෙන්වන්නේ අපේ සරල විද්\u200dයාවය පුළුවන් කියලා, අපේ සාමාන්\u200dය විද්\u200dයාවය පුළුවන් පුළුවන් පුළුවන් වි අන්තිමේදී, අපිට ප්\u200dරයෝජන විධානයේ භාවිතාවක් තේරුම් ගන්න පුළුවන් සම්බන්ධයක් තියෙනවා අපේ ප්\u200dරයෝජන විධා', 'ta': 'NLP அமைப்புகளில் சமீபத்தில் முன்னேற்றம், குறிப்பாக மழை- மற்றும் முன்னேற்றும் பரிமாணத்தில், முன்வெற்றியடைந்து விட் ஆயினும், இந்த அமைப்புகள் பொதுவாக சிறப்பாக அளவிடப்படுத்தப்படவில்லை பெரும்பாலான முறைமைகள் பார்க்கப்பட்டுள்ளது, முன்னோட்டு உறுதிப்படுத்த மாதிரி வெளியீடு இந்த வேலையில், இந்த சில முறைகளில் நாம் ஒரு அமைப்பான ஆய்வு கொடுக்கிறோம். உரை வகைப்படுத்தல் பணியின் மேல் கவனம் செலுத்தி பெரிய மொழி மாதிரிகளின் மேல், முதலில் நாம் காண்பிக்கிறோம் பெட்டியில் இருந்து பெரிய மாதிரிகள் பெட்டியி அடுத்து, பின்னர், நாம் வெறுப்பு மற்றும் calibration இடையே ஒரு இணைப்பை வெளிப்படுத்துகிறோம். மாணவர் மாதிரி மாதிரியை வெளியீட்டிற்கு உறுதிப்படுத்தும் வேறுபாடு என்று நாம் பார்க்கிறோம். ஆசிரியர் மாதிரிய இந்த பார்வையுடன், நாம் சுலபமான ஒழுங்குபடுத்தல் முறைமைகளை உருவாக்குகிறோம் கூடுதலான நேர செலவு இல்லாமல் பிரித்தல நாங்கள் GLUE பெங்குறிப்பில் காட்டுகிறோம் எளிதான முறைமைகள் தளத்திலிருந்து வெளியேற்றும் களஞ்சியத்தின் செயல்பாட்டை பெற முடி Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.', 'ur': 'NLP سیستموں میں اچھی طرح کی پیشرفت، مخصوصاً آرام و آرام کی پارادیگ، بہت بڑی کامیابی پہنچ گئی ہے۔ لیکن ان سیستموں کو معمولاً بغیر یقین کے لئے بہترین اندازے نہیں کی جاتی۔ بہت سی دوبارہ کالیبرال طریقے لکھائی میں پیش بینی غیر قطعی اور موڈل کے نتائج کا مقدار کرنے کے لئے پیش بینی کی گئی ہیں، مختلف درجے پیچیدگی کے ساتھ۔ ہم اس کام میں ایک سیستمائی تحقیق کریں گے ان طریقوں میں سے تھوڑی سیستمائی تحقیق۔ ہم پہلے دکھاتے ہیں کہ بہت سے اچھے بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر بغیر کیلیبراٹ نہیں ہوتے، مخصوصا جبکہ ڈاٹ ڈومین سے آتی ہے۔ آگے، ہم ایک چند گھیرے سے استعمال کیے ہوئے دوبارہ کالیبراشن طریقے کے اثرات کے مطابق مقایسہ کرتے ہیں (جیسے انسبلز، تولیدہ اسکیلینگ)۔ پھر ہم مطمئن طور پر تفریق اور کالیبریٹ کے درمیان ایک ارتباط دکھاتے ہیں ہم تفریق کو ایک قانون تفریق کی توریت کے طور پر دیکھتے ہیں کہ استاد مدل کو غیر قطعات کے اخراج کرنے کے لئے ڈراتا ہے جو ایک استاد مدل کے مطابق مطابق ہے. اس نظر کے ساتھ، ہم آسان تخلیق طریقوں کی تخلیق پر بنیاد رکھتے ہیں جن کے بدلے کوئی اضافہ تخلیق وقت کے قیمت نہیں ہے. ہم GLUE بنچم مارک پر نشان دیتے ہیں کہ ہمارے سادہ طریقے کو دومین (OOD) کے بیرون کالیبریٹ کامپیوتر پہنچ سکتے ہیں بالآخر، ہم اپنے پیشنهاد طریقے کے مطابقتوں کو سمجھنے کے لئے قابلیت شامل کرتے ہیں اور تقسیم کے ذریعہ کالیبراٹ کے قابلیت کو تحقیق کرتے ہیں.', 'uz': "Yaqinda, NLP tizimlarida muvaffaqiyatlar, hozir oldinga va suhbatning paradigligi, predikt tashkilotda muvaffaqiyatli muvaffaqiyatli topdi. Lekin, bu tizimlar odatda haqiqiqatdan tashqi holatga tayyorlanmagan. Koʻp taklif qilish usullari tahrirchisi haqida o'xshash va model natijalarini aniqlash uchun talab qilindi, murakkab darajadagi darajada. In this work, we present a systematic study of a few of these methods.  Matn classification vazifasi va katta tahrirlangan tillar modellariga fokuslangan, birinchi marta ko'pchilik modellarning ko'pchiligi qo'shimcha modellarni quyidagi holatdan kelmaydi, hususan domen moslamalaridan chiqishda maʼlumot kelmaydi. Keyingi biz bir necha ko'p ishlatilgan rekalibratsiyalash usullarining effektligini kamaytamiz (masalan shamollar, temperatur scaling). Keyin biz ajratish va kaliblikning orasidagi aloqalarni aniqlash mumkin. Biz o'quvchi modelni o'qituvchi modellarga mos keladigan haqiqatgina ishlab chiqarishni ko'rinamiz. Bu ko'rinishimiz bilan, biz qoʻshimcha vaqt qiymati yo'q o'zgarish asosida oddiy rekalibratsiyalash usullarini yaratishmiz. Biz GLUE sahifani ko'rsatdik, biz oddiy usullamiz domen'dan tasdiqlash imkoniyatini bajarishimiz mumkin, masalan ko'proq qiyin usullar. Oxirgi, biz প্রস i qilingan usullarning foydalanishini tushunishga qo'llanmiz va kalitlarni o'zgartirish orqali o'zgarishni o'rganamiz.", 'vi': 'Những tiến bộ gần đây trong hệ thống NLP, và nhất là hệ thống sản xuất tiên thời và độ chính xác dự đoán. Tuy nhiên, những hệ thống này thường không được hiệu chỉnh cẩn thận cho việc không rõ ràng. Nhiều phương pháp tái lập đã được đề xuất trong văn học để xác định tỷ lệ rủi ro dự đoán và đo đạc các sản phẩm mẫu, với độ phức tạp khác nhau. Trong công việc này, chúng tôi có một nghiên cứu hệ thống về một số các phương pháp này. Tập trung vào nhiệm vụ phân loại văn bản và các mô hình ngôn ngữ lớn được chỉnh sửa cẩn thận, trước tiên chúng tôi cho thấy rằng nhiều mô hình chưa được chỉnh cẩn thận không được hiệu chỉnh, nhất là khi dữ liệu đến từ các thiết lập ngoài miền. Tiếp theo, chúng ta so sánh hiệu quả của vài phương pháp tái kiểm toán phổ biến (như kết hợp, đo nhiệt độ). Sau đó, tôi có kinh nghiệm minh họa mối liên hệ giữa chưng cất và chỉnh sửa. Chúng tôi coi việc chưng cất là một thuật ngữ quy tắc thúc đẩy mẫu sinh viên xuất ra những bất ngờ tương đương với mẫu giáo. Với cái nhìn này, chúng tôi phát triển các phương pháp tái lập đơn giản dựa trên chưng cất mà không có chi phí liên kết thời gian. Chúng t ôi cho thấy t r ên tiêu chuẩn GLUE rằng phương pháp đơn giản của chúng tôi có thể đạt hiệu ứng độ cân bằng ngoài miền (OOOD) cạnh tranh tốn kém hơn nhiều phương pháp. Cuối cùng, chúng ta có thể loại bỏ khả năng hiểu được sự hữu dụng của các thành phần trong phương pháp đã đề nghị và xem xét khả năng chuyển giao giữa các phân phối.', 'bg': 'Неотдавнашният напредък в системите за НЛП, особено парадигмата за предтрениране и фина настройка, постигна голям успех в прогнозната точност. Въпреки това, тези системи обикновено не са добре калибрирани за неопределеност извън кутията. Много методи за рекалибриране са предложени в литературата за количествено определяне на предсказуемата неопределеност и калибриране на изходните модели с различна степен на сложност. В тази работа представяме систематично проучване на някои от тези методи. Фокусирайки се върху задачата за класификация на текста и фино настроени големи предварително обучени езикови модели, ние първо показваме, че много от фино настроените модели не са добре калибрирани извън кутията, особено когато данните идват от настройки извън домейна. След това сравняваме ефективността на няколко широко използвани методи за рекалибриране (като ансамбли, температурно скалиране). След това емпирично илюстрираме връзката между дестилацията и калибрирането. Ние разглеждаме дестилацията като термин за регулиране, насърчаващ студентския модел да произвежда несигурности, които съответстват на тези на учителския модел. С тази проницателност разработваме прости методи за рекалибриране, базирани на дестилация, без допълнителни разходи за времето за заключение. Показваме на бенчмарка, че нашите прости методи могат да постигнат конкурентни резултати за калибриране извън домейна с по-скъпи подходи. Накрая, включваме аблации, за да разберем полезността на компонентите на нашия предложен метод и да изследваме преносимостта на калибрирането чрез дестилация.', 'hr': 'Nedavni napredak u sustavima NLP-a, posebno paradigma pretkivanja i finetuiranja, postigli su veliki uspjeh u predvidnoj točnosti. Međutim, ovi sustavi obično nisu dobro kalibrirani zbog nesigurnosti izvan kutije. U književnosti je predloženo mnoge metode rekalibracije za kvantificiranje predvidne nesigurnosti i kalibriranje modela s različitim stupnjevima kompleksnosti. U ovom poslu predstavljamo sistematsku studiju o nekoliko ovih metoda. Fokusirajući se na zadatak klasifikacije teksta i najfiniji veliki jezički modeli, prvo pokazujemo da mnogi od najfinijih modela nisu dobro kalibrirani izvan kutije, posebno kada podaci dolaze iz settings izvan domena. Sljedeće uspoređujemo učinkovitost nekoliko široko korišćenih metoda rekalibracije (poput ensembla, skaliranja temperature). Onda, empirički ilustrujemo vezu između destilacije i kalibracije. Mi smatramo destilacijom kao termin regularizacije koji ohrabruje studentski model da izvede nesigurnosti koje odgovaraju onima učiteljskog model a. S ovim uvidom razvijemo jednostavne metode rekalibracije temeljne na destilaciji bez dodatnih troškova za vrijeme infekcije. Mi pokazujemo na kriteriji GLUE da naše jednostavne metode mogu postići konkurentne izvan domena (OOD) kalibracije skupljih pristupa. Konačno uključujemo mogućnosti razumijeti korisnost komponenta našeg predloženog metode i pregledati prebacivanje kalibracije putem destilacije.', 'nl': 'Recente vooruitgang in NLP-systemen, met name het pretraining- en finetuning-paradigma, heeft grote successen geboekt op het gebied van voorspellende nauwkeurigheid. Deze systemen zijn echter meestal niet goed gekalibreerd voor onzekerheid out-of-the-box. Veel herkalibratiemethoden zijn in de literatuur voorgesteld voor het kwantificeren van voorspellende onzekerheid en het kalibreren van modelproducten, met verschillende niveaus van complexiteit. In dit werk presenteren we een systematische studie van een aantal van deze methoden. We concentreren ons op de tekstclassificatietaak en verfijnde grote vooraf getrainde taalmodellen, en laten eerst zien dat veel van de verfijnde modellen niet goed gekalibreerd zijn out-of-the-box, vooral wanneer de gegevens afkomstig zijn van out-of-domain instellingen. Vervolgens vergelijken we de effectiviteit van enkele veelgebruikte herkalibratiemethoden (zoals ensembles, temperatuurschalen). Vervolgens illustreren we empirisch een verband tussen destillatie en kalibratie. We zien destillatie als een regularisatieterm die het studentenmodel aanmoedigt om onzekerheden uit te voeren die overeenkomen met die van een lerarenmodel. Met dit inzicht ontwikkelen we eenvoudige herkalibratiemethoden gebaseerd op distillatie zonder extra inferentietijd kosten. Op de GLUE benchmark laten we zien dat onze eenvoudige methoden concurrerende out-of-domain (OOD) kalibratie prestaties kunnen bereiken zonder duurdere benaderingen. Tot slot nemen we ablaties op om het nut van componenten van onze voorgestelde methode te begrijpen en de overdraagbaarheid van kalibratie via distillatie te onderzoeken.', 'de': 'Jüngste Fortschritte bei NLP-Systemen, insbesondere das Paradigma Vortraining und Feinabstimmung, haben große Erfolge bei der Vorhersagegenauigkeit erzielt. Allerdings sind diese Systeme in der Regel nicht gut auf Unsicherheit aus der Box kalibriert. Viele Rekalibrierungsmethoden wurden in der Literatur vorgeschlagen, um Vorhersageunsicherheit zu quantifizieren und Modellergebnisse mit unterschiedlichem Komplexitätsgrad zu kalibrieren. In dieser Arbeit stellen wir eine systematische Untersuchung einiger dieser Methoden vor. Mit Blick auf die Textklassifikationsaufgabe und fein abgestimmte große vortrainierte Sprachmodelle zeigen wir zunächst, dass viele der fein abgestimmten Modelle nicht sofort gut kalibriert sind, insbesondere wenn die Daten aus Out-of-Domain-Einstellungen stammen. Als nächstes vergleichen wir die Effektivität einiger weit verbreiteter Rekalibrierungsmethoden (wie Ensembles, Temperaturskalierung). Anschließend zeigen wir empirisch einen Zusammenhang zwischen Destillation und Kalibrierung auf. Wir betrachten Destillation als Regularisierungsbegriff, der das Schülermodell ermutigt, Unsicherheiten auszugeben, die denen eines Lehrermodells entsprechen. Mit diesen Erkenntnissen entwickeln wir einfache Rekalibrierungsmethoden basierend auf Destillation ohne zusätzliche Inferenz-Zeit-Kosten. Wir zeigen am GLUE Benchmark, dass unsere einfachen Methoden wettbewerbsfähige Out-of-Domain (OOD) Kalibrierleistungen ohne teurere Ansätze erzielen können. Abschließend schließen wir Ablationen ein, um die Nützlichkeit der Komponenten unserer vorgeschlagenen Methode zu verstehen und die Übertragbarkeit der Kalibrierung durch Destillation zu untersuchen.', 'id': 'Kemajuan baru-baru ini dalam sistem NLP, terutama paradigma pretraining-dan-finetuning, telah mencapai sukses besar dalam akurasi prediksi. Namun, sistem-sistem ini biasanya tidak dikalibrasi dengan baik untuk ketidakpastian keluar dari kotak. Banyak metode rekalibrasi telah diusulkan dalam literatur untuk mengurangi ketidakpastian prediksif dan kalibrasi model output, dengan derajat yang berbeda kompleksitas. Dalam pekerjaan ini, kami mempersembahkan sebuah studi sistematis dari beberapa metode ini. Fokus pada tugas klasifikasi teks dan model bahasa besar yang dilatih sebelum dilatih, kami pertama-tama menunjukkan bahwa banyak model yang dilatih tidak baik dikalibrasi keluar dari kotak, terutama ketika data berasal dari pengaturan luar domain. Selanjutnya, kita membandingkan efektivitas beberapa metode rekalibrasi yang banyak digunakan (seperti ensembles, skala suhu). Kemudian, kita secara empiris ilustrasi hubungan antara distillasi dan kalibrasi. Kami melihat destilasi sebagai istilah regularisasi yang mendorong model siswa untuk mengeluarkan ketidakpastian yang cocok dengan model guru. Dengan pengetahuan ini, kami mengembangkan metode rekalibrasi sederhana berdasarkan destilasi tanpa biaya tambahan inferensi-waktu. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches.  Akhirnya, kita termasuk ablasi untuk memahami penggunaan komponen dari metode kami yang diusulkan dan memeriksa transferabilitas kalibrasi melalui destilasi.', 'da': 'De seneste fremskridt inden for NLP-systemer, især paradigmet pre-training og finjustering, har opnået stor succes med hensyn til prædiktiv nøjagtighed. Disse systemer er dog normalt ikke godt kalibreret til usikkerhed ud af boksen. Mange rekalibreringsmetoder er blevet foreslået i litteraturen til kvantificering af forudsigelig usikkerhed og kalibrering af modeludgange med varierende grad af kompleksitet. I dette arbejde præsenterer vi en systematisk undersøgelse af nogle få af disse metoder. Med fokus på tekstklassifikationsopgaven og finjusterede store forudtrænede sprogmodeller viser vi først, at mange af de finjusterede modeller ikke er godt kalibreret ud af boksen, især når dataene kommer fra indstillinger uden for domænet. Dernæst sammenligner vi effektiviteten af nogle få almindeligt anvendte omkalibreringsmetoder (såsom ensembler, temperaturskalering). Derefter illustrerer vi empirisk en sammenhæng mellem destillation og kalibrering. Vi ser destillation som et regulariseringsbegreb, der opfordrer elevmodellen til at producere usikkerheder, der matcher lærermodellen. Med denne indsigt udvikler vi enkle omkalibreringsmetoder baseret på destillation uden yderligere omkostninger for inference-tid. Vi viser på GLUE benchmark, at vores enkle metoder kan opnå konkurrencedygtige kalibreringsydelser uden for domænet (OOD) med dyrere metoder. Endelig inkluderer vi ablationer for at forstå nytten af komponenter i vores foreslåede metode og undersøge overførbarheden af kalibrering via destillation.', 'ko': 'NLP 시스템의 최신 진전, 특히 예훈련과 마이크로스피커 모델은 정확성을 예측하는 데 큰 성공을 거두었다.그러나 이 시스템들은 통상적으로 상자를 열면 바로 사용할 수 있는 불확실성을 잘 조정하지 못한다.문헌에서 많은 재교정 방법을 제시했는데 양적 예측 불확실성과 교정 모델 출력에 사용되는데 그 복잡도는 각각 다르다.이 업무에서 우리는 그 중의 몇 가지 방법에 대해 체계적인 연구를 진행하였다.텍스트 분류 임무와 정교하게 조정된 대형 예비 훈련 언어 모델을 중점적으로 주목한다. 우리는 먼저 많은 정교하게 조정된 모델들이 상자를 열면 바로 사용할 수 있는 교정을 잘 하지 못했고 특히 데이터가 역외에서 설정되었을 때를 나타낸다.다음에 우리는 몇 가지 광범위하게 사용되는 재교정 방법(예를 들어 집적, 온도 표시)의 유효성을 비교했다.그리고 우리는 증류와 교정 사이의 관계를 경험으로 설명한다.우리는 증류를 정규화 용어로 보고 학생 모델의 수출이 교사 모델과 일치하는 불확실성을 장려할 것이다.이러한 인식을 바탕으로 우리는 증류를 바탕으로 하는 간단한 재교정 방법을 개발하여 추가 시간 원가를 추정할 필요가 없다.GLUE 벤치마크 테스트에서 Dell의 단순한 접근 방식은 경쟁력 있는 도메인 밖(OOD) 정렬 성능을 보다 비싸게 구현할 수 있음을 보여줍니다.마지막으로, 우리는 소식으로 우리가 제시한 방법의 부품의 유용성을 파악하고, 증류를 통해 교정할 수 있는 전이성을 검사한다.', 'sw': 'Maendeleo ya hivi karibuni katika mfumo wa NLP, hususani upinzani wa matumizi ya mvua na mipango, yamekuwa na mafanikio makubwa katika ukweli unaotabiri. Hata hivyo, mifumo hii mara nyingi hazitangazwa vizuri kwa usio na uhakika kutoka kwenye boksi hilo. Utawala mwingi wa upatikanaji umependekezwa katika fasihi kwa kuhakikisha uthibitisho wa ukweli wa kitabiri na matokeo ya mifano ya kupiga simu, yenye vyeo tofauti vya utata. Katika kazi hii, tunaweka utafiti wa mfumo wa namna chache hizi. Kuchungulia kazi ya usambazaji wa maandishi na mifano makubwa ya lugha iliyoandaliwa, tunaonyesha kwanza kuwa baadhi ya mifano ya vizuri hazina vizuri ya kutolewa ndani, hasa pale taarifa zinapotokea kutoka kwenye mazingira ya ndani. Baadae, tunalinganisha ufanisi wa mbinu chache zilizotumika kwa kutumia upya (kama vile mabomu, upepo wa joto). Kisha tunaonyesha uhusiano kati ya kutenganisha na ukarabati. Tunaona tofauti kama neno la kudhibiti linalowahamasisha muundo wa wanafunzi kutoa utoaji usio na uhakika unaohusisha mifano ya mwalimu. Kwa mtazamo huu, tunaendelea njia rahisi za kujitengeneza kwa kutumia tofauti bila gharama za muda mrefu za ugonjwa. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches.  Mwisho, tunajumuisha vifaa ili kuelewa matumizi ya vifaa vya mbinu zetu zilizopendekezwa na kuchunguza mabadiliko ya ukarabati kupitia tofauti.', 'fa': 'پیشرفت اخیرا در سیستم\u200cهای NLP، مخصوصاً پارادیگ\u200cهای پیش\u200cبینی و نیکویی، موفقیت بزرگی در دقیق پیش\u200cبینی رسیده است. ولی این سیستم ها معمولا به خاطر مطمئن شدن بیرون جعبه خیلی خوب نیستند. بسیاری از روش\u200cهای بازگرداندن در ادبیات پیشنهاد شده\u200cاند که برای مقدار اندازه\u200cگیری غیرقابل پیش\u200cبینی و نتیجه\u200cهای مدل کالیبری با درجه\u200cهای پیچیدگی متفاوت باشد. در این کار، ما یک تحقیق سیستماتی از چند از این روش را پیشنهاد می کنیم. با تمرکز روی کار جدید کردن متن و مدل های زبانی بزرگی که زیبایی نداشته باشند، اول نشان می دهیم که بسیاری از مدل های زیبایی که نداشته باشند از بیرون جعبه، به خصوص زمانی که داده ها از تنظیمات خارج از دامنی می آیند، خیلی خوب kaliبره نمی شوند. بعدش، ما موثرت چند روش بازگشتی استفاده شده را مقایسه می کنیم (مثل انجمبل، مقایسه دما). سپس، ما به طور امپراطوری ارتباطی بین جدا شدن و کالیبران را نشان می دهیم. ما تفاوت را به عنوان کلمه ساده\u200cسازی نگاه می\u200cکنیم که به مدل دانش آموزان تشویق می\u200cکند تا مطمئن\u200cهایی که با آن\u200cها از مدل معلم پیدا می\u200cکنند. با این مشاهده، ما روش\u200cهای ساده بازسازی را بر اساس جدایی با هزینه\u200cهای بیشتری در زمان آلودگی توسعه می\u200cکنیم. ما روی صندوق GLUE نشان می دهیم که روش ساده\u200cهای ما می\u200cتوانند به انجام کالیبران بیرون از دومین (OOD) مسابقه\u200cای برسد که به طریق گران\u200cتر است. بالاخره، ما شامل توانایی برای فهمیدن استفاده از بخش\u200cهای روش پیشنهاد ما هستیم و تحقیق قابلیت انتقال کالیبرازی از طریق استفاده از طریق استفاده.', 'af': "Onlangse vorderings in NLP-stelsels, spesifieke die pretraining-en-finetuning paradigme, het groot sukses in voorskoude presisie bereik. Maar hierdie stelsels is gewoonlik nie goed kalibreer vir onbevestigheid uit die boks nie. Baie rekalibreeringsmetodes is voorgestel in die literateit om voorskou onbevestigheid en kalibreeringsmodel uitvoerings te quantifiseer met verskillende grade van kompleksiteit. In hierdie werk voorsien ons 'n sistematiese studie van 'n paar van hierdie metodes. Fokus op die teks klasifikasie taak en finetuned groot pretrained taal modelles, wys ons eerste dat baie van die finetuned modelles nie goed kalibreer uit- van- the- box nie, veral wanneer die data uit- van- domein instellings kom. Volgende, ons vergelyk die effektiviteit van 'n paar brei gebruikte rekalibreeringsmetodes (soos ensembles, temperatureskalagging). Dan, ons is empiriese 'n verbinding tussen destilasie en kalibrering illustreer. Ons sien distillasie as 'n regularisasie term wat die student model versoek om onversekerheid uit te voer wat ooreenstem die wat van 'n onderwyser model ooreenstem. Met hierdie insig ontwikkel ons eenvoudige rekalibreeringsmetodes gebaseer op destilasie met geen addisionele inferensie-tyd koste nie. Ons wys op die GLUE-benchmark dat ons eenvoudige metodes kan rekenaar gemeenskaplike uit-domein (OOD) kalibreeringsprestasie, w.r.t. meer koste toegang kan bereik. Eindelik, ons insluit toepassings om te verstaan die bruikbaarheid van komponente van ons voorgestelde metode en ondersoek die oordragbaarheid van kalibrering deur destilasie.", 'tr': 'NLP sistemalarynda ýöne ýakyn ösümlikler, ýöne ýakyn ýakyn-ösümli paradigma, tahmin edilen dogrylyklarda gaty başarnyk başardy. Ýöne bu sistemler adatça gorkunçylyk üçin gowy kalibrelenmez. literatura öňünden näçe çykyş we çykyş derejesi çarpmak üçin öngörümli kesgitlenme we kalibrleme nusgalaryny çarpmak üçin tekrarlama yöntemleri teklip edildi. Bu işde, biz bu ýagdaýlaryň birnäçe sistematik bir öwrenmesini görkeýäris. Metin klasifikasyýasy täblisasyny we uly süýtgeli dil nusgalaryny üns bermek üçin birinji gezek, iň bellenen nusgalaryň köpüsi kastdan daşaryk kalibrelenmedigini görkez. Sonra, birkaç geniş şekilde kullanılan tekrarlama yöntemlerinin etkinliğini karşılaştırıyoruz. Soňra biz empiriýa daşlyk we kalibr arasynyň bir baglaýyşyny görkez. Biz paýlaşmagy düzenli terjime diýip okuwçylar nusgasyny mugallymyň nusgasyna mejbur bolan çykarmak üçin örän düzenli terjime edip görýäris. Bu düşünjelerimiz bilen, daýratma täzeliklerini boýunça hasaplamak täzeliklerine daýanýar. Biz GLUE düzlemlerinde basit yönlerimiz domaýdan (OOD) kalibr etmäniň üstüne ýetip biljekdigini görkeýäris. Soňunda, teklip eden yönümizdeki aýratynyň ulanylygyny düşünmek üçin kynçylyklary dahil edýäris we kalibreleme taýýarlançylygyny göçürmek üçin bardyrys.', 'az': 'NLP sistemlərində son dəyişiklik, özlərinə də pretraining-and-finetuning paradigmi, tədbir edilən doğruluqlarda böyük başarıya çatdı. Ancaq bu sistemlər genellikle qeyri-təhlükəsizlik üçün çox yaxşı kalibr edilməz. Bir çox yenidən kalibrasyon metodları, müxtəlif dərəcələr kompleksitə ilə, tədbirli uncertanlıqları və modellərin çıxışlarını quantifik etmək üçün dəftərdə təklif edilmişdir. Bu işdə bir neçə metodlardan sistematik bir təhsil göstəririk. Metin klasifikasyonu və böyük dil modellərinə təsirləndirək, ilk dəfə ən yaxşı modellərin çoxu qutudan kalibrlənmədiyini göstərdik, özlərinə də məlumatlar domeindən çıxdığı zaman. Sonra, biz çox geniş istifadə edilən bir neçə yeni kalibrasyon metodlarının (ensembles, temperature scaling kimi, ensembles, temperature scaling kimi) efektivitətini salırıq. Sonra, biz destilasyon və kalibrasyon arasındakı bağlantı göstəririk. Biz distillasyonu müəllimlərin modellərinə uyğunlaşan təhsil modelini təşkil etmək üçün düzgün təhsil olaraq görürük. Bu baxışla, bizim destilasyona dayanan basit yenidən kalibrasyon metodlarını təhsil edirik. Biz GLUE benchmark ında basit metodlarımızın döyüşə çıxıb (OOD) kalibrləmə performansını və.r.t. daha pahalı t ərzlərini başa düşə biləcəyini göstəririk. Sonunda, təbliğ etdiyimiz metodların faydalanılığını anlamaq və destilasyon vasitəsilə kalibrləmə qabiliyyətini incidirik.', 'sq': 'Përparimet e fundit në sistemet NLP, veçanërisht paradigma e parastërvitjes dhe përmirësimit, kanë arritur sukses të madh në saktësinë parashikuese. Megjithatë, këto sisteme zakonisht nuk janë të kalibruar mirë për pasigurinë jashtë kutisë. Shumë metoda rikalibrimi janë propozuar në letërsinë për kuantifikimin e pasigurisë parashikuese dhe kalibrimin e rezultateve të modelit, me grada të ndryshme kompleksiteti. Në këtë punë, ne paraqesim një studim sistematik të disa nga këto metoda. Duke u përqëndruar në detyrën e klasifikimit të tekstit dhe modelet e mëdha të paratrajnuara të gjuhës, ne së pari tregojmë se shumë nga modelet e përmirësuar nuk janë kalibruar mirë jashtë kutisë, veçanërisht kur të dhënat vijnë nga rregullimet jashtë domenit. Pastaj, ne krahasojmë efektshmërinë e disa metodave të risalibrimit të përdorur gjerësisht (të tilla si ansamblet, shkallëzimi i temperaturës). Pastaj, ne empirikisht ilustrojmë një lidhje midis distillacionit dhe kalibrimit. Ne e shohim distillacionin si një term ë rregullimi duke inkurajuar modelin student ësh për të prodhuar pasiguri që përputhen me a to të modelit mësues. Me këtë kuptim, ne zhvillojmë metoda të thjeshta rilibrimi bazuar në distillacion pa kosto shtesë kohe-inferencë. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches.  Më në fund, ne përfshijmë aftësitë për të kuptuar dobinë e komponenteve të metodës sonë të propozuar dhe për të shqyrtuar transferueshmërinë e kalibrimit nëpërmjet distillacionit.', 'bn': 'এনএলপি সিস্টেমের সম্প্রতি উন্নয়ন, বিশেষ করে বৃষ্টি এবং ফিনিউটিং প্যারাডিম, ভবিষ্যতীয় সঠিক পরিস্থিতিতে বিশাল সফল হয়ে তবে সাধারণত এই ব্যবস্থাগুলো নিশ্চিত বাক্সের জন্য ভালো ক্যালিবার্ট করা হয় না। সাহিত্যে অনেক পুনরাবৃত্তির পদ্ধতি প্রস্তাব করা হয়েছে ভবিষ্যতীয় সন্দেহ এবং মডেল আউটপুটের পরিমাণ নির্ধারণ করার জন্য। এই কাজে আমরা সিস্টেমিক গবেষণা উপস্থাপন করছি এই কিছু পদ্ধতির ব্যাপারে। টেক্সট গ্রাফিকেশন কাজের উপর মনোযোগ আকর্ষণ করে বিশাল ভাষার মডেল, আমরা প্রথম দেখাচ্ছি যে অনেক সুন্দর মডেল বাক্স-বাক্স থেকে ভালো ক্যালিবার্ড করা হয়নি, বিশেষ কর পরবর্তীতে, আমরা কয়েকটি ব্যাপক ব্যবহার করা রিসালিব্রেশন পদ্ধতির কার্যক্রমের তুলনা করি (যেমন বিস্ফোরণ, তাপমাত্রা ক্যালেলিং তারপর আমরা ক্ষমতাভাবে বিচ্ছিন্ন এবং ক্যালিবেশনের মধ্যে একটি সংযোগ ব্যক্ত করি। আমরা শিক্ষক মডেলের সাথে যারা শিক্ষক মডেলের সাথে মিলে তাদের নিশ্চিতভাবে উৎপাদন করার একটি নিয়মিত শব্দ হিসেবে বিচ্ছ এই দৃষ্টিভঙ্গি দিয়ে আমরা সাধারণ পুনরাবৃত্তির পদ্ধতি তৈরি করি যা বিচ্ছিন্ন করা হয়েছে, যার ফলে কোনো ক্ষুদ্র সময়ের খরচ নে We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches.  শেষ পর্যন্ত আমরা আমাদের প্রস্তাবিত পদ্ধতির ব্যাপারে বুঝতে পারি ক্যালিবেশনের পরিবর্তন পরীক্ষা করি।', 'am': 'የቀድሞው የNLP ስርዓቶች፣ በተለይ የዝናብ እና የፍላጎት ፍላጎት፣ የመፍጠር ውጤት ትልቅ ድል አግኝቷል፡፡ ምንም እንኳን፣ እነዚህ ስርዓቶች ብዛት ከቦታው ውጭ ለመሆን አይቆጠሩም፡፡ ብዙዎች አካባቢ ሥርዓቶች በተለየ ደረጃዎች በተለየ ጥያቄ እና የሞዴል ውጤቶችን በማስተካከል በመጻሕፍት ውስጥ ጥያቄ ተሰጥተዋል፡፡ በዚህ ሥራ የጥቂት ዓይነቶች የስርዓት ትምህርት እናቀርባለን፡፡ የጽሑፍ ክፍል አድራሻ እና የበለጠ የቋንቋ ምሳሌዎችን በመጠቀም ላይ እናሳየዋለን፡፡ የሚቀጥለውን የአንዳንድ ጥቂት የተጠቀሙትን ስፋት እናሳያታለን (እንደ ስምፕቦል፣ የቁመት መስኮት) በኋላም በተለይ እና በማስተካከል ግንኙነትን እናሳውቃለን፡፡ ተማሪውን ሞዴል የሚያስፈልገውን የአስተማሪውን ሞዴል የሚያስፈልገውን ግንኙነት እንዲያወጣ እናየዋለን፡፡ በዚህ ዓይነት፣ በተጨማሪው የጊዜው ዋጋ ሳይኖር የተለየን ቀላል ተሳካሚ ሥርዓት እናደርጋለን፡፡ We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches.  በመጨረሻው የካሌብሪክ ክፍተቶችን ለማስተዋል ጥቅም ለማስተዋል እና በመለወጥ የክሊፕቦርዱን ማቀናቀል እናደርጋለን፡፡', 'hy': 'Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy.  Այնուամենայնիվ, այս համակարգերը սովորաբար լավ չեն կալիբրացված անորոշության համար արկղից դուրս: Գրագրության մեջ առաջարկվել են բազմաթիվ վերականգնման մեթոդներ, որոնք օգտագործում են կանխատեսողական անորոշությունը և մոդելի արտադրանքները կալիբրավորելու համար, տարբեր բարդությունների աստիճաններով: Այս աշխատանքի ընթացքում մենք ներկայացնում ենք այս մեթոդներից մի քանի սիստեմատիկ ուսումնասիրություն: Սկզբում կենտրոնացնելով տեքստի դասակարգման խնդրի վրա և բարձրացված մեծ նախադասակարգված լեզվի մոդելների վրա, մենք առաջին անգամ ցույց ենք տալիս, որ բարձրացված մոդելներից շատերը լավ չեն կալիբրացված արկղից, հատկապես երբ տվյալները գալիս են արկղից դու Հաջորդ, մենք համեմատում ենք մի քանի լայնորեն օգտագործված վերականգնման մեթոդների արդյունավետությունը (ինչպիսիք են համակարգերը, ջերմաստիճանի աճը): Հետո մենք էմպիրիկապես ցույց ենք տալիս դիսլիլացիայի և կալիբրացիայի միջև կապը: Մենք դիտարկում ենք դիսլիլացիան որպես կանոնավոր տերմին, որը խրախուսում է ուսանողի մոդելը արտադրել անորոշություններ, որոնք համապատասխանում են ուսուցչի մոդելի մոդելներին: Այս ընկալումների միջոցով մենք զարգանում ենք պարզ վերականգնման մեթոդներ, որոնք հիմնված են դիսլիլացիայի վրա, առանց ավելին հետևանքի-ժամանակի արժեքի: Մենք ցույց ենք տալիս GLUE-ի հարաբերականի վրա, որ մեր պարզ մեթոդները կարող են հասնել մրցակցության արտադրյալ կալիբրացիայի արդյունքներին, որոնք ավելի թանկ են: Վերջապես, մենք ներառում ենք մեր առաջարկած մեթոդի բաղադրիչների օգտակարության հասկանալու ունակությունը և քննարկում ենք կալիբրացիայի փոխանցելիությունը դիսլիլացիայի միջոցով:', 'bs': 'Nedavni napredak u NLP-ovim sistemima, posebno paradigma pretkivanja i finetuiranja, ostvario je veliki uspjeh u predvidnoj tačnosti. Međutim, ovi sistemi obično nisu dobro kalibrirani zbog nesigurnosti izvan kutije. U literaturi je predloženo mnoge metode rekalibracije za kvantificiranje predvidne nesigurnosti i kalibriranje modela s različitim stupnjevima kompleksnosti. U ovom poslu predstavljamo sistematsku studiju o nekoliko ovih metoda. Fokusirajući se na zadatak klasifikacije teksta i najfiniji veliki jezički modeli, prvo pokazujemo da mnogi od fino neodređenih modela nisu dobro kalibrirani izvan kutije, posebno kada podaci dolaze iz settings izvan domena. Sledeće, uspoređujemo učinkovitost nekoliko široko korišćenih metoda rekalibracije (poput ensembla, skaliranja temperature). Onda, empirički ilustrujemo vezu između destilacije i kalibracije. Mi smatramo destilaciju kao termin regularizacije koji ohrabruje studentski model da izvede nesigurnosti koje odgovaraju onima koji odgovaraju učiteljskom modelu. S ovim uvidom razvijamo jednostavne metode rekalibracije na osnovu destilacije bez dodatnih troškova infekcije. Mi pokazujemo na kriteriji GLUE da naše jednostavne metode mogu postići konkurentne izvan domena (OOD) kvalifikacije skupljih pristupa. Na kraju, uključujemo mogućnosti da razumijemo korisnost komponenta našeg predloženog metoda i pregledamo prebacivanje kalibracije putem destilacije.', 'ca': "Els avanços recents en els sistemes NLP, sobretot el paradigma de pré-capacitació i perfeccionament, han aconseguit un gran èxit en la precisió preditiva. Però normalment aquests sistemes no són ben calibrats per incertituds fora de la caixa. A la literatura s'han proposat molts mètodes de recalibració per quantificar la incertitud preditiva i calibrar els resultats del model, amb diferents graus de complexitat. En aquest treball, presentem un estudi sistemàtic d'alguns d'aquests mètodes. En centrar-nos en la tasca de classificació de textos i els grans models de llenguatge predefinit, primer demostram que molts dels models finats no són ben calibrats fora de la caixa, especialment quan les dades provenen de configuracions fora de domini. Després, comparem l'eficacia d'alguns mètodes de recalibració generalitzats (com els conjunts, l'escala de temperatura). Llavors, empiricament il·lustrem una connexió entre distillació i calibració. Veiem la distillació com un terme de regularització que alenta al model d'estudiants a produir incertituds que s'ajusten a aquelles d'un model professor. Amb aquesta visió, desenvolupem mètodes simples de recalibració basats en la distillació sense cap cost adicional de inferència-temps. En el criteri de referència GLUE demostram que els nostres mètodes senzills poden aconseguir un rendiment competitiu de calibració fora de domini (OOD) amb enfocaments més cars. Finalment, inclouem ablacions per entendre l'utilitat dels components del nostre mètode proposat i examinar la transferibilitat de la calibració a través de la distillació.", 'cs': 'Nedávné pokroky v oblasti NLP systémů, zejména paradigma předškolení a jemného ladění, dosáhly velkého úspěchu v oblasti prediktivní přesnosti. Nicméně tyto systémy nejsou obvykle dobře kalibrovány pro nejistotu z krabice. V literatuře bylo navrženo mnoho rekalibračních metod pro kvantifikaci prediktivní nejistoty a kalibraci modelových výstupů s různým stupněm složitosti. V této práci představujeme systematickou studii několika z těchto metod. Zaměřením se na úlohu klasifikace textu a jemně vyladěné velké předtrénované jazykové modely nejprve ukážeme, že mnoho z jemně vyladěných modelů není dobře kalibrováno out-of-the-box, zejména pokud data pocházejí z mimo doménu nastavení. Dále porovnáváme efektivitu několika široce používaných rekalibračních metod (jako jsou soubory, teplotní škálování). Následně empiricky ilustrujeme souvislost mezi destilací a kalibrací. Destilaci považujeme za regularizační termín, který povzbuzuje studentský model k výstupu nejistot, které odpovídají modelu učitele. S tímto poznatkem vyvíjíme jednoduché metody rekalibrace založené na destilaci bez dodatečných nákladů na čas inference. Na GLUE benchmarku ukazujeme, že naše jednoduché metody dokážou dosáhnout konkurenčního mimo doménu (OOD) kalibračního výkonu bez dražších přístupů. Nakonec zahrnujeme ablace, abychom porozuměli užitečnosti složek naší navrhované metody a zkoumali přenositelnost kalibrace pomocí destilace.', 'et': 'Hiljutised edusammud uue tööprogrammi süsteemides, eelkõige eelõpetamise ja täpsustamise paradigmas, on saavutanud suure edu prognoositavuse osas. Siiski ei ole need süsteemid tavaliselt hästi kalibreeritud mõõtemääramatuse suhtes kasutuselt. Kirjanduses on välja pakutud mitmeid rekalibreerimismeetodeid prognoosiva määramatuse kvantifitseerimiseks ja mudeli väljundite kalibreerimiseks erineva keerukusastmega. Käesolevas töös esitame süstemaatilise uuringu mõned neist meetoditest. Keskendudes teksti klassifitseerimise ülesandele ja suurtele eeltreenitud keelemudelitele, näitame esmalt, et paljud peenestatud mudelid ei ole hästi kalibreeritud koheselt, eriti kui andmed tulevad domeenivälistest seadetest. Seejärel võrdleme mõne laialdaselt kasutatud rekalibreerimismeetodi (nt komplektid, temperatuuri skaleerimine) efektiivsust. Seejärel illustreerime empiiriliselt seost destilleerimise ja kalibreerimise vahel. Me näeme destilleerimist kui regulatsiooni terminit, mis julgustab õpilase mudelit väljastama ebakindlust, mis sobib õpetaja mudeli omadega. Selle ülevaate abil töötame välja lihtsad rekalibreerimismeetodid, mis põhinevad destilleerimisel ilma täiendava järeldusaja kuludeta. Näitame GLUE võrdlusalusel, et meie lihtsad meetodid suudavad saavutada konkurentsivõimelise domeenivälise kalibreerimise (OOD) tulemuslikkuse kallimate lähenemisviisidega. Lõpuks lisame ablatsioone, et mõista meie kavandatud meetodi komponentide kasulikkust ja uurida kalibreerimise ülekantavust destilleerimise kaudu.', 'fi': 'NLP-jﾃ､rjestelmien viimeaikainen kehitys, erityisesti esikoulutus- ja hienosﾃ､ﾃ､tﾃｶparadigma, on saavuttanut suurta menestystﾃ､ ennakoivan tarkkuuden suhteen. Nﾃ､mﾃ､ jﾃ､rjestelmﾃ､t eivﾃ､t kuitenkaan yleensﾃ､ ole hyvin kalibroituja epﾃ､varmuutta varten. Kirjallisuudessa on ehdotettu useita uudelleenkalibrointimenetelmiﾃ､ ennakoivan epﾃ､varmuuden kvantifioimiseksi ja mallituotosten kalibroimiseksi vaihtelevilla monimutkaisuusasteilla. Tﾃ､ssﾃ､ tyﾃｶssﾃ､ esitellﾃ､ﾃ､n systemaattinen tutkimus muutamista nﾃ､istﾃ､ menetelmistﾃ､. Keskittymﾃ､llﾃ､ tekstiluokittelutehtﾃ､vﾃ､ﾃ､n ja hienoviritettyihin suuriin esikoulutettuihin kielimalleihin nﾃ､ytﾃ､mme ensin, ettﾃ､ monet hienoviritetyistﾃ､ malleista eivﾃ､t ole hyvin kalibroituja heti valmiina, varsinkin kun tiedot tulevat toimialueen ulkopuolisista asetuksista. Seuraavaksi vertaillaan muutamien laajalti kﾃ､ytettyjen uudelleenkalibrointimenetelmien (kuten kokoonpanojen, lﾃ､mpﾃｶtilan skaalauksen) tehokkuutta. Tﾃ､mﾃ､n jﾃ､lkeen havainnollistamme empiirisesti tislauksen ja kalibroinnin vﾃ､listﾃ､ yhteyttﾃ､. Nﾃ､emme tislauksen sﾃ､ﾃ､nnﾃｶnmukaisuuden terminﾃ､, joka kannustaa opiskelijamallia tuottamaan epﾃ､varmuuksia, jotka vastaavat opettajamallin epﾃ､varmuutta. Tﾃ､mﾃ､n nﾃ､kemyksen avulla kehitﾃ､mme yksinkertaisia tislaukseen perustuvia uudelleenkalibrointimenetelmiﾃ､ ilman ylimﾃ､ﾃ､rﾃ､isiﾃ､ pﾃ､ﾃ､ttelyaikakustannuksia. Osoitamme GLUE-vertailussa, ettﾃ､ yksinkertaisilla menetelmillﾃ､ voidaan saavuttaa kilpailukykyinen out-of-domain (OOD) -kalibrointisuorituskyky kalliimmilla menetelmillﾃ､. Lopuksi sisﾃ､llytﾃ､mme ablaatioita ymmﾃ､rtﾃ､mﾃ､ﾃ､n ehdotetun menetelmﾃ､n komponenttien hyﾃｶdyllisyyttﾃ､ ja tutkimaan kalibroinnin siirrettﾃ､vyyttﾃ､ tislauksen avulla.', 'jv': 'text-tool-presets Nanging, sistem iki dadi bisa akeh ora cukalir kanggo nganggo layang. Awak dhéwé Nang-uwong iki, awake dhewe nyimpen sistematik kanggo sabanjuré karo nganggo sistem sing iki. politenessoffpolite"), and when there is a change ("assertivepolite"), and when there is a change ("assertivepoliteness politenessoffpolite"), and when there is a change ("assertive Njuk,awak dhéwé ngerasakno ngono kalibrasan itéparan Awakdhéwé ngerti persilangan nganggo kuwi ngucap persilangan ingkang kuwi model kuwi mau. Ngomongke iki luwih-luwih apik, kita nguasai layanan sampeyan sampeyan nguasai nêmên basa supoyo ajeng-ajeng. We show on the GLUE Lha wih-wih, kita tambah kapan-kapan kanggo ngerasai kapan pawaran gambar nggawe barang nggawe barang nggawe barang kelibrasan kanggo ngilangno wektu nggawe', 'sk': 'Nedavni napredek na področju sistemov NLP, zlasti paradigme predusposabljanja in natančnega uravnavanja, je dosegel velik uspeh pri napovedovalni natančnosti. Vendar pa ti sistemi običajno niso dobro umerjeni za negotovost, ki je pripravljena. V literaturi so predlagane številne metode rekalibracije za kvantifikacijo napovedne negotovosti in kalibriranje rezultatov modela z različnimi stopnjami kompleksnosti. V tem delu predstavljamo sistematično študijo nekaterih teh metod. Z osredotočanjem na nalogo klasifikacije besedila in natančno nastavljene velike predtrenirane jezikovne modele najprej pokažemo, da mnogi izbrani modeli niso dobro umerjeni, še posebej, če podatki prihajajo iz nastavitev zunaj domene. Nato primerjamo učinkovitost nekaj pogosto uporabljenih metod rekalibracije (npr. kompleti, temperaturno skaliranje). Nato empirično ponazarjamo povezavo med destilacijo in kalibracijo. Destilacijo vidimo kot regulacijski izraz, ki spodbuja študentski model k izdelavi negotovosti, ki se ujemajo z negotovostmi učiteljskega modela. S tem vpogledom razvijamo preproste metode rekalibracije, ki temeljijo na destilaciji brez dodatnih stroškov sklepanja časa. Na referenčni vrednosti GLUE pokažemo, da lahko naše preproste metode dosežejo konkurenčno zunajdomensko (OOD) kalibracijsko zmogljivost z dražjimi pristopi. Na koncu vključujemo ablacije za razumevanje uporabnosti komponent naše predlagane metode in preučitev prenosljivosti kalibracije z destilacijo.', 'ha': "Tsarin da ake ƙara a cikin tsarin NLP, haske da paradigm mai daɗewa da finsuwa, sun sami babban rabo a cikin tabbacin da ya yi ƙayyade. A lokacin da haka, ba za a iya ƙayyade wannan na'ura ba don a iya daidaita wa masu buƙata-bayani. An buƙata wasu shiryoyin rehabilita cikin littafa dõmin a ƙaddara masu bashiri da tabbatarwa da kuma masu kalibori motel, da darajõji daban-dabam. Daga wannan aikin, Munã halatar da wani littãfi na'ura da akan waɗannan hanyõyi. Fokus a kan aikin mai fasa matsayin kuma masu buƙata masu motsi na harshen da aka ƙayyade, muna nuna farkon da ke nuna cewa, wasu masu shiryoyin ayuka da aka kifi, ba su ƙayyade mai kyau ba daga-box, haske idan data za'a fito daga tsarin-don-Domin. Na ƙara, muna samfanar da aikin da aka yi amfani da akan mai amfani da shi (kamar fanel, mai kifi). Then, we empirically illustrate a connection between distillation and calibration.  Tuna ganin ka rarrabi kamar wata kalma na taƙaita, tanã yardar da misalin ardani su fitar da masu da hakki da suka sami da misalin mafarin. Ga wannan na gane, Munã buɗa hanyoyin mai saukarwa a kan rarraba ko kuma bã da wani ƙari wa'urar-lokaci. Tuna nuna kan bangon GLUU da hanyoyinmu masu sauƙi za'a sami tsarin mai ƙidãya daga-Domen (OOD), misali, masu ƙari masu kyauta. Gani, za mu sami tamkar mutane dõmin mu fahimta amfani da amfani na ƙananan hanyonmu da aka buƙata, kuma mu yi bincike a canza musanya na kalibarin da za'a yi rarraba.", 'he': 'ההתקדמות האחרונות במערכות NLP, במיוחד הפרדיגמה של התאמנות הקדמות והדקות, השיגו הצלחה גדולה בדיוק צפוי. However, these systems are usually not well calibrated for uncertainty out-of-the-box.  השיטות רבות של שיקול מחדש הציעו בספרות כדי לקוונטיזם אי-בטוחות צפויה ולקבל תוצאות מודל, עם מעלות מורכבות שונות. בעבודה הזו, אנחנו מציגים מחקר שיטתי של כמה משיטות אלה. מתמקדים במשימת ההקלטה הטקסטית ומדגמנים גדולים של שפת מתאמנים מראשונה, אנו מראים שרבים מהדוגמנים מתאימים אינם מתאימים היטב מחוץ לקופסא, במיוחד כשהנתונים מגיעים ממוסדות מחוץ לתחום. לאחר מכן, אנחנו משווה את היעילות של כמה שיטות שיקול מחדש משתמשים רחב (כמו אסמבלים, מידה טמפרטורה). Then, we empirically illustrate a connection between distillation and calibration.  אנו רואים את הדיסטילה כמונח רגיל שמעודד את מודל הסטודנטים להוציא אי-בטוחות שמתאימות לאלה של מודל מורה. עם ההבנה הזו, אנחנו מפתחים שיטות שיקול חדש פשוטות מבוססות על משקה בלי עלות תוספת זמן. אנו מראים על רמז GLUE ששיטות פשוטות שלנו יכולות להשיג ביצועי קליבריה מחוץ לתחום תחרותי (OOD) w.r.t. גישות יקרות יותר. סוף סוף, אנחנו כוללים היכולות להבין את השימוש של המרכיבים של השיטה המוצעת שלנו ולבדוק את היכולת להעביר את הקליברציה באמצעות הדיסטל.', 'bo': 'རྒྱལ་ཁབ་ཀྱི་NLP་མ་ལག་གི་ཡར་རྒྱས་འགྲོ་བཞིན་པའི་སྔོན་ཤུགས་དང་མཐའ་མེ་ཤེས་ཀྱི་paradigm་ལ་ཁྱད་པར་རྐྱེན་སྐྱོང་བ་རེད། ཡིན་ནའང་། མ་ལག་འདི་ཚོ་རྒྱུན་ལྡན་གྱིས་གླེང་སྒྲོམ་གྱི་ཕྱི་ཁར་ནས་ངེས་འཛིན་བྱས་མེད་པར། ཞིབ་བཟོ་བྱེད་ཀྱི་ཐབས་ལམ་མང་པོ་ཞིག་དག་གི་འཕྲིན་ཡིག འོན་ཀྱང་། ང་ཚོས་ཐབས་ལམ་འདི་དག་ལས་རིམ་སྒྲིག་གི་ཕྱོགས་ལམ་ཞིག་འཆར་བྱེད་ཀྱི་ཡོད། Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. འོན་ཀྱང་། ང་ཚོས་ལག་ལེན་འཐབ་པའི་བསྐྱར་བཟོ་བ་ཀྱི་ཐབས་ལམ་ལ་ཉུང་བའི་ལྟ་བུའི་བཟོ་བཅོས་བྱེད་ཀྱི་ཡོད། འོན་ཀྱང་། ང་ཚོས་གསལ་བཤད་དང་ཞེང་ཚད་དབར་གྱི་སྦྲེལ་མཐུད་གཅིག་སྟོན་བྱེད་ཀྱི་ཡོད། ང་ཚོས་རྒྱུན་ལྡན་གྱི་མ་དབྱིབས་འགྱུར་བའི་གྲངས་སུ་མཐོང་བ་དང་མཐུན་རྐྱེན་ཁག་ཅིག་བྱེད་ཀྱི་ཡོད། མཐོང་ཚུལ་འདི་ལྟར་ན། ང་ཚོས་རང་ཉིད་ཀྱི་རྒྱ་ལས་དུས་ཚོད་སྦྱར་བའི་ཐབས་ལམ་ལུགས་སྔར་བས། We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. མཐའ་མར་དུ། འུ་ཚོས་དུས་རྐྱེན་གྱིས་ང་ཚོའི་གྲོས་འཆར་བྱས་པའི་ཆ་ཤས་ཀྱི་སྤྱོད་སྟངས་ཡོད་པ་དང་གསལ་བཤད་ཀྱི་གནས'}
{'en': 'Direction is what you need : Improving Word Embedding Compression in Large Language Models', 'ar': 'الاتجاه هو ما تحتاجه: تحسين ضغط تضمين الكلمات في نماذج اللغات الكبيرة', 'pt': 'A direção é o que você precisa: Melhorar a compactação de incorporação de palavras em modelos de linguagem grandes', 'fr': "La direction est ce dont vous avez besoin\xa0: améliorer la compression de l'incorporation de mots dans les grands modèles linguistiques", 'es': 'La dirección es lo que necesita: mejorar la compresión de incrustación de palabras en modelos de lenguaje grandes', 'zh': '君所须,改入大语形中词嵌压缩', 'ja': '必要なのは方向性です：大規模言語モデルでのワード埋め込み圧縮の改善', 'ru': 'Направление - это то, что вам нужно: Улучшение сжатия при встраивании слов в больших языковых моделях', 'hi': 'दिशा वह है जिसकी आपको आवश्यकता है: बड़ी भाषा मॉडल में वर्ड एम्बेडिंग संपीड़न में सुधार', 'ga': 'Is é an treo atá uait: Comhbhrú Leabú Focal a Fheabhsú i Múnlaí Móra Teanga', 'hu': 'Az ir찼ny az, amire sz체ks챕ge van: A sz처 t철m철r챠t챕s챕nek jav챠t찼sa nagy nyelvi modellekben', 'el': 'Κατεύθυνση είναι αυτό που χρειάζεστε: Βελτίωση της ενσωμάτωσης λέξεων σε μοντέλα μεγάλης γλώσσας', 'it': 'La direzione è ciò di cui hai bisogno: migliorare la compressione di incorporazione delle parole nei modelli di lingue di grandi dimensioni', 'ka': 'დირექცია იქნება, რომელიც თქვენ გვჭირდება: სიტყვების შებეჭირება კომპრესია დიდი ენის მოდელში', 'lt': 'Jums reikia krypties: gerinti žodžių įterpimo spaudimą dideliuose kalbos modeliuose', 'ml': 'നിങ്ങള്\u200dക്ക് തിരിച്ചറിയേണ്ടതാണ്: വലിയ ഭാഷയുടെ മോഡലുകളില്\u200d വാക്കിന്റെ എംബെഡിങ്ങ് പ്രോഷന്\u200d മുന', 'kk': 'Бағыттауы - үлкен тіл үлгілерінде сөздерді ендіру', 'mt': 'Id-direzzjoni hija dak li għandek bżonn: It-titjib tal-kompressjoni tal-inkorporazzjoni tal-kliem f’Mudelli ta’ Lingwa Kbar', 'mk': 'Направа е она што ви треба: подобрување на компресијата за внесување на зборови во големите јазички модели', 'ms': 'Arah adalah apa yang anda perlukan: Menembangkan Pemampatan Perkataan dalam Model Bahasa Besar', 'ro': 'Direcția este ceea ce aveți nevoie: Îmbunătățirea comprimării Word în modelele de limbi mari', 'no': 'Retning er det du treng: forbetra kompresjon med ord innebæring i store språk- modeller', 'sr': 'Uprava je ono što vam treba: poboljšavanje kompresije u velikim jezičkim modelima', 'si': 'දිශාව තමයි ඔයාට ඕනේ: ලොකු භාෂාව මොඩල් වලට වචනය සම්පූර්ණය වැඩ කරන්න', 'sv': 'Riktning är vad du behöver: Förbättra Word Embedding Compression i stora språkmodeller', 'pl': 'Kierunek jest tym, czego potrzebujesz: ulepszenie kompresji osadzania słów w dużych modelach językowych', 'mn': 'Танд хэрэгтэй зүйл бол: Томоохон хэл загварын үеийн дамжуулалтыг сайжруулах', 'ta': 'திசை', 'ur': 'آپ کی ضرورت یہ ہے: بڑی زبان موڈل میں کلام جمع کرنے والی جمع کرنے والی جمع کرنا', 'so': 'Hagaagta waa waxa aad u baahan tahay: Improvementing Word Deganaanshaha Midabka Luqada wayn', 'vi': 'Hướng dẫn là thứ bạn cần: Tăng cường khả năng bao dung từ trong mô- đun ngôn ngữ lớn', 'uz': '@ info: whatsthis', 'bg': 'Посоката е това, което ви трябва: Подобряване на компресията за вграждане на думи в големи езикови модели', 'nl': 'Richting is wat je nodig hebt: Verbeteren van Word Embedding Compressie in Large Language Modellen', 'da': 'Retning er, hvad du har brug for: Forbedring af Word Embedding Compression i store sprogmodeller', 'de': 'Richtung ist, was Sie brauchen: Verbesserung der Word Embedding Compression in großsprachigen Modellen', 'id': 'Direction is what you need: Improving Word Embedding Compression in Large Language Models', 'hr': 'Uprava je ono što trebate: poboljšavanje kompresije u velikim jezičkim modelima', 'sw': 'Kwa muelekeo ni kile unachokihitaji: Kuboresha Kishinikizo cha Neno kinachoanza katika Modeli kubwa ya Lugha', 'ko': '방향은 당신이 필요로 하는 것이다. 대형 언어 모델의 단어 삽입 압축을 개선하는 것이다', 'fa': 'مسیر چیزی است که نیاز دارید: بهتر کردن فشار جمع کردن کلمه در مدل زبان بزرگ', 'tr': 'Saýlanan hatlary nusgala', 'af': 'Regting is wat jy nodig het: verbetering Woord Inbeter Komprimering in Groot Taal Modelle', 'sq': 'Direction is what you need: Improving Word Embedding Compression in Large Language Models', 'am': 'አቀማመጥ', 'hy': 'Direction is what you need: Improving Word Embedding Compression in Large Language Models', 'bs': 'Uprava je ono što trebate: poboljšavanje kompresije u velikim jezičkim modelima', 'ca': "La direcció és el que necessites: millorar la compressió d'incorporació de paraules en models de llenguatge grans", 'az': 'İhtiyacınızın doğruluğu: Büyük Dil Modellərində Sözü İçində Sözü İşləməsi', 'bn': 'ডিসেক্টরেশন আপনার প্রয়োজন: বড় ভাষার মোডেলে শব্দের এমবেডিং আপ্রেশন উন্নতি করা হচ্ছে', 'et': 'Suund on see, mida vajate: sõna manustamise tihendamise parandamine suurtes keelemudelites', 'fi': 'Suunta on se, mitä tarvitset: Parannetaan Word Embedding Compression -pakkausta suurissa kielimalleissa', 'cs': 'Směr je to, co potřebujete: Zlepšení komprese vložení slov ve velkých jazykových modelech', 'jv': 'structural navigation', 'sk': 'Potrebujete smer: Izboljšanje stiskanja besedila v velikih jezikovnih modelih', 'ha': 'Farawa yana da amfani da: Kuƙara Kwanan Ƙara da Ƙaramifi cikin Modellun Large', 'bo': 'ཁྱོད་ཀྱིས་དགོས་པའི་འཇུག་སྣེ་འདྲ་ཡིན། སྐད་རིགས་ཀྱི་དཔེ་དབྱིབས་ཆེ་ཆུང་བའི་ནང', 'he': 'הכיוון הוא מה שאתה צריך: שיפור לחיצת קידום מילים במודלים לשפה גדולים'}
{'en': 'The adoption of Transformer-based models in  natural language processing (NLP)  has led to great success using a massive number of parameters. However, due to deployment constraints in  edge devices , there has been a rising interest in the  compression  of these models to improve their inference time and memory footprint. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. The proposed method is task-agnostic and does not require further language modeling pre-training. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios. Our code is public.', 'ar': 'أدى اعتماد النماذج القائمة على المحولات في معالجة اللغة الطبيعية (NLP) إلى نجاح كبير باستخدام عدد هائل من المعلمات. ومع ذلك ، نظرًا لقيود النشر في الأجهزة المتطورة ، كان هناك اهتمام متزايد بضغط هذه النماذج لتحسين وقت الاستدلال وبصمة الذاكرة. تقدم هذه الورقة هدفًا جديدًا للخسارة لضغط عمليات دمج الرمز المميز في النماذج المستندة إلى المحولات من خلال الاستفادة من بنية التشفير التلقائي. وبشكل أكثر تحديدًا ، نؤكد على أهمية اتجاه حفلات الزفاف المضغوطة فيما يتعلق بحفلات الزفاف الأصلية غير المضغوطة. الطريقة المقترحة غير محددة المهمة ولا تتطلب مزيدًا من النمذجة اللغوية للتدريب المسبق. تتفوق طريقتنا بشكل كبير على نهج عامل المصفوفة القائم على SVD المستخدم بشكل شائع من حيث نموذج اللغة الأولية "الحيرة". علاوة على ذلك ، نقوم بتقييم نهجنا المقترح على مجموعة بيانات SQuAD v1.1 والعديد من المهام النهائية من معيار GLUE ، حيث نتفوق أيضًا على خط الأساس في معظم السيناريوهات. كودنا عام.', 'fr': "L'adoption de modèles basés sur Transformer dans le traitement du langage naturel (NLP) a donné lieu à un grand succès en utilisant un grand nombre de paramètres. Cependant, en raison des contraintes de déploiement dans les périphériques périphériques, la compression de ces modèles suscite un intérêt croissant afin d'améliorer leur temps d'inférence et leur empreinte mémoire. Cet article présente un nouvel objectif de perte pour compresser les intégrations de jetons dans les modèles basés sur Transformer en exploitant une architecture AutoEncoder. Plus précisément, nous soulignons l'importance de la direction des intégrations compressées par rapport aux intégrations non compressées d'origine. La méthode proposée est indépendante des tâches et ne nécessite pas de formation préalable à la modélisation linguistique supplémentaire. Notre méthode surpasse largement l'approche de factorisation matricielle basée sur SVD couramment utilisée en termes de perplexité du modèle de langage initial. De plus, nous évaluons notre approche proposée sur le jeu de données SQuad v1.1 et plusieurs tâches en aval du benchmark GLUE, où nous surpassons également la base de référence dans la plupart des scénarios. Notre code est public.", 'es': 'La adopción de modelos basados en Transformer en el procesamiento del lenguaje natural (NLP) ha llevado a un gran éxito al utilizar una enorme cantidad de parámetros. Sin embargo, debido a las restricciones de implementación en los dispositivos perimetrales, ha habido un interés creciente en la compresión de estos modelos para mejorar el tiempo de inferencia y la huella de memoria. Este artículo presenta un nuevo objetivo de pérdida para comprimir las incrustaciones de tokens en los modelos basados en Transformer mediante el aprovechamiento de una arquitectura de AutoEncoder. Más específicamente, enfatizamos la importancia de la dirección de las incrustaciones comprimidas con respecto a las incrustaciones originales sin comprimir. El método propuesto es independiente de las tareas y no requiere más capacitación previa al modelado del lenguaje. Nuestro método supera significativamente el enfoque de factorización de matrices basado en SVD comúnmente utilizado en términos de modelo de lenguaje inicial Perplexity. Además, evaluamos nuestro enfoque propuesto sobre el conjunto de datos sQuad v1.1 y varias tareas posteriores del punto de referencia GLUE, donde también superamos la línea de base en la mayoría de los escenarios. Nuestro código es público.', 'pt': 'A adoção de modelos baseados em transformadores no processamento de linguagem natural (NLP) levou a um grande sucesso usando um grande número de parâmetros. No entanto, devido a restrições de implantação em dispositivos de borda, tem havido um interesse crescente na compactação desses modelos para melhorar seu tempo de inferência e espaço de memória. Este artigo apresenta um novo objetivo de perda para compactar incorporações de token nos modelos baseados em Transformer, aproveitando uma arquitetura AutoEncoder. Mais especificamente, enfatizamos a importância da direção dos embeddings compactados em relação aos embeddings originais não compactados. O método proposto é independente de tarefas e não requer pré-treinamento de modelagem de linguagem adicional. Nosso método supera significativamente a abordagem de fatoração de matriz baseada em SVD comumente usada em termos de Perplexidade do modelo de linguagem inicial. Além disso, avaliamos nossa abordagem proposta sobre o conjunto de dados SQuAD v1.1 e várias tarefas downstream do benchmark GLUE, onde também superamos a linha de base na maioria dos cenários. Nosso código é público.', 'ja': '自然言語処理（ NLP ）におけるTransformerベースのモデルの採用は、膨大な数のパラメータを使用して大成功を収めました。 しかしながら、エッジデバイスにおける展開の制約により、推論時間及びメモリフットプリントを改善するために、これらのモデルの圧縮に対する関心が高まっている。 オートエンコーダーアーキテクチャを活用してトークンの埋め込みをトランスフォーマーベースモデルに圧縮する新規損失目標を提示した。 より具体的には、元の非圧縮埋め込みに関して圧縮埋め込みの方向性の重要性を強調します。 提案された方法はタスク非依存であり、さらなる言語モデリングの事前トレーニングを必要としない。 私たちの方法は、初期言語モデルPerplexityの点で、一般的に使用されているSVDベースの行列因子化アプローチを大幅に上回っています。 さらに、SQuAD v1.1データセットといくつかのダウンストリームタスクを介して提案されたアプローチをグルーベンチマークから評価し、ほとんどのシナリオでベースラインを上回るパフォーマンスを発揮します。コードは公開されています。', 'zh': '自然语言处(NLP)中,Transformer形已致大参数之巨大成功。 然缘边设备之限,人压缩其兴日厚,以善其时内存占。 本文立新损,因自编码器架构于Transformer中压缩令牌嵌。 更具体地说,压缩嵌向相对未压缩嵌之要。 所陈无关务,不须更建模预训练。 吾法于初始语言模形困惑显优于常用之本SVD矩阵分解之法。 此外SQuAD v1.1数集GLUE准试诸下流,估其法,众多者,亦优于基线。 吾代码明矣。', 'hi': 'प्राकृतिक भाषा प्रसंस्करण (एनएलपी) में ट्रांसफॉर्मर-आधारित मॉडल को अपनाने से बड़ी संख्या में मापदंडों का उपयोग करके बड़ी सफलता मिली है। हालांकि, किनारे के उपकरणों में तैनाती बाधाओं के कारण, उनके अनुमान समय और स्मृति पदचिह्न में सुधार करने के लिए इन मॉडलों के संपीड़न में बढ़ती रुचि रही है। यह पेपर एक ऑटोएनकोडर आर्किटेक्चर का लाभ उठाकर ट्रांसफॉर्मर-आधारित मॉडल में टोकन एम्बेडिंग को संपीड़ित करने के लिए एक उपन्यास हानि उद्देश्य प्रस्तुत करता है। अधिक विशेष रूप से, हम मूल असम्पीडित एम्बेडिंग के संबंध में संपीड़ित एम्बेडिंग की दिशा के महत्व पर जोर देते हैं। प्रस्तावित विधि कार्य-अज्ञेयवादी है और इसके लिए आगे की भाषा मॉडलिंग पूर्व-प्रशिक्षण की आवश्यकता नहीं है। हमारी विधि काफी प्रारंभिक भाषा मॉडल उलझन के मामले में आमतौर पर इस्तेमाल किया SVD-आधारित मैट्रिक्स-factorization दृष्टिकोण outperforms. इसके अलावा, हम SQuAD v1.1 डेटासेट पर हमारे प्रस्तावित दृष्टिकोण और GLUE बेंचमार्क से कई डाउनस्ट्रीम कार्यों का मूल्यांकन करते हैं, जहां हम अधिकांश परिदृश्यों में बेसलाइन को भी बेहतर बनाते हैं। हमारा कोड सार्वजनिक है।', 'ru': 'Принятие моделей на основе трансформаторов в обработке естественного языка (NLP) привело к большому успеху с использованием огромного количества параметров. Тем не менее, из-за ограничений развертывания в периферийных устройствах, растет интерес к сжатию этих моделей, чтобы улучшить их время вывода и отпечаток памяти. Эта статья представляет новую цель потерь для сжатия вложений токенов в моделях на основе трансформатора, используя архитектуру AutoEncoder. Более конкретно, мы подчеркиваем важность направления сжатых закладных деталей относительно оригинальных несжатых закладных деталей. Предложенный способ является задача-диагностическим и не требует дальнейшего предварительного обучения языковому моделированию. Наш метод значительно превосходит широко используемый подход матричной факторизации на основе SVD с точки зрения исходной языковой модели Perplexity. Кроме того, мы оцениваем предлагаемый нами подход в отношении набора данных SQuAD v1.1 и нескольких задач ниже по потоку от эталона КЛЕЯ, где мы также превосходим базовый уровень в большинстве сценариев. Наш код является общедоступным.', 'ga': "D'éirigh thar barr le samhlacha Trasfhoirmeoir-bhunaithe a ghlacadh i bpróiseáil teanga nádúrtha (NLP) ag baint úsáide as líon ollmhór paraiméadair. Mar gheall ar shrianta imlonnaithe i bhfeistí imeallacha, áfach, tá suim ag méadú i gcomhbhrú na múnlaí sin chun a n-am tátal agus a lorg cuimhne a fheabhsú. Cuireann an páipéar seo i láthair cuspóir caillteanais nua chun leabaithe comharthaí a chomhbhrú sna samhlacha atá bunaithe ar an gClaochladán trí ailtireacht AutoEncoder a ghiaráil. Go sonrach, leagaimid béim ar a thábhachtaí atá treo na leabaithe comhbhrúite maidir le bunleabaithe neamh-chomhbhrúite. Is modh tasc-agnostic an modh atá beartaithe agus ní theastaíonn réamhoiliúint samhaltaithe teanga breise uaidh. Sáraíonn ár modh an cur chuige maitrís-fachtóirithe atá bunaithe ar SVD a úsáidtear go coitianta i dtéarmaí Oirphléacstacht na samhla teanga tosaigh. Ina theannta sin, déanaimid meastóireacht ar an gcur chuige atá beartaithe againn maidir le tacar sonraí SQuAD v1.1 agus roinnt tascanna iartheachtacha ón tagarmharc GLUE, áit a ndéanaimid sárobair ar an mbonnlíne i bhformhór na gcásanna freisin. Tá ár gcód poiblí.", 'hu': 'A transzformátor alapú modellek alkalmazása a természetes nyelv feldolgozásában (NLP) nagy sikerhez vezetett nagyszámú paraméter felhasználásával. Azonban az éleszközök telepítési korlátai miatt egyre nagyobb az érdeklődés a modellek tömörítése iránt, hogy javítsák a következtetési időt és a memória lábnyomát. Ez a tanulmány bemutatja egy új veszteségi célkitűzést, amely az AutoEncoder architektúra használatával tömöríti a tokenbeágyazásokat a Transformer-alapú modellekbe. Pontosabban hangsúlyozzuk a tömörített beágyazások irányának fontosságát az eredeti tömörítetlen beágyazásokhoz képest. A javasolt módszer feladat-agnosztikus, és nem igényel további nyelvmodellezést előkészítő képzést. Módszerünk jelentősen felülmúlja a gyakran használt SVD-alapú mátrixfaktorizációs megközelítést a Perplexity kezdeti nyelvi modell szempontjából. Ezenkívül értékeljük javasolt megközelítésünket az SQUAD v1 felett. 1 adatkészlet és több downstream feladat a GLUE referenciaértékből, ahol a legtöbb forgatókönyvben is túlteljesítjük az alapvető teljesítményt. A kódunk nyilvános.', 'el': 'Η υιοθέτηση μοντέλων βασισμένων στον μετασχηματιστή στην επεξεργασία φυσικής γλώσσας (έχει οδηγήσει σε μεγάλη επιτυχία χρησιμοποιώντας έναν τεράστιο αριθμό παραμέτρων. Ωστόσο, λόγω των περιορισμών ανάπτυξης σε συσκευές ακρών, υπάρχει αυξανόμενο ενδιαφέρον για τη συμπίεση αυτών των μοντέλων για τη βελτίωση του χρόνου συναγωγής τους και του αποτυπώματος μνήμης. Αυτή η εργασία παρουσιάζει έναν νέο στόχο απώλειας για τη συμπίεση των ενσωμάτωσης συμβολαίων στα μοντέλα που βασίζονται στον μετασχηματιστή χρησιμοποιώντας μια αρχιτεκτονική αυτόματου κωδικοποιητή. Πιο συγκεκριμένα, τονίζουμε τη σημασία της κατεύθυνσης των συμπιεσμένων ενσωματώσεων σε σχέση με τις αρχικές μη συμπιεσμένες ενσωματώσεις. Η προτεινόμενη μέθοδος είναι αναλόγως των εργασιών και δεν απαιτεί περαιτέρω εκπαίδευση γλωσσικής μοντελοποίησης. Η μέθοδος μας ξεπερνά σημαντικά την ευρέως χρησιμοποιούμενη προσέγγιση της μήτρας-παραγοντισμού ως προς το αρχικό γλωσσικό μοντέλο Perplexity. Επιπλέον, αξιολογούμε την προτεινόμενη προσέγγισή μας σε σχέση με το SQuAD v1. 1 σύνολο δεδομένων και αρκετές μεταγενέστερες εργασίες από το σημείο αναφοράς όπου επίσης ξεπερνάμε τη γραμμή βάσης στα περισσότερα σενάρια. Ο κώδικας μας είναι δημόσιος.', 'ka': 'ტრანფორმენტერის ბაზაციული მოდელების გავაკეთება (NLP) ნახვა ენის პროცესიაში დიდი წარმატების გამოყენება. მაგრამ, განვითარების შედგენების გამოსახულებლად, ამ მოდელების კომპრექციაში უფრო დიდი ინტერესტი იყო, რომლებიც მათი ინფრექციის დროს და მეხსიერების ფესტის დამატებლად ეს დოკუმენტი ახალგაზრდება პრომენტის დაკავშირების მისამართება, რომელიც ტრანფორმეტრის ბაზეული მოდელში ტექსპენტირების კომპრექტირებისთვის ტექსპენტირების შე უფრო განსაკუთრებულია, ჩვენ აღმოჩნეთ კომპრექტირებული ინბიდნეციების მიერ მნიშვნელობა, რომელიც უფრო კომპრექტირებული ინბიდნეციების მიერ პროგრამის შესაძლებელი მეტი არის task-agnostic და არ მოჭირდება მეტად ენის მოდელირება პროგრამის შესაძლებლობა. ჩვენი მეტი ძალიან უფრო მეტად გამოყენებულია SVD-ზე გამოყენებული მარტიკის ფექტურაციის მიზეზი პერპლექტიის მიზეზით. დამატებით, ჩვენ გავამუშავებთ ჩვენი წინასწარმოდგენებული პროგრამა SQuAD v1-ზე. 1 მონაცემები და რამდენიმე მონაცემები GLUE ბანქმერიდან, სადაც ჩვენ უფრო მეტი სინარიოში დავაკეთებთ ბანქმერის ხაზი. ნაქთჲრ კჲე ვ ოსბლთკა.', 'it': "L'adozione di modelli basati su Transformer nell'elaborazione del linguaggio naturale (NLP) ha portato a un grande successo utilizzando un numero enorme di parametri. Tuttavia, a causa dei vincoli di distribuzione nei dispositivi edge, c'è stato un crescente interesse per la compressione di questi modelli per migliorare il loro tempo di inferenza e l'impronta di memoria. Questo articolo presenta un nuovo obiettivo di perdita per comprimere le incorporazioni di token nei modelli basati su Transformer sfruttando un'architettura AutoEncoder. In particolare, sottolineiamo l'importanza della direzione degli incorporamenti compressi rispetto agli incorporamenti non compressi originali. Il metodo proposto è task-agnostic e non richiede ulteriore formazione di modellazione linguistica. Il nostro metodo supera significativamente l'approccio di fattorizzazione matrice basato su SVD comunemente utilizzato in termini di Perplexity modello di linguaggio iniziale. Inoltre, valutiamo il nostro approccio proposto su SQUAD v1. 1 set di dati e diverse attività a valle dal benchmark GLUE, in cui superiamo anche la linea di base nella maggior parte degli scenari. Il nostro codice e' pubblico.", 'lt': 'The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters.  Tačiau dėl krašto įtaisų diegimo apribojimų vis labiau domisi šių modelių kompresija, siekiant pagerinti jų išvados laiką ir atminties pėdsakus. Šiame dokumente pateikiamas naujas nuostolių tikslas suspausti žymenų įdėjimą į Transformer pagrįstus modelius naudojant AutoEncoder architektūrą. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings.  Siūlomas metodas yra užduočių agnostinis ir nereikalauja tolesnio kalbų modeliavimo parengiamojo mokymo. Mūsų metodas gerokai viršija paprastai naudojamą matricos faktorizacijos metodą pagal pradinį kalbos model į Perplexity. Be to, vertiname savo siūlomą požiūrį į SQuAD v1. 1 duomenų rinkinys ir kelios tolesnės užduotys pagal GLUE lyginamąjį rodiklį, kur daugumoje scenarijų taip pat viršijame bazinį rodiklį. Mūsų kodas yra viešas.', 'kk': 'Тізбекті тілді өңдеу (NLP) үлгілерін түрлендіру үлгілерін қолдану үшін көп параметрлерді қолдану үшін үлкен сәтті болды. Бірақ бұл үлгілерді шектеу құрылғыларының шектеулерінің себебі, олардың бағыттау уақыты мен жадын басып шығару үшін осы үлгілердің сығыстырудың көпшілігі өте көп болды Бұл қағаз автокодер архитектурасын түрлендіру үлгілерінде тегіс ендіру үшін романдық жоғалу мақсатын көрсетеді. Көбірек, біз бастапқы компрессияланбаған ендірудің көбіректігін бағыттамыз. Келтірілген әдіс - тапсырма- агностикалық және алдыңғы тіл моделі үлгілеу керек емес. Біздің әдіміміз SVD- негіздеген матрица факторизациясының бастапқы тіл үлгісінің Perplexity үлгісінің қасиетінде көпшілікті жасайды. Сонымен қатар, SQuAD v1 арқылы қолданылған тәсілімізді бағалаймыз. 1 деректер жиыны және көбірек сценариялардың негізгі жолын бастадық. Біздің кодмыз көпшілік.', 'mk': 'Усвојувањето на модели базирани на Трансформи во природното обработување јазик (НЛП) доведе до голем успех користејќи масовен број параметри. Сепак, поради ограничувањата на распоредувањето на уредовите на работ, постои сé поголем интерес за компресијата на овие модели за подобрување на нивното времето на инференција и отпечатокот на меморијата. Овој документ претставува нова цел за загуба за компресирање на вградувањата на знаци во моделите базирани на Трансформер со влијание на архитектурата на Авто-кодерот. Поконкретно, ја истакнуваме важноста на насоката на компресираните внесувања во однос на оригиналните некомпресирани внесувања. Предложениот метод е задачна агностика и не бара понатамошна предобука за моделирање на јазикот. Нашиот метод значително го надминува често употребениот пристап на матрица-факторизација базиран на СВД во однос на првичниот јазички модел Перплексити. Покрај тоа, го проценуваме нашиот предложен пристап во врска со SQuAD v1. 1 податок и неколку понатамошни задачи од benchmark GLUE, каде што, исто така, ја надминуваме основата во повеќето сценарија. Нашиот код е јавен.', 'mt': 'L-adozzjoni ta’ mudelli bbażati fuq it-Transformer fl-ipproċessar tal-lingwi naturali (NLP) wasslet għal suċċess kbir bl-użu ta’ għadd kbir ta’ parametri. Madankollu, minħabba restrizzjonijiet ta’ użu fit-tagħmir tat-tarf, kien hemm interess dejjem jikber fil-kompressjoni ta’ dawn il-mudelli biex jittejbu l-ħin ta’ inferenza u l-impronta tal-memorja tagħhom. Dan id-dokument jippreżenta għan ġdid ta’ telf biex jiġu kkompressi l-inkorporazzjonijiet tat-tokens fil-mudelli bbażati fuq it-Transformer billi tintuża l-ingranaġġ ta’ arkitettura tal-AutoEncoder. B’mod aktar speċifiku, aħna enfasizzaw l-importanza tad-direzzjoni tal-inkorporazzjonijiet kompressati fir-rigward tal-inkorporazzjonijiet oriġinali mhux kompressati. Il-metodu propost huwa kompitu-agnostiku u ma jeħtieġx aktar mudellar lingwistiku ta’ taħriġ minn qabel. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity.  Barra minn hekk, jivvalutaw l-approċċ propost tagħna fuq SQuAD v1. Sett ta’ dejta wieħed u diversi kompiti downstream mill-punt ta’ referenza GLUE, fejn inqabżu wkoll il-linja bażi fil-biċċa l-kbira tax-xenarji. Il-kodiċi tagħna huwa pubbliku.', 'mn': 'Байгалийн хэл боловсруулах (NLP) дахь Трансформер суурилсан загварын хүлээн зөвшөөрөл нь маш олон параметр ашиглан гайхалтай амжилтыг хүргэсэн. Гэвч, хэмжээний төхөөрөмжийн хязгаарлалтын тулд эдгээр загваруудын даралтын цаг болон санамжийн хэвлэлийг сайжруулахын тулд их сонирхолтой болсон. Энэ цаас нь автоEncoder архитектурыг ашиглан шинэ алдагдах зорилго өгдөг. Илүү тодорхой нь, бид үндсэн хэвлэгдэх хэвлэгдэх зүйлсийн чухал зүйлийг анхны хэвлэгдэхгүй хэвлэгдэх зүйлсийг тодорхойлж байна. Зарим санал өгсөн арга бол ажлын агностик болон илүү олон хэл загварын урд сургалтын шаардлагагүй. Бидний арга нь анхны хэл загварын Перплексийн тухай ихэвчлэн хэрэглэгддэг SVD-ийн матрикс-факторчлалын арга загварыг илүү их ашигладаг. Мөн бид SQuAD v1-ийн талаар санал өгсөн арга барилгыг үнэлэх болно. 1 өгөгдлийн сангууд болон олон доорх үйл ажиллагаа GLUE багцлагаас гарч ирсэн. Бид ихэнх хувилбаруудын суурь шугам дээр ажилладаг. Бидний код бол нийтлэг.', 'ml': 'സ്വാഭാവിക ഭാഷയുടെ പ്രക്രിയഭാഷയുടെ (NLP) ഉപയോഗിച്ച് ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d അടിസ്ഥാനമായ മോഡലുകള്\u200d പ്രാവര്\u200dത്തികമാക് എന്നാലും, വിഭാഗ ഉപകരണങ്ങളിലെ നിയന്ത്രണങ്ങള്\u200d കാരണം ഈ മോഡലുകളുടെ പ്രോപ്ഷന്\u200d ചെയ്യുന്നതില്\u200d താല്\u200dപര്യമുണ്ട് അവരുടെ അസുഖ്യ സമയവും മെമ്മറിയു ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d അടിസ്ഥാനത്തുള്ള മോഡലുകളില്\u200d ഉള്\u200dപ്പെടുത്തുന്നതിനായി ഒരു നോവല്\u200d നഷ്ടപ്പെടുത്താനുള്ള ലക്ഷ്യം ഈ പേപ് കൂടുതല്\u200d പ്രത്യേകിച്ച്, നമ്മള്\u200d പ്രധാനപ്പെടുത്തിക്കൊണ്ടിരിക്കുന്നത് പ്രധാനപ്പെട്ടതാണ്. ആദ്യമായി പ്രശ്നമില് പ്രൊദേശിച്ച രീതി നമ്മുടെ രീതിയില്\u200d സാധാരണ SVD-അടിസ്ഥാനത്തിലുള്ള മാട്രിക്സ്-ഫാക്ട്രിക്സ്-ഫാക്ടോറിഷഷന്\u200d നടപടികള്\u200d പ്രധാനപ്പെടുത് അതുകൊണ്ടും, നമ്മുടെ പ്രോദേശിക്കപ്പെട്ട സമീപത്തിന്\u200dറെ നേരെ ഞങ്ങള്\u200d വിലാസിക്കുന്നു. GLUE ബെങ്ക്മാര്\u200dക്കില്\u200d നിന്നും കുറച്ച് ഡാറ്റാസെറ്റും ഡാറ്റാമ്പില്\u200d നിന്നും പ്രവര്\u200dത്തിക്കുന്നു. അവിടെ നമ്മള നമ്മുടെ കോഡ് പൊതുവാണ്.', 'ms': 'Penerimaan model berasaskan Transformer dalam pemprosesan bahasa semulajadi (NLP) telah membawa kepada sukses besar menggunakan sejumlah besar parameter. Namun, disebabkan kekangan penerbangan dalam peranti pinggir, terdapat kepentingan meningkat dalam pemampatan model ini untuk meningkatkan masa kesimpulan dan jejak memori mereka. Kertas ini menghasilkan objektif kehilangan baru untuk memampatkan penyampaian token dalam model berasaskan-Transformer dengan menggunakan arkitektur AutoEncoder. Lebih khususnya, kami menekankan kepentingan arah penyampatan yang dipampat berkaitan dengan penyampatan yang tidak dipampat asal. The proposed method is task-agnostic and does not require further language modeling pre-training.  Kaedah kami jauh melebihi pendekatan faktorisasi matriks berdasarkan SVD yang biasa digunakan dalam terma model bahasa awal Perplexity. Selain itu, kita menilai pendekatan yang kita cadangkan atas SQuAD v1. 1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios.  Kod kita adalah awam.', 'no': 'Adopteringa av transformeringsbaserte modeller i naturspråkshandtering (NLP) har ført til stor suksess ved å bruka massivt tal parametra. Men på grunn av utføring av begrensningar i kanteiningar har det oppstått interesse i komprimeringa av desse modelane for å forbetra sine inferensningstid og minnetekst. Denne papiret viser eit nytt tap-mål for å komprimera innbygging av token i Transformeringsmodellen ved å levera eit AutoEncoder-arkitektur. Dette er meir spesifikke viktig for retningen av komprimerte innbyggingar med hensyn til originale ukomprimerte innbyggingar. Dette første metoden er task-agnostic og treng ikkje fleire språk-modellering før opplæring. Metoden vårt utfører det vanleg brukte matrisefaktoriseringstilnærminga med SVD-baserte matrisefaktorisering i uttrykk av startspråk-modellen Perplexity. I tillegg evaluerer vi vår foreslått tilnærming over SQuAD v1. 1 dataset og fleire nedstrømmeoppgåver frå GLUE-benchmarken, der vi også utfører baselinja i dei fleste scenarioane. Koden vårt er offentleg.', 'sr': 'Usvajanje modela na transformeru u procesu prirodnog jezika (NLP) dovelo je do velikog uspeha koristeći veliki broj parametara. Međutim, zbog ograničenja rasporedanja granica uređaja, povećala je interes za kompresiju ovih modela kako bi poboljšala njihovo vrijeme infekcije i otisak pamćenja. Ovaj papir predstavlja novi cilj gubitka da kompresira ugrađenje znakova u modelima na osnovu transformera pod uticajem arhitekture AutoEncoder a. Posebnije, naglašavamo važnost smjera kompresiranih integracija u pogledu originalnih nekompresiranih integracija. Predložena metoda je zadatak-agnostik i ne zahteva daljnje modeliranje jezika pre obuke. Naša metoda značajno iznosi uobičajeno korišteni pristup faktorizaciji matrice na SVD-u u smislu prvog jezičkog modela Perplexity. Osim toga, procjenjujemo naš predloženi pristup preko SQuAD v1. 1. seta podataka i nekoliko sledećih zadataka iz skloništa GLUE-a, gde takođe izvršavamo početnu liniju u većini scenarija. Naš kod je javan.', 'pl': 'Przyjęcie modeli opartych na Transformerze w przetwarzaniu języka naturalnego (NLP) doprowadziło do ogromnego sukcesu z wykorzystaniem ogromnej liczby parametrów. Jednakże ze względu na ograniczenia wdrożeniowe w urządzeniach krawędziowych wzrasta zainteresowanie kompresją tych modeli w celu poprawy ich czasu wnioskowania i śladu pamięci. W niniejszym artykule przedstawiono nowy cel stratowy w celu skompresowania osadzeń tokenów w modelach opartych na Transformerze poprzez wykorzystanie architektury AutoEncoder. Dokładniej podkreślamy znaczenie kierunku skompresowanych osadzeń w odniesieniu do oryginalnych nieskompresowanych osadzeń. Proponowana metoda jest agnostyczna od zadań i nie wymaga dalszego szkolenia wstępnego modelowania językowego. Nasza metoda znacznie przewyższa powszechnie stosowane podejście macierzowe oparte na SVD pod względem początkowego modelu językowego Perplexity. Ponadto oceniamy proponowane podejście w stosunku do SQuAD v1. 1-zbiór danych i kilka dalszych zadań z referencji GLUE, w których w większości scenariuszy również przewyższamy wartość bazową. Nasz kod jest publiczny.', 'so': 'Dalbashada modelalka afka dabiicadda ah (NLP) waxaa sababtay liibaano weyn isticmaalka parameters aad u badan. Si kastaba ha ahaatee, sababtoo ah qasab ka soo dejinta qalabka darafka, waxaa jirtay xiiso aad u kordhaya daboolka modelladan si ay u hagaajiyaan waqtigooda cudurka iyo calaamadaha xusuusta. Kanu warqaddaas wuxuu keenaa lumbarka qoyska oo ay compresses token embeddings in the Transformer-based models by levering an AutoEncoder architecture. Si gaar ah, waxaynu ku qoraynaa muhiimka u ah hagitaanka guud oo ku saabsan meelaha hore oo aan la xisaabin. Midabka la soo jeeday waa shaqo-aragnimo, loomana baahna in luqad dheeraad ah u sameynta waxbarasho ka horeeya. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity.  Sidoo kale waxaynu qiimeynaynaa qaabilaadeenna la soo jeeday ee SQuAD v1. 1 taariikhda iyo shaqooyinka hoose ee GLUE (GLUE) laga sameeyo, kaas oo ah meesha aan ku sameyno qoraalka hoose ee ugu badnaan dhaqdhaqaaqa. Qoidayadeenu waa dhamaan.', 'sv': 'Antagandet av Transformer-baserade modeller i Natural Language Processing (NLP) har lett till stor framgång med hjälp av ett stort antal parametrar. På grund av driftsättningsbegränsningar i edge-enheter har intresset för komprimering av dessa modeller ökat för att förbättra deras sluttid och minnesavtryck. Denna uppsats presenterar ett nytt förlustmål för att komprimera token inbäddningar i Transformer-baserade modeller genom att utnyttja en AutoEncoder-arkitektur. Mer specifikt betonar vi vikten av riktningen för komprimerade inbäddningar jämfört med original okomprimerade inbäddningar. Den föreslagna metoden är task-agnostic och kräver inte ytterligare språkmodellering pre-training. Vår metod överträffar signifikant den vanligaste SVD-baserade matrisfaktoriseringsmetoden i termer av initial språkmodell Perplexity. Dessutom utvärderar vi vårt föreslagna tillvägagångssätt över SQUAD v1. 1 dataset och flera nedströmsuppgifter från GLUE-riktmärket, där vi också presterar bättre än baslinjen i de flesta scenarier. Vår kod är offentlig.', 'ro': 'Adoptarea modelelor bazate pe Transformer în procesarea limbajului natural (PNL) a condus la un mare succes utilizând un număr masiv de parametri. Cu toate acestea, din cauza constrângerilor de implementare în dispozitivele edge, a existat un interes în creștere în compresia acestor modele pentru a îmbunătăți timpul de deducere și amprenta memoriei lor. Această lucrare prezintă un nou obiectiv de pierdere de comprimare a încorporărilor token în modelele bazate pe Transformer, utilizând o arhitectură AutoEncoder. Mai precis, subliniem importanța direcției încorporărilor comprimate în raport cu încorporările originale necomprimate. Metoda propusă este agnostică și nu necesită pregătirea de modelare lingvistică suplimentară. Metoda noastră depășește semnificativ abordarea bazată pe factorizarea matricei SVD utilizată în mod obișnuit în termeni de modelul lingvistic inițial Perplexity. Mai mult decât atât, evaluăm abordarea propusă asupra SQUAD v1. 1 set de date și mai multe activități în aval din benchmark GLUE, unde depășim, de asemenea, valoarea de referință în majoritatea scenariilor. Codul nostru este public.', 'si': 'ස්වභාවික භාෂාව ප්\u200dරක්\u200dරියාපනය (NLP) වලින් ප්\u200dරමාණකය සඳහා ප්\u200dරමාණකය අධාරිත විද්\u200dයාපනය විද්\u200dයාපනය කරලා  නමුත්, කුණු උපකරණයේ සීමාවන් විසින් විසින් විසින්, මේ මොඩේල් එකේ සීමාවන් විසින් විසින් විශේෂ වෙලා තියෙනවා ඔව Name තව විශේෂයෙන්, අපි ප්\u200dරශ්නයක් තියෙනවා ප්\u200dරශ්නයක් නැති ප්\u200dරශ්නයක් ගැන ප්\u200dරශ්නයක් තියෙනවා. ප්\u200dරශ්න විධානය ක්\u200dරියාව-agnoසික් වෙන්නේ ඒ වගේම වඩා භාෂාව ප්\u200dරශ්නයක් අවශ්\u200dය නැහැ. අපේ විධානය ගොඩක් විශේෂයෙන් ප්\u200dරභාවිත වෙන්නේ SVD- අධාරිත මැට්\u200dරික්ස්- අභාවිත විධානය ප්\u200dරභාවිත වි ඒවගේම, අපි SQuAD v1 වලින් අපේ ප්\u200dරතිචාර ප්\u200dරතිචාර විශ්වාස කරනවා. GLUE බෙන්ච්මාර්ක් වලින් දත්ත සෙට් එක්ක සහ බිම්ස්ට්\u200dරීම් වැඩිය ගොඩක් වැඩිය ගොඩක් සිනාරියෝ වලින් අපි පස අපේ කෝඩ් එක ප්\u200dරජාතිකාරයි.', 'ur': 'طبیعی زبان پریشینگ (NLP) میں ترنسفور بنیادی موڈل کے مطابق مطابق بہت سی پارامتر کے مطابق بہت بڑی موفقیت کو پہنچا رہا ہے۔ However, due to deployment constraints in edge devices, there has been a rising interest in compression of these models to improve their inference time and memory footprint. This paper presents a new loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. اور زیادہ مخصوص، ہم مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مضبوط مض پیشنهاد کی طریقہ task-agnostic ہے اور اس کے لئے اضافہ زبان کی مدلینگ کی ضرورت نہیں ہے۔ ہمارا طریقہ صریح طور پر صریح استعمال کیا گیا ہے SVD-بنیاد ماٹریکس-فاکتوریزی طریقہ پر پہلی زبان موڈل پرپرپرلکستی کے مطابق۔ اور ہم نے SQuAD v1 پر ہماری پیشنهاد کی طریقہ کا ارزش کیا ہے. ایک ڈاٹ سٹ اور بہت سی سناریوں میں سے نیچے ڈاٹ سٹ اور بہت سی کام GLUE بنچم مارک سے، جہاں ہم نیچے بنسٹ لین کو بھی زیادہ سناریوں میں انجام دیتے ہیں. ہمارا کیڈ عمومی ہے.', 'ta': 'இயல்பான மொழி செயல்படுத்தல் (NLP) மாற்று அடிப்படையான மாதிரிகளை பெற்றுக் கொண்டு பெரிய வெற்றிகரமான அளவுருக்களை பயன்படு ஆனால், விளிம்பு சாதனங்களில் வெளியீட்டு கட்டுப்பாடுகள் காரணத்தால், இந்த மாதிரிகளின் சுருக்கத்திற்கு விருப்பம் ஏற்பட்டுள்ளது அதி இந்த தாள் ஒரு புதிய இழப்பு பொருளை கூட்டும் குறியீட்டு பொருளை சுருக்கும் பொருளை தானியங்கி குறியீட்டாளர் உருவாக்கும மேலும் குறிப்பிட்டு, நாம் சுருக்கப்பட்ட திசையின் முக்கியத்தை குறிப்பிடுகிறோம் உண்மையான குறிப்பிடாத பொரு பரிந்துரைக்கப்பட்ட முறைமையெல்லாம் பணி குறிக்கும் முன் பயிற்சி மாதிரி மாதிரி தேவையில்லை. நம்முடைய முறைமையில் பொதுவாக SVD-அடிப்படையில் உள்ள மாட்ரிக்ஸ்-காரணிகள் செயல்படுத்தும் முதல் மொழி மாதிரி பிரச்சியல்  மேலும், நாம் SQuAD v1 க்கு மேல் எங்கள் பரிந்துரைய நெருக்கம் மதிப்பிடுகிறோம். 1 தரவு அமைப்புகள் மற்றும் GLUE பெங்குருக்குப் பணிகளில் இருந்து பல கீழ் நீர்வுச் செய்திகள், அதில் நாம் பெரும்பாலா எங்கள் குறியீடு பொது உள்ளது.', 'uz': "Name Lekin, chegara uskunalarni qo'yish chegaralari sababi, bu modellarni bosib chiqarish uchun juda qiziqarli edi va xotira va foydalanishni oshirish uchun foydalanadi. Name Ko'pchilik, biz asl qismlari haqida o'zgartiriladigan qismlarning muhimligini o'zgartiraymiz. Talab qilingan usul - vazifa- agnostik, va oldingi taʼminlovdan oldin tilni modellash kerak emas. Bizning usuli umumiy SVD asosida ishlatilgan matrix-factorization usulini birinchi tillar modeli Perpleksiya darajada bajaradi. Ko'rib, biz SQuAD v1 haqida o'ylaymiz. @ info: status Kodlarimiz jamiyat.", 'vi': 'Cách sử dụng các mô hình biến hình cho ngôn ngữ tự nhiên (NLP) đã có thành công lớn nhờ vào một số lượng lớn các tham số. Tuy nhiên, do giới hạn triển khai trong các thiết bị cạnh, có sự hứng thú tăng dần với việc nén các mô hình này để cải thiện thời gian suy luận và kí ức của chúng. Tờ giấy này đề xuất một mục tiêu mất mát mới để thu thập các sự nhúng tay trong các mô hình transformer bằng cách dùng một kiến trúc AutoEncoder. Cụ thể hơn, chúng tôi nhấn mạnh tầm quan trọng của sự hướng dẫn của sự gắn ép với sự nhúng vào không có nén gốc. Phương pháp được đề nghị là ngôn ngữ bồ nghĩa và không cần phải học thêm lớp mẫu ngôn ngữ. Cách của chúng ta vượt qua khả năng ngẫu nhiên của phương pháp phân chia ma trận có hệ thống dạng dạng chữ xoay vần. Hơn nữa, chúng tôi đánh giá phương pháp kế hoạch của chúng tôi về SHADD v1. 1 bộ dữ liệu và vài công việc xuôi dòng từ tiêu chuẩn GLUE, nơi chúng ta cũng vượt hoàn thiện cơ sở trong hầu hết các viễn cảnh. Luật của chúng tôi công khai.', 'bg': 'Приемането на трансформаторни модели в обработката на естествен език (НЛП) доведе до голям успех при използване на огромен брой параметри. Въпреки това, поради ограниченията за внедряване в крайни устройства, има нарастващ интерес към компресирането на тези модели, за да се подобри тяхното време за заключение и отпечатък на паметта. Тази статия представя нова цел за компресиране на вгражданията на токени в моделите, базирани на трансформатори, чрез използване на архитектурата на Автокодер. По-конкретно, подчертаваме значението на посоката на компресираните вграждания по отношение на оригиналните непомпресирани вграждания. Предложеният метод е агностичен към задачите и не изисква допълнително обучение по езиково моделиране. Методът ни значително превъзхожда често използвания подход на матрично-факторизация по отношение на първоначалния езиков модел Перплексити. Освен това, ние оценяваме нашия предложен подход към SQuAD v1. 1 набор от данни и няколко задачи надолу по веригата от бенчмарка където също надминаваме базовата база в повечето сценарии. Кодексът ни е публичен.', 'hr': 'Usvajanje modela na transformeri u procesu prirodnog jezika (NLP) dovelo je do velikog uspjeha koristeći veliki broj parametara. Međutim, zbog ograničenja rasporedanja graničnih uređaja, povećala je interes za kompresiju tih modela kako bi poboljšala njihovo vrijeme infekcije i otisak sjećanja. Ovaj papir predstavlja novi cilj gubitka za kompresiranje ugrađenja znakova u modelima na osnovu transformera, koristeći arhitekturu AutoEncoder a. Posebnije, naglašavamo važnost smjera kompresiranih ugrađenja u pogledu originalnih nekompresiranih ugrađenja. Predloženi metod je zadatak-agnostičan i ne zahtijeva daljnje modeliranje jezika prije obuke. Naša metoda značajno iznosi uobičajeno korišteni pristup faktorizaciji matrice na SVD-u u smislu početnog jezičkog modela Perplexity. Osim toga, procjenjujemo naš predloženi pristup preko SQuAD v1. 1. seta podataka i nekoliko zadataka iz skloništa GLUE-a, gdje smo također izvršili početnu liniju u većini scenarija. Naš kod je javan.', 'da': 'Anvendelsen af Transformer-baserede modeller i naturlig sprogbehandling (NLP) har ført til stor succes ved hjælp af et stort antal parametre. På grund af implementeringsbegrænsninger i edge-enheder har der imidlertid været en stigende interesse for komprimering af disse modeller for at forbedre deres sluttid og hukommelsesmæssige fodaftryk. Denne artikel præsenterer et nyt tabsmål om at komprimere token-indlejringer i Transformer-baserede modeller ved at udnytte en AutoEncoder-arkitektur. Mere specifikt understreger vi vigtigheden af retningen af komprimerede indlejringer i forhold til originale ukomprimerede indlejringer. Den foreslåede metode er opgaveagnostisk og kræver ikke yderligere sprogmodellering foruddannelse. Vores metode overgår betydeligt den almindeligt anvendte SVD-baserede matrix-factorization tilgang med hensyn til den oprindelige sprogmodel Perplexity. Desuden evaluerer vi vores foreslåede tilgang over SQUAD v1. 1 datasæt og flere downstream opgaver fra GLUE benchmark, hvor vi også overperformer baseline i de fleste scenarier. Vores kodeks er offentlig.', 'nl': "De invoering van Transformer-gebaseerde modellen in Natural Language Processing (NLP) heeft geleid tot groot succes met behulp van een enorm aantal parameters. Vanwege implementatiebeperkingen in randapparaten is er echter een toenemende belangstelling voor de compressie van deze modellen om hun inferentietijd en geheugenvoetafdruk te verbeteren. Dit artikel presenteert een nieuwe verliesdoelstelling voor het comprimeren van token embeddings in de Transformer-gebaseerde modellen door gebruik te maken van een AutoEncoder-architectuur. Meer specifiek benadrukken we het belang van de richting van gecomprimeerde embeddings ten opzichte van originele ongecomprimeerde embeddings. De voorgestelde methode is taakagnostisch en vereist geen verdere taalmodellering pre-training. Onze methode presteert aanzienlijk beter dan de veelgebruikte SVD-gebaseerde matrix-factorization benadering in termen van initiële taalmodel Perplexity. Bovendien evalueren we onze voorgestelde aanpak over SQuAD v1. 1 dataset en verschillende downstreamtaken uit de GLUE benchmark, waarbij we in de meeste scenario's ook beter presteren dan de baseline. Onze code is openbaar.", 'de': 'Die Einführung von Transformer-basierten Modellen in der Natural Language Processing (NLP) hat mit einer Vielzahl von Parametern zu großem Erfolg geführt. Aufgrund von Bereitstellungsbeschränkungen in Edge-Geräten gibt es jedoch ein wachsendes Interesse an der Komprimierung dieser Modelle, um deren Inferenzzeit und Speicherbedarf zu verbessern. In diesem Beitrag wird ein neuartiges Verlustziel vorgestellt, um Token-Einbettungen in Transformer-basierte Modelle mithilfe einer AutoEncoder-Architektur zu komprimieren. Insbesondere betonen wir die Bedeutung der Richtung komprimierter Einbettungen in Bezug auf originale unkomprimierte Einbettungen. Die vorgeschlagene Methode ist aufgabenunabhängig und erfordert keine weiteren Sprachmodellierungsvorbereitungen. Unsere Methode übertrifft den häufig verwendeten SVD-basierten Matrixfaktorisierungsansatz in Bezug auf das Ausgangssprachenmodell Perplexity signifikant. Darüber hinaus evaluieren wir unseren vorgeschlagenen Ansatz über SQuAD v1. 1-Datensatz und mehrere nachgelagerte Aufgaben aus dem GLUE Benchmark, bei denen wir in den meisten Szenarien auch die Baseline übertreffen. Unser Code ist öffentlich.', 'id': 'Adopsi model berdasarkan Transformer dalam proses bahasa alam (NLP) telah membawa sukses besar menggunakan sejumlah besar parameter. Namun, karena keterangan penerbangan dalam perangkat pinggir, ada kepentingan meningkat dalam kompresi model ini untuk meningkatkan waktu dan jejak ingatan mereka. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture.  Lebih spesifik, kami menekankan penting arah penerbangan terkompresi dengan kaitan dengan penerbangan yang tidak terkompresi asli. Metode yang diusulkan adalah tugas-agnostik dan tidak membutuhkan model bahasa lebih lanjut pra-pelatihan. Metode kita jauh melebihi pendekatan matriks-faktorisasi berdasarkan SVD yang biasanya digunakan dalam terma model bahasa awal Perplexity. Selain itu, kami mengevaluasi pendekatan kami yang diusulkan atas SQuAD v1. 1 dataset dan beberapa tugas turun dari benchmark GLUE, di mana kita juga melampaui batas dasar dalam kebanyakan skenario. Kode kita publik.', 'sw': 'The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters.  Hata hivyo, kwa sababu ya vikwazo vya kutumia vifaa vinavyoenea, kumekuwa na maslahi yanayoongezeka kwenye shinikizo la miundo mbili hizi ili kuboresha muda wao na nyakati za kumbukumbu. Gazeti hili linaonyesha lengo la kupoteza riwaya ili kuingia alama inayotokana na mifano ya zamani kwa kutumia ujenzi wa AutoEncoder. Zaidi zaidi, tunasisitiza umuhimu wa mwelekeo wa maeneo yanayoshindwa na kuhusiana na maeneo ya awali yasiyoeleweka. Utawala unapendekezwa ni upuuzi wa kazi na hauhitaji kuonyesha mifano ya lugha ya kabla ya mafunzo. Utawala wetu unaonyesha utaratibu wa kutengeneza matrix-yenye msingi wa SVD kwa kiasi kikubwa wa mifano ya lugha ya awali. Zaidi ya hayo, tunatathmini mbinu zetu zilizopendekezwa kupitia SQuAD v1. 1 takwimu pamoja na kazi kadhaa za mito ya chini kutoka kwenye bendera ya GLUE, ambapo pia tunafanya msingi katika hali nyingi. Utawala wetu ni wazi.', 'ko': '자연 언어 처리(NLP)에서 변환기 기반 모델을 채택하여 대량의 파라미터를 사용하는 데 큰 성공을 거두었다.그러나 테두리 설비의 배치 제한으로 인해 사람들은 추리 시간과 메모리 점용을 높이기 위해 이러한 모델에 대한 압축에 점점 더 관심을 기울이고 있다.본고는 자동 인코더 구조를 이용하여 변압기 기반 모델에 있는 영패를 압축하는 새로운 손실 목표를 제시했다.더욱 구체적으로 말하면, 우리는 압축 삽입이 원시적으로 압축되지 않은 삽입 방향에 대한 중요성을 강조한다.이 방법은 임무에 의존하지 않고 더 이상의 언어 모델링 훈련이 필요 없다.초기 언어 모델의 복잡도에 있어서 우리의 방법은 흔히 사용하는 기이한 값 분해를 바탕으로 하는 행렬 분해 방법보다 현저히 우수하다.그 밖에 우리는 우리가 제시한 방법에 대해 평가를 진행했다.1개의 데이터 세트와 몇 개의 GLUE 기준의 하위 임무는 대다수 상황에서도 기준보다 우수하다.우리의 코드는 공개된 것이다.', 'fa': 'قبول مدل\u200cهای متغیرگر در پردازش زبان طبیعی (NLP) موفقیت بزرگی را با استفاده از تعداد بزرگی از پارامترها به دست آورد. با این حال، به دلیل محدودیت در دستگاه\u200cهای لبه\u200cها، علاقه\u200cای به فشار این مدل\u200cها افزایش می\u200cیابد تا زمان آلودگی و چاپ پای حافظه\u200cشان را بهتر کند. این کاغذ هدف از دست دادن نویسی را نشان می دهد که در مدل\u200cهای تغییر\u200cپذیر با استفاده از یک معماری AutoEncoder تغییر دهد. دقیقاً ما مهمترین مسیر پیچیده\u200cهای پیچیده را با احترام به پیچیده\u200cهای اصلی تشکیل می\u200cدهیم. این روش پیشنهاد پیش آموزش کار-agnostic است و نیاز به نمونه\u200cهای پیش آموزش زبان بیشتری ندارد. روش ما به طور معمولی از طریق ماتریکس-فاکتوریزی که معمولاً از SVD استفاده می\u200cشود به عنوان مدل اصلی زبان پرپرپرلکستی را اجرا می\u200cکند. در ضمن، ما روش پیشنهاد خود را بر اساس SQuAD v1 ارزیابی می کنیم. یک مجموعه داده\u200cها و چندین کار پایین\u200cترین از نقشه\u200cهای GLUE، جایی که ما همچنین خط پایین\u200cترین صحنه\u200cها را در بیشترین صحنه\u200cها انجام می\u200cدهیم. کد ما عمومی است.', 'tr': 'Natal dil işlemekde terjime edilen nusgalaryň (NLP) geçişinde gaty bir sany parameterler ulanyp ýöredi. Ýöne, çukur düzümlerini deplemek üçin bu nusgalaryň azalyşynyň wagtyny we ýagdaýyň futbolyny düzeltmek üçin ýokary gyzyklanýar. Bu kagyz bir otoKödler arhitektegi üçin token integrlerini Transformer tabanly nusgalarynda süýtgetmek üçin bir nusgala ýitirmek maksadyny görkezýär. Aýratyn görä, biz özüniň basylmadyk ködlemeler üçin gaty möhümligini emplaýarys. Maslahat edilen yöntem gören-agnostik we başga dillerden öň-okuwçylmagy gerek däl. Biziň yöntemimiz, başlangıç dil modi Perpleksiýa ýagdaýynda adatça işledilen SVD-den matris faktörlemek ýagdaýyny çykarýar. Soňra SQuAD we 1 (v1) arasynda teklip eden ýazşymyzy çykarýarys. 1 rakam seti we gürrüň derejesinden birnäçe aşaky takyk görevler, ol ýerde birnäçe senaryoda baselini çykarýarys. Biziň kodymyz jemgyýetdir.', 'af': "Die aanvaar van Transformer-gebaseerde modele in natuurlike taal-prosessering (NLP) het na groot sukses gelei deur 'n groot aantal parameters te gebruik. Maar, vanweë die uitbreiding van grense in randtoestelle, is daar 'n opgroei belang in die komprimering van hierdie modele om hul inferensie tyd en geheue voetskrif te verbeter. Hierdie papier stel 'n nuwe verlies objekte om token inbettings in die Transformer-gebaseerde modele te komprimeer deur 'n AutoEncoder arkitektuur te verwyder. Meer spesifieke, ons bepaal die belangrikheid van die rigting van komprimeerde inbêdings met respek na oorspronklike onkomprimeerde inbêdings. Die voorgestelde metode is task-agnostic en benodig nie verdere taal modellering voor-oefening nie. Ons metode betekenlik uitvoer die gewoonlik gebruik SVD- gebruik matriks- faktoriseering toegang in terms van aanvanklike taal model Perplexity. Ook, ons evalueer ons voorgestelde toegang oor SQuAD v1. 1 datastel en verskeie onderstreem opdragte van die GLUE benchmark, waar ons ook die basisline in meeste scenarios uitvoer. Ons kode is openbare.", 'am': 'በተለየ ቋንቋ ማቀናቀል (NLP) የተመሳሳይ የተደረገው የፊደል መተላለፊያ ምሳሌዎችን በመስጠት በጥልቅ ቁጥጥር በተለይ መጠቀም አቅኝቷል፡፡ ምንም እንኳን፣ በክፍለ ዕቃዎች ላይ ግንኙነት በመውሰድ ምክንያት፣ እነዚህን ሞዴላዎች ማጨነቅ ጊዜ እና የመታሰቢያ እግር ማድረግ የሚጠቅምበት ውጤት አበዛ፡፡ ይህ ፕሮግራም የዶሴ መሠረት መሠረት በመጠቀም በTransformer-based models በተጨማሪም፣ በተጨማሪው አካባቢ አካባቢዎች ላይ አካባቢ መሆኑን አዋጅ እናሳውቃለን፡፡ የተዘጋጀው ሥርዓት የስራ-አቆስቲ ነው እናም የቋንቋ ምሳሌ የፊተኛውን ትምህርት መግለጫ አያስፈልግም፡፡ የፊደል ቋንቋ ምሳሌ የፊደል ጥያቄ ሲሆን የSVD-based matrix-factorization ሥርዓት በሚያሳየው አካባቢ ነው፡፡ ከዚህም በላይ SquaAD v1 ላይ የተዘጋጀውን ቅድሚያ እናስተዋልታለን፡፡ የGLUE benchmark (GLUE) የዳታ ሰርሰት እና የውኃ ፈሳሽ ስራዎችን አብዛኛዎቹ እየተሳተፍን እናደርጋለን፡፡ ኮድማችን ግልፅ ነው።', 'hy': 'Տանֆերմերների հիմնված մոդելների ընդունելը բնական լեզվի վերամշակում (ՆԼՊ) մեծ հաջողություն է հանգեցրել օգտագործելով մեծ քանակությամբ պարամետրեր: Այնուամենայնիվ, ծայրային սարքերի տարածման սահմանափակումների պատճառով, այս մոդելների ճնշումը աճում է, որպեսզի բարելավվի նրանց հետևյալների ժամանակը և հիշողության ոտքերը: Այս թղթին ներկայացնում է նոր կորստի օբյեկտիվ, որը նշանակում է ճնշումն ավտոկոդերի ճարտարապետության օգնությամբ տրանսֆորմային մոդելներում գտնվող նշանները: Ավելի հատկապես, մենք շեշտում ենք ճնշված ներդրումների ուղղության կարևորությունը վերաբերյալ սկզբնական անճնշված ներդրումներին: Պատրաստված մեթոդը խնդիրների ագնոստիկ է և չի պահանջում նախապատրաստ լեզվի մոդելավորման: Մեր մեթոդը նշանակալիորեն գերազանցում է սովորաբար օգտագործված ՄԱՎ-ի հիմնված մաթրիքսային գործոնավորումների մոտեցումը սկզբնական լեզվի մոդելի պարլեքսիվության տեսանկյունից: Ավելին, մենք գնահատում ենք մեր առաջարկված մոտեցումը SQUAD1-ի դեպքում: 1 տվյալների համակարգ և մի քանի հետագա խնդիրներ, որոնք գտնվում են GLUE-ի համեմատային նպատակից, որտեղ մենք նաև գերազանցում ենք հիմնական արտադրողականությունը սցենարների մեծ մասում: Մեր կոդը հանրային է:', 'sq': 'miratimi i modeleve me bazë në Transformer në procesimin natyror të gjuhës (NLP) ka çuar në sukses të madh duke përdorur një numër masiv parametrash. Megjithatë, për shkak të kufizimeve të vendosjes në pajisjet e kufirit, ka patur një interes në rritje në shtypjen e këtyre modeleve për të përmirësuar kohën e tyre të përfundimit dhe gjurmën e kujtesës. Ky dokument paraqet një objektiv të humbjes së re për të kompresuar përfshirjet e token në modelet me bazë në Transformer duke përdorur një arkitekturë AutoEncoder. Më specifikisht, ne theksojmë rëndësinë e drejtimit të përfshirjeve të shtypura lidhur me përfshirjet origjinale të pakompresuara. Metoda e propozuar është agnostike e detyrave dhe nuk kërkon modelim të mëtejshëm gjuhësh paratrajnimi. Metoda jonë mbivlerëson ndjeshëm metodën e përdorur në mënyrë të zakonshme të matricës-faktorizimit bazuar në SVD në termat e modelit fillestar të gjuhës Perplexity. Përveç kësaj, ne vlerësojmë qasjen tonë të propozuar mbi SQuAD v1. 1 set të dhënash dhe disa detyra poshtë nga standarti GLUE, ku ne gjithashtu mbikalojmë bazën në shumicën e skenarëve. Kodi ynë është publik.', 'bn': 'প্রাকৃতিক ভাষা প্রক্রিয়ায় ট্রান্সফার্ন ভিত্তিক মডেল (এনএলপি) নির্বাচনের ফলে বিশাল সংখ্যার পরিমাপ ব্যবহার করে  তবে প্রান্ত ডিভাইসে নিষেধাজ্ঞা নিয়ন্ত্রণের কারণে এই মডেলগুলোর প্রাপ্তে আগ্রহ বাড়ছে যাতে তাদের অস্থিরতা সময় ও স্মৃতির ফুটপুল্ট উন্ এই পত্রিকাটি একটি স্বয়ংক্রিয় এনকোডার আর্কিটেক্টার বানানোর মাধ্যমে ট্রান্সফ্রেক্রান্স ভিত্তিক মডেলে আটকানোর জন্য একটি ন আরো বিশেষ করে, আমরা প্রাথমিক অসম্পূর্ণ বিষয়বস্তু সম্পর্কে সম্মুখীন ভিত্তির দিকের গুরুত্বপূর্ণ দিকে জোর দিচ্ছি। প্রস্তাবিত পদ্ধতি হচ্ছে কাজ-অ্যাঙ্কোস্টিক এবং পূর্ব প্রশিক্ষণের জন্য আরো ভাষা মডেলের প্রয়োজন নেই। আমাদের পদ্ধতি প্রাথমিক ভাষার মডেল পার্পলিক্সির মাধ্যমে সাধারণত এসভিডি ভিত্তিক ম্যাট্রিক্স-ফ্যাক্টরিজেশন প্রয এছাড়াও, আমরা স্কুয়াড ভি১-এর উপর আমাদের প্রস্তাবিত পদ্ধতি মূল্যায়ন করি। জিএলউই বেনকম্যার্ক থেকে ১ ডাটাসেট এবং বেশ কয়েকটি নিচে নদীর কাজ, যেখানে আমরা বেশিরভাগ দৃশ্যের বেসারেলাইন চালু করি। Our code is public.', 'bs': 'Usvajanje modela na transformeru u procesu prirodnog jezika (NLP) dovelo je do velikog uspjeha koristeći veliki broj parametara. Međutim, zbog ograničenja rasporedanja granica uređaja, povećao je interes za kompresiju ovih modela kako bi poboljšao njihovo vrijeme infekcije i otisak pamćenja. Ovaj papir predstavlja novi cilj gubitka za kompresiranje ugrađenja znakova u modelima na transformaciji pod uticajem arhitekture AutoEncoder a. Posebnije, naglašavamo važnost smjera kompresiranih ugrađenja u pogledu originalnih nekompresiranih ugrađenja. Predloženi metod je zadatak-agnostičan i ne zahtijeva daljnje modeliranje jezika pre obuke. Naša metoda značajno iznosi uobičajeno korišteni pristup faktorizaciji matrice na SVD-u u smislu prvog jezičkog modela Perplexity. Osim toga, procjenjujemo naš predloženi pristup preko SQuAD v1. 1. seta podataka i nekoliko zadataka iz granice GLUE-a, gdje smo također izvršili početnu liniju u većini scenarija. Naš kod je javan.', 'az': 'Təbiətli dil işləməsi (NLP) içində Transformer tabanlı modellərin istifadəsi böyük bir sayı parametru vasitəsilə çox başarılı olaraq gəldi. Lakin bu modellərin sıxıntılarını artırmaq üçün çərçi aygıtlarında müəyyən edilmə səbəbi, həmin modellərin sıxıntılarının vaxtını və yada izlərini yaxşılaşdırmaq üçün artırmaq məqsədilə artırdı. Bu kağıt AutoEncoder arhitektüsünü təmin edib, Transformer tabanlı modellərdə token in şallarını sıkıştırmaq üçün yeni ziyana uğramaq məqsədini göstərir. Daha müəyyən olaraq, biz təsirlənmiş inbinglərin tərəfindən təsirlənmiş inbinglərin məqsədini təsdiqləyirik. Bu tədbir metodu task-agnostik və daha çox dil modellərinin əvvəlcə təhsil edilməsi lazımdır. İlk dil modeli Perplexity vasitəsilə müxtəlif istifadə edilən SVD-ə dayanan matris-faktörizasyon metodlarını çox çətin edir. Daha sonra, təklif etdiyimiz tərzimi SQuAD v1 ilə değerləşdiririk. GLUE benchmarkindən bir neçə verilən quruluş və bir neçə aşağı işlər, orada çox scenariolarda baz çizgisini də artırdıq. Bizim kodumuz halkıdır.', 'ca': "The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters.  Però, a causa de les restriccions d'implantació dels dispositius de bord, hi ha hagut un interès creixent en la compressió d'aquests models per millorar el seu temps de inferència i la petjada de la memòria. Aquest article presenta un nou objectiu de pèrdua per comprimir les integracions de fitxes en els models basats en Transformer aprofitant una arquitectura d'AutoEncoder. Més específicament, destaquem l'importància de la direcció de les incorporacions comprimides en relació amb les incorporacions originals no comprimides. El mètode proposat és una tasca-agnòstica i no requereix més modelació de llenguatges. El nostre mètode supera significativament l'enfocament de fabricació matricial basat en SVD en termes del model inicial de llenguatge Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1. 1 conjunt de dades i diverses tasques a avall del benchmark GLUE, on també superem el basal en la majoria dels escenaris. El nostre codi és públic.", 'cs': 'Přijetí transformátorových modelů v oblasti zpracování přirozeného jazyka (NLP) vedlo k velkému úspěchu s využitím obrovského množství parametrů. Nicméně vzhledem k omezením nasazení v okrajových zařízeních roste zájem o kompresi těchto modelů za účelem zlepšení jejich času inference a paměti. Tento článek představuje nový ztrátový cíl komprese vložení tokenů do modelů založených na Transformeru s využitím architektury AutoEncoder. Konkrétněji zdůrazňujeme význam směru komprimovaných vložení s ohledem na originální nekomprimované vložení. Navrhovaná metoda je úlohově agnostická a nevyžaduje další předškolení jazykového modelování. Naše metoda výrazně překonává běžně používaný přístup maticové faktorizace založený na SVD z hlediska počátečního jazykového modelu Perplexity. Navíc hodnotíme náš navrhovaný přístup přes SQuAD v1. 1 datová sada a několik následných úloh z GLUE benchmarku, kde ve většině scénářů také překonáváme základní hodnotu. Náš kód je veřejný.', 'et': 'Transformeritel põhinevate mudelite kasutuselevõtt looduskeele töötlemisel (NLP) on toonud kaasa suure edu, kasutades tohutut hulka parameetreid. Siiski on servaseadmete juurutamispiirangute tõttu suurenenud huvi nende mudelite tihendamise vastu, et parandada nende järeldusaega ja mälu jalajälge. Käesolevas töös esitatakse uudne kahjumi eesmärk tihendada tokeni manustamine Transformer-põhistes mudelites, kasutades AutoEncoderi arhitektuuri. Täpsemalt rõhutame tihendatud manuste suuna olulisust originaalsete tihendamata manuste suhtes. Kavandatud meetod on ülesannetest sõltumatu ja ei nõua täiendavat keele modelleerimise eelkoolitust. Meie meetod on oluliselt suurem kui tavaliselt kasutatav SVD-põhine maatriksi faktorisatsioon algse keelemudeli Perplexity osas. Lisaks hindame oma kavandatud lähenemisviisi SQuAD v1 kohta. Üks andmekogum ja mitu järgnevat ülesannet GLUE võrdlusaluse alusel, kus me tuleme enamikus stsenaariumides ka lähtetasemest üle. Meie koodeks on avalik.', 'fi': 'Muuntajapohjaisten mallien käyttöönotto luonnollisen kielen prosessoinnissa (NLP) on johtanut valtavaan menestykseen useiden parametrien avulla. Edge-laitteiden käyttöönoton rajoituksista johtuen näiden mallien pakkaaminen on kuitenkin lisääntynyt, jotta niiden päättelyaika ja muistijalanjälki paranevat. Tässä artikkelissa esitellään uusi häviötavoite, jonka tarkoituksena on pakata token-upotuksia Transformer-pohjaisiin malleihin hyödyntämällä AutoEncoder-arkkitehtuuria. Erityisesti korostamme pakattujen upotusten suunnan merkitystä alkuperäisten pakkaamattomien upotusten suhteen. Ehdotettu menetelmä on tehtäväagnostinen eikä vaadi lisää kielimallinnuksen esikoulutusta. Menetelmämme suoriutuu merkittävästi paremmin kuin yleisesti käytetty SVD-pohjainen matriisifaktorisointi alkuperäisessä kielimallissa Perplexity. Lisäksi arvioimme ehdotettua lähestymistapaa SQuAD v1:n suhteen. 1 datajoukko ja useita loppupään tehtäviä GLUE-vertailusta, joissa myös suoriudumme perusaikataulusta useimmissa skenaarioissa. Säännömme on julkinen.', 'ha': "Tsarin misãlai na Transformer-based cikin jarraba harshen aiki (NLP) ya led to babban rabo da amfani da wasu parameters mai girma. A lokacin da aka wajabta tsari cikin kayan kanyuta gefen, to, an ƙara masĩfa na danna dangantar da waɗannan misãlai dõmin ya kyautata lokacin kasa da fomat na kumbar. @ info Kayya, Munã ƙayyade muhimu ga shirin ayuka da aka samu da wajen filayen da ba'a sami ba. @ action: button MethoyinMu yana fara ta da amfani da matrix-factori-wanda aka ƙayyade a SV D a cikin muhimmin misalin harshen ta farko. Kayya, Munã ƙaddara hanyoyinmu da aka buƙata a kan SQuAD v1. QXml Kowanmu na bayyane.", 'sk': 'Sprejetje transformatorskih modelov v obdelavi naravnega jezika (NLP) je pripeljalo do velikega uspeha z uporabo ogromnega števila parametrov. Vendar pa je zaradi omejitev uvajanja v robnih napravah vse večje zanimanje za stiskanje teh modelov za izboljšanje časa sklepanja in pomnilniškega odtisa. V prispevku je predstavljen nov cilj izgube za stiskanje vdelav žetonov v modele, ki temeljijo na transformatorju, z uporabo arhitekture AutoEncoder. Natančneje poudarjamo pomen smeri stisnjenih vdelav glede na originalne nestisnjene vdelave. Predlagana metoda je agnostična in ne zahteva nadaljnjega predusposabljanja za modeliranje jezikov. Naša metoda v smislu začetnega jezikovnega modela Perplexity bistveno presega običajno uporabljen matrični faktorizacijski pristop SVD. Poleg tega smo ocenili naš predlagani pristop na SQuAD v1. 1 podatkovni nabor in več nadaljnjih opravil iz referenčne vrednosti GLUE, pri čemer v večini scenarijev tudi presegamo osnovno vrednost. Naš kodeks je javen.', 'jv': 'Transformer Nanging, nggawe nguasai mruput-suaraning nggawe Peringatan ora ono, dumadhi kapan nguasai tindakan Gambar kuwi ngewehi bukun bukun nggolong kanggo kowe nyimpen token karo model sing bisa Transformer dumadhi sing nyimpen architecture autokoder string" in "context_BAR_stringLink Perintah sing dipunangguna kuwi task-angustik lan ora nyatakake tarjamahan kanggo model urip banter. Awak dhéwé yèn ngerti nggawe barang nggawe barang urip nggawe Laptop" and "Desktop slot type Kowuhé punika publik.', 'bo': 'The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters. However, due to deployment constraints in edge devices, there has been a rising interest in the compression of these models to improve their inference time and memory footprint. ཤོག་བྱང་འདིས་གསར་བ་ལུགས་པའི་དམིགས་ཡུལ་ཞིག་འདུག་པས་རྟགས་ཀྱི་གནས་ཚུལ་མཛོད་ཀྱི་དཔེ་གཞི་བཟོ་བྱེད་ཀྱི་མ་དཔེ་གཞི་བཟོ་བྱེད་སྟངས ང་ཚོས་གསལ་བཤད་པ་གསལ་བཤད་ནི་ཐོག་མའི་གནས་ཚུལ་གྱི་གལ་ཆེ་ཤིག་གཅིག་མཚུངས་ཡིན། དམིགས་འཛུགས་ཀྱི་ཐབས་ལམ་ནི་task-agnostic རེད་སྤྱོད་མེད་པར། སྐད་རིགས་ཀྱི་རྣམ་གྲངས་སྔོན་གྲངས་བསྒྲིག་གནང་བ་དགོ ང་ཚོའི་ཐབས་ལམ་ལུགས་འདིས་གཞི་བྱས་ཡོད་པའི་SVD ཕྱོགས་རིས་ཆ་རྐྱེན་བཟོ་བྱེད་ཀྱི་གཟུགས འོན་ཀྱང་། ང་ཚོས་རང་གི་འཆར་བཀོད་པའི་གཟུགས་རིས་SQuAD v1 ལ་ཞིབ་བྱས་པ་ཡིན། GLUE benchmark ནང་ལས་གཟུགས་བརྗོད་ཀྱི་གནད་སྡུད་འདི་དང་downstream tasks་མང་པོ་ཞིག་ཡིན། ང་ཚོའི་ཨང་མེད་མང་ཆེན་པོ་རེད།', 'he': 'האימוץ של דוגמנים מבוססים בטרנספורס בעבודת שפת טבעית (NLP) הוביל להצלחה גדולה באמצעות מספר מסיבי של פרמטרים. However, due to deployment constraints in edge devices, there has been a rising interest in the compression of these models to improve their inference time and memory footprint.  העיתון הזה מציג אובייקטיב אובדן חדש כדי ללחוץ קישורים של סימנים במודלים המבוססים בטרנספורר על ידי השתמש בארכיטקטורה של AutoEncoder. במיוחד יותר, אנחנו מזכירים את חשיבות הכיוון של התכניות המולחצות בנוגע לתכניות לא מולחצות מקוריות. השיטה המוצעת היא אגנוסטית משימה ולא דורשת אימון מודל לשפה נוסף. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity.  Moreover, we evaluate our proposed approach over SQuAD v1. קבוצת נתונים אחת וכמה משימות לאורך המדרגות מראש התייחסות של GLUE, שבו אנחנו גם מעלים את המרכז בסיסי ברוב התרחישים. הקוד שלנו ציבורי.'}
