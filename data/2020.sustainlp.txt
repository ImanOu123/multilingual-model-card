{'en': 'Knowing Right from Wrong : Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?', 'ar': 'معرفة الصواب من الخطأ: هل يجب علينا استخدام نماذج أكثر تعقيدًا لتسجيل النقاط التلقائي قصير الإجابة في لغة البهاسا الإندونيسية؟', 'fr': 'Connaître le bien du mal\xa0: Devons-nous utiliser des modèles plus complexes pour la notation automatique des réponses courtes en bahasa indonesia\xa0?', 'pt': 'Saber o certo do errado: devemos usar modelos mais complexos para pontuação automática de respostas curtas em Bahasa Indonésia?', 'es': 'Saber lo correcto de lo incorrecto: ¿Deberíamos utilizar modelos más complejos para la puntuación automática de respuestas cortas en bahasa indonesia?', 'ja': '間違いから正しいことを知る：インドネシアのバハサでの自動短答スコアリングには、より複雑なモデルを使用する必要がありますか？', 'zh': '知是非:宜用更杂以自印尼语者简答题评分?', 'hi': 'गलत से सही जानना: क्या हमें बहासा इंडोनेशिया में स्वचालित शॉर्ट-आंसर स्कोरिंग के लिए अधिक जटिल मॉडल का उपयोग करना चाहिए?', 'ru': 'Знание правильного и неправильного: следует ли нам использовать более сложные модели для автоматической оценки коротких ответов в Бахасе, Индонезия?', 'ga': 'Eolas Ceart ó Mhícheart: Ar Chóir dúinn Múnlaí Níos Coimpléascacha a Úsáid le haghaidh Scóráil Uathoibríoch Gearrfhreagraí i Bahasa Indonesia?', 'ka': 'შეცდომის მარჯვნიდან ვიცით: უფრო კომპლექტური მოდელების გამოყენება ავტომატური კომპლექტური პასუხისთვის ბასასა ინდონეციაში?', 'el': 'Γνωρίζοντας το σωστό από το λάθος: Θα πρέπει να χρησιμοποιήσουμε πιο σύνθετα μοντέλα για την αυτόματη βαθμολογία σύντομης απάντησης στην Ινδονησία;', 'hu': 'A helyes és a rossz ismerete: Használjunk összetettebb modelleket az automatikus rövid válasz pontszámozáshoz Bahasa Indonéziában?', 'it': 'Conoscere il giusto da quello sbagliato: dovremmo usare modelli più complessi per il punteggio automatico delle risposte brevi in Bahasa Indonesia?', 'lt': 'Ar turėtume naudoti sudėtingesnius modelius automatiniam trumpų atsakymų vertinimui Bahase Indonezijoje?', 'kk': 'Жарамсыз мәліметті біледі: Бахаса Индонезиядағы автоматты қысқа жауап нәтижесін автоматты түрде қолдануға болады ба?', 'mk': 'Дали треба да користиме повеќе комплексни модели за автоматско скоро одговорно оценување во Бахаса Индонезија?', 'ms': 'Mengetahui Kanan dari Salah: Adakah kita patut Guna Model Lebih Kompleks untuk Skor Jawapan Kependek Automatik di Bahasa Indonesia?', 'ml': 'Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?', 'mn': 'Багасад Индонезид автоматтын богино хариултын тооцоололтоор илүү комплекс загварыг ашиглах уу?', 'mt': "Id-Dritt tal-Għarfien minn Żbaljat: Għandna nużaw Mudelli Aktar Komplessi għall-Punteġġ Awtomatiku ta' Tweġibiet Qasira fil-Bajasa Indoneżja?", 'no': 'Kjente høgre frå feil: Skal vi bruka fleire komplekse modeller for automatisk kort svar- poeng i Bahasa Indonesia?', 'pl': 'Wiedząc dobro od zła: Czy powinniśmy używać bardziej złożonych modeli do automatycznego punktowania krótkich odpowiedzi w Bahasa Indonesia?', 'ro': 'Cunoașterea dreptului de la greșit: Ar trebui să folosim modele mai complexe pentru scorarea automată a răspunsurilor scurte în România?', 'sr': 'Znajući pravo iz pogrešnog: da li bi trebali koristiti više kompleksnih modela za automatski rezultat kratkih odgovora u Bahasa Indoneziji?', 'si': 'වැරදි වලින් දකින්න: අපි බාහාසා ඉන්දෝනේසියාවේ ස්වයංක්\u200dරිය ප්\u200dරතිචාර ප්\u200dරතිචාරය සඳහා තව සම්පූ', 'so': 'Ogaanshada xaqa ee khalad ah: Ma u baahnaa in aan u isticmaalno qaababka dheeraad ee komplex, si aan u isticmaalno Automatic Short-Answer Scoring in Bahasa Indonesia?', 'sv': 'Att veta rätt från fel: Ska vi använda mer komplexa modeller för automatisk kortsvarspoäng i Sverige?', 'ta': 'தவறிலிருந்து வலது தெரியும்: பாஹாசா இந்தோனேசியாவில் தானியங்கி சுருக்கி வருடுதலுக்கு மேலும் குறுக்கும் மாதிரி', 'ur': 'غلط سے دائیں جانتے ہیں: کیا ہم باہاسا انڈونسیا میں آٹوٹی کوٹی جواب کا نمونہ استعمال کریں؟', 'uz': 'Xato tomonidan aniqlash: Bahasa Indoneziyaga avtomatik qisqa javob chiqarish uchun qoʻshimcha kompleks modellaridan foydalanishni istaysizmi?', 'vi': 'Biết đúng từ Sai: Chúng ta nên dùng chế độ phức tạp hơn cho Điểm đánh giá ngắn gọn tự động ở Bahamas Indonesia?', 'hr': 'Znajući pravo iz pogrešnog: da li bi trebali koristiti više kompleksnih modela za automatski rezultat kratkih odgovora u Bahasa Indoneziji?', 'bg': 'Да знаем правилно от грешно: Трябва ли да използваме по-сложни модели за автоматичен резултат на кратки отговори в България?', 'nl': 'Weten goed van fout: Moeten we complexere modellen gebruiken voor automatische short-answer scoren in Bahasa Nederland?', 'da': 'At vide rigtigt fra forkert: Skal vi bruge mere komplekse modeller til automatisk kort svar scoring i Danmark?', 'de': 'Richtig vom Falschen wissen: Sollten wir komplexere Modelle für die automatische Kurzantwort-Bewertung in Bahasa Indonesien verwenden?', 'id': 'Mengetahui Kanan dari Salah: Haruskah Kita Gunakan Model Lebih Kompleks untuk Skor Otomatis Jawaban Kurang di Bahasa Indonesia?', 'ko': '시비를 분명히 가리다: 우리는 인도네시아 바하사어에서 더욱 복잡한 자동 간단한 답안 평점 모델을 사용해야 합니까?', 'fa': 'دانستن حق از اشتباه: آیا باید از مدل\u200cهای پیچیده\u200cتری برای امتیاز پاسخ کوتاه\u200cهای خودکار در اندونزیا استفاده کنیم؟', 'sw': 'Kujua haki kutokana na makosa: Je, tutumie miundo ya kompyuta zaidi kwa ajili ya Uchaguzi mfupi wa kujibu jijini Bahasa Indonesia?', 'tr': 'Dogrydan bilýän Dogrydan Bilýän: Biz otomatik gysga-jogabat merkezi Bahasa Indoneziýada Daha kompleks Modellerden ullanmalymy?', 'af': 'Weet Regterkant van Verkeerde: Moet ons meer kompleks Modelle gebruik vir Outomatiese kort- antwoord Telling in Bahasa Indonesia?', 'sq': 'E dini të drejtën nga e gabuar: A duhet të përdorim modele më komplekse për shënimin automatik të përgjigjeve të shkurtra në Bahasa Indonezi?', 'am': 'የስህተት መብትን እናውቃለን፤ በባሐሳ ഇന ንዶኒስያ ውስጥ የበዛ ኮምፕሎክስክ ሞዴል ለራሳቸው አቋራጭ መልስ ቀለሞች እናስቀምጥ ዘንድ ይገባናል?', 'az': 'Doğrudan Bilən Haqqı: Daha çox kompleks modelləri Bahasa Indoneziyada Avtomatik Qısqa Cevap Görüntüsü üçün istifadə etməliyiz?', 'hy': 'Սխալ պատասխաններից ճիշտ գիտելը. Արդյո՞ք պետք է օգտագործենք ավելի բարդ մոդելներ Բահասայի Ինդոնեզիայում ավտոմատիկ կարճ պատասխանների գնահատման համար:', 'bn': 'ভুল থেকে সঠিক জানা: বাহাসা ইন্দোনেশিয়াতে স্বয়ংক্ষিপ্ত সংক্ষিপ্ত উত্তর স্কোরের জন্য আমরা কি আরো কম্পোলেক্স মডেল ব্যবহ', 'ca': 'Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?', 'cs': 'Vědět správné od špatné: Měli bychom používat složitější modely pro automatické skórování krátkých odpovědí v Bahasa Indonesia?', 'et': 'Teades õiget valest: Kas me peaksime kasutama keerukamaid mudeleid automaatseks lühikeste vastuste skoorimiseks Bahasa Indoneesia?', 'bs': 'Znajući pravo iz pogrešnog: da li bi trebali koristiti kompleksnije modele za automatski rezultat kratkih odgovora u Bahasa Indoneziji?', 'fi': 'Tiet채en oikean v채채r채st채: Pit채isik철 meid채n k채ytt채채 monimutkaisempia malleja automaattiseen lyhyen vastauksen pisteytykseen Bahasa Indonesia?', 'ha': 'Masana dama daga wata sharri: Shin, ko zã Mu yi amfani da Modellan Complex na ƙaranci wa masu motsi na farat ɗaya cikin Bahasa Industoniya?', 'he': 'Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?', 'jv': 'Peringatan ora nggawe Peringatan Gak: iso nggunakake model komplikasi tambah kanggo Peringatan ora otomatik ?', 'sk': 'Vedeti pravo od narobe: Ali bi morali uporabiti bolj kompleksne modele za samodejno ocenjevanje kratkih odgovorov v Sloveniji?', 'bo': 'Knowing Right from Wrong: Should we use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?'}
{'en': 'We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring : single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2 % difference in F1 compared to the deep learning approach with 1/18 time for model training.', 'ar': 'قمنا بمقارنة ثلاثة حلول لتحدي UKARA 1.0 في تسجيل الإجابات القصيرة الآلية: التعلم الكلاسيكي الفردي والتعلم الكلاسيكي والعميق. تتمثل المهمة في تصنيف الإجابات المعطاة لسؤالين ، سواء أكانوا على صواب أم خطأ. بينما يُظهر التطور الأخير تعقيدًا متزايدًا للنموذج لدفع الأداء المعياري ، فإنها تميل إلى أن تتطلب الموارد مع التحسين العادي. بالنسبة لمهمة UKARA ، وجدنا أن مناهج كيس الكلمات والتعلم الآلي الكلاسيكي يمكن أن تتنافس مع نماذج المجموعات ونموذج Bi-LSTM مع تضمين word2vec المُدرَّب مسبقًا من 200 مليون كلمة. في هذه الحالة ، حقق التعلم الآلي الكلاسيكي الفردي فرقًا أقل من 2 ٪ في F1 مقارنة بنهج التعلم العميق مع 1/18 مرة لتدريب النموذج.', 'es': 'Comparamos tres soluciones al desafío de UKARA 1.0 en la puntuación automática de respuestas cortas: clásica simple, clásica de conjunto y aprendizaje profundo. La tarea consiste en clasificar las respuestas dadas a dos preguntas, sean correctas o incorrectas. Si bien el desarrollo reciente muestra una complejidad creciente del modelo para impulsar los rendimientos de referencia, tienden a exigir recursos con mejoras mundanas. Para la tarea de UKARA, descubrimos que los enfoques de bolsa de palabras y aprendizaje automático clásico pueden competir con los modelos de conjunto y el modelo Bi-LSTM con la incrustación de word2vec preentrenada a partir de 200 millones de palabras. En este caso, el aprendizaje automático clásico único logró una diferencia de menos del 2% en F1 en comparación con el enfoque de aprendizaje profundo con 1/18 de tiempo para el entrenamiento con modelos.', 'pt': 'Comparamos três soluções para o desafio UKARA 1.0 na pontuação automatizada de respostas curtas: single classic, ensemble classic e deep learning. A tarefa é classificar as respostas dadas a duas perguntas, se estão certas ou erradas. Embora o desenvolvimento recente mostre uma complexidade crescente do modelo para impulsionar os desempenhos de referência, eles tendem a exigir recursos com melhorias mundanas. Para a tarefa UKARA, descobrimos que as abordagens clássicas de aprendizado de máquina e saco de palavras podem competir com modelos de conjunto e modelo Bi-LSTM com incorporação word2vec pré-treinada de 200 milhões de palavras. Nesse caso, o aprendizado de máquina clássico único alcançou menos de 2% de diferença em F1 em comparação com a abordagem de aprendizado profundo com 1/18 do tempo para treinamento de modelo.', 'ja': '私たちは、自動短答スコアリングに関するUKARA 1.0の課題に対する3つのソリューションを比較します。単一のクラシック、アンサンブルのクラシック、およびディープラーニングです。タスクは、正しいか間違っているかにかかわらず、2つの質問に対する回答を分類することです。最近の開発では、ベンチマークのパフォーマンスを推進するためにモデルの複雑さが増していますが、一般的な改善ではリソースが要求される傾向があります。UKARAタスクでは、単語の袋と古典的な機械学習アプローチが、2億語からの事前に訓練されたword 2 vec埋め込みのアンサンブルモデルとBi - LSTMモデルと競合することがわかりました。この場合、単一の古典的機械学習は、モデルトレーニングの1/18時間での深層学習アプローチと比較して、F 1で2%未満の差を達成した。', 'fr': "Nous comparons trois solutions au défi UKARA 1.0 sur la notation automatique à réponse courte\xa0: classique unique, classique d'ensemble et apprentissage profond. La tâche consiste à classer les réponses données à deux questions, qu'elles soient bonnes ou mauvaises. Bien que les développements récents montrent une complexité croissante des modèles pour améliorer les performances de référence, ils ont tendance à être exigeants en ressources avec des améliorations banales. Pour la tâche UKARA, nous avons découvert que les approches classiques de sac de mots et d'apprentissage automatique peuvent concurrencer les modèles d'ensemble et le modèle Bi-LSTM avec intégration word2vec pré-entraînée à partir de 200 millions de mots. Dans ce cas, l'apprentissage automatique classique unique a obtenu une différence de moins de 2\xa0% en F1 par rapport à l'approche de deep learning avec 1/18 de temps pour la formation sur modèle.", 'hi': 'हम स्वचालित लघु-उत्तर स्कोरिंग पर UKARA 1.0 चुनौती के लिए तीन समाधानों की तुलना करते हैं: एकल शास्त्रीय, पहनावा शास्त्रीय, और गहरी शिक्षा। कार्य दो प्रश्नों के दिए गए उत्तरों को वर्गीकृत करना है, चाहे वे सही हों या गलत। जबकि हाल के विकास से पता चलता है कि बेंचमार्क प्रदर्शन को धक्का देने के लिए मॉडल की जटिलता बढ़ रही है, वे सांसारिक सुधार के साथ संसाधन-मांग करते हैं। UKARA कार्य के लिए, हमने पाया कि बैग-ऑफ-वर्ड्स और शास्त्रीय मशीन लर्निंग दृष्टिकोण 200 मिलियन शब्दों से पूर्व-प्रशिक्षित वर्ड 2वेक एम्बेडिंग के साथ पहनावा मॉडल और द्वि-एलएसटीएम मॉडल के साथ प्रतिस्पर्धा कर सकते हैं। इस मामले में, एकल शास्त्रीय मशीन लर्निंग ने मॉडल प्रशिक्षण के लिए 1/18 समय के साथ गहरे सीखने के दृष्टिकोण की तुलना में F1 में 2% से कम अंतर हासिल किया।', 'zh': '三解决方案与UKARA 1.0挑战论自简答评分较之:单经典,合奏经深。 二者给定类也,无是非也。 虽近者之明,推基准性,模形复杂性增益,往往须资,并为凡改。 UKARA之务,见词袋与经典机器学与成模争Bi-LSTM,有豫练之word2vec嵌,以自2亿个单词也。 若此者,比于深度,单经机器学于F1,不及2%之异,其训日为1/18。', 'ru': 'Мы сравниваем три решения задачи UKARA 1.0 по автоматизированному подсчету коротких ответов: одиночное классическое, ансамблевое классическое и глубокое обучение. Задача состоит в том, чтобы классифицировать данные ответы на два вопроса, являются ли они правильными или неправильными. Несмотря на то, что последние разработки показывают, что модель становится все более сложной, чтобы подтолкнуть показатели эталонных показателей, они, как правило, требуют значительных ресурсов при обыденном улучшении. Для задачи UKARA мы обнаружили, что Bag-of-words и классические подходы к машинному обучению могут конкурировать с моделями ансамблей и моделью Bi-LSTM с предварительно обученным word2vec встраиванием из 200 миллионов слов. В этом случае единичное классическое машинное обучение достигло менее 2% разницы в F1 по сравнению с подходом глубокого обучения с 1/18 времени для обучения модели.', 'ga': 'Déanaimid comparáid idir trí réiteach ar dhúshlán UKARA 1.0 ar scóráil ghearrfhreagraí uathoibrithe: clasaiceach aonair, ensemble clasaiceach, agus foghlaim dhomhain. Is é an tasc ná freagraí tugtha ar dhá cheist a rangú, cibé an bhfuil siad ceart nó mícheart. Cé go léiríonn forbairt le déanaí castacht na samhla atá ag dul i méid chun na feidhmíochtaí tagarmharcála a bhrú, is gnách go mbíonn siad ag éileamh acmhainní agus feabhsuithe gan staonadh. Maidir le tasc UKARA, fuaireamar amach gur féidir le málaí de na focail agus cineálacha cur chuige foghlama meaisín clasaiceacha dul san iomaíocht le samhlacha ensemble agus samhail Bi-LSTM le leabú word2vec réamh-oilte ó 200 milliún focal. Sa chás seo, bhain an fhoghlaim meaisín clasaiceach aonair difríocht níos lú ná 2% i F1 i gcomparáid leis an gcur chuige foghlama domhain le 1/18 am le haghaidh oiliúna múnla.', 'hu': 'Három megoldást hasonlítunk össze az UKARA 1.0 kihíváshoz az automatizált rövid válaszok pontozásával: egyetlen klasszikus, együttes klasszikus és mélytanulás. A feladat két kérdésre adott válasz osztályozása, hogy helyesek-e vagy rosszak. Míg a közelmúltbeli fejlesztések növekvő modellbonyolultságot mutatnak a benchmark teljesítményének előmozdítása érdekében, ezek általában erőforrásokat igényelnek, hétköznapi javulással. Az UKARA feladathoz megállapítottuk, hogy a szavak zsákja és a klasszikus gépi tanulási megközelítések versenyezhetnek az együttműködési modellekkel és a Bi-LSTM modellekkel, amelyek előre képzett word2vec beágyazásával 200 millió szóból állnak. Ebben az esetben az egyetlen klasszikus gépi tanulás kevesebb mint 2%-os különbséget ért el az F1-ben a mélytanulási megközelítéshez képest, a modellképzés 1/18 idejével.', 'el': 'Συγκρίνουμε τρεις λύσεις για την πρόκληση του στην αυτοματοποιημένη βαθμολογία σύντομων απαντήσεων: ενιαία κλασική, κλασική ομάδα και βαθιά μάθηση. Το καθήκον είναι να ταξινομηθούν οι απαντήσεις σε δύο ερωτήσεις, είτε είναι σωστές είτε λάθος. Ενώ η πρόσφατη ανάπτυξη δείχνει αυξανόμενη πολυπλοκότητα μοντέλων για να ωθήσει τις επιδόσεις αναφοράς, τείνουν να είναι απαιτητικές σε πόρους με συνηθισμένη βελτίωση. Για την εργασία του βρήκαμε ότι η σακούλα λέξεων και οι κλασικές προσεγγίσεις μηχανικής μάθησης μπορούν να ανταγωνιστούν τα μοντέλα συνόλων και το μοντέλο με προ-εκπαιδευμένη ενσωμάτωση από 200 εκατομμυρίων λέξεων. Στην περίπτωση αυτή, η ενιαία κλασική μηχανική μάθηση πέτυχε μικρότερη από 2% διαφορά στο F1 σε σύγκριση με την προσέγγιση βαθιάς μάθησης με χρόνο 1/18 για εκπαίδευση μοντέλου.', 'ka': 'ჩვენ სამი გარეშე UKARA 1.0 გამოსახულებლად ავტომატურებული კონტაქტიური პასუხის შესახებ: ერთი კლასიკალური, კლასიკალური და ძალიან სწავლებელი შესახებ. პარამეტრები არის, რომ ორი კითხვაზე მიღებული პასუხი კლასიფიკაცია, თუ არა მარტივია თუ არა მარტივია. მაშინ ახალი განვითარება ჩვენებს მოდელური კომპლექსიტება, რომელიც ბანქმენტური გამოსახულებების გარჩენა, ისინი უნდა იყოს რესურსების მოსახულებელი მსოფლი UKARA პარამეტრებისთვის, ჩვენ აღმოჩნეთ, რომ სიტყვების და კლასიკური მაქანის სწავლების მიღება შეიძლება კონტებური მოდელთან და Bi-LSTM მოდელთან 200 მილიონის სიტყვებისთვის უნდა კონტებუროთ. ამ შემთხვევაში, ერთი კლასიკური მანქანის სწავლება F1-ში უფრო ნაკლები 2% განსხვავებულია, როგორც 1/18 დროს მოდელური სწავლების განსხვავებას შედგენა.', 'it': "Confrontiamo tre soluzioni alla sfida UKARA 1.0 sul punteggio automatico delle risposte brevi: single classic, ensemble classic e deep learning. Il compito è quello di classificare le risposte date a due domande, che siano giuste o sbagliate. Mentre i recenti sviluppi mostrano una crescente complessità del modello per spingere le prestazioni di riferimento, tendono a richiedere risorse con miglioramenti banali. Per il compito UKARA, abbiamo scoperto che il sacco di parole e gli approcci classici di apprendimento automatico possono competere con i modelli ensemble e il modello Bi-LSTM con l'incorporazione di word2vec pre-addestrata da 200 milioni di parole. In questo caso, il singolo machine learning classico ha ottenuto meno del 2% di differenza in F1 rispetto all'approccio deep learning con 1/18 tempo per l'allenamento modello.", 'lt': 'Palyginame tris sprendimus su UKARA 1.0 iššūkiu dėl automatinio trumpalaikio atsakymo vertinimo: vienintelis klasikinis, klasikinis ensemble ir gilus mokymasis. Užduotis – klasifikuoti pateiktus atsakymus į du klausimus, nesvarbu, ar jie yra teisingi ar neteisingi. Nors pastarojo meto vystymasis rodo, kad modelis tampa vis sudėtingesnis siekiant nustatyti lyginamuosius rodiklius, jie paprastai reikalauja išteklių ir paprastai tobulina. UKARA užduoties atžvilgiu nustatėme, kad žodžių maišelio ir klasikinio mašininio mokymosi metodai gali konkuruoti su ensemble modeliais ir Bi-LSTM modeliu su iš anksto apmokytais žodžiais 2vec, įdedančiais iš 200 milijonų žodžių. Tokiu atveju vieno klasikinio mašininio mokymosi metu F1 skirtumas buvo mažesnis kaip 2 %, palyginti su giliavandenio mokymosi metodu, o modelio mokymui buvo skirtas 1/18 laiko.', 'mk': 'Ние споредуваме три решенија со предизвикот УКАРА 1.0 за автоматизирано скоро одговорно оценување: еден класичен, класичен ансамбл и длабоко учење. The task is to classify given answers to two questions, whether they are right or wrong.  И покрај тоа што неодамнешниот развој покажува зголемена комплексност на моделот за поттикнување на референтните резултати, тие обично бараат ресурси со обично подобрување. За задачата на УКАРА, откривме дека пристапите со вреќа на зборови и класично машинско учење можат да се натпреваруваат со ансембл модели и Би-LSTM модел со претренирани зборови 2vec вклучени од 200 милиони зборови. Во овој случај, единственото класично машинско учење постигна помалку од 2 отсто разлика во Ф1 во споредба со пристапот на длабоко учење со 1/18 време за моделна обука.', 'kk': 'Біз UKARA 1. 0- ге автоматты қысқа жауап сұрауының үш шешімін салыстырып тұрамыз: бір классикалық, классикалық және түсінікті оқыту. Тапсырма - олар дұрыс немесе дұрыс емес екі сұраққа берілген жауаптарды бірлеу. Жуырдағы жасау үлгісінің көпшілігін бақылау үшін үлгісінің көпшілігін көрсетеді. Олар көпшілікті жасау үшін ресурстарды талап етеді. UKARA тапсырмасы үшін біз 200 миллион сөзден ендірілген сөздер мен классикалық машинаның оқыту жағдайларына көмектеседі. Бұл жағдайда, бір классикалық машинаны оқыту үшін үлгі оқыту үшін 1/18 уақытта, F1 дегенде 2% артық айырмашылығымен қалыпты.', 'ms': 'Kita membandingkan tiga penyelesaian dengan cabaran UKARA 1.0 pada skor jawapan pendek automatik: klasik tunggal, ensemble klasik, dan belajar dalam. Tugas adalah untuk mengklasifikasikan jawapan yang diberikan kepada dua soalan, sama ada ia betul atau salah. Sementara pembangunan baru-baru ini menunjukkan peningkatan kompleksiti model untuk mendorong prestasi tanda referensi, mereka cenderung untuk memerlukan sumber dengan peningkatan biasa. Untuk tugas UKARA, kami mendapati bahawa beg-of-words dan pendekatan pembelajaran mesin klasik boleh bersaing dengan model ensemble dan model Bi-LSTM dengan word2vec terlatih terlibat dari 200 juta kata. Dalam kes ini, pembelajaran mesin klasik tunggal mencapai kurang dari 2% perbezaan dalam F1 dibandingkan dengan pendekatan pembelajaran dalam dengan 1/18 masa untuk latihan model.', 'ml': 'We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning.  രണ്ടു ചോദ്യങ്ങള്\u200dക്ക് ഉത്തരം വിശദീകരിക്കാനുള്ള പ്രവര്\u200dത്തിയാണ്, അവയ്\u200d ശരിയാണോ തെറ്റാണോ എന്ന്. അടുത്തിടെ വികസിപ്പിക്കുമ്പോള്\u200d മോഡലിന്റെ കൂടുതല്\u200d സങ്കീര്\u200dണ്ണത കാണിക്കുന്നു. ബെന്\u200dച്മാര്\u200dക്ക് പ്രദര്\u200dശനങ്ങള്\u200d പ്രവര്\u200dത് യുക്കാരാ ജോലിക്ക് വേണ്ടി ഞങ്ങള്\u200d കണ്ടെത്തിയിരിക്കുന്നു ബാഗ് വാക്കുകളുടെയും ക്ലാസിക്കല്\u200d യന്ത്രങ്ങളുടെയും പഠിപ്പിന്റെയും സാഹചര്യങ്ങള്\u200dക്ക് മോഡല ഈ കാര്യത്തില്\u200d, ഒരു ക്ലാസിക്കല്\u200d യന്ത്രം പഠിക്കുന്നത് F1 ല്\u200d 2% വ്യത്യാസം കുറഞ്ഞതാണ്. മോഡല്\u200d പരിശീലിക്കുന്നതിന് 1/18 മണിക്', 'mt': 'Aħna nqabblu tliet soluzzjonijiet mal-isfida tal-UKARA 1.0 dwar il-punteġġ awtomatizzat b’risposta qasira: tagħlim klassiku wieħed, ensemble klassiku, u profond. Il-kompitu huwa li jiġu kklassifikati tweġibiet mogħtija għal żewġ mistoqsijiet, kemm jekk huma korretti kif ukoll jekk le. Filwaqt li żvilupp reċenti juri kumplessità dejjem tikber tal-mudell biex jimbutta l-prestazzjonijiet ta’ referenza, dawn għandhom it-tendenza li jkunu rikjesti r-riżorsi b’titjib globali. Għall-kompitu UKARA, sabna li l-approċċi tat-tagħlim bil-borża tal-kliem u tal-magni klassiċi jistgħu jikkompetu ma’ mudelli ta’ ensemble u mudell Bi-LSTM bil-kliem imħarreġ minn qabel 2vec li jinkorpora minn 200 miljun kelma. F’dan il-każ, it-tagħlim uniku tal-magni klassiċi kiseb inqas minn 2 % differenza f’F1 meta mqabbel mal-approċċ ta’ tagħlim profond b’1/18-il ħin għat-taħriġ mudell.', 'mn': 'Бид UKARA 1.0-тэй автоматжуулсан богино хариултын шаардлагатай гурван шийдэл харьцуулж байна: ганц классик, классик, гүн гүнзгий суралцах. Үүнийг зөв эсвэл буруу эсвэл хоёр асуултын хариултыг ангилах юм. Саяхан хөгжлийн байгууллага нь загварын төвөгтэй төвөгтэй байдлыг харуулж байгаа ч тэд дэлхийн сайжруулалтын тулд нөөц хэрэгтэй байдаг. UKARA даалгаварын тулд бид 200 сая үгээс аль сургалтын үг болон классик машины суралцах арга загвартай өрсөлдөж чадна. Энэ тохиолдолд нэг классик машины суралцах нь F1-н 2%-аас бага ялгаатай байлаа. Загварын суралцах нь 1/18 хугацаатай гүн суралцах аргыг харьцуулсан.', 'pl': 'Porównujemy trzy rozwiązania dla wyzwania UKARA 1.0 w zakresie automatycznego punktowania krótkich odpowiedzi: klasycznego pojedynczego, klasycznego zespołu i głębokiego uczenia. Zadaniem jest klasyfikacja udzielonych odpowiedzi na dwa pytania, czy są one słuszne czy złe. Podczas gdy ostatnie rozwoje pokazują rosnącą złożoność modeli, aby przyspieszyć wyniki referencyjne, zwykle są one wymagające zasobów i przyziemne ulepszenia. W przypadku zadania UKARA stwierdziliśmy, że worki-of-words i klasyczne metody uczenia maszynowego mogą konkurować z modelami zespołowymi i modelami Bi-LSTM z wstępnie wytrenowanym osadzeniem word2vec z 200 milionów słów. W tym przypadku pojedyncze klasyczne uczenie maszynowe osiągnęło mniej niż 2% różnicy w F1 w porównaniu z podejściem głębokiego uczenia z 1/18 czasu na szkolenie modelu.', 'no': 'Vi samanliknar tre løysing med UKARA 1,0 utfordring på automatisk kort svar: enkelt klassisk, ensembler klassisk og dypt læring. Oppgåva er å klassifisera oppgitte svar til to spørsmål, om dei er rette eller feil. Mens det siste utviklinga viser å auke modellekompleksitet for å trykke utviklingane på benchmarket, har dei vanskeleg å vera ressurskrev med mundane forbetringar. For UKARA-oppgåva fann vi at tilnærmingar med søk av ord og klassisk maskinelæring kan konkurrere med ensemble modeller og Bi-LSTM-modeller med føretrainert ord2vec innebygd frå 200 millioner ord. I dette tilfellet fikk den enkle klassiske maskinelæringa mindre enn 2 % forskjellen i F1 sammenlignet med den dype læringstilnærminga med 1/18 tid for modellelæring.', 'ro': 'Comparăm trei soluții la provocarea UKARA 1.0 privind scorarea automată a răspunsurilor scurte: clasic unic, clasic ansamblu și învățare profundă. Sarcina este de a clasifica răspunsurile date la două întrebări, fie că sunt corecte sau greșite. Deși dezvoltarea recentă arată o complexitate tot mai mare a modelului pentru a împinge performanțele de referință, acestea tind să fie solicitante de resurse, cu îmbunătățiri banale. Pentru sarcina UKARA, am descoperit că abordările clasice de învățare automată și sacul de cuvinte pot concura cu modelele de ansamblu și modelul Bi-LSTM cu încorporarea pre-antrenată word2vec de la 200 de milioane de cuvinte. În acest caz, învățarea automată clasică unică a atins mai puțin de 2% diferență în F1 comparativ cu abordarea de învățare profundă cu 1/18 timp pentru formarea modelelor.', 'sr': 'Uspoređujemo tri rješenja sa UKARA 1,0 izazova na automatski izvlačenje kratkog odgovora: jedno klasično, ensemblirajuće klasično i duboko učenje. Taj zadatak je da klasifikišemo odgovore na dva pitanja, jesu li u pravu ili pogrešnu. Iako nedavno razvoj pokazuje povećanje modelne kompleksnosti za pritiskanje standardnih izvora, oni su tendencija da zahtevaju resurse sa mundanom poboljšanjem. Za UKARA zadatak, pronašli smo da pristupi sa vrećima riječi i klasičnim mašinama mogu se natjecati sa modelima i model bi-LSTM sa predobučenim rečima 2vec koji se uključuje iz 200 miliona reči. U ovom slučaju, jedino klasično učenje mašine postiglo je manje od 2% razlike u F1 u usporedbi sa dubokim pristupom učenja sa 1/18 vremena za obuku modela.', 'sv': 'Vi jämför tre lösningar till UKARA 1.0 utmaningen på automatiserad kortvarig poäng: singel classic, ensemble classic och deep learning. Uppgiften är att klassificera givna svar på två frågor, vare sig de är rätt eller fel. Även om den senaste utvecklingen visar på ökande modellkomplexitet för att driva riktmärkets prestanda tenderar de att vara resurskrävande med vardagliga förbättringar. För UKARA-uppgiften fann vi att säck-of-words och klassiska maskininlärningsmetoder kan konkurrera med ensemblemodeller och Bi-LSTM-modell med förintränad word2vec inbäddning från 200 miljoner ord. I detta fall uppnådde klassisk maskininlärning mindre än 2% skillnad i F1 jämfört med djupinlärningsmetoden med 1/18 tid för modellträning.', 'si': 'අපි UKARA 1.0 එක්ක ප්\u200dරතිචාර ප්\u200dරතිචාර ප්\u200dරතිචාර ප්\u200dරතිචාර විශේෂ තුනක් සම්බන්ධ කරනවා: ස්වයංක්\u200dරීය ප්\u200dරතිච වැඩේ තමයි ප්\u200dරශ්න දෙකට ප්\u200dරතිච්චිත ප්\u200dරතිචාර දෙකට විවෘත කරන්න, ඔවුන් හරි නැත්තේ නැත්ත අලුත් විකාශය පෙන්වන්නේ බෙන්ච්මාර්ක් සැලසුම් සැලසුම් සැලසුම් විශාලනය වෙන්න, ඔවුන් පුළුවන් විශාල UKARA වැඩ සඳහා, අපි හොයාගත්තා බෑග් වචන් සහ ලාස්ක්\u200dරිය පද්ධතිය ඉගෙන ගන්න පුළුවන් අනිවාර්යාත්මක වචනය 200 මිලියන වචනයෙන් පුළුවන් වචනය සහ මේ විදියට, එකම ප්\u200dරමාණික පණිවිධාන පණිවිධාන පණිවිධාන පණිවිධානය F1 වලින් අනතුරු 2% වඩා අඩුවෙන් ඉග', 'so': 'Saddex xalal oo ah UKARA 1.0, waxaynu isbarbardhignaa qiimeynta kooban ee iskuullada kaliya iyo waxbarashada mool dheer. The task is to classify given answers to two questions, whether they are right or wrong.  Intii la soo dhowaaday horumarinta ayaa muuqata qallafsanaanshaha modellka si uu u dhaqaajiyo horumarinta bangmooyinka, waxay inta badan u baahan yihiin nolol-horumarinta. Shaqada UKARA, waxaynu helnay in qalabka lagu barto iyo qalabka iskuulka ah ay ka tartami karaan modellada ensemble iyo modelalka Bi-LSTM oo ku qoran hadalka hore 2vec oo ka soo baxa 200 million oo eray ah. Markaas waxbarashada kaliga ah ee F1 waxay gaadhay in ka yar 2% kala duwan, taasoo la barbartay habka waxbarashada ee moolka dheer ee u dhexeeya 1/18 waqti waxbarashada tusaale ahaan.', 'ta': 'We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning.  இரண்டு கேள்விகளுக்கு கொடுக்கப்பட்ட விடைகளை வகைப்படுத்த வேண்டும், சரியா அல்லது தவறா என்பது. சமீபத்தில் வளர்ச்சி மாதிரியின் சிக்கல்களை அதிகரிக்கும் பென்க்மார்க் செயல்களை அழுத்துவதற்கு காட்டும் போது, அவர்கள் நாள் மூலம்  UKARA செயலுக்காக, நாங்கள் கண்டுபிடித்த பைக்கு சொல்லும் கிளாசிக்கல் இயந்திரம் கற்றுக்கொள்ளும் முறைமையான மாதிரிகளுடனும் ஒதுக்கும் மாதிரிகளுடனு இப்படியில், ஒரே சிறந்த இயந்திரம் கற்றுக் கொள்ளும் மாதிரிய பயிற்சிக்கு 1/18 முறையை ஒப்பிட்டுக் கொண்டு F1 ல் 2% வித்தியா', 'ur': 'ہم نے UKARA 1.0 کے تین حل کو اٹوٹی چھوٹی جواب سکونگ کے بارے میں مقایسہ کیا ہے: ایک کلاسیک، کلاسیک، اور عمیق تعلیم کے بارے میں۔ یہ کام ہے کہ دو سؤال کے جواب دو فریقوں کو تقسیم کرنا چاہے یہ درست ہیں یا غلط ہیں اچھی طرح کی توسعہ نمادل کی پیچیدگی بڑھتی ہے کہ بنچمبارک نمائندوں کو دھوپ دینے کے لئے، وہ دنیاوی توسعہ کے ذریعہ منصفات کے ساتھ منصفات کے سوال کرنے والے ہیں. UKARA کے کام کے لئے ہم نے دیکھا کہ بیسِمبل موڈل اور بیس\u200cٹم موڈل سے پہلے تدریس کی کلیسی کلیسی کلیسی کلیسی کلیسی کلیسی مشین کی آموزش کی تقریبیں 200 میلیون کلیزوں سے پیدا ہوئی کلیزوں کے ساتھ مسابقه کرسکتی ہیں۔ اس کے مطابق، ایک کلاسیک ماشین کی تعلیم F1 میں 2% سے کم فائدہ پہنچا گیا تھا، 1/18 موقع کے مطابق مدل تعلیم کے لئے عمیق تعلیم کے مطابق۔', 'uz': "Biz UKARA 1.0 muammolariga avtomatik qisqa javob qiymatiga uchta muammolarni kamaytamiz: bitta klassik, ensemblik klassik va juda qisqa o'rganish. Bu vazifa ikkita savollar javoblarini koʻrsatish, ularning to ʻgʻri yoki notoʻgʻri. Yaqinda taʼminlovchi taʼminlovchi modelni foydalanish uchun murakkablikni ko'rsatadi. Ular davom etishni davom etishni xohlaydi. UKARA vazifasi uchun biz bir so'zlar va klassik mashinalar o'rganish usullarini topdik, uning modellari va Bi-LSTM modeli bilan bir marta o'rganilgan so'z 2 million so'zlaridan boshlanadi. Bunday holatda, bir klassik mashinasi o'rganishni F1'da 2% dan ortiq o'zgarishga o'rganish muvaffaqiyatlaridan 1/18 marta o'rganadi.", 'vi': 'Chúng tôi so sánh ba giải với thách thức UHRA 1.0 bằng cách đánh giá ngắn gọn tự động: đơn giản nhạc cổ điển, đơn đủ nhạc và học sâu. Nhiệm vụ là phân loại các câu trả lời cho hai câu hỏi, cho dù chúng đúng hay sai. Mặc dù phát triển gần đây cho thấy mô hình phức tạp hơn để thúc đẩy khả năng tiêu chuẩn, nhưng chúng có xu hướng đòi hỏi tài nguyên bằng cải tiến thông thường. Với nhiệm vụ UHRA, chúng tôi phát hiện ra rằng các phương pháp học tập về máy cổ điển và các phương pháp học cổ điển có thể cạnh tranh với các mô hình chung kết và mô hình Bi-LSTM với từ một triệu từ được đào tạo sẵn. Trong trường hợp này, việc học cỗ máy đơn thuần cổ điển đạt được ít hơn 2+ khác biệt trong dạng F1 so với cách tiếp cận học sâu hơn 1/18 thời gian để đào tạo mô hình.', 'da': 'Vi sammenligner tre løsninger til UKARA 1.0 udfordring på automatiseret korte svar scoring: single classic, ensemble classic og deep learning. Opgaven er at klassificere givet svar på to spørgsmål, om de er rigtige eller forkerte. Mens den seneste udvikling viser stigende modelkompleksitet for at skubbe benchmarks præstationer, har de tendens til at være ressourcekrævende med verdslige forbedringer. Til UKARA-opgaven fandt vi, at taske-of-words og klassiske maskinlæringsmetoder kan konkurrere med ensemble modeller og Bi-LSTM model med prætrænet word2vec integrering fra 200 millioner ord. I dette tilfælde opnåede den enkelte klassiske maskinlæring mindre end 2% forskel i F1 sammenlignet med deep learning-tilgangen med 1/18 tid til modeltræning.', 'bg': 'Сравняваме три решения на предизвикателството за автоматизирано класиране на кратки отговори: единичен класически, ансамбъл класически и дълбоко обучение. Задачата е да се класифицират дадените отговори на два въпроса, дали са прави или грешни. Докато неотдавнашното развитие показва нарастваща сложност на модела, за да тласне показателите на референтните показатели, те са склонни да изискват ресурси с ежедневно подобрение. За задачата установихме, че торбичката с думи и класическите подходи за машинно обучение могат да се конкурират с ансамбълните модели и модела с предварително обучено вграждане от 200 милиона думи. В този случай еднократното класическо машинно обучение постига по-малко от 2% разлика във Формула в сравнение с подхода за дълбоко обучение с 1/18 време за обучение по модел.', 'hr': 'Uspoređujemo tri rješenja s UKARA 1,0 izazova na automatski izvlačenje kratkog odgovora: jedno klasično, ensemble klasično i duboko učenje. Taj zadatak je klasifikacija određenih odgovora na dva pitanja, jesu li u pravu ili pogrešnu. Iako nedavno razvoj pokazuje povećanje modelne kompleksnosti za poticanje rezultata, to je tendencija da zahtijevaju resurse sa svijetskim poboljšanjem. Za UKARA zadatak, pronašli smo da pristupi učenja torbe riječi i klasične strojeve mogu se natjecati sa modelima ensemble i model Bi-LSTM sa predobučenim rečima 2vec uključujući od 200 milijuna riječi. U tom slučaju, jedino klasično učenje stroja postiglo je manje od 2% razlike u F1 u usporedbi s dubokim pristupom učenja s 1/18 vremena za obuku modela.', 'nl': 'We vergelijken drie oplossingen voor UKARA 1.0 uitdaging op geautomatiseerde short-respons scoren: single classic, ensemble classic en deep learning. De taak is om gegeven antwoorden op twee vragen te classificeren, of ze nu juist of fout zijn. Hoewel recente ontwikkelingen laten zien dat de complexiteit van het model toeneemt om de benchmark prestaties te verhogen, zijn ze veeleisend met alledaagse verbeteringen. Voor de UKARA-taak vonden we dat bag-of-words en klassieke machine learning benaderingen kunnen concurreren met ensemble modellen en Bi-LSTM model met voorgetrainde word2vec embedded van 200 miljoen woorden. In dit geval bereikte de enkelvoudige klassieke machine learning minder dan 2% verschil in F1 in vergelijking met de deep learning benadering met 1/18 tijd voor modeltraining.', 'de': 'Wir vergleichen drei Lösungen zur UKARA 1.0-Herausforderung für automatisierte Kurzantwort-Scoring: Einzelklassik, Ensembleklassik und Deep Learning. Die Aufgabe besteht darin, Antworten auf zwei Fragen zu klassifizieren, ob sie richtig oder falsch sind. Während die jüngste Entwicklung eine zunehmende Modellkomplexität zeigt, um die Benchmark-Performance zu steigern, sind sie in der Regel ressourcenaufwendig mit alltäglichen Verbesserungen. Für die UKARA-Aufgabe fanden wir heraus, dass Bag-of-Words und klassische maschinelle Lernansätze mit Ensemble-Modellen und Bi-LSTM-Modellen mit vortrainierter word2vec Einbettung von 200 Millionen Wörtern konkurrieren können. In diesem Fall erzielte das klassische maschinelle Lernen weniger als 2% Differenz in F1 im Vergleich zum Deep Learning Ansatz mit 1/18 Zeit für Modelltraining.', 'id': 'Kita membandingkan tiga solusi dengan tantangan UKARA 1.0 pada skor jawaban pendek otomatis: satu klasik, ensemble klasik, dan belajar dalam. Tugas adalah untuk mengklasifikasi jawaban yang diberikan kepada dua pertanyaan, apakah mereka benar atau salah. Sementara perkembangan baru-baru ini menunjukkan peningkatan kompleksitas model untuk mendorong prestasi benchmark, mereka cenderung membutuhkan sumber daya dengan peningkatan dunia. Untuk tugas UKARA, kami menemukan bahwa kantong-kata dan pendekatan belajar mesin klasik dapat berkompetisi dengan model ensemble dan model Bi-LSTM dengan word2vec terlatih terlibat dari 200 juta kata. Dalam kasus ini, pembelajaran mesin klasik tunggal mencapai kurang dari 2% perbedaan dalam F1 dibandingkan pendekatan belajar dalam dengan 1/18 waktu untuk pelatihan model.', 'ko': 'UKARA 1.0 과제에 대한 자동 간단한 답안 평가 솔루션인 단일 고전, 전체 고전, 딥러닝 세 가지를 비교했습니다.임무는 두 문제의 주어진 답안을 분류하는 것이다. 그것이 옳든 틀리든.최근의 발전에 따르면 기준 성능을 향상시키기 위해 모델의 복잡성이 끊임없이 증가하고 있지만 이들은 대량의 자원을 필요로 하고 평범한 개선이 필요하다.UKARA 임무에 대해 우리는 단어 패키지와 클래식 머신 학습 방법이 통합 모델과 Bi LSTM 모델과 경쟁할 수 있음을 발견했다. 그 중에서 미리 훈련된 단어 2VEC의 삽입량은 2억 단어이다.이런 상황에서 딥러닝 방법에 비해 단일 클래식 머신러닝은 F1에서의 차이가 2%도 안 되고 딥러닝 방법의 모형 훈련 시간은 1/18이다.', 'fa': 'ما سه راه حل با چالش UKARA ۱.۰ در مورد آزمایش کوتاه جواب خودکار مقایسه می کنیم: یک کلاسیک، کلاسیک و یادگیری عمیق را جمع می کنیم. وظیفه این است که پاسخ داده شده را به دو سؤال تقسیم کنیم، چه درست باشند یا اشتباه باشند. در حالی که توسعه اخیرا به افزایش پیچیدگی مدل برای فشار اجرای صندوق نشان می دهد، آنها عادت دارند که با توسعه جهانی به منابع نیاز دارند. برای وظیفه UKARA، ما پیدا کردیم که دسترسی یادگیری کیف کلاسیک و ماشین کلاسیک می\u200cتوانند با مدل\u200cهای ابزار و مدل Bi-LSTM مسابقه کنند با کلمه\u200cهای ابزار آموزش ۲vek که از ۲۰۰ میلیون کلمه استفاده می\u200cکنند. در این مورد، یادگیری یک ماشین کلاسیک کمتر از دو درصد تفاوت در F1 در مقایسه با روش عمیق یادگیری با زمان ۱/۸ برای آموزش مدل رسید.', 'sw': 'We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning.  Kazi ni kuelezea majibu yaliyotolewa kwa maswali mawili, ikiwa ni sahihi au sahihi. Wakati maendeleo ya hivi karibuni yanaonyesha kuongezeka kwa mfumo wa mifano ili kushinikiza maonyesho ya bendera, wanachukua kuwa rasilimali zinazohitaji kuboreshwa kwa kila siku. Kwa jukumu la UKARA, tuligundua kuwa mbinu za kujifunza kwa maneno na mashine ya klassi zinaweza kushindana na mifano ya mfumo na mtindo wa Bi-LSTM mwenye maneno ya awali yaliyofundishwa kutoka kwa maneno milioni 200. Katika hali hii, mashine moja ya kujifunza kwa klassi ilifanikiwa kuwa tofauti zaidi ya asilimia 2 nchini F1 ukilinganisha na mbinu za kujifunza za kina na wakati 1/18 kwa ajili ya mafunzo ya modeli.', 'tr': "Biz UKARA 1 Bu zada berilen jogablary iki soraga sapakla힊dyrmak, dogry 첵a-da 첵al흫y힊 bolsa흫yz. 횦akyndaky 철s체힊im seb채bi benchmark eserlerini guramak 체챌in 철r채n kompleksitede k철pr채k g철rkezil첵채ndir. Olar d체n첵채 geli힊meler bilen resurslar isle첵채ndir. UKARA i힊i 체챌in, 200 milyon s철zden 철wrenmeli 챌철p-s철z we klasik enjamy흫 gola첵lary bilen 첵ary힊yp bil첵채r. Bu 첵agda첵da, F1'de 첵ekeje klasik ma힊yny 철wrenmek 체챌in 1/18 gezek nusga 철wrenmek 체챌in 2%-den az 철첵tge힊ik tapdy.", 'sq': 'Ne krahasojmë tre zgjidhje me sfidën UKARA 1.0 në pikëpamjen automatike të përgjigjeve të shkurtra: klasike të vetme, klasike ensemble dhe mësim të thellë. Detyra është të klasifikojmë përgjigjet e dhëna në dy pyetje, nëse janë të drejta apo të gabuara. Ndërsa zhvillimi i kohëve të fundit tregon rritjen e kompleksitetit të modelit për të shtyrë paraqitjet referenciale, ato tenderojnë të kërkojnë burime me përmirësim të zakonshëm. Për detyrën e UKARA, gjetëm se metodat klasike të mësimit të makinave mund të konkurrojnë me modelet e ensembleve dhe modelin Bi-LSTM me fjalët e paratrajnuara 2vec të përfshira nga 200 milion fjalë. Në këtë rast, mësimi i vetëm klasik i makinës arriti më pak se 2% diferencë në F1 krahasuar me qasjen e mësimit të thellë me 1/18 kohë për trajnimin e modelit.', 'af': 'Ons vergelyk drie oplossing met UKARA 1. 0 uitdrukking op outomatiese kort antwoord skoring: enkele klassieke, ensembleer klassieke en diep leer. Die taak is om gegewe antwoorde na twee vrae te klassifiseer, of hulle reg of verkeerd is. Alhoewel onlangse ontwikkeling wys die vergroot model kompleksiteit om die benchmarkuitvoerings te druk, het hulle gewoonlik hulpbron-vervraag met munde verbetering. Vir die UKARA taak, het ons gevind dat sak van woorde en klassieke masjien leer toegang kan kompeteer met ensemble modele en Bi-LSTM model met voor-onderwerp woorde 2vec ingesluit van 200 miljoen woorde. In hierdie geval het die enkele klassieke masjien leer minder as 2% verskil in F1 vergelyk met die diep leer toegang met 1/18 tyd vir model oefening.', 'az': "Üç çözüm UKARA 1.0 ilə avtomatik qısa cevap scoring üzərində qarşılaşdırırıq: tək klasik, klasik və derin öyrənmək. Hesab iki suala verilən cavabları, doğru ya da yanlış tərzdə tərzdə ayrılmaqdır. Yaxınlıq təhsil etmək üçün modellərin kompleksitəsini artırmağını göstərən kimi, onlar dünyanın təhsil edilməsi ilə qüvvətli təhsil istəyirlər. UKARA işləri üçün, 200 milyon sözlərdən əvvəl təhsil edilmiş sözlər və klasik maşın öyrənməsi yaxınlıqları ilə müraciət edə bilər. Bu vəziyyətdə, tək klasik maşın öyrənməsi F1'də 2%-dən az fərqli oldu, model öyrənməsi üçün 1/18 vaxtı ilə dərin öyrənmə metodları ilə.", 'hy': 'Մենք համեմատում ենք UCARA1.0 մարտահրավերի երեք լուծումներ ավտոմատիկ կարճ պատասխանների գնահատման հետ. միակ դասական, համակարգչային դասական և խորը ուսումնասիրություն: Պատասխանը երկու հարցերին պատասխաններ դասակարգելն է, արդյոք դրանք ճիշտ են, թե սխալ են: Մինչդեռ վերջին զարգացումը ցույց է տալիս, որ մոդելների բարդությունը աճում է համեմատական արտադրողությունների վրա, նրանք հակված են ռեսուրսներ պահանջող լինել աշխարհային բարելավման հետ: For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words.  Այս դեպքում միակ դասական մեքենայի ուսումնասիրությունը F1-ում 2 տոկոսից քիչ տարբերություն հասավ, համեմատած խորը ուսումնասիրության մոտեցումներին մոդելների ուսումնասիրության համար 1:18 ժամանակով:', 'am': 'ከ.አ.0 ጋር ሦስት መፍትሄቶችን በአካባቢው አቋራጭ መልስ ሳጥን እናሳያታለን፡፡ አንዲት ክላሲካዊ፣ አንድ ክላሲካዊ እና ጥልቅ ትምህርት ነው፡፡ ስራው እውነተኛ ወይም ስህተት መሆኑን ለሁለት ጥያቄዎች መልስ መግለጫ ነው፡፡ የአሁኑ ግንኙነት አካባቢው የbenchmark ፍሬዎችን ለመጨመር የሚጨመር ሞዴል አካባቢነት ሲያሳየው፣ የዕድሜ ክፍተት ማድረግ ሲጠይቁት ነው፡፡ For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words.  በዚህም ጉዳይ፣ አንድ ክላሲካዊ መሣሪያ ትምህርት በሞዴል ትምህርት ላይ 1/18 ጊዜ ለመግለጽ ከ2% የሚበልጥ ልዩነት አግኝቷል፡፡', 'ca': "Comparem tres solucions amb el repte UKARA 1.0 en la puntuació de curtes respostes automatitzada: un únic aprenentatge clàssic, clàssic d'ensemble i profund. La tasca és classificar les respostes dadas a dues preguntes, siguin correctes o equivocades. Mentre el desenvolupament recent mostra una complexitat creixent del model per impulsar les performances de referència, tendeixen a exigir recursos amb millora mundana. Per a la tasca UKARA, vam descobrir que els enfocaments d'aprenentatge clàssic de paraules i màquines poden competir amb models d'ensemble i Bi-LSTM amb paraules pré-entrenats incorporant-se a partir de 200 milions de paraules. En aquest cas, l'aprenentatge clàssic de màquines va aconseguir menys del 2% de diferència en F1 en comparació amb l'enfocament d'aprenentatge profund amb 1/18 de temps per a l'entrenament model.", 'bs': 'Uspoređujemo tri rješenja sa UKARA 1,0 izazova na automatski izvlačenje kratkog odgovora: jedno klasično, ensemble klasično i duboko učenje. Taj zadatak je klasifikacija određenih odgovora na dva pitanja, jesu li u pravu ili pogrešnu. Iako nedavno razvoj pokazuje povećanje modelne kompleksnosti za pritiskanje rezultata, to je tendencija da zahtijevaju resurse sa mundanom poboljšanjem. Za UKARA zadatak, pronašli smo da pristupi sa vrećima riječi i klasičnim strojevima mogu se natjecati sa modelima zaštite i model bi-LSTM sa predobučenim rečima 2vec koji se uključuje iz 200 miliona riječi. U ovom slučaju, jedino klasično učenje mašine postiglo je manje od 2% razlike u F1 u usporedbi sa dubokim pristupom učenja s 1/18 vremena za obuku modela.', 'cs': 'Porovnáváme tři řešení pro výzvu UKARA 1.0 v automatizovaném skórování krátkých odpovědí: jednotlivá klasika, souborová klasika a hluboké učení. Úkolem je klasifikovat odpovědi na dvě otázky, zda jsou správné nebo špatné. Zatímco nedávný vývoj ukazuje zvyšující se složitost modelu, aby posunul výkonnost referenčního měřítka, mají tendenci být náročné na zdroje s běžným zlepšením. Pro úkol UKARA jsme zjistili, že taška slov a klasické přístupy strojového učení mohou konkurovat souborovým modelům a Bi-LSTM modelům s předškoleným vložením Word2vec z 200 milionů slov. V tomto případě dosáhlo klasického strojového učení menšího než 2% rozdílu v F1 ve srovnání s přístupem hlubokého učení s 1/18 časem pro trénink modelu.', 'bn': 'আমরা স্বয়ংক্রিয়ভাবে সংক্ষিপ্ত উত্তরের সংক্রান্ত সংক্রান্ত সংক্রান্ত সংক্রান্ত সংক্রান্ত সংক্রান্ত চ্যালেঞ্জের স এই কাজ হচ্ছে দুই প্রশ্নের উত্তর বিশ্লেষণ করা, তারা সঠিক না ভুল কিনা। সাম্প্রতিক উন্নয়নগুলোতে মডেলের বেঞ্চমার্কের অনুষ্ঠান চালানোর জন্য মডেলের জটিলতা বৃদ্ধি প্রদর্শন করে, তারা মানব উন্নয়নের ইউকারারা কাজের জন্য আমরা পেয়েছি যে ব্যাগ-অফ-শব্দ এবং ক্লাসিক্যাল মেশিনের শিক্ষা পদক্ষেপের ক্ষেত্রে প্রতিযোগিতা করতে পারি এক্সপেল মডেল এবং বি এলস্ In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.', 'et': 'Võrdleme kolme lahendust UKARA 1.0 väljakutsele automatiseeritud lühikeste vastuste hindamisel: üks klassikaline, ansambli klassikaline ja sügavõpe. Ülesanne on klassifitseerida antud vastused kahele küsimusele, kas need on õiged või valed. Kuigi hiljutine areng näitab mudeli üha keerukamaks muutumist võrdlusnäitajate saavutamiseks, kipuvad need olema ressursinõudlikud ja igapäevaselt täiustatud. UKARA ülesande puhul leidsime, et sõnakott ja klassikalised masinõppe lähenemisviisid võivad konkureerida ansambli mudelitega ja Bi-LSTM mudeliga, kus eelnevalt koolitatud word2vec põimib 200 miljonit sõna. Sellisel juhul saavutas klassikaline masinõpe F1 erinevuse vähem kui 2% võrreldes sügavõppe lähenemisviisiga 1/18 ajaga mudeli koolituseks.', 'fi': 'Vertaamme UKARA 1.0 -haasteeseen kolmea ratkaisua automaattisessa lyhyen vastauksen pisteytyksessä: yksinkertainen klassinen, ryhmäklassinen ja syväoppiminen. Tehtävänä on luokitella annetut vastaukset kahteen kysymykseen, olivatpa ne oikeassa vai väärässä. Vaikka viimeaikainen kehitys osoittaa, että mallien monimutkaistuminen nostaa vertailutuloksia, ne ovat yleensä resursseja vaativia ja arkipäiväisiä parannuksia. UKARA-tehtävässä havaittiin, että sanapussi ja klassiset koneoppimismenetelmät voivat kilpailla ensemblemallien ja Bi-LSTM-mallin kanssa, jossa on valmiiksi koulutettu word2vec upotettu 200 miljoonasta sanasta. Tässä tapauksessa yksittäinen klassinen koneoppiminen saavutti alle 2%:n eron F1:ssä verrattuna syväoppimiseen 1/18-ajalla malliharjoitteluun.', 'jv': 'Awak dhéwé nambah telu hasar karo UKARI 1.0 desambe nang otomatik bukal gampang task iku kelompok responsa sing wis diputuhke gambaran iki, sapa sira diapakan apa ora diapakan. Alpha ain Nang kuwi, sing sekang kelas kuwi, ingkang sampeyan kelas telas nang 2% ulih dumadhi kanggo didasara supaya karo hal-ingkang sampeyan ingkang 1/18.', 'ha': "Tuna samfani masu suluɗi uku zuwa UKERA 1.0 na kanana a kan karatun kure na farat-jibar: kima guda mai classical, wanda ya samu'a cikin fasalin, da sanar da masu ƙari. The task is to classify given answers to two questions, whether they are right or wrong.  A lokacin da ake nuna wa ƙara masu kamfata misãlin da za'a damƙara saurin fanikin bangon-bangon, zasu zaman su zama suna tambayar resource-daidai da kyautatawa a danne. Kayyan aikin UKERA, mun sãmi musamman-masu maganar da masu karanta masu fasalin ayuka na fasalin ayuka, za'a iya yi ta yin jihãdi da misãlai na embemboli da misalin Bi-LSM da sauran da aka sanar da shi a gaba ɗaya, 2ve ke cikin magana 200 millions. A cikin wannan, ma'abũcin kwamfyuta mai kwamfyuta na karatun ya kai ƙari daga %2 a cikin F1 sammenfanin maganar ta ƙari da amfani da lokaci 1/18 wa shirin motsi.", 'sk': 'Primerjamo tri rešitve z izzivom UKARA 1.0 pri avtomatiziranem ocenjevanju kratkih odgovorov: enojno klasično, ansambelno klasično in globoko učenje. Naloga je razvrstiti odgovore na dve vprašanji, ne glede na to, ali sta prav ali napačna. Medtem ko je nedavni razvoj pokazal naraščajočo kompleksnost modelov, da bi spodbudil uspešnost referenčnih rezultatov, so ti običajno potrebni viri z vsakdanjimi izboljšavami. Za nalogo UKARA smo ugotovili, da lahko vreča besed in klasični pristopi strojnega učenja tekmujejo z modeli ansamblov in Bi-LSTM modelom z vnaprej usposobljenim word2vec vdelavo iz 200 milijonov besed. V tem primeru je posamezno klasično strojno učenje doseglo manj kot 2% razlike v F1 v primerjavi s pristopom globokega učenja z 1/18 časa za usposabljanje modela.', 'he': 'אנחנו משוותים שלושה פתרונות לאתגר UKARA 1.0 על נקודת תשובה קצרה אוטומטית: קלאסית אחת, אנזמבל קלאסית, ולמדה עמוקה. המשימה היא לקlassifika תשובות נתנות לשני שאלות, בין אם הן צודקות או לא. בעוד התפתחות האחרונות מראות מורכבות מודל גדולה כדי לדחוף את ההופעות של המרמז, הם נוטים להיות דורשים משאבים עם שיפור עולמי. עבור משימה UKARA, מצאנו ששקית של מילים והגישות ללמוד מכונות קלאסיות יכולות להתחרות עם דוגמנים אנסמבל ובי-LSTM עם מילים 2vec מאומנות מראש ממילים 200 מיליון. במקרה הזה, למידת מכונות קלאסית אחת השיגה פחות מ-2% הבדל ב-F1 בהשוואה לגישה למידה עמוקה עם 1/18 זמן לאימון מודל.', 'bo': 'We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. བྱ་འགུལ་ནི་རྒྱབ་སྐྱོར་གཉིས་ཀྱིས་དམིགས་འཛུགས་བྱས་པ་ལས་གནད་དོན་ཡིན་ཡང་ན་ནོར་འཁྲུལ་ཞིག ད་ལྟ་མ་འོངས་པར་མཐོང་སྣེ་མང་དུ་མཐོང་ནུས་མེད་པར་རྐྱེན་གྱིས་མཐོང་རྐྱེན་བཟོ་བ་ཡིན་ནའང་མ་ཟད། For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.'}
{'en': 'Learning Informative Representations of Biomedical Relations with Latent Variable Models', 'ar': 'تعلم تمثيلات إعلامية للعلاقات الطبية الحيوية مع النماذج المتغيرة الكامنة', 'fr': 'Apprentissage de représentations informatives des relations biomédicales avec des modèles de variables latentes', 'pt': 'Aprendendo Representações Informativas de Relações Biomédicas com Modelos de Variáveis Latentes', 'es': 'Aprendizaje de representaciones informativas de relaciones biomédicas con modelos de variables latentes', 'ja': '潜在的変数モデルとの生物医学的関係の情報的表現を学習する', 'ru': 'Изучение информативных представлений биомедицинских отношений с латентными вариабельными моделями', 'hi': 'अव्यक्त चर मॉडल के साथ जैव चिकित्सा संबंधों के जानकारीपूर्ण प्रतिनिधित्व सीखना', 'zh': '学有潜变量之生物医学', 'ga': 'Ag Foghlaim Léiriúcháin Eolasacha ar Chaidreamh Bithleighis le Múnlaí Athraitheacha Folaithe', 'hu': 'Bioorvosi kapcsolatok tanulási információs ábrázolása Latent Variable Modellekkel', 'it': 'Rappresentazioni informative di apprendimento delle relazioni biomediche con modelli variabili latenti', 'kk': 'Кейінгі өзгертілген үлгілер мен биомедицина қатынастарының мәліметті таңбаларын үйрену', 'lt': 'Mokymasis informacinėmis biomedicininių santykių su vėlesniais kintamųjų modeliais reprezentacijomis', 'ka': 'ბიომედიციური შესახებების ინფორმაციური გამოსახულება შემდეგ ცვლილებული მოდელებით', 'el': 'Μάθηση πληροφοριακών αναπαραστάσεων βιοιατρικών σχέσεων με επίκαιρα μεταβλητά μοντέλα', 'mk': 'Учење информативни претставувања на биомедицинските односи со последните променливи модели', 'ms': 'Belajar Perwakilan Informatif Hubungan Biomedis dengan Model Pembolehubah Terkini', 'ml': 'ബൈയോമിഡിക്കല്\u200d ബന്ധങ്ങളുടെ വിവരങ്ങള്\u200d പഠിക്കുന്നു', 'mn': 'Сүүлийн өөрчлөлтийн загвартай биологийн эмнэлгийн харилцааны мэдээллийн загвар суралцах', 'mt': 'Tagħlim Rappreżentazzjonijiet Informattivi tar-Relazzjonijiet Bijomediċiċi ma’ Mudelli Varjabbli Latenti', 'no': 'Læring informativ representasjonar av biomediske relasjonar med siste variabel modeller', 'ro': 'Reprezentarea informativă a relaţiilor biomedicale cu modele variabile latente', 'pl': 'Uczenie się informacyjnych reprezentacji relacji biomedycznych z modelami zmiennymi Latent', 'sr': 'Naučenje informativnih predstavljanja biomedicinskih odnosa sa poslednjim variabilnim modelima', 'so': 'Barista macluumaad ku noolaanshada Biomedical xiriir la xiriira dhaadheer', 'si': 'ඉගෙනගන්න තොරතුරු ප්\u200dරතිනිධාන ජීවිත විද්\u200dයාත්මක සම්බන්ධතාවක් සමග පස්සේ වෙනස් මොඩේල්', 'sv': 'Lärande Informativa representationer av biomedicinska relationer med Latent Variable Models', 'ta': 'பையோமிடியல் தொடர்புகளுடன் மாறிகளுடன் தகவல் குறிப்புகளை கற்றுக்கொண்டுள்ளது', 'ur': 'Latent Variable Models', 'uz': 'Name', 'vi': 'Học Giám mục Truyền Thông Sản Liên quan sinh học với Chế độ biến thể mới', 'da': 'Informative repræsentationer af biomedicinske relationer med latente variable modeller', 'bg': 'Учене на информативни представи на биомедицинските отношения с латентни променливи модели', 'hr': 'Naučenje informativnih predstavljanja biomedicinskih odnosa s posljednjim variabilnim modelima', 'nl': 'Informatieve representaties van biomedische relaties met latente variabele modellen leren', 'de': 'Lernen informativer Darstellungen biomedizinischer Beziehungen mit latenten Variablenmodellen', 'id': 'Belajar Perwakilan Informatif Hubungan Biomedis dengan Model Variabel Terkini', 'ko': '잠재 변수 모형으로 생물 의학 관계의 정보 표시를 학습하다', 'fa': 'یاد گرفتن نمایندگان اطلاعاتی رابطه های بیولوژیک با مدل های متغیر اخیر', 'af': 'Leer Informatiewe voorstellings van Biomediese Relasies met Latent Variable Models', 'sw': 'Kujifunza Mazungumzo ya Taarifa ya Uhusiano wa Kibiomedica na Modeli zilizobadilika hivi karibuni', 'tr': 'Biomedical Baglaýyşlaryň Mazmunlary Görkezilişini Öwrenmek', 'sq': 'Mësimi i përfaqësimeve informative të marrëdhënieve biomjekësore me modelet e ndryshueshme të fundit', 'am': 'ምርጫዎች', 'hy': 'Բիոբիոբժշկական հարաբերությունների ինֆորմատիվ ներկայացումների սովորելը վերջին փոփոխական մոդելների հետ', 'az': 'SonrakńĪ DeńüiŇüikli Modell…ôr il…ô Biomedical ńįliŇükil…ôrin Informative Representations of Learning', 'bs': 'Naučenje informativnih predstavljanja biomedicinskih odnosa sa poslednjim variabilnim modelima', 'bn': 'বায়োমিকাল সম্পর্কের তথ্য প্রতিনিধি', 'ca': 'Aprendre Representacions Informatives de Relacions Biomèdiques amb Models Variables Llavors', 'cs': 'Učení se informačních reprezentací biomedicínských vztahů s latentními proměnnými modely', 'et': 'Biomeditsiiniliste suhete informatiivsete representatsioonide õppimine latentsete muutuvmudelitega', 'fi': 'Oppiminen Informatiivisia representaatioita biolääketieteellisistä suhteista latenttimuuttujamalleihin', 'jv': 'Learning Informable representations of Bidirectial', 'he': 'ללמוד מייצגים מידעיים של יחסים ביורפואיים עם מודלים משתנים מאוחרים', 'ha': 'KCharselect unicode block name', 'sk': 'Učenje informativnih predstavitev biomedicinskih odnosov z latentnimi variabilnimi modeli', 'bo': 'ཤུལ་མ་དུ་འགྱུར་བའི་མ་དབུལ་གྱི་སྐྱེས་པའི་བརྡ་སྟོན་པའི་བྱ་ཚུལ་གྱི་འབྲེལ་བ་དང་མཉམ་དུ་བསླབས་པའི་'}
{'en': 'Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation ; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.', 'ar': 'يعد استخراج العلاقات الطبية الحيوية من مجموعة كبيرة من الوثائق العلمية مهمة صعبة لمعالجة اللغة الطبيعية. عادة ما تركز المناهج الحالية على تحديد العلاقة إما في جملة واحدة (مستوى الإشارة) أو عبر مجموعة كاملة (على مستوى الزوج). في كلتا الحالتين ، حققت الأساليب الحديثة نتائج قوية من خلال تعلم تقدير النقطة لتمثيل العلاقة ؛ ثم يتم استخدام هذا كمدخل لمصنف العلاقة. ومع ذلك ، فإن العلاقة المعبر عنها في النص بين زوج من الكيانات الطبية الحيوية غالبًا ما تكون أكثر تعقيدًا مما يمكن التقاطه من خلال تقدير النقاط. لمعالجة هذه المشكلة ، نقترح نموذجًا كامنًا متغيرًا بتوزيع مرن عشوائيًا لتمثيل العلاقة بين زوج كيان. بالإضافة إلى ذلك ، يوفر نموذجنا بنية موحدة لكل من استخراج العلاقة على مستوى الذكر والمستوى الزوجي. لقد أثبتنا أن نموذجنا يحقق نتائج تنافسية مع خطوط أساس قوية لكلتا المهمتين مع وجود عدد أقل من المعلمات ويكون التدريب أسرع بشكل ملحوظ. نجعل الكود الخاص بنا متاحًا للجمهور.', 'fr': "L'extraction de relations biomédicales à partir de vastes corpus de documents scientifiques est une tâche difficile de traitement du langage naturel. Les approches existantes se concentrent généralement sur l'identification d'une relation soit en une seule phrase (au niveau de la mention), soit dans l'ensemble d'un corpus (au niveau de la paire). Dans les deux cas, les méthodes récentes ont donné de bons résultats en apprenant une estimation ponctuelle pour représenter la relation\xa0; celle-ci est ensuite utilisée comme entrée dans un classificateur de relations. Cependant, la relation exprimée dans le texte entre une paire d'entités biomédicales est souvent plus complexe que ne peut être saisie par une estimation ponctuelle. Pour résoudre ce problème, nous proposons un modèle de variable latente avec une distribution arbitrairement flexible pour représenter la relation entre une paire d'entités. De plus, notre modèle fournit une architecture unifiée pour l'extraction de relations au niveau de la mention et au niveau de la paire. Nous démontrons que notre modèle atteint des résultats compétitifs avec des bases de référence solides pour les deux tâches tout en ayant moins de paramètres et en étant beaucoup plus rapide à entraîner. Nous mettons notre code à la disposition du public.", 'pt': 'Extrair relações biomédicas de grandes corpora de documentos científicos é uma tarefa desafiadora de processamento de linguagem natural. As abordagens existentes geralmente se concentram na identificação de uma relação em uma única frase (nível de menção) ou em todo um corpus (nível de par). Em ambos os casos, métodos recentes alcançaram resultados fortes ao aprender uma estimativa pontual para representar a relação; isso é então usado como entrada para um classificador de relação. No entanto, a relação expressa no texto entre um par de entidades biomédicas é muitas vezes mais complexa do que pode ser capturada por uma estimativa pontual. Para resolver este problema, propomos um modelo de variável latente com uma distribuição arbitrariamente flexível para representar a relação entre um par de entidades. Além disso, nosso modelo fornece uma arquitetura unificada para extração de relação de nível de menção e nível de par. Demonstramos que nosso modelo alcança resultados competitivos com linhas de base fortes para ambas as tarefas, tendo menos parâmetros e sendo significativamente mais rápido para treinar. Disponibilizamos nosso código publicamente.', 'es': 'Extraer relaciones biomédicas de grandes corpus de documentos científicos es una tarea difícil de procesar el lenguaje natural. Los enfoques existentes generalmente se centran en identificar una relación en una sola oración (nivel de mención) o en todo un corpus (nivel de pares). En ambos casos, los métodos recientes han logrado buenos resultados al aprender una estimación puntual para representar la relación; esto se utiliza como entrada para un clasificador de relaciones. Sin embargo, la relación expresada en el texto entre un par de entidades biomédicas suele ser más compleja de lo que se puede captar con una estimación puntual. Para abordar este problema, proponemos un modelo de variables latentes con una distribución arbitrariamente flexible para representar la relación entre un par de entidades. Además, nuestro modelo proporciona una arquitectura unificada para la extracción de relaciones a nivel de mención y a nivel de pares. Demostramos que nuestro modelo logra resultados competitivos con bases de referencia sólidas para ambas tareas, al mismo tiempo que tiene menos parámetros y es significativamente más rápido de entrenar. Ponemos nuestro código a disposición del público.', 'ja': '大量の科学文書から生物医学的関係を抽出することは、自然言語処理の課題です。 既存のアプローチは、通常、単一の文（言及レベル）またはコーパス全体（ペアレベル）のいずれかで関係を識別することに焦点を当てています。 どちらの場合も、最近の方法は、関係を表すためのポイント推定値を学習することによって強力な結果を達成しています。これは、関係分類子への入力として使用されます。 しかしながら、一対の生物医学的実体間のテキストで表現される関係は、点推定によって捉えることができる以上に複雑であることが多い。 この問題に対処するために、エンティティペア間の関係を表すために、任意に柔軟な分布を持つ潜在的な変数モデルを提案します。 さらに、当社のモデルは、メンションレベルとペアレベルリレーションズ抽出の両方に統一されたアーキテクチャを提供します。 私たちのモデルは、パラメータが少なく、トレーニングが著しく早い一方で、両方のタスクの強力なベースラインで競争力のある結果を達成することを実証しています。 コードは一般公開されています。', 'zh': '取科学文献生物医学一挑战性之自然语言也。 今法常侧重于单句(提级)或全语料库(配对)中识别关系。 此二者,近法皆因学度以取强也; 然后,将用作分类器的输入。 然生物医学实体以文本示常比点估计值所获尤杂。 为此者,发一任情之变量以见实体。 以配对为一体系结构。 吾二事之强基线下皆得竞争力,参数益少,习之速也。 我显发我代码。', 'ru': 'Извлечение биомедицинских отношений из больших корпусов научных документов является сложной задачей обработки естественного языка. Существующие подходы обычно сосредоточены на выявлении связи либо в одном предложении (уровень упоминания), либо во всем корпусе (парный уровень). В обоих случаях недавние методы добились сильных результатов, изучив точечную оценку для представления отношения; она затем используется в качестве входных данных для классификатора отношения. Однако соотношение, выраженное в тексте между парой биомедицинских объектов, часто является более сложным, чем может быть зафиксировано точечной оценкой. Для решения этой проблемы мы предлагаем модель скрытых переменных с произвольным гибким распределением, чтобы представить связь между парой сущностей. Кроме того, наша модель обеспечивает унифицированную архитектуру для извлечения отношений как на уровне упоминания, так и на уровне пар. Мы демонстрируем, что наша модель достигает конкурентоспособных результатов с сильными базовыми линиями для обеих задач, при этом имея меньшее количество параметров и значительно быстрее обучаться. Мы делаем наш код общедоступным.', 'hi': 'वैज्ञानिक दस्तावेजों के बड़े कॉर्पोरेट से बायोमेडिकल संबंधों को निकालना एक चुनौतीपूर्ण प्राकृतिक भाषा प्रसंस्करण कार्य है। मौजूदा दृष्टिकोण आमतौर पर एक ही वाक्य (उल्लेख-स्तर) या पूरे कॉर्पस (जोड़ी-स्तर) में एक संबंध की पहचान करने पर ध्यान केंद्रित करते हैं। दोनों मामलों में, हाल के तरीकों ने संबंध का प्रतिनिधित्व करने के लिए एक बिंदु अनुमान सीखकर मजबूत परिणाम प्राप्त किए हैं; यह तो एक संबंध क्लासिफायर के लिए इनपुट के रूप में प्रयोग किया जाता है. हालांकि, बायोमेडिकल संस्थाओं की एक जोड़ी के बीच पाठ में व्यक्त संबंध अक्सर एक बिंदु अनुमान द्वारा कब्जा किए जाने की तुलना में अधिक जटिल होता है। इस समस्या को हल करने के लिए, हम एक इकाई जोड़ी के बीच संबंध का प्रतिनिधित्व करने के लिए एक मनमाने ढंग से लचीला वितरण के साथ एक अव्यक्त चर मॉडल का प्रस्ताव करते हैं। इसके अतिरिक्त, हमारा मॉडल उल्लेख-स्तर और जोड़ी-स्तर संबंध निष्कर्षण दोनों के लिए एक एकीकृत वास्तुकला प्रदान करता है। हम प्रदर्शित करते हैं कि हमारा मॉडल दोनों कार्यों के लिए मजबूत बेसलाइन के साथ प्रतिस्पर्धी परिणाम प्राप्त करता है, जबकि कम पैरामीटर होते हैं और प्रशिक्षित करने के लिए काफी तेज होते हैं। हम अपने कोड को सार्वजनिक रूप से उपलब्ध कराते हैं।', 'ga': "Is tasc próiseála teanga nádúrtha dúshlánach é caidreamh bithleighis a bhaint as corpas mór doiciméad eolaíochta. Is iondúil go ndírítear cur chuige reatha ar ghaolmhaireacht a aithint in aon abairt amháin (luaite-leibhéal) nó thar chorpas iomlán (péire-leibhéal). Sa dá chás, tá torthaí láidre bainte amach ag modhanna le déanaí trí mheastachán pointe a fhoghlaim chun an gaol a léiriú; úsáidtear é seo ansin mar ionchur d'aicmitheoir gaol. Mar sin féin, is minic a bhíonn an gaol a chuirtear in iúl i dtéacs idir péire eintiteas bithleighis níos casta ná mar is féidir a ghabháil le meastachán pointe. Chun aghaidh a thabhairt ar an tsaincheist seo, molaimid múnla athróg folaigh le dáileadh solúbtha treallach chun an gaol idir péire aonáin a léiriú. Ina theannta sin, cuireann ár múnla ailtireacht aontaithe ar fáil le haghaidh eastóscadh caidreamh leibhéal lua agus péire-leibhéal. Léirímid go sroicheann ár múnla torthaí iomaíocha le bonnlínte láidre don dá thasc agus ag an am céanna go bhfuil níos lú paraiméadair againn agus go bhfuil sé i bhfad níos tapúla le hoiliúint. Cuirimid ár gcód ar fáil go poiblí.", 'ka': 'ბიომედიციური შესახებ ძალიან მეცნიერო დოკუმენტების კოპორაციის გამოყენება არის საშუალებელი ბიომედიციური გამოყენება. მსგავსი წარმოდგენები საერთო წარმოდგენების განსაზღვრებისთვის, ან ერთი წარმოდგენების განსაზღვრებისთვის, ან ყველა კორპუსს (ორი დონე). ორივე შემთხვევაში, ახალი მეტოვები ძალიან წარმოდგენა, მასწავლებით წარმოდგენა წარმოდგენების შესახებ, რომლებიც შესახებ შესახებ; ეს შემდეგ გამოიყენება, როგორც შესაბამისი კლასიფიკაციის შესაბამისად. მაგრამ, ტექსტიში გამოსახულებული პირობა ვიომედიციური ინტერტიკების შორის უფრო კომპლექსია, ვიდრე შეიძლება წიგნის განსახულებით გადატანა. ამ პრობლემას გადაწყვეტისთვის, ჩვენ მინდომებით განცვლელი მოდელს, რომელიც არგებულად гиბექტიური გადაწყვეტისთვის, რომელიც ინტერტის ზოგის შორის შესახებ გა დამატებით, ჩვენი მოდელი ერთადერთი აქტიქტიქტიკური ექსტრაქციის განმავლობაში და ორი დონეში. ჩვენ მოდენსტურებთ, რომ ჩვენი მოდელი გავაკეთებთ შედეგი კონსპენტები ძალიან ბაზი ხაზებით ორივე საქმებისთვის, როცა უფრო ცოტა პარამეტრები და უფრო ბ ჩვენ ჩვენი კოდის ადგილურად ხელსახულება.', 'it': "Estrarre relazioni biomediche da grandi corpora di documenti scientifici è un compito impegnativo di elaborazione del linguaggio naturale. Gli approcci esistenti di solito si concentrano sull'identificazione di una relazione in una singola frase (livello di menzione) o attraverso un intero corpus (livello di coppia). In entrambi i casi, i metodi recenti hanno raggiunto risultati forti imparando una stima puntuale per rappresentare la relazione; Questo viene poi utilizzato come input per un classificatore di relazioni. Tuttavia, la relazione espressa nel testo tra una coppia di entità biomediche è spesso più complessa di quanto possa essere catturata da una stima puntuale. Per affrontare questo problema, proponiamo un modello variabile latente con una distribuzione arbitrariamente flessibile per rappresentare la relazione tra una coppia di entità. Inoltre, il nostro modello fornisce un'architettura unificata per l'estrazione delle relazioni sia a livello di menzione che a livello di coppia. Dimostriamo che il nostro modello raggiunge risultati competitivi con solide linee di base per entrambe le attività, pur avendo meno parametri e essendo significativamente più veloce da addestrare. Rendiamo pubblico il nostro codice.", 'kk': 'Биологиялық құжаттардың үлкен корпорасынан биомедикалық қатынастарын тарқату - табиғалық тілді өңдеу тапсырмасы. Бар жағдайдағы жағдайлары кәдімгі бір сөзде не толық корпус (екі деңгейінде) қатынасын анықтауға көмектеседі. Екі жағдайда, жаңа әдістер қатынасын көрсету үшін нәтижелерді оқу үшін күшті нәтижелерді жеткізді. содан кейін қатынас классификациясына енгізу үшін қолданылады. Бірақ бірнеше биомедикалық құрылғылар арасындағы мәтінде көрсетілген қатынасы көбінесе бұрышты бағалау үшін көптеген. Бұл мәселеге қатынау үшін, біз бөліктердің екі арасындағы қатынасын көрсету үшін соңғы айнымалы үлгісін ұсынамыз. Қосымша, біздің үлгіміз сөйлейтін деңгейі мен екі деңгейі қатынау үшін біріктірілген архитектура береді. Біз үлгіміздің түріміздің күш негізгі жолдарымыздың әсерін жеткізеді. Екі тапсырмалардың параметрлері аз болып, оқыту үшін ең жылдам болып тұрады. Біз кодымызды жалпы жұмыс істейміз.', 'lt': 'Biomedicininių santykių ekstrahavimas iš didelių mokslinių dokumentų korporacijos yra sudėtinga gamtos kalbų apdorojimo užduotis. Esami metodai paprastai sutelkiami į santykių nustatymą vienu sakiniu (paminėjimo lygiu) arba visame korpuse (poros lygiu). Abiem atvejais neseniai taikomi metodai pasiekė tvirtų rezultatų, išmokę įvertinti tašką, kuris atspindėtų santykį; tada jis naudojamas kaip santykių klasifikatoriaus įvestis. Tačiau tekste išreikštas santykis tarp dviejų biomedicinų subjektų dažnai yra sudėtingesnis nei galima nustatyti pagal taškų skaičiavimą. Siekiant išspręsti šį klausimą, siūlome latentišką kintamąjį model į su savavališkai lanksčiu paskirstymu, kuris atspindėtų subjektų poros santykį. Be to, mūsų modelis suteikia vieningą architektūrą tiek paminėto lygio, tiek poros lygio santykių ekstrahavimui. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train.  We make our code publicly available.', 'el': 'Η εξαγωγή βιοϊατρικών σχέσεων από μεγάλα σώματα επιστημονικών εγγράφων είναι ένα δύσκολο έργο επεξεργασίας φυσικής γλώσσας. Οι υπάρχουσες προσεγγίσεις επικεντρώνονται συνήθως στον προσδιορισμό μιας σχέσης είτε σε μία πρόταση (επίπεδο αναφοράς) είτε σε ολόκληρο σώμα (επίπεδο ζεύγους). Και στις δύο περιπτώσεις, οι πρόσφατες μέθοδοι έχουν επιτύχει ισχυρά αποτελέσματα μαθαίνοντας μια εκτίμηση σημείων για να αντιπροσωπεύει τη σχέση. αυτό χρησιμοποιείται στη συνέχεια ως η εισαγωγή σε έναν ταξινομητή σχέσεων. Ωστόσο, η σχέση που εκφράζεται στο κείμενο μεταξύ ενός ζεύγους βιοϊατρικών οντοτήτων είναι συχνά πιο περίπλοκη από ό, τι μπορεί να συλληφθεί με μια εκτίμηση σημείου. Για να αντιμετωπιστεί αυτό το ζήτημα, προτείνουμε ένα λανθάνον μεταβλητό μοντέλο με μια αυθαίρετα ευέλικτη κατανομή που αντιπροσωπεύει τη σχέση μεταξύ ενός ζευγαριού οντότητας. Επιπλέον, το μοντέλο μας παρέχει μια ενοποιημένη αρχιτεκτονική για εξαγωγή σχέσεων σε επίπεδο αναφοράς και σε επίπεδο ζεύγους. Αποδεικνύουμε ότι το μοντέλο μας επιτυγχάνει αποτελέσματα ανταγωνιστικά με ισχυρές βάσεις και για τις δύο εργασίες, ενώ έχει λιγότερες παραμέτρους και είναι σημαντικά ταχύτερη στην εκπαίδευση. Κάνουμε τον κώδικα μας διαθέσιμο στο κοινό.', 'hu': 'Az orvosbiológiai kapcsolatok kivonása a tudományos dokumentumok nagy tömegeiből kihívást jelentő természetes nyelvfeldolgozási feladat. A meglévő megközelítések általában a kapcsolat azonosítására összpontosítanak egyetlen mondatban (említési szint), vagy egy teljes korpuszon (párszint). Mindkét esetben a közelmúltbeli módszerek erős eredményeket értek el azáltal, hogy megtanulták a kapcsolatot reprezentáló pontbecslést; Ezt követően egy kapcsolatosztályozó bemenetként használják. Azonban az orvosbiológiai egységek párja közötti szövegben kifejezett kapcsolat gyakran bonyolultabb, mint amennyit egy pontbecsléssel lehet rögzíteni. A probléma megoldására egy látens változó modellt javasolunk önkényesen rugalmas eloszlással, amely egy entitáspár közötti kapcsolatot képviseli. Ezenkívül modellünk egységes architektúrát biztosít mind említésszintű, mind párszintű kapcsolatok kivonásához. Bemutatjuk, hogy modellünk mindkét feladat esetében versenyképes eredményeket ér el, miközben kevesebb paraméterrel rendelkezik, és jelentősen gyorsabb a képzés. Nyilvánosan elérhetővé tesszük a kódunkat.', 'ml': 'ശാസ്ത്ര രേഖകളുടെ വലിയ കോര്\u200dപ്പോരയില്\u200d നിന്നും ജീവിയോമിക്കല്\u200d ബന്ധങ്ങള്\u200d പുറത്തെടുക്കുന്നത് സ്വാഭാവിക ഭാഷ പ നിലവിലുള്ള അടിസ്ഥാനങ്ങള്\u200d ഒരു വാക്കില്\u200d ഒരു ബന്ധത്തെ തിരിച്ചറിയാന്\u200d സാധാരണമായി ശ്രദ്ധിക്കുന്നു രണ്ടു കാര്യങ്ങളിലും, ബന്ധത്തെ പ്രതിനിധിക്കാനുള്ള ഒരു പോയിന്റ് അഭിപ്രായം പഠിക്കുന്നതിനാല്\u200d അടുത്ത മാര പിന്നീട് ഇത് ഒരു ബന്ധപ്രഖ്യാപിക്കാനുള്ള ഇന്\u200dപുട്ടായി ഉപയോഗിക്കുന്നു. എന്നാലും ഒരു ജീവിയ മെഡിക്കല്\u200d വസ്തുക്കള്\u200dക്കിടയിലുള്ള വാചകത്തില്\u200d പ്രസ്താവിക്കുന്ന ബന്ധം ഒരു പോയിന്റ് കണക്കിനെക ഈ പ്രശ്നത്തെക്കുറിച്ച് വിശദീകരിക്കാന്\u200d ഞങ്ങള്\u200d ഒരു സാധാരണ മോഡല്\u200d പ്രായസ്ത്രീകരിക്കുന്നു. ഒരു വസ്തുവിന്റെ ജോടിയുടെ ഇടയ കൂടുതലായി, നമ്മുടെ മോഡല്\u200d ഒരു യൂണിക്കൂട്ടിയ ആര്\u200dക്ടിക്കറ്റര്\u200d നല്\u200dകുന്നു. നിലവും രണ്ട് നില ബന്ധം പുറത്തിറങ്ങുന് ഞങ്ങള്\u200d പ്രത്യക്ഷപ്പെടുത്തുന്നു, നമ്മുടെ മോഡല്\u200d ശക്തിയുള്ള അടിസ്ഥാനങ്ങളില്\u200d മത്സരിക്കുന്ന ഫലങ്ങള്\u200d എതിര്\u200dക്കുന്നു എന്ന് നമ്മുട നമ്മള്\u200d നമ്മുടെ കോഡ് പ്രസിദ്ധമാക്കുന്നു.', 'mt': 'L-estrazzjoni tar-relazzjonijiet bijomediċi minn korpura kbira ta’ dokumenti xjentifiċi hija kompitu ta’ pproċessar naturali tal-lingwi ta’ sfida. L-approċċi eżistenti normalment jiffukaw fuq l-identifikazzjoni ta’ relazzjoni jew f’sentenza waħda (livell ta’ referenza) jew f’korpus sħiħ (livell ta’ pari). Fiż-żewġ każijiet, il-metodi reċenti kisbu riżultati qawwija bit-tagħlim ta’ stima ta’ punt li tirrappreżenta r-relazzjoni; this is then used as the input to a relation classifier.  Madankollu, ir-relazzjoni espressa fit-test bejn par ta’ entitajiet bijomediċi ħafna drabi hija aktar kumplessa milli tista’ tinqabad minn stima tal-punt. Biex nindirizzaw din il-kwistjoni, nipproponu mudell varjabbli moħbi bi distribuzzjoni arbitrarjament flessibbli li tirrappreżenta r-relazzjoni bejn par ta’ entitajiet. Barra minn hekk, il-mudell tagħna jipprovdi arkitettura unifikata kemm għall-estrazzjoni tar-relazzjoni fil-livell ta’ referenza kif ukoll f’livell ta’ pari. Aħna nuru li l-mudell tagħna jikseb riżultati kompetittivi b’linji bażi b’saħħithom għaż-żewġ kompiti filwaqt li jkollu inqas parametri u jkun ferm aktar mgħaġġel biex inħarrġu. Aħna nagħmlu l-kodiċi tagħna disponibbli għall-pubbliku.', 'mk': 'Екстрактирањето на биомедицинските односи од голема корпора научни документи е предизвикувачка задача за процес на природен јазик. Постојаните пристапи обично се фокусираат на идентификацијата на односот или во една реченица (спомено ниво) или низ целиот корпус (пар ниво). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation;  ова потоа се користи како влез во класификатор на односи. Сепак, односот изразен во текст помеѓу еден пар биомедицински ентитети е честопати покомплексен отколку што може да се фати со проценка на точки. За да го решиме ова прашање, предлагаме лантен модел на променливи со арбитрално флексибилна дистрибуција за да ја претставува врската помеѓу пар ентитети. Покрај тоа, нашиот модел обезбедува единствена архитектура за екстракција на спомено-ниво и на двојно ниво. Демонстрираме дека нашиот модел постигнува резултати конкурентни со силни основни линии за двете задачи, истовремено имајќи помалку параметри и значително побрзо обука. Го обезбедуваме нашиот код јавно.', 'ms': 'Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task.  pendekatan yang wujud biasanya fokus pada mengenalpasti hubungan sama ada dalam satu kalimat (aras-sebutan) atau di seluruh korpus (aras-pasangan). Dalam kedua-dua kes, kaedah baru-baru ini telah mencapai keputusan yang kuat dengan belajar nilai titik untuk mewakili hubungan; ia kemudian digunakan sebagai input ke pengklasifikasi hubungan. Namun, hubungan yang diekspresikan dalam teks diantara pasangan entiti biomedikal sering lebih kompleks daripada boleh ditangkap oleh anggaran titik. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair.  Lagipun, model kami menyediakan arkitektur bersatu untuk ekstraksi hubungan sebutan-level dan pasangan-level. Kami menunjukkan bahawa model kami mencapai keputusan kompetitif dengan garis dasar yang kuat untuk kedua-dua tugas sementara mempunyai lebih sedikit parameter dan lebih cepat untuk berlatih. Kami membuat kod kami tersedia secara umum.', 'pl': 'Wydobycie relacji biomedycznych z dużych korpusów dokumentów naukowych jest wymagającym zadaniem przetwarzania języka naturalnego. Istniejące podejścia koncentrują się zwykle na identyfikacji relacji w jednym zdaniu (poziom wzmianki) lub w całym korpusie (poziom pary). W obu przypadkach ostatnie metody osiągnęły silne rezultaty dzięki nauce szacunku punktowego reprezentującego relację; jest to następnie używane jako wejście do klasyfikatora relacji. Jednak relacja wyrażona w tekście pomiędzy parą podmiotów biomedycznych jest często bardziej złożona niż może być uchwycona przez szacunek punktowy. Aby rozwiązać ten problem, proponujemy model utajonej zmiennej z dowolnie elastycznym rozkładem reprezentującym relację między parą jednostek. Ponadto nasz model zapewnia ujednoliconą architekturę zarówno dla ekstrakcji relacji na poziomie wzmianki, jak i na poziomie pary. Pokazujemy, że nasz model osiąga wyniki konkurencyjne z silnymi liniami bazowymi dla obu zadań, mając mniejszą liczbę parametrów i jest znacznie szybszy w treningu. Udostępniamy nasz kod publicznie.', 'mn': 'Биологийн эмнэлгийн харилцаа олон шинжлэх ухааны баримтуудын корпорациас гаргах нь байгалийн хэл боловсруулах үйл ажил юм. Existing approaches usually focus on identifying a relationship in a single sentence (mention-level) or an entire corpus (pair-level) across. Хоёр тохиолдолд, саяхан арга нь харилцааныг илэрхийлж суралцаж хүчтэй үр дүнг гаргасан. дараа нь харилцааны хуваалцагчдын оролцоо гэж хэрэглэгддэг. Гэхдээ биологийн эмнэлгийн бүтээгдэхүүн хоорондын текст дээр илэрхийлэгдсэн харилцаа ихэвчлэн цэгийн тооцооллоос илүү төвөгтэй. Энэ асуудлыг бодохын тулд бид нэг хоёр хоорондын харилцааныг илэрхийлэх боломжтой хувьсагчийн загвар загварыг санал болгоно. Мөн бидний загвар нь хэмжээний түвшинд, хоёр түвшинд хоёр түвшинд хамааралтай байгууллагын нэгтгэл бүтэц хангадаг. Бид загварын загвар нь хоёр даалгаварын хүчтэй суурь шугам дээр өрсөлдөг үр дүнг бага хэмжээний параметр болон сургуульд маш хурдан хүртэл хүртэл гаргадаг гэдгийг харуулж байна. Бид бидний кодыг олон нийтэд ашиглаж чадна.', 'no': 'Utpakking av biomedisinske forhold frå stor korpora av vitenskapelige dokument er eit vanskeleg naturspråkshandteringsoppgåve. Den eksisterande tilnærminga er vanlegvis fokusert på å identifisera ein relasjon anten i eit enkelt setning (minningsnivå) eller over ein heil korpus (par nivå). I begge tilfeller har siste metodar oppnådd sterke resultat ved å lære eit punktestimating for å representera forholdet. denne vert derfor brukt som inndata til ei relasjonsklassifisering. Men forholdet uttrykket i tekst mellom ein par biomedisinske einingar er ofte meir komplekse enn det kan hentast av eit punktestimat. For å handtera dette problemet, foreslår vi ein siste variabel modell med ein tilfeldig fleksibel distribusjon for å representera relasjonen mellom ein einingspar. I tillegg vil modellen vårt gjeve eit einaste arkitektur for både utpakking av referansnivå og par-nivå. Vi demonstrerer at modellen vårt når resultatet er konkurentivt med sterke baselinjer for begge oppgåver mens det har feire parametrar og er mykje raskare å trene. Vi gjer koden vårt offentlig tilgjengeleg.', 'so': 'Ka soo bixinta xiriirka dhakhaatiirta ah ee shirkadaha waaweyn ee dukumentiyada cilmiga ah waa shaqa ka baaraandegista luqada dabiicadda ah. Dhaqdhaqaaqyada joogtada ah sida caadiga ah waa inaad ku kalsoonaataa aqoonsashada xiriirka hal eray (hal-heer) ama hal qof oo dhan (labo-heer). Xaaladaha labadooda marka lagu jiro, qaababka ugu dambeeyey waxay soo gaadheen resulto xoog leh, marka lagu barto qiimeynta la qiimeeyo in la represento xiriirka; this is then used as the input to a relation classifier.  Si kastaba ha ahaatee xiriirka ku qoran qoraalka u dhexeeya labada jidh oo dhakhaatiirta ah inta badan waa ka adag yihiin qiimeynta lagu qabsan karo. Si a an ula macaamiloono arrintan, waxaan u soo jeedinnaa model bedelan ee ugu dambeeya, kaas oo ah qaybinta si qallafsan, si aan uga dhigno xiriirka u dhexeeya labada ruux. Sidoo kale modellkayagu wuxuu u fidiyaa dhismo la xiriira labada heer iyo labada heer oo xiriir ah. Waxaynu muujinnaa in modellkayagu uu soo kordhiyo resultiyada oo ku saabsan samooyin xoog leh oo labada shaqaalaha lagu sameynayo marka ay leedahay parameters yar iyo in aad u dhaqso waxbarasho. Codsheenna si bayaan ah ayaannu u isticmaalnaa.', 'ro': 'Extragerea relațiilor biomedicale din corpuri mari de documente științifice este o sarcină dificilă de procesare a limbajului natural. Abordările existente se concentrează de obicei pe identificarea unei relații fie într-o singură propoziție (nivel de mențiune), fie într-un corpus întreg (nivel de pereche). În ambele cazuri, metodele recente au obținut rezultate puternice prin învățarea unei estimări punctuale pentru a reprezenta relația; Aceasta este apoi folosită ca intrare pentru un clasificator de relații. Cu toate acestea, relația exprimată în text între o pereche de entități biomedicale este adesea mai complexă decât poate fi captată de o estimare punctuală. Pentru a aborda această problemă, propunem un model variabil latent cu o distribuție flexibilă arbitrară pentru a reprezenta relația dintre o pereche de entități. În plus, modelul nostru oferă o arhitectură unificată atât pentru extragerea relațiilor la nivel de mențiune, cât și pentru nivel de pereche. Demonstrăm că modelul nostru obține rezultate competitive cu linii de bază puternice pentru ambele sarcini, având în același timp mai puțini parametri și fiind semnificativ mai rapid de formare. Facem codul nostru disponibil public.', 'sr': 'Izvlačenje biomedicinskih odnosa iz velike korporacije znanstvenih dokumenta je izazovni zadatak za procesiranje prirodnog jezika. Postojeći pristupi obično se fokusiraju na identifikaciju odnosa ili u jednoj rečenici (nivou spomena) ili preko cijelog korpusa (nivoa par). U oba slučaja, nedavne metode su postigle jake rezultate učeći procjenu tačke za predstavljanje odnosa; ovo se onda koristi kao ulaz u povezanje sa klasifikatorom. Međutim, odnos izražen u tekstu između par biomedicinskih entiteta često je kompleksniji nego što se može uhvatiti tačkom procjenom. Za rješavanje ovog pitanja predlažemo poslednji model varijanta sa proizvoljno fleksibilnim distribucijom da predstavimo odnos između parova entiteta. Osim toga, naš model obezbeđuje ujedinjenu arhitekturu za izvlačenje odnosa na nivou spominjanja i na nivou par. Pokazujemo da naš model postigne rezultate konkurentne sa jakim osnovnim linijama za obe zadatke dok ima manje parametara i značajno brže trenirati. Objavljujemo naš kod.', 'ta': 'பெரிய அறிவியல் ஆவணங்களிலிருந்து உயிரிய மருத்துவ தொடர்புகளை வெளியேற்றுதல் என்பது இயல்பான மொழி செயல்பாட்டில்  இருக்கும் நெறிமுறைகள் ஒரு வாக்கில் அல்லது முழு நிலையில் (ஜோடி- மட்டத்தில்) ஒரு தொடர்பை கண்டுபிடிக்கும் பொதுவாக கவனம் செல இரு நிகழ்ச்சிகளில், சமீபத்தில் தொடர்பை குறிப்பிடுவதற்கான ஒரு புள்ளியின் மதிப்பை கற்றுக் கொண்டு வலுவான முட பிறகு இது ஒரு உறவு வகுப்பாளருக்கு உள்ளீட்டாக பயன்படுத்தப்படுகிறது. ஆனால், ஜோடி ஜோடி மருத்துவ பொருள்களுக்கிடையே உரையில் கூறிய தொடர்பு புள்ளியின் மதிப்பில் பிடிக்கப்படுவதை விட அதிகம இந்த பிரச்சனையை முகவரிக்க, நாம் ஒரு சமீபத்தில் மாறிய மாதிரி மாதிரியை ஒரு பிரிவியலாக flexible distribution வழங்கும் பொருள் ஜோடிக்க கூடுதலாக, எங்கள் மாதிரி ஒரு ஒருங்கிணைக்கப்பட்ட கட்டுப்பாடுகளை குறிப்பிட்ட மட்டத்திற்கும் இரண்டு மட்டத்தின் தொ நாம் எங்கள் மாதிரி முடிவுகள் பெறுகிறது என்று காட்டுகிறோம் என்றால் இருவரும் பணிகளுக்கும் வலிமையான அடிப்படைகளுடன் போராடும் மு நாங்கள் எங்கள் குறியீடு பொதுவாக கிடைக்கும்.', 'sv': 'Att utvinna biomedicinska relationer från stora mängder vetenskapliga dokument är en utmanande uppgift att bearbeta naturligt språk. Befintliga metoder fokuserar vanligtvis på att identifiera en relation antingen i en enda mening (omnämningsnivå) eller över en hel korpus (parnivå). I båda fallen har de senaste metoderna uppnått starka resultat genom att lära sig en punktuppskattning för att representera relationen; Detta används sedan som indata till en relationsklassifierare. Relationen uttryckt i text mellan ett par biomedicinska enheter är dock ofta mer komplex än vad som kan fångas av en punktuppskattning. För att ta itu med denna fråga föreslår vi en latent variabel modell med en godtyckligt flexibel fördelning för att representera förhållandet mellan ett entitetspar. Dessutom ger vår modell en enhetlig arkitektur för både omnämningsnivå och parnivå relation extraktion. Vi visar att vår modell uppnår resultat konkurrenskraftiga med starka baslinjer för båda uppgifterna samtidigt som den har färre parametrar och är betydligt snabbare att träna. Vi gör vår kod tillgänglig för allmänheten.', 'si': 'විද්\u200dයාත්මක විද්\u200dයාත්මක සම්බන්ධ විද්\u200dයාත්මක සම්බන්ධතාවක් නික්\u200dරියා කරන්නේ ප්\u200dරශ්ණ භාෂාව ප ඉතින් ඉතින් ප්\u200dරවේශය සාමාන්\u200dයයෙන්ම සංවේශයක් එක්ක වාක්ය (කියන්න- මට්ටම) නැත්තම් සම්බන්ධයක් හඳුන්න සම්බන්ධ දෙන්නම විදියට, අලුත් විදියට ශක්තිමත් ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරති මේක පස්සේ විශේෂකයෙක් සම්බන්ධතාවක් වෙනුවෙන් ප්\u200dරවිච්චි වෙනුවෙන්. නමුත්, බියෝවිස්ත්\u200dරීය විද්\u200dයාවක් සම්බන්ධතාවක් අතර පැත්තට පිළිගත්ත සම්බන්ධතාවක් වඩා වඩා සංශ්\u200dය වි මේ ප්\u200dරශ්නයක් විස්තර කරන්න, අපි ප්\u200dරශ්නයක් තියෙන්නේ අන්තිම වෙනස් මෝඩේල් එකක් සමග සිද්ධ වෙන්න ප්\u200dරශ්නයක් තිය තවත්, අපේ මෝඩල් එක්ක ස්ථාපනයක් ප්\u200dරවර්තනය කරනවා කියලා කියන්න පුළුවන් ස්ථාපනයක් සහ සම්බන්ධය සම් අපි පැහැදිලි කරනවා අපේ මොඩේල් එක්ක ප්\u200dරතිචාරයක් ප්\u200dරතිචාරයක් ලැබෙනවා කියලා බලාපොරොත්තු මූලින් එක්ක දෙන් අපි අපේ කෝඩ් ප්\u200dරතිකාරයෙන් ප්\u200dරවේශ කරන්න.', 'ur': 'سائنس ڈیکومٹن کے بڑے سائنس کورپور سے بیویڈیسی رابطہ نکالنا ایک مشکل طبیعی زبان پردازی کا کام ہے. Existing approaches usually focus on identifying a relationship either in a single sentence (mention-level) or in a whole corpus (pair-level). دونوں قسموں میں، اچھے طریقے پر مضبوط نتیجے پہنچ گئے ہیں، ایک نقطہ کا ارتباط معرفی کرنے کے لئے ایک نقطہ کا ارتباط سیکھنے کے ذریعہ، اس کے بعد یہ ایک رابطہ کلیسٹر کے لئے اپنا انپیٹ بنا کر استعمال کیا جاتا ہے۔ However, the relationship expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. اس مسئلہ کے بارے میں ہم ایک لاٹینٹ متغیر موڈل کی پیشنهاد کرتے ہیں کہ ایک واحد جوڑے کے درمیان رابطہ کا نمایش کریں۔ اور اضافہ بھی، ہماری مدل ایک متحدہ معماری پیش کرتا ہے جتنا ذکر سطح اور جفت سطح کے ارتباط اٹھانے کے لئے۔ ہم دکھاتے ہیں کہ ہماری مدل دونوں کاموں کے لئے مضبوط بنسٹ لینوں کے ساتھ نتیجے حاصل کرتے ہیں جب کہ کم پارامتر ہوتے ہیں اور بہت تیز تر ترین ہوتے ہیں۔ ہم اپنے کوڈ کو ظاہر طور پر موجود بناتے ہیں.', 'uz': "Ilmiy hujjatlardan katta bo'lgan biomediya aloqalarini ajratish bu tabiiy tilni boshqarish vazifa. Mavjud usullar oddiy bir so'z (taʼminlovchi darajada) yoki butun kompyuter (ikkita darajada) bilan aloqani aniqlash uchun foydalanadi. Ikkita holatda, yangi usullar munosabatlarni tashkil qilishni o'rganish mumkin; keyin buni bir munosabatlar uchun ishlatiladi. Lekin, bir necha bo'lgan biomedical tibbiy narsa bir necha bog'liq qiymatdan ham murakkab bo'ladi. Bu muammani boshqarish uchun biz bir nechta fleksiblik tarqatish modeli bilan boshlanamiz Qo'shimcha, modelimizning taʼminlovchi darajaga va ikkita darajaga ulanish uchun bir bir tenglar arxituvni yaratadi. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train.  Biz kodlash shaxsiy imkoniyatlarimiz.", 'vi': 'Việc khai thác các quan hệ sinh học từ chủ tịch lớn của các tài liệu khoa học là một thử thách trong việc xử lý ngôn ngữ tự nhiên. Các phương pháp tồn tại thường tập trung vào việc xác định mối quan hệ trong một câu (mức chỉ tên) hoặc trong cả một tập thể (mức cặp). Trong cả hai trường hợp, các phương pháp gần đây đã đạt được kết quả mạnh nhờ học được một dự báo điểm để đại diện cho quan hệ. cái này được dùng để nhập vào một người phân loại liên quan. Tuy nhiên, quan hệ trong văn bản giữa hai thực thể sinh học thường phức tạp hơn nhiều so với một dự đoán điểm. Để giải quyết vấn đề này, chúng tôi đề xuất một mô hình biến thần bí có một phân phối linh động linh hoạt linh hoạt có thể đại diện cho mối quan hệ giữa một cặp thực thể. Thêm nữa, mô hình của chúng tôi cung cấp một kiến trúc thống nhất cho cả mức nhắc và mức liên hệ hai. Chúng tôi chứng minh rằng mẫu của chúng tôi đạt được kết quả cạnh tranh với những đường hầm mạnh cho cả hai công việc, trong khi có ít thông số hơn và nhanh hơn để huấn luyện. Chúng tôi công khai mã của mình.', 'bg': 'Извличането на биомедицински отношения от големи корпуси от научни документи е предизвикателна задача за обработка на естествения език. Съществуващите подходи обикновено се фокусират върху идентифицирането на връзка или в едно изречение (ниво споменаване) или в целия корпус (ниво двойка). И в двата случая последните методи са постигнали силни резултати чрез изучаване на точкова оценка, която да представлява връзката; след това се използва като вход към класификатор на релации. Въпреки това, връзката, изразена в текст между двойка биомедицински субекти, често е по-сложна, отколкото може да бъде уловена чрез точкова оценка. За да се справим с този проблем, предлагаме латентен модел на променливи с произволно гъвкаво разпределение, което да представлява връзката между двойка субекти. Освен това нашият модел осигурява единна архитектура както за извличане на релации на ниво споменаване, така и на ниво двойка. Ние демонстрираме, че нашият модел постига конкурентни резултати със силни базови линии и за двете задачи, като същевременно има по-малко параметри и е значително по-бърз за обучение. Правим кода си публично достъпен.', 'hr': 'Izvlačenje biomedicinskih odnosa iz velike tijela znanstvenih dokumenta je izazovni zadatak obrade prirodnog jezika. Postojeći pristupi obično se fokusiraju na identifikaciju odnosa ili u jednoj rečenici (nivou spomena) ili preko cijelog korpusa (nivoa parova). U oba slučaja, nedavne metode postigle su jake rezultate učeći procjenu tačke za predstavljanje odnosa; to se onda koristi kao ulaz do povezanog klasifikatora. Međutim, odnos izražen u tekstu između par biomedicinskih subjekta često je složeniji nego što se može uhvatiti procjenom točke. Za rješavanje ovog pitanja predlažemo zadnji model promjene s proizvoljno fleksibilnim distribucijom da predstavljamo odnos između parova entiteta. Osim toga, naš model pruža ujedinjenu arhitekturu za izvlačenje odnosa na razini spominjanja i razini par. Pokazujemo da naš model postigne rezultate konkurentne sa jakim osnovnim linijama za obje zadatke dok ima manje parametara i značajno brže trenirati. Objavljujemo naš kod.', 'da': 'At udtrække biomedicinske relationer fra store korpora af videnskabelige dokumenter er en udfordrende naturlig sprogbehandling opgave. Eksisterende tilgange fokuserer normalt på at identificere en relation enten i en enkelt sætning (omtale-niveau) eller på tværs af en hel korpus (par-niveau). I begge tilfælde har de seneste metoder opnået stærke resultater ved at lære et punktestimat, der repræsenterer relationen; Dette bruges derefter som input til en relationsklassificator. Men det forhold, der udtrykkes i tekst mellem et par biomedicinske enheder, er ofte mere komplekst, end man kan fange ved et punktestimat. For at løse dette problem foreslår vi en latent variabel model med en vilkårligt fleksibel fordeling, der repræsenterer relationen mellem et entitetspar. Desuden giver vores model en samlet arkitektur til både nævnelsesniveau og parniveau relationsudtrækning. Vi demonstrerer, at vores model opnår resultater konkurrencedygtige med stærke baselines for begge opgaver, samtidig med at vi har færre parametre og er betydeligt hurtigere at træne. Vi gør vores kode offentligt tilgængelig.', 'nl': 'Het extraheren van biomedische relaties uit grote corpora wetenschappelijke documenten is een uitdagende natuurlijke taalverwerkingstaak. Bestaande benaderingen richten zich meestal op het identificeren van een relatie in één zin (vermeldingsniveau) of over een geheel corpus (pair-level). In beide gevallen hebben recente methoden sterke resultaten bereikt door een puntschatting te leren om de relatie weer te geven; Dit wordt dan gebruikt als invoer voor een relatieklassifier. De relatie die in tekst wordt uitgedrukt tussen een paar biomedische entiteiten is echter vaak complexer dan met een puntschatting kan worden vastgelegd. Om dit probleem aan te pakken, stellen we een latent variabele model voor met een willekeurig flexibele verdeling om de relatie tussen een entiteitspaar weer te geven. Bovendien biedt ons model een uniforme architectuur voor zowel het extraheren van relaties op vermeldingsniveau als het extraheren van relaties op parenniveau. We tonen aan dat ons model concurrerende resultaten behaalt met sterke basislijnen voor beide taken, terwijl het minder parameters heeft en aanzienlijk sneller te trainen is. We maken onze code openbaar.', 'de': 'Biomedizinische Beziehungen aus großen Korpora wissenschaftlicher Dokumente zu extrahieren, ist eine herausfordernde Aufgabe der Verarbeitung natürlicher Sprache. Bestehende Ansätze konzentrieren sich in der Regel darauf, eine Beziehung entweder in einem einzigen Satz (Erwähnungsebene) oder über einen ganzen Korpus (Paarebene) zu identifizieren. In beiden Fällen haben die jüngsten Methoden starke Ergebnisse erzielt, indem sie eine Punktschätzung gelernt haben, um die Beziehung darzustellen; Dies wird dann als Eingabe für einen Relationsklassifikator verwendet. Allerdings ist die im Text ausgedrückte Beziehung zwischen einem Paar biomedizinischer Entitäten oft komplexer, als durch eine Punktschätzung erfasst werden kann. Um dieses Problem anzugehen, schlagen wir ein latentes Variablenmodell mit einer beliebig flexiblen Verteilung vor, um die Beziehung zwischen einem Entitätspaar darzustellen. Darüber hinaus bietet unser Modell eine einheitliche Architektur für die Beziehungsextraktion auf Erwähnungs- und Paarebene. Wir zeigen, dass unser Modell wettbewerbsfähige Ergebnisse mit starken Basislinien für beide Aufgaben erzielt, gleichzeitig weniger Parameter hat und deutlich schneller trainiert werden kann. Wir machen unseren Code öffentlich zugänglich.', 'ko': '대량의 과학 문헌에서 생물의학 관계를 추출하는 것은 도전적인 자연 언어 처리 임무이다.기존의 방법은 보통 한 문장(언급 차원)이나 전체 어료 라이브러리(배합 차원)의 관계를 식별하는 데 중심을 두었다.이 두 가지 상황에서 최근의 방법은 학습점 평가를 통해 관계를 나타내고 좋은 효과를 거두었다.그런 다음 관계 분류기의 입력으로 사용합니다.그러나 한 쌍의 생물의학 실체 간의 텍스트 관계는 종종 예측할 수 있는 것보다 더 복잡하다.이 문제를 해결하기 위해 우리는 임의로 유연하게 분포하는 잠재 변수 모델을 제시하여 실체 대 간의 관계를 나타냈다.그 밖에 우리의 모델은 언급급과 대급 관계 추출에 통일된 체계 구조를 제공했다.우리는 우리의 모델이 두 가지 임무에서 모두 강기선과 경쟁하는 결과를 얻었고 파라미터가 비교적 적으며 훈련 속도가 현저히 빨라졌다는 것을 증명했다.우리는 우리의 코드를 공개한다.', 'fa': 'اخراج رابطه\u200cهای بیولوپزشکی از شرکت بزرگ سند علمی یک کار پرداخت زبان طبیعی سخت است. نزدیک\u200cهای موجود معمولاً روی شناسایی رابطه یا در یک جمله (سطح یادآوری) یا در یک سطح (سطح جفت) تمرکز می\u200cکند. در هر دو مورد، روش\u200cهای اخیر به نتیجه\u200cهای قوی با یاد گرفتن تخمین نقطه\u200cای برای نمایش ارتباط رسیده\u200cاند. این سپس به عنوان ورودی به یک رابطه\u200cگر استفاده می\u200cشود. ولی ارتباطی که در متن توضیح داده می شود بین یک جفت شرکت بیولوپزشکی اغلب پیچیده تر از آن است که می تواند با تخمین نقطه گرفته شود. برای حل این مسئله، ما یک مدل تغییر اخیر را پیشنهاد می\u200cکنیم با یک توزیع قابل flexible به طور قابل توزیع برای نمایش ارتباط بین یک جفت entity. علاوه بر این، مدل ما یک معماری متحد برای استخراج از سطح یادآوری و سطح جفت را پیشنهاد می\u200cکند. ما نشان می دهیم که مدل ما نتیجه های مسابقه با خطوط بنیادی قوی برای هر دو کار در حالی که پارامترهای کمتر و بسیار سریع تر برای تمرین است. ما کد خودمون رو به طور عمومي در دسترسي ميکنيم', 'id': 'Ekstraksi hubungan biomedis dari korpora besar dokumen ilmiah adalah tugas memproses bahasa alam yang menantang. Pendekatan yang ada biasanya fokus pada mengidentifikasi hubungan baik dalam satu kalimat (tingkat-sebutan) atau di seluruh tubuh (tingkat-pasangan). Dalam kedua kasus, metode baru-baru ini telah mencapai hasil yang kuat dengan mempelajari perhitungan titik untuk mewakili hubungan; ini kemudian digunakan sebagai input ke klasifikasi hubungan. Namun, hubungan yang diungkapkan dalam teks antara pasangan entitas biomedis sering lebih kompleks daripada dapat ditangkap oleh perhitungan titik. Untuk mengatasi masalah ini, kami mengusulkan model variabel latent dengan distribusi fleksibel secara arbitrar untuk mewakili hubungan antara pasangan entitas. Selain itu, model kami menyediakan arsitektur yang bersatu untuk ekstraksi hubungan tingkat-sebut dan tingkat-pasangan. Kami menunjukkan bahwa model kami mencapai hasil kompetitif dengan garis dasar yang kuat untuk kedua tugas sementara memiliki lebih sedikit parameter dan lebih cepat untuk berlatih. Kami membuat kode kami tersedia publik.', 'sw': 'Kutoa mahusiano ya kitabibu kutoka makampuni makubwa ya nyaraka za kisayansi ni kazi yenye changamoto ya upasuaji wa lugha za asili. Matokeo yanayopo mara nyingi huwa yanalenga kutambua uhusiano au katika neno moja (kutaja kiwango cha kutambua) au kupitia chombo chote (kiwango cha viwili). Katika hali mbili hizi, mbinu za hivi karibuni zimefanika matokeo makubwa kwa kujifunza hatua inayokadiriwa kuwakilisha uhusiano; hii inatumiwa kama kituo cha kuhusiana. Hata hivyo, uhusiano uliotolewa katika maandishi kati ya vitu viwili vya kitabibu mara nyingi ni tatizo kuliko inavyokadiriwa. Ili kushughulikia suala hili, tunapendekeza muundo wa mabadiliko ya hivi karibuni na usambazaji wa kiutaratibu wa kidini wa kuwakilisha uhusiano kati ya wanandoa wa entity. Zaidi ya hayo, modeli yetu inatoa muundo wa muungano wa pamoja kwa ajili ya kuondolewa kwa kiwango cha kutajwa na kiwango cha mahusiano mawili. Tunaonyesha kuwa mifano yetu inafanikiwa matokeo ya kushindana na misingi yenye nguvu kwa kazi zote wakati na kuwa na parameter wachache na kuwa haraka sana kwa mafunzo. Tunafanya sheria zetu kwa uwazi.', 'sq': 'Ekstraktimi i marrëdhënieve biomedice nga korpora e madhe dokumentesh shkencore është një detyrë sfiduese e procesimit natyror të gjuhës. Përqasjet ekzistuese zakonisht përqëndrohen në identifikimin e një marrëdhënie ose në një fjali të vetme (niveli përmendimi) ose nëpër një trup të tërë (niveli çifti). Në të dy rastet, metodat e fundit kanë arritur rezultate të forta duke mësuar një vlerësim pike për të përfaqësuar marrëdhënien; kjo pastaj përdoret si hyrje në një klasifikues lidhjesh. Megjithatë, marrëdhënia e shprehur në tekst midis një çifti njësive biomedike është shpesh më komplekse sesa mund të kapet nga një vlerësim pike. Për të trajtuar këtë çështje, ne propozojmë një model të ndryshueshëm të fshehtë me një shpërndarje arbitrarisht fleksible për të përfaqësuar marrëdhënien midis një çifti njësie. Përveç kësaj, modeli ynë ofron një arkitekturë të unifikuar si për nxjerrjen e nivelit të përmendur ashtu dhe të nivelit të çiftit të marrëdhënieve. Ne demonstrojmë se modeli ynë arrin rezultate konkurruese me linja bazë të forta për të dy detyrat duke pasur më pak parametra dhe duke qenë ndjeshëm më të shpejtë për të trajnuar. Ne e bëjmë kodin tonë të disponueshëm publikisht.', 'tr': 'Ilmi senedlerin uly korporatyndan biomedical baglaşyklary a çmak tebigy dil işlemegi aňsat däldir. Daşary ýakynlaşylar adatça bir sözlemde ýa-da bir baglaýyşy tanamak üçin fokus berýär (çift derejesi). Iki ýagdaýda, iň soňky yöntemler baglaýyşyny tanamak üçin güýçli netijelere ýetdi; bu soňra klasifikatçy bilen giriş hökmünde ullanýar. Ýöne, biomedical birnäçe guramlaryň arasynda metin edip görkezilýän baglaýyşy birnäçe sany görä almakdan köplenç karmaşık. Bu meseleyi çözmek için, bir tek çift arasındaki ilişkileri ifade etmek için son değişkenli bir modeli öneririz. Munuň üçin biziň modelimiz agzalan derejesi we çift derejesi bir arhitektura üýtgedýär. Biziň nusgymyzyň hem işiň üçin güýçli baselinikler bilen ýakynlaşyklygyny görkezýäris. Bu nusgymyz düşür parameterlerimiz bolup, we otlamak üçin örän çalt bolup ýöräni görkezýäris. Biz öz kodymyzy publika mümkin edip bilýäris.', 'af': "Die uitpakking van biomediese relasies van groot korpora van wetenskaplike dokumente is 'n pragtige natuurlike taal verwerking taak. Bestaande toegang is gewoonlik fokus op die identifiseer van 'n verwanting of in 'n enkele seting (mention-vlak) of oor' n hele korpus (paar-vlak). In beide gevalle het onlangse metodes sterke resultate bereik deur 'n punt-estimatie te leer om die verwanting te verteenwoordig; hierdie word dan gebruik as die invoer na 'n verwanting klassifiseerder. Maar die verwanting wat in teks uitgevoer is tussen 'n paar biomediese entiteite is dikwels meer kompleks as kan gevang word deur 'n punt estimatie. Om hierdie probleem te adres, voorstel ons 'n latente veranderlike model met 'n willekeurige fleksibel verspreiding om die verwanting tussen 'n entiteite paar te verteenwoordig. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. Ons wys dat ons model resultate verkry met sterke basis lyne vir beide opdragte terwyl minder parameters het en betekenlik vinniger om te trein. Ons maak ons kode openlik beskikbaar.", 'am': 'የbiomedical ግንኙነት ከትልቁ የሳይንቀሳዊ ሰነዶች ማውጣት የፍጥረት ቋንቋ ማቀናጃ ሥራ ነው፡፡ የአሁኑን ግንኙነት በአንድ ቃላት (ታሪክ-ደረጃ) ወይም በሙሉ ካርፓስ (ሁለት-ደረጃ) ላይ የሚታወቅ ግንኙነትን በመጠቀም ላይ ትክክል ያደርጋል፡፡ በሁለቱም ጉዳይ፣ የቅርብ ሥርዓት ግንኙነትን ለመቆጣጠር ጥሩ ፍሬዎችን አግኝተዋል፡፡ this is then used as the input to a relation classifier.  ነገር ግን በሁለት ባሕርያዊ አካባቢዎች መካከል የጽሑፍ ግንኙነት በብዙ ጊዜ ከአስቸጋሪው መጠቀም ይጨመርበታል፡፡ ለዚህ ጉዳይ ለመቀበል፣ አካባቢ ሁለት መካከል ግንኙነት ለመቆጣጠር በተለየ የልዩ ሞዴል እናሳውቃለን፡፡ በተጨማሪም፣ ሞዴላያችን የቁጥጥር ደረጃ እና የሁለት ደረጃ ግንኙነት ማውጣት የተጠቃሚ መሠረት ይሰጣል፡፡ ምሳሌያችን በሁለቱ ስራዎች ላይ የበረታች መሠረት መሆኑን እናሳያቸዋለን፡፡ የኮድራችንን ግልፅ እናደርጋለን፡፡', 'hy': 'Բիոբիոբժշկական հարաբերությունները վերացնելը գիտական փաստաթղթերի մեծ կառուցվածքից դժվար է բնական լեզվի վերամշակման խնդիր: Գոյություն ունեցող մոտեցումները սովորաբար կենտրոնանում են հարաբերության հայտնաբերման վրա կամ մեկ նախադասության մեջ (նշելու մակարդակի) կամ ամբողջ կորպուսի մեջ (զույգ մակարդակի): Երկու դեպքում վերջին մեթոդները հասել են ուժեղ արդյունքներին, սովորելով հարաբերությունը ներկայացնելու կետի գնահատականը: ապա այն օգտագործվում է որպես հարաբերությունների դասակարգիչը: Այնուամենայնիվ, տեքստում արտահայտված հարաբերությունը կենսաբժշկական մի զույգի միջև հաճախ ավելի բարդ է, քան կարող է գրանցվել կետի գնահատման արդյունքում: Այս խնդիրը լուծելու համար մենք առաջարկում ենք թաքնված փոփոխական մոդել, որը կամավորապես ճկուն բաշխման միջոցով է, որպեսզի ներկայացնի էության զույգի հարաբերությունը: Ավելին, մեր մոդելը ստեղծում է միասնական ճարտարապետություն, որպեսզի բացահայտենք երկու մակարդակի հարաբերությունները: Մենք ցույց ենք տալիս, որ մեր մոդելը ստանում է արդյունքներ, որոնք մրցակցում են ուժեղ հիմնական գծերով երկու խնդիրների համար, մինչդեռ ավելի քիչ պարամետրեր ունեն և ավելի արագ է վարժվում: Մենք մեր կոդը հասանելի ենք դարձնում:', 'bn': 'বিজ্ঞানিক নথি থেকে বায়োমেডিকেল সম্পর্ক বের করা হচ্ছে প্রাকৃতিক ভাষা প্রক্রিয়ার কাজ। বিদ্যমান পদ্ধতি সাধারণত একটি বাক্যে (উল্লেখ-স্তর) অথবা পুরো কোর্পাস (জোড়া স্তরে) পাশে একটি সম্পর্ক চিহ্নিত করার প্রতি মনোযোগ দুটো ক্ষেত্রে সম্পর্কের প্রতিনিধিত্বের মাধ্যমে সাম্প্রতিক পদ্ধতি শক্তিশালী ফলাফল অর্জন করেছে; this is then used as the input to a relation classifier.  তবে বায়োমেডিকেল বিষয়গুলোর মধ্যে প্রকাশিত লেখার সম্পর্ক প্রায়শ কঠিন বিষয়টি একটি বিন্দুর হিসেবে গ্রেফতার করা য এই বিষয়টি নিয়ে কথা বলার জন্য আমরা একটি সাম্প্রতিক ভেরিয়েল মডেল প্রস্তাব করছি যার মধ্যে একটি প্রচণ্ড ফ্লিক্সিবল বিতরণের প্রস্তাব করে  তাছাড়াও, আমাদের মডেল উল্লেখের স্তর এবং জোড়া স্তরের সম্পর্ক বের করার জন্য একটি একত্রিত আর্কিটেক্টার প্রদান করে। আমরা দেখাচ্ছি যে আমাদের মডেল দুই কাজের জন্য প্রতিযোগিতায় প্রতিযোগিতার ফলাফল অর্জন করেছে যেখানে প্যারামিটার কম থাকে এবং ট্রেনের আমরা আমাদের কোড প্রকাশ্যে পাওয়া যাচ্ছি।', 'bs': 'Izvlačenje biomedicinskih odnosa iz velike korporacije naučnih dokumenta je izazovni zadatak obrade prirodnog jezika. Postojeći pristupi obično se fokusiraju na identifikaciju odnosa ili u jednoj rečenici (nivou spomena) ili preko cijelog korpusa (nivoa par). U oba slučaja, nedavne metode postigle su jake rezultate učeći procjenu tačke za predstavljanje odnosa; ovo se onda koristi kao ulaz u povezanje sa klasifikatorom. Međutim, odnos izražen u tekstu između par biomedicinskih entiteta često je kompleksniji nego što se može uhvatiti procjenom tačke. Za rješavanje ovog pitanja predlažemo zadnji model varijanta sa proizvoljno fleksibilnim distribucijom da predstavljamo odnos između parova entiteta. Osim toga, naš model pruža ujedinjenu arhitekturu za izvlačenje odnosa na nivou spominjanja i na nivou par. Pokazujemo da naš model postigne rezultate konkurentne sa jakim osnovnim linijama za obje zadatke dok ima manje parametara i značajno brže trenirati. Objavljujemo naš kod.', 'ca': "Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task.  Els enfocaments existents normalment es centren en identificar una relació en una sola frase (nivell de menció) o a través d'un cos sencer (nivell de parell). En ambdós casos, els mètodes recents han aconseguit resultats forts aprenent una estimació puntual que represent a la relació; això es fa servir com a entrada a un classificador de relacions. Però la relació expressa en text entre un parell d'entitats biomèdiques és sovint més complex a del que pot ser capturada per una estimació puntual. Per abordar aquest tema, proposem un model variable latent amb una distribució arbitrariament flexible per representar la relació entre un parell d'entitats. A més, el nostre model proporciona una arquitectura unificada per a l'extracció tant de nivell de menció com de nivell de parell. Demonstrem que el nostre model aconsegueix resultats competitius amb línies de base fortes per ambdues tasques mentre té menys paràmetres i és significativament més ràpid d'entrenar. Fem públic el nostre codi.", 'az': 'Büyük bilimsel belələrin korporasından biomedicin ilişkilerini çıxartmaq təbiətli dil işləməsi işidir. Existing yaxınlıqlar genellikle bir cümlədə və ya bütün bir korpus (iki səviyyədə) arasında bir bağlantı tanımmağa odaqlanır. İkisində də, son metodlar ilişkisini göstərmək üçün çox qüvvətli sonuçları öyrənib müəyyən etdilər. bu səviyyənin qohumluğu kimi istifadə edilir. Ancaq bir nöqtə təcrübəsi ilə yaxınlaşdırılabilir. Bu məsələdən çəkinmək üçün, bir növ çift arasındakı ilişkisini göstərmək üçün müəyyən bir dəyişiklik modeli təklif edirik. Əksinə, modellərimiz hər ikisini də yada salmaq üçün birləşdirilmiş arhitektir. Bizim modellərimiz hər iki işin üçün qüvvətli sətir ilə müqayisədə sonuçlarını daha az parametru və təhsil etmək üçün daha hızlı olaraq göstəririk. Biz kodumuzu açıq-aşkar mümkün edirik.', 'cs': 'Extrahování biomedicínských vztahů z velkých korpusů vědeckých dokumentů je náročným úkolem zpracování přirozeného jazyka. Stávající přístupy se obvykle zaměřují na identifikaci vztahu buď v jedné větě (úroveň zmínky) nebo napříč celým korpusem (úroveň dvojice). V obou případech dosáhly nedávné metody silných výsledků tím, že se naučily bodový odhad reprezentovat vztah; toto je pak použito jako vstup do klasifikátoru vztahů. Nicméně vztah vyjádřený v textu mezi dvojicí biomedicínských entit je často složitější, než lze zachytit bodovým odhadem. Pro řešení tohoto problému navrhujeme model latentních proměnných s libovolně flexibilním rozdělením, který reprezentuje vztah mezi entitním párem. Navíc náš model poskytuje jednotnou architekturu pro extrakci vztahů na úrovni zmínky i na úrovni dvojice. Dokazujeme, že náš model dosahuje výsledků konkurenceschopných se silnými základními liniemi pro obě úkoly, přičemž má méně parametrů a je výrazně rychlejší trénovat. Zveřejňujeme náš kód.', 'et': 'Biomeditsiiniliste suhete väljavõtmine teadusdokumentide suurtest korpustest on keeruline looduskeele töötlemise ülesanne. Olemasolevad lähenemisviisid keskenduvad tavaliselt seose tuvastamisele kas ühes lauses (mainimise tasandil) või kogu korpuses (paari tasandil). Mõlemal juhul on hiljutised meetodid saavutanud tugevaid tulemusi, õppides suhet esindava punktihinnangu; Seda kasutatakse seejärel seoste klassifitseerija sisendina. Kuid tekstis väljendatud seos biomeditsiiniliste üksuste vahel on sageli keerulisem, kui punktihinnanguga võib jäädvustada. Selle probleemi lahendamiseks pakume välja latentse muutujate mudeli, millel on meelevaldselt paindlik jaotus, et esindada olemipaari suhet. Lisaks pakub meie mudel ühtset arhitektuuri nii mainimistaseme kui ka paaritaseme suhete ekstraheerimiseks. Näitame, et meie mudel saavutab konkurentsivõimelised tulemused, tugevad lähtejooned mõlema ülesande jaoks, samas kui sellel on vähem parameetreid ja on oluliselt kiirem treenida. Me teeme oma koodi avalikult kättesaadavaks.', 'fi': 'Biolääketieteellisten suhteiden poimiminen suurista tieteellisistä asiakirjoista on haastava luonnollisen kielen käsittelytehtävä. Olemassa olevat lähestymistavat keskittyvät yleensä suhteeseen joko yhdessä lauseessa (mainintataso) tai koko korpusessa (paritaso). Molemmissa tapauksissa viimeaikaisilla menetelmillä on saavutettu vahvoja tuloksia oppimalla suhdetta kuvaava pistearvio. Tätä käytetään syötteenä relaatioluokittelijaan. Tekstissä ilmaistu suhde biolääketieteellisten kokonaisuuksien välillä on kuitenkin usein monimutkaisempi kuin pistearviolla voidaan kuvata. Tämän ongelman ratkaisemiseksi ehdotamme piilevää muuttujamallia, jossa on mielivaltaisesti joustava jakauma edustamaan entiteettiparin välistä suhdetta. Lisäksi mallimme tarjoaa yhtenäisen arkkitehtuurin sekä mainintatason että paritason suhteen purkamiseen. Osoitamme, että mallimme saavuttaa kilpailukykyisiä tuloksia vahvoilla lähtölinjoilla molemmissa tehtävissä, mutta sillä on vähemmän parametreja ja se on huomattavasti nopeampi kouluttaa. Annamme koodimme julkisesti saataville.', 'jv': 'Ngawe nggoleki perusahaan anyar-ingkang dipunanggé ning larang sampeyan anyar tentang kanggo ngilangno nggawe barang. echoH e l l o space w o r l d periodHelloworldHello worldkey echo Rasané awak dhéwé, dadi sing ngerasakno dadi iki bakal kelangan anyir bagian nggawe barang nggawe barang nggawe gerakan wigat; set Nanging, ngregani uwong kelalaman itong mungkin katimbang kelas nang sampeyan biyo dipunangé luwih apik sing katon kaya tau nguasai nêmên. string" in "context_BAR_stringLink Label Awak dhéwé éntuk sawetara dadi model nambarang nggawe dolanan ambekan iki dadi wis luwih apik dadi, nik awak dhéwé parameters lan tambah bantêr susahé awak dhéwé. Awak dhéwé ngewehke kodem nang publika.', 'sk': 'Izvlečevanje biomedicinskih odnosov iz velikih korpusov znanstvenih dokumentov je zahtevna naloga obdelave naravnega jezika. Obstoječi pristopi se običajno osredotočajo na identifikacijo odnosa bodisi v enem stavku (raven omenjanja) bodisi v celotnem korpusu (raven parov). V obeh primerih so nedavne metode dosegle močne rezultate z učenjem točkovne ocene, ki predstavlja odnos; to se nato uporabi kot vhod v razvrščevalnik relacij. Vendar pa je odnos, izražen v besedilu med parom biomedicinskih entitet, pogosto bolj kompleksen, kot ga je mogoče zajeti s točkovno oceno. Za reševanje tega vprašanja predlagamo latentni model spremenljivk s poljubno fleksibilno porazdelitvijo, ki predstavlja razmerje med parom entitet. Poleg tega naš model zagotavlja enotno arhitekturo za ekstrakcijo relacij na ravni omenjanja in pare. Dokazujemo, da naš model dosega konkurenčne rezultate z močnimi osnovnimi linijami za obe nalogi, hkrati pa ima manj parametrov in je bistveno hitrejši za usposabljanje. Naša koda je javno dostopna.', 'he': 'Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task.  גישות קיימות בדרך כלל מתמקדות בזיהוי יחסים בין משפט אחד (רמת הזכרות) או ברחבי קופוס שלם (רמת זוגות). בשני המקרים, השיטות האחרונות השיגו תוצאות חזקות על ידי ללמוד הערכה נקודה לייצג את היחסים; this is then used as the input to a relation classifier.  עם זאת, היחסים המוביעים בטקסט בין זוג יחידות ביולורפואית הם לעתים קרובות יותר מורכבים מאשר יכולים להיתפס על ידי הערכה נקודה. כדי להתמודד עם הנושא הזה, אנו מציעים מודל משתנה מוסתר עם פיצוח גמיש באופן arbitrary כדי לייצג את היחסים בין זוג יחידות. בנוסף, הדוגמא שלנו מספקת ארכיטקטורה מאוחדת גם לרמה הזכרה וגם לחלץ יחסים ברמה זוג. אנחנו מראים שהמודל שלנו משיג תוצאות תחרותיות עם קווי בסיס חזקים לשני המשימות בזמן שיש פחות פרמטרים ולהיות מהיר יותר באופן משמעותי לאימון. אנחנו פונים את הקוד שלנו לפומבי.', 'ha': "Akwai fitar da dangantaka na'asa daga makampuni mai girma na takardar littafan da na sani, shi ne wani abu mai ƙaranci ga aiki na fassarar harshen farko. Yi amfani da ake kai yanzu a ɗabi'a, yana zura ido a kan ka gane wata haɗi ko kuma a kan wata cire guda (aka faɗa-daraja) ko kan duk nau'in rubutu (daraja biyu). A cikin duk biyu, metoden nan da suka ƙari mafiya ƙaranci matsalar da aka sanar wani point ya yi ƙidãya ga wakin mazauni; wannan na yi amfani da shi zuwa wani mai haɗi. A lokacin da, da aka nuna mazaɓa cikin matsayin a tsakanin wasu abubuwa biyu na biyayyaki ko da yawa za'a kãma shi da wani point. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair.  Ina ƙaranci, misalinmu yana samar da wani matsayi mai daidaita wa masu nuna-daraja da cire-daraja biyu. Tuna nũna cewa misalinmu ya sami matsalar ta sami tarakin mai ƙarfi a kan aikin duk da za'a samu da parameteri kaɗan kuma yana da kasi kashi ga yin tafiyar. Munã samar da kowanmu bayyane.", 'bo': 'ཚན་རིག་གི་ཡིག Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or in a whole body (level) གལ་སྲིད་གཉིས་པ་དང་། མ་འོངས་གྱི་ལམ་ལུགས་འདིས་མཐུན་ལམ་ལུགས་རྐྱེན་བྱས་པ་ལས་མཐུན་རྐྱེན་དང་། འདི་ནས་དབྱེ་བ་དང་འབྲེལ་བ་ཞིག་གི་འཇུག་སྣོད་ཀྱི་འཇུག་སྣོད་ལ་སྤྱོད་སྟེ། ཡིན་ནའང་། འབྲེལ་བ་ནི་ཤིག་བྱས་པར་ཆས་ཀྱི་དབང་ཆ་གཉིས་ཀྱི་དབར་གྱི་འབྲེལ་བ་ནི་རྒྱ་ནག་མི་མཐུན་གྲངས་ཁག་ཞིག་འཛིན་སྲིད། གནད་དོན་འདི་ལ་བཤད་ན་ངེད་ཚོས་གནས་ཚུལ་གྱི་རྣམ་པ་ཞིག་གིས་ཕན་ཚུན་མེད་པའི་བགོ་སྤྲོད་རྒྱུ་དངོས་མཐུན་རྐྱེན་གྱི་རྩིས འོན་ཀྱང་། ང་ཚོའི་མིག་སྔར་སྒྲིག་འཛིན་གྱི་ཚད་རྩིས་ཐུབ་དང་མཐུན་རྐྱེན་གཉིས་ཆ་གཅིག་སྒྲིག་གཏོང་བྱེད། We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. ང་ཚོས་མིའི་རྩིས་ཡིག་ཆ་སྤྱད་པར་མང་ཙམ་ཞིག་འཇུག་ཐུབ་པ'}
{'en': 'End to End Binarized Neural Networks for Text Classification', 'ar': 'نهاية إلى نهاية الشبكات العصبية الثنائية لتصنيف النص', 'fr': 'Réseaux de neurones binarisés de bout en bout pour la classification de texte', 'pt': 'Redes neurais binarizadas de ponta a ponta para classificação de texto', 'es': 'Redes neuronales binarizadas de extremo a extremo para la clasificación de textos', 'ja': 'テキスト分類のためのエンドツーエンド二値化ニューラルネットワーク', 'zh': '文本之端到端二值化神经网络', 'ru': 'Двухсторонние нейронные сети для классификации текста', 'hi': 'पाठ वर्गीकरण के लिए अंत Binarized तंत्रिका नेटवर्क के लिए अंत', 'ga': "Líonraí Néaracha Dénártha d'Aicmiú Téacs ó Dheireadh go Deireadh", 'ka': 'ტექსტის კლასიფიკაციისთვის ბინერალური ნეირალური ქსელების დასრულება', 'el': 'Δίδυμα Νευρικά Δίκτυα για την ταξινόμηση Κειμένου', 'hu': 'Végtől végig binárizált neurális hálózatok a szövegosztályozáshoz', 'it': 'Reti neurali binarizzate end to end per la classificazione del testo', 'mk': 'Крај до крај на бинаризираните неврални мрежи за класификација на текст', 'kk': 'Мәтін классификациясының бинарлық невралдық желілерін аяқтау', 'lt': 'Teksto klasifikavimo dviejų nervinių tinklų pabaiga ir pabaiga', 'ms': 'Name', 'ml': 'പദാവലി ക്ലാസിഷനിക്കുവേണ്ടിയുള്ള നെയുറല്\u200d നെറ്റ്വര്\u200dക്കുകള്\u200d അവസാനിപ്പിക്കുക', 'mt': 'Netwerks Newrali Binarizzati għat-Tmiem sa Tmiem għall-Klassifikazzjoni tat-Test', 'mn': 'Текст классификацийн бинарлын мэдрэлийн сүлжээний төгсгөлд', 'pl': 'Binaryzowane sieci neuronowe od końca do końca dla klasyfikacji tekstu', 'ro': 'Rețele neurale binarizate de la sfârșit la sfârșit pentru clasificarea textelor', 'sr': 'Kraj do kraja binariziranih neuronskih mreža za klasifikaciju teksta', 'no': 'Slutt til slutt binært neuralnettverk for tekstklassifisering', 'si': 'පස්සේ ක්\u200dලාසික්ෂිකරණය සඳහා බිනාරිස් න්\u200dයූරල් ජාලය අවසානයට අවසානය කරන්න', 'so': 'Ku dhamaado shabakadda neurada ee dhamaadka qoraalka', 'sv': 'Binariserade neurala nätverk från slut till slut för textklassificering', 'ur': 'پابندی کے لئے بیناریز نیورال نیٹورک', 'ta': 'Comment', 'uz': 'Comment', 'vi': 'Kết thúc kết thúc các mạng thần kinh nhị phân tách văn bản', 'nl': 'End to End Binarized Neural Networks voor Tekst Classificatie', 'da': 'Binariserede neurale netværk fra slut til slut for tekstklassifikation', 'hr': 'Kraj do kraja binariziranih neuronskih mreža za klasifikaciju teksta', 'ko': '텍스트 분류에 사용되는 단말기부터 단말기까지의 이치화 신경 네트워크', 'bg': 'Бинаризирани нервни мрежи от край до край за класификация на текста', 'id': 'Akhir hingga Akhir Rangkaian Neural Binaris untuk Klasifikasi Teks', 'fa': 'پایان به پایان شبکه\u200cهای عصبی بینایی برای کلاس\u200cسازی متن', 'tr': 'Metin Sözlemek üçin Ahyrlamak üçin Jawanlary Ahyrlamak', 'af': 'Einde na Einde Binariseerde Neurale Netwerke vir Teks Klassifikasie', 'sq': 'Mbaron deri në fund rrjetet nervore të binarizuara për klasifikimin e tekstit', 'de': 'Binarisierte neuronale Netze für die Textklassifizierung', 'az': 'Metin Sınıflaması üçün Binarlı Nöral Ağlarını Bitir', 'sw': 'Mwisho wa Kumaliza Mtandao wa Neural wa Makala', 'bn': 'টেক্সট ক্লাসিফেশনের জন্য বাইনারিজ নেউরাল নেটওয়ার্ক শেষ করা হবে', 'am': 'አዲስ ዶሴ ፍጠር', 'cs': 'Binarizované neuronové sítě od konce do konce pro klasifikaci textu', 'bs': 'Kraj do kraja Binarizirane Neuralne mreže za klasifikaciju teksta', 'et': 'Teksti klassifitseerimiseks binariseeritud neuraalsed võrgud otsast otsani', 'fi': 'End to End Binarized Neural Networks for Text Classification', 'ca': 'End to End Binarized Neural Networks for Text Classification', 'hy': 'End to End Binarized Neural Networks for Text Classification', 'jv': 'FindOK', 'sk': 'Binarizirana živčna omrežja od konca do konca za klasifikacijo besedila', 'ha': '@ action', 'bo': 'End to End Binarized Neural Networks for Text Classification', 'he': 'End to End Binarized Neural Networks for Text Classification'}
{'en': 'Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40 % lesser memory and training time compared to the benchmarks.', 'es': 'Las redes neuronales profundas han demostrado su rendimiento superior en casi todas las tareas de procesamiento del lenguaje natural, sin embargo, su creciente complejidad genera preocupación. Una preocupación particular es que estas redes plantean altos requisitos para el hardware informático y los presupuestos de capacitación. Los modelos de transformadores de última generación son un claro ejemplo. La simplificación de los cálculos realizados por una red es una forma de abordar el problema de la creciente complejidad. En este artículo, proponemos una red neuronal binarizada de extremo a extremo para la tarea de clasificación de intenciones y textos. Para aprovechar al máximo el potencial de la binarización de extremo a extremo, tanto las representaciones de entrada (incrustaciones vectoriales de estadísticas de tokens) como el clasificador se binarizan. Demostramos la eficiencia de una red de este tipo en la clasificación por intención de textos cortos en tres conjuntos de datos y la clasificación de textos con un conjunto de datos más grande. En los conjuntos de datos considerados, la red propuesta logra resultados comparables a los de última generación, al tiempo que utiliza entre un 20 y un 40% menos de memoria y tiempo de entrenamiento en comparación con los puntos de referencia.', 'fr': "Les réseaux de neurones profonds ont démontré leurs performances supérieures dans presque toutes les tâches de traitement du langage naturel, mais leur complexité croissante soulève des préoccupations. Ce qui est particulièrement préoccupant, c'est que ces réseaux présentent des exigences élevées en termes de matériel informatique et de budgets de formation Les modèles de transformateurs de pointe en sont un exemple frappant. La simplification des calculs effectués par un réseau est un moyen de résoudre le problème de la complexité croissante. Dans cet article, nous proposons un réseau neuronal binarisé de bout en bout pour la tâche de classification des intentions et du texte. Afin d'utiliser pleinement le potentiel de binarisation de bout en bout, les représentations d'entrée (intégration vectorielle des statistiques de jetons) et le classificateur sont binarisés. Nous démontrons l'efficacité d'un tel réseau sur la classification intentionnelle de textes courts sur trois ensembles de données et la classification de texte avec un ensemble de données plus important. Sur les ensembles de données considérés, le réseau proposé atteint des résultats comparables aux résultats les plus récents tout en utilisant 20 à 40\xa0% de mémoire et de temps d'entraînement en moins par rapport aux points de référence.", 'pt': 'As redes neurais profundas demonstraram seu desempenho superior em quase todas as tarefas de processamento de linguagem natural, no entanto, sua crescente complexidade levanta preocupações. Uma preocupação particular é que essas redes apresentam altos requisitos para hardware de computação e orçamentos de treinamento. Os modelos de transformadores de última geração são um exemplo vívido. Simplificar os cálculos realizados por uma rede é uma forma de abordar a questão do aumento da complexidade. Neste artigo, propomos uma rede neural binarizada de ponta a ponta para a tarefa de classificação de intenção e texto. Para utilizar plenamente o potencial da binarização de ponta a ponta, tanto as representações de entrada (incorporação de vetores de estatísticas de tokens) quanto o classificador são binarizados. Demonstramos a eficiência de tal rede na classificação de intenção de textos curtos em três conjuntos de dados e classificação de texto com um conjunto de dados maior. Nos conjuntos de dados considerados, a rede proposta alcança resultados comparáveis ao estado da arte ao utilizar 20-40% menos memória e tempo de treinamento em comparação com os benchmarks.', 'ar': 'أظهرت الشبكات العصبية العميقة أدائها المتفوق في كل مهمة معالجة لغة طبيعية تقريبًا ، ومع ذلك ، فإن تعقيدها المتزايد يثير مخاوف. مصدر قلق خاص هو أن هذه الشبكات تفرض متطلبات عالية لأجهزة الحوسبة وميزانيات التدريب. تعد نماذج المحولات الحديثة مثالًا حيًا. يعد تبسيط العمليات الحسابية التي تقوم بها الشبكة إحدى طرق معالجة مشكلة التعقيد المتزايد. في هذه الورقة ، نقترح شبكة عصبية ثنائية النهاية لنهاية لمهمة تصنيف النوايا والنص. من أجل الاستفادة الكاملة من إمكانات الترميز الثنائي من طرف إلى طرف ، يتم تحويل كل من تمثيلات الإدخال (تضمين المتجهات لإحصاءات الرموز) والمصنف إلى ثنائيات. نبرهن على كفاءة هذه الشبكة في التصنيف المقصود للنصوص القصيرة عبر ثلاث مجموعات بيانات وتصنيف النص بمجموعة بيانات أكبر. في مجموعات البيانات المدروسة ، تحقق الشبكة المقترحة نتائج مماثلة لأحدث النتائج مع استخدام ذاكرة ووقت تدريب أقل بنسبة 20-40٪ مقارنة بالمعايير.', 'ja': '深層ニューラルネットワークは、ほとんどすべての自然言語処理タスクで優れたパフォーマンスを示していますが、その複雑さが増していることが懸念されます。 特に懸念されるのは、これらのネットワークが、コンピューティングハードウェアおよびトレーニング予算に高い要件をもたらすことである。 最先端の変圧器モデルは生き生きとした例です。 ネットワークによって実行される計算を簡素化することは、増大する複雑さの問題に対処する１つの方法である。 本稿では、意図とテキスト分類のタスクのためのエンドツーエンドの二値化ニューラルネットワークを提案する。 エンドツーエンド二値化の可能性を十分に利用するために、入力表現（トークン統計のベクトル埋め込み）と分類子の両方が二値化される。 私たちは、3つのデータセットにわたる短いテキストのインテント分類と、より大きなデータセットを持つテキスト分類に関するこのようなネットワークの効率性を実証します。 検討されたデータセットでは、提案されたネットワークは、ベンチマークと比較して20 ～ 40 ％のメモリとトレーニング時間を削減しながら、最先端の結果と同等の成果を達成します。', 'zh': '深神经网络卓异于自然语言,而日益之复杂性有忧。 尤可忧者,网络高计硬件培训之数也。 先进之变压器,生动之类也。 简化网络行之数,所以益复杂性也。 本文中,发端到端二值化神经网络,以意类之。 用端到端二值化之力,输(令牌计其息向量嵌)类器皆二值化也。 臣等展此网络于三数集上之短文本意分类及用更大数集之效率。 于所虑之数集上,所陈网络得与先进相当,而比之基准,用内存练日损20-40%', 'hi': 'गहरे तंत्रिका नेटवर्क ने लगभग हर प्राकृतिक भाषा प्रसंस्करण कार्य में अपने बेहतर प्रदर्शन का प्रदर्शन किया है, हालांकि, उनकी बढ़ती जटिलता चिंताओं को बढ़ाती है। एक विशेष चिंता यह है कि ये नेटवर्क हार्डवेयर और प्रशिक्षण बजट की गणना के लिए उच्च आवश्यकताएं पैदा करते हैं। अत्याधुनिक ट्रांसफॉर्मर मॉडल एक ज्वलंत उदाहरण हैं। एक नेटवर्क द्वारा किए गए संगणनाओं को सरल बनाना बढ़ती जटिलता के मुद्दे को संबोधित करने का एक तरीका है। इस पेपर में, हम इरादे और पाठ वर्गीकरण के कार्य के लिए binarized तंत्रिका नेटवर्क को समाप्त करने के लिए एक अंत का प्रस्ताव करते हैं। अंत से अंत binarization की क्षमता का पूरी तरह से उपयोग करने के लिए, दोनों इनपुट अभ्यावेदन (टोकन आंकड़ों के वेक्टर एम्बेडिंग) और क्लासिफायर को बिनाराइज़ किया जाता है। हम एक बड़े डेटासेट के साथ तीन डेटासेट और पाठ वर्गीकरण पर छोटे ग्रंथों के इरादे वर्गीकरण पर इस तरह के नेटवर्क की दक्षता का प्रदर्शन करते हैं। माना डेटासेट पर, प्रस्तावित नेटवर्क बेंचमार्क की तुलना में 20-40% कम स्मृति और प्रशिक्षण समय का उपयोग करते हुए अत्याधुनिक परिणामों के तुलनीय प्राप्त करता है।', 'ru': 'Глубокие нейронные сети продемонстрировали свою превосходную производительность почти в каждой задаче обработки естественного языка, однако их растущая сложность вызывает беспокойство. Особую озабоченность вызывает тот факт, что эти сети предъявляют высокие требования к аппаратному обеспечению и бюджетам на подготовку кадров. Современные модели трансформаторов являются ярким примером. Упрощение расчетов, выполняемых сетью, является одним из способов решения проблемы возрастающей сложности. В данной работе мы предлагаем конечную бинаризованную нейронную сеть для задачи классификации намерений и текста. Чтобы в полной мере использовать потенциал сквозной бинаризации, бинаризуются как входные представления (векторные вложения статистики токенов), так и классификатор. Мы демонстрируем эффективность такой сети по классификации намерений коротких текстов по трем наборам данных и классификации текста по большему набору данных. В рассматриваемых наборах данных предлагаемая сеть достигает результатов, сопоставимых с самыми современными, при этом используется на 20-40% меньше памяти и времени обучения по сравнению с контрольными показателями.', 'ga': 'Léirigh líonraí néarúla doimhne a bhfeidhmíocht níos fearr i mbeagnach gach tasc Próiseála Teanga Nádúrtha, ach is ábhar imní iad a gcastacht mhéadaitheach. Ábhar imní ar leith is ea go bhfuil riachtanais arda ag baint leis na gréasáin seo maidir le crua-earraí ríomhaireachta agus buiséid oiliúna. Is sampla beoga iad na samhlacha claochladáin den scoth. Bealach amháin chun aghaidh a thabhairt ar an tsaincheist a bhaineann le castacht mhéadaitheach is ea na ríomhanna a dhéanann líonra a shimpliú. Sa pháipéar seo, molaimid líonra néarúil dénártha ó cheann ceann go ceann don tasc a bhaineann le hintinn agus le haicmiú téacs. Chun lánúsáid a bhaint as an acmhainneacht a bhaineann le dénearúchán ó cheann ceann go ceann, déantar na huiríll ionchuir (leabú veicteoirí ar staitisticí comharthaí) agus an t-aicmitheoir araon a dhénártha. Léirímid éifeachtacht líonra den sórt sin maidir le haicmiú rún na dtéacsanna gearra thar thrí thacar sonraí agus aicmiú téacs le tacar sonraí níos mó. Ar na tacair sonraí a ndearnadh machnamh orthu, baineann an líonra atá beartaithe amach atá inchomparáide leis na torthaí úrscothacha agus é ag baint úsáide as 20-40% níos lú ama cuimhne agus oiliúna i gcomparáid leis na tagarmharcanna.', 'ka': 'მაგრამ ძალიან ნეიროლური ქსელები მათი საუკეთესო პროცესის სამუშაო სამუშაო სიტყვების პროცესი დამუშაობაში, მაგრამ მათი უფრო მეტი განსაკუთრებული დარწმუნება, რომ ეს ქსელები უფრო დიდი საჭიროება ჰაპეტერული და განაკეთებული ბიზეტების კომპიუტერებისთვის. სურათის ტრანფორმაციის მოდელები ცხოვრებული მაგალითი. ქსელის გავაკეთებული კომპუტიკაციების გასაკეთება ერთი გზა იყო, რომელიც უფრო დიდება კომპლექსიტეტის შესახებ. ამ დომენტში ჩვენ დავიწყებთ ბინარიზებული ნეიროლური ქსელის დასრულება მისამართლად და ტექსტის კლასიფიკაციაზე. ბინარიზაციის დასასრულებლად საკუთარი პროცენტის გამოყენება, ორივე შეტყობინებების გამოყენება (ბეკენ სტატისტიკის გვეკტორის დაყენება) და კლასიფიკაციის ბინარიზაცია ჩვენ ასეთი ქსელის ეფექციურობას გამოჩვენებთ სამი მონაცემების კლასიფიკაციაში სამი მონაცემების და ტექსტის კლასიფიკაციაში დიდი მონაცემების კლასიფიკაციაში. მონაცემების შესახებ მონაცემების კონფიგურაციის შესაბამისი ქსელი იქნება, რომელიც 20-40% უფრო მეტი მეხსიერება და განაცემების დროის გამოყენება.', 'el': 'Τα βαθιά νευρωνικά δίκτυα έχουν αποδείξει την ανώτερη απόδοσή τους σε σχεδόν κάθε εργασία επεξεργασίας φυσικής γλώσσας, ωστόσο, η αυξανόμενη πολυπλοκότητά τους προκαλεί ανησυχίες. Ιδιαίτερη ανησυχία είναι ότι τα δίκτυα αυτά θέτουν υψηλές απαιτήσεις για υπολογιστικό υλικό και προϋπολογισμούς κατάρτισης. Τα σύγχρονα μοντέλα μετασχηματιστών αποτελούν ένα ζωντανό παράδειγμα. Η απλούστευση των υπολογισμών που εκτελούνται από ένα δίκτυο είναι ένας τρόπος αντιμετώπισης του προβλήματος της αυξανόμενης πολυπλοκότητας. Στην παρούσα εργασία, προτείνουμε ένα τέλος σε τέλος δυαδικό νευρωνικό δίκτυο για το έργο της πρόθεσης και της ταξινόμησης κειμένου. Προκειμένου να αξιοποιηθεί πλήρως το δυναμικό της εξ ολοκλήρου δυαδικοποίησης, τόσο οι αναπαραστάσεις εισόδου (διανυσματικές ενσωμάτωση στατιστικών σημάτων) όσο και ο ταξινομητής δυαδικοποιούνται. Αποδεικνύουμε την αποτελεσματικότητα ενός τέτοιου δικτύου στην ταξινόμηση των μικρών κειμένων σε τρία σύνολα δεδομένων και στην ταξινόμηση κειμένου με ένα μεγαλύτερο σύνολο δεδομένων. Στα υπό εξέταση σύνολα δεδομένων, το προτεινόμενο δίκτυο επιτυγχάνει συγκρίσιμα με τα αποτελέσματα τελευταίας τεχνολογίας, ενώ χρησιμοποιεί 20-40% μικρότερο χρόνο μνήμης και κατάρτισης σε σύγκριση με τα κριτήρια αναφοράς.', 'hu': 'A mélyneurális hálózatok szinte minden természetes nyelvfeldolgozási feladatban kimutatták a kiváló teljesítményüket, azonban egyre nagyobb összetettségük aggodalomra ad okot. Különösen aggodalomra ad okot, hogy ezek a hálózatok magas követelményeket jelentenek a számítástechnikai hardverek és a képzési költségvetések tekintetében. A korszerű transzformátormodellek élénk példát mutatnak. A hálózat által végzett számítások egyszerűsítése a növekvő összetettség kérdésének egyik módja. Ebben a tanulmányban javaslatot teszünk egy end to end binarizált neurális hálózatra a szándék és a szövegosztályozás feladatához. Annak érdekében, hogy teljes mértékben kihasználjuk a end to end binarizáció potenciálját, mind a bemeneti reprezentációk (tokenstatisztikák vektoros beágyazása) mind az osztályozó binarizálódik. Egy ilyen hálózat hatékonyságát a rövid szövegek három adatkészletben történő szándékos osztályozásával és a szövegek nagyobb adatkészlettel történő osztályozásával mutatjuk be. A figyelembe vett adatkészleteken a javasolt hálózat a legkorszerűbb eredményekhez hasonló eredményeket ér el, miközben a referenciaértékekhez képest 20-40%-kal kevesebb memóriát és képzési időt használ fel.', 'it': "Le reti neurali profonde hanno dimostrato le loro prestazioni superiori in quasi ogni compito di elaborazione del linguaggio naturale, tuttavia, la loro crescente complessità solleva preoccupazioni. Una preoccupazione particolare è che queste reti pongono elevati requisiti per l'hardware informatico e i bilanci per la formazione. I modelli di trasformatori all'avanguardia sono un esempio vivido. Semplificare i calcoli effettuati da una rete è un modo per affrontare il problema della crescente complessità. In questo articolo, proponiamo una rete neurale binarizzata end to end per il compito di intenti e classificazione del testo. Al fine di sfruttare appieno il potenziale della binarizzazione end to end, sia le rappresentazioni di input (incorporazioni vettoriali di statistiche di token) che il classificatore sono binarizzate. Dimostriamo l'efficienza di tale rete sulla classificazione intenzionale dei testi brevi su tre set di dati e sulla classificazione del testo con un set di dati più ampio. Sui set di dati considerati, la rete proposta raggiunge risultati paragonabili allo stato dell'arte utilizzando il 20-40% di memoria e tempo di formazione inferiori rispetto ai benchmark.", 'mk': 'Длабоките нервни мрежи ја покажаа својата супериорна резултат во скоро секоја задача за процес на природен јазик, но нивната зголемена комплексност предизвикува загриженост. Посебна загриженост е што овие мрежи претставуваат високи барања за компјутерирање на хардверот и буџетот за обука. Моделите на најновите трансформатори се жив пример. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity.  Во овој весник предлагаме крај на бинаризираната нервна мрежа за задачата на намерата и класификацијата на текстот. За целосно искористување на потенцијалот од крај до крај на бинаризацијата, се бинаризираат и внатрешните претставувања (векторни внатрешни статистички податоци) како и класификаторот. Ние ја демонстрираме ефикасноста на ваквата мрежа на намерната класификација на кратки тексти преку три податоци и класификација на текст со поголем податок. Во врска со разгледуваните податоци, предложената мрежа постигнува споредливи со најновите резултати, истовремено користејќи 20-40 отсто помалку меморија и време за обука во споредба со benchmarks.', 'lt': 'Dideli nervų tinklai parodė, kad beveik visose gamtinės kalbos apdorojimo užduotyse jų rezultatai yra geresni, tačiau dėl jų vis sudėtingesnio susirūpinimo kyla. Ypač susirūpinimą kelia tai, kad šie tinklai kelia didelius kompiuterinės įrangos ir mokymo biudžeto reikalavimus. Naujausi transformatoriaus modeliai yra gyvas pavyzdys. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity.  Šiame dokumente siūlome nutraukti dvišalį nervinį tinklą ketinimų ir teksto klasifikavimo užduotims. Siekiant visapusiškai išnaudoti galimą dvišalį derinimą nuo galo iki galo, dvišalės įvedimo charakteristikos (žymenų statistikos vektorių įterpimas) ir klasifikatorius. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset.  Kalbant apie svarstomus duomenų rinkinius, siūlomas tinklas pasiekia panašius rezultatus kaip naujausi rezultatai, tuo pat metu naudojant 20–40 proc. mažiau atminties ir mokymo laiko, palyginti su lyginamaisiais rodikliais.', 'kk': 'Тегіс невралды желілері әрбір Түзіндік тіл процессорындағы тапсырмалардың жоғары әрекетін көрсетті, бірақ олардың көптеген тәжірибесі көптеген қайшылықтарды к Бұл желілер жабдық және оқыту бюджеттерін есептеу үшін жоғары талаптарды. Сурет түрлендіруші үлгілері жақсы мысал. Желі өзгертілген есептеулерді қарапайым көптеген жұмыстың бір әдісі. Бұл қағазда бинарлық невралдық желі мақсатты және мәтін классификациясының тапсырмасын аяқтау үшін аяқтауға болады. Бинаризацияны аяқтау мүмкіндігін толық пайдалану үшін келтірілген (белгілер статистикасының вектордың ендіру) және классификаторы бинаризацияланады. Біз бұл желінің ефективдігін үш деректер қорларына және үлкен деректер қорларына арналған қысқа мәтіндерді шектеу үшін көрсетедік. Табылған деректер жиындарында, ұсынылған желі, салыстыру белгілерімен қалыпты 20-40% деген жады мен оқыту уақытын қолданады.', 'ms': 'Rangkaian saraf dalam telah menunjukkan prestasi mereka yang lebih baik dalam hampir setiap tugas Pemprosesan Bahasa Alami, bagaimanapun, kompleksiti mereka semakin meningkat mengakibatkan bimbang. Perhatian tertentu ialah rangkaian ini mengandungi keperluan yang tinggi untuk pengiraan perisian dan anggaran latihan. Model pengubah state-of-the-art adalah contoh yang nyata. Permudahan pengiraan yang dilakukan oleh rangkaian adalah satu cara untuk mengatasi isu kompleksiti yang semakin meningkat. Dalam kertas ini, kami cadangkan akhir untuk mengakhiri rangkaian saraf binarizasi untuk tugas niat dan kelasukan teks. Untuk menggunakan sepenuhnya potensi penghujung hingga penghujung binarisasi, kedua-dua perwakilan input (penyampilan vektor statistik token) dan pengelasah binarisasi. Kami menunjukkan efisiensi rangkaian seperti itu pada kelasukan niat teks pendek atas tiga set data dan kelasukan teks dengan set data yang lebih besar. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.', 'mt': 'In-netwerks newrali profondi wrew il-prestazzjoni superjuri tagħhom fi kważi kull kompitu tal-ipproċessar tal-lingwi naturali, madankollu, il-kumplessità dejjem tikber tagħhom tqajjem tħassib. Tħassib partikolari huwa li dawn in-netwerks joħolqu rekwiżiti għoljin għall-kompjuter tal-baġits tal-hardware u t-taħriġ. Il-mudelli tat-trasformatur l-aktar avvanzati huma eżempju ċar. Is-semplifikazzjoni tal-kalkoli mwettqa minn netwerk hija mod wieħed kif tiġi indirizzata l-kwistjoni tal-kumplessità li qed tiżdied. F’dan id-dokument, qed nipproponu tmiem biex jintemm in-netwerk newrali binarizzat għall-kompitu tal-klassifikazzjoni tal-intenzjoni u t-test. Sabiex jiġi sfruttat bis-sħiħ il-potenzjal tal-binarizzazzjoni minn tmiem sa tmiem, kemm ir-rappreżentazzjonijiet tal-input (inkorporazzjonijiet tal-vetturi tal-istatistika tat-tokens) kif ukoll il-klassifikatur huma binarizzati. Aħna nuru l-effiċjenza ta’ tali netwerk dwar il-klassifikazzjoni intenzjonata ta’ testi qosra fuq tliet settijiet ta’ dejta u klassifikazzjoni tat-test b’sett ta’ dejta akbar. Fir-rigward tas-settijiet tad-dejta kkunsidrati, in-netwerk propost jikseb riżultati komparabbli mar-riżultati l-aktar avvanzati filwaqt li juża 20-40% inqas memorja u ħin ta’ taħriġ meta mqabbel mal-parametri referenzjarji.', 'ml': 'ആഴത്തെ ന്യൂറല്\u200d നെറ്റര്\u200dവര്\u200dക്കുകള്\u200d അവരുടെ സ്വാഭാവിക ഭാഷയുടെ പ്രവര്\u200dത്തനത്തില്\u200d ഏറ്റവും മികച്ച പ്രവര്\u200dത്തനങ്ങള്\u200d പ്രകടനം കാണിച്ചിരി പ്രത്യേക വിചാരിക്കുന്നത് ഈ ശൃംഖലകള്\u200d ഹാര്\u200dഡ്\u200cവെയര്\u200d ബാഗറ്റുകള്\u200d കണക്ക് ചെയ്യുന്നതിനും ഉയര്\u200dന്ന ആവശ്യങ രാജ്യത്തിന്റെ സ്റ്റേറ്റ്-ആർട്ട് മാറ്റങ്ങളുടെ മോഡലുകൾ ഒരു വിവരമായ ഉദാഹരണമാണ്. ഒരു ശൃംഖലയാല്\u200d പ്രവര്\u200dത്തിക്കുന്ന കണക്കൌണ്ടുകള്\u200d എളുപ്പമാക്കുന്നത് കൂടുതല്\u200d സങ്കീര്\u200dണ്ണതയുള്ള പ്രശ്നത്തെപ് ഈ പത്രത്തില്\u200d, നമ്മള്\u200d നിര്\u200dദ്ദേശിക്കുന്നത് ബൈനറിയേറ്റ് നെയൂറല്\u200d നെറ്റര്\u200d നെറ്റ്റര്\u200d നെറ്റ്റര്\u200d അവസാനിപ്പി ബൈനരിഷന്\u200d അവസാനിപ്പിക്കാനുള്ള അവസാനത്തിന്റെ സാധ്യതകള്\u200d പൂര്\u200dണ്ണമായും ഉപയോഗിക്കാന്\u200d, ഇന്\u200dപുട്ട് പ്രതിനിധികളും (ചിഹ്നങ്ങളുടെ പ്രതിനിധി We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset.  വിചാരിക്കപ്പെട്ട ഡാറ്റാസറ്റുകളില്\u200d, ബെന്\u200dമാര്\u200dക്കുകളോടൊപ്പം 20-40% കുറഞ്ഞ മെമ്മറിയും പരിശീലന സമയവും ഉപയോഗിക്കുമ്പോള്\u200d പ്രൊദ്ദേശിച്', 'no': 'Djupne neuralnettverk har demonstrert dei øvre utviklingane i nesten kvar naturleg språk-handteringsoppgåve, men deres økende kompleksitet fører opp bekymringar. Eit spesielt bekymring er at desse nettverkene har høg krev for datamaskinvare og treningsbudsjetar. Modellene for kunsttransformeringa er eit levert eksempel. Dette er ein måte å bestemme oppgåva av det økende kompleksitetet for å forenkla datamaskina utført av eit nettverk. I denne papiret foreslår vi ein slutt for å avslutta binariserte neuralnettverk for oppgåva med vilje og tekstklassifikasjon. For å fullstendig bruka potensialen til slutten for å slutta binarisering, både inndatarepresentasjonane (vektorinnbygging av tokensstatistikk) og klassifiseringen er binarisert. Vi demonstrerer effektiviteten av slike nettverk på den måten å klassifisera korte tekstar over tre datasett og tekstklassifisera med ein større datasett. På datasettet er det foreslått nettverket sammenlignbar med resultatet av kunsten ved bruk av 20-40 % mindre minne og treningstid sammenlignet med benchmarkene.', 'pl': 'Głębokie sieci neuronowe wykazały swoją doskonałą wydajność w prawie każdym zadaniu przetwarzania języka naturalnego, jednak ich rosnąca złożoność budzi obawy. Szczególną troską jest fakt, że sieci te stawiają wysokie wymagania w zakresie sprzętu obliczeniowego i budżetów szkoleniowych. Najnowocześniejsze modele transformatorów są żywym przykładem. Uproszczenie obliczeń wykonywanych przez sieć jest jednym ze sposobów rozwiązania problemu rosnącej złożoności. W niniejszym artykule proponujemy kompleksową binaryzowaną sieć neuronową dla zadania klasyfikacji intencji i tekstu. Aby w pełni wykorzystać potencjał binaryzacji od końca do końca, zarówno reprezentacje wejściowe (wektorowe osadzenia statystyk tokenów) jak i klasyfikator są binaryzowane. Wykazujemy skuteczność takiej sieci w zakresie klasyfikacji intencyjnej krótkich tekstów na trzy zbiory danych oraz klasyfikacji tekstów z większym zbiorem danych. W przypadku analizowanych zbiorów danych proponowana sieć osiąga porównywalne z najnowocześniejszymi wynikami przy wykorzystaniu 20-40% mniejszej pamięci i czasu treningowego w porównaniu do wskaźników referencyjnych.', 'mn': 'Харин гүнзгий мэдрэлийн сүлжээнд байгалийн хэл Процессорын ажил бүрийн дээд ажиллагааг харуулсан. Гэхдээ тэдний төвөгтэй төвөгтэй байдал нь сэтгэл санааг нэмэгдүүлдэг. Эдгээр сүлжээн нь техник хангамж болон сургалтын бюджетүүдийн тооцоолох шаардлагатай байдаг. Урлагийн шилжүүлэгч загвар бол амьдралын жишээ. Шүлжээнд хийсэн тооцоололтуудыг хялбарчлах нь илүү нарийн төвөгтэй асуудлыг олох нэг арга юм. Энэ цаасан дээр бид хоёр дахь мэдрэлийн сүлжээ зогсохыг санал болгодог. Эцэст нь хоёр дахин хоёр дахин хуваалцах боломжтой байдлыг бүрэн ашиглахын тулд орлуулагдсан үзүүлэлт (тодорхойлолтын векторын тодорхойлолт) болон хуваалцагч нь хоёр дахин хоёр дахин ху Бид ийм сүлжээний үр дүнтэй байдлыг гурван өгөгдлийн санаа, том өгөгдлийн санаагаар хуваалцах зорилго дээр харуулж байна. Тодорхойлогдсон өгөгдлийн сан дээр санал өгөгдлийн сүлжээний хувьд урлагийн үр дүнтэй харьцуулахад 20-40% бага санамж, сургалтын цаг багасгаж байдаг.', 'ro': 'Rețelele neuronale profunde și-au demonstrat performanța superioară în aproape fiecare sarcină de procesare a limbajului natural, cu toate acestea, complexitatea lor crescândă ridică preocupări. O preocupare deosebită este faptul că aceste rețele prezintă cerințe ridicate în ceea ce privește hardware-ul de calcul și bugetele de formare. Modelele de transformare de ultimă generație sunt un exemplu viu. Simplificarea calculelor efectuate de o rețea este o modalitate de a aborda problema complexității tot mai mari. În această lucrare, propunem o rețea neurală binarizată end to end pentru sarcina de intenție și clasificare a textului. Pentru a utiliza pe deplin potențialul binarizării end-to-end, atât reprezentările de intrare (încorporarea vectorială a statisticilor tokenilor) cât și clasificatorul sunt binarizate. Demonstrăm eficiența unei astfel de rețele privind clasificarea intenționată a textelor scurte pe trei seturi de date și clasificarea textelor cu un set de date mai mare. Pe seturile de date luate în considerare, rețeaua propusă obține rezultate comparabile cu cele de ultimă generație, utilizând în același timp cu 20-40% mai puțin memorie și timp de formare comparativ cu criteriile de referință.', 'sr': 'Duboke neuronske mreže su pokazale svoju nadmoćnu funkciju u skoro svakom zadatku prirodnog jezika, međutim, njihova povećana kompleksnost povećava zabrinutost. Posebna zabrinutost je da ove mreže predstavljaju visoke zahteve za raèunanje kompjutera i obuku budžeta. Modeli transformacije umetnosti su živi primer. Jednostavno pojednostavljanje računala koje je izvršila mreža je jedan način da se riješi pitanje povećanja kompleksnosti. U ovom papiru predlažemo kraj okončanja binarizirane neuralne mreže za zadatak namere i klasifikacije teksta. Da bi se u potpunosti iskoristio potencijal kraja do okončanja binarizacije, obje predstave ulaza (uključenje vektora statistike znakova) i klasifikacija su binarizirani. Mi pokazujemo efikasnost takve mreže o namjernoj klasifikaciji kratkih tekstova preko tri seta podataka i klasifikaciji teksta sa većim setom podataka. Na razmotrenim setima podataka, predložena mreža postiže usporedno sa rezultatima stanja umjetnosti dok koristi 20-40% manje pamćenja i treninga u usporedbi sa kriterijama.', 'si': 'ගොඩක් න්\u200dයූරල් ජාලය ප්\u200dරදේශ කරලා තියෙනවා ඔවුන්ගේ ප්\u200dරධාන ක්\u200dරියාත්මක ප්\u200dරදේශය හැම ප්\u200dරධාන භාෂාව ප්\u200dර විශේෂ ප්\u200dරශ්නයක් තමයි මේ ජාලයේ හාර්ඩ් වර්ගය සහ ප්\u200dරශ්නය බුද්ජිට් ගණනය කරන්න අවශ්\u200dයය අ ස්ථානයේ ක්\u200dරියාත්මක වෙනස් කරුණාකරණයේ මොඩේල්ස් එකක්. ජාලයෙක් වෙනුවෙන් කරපු ගණනුකරණය සාමාන්\u200dය කරන්න එක ප්\u200dරශ්නයක් විශ්වාස කරන්න ප්\u200dරශ්නයක්. මේ පත්තරේ අපි අවසානයක් අවස්ථාව කරනවා අවස්ථාව අවසානය කරන්න බින්යුරාල් ජාලය අවස්ථාව කරනවා අවස්ථා අවසානය සම්පූර්ණයෙන් අවසානය සම්පූර්ණයෙන් අවසානය කරන්න, අවසානය සම්පූර්ණයෙන් අවසානය කරන්න, අවසානය සම්පූර්ණය ( අපි ඒ වගේ ජාලයේ ප්\u200dරශ්ණතාවය පෙන්වන්නම් ප්\u200dරශ්ණතා කරනවා දත්ත සෙට් තුනක් වලින් කොටු පණිවිඩය සහ ලේඛන පණි හිතන්නේ දත්ත සැට් වලින්, ප්\u200dරතිශ්න ජාලය සඳහා ප්\u200dරතිශ්න විදිහට 20-40% ප්\u200dරතිශ්න මතකය සහ ප්\u200dරධානය සඳහා ප්\u200dරතිශ්න වෙල', 'sv': 'Djupa neurala nätverk har visat sin överlägsna prestanda i nästan varje Natural Language Processing uppgift, men deras ökande komplexitet väcker oro. Ett särskilt problem är att dessa nätverk ställer höga krav på datorhårdvara och utbildningsbudgetar. De senaste transformatormodellerna är ett levande exempel. Att förenkla de beräkningar som utförs av ett nätverk är ett sätt att ta itu med frågan om den ökande komplexiteten. I denna uppsats föreslår vi ett end to end binariserat neuralt nätverk för uppgiften avsikt och textklassificering. För att fullt ut kunna utnyttja potentialen med binarisering från slut till slut är både indatarepresentationer (vektorinbäddningar av tokenstatistik) och klassificeraren binariserade. Vi visar hur effektivt ett sådant nätverk är när det gäller avsiktsklassificering av korta texter över tre dataset och textklassificering med en större dataset. På de aktuella datauppsättningarna uppnår det föreslagna nätverket jämförbara med de senaste resultaten samtidigt som det utnyttjar 20–40% mindre minne och träningstid jämfört med riktmärkena.', 'so': 'Shabakadaha neurada ee hoos u dheer waxay muujiyaan tababar ka sareeya shaqo la xiriira afka asalka ah oo dhan, habase yeeshee dhibaatadooda sii kordhaya wuxuu kordhaa welwelo. Shaqooyin gaar ah waa in shabakaddaasu ay leeyihiin baahida aad u sareeya xisaabinta qasabka qalabka iyo tababarka. The state-of-the-art transformer models are a vivid example.  Isku fududeynta xisaabinta shabakadda lagu sameeyo waa hal qaab ay ku saabsato arimaha adag ee kordhaya. Qoraalkan waxaynu soo jeedaynaa in uu dhamaado shabakadda neurada oo labada nooc ah ee shaqada fasaxa qoraalka iyo qoraalka. Si aad u isticmaaltid dhamaanka ku dhamaanshaha labada is-barashada, waxaa la isku biiriyaa qofka la soo saaray (vector ku dhex gala statistics calaamado) iyo fasaxa. Waxaan muujinnaa faa’iidada shabakadda caynkaas ah oo ku qoran qoraal gaaban oo ka sarreeya saddex databases iyo fasax qoraal ah oo ku qoran macluumaad weyn. Shabakadda la soo jeeday wuxuu ka helaa suurtagal u eg arimaha farshaxanka marka lagu isticmaalayo 20-40% ka yar xusuusta iyo waqtiga waxbarashada oo la barbaranayo isbarbardhigga bangiyada.', 'ur': 'اگرچہ ان کی بڑی پیچیدگی اضافہ کرتی ہے۔ ایک خاص مشکل یہ ہے کہ یہ نیٹورک هارڈیور اور تربین بیڈیٹوں کی کمپیوٹرینگ کے لئے اچھی لازم رکھتے ہیں۔ آرت ترنسفور موڈل ایک زندہ مثال ہے۔ ایک نیٹورک کے ذریعے کامپیوتروں کو سادھا کرنے کا ایک طریقہ ہے کہ بڑھنے کی پیچیدگی کا مسئلہ حل کریں۔ اس کاغذ میں، ہم ایک پایان پیش کریں گے کہ دوئنائرل نیورل نیٹ ورک کو مصیبت اور ٹیکسٹ کلاسپیٹ کے کام کے لئے پایان کرے۔ اس لئے کہ آخر کے امکانات کو دوئناریزی کے پانے کے لئے کامل استعمال کرنے کے لئے، دونوں اینپیٹ نمائش (ٹوکنز کے ویکتور انڈینگ) اور کلاسیر دوئناریزی کیے جاتے ہیں. ہم ایسے نیٹ ورک کے مطابقت کو تین ڈاٹ سٹ پر چھوٹے ٹکسٹ کے کلاسپیٹ پر دکھاتے ہیں اور ایک بڑے ڈاٹ سٹ کے ساتھ ٹکسٹ کلاسپیٹ کے مطابق دکھاتے ہیں نظر کیا گیا ہے ڈاٹ سٹ پر، پیشنهاد نیٹ ورک بنچم مارک کے مقابلہ میں 20-40% کم یاد اور تمرین وقت حاصل کرتا ہے.', 'ta': 'ஆழமான புதிரையின் வலைப்பின்னல்கள் கிட்டத்தட்ட ஒவ்வொரு இயல்பான மொழி செயல்பாட்டிலும் மேலான செயல்பாட்டை காண்பித்துள்ளார ஒரு குறிப்பிட்ட கவலை என்னவென்றால் இந்த வலைப்பின்னல்கள் வன்பொருள் மற்றும் பயிற்சி பெட்டிகளை கணிக்க உ கலை மாற்றும் மாதிரிகளின் நிலைமை Name இந்த காகிதத்தில், நாம் முடிவு நிறைவேற்றுகிறோம் இசைக்கும் உரை வகைப்படுத்தலுக்கும் செயல்பாட்டிற்கும் இரு முழுமையாக பயன்படுத்த முடிவு இருமுறை நிறுத்துவதற்கு, உள்ளீட்டு பிரிவுகள் (குறியீட்டு புள்ளிவிவரங்கள் உள்ளிடும்) மற்றும் வகைப்பாளர் இரு மூன்று தரவுத்தளங்கள் மற்றும் உரை வகைப்படுத்தல் பெரிய தரவுத்தளத்துடன் இந்த வலைப்பின்னலின் விளைவை காட்டுகிறது. கருத்தப்பட்ட தகவல் அமைப்புகளில், பென்குறிப்புகளை ஒப்பிட்டு 20- 40% நினைவகத்தையும் பயிற்சி நேரத்தையும் பயன்படுத்தும் போது பரிந்துரைக்கப', 'vi': 'Mạng thần kinh sâu đã chứng minh khả năng vượt trội của họ trong hầu hết các nhiệm vụ xử lý ngôn ngữ tự nhiên, nhưng sự phức tạp ngày càng tăng lên làm phiền. Một mối quan tâm đặc biệt là các mạng lưới này đòi hỏi cao về ngân sách tính toán phần cứng và huấn luyện. Những mẫu máy biến đổi hiện đại là một ví dụ sống động. Một cách để giải quyết vấn đề về sự phức tạp ngày càng gia tăng. Trong tờ giấy này, chúng tôi đề nghị kết thúc mạng thần kinh nhị phân phối các nhiệm vụ của mục đích và phân loại văn bản. Để hoàn to àn sử dụng khả năng kết thúc chương trình, cả các biểu tượng nhập (sự nhúng vào vector of tokens statistics) và người phân loại được nhị phân. Chúng tôi chứng minh sự hiệu quả của một mạng lưới như thế dựa trên việc phân loại các văn bản ngắn hơn ba bộ dữ liệu và phân loại văn bản với một bộ dữ liệu lớn hơn. Trên các nhà dữ liệu được xem xét, mạng dự thảo đạt được tương đương với kết quả hiện đại trong khi sử dụng số lượng bộ nhớ và thời gian huấn luyện ít hơn so với các tiêu chuẩn.', 'uz': "Tayyoviy tarmoqlar har bir tabiiy tillar jarayonlaridagi eng yuqori jarayonlarini ko'rsatadi, ammo ularning murakkablarining ko'payishi murakkabliklarini foydalanadi. Shu tarmoqlar kompyuterni hisoblash va taʼminlovchi bajarettlari uchun juda katta kerakli. Shaxsiy o'zgartirishning holati modeli juda yaxshi misol. Name Bu hujjatda, biz qanday va matn darajasining vazifasini bir necha tarmoq tugatishini tugatishini talab qilamiz. Ikkinchi marta bir necha to ʻgʻri birlashtirishga foydalanish uchun, kiritish representlari (qiymatlar statistikasi qismiga kiritilgan vector) va klassifiker ikkinchi marta birlashtiriladi. Biz bu tarmoqning foydalanishini uchta maʼlumot sahifadagi qisqa matnlarning darajasini ko'rsatamiz va katta maʼlumotlar tarmoqda. Koʻrsatilgan maʼlumotlar tarmoqda, ilova qilingan tarmoq parametrlar bilan birga qisqa xotira va taʼminlovchi vaqtdan foydalanadi.", 'bg': 'Дълбоките невронни мрежи демонстрират своята превъзходна производителност в почти всяка задача за обработка на естествения език, но нарастващата им сложност поражда притеснения. Особено загрижено е, че тези мрежи поставят високи изисквания за компютърен хардуер и бюджети за обучение. Модерните трансформаторни модели са ярък пример. Опростяването на изчисленията, извършвани от мрежата, е един от начините за решаване на въпроса за нарастващата сложност. В настоящата статия предлагаме бинаризирана невронна мрежа от край до край за задачата за класификация на намеренията и текста. За да се използва напълно потенциалът на бинаризация от край до край, както входните представяния (векторни вграждания на статистиката на токените), така и класификаторът са бинаризирани. Демонстрираме ефективността на такава мрежа при класификацията на намеренията на кратки текстове върху три набора от данни и класификацията на текста с по-голям набор от данни. На разглежданите набори от данни предложената мрежа постига сравними с най-съвременните резултати, като използва 20-40% по-малко памет и време за обучение в сравнение с референтните показатели.', 'da': 'Dybe neurale netværk har vist deres overlegne ydeevne i næsten alle Natural Language Processing opgaver, men deres stigende kompleksitet rejser bekymringer. En særlig bekymring er, at disse netværk stiller store krav til computerhardware og uddannelsesbudgeter. De avancerede transformatormodeller er et levende eksempel. En forenkling af de beregninger, der udføres af et netværk, er en måde at løse problemet med den stigende kompleksitet på. I denne artikel foreslår vi et end-to-end binariseret neuralt netværk til opgaven med henblik på hensigt og tekst klassifikation. For fuldt ud at udnytte potentialet for end to end binarisering, er både input repræsentationer (vektor indlejringer af tokens statistik) og klassifikationen binariseret. Vi demonstrerer effektiviteten af et sådant netværk på hensigtsklassificering af korte tekster over tre datasæt og tekstklassificering med et større datasæt. På de overvejede datasæt opnår det foreslåede netværk sammenlignelige med de nyeste resultater, samtidig med at det udnytter 20-40% mindre hukommelse og træningstid sammenlignet med benchmarks.', 'hr': 'Duboke neuronske mreže pokazale su svoju nadmoćnu učinku u skoro svakom zadatku prirodnog procesa jezika, međutim, njihova povećana kompleksnost povećava zabrinutost. Posebna zabrinutost je da ove mreže predstavljaju visoke zahtjeve za računalo računala i proračunala za obuku. Modeli transformatora stanja umjetnosti su živi primjer. Jednostavno objašnjenje računala izvršene mrežom je način rješavanja problema povećanja kompleksnosti. U ovom papiru predlažemo kraj okončanja binarizirane neuralne mreže za zadatak namjere i klasifikacije teksta. Kako bi se u potpunosti iskoristio potencijal kraja do okončanja binarizacije, obje predstave ulaza (ugrađenje vektora statistike znakova) i klasifikacija su binarizirane. Mi pokazujemo učinkovitost takve mreže o namjernoj klasifikaciji kratkih tekstova preko tri seta podataka i klasifikaciji teksta sa većim setom podataka. Na razmotrenim setima podataka, predložena mreža postiže usporedno s rezultatima stanja umjetnosti dok se koristi 20-40% manje pamćenja i treninga u usporedbi s kriterijama.', 'nl': 'Diepe neurale netwerken hebben hun superieure prestaties bewezen in bijna elke Natural Language Processing taak, maar hun toenemende complexiteit roept zorgen op. Een bijzondere zorg is dat deze netwerken hoge eisen stellen aan computerhardware en opleidingsbudgetten. De state-of-the-art transformatormodellen zijn een levendig voorbeeld. Het vereenvoudigen van de berekeningen van een netwerk is een manier om het probleem van de toenemende complexiteit aan te pakken. In dit artikel stellen we een end to end binarized neural network voor voor de taak van intentie en tekstclassificatie. Om het potentieel van end to end binarisatie volledig te benutten, worden zowel de input representaties (vector embeddings van tokens statistieken) als de classificator binariseerd. We demonstreren de efficiëntie van een dergelijk netwerk op het gebied van intent classificatie van korte teksten over drie datasets en tekstclassificatie met een grotere dataset. Op de bestudeerde datasets bereikt het voorgestelde netwerk vergelijkbaar met de state-of-the-art resultaten terwijl het gebruik van 20-40% minder geheugen en trainingstijd ten opzichte van de benchmarks.', 'de': 'Tiefe neuronale Netze haben ihre überlegene Leistung in fast jeder Natural Language Processing Aufgabe unter Beweis gestellt, aber ihre zunehmende Komplexität wirft Bedenken auf. Besonders besorgniserregend ist, dass diese Netze hohe Anforderungen an Computerhardware und Schulungsbudgets stellen. Ein anschauliches Beispiel dafür sind die hochmodernen Transformatormodelle. Die Vereinfachung der Berechnungen durch ein Netzwerk ist eine Möglichkeit, das Problem der zunehmenden Komplexität anzugehen. In diesem Beitrag schlagen wir ein Ende-zu-Ende binarisiertes neuronales Netzwerk für die Aufgabe der Intent- und Textklassifizierung vor. Um das Potenzial der End-to-End-Binarisierung voll auszuschöpfen, werden sowohl die Eingabedarstellungen (Vektoreinbettungen von Token-Statistiken) als auch der Klassifikator binarisiert. Wir demonstrieren die Effizienz eines solchen Netzwerks bei der Intent-Klassifizierung von Kurztexten über drei Datensätze und der Textklassifizierung mit einem größeren Datensatz. Auf den betrachteten Datensätzen erzielt das vorgeschlagene Netzwerk vergleichbar mit den aktuellen Ergebnissen und nutzt 20-40% weniger Speicher und Trainingszeit als die Benchmarks.', 'id': 'Jaringan saraf dalam telah menunjukkan prestasi mereka yang lebih tinggi dalam hampir setiap tugas Proses Bahasa Alami, bagaimanapun, kompleksitas mereka meningkat menaikkan kekhawatiran. Kecemasan khusus adalah bahwa jaringan ini menunjukkan keperluan tinggi untuk komputer perangkat keras dan anggaran pelatihan. Model transformer state-of-the-art adalah contoh yang nyata. Simplifikasi perhitungan yang dilakukan oleh jaringan adalah satu cara untuk mengatasi isu kompleksitas yang semakin meningkat. Dalam kertas ini, kami mengusulkan akhir untuk mengakhiri jaringan saraf binarisasi untuk tugas tujuan dan klasifikasi teks. Untuk memanfaatkan sepenuhnya potensi dari akhir ke akhir binarisasi, kedua-duanya representation input (vector embedding of token statistics) dan klasifikasi binarisasi. Kami menunjukkan efisiensi jaringan seperti itu pada klasifikasi niat teks pendek di atas tiga set data dan klasifikasi teks dengan set data yang lebih besar. Dalam set data yang dipertimbangkan, jaringan yang diusulkan mencapai yang dapat dibandingkan dengan hasil state-of-the-art sementara menggunakan memori 20-40% kurang dan waktu latihan dibandingkan dengan benchmarks.', 'fa': 'شبکه\u200cهای عصبی عمیق عملکرد عملکرد خود را در تقریباً هر کار پردازش زبان طبیعی نشان داده است، ولی پیچیدگی\u200cشان افزایش\u200cتر از آن نگرانی می\u200cدهد. نگرانی مخصوص این است که این شبکه نیازهای بالا برای محاسبه بودجه های حافظه و آموزشی قرار می دهد. مدل تغییر دهنده هنر یک مثال زنده است. ساده کردن محاسبات\u200cهایی که توسط یک شبکه انجام می\u200cدهند یک راه برای حل مشکل پیچیدگی افزایش است. در این کاغذ، ما پیشنهاد می\u200cکنیم که پایان پایان شبکه\u200cهای عصبی بیناوری را برای وظیفه\u200cی قضاوت و ترکیب متن انجام دهیم. برای کامل استفاده از پتانسیل پایان تا پایان دوگانه\u200cسازی، هر دو نمایش\u200cهای ورودی (استفاده\u200cهای ویکتوری از آمار شناسایی معجزه\u200cها) و کلیسازی دوگانه\u200cسازی می\u200cشوند. ما فعالیت این شبکه را نشان می دهیم که با یک مجموعه اطلاعات بزرگتری از متن\u200cهای کوتاه بر سه مجموعه داده\u200cها و مجموعه\u200cی متن\u200cها با یک مجموعه داده\u200cهای بزرگتری است. در مجموعه داده\u200cها، شبکه پیشنهاد قابل مقایسه با نتیجه\u200cهای موجود هنر در حال استفاده از حافظه\u200cهای بیست و ۴۰ درصد کمتر و زمان آموزش در مقایسه با مقایسه\u200cهای مقایسه می\u200cشود.', 'ko': '심층 신경 네트워크는 거의 모든 자연 언어 처리 임무에서 우수한 성능을 보였지만, 갈수록 증가하는 복잡성이 사람들의 관심을 끌었다.특히 이들 네트워크가 하드웨어와 교육 예산을 계산하는 데 높은 요구를 하고 있다는 사실이 우려된다.가장 선진적인 변압기 모형은 바로 생동감 있는 예이다.네트워크 실행을 간소화하는 계산은 날로 복잡해지는 문제를 해결하는 방법의 하나다.본고에서 우리는 의도와 텍스트 분류에 사용되는 단말기부터 단말기까지의 이치화 신경 네트워크를 제시했다.단말기부터 단말기까지의 이치화 잠재력을 충분히 이용하기 위해 입력 표시(통계를 표시하는 벡터 삽입)와 분류기는 모두 이치화되었다.우리는 이러한 네트워크가 세 개의 데이터 집합에 있는 짧은 텍스트 의도 분류와 더 큰 데이터 집합에 있는 텍스트 분류의 효율을 증명했다.고려된 데이터 집합에서 제시된 네트워크는 가장 선진적인 결과와 상당한 효과를 거두었고 기준에 비해 사용된 메모리와 훈련 시간이 20-40% 감소했다.', 'sw': 'Mitandao ya ndani ya kiserikali imeonyesha utendaji wao wa juu katika karibu kila kazi ya mchakato wa lugha ya asili, hata hivyo, ongezeko la utata unaongezea wasiwasi. Tatizo maalum ni kuwa mitandao hii ina hitaji kubwa kwa ajili ya kompyuta vifaa na mafunzo. Mfano wa mabadiliko ya hali ya sanaa ni mfano mzuri. Kurahisisha hisabati zilizofanywa na mtandao ni njia moja ya kutatua suala la utata unaoongezeka. Katika gazeti hili, tunapendekeza kusitisha mtandao wa neura unaoendelea kwa ajili ya kazi ya kutangazwa kwa lengo na kuandika maandishi. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized.  Tunaonyesha ufanisi wa mtandao wa aina hiyo kwa lengo la kutangaza maandishi fupi zaidi ya seti tatu za data na usambazaji wa maandishi kwa seti kubwa zaidi ya taarifa. Katika seti za taarifa zilizotazama, mtandao ulipendekezwa unafanikiwa na matokeo ya sanaa wakati wakitumia asilimia 20-40 chini ya kumbukumbu na muda wa mafunzo ukilinganisha na bendera.', 'af': "Deep neuralnetwerke het hul bo-oerspronklikheid in amper elke Natuurlike Taal-Prosessering-taak bevestig, maar hulle grootmaak kompleksiteit verwek bekommerdes. 'n Spesifieke bekommerding is dat hierdie netwerke hoë benodigte stel vir die rekenaar van hardware en onderwerp budgete. Die state-of-the-art transformer modele is 'n lewendige voorbeeld. Vereenvoudiging van die rekenaar wat deur 'n netwerk uitgevoer is is een manier om die probleem van die vergroei kompleksiteit te raak. In hierdie papier, voorstel ons 'n einde om binnereerde neuralnetwerk te einde vir die taak van doel en teks klasifikasie. Om die potensiaal van einde te einde binêrisasie volledig te gebruik, is beide die invoer voorstellings (vektor inbêring van tokens statistiek) en die klassifiseerder binêriseer. Ons wys die effektiviteit van sodanige 'n netwerk op die doel klassifikasie van kort teks oor drie datastel en teks klassifikasie met 'n groter datastel. Op die aangesien datastelle, word die voorgestelde netwerk vergelykbaar met die standaard van die kuns resultate terwyl die 20-40% minder geheue en onderwerp tyd gebruik word vergelykbaar met die benchmarke.", 'tr': 'Derrew neural şebekeleri öz ýokary ukyplaryny näçe baýram dil işleýişinde görkezilýär, ýöne olaryň öňünden gaty kynçylyklaryny görkezilýär. A ýratyn mesele şudyr, bu şebekler hardware we okuwçylyk budgetleriň hasaplamagyna ýokary gereklikleri ýokarylýar. Sanat transformasy nusgalary ýagdaýdyr. Bir şebek tarapyndan işledilen hesaplamalary bejermek a ňsatlyk çykyşlyklaryň meselesini çözmek üçin bir yoldur. Bu kagyzda, bir nusga niýeti we metin klasifikasy üçin binalyk netijesini soňlamak üçin gutarmagy teklip edýäris. Bu binalizasyon bitirmek için sonun potansiyelini tamamen kullanmak için, girdi temsilleri de (token istatistiklerinin vektörleri) ve klasifikatörleri de binalizdir. Biz bu şebekiň etkinliýetini üç datasetiň üstünde kısa metin klasifikasiýasyny we uly bir datasetiň bilen metin klasifikasiýasyny ukyp edýäris. Akyllanýan veri setirlerinde, teklip eden syýahat sanat netijesi bilen 20-40% iň az ýagty we eğitim zamany benchmarklaryň derejesi bilen karşılaşýar.', 'am': 'ጥልቅ የናውሬው መረብ የባሕላዊ ቋንቋ ማቀናቀል ስራ ሁሉ የበለጠ ስርዓታቸውን አሳልፎአል፡፡ A particular concern is that these networks pose high requirements for computing hardware and training budgets.  የ-የ-አርድ ለውጦ ሞዴላዎች የደኀብ ምሳሌ ናቸው፡፡ በመረብ የተደረገውን ቁጥጥር ቀላል ማቀናጃ የሚጨምረው ውጤቱን ለመቀበል አንድ መንገድ ነው፡፡ In this paper, we propose an end to end binarized neural network for the task of intent and text classification.  በሙሉ የመጨረሻውን በጥያቄ ለመፍጠር፣ የጥያቄው መልዕክቶች (የምልክት ተሳታፊዎች የግንኙነት ግንኙነት) እና ክፍለኛውን በይነመረብ ለመጠቀም ነው፡፡ በሦስት ዳታተሮች እና የጽሑፍ ክፍል በተለይ የጽሑፎች ማቀናቀል እና የጽሑፍ ክፍል በተለየ ትልቅ ዳታተር ማሳየትን እናሳየዋለን፡፡ በተመለከተው ዳታተሮች ላይ የተዘጋጀው መረብ ከbenchmarks በተያይነ 20-40 በመቶ ትንሽ ማስታሰቢያ እና ትምህርት ሰዓት በመጠቀም ይችላል፡፡', 'az': 'Ən böyük nöral ağları, hətta hər təbiətli dil işləməsi işində onların ən yaxşı performansını göstərmişdir. Ancaq onların daha çox kompleksitəsi nəticəsini artırar. Bu şəbəkələr hardware çi və təhsil budžetlarını hesablamaq üçün yüksək şartları yaratmaq üçün məqsədilə endirilməlidir. Sanat transformatçısı modelləri həyatlı bir nümunədir. Ağ tərəfindən icra edilən hesapları asanlaşdırmaq daha çox sarmaqlıqların məsələsini çəkmək bir yoldur. Bu kağızda, niyyəti və mətn klasifikasiyasının işi üçün binarizlənmiş nöral ağlanı bitirməyi təklif edirik. Binarlıq bitirmək üçün sonun potansiyelini tamamlamaq üçün, giriş göstəriciləri (token istatistiklərinin vektor inşalları) və klasifikatçıları binalizlənir. Biz böyük şəbəkənin faydallığını üç veri qutusu üstündə qısa mətnlərin klasifikasyonu və daha böyük veri qutusu ilə mətn klasifikasyonu göstəririk. Gördüyünüz verilən qurğular üzerində, təklif edilən şəbəkə, sanat sonuçlarıyla 20-40% daha az anı və təhsil vaxtını benchmarklarla qarşılaşdırar.', 'bn': 'নিউরেল নেটওয়ার্ক প্রায় প্রত্যেক স্বাভাবিক ভাষার প্রক্রিয়ার কাজে তাদের উচ্চতা প্রদর্শন করেছে, তবে তাদের বাড়তে থাকা কঠোর ব্যাপ বিশেষ উদ্বেগ হচ্ছে এই নেটওয়ার্ক হার্ডওয়্যার এবং প্রশিক্ষণ বাজেট কমিউনিট করার জন্য উচ্চ দরকার। রাষ্ট্র-অফ-শিল্প পরিবর্তন মডেল হচ্ছে ভীষণ উদাহরণ। একটি নেটওয়ার্ক দ্বারা অনুষ্ঠিত গণনা সহজ করা হচ্ছে বাড়তে থাকা সমস্যার ব্যাপারে একটি উপায়। এই কাগজটিতে আমরা প্রস্তাব করছি বাইনারিয়ার নিউরেল নেটওয়ার্ক শেষ করার জন্য যে উদ্দেশ্য এবং লেখা শ্রেণীকরণের কাজের জন্ বাইনারেশন শেষের সম্ভাবনা সমাপূর্ণ ব্যবহার করার জন্য ইনপুটের প্রতিনিধিত্ব (প্রতীক পরিসংখ্যানের ভেক্টরের প্রতিনিধিত্ব) এবং বিশ্লেষক বি আমরা এই ধরনের নেটওয়ার্কের কার্যক্রম প্রদর্শন করি তিন তথ্য সংক্ষিপ্ত টেক্সট এবং টেক্সট বিভাগের উপর সংক্ষিপ্ত টেক্সট ব্যবহারের ক্ষ On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.', 'bs': 'Duboke neuronske mreže pokazale su svoju nadmoćnu funkciju u skoro svakom zadatku prirodnog jezika, međutim, njihova povećana kompleksnost povećava zabrinutost. Posebna zabrinutost je da ove mreže predstavljaju visoke zahtjeve za računalizanje kompjutera i obuku. Modeli transformatora stanja umjetnosti su živi primjer. Jednostavno objašnjavanje računala izvedenih mreža je jedan način da se riješi pitanje povećanja kompleksnosti. U ovom papiru predlažemo kraj okončanja binarizirane neuralne mreže za zadatak namjere i klasifikacije teksta. Da bi se u potpunosti iskoristio potencijal kraja do okončanja binarizacije, obje predstave ulaza (ugrađenje vektora statistike znakova) i klasifikatora su binarizirani. Pokazujemo učinkovitost takve mreže o namjernoj klasifikaciji kratkih tekstova preko tri seta podataka i klasifikaciji teksta sa većim setom podataka. Na razmotrenim setima podataka, predložena mreža postiže usporedno sa rezultatima stanja umjetnosti dok koristi 20-40% manje pamćenja i treninga u usporedbi s kriterijama.', 'ca': "Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns.  Una preocupació especial és que aquestes xarxes posen altes necessitats per a la computació del hardware i dels pressupostes de formació. The state-of-the-art transformer models are a vivid example.  Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity.  In this paper, we propose an end to end binarized neural network for the task of intent and text classification.  In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized.  Demonstrem l'eficiència d'aquesta xarxa en la classificació intencional de textos curts sobre tres conjunts de dades i classificació de text amb un conjunt de dades més gran. En els conjunts de dades considerats, la xarxa proposada aconsegueix comparables amb els resultats més avançats mentre utilitza 20-40% menys temps de memòria i entrenament comparats amb els punts de referència.", 'cs': 'Hluboké neuronové sítě prokázaly svůj vynikající výkon téměř ve všech úkolech zpracování přirozeného jazyka, nicméně jejich rostoucí složitost vyvolává obavy. Zvláštní obavou je, že tyto sítě představují vysoké požadavky na výpočetní hardware a rozpočty na školení. Nejmodernější modely transformátorů jsou živým příkladem. Zjednodušení výpočtů prováděných sítí je jedním ze způsobů, jak řešit problém rostoucí složitosti. V tomto článku navrhujeme end to end binarizovanou neuronovou síť pro úkol záměru a klasifikace textu. Pro plné využití potenciálu end to end binarizace jsou binarizovány jak vstupní reprezentace (vektorové vložení statistik tokenů), tak klasifikátor. Účinnost takové sítě demonstrujeme na záměrné klasifikaci krátkých textů do tří datových sad a klasifikaci textů s větší sadou dat. Na zvažovaných datových sadách dosahuje navržená síť srovnatelných s nejmodernějšími výsledky při využití 20-40% menší paměti a tréninkové doby ve srovnání s referenčními hodnotami.', 'sq': 'Rrjetet e thella nervore kanë demonstruar performancën e tyre më të lartë në pothuajse çdo detyrë të Procesimit të Gjuhave Natyrore, megjithatë kompleksiteti i tyre në rritje ngre shqetësime. A particular concern is that these networks pose high requirements for computing hardware and training budgets.  Modelet e transformuesve më të lartë janë një shembull i gjallë. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity.  Në këtë letër, propozojmë një fund për të përfunduar rrjetin nervor të binarizuar për detyrën e klasifikimit të qëllimit dhe tekstit. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized.  Ne demonstrojmë efektshmërinë e një rrjeti të tillë në klasifikimin me qëllim të teksteve të shkurtra mbi tre grupe të dhënash dhe klasifikimin e tekstit me një grup të dhënash më të madh. Në grupet e të dhënave të konsideruara, rrjeti i propozuar arrin të krahasueshëm me rezultatet më të mëdha ndërsa përdor 20-40% më pak kujtesë dhe kohë stërvitjeje krahasuar me pikat e referimit.', 'fi': 'Syvät hermoverkot ovat osoittaneet ylivoimaisen suorituskykynsä lähes jokaisessa luonnollisen kielen prosessointitehtävässä, mutta niiden kasvava monimutkaisuus herättää huolta. Erityisen huolestuttavaa on, että nämä verkot asettavat korkeat vaatimukset tietokonelaitteistoille ja koulutusbudjeteille. Modernit muuntajamallit ovat eloisa esimerkki. Verkon suorittamien laskelmien yksinkertaistaminen on yksi tapa ratkaista monimutkaisuutta. Tässä työssä ehdotamme end to end binarisoitua neuroverkkoa intent- ja tekstiluokituksen tehtävään. Jotta lopusta päähän tapahtuvan binarisoinnin potentiaali voitaisiin hyödyntää täysimääräisesti, sekä syöttöesitykset (polettitilastojen vektoriupotukset) että luokittelija on binarisoitu. Osoitamme tällaisen verkoston tehokkuuden lyhyiden tekstien intent-luokittelussa kolmen aineiston yli ja tekstin luokittelussa suuremmalla aineistolla. Tarkasteltavissa aineistoissa ehdotettu verkosto saavuttaa vertailukelpoisia tuloksia hyödyntäen 20-40% vähemmän muistia ja harjoitusaikaa vertailuarvoihin verrattuna.', 'et': 'Sügavad närvivõrgud on näidanud oma suurepärast jõudlust peaaegu igas loodusliku keele töötlemise ülesandes, kuid nende üha keerukam keerukus tekitab muret. Eriti murettekitav on see, et need võrgud esitavad kõrgeid nõudeid arvutiriistvara ja koolituse eelarvetele. Tipptasemel trafo mudelid on elav näide. Võrgu tehtavate arvutuste lihtsustamine on üks võimalus lahendada kasvava keerukuse probleemi. Käesolevas töös pakume välja binariseeritud närvivõrgu lõpp-lõpuni kavatsuse ja teksti klassifitseerimise ülesandeks. Selleks, et täielikult ära kasutada lõpp-lõpuni binariseerimise potentsiaali, on nii sisendrepresentatsioonid (vektorite manustamine tokenitest statistika) kui ka klassifitseerija binariseeritud. Näitame sellise võrgustiku efektiivsust lühitekstide kavatsuslikul klassifitseerimisel kolme andmekogumi vahel ja teksti klassifitseerimisel suurema andmekogumiga. Kaalutud andmekogumite puhul saavutab kavandatud võrgustik võrreldavad uusimate tulemustega, kasutades võrdlusalustega võrreldes 20–40% vähem mälu- ja treeningaega.', 'hy': 'Խաղր նյարդային ցանցերը ցույց են տալիս իրենց գերազանցությունը գրեթե բոլոր բնական լեզվի վերլուծության խնդիրներում, սակայն նրանց բարդությունը աճում է անհանգստացնում: Հատկապես մտահոգված է այն, որ այս ցանցերը բարձր պահանջներ են հանդիսանում համակարգչային սարքավորումների և ուսուցման բյուջեսի համար: The state-of-the-art transformer models are a vivid example.  Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity.  In this paper, we propose an end to end binarized neural network for the task of intent and text classification.  Բինարիզացիայի վերջ և վերջ պոտենցիալի ամբողջովին օգտագործելու համար, և ներմուծների ներմուծների ներմուծքները (վիճակագրության վիճակագրության վեկտորների ներմուծքները) և դասակարգիչը երկարավորված են: We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset.  Հաշվի առնելով տվյալների համակարգերը, առաջարկած ցանցը համեմատուկ է ամենաբարձր արդյունքների հետ, օգտագործելով 20-40 տոկոսով ավելի քիչ հիշողություն և ուսուցման ժամանակ, համեմատությամբ հարաբերականների հետ:', 'sk': 'Globoka nevronska omrežja so pokazala svojo vrhunsko zmogljivost pri skoraj vseh nalogah obdelave naravnega jezika, vendar njihova naraščajoča kompleksnost vzbuja zaskrbljenost. Posebna skrb je, da ta omrežja predstavljajo visoke zahteve za računalniško strojno opremo in proračun za usposabljanje. Najsodobnejši modeli transformatorjev so živ primer. Poenostavitev izračunov, ki jih izvaja omrežje, je eden od načinov reševanja vprašanja naraščajoče kompleksnosti. V prispevku predlagamo binarizirano nevronsko omrežje od konca do konca za nalogo namena in klasifikacije besedila. Da bi v celoti izkoristili potencial binarizacije od konca do konca, sta tako vhodni predstavitvi (vektorske vdelave statistike žetonov) kot klasifikator binarizirani. Prikazujemo učinkovitost takšne mreže pri namenski klasifikaciji kratkih besedil preko treh naborov podatkov in klasifikaciji besedil z večjim naborom podatkov. Na obravnavanih naborih podatkov predlagana mreža dosega primerljive z najsodobnejšimi rezultati, hkrati pa uporablja 20-40% manj pomnilnika in časa usposabljanja v primerjavi z referenčnimi vrednostmi.', 'ha': "Haƙĩƙa, masu ƙaranci na neura sun nuna mafiya kyautar aiki a cikin duk aikin aiki na Taural, kuma amma, ƙaranci da adadin su na ƙara yana ƙaranci masu hushi. Babu wani abu na hushi da cẽwa waɗancan garwaya sun ƙayyade umakinta mai girma ga lissafi na'urar jerofayuka da mai ƙidãya. Halin mai musanya na-sanar-zasu misali ne mai kifi. Yi sauƙi da lissafin da aka aiko na shirin wata shawara, yana da hanya guda zuwa masu shirya matsalar masu ƙara da za'a ƙãra. In this paper, we propose an end to end binarized neural network for the task of intent and text classification.  Dõmin ya yi amfani da cikakken ikon ya ƙara bincike-bincike, za'a riƙe masu motsi da ake cikin cikin shirin ayuka (masu shiga cikin fassarar ayuka) da mai fassarawa. Kana nũna fikar wannan shirin tarakin nan a kan ƙididdige matsayin nau'i-nau'in rubutu ko kuma darajan matsayi na ƙaranci. Daga tsarin da aka yi bincike, shirin kwamfyuta wanda aka buƙata yana sami da fassarar-the-art sami kuma yana yin amfani da lokaci ƙaranci daga lokacin lokaci na lokaci da ya yi amfani da lokaci 20-40 sami sami da bangon-bangon.", 'jv': 'Open Source Bengkane kabeh wektu sing paling-paling kuwi tambah kuwi wis ana sakjane perusahaan kanggo ngakus batirmu lan tukang budheh. Laptop" and "Desktop Display routing Slackfree" rather than "slackfree Display boxes"), ("Display boxes instead of paths only"), "paths Awak dhéwé éntukno efekasi tambah sing dibenalke seneng pisan kelas nang butuh dataset karo telu dataset karo perusahaan seneng dataset sing wis luwih Nanging dataset, online supoyo nggawe gerangkamu penting nggawe gerangkamu karo hal-karangkamu supoyo sekondiki sing nyimpen gampang', 'he': 'רשתות עצביות עמוקות הראו את ההופעה העליונה שלהם כמעט בכל משימה מעבדת שפת טבעית, עם זאת, המורכבות הגדולה שלהם מעלה דאגות. דאגה מיוחדת היא שהרשתות האלה מציבות דרישות גבוהות לחישוב חומרים ומקציב אימונים. דוגמאות המעבר המיוחדות הן דוגמא חיה. הפשטת החישובים שעושים על ידי רשת היא דרך אחת להתמודד עם הנושא של המורכבות הגדולה. בעיתון הזה, אנחנו מציעים סוף לסיים רשת עצבית בינריזת למשימה של הכוונה והקליזציה טקסטית. כדי להשתמש באופן מלא בפוטנציאל של הסוף עד הסוף בינריזציה, שני מייצגי הכניסה (הוקצות וקטורים של סטטיסטיקה של סימנים) והמסווג בינריזציה. אנו מראים את היעילות של רשת כזו על סיפור הכוונה של טקסטים קצרים מעל שלושה קבוצות נתונים וטקסטים עם קבוצת נתונים גדולה יותר. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.', 'bo': 'ནའང་མིའི་དྲ་རྒྱ་སྟངས་དྲ་རྒྱ་གྱིས་རང་ཉིད་ཀྱི་ལས་འཕར་རིས་སྒེར་གྱི་རེ་བ་ཡིན་ནའང་མིའི་སྐད་ཆ་ལས་སྦྱོར་ལས་ཀར་ཆེ་ཤ དྲ་དམངས་གཙོ་བོ་ཞིག་ནི་ཚོགས་འབྲེལ་འདིས་མཉེན་ཆས་དང་རྩིས་སྒྲིག་ཆ་རྩིས་འཁོར་སྐྱོང་བ་དང་། སྔོན་སྒྲིག་གི་གནས་སྟངས་དང་འཆར་བརྒྱུད་པའི་མིག་ཆས་འདི་དཔེར་བརྗོད་པ་ཞིག་རེད། དྲ་བ་ཞིག་གིས་ལག་སྟར་བྱེད་པའི་རྩིས་འཁོར་སྟོན་པ་དེ་ཆེས་དཀའ་ངལ་ཆེ་ཤོས་ལ་བསླབ་ཐབས་ཤིག་རེད། འུ་ཅག་གིས་ཤོག་བྱང་འདིའི་ནང་དུ་དམིགས་ཡུལ་དང་ཡིག་གི་དབྱེ་རིགས་ལ་འཇོག་རྒྱུ་དང་མཇུག་མཐུན་གྱི་རྩོམ་པ་ཞིག In order to fully use the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. ང་ཚོས་ཡིག་ཆ་རག་པའི་དབྱིབས་ཡིག་ཆ་གསུམ་དང་ཁ་ཡིག་གི་དབྱེ་སྟངས་ལ་ལག་སྟར་བཀྲམ་སྟོན་ཡོད་པ། འཆར་ཡོད་པའི་གནད་སྡུད་ཚང་ནང་དུ་བྱ་ཟིན་པའི་དྲ་བ་དྲ་བ་དང་མཉམ་སྤྲོད་ཀྱི་གནས་སྟངས་དང་མཐུན་རྐྱེན་ཐུབ་པ་ཡིན།'}
{'en': 'Exploring the Boundaries of Low-Resource BERT Distillation', 'ar': 'استكشاف حدود تقطير بيرت منخفض الموارد', 'pt': 'Explorando os limites da destilação BERT de poucos recursos', 'es': 'Explorando los límites de la destilación de BERT de bajos recursos', 'fr': 'Exploration des limites de la distillation BERT à faibles ressources', 'ja': '低資源バート蒸留の境界を探る', 'zh': '探低资源BERT蒸馏界', 'hi': 'कम संसाधन BERT आसवन की सीमाओं की खोज', 'ru': 'Изучение границ низкоресурсной дистилляции BERT', 'ga': 'Iniúchadh ar Theorainneacha Dhriogadh Íseal Acmhainne CRET', 'ka': 'დამატებული რესურსის ბერტის განსხვავება', 'hu': 'Az alacsony erőforrású BERT desztilláció határainak feltárása', 'el': 'Εξερεύνηση των ορίων της απόσταξης BERT χαμηλής περιεκτικότητας σε πόρους', 'it': 'Esplorare i confini della distillazione BERT a basso contenuto di risorse', 'kk': 'Төменгі ресурс BERT шектерін зерттеу', 'lt': 'Mažų išteklių BERT distiliacijos ribų tyrimas', 'ms': 'Menjelaskan Had Distillasi BERT Sumber Terrendah', 'mk': 'Истражување на границите на дистилацијата BERT со ниски ресурси', 'ml': 'കുറഞ്ഞ വിഭവങ്ങളുടെ അതിരുകള്\u200d പരിശോധിക്കുന്നു', 'mn': 'Бага боловсролын хэмжээсүүдийн хэмжээсүүдийг судалж,', 'pl': 'Badanie granic destylacji BERT o niskich zasobach', 'no': 'Utforskar grensene for låg- ressurs BERT- distribusjon', 'ro': 'Explorarea limitelor distilării BERT cu resurse reduse', 'si': 'බෙර්ට් විශාලයේ අඩු සම්පත්ත සීමාව ප්\u200dරශ්නය කරන්න', 'sr': 'Istražujući granice niskog resursa', 'sv': 'Utforska gränserna för BERT-destillation med låg resurs', 'ta': 'குறைந்த மூலத்தின் எல்லைகளை கண்டுபிடிக்கிறது', 'so': 'Xuduudaha Low-Resource BERT Distribution', 'mt': 'L-esplorazzjoni tal-Limiti tad-Distillazzjoni BERT b’Riżorsi Bażi', 'ur': 'نیچے رسورس BERT تقسیم کے مرزے تحقیق کرتے ہیں', 'uz': 'Exploring the Boundaries of Low-Resource BERT Distillation', 'vi': 'Khám phá Kết giới của phải nấu đông phải', 'bg': 'Изследване на границите на дестилацията BERT с ниски ресурси', 'da': 'Udforskning af grænserne for BERT-destillation med lav ressource', 'de': 'Erforschung der Grenzen der ressourcenarmen BERT-Destillation', 'id': 'Exploring the Boundaries of Low-Resource BERT Distillation', 'nl': 'De grenzen van BERT-distillatie met lage hulpbronnen verkennen', 'fa': 'تحقیق مرزهای تقسیم کم منابع BERT', 'sw': 'Kuelezea mipaka ya Kupungua rasilimali BERT', 'tr': "BERT'iň azan resurslaryň döwletlerini barlaýar", 'af': 'Verskyn die Grense van Laag- Hulpbron BERT Verspreiding', 'ko': '저자원의 경계를 탐색하다', 'am': 'BERT መደረጃ ምርጫዎች', 'hy': 'Նվագ ռեսուրսների BER դիստիլացիայի սահմանները ուսումնասիրելը', 'hr': 'Ispitivanje granica destilacije BERT niskog resursa', 'sq': 'Exploring the Boundaries of Low-Resource BERT Distillation', 'ca': 'Explorar els límits de la destilació BERT de baix recursos', 'bs': 'Ispitivanje granica destilacije BERT-a s niskim resursima', 'bn': 'নিম্ন- সম্পদ বেরেট ডিস্ট্রিলেশনের সীমানা অনুসন্ধান করা হচ্ছে', 'et': 'Väikese ressursiga BERT destilleerimise piiride uurimine', 'fi': 'Vähävaraisen BERT-tislauksen rajojen tutkiminen', 'cs': 'Zkoumání hranic destilace BERT s nízkými zdroji', 'az': 'A≈üaƒüƒ±-Ressours BERT S…ôdirl…ôrini Exploring the boundaries of Low-Resource Distillation', 'jv': 'Ngubah Jejaring', 'he': 'חקירת הגבולות של שינוי BERT של משאבים נמוכים', 'ha': 'KCharselect unicode block name', 'sk': 'Raziskovanje meja destilacije BERT z nizkimi viri', 'bo': 'BERT སྒྲིག་ཐང་ཚད་དཔག་པོའི་བར་ཆ་ཉུང་བའི་ཚད་གཞི་བཙལ་ཞིབ་བྱེད་བཞིན་པ'}
{'en': 'In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models’ large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, computational load and data efficiency. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.', 'ar': 'في السنوات الأخيرة ، أظهرت النماذج الكبيرة المدربة مسبقًا أداءً متطورًا في العديد من مهام البرمجة اللغوية العصبية. ومع ذلك ، فإن نشر هذه النماذج على الأجهزة ذات الموارد المحدودة يمثل تحديًا بسبب الاستهلاك الحسابي الكبير للنماذج ومتطلبات الذاكرة. علاوة على ذلك ، فإن الحاجة إلى قدر كبير من بيانات التدريب المصنفة تعيق أيضًا سيناريوهات النشر في العالم الحقيقي. أظهر التقطير النموذجي نتائج واعدة لتقليل حجم النموذج والحمل الحسابي وكفاءة البيانات. في هذا البحث قمنا باختبار حدود تقطير نموذج بيرت من حيث ضغط النموذج وكفاءة الاستدلال وندرة البيانات. نوضح أن مهام التصنيف التي تتطلب التقاط دلالات معجمية عامة يمكن تقطيرها بنجاح من خلال نماذج بسيطة وفعالة للغاية وتتطلب كمية صغيرة نسبيًا من بيانات التدريب المصنفة. نظهر أيضًا أن تقطير النماذج الكبيرة المدربة مسبقًا أكثر فعالية في سيناريوهات الحياة الواقعية حيث تتوفر كميات محدودة من التدريب المسمى.', 'pt': 'Nos últimos anos, grandes modelos pré-treinados demonstraram desempenho de última geração em muitas tarefas de PNL. No entanto, a implantação desses modelos em dispositivos com recursos limitados é um desafio devido ao grande consumo computacional e requisitos de memória dos modelos. Além disso, a necessidade de uma quantidade considerável de dados de treinamento rotulados também dificulta os cenários de implantação do mundo real. A destilação do modelo mostrou resultados promissores para reduzir o tamanho do modelo, a carga computacional e a eficiência dos dados. Neste artigo, testamos os limites da destilação do modelo BERT em termos de compressão do modelo, eficiência de inferência e escassez de dados. Mostramos que tarefas de classificação que requerem a captura de semântica léxica geral podem ser destiladas com sucesso por modelos muito simples e eficientes e requerem uma quantidade relativamente pequena de dados de treinamento rotulados. Também mostramos que a destilação de grandes modelos pré-treinados é mais eficaz em cenários da vida real, onde quantidades limitadas de treinamento rotulado estão disponíveis.', 'es': 'En los últimos años, los grandes modelos preentrenados han demostrado un rendimiento de vanguardia en muchas de las tareas de PNL. Sin embargo, la implementación de estos modelos en dispositivos con recursos limitados es un desafío debido al gran consumo computacional y los requisitos de memoria de los modelos. Además, la necesidad de una cantidad considerable de datos de entrenamiento etiquetados también dificulta los escenarios de implementación en el mundo real. La destilación de modelos ha mostrado resultados prometedores para reducir el tamaño del modelo, la carga computacional y la eficiencia de los datos. En este artículo probamos los límites de la destilación del modelo BERT en términos de compresión del modelo, eficiencia de inferencia y escasez de datos. Demostramos que las tareas de clasificación que requieren la captura de semántica léxica general pueden destilarse con éxito mediante modelos muy simples y eficientes y requieren una cantidad relativamente pequeña de datos de entrenamiento etiquetados. También demostramos que la destilación de grandes modelos preentrenados es más eficaz en escenarios de la vida real en los que se dispone de cantidades limitadas de capacitación etiquetada.', 'fr': "Ces dernières années, de grands modèles pré-entraînés ont démontré des performances de pointe dans de nombreuses tâches de PNL. Cependant, le déploiement de ces modèles sur des appareils aux ressources limitées est difficile en raison de la consommation de calcul importante des modèles et de leurs exigences en matière de mémoire. En outre, le besoin d'une quantité considérable de données de formation étiquetées entrave également les scénarios de déploiement dans le monde réel. La distillation de modèles a donné des résultats prometteurs pour réduire la taille du modèle, la charge de calcul et l'efficacité des données. Dans cet article, nous testons les limites de la distillation du modèle BERT en termes de compression du modèle, d'efficacité d'inférence et de rareté des données. Nous montrons que les tâches de classification qui nécessitent la saisie de sémantique lexicale générale peuvent être distillées avec succès par des modèles très simples et efficaces et nécessitent une quantité relativement faible de données d'apprentissage étiquetées. Nous montrons également que la distillation de grands modèles pré-entraînés est plus efficace dans des scénarios réels où des quantités limitées de formations étiquetées sont disponibles.", 'ja': '近年、大規模な事前訓練モデルは、多くのNLPタスクで最先端のパフォーマンスを示しています。 しかし、これらのモデルのリソースが限られているデバイスへの展開は、モデルの大きな計算消費量とメモリ要件のために困難です。 さらに、かなりの量のラベル付けされたトレーニングデータの必要性は、実際の展開シナリオを妨げる。 モデル蒸留は、モデルサイズ、計算負荷、およびデータ効率を低減するための有望な結果を示しています。 本稿では、モデル圧縮、推論効率、およびデータ希少性の観点から、BERTモデル蒸留の境界を検証する。 一般的な語彙セマンティクスのキャプチャを必要とする分類タスクは、非常に単純で効率的なモデルによって成功裏に抽出することができ、比較的少量のラベル付けされたトレーニングデータを必要とすることが示されています。 また、大規模な事前トレーニングモデルの蒸留は、限られた量のラベル付けされたトレーニングが利用可能な現実のシナリオでより効果的であることも示しています。', 'zh': '近年以来,大预训练多NLP,务在先进。 然计内存多,故设备于有限而挑战性之。 此外,多所标练数据,亦阻实部方案。 凡模蒸馏于减小尺寸、算载、数效率见有期。 于本文中,于模压缩、推理效率、数据稀缺性试BERT提炼之界。 吾明得词汇语义之类,可以非常简单效,可以少量而练数也。 又明训练量有限之现实生活,有效于大预。', 'hi': 'हाल के वर्षों में, बड़े पूर्व-प्रशिक्षित मॉडल ने एनएलपी कार्यों में से कई में अत्याधुनिक प्रदर्शन का प्रदर्शन किया है। हालांकि, सीमित संसाधनों वाले उपकरणों पर इन मॉडलों की तैनाती मॉडल की बड़ी कम्प्यूटेशनल खपत और स्मृति आवश्यकताओं के कारण चुनौतीपूर्ण है। इसके अलावा, लेबल किए गए प्रशिक्षण डेटा की काफी मात्रा की आवश्यकता भी वास्तविक दुनिया के परिनियोजन परिदृश्यों में बाधा डालती है। मॉडल आसवन ने मॉडल आकार, कम्प्यूटेशनल लोड और डेटा दक्षता को कम करने के लिए आशाजनक परिणाम दिखाए हैं। इस पेपर में हम मॉडल संपीड़न, अनुमान दक्षता और डेटा की कमी के संदर्भ में BERT मॉडल आसवन की सीमाओं का परीक्षण करते हैं। हम दिखाते हैं कि वर्गीकरण कार्य जिन्हें सामान्य लेक्सिकल शब्दार्थ पर कब्जा करने की आवश्यकता होती है, उन्हें बहुत ही सरल और कुशल मॉडल द्वारा सफलतापूर्वक आसुत किया जा सकता है और अपेक्षाकृत कम मात्रा में लेबल किए गए प्रशिक्षण डेटा की आवश्यकता होती है। हम यह भी दिखाते हैं कि बड़े पूर्व-प्रशिक्षित मॉडल का आसवन वास्तविक जीवन के परिदृश्यों में अधिक प्रभावी है जहां लेबल प्रशिक्षण की सीमित मात्रा उपलब्ध है।', 'ru': 'В последние годы крупные предварительно обученные модели продемонстрировали самые современные показатели во многих задачах NLP. Тем не менее, развертывание этих моделей на устройствах с ограниченными ресурсами является сложной задачей из-за большого вычислительного потребления моделей и требований к памяти. Кроме того, потребность в значительном объеме маркированных данных о профессиональной подготовке также затрудняет реальные сценарии развертывания. Модельная дистилляция показала многообещающие результаты для уменьшения размера модели, вычислительной нагрузки и эффективности данных. В этой статье мы тестируем границы дистилляции модели BERT с точки зрения сжатия модели, эффективности вывода и дефицита данных. Показано, что задачи классификации, требующие захвата общей лексической семантики, могут быть успешно дистиллированы очень простыми и эффективными моделями и требуют относительно небольшого объема маркированных обучающих данных. Мы также показываем, что дистилляция больших предварительно обученных моделей более эффективна в реальных сценариях, где имеются ограниченные объемы обучения с маркировкой.', 'ga': 'Le blianta beaga anuas, léirigh samhlacha móra réamhoilte feidhmíocht úrscothach i gcuid mhaith de thascanna NLP. Mar sin féin, tá sé dúshlánach na samhlacha sin a imscaradh ar fheistí a bhfuil acmhainní teoranta acu mar gheall ar riachtanais mhóra tomhaltais ríomhaireachta agus cuimhne na samhlacha. Ina theannta sin, cuireann an gá atá le méid suntasach sonraí oiliúna lipéadaithe bac freisin ar chásanna imlonnaithe sa saol mór. Tá torthaí geallta léirithe ag driogadh samhlacha maidir le méid an mhúnla a laghdú, ualach ríomhaireachtúil agus éifeachtúlacht sonraí. Sa pháipéar seo déanaimid tástáil ar theorainneacha driogtha múnla BERT i dtéarmaí comhbhrú múnla, éifeachtúlacht tátail agus ganntanas sonraí. Léirímid gur féidir tascanna aicmithe a éilíonn gabháil shéimeantaic ghinearálta fhoclóra a dhriogadh go rathúil trí mhúnlaí an-simplí agus éifeachtacha agus go dteastaíonn méid measartha beag sonraí oiliúna lipéadaithe uathu. Léirímid freisin go bhfuil driogadh samhlacha móra réamhoilte níos éifeachtaí i gcásanna fíor-saoil ina bhfuil méid teoranta oiliúna lipéadaithe ar fáil.', 'ka': 'ბოლო წლის შემდეგ, დიდი წინასწარმოადგენებული მოდელები მოდენსტრებულია NLP მომუშაობების მრავალში. მაგრამ ამ მოდელების გამოყენება მოწყობილობისთვის განსაზღვრებული რესურსების მოდელების დიდი კომპუტაციალური გამოყენება და მეხსიერების განსაზღვრებისთვის შესაძლებელია დამატებით, მნიშვნელოვანი მნიშვნელოვანი მნიშვნელოვანი მონაცემების მონაცემების შესაძლებლობა შეუძლებელია რეალური მსოფლიოს სენარიოები. მოდელის დისტლიაცია ჩვენება მოდელის ზომა, კომპუტაციონალური ჩატვირთვა და მონაცემების ეფექტიურობის გადასრულებელი შედეგები. ამ დოკუნტში ჩვენ შევცვალობთ BERT მოდელის დისტლიაციის დროები მოდელის კომპრეციის, ინფრენციის ეფექციურობის და მონაცემების შესახებ. ჩვენ ჩვენ აჩვენებთ, რომ კლასიფიკაციის დავალებები, რომლებიც საჭირო ყველა ლექსიკალური სმენტიკის დაატვირთვა, შეიძლება წარმატებით გადასვლა ძალიან მარტივი და ეფექტიური მო ჩვენ ასევე გამოჩვენებთ, რომ დისტალაცია დიდი წინატვირთებული მოდელების უფრო ეფექტიურია რეალური ცხოვრების სინარიოში, რომლებიც შესაძლებელია უფრო ცხოვრებული მოდელ', 'hu': 'Az elmúlt években a nagy, előre képzett modellek a legkorszerűbb teljesítményt mutatták számos NLP feladatban. Ezeknek a modelleknek a korlátozott erőforrásokkal rendelkező eszközökre történő telepítése azonban kihívást jelent a modellek nagy számítástechnikai fogyasztása és memóriaigényei miatt. Ezenkívül a jelentős mennyiségű címkézett képzési adat szükségessége is akadályozza a valós bevezetési forgatókönyveket. A modell desztillációja ígéretes eredményeket mutatott a modell mérete, a számítási terhelés és az adathatékonyság csökkentésében. Ebben a tanulmányban teszteljük a BERT modell desztillációjának határait modelltömörítés, következtetési hatékonyság és adathűség szempontjából. Megmutatjuk, hogy az általános lexikai szemantika rögzítését igénylő osztályozási feladatok nagyon egyszerű és hatékony modellekkel sikeresen lepárlásolhatók, és viszonylag kis mennyiségű címkézett képzési adatot igényelnek. Azt is megmutatjuk, hogy a nagyméretű, előre képzett modellek desztillációja hatékonyabb a valós forgatókönyvekben, ahol korlátozott mennyiségű címkézett képzés áll rendelkezésre.', 'el': 'Τα τελευταία χρόνια, μεγάλα προ-εκπαιδευμένα μοντέλα έχουν επιδείξει επιδόσεις τελευταίας τεχνολογίας σε πολλά από τα καθήκοντα NLP. Ωστόσο, η ανάπτυξη αυτών των μοντέλων σε συσκευές με περιορισμένους πόρους αποτελεί πρόκληση λόγω της μεγάλης υπολογιστικής κατανάλωσης και των απαιτήσεων μνήμης των μοντέλων. Επιπλέον, η ανάγκη για σημαντική ποσότητα δεδομένων κατάρτισης με ετικέτα εμποδίζει επίσης τα σενάρια ανάπτυξης πραγματικού κόσμου. Η απόσταξη προτύπων έχει δείξει πολλά υποσχόμενα αποτελέσματα για τη μείωση του μεγέθους του μοντέλου, του υπολογιστικού φορτίου και της αποδοτικότητας δεδομένων. Σε αυτή την εργασία εξετάζουμε τα όρια της απόσταξης μοντέλου από την άποψη της συμπίεσης μοντέλου, της αποδοτικότητας συμπερασμάτων και της έλλειψης δεδομένων. Δείχνουμε ότι οι εργασίες ταξινόμησης που απαιτούν την καταγραφή της γενικής λεξικής σημασιολογίας μπορούν να αποσταχθούν με επιτυχία από πολύ απλά και αποτελεσματικά μοντέλα και απαιτούν σχετικά μικρή ποσότητα επισημασμένων δεδομένων κατάρτισης. Δείχνουμε επίσης ότι η απόσταξη μεγάλων προ-εκπαιδευμένων μοντέλων είναι πιο αποτελεσματική σε σενάρια πραγματικής ζωής όπου υπάρχουν περιορισμένες ποσότητες εκπαίδευσης με ετικέτα.', 'it': "Negli ultimi anni, grandi modelli pre-addestrati hanno dimostrato prestazioni all'avanguardia in molte delle attività NLP. Tuttavia, l'implementazione di questi modelli su dispositivi con risorse limitate è difficile a causa dei grandi consumi computazionali e requisiti di memoria dei modelli. Inoltre, la necessità di una notevole quantità di dati formativi etichettati ostacola anche gli scenari di diffusione nel mondo reale. La distillazione dei modelli ha mostrato risultati promettenti per ridurre le dimensioni del modello, il carico computazionale e l'efficienza dei dati. In questo articolo testiamo i limiti della distillazione del modello BERT in termini di compressione del modello, efficienza di inferenza e scarsità di dati. Mostriamo che i compiti di classificazione che richiedono l'acquisizione della semantica lessicale generale possono essere distillati con successo da modelli molto semplici ed efficienti e richiedono una quantità relativamente piccola di dati di formazione etichettati. Mostriamo anche che la distillazione di grandi modelli pre-addestrati è più efficace in scenari reali in cui sono disponibili quantità limitate di formazione etichettata.", 'kk': 'Соңғы жылдар бойынша, көп NLP тапсырмаларында үлкен алдын- оқылған үлгілерді көрсету үшін. Бірақ бұл үлгілерді шектелген ресурстардың құрылғыларына жүктеу үлкен компьютерлік пайдалану мен жады талаптарының үлкен үлкен құрылғыларына көмектесуі болады. Сонымен қатар, жарлық мәліметтердің маңызды мәліметтері жұмыс істеу сценариясын бастайды. Үлгі дислилациясы үлгі өлшемін, компьютерлік жүктеу және деректердің эффективнігін азайту үшін үлгі нәтижелерін көрсетті. Бұл қағазда біз BERT үлгісінің шектерін шектеп сәйкестіруді, көмектесуді және деректердің жеткілігін тексереміз. Біз жалпы лексикалық семантикаларды түсіру керек классификациялық тапсырмаларын өте қарапайым және эффективні моделлерден бөліп, сәйкес келіп жазылған мәліметтерді керек болады. Біз сондай-ақ үлкен алдын- оқылған үлгілердің дистриляциясы шындық өмір сценариясында көбірек болып тұрады. Бұл жерде жарлықталған оқыту мөлшерлері бар.', 'lt': 'Pastaraisiais metais dideli iš anksto parengti modeliai parodė pažangiausius rezultatus daugelyje NLP užduočių. Tačiau šių modelių diegimas ribotų išteklių prietaisams yra sunkus dėl didelių modelių skaičiavimo suvartojimo ir atminties reikalavimų. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios.  Distiliacijos modelis parodė perspektyvius modelio dydžio, skaičiavimo apkrovos ir duomenų efektyvumo mažinimo rezultatus. Šiame dokumente bandome BERT modelio distiliavimo ribas modelio suspaudimo, išvadų efektyvumo ir duomenų trūkumo atžvilgiu. Mes parodome, kad klasifikavimo užduotys, kurioms reikalingas bendrosios leksinės semantikos surinkimas, gali būti sėkmingai distiliuojamos labai paprastai ir veiksmingais modeliais ir reikalauja palyginti mažo kiekio pažymėtų mokymo duomenų. Taip pat parodome, kad didelių iš anksto apmokytų modelių distiliavimas yra veiksmingesnis realiojo gyvenimo scenarijais, kai yra ribotas ženklinamo mokymo kiekis.', 'mk': 'Во последниве години големите предобучени модели покажаа најсовремени резултати во многу од задачите на НЛП. Сепак, распоредувањето на овие модели на уредите со ограничени ресурси е предизвикувачки поради големите обврзнички потрошувања и потреби од меморија на моделите. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios.  Моделната дистилација покажа ветувачки резултати за намалување на големината на моделот, компјутерската товарност и ефикасноста на податоците. Во оваа хартија ги тестираме границите на дистилацијата на моделот BERT во поглед на моделот компресија, ефикасност на инференцијата и недостаток на податоци. Ние покажуваме дека класификациските задачи кои бараат апсење на генералната лексикална семантика може да бидат успешно дестилирани со многу едноставни и ефикасни модели и бараат релативно мала количина обележани податоци за обука. Исто така покажуваме дека дестилацијата на големите предобучени модели е поефикасна во реалните сценарија каде што се достапни ограничени количини обележани обуки.', 'ms': 'Pada tahun-tahun terakhir, model besar yang dilatih-dilatih telah menunjukkan prestasi state-of-the-art dalam banyak tugas NLP. Namun, penerbangan model ini pada peranti dengan sumber terbatas adalah menantang disebabkan keperluan penggunaan dan ingatan besar model. Selain itu, perlukan sejumlah besar data latihan yang ditabel juga menghalangi skenario penerbangan dunia nyata. Distillasi model telah menunjukkan keputusan yang berjanji untuk mengurangkan saiz model, muatan pengiraan dan kegunaan data. Dalam kertas ini, kita menguji sempadan pemampatan model BERT dalam terma pemampatan model, efisiensi kesimpulan dan kekurangan data. Kami menunjukkan bahawa tugas klasifikasi yang memerlukan penangkapan semantik leksikal umum boleh berjaya dipotong oleh model yang sangat mudah dan efisien dan memerlukan jumlah relatif kecil data latihan yang ditabel. Kami juga menunjukkan bahawa penapisan model besar yang dilatih-dilatih lebih berkesan dalam skenario kehidupan sebenar di mana jumlah terbatas pelatihan yang ditanda tersedia.', 'ml': 'അടുത്ത കൊല്ലങ്ങളില്\u200d, മുമ്പ് പരിശീലിക്കപ്പെട്ടിരിക്കുന്ന വലിയ മോഡലുകള്\u200d NLP ജോലികളില്\u200d അധികപേരില്\u200d കാണിച്ചിരിക എന്നാലും മോഡലുകളുടെ വലിയ കണക്കൂട്ടുണ്ടാക്കുന്നതും മെമ്മറിയുടെ ആവശ്യങ്ങളും കാരണം ഈ മോഡലുകള്\u200d ഉപകരണങ്ങളില്\u200d വിതരണം ചെയ്യുന് അതുകൊണ്ടും, ഒരുപാട് ലേബല്\u200d ചെയ്ത പരിശീലന വിവരങ്ങളുടെ ആവശ്യമുണ്ട്, യഥാര്\u200dത്ഥ ലോകത്തിലെ പ്രവർത്തിപ്പിക്കു മോഡല്\u200d വലിപ്പം, കണക്കുണ്ടാക്കുന്ന ലോഡും ഡേറ്റാ സാധ്യതയും കുറയ്ക്കുന്നതിനുള്ള പ്രത്യേക ഫലങ്ങള്\u200d മോഡല്\u200d വി ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d ബെര്\u200dട്ടി മോഡല്\u200d വേര്\u200dതിരിക്കുന്നതിന്\u200dറെ അതിരുകള്\u200d പരീക്ഷിക്കുന്നു. മോഡല്\u200d സമ്പൂര്\u200dണ്ണമാക്കുന്നത്, അപകടത സാധാരണ ലെക്സിക്കല്\u200d സെമാന്റിക്കുകളെ പിടികൂടാന്\u200d ആവശ്യമുള്ള ക്ലാസ്ഫിക്ഷന്\u200d ജോലികള്\u200d വിജയകരമായി വ്യത്യസ്തമാക്കുവാന്\u200d സാധിക്കുന്നു. വളരെ സ We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.', 'mn': 'Сүүлийн жилүүдэд маш олон сургалтын загварууд NLP даалгаварын олон даалгавруудын төвшин урлагийн үйл ажиллагааг харуулсан. Гэхдээ эдгээр загваруудыг хязгаарлагдсан нөөцийн төхөөрөмж дээр ашиглах нь загваруудын том тооны хэрэглээ болон санамж хэрэглэх шаардлагатай учраас шаардлагатай. Үүнээс гайхалтай хэмжээний суралцах өгөгдлийн хэрэгцээ нь мөн жинхэнэ дэлхийн хөгжлийн хувилбаруудыг зогсоож байна. Загварын шинжилгээ нь загварын хэмжээг, тооцоололтын түүх, өгөгдлийн үр дүнг багасгахад амлалтай үр дүнг харуулсан. Энэ цаасан дээр бид BERT загварын хуваалцааны хил хязгаарыг загварын даралт, халдварын үр дүнтэй болон өгөгдлийн хамааралтай холбоотой талаар шалгана. Бид энгийн, үр дүнтэй загвараас амжилттай хуваалцаж, харьцангуй жижиг хэмжээний тэмдэглэгдсэн сургалтын өгөгдлийг хэрэгжүүлж чадна гэдгийг харуулж байна. Мөн бид илүү их сургалтын загварын тодорхойлолт нь жинхэнэ амьдралын хувилбарт илүү үр дүнтэй гэдгийг харуулж байна.', 'no': 'I løpet av siste år har stor føretrengde modeller demonstrert utviklinga av kunsten i mange av NLP-oppgåver. Desse modelane er imidlertid vanskeleg å utføra på einingar med begrensede ressursar på grunn av modellen større databruk og minnebruk. I tillegg kan det også hindra scenarioane for verkeleg utviklingar for å ha eit stort mengd merket opplæringsdata. Name I denne papiret tester vi grensene for BERT-modelledestilasjon i uttrykket av komprimering, infeksjonsfeilighet og data feil. Vi viser at klassifikasjonsoppgåver som krev å henta generelle leksiske semantikk kan vellukkast distilerast av veldig enkle og effektive modeller og krev relativt lite mengda merkelige treningsdata. Vi viser også at destilasjonen av store føreøvinga modeller er meir effektiv i verkeleg livsskrinisjonar der begrenset mengder merkelige trening er tilgjengeleg.', 'pl': 'W ostatnich latach duże wstępnie przeszkolone modele wykazały najnowocześniejszą wydajność w wielu zadaniach NLP. Jednak wdrożenie tych modeli na urządzeniach o ograniczonych zasobach jest trudne ze względu na duże zużycie obliczeń i wymagania pamięci modeli. Ponadto potrzeba znacznej ilości oznaczonych danych szkoleniowych utrudnia również realne scenariusze wdrożenia. Destylacja modeli wykazała obiecujące rezultaty w zakresie zmniejszenia wielkości modelu, obciążenia obliczeniowego i wydajności danych. W artykule badamy granice destylacji modelu BERT pod względem kompresji modelu, efektywności wnioskowania i niedoboru danych. Pokazujemy, że zadania klasyfikacyjne wymagające przechwytywania ogólnej semantyki leksykalnej mogą być z powodzeniem destylowane za pomocą bardzo prostych i wydajnych modeli i wymagają stosunkowo niewielkiej ilości etykietowanych danych treningowych. Pokazujemy również, że destylacja dużych wstępnie przeszkolonych modeli jest skuteczniejsza w rzeczywistych scenariuszach, w których dostępne są ograniczone ilości szkoleń etykietowanych.', 'ro': 'În ultimii ani, modele mari pre-instruite au demonstrat performanțe de ultimă oră în multe dintre sarcinile PNL. Cu toate acestea, implementarea acestor modele pe dispozitive cu resurse limitate este o provocare datorită consumului computațional mare și cerințelor de memorie ale modelelor. În plus, necesitatea unei cantități considerabile de date de formare etichetate împiedică, de asemenea, scenariile de implementare în lumea reală. Distilarea modelului a arătat rezultate promițătoare pentru reducerea dimensiunii modelului, a sarcinii computaționale și a eficienței datelor. În această lucrare testăm limitele distilării modelului BERT în ceea ce privește compresia modelului, eficiența inferenței și deficitul de date. Arătăm că sarcinile de clasificare care necesită captarea semanticii lexicale generale pot fi distilate cu succes prin modele foarte simple și eficiente și necesită cantități relativ mici de date de instruire etichetate. De asemenea, demonstrăm că distilarea modelelor mari pre-instruite este mai eficientă în scenariile reale în care sunt disponibile cantități limitate de formare etichetată.', 'sr': 'Za poslednje godine, veliki predobučeni modeli su pokazali predstavu umjetnosti u mnogim zadacima NLP-a. Međutim, razvoj tih modela na uređajima sa ograničenim resursima je izazovan zbog velike potrošnje kompjuterskih potrošnja i zahteva pamćenja modela. Osim toga, potreba za značajnom količinom označenih podataka o obuci takođe sprječava scenarije razvoja stvarnog svijeta. Modelna destilacija pokazala je obećavajuće rezultate za smanjenje veličine modela, računalnog opterećenja i efikasnosti podataka. U ovom papiru testiramo granice destilacije modela BERT u smislu kompresije modela, infekcije efikasnosti i nedostatka podataka. Pokazujemo da klasifikacijski zadatak koji zahtevaju uhvaćenje općeg leksičkog semantika može uspešno destilati vrlo jednostavnim i efikasnim modelima i zahtevaju relativno mala količina označenih podataka o obuci. Takođe pokazujemo da je destilacija velikih predobučenih modela efikasniji u scenarijima stvarnog života gde su dostupne ograničene količine označene obuke.', 'so': 'Sannadihii ugu dambeeyey, tusaalooyin waaweyn oo horay loo tababaray waxay ku muujiyeen xaalad farshaxan-farshaxan oo badan oo ah shaqada NLP. Si kastaba ha ahaatee samooyinkaas lagu soo dejiyo qalabka ay haystaan hanti xadiiqad ah waxay ku dhibaataysaa isticmaalka tirada badan iyo baahida xusuusta. Sidoo kale baahida loo baahan yahay macluumaad aad u badan oo waxbarasho la xiriiray ayaa ka hor mariya muuqashada shaqada ee caalamka ah. Isku-bedelka modellka waxaa tusay resultooyin ballan ah oo ay ka fekeraan tirada modellka, xisaabinta iyo faa’iidada macluumaadka. Qoraalkan waxaan ku tijaabinaynaa xuduudaha BERT-modelka, daboolka, faa’iidada iftiinka iyo baahida macluumaadka. Waxaynu tusnaynaa in shaqada fasaxda looga baahan yahay in la qabsado jimicsiga caadiga ah ee leksikada waxaa lagu kala sooci karaa tusaalayaal fudud oo faa’iido leh, waxaana u baahan karaa qiyaastii yar oo waxbarashada la qoray. Sidoo kale waxaynu tusnaynaa in kala soocsiga modeliyada waaweyn ee la tababaray ay ka faa’iido badan yihiin aragtida nololeed ee ay heli karaan waxbarasho cayiman.', 'si': 'අන්තිම අවුරුද්දු වලින්, ලොකු ප්\u200dරධානය කරලා තියෙන්නේ නිල්ප් වැඩක් ගොඩක් වැඩක් තියෙනවා. නමුත්, මේ මොඩේල් එක්ක සීමාවිත උපකරණය සඳහා සීමාවිත උපකරණය සඳහා ප්\u200dරමාණය ප්\u200dරමාණය සඳහා මොඩේල් එක්ක ලොකු ගණණනය ඒ වගේම, ලේබල් කරපු ප්\u200dරධාන දත්ත අවශ්\u200dයයෙන් ඇත්ත ලෝකයේ ප්\u200dරධාන සිද්ධානයක් අවධානය කරනවා. මොඩේල් විස්තරණය පෙන්වන්න ප්\u200dරතිචාර ප්\u200dරතිචාරයක් පෙන්වන්න ප්\u200dරතිචාර ප්\u200dරතිචාර, පරිගණනය ව මේ පත්තරේ අපි BERT මොඩල් විශේෂණයේ සීමාව පරීක්ෂා කරනවා මොඩල් සංකීර්ණය, පරීක්ෂණය සහ දත්ත අවශ්\u200dය. අපි පෙන්වන්නේ සාමාන්\u200dය ලෙක්සිකල් සෙමැන්ටික්ස් එක්ක අල්ලගන්න අවශ්\u200dය විදියට පරීක්ෂා වෙන්න පුළුවන් සාමාන්\u200dය සහ හැකිය අපිට පෙන්වන්න පුළුවන් විශාල ප්\u200dරධානය කරලා තියෙන්න පුළුවන් විශාල ප්\u200dරධානයක් ඇත්ත ජීවිත සිද්ධානයේ වඩා', 'sv': 'Under de senaste åren har stora förkunskaperade modeller visat toppmoderna prestanda i många av NLP-uppgifter. Användningen av dessa modeller på enheter med begränsade resurser är dock utmanande på grund av modellernas stora datorförbrukning och minneskrav. Behovet av en betydande mängd märkta utbildningsdata hindrar också de verkliga scenarierna för utbyggnad. Modelldestillation har visat lovande resultat för att minska modellstorlek, beräkningsbar belastning och dataeffektivitet. I denna uppsats testar vi gränserna för BERT modelldestillation i termer av modellkomprimering, inferenseffektivitet och databrist. Vi visar att klassificeringsuppgifter som kräver insamling av allmän lexikal semantik framgångsrikt kan destilleras med mycket enkla och effektiva modeller och kräver relativt liten mängd märkta träningsdata. Vi visar också att destillationen av stora förutbildade modeller är effektivare i verkliga scenarier där begränsade mängder märkt utbildning finns tillgänglig.', 'ta': 'அண்மைய ஆண்டுகளில், பெரிய பயிற்சி முன்பு பயிற்சி மாதிரிகள் பல NLP பணிகளில் உள்ள கலை செயல்பாட்டின் நிலையைக் காட்டி எனினும், இந்த மாதிரிகள் சாதனங்களின் மீது வெளியேற்றப்படும் வரம்பு வளங்கள் உள்ளன, மாதிரிகளின் பெரிய கணக்கீடு பயன்படுத்தல் மற்று மேலும், ஒரு குறிப்பிட்ட பயிற்சி தகவல் தேவையும் உண்மையான உலக வெளியீட்டு காட்சியை தடுக்கிறது. மாதிரி பிரிவுகள் மாதிரி அளவு, கணக்கீடு ஏற்றம் மற்றும் தரவு விளைவுகளை குறைக்க முடியும் முடிவுகளை காட்டுகிறது. இந்த காகிதத்தில் நாம் BERT மாதிரி பிரிப்பின் எல்லைகளை மாதிரி சுருக்கத்திற்கு, குறைவு விளைவுகள் மற்றும் தரவு குறைவுகள நாம் வகுப்பு பணிகளை காட்டுகிறோம் பொது லெக்சிக்சியல் பிடிப்பது தேவைப்படும் சுலபமான மற்றும் செயல்படும் மாதிரிகளால் வெற்றிகரமாக மாற்றப்பட பெரிய பயிற்சி முன் பயிற்சி மாதிரிகளை பிரிப்பது உண்மையான வாழ்க்கையின் காட்சிகளில் மிகவும் செயல்படுத்தப்படும் என்பதை நாம', 'mt': 'F’dawn l-aħħar snin, mudelli kbar imħarrġa minn qabel urew prestazzjoni avvanzata f’ħafna mill-kompiti tal-NLP. Madankollu, l-użu ta’ dawn il-mudelli fuq tagħmir b’riżorsi limitati huwa sfida minħabba r-rekwiżiti kbar tal-konsum u l-memorja tal-mudelli. Barra minn hekk, il-ħtieġa għal ammont konsiderevoli ta’ dejta ta’ taħriġ ittikkettata tfixkel ukoll ix-xenarji ta’ skjerament fid-dinja reali. Id-distillazzjoni tal-mudell uriet riżultati promettenti għat-tnaqqis tad-daqs tal-mudell, it-tagħbija komputattiva u l-effiċjenza tad-dejta. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity.  Aħna nuru li l-kompiti ta’ klassifikazzjoni li jeħtieġu l-qbid ta’ semantika lexika ġenerali jistgħu jiġu distillati b’suċċess minn mudelli sempliċi u effiċjenti ħafna u jeħtieġu ammont relattivament żgħir ta’ dejta ta’ taħriġ ittikkettata. Aħna nuru wkoll li d-distillazzjoni ta’ mudelli kbar imħarrġa minn qabel hija aktar effettiva f’xenarji reali fejn hemm disponibbli ammonti limitati ta’ taħriġ ittikkettat.', 'ur': 'اگلے سالوں میں، بڑے پیش آموزش کی مدلکوں نے NLP کے بہت سے کاموں میں اثر کی حالت کی فعالیت دکھائی ہے. لیکن، یہ موڈلیوں کو محدود منبع کے دستگاه پر استعمال کرنا مشکل ہے مدلکوں کی بڑی کمپیوٹریشن مصرف اور یاد حاصل کی ضرورت کے باعث۔ اور اس کے علاوہ بہت زیادہ لابلیٹ کی تعلیم دیٹے کی ضرورت بھی حقیقی دنیا کی پرداخت سیناریوں سے روکتی ہے. Model distillation has shown promising results for reducing model size, computational load and data efficiency. ہم اس کاغذ میں BERT موڈل کی محدودیت کی آزمائش کرتے ہیں مدل کمپریشون، نازل کے اقتصادی اور ڈیٹا کمزوری کے مطابق۔ ہم دکھاتے ہیں کہ کلاسپیٹ کے کام جن کی نیاز ہے کہ عمومی لکسیکل سیمانٹیکوں کو پکڑنے کی ضرورت کرتی ہیں بہت ساده اور عمدہ موڈل کے ذریعہ سے جدا کر سکتے ہیں اور نسبتا چھوٹی سیمانٹیکوں کی نیاز ہے ہم بھی دکھاتے ہیں کہ بڑے پیش آموزش کی مدلکوں کی تفریق حقیقی زندگی سناریوں میں زیادہ اثر ہے جہاں لکڑی کی تعلیم محدود ہے۔', 'uz': "Yaqinda yillarda katta taʼminlovchi oldin modellar NLP vazifalari ko'pchilik vazifalarda saqlash holatini ko'rsatadi. Lekin, bu modellarni cheksiz manbalar bilan ishlashtirish, modellarning katta kompyutlar foydalanishi va xotira kerakligi sababchi muammolari. Ko'rib, ko'pchilik taʼminlovchi taʼminlovchi maʼlumot kerakligi ham dunyoning haqiqiqiy ishlab chiqarishga harakat qiladi. Name Bu qogʻozda biz BERT modelning chegaralarini o'zgartiraymiz model kompyuterning chegaralarini, cheksizlik va maʼlumot bajarligi darajada. Biz tashkilotlarni ko'rsatishimiz mumkin, umumiy leksikal semantika olishga kerak bo'lgan vazifalar muvaffaqiyatli sodda va effektiv modellar bilan ajratish mumkin va juda kichkina taʼminlovchi maʼlumot kerak. Шунингдек, биз ҳам кўрсатамиз, бундан аввал-trained modellarni ajratish haliy hayoti scenarida ko'proq ishlaydi. Bu yerda qo'shimcha taʼminlovchilar mavjud.", 'vi': 'Trong những năm gần đây, những mô hình được huấn luyện lớn đã chứng minh hiệu quả hiện đại nhất trong nhiều nhiệm vụ Njala. Tuy nhiên, việc áp dụng những mô hình này trên thiết bị có nguồn lực hạn chế là một thử thách nhờ vào nhu cầu tính to án và trí nhớ lớn của các mô-đun. Hơn nữa, sự cần thiết một lượng đáng kể dữ liệu huấn luyện cũng cản trở các viễn cảnh triển khai trên thế giới thực. Mô hình chưng cất đã cho thấy kết quả hứa hẹn để giảm cỡ mẫu, tải tính và hiệu quả dữ liệu. Trong tờ giấy này, chúng tôi kiểm tra các ranh giới của chưng cất kiểu BERT về khả năng nén mô hình, hiệu quả nhận biết và sự khan hiếm dữ liệu. Chúng tôi cho thấy các nhiệm vụ phân loại yêu cầu bắt giữ các mẫu văn học chung có thể được chưng cất thành công bằng các mô hình đơn giản và hiệu quả và đòi hỏi một lượng nhỏ các dữ liệu đào tạo được đánh dấu. Chúng tôi cũng cho thấy việc chưng cất các mô hình lớn được huấn luyện trước hiệu quả hơn trong các trường hợp sống thực tế, nơi có ít lượng được đánh dấu.', 'nl': "De afgelopen jaren hebben grote voorgetrainde modellen state-of-the-art prestaties aangetoond bij veel NLP-taken. De implementatie van deze modellen op apparaten met beperkte resources is echter een uitdaging vanwege het grote rekenverbruik en de geheugenvereisten van de modellen. Bovendien belemmert de behoefte aan een aanzienlijke hoeveelheid gelabelde trainingsgegevens ook reale implementatiescenario's. Modeldistillatie heeft veelbelovende resultaten laten zien voor het verminderen van modelgrootte, rekenbelasting en data-efficiëntie. In dit artikel testen we de grenzen van BERT modeldillatie in termen van modelcompressie, inferentieefficiëntie en dataschaarste. We laten zien dat classificatietaken die het vastleggen van algemene lexicale semantiek vereisen succesvol kunnen worden gedistilleerd door zeer eenvoudige en efficiënte modellen en relatief weinig gelabelde trainingsgegevens vereisen. We laten ook zien dat de distillatie van grote voorgetrainde modellen effectiever is in real-life scenario's waar beperkte hoeveelheden gelabelde training beschikbaar zijn.", 'hr': 'U posljednjih godina, veliki predobučeni modeli pokazali su stanje umjetnosti u mnogim zadatkima NLP-a. Međutim, raspoređenje tih modela na uređaje s ograničenim resursima izazovno je zbog velike potrebe za računalnom potrošnju i sjećanjem modela. Osim toga, potreba za značajnom količinom označenih podataka o obuci također sprječava scenarije razvoja stvarnog svijeta. Modelna destilacija pokazala je obećavajuće rezultate za smanjenje veličine modela, računalnog opterećenja i učinkovitosti podataka. U ovom papiru testiramo granice destilacije modela BERT u smislu kompresije modela, učinkovitosti infekcije i nedostatka podataka. Pokazujemo da klasifikacijski zadatak koji zahtijevaju uhvaćenje općeg leksičkog semantika uspješno može biti destilirani vrlo jednostavnim i učinkovitim modelima i zahtijevaju relativno mala količina označenih podataka o obuci. Također pokazujemo da je destilacija velikih predobučenih modela učinkovitija u scenarijima stvarnog života gdje su dostupne ograničene količine označene obuke.', 'bg': 'През последните години големите предварително обучени модели демонстрират най-съвременни резултати в много от задачите на НЛП. Въпреки това, внедряването на тези модели върху устройства с ограничени ресурси е предизвикателство поради големите изисквания за изчислителна консумация и памет на моделите. Освен това необходимостта от значително количество обозначени данни за обучението също възпрепятства сценариите за реалистично разгръщане. Дестилацията на модела показва обещаващи резултати за намаляване на размера на модела, изчислителния товар и ефективността на данните. В настоящата статия тестваме границите на дестилацията на модела по отношение на компресията на модела, ефективността на заключенията и недостига на данни. Показваме, че класификационните задачи, които изискват улавяне на обща лексикална семантика, могат успешно да бъдат дестилирани чрез много прости и ефективни модели и изискват сравнително малко количество етикетирани данни за обучение. Показваме също, че дестилацията на големи предварително обучени модели е по-ефективна в реални сценарии, където има ограничени количества етикетирано обучение.', 'de': 'In den letzten Jahren haben große vortrainierte Modelle in vielen NLP-Aufgaben den neuesten Stand der Technik gezeigt. Die Bereitstellung dieser Modelle auf Geräten mit begrenzten Ressourcen ist jedoch aufgrund des hohen Rechenverbrauchs und des hohen Speicherbedarfs schwierig. Darüber hinaus behindert der Bedarf an einer beträchtlichen Menge an gekennzeichneten Trainingsdaten auch reale Einsatzszenarien. Die Modelldestillation hat vielversprechende Ergebnisse zur Reduzierung von Modellgröße, Rechenlast und Dateneffizienz gezeigt. In diesem Beitrag untersuchen wir die Grenzen der BERT-Modelldestillation in Bezug auf Modellkompression, Inferenzeffizienz und Datenknappheit. Wir zeigen, dass Klassifizierungsaufgaben, die die Erfassung allgemeiner lexikalischer Semantik erfordern, durch sehr einfache und effiziente Modelle erfolgreich destilliert werden können und relativ geringe Mengen an markierten Trainingsdaten erfordern. Wir zeigen auch, dass die Destillation großer vortrainierter Modelle in realen Szenarien effektiver ist, in denen begrenzte Mengen an beschrifteten Trainings verfügbar sind.', 'da': 'I de seneste år har store præuddannede modeller vist state-of-the-art performance i mange af NLP-opgaver. Implementeringen af disse modeller på enheder med begrænsede ressourcer er dog udfordrende på grund af modellernes store beregningsforbrug og hukommelseskrav. Desuden hindrer behovet for en betydelig mængde mærkede uddannelsesdata også implementeringsscenarier i den virkelige verden. Modellestillation har vist lovende resultater for at reducere modelstørrelse, beregningsbelastning og dataeffektivitet. I denne artikel tester vi grænserne for BERT modeldestillation med hensyn til modelkomprimering, inferenceeffektivitet og dataknaphed. Vi viser, at klassifikationsopgaver, der kræver indsamling af generel leksikalsk semantik, med succes kan destilleres ved hjælp af meget enkle og effektive modeller og kræver relativt lille mængde mærkede træningsdata. Vi viser også, at destillationen af store prætrænede modeller er mere effektiv i virkelige scenarier, hvor begrænsede mængder af mærket træning er tilgængelig.', 'id': 'Pada tahun-tahun terakhir, model besar yang terlatih telah menunjukkan prestasi terbaik dalam banyak tugas NLP. Namun, penerbangan model ini pada perangkat dengan sumber daya terbatas adalah tantangan karena konsumsi komputasi besar model dan keperluan ingatan. Selain itu, kebutuhan untuk jumlah besar data pelatihan yang ditabel juga menghalangi skenario pengambilan dunia nyata. Destilasi model telah menunjukkan hasil yang berjanji untuk mengurangi ukuran model, beban komputasi dan efisiensi data. Dalam kertas ini kami menguji batas dari distillasi model BERT dalam terma kompresi model, efisiensi inferensi dan kekurangan data. Kami menunjukkan bahwa tugas klasifikasi yang memerlukan penangkapan semantik lexik umum dapat berhasil didistil oleh model yang sangat sederhana dan efisien dan memerlukan jumlah relatif sedikit data pelatihan yang ditabel. Kami juga menunjukkan bahwa destilasi dari model besar yang sudah dilatih lebih efektif dalam skenario kehidupan nyata di mana jumlah terbatas pelatihan yang ditabel tersedia.', 'ko': '최근 몇 년 동안 대형 예비 훈련 모델은 많은 NLP 임무에서 가장 선진적인 성능을 나타냈다.그러나 이러한 모델은 대량의 계산량과 메모리를 필요로 하기 때문에 자원이 제한된 설비에 이러한 모델을 배치하는 것은 도전이다.이 밖에 대량의 표기 훈련 데이터에 대한 수요도 실제 배치 장면을 방해한다.모델 증류는 모델의 사이즈, 계산량, 데이터 효율을 줄이는 데 유망한 결과를 보였다.본고에서 우리는 모델 압축, 추리 효율과 데이터 희소성의 측면에서 버트 모델 증류의 경계를 검증했다.우리는 일반 어휘의 의미를 포획해야 하는 분류 임무는 매우 간단하고 효과적인 모델을 통해 성공적으로 추출할 수 있으며 상대적으로 적은 표기 훈련 데이터가 필요하다는 것을 보여준다.우리는 또한 유한한 수량의 표기훈련이 사용할 수 있는 현실 생활 장면에서 대형 예비훈련 모델을 추출하는 것이 더욱 효과적이라고 밝혔다.', 'fa': 'در سال های اخیر، مدل های پیش آموزش بزرگ در بسیاری از کارهای NLP نشان داده اند. با این حال، استفاده از این مدلها بر دستگاه\u200cهایی با منابع محدود، به دلیل مصرف و حافظه\u200cهای بزرگ کامپیوتری و حافظه\u200cهای مدلها مشکل دارد. علاوه بر این، نیاز به مقدار زیادی از اطلاعات آموزش مشخص شده، همچنین سناریو\u200cهای فعالیت جهان واقعی را مانع می\u200cکند. تفریح مدل نتایج قول\u200cدهنده را برای کاهش اندازه مدل، بار محاسبه و موثیت داده نشون داده است. در این کاغذ ما مرزهای متفاوت مدل BERT را به عنوان تزریق مدل، فعالیت آلودگی و کمبود داده\u200cها آزمایش می\u200cکنیم. ما نشان می دهیم که وظیفه\u200cهای مختصری که نیاز دارند دستگیری سیمانتی\u200cهای ژنرال زبانی\u200cشناسی را با موفقیت با مدل\u200cهای بسیار ساده و موثرت جدا کنند و نیاز به مقدار نسبتا اندکی از داده\u200cهای آموزش\u200cشناسی مشخص شده است. ما همچنین نشان می دهیم که تفریح مدل های پیش آموزشی بزرگ در سناریو های زندگی واقعی بیشتر موثر است که مقدار آموزشی محدودیت مورد علامت قرار دارد.', 'af': "In onlangse jaar het groot voorafgeleerde modele in baie NLP-opdragte gewys. Maar die verwydering van hierdie modele op toestelle met beperkte hulpbronne is aangelyk vanweë die groot rekenaarske verbruik en geheue benodigte van die modele. Ook, die behoefte vir 'n aansienlike hoeveelheid etiketeerde onderwerp data hinder ook reël-wêreld verwydering scenarios. Name In hierdie papier toets ons die grense van BERT model destilasie in terms of model compression, inference efficiency and data scarcity. Ons wys dat klasifikasie opdragte wat die opvang van algemene leksiese semantieke nodig kan suksesvol deur baie eenvoudige en effektief modele verskaf word en relativief klein hoeveelheid etiketeerde opvoerde data nodig word. Ons wys ook dat die destilasie van groot voorafgeleerde modele is meer effektief in reël lewe scenarios waar beperk hoeveelheid etiketeerde onderwerp beskikbaar is.", 'sq': 'Në vitet e fundit, modelet e mëdha të paratrajnuara kanë demonstruar performancë më të larta në shumë nga detyrat e NLP. Megjithatë, vendosja e këtyre modeleve në pajisje me burime të kufizuara është sfiduese për shkak të kërkesave të mëdha të konsumit kompjuterik të modeleve dhe kujtesës. Përveç kësaj, nevoja për një sasi të konsiderueshme të dhënash të trajnimit të etiketuar pengon gjithashtu skenarët e vendosjes në botën reale. Model distillation has shown promising results for reducing model size, computational load and data efficiency.  Në këtë letër ne testojmë kufijtë e distillacionit të modelit BERT në terma të komprimit të modelit, efektshmërisë së përfundimit dhe mungesës së të dhënave. Ne tregojmë se detyrat e klasifikimit që kërkojnë kapjen e semantikës së përgjithshme lexike mund të distilohen me sukses nga modele shumë të thjeshta dhe efikase dhe kërkojnë sasi relativisht të vogël të dhënash të trajnimit të etiketuar. Ne gjithashtu tregojmë se distilimi i modeleve të mëdha të paratrajnuar është më efektiv në skenarët e jetës reale ku janë në dispozicion sasi të kufizuara të trajnimit të etiketuar.', 'sw': 'Katika miaka ya hivi karibuni, mifano makubwa ya mafunzo ya zamani yameonyesha hali ya mambo ya sanaa katika kazi nyingi za NLP. Hata hivyo, kuandaliwa kwa mifano hii kwenye vifaa vikubwa vya vyanzo vikubwa vinavyochanganya kutokana na matumizi makubwa ya kompyuta na mahitaji ya kumbukumbu. Zaidi ya hayo, haja ya takwimu kubwa za mafunzo yanayoonyesha pia inazuia matukio ya usafiri wa kimataifa. Utofauti wa Modeli umeonyesha matokeo yanayoweza kupunguza ukubwa wa mifano, uzao wa kompyuta na ufanisi wa data. Katika gazeti hili tunajaribu mipaka ya utofauti wa modeli ya BERT kwa mukhtadha wa kompyuta, ufanisi wa uchunguzi na ukosoaji wa data. Tunaonyesha kwamba kazi za usambazaji zinazohitaji kuchukuliwa kwa ajili ya mifumo ya kijamii ya lexico inaweza kufanikiwa na mifano rahisi na yenye ufanisi na inahitaji kiasi kidogo cha taarifa za mafunzo. Tunaonyesha pia kuwa utofauti wa mifano makubwa ya mafunzo ya zamani ni ufanisi zaidi katika mazingira ya maisha halisi ambapo idadi kubwa ya mafunzo yanapatikana.', 'az': 'Son illərdə böyük öyrənmiş modellər NLP işlərinin çoxluğunda mövcuddur. Lakin bu modellərin müəyyən edilmiş kaynaqlar olan avadanlıqlara istifadə edilməsi modellərin böyük kompjuterlərin istifadəsi və yaddaş şartları üzündən çətin olar. Daha sonra, etiketli təhsil məlumatlarının çox dəyişiklik ehtiyacı həmçinin həqiqət dünyanın yayılması senaryoylarına da mane edir. Model destilyası modellərin böyüklüyünü, hesablama yükünü və məlumatların faydalanılığını azaltmaq üçün vəd verici sonuçlarını göstərdi. Bu kağızda BERT modeli destilyasının sınırlarını modellərin sıkıştırması, zəiflik etkinlik və məlumatların zəifliyi ilə imtahana çəkirik. Biz göstəririk ki, genel leksik semantik alması lazımdır olan klasifikasiya işləri çox basit və etkili modellərlə müvəffəqiyyətlə ayırıla bilər və çox az etiket edilmiş təhsil məlumatlarını lazımdır. Biz də göstəririk ki, böyük əvvəlcə təhsil edilmiş modellərin destilyası həyat scenariolarında daha etkilidir ki, onlar müəyyən edilmiş təhsil sayıları faydalanır.', 'tr': 'Soňky ýyllarda, uly öň-bilim nusgalary NLP işiň köpüsiniň durumyny ukyp görkezildi. Ýöne bu modelleriň janlaşdyryly resurslar bilen janlaşdyrylmagy nusgala hasaplanýar. Munuň ýene-de, etiket etitlenýän bilim maglumatynyň uly daşyrlygyny hem dünýäde ýerleşmäge senaryony ýagdaýlaýar. Çaltylyk Bu kagyzda BERT nusgynyň çykyşlygyny örän sıkıştyrmak, hassyzlyk etkinlik we maglumatyň ýetmegini barlaýarys. Biz klasifikasiýa görevlerini umumy lexikalar semantikleriniň yakalamasyna gerek bolan şekilde örän basit we etkinlik nusgalaryň bilen aýdylşyrlyp biler we etiket edilen az sanlaryň bardygyny görkezip görkezip görkezip bileris. Biz hem kän öňünden öňünden öňünden eğitilen nusgalaryň tapawutlandyrylmagy hakyky ýaşaýyş senarylarynda täsirli däldigini görkez.', 'bs': 'U posljednjih godina, veliki predobučeni modeli su pokazali predstavu umjetnosti u mnogim zadacima NLP-a. Međutim, raspoređenje tih modela na uređajima sa ograničenim resursima je izazovno zbog velike potrošnje računalnih potrošnja i zahtjeva za pamćenje modela. Osim toga, potreba za značajnom količinom označenih podataka o obuci također sprječava scenarije razvoja stvarnog svijeta. Modelna destilacija pokazala je obećavajuće rezultate za smanjenje veličine modela, računalnog opterećenja i učinkovitosti podataka. U ovom papiru testiramo granice destilacije modela BERT u smislu kompresije modela, infekcije efikasnosti i nedostatka podataka. Pokazujemo da klasifikacijski zadatak koji zahtijevaju uhvaćenje općeg leksičkog semantika može uspješno destilati vrlo jednostavnim i efikasnim modelima i zahtijevati relativno mala količina označenih podataka o obuci. Također pokazujemo da je destilacija velikih predobučenih modela učinkovitija u scenarijima stvarnog života gdje su dostupne ograničene količine označene obuke.', 'hy': 'In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks.  Այնուամենայնիվ, այս մոդելների կիրառումը սահմանափակ ռեսուրսներով սարքերի վրա դժվար է, քանի որ մոդելների մեծ հաշվարկների սպառումը և հիշողության պահանջները: Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios.  Model distillation has shown promising results for reducing model size, computational load and data efficiency.  In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity.  We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data.  Մենք նաև ցույց ենք տալիս, որ մեծ նախապատրաստված մոդելների դիսլիլացիան ավելի արդյունավետ է իրական կյանքի սցենարներում, որտեղ հնարավոր է սահմանափակ քանակությամբ պիտակ վարժեցման:', 'am': 'ባለፈው ዓመታት፣ ብዙ የፊተኛ ተማሪ ሞዴላዎች የልዩ አርእስት ሥርዓት በNLP ስራቶች ውስጥ አሳልፈዋል፡፡ ምንም እንኳን፣ እነዚህን ሞዴላዎች በተቃራኒ ሀብት ላይ በመውሰድ እና የመስታሰቢያ ፈቃድ በመጠቀም የሞዴሎቹ ትልቅ ቁጥጥር እና ማስታወሻ ፈቃድ ነው፡፡ ደግሞም፣ የሚያስፈልገው ብዙ ተማሪ ዳታዎችን ደግሞ እውነተኛውን ዓለማዊ የስርዓት ስልጣንን የሚከለክል ነው፡፡ የሞዴል መለያየት የሞዴል መጠን፣ ቁጥጥር ሎዳ እና የዳታ ጥያቄን የሚያሳዝን ፍሬዎችን አሳየ፡፡ በዚህ ፕሮግራም የBERT model ዳርቻን የምንፈትነው እናደርጋለን፡፡ እናሳያቸዋለን የልስክሲካዊ ሰሜንቲካን ማግኘት የሚያስፈልገውን ሥርዓቶች በሙሉ ቀላል እና በጥያቄ ዓይነቶች ሊለይ ይችላል እናም የጽሑፍ ማህበረሰብ ብዛት ታናሽ ማህበረት ዳታ እንዲያስፈልጋል፡፡ We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.', 'et': 'Viimastel aastatel on suured eelkoolitud mudelid näidanud kaasaegset tulemuslikkust paljudes uue tööprogrammi ülesannetes. Siiski on nende mudelite kasutuselevõtt piiratud ressurssidega seadmetel keeruline tänu mudelite suurele arvutustarbimisele ja mälu vajadusele. Lisaks takistab vajadus märkimisväärse hulga märgistatud koolitusandmete järele ka reaalmaailma kasutuselevõtu stsenaariume. Mudeli destilleerimine on näidanud paljulubavaid tulemusi mudeli suuruse, arvutuskoormuse ja andmete efektiivsuse vähendamisel. Käesolevas töös testime BERT mudeli destilleerimise piire mudeli tihendamise, järelduste efektiivsuse ja andmete nappuse osas. Näitame, et klassifitseerimisülesandeid, mis nõuavad üldise leksikaalse semantika kogumist, saab edukalt destilleerida väga lihtsate ja tõhusate mudelitega ning nõuavad suhteliselt väikest hulka märgistatud koolitusandmeid. Samuti näitame, et suurte eelkoolitud mudelite destilleerimine on tõhusam reaalsetes stsenaariumides, kus on saadaval piiratud hulgas märgistatud koolitust.', 'bn': 'সাম্প্রতিক বছরগুলোতে বিশাল প্রশিক্ষণ পূর্ব প্রশিক্ষিত মডেল অনেক এনএলপির কাজে অবস্থান-শিল্পের প্রদর্শন করেছে। তবে মডেলের বিশাল গণিত ব্যবহার এবং স্মৃতির প্রয়োজনের কারণে এই মডেলগুলো ডিভাইসগুলোর উপর নির্মাণ করা সীমিত সম্পদের উপর চ্যালেঞ্জ করছ এছাড়াও বিশ্বের প্রশিক্ষণের প্রয়োজনীয় পরিমাণ প্রশিক্ষণের তথ্য বাধা দিয়েছে। মডেলের আকার, গণনাত্রিক লোড এবং তথ্যের কার্যক্রম কমানোর জন্য প্রতিশ্রুত ফলাফল প্রদর্শন করা হয়েছে। এই কাগজটিতে আমরা বের্টি মডেলের সীমানা পরীক্ষা করি মডেলের প্রেম, সংক্রান্ত কার্যক্রম এবং তথ্যের ক্ষতির মাধ্যমে। আমরা দেখাচ্ছি যে সাধারণ লেক্সিক্যাল সেম্যান্টিক্সের গ্রেফতারের কাজের প্রয়োজন খুব সাধারণ এবং কার্যকর মডেল দ্বারা বিচ্ছিন্ন করা যাবে এবং ত আমরা একই সাথে দেখাচ্ছি যে পূর্ব প্রশিক্ষিত মডেলের বিচ্ছিন্ন করা বাস্তব জীবনের দৃশ্যে আরো কার্যকর, যেখানে লেবেল প্রশিক্ষণের স', 'fi': 'Viime vuosina suuret esikoulutetut mallit ovat osoittaneet huipputason suorituskykyä monissa NLP-tehtävissä. Näiden mallien käyttöönotto laitteissa, joilla on rajalliset resurssit, on kuitenkin haastavaa mallien suuren laskennallisen kulutuksen ja muistitarpeen vuoksi. Lisäksi merkittävä määrä merkittyjä koulutustietoja vaikeuttaa myös reaalimaailman käyttöönottosskenaarioita. Mallitislaus on osoittanut lupaavia tuloksia mallin koon, laskennallisen kuormituksen ja datatehokkuuden vähentämisessä. Tässä työssä testaamme BERT-mallitislauksen rajoja mallin puristuksen, päättelytehokkuuden ja datan niukkuuden suhteen. Osoitamme, että yleisen leksikaalisen semantiikan talteenottoa vaativat luokittelutehtävät voidaan onnistuneesti tislata hyvin yksinkertaisilla ja tehokkailla malleilla ja vaativat suhteellisen pienen määrän merkittyä koulutustietoa. Osoitamme myös, että suurten esikoulutettujen mallien tislaus on tehokkaampaa tosielämän skenaarioissa, joissa on saatavilla rajoitettu määrä merkittyä koulutusta.', 'ca': "En els últims anys, grans models pré-entrenats han demostrat el desempeny més avançat en moltes de les tasques del NLP. No obstant això, la implantació d'aquests models en dispositius amb recursos limitats és difícil a causa dels grans requisits de consum computacional i memòria dels models. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios.  La destilació model ha demostrat resultats prometedors per reduir la mida del model, la càrrega computacional i l'eficiència de les dades. En aquest paper testem els límits de la distillació del model BERT en termes de compressió del model, eficiència de inferència i escassetat de dades. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data.  També demostram que la destilació de grans models pré-entrenats és més efectiva en escenaris de vida real on hi ha quantitats limitades d'entrenament etiquetat.", 'cs': 'V posledních letech prokázaly velké předškolené modely nejmodernější výkon v mnoha úkolech NLP. Nasazení těchto modelů na zařízeních s omezenými zdroji je však náročné vzhledem k velké výpočetní spotřebě a požadavkům na paměť modelů. Kromě toho potřeba značného množství označených výcvikových dat brání také scénářům nasazení v reálném světě. Destilace modelů přinesla slibné výsledky pro snížení velikosti modelu, výpočetního zatížení a účinnosti dat. V tomto článku testujeme hranice destilace modelu BERT z hlediska komprese modelu, efektivity inference a nedostatku dat. Ukazujeme, že klasifikační úlohy, které vyžadují zachycení obecné lexikální sémantiky, mohou být úspěšně destilovány velmi jednoduchými a efektivními modely a vyžadují relativně malé množství označených tréninkových dat. Také ukazujeme, že destilace velkých předtrénovaných modelů je efektivnější v reálných scénářích, kde je k dispozici omezené množství školení s označením.', 'ha': "A cikin shekara na farko, masu motsi masu girma da aka sanar ta suka nuna halin-the-art cikin masu yawa na aikin NLP. A lokacin da, deployment of these misãlai kan kayan aiki da yana da abinci wanda aka ƙudura, yana tsõratar, saboda matuƙar mai girma na amfani da kwamfyutan hukuru. Kayya, umakinci na da gwargwadon mutane na ƙidãya da aka rubũta shi, yana hani da sauri masu cikin taska da halin dũniya. Tsarin Model ya nuna matsala masu yi wa'adi da za'a ƙara girma wa motel, loda lissafi da fassarar data. Ga wannan takardan, Munã jarraba tsakanin misalin BERT, cikin muhimman zane-zane da zane-zane-zane cikin zane-zane-zane-zane-zane-zane-zane. Tuna nũna wa aikin classified wanda ke bukatar ta kãma wa mai jama'a na leksisi, za'a buƙata shi da misãlai masu sauƙi da mai amfani, kuma ana ƙayyade data masu ƙaranci da aka rubũta. Kayya, Munã nũna cewa rarrabin masu motsi masu yawa da aka yi wa zaman-tunni, yana mafi mafiya amfani a cikin masu halin rãyuwa, inda yana iya sãmun ma'anar mafaka da aka rubuta.", 'he': 'בשנים האחרונות, דוגמנים גדולים מאומנים מראש הראו ביצועים חדשים ברוב משימות NLP. עם זאת, השימוש של הדוגמנים האלה על מכשירים עם משאבים מוגבלים הוא מאתגר בגלל הצרכה החישובית הגדולה של הדוגמנים והזכרון. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios.  הדיסטילה של מודל הראה תוצאות מבטיחות להפחית גודל מודל, עומס חישובי ויעילות נתונים. בנייר הזה אנו בודקים את הגבולות של הדיסטל של מודל BERT במונחים של דחיסה מודל, יעילות ההנחה והחסר נתונים. אנו מראים כי משימות מסווג שדורשות את תפיסת הסמנטיקה הלקסית הכללית יכולות להוציא בהצלחה על ידי דוגמנים מאוד פשוטים ויכולים ולדרוש כמות קטנה יחסית של נתוני אימונים מסוימים. אנחנו גם מראים שדגימות גדולות מאומנות מראש הן יותר יעילות בתרחישים של חיים אמיתיים שבו יש כמויות מוגבלות של אימונים תווים זמינות.', 'jv': 'Dino ngono akeh sing dumadhi, akeh model sing sampeyan luwih dumadhi iki, iso ngomongke akeh state-of-the-arts banjure akeh operasi NLP Ngayon segala, deputasi model iki dadi ono alat nang unarane perusahaan bakal nggawe nguasai nggawe modèl kuwi kaluwargane kapan pangan ning sampek Nambah, ngilanggar-sistem sing dianggap akeh luwih operasi sing paling berarti dadi nggawe nyimpen kuwi cah sistem sing gawe ngupakan negoro iki luwih apik. sample Nang pebuk iki, kita ujian luwih dumateng model BERT sampek dulasar anyar tentang Compresion, Jagger barang nggambar obang donde Awak dhéwé ngerasakno alamat sakjane sampeyan akeh operasi dipunangé sampeyan luwih Awak dhéwé éntuk ngerasakno kanggo ngerasakno akeh model sing wis ana luwih dumadhi, nik awak dhéwé kuwi scénares layang-layang, wong dhéwé kuwi kesempatan kanggo ngerasakno luwih apik dhéwé.', 'sk': 'V zadnjih letih so veliki predhodno usposobljeni modeli pokazali najsodobnejšo uspešnost pri številnih nalogah NLP. Vendar pa je uvedba teh modelov na napravah z omejenimi viri zahtevna zaradi velike računalniške porabe in zahteve po pomnilniku modelov. Poleg tega potreba po znatni količini označenih podatkov o usposabljanju ovira tudi scenarije dejanske uvedbe. Modelska destilacija je pokazala obetavne rezultate za zmanjšanje velikosti modela, računalniške obremenitve in učinkovitosti podatkov. V prispevku testiramo meje destilacije modela BERT v smislu kompresije modela, sklepne učinkovitosti in pomanjkanja podatkov. Pokazali smo, da je klasifikacijske naloge, ki zahtevajo zajem splošne leksikalne semantike, mogoče uspešno destilirati z zelo preprostimi in učinkovitimi modeli in zahtevajo relativno majhno količino označenih podatkov o usposabljanju. Pokazali smo tudi, da je destilacija velikih predhodno usposobljenih modelov učinkovitejša v realnih scenarijih, kjer je na voljo omejena količina označenega usposabljanja.', 'bo': "དབུས་གཞུང་གི་ལོ་དག་ནང་དུ་མི་སྔོན་གྲངས་སྔོན་བསྒྲིག་པའི་མིག་ཆས་པ་ཚོས་NLP་ལས་ཀ་མང་པོ་ཞིག་སྟོན་ཡོད། However, the deployment of these models on devices with limited resources is challenging due to the models' large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. མ་དབྱིབས་བཀྲམ་སྟོན་པར་མཐུན་རྣམ་གྲངས་ཀྱི་ཆེ་ཆུང་ཚད་དམའ་རུ་གཏོང་བའི་འབྲས་བ་མངོན་གསལ་བརྗོད་བྱས། འོག་གི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་BERT མིག་ཆས་བཀོད་པའི་ཚད་གཞི་གསལ་བཤད་ཀྱི་ཆ་བརྟན་ཞིབ་བྱེད་ཀྱི་ཡོད། We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available."}
{'en': 'Efficient Estimation of Influence of a Training Instance', 'ar': 'التقدير الفعال لتأثير مؤسسة التدريب', 'es': 'Estimación eficiente de la influencia de una instancia de formación', 'pt': 'Estimativa eficiente da influência de uma instância de treinamento', 'fr': "Estimation efficace de l'influence d'une instance de formation", 'ja': 'トレーニングインスタンスの影響の効率的な推定', 'hi': 'एक प्रशिक्षण उदाहरण के प्रभाव का कुशल अनुमान', 'ru': 'Эффективная оценка влияния учебного экземпляра', 'zh': '有效', 'ga': 'Meastachán Éifeachtach ar an Tionchar a bhíonn ag Cás Oiliúna', 'el': 'Αποτελεσματική εκτίμηση της επίδρασης ενός εκπαιδευτικού φορέα', 'ka': 'შესაბამისი ინსტენსტის შესაძლებლობას ეფექტიური განსაზღვრება', 'hu': 'Egy képzési példány hatásos becslése', 'it': "Stima efficiente dell'influenza di un'istanza di formazione", 'kk': 'Оқыту инстанциясының күресінің ең жақсы оқу', 'mk': 'Ефикасна проценка на влијанието на инстанцијата за обука', 'lt': 'Veiksmingas mokymo įstaigos poveikio vertinimas', 'mt': 'Stima Effiċjenti tal-Influwenza ta’ Instanza ta’ Taħriġ', 'ms': 'Penghargaan Efisien Influence of a Training Instance', 'ml': 'പഠിപ്പിക്കുന്ന ഇന്\u200dസ്റ്റെന്\u200dസിന്റെ ഇന്\u200dഫ്രൂയെന്\u200dസിന്റെ പ്രായോഗ്യം എണ്ണിമേഷന്\u200d', 'mn': 'Сургуулийн сургуулийн нөлөөний үр дүнтэй тооцоолол', 'pl': 'Skuteczna ocena wpływu instancji szkoleniowej', 'no': 'Name', 'ro': 'Estimarea eficientă a influenței unei instanțe de formare', 'sr': 'Efikasna procjena utjecaja treninga', 'si': 'Name', 'so': 'Effective Estimation of Influence of a Training Institute', 'sv': 'Effektiv uppskattning av påverkan av en utbildningsinstans', 'ta': 'பயிற்சி நிறுவனத்தின் தெளிவுப்பெற்ற கணக்கீடு', 'ur': 'ٹرینگ انٹنسس کے مالک کا مطابق حساب', 'uz': 'Name', 'vi': 'Giá trị hiệu quả về sự ảnh hưởng của một cơ quan đào tạo', 'bg': 'Ефективна оценка на влиянието на обучителна инстанция', 'da': 'Effektiv vurdering af indflydelsen af en uddannelsesinstans', 'hr': 'Efikasna procjena utjecaja institucije obuke', 'nl': 'Efficiënte schatting van de invloed van een opleidingsinstituut', 'de': 'Effiziente Einschätzung des Einflusses einer Ausbildungsinstanz', 'id': 'Penghargaan Efisien Influensi Instansi Pelatihan', 'ko': '훈련 실례의 영향을 효과적으로 예측하다', 'fa': 'تأثیر فعالی اثر یک ابتدایی آموزش', 'sw': 'Kuimarishwa kwa ufanisi wa Usafiti wa Mafunzo', 'tr': 'Ewezam Instanzynyň etkinligini ýeterlik Taýýarlama', 'af': "Effektiewe Oefening van 'n Oefening Instans", 'sq': 'Vlerësim i efektshëm i ndikimit të një Instance trainimit', 'am': 'ምርጫዎች', 'az': 'Eğitim Instancının Etkinliği', 'hy': 'Արժեքական դասընթացի ազդեցության արդյունավետ գնահատումը', 'bn': 'প্রশিক্ষণ ইনস্টেন্সের প্রভাব ফ্লুয়েন্সের কার্যকর গণনা', 'bs': 'Efikasna procjena utjecaja institucije obuke', 'ca': "Estimat eficient de l'influència d'una instancia de formació", 'cs': 'Efektivní odhad vlivu vzdělávací instance', 'et': 'Koolitusasutuse mõju tõhus hindamine', 'fi': 'Koulutuksen vaikutustenarviointi', 'jv': 'Suasai tanggal kapan kelas barang tengahan', 'ha': 'KCharselect unicode block name', 'sk': 'Učinkovita ocena vpliva usposabljanja', 'he': 'Efficient Estimation of Influence of a Training Instance', 'bo': 'སློབ་ཐག་འཛིན་གྱི་གཟུགས་རིས་ལ།'}
{'en': 'Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model’s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.', 'ar': 'يؤدي فهم تأثير مثيل التدريب على نموذج الشبكة العصبية إلى تحسين القابلية للتفسير. ومع ذلك ، من الصعب وغير الفعال تقييم التأثير ، مما يوضح كيف يمكن تغيير تنبؤ النموذج إذا لم يتم استخدام مثيل التدريب. في هذه الورقة ، نقترح طريقة فعالة لتقدير التأثير. طريقتنا مستوحاة من التسرب ، والذي لا يخفي شبكة فرعية ويمنع الشبكة الفرعية من تعلم كل حالة تدريب. من خلال التبديل بين أقنعة التسرب ، يمكننا استخدام الشبكات الفرعية التي تعلمت أو لم تتعلم كل حالة تدريب وتقدير تأثيرها. من خلال التجارب مع BERT و VGGNet على مجموعات بيانات التصنيف ، نوضح أن الطريقة المقترحة يمكن أن تلتقط تأثيرات التدريب ، وتعزز قابلية تفسير تنبؤات الخطأ ، وتنظف مجموعة بيانات التدريب لتحسين التعميم.', 'fr': "Comprendre l'influence d'une instance d'entraînement sur un modèle de réseau neuronal permet d'améliorer l'interprétabilité. Cependant, il est difficile et inefficace d'évaluer l'influence, ce qui montre comment la prédiction d'un modèle serait modifiée si une instance de formation n'était pas utilisée. Dans cet article, nous proposons une méthode efficace pour estimer l'influence. Notre méthode s'inspire du décrochage, qui masque à zéro un sous-réseau et empêche le sous-réseau d'apprendre chaque instance d'entraînement. En basculant entre les masques d'abandon, nous pouvons utiliser des sous-réseaux qui ont appris ou n'ont pas appris chaque instance d'entraînement et estimer leur influence. Grâce à des expériences avec BERT et VGGnet sur des ensembles de données de classification, nous démontrons que la méthode proposée peut saisir les influences de l'apprentissage, améliorer l'interprétabilité des prédictions d'erreurs et nettoyer le jeu de données d'apprentissage pour améliorer la généralisation.", 'pt': 'Compreender a influência de uma instância de treinamento em um modelo de rede neural leva a melhorar a interpretabilidade. No entanto, é difícil e ineficiente avaliar a influência, o que mostra como a previsão de um modelo seria alterada se uma instância de treinamento não fosse usada. Neste artigo, propomos um método eficiente para estimar a influência. Nosso método é inspirado no dropout, que mascara uma sub-rede e impede que a sub-rede aprenda cada instância de treinamento. Ao alternar entre as máscaras de dropout, podemos usar sub-redes que aprenderam ou não aprenderam cada instância de treinamento e estimar sua influência. Por meio de experimentos com BERT e VGGNet em conjuntos de dados de classificação, demonstramos que o método proposto pode capturar influências de treinamento, melhorar a interpretabilidade de previsões de erros e limpar o conjunto de dados de treinamento para melhorar a generalização.', 'es': 'Comprender la influencia de una instancia de entrenamiento en un modelo de red neuronal mejora la interpretabilidad. Sin embargo, es difícil e ineficiente evaluar la influencia, lo que demuestra cómo cambiaría la predicción de un modelo si no se utilizara una instancia de entrenamiento. En este artículo, proponemos un método eficiente para estimar la influencia. Nuestro método se inspira en la deserción, que enmascara cero una subred e impide que la subred aprenda cada instancia de entrenamiento. Al cambiar entre máscaras de abandono, podemos usar subredes que aprendieron o no aprendieron cada instancia de capacitación y estimar su influencia. A través de experimentos con BERT y VGGNet en conjuntos de datos de clasificación, demostramos que el método propuesto puede captar las influencias del entrenamiento, mejorar la interpretabilidad de las predicciones de errores y limpiar el conjunto de datos de entrenamiento para mejorar la generalización.', 'ja': 'トレーニングインスタンスがニューラルネットワークモデルに与える影響を理解することは、解釈可能性の向上につながります。ただし、影響を評価するのは難しく、非効率的です。これは、トレーニングインスタンスが使用されていない場合、モデルの予測がどのように変更されるかを示しています。本稿では，その影響を推定するための効率的な手法を提案する．私たちの方法は、ドロップアウトからインスピレーションを得ています。ドロップアウトは、サブネットワークをゼロマスクし、サブネットワークが各トレーニングインスタンスを学習するのを防ぎます。ドロップアウトマスクを切り替えることで、各トレーニングインスタンスを学習した、または学習しなかったサブネットワークを使用して、その影響を推定することができます。BERTとVGGNetによる分類データセットの実験を通じて、提案された方法は、トレーニングの影響を捕捉し、エラー予測の解釈可能性を高め、トレーニングデータセットをクレンジングして、一般化を改善することができることを実証します。', 'zh': '知训实神经网络可以崇可解释性。 然评估之难而低效,显而不练,其占将何以易之。 在本文中,我们发出一种有效的方法。 吾道启于 dropout ,dropout 零掩码于子网,而止子网学。 切换于 dropout 掩码之间,可以学未习子网,而度其影响。 以BERTVGGNet分类数集上之实验,证其法以收练之,增差占之可解释性,而清练数集以以崇一般化。', 'hi': 'एक तंत्रिका नेटवर्क मॉडल पर एक प्रशिक्षण उदाहरण के प्रभाव को समझने से व्याख्या में सुधार होता है। हालांकि, प्रभाव का मूल्यांकन करना मुश्किल और अक्षम है, जो दिखाता है कि यदि प्रशिक्षण उदाहरण का उपयोग नहीं किया गया था तो मॉडल की भविष्यवाणी को कैसे बदला जाएगा। इस पेपर में, हम प्रभाव का अनुमान लगाने के लिए एक कुशल विधि का प्रस्ताव करते हैं। हमारी विधि ड्रॉपआउट से प्रेरित है, जो शून्य-मास्क एक उप-नेटवर्क और उप-नेटवर्क को प्रत्येक प्रशिक्षण उदाहरण को सीखने से रोकती है। ड्रॉपआउट मास्क के बीच स्विच करके, हम उप-नेटवर्क का उपयोग कर सकते हैं जो प्रत्येक प्रशिक्षण उदाहरण को सीखते हैं या नहीं सीखते हैं और इसके प्रभाव का अनुमान लगाते हैं। वर्गीकरण डेटासेट पर BERT और VGGNet के साथ प्रयोगों के माध्यम से, हम प्रदर्शित करते हैं कि प्रस्तावित विधि प्रशिक्षण प्रभावों को कैप्चर कर सकती है, त्रुटि भविष्यवाणियों की व्याख्याक्षमता को बढ़ा सकती है, और सामान्यीकरण में सुधार के लिए प्रशिक्षण डेटासेट को शुद्ध कर सकती है।', 'ru': 'Понимание влияния обучающего экземпляра на модель нейронной сети приводит к улучшению интерпретируемости. Однако оценить влияние трудно и неэффективно, что показывает, как изменился бы прогноз модели, если бы учебный экземпляр не был использован. В данной работе мы предлагаем эффективный метод оценки влияния. Наш метод вдохновлен отсевом, который маскирует нулевую подсеть и не позволяет ей изучать каждый учебный экземпляр. Переключаясь между масками отсева, мы можем использовать подсети, которые изучили или не изучили каждый учебный пример и оценить его влияние. Экспериментируя с BERT и VGGNet по классификационным наборам данных, мы демонстрируем, что предлагаемый метод может захватывать обучающие влияния, повышать интерпретируемость предсказаний ошибок и очищать обучающий набор данных для улучшения обобщения.', 'ga': "Is é an toradh a bhíonn ar thuiscint ar an tionchar a bhíonn ag cás oiliúna ar shamhail líonra néarúil ná go bhfeabhsaítear inléirmhíniú. Mar sin féin, tá sé deacair agus neamhéifeachtach an tionchar a mheas, rud a thaispeánann conas a d’athrófaí tuar múnla mura n-úsáidfí sampla oiliúna. Sa pháipéar seo, molaimid modh éifeachtach chun an tionchar a mheas. Tá ár modh spreagtha ag titim amach, a fholóidh náid fo-líonra agus a chuireann cosc ar an bhfo-líonra ó gach cás oiliúna a fhoghlaim. Trí athrú idir maisc fágála, is féidir linn fo-líonraí a d'fhoghlaim nó nár fhoghlaim gach cás oiliúna a úsáid agus a dtionchar a mheas. Trí thurgnaimh le BERT agus VGGNet ar thacair sonraí aicmithe, léirímid gur féidir leis an modh atá beartaithe tionchair oiliúna a ghabháil, inmhínithe earráidí a thuar, agus an tacar sonraí oiliúna a ghlanadh chun ginearálú a fheabhsú.", 'hu': 'A tréningpéldány neurális hálózati modellre gyakorolt hatásának megértése javítja az értelmezhetőséget. A befolyást azonban nehéz és nem hatékony értékelni, ami azt mutatja, hogy egy modell előrejelzése hogyan változna meg, ha nem használnánk egy tréningpéldányt. Ebben a tanulmányban hatékony módszert javasolunk a hatás becslésére. Módszerünket a lemaradás ihlette, amely nulla maszkot ad egy alhálózatra, és megakadályozza, hogy az alhálózat tanuljon minden egyes képzési példányt. A lemaradó maszkok közötti váltással olyan alhálózatokat használhatunk, amelyek megtanulták vagy nem tanulták az egyes képzési példányokat és becsülték annak hatását. A BERT és VGNet osztályozási adatkészleteken végzett kísérletek révén bebizonyítjuk, hogy a javasolt módszer képes megfigyelni az edzési hatásokat, javítani a hibaelőrejelzések értelmezhetőségét, és tisztítani az edzési adatkészletet az általánosítás javítása érdekében.', 'el': 'Η κατανόηση της επιρροής μιας εκπαιδευτικής μονάδας σε ένα μοντέλο νευρωνικού δικτύου οδηγεί στη βελτίωση της ερμηνείας. Ωστόσο, είναι δύσκολο και αναποτελεσματικό να αξιολογηθεί η επίδραση, γεγονός που δείχνει πώς θα άλλαζε η πρόβλεψη ενός μοντέλου αν δεν χρησιμοποιούνταν ένα εκπαιδευτικό παράδειγμα. Στην παρούσα εργασία, προτείνουμε μια αποτελεσματική μέθοδο εκτίμησης της επιρροής. Η μέθοδος μας είναι εμπνευσμένη από το παράπτωμα, το οποίο μηδενίζει μάσκα σε ένα δευτερεύον δίκτυο και εμποδίζει το δευτερεύον δίκτυο να μάθει κάθε φορά εκπαίδευσης. Με την εναλλαγή μεταξύ μάσκων παράπτωσης, μπορούμε να χρησιμοποιήσουμε υπο-δίκτυα που έμαθαν ή δεν έμαθαν κάθε εκπαιδευτική περίπτωση και να εκτιμήσουμε την επιρροή της. Μέσα από πειράματα με και σε σύνολα δεδομένων ταξινόμησης, καταδεικνύουμε ότι η προτεινόμενη μέθοδος μπορεί να συλλάβει τις επιρροές κατάρτισης, να ενισχύσει την ερμηνεία των προβλέψεων σφαλμάτων και να καθαρίσει το σύνολο δεδομένων κατάρτισης για τη βελτίωση της γενικοποίησης.', 'ka': 'შესწავლობა ნეიროლური ქსელის მოდელის შესაძლებლობას შესაძლებლობას გაუკეთება. მაგრამ მარტივია და არაფექციელია, რომელიც აჩვენებს როგორ მოდელის წინასწორება შეცვლა, თუ არა გამოყენებულია მარტივის ინსტანცია. ჩვენ ამ დოკუნეში ეფექტიური მეთოდის გამოყენება. ჩვენი მეთოდი იქნება საქაღალდეს, რომელიც ნულ-მასკის სუბქონის შესაძლებელია და სუბქონის შესაძლებელია ყოველ განსწავლების შესწავლებას. ჩვენ შეგვიძლია გამოვიყენოთ სპექსტური ქსელები, რომლებიც სწავლა ან არ ვისწავლა ყოველ სტრინციის ინსტანციას და გადავიწყენოთ მისი გეტვირ BERT და VGGNet ექსპერიმენტებით კლასიფიკაციის მონაცემების კონფიკაციაციის მონაცემებით ჩვენ გამოჩვენებთ, რომ პროგრამის მეტი შეიძლება გააკეთოთ განაკეთება, შეცდომების განაკეთება უფრო მეტივად და გან', 'it': "Comprendere l'influenza di un'istanza di formazione su un modello di rete neurale porta a migliorare l'interpretabilità. Tuttavia, è difficile e inefficiente valutare l'influenza, il che dimostra come la previsione di un modello sarebbe cambiata se non fosse utilizzata un'istanza di formazione. In questo articolo, proponiamo un metodo efficace per stimare l'influenza. Il nostro metodo è ispirato al dropout, che maschera una sottorete e impedisce alla sottorete di apprendere ogni istanza di formazione. Passando da una maschera all'altra, possiamo utilizzare sottoreti che hanno imparato o non hanno imparato ogni istanza di formazione e stimarne l'influenza. Attraverso esperimenti con BERT e VGNet su set di dati di classificazione, dimostriamo che il metodo proposto può catturare le influenze dell'allenamento, migliorare l'interpretabilità delle previsioni di errore e pulire il set di dati di allenamento per migliorare la generalizzazione.", 'mk': 'Разбирањето на влијанието на инцидентот за обука на моделот на нервната мрежа води до подобрување на интерпретабилноста. Сепак, тешко е и неефикасно да се процени влијанието, што покажува како би се променило предвидувањето на моделот ако не се користи пример за обука. Во овој весник предложуваме ефикасен метод за проценка на влијанието. Нашиот метод е инспириран од отпуштање, кое нула маска на подмрежа и ја спречува подмрежата да научи секоја инстанција на обука. Со промена помеѓу маските за отпуштање, можеме да користиме подмрежи кои научиле или не научиле секоја инстанција на обука и да го процениме своето влијание. Со експерименти со BERT и VGGNet на податоци за класификација, демонстрираме дека предложениот метод може да заземе влијанија на обуката, да ја зголеми интерпретабилноста на предвидувањата за грешки и да го исчисти податоците за обука за подобрување на генерализацијата.', 'kk': 'Невралдық желі моделінің оқыту инстанциясының әсерін түсініп, толықты жақсарту үшін болады. Бірақ бұл үлгісін бақылау қиын және әсер етпейді. Бұл үлгісінің бақылауын қалай өзгертуге болады, егер оқыту инстанциясы қолданылмаса. Бұл қағазда, бұл әсерін бағалау үшін эффективті тәсілді ұсынамыз. Біздің әдіміміз сүйреп тұрған, бұл желі нөл қалқалады және ішкі желі әрбір оқыту инстанциясын үйренуге болмайды. Қалқын қалқалар арасында ауысып, біз әрбір оқыту инстанциясын үйренген не оқылмаған субжелі қолдана аламыз. BERT және VGGNet секреттеу деректер жиындарындағы тәжірибелер арқылы, келтірілген тәжірибелердің тәжірибелерін түсіндіре алатын, қатенің алдын- алатын тәжірибелерін көтеріп, жалпы жақсарту үшін оқыту', 'lt': 'Suprantant mokymo įstaigos įtaką nervų tinklo modeliui, gali būti geriau aiškinama. Tačiau įtakos įvertinimas yra sudėtingas ir neveiksmingas, o tai rodo, kaip modelio prognozė būtų pakeista, jei nebūtų naudojamas mokymo pavyzdys. Šiame dokumente siūlome veiksmingą poveikio vertinimo metodą. Mūsų metodą įkvėpia atsisakymas, kuris nuliniu būdu maskuoja posistemį ir neleidžia posistemiui mokytis kiekvieno mokymo pavyzdžio. Keičiant nuo išėjimo kaukų galime naudoti tinklus, kurie išmoko arba nepamokė kiekvieno mokymo pavyzdžio ir įvertino jo įtaką. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.', 'ms': 'Memahami pengaruh contoh latihan pada model rangkaian saraf membawa kepada peningkatan interpretabiliti. Namun, sukar dan tidak efisien untuk menilai pengaruh, yang menunjukkan bagaimana ramalan model akan berubah jika contoh latihan tidak digunakan. Dalam kertas ini, kami mengusulkan kaedah efisien untuk menghargai pengaruh. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance.  Dengan menukar antara topeng keluar, kita boleh guna subrangkaian yang belajar atau tidak belajar setiap contoh latihan dan menghargai pengaruhnya. Melalui eksperimen dengan BERT dan VGGNet pada set data klasifikasi, kami menunjukkan bahawa kaedah yang diusulkan boleh menangkap pengaruh latihan, meningkatkan interpretabiliti ramalan ralat, dan membersihkan set data latihan untuk meningkatkan generalisasi.', 'mt': 'Il-fehim tal-influwenza ta’ istanza ta’ taħriġ fuq mudell ta’ netwerk newrali jwassal għal titjib fl-interpretabbiltà. Madankollu, huwa diffiċli u ineffiċjenti li tiġi evalwata l-influwenza, li turi kif it-tbassir ta’ mudell jinbidel jekk ma tintużax eżempju ta’ taħriġ. F’dan id-dokument, qed nipproponu metodu effiċjenti għall-istima tal-influwenza. Il-metodu tagħna huwa ispirat mill-waqfien mill-iskola, li żero maskra sottonetwerk u jipprevjeni lis-sottonetwerk milli jitgħallem kull eżempju ta’ taħriġ. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence.  Permezz ta’ esperimenti mal-BERT u l-VGGNet dwar settijiet ta’ dejta ta’ klassifikazzjoni, nagħmlu evidenza li l-metodu propost jista’ jaqbad influwenzi ta’ taħriġ, itejjeb l-interpretabbiltà tat-tbassir ta’ żbalji, u jnaddaf is-sett ta’ dejta ta’ taħriġ għat-titjib tal-ġeneralizzazzjoni.', 'ml': 'ന്യൂറല്\u200d നെറ്റര്\u200d നെറ്റ്\u200cവര്\u200dക്ക് മോഡലിന്റെ ഒരു ട്രെയിനിങ്ങളുടെ പ്രഭാവം മനസ്സിലാക്കുന്നത് വ്യാഖ്യാ എന്നാലും ഒരു ട്രെയിനിങ്ങള്\u200d ഉപയോഗിച്ചില്ലെങ്കില്\u200d മോഡലിന്റെ പ്രവചനം എങ്ങനെ മാറ്റിമാറ്റുമെന്ന് കാണിക്കുന്നത് പ്രയാസകരമ ഈ പത്രത്തില്\u200d, ഈ പ്രഭാതത്തിന്റെ പ്രഭാവം കണക്കാക്കാന്\u200d ഞങ്ങള്\u200d ഒരു സാധാരണ രീതിയില്\u200d പ്രായശ്ചിത്തമാക നമ്മുടെ രീതിയില്\u200d നിന്നും പുറത്തുകടക്കുന്നതില്\u200d നിന്നും നമ്മുടെ രീതിയില്\u200d പ്രസ്താവിക്കപ്പെടുന്നു. അത് ഒരു ശൂന്യമായ ശൃ പുറത്തുകടക്കുന്ന മുഖങ്ങള്\u200dക്കിടയില്\u200d മാറുന്നതിനാല്\u200d, എല്ലാ ട്രെയിനിങ്ങളുടെ പ്രഭാവം പഠിച്ചോ പഠിക്കാതിരിക്ക ക്ലാസ്ഫിക്കല്\u200d ഡാറ്റാസറ്റുകളില്\u200d ബെര്\u200dട്ടിയും വിജിഗ്നെറ്റുമുള്ള പരീക്ഷണങ്ങള്\u200d മുഖേന ഞങ്ങള്\u200d കാണിച്ചുകൊണ്ടിരിക്കുന്നു, പ്രൊദ്ദേശിച്ച രീതിയില്\u200d പര', 'mn': 'Сургуулийн үйл явцын нөлөөг ойлгох нь мэдрэлийн сүлжээний загвар дээр илүү ойлгомжтой болгодог. Гэхдээ энэ нөлөөлөлийг үнэлэх нь хэцүү ба үр ашиггүй. Энэ нь загварын таамаглал хэрхэн өөрчлөгдөх вэ гэдгийг харуулдаг. Энэ цаасан дээр бид нөлөөлөлийг тооцоолохын тулд үр дүнтэй аргыг санал болгож байна. Бидний арга нь суб сүлжээний зураг зурж, суб сүлжээний суб-сүлжээг суралцах үйл явц бүр суралцах боломжтой болгодог. Ингээд бид суралцах эсвэл суралцаагүй суб-сүлжээг ашиглаж, нөлөөлөлийг тооцоолж чадна. BERT болон VGGNet-тай хуваалцах өгөгдлийн сангийн туршилтаар бид тайлбарласан арга нь суралцах нөлөөлөлийг авах боломжтой, алдаа гаргах таамаглалын тухай илүү ойлгох боломжтой боломжтой болгож, ерөнхийлөгчийн сайжруулах боломжтой сура', 'no': 'Å forstå påvirkninga av eit øvingsinstans på eit neuralnettverksmodell fører til å forbetra tolkingsfeilighet. Det er imidlertid vanskelig og ikkje-effektiv å evaluera effekten, som viser korleis foregåva av eit modell vert endra dersom ein treningsinstans ikkje vert brukt. I denne papiret foreslår vi ein effektiv metode for å estimere effekten. Metoden vårt er inspirert av dropout, som null maskerer ein undernettverk og hindrar undernettverket frå læring av kvar øvingsinstans. Ved å byta mellom masker, kan vi bruka undernettverk som lærte eller ikkje lærte kvar øvingsinstans og vurdere effekten. Då gjennom eksperimenter med BERT og VGGNet på klassifiseringsdata, viser vi at den foreslåde metoden kan henta opplæringspåvirkningar, forbetra tolkingsbehandlinga av feilforhåndsvising, og tømme opplæringsdatasettet for forbedring av generellisering.', 'ro': 'Înțelegerea influenței unei instanțe de formare asupra unui model de rețea neurală duce la îmbunătățirea interpretabilității. Cu toate acestea, este dificil și ineficient să se evalueze influența, ceea ce arată modul în care predicția unui model ar fi schimbată dacă nu ar fi utilizată o instanță de formare. În această lucrare, propunem o metodă eficientă de estimare a influenței. Metoda noastră este inspirată de abandon, care nu maschează o subrețea și împiedică subrețeaua să învețe fiecare instanță de formare. Trecând între măștile de abandon, putem folosi subrețele care au învățat sau nu fiecare instanță de formare și estimăm influența acestora. Prin experimente cu BERT și VGNet pe seturi de date de clasificare, demonstrăm că metoda propusă poate capta influențele antrenamentului, spori interpretabilitatea predicțiilor erorilor și curăța setul de date de instruire pentru îmbunătățirea generalizării.', 'pl': 'Zrozumienie wpływu instancji treningowej na model sieci neuronowej prowadzi do poprawy interpretowalności. Ocena wpływu jest jednak trudna i nieefektywna, co pokazuje, w jaki sposób prognozowanie modelu uległoby zmianie, gdyby instancja treningowa nie została wykorzystana. W niniejszym artykule proponujemy skuteczną metodę oszacowania wpływu. Nasza metoda jest inspirowana dropoutem, który zero maskuje podsieć i uniemożliwia jej uczenie się każdej instancji szkoleniowej. Przełączając się między maskami wypadków, możemy wykorzystać podsieci, które nauczyły się lub nie nauczyły się każdej instancji treningowej i oszacować jej wpływ. Poprzez eksperymenty z BERT i VGGNet na zbiorach danych klasyfikacyjnych pokazujemy, że proponowana metoda może uchwycić wpływy treningowe, zwiększyć interpretację przewidywań błędów i oczyścić zbiór danych treningowych w celu poprawy uogólnienia.', 'sr': 'Razumevanje utjecaja treninga na model neuralne mreže vodi do poboljšanja interpretabilnosti. Međutim, teško je i neefektivno procijeniti uticaj, što pokazuje kako bi se predviđanje model a promijenilo ako se ne koristi trening instance. U ovom papiru predlažemo efikasnu metodu za procjenu utjecaja. Naša metoda je inspirisana dropout, koja nula maskira podmrežu i sprečava podmrežu da uči svaki trening instant. Premjenjujući između maski dropout, možemo koristiti podmreže koje su naučile ili nisu naučili svaku trening instancu i procenili njegov utjecaj. Kroz eksperimente sa BERT i VGGNet na klasifikacijskim setima podataka, pokazujemo da predloženi metod može da uhvati utjecaje na obuku, poboljšava interpretabilnost predviđanja grešaka i očisti setu podataka obuke za poboljšanje generalizacije.', 'si': 'ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තේරුම් ගන්න ප්\u200dරශ්නයක් නිර්මාණ ජාලයේ මොඩේල් එකේ ප්\u200dරශ්නයක් විශ නමුත් ඒක අමාරුයි සහ අපරාධක විශ්වාස කරන්න, ඒක පෙන්වන්නේ ප්\u200dරශ්නයක් ප්\u200dරයෝජනයක් නොවුනොත් ප්\u200dරයෝජනයක් වෙන මේ පත්තරේ අපි ප්\u200dරශ්නයක් අනුමාන කරන්න ප්\u200dරශ්නයක් ප්\u200dරයෝජනය කරනවා. අපේ විධානය ප්\u200dරශ්න විදිහට ප්\u200dරශ්න වෙනවා, ඒක සුබ- ජාලයෙක් සුබ- මාස්ක් කරනවා, සබ- ජාලයෙක් ප්\u200dරශ්න විදිහ බ්\u200dරෝපුට් මාස්ක් අතර වෙනස් කරන්න, අපිට පුළුවන් සබ් ජාලය ප්\u200dරයෝජනය කරන්න පුළුවන් හැම ප්\u200dරයෝජනයක්ම ඉගෙ BERT සහ VGGnet සඳහා පරීක්ෂණාවන් විශේෂණ දත්ත සේට් වල, අපි පෙන්වන්න පුළුවන් විදිහට ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් අල්ලගන්න පුළුවන් කියලා, වැ', 'so': 'Marka la garanayo saameyn ku yeelashada tusaale ahaan tababar-barashada sameynta shabakadda naafada ah waxay ku socotaa horumarinta turjubaanka. Si kastaba ha ahaatee waa adag tahay, waana adag tahay in la qiimeeyo saameynta, taas oo tusaale ahaan loo beddeli karo qaababka qaababka, haddii aan lagu isticmaalin tusaale waxbarasho. Qoraalkan waxaynu soo jeedaynaa qaab faa’iido leh oo ku qiimeynaya saamaynta. Metalkeena waxaa loo waxyooday in la soo saaro, kaas oo aan nuurka u qabsado shabakadda shabakadda, wuxuuna ka hor marsadaa in lagu barto tusaale kasta waxbarasho. Si aan u beddelo maskaxda, waxaynu isticmaali karnaa shabakado hoose ah oo baratay ama aan baran tusaale kasta oo waxbarasho ah islamarkaasna qiimeyn karnaa saameyntiisa. Imtixaanka BERT iyo VGGNet ee ku saabsan macluumaadka tababarida, waxaynu caddaynaynaa in qaababka la soo jeeday ay ay saameyn ku yeelan karaan waxbarashada, uu kordhin karo turjumaadda wixii la sii sheegay qaladka, waxaana nadiifin karnaa macluumaadka waxbarashada si uu u beddelo wax la soo saaray.', 'ta': 'ஒரு புதிய வலைப்பின்னல் மாதிரியில் பயிற்சி முக்கிய விளைவு புரிந்து கொள்ளும் போது முறையில் மாற்ற ஆனால், தாக்கத்தை மதிப்பதற்கு கடினமானது மற்றும் போதுமானது, ஒரு பயிற்சி நிகழ்வு பயன்படுத்தப்படவில்லை இந்த காகிதத்தில், நாம் தாக்கத்தை மதிப்பதற்கு ஒரு தேவையான முறையை பரிந்துரைக்கிறோம். எங்கள் முறையில் வெளியேறுதல் மூலம் தெரியப்படுத்தப்படுகிறது, அது ஒரு துணை வலைப்பிணையம் முகத்தை மூடுகிறது மற்றும் ஒவ்வொரு பயி வெளியேறும் முகங்களுக்கிடையே மாறுதலால், ஒவ்வொரு பயிற்சி நிகழ்வை கற்றுக் கொண்டும் அல்லது கற்றுக் கொள்ளாமலும் துணை  வகுப்பு தரவுத்தளங்களில் BERT மற்றும் VGGNet மூலம் சோதனைகள் மூலம் நாம் காண்பிக்கிறோம் பரிந்துரைக்கப்பட்ட முறையில் பயிற்சி விளைவுகளை பிடிக்க முடியும், ப', 'sv': 'Att förstå hur en träningsinstans påverkar en neural nätverksmodell leder till förbättrad tolkning. Det är dock svårt och ineffektivt att utvärdera påverkan, vilket visar hur en modells förutsägelse skulle ändras om en träningsinstans inte användes. I denna uppsats föreslår vi en effektiv metod för att uppskatta påverkan. Vår metod är inspirerad av dropout, som nollmaskerar ett delnätverk och hindrar delnätverket från att lära sig varje utbildningsinstans. Genom att byta mellan dropout masker kan vi använda undernätverk som lärt oss eller inte lärt oss varje utbildningsinstans och uppskatta dess påverkan. Genom experiment med BERT och VGNet på klassificeringsdatauppsättningar visar vi att den föreslagna metoden kan fånga träningsinfluenser, förbättra tolkningen av felförutsägelser och rensa träningsdatauppsättningen för att förbättra generaliseringen.', 'ur': 'ایک نیورل نیٹ ورک موڈل پر تطارین کے مثال کا تأثیر سمجھنے کے باعث تفصیل کا تأثیر ہے۔ اگرچہ، اثرات کا ارزش کرنا مشکل اور غیر اثرات ہے، جو دکھاتا ہے کہ ایک نمڈل کا ارزش کس طرح بدل دیا جائے گا اگر کوئی تعلیم نمونہ استعمال نہ ہوتا۔ اس کاغذ میں ہم ایک مفید طریقہ پیشنهاد کرتے ہیں تاثیر کا مطالبہ کرنے کے لئے۔ ہمارا طریقہ ڈروپوٹ کے ذریعے الهام کی جاتی ہے، جسے صفر-ماسک ایک سوب-نیٹ ورک کرتا ہے اور سوب-نیٹ ورک کو ہر تدریس مثال کی تعلیم سے روکتا ہے. ڈروپوٹ ماسک کے درمیان تغییر دینے کے ذریعہ ہم سوب-نیٹورک کو استعمال کر سکتے ہیں جو ہر تغییر کے مطابق سکھائی یا نہ سکھائی اور اس کے تاثیر کا ارزش کرسکتے ہیں. BERT اور VGGNet کے آزمائش کے ذریعے ہم دکھاتے ہیں کہ پیشنهاد کی طریقہ تدریس اثرات کو پکڑ سکتی ہے، خطا پیش بینی کی تدریجی قابلیت کو بڑھاتی ہے، اور ترینس ڈیٹ سٹ کو عمومی تدریجی کے لئے پاک کر سکتی ہے.', 'uz': "Name Lekin, tashkilotni qiymatish juda qiyin va qo'llanmagan, bu modelning oldini qanday o'zgartirish mumkin, agar ta'lim imkoniyatini ishlatilmasa. Bu qogʻozda, biz tashkilotni tasavvur qilish uchun qo'llanmalar. Bizning usuli o'chirib chiqishga imkoniyat beradi, bu tub tarmoqni hech narsa yozib qo'yish va har bir trening imkoniyatlarini o'rganishdan foydalanadi. Ulash uskunalarni o'zgartirish orqali biz har bir trening misollarini o'rganish yoki o'rganish amalni o'rganish mumkin va uning effektini qiymatimiz mumkin. Tafsilotlar maʼlumotlar bazasini BERT va VGGNet bilan taʼminlovchi imtiyozlar orqali biz rivojlanadigan usulni koʻrsatishimiz mumkin. Taʼminlovchi usulni taʼminlovchi amalni qabul qilishi mumkin, xato 预测larining tolarini oshirish va generalizni oshirish uchun taʼminlovchi maʼlumotlarni tozalash mumkin.", 'vi': 'Hiểu được tầm ảnh hưởng của một tiến trình huấn luyện trên mô hình mạng thần kinh dẫn đến việc cải thiện sự thấu hiểu. Tuy nhiên, đánh giá ảnh hưởng khó khăn và không hiệu quả, cho thấy khả năng dự đoán của mô-đun s ẽ bị thay đổi nếu không sử dụng một trường hợp huấn luyện. Trong tờ giấy này, chúng tôi đề xuất một phương pháp hiệu quả để đánh giá ảnh hưởng. Phương pháp của chúng tôi được truyền cảm hứng từ việc bỏ học, không che giấu một mạng phụ và ngăn cản xã giao học mỗi trường đào tạo. Thay vào giữa các mặt nạ bỏ ra, chúng ta có thể sử dụng các mạng phụ học được hoặc không học mỗi trường đào tạo và ước lượng sức ảnh hưởng của nó. Qua các thí nghiệm với thiếu sót sót sót sót của thiếu sót sót sót sót sót sót sót và VGG đã được xác định. Chúng tôi đã chứng minh rằng phương pháp đã được áp dụng có thể nắm bắt các ảnh hưởng của huấn luyện, nâng cao tính toán lỗi, và thanh lọc dữ liệu đào tạo để cải thiện quy mô.', 'da': 'Forståelse af indflydelsen af en træningsinstans på en neural netværksmodel fører til forbedring af fortolkningen. Det er imidlertid vanskeligt og ineffektivt at vurdere indflydelsen, hvilket viser, hvordan en models forudsigelse ville blive ændret, hvis en træningsinstans ikke blev brugt. I denne artikel foreslår vi en effektiv metode til at vurdere indflydelsen. Vores metode er inspireret af dropout, som nul-maskerer et undernetværk og forhindrer undernetværket i at lære hver træningsinstans. Ved at skifte mellem dropout masker kan vi bruge undernetværk, der lærte eller ikke lærte hver træningsinstans, og vurdere dens indflydelse. Gennem eksperimenter med BERT og VGNet på klassificeringsdatasæt viser vi, at den foreslåede metode kan indfange træningspåvirkninger, forbedre fortolkningen af fejlforudsigelser og rense træningsdatasættet for at forbedre generaliseringen.', 'nl': 'Inzicht in de invloed van een trainingsinstantie op een neuraal netwerkmodel leidt tot een betere interpreteerbaarheid. Het is echter moeilijk en inefficiënt om de invloed te evalueren, wat laat zien hoe de voorspelling van een model zou worden gewijzigd als een trainingsinstantie niet zou worden gebruikt. In dit artikel stellen we een efficiënte methode voor om de invloed te schatten. Onze methode is geïnspireerd op drop-out, waarbij een sub-netwerk wordt gemaskeerd en voorkomt dat het sub-netwerk elke trainingsinstantie leert. Door te schakelen tussen drop-out maskers, kunnen we subnetwerken gebruiken die elk trainingsexemplaar al dan niet hebben geleerd en de invloed ervan inschatten. Door experimenten met BERT en VGGNet op classificatiedatasets tonen we aan dat de voorgestelde methode trainingsinvloeden kan vastleggen, de interpreteerbaarheid van foutvoorspellingen kan verbeteren en de trainingsdataset kan reinigen om de generalisatie te verbeteren.', 'hr': 'Razumijevanje utjecaja treninga na model neurone mreže dovede do poboljšanja interpretabilnosti. Međutim, teško je i neefektivno procijeniti utjecaj, što pokazuje kako će se predviđanje model a promijeniti ako se ne koristi primjer obuke. U ovom papiru predlažemo efikasnu metodu procjene utjecaja. Naš metod je inspirisan od dropout, koja nula maskira podmrežu i sprječava podmrežu da uči svaki trening primjer. Premjenjujući između maski dropout, možemo koristiti podmreže koje su naučile ili nisu naučili svaki trening primjer i procijeniti njegov utjecaj. Kroz eksperimente sa BERT i VGGNet na klasifikacijskim podacima, pokazujemo da predloženi metod može uhvatiti utjecaje na obuku, poboljšati interpretabilnost predviđanja grešaka i očistiti sastanak podataka obuke za poboljšanje generalizacije.', 'bg': 'Разбирането на влиянието на обучителната инстанция върху модела на невронната мрежа води до подобряване на интерпретацията. Въпреки това е трудно и неефективно да се оцени влиянието, което показва как прогнозата на модела ще бъде променена, ако не се използва тренировъчна инстанция. В настоящата статия предлагаме ефективен метод за оценка на влиянието. Нашият метод е вдъхновен от отпадането, което нулево маскира подмрежа и предотвратява подмрежата да научи всяка обучителна инстанция. Чрез превключване между маски за отпадане, можем да използваме подмрежи, които са научили или не са научили всяка обучителна инстанция и да оценим влиянието й. Чрез експерименти с BERT и VGGNet върху класификационните набори от данни, ние демонстрираме, че предложеният метод може да улови влиянието на обучението, да подобри интерпретацията на прогнозите за грешки и да изчисти набора от данни за обучение за подобряване на обобщението.', 'de': 'Das Verständnis des Einflusses einer Trainingsinstanz auf ein neuronales Netzwerkmodell führt zu einer besseren Interpretierbarkeit. Es ist jedoch schwierig und ineffizient, den Einfluss zu bewerten, was zeigt, wie sich die Vorhersage eines Modells ändern würde, wenn eine Trainingsinstanz nicht verwendet würde. In diesem Beitrag schlagen wir eine effiziente Methode zur Abschätzung des Einflusses vor. Unsere Methode ist inspiriert von Dropout, der ein Sub-Netzwerk maskiert und verhindert, dass das Sub-Netzwerk jede Trainingsinstanz lernt. Durch Umschalten zwischen Dropout-Masken können wir Subnetzwerke nutzen, die jede Trainingsinstanz gelernt haben oder nicht, und deren Einfluss abschätzen. Durch Experimente mit BERT und VGGNet an Klassifikationsdatensätzen zeigen wir, dass die vorgeschlagene Methode Trainingseinflüsse erfassen, die Interpretierbarkeit von Fehlervorhersagen verbessern und den Trainingsdatensatz zur Verbesserung der Generalisierung bereinigen kann.', 'id': 'Memahami pengaruh dari contoh latihan pada model jaringan saraf mengarah untuk meningkatkan interpretabilitas. Namun, sulit dan tidak efisien untuk mengevaluasi pengaruh, yang menunjukkan bagaimana prediksi model akan berubah jika contoh latihan tidak digunakan. In this paper, we propose an efficient method for estimating the influence.  Metode kita terinspirasi oleh keluar, yang nol-topeng sub-jaringan dan mencegah sub-jaringan dari belajar setiap contoh pelatihan. Dengan menukar antara topeng keluar, kita dapat menggunakan sub-jaringan yang belajar atau tidak belajar setiap contoh pelatihan dan memperkirakan pengaruhnya. Melalui eksperimen dengan BERT dan VGGNet pada set data klasifikasi, kami menunjukkan bahwa metode yang diusulkan dapat menangkap pengaruh pelatihan, meningkatkan interpretabilitas prediksi kesalahan, dan membersihkan set data pelatihan untuk meningkatkan generalisasi.', 'ko': '훈련 실례가 신경망 모델에 미친 영향을 이해하는 것은 해석성을 높이는 데 도움이 된다.그러나 평가의 영향은 어렵고 효과가 적으며 훈련 실례를 사용하지 않으면 모델의 예측이 어떻게 바뀔지 보여준다.본문에서 우리는 효과적인 방법을 제시하여 영향을 평가했다.우리의 방법은 학업을 그만두는 계발을 받았다. 그것은 서브네트워크를 제로 차단하여 서브네트워크가 모든 훈련 실례를 학습하는 것을 방지한다.마스크를 종료하는 동안 전환하여 각 트레이닝 인스턴스를 학습하거나 학습하지 않은 서브넷을 사용하여 영향을 추정할 수 있습니다.분류 데이터 세트에서 BERT와 VGGnet을 사용한 실험을 통해 우리는 이 방법이 훈련의 영향을 포획하고 오류 예측의 해석성을 강화하며 훈련 데이터 세트를 정리하여 범위화 능력을 향상시킬 수 있음을 증명하였다.', 'fa': 'درک تاثیر یک نمونه آموزش روی یک نمونه شبکه عصبی به بهترین تعبیرات می\u200cرسد. ولی برای ارزیابی تأثیر این تأثیر سخت و بی\u200cفایده است که نشان می\u200cدهد چگونه پیش\u200cبینی یک مدل تغییر می\u200cشود اگر یک نمونه آموزش استفاده نمی\u200cشود. در این کاغذ، ما یک روش موثری برای ارزیابی تاثیر را پیشنهاد می کنیم. روش ما توسط سقوط نابودی الهام می\u200cشود که صفر یک شبکه زیر می\u200cسازد و جلوی شبکه زیر را از یادگرفتن هر نمونه آموزش می\u200cگیرد. با تغییر بین ماسک\u200cهای غبار، می\u200cتوانیم از شبکه\u200cهای زیر\u200cشبکه\u200cهایی استفاده کنیم که یاد گرفته یا یاد نداده\u200cاند هر نمونه آموزش و تأثیرش را ارزیابی کنیم. از طریق آزمایشات با BERT و VGGNet در مجموعه\u200cهای داده\u200cهای classification، نشان می\u200cدهیم که این روش پیشنهاد می\u200cتواند تأثیرات آموزش را بگیرد، تأثیر قابلیت پیش\u200cبینی خطا را بیشتر کند، و مجموعه داده\u200cهای آموزش را برای بهترین توسعه عمومی پاک کند', 'sw': 'Kuelewa madhara ya mifano ya mafunzo kwenye muundo wa mtandao wa neura unapelekea kuboresha tafsiri. Hata hivyo, ni vigumu na haitoshi kutathmini ushawishi huo, ambao unaonyesha namna utabiri wa model utabadilika kama namna ya mafunzo isivyotumiwa. Katika karatasi hii, tunapendekeza njia yenye ufanisi wa kutathmini athari. Utawala wetu unahamasishwa na kuachia, ambao haujaweka sifuri kwenye mtandao wa intaneti na kuzuia mtandao wa intaneti ili kujifunza kila namna ya mafunzo. Kwa kubadilisha kati ya vifaa vya kuachia, tunaweza kutumia mitandao ya kijamii ambazo zilifunza au hawakujifunza kila namna ya mafunzo na kutathmini athari yake. Kwa kupitia majaribio ya BERT na VGGNet kwenye seti za taarifa za usafi, tunaonyesha kuwa mbinu zilizopendekezwa inaweza kushika madhara ya mafunzo, kuongeza tafsiri ya kutabiri za makosa, na kusafisha seti ya mafunzo kwa ajili ya kuboresha ujamaduni.', 'tr': 'Näral şebeke nusgasynda okuw öräniniň täsirini düşünmek terjime edip ýagdaýyny gowlaşdyrar. Ýöne, täsirini baýlamak kyn we ýetersiz. Bu nusga örän nusga üýtgetmelidigini görkezýär. Bu kagyzda, güýjäni tahmin etmek üçin etkinlik täsirini maslahat berýäris. Biziň ýüregimiz çöplük tarapyndan ilham alýar, bu syýasy altyndaky şebekeden ýüz-şebekesini her okuw öwrenmesini engelleýär. Çagajyk maskalaryň arasynda öwrenip bilmeýän, öwrenip bilmeýän alt şebekeleri ulanyp bileris we täsirini tahmin edip bileris. BERT we VGGNet bilen klasifikasiýa veri setirlerinde deneyler bilen, teklip eden metoduň eğitim näsirlerini almak üçin, hata öňünden terjime edip biljekdigini we generalizasyýany gowuramak üçin okuw setirini temizleýäris.', 'af': "Om te verstaan die influens van 'n oefening voorbeeld op 'n neurale netwerk model, lei dit na verbetering van uitleggingsverklaring. Maar dit is moeilik en onvoldoende om die influens te evalueer, wat wys hoe 'n model se voorskou sal verander word a s 'n onderwerp voorbeeld nie gebruik word nie. In hierdie papier voorstel ons 'n effektief metode om die influens te evalueer. Ons metode is inspireer deur dropout, wat nul masker 'n sub-netwerk en verhinder die sub-netwerk om elke oefening voorbeeld te leer. Deur te wissel tussen dropout maskers, kan ons sub-netwerke gebruik wat leer of nie geleer het elke oefening voorbeeld en sy influens evalueer nie. Deur eksperimente met BERT en VGGNet op klassifikasie datastelle, wys ons dat die voorgestelde metode onderwerp influense kan opneem, die uittelbare verklaring van fout voorskou verhoog en die onderwerp datastel skoonmaak vir verbetering van generalisering.", 'sq': 'Përkuptimi i ndikimit të një rasti trajnimi në një model rrjeti nervor shpie në përmirësimin e interpretueshmërisë. Megjithatë, është e vështirë dhe e paefektshme të vlerësohet ndikimi, i cili tregon se si parashikimi i një modeli do të ndryshohet po të mos përdorej një shembull trajnimi. Në këtë letër, propozojmë një metodë të efektshme për vlerësimin e ndikimit. Metoda jonë është frymëzuar nga dalja, e cila zero-maskon një nënrrjet dhe pengon nënrrjetin që të mësojë çdo rast trajnimi. Duke shkëmbyer mes maskave të largimit, ne mund të përdorim nënrrjete që mësuan ose nuk mësuan çdo rast trajnimi dhe vlerësojnë ndikimin e tij. Nëpërmjet eksperimenteve me BERT dhe VGGNet mbi grupet e të dhënave klasifikuese, ne demonstrojmë se metoda e propozuar mund të kapë ndikimet e trajnimit, të përmirësojë interpretueshmërinë e parashikimeve të gabimeve dhe të pastrojë grupin e të dhënave të trajnimit për përmirësimin e gjeneralizimit.', 'am': 'በናውራዊ መረብ ሞዴል ላይ የትምህርት ምሳሌ የሚደረገውን ለማስተዋል ትርጓሜውን ለማሻል ይችላል፡፡ ነገር ግን የሞዴል ትንቢት እንዴት እንደተለወጠ የሞዴል ውይይት እንዳይለወጥ ይችላል፡፡ በዚህ ካላት፣ የጥቅም ጉዳይ መቆጣጠር የሚችል ሥርዓት እናሳውቃለን፡፡ የሥርዓታችን ልማድ በጥቅምት አዳራሽ ነው፡፡ በመለስ፣ ሁሉንም ትምህርት ምሳሌ በማስተማሩ ወይም በማያስተማሩ የመደቡን መረብ እናስቀናለን፡፡ በBERT እና VGGNet በተፈተና ክፍተቶችን በሚያሳየው የዳታ ማህበረሰብ ማቀናጃ እንዲያገኝ፣ የስህተት ቅድሚያ ማተርጓሜን እንዲያበዛ እና ማስተማርንም እንዲያበዛ እና ማስተካከል ማስተካከል ዳታዎችን ማነጻ እናደርጋለን፡፡', 'hy': 'Նյարդային ցանցի մոդելի վրա ուսուցման օրինակի ազդեցությունը հասկանալը հանգեցնում է բարելավելու թարգմանելիությունը: Այնուամենայնիվ, ազդեցությունը գնահատելու դժվար և անարդյունավետ է, ինչը ցույց է տալիս, թե ինչպես կփոխվի մոդելի կանխատեսումը, եթե փորձի օրինակ չօգտագործվի: Այս թղթի մեջ մենք առաջարկում ենք ազդեցության գնահատման արդյունավետ մեթոդ: Մեր մեթոդը ոգեշնչվում է դուրս գալուց, որը զրոյական դիմակ է տալիս ենթացանցի և կանխում է ենթացանցի ուսումնասիրելուց յուրաքանչյուր ուսուցման օրինակ: Առաջնական դիմակների միջև փոխելով, մենք կարող ենք օգտագործել ենթացանցեր, որոնք սովորել են կամ չեն սովորել յուրաքանչյուր ուսուցման օրինակ և գնահատել դրա ազդեցությունը: BER-ի և VGGNet-ի հետ կապված փորձարկումների միջոցով դասակարգումների տվյալների համակարգերի վրա մենք ցույց ենք տալիս, որ առաջարկված մեթոդը կարող է վերցնել ուսուցման ազդեցություններ, բարելավել սխալների կանխատեսումների մեկնաբանելիությունը և մաքրել ուսուցման տվյալների համակար', 'az': 'N…ôyral a ńü modelinin t…ôhsil edilm…ôsini anlamaq t…ôhsil edir. Lakin, t…ôsiri baxmayaraq, modelin t…ôsiri nec…ô d…ôyiŇüdiril…ôc…ôyini g√∂st…ôrm…ôk √ß…ôtin v…ô faydalanmaz. Bu kańüńĪzda, t…ôsiri hesablamaq √ľ√ß√ľn faydalńĪ bir yol t…ôklif edirik. Bizim metodumuz d…ôyiŇüiklikd…ôn ilham alńĪr ki, sńĪfńĪr-a ńüńĪ gizl…ôdir v…ô h…ôr t…ôhsil √∂yr…ônm…ôsin…ô mane olur. S√ľf√ľr…ôn maskalar arasńĪnda d…ôyiŇü…ôr…ôk, √∂yr…ôndiyimiz v…ô ya √∂yr…ônm…ôdiyimiz sub-networks istifad…ô ed…ô bil…ôrik. BERT v…ô VGGNet t…ôcr√ľb…ôl…ôrinin klasifikasiya veril…ôn qurularńĪ il…ô t…ôcr√ľb…ôl…ôrin t…ôcr√ľb…ôsini almaq, x…ôta t…ôcr√ľb…ôl…ôrinin yorumluluńüunu artńĪrmaq v…ô t…ôcr√ľb…ô veril…ôn quruluńüu generalizasyonu yaxŇüńĪlaŇüdńĪrmaq √ľ√ß√ľn t…ôcr√ľb…ô quruluńüunu t…ômizl…ôndir…ôk g√∂st…ôririk.', 'bn': 'নিউরেল নেটওয়ার্ক মডেলের উপর একটি প্রশিক্ষণের প্রভাব বুঝতে পারে সেটা বুঝতে পারে যে ব্যাখ্যাতি উন্নত করে। তবে প্রভাব মূল্যায়ন কঠিন এবং কঠিন, যা দেখাচ্ছে কিভাবে একটি মডেলের ভবিষ্যৎবাণী পরিবর্তন করা হবে যদি প্রশিক্ষণ না করা হয়। এই কাগজটিতে আমরা প্রভাব হিসেবে কার্যকর পদ্ধতি প্রস্তাব করি। আমাদের পদ্ধতি ফেলে দেওয়া হয়েছে, যা একটি সাব-নেটওয়ার্ক মুখোশ করে এবং প্রত্যেকটি প্রশিক্ষণের উদ্দেশ্য শিখতে বাধা দেয়। মুখোশ পরিবর্তনের মাধ্যমে আমরা সাব-নেটওয়ার্ক ব্যবহার করতে পারি যা শিখেছে অথবা প্রত্যেক প্রশিক্ষণের উদ্দেশ্য শিখতে পারে না এবং ত বার্টি এবং ভিজিজিনেটের পরীক্ষার মাধ্যমে আমরা প্রমাণ করছি যে প্রস্তাবিত পদ্ধতি প্রশিক্ষণের প্রভাব গ্রহণ করতে পারে, তার ভুল ভবিষ্যতের ব্যাখ্যা বাড়িয়ে দিতে পারে এবং', 'bs': 'Razumijevanje utjecaja treninga na model neurone mreže vodi do poboljšanja interpretabilnosti. Međutim, teško je i neefektivno procijeniti utjecaj, što pokazuje kako bi se predviđanje model a promijenilo ako se ne koristi trening instance. U ovom papiru predlažemo efikasnu metodu procjene utjecaja. Naša metoda je inspirisana dropout, koja nula maskira podmrežu i sprječava podmrežu da uči svaki trening primjer. Premjenjujući između maski dropout, možemo koristiti podmreže koje su naučile ili nisu naučili svaku trening instancu i procijeniti njegov utjecaj. Kroz eksperimente sa BERT i VGGNet na klasifikacijskim podacima, pokazujemo da predloženi metod može uhvatiti utjecaje na obuku, poboljšati interpretabilnost predviđanja grešaka i očistiti setu podataka obuke za poboljšanje generalizacije.', 'cs': 'Pochopení vlivu tréninkové instance na model neuronové sítě vede ke zlepšení interpretovatelnosti. Je však obtížné a neefektivní vyhodnotit vliv, což ukazuje, jak by se predikce modelu změnila, pokud by nebyla použita tréninková instance. V tomto článku navrhujeme efektivní metodu odhadu vlivu. Naše metoda je inspirována dropoutem, který nulově maskuje podsíť a brání tomu, aby se subsíť učila každou tréninkovou instanci. Přepínáním mezi ukončovacími maskami můžeme použít subsítě, které se naučily nebo se neučily každou tréninkovou instanci a odhadnout její vliv. Prostřednictvím experimentů s BERT a VGGNet na klasifikačních datových sadách demonstrujeme, že navržená metoda dokáže zachytit vlivy tréninku, zlepšit interpretovatelnost chybových predikcí a vyčistit tréninkový datový set pro zlepšení zobecnění.', 'ca': "Entendre l'influència d'una instancia d'entrenament en un model de xarxa neural porta a millorar l'interpretabilitat. Tanmateix, és difícil i ineficient avaluar l'influència, que mostra com es canviaria la predicció d'un model si no s'utilitzés una instancia d'entrenament. En aquest paper, proposem un mètode eficient per estimar l'influència. El nostre mètode està inspirat en l'abandon, que no mascarauna subxarxa i impedeix que la subxarxa aprenga cada exemple d'entrenament. Al canviar entre màscares d'abandon, podem utilitzar subxarxes que aprenen o no aprenen cada exemple d'entrenament i estimen la seva influència. Amb experiments amb BERT i VGGNet en conjunts de dades de classificació, demostram que el mètode proposat pot capturar influències de formació, millorar l'interpretabilitat de les prediccions d'errors i netejar el conjunt de dades de formació per millorar la generalització.", 'et': 'Koolituse mõju mõistmine närvivõrgu mudelile aitab parandada tõlgendatavust. Mõju on aga raske ja ebatõhus hinnata, mis näitab, kuidas mudeli prognoosi muudetakse, kui koolitust ei kasutata. Käesolevas dokumendis pakume välja tõhusa meetodi mõju hindamiseks. Meie meetod on inspireeritud väljalangemisest, mis null-maskeerib alamvõrgu ja takistab alamvõrgul õppida iga koolituse eksemplari. Lülitudes väljalangemismaskide vahel saame kasutada alamvõrgustikke, mis õppisid või ei õppinud iga koolituse eksemplari ja hindavad selle mõju. BERT ja VGGNet klassifitseerimisandmekogumitega tehtavate katsete kaudu näitame, et kavandatud meetod suudab jäädvustada treeningu mõju, parandada veaprognooside tõlgendatavust ja puhastada treeningu andmekogumit üldistamise parandamiseks.', 'fi': 'Harjoittelun vaikutuksen ymmärtäminen neuroverkkomalliin johtaa tulkittavuuden parantamiseen. Vaikutuksen arviointi on kuitenkin vaikeaa ja tehotonta, mikä osoittaa, miten mallin ennuste muuttuisi, jos koulutusinstanssia ei käytettäisi. Tässä työssä ehdotamme tehokasta menetelmää vaikutusten arvioimiseksi. Menetelmämme inspiraationa on keskeyttäminen, joka nollaa aliverkon ja estää aliverkkoa oppimasta jokaista harjoitusta. Vaihtamalla pudotusmaskien välillä voimme käyttää aliverkkoja, jotka oppivat tai eivät oppineet jokaista harjoitusta ja arvioida sen vaikutusta. BERT- ja VGGNet-kokeilla osoitetaan, että ehdotettu menetelmä voi tallentaa harjoitusvaikutteita, parantaa virheennusteiden tulkittavuutta ja puhdistaa harjoitusaineiston yleistymisen parantamiseksi.', 'jv': 'Rasané tatauran anyar kang dipolehasan kuwi modèl kuwi nawala politenessoffpolite"), and when there is a change ("assertivepoliteness Nang pemilih iki, kita mudhun cara-cara sing luwih apik kanggo nggawe barang nggawe luwih apik. Awakdhéwé éntuk kelompok nggambar Drop, sing nul-mask teko netwisik lan nggawe ngubah netwisik apak-netwisik kanggo nggambar sapa téraning sakjane politenessoffpolite"), and when there is a change ("assertivepoliteness Ato neng éntuk perbudhakan karo BERT karo VGGNet nang dataset klasik, kéné ampuhi nggawe supoyo supoyo nggawe aturan sing beraksi tindakan cara-cara, supoyo perbudhakan kanggo ngerasai perbudhakan kanggo ngerasai perbudhakan kanggo ndelok dhéwé, lan susungi nggawe dataset tukang nggawe General', 'sk': 'Razumevanje vpliva treninga na model nevronskega omrežja vodi k izboljšanju interpretabilnosti. Vendar pa je težko in neučinkovito oceniti vpliv, kar kaže, kako bi se napoved modela spremenila, če ne bi bila uporabljena usposabljanja. V prispevku predlagamo učinkovito metodo ocenjevanja vpliva. Naša metoda je navdihnjena z opustitvijo, ki nič zakriva podomrežje in preprečuje, da bi se podomrežje naučilo vsakega treninga. S preklopom med maskami za opustitev lahko uporabimo podomrežja, ki se naučijo ali niso naučile vsakega treninga in ocenimo njegov vpliv. Z eksperimenti z BERT in VGGNet o klasifikacijskih naborih podatkov dokazujemo, da lahko predlagana metoda zajame vplive treninga, izboljša interpretabilnost napovedi napak in očisti nabor podatkov treninga za izboljšanje generalizacije.', 'ha': "Ana fahimta mai amfani da misali na zaman shagala a kan wata misãlin shirin neural, yana ƙara wa gyara fassarar. However, it is difficult and inefficient to evaluate the influence, which shows how a model's prediction would be changed if a training instance were not used.  Ga wannan takarda, Munã buɗa wata metode mai fasahan da za'a estiraye muhimmada. An yi wahayi zuwa hanyoyinmu da ɓacewa, wanda yana gaurar da haske-maɓallin-shawarar, kuma yana hani da shirin-tarayya don ya sanar da kowace misãlai. Ina iya canza a tsakanin maɓallin aiki, ko za mu iya amfani da waɗancan masu ƙarƙashin tarayya waɗanda suka sanar ko ba su sanar da kowane misãlai na ƙidãya mai amfani da shi ba. Ko bayan jarrabãwa da BERT da vGGNet kan danne-tsari, za mu nuna cewa shirin da aka buƙata yana iya kãma mai amfani da amfani, kuma yana ƙara fassarar fassarar abin da ake yi na gabanin ɓata, kuma Mu tsarkake danna na tsarin da za'a yi amfani da wajen kyautatawa.", 'bo': 'དབུལ་མཐུད་དྲ་མ་དཔེ་གཏོང་གི་གནོད་སྐུལ་ཤུགས་ཀྱི་གོ་སྐབས་ཡོད་པ་དེ་གླེང་སྒྲུབ་ལ་ཡར་རྒྱས ཡིན་ནའང་ནི་དཀའ་ངལ་དང་ལས་རྐྱེན་ཚད་ལྟར་ཞིབ་བྱེད་རྒྱུ་མེད་པས། གལ་སྲིད། དཔེར ཤོག་བུ་འདིའི་ནང་དུ་འུ་ཅག་གིས་དབུགས་ཕྱོགས་ནུས་ཡོད་ཐབས་ལམ་ཞིག་བསམ་བྱེད་ཀྱི་ཡོད། ང་ཚོའི་ལམ་ལུགས་འདི་སྐུད་གཙང་གཅད་ཀྱིས་མེད་པར་ཐག་གཙང་འགོད་པ་དང་དྲ་རྒྱ་སྟངས་སྔོན་བསྐྱུར་བྱེད་ཀྱི་ཡོད། གནད་དོན་གདོང་འགེས་དག་སའི་བར་དུ་བསྒྱུར་བཅོས་བྱས་ན། ང་ཚོས་ཤེས་པའི་མཐུད་དྲ་རྒྱ་སྟངས BERT དང་VGGNet ཡི་གྲངས་སྒྲིག་ཆ་ཡིག་ཆ་ལ་སྤྱོད་པའི་བརྟག་ཞིབ་བྱས་ལས། འུ་ཅག་གིས་དམིགས་འཛུགས་པའི་ཐབས་ལམ་གྱིས་གཟུགས་རིས་འགྱུར་བ་དང་ནོར་འཁྲུལ་རྒྱས་སྤྲོད་ཀ', 'he': "להבין את השפעה של מקרה אימון על מודל רשת עצבית מוביל לשפר את היכולת לפרש. However, it is difficult and inefficient to evaluate the influence, which shows how a model's prediction would be changed if a training instance were not used.  בעיתון הזה, אנו מציעים שיטה יעילה להערכה את ההשפעה. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance.  By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence.  באמצעות ניסויים עם BERT ו VGGNet על קבוצות נתונים מסווגים, אנו מראים שהשיטה המוצעת יכולה לתפוס השפעות אימונים, לשפר את אפשרות הפרשנות של חזיונות שגיאות, ולנקות את קבוצת נתונים האימונים לשפר את הגנרליזציה."}
{'en': 'Efficient Inference For Neural Machine Translation', 'ar': 'الاستدلال الفعال لترجمة الآلة العصبية', 'pt': 'Inferência eficiente para tradução automática neural', 'es': 'Inferencia eficiente para la traducción automática neuronal', 'fr': 'Inférence efficace pour la traduction automatique neuronale', 'ja': '神経機械翻訳のための効率的な推論', 'zh': '神经机器翻译高效推理', 'hi': 'तंत्रिका मशीन अनुवाद के लिए कुशल अनुमान', 'ru': 'Эффективный вывод для нейронного машинного перевода', 'ga': "Tátal Éifeachtach d'Aistriúchán Meaisín Néarach", 'ka': 'Name', 'hu': 'Hatékony fertőzés a neurális gépi fordításhoz', 'it': 'Inferenza efficiente per la traduzione automatica neurale', 'kk': 'Нейрондық машинаны аудару үшін әсер етілген қасиеттер', 'mk': 'Ефикасна инференција за превод на неврална машина', 'el': 'Αποτελεσματικό συμπέρασμα για τη νευρωνική μηχανική μετάφραση', 'lt': 'Veiksmingas nervinių mašinų vertimas', 'ml': 'നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നതിനുള്ള പ്രയോജനപ്പെടുത്തല്\u200d', 'mt': 'Efficient Inference For Neural Machine Translation', 'ms': 'Inferensi Efisien untuk Terjemahan Mesin Neural', 'mn': 'Сэтгэл машины хөрөнгө оруулалтын үр дүнтэй нөлөөлдөг', 'no': 'Effektiv forskjell for neuralmaskinsomsetjing', 'pl': 'Skuteczne wnioski dla neuronowego tłumaczenia maszynowego', 'ro': 'Inferență eficientă pentru traducerea mașinii neurale', 'so': 'Efficient Inference For Neural machine Translation', 'sr': 'Efektan pogodak za neuronski prevod mašine', 'si': 'න්\u200dයූරල් මැෂින් පරිවර්තනය වෙනුවෙන් ප්\u200dරශ්ණ විශ්වාස කරන්න', 'ur': 'نیورال ماشین ترجمہ کے لئے اثر انفارنس', 'sv': 'Effektiv inferens för neural maskinöversättning', 'ta': 'புதிய இயந்திரத்தின் மொழிபெயர்ப்புக்கான விளைவு', 'uz': 'Tarjima qilish', 'vi': 'Sự liên hệ hiệu quả cho phiên dịch máy thần kinh', 'da': 'Effektiv inferens til neural maskinoversættelse', 'nl': 'Efficiënte Inferentie voor Neurale Machine Translation', 'hr': 'Učinjena šteta za neuronski prevod strojeva', 'bg': 'Ефективно заключение за неврален машинен превод', 'de': 'Effiziente Schlussfolgerung für neuronale maschinelle Übersetzung', 'fa': 'تفاوت فعالی برای ترجمه ماشین عصبی', 'id': 'Inferensi Efisien untuk Translation Mesin Neural', 'ko': '신경 기계 번역 중의 효율적인 추리', 'sq': 'Inferencë Efikase për Translacionin e Makinës Neurale', 'sw': 'Tafsiri yenye ufanisi', 'am': 'Efficient Inference For Neural Machine Translation', 'af': 'Name', 'hy': 'Նյարդային մեքենայի թարգմանման արդյունավետ ինֆերանսը', 'az': 'NĂ¶ral maĹźÄ±na Ă§evirilmÉ™si ĂĽĂ§ĂĽn ehtiyacÄ±', 'bn': 'নিউরাল মেশিন অনুবাদের জন্য কার্যকর ইনফেরেন্স', 'tr': 'Neural Maşynyň terjimesine ýeterlik ýeterlik', 'bs': 'Učinjena šteta za neuronski prevod mašine', 'cs': 'Efektivní závěr pro neuronový strojový překlad', 'ca': 'Inferència eficient per a la traducció de màquines neurones', 'et': 'Tõhus järeldus neuroaalse masintõlke jaoks', 'fi': 'Tehokas pĂ¤Ă¤telmĂ¤ hermojen konekĂ¤Ă¤nnĂ¶kselle', 'jv': 'layer-mode-effects', 'he': 'אינפרנסה יעילה לתרגום מכונת נוירולית', 'sk': 'Učinkovita ugotovitev za strojno prevajanje nevronov', 'ha': '@ action', 'bo': 'ནུས་ཡོད་པའི་མེ་འཁོར་གྱི་ལག་འཁྱེར་ལ་བསྒྱུར་ནུས་ཡོད་པ'}
{'en': 'Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109 % and 84 % speedup on CPU and GPU respectively and reduce the number of parameters by 25 % while maintaining the same translation quality in terms of BLEU.', 'ar': 'حققت نماذج المحولات الكبيرة أحدث النتائج في الترجمة الآلية العصبية وأصبحت قياسية في هذا المجال. في هذا العمل ، نبحث عن مزيج مثالي من الأساليب المعروفة لتحسين سرعة الاستدلال دون التضحية بجودة الترجمة. نجري دراسة تجريبية تجمع بين الأساليب المختلفة وتوضح أن الجمع بين استبدال الاهتمام الذاتي لوحدة فك التشفير بوحدات متكررة مبسطة ، واعتماد مشفر عميق وبنية وحدة فك ترميز ضحلة وتقليم الانتباه متعدد الرؤوس يمكن أن يحقق ما يصل إلى 109٪ و 84٪ تسريع على CPU و GPU على التوالي وتقليل عدد المعلمات بنسبة 25٪ مع الحفاظ على نفس جودة الترجمة من حيث BLEU.', 'pt': 'Grandes modelos de transformadores alcançaram resultados de última geração em tradução automática neural e se tornaram padrão no campo. Neste trabalho, procuramos a combinação ideal de técnicas conhecidas para otimizar a velocidade de inferência sem sacrificar a qualidade da tradução. Conduzimos um estudo empírico que empilha várias abordagens e demonstra que a combinação da substituição da autoatenção do decodificador por unidades recorrentes simplificadas, a adoção de um codificador profundo e uma arquitetura de decodificador rasa e a poda de atenção multi-head podem atingir até 109% e 84% de aceleração em CPU e GPU, respectivamente, e reduzem o número de parâmetros em 25%, mantendo a mesma qualidade de tradução em termos de BLEU.', 'es': 'Los modelos Transformer de gran tamaño han logrado resultados de vanguardia en la traducción automática neuronal y se han convertido en un estándar en el campo. En este trabajo, buscamos la combinación óptima de técnicas conocidas para optimizar la velocidad de inferencia sin sacrificar la calidad de la traducción. Llevamos a cabo un estudio empírico que combina varios enfoques y demuestra que la combinación de reemplazar la autoatención del decodificador con unidades recurrentes simplificadas, la adopción de un codificador profundo y una arquitectura de decodificador poco profunda y la reducción de atención de múltiples cabezales puede lograr una aceleración de hasta el 109% y el 84% en CPU y GPU respectivamente, y reducir el número de parámetros en un 25% manteniendo la misma calidad de traducción en términos de BLEU.', 'fr': "Les modèles de grands transformateurs ont obtenu des résultats de pointe en matière de traduction automatique neuronale et sont devenus la norme dans le domaine. Dans ce travail, nous recherchons la combinaison optimale de techniques connues pour optimiser la vitesse d'inférence sans sacrifier la qualité de la traduction. Nous menons une étude empirique qui empile différentes approches et démontre que la combinaison du remplacement de l'attention personnelle du décodeur par des unités récurrentes simplifiées, l'adoption d'un encodeur profond et d'une architecture de décodeur superficielle et l'élagage de l'attention multi-têtes peut atteindre une accélération allant jusqu'à 109\xa0% et 84\xa0% sur le processeur et le GPU respectivement et de réduire le nombre de paramètres de 25\xa0% tout en maintenant la même qualité de traduction en termes d'UEBL.", 'ja': '大型変圧器モデルは、ニューラル機械翻訳で最先端の結果を達成し、この分野で標準となっています。この研究では、翻訳品質を犠牲にすることなく推論速度を最適化するために、既知の技術の最適な組み合わせを探します。様々なアプローチを積み重ね、デコーダの自己注目を簡略化されたリカレントユニットに置き換え、深いエンコーダと浅いデコーダアーキテクチャを採用し、マルチヘッド注目の枝刈りを行うことで、CPUとGPUでそれぞれ最大109 ％と84 ％のスピードアップを達成し、BLEUの点で同じ翻訳品質を維持しながらパラメータの数を25 ％削減できることを実証する実証研究を行っています。', 'zh': '大Transformer先神经机器翻译,以为率土。 于此之中,求已知之最,以不牺牲译优化推理速度。 一实考之,累累其方,证将解码器自意代为循环单元,用深度编码器浅层解码器架构及多头意修合,可于CPUGPU上各得达109%84%之速,并减参数数25%,兼同BLEU转质。', 'ru': 'Модели крупных трансформаторов достигли самых современных результатов в нейронном машинном переводе и стали стандартными в данной области. В этой работе мы ищем оптимальное сочетание известных методов для оптимизации скорости вывода без ущерба для качества перевода. Мы проводим эмпирическое исследование, которое суммирует различные подходы и демонстрирует, что сочетание замены самовнимания декодера на упрощенные повторяющиеся единицы, принятия глубокого кодера и неглубокой архитектуры декодера и многоголовочной обрезки внимания может достичь до 109% и 84% ускорения на CPU и GPU соответственно и уменьшить количество параметров на 25% при сохранении того же качества трансляции в терминах BLEU.', 'hi': 'बड़े ट्रांसफॉर्मर मॉडल ने तंत्रिका मशीन अनुवाद में अत्याधुनिक परिणाम प्राप्त किए हैं और क्षेत्र में मानक बन गए हैं। इस काम में, हम अनुवाद की गुणवत्ता का त्याग किए बिना अनुमान की गति को अनुकूलित करने के लिए ज्ञात तकनीकों के इष्टतम संयोजन की तलाश करते हैं। हम एक अनुभवजन्य अध्ययन करते हैं जो विभिन्न दृष्टिकोणों को ढेर करता है और यह दर्शाता है कि सरलीकृत आवर्तक इकाइयों के साथ डिकोडर आत्म-ध्यान को बदलने का संयोजन, एक गहरी एन्कोडर और एक उथले डिकोडर आर्किटेक्चर और मल्टी-हेड ध्यान प्रूनिंग को अपनाने से सीपीयू और जीपीयू पर क्रमशः 109% और 84% स्पीडअप प्राप्त हो सकता है और BLEU के संदर्भ में एक ही अनुवाद गुणवत्ता को बनाए रखते हुए पैरामीटर की संख्या को 25% तक कम किया जा सकता है।', 'ga': 'Tá torthaí úrscothacha bainte amach ag samhlacha Trasfhoirmeoirí Móra san aistriúchán meaisín néarach agus tá siad tar éis éirí caighdeánach sa réimse. Sa saothar seo, féachaimid don chomhcheangal is fearr de theicnící aitheanta chun an luas tátail a bharrfheabhsú gan caighdeán an aistriúcháin a íobairt. Déanaimid staidéar eimpíreach a chruann cineálacha cur chuige éagsúla agus a thaispeánann gur féidir suas le 109% agus 84% luas suas le 109% agus 84% a bhaint amach le hionadú féin-aird an díchódóra le haonaid athfhillteacha simplithe, trí ionchódóir domhain agus ailtireacht díchódóra éadomhain a ghlacadh. LAP agus GPU faoi seach agus laghdaítear líon na bparaiméadar faoi 25% agus an caighdeán aistriúcháin céanna á choinneáil i dtéarmaí BLEU.', 'ka': 'დიდი ტრანფორმენტერის მოდელები გავაკეთეთ სიცოცხლის შედეგები ნეიროლური მაქინის გაგრძელებაში და გავაკეთეთ სტანდარტულება. ამ სამუშაოში, ჩვენ ძირებთ უცნობილი ტექნოგიების ოპტიმალური კომბინეცია, რომ ინფრენციის სიჩქარე ოპტიმიზრებად დავიწყებთ, უცნობიერებელი გა ჩვენ ემპერიკალური სწავლობას, რომელიც განსხვავებული მიზეზების შესაძლებლობა დავწყება და გამოჩვენება, რომ სწორედ განსხვავებული განსხვავებული განსხვავებული განსხვავებული ერთეზ შეიძლება გავაკეთოთ დიბოლო კოდერი და დიბოლო კოდერის აქტიქტურაცია და მრავალთან დაახლოების აღმოჩენა 109% და 84% სიჩქარე CPU და GPU-ზე და 25% პარამეტრის რაოდენობას დაახლოებით, როცა BLEU-ის განმავლობაში იგივე გადაწყვეტილების', 'el': 'Τα μεγάλα μοντέλα μετασχηματιστών έχουν επιτύχει αποτελέσματα τελευταίας τεχνολογίας στη νευρωνική μηχανική μετάφραση και έχουν γίνει πρότυπο στον τομέα. Σε αυτή την εργασία, αναζητούμε τον βέλτιστο συνδυασμό γνωστών τεχνικών για τη βελτιστοποίηση της ταχύτητας συμπερασμάτων χωρίς να θυσιάζουμε την ποιότητα της μετάφρασης. Διεξάγουμε μια εμπειρική μελέτη που συσσωρεύει διάφορες προσεγγίσεις και αποδεικνύει ότι ο συνδυασμός αντικατάστασης της αυτοπροσοχής του αποκωδικοποιητή με απλοποιημένες επαναλαμβανόμενες μονάδες, υιοθετώντας έναν βαθύ κωδικοποιητή και μια ρηχή αρχιτεκτονική αποκωδικοποιητή και κλαδέματος προσοχής πολλαπλών κεφαλών μπορούν να επιτύχουν έως 109% και 84% επιτάχυνση στην ΚΜΕ και τη GPU αντίστοιχα και να μειώσουν τον αριθμό των παραμέτρων κατά 25% διατηρώντας την ίδια ποιότητα μετάφρασης όσον αφορά την BLEU.', 'hu': 'A nagy transzformátor modellek korszerű eredményeket értek el a neurális gépi fordításban, és szabványossá váltak a területen. Ebben a munkában az ismert technikák optimális kombinációját keressük, hogy optimalizáljuk a következtetési sebességet anélkül, hogy feláldoznánk a fordítási minőséget. Egy empirikus tanulmányt végzünk, amely különböző megközelítéseket halmoz össze, és bebizonyítja, hogy a dekóder önfigyelem helyettesítése egyszerűsített visszatérő egységekkel, A mélykódoló és a sekély dekódoló architektúra alkalmazása, valamint a többfejű figyelem levágása akár 109%-os, illetve 84%-os gyorsulást érhet el a CPU és a GPU esetében, és 25%-kal csökkentheti a paraméterek számát, miközben ugyanazon fordítási minőséget tart fenn a BLEU tekintetében.', 'lt': 'Dideli transformatoriai pasiekė pažangiausius rezultatus, susijusius su nervinių mašin ų vertimu, ir tapo standartiniais šioje srityje. Šiame darbe siekiame optimalaus žinomų metodų derinio siekiant optimizuoti išvados greitį nepažeidžiant vertimo kokybės. Atliekame empirinį tyrimą, kuriame nustatomi įvairūs metodai ir įrodoma, kad dekoderių savarankiškumo pakeitimas supaprastintais pakartotiniais vienetais, Priėmus gilų kodavimo kodą ir plokščią dekoderių architektūrą bei daugiakalbį dėmesį, galima atitinkamai pasiekti iki 109 % ir 84 % greičio CPU ir GPU ir sumažinti parametrų skaičių 25 %, išlaikant tą pačią vertimo kokybę BLEU atžvilgiu.', 'mk': 'Големите трансформски модели постигнаа најсовремени резултати во преводот на невропските машини и станаа стандардни на теренот. Во оваа работа бараме оптимална комбинација на познати техники за оптимизација на брзината на конференцијата без жртвување на квалитетот на превод. Правиме емпириска студија која собира различни пристапи и демонстрира дека комбинацијата на замена на самото внимание на декодерот со едноставни рецидентни единици, Примената на длабокиот кодер и ниска архитектура на декодерот и привлекувањето на повеќето глави на вниманието може да достигне до 109 отсто и 84 отсто брзина на процесорот и GPU, односно, и да го намали бројот на параметри за 25 отсто, при што ќе се одржи истиот квалитет на превод во поглед на', 'kk': 'Үлкен түрлендіру үлгілері невралдық компьютердің аудармасының күйін жеткізді және өрісте стандартты болды. Бұл жұмыста біз белгілі техникалардың оптималдығын іздейміз, аудармалардың сапасын көмектесу үшін инференциялық жылдамдығын оптимизациялау үшін. Біз әртүрлі жағдайларды топтастырып, декодтардың өзіне қайталанатын бірліктермен ауыстыруын көрсетеді. Тіпті кодерді және көпшілік декодер архитектурасын қолдану және көпшілік басып тұрғысын қолдану мүмкін процессорды және GPU арқылы 109% және 84% жылдамдығына жеткізе алады және бір аудармалы сапатты BLEU арқылы қалаған параметрлердің санын 25% деп аза', 'ms': 'Model Transformer Besar telah mencapai keputusan-state-of-the-art dalam terjemahan mesin saraf dan telah menjadi piawai dalam medan. Dalam kerja ini, kita mencari kombinasi optimal teknik yang diketahui untuk optimize kelajuan kesimpulan tanpa mengorbankan kualiti terjemahan. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Mengadopsi pengekod dalam dan arkitektur pengekod rendah dan pemotongan perhatian berbilang-kepala boleh mencapai hingga 109% dan 84% kecepatan pada CPU dan GPU secara berdasarkan dan mengurangkan bilangan parameter dengan 25% sementara menjaga kualiti terjemahan yang sama dalam terma BLEU.', 'it': "I modelli Large Transformer hanno raggiunto risultati all'avanguardia nella traduzione automatica neurale e sono diventati standard nel campo. In questo lavoro, cerchiamo la combinazione ottimale di tecniche conosciute per ottimizzare la velocità di inferenza senza sacrificare la qualità della traduzione. Conduciamo uno studio empirico che impila vari approcci e dimostra che la combinazione di sostituire l'auto-attenzione decodificatore con unità ricorrenti semplificate, L'adozione di un encoder profondo e di un'architettura decodificatore poco profonda e la potatura dell'attenzione multi-testa può raggiungere una velocità fino al 109% e all'84% rispettivamente su CPU e GPU e ridurre il numero di parametri del 25% mantenendo la stessa qualità di traduzione in termini di BLEU.", 'ml': 'വലിയ ട്രാന്\u200dസ്ഫോര്\u200dമാന്\u200dസ്ഫോര്\u200dമാറ്റര്\u200d മോഡലുകള്\u200d ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷണത്തിന്റെ അവസ്ഥ നേരിട്ടുണ്ട്. പിന്നെ  ഈ ജോലിയില്\u200d, പരിചയപ്പെട്ട ട ടെക്നിക്കങ്ങളുടെ ഐപ്റ്റമില്ലാത്ത ഒരുമിച്ചിട്ടുണ്ടാക്കാന്\u200d ഞങ്ങള്\u200d നോക്കുന്നു,  വ്യത്യസ്ത വഴികള്\u200d സ്ഥാപിക്കുന്നു എന്നിട്ട് സ്വയം ആത്മാര്\u200dത്ഥ്യം മാറ്റുന്നതിനുള്ള സ്വയം ശ്രദ്ധ കൂട്ടുന്നതിനെ കൂട്ടി ഒരു ആഴത്തെ കോഡെര്\u200d എടുക്കുന്നതും ഒരു തണുത്ത ഡെക്കോഡേര്\u200d ആര്\u200dക്കെക്ട്രെക്റ്റിക്കേറ്റര്\u200d ശ്രദ്ധ കാണിക്കുന്നതും സിപിയുവിലും 84% വേഗത്തില്\u200d എത്തുന്നതും ബിലിയുവിന്റെ വിഭാ', 'no': 'Stor transformeringsmodeller har oppnådd tilstanden av kunsten i omsetjinga av neuralmaskina og har blitt standard i feltet. I dette arbeidet ser vi etter optimalt kombinasjon av kjende teknikk for å optimalisera infeksjonsfartet utan å oftasta omsetjingskvalitet. Vi gjer eit empirisk studie som stakkar ulike tilnærmingar og demonstrerer at kombinasjonen av å byta ut dekoder selvmerksomhet med enkelte rekurserte einingar, Eit dyp koder og ein sårba dekoderarkitektur og fleire hovuddekoder kan oppnå opp til 109 % og 84 % raskare på CPU og GPU, og redusere talet på parametrar med 25 % mens det gjeld samme omsetjingskvalitet under BLEU.', 'mt': 'Il-mudelli tat-Transformer il-kbar kisbu riżultati l-aktar avvanzati fit-traduzzjoni tal-magni newrali u saru standard fil-qasam. F’dan ix-xogħol, aħna qed ifittxu l-aħjar kombinazzjoni ta’ tekniki magħrufa biex nimmassimizzaw il-veloċità ta’ inferenza mingħajr ma nisakrifikaw il-kwalità tat-traduzzjoni. Għandna nagħmlu studju empiriku li jimpjega diversi approċċi u juri li l-kombinazzjoni ta’ sostituzzjoni tal-awtonomija tad-dekoder b’unitajiet rikorrenti ssimplifikati, L-adozzjoni ta’ kodifikatur profond u arkitettura ta’ dekoder baxx u pruning ta’ attenzjoni b’ħafna ras jistgħu jilħqu sa 109% u 84% velodup fuq CPU u GPU rispettivament u jnaqqsu n-numru ta’ parametri b’25% filwaqt li jżommu l-istess kwalità ta’ traduzzjoni f’termini ta’ BLEU.', 'mn': 'Том шилжүүлэгч загварууд мэдрэлийн машины хөрөнгө оруулалт болон стандарт болж байна. Энэ ажлын тулд бид мэддэг техникуудын хамтдаа халдварын хурдыг илүү сайжруулахын тулд илүү сайжруулагддаг. Бид өөр өөр арга барилгыг багтаж, өөрийн анхаарлыг хялбарчлан дахин дахин дахин дахин дахин дахин дахин дахин анхаарлаа орлуулж, Гүн гүнзгий кодер болон гүнзгий декодер архитектур болон олон толгой анхаарлын удирдлага нь CPU болон GPU дээр хурдан 109% болон 84% хүртэл хүрэх боломжтой болно. БЛЕУ-ын хувьд адилхан орчуулах чадварыг 25% багасгаж байна.', 'pl': 'Duże modele transformatorów osiągnęły najnowocześniejsze wyniki w neuronowym tłumaczeniu maszynowym i stały się standardem w tej dziedzinie. W niniejszej pracy poszukujemy optymalnego połączenia znanych technik w celu optymalizacji szybkości wnioskowania bez obniżania jakości tłumaczenia. Przeprowadzamy badanie empiryczne, które łączy różne podejścia i pokazuje, że połączenie zastępowania samoobserwacji dekodera uproszczonymi jednostkami powtarzającymi się, Zastosowanie głębokiego kodera i płytkiej architektury dekodera oraz wielogłowicowego przycinania uwagi może osiągnąć do 109% i 84% przyspieszenie procesora i GPU odpowiednio oraz zmniejszyć liczbę parametrów o 25% przy zachowaniu tej samej jakości tłumaczenia pod względem BLEU.', 'ro': 'Modelele de transformare mari au obținut rezultate de ultimă oră în traducerea mașinii neurale și au devenit standard în domeniu. În această lucrare, căutăm combinația optimă de tehnici cunoscute pentru a optimiza viteza inferenței fără a sacrifica calitatea traducerii. Realizăm un studiu empiric care stivuiește diferite abordări și demonstrează că combinația de înlocuire a auto-atenției decodorului cu unități recurente simplificate, Adoptarea unui encoder profund și a unei arhitecturi de decodare superficială și reducerea atenției cu mai multe capete pot atinge o viteză de până la 109% și, respectiv, 84% pe CPU și GPU și reduce numărul de parametri cu 25%, menținând în același timp aceeași calitate a traducerii în ceea ce privește BLEU.', 'so': 'Tusaale wayn oo turjubaal ah waxay gaadheen xaalad-farshaxan, waxayna ka heleen tarjumaadda maskinada neurada ah, waxayna noqdeen standard duurka. Shaqodaas waxaynu raadinnaa iskuulka suurtagalka ah ee loo yaqaan si aan u faa’iideyn karin dhaqdhaqaaq la’aanta tarjumaadda. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Waxyaabaha aad u dheer kartid iyo dhismaha aad u deynta iyo xannaaneynta madaxa kala duduwanba waxay gaadhi karaan ugu badnaan 109% iyo 84% si gaar ah CPU iyo GPU, waxayna hoos u dhigi karaan tirada tirada ah 25% inta ay xajisanayso isku qiimaha tarjumaadka oo ah BLEU.', 'si': 'විශාල ප්\u200dරවර්තනයක් විදිහට ප්\u200dරමාණයක් ලැබුනා න්\u200dයූරල් මැෂින් පරිවර්තනයේ ස්ථිතිය- of- the- art ප්\u200dරතිප මේ වැඩේ අපි හොයාගෙන ඉන්නේ දන්න තාක්ෂණිකාවගේ හොඳම සම්බන්ධයක් හොයාගෙන ඉන්නේ අනුවාර්ථ වේගයක් නැති  අපි ඉම්පිරිකාලික අධ්\u200dයානයක් කරනවා වගේම විවිදියට ප්\u200dරතික්\u200dරියා කරනවා ඒ වගේම ප්\u200dරතික්\u200dරියා කරනවා කියලා සාමාන්\u200dය ව ගොඩක් ඇන්කෝඩර් එකක් සහ ගොඩක් ඩිකෝඩර් ස්ථාපනයක් සහ ගොඩක් හෙඩක් අවධානයක් ඉන්න පුළුවන් CPU සහ GPU වලින් ඉක්මනට 109% සහ 84% ඉක්මනට සම්පූර්ණයෙන් ඉන්න සහ 25%', 'ta': 'பெரிய மாற்று மாற்றும் மாதிரிகள் நிலைமை- கலை மாற்றியமைத்து புதிய இயந்திரம் மொழிமாற்றி புலத்தில் இயல்பான மாத இந்த வேலையில், நாம் தெரியும் தொழில்நுட்பத்தின் வேகத்தை அதிகப்படுத்த வேண்டும் என்று தேடுகிறோம் மொழிபெயர்ப் நாம் சுலபமான திரும்ப அலகுகளை மாற்றும் குறியீட்டு தன்னுடைய கவனத்தை மாற்றும் குறியீட்டை மாற்றும் சுலபமான திரும்ப அலக ஒரு ஆழமான குறியீட்டு மற்றும் ஒரு மழுமையான குறியீட்டாளர் அமைப்பு மற்றும் பல தலைப்பு கவனத்தை புரிந்து பிபியு மற்றும் GPU மற்றும் 84% வேகத்தை பெற முடியும் மற்றும் பிலியூவி', 'sr': 'Veliki modeli transformera postigli su rezultate umetnosti u prevodu neuralne mašine i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Vodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevođenja u smislu BLEU-a.', 'sv': 'Stora Transformermodeller har uppnått toppmoderna resultat inom neural maskinöversättning och har blivit standard inom området. I detta arbete letar vi efter den optimala kombinationen av kända tekniker för att optimera inferenshastigheten utan att offra översättningskvaliteten. Vi genomför en empirisk studie som staplar olika tillvägagångssätt och visar att kombinationen av att ersätta avkodare självuppmärksamhet med förenklade återkommande enheter, Genom att använda en djup kodare och en ytlig avkodningsarkitektur och flerskalig uppmärksamhet kan man uppnå upp till 109% respektive 84% snabbare på CPU respektive GPU och minska antalet parametrar med 25% samtidigt som man bibehåller samma översättningskvalitet när det gäller BLEU.', 'ur': 'بڑے ترنسفورر نمڈلوں نے نئورل ماشین ترجمہ کے نتیجے پہنچ گئے ہیں اور کھیل میں استاندارڈ ہوگئے ہیں. اس کام میں، ہم جانے والی تکنیک کی اچھی ترکیب کے لئے تلاش کرتے ہیں کہ اس کے ذریعہ مہربانی کی سرعت مہربانی کریں بغیر ترکیب کی کیفیت کے۔ ہم ایک مصریح تحقیق کرتے ہیں جو مختلف طریقوں کو ٹکڑے رکھتا ہے اور دکھاتے ہیں کہ دکور کی اپنا توجه سادھا دوبارہ واحدوں سے بدل دینے کی ترکیب ہے، سی پی یو اور جی پی یو پر چڑھا ہوا تھا اور ایک گہرے ڈیکوڈر معماری اور بہت سی سروں کی توجه پرینگ کے ذریعے سی پی یو اور جی پی یو پر چڑھا ہوا تھا اور اس کی تعداد 25% کے ذریعے کم کر سکتا ہے جب کہ بلیو کے اندر ایک ہی ترجمہ کیفیت کی حفاظت کرتی ہے۔', 'uz': "Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field.  Bu vazifanda, biz tanlangan teknikalarning optimal birlashtirishni istaysizmi, tarjima sifatini tozalash mumkin. Biz bir tashkilotni bajaramiz, har xil usullarni qo'yish va o'sha narsalarni o'zgartirish qo'shimcha o'zimni o'zgartirish bilan soddalashtirish uchun o'zgartirish mumkin, AQSH", 'vi': "Các mô hình biến hình lớn đã đạt được kết quả tối tân trong việc dịch chuyển cỗ máy thần kinh và trở thành tiêu chuẩn trên chiến trường. Trong công việc này, chúng tôi tìm kiếm sự kết hợp tối ưu tiên của kỹ thuật được biết để tối ưu tiên tốc độ nhận biết mà không hy sinh chất lượng dịch. Chúng tôi thực hiện một nghiên cứu có kinh nghiệm mang theo nhiều phương pháp khác nhau và chứng minh rằng kết hợp của việc thay thế bộ lọc bằng đơn vị thường xuyên, Một máy mã hóa sâu và một cấu trúc giải mã nông cạn và việc cắt giảm tập trung đa đầu có thể tăng tốc lên đến 109=và84=.='trên hai bộ vi xử lý cộng cộng đồng và giảm bớt số lượng các tham số bằng 25t. Trong khi vẫn giữ nguyên chất dịch bản phân loại tương tự.", 'da': 'Store Transformermodeller har opnået state-of-the-art resultater inden for neural maskinoversættelse og er blevet standard på området. I dette arbejde leder vi efter den optimale kombination af kendte teknikker til at optimere inferencehastigheden uden at gå på kompromis med oversættelseskvaliteten. Vi gennemfører en empirisk undersøgelse, der stabler forskellige tilgange og demonstrerer, at kombinationen af at erstatte dekoder selvopmærksomhed med forenklede tilbagevendende enheder, Vedtagelse af en dyb encoder og en lavvandet dekoderkarkitektur og opmærksomhedsbeskæring med flere hoveder kan opnå op til 109% og 84% hastighed på henholdsvis CPU og GPU og reducere antallet af parametre med 25% samtidig med at den samme oversættelseskvalitet med hensyn til BLEU opretholdes.', 'de': 'Große Transformatormodelle haben modernste Ergebnisse in der neuronalen maschinellen Übersetzung erzielt und sind in diesem Bereich Standard geworden. In dieser Arbeit suchen wir nach der optimalen Kombination bekannter Techniken, um die Inferenzgeschwindigkeit zu optimieren, ohne die Übersetzungsqualität zu beeinträchtigen. Wir führen eine empirische Studie durch, die verschiedene Ansätze stapelt und zeigt, dass die Kombination aus dem Ersetzen der Selbstaufmerksamkeit des Decoders durch vereinfachte wiederkehrende Einheiten, Durch die Verwendung eines tiefen Encoders und einer flachen Decoderarchitektur und des Aufmerksamkeitsschnitts mit mehreren Köpfen kann eine Beschleunigung von bis zu 109% bzw. 84% auf CPU und GPU erreicht und die Anzahl der Parameter um 25% reduziert werden, während die Übersetzungsqualität in Bezug auf BLEU beibehalten wird.', 'bg': 'Големите трансформаторни модели са постигнали най-съвременни резултати в невронния машинен превод и са станали стандарт в областта. В тази работа търсим оптималната комбинация от известни техники за оптимизиране на скоростта на заключение, без да жертваме качеството на превода. Извършваме емпирично проучване, което подрежда различни подходи и демонстрира, че комбинацията от заместване на декодерното самовнимание с опростени повтарящи се единици, приемането на дълбок кодер и повърхностна декодерна архитектура и подрязването на вниманието с няколко глави може да постигне до 109% и 84% ускорение съответно на процесора и графичния процесор и да намали броя на параметрите с 25%, като същевременно поддържа същото качество на превода по отношение на Блеу.', 'id': 'Model Transformer Besar telah mencapai hasil terbaik dalam terjemahan mesin saraf dan telah menjadi standar di lapangan. Dalam pekerjaan ini, kita mencari kombinasi optimal dari teknik yang dikenal untuk optimisasi kecepatan inferensi tanpa mengorbankan kualitas terjemahan. Kami melakukan sebuah studi empiris yang mengumpulkan berbagai pendekatan dan menunjukkan bahwa kombinasi menggantikan perhatian diri dekoder dengan unit rekuren sederhana, Mengadopsi koder dalam dan arsitektur dekoder rendah dan pemotongan perhatian multi-kepala dapat mencapai sampai 109% dan 84% speedup pada CPU dan GPU secara respektif dan mengurangi jumlah parameter dengan 25% sementara mempertahankan kualitas terjemahan yang sama dalam terma BLEU.', 'nl': 'Grote Transformatormodellen hebben state-of-the-art resultaten behaald in neurale machinevertaling en zijn standaard in het veld geworden. In dit werk zoeken we naar de optimale combinatie van bekende technieken om de inferentiesnelheid te optimaliseren zonder afbreuk te doen aan de vertaalkwaliteit. We voeren een empirische studie uit die verschillende benaderingen stapelt en aantoont dat de combinatie van het vervangen van decoder zelfaandacht door vereenvoudigde terugkerende eenheden, Door gebruik te maken van een diepe encoder en een ondiepe decoderarchitectuur en multi-head attentie snoeien kan een versnelling tot 109% en 84% van respectievelijk CPU en GPU worden bereikt en het aantal parameters met 25% worden verminderd terwijl dezelfde vertaalkwaliteit in termen van BLEU wordt gehandhaafd.', 'hr': 'Veliki modeli transformera postigli su rezultate umjetnosti u prevodu neuralnih strojeva i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Provodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevoda u smislu BLEU-a.', 'sw': 'Mradi mkubwa wa Tafsiri umefanikiwa kuwa na hali ya sanaa na matokeo ya utafsiri wa mashine ya kisasa na yamekuwa kiwango cha kawaida katika uwanja. Katika kazi hii, tunatafuta muunganiko bora wa mbinu zinazofahamika kuboresha kiwango cha uchunguzi bila kutoa sifa za tafsiri. Tunafanya utafiti wa msisitizo ambao unaweka hatua mbalimbali na kuonyesha kuwa muunganiko wa kubadilisha nafuu kwa vitengo rahisi vya kurudi, Kwa kuchukua kiwango cha ndani na ujenzi mdogo wa decodi na uchunguzi wa kichwa vingi unaweza kufikia asilimia 109 na asilimia 84 kwa kiwango cha CPU na GPU na kupunguza idadi ya parameter kwa asilimia 25 wakati wakiendelea kutangaza kiwango hicho cha tafsiri kwa mujibu wa BLEU.', 'af': "Name In hierdie werk, ons soek na die optimale kombinasie van bekende teknike om inferensie spoed te optimaliseer sonder om vertaling kwaliteit te offer. Ons doen 'n empiriese studie wat verskeie toegange stap en wys dat kombinasie van vervanging van dekoder self-aandag met eenvoudige herhaalde eenhede, Die aanvaar van 'n diep enkoder en 'n skaal dekoder-arkitektuur en meer-kop aanmerking kan tot 109% en 84% speeduig op CPU en GPU aanvaar en die nommer van parameters met 25% verduur terwyl die selfde vertaling-kwaliteit in terms van BLEU onderhou.", 'tr': 'Ullakan Transformer nusgalary neural maşynyň terjimesinde ýetip bardylar we sahypada standart boldy. Bu işde, biz bilinen teknikleriň optimal birleşigini terjime etmek üçin azalyş ýigrimizi bejermek üçin gözleýäris. Biz empirik bir aralygy ýerine ýetirýäris we munuň birnäçe nusgalaryny ýerleşdirýändigini görkezýäris. Garyp ködleme we çukultyk arhitektura we köp kelläp üns berişi 109% we 84% CPU we GPU-a süýtgelip biler we ayn terjime howpsaly BLEU-a garaşyp biler parameterleriň sanyny 25% tarapyndan azaltyp biler.', 'fa': 'مدل\u200cهای تغییر\u200cپذیر بزرگ به حالت هنر نتیجه\u200cهای تغییر\u200cپذیر ماشین عصبی رسیده\u200cاند و در زمینه استاندارد شده\u200cاند. در این کار، ما دنبال ترکیب بهترین تکنیک\u200cهای شناخته می\u200cگردیم تا سرعت آلودگی را بدون قربانی کیفیت ترکیب بهترین کنیم. ما یک مطالعه امپراتیک را انجام می دهیم که تقریبا مختلف را جمع می کند و نشان می دهد که ترکیب توجه خود را با واحدهای ساده تکرار می دهد، با پذیرفتن یک کودهر عمیق و یک معماری دکوردر عمیق و حفظ توجه بسیاری از سرها می تواند تا 109 درصد و 84 درصد سرعت بر CPU و GPU را به طور مستقل رسید و تعداد پارامتر را به 25 درصد کاهش دهد در حالی که با حفظ یک کیفیت ترجمه را به عنوان BLEU نگه می دارد.', 'ko': '대형 변압기 모형은 신경기계 번역 분야에서 가장 선진적인 성과를 거두었고 이미 이 분야의 표준이 되었다.이 작업에서 우리는 이미 알고 있는 기술의 가장 좋은 조합을 찾아 추리 속도를 최적화하는 동시에 번역의 질을 희생하지 않는다.우리는 실증 연구를 실시하여 각종 방법을 총결하고 간소화된 중복 단원으로 자신의 주의를 대체하는 것을 증명했다.깊이 인코더와 얕은 디코더 구조, 다중 주의 가지치기로 각각 CPU와 GPU에서 109%와 84%의 가속을 실현하고 파라미터 수량을 25% 줄이는 동시에 BLEU에서 같은 번역 품질을 유지할 수 있다.', 'az': 'Büyük Transformer modelləri nöral maşına çevirilməsi ilə mümkün olduğu və sahədə standart oldular. Bu işdə, biz bilinmiş tekniklərin optimal kombinatsiyasını istəyirik ki, dəyişiklik sürətini qurbanlıq etmədən optimizləsin. Biz müxtəlif yaxınlıqları birləşdirən empirik təhsil etdik və dekoderin özünü təhsil etməsini basit təhsil edilən biriklərlə dəyişdiririk. Dərzini kodlayıcı və çətinli dekoder arhitektarını və çoxlu başlıqların gözləməsi CPU və GPU ilə müqayisədə 109%-ə və 84%-ə hızlandıra bilər və aynı tercümə keyfiyyətini BLEU ilə qoruyarkən parametru sayını 25%-ə əskilə bilər.', 'sq': 'Modelet e mëdha të Transformës kanë arritur rezultate më të larta në përkthimin e makinave nervore dhe janë bërë standarde në fushë. Në këtë punë, ne kërkojmë kombinimin optimal të teknikave të njohura për të optimizuar shpejtësinë e përfundimit pa sakrifikuar cilësinë e përkthimit. Ne kryejmë një studim empirik që grumbullon metoda të ndryshme dhe demonstron se kombinimi i zëvendësimit të vetëvëmendjes së dekoderit me njësitë e thjeshta të përsëritura, - miratimi i një kodifikuesi të thellë dhe një arkitekture dekoderi të thellë dhe shtrëngimi i vëmendjes me shumë koka mund të arrijë deri në 109% dhe 84% shpejtësi respektivisht në CPU dhe GPU dhe të reduktojë numrin e parametrave me 25% duke mbajtur të njëjtën cilësi përkthimi në termat e BLEU.', 'bn': 'Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field.  এই কাজে আমরা পরিচিত প্রযুক্তির সম্মিলনের অপেক্ষা করছি অনুবাদের মান ছাড়া ইনভেন্সের গতি বৃদ্ধি করার জন্য। আমরা একটি সম্মানিত গবেষণা করি যা বিভিন্ন উপায় স্থাপন করে এবং প্রদর্শন করে যে সুস্পষ্ট পুনরাবর্তন ইউনিটের সাথে নিজেকে আত্মমনোযোগ প্রতিস একটি গভীর এনকোডার গ্রহণ করা এবং একটি ধুলো কোডার আর্কিডেক্টার এবং বহুমাথার মনোযোগ প্রদান করা যায়, সিপিউ এবং জিপিউ-তে প্রায় ১০৯% এবং ৮৪% বেড়ে যাবে এবং বিলিউ এর মাধ্যমে একই অনুবাদের মান', 'am': 'ትልቅ ትርጉም ሚድልቶች የ-የ-አርእስት ግንኙነትን አግኝተዋል እና በሜዳው የተመሳሳይ ሆኖአል፡፡ በዚህ ስራ፣ የተታወቀ ስህተት ማቀናጃ ጥቅም ሳይያሳርፍ ፍጥረትን ማሻሻል እናስፈልጋለን፡፡ የተለየ ልዩ ልቦችን የሚቆርጥ እና የድምፅ አካባቢ ተቃውሞ የሚለውጥ የራሱን ትኩረት በመለስ እናሳያልን፡፡ የጥልቅ ኮድ እና የጥቁር አካባቢ መሠረት እና የብዙራዊ አካባቢ ጉዳይ ጉዳይ በCPU እና GPU ላይ አቅራቢያ 109 በመቶ እና 84 በመቶው ይደርሳል፡፡', 'hy': 'Մեծ տրանֆերմերների մոդելները հասել են նորագույն արդյունքներին նյարդային մեքենայի թարգմանման մեջ և դառնում են դաշտում ստանդարտ: Այս աշխատանքում մենք փնտրում ենք հայտնի մեթոդների օպտիմալ համադրումը, որպեսզի օպտիմացվի եզրակացության արագությունը առանց թարգմանման որակի զոհաբերելու: We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Խելամիտ կոդերի, մակերեսային կոդերի ճարտարապետության և բազմագլխավոր ուշադրության կրճատման ընդունելը կարող է հասնել մինչև 109 և 84 տոկոսի արագություն համակարգչային համակարգի և GPU-ի վրա և նվազեցնել պարամետրերի թիվը 25 տոկոսով, մինչդեռ պահպանել նույն թարգմանման որակ', 'bs': 'Veliki modeli transformera postigli su rezultate umjetnosti u prevodu neuralnih strojeva i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Provodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevoda u smislu BLEU-a.', 'ca': "Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field.  En aquest treball busquem la combinació optima de tècniques conegudes per optimitzar la velocitat de inferència sense sacrificar la qualitat de traducció. Realitzem un estudi empíric que agrupa diversos enfocaments i demostra que la combinació de substituir l'autoatenció del decodificador per unitats recurrents simplificades, L'adopció d'un codificador profund i d'una arquitectura de decodificació superficial i una pruning d'atenció multicapa poden aconseguir fins al 109% i l'84% de velocitat en CPU i GPU respectivament i reduir el nombre de paràmetres un 25% mantenint la mateixa qualitat de traducció en termes de BLEU.", 'cs': 'Modely velkých transformátorů dosáhly nejmodernějších výsledků v oblasti neuronového strojového překladu a staly se standardem v oboru. V této práci hledáme optimální kombinaci známých technik pro optimalizaci rychlosti inference bez snížení kvality překladu. Provádíme empirickou studii, která shromažďuje různé přístupy a demonstruje, že kombinace nahrazení sebepozornosti dekodéru zjednodušenými opakujícími se jednotkami, Přijetím hlubokého snímače a mělké architektury dekodéru a vícehlavového prořezávání pozornosti lze dosáhnout až 109% a 84% urychlení procesoru a GPU a snížit počet parametrů o 25% při zachování stejné kvality překladu z hlediska BLEU.', 'et': 'Suurte transformaatorite mudelid on saavutanud tipptasemel tulemusi neuromasintõlkes ja muutunud valdkonnas standardiks. Selles töös otsime teadaolevate tehnikate optimaalset kombinatsiooni järelduste kiiruse optimeerimiseks ilma tõlkekvaliteeti ohverdamata. Viime läbi empiirilise uuringu, mis koondab erinevaid lähenemisviise ja näitab, et dekooderi enesetähelepanu asendamise kombinatsioon lihtsustatud korduvate üksustega, Sügava kodeerija ja madala dekooderi arhitektuuri kasutuselevõtmine ning mitme peaga tähelepanu vähendamine võib saavutada protsessori ja graafikaprotsessori kiiruse vastavalt 109% ja 84% ning vähendada parameetrite arvu 25% võrra, säilitades samal ajal BLEU-s sama tõlkekvaliteedi.', 'fi': 'Suuret muuntajamallit ovat saavuttaneet huippuluokan tuloksia neurokonekäännöksessä ja niistä on tullut alan standardi. Tässä työssä etsimme tunnettujen tekniikoiden optimaalista yhdistelmää päättelynopeuden optimoimiseksi kääntämisen laadusta tinkimättä. Teemme empiirisen tutkimuksen, joka pinoaa erilaisia lähestymistapoja ja osoittaa, että yhdistelmä dekooderin itsetunnon korvaaminen yksinkertaistetuilla toistuvilla yksiköillä, Syväkooderin ja matalan dekooderiarkkitehtuurin käyttöönotto ja monipäinen huomioleikkaus voivat nopeuttaa suoritinta 109% ja grafiikkasuoritinta 84% ja vähentää parametrien määrää 25% säilyttäen samalla BLEU:n käännöslaadun.', 'jv': 'string" in "context_BAR_stringLink Nang barêng-barêng iki, kita sampeyan kanggo ngerasakno ampliwat karo teknik sing berarti ujian kanggo ngerasakno luwih apik, lan akeh nyong ngerasakno. Awak dhéwé éntuk éntuk empir sing nggawe barang nggawe gerakan sampeyan karo ngono nggambar perusahaan winih dhéwé kuwi nggawe gerakan kelas perusahaan karo perusahaan sugih deep', 'sk': 'Modeli velikih transformatorjev so dosegli najsodobnejše rezultate v nevronskem strojnem prevajanju in postali standardni na tem področju. V tem delu iščemo optimalno kombinacijo znanih tehnik za optimizacijo hitrosti sklepanja brez žrtvovanja kakovosti prevajanja. Izvedli smo empirično študijo, ki zloži različne pristope in dokazuje, da kombinacija zamenjave samopozornosti dekoderja s poenostavljenimi ponavljajočimi enotami, S sprejetjem globokega kodirnika in plitve arhitekture dekodirnika ter večglavnega obrezovanja pozornosti lahko dosežete do 109% oziroma 84% pospešitev na CPU oziroma GPU ter zmanjšate število parametrov za 25%, hkrati pa ohranjate enako kakovost prevajanja v smislu BLEU.', 'ha': "@ action: button Daga wannan aikin, munã dãkin komai mai amfani da shiryoyin ayuka da aka sani wajen kwamfyuta saukarwa na kasancẽwa idan ba da tsarin fassarar ta ba. Tuna sami wani littãfi na tamkar da ke samun hanyõyi dabam-daban kuma ke nuna cewa da za'a bada sauri-rayi kanaga-raye da sunayen da aka sauce, @ info: whatsthis", 'he': 'דוגמנים גדולים של טרנספורר השיגו תוצאות חדשות בתרגום מכונות עצביות והפכו לסטנדרטים בשטח. בעבודה הזו, אנו מחפשים שילוב אופטימלי של טכניקות ידועות כדי לאופטימיזם מהירות המסקנה בלי להקריב איכות התרגום. אנו מבצעים מחקר אמפירי שמעריך גישות שונות ומוכיח שילוב של החלפת תשומת לב עצמית של מפענח עם יחידות חדשות פשוטות, באמצעות קודד עמוק וארכיטקטורת קודד גבוהה ומעטפת תשומת לב רב-ראשית יכולה להשיג עד 109% ו-84% מהירות על CPU ו-GPU בהתאם ולפחות את מספר הפרמטרים ב-25% בזמן לשמור על אותו איכות התרגום במונחים של BLEU.', 'bo': 'རྩིས་པ་ཆེ་བའི་དབྱིབས་བཟོ་བྱེད་མ་དབྱིབས་སྣང་བའི་གནས་སྟངས་དང་མཐུན་རྐྱེན་གྱིས འོན་ཀྱང་། ང་ཚོས་གནས་ཚུལ་འདིའི་ནང་དུ་ཆེས་ཤུགས་ཀྱི་མཉམ་དུ་མཐུན་རྐྱེན་ཚད་མེད་སྤྲོད་ཀྱི་མཚམས་མཐུན་དང་། ང་ཚོས་རང་ཉིད་ཀྱི་གནད་དོན་དག་གི་ཐབས་ལམ་མ་འདྲ་བརྩལ་བ་ཞིག་བྱེད་ཀྱི་རྩོལ་ཞིག་བྱས་ནས། - adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.'}
{'en': 'Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm', 'ar': 'تحسينات متفرقة للتلخيص الاستخراجي غير الخاضع للرقابة للوثائق الطويلة باستخدام خوارزمية فرانك وولف', 'es': 'Optimización dispersa para la sumarización extractiva no supervisada de documentos largos con el algoritmo de Frank-Wolfe', 'pt': 'Otimização esparsa para sumarização extrativa não supervisionada de documentos longos com o algoritmo de Frank-Wolfe', 'fr': "Optimisation fragmentée pour la synthèse extractive non supervisée de documents longs avec l'algorithme de Frank-Wolfe", 'zh': '用 Frank-Wolfe 算法对长文档无督提汇总疏优化', 'ja': 'フランク＝ウォルフ・アルゴリズムによる長い文書の非監視抽出要約のためのまばらな最適化', 'hi': 'फ्रैंक-वोल्फ एल्गोरिथ्म के साथ लंबे दस्तावेज़ों के असुरक्षित निष्कर्षण सारांशीकरण के लिए विरल अनुकूलन', 'ru': 'Резкая оптимизация для неконтролируемого извлечения длинных документов с помощью алгоритма Франка-Вульфа', 'ga': 'Optamú Gann le haghaidh Achoimriú Eastóscach Gan Mhaoirseacht ar Dhoiciméid Fada leis an Algartam Frank-Wolfe', 'ka': 'ფანკ- გულტ ალგორიტიმმა განსხვავებული ექსტრაქტური კომპანიზაციაზე განსხვავებული განსხვავებული კომპანიზაცია', 'el': 'Σπάνια βελτιστοποίηση για μη επιτηρημένη εκχυλιστική περίληψη μακρών εγγράφων με τον αλγόριθμο Φρανκ-Γουλφ', 'hu': 'Ritka optimalizálás a hosszú dokumentumok felügyeletlen extraktív összefoglalásához Frank-Wolfe algoritmussal', 'it': "Ottimizzazione sparsa per una sintesi estrattiva non supervisionata di documenti lunghi con l'algoritmo Frank-Wolfe", 'kk': 'Франк- Волф алгоритмімен ұзын құжаттардың шығарылмаған шығарылмаған тұжырымдамасының аралық оптимизациясы', 'lt': 'Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm', 'mk': 'Name', 'ml': 'പരിശോധിക്കാത്ത പുറത്താക്കുവാന്\u200d സാധ്യതയുടെ ഐച്ഛികങ്ങള്\u200d', 'ms': 'Optimisasi Tinggi untuk Penapisan Ekstraktif Tidak Disupervisi Dokumen Panjang dengan Algoritma Frank-Wolfe', 'pl': 'Oszczędna optymalizacja dla nienadzorowanej ekstrakcyjnej podsumowania długich dokumentów za pomocą algorytmu Frank-Wolfe', 'mn': 'Франк-Волф алгоритмын хамт урт баримтуудын нэмэгдүүлэлтийн нэмэгдүүлэлт', 'ro': 'Optimizare spartă pentru rezumarea extractivă nesupravegheată a documentelor lungi cu algoritmul Frank-Wolfe', 'mt': 'Optimizzazzjoni ħafifa għas-Sommarju Estrattiv Mhux Sorveljat tad-Dokumenti twal mal-Algoritmu Frank-Wolfe', 'so': 'Kursor Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm', 'no': 'Optimalisering for ulike ekstraktiv samandrag av lange dokument med Frank-Wolfe-algoritmen', 'si': 'ප්\u200dරෑන්ක්-වෝල්ෆ් ඇල්ගෝරිතම් එක්ක ලොකු ලොකු ලොකු දස්තුවන්ගේ ප්\u200dරතික්\u200dරියාත්මක සංස්කරණය සඳහ', 'sv': 'Spar optimering för oövervakad extraktiv sammanfattning av långa dokument med Frank-Wolfe algoritmen', 'ta': 'கண்காணிக்கப்படாத வெளியேற்ற சுருக்கம்', 'ur': 'فرانک وولف الگوریٹم کے ساتھ لانگ دفتروں کی غیر قابل تحقیقات کے لئے اسپارس اپنا غیر قابل تحقیقات', 'sr': 'Sparse Optimizacija za neodređenu ekstraktivnu sažetku dugih dokumenta sa algoritmom Frank-Wolfe', 'uz': 'Name', 'vi': 'Ảnh hưởng thô cho Pháp thủ chưa được đóng cấp chính xác Long với sóc tượng Frank-Wolfe', 'nl': 'Sparse optimalisatie voor niet-gecontroleerde extractieve samenvatting van lange documenten met het Frank-Wolfe algoritme', 'da': 'Sparsom optimering til ukontrolleret ekstraktiv opsummering af lange dokumenter med Frank-Wolfe algoritmen', 'hr': 'Optimizacija prostora za neodržavanu ekstraktivnu sažetku dugih dokumenta s algoritmom Frank-Wolfe', 'bg': 'Спешна оптимизация за необуздано екстрактивно обобщаване на дълги документи с алгоритъма Франк-Улф', 'id': 'Optimisasi Tinggi untuk Penapisan Ekstraktif Tak Tersupervisi Dokumen Panjang dengan Algoritma Frank-Wolfe', 'sw': 'Uchaguzi wa Uhispania kwa Ujumbe wa Makala ya Nchini ya Utawala na Algorithi ya Frank-Wolfe', 'ko': 'Frank-Wolfe 알고리즘 기반의 무감독장 문서 요약 추출 희소 최적화', 'de': 'Sparse Optimierung zur unbeaufsichtigten extraktiven Zusammenfassung langer Dokumente mit dem Frank-Wolfe Algorithmus', 'tr': 'Frank-Wolfe Algoritmi bilen gaýd edilmedik Çikgi Senedleriň Iňleşik Opşenleri', 'sq': 'Optimizimi i shpejtë për përmbledhjen e jashtëzakonshme të dokumenteve të gjata me algoritmin Frank-Wolfe', 'fa': 'Optimization Sparse for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm', 'af': 'Sparse Optimalisasie vir Ononderwerpende Extraktiewe Opsomming van Lange Dokumente met die Frank- Wolfe Algoritme', 'az': 'Frank-Wolfe Algoritimi ilə Uzun Belgələrin Əlaqətləşdirilmiş Əlaqətləşdirilməsi üçün Sparse Optimization', 'am': 'ምርጫዎች', 'bn': 'ফ্র্যাঙ্ক- উলফে অ্যালগরিদমের সাথে দীর্ঘ নথিপত্রের সংক্রান্ত সংক্রান্ত সংক্ষেপের জন্য স্প্যারের অপশনিমেশন', 'ca': 'Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm', 'bs': 'Sparse Optimizacija za neodređenu ekstraktivnu sažetku dugih dokumenta sa algoritmom Frank-Wolfe', 'et': 'Haruldane optimeerimine pikkade dokumentide järelevalveta ekstraktiivseks kokkuvõtmiseks Frank-Wolfe algoritmi abil', 'fi': 'Sparse Optimointi pitkien asiakirjojen valvomattomaan ekstraktiiviseen yhteenvetoon Frank-Wolfe-algoritmilla', 'hy': 'Ալգորիթմ Ֆրանկ-Ուլֆ', 'cs': 'Řídká optimalizace pro nekontrolovanou extraktivní shrnutí dlouhých dokumentů pomocí Frank-Wolfeho algoritmu', 'jv': 'Sparsi Optimisation kanggo Keterangan Gak Bewasirno Atisasi Layar Dokumen nganggo French-wolf Algorithm', 'ha': 'KCharselect unicode block name', 'he': 'אופטימיזציה מיוחדת לסומריזציה אקסטרקטיבית ללא השגחה של מסמכים ארוכים עם אלגוריתם פרנק-וולף', 'sk': 'Redka optimizacija za nenadzorovano ekstraktivno povzemanje dolgih dokumentov z algoritmom Frank-Wolfe', 'bo': 'Frank-Wolfe སྤྱོད་མཁན་གྱི་རྒྱ་ཆེ་བ་ཡིག་ཆའི་ཁྱད་དུ་འཕགས་རིས་ཀྱི་བཅུད་སྡུད་རྗེས་ཐོག་སྟོན་པ'}
{'en': 'We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.', 'ar': 'نعالج مشكلة تلخيص المستندات الاستخراجية غير الخاضع للرقابة ، خاصة بالنسبة للوثائق الطويلة. قمنا بنمذجة المشكلة غير الخاضعة للإشراف باعتبارها مشكلة انحدار تلقائي متفرقة ونقارب المشكلة الاندماجية الناتجة عبر مشكلة محدبة مقيدة بالمعايير. نقوم بحلها باستخدام خوارزمية فرانك وولف مخصصة. لإنشاء ملخص بجمل k ، تحتاج الخوارزمية فقط إلى تنفيذ تكرارات k تقريبًا ، مما يجعلها فعالة للغاية لمستند طويل. نقوم بتقييم نهجنا مقابل طريقتين أخريين غير خاضعين للرقابة باستخدام كل من درجات ROUGE المعجمية (القياسية) ، وكذلك الدلالات (القائمة على التضمين). تحقق طريقتنا نتائج أفضل مع مجموعتي البيانات وتعمل بشكل جيد بشكل خاص عند دمجها مع حفلات الزفاف للحصول على ملخصات معاد صياغتها بشكل كبير.', 'pt': 'Abordamos o problema da sumarização de documentos extrativos não supervisionados, especialmente para documentos longos. Modelamos o problema não supervisionado como um problema de auto-regressão esparsa e aproximamos o problema combinatório resultante por meio de um problema convexo e restrito por normas. Resolvemos isso usando um algoritmo dedicado de Frank-Wolfe. Para gerar um resumo com k sentenças, o algoritmo precisa apenas executar aproximadamente k iterações, tornando-o muito eficiente para um documento longo. Avaliamos nossa abordagem em relação a dois outros métodos não supervisionados usando pontuações ROUGE lexicais (padrão) e semânticas (baseadas em incorporação). Nosso método alcança melhores resultados com ambos os conjuntos de dados e funciona especialmente bem quando combinado com embeddings para resumos altamente parafraseados.', 'es': 'Abordamos el problema de la sumarización extractiva de documentos sin supervisión, especialmente para documentos largos. Modelamos el problema no supervisado como un problema de autorregresión escasa y aproximamos el problema combinatorio resultante a través de un problema convexo y restringido por normas. Lo resolvemos con un algoritmo Frank-Wolfe dedicado. Para generar un resumen con k oraciones, el algoritmo solo necesita ejecutar aproximadamente k iteraciones, lo que lo hace muy eficiente para un documento largo. Evaluamos nuestro enfoque en comparación con otros dos métodos no supervisados que utilizan puntuaciones ROUGE léxicas (estándar) y semánticas (basadas en incrustación). Nuestro método logra mejores resultados con ambos conjuntos de datos y funciona especialmente bien cuando se combina con incrustaciones para resúmenes altamente parafraseados.', 'fr': "Nous abordons le problème de la synthèse non supervisée des documents extractifs, en particulier pour les documents longs. Nous modélisons le problème non supervisé comme un problème d'auto-régression clairsemée et nous approximons le problème combinatoire résultant via un problème convexe contraint par la norme. Nous le résolvons à l'aide d'un algorithme de Frank-Wolfe dédié. Pour générer un résumé de k phrases, l'algorithme n'a besoin que d'exécuter environ k itérations, ce qui le rend très efficace pour un document long. Nous évaluons notre approche par rapport à deux autres méthodes non supervisées utilisant à la fois des scores ROUGE lexicaux (standard) et des scores sémantiques (basés sur l'intégration). Notre méthode permet d'obtenir de meilleurs résultats avec les deux ensembles de données et fonctionne particulièrement bien lorsqu'elle est combinée avec des intégrations pour des résumés hautement paraphrasés.", 'ja': '特に長い文書については、監督者なしで抽出文書を要約する問題に取り組んでいます。監視されていない問題をまばらな自動回帰問題としてモデル化し、得られた組み合わせ問題を凸なノルム制約問題を介して近似する。専用のフランク＝ウォルフ・アルゴリズムを使用して解決します。k個の文で要約を生成するには、アルゴリズムは約k回の反復を実行するだけで、長いドキュメントにとって非常に効率的です。私たちは、語彙的（標準的）ルージュスコアとセマンティック（埋め込みベース）スコアの両方を使用して、他の2つの監督されていない方法と比較してアプローチを評価します。当社のメソッドは、データセットの両方でより良い結果を達成し、特に高度にパラフレーズ化されたサマリーの埋め込みと組み合わせるとうまく機能します。', 'zh': '吾曹决无监取文档摘要,特于长文档。 将无监督建模为疏自归,因凸范数约束近似结果组合。 吾用Frank-Wolfe算法以决之。 欲生 k 句之摘要,算法但行 k 次迭代,于长文档有效。 用词汇(率)ROUGE分数及语义(基于嵌)之分以质之。 吾法于两数集上皆得善终,而于与高转摘要嵌合用之效尤佳。', 'hi': 'हम unsupervised extractive दस्तावेज़ summarization की समस्या को संबोधित करते हैं, विशेष रूप से लंबे दस्तावेजों के लिए। हम एक विरल ऑटो-प्रतिगमन के रूप में असुरक्षित समस्या को मॉडल करते हैं और एक उत्तल, आदर्श-विवश समस्या के माध्यम से परिणामी संयुक्त समस्या का अनुमान लगाते हैं। हम इसे एक समर्पित फ्रैंक-वोल्फ एल्गोरिथ्म का उपयोग करके हल करते हैं। के वाक्यों के साथ एक सारांश उत्पन्न करने के लिए, एल्गोरिथ्म को केवल लगभग के पुनरावृत्तियों को निष्पादित करने की आवश्यकता होती है, जिससे यह एक लंबे दस्तावेज़ के लिए बहुत कुशल हो जाता है। हम लेक्सिकल (मानक) रूज स्कोर दोनों का उपयोग करके दो अन्य असुरक्षित तरीकों के साथ-साथ शब्दार्थ (एम्बेडिंग-आधारित) दोनों का उपयोग करके हमारे दृष्टिकोण का मूल्यांकन करते हैं। हमारी विधि दोनों डेटासेट के साथ बेहतर परिणाम प्राप्त करती है और विशेष रूप से अच्छी तरह से काम करती है जब अत्यधिक संक्षिप्त सारांश के लिए एम्बेडिंग के साथ संयुक्त होती है।', 'ru': 'Мы рассматриваем проблему неконтролируемого извлечения документов, особенно объемных документов. Мы моделируем неконтролируемую задачу как редкую авторегрессию и аппроксимируем результирующую комбинаторную задачу через выпуклую, нормоограниченную задачу. Мы решаем это с помощью специального алгоритма Фрэнка-Вульфа. Чтобы сгенерировать резюме с k предложениями, алгоритму нужно выполнить только приблизительно k итераций, что делает его очень эффективным для длинного документа. Мы оцениваем наш подход по сравнению с двумя другими неконтролируемыми методами, используя как лексические (стандартные) баллы РУЖА, так и семантические (основанные на вложениях). Наш метод достигает лучших результатов с обоими наборами данных и работает особенно хорошо в сочетании с вложениями для сильно перефразированных сводок.', 'ga': 'Tugaimid aghaidh ar an bhfadhb a bhaineann le hachoimriú doiciméad eastóscach gan mhaoirseacht, go háirithe maidir le doiciméid fhada. Múnlaímid an fhadhb gan mhaoirseacht mar fhadhb gann ar aischéimniú uathoibríoch agus déanaimid neas-fhadhb an chomhcheangail a bhíonn mar thoradh uirthi trí fhadhb dhronnach, shrianta normálta. Réitímid é ag baint úsáide as algartam tiomanta Frank-Wolfe. Chun achoimre a ghiniúint le k abairtí, ní gá don algartam ach thart ar k atriallta a dhéanamh, rud a fhágann go bhfuil doiciméad fada an-éifeachtach. Déanaimid ár gcur chuige a mheas i gcoinne dhá mhodh eile gan mhaoirseacht ag baint úsáide as scóir foclóireachta (caighdeánach) ROUGE, chomh maith le cinn shéimeantacha (bunaithe ar neadú). Baineann ár modh torthaí níos fearr amach leis an dá thacar sonraí agus oibríonn sé go han-mhaith nuair a chomhcheanglaítear é le leabú achoimrí an-athfhorleathana.', 'hu': 'A felügyelet nélküli kinyerő dokumentumok összefoglalásának problémájával foglalkozunk, különösen a hosszú dokumentumok esetében. A felügyelet nélküli problémát ritka automatikus regressziós modellezésként modellezzük, és konvex, normkorlátozott problémával közelítjük meg az ebből eredő kombinációs problémát. Egy dedikált Frank-Wolfe algoritmussal oldjuk meg. A k mondatokkal történő összefoglalás létrehozásához az algoritmusnak csak kb. k iterációkat kell végrehajtania, ami nagyon hatékony egy hosszú dokumentum esetén. Két másik, felügyelet nélküli módszerrel szemben értékeljük megközelítésünket lexikai (standard) ROUGE pontszámokkal, valamint szemantikai (beágyazási alapú) pontszámokkal. Módszerünk mindkét adatkészlettel jobb eredményeket ér el, és különösen jól működik, ha beágyazásokkal kombináljuk a nagymértékben parafrázott összefoglalókat.', 'it': "Affrontiamo il problema della sintesi non supervisionata dei documenti estrattivi, soprattutto per i documenti lunghi. Modelliamo il problema non supervisionato come un problema di auto-regressione sparsa e approssimamo il problema combinatorio risultante tramite un problema convesso, vincolato dalla norma. Lo risolviamo usando un algoritmo Frank-Wolfe dedicato. Per generare un riepilogo con frasi k, l'algoritmo ha solo bisogno di eseguire approssimativamente iterazioni k, rendendolo molto efficiente per un documento lungo. Valutiamo il nostro approccio rispetto ad altri due metodi non supervisionati utilizzando sia punteggi lessicali (standard) ROUGE, che semantici (embedding-based). Il nostro metodo raggiunge risultati migliori con entrambi i set di dati e funziona particolarmente bene se combinato con incorporazioni per riassunti altamente parafrasati.", 'ka': 'ჩვენ გადაწყენებთ პრობლემას, რომელიც არ განსხვავებული ექსტრაქტიური დოკუმენტის სიმბოლობას, განსაკუთრებით ძალიან დოკუმენ ჩვენ მოდელურებთ არსებული პრობლემა, როგორც წარმოიდგინული ავტორეგრესია ერთი და შემდეგ კომბინუტორიული პრობლემა კონგექსის, ნორმის დარწმუნებული პრობლემა. ჩვენ ამას გავაკეთებთ, გამოყენებით განსაკუთრებული ტრანკს-ვალფეს ალგორიტიმა. k სიტყვების შექმნა, ალგორიტიმმა უნდა მხოლოდ k თეტრაციების გამოყენება, რაც ძალიან ეფექტიური დოკუმენტისთვის. ჩვენ ჩვენი პროგრამის შესახებ ორი სხვა განსხვავებული მეტოვების გამოყენება ლექსიკალური (სტანდარტული) ROUGE წერტილების გამოყენებით, და სმენტიკური (დაყენებული) ჩვენი მეთოდი უკეთესი შედეგი მონაცემების შედეგი და მუშაობა სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადას', 'mk': 'Го решаваме проблемот со ненадгледуваната резултатација на екстрактивниот документ, особено за долги документи. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem.  Го решаваме со посветен алгоритм Френк-Волф. За да се генерира сумирање со k реченици, алгоритмот треба да изврши само приближно k iterations, што го прави многу ефикасен за долг документ. Го проценуваме нашиот пристап во споредба со два други ненадгледувани методи користејќи ги и лексичките (стандардни) ROUGE резултати, како и семантичките (базирани на вградување). Нашиот метод постигнува подобри резултати со двата податоци и функционира особено добро кога се комбинира со вложувања за високо парафразирани резултати.', 'kk': 'Құжаттарды қолданып шығарылмаған құжаттың тұжырымдамасының мәселесін өзгертеміз, өзгеше ұзын құжаттар үшін. Біз көзгертілмеген мәселеді бір-бірінші автоматты- регрессия ретінде үлгілеп, конвекс, нормалық шектелген мәселе арқылы жинақталған мәселелерді шектеп береміз. Біз оны Франк-Волф алгоритмімен шешіміз. k сөздермен тұжырымдама құру үшін алгоритм тек k қайталауын орындау керек, ұзын құжат үшін бұл құжат үшін әсер етеді. Біз лексикалық (стандартты) ROUGE нәтижелерін қолдану үшін басқа екі әдістерінің қарсымызды бағалаймыз, сондай-ақ семантикалық (ендіру негізінде) нәтижелерін қолданатын. Біздің әдіміміз деректер қорларының екі жақсы нәтижелерін жеткізеді және әдетте парафраз қорларының ендіру үшін жақсы жұмыс істейді.', 'ms': 'Kami mengatasi masalah pengringkasan dokumen ekstraktif tidak diawasi, terutama untuk dokumen panjang. Kita model masalah yang tidak diawasi sebagai regresi-automatik yang jarang satu dan kira-kira masalah kombinatorial yang berasal melalui masalah yang dikonveks, yang dikuasai norm a. We solve it using a dedicated Frank-Wolfe algorithm.  Untuk menghasilkan ringkasan dengan kalimat k, algoritma hanya perlu melaksanakan kira-kira k iterasi, menjadikannya sangat efisien untuk dokumen panjang. Kami menilai pendekatan kita melawan dua kaedah tidak diawasi lain menggunakan kedua-dua skor ROUGE lexik (piawai), serta yang semantik (berdasarkan penyembedding). Kaedah kami mencapai keputusan yang lebih baik dengan kedua-dua set data dan berfungsi terutama dengan baik apabila digabungkan dengan penyambungan untuk ringkasan parafrasa tinggi.', 'ml': 'സൂക്ഷിച്ചിട്ടില്ലാത്ത പ്രശ്നത്തിന്റെ പ്രശ്നം ഞങ്ങള്\u200d വിശദീകരിക്കുന്നു. പ്രത്യേകിച്ച് നീണ്ട ര സൂക്ഷിക്കപ്പെടാത്ത പ്രശ്നത്തെ ഞങ്ങള്\u200d മാതൃകയാക്കുന്നു. സ്പോര്\u200dട്ടില്\u200d നിര്\u200dബന്ധിതമായ ഒരു പ്രശ്നം സ്വയം വീണ്ടെടുക്കുന്നതിനെ  ഫ്രാങ്ക്-വോള്\u200dഫ് ആല്\u200dഗോരിതം ഉപയോഗിച്ച് നമ്മള്\u200d അത് തീരുമാനിച്ചു. കെ വാക്കുകളുടെ കൂടെ ഒരു ചുരുക്കം ഉണ്ടാക്കാന്\u200d ആല്\u200dഗോരിതം മാത്രമേ പ്രവര്\u200dത്തിപ്പിക്കേണ്ടതുള്ളൂ, ഒരു നീണ്ട രേഖയ്ക്ക് അതി സംരക്ഷിക്കപ്പെടാത്ത രണ്ടു മാര്\u200dഗങ്ങള്\u200dക്കെതിരായി നമ്മുടെ നടപടികള്\u200d വിലയിച്ചുകൊടുക്കുന്നു. രണ്ട് ലെക്സിക്കല്\u200d (സാധാരണ) റൂജെയിന്\u200dറ നമ്മുടെ രീതിയില്\u200d ഡാറ്റാസറ്റുകള്\u200d രണ്ടുപേര്\u200dക്കും നല്ല ഫലങ്ങള്\u200d നേടുകയും, പ്രത്യേകിച്ച് പ്രവര്\u200dത്തിക്കുകയും ചെയ്യുന്നത', 'mt': 'Aħna nindirizzaw il-problema tas-sommarju tad-dokumenti estrattivi mhux sorveljati, speċjalment għal dokumenti twal. Aħna nimmudellaw il-problem a mhux sorveljata bħala waħda ta’ awto-rigressjoni baxxa u nipprossimaw il-problema kombinatorja li tirriżulta permezz ta’ problema konvessa u ristretta min-normi. Aħna nagħmluha permezz ta’ algoritmu dedikat ta’ Frank-Wolfe. Biex jiġġenera sommarju bis-sentenzi k, l-algoritmu jeħtieġ li jeżegwixxi biss madwar k iterazzjonijiet, li jagħmluha effiċjenti ħafna għal dokument twil. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones.  Il-metodu tagħna jikseb riżultati aħjar kemm b’settijiet ta’ dejta kif ukoll jaħdem b’mod speċjali tajjeb meta kkombinat ma’ inkorporazzjonijiet għal sommarji parafrażizzati ħafna.', 'no': 'Vi adresserer problemet med usikkerte ekstraktiv dokumentsamandringa, spesielt for lang dokument. Vi modeller den ugjennomsiktige problemet som eit sparse automatisk regresjon, og omtreng det resultanta kombinasjonsfeilet med eit konveks, norm-avgrensa problem. Vi løser det med ein spesifisert Frank-Wolfe algoritme. For å laga eit samandrag med k- setningar må algoritmen berre k øyra omtrent k- gjentakingar, og gjera det veldig effektivt for eit langt dokument. Vi evaluerer tilnærminga vårt mot to andre usikkerte metodar med både leksiske (standard) ROUGE- poeng, og semantiske (innbyggingsbaserte) poeng. Metoden vårt oppnår bedre resultat med både datasett og fungerer spesielt godt når sammen med innbygging for høg parafraserte samandrag.', 'mn': 'Бид өөрсдийгөө хүлээн зөвшөөрөгдсөн нэмэлт баримт хэвлүүлэх асуудлыг, ялангуяа урт баримтуудын тухай ярилцаж байна. Бид тодорхойлолтгүй асуудлыг автоматжуулалтын нэг хэлбэрээр загварлаж, үр дүнтэй холбоотой асуудлыг холбоотой. Бид үүнийг Франк-Волф алгоритмыг ашиглан шийдвэрлэх юм. k өгүүлбэртэй жинхэнэ хэмжээсүүд бий болгохын тулд алгоритм нь зөвхөн k-ийн жинхэнэ хэмжээсүүд хийх хэрэгтэй, урт баримт нь маш үр дүнтэй болгодог. Бид өөр өөр хоёр давхарагдаагүй арга замыг хоёр (стандарт) ROUGE тоонуудыг ашиглан, мөн semantic (дотор) тоонуудыг ашиглан үнэлдэг. Бидний арга нь өгөгдлийн сангууд хоёулаа ч илүү сайн үр дүнг гаргаж, ялангуяа сайн ажиллаж байгаа юм.', 'lt': 'Mes sprendžiame nepastebimo išgaunamojo dokumento santraukos problem ą, ypač ilgų dokumentų atveju. Mes modeliuojame nepastebimą problem ą kaip nedidelę automatinę regresiją ir apytikriai įvertiname atsiradusią kombinacinę problemą per konveksinę, normomis apribotą problemą. Mes jį išspręsime naudojant specialią Frank-Wolfe algoritmą. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document.  We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones.  Mūsų metodas pasiekia geresnių rezultatų su abiem duomenų rinkiniais ir ypač gerai veikia kartu su labai parafrazuotų santraukų įtraukomis.', 'ro': 'Abordăm problema rezumatului documentelor extractive nesupravegheate, în special pentru documentele lungi. Modelăm problema nesupravegheată ca una rară de auto-regresie și aproximăm problema combinatorie rezultată printr-o problemă convexă, constrânsă de norme. O rezolvăm folosind un algoritm dedicat Frank-Wolfe. Pentru a genera un rezumat cu propoziții k, algoritmul trebuie doar să execute aproximativ k iterații, ceea ce îl face foarte eficient pentru un document lung. Evaluăm abordarea noastră în raport cu alte două metode nesupravegheate utilizând atât scoruri lexicale (standard) ROUGE, cât și cele semantice (bazate pe încorporare). Metoda noastră obține rezultate mai bune atât cu seturi de date și funcționează foarte bine atunci când este combinată cu încorporări pentru rezumate foarte parafrazate.', 'pl': "Rozwiązujemy problem bez nadzoru ekstrakcyjnego podsumowywania dokumentów, zwłaszcza w przypadku długich dokumentów. Modelujemy problem bez nadzoru jako rzadki autoregresyjny i przybliżamy powstały problem kombinatorski poprzez wypukły, ograniczony normą problem. Rozwiązujemy to za pomocą dedykowanego algorytmu Frank-Wolfe'a. Aby wygenerować podsumowanie ze zdaniami k, algorytm musi tylko wykonać około k iteracje, co sprawia, że jest bardzo wydajny dla długiego dokumentu. Oceniamy nasze podejście na tle dwóch innych metod bez nadzoru, używając zarówno leksykalnych (standardowych) wyników ROUGE, jak i semantycznych (opartych na osadzeniu). Nasza metoda osiąga lepsze wyniki z obu zbiorów danych i działa szczególnie dobrze w połączeniu z osadzeniami dla wysoce parafrazowanych podsumowań.", 'sr': 'Mi rješavamo problem neodređenog ekstraktivnog dokumenta, posebno za duge dokumente. Mi modeliramo neodređeni problem kao rezervna automatska regresija jedan i približavamo rezultatni kombinacijski problem preko konveksa, norm ograničenog problem a. Rešiæemo ga koristeći posveæeni Frank-Wolfe algoritam. Da bi stvorio sažetak s k rečenicama, algoritam samo treba da izvrši približno k iteracije, čineći ga veoma efikasnim za dug dokument. Procjenjujemo naš pristup protiv dva druga neodređena metoda koristeći obje leksičke (standardne) rezultate ROUGE, kao i semantičke (osnovane na osnovu ugrađenja). Naša metoda postiže bolji rezultat sa obje sete podataka i radi posebno dobro kada se kombiniraju sa ugrađenjem za visoko parafrazirane sažetke.', 'sv': 'Vi tar itu med problemet med oövervakad sammanfattning av extraktionsdokument, särskilt för långa dokument. Vi modellerar det oövervakade problemet som ett sparsamt auto-regressionsproblem och approximera det resulterande kombinatoriska problemet via ett konvex, normbegränsat problem. Vi löser det med hjälp av en dedikerad Frank-Wolfe algoritm. För att generera en sammanfattning med k-meningar behöver algoritmen bara köra ungefär k iterationer, vilket gör det mycket effektivt för ett långt dokument. Vi utvärderar vårt tillvägagångssätt mot två andra oövervakade metoder med både lexikala (standard) ROUGE poäng, såväl som semantiska (inbäddningsbaserade) metoder. Vår metod ger bättre resultat med både datauppsättningar och fungerar särskilt bra i kombination med inbäddningar för mycket parafraserade sammanfattningar.', 'so': 'Dhibaatooyinka aan la ilaalinayn oo la soo saari karo, gaar ahaan dukumentiyada dhaadheer. Dhibaatada aan la ilaalinayno waxaynu u sameynaa sida dhibaato dab-dib u celinta iyo sida dhibaatada labaatanka lagu soo jeedo marka lagu jiro dhibaato caadi ah oo caadi ah. Waxaynu ku xajinnay qoraal gaar ah ee Frank-Wolfe. Si uu u soo saaro dhamaan qoraalka k, waxaa loo baahan yahay in la sameeyo qoraalka oo kaliya, oo uu ku shaqeeyo warqad dheer. Waxaynu qiimeynaynaa qaababkayaga ka geesta ah labada qaab oo kale oo aan la ilaalinayn, sida loo isticmaalayo labada kooxood oo lexico ah (standard) ROUGE iyo sidoo kale semantic (embedding-based). Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.', 'si': 'අපි ප්\u200dරශ්නයක් විශේෂයෙන් ප්\u200dරශ්නයක් කරනවා, විශේෂයෙන්ම විශේෂයෙන් ලොකු දස්තාවෙන්. අපි ප්\u200dරශ්නයක් නැති ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ස්වයංක්\u200dරීය විදියට ප්\u200dරශ්නයක් විදියට ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක අපි ඒක විස්තර කරන්නේ ෆ්\u200dරෑන්ක්-වෝල්ෆ් ඇල්ගෝරිතම් එකක් පාවිච්චි කරන්න. k වචනය සමඟ සංශ්\u200dයයක් නිර්මාණය කරන්න, ඇල්ගෝරිතම් විතරයි k වචනයක් නිර්මාණය කරන්න ඕන, ඒක ලොකු ලොකු ලේඛනයක් වෙනුව අපි අනිත් අනිත් විධානය දෙන්නා විරුද්ධ විධානය කරන්නේ ලෙක්සිකාල (ස්ථානය) ROUGE ස්කෝර් දෙන්නම් ප්\u200dරයෝජනය කරන්න, ස අපේ විධානය හොඳ ප්\u200dරතිචාරයක් ලැබෙනවා දත්ත සේට් දෙන්නම් හොඳයි වැඩ කරනවා විශේෂයෙන්ම හොඳයි සම්බන්ධ', 'ta': 'பாதுகாப்பில்லாத பிரச்சனையை நாம் குறிப்பிடுகிறோம், குறிப்பாக நீண்ட ஆவணங்களுக்கு. பாதுகாப்பாக்கப்படாத பிரச்சனையை நாம் முறைமையாக வடிவமைக்கிறோம் ஒரு சிறிய தானியங்கி திரும்பப்படுத்தல் ஒன்றாக மற்றும் செல்ல நாங்கள் ஒரு தனிப்படுத்தப்பட்ட பிராங்க்-Wolfe ஆல்பரிதம் பயன்படுத்தி அதை தீர்வு செய்கிறோம். k வாக்கியங்களுடன் சுருக்கம் உருவாக்குவதற்கு, algorithm மட்டும் செயல்படுத்த வேண்டும், அதை நீண்ட ஆவணத்துக்கு மிகவும் தேவையானது. நாம் மற்ற இரண்டு மற்றும் பாதுகாப்பாக்கப்படாத முறைகளை பயன்படுத்தி நமது செயல்பாட்டை மதிப்பிடுக எங்கள் முறைமை இரு தரவுத்தளங்களுடனும் சிறந்த முடிவுகள் பெறுகிறது மற்றும் குறிப்பிட்ட கூட்டத்திற்கு மிகவும் சிறப்ப', 'ur': 'ہم اس مسئلہ میں مشکل کریں گے جو غیرقابل غیرقابل غیرقابل دکھانے کے سامنے ہے، مخصوصاً طویل دکھانے کے لئے۔ ہم نے غیر محفوظ مسئلہ کی مدل کرلی ہے ایک چھوٹی اٹو ریگرس کی حالت میں اور اس کے نتیجے میں کونکس، نورمن محفوظ مسئلہ کے ذریعہ ایک مشکل کے مطابق مشکل کو تقریبا کر رہے ہیں. ہم اسے ایک خاص فرانک والف الگوریتم سے حل کر رہے ہیں۔ k جماعتوں کے ساتھ ایک جماعت پیدا کرنے کے لئے، الگوریتم کو صرف تقریبا k تکرار کرنے کی ضرورت ہے، اور اس کو ایک طویل سند کے لئے بہت اثرات بنانے کے لئے۔ ہم اپنے طریقے کو دوسرے ناپابندی طریقے کے مقابلہ میں مطالبہ کرتے ہیں جو دونوں لکسیکل (استاندارڈ) ROUGE اسکوروں کے استعمال کرتے ہیں، اور سیمنٹی (بنڈنگ بنیادی) طریقے سے۔ ہمارا طریقہ بہتر نتیجے حاصل کرتا ہے دونوں ڈیٹ سٹ کے ساتھ اور مخصوصاً اچھا کام کرتا ہے جب بہت پارافریزڈ جمع کرنے کے لئے ابڈینگ کے ساتھ ملے جاتے ہیں.', 'el': 'Αντιμετωπίζουμε το πρόβλημα της ανεξέλεγκτης εξαγωγικής συνοψίας εγγράφων, ειδικά για μεγάλα έγγραφα. μοντελοποιούμε το πρόβλημα χωρίς επίβλεψη ως ένα αραιό πρόβλημα αυτόματης παλινδρόμησης και προσεγγίζουμε το προκύπτον συνδυαστικό πρόβλημα μέσω ενός κυρτού, περιορισμένου προτύπου προβλήματος. Το λύνουμε χρησιμοποιώντας έναν ειδικό αλγόριθμο Φρανκ-Γουλφ. Για να δημιουργήσετε μια περίληψη με προτάσεις ο αλγόριθμος χρειάζεται μόνο να εκτελέσει επαναλήψεις περίπου, καθιστώντας τον πολύ αποτελεσματικό για ένα μακρύ έγγραφο. Αξιολογούμε την προσέγγισή μας σε σχέση με δύο άλλες μεθόδους χωρίς επίβλεψη χρησιμοποιώντας τόσο λεξικές (τυποποιημένες) βαθμολογίες όσο και σημασιολογικές (βασισμένες στην ενσωμάτωση). Η μέθοδος μας επιτυγχάνει καλύτερα αποτελέσματα και με τα δύο σύνολα δεδομένων και λειτουργεί ιδιαίτερα καλά όταν συνδυάζεται με ενσωμάτωση για εξαιρετικά παράφρασες περιλήψεις.', 'uz': "@ info Biz qo'llanmagan muammolarni avtomatik boshqarish sifatida modellashiz va kelib, konveks, oddiy qo'llangan muammolar orqali komatoriy muammolarning muammolari bilan bog'liq. Biz buni faqat Frank-Wolfe algoritdan foydalanamiz. K tugmalar birikmasini yaratish uchun, algoritni faqat qisqa qismi elementlarini ishga tushirish kerak. Biz cheksiz (standard) qoʻllanmalarni va semantik (asosida) qo'llanmalar bilan boshqa qo'llanmalarni qiymatmiz. Bizning usuli maʼlumotlar tarkibida yaxshi natijalarini bajaradi va juda yaxshi ishlayapti va eng juda katta paraphras muhim muhtasari bilan birlashtirilganda.", 'vi': 'Chúng tôi giải quyết vấn đề về việc mô phỏng tài liệu khai thác không giám sát, đặc biệt là những tài liệu dài. Chúng tôi mô tả vấn đề không được giám sát thành vấn đề tự phục hồi thoáng qua và ước lượng vấn đề kết hợp với nhau bằng cách thông thường, giới hạn tiêu chuẩn. Chúng ta giải quyết nó bằng thuật toán Dành cho Frank-Wolfe. Để tạo một bản tóm tắt với các câu K, thuật to án chỉ cần thực hiện độ lặp lại xấp xỉ, làm cho nó rất hiệu quả cho một tài liệu dài. Chúng tôi đánh giá cách tiếp cận của chúng tôi dựa trên hai phương pháp không giám sát khác, dựa trên cả ghi chép từ(chuẩn) và từ ngữ (nhúng vào) cũng được. Phương pháp của chúng tôi đạt được kết quả tốt hơn với cả hai bộ dữ liệu và hoạt động đặc biệt tốt khi kết hợp với sự nhúng vào cho các bản tóm tắt cao độ.', 'da': 'Vi løser problemet med uautoriseret opsummering af ekstraktive dokumenter, især for lange dokumenter. Vi modellerer det uopvågede problem som et sparsomt auto-regressionsproblem og tilnærmer det resulterende kombinatoriske problem via et konvekst, normbegrænset problem. Vi løser det ved hjælp af en dedikeret Frank-Wolfe algoritme. For at generere et resumé med k-sætninger behøver algoritmen kun at udføre ca. k iterationer, hvilket gør det meget effektivt for et langt dokument. Vi evaluerer vores tilgang mod to andre ikke-overvågede metoder ved hjælp af både lexikale (standard) ROUGE scores, såvel som semantiske (embedding-baserede) metoder. Vores metode opnår bedre resultater med både datasæt og fungerer især godt, når den kombineres med indlejringer til stærkt parafraserede resuméer.', 'bg': 'Ние решаваме проблема с ненадзорните извличащи документи, особено за дълги документи. Моделираме нерегламентираната задача като рядка авторегресия и приближаваме получената комбинаторна задача чрез изпъкнала, ограничена от норми задача. Решаваме го със специален алгоритъм на Франк-Уолф. За да генерира обобщение с k изречения, алгоритъмът трябва да изпълни само приблизително k итерации, което го прави много ефективен за дълъг документ. Ние оценяваме нашия подход спрямо два други ненадзорни метода, използвайки както лексикални (стандартни) резултати, така и семантични (базирани на вграждане) такива. Нашият метод постига по-добри резултати и с двете набори от данни и работи особено добре, когато се комбинира с вграждания за високо парафразирани резюмета.', 'nl': 'We behandelen het probleem van extractieve samenvatting van documenten zonder toezicht, vooral voor lange documenten. We modelleren het probleem zonder toezicht als een schaarse auto-regressie en benaderen het resulterende combinatorische probleem via een convex, normbeperkt probleem. We lossen het op met behulp van een speciaal Frank-Wolfe algoritme. Om een samenvatting te genereren met k zinnen hoeft het algoritme slechts ongeveer k iteraties uit te voeren, waardoor het zeer efficiënt is voor een lang document. We evalueren onze aanpak ten opzichte van twee andere onbeheerde methoden met zowel lexicale (standaard) ROUGE scores als semantische (embedding-based) scores. Onze methode bereikt betere resultaten met beide datasets en werkt vooral goed in combinatie met embeddings voor sterk geparafraseerde samenvattingen.', 'hr': 'Mi rješavamo problem nepotrebne ekstraktivne sažetke dokumenta, posebno za duge dokumente. Mi modeliramo neodređeni problem kao rezervna automatska regresija jedna i približavamo rezultatni kombinacijski problem preko konveksa, norm ograničenog problem a. Riješimo ga koristeći posvećeni Frank-Wolfe algoritam. Da bi stvorio sažetak s k rečenicama, algoritam samo treba izvršiti približno k iteracije, čineći ga vrlo učinkovitom za dug dokument. Procjenjujemo naš pristup protiv dvije druge nepotrebne metode koristeći obje leksičke (standardne) rezultate ROUGE-a, kao i semantičke (osnovane na ugrađenju). Naša metoda postigne bolji rezultat i sa podacima, a posebno dobro funkcionira kada se kombiniraju s ugradnjem za visoko parafrazirane sažetke.', 'de': 'Wir behandeln das Problem der unbeaufsichtigten extraktiven Dokumentenverfassung, insbesondere bei langen Dokumenten. Wir modellieren das unbeaufsichtigte Problem als spärliche Auto-Regression und nähern das resultierende kombinatorische Problem über ein konvexes, normbeschränktes Problem an. Wir lösen es mit einem dedizierten Frank-Wolfe Algorithmus. Um eine Zusammenfassung mit k Sätzen zu generieren, muss der Algorithmus nur etwa k Iterationen ausführen, was ihn für ein langes Dokument sehr effizient macht. Wir bewerten unseren Ansatz mit zwei anderen unüberwachten Methoden, die sowohl lexikalische (Standard) ROUGE Scores als auch semantische (Einbettungsbasierte) Scores verwenden. Unsere Methode erzielt bessere Ergebnisse mit beiden Datensätzen und funktioniert besonders gut in Kombination mit Einbettungen für stark paraphrasierte Zusammenfassungen.', 'id': 'Kami mengatasi masalah penglihatan dokumen ekstraktif yang tidak diawasi, terutama untuk dokumen panjang. Kami model masalah yang tidak diawasi sebagai auto-regresi yang sedikit dan mendekati masalah kombinatorial yang berasal melalui masalah konveks, norm-constrained. Kita menyelesaikannya dengan menggunakan algoritma Frank-Wolfe yang didedikasikan. Untuk menghasilkan ringkasan dengan kalimat k, algoritma hanya perlu mengeksekusi sekitar k iterasi, membuatnya sangat efisien untuk dokumen panjang. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones.  Metode kami mencapai hasil yang lebih baik dengan kedua dataset dan bekerja terutama dengan baik ketika bergabung dengan embedding untuk ringkasan parafrasa tinggi.', 'tr': 'Biz janlaşdyrmadyk tapylmadyk sened sumynyň meselesini çözdiris, ýöne uzak sened üçin. Biz suytalanmaýan meseläni küçük bir otomatik regressiýa hökmünde örän çykýarys we netijeli kombinatorial meseläni bir konveks we norm hasaplanýan mesele bilen çykýarys. Biz muny Frank Wolfe algoritmi bilen çözdik. k sözleri bilen laýyşyk döretmek üçin algoritmyň diňe k täzelikleri döretmek gerek, ol bir sened üçin örän täsirli bolmaly. Biziň golamyzy beýleki iki sany golaýda leksiýaly (standart) ROUGE sanlaryny we semantik (daşary) golaýlaryny ulanarak çykýarys. Biziň ýüregimiz hem veri setirleri bilen has gowy netijeleri ýetip bilýär we özellikle gowy işleýär, haçan parafrazler jemgyýetleri üçin birleştirilýän bolsa.', 'ko': '우리는 특히 긴 문서에 대한 감독이 없는 추출 문서 요약 문제를 해결했다.우리는 무감독 문제를 희소한 자귀환 문제로 모델링하고 튀어나온 범수 구속 문제를 통해 이로 인해 발생하는 조합 문제에 접근할 것이다.우리는 전문적인 Frank Wolfe 알고리즘을 사용하여 그것을 해결한다.k개의 문장을 포함하는 요약을 만들기 위해 이 알고리즘은 약 k번의 교체만 실행하면 긴 문서에 매우 효과적이다.우리는 어휘(표준) ROUGE 점수와 의미(삽입에 기반) 점수를 사용하여 우리의 방법과 다른 두 가지 무감독 방법을 비교했다.우리의 방법은 두 데이터 집합에서 모두 더욱 좋은 결과를 얻었는데, 특히 삽입과 결합할 때 고도로 복술한 요약에 대한 효과가 더욱 좋다.', 'fa': 'ما مشکل جمع کردن سند خارجی غیرقابل تحویل دادیم، مخصوصا برای سند طولانی. ما مشکل غیرقابل تحقیق را مدل می\u200cکنیم به عنوان یک بازگشت خودکار کوچک یک و مشکل اتحادیه\u200cای را از طریق یک مشکل قابل تحقیق و ناروم محدود می\u200cکنیم. ما با استفاده از الگوریتم فرانک والف استفاده می کنیم. برای تولید کردن یک جمله با جمله k، الگوریتم تنها نیاز به اجرای تقریباً k تکرار می\u200cکند و برای یک سند طولانی بسیار موثر می\u200cکند. ما روش خود را در مقابل دو روش غیرقابل تحقیق می\u200cکنیم با استفاده از نقطه\u200cهای زبانی (استاندارد) ROUGE، و همچنین از نقطه\u200cهای semantic (بنیادی) استفاده می\u200cکنیم. روش ما نتیجه\u200cهای بهتر با هر مجموعه\u200cی داده\u200cها و مخصوصاً خوب کار می\u200cکند وقتی با جمع\u200cآوری\u200cها برای جمع\u200cآوری\u200cهای بسیار پارافریز متصل می\u200cشوند.', 'sq': 'Ne trajtojmë problemin e përmbledhjes së dokumenteve të jashtëzakonshme ekstraktive, veçanërisht për dokumente të gjata. Ne modelojmë problemin e pa mbikqyrur si një auto-regresion të vogël dhe përafërtojmë problemin kombinatorial që rezulton nëpërmjet një problemi të konveksit, të kufizuar nga normat. We solve it using a dedicated Frank-Wolfe algorithm.  Për të gjeneruar një përmbledhje me fjalët k, algoritmi duhet vetëm të ekzekutojë përafërsisht k iterations, duke e bërë atë shumë efektiv për një dokument të gjatë. Ne vlerësojmë qasjen tonë ndaj dy metodave të tjera të pazgjidhura duke përdorur si rezultatet lexike (standarte) ROUGE, si dhe ato semantike (bazuar në përfshirje). Metoda jonë arrin rezultate më të mira me të dy grupet e të dhënave dhe funksionon veçanërisht mirë kur kombinohet me përfshirje për përmbledhje shumë parafrazuese.', 'af': "Ons adres die probleem van ononderwerpende ekstraktiewe dokument opsomming, veral vir lang dokumente. Ons model die ononderwerpende probleem as 'n sparse outomatiese regresie een en omtrent die resulteerde kombinasie probleem deur 'n konveks, norm-beperkte probleem. Ons los dit met 'n aangestelde Frank-Wolfe algoritme. Om 'n opsomming met k setings te genereer, moet die algoritme slegs uitvoer omtrent k iterasies, maak dit baie effektief vir 'n lang dokument. Ons evalueer ons toegang teen twee ander ongeonderwerpende metodes met gebruik van beide leksiese (standaard) ROUGE poeiers en semantiese (inbêding-gebaseerde) metodes. Ons metode verkry beter resultate met beide datastelle en werk besonder goed wanneer gekombineer word met inbêding vir baie parafraseerde opsommings.", 'sw': 'Tunaongelea tatizo la muhtasari wa nyaraka zisizo na uhakika, hususani kwa nyaraka ndefu. Tunaonyesha tatizo hilo lisiloelewekwa kama utaratibu wa kujidhibiti wa kujitegemea moja na takribani tatizo la muunganiko kupitia tatizo la convex, la kawaida lililolazimika. Tutatulia kwa kutumia algorithi maalum ya Frank-Wolfe. Ili kutengeneza muhtasari wa hukumu za k, utaratibu unahitaji tu kutekeleza vifaa vya kiasi kikubwa, na kufanya hivyo kuwa na ufanisi mkubwa kwa ajili ya nyaraka ndefu. Tunatathmini mbinu zetu dhidi ya mbinu nyingine zisizo na uhakika kwa kutumia vipimo vyote viwili vya lexico (viwango vya kawaida) vya UKIMWI, pamoja na zile zile za kimapenzi (zinazotumiwa kwa kutumia vifaa vya kimapenzi). Utawala wetu unafanikiwa matokeo bora na vyote vya taarifa na hufanya kazi vizuri hasa pale unapounganishwa na mabadiliko kwa ajili ya kipindi cha muhtasari kinachoelezwa sana.', 'am': 'የማይጠበቀው የውጭ የሰነድ አቀማመጥ ማቀናጃ፣ በተለይም ለረጅም ሰነዶች እናስቸግራለን፡፡ የማይጠበቀውን ጉዳይ የራሱ አካባቢ መግለጫ እናደርጋለን፡፡ በተለየ የፋርንክ-Wolfe algorithm በመጠቀም እናፍረዋለን፡፡ ማጠቃለያ በk ቃላት ለመፍጠር፣ አጎርጂም ለረጅም ሰነድ ጥቅም እንዲያደርግ ብቻ ያስፈልጋል፡፡ የሌክሲካዊ (standard) የሮዩጂ ደረጃዎች እና የሴሜንቲካ (የውስጥ-መሠረት) የሁለት ሌሎች ያልጠበቁትን ሥርዓት እናስተዋልናለን፡፡ Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.', 'az': 'Biz müəyyən edilməmiş ekstraktif döküm təmizlənməsinin problemini çəkirik, özlərinə də uzun dökümlər üçün. Biz müəyyən edilməyən problemi bir küçük avtomatik-regresiya kimi modelləşdiririk və sonuçları birləşdirilmiş problemi bir konveks, norm müəyyən edilmiş problemi vasitəsilə yaxınlaşdırırıq. Biz bunu Frank-Wolfe algoritmi vasitəsilə çəkirik. K cümlələri ilə istifadə etmək üçün, algoritm yalnızca k iterasyonlarını idarə etmək lazımdır, uzun bir dök üm üçün çox faydalandırmaq üçün. Biz ikisini də leksik (standart) ROUGE nöqtələrini və semantik (yerləşdirmək tabanlı) ilə müəyyən edilməmiş başqa iki metodlara qarşı tərzimizi değerləşdiririk. Bizim metodumuz hər iki verilən qurğuları ilə daha xeyirli sonuçları başa düşər və özlərinə də yaxşı işlər edir, çünki yüksək parafraz qurğuları ilə birləşdirildikdə.', 'bn': 'বিশেষ করে দীর্ঘ নথিপত্রের জন্য আমরা সংক্ষেপের সমস্যার কথা বলি। আমরা অরক্ষিত সমস্যাটিকে স্বয়ংক্রিয়ভাবে পুনরুদ্ধার হিসেবে মডেল করি এবং একটি কনস্যোক্স, স্বাভাবিক নির্ধারিত সমস্যার মাধ্যমে প্রা আমরা এটা সমাধান করি ফ্র্যাঙ্ক-উলফ অ্যালগরিদম ব্যবহার করে। কি বাক্যের সাথে একটি সার্গার তৈরি করার জন্য, অ্যালগরিদম কেবল প্রায় এক কি বিষয়বস্তু প্রয়োজন, যা দীর্ঘ নথির জন্য এটি খুব কার্যকর। লেক্সিক্যাল (স্ট্যান্ডার্ড) রুজের স্কোর ব্যবহার করে আর সেম্যান্টিক (ভিত্তিক ভিত্তিক) ব্যবহার করে আমাদের দুটি অরক্ষিত পদ্ধতির ব আমাদের পদ্ধতি দুটো ডাটাসেটের সাথে ভাল ফলাফল অর্জন করে এবং বিশেষ করে বিশেষ করে বিশেষ করে বিশেষ করে যখন বিভিন্ন সংক্ষেপের জন্য', 'bs': 'Mi rješavamo problem nepotrebne ekstraktivne sažetke dokumenta, posebno za duge dokumente. Mi modeliramo neodređeni problem kao rezervna automatska regresija jedna i približavamo rezultatni kombinacijski problem preko konveksa, norm ograničenog problem a. Rešimo to sa posvećenim Frank-Wolfe algoritmom. Da bi stvorio sažetak s k rečenicama, algoritam samo treba izvršiti približno k iteracije, čineći ga vrlo efikasnim za dug dokument. Procjenjujemo naš pristup protiv dva druga neodređena metoda koristeći obje leksičke (standardne) rezultate ROUGE, kao i semantičke (osnovane na osnovu ugrađenja). Naša metoda postigne bolji rezultat sa obje komplete podataka i radi posebno dobro kada se kombiniraju sa integracijama za visoko parafrazirane sažetke.', 'cs': 'Řešíme problém bez dohledu extrakčního shrnutí dokumentů, zejména u dlouhých dokumentů. Modelujeme problém bez dozoru jako řídkou autoregresí a výsledný kombinatorní problém aproximujeme konvexním, normou omezeným problémem. Vyřešíme to pomocí speciálního Frank-Wolfe algoritmu. Chcete-li generovat souhrn s k větami, algoritmus musí provádět pouze přibližně k iterace, což je velmi efektivní pro dlouhý dokument. Náš přístup hodnotíme proti dvěma dalším metodám bez dozoru, které využívají jak lexikální (standardní) ROUGE skóre, tak i sémantické (založené na embeddingu). Naše metoda dosahuje lepších výsledků s oběma datovými sadami a funguje obzvláště dobře v kombinaci s vložením pro vysoce parafrázované souhrny.', 'et': "Tegeleme järelevalveta dokumentide kokkuvõtliku koostamise probleemiga, eriti pikkade dokumentide puhul. Me modelleerime järelevalveta probleemi hõreda automaatse regressioonina ja lähendame sellest tulenevat kombinatoorset probleemi kumera normiga piiratud probleemi kaudu. Me lahendame selle Frank-Wolfe'i algoritmi abil. Kokkuvõtte koostamiseks k lausetega peab algoritm käivitama ainult ligikaudu k iteratsiooni, muutes selle pika dokumendi jaoks väga tõhusaks. Hindame oma lähenemisviisi kahe teise järelevalveta meetodi suhtes, kasutades nii leksikaalseid (standardseid) ROUGE skoore kui ka semantilisi (manustamispõhiseid) skoore. Meie meetod saavutab paremad tulemused nii andmekogumitega kui see on kombineeritud põimitud kokkuvõtete jaoks.", 'fi': 'Käsittelemme valvomattoman tiivistelmän ongelmaa erityisesti pitkien asiakirjojen osalta. Mallistamme valvomattoman ongelman harvaantuneena automaattisena regressiona ja lähestymme tuloksena syntyvää kombinatorista ongelmaa kuperan, normirajoituksen avulla. Ratkaisemme sen Frank-Wolfen algoritmilla. Yhteenvedon tuottamiseksi k lauseilla algoritmin tarvitsee suorittaa vain noin k iteraatiota, mikä tekee siitä erittäin tehokkaan pitkän asiakirjan. Arvioimme lähestymistapaamme kahteen muuhun valvomattomaan menetelmään käyttäen sekä lexikaalisia (vakio) ROUGE-pisteitä että semanttisia (upotuspohjaisia) menetelmiä. Menetelmämme tuottaa parempia tuloksia molemmilla aineistoilla ja toimii erityisen hyvin yhdistettynä upotuksiin hyvin muotoiltuihin yhteenvetoihin.', 'ca': "Ens ocupem del problema de resumir els documents extractius sense supervisió, especialment per a documents llargs. Modelem el problem a sense supervisió com una ràpida auto-regressió i aproximam el problema combinatori resultant a través d'un problema convès i restringit a la norma. Ho resolem amb un algoritme Frank-Wolfe dedicat. Per generar un resum amb frases k, l'algoritme només necessita executar aproximadament k iteracions, fent-lo molt eficient per un llarg document. Evaluam el nostre enfocament en comparació amb dos altres mètodes no supervisats utilitzant les puntuacions ROUGE lècsiques (estàndard) i semàntiques (basades en l'incorporació). El nostre mètode aconsegueix millors resultats amb ambdós conjunts de dades i funciona especialment bé si es combina amb incorporacions per resumes altament parafrasats.", 'hy': 'Մենք լուծում ենք անվերահսկված փաստաթղթերի համառոտագրման խնդիրը, հատկապես երկար փաստաթղթերի համար: We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem.  Մենք լուծում ենք այն օգտագործելով նվիրված Ֆրանկ-Ուլֆի ալգորիթմ: Որպեսզի ստանանք k նախադասությունների համառոտագրություն, ալգորիթմը պետք է միայն կատարի մոտավորապես k կրկնություններ, դարձնելով այն շատ արդյունավետ երկար փաստաթղթի համար: Մենք գնահատում ենք մեր մոտեցումը երկու այլ անվերահսկված մեթոդի հետ, օգտագործելով երկու լեքսիկական (ստանդարտ) ROUGe-ի գնահատականներ, ինչպես նաև սեմանտիկ (ներգրավման հիմնված) գնահատականներ: Մեր մեթոդը ավելի լավ արդյունք է ստանում երկու տվյալների համակարգերի միջոցով և հատկապես լավ է աշխատում, երբ համադրվում է բարձր պարաֆրանսավորված համառոտագրությունների ներդրումների հետ:', 'jv': 'Awak dh챕w챕 ngejarak챕 perkoro sing sumulahing dokumen ora bisa nggawe Awak dh챕w챕 model sing perbudhakan sing gak nggawe perbudhakan karo sekasar auto-Regresyon siji dadi iki bakal dumadhi supoyo mulasar convex, Norm-limiter perbudhakan Awakdh챕w챕 nggawe nyimpen ngerasakno karo Algorithm layan Link-wolf. Jejaring Awak dh챕w챕 챕ntukno dh챕w챕 챕ntukno karo hal-hal sing gak dh챕w챕 cara nggawe sistem sing gak dh챕w챕. Awak dh챕w챕 챕ntukno sistem sing gak dh챕w챕 (awak dh챕w챕) rotUGE, lan semanti (embedding-basa) sing wis dipunang챕. method', 'ha': "Munã tambayar masu zartar da kurarin takardar nan da ba'a tsare ba, kuma hususan don takardun aiki daidai. Mu motsa zaɓallin da ba'a tsare ba kamar wata matabbata mai shimfiɗawa farat ɗaya da ke samu'a matsalar ta ƙara a matsayin komatiyatori a matsayin koman, wata mataimaki da aka tsare ta. Suna solar ta da algoritin Franki-Wolfe da aka ƙayyade shi. Ga ya ƙãga wani ƙari da a k a yi wa k cewa, algoritum na kamata kawai, ya yi amfani da su zartar da abubuwa baka ke daidai, kuma ya sanya shi mai amfani da wa takardar tsawo. Mu ƙaddara hanyarmu a kan hanyõyin biyu na'yan da ba'a tsare su ba da amfani da kowane nau'i biyu na leksisi (nau'in rubutu) da kuma masu sakanti (da aka baka-bane). Kayan hanyõyinmu, za'a sãmu mafiya alhẽri matsalar da su sami biyu na danganta kuma yana aiki mai kyau da amfani da idan an haɗa da filinaikin da masu ƙari da aka faɗa.", 'he': 'אנחנו מתמודדים עם הבעיה של מסמכים חושפים לא מעוקבים, במיוחד עבור מסמכים ארוכים. אנו דוגמנים את הבעיה הבלתי משגיחה כתחזור אוטומטי נמוך אחד ולהעביר את הבעיה הקומבונטוריאלית הנוצאה באמצעות בעיה משותפת, מוגבלת בנורמה. אנחנו פותרים את זה באמצעות אלגוריתם מוקדש של פרנק וולף. כדי ליצור סכם עם משפטים k, האלגוריתם רק צריך להוציא ביצע מחזור k בערך, הופך אותו מאוד יעיל למסמך ארוך. אנו מעריכים את הגישה שלנו נגד שתי שיטות נוספות ללא השגחה בשימוש גם נקודות ROUGE לקסיות (סטנדרטיות), וגם נקודות סמנטיות (מבוססות על קישור). השיטה שלנו משיגה תוצאות טובות יותר עם שני קבוצות נתונים ופעולה היטב במיוחד כשהיא משולבלת עם תוכניות לסרטים מופרסים גבוהים.', 'sk': 'Obravnavamo problem nenadzorovanega ekstraktivnega povzetka dokumentov, zlasti za dolge dokumente. Nenadzorovan problem modeliramo kot redko avtomatizirano regresijo in približujemo nastali kombinatorski problem preko konveksnega, normsko omejenega problema. Rešimo ga s pomočjo Frankovega algoritma. Če želite ustvariti povzetek s k stavki, mora algoritem izvesti le približno k iteracij, zaradi česar je zelo učinkovit za dolg dokument. Naš pristop ocenjujemo glede na dve drugi nenadzorovani metodi z uporabo tako leksikalnih (standardnih) ROUGE ocen kot tudi semantičnih (na osnovi vdelave). Naša metoda dosega boljše rezultate z obema naboroma podatkov in še posebej dobro deluje v kombinaciji z vdelavami za visoko parafrazirane povzetke.', 'bo': 'ང་ཚོས་རྗེས་སུ་འཇུག་བྱེད་མི་ཐུབ་པའི་ཁྱད་དུ་འཕགས་རིས་ཡིག་གེའི་བཅུད་སྡུད་དུ་གཏོང་བ ང་ཚོས་རང་བཞིན་གྱིས་བསམ་བཤེར་གྱི་དཀའ་ངལ་སྤྲོད་དགོས་མེད་པའི་དཀའ་ངལ་གཅིག ང་ཚོས་དེ་ལ་དམིགས་འཛུགས་ཅན་གྱི་སྐྱེས་ཆེན་བོལ་གྱི་སྒྲིག་སྟངས་ཞིག་ཡིན་པ་རེད། འདི་ལྟ་བུའི་ཚིག་རྟགས་དང་k ཡིག་རྟགས་ཀྱི་བཅུད་སྡུད་ཞིག་གསར་བཟོ་བྱེད་དགོས་པ We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. ང་ཚོའི་ཐབས་ལམ'}
{'en': 'A Two-stage Model for Slot Filling in Low-resource Settings : Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings', 'ar': 'نموذج من مرحلتين لملء الفتحات في الإعدادات منخفضة الموارد: الحد من غير المحدد بالمجال والتضمينات السياقية المحددة مسبقًا', 'fr': 'Un modèle en deux étapes pour le remplissage des emplacements dans les environnements à faibles ressources\xa0: réduction indépendante du domaine sans slot et intégrations contextuelles préentraînées', 'es': 'Un modelo de dos etapas para el llenado de ranuras en entornos de bajos recursos: reducción sin ranuras independiente del dominio e incrustaciones contextuales preentrenadas', 'pt': 'Um modelo de dois estágios para preenchimento de slot em configurações de poucos recursos: redução sem slot independente de domínio e incorporações contextuais pré-treinadas', 'ja': '低リソース設定におけるスロット充填のための2段階モデル：ドメイン非依存のノンスロット削減と事前に訓練されたコンテキスト埋め込み', 'zh': '于低资源置中充插槽两段:与域无关者非插槽缩预练之上下文嵌之', 'hi': 'कम-संसाधन सेटिंग्स में स्लॉट भरने के लिए एक दो-चरण मॉडल: डोमेन-अज्ञेयवादी गैर-स्लॉट कमी और Pretrained प्रासंगिक एम्बेडिंग', 'ru': 'Двухступенчатая модель заполнения слотов в малоресурсных настройках: Доменно-диагностическое сокращение без слотов и предварительно обученные контекстные вставки', 'ga': 'Samhail Dhá Chéim le haghaidh Sliotáin a Chomhlánú i Socruithe Ísealacmhainne: Laghdú Neamh-sliotán Agnostic Fearainn agus Leabaithe Comhthéacsúla Réamhthraenáilte', 'hu': 'Kétlépcsős modell az alacsony erőforrású beállításokban történő kitöltéshez: Domain-agnosztikus nem-slot csökkentés és előzetes kontextuális beágyazások', 'ka': 'Name', 'el': 'Ένα μοντέλο δύο σταδίων για την πλήρωση αυλακώσεων σε ρυθμίσεις χαμηλής περιεκτικότητας: Μείωση χωρίς αυλακώσεις και προκαθορισμένες ενσωμάτωση περιβάλλοντος', 'kk': 'Слотты төменгі ресурстарды толтыру үшін екі- этап үлгісі: Доменге агностикалық слот емес қысқарту және қарсы контексті ендіру', 'it': 'Un modello a due fasi per il riempimento delle slot nelle impostazioni a basso consumo di risorse: riduzione non slot agnostica del dominio e incorporazioni contestuali pretrained', 'mk': 'Две фази модел за пополнување на слотови со поставувања со ниски ресурси: намалување на слотови без домен и претренирани контекстни вградувања', 'lt': 'A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings', 'ms': 'Name', 'ml': 'കുറഞ്ഞ വിഭവങ്ങളുടെ സജ്ജീകരണങ്ങളില്\u200d സ്ലോട്ട് നിറയ്ക്കുന്നതിനുള്ള രണ്ട് സ്റ്റേജ് മോഡിള്\u200d: ഡൊമെയിന്\u200d അഗ്നോസ്റ്റിക്ക് സ്ല', 'no': 'Name', 'mt': 'Mudell f’żewġ stadji għall-Mili tas-Slots fl-Istituzzjonijiet b’Riżorsi Bażi: Tnaqqis ta’ Domain-agnostic Non-slots u Embeddings Contextual Pretrained', 'mn': 'Slot Filling in Low-Resource Settings хоёр дахь загвар: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings', 'pl': 'Dwustopniowy model wypełniania gniazd w ustawieniach niskiej ilości zasobów: redukcja bez gniazd domeny i wstępnie przeszkolone osadzenia kontekstowe', 'sr': 'Model dva faze za ispunjavanje slota u nizim resursima: smanjenje domena agnostika bez slota i pretisnute kontekstualne integracije', 'so': 'A Two stage for Slot buuxin in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Content Embeds', 'sv': 'En tvåstegsmodell för fyllning av spelautomater i lågresursinställningar: Domänagnostisk icke-spelautomatsreduktion och förminskad kontextuell inbäddning', 'ro': 'Un model în două etape pentru umplerea sloturilor în setări cu resurse reduse: reducerea non-sloturilor agnostice de domeniu și încorporarea contextuală pretextuală', 'si': 'Name', 'ur': 'Name', 'ta': 'Name', 'vi': 'Mô hình hai giai đoạn cho độ dốc khai lắp vào thiết lập ít tài nguyên:', 'uz': 'Name', 'bg': 'Двуетапен модел за запълване на слотове в настройки с ниски ресурси: Намаляване на домейна без слотове и предварително тренирани контекстни вграждания', 'hr': 'Model dva stupnja za ispunjavanje slota u nizim resursima: smanjenje domena agnostičkih nedostatka i suštinskih kontekstnih uključenja', 'da': 'En totrinsmodel til udfyldning af slot i indstillinger med lav ressource: Domæneagnostisk ikke-slot reduktion og prætrænede kontekstuelle indlejringer', 'nl': 'Een tweetrapsmodel voor het vullen van sleuven in instellingen met weinig bronnen: domeinagnostische reductie zonder sleuven en vooraf getrainde contextuele insluitingen', 'de': 'Ein zweistufiges Modell für die Slot-Füllung in ressourcenarmen Einstellungen: domänennagnostische Reduktion ohne Slot und vorgetrainierte kontextuelle Einbettungen', 'ko': '저자원 환경에서 틈새 충전의 2단계 모델: 분야와 무관한 비틈새 축소와 예훈련의 상하문 삽입', 'id': 'Model Dua tahap untuk Penisi Slot dalam Pengaturan sumber daya rendah: Pengurangan Domain-agnostik Non-slot dan Pencampuran Konteksual Berlatihan', 'fa': 'یک مدل دو مرحله برای پر کردن Slot در تنظیمات منابع پایین: کاهش\u200cسازی دامنه\u200cهای غیر محل و تنظیمات موقعیت\u200cهای پیش\u200cگیری', 'af': 'Name', 'sw': 'Mradi wa viwanja viwili wa Ujadala unajaza katika Vituo vya chini vya rasilimali: Upunguzo wa Uchunguzi wa Nyumbani na Mafuriko ya Ndani', 'tr': 'Slot Filling in Low-resource Settings for A Two-stage Model: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings', 'sq': 'Një model dy-fazor për mbushjen e kohëve në rregullimet e burimeve të ulëta: Shkurtimi jo-kohëve në domain-agnostik dhe përfshirjet kontekstuale të parastërvitura', 'hy': 'A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings', 'am': 'ምርጫዎች', 'bn': 'নিম্ন- সম্পদের বৈশিষ্ট্যাবলীতে স্লোট পূরণের জন্য একটি দুই মোডেল: ডোমেইন- অ্যাঙ্কোস্টিক না স্ল্যাট কমানো এবং প্রশিক্ষিত বিদ্যমান এ', 'az': 'Slot Filling in Low-resource Settings - Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings', 'ca': "Un model de dos etapes de rellenç d'intervals en configuracions de baix recursos: Reducció d'intervals de domini-agnòstics sense intervals i incorporacions contextuals pretrained", 'bs': 'Model za ispunjavanje slota u nizim resursima: smanjenje domena-agnostičkih nedostatka i suosjećajnih kontekstualnih integracija', 'et': 'Kaheastmeline mudel teenindusaegade täitmiseks vähese ressursiga seadetes: domeenist sõltumatu teenindusaegade vähendamine ja eelnevad kontekstipõimimised', 'fi': 'Kaksivaiheinen malli lähtöpaikkojen täyttämiseen vähävaraisissa asetuksissa: verkkotunnus-agnostinen ei-lähtöpaikkojen vähentäminen ja ennalta ohjatut kontekstuaaliset upotukset', 'cs': 'Dvoustupňový model pro vyplňování slotů v nastavení s nízkými zdroji: redukce bez slotů a předtrénované kontextové vložení', 'he': 'דוגמא בשתי שלבים למלאי ארגונים במערכות משאבים נמוכים: חיסום ארגוני-דומין ללא ארגונים', 'sk': 'Dvostopenjski model za polnjenje rež v nastavitvah z majhnimi viri: zmanjšanje brez rež, ki ni odvisno od domene in predhodno uveljavljene kontekstualne vdelave', 'ha': '@ action', 'jv': 'A Two-phase model for slot filling in low-source settings: domain-ageostik Not-slot Refruction and Preshined contextual embedding', 'bo': 'Slot Filling in Low-resource Settings ཡི་སྒྲིག་འཛུགས་ཀྱི་ཚད་ལྡོག་གཉིས་མཚམས་Model: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings'}
{'en': 'Learning-based slot filling-a key component of spoken language understanding systems-typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.', 'ar': 'عادةً ما يتطلب ملء الفتحات القائمة على التعلم - وهو مكون رئيسي لأنظمة فهم اللغة المنطوقة - قدرًا كبيرًا من البيانات ذات العلامات اليدوية في المجال للتدريب. في هذه الورقة ، نقترح بنية نموذجية جديدة من مرحلتين يمكن تدريبها باستخدام عدد قليل من الأمثلة الموضحة يدويًا في المجال. تم تصميم الخطوة الأولى لإزالة الرموز المميزة غير ذات الفتحات (على سبيل المثال ، الرموز المميزة التي تحمل علامة O) ، لأنها تُدخل ضوضاء في مدخلات نماذج تعبئة الفتحات. هذه الخطوة غير محددة المجال وبالتالي يمكن تدريبها من خلال استغلال البيانات خارج المجال. تحدد الخطوة الثانية أسماء الفتحات فقط للرموز المميزة للفتحات باستخدام أحدث عمليات التضمين السياقية المحددة مسبقًا مثل ELMO و BERT. نوضح أن نهجنا يتفوق في الأداء على الأنظمة الحديثة الأخرى في مجموعة البيانات المعيارية SNIPS.', 'fr': "Le remplissage des créneaux basé sur l'apprentissage - un élément clé des systèmes de compréhension de la langue parlée - nécessite généralement une grande quantité de données étiquetées manuellement dans le domaine pour la formation. Dans cet article, nous proposons une nouvelle architecture de modèle en deux étapes qui peut être entraînée avec seulement quelques exemples étiquetés manuellement dans le domaine. La première étape est conçue pour supprimer les jetons sans emplacement (c'est-à-dire les jetons marqués O), car ils introduisent du bruit dans l'entrée des modèles de remplissage d'emplacements. Cette étape est indépendante du domaine et peut donc être entraînée en exploitant des données hors domaine. La deuxième étape identifie les noms des emplacements uniquement pour les jetons de machine à sous en utilisant des intégrations contextuelles préentraînées de pointe, telles que ELMO et BERT. Nous montrons que notre approche surpasse les autres systèmes de pointe sur l'ensemble de données de référence SNIPS.", 'es': 'El llenado de espacios basado en el aprendizaje, un componente clave de los sistemas de comprensión del lenguaje oral, generalmente requiere una gran cantidad de datos etiquetados a mano en el dominio para la capacitación. En este artículo, proponemos una arquitectura de modelo novedosa de dos etapas que se puede entrenar con solo unos pocos ejemplos etiquetados a mano en el dominio. El primer paso está diseñado para eliminar los tokens sin ranura (es decir, los tokens con etiqueta O), ya que introducen ruido en la entrada de los modelos de llenado de ranuras. Este paso es independiente del dominio y, por lo tanto, se puede entrenar aprovechando los datos fuera del dominio. El segundo paso identifica los nombres de tragamonedas solo para los tokens de tragamonedas mediante el uso de incorporaciones contextuales preentrenadas de última generación, como ELMO y BERT. Demostramos que nuestro enfoque supera a otros sistemas de última generación en el conjunto de datos de referencia de SNIPS.', 'pt': 'O preenchimento de slots baseado em aprendizado - um componente-chave dos sistemas de compreensão da linguagem falada - normalmente requer uma grande quantidade de dados rotulados à mão no domínio para treinamento. Neste artigo, propomos uma nova arquitetura de modelo de dois estágios que pode ser treinada com apenas alguns exemplos rotulados à mão no domínio. A primeira etapa é projetada para remover tokens que não são de slot (ou seja, tokens rotulados com O), pois eles introduzem ruído na entrada de modelos de preenchimento de slot. Esta etapa é independente de domínio e, portanto, pode ser treinada explorando dados fora do domínio. A segunda etapa identifica nomes de slots apenas para tokens de slots usando embeddings contextuais pré-treinados de última geração, como ELMO e BERT. Mostramos que nossa abordagem supera outros sistemas de última geração no conjunto de dados de benchmark SNIPS.', 'ja': '口語理解システムの重要なコンポーネントである学習ベースのスロット充填は、通常、トレーニングのために大量のドメイン内ハンドラベルデータを必要とします。本稿では、わずかなドメイン内ハンドラベルの例だけで訓練できる新規の2段階モデルアーキテクチャを提案する。第１のステップは、非スロットトークン（すなわち、Ｏラベルトークン）がスロット充填モデルの入力にノイズを導入するときに除去するように設計される。このステップはドメインに依存しないため、ドメイン外のデータを利用してトレーニングできます。2番目のステップでは、ELMOやBERTなどの最先端の事前トレーニングされたコンテキスト埋め込みを使用して、スロットトークンのスロット名のみを識別します。私たちのアプローチは、SNIPSベンチマークデータセットの他の最先端のシステムよりも優れていることを示しています。', 'zh': '盖学插槽填充 - 口语解统之要组成部分 - 常须大域内手动标记之数以教习之。 新颖两架构,架构用数域手动标示例训练。 第一步旨删非槽标记(即O标记),以其会于槽填模之输入中引入噪声。 此步骤与域无关,故可因域数训练。 第二步惟用最先进预练上下文嵌(如 ELMO 、 BERT)以识槽令牌之名。 吾法优于SNIPS准数集上其余最先进之统。', 'hi': 'लर्निंग-आधारित स्लॉट भरने - बोली जाने वाली भाषा समझने वाली प्रणालियों का एक प्रमुख घटक - आमतौर पर प्रशिक्षण के लिए बड़ी मात्रा में इन-डोमेन हैंड-लेबल डेटा की आवश्यकता होती है। इस पेपर में, हम एक उपन्यास दो-चरण मॉडल आर्किटेक्चर का प्रस्ताव करते हैं जिसे केवल कुछ इन-डोमेन हाथ से लेबल किए गए उदाहरणों के साथ प्रशिक्षित किया जा सकता है। पहला कदम गैर-स्लॉट टोकन (यानी, ओ लेबल टोकन) को हटाने के लिए डिज़ाइन किया गया है, क्योंकि वे स्लॉट भरने वाले मॉडल के इनपुट में शोर पेश करते हैं। यह चरण डोमेन-अज्ञेयवादी है और इसलिए, आउट-ऑफ-डोमेन डेटा का शोषण करके प्रशिक्षित किया जा सकता है। दूसरा चरण केवल स्लॉट टोकन के लिए स्लॉट नामों की पहचान करता है, जैसे कि ELMO और BERT जैसे अत्याधुनिक पूर्वप्रशिक्षित प्रासंगिक एम्बेडिंग का उपयोग करके। हम दिखाते हैं कि हमारा दृष्टिकोण SNIPS बेंचमार्क डेटासेट पर अन्य अत्याधुनिक प्रणालियों को मात देता है।', 'ru': 'Заполнение слотов на основе обучения - ключевой компонент систем понимания разговорного языка - обычно требует большого количества данных, помеченных вручную в домене, для обучения. В этой статье мы предлагаем новую двухступенчатую модельную архитектуру, которая может быть обучена только с несколькими примерами в домене с ручной маркировкой. Первый этап предназначен для удаления токенов, не помеченных слотом (т.е. токенов с меткой O), поскольку они создают шум при вводе моделей заполнения слотов. Этот этап является доменно-диагностическим и, следовательно, может быть обучен с использованием внедоменных данных. Второй шаг идентифицирует имена слотов только для токенов слотов с помощью самых современных предварительно обученных контекстных вложений, таких как ELMO и BERT. Мы показываем, что наш подход превосходит другие современные системы по эталонному набору данных SNIPS.', 'ga': 'Go hiondúil teastaíonn méid mór sonraí lámh-lipéadaithe le haghaidh oiliúna chun sliotán atá bunaithe ar fhoghlaim a líonadh - príomh-chomhpháirt de chórais tuisceana teanga labhartha. Sa pháipéar seo, molaimid ailtireacht samhail dhá chéim nua ar féidir a oiliúint gan ach roinnt samplaí in-fearainn lipéadaithe láimhe. Tá an chéad chéim deartha chun comharthaí neamh-sliotán a bhaint (i.e., O comharthaí lipéadaithe), toisc go dtugann siad torann isteach in ionchur samhlacha líonadh sliotán. Céim agnostic fearainn atá sa chéim seo agus mar sin is féidir í a oiliúint trí leas a bhaint as sonraí lasmuigh den fhearann. Sainaithnítear sa dara céim ainmneacha sliotán do chomharthaí sliotán amháin trí úsáid a bhaint as leabaithe comhthéacsúla réamhoilte den scoth ar nós ELMO agus BERT. Léirímid go sáraíonn ár gcur chuige na córais úrscothacha eile ar thacar sonraí tagarmharcála SNIPS.', 'el': 'Η συμπλήρωση αυλακώσεων με βάση τη μάθηση αποτελεί βασικό συστατικό των συστημάτων κατανόησης της προφορικής γλώσσας που συνήθως απαιτεί μεγάλη ποσότητα δεδομένων που φέρουν το χέρι στο πεδίο της εκπαίδευσης. Σε αυτή την εργασία, προτείνουμε μια νέα αρχιτεκτονική μοντέλου δύο σταδίων που μπορεί να εκπαιδευτεί με λίγα μόνο παραδείγματα αυτοτελούς. Το πρώτο βήμα έχει σχεδιαστεί για να αφαιρέσει μάρκες μη-αυλακώσεων (δηλ. μάρκες με ετικέτα Ο), καθώς εισάγουν θόρυβο στην είσοδο των μοντέλων πλήρωσης αυλακώσεων. Αυτό το βήμα είναι αγνωστικό τομέα και ως εκ τούτου, μπορεί να εκπαιδευτεί με την εκμετάλλευση δεδομένων εκτός τομέα. Το δεύτερο βήμα προσδιορίζει ονόματα κουλοχέρηδων μόνο για μάρκες κουλοχέρηδων χρησιμοποιώντας προηγμένες προκαθορισμένες ενσωματώσεις περιβάλλοντος όπως ELMO και BERT. Δείχνουμε ότι η προσέγγισή μας ξεπερνά τα άλλα συστήματα τελευταίας τεχνολογίας στο σύνολο δεδομένων αναφοράς SNIPS.', 'ka': 'სწავლების დაფართებული სიტყვის დაყენება - სიტყვის გასაგების კონომპონენტი სისტემების გასაგების კონომპონენტი - ტიპულად უნდა დიდი დიომინული მონაცემები ამ დომენში, ჩვენ მივეღებთ პრომენტის ორ ფაეჯის მოდელური აქტიქტურაცია, რომელიც შეიძლება განაკეთება მხოლოდ რამდენიმე მაგალითან მაგალითან, რომელიც მოგვეძლი პირველი კონფიგურაცია მოქმედება, როდესაც ისინი ჩვენებენ ბოლოტის მოდელების შეტყობინებაში. ეს მონაცემები დიომინი-ადნოსტიურია და ამიტომ, შეიძლება იყოს მონაცემების გამოყენებით. მეორე კონტექსტური სახელი განსაზღვრდება მხოლოდ სლოტის სახელისთვის, როგორც ELMO და BERT გამოყენებით. ჩვენ ჩვენი პროგორმა გავაკეთებთ სხვა სახელსაწყისი სისტემებზე, რომელიც SNIPS ბენქმარკის მონაცემების სახელსაწყისათვის.', 'it': "Il riempimento di slot basato sull'apprendimento - una componente chiave dei sistemi di comprensione della lingua parlata - richiede in genere una grande quantità di dati etichettati manualmente nel dominio per la formazione. In questo articolo, proponiamo una nuova architettura modello a due stadi che può essere addestrata con solo pochi esempi in-domain-label manualmente. Il primo passo è stato progettato per rimuovere i token non slot (cioè i token etichettati O), in quanto introducono rumore nell'ingresso dei modelli di riempimento slot. Questo passaggio è agnostico del dominio e quindi può essere addestrato sfruttando dati fuori dominio. Il secondo passo identifica i nomi delle slot solo per i token slot utilizzando incorporazioni contestuali pre-addestrate all'avanguardia come ELMO e BERT. Dimostriamo che il nostro approccio supera altri sistemi all'avanguardia sul set di dati di benchmark SNIPS.", 'kk': 'Оқыту негіздеген слотты толтыру - сөйлейтін тілдерді түсіну жүйелерінің кілт компоненті - кәдімгі, домендегі қолмен жарлық деректер үшін үлкен мәліметтерді талап етеді. Бұл қағазда, біз романдың екі этап үлгі архитектурасын таңдаймыз. Бұл доменде тек бірнеше мәселелерден оқылған. Бірінші қадам слот емес белгілерді өшіру үшін құрылған (т.е. O белгілері) слот толтыру үлгілерінің келтірілгенде дыбыс келтіріледі. Бұл қадам - доменге агностикалық, сондықтан доменге шығыс деректерді қолдануға болады. Екінші қадам, ELMO және BERT секілді контексті ендіруді қолдану үшін тек слот белгілерінің слот атауын анықтайды. Біз SNIPS бағдарламаларының бағдарламаларының басқа әртүрлі жүйелерін жасайды.', 'hu': 'A tanulásalapú rések kitöltése - a beszélt nyelv megértésének kulcsfontosságú eleme - általában nagy mennyiségű, kézzel címkézett, tartományon belüli adatot igényel a képzéshez. Ebben a tanulmányban egy új, kétlépcsős modellépítészetet javasolunk, amely csak néhány domain kézzel jelölt példával képezhető. Az első lépés célja, hogy eltávolítsa a nem slot tokeneket (azaz O jelzésű tokeneket), mivel zajt vezetnek be a slot töltő modellek bevitelében. Ez a lépés domain-agnosztikus, ezért a domain kívüli adatok kihasználásával képezhető. A második lépés a nyerőgépek nevét csak a nyerőgépek tokenjeinek azonosítja a legkorszerűbb, előkészített kontextuális beágyazásokkal, mint például az ELMO és a BERT. Megmutatjuk, hogy megközelítésünk felülmúlja a SNIPS benchmark adatkészleten más korszerű rendszereket.', 'mk': 'Заполнувањето на слотови со основа на учењето - клучен компонент на системите за разбирање на говорниот јазик - обично бара голема количина рачно означени податоци за обука. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples.  Првиот чекор е дизајниран со цел да се отстранат симболи без слотови (т.е., симболи означени со O), бидејќи тие вложуваат бука во внесувањето на модели за полнување на слотови. Овој чекор е домен-агностичен и затоа може да се тренира со искористување на податоци надвор од домен. Вториот чекор ги идентификува имињата на локациите само за локациите со користење на најновите контекстни вградувања како што се ЕЛМО и БЕРТ. Ние покажуваме дека нашиот пристап е поголем од другите најсовремени системи на податоците од СНИПС.', 'ms': 'Penisian slot berdasarkan belajar - komponen kunci sistem pemahaman bahasa bercakap - biasanya memerlukan sejumlah besar data ditabel-tangan dalam domain untuk latihan. Dalam kertas ini, kami cadangkan arkitektur model dua tahap novel yang boleh dilatih hanya dengan beberapa contoh ditabel tangan dalam domain. Langkah pertama direka untuk membuang token bukan slot (iaitu token labeled O), kerana ia memperkenalkan bunyi dalam input model penuhian slot. Langkah ini adalah domain-agnostic dan oleh itu, boleh dilatih dengan mengeksploitasi data luar domain. Langkah kedua mengenalpasti nama slot hanya untuk token slot dengan menggunakan bentuk kontekstual yang dilatih-dahulu state-of-the-art seperti ELMO dan BERT. Kami menunjukkan bahawa pendekatan kami lebih berharga daripada sistem-sistem terbaik lain pada set data benchmark SNIPS.', 'mn': 'суралцах сургалтын слотын дүүрэн - хэлний ойлголтын чухал компонент - ихэвчлэн суралцах хэрэгтэй маш их хэмжээний гартай өгөгдлийн хэрэгтэй. Энэ цаасан дээр бид хоёр дахь загварын загварын архитектурыг зөвхөн хэд хэдэн гартай жишээлүүдтэй суралцаж болно. Эхний алхам нь слотгүй тэмдэгтийг устгахад зохион байгуулагдсан (т.е. O тэмдэглэгдсэн тэмдэгтийг) слотын дүүргэх загварын орлуулалт дуу гаргадаг. Энэ алхам бол холбоотой агностик, учир нь холбоотой мэдээллийг ашиглаж болох юм. Хоёр дахь алхам нь зөвхөн слотын тодорхойлолтын слотын нэрлүүдийг илэрхийлж, ELMO болон BERT зэрэг урлагийн хувьсгал хөдлөлүүдийг ашиглаж байдаг. Бид өөрсдийн арга хэмжээ нь СNIPS банк өгөгдлийн сангийн бусад урлагийн системүүдийг дамжуулдаг гэдгийг харуулж байна.', 'lt': 'Mokymosi sąlygomis grindžiamas laiko tarpsnių užpildymas – pagrindinis kalbos supratimo sistemų komponentas – paprastai reikalauja didelio masto rankiniu ženklu pažymėtų duomenų mokymui. Šiame dokumente siūlome naują dviejų etapų modelio architektūrą, kuri gali būti mokoma tik su keliais srities rankiniu ženklu pažymėtais pavyzdžiais. Pirmasis žingsnis skirtas pašalinti ne laiko tarpsnių ženklus (t. y. ženklinamus O ženklais), nes jie sukelia triukšmą laiko tarpsnių užpildymo modeliuose. Šis žingsnis yra domininis, todėl jis gali būti mokomas naudojant ne domininius duomenis. Antrasis žingsnis nustato laiko tarpsnių pavadinimus tik laiko tarpsnių ženklams, naudojant naujausius iš anksto parengtus kontekstinius įdėjimus, pvz., ELMO ir BERT. Mes rodome, kad mūsų požiūris viršija naujausias SNIPS lyginamųjų duomenų rinkinio sistemas.', 'ml': 'പഠിക്കുന്നതിന്റെ അടിസ്ഥാനത്തുള്ള സ്ലോട്ട് നിറയ്ക്കുന്നത് - സംസാരിക്കുന്ന ഭാഷ ബുദ്ധിമുട്ടുകളുടെ ഒരു കീ ഘടകം- സാധാരണയ ഈ പത്രത്തില്\u200d, നമ്മള്\u200d രണ്ട് സ്റ്റേഡ് മോഡല്\u200d ആര്\u200dക്കിട്ടറിക്കാന്\u200d പ്രാദോധിപ്പിക്കുന്നു. അത് കുറച്ച് മാത്രമേ ഡോമെയിനില്\u200d  സ്ലോട്ട് അടയാളങ്ങള്\u200d നീക്കം ചെയ്യാന്\u200d ആദ്യത്തെ പടി നിര്\u200dമ്മിക്കപ്പെട്ടിരിക്കുന്നു. സ്ലോട്ട് നിറയ്ക്കുന്ന മോഡലുകളില്\u200d ശബ ഈ പടി ഡൊമൈന്\u200d അഗ്നോസ്റ്റിക്കാണ്. അതുകൊണ്ട്, ഡൊമെയിനില്\u200d നിന്നും ഡേറ്റാ ഉപയോഗിക്കാന്\u200d പരിശീലിക്കാം. രണ്ടാമത്തെ പടി സ്ലോട്ട് ചിഹ്നങ്ങള്\u200dക്ക് മാത്രമേ സ്ലോട്ട് അടയാളങ്ങള്\u200dക്കുള്ളൂ എന്ന് നിരീക്ഷിക്കുന്നുള്ളൂ. ഞങ്ങള്\u200d കാണിക്കുന്നത് നമ്മുടെ അടുത്തുനിന്ന് സ്റ്റേറ്റ് ഓഫ് കലാകാര്യ സിസ്റ്റം പ്രവര്\u200dത്തിപ്പിക്കുന്നു', 'mt': 'Il-mili ta’ slots ibbażati fuq it-tagħlim – komponent ewlieni tas-sistemi ta’ fehim tal-lingwa mitkellma – tipikament jeħtieġ ammont kbir ta’ dejta ttikkettata bl-idejn fid-dominju għat-taħriġ. F’dan id-dokument, qed nipproponu arkitettura mudell ġdid ta’ żewġ stadji li tista’ tiġi mħarrġa b’ftit biss eżempji ttikkettati bl-idejn fid-dominju. L-ewwel pass huwa ddisinjat biex in e ħħi tokens li mhumiex slots (jiġifieri tokens ittikkettati b’O), billi jintroduċu storbju fl-input ta’ mudelli ta’ mili ta’ slots. Dan il-pass huwa agnostiku fid-dominju u għalhekk jista’ jitħarreġ billi tiġi sfruttata dejta barra mid-dominju. It-tieni pass jidentifika l-ismijiet tas-slots biss għat-tokens tas-slots billi juża inkorporazzjonijiet kontekstwali mħarrġa minn qabel l-aktar avvanzati bħall-ELMO u l-BERT. Aħna nuru li l-approċċ tagħna huwa aktar avvanzat minn sistemi moderni oħra dwar is-sett ta’ dejta ta’ referenza tal-SNIPS.', 'sr': 'Zapunjenje slot a na učenju - ključni komponent rečenih jezičkih sustava razumevanja - obično zahteva veliku količinu podataka u domenu, označenih rukama za obuku. U ovom papiru predlažemo romansku modelsku arhitekturu koja može biti obučena samo sa nekoliko primjera u domenu na rukama. Prvi korak je dizajniran za uklanjanje znakova bez slota (tj. O znakova označene), dok predstavljaju buku u ulazu modela punjenja slota. Ovaj korak je domene-agnostičan i stoga se može obučiti iskorištavanjem podataka iz domena. Drugi korak identifikuje ime slota samo za znakove slota koristeći suštinske suštine kao što su ELMO i BERT. Pokazujemo da naš pristup nadmašuje drugim sistemima umetnosti na snimku podataka SNIPS-a.', 'no': 'Fylling av læringsbasert plass - ein nøkkelkomponent av språksforstørringssystemet - vanlegvis krev stor mengd data med håndmerket i domenet for opplæring. I denne papiret foreslår vi eit roman to stadige modellarkitektur som kan trennast med berre noen eksemplar med håndmerket i domenet. Den første stegen er designert for å fjerna ikkje- plass- teikn (t.d. O merkelige teikn), sidan dei introduserer støy i inndata av plass- fyllmodeller. Denne stegen er domeneagnostisk, og derfor kan utlærast ved å bruka ut- av- domenedata. Den andre stegen identifiserer berre slottnamn for slottteikn ved å bruka tilstanden til kunstøtte område kontekstinnbygging som ELMO og BERT. Vi viser at tilnærminga vårt utfører andre kunstsystemer på SNIPS-benchmarkdatasettet.', 'pl': 'Wypełnianie slotów opartych na uczeniu się jest kluczowym elementem systemów rozumienia języka mówionego, które zazwyczaj wymaga dużej ilości ręcznie oznakowanych danych w domenie do szkolenia. W niniejszym artykule proponujemy nową dwustopniową architekturę modelu, którą można trenować na podstawie kilku ręcznie oznakowanych przykładów w domenie. Pierwszy krok ma na celu usunięcie tokenów innych niż sloty (tj. tokenów oznaczonych O), ponieważ wprowadzają one szumy w wejściu modeli wypełniania slotów. Ten krok jest agnostyczny dla domeny i dlatego można go szkolić poprzez wykorzystanie danych poza domeną. Drugi krok identyfikuje nazwy slotów tylko dla tokenów slotów za pomocą najnowocześniej przeszkolonych kontekstowych osadzeń, takich jak ELMO i BERT. Pokazujemy, że nasze podejście przewyższa inne najnowocześniejsze systemy w zbiorze danych referencyjnym SNIPS.', 'si': 'ඉගෙනගන්න- අධාරිත ස්ලෝට් පුරවන්න - කතා කරපු භාෂාව තේරුම් පද්ධතියේ යතුරු කොටසක් - සාමාන්\u200dය විශාලිත විශ මේ පත්තරේ අපි ප්\u200dරශ්නයක් කරන්නේ ප්\u200dරශ්නයක් දෙකක් ප්\u200dරශ්නයක් තියෙන්නේ. ඒක ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් විතරයි ප්\u200d පළමු පැත්තේ ස්ලෝට් නොන් ටෝකන්ස් විස්තර කරලා තියෙන්නේ (ඉතින්, O ලේබල් කරපු ටෝකන්ස්), ඔවුන් ස්ලෝට් පුරවන් මොඩ මේ පැත්තේ ඩෝමේන්ස්ටික්-agnoස්ටික්, ඉතින්, ප්\u200dරශ්නය කරන්න පුළුවන් ඩෝමේන්ස් දත්ත ප්\u200dරයෝජනය කරන දෙවෙනි පැත්තේ ස්ලෝට් නම් ස්ලෝට් ටෝකෙන්ස් විතරයි ස්ථිතිය-of-the-art ප්\u200dරීට්\u200dරේන්ස් සංවිධානය සඳහා ELMO සහ BERT වගේ  අපි පෙන්වන්නේ අපේ ප්\u200dරවේශනය SNIPS බෙන්ච්මාර්ක් දත්ත සෙට්ටුවේ අනිත් ස්ථානයේ ක්\u200dරියාත්මක පද්ධතිය', 'ro': 'Completarea sloturilor bazate pe învățare - o componentă cheie a sistemelor de înțelegere a limbilor vorbite - necesită, de obicei, o cantitate mare de date etichetate manual în domeniu pentru formare. În această lucrare, propunem o arhitectură nouă de model în două etape, care poate fi instruită cu doar câteva exemple în domeniu etichetate manual. Primul pas este conceput pentru a elimina jetoanele non-slot (adică jetoanele etichetate O), deoarece acestea introduc zgomot în introducerea modelelor de umplere slot. Acest pas este agnostic domeniului și, prin urmare, poate fi instruit prin exploatarea datelor din afara domeniului. Al doilea pas identifică numele sloturilor numai pentru jetoanele sloturilor utilizând încorporări contextuale de ultimă generație, precum ELMO și BERT. Aratăm că abordarea noastră depășește alte sisteme de ultimă generație din setul de date de referință SNIPS.', 'so': 'Sida caadiga ah waxaa loo baahan yahay macluumaad badan oo gacanta lagu qoray gudaha. Kanu warqaddan waxaan ka soo jeedaynaa dhismaha muusikada labada darajad, kaas oo lagu baran karo tusaalooyin yar oo gudaha ku qoran. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models.  Stepigan waa danbiilka gudaha, sababtoo darteed waxaa lagu baran karaa macluumaad ka baxsan danaha domain. Xarunta labaad ayaa magacyada saqafka looga yaqaan calaamadaha saqafka oo keliya oo lagu isticmaalayo wadamada-hore oo la mid ah sida ELMO iyo BERT. Waxaynu muujinnaa in dhaqdhaqaalahayagu uu ka muujiyo nidaam kale oo xaalad farshaxan ah oo ku qoran SNIPS-ka.', 'sv': 'Inlärningsbaserad fyllning av platser – en viktig komponent i system för språkförståelse – kräver vanligtvis en stor mängd handmärkta data inom domänen för utbildning. I denna uppsats föreslår vi en ny tvåstegs modellarkitektur som kan tränas med endast ett fåtal handmärkta exempel inom domänen. Det första steget är utformat för att ta bort icke-slots tokens (dvs O-märkta tokens), eftersom de introducerar buller i inmatningen av slots fyllningsmodeller. Detta steg är domänagnostiskt och kan därför utbildas genom att utnyttja data utanför domänen. Det andra steget identifierar slots namn endast för slots tokens genom att använda state-of-the-art förbelagda kontextuella inbäddningar som ELMO och BERT. Vi visar att vårt tillvägagångssätt överträffar andra state-of-art system på SNIPS benchmark datauppsättningen.', 'ta': 'கற்றுக் கொண்டிருக்கும் செருகுவாய் நிரப்பும் - பேச்சு மொழி புரிந்து கொள்ளும் முறைமைகளின் விசை பொருள் - பயிற்சிக்கு வழ In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples.  முதல் படி செருகுவாய்ப்பு குறியீடுகளை நீக்க வடிவமைக்கப்பட்டுள்ளது (அதாவது, O சுட்டியாக்கப்பட்ட குறியீடுகள்) செருகுவாய் நிரப்பும இந்த படிப்பு டோமைன் குறிப்பாக இருக்கும் அதனால் டோமைன் தகவலை பயன்படுத்தி பயிற்சி செய்யலாம். இரண்டாவது படி சுருக்குப் பெயர்களை மட்டும் சுருக்குக் குறிகளுக்கு மட்டும் கண்டுபிடிக்கும் நாங்கள் எங்கள் அணுக்கம் மற்ற நிலையில் கலை அமைப்புகளை செயல்படுத்துகிறது என்று காட்டுகிறோம் SNIPS பென்மார்க', 'ur': 'سیکھنے کی بنیادی اسلوٹ بھرنی - بات کی زبان سمجھنے کی سیسٹم کی ایک کلی رقم - عمدہ طور پر دومین کے ہاتھ لیبل ڈاٹ کی بہت بڑی مقدار کی ضرورت ہے اس کاغذ میں، ہم ایک کاغذ دو سٹیج موڈل معماری پیشنهاد کرتے ہیں جو صرف تھوڑے مثال کے ساتھ سکھائی جاتی ہے جن کو ڈومین میں ہاتھ لیبل کیا جاتا ہے. پہلی قدم کی طراحی کی گئی ہے کہ non-slot tokens (یعنی O labeled tokens) کو ہٹانے کے لئے (یعنی O labeled tokens) بنایا گیا ہے، جب وہ اسلوت پر مدل کے اندر صوت پیش کرتی ہیں. This step is domain-agnostic and therefore, can be trained by using out-of-domain data. دوسری سٹم صرف اسلوٹ ٹوکنوں کے لئے اسلوٹ نام پہچان دیتا ہے، جیسے ELMO اور BERT کے استعمال سے۔ ہم دکھاتے ہیں کہ ہمارا تقریبا SNIPS بنچم مارک ڈیٹ سٹ پر دوسری ایٹ آرت سیستموں کو اضافہ کرتا ہے.', 'uz': "Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training.  Bu qogʻozda biz ikki darajadagi model arxituvchisi bilan faqat bir necha misol bilan o'rganish mumkin. Birinchi qadam, slot to ʻldirish modellarini olib tashlash (m- n yorliq belgini) yaratish mumkin. Bu qadam domen-agnostik, va shunday uchun domen maʼlumotni ishlatish mumkin. Ikkinchi qadam Biz buning usuli SNIPS benchmark maʼlumotlarida boshqa holat tizimini bajaradi.", 'vi': 'Việc điền vào ổ đĩa dựa vào giáo dục- một thành phần quan trọng của hệ thống hiểu ngôn ngữ nói- thường yêu cầu một lượng lớn các dữ liệu được đánh dấu bằng tay trong miền cho huấn luyện. Trong tờ giấy này, chúng tôi đề xuất một kiến trúc mẫu hai giai đoạn mới có thể được huấn luyện chỉ với vài trường hợp được đánh dấu bằng tay. Đầu tiên là bước được thiết kế để gỡ bỏ những vật thể không có khe (v.d., O nhãn các vật thể) khi chúng giới thiệu nhiễu trong các mẫu đi ền vào suất. Đây là miền-chẩn bị và do đó, có thể được huấn luyện bằng cách khai thác dữ liệu ngoài miền. Một bước thứ hai chỉ xác định tên hiệu suất chỉ cho thẻ thành công bằng cách sử dụng những cài ghép định dạng nền hình nền tiên nghệ như ElMO và BERT. Chúng tôi cho thấy phương pháp của chúng tôi vượt trội hơn những hệ thống thông tin cao cấp hơn.', 'bg': 'Попълването на слотове въз основа на ученето - ключов компонент на системите за разбиране на говоримия език - обикновено изисква голямо количество ръчно обозначени данни в областта за обучение. В тази статия предлагаме нова двуетапна архитектура на модела, която може да бъде обучена само с няколко примера, ръчно обозначени в домейна. Първата стъпка е предназначена да премахне токените, които не са слотове (т.е. токените с етикет О), тъй като те въвеждат шум в входа на моделите за пълнене на слотове. Тази стъпка е агностична към домейна и следователно може да бъде обучена чрез експлоатиране на данни извън домейна. Втората стъпка идентифицира имената на слотове само за слот жетони, като използва най-съвременни предварително обучени контекстуални вграждания като ЕЛМО и БЕРТ. Показваме, че нашият подход превъзхожда други съвременни системи в сравнителния набор от данни.', 'nl': 'Leergebaseerde slotvulling is een belangrijk onderdeel van systemen voor het begrijpen van gesproken taal en vereist meestal een grote hoeveelheid in-domain hand gelabelde gegevens voor training. In dit artikel stellen we een nieuwe tweetraps modelarchitectuur voor die getraind kan worden met slechts een paar in-domain hand gelabelde voorbeelden. De eerste stap is ontworpen om niet-slot tokens (d.w.z. O-gelabelde tokens) te verwijderen, omdat ze ruis veroorzaken in de invoer van slot vulling modellen. Deze stap is domein-agnostisch en kan daarom getraind worden door out-of-domain data te exploiteren. De tweede stap identificeert slotnamen alleen voor slottokens door gebruik te maken van state-of-the-art vooraf getrainde contextuele embeddings zoals ELMO en BERT. We laten zien dat onze aanpak beter presteert dan andere state-of-art systemen op de SNIPS benchmark dataset.', 'da': 'Indlæringsbaseret udfyldning af pladser - en vigtig komponent i systemer til talesprogforståelse - kræver typisk en stor mængde håndmærkede data inden for domænet til træning. I denne artikel foreslår vi en ny to-trins modelarkitektur, der kun kan trænes med få in-domæne håndmærkede eksempler. Det første trin er designet til at fjerne ikke-slot tokens (dvs. O mærkede tokens), da de introducerer støj i input af slot fyldningsmodeller. Dette trin er domæneagnostisk og kan derfor trænes ved at udnytte data uden for domænet. Det andet trin identificerer kun slots navne for slot tokens ved at bruge state-of-the-art forudtrænede kontekstuelle indlejringer som ELMO og BERT. Vi viser, at vores tilgang overgår andre state-of-art systemer på SNIPS benchmark datasættet.', 'hr': 'Zapunjenje slot a na temelju učenja - ključni komponent sustava razumijevanja govornih jezika - obično zahtijeva veliku količinu podataka u domenu označenih rukama za obuku. U ovom papiru predlažemo novu modelsku arhitekturu na dvije faze koja se može obučiti samo s nekoliko primjera u domenu na rukama. Prvi korak je dizajniran za uklanjanje znakova bez slota (tj. O označene znakove), dok predstavljaju buku u ulazu modela punjenja slota. Ovaj korak je domene-agnostičan i stoga se može obučavati iskorištavanjem podataka izvan domena. Drugi korak identificira imena slota samo za znakove slota koristeći kontekstne ugrađene stanje umjetnosti poput ELMO i BERT. Mi pokazujemo da naš pristup nadmašuje drugim sustavima stanja umjetnosti na snimku podataka SNIPS-a.', 'de': 'Lernbasierte Slot-Fﾃｼllung ist eine Schlﾃｼsselkomponente von Sprachverstﾃ､ndnissystemen, die normalerweise eine groﾃ歹 Menge an in-domain handbeschrifteten Daten fﾃｼr Schulungen erfordert. In diesem Beitrag schlagen wir eine neuartige zweistufige Modellarchitektur vor, die mit nur wenigen handbeschrifteten Beispielen trainiert werden kann. Der erste Schritt ist entworfen, um Nicht-Slot-Token (d. h. O-markierte Token) zu entfernen, da sie Rauschen in der Eingabe von Slot-Fﾃｼllungsmodellen verursachen. Dieser Schritt ist domain-agnostisch und kann daher durch Ausnutzung von Out-of-Domain-Daten trainiert werden. Der zweite Schritt identifiziert Slot-Namen nur fﾃｼr Slot-Token mithilfe modernster, vortrainierter kontextueller Einbettungen wie ELMO und BERT. Wir zeigen, dass unser Ansatz andere State-of-Art-Systeme im SNIPS Benchmark-Datensatz ﾃｼbertrifft.', 'id': 'Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training.  Dalam kertas ini, kami mengusulkan arsitektur model dua tahap novel yang dapat dilatih dengan hanya beberapa contoh yang ditabel tangan di domain. Langkah pertama dirancang untuk menghapus token bukan slot (i.e., token berlabel O), karena mereka memperkenalkan suara dalam masukan model penuh slot. Langkah ini adalah domain-agnostic dan oleh itu, dapat dilatih dengan mengeksploitasi data luar domain. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT.  Kami menunjukkan bahwa pendekatan kita melebihi sistem terbaik lainnya pada set data benchmark SNIPS.', 'fa': 'پر کردن نقطه\u200cهای بنیاد یادگیری - یک بخش کلید از سیستم\u200cهای درک زبانی صحبت می\u200cکند - معمولاً نیاز به اندازه اندازه\u200cای از داده\u200cهای با برچسب\u200cهای دست در دامن برای آموزش است. در این کاغذ، ما یک معماری مدل دو مرحله را پیشنهاد می کنیم که می تواند با فقط چند مثال با برچسب\u200cهای دست در دامنه آموزش داده شود. اولین قدم برای حذف نشانه\u200cهای غیر اسلوت (یعنی نشانه\u200cهای برچسب) طراحی شده است، در حالی که آنها صدا را در ورودی مدل پر کردن اسلوت معرفی می\u200cکنند. این مرحله دامنی-agnostic است و بنابراین می تواند با استفاده از داده های خارج از دامنی آموزش داده شود. مرحله دوم نام\u200cهای slot را فقط برای نشان\u200cهای slot با استفاده از استفاده از وضعیت\u200cهای موضوعی\u200cهای پیش\u200cآمیز هنری مثل ELMO و BERT شناسایی می\u200cکند. ما نشان می دهیم که دستور ما به سیستم های دولتی هنر روی مجموعه داده های نقاشی SNIPS انجام می دهد.', 'ko': '학습의 틈새 채우기 - 구어 이해 시스템의 관건적인 구성 부분인 대량의 영역에서 수동으로 표시된 데이터를 훈련해야 한다.본고에서 우리는 새로운 2단계 모델 구조를 제시했는데 이 구조는 몇 가지 영역 내의 손 표기 예시만 있으면 훈련을 할 수 있다.첫 번째 단계는 비시극 영패 (즉 O 표시된 영패) 를 삭제하는 것이다. 왜냐하면 이것은 시극 충전 모델의 입력에 소음을 도입하기 때문이다.이 단계는 분야와 무관하기 때문에 역외 데이터를 활용해 훈련할 수 있다.두 번째 단계는 가장 선진적인 사전 훈련 상하문 삽입(예를 들어 ELMO와 BERT)을 사용하여 슬롯 영패에 슬롯 이름만 표시한다.SNIPS 벤치마크 데이터 세트에서 가장 진보된 다른 시스템보다 우수한 방법을 사용하고 있음을 증명합니다.', 'sw': 'Mfumo wa kuelewa lugha yenye msingi wa kujifunza - kifungu muhimu cha mfumo wa kuelewa kwa lugha - kwa kawaida unahitaji kiasi kikubwa cha taarifa zilizowekwa kwa mikono ya ndani kwa ajili ya mafunzo. Katika karatasi hii, tunapendekeza ujenzi wa kiwango cha mitindo miwili ambacho kinaweza kufundishwa kwa mifano michache tu ya ndani. Hatua ya kwanza imelengwa kuondoa alama zisizo sahihi (yaani ishara za O zilizoonyesha), wakati wanaonyesha kelele katika viwanja vya ujazo vya maduka. Hatua hii ni maarufu ya ndani na kwa hiyo, inaweza kufundishwa kwa kutumia taarifa nje ya ndani. Hatua ya pili inaonyesha majina ya slot pekee kwa ajili ya alama za kisasa kwa kutumia vifaa vya serikali-ya-sanaa vilivyowekwa kama vile ELMO na BERT. Tunaonyesha kwamba mbinu yetu inafanya mifumo mingine ya sanaa kwenye seti ya taarifa za bendera ya SNIPS.', 'tr': 'Otulmak tabanly slot doldurulýan - gürleýän diller düşünüp sistemleriniň a çyk komponenti - adatça domeniň elleri gol etitlenýän maglumaty üçin gaty uly bir sany gerek. Bu kagyzda, biz 2-staýan nusga arhitektura teklip edýäris. Diňe domençe elimde diňe birnäçe eserler bilen öwrenmeli. Ilkinji adım slot-olmayan i şaretlerden çykarmak üçin tasarlanmış, o etilen işaretlerden, slot doldurulma modellerinin girişinde sesi girdirmek üçin. Bu adım domain-agnostik ve bu yüzden, domain verisini kullanarak eğitilebilir. Ikinji adım, ELMO we BERT gibi slot işaretleri üçin slot adlarını tanıyır. Biziň ýaryşymyz SNIPS benchmark veri setinde başga sanat sistemalarynyň üstüne çykarýandygyny görkezýäris.', 'af': "Leer- gebaseerde slot opvulling -  'n sleutel komponent van praat taal verstaan stelsels - tipies benodig 'n groot hoeveelheid in- domein hand- etiket data vir opvulling. In hierdie papier, voorstel ons 'n roman twee-stadige model-arkitektuur wat net 'n paar in-domein-hand-etiketeerde voorbeelde kan onderwerp word. Die eerste stap is ontwerp om non- slot tokens te verwyder (i.e. O etiketeerde tokens), wanneer hulle die geluid in die invoer van slot opvul modele te introduseer. Hierdie stap is domein-agnostik en dan kan onderwerp word deur uitgebruik van domein-data. Die tweede stap identifiseer slot name slegs vir slot tokens deur gebruik van state- of- the- art pretrained contextual embeddings soos ELMO en BERT. Ons wys dat ons toegang uitvoer ander staat-kunstenstelsels op die SNIPS benchmark datastel.", 'am': 'ለመማር የተገኘው ስርጭት - የቋንቋ ማስተዋል ስርዓቶች ቁልፍ ክፍል - በተጨማሪው የዶሜን እጁ-labeled ዳታ ለማስተማር ያስፈልጋል። በዚህ ፕሮግራም፣ የሁለት ደረጃዎች የሞዴል መሠረት አካባቢ እናሳውቃለን፡፡ የመጀመሪያይቱ ደረጃዎች የደረጃ ምልክቶችን ለማስወገድ (አዲስ የደረጃ ምልክቶች) ድምፅ በጭብጥ ሙሉ ሞላት ሞላዎች ውስጥ ሲያቀርቡ ነው፡፡ ይሄ ደረጃው ዶሜን-አጎኖስቲ ነው ስለዚህም ከዶሜን ዳታ በመጠቀም ይችላል፡፡ በሁለተኛው ደረጃ ላይ እንደELMO እና BERT በመጠቀም የደረጃ ምልክቶች ብቻ ሲያሳያል፡፡ አካሄዳችን ከSNIPS የbenchmark ዳርቻ ላይ ሌሎችን የሥርዓት ሥርዓት ሲሞክራችንን እናሳየዋለን፡፡', 'bn': 'শিক্ষা ভিত্তিক স্লোট ফোল্ডার - ভাষা বুঝতে পারে ভাষার ব্যাখ্যা সিস্টেমের একটি কী অংশ - সাধারণত প্রশিক্ষণের জন্য ডোমেইনের হা এই কাগজটিতে আমরা একটি উপন্যাস প্রস্তাব করছি দুই মেডোল মডেলের আর্কিটেক্টার যা শুধুমাত্র কয়েকটি হাতে লেবেল করা উদাহরণ দিয়ে প্রশিক প্রথম পদক্ষেপ নির্ধারণ করা হয়েছে স্লোট ফোল্ডার মডেলের ইনপুটে শব্দ দেখানোর জন্য (যেমন ওই লেবেলেট প্রতীক), যেহেতু তারা স্লোট ফ এই পদক্ষেপ ডোমেইন-অ্যাগনোস্টিক এবং তাই ডোমেইনের বাইরে ডাটা ব্যবহার করে প্রশিক্ষণ প্রদান করা যাবে। দ্বিতীয় পদক্ষেপ শুধুমাত্র স্লোট চিহ্নের জন্য স্লোটের নাম চিহ্নিত করা হয়েছে, যেমন ELMO এবং BERT এর মত প্রাক্তন স্থানীয় বিভিন্ন স We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.', 'hy': 'Սովորելով հիմնված վայրերի լրացումը, խոսվող լեզվի հասկանալու համակարգերի կարևոր բաղադրիչը, սովորաբար հարկավոր է մեծ քանակությամբ բնագավառներում ձեռքով նշանված տվյալներ վարժելու համար: Այս թղթի մեջ մենք առաջարկում ենք նոր երկու փուլում գտնվող ճարտարապետական կառուցվածք, որը կարող է սովորեցնել միայն մի քանի բնագավառի ձեռքով նշաններով: Առաջին քայլը նախագծված է, որպեսզի հեռացնի ոչ վահանակների նշանները (այսինքն՝ O նշանները), քանի որ դրանք ներկայացնում են աղմուկ վահանակների լրացման մոդելների ներմուծքում: Այս քայլը տիեզերական ագնոստիկ է, և հետևաբար կարող է սովորեցնել տիեզերական տվյալների օգտագործման միջոցով: Երկրորդ քայլը բացահայտում է անկյունների անունները միայն անկյունների նշանների համար, օգտագործելով ամենաբարձր նախապատրաստված կոնտեքստային ներդրումներ, ինչպիսիք են ELMo և BER: We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.', 'bs': 'Zapunjenje slot a na osnovu učenja - ključni komponent sustava razumijevanja govornih jezika - obično zahtijeva veliku količinu podataka na području na domenu za obuku. U ovom papiru predlažemo romansku modelsku arhitekturu koja može biti obučena samo sa nekoliko primjera u domenu na rukama. Prvi korak je dizajniran za uklanjanje znakova bez slota (tj. O znakova označene), dok predstavljaju buku u ulazu modela punjenja slota. Ovaj korak je domene-agnostičan i stoga se može obučiti koristeći podatke iz domena. Drugi korak identificira imena slota samo za znakove slota koristeći kontekstne ugrađene stanje umjetnosti kao što su ELMO i BERT. Pokazujemo da naš pristup nadmašuje druge državne umjetne sustave na snimku podataka SNIPS-a.', 'ca': "El rellenç de slots basat en l'aprenentatge - un component clau dels sistemes de comprensió del llenguatge parlat - normalment requereix una gran quantitat de dades etiquetades a mà en domini per a formar-se. En aquest article, proposem una nova arquitectura model de dos etapes que només pot ser entrenada amb alguns exemples marcats a mà en domini. El primer pas està dissenyat per eliminar les fitxes sense slots (és a dir, fitxes etiquetades amb O), mentre introdueixen soroll a l'entrada de models de rellenç de slots. Aquest pas és agnòstic de domini i, per tant, pot ser entrenat explotant dades fora de domini. El segon pas identifica noms de slots només per a les fitxes de slots utilitzant incorporacions contextuals avançades com ELMO i BERT. Mostrem que el nostre enfocament supera altres sistemes més avançats en el conjunt de dades de referència SNIPS.", 'cs': 'Výukové vyplnění slotů je klíčovou součástí systémů porozumění mluvenému jazyku, které obvykle vyžaduje velké množství v doméně ručně označených dat pro školení. V tomto článku navrhujeme novou dvoustupňovou modelovou architekturu, kterou lze trénovat pouze s několika ručně označenými příklady v doméně. První krok je navržen k odstranění žetonů bez slotu (tj. žetonů označených O), protože zavádějí šum do vstupu modelů plnění slotů. Tento krok je doménově agnostický a proto může být trénován využitím dat mimo doménu. Druhý krok identifikuje názvy slotů pouze pro žetony slotů pomocí nejmodernějších kontextových vložení, jako jsou ELMO a BERT. Ukazujeme, že náš přístup překonává ostatní nejmodernější systémy na referenčním datovém sadě SNIPS.', 'et': 'Õppepõhine teenindusaegade täitmine – räägitava keele mõistmise süsteemide põhikomponent – nõuab koolituseks tavaliselt suurt hulka domeenisiseseid käsitsi märgistatud andmeid. Käesolevas töös pakume välja uudse kaheastmelise mudeli arhitektuuri, mida saab koolitada vaid mõne domeenisisese käsitsi märgistatud näitega. Esimene samm on mõeldud eemaldama mitteseotud tokenid (st O-märgistusega tokenid), kuna need tekitavad müra pesade täitmise mudelite sisenduses. See samm on domeenist sõltumatu ja seetõttu saab seda koolitada domeenivälisete andmete kasutamisega. Teises etapis tuvastatakse teenindusaegade nimed ainult teenindusaegade tokenite jaoks, kasutades kaasaegseid eeltreenitud kontekstipõhiseid manustamisi, nagu ELMO ja BERT. Näitame, et meie lähenemisviis ületab SNIPSi võrdlusandmete hulgas teisi kaasaegseid süsteeme.', 'fi': 'Oppimiseen perustuva lähtöpaikkatäyttö, joka on keskeinen osa puhutun kielen ymmärtämisjärjestelmiä, vaatii yleensä suuren määrän käsin merkittyä tietoa koulutukseen. Tässä työssä ehdotamme uutta kaksivaiheista malliarkkitehtuuria, jota voidaan kouluttaa vain muutamilla käsin merkityillä esimerkeillä. Ensimmäinen vaihe on suunniteltu poistamaan ei-slot poletit (eli O-merkityt poletit), koska ne aiheuttavat melua slot täyttömallien syötössä. Tämä vaihe on verkkotunnuksen riippumaton, joten sitä voidaan kouluttaa hyödyntämällä verkkotunnuksen ulkopuolista tietoa. Toinen vaihe tunnistaa kolikkopelien nimet vain kolikkopelien poletteille käyttämällä huippuluokan esikoulutettuja kontekstuaalisia upotuksia, kuten ELMO ja BERT. Osoitamme, että lähestymistapamme on parempi kuin muut SNIPS-vertailuaineiston uusimmat järjestelmät.', 'sq': 'Mbledhja e slots me bazë mësimi - një komponent kyç i sistemeve të kuptimit të gjuhës së folur - tipikisht kërkon një sasi të madhe të dhënash me etiketë dore në domeni për trajnimin. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples.  Hapi i parë është projektuar për të hequr shenjat jo-slot (pra, shenjat me etiketë O), pasi ato futin zhurmë në hyrjen e modeleve të mbushjes së slot. Ky hap është domain-agnostic dhe prandaj, mund të trajnohet duke shfrytëzuar të dhënat jashtë domenit. Hapi i dytë identifikon emrat e slots vetëm për toket e slots duke përdorur përfshirjet kontekstuale më të larta të parastërvitura si ELMO dhe BERT. Ne tregojmë se qasja jonë është më e lartë se sistemet e tjera të modernizuara në bazën e të dhënave të SNIPS-it.', 'az': '√Ėyr…ônm…ô tabanlńĪ slot doldurulmasńĪ - danńĪŇüan dil anlama sisteml…ôrinin anahtar komponenti - …ôks…ôriyy…ôti t…ôhsil √ľ√ß√ľn √ßox d…ôyiŇüdirilmiŇü m…ôlumatlarńĪn ehtiyacńĪ vardńĪr. Bu kańüńĪzda, biz yeni iki s…ôb…ôb modeli arhitektura t…ôklif edirik ki, domeind…ô ancaq bir ne√ß…ô …ôl etiketli m…ôs…ôll…ôrl…ô t…ôhsil edil…ô bil…ôr. ńįlk adńĪm slot olmayan m√∂c√ľz…ôl…ôri silm…ôk √ľ√ß√ľn tasarlandńĪrńĪlmńĪŇüdńĪr, onlar slot dolduran modellerin giriŇüind…ô s…ôsl…ôri tanńĪdńĪrlar. Bu adńĪm domain-agnostik v…ô buna g√∂r…ô d…ô domain veril…ôri istifad…ô ed…ôr…ôk t…ôhsil edil…ô bil…ôr. ńįkinci adńĪm, ELMO v…ô BERT kimi m√ľxt…ôlif m…ôs…ôl…ôl…ôr istifad…ô ed…ôr…ôk slot adlarńĪnńĪ yalnńĪz slot iŇüar…ôl…ôri √ľ√ß√ľn tanńĪyńĪr. Bizim t…ôr…ôfimiz SNIPS benchmark veril…ôr qutusunda baŇüqa sanat sisteml…ôrini baŇüa d√ľŇü…ôr.', 'ha': "An cika filin slot da aka sanar - wata maɓalli na cikin tsarin fahimtar harshen da aka yi wa magana - ana ƙayyade kodi mai girma cikin-Domen da aka rubuta wa tsarin. Daga wannan takardan, Munã buɗa wani matsayin misalin na cikin hannayen da aka rubũta wani littãfi biyu. Babbar ta farkon aka ƙayyade shi dõmin a tafiyar da wasu ayõyin slot ba (misali, alamomi da aka lissafa), idan sun shiga cikin shirin ayuka da ke cikakken slot. Wannan ƙẽtare ya zama danne-agonostic na Domen, don haka, za'a iya amfani da shi da ya yi amfani da data masu fitarwa daga-Domen. Gagon ta biyu na gane sunan slot kawai wa alama ɗin slot game da amfani da state-of-the-art wanda aka keɓa wa daidai kamar ELMO da BERT. Tuna nũna cewa hanyoyinmu yana samun wasu na'urar-sanatarwa na SNIPS kan danne-bangon ayuka.", 'sk': 'Zapolnjevanje mest na podlagi učenja – ključna sestavina sistemov razumevanja govorjenega jezika – običajno zahteva veliko količino ročno označenih podatkov za usposabljanje. V prispevku predlagamo novo dvostopenjsko modelno arhitekturo, ki jo je mogoče usposabljati z le nekaj ročno označenimi primeri. Prvi korak je zasnovan za odstranitev žetonov brez rež (tj. žetonov z oznako O), saj vnesejo hrup v vnos modelov polnjenja rež. Ta korak je domensko agnostičen in ga je zato mogoče usposabljati z izkoriščanjem zunajdomenskih podatkov. Drugi korak identificira imena rež samo za igralne žetone z uporabo najsodobnejših predtreniranih kontekstualnih vdelav, kot sta ELMO in BERT. Pokazujemo, da naš pristop presega druge najsodobnejše sisteme v referenčnem naboru SNIPS.', 'he': 'מילוי קופסאות על בסיס הלימוד - חלק מפתח של מערכות הבנה לשפה מדברת - בדרך כלל דורש כמות גדולה של נתונים על תווית יד בתחום לאימון. בעיתון הזה, אנו מציעים ארכיטקטורה דוגמנית חדשה בשתי שלבים שאפשר לאמן רק עם כמה דוגמאות על תווית יד בתחום. השלב הראשון מעוצב כדי להסיר סימנים לא סגורים (כלומר סימנים עם סימנים O), כפי שהם מכניסים רעש בכניסה של דוגמנים למלא סגורים. הצעד הזה הוא אגנוסטי דומינה ולכן, יכול להיות מאומן על ידי ניצלון נתונים מחוץ לדמיון. הצעד השני מזהה שמות קופסאות רק בשביל קופסאות קופסאות על ידי השימוש במערכות קונטקסטיות מוקדמות מראש כמו ELMO ובRT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.', 'jv': 'Learning-supported slot filling - a key komputator of spoken language compromise sistems - Typically obligates a big amount of in-domain hand-label data for tutorial. In this paper, we proposal a rome 2-phase model architecture that can be cared with only a little in-domain hand-label example. Ehe perusahaan sing dibenalke nggawe token-slot This phase is domain-ageostik and so, can be traced by expanding out-of-domain data. Laptop" and "Desktop Awak dhéwé ngerti, ditambahaké dhéwé iso nggawe sistem ité-pernik nik nik nggawe dataset', 'bo': 'Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. ང་ཚོས་རང་གི་ཐབས་ལམ་དེ་SNIPS འཆར་བཀོད་པའི་གནས་སྟངས་གཞན་ལས་སྒྲུབ་པའི་གནས་སྟངས་གཞན་པ་ཞིག་བཀོལ་བྱེད་ཀྱི་ཡོད།'}
{'en': 'Early Exiting BERT for Efficient Document Ranking', 'fr': 'Sortie anticipée du BERT pour un classement efficace des documents', 'es': 'Salida anticipada de BERT para una clasificación eficiente de documentos', 'ar': 'الخروج المبكر من BERT لترتيب المستندات الفعال', 'pt': 'Saída antecipada do BERT para classificação eficiente de documentos', 'zh': '先出 BERT,高效文档名', 'ja': '効率的な文書ランキングのための早期終了BERT', 'ru': 'Ранний выход из BERT для эффективного ранжирования документов', 'hi': 'कुशल दस्तावेज़ रैंकिंग के लिए BERT से जल्दी बाहर निकलना', 'ga': 'BERT Luathscoir le haghaidh Rangú Éifeachtach Doiciméad', 'ka': 'ეფექტიური დოკუმენტის გარეშე BERT', 'el': 'Πρώιμη έξοδος από το BERT για αποτελεσματική κατάταξη εγγράφων', 'it': 'Uscita anticipata dal BERT per una classificazione efficiente dei documenti', 'kk': 'Бұл құжатты таңдау үшін BERT- ден бастау үшін', 'lt': 'Ankstyvas išėjimas iš BERT veiksmingam dokumentų klasifikavimui', 'mk': 'Early Exiting BERT for Efficient Document Ranking', 'ms': 'BERT Keluar Awal untuk Rangkaian Dokumen Efisien', 'ml': 'രേഖയുടെ റാങ്ങിങ്ങിന് വേണ്ടി മുമ്പ് BERT പുറത്തുകടക്കുന്നു', 'mt': 'BERT li joħroġ kmieni għall-Klassifikazzjoni Effiċjenti tad-Dokumenti', 'mn': 'Эртний БЕРТ-ээс гарч ирэхэд хүчирхэг баримт хуваалцах', 'no': 'Early Exiting BERT for Efficient Document Ranking', 'hu': 'A BERT korai kilépése a hatékony dokumentumok rangsorolása érdekében', 'pl': 'Wczesne wyjście z BERT dla efektywnego rankingu dokumentów', 'ro': 'Ieșirea timpurie a BERT pentru clasificarea eficientă a documentelor', 'sr': 'Rani izlaz BERT za učinkovito snimanje dokumenta', 'si': 'පළවෙනි BERT පිටවෙන්න පුළුවන් විසින් විසින් පරීක්ෂා විසින් විසින්න', 'sv': 'Tidigt utträde från BERT för effektiv dokumentrankning', 'so': 'Early exiting BERT for Efficient Document Ranking', 'ta': 'Early Exiting BERT for Efficient Document Ranking', 'ur': 'فعال دفتر رنگ کے لئے BERT سے پہلے باہر نکلنا', 'vi': 'Đang xếp hạng Tài liệu hiệu quả', 'uz': 'Comment', 'bg': 'Ранно напускане на BERT за ефективно класиране на документите', 'hr': 'Rani izlazak BERT za učinkovito snimanje dokumenta', 'nl': 'BERT vroegtijdig verlaten voor een efficiënte rangschikking van documenten', 'da': 'Tidlig afslutning af BERT for effektiv dokumentplacering', 'ko': '효율적인 문서 순위를 위해 조기 퇴출', 'de': 'Frühzeitiger Ausstieg aus BERT für effizientes Dokumentenranking', 'id': 'Early Exiting BERT for Efficient Document Ranking', 'fa': 'اولین خروج BERT برای تنظیم سند تاثیر', 'sw': 'Early Exiting BERT for Efficient Document Ranking', 'af': 'Vroeg Uitgaande BERT vir Efficient Document Ranking', 'tr': 'Bir sened bejermek üçin Edilkinji Çikgi', 'sq': 'BERT për një renditje të efektshme dokumentesh', 'am': 'ሰነድ', 'az': "Etkin Dökümat Ranking üçün BERT'dan İlk Çıqış", 'hy': 'Ավելի արդյունավետ փաստաթղթի դասակարգման համար', 'bn': 'কার্যকর ডকুমেন্ট র\u200d্যাঙ্কিং এর জন্য প্রাথমিকভাবে বেরেটি বের হচ্ছে', 'bs': 'Rani izlazak BERT za učinkovito snimanje dokumenta', 'cs': 'Předčasný ukončení BERT pro efektivní hodnocení dokumentů', 'et': 'BERT varajane lahkumine tõhusa dokumentide järjestuse saavutamiseks', 'fi': 'BERT:stä poistuminen varhaisessa vaiheessa tehokkaasta asiakirjojen luokittelusta', 'ca': 'BERT de sortida precoce per a classificar documents eficients', 'jv': 'BERT Digawe Sistem Jajal Bapake Jejaring', 'he': 'BERT יציאה מוקדמת לטווח מסמכים יעיל', 'ha': '@ action', 'sk': 'Zgodnji izstop BERT za učinkovito razvrstitev dokumentov', 'bo': 'ནུས་ཡོད་པའི་ཡིག་ཆ་ལེན་པའི་སྔོན་ལ་ཞིབ་ཕྱིར་ཐོན།'}
{'en': 'Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.', 'es': 'Los modelos lingüísticos previamente entrenados, como BERT, han demostrado su eficacia en diversas tareas. A pesar de su potencia, se sabe que son computacionalmente intensivos, lo que dificulta las aplicaciones del mundo real. En este artículo, presentamos la salida temprana del BERT para la clasificación de documentos. Con una ligera modificación, BERT se convierte en un modelo con múltiples rutas de salida, y cada muestra de inferencia puede salir antes de estas rutas. De esta manera, el cálculo se puede asignar eficazmente entre las muestras, y la latencia general del sistema se reduce significativamente mientras se mantiene la calidad original. Nuestros experimentos en dos conjuntos de datos de clasificación de documentos demuestran una aceleración de inferencia de hasta 2,5 veces con una degradación mínima de la calidad. El código fuente de nuestra implementación se encuentra en https://github.com/castorini/earlyexiting-monobert.', 'fr': "Les modèles linguistiques pré-entraînés tels que BERT ont démontré leur efficacité dans diverses tâches. Malgré leur puissance, ils sont connus pour être gourmands en ressources informatiques, ce qui entrave les applications du monde réel. Dans cet article, nous présentons la sortie anticipée du BERT pour le classement des documents. Avec une légère modification, BERT devient un modèle avec plusieurs chemins de sortie, et chaque échantillon d'inférence peut sortir plus tôt de ces chemins. De cette manière, le calcul peut être réparti efficacement entre les échantillons, et la latence globale du système est considérablement réduite tout en conservant la qualité d'origine. Nos expériences sur deux ensembles de données de classement de documents démontrent une accélération de l'inférence jusqu'à 2,5 fois avec une dégradation minimale de la qualité. Le code source de notre implémentation se trouve à l'adresse https://github.com/castorini/earlyexiting-monobert.", 'ar': 'أظهرت نماذج اللغة المدربة مسبقًا مثل BERT فعاليتها في المهام المختلفة. على الرغم من قوتهم ، فمن المعروف أنهم مكثفون من الناحية الحسابية ، مما يعيق تطبيقات العالم الحقيقي. في هذه الورقة ، نقدم BERT مبكرًا لتصنيف المستندات. مع تعديل طفيف ، يصبح BERT نموذجًا بمسارات إخراج متعددة ، ويمكن لكل عينة استنتاج الخروج مبكرًا من هذه المسارات. بهذه الطريقة ، يمكن تخصيص الحساب بشكل فعال بين العينات ، كما يتم تقليل زمن انتقال النظام بشكل كبير مع الحفاظ على الجودة الأصلية. تُظهر تجاربنا على مجموعتي بيانات لترتيب المستندات ما يصل إلى 2.5x تسريع للاستدلال مع الحد الأدنى من تدهور الجودة. يمكن العثور على الكود المصدري لتطبيقنا على https://github.com/castorini/earlyexiting-monobert.', 'pt': 'Modelos de linguagem pré-treinados, como o BERT, mostraram sua eficácia em várias tarefas. Apesar de seu poder, eles são conhecidos por serem computacionalmente intensivos, o que dificulta as aplicações do mundo real. Neste artigo, apresentamos o BERT de saída antecipada para classificação de documentos. Com uma pequena modificação, o BERT se torna um modelo com vários caminhos de saída e cada amostra de inferência pode sair antecipadamente desses caminhos. Dessa forma, a computação pode ser efetivamente alocada entre as amostras e a latência geral do sistema é significativamente reduzida enquanto a qualidade original é mantida. Nossos experimentos em dois conjuntos de dados de classificação de documentos demonstram uma aceleração de inferência de até 2,5x com degradação de qualidade mínima. O código-fonte de nossa implementação pode ser encontrado em https://github.com/castorini/earlyexiting-monobert.', 'ja': 'BERTのような事前にトレーニングを受けた言語モデルは、さまざまなタスクで効果を示しています。それらのパワーにもかかわらず、それらは計算集約的であることが知られており、現実のアプリケーションを妨げています。本稿では、文書ランキングのための早期退学BERTを紹介する。わずかな修正で、BERTは複数の出力経路を持つモデルとなり、各推論サンプルはこれらの経路から早期に終了することができます。このようにして、計算をサンプル間で効果的に割り当てることができ、元の品質が維持されている間、システム全体の待ち時間が大幅に短縮される。2つのドキュメントランキングデータセットの実験では、最小限の品質劣化で最大2.5倍の推論スピードアップが実証されています。当社の実装のソースコードは、https://github.com/castorini/earlyexiting-monobertにあります。', 'hi': 'BERT जैसे पूर्व-प्रशिक्षित भाषा मॉडल ने विभिन्न कार्यों में अपनी प्रभावशीलता दिखाई है। उनकी शक्ति के बावजूद, वे कम्प्यूटेशनल रूप से गहन होने के लिए जाने जाते हैं, जो वास्तविक दुनिया के अनुप्रयोगों में बाधा डालते हैं। इस पेपर में, हम दस्तावेज़ रैंकिंग के लिए जल्दी बाहर निकलने वाले BERT का परिचय देते हैं। एक मामूली संशोधन के साथ, BERT कई आउटपुट पथों के साथ एक मॉडल बन जाता है, और प्रत्येक अनुमान नमूना इन पथों से जल्दी बाहर निकल सकता है। इस तरह, गणना को नमूनों के बीच प्रभावी ढंग से आवंटित किया जा सकता है, और मूल गुणवत्ता बनाए रखने के दौरान समग्र सिस्टम विलंबता काफी कम हो जाती है। दो दस्तावेज़ रैंकिंग डेटासेट पर हमारे प्रयोग न्यूनतम गुणवत्ता गिरावट के साथ 2.5x अनुमान स्पीडअप तक प्रदर्शित करते हैं। हमारे कार्यान्वयन का स्रोत कोड https://github.com/castorini/earlyexiting-monobert पर पाया जा सकता है।', 'zh': '先训语言(如BERT)已见其有效性。 虽有强大之功,以为计密集型,碍世界之用也。 本文引文档早退BERT。 稍修改,BERT 将为多输路径,而每推理样本皆得先退。 如此者,分算于样本之间,而显其本而系之迟也。 二文档列名数集上之实验,推理倍 2.5 ,质降幅度小。 可得源代码于 https://github.com/castorini/earlyexiting-monobert。', 'ru': 'Предварительно обученные языковые модели, такие как BERT, показали свою эффективность в различных задачах. Несмотря на свою мощь, они, как известно, являются вычислительно интенсивными, что препятствует реальным приложениям. В этой статье мы представляем ранний выход из BERT для ранжирования документов. С небольшой модификацией, BERT становится моделью с несколькими выходными путями, и каждая выборка вывода может выходить из этих путей рано. Таким образом, вычисления могут быть эффективно распределены между выборками, а общая задержка системы значительно уменьшена при сохранении исходного качества. Наши эксперименты на двух наборах данных для ранжирования документов демонстрируют ускорение вывода в 2,5 раза с минимальным ухудшением качества. Исходный код нашей реализации можно найти по адресу https://github.com/castorini/earlyexiting-monobert.', 'ga': 'Léirigh múnlaí teanga réamhoilte ar nós BERT a n-éifeachtacht i dtascanna éagsúla. In ainneoin a gcumhacht, is eol go bhfuil siad dian ó thaobh ríomha de, rud a chuireann bac ar fheidhmchláir sa saol fíor. Sa pháipéar seo, tugaimid isteach BERT luathscortha le haghaidh rangú doiciméad. Le modhnú beag, déantar samhail de BERT le cosáin aschuir iolracha, agus is féidir le gach sampla tátail imeacht go luath ó na cosáin seo. Ar an mbealach seo, is féidir an ríomh a leithdháileadh go héifeachtach i measc samplaí, agus laghdaítear latency foriomlán an chórais go suntasach agus an caighdeán bunaidh á chothabháil. Léiríonn ár dturgnaimh ar dhá thacar sonraí rangaithe doiciméad suas le 2.5x luas suas tátal le díghrádú cáilíochta íosta. Is féidir cód foinse ár gcur chun feidhme a fháil ag https://github.com/castorini/earlyexiting-monobert.', 'hu': 'Az előképzett nyelvi modellek, mint például a BERT, bizonyították hatékonyságukat különböző feladatokban. Hatalmuk ellenére ismert, hogy számítástechnikailag intenzív, ami akadályozza a valós alkalmazásokat. Ebben a tanulmányban bemutatjuk a BERT korai kilépését a dokumentumok rangsorolására. Egy kisebb módosítással a BERT több kimeneti útvonalú modell lesz, és minden következtetési minta korán kiléphet ezekből az útvonalakból. Ily módon a számítások hatékonyan eloszthatók a minták között, és az általános rendszer késleltetése jelentősen csökken, miközben az eredeti minőség fenntartható. Két dokumentum rangsorolási adatkészleten végzett kísérleteink akár 2,5x-es következtetési gyorsulást mutatnak, minimális minőségbomlás mellett. Megvalósításunk forráskódja a következő oldalon található: https://github.com/castorini/earlyexiting-monobert.', 'ka': 'წინ განაკეთებული ენის მოდელები, როგორც BERT, ჩვენი ეფექტიურობა განსხვავებული დავალებში ჩვენება. მათი ძალიან მაგრამ, ისინი კომპუტაციალურად ინტენექტიურია, რომელიც შეუძლებელია რეალური მსოფლიო პროგრამები. ამ დოკუმენტში ჩვენ გვეჩვენებთ BERT-ს წინ გადასვლა დოკუმენტის რენექციისთვის. BERT იქნება მრავალ გამოყენების მოდელი, და ყოველ ინფრენციის მოდელი შეიძლება ამ გზებიდან წინ გადასვლა. ასე რომ, კომპუტირება შეიძლება ეფექტიურად მონაცემების შორის განყოფილება, და ყველა სისტემის წინასწორეობა მნიშვნელოვანია გადასრულებული, როცა ორიგინალური კა ჩვენი ექსპერიმენტები ორი დოკუმენტის რენექტირების მონაცემების კონფიგურაციაში 2,5x ინფრენციის სიჩქარე მინიმალური კავირტური დეგრადიაციაზე ჩვენი გამოყენების ფორმა კოდი შეიძლება მოიძლება https://github.com/castorini/earlyexiting-monobert.', 'it': "Modelli linguistici pre-formati come BERT hanno dimostrato la loro efficacia in vari compiti. Nonostante la loro potenza, sono noti per essere computazionali intensivi, il che ostacola le applicazioni del mondo reale. In questo articolo presentiamo l'uscita anticipata del BERT per la classifica dei documenti. Con una leggera modifica, BERT diventa un modello con più percorsi di output e ogni campione di inferenza può uscire presto da questi percorsi. In questo modo, il calcolo può essere efficacemente allocato tra i campioni e la latenza complessiva del sistema viene significativamente ridotta mentre la qualità originale viene mantenuta. I nostri esperimenti su due set di dati di classificazione dei documenti dimostrano una velocità di inferenza fino a 2,5x con un minimo degrado della qualità. Il codice sorgente della nostra implementazione può essere trovato all'indirizzo https://github.com/castorini/earlyexiting-monobert.", 'lt': 'Pre-trained language models such as BERT have shown their effectiveness in various tasks.  Nepaisant jų galios, žinoma, kad jos yra skaičiavimo požiūriu intensyvios, o tai trukdo realiam pasauliui taikyti. Šiame dokumente pristatome ankstyvą BERT išėjimą dokumentų klasifikavimui. Nedideliu pakeitimu BERT tampa modeliu su keliomis išėjimo keliomis, ir kiekvienas išvados mėginys gali išeiti anksti iš šių kelių. Tokiu būdu skaičiavimus galima veiksmingai paskirstyti mėginiams, o bendras sistemos trūkumas gerokai mažėja, kol išlieka pradinė kokybė. Mūsų eksperimentai su dviem dokumentų klasifikavimo duomenų rinkiniais rodo iki 2,5x išvados greitį su minimaliu kokybės blogėjimu. Mūsų įgyvendinimo pradinis kodas pateikiamas https://github.com/castorini/earlyexiting-monobert.', 'kk': 'BERT секілді алдын- ала оқылған тіл үлгілері әртүрлі тапсырмаларда әсер ететіндігін көрсетті. Олардың күшін қарамастан, олар компьютерлік үлкен деп белгіледі, бұл шын әлемді қолданбаларды бұғаттайды. Бұл қағазда біз құжаттарды сақтау үшін BERT-ден бастап шығуды келтіреміз. Бірнеше өзгертілген кезде BERT бірнеше шығыс жолдарының үлгісі болады, және әрбір бақылау үлгісі осы жолдардан бастап шығуға болады. Бұл үшін есептеу үлгілер арасында әсер етеді, жүйелік соңғылығы негізгі сапасы сақталғанда әсер етеді. Екі құжатты сақтау деректер баптауларының тәжірибелеріміз 2, 5x бағдарламасының төменгі сапатты деградациялауына көмектеседі. Орындау көзі https://github.com/castorini/earlyexiting-monobert.', 'mk': 'Преобучени јазички модели како што е БЕРТ ја покажаа својата ефикасност во различни задачи. И покрај нивната моќ, се знае дека се компјутерски интензивни, што ги попречува апликациите во реалниот свет. Во овој весник, го претставуваме раното излегување од BERT за рангирање на документите. Со мала модификација, БЕРТ станува модел со повеќе излезни патеки, и секој примерок може да излезе рано од овие патеки. На овој начин, компјутацијата може да биде ефикасно распределена помеѓу примероците, а целокупната затемнување на системот е значително намалена додека се одржува оригиналниот квалитет. Нашите експерименти на две податоци за рангирање на документи покажуваат до 2,5x брзина на инференција со минимална деградација на квалитетот. Изворниот код на нашата имплементација може да се најде на https://github.com/castorini/earlyexiting-monobert.', 'ms': 'Model bahasa terlatih-awal seperti BERT telah menunjukkan kegunaan mereka dalam berbagai tugas. Walaupun kuasa mereka, mereka diketahui sebagai komputasi yang intens, yang menghalangi aplikasi dunia nyata. Dalam kertas ini, kami memperkenalkan BERT keluar awal untuk peringkat dokumen. Dengan sedikit pengubahsuaian, BERT menjadi model dengan laluan output berbilang, dan setiap sampel kesimpulan boleh keluar awal dari laluan ini. Dengan cara ini, pengiraan boleh diberikan secara efektif di antara sampel, dan keseluruhan kelemahan sistem dikurangi secara signifikan semasa kualiti asal disimpan. Eksperimen kami pada dua set data peringkat dokumen menunjukkan hingga 2.5x kecepatan kesimpulan dengan kerosakan kualiti minimal. Kod sumber pelaksanaan kita boleh ditemui di https://github.com/castorini/earlyexiting-monobert.', 'ml': 'ബെര്\u200dട്ടി പോലുള്ള മുമ്പ് പരിശീലന ഭാഷ മോഡലുകള്\u200d വ്യത്യസ്തമായ ജോലികളില്\u200d അവരുടെ ഫലവും കാണിച്ചു. Despite their power, they are known to be computationally intensive, which hinders real-world applications.  ഈ പത്രത്തില്\u200d നമ്മള്\u200d ബെര്\u200dട്ടിയില്\u200d നിന്ന് പുറത്ത് കടക്കുന്നതിന് നേരത്തെ പരിചയപ്പെടുത്തും. ഒരു ചെറിയ മാറ്റങ്ങള്\u200d കൊണ്ട്, ബെര്\u200dട്ടി പല പുറപ്പെട്ട വഴികളുമായി ഒരു മോഡലായിത്തീരുന്നു. എല്ലാ അപകടത്തിന്റെയും മാമ്പ ഇങ്ങനെയാണ്, മാമ്പുകളുടെ ഇടയില്\u200d കണക്കുണ്ടാക്കുവാന്\u200d സാധ്യതയുണ്ടാവുന്നത്. മൊത്തം സിസ്റ്റത്തിന്റെ ലേറ്റിസ്റ്റി മാ രണ്ടു രേഖയുടെ റാങ്ങ് ഡേറ്റാസെറ്റില്\u200d നമ്മുടെ പരീക്ഷണങ്ങള്\u200d 2.5x അപരീക്ഷണത്തിന്റെ വേഗത കുറച്ച് ചെറുതായ വ്യവസ നമ്മുടെ പ്രവര്\u200dത്തനത്തിന്റെ ഉറവിട കോഡ് കണ്ടെത്താന്\u200d സാധിക്കുന്നു https://github.com/castorini/earlyexiting-monobert.', 'mt': 'Mudelli lingwistiċi mħarrġa minn qabel bħall-BERT urew l-effettività tagħhom f’diversi kompiti. Minkejja s-setgħa tagħhom, huma magħrufa li huma komputazzjonalment intensivi, li jfixklu l-applikazzjonijiet fid-dinja reali. F’dan id-dokument, aħna nintroduċu BERT li joħroġ kmieni għall-klassifikazzjoni tad-dokumenti. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths.  In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained.  L-esperimenti tagħna fuq żewġ settijiet ta’ dejta ta’ klassifikazzjoni ta’ dokumenti juru sa 2.5x veloċità ta’ inferenza b’degradazzjoni minima tal-kwalità. Il-kodiċi tas-sors tal-implimentazzjoni tagħna jinstab fuq https://github.com/castorini/earlyexiting-monobert.', 'el': 'Προεκπαιδευμένα γλωσσικά μοντέλα όπως το BERT έχουν δείξει την αποτελεσματικότητά τους σε διάφορα καθήκοντα. Παρά τη δύναμή τους, είναι γνωστό ότι είναι υπολογιστικά εντατικά, γεγονός που εμποδίζει τις πραγματικές εφαρμογές. Σε αυτή την εργασία, εισάγουμε την έγκαιρη έξοδο από το BERT για την κατάταξη εγγράφων. Με μια μικρή τροποποίηση, γίνεται ένα μοντέλο με πολλαπλές διαδρομές εξόδου, και κάθε δείγμα συμπερασμάτων μπορεί να βγει νωρίς από αυτές τις διαδρομές. Με αυτόν τον τρόπο, ο υπολογισμός μπορεί να κατανεμηθεί αποτελεσματικά μεταξύ των δειγμάτων και η συνολική καθυστέρηση του συστήματος μειώνεται σημαντικά ενώ διατηρείται η αρχική ποιότητα. Τα πειράματά μας σε δύο σύνολα δεδομένων κατάταξης εγγράφων αποδεικνύουν μέχρι 2,5την επιτάχυνση συμπερασμάτων με ελάχιστη υποβάθμιση της ποιότητας. Ο πηγαίος κώδικας της εφαρμογής μας μπορείτε να βρείτε στο https://github.com/castorini/earlyexiting-monobert.', 'mn': 'БЕРТ шиг өмнө сургалтын хэл загварууд өөрсдийнхөө үр дүнг олон ажил дээр харуулсан. Тэдний эрх мэдэл ч гэсэн, тэд маш их хүчтэй гэдгийг мэддэг. Энэ нь бодит ертөнцийн хэрэглээ зогсохгүй. Энэ цаасан дээр бид баримт дүрслэхэд BERT-аас эхлээд гарч ирсэн. Бага зэрэг өөрчлөлтийн тулд, BERT олон үржүүлэх замыг загвар болгож, халдварын жишээ бүр эдгээр замын эхлээд гарах боломжтой. Иймээс тооцоолол жишээн дээр үр дүнтэй хуваалцагдаж болох бөгөөд ерөнхий системийн сарын дараа нь үндсэн чанарыг хадгалах үед багасгаж байна. Бидний 2 документийн хэмжээний өгөгдлийн сангийн туршилтууд 2.5x халдвар нь багахан чанарын халдвартай хурдтай харагдаж байна. Бидний үйлдвэрлэлийн эх үүсвэрийн код нь https://github.com/castorini/earlyexiting-monobert.', 'pl': 'Wstępnie przeszkolone modele językowe, takie jak BERT, wykazały swoją skuteczność w różnych zadaniach. Pomimo ich mocy, są one znane jako intensywne obliczeniowe, co utrudnia realne aplikacje. W niniejszym artykule przedstawiamy wczesne wyjście z BERT do rankingu dokumentów. Przy niewielkiej modyfikacji BERT staje się modelem z wieloma ścieżkami wyjściowymi, a każda próbka wniosku może wcześniej wyjść z tych ścieżek. W ten sposób obliczenia mogą być skutecznie alokowane między próbki, a ogólne opóźnienia systemu są znacznie zmniejszone przy zachowaniu oryginalnej jakości. Nasze eksperymenty na dwóch zestawach danych rankingowych dokumentów wykazują nawet 2,5x przyspieszenie wnioskowania przy minimalnej degradacji jakości. Kod źródłowy naszej implementacji można znaleźć pod adresem: https://github.com/castorini/earlyexiting-monobert.', 'ro': 'Modelele lingvistice pre-instruite precum BERT și-au demonstrat eficacitatea în diferite sarcini. În ciuda puterii lor, ele sunt cunoscute ca fiind intense din punct de vedere computațional, ceea ce împiedică aplicațiile din lumea reală. În această lucrare, prezentăm ieșirea timpurie a BERT pentru clasificarea documentelor. Cu o ușoară modificare, BERT devine un model cu mai multe căi de ieșire, iar fiecare eșantion de inferență poate ieși devreme din aceste căi. În acest fel, calculul poate fi alocat eficient între eșantioane, iar latența generală a sistemului este redusă semnificativ în timp ce calitatea inițială este menținută. Experimentele noastre pe două seturi de date de clasificare a documentelor demonstrează o viteză de inferență de până la 2,5x cu degradare minimă a calității. Codul sursă al implementării noastre poate fi găsit la https://github.com/castorini/earlyexiting-monobert.', 'so': 'Tusaalada afka hore oo la tababaray, sida BERT ayaa saameyn ku tusay shaqaalaha kala duduwan. In kastoo ay awooddoodu tahay, waxaa loo yaqaan inay tahay xisaab aad u adag, taasoo ka hor jeediya codsiyada caalamiga ee halista ah. Warqaddan waxaan horay ugu soo bandhignaa BERT si aan u sameynno dukumentiga. Isbedelka yar ayaa BERT wuxuu noqonayaa model ku yaal wadooyin kala duduwan, samooyin kasta oo cudur la’aanuna wuxuu ka bixi karaa wadooyinkaas aroor hore. Sidan darteed samooyinka waxaa lagu qeybin karaa xisaabin si faa’iido ah, oo waxaa lagu hoos qaataa dhaqdhaqaaqa nidaamka oo dhan marka la sii haysto qiimaha asalka ah. Imtixaankayada labada dukumenti ee danbiyada lagu sameynayo waxay muuqataa inay u dhaqdhaqaaqo 2.5x dhibaato hoos u dhigista qiimo yar. Kaarka asalka la soo dejinta waxaa laga heli karaa https://github.com/castorini/earlyexiting-monobert.', 'sr': 'Pre obučeni jezički modeli poput BERT pokazali su svoju efikasnost u različitim zadacima. Uprkos njihovoj moći, poznati su da su kompjuterski intenzivni, što sprječava aplikacije realnog svijeta. U ovom papiru predstavljamo rano izlazak iz BERT za poziciju dokumenta. Sa malo modifikacije, BERT postaje model sa višestrukim putevima izlaza, i svaki uzorak infekcije može izaći ranije iz ovih puteva. Na taj naèin, raèunanje se može efektivno odrediti među uzorcima, a ukupna sistemska latencija se značajno smanjuje dok se održava originalna kvalitet. Naši eksperimenti na dvojici dokumentovanih podataka pokazuju do 2,5x infekcije ubrzano sa minimalnom degradacijom kvalitete. Izvorni kod naše provedbe se nalazi na https://github.com/castorini/earlyexiting-monobert.', 'si': 'BERT වගේ ප්\u200dරධානය කළ භාෂාව මොඩේල් වලින් ඔවුන්ගේ විවිධ වැඩක් වලින් ප්\u200dරශ්නයක් පෙන්වන් ඔවුන්ගේ ශක්තිය නමුත් ඔවුන්ට පරිගණනය විශේෂයෙන් කියලා දන්නවා, ඒකෙන් ඇත්ත ලෝක යුද්ධියක් අවධ මේ පත්තරේ අපි පටන් ගත්තා BERT වලින් පටන් ගත්තා. ටිකක් වෙනස් වෙනුවෙන් BERT වෙනුවෙන් විශාල ප්\u200dරවේශනයක් වෙන්න පුළුවන් වෙනවා, හැම ප්\u200dරවේශනයක්ම සාම්ප්ලේම මෙහෙම විදියට, පරිගණනය සම්පූර්ණයෙන් සාම්ප්ලේස් එක්ක සම්පූර්ණයෙන් වෙන්න පුළුවන්, සම්පූර්ණ පද්ධත අපේ පරීක්ෂණ දත්ත සැකසුම් දෙකක් ප්\u200dරතික්\u200dරියා කරන්න පුළුවන් 2.5x පරීක්ෂණ වේගයක් ප්\u200dරතික්ෂණය කරන්න පුළු අපේ පරීක්ෂණයේ මූල කෝඩ හොයාගන්න පුළුවන් https://github.com/castorini/earlyexiting-monobert.', 'sv': 'Förutbildade språkmodeller som BERT har visat sin effektivitet i olika uppgifter. Trots sin kraft är de kända för att vara beräkningsintensiva, vilket hindrar verkliga tillämpningar. I denna uppsats introducerar vi tidigt utträdande BERT för dokumentrankning. Med en liten ändring blir BERT en modell med flera utgångsvägar, och varje inferensprov kan lämna dessa banor tidigt. På så sätt kan beräkningen fördelas effektivt mellan prover, och den totala systemfördröjningen minskas avsevärt samtidigt som den ursprungliga kvaliteten bibehålls. Våra experiment på två dokumentrankningsdatauppsättningar visar upp till 2,5x inferenshastighet med minimal kvalitetsförsämring. Källkoden för vår implementering finns på https://github.com/castorini/earlyexiting-monobert.', 'ta': 'BERT போன்ற மொழி மாதிரி மாதிரிகள் பல பணிகளில் அவர்களுடைய விளைவை காட்டியுள்ளது. அவர்களுடைய சக்தி இருந்தாலும், அவர்கள் கணக்கில் கடுமையான உறுதியாக தெரியும், அது உண்மையான உலக பயன்பாடுகளை தடுக்கிறது. இந்த காகிதத்தில், நாம் பிரெட் வெளியேறும் முன்னால் அறிமுகப்படுத்துகிறோம். ஒரு சிறிய மாற்றம் கொண்டு, BERT பல வெளியீட்டு பாதைகளுடன் ஒரு மாதிரி ஆகும், ஒவ்வொரு குறைவான மாதிரி இந்த பாதைகளிலிருந்து ம இவ்வாறு, மாதிரிகளுக்கிடையில் கணிப்பு வெளிப்படையாக ஒதுக்கப்படும், மொத்த கணினியின் தொகுதி முதல் தரம் வைத்திருக்கும் ப இரண்டு ஆவணங்கள் சேரும் தரவுத்தளங்களில் எங்கள் சோதனைகள் குறைந்தபட்ச தரம் குறைந்தது. எங்கள் செயல்பாட்டின் மூல குறியீடு கண்டுபிடிக்க முடியும் https://github.com/castorini/earlyexiting-monobert.', 'ur': 'پہلے تدریس کی زبان موڈل جیسے BERT نے مختلف کاموں میں اثرات دکھائی ہے۔ ان کے طاقت کے بغیر، انہیں کامپیوتروں سے زیادہ سخت معلوم ہوتا ہے، جو حقیقی دنیا کی کاربریوں کو روکتے ہیں۔ اس کاغذ میں ہم نے سند رینگ کے لئے BERT سے پہلے نکلنے کے لئے پیش آئے۔ ایک تھوڑے بدلنے کے ساتھ BERT ایک موڈل بن جاتا ہے جو بہت سی آئٹ پاؤں کے ساتھ ہو جاتا ہے، اور ہر نقصان نمونہ اس طرح سے پہلے نکل سکتا ہے. اسی طرح، کمپیوٹر نمونے کے درمیان اثبات سے تقسیم کر سکتا ہے، اور سارے سیستم لاٹنسیٹی اثبات سے کم کر دی جاتی ہے جبکہ اصلی کیفیت حفاظت کی جاتی ہے. ہماری آزمائش دو ڈیسمکینٹ رینگ ڈیٹ سٹ کے ذریعہ سے 2.5x نازل کی آزمائش کی گئی ہے۔ ہمارے عملومات کے سراسر کوڈ کو پا سکتا ہے https://github.com/castorini/earlyexiting-monobert.', 'no': 'Førehandsvis språk- modeller som BERT har vist effektiviteten sine i ulike oppgåver. Til tross av kraft, er dei kjent til å vera dataintensivt, som hindrar verkelege programmer. I denne papiret introduserer vi tidlegare avslutting av BERT for dokumentranking. Med ein liten endring kan BERT bli eit modell med fleire utdata-baner, og kvar infeksjonssprøver kan avslutta tidlegare frå desse banene. På denne måten kan rekninga bli effektivt tildelt blant prøver, og heile systemlatensitet blir signifikant redusert mens den opprinnelige kvaliteten vert beholda. Eksperimentane våre på to dokumentrankeringsdatasett viser opp til 2,5x infeksjonssfart med minimal kvalitetforgradering. Kjeldekode for implementasjonen vår kan finnast på https://github.com/castorini/earlyexiting-monobert.', 'uz': "BERT kabi vazifalar bilan o'rganilgan tillar modellari ularning effektini ko'rsatadi. Ular maktablari bilan hisoblash muvaffaqiyatli bo'lishi mumkin, bu aslida dunyo dasturlarini sabab beradi. Bu qogʻozda biz BERT'dan chiqishdan avval hujjatni qidirish uchun ishlab chiqaramiz. Bir necha oʻzgarishlar bilan BERT bir necha chizilgan yoʻllar bilan model bo'ladi, va har bir kichkina uslub bu yoʻllardan avval chiqishi mumkin. Bunday qilib, kompyuterni tashqi qilish mumkin, va asl sifatini davom etishda umumiy tizim taʼminligi juda kamaytirish mumkin. Икки ҳужжат кўчирилган маълумотлар тайёрларини кўриб чиқади. 2.5x сифатида кам сифат сифатида бажаришдан афзал кўриб чиқади. Bajarish uchun manba kodi https://github.com/castorini/earlyexiting-monobert.", 'vi': 'Những mẫu ngôn ngữ được đào tạo trước như BERT đã thể hiện hiệu quả trong nhiều nhiệm vụ khác nhau. Bất chấp sức mạnh của họ, họ được biết đến là dữ dội, điều đó cản trở các ứng dụng thế giới thực. Trong tờ báo này, chúng tôi giới thiệu loại bỏ sót sót sớm cho thứ hạng tài liệu. Với một sự thay đổi nhẹ, BERT trở thành mô hình có nhiều đường dẫn xuất, và mỗi mẫu nhận ra có thể thoát ra từ đầu các đường mòn này. Bằng cách này, tính toán có thể được phân chia hiệu quả giữa các mẫu, và tải trí hệ thống trên cùng bị giảm đáng kể trong khi chất lượng ban đầu được duy trì. Những thí nghiệm của chúng tôi trên hai tập tin xếp hạng tài liệu cho thấy tăng tốc độ tối thiểu với độ thoái hóa chất lượng tối thiểu. Mã nguồn của việc thực hiện chúng ta có thể tìm thấy ở https://github.com/castorini/earlyexiting-monobert.', 'da': 'Forududdannede sprogmodeller som BERT har vist deres effektivitet i forskellige opgaver. På trods af deres kraft er de kendt for at være beregningsmæssigt intensive, hvilket hindrer virkelige applikationer. I denne artikel introducerer vi tidligt afsluttende BERT til dokumentranking. Med en lille ændring bliver BERT til en model med flere outputstier, og hver konklusionsprøve kan forlade disse stier tidligt. På denne måde kan beregningen effektivt fordeles mellem prøver, og den samlede systemlatens reduceres betydeligt, mens den oprindelige kvalitet opretholdes. Vores eksperimenter med to dokumentranking datasæt viser op til 2,5x inference hastighed med minimal kvalitetsforringelse. Kildekoden til vores implementering kan findes på https://github.com/castorini/earlyexiting-monobert.', 'nl': 'Vooropgeleide taalmodellen zoals BERT hebben hun effectiviteit in verschillende taken laten zien. Ondanks hun kracht staan ze bekend als rekenintensief, wat toepassingen in de echte wereld belemmert. In dit artikel introduceren we early exit BERT voor document ranking. Met een kleine aanpassing wordt BERT een model met meerdere uitvoerpaden, en elke inferentiesteekproef kan vroegtijdig uit deze paden vertrekken. Op deze manier kan berekening effectief worden toegewezen aan samples, en de totale systeemlatentie wordt aanzienlijk verminderd terwijl de oorspronkelijke kwaliteit wordt gehandhaafd. Onze experimenten met twee document ranking datasets tonen aan dat tot 2,5x inferentie versneld wordt met minimale kwaliteitsdegradatie. De broncode van onze implementatie is te vinden op: https://github.com/castorini/earlyexiting-monobert.', 'bg': 'Предварително обучените езикови модели като BERT показаха своята ефективност при различни задачи. Въпреки тяхната мощ, те са известни с изчислителна интензивност, което пречи на реалните приложения. В тази статия представяме ранното напускане на BERT за класиране на документи. С лека модификация се превръща в модел с множество изходни пътища и всяка извадка от изводи може да излезе рано от тези пътища. По този начин изчисленията могат ефективно да бъдат разпределени между пробите и цялостната латентност на системата значително се намалява, докато първоначалното качество се поддържа. Нашите експерименти с две набори от данни за класиране на документи демонстрират до 2,5 пъти ускорение на заключенията с минимално влошаване на качеството. Източният код на нашата реализация може да бъде намерен на адрес: https://github.com/castorini/earlyexiting-monobert.', 'hr': 'Preobučeni jezički modeli poput BERT pokazali su svoju učinkovitost u raznim zadacima. Uprkos njihovoj moći, poznato je da su računalno intenzivni, što sprječava primjene stvarnog svijeta. U ovom papiru predstavljamo rano izlazak iz BERT-a za ured dokumenta. S malo modifikacije, BERT postaje model sa višestrukim putevima izlaza, a svaki uzorak infekcije može izaći ranije iz tih puteva. Na taj način, računalo se može učinkovito odrediti među uzorcima, a ukupna latencija sustava se značajno smanjuje dok se održava originalna kvalitet. Naši eksperimenti na dvije rezervacije podataka dokumenta pokazuju do 2,5x infekcije ubrzano s minimalnom degradacijom kvalitete. Izvorni kod naše provedbe se nalazi na https://github.com/castorini/earlyexiting-monobert.', 'de': 'Vortrainierte Sprachmodelle wie BERT haben ihre Wirksamkeit bei verschiedenen Aufgaben unter Beweis gestellt. Trotz ihrer Leistung sind sie dafür bekannt, rechenintensiv zu sein, was reale Anwendungen erschwert. In diesem Beitrag stellen wir das frühe Aussteigen von BERT für das Dokumenten-Ranking vor. Mit einer leichten Modifikation wird BERT zu einem Modell mit mehreren Ausgabepfaden, und jede Inferenzprobe kann diese Pfade frühzeitig verlassen. Auf diese Weise kann die Berechnung effektiv auf Samples verteilt werden und die Gesamtsystemlatenz wird deutlich reduziert, während die ursprüngliche Qualität beibehalten wird. Unsere Experimente an zwei Dokumenten-Ranking-Datensätzen zeigen eine bis zu 2,5-fache Inferenz-Beschleunigung bei minimalem Qualitätsverlust. Den Quellcode unserer Implementierung finden Sie unter https://github.com/castorini/earlyexiting-monobert.', 'id': 'Model bahasa terlatih sebelumnya seperti BERT telah menunjukkan efektivitas mereka dalam berbagai tugas. Meskipun kekuatan mereka, mereka dikenal sebagai komputasi intensiv, yang menghalangi aplikasi dunia nyata. Dalam koran ini, kami memperkenalkan BERT keluar awal untuk peringkat dokumen. Dengan sedikit modifikasi, BERT menjadi model dengan banyak jalur keluaran, dan setiap contoh kesimpulan dapat keluar lebih awal dari jalur ini. Dengan cara ini, komputasi dapat secara efektif dialokasi di antara sampel, dan keseluruhan latensi sistem secara signifikan dikurangi sementara kualitas asli dipelihara. Eksperimen kami pada dua set data ranging dokumen menunjukkan hingga 2,5x kecepatan inferensi dengan degradasi kualitas minimal. Kode sumber implementasi kita dapat ditemukan di https://github.com/castorini/earlyexiting-monobert.', 'fa': 'مدل های پیش آموزش زبانی مانند BERT موثیت خود را در کار مختلف نشان داده است. با وجود قدرتشان، شناخته می\u200cشوند که به صورت محاسبه شدید باشند، که کاربردهای دنیای واقعی را مانع می\u200cکند. در این کاغذ، ما به زودی از بیرون رفتن BERT برای رقابت سند معرفی می کنیم. با یک تغییر کوچک، BERT یک مدل با مسیرهای متعدد خروج می شود، و هر نمونه آلودگی می تواند زود از این مسیرها خارج شود. در این صورت، محاسبات می\u200cتواند موثرت در میان نمونه\u200cها تقسیم شود، و در حالی که کیفیت اصلی حفظ می\u200cشود، در کل سیستم تاریکی بسیار کمتر می\u200cشود. آزمایش\u200cهای ما روی دو دسته\u200cهای داده\u200cهای رقابت سند تا ۲.۵x به سرعت افزایش کیفیت minimal نشان می\u200cدهند. کد منبع عملکرد ما در https://github.com/castorini/earlyexiting-monobert.', 'ko': 'BERT와 같은 미리 훈련된 언어 모델은 이미 각종 임무에서 그것들의 유효성을 나타냈다.비록 그것들은 매우 강하지만, 모두가 알고 있는 바와 같이, 그것들의 계산량이 매우 커서, 이것은 실제 응용을 방해한다.본문에서 우리는 초기의 문서 정렬 방법을 소개했다.조금만 수정하면 버트는 여러 개의 출력 경로를 가진 모델이 되어 모든 추리 견본이 이 경로를 앞당겨 퇴출할 수 있다.이런 방식을 통해 견본 간에 계산을 효과적으로 분배할 수 있고 원시적인 품질을 유지하는 동시에 전체 시스템의 지연을 현저히 줄일 수 있다.우리가 두 문서 랭킹 데이터 집합에서의 실험에 의하면 추리 속도는 2.5배에 달하고 품질은 가장 적게 떨어진다고 한다.우리가 실현한 원본 코드는https://github.com/castorini/earlyexiting-monobert.', 'sw': 'Mfano wa lugha zilizojifunza kabla kama vile BERT umeonyesha ufanisi wao katika kazi mbalimbali. Pamoja na nguvu zao, wanajulikana kuwa na mkanganyiko mkubwa, ambao unazuia matumizi halisi ya dunia. Katika karatasi hii, tunautambulisha mapema kutoka BERT kwa ajili ya kuandaa nyaraka. Kwa mabadiliko kidogo, BERT inakuwa mfano wenye njia mbalimbali za utoaji, na kila sampuli ya ugonjwa unaweza kutoka mapema katika njia hizi. Kwa namna hii, kompyuta inaweza kusambazwa kwa ufanisi kati ya sampuli, na kwa ujumla wa mfumo unapunguzwa kwa kiasi kikubwa wakati kiwango cha asili kinaendelea. Majaribio yetu kwenye seti za taarifa hizo mbili zinaonyesha kuongezeka kwa ugonjwa wa 2.5x kwa kiwango cha chini cha kiwango cha ubora. Chanzo cha utekelezaji wetu kinaweza kupatikana katika https://github.com/castorini/earlyexiting-monobert.', 'af': "Vorige opgelei taal modele soos BERT het hulle effektiviteit in verskillende werke vertoon. Onthou hul krag, is hulle bekend om rekenaar intensief te wees, wat hinder regte wêreld toepassings. In hierdie papier, introduseer ons vroeg uitgang van BERT vir dokument rangering. Met 'n klein verandering, BERT word 'n model met veelvuldige uitvoer pad, en elke inferensie voorbeeld kan vroeg van hierdie paaie uitgaan. Op hierdie manier kan rekenaar effektief onder voorbeelde toegewys word, en die hele stelsel latensie is betekenlik verduur terwyl die oorspronklike kwaliteit onderhou word. Ons eksperimente op twee dokument rangering datastelle vertoon tot 2. 5x inferensie speeduiging met minimaal kwaliteit versterking. Die bronkode van ons implementasie kan gevind word by https://github.com/castorini/earlyexiting-monobert.", 'sq': 'Modelet e paratrajnuara gjuhësh të tilla si BERT kanë treguar efektshmërinë e tyre në detyra të ndryshme. Megjithë fuqinë e tyre, ata janë të njohur për të qenë kompjuterisht intensive, gjë që pengon aplikimet e botës reale. Në këtë gazetë, ne paraqesim BERT për renditjen e dokumenteve. Me një modifikim të vogël, BERT bëhet një model me shumë rrugë të daljes, dhe çdo shembull përfundimi mund të dalë herët nga këto rrugë. Në këtë mënyrë, llogaritja mund të shpërndahet efektivisht midis mostrave dhe vonesa e përgjithshme e sistemit reduktohet ndjeshëm ndërsa cilësia origjinale mbahet. Eksperimentet tona në dy grupe të dhënash të renditjes së dokumenteve demonstrojnë deri në 2.5x inferencë shpejtësi me degradim minimal kualiteti. Kodi burimi i zbatimit tonë mund të gjendet në https://github.com/castorini/earlyexiting-monobert.', 'am': 'BERT እንደተደረገው የቋንቋ ምሳሌዎች በተለያዩ ስራ ውስጥ ጥቃታቸውን አሳየዋል፡፡ ምንም እንኳን ኃይላቸው ቢሆንም እውነተኛ ዓለምን ፕሮግራሞች የሚከለክሉ አካባቢ ሆኖ የሚታወቁ ናቸው፡፡ በዚህ ገጽ ውስጥ BERT ለመውጣት ለመጀመሪያ እናሳውቃለን፡፡ በጥቂት ለውጥ BERT በብዛት የውጤት መንገዶች ምሳሌ ሆኖ ይሆናል፣ ሁሉም ነጥብ ምሳሌ ከዚህች መንገዶች ሲወጣ ይችላል። እንደዚህም፣ ቁጥጥር በጥሩ በሚምሳሌዎች መካከል ይካፈላል፣ የጥያቄው ጥያቄ በተጠበቀው ጊዜ በሙሉ ስርዓት በጣም ያጎድላል፡፡ በሁለቱ ሰነድ ላይ የዳታ መስመር ፈተናዎቻችን 2.5x ጥቅምት በሚያነስ ጥቁረት የሚያሳዩ ናቸው፡፡ የአስፈላጊያችን የኩነቶች ኮድ https://github.com/castorini/earlyexiting-monobert.', 'hy': 'Նախապատրաստված լեզվի մոդելները, ինչպիսիք են BER-ը, ցույց են տվել իրենց արդյունավետությունը տարբեր խնդիրներում: Despite their power, they are known to be computationally intensive, which hinders real-world applications.  Այս թղթի մեջ մենք ներկայացնում ենք վաղ դուրս գալիս BER-ից փաստաթղթերի դասավորման համար: Մի փոքր փոփոխության դեպքում BER-ը դառնում է մի մոդել, որը ունի բազմաթիվ ելքի ուղիներ, և յուրաքանչյուր եզրակացության նմուշ կարող է վաղ դուրս գալ այս ուղիներից: In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained.  Մեր երկու փաստաթղթերի դասակարգման տվյալների փորձարկումները ցույց են տալիս մինչև 2.5x հետևյալների արագությունը մինիմալ որակի դեգրադացիայի հետ: Մեր իրականացման աղբյուր կոդը կարող է գտնվել https://github.com/castorini/earlyexiting-monobert.', 'az': "BERT kimi əvvəl təhsil edilmiş dil modelləri müxtəlif işlərdə etkinlik göstərdilər. Onların gücünə baxmayaraq, onlar bilərlər ki, həqiqət dünya uyğulamalarını engelleyirlər. Bu kağızda, belə yazılmış səhifələr üçün BERT'dan ilk çıxmaq üçün xəbər veririk. Biraz dəyişiklik ilə, BERT çoxlu çıxış yolları ilə modeli olar, və hər a şağılıq nümunələri bu yollardan erkən çıxır. Bu yolda, hesaplama nümunələri arasında faydalı olaraq sadə edilə bilər, və bütün sistem sonrakı səbəbi təmizlənərkən təmizlənir. İki dökümət sıralaması veri qurğuları təminatlarımız minimal keyfiyyət degradasyonu ilə 2,5x aşağılığı göstərir. Bizim istifadəçimizin mənbə kodu https://github.com/castorini/earlyexiting-monobert.", 'tr': "BERT ýaly öň-bilinmeli dil nusgalary görkezilýän işlerinde etkinliýeti görkezildi. Ýerleriniň güýçlerine raýratyn bilýän bilýänler kompýuterli ýigrenç bolup, bu ähli dünýä uygulamalaryny çykarýar. Bu kagyzda, sened derejesi üçin BERT'den ir çykyp taýýarlaýarys. Biraz üýtgetmek bilen, BERT birnäçe çykyş ýunlary bilen bir nusga bolar we her çykyş nusga bu ýunlardan irräk çykar. Bu şekilde, hesaplamak örnekler arasynda efektiv bölünebilir we dünýä sistemiň geçmişi derejesi özine çykarylýar. Iki sened sahypalarynda synanyşlarymyz az kwalitet ýitirjek bilen 2.5x hasaplanjak taýýarlandyrylýar. Biziň implementasiýanyň çeşme kody tapylyp biler. https://github.com/castorini/earlyexiting-monobert.", 'bn': 'পূর্ব প্রশিক্ষিত ভাষার মডেল বিভিন্ন কাজে তাদের কার্যকর প্রদর্শন করেছে। তাদের ক্ষমতা সত্ত্বেও তারা গণনাত্রিকভাবে শক্তিশালী হিসেবে পরিচিত, যা আসল বিশ্বের অ্যাপলিকেশন বাধা দে এই কাগজটিতে আমরা বের্টি থেকে প্রাথমিক দেখাচ্ছি ডকুমেন্ট রেঙ্কিং এর জন্য। সামান্য পরিবর্তনের মাধ্যমে বিআরটি বেশ কয়েকটি আউটপুট পাথের মডেলে পরিণত হয় এবং প্রত্যেকটি ইনফেন্সের নমুনা এই পথ থেকে দ্রুত ব এভাবে নমুনালের মধ্যে কার্যকর ভাবে গণনা বিতরণ করা যাবে এবং সাধারণ সিস্টেমের সাম্প্রতিক সংক্রান্ত গুরুত্বপূর্ণ কমে যাবে  দুই নথির রেঙ্কিং ডাটাসেটে আমাদের পরীক্ষার পরীক্ষা দেখা যাচ্ছে ২.৫x এর সংক্রান্ত ব্যবস্থা দ্রুত গতির দিকে য আমাদের ব্যবস্থাপনার উৎস কোড পাওয়া যাবে https://github.com/castorini/earlyexiting-monobert.', 'bs': 'Preobučeni jezički modeli poput BERT pokazali su svoju učinkovitost u različitim zadacima. Uprkos njihovoj moći, poznato je da su kompjuterski intenzivni, što sprječava primjene stvarnog svijeta. U ovom papiru predstavljamo rano izlazak iz BERT-a za ured dokumenta. S malo modifikacije, BERT postaje model sa višestrukim putevima izlaza, i svaki uzorak infekcije može izaći ranije iz tih puteva. Na taj način, računalo se može učinkovito odrediti među uzorcima, a ukupna sistemska latencija se značajno smanjuje dok se održava originalna kvalitet. Naši eksperimenti na dvije kategorije dokumenta pokazuju do 2,5x infekcije ubrzano sa minimalnom degradacijom kvalitete. Izvorni kod naše provedbe se nalazi na https://github.com/castorini/earlyexiting-monobert.', 'cs': 'Předškolené jazykové modely jako BERT prokázaly svou efektivitu v různých úkolech. Navzdory jejich výkonu jsou známy jako výpočetně náročné, což brání reálným aplikacím. V tomto článku představujeme časné ukončení BERT pro pořadí dokumentů. S mírnou úpravou se BERT stává modelem s více výstupními cestami a každý vzorek inference může z těchto cest brzy opustit. Tímto způsobem lze výpočet efektivně rozdělit mezi vzorky a celková latence systému je výrazně snížena při zachování původní kvality. Naše experimenty na dvou datových sadách hodnocení dokumentů ukazují až 2,5x zrychlení inference s minimální degradací kvality. Zdrojový kód naší implementace naleznete na adrese: https://github.com/castorini/earlyexiting-monobert.', 'et': 'Eelkoolitud keelemudelid, nagu BERT, on näidanud oma efektiivsust erinevates ülesannetes. Hoolimata nende võimsusest on nad teadaolevalt arvutusintensiivsed, mis takistab reaalmaailma rakendusi. Käesolevas dokumendis tutvustame BERTi varajast väljumist dokumentide järjestamiseks. Väikese muudatusega muutub BERT mudeliks, millel on mitu väljundteed ja iga järeldusnäidis võib neist teedest varakult väljuda. Sel viisil saab arvutusi tõhusalt jaotada proovide vahel ja süsteemi üldine latentsus väheneb oluliselt, samal ajal kui säilitatakse algne kvaliteet. Meie katsed kahe dokumendi järjestuse andmekogumiga näitavad kuni 2,5-kordset järelduste kiirenemist minimaalse kvaliteedi halvenemisega. Meie rakenduse lähtekoodi leiate aadressilt https://github.com/castorini/earlyexiting-monobert.', 'fi': 'Esikoulutetut kielimallit, kuten BERT, ovat osoittaneet tehokkuutensa erilaisissa tehtävissä. Tehostaan huolimatta niiden tiedetään olevan laskennallisesti intensiivisiä, mikä vaikeuttaa reaalimaailman sovelluksia. Tässä artikkelissa esittelemme varhaisen poistumisen BERT-järjestelmästä asiakirjojen luokittelua varten. Pienellä muutoksella BERT:stä tulee malli, jossa on useita lähtöpolkuja, ja jokainen päätelmänäyte voi poistua näistä poluista aikaisin. Näin laskenta voidaan jakaa tehokkaasti näytteiden kesken, ja järjestelmän kokonaisviive pienenee merkittävästi alkuperäisen laadun säilyessä. Kahdella asiakirjasijoitusaineistolla tehdyt kokeemme osoittavat jopa 2,5-kertaisen päättelynopeuden ja minimaalisen laadun heikkenemisen. Toteutuksemme lähdekoodi löytyy osoitteesta https://github.com/castorini/earlyexiting-monobert.', 'ca': "Models de llenguatges pré-entrenats com BERT han demostrat la seva eficacia en diverses tasques. Malgrat el seu poder, es coneix que són computacionalment intensives, que dificulta les aplicacions del món real. En aquest article, introduïm BERT de sortida anticipada per a classificar els documents. Amb una lleugera modificació, BERT es converteix en un model amb múltiples camins de sortida, i cada mostra de inferència pot sortir aviat d'aquests camins. D'aquesta manera, els càlculs es poden distribuir efectivament entre les mostres, i la latencia global del sistema es redueix significativament mentre la qualitat original es manté. Els nostres experiments en dos grups de dades de classificació de documents demostren fins a 2,5x velocitats de inferència amb una degradació mínima de qualitat. El codi original de la nostra implementació es pot trobar a https://github.com/castorini/earlyexiting-monobert.", 'sk': 'Predhodno usposobljeni jezikovni modeli, kot je BERT, so pokazali svojo učinkovitost pri različnih nalogah. Kljub svoji moči so znani kot računalniško intenzivni, kar ovira aplikacije v realnem svetu. V tem članku predstavljamo zgodnji izhod iz BERT za razvrščanje dokumentov. Z rahlo spremembo BERT postane model z več izhodnimi potmi in vsak vzorec sklepanja lahko zgodaj zapusti te poti. Na ta način se lahko izračun učinkovito razporedi med vzorce, skupna latenca sistema pa se občutno zmanjša, medtem ko se ohranja prvotna kakovost. Naši poskusi na dveh naborih podatkov razvrščanja dokumentov dokazujejo do 2,5-kratno pospešitev sklepanja z minimalnim poslabšanjem kakovosti. Izvorno kodo naše implementacije lahko najdete na spletni strani https://github.com/castorini/earlyexiting-monobert.', 'jv': 'Rasane Lane model sing wis prakargu banget, koyo BERT iso nggawe efetisi nang sampeyan operasi banget Ing kabèh ning pisan wong, wong liya sing ngerasakno ning acara komputer, sing nggawe program sing apik tur angel. Nang papat iki, kita ngubah tanggal nggo sabanjur BERT kanggo awak dhéwé. BERT kuwi model sing dibutuhke Nang budhakan iku, sampeyan iso nggawe saiki akeh operasi ning sampeyan, lan kabèh sistem dadi sing isih vewidigoroh dumadhi kapan ingkang sampeyan ora bisa dumadhi. Awakdhéwé éntuk kayané durung dokumen punika dataSet kuwi nggawe dataSet kuwi 2.5 x dituruti kesempakan karo degradisi nggawe kalite minimal Kowe Sistem Where https://github.com/castorini/earlyexiting-monobert.', 'bo': 'BERT (BERT)ནང་གི་སྔོན་གྲངས་སྒྲིག་གི་སྐད་ཡིག་གཟུགས་རིས་མངོན་གསལ་བ་ཡིན་པ། དེ་ཚོའི་ནུས་ཀྱི་སྒུལ་གྱིས་དེ་དག་རྩིས་རྩིས་འཁོར་གྱི་ཐབས་ལམ་དེ་འགྱུར་བའི་དྲན་ཤེས་ཀྱི་ཡོད། ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་ཡིག་གེ་ལྟར་སྔོན་ཕྱིར་ཐོན་པའི་BERT་ནས་ཕྱིར་འཐོན་བྱེད་དུས། With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. འདི་ལྟར། རྩིས་འཁོར་རྩིས་ཐོག ང་ཚོའི་བརྟག་ཞིབ་ཡིག་གེ་གནས་སྟངས་ranking གནད་སྡུད་ཚན་གཉིས་ཀྱིས་མཐོང་བ་རྐྱེན་ཚད་ཉུང་བའི་རྐྱེན་ཚད་ལྡན་ཉེན་པས ང་ཚོའི་འཇུག་སྣོད་ཀྱི་ཐོག་ཁུངས་ཨང་རྙེད་ཐུབ་པ https://github.com/castorini/earlyexiting-monobert.', 'he': 'דוגמני שפה מאומנים מראש כמו BERT הראו את היעילות שלהם במשימות שונות. למרות הכוח שלהם, הם ידועים להיות חישובים אינטנסיביים, מה שמחסך את היישומים בעולם האמיתי. בעיתון הזה, אנחנו מציגים את BERT יציאה מוקדמת לדרגה מסמכים. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths.  In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained.  הניסויים שלנו על שני מסמכים מערכות מידע מראים עד 2.5x מהירות ההנחה עם פיצול איכות מינימלי. קוד המקור של ההפעלה שלנו ניתן למצוא ב https://github.com/castorini/earlyexiting-monobert.', 'ha': '@ info: whatsthis Yana sanin su ƙidãyar matsayi, kuma kan kange shiryoyin ayuka masu cikin dũniya. Ga wannan takardan nan, za mu fara tafiyar da BERT don yin danna karatun. Idan an sake gyare wani abu kaɗan, BERT ya kasance wata motel mai haɗi na hanyõyi masu yawa, kuma kowace misali na kasa iya fita fara daga wannan hanyõyi. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained.  Kayan jarrabõyinmu na samun danne biyu na takardar aiki, sun nuna kashi 2.5x don ya ƙara da lokacin da aka girmama girmama. Ana iya sãmun kodi na aikin mu da https://github.com/castorini/earlyexiting-monobert.'}
{'en': 'A Little Bit Is Worse Than None : Ranking with Limited Training Data', 'ar': 'قليلا أسوأ من لا شيء: الترتيب مع بيانات تدريب محدودة', 'fr': "Un peu est pire que rien\xa0: classement avec des données d'entraînement limitées", 'es': 'Un poco es peor que nada: clasificación con datos de entrenamiento limitados', 'pt': 'Um pouco é pior do que nada: classificação com dados de treinamento limitados', 'ja': '少しでも悪くない：限られたトレーニングデータによるランキング', 'ru': 'Немного хуже, чем нет: рейтинг с ограниченными учебными данными', 'zh': '一点比无更,练数有限名', 'hi': 'थोड़ा सा किसी से भी बदतर नहीं है: सीमित प्रशिक्षण डेटा के साथ रैंकिंग', 'ga': 'Tá Giotán Beag Níos measa ná Ceann ar bith: Rangú le Sonraí Oiliúna Teoranta', 'el': 'Ένα μικρό κομμάτι είναι χειρότερο από κανένα: Κατάταξη με περιορισμένα δεδομένα κατάρτισης', 'ka': 'მალკჲ მალკჲ ვ ოჲ-დჲლწმა ჲრ ნთკჲი: პანჟთპაŒვ ჟ ლთფნთ რპვნთნდ ეატარა', 'lt': 'Truputį blogiau nei nė vienas: riboto mokymo duomenys', 'kk': 'Жоғарылған оқыту деректерімен ауысу', 'ms': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'mk': 'Мал бит е полош од ништо: поставување со ограничени податоци за обука', 'it': "Un po 'è peggio di nessuno: classifica con dati di allenamento limitati", 'hu': 'Egy kicsit rosszabb, mint senki: rangsorolás korlátozott edzési adatokkal', 'mt': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'no': 'Name', 'ro': 'Un pic este mai rău decât niciunul: Clasament cu date limitate de antrenament', 'pl': 'Trochę jest gorsze niż żaden: ranking z ograniczonymi danymi treningowymi', 'mn': 'Бага зэрэг бага зэрэг бага зэрэг биш: Хязгаарлагдсан суралцах өгөгдлийн хувьд', 'so': 'Bit yar is worse than None: Ranking with Limited Training Data', 'sv': 'Lite är värre än ingen: Rankning med begränsad träningsdata', 'si': 'පුංචි බිට් වඩා වඩා වඩා වඩා වැඩියි: සීමාන්\u200dය ප්\u200dරකාශනය දත්ත තියෙන වැඩියි.', 'ta': 'ஒரு சிறிய பிட் எதுவும் மோசமானது இல்லை: வரம்பு பயிற்சி தரவுடன் Ranking', 'ml': 'ഒരു ചെറിയ ബിറ്റ് ഒന്നിനെക്കാളും മോശമാണ്: പരിശീലന വിവരങ്ങളുമായി റാങ്ങിംഗ് ചെയ്യുന്നു', 'sr': 'Malo je gore od ničega: Ranking sa ograničenim podacima obuke', 'ur': 'A little bit is worse than none: Ranking with Limited Training Data', 'vi': 'Có một ít tác dụng còn tệ hơn không:', 'uz': 'Name', 'bg': 'Малко е по-лошо от никой: класиране с ограничени данни за обучение', 'hr': 'Malo je gore od ničega: Ranking s ograničenim podacima obuke', 'nl': 'Een beetje is slechter dan geen: Ranking met beperkte trainingsgegevens', 'da': 'En lille smule er værre end ingen: Ranking med begrænsede træningsdata', 'de': 'Ein bisschen ist schlimmer als keine: Ranking mit begrenzten Trainingsdaten', 'ko': '훈련 데이터의 유한한 순위', 'fa': 'یه ذره بدتر از هیچکدوم نیست: Ranking with Limited Training Data', 'id': 'Sedikit Bit Lebih Terburuk Dari Tiada: Ranking dengan Data Latihan Terbatas', 'sw': 'Biti ndogo ni mbaya zaidi ya hakuna: Kuhusu na Taarifa Mipaka', 'tr': 'Biraz Bit Is Worse than None: Ranking with Limited Training Data', 'af': 'Name', 'sq': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'hy': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'am': 'ምንም', 'az': 'Biraz daha Worst than None: Ranking with Limited Training Data', 'cs': 'Malý kousek je horší než žádný: hodnocení s omezenými tréninkovými daty', 'bs': 'Malo je gore od ničega: Ranking sa ograničenim podacima obuke', 'bn': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'fi': 'Hieman huonompi kuin ei mitään: Ranking rajoitetulla harjoitustiedolla', 'ca': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'et': 'Natuke on hullem kui puudub: piiratud koolitusandmetega järjestus', 'jv': 'A little bit is Worse than None: ranking with limiting Learning data', 'ha': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'sk': 'Malo je slabše od nič: uvrstitev z omejenimi podatki o usposabljanju', 'bo': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data', 'he': 'A Little Bit Is Worse Than None: Ranking with Limited Training Data'}
{'en': 'Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question : How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that some labeled in-domain data can be worse than none at all.', 'ar': 'اقترح الباحثون تقنيات بسيطة لكنها فعالة لمشكلة الاسترجاع استنادًا إلى استخدام BERT كمصنف صلة لإعادة تصنيف المرشحين الأوليين من البحث عن الكلمات الرئيسية. في هذا العمل ، نتصدى للتحدي المتمثل في ضبط هذه النماذج لمجالات محددة في البيانات وبطريقة حسابية فعالة. عادةً ما يقوم الباحثون بضبط النماذج بدقة باستخدام البيانات المُصنفة الخاصة بالمجموعة من مصادر مثل TREC. نجيب أولاً على السؤال: ما مقدار البيانات التي نحتاجها من هذا النوع؟ إدراكًا أن التدريب الأكثر كفاءة من الناحية الحسابية ليس تدريبًا ، فإننا نستكشف الترتيب الصفري باستخدام نماذج BERT التي تم ضبطها بالفعل باستخدام مجموعة بيانات استرجاع ممر MS MARCO الكبيرة. لقد توصلنا إلى اكتشاف مفاجئ وجديد وهو أن "بعض" بيانات النطاق التي تحمل تصنيفًا يمكن أن تكون أسوأ من عدم وجود أي بيانات على الإطلاق.', 'es': 'Los investigadores han propuesto técnicas simples pero eficaces para el problema de recuperación basadas en el uso de BERT como clasificador de relevancia para clasificar a los candidatos iniciales de la búsqueda de palabras clave. En este trabajo, abordamos el desafío de ajustar estos modelos para dominios específicos de una manera eficiente desde el punto de vista computacional y de los datos. Por lo general, los investigadores ajustan los modelos utilizando datos etiquetados específicos del cuerpo de fuentes como TREC. Primero respondemos a la pregunta: ¿Cuántos datos de este tipo necesitamos? Reconociendo que el entrenamiento más eficiente desde el punto de vista computacional no es el entrenamiento, exploramos la clasificación de tiro cero utilizando modelos BERT que ya se han ajustado con el gran conjunto de datos de recuperación de pasajes MS MARCO. Llegamos al sorprendente y novedoso hallazgo de que «algunos» datos dentro del dominio etiquetados pueden ser peores que ninguno.', 'fr': "Les chercheurs ont proposé des techniques simples mais efficaces pour résoudre le problème de la recherche basées sur l'utilisation du BERT comme classificateur de pertinence pour classer les candidats initiaux à partir de la recherche par mots clés. Dans ce travail, nous relevons le défi d'affiner ces modèles pour des domaines spécifiques d'une manière efficace sur le plan des données et des calculs. Généralement, les chercheurs affinent les modèles à l'aide de données étiquetées spécifiques au corpus provenant de sources telles que TREC. Nous répondons d'abord à la question\xa0: De quelle quantité de données de ce type avons-nous besoin\xa0? Reconnaissant que l'entraînement le plus efficace sur le plan informatique est l'absence d'entraînement, nous explorons le classement zéro tir à l'aide de modèles BERT qui ont déjà été affinés avec le vaste ensemble de données de récupération de passages MS MARCO. Nous arrivons à la découverte surprenante et novatrice selon laquelle «\xa0certaines\xa0» données dans le domaine étiquetées peuvent être pires qu'aucune donnée du tout.", 'pt': 'Pesquisadores propuseram técnicas simples, porém eficazes, para o problema de recuperação com base no uso do BERT como um classificador de relevância para reclassificar os candidatos iniciais da pesquisa de palavras-chave. Neste trabalho, enfrentamos o desafio de ajustar esses modelos para domínios específicos de maneira eficiente em dados e computacionalmente. Normalmente, os pesquisadores ajustam os modelos usando dados rotulados específicos do corpus de fontes como TREC. Primeiro respondemos à pergunta: De quantos dados desse tipo precisamos? Reconhecendo que o treinamento mais eficiente computacionalmente não é nenhum treinamento, exploramos a classificação zero-shot usando modelos BERT que já foram ajustados com o grande conjunto de dados de recuperação de passagem MS MARCO. Chegamos à descoberta surpreendente e inovadora de que “alguns” dados rotulados no domínio podem ser piores do que nenhum.', 'ja': '研究者は、BERTを関連性分類子として使用して、キーワード検索から初期候補を再ランク付けすることに基づいて、単純でありながら効果的な検索問題の手法を提案している。この研究では、データと計算効率的な方法で特定のドメインのためにこれらのモデルを微調整するという課題に取り組んでいます。通常、研究者はTRECなどのソースからのコーパス固有のラベル付けされたデータを使用してモデルを微調整する。まず質問に答えますこのタイプのデータはどれくらい必要ですか？最も計算効率の高いトレーニングはトレーニングではないことを認識しているため、大型MS MARCOパッセージ検索データセットですでに微調整されているBERTモデルを使用して、ゼロショットのランキングを探ります。私たちは、ドメイン内データにラベル付けされた「いくつか」は、まったくないよりも悪い可能性があるという驚くべき新規の発見に到達します。', 'zh': '论者举简而效术,基于用BERT为相关性类器来复排名关键字搜索之初候选者。 于是以数效率为特定率。 凡治人用TREC等特定于语料库标数以调之。 先应曰:须多少之数? 知计效率至教为无训练,吾以BERT索零次名,已用大MS MARCO道检数而微调之。 我们得出了一个可惊的新见,就"些"标记的域内数据比没有更糟粕。', 'hi': 'शोधकर्ताओं ने कीवर्ड खोज से प्रारंभिक उम्मीदवारों को फिर से तैयार करने के लिए एक प्रासंगिकता क्लासिफायर के रूप में BERT का उपयोग करने के आधार पर पुनर्प्राप्ति समस्या के लिए सरल अभी तक प्रभावी तकनीकों का प्रस्ताव दिया है। इस काम में, हम एक डेटा और कम्प्यूटेशनल रूप से कुशल तरीके से विशिष्ट डोमेन के लिए इन मॉडलों को ठीक-ट्यूनिंग की चुनौती से निपटते हैं। आमतौर पर, शोधकर्ताओं ने TREC जैसे स्रोतों से कॉर्पस-विशिष्ट लेबल डेटा का उपयोग करके मॉडल को ठीक किया। हम पहले इस सवाल का जवाब देते हैं: हमें इस प्रकार के कितने डेटा की आवश्यकता है? यह मानते हुए कि सबसे कम्प्यूटेशनल रूप से कुशल प्रशिक्षण कोई प्रशिक्षण नहीं है, हम BERT मॉडल का उपयोग करके शून्य-शॉट रैंकिंग का पता लगाते हैं जो पहले से ही बड़े एमएस मार्को मार्ग पुनर्प्राप्ति डेटासेट के साथ ठीक-ठाक हो चुके हैं। हम आश्चर्यजनक और उपन्यास खोज पर पहुंचते हैं कि "कुछ" लेबल किए गए इन-डोमेन डेटा किसी से भी बदतर नहीं हो सकते हैं।', 'ru': 'Исследователи предложили простые, но эффективные методы для задачи поиска, основанные на использовании BERT в качестве классификатора релевантности для перегруппировки первоначальных кандидатов из поиска по ключевым словам. В этой работе мы решаем задачу доработки этих моделей для конкретных областей с использованием данных и вычислительной эффективности. Как правило, исследователи точно настраивают модели с использованием корпусно-специфических маркированных данных из таких источников, как TREC. Сначала мы отвечаем на вопрос: сколько данных такого типа нам нужно? Признавая, что наиболее вычислительно эффективное обучение - это отсутствие обучения, мы исследуем ранжирование с нулевым выстрелом с использованием моделей BERT, которые уже были доработаны с большим набором данных для извлечения пассажей MS MARCO. Мы приходим к удивительному и новому выводу, что «некоторые» помеченные внутридоменные данные могут быть хуже, чем вообще никакие.', 'ga': 'Tá teicníochtaí simplí fós éifeachtacha molta ag taighdeoirí don fhadhb aisghabhála bunaithe ar úsáid a bhaint as BERT mar aicmitheoir ábharthachta chun iarrthóirí tosaigh a athrangú ó chuardach eochairfhocail. San obair seo, téimid i ngleic leis an dúshlán a bhaineann le mionchoigeartú a dhéanamh ar na samhlacha seo d’fhearainn ar leith ar bhealach atá éifeachtach ó thaobh sonraí agus ríomhaireachta de. Go hiondúil, déanann taighdeoirí mionchoigeartú ar shamhlacha ag baint úsáide as sonraí lipéadaithe corpais-shonracha ó fhoinsí ar nós TREC. Ar dtús freagraimid an cheist: Cé mhéad sonraí den chineál seo a theastaíonn uainn? Agus sinn ag aithint gurb í an oiliúint is éifeachtaí ó thaobh ríomhaireachtúil ná aon oiliúint, déanaimid iniúchadh ar rangú náid-shots ag baint úsáide as samhlacha BERT atá mionchoigeartaithe cheana féin le tacar sonraí mór aisghabhála pasáiste MS MARCO. Tagann muid ar an gcinneadh iontasach agus nua-aimseartha gur féidir le sonraí “roinnt” lipéadaithe san fhearann a bheith níos measa ná aon cheann ar bith.', 'ka': "პროცექტორები უფრო მარტივი, მაგრამ ეფექტიური ტექნოგიები გამოყენება BERT-ის გამოყენებაზე, როგორც მნიშვნელოვანი კლასიფიკაციატორია, რომ კლასიფორმაციის სიტყვების ამ სამუშაოში, ჩვენ განვითარებთ ამ მოდელს სპექტიფიკური დიომენზე და კომპუტაციულად ეფექტიურად გამოყენება. ტიპულად, შესწავლობელი მოდელები, რომლებიც TREC იყენებენ corpus-სპექტიფიკალური მონაცემების გამოყენება. ჩვენ პირველი პასუხით კითხვაზე: ამ ტიპის რამდენი მონაცემები გვჭირდება? მოცნობით, რომ ყველაზე კომპუტაციალურად ეფექტიური განაცნობა არ არის განაცნობა, ჩვენ ვაკეთებთ ნულ სტატის რენექტირების გამოყენება BERT მოდელების გამოყენება, რომლებიც უკვე უფრო და ჩვენ მივიღეთ საინტერესო და პრომენში, რომელსაც აღმოჩენა, რომ 'ზოგიერთი' მონაცემები, რომელსაც საკუთარი დომინში იქნება უკეთესია, ვიდრე არაფერ", 'hu': 'A kutatók egyszerű, mégis hatékony technikákat javasoltak a lekeresési probléma tekintetében, amelyek alapján a BERT relevancia osztályozóként használják a kulcsszókeresésből származó kezdeti jelöltek rangsorolását. Ebben a munkában foglalkozunk azzal a kihívással, hogy ezeket a modelleket adatok és számítástechnikai szempontból hatékony módon finomhangoljuk. A kutatók általában a modelleket olyan forrásokból származó, corpus-specifikus címkézett adatokkal finomhangolják, mint a TREC. Először is válaszolunk a kérdésre: Mennyi ilyen típusú adatra van szükségünk? Felismerve, hogy a számítástechnikailag leghatékonyabb edzés nem az edzés, ezért olyan BERT modellek segítségével vizsgáljuk fel, amelyeket már finomhangoltunk a nagyméretű MS MARCO áthaladó visszakereső adatkészlettel. Meglepő és újszerű megállapításhoz érkezünk, hogy "néhány" domain-címkével ellátott adat rosszabb lehet, mint egyáltalán nem.', 'it': "I ricercatori hanno proposto tecniche semplici ma efficaci per il problema di recupero basate sull'utilizzo di BERT come classificatore di rilevanza per riorientare i candidati iniziali dalla ricerca di parole chiave. In questo lavoro affrontiamo la sfida di perfezionare questi modelli per domini specifici in modo dati ed efficiente dal punto di vista computazionale. In genere, i ricercatori affinano i modelli utilizzando dati etichettati specifici del corpo provenienti da fonti come TREC. Per prima cosa rispondiamo alla domanda: Quanti dati di questo tipo abbiamo bisogno? Riconoscendo che l'allenamento più efficiente dal punto di vista computazionale non è l'allenamento, esploriamo il ranking zero-shot utilizzando modelli BERT già perfezionati con il grande set di dati MS MARCO per il recupero dei passaggi. Arriviamo alla sorprendente e innovativa scoperta che alcuni dati in-domain etichettati possono essere peggiori di nessuno.", 'kk': "Рұқсаттары BERT бағдарламасын іздеу үшін бастапқы кандидаттарды қайталау үшін қарапайым, бірақ эффективні мәселеге негізделген қарапайым технологиялар жұмыс істеді. Бұл жұмыс ішінде бұл үлгілерді өзгерту үшін деректерді және компьютерлік кәдімгі көмектесетін үлгілерді жақсы түзету үшін шешіміз. Жалпы түрде, зерттеушілер TREC секілді корпус арқылы мәліметтерден мәліметтерді қолданады. Біз бірінші сұраққа жауап береміз: Бұл түрінің қанша деректер керек? Біз компьютерлік және ең эффективті оқыту оқыту емес дегенді түсініп, BERT моделдерінің үлкен MS MARCO пассажасын алу деректер жиынымен керек келтірілген берт үлгілерін қолданатын нөл шарттарды зерттеп,  Біз сәйкес және романға келдік, доменде жазылған 'кейбір' деректері ешқандай жаман болуы мүмкін.", 'lt': 'Mokslininkai pasiūlė paprastus, tačiau veiksmingus būdus atkūrimo problemai spręsti, grindžiamus BERT kaip svarbos klasifikatoriaus naudojimu, kad pirminiai kandidatai būtų iš naujo sujungti iš pagrindinių žodžių paieškos. Šiame darbe sprendžiame uždavinį tiksliai pritaikyti šiuos konkrečių sričių modelius duomenų ir skaičiavimo požiūriu veiksmingu būdu. Paprastai mokslininkai patobulina modelius, naudojančius korpusui specifinius ženklintus duomenis iš tokių šaltinių kaip TREC. Pirmiausia atsakome į klausimą: kiek tokių duomenų mums reikia? Atsižvelgdami į tai, kad skaičiavimo požiūriu veiksmingiausias mokymas nėra mokymas, mes tiriame nulinę klasifikaciją naudojant BERT modelius, kurie jau buvo patobulinti su dideliu MS MARCO perėjimo paieškos duomenų rinkiniu. Mes darome nuostabią ir naują išvadą, kad "kai kurie" paženklinti domeniniai duomenys gali būti blogesni nei jokie.', 'el': 'Οι ερευνητές έχουν προτείνει απλές αλλά αποτελεσματικές τεχνικές για το πρόβλημα ανάκτησης βασισμένες στη χρήση του BERT ως ταξινομητή συνάφειας για την επανακατάταξη των αρχικών υποψηφίων από την αναζήτηση λέξεων-κλειδιών. Σε αυτή την εργασία, αντιμετωπίζουμε την πρόκληση της τελειοποίησης αυτών των μοντέλων για συγκεκριμένους τομείς με τρόπο δεδομένων και υπολογιστικά αποδοτικό. Συνήθως, οι ερευνητές συντονίζουν μοντέλα χρησιμοποιώντας δεδομένα με ετικέτα ειδικά για το σώμα από πηγές όπως το TREC. Πρώτα απαντάμε στο ερώτημα: Πόσα δεδομένα αυτού του τύπου χρειαζόμαστε; Αναγνωρίζοντας ότι η πιο υπολογιστικά αποδοτική εκπαίδευση δεν είναι εκπαίδευση, εξερευνούμε την κατάταξη μηδενικών πυροβολισμών χρησιμοποιώντας μοντέλα που έχουν ήδη ρυθμιστεί με το μεγάλο σύνολο δεδομένων ανάκτησης περάσματος. Φτάνουμε στο εκπληκτικό και καινοτόμο συμπέρασμα ότι "ορισμένα" δεδομένα εντός του τομέα μπορούν να είναι χειρότερα από κανένα απολύτως.', 'mk': "Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search.  In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner.  Обично, истражувачите ги прилагодуваат моделите користејќи корпус-специфични податоци од извори како што е TREC. We first answer the question: How much data of this type do we need?  Признавајќи дека најефикасната обука не е обука, истражуваме рангирање со нула стрела користејќи BERT модели кои веќе се финетизирани со големиот набор на податоци за преземање на MS MARCO. We arrive at the surprising and novel finding that 'some' labeled in-domain data can be worse than none at all.", 'ms': "Penyelidik telah melamar teknik sederhana tetapi berkesan untuk masalah pemulihan berdasarkan menggunakan BERT sebagai pengklasifikasi relevansi untuk menyatukan semula calon awal dari pencarian kata kunci. Dalam kerja ini, kita menghadapi cabaran untuk menyesuaikan model ini untuk domain spesifik dengan cara data dan berkomputasi efisien. Biasanya, penyelidik menyesuaikan model yang menggunakan data yang ditabel-spesifik-corpus dari sumber seperti TREC. We first answer the question: How much data of this type do we need?  Mengetahui bahawa latihan yang paling efisien secara komputasi bukanlah latihan, kami mengeksplorasi peringkat tembakan sifar menggunakan model BERT yang telah disesuaikan dengan set data pemulihan MS MARCO yang besar. We arrive at the surprising and novel finding that 'some' labeled in-domain data can be worse than none at all.", 'mt': 'Ir-riċerkaturi pproponew tekniki sempliċi iżda effettivi għall-problem a ta’ rkupru bbażati fuq l-użu tal-BERT bħala klassifikatur ta’ rilevanza biex jerġgħu jinħolqu l-kandidati inizjali mit-tiftix tal-kliem ewlieni. F’dan ix-xogħol, nindirizzaw l-isfida li dawn il-mudelli jiġu rfinuti għal oqsma speċifiċi b’dejta u b’mod komputazzjonalment effiċjenti. Typically, researchers fine-tune models using corpus-specific labelled data from sources such as TREC. L-ewwel nagħtu tweġiba għall-mistoqsija: Kemm għandna bżonn dejta ta’ dan it-tip? Filwaqt li nirrikonoxxu li t-taħriġ l-aktar effiċjenti fil-komputazzjoni mhuwiex taħriġ, nesploraw il-klassifikazzjoni żero-shot bl-użu ta’ mudelli BERT li diġà ġew aġġustati b’sett kbir ta’ dejta dwar l-irkupru tal-passaġġ tal-MS MARCO. Aħna waslu għas-sejba sorprendenti u ġdida li d-dejta "uħud" ittikkettata fid-dominju tista\' tkun agħar minn xejn.', 'ml': 'ബെര്\u200dട്ടിയെ ഉപയോഗിച്ച് പ്രധാനപ്പെടുത്തിയ പ്രശ്നത്തിനായി പരിശോധിക്കുന്നവര്\u200d എളുപ്പമുള്ള സാങ്കേതികവിദ്യകള്\u200d പ്രാര്\u200dത്ഥിക ഈ പ്രവര്\u200dത്തനത്തില്\u200d, ഈ മോഡലുകള്\u200d പ്രത്യേകിച്ച ഡൊമെയിനുകള്\u200dക്ക് വേണ്ടിയുള്ള വിലാസങ്ങള്\u200d നമ്മള്\u200d തിരിച്ചറിയുന്നു. വിവരങ സാധാരണ, TREC പോലുള്ള സ്രോതസ്സുകളില്\u200d നിന്നും കോര്\u200dപ്പുസിന്റെ പ്രത്യേക വിവരങ്ങള്\u200d ഉപയോഗിക്കുന്ന പരിശീലകന്\u200dമാര്\u200d നല നമ്മള്\u200d ആദ്യം ചോദ്യത്തിന് ഉത്തരം പറയുന്നു: ഈ തരം വിവരങ്ങള്\u200d എത്ര വേണം? ഏറ്റവും കൂടുതല്\u200d സാധുവായ പരിശീലം പരിശീലനമല്ലെന്ന് തിരിച്ചറിയുന്നു. ബെര്\u200dട്ടി മോഡലുകള്\u200d ഉപയോഗിച്ച് ബെര്\u200dട്ടി മോഡല്\u200d ഉപയോഗിച്ച് നമ്മള്\u200d പൂര്\u200dണ്ണമായ നമ്മള്\u200d അത്ഭുതപ്പെടുന്നതും നോവലില്\u200d എത്തുന്നു, ഡൊമൈനിലെ ചില ഡേറ്റാകള്\u200d കുറച്ച് മോശമാണെന്ന് കണ്ടെത്തുന്നു.', 'no': 'Forskere har foreslått enkle enn effektive teknikk for henting av problemet basert på å bruka BERT som ein relevant klassifiserer for å gjenoppretta opphavsredige kandidatar frå nøkkelordsøk på nytt. I denne arbeida løyser vi utfordringen av å finne opp desse modelane for spesifikke domene på ein data og dataeffektiv måte. Normalt kan forskere finnstillingsmodeller bruka korpusspesifikke merkelige data frå kilder som TREC. Vi svarer første på spørsmålet: Kor mykje data av denne typen treng vi? Å gjenkjenne at det mest effektivste opplæringa i datamaskin er ingen trening, vi utforskar nullsattrekningar ved hjelp av BERT-modeller som allereie er fint oppretta med den store datasettet for MARCO-passasjon. Vi kommer til den overraskende og romanen som finn at «noen» som er merket i domenedata kan vera verre enn ingen.', 'pl': 'Naukowcy zaproponowali proste, ale skuteczne techniki problemu odzyskiwania oparte na wykorzystaniu BERT jako klasyfikatora relevantności do ponownego rankingu początkowych kandydatów z wyszukiwania słów kluczowych. W niniejszej pracy stawiamy czoła wyzwaniu dostosowania tych modeli do konkretnych dziedzin w sposób wydajny danych i obliczeniowo. Zazwyczaj naukowcy dostosowują modele za pomocą oznaczonych dla korpusu danych pochodzących ze źródeł takich jak TREC. Najpierw odpowiadamy na pytanie: Ile danych tego typu potrzebujemy? Uznając, że najbardziej wydajnym pod kątem obliczeniowym treningiem nie jest trening, badamy ranking zero-shot przy użyciu modeli BERT, które zostały już dostrojone za pomocą dużego zbioru danych pobierania pasaży MS MARCO. Dochodzimy do zaskakującego i nowatorskiego stwierdzenia, że "niektóre" oznaczone w domenie dane mogą być gorsze niż żadne w ogóle.', 'si': "පරීක්ෂකයන්ට ප්\u200dරශ්නයක් තියෙනවා BERT විශ්වාස කරන්න ප්\u200dරශ්නයක් සඳහා ප්\u200dරශ්නයක් සඳහා පරීක්ෂණ ප්\u200dරශ්නයක් ප්\u200dරශ්නයක මේ වැඩේ අපි දත්ත සහ පරීක්ෂණාත්මක විශේෂ ප්\u200dරමාණයක් සඳහා මේ මොඩේල් එක්ක හොඳ සැකසුම් කරනවා. සාමාන්\u200dය විදියට, පරීක්ෂකයෝ ප්\u200dරමාණය සඳහා කොර්පස් විශේෂ පරීක්ෂකයෝ ප්\u200dරමාණය සඳහා ප්\u200dරමාණය සඳහ අපි මුලින්ම ප්\u200dරශ්නයට උත්තර දෙන්නේ: අපිට මේ වර්ගයේ කොච්චර දත්ත ඕනේ? අඳුරගන්නේ වැඩිම ගුණුම්පරික්ෂණ ප්\u200dරශ්නයක් කිසිම ප්\u200dරශ්නයක් නෙවෙයි කියලා, අපි පරීක්ෂණය කරන්නේ BERT මොඩල් භාවිතා සුන්ධ විශ අපි පුදුම විදියට ආවේ සහ පුදුම විදියට හොයාගත්තා කියලා 'සමහර' ලේබල් කරලා තියෙන්න පුළුවන් කිසිම දේවල් ව", 'sr': 'Istraživači su predložili jednostavne, ali efikasne tehnike za problem povlačenja na osnovu korištenja BERT kao klasifikatora relevantnosti da ponovo izvede prve kandidate iz pretraživanja ključnih reči. U ovom poslu, riješimo izazov finalnog prilagođenja ovih modela za specifične domene na podacima i računalno efikasno. Tipièno, istraživači su pravi modeli koji koriste podatke označene za korpus iz izvora poput TREC. Prvo odgovorimo na pitanje: Koliko podataka ovog tipa trebamo? Prepoznajući da najračunalno efikasniji trening nije trening, istražujemo ranking nule snimke koristeći BERT modele koji su već ispravno sređeni sa velikim kompletom podataka o preuzimanju prolaza MS MARCO. Došli smo do iznenađujuće i romana otkrivajući da podaci označene u domenu mogu biti gore od nikoga.', 'ro': 'Cercetătorii au propus tehnici simple, dar eficiente pentru problema de recuperare bazate pe utilizarea BERT ca clasificator de relevanță pentru a reerannsa candidații inițiali din căutarea cuvintelor cheie. În această lucrare, abordăm provocarea de a regla fin aceste modele pentru domenii specifice într-o manieră de date și eficientă din punct de vedere computațional. De obicei, cercetătorii reglează modelele cu ajutorul datelor etichetate specifice corpului din surse precum TREC. Mai întâi răspundem la întrebarea: De câte date de acest tip avem nevoie? Recunoscând că instruirea cea mai eficientă din punct de vedere computațional nu este instruirea, explorăm clasamentul zero-shot folosind modele BERT care au fost deja reglate fin cu setul mare de date de recuperare a pasajelor MS MARCO. Ajungem la surprinzătoarea și noua descoperire că "unele" date în domeniu etichetate pot fi mai rele decât niciuna.', 'so': 'Baaritaanayaal waxay soo jeedeen qalabka fudud ee ay u shaqeeyaan dhibaatada dib u soo qaadashada, taasoo lagu saleynayo isticmaalka BERT sida fasax muhiim ah, si uu u soo celiyo kandidada hore oo raadinta keyword. Markaas waxan, waxaynu tacliinaynaa dhibaatada hagitaanka tusaalahan si fiican looga dhigo macluumaad iyo si saameyn ah. Sida caadiga ah, cilmi-baaritaanayaal waxay u isticmaali jireen macluumaad cayiman oo ku qoran sida TREC. We first answer the question: How much data of this type do we need?  Waxaynu ogaanaynaa in waxbarashada ugu saameysan ee ugu fiican ee aan ahayn waxbarasho, waxaynu baaraynaa jaranjarka zero-shoot ah oo lagu isticmaalayo modellada BERT oo horay u hagaajiyey sameynta baasabka baasabka ee MS MARCO. Waxaynu gaadhnay shabakadda la yaabo iyo warqadda, waxaynu ogaanaynaa in macluumaadka internetka lagu qoray qaarkood ay ka sii xumaad badnaan karto.', 'sv': 'Forskare har föreslagit enkla men effektiva tekniker för återvinningsproblemet baserat på att använda BERT som relevansklassifierare för att omräkna initiala kandidater från sökordssökning. I detta arbete tar vi oss an utmaningen att finjustera dessa modeller för specifika domäner på ett data- och beräkningseffektivt sätt. Vanligtvis finjusterar forskare modeller med hjälp av korpusspecifika märkta data från källor som TREC. Vi svarar först på frågan: Hur mycket data av denna typ behöver vi? Eftersom vi inser att den mest beräkningsmässigt effektiva träningen inte är någon utbildning undersöker vi noll-shot ranking med hjälp av BERT-modeller som redan har finjusterats med den stora MS MARCO passagehämtning datauppsättningen. Vi kommer fram till den överraskande och nya upptäckten att "vissa" märkta domändata kan vara värre än ingen alls.', 'ta': "ஆராய்ச்சியாளர்கள் விசைவார்த்தை தேடுதலில் இருந்து மீண்டும் தேடுதலில் இருந்து தொடர்பு வகுப்பாளராக மீண்டும் துவங்கும் முதல் தே இந்த வேலையில், நாம் இந்த குறிப்பிட்ட களங்களுக்கு நன்மையான மாதிரிகளை பாதுகாக்க வேண்டும் தகவல் மற்றும் கணிப்பாட்டில Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC.  நாம் முதலில் கேள்விக்கு பதில் கூறுகிறோம்: இந்த வகையில் எத்தனை தகவல் தேவைப்படுகிறது? பெரிய எம்எஸ் MARCO கடவுச்செல்லும் தகவல் மீட்டெடுப்பு அமைப்பில் ஏற்கனவே நன்றாக துண்டிக்கப்பட்டுள்ளது என்பதை கண்டறிந்து கொள்கிறோம். நாங்கள் ஆச்சரியமாக வந்துவிட்டோம் மற்றும் புதையத்தில் நாம் கண்டுபிடிக்கும் 'சில' டோமைன் தகவல் குறிப்பிட்ட", 'ur': 'تحقیقات کرنے والوں نے BERT کے استعمال کے ذریعہ بنے ہوئے آسان لیکن اثر فعال تکنیک پیشنهاد کی ہے کہ کلورڈ تلاش سے پہلی کڈینڈیٹوں کو دوبارہ پیدا کریں۔ اس کام میں ہم نے ان موڈلوں کے مطابق مطابق ڈومین کے لئے مطابق تنظیم کرنے کی چال کو ایک ڈیٹا اور کامپیوتر کے مطابق کامل طریقے سے حل لیا ہے۔ قابل طور پر، تحقیقات کرنے والوں نے قارپوس کے مطابق مخصوص لابلیٹ ڈیٹے کے استعمال کر رکھے ہیں جیسے TREC کے ساتھ. ہم پہلی بار سوال کو جواب دیتے ہیں: اس طرح کی کیسی ڈیٹا ہمیں ضرورت ہے؟ پتہ لیتے ہیں کہ سب سے زیادہ کامپیوتروں کے ذریعہ کامپیوتروں کی تعلیم نہیں ہے، ہم نے BERT موڈل کے مطابق صفر-شاٹ رینگ کا تحقیق کیا ہے جو پہلے بڑے MS MARCO پھیر لینے والی ڈیٹ سٹ کے ساتھ بہت سی تغییر کی گئی ہے. ہم تعجب اور روانی پر پہنچتے ہیں کہ "کچھ" ڈومین میں لکھی ہوئی ڈاٹی سے کچھ بھی بدتر نہیں ہو سکتے۔', 'mn': 'Судлаачид БЕРТ-г ашиглан анхны удирдагчдыг хайж эхлэх үгнээс дахин нэвтрүүлэх боломжтой хэлбэрээр ашиглах боломжтой боловч энгийн эффективны технологийг санал болгож байна. Энэ ажлын тулд бид эдгээр загваруудыг тодорхойлох болон тооцоолон үр дүнтэй аргаар тодорхойлох зорилгоор зогсоож байна. ТРЕК шиг эх үүсвэрээс корпус тодорхойлогдсон өгөгдлийг ашиглаж судлаачид загварыг тодорхойлдог. Бид эхлээд асуултыг хариулна: Энэ төрлийн хэр хэмжээний мэдээлэл хэрэгтэй вэ? Хамгийн үр дүнтэй тооцооллын дасгал хөдөлгөөн бол сургалт биш, бид BERT загваруудыг ашиглан тэг загварыг судалж, хамгийн том MS MARCO-ын дасгал авах өгөгдлийн сантай сайжруулсан. Бид гайхамшигтай, зохиол руу ирсэн юм. Зарим хэсэг нь тодорхойлолтой мэдээллээс ч илүү муу байж чадахгүй гэдгийг олж мэдсэн.', 'uz': "@ info: whatsthis Bu ishda, biz bu modellarni yaxshi o'xshash o'ylab, ma'lumot va kompyuterni juda effektiv usulda qaramaymiz. Oddiy, TREC kabi manbalar kabi notoʻgʻri bajarilgan maʼlumot yordamida o'qituvchilar Biz birinchi savol javob beramiz: Bu turdagi qancha maʼlumot kerak? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset.  Biz hayajonroq va novel bilan kelamiz, bu domen maʼlumotlari hech qachon ham yomon deb tushunishimiz mumkin.", 'vi': 'Các nhà nghiên cứu đã đề xuất một kỹ thuật đơn giản nhưng hiệu quả cho vấn đề tìm kiếm dựa trên sử dụng BERT làm người phân loại liên quan để lần lại những ứng viên đầu tiên từ một từ khoá tìm kiếm. Trong công việc này, chúng ta phải giải quyết thử thách sửa chữa những mô hình này cho miền cụ thể bằng một cách dữ liệu và hiệu quả. Theo nguyên tắc, các nhà nghiên cứu mẫu tinh chỉnh sử dụng dữ liệu có dạng hợp lý từ nguồn như TREC. Câu hỏi đầu tiên là chúng ta cần bao nhiêu dữ liệu loại này? Nhận ra rằng huấn luyện hiệu quả kinh nghiệm nhất không phải là huấn luyện, chúng tôi khám phá cấp bắn bằng hạng không bằng cách sử dụng các mô hình BERT đã được hoàn chỉnh chính xác với bộ dữ liệu thu hồi hành trình lớn của MS MACO. Chúng tôi nhận ra rằng "một số" được dán nhãn trong miền có thể tồi tệ hơn không một chút nào.', 'bg': 'Изследователите са предложили прости, но ефективни техники за проблема с извличането въз основа на използването на BERT като класификатор за релевантност за пренареждане на първоначалните кандидати от търсенето на ключови думи. В тази работа се справяме с предизвикателството за фина настройка на тези модели за конкретни области по данни и изчислително ефективен начин. Обикновено изследователите фино настройват модели, използвайки специфични за корпуса данни от източници като TREC. Първо отговаряме на въпроса: Колко данни от този тип ни трябват? Осъзнавайки, че най-ефективното изчислително обучение не е обучение, ние изследваме нулевото класиране, използвайки модели, които вече са фино настроени с големия набор от данни за извличане на пасажи. Стигаме до изненадващото и ново откритие, че "някои" маркирани в домейна данни могат да бъдат по-лоши от никакви.', 'hr': 'Istraživači su predložili jednostavne, ali učinkovite tehnike za problem povlačenja na temelju korištenja BERT kao klasifikatora važnosti za ponovno izvlačenje početnih kandidata iz pretraživanja ključnih riječi. U ovom poslu rješavamo izazov finaliziranja ovih modela za specifične domene na podacima i računalno učinkoviti način. Obično, istraživači dobro navode modele koristeći podatke označene za korpus iz izvora poput TREC-a. Prvo odgovorimo na pitanje: Koliko podataka ovog tipa trebamo? Prepoznajući da najračunalno učinkovitija obuka nije obuka, istražujemo ranking nula pucnjava koristeći BERT modele koji su već ispravno sređeni sa velikim kompletom podataka o preuzimanju prolaza MS MARCO-a. Došli smo do iznenađujuće i romana otkrivajući da podaci označene u domenu mogu biti gore od nikoga.', 'nl': "Onderzoekers hebben eenvoudige maar effectieve technieken voorgesteld voor het retrieval probleem gebaseerd op het gebruik van BERT als relevantieclassificator om initiële kandidaten van zoekwoorden opnieuw te rangschikken. In dit werk gaan we de uitdaging aan om deze modellen op een data- en rekenkundige efficiënte manier af te stemmen voor specifieke domeinen. Meestal verfijnen onderzoekers modellen met corpusspecifieke gelabelde gegevens uit bronnen zoals TREC. We beantwoorden eerst de vraag: Hoeveel data van dit type hebben we nodig? Erkend dat de meest computerefficiënte training geen training is, onderzoeken we zero-shot ranking met behulp van BERT modellen die al zijn verfijnd met de grote MS MARCO passage retrieval dataset. We komen tot de verrassende en nieuwe bevinding dat 'sommige' gelabelde in-domain data slechter kunnen zijn dan helemaal geen.", 'da': "Forskere har foreslået enkle, men effektive teknikker til hentningsproblemet baseret på at bruge BERT som relevansklassificator til at omdirigere de første kandidater fra søgeordssøgning. I dette arbejde tackler vi udfordringen med at finjustere disse modeller for specifikke domæner på en data- og beregningseffektiv måde. Typisk finjusterer forskere modeller ved hjælp af korpusspecifikke mærkede data fra kilder som TREC. Vi besvarer først spørgsmålet: Hvor mange data af denne type har vi brug for? I erkendelse af, at den mest beregningsmæssigt effektive træning ikke er træning, undersøger vi nul-shot ranking ved hjælp af BERT-modeller, der allerede er finjusteret med det store MS MARCO passage hentning datasæt. Vi når frem til den overraskende og nye konstatering, at 'nogle' mærkede in-domæne data kan være værre end slet ingen.", 'de': 'Forscher haben einfache, aber effektive Techniken für das Retrieval-Problem vorgeschlagen, die auf der Verwendung von BERT als Relevanz-Klassifikator basieren, um erste Kandidaten aus der Keyword-Suche neu zu rangieren. In dieser Arbeit stellen wir uns der Herausforderung, diese Modelle für bestimmte Domänen dateneffizient und rechnerisch zu verfeinern. Typischerweise verfeinern Forscher Modelle mithilfe korpusspezifisch beschrifteter Daten aus Quellen wie TREC. Zunächst beantworten wir die Frage: Wie viele Daten dieser Art benötigen wir? Da wir uns bewusst sind, dass das computereffizienteste Training kein Training ist, untersuchen wir Zero-Shot-Ranking mit BERT-Modellen, die bereits mit dem großen MS MARCO Passage Retrieval Dataset fein abgestimmt wurden. Wir kommen zu dem überraschenden und neuartigen Ergebnis, dass "einige" in-domain-Daten schlechter sein können als überhaupt keine.', 'id': "Peneliti telah mengusulkan teknik sederhana namun efektif untuk masalah pemulihan berdasarkan menggunakan BERT sebagai klasifikasi relevansi untuk mengorbankan ulang kandidat awal dari pencarian kata kunci. Dalam pekerjaan ini, kita mengatasi tantangan untuk memperbaiki model-model ini untuk domain spesifik dengan cara data dan komputasi efisien. Biasanya, para peneliti memperbaiki model menggunakan data spesifik corpus yang ditabel dari sumber seperti TREC. We first answer the question: How much data of this type do we need?  Mengetahui bahwa pelatihan yang paling efisien secara komputasional bukan pelatihan, kami mengeksplorasi rangkaian 0-shot menggunakan model BERT yang telah disesuaikan dengan set data pengumpulan MS MARCO besar. Kami tiba di penemuan yang mengejutkan dan novel bahwa 'beberapa' yang ditabel data dalam domain dapat lebih buruk dari tidak ada sama sekali.", 'ko': "연구진은 BERT를 관련 분류기로 사용해 키워드 검색에서 초기 후보어를 다시 배열하는 것을 바탕으로 간단하고 효과적인 검색 기술을 제시했다.이 작업에서 우리는 데이터와 계산 효율이 높은 방식으로 특정 분야에서 이 모델들을 미세하게 조정하는 도전을 해결했다.일반적으로 연구원들은 TREC 등 출처에서 온 자료 라이브러리의 특정 표기 데이터 마이크로스피커 모델을 사용한다.우리는 먼저 이 문제에 대답했다. 우리는 이런 데이터가 얼마나 필요합니까?계산 효율이 가장 높은 훈련이 무훈련이라는 것을 깨닫고 우리는 버트 모델을 이용하여 제로 렌즈 정렬을 탐색했다. 이 모델은 이미 대형 MS MARCO 채널 검색 데이터 집합에서 미세하게 조정되었다.우리는 지역 데이터에 '일부' 를 표시하는 것이 없는 것보다 더 나쁠 수도 있다는 놀라운 발견을 얻었다.", 'fa': 'تحقیقات\u200cکنندگان تکنیک\u200cهای ساده و تاثیر برای مشکل بازیابی بر اساس استفاده از BERT به عنوان کلی\u200cشناسی مربوط به بازیابی کاندیده\u200cهای اولیه از جستجوی کلید پیشنهاد کردند. در این کار، ما با چالش\u200cهایی که این مدل\u200cها را به طریق داده\u200cها و کاملاً موثرت می\u200cدهند، حل می\u200cکنیم. معمولاً تحقیقات\u200cکنندگان از استفاده از داده\u200cهای مشخص\u200cشده\u200cی کورپوس از منابع\u200cهای مانند TREC، مدل\u200cهای خوب تنظیم می\u200cکنند. اولین بار به سوال جواب می دهیم: چقدر اطلاعات این نوع نیاز داریم؟ تشخیص دادن که تمرین\u200cهای کامپیوتری\u200cترین کامپیوتری در محاسبه\u200cها هیچ تمرین نیست، ما با استفاده از مدل\u200cهای BERT صفر را تحقیق می\u200cکنیم که قبلاً با مجموعه\u200cی داده\u200cهای بازیابی بزرگ MS MARCO را خوب کنترل کرده\u200cاند. ما به داستان تعجب و رمان رسیدیم و پیدا کردیم که بعضی از اطلاعات داخل داخلی نامیده شده می توانند بدتر از هیچ کس باشند.', 'sw': 'Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search.  In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner.  Kwa kawaida, watafiti watafiti mifano nzuri kwa kutumia taarifa maalum za makampuni kutoka vyanzo kama vile TREC. Tunajibu swali la kwanza: Takwimu ngapi ya aina hii tunahitaji? Kutambua kwamba mafunzo yenye ufanisi zaidi ya kompyuta hayana mafunzo, tunachunguza rangi yenye risasi sifuri kwa kutumia mifano ya BERT ambayo tayari yamekuwa vizuri na seti kubwa ya upatikanaji wa data za MS MARCO. Tunaja kwa kushangaza na riwaya inayogundua kuwa taarifa za ndani zinaweza kuwa mbaya zaidi ya chochote.', 'tr': "Araştyranlar BERT'i ulanmak üçin basit we etkinlik teknikleri tekniýetlendirdiler. Kişilik sözlerinden başlangıç kandidýalary ýene-täzeden tapmak üçin basit we çykyş teknikleri tekniýetlendirdiler. Biz bu işde bu nusgalary belli alanlar üçin has we hesaplamada etkinlik şeklinde täsirleşmeler kynçylygyny çözeriz. Adatça, araştırmacılar TREC ýaly köpüs hasaplanýan hasaplanyň şeklinde nusgalary ullanýarlar. Biz ilkinji gezek soragyna jogap berdik: Bu tip üçin näçe maglumat gerek? Iň computasiýaly etkinlik taýýarlanmagy üçin ýok taýýarlanmagy däldir, biz BERT modellerini ulanyp 0-aty derejesini öňünden beýleki MS MARCO passage alyp veri sahypalary bilen gözleýäris Biz şaşırtýan we romana geldik şol domaýda 'käbirleri' diýilip atlanan ýagdaýlaryň hiçbirinden erbet bolup biler.", 'af': "Resekers het eenvoudige nog effektiewe teknike voorgestel vir die ontvang probleme gebaseer op die gebruik van BERT as 'n relevante klassifiseerder om eerste kandidate van sleutelwoord soek te herank. In hierdie werk is ons die uitdrukking van die fin-tuning van hierdie modele vir spesifieke domeine op 'n data en rekenasielik effektief manier. Typies, resekers fin-tune modele gebruik corpus-spesifieke etiketeerde data van bronne soos TREC. Ons antwoord eerste die vraag: Hoeveel data van hierdie tipe nodig ons? Herken dat die mees rekenaar effektief onderwerp is geen onderwerp nie, ons ondersoek nul-skoot ranking deur BERT modele wat reeds is fin-tuned met die groot MS MARCO passage retrieval dataset. Ons kom by die verwondering en roman om te vind dat 'sommige' in-domein-data beteken kan wees verder as niks nie.", 'sq': "Kërkuesit kanë propozuar teknika të thjeshta megjithatë efektive për problemin e marrjes në bazë të përdorimit të BERT si një klasifikues relevance për të ribashkuar kandidatët fillestarë nga kërkimi i fjalës kyçe. Në këtë punë, ne trajtojmë sfidën e rregullimit të këtyre modeleve për fusha të caktuara në një mënyrë të dhëna dhe të efektshme në llogari. Zakonisht, kërkuesit rregullojnë modelet duke përdorur të dhëna të etiketuara specifike për korpus nga burime të tilla si TREC. Ne përgjigjemi së pari në pyetje: Sa të dhëna të këtij lloji na duhen? Duke njohur se trajnimi më i efektshëm në llogari nuk është trajnimi, ne eksplorojmë renditjen zero-shot duke përdorur modelet BERT që tashmë janë rregulluar me të dhënat e mëdha të marrjes së pasazhit MS MARCO. Ne arrijmë në gjetjen e çuditshme dhe të re se 'disa' të etiketuara në domeni mund të jenë më të këqija se asnjë.", 'bn': 'গবেষকেরা কীবোর্ড অনুসন্ধান থেকে প্রার্থীদের পুনরায় প্রার্থীদের পুনরায় প্রার্থী হিসেবে ভিত্তিক বিERT ব্যবহার করে পুনঃসাধারণ ক এই কাজের মাধ্যমে আমরা এই মডেলের সুন্দর প্রতিষ্ঠানের চ্যালেঞ্জের মুখোমুখি হয়েছি ডাটা এবং গণতান্ত্রিকভাবে কার্যকর। স্বাভাবিকভাবে, গবেষকদের কোর্পাস-নির্দিষ্ট লেবেলেটের তথ্য ব্যবহার করে সুনির্দিষ্ট মডেল, যেমন TREC-এর মত। প্রথমে আমরা প্রশ্নের উত্তর দিচ্ছি: এই ধরনের তথ্য কত দরকার? স্বীকার করে যে সবচেয়ে গণতান্ত্রিক কার্যকর প্রশিক্ষণ কোন প্রশিক্ষণ নেই, আমরা বেরেটি মডেল ব্যবহার করে শুট গুলি খুঁজে বের করি যা ইতোমধ্যে বিশাল এমএস মার্কো  আমরা বিস্ময়কর এবং উপন্যাসে এসেছি যে ডোমেইনের কিছু তথ্যের চেয়ে খারাপ হতে পারে।', 'am': 'ምርምርተኞች BERT በመጠቀም የፊደል ቃላት ምናዳዎችን ከቁልፍ ቃላት ለማስቀመጥ የተጠቃሚ የግንኙነት ክፍል እንዲያስተካክሉ ቀላል ነገር ግን የሚያስፈልገውን መግለጫ አግኝተዋል፡፡ በዚህ ስራ፣ እነዚህን ምሳሌዎች ለቁጥር ዲሞሎችን በመጠቀም እና በቁጥጥር በጥያቄ እናደርጋለን፡፡ በተለመደው፣ እንደTREC የሚመስል ክሮፕስ-የተለየ የጽሑፍ ዳታዎችን በመጠቀም አስተማሪዎች መልካም-tune models. የመጀመሪያ ጥያቄን እንመልሳለን። ምን ያህል የዚህ ዓይነት ዳታ እናስፈልጋለን? በቁጥጥር የበለጠ ትምህርት ማህበረሰብ ማኅበረሰብ እንደሌለ እንደምናውቅ፣ የBERT ሞዴላዎችን በመጠቀም ብዙው የኢሜএস ማርኮ የፋሲል መግቢያ ዳታ ማግኘት የተጠቃለሉትን የzero-shot ዘርፍ እንፈልጋለን፡፡ የአንዳንዶችን ዳታ ከሁሉ በላይ የሚከፋ እንደሆነ እናግኘዋለን፡፡', 'hy': 'Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search.  In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner.  Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC.  We first answer the question: How much data of this type do we need?  Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset.  Մենք հասնում ենք զարմանալի և նորարար եզրակացության, որ "որոշ" տիեզերքում նշված տվյալները կարող են ավելի վատ լինել, քան ոչ մեկը:', 'az': 'Araştırmacılar BERT vasitəsilə istifadə etmək üçün başlanğıç sözlərin araştırmasından başlanğıç kandidātları yenidən təkrar etmək üçün asanlıqla istifadə edən istifadə edən və etkili teknikləri təklif etdilər. Bu işdə, bu modelləri məlumatlardan və hesaplayaraq faydalı bir şekilde düzəltmək üçün gözəl düzəltmək üçün çəkilirik. Tipik olaraq, araştırmacılar TREC kimi qaynaqlardan corpus-specific etiketli məlumatlarını istifadə edirlər. Biz ilk dəfə suala cavab veririk: Bu növünün nə qədər məlumatlarına ihtiyacımız var? Ən hesaplayaraq təhsil edilən təhsil təhsil is ə təhsil edilməz, BERT modelləri ilə sıfır-shot səviyyəsini keşif edirik ki, çox böyük MS MARCO keçirmək veri qutusu ilə təhsil edilmişdir. Biz təəccüblü və romanlara gəldik ki, "bəzi" məlumatların heç kəsdən daha pis olar.', 'bs': 'Istraživači su predložili jednostavne, ali efikasne tehnike za problem povlačenja na temelju korištenja BERT kao klasifikatora relevantnosti za ponovno izvlačenje prvih kandidata iz pretraživanja ključnih riječi. U ovom poslu rješavamo izazov finalnog prilagođenja ovih modela za specifične domene na podacima i računalno efikasno. Tipično, istraživači dobro tuniraju modele koristeći podatke označene za korpus iz izvora poput TREC. Prvo odgovorimo na pitanje: Koliko podataka ovog tipa trebamo? Prepoznajući da najračunalno učinkovitija obuka nije obuka, istražujemo ranking nule snimke koristeći BERT modele koji su već ispravno sređeni sa velikim kompletom prikupljanja podataka o prolazu MS MARCO-a. Došli smo do iznenađujućeg i romana otkrivajući da podaci označene u domenu mogu biti gore od nikoga.', 'et': 'Teadlased on välja pakkunud lihtsaid, kuid tõhusaid meetodeid, mis põhinevad BERTi kasutamisel asjakohasuse klassifitseerijana, et esmaseid kandidaate märksõna otsingust ümber panustada. Käesolevas töös käsitleme väljakutset, mille kohaselt tuleb neid mudeleid konkreetsetele valdkondadele täpsustada andmete ja arvutuslikult tõhusalt. Tavaliselt täpsustavad teadlased mudeleid, kasutades korpusepõhiseid märgistatud andmeid sellistest allikatest nagu TREC. Kõigepealt vastame küsimusele: Kui palju sellist tüüpi andmeid me vajame? Tunnistades, et arvutuslikult tõhusaim koolitus ei ole koolitus, uurime null-shot järjestust BERT mudelite abil, mida on juba täpselt häälestatud suure MS MARCO läbipääsu tagasivõtmise andmekogumiga. Me jõuame üllatavale ja uudsele järeldusele, et "mõned" märgistatud domeenisisesed andmed võivad olla hullemad kui üldse mitte ükski.', 'fi': 'Tutkijat ovat ehdottaneet yksinkertaisia mutta tehokkaita tekniikoita hakuongelmaan, jotka perustuvat BERT:n käyttämiseen relevanssiluokittajana avainsanahaun alkuehdokkaiden uudelleensijoittamiseen. Tässä työssä vastaamme haasteeseen näiden mallien hienosäätämisestä tietyille toimialoille data- ja laskennallisesti tehokkaalla tavalla. Tyypillisesti tutkijat hienosäätävät malleja käyttämällä korpuskohtaisesti merkittyä tietoa TREC:n kaltaisista lähteistä. Vastaamme ensin kysymykseen: Kuinka paljon tällaista tietoa tarvitsemme? Kun tiedostamme, että laskennallisesti tehokkain koulutus ei ole harjoittelu, tutkimme nollalaukauksia käyttämällä BERT-malleja, jotka on jo hiottu suurella MS MARCO -tiedonkeruuaineistolla. Saavutamme yllättävään ja uudenlaiseen havaintoon, että "jotkut" verkkotunnuksilla varustetut tiedot voivat olla huonompia kuin ei lainkaan.', 'cs': 'Výzkumníci navrhli jednoduché, ale účinné techniky pro problém vyhledávání, založené na použití BERT jako klasifikátoru relevance pro přerozdělení počátečních kandidátů z hledání klíčových slov. V této práci řešíme výzvu jemného ladění těchto modelů pro konkrétní domény datově a výpočetně efektivním způsobem. Výzkumníci obvykle jemně ladí modely pomocí korpusově specifických označených dat ze zdrojů, jako je TREC. Nejprve odpovíme na otázku: Kolik dat tohoto typu potřebujeme? Uvědomujeme si, že výpočetně nejúčinnějším tréninkem není žádný trénink, a proto zkoumáme nulové hodnocení pomocí modelů BERT, které již byly vyladěny s velkým datovým souborem MS MARCO pro vyhledávání pasáží. Dospěli jsme k překvapivému a novému zjištění, že některá data označená v doméně mohou být horší než žádná.', 'ca': 'Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search.  In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner.  Normalment, els investigadors ajusten models utilitzant dades etiquetades específices per a corpus de fonts com TREC. Primer responem a la pregunta: Quantes dades d\'aquest tipus necessitem? Reconocint que l\'entrenament més eficient en computació no és cap entrenament, explorem la classificació zero amb models BERT que ja han estat ajustats amb el gran conjunt de dades de recuperació de passatges MS MARCO. Llavors arribem a la descoberta sorprenent i nova que "algunes" etiquetades en domini poden ser pitjors que cap.', 'jv': "Wuhan Nang gawar iki, kita sakjane perbudhakan kanggo nggawe model iki dadi kanggo ngilangno dadi lan sakjane sampeyan sing luwih apik. text-box-mode Awak dhéwé ngomong sapa kerjane boten: Pilihan data sing butuh dikenakno iki? Rasané awak dhéwé kuwi kalem sistem sing gak perusahaan kang karo netarang, awak dhéwé ngecat perusahaan linep nggambar model BERT sing wis nguasai duruh akeh operasi kaya pasang dumadhi sing gak nggawe dataset malah MARO kuwi. Awak dhéwé ngeweh luwih lan nganggo kuwi 'some' sing paling-alam data in-domain iso dianggawe luwih apik sing gak dhéwé.", 'sk': 'Raziskovalci so predlagali preproste, vendar učinkovite tehnike za problem iskanja, ki temeljijo na uporabi BERT kot klasifikatorja relevantnosti za ponovno razvrščanje začetnih kandidatov iz iskanja ključnih besed. V tem delu se spopadamo z izzivom natančnega nastavitve teh modelov za določena področja na podatkovno in računalniško učinkovit način. Običajno raziskovalci natančno nastavijo modele z uporabo podatkov, označenih za korpus, iz virov, kot je TREC. Najprej odgovorimo na vprašanje: Koliko podatkov te vrste potrebujemo? Ker se zavedamo, da računalniško najbolj učinkovito usposabljanje ni usposabljanje, raziskujemo ničelno razvrščanje z uporabo BERT modelov, ki so že bili natančno nastavljeni z velikim naborom podatkov o iskanju prehodov MS MARCO. Pridemo do presenetljive in nove ugotovitve, da so "nekateri" označeni podatki v domeni lahko slabši kot sploh nič.', 'he': 'Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search.  In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner.  בדרך כלל, חוקרים מתאימים מודלים בשימוש נתונים מסויימים לקורפוס ממקורים כמו TREC. קודם לענות על השאלה: כמה נתונים מהסוג הזה אנחנו צריכים? בהכרה שהאימון הכי יעיל בחשבון הוא לא אימון, אנו חוקרים רמה אפס-יריות באמצעות דוגמנים BERT שכבר התאימו עם קבוצת הנתונים הגדולה של MS MARCO. אנחנו מגיעים למציאה המפתיעה והרומנית כי "חלק" מסומנים בתחום יכול להיות גרוע מכל אחד בכלל.', 'ha': "Wata kebuɗe sun goyyade masu sauƙi ko kuma masu amfani da matoyi wa masu amfani da BERT kamar wani mai muhimmi wa ka sake fara kando na farko daga search of keyword. Daga wannan aikin, Munã karɓi musamman masu tunkuɗe waɗannan misãlai masu hushi a cikin wasu mutane da kuma masu kamfata masu fasahan. Kima da ɗabi'a, watani masu yin motsi masu fin-tune, sunã amfani da data na rubutu-ƙayyade daga sourcen kamar TREC. Kayya da na karɓa wa tambayi: Nama zaɓa masu amfani da wannan? Ina gane cewa mafarin da ke da lissafa mafi cikakken aikin lissafa ba ta zama na aikin aiki ba, kuma muna buɗe shirin sifo-shot da za'a yi amfani da misãlai BERT wanda aka riga aka samu da babban matsayin motsarori na MARCO. Muna sami da mãmãki da yanzu, kuma munã gane cewa wasu 'data' da aka rubuta cikin-Domen za'a zama mafi sharri daga kõme ba.", 'bo': "དབྱེ་ཞིབ་ཚོས་ཀྱིས་BERT སྤྱོད་མཁན་གྱི་དཀའ་ངལ་ཚོད་ལས་སླར་བའི་ཐབས་ལམ་ལ་སླ་གཏོང་བ་ཞིག་ཡོད། འོན་ཀྱང་། ང་ཚོས་གནས་སྟངས་དང་རྩིས་འཁོར་གྱི་སྤྱོད་ཆ་ཁག་ཅིག་གི་མིག་ཆས་འདི་ཚོར་སྒྲིག Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. ང་ཚོས་འདྲི་ཚིག་དང་པོར་ལབ་ཐུབ་པ། རིགས་འདིའི་རིགས་ཀྱི་ཆ་འཕྲིན་བཏོན་དགོས་སམ། Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that 'some' labeled in-domain data can be worse than none at all."}
{'en': 'Load What You Need : Smaller Versions of Mutililingual BERT', 'ar': 'قم بتحميل ما تحتاجه: إصدارات أصغر من BERT متعدد اللغات', 'pt': 'Carregue o que você precisa: versões menores do BERT mutililíngue', 'fr': 'Chargez ce dont vous avez besoin\xa0: versions plus petites du BERT mutililingue', 'es': 'Cargue lo que necesita: versiones más pequeñas de BERT multilingüe', 'hi': 'लोड क्या आप की जरूरत है: Mutililingual BERT के छोटे संस्करणों', 'ja': '必要なものを読み込む： Mutililingual BERTの小さなバージョン', 'zh': '加载所需,更小版本多言 BERT', 'ru': 'Загрузите то, что вам нужно: Меньшие версии Mutililingual BERT', 'ga': 'Luchtaigh An Rud atá uait: Leaganacha Níos Lú de BERT Ilteangach', 'ka': 'ჩატვირთვა რა თქვენ უნდა: უფრო პატარა ვირციები მუშაობელი ბერტის', 'el': 'Φορτώστε αυτό που χρειάζεστε: Μικρότερες εκδόσεις του αμοιβαίου γλωσσικού BERT', 'hu': 'Töltse be, amire szüksége van: A többnyelvű BERT kisebb verziói', 'lt': 'Įkelti, ko reikia: mažesnės Mutililingual BERT versijos', 'it': 'Carica quello che ti serve: versioni più piccole di BERT Mutilingue', 'mk': 'Вчитај што ви треба: Помали верзии на Mutililingual BERT', 'kk': 'Қалаған нұсқаларды жүктеу', 'ml': 'നിങ്ങള്\u200dക്ക് ആവശ്യമുള്ളത് ലോഡ് ചെയ്യുക: മുറിലില്\u200dലിങ്കുള്ള വേര്\u200dഷന്\u200d ബെര്\u200dട്ടില്\u200d', 'ms': 'Muat Apa yang Anda perlukan: Versi Kecil BERT Mutililingual', 'mt': 'Tagħbija X’għandek bżonn: Verżjonijiet iżgħar ta’ BERT Mutililingwi', 'mn': 'Танд хэрэгтэй зүйлсийг ачааллаа: Бага хэлний BERT', 'no': 'Last inn kva du treng: mindre versjonar av Mutililingual BERT', 'pl': 'Ładuj to, czego potrzebujesz: Mniejsze wersje multilingual BERT', 'ro': 'Încărcați ceea ce aveți nevoie: Versiuni mai mici de BERT Mutilingv', 'sr': 'Napunite što trebate: manje verzije međujezičkog BERT-a', 'si': 'ඔබට අවශ්\u200dය දේවල් ලෝඩ් කරන්න', 'so': 'Load What You Need: Smaller Versions of Mutililingual BERT', 'sv': 'Ladda vad du behöver: Mindre versioner av flerspråkig BERT', 'ta': 'நீங்கள் தேவைப்படுவதை ஏற்று', 'ur': 'آپ کی نیاز ہے لڈ کریں: کم نسخے متحد زبان BERT کی', 'uz': '@ info: whatsthis', 'vi': 'Nạp những gì bạn cần: Phiên bản nhỏ của hỗn độn Mutant', 'da': 'Indlæs hvad du har brug for: Mindre versioner af flersproget BERT', 'nl': 'Laden wat u nodig hebt: kleinere versies van mutililingual BERT', 'bg': 'Зареждане на това, от което се нуждаете: по-малки версии на мултилезичен BERT', 'id': 'Muat apa yang Anda butuhkan: Versi Kecil dari BERT Mutililingual', 'de': 'Laden Sie, was Sie brauchen: Kleinere Versionen von Mutililingual BERT', 'hr': 'Napunite što trebate: manje verzije međujezičkog BERT-a', 'sw': 'Paa kile unachohitaji: Verithi ndogo ya lugha ya Kitengo BERT', 'tr': "Näme gerek ýükle: Mutililingy BERT'iň kiçi", 'fa': 'بار آنچه لازم دارید: نسخه\u200cهای کوچکتر از BERT متحد زبان', 'af': 'Laai Wat jy nodig: Klein Versione van Mutililingual BERT', 'sq': 'Ngarko atë që të duhet: Versione më të vogla të BERT mutililingual', 'am': 'Load What You Need: Smaller Versions of Mutililingual BERT', 'hy': 'Բեռնել այն, ինչի կարիք ունի. ավելի փոքր տարբերակներ', 'az': "İhtiyacınızı yükləyin: Bircə dilli BERT'nin küçük versiyonları", 'bn': 'Load What You Need: Smaller Versions of Mutililingual BERT', 'ko': '필요한: 다국어 이터레이션 불러오기', 'ca': 'Cargar el que necessites: Versions més petites del BERT mutilingüe', 'cs': 'Načíst, co potřebujete: Menší verze mutililingual BERT', 'bs': 'Napunite što trebate: manje verzije međujezičkog BERT-a', 'et': 'Laadi mida vajad: väiksemad versioonid Mutililingual BERT', 'fi': 'Lataa mitä tarvitset: Pienemmät versiot Mutililingual BERT', 'jv': 'undo-type', 'he': 'טען מה שאתה צריך: גרסאות קטנות יותר של BERT Mutililingual', 'ha': '@ action', 'sk': 'Naložite kar potrebujete: manjše različice Mutililingual BERT', 'bo': 'ཁྱོད་ཀྱིས་དགོས་པ་ཅི་མངོན་གསལ་འཇུག་བྱེད། སྒྲ་བརྙན་རིས་ཀྱི་ཐོན་རིམ་ཆུང་ཀྱི་BERT'}
{'en': 'Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45 % of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7 % to 6 % drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.', 'ar': 'تحقق النماذج المستندة إلى المحولات المدربة مسبقًا نتائج متطورة على مجموعة متنوعة من مجموعات بيانات معالجة اللغة الطبيعية. ومع ذلك ، فإن حجم هذه النماذج غالبًا ما يكون عيبًا لنشرها في تطبيقات الإنتاج الحقيقية. في حالة النماذج متعددة اللغات ، توجد معظم المعلمات في طبقة الزخارف. لذلك ، يجب أن يكون لتقليل حجم المفردات تأثير مهم على العدد الإجمالي للمعلمات. في هذه الورقة ، نقترح استخراج نماذج أصغر تتعامل مع عدد أقل من اللغات وفقًا للمجموعة المستهدفة. نقدم تقييمًا لإصدارات أصغر من BERT متعدد اللغات على مجموعة بيانات XNLI ، لكننا نعتقد أنه يمكن تطبيق هذه الطريقة على محولات أخرى متعددة اللغات. تؤكد النتائج التي تم الحصول عليها أنه يمكننا إنشاء نماذج أصغر تحافظ على نتائج قابلة للمقارنة ، مع تقليل ما يصل إلى 45٪ من العدد الإجمالي للمعلمات. قارنا نماذجنا مع DistilmBERT (نسخة مقطرة من BERT متعدد اللغات) وأظهرنا أنه على عكس تقليل اللغة ، تسبب التقطير في انخفاض بنسبة 1.7٪ إلى 6٪ في الدقة الكلية لمجموعة بيانات XNLI. النماذج والكود المقدمان متاحان للجمهور.', 'es': 'Los modelos preentrenados basados en Transformer están logrando resultados de vanguardia en una variedad de conjuntos de datos de procesamiento del lenguaje natural. Sin embargo, el tamaño de estos modelos suele ser un inconveniente para su implementación en aplicaciones de producción reales. En el caso de los modelos multilingües, la mayoría de los parámetros se encuentran en la capa de incrustaciones. Por lo tanto, reducir el tamaño del vocabulario debería tener un impacto importante en el número total de parámetros. En este documento, proponemos extraer modelos más pequeños que manejen menos idiomas de acuerdo con los corpus objetivo. Presentamos una evaluación de versiones más pequeñas de BERT multilingüe en el conjunto de datos XNLI, pero creemos que este método puede aplicarse a otros transformadores multilingües. Los resultados obtenidos confirman que podemos generar modelos más pequeños que mantienen resultados comparables, al tiempo que reducimos hasta un 45% del número total de parámetros. Comparamos nuestros modelos con DistilMbert (una versión destilada de BERT multilingüe) y demostramos que, a diferencia de la reducción del lenguaje, la destilación inducía una caída del 1,7% al 6% en la precisión general del conjunto de datos XNLI. Los modelos y el código presentados están a disposición del público.', 'fr': "Les modèles pré-entraînés basés sur Transformer permettent d'obtenir des résultats de pointe sur une variété d'ensembles de données de traitement du langage naturel. Cependant, la taille de ces modèles est souvent un inconvénient pour leur déploiement dans des applications de production réelles. Dans le cas de modèles multilingues, la plupart des paramètres sont situés dans la couche d'intégration. Par conséquent, la réduction de la taille du vocabulaire devrait avoir un impact important sur le nombre total de paramètres. Dans cet article, nous proposons d'extraire des modèles plus petits qui traitent moins de langues en fonction des corpus ciblés. Nous présentons une évaluation de versions plus petites du BERT multilingue sur l'ensemble de données XNLI, mais nous pensons que cette méthode peut être appliquée à d'autres transformateurs multilingues. Les résultats obtenus confirment que nous pouvons générer des modèles plus petits qui conservent des résultats comparables, tout en réduisant jusqu'à 45\xa0% du nombre total de paramètres. Nous avons comparé nos modèles avec DistilMbert (une version distillée du BERT multilingue) et avons montré que, contrairement à la réduction de la langue, la distillation induisait une baisse de 1,7\xa0% à 6\xa0% de la précision globale de l'ensemble de données XNLI. Les modèles et le code présentés sont accessibles au public.", 'pt': 'Modelos pré-treinados baseados em Transformer estão alcançando resultados de última geração em uma variedade de conjuntos de dados de Processamento de Linguagem Natural. No entanto, o tamanho desses modelos geralmente é uma desvantagem para sua implantação em aplicativos de produção reais. No caso de modelos multilíngues, a maioria dos parâmetros está localizada na camada de embeddings. Portanto, reduzir o tamanho do vocabulário deve ter um impacto importante no número total de parâmetros. Neste artigo, propomos extrair modelos menores que lidam com menor número de linguagens de acordo com os corpora alvo. Apresentamos uma avaliação de versões menores do BERT multilíngue no conjunto de dados XNLI, mas acreditamos que esse método possa ser aplicado a outros transformadores multilíngues. Os resultados obtidos confirmam que podemos gerar modelos menores que mantêm resultados comparáveis, reduzindo em até 45% o número total de parâmetros. Comparamos nossos modelos com o DistilmBERT (uma versão destilada do BERT multilíngue) e mostramos que, ao contrário da redução do idioma, a destilação induziu uma queda de 1,7% a 6% na precisão geral do conjunto de dados XNLI. Os modelos e códigos apresentados estão disponíveis publicamente.', 'ja': '事前トレーニングを受けたトランスフォーマーベースのモデルは、さまざまな自然言語処理データセットで最先端の結果を達成しています。 しかし、これらのモデルのサイズは、実際の生産アプリケーションでの展開の欠点であることが多い。 多言語モデルの場合、ほとんどのパラメータは埋め込みレイヤーにあります。 したがって、語彙サイズを縮小することは、パラメータの総数に重要な影響を与えるはずです。 本稿では，対象言語に応じてより少ない数の言語を扱う小さなモデルを抽出することを提案する． XNLIデータセット上の多言語BERTの小さいバージョンの評価を提示していますが、この方法は他の多言語変圧器にも適用できると考えています。 得られた結果は、パラメータの総数の最大45%を削減しながら、比較可能な結果を維持する小さなモデルを生成できることを確認しました。 私たちは、DistilmBERT （多言語BERTの蒸留バージョン）とモデルを比較し、言語縮小とは異なり、蒸留はXNLIデータセットの全体的な精度の1.7 ％から6 ％の低下を誘発することを示しました。 提示されたモデルとコードは一般に公開されている。', 'zh': '先练之Transformer,正诸自然语言数集上最先进也。 然大小常以实用程序中为病。 其于多言模样,多参数嵌层中。 故损词汇量宜参数数。 于本文,臣等议据语料库提取少言数者为小模。 吾等于XNLI数集上立小本多言BERT评,谓此法可施于多言转换器。 得其实,成其小,以持其数,损其高45%参数数。 以模形与DistilmBERT(多言BERT者蒸馏版本)校之,明与言语还原不同,蒸馏致XNLI数集之体准确性降1.7%至6%。 形代码明矣。', 'hi': 'पूर्व प्रशिक्षित ट्रांसफॉर्मर-आधारित मॉडल प्राकृतिक भाषा प्रसंस्करण डेटा सेट की एक किस्म पर अत्याधुनिक परिणाम प्राप्त कर रहे हैं। हालांकि, इन मॉडलों का आकार अक्सर वास्तविक उत्पादन अनुप्रयोगों में उनकी तैनाती के लिए एक दोष होता है। बहुभाषी मॉडल के मामले में, अधिकांश पैरामीटर एम्बेडिंग परत में स्थित हैं। इसलिए, शब्दावली आकार को कम करने से पैरामीटर की कुल संख्या पर एक महत्वपूर्ण प्रभाव होना चाहिए। इस पेपर में, हम छोटे मॉडल निकालने का प्रस्ताव करते हैं जो लक्षित कॉर्पोरेट के अनुसार भाषाओं की कम संख्या को संभालते हैं। हम XNLI डेटा सेट पर बहुभाषी BERT के छोटे संस्करणों का मूल्यांकन प्रस्तुत करते हैं, लेकिन हमारा मानना है कि इस विधि को अन्य बहुभाषी ट्रांसफार्मरों पर लागू किया जा सकता है। प्राप्त परिणाम पुष्टि करते हैं कि हम छोटे मॉडल उत्पन्न कर सकते हैं जो तुलनीय परिणाम रखते हैं, जबकि मापदंडों की कुल संख्या के 45% तक कम करते हैं। हमने अपने मॉडल की तुलना DistilmBERT (बहुभाषी BERT का एक आसुत संस्करण) के साथ की और दिखाया कि भाषा में कमी के विपरीत, आसवन ने XNLI डेटा सेट पर समग्र सटीकता में 1.7% से 6% की गिरावट को प्रेरित किया। प्रस्तुत मॉडल और कोड सार्वजनिक रूप से उपलब्ध हैं।', 'ru': 'Предварительно обученные Трансформаторные модели достигают самых современных результатов на различных наборах данных Обработки Естественного Языка. Однако размер этих моделей часто является недостатком для их развертывания в реальных производственных приложениях. В случае многоязычных моделей большинство параметров расположены в слое вложений. Поэтому уменьшение объема словарного запаса должно оказать существенное влияние на общее количество параметров. В этой статье мы предлагаем извлечь более мелкие модели, которые обрабатывают меньшее количество языков в соответствии с целевыми корпусами. Мы представляем оценку меньших версий многоязычного BERT на наборе данных XNLI, но мы считаем, что этот метод может быть применен к другим многоязычным трансформаторам. Полученные результаты подтверждают, что мы можем генерировать более мелкие модели, которые сохраняют сопоставимые результаты, при этом уменьшая до 45% от общего количества параметров. Мы сравнили наши модели с DistilmBERT (дистиллированной версией многоязычного BERT) и показали, что в отличие от сокращения языка, дистилляция вызвала снижение общей точности набора данных XNLI на 1,7-6%. Представленные модели и код находятся в открытом доступе.', 'ga': 'Tá samhlacha réamh-oilte Trasfhoirmeoir-bhunaithe ag baint amach torthaí úrscothacha ar thacair sonraí éagsúla um Phróiseáil Teanga Nádúrtha. Mar sin féin, is minic a bhíonn méid na samhlacha seo ina míbhuntáiste dá n-imscaradh i bhfíor-fheidhmchláir táirgthe. I gcás samhlacha ilteangacha, tá an chuid is mó de na paraiméadair suite sa chiseal leabaithe. Mar sin, ba cheart go mbeadh tionchar tábhachtach ag laghdú méid an stór focal ar líon iomlán na bparaiméadar. Sa pháipéar seo, tá sé beartaithe againn samhlacha níos lú a bhaint as a láimhseálann líon níos lú teangacha de réir an sprioc-chorpóra. Cuirimid i láthair meastóireacht ar leaganacha níos lú de BERT ilteangach ar thacar sonraí XNLI, ach creidimid go bhféadfaí an modh seo a chur i bhfeidhm ar chlaochladáin ilteangacha eile. Deimhníonn na torthaí a fhaightear gur féidir linn samhlacha níos lú a ghiniúint a choimeádann torthaí inchomparáide, agus ag an am céanna laghdú suas le 45% de líon iomlán na bparaiméadar. Chuireamar ár samhlacha i gcomparáid le DistilmBERT (leagan driogtha de BERT ilteangach) agus léiríodh gur tharla titim 1.7% go 6% ar chruinneas iomlán thacar sonraí XNLI murab ionann agus laghdú teanga, mar gheall ar dhriogadh. Tá na samhlacha agus an cód curtha i láthair ar fáil go poiblí.', 'el': 'Τα προ-εκπαιδευμένα μοντέλα με βάση τον μετασχηματιστή επιτυγχάνουν αποτελέσματα τελευταίας τεχνολογίας σε μια ποικιλία συνόλων δεδομένων επεξεργασίας φυσικής γλώσσας. Ωστόσο, το μέγεθος αυτών των μοντέλων είναι συχνά μειονέκτημα για την ανάπτυξη τους σε πραγματικές εφαρμογές παραγωγής. Στην περίπτωση πολυγλωσσικών μοντέλων, οι περισσότερες από τις παραμέτρους βρίσκονται στο στρώμα ενσωμάτωσης. Ως εκ τούτου, η μείωση του μεγέθους του λεξιλογίου θα πρέπει να έχει σημαντικό αντίκτυπο στον συνολικό αριθμό των παραμέτρων. Στην παρούσα εργασία, προτείνουμε την εξαγωγή μικρότερων μοντέλων που χειρίζονται λιγότερο αριθμό γλωσσών σύμφωνα με τα στοχευμένα σώματα. Παρουσιάζουμε μια αξιολόγηση μικρότερων εκδόσεων του πολύγλωσσου BERT στο σύνολο δεδομένων αλλά πιστεύουμε ότι αυτή η μέθοδος μπορεί να εφαρμοστεί σε άλλους πολύγλωσσους μετασχηματιστές. Τα αποτελέσματα που λαμβάνονται επιβεβαιώνουν ότι μπορούμε να παράγουμε μικρότερα μοντέλα που διατηρούν συγκρίσιμα αποτελέσματα, μειώνοντας μέχρι 45% του συνολικού αριθμού παραμέτρων. Συγκρίναμε τα μοντέλα μας με το DistilmBERT (μια αποσταγμένη έκδοση του πολυγλωσσικού BERT) και δείξαμε ότι σε αντίθεση με τη μείωση της γλώσσας, η απόσταξη προκάλεσε πτώση 1,7% έως 6% στη συνολική ακρίβεια στο σύνολο δεδομένων XNLI. Τα μοντέλα και ο κώδικας που παρουσιάζονται είναι διαθέσιμα στο κοινό.', 'hu': 'Az előre képzett Transformer-alapú modellek a legmodernebb eredményeket érnek el a különböző Natural Language Processing adatkészleteken. Ezeknek a modelleknek a mérete azonban gyakran hátrányt jelent a valós gyártási alkalmazásokban való telepítésük szempontjából. Többnyelvű modellek esetében a paraméterek többsége a beágyazási rétegben található. Ezért a szókincs méretének csökkentése fontos hatással lehet a paraméterek teljes számára. Ebben a tanulmányban olyan kisebb modellek kivonását javasoljuk, amelyek kevesebb nyelvet kezelnek a célzott corpora szerint. Az XNLI adatkészleten bemutatjuk a többnyelvű BERT kisebb verzióinak értékelését, de úgy gondoljuk, hogy ez a módszer alkalmazható más többnyelvű transzformátorokra is. A kapott eredmények megerősítik, hogy kisebb modelleket tudunk létrehozni, amelyek hasonló eredményeket tartanak fenn, miközben a teljes paraméterek számának akár 45%-át is csökkentik. Modelljeinket a DistilmBERT-tel (a többnyelvű BERT desztillált változatával) hasonlítottuk össze, és megmutattuk, hogy a nyelvcsökkentéssel ellentétben a desztilláció az XNLI adatkészlet általános pontosságának 1,7-6%-os csökkenését eredményezte. A bemutatott modellek és kódok nyilvánosan hozzáférhetők.', 'ka': 'პირველ განსწავლებული ტრანფორმეტრის დაბათებული მოდელები თავისწავლის განსხვავებაში მონაცემების პროცესირების განსხვავებაში მიიღება. მაგრამ, ამ მოდელების ზომა ზოგიერთად არის მათი გამოყენება რეალური პროდისტურის პროგრამებში. მრავალენგური მოდელების შემთხვევაში, უფრო მეტი პარამეტრები დაიყენება ინბიდნენგის ჩატვირთში. ამიტომ, სიტყვალის ზომის შემცირება უნდა იქნება მნიშვნელოვანი პარამეტრის ზომის შემდეგ. ამ დოკუნეში ჩვენ მინდომებით პატარა მოდელების გამოყენება, რომლებიც უფრო ცოტა ენების რაოდენობა უფრო მცირე გამოყენება მინიშვნელოვანი კო ჩვენ XNLI მონაცემების მონაცემების მცირე ენერგიის BERT-ის პროცემების გავამუშავებთ, მაგრამ ჩვენ ვფიქრობთ, რომ ეს მეტი შეიძლება დააყენება სხვა მცირე ენერგიის რპნ მიიღებული წარმოდგენები დარწმუნდება, რომ ჩვენ შეგვიძლია შევქმნა ცოტა მოდელები, რომლებიც შემდგენებელი წარმოდგენების შემდგენება, რომლებიც პრამეტრების ჩვენ ჩვენი მოდელები DistilmBERT (მრავალენგური BERT-ის განსხვავებული ვერსია) და გამოჩვენეთ, რომ ენის შემცირების განსხვავებას განსხვავებულია, განსხვავება შემცირებული 1,7% დან 6% დაკავშირება XNLI მონაცემების საერთო ჩვენი მოდელები და კოდეები ადგილურად ხელსახულია.', 'it': "Modelli pre-addestrati basati su Transformer stanno ottenendo risultati all'avanguardia su una varietà di set di dati Natural Language Processing. Tuttavia, la dimensione di questi modelli è spesso uno svantaggio per la loro implementazione in applicazioni di produzione reali. Nel caso dei modelli multilingue, la maggior parte dei parametri si trova nel livello di incorporazione. Pertanto, ridurre le dimensioni del vocabolario dovrebbe avere un impatto importante sul numero totale di parametri. In questo articolo, proponiamo di estrarre modelli più piccoli che gestiscono un minor numero di lingue in base ai corpora target. Presentiamo una valutazione di versioni più piccole di BERT multilingue sul set di dati XNLI, ma riteniamo che questo metodo possa essere applicato ad altri trasformatori multilingue. I risultati ottenuti confermano che siamo in grado di generare modelli più piccoli che mantengono risultati comparabili, riducendo fino al 45% del numero totale di parametri. Abbiamo confrontato i nostri modelli con DistilmBERT (una versione distillata di BERT multilingue) e abbiamo mostrato che, a differenza della riduzione linguistica, la distillazione induce un calo dell'accuratezza complessiva dell'insieme di dati XNLI dall'1,7% al 6%. I modelli e il codice presentati sono disponibili al pubblico.", 'lt': 'Iš anksto parengti modeliai, pagrįsti transformatoriais, užtikrina pažangiausius įvairių gamtos kalbų apdorojimo duomenų rinkinių rezultatus. Tačiau šių modelių dydis dažnai yra jų panaudojimo realiose gamybos programose trūkumas. Daugiakalbių modelių atveju dauguma parametrų yra įterpiamojo sluoksnio. Todėl žodyno dydžio mažinimas turėtų turėti didelį poveikį bendram parametrų skaičiui. Šiame dokumente siūlome išimti mažesnius modelius, kuriuose pagal tikslinę korprą tvarkoma mažiau kalbų. Mes pateikiame XNLI duomenų rinkinyje mažesnių daugiakalbių BERT versijų vertinimą, tačiau manome, kad šis metodas gali būti taikomas kitiems daugiakalbiams transformatoriams. Gauti rezultatai patvirtina, kad galime sukurti mažesnius modelius, kurie išlaikytų palyginamus rezultatus ir kartu sumažintų iki 45 % bendro parametrų skaičiaus. Palyginome savo modelius su DistilmBERT (distiliuota daugiakalbės BERT versija) ir parodėme, kad, priešingai nei kalbos mažinimas, distiliacija sukėlė 1,7–6 % sumažėjimą bendro tikslumo XNLI duomenų rinkinyje. Pateikti modeliai ir kodas yra viešai prieinami.', 'mk': 'Претренираните трансформски модели постигнуваат најсовремени резултати на различни набори податоци за процес на природен јазик. Сепак, големината на овие модели честопати претставува недостаток за нивното распоредување во реалните производски апликации. Во случајот на мултијазични модели, повеќето параметри се лоцирани во слојот на вградување. Затоа, намалувањето на големината на речникот треба да има важно влијание врз вкупниот број на параметри. Во овој весник предлагаме да се извлечат помали модели кои се справуваат со помалку јазици според целното тело. Презентираме евалуација на помалите верзии на мултијазичниот БЕРТ на XNLI податоците, но веруваме дека овој метод може да се примени на други мултијазични трансформатори. Добиените резултати потврдуваат дека можеме да генерираме помали модели кои задржуваат споредливи резултати, додека се намалуваат до 45 отсто од вкупниот број на параметри. Ги споредивме нашите модели со DistilmBERT (дистилирана верзија на мултијазичниот BERT) и покажавме дека за разлика од намалувањето на јазикот, дистилирањето предизвика пад од 1,7 до 6 отсто на вкупната точност на XNLI податоците. Презентираните модели и код се јавно достапни.', 'kk': 'Алдыңғы оқылған түрлендіруші негіздеген үлгілер табиғи тілдерді өңдеу деректер жиындарының түрлі нәтижелерін жеткізеді. Бірақ бұл үлгілердің өлшемі көбінесе олардың шындық производылық қолданбаларында жүктеу үшін қалпына келтіріледі. Көптілік үлгілер үшін, параметрлердің көпшілігі ендіру қабатында орналасады. Сондықтан сөздік өлшемін азайту параметрлердің жалпы санына маңызды нәтижесі болу керек. Бұл қағазда, көп тілдерді бағытталған корпораға сәйкес істейтін кішкентай үлгілерді тарқату керек. Біз XNLI деректер жиынында көп тілді BERT нұсқаларының кіші нұсқаларын бағалаймыз, бірақ бұл әдіс басқа көп тілді түрлендірушерге қолданылатын деп ойлаймыз. Табылған нәтижелер біз салыстырылатын нәтижелерді салыстырып, жалпы параметрлердің жалпы санынан 45% дейін төмендетуге болады. Біз үлгілерімізді DistilmBERT (көп тілді BERT нұсқасы) мен салыстырып, тілді азайту қасиетіне қарамастырып, дистиллация XNLI деректер жиынында 1,7% мен 6% деңгейіне түсті. Келтірілген үлгілер мен код жалпы жеткізіледі.', 'ms': 'Model berasaskan-Transformer terlatih terlatih mencapai keputusan-state-of-the-art pada pelbagai set data Pemprosesan Bahasa Alami. Namun, saiz model ini sering merupakan kelemahan untuk penggunaan mereka dalam aplikasi produksi sebenar. Dalam kes model berbilang bahasa, kebanyakan parameter ditempatkan dalam lapisan penyembedding. Oleh itu, mengurangi saiz kamus patut mempunyai kesan penting pada jumlah parameter. Dalam kertas ini, kami cadangkan untuk mengekstrak model yang lebih kecil yang mengendalikan lebih sedikit bahasa menurut korpra sasaran. Kami memperkenalkan penilaian versi-versi lebih kecil BERT berbilang bahasa pada set data XNLI, tetapi kami percaya bahawa kaedah ini mungkin dilaksanakan pada pengubah berbilang bahasa lain. Hasil yang diperoleh mengesahkan bahawa kita boleh menghasilkan model yang lebih kecil yang menyimpan hasil yang boleh dibandingkan, sementara mengurangkan hingga 45% jumlah parameter. Kami membandingkan model kami dengan DistilmBERT (versi berwarna berbilang bahasa BERT) dan menunjukkan bahawa tidak seperti pengurangan bahasa, pengurangan mengakibatkan 1.7% hingga 6% dalam keseluruhan ketepatan pada set data XNLI. The presented models and code are publicly available.', 'ml': 'മുമ്പ് പരിശീലിക്കപ്പെട്ട ട ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d അടിസ്ഥാനത്തുള്ള മോഡലുകള്\u200d വ്യത്യസ്താഭാവ ഭാഷ പ്രവര്\u200dത്തിപ്പിക്കുന്ന എങ്കിലും ഈ മോഡലുകളുടെ വലിപ്പം യഥാര്\u200dത്ഥ ഉല്\u200dപാദിപ്പിക്കുന്ന പ്രയോഗങ്ങളില്\u200d അവരുടെ പ്രോഗ്രേഷനിലേക് In the case of multilingual models, most of the parameters are located in the embeddings layer.  അതുകൊണ്ട്, പദാവലിയുടെ വലിപ്പം കുറയ്ക്കുന്നതിനാല്\u200d മൊത്ത പരാമീറ്ററുകളുടെ മൊത്തം പ്രധാനപ്പെട്ട പ ഈ പത്രത്തില്\u200d, ലക്ഷ്യം ചെയ്യപ്പെട്ട കോര്\u200dപ്പോരയില്\u200d കുറച്ച് ഭാഷകള്\u200d കൈകാര്യം ചെയ്യുന്ന ചെറിയ മോഡലുകള്\u200d പുറത് എക്സ്\u200cനിലിയുടെ ഡാറ്റാ സജ്ജീകരണത്തില്\u200d പല ഭാഷകങ്ങളുടെ ചെറിയ പതിപ്പുകള്\u200d ബെര്\u200dട്ടിയുടെ വിലാസങ്ങള്\u200d ഞങ്ങള്\u200d കൊണ്ടുവരുന്നു. പക്ഷെ ഈ രീ സമ്പാദിച്ചതിന്റെ ഫലങ്ങള്\u200d തുല്യമായ ഫലങ്ങള്\u200d സൂക്ഷിക്കുന്ന ചെറിയ മോഡലുകള്\u200d ഉണ്ടാക്കാന്\u200d നമുക്ക് സാധ്യമല്ലെന്ന് ഉറപ്പ് വരു ഞങ്ങള്\u200d നമ്മുടെ മോഡലുകളെ ഡിസ്റ്റില്\u200dബെര്\u200dട്ടിയോടൊപ്പം തുല്യമാക്കി (പല ഭാഷ ബെര്\u200dട്ടിയുടെ ഒരു വ്യത്യസ്തമായ സംസ്കാരം) കാണിച്ചു. ഭാഷ കുറഞ്ഞത് തുല്യമായ ഭാ പ്രദര്\u200dശിപ്പിച്ച മോഡലുകളും കോഡുകളും പ്രത്യേകം ലഭ്യമാണ്.', 'mt': 'Mudelli bbażati fuq it-Trasformer imħarrġa minn qabel qed jiksbu riżultati l-aktar avvanzati fuq varjetà ta’ settijiet ta’ dejta dwar l-ipproċessar tal-lingwi naturali. However, the size of these models is often a drawback for their deployment in real production applications.  Fil-każ ta’ mudelli multilingwi, il-biċċa l-kbira tal-parametri jinsabu fis-saff tal-inkorporazzjoni. Għalhekk, it-tnaqqis tad-daqs vokabulari għandu jkollu impatt importanti fuq in-numru totali ta’ parametri. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora.  Aħna nippreżentaw evalwazzjoni ta’ verżjonijiet iżgħar ta’ BERT multilingwi fuq is-sett ta’ dejta XNLI, iżda aħna nemmnu li dan il-metodu jista’ jiġi applikat għal trasformaturi multilingwi oħra. Ir-riżultati miksuba jikkonfermaw li nistgħu niġġeneraw mudelli iżgħar li jżommu riżultati komparabbli, filwaqt li jnaqqsu sa 45% tan-numru totali ta’ parametri. Qabbelna l-mudelli tagħna ma’ DistilmBERT (verżjoni distillata ta’ BERT multilingwi) u wrew li kuntrarjament għat-tnaqqis tal-lingwa, id-distillazzjoni kkawżat tnaqqis ta’ 1.7% sa 6% fil-preċiżjoni globali tas-sett tad-dejta XNLI. Il-mudelli u l-kodiċi ppreżentati huma disponibbli għall-pubbliku.', 'pl': 'Wstępnie przeszkolone modele oparte na transformatorze osiągają najnowocześniejsze wyniki na różnych zbiorach danych przetwarzania języka naturalnego. Jednak wielkość tych modeli jest często wadą dla ich wdrożenia w rzeczywistych aplikacjach produkcyjnych. W przypadku modeli wielojęzycznych większość parametrów znajduje się w warstwie osadzeń. Dlatego zmniejszenie wielkości słownictwa powinno mieć istotny wpływ na całkowitą liczbę parametrów. W niniejszym artykule proponujemy wyodrębnienie mniejszych modeli obsługujących mniejszą liczbę języków zgodnie z docelowymi korpusami. Przedstawiamy ocenę mniejszych wersji wielojęzycznego BERT na zbiorze danych XNLI, ale uważamy, że metoda ta może być stosowana do innych wielojęzycznych transformatorów. Uzyskane wyniki potwierdzają, że możemy generować mniejsze modele, które zachowują porównywalne wyniki, przy jednoczesnym zmniejszeniu do 45% całkowitej liczby parametrów. Porównaliśmy nasze modele z DistilmBERT (destylowaną wersją wielojęzycznego BERT) i wykazaliśmy, że w przeciwieństwie do redukcji języka, destylacja powodowała spadek ogólnej dokładności zbioru danych XNLI od 1,7% do 6% . Prezentowane modele i kod są publicznie dostępne.', 'mn': 'Өмнөх сургалтын Трансформатор суурилсан загварууд нь байгалийн хэл Процесс өгөгдлийн хэлбэрээр олон төрлийн үр дүнг хүртэл байна. Гэвч эдгээр загваруудын хэмжээ нь жинхэнэ үйлдвэрлэлийн хэрэглэмжүүд дээр ажиллах загвар юм. Ихэнх хэл загварын тухай ихэнх параметрлүүд байрлуулагдаж байгаа юм. Тиймээс үгийн хэмжээг багасгах нь нийт параметр дээр чухал нөлөө үзүүлэх хэрэгтэй. Энэ цаасан дээр бид жижиг загваруудыг ашиглах нь зориулагдсан Корпора-ын хувьд хэдэн хэл бага байдаг. Бид XNLI өгөгдлийн бага хэлний BERT-ын бага хувилбарын үнэлгээг XNLI-н өгөгдлийн багтаад үзүүлнэ. Гэхдээ бид энэ арга бусад олон хэлний шилжүүлэгчид хэрэглэгдэх боломжтой гэж боддог. Шинэ гарсан үр дүнд бид харьцуулах үр дүнг үргэлжлүүлэх жижиг загваруудыг гаргаж чадна гэдгийг баталдаг. Бид өөрсдийн загварыг DistilmBERT-тай харьцуулсан (олон хэл BERT-ын хувилбар) болон хэлний багасгалын ялгаатай харуулсан бөгөөд distillation нь XNLI өгөгдлийн хэмжээний нийтлэг тодорхойлолт нь 1.7% болон 6% бууруулсан байна. Холбоотой загвар болон код олон нийтэд хангалттай.', 'ro': 'Modelele pre-instruite bazate pe Transformer obțin rezultate de ultimă oră pe o varietate de seturi de date de procesare a limbajului natural. Cu toate acestea, dimensiunea acestor modele este adesea un dezavantaj pentru implementarea lor în aplicații reale de producție. În cazul modelelor multilingve, majoritatea parametrilor se află în stratul de încorporare. Prin urmare, reducerea dimensiunii vocabularului ar trebui să aibă un impact important asupra numărului total de parametri. În această lucrare, propunem extragerea unor modele mai mici care gestionează mai puțin număr de limbi în funcție de corporele vizate. Prezentăm o evaluare a versiunilor mai mici ale BERT multilingve pe setul de date XNLI, dar credem că această metodă poate fi aplicată altor transformatoare multilingve. Rezultatele obținute confirmă faptul că putem genera modele mai mici care păstrează rezultate comparabile, reducând în același timp până la 45% din numărul total de parametri. Am comparat modelele noastre cu DistilmBERT (o versiune distilată a BERT multilingvă) și am arătat că, spre deosebire de reducerea limbii, distilarea a indus o scădere de 1,7% până la 6% a acurateții globale a setului de date XNLI. Modelele și codurile prezentate sunt disponibile publicului.', 'no': 'Førehandsvis transformeringsbaserte modeller har tilgang til kunsttilstanden på ulike datasett for naturspråk- handsaming. Men storleiken på desse modelane er ofte ein trekking for å utføra dei i verkeleg produksjonsprogram. I tillegg til fleire språk-modeller finn dei fleste parametra i innbyggingslaget. Derfor må du redusera ordlistestorleiken ha viktig effekt på totalt tal på parametrar. I denne papiret foreslår vi å pakka ut mindre modeller som handterar mindre tal språk etter målte korpora. Vi presenterer ein evaluering av mindre versjonar av fleirspråkspelske BERT på XNLI- datasettet, men vi tror at denne metoden kan brukast til andre fleirspråksomformarar. Den oppteke resultaten stadfestar at vi kan laga mindre modeller som held sammenlignbare resultat, mens du reduserer opp til 45 % av totalt tal av parametrar. Vi samanlikna modellen våre med DistilmBERT (ein distilisert versjon av fleirspråk BERT) og viste at forskjellig språkkreduksjon fører distillasjon til ein 1,7% til 6% nedtrekk i den generelle nøyaktigheten på XNLI- datasettet. Den presenterte modellen og koden er tilgjengeleg offentlig.', 'sr': 'Preobučeni modeli na transformeru postižu rezultate umjetnosti na raznim setima podataka o procesu prirodnog jezika. Međutim, veličin a ovih modela je često odvraćanje za njihovu rasporedu u pravim proizvodnim aplikacijama. U slučaju multijezičkih modela, većina parametara se nalazi u sloju ugrađenja. Stoga smanjenje veličine rečenika treba imati važan uticaj na ukupni broj parametara. U ovom papiru predlažemo da izvučemo manje modele koji se bave manjim brojem jezika prema ciljanoj korpori. Predstavljamo procjenu manjih verzija multijezičkih BERT-a na setu XNLI podataka, ali vjerujemo da se ovaj metod može primjenjivati drugim multijezičkim transformatorima. Pronađeni rezultati potvrđuju da možemo stvoriti manje modele koji održavaju usporedbene rezultate, dok smanjujemo do 45% ukupnog broja parametara. Usporedili smo naše modele sa DistilmBERT (destilirana verzija multijezičkog BERT) i pokazali da, za razliku od smanjenja jezika, destilacija je izazvala 1,7% na 6% pad ukupne tačnosti na setu XNLI podataka. Predstavljeni modeli i kodovi su javno dostupni.', 'si': 'ප්\u200dරධාන ප්\u200dරවේශනය කරන්න ප්\u200dරවේශනය කරන්න ප්\u200dරවේශනය සඳහා ප්\u200dරවේශනය භාෂාව ප්\u200dරවේශනය කරන්න තොරතුරු සැකසුම් වලින් ස නමුත්, මේ මොඩේල්ස් වල ප්\u200dරමාණය හැමවෙලාවටම ඔවුන්ගේ ඇත්ත උත්පාදනය වැඩසටහන් කරන්න ප්\u200dරමාණයක් වෙ ගොඩක් භාෂාවක් මොඩල් වලින්, ගොඩක් පැරැමීටර් ස්ථාපනය සම්බන්ධ කරලා තියෙනවා. ඉතින්, වාක්ෂාව ප්\u200dරමාණය අඩු කරන්න පුළුවන් ප්\u200dරමාණයක් සම්පූර්ණ සංඛ්යාවට වැදගත් ප්\u200dරමා මේ පත්තරේ අපි ප්\u200dරශ්නයක් කරනවා පොඩි මොඩේල් එකක් අරගෙන ඉලක්කු කොර්පෝරා වලට අඩු භාෂාවල් අඩු අඩු අඩු  අපි XNLI දත්ත සූදානයේ පොඩි භාෂාවක් BERT ගේ පොඩි සංවිධානයක් විශ්වාස කරනවා, ඒත් අපි විශ්වාස කරනවා මේ විධානය අනි ගත්ත ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරත අපි අපේ මොඩේල්ස් එක්ක Distil mBERT (විශේෂ භාෂාවක් BERT ගේ විශේෂ සංස්කරණයක්) සහ පෙන්වන්න පුළුවන් වුනා කියලා භාෂාවක් විශේෂ වෙනුවෙන් විශ පෙන්වන්න ප්\u200dරමාණය සහ කෝඩ් සාමාන්\u200dය විදියට ප්\u200dරතිකාරයෙන් තියෙනවා.', 'so': "Tusaalooyinka horay loo tababariyey waxay heli karaan xaaladda farshaxanka, waxayna ku heli karaan sawirro kala duduwan oo kala duduwan shaqeynta luqada asalka ah. Si kastaba ha ahaatee tirada modelladan inta badan waa dib u soo celin karo codsiga dhaqaalaha ee dhabta ah. Haddii ay ku dhacdo tusaalooyin luuqado badan, heerarka badankoodu waxay ku yaalaan darafka aad ku jirto. Sidaa darteed reducing tirada hadalka waa in ay saameyn muhiim u yeelan karaan lambarka lambarka ah oo dhan. Qoraalkan waxaan ku talo galaynaa in aan soo bixino modelal yar oo qabanqaabiya luuqadaha in yar sida shirkadda la hagayo. Waxaynu soo bandhignaa qiimeyn yar oo ka mid ah BERT luuqado kala duduwan ee XNLI, laakiin waxaynu aaminsanahay in qaabkan lagu codsan karo isbedelka luuqadaha kale oo kala duduwan. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters.  Tusaale'yadeena ayaannu isbarbardhignay DistilmBERT (warqad kala duduwan BERT) waxaana tusnay in hoos u dhigista luuqadaha kala duduwan, kala soocsiga waxaa ka soo baxay 1.7% ilaa 6% si rasmi ah oo ku qoran kooxda data XNLI. Tusaalada la soo saaray iyo codsiga waxaa si bayaan ah loo heli karaa.", 'sv': 'Förutbildade Transformer-baserade modeller uppnår toppmoderna resultat på en mängd olika datauppsättningar för Natural Language Processing. Storleken på dessa modeller är dock ofta en nackdel för deras distribution i verkliga produktionsapplikationer. För flerspråkiga modeller finns de flesta parametrar i inbäddningslaget. Därför bör en minskning av ordförrådsstorleken ha en viktig inverkan på det totala antalet parametrar. I denna uppsats föreslår vi att extrahera mindre modeller som hanterar färre språk enligt de riktade korporarna. Vi presenterar en utvärdering av mindre versioner av flerspråkig BERT på XNLI datauppsättningen, men vi tror att denna metod kan tillämpas på andra flerspråkiga transformatorer. De erhållna resultaten bekräftar att vi kan generera mindre modeller som håller jämförbara resultat samtidigt som vi minskar upp till 45% av det totala antalet parametrar. Vi jämförde våra modeller med DistilmBERT (en destillerad version av flerspråkig BERT) och visade att till skillnad från språkreduktion inducerade destillation en minskning på 1,7% till 6% i den totala noggrannheten på XNLI-datauppsättningen. De presenterade modellerna och koden är allmänt tillgängliga.', 'ta': 'முன்பு பயிற்சி மாற்றும் மாதிரிகள் பல இயல்பான மொழி செயல்பாடு தரவு அமைப்புகளின் நிலையில்- கலை முடிவுகளை பெறுகிறது. எனினும், இந்த மாதிரிகளின் அளவு பெரும்பாலும் உண்மையான உறுப்புப் பொருள் பயன்பாடுகளில் அவர்கள் வெளியீட்டை  பல மொழி மாதிரிகளின் பெரும்பாலான அளபுருக்கள் உள்ளடக்க அடுக்கில் உள்ளது. எனவே, சொல்வளத்தின் அளவை குறைத்தால் அளபுருக்களின் மொத்த எண்ணிக்கையில் முக்கியமான பாதிக்க வேண்டும். இந்த காகிதத்தில், நாம் சிறிய மாதிரிகளை வெளியேற்ற வேண்டும் என்று பரிந்துரைக்கிறோம். இலக்கு நிறுவனத்தை பொ XNLI தகவல் அமைப்பில் ஒரு சிறிய மொழி பிரெட்டின் சிறிய பதிப்புகளை மதிப்பிடுகிறோம். ஆனால் இந்த முறைமை மற்ற பல மொழி மாற்றங்களுக்கு பயன்படுத கண்டுபிடிக்கப்பட்ட முடிவுகளை நாம் சிறிய மாதிரிகளை உருவாக்க முடியும் என்று உறுதி செய்கிறது, அது ஒப்பிடும் முடிவுகளை வை நாம் எங்கள் மாதிரிகளை டிஸ்டில்எம்பெர்ட்( பல மொழி பிரெட்டின் வித்தியாசமான பதிப்பு) ஒப்பிட்டோம். மொழி குறைப்பு, வேறுபாடு XNLI தகவல் அமைப்பில் மொத்த த து கொடுக்கப்பட்ட மாதிரிகள் மற்றும் குறியீடு பொதுவாக கிடைக்கும்.', 'ur': 'پہلے تدریس کیا گیا ترنسفور بنیادی موڈل ایک مختلف طبیعی زبان پرینسٹ ڈیٹ سٹ پر موجود ہیں. لیکن ان نمڈلوں کا اندازہ اغلب ایک دفعہ ہے کہ حقیقی پیدائش کاربریوں میں ان کے استعمال کرنے کے لئے۔ بہت سی زبان موڈل کے مطابق، اکثر پارامیٹروں میں انڈینگ لائر میں موجود ہیں. لہٰذا، کلمات کی اندازہ کم کرنے کی ضرورت ہے کہ پارامیٹروں کی جمع تعداد پر ایک اثر ہو جائے۔ اس کاغذ میں ہم ایک چھوٹی موڈل اٹھانے کے لئے پیشنهاد کرتے ہیں جن کی تعداد کم زبانوں کو موجود کرتا ہے۔ ہم XNLI ڈاٹ سٹ پر بہت سی زبان کی BERT کی چھوٹی نسخہ کی ایک ارزیابی پیش کرتے ہیں لیکن ہم سمجھتے ہیں کہ یہ طریقہ دوسرے بہت سی زبان تبدیل کرنے والوں کے لئے لازم ہو سکتا ہے۔ پیدا ہوئے نتیجے ثابت کرتے ہیں کہ ہم چھوٹے موڈل پیدا کر سکتے ہیں جو مقایسہ نتیجے رکھتے ہیں، حالانکہ پارامتروں کی جمع تعداد 45% کم کر سکتے ہیں. ہم نے ہمارے مدلکوں کو DistilmBERT کے ساتھ مقایسہ کیا اور دکھایا کہ زبان کاٹنے کے مطابق مختلف طریقہ سے ایک.7% سے 6% ڈوبنے کو XNLI ڈیٹ سٹ کے سارے دقیقات میں ڈوبا دیا۔ پیش کیے ہوئے موڈل اور کوڈ ظاہر طور پر موجود ہیں.', 'vi': 'Các mô hình transformer đã được đào tạo trước đang đạt kết quả tối tân trên một số đơn vị dữ liệu xử lý ngôn ngữ tự nhiên. Tuy nhiên, kích thước của những mô hình này thường là một trở ngại cho việc triển khai trong ứng dụng sản xuất thực tế. Trong các mô hình đa dạng, hầu hết các tham số đều nằm trong lớp nhúng. Do đó, việc giảm kích thước ngôn ngữ nên có tác động quan trọng tới số tham số tổng hợp. Trong tờ giấy này, chúng tôi đề nghị trích xuất các mô hình nhỏ hơn, sử dụng số ngôn ngữ ít hơn theo lời hạ sĩ đích thực. Chúng tôi đưa ra đánh giá các phiên bản nhỏ hơn của hỗn hợp BERT ở bộ dữ liệu XLI, nhưng chúng tôi tin rằng phương pháp này có thể được áp dụng cho các máy biến đổi đa dạng khác nhau. Kết quả xác nhận chúng tôi có thể tạo ra các mô hình nhỏ hơn để giữ kết quả có thể so sánh, trong khi giảm đến 45 kg trong số tham số tổng thể. Chúng tôi đã so sánh các mô hình của chúng tôi với phẩm cách chưng cất thực phẩm thiếu ngôn ngữ, và cho thấy, không giống với việc giảm ngôn ngữ, chưng cất gây giảm độ chính xác cao cả trên bộ dữ liệu XMLI. Các mẫu và mã được trình bày công khai.', 'uz': "Formerli taʼminlovchi modellar turli taʼminlovchi taʼminotlar tarkibi tabiiy tilni boshqaruvchi maʼlumot moslamalariga holatni bajaradi. Lekin, bu modellarning oʻlchami ko'pincha ishlab chiqarish uchun ishlab chiqaradi. Ko'pchilik tili modellarida, ko'pchilik parametrlar chegaraga joylashadi. Shunday qilib, soʻzning oʻlchamini kamaytirish parametrlar soni ham muhim 影响 kerak. Bu qogʻozda biz qancha tilni boshqaruvchi kichkina modellarni chiqarishni talab qilamiz. Biz XNLI maʼlumotlar tarkibida bir nechta tildagi BERT versiyalarini qiymatlashimiz mumkin, lekin biz ishonamiz, bu usul boshqa tillar o'zgarishga qoʻllaniladi. Muvaffaqiyatli natijalarni tasdiqlash mumkin, biz mos keladigan natijalarni yaratishimiz mumkin, va hamma parametrlar sonini 45% kamaytirish mumkin. Biz modellarimizni DistilmBERT (bir xil tilning BERT versiyasi) bilan kamaytirib ko'rsatdik va o'z tilning ko'payishini ko'rsatdik, ajratish XNLI maʼlumotlari to ʻliq aniqligiga 1.7% dan 6% chiqaradi. Koʻrsatilgan modellar va kodlash umumiy mavjud.", 'bg': 'Предварително обучените модели, базирани на трансформатори, постигат най-съвременни резултати върху различни набори от данни за обработка на естествен език. Размерът на тези модели обаче често е недостатък за внедряването им в реални производствени приложения. В случай на многоезични модели повечето параметри се намират в слоя вграждания. Следователно намаляването на размера на речника трябва да има важно въздействие върху общия брой параметри. В тази статия предлагаме да се извлекат по-малки модели, които обработват по-малко езици според целевите корпуси. Представяме оценка на по-малки версии на многоезичния BERT на набора данни, но считаме, че този метод може да се приложи и към други многоезични трансформатори. Получените резултати потвърждават, че можем да генерираме по-малки модели, които запазват сравними резултати, като същевременно намаляват до 45% от общия брой параметри. Сравнихме нашите модели с дестилирана версия на многоезичен BERT и показахме, че за разлика от езиковото намаление, дестилацията индуцира спад от 1,7% до 6% в общата точност на набора данни. Представените модели и код са публично достъпни.', 'nl': 'Vooropgeleide Transformer-gebaseerde modellen bereiken state-of-the-art resultaten op een verscheidenheid van Natural Language Processing datasets. De grootte van deze modellen is echter vaak een nadeel voor hun inzet in echte productietoepassingen. In het geval van meertalige modellen bevinden de meeste parameters zich in de insluitingslaag. Daarom zou het verminderen van de woordenschat een belangrijke invloed moeten hebben op het totale aantal parameters. In dit artikel stellen we voor om kleinere modellen te extraheren die minder talen hanteren volgens de beoogde corpora. We presenteren een evaluatie van kleinere versies van meertalige BERT op de XNLI dataset, maar we geloven dat deze methode kan worden toegepast op andere meertalige transformatoren. De verkregen resultaten bevestigen dat we kleinere modellen kunnen genereren die vergelijkbare resultaten behouden, terwijl we tot 45% van het totale aantal parameters verminderen. We vergeleken onze modellen met DistilmBERT (een gedistilleerde versie van meertalige BERT) en toonden aan dat distillatie, in tegenstelling tot taalreductie, een afname van 1,7% tot 6% veroorzaakte in de algehele nauwkeurigheid van de XNLI dataset. De gepresenteerde modellen en code zijn openbaar beschikbaar.', 'da': 'Forududdannede Transformer-baserede modeller opnår state-of-the-art resultater på en række Natural Language Processing datasæt. Størrelsen af disse modeller er dog ofte en ulempe for deres implementering i reelle produktionsapplikationer. I tilfælde af flersprogede modeller er de fleste parametre placeret i indlejringslaget. Derfor bør reduktion af ordforrådsstørrelsen have en vigtig indvirkning på det samlede antal parametre. I denne artikel foreslår vi at udtrække mindre modeller, der håndterer færre antal sprog i henhold til de målrettede korpora. Vi præsenterer en evaluering af mindre versioner af flersproget BERT på XNLI datasættet, men vi mener, at denne metode kan anvendes på andre flersprogede transformatorer. De opnåede resultater bekræfter, at vi kan generere mindre modeller, der holder sammenlignelige resultater, samtidig med at vi reducerer op til 45% af det samlede antal parametre. Vi sammenlignede vores modeller med DistilmBERT (en destilleret version af flersproget BERT) og viste, at destillation i modsætning til sprogreduktion inducerede et fald på 1,7% til 6% i den samlede nøjagtighed på XNLI datasættet. De præsenterede modeller og kode er offentligt tilgængelige.', 'de': 'Vortrainierte Transformer-basierte Modelle erzielen State-of-the-Art Ergebnisse auf einer Vielzahl von Natural Language Processing Datensätzen. Die Größe dieser Modelle ist jedoch oft ein Nachteil für den Einsatz in realen Produktionsanwendungen. Bei mehrsprachigen Modellen befinden sich die meisten Parameter in der Einbettungsebene. Daher sollte die Reduzierung der Vokabelgröße einen wichtigen Einfluss auf die Gesamtzahl der Parameter haben. In diesem Beitrag schlagen wir vor, kleinere Modelle zu extrahieren, die entsprechend der Zielkorpora weniger Sprachen verarbeiten. Wir präsentieren eine Bewertung kleinerer Versionen von mehrsprachigem BERT auf dem XNLI Datensatz, aber wir glauben, dass diese Methode auf andere mehrsprachige Transformatoren angewendet werden kann. Die erhaltenen Ergebnisse bestätigen, dass wir kleinere Modelle generieren können, die vergleichbare Ergebnisse beibehalten und dabei bis zu 45% der Gesamtzahl der Parameter reduzieren. Wir verglichen unsere Modelle mit DistilmBERT (einer destillierten Version von mehrsprachigem BERT) und zeigten, dass die Destillation im Gegensatz zur Sprachreduktion einen Rückgang von 1,7% auf 6% in der Gesamtgenauigkeit des XNLI-Datensatzes bewirkte. Die vorgestellten Modelle und Codes sind öffentlich zugänglich.', 'hr': 'Preobučeni modeli na transformeri postižu rezultate umjetnosti na raznim setima podataka o procesu prirodnog jezika. Međutim, veličin a tih modela često je povlačenje za njihovu rasporedu u pravim proizvodnjim primjenama. U slučaju multijezičkih modela, većina parametara se nalazi u sloju ugrađenja. Stoga smanjenje veličine riječnika treba imati važan utjecaj na ukupni broj parametara. U ovom papiru predlažemo izvući manje modele koji se bave manjim brojem jezika prema ciljanoj korpori. Predstavljamo procjenu manjih verzija multijezičkih BERT-a na setu XNLI podataka, ali vjerujemo da se ovaj metod može primjenjivati drugim multijezičkim transformatorima. Pronađeni rezultati potvrđuju da možemo stvoriti manje modele koji održavaju usporedbene rezultate, dok smanjujemo do 45% ukupnog broja parametara. Usporedili smo naše modele s DistilmBERT (destilirana verzija višejezičkog BERT) i pokazali smo da, za razliku od smanjenja jezika, destilacija je izazvala smanjenje od 1,7% do 6% ukupne to čnosti na setu XNLI podataka. Predstavljeni modeli i kod su javno dostupni.', 'ko': 'Transformer 기반의 예비 훈련 모델은 각종 자연 언어 처리 데이터 집합에서 가장 선진적인 결과를 실현하고 있다.그러나 이런 모델의 크기는 흔히 실제 생산 응용에 배치된 단점이다.다중 언어 모델의 경우 대부분의 매개변수가 포함된 레이어에 있습니다.따라서 어휘량을 줄이는 것은 매개 변수의 총수에 중요한 영향을 미쳐야 한다.본고에서 우리는 목표 어료 라이브러리에 따라 비교적 적은 언어의 작은 모델을 추출 처리하는 것을 권장한다.우리는 XNLI 데이터 세트에서 다국어 BERT의 비교적 작은 버전을 평가했지만, 이 방법은 다른 다국어 변압기에 적용될 수 있다고 생각한다.얻은 결과에 의하면 우리는 더욱 작은 모델을 생성하여 비교 가능한 결과를 유지할 수 있을 뿐만 아니라 45%에 달하는 매개 변수의 총수를 줄일 수 있다.Dell의 모델을 DistilmBERT(다중 언어 BERT의 증류 버전)와 비교한 결과, 언어 단순화와 달리 증류로 인해 XNLI 데이터 세트의 전체 정확성이 1.7%에서 6% 감소한 것으로 나타났습니다.제공된 모델과 코드는 공개된 것이다.', 'id': 'Model Transformer yang dilatih sebelumnya mencapai hasil terbaik dari berbagai set data Proses Bahasa Alam. Namun, ukuran model ini sering merupakan kelemahan untuk penerbangan mereka dalam aplikasi produksi nyata. Dalam kasus model berbagai bahasa, sebagian besar parameter ditempatkan dalam lapisan penyembedding. Oleh karena itu, mengurangi ukuran vokal seharusnya memiliki dampak penting pada jumlah total parameter. Dalam kertas ini, kami mengusulkan untuk mengekstrak model yang lebih kecil yang menangani lebih sedikit bahasa menurut corpora yang ditujukan. Kami mempersembahkan evaluasi versi-versi lebih kecil dari BERT multibahasa pada set data XNLI, tapi kami percaya bahwa metode ini mungkin diterapkan pada transformator multibahasa lainnya. Hasil yang diperoleh mengkonfirmasi bahwa kita dapat menghasilkan model yang lebih kecil yang menjaga hasil yang dapat dibandingkan, sementara mengurangi sampai 45% dari jumlah total parameter. Kami membandingkan model kami dengan DistilmBERT (versi destil dari BERT berbilang bahasa) dan menunjukkan bahwa tidak seperti pengurangan bahasa, destilasi mengakibatkan 1,7% hingga 6% drop dalam keseluruhan akurasi pada set data XNLI. Model dan kode yang ditemukan tersedia publik.', 'tr': "Taryh dili işleýän hatlaryň bir näçe çeşitli çykyşyna taýýarlanýar. Ýöne bu nusgalaryň ulysy köplenç özleriniň hakyky prowada uygulamalarynda ýerine ýetirmek üçin bir nusga edýär. Birnäçe dilli nusgalar bolsa, köp parameterler ýerleşdirilýär. Şonuň üçin sözleriň ululykyny azaltmak toplam parameterler üçin wajyp täsir bolmaly. Bu kagyzda, netijeli korpora görä azajyk nusgalary çykarmagy teklip edýäris. XNLI data setirinde, multi dilli BERT'yň kiçi sanlarynyň çykyşyny çykarýarys, ýöne bu yöntemi başga multi dilli terjimelere uygulanabilir diýip pikir edýäris. Kabul edilen netijeler, netijeleri karşılaştırılýan kiçi modeller oluşturabileceğimizi onaylaýar, toplam parametrlerin 45% orana düşürüp biler. Biziň modellerimizi DistilmBERT (köp dilli BERT'yň a ýratyn versiýasy) bilen daýaryşdyrdyk we bu şekilde dilleriň azalyşygy ýaly bir şekilde täsirlenmek XNLI maglumatyň düýbünde 1.7% we 6% derejesi düşürdik. Görkezilen nusgalar we ködler publika içinde bar.", 'sw': 'Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets.  Hata hivyo, ukubwa wa mifano hii mara nyingi ni mchoro wa kutumika kwa ajili ya matumizi halisi ya uzalishaji. Katika mifano ya lugha mbalimbali, parameter nyingi ziko kwenye daraja la viwanja. Kwa hiyo, kupunguza ukubwa wa lugha unapaswa kuwa na athari muhimu juu ya jumla ya parameter. Katika karatasi hii, tunapendekeza kuondoa mifano madogo ambayo yanakabiliwa na lugha ndogo kwa mujibu wa kampuni inayolengwa. Tunatoa tathmini ndogo ya toleo la BERT kwa lugha mbalimbali kwenye seti ya data za XNLI, lakini tunaamini kuwa njia hii inaweza kutumika kwa mabadiliko mengine ya lugha mbalimbali. Matokeo yaliyopata yanathibitisha kuwa tunaweza kutengeneza mifano madogo ambayo yanaendelea kuwa na matokeo yanafanana, wakati kupunguza hadi asilimia 45 ya jumla ya kipimo cha parameters. Tulifananisha mifano yetu na DistilmBERT (toleo tofauti la BERT kwa lugha mbalimbali) na kuonyesha kwamba kupungua kwa tofauti na lugha, tofauti ilisababisha kupungua asilimia 1.7 hadi asilimia 6 kwenye kiwango cha sahihi cha takwimu za XNLI. Mradi na sheria zilizotolewa zinapatikana hadharani.', 'af': "Vorige onderwerp Transformeerder- gebaseerde modele word toegang van state- of- the- art resultate op 'n verskeie van Natuurlike Taal Verwerking data stelle. Alhoewel, die grootte van hierdie modele is dikwels 'n teken vir hulle verwydering in werklike produksie toepassings. In die geval van veelvuldige modele, die meeste van die parameters is in die inbêding laag. Daarom moet die verduur van die woordeboekgrootte 'n belangrike invloek op die totaal aantal parameters hê. In hierdie papier, ons voorstel om kleinere modele te uitpak wat minder aantal tale hanteer volgens die doel van die korpora. Ons stel 'n evaluasie van kleinere weergawe van multilinglike BERT op die XNLI data stel, maar ons glo dat hierdie metode dalk aan ander multilinglike transformeerders toewend kan word. Die ontvangde resultate bevestig dat ons kleiner modele kan genereer wat vergelykbaar resultate hou, terwyl ons verduur tot 45% van die totaal aantal parameters. Ons vergelyk ons modele met DistilmBERT ( 'n verskillende weergawe van multilinglike BERT) en wys dat ongelukkig taal reduksie, destilasie het 'n 1. 7% na 6% afval in die hele presisie op die XNLI data stel aangedoen. Die voorgestelde modele en kode is openlik beskikbaar.", 'sq': 'Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets.  Megjithatë, madhësia e këtyre modeleve shpesh është një pengesë për vendosjen e tyre në aplikimet reale të prodhimit. Në rastin e modeleve shumëgjuhësore, shumica e parametrave janë vendosur në shtresën e përfshirjes. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters.  Në këtë letër, propozojmë të nxjerrim modele më të vogla që trajtojnë më pak gjuhë sipas korprës së synuar. Ne paraqesim një vlerësim të versioneve më të vogla të BERT shumëgjuhës në grupin e të dhënave XNLI, por besojmë se ky metodë mund të aplikohet për transformuesit e tjerë shumëgjuhës. Rezultatet e fituara konfirmojnë se ne mund të gjenerojmë modele më të vogla që mbajnë rezultate të krahasueshme, duke reduktuar deri në 45% të numrit total të parametrave. Ne krahasuam modelet tona me DistilmBERT (një version i distilluar i BERT shumëgjuhës) dhe treguam se në ndryshim nga zvogëlimi i gjuhës, distillacioni induktoi një rënie prej 1.7% deri në 6% në saktësinë e përgjithshme në grupin e të dhënave XNLI. Modelet dhe kodet e paraqitura janë në dispozicion publik.', 'am': 'የቀድሞው የተማረከ የፊደል መተላለፊያ ዓይነቶች በተለያዩ የባሕላዊ ቋንቋ ፕሮጀክት ዳታ ማዘጋጀት የሥርዓት-የ- art ፍሬዎችን አግኝተዋል፡፡ ነገር ግን የእነዚህ ሞዴላዎች መጠን ብዙ ጊዜ የእውነቱ አካባቢ ፕሮግራሞች ውስጥ ለመውሰድ የሚያስፈልገውን መልስ ነው፡፡ በብዙ ቋንቋዎች ምሳሌዎች ቢሆን፣ አብዛኞቹ ምርጫዎች በአካባቢው ደረጃ ውስጥ ናቸው፡፡ Therefore, reducing the vocabulary size should have an important impact on the total number of parameters.  በዚህ ፕሮግራም፣ በተመሳሳይ ኮርፖርት ብዛት ጥቂት ቋንቋዎችን የሚቆጥሩትን ትንሽ ሞዴላዎችን ለማውጣት እናዘጋጅታለን፡፡ በXNLI ዳታዎች ላይ የበዛ ቋንቋዎች የBERT ትንሽ ሽፋን እናሳውቃለን፤ ግን ይህ ሥርዓት በሌላ ብልቋንቋ ለውጦች እንዲጠቀም እናምናለን፡፡ አግኝቷል ፍሬዎችን የሚያስተካክል ፍሬዎችን የሚያደርግ ትንሹ ሞዴላዎችን እናደርጋለን፡፡ እና ምሳሌዎቻችንን ከDistilmBERT (ለብዙልቋንቋ BERT ክፍል) በተለየ ቋንቋ አነሳቅስ፣ ልዩነት በXNLI ዳታ ማሰናከል ሙሉ ቁጥር 1.7 በመቶ 6 በመቶ ያፈስሳል፡፡ የተጠቃሚ ሞዴል እና ኮድ በሙሉ የተገኘ ነው።', 'az': 'Əvvəlcə təhsil edilmiş transformer-tabanlı modelleri təbiətli dil işləmə məlumatlarının müxtəlif növbətlərinə müxtəlif təhsil müəyyənləşdirirlər. Ancaq bu modellərin böyüklüyü həqiqət ürək proqramlarında istifadə etmək üçün çox sıxıntıdır. Çoxlu dil modellərin vaxtında, parametrlərin əksəriyyəti inbing səviyyəsində yerlənir. Beləliklə, sözlərin böyüklüyünü azaltmaq parametroların toplam sayısına möhüm təsiri olmalı. Bu kağızda, məqsəd korporasına görə daha az dilləri istifadə edən kiçik modelləri çıxartmağı təklif edirik. XNLI verilər qutusunda çoxlu dilli BERT verzijlərinin daha kiçik qiymətlərini göstəririk, amma biz bu metodu başqa çoxlu dil transformatçılara uyğulana bilər. Qazanmış sonuçlar təsdiqləyir ki, biz salışılabilir sonuçları saxlayan kiçik modelləri yaratabiliriz, tamamilə parametrlərin sayını 45%-ə düşürür. Biz modellərimizi DistilmBERT ilə (çoxlu dilli BERT verziji ilə müxtəlif kimi) saldıq və dillərin azalmasına bənzər, distillasyon XNLI verilən qutusundakı bütün doğruluqlarını 1,7%-ə 6%-ə düşürdü. Göndərilən modellər və kodlar açıq-aşkar mövcuddur.', 'fa': 'مدل\u200cهای پیش\u200cآموزش\u200cشده\u200cی تغییر\u200cپذیر\u200cکننده\u200cها به نتیجه\u200cهای موقعیت هنری در مجموعه\u200cهای داده\u200cهای پرداخت زبان طبیعی رسیده\u200cاند. ولی اندازه این مدلها اغلب یک بازگشت برای استفاده کردن آنها در کاربردهای واقعی تولید است. در مورد مدل\u200cهای زیادی زبان بیشتر پارامتر در طبقه\u200cای استفاده می\u200cشوند. بنابراین، کاهش اندازه کلمات باید تاثیر مهم بر تعداد کلمات پارامتر داشته باشد. در این کاغذ، ما پیشنهاد می\u200cکنیم که مدل کوچک\u200cتر را که تعداد زبان کمتری را بر اساس شرکت هدف می\u200cکند استخراج کنیم. ما ارزیابی نسخه\u200cهای کوچکتر از BERT چندین زبان را در مجموعه داده\u200cهای XNLI پیشنهاد می\u200cکنیم، اما ما باور می\u200cکنیم که این روش ممکن است برای تغییردهندگان چندین زبان کاربرد شود. نتایج یافته تایید می\u200cکند که ما می\u200cتوانیم مدل\u200cهای کوچکتر را تولید کنیم که نتایج قابل مقایسه نگه می\u200cدارند، در حالی که تا 45% از تعداد عمومی پارامتر کاهش می\u200cدهیم. ما مدلهای ما را با DistilmBERT (نسخه متفاوت از BERT multilingual) مقایسه کردیم و نشان دادیم که به مخالف کاهش زبان، متفاوت یک کاهش ۱.۷% به ۶% در جمعیت دقیق در مجموعه داده های XNLI رخ داد. مدل و کد پیشنهاد به طور عمومی دسترسی دارند.', 'bn': 'প্রাক্তন প্রশিক্ষিত ট্রান্সফার্ন ভিত্তিক মডেল বিভিন্ন ধরনের প্রাকৃতিক ভাষা প্রক্রিয়া ডাটা সেটের উপর রাষ্ট্র-অফ- তবে এই মডেলের আকার প্রায়শই বাস্তব প্রযুক্তি অ্যাপ্লিকেশনে তাদের প্রস্তুতি প্রদানের জন্য ড্রাফিকের প বহুভাষার মডেলের ক্ষেত্রে বেশীরভাগ প্যারামিটার স্তরে অবস্থিত। সুতরাং শব্দভাণ্ডারের আকার কমানোর সংখ্যার মোট সংখ্যার উপর গুরুত্বপূর্ণ প্রভাব ফেলবে। এই কাগজটিতে আমরা সামান্য মডেল বের করার প্রস্তাব দিচ্ছি যারা লক্ষ্য কর্পোরার অনুসারে সংখ্যার কয়েকটি ভাষা নিয়ে  আমরা এক্সএনলির তথ্য সেটে বহুভাষার বিবেরেটের ছোট সংস্করণের মূল্য উপস্থাপন করি, কিন্তু আমরা বিশ্বাস করি এই পদ্ধতি অন্যান্য বহুভাষায় রান্ন অর্জনের ফলাফল নিশ্চিত করে যে আমরা সামান্য মডেল তৈরি করতে পারি যারা তুলনায় ফলাফল রাখতে পারে, আর মোট পরামিটারের মধ্যে ৪৫% কমে যায়। আমরা ডিস্টিলিমবের্টের সাথে আমাদের মডেলের তুলনা করেছি (মাল্টিভাষার বিবেরেটের একটি বিচ্ছিন্ন সংস্করণ) এবং দেখিয়েছি যে ভাষার সংক্রান্ত ভাষার সংক্রান্ত প্রদত্ত মডেল এবং কোড প্রকাশ্যে উপস্থিত।', 'bs': 'Preobučeni modeli na transformeru postižu rezultate umjetnosti na raznim setima podataka o procesu prirodnog jezika. Međutim, veličin a tih modela je često odvraćanje za njihovu rasporedu u pravim proizvodnjim aplikacijama. U slučaju multijezičkih modela, većina parametara se nalazi u sloju ugrađenja. Stoga smanjenje veličine riječnika treba imati važan uticaj na ukupni broj parametara. U ovom papiru predlažemo izvući manje modele koji se bave manjim brojem jezika prema ciljanoj korpori. Predstavljamo procjenu manjih verzija multijezičkih BERT-a na setu XNLI podataka, ali vjerujemo da se ovaj metod može primjenjivati drugim multijezičkim transformatorima. Pronađeni rezultati potvrđuju da možemo stvoriti manje modele koji održavaju usporedbene rezultate, dok smanjujemo do 45% ukupnog broja parametara. Usporedili smo naše modele sa DistilmBERT (destilirana verzija multijezičkog BERT) i pokazali da, za razliku od smanjenja jezika, destilacija je izazvala 1,7% na 6% pad ukupne tačnosti na setu XNLI podataka. Predstavljeni modeli i kod su javno dostupni.', 'ca': "Models pré-entrenats basats en transformadors estan aconseguint resultats més avançats en una varietat de conjunts de dades de processament de llenguatges naturals. No obstant això, la mida d'aquests models sovint és un inconvenient per a la seva implantació en aplicacions de producció reals. En el cas de models multilingües, la majoria dels paràmetres es troben a la capa d'incorporació. Per tant, reduir la mida del vocabulari hauria de tenir un impacte important en el nombre total de paràmetres. En aquest paper, proposem extrair models més petits que manejen menys llengües segons el corpora mirat. Presentam una evaluació de versions més petites de BERT multilingüe en el conjunt de dades XNLI, però creiem que aquest mètode pot ser aplicat a altres transformadors multilingües. Els resultats obtenits confirmen que podem generar models més petits que mantenen resultats comparables, mentre redueixen fins al 45% del nombre total de paràmetres. Vam comparar els nostres models amb DistilmBERT (una versió destilada de BERT multillengua) i vam demostrar que a diferència de la reducció del llenguatge, la destilació va induir una baixa del 1,7% al 6% de la precisió global del conjunt de dades XNLI. Els models i codis presentats estan a disposició del públic.", 'et': 'Eelkoolitud Transformeritel põhinevad mudelid saavutavad tipptasemel tulemusi mitmesuguste looduskeele töötlemise andmekogumitega. Kuid nende mudelite suurus on sageli puuduseks nende kasutuselevõtule reaalsetes tootmisrakendustes. Mitmekeelsete mudelite puhul asub enamik parameetreid manustamiskihis. Seetõttu peaks sõnavara suuruse vähendamine oluliselt mõjutama parameetrite koguarvu. Käesolevas töös teeme ettepaneku ekstraheerida väiksemaid mudeleid, mis käsitlevad vähem keeli vastavalt sihtotstarbelistele korpustele. Esitame XNLI andmekogumi väiksemate mitmekeelsete BERT versioonide hindamise, kuid usume, et seda meetodit võib rakendada ka teiste mitmekeelsete transformaatorite puhul. Saadud tulemused kinnitavad, et saame luua väiksemaid mudeleid, mis säilitavad võrreldavad tulemused, vähendades samal ajal kuni 45% parameetrite koguarvust. Me võrdlesime oma mudeleid DistilmBERTiga (mitmekeelse BERTi destilleeritud versioon) ja näitasime, et erinevalt keele vähendamisest põhjustas destilleerimine XNLI andmekogumi üldise täpsuse languse 1,7–6%. Esitatud mudelid ja kood on avalikult kättesaadavad.', 'cs': 'Předškolené modely založené na transformátoru dosahují nejmodernějších výsledků na různých datových sadách zpracování přirozeného jazyka. Velikost těchto modelů je však často nevýhodou pro jejich nasazení v reálných výrobních aplikacích. V případě vícejazyčných modelů se většina parametrů nachází ve vrstvě vložení. Snížení velikosti slovní zásoby by proto mělo mít významný dopad na celkový počet parametrů. V tomto článku navrhujeme extrahovat menší modely, které zvládají menší počet jazyků podle cílových korpusů. Představujeme vyhodnocení menších verzí vícejazyčného BERT na datové sadě XNLI, ale věříme, že tato metoda lze aplikovat i na další vícejazyčné transformátory. Získané výsledky potvrzují, že můžeme generovat menší modely, které udržují srovnatelné výsledky, přičemž snižují až 45% celkového počtu parametrů. Porovnali jsme naše modely s DistilmBERT (destilovanou verzí vícejazyčného BERT) a ukázali, že na rozdíl od jazykové redukce, destilace indukovala 1,7% až 6% pokles celkové přesnosti XNLI datové sady. Prezentované modely a kód jsou veřejně dostupné.', 'fi': 'Esikoulutetut Transformer-pohjaiset mallit saavuttavat huippuluokan tuloksia erilaisissa Natural Language Processing -tietosarjoissa. Näiden mallien koko on kuitenkin usein haittapuoli niiden käyttöönotolle todellisissa tuotantosovelluksissa. Monikielisten mallien tapauksessa suurin osa parametreista sijaitsee upotustasolla. Siksi sanaston koon pienentämisellä pitäisi olla merkittävä vaikutus parametrien kokonaismäärään. Tässä työssä ehdotamme pienempiä malleja, jotka käsittelevät vähemmän kieliä kohdekorpusten mukaan. Esitämme arvion monikielisen BERT:n pienemmistä versioista XNLI-aineistossa, mutta uskomme, että tätä menetelmää voidaan soveltaa muihin monikielisiin muuntajiin. Saadut tulokset vahvistavat, että pystymme tuottamaan pienempiä malleja, jotka säilyttävät vertailukelpoiset tulokset ja vähentämään jopa 45% parametrien kokonaismäärästä. Vertasimme malliamme DistilmBERT:iin (monikielisen BERT:n tislattu versio) ja osoittimme, että toisin kuin kielen pelkistys, tislaus aiheutti 1,7–6% laskua XNLI-aineiston kokonaistarkkuussa. Esitetyt mallit ja koodi ovat julkisesti saatavilla.', 'hy': 'Նախապատրաստված տրանսֆերմերների հիմնված մոդելները հասնում են ամենաբարձր արդյունքներ բնական լեզվի վերաբերյալ տվյալների բազմաթիվ համակարգերի վրա: Այնուամենայնիվ, այս մոդելների չափսը հաճախ հանդիսանում է իրենց կիրառումը իրական արտադրման ծրագրերում: In the case of multilingual models, most of the parameters are located in the embeddings layer.  Այսպիսով, բառարանի չափսի կրճատմանը պետք է կարևոր ազդեցություն ունենա պարամետրերի ընդհանուր քանակի վրա: Այս թղթի մեջ մենք առաջարկում ենք վերցնել փոքր մոդելներ, որոնք ավելի քիչ լեզուներ են վարվում ըստ նպատակային մարմնի: We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers.  Ընդհանուր արդյունքները հաստատում են, որ մենք կարող ենք ստեղծել ավելի փոքր մոդելներ, որոնք պահպանում են համեմատական արդյունքներ, մինչդեռ նվազեցնում են մինչև 45 տոկոսը պարամետրերի ընդհանուր քանակից: Մենք համեմատեցինք մեր մոդելները Դիստիլմբերտի հետ և ցույց տվեցինք, որ ի տարբերություն լեզվի կրճատման, դիստիլլացիան առաջացրեց XNSI տվյալների համաշխարհային ճշգրտության 1.7-6 տոկոսի կրճատման: Պատրաստված մոդելները և կոդը հանրային հասանելի են:', 'jv': 'Laptop" and "Desktop Nanging, ukuran model iki dadi nyimpen kanggo nggawe program sing apik banget nggo ngono nggawe Text editor echoH e l l o space w o r l d periodHelloworldHello world Nanging kuwi iki, kita ndhukung nggampang model sing nyenyane ngomong kudu tindang ing pisan banget supoyo supoyo uwong Awak dhéwé éntukno asseksi kanggo bersi sing nyenyang liyane BERT kanggo nggawe dadi XNLI, sane awak dhéwé ngrusak kuwi method iki dadi bisa aplikasi kanggo transformer multilanggar. Laptop" and "Desktop Awak dhéwé nggawe modelan karo DistilmBERT Laptop" and "Desktop', 'he': 'מודלים מבוססים בטרנספורטרים מאומנים מראש משיגים תוצאות חדשות על מגוון של קבוצות מידע מעבדת שפת טבעית. עם זאת, הגודל של הדוגמנים האלה הוא לעתים קרובות חסר תועלת לשימוש שלהם בתוכניות יצירה אמיתיות. במקרה של דוגמנים רבות שפות, רוב הפרמטרים נמצאים בשכבת ההכנות. לכן, להפחית את גודל המילים צריך להיות השפעה חשובה על מספר הכולל של פרמטרים. בעיתון הזה, אנו מציעים לחלץ דוגמנים קטנים יותר שמטפלים במספר פחות שפות לפי גופורה המטרה. אנו מציגים עריכה של גרסאות קטנות יותר של BERT רבות שפות על קבוצת הנתונים XNLI, אבל אנו מאמינים ששיטה זו יכולה להיות שימוש על משתנים רבות שפות אחרים. התוצאות הנקבלות מאשרות שאנחנו יכולים ליצור דוגמנים קטנים שמשמרים תוצאות שוויות, בעוד להפחית עד 45% ממספר המספר הכולל של פרמטרים. השוונו את הדוגמנים שלנו עם דיסטילמברט (גרסה מושלמת של BERT רב-שפתי) והראינו כי בניגוד להפחית שפת, הדיסיל גרם לרדת 1.7% עד 6% בדיקת הכללית על קבוצת הנתונים XNLI. הדוגמנים והקוד המוציאים זמינים לציבור.', 'ha': "@ action: button Amma, girmar waɗannan misãlai ko da yawa, za'a zama mai amfani da su a cikin shiryoyin ayuka masu shiryuwa. In case of misãlai masu multi-lingui, mafi yawan parameter za'a ƙayyade cikin layin da aka shigar. Dõmin haka, ƙaranci girmar maganar, ya kamata ta da wani mai muhimu kan yawan parameteri. Daga wannan takardan, Munã bukãta ka sami misãlai ƙarami wanda ke samun wasu harshen da ke ƙaranci daidai da makampuni da aka yi amfani da shi. Tuna ƙidãya masu ƙaranci na BERT masu cikin mulki-lingui da ke daidaita data na XNLI, kuma amma munã aminci cẽwa, za'a amfani da wannan hanyor da ke cikin wasu mutane na'ura. Ana gaskata matsala ya gaskata cewa za'a iya ƙiƙira motsi masu ƙarami wanda ke tsara matsalan daidai, kuma a ƙara zuwa asilimin 45 cikin jumla parameteri. Mun samfani misalinmu da DistilmBERT (wani daraja na'urar BERT na da mulki-linguin) kuma muka nuna cewa, ƙaramin harshen da ba'a daidaita ba, rarrabẽwa ya ƙara 1.7% zuwa 6% duk cikin tsarin da aka daidaita data na XNLI. Ana sami motsi da kodi da aka bayyana.", 'sk': 'Predhodno usposobljeni modeli na podlagi transformatorjev dosegajo najsodobnejše rezultate na različnih naborih podatkov za obdelavo naravnega jezika. Vendar pa je velikost teh modelov pogosto pomanjkljivost za njihovo uvajanje v realnih proizvodnih aplikacijah. V primeru večjezičnih modelov se večina parametrov nahaja v plasti vdelav. Zato bi moralo zmanjšanje velikosti besedišča pomembno vplivati na skupno število parametrov. V tem prispevku predlagamo izvleči manjše modele, ki obravnavajo manj jezikov glede na ciljne korpuse. Predstavljamo oceno manjših različic večjezičnega BERT na podatkovnem nizu XNLI, vendar menimo, da se ta metoda lahko uporablja za druge večjezične transformatorje. Dobljeni rezultati potrjujejo, da lahko ustvarimo manjše modele, ki ohranjajo primerljive rezultate, hkrati pa zmanjšujejo do 45% skupnega števila parametrov. Naše modele smo primerjali z DistilmBERT (destilirano različico večjezičnega BERT) in pokazali, da je za razliko od jezikovnega redukcije destilacija povzročila 1,7% do 6% padec splošne natančnosti na naboru podatkov XNLI. Predstavljeni modeli in koda so javno dostopni.', 'bo': 'སྔོན་གྲངས་བསྐུལ་ནས་བཟོ་བཅོས་བྱེད་མཁན་གྱི་མིག ཡིན་ནའང་། མིག་རྩལ་འདི་དག་གི་ཆེ་ཆུང་དེ་རང་ཉིད་ཀྱི་ལས་རིམ་སྤྲོད་ཀྱི་ཉེར་སྤྱོད་དེ་དང་། In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. འུ་ཅག་གི་ཤོག་བུ་འདིའི་ནང་དུ་དམིགས་འབེབས་བྱས་པའི་སྐད་ཡིག་ཆ་ཉུང་བའི་མིང་དཔེ་གཏོང་པར་སྤྲོད་ཡོད། We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method can be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. སྔོན་སྟོན་པའི་མིག་དཔེ་དང་ཨང་རིས་མང་ཆོས་སྤྱོད་ཐུབ་པ'}
{'en': 'Towards Accurate and Reliable Energy Measurement of NLP Models', 'ar': 'نحو قياس دقيق وموثوق للطاقة لنماذج البرمجة اللغوية العصبية', 'es': 'Hacia una medición energética precisa y fiable de los modelos de PNL', 'fr': "Vers une mesure d'énergie précise et fiable des modèles de PNL", 'pt': 'Em direção à medição de energia precisa e confiável de modelos de PNL', 'ja': 'NLPモデルの正確で信頼できるエネルギー測定に向けて', 'ru': 'К Точному и Надежному Измерению Энергии Моделей NLP', 'zh': '成NLP之正,可量也', 'hi': 'एनएलपी मॉडल के सटीक और विश्वसनीय ऊर्जा मापन की ओर', 'ga': 'I dTreo Fuinneamh Cruinn agus Iontaofa a Thomhas Múnlaí NLP', 'ka': 'NLP მოდელების წარმოადგილი და შესაძლებელი ენერგიის მოზომილება', 'el': 'Προς την ακριβή και αξιόπιστη ενεργειακή μέτρηση των μοντέλων ΝΛΠ', 'hu': 'Az NLP modellek pontos és megbízható energiamérése felé', 'it': "Verso una misurazione accurata e affidabile dell'energia dei modelli PNL", 'kk': 'NLP үлгілерінің нақты және сенімді энергия өлшеміне қарсы', 'lt': 'Tinkamo ir patikimo NLP modelių energijos matavimo link', 'mk': 'За точна и доверлива мерка на енергијата на моделите на НЛП', 'ms': 'Ke arah Keukuran tenaga yang tepat dan boleh dipercayai bagi Model NLP', 'ml': 'NLP മോഡലുകളുടെ കണക്കിലേക്കും വിശ്വാസമുള്ള ഊര്\u200dജ്ജി അളവു്', 'mt': 'Lejn Kejl tal-Enerġija Aċiż u Fidabbli tal-Mudelli NLP', 'mn': 'NLP загварын тодорхой, итгэлтэй эрчим хүчний хэмжээнд', 'pl': 'W kierunku dokładnego i niezawodnego pomiaru energii modeli NLP', 'ro': 'Către măsurarea precisă și fiabilă a energiei modelelor PNL', 'sr': 'Do tačne i pouzdane energetske mjere NLP modela', 'si': 'NLP මොඩේල්ගේ සිද්ධ සහ විශ්වාසිත ශක්තිමත් විශ්වාස කරන්න පුළුවන්', 'so': 'Measures of NLP Models', 'sv': 'Mot noggrann och tillförlitlig energimätning av NLP-modeller', 'ta': 'NLP மாதிரிகளின் கணக்கு மற்றும் செயல்படுத்தக்கூடிய சக்தி அளவு', 'ur': 'NLP موڈلوں کی دقیق اور قابل اعتماد انرژی اندازے کی طرف', 'no': 'NLP-modeller', 'uz': 'Comment', 'vi': 'Hướng tới biện pháp tích chính xác và đáng tin cậy về năng lượng NLP.', 'da': 'På vej mod nøjagtig og pålidelig energimåling af NLP-modeller', 'bg': 'Към точното и надеждно измерване на енергията на моделите на НЛО', 'hr': 'prema točnoj i pouzdanoj mjeri energije modela NLP-a', 'nl': 'Naar nauwkeurige en betrouwbare energiemeting van NLP-modellen', 'de': 'Auf dem Weg zu präziser und zuverlässiger Energiemessung von NLP-Modellen', 'ko': 'NLP 모형의 정확하고 신뢰할 수 있는 에너지 측정', 'fa': 'به سمت اندازه انرژی دقیق و قابل اعتماد مدل NLP', 'id': 'Ke Ukuran Energi Tepat dan Dapat Dipercayai Model NLP', 'sw': 'Towards Accurate and Reliable Energy Measurement of NLP Models', 'af': 'Gaan na Accurate and Reliable Energy Measurement of NLP Models', 'am': 'image-action', 'hy': 'ՆԼՊ մոդելների ճշգրիտ և վստահելի էներգիայի չափման ուղղությամբ', 'tr': 'NLP Modelleriniň Dyggat we Maglumatly Energiň ölçüsi', 'sq': 'Për matjen e saktë dhe të besueshme të energjisë të modeleve NLP', 'az': 'NLP Modellərinin doğru və güvenilir enerji ölçülərinə tərəf', 'bn': 'NLP মোডেলের মাপ পরিমাপ', 'bs': 'Prema točnoj i pouzdanoj mjeri energije NLP modela', 'et': 'NLP mudelite täpse ja usaldusväärse energiamõõtmise suunas', 'ca': "Vers la mesura exacta i fiable de l'energia dels models NLP", 'cs': 'K přesnému a spolehlivému měření energie NLP modelů', 'fi': 'Kohti NLP-mallien tarkkaa ja luotettavaa energianmittausta', 'he': 'לעבור למדידת אנרגיה מדויקת ומאמינה של מודלים NLP', 'ha': '@ action', 'jv': 'directions', 'bo': 'NLP མ་དབྱིབས་ཀྱི་ཚད་བདེ་སུ་དང་མཐུན་རུང་བའི་ནུས་པས་ཚད་བཅས་', 'sk': 'Za natančno in zanesljivo merjenje energije modelov NLP'}
{'en': 'Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.', 'ar': 'يعد القياس الدقيق والموثوق لاستهلاك الطاقة أمرًا بالغ الأهمية لاتخاذ خيارات تصميم مستنيرة عند اختيار وتدريب نماذج البرمجة اللغوية العصبية على نطاق واسع. في هذا العمل ، نوضح أن تقديرات الطاقة الحالية المستندة إلى البرامج ليست دقيقة لأنها لا تأخذ في الاعتبار اختلافات الأجهزة وكيف يؤثر استخدام الموارد على استهلاك الطاقة. نجري تجارب قياس الطاقة بأربعة نماذج مختلفة لمهمة الإجابة على السؤال. نحدد خطأ تقديرات الطاقة الحالية المستندة إلى البرامج باستخدام مقياس طاقة الأجهزة الذي يوفر قياسات طاقة عالية الدقة. إن الوجبات الجاهزة الرئيسية لدينا هي الحاجة إلى نموذج أكثر دقة لتقدير الطاقة يأخذ في الاعتبار متغيرات الأجهزة والعلاقة غير الخطية بين استخدام الموارد واستهلاك الطاقة. نصدر الكود والبيانات على https://github.com/csarron/sustainlp2020-energy.', 'es': 'La medición precisa y confiable del consumo de energía es fundamental para tomar decisiones de diseño bien informadas al elegir y entrenar modelos de PNL a gran escala. En este trabajo, mostramos que las estimaciones de energía basadas en software existentes no son precisas porque no tienen en cuenta las diferencias de hardware y cómo la utilización de los recursos afecta el consumo de energía. Llevamos a cabo experimentos de medición de energía con cuatro modelos diferentes para una tarea de respuesta a preguntas. Cuantificamos el error de las estimaciones de energía existentes basadas en software mediante el uso de un medidor de potencia de hardware que proporciona mediciones de energía de alta precisión. Nuestra conclusión clave es la necesidad de un modelo de estimación de energía más preciso que tenga en cuenta las variabilidades del hardware y la relación no lineal entre la utilización de los recursos y el consumo de energía. Publicamos el código y los datos en https://github.com/csarron/sustainlp2020-energy.', 'fr': "Une mesure précise et fiable de la consommation d'énergie est essentielle pour faire des choix de conception éclairés lors du choix et de la formation de modèles NLP à grande échelle. Dans ce travail, nous montrons que les estimations d'énergie existantes basées sur des logiciels ne sont pas précises car elles ne tiennent pas compte des différences matérielles et de la façon dont l'utilisation des ressources affecte la consommation d'énergie. Nous menons des expériences de mesure de l'énergie avec quatre modèles différents pour répondre à une question. Nous quantifions l'erreur des estimations d'énergie basées sur des logiciels existants à l'aide d'un compteur de puissance matériel qui fournit des mesures d'énergie très précises. Notre principal point à retenir est la nécessité d'un modèle d'estimation de l'énergie plus précis qui tienne compte des variabilités matérielles et de la relation non linéaire entre l'utilisation des ressources et la consommation d'énergie. Nous publions le code et les données sur https://github.com/csarron/sustainlp2020-energy.", 'pt': 'A medição precisa e confiável do consumo de energia é fundamental para fazer escolhas de projeto bem informadas ao escolher e treinar modelos de PNL de grande escala. Neste trabalho, mostramos que as estimativas de energia baseadas em software existentes não são precisas porque não levam em consideração as diferenças de hardware e como a utilização de recursos afeta o consumo de energia. Conduzimos experimentos de medição de energia com quatro modelos diferentes para uma tarefa de resposta a perguntas. Quantificamos o erro das estimativas de energia baseadas em software existentes usando um medidor de energia de hardware que fornece medições de energia altamente precisas. Nossa principal lição é a necessidade de um modelo de estimativa de energia mais preciso que leve em consideração as variabilidades de hardware e a relação não linear entre a utilização de recursos e o consumo de energia. Liberamos o código e os dados em https://github.com/csarron/sustainlp2020-energy.', 'ja': 'エネルギー消費量の正確で信頼性の高い測定は、大規模なNLPモデルを選択およびトレーニングする際に、十分に情報に基づいた設計選択を行うために重要です。この研究では、既存のソフトウェアベースのエネルギー推定は、ハードウェアの違いと資源利用がエネルギー消費にどのように影響するかを考慮していないため、正確ではないことを示しています。質問応答タスクのために、4つの異なるモデルでエネルギー測定実験を行います。高精度のエネルギー測定を提供するハードウェア電力計を使用して、既存のソフトウェアベースのエネルギー推定の誤差を定量化します。当社の重要なポイントは、ハードウェアの変動と資源利用とエネルギー消費の間の非線形関係を考慮した、より正確なエネルギー推定モデルの必要性です。コードとデータはhttps://github.com/csarron/sustainlp2020-energyで公開されています。', 'zh': '大NLP教训,审能耗度智。 于此之中,明软件之量不正,盖不虑硬件异及资源利用率何以及能源消耗。 四者量实验,以问其务。 用度之硬件功率量化有软件之差。 凡我大要,须一量估模,当硬件可变性及资源利用率与能耗非线性关系。 以 https://github.com/csarron/sustainlp2020-energy 发代码数。', 'ru': 'Точное и надежное измерение энергопотребления имеет решающее значение для принятия хорошо информированных проектных решений при выборе и обучении крупномасштабных моделей NLP. В этой работе мы показываем, что существующие программные оценки энергии не являются точными, поскольку они не учитывают аппаратные различия и то, как использование ресурсов влияет на потребление энергии. Мы проводим эксперименты по измерению энергии с четырьмя различными моделями для ответа на вопрос. Мы количественно оцениваем погрешность существующих программных оценок энергии с помощью аппаратного измерителя мощности, который обеспечивает высокоточные измерения энергии. Нашим ключевым выводом является необходимость в более точной модели оценки энергии, которая учитывает вариабельность аппаратного обеспечения и нелинейную связь между использованием ресурсов и потреблением энергии. Мы публикуем код и данные по адресу https://github.com/csarron/sustainlp2020-energy.', 'hi': 'ऊर्जा की खपत का सटीक और विश्वसनीय माप अच्छी तरह से सूचित डिजाइन विकल्प बनाने के लिए महत्वपूर्ण है जब बड़े पैमाने पर एनएलपी मॉडल का चयन और प्रशिक्षण होता है। इस काम में, हम दिखाते हैं कि मौजूदा सॉफ़्टवेयर-आधारित ऊर्जा अनुमान सटीक नहीं हैं क्योंकि वे हार्डवेयर मतभेदों को ध्यान में नहीं रखते हैं और संसाधन उपयोग ऊर्जा की खपत को कैसे प्रभावित करता है। हम एक प्रश्न का उत्तर देने वाले कार्य के लिए चार अलग-अलग मॉडलों के साथ ऊर्जा माप प्रयोग करते हैं। हम एक हार्डवेयर पावर मीटर का उपयोग करके मौजूदा सॉफ़्टवेयर-आधारित ऊर्जा अनुमानों की त्रुटि को मापते हैं जो अत्यधिक सटीक ऊर्जा माप प्रदान करता है। हमारी प्रमुख टेकअवे एक अधिक सटीक ऊर्जा अनुमान मॉडल की आवश्यकता है जो हार्डवेयर वैरिएबिलिटी और संसाधन उपयोग और ऊर्जा खपत के बीच गैर-रैखिक संबंध को ध्यान में रखती है। हम https://github.com/csarron/sustainlp2020-energy पर कोड और डेटा जारी करते हैं।', 'ga': 'Tá tomhas cruinn agus iontaofa ar ídiú fuinnimh ríthábhachtach chun roghanna deartha eolasacha a dhéanamh agus samhlacha NLP ar mhórscála á roghnú agus á dtraenáil. San obair seo, léirímid nach bhfuil na meastacháin fuinnimh atá bunaithe ar bhogearraí atá ann cheana féin cruinn toisc nach gcuireann siad difríochtaí crua-earraí san áireamh agus an tionchar a bhíonn ag úsáid acmhainní ar ídiú fuinnimh. Déanaimid turgnaimh tomhais fuinnimh le ceithre mhúnla dhifriúla le haghaidh tasc freagartha ceiste. Déanaimid earráid na meastachán fuinnimh bogearraí-bhunaithe atá ann cheana a chainníochtú trí úsáid a bhaint as méadar cumhachta crua-earraí a sholáthraíonn tomhais fuinnimh an-chruinn. Is é an rud is tábhachtaí atá againn ná an gá atá le múnla meastacháin fuinnimh níos cruinne a chuireann san áireamh éagsúlachtaí crua-earraí agus an gaol neamhlíneach idir úsáid acmhainní agus ídiú fuinnimh. Eisímid an cód agus na sonraí ag https://github.com/csarron/sustainlp2020-energy.', 'hu': 'Az energiafogyasztás pontos és megbízható mérése kulcsfontosságú ahhoz, hogy jól tájékozott tervezési döntéseket hozzon a nagyméretű NLP modellek kiválasztásakor és képzésekor. Ebben a munkában megmutatjuk, hogy a meglévő szoftver alapú energiabebecslések nem pontosak, mert nem veszik figyelembe a hardveres különbségeket és az erőforrás-felhasználás hogyan befolyásolja az energiafogyasztást. Energiamérési kísérleteket végzünk négy különböző modellel egy kérdésre válaszoló feladathoz. Számszerűsítjük a meglévő szoftver alapú energiafelhasználások hibáját olyan hardveres teljesítménymérővel, amely rendkívül pontos energiafelhasználást biztosít. A legfontosabb feladatunk egy pontosabb energiafogyasztási modell szükségessége, amely figyelembe veszi a hardverváltozásokat, valamint az erőforrás-felhasználás és az energiafogyasztás nemlineáris kapcsolatát. Kiadjuk a kódot és az adatokat a https://github.com/csarron/sustainlp2020-energy.', 'el': 'Η ακριβής και αξιόπιστη μέτρηση της κατανάλωσης ενέργειας είναι κρίσιμη για τη λήψη καλά ενημερωμένων σχεδιαστικών επιλογών κατά την επιλογή και εκπαίδευση μεγάλων προτύπων. Σε αυτή την εργασία, καταδεικνύουμε ότι οι υφιστάμενες ενεργειακές εκτιμήσεις που βασίζονται στο λογισμικό δεν είναι ακριβείς επειδή δεν λαμβάνουν υπόψη τις διαφορές υλικού και τον τρόπο με τον οποίο η χρήση πόρων επηρεάζει την κατανάλωση ενέργειας. Διεξάγουμε πειράματα ενεργειακής μέτρησης με τέσσερα διαφορετικά μοντέλα για μια εργασία απάντησης σε ερωτήσεις. Καταγράφουμε ποσοτικά το σφάλμα των υφιστάμενων ενεργειακών εκτιμήσεων βασισμένων στο λογισμικό χρησιμοποιώντας έναν μετρητή ισχύος υλικού που παρέχει εξαιρετικά ακριβείς ενεργειακές μετρήσεις. Το βασικό μας στοιχείο είναι η ανάγκη για ένα ακριβέστερο μοντέλο εκτίμησης ενέργειας που λαμβάνει υπόψη τις μεταβλητές υλικού και τη μη γραμμική σχέση μεταξύ χρήσης πόρων και κατανάλωσης ενέργειας. Απελευθερώνουμε τον κωδικό και τα δεδομένα στο https://github.com/csarron/sustainlp2020-energy.', 'ka': 'ენერგიის გამოყენება კრიტიკური და დარწმუნებელი განზომილება, როდესაც ამოირჩევა და განსწავლა დიდი განზომილები NLP მოდელების შესაძლებელად კრიტიკურია. ამ სამუშაოში ჩვენ ჩვენ ჩვენ აჩვენებთ, რომ პროგრამის განსაზღვრება ენერგიის განსაზღვრება არ არის მარტივია, რადგან ისინი არ აჩვენებენ ჰაპეტერული განსხვავებების და როგორ რეს ჩვენ ენერგიის განზომილების ექსპერიმენტები გავაკეთებთ 4 განსხვავებული მოდელებით კითხვების დასაუმატებისთვის. ჩვენ კონტაქტირებთ პროგრამეტური ენერგიის განსაზღვრების შეცდომა, რომელიც მარტივი ენერგიის განსაზღვრების გამოყენებით. ჩვენი გასაღები არის უფრო მარტივი ენერგიის მოდულისთვის მოდულისთვის, რომელიც აღმოჩნდება ჰაპექტურის გარიგებულობები და ბოლონური შესახებ რესურსის გამოყენება და ენერგიის გამოყ ჩვენ კოდის და მონაცემების გახსნა https://github.com/csarron/sustainlp2020-energy.', 'it': "La misurazione accurata e affidabile del consumo energetico è fondamentale per fare scelte progettuali ben informate nella scelta e nella formazione di modelli NLP su larga scala. In questo lavoro, mostriamo che le stime energetiche basate su software esistenti non sono accurate perché non tengono conto delle differenze hardware e di come l'utilizzo delle risorse influisce sul consumo energetico. Conduciamo esperimenti di misurazione dell'energia con quattro diversi modelli per un compito di risposta alle domande. Quantifichiamo l'errore delle stime energetiche basate su software esistenti utilizzando un misuratore di potenza hardware che fornisce misurazioni energetiche altamente accurate. Il nostro punto di forza è la necessità di un modello di stima energetica più accurato che tenga conto delle variabili hardware e della relazione non lineare tra utilizzo delle risorse e consumo energetico. Rilasciamo il codice e i dati a https://github.com/csarron/sustainlp2020-energy.", 'kk': 'Энергия пайдалануының дұрыс және сенімді өлшемі NLP үлгілерін таңдағанда жақсы мәліметті дизайнды таңдау және оқыту үлкен. Бұл жұмыс ішінде бар бағдарлама негіздеген энергия бағалауы дұрыс емес, себебі олар жабдықтағы айырмашылықтарын және ресурс қолданудың энергия пайдалануына қалай әсер ететін нәрс Біз сұрақ жауап беретін тапсырманың төрт түрлі моделлерімен энергия өлшемінің тәжірибелерін жасаймыз. Біз бағдарлама негіздеген энергия бағаламаларының қатесін өте дұрыс енергия өлшемін беретін жабдық энергия метрін қолдануға арналған. Біздің кілтіміз - ресурс утилизациясы мен энергия пайдалануының арасындағы сызықты қатынасын есептеу үлгісі. Код мен деректерді https://github.com/csarron/sustainlp2020-energy.', 'lt': 'Tinkamas ir patikimas energijos suvartojimo matavimas yra labai svarbus gerai informuotiems projektavimo pasirinkimams pasirinkant ir rengiant didelio masto NLP modelius. Šiame darbe parodome, kad esami programinės įrangos pagrindu pagrįsti energijos vertinimai nėra tikslūs, nes jie neatsižvelgia į techninės įrangos skirtumus ir į tai, kaip išteklių naudojimas daro poveikį energijos suvartojimui. We conduct energy measurement experiments with four different models for a question answering task.  Apskaičiuojame esamų programinės įrangos pagrindu pagrįstų energijos apskaičiavimų klaidą naudojant techninį galios matuoklį, kuris suteikia labai tikslius energijos matavimus. Mūsų pagrindinis metodas yra tikslesnio energijos vertinimo modelio, kuriame būtų atsižvelgta į techninės įrangos skirtumus ir nelinijinį išteklių naudojimo ir energijos suvartojimo ryšį, poreikis. Mes išleidžiame kodą ir duomenis https://github.com/csarron/sustainlp2020-energy.', 'mk': 'Прецизното и доверливо мерење на потрошувачката на енергија е критично за правење добро информирани избори за дизајн кога се избираат и обучуваат големи модели на НЛП. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption.  Правиме експерименти за мерење на енергијата со четири различни модели за задача на одговор на прашања. Ја квантификуваме грешката на постоечките софтверски енергетски проценки со користење хардверски моќен метар кој обезбедува многу прецизни енергетски проценки. Нашиот клучен преземок е потребата од попрецизен модел за проценка на енергијата кој ги зема предвид варијабилностите на хардверот и нелинијарната врска помеѓу употребата на ресурсите и потрошувањето на енергијата. Ги пуштаме кодовите и податоците на https://github.com/csarron/sustainlp2020-energy.', 'mt': 'Il-kejl eżatt u affidabbli tal-konsum tal-enerġija huwa kritiku biex isiru għażliet ta’ disinn infurmati sew meta jintgħażlu u jitħarrġu mudelli NLP fuq skala kbira. F’dan ix-xogħol, naraw li l-istimi eżistenti tal-enerġija bbażati fuq is-softwer mhumiex preċiżi għaliex ma jqisux id-differenzi fil-ħardwer u kif l-użu tar-riżorsi jaffettwa l-konsum tal-enerġija. Għandna nagħmlu esperimenti ta’ kejl tal-enerġija b’erba’ mudelli differenti għal kompitu ta’ tweġiba għall-mistoqsijiet. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements.  It-teħid ewlieni tagħna huwa l-ħtieġa għal mudell ta’ stima tal-enerġija aktar preċiż li jqis il-varjabbiltajiet tal-hardware u r-relazzjoni mhux lineari bejn l-użu tar-riżorsi u l-konsum tal-enerġija. Aħna nħarġu l-kodiċi u d-dejta fuq https://github.com/csarron/sustainlp2020-energy.', 'mn': 'Энергийн хэрэглээний тодорхой, итгэлтэй хэмжээсүүд нь том хэмжээний NLP загваруудыг сонгоход сайн мэдээллийн дизайны сонголтыг хийх нь чухал. Энэ ажил дээр бид програм хангамжийн суурилсан энергийнхээ тооцоолол зөв биш. Учир нь тэд техник хангамжийн ялгааг харьцуулахгүй ба эрчим хүчний хэрэглээнд хэрхэн нөлөөлж байгааг харуулдаг. Бид эрчим хүчний хэмжээний туршилтыг асуултын хариултын даалгавартай 4 өөр загвартай хийдэг. Бид програм хангамжийн үндсэн энергийнхэмжээний алдааны хэмжээг маш нарийн энергийнхэмжээсүүдийг ашиглан тооцоолж байна. Бидний ач холбогдол бол энергийн тооцоололтын загвар юм. Энэ нь техник хангамжийн өөрчлөлтийг, нөөцийн хэрэглээ болон энергийн хэрэглээ хоорондын шулуун биш харилцааны шаардлагатай. Бид код болон өгөгдлийг https://github.com/csarron/sustainlp2020-energy.', 'ml': 'എംഎല്\u200dപി മോഡലുകള്\u200d തിരഞ്ഞെടുക്കുകയും പഠിപ്പിക്കുകയും ചെയ്യുമ്പോള്\u200d ഊര്\u200dജ്ജ ഉപകരണത്തിന്റെ വിശ്വാസവും അളവിലുള്ള അളവ്  ഈ പ്രവര്\u200dത്തനത്തില്\u200d, നിലവിലുള്ള സോഫ്റ്റ്\u200cവെയര്\u200d അടിസ്ഥാനമായ ഊര്\u200dജ്ജം കണക്കുകള്\u200d ശരിയല്ല എന്ന് ഞങ്ങള്\u200d കാണിക്കുന്നു. കാരണം അവര്\u200d ഹാര്\u200dഡ്വെയര ഒരു ചോദ്യം ഉത്തരം ചെയ്യുന്ന ജോലിക്ക് വേണ്ടി നാലു വ്യത്യസ്ത മോഡലുകളുമായി ഊര്\u200dജ്ജ അളക്കുന് നിലവിലുള്ള സോഫ്റ്റ്\u200cവെയര്\u200d അടിസ്ഥാനമായ ഊര്\u200dജ്ജ എണ്ണിഷനുകള്\u200d ഉപയോഗിച്ചു് നിലവിലുള്ള പിശകു് നമ്മള്\u200d പരിശോധിക്കുന നമ്മുടെ താക്കോല്\u200d എടുക്കുന്നത് വിഭവങ്ങള്\u200d ഉപയോഗിക്കുന്നതിനും ഊര്\u200dജ്ജ ഉപയോഗിക്കുന്നതിനും തമ്മിലുള്ള വ്യത്യാസവും കണക്കാക്കുന്ന ഒരു ക നമ്മള്\u200d കോഡും ഡേറ്റായും വിടുന്നു https://github.com/csarron/sustainlp2020-energy.', 'no': 'Når du velje og treng stor NLP-modeller, er nøyaktig og betrig måling av energieforbruk kritisk for å gjera godinformerte utval av design. I denne arbeida viser vi at dei eksisterande programvarebaserte energiestingar er ikkje nøyaktig fordi dei ikkje tar inn i konto skilnader på maskinvaren og korleis ressursbruk påvirkar energibruk. Vi gjer eksperimenter for energimålingar med fire ulike modeller for eit spørsmål som svarar oppgåve. Vi kvantifiserer feilen på eksisterande programvarebaserte energi ved å bruka ein maskinvare straummeter som tilbyr svært nøyaktig energimålingar. Nøkkelen vårt tømmer er nødvendighet for ein meir nøyaktig energi-estimeringsmodell som tar inn i konto maskinvare-variabilitet og ikkje-lineær forhold mellom ressursbruk og energibruk. Vi løyser koden og data på https://github.com/csarron/sustainlp2020-energy.', 'pl': 'Dokładny i niezawodny pomiar zużycia energii ma kluczowe znaczenie dla dokonywania dobrze świadomych wyborów projektowych przy wyborze i szkoleniu dużych modeli NLP. W niniejszej pracy pokazujemy, że istniejące szacunki energii oparte na oprogramowaniu nie są dokładne, ponieważ nie uwzględniają różnic sprzętowych i tego, jak wykorzystanie zasobów wpływa na zużycie energii. Przeprowadzamy eksperymenty pomiaru energii z czterema różnymi modelami do zadania odpowiedzi na pytania. Ilościowo określamy błąd istniejących oszacowań energii opartych na oprogramowaniu za pomocą sprzętowego miernika mocy, który zapewnia bardzo dokładne pomiary energii. Naszym kluczowym wyjściem jest potrzeba bardziej dokładnego modelu oszacowania energii, który uwzględnia zmienności sprzętu i nieliniową relację między wykorzystaniem zasobów a zużyciem energii. Udostępniamy kod i dane pod adresem https://github.com/csarron/sustainlp2020-energy.', 'ro': 'Măsurarea precisă și fiabilă a consumului de energie este esențială pentru a face alegeri de proiectare bine informate atunci când alegeți și instruiți modele NLP la scară largă. În această lucrare, arătăm că estimările energetice existente bazate pe software nu sunt corecte deoarece nu iau în considerare diferențele hardware și modul în care utilizarea resurselor afectează consumul de energie. Efectuăm experimente de măsurare a energiei cu patru modele diferite pentru o sarcină de răspuns la întrebări. Cuantificăm eroarea estimărilor energetice existente bazate pe software folosind un contor de putere hardware care oferă măsurători de energie foarte precise. Principala noastră soluție este necesitatea unui model de estimare a energiei mai precis, care să ia în considerare variabilitățile hardware și relația neliniară dintre utilizarea resurselor și consumul de energie. Noi eliberăm codul și datele la https://github.com/csarron/sustainlp2020-energy.', 'sr': 'Tačna i pouzdana mjera potrošnje energije je kritična za donošenje dobro informiranih izbora dizajna kada bira i obučava velike modele NLP. U ovom poslu pokazujemo da postojeće procjene energije na softveru nisu tačne jer ne uzimaju u obzir razlike hardware i kako korištenje resursa utiče na potrošnju energije. Provodimo eksperimente za mjerenje energije sa četiri različite modela za odgovor na pitanje. Mi kvantificiramo grešku postojećih procenata energije na softveru koristeći kompjuterski metar energije koji pruža veoma tačne mjere energije. Naš ključ je potreba za preciznijim model procjene energije koji uzima u obzir variabilnosti hardware a i neolinearne veze između korištenja resursa i potrošnje energije. Puštamo šifru i podatke na https://github.com/csarron/sustainlp2020-energy.', 'so': "Heshiiska isticmaalka energiyadu waa muhiim in loo sameeyo doorasho aad u yaqaan, marka aad dooranaysid iyo waxbarasho modellada waaweyn ee NLP. Shuqulkaas waxan ku tusnaa in qiimeynta energiga ee software-based aysan sax aheyn sababtoo ah aysan xisaabin kala duwanaanshaha qalabka qalabka iyo sida isticmaalka resiliyadu ay saameyn ku yeelato isticmaalka energiga. Waxaynu sameynaa imtixaan qiyaasta energiga, waxaana ku sameynaa afar model oo kala duduwan si aan u sameyno su'aal ka jawaabta. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements.  Soo qaadashada furayaashayada waa u baahan qaababka qiimeynta energiga ee saxda ah, kaas oo ku xisaabta isbedelka qalabka qalabka iyo xiriirka aan la xiriirayn isticmaalka nolosha iyo isticmaalka energiga. Waxaynu furaynaa sumadda iyo macluumaadka https://github.com/csarron/sustainlp2020-energy.", 'ms': 'Keukuran konsumsi tenaga yang tepat dan boleh dipercayai adalah penting untuk membuat pilihan desain yang diberitahu dengan baik apabila memilih dan melatih model NLP skala besar. Dalam kerja ini, kami menunjukkan bahawa penilaian tenaga berdasarkan perisian yang wujud tidak tepat kerana mereka tidak mempertimbangkan perbezaan perisian dan bagaimana penggunaan sumber mempengaruhi konsumsi tenaga. Kami melakukan eksperimen pengukuran tenaga dengan empat model yang berbeza untuk tugas menjawab soalan. Kami kuantifikasikan ralat pengiraan tenaga berdasarkan perisian yang wujud dengan menggunakan meter tenaga perkakasan yang menyediakan pengukuran tenaga yang tepat. Key takeaway kami adalah perlukan model penilaian tenaga yang lebih tepat yang mempertimbangkan variabiliti perkakasan dan hubungan tidak linear antara penggunaan sumber dan penggunaan tenaga. Kami melepaskan kod dan data di https://github.com/csarron/sustainlp2020-energy.', 'sv': 'Noggrann och tillförlitlig mätning av energiförbrukningen är avgörande för välinformerade designval vid val och träning av storskaliga NLP-modeller. I detta arbete visar vi att befintliga programvarubaserade energiskattningar inte är korrekta eftersom de inte tar hänsyn till maskinvaruskillnader och hur resursutnyttjande påverkar energiförbrukningen. Vi genomför energimätningsexperiment med fyra olika modeller för en frågeställningsuppgift. Vi kvantifierar felet i befintliga programvarubaserade energiskattningar genom att använda en hårdvaruteffektmätare som ger mycket exakta energimätningar. Vår viktigaste uppgift är behovet av en mer exakt energiskattningsmodell som tar hänsyn till maskinvaruvariationer och det icke-linjära förhållandet mellan resursutnyttjande och energiförbrukning. Vi släpper koden och data på https://github.com/csarron/sustainlp2020-energy.', 'si': 'ඇත්තටම සහ විශ්වාසිත විශ්වාසිත විශ්වාස කරන්න පුළුවන් විශ්වාසිත විශ්වාස කරනවා NLP මොඩේල් තෝරාගන්න සහ ප මේ වැඩේ අපි පෙන්වන්නේ ඉතින් සොෆ්ටවර් අධාරිත ශක්තිය අනුමාණය නිර්දේශය නෙවෙයි මොකද ඔවුන් හාර්ඩ්වර් වෙනස් විදිය අපි ප්\u200dරශ්නයක් උත්තර දෙන්න ප්\u200dරශ්නයකට ප්\u200dරශ්නයක් වෙනුවෙන් වෙනස් මොඩේල් හතරක් තියෙන බලය අපි තියෙන්නේ සොෆ්ටවේර් අධාරිත ශක්තිය අනුමාණයේ වැරැද්දක් විශ්වාස කරනවා හාර්ඩ්වේර් බලය මීටර් භා අපේ යතුරු අරගෙන යන්න තමයි තරම් සාධාරණ ශක්තිය අනුමාණයක් වෙනුවෙන් අවශ්\u200dය විශේෂතාවක් සහ ශක්තිව්\u200dය භාවිතාවක් සමගේ  අපි කෝඩ් සහ දත්ත නිදහස් කරනවා https://github.com/csarron/sustainlp2020-energy.', 'ta': 'மின்சக்தி பயன்பாட்டின் சரியான மற்றும் நம்பிக்கையான அளக்கம் என்எல்பி மாதிரிகளை தேர்வு செய்யும் போது நன்கு அறிவிக்கப்பட்ட வட இந்த வேலையில், தற்போதைய மென்பொருள் அடிப்படையிலுள்ள சக்தி மதிப்புகள் சரியாக இல்லை ஏனெனில் அவர்கள் வன்பொருள் வேறுபாடுகளை பார்க்கவில்ல நாம் மின்சக்தி அளக்கம் சோதனைகளை நான்கு வித்தியாசமான மாதிரிகளால் செயல்படுத்துகிறோம் கேள்வி  நாம் தற்போதைய மென்பொருள் அடிப்படையிலுள்ள சக்தி கணக்கீடுகளை பயன்படுத்தி ஒரு வன்பொருள் சக்தி மீட்டரை பயன்படுத்தி பிழையை கு எங்கள் விசை எடுத்தல் தேவையானது மூலத்தின் பயன்பாடு மற்றும் சக்தி பயன்பாட்டிற்கும் இடையே உள்ள மாற்றங்களை கணக்கிடும் ஒரு மிக சரியான சக்தி மதிப்ப நாம் குறியீடு மற்றும் தகவல்களை வெளியிடுகிறோம் https://github.com/csarron/sustainlp2020-energy.', 'ur': 'انرژی مصرف کا دقیق اور قابل اندازہ انرژی مصرف کرنے کے لئے اچھی طراحی کے انتخاب کرنے کے لئے ضرورت ہے جب انرژی مصرف کا انتخاب کریں اور بڑے اسکیل NLP موڈل کو تربیت کریں۔ ہم اس کام میں دکھاتے ہیں کہ موجود سوفٹر بنیاد پر انرژی کا ارزش دقیق نہیں ہے کیونکہ وہ ہرڈر کے اختلاف میں حساب نہیں لیتے اور کس طرح سوفٹر استعمال انرژی مصرف پر اثر دیتا ہے۔ ہم ایک سوال جواب دینے کے لئے چار مختلف موڈل کے ساتھ انرژی اندازے کی آزمائش کرتے ہیں. ہم موجود سافٹوفر بنیادی انرژی قدرت کی خطا کا مقدار کررہے ہیں ایک ہرڈوفر پاور میٹر کے استعمال کے ذریعہ جو بہت دقیق انرژی مقدار کے طور پر پیش کرتا ہے. ہماری کلیدی اٹھانے کی ضرورت ہے ایک زیادہ دقیق انرژی اٹھانے کے موڈل کے لئے جو ہرڈارڈ کے بدلنے والیاں اور سواروں کے استعمال اور انرژی مصرف کے درمیان نالینی رابطہ کا حساب لیتا ہے. ہم کوڈ اور ڈیٹا کو آزاد کرتے ہیں https://github.com/csarron/sustainlp2020-energy.', 'uz': "Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models.  Bu ishda, biz bu yerda mavjud dastur asosiy energiya qiymatlari haqiqida emas, chunki ular dastur turlarini o'ylamaydi va manbaning foydalanishi energiga foydalanishini qanday qiladi. Biz esa savol javob berish uchun to'rt boshqa model bilan energiya tizimni bajaramiz. @ info Bizning kalitlarni olib tashlash kerakligimiz, qisqarli energiya qiymati modeli uchun kerak. Hardwar variabiliyatlarini hisoblash va manbaning foydalanish va energiya foydalanish orasidagi aloqalarni o'zgartirish mumkin. Biz kodlash va maʼlumot https://github.com/csarron/sustainlp2020-energy.", 'vi': 'Tính chính xác và đáng tin cậy về tiêu thụ năng lượng là điều then chốt cho việc lựa chọn thiết kế cẩn thận khi chọn và huấn luyện các mô hình NMB trên diện rộng. Trong công việc này, chúng tôi cho thấy các dự đoán năng lượng dựa trên phần mềm hiện thời không chính xác vì chúng không tính toán về sự khác biệt về phần cứng và cách sử dụng tài nguyên ảnh hưởng đến tiêu thụ năng lượng. Chúng tôi tiến hành thử nghiệm đo năng lượng với bốn mẫu khác nhau cho một nhiệm vụ trả lời câu hỏi. Chúng tôi định giá lỗi của dự đoán năng lượng dựa trên phần mềm hiện tại bằng cách dùng một máy đo sức mạnh phần cứng cung cấp các đo năng lượng chính xác cao. Cách giải quyết chủ yếu của chúng ta là cần một mô hình đánh giá năng lượng chính xác hơn, để cân nhắc sự đa dạng máy móc và mối quan hệ không tuyến tính giữa sử dụng tài nguyên và tiêu thụ năng lượng. Chúng tôi giải mã và dữ liệu ở... https://github.com/csarron/sustainlp2020-energy.', 'nl': 'Nauwkeurige en betrouwbare meting van het energieverbruik is cruciaal voor het maken van goed geïnformeerde ontwerpkeuzes bij het kiezen en trainen van grootschalige NLP-modellen. In dit werk laten we zien dat bestaande softwaregebaseerde energieschattingen niet accuraat zijn omdat ze geen rekening houden met hardwareverschillen en hoe het gebruik van hulpbronnen het energieverbruik beïnvloedt. Voor een vragenbeantwoordingstaak voeren we energiemeetexperimenten uit met vier verschillende modellen. We kwantificeren de fout van bestaande softwaregebaseerde energieschattingen met behulp van een hardware vermogensmeter die zeer nauwkeurige energiemetingen levert. Onze belangrijkste conclusie is de behoefte aan een nauwkeuriger energieschattingsmodel dat rekening houdt met hardwarevariabelen en de niet-lineaire relatie tussen hulpbronnengebruik en energieverbruik. We geven de code en gegevens vrij op https://github.com/csarron/sustainlp2020-energy.', 'hr': 'Tačna i pouzdana mjera potrošnje energije je kritična za donošenje dobro informiranih izbora dizajna kada biraju i obučavaju velike modele NLP-a. U ovom poslu pokazujemo da postojeće procjene energije na softveru nisu precizne jer ne uzimaju u obzir razlike hardware i kako korištenje resursa utječe na potrošnju energije. Provodimo eksperimente za mjerenje energije s četiri različite modela odgovornog zadatka. Mi kvantificiramo grešku postojećih procjena energije na softveru koristeći električni metar hardware a koji pruža vrlo precizne mjere energije. Naš ključni oduzimanje je potreba za preciznijim modelom procjene energije koja uzima u obzir variabilnosti hardware a i neolinearnu vezu između korištenja resursa i potrošnje energije. Puštamo šifru i podatke na https://github.com/csarron/sustainlp2020-energy.', 'da': 'Nøjagtig og pålidelig måling af energiforbruget er afgørende for at træffe velinformerede designvalg, når du vælger og træner storstilede NLP-modeller. I dette arbejde viser vi, at eksisterende softwarebaserede energiestimater ikke er korrekte, fordi de ikke tager hensyn til hardwareforskelle og hvordan ressourceudnyttelse påvirker energiforbruget. Vi udfører energimålinger med fire forskellige modeller til en spørgsmålsopgave. Vi kvantificerer fejlen ved eksisterende softwarebaserede energiestimater ved hjælp af en hardwareeffektmåler, der giver meget nøjagtige energimålinger. Vores vigtigste ting er behovet for en mere præcis energiestimeringsmodel, der tager højde for hardware variabiliteter og det ikke-lineære forhold mellem ressourceudnyttelse og energiforbrug. Vi frigiver koden og data på https://github.com/csarron/sustainlp2020-energy.', 'bg': 'Точното и надеждно измерване на консумацията на енергия е от решаващо значение за вземането на добре информиран дизайн при избора и обучението на широкомащабни модели на НЛО. В тази работа показваме, че съществуващите софтуерно базирани енергийни оценки не са точни, тъй като не отчитат хардуерните разлики и как използването на ресурсите влияе върху потреблението на енергия. Провеждаме експерименти за измерване на енергията с четири различни модела за задача за отговор на въпроси. Ние количествено измерваме грешката на съществуващите софтуерно базирани енергийни оценки, като използваме хардуерен електромер, който осигурява високо точни енергийни измервания. Нашият основен извод е необходимостта от по-точен модел за оценка на енергията, който отчита хардуерните променливи и нелинейната връзка между използването на ресурсите и потреблението на енергия. Ще пуснем кода и данните на https://github.com/csarron/sustainlp2020-energy.', 'de': 'Eine genaue und zuverlässige Messung des Energieverbrauchs ist entscheidend für fundierte Designentscheidungen bei der Auswahl und Schulung großer NLP-Modelle. In dieser Arbeit zeigen wir, dass bestehende softwarebasierte Energieschätzungen nicht genau sind, da sie Hardwareunterschiede und den Einfluss der Ressourcennutzung auf den Energieverbrauch nicht berücksichtigen. Für eine Fragestellung führen wir Energiemessversuche mit vier verschiedenen Modellen durch. Wir quantifizieren den Fehler bestehender softwarebasierter Energieschätzungen mithilfe eines Hardware-Leistungsmessers, der hochpräzise Energiemessungen liefert. Unsere wichtigste Erkenntnis ist die Notwendigkeit eines genaueren Energieschätzungsmodells, das Hardwarevariablen und die nichtlineare Beziehung zwischen Ressourcennutzung und Energieverbrauch berücksichtigt. Wir veröffentlichen den Code und die Daten unter https://github.com/csarron/sustainlp2020-energy.', 'id': 'Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models.  Dalam pekerjaan ini, kami menunjukkan bahwa penilaian energi berdasarkan perangkat lunak yang ada tidak akurat karena mereka tidak mempertimbangkan perbedaan perangkat keras dan bagaimana penggunaan sumber daya mempengaruhi konsumsi energi. Kami melakukan eksperimen pengukuran energi dengan empat model yang berbeda untuk tugas menjawab pertanyaan. Kami mengurangi kesalahan dari penilaian energi berdasarkan perangkat lunak yang ada dengan menggunakan meter tenaga perangkat keras yang menyediakan pengukuran energi yang sangat akurat. Key takeaway kami adalah kebutuhan untuk model penilaian energi yang lebih akurat yang mempertimbangkan variabilitas perangkat keras dan hubungan non-linear antara penggunaan sumber daya dan konsumsi energi. Kami melepaskan kode dan data di https://github.com/csarron/sustainlp2020-energy.', 'ko': '대규모 NLP 모델을 선택하고 훈련할 때 정확한 에너지 소모 측정은 현명한 디자인 선택에 매우 중요하다.이 작업에서 우리는 기존의 소프트웨어 기반 에너지 평가가 정확하지 않다는 것을 보여준다. 왜냐하면 하드웨어 차이와 자원 이용이 에너지 소모에 어떻게 영향을 미치는지 고려하지 않았기 때문이다.우리는 네 가지 다른 모형으로 에너지 측정 실험을 진행하였다.우리는 고정밀 에너지 측정을 제공하는 하드웨어 전력계를 사용하여 기존의 소프트웨어 기반 에너지 평가 오차를 계량화한다.우리의 주요 수확은 더욱 정확한 에너지 추산 모델이 필요하다는 것이다. 이 모델은 하드웨어의 변화와 자원 이용률과 에너지 소모 간의 비선형 관계를 고려했다.우리https://github.com/csarron/sustainlp2020-energy.', 'fa': 'اندازه دقیق و قابل اعتماد از مصرف انرژی برای انتخاب انتخاب طراحی خوب اطلاع شده در زمان انتخاب کردن و آموزش مدلهای مقیاس بزرگ NLP است. در این کار، ما نشان می دهیم که ارزیابی انرژی بر پایه نرم\u200cافزار موجود دقیق نیستند چون آنها تفاوت هاروارد را به حساب نمی\u200cگیرند و چگونه استفاده از منابع بر مصرف انرژی تأثیر می\u200cدهد. ما آزمایشات اندازه انرژی را با چهار مدل متفاوت برای یک کار جواب سوال انجام می دهیم. ما خطای ارزیابی انرژی بر پایه نرم\u200cافزار موجود را با استفاده از یک متر برق حافظه\u200cای که اندازه\u200cهای انرژی بسیار دقیق را پیشنهاد می\u200cدهد مقدار می\u200cدهیم. کلید ما نیاز به یک مدل ارزیابی انرژی دقیق تر است که با تغییرات حافظه و ارتباط غیرخط بین استفاده از منابع و مصرف انرژی به حساب می گیرد. ما کد و اطلاعات را در https://github.com/csarron/sustainlp2020-energy.', 'sq': 'Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models.  Në këtë punë, ne tregojmë se vlerësimet ekzistuese të energjisë bazuar në programe nuk janë të sakta sepse ato nuk marrin parasysh dallimet në hardware dhe se si përdorimi i burimeve ndikon në konsumin e energjisë. Ne kryejmë eksperimente të matjes së energjisë me katër modele të ndryshme për një detyrë përgjigjeje pyetjesh. Ne kuantifikojmë gabimin e vlerësimeve të energjisë ekzistuese bazuar në softuer duke përdorur një metrues energjie hardware që ofron matje të sakta energjie. Vendi ynë kryesor është nevoja për një model më të saktë të vlerësimit të energjisë që merr parasysh variabilitetet e hardware dhe marrëdhëniet jo-lineare midis përdorimit të burimeve dhe konsumit të energjisë. Ne lëshojmë kodin dhe të dhënat në https://github.com/csarron/sustainlp2020-energy.', 'sw': 'Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models.  Katika kazi hii, tunaonyesha kwamba kadiri za nishati zinazotokana na programu hazina sahihi kwa sababu hawajazingatia tofauti za vifaa vya vifaa na jinsi matumizi ya rasilimali yanavyoathiri matumizi ya nishati. Tunafanya majaribio ya upasuaji wa nishati na mifano minne tofauti kwa ajili ya kazi ya kujibu swali. Tunaweza kuhakikisha kosa la kadiria za nishati zilizopo kwa kutumia mita ya nishati ya vifaa vya nishati inayotoa kipimo sahihi kabisa cha nishati. Utafiti wetu wa ufunguo ni haja ya mifano sahihi ya uchunguzi wa nishati inayochukulia mabadiliko ya vifaa na mahusiano yasiyo na msingi kati ya matumizi ya rasilimali na matumizi ya nishati. Tunaachia sheria na taarifa https://github.com/csarron/sustainlp2020-energy.', 'tr': 'Energiň tüketiminiň dogry we ynamly ölçüsi NLP modellerini saýlap we gowy bilgili dizayn saýlawlary saýlamak üçin wajypdyr. Bu işde, biz bar softwaryň tabanly enerji hasaplamalarynyň dogry däldir sebäbi olar Hardwareň üýtgeşmelerini we resurslaryň enerji tüketmesine nähili täsirleýändigini görüp bilmeýäris. Biz enerji ölçülemek deneylerini soragy jogap bermek üçin dört dürli nusga bilen etdik. Biz bar softwaryň tabanly enerji hasaplamalarynyň ýüze dogry enerji ölçümlerini saýlayarak hatasyny çykýarys. Biziň a çarymyz çykyşymyz enerji hasaplamak nusgasyna degişli hardware üýtgeşmelerini we ressoural ulanmaklaryň we enerji consumesiniň arasyndaky çyzgytlyk baglaşymyzdyr. Biz ködi we maglumatlary boşadyrys https://github.com/csarron/sustainlp2020-energy.', 'af': "Akkuraat en betroubaar gemaak van energie verbruik is kritiese vir die maak van goed inligtige ontwerp keuses wanneer groot skaal NLP modelles kies en onderrig word. In hierdie werk, wys ons dat bestaande sagteware-gebaseerde energie-estimaties nie presies is nie, omdat hulle nie in rekening van hardware verskille neem en hoe hulpbron-gebruikering effekteer energie-consumption nie. Ons doen energiemeasurement eksperimente met vier verskillende modele vir 'n vraag antwoord taak. Ons quantifiseer die fout van bestaande sagteware-gebaseerde energie-estimaties deur 'n hardware kragtermeter te gebruik wat baie presies energiemeasuremente verskaf. Ons sleutel wegneem is die benodig vir 'n meer presies energie-estimatie model wat neem in rekening van hardware-veranderlikhede en die nie-lineêre verhouding tussen hulpbron-gebruik en energie-verbruik. Ons verlos die kode en data op https://github.com/csarron/sustainlp2020-energy.", 'am': 'የenergy መጠቀሚያ መጠቀሚያ የውስጠ የጥናት ምርጫዎች በመምረጥ እና ትልቅ የNLP ዓይነቶች ማስተካከል የሚያስፈልግ ግድ ነው፡፡ በዚህ ሥራ፣ የሶፍትዌር-መሠረት የኢትዮጵያ የenergy መጠቀሚያ ፍላጎቶችን ስለማያስቡ፣ የከተማውም መጠቀም የenergy መጠቀሚያ እንዴት እንደሚያሳስል እናሳያቸዋለን፡፡ ለጥያቄ መልስ ስራ አራት ልዩ ዓይነቶችን እናደርጋለን፡፡ የሶፍትዌር-መሠረት የenergy መጠቀሚያ ስህተት በትክክል የenergy ሚትር በመጠቀም እናቆጥራለን፡፡ የቁልፈታችን መቀበል የenergy መጠቀሚያ ሞዴል ነው፤ የዌዳር መለያየት እና የሀብት መጠቀሚያ እና በenergy መጠቀሚያ መካከል ያልተቋረጠ ግንኙነት ያስፈልጋል፡፡ የኮድ እና ዳራዎችን በ https://github.com/csarron/sustainlp2020-energy.', 'az': 'NLP modellərini seçərkən və böyük ölçüdə təhsil edərkən müəyyən edilmiş dizayn seçimləri təhsil etmək üçün ədalətli və güvenilir ölçülərdir. Bu işdə, biz yazılım tabanlı enerji hesablamaları do ğru deyildir, çünki onlar hardware ç fərqliyinə baxmırlar və ressurs istifadəsi enerji istifadəsinə necə etkilər. Biz enerji ölçüsü təcrübələrini dörd müxtəlif modellərlə soruşmaq üçün təşkil edirik. Biz, proqramma tabanlı enerji hesablamalarının xətasını çox doğru enerji ölçülərini sağlayan hardwareq enerji metrini kullanarak hesablayırıq. Bizim anahtarımız istifadə etmək və enerji istifadəsi arasındakı çox düzgün enerji hesablama modeli üçün ehtiyacımızdır. Biz kodu və məlumatları https://github.com/csarron/sustainlp2020-energy.', 'bn': 'এনএলপি মডেল নির্বাচন এবং প্রশিক্ষণ করার সময় সঠিক এবং বিশ্বস্ত বিদ্যুত ব্যবহারের পরিমাপ গুরুত্বপূর্ণ। এই কাজে আমরা দেখাচ্ছি যে বিদ্যমান সফটওয়্যার ভিত্তিক শক্তির হিসাব সঠিক নয় কারণ তারা হার্ডওয়্যারের পার্থক্যের প্রতি বিভিন্ন ভিন্ন ভি আমরা একটি প্রশ্নের উত্তরের কাজের জন্য চারটি ভিন্ন মডেল দিয়ে শক্তি পরিমাপ পরীক্ষা করি। আমরা বিদ্যমান সফটওয়্যার-ভিত্তিক শক্তি হিসেবে বিদ্যুৎ মিটার ব্যবহার করে বিদ্যমান হার্ডওয়্যার বিদ্যুৎ মিটার ব্যবহার করে  আমাদের চাবিকা তুলে ধরার প্রয়োজন হচ্ছে আরো সঠিক শক্তি হিসেবে হার্ডওয়্যারের বৈষম্য এবং সম্পদ ব্যবহার এবং শক্তি ব্যবহারের মধ্যে না লাইনের সম্পর্কের ব্ We release the code and data at  https://github.com/csarron/sustainlp2020-energy.', 'hy': 'Էներգիայի սպառության ճշգրիտ և վստահելի չափումները կարևոր են լավ տեղեկացված դիզայնի ընտրությունների կայացնելու համար, երբ ընտրվում են և ուսումնասիրում են մեծ մակարդակի ՆԼՊ մոդելներ: Այս աշխատանքի ընթացքում մենք ցույց ենք տալիս, որ գոյություն ունեցող ծրագրային հիմնված էներգիայի գնահատումները ճշգրիտ չեն, որովհետև դրանք չեն հաշվի առնում հնարավորությունների տարբերությունները և թե ինչպես են ռեսուրսների օգտագործումը ազ Մենք կատարում ենք էներգիայի չափման փորձեր չորս տարբեր մոդելների հետ հարցերի պատասխանելու համար: Մենք քանակությամբ գնահատում ենք գոյություն ունեցող ծրագրային հիմնված էներգիայի գնահատումների սխալը օգտագործելով սարքավորական էներգիայի չափչափ, որը պարունակում է շատ ճշգրիտ էներգիայի չափ Մեր հիմնական գործընթացն այն է, որ ավելի ճշգրիտ էներգիայի գնահատման մոդելի կարիքն է, որը հաշվի առնում է սարքավորման տարբերությունները և ռեսուրսների օգտագործման և էներգիայի սպառման միջև ոչ գծային հարաբերությունները: Մենք հրապարակում ենք կոդը և տվյալները https://github.com/csarron/sustainlp2020-energy.', 'bs': 'Tačna i pouzdana mjera potrošnje energije je kritična za donošenje dobro informiranih izbora dizajna kada bira i obučava velike modele NLP-a. U ovom poslu pokazujemo da postojeće procjene energije na softveru nisu precizne jer ne uzimaju u obzir razlike hardware i kako korištenje resursa utječe na potrošnju energije. Vodimo eksperimente za mjerenje energije sa četiri različite modele za odgovor na pitanje. Mi kvantificiramo grešku postojećih procjena energije na softveru koristeći električni metar hardware a koji pruža vrlo precizne mjere energije. Naš ključ je potreba za preciznijim modelom procjene energije koji uzima u obzir variabilnosti hardware a i neolinearnu vezu između korištenja resursa i potrošnje energije. Puštamo šifru i podatke na https://github.com/csarron/sustainlp2020-energy.', 'cs': 'Přesné a spolehlivé měření spotřeby energie je klíčové pro dobře informované rozhodnutí o konstrukci při výběru a školení velkých modelů NLP. V této práci ukazujeme, že stávající softwarové energetické odhady nejsou přesné, protože neberou v úvahu hardwarové rozdíly a to, jak využití zdrojů ovlivňuje spotřebu energie. Pro úkol zodpovězení otázek provádíme experimenty měření energie se čtyřmi různými modely. Chybu stávajících softwarových odhadů energie kvantifikujeme pomocí hardwarového měřiče výkonu, který poskytuje vysoce přesná měření energie. Naším klíčovým významem je potřeba přesnějšího modelu odhadu energie, který zohledňuje proměnlivosti hardwaru a nelineární vztah mezi využitím zdrojů a spotřebou energie. Kód a data uvolníme na adrese https://github.com/csarron/sustainlp2020-energy.', 'et': 'Energiatarbimise täpne ja usaldusväärne mõõtmine on oluline, et teha teadlikke projekteerimisvalikuid suuremahuliste NLP mudelite valimisel ja koolitamisel. Käesolevas töös näitame, et olemasolevad tarkvarapõhised energiahinnangud ei ole täpsed, sest need ei võta arvesse riistvara erinevusi ja kuidas ressursside kasutamine mõjutab energiatarbimist. Teostame energiamõõtmise katseid nelja erineva mudeliga küsimustele vastamise ülesandeks. Olemasolevate tarkvarapõhiste energiahinnangute vigade kvantifitseerimiseks kasutame riistvara võimsusarvesti, mis tagab väga täpsed energiamõõtmised. Meie võtmetähtsuseks on vajadus täpsema energia hindamise mudeli järele, mis võtab arvesse riistvara muutuvust ning mittelineaarset seost ressursside kasutamise ja energiatarbimise vahel. Me avaldame koodi ja andmed aadressil https://github.com/csarron/sustainlp2020-energy.', 'fi': 'Energiankulutuksen tarkka ja luotettava mittaus on ratkaisevan tärkeää, jotta voidaan tehdä tietoisia suunnitteluvalintoja suuren mittakaavan NLP-malleja valittaessa ja koulutettaessa. Tässä työssä osoitetaan, että olemassa olevat ohjelmistopohjaiset energiaarviot eivät ole tarkkoja, koska niissä ei oteta huomioon laitteistoeroja ja miten resurssien käyttö vaikuttaa energiankulutukseen. Teemme energianmittauskokeita neljällä eri mallilla kysymystehtävään. Määritämme olemassa olevien ohjelmistopohjaisten energiaarvioiden virheen käyttämällä laitteistotehomittaria, joka tuottaa erittäin tarkat energiamittaukset. Keskeinen johtopäätöksemme on tarve tarkemmalle energianarviointimallille, jossa otetaan huomioon laitteiston vaihtelut sekä resurssien käytön ja energiankulutuksen epälineaarinen suhde. Julkaisemme koodin ja datan osoitteessa https://github.com/csarron/sustainlp2020-energy.', 'ca': "Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models.  En aquesta feina, demostram que les estimacions d'energia existents basades en software no són exactes perquè no tenen en compte les diferències del hardware i com l'ús de recursos afecta al consum d'energia. Fem experiments de mesura de l'energia amb quatre models diferents per respondre a preguntes. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements.  La nostra solució clau és la necessitat d'un model d'estimació més precisa de l'energia que tenga en compte les variabilitats del hardware i la relació no linear entre l'ús de recursos i el consum d'energia. We release the code and data at  https://github.com/csarron/sustainlp2020-energy.", 'sk': 'Natančno in zanesljivo merjenje porabe energije je ključnega pomena za dobro informirane izbire oblikovanja pri izbiri in usposabljanju obsežnih modelov NLP. V tem delu smo pokazali, da obstoječe programsko temelječe energetske ocene niso točne, ker ne upoštevajo razlik v strojni opremi in kako izraba virov vpliva na porabo energije. Izvajamo eksperimente merjenja energije s štirimi različnimi modeli za nalogo odgovarjanja na vprašanja. Napako obstoječih programskih ocen energije kvantificiramo s strojnim merilnikom moči, ki zagotavlja visoko natančne meritve energije. Naš ključni pomen je potreba po natančnejšem modelu ocenjevanja energije, ki upošteva variabilnosti strojne opreme in nelinearno razmerje med izkoriščanjem virov in porabo energije. Izdajamo kodo in podatke na https://github.com/csarron/sustainlp2020-energy.', 'jv': 'buddy Nang barêng-barêng iki, kéné ngerasahan kanggo kowé éneng pisan sing wis ana dadi kanggo nik nggawe barang-alaman sing ora bisa tekan dengan perangkat lunak-alaman sing bakal terus nggawe barang-alaman sing bakal terus nggawe ketahanan pangan sing bisa pasar alaman sing ngenda Name Awak dhéwé éntuk kesempatan kanggo nggawe barang nggawe jinêr software sing digawen karo nggawe barang pengaturan nyetas kuwi nggawe barang ketahan Awak dhéwé éntukno ing sakjané nggawe sistem dadi nggawe énèh sing bisa nguasai pernik-pernik nik ingkang sampeyan pakan karo akeh barang-sistem sing gak dhéwé Awak dhéwé mbukak kode lan data sak https://github.com/csarron/sustainlp2020-energy.', 'he': 'דידוד מדויק ואמין של שימוש אנרגיה הוא קריטי כדי לעשות בחירות עיצוב מוודעות כשבוחרים ואימונים דוגמנים NLP בקנה מידה גדולה. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption.  אנו מבצעים ניסויים למדידת אנרגיה עם ארבעה דוגמנים שונים עבור משימה לענות על שאלות. אנו מכוונים את הטעות של הערכות האנרגיה הנוכחות מבוססות בתוכנה על ידי השימוש במספר כוח חומר שמספק מדידות אנרגיה מדויקות מאוד. המפתח שלנו הוא הצורך למודל הערכת אנרגיה מדויק יותר ששוקל את שוויריות החומר והיחסים הלא-לינריים בין השימוש במשאבים לבין השימוש באנרגיה. אנחנו משחררים את הקוד והנתונים ב https://github.com/csarron/sustainlp2020-energy.', 'ha': "Taƙaitacce da mai inganci wa matuƙar na nishati na muhimu wa ka sami zaɓen zaɓen wanda aka sani da shi idan ya zãɓi kuma ya yi amfani da misãlai masu girma na NLP. Daga wannan aikin, Munã nũna masa cewa ƙidãya na nishati wanda ke kan kwamfyuta bai zama daidai ba, dõmin ba su yi bincike wa diffuka na nau'i ko da abincin na zartar da abincin na zartar da amfani da nishati. Munã tafiyar jarrabo ta lissafa ƙarfi da misãlai huɗu dabam-dabam zuwa wani tambayi wanda ke karɓa. Kana ƙayyade kure ga kimar ajiya wanda ke cikin kwamfyutan tebur da kwamfyutan tebur da za'a iya amfani da wata metre na wuri wanda ke samar da gwargwadon ƙarfi da inganci. Yin amfani da maɓallinmu, yana da amfani da wata misali mai ƙayyade nishati da zai lissafa variablewa na jeri da danganci na amfani da abincin ajiya. Munã sakar da kodi da data a kan https://github.com/csarron/sustainlp2020-energy.", 'bo': 'Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. འོན་ཀྱང་། ང་ཚོས་གནས་ཡོད་པའི་མཉེན་ཆས་གཞི་བྱས་པའི་མཉེན་ཆས་ཕྱོགས་ཀྱི་ནུས ང་ཚོས་རྟོགས་ཀྱི་ཚད་འཛིན་སྟངས་ལ་རྟགས་མེད་པའི་མིག་གཟུགས་རིས་ལྟར་བྱེད་ཀྱི་ཡོད། ང་ཚོས་མཉེན་ཆས་གཞི་བརྟེན་པའི་མཉེན་ཆས་ཕྱོགས་ཀྱི་ནུས་མཐུན་རྐྱེན་ཚད་ལྟར་བདེ་ཞིག་བྱེད་པའི་མཉེན་ཆས་ཕྱོགས་ཆས་ ང་ཚོའི་མཐོང་ཆེ་རྩལ་ནུས་འདོད་ན་རྐྱེན་གྱི་ནུས་པས་བདེན་བཤད་ཀྱི་རྩ་སྒྲིག་ཆེན་དང་། ང་ཚོས་མཚོན་རིས་དང་ཞིབ་བཤེར་བྱེད་ཀྱི་ཡོད་པ https://github.com/csarron/sustainlp2020-energy.'}
{'en': 'Overview of the SustaiNLP 2020 Shared Task', 'pt': 'Visão geral da tarefa compartilhada SustainNLP 2020', 'ar': 'نظرة عامة على المهمة المشتركة SustaiNLP 2020', 'es': 'Descripción general de la tarea compartida de SustainLP 2020', 'fr': 'Présentation de la tâche partagée SustainLP 2020', 'ja': 'SustaiNLP 2020共有タスクの概要', 'zh': 'SustaiNLP 2020 共事概述', 'hi': 'SustaiNLP 2020 साझा कार्य का अवलोकन', 'ru': 'Обзор совместной задачи SustaiNLP 2020', 'ga': 'Forbhreathnú ar Thasc Comhroinnte SutaiNLP 2020', 'ka': 'SustaiNLP 2020 საზოგადომი დავალების დანახვა', 'el': 'Επισκόπηση της κοινής εργασίας SustaiNLP 2020', 'hu': 'A SustaiNLP 2020 megosztott feladatának áttekintése', 'it': "Panoramica dell'attività condivisa SustaiNLP 2020", 'kk': 'SustaiNLP 2020 ортақтастырылған тапсырманың қарау', 'lt': 'Bendros užduoties „SustainNLP 2020“ apžvalga', 'mk': 'Преглед на заедничката задача SustaiNLP 2020', 'ms': 'Paparan ringkasan tugas terkongsi SustaiNLP 2020', 'mt': 'Ħarsa ġenerali lejn il-Kompitu Konġunt SustaiNLP 2020', 'ml': 'സൂഷ്ടായിന്\u200dഎല്\u200dപി 2020 പങ്കുചേര്\u200dത്ത ജോലിയുടെ നിരീക്ഷിക്കുക', 'no': 'Oversyning av delt oppgåve for SustaiNLP 2020', 'pl': 'Przegląd wspólnego zadania SustaiNLP 2020', 'mn': 'SustaiNLP 2020 хуваалтын ажил', 'so': 'Overview of the SustaiNLP 2020 Shared Task', 'ro': 'Prezentare generală a sarcinii partajate SustaiNLP 2020', 'sv': 'Översikt över SustaiNLP 2020 delad uppgift', 'sr': 'Overview of the SustaiNLP 2020 shared task', 'si': 'සුස්ටයින්ල් 2020ක් සමාගත වැඩක් ගැන බලන්න', 'ta': 'Overview of the SustaiNLP 2020 Shared Task', 'ur': 'SustaiNLP 2020 Shared Task', 'uz': 'Name', 'vi': 'Xem tổng hợp công việc được chia sẻ', 'bg': 'Преглед на общата задача за продължаване на НПП 2020', 'da': 'Oversigt over SustaiNLP 2020 delt opgave', 'nl': 'Overzicht van de gedeelde taak SustaiNLP 2020', 'hr': 'Overview of the SustaiNLP 2020 shared task', 'de': 'Überblick über die gemeinsame Aufgabe SustaiNLP 2020', 'id': 'Overview of the SustaiNLP 2020 Shared Task', 'ko': 'SustaiNLP 2020 공유 작업 개요', 'fa': 'Overview of the SustaiNLP 2020 Shared Task', 'sw': 'Tazama la SustaiNLP 2020', 'tr': 'SustaiNLP 2020-nji Bölüniş Görnöşi', 'af': 'Oorsig van die SustaiNLP 2020 Gedeelde Opdrag', 'sq': 'Përfshirje e detyrës së përbashkët SustaiNLP 2020', 'am': 'አዲስ ዶሴ ፍጠር', 'az': 'SustaiNLP 2020 paylaşılan işin görünüşü', 'hy': '«Սուստայն 2020» ընդհանուր խնդիրը', 'bn': 'sustaiNLP ২০২০ শেয়ার করা কাজের উপর দেখাও', 'bs': 'Overview of the SustaiNLP 2020 shared task', 'cs': 'Přehled sdíleného úkolu SustaiNLP 2020', 'et': 'Ülevaade jätkusuutliku NLP 2020 ühisest ülesandest', 'ca': 'Vista general de la tasca compartida SustaiNLP 2020', 'fi': 'Yleiskatsaus SustaiNLP 2020 -ohjelman yhteiseen tehtävään', 'jv': 'Global', 'he': 'תצוגה על המשימה המשותפת של SustaiNLP 2020', 'ha': 'QShortcut', 'sk': 'Pregled skupne naloge trajnostnega NLP 2020', 'bo': 'SustaiNLP 2020 མཉམ་སྤྱོད་པའི་བྱ་འགུལ་གྱི་བསྟུན་ནས་ཀློག་སྟངས'}
{'en': 'We describe the SustaiNLP 2020 shared task : efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.', 'ar': 'نصف المهمة المشتركة SustaiNLP 2020: الاستدلال الفعال على معيار SuperGLUE (Wang et al. ، 2019). يتم تقييم المشاركين بناءً على الأداء وفقًا للمعيار وكذلك الطاقة المستهلكة في عمل تنبؤات على مجموعات الاختبار. نصف المهمة وتنظيمها والأنظمة المقدمة. عبر التقديمات الستة للمهمة المشتركة ، حقق المشاركون مكاسب كفاءة قدرها 20 على خط الأساس BERT القياسي (Devlin et al. ، 2019) ، بينما فقدوا أقل من نقطة مطلقة في الأداء.', 'fr': "Nous décrivons la tâche partagée SustainLP 2020\xa0: une inférence efficace sur le benchmark SuperGlue (Wang et al., 2019). Les participants sont évalués sur la base des performances de l'indice de référence ainsi que de l'énergie consommée lors de la réalisation de prévisions sur les ensembles de test. Nous décrivons la tâche, son organisation et les systèmes soumis. Sur les six soumissions à la tâche partagée, les participants ont réalisé des gains d'efficacité de 20\xa0% par rapport à une base de référence BERT standard (Devlin et al., 2019), tout en perdant moins d'un point absolu de performance.", 'es': 'Describimos la tarea compartida de SustainLP 2020: inferencia eficiente en el punto de referencia SuperGlue (Wang et al., 2019). Los participantes son evaluados en función del rendimiento en el punto de referencia, así como de la energía consumida al hacer predicciones en los conjuntos de pruebas. Describimos la tarea, su organización y los sistemas enviados. En las seis presentaciones a la tarea compartida, los participantes lograron ganancias de eficiencia de 20 con respecto a una línea de base BERT estándar (Devlin et al., 2019), mientras que perdieron menos de un punto absoluto en el rendimiento.', 'pt': 'Descrevemos a tarefa compartilhada do SustainNLP 2020: inferência eficiente no benchmark SuperGLUE (Wang et al., 2019). Os participantes são avaliados com base no desempenho no benchmark, bem como na energia consumida ao fazer previsões nos conjuntos de teste. Descrevemos a tarefa, sua organização e os sistemas apresentados. Nos seis envios para a tarefa compartilhada, os participantes obtiveram ganhos de eficiência de 20 em relação a uma linha de base padrão do BERT (Devlin et al., 2019), perdendo menos de um ponto absoluto no desempenho.', 'ja': 'SustaiNLP 2020共有タスク： SuperGLUEベンチマークでの効率的な推論について説明します（ Wang et al., 2019 ）。参加者は、ベンチマーク上のパフォーマンスと、テストセットで予測を行う際に消費されるエネルギーに基づいて評価されます。タスク、その組織、および提出されたシステムについて説明します。共有タスクへの6つの提出物を通じて、参加者は標準的なBERT （ Devlin et al., 2019 ）ベースラインを超えて20の効率向上を達成したが、パフォーマンスでは絶対的なポイント未満の損失であった。', 'zh': '述SustaiNLP 2020之共同任务:推SuperGLUE准之高效(Wang等,2019)。 准测之表及集上测时所耗参与者量。 我们描述职务、其部伍和提交的系统。 凡向之六份,参与者于格BERT(Devlin等,2019)基线成20之效率升,而性能之失不绝对点。', 'hi': 'हम SustaiNLP 2020 साझा कार्य का वर्णन करते हैं: SuperGLUE बेंचमार्क (वांग एट अल। प्रतिभागियों का मूल्यांकन बेंचमार्क पर प्रदर्शन के साथ-साथ परीक्षण सेट पर भविष्यवाणियां करने में खपत ऊर्जा के आधार पर किया जाता है। हम कार्य, उसके संगठन और सबमिट किए गए सिस्टम का वर्णन करते हैं। साझा कार्य के लिए छह प्रस्तुतियों में, प्रतिभागियों ने एक मानक BERT (Devlin et al., 2019) बेसलाइन पर 20 की दक्षता लाभ प्राप्त किया, जबकि प्रदर्शन में एक पूर्ण बिंदु से कम खो दिया।', 'ru': 'Мы описываем общую задачу SustaiNLP 2020: эффективный вывод по эталону SuperGLUE (Wang et al., 2019). Участники оцениваются на основе производительности по контрольному образцу, а также энергии, затраченной при составлении прогнозов на тестовых наборах. Мы описываем задачу, ее организацию и представленные системы. Во всех шести представлениях к общей задаче участники добились повышения эффективности на 20 пунктов по сравнению со стандартным базовым уровнем BERT (Devlin et al., 2019), при этом потеряв менее абсолютного пункта в эффективности.', 'ga': 'Déanaimid cur síos ar thasc roinnte SutaiNLP 2020: tátal éifeachtach ar thagarmharc SuperGLUE (Wang et al., 2019). Déantar rannpháirtithe a mheas bunaithe ar fheidhmíocht an tagarmhairc chomh maith leis an bhfuinneamh a ídítear chun tuar a dhéanamh ar na tacair tástála. Déanaimid cur síos ar an tasc, ar a eagrú, agus ar na córais a cuireadh isteach. Thar na sé aighneacht don tasc roinnte, bhain rannpháirtithe gnóthachain éifeachtúlachta de 20 amach thar bhunlíne caighdeánach BERT (Devlin et al., 2019), agus cailleadh níos lú ná pointe absalóideach san fheidhmíocht.', 'ka': 'ჩვენ განახსენებთ SustaiNLP 2020 გაყოფილი დავალება: ეფექტიური ინფრენცია SuperGLUE ბანქმერი (Wang et al., 2019). მოთავსწავლებელი განსაზღვრებულებაზე გამოყენებულია კონფექტურაზე და ენერგიაზე გამოყენებულია ტესტის კონფექტურაზე. ჩვენ დავაწერეთ რაოდენობას, მისი ორგანიზაციას და გადატანა სისტემი. შვიდი წარმოდგენების გარეშე საზოგადომი საქმე დაახლოებით, მოთავსებელი 20 წლის ეფექტიურობის გარეშე სტანდარტული BERT (Devlin et al., 2019) სტანდარტული წარმოდგენებით, როცა აბსოლუტური წარ', 'el': 'Περιγράφουμε το κοινό καθήκον του 2020: αποτελεσματική συμπέρασμα σχετικά με το σημείο αναφοράς (κ.α., 2019). Οι συμμετέχοντες αξιολογούνται με βάση την απόδοση στο σημείο αναφοράς καθώς και την ενέργεια που καταναλώνεται για την πραγματοποίηση προβλέψεων για τα σετ δοκιμής. Περιγράφουμε το έργο, την οργάνωσή του και τα υποβαλλόμενα συστήματα. Σε όλες τις έξι υποβολές στην κοινή εργασία, οι συμμετέχοντες πέτυχαν κέρδη αποδοτικότητας των 20 σε σχέση με μια τυπική βάση βάσης BERT (Devlin et al., 2019), ενώ έχασαν λιγότερο από ένα απόλυτο σημείο απόδοσης.', 'hu': 'Leírjuk a SustaiNLP 2020 közös feladatot: hatékony következtetés a SuperGLUE referenciaértékre (Wang et al., 2019). A résztvevőket a referenciaérték alapján értékelik, valamint a tesztkészletekre vonatkozó előrejelzések során felhasznált energia alapján. Leírjuk a feladatot, annak szervezetét és a benyújtott rendszereket. A megosztott feladathoz benyújtott hat beadvány során a résztvevők 20%-os hatékonyságnövekedést értek el a szabványos BERT (Devlin et al., 2019), miközben kevesebb mint abszolút teljesítménypontot vesztettek el.', 'it': "Descriviamo il compito condiviso SustaiNLP 2020: inferenza efficiente sul benchmark SuperGLUE (Wang et al., 2019). I partecipanti sono valutati in base alle prestazioni sul benchmark e all'energia consumata nel fare previsioni sui set di test. Descriviamo il compito, la sua organizzazione e i sistemi presentati. Attraverso le sei candidature all'attività condivisa, i partecipanti hanno ottenuto un aumento di efficienza di 20 rispetto a una base di riferimento BERT standard (Devlin et al., 2019), perdendo meno di un punto assoluto in termini di prestazioni.", 'kk': 'Біз SustaiNLP 2020 ортақтастырылған тапсырманы түсіндіреміз: SuperGLUE бағдарламасының эффективні инфекциясы (Wang et al., 2019). Қатысушылар тескерту жиындарындағы прогноз жасау үшін тескерту белгісінің негізінде және энергия қолданылады. Біз тапсырманы, оның құрылымын және жүйелерді таңдаймыз. Ортақ тапсырмаға алты жіберілген тапсырманың көмегімен, қатысушылар стандартты BERT (Devlin et al., 2019) бағдарламасынан 20 жылдамдығын жеткізді, бірақ абсолюттік тапсырманың көмегінен кем жоқ болып тұрды.', 'mk': 'Ја опишуваме заедничката задача на СустајНЛП 2020: ефикасна инференција за benchmark СуперGLUE (Ванг и ал., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets.  Ја опишуваме задачата, нејзината организација и поднесените системи. Покрај шестте поднесувања на заедничката задача, учесниците постигнаа ефикасна зголемување од 20 во однос на стандардната основа на БЕРТ (Девлин и ал., 2019), истовремено губејќи помалку од апсолутна точка во перформансата.', 'lt': 'We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019).  Dalyviai vertinami remiantis lyginamuoju rodikliu ir energija, suvartojama atliekant bandymų rinkinių prognozes. Mes apibūdiname užduotį, jos organizavimą ir pateiktas sistemas. Per šešis bendros užduoties pareiškimus dalyviai pasiekė 20 efektyvumo padidėjimą, palyginti su standartiniu BERT (Devlin et al., 2019 m.), tačiau prarado mažiau nei absoliutus rezultatų rodiklius.', 'ms': 'Kami menggambarkan tugas berkongsi SustaiNLP 2020: kesimpulan efisien pada tanda referensi SuperGLUE (Wang et al., 2019). Peserta diteliti berdasarkan prestasi pada tanda referensi serta tenaga yang dikonsumsi dalam membuat ramalan pada set ujian. Kami menggambarkan tugas, organisasinya, dan sistem yang dihantar. Melalui enam penghantaran kepada tugas berkongsi, peserta mencapai peningkatan efisiensi 20 atas dasar BERT piawai (Devlin et al., 2019), sementara kehilangan kurang dari titik mutlak dalam prestasi.', 'ml': 'സുസ്സായെന്\u200dഎല്\u200dപി 2020 പങ്കെടുത്ത പണിയെക്കുറിച്ച് ഞങ്ങള്\u200d വിവരിച്ചുകൊടുക്കുന്നു: സൂപ്പര്\u200dജില്ലൂയി ബെന്\u200dച്മാര്\u200dക്കി പങ്കാളികള്\u200d ബെന്\u200dച്മാര്\u200dക്കിന്\u200dറെ പ്രവര്\u200dത്തനത്തിന്\u200dറെ അടിസ്ഥാനത്തും പരീക്ഷ സജ്ജീകരണങ്ങളില്\u200d പ്രവചനങ്ങള്\u200d ഉണ്ടാക്കുന ഞങ്ങള്\u200d ജോലിയെയും അതിന്\u200dറെ സംഘടനയെയും കീഴ്പെടുത്തിയ സിസ്റ്റത്തെയും വിശദീകരിക്കുന്നു. പങ്കെടുത്ത ജോലിയിലേക്കുള്ള ആറു കീഴ്പെടുത്തിയാല്\u200d പങ്കാളികള്\u200dക്ക് സാധാരണ ബെര്\u200dട്ടിന്\u200dറെ (ഡെവെല്\u200dലിന്\u200d et al., 2019) ബെസ്റ്റിലേക്ക് 20 കൂടുതല്\u200d', 'mn': 'Бид SustaiNLP 2020-ын хуваалцаагүй ажлыг тайлбарлаж байна: SuperGLUE багц дээр үр дүнтэй халдвар (Wang et al., 2019). Хэрэглэгчдийн хувьд шалгалтын хэмжээнд хэрэглэгдсэн энерги болон үйл ажиллагааны үндэслэлд үнэлдэг. Бид ажлыг, байгууллагын байгууллагын системийг тайлбарлаж байна. Холбоотой ажил дээр зургаан давтамжтайгаар оролцогчид стандарт BERT (Devlin et al., 2019) дээр 20 гаруй үр дүнтэй зарцуулалт гарч ирсэн. Гэхдээ ажиллагааны цэгээс бага байдаг.', 'mt': 'Aħna niddeskrivu l-kompitu komuni tas-SustaiNLP 2020: inferenza effiċjenti fuq il-punt ta’ riferiment tas-SuperGLUE (Wang et al., 2019). Il-parteċipanti huma evalwati abbażi tal-prestazzjoni tal-parametru referenzjarju kif ukoll tal-enerġija kkonsmata fit-tfassil tal-previżjonijiet fuq is-settijiet tat-test. Aħna niddeskrivu l-kompitu, l-organizzazzjoni tiegħu, u s-sistemi sottomessi. Across the six submissions to the shared task, participants achieved efficiency gains of 20 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.', 'ro': 'Descriem sarcina comună SustaiNLP 2020: inferență eficientă asupra benchmark-ului SuperGLUE (Wang et al., 2019). Participanții sunt evaluați pe baza performanței pe baza criteriului de referință, precum și a energiei consumate în realizarea previziunilor asupra seturilor de teste. Descriem sarcina, organizarea acesteia și sistemele prezentate. Pe parcursul celor șase depuneri la sarcina partajată, participanții au obținut câștiguri de eficiență de 20 față de o bază standard BERT (Devlin et al., 2019), pierzând în același timp mai puțin de un punct absolut în performanță.', 'sr': 'Opišemo zajednički zadatak SustaiNLP 2020: efikasna infekcija na referenciji SuperGLUE (Wang et al., 2019). Učesnici se procjenjuju na osnovu provedbe na kriteriji, kao i energije potrošena u predviđanju na testovima. Opišemo zadatak, organizaciju i podignute sisteme. Preko šest podataka zajedničkom zadatku, učesnici su postigli dobitak učinkovitosti od 20 iznad standardnog BERT-a (Devlin et al., 2019), dok su gubili manje od apsolutnog tačke izvršnosti.', 'pl': 'Opisujemy wspólne zadanie SustaiNLP 2020: efektywne wnioskowanie na benchmark SuperGLUE (Wang et al., 2019). Uczestnicy oceniani są na podstawie wydajności na poziomie referencyjnym oraz zużycia energii przy prognozowaniu zestawów testowych. Opisujemy zadanie, jego organizację oraz przesłane systemy. Podczas sześciu zgłoszeń do wspólnego zadania uczestnicy osiągnęli wzrost efektywności 20-ciu w stosunku do standardowej bazy danych BERT (Devlin et al., 2019), przy jednoczesnym utracie mniej niż bezwzględny punkt wydajności.', 'no': 'Vi beskriver delt oppgåve SustaiNLP 2020: effektivt infeksjon på superGLUE-benchmark et (Wang et al., 2019). Deltakarar er evaluert basert på utviklinga på benchmarket og energi brukt i å gjera foregåver på testinnstillingane. Vi beskriver oppgåva, organisasjonen sin, og dei sendte systema. Gjennom dei seks oppgåva i delt oppgåva, har deltakarane oppnådd effektiviteten av 20 over ein standardsbaseline BERT (Devlin et al., 2019), mens dei tapte mindre enn ein absolutt punkt i utføring.', 'so': 'Waxaannu sawiraynaa shaqada la qaybsaday SustaiNLP 2020: dhibaato faa’iido ah oo ku saabsan SuperGLUE benchmark (Wang et al., 2019). Aqoonsashada waxaa lagu qiimeeyaa sameynta sameynta bangiga iyo energiga lagu isticmaalayo wixii lagu sii sheegay samooyinka imtixaanka. Waxaynu qornaa shaqada, ururkiisa iyo nidaamka la soo dhiibay. Lix ka badan lixda hoos u dhigista shaqada la waday, kuwa ka qeybqaaday waxay gaadheen waxyaabaha saameyn ah 20 ka badan oo ka badan qoraalka hoose ee standard BERT (Devlin et al., 2019), iyadoo lumaya wax ka yar oo ka yar hal xal kamid ah.', 'si': 'අපි සුස්ටේයින්ල් බෙන්ච්මාර්ක් එක්ක සාමාන්\u200dය වැඩක් විස්තර කරනවා: සුප්ටයින්ලු බෙන්ච්මාර්ක් එක්ක ප්\u200dරශ්න අංශිකාරියෝ පරීක්ෂණ සෙට්ටුවේ ප්\u200dරමාණය අධික විශ්වාස කරලා තියෙනවා. අපි කාර්යය, ඒකේ සංවිධානය, සහ පද්ධතිය සංවිධානය විස්තර කරනවා. සාමාන්\u200dය වැඩකට පිළිගන්න හයක් පිළිගන්න, සාමාන්තාවන්ට ප්\u200dරමාණය BERT (Devlin et al., 2019) ප්\u200dරමාණයෙන් ප්\u200dරමාණයෙන් පිළිගන්න පුළුවන් විතර', 'sv': 'Vi beskriver SustaiNLP 2020 delad uppgift: effektiv slutsats på SuperGLUE-riktmärket (Wang et al., 2019). Deltagarna utvärderas utifrån prestandan utifrån riktmärket samt energianvändning vid prognoser på testuppsättningarna. Vi beskriver uppgiften, dess organisation och de inlämnade systemen. Under de sex inlämningarna till den delade uppgiften uppnådde deltagarna effektivitetsvinster på 20 jämfört med en standardreferensnivå för BERT (Devlin et al., 2019), samtidigt som de förlorade mindre än en absolut prestationspoäng.', 'ur': 'ہم SustaiNLP 2020 مشترک کام کو توصیح دیتے ہیں: SuperGLUE benchmark (Wang et al., 2019) پر قابل تفاوت ہے۔ شرکت کرنے والوں کو بنچم مارک پر فعالیت پر بنیاد رکھا گیا ہے اور انرژی کو آزمائش سٹوں پر پیش بینی کرنے کے لئے مصرف کیا گیا ہے۔ ہم نے اس کام کو، اس کی سازمانی کو اور اسلام دینے والی سیستموں کو توصیح دیتے ہیں۔ شش مسائل مشترک کام کے لئے، مشترکین نے ایک استاندارد BERT (Devlin et al., 2019) کے مقابلہ میں 20 فائدہ اضافہ حاصل کی، حالانکہ عملکرد میں ایک مطلق نقطہ سے کم خسارہ کر رہے تھے.', 'ta': 'SustaiNLP 2020 பங்கிடப்பட்ட பணியை நாம் விவரிக்கிறோம்: சூப்பர் GLUE பென்ச்மார்க்கு (வாங் et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets.  நாம் பணியை விவரிக்கிறோம், அதன் நிறுவனம், மற்றும் கொடுக்கப்பட்ட அமைப்புகள். பகிர்ந்த பணிக்கு ஆறு ஒப்புக்கொடுக்கும் போது, பங்கீட்டாளர்கள் செயல்பாட்டில் முழுமையான புள்ளியை விட குறைவாக இழந்துவிட்டார்கள்.', 'uz': "Biz SustaiNLP 2020 bilan birlashtirilgan vazifani tahlil qilamiz: SuperGLUE benchmark (Wang et al., 2019). Bogʻlamalar benchmark va sinov moslamalarda ishlatishda va energiya ishlatiladi. Biz vazifani, tashkilotni, tashkilotni va qo'shilga tizimlarni anglatamiz. Boʻlishilgan vazifa orqali sakkiz taʼminlovchiga ega bo'lgan qismilar andoza BERT (Devlin et al., 2019) bazasini 20 dan ortiq natijaga yetdi, va bajarish davomida kam narsa yoʻqoladi.", 'vi': 'Chúng tôi mô tả công việc chia sẻ SustaiNLP 2020: nhận biết hiệu quả về tiêu chuẩn SuperGLUE (Wang et al., 209). Người tham dự được đánh giá dựa trên khả năng tiêu chuẩn và năng lượng tiêu thụ trong việc dự đoán các bộ thử. Chúng tôi mô tả nhiệm vụ, tổ chức của nó, và hệ thống được gửi đến. Trong suốt sáu cuộc thuyết trình cho công việc chia sẻ, những người tham gia đã đạt được lợi nhuận hiệu suất cao hơn chục năm so với một cơ sở TNgiao ước tiêu chuẩn.', 'nl': 'We beschrijven de SustaiNLP 2020 gedeelde taak: efficiënte inferentie op de SuperGLUE benchmark (Wang et al., 2019). Deelnemers worden geëvalueerd op basis van prestaties op basis van de benchmark en energieverbruik bij het maken van voorspellingen op de testsets. We beschrijven de taak, de organisatie en de ingediende systemen. Tijdens de zes inzendingen voor de gedeelde taak bereikten de deelnemers efficiëntieverbeteringen van 20 boven een standaard BERT (Devlin et al., 2019) baseline, terwijl ze minder dan een absoluut prestatiepunt verloren.', 'bg': 'Описваме споделената задача за устойчиво развитие 2020: ефективно заключение по референтния показател (Уанг и др., 2019). Участниците се оценяват въз основа на показателите, както и на консумираната енергия при изготвянето на прогнози за тестовите комплекти. Описваме задачата, нейната организация и представените системи. В рамките на шестте заявления за споделената задача участниците постигнаха увеличение на ефективността от 20 спрямо стандартната база, като същевременно загубиха по-малко от абсолютна точка в представянето.', 'da': 'Vi beskriver SustaiNLP 2020 delte opgave: effektiv udledning på SuperGLUE benchmark (Wang et al., 2019). Deltagerne evalueres på baggrund af resultaterne på grundlag af benchmark samt energiforbruget ved at udarbejde forudsigelser på testsættene. Vi beskriver opgaven, dens organisation og de indsendte systemer. På tværs af de seks indsendelser til den fælles opgave opnåede deltagerne effektivitetsgevinster på 20 i forhold til en standard BERT (Devlin et al., 2019), mens de tabte mindre end et absolut præstationspunkt.', 'hr': 'Opišemo zajednički zadatak SustaiNLP 2020: učinkovita infekcija na referenciji SuperGLUE (Wang et al., 2019). Učesnici se procjenjuju na temelju učinkovitosti na referenciji, kao i energije potrošena u predviđanju na testovima. Opišemo zadatak, organizaciju i podignute sustave. Preko šest podataka zajedničkom zadatku, učesnici su postigli dobitak učinkovitosti od 20 iznad standardnog BERT-a (Devlin et al., 2019), dok su gubili manje od apsolutnog tačke učinkovitosti.', 'de': 'Wir beschreiben die gemeinsame Aufgabe SustaiNLP 2020: effiziente Inferenz auf den SuperGLUE Benchmark (Wang et al., 2019). Die Teilnehmer werden anhand der Leistung des Benchmarks sowie des Energieverbrauchs bei der Vorhersage der Testsets bewertet. Wir beschreiben die Aufgabe, ihre Organisation und die eingereichten Systeme. Bei den sechs Einreichungen für die gemeinsame Aufgabe erreichten die Teilnehmer Effizienzgewinne von 20 gegenüber einer Standard-BERT-Basislinie (Devlin et al., 2019), während sie weniger als einen absoluten Leistungspunkt verloren.', 'fa': 'ما وظیفه مشترک SustaiNLP 2020 را توصیف می\u200cکنیم: آلودگی موثرت در سنجام SuperGLUE (Wang et al., 2019). شرکتگران بر اساس عملکرد روی صندوق و انرژی که در پیش\u200cبینی\u200cها در مجموعه\u200cهای آزمایش استفاده می\u200cشود، ارزیابی می\u200cشوند. ما این کار، سازمان آن و سیستم\u200cهای تسلیم شده را توصیف می\u200cکنیم. در طول شش تسلیم به وظیفه مشترک مشترک، مشترک\u200cکننده\u200cها از بیست نفر بر اساس استاندارد BERT (Devlin et al., 2019) در حالی که کمتر از یک نقطه کامل در عملکرد از دست دادند.', 'id': 'Kami menggambarkan tugas bersama SustaiNLP 2020: kesimpulan efisien pada benchmark SuperGLUE (Wang et al., 2019). Peserta diuji berdasarkan prestasi pada benchmark dan energi yang dikonsumsi dalam membuat prediksi pada set tes. Kami menggambarkan tugas, organisasinya, dan sistem yang diserahkan. Melalui enam pengiriman ke tugas bersama, peserta mencapai peningkatan efisiensi 20 atas dasar BERT standar (Devlin et al., 2019), sementara kehilangan kurang dari titik absolut dalam prestasi.', 'ko': '우리는 SustaiNLP 2020 공유 임무인 슈퍼 GLUE 기준에 대한 효과적인 추정(Wang 등, 2019년)을 묘사했다.참여자의 평가는 기준 테스트의 성능과 테스트 집합에서 소모된 에너지를 예측하는 것을 바탕으로 한다.우리는 임무, 임무 조직, 제출 시스템을 설명했다.공유 임무에 제출된 6개 보고서 중 참여자의 효율은 표준인 BERT(Devlin 등·2019년) 기준선보다 20포인트 높아졌지만, 표현상 손실은 절대 점수가 하나도 되지 않았다.', 'sw': 'Tunaelezea jukumu la SustaiNLP 2020 lililoshirikishwa: kutokuwepo kwa ufanisi kwenye bendera ya SuperGLUE (Wang et al., 2019). Washiriki wanatathmini kwa kutumia utendaji wa bendera pamoja na umeme unaotumiwa katika kutengeneza utabiri katika seti za jaribio. Tunaelezea kazi, shirika hilo, na mfumo uliotolewa. Katika madai sita ya kazi hiyo, washiriki walifanikiwa kupata ufanisi wa mafanikio ya 20 zaidi ya msingi wa BERT (Devlin et al., 2019), wakati wakipoteza chini ya pointi kamili katika utendaji.', 'sq': 'Ne e përshkruajmë detyrën e përbashkët SustaiNLP 2020: përfundim efektiv mbi referencën SuperGLUE (Wang et al., 2019). Pjesëmarrësit vlerësohen në bazë të performancës në normën si dhe energjisë së konsumuar në bërjen e parashikimeve në grupet e testit. Ne e përshkruajmë detyrën, organizatën e saj dhe sistemet e paraqitura. Përballë gjashtë paraqitjeve të detyrës së përbashkët, pjesëmarrësit arritën rritje efektshmërie prej 20 mbi një bazë standarte BERT (Devlin et al., 2019), duke humbur më pak se një pikë absolute në performancë.', 'am': 'የሱሳይነልፕ 2020 የተካፈለ ስራዎችን እናሳውቃለን፤ በSuperGLUE benchmark (Wang et al., 2019). የውይይት መድረክ እና የድምፅ ውይይት በመፈተናው ላይ የመፍጠር ውይይት በመጠቀም ላይ የሚጠቀሙት ኃይል ተካክሎአል፡፡ ስራውን እና ድርጅቷን እና የተገኘውን ስርዓት እናሳውቃለን፡፡ በተካፈሉት ስድስት ስራ አቀራቢዎች፣ ተጋሪዎቹ የድምፅ ብኤርቴን (ዴብሊን et al., 2019) በጥቅረት ላይ 20 ጥቅም አግኝተዋል፡፡', 'tr': "SustaiNLP 2020'yň paylaşyk zadyny tassyýarys: SuperGLUE benchmarkynda etkinlik hasaplanja (Wang et al., 2019). Çagyşçylar benchmarkyň üstünde performans we testi düzlemlerinde ulanylan enerji we çykyşlar üçin deňlenýärler. Biz bu zady, organizasiýasyny we gönderilen sistemalary tassyýarys. Sahypa görevini golaýlaşdyran alty teslim edip, iştirakçiler 20 sany standart BERT (Devlin et al., 2019) üstünde etkinlik gazanlygyna ýetdiler, ýöne çykyş noktadan az bolup ýitirdiler.", 'af': "Ons beskrywe die SustaiNLP 2020 gedeelde taak: effektief inferensie op die SuperGLUE benchmark (Wang et al., 2019). Deelnaders word uitgewerk gebaseer op prestasie op die benchmark en energie wat gebruik word in voorskou maak op die toets stel. Ons beskrywe die taak, sy organisasie en die onderdra stelsels. Oor die ses onderskrifte a an die gedeelde taak het deelnaders effektief verkry van 20 oor 'n standaard BERT (Devlin et al., 2019) basisline, terwyl minder as 'n absolute punt in prestasie verloor het.", 'hy': 'Մենք նկարագրում ենք ՍուստայՆԼՊ 2020-ի ընդհանուր խնդիրը՝ արդյունավետ եզրակացություն ՍուպերԳԼԵՎ հարաբերականի վրա (Վանգ և այլն., 2019 թ․։ մասնակիցներին գնահատվում են համեմատային նպատակի արդյունքների վրա, ինչպես նաև փորձարկման համակարգերի կանխատեսումների ընթացքում օգտագործվող էներգիայի վրա: Մենք նկարագրում ենք խնդիրը, դրա կազմակերպությունը և ներկայացված համակարգերը: Համադրված խնդրի վեց ներկայացումների ընթացքում մասնակիցները արդյունավետության բարձրացում էին 20-ից ստանդարտ BER-ի (Devլին և այլն., 2019) հիմքում, մինչդեռ արդյունավետության պակաս կորցրին:', 'az': 'SustaiNLP 2020 paylaşılan işləri təsdiq edirik: SuperGLUE benchmark (Wang et al., 2019). İtirafçılar, sınama qurğularının tədbirlərini yaratmaq üçün istifadə edilən enerji və tədbirlərin istifadə edilməsi üzərində müəyyən edilir. Biz bu işi, onun organizasiyasını və göndərilmiş sistemləri təsdiqləyirik. Bölünən altı təbliğ təbliğlərindən ötrü, iştirakçilər standart BERT (Devlin et al., 2019) ilə 20 dəfə müvəffəqiyyət qazandıqlarını başa düşdülər, mütləq müəyyən edilmə məqsədilindən az qaldıqları halda.', 'bn': 'আমরা সুসাইটেনএলপি ২০২০ শেয়ার কর্মসূচি বর্ণনা করেছি: সুপার্জিলুই বেনম্যার্ক (ওয়াং এন্ট আল, ২০১৯)। অংশগ্রহণকারীরা বেনম্যার্ক এবং পরীক্ষা সেটের ভবিষ্যৎবাণী তৈরি করার ভিত্তিক ভিত্তিতে মূল্যায়ন করা হয়েছে। আমরা কাজ, এর সংগঠন এবং জমা দেয়া সিস্টেম বর্ণনা করি। অংশগ্রহণকারীরা সাধারণ বিবেরেট (ডেভেলিন এন্ট এল, ২০১৯) বেসালাইনে ২০টির বেশি কার্যকর অর্জন পেয়েছেন, যখন তারা প্রদর্শনের পরিপূর্ণ বিন্দুর কম হারেছে।', 'bs': 'Opišemo zajednički zadatak SustaiNLP 2020: učinkovita infekcija na referenciji SuperGLUE (Wang et al., 2019). Učesnici se procjenjuju na temelju učinkovitosti na referenciji, kao i energije potrošena u predviđanju na testovima. Opišemo zadatak, organizaciju i podignute sustave. Preko šest podataka zajedničkom zadatku, učesnici su postigli dobitak učinkovitosti od 20 na početnoj liniji BERT (Devlin et al., 2019), dok su gubili manje od apsolutnog tačke učinkovitosti.', 'ca': "Descrivem la tasca compartida SustaiNLP 2020: inferència eficient sobre el benchmark SuperGLUE (Wang et al., 2019). Els participants s'evaluen basant en el rendiment de l'indicador de referència i en l'energia consumida per fer prediccions sobre els conjunts d'exàmens. Descrivem la tasca, la seva organització i els sistemes submetits. A través de les sis propostes a la tasca compartida, els participants van aconseguir gains d'eficiència de 20 sobre una base normal de BERT (Devlin et al., 2019), perdent menys d'un punt absolut de rendiment.", 'cs': 'Popisujeme společný úkol SustaiNLP 2020: efektivní odvoz na benchmark SuperGLUE (Wang et al., 2019). Účastníci jsou hodnoceni na základě výkonnosti na benchmarku a spotřeby energie při provádění predikcí na testovacích sadách. Popisujeme úkol, jeho organizaci a předložené systémy. V průběhu šesti příspěvků ke sdílenému úkolu účastníci dosáhli zvýšení efektivity ve výši 20 nad standardní základní hodnotou BERT (Devlin et al., 2019), přičemž ztratili méně než absolutní bod výkonu.', 'et': 'Me kirjeldame SustaiNLP 2020 jagatud ülesannet: tõhusat järeldust SuperGLUE võrdlusaluse kohta (Wang et al., 2019). Osalejaid hinnatakse võrdlusaluse tulemuslikkuse ja katsekomplektide prognooside tegemisel tarbitud energia põhjal. Kirjeldame ülesannet, selle korraldust ja esitatud süsteeme. Ühise ülesande kuue taotluse puhul saavutasid osalejad tõhususe suurenemist 20 võrra võrreldes standardse BERT-i (Devlin jt., 2019) lähtetasemega, kaotades samal ajal vähem kui absoluutse tulemuspunkti.', 'fi': 'Kuvaamme SustaiNLP 2020:n jaettua tehtävää: tehokasta päättelyä SuperGLUE-vertailuarvosta (Wang et al., 2019). Osallistujat arvioidaan vertailuarvon perusteella sekä testisarjojen ennusteiden tekemiseen käytetyn energian perusteella. Kuvaamme tehtävän, sen organisaation ja toimitetut järjestelmät. Kuudessa yhteiseen tehtävään tehdyssä hakemuksessa osallistujat saavuttivat 20 tehokkuusparannusta normaaliin BERT-viitetasoon verrattuna (Devlin et al., 2019), mutta menettivät vähemmän kuin absoluuttisen pisteen suorituskyvyssä.', 'sk': 'Opisujemo skupno nalogo SustaiNLP 2020: učinkovit sklep o referenčni vrednosti SuperGLUE (Wang et al., 2019). Udeleženci se ocenjujejo na podlagi učinkovitosti na podlagi referenčne vrednosti in porabljene energije pri napovedovanju preskusnih sklopov. Opisujemo nalogo, njeno organizacijo in predložene sisteme. Med šestimi prijavami za skupno nalogo so udeleženci dosegli 20 večjo učinkovitost v primerjavi s standardnim izhodiščem BERT (Devlin et al., 2019), medtem ko so izgubili manj kot absolutno točko uspešnosti.', 'ha': "Tuna bayyana al'amarin SuperGLUU (Wang et al., 2019). Ana ƙaddara shirin mãsu haɗuwa a kan tafiyar da shi a kan bangon-bangon, da kuma an yi amfani da abinci da kuma a yi bayani-bayani ga matsayin jarrabãwa. We describe the task, its organization, and the submitted systems.  Ko cikin sakan wasiyyar zuwa aikin da aka raba shi, masu haɗi sun sami fassara matsayin 20 over a ƙaramin basalin BERT (Devlin et al., 2019) da kuma suna da mafi ƙaranci daga duk point a cikin aikin.", 'he': 'אנחנו מתארים את המשימה המשותפת של SustaiNLP 2020: תוצאה יעילה על המרמז של SuperGLUE (Wang et al., 2019). השתתפים מוערכים בהתבסס על ביצועים על נקודת התייחס, כמו גם על אנרגיה שתוצרפת בכך שיוצרו חזיונות על קבוצות הבדיקות. אנחנו מתארים את המשימה, הארגון שלה, והמערכות המועברות. Across the six submissions to the shared task, participants achieved efficiency gains of 20 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.', 'bo': 'ང་ཚོས་SustaiNLP 2020་དེ་སྤྱད་དོན་གྱི་ལས་ཀ་གསལ་བཤད་པ་ཡིན། SuperGLUE benchmark(Wang et al., 2019). ཞུགས་མི་དག་གིས་བརྟག་དཔྱད་གཞི་རྟེན་ནས་བྱ་རིམ་དང་ནུས་པ་སྟོན་པའི་དཔག་འབྲས་ཀྱི་ནང་དུ་ཞིབ་དཔྱད་བྱེད་ཀྱི་ཡོད། ང་ཚོས་བྱ་འགུལ་དང་། དེའི་སྒྲིག་འཛུགས་དང་། མ་ལག་གནང་བ་ཚ་ལ་འགྲེལ་བཤད་ཀྱི་ཡོད། མཉམ་སྤྱོད་གྱི་ལས་འགུལ་གྱི་གནད་དོན་ཞིག་ལ་བརྟེན་ནས། ཞུགས་སྤྱི་ཚོགས་ཀྱིས་མཐུན་རྐྱེན་ཚད་གཞི་BERT (Devlin et al., 2019) ལ་ཟུར་བ་རྒྱུ་དང་།', 'jv': 'Awakdhéwé ngerasakno nggawe Delajeng SusaiNLP 2020 nggawe gerakan: nggawe barang penggunan luwih nggawe bench (Wang et al, 2011). Jamo Awakdhéwé pisan nggambar nggawé, akeh pancening nggawe lan sistem sing nyimpen. Nanging ono sithik sing rumangsa nggawe nggawe task gawe, wong-wong liyane wis mpungan kanggo awak dhéwé sing luwih dumadhi sabanjuré BERT (Devline et al, 2011), dadi kapan tanggal sing titik dhéwé, nik awak dhéwé kuwi sing perusahaan akses.'}
